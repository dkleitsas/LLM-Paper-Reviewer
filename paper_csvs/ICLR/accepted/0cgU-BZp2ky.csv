Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029154518950437317,"Human intervention is an effective way to inject human knowledge into the loop
of reinforcement learning, bringing fast learning and training safety. But given the
very limited budget of human intervention, it is challenging to design when and
how human expert interacts with the learning agent in the training. In this work,
we develop a novel human-in-the-loop learning method called Human-AI Copi-
lot Optimization (HACO). To allow the agent’s sufﬁcient exploration in the risky
environments while ensuring the training safety, the human expert can take over
the control and demonstrate to the agent how to avoid probably dangerous situa-
tions or trivial behaviors. The proposed HACO then effectively utilizes the data
collected both from the trial-and-error exploration and human’s partial demonstra-
tion to train a high-performing agent. HACO extracts proxy state-action values
from partial human demonstration and optimizes the agent to improve the proxy
values while reducing the human interventions. No environmental reward is re-
quired in HACO. The experiments show that HACO achieves a substantially high
sample efﬁciency in the safe driving benchmark. It can train agents to drive in un-
seen trafﬁc scenes with a handful of human intervention budget and achieve high
safety and generalizability, outperforming both reinforcement learning and imita-
tion learning baselines with a large margin. Code and demo videos are available
at: https://decisionforce.github.io/HACO/."
INTRODUCTION,0.0058309037900874635,"1
INTRODUCTION"
INTRODUCTION,0.008746355685131196,"How to effectively inject human knowledge into the learning process is one of the key challenges to
training reliable autonomous agents in safety-critical applications. In reinforcement learning (RL),
researchers can inject their intentions into the carefully designed reward function. The learning
agent freely explores the environment to collect the data and develops the desired behaviors induced
by the reward function. However, RL methods bear two drawbacks that limit their applications in
safety-critical tasks: First, the nature of trial-and-error exploration exposes RL agent to dangerous
situations (Saunders et al., 2017). Second, it is difﬁcult to summarize all the intended behaviors to
be learned into the reward function. Taking the driving vehicle as an example, an ideal policy should
obtain a set of skills, such as overtaking, yielding, emergent stopping, and negotiation with other ve-
hicles. It is intractable to manually design a reward function that leads to the emergence of all those
behaviors in the trained agent. To mitigate these two challenges, practitioners enforce the human
intentions through imitation learning (IL) where the agent is trained to imitate the expert-generated
state and action sequences. During the demonstration, the premature agent does not interact with
the risky environment and thus the training safety is ensured. High-quality expert demonstrations
provide direct the optimal solution for the agent to imitate from. However, IL paradigm suffers from
the distributional shift problem (Ross & Bagnell, 2010; Ross et al., 2011) while the induced skills
are not sufﬁciently robust with respect to changes in the control task (Camacho & Michie, 1995)."
INTRODUCTION,0.011661807580174927,"Different from vanilla RL or IL, human-in-the-loop learning is an alternative paradigm to inject
human knowledge, where a human subject accompanies the agent and oversees its learning process.
Previous works require the human to either passively advise which action is good (Mandel et al.,
2017) or evaluate the collected trajectories (Christiano et al., 2017; Guan et al., 2021; Reddy et al.,"
INTRODUCTION,0.014577259475218658,∗Quanyi Li and Zhenghao Peng contribute equally to this work.
INTRODUCTION,0.01749271137026239,Published as a conference paper at ICLR 2022
INTRODUCTION,0.02040816326530612,Environment Agent
INTRODUCTION,0.023323615160349854,A. Human-AI Copilot
INTRODUCTION,0.026239067055393587,Safe Action
INTRODUCTION,0.029154518950437316,Intervention Cost
INTRODUCTION,0.03206997084548105,C. Copilot Trajectory
INTRODUCTION,0.03498542274052478,Agent Action
INTRODUCTION,0.037900874635568516,B. Human Interface State
INTRODUCTION,0.04081632653061224,Human Expert
INTRODUCTION,0.043731778425655975,"Figure 1: A. In the proposed human-in-the-loop method, human subjects will decide whether to
intervene in the execution of the learning agent and demonstrate the correct action. Such intervention
incurs a cost to the agent such that it learns to minimize the total cost during the training. B. The
human interface for copilot during the training. The human subject can steer the wheel or press the
paddle to start the intervention and then the demonstration data will be recorded. C. An illustrative
driving trajectory where the green segments are generated by the agent and the blue segments are
the demonstrations of the human expert to overcome the dangerous situations."
INTRODUCTION,0.04664723032069971,"2018; Warnell et al., 2018; Christiano et al., 2017; Sadigh et al., 2017; Palan et al., 2019). This
kind of passive human involvement exposes the human-AI system to risks since the agent explores
the environment without protection. Some other works require the human to merely intervene in
the exploration by terminating the episode (Saunders et al., 2018; Zhang & Cho, 2016), but it is
not practical to terminate and reset the environment instantly in the real world (Xu et al., 2020).
Intervening and taking over the control from the learning agent is a natural approach to safeguard
the human-AI system (Kelly et al., 2019; Spencer et al., 2020). However, a challenge exhibited in
previous works is the budget of human intervention. Since human cognitive resource is precious and
limited, it is essential to carefully design when and how the human expert involves in the learning
process so that the human knowledge can be injected effectively."
INTRODUCTION,0.04956268221574344,"In this work, we propose an efﬁcient human-in-the-loop learning method called Human-AI Copilot
Optimization (HACO). The key feature of HACO is that it can learn to minimize the human inter-
vention and adjust the level of automation to the learning agent adaptively during the training. As
shown in Figure 1 A, HACO allows the human expert to take over the human-AI system in a proac-
tive manner. If the human decides to intervene in the action of the agent, he/she should demonstrate
the correct actions to overcome current undesired situations to the learning agent. The human inter-
vention and the partial demonstration are two sources of informative training data. We use ofﬂine RL
technique to maintain a proxy value function of the human-AI mixed behavior policy even though
the agent doesn’t have the access to the environmental reward during training. To encourage the
exploration in the state-action space permitted by human, we also maximize the entropy of action
distribution of the agent if the agent is not taken over."
INTRODUCTION,0.052478134110787174,"Experiments in the virtual driving environments MetaDrive (Li et al., 2021) and CARLA (Doso-
vitskiy et al., 2017) show that, with an economic human budget, HACO outperforms RL and IL
baselines with a substantial margin in terms of sample efﬁciency, performance, safety, and general-
izability in the unseen testing environment. Thus the human-AI copilot optimization is an efﬁcient
learning paradigm to inject human knowledge in an online setting."
RELATED WORK,0.05539358600583091,"2
RELATED WORK"
RELATED WORK,0.05830903790087463,"Learning from Demonstration. Passive imitation learning such as behavior cloning (Widrow,
1964; Osa et al., 2018; Huang et al., 2020; Sun et al., 2020) and recently proposed ofﬂine RL
methods (Kumar et al., 2020; Fujimoto et al., 2018; Wu et al., 2019) train agents from an out-
of-the-shelf data set and guarantee the training safety, since no interaction with the environment
is needed. Inverse RL methods (Ng et al., 2000; Abbeel & Ng, 2004; Fu et al., 2017; Bloem &
Bambos, 2014) learn a reward function from the human demonstration and then use it to incentivize
the agents to master the intended behaviors. Proposed more recently, GAIL (Ho & Ermon, 2016)
and its variants (Song et al., 2018; Sasaki et al., 2018; Kostrikov et al., 2018) and SQIL (Reddy et al.,
2019) compare the trajectory similarity between agents and humans and thus require the agent to
interact with the environment. Similar to RL methods, this paradigm exposes the agent to potentially
dangerous situations."
RELATED WORK,0.061224489795918366,Published as a conference paper at ICLR 2022
RELATED WORK,0.0641399416909621,"Human-in-the-loop Learning Methods. Many works focus on incorporating human in the train-
ing loop of conventional RL or IL paradigms. DAgger (Ross et al., 2011) and its extended meth-
ods (Kelly et al., 2019; Zhang & Cho, 2016; Hoque et al., 2021) correct the compounding error (Ross
& Bagnell, 2010) of behavior cloning by periodically requesting expert to provide more demonstra-
tion. Instead of proving demonstration upon requests, Human-Gated DAgger (HG-DAgger) (Kelly
et al., 2019), Expert Intervention Learning (EIL) (Spencer et al., 2020) and Intervention Weighted
Regression (IWR) (Mandlekar et al., 2020) empower the expert to intervene exploration and carry
the agent to safe states. However, these methods do not impose constraints to reduce human inter-
vention and do not utilize the data from the free exploration of the agent. Human subjects can also
involve in the loop providing preferences based on evaluative feedback on two behavior sequences
generated by the agent (Christiano et al., 2017; Sadigh et al., 2017; Palan et al., 2019; Ibarz et al.,
2018; Cui & Niekum, 2018)."
RELATED WORK,0.06705539358600583,"Human-AI copilot or shared autonomy is a more intimate form of the human-in-the-loop methods.
The AI agent and human are working together simultaneously to achieve a common goal. By giv-
ing human guidance and feedback at run-time instantly, the explorable state and action spaces can
be greatly narrowed down (Saunders et al., 2018). The learning goal can further match the task
objective by providing extra human feedback combined with reward function (Reddy et al., 2018;
Warnell et al., 2018; Wu et al., 2021; Cederborg et al., 2015; Arumugam et al., 2019). Human-AI
copilot is helpful and practical when applying RL to real world tasks where safety constraints must
be satisﬁed (Garcıa & Fern´andez, 2015; Amodei et al., 2016; Bharadhwaj et al., 2020; Alshiekh
et al., 2018). In our previous work (Peng et al., 2021), we made attempt to develop a method called
Expert-Guided Policy Optimization (EGPO) where a PPO expert policy is involved to monitor the
learning agent. The difference can be summarized as twofold: (1) We substitute the expert with
human and design special mechanism to mitigate the delayed feedback error; (2) Based on the com-
prehensive ablation study and prototyping, we remove redundant designs like takeover function and
the need of reward function, making the proposed method simple yet effective."
RELATED WORK,0.06997084548104957,"Reducing human burden is a major challenge in human-in-the-loop methods. A feasible solution is
to learn an intervention function that imitates human intervention signals and stops the catastrophic
actions of agents (Kelly et al., 2019; Zhang & Cho, 2016; Saunders et al., 2017; Abel et al., 2017),
which can relieve the mental stress of the human subject during training. In this work, we devise our
learning scheme explicitly to include the human cognitive cost as one of the objectives to minimize."
HUMAN-AI COPILOT OPTIMIZATION,0.0728862973760933,"3
HUMAN-AI COPILOT OPTIMIZATION"
HUMAN-AI COPILOT OPTIMIZATION,0.07580174927113703,"In this section, we introduce Human-AI Copilot Optimization (HACO), an efﬁcient learning algo-
rithm that trains agents from human interventions, partial demonstrations and free exploration. For
human-in-the-loop learning, it is essential to design when and how to engage human subjects. The
major issue is the cognitive cost of the human subject (Zhang et al., 2021). Frequent querying might
bring tremendous cognitive cost and exhaust the human expert, causing incorrect or delayed feed-
back that hinders the training. Thus the proposed pipeline aims to minimize the human intervention
cost during the training, which reduces the reliance on the expert’s demonstration over time and im-
proves the learning agent’s autonomy. The overall workﬂow of HACO is presented in Algorithm 1."
HUMAN-AI COPILOT TRAINING PARADIGM,0.07871720116618076,"3.1
HUMAN-AI COPILOT TRAINING PARADIGM"
HUMAN-AI COPILOT TRAINING PARADIGM,0.08163265306122448,"We aim to learn an autonomous agent with policy πn(an|s) that can make informed action an in
state s. As shown in Fig. 1, we frame the human-AI copilot paradigm that extends the standard
reinforcement learning diagram by incorporating a human expert. At each step, the human expert
oversees current state and decides whether to intervene. If necessary, he/she will execute human
action ah to overwrite the agent’s action an. We denote the human intervention by a Boolean
indicator I(s, an) and thus the action applied to the environment is called the safe action ˆa =
I(s, an)ah + (1 −I(s, an))an. Denoting the human policy as πh, the actual trajectories occurred
during training are derived from a shared behavior policy πb:"
HUMAN-AI COPILOT TRAINING PARADIGM,0.08454810495626822,"πb(a|s) = πn(a|s)(1 −I(s, a)) + πh(a|s)G(s),
(1)"
HUMAN-AI COPILOT TRAINING PARADIGM,0.08746355685131195,"wherein G(s) =
R"
HUMAN-AI COPILOT TRAINING PARADIGM,0.09037900874635568,"a′∈A I(s, a′)πn(a′|s)da′ is the probability of the agent choosing an action that
will be rejected by the human."
HUMAN-AI COPILOT TRAINING PARADIGM,0.09329446064139942,Published as a conference paper at ICLR 2022
HUMAN-AI COPILOT TRAINING PARADIGM,0.09620991253644315,"We call the transition sequences during the takeover {(st, an,t, ah,t, I(st, an,t), st+1), ...} as the
partial demonstration.
The partial demonstration and the free exploration transitions will be
recorded in the replay buffer B and fed to the training pipeline. Note that we do not require to
store environmental reward and cost into the buffer since the proposed method does not need them."
HUMAN-AI COPILOT TRAINING PARADIGM,0.09912536443148688,"In the human-AI copilot training, the human is obligated to guide the agent learning and safeguard
the learning process by proactively taking over the control if necessary. This paradigm rules out the
dispensable states and mitigates the safety concern in free exploration of RL and active imitation
learning methods (Ross et al., 2011). Different from previous ofﬂine RL works training from ﬁxed
dataset (Bojarski et al., 2016; Ho & Ermon, 2016; Reddy et al., 2019; Kumar et al., 2020; Fujimoto
et al., 2018; Wu et al., 2019) where no closed loop feedback is accessible, the human-AI copilot
training produces partial demonstrations that contains the necessary human knowledge to overcome
dangerous situations into the learning. The copilot nature alleviates the distributional shift prob-
lem, since the human intervenes when the agent performs suspicious behaviors, so that there is a
continuity of the state visitation between the agent and the expert."
HUMAN-AI COPILOT TRAINING PARADIGM,0.10204081632653061,"In next section, we will introduce how we instantiate the human-AI copilot paradigm with a human-
efﬁcient algorithm that can effectively optimize the agent toward safe and high-performing policy."
LEARNING OBJECTIVES,0.10495626822157435,"3.2
LEARNING OBJECTIVES"
LEARNING OBJECTIVES,0.10787172011661808,"We form three objectives that fully utilize the human data: (1) Agent should maximize a proxy value
function Q(s, a) which reﬂects human intentions on how to ﬁnish the task. (2) Agent should explore
thoroughly to visit the state-action subspace permitted by the human. Concretely, we maximize the
action distribution entropy H(π(·|s)). (3) Agent should maximize the level of automation and reduce
human intervention. Episodic human intervention is estimated by an intervention value function
QI(s, a) based on the step-wise intervention cost C(s, a). Thus the overall learning objective of
HACO becomes:
max
π
E[Q(s, a) + H(π) −QI(s, a)].
(2)"
LEARNING OBJECTIVES,0.11078717201166181,We then discuss the practical implementation of aforementioned design goals.
LEARNING OBJECTIVES,0.11370262390670553,"Proxy value function. HACO follows reward-free setting so we can’t estimate the expected state-
action value based on a ground-truth reward function deﬁned by the environment. We instead es-
timate a proxy value function Q(s, a; φ) (φ is model parameters) that captures the ordinal pref-
erence of human experts, which implicitly reﬂects human intentions. We utilize the conservative
Q-learning (Kumar et al., 2020) and form the optimization problem of the proxy value function as:"
LEARNING OBJECTIVES,0.11661807580174927,"min
φ
E
(s,an,ah,I(s,an))∼B[I(s, an)(Q(s, an; φ) −Q(s, ah; φ))].
(3)"
LEARNING OBJECTIVES,0.119533527696793,"The above optimization objective can be interpreted as being optimistic to the human’s action ah
and pessimistic to the agent’s action an. The proxy value function learns to represent the high-value
state-action subspace preferred by the human expert."
LEARNING OBJECTIVES,0.12244897959183673,"Entropy regularization. If the learning agent visits human-preferable subspace insufﬁciently dur-
ing free explorable sampling, the states evoking high proxy value are rarely encountered, making
the back-propagation of the proxy value to preceding states difﬁcult and thus damaging the learning.
To encourage exploration, we adopt the entropy regularization technique in (Haarnoja et al., 2018)
and forms auxiliary signal to update the proxy value function apart from Eq. 3:"
LEARNING OBJECTIVES,0.12536443148688048,"min
φ
E
(st,ˆat,st+1)∼B[y −Q(st, ˆat; φ)]2,
y = γ
E
a′∼πn(·|st+1)[Q(st+1, a′; φ′) −α log πn(a′|st+1)],"
LEARNING OBJECTIVES,0.1282798833819242,"(4)
wherein ˆat is the executed action at state st, φ′ denotes the delay updated parameter of the target
network, γ is the discount factor. Since the environment reward is not accessible to HACO, we
remove the reward term in the update target y. Combining Eq. 3 and Eq. 4, the formal optimization
objective of the proxy value function becomes:"
LEARNING OBJECTIVES,0.13119533527696792,"min
φ E
B[(y −Q(st, ˆat; φ))2 + I(st, an,t)(Q(st, an,t; φ) −Q(st, ah,t; φ))].
(5)"
LEARNING OBJECTIVES,0.13411078717201166,Published as a conference paper at ICLR 2022
LEARNING OBJECTIVES,0.13702623906705538,Algorithm 1: The workﬂow of HACO during training
INITIALIZE AN EMPTY REPLAY BUFFER B,0.13994169096209913,1 Initialize an empty replay buffer B
INITIALIZE AN EMPTY REPLAY BUFFER B,0.14285714285714285,2 while Training is not ﬁnished do
WHILE EPISODE IS NOT TERMINATED DO,0.1457725947521866,"3
while Episode is not terminated do"
WHILE EPISODE IS NOT TERMINATED DO,0.14868804664723032,"4
an,t ∼πn(·|st) Retrieve agent’s action"
WHILE EPISODE IS NOT TERMINATED DO,0.15160349854227406,"5
I(st, an,t) ←Human expert decides whether to intervene by observing current state st"
WHILE EPISODE IS NOT TERMINATED DO,0.15451895043731778,"6
if I(st, an,t) is True then"
WHILE EPISODE IS NOT TERMINATED DO,0.15743440233236153,"7
ah,t ←πh(·|st) Retrieve human’s action"
WHILE EPISODE IS NOT TERMINATED DO,0.16034985422740525,"8
Apply ah,t to the environment"
ELSE,0.16326530612244897,"9
else"
ELSE,0.1661807580174927,"10
Apply an,t to the environment"
ELSE,0.16909620991253643,"11
if I(st, an,t) is True and I(st−1, an,t−1) is False then"
ELSE,0.17201166180758018,"12
C(st, an,t) ←Compute intervention cost following Eq. 6"
ELSE,0.1749271137026239,"13
else"
ELSE,0.17784256559766765,"14
C(st, an,t) ←0 Set intervention cost to zero"
ELSE,0.18075801749271136,"15
Record st, an,t, I(st, an,t) and ah,t (if I(st, an,t)) to the buffer B"
ELSE,0.1836734693877551,"16
Update proxy value Q, intervention value QI and policy π according to Eq. 5, Eq. 7, Eq. 8
respectively"
ELSE,0.18658892128279883,"Reducing human interventions. Directly optimizing the agent policy according to the proxy value
function will lead to failure when evaluating the agent without human participation. This is because
Q(s, a) represents the proxy value of the mixed behavior policy πb instead of the learning agent’s
πn due to the existence of human intervention. It is possible that the agent learns to deliberately
abuse human intervention by always taking actions that violate human intentions, such as driving
off the road when near the boundary, which forces human to take over and provide demonstrations.
In this case, the level of automation for the agent is low and the human subject exhausts to provide
demonstrations. Ablation study result in Table 2(c) illustrates this phenomenon."
ELSE,0.18950437317784258,"To economically utilize human budget and reduce the human interventions over time, we punish
the agent action that triggers human intervention in a mild manner by using the cosine similarity
between agent’s action and human’s action as the intervention cost function in the form below:"
ELSE,0.1924198250728863,"C(s, an) = 1 −
anTah
||an||||ah||, ah ∼πh(·|s).
(6)"
ELSE,0.19533527696793002,"The agent will receive large penalty only when its action is signiﬁcantly different from the expert
action in terms of cosine similarity."
ELSE,0.19825072886297376,"A straightforward form of C is a constant +1 when human expert issues intervention. However, we
ﬁnd that there usually exists temporal mismatch ϵ between human intervention and faulty actions so
that the intervention cost is given to the agent at a delayed time step t + ϵ. It is possible that the
agent’s action an,t+ϵ is a correct action that saves the agent itself from dangers but is mistakenly
marked as faulty action that triggers human intervention. In the ablation study, we ﬁnd that using
the constant cost raises inferior performance compared to the cosine similarity."
ELSE,0.20116618075801748,"As shown in Line 11-14 of Algorithm 1, we only yield non-zero intervention cost at the ﬁrst step
of human intervention. This is because the human intervention triggered by the exact action an,t
indicates this action violates the underlying intention of human at this moment. Minimizing the
chance of those actions will increase the level of automation."
ELSE,0.20408163265306123,"To improve the level of automation, we form an additional intervention value function QI(s, a) as
the expected cumulative intervention cost, similar to estimating the state-action value in Q-learning
through Bellman equation:"
ELSE,0.20699708454810495,"QI(st, an,t) = C(st, an,t) + γ
E
st+1∼B,at+1∼πn(·|st+1)[QI(st+1, at+1)].
(7)"
ELSE,0.2099125364431487,This value function is used to directly optimize the policy.
ELSE,0.21282798833819241,"Learning policy. Using the entropy-regularized proxy value function Q(s, a) as well as the inter-
vention value function QI(s, a), we form the the policy improvement objective as:"
ELSE,0.21574344023323616,"max
θ
E
st∼B[Q(st, an) −α log πn(an|st; θ) −QI(st, an)], an ∼πn(·|st; θ).
(8)"
ELSE,0.21865889212827988,Published as a conference paper at ICLR 2022
ELSE,0.22157434402332363,Figure 2: Examples of the safe driving environments from MetaDrive used in the experiments.
EXPERIMENTS,0.22448979591836735,"4
EXPERIMENTS"
EXPERIMENTAL SETTINGS,0.22740524781341107,"4.1
EXPERIMENTAL SETTINGS"
EXPERIMENTAL SETTINGS,0.2303206997084548,"Task. We focus on the driving task in this work. This is because driving is an important decision
making problem with a huge social impact, where safety and training efﬁciency are critical. Since
many researches on autonomous driving employ human in a real vehicle (Bojarski et al., 2016;
Kelly et al., 2019), the human safety and human cognitive cost become practical challenges that
limit the application of learning-based methods in industries. Therefore, the driving task is an ideal
benchmark for the human-AI copilot paradigm."
EXPERIMENTAL SETTINGS,0.23323615160349853,"Simulator. Considering the potential risks of employing human subjects in physical experiments,
we benchmark different approaches in the driving simulator. We employ a lightweight driving sim-
ulator MetaDrive (Li et al., 2021), which preserves the capacity to evaluate the safety and gener-
alizability in unseen environments. The simulator is implemented based on Panda3D (Goslin &
Mine, 2004) and Bullet Engine that has high efﬁciency as well as accurate physics-based 3D ki-
netics. MetaDrive uses procedural generation to synthesize an unlimited number of driving maps
for the split of training and test sets, which is useful to benchmark the generalization capability of
different approaches in the context of safe driving. Some generated driving scenes are presented in
Fig. 2. The simulator is also extremely efﬁcient and ﬂexible so that we can run the human-AI copilot
experiment in real-time. Though we mainly describe the setting of MetaDrive in this section, we
also experiment on CARLA (Dosovitskiy et al., 2017) simulator in Sec. 4.3."
EXPERIMENTAL SETTINGS,0.23615160349854228,"Training Environment. In the simulator, the task for the agent is to steer the target vehicle with
low-level control signal, namely acceleration, brake and steering, to reach the predeﬁned destination
and receive a success ﬂag. The ratio of episodes where the agent successfully reaches the destination
is called the success rate. To increase the difﬁculty of the task, we scatter obstacles randomly in each
driving scene such as movable trafﬁc vehicles, ﬁxed trafﬁc cones, and warning triangles."
EXPERIMENTAL SETTINGS,0.239067055393586,"The observation contains (1) the current states such as the steering, heading, velocity and relative
distance to boundaries etc., (2) the navigation information that guides the vehicle toward the desti-
nation, and (3) the surrounding information encoded by a vector of 240 Lidar-like distance measures
of the nearby vehicles."
EXPERIMENTAL SETTINGS,0.24198250728862974,"Though HACO does not receive environmental reward during training, we provide reward function
to train baseline methods and evaluate HACO in test time. The reward function contains a dense
driving reward, speed reward and a sparse terminal reward. The driving reward measures the longi-
tudinal movement toward destination. We also reward agent according to its velocity and give the
sparse reward +20 when the agent arrives at the destination."
EXPERIMENTAL SETTINGS,0.24489795918367346,"Each collision to the trafﬁc vehicles or obstacles yields +1 environmental cost. Note that HACO
can not access this cost during the training. This cost is used to train safe RL baselines as well as
for testing the safety of trained policies. We term the episodic cost as safety violation which is the
measurement on the safety of a policy."
EXPERIMENTAL SETTINGS,0.2478134110787172,"We invite the human expert to supervise the real-time exploration of the learning agent with hands
on the steering wheel, as shown in the Fig. 1B. When a dangerous situation is going to happen, the
human takes over the vehicle by pressing the paddle besides the wheel and starts controlling the
vehicle by steering the wheel and stepping the pedals."
EXPERIMENTAL SETTINGS,0.25072886297376096,"Split of training and test sets. Different from the conventional RL setting where the agent is trained
and tested in the same ﬁxed environment, we focus on evaluating the generalization performance
through testing the trained agents in separated test environments. We split the driving scenes into
the training set and test set with 50 different scenes in each set. After each training iteration, we roll
out the learning agent without guardian in the test environments and record success rate and safety
violation given by the environment and present it in Table 1."
EXPERIMENTAL SETTINGS,0.2536443148688047,Published as a conference paper at ICLR 2022
EXPERIMENTAL SETTINGS,0.2565597667638484,Table 1: The performance of different approaches in safe driving benchmark.
EXPERIMENTAL SETTINGS,0.2594752186588921,"Category
Method
Total Training
Safety Violation
Training
Data Usage
Test
Return
Test Safety
Violation
Test
Success Rate"
EXPERIMENTAL SETTINGS,0.26239067055393583,"Expert
Human
-
-
358.19 ±86.00
0.16±0.61
0.98"
EXPERIMENTAL SETTINGS,0.2653061224489796,"RL
SAC-RS
2.76K ± 0.95K
1M
386.77 ±35.1
0.73 ±1.18
0.82 ±0.18
PPO-RS
24.34K ±3.56K
1M
335.39 ±12.41
3.41 ±1.11
0.69±0.08"
EXPERIMENTAL SETTINGS,0.26822157434402333,"Safe RL
SAC-Lag
1.84K ± 0.49K
1M
351.96 ±101.88
0.72 ±0.49
0.73 ±0.29
PPO-Lag
11.64K ± 4.16K
1M
299.99 ±49.46
1.18 ±0.83
0.51 ±0.17
CPO
4.36K ±2.22K
1M
194.06 ±108.86
1.71 ±1.02
0.21 ±0.29"
EXPERIMENTAL SETTINGS,0.27113702623906705,"Ofﬂine RL
CQL
-
36K
156.4 ±31.94
6.82 ±5.1
0.11 ±0.07"
EXPERIMENTAL SETTINGS,0.27405247813411077,"IL
BC
-
36K
101.63 ±16.06
1.00 ±0.45
0.01 ±0.03
GAIL
3.70K ±2.43K
36K
136.08 ±18.73
4.42 ±1.89
0.13 ±0.03"
EXPERIMENTAL SETTINGS,0.27696793002915454,"Human-in-
the-loop
HG-DAgger
38.35
50K
111.87
2.38
0.04
IWR
77.38
50K
299.78
3.39
0.64"
EXPERIMENTAL SETTINGS,0.27988338192419826,"Ours
HACO
30.14 ± 11.36
30K*
349.25 ± 11.45
0.79 ± 0.31
0.83 ± 0.04"
EXPERIMENTAL SETTINGS,0.282798833819242,"* During HACO training, in 8316 ± 497.90 steps out of the total 30K steps the human expert
intervenes and overwrites the agent’s actions. The whole training takes about 50 minutes."
EXPERIMENTAL SETTINGS,0.2857142857142857,"Implementation details. We conduct experiments on the driving simulator and implement algo-
rithms using RLLib (Liang et al., 2018), an efﬁcient distributed learning system. When training
the baselines, we host 8 concurrent trials in an Nvidia GeForce RTX 2080 Ti GPU. Each trial con-
sumes 2 CPUs with 8 parallel rollout workers. Except human-in-the-loop experiments, all baseline
experiments are repeated 5 times with different random seeds. The main experiments of HACO is
conducted on a local computer with an Nvidia GeForce RTX 2070 and repeat 3 times. The ablations
and baseline human-in-the-loop experiments repeat once due to the limited human budget. One
human subject participates in each experiment. In all tables and ﬁgures, we provide the standard
deviation if the experiments are repeated multiple runs with different random seeds. Information
about other hyper-parameters is given in the Appendix."
BASELINE COMPARISON,0.2886297376093295,"4.2
BASELINE COMPARISON"
BASELINE COMPARISON,0.2915451895043732,"We compare our method to vanilla RL and Safe RL methods which inject the human intention and
constraint through the pre-deﬁned reward function and cost function. We test native RL methods,
PPO (Schulman et al., 2017) and SAC (Haarnoja et al., 2018), with cost added to the reward as
auxiliary negative reward, called reward shaping (RS). Three common safe RL baselines Constraint
Policy Optimization (CPO) (Achiam et al., 2017), PPO-Lagrangian (Stooke et al., 2020), SAC-
Lagrangian (Ha et al., 2020) are evaluated."
BASELINE COMPARISON,0.2944606413994169,"Apart from the RL methods, we also generate a human demonstration dataset containing one-hour
expert’s demonstrations where there are about 36K transitions in the training environments. For
the high-quality demonstrations in the dataset, the success rate of the episodes reaches 98% and
the safety violation is down to 0.16. Using this dataset, we evaluate passive IL method Behavior
Cloning, active IL method GAIL (Ho & Ermon, 2016) and ofﬂine RL method CQL (Kumar et al.,
2020). We also run the Human-Gated DAgger (HG-DAgger) (Kelly et al., 2019) and Intervention
Weighted Regression (IWR) (Mandlekar et al., 2020) as the baselines of human-in-the-loop methods
based on this dataset and the human-AI copilot workﬂow."
BASELINE COMPARISON,0.29737609329446063,"Training-time Safety. The training-time safety is measured by the total training safety violation,
the total number of critical failures occurring in the training. Note that the environmental cost here
is different from the human intervention cost in HACO. As illustrated in Table 1 and Fig. 3A, HACO
achieves huge success in training time safety. Apart from the empirical results, we provide proof to
show the training safety can be bound by the guardian in Appendix. Under the protection of the hu-
man expert, HACO yields only 30.14 total safety violations in the whole training process, two orders
of magnitude better than other RL baselines, even though HACO does not access the environmental
cost. IWR and HG-DAgger also achieve drastically lower training safety violations, showing the
power of human-in-the-loop methods. The most competitive RL baseline SAC-RS, which achieves
similar test success rate, causes averagely 2767.77 training safety violations which are much higher"
BASELINE COMPARISON,0.30029154518950435,Published as a conference paper at ICLR 2022
BASELINE COMPARISON,0.3032069970845481,"A. Baseline Comparison
B. Learning Dynamics"
BASELINE COMPARISON,0.30612244897959184,"Figure 3: A. The performance of RL methods and HACO. HACO achieves superior training time
safety and test success rate. B. Dynamics of human cognitive cost. The takeover rate is the propor-
tion of the human takeover steps in an episode. In the training process, the takeover rate and the
episodic intervention cost gradually reduce and the agent obtains higher level of autonomy."
BASELINE COMPARISON,0.30903790087463556,"than HACO. The active IL method GAIL also has signiﬁcantly higher safety violations than HACO
and its performance is unsatisfactory."
BASELINE COMPARISON,0.3119533527696793,"From the perspective of safety, we ﬁnd that the reward shaping technique is inferior compared to
the Lagrangian method, both for SAC and PPO variants. PPO causes more violations than SAC,
probably due to the relatively lower sample efﬁciency and slower convergence speed."
BASELINE COMPARISON,0.31486880466472306,"Sample Efﬁciency and Human Cognitive Cost. The human-AI system is not only protected so
well by the human, but achieves superior sample efﬁciency with limited data usage. As shown in
Fig. 3A and Table 1, we ﬁnd that HACO is an order of magnitude more efﬁcient than RL baselines.
HACO achieves 0.83 test success rate by merely interacting with the environment in the 30K steps,
wherein only averagely 8,316 steps the human provides safe actions as demonstration. During nearly
50 minutes of human-AI copilot, there are only 27% steps that the human provides demonstrations."
BASELINE COMPARISON,0.3177842565597668,"Human-in-the-loop baselines IWR and HG-DAgger consume 50K steps of human budget and only
IWR can achieve satisfactory success rate. By prioritizing samples from human intervention, IWR
manages to learn key actions from human intervention to escape dangerous situations caused by the
compounding error. Without re-weighting the human takeover data, HG-Dagger fails to learn from
a few but important human demonstrations. The learning curves of these two methods can be found
in the Appendix."
BASELINE COMPARISON,0.3206997084548105,"Unlike the success of HACO, all the learning-from-demonstration methods fail with the dataset
containing 36K transitions. Compared to IL methods which optimize agents to imitate exact actions
at each time step, HACO considers the learning on the trajectory basis. We incentivize the agent
to choose an action that can bring potential return in future trajectory, instead of only mimicking
the expert’s behaviors at each step. On the other hand, HACO gathers expert data in an online
manner through human-AI copilot, which better mitigates the distributional shift severe in ofﬂine
RL methods."
BASELINE COMPARISON,0.3236151603498542,"Learning Dynamics. The intervention minimization mechanism in HACO reduces human cognitive
cost. As shown in Fig. 3B, the takeover rate gradually decreases in the course of learning. The curve
of episodic intervention cost suggests that the human intervention frequency becomes lower and the
similarity between agent’s action and human’s action increases. We also provide visualization of
the learned proxy value function in the Appendix, showing that the learning scheme of HACO can
effectively encode human preference into the proxy values."
ABLATION STUDY,0.32653061224489793,"4.3
ABLATION STUDY"
ABLATION STUDY,0.3294460641399417,"Takeover Policy Analysis. We request the human subjects to try two intervention strategies. The
ﬁrst is to take over in a low frequency and produce a long trajectory at each intervention. In this way
the intervention cost becomes sparse. The other strategy is to intervene more frequently and provide
fragmented demonstrations. In Table 2(a), the experiment shows that the proposed HACO works
better with dense human intervention signals. Agent trained with long trajectories achieves inferior
success rate and episodic reward than agents trained with dense intervention signals."
ABLATION STUDY,0.3323615160349854,Published as a conference paper at ICLR 2022
ABLATION STUDY,0.33527696793002915,"Table 2: The test performance when ablating components in HACO. All experiments here are con-
ducted by human subjects. Some experiments only repeat once due to limited human budget."
ABLATION STUDY,0.33819241982507287,"Experiment
Test
Return
Test
Cost
Test
Success Rate"
ABLATION STUDY,0.34110787172011664,"(a) Human intervenes less frequently
220.03
0.867
0.397
(b) W/o cosine similarity intervention cost
23.23
0.42
0.00
(c) W/o intervention minimization
84.77
1.07
0.00"
ABLATION STUDY,0.34402332361516036,"HACO
349.25 ± 11.45
0.79 ± 0.31
0.83 ± 0.04"
ABLATION STUDY,0.3469387755102041,"Table 3: Comparison between HACO and PPO. Agents are trained in CARLA town 1 and tested in
the unseen CARLA town 2."
ABLATION STUDY,0.3498542274052478,"Algorithm
Test Safety Violation
Test Return
Test Success Rate
Train Samples"
ABLATION STUDY,0.35276967930029157,"PPO
80.84
1591.00
0.35
500,000
HACO
11.84
1579.03
0.35
8,000"
ABLATION STUDY,0.3556851311953353,"Cosine Similarity Cost Function. As shown in Table 2(b), we replace the intervention cost function
in Eq. 6 to a constant value +1 if human intervention happens. We ﬁnd the agent learns to stay in
the spawn points and does not move at all in test time. As discussed in Sec. 3.2, it is possible that
the human intervenes in incorrect timing. This makes agent fail to identify how to drive correctly.
Using the negative cosine similarity to measure the divergence between agent and human’s actions
alleviates this phenomenon since the human intervention penalty is down-weighted when the agent
provides action that adheres human intention."
ABLATION STUDY,0.358600583090379,"Intervention Minimization. As shown in Table 2(c), when removing the intervention minimization
mechanism, the agent drives directly toward the boundary. This is because the agent learns to abuse
human expert to take over all the time, which increases proxy values but causes consistent out-of-
the-road failures in testing. This result shows the importance of intervention minimization."
ABLATION STUDY,0.36151603498542273,"CARLA Experiment. To test the generality of HACO, we run HACO in the CARLA simula-
tor (Dosovitskiy et al., 2017). We use the top-down semantic view provided by CARLA as the input
and a 3-layer CNN as the feature extractor for HACO and the PPO baseline. For PPO, the reward
follows the setting described in CARLA and is based on the velocity and the completion of the
road. We train HACO (with a human expert) and PPO in CARLA town 1 and report the test per-
formance in CARLA town 2. Table 3 shows that the proposed HACO can be successfully deployed
in the CARLA simulator with visual observation and achieve comparable results. Also, it can train
the driving agent with a new CNN feature-extractor in 10 minutes with only 8,000 samples in the
environment. The video is available at: https://decisionforce.github.io/HACO/."
CONCLUSION,0.36443148688046645,"5
CONCLUSION"
CONCLUSION,0.3673469387755102,"We develop an efﬁcient human-in-the-loop learning method, Human-AI Copilot Optimization
(HACO), which trains agents from the human interventions and partial demonstrations. The method
incorporates the human expert in the interaction between agent and environment to ensure safe and
efﬁcient exploration. The experiments on safe driving show that the proposed method achieves su-
perior training-time safety, outperforming RL and IL baselines. Besides, it shows a high sample
efﬁciency for rapid learning. The constrained optimization technique is used to prevent the agent
from excessively exploiting the human expert, which also decreases the takeover frequency and
saves valuable human budget."
CONCLUSION,0.37026239067055394,"One limitation of this work is that the trained agents behave conservatively compared to the agents
from RL baselines. Aiming to ensure the training time safety of the copilot system, human expert
typically slow the vehicle down to rescue it from risky situations. This makes the agent tend to drive
slowly and exhibit behaviors such as frequent yielding in the intersection. In future work, we will
explore the possibility of learning more sophisticated skills."
CONCLUSION,0.37317784256559766,"Acknowledgments This project was supported by the Centre for Perceptual and Interactive Intelli-
gence (CPII) Ltd under InnoHK supported by the Innovation and Technology Commission."
CONCLUSION,0.3760932944606414,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.37900874635568516,ETHICS STATEMENT
ETHICS STATEMENT,0.3819241982507289,"The proposed Human-AI Copilot Optimization algorithm aims at developing a new human-friendly
human-in-the-loop training framework.
We successfully increase the level of automation after
human-efﬁcient training. We believe this work has a great positive social impact which advances the
development of more intelligent AI systems that costs less human burdens."
ETHICS STATEMENT,0.3848396501457726,"We employ human subjects to participate in the experiments. Human subjects can stop the exper-
iment if any discomfort happens. No human subjects were harmed in the experiments since we
test in the driving simulator. The human subjects earn an hourly salary more than average in our
community. Each experiment lasts near one hour. Human participants will rest at least three hours
after one experiment. During training and data processing, no personal information is revealed in
the collected dataset or the trained agents."
REFERENCES,0.3877551020408163,REFERENCES
REFERENCES,0.39067055393586003,"Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-ﬁrst international conference on Machine learning, pp. 1, 2004."
REFERENCES,0.3935860058309038,"David Abel, John Salvatier, Andreas Stuhlm¨uller, and Owain Evans. Agent-agnostic human-in-the-
loop reinforcement learning. arXiv preprint arXiv:1701.04079, 2017."
REFERENCES,0.3965014577259475,"Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 22–31.
JMLR. org, 2017."
REFERENCES,0.39941690962099125,"Mohammed Alshiekh, Roderick Bloem, R¨udiger Ehlers, Bettina K¨onighofer, Scott Niekum, and
Ufuk Topcu. Safe reinforcement learning via shielding. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, volume 32, 2018."
REFERENCES,0.40233236151603496,"Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Con-
crete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016."
REFERENCES,0.40524781341107874,"Dilip Arumugam, Jun Ki Lee, Sophie Saskin, and Michael L Littman. Deep reinforcement learning
from policy-dependent human feedback. arXiv preprint arXiv:1902.04257, 2019."
REFERENCES,0.40816326530612246,"Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and Ani-
mesh Garg. Conservative safety critics for exploration. arXiv preprint arXiv:2010.14497, 2020."
REFERENCES,0.4110787172011662,"Michael Bloem and Nicholas Bambos. Inﬁnite time horizon maximum causal entropy inverse re-
inforcement learning. In 53rd IEEE conference on decision and control, pp. 4911–4916. IEEE,
2014."
REFERENCES,0.4139941690962099,"Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning
for self-driving cars. arXiv preprint arXiv:1604.07316, 2016."
REFERENCES,0.41690962099125367,"Rui Camacho and Donald Michie. Behavioral cloning a correction. AI Magazine, 16(2):92–92,
1995."
REFERENCES,0.4198250728862974,"Thomas Cederborg, Ishaan Grover, Charles L Isbell, and Andrea L Thomaz. Policy shaping with
human teachers. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015."
REFERENCES,0.4227405247813411,"Paul Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. arXiv preprint arXiv:1706.03741, 2017."
REFERENCES,0.42565597667638483,"Yuchen Cui and Scott Niekum. Active reward learning from critiques. In 2018 IEEE international
conference on robotics and automation (ICRA), pp. 6907–6914. IEEE, 2018."
REFERENCES,0.42857142857142855,"Alexey Dosovitskiy, Germ´an Ros, Felipe Codevilla, Antonio M. L´opez, and Vladlen Koltun.
CARLA: an open urban driving simulator.
CoRR, abs/1711.03938, 2017.
URL http://
arxiv.org/abs/1711.03938."
REFERENCES,0.4314868804664723,Published as a conference paper at ICLR 2022
REFERENCES,0.43440233236151604,"Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse rein-
forcement learning. arXiv preprint arXiv:1710.11248, 2017."
REFERENCES,0.43731778425655976,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. arXiv preprint arXiv:1812.02900, 2018."
REFERENCES,0.4402332361516035,"Javier Garcıa and Fernando Fern´andez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research, 16(1):1437–1480, 2015."
REFERENCES,0.44314868804664725,"Mike Goslin and Mark R Mine. The panda3d graphics engine. Computer, 37(10):112–114, 2004."
REFERENCES,0.446064139941691,"Lin Guan, Mudit Verma, Sihang Guo, Ruohan Zhang, and Subbarao Kambhampati.
Widening
the pipeline in human-guided reinforcement learning with explanation and context-aware data
augmentation. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.4489795918367347,"Sehoon Ha, Peng Xu, Zhenyu Tan, Sergey Levine, and Jie Tan. Learning to walk in the real world
with minimal human effort, 2020."
REFERENCES,0.4518950437317784,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, pp. 1861–1870, 2018."
REFERENCES,0.45481049562682213,"Jonathan Ho and Stefano Ermon.
Generative adversarial imitation learning.
arXiv preprint
arXiv:1606.03476, 2016."
REFERENCES,0.4577259475218659,"Ryan Hoque, Ashwin Balakrishna, Ellen Novoseller, Albert Wilcox, Daniel S. Brown, and Ken
Goldberg. Thriftydagger: Budget-aware novelty and risk gating for interactive imitation learning,
2021."
REFERENCES,0.4606413994169096,"Junning Huang, Sirui Xie, Jiankai Sun, Qiurui Ma, Chunxiao Liu, Dahua Lin, and Bolei Zhou.
Learning a decision module by imitating driver’s control behaviors. In Proceedings of the Con-
ference on Robot Learning (CoRL), 2020."
REFERENCES,0.46355685131195334,"Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward
learning from human preferences and demonstrations in atari. In Proceedings of the 32nd Inter-
national Conference on Neural Information Processing Systems, pp. 8022–8034, 2018."
REFERENCES,0.46647230320699706,"Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
In Proc. 19th International Conference on Machine Learning. Citeseer, 2002."
REFERENCES,0.46938775510204084,"Michael Kelly, Chelsea Sidrane, Katherine Driggs-Campbell, and Mykel J Kochenderfer.
Hg-
dagger: Interactive imitation learning with human experts. In 2019 International Conference
on Robotics and Automation (ICRA), pp. 8077–8083. IEEE, 2019."
REFERENCES,0.47230320699708456,"Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tomp-
son. Discriminator-actor-critic: Addressing sample inefﬁciency and reward bias in adversarial
imitation learning. arXiv preprint arXiv:1809.02925, 2018."
REFERENCES,0.4752186588921283,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020."
REFERENCES,0.478134110787172,"Quanyi Li, Zhenghao Peng, Zhenghai Xue, Qihang Zhang, and Bolei Zhou. Metadrive: Composing
diverse driving scenarios for generalizable reinforcement learning. 2021."
REFERENCES,0.48104956268221577,"Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gon-
zalez, Michael Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcement learning.
In International Conference on Machine Learning, pp. 3053–3062, 2018."
REFERENCES,0.4839650145772595,"Travis Mandel, Yun-En Liu, Emma Brunskill, and Zoran Popovi´c. Where to add actions in human-
in-the-loop reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, volume 31, 2017."
REFERENCES,0.4868804664723032,"Ajay Mandlekar, Danfei Xu, Roberto Mart´ın-Mart´ın, Yuke Zhu, Li Fei-Fei, and Silvio
Savarese.
Human-in-the-loop imitation learning using remote teleoperation.
arXiv preprint
arXiv:2012.06733, 2020."
REFERENCES,0.4897959183673469,Published as a conference paper at ICLR 2022
REFERENCES,0.49271137026239065,"Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml,
volume 1, pp. 2, 2000."
REFERENCES,0.4956268221574344,"Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, and Jan Peters.
An algorithmic perspective on imitation learning. arXiv preprint arXiv:1811.06711, 2018."
REFERENCES,0.49854227405247814,"Malayandi Palan, Nicholas C Landolﬁ, Gleb Shevchuk, and Dorsa Sadigh. Learning reward func-
tions by integrating human demonstrations and preferences. arXiv preprint arXiv:1906.08928,
2019."
REFERENCES,0.5014577259475219,"Zhenghao Peng, Quanyi Li, Chunxiao Liu, and Bolei Zhou. Safe driving via expert guided policy
optimization. In 5th Annual Conference on Robot Learning, 2021."
REFERENCES,0.5043731778425656,"Siddharth Reddy, Anca D Dragan, and Sergey Levine. Shared autonomy via deep reinforcement
learning. Robotics: Science and Systems, 2018."
REFERENCES,0.5072886297376094,"Siddharth Reddy, Anca D Dragan, and Sergey Levine. Sqil: Imitation learning via reinforcement
learning with sparse rewards. arXiv preprint arXiv:1905.11108, 2019."
REFERENCES,0.5102040816326531,"St´ephane Ross and Drew Bagnell. Efﬁcient reductions for imitation learning. In Proceedings of the
thirteenth international conference on artiﬁcial intelligence and statistics, pp. 661–668. JMLR
Workshop and Conference Proceedings, 2010."
REFERENCES,0.5131195335276968,"St´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Proceedings of the fourteenth international con-
ference on artiﬁcial intelligence and statistics, pp. 627–635. JMLR Workshop and Conference
Proceedings, 2011."
REFERENCES,0.5160349854227405,"Dorsa Sadigh, Anca D Dragan, Shankar Sastry, and Sanjit A Seshia. Active preference-based learn-
ing of reward functions. UC Berkeley, 2017."
REFERENCES,0.5189504373177842,"Fumihiro Sasaki, Tetsuya Yohira, and Atsuo Kawaguchi. Sample efﬁcient imitation learning for
continuous control. In International conference on learning representations, 2018."
REFERENCES,0.521865889212828,"William Saunders, Girish Sastry, Andreas Stuhlmueller, and Owain Evans. Trial without error:
Towards safe reinforcement learning via human intervention. arXiv preprint arXiv:1707.05173,
2017."
REFERENCES,0.5247813411078717,"William Saunders, Girish Sastry, Andreas Stuhlmueller, and Owain Evans. Trial without error:
Towards safe reinforcement learning via human intervention. In Proceedings of the 17th Interna-
tional Conference on Autonomous Agents and MultiAgent Systems, pp. 2067–2069. International
Foundation for Autonomous Agents and Multiagent Systems, 2018."
REFERENCES,0.5276967930029155,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.5306122448979592,"John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.
High-
dimensional continuous control using generalized advantage estimation, 2018."
REFERENCES,0.5335276967930029,"Jiaming Song, Hongyu Ren, Dorsa Sadigh, and Stefano Ermon. Multi-agent generative adversarial
imitation learning. arXiv preprint arXiv:1807.09936, 2018."
REFERENCES,0.5364431486880467,"Jonathan Spencer, Sanjiban Choudhury, Matthew Barnes, Matthew Schmittle, Mung Chiang, Peter
Ramadge, and Siddhartha Srinivasa. Learning from interventions. In Robotics: Science and
Systems (RSS), 2020."
REFERENCES,0.5393586005830904,"Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning
by pid lagrangian methods. In International Conference on Machine Learning, pp. 9133–9143.
PMLR, 2020."
REFERENCES,0.5422740524781341,"Jiankai Sun, Hao Sun, Tian Han, and Bolei Zhou. Neuro-symbolic program search for autonomous
driving decision module design. In Proceedings of the Conference on Robot Learning (CoRL),
2020."
REFERENCES,0.5451895043731778,Published as a conference paper at ICLR 2022
REFERENCES,0.5481049562682215,"Garrett Warnell, Nicholas Waytowich, Vernon Lawhern, and Peter Stone. Deep tamer: Interactive
agent shaping in high-dimensional state spaces. In Thirty-Second AAAI Conference on Artiﬁcial
Intelligence, 2018."
REFERENCES,0.5510204081632653,"Bernard Widrow. Pattern recognition and adaptive control. IEEE Transactions on Applications and
Industry, 83(74):269–277, 1964."
REFERENCES,0.5539358600583091,"Jingda Wu, Zhiyu Huang, Chao Huang, Zhongxu Hu, Peng Hang, Yang Xing, and Chen Lv. Human-
in-the-loop deep reinforcement learning with application to autonomous driving. arXiv preprint
arXiv:2104.07246, 2021."
REFERENCES,0.5568513119533528,"Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019."
REFERENCES,0.5597667638483965,"Kelvin Xu, Siddharth Verma, Chelsea Finn, and Sergey Levine. Continual learning of control prim-
itives: Skill discovery via reset-games. Advances in Neural Information Processing Systems, 33,
2020."
REFERENCES,0.5626822157434402,"Jiakai Zhang and Kyunghyun Cho. Query-efﬁcient imitation learning for end-to-end autonomous
driving. arXiv preprint arXiv:1605.06450, 2016."
REFERENCES,0.565597667638484,"Ruohan Zhang, Faraz Torabi, Garrett Warnell, and Peter Stone. Recent advances in leveraging
human guidance for sequential decision-making tasks. Autonomous Agents and Multi-Agent Sys-
tems, 35(2):1–39, 2021."
REFERENCES,0.5685131195335277,Published as a conference paper at ICLR 2022
REFERENCES,0.5714285714285714,APPENDIX
REFERENCES,0.5743440233236151,"The demonstrative video and the code of HACO and baselines are provided at: https://
decisionforce.github.io/HACO/"
REFERENCES,0.577259475218659,"A
MAIN THEOREM AND THE PROOF"
REFERENCES,0.5801749271137027,"In this section, we derive the upper bound of the discounted probability of failure of HACO, showing
that we can bound the training safety with the guardian.
Theorem 1 (Upper bound of training risk). The expected cumulative probability of failure Vπb of
the behavior policy πb in HACO is bounded by the error rate of the human expert action ϵ, the error
rate of the human expert intervention κ and the tolerance of the human expert K′:"
REFERENCES,0.5830903790087464,"Vπb ≤
1
1 −γ[ϵ + κ + γϵ2"
REFERENCES,0.5860058309037901,"1 −γK′],"
REFERENCES,0.5889212827988338,"wherein K′ = maxs K(s) = maxs
R"
REFERENCES,0.5918367346938775,a∈Ah(s) da ≥0 is called human expert tolerance.
REFERENCES,0.5947521865889213,"The human expert tolerance K′ will becomes larger, if human relieves its intervention and allows
the agent to explore the environment more freely."
REFERENCES,0.597667638483965,The proof is given as follows.
REFERENCES,0.6005830903790087,"Notations. Before starting, we ﬁrstly recap and describe the notations. In HACO, a human subject
copilots with the learning agent. The agent’s policy is πn, the human’s policy is πh. Both policies
produces action in the bounded action space A ∈R|A|. The human expert decides to intervention
under certain state and agent’s action an. The human intervention is denoted by a Boolean function:
I(s, a). The mixed behavior policy πb that produces the real actions applied to the environment is
denoted as:
πb(a|s) = πn(a|s)(1 −I(s, a)) + πh(a|s)G(s),
(9)
wherein G(s) =
R"
REFERENCES,0.6034985422740525,"a′∈A I(s, a′)πn(a′|s)da′ is a function which denotes the probability of choosing
an action that will be rejected by the human."
REFERENCES,0.6064139941690962,"Therefore, at a given state, we can split the action space into two parts: where intervention will
happen or will not happen if the agent sample action in it. We denote the conﬁdent action space as:"
REFERENCES,0.60932944606414,"Ah(s) = {a : I(a|s) is False}.
(10)"
REFERENCES,0.6122448979591837,The conﬁdent action space contains the actions that will not be rejected by human expert at state s.
REFERENCES,0.6151603498542274,"We also deﬁne the ground-truth indicator Cgt denoting whether the action will lead to unsafe state.
This unsafe state is determined by the environment and is not revealed to learning algorithm:"
REFERENCES,0.6180758017492711,"Cgt(s, a) =
1,
if s′ = P(s′|s, a) is an unsafe state,
0,
otherwise.
(11)"
REFERENCES,0.6209912536443148,"Therefore, at a given state s the step-wise probability of failure for arbitrary policy π is:"
REFERENCES,0.6239067055393586,"E
a∼π(·|s) Cgt(s, a) ∈[0, 1].
(12)"
REFERENCES,0.6268221574344023,Now we denote the cumulative discounted probability of failure as:
REFERENCES,0.6297376093294461,"Vπ(st) = E
τ∼π X"
REFERENCES,0.6326530612244898,"t′=t
γt′−tCgt(st′, at′),
(13)"
REFERENCES,0.6355685131195336,"which counts for the chance of entering dangerous states in current time step as well as in future
trajectories deduced by the policy π. We use Vπh = Eτ∼πh Vπh(s0) to denote the expected cu-
mulative discounted probability of failure of the human. Following the same deﬁnition as Vπh, we
can also write the expected cumulative discounted probability of failure of the behavior policy as:
Vπb = Eτ∼πb Vπb(s0) = Eπb
P"
REFERENCES,0.6384839650145773,"t=0 γtCgt(st, at)."
REFERENCES,0.641399416909621,Assumption. Now we introduce two important assumptions on the human expert.
REFERENCES,0.6443148688046647,Published as a conference paper at ICLR 2022
REFERENCES,0.6472303206997084,"Assumption 1 (Error rate of human action). For all states, the step-wise probability of that the
human expert produces an unsafe action is bounded by a small value ϵ < 1:"
REFERENCES,0.6501457725947521,"E
a∼πh(·|s) Cgt(s, a) ≤ϵ.
(14)"
REFERENCES,0.6530612244897959,"Assumption 2 (Error rate of human intervention). For all states, the step-wise probability of that
the human expert does not intervene when agent produces an unsafe action is bounded by a small
value κ < 1:
Z a∈A"
REFERENCES,0.6559766763848397,"[1 −I(s, a)]Cgt(s, a)da =
Z"
REFERENCES,0.6588921282798834,a∈Ah(s)
REFERENCES,0.6618075801749271,"Cgt(s, a)da ≤κ.
(15)"
REFERENCES,0.6647230320699709,These two assumptions does not impose any constrain on the structure of the human expert policy.
REFERENCES,0.6676384839650146,"Lemmas. We propose several useful lemmas and the correspondent proofs, which are used in the
main theorem."
REFERENCES,0.6705539358600583,Lemma 2 (The performance difference lemma).
REFERENCES,0.673469387755102,"Vπb = Vπh +
1
1 −γ
E
s∼Pπb
E
a∼πb
[Aπh(s, a)].
(16)"
REFERENCES,0.6763848396501457,"Here the Pπb means the states are subject to the marginal state distribution deduced by the behav-
ior policy πb. Aπh(s, a) is the advantage of the expert in current state action pair: Aπh(s, a) =
Cgt(s, a) + γVπh(s′) −Vπh(s) and s′ = P(s, a) is the next state. This lemma is proposed and
proved by Kakade & Langford (2002) and is useful to show the behavior policy’s safety. In the
original proposition, the V and A represents the expected discounted return and advantage w.r.t.
the reward, respectively. However, we replace the reward with the indicator Cgt so that the value
function Vπb and Vπh presenting the expected cumulative failure probability."
REFERENCES,0.6793002915451894,Lemma 3. The cumulative probability of failure of the expert Vπh(s) is bounded for all state:
REFERENCES,0.6822157434402333,"Vπh(s) ≤
ϵ
1 −γ"
REFERENCES,0.685131195335277,Proof. Following Assumption 1:
REFERENCES,0.6880466472303207,"Vπh(st) = E
πh
[ ∞
X"
REFERENCES,0.6909620991253644,"t′=t
γt′−tCgt(st′, at′)] = ∞
X"
REFERENCES,0.6938775510204082,"t′=t
γt′−t E
πh
[Cgt(st′, at′)] ≤ ∞
X"
REFERENCES,0.6967930029154519,"t′=t
γt′−tϵ =
ϵ
1 −γ
(17)"
REFERENCES,0.6997084548104956,"Theorem. We introduce the main theorem of this work above, which shows that the training safety
is related to the error rate on action ϵ and the error rate on intervention κ of the human expert. The
proof is given as follows."
REFERENCES,0.7026239067055393,Proof. We ﬁrstly decompose the advantage by splitting the behavior policy:
REFERENCES,0.7055393586005831,"E
a∼πb(·|s) Aπh(s, a) =
Z"
REFERENCES,0.7084548104956269,"a∈A
πb(a|s)Aπh(s, a) =
Z"
REFERENCES,0.7113702623906706,"a∈A
{πn(a|s)(1 −I(s, a))Aπh(s, a) + πh(a|s)G(s)Aπh(s, a)}da =
Z"
REFERENCES,0.7142857142857143,"a∈Ah(s)
[πn(a|s)Aπh(s, a)]da + G(s)
E
a∼πh
[Aπh(s, a)]. (18)"
REFERENCES,0.717201166180758,Published as a conference paper at ICLR 2022
REFERENCES,0.7201166180758017,"The second term is equal to zero according to the deﬁnition of advantage. We only need to compute
the ﬁrst term. We expand the advantage into detailed form, we have:"
REFERENCES,0.7230320699708455,"E
a∼πb(·|s) Aπh(s, a) =
Z"
REFERENCES,0.7259475218658892,a∈Ah(s)
REFERENCES,0.7288629737609329,"[πn(a|s)Aπh(s, a)]da =
Z"
REFERENCES,0.7317784256559767,a∈Ah(s)
REFERENCES,0.7346938775510204,"πn(a|s)[Cgt(s, a) + γVπh(s′) −Vπh(s)]da =
Z"
REFERENCES,0.7376093294460642,a∈Ah(s)
REFERENCES,0.7405247813411079,"π(a|s)Cgt(s, a)da"
REFERENCES,0.7434402332361516,"|
{z
}
(a) + γ
Z"
REFERENCES,0.7463556851311953,a∈Ah(s)
REFERENCES,0.749271137026239,π(a|s)Vπh(s′)da
REFERENCES,0.7521865889212828,"|
{z
}
(b) −
Z"
REFERENCES,0.7551020408163265,a∈Ah(s)
REFERENCES,0.7580174927113703,π(a|s)Vπh(s)da
REFERENCES,0.760932944606414,"|
{z
}
(c) . (19)"
REFERENCES,0.7638483965014577,"Following the Assumption 1, the term (a) can be bounded as: Z"
REFERENCES,0.7667638483965015,a∈Ah(s)
REFERENCES,0.7696793002915452,"π(a|s)Cgt(s, a)da ≤
Z"
REFERENCES,0.7725947521865889,a∈Ah(s)
REFERENCES,0.7755102040816326,"Cgt(s, a)da ≤κ.
(20)"
REFERENCES,0.7784256559766763,"Following the Lemma 3, the term (b) can be written as: γ
Z"
REFERENCES,0.7813411078717201,a∈Ah(s)
REFERENCES,0.7842565597667639,"π(a|s)Vπh(s′)da ≤γ
Z"
REFERENCES,0.7871720116618076,a∈Ah(s)
REFERENCES,0.7900874635568513,"Vπh(s′)da ≤
γϵ
1 −γ Z"
REFERENCES,0.793002915451895,a∈Ah(s)
REFERENCES,0.7959183673469388,"da =
γϵ
1 −γK(s),
(21)"
REFERENCES,0.7988338192419825,"wherein K(s) =
R"
REFERENCES,0.8017492711370262,"a∈Ah(s) da denoting the area of human-preferable region in the action space. It
is a function related to the human expert and state."
REFERENCES,0.8046647230320699,"The term (c) is always non-negative, so after applying the minus to term (c) the negative term will
always be ≤0."
REFERENCES,0.8075801749271136,"Aggregating the upper bounds of three terms, we have the bound on the advantage:"
REFERENCES,0.8104956268221575,"E
a∼πb
Aπh(s, a) ≤κ +
γϵ
1 −γK(s)
(22)"
REFERENCES,0.8134110787172012,"Now we put Eq. 22 as well as Lemma 3 into the performance difference lemma (Lemma 2), we
have:"
REFERENCES,0.8163265306122449,"Vπb = Vπh +
1
1 −γ
E
s∼Pπb
E
a∼πb
[Aπh(s, a)]"
REFERENCES,0.8192419825072886,"≤
ϵ
1 −γ +
1
1 −γ[κ +
γϵ
1 −γ max
s
K(s)]]"
REFERENCES,0.8221574344023324,"=
1
1 −γ[ϵ + κ + γϵ2"
REFERENCES,0.8250728862973761,"1 −γK′], (23)"
REFERENCES,0.8279883381924198,"wherein K′ = maxs K(s) = maxs
R"
REFERENCES,0.8309037900874635,"a∈Ah(s) da ≥0 is correlated to the tolerance of the expert. If
the human expert has higher tolerance then K′ should be greater."
REFERENCES,0.8338192419825073,"Now we have proved the upper bound of the discounted probability of failure for the behavior policy
in our method."
REFERENCES,0.8367346938775511,Published as a conference paper at ICLR 2022
REFERENCES,0.8396501457725948,"B
VISUALIZATION OF LEARNED PROXY VALUE FUNCTION"
REFERENCES,0.8425655976676385,"(a)
(b)
(c)
(d)"
REFERENCES,0.8454810495626822,Figure 4: Visualization of proxy Q value learned by HACO.
REFERENCES,0.8483965014577259,"To understand how well the proxy value function learns, we visualize 4 common scenarios in 4 pairs
of ﬁgures as shown above. The left sub-ﬁgure of each pair shows a top-down view of a driving
scenario, where a sequence of snapshots of the control vehicle is plotted, showing its trajectory.
The right sub-ﬁgure of each pair overlaps the heatmap of proxy values in the top-down image. We
manually position the vehicle in different location in the map and query the policy to get action and
run the proxy Q function to get the value Q(s, a). Region in red color indicates the proxy value is
low if the agent locates there and vice versa."
REFERENCES,0.8513119533527697,"In Fig. 4(a), the agent performs a lane change behavior to avoid potential collisions with a trafﬁc
vehicle which is merging into the middle lane. The region near the trafﬁc vehicle has extremely low
values and thus the agent has small probability to enter this area."
REFERENCES,0.8542274052478134,"In Fig. 4(b), trafﬁc cones spread in the left lane. The agent learns to avoid crashes and the proxy
value heatmap shows a large region of low values."
REFERENCES,0.8571428571428571,"As shown in the trajectory in Fig. 4(c), though the agent can choose to bypass the trafﬁc vehicle in
both left-hand side or right-hand side, it chooses the right-hand side. The heatmap shows that much
higher proxy Q value is produced on right bypassing path compared to left path. This behavior
resembles the preference of human who prefers right-hand side detour."
REFERENCES,0.8600583090379009,"In addition, in some ares where paths boundary is ambiguous such as the intersection, the agent
manages to learn a virtual boundary in the proxy Q space for efﬁciently passing these areas, as
shown in the Fig. 4(d)."
REFERENCES,0.8629737609329446,"The proxy Q value distribution shown in this section not only explains the avoidance behaviors, but
also serves as a good indicator for the learned human preference."
REFERENCES,0.8658892128279884,Published as a conference paper at ICLR 2022
REFERENCES,0.8688046647230321,"C
DETAILS OF HUMAN-IN-THE-LOOP BASELINES"
REFERENCES,0.8717201166180758,"3.00
3.25
3.50
3.75
4.00
4.25
4.50
4.75
5.00
Data Usage
1e4 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
REFERENCES,0.8746355685131195,Test Success Rate
REFERENCES,0.8775510204081632,"IWR
HG-DAgger"
REFERENCES,0.880466472303207,Figure 5: Detailed learning curves of human-in-the-loop baselines.
REFERENCES,0.8833819241982507,"We benchmark the performance of two human-in-the-loop methods HG-DAgger (Kelly et al., 2019)
and IWR (Mandlekar et al., 2020). Both methods require warming up through behavior cloning on
a pre-collected dataset. In practice, we ﬁnd that using 10K or 20K steps of human collected data
is not enough to initialize the policy with basic driving skills. Therefore, we use the pre-collected
human dataset containing 30K transitions to warm up the policies. After warming up, HG-DAgger
and IWR then aggregate human intervention data to the training buffer and conduct behavior cloning
again to update policy for 4 epochs. In each epoch the human-AI system collects 5000 transitions.
The above ﬁgure shows the learning curves of IWR and HG-DAgger. As discussed in the main body
of paper, we credit the success of IWR to the re-weighting of human intervention data, which is not
emphasized in HG-DAgger."
REFERENCES,0.8862973760932945,"D
MORE ZOOM-IN PLOT OF THE LEARNING CURVES"
REFERENCES,0.8892128279883382,"0.0
0.2
0.4
0.6
0.8
1.0
Sampled Steps
1e5 0 1 2 3 4"
REFERENCES,0.892128279883382,Training-time Safety Violation
REFERENCES,0.8950437317784257,"PPO-Lag
PPO-RS
SAC-Lag
SAC-RS
HACO"
REFERENCES,0.8979591836734694,"0.0
0.2
0.4
0.6
0.8
1.0
Sampled Steps
1e5 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9008746355685131,Test Success Rate
REFERENCES,0.9037900874635568,"PPO-Lag
PPO-RS
SAC-Lag
SAC-RS
HACO"
REFERENCES,0.9067055393586005,Figure 6: The zoomed in learning curves of RL baselines and HACO.
REFERENCES,0.9096209912536443,"The above ﬁgures present the zoomed in learning curves of RL baselines and HACO, showing the
superior sample efﬁciency of HACO compared to RL baselines."
REFERENCES,0.9125364431486881,Published as a conference paper at ICLR 2022
REFERENCES,0.9154518950437318,"E
HYPER-PARAMETERS"
REFERENCES,0.9183673469387755,Table 4: HACO
REFERENCES,0.9212827988338192,"Hyper-parameter
Value"
REFERENCES,0.924198250728863,"Discounted Factor γ
0.99
τ for Target Network Update
0.005
Learning Rate
0.0001
Environmental Horizon T
1000
Steps before Learning Start
100
Steps per Iteration
100
Train Batch Size
1024
CQL Loss Temperature
10.0
Target Entropy
2.0"
REFERENCES,0.9271137026239067,Table 5: PPO/PPO-Lag
REFERENCES,0.9300291545189504,"Hyper-parameter
Value"
REFERENCES,0.9329446064139941,"KL Coefﬁcient
0.2
λ for GAE (Schulman et al., 2018)
0.95
Discounted Factor γ
0.99
Number of SGD epochs
20
Train Batch Size
4000
SGD mini batch size
100
Learning Rate
0.00005
Clip Parameter ϵ
0.2"
REFERENCES,0.9358600583090378,"Cost Limit for PPO-Lag
1"
REFERENCES,0.9387755102040817,Table 6: SAC/SAC-Lag/CQL
REFERENCES,0.9416909620991254,"Hyper-parameter
Value"
REFERENCES,0.9446064139941691,"Discounted Factor γ
0.99
τ for target network update
0.005
Learning Rate
0.0001
Environmental horizon T
1500
Steps before Learning start
10000"
REFERENCES,0.9475218658892128,"Cost Limit for SAC-Lag
1"
REFERENCES,0.9504373177842566,"BC iterations for CQL
200000
CQL Loss Temperature β
5
Min Q Weight Multiplier
0.2"
REFERENCES,0.9533527696793003,Table 7: BC
REFERENCES,0.956268221574344,"Hyper-parameter
Value"
REFERENCES,0.9591836734693877,"Dataset Size
36,000
SGD Batch Size
32
SGD Epoch
200000
Learning Rate
0.0001"
REFERENCES,0.9620991253644315,Table 8: CPO
REFERENCES,0.9650145772594753,"Hyper-parameter
Value"
REFERENCES,0.967930029154519,"KL Coefﬁcient
0.2
λ for GAE (Schulman et al., 2018)
0.95
Discounted Factor γ
0.99
Number of SGD epochs
20
Train Batch Size
8000
SGD mini batch size
100
Learning Rate
0.00005
Clip Parameter ϵ
0.2"
REFERENCES,0.9708454810495627,"Cost Limit
1"
REFERENCES,0.9737609329446064,Table 9: GAIL
REFERENCES,0.9766763848396501,"Hyper-parameter
Value"
REFERENCES,0.9795918367346939,"Dataset Size
36,000
SGD Batch Size
64
Sample Batch Size
12800
Generator Learning Rate
0.0001
Discriminator Learning Rate
0.005
Generator Optimization Epoch
5
Discriminator Optimization Epoch
2000
Clip Parameter ϵ
0.2"
REFERENCES,0.9825072886297376,Table 10: HG-DAgger
REFERENCES,0.9854227405247813,"Hyper-parameter
Value"
REFERENCES,0.9883381924198251,"Initializing dataset size
30K
Number of data aggregation epoch
4
Interactions per round
5000
SGD batch size
256
Learning rate
0.0004"
REFERENCES,0.9912536443148688,Table 11: IWR
REFERENCES,0.9941690962099126,"Hyper-parameter
Value"
REFERENCES,0.9970845481049563,"Initializing dataset size
30K
Number of data aggregation epoch
4
Interactions per round
5000
SGD batch size
256
Learning rate
0.0004
Re-weight data distribution
True"
