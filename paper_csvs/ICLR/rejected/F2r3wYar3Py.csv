Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.006289308176100629,"Humans can generalize from only a few examples and from little pre-training on
similar tasks. Yet, machine learning (ML) typically requires large data to learn or
pre-learn to transfer. Inspired by nativism, we directly model basic human-innate
priors in abstract visual tasks e.g., character/doodle recognition. This yields a white-
box model that learns transformation-based topological similarity by mimicking
how humans naturally “distort” an object when ﬁrst seeing it. Using the simple
nearest-neighbor classiﬁer in this similarity space, our model achieves human-level
character recognition using only 1–10 examples per class and nothing else (no pre-
training). This differs from few-shot learning (FSL) using signiﬁcant pre-training.
On standard benchmarks MNIST, EMNIST-letters, and the Omniglot challenge, our
model outperforms both neural-network-based and classical ML in the “tiny-data”
regime, including FSL pre-trained on large data. Further, mimicking k-means but
in a non-Euclidean space, our model enables unsupervised learning and generates
human-interpretable archetypes as cluster “centroids”."
INTRODUCTION,0.012578616352201259,"1
INTRODUCTION"
INTRODUCTION,0.018867924528301886,"Modern machine learning (ML) has made remarkable progress, but this is accompanied by increasing
model complexity, with hundreds of neural layers (e.g., ResNet-152) and millions of parameters
(e.g., AlexNet: 62.3M, VGG16: 138M, BERT: 110M, GTP-3: 175B). This results in a huge appetite
for data and increasing difﬁculty in model interpretability—both for users to understand and for
developers to tune (e.g., hyperparameters, architecture). As such, AI researchers have pushed for ML
models that are prior- and data-efﬁcient (Chollet, 2019), that are human-like (Lake et al., 2015), and
that exhibit human-interpretable behaviors (Adadi & Berrada, 2018)."
INTRODUCTION,0.025157232704402517,"This poses the fundamental scientiﬁc question: How can humans learn so much from so little (May,
2015), whereas ML models, e.g., those achieving near-perfection on MNIST using all 60k training
images, deteriorate rapidly as (pre)training reduces? Being data-hungry can pose a real challenge in
data-scarce domains, e.g., in a rapidly evolving pandemic or a low-resource environment. Focusing
on its scientiﬁc contribution, this paper presents a theoretically sound, white-box model that learns
like humans do, with initial success on a ﬁrst set of benchmarks. This includes our state-of-the-art
results of hitting 80% / 90% MNIST accuracy using the only ﬁrst / ﬁrst four training images per
class and achieving 6.75% error (human performance is 4.5%) in the Omniglot one-shot learning
challenge without pre-training—one and only one shot."
INTRODUCTION,0.031446540880503145,"We follow the nativist principle: given an example, humans make abstractions: we envision an
equivalence class of unseen examples, equivalent to the given, in several abstract senses. Many
abstraction abilities are inborn in humans and occur unconsciously. When a baby sees a mug, (s)he
can immediately recognize it regardless of whether it is translated, rotated, scaled, or deformed. Such
abstraction abilities are considered innate as Core Knowledge priors (Spelke & Kinzler, 2007), rather
than acquired later in life such as learning that a mug is topologically a doughnut."
INTRODUCTION,0.03773584905660377,"We computationally realize the above intuition via our so-called distortable canvas—imagining every
image smoothly painted on a rubber canvas that can be distorted in many ways, e.g., bent, stretched,
squeezed (Figure 1A). Due to rubber’s viscosity, more distorted canvas transformations expend more
energy. This induces a topological similarity, or distance, based on minimal energy: two images are
similar if one can almost transform into the other with little distortion (energy). Distance is computed
by minimizing color and canvas distortions. In general, visual similarity involves not only a canvas
but also a color transformation. Here, we focus on canvas transformations and grayscale images only."
INTRODUCTION,0.0440251572327044,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.050314465408805034,"bent
stretched
squeezed
conformal
identity A."
INTRODUCTION,0.05660377358490566,transformation ﬂows
INTRODUCTION,0.06289308176100629,"( visualized on the 
  standard canvas ) B."
INTRODUCTION,0.06918238993710692,"smaller canvas distortion
larger canvas distortion (twisted, torn more)"
INTRODUCTION,0.07547169811320754,"smaller color distortion
larger color distortion … … ⇡
⇡"
INTRODUCTION,0.08176100628930817,C. canvas transformations and distortions
INTRODUCTION,0.0880503144654088,"Figure 1: Canvas transformations (A), transformation ﬂows (B), and distortions (C)."
INTRODUCTION,0.09433962264150944,"Besides a ﬁnal desired transformation (and distance), we apply the minimal-energy principle to
the entire transformation process, i.e., to keep canvas and color distortions small along the entire
optimization process (Mesa et al., 2019). When perceiving a translation, our mind does not process
it as a sudden displacement from one location to another, but auto-completes a translation path—
continuous and preferably short. Gradient descent naturally ﬁts this goal by always choosing the
steepest descent. Yet, it suffers from the curse of local minima. Our solution is to lift gradient descent
to multiple levels of abstraction via multiscale canvas lattices and color blurring, mimicking human
abstraction ability that is extremely ﬂexible in multiscale optimization. This yields visualizable and
interpretable transformation ﬂows (Figure 1B) that either match human intuition (e.g., what humans
would naturally do to transform a “7” to a “1”) or provide human intuition to initially nonintuitive
settings. The latter case suggests new, human-interpretable transformations: “ah, I did not realize this
other way of transforming ‘7’ into ‘1’, but now I see it and it makes perfect sense!”"
INTRODUCTION,0.10062893081761007,"We show initial success on abstract visual tasks such as character and doodle recognition (future work
generalizes to photorealistic images after a preprocessing technique to turn them into abstract doodles
or “emojis”). Our model can be used with the simple nearest-neighbor method to classify images and
also in a simple k-means style to cluster images. On benchmarks including MNIST (LeCun et al.,
1998), EMNIST-letters (Cohen et al., 2017), and the more taxing Omniglot challenge (Lake et al.,
2015), running nearest-neighbor on our learned similarity space outperforms both neural-network-
based and classical ML in the “tiny-data” or single-datum regime. This includes beating few-shot
models pre-trained on extra background data (which our model does not require). On MNIST, we
need one and only one training image per class to hit 80% accuracy and four to hit 90%. In an
unsupervised setting, our model enables k-means-like clustering, but on our learned similarity space.
We generate archetypes as cluster “centroids”, e.g., different ways of writing “7” or doodling giraffes."
INTRODUCTION,0.1069182389937107,"Literature on data scarcity: few-shot learning (FSL). One-/few-shot learning (Lake et al., 2011;
Wang et al., 2020) via transfer or meta learning (Pan & Yang, 2009; Finn et al., 2017; Hsu et al., 2019)
has achieved impressive success in data-scarce scenarios. FSL’s success relies on the assumption that
source and target tasks are similar enough for pre-training to be relevant (Storkey, 2009). However,
knowing a priori how relevant the tasks are and understanding what has been pre-learned are often
considered “black art”. It remains challenging for FSL practitioners to pick proper source data/models
for pre-training so as to transfer most effectively and avoid negative transfer (Pan & Yang, 2009;
Meiseles & Rokach, 2020). This is especially the case in new domains (e.g., discovering rising trends)
over classical ones (e.g., vision, language). So, rather than “big transfer” (Kolesnikov et al., 2020),
the Omniglot challenge urges “small transfer”—advocating reduced pre-training (Lake et al., 2019).
This paper follows Omniglot’s pursuit and pushes such reduction to the limit: with absolutely zero
pre-training, we still achieved near-human performance and human-interpretability."
INTRODUCTION,0.11320754716981132,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.11949685534591195,"Literature on data scarcity: transformation and data augmentation. Our distortable canvas is
conceptually akin to other transformation-based models, e.g., optimal-transport maps (Villani, 2009),
transformation-induced descriptors (Lowe, 1999) and equivariances (Bronstein et al., 2017). Like
all these models, we build transformations into the model rather than into the data as through data
augmentation (Krizhevsky et al., 2012). This has the native advantage of harnessing transformational
properties directly rather than learning them from data. Models with built-in transformations may be
viewed as those perfectly learned from inﬁnitely augmented data. Unlike transformations commonly
considered in existing models and data augmentation techniques, we do not encode domain knowledge
about preferred transformations such as translation, rotation, or scaling. Instead, our model considers
all transformations while still maintaining efﬁciency via our abstracted gradient descent."
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.12578616352201258,"2
SMOOTH IMAGE ON DISTORTABLE CANVAS"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.1320754716981132,"We introduce a distortable canvas model: any image is thought of as smoothly painted on a rubber
canvas that can be bent, stretched, etc. We further introduce canvas transformations that can ﬂexibly
“distort” an image as we naturally simulate in our mind. More speciﬁcally, we deﬁne a smooth image"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.13836477987421383,"by a piecewise differentiable M : R2 →R+, where R2 denotes an inﬁnite canvas and R+ denotes
color (grayscale in this paper). We deﬁne a canvas transformation by α : R2 →R2, which “reshapes”
the underlying canvas of a smooth image. Examples include translation, rotation, scaling, and more.
We also deﬁne a color transformation by χ : R+ →R+, which “repaints” a color. In this paper,
we simplify color transformation and only use it to adjust image contrast via afﬁne χ(c) := ac + b.
Nevertheless, we do not restrict canvas transformation, but consider all 2D transformations. Given
M, α, χ, the composition χ ◦M ◦α denotes the transformed image of M by transformations α, χ."
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.14465408805031446,"To mimic innate human intuition about topological similarity, we introduce canvas distortion DV (α)
for any canvas transformation α and color distortion DC(M, M′) between two smooth images
M, M′. Our idea is to search for a transformation that mimics what humans naturally do to transform
one image into another. That is, a low-distorted α which makes little difference in color between M
and the transformed M′, or more precisely, an α that minimizes both DV (α) and DC(M, χ◦M′◦α)."
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.1509433962264151,"Representating digital and smoothed images. An m×n digital image is a discrete M : [m]×[n] →
[0, 1], where [k] := {0, 1, . . . , k −1}. We call [m] × [n] the canvas grid and any z ∈[m] × [n] a
grid point. For any m × n digital image M, we smooth it to M via a sum of kernels:"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.15723270440251572,"M(x) :=
X"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.16352201257861634,"z∈[m]×[n]
M(z) · κ(ρ(z, x))
for any x ∈R2,
(1)"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.16981132075471697,"where a kernel κ : R+ →R+ is a decaying function (e.g., linear, polynomial, Gaussian decay) and
ρ is a metric on R2 (e.g., ℓ1, ℓ2, ℓ∞). In this paper, we use linear decay and ℓ∞, i.e., κ(ρ(z, x)) =
1 −1"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.1761006289308176,"ρc ∥z −x∥∞if ∥z −x∥∞< ρc (for some cutoff radius ρc > 0) and κ(ρ(z, x)) = 0 otherwise.
Note: M is deﬁned everywhere on R2. This differs from Gaussian blurring as we do not discretize
kernels. It is key to use the smoothed image as input, which allows computing gradients analytically.
As such, we always smooth any digital image at ﬁrst and then only manipulate the smoothed image."
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.18238993710691823,"Representing arbitrary canvas transformations. We consider all 2D transformations (including
those without a formula), but how do we represent them in a computer? With respect to the standard
grid [m] × [n], we use the transformed grid α([m] × [n]) to represent α digitally. Thus, any canvas
transformation α is digitally represented by (
d=) a matrix α ∈R(mn)×2 whose ith row is the 2D
coordinate of the transformed ith grid point. We use the lexicographical order of a 2D grid, e.g., with
respect to [2]×[3], the identify transformation id
d= id = [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2]]. Any"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.18867924528301888,"transformed image M◦α
d= M(α) := ( M(α0), . . . , M(α(mn−1)) ) ∈R(mn), i.e., a (vectorized)
digital image sampled from M at the transformed grid α."
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.1949685534591195,"Representing color and canvas distortions. The color distortion DC measures the color discrepancy
between M(id) and M′(α) up to an afﬁne color transformation χ. The canvas distortion DV
measures the distortion between the original grid id and the transformed grid α. Formally,"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.20125786163522014,"DC(M, χ ◦M′ ◦α)
d= DC( M(id), χ(M′(α)) ) := ∥aM′(α) + b −M(id)∥2
2
(2)"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.20754716981132076,"DV (α)
d= DV (id, α) :=
max
{{i,j},{i′,j′}}∈BE"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.2138364779874214,"∆α
{i,j} −∆α
{i′,j′}
 , ∆α
{i,j} := log ∥αi −αj∥2"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.22012578616352202,"∥idi −idj∥2
(3)"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.22641509433962265,Under review as a conference paper at ICLR 2022
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.23270440251572327,"canvas grid
canvas lattice"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.2389937106918239,"canvas distortion 
(at a pair of neighboring edges)"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.24528301886792453,"(0,0) (0,1)
(0,2) (1,0) ↵"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.25157232704402516,"Figure 2: Canvas grid and its corresponding lattice. Local distortions are computed at every pair of
neighboring edges. One example of neighboring edges is highlighted in red."
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.2578616352201258,"Here, BE comprises all pairs of neighboring edges in a canvas lattice (introduced below). Eq. (3) is
derived from the mathematical deﬁnition of distortion of a function by discretizing it across the canvas
lattice. This formula measures how far an arbitrary transformation is from being conformal, which is
ﬂexible for local isometries and scaling. Given a canvas grid [m]×[n], its corresponding canvas lattice
is an undirected graph L = (V, E), with the set of vertices V = [m]×[n] and the set of edges obtained
by connecting neighboring vertices in the ℓ∞sense: E = {{i, j} | ∥vi −vj∥∞= 1 for vi, vj ∈V }.
We say two edges are neighbors if they form a 45◦angle (Figure 2)."
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.2641509433962264,"Computing topological distance by minimizing distortions. To minimize color and canvas distor-
tions (2) and (3), we consider two dual views: minimizing DC among low-distorted α’s or minimizing
DV among best-matching α’s. We write the two views as the following two constrained optimization
problems, together with their respective unconstrained equivalents: with ϵ →0+ and µ →0+,
min.
α,χ DC(M, χ ◦M′ ◦α)
s.t. DV (α) ≤ϵ ⇐⇒min.
α,χ DV (α) + µDC(M, χ ◦M′ ◦α) (4)"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.27044025157232704,"min.
α,χ DV (α)
s.t. DC(M, χ ◦M′ ◦α) ≤ϵ ⇐⇒min.
α,χ DC(M, χ ◦M′ ◦α) + µDV (α) (5)"
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.27672955974842767,"We let the optima D⋆
C for (4) and D⋆
V for (5) denote two versions of our desired topological distance
that mimics innate human intuition. We call them DC-distance and DV -distance, respectively."
SMOOTH IMAGE ON DISTORTABLE CANVAS,0.2830188679245283,"Transformation ﬂow. To obtain both transformations and transformation processes that are human-
like, we run (projected) gradient descent. The iterative gradient steps yield not only a transformation
α⋆in the end but also a transformation ﬂow id = α(0) →α(1) →· · · →α⋆. The resulting sequence
of transformed images M′ = M′ ◦α(0) →M′ ◦α(1) →· · · →M′ ◦α⋆≈M (we omit χ for
simplicity) makes up an animation (Figure 1), which helps with human intuition on transforming M′
to M. However, directly running (projected) gradient descent on (4) or (5) does not work, because it
suffers from the curse of local minima, which we discuss and solve in the next section."
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.2893081761006289,"3
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION"
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.29559748427672955,"The canvas distortion DV is invariant under a variety of transformations (e.g., DV (α) = 0 for any
conformal α), which nicely mimics humans’ ﬂexible transformation options. But this also implies
lots of local/global minima and other critical points where the gradient is zero. How much the color
distortion DC ﬂuctuates as a function of α depends on the images M, M′. But in most cases, DC also
has lots of local/global minima, the majority of which represent unwanted “short cuts”—unnatural
transformations that make DC →0 but would break the rubber canvas or create holes in it. The
curse of vanishing gradients can freeze gradient descent. To unfreeze it, we lift gradient descent to
higher levels, mimicking once again humans’ abstraction power, as our internal optimization system
is quite ﬂexible in pursuing “gradient-descent” moves at multiple levels of abstraction. We design
two abstraction techniques: a chain of anchor lattices to make hierarchical abstractions of canvas
transformations and a chain of color blurring to make hierarchical abstractions of image painting."
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.3018867924528302,"Anchor grids and lattices. An anchor grid and its corresponding anchor lattice offer a simpler
parameterization (i.e., an abstraction) of canvas transformations. Without such an abstraction, any
transformed [m] × [n] grid α ∈R(mn)×2 consists of 2mn free parameters. So, the optimization
problems (4) and (5) are 2mn+2 dimensional, which is not only computationally inefﬁcient for large
images but also has too much room for vanishing gradient. We use a simpler α-parameterization that
regularizes transformation, lowers distortion, and agrees with our intuition on rubber transformations."
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.3081761006289308,"Formally, an anchor system (G, ˆG) = (M×N, ˆ
M× ˆN) uses two layers of grids: an underlying grid G
and an anchor grid ˆG atop, satisfying ˆ
M ⊆M, ˆN ⊆N, and G ⊆ConvexHull( ˆG). Figure 3A shows"
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.31446540880503143,Under review as a conference paper at ICLR 2022
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.32075471698113206,"A.
B.
C."
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.3270440251572327,"Figure 3: An anchor system (A) and its transformation (B). (C) exempliﬁes a conﬁguration of
( ˆG, ρc)-solution path consisting of a chain of anchor grids/lattices and a chain of blurring."
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.3333333333333333,"one example, where G = [5]×[6] = {0, . . . , 4}×{0, . . . , 5} and ˆG = {0, 2, 4}×{0, 2, 5}. Under an
anchor system, we can uniquely represent any grid point g ∈G via four anchors Ag, Bg, Cg, Dg ∈ˆG
via proportional interpolation, or more precisely, the following double convex combination
g = (1 −λg)(1 −νg)Ag + (1 −λg)νgBg + λg(1 −νg)Cg + λgνgDg.
(6)"
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.33962264150943394,"Here, AgBgDgCg can be uniquely selected as the smallest rectangle in ˆG’s lattice containing g; the
two weight parameters λg, νg are computed based on relative position, e.g., as in Figure 3A. The
relation between grid points and anchors can be summarized by a weight matrix W ∈R|G|×| ˆ
G|. Its
ith row stores weights for the ith grid point (say g in (6)) and contains at most four non-zero entries
(i.e., coefﬁcients in (6)) located at the columns corresponding to Ag, Bg, Cg, Dg, respectively."
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.34591194968553457,"Given an anchor system (G, ˆG), any canvas transformation α
d= α ∈R|G|×2 under G and
d= ˆα ∈
R| ˆ
G|×2 under ˆG. ˆα is a submatrix of α, which induces an equivalence relation on the set of all canvas
transformations: α, β are equivalent iff ˆα = ˆβ, and ˆα abstracts the equivalence class {β | ˆβ = ˆα}.
Based on the maximum entropy principle (Jaynes, 1957), a reasonable selection of a representative of
this equivalence class is W ˆα, because W ˆα ∈{β | ˆβ = ˆα} and evenly distributes the transformed
grid points. Figure 3B illustrates this type of even distribution, which agrees with human intuition on
how a rubber surface would naturally react when transforming forces are applied at anchors."
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.3522012578616352,"Using an anchor system in optimization problems (4) and (5) adds very little to computing distortions
and gradients: we reuse the computation with α = W ˆα and perform only one additional chain-rule
step ∂α/∂ˆα = W. By doing so, however, the number of optimization variables in (4) or (5) reduces
from |G|+2 to | ˆG|+2 (e.g., if G = [28]×[28] and ˆG = {0, 27}×{0, 27}, the number reduces from
1570 to 10). It is important to note that using a simpler anchor grid is not the same as downsampling.
If it were, one would plug in α ←ˆα, but we plug in α ←W ˆα. In our case, image colors are still
sampled from the underlying grid rather than downsampled from the anchor grid. So, using our
anchor system is not information lossy while still beneﬁting from reduced optimization size. Running
gradient descent (w.r.t. anchors) in abstracted optimization spaces effectively bypasses critical points."
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.3584905660377358,"Blurring. Another view to lifting gradient descent to a high-level, abstracted optimization space, is
to blur the image. Intuitively, blurring ignores low-level ﬂuctuation, similar to how humans naturally
abstract an image. Blurring helps remedy vanishing gradients and is done in our image smoothing
process. The cutoff radius ρc in κ in (1) controls the blurring extent: larger ρc means more blurred."
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.36477987421383645,"Guided gradient descent. Mixing the two abstraction techniques yields our guided gradient descent
proceeding from higher- to lower-level abstractions. Given an anchor grid ˆG and a cutoff radius ρc,
we denote the corresponding (4) and (5) by DC( ˆG, ρc) and DV ( ˆG, ρc), respectively. For either, we
solve for a ( ˆG, ρc)-solution path, from coarser ˆG and larger ρc to ﬁner ˆG and smaller ρc. Let ˆGk be a
k × k evenly distributed anchor grid and ˆLk be its corresponding lattice. Figure 3C shows a chain
of anchor lattices {ˆL3i+1}i=0,1,2,... and cutoff radii {ηjρc0}j=0,1,2,.... It is easy initially to align
two blurred blobs via small canvas adjustments, implying a small number of iterations to converge
to DC ≈DV ≈0. As we proceed along the solution path, the images restore more detail but the
ﬁner ˆLk helps manage that detail. In a solution path, an earlier solution is used to warm start the
subsequent solve step, which further alleviates the curse of vanishing gradients. Notably, even the
starting ˆL2 comprising only four corner anchors parameterizes a large family of transformations
containing all afﬁne transformations. Finer anchor grids/lattices express more ﬂexible transformations
(including local, global, piecewise afﬁne, and more), approaching human-level ﬂexibility."
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.3710691823899371,Under review as a conference paper at ICLR 2022
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.37735849056603776,Dc-Nearest-
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.3836477987421384,"Neighbor
Dv-Nearest-"
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.389937106918239,Neighbor
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.39622641509433965,TextCaps SVM
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.4025157232704403,NeuralNetwork
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.4088050314465409,"Nearest-
Neighbor
RandomForest"
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.41509433962264153,DecisionTree 4 6 28 65 109 183 200+ 200+
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.42138364779874216,"Minimum number of training examples /class needed to reach 90%
Number of training examples /class (N)"
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.4276729559748428,Test set accuracy MNIST MNIST
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.4339622641509434,+data augment
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.44025157232704404,transfer learning
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.44654088050314467,CNN+dropout
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.4528301886792453,Capsule+Siamese
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.4591194968553459,"Figure 4: MNIST (10 classes) in the tiny-data regime: ﬁrst 1–20 training images per class and full
test set (examples shown on top). For each model listed in the legend, we plot its test accuracy versus
the training size N (bottom left) and also the smallest N needed to reach a threshold of 90% accuracy
(bottom right). Our model outperforms all other models for all N ∈{1, . . . , 20}, requiring the fewest
training examples (ﬁrst four or six per class) to reach 90% accuracy. 11 12 18 71 83 200+ 200+ 200+"
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.46540880503144655,"Minimum number of training examples /class needed to reach 75%
Number of training examples /class (N)"
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.4716981132075472,Test set accuracy
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.4779874213836478,"EMNIST-LETTERS
EMNIST-LETTERS
Dc-Nearest-"
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.48427672955974843,"Neighbor
Dv-Nearest-"
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.49056603773584906,Neighbor
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.4968553459119497,TextCaps SVM
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.5031446540880503,NeuralNetwork
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.5094339622641509,"Nearest-
Neighbor
RandomForest"
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.5157232704402516,DecisionTree
GRADIENT DESCENT AT HIGHER LEVELS OF ABSTRACTION,0.5220125786163522,"Figure 5: EMNIST-letters (26 classes) in the tiny-data regime: ﬁrst 1–20 training images per class
and full test set (examples shown on top). Results are shown in the same way as in Figure 4. Our
model outperforms all other models for all N ∈{1, . . . , 20}, requiring the fewest training examples
(ﬁrst eleven or twelve per class) to reach 75% accuracy (due to increased difﬁculty in this dataset)."
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.5283018867924528,"4
IMAGE CLASSIFICATION IN THE TINY DATA REGIME"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.5345911949685535,"We use our learned DC- / DV -distance in the simplest nearest-neighbor method to classify grayscale
images, named DC- / DV -nearest-neighbor. The whole process of metric learning and classiﬁcation is
human intuitive and interpretable. We show classiﬁcation performances on three standard benchmarks:
the MNIST and EMNIST datasets of handwritten digits and letters restricted to the tiny-data regime,
as well as the Omniglot challenge."
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.5408805031446541,"MNIST in the tiny-data regime. The original benchmark has 60k images for training and 10k for
testing, spanning 10 classes. To evaluate how a model performs in the tiny data regime, we train the
model on the ﬁrst N images per class from the original training set, test it on the full test set, and
record test accuracy versus N = 1, 2, 3, . . .. We compare our model to both neural-network-based and
classical ML models, including TextCaps (Jayasundara et al., 2019) with state-of-the-art performance
in the small-data regime, SVM, nearest neighbor, etc. Classical ML is included to show that nailing
the tiny-data regime does not mean just using simple models. For stochastic models, we record mean
and standard deviation from 5 independent runs. TextCaps only runs when N ≥4 and sometimes
returns a random guess (10%), so, we record trimmed mean and standard deviation from 11 runs
(where we trim the best two and worst four). We also copy results from other references that ran
MNIST in a similar tiny-data setting, including FSL that uses extra data for pre-training (whereas all
our other selected models do not). These results are from the same training-testing sizes but not the
same data sets, and hence, are considered indirect comparisons. We present all results in Figure 4."
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.5471698113207547,Under review as a conference paper at ICLR 2022 Run 1 train test Run 2 train test
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.5534591194968553,Background set size
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.559748427672956,(for pre-training)
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.5660377358490566,"original
reduced none"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.5723270440251572,augmented BPL
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.5786163522012578,"3.3%
4.2%"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.5849056603773585,Humans 4.5% Ours 6.75% 7.3%
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.5911949685534591,"RCN
Simple 
ConvNet"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.5974842767295597,"13.5%
23.2%"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6037735849056604,Prototypical Net
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.610062893081761,"13.7%
30.1% VHE 18.7%"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6163522012578616,"Siamese 
ConvNet 8% A. B."
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6226415094339622,"Figure 6: One-shot classiﬁcation in the Omniglot challenge (A) and its error-rate leaderboard (B).
The red bounding area marks one out of 400 unit tasks, made up of 1 test and 10 training images."
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6289308176100629,"EMNIST-letters in the tiny-data regime. The original benchmark has 4.8k training images per
class and 0.8k test images per class, spanning 26 classes of case-insensitive English letters. We
keep the same experimental setting as in MNIST (except for TextCaps being more stable now: we
do 7 independent runs for each N and trim the best and the worst). Results are shown in Figure 5.
EMNIST-letters is harder, not only with more classes but also more intrinsic ambiguities, e.g., an l
and an I can look identical, so can an h and an n when written carelessly. Hence, all models perform
signiﬁcantly worse than in MNIST. The intrinsic ambiguity, as well as more labeling errors, narrows
our superiority over other models as training size increases. This is especially true for the state-of-
the-art TextCaps model, catching up quickly in Figure 5. Being sensitive to ambiguities and outliers,
however, is not a deﬁciency of our distortable canvas model, but a property of nearest-neighbor. To
improve, we may integrate our model with more robust classiﬁers, e.g., k-nearest-neighbor (k-NN)
with proper voting. However, k-NN is not applicable in the tiny-data regime, not only because the
training size can be as small as k but also there is little room to hold out a validation set for selecting k.
An adaptive k-NN may be desired, with k remaining 1 in the tiny-data regime and becoming tunable
when training size increases to a level that affords a held-out validation set. A related issue due to
lacking validation data is about picking a proper model conﬁguration. One may expect better results
from any selected model in Figures 4 and 5 by attempting new conﬁgurations. Yet, it is unclear what
heuristics one may use. For TextCaps, we used its original implementation and conﬁguration; for the
rest, we used scikit-learn implementations with default conﬁgurations (except for small tweaks for the
tiny-data regime e.g., neural-network size and stronger regularization). By contrast, our distortable
canvas model requires little to tune, other than the ( ˆG, ρc)-solution path. Theoretically, the more
gradual the path, the better. We picked ( ˆG, ρc) based on only image size (28 here) and runtime."
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6352201257861635,"The Omniglot challenge for one-shot classiﬁcation. The Omniglot dataset contains handwritten
characters from 50 different alphabets, which include historical, present, and artiﬁcial scripts (e.g.,
Hebrew, Korean, “Futurama”) and thus, are far more complex than MNIST digits and EMNIST
letters. The characters are stored as both images and stroke movements. Unlike MNIST/EMNIST
coming with large training data, the Omniglot challenge was specially designed for human-level
concept learning from small data. Its one-shot classiﬁcation task was benchmarked to evaluate how
humans and machines can learn from a single example. This benchmark contains 20 independent
runs of 20-way within-alphabet classiﬁcations. The (2k −1)th and (2k)th runs for k = 1, . . . , 10
use the same set of 20 characters from a single alphabet. Each run uses 40 images: one training and
one test image per character. The unit task here is to predict for each test image, the character class to
which it belongs (one of 20), based on the 20 training images. In total, there are 400 independent unit
tasks across all 20 runs. Figure 6A shows a unit task (in red) and the ﬁrst two runs in the benchmark,
covering 1 alphabet, 20 characters, and 80 distinct images."
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6415094339622641,"The Omniglot benchmark adopted the standard FSL setting, where it also provided a background
set for pre-training. The original background set contains 964 character classes from 30 alphabets;
a reduced background set was proposed later to make the classiﬁcation task more challenging. We
run our DC-nearest-neighbor without any background set or any stroke-movement information. In"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6477987421383647,Under review as a conference paper at ICLR 2022
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6540880503144654,Number of clusters (k) WCSD
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.660377358490566,"Clustering “7”s from MNIST
Clustering doodles of giraﬀes"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6666666666666666,Figure 7: Archetype generation via k-means-style clustering in our learned similarity space.
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6729559748427673,"other words, in each unit task, we predict the test image based on one and only that training image
per character, and we read all images from their raw pixels. Shown in Figure 6B, our model (with a
6.75% error rate) approaches human performance (4.5%) and outperforms all models in the Omniglot
leaderboard (Lake et al., 2019), except for BPL specially designed for the Omniglot challenge by
making additional use of both the background set and the stroke-movement information."
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6792452830188679,"5
UNSUPERVISED LEARNING: ARCHETYPE GENERATION"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6855345911949685,"Beyond use with a classiﬁer, our distortable canvas model can also perform k-means-style clustering,
but within its human-intuitive topological similarity landscape. As in other metric learning and
non-Euclidean settings (Cuturi & Doucet, 2014), a naive way of explicitly computing distances and
then running k-means is not realistic. Learning distance in our model requires solving an optimization
problem, which is not as cheap as computing Euclidean distance. Further, computing a “centroid”
in non-Euclidean space requires solving another optimization problem (i.e., minimizing the sum of
within-cluster distances)—so an optimization problem of optimization problems—which is not as
simple as an arithmetic mean. Our idea of transformation ﬂow between two images can be extended
to multi-ﬂows among multiple images. Under this multi-ﬂow extension, we do not explicitly compute
pairwise distances, i.e., we do not solve the inner optimizations ﬁrst. Instead, we solve the inner and
outer optimizations at the same time, ﬂattening the nested optimizations into a single one. Formally,
given N images M1, . . . , MN, to cluster them into K clusters, we solve"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6918238993710691,"minimize
α1,...,αN
α1,...,αK
C1,...,CK K
X k=1 X"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.6981132075471698,"i∈Ck
DC(Mk ◦αk, Mi ◦αi)
subject to N
X"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.7044025157232704,"i=1
DV (αi) ≤ϵ,
(7)"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.710691823899371,"where Ck denotes the kth cluster, Mk ◦αk denotes the kth centroid, and Mi ◦αi denotes the ith
transformed image ﬂowing to its corresponding centroid together with all other N −1 transformed
images. One can check (7) is an extension of (4) where we omitted χ for simplicity. Solving (7) is
similar to k-means via alternating reﬁnement: the assignment step assigns each transformed image
Mi ◦αi to Ck⋆according to k⋆= arg mink=1,...,K DC(Mk ◦αk, Mi ◦αi); the update step solves
(7) for one gradient-descent step given the Cks. Upon convergence, we obtain C⋆
1, . . . , C⋆
K as clusters
and M1 ◦α1, . . . , MK ◦αK as centroids. We treat the learned centroids as archetypes of the N
images given at the beginning, which may be further deployed for education purposes."
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.7169811320754716,"We run the above procedure to generate archetypes of a particular image class. Like common practices
used in k-means, we try different k. For each k, we try multiple random starts and record the best
within-cluster sum of distances (WCSD). We use the elbow method to pick good values for k. Figure 7
shows the WCSD-versus-k curve obtained by running our clustering method on a set of 16 images
of “7”s from MNIST. The curve indicates k = 2 or 3 as a potential elbow point. The resulting two
clusters of “7” agree with human intuition regarding two general ways of writing “7”, depending on
whether there is an extra stroke. The resulting three clusters further divide the cluster of “simpler 7s”
based on the angle of the transverse stroke. Moving away from more strict symbol systems towards"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.7232704402515723,Under review as a conference paper at ICLR 2022
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.7295597484276729,distortable
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.7358490566037735,"canvas
“emoji” 
creation"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.7421383647798742,"classiﬁers 
beyond 1-NN
…"
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.7484276729559748,real-world images
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.7547169811320755,(photorealistic)
IMAGE CLASSIFICATION IN THE TINY DATA REGIME,0.7610062893081762,"“emojis” 
(of real-world images)"
ABSTRACT,0.7672955974842768,"abstract images 
(“emojis” already)
current model 
future work
future work"
ABSTRACT,0.7735849056603774,Figure 8: Generalization of our distortable canvas model for two major future directions (two ends).
ABSTRACT,0.779874213836478,"free-form art, we try our model on doodle data where people were tasked with freely drawing abstract
sketches of real-world objects. Figure 7 also shows four ways of doodling a giraffe, learned from the
ﬁrst 16 giraffes in Google’s Quick Draw dataset (Jongejan et al., 2016). The four learned archetypes
cleanly separate outline sketches, pose orientations, as well as focused views of the neck."
ABSTRACT,0.7861635220125787,"6
CONCLUSION, LIMITATION, AND FUTURE WORK"
ABSTRACT,0.7924528301886793,"Focusing on its scientiﬁc contribution, this paper designs a human-intuitive model from ﬁrst principles
to learn from few and only those few shots—in particular one and only one shot—requiring no extra
data for pre-training. Based on nativism, our introduced distortable canvas effectively models humans’
topological intuition and learns transformation-based visual similarity akin to how humans naturally
“distort” objects for comparison. This notion of similarity is formalized in our proposed optimization
problem, which minimizes canvas and color distortions so as to transform one object to another with
minimal distortion. To remedy vanishing gradients and solve the optimization efﬁciently, we mimic
human abstraction ability by chaining anchor lattices and image blurs into a solution path. This
yields our gradient descent method capable of optimizing at multiple levels of abstraction. Our model
outputs not only transformations but also transformation ﬂows that mimic human thought processes.
We demonstrate initial empirical success in a ﬁrst set of benchmarks focused on abstract visual tasks
such as character and doodle recognition. By simply using 1-NN, we achieved state-of-the-art results
in the tiny-data and single-datum regime on MNIST/EMNIST and achieved near-human performance
in the Omniglot challenge. Our model also enables k-means-style clustering to generate human-
interpretable archetypes. This paper is a ﬁrst step towards a general theory of a comprehensive,
human-like framework for human-level performance in diverse applications. The current paper
focuses on an initial scope, but opens the pathway to future generalizations as detailed below."
ABSTRACT,0.7987421383647799,"Consider two general types of images: 1) images of abstract patterns, or abstract images, e.g., those
of symbols and doodles; 2) photorealistic images of real-world objects, or real-world images, e.g.,
those in CIFAR10/100 (Krizhevsky & Hinton, 2009). This paper focuses on the ﬁrst type, handling
abstract images only, by modeling humans’ distortion-based intuition. For real-world images, it
may be more efﬁcient to ﬁrst model cognitive simpliﬁcation and then apply our current distortion
model. It is a reasonable assumption that humans have evolved to classify real-world images by ﬁrst
converting them into abstract icons, or “e (picture)+moji (character)”s (e.g., the emoji of a face, the
outline of a mountain, the shape of a lake) and then comparing these simpliﬁcations. Following this,
an efﬁcient way to apply our method to real-world images is to follow this pipeline—preprocessing
them ﬁrst into “emojis” and then comparing “emojis” using our distortion model. In fact, abstract
images (e.g., doodles of giraffes, hieroglyphics) are those that can be treated as “emojis” already.
There are baselines to attempt ﬁrst, e.g., smart edge detectors (Xie & Tu, 2015), but the human visual
system does more than edge detection. In the future, we will work on a complete theory of icon or
“emoji” creation mimicking human capacity and deal with real-world images, 3D objects, and more."
ABSTRACT,0.8050314465408805,"Although our model has shown dominant classiﬁcation performance in the tiny-data regime of the
presented benchmarks, its dominance diminishes when training size increases. This is due to 1-NN
being 100% biased towards the “nearest neighbor” and hence fragile against noisy, erroneous, and
ambiguous training examples. This suggests another future direction: our distortable canvas model
may be designed jointly with a new, human-like classiﬁer that introduces a small amount of learning
into classiﬁcation. The goal is to achieve state-of-the-art results on all training sizes, which is not
merely about swapping in and out existing classiﬁers. We will not be using black-box models, but
maintain model interpretability by modeling “the direct human way”, where we learn (from a small
training set) a particular function to be integrated into our distortion formulas. We will introduce
such functions in future work and continue to improve them. Figure 8 summarizes the pipeline for
generalizing our current distortable canvas model into the future directions sketched above."
ABSTRACT,0.8113207547169812,Under review as a conference paper at ICLR 2022
REFERENCES,0.8176100628930818,REFERENCES
REFERENCES,0.8238993710691824,"Amina Adadi and Mohammed Berrada. Peeking inside the black-box: A survey on explainable
artiﬁcial intelligence (XAI). IEEE Access, 6:52138–52160, 2018."
REFERENCES,0.8301886792452831,"Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric
deep learning: Going beyond Euclidean data. IEEE Signal Process. Mag., 34(4):18–42, 2017."
REFERENCES,0.8364779874213837,"Franc¸ois Chollet. On the measure of intelligence. arXiv:1911.01547v2 [cs.AI], 2019."
REFERENCES,0.8427672955974843,"Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik. EMNIST: An extension of
MNIST to handwritten letters. In Proc. 2017 Int. Joint Conf. Neural Netw. (IJCNN), pp. 2921–2926,
2017."
REFERENCES,0.8490566037735849,"Marco Cuturi and Arnaud Doucet. Fast computation of Wasserstein barycenters. In Proc. 31st Int.
Conf. Mach. Learn. (ICML 2014), pp. 685–693, 2014."
REFERENCES,0.8553459119496856,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proc. 34th Int. Conf. Mach. Learn. (ICML 2017), pp. 1126–1135, 2017."
REFERENCES,0.8616352201257862,"Kyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised learning via meta-learning. In Proc. 7th
Int. Conf. Learn. Represent. (ICLR 2019), 2019."
REFERENCES,0.8679245283018868,"Vinoj Jayasundara, Sandaru Jayasekara, Hirunima Jayasekara, Jathushan Rajasegaran, Suranga
Seneviratne, and Ranga Rodrigo. TextCaps: Handwritten character recognition with very small
datasets. In Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), pp. 254–262, 2019."
REFERENCES,0.8742138364779874,"Edwin T Jaynes. Information theory and statistical mechanics. Phys. Rev., 106(4):620–630, 1957."
REFERENCES,0.8805031446540881,"Jonas Jongejan, Henry Rowley, Takashi Kawashima, Jongmin Kim, and Nick Fox-Gieg. The Quick,
Draw!—AI Experiment. https://quickdraw.withgoogle.com, 2016."
REFERENCES,0.8867924528301887,"Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and
Neil Houlsby. Big transfer (bit): General visual representation learning. In Proc. 2020 European
Conf. Computer Vision (ECCV), pp. 491–507, 2020."
REFERENCES,0.8930817610062893,"Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images, 2009."
REFERENCES,0.89937106918239,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. Advances Neural Inf. Process. Syst., 25:1097–1105, 2012."
REFERENCES,0.9056603773584906,"Brenden M Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B Tenenbaum. One shot learning of
simple visual concepts. In Proc. 33rd Annu. Conf. Cognitive Sci. Soc., volume 33, pp. 2568–2573,
2011."
REFERENCES,0.9119496855345912,"Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332–1338, 2015."
REFERENCES,0.9182389937106918,"Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. The Omniglot challenge: A
3-year progress report. Current Opinion in Behavioral Sciences, 29:97–104, 2019."
REFERENCES,0.9245283018867925,"Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proc. IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.9308176100628931,"David G Lowe. Object recognition from local scale-invariant features. In Proc. 7th IEEE Int. Conf.
Computer Vision, volume 2, pp. 1150–1157, 1999."
REFERENCES,0.9371069182389937,"Kate T May.
How children learn so much from so little so quickly:
Laura Schulz at
TED2015.
https://blog.ted.com/how-children-learn-so-much-from-so-
little-so-quickly-laura-schulz-at-ted2015, 2015."
REFERENCES,0.9433962264150944,"Amiel Meiseles and Lior Rokach. Source model selection for deep learning in the time series domain.
IEEE Access, 8:6190–6200, 2020."
REFERENCES,0.949685534591195,Under review as a conference paper at ICLR 2022
REFERENCES,0.9559748427672956,"Diego A Mesa, Justin Tantiongloc, Marcela Mendoza, Sanggyun Kim, and Todd P Coleman. A
distributed framework for the construction of transport maps. Neural Computation, 31(4):613–652,
2019. doi: 10.1162/neco a 01172."
REFERENCES,0.9622641509433962,"Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22
(10):1345–1359, 2009."
REFERENCES,0.9685534591194969,"Elizabeth S Spelke and Katherine D Kinzler. Core knowledge. Developmental Sci., 10(1):89–96,
2007."
REFERENCES,0.9748427672955975,"Amos Storkey. When training and test sets are different: Characterizing learning transfer. Dataset
Shift Mach. Learn., 30:3–28, 2009."
REFERENCES,0.9811320754716981,"Cedric Villani. Optimal Transport: Old and New. Springer, 2009."
REFERENCES,0.9874213836477987,"Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples:
A survey on few-shot learning. ACM Comput. Surv., 53(3):1–34, 2020."
REFERENCES,0.9937106918238994,"Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In Proc. 2015 IEEE Int. Conf.
Computer Vision (ICCV), pp. 1395–1403, 2015."
