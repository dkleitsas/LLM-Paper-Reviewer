Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0008532423208191126,"Generalizing to out-of-distribution (OOD) data – that is, data from domains unseen
during training – is a key challenge in modern machine learning, which has only
recently received much attention. Some existing approaches propose leveraging
larger models and pre-training on larger datasets. In this paper, we provide new
insights in applying these approaches. Concretely, we show that larger models and
larger datasets need to be simultaneously leveraged to improve OOD performance
on image classiﬁcation. Moreover, we show that using smaller learning rates during
ﬁne-tuning is critical to achieving good results, contrary to popular intuition that
larger learning rates generalize better when training from scratch. We show that
strategies that improve in-distribution accuracy may, counter-intuitively, lead to
poor OOD performance despite strong in-distribution performance. Our insights
culminate to a method that achieves state-of-the-art results on a number of OOD
generalization benchmark tasks, often by a signiﬁcant margin."
INTRODUCTION,0.0017064846416382253,"1
INTRODUCTION"
INTRODUCTION,0.002559726962457338,"Most machine learning (ML) models assume that test data is drawn from the same distribution
as training data. However, this assumption does not hold in many real-world applications. As a
result, ML models often fail to generalize to out-of-distribution (OOD) data encountered during their
deployment and suffer from signiﬁcant performance drops compared with the model performance
on in-distribution (ID) data (Qui˜nonero-Candela et al., 2009; Torralba & Efros, 2011). For example,
common distribution shifts prevalent during test time include variation in locations (Koh et al., 2021)
and weather (Volk et al., 2019), noise and blur corruptions (Hendrycks & Dietterich, 2018), and
small adversarial perturbations (Szegedy et al., 2013). As ML models are increasingly deployed in
safety-critical applications, it is becoming ever more critical to ensure strong OOD generalization for
such models, i.e., the models robustly generalizing to relevant OOD data not seen during training."
INTRODUCTION,0.0034129692832764505,"While this problem is indeed difﬁcult since the goal is to generalize to data that are not seen during
training, there have been a handful of methods recently proposed to improve OOD generalization.
Some methods propose specialized training methods, such as simulating OOD data during training (Li
et al., 2018a), learning invariant representations (Arjovsky et al., 2019), and performing adversarial
data augmentation (Volpi et al., 2018). Intriguingly, Gulrajani & Lopez-Paz (2020) conducted an
extensive empirical evaluation on domain generalization benchmark datasets, and demonstrated
that classical empirical risk minimization (ERM) approach achieves nearly state-of-the-art OOD
generalization performance compared with these specialized methods. On the other hand, most of the
approaches apply small or medium size networks that are usually pre-trained on the ImageNet-1k
dataset, such as pre-trained ResNet50 (Gulrajani & Lopez-Paz, 2020). On the other hand, recent
works ﬁnd that pre-training on larger and more diverse data is one of the most effective paths toward
generalizing to out-of-distribution data on ImageNet (Taori et al., 2020)."
INTRODUCTION,0.004266211604095563,"In this paper, we systematically investigate the importance of pre-trained models for OOD gen-
eralization. Speciﬁcally, we conduct extensive experiments on models with different model sizes
that are pre-trained on large datasets. The pre-trained models are then ﬁne-tuned on the training
data for the underlying task. Instead of focusing on achieving state-of-the-art results on benchmark
OOD datasets, our empirical study aims to develop a better understanding of the critical role that
pre-trained models along with different design choices for ﬁne-tuning such models play in ensuring"
INTRODUCTION,0.005119453924914676,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.005972696245733789,(a) Fine-tuning procedure for OOD generalization.
INTRODUCTION,0.006825938566552901,"larger 
model"
INTRODUCTION,0.007679180887372013,"larger 
pre-training"
INTRODUCTION,0.008532423208191127,dataset
INTRODUCTION,0.00938566552901024,(b) ID v.s. OOD.
INTRODUCTION,0.010238907849829351,"Figure 1: (Left) Illustration of our ﬁne-tuning procedure used throughout this paper, i.e., we ﬁne-tune
a pre-trained model on training dataset D and evaluate the model performance on OOD data Tood – a
data domain not seen during training. (Right) Evaluating ID and OOD accuracies for two classes of
pre-trained models. Orange squares represent the ImageNet-1k pre-trained models and blue circles
represent the IG-1B pre-trained models, where IG-1B is a larger dataset than ImageNet-1k. Larger
marker size means the model size is larger."
INTRODUCTION,0.011092150170648464,"good OOD generalization. This provides novel insights towards closing the gap between ID and
OOD generalization for future research."
INTRODUCTION,0.011945392491467578,The main contributions of our work are as follows:
INTRODUCTION,0.012798634812286689,"• When leveraging larger models for OOD generalization, we ﬁnd that both large pre-training
dataset sizes and large model sizes are critical. Missing either one of the two components may
hurt OOD generalization."
INTRODUCTION,0.013651877133105802,"• We show that using a small learning rate generalizes better for OOD when we leverage a pre-
trained model. This is a complementary argument to Li et al. (2019) that suggested to use a large
learning rate for better generalization in the case of no pre-training."
INTRODUCTION,0.014505119453924915,"• We show cases where improving in-distribution performance actually leads to worse OOD
performance suggesting that ID performance is not a reliable indicator of OOD performance."
INTRODUCTION,0.015358361774744027,"• Our insights culminate in a method that achieves SOTA, often by a signiﬁcant margin, on a
number of OOD generalization benchmark tasks including PACS, VLCS, and Ofﬁce-Home."
PRELIMINARIES AND EXPERIMENTAL SETUP,0.016211604095563138,"2
PRELIMINARIES AND EXPERIMENTAL SETUP"
PRELIMINARIES AND EXPERIMENTAL SETUP,0.017064846416382253,"We begin this section by introducing the out-of-distribution generalization problem, with the primary
focus on a special case of this broader issue, namely domain generalization. Subsequently, we
describe the experimental setup adopted in our empirical study and various parameter choices we
make throughout the paper."
PRELIMINARIES AND EXPERIMENTAL SETUP,0.017918088737201365,"In this paper, we study a general multi-class classiﬁcation setting, with all input instances and their
labels belonging to X and Y := {1, . . . , K}, respectively. Let D denote the training dataset which
may potentially comprises data belong to k different training domains {Dj}j∈[k]:={1,...,k}, i.e., D =
∪j∈[k]Dj. We assume that the training data from the j-th domain Dj = {(xj
i, yj
i )}i∈[nj] ⊂X × Y
is sampled from the distribution P j
id, i.e., (xj
i, yj
i ) ∼P j
id. At test time, we evaluate a trained model
for both its in-distribution and out-of-distribution performance. The in-distribution performance is
evaluated on a test dataset Tid that consists of instances belonging to the domains encountered in the
training dataset D. On the other hand, we utilize an OOD test dataset Tood from an unseen domain
with distribution Pood to assess the model’s OOD performance."
PRELIMINARIES AND EXPERIMENTAL SETUP,0.01877133105802048,"Given a family of candidate classiﬁers F = {f : X →Y} and the underlying training dataset D, we
primarily employ standard empirical risk minimization (ERM) (Vapnik, 1998) to learn a classiﬁer ˆf
as follows:"
PRELIMINARIES AND EXPERIMENTAL SETUP,0.01962457337883959,"bf = argmin
f∈F 1
|D| X"
PRELIMINARIES AND EXPERIMENTAL SETUP,0.020477815699658702,"i∈D
ℓ(f(xi), yi),
(1)"
PRELIMINARIES AND EXPERIMENTAL SETUP,0.021331058020477817,Under review as a conference paper at ICLR 2022
PRELIMINARIES AND EXPERIMENTAL SETUP,0.02218430034129693,"where ℓ(·, ·) denotes the cross-entropy loss function. For a classiﬁer f, we deﬁne its in-distribution
accuracy Accid,f and out-of-distribution accuracy Accood,f as follows:"
PRELIMINARIES AND EXPERIMENTAL SETUP,0.02303754266211604,"Accid,f = E(x,y)∈Tid

1
 bf(x) ̸= y
	
;
Accood,f = E(x,y)∈Tood

1
 bf(x) ̸= y
	
,
(2)"
PRELIMINARIES AND EXPERIMENTAL SETUP,0.023890784982935155,"where 1{·} denotes the standard indicator function. Often, models that achieve large in-distribution
accuracy Accid only achieve relatively small out-of-distribution accuracy Accood, i.e., Accid ≫
Accood (Torralba & Efros, 2011; Hendrycks & Gimpel, 2016; Gulrajani & Lopez-Paz, 2020). Thus,
under the domain generalization problem, one particularly focuses on designing training methods that
result in classiﬁer with good performance on both out-of-distribution data and in-distribution data."
PRELIMINARIES AND EXPERIMENTAL SETUP,0.024744027303754267,"Pre-trained models.
We mainly focus on ﬁne-tuning pre-trained models on the training dataset D
by using ERM. We explore four classes of pre-trained models for OOD generalization: (1). ResNet-
based models (He et al., 2016a; Xie et al., 2017) pre-trained on ImageNet (Russakovsky et al., 2015)
(ResNet50, ResNext50-32x4d, and ResNext101-32x8d); (2). (BiTm)-ResNet-v2-based models (He
et al., 2016b) pre-trained on ImageNet-21k (Deng et al., 2009) (ResNetV2-50x1, ResNetV2-50x3, and
ResNetV2-101x1) (Kolesnikov et al., 2020), where group normalization (Wu & He, 2018) and weight
standardization (Qiao et al., 2019) are used in ResNetV2; (3). (SWSL)-ResNet-based semi-weakly
supervised ImageNet models pre-trained on IG-1B-Targeted data (Yalniz et al., 2019); and (4). Vision
transformer (ViT) pre-trained on ImageNet-21k (Dosovitskiy et al., 2020). A detailed description of
these pre-trained models can be found in Table 3 (in Appendix)."
PRELIMINARIES AND EXPERIMENTAL SETUP,0.025597269624573378,"OOD datasets.
In our experiments, we use four vision datasets used to benchmark domain gen-
eralization algorithms (Gulrajani & Lopez-Paz, 2020): (1). PACS dataset (Li et al., 2017); (2).
Ofﬁce-Home dataset (Venkateswara et al., 2017); (3). VLCS dataset (Fang et al., 2013); and (4).
TerraIncognita dataset (Beery et al., 2018). Each of these datasets contains 4 different domains.
We train the models on 3 domains and treat the examples from the remaining domain as the out-of-
distribution data Tood. For the three training domains, we use 80% data as training dataset and the
remaining 20% data for evaluation. We add the test domain information after the dataset to specify
the test domain, for example, PACS (S) means the training domains are ‘P’, ‘A’, and ‘C’ and the test
(OOD) domain is ‘S’."
PRELIMINARIES AND EXPERIMENTAL SETUP,0.026450511945392493,"Fine-tuning and model selection.
We use stochastic gradient descent (SGD) with a momen-
tum of 0.9 for ﬁne-tuning all pre-trained models considered in this paper. The default weight
decay for SGD is set to be 0. We use a cosine learning rate decay (Loshchilov & Hutter, 2016)
as learning rate scheduler for SGD. For initial learning rates η we compare the following set
{0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002, 0.0001}. For evaluation, we pick the ﬁve
checkpoints from each model with highest in-distribution accuracy. We then compute the aver-
age OOD accuracy of these ﬁve checkpoints."
MAIN RESULTS,0.027303754266211604,"3
MAIN RESULTS"
MAIN RESULTS,0.028156996587030716,"We present our main experimental results in this section. First, we highlight the importance of models
pre-trained on large and diverse datasets for OOD generalization. Next, we investigate the effect of
ﬁne-tuning learning rates, especially for models pre-trained on diverse datasets. Then, we perform
a systematic examination of several components of pre-trained models for OOD generalization,
including model size, pre-training dataset, and model architecture. Finally, we evaluate whether
techniques used for improving ID accuracy can also enhance OOD generalization."
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.02901023890784983,"3.1
IMPORTANCE OF A BETTER PRE-TRAINED MODEL"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.029863481228668942,"Models pre-trained on more diverse datasets have been shown to achieve better OOD generalization
on real-world distribution shifts (Taori et al., 2020; Hendrycks et al., 2020a). As a warm-up for un-
derstanding the properties of pre-trained models on OOD data, we ﬁrst study the OOD generalization
performance of a ResNet-based model that is pre-trained on a large and diverse pre-training dataset.
In particular, we focus on an SWSL-ResNext101-32x4d model pre-trained on the IG-1B-Targeted
data (Yalniz et al., 2019), which is a much larger and more diverse dataset than ImageNet."
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.030716723549488054,"The ﬁne-tuning approach described in Section 2 with different learning rate signiﬁcantly outperforms
the baseline results from Gulrajani & Lopez-Paz (2020) (cf. Table 1). This shows that a better pre-
trained model indeed improves OOD generalization without using specialized algorithms for domain"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.031569965870307165,Under review as a conference paper at ICLR 2022
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.032423208191126277,"Table 1: Comparison with ERM baseline results from Gulrajani & Lopez-Paz (2020). We compare
our approach with the baseline in terms of the OOD accuracy Accood. Note that our approach
amounts to ﬁne-tuning SWSL-ResNext101-32x4d (Yalniz et al., 2019) with different learning rates
and employing the models selection procedure described in Section 2. Note that, for each benchmark,
we treat one of the four domains as the OOD domain and ﬁne-tune the model on the remaining three
domains. We report results for all four choices for the OOD domain on each benchmark."
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.033276450511945395,"OOD Domain
OOD Domain"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.034129692832764506,"PACS
A
C
P
S
VLCS
C
L
S
V"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.03498293515358362,"Baseline
88.1
78.0
97.8
79.1
Baseline
97.6
63.3
72.2
76.4
Ours
96.2
94.6
99.4
91.3
Ours
98.2
66.1
77.0
80.5"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.03583617747440273,"Ofﬁce-Home
A
C
P
R
TerraIncognita
L100
L38
L43
L46"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.03668941979522184,"Baseline
62.7
53.4
76.5
77.3
Baseline
50.8
42.5
57.9
37.6
Ours
76.4
68.5
86.5
87.6
Ours
48.3
47.5
57.2
43.7"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.03754266211604096,"0.94
0.96
0.98
In-distribution Accuracy 0.70 0.75 0.80 0.85 0.90 0.95"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.03839590443686007,OOD Accuracy
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.03924914675767918,"PACS, Test domain:C"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.04010238907849829,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.040955631399317405,(a) PACS (C).
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.041808873720136516,"0.70
0.75
0.80
0.85
0.90
In-distribution Accuracy 0.4 0.5 0.6 0.7"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.042662116040955635,OOD Accuracy
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.043515358361774746,"OfficeHome, Test domain:C"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.04436860068259386,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.04522184300341297,(b) OfﬁceHome (C).
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.04607508532423208,"0.75
0.80
0.85
0.90
In-distribution Accuracy 0.55 0.60 0.65 0.70 0.75 0.80"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.04692832764505119,OOD Accuracy
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.04778156996587031,"VLCS, Test domain:S"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.04863481228668942,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.04948805460750853,(c) VLCS (S).
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.050341296928327645,"0.92
0.93
0.94
In-distribution Accuracy 0.2 0.3 0.4 0.5"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.051194539249146756,OOD Accuracy
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.05204778156996587,"TerraIncognita, Test domain:L38"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.052901023890784986,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.0537542662116041,(d) Terra (L38).
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.05460750853242321,"10
4
10
3
10
2
Learning rate 0.7 0.8 0.9 1.0"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.05546075085324232,Accuracy
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.05631399317406143,"PACS, Test domain:C"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.05716723549488054,"Baseline
In-distribution accuracy
OOD accuracy"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.05802047781569966,(e) PACS (C).
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.05887372013651877,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.059726962457337884,Accuracy
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.060580204778156996,"OfficeHome, Test domain:C"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.06143344709897611,"Baseline
In-distribution accuracy
OOD accuracy"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.06228668941979522,(f) OfﬁceHome (C).
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.06313993174061433,"10
4
10
3
10
2
Learning rate 0.6 0.7 0.8 0.9"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.06399317406143344,Accuracy
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.06484641638225255,"VLCS, Test domain:S"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.06569965870307168,"Baseline
In-distribution accuracy
OOD accuracy"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.06655290102389079,(g) VLCS (S).
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.0674061433447099,"10
3
10
2
Learning rate 0.2 0.4 0.6 0.8"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.06825938566552901,Accuracy
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.06911262798634812,"TerraIncognita, Test domain:L38"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.06996587030716724,"Baseline
In-distribution accuracy
OOD accuracy"
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.07081911262798635,(h) Terra (L38).
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.07167235494880546,"Figure 2: Evaluating models (SWSL-ResNext101-32x4d) ﬁne-tuned using different learning rates
on ID and OOD data. (Top row) Scatter plot of ID accuracy (X-axis) and OOD accuracy (Y -axis).
(Bottom row) Compare ID accuracy with OOD accuracy w.r.t. learning rate (X-axis). The green
dashed line corresponds to the baseline OOD accuracy, and the blue dash-dotted line represents the
selected model (by selecting the model with best ID accuracy)."
IMPORTANCE OF A BETTER PRE-TRAINED MODEL,0.07252559726962457,"generalization. For example, the OOD accuracy improves from 79.1% to 91.3% on PACS (S), and
62.7% to 76.4% on Ofﬁce-Home (A). Overall, Table 1 suggests that using a larger model pre-trained
on more data can be very effective for better OOD generalization. Next, we conduct more detailed
experiments to better understand the OOD generalization performance of various pre-trained models."
EFFECT OF FINE-TUNE LEARNING RATE,0.07337883959044368,"3.2
EFFECT OF FINE-TUNE LEARNING RATE"
EFFECT OF FINE-TUNE LEARNING RATE,0.07423208191126279,"Given the fact that simply ﬁne-tuning the SWSL model leads to compelling improvement in the OOD
generalization, we now take a closer look at models trained with different learning rates. We evaluate
models trained with different learning rates on both ID and OOD test data. Figure 2 summarizes the
results for ﬁne-tuning SWSL-ResNext101-32x4d with different learning rates. In Figure 2(a)-2(d),
each point in the plot corresponds to a model trained with a distinct learning rate. As mentioned in
Section 2, we select models that achieves the top-5 ID accuracy, and depict the standard deviation
of both Accid and Accood for each model. We only show results for models that achieve > 95%
training accuracy to better compare model performance."
EFFECT OF FINE-TUNE LEARNING RATE,0.07508532423208192,"Regularization effect of small learning rate.
Based on Figure 2, our main ﬁnding is that the
ﬁne-tuning learning rate plays a key role in determining both ID and OOD accuracy. In particular, we"
EFFECT OF FINE-TUNE LEARNING RATE,0.07593856655290103,Under review as a conference paper at ICLR 2022
EFFECT OF FINE-TUNE LEARNING RATE,0.07679180887372014,"10
3
10
2
10
1
100
101
102
In-distribution Training Loss 0.6 0.7 0.8 0.9"
EFFECT OF FINE-TUNE LEARNING RATE,0.07764505119453925,OOD Accuracy
EFFECT OF FINE-TUNE LEARNING RATE,0.07849829351535836,"PACS, Test domain:C"
EFFECT OF FINE-TUNE LEARNING RATE,0.07935153583617748,"Baseline
LR=0.01
LR=0.005
LR=0.001"
EFFECT OF FINE-TUNE LEARNING RATE,0.08020477815699659,(a) PACS (C).
EFFECT OF FINE-TUNE LEARNING RATE,0.0810580204778157,"10
2
10
1
100
In-distribution Training Loss 0.4 0.5 0.6 0.7"
EFFECT OF FINE-TUNE LEARNING RATE,0.08191126279863481,OOD Accuracy
EFFECT OF FINE-TUNE LEARNING RATE,0.08276450511945392,"OfficeHome, Test domain:C"
EFFECT OF FINE-TUNE LEARNING RATE,0.08361774744027303,"Baseline
LR=0.01
LR=0.005
LR=0.001"
EFFECT OF FINE-TUNE LEARNING RATE,0.08447098976109214,(b) OfﬁceHome (C).
EFFECT OF FINE-TUNE LEARNING RATE,0.08532423208191127,"10
3
10
2
10
1
100
In-distribution Training Loss 0.6 0.7 0.8"
EFFECT OF FINE-TUNE LEARNING RATE,0.08617747440273038,OOD Accuracy
EFFECT OF FINE-TUNE LEARNING RATE,0.08703071672354949,"VLCS, Test domain:S"
EFFECT OF FINE-TUNE LEARNING RATE,0.0878839590443686,"Baseline
LR=0.01
LR=0.005
LR=0.001"
EFFECT OF FINE-TUNE LEARNING RATE,0.08873720136518772,(c) VLCS (S).
EFFECT OF FINE-TUNE LEARNING RATE,0.08959044368600683,"Figure 3: OOD accuracy of models (SWSL-ResNext101-32x4d) during training. We visualize models
trained with three different learning rates in terms of OOD accuracy vs. training loss. Each point in
the above plots represents the model evaluated at one iteration during training"
EFFECT OF FINE-TUNE LEARNING RATE,0.09044368600682594,"observe that, in most of the settings considered in this paper, the models trained with smaller learning
rates achieve much better OOD generalization1, even when the ID accuracy does not change much.
This is different from the fact that larger learning rates generalize better when the model is directly
trained on the training data D without a pre-training phase (Li et al., 2019; Lewkowycz et al., 2020;
Nakkiran, 2020). We also observe similar behavior with SWSL-ResNext101-32x8d models, where
smaller learning rates lead to better OOD performance (see Figure 9 in Appendix). Intriguingly, we
do not observe this phenomenon for models pre-trained on ImageNet. There, as shown in Figure 12
in Appendix, when the ID accuracy is similar, a model trained with larger learning rate achieves
better OOD generalization (e.g., see Figure 12(e) and 12(f)). To summarize, our results indicates that
when models are pre-trained on more diverse datasets, smaller learning rates can lead to better OOD
generalization."
EFFECT OF FINE-TUNE LEARNING RATE,0.09129692832764505,"To obtain a better understanding of the effect of learning rate, we also study the OOD accuracy during
training. In Figure 3, we visualize the OOD accuracy vs. training loss as measured every 100 SGD
iterations for models trained with three different learning rates η ∈{0.01, 0.005, 0.001}. Note that
all three learning rates eventually achieve similar training loss, but the models trained with smaller
learning rates have better OOD generalization. Figure 3 also suggests that models trained with larger
learning rates cannot achieve similar OOD accuracy by using early stopping. Meanwhile, the ﬁgure
also conﬁrms the regularization effect of small learning rates for ﬁne-tuning pre-trained models."
EFFECT OF FINE-TUNE LEARNING RATE,0.09215017064846416,"Overall, we ﬁnd that the learning rate is a key parameter for achieving good OOD generalization.
While different learning rates may not affect ID accuracy much, OOD accuracy can be very sensitive
to the choice of learning rate. Our results also highlight a limitation of performing model selection
based on ID accuracy as models with similar ID accuracy may have very different OOD performance."
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.09300341296928327,"3.3
PRE-TRAINING FOR BETTER OOD GENERALIZATION"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.09385665529010238,"Now, we systematically explore the role that pre-trained models play in improving OOD gener-
alization. Speciﬁcally, we study three aspects of pre-trained models: pre-training dataset, model
architecture, and model size. Furthermore, we compare the models trained from scratch (i.e., without
pre-training) with pre-trained models with respect to OOD generalization."
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.0947098976109215,"Effect of pre-training dataset.
We study the OOD performance of models pre-trained on different
datasets, inlcuding ResNext101-32x8d pre-trained on ImageNet, BiTm-ResNetV2-50x3 pretrained
on ImageNet-21k, and SWSL-ResNext101-32x8d pre-trained on IG-1B-Targeted. The SWSL model
and BiTm model achieve similar Top-1 accuracies on ImageNet, 84.2% and 84.0%, respectively, and
the standard ResNext101-32x8d achieves a slightly worse in terms of Top-1 accuracy of 79.3%. The
results for this comparison are summarized in Figure 4 and Figure 15 (in Appendix)."
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.09556313993174062,"Our ﬁrst observation is that the pre-training dataset has a big impact on OOD generalization perfor-
mance. With same architecture and model size, the SWSL pre-trained model consistently outperforms
the standard ImageNet pre-trained model across. Secondly, we ﬁnd that SWSL generally performs
the best among all three models as it is pre-trained on the largest and the most diverse pre-training
dataset. Furthermore, we ﬁnd that the BiTm model pre-trained on ImageNet-21k also generally"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.09641638225255973,"1Among models achieve similar ID generalization, models trained with smaller learning rates achieve better
OOD generalization."
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.09726962457337884,Under review as a conference paper at ICLR 2022
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.09812286689419795,"0.92
0.94
0.96
0.98
In-distribution Accuracy 0.70 0.75 0.80 0.85 0.90 0.95"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.09897610921501707,OOD Accuracy
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.09982935153583618,"PACS, Test domain:C"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.10068259385665529,"Baseline
resnext101_32x8d
resnetv2_50x3_bitm
resnext101_32x8d_swsl"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.1015358361774744,(a) PACS (C).
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.10238907849829351,"0.6
0.7
0.8
0.9
In-distribution Accuracy 0.3 0.4 0.5 0.6 0.7"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.10324232081911262,OOD Accuracy
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.10409556313993173,"OfficeHome, Test domain:C"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.10494880546075085,"Baseline
resnext101_32x8d
resnetv2_50x3_bitm
resnext101_32x8d_swsl"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.10580204778156997,(b) OfﬁceHome (C).
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.10665529010238908,"0.84
0.86
0.88
0.90
In-distribution Accuracy 0.65 0.70 0.75 0.80"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.1075085324232082,OOD Accuracy
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.1083617747440273,"VLCS, Test domain:S"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.10921501706484642,"Baseline
resnext101_32x8d
resnetv2_50x3_bitm
resnext101_32x8d_swsl"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.11006825938566553,(c) VLCS (S).
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.11092150170648464,"Figure 4: Evaluating OOD and ID performance of models pre-trained on different datasets. Each
color corresponds to the models pre-trained on a distinct dataset and the dash-dotted line represents
the model picked by our model selection procedure. For each model we report the accuracy across
different ﬁne-tuning learning rates."
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.11177474402730375,"0.85
0.90
0.95
In-distribution Accuracy 0.60 0.65 0.70 0.75 0.80 0.85"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.11262798634812286,OOD Accuracy
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.11348122866894197,"PACS, Test domain:C"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.11433447098976109,"Baseline
vit
resnetv2_bitm"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.11518771331058021,(a) PACS (C).
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.11604095563139932,"0.7
0.8
0.9
In-distribution Accuracy 0.3 0.4 0.5 0.6 0.7"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.11689419795221843,OOD Accuracy
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.11774744027303755,"OfficeHome, Test domain:C"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.11860068259385666,"Baseline
vit
resnetv2_bitm"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.11945392491467577,(b) OfﬁceHome (C).
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.12030716723549488,"0.775
0.800
0.825
0.850
0.875
0.900
In-distribution Accuracy 0.60 0.65 0.70 0.75 0.80"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.12116040955631399,OOD Accuracy
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.1220136518771331,"VLCS, Test domain:S"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.12286689419795221,"Baseline
vit
resnetv2_bitm"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.12372013651877133,(c) VLCS (S).
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.12457337883959044,"Figure 5: A comparison of four ViT models and three BiTm models on OOD accuracy and ID
accuracy. The orange squares represent ViT models and the blue circles represent BiTm models. The
dash-dotted lines represent the selected models. We do not distinguish the model architectures within
the same model class."
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.12542662116040956,"performs better than the ResNext model pre-trained on ImageNet (except for the PACS dataset, where
both models have similar OOD performance)."
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.12627986348122866,"Another interesting ﬁnding is that the OOD accuracy can be signiﬁcantly improved by chosing the
right ﬁne-tuning learning rate, while the improvement in the ID accuracy is small. For example,
in Figure 4(a), the ID accuracy improves from 98% to 99%, whereas the OOD accuracy increases
∼10%. Our results suggest that the “accuracy on the line” phenomenon (Miller et al., 2021) does
not always hold for the benchmark datasets used in domain generalization. This also echoes the
observation from D’Amour et al. (2020), that OOD generalization can be very different among
models with similar ID accuracies."
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.12713310580204779,"Effect of model architecture: ViTs vs. CNNs.
We now investigate the role that the model archi-
tecture plays in ensuring good OOD generalization. Compared with convolutional neural networks
(CNNs), the recently proposed Vision Transformers (ViT) achieves similar or even better performance
on image classiﬁcation tasks (Dosovitskiy et al., 2020). This raises the question whether vision
transformers behave differently from CNNs in terms of their OOD performance? Towards this, we
explore three BiT-ResNetV2 pre-trained models (Kolesnikov et al., 2020) and four ViT pre-trained
models (Dosovitskiy et al., 2020). In particular, we compare BiTm-ResNetV2-{50x1, 101x1, 50x3}
with ViT-{small-patch32, small-patch16, base-patch32, base-patch16}. We present the comparison
between ViTs and BiTms on OOD benchmarks in Figure 5 and Figure 16 (in Appendix)."
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.12798634812286688,"We ﬁnd that the OOD generalization accuracy of ViT models is similar to that of BiTm models. In
some cases, BiTs slightly outperform ViTs on OOD generalization, for example, results shown in
Figure 5(b), 5(c), 16(a). Since both classes of models are pre-trained on the same pre-training dataset
(i.e., ImageNet-21k) and achieve similar ImageNet Top-1 accuracies (84.5% for ViTs vs. 84.0% for
BiTms), our results indicate that replacing convolution operation with self-attention operation does
not bring additional beneﬁts in terms of OOD generalization for the settings we consider in this paper."
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.128839590443686,"Effect of model size.
It is evident from Figure 4, where both the baseline model (i.e., ResNet50)
and ResNext101-32x8d are pre-trained on the ImageNet-1k dataset, that increasing the model"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.1296928327645051,Under review as a conference paper at ICLR 2022
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.13054607508532423,"PACS
OfficeHome
VLCS 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.13139931740614336,OOD Accuracy
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.13225255972696245,ImageNet Pretrain
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.13310580204778158,"ResNet50
ResNext50
ResNext101"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.13395904436860068,(a) ImageNet pre-training.
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.1348122866894198,"PACS
OfficeHome
VLCS 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.1356655290102389,OOD Accuracy
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.13651877133105803,BiTm Pretrain
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.13737201365187712,"BiTm-ResNetV2_50x1
BiTm-ResNetV2_101x1
BiTm-ResNetV2_50x3"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.13822525597269625,(b) BiTm pre-training.
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.13907849829351535,"PACS
OfficeHome
VLCS 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.13993174061433447,OOD Accuracy
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.1407849829351536,SWSL Pretrain
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.1416382252559727,"SWSL-ResNet50
SWSL-ResNext50_32x4d
SWSL-ResNext101_32x8d"
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.14249146757679182,(c) SWSL pre-training.
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.14334470989761092,"Figure 6: Evaluating OOD generalization for three classes of models with different model sizes. Left:
Results for ResNe(x)t models pre-trained on ImageNet-1k. Middle: Results for BiTm-ResNetV2
models pre-trained on ImageNet-21k. Right: Results for SWSL-ResNe(x)t models pre-trained on
IG-1B-Targeted. For a given model class, each model size is represented by a distinct color."
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.14419795221843004,"size from ResNet50 to ResNext101-32x8d can improve OOD generalization. For example, OOD
generalization is improved from 78.0% to 84.9% in Figure 4(a). Motivated by these promising results,
we conduct experiments to understand the role of model size for OOD generalization. We consider
increasing model size on three classes of models, including ResNe(x)t pre-trained on ImageNet-1k,
BiTm-ResNetV2 pre-trained on ImageNet-21k, and ResNe(x)t pre-trained on IG-1B-Targeted. We
investigate three model sizes for each class of pre-trained models and the results are summarized in
Figure 6 and Table 4-6 (in Appendix)."
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.14505119453924914,"For all three classes of models, we ﬁnd that increasing the model size can improve OOD generalization
in many settings. Overall, this shows that model size is a crucial design choice for improving OOD
generalization. Furthermore, we note that SWSL-ResNet50 is pre-trained on a larger and more
diverse dataset than BiTm-ResNetV2-50x3 and both models achieve ∼100% training accuracy, but
SWSL-ResNet50 has lower OOD accuracy than BiTm-ResNetV2-50x3 on OfﬁceHome (C). This
suggests that both the pre-training dataset and the model size play key role in determining the OOD
generalization, and ideally it is preferable to employ larger models pre-trained on larger and more
diverse pre-training data."
PRE-TRAINING FOR BETTER OOD GENERALIZATION,0.14590443686006827,"OOD performance of models trained from random initialization.
To further validate the impor-
tance of pre-training data along with the model size for better OOD generalization, we train models
with increasing model sizes from random initialization, i.e., without employing a pre-training phase.
Our results for this experiment (cf. Table 8) show that larger models do not signiﬁcantly improve the
OOD generalization without the use of pre-training. For example, increasing ResNext50-32x4d to
ResNext101-32x8d only improves the OOD accuracy on OfﬁceHome (C) by less than 2%. Thus, our
results suggest that without pre-training, only increasing model size is not very effective in improving
the OOD performance."
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.14675767918088736,"3.4
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.1476109215017065,"In this subsection, we examine various techniques that have been shown to improve ID accuracy to
assess their utility in achieving good OOD generalization. First, we study the impact of training data
size on the OOD generalization, as increasing the number of training samples is an effective approach
to improve ID generalization. Then we evaluate four techniques in our OOD setting, including label
smoothing (Szegedy et al., 2016), AutoAugment (Cubuk et al., 2018), PatchGaussian (Lopes et al.,
2019), and Sharpness-Aware Minimization (SAM) (Foret et al., 2020)."
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.14846416382252559,"Utility of more training data.
We consider ﬁne-tuning with datsets of four different sizes, i.e.,
100%, 50%, 25%, and 12.5% of the total training data for a given benchmark. Our results in Figure 7
show that increasing the training data from 12.5% to 100% does not signiﬁcantly improve OOD
generalization. In fact, on VLCS (S), a better OOD generalization is realized when we utilize less
training data to train a ResNext101-32x8d model (cf. Figure 7(c)). This suggests that increasing ID
training samples is not as effective as using larger and better pre-trained models."
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.1493174061433447,"Methods for improving ID accuracy.
We ﬁnd that applying the methods listed in Table 2 does
not signiﬁcantly improve the OOD generalization across four datasets compared with scaling model
size and pre-training dataset size, and utilizing augmentations/regularization can potentially even hurt"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.15017064846416384,Under review as a conference paper at ICLR 2022
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.15102389078498293,"0
20
40
60
80
100
Training Data Size (%) 0.80 0.85 0.90"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.15187713310580206,OOD Accuracy
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.15273037542662116,"PACS, Test domain:S"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.15358361774744028,"Baseline
swsl_resnext50_32x4d
swsl_resnext101_32x8d"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.15443686006825938,(a) PACS-OOD (S).
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.1552901023890785,"0
20
40
60
80
100
Training Data Size (%) 0.55 0.60 0.65 0.70"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.1561433447098976,OOD Accuracy
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.15699658703071673,"OfficeHome, Test domain:C"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.15784982935153583,"Baseline
swsl_resnext50_32x4d
swsl_resnext101_32x8d"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.15870307167235495,(b) OfﬁceHome-OOD (C).
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.15955631399317405,"0
20
40
60
80
100
Training Data Size (%) 0.64 0.66 0.68 0.70"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.16040955631399317,OOD Accuracy
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.1612627986348123,"VLCS, Test domain:L"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.1621160409556314,"Baseline
swsl_resnext50_32x4d
swsl_resnext101_32x8d"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.16296928327645052,(c) VLCS-OOD (L).
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.16382252559726962,"0
20
40
60
80
100
Training Data 0.975 0.980 0.985 0.990 0.995"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.16467576791808874,Accuracy
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.16552901023890784,"PACS, Test domain:S"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.16638225255972697,"swsl_resnext50_32x4d
swsl_resnext101_32x8d"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.16723549488054607,(d) PACS-ID (S).
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.1680887372013652,"0
20
40
60
80
100
Training Data 0.84 0.86 0.88 0.90 0.92 0.94"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.1689419795221843,Accuracy
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.1697952218430034,"OfficeHome, Test domain:C"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.17064846416382254,"swsl_resnext50_32x4d
swsl_resnext101_32x8d"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.17150170648464164,(e) OfﬁceHome-ID (C).
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.17235494880546076,"0
20
40
60
80
100
Training Data 0.91 0.92 0.93"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.17320819112627986,Accuracy
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.17406143344709898,"VLCS, Test domain:L"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.17491467576791808,"swsl_resnext50_32x4d
swsl_resnext101_32x8d"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.1757679180887372,(f) VLCS-ID (L).
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.1766211604095563,"Figure 7: Evaluating OOD generalization performance of models trained with different number of
training samples. X-axis represents the number of training samples. We use SWSL-ResNext50-
32x4d and SWSL-ResNext101-32x8d as the pre-trained models. For each pre-trained model, we
visualize the OOD accuracies of the top-3 models selected by ID accuracy."
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.17747440273037543,"Table 2: Evaluation of four techniques (label smoothing, AutoAugment, PatchGaussian, and SAM)
for OOD generalization. We use the same pre-trained model (SWSL-ResNext101-32x4d) across
all settings. The number inside the parentheses after the method name represents the value of the
technique-speciﬁc hyperparameter, e.g., PatchGaussian (1.0) corresponds to employing PatchGaus-
sian (Lopes et al., 2019) with σ = 1.0. We highlight the best two OOD accuracies for each dataset
with bold text."
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.17832764505119453,"Method
PACS (C)
Ofﬁce (C)
VLCS (L)
Terra (L46)"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.17918088737201365,"ERM (in Table 1)
94.6
68.5
66.1
43.7
Label Smoothing (0.1)
91.6
70.6
65.2
42.5
Label Smoothing (0.2)
93.5
70.8
66.3
44.6
AutoAugment
93.5
70.7
65.2
38.1
PatchGaussian (1.0)
92.8
65.8
64.6
13.1
PatchGaussian (0.5)
94.4
69.3
63.6
9.7
SAM (0.02)
93.5
72.2
67.1
44.7
SAM (0.05)
92.8
71.3
67.5
42.3"
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.18003412969283278,"OOD generalization. For example, applying PatchGaussian decreases OOD accuracy on Terra (L46)
from 43.7% to 13.1% and 9.7% with σ = 1.0 and 0.5, respectively. In constrast, PatchGaussian has
very minimal impact on the ID performance, where it achieves Accid = 95.8% and Accid = 95.6%,
respectively; ERM attains Accid = 95.9%. Contrary to our results, Lopes et al. (2019) notice that
PatchGaussian can improve both the clean (ID) accuracy and robustness to common corruptions
(OOD accuracy). This suggests that one should employing augmentation/regularization techniques
carefully so as to not harm OOD generalization as a side effect. On the other hand, SAM with
parameter2 ρ = 0.02 improves the OOD generalization on three benchmarks. Overall, we ﬁnd that
methods used for improving ID accuracy do not necessarily improve OOD accuracy, when compared
with the simple ERM-based ﬁne-tuning approach."
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.18088737201365188,2The perturbation parameter ρ is deﬁned in Foret et al. (2020).
ANALYSIS OF TECHNIQUES USED FOR IMPROVING ID ACCURACY,0.181740614334471,Under review as a conference paper at ICLR 2022
RELATED WORK,0.1825938566552901,"4
RELATED WORK"
RELATED WORK,0.18344709897610922,"Domain adaptation and domain generalization.
Ben-David et al. (2010) proposed H∆H-
divergence and applied it to develop error bounds on new test data distribution that are different
from the training distribution. Motivated by the theoretical results in Ben-David et al. (2010), a
large body of work was devoted to learning domain-invariant representations for domain adaptation
where unlabeled data from the target domain are available during training (Ajakan et al., 2014; Ganin
et al., 2016; Tzeng et al., 2017; Zhao et al., 2018). Different from the domain adaptation problem,
no information from the target domain is available in the domain generalization problem. Recent
works proposed various algorithms to improve domain generalization, including learning invariant
representations across training domains (Li et al., 2018b; Arjovsky et al., 2019), distributionally
robust optimization Sagawa et al. (2020), data augmentation (Volpi et al., 2018; Zhou et al., 2020),
and causal framework (Heinze-Deml & Meinshausen, 2017; Mahajan et al., 2021). Zhou et al. (2021);
Wang et al. (2021) provide a more detailed review of the topic of domain generalization.
OOD generalization and model robustness.
Recent works developed new datasets to evaluate
model robustness to out-of-distribution data, including ImageNet-V2 (Recht et al., 2019), CIFAR-
10.1 (Recht et al., 2018), ImageNet-C and CIFAR-10-C (Hendrycks & Dietterich, 2018), ImageNet-
R (Hendrycks et al., 2020a), WILDS (Koh et al., 2021), etc. A line of work proposed methods to
improve model robustness to commons corruptions (Lopes et al., 2019; Xie et al., 2020; Hendrycks
et al., 2020b; Calian et al., 2021; Yi et al., 2021). Xie et al. (2021) investigated how to leverage
auxiliary information and pre-training to improve OOD generalization.
On multiple datasets,
the researchers have observed a near linear correlation between the OOD accuracy and the ID
accuracy (Recht et al., 2019; Miller et al., 2020; Mania & Sra, 2020; Miller et al., 2021). However,
Taori et al. (2020) found that most approaches, including the ones that improve robustness to synthetic
distribution shifts, do not improve model robustness to natural distribution shifts, except for the
models that are pre-trained on larger datasets. Similar observation has been made in Hendrycks
et al. (2020a). Our work focuses on the domain generalization benchmark datasets as well as how to
perform ﬁne-tuning one various pre-trained models for better OOD generalization."
DISCUSSION AND FUTURE WORK,0.18430034129692832,"5
DISCUSSION AND FUTURE WORK"
DISCUSSION AND FUTURE WORK,0.18515358361774745,"Pre-training is one of the most effective approaches for improving model performance in a wide
range of machine learning tasks. In this work, we perform an empirical study of ﬁne-tuning a diverse
set of pre-trained models and evaluate their OOD generalization. We ﬁnd that simply ﬁne-tuning
larger models pre-trained on more (diverse) data can signiﬁcantly improve model performance on
OOD data. Additionally, and we also identify the regularization effect of small learning rates that is
important for achieving better OOD generalization. Further, through extensive experimentation, we
demonstrate that, while relying on pre-trained models, model size and pre-training dataset play a key
role in ensuring good OOD generalization. We hope our results can further inspire future research
on bridging the gap between in-distribution accuracy and out-of-distribution accuracy. There are
multiple interesting immediate directions to explore in future work that we discuss next.
Scaling model size and pre-training dataset size. Our empirical results indicate that model size and
the pre-training dataset size are two essential factors for improving OOD performance. Even though
we primarily focus on image classiﬁcation benchmarks, similar behavior has been observed in NLP
domain (Brown et al., 2020), where increasing model size leads to monotonic improvements in
zero-shot performance on unseen tasks for example. It is worth noting that while increasing model
size or pre-training dataset size only marginally improves the in-distribution accuracy, the increase in
OOD accuracy can be much larger. It is an interesting direction to explore the differences in ID and
OOD generalization in other domains.
How to perform model selection? Our study indicates that models trained via different methods can
exhibit a large variation in terms of their OOD performance, even when their in-distribution accuracy
is very similar, e.g., see Figure 4(c) and Figure 5(c). We thus believe that the development of better
model selection approaches for OOD generalization will be a key direction of future work.
Limitations of ERM. Despite impressive results for OOD generalization, we ﬁnd that ERM-based ﬁne-
tuning of pre-trained models is unlikely to close the in-distribution to out-of-distribution generalization
gap , even when the model size or the pre-training dataset becomes much larger. To generalize to
unseen domains, it might be necessary to have access to extra information about the OOD dataset
(e.g., unlabeled OOD data as available in the domain adaptation setting (Pan & Yang, 2009)) and/or
design novel algorithms (e.g., updating the model parameters during test time (Sun et al., 2020))."
DISCUSSION AND FUTURE WORK,0.18600682593856654,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.18686006825938567,"Reproducibility Statement.
In Section 2 and Appendix A.2, We provide detailed descriptions on
the implementations used in this paper, including about the pre-trained models, datasets, models
selection, hyperparameters, and how to train the models."
REFERENCES,0.18771331058020477,REFERENCES
REFERENCES,0.1885665529010239,"Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois Laviolette, and Mario Marchand. Domain-
adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014."
REFERENCES,0.189419795221843,"Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019."
REFERENCES,0.19027303754266212,"Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the
European conference on computer vision (ECCV), pp. 456–473, 2018."
REFERENCES,0.19112627986348124,"Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning, 79(1):151–175, 2010."
REFERENCES,0.19197952218430034,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.19283276450511946,"Dan A Calian, Florian Stimberg, Olivia Wiles, Sylvestre-Alvise Rebufﬁ, Andras Gyorgy, Timothy
Mann, and Sven Gowal. Defending against image corruptions through adversarial augmentations.
arXiv preprint arXiv:2104.01086, 2021."
REFERENCES,0.19368600682593856,"Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018."
REFERENCES,0.1945392491467577,"Alexander D’Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel,
Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Underspeciﬁcation
presents challenges for credibility in modern machine learning. arXiv preprint arXiv:2011.03395,
2020."
REFERENCES,0.19539249146757678,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.1962457337883959,"Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith.
Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping.
arXiv preprint arXiv:2002.06305, 2020."
REFERENCES,0.197098976109215,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020."
REFERENCES,0.19795221843003413,"Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple
datasets and web images for softening bias. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 1657–1664, 2013."
REFERENCES,0.19880546075085323,"Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efﬁciently improving generalization. In International Conference on Learning Representations,
2020."
REFERENCES,0.19965870307167236,"Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
The journal of machine learning research, 17(1):2096–2030, 2016."
REFERENCES,0.20051194539249148,"Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint
arXiv:2007.01434, 2020."
REFERENCES,0.20136518771331058,Under review as a conference paper at ICLR 2022
REFERENCES,0.2022184300341297,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016a."
REFERENCES,0.2030716723549488,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630–645. Springer, 2016b."
REFERENCES,0.20392491467576793,"Christina Heinze-Deml and Nicolai Meinshausen. Conditional variance penalties and domain shift
robustness. arXiv preprint arXiv:1710.11469, 2017."
REFERENCES,0.20477815699658702,"Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. In International Conference on Learning Representations, 2018."
REFERENCES,0.20563139931740615,"Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution
examples in neural networks. arXiv preprint arXiv:1610.02136, 2016."
REFERENCES,0.20648464163822525,"Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul
Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical
analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020a."
REFERENCES,0.20733788395904437,"Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-
narayanan. AugMix: A simple data processing method to improve robustness and uncertainty.
Proceedings of the International Conference on Learning Representations (ICLR), 2020b."
REFERENCES,0.20819112627986347,"Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361, 2020."
REFERENCES,0.2090443686006826,"Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsub-
ramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne
David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec,
Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A
benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning
(ICML), 2021."
REFERENCES,0.2098976109215017,"Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and
Neil Houlsby. Big transfer (bit): General visual representation learning. In Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16, pp.
491–507. Springer, 2020."
REFERENCES,0.21075085324232082,"Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218,
2020."
REFERENCES,0.21160409556313994,"Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain
generalization. In Proceedings of the IEEE international conference on computer vision, pp.
5542–5550, 2017."
REFERENCES,0.21245733788395904,"Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-
learning for domain generalization. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
2018a."
REFERENCES,0.21331058020477817,"Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, and
Stefano Soatto. Rethinking the hyperparameters for ﬁne-tuning. arXiv preprint arXiv:2002.11770,
2020."
REFERENCES,0.21416382252559726,"Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao.
Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the
European Conference on Computer Vision (ECCV), pp. 624–639, 2018b."
REFERENCES,0.2150170648464164,"Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2019."
REFERENCES,0.2158703071672355,Under review as a conference paper at ICLR 2022
REFERENCES,0.2167235494880546,"Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, and Ekin D Cubuk. Improving
robustness without sacriﬁcing accuracy with patch gaussian augmentation.
arXiv preprint
arXiv:1906.02611, 2019."
REFERENCES,0.2175767918088737,"Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016."
REFERENCES,0.21843003412969283,"Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In
International Conference on Machine Learning, pp. 7313–7324. PMLR, 2021."
REFERENCES,0.21928327645051193,"Horia Mania and Suvrit Sra. Why do classiﬁer accuracies show linear trends under distribution shift?
arXiv preprint arXiv:2012.15483, 2020."
REFERENCES,0.22013651877133106,"John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of natural distribution
shift on question answering models. In International Conference on Machine Learning, pp.
6905–6916. PMLR, 2020."
REFERENCES,0.22098976109215018,"John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar,
Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation
between out-of-distribution and in-distribution generalization. In International Conference on
Machine Learning, pp. 7721–7735. PMLR, 2021."
REFERENCES,0.22184300341296928,"Preetum Nakkiran. Learning rate annealing can provably help generalization, even for convex
problems. arXiv preprint arXiv:2005.07360, 2020."
REFERENCES,0.2226962457337884,"Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345–1359, 2009."
REFERENCES,0.2235494880546075,"Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Micro-batch training with
batch-channel normalization and weight standardization. arXiv preprint arXiv:1903.10520, 2019."
REFERENCES,0.22440273037542663,"Joaquin Qui˜nonero-Candela, Masashi Sugiyama, Neil D Lawrence, and Anton Schwaighofer. Dataset
shift in machine learning. Mit Press, 2009."
REFERENCES,0.22525597269624573,"Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classiﬁers
generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018."
REFERENCES,0.22610921501706485,"Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers
generalize to imagenet? In International Conference on Machine Learning, pp. 5389–5400. PMLR,
2019."
REFERENCES,0.22696245733788395,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International journal of computer vision, 115(3):211–252, 2015."
REFERENCES,0.22781569965870307,"Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust
neural networks. In International Conference on Learning Representations, 2020."
REFERENCES,0.22866894197952217,"Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training
with self-supervision for generalization under distribution shifts. In International Conference on
Machine Learning, pp. 9229–9248. PMLR, 2020."
REFERENCES,0.2295221843003413,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013."
REFERENCES,0.23037542662116042,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818–2826, 2016."
REFERENCES,0.23122866894197952,"Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig
Schmidt.
Measuring robustness to natural distribution shifts in image classiﬁcation.
arXiv
preprint arXiv:2007.00644, 2020."
REFERENCES,0.23208191126279865,"Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pp. 1521–1528.
IEEE, 2011."
REFERENCES,0.23293515358361774,Under review as a conference paper at ICLR 2022
REFERENCES,0.23378839590443687,"Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
7167–7176, 2017."
REFERENCES,0.23464163822525597,"V Vapnik. Statistical learning theory new york. NY: Wiley, 1:2, 1998."
REFERENCES,0.2354948805460751,"Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep
hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 5018–5027, 2017."
REFERENCES,0.2363481228668942,"Georg Volk, Stefan M¨uller, Alexander von Bernuth, Dennis Hospach, and Oliver Bringmann. Towards
robust cnn-based object detection through augmentation with synthetic rain variations. In 2019
IEEE Intelligent Transportation Systems Conference (ITSC), pp. 285–292. IEEE, 2019."
REFERENCES,0.23720136518771331,"Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John Duchi, Vittorio Murino, and Silvio
Savarese. Generalizing to unseen domains via adversarial data augmentation. arXiv preprint
arXiv:1805.12018, 2018."
REFERENCES,0.2380546075085324,"Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Wenjun Zeng, and Tao Qin. Generalizing
to unseen domains: A survey on domain generalization. arXiv preprint arXiv:2103.03097, 2021."
REFERENCES,0.23890784982935154,"Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on
computer vision (ECCV), pp. 3–19, 2018."
REFERENCES,0.23976109215017063,"Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and Quoc V Le. Adversarial
examples improve image recognition. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 819–828, 2020."
REFERENCES,0.24061433447098976,"Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1492–1500, 2017."
REFERENCES,0.24146757679180889,"Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-
n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness.
In International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=jznizqvr15J."
REFERENCES,0.24232081911262798,"I Zeki Yalniz, Herv´e J´egou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-
supervised learning for image classiﬁcation. arXiv preprint arXiv:1905.00546, 2019."
REFERENCES,0.2431740614334471,"Mingyang Yi, Lu Hou, Jiacheng Sun, Lifeng Shang, Xin Jiang, Qun Liu, and Zhiming Ma. Improved
ood generalization via adversarial training and pretraing. In Marina Meila and Tong Zhang
(eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of
Proceedings of Machine Learning Research, pp. 11987–11997. PMLR, 18–24 Jul 2021."
REFERENCES,0.2440273037542662,"Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.
arXiv preprint arXiv:2106.04560, 2021."
REFERENCES,0.24488054607508533,"Han Zhao, Shanghang Zhang, Guanhang Wu, Jos´e MF Moura, Joao P Costeira, and Geoffrey J Gordon.
Adversarial multiple source domain adaptation. Advances in Neural Information Processing
Systems, 31:8559–8570, 2018."
REFERENCES,0.24573378839590443,"Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In
International Conference on Learning Representations, 2020."
REFERENCES,0.24658703071672355,"Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A
survey. arXiv preprint arXiv:2103.02503, 2021."
REFERENCES,0.24744027303754265,Under review as a conference paper at ICLR 2022
REFERENCES,0.24829351535836178,"A
APPENDIX"
REFERENCES,0.24914675767918087,"A.1
OTHER RELATED WORKS
Pre-training and ﬁne-tuning.
Li et al. (2020) examined how to select hyperparameters for ﬁne-
tuning on a various benchmark vision datasets. Dodge et al. (2020) performed an extensive study on
ﬁne-tuning pre-trained language models on NLP benchmarks, and investigated the impact of weight
initialization and training data order on model performance. Kaplan et al. (2020) studied the scaling
laws, including pre-training dataset size, model size, and compute budget for training, for language
models. They found that increasing model size for pre-training can improve model performance on
OOD data. More recently, Zhai et al. (2021) explored how to improve ViT model performance on
ImageNet and few-shot learning tasks through scaling the model size and data size."
REFERENCES,0.25,"A.2
ADDITIONAL EXPERIMENTAL DETAILS"
REFERENCES,0.2508532423208191,"Implementation details.
Our implementation is mainly adapted from DomainBed (developed in
Gulrajani & Lopez-Paz (2020)). Before we ﬁne-tune the pre-trained models, we replace the ﬁnal
linear layer of the pre-trained model with a random initialized linear layer with output dimension
equals to the number of classes for the dataset. Most of the pre-trained models are from PyTorch
Image Models (timm). The default training iteration is 5, 000. For the training from scratch setting
(i.e., without pre-training), we set the training iteration as 10, 000 and weight decay as 10−5. During
the ﬁne-tuning, we save model checkpoint every 100 iterations and select ﬁve checkpoints that
achieves the top-5 in-distribution accuracies. The input size of the ViT models is (3, 224, 224). In
Table 3, we summarize the top-1/top-5 ImageNet accuracies of pre-trained models used in this paper."
REFERENCES,0.25170648464163825,"Table 3: Top-1 accuracy and Top-5 accuracy of pre-trained models considered in this paper.
(ImageNet) ResNet-based models pre-trained on ImageNet-1k. (BiTm) ResNetV2-based mod-
els pre-trained on ImageNet-21k. (SWSL) ResNet-based models pre-trained on IG-1B-Targeted.
(ViT) Vision transformer-based models pre-trained on ImageNet-21k."
REFERENCES,0.2525597269624573,"ImageNet
ResNet50
ResNext50-32x4d
ResNext101-32x8d"
REFERENCES,0.25341296928327645,"Top-1 Acc
76.13
77.62
79.30
Top-5 Acc
92.86
93.69
94.51"
REFERENCES,0.25426621160409557,"BiTm
ResNetV2-50x1
ResNetV2-101x1
ResNetV2-50x3"
REFERENCES,0.2551194539249147,"Top-1 Acc
80.34
82.33
84.01
Top-5 Acc
95.68
96.51
97.12"
REFERENCES,0.25597269624573377,"SWSL
ResNet50
ResNext50-32x4d
ResNext101-32x4d
ResNext101-32x8d"
REFERENCES,0.2568259385665529,"Top-1 Acc
81.16
82.18
83.23
84.28
Top-5 Acc
95.97
96.23
96.76
97.17"
REFERENCES,0.257679180887372,"ViT
Small-Patch32
Base-Patch32
Small-Patch16
Base-Patch16"
REFERENCES,0.25853242320819114,"Top-1 Acc
75.99
80.72
81.40
84.53
Top-5 Acc
93.27
95.56
96.13
97.29"
REFERENCES,0.2593856655290102,"A.3
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.26023890784982934,"In this subsection, we present additional experimental results in Section 3."
REFERENCES,0.26109215017064846,"Effect of ﬁne-tune learning rate in Section 3.2.
In addition to Figure 2 and Figure 3, we provide
more experimental results on the effect of ﬁne-tune learning rate. For ﬁne-tuning results with different
learning rates, we present more results for ﬁne-tuning SWSL-ResNext101-32x4d in Figure 8, results
for ﬁne-tuning SWSL-ResNext101-32x8d in Figure 9, results for ﬁne-tuning SWSL-ResNext50-
32x4d in Figure 10, results for ﬁne-tuning BiTm-ResNetV2-50x3 in Figure 11, results for ﬁne-tuning
ResNext101-32x8d (ImageNet pre-trained) in Figure 12, and results for ﬁne-tuning ResNext50-
32x4d (ImageNet pre-trained) in Figure 13. Meanwhile, in Figure A.3, we provide more results on
visualizing the OOD accuracy vs. training loss for SWSL-ResNext101-32x4d."
REFERENCES,0.2619453924914676,Under review as a conference paper at ICLR 2022
REFERENCES,0.2627986348122867,"Effect of pre-training dataset.
In Figure 15, we provide additional experimental results on study-
ing the effect of pre-training dataset."
REFERENCES,0.2636518771331058,"Effect of model architecture: ViTs vs. CNNs.
In Figure 16, we provide additional experimental
results on comparing the performance of ViTs and CNNs."
REFERENCES,0.2645051194539249,"Effect of model size.
We provide additional results on comparing models with different model
sizes in Table 4 (ImageNet-1k pre-trained models), Table 5 (BiTm pre-trained models (Kolesnikov
et al., 2020)), Table 6 (SWSL pre-trained models (Yalniz et al., 2019)), Table 7 (ViT pre-trained
models (Dosovitskiy et al., 2020)), and Table 8 (without pre-training)."
REFERENCES,0.26535836177474403,"Utility of more training data.
In Figure 17, we provide additional experimental results on investi-
gating the impact of the number of training samples on the TerraIncognita dataset, including the
ID/OOD accuracy results."
REFERENCES,0.26621160409556316,"Methods for improving ID accuracy.
In Table 9, we provide the in-distribution accuracy evalua-
tions of the methods described in Table 2. Also, in Table 10 and 11, we provide additional results on
the ID/OOD accuracy evaluations of different methods (on more datasets)."
REFERENCES,0.26706484641638223,"0.94
0.96
0.98
In-distribution Accuracy 0.80 0.85 0.90"
REFERENCES,0.26791808873720135,OOD Accuracy
REFERENCES,0.2687713310580205,"PACS, Test domain:S"
REFERENCES,0.2696245733788396,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.27047781569965873,(a) PACS (S).
REFERENCES,0.2713310580204778,"0.70
0.75
0.80
0.85
0.90
In-distribution Accuracy 0.3 0.4 0.5 0.6 0.7 0.8"
REFERENCES,0.2721843003412969,OOD Accuracy
REFERENCES,0.27303754266211605,"OfficeHome, Test domain:A"
REFERENCES,0.2738907849829352,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.27474402730375425,(b) OfﬁceHome (A).
REFERENCES,0.27559726962457337,"0.84
0.86
0.88
0.90
0.92
In-distribution Accuracy 0.62 0.64 0.66 0.68"
REFERENCES,0.2764505119453925,OOD Accuracy
REFERENCES,0.2773037542662116,"VLCS, Test domain:L"
REFERENCES,0.2781569965870307,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.2790102389078498,(c) VLCS (L).
REFERENCES,0.27986348122866894,"0.93
0.94
0.95
0.96
In-distribution Accuracy 0.35 0.40 0.45"
REFERENCES,0.28071672354948807,OOD Accuracy
REFERENCES,0.2815699658703072,"TerraIncognita, Test domain:L46"
REFERENCES,0.28242320819112626,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005"
REFERENCES,0.2832764505119454,(d) Terra (L46).
REFERENCES,0.2841296928327645,"10
4
10
3
10
2
Learning rate 0.80 0.85 0.90 0.95 1.00"
REFERENCES,0.28498293515358364,Accuracy
REFERENCES,0.2858361774744027,"PACS, Test domain:S"
REFERENCES,0.28668941979522183,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.28754266211604096,(e) PACS (S).
REFERENCES,0.2883959044368601,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8"
REFERENCES,0.28924914675767915,Accuracy
REFERENCES,0.2901023890784983,"OfficeHome, Test domain:A"
REFERENCES,0.2909556313993174,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.29180887372013653,(f) OfﬁceHome (A).
REFERENCES,0.29266211604095566,"10
4
10
3
10
2
Learning rate 0.6 0.7 0.8 0.9"
REFERENCES,0.2935153583617747,Accuracy
REFERENCES,0.29436860068259385,"VLCS, Test domain:L"
REFERENCES,0.295221843003413,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.2960750853242321,(g) VLCS (L).
REFERENCES,0.29692832764505117,"10
3
10
2
Learning rate 0.4 0.6 0.8"
REFERENCES,0.2977815699658703,Accuracy
REFERENCES,0.2986348122866894,"TerraIncognita, Test domain:L46"
REFERENCES,0.29948805460750855,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.3003412969283277,(h) Terra (L46).
REFERENCES,0.30119453924914674,"Figure 8: (Additional results) Evaluating models (SWSL-ResNext101-32x4d) ﬁne-tuned by different
learning rates on in-distribution and out-of-distribution data. (Top row) Scatter plot of in-distribution
accuracy (X-axis) and out-of-distribution accuracy (Y -axis). (Bottom row) Compare in-distribution
accuracy with out-of-distribution accuracy w.r.t. learning rate (X-axis). The green dashed line
corresponds to the baseline OOD accuracy, and the blue dash-dotted line represents the selected
model (by selecting the model with best in-distribution accuracy)."
REFERENCES,0.30204778156996587,Under review as a conference paper at ICLR 2022
REFERENCES,0.302901023890785,"0.94
0.96
0.98
In-distribution Accuracy 0.75 0.80 0.85 0.90 0.95"
REFERENCES,0.3037542662116041,OOD Accuracy
REFERENCES,0.3046075085324232,"PACS, Test domain:C"
REFERENCES,0.3054607508532423,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.30631399317406144,(a) PACS (C).
REFERENCES,0.30716723549488056,"0.6
0.7
0.8
0.9
In-distribution Accuracy 0.3 0.4 0.5 0.6 0.7"
REFERENCES,0.30802047781569963,OOD Accuracy
REFERENCES,0.30887372013651876,"OfficeHome, Test domain:C"
REFERENCES,0.3097269624573379,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.310580204778157,(b) OfﬁceHome (C).
REFERENCES,0.31143344709897613,"0.87
0.88
0.89
0.90
In-distribution Accuracy 0.675 0.700 0.725 0.750 0.775 0.800"
REFERENCES,0.3122866894197952,OOD Accuracy
REFERENCES,0.31313993174061433,"VLCS, Test domain:S"
REFERENCES,0.31399317406143346,"Baseline
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.3148464163822526,(c) VLCS (S).
REFERENCES,0.31569965870307165,"0.92
0.93
0.94
0.95
In-distribution Accuracy 0.425 0.450 0.475 0.500 0.525"
REFERENCES,0.3165529010238908,OOD Accuracy
REFERENCES,0.3174061433447099,"TerraIncognita, Test domain:L38"
REFERENCES,0.318259385665529,"Baseline
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002"
REFERENCES,0.3191126279863481,(d) TerraIncognita (L38).
REFERENCES,0.3199658703071672,"10
4
10
3
10
2
Learning rate 0.75 0.80 0.85 0.90 0.95 1.00"
REFERENCES,0.32081911262798635,Accuracy
REFERENCES,0.3216723549488055,"PACS, Test domain:C"
REFERENCES,0.3225255972696246,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.32337883959044367,(e) PACS (C).
REFERENCES,0.3242320819112628,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8"
REFERENCES,0.3250853242320819,Accuracy
REFERENCES,0.32593856655290104,"OfficeHome, Test domain:C"
REFERENCES,0.3267918088737201,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.32764505119453924,(f) OfﬁceHome (C).
REFERENCES,0.32849829351535836,"10
4
10
3
10
2
Learning rate 0.70 0.75 0.80 0.85 0.90"
REFERENCES,0.3293515358361775,Accuracy
REFERENCES,0.3302047781569966,"VLCS, Test domain:S"
REFERENCES,0.3310580204778157,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.3319112627986348,(g) VLCS (S).
REFERENCES,0.33276450511945393,"10
3
10
2
Learning rate 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.33361774744027306,Accuracy
REFERENCES,0.33447098976109213,"TerraIncognita, Test domain:L38"
REFERENCES,0.33532423208191126,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.3361774744027304,(h) TerraIncognita (L38).
REFERENCES,0.3370307167235495,"0.8
0.9
1.0
In-distribution Accuracy 0.4 0.6 0.8"
REFERENCES,0.3378839590443686,OOD Accuracy
REFERENCES,0.3387372013651877,"PACS, Test domain:S"
REFERENCES,0.3395904436860068,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.34044368600682595,(i) PACS (S).
REFERENCES,0.3412969283276451,"0.70
0.75
0.80
0.85
0.90
In-distribution Accuracy 0.4 0.6 0.8"
REFERENCES,0.34215017064846415,OOD Accuracy
REFERENCES,0.3430034129692833,"OfficeHome, Test domain:A"
REFERENCES,0.3438566552901024,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.3447098976109215,(j) OfﬁceHome (A).
REFERENCES,0.3455631399317406,"0.75
0.80
0.85
0.90
In-distribution Accuracy 0.575 0.600 0.625 0.650 0.675"
REFERENCES,0.3464163822525597,OOD Accuracy
REFERENCES,0.34726962457337884,"VLCS, Test domain:L"
REFERENCES,0.34812286689419797,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.34897610921501704,(k) VLCS (L).
REFERENCES,0.34982935153583616,"0.945
0.950
0.955
0.960
0.965
In-distribution Accuracy 0.35 0.40 0.45"
REFERENCES,0.3506825938566553,OOD Accuracy
REFERENCES,0.3515358361774744,"TerraIncognita, Test domain:L46"
REFERENCES,0.35238907849829354,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005"
REFERENCES,0.3532423208191126,(l) TerraIncognita (L46).
REFERENCES,0.35409556313993173,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8 1.0"
REFERENCES,0.35494880546075086,Accuracy
REFERENCES,0.35580204778157,"PACS, Test domain:S"
REFERENCES,0.35665529010238906,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.3575085324232082,(m) PACS (S).
REFERENCES,0.3583617747440273,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8"
REFERENCES,0.35921501706484643,Accuracy
REFERENCES,0.36006825938566556,"OfficeHome, Test domain:A"
REFERENCES,0.3609215017064846,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.36177474402730375,(n) OfﬁceHome (A).
REFERENCES,0.3626279863481229,"10
4
10
3
10
2
Learning rate 0.6 0.7 0.8 0.9"
REFERENCES,0.363481228668942,Accuracy
REFERENCES,0.3643344709897611,"VLCS, Test domain:L"
REFERENCES,0.3651877133105802,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.3660409556313993,(o) VLCS (L).
REFERENCES,0.36689419795221845,"10
3
10
2
Learning rate 0.4 0.6 0.8 1.0"
REFERENCES,0.3677474402730375,Accuracy
REFERENCES,0.36860068259385664,"TerraIncognita, Test domain:L46"
REFERENCES,0.36945392491467577,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.3703071672354949,(p) TerraIncognita (L46).
REFERENCES,0.371160409556314,"Figure 9: Evaluating models (SWSL-ResNext101-32x8d) ﬁne-tuned by different learning rates
on in-distribution and out-of-distribution data. (Top row) Scatter plot of in-distribution accuracy
(X-axis) and out-of-distribution accuracy (Y -axis). (Bottom row) Compare in-distribution accuracy
with out-of-distribution accuracy w.r.t. learning rate (X-axis). The green dashed line corresponds to
the baseline OOD accuracy, and the blue dash-dot line represents the selected model (by selecting the
model with best in-distribution accuracy)."
REFERENCES,0.3720136518771331,"Table 4: Comparing ImageNet pre-trained models with different model sizes. We evaluate both
in-distribution accuracy and out-of-distribution. (left: ID accuracy, right: OOD accuracy)."
REFERENCES,0.3728668941979522,"Models
PACS (C)
PACS (S)"
REFERENCES,0.37372013651877134,"ResNet50
0.977 ± 0.000 / 0.785 ± 0.006
0.980 ± 0.001 / 0.698 ± 0.008
ResNext50-32x4d
0.977 ± 0.001 / 0.825 ± 0.005
0.982 ± 0.001 / 0.698 ± 0.002
ResNext101-32x8d
0.981 ± 0.000 / 0.849 ± 0.005
0.988 ± 0.000 / 0.765 ± 0.013"
REFERENCES,0.37457337883959047,"Models
Ofﬁce-Home (A)
Ofﬁce-Home (C)"
REFERENCES,0.37542662116040953,"ResNet50
0.869 ± 0.001 / 0.662 ± 0.002
0.874 ± 0.002 / 0.542 ± 0.003
ResNext50-32x4d
0.874 ± 0.000 / 0.645 ± 0.003
0.887 ± 0.002 / 0.558 ± 0.003
ResNext101-32x8d
0.887 ± 0.000 / 0.717 ± 0.002
0.903 ± 0.000 / 0.624 ± 0.003"
REFERENCES,0.37627986348122866,"Models
VLCS (L)
VLCS (S)"
REFERENCES,0.3771331058020478,"ResNet50
0.898 ± 0.000 / 0.645 ± 0.003
0.885 ± 0.001 / 0.745 ± 0.003
ResNext50-32x4d
0.905 ± 0.000 / 0.646 ± 0.007
0.891 ± 0.002 / 0.733 ± 0.003
ResNext101-32x8d
0.909 ± 0.001 / 0.643 ± 0.002
0.888 ± 0.001 / 0.732 ± 0.005"
REFERENCES,0.3779863481228669,Under review as a conference paper at ICLR 2022
REFERENCES,0.378839590443686,"0.7
0.8
0.9
1.0
In-distribution Accuracy 0.4 0.6 0.8"
REFERENCES,0.3796928327645051,OOD Accuracy
REFERENCES,0.38054607508532423,"PACS, Test domain:C"
REFERENCES,0.38139931740614336,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.3822525597269625,(a) PACS (C).
REFERENCES,0.38310580204778155,"0.7
0.8
0.9
In-distribution Accuracy 0.4 0.5 0.6"
REFERENCES,0.3839590443686007,OOD Accuracy
REFERENCES,0.3848122866894198,"OfficeHome, Test domain:C"
REFERENCES,0.3856655290102389,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.386518771331058,(b) OfﬁceHome (C).
REFERENCES,0.3873720136518771,"0.84
0.86
0.88
0.90
In-distribution Accuracy 0.65 0.70 0.75 0.80"
REFERENCES,0.38822525597269625,OOD Accuracy
REFERENCES,0.3890784982935154,"VLCS, Test domain:S"
REFERENCES,0.38993174061433444,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.39078498293515357,(c) VLCS (S).
REFERENCES,0.3916382252559727,"0.92
0.93
0.94
0.95
In-distribution Accuracy 0.400 0.425 0.450 0.475 0.500 0.525"
REFERENCES,0.3924914675767918,OOD Accuracy
REFERENCES,0.39334470989761094,"TerraIncognita, Test domain:L38"
REFERENCES,0.39419795221843,"Baseline
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002"
REFERENCES,0.39505119453924914,(d) TerraIncognita (L38).
REFERENCES,0.39590443686006827,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8 1.0"
REFERENCES,0.3967576791808874,Accuracy
REFERENCES,0.39761092150170646,"PACS, Test domain:C"
REFERENCES,0.3984641638225256,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.3993174061433447,(e) PACS (C).
REFERENCES,0.40017064846416384,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8"
REFERENCES,0.40102389078498296,Accuracy
REFERENCES,0.40187713310580203,"OfficeHome, Test domain:C"
REFERENCES,0.40273037542662116,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.4035836177474403,(f) OfﬁceHome (C).
REFERENCES,0.4044368600682594,"10
4
10
3
10
2
Learning rate 0.65 0.70 0.75 0.80 0.85 0.90"
REFERENCES,0.4052901023890785,Accuracy
REFERENCES,0.4061433447098976,"VLCS, Test domain:S"
REFERENCES,0.4069965870307167,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.40784982935153585,(g) VLCS (S).
REFERENCES,0.4087030716723549,"10
3
10
2
Learning rate 0.4 0.6 0.8"
REFERENCES,0.40955631399317405,Accuracy
REFERENCES,0.4104095563139932,"TerraIncognita, Test domain:L38"
REFERENCES,0.4112627986348123,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.4121160409556314,(h) TerraIncognita (L38).
REFERENCES,0.4129692832764505,"0.75
0.80
0.85
0.90
0.95
1.00
In-distribution Accuracy 0.4 0.6 0.8"
REFERENCES,0.4138225255972696,OOD Accuracy
REFERENCES,0.41467576791808874,"PACS, Test domain:S"
REFERENCES,0.41552901023890787,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.41638225255972694,(i) PACS (S).
REFERENCES,0.41723549488054607,"0.75
0.80
0.85
0.90
In-distribution Accuracy 0.3 0.4 0.5 0.6 0.7 0.8"
REFERENCES,0.4180887372013652,OOD Accuracy
REFERENCES,0.4189419795221843,"OfficeHome, Test domain:A"
REFERENCES,0.4197952218430034,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.4206484641638225,(j) OfﬁceHome (A).
REFERENCES,0.42150170648464164,"0.86
0.88
0.90
0.92
In-distribution Accuracy 0.63 0.64 0.65 0.66 0.67"
REFERENCES,0.42235494880546076,OOD Accuracy
REFERENCES,0.4232081911262799,"VLCS, Test domain:L"
REFERENCES,0.42406143344709896,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.4249146757679181,(k) VLCS (L).
REFERENCES,0.4257679180887372,"0.94
0.95
0.96
0.97
In-distribution Accuracy 0.35 0.40 0.45"
REFERENCES,0.42662116040955633,OOD Accuracy
REFERENCES,0.4274744027303754,"TerraIncognita, Test domain:L46"
REFERENCES,0.4283276450511945,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002"
REFERENCES,0.42918088737201365,(l) TerraIncognita (L46).
REFERENCES,0.4300341296928328,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8 1.0"
REFERENCES,0.4308873720136519,Accuracy
REFERENCES,0.431740614334471,"PACS, Test domain:S"
REFERENCES,0.4325938566552901,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.4334470989761092,(m) PACS (S).
REFERENCES,0.43430034129692835,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8"
REFERENCES,0.4351535836177474,Accuracy
REFERENCES,0.43600682593856654,"OfficeHome, Test domain:A"
REFERENCES,0.43686006825938567,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.4377133105802048,(n) OfﬁceHome (A).
REFERENCES,0.43856655290102387,"10
4
10
3
10
2
Learning rate 0.7 0.8 0.9"
REFERENCES,0.439419795221843,Accuracy
REFERENCES,0.4402730375426621,"VLCS, Test domain:L"
REFERENCES,0.44112627986348124,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.44197952218430037,(o) VLCS (L).
REFERENCES,0.44283276450511944,"10
3
10
2
Learning rate 0.4 0.6 0.8 1.0"
REFERENCES,0.44368600682593856,Accuracy
REFERENCES,0.4445392491467577,"TerraIncognita, Test domain:L46"
REFERENCES,0.4453924914675768,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.4462457337883959,(p) TerraIncognita (L46).
REFERENCES,0.447098976109215,"Figure 10: Evaluating models (SWSL-ResNext50-32x4d) ﬁne-tuned by different learning rates
on in-distribution and out-of-distribution data. (Top row) Scatter plot of in-distribution accuracy
(X-axis) and out-of-distribution accuracy (Y -axis). (Bottom row) Compare in-distribution accuracy
with out-of-distribution accuracy w.r.t. learning rate (X-axis). The green dashed line corresponds to
the baseline OOD accuracy, and the blue dash-dot line represents the selected model (by selecting the
model with best in-distribution accuracy)."
REFERENCES,0.44795221843003413,"Table 5: Comparing BiTm models with different model sizes. We evaluate both in-distribution
accuracy and out-of-distribution. (left: ID accuracy, right: OOD accuracy)."
REFERENCES,0.44880546075085326,"Models
PACS (C)
PACS (S)"
REFERENCES,0.4496587030716723,"BiTm-ResNetV2-50x1
0.980 ± 0.001 / 0.793 ± 0.005
0.983 ± 0.001 / 0.618 ± 0.002
BiTm-ResNetV2-101x1
0.984 ± 0.000 / 0.831 ± 0.002
0.990 ± 0.001 / 0.669 ± 0.007
BiTm-ResNetV2-50x3
0.984 ± 0.000 / 0.805 ± 0.004
0.989 ± 0.000 / 0.704 ± 0.013"
REFERENCES,0.45051194539249145,"Models
Ofﬁce-Home (A)
Ofﬁce-Home (C)"
REFERENCES,0.4513651877133106,"BiTm-ResNetV2-50x1
0.887 ± 0.001 / 0.721 ± 0.001
0.895 ± 0.001 / 0.609 ± 0.002
BiTm-ResNetV2-101x1
0.904 ± 0.001 / 0.756 ± 0.001
0.910 ± 0.000 / 0.651 ± 0.000
BiTm-ResNetV2-50x3
0.912 ± 0.000 / 0.792 ± 0.001
0.928 ± 0.000 / 0.685 ± 0.002"
REFERENCES,0.4522184300341297,"Models
VLCS (L)
VLCS (S)"
REFERENCES,0.45307167235494883,"BiTm-ResNetV2-50x1
0.922 ± 0.000 / 0.647 ± 0.010
0.896 ± 0.000 / 0.759 ± 0.006
BiTm-ResNetV2-101x1
0.923 ± 0.002 / 0.656 ± 0.001
0.900 ± 0.000 / 0.783 ± 0.008
BiTm-ResNetV2-50x3
0.931 ± 0.001 / 0.655 ± 0.009
0.906 ± 0.001 / 0.795 ± 0.005"
REFERENCES,0.4539249146757679,Under review as a conference paper at ICLR 2022
REFERENCES,0.454778156996587,"0.982
0.983
0.984
In-distribution Accuracy 0.78 0.80 0.82 0.84 0.86"
REFERENCES,0.45563139931740615,OOD Accuracy
REFERENCES,0.4564846416382253,"PACS, Test domain:C"
REFERENCES,0.45733788395904434,"Baseline
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.45819112627986347,(a) PACS (C).
REFERENCES,0.4590443686006826,"0.82
0.84
0.86
0.88
0.90
0.92
In-distribution Accuracy 0.50 0.55 0.60 0.65"
REFERENCES,0.4598976109215017,OOD Accuracy
REFERENCES,0.46075085324232085,"OfficeHome, Test domain:C"
REFERENCES,0.4616040955631399,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.46245733788395904,(b) OfﬁceHome (C).
REFERENCES,0.46331058020477817,"0.84
0.86
0.88
0.90
In-distribution Accuracy 0.65 0.70 0.75 0.80"
REFERENCES,0.4641638225255973,OOD Accuracy
REFERENCES,0.46501706484641636,"VLCS, Test domain:S"
REFERENCES,0.4658703071672355,"Baseline
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.4667235494880546,(c) VLCS (S).
REFERENCES,0.46757679180887374,0.925 0.930 0.935 0.940 0.945 0.950
REFERENCES,0.4684300341296928,In-distribution Accuracy 0.400 0.425 0.450 0.475 0.500
REFERENCES,0.46928327645051193,OOD Accuracy
REFERENCES,0.47013651877133106,"TerraIncognita, Test domain:L38"
REFERENCES,0.4709897610921502,"Baseline
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.4718430034129693,(d) TerraIncognita (L38).
REFERENCES,0.4726962457337884,"10
4
10
3
Learning rate 0.80 0.85 0.90 0.95"
REFERENCES,0.4735494880546075,Accuracy
REFERENCES,0.47440273037542663,"PACS, Test domain:C"
REFERENCES,0.47525597269624575,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.4761092150170648,(e) PACS (C).
REFERENCES,0.47696245733788395,"10
4
10
3
10
2
Learning rate 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.4778156996587031,Accuracy
REFERENCES,0.4786689419795222,"OfficeHome, Test domain:C"
REFERENCES,0.47952218430034127,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.4803754266211604,(f) OfﬁceHome (C).
REFERENCES,0.4812286689419795,"10
4
10
3
Learning rate 0.65 0.70 0.75 0.80 0.85 0.90"
REFERENCES,0.48208191126279865,Accuracy
REFERENCES,0.48293515358361777,"VLCS, Test domain:S"
REFERENCES,0.48378839590443684,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.48464163822525597,(g) VLCS (S).
REFERENCES,0.4854948805460751,"10
4
10
3
Learning rate 0.4 0.6 0.8"
REFERENCES,0.4863481228668942,Accuracy
REFERENCES,0.4872013651877133,"TerraIncognita, Test domain:L38"
REFERENCES,0.4880546075085324,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.48890784982935154,(h) TerraIncognita (L38).
REFERENCES,0.48976109215017066,"0.975
0.980
0.985
0.990
In-distribution Accuracy 0.675 0.700 0.725 0.750 0.775"
REFERENCES,0.4906143344709898,OOD Accuracy
REFERENCES,0.49146757679180886,"PACS, Test domain:S"
REFERENCES,0.492320819112628,"Baseline
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.4931740614334471,(i) PACS (S).
REFERENCES,0.49402730375426623,"0.86
0.88
0.90
In-distribution Accuracy 0.55 0.60 0.65 0.70 0.75 0.80"
REFERENCES,0.4948805460750853,OOD Accuracy
REFERENCES,0.49573378839590443,"OfficeHome, Test domain:A"
REFERENCES,0.49658703071672355,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.4974402730375427,(j) OfﬁceHome (A).
REFERENCES,0.49829351535836175,"0.88
0.89
0.90
0.91
0.92
0.93
In-distribution Accuracy 0.64 0.65 0.66"
REFERENCES,0.4991467576791809,OOD Accuracy
REFERENCES,0.5,"VLCS, Test domain:L"
REFERENCES,0.5008532423208191,"Baseline
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.5017064846416383,(k) VLCS (L).
REFERENCES,0.5025597269624573,"0.94
0.95
0.96
In-distribution Accuracy 0.40 0.45 0.50"
REFERENCES,0.5034129692832765,OOD Accuracy
REFERENCES,0.5042662116040956,"TerraIncognita, Test domain:L46"
REFERENCES,0.5051194539249146,"Baseline
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.5059726962457338,(l) TerraIncognita (L46).
REFERENCES,0.5068259385665529,"10
4
10
3
Learning rate 0.7 0.8 0.9 1.0"
REFERENCES,0.507679180887372,Accuracy
REFERENCES,0.5085324232081911,"PACS, Test domain:S"
REFERENCES,0.5093856655290102,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.5102389078498294,(m) PACS (S).
REFERENCES,0.5110921501706485,"10
4
10
3
10
2
Learning rate 0.6 0.7 0.8 0.9"
REFERENCES,0.5119453924914675,Accuracy
REFERENCES,0.5127986348122867,"OfficeHome, Test domain:A"
REFERENCES,0.5136518771331058,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.514505119453925,(n) OfﬁceHome (A).
REFERENCES,0.515358361774744,"10
4
10
3
Learning rate 0.7 0.8 0.9"
REFERENCES,0.5162116040955631,Accuracy
REFERENCES,0.5170648464163823,"VLCS, Test domain:L"
REFERENCES,0.5179180887372014,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.5187713310580204,(o) VLCS (L).
REFERENCES,0.5196245733788396,"10
4
10
3
Learning rate 0.4 0.6 0.8"
REFERENCES,0.5204778156996587,Accuracy
REFERENCES,0.5213310580204779,"TerraIncognita, Test domain:L46"
REFERENCES,0.5221843003412969,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.523037542662116,(p) TerraIncognita (L46).
REFERENCES,0.5238907849829352,"Figure 11: Evaluating BiTm models (BiTm-ResNetV2-50x3) ﬁne-tuned by different learning rates
on in-distribution and out-of-distribution data. (Top row) Scatter plot of in-distribution accuracy
(X-axis) and out-of-distribution accuracy (Y -axis). (Bottom row) Compare in-distribution accuracy
with out-of-distribution accuracy w.r.t. learning rate (X-axis). The green dashed line corresponds to
the baseline OOD accuracy, and the blue dash-dot line represents the selected model (by selecting the
model with best in-distribution accuracy)."
REFERENCES,0.5247440273037542,"Table 6: Comparing SWSL models with different model sizes. We evaluate both in-distribution
accuracy and out-of-distribution. (left: ID accuracy, right: OOD accuracy)."
REFERENCES,0.5255972696245734,"Models
PACS (C)
PACS (S)"
REFERENCES,0.5264505119453925,"SWSL-ResNet50
0.987 ± 0.000 / 0.870 ± 0.008
0.992 ± 0.000 / 0.856 ± 0.005
SWSL-ResNext50-32x4d
0.987 ± 0.000 / 0.902 ± 0.002
0.994 ± 0.000 / 0.887 ± 0.003
SWSL-ResNext101-32x8d
0.990 ± 0.000 / 0.951 ± 0.002
0.997 ± 0.000 / 0.919 ± 0.006"
REFERENCES,0.5273037542662116,"Models
Ofﬁce-Home (A)
Ofﬁce-Home (C)"
REFERENCES,0.5281569965870307,"SWSL-ResNet50
0.886 ± 0.000 / 0.729 ± 0.003
0.908 ± 0.001 / 0.658 ± 0.002
SWSL-ResNext50-32x4d
0.898 ± 0.000 / 0.760 ± 0.001
0.914 ± 0.001 / 0.663 ± 0.001
SWSL-ResNext101-32x8d
0.908 ± 0.000 / 0.818 ± 0.001
0.936 ± 0.002 / 0.719 ± 0.002"
REFERENCES,0.5290102389078498,"Models
VLCS (L)
VLCS (S)"
REFERENCES,0.5298634812286689,"SWSL-ResNet50
0.920 ± 0.000 / 0.662 ± 0.006
0.900 ± 0.000 / 0.782 ± 0.003
SWSL-ResNext50-32x4d
0.926 ± 0.003 / 0.664 ± 0.008
0.903 ± 0.001 / 0.792 ± 0.001
SWSL-ResNext101-32x8d
0.930 ± 0.000 / 0.673 ± 0.002
0.903 ± 0.001 / 0.787 ± 0.004"
REFERENCES,0.5307167235494881,Under review as a conference paper at ICLR 2022
REFERENCES,0.5315699658703071,"0.92
0.94
0.96
0.98
In-distribution Accuracy 0.70 0.75 0.80 0.85"
REFERENCES,0.5324232081911263,OOD Accuracy
REFERENCES,0.5332764505119454,"PACS, Test domain:C"
REFERENCES,0.5341296928327645,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.5349829351535836,(a) PACS (C).
REFERENCES,0.5358361774744027,"0.80
0.82
0.84
0.86
0.88
0.90
In-distribution Accuracy 0.525 0.550 0.575 0.600 0.625"
REFERENCES,0.5366894197952219,OOD Accuracy
REFERENCES,0.537542662116041,"OfficeHome, Test domain:C"
REFERENCES,0.53839590443686,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002"
REFERENCES,0.5392491467576792,(b) OfﬁceHome (C).
REFERENCES,0.5401023890784983,"0.865
0.870
0.875
0.880
0.885
0.890
In-distribution Accuracy 0.68 0.70 0.72 0.74 0.76"
REFERENCES,0.5409556313993175,OOD Accuracy
REFERENCES,0.5418088737201365,"VLCS, Test domain:S"
REFERENCES,0.5426621160409556,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002"
REFERENCES,0.5435153583617748,(c) VLCS (S).
REFERENCES,0.5443686006825939,"0.92
0.93
0.94
0.95
In-distribution Accuracy 0.38 0.40 0.42 0.44"
REFERENCES,0.5452218430034129,OOD Accuracy
REFERENCES,0.5460750853242321,"TerraIncognita, Test domain:L38"
REFERENCES,0.5469283276450512,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005"
REFERENCES,0.5477815699658704,(d) TerraIncognita (L38).
REFERENCES,0.5486348122866894,"10
4
10
3
10
2
Learning rate 0.7 0.8 0.9"
REFERENCES,0.5494880546075085,Accuracy
REFERENCES,0.5503412969283277,"PACS, Test domain:C"
REFERENCES,0.5511945392491467,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.5520477815699659,(e) PACS (C).
REFERENCES,0.552901023890785,"10
3
10
2
Learning rate 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.5537542662116041,Accuracy
REFERENCES,0.5546075085324232,"OfficeHome, Test domain:C"
REFERENCES,0.5554607508532423,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.5563139931740614,(f) OfﬁceHome (C).
REFERENCES,0.5571672354948806,"10
3
10
2
Learning rate 0.70 0.75 0.80 0.85 0.90"
REFERENCES,0.5580204778156996,Accuracy
REFERENCES,0.5588737201365188,"VLCS, Test domain:S"
REFERENCES,0.5597269624573379,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.560580204778157,(g) VLCS (S).
REFERENCES,0.5614334470989761,"10
3
10
2
Learning rate 0.4 0.6 0.8"
REFERENCES,0.5622866894197952,Accuracy
REFERENCES,0.5631399317406144,"TerraIncognita, Test domain:L38"
REFERENCES,0.5639931740614335,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.5648464163822525,(h) TerraIncognita (L38).
REFERENCES,0.5656996587030717,"0.75
0.80
0.85
0.90
0.95
1.00
In-distribution Accuracy 0.4 0.5 0.6 0.7 0.8"
REFERENCES,0.5665529010238908,OOD Accuracy
REFERENCES,0.5674061433447098,"PACS, Test domain:S"
REFERENCES,0.568259385665529,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.5691126279863481,(i) PACS (S).
REFERENCES,0.5699658703071673,"0.80
0.82
0.84
0.86
0.88
In-distribution Accuracy 0.5 0.6 0.7"
REFERENCES,0.5708191126279863,OOD Accuracy
REFERENCES,0.5716723549488054,"OfficeHome, Test domain:A"
REFERENCES,0.5725255972696246,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005"
REFERENCES,0.5733788395904437,(j) OfﬁceHome (A).
REFERENCES,0.5742320819112628,"0.88
0.89
0.90
0.91
In-distribution Accuracy 0.64 0.65 0.66"
REFERENCES,0.5750853242320819,OOD Accuracy
REFERENCES,0.575938566552901,"VLCS, Test domain:L"
REFERENCES,0.5767918088737202,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.5776450511945392,(k) VLCS (L).
REFERENCES,0.5784982935153583,"0.92
0.93
0.94
0.95
0.96
0.97
In-distribution Accuracy 0.300 0.325 0.350 0.375 0.400 0.425"
REFERENCES,0.5793515358361775,OOD Accuracy
REFERENCES,0.5802047781569966,"TerraIncognita, Test domain:L46"
REFERENCES,0.5810580204778157,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005"
REFERENCES,0.5819112627986348,(l) TerraIncognita (L46).
REFERENCES,0.5827645051194539,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8 1.0"
REFERENCES,0.5836177474402731,Accuracy
REFERENCES,0.5844709897610921,"PACS, Test domain:S"
REFERENCES,0.5853242320819113,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.5861774744027304,(m) PACS (S).
REFERENCES,0.5870307167235495,"10
3
10
2
Learning rate 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.5878839590443686,Accuracy
REFERENCES,0.5887372013651877,"OfficeHome, Test domain:A"
REFERENCES,0.5895904436860068,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.590443686006826,(n) OfﬁceHome (A).
REFERENCES,0.591296928327645,"10
4
10
3
10
2
Learning rate 0.7 0.8 0.9"
REFERENCES,0.5921501706484642,Accuracy
REFERENCES,0.5930034129692833,"VLCS, Test domain:L"
REFERENCES,0.5938566552901023,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.5947098976109215,(o) VLCS (L).
REFERENCES,0.5955631399317406,"10
3
10
2
Learning rate 0.4 0.6 0.8 1.0"
REFERENCES,0.5964163822525598,Accuracy
REFERENCES,0.5972696245733788,"TerraIncognita, Test domain:L46"
REFERENCES,0.5981228668941979,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.5989761092150171,(p) TerraIncognita (L46).
REFERENCES,0.5998293515358362,"Figure 12: Evaluating models (ResNext101-32x8d pre-trained on ImageNet) ﬁne-tuned by different
learning rates on in-distribution and out-of-distribution data. (Top row) Scatter plot of in-distribution
accuracy (X-axis) and out-of-distribution accuracy (Y -axis). (Bottom row) Compare in-distribution
accuracy with out-of-distribution accuracy w.r.t. learning rate (X-axis). The green dashed line
corresponds to the baseline OOD accuracy, and the blue dash-dot line represents the selected model
(by selecting the model with best in-distribution accuracy)."
REFERENCES,0.6006825938566553,Under review as a conference paper at ICLR 2022
REFERENCES,0.6015358361774744,"0.75
0.80
0.85
0.90
0.95
In-distribution Accuracy 0.4 0.5 0.6 0.7 0.8"
REFERENCES,0.6023890784982935,OOD Accuracy
REFERENCES,0.6032423208191127,"PACS, Test domain:C"
REFERENCES,0.6040955631399317,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.6049488054607508,(a) PACS (C).
REFERENCES,0.60580204778157,"0.775
0.800
0.825
0.850
0.875
In-distribution Accuracy 0.46 0.48 0.50 0.52 0.54 0.56"
REFERENCES,0.606655290102389,OOD Accuracy
REFERENCES,0.6075085324232082,"OfficeHome, Test domain:C"
REFERENCES,0.6083617747440273,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005"
REFERENCES,0.6092150170648464,(b) OfﬁceHome (C).
REFERENCES,0.6100682593856656,"0.75
0.80
0.85
0.90
In-distribution Accuracy 0.50 0.55 0.60 0.65 0.70 0.75"
REFERENCES,0.6109215017064846,OOD Accuracy
REFERENCES,0.6117747440273038,"VLCS, Test domain:S"
REFERENCES,0.6126279863481229,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002"
REFERENCES,0.613481228668942,(c) VLCS (S).
REFERENCES,0.6143344709897611,"0.92
0.93
0.94
0.95
In-distribution Accuracy 0.425 0.450 0.475 0.500 0.525"
REFERENCES,0.6151877133105802,OOD Accuracy
REFERENCES,0.6160409556313993,"TerraIncognita, Test domain:L38"
REFERENCES,0.6168941979522184,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005"
REFERENCES,0.6177474402730375,(d) TerraIncognita (L38).
REFERENCES,0.6186006825938567,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8 1.0"
REFERENCES,0.6194539249146758,Accuracy
REFERENCES,0.6203071672354948,"PACS, Test domain:C"
REFERENCES,0.621160409556314,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.6220136518771331,(e) PACS (C).
REFERENCES,0.6228668941979523,"10
3
10
2
Learning rate 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.6237201365187713,Accuracy
REFERENCES,0.6245733788395904,"OfficeHome, Test domain:C"
REFERENCES,0.6254266211604096,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.6262798634812287,(f) OfﬁceHome (C).
REFERENCES,0.6271331058020477,"10
3
10
2
Learning rate 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.6279863481228669,Accuracy
REFERENCES,0.628839590443686,"VLCS, Test domain:S"
REFERENCES,0.6296928327645052,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.6305460750853242,(g) VLCS (S).
REFERENCES,0.6313993174061433,"10
3
10
2
Learning rate 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.6322525597269625,Accuracy
REFERENCES,0.6331058020477816,"TerraIncognita, Test domain:L38"
REFERENCES,0.6339590443686007,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.6348122866894198,(h) TerraIncognita (L38).
REFERENCES,0.6356655290102389,"0.850
0.875
0.900
0.925
0.950
0.975
In-distribution Accuracy 0.4 0.5 0.6 0.7 0.8"
REFERENCES,0.636518771331058,OOD Accuracy
REFERENCES,0.6373720136518771,"PACS, Test domain:S"
REFERENCES,0.6382252559726962,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.6390784982935154,(i) PACS (S).
REFERENCES,0.6399317406143344,"0.82
0.84
0.86
In-distribution Accuracy 0.50 0.55 0.60 0.65 0.70"
REFERENCES,0.6407849829351536,OOD Accuracy
REFERENCES,0.6416382252559727,"OfficeHome, Test domain:A"
REFERENCES,0.6424914675767918,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005"
REFERENCES,0.643344709897611,(j) OfﬁceHome (A).
REFERENCES,0.64419795221843,"0.885
0.890
0.895
0.900
0.905
In-distribution Accuracy 0.63 0.64 0.65 0.66"
REFERENCES,0.6450511945392492,OOD Accuracy
REFERENCES,0.6459044368600683,"VLCS, Test domain:L"
REFERENCES,0.6467576791808873,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002"
REFERENCES,0.6476109215017065,(k) VLCS (L).
REFERENCES,0.6484641638225256,"0.94
0.95
0.96
0.97
In-distribution Accuracy 0.36 0.38 0.40 0.42 0.44 0.46"
REFERENCES,0.6493174061433447,OOD Accuracy
REFERENCES,0.6501706484641638,"TerraIncognita, Test domain:L46"
REFERENCES,0.6510238907849829,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005"
REFERENCES,0.6518771331058021,(l) TerraIncognita (L46).
REFERENCES,0.6527303754266212,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8 1.0"
REFERENCES,0.6535836177474402,Accuracy
REFERENCES,0.6544368600682594,"PACS, Test domain:S"
REFERENCES,0.6552901023890785,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.6561433447098977,(m) PACS (S).
REFERENCES,0.6569965870307167,"10
3
10
2
Learning rate 0.5 0.6 0.7 0.8"
REFERENCES,0.6578498293515358,Accuracy
REFERENCES,0.658703071672355,"OfficeHome, Test domain:A"
REFERENCES,0.659556313993174,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.6604095563139932,(n) OfﬁceHome (A).
REFERENCES,0.6612627986348123,"10
3
10
2
Learning rate 0.7 0.8 0.9"
REFERENCES,0.6621160409556314,Accuracy
REFERENCES,0.6629692832764505,"VLCS, Test domain:L"
REFERENCES,0.6638225255972696,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.6646757679180887,(o) VLCS (L).
REFERENCES,0.6655290102389079,"10
3
10
2
Learning rate 0.4 0.6 0.8 1.0"
REFERENCES,0.6663822525597269,Accuracy
REFERENCES,0.6672354948805461,"TerraIncognita, Test domain:L46"
REFERENCES,0.6680887372013652,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.6689419795221843,(p) TerraIncognita (L46).
REFERENCES,0.6697952218430034,"Figure 13: Evaluating models (ResNext50-32x4d pre-trained on ImageNet) ﬁne-tuned by different
learning rates on in-distribution and out-of-distribution data. (Top row) Scatter plot of in-distribution
accuracy (X-axis) and out-of-distribution accuracy (Y -axis). (Bottom row) Compare in-distribution
accuracy with out-of-distribution accuracy w.r.t. learning rate (X-axis). The green dashed line
corresponds to the baseline OOD accuracy, and the blue dash-dot line represents the selected model
(by selecting the model with best in-distribution accuracy)."
REFERENCES,0.6706484641638225,"10
4
10
3
10
2
10
1
100
In-distribution Training Loss 0.6 0.7 0.8 0.9"
REFERENCES,0.6715017064846417,OOD Accuracy
REFERENCES,0.6723549488054608,"PACS, Test domain:S"
REFERENCES,0.6732081911262798,"Baseline
LR=0.01
LR=0.005
LR=0.001"
REFERENCES,0.674061433447099,(a) PACS (S).
REFERENCES,0.6749146757679181,"10
2
10
1
100
In-distribution Training Loss 0.5 0.6 0.7 0.8"
REFERENCES,0.6757679180887372,OOD Accuracy
REFERENCES,0.6766211604095563,"OfficeHome, Test domain:A"
REFERENCES,0.6774744027303754,"Baseline
LR=0.01
LR=0.005
LR=0.001"
REFERENCES,0.6783276450511946,(b) OfﬁceHome (A).
REFERENCES,0.6791808873720137,"10
3
10
2
10
1
100
In-distribution Training Loss 0.45 0.50 0.55 0.60 0.65 0.70"
REFERENCES,0.6800341296928327,OOD Accuracy
REFERENCES,0.6808873720136519,"VLCS, Test domain:L"
REFERENCES,0.681740614334471,"Baseline
LR=0.01
LR=0.005
LR=0.001"
REFERENCES,0.6825938566552902,(c) VLCS (L).
REFERENCES,0.6834470989761092,"Figure 14: OOD accuracy of models (SWSL-ResNext101-32x4d) during training. We visualize
models trained with three different learning rates in terms of OOD accuracy v.s. training loss. Each
point in the above plots represents the model evaluated at one iteration during training."
REFERENCES,0.6843003412969283,Under review as a conference paper at ICLR 2022
REFERENCES,0.6851535836177475,"0.8
0.9
1.0
In-distribution Accuracy 0.4 0.6 0.8"
REFERENCES,0.6860068259385665,OOD Accuracy
REFERENCES,0.6868600682593856,"PACS, Test domain:S"
REFERENCES,0.6877133105802048,"Baseline
resnext101_32x8d
resnetv2_50x3_bitm
resnext101_32x8d_swsl"
REFERENCES,0.6885665529010239,(a) PACS (S).
REFERENCES,0.689419795221843,"0.70
0.75
0.80
0.85
0.90
In-distribution Accuracy 0.4 0.6 0.8"
REFERENCES,0.6902730375426621,OOD Accuracy
REFERENCES,0.6911262798634812,"OfficeHome, Test domain:A"
REFERENCES,0.6919795221843004,"Baseline
resnext101_32x8d
resnetv2_50x3_bitm
resnext101_32x8d_swsl"
REFERENCES,0.6928327645051194,(b) OfﬁceHome (A).
REFERENCES,0.6936860068259386,"0.75
0.80
0.85
0.90
In-distribution Accuracy 0.575 0.600 0.625 0.650 0.675"
REFERENCES,0.6945392491467577,OOD Accuracy
REFERENCES,0.6953924914675768,"VLCS, Test domain:L"
REFERENCES,0.6962457337883959,"Baseline
resnext101_32x8d
resnetv2_50x3_bitm
resnext101_32x8d_swsl"
REFERENCES,0.697098976109215,(c) VLCS (L).
REFERENCES,0.6979522184300341,"Figure 15: Evaluating out-of-distribution and in-distribution performance of models pre-trained on
different datasets. Each color corresponds to models pre-trained on one dataset and the dash-dot line
represents the selected model."
REFERENCES,0.6988054607508533,"0.85
0.90
0.95
1.00
In-distribution Accuracy 0.2 0.4 0.6 0.8"
REFERENCES,0.6996587030716723,OOD Accuracy
REFERENCES,0.7005119453924915,"PACS, Test domain:S"
REFERENCES,0.7013651877133106,"Baseline
vit
resnetv2_bitm"
REFERENCES,0.7022184300341296,(a) PACS (S).
REFERENCES,0.7030716723549488,"0.75
0.80
0.85
0.90
In-distribution Accuracy 0.4 0.5 0.6 0.7 0.8"
REFERENCES,0.7039249146757679,OOD Accuracy
REFERENCES,0.7047781569965871,"OfficeHome, Test domain:A"
REFERENCES,0.7056313993174061,"Baseline
vit
resnetv2_bitm"
REFERENCES,0.7064846416382252,(b) OfﬁceHome (A).
REFERENCES,0.7073378839590444,"0.80
0.85
0.90
In-distribution Accuracy 0.60 0.62 0.64 0.66"
REFERENCES,0.7081911262798635,OOD Accuracy
REFERENCES,0.7090443686006825,"VLCS, Test domain:L"
REFERENCES,0.7098976109215017,"Baseline
vit
resnetv2_bitm"
REFERENCES,0.7107508532423208,(c) VLCS (L).
REFERENCES,0.71160409556314,"0.91
0.92
0.93
0.94
0.95
In-distribution Accuracy 0.2 0.3 0.4 0.5"
REFERENCES,0.712457337883959,OOD Accuracy
REFERENCES,0.7133105802047781,"TerraIncognita, Test domain:L38"
REFERENCES,0.7141638225255973,"Baseline
vit
resnetv2_bitm"
REFERENCES,0.7150170648464164,(d) TerraIncognita (L38).
REFERENCES,0.7158703071672355,"0.92
0.93
0.94
0.95
0.96
0.97
In-distribution Accuracy 0.3 0.4 0.5"
REFERENCES,0.7167235494880546,OOD Accuracy
REFERENCES,0.7175767918088737,"TerraIncognita, Test domain:L46"
REFERENCES,0.7184300341296929,"Baseline
vit
resnetv2_bitm"
REFERENCES,0.7192832764505119,(e) TerraIncognita (L46).
REFERENCES,0.7201365187713311,"Figure 16: A comparison of four ViT models and three BiTm models on out-of-distribution accuracy
and in-distribution accuracy. The orange squares represent ViT models and the blue circles represent
BiTm models. The dash-dot lines represent the selected models. We do not distinguish the model
architectures within the same model class."
REFERENCES,0.7209897610921502,"0
20
40
60
80
100
Training Data Size (%) 0.375 0.400 0.425 0.450 0.475"
REFERENCES,0.7218430034129693,OOD Accuracy
REFERENCES,0.7226962457337884,"TerraIncognita, Test domain:L46"
REFERENCES,0.7235494880546075,"Baseline
swsl_resnext50_32x4d
swsl_resnext101_32x8d"
REFERENCES,0.7244027303754266,(a) Terra-OOD (L46).
REFERENCES,0.7252559726962458,"0
20
40
60
80
100
Training Data 0.85 0.90 0.95"
REFERENCES,0.7261092150170648,Accuracy
REFERENCES,0.726962457337884,"TerraIncognita, Test domain:L46"
REFERENCES,0.7278156996587031,"swsl_resnext50_32x4d
swsl_resnext101_32x8d"
REFERENCES,0.7286689419795221,(b) Terra-ID (L46).
REFERENCES,0.7295221843003413,"Figure 17: Evaluating OOD generalization performance of models trained with different number of
training samples on the TerraIncognita dataset. X-axis represents the number of training samples.
We use SWSL-ResNext50-32x4d and SWSL-ResNext101-32x8d as the pre-trained models. For each
pre-trained model, we visualize the OOD accuracies of the top-3 models selected by ID accuracy."
REFERENCES,0.7303754266211604,Under review as a conference paper at ICLR 2022
REFERENCES,0.7312286689419796,"Table 7: Comparing ViTs models with different model sizes. We evaluate both in-distribution
accuracy and out-of-distribution. (left: ID accuracy, right: OOD accuracy)."
REFERENCES,0.7320819112627986,"Models
PACS (C)
PACS (S)"
REFERENCES,0.7329351535836177,"ViT-small-patch32
0.951 ± 0.001 / 0.746 ± 0.001
0.964 ± 0.001 / 0.404 ± 0.005
ViT-small-patch16
0.982 ± 0.000 / 0.825 ± 0.005
0.985 ± 0.000 / 0.494 ± 0.004
ViT-base-patch32
0.971 ± 0.000 / 0.789 ± 0.018
0.982 ± 0.000 / 0.478 ± 0.013
ViT-base-patch16
0.985 ± 0.000 / 0.832 ± 0.001
0.992 ± 0.000 / 0.530 ± 0.023"
REFERENCES,0.7337883959044369,"Models
Ofﬁce-Home (A)
Ofﬁce-Home (C)"
REFERENCES,0.734641638225256,"ViT-small-patch32
0.856 ± 0.001 / 0.618 ± 0.001
0.867 ± 0.000 / 0.506 ± 0.001
ViT-small-patch16
0.888 ± 0.001 / 0.772 ± 0.001
0.907 ± 0.001 / 0.584 ± 0.003
ViT-base-patch32
0.895 ± 0.001 / 0.758 ± 0.004
0.909 ± 0.001 / 0.596 ± 0.001
ViT-base-patch16
0.907 ± 0.000 / 0.796 ± 0.001
0.927 ± 0.001 / 0.647 ± 0.001"
REFERENCES,0.735494880546075,"Models
VLCS (L)
VLCS (S)"
REFERENCES,0.7363481228668942,"ViT-small-patch32
0.897 ± 0.001 / 0.628 ± 0.001
0.883 ± 0.003 / 0.721 ± 0.004
ViT-small-patch16
0.918 ± 0.001 / 0.643 ± 0.015
0.894 ± 0.001 / 0.759 ± 0.004
ViT-base-patch32
0.910 ± 0.001 / 0.631 ± 0.005
0.892 ± 0.002 / 0.733 ± 0.001
ViT-base-patch16
0.928 ± 0.001 / 0.659 ± 0.003
0.904 ± 0.000 / 0.759 ± 0.000"
REFERENCES,0.7372013651877133,"Table 8: Comparing models trained from scratch with different model sizes. We evaluate both
in-distribution accuracy and out-of-distribution. (left: ID accuracy, right: OOD accuracy)."
REFERENCES,0.7380546075085325,"Models
PACS (C)
PACS (S)"
REFERENCES,0.7389078498293515,"ResNet50
0.812 ± 0.003 / 0.470 ± 0.005
0.817 ± 0.005 / 0.294 ± 0.004
ResNext50-32x4d
0.816 ± 0.003 / 0.497 ± 0.004
0.785 ± 0.001 / 0.192 ± 0.011
ResNext101-32x8d
0.681 ± 0.004 / 0.505 ± 0.010
0.770 ± 0.002 / 0.287 ± 0.007"
REFERENCES,0.7397610921501706,"Models
Ofﬁce-Home (A)
Ofﬁce-Home (S)"
REFERENCES,0.7406143344709898,"ResNet50
0.643 ± 0.002 / 0.201 ± 0.001
0.557 ± 0.004 / 0.226 ± 0.004
ResNext50-32x4d
0.656 ± 0.000 / 0.225 ± 0.001
0.560 ± 0.002 / 0.233 ± 0.001
ResNext101-32x8d
0.654 ± 0.002 / 0.216 ± 0.001
0.575 ± 0.001 / 0.249 ± 0.001"
REFERENCES,0.7414675767918089,"Models
VLCS (L)
VLCS (S)"
REFERENCES,0.742320819112628,"ResNet50
0.758 ± 0.003 / 0.570 ± 0.012
0.747 ± 0.001 / 0.509 ± 0.000
ResNext50-32x4d
0.757 ± 0.000 / 0.567 ± 0.013
0.751 ± 0.001 / 0.513 ± 0.005
ResNext101-32x8d
0.760 ± 0.002 / 0.567 ± 0.004
0.739 ± 0.003 / 0.502 ± 0.002"
REFERENCES,0.7431740614334471,"Table 9: In-discribution accuracy evaluation of four techniques (label smoothing, AutoAugment,
PatchGaussian, and SAM) for OOD generalization. We use the same pre-trained model (SWSL-
ResNext101-32x4d) across all settings. The number inside the parentheses after the method name
represents the value of the technique-speciﬁc hyperparameter, e.g., PatchGaussian (1.0) corresponds
to employing PatchGaussian (Lopes et al., 2019) with σ = 1.0."
REFERENCES,0.7440273037542662,"Method
PACS (C)
Ofﬁce (C)
VLCS (L)
Terra (L46)"
REFERENCES,0.7448805460750854,"ERM (in Table 1)
99.1
92.1
92.4
95.9
Label Smoothing (0.1)
99.0
92.4
92.9
95.8
Label Smoothing (0.2)
99.0
91.8
93.1
95.8
AutoAugment
98.4
90.6
91.9
93.8
PatchGaussian (1.0)
99.1
92.9
92.6
95.8
PatchGaussian (0.5)
99.0
92.3
92.6
95.6
SAM (0.02)
99.2
92.9
93.3
96.1
SAM (0.05)
99.0
93.2
93.0
95.1"
REFERENCES,0.7457337883959044,Under review as a conference paper at ICLR 2022
REFERENCES,0.7465870307167235,"Table 10: Out-of-discribution accuracy evaluation of four techniques (label smoothing, AutoAugment,
PatchGaussian, and SAM) for OOD generalization. We use the same pre-trained model (SWSL-
ResNext101-32x4d) across all settings. The number inside the parentheses after the method name
represents the value of the technique-speciﬁc hyperparameter, e.g., PatchGaussian (1.0) corresponds
to employing PatchGaussian (Lopes et al., 2019) with σ = 1.0. We highlight the best two OOD
accuracies for each dataset with bold text."
REFERENCES,0.7474402730375427,"Method
PACS (S)
Ofﬁce (A)
VLCS (S)
Terra (L38)"
REFERENCES,0.7482935153583617,"ERM (in Table 1)
91.3
76.4
77.0
47.5
Label Smoothing (0.1)
91.7
78.0
78.0
48.3
Label Smoothing (0.2)
91.7
78.6
78.6
50.3
AutoAugment
91.3
78.1
77.1
46.9
PatchGaussian (1.0)
90.3
66.4
71.6
12.4
PatchGaussian (0.5)
90.0
73.5
75.9
6.2
SAM (0.02)
89.7
80.3
79.2
42.0
SAM (0.05)
90.7
80.4
79.7
43.9"
REFERENCES,0.7491467576791809,"Table 11: In-discribution accuracy evaluation of four techniques (label smoothing, AutoAugment,
PatchGaussian, and SAM) for OOD generalization. We use the same pre-trained model (SWSL-
ResNext101-32x4d) across all settings. The number inside the parentheses after the method name
represents the value of the technique-speciﬁc hyperparameter, e.g., PatchGaussian (1.0) corresponds
to employing PatchGaussian (Lopes et al., 2019) with σ = 1.0."
REFERENCES,0.75,"Method
PACS (S)
Ofﬁce (A)
VLCS (S)
Terra (L38)"
REFERENCES,0.7508532423208191,"ERM (in Table 1)
99.6
89.7
90.1
94.6
Label Smoothing (0.1)
99.5
89.8
90.6
94.8
Label Smoothing (0.2)
99.5
89.8
90.5
94.5
AutoAugment
99.6
88.6
89.9
92.2
PatchGaussian (1.0)
99.6
90.1
90.1
94.0
PatchGaussian (0.5)
99.6
90.2
90.3
94.1
SAM (0.02)
99.2
91.0
90.9
94.5
SAM (0.05)
99.7
91.2
90.9
93.3"
REFERENCES,0.7517064846416383,Under review as a conference paper at ICLR 2022
REFERENCES,0.7525597269624573,"0.94
0.96
0.98
In-distribution Accuracy 0.80 0.85 0.90"
REFERENCES,0.7534129692832765,OOD Accuracy
REFERENCES,0.7542662116040956,"PACS, Test domain:C"
REFERENCES,0.7551194539249146,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.7559726962457338,(a) PACS (C).
REFERENCES,0.7568259385665529,"0.6
0.7
0.8
0.9
In-distribution Accuracy 0.3 0.4 0.5 0.6"
REFERENCES,0.757679180887372,OOD Accuracy
REFERENCES,0.7585324232081911,"OfficeHome, Test domain:C"
REFERENCES,0.7593856655290102,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.7602389078498294,(b) OfﬁceHome (C).
REFERENCES,0.7610921501706485,"0.84
0.86
0.88
0.90
In-distribution Accuracy 0.65 0.70 0.75 0.80"
REFERENCES,0.7619453924914675,OOD Accuracy
REFERENCES,0.7627986348122867,"VLCS, Test domain:S"
REFERENCES,0.7636518771331058,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.764505119453925,(c) VLCS (S).
REFERENCES,0.765358361774744,"0.90
0.91
0.92
0.93
0.94
In-distribution Accuracy 0.400 0.425 0.450 0.475 0.500 0.525"
REFERENCES,0.7662116040955631,OOD Accuracy
REFERENCES,0.7670648464163823,"TerraIncognita, Test domain:L38"
REFERENCES,0.7679180887372014,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002"
REFERENCES,0.7687713310580204,(d) TerraIncognita (L38).
REFERENCES,0.7696245733788396,"10
4
10
3
10
2
Learning rate 0.80 0.85 0.90 0.95 1.00"
REFERENCES,0.7704778156996587,Accuracy
REFERENCES,0.7713310580204779,"PACS, Test domain:C"
REFERENCES,0.7721843003412969,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.773037542662116,(e) PACS (C).
REFERENCES,0.7738907849829352,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8"
REFERENCES,0.7747440273037542,Accuracy
REFERENCES,0.7755972696245734,"OfficeHome, Test domain:C"
REFERENCES,0.7764505119453925,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.7773037542662116,(f) OfﬁceHome (C).
REFERENCES,0.7781569965870307,"10
4
10
3
10
2
Learning rate 0.65 0.70 0.75 0.80 0.85 0.90"
REFERENCES,0.7790102389078498,Accuracy
REFERENCES,0.7798634812286689,"VLCS, Test domain:S"
REFERENCES,0.7807167235494881,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.7815699658703071,(g) VLCS (S).
REFERENCES,0.7824232081911263,"10
3
10
2
Learning rate 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.7832764505119454,Accuracy
REFERENCES,0.7841296928327645,"TerraIncognita, Test domain:L38"
REFERENCES,0.7849829351535836,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.7858361774744027,(h) TerraIncognita (L38).
REFERENCES,0.7866894197952219,"0.7
0.8
0.9
1.0
In-distribution Accuracy 0.2 0.4 0.6 0.8"
REFERENCES,0.787542662116041,OOD Accuracy
REFERENCES,0.78839590443686,"PACS, Test domain:S"
REFERENCES,0.7892491467576792,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.7901023890784983,(i) PACS (S).
REFERENCES,0.7909556313993175,"0.70
0.75
0.80
0.85
0.90
In-distribution Accuracy 0.3 0.4 0.5 0.6 0.7 0.8"
REFERENCES,0.7918088737201365,OOD Accuracy
REFERENCES,0.7926621160409556,"OfficeHome, Test domain:A"
REFERENCES,0.7935153583617748,"Baseline
lr=0.05
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.7943686006825939,(j) OfﬁceHome (A).
REFERENCES,0.7952218430034129,"0.84
0.86
0.88
0.90
0.92
In-distribution Accuracy 0.62 0.64 0.66"
REFERENCES,0.7960750853242321,OOD Accuracy
REFERENCES,0.7969283276450512,"VLCS, Test domain:L"
REFERENCES,0.7977815699658704,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002
lr=0.0001"
REFERENCES,0.7986348122866894,(k) VLCS (L).
REFERENCES,0.7994880546075085,"0.93
0.94
0.95
0.96
In-distribution Accuracy 0.35 0.40 0.45"
REFERENCES,0.8003412969283277,OOD Accuracy
REFERENCES,0.8011945392491467,"TerraIncognita, Test domain:L46"
REFERENCES,0.8020477815699659,"Baseline
lr=0.02
lr=0.01
lr=0.005
lr=0.002
lr=0.001
lr=0.0005
lr=0.0002"
REFERENCES,0.802901023890785,(l) TerraIncognita (L46).
REFERENCES,0.8037542662116041,"10
4
10
3
10
2
Learning rate 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8046075085324232,Accuracy
REFERENCES,0.8054607508532423,"PACS, Test domain:S"
REFERENCES,0.8063139931740614,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.8071672354948806,(m) PACS (S).
REFERENCES,0.8080204778156996,"10
4
10
3
10
2
Learning rate 0.4 0.6 0.8"
REFERENCES,0.8088737201365188,Accuracy
REFERENCES,0.8097269624573379,"OfficeHome, Test domain:A"
REFERENCES,0.810580204778157,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.8114334470989761,(n) OfﬁceHome (A).
REFERENCES,0.8122866894197952,"10
4
10
3
10
2
Learning rate 0.6 0.7 0.8 0.9"
REFERENCES,0.8131399317406144,Accuracy
REFERENCES,0.8139931740614335,"VLCS, Test domain:L"
REFERENCES,0.8148464163822525,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.8156996587030717,(o) VLCS (L).
REFERENCES,0.8165529010238908,"10
3
10
2
Learning rate 0.4 0.6 0.8"
REFERENCES,0.8174061433447098,Accuracy
REFERENCES,0.818259385665529,"TerraIncognita, Test domain:L46"
REFERENCES,0.8191126279863481,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.8199658703071673,(p) TerraIncognita (L46).
REFERENCES,0.8208191126279863,"Figure 18: Evaluating models (SWSL-ResNext50-32x4d) ﬁne-tuned by different learning rates (with
stage-wise learning rate decay) on in-distribution and out-of-distribution data. (Top row) Scatter
plot of in-distribution accuracy (X-axis) and out-of-distribution accuracy (Y -axis). (Bottom row)
Compare in-distribution accuracy with out-of-distribution accuracy w.r.t. learning rate (X-axis). The
green dashed line corresponds to the baseline OOD accuracy, and the blue dash-dot line represents
the selected model (by selecting the model with best in-distribution accuracy)."
REFERENCES,0.8216723549488054,Under review as a conference paper at ICLR 2022
REFERENCES,0.8225255972696246,"0.6
0.8
1.0
In-distribution Accuracy 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8233788395904437,OOD Accuracy
REFERENCES,0.8242320819112628,"PACS, Test domain:C"
REFERENCES,0.8250853242320819,"Random Init
SWSL Pre-train"
REFERENCES,0.825938566552901,(a) PACS (C).
REFERENCES,0.8267918088737202,"0.5
0.6
0.7
0.8
0.9
1.0
In-distribution Accuracy 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.8276450511945392,OOD Accuracy
REFERENCES,0.8284982935153583,"PACS, Test domain:S"
REFERENCES,0.8293515358361775,"Random Init
SWSL Pre-train"
REFERENCES,0.8302047781569966,(b) PACS (C).
REFERENCES,0.8310580204778157,"0.4
0.6
0.8
In-distribution Accuracy 0.2 0.4 0.6 0.8"
REFERENCES,0.8319112627986348,OOD Accuracy
REFERENCES,0.8327645051194539,"OfficeHome, Test domain:A"
REFERENCES,0.8336177474402731,"Random Init
SWSL Pre-train"
REFERENCES,0.8344709897610921,(c) OfﬁceHome (A).
REFERENCES,0.8353242320819113,"0.4
0.6
0.8
In-distribution Accuracy 0.2 0.4 0.6"
REFERENCES,0.8361774744027304,OOD Accuracy
REFERENCES,0.8370307167235495,"OfficeHome, Test domain:C"
REFERENCES,0.8378839590443686,"Random Init
SWSL Pre-train"
REFERENCES,0.8387372013651877,(d) OfﬁceHome (C).
REFERENCES,0.8395904436860068,"0.5
0.6
0.7
0.8
0.9
In-distribution Accuracy 0.45 0.50 0.55 0.60 0.65 0.70"
REFERENCES,0.840443686006826,OOD Accuracy
REFERENCES,0.841296928327645,"VLCS, Test domain:L"
REFERENCES,0.8421501706484642,"Random Init
SWSL Pre-train"
REFERENCES,0.8430034129692833,(e) VLCS (L).
REFERENCES,0.8438566552901023,"0.5
0.6
0.7
0.8
0.9
In-distribution Accuracy 0.4 0.5 0.6 0.7 0.8"
REFERENCES,0.8447098976109215,OOD Accuracy
REFERENCES,0.8455631399317406,"VLCS, Test domain:S"
REFERENCES,0.8464163822525598,"Random Init
SWSL Pre-train"
REFERENCES,0.8472696245733788,(f) VLCS (S).
REFERENCES,0.8481228668941979,"0.5
0.6
0.7
0.8
0.9
In-distribution Accuracy 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.8489761092150171,OOD Accuracy
REFERENCES,0.8498293515358362,"TerraIncognita, Test domain:L38"
REFERENCES,0.8506825938566553,"Random Init
SWSL Pre-train"
REFERENCES,0.8515358361774744,(g) TerraIncognita (L38).
REFERENCES,0.8523890784982935,"0.5
0.6
0.7
0.8
0.9
In-distribution Accuracy 0.2 0.3 0.4 0.5"
REFERENCES,0.8532423208191127,OOD Accuracy
REFERENCES,0.8540955631399317,"TerraIncognita, Test domain:L46"
REFERENCES,0.8549488054607508,"Random Init
SWSL Pre-train"
REFERENCES,0.85580204778157,(h) TerraIncognita (L46).
REFERENCES,0.856655290102389,"Figure 19: A comparison of ResNext101-32x8d models pre-trained on IG-1B-Targeted (SWSL-
ResNext101-32x8d) v.s. ResNext101-32x8d without pre-training on out-of-distribution accuracy and
in-distribution accuracy. The orange squares represent ResNext101-32x8d without pre-training and
the blue circles represent models pre-trained on IG-1B-Targeted. We evaluate the models on both ID
and OOD data every 100 iterations, and we visualize all the ID/OOD results in the above ﬁgures."
REFERENCES,0.8575085324232082,Under review as a conference paper at ICLR 2022
REFERENCES,0.8583617747440273,"0.825
0.850
0.875
0.900
0.925
In-distribution Accuracy 0.50 0.55 0.60 0.65 0.70"
REFERENCES,0.8592150170648464,OOD Accuracy
REFERENCES,0.8600682593856656,"PACS, Test domain:C"
REFERENCES,0.8609215017064846,"1/8 classes
1/8 samples"
REFERENCES,0.8617747440273038,(a) PACS (C).
REFERENCES,0.8626279863481229,"0.80
0.85
0.90
0.95
In-distribution Accuracy 0.2 0.3 0.4 0.5 0.6"
REFERENCES,0.863481228668942,OOD Accuracy
REFERENCES,0.8643344709897611,"PACS, Test domain:S"
REFERENCES,0.8651877133105802,"1/8 classes
1/8 samples"
REFERENCES,0.8660409556313993,(b) PACS (C).
REFERENCES,0.8668941979522184,"0.70
0.72
0.74
0.76
0.78
0.80
In-distribution Accuracy 0.38 0.40 0.42 0.44"
REFERENCES,0.8677474402730375,OOD Accuracy
REFERENCES,0.8686006825938567,"OfficeHome, Test domain:A"
REFERENCES,0.8694539249146758,"1/8 classes
1/8 samples"
REFERENCES,0.8703071672354948,(c) OfﬁceHome (A).
REFERENCES,0.871160409556314,"0.70
0.72
0.74
0.76
In-distribution Accuracy 0.32 0.34 0.36 0.38 0.40"
REFERENCES,0.8720136518771331,OOD Accuracy
REFERENCES,0.8728668941979523,"OfficeHome, Test domain:C"
REFERENCES,0.8737201365187713,"1/8 classes
1/8 samples"
REFERENCES,0.8745733788395904,(d) OfﬁceHome (C).
REFERENCES,0.8754266211604096,"0.84
0.85
0.86
In-distribution Accuracy 0.60 0.62 0.64"
REFERENCES,0.8762798634812287,OOD Accuracy
REFERENCES,0.8771331058020477,"VLCS, Test domain:L"
REFERENCES,0.8779863481228669,"1/8 classes
1/8 samples"
REFERENCES,0.878839590443686,(e) VLCS (L).
REFERENCES,0.8796928327645052,"0.83
0.84
0.85
In-distribution Accuracy 0.61 0.62 0.63 0.64 0.65 0.66"
REFERENCES,0.8805460750853242,OOD Accuracy
REFERENCES,0.8813993174061433,"VLCS, Test domain:S"
REFERENCES,0.8822525597269625,"1/8 classes
1/8 samples"
REFERENCES,0.8831058020477816,(f) VLCS (S).
REFERENCES,0.8839590443686007,"0.80
0.85
0.90
0.95
In-distribution Accuracy 0.2 0.3 0.4"
REFERENCES,0.8848122866894198,OOD Accuracy
REFERENCES,0.8856655290102389,"TerraIncognita, Test domain:L38"
REFERENCES,0.886518771331058,"1/8 classes
1/8 samples"
REFERENCES,0.8873720136518771,(g) TerraIncognita (L38).
REFERENCES,0.8882252559726962,"0.80
0.85
0.90
0.95
In-distribution Accuracy 0.250 0.275 0.300 0.325 0.350"
REFERENCES,0.8890784982935154,OOD Accuracy
REFERENCES,0.8899317406143344,"TerraIncognita, Test domain:L46"
REFERENCES,0.8907849829351536,"1/8 classes
1/8 samples"
REFERENCES,0.8916382252559727,(h) TerraIncognita (L46).
REFERENCES,0.8924914675767918,"Figure 20: A comparison of ResNet50 pre-trained on 1/8 classes of the ImageNet-1k v.s. ResNet50
pre-trained on 1/8 training samples of the ImageNet-1k on out-of-distribution accuracy and in-
distribution accuracy. The orange squares represent models pre-trained on 1/8 classes and the blue
circles represent models pre-trained on 1/8 training samples."
REFERENCES,0.893344709897611,Under review as a conference paper at ICLR 2022
REFERENCES,0.89419795221843,"10
5
10
4
10
3
10
2
Distance to initial pre-trained weights 0.4 0.6 0.8 1.0"
REFERENCES,0.8950511945392492,Accuracy
REFERENCES,0.8959044368600683,"PACS, Test domain:C"
REFERENCES,0.8967576791808873,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.8976109215017065,(a) PACS (C).
REFERENCES,0.8984641638225256,"10
5
10
4
10
3
10
2
Distance to initial pre-trained weights 0.4 0.6 0.8 1.0"
REFERENCES,0.8993174061433447,Accuracy
REFERENCES,0.9001706484641638,"PACS, Test domain:S"
REFERENCES,0.9010238907849829,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.9018771331058021,(b) PACS (C).
REFERENCES,0.9027303754266212,"10
4
10
3
10
2
Distance to initial pre-trained weights 0.4 0.6 0.8"
REFERENCES,0.9035836177474402,Accuracy
REFERENCES,0.9044368600682594,"OfficeHome, Test domain:A"
REFERENCES,0.9052901023890785,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.9061433447098977,(c) OfﬁceHome (A).
REFERENCES,0.9069965870307167,"10
4
10
3
10
2
Distance to initial pre-trained weights 0.4 0.6 0.8"
REFERENCES,0.9078498293515358,Accuracy
REFERENCES,0.908703071672355,"OfficeHome, Test domain:C"
REFERENCES,0.909556313993174,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.9104095563139932,(d) OfﬁceHome (C).
REFERENCES,0.9112627986348123,"10
5
10
4
10
3
Distance to initial pre-trained weights 0.7 0.8 0.9"
REFERENCES,0.9121160409556314,Accuracy
REFERENCES,0.9129692832764505,"VLCS, Test domain:L"
REFERENCES,0.9138225255972696,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.9146757679180887,(e) VLCS (L).
REFERENCES,0.9155290102389079,"10
5
10
4
10
3
10
2
10
1"
REFERENCES,0.9163822525597269,Distance to initial pre-trained weights 0.5 0.6 0.7 0.8 0.9
REFERENCES,0.9172354948805461,Accuracy
REFERENCES,0.9180887372013652,"VLCS, Test domain:S"
REFERENCES,0.9189419795221843,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.9197952218430034,(f) VLCS (S).
REFERENCES,0.9206484641638225,"10
4
10
3
Distance to initial pre-trained weights 0.4 0.6 0.8"
REFERENCES,0.9215017064846417,Accuracy
REFERENCES,0.9223549488054608,"TerraIncognita, Test domain:L38"
REFERENCES,0.9232081911262798,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.924061433447099,(g) TerraIncognita (L38).
REFERENCES,0.9249146757679181,"10
4
10
3
Distance to initial pre-trained weights 0.4 0.6 0.8 1.0"
REFERENCES,0.9257679180887372,Accuracy
REFERENCES,0.9266211604095563,"TerraIncognita, Test domain:L46"
REFERENCES,0.9274744027303754,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.9283276450511946,(h) TerraIncognita (L46).
REFERENCES,0.9291808873720137,"Figure 21: Evaluating models (SWSL-ResNext50-32x4d) ﬁne-tuned by different learning rates on
in-distribution and out-of-distribution data. X-axis represents the distance between the ﬁne-tuned
model weights and initial pre-trained model weights (i.e., ∥θFine-tuned −θInit∥2
2). Each point in the
scatter plot corresponds to one learning rate. The green dashed line corresponds to the baseline
OOD accuracy. The orange squares represent the in-distribution accuracy results and the blue circles
represent out-of-distribution accuracy results."
REFERENCES,0.9300341296928327,Under review as a conference paper at ICLR 2022
REFERENCES,0.9308873720136519,"0.2
0.4
0.6
0.8
1.0
In-distribution Accuracy 0.2 0.4 0.6 0.8"
REFERENCES,0.931740614334471,OOD Accuracy
REFERENCES,0.9325938566552902,"PACS, Test domain:C"
REFERENCES,0.9334470989761092,"Baseline
lr=0.05
lr=0.01
lr=0.005
lr=0.001
lr=0.0005
lr=0.0001
lr=5e-05
lr=1e-05
lr=5e-06
lr=1e-06
lr=5e-07
lr=1e-07"
REFERENCES,0.9343003412969283,(a) PACS (C).
REFERENCES,0.9351535836177475,"0.0
0.2
0.4
0.6
0.8
In-distribution Accuracy 0.0 0.2 0.4 0.6"
REFERENCES,0.9360068259385665,OOD Accuracy
REFERENCES,0.9368600682593856,"OfficeHome, Test domain:C"
REFERENCES,0.9377133105802048,"Baseline
lr=0.05
lr=0.01
lr=0.005
lr=0.001
lr=0.0005
lr=0.0001
lr=5e-05
lr=1e-05
lr=5e-06
lr=1e-06
lr=5e-07
lr=1e-07"
REFERENCES,0.9385665529010239,(b) OfﬁceHome (C).
REFERENCES,0.939419795221843,"0.4
0.6
0.8
In-distribution Accuracy 0.2 0.4 0.6 0.8"
REFERENCES,0.9402730375426621,OOD Accuracy
REFERENCES,0.9411262798634812,"VLCS, Test domain:S"
REFERENCES,0.9419795221843004,"Baseline
lr=0.05
lr=0.01
lr=0.005
lr=0.001
lr=0.0005
lr=0.0001
lr=5e-05
lr=1e-05
lr=5e-06
lr=1e-06
lr=5e-07
lr=1e-07"
REFERENCES,0.9428327645051194,(c) VLCS (S).
REFERENCES,0.9436860068259386,"0.4
0.6
0.8
In-distribution Accuracy 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.9445392491467577,OOD Accuracy
REFERENCES,0.9453924914675768,"TerraIncognita, Test domain:L38"
REFERENCES,0.9462457337883959,"Baseline
lr=0.05
lr=0.01
lr=0.005
lr=0.001
lr=0.0005
lr=0.0001
lr=5e-05
lr=1e-05
lr=5e-06
lr=1e-06
lr=5e-07
lr=1e-07"
REFERENCES,0.947098976109215,(d) TerraIncognita (L38).
REFERENCES,0.9479522184300341,"10
7
10
6
10
5
10
4
10
3
10
2
Learning rate 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9488054607508533,Accuracy
REFERENCES,0.9496587030716723,"PACS, Test domain:C"
REFERENCES,0.9505119453924915,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.9513651877133106,(e) PACS (C).
REFERENCES,0.9522184300341296,"10
7
10
6
10
5
10
4
10
3
10
2
Learning rate 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.9530716723549488,Accuracy
REFERENCES,0.9539249146757679,"OfficeHome, Test domain:C"
REFERENCES,0.9547781569965871,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.9556313993174061,(f) OfﬁceHome (C).
REFERENCES,0.9564846416382252,"10
7
10
6
10
5
10
4
10
3
10
2
Learning rate 0.2 0.4 0.6 0.8"
REFERENCES,0.9573378839590444,Accuracy
REFERENCES,0.9581911262798635,"VLCS, Test domain:S"
REFERENCES,0.9590443686006825,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.9598976109215017,(g) VLCS (S).
REFERENCES,0.9607508532423208,"10
7
10
6
10
5
10
4
10
3
10
2
Learning rate 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.96160409556314,Accuracy
REFERENCES,0.962457337883959,"TerraIncognita, Test domain:L38"
REFERENCES,0.9633105802047781,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.9641638225255973,(h) TerraIncognita (L38).
REFERENCES,0.9650170648464164,"0.2
0.4
0.6
0.8
1.0
In-distribution Accuracy 0.2 0.4 0.6 0.8"
REFERENCES,0.9658703071672355,OOD Accuracy
REFERENCES,0.9667235494880546,"PACS, Test domain:S"
REFERENCES,0.9675767918088737,"Baseline
lr=0.05
lr=0.01
lr=0.005
lr=0.001
lr=0.0005
lr=0.0001
lr=5e-05
lr=1e-05
lr=5e-06
lr=1e-06
lr=5e-07
lr=1e-07"
REFERENCES,0.9684300341296929,(i) PACS (S).
REFERENCES,0.9692832764505119,"0.0
0.2
0.4
0.6
0.8
In-distribution Accuracy 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.9701365187713311,OOD Accuracy
REFERENCES,0.9709897610921502,"OfficeHome, Test domain:A"
REFERENCES,0.9718430034129693,"Baseline
lr=0.05
lr=0.01
lr=0.005
lr=0.001
lr=0.0005
lr=0.0001
lr=5e-05
lr=1e-05
lr=5e-06
lr=1e-06
lr=5e-07
lr=1e-07"
REFERENCES,0.9726962457337884,(j) OfﬁceHome (A).
REFERENCES,0.9735494880546075,"0.2
0.4
0.6
0.8
In-distribution Accuracy 0.4 0.5 0.6"
REFERENCES,0.9744027303754266,OOD Accuracy
REFERENCES,0.9752559726962458,"VLCS, Test domain:L"
REFERENCES,0.9761092150170648,"Baseline
lr=0.05
lr=0.01
lr=0.005
lr=0.001
lr=0.0005
lr=0.0001
lr=5e-05
lr=1e-05
lr=5e-06
lr=1e-06
lr=5e-07
lr=1e-07"
REFERENCES,0.976962457337884,(k) VLCS (L).
REFERENCES,0.9778156996587031,"0.4
0.6
0.8
In-distribution Accuracy 0.2 0.3 0.4 0.5"
REFERENCES,0.9786689419795221,OOD Accuracy
REFERENCES,0.9795221843003413,"TerraIncognita, Test domain:L46"
REFERENCES,0.9803754266211604,"Baseline
lr=0.05
lr=0.01
lr=0.005
lr=0.001
lr=0.0005
lr=0.0001
lr=5e-05
lr=1e-05
lr=5e-06
lr=1e-06
lr=5e-07
lr=1e-07"
REFERENCES,0.9812286689419796,(l) TerraIncognita (L46).
REFERENCES,0.9820819112627986,"10
7
10
6
10
5
10
4
10
3
10
2
Learning rate 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9829351535836177,Accuracy
REFERENCES,0.9837883959044369,"PACS, Test domain:S"
REFERENCES,0.984641638225256,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.985494880546075,(m) PACS (S).
REFERENCES,0.9863481228668942,"10
7
10
6
10
5
10
4
10
3
10
2
Learning rate 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.9872013651877133,Accuracy
REFERENCES,0.9880546075085325,"OfficeHome, Test domain:A"
REFERENCES,0.9889078498293515,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.9897610921501706,(n) OfﬁceHome (A).
REFERENCES,0.9906143344709898,"10
7
10
6
10
5
10
4
10
3
10
2
Learning rate 0.2 0.4 0.6 0.8"
REFERENCES,0.9914675767918089,Accuracy
REFERENCES,0.992320819112628,"VLCS, Test domain:L"
REFERENCES,0.9931740614334471,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.9940273037542662,(o) VLCS (L).
REFERENCES,0.9948805460750854,"10
7
10
6
10
5
10
4
10
3
10
2
Learning rate 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9957337883959044,Accuracy
REFERENCES,0.9965870307167235,"TerraIncognita, Test domain:L46"
REFERENCES,0.9974402730375427,"Baseline
In-distribution accuracy
OOD accuracy"
REFERENCES,0.9982935153583617,(p) TerraIncognita (L46).
REFERENCES,0.9991467576791809,"Figure 22: Evaluating models (SWSL-ResNext50-32x4d) ﬁne-tuned by different learning rates (with
more learning rates, i.e., η ∈{5 × 10−2, 1 × 10−2, 5 × 10−3, 1 × 10−3, 5 × 10−4, 1 × 10−4, 5 ×
10−5, 1 × 10−5, 5 × 10−6, 1 × 10−6, 5 × 10−7, 1 × 10−7}) on in-distribution and out-of-distribution
data. (Top row) Scatter plot of in-distribution accuracy (X-axis) and out-of-distribution accuracy
(Y -axis). (Bottom row) Compare in-distribution accuracy with out-of-distribution accuracy w.r.t.
learning rate (X-axis). The green dashed line corresponds to the baseline OOD accuracy, and the
blue dash-dot line represents the selected model (by selecting the model with best in-distribution
accuracy). Here we visualize the results for all learning rates, and do not only show results for models
that achieve > 95% training accuracy."
