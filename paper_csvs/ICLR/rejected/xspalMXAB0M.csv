Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021691973969631237,"We study efﬁcient algorithms for reinforcement learning in Markov decision pro-
cesses, whose complexity is independent of the number of states. This formulation
succinctly captures large scale problems, but is also known to be computationally
hard in its general form. Previous approaches attempt to circumvent the compu-
tational hardness by assuming structure in either transition function or the value
function, or by relaxing the solution guarantee to a local optimality condition.
We consider the methodology of boosting, borrowed from supervised learning, for
converting weak learners into an effective policy. The notion of weak learning we
study is that of sampled-based approximate optimization of linear functions over
policies. Under this assumption of weak learnability, we give an efﬁcient algo-
rithm that is capable of improving the accuracy of such weak learning methods
iteratively. We prove sample complexity and running time bounds on our method,
that are polynomial in the natural parameters of the problem: approximation guar-
antee, discount factor, distribution mismatch and number of actions. In particular,
our bound does not explicitly depend on the number of states.
A technical difﬁculty in applying previous boosting results, is that the value func-
tion over policy space is not convex. We show how to use a non-convex variant of
the Frank-Wolfe method, coupled with recent advances in gradient boosting that
allow incorporating a weak learner with multiplicative approximation guarantee,
to overcome the non-convexity and attain global optimality guarantees."
INTRODUCTION,0.004338394793926247,"1
INTRODUCTION"
INTRODUCTION,0.006507592190889371,"The ﬁeld of reinforcement learning, formally modelled as learning in Markov decision processes
(MDP), models the mechanism of learning from rewards, as opposed to examples. Although the
case of tabular MDPs is well understood, the main difﬁculty in applying RL to practice is the size
of the state space."
INTRODUCTION,0.008676789587852495,"Various techniques have been suggested and applied to cope with very large MDPs. The most
common of which is function approximation of either the value or the transition function of the
underlying MDP, many times using deep neural networks. Training deep neural networks in the
supervised learning model is known to be computationally hard. Therefore reinforcement learning
with neural function approximation is also computationally hard in general, and for this reason lacks
provable guarantees."
INTRODUCTION,0.010845986984815618,"This challenge of ﬁnding efﬁcient and provable algorithms for MDPs with large state space is the
focus of our study. Previous approaches can be categorized in terms of the structural assumptions
made on the MDP to circumvent the computational hardness. Some studies focus on structured
dynamics, whereas others on structured value function or policy classes w.r.t. to the dynamics."
INTRODUCTION,0.013015184381778741,"In this paper we study another methodology to derive provable algorithms for reinforcement learn-
ing: ensemble methods for aggregating weak or approximate algorithms into substantially more
accurate solutions. Our method can be thought of as extending the methodology of boosting from
supervised learning (Schapire & Freund, 2012) to reinforcement learning. Interestingly, however,
our resulting aggregation of weak learners is not linear."
INTRODUCTION,0.015184381778741865,"In order to circumvent the computational hardness of solving general MDPs with function approx-
imation, we assumes access to a weak learner: an efﬁcient sample-based procedure that is capable"
INTRODUCTION,0.01735357917570499,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.019522776572668113,"of generating an approximate solution to any linear optimization objective over the space of poli-
cies. We describe an algorithm that iteratively calls this procedure on carefully constructed new
objectives, and aggregates the solution into a single policy. We prove that after sufﬁciently many
iterations, our resulting policy is provably near-optimal."
CHALLENGES AND TECHNIQUES,0.021691973969631236,"1.1
CHALLENGES AND TECHNIQUES"
CHALLENGES AND TECHNIQUES,0.02386117136659436,"Reinforcement learning is quite different from supervised learning and several difﬁculties have to be
circumvented for boosting to work. Amongst the challenges that the reinforcement learning setting
presents, consider the following,"
CHALLENGES AND TECHNIQUES,0.026030368763557483,"(a) The value function is not a convex or concave function of the policy. This is true even in
the tabular case, and even more so if we use a parameterized policy class."
CHALLENGES AND TECHNIQUES,0.028199566160520606,"(b) The transition matrix is unknown, or prohibitively large to manipulate for large state spaces.
This means that even evaluation of a policy cannot be exact, and can only be computed
approximately."
CHALLENGES AND TECHNIQUES,0.03036876355748373,"(c) It is unrealistic to expect a weak learner that attains near-optimal value for a given linear
objective over the policy class. At most one can hope for a multiplicative and/or additive
approximation of the overall value."
CHALLENGES AND TECHNIQUES,0.03253796095444685,"Our approach overcomes these challenges by applied several new as well as recently developed
techniques. To overcome the nonconvexity of the value function, we use a novel variant of the
Frank-Wolfe optimization algorithm that simultaneously delivers on two guarantees. First, it ﬁnds a
ﬁrst order stationary point with near-optimal rate. Secondly, if the objective happens to admit a cer-
tain gradient domination property, an important generalization of convexity, it also guarantees near
optimal value. The application of the nonconvex Frank-Wolfe method is justiﬁed due to previous
recent investigation of the policy gradient algorithm (Agarwal et al., 2019; 2020a), which identiﬁed
conditions under which the value function is gradient dominated."
CHALLENGES AND TECHNIQUES,0.03470715835140998,"The second information-theoretic challenge of the unknown transition function is overcome by care-
ful algorithmic design: our boosting algorithm requires only samples of the transitions and rewards.
These are obtained by rollouts on the MDP."
CHALLENGES AND TECHNIQUES,0.0368763557483731,"The third challenge is perhaps the most difﬁcult to overcome. Thus far, the use of the Frank-Wolfe
method in reinforcement learning did not include a multiplicative approximation, which is critical
for our application. Luckily, recent work in the area of online convex optimization (Hazan & Singh,
2021) studies boosting with a multiplicative weak learner. We make critical use of this new technique
which includes a non-linear aggregation (using a 2-layer neural network) of the weak learners. This
aspect is perhaps of general interest to boosting algorithm design, which is mostly based on linear
aggregation."
OUR CONTRIBUTIONS,0.039045553145336226,"1.2
OUR CONTRIBUTIONS"
OUR CONTRIBUTIONS,0.04121475054229935,"Our main contribution is a novel efﬁcient boosting algorithm for reinforcement learning. The input
to this algorithm is a weak learning method capable of approximately optimizing a linear function
over a certain policy class."
OUR CONTRIBUTIONS,0.04338394793926247,"The output of the algorithm is a policy which does not belong to the original class considered. It is
rather a non-linear aggregation of policies from the original class, according to a two-layer neural
network. This is a result of the two-tier structure of our algorithm: an outer loop of non-convex
Frank-Wolfe method, and an inner loop of online convex optimization boosting. The ﬁnal policy
comes with provable guarantees against the class of all possible policies."
OUR CONTRIBUTIONS,0.0455531453362256,"Our algorithm and guarantees come in four ﬂavors, depending on the mode of accessing the MDP
(two options), and the boosting methodology for the inner online convex optimization problem (two
options)."
OUR CONTRIBUTIONS,0.04772234273318872,"It is important to point out that we study the question from an optimization perspective, and hence,
assume the availability of an efﬁcient exploration scheme – either via access to a reset distribution
that has some overlap with the state distribution of the optimal policy, or constraining the policy"
OUR CONTRIBUTIONS,0.049891540130151846,Under review as a conference paper at ICLR 2022
OUR CONTRIBUTIONS,0.052060737527114966,"Supervised weak learner
Online weak learner"
OUR CONTRIBUTIONS,0.05422993492407809,"Episodic model
C6
∞(Π)/α4ε5
C4
∞(Π)/α2ε3
C∞= maxπ∈Π
 dπ∗"
OUR CONTRIBUTIONS,0.05639913232104121,"dπ

∞"
OUR CONTRIBUTIONS,0.05856832971800434,"Rollouts w. ν-resets
D6
∞/α4ε6
D4
∞/α2ε4
D∞=
 dπ∗"
OUR CONTRIBUTIONS,0.06073752711496746,"ν

∞"
OUR CONTRIBUTIONS,0.06290672451193059,"Table 1: Sample complexity of the proposed algorithms for different α-weak learning models (su-
pervised & online) and modes of accessing the MDP (rollouts & rollouts with reset distribution ν),
suppressing polynomial factors in |A|, 1/(1 −γ). See Theorem 11 for details."
OUR CONTRIBUTIONS,0.0650759219088937,"class to policies that explore sufﬁciently. Such considerations also arise when reducing reinforce-
ment learning to a sequence of supervised learning problems, e.g. Conservative Policy Iteration
(Kakade & Langford, 2002) assumes the former. One contribution we make here is to quantita-
tively differentiate between these two modes of exploration in terms of the rates of convergence they
enable for the boosting setting."
RELATED WORK,0.06724511930585683,"1.3
RELATED WORK"
RELATED WORK,0.06941431670281996,"To cope with prohibitively large MDPs, the method of choice to approximate the policy and transi-
tion space are deep neural networks, dubbed “deep reinforcement learning"". Deep RL gave rise to
beyond human performance in games such as Go, protein folding, as well as near-human level au-
tonomous driving. In terms of provable methods for deep RL, there are two main lines of work. The
ﬁrst is a robust analysis of the policy gradient algorithm (Agarwal et al., 2019; 2020a). Importantly,
the gradient domination property of the value function established in this work is needed in order to
achieve global convergence guarantees of our boosting method."
RELATED WORK,0.07158351409978309,"The other line of work for provable approaches is policy iteration, which uses a restricted policy
class, making incremental updates, such as Conservative Policy Iteration (CPI) (Kakade & Langford,
2002; Scherrer & Geist, 2014), and Policy Search by Dynamic Programming (PSDP)(Bagnell et al.,
2003)."
RELATED WORK,0.0737527114967462,"Our boosting approach for provable deep RL builds on the vast literature of boosting for supervised
learning (Schapire & Freund, 2012), and recently online learning (Leistner et al., 2009; Chen et al.,
2012; 2014; Beygelzimer et al., 2015; Jung et al., 2017; Jung & Tewari, 2018). One of the crucial
techniques important for our application is the extension of boosting to the online convex optimiza-
tion setting, with bandit information (Brukhim & Hazan, 2021), and critically with a multiplicative
weak learner (Hazan & Singh, 2021). This latter technique implies a non-linear aggregation of the
weak learners. Non-linear boosting was only recently investigated in the context of classiﬁcation
(Alon et al., 2020), where it was shown to potentially enable signiﬁcantly more efﬁcient boosting."
RELATED WORK,0.07592190889370933,"Perhaps the closest work to ours is boosting in the context of control of dynamical systems (Agar-
wal et al., 2020b). However, this work critically requires knowledge of the underlying dynamics
(transitions), which we do not, and cannot cope with a multiplicative approximate weak learner."
RELATED WORK,0.07809110629067245,"The Frank-Wolfe algorithm is extensively used in machine learning, see e.g. (Jaggi, 2013), refer-
ences therein, and recent progress in stochastic Frank-Wolfe methods (Hassani et al., 2017; Mokhtari
et al., 2018; Chen et al., 2018; Xie et al., 2019). Recent literature has applied a variant of this algo-
rithm to reinforcement learning in the context of state space exploration (Hazan et al., 2019)."
PRELIMINARIES,0.08026030368763558,"2
PRELIMINARIES"
PRELIMINARIES,0.0824295010845987,"Optimization.
We say that a differentiable function f : K 7→R over some domain K is L-smooth
with respect to some norm ∥· ∥∗if for every x, y ∈K we have"
PRELIMINARIES,0.08459869848156182,"f(y) −f(x) −∇f(x)⊤(y −x)
 ≤L"
PRELIMINARIES,0.08676789587852494,"2 ∥x −y∥2
∗."
PRELIMINARIES,0.08893709327548807,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.0911062906724512,"For constrained optimization (such as over ∆A), the projection Γ : R|A| →∆A of a point x to onto
a domain ∆A is
Γ[x] = arg min
y∈∆A
∥x −y∥."
PRELIMINARIES,0.09327548806941431,"An important generalization of convex function we use henceforth is that of gradient domination,
Deﬁnition 1 (Gradient Domination). A function f : K →R is said to be (κ, τ, K1, K2)-locally
gradient dominated (around K1 by K2) if for all x ∈K1, it holds that"
PRELIMINARIES,0.09544468546637744,"max
y∈K f(y) −f(x) ≤κ × max
y∈K2"
PRELIMINARIES,0.09761388286334056,"
∇f(x)⊤(y −x)
	
+ τ."
PRELIMINARIES,0.09978308026030369,"Markov decision process.
An inﬁnite-horizon discounted Markov Decision Process (MDP) M =
(S, A, P, r, γ, d0) is speciﬁed by: a state space S, an action space A, a transition model P where
P(s′|s, a) denotes the probability of immediately transitioning to state s′ upon taking action a at
state s, a reward function r : S × A →[0, 1] where r(s, a) is the immediate reward associated with
taking action a at state s, a discount factor γ ∈[0, 1); a starting state distribution d0 over S. For any
inﬁnite-length state-action sequence (hereafter, called a trajectory), we assign the following value"
PRELIMINARIES,0.1019522776572668,"V (τ = (s0, a0, s1, a1, . . . )) = ∞
X"
PRELIMINARIES,0.10412147505422993,"t=0
γtr(st, at)."
PRELIMINARIES,0.10629067245119306,"The agent interacts with the MDP through the choice of stochastic policy π : S →∆A it exe-
cutes, where ∆A denotes the probability simplex over A. The execution of such a policy induces a
distribution over trajectories τ = (s0, a0, . . . ) as"
PRELIMINARIES,0.10845986984815618,"P(τ|π) = d0(s0) ∞
Y"
PRELIMINARIES,0.11062906724511931,"t=0
(P(st+1|st, at)π(at|st)).
(1)"
PRELIMINARIES,0.11279826464208242,"Using this description we can associate a state V π(s) and state-action Qπ(s, a) value function with
any policy π. For an arbitrary distrbution d over S, deﬁne:"
PRELIMINARIES,0.11496746203904555,"Qπ(s) = E "" ∞
X"
PRELIMINARIES,0.11713665943600868,"t=0
γtr(st, at)
 π, s0 = s, a0 = a # ,"
PRELIMINARIES,0.1193058568329718,"V π(s) = Ea∼π(·|s) [Qπ(s, a)|π, s] ,
V π
d = Es0∼d [V π(s)|π] .
Here the expectation is with respect to the randomness of the trajectory induced by π in M. When
convenient, we shall use V π to denote V π
d0, and V ∗to denote maxπ V π."
PRELIMINARIES,0.12147505422993492,"Similarly, to any policy π, one may ascribe a (discounted) state-visitation distribution dπ = dπ
d0."
PRELIMINARIES,0.12364425162689804,"dπ
d(s) = (1 −γ) ∞
X"
PRELIMINARIES,0.12581344902386118,"t=0
γt X"
PRELIMINARIES,0.1279826464208243,"τ:st=s
P(τ|π, s0 ∼d)"
PRELIMINARIES,0.1301518438177874,"Modes of Accessing the MDP.
We henceforth consider two modes of accessing the MDP, that are
standard in the reinforcement learning literature, and provide different results for each."
PRELIMINARIES,0.13232104121475055,"The ﬁrst natural access model is called the episodic rollout setting. This mode of interaction allows
us to execute a policy, stop and restart at any point, and do this multiple times."
PRELIMINARIES,0.13449023861171366,"Another interaction model we consider is called rollout with ν-restarts. This is similar to the
episodic setting, but here the agent may draw from the MDP a trajectory seeded with an initial state
distribution ν ̸= d0. This interaction model was considered in prior work on policy optimization
Kakade & Langford (2002); Agarwal et al. (2019). The motivation for this model is two-fold:
ﬁrst, ν can be used to incorporate priors (or domain knowledge) about the state coverage of the
optimal policy; second, ν provides a mechanism to incorporate exploration into policy optimization
procedures."
PRELIMINARIES,0.13665943600867678,"3
SETTING: POLICY AGGREGATION AND WEAK LEARNING"
PRELIMINARIES,0.13882863340563992,"Our boosting algorithms henceforth call upon weak learners to generate weak policies, and aggre-
gate these policies in a way that guarantees eventual convergence to optimality. In this section we
formalize both components."
PRELIMINARIES,0.14099783080260303,"Under review as a conference paper at ICLR 2022 ¯π Γ[λ2] λ1 π1 w1 π2
π3 w′
1 Γ[λ2] λ2"
PRELIMINARIES,0.14316702819956617,"π4
π5
π6 w6 w′
2"
PRELIMINARIES,0.14533622559652928,"A Shrub
λt ∈Λ(Π, N)"
PRELIMINARIES,0.1475054229934924,"A Policy Tree
¯π ∈Π(Π, N, T)"
PRELIMINARIES,0.14967462039045554,"Figure 1: The ﬁgure illustrates a Policy Tree hierarchy (see Deﬁnition 5), obtained by setting N = 3
on the inner loop, and T = 2 on the outer loop, to overall get all base policies π1, ..., π6 ∈ΠW on
the lower level. The middle level holds T = 2 Policy Shurbs (see Deﬁnition 4), where each Shrub
λt ∈Λ(Π, N) is an aggregation of base policies. The top level is an weighted aggregation of the
projected shrubs Γ[λt] , which forms the overall Policy Tree ¯π ∈Π(Π, N, T)."
POLICY AGGREGATION,0.15184381778741865,"3.1
POLICY AGGREGATION"
POLICY AGGREGATION,0.1540130151843818,"For a base class of policies ΠW , our algorithm incrementally builds a more expressive policy class
by aggregating base policies via both linear combinations and non-linear transformations. In effect,
the algorithm produces a ﬁnite-width depth-2 circuit over some subset of the base policy class. We
start with the simpler linear aggregation."
POLICY AGGREGATION,0.1561822125813449,"Deﬁnition 2 (Function Aggregation). Given some N0 ∈Z+, w ∈RN0, (f1, . . . fN0) ∈(S →
R|A|)⊗N0, we deﬁne f = PN0
n=1 wnfn to be the unique function f : S →RA for which simultane-
ously for all s ∈S, it holds"
POLICY AGGREGATION,0.15835140997830802,"f(s) = N0
X"
POLICY AGGREGATION,0.16052060737527116,"n=1
wnf(s)."
POLICY AGGREGATION,0.16268980477223427,"Next, the projection operation below may be viewed as a non-linear activation, such as ReLU, in
deep learning terms. Note that the projection of any function from S to R|A| produces a policy, i.e.
a mapping from states to distributions over actions."
POLICY AGGREGATION,0.1648590021691974,"Deﬁnition 3 (Policy Projection). Given a function f : S →R|A|, deﬁne a projected policy π = Γ[f]
to be a policy such that simultaneously for all s ∈S, it holds that π(·|s) = Γ [f(s)] ."
POLICY AGGREGATION,0.16702819956616052,"The next deﬁnition deﬁnes the class of functions represented by circuits of depth 1 over a base
policy class. Note that these function do not necessarily represent policies since they take an afﬁne
(vs. convex) combination of policies."
POLICY AGGREGATION,0.16919739696312364,"Deﬁnition 4 (Shrub). For an arbitrary base policy class Π ⊆S →∆A, deﬁne Λ(Π, N) to be a set
such that λ ∈Λ(Π, N) if and only if there exists N0 ≤N, w ∈RN0, (π1, . . . πN0) ∈Π⊗N0 such
that λ = PN0
n=1 wnπn."
POLICY AGGREGATION,0.17136659436008678,The ﬁnal deﬁnition describes the set of possible outputs of the boosting procedure.
POLICY AGGREGATION,0.1735357917570499,"Deﬁnition 5 (Policy Tree). For an arbitrary base policy class Π ⊆S →∆A, deﬁne Π(Π, N, T) to be
a policy class such that π ∈Π(Π, N, T) if and only if there exists T0 ≤T, w ∈∆T0, (λ1, . . . λT0) ∈
Λ(Π, N)⊗T0 such that π = PT0
t=1 wtΓ[λt]."
POLICY AGGREGATION,0.175704989154013,"It is important that the policy that the boosting algorithm outputs can be evaluated efﬁciently. In the
appendix we show it is indeed the case (see Lemma 15)."
POLICY AGGREGATION,0.17787418655097614,Under review as a conference paper at ICLR 2022
MODELS OF WEAK LEARNING,0.18004338394793926,"3.2
MODELS OF WEAK LEARNING"
MODELS OF WEAK LEARNING,0.1822125813449024,"We consider two types of weak learners, and give different end results based on the different as-
sumptions: weak supervised and weak online learners. In the discussion below, let πr be a uniformly
random policy, i.e. ∀(s, a) ∈S × A, πr(a|s) = 1/|A|."
MODELS OF WEAK LEARNING,0.1843817787418655,"Supervised Learning.
The natural way to deﬁne weak learning is an algorithm whose perfor-
mance is always slight better than that of random policy, one that chooses an action uniformly at
random at any given state. However, in general no learner can outperform a random learner over
all label distributions (this is called the “no free lunch"" theorem). This motivates the literature on
agnostic boosting (Kanade & Kalai, 2009; Brukhim et al., 2020; Hazan & Singh, 2021) that deﬁnes
a weak learner as one that can approximate the best policy in a given policy class.
Deﬁnition 6 (Weak Supervised Learner). Let α ∈(0, 1). Consider a class L of linear loss functions
ℓ: RA →R, and D a family of distributions that are supported over S × L, policy classes ΠW , Π.
A weak supervised learning algorithm, for every ε, δ > 0, given m(ε, δ) samples Dm from any
distribution D ∈D outputs a policy W(Dm) ∈ΠW such that with probability 1 −δ,"
MODELS OF WEAK LEARNING,0.18655097613882862,"E(s,ℓ)∼D

ℓ(W(Dm))

≥α max
π∗∈Π E(s,ℓ)∼D

ℓ(π∗(s))

+ (1 −α)E(s,ℓ)∼D

ℓ(πr(s))

−ε."
MODELS OF WEAK LEARNING,0.18872017353579176,"Note that the weak learner outputs a policy in ΠW which is approximately competitive against
the class Π. As an additional relaxation, instead of requiring that the weak learning guarantee
holds for all distributions, in our setup, it will be sufﬁcient that the weak learning assumption holds
over natural distributions. We deﬁne these below. Hereafter, we refer to Π(ΠW , N, T) as Π for
N, T = O(poly(|A|, (1 −γ)−1, ε−1, α−1, log δ−1)) speciﬁed later.
Assumption 1 (Weak Supervised Learning). The booster has access to a weak supervised learning
oracle (Deﬁnition 6) over the policy class Π, for some α ∈(0, 1). Furthermore, the weak learning
condition holds only for a class of natural distributions D – D ∈D if and only if there exists some
π ∈Π such that"
MODELS OF WEAK LEARNING,0.19088937093275488,"DS(s) =
Z"
MODELS OF WEAK LEARNING,0.19305856832971802,"ℓ
D(s, ℓ)dµ(ℓ) = dπ(s)."
MODELS OF WEAK LEARNING,0.19522776572668113,"In particular, while a natural distribution may have arbitrary distribution over labels, its marginal
distribution over states must be realizable as the state distribution of some policy in Π over the MDP
M. Therefore, the complexity of weak learning adapts to the complexity of the MDP itself. As an
extreme example, in stochastic contextual bandits where policies do not affect the distribution of
states (say d0), it is sufﬁcient that the weak learning condition holds with respect to all couplings of
a single distribution d0."
MODELS OF WEAK LEARNING,0.19739696312364424,"Online Learning.
The second model of weak learning we consider requires a stronger assumption,
but will give us better sample and oracle complexity bounds henceforth.
Deﬁnition 7 (Weak Online Learner). Let α ∈(0, 1). Consider a class L of linear loss functions
ℓ: RA →R. A weak online learning algorithm, for every M > 0, incrementally for each timestep
computes a policy Wm ∈ΠW and then observes the state-loss pair (s, ℓt) ∈S × L such that M
X"
MODELS OF WEAK LEARNING,0.19956616052060738,"m=1
ℓm(Wm(sm)) ≥α max
π∗∈Π M
X"
MODELS OF WEAK LEARNING,0.2017353579175705,"m=1
ℓm(π∗(sm)) + (1 −α) M
X"
MODELS OF WEAK LEARNING,0.2039045553145336,"m=1
ℓm(πr(sm)) −RW(M)."
MODELS OF WEAK LEARNING,0.20607375271149675,"Assumption 2 (Weak Online Learning). The booster has access to a weak online learning oracle
(Deﬁnition 7) over the policy class Π, for some α ∈(0, 1).
Remark 8. A similar remark about natural distributions applies to the online weak learner. In par-
ticular, it is sufﬁcient the guarantee in 7 holds for arbitrary sequence of loss functions with high
probability over the sampling of the state from dπ for some π ∈Π. Although stronger than su-
pervised weak learning, this oracle can be interpreted as a relaxation of the online weak learning
oracle considered in (Brukhim et al., 2020; Brukhim & Hazan, 2021; Hazan & Singh, 2021). A sim-
ilar model of hybrid adversarial-stochastic online learning was considered in (Rakhlin et al., 2011;
Lazaric & Munos, 2009; Beygelzimer et al., 2011). In particular, it is known (Lazaric & Munos,
2009) that unlike online learning, the capacity of a hypothesis class for this model is governed by its
VC dimension (vs. Littlestone dimension)."
MODELS OF WEAK LEARNING,0.20824295010845986,Under review as a conference paper at ICLR 2022
ALGORITHM & MAIN RESULTS,0.210412147505423,"4
ALGORITHM & MAIN RESULTS"
ALGORITHM & MAIN RESULTS,0.21258134490238612,"In this section we describe our RL boosting algorithm. Here we focus on the case where a supervised
weak learning is provided. The online weak learners variant of our result is detailed in the appendix.
We next deﬁne several deﬁnitions and algorithmic subroutines required for our method."
ALGORITHM & MAIN RESULTS,0.21475054229934923,"The Extension Operator.
The extension operator (Hazan & Singh, 2021) operate overs functions
and modiﬁes their value outside and near the boundary of the convex set ∆A to aid the boosting
algorithm."
ALGORITHM & MAIN RESULTS,0.21691973969631237,"FG,β[f](x) = min
y∈∆A"
ALGORITHM & MAIN RESULTS,0.21908893709327548,"
f(y) + G min
z∈∆A ∥y −z∥+ 1"
ALGORITHM & MAIN RESULTS,0.22125813449023862,"2β ∥x −y∥2
"
ALGORITHM & MAIN RESULTS,0.22342733188720174,"To state the results, we need the following deﬁnitions. The ﬁrst generalizes the policy completeness
notion from (Scherrer & Geist, 2014). It may be seen as the policy-equivalent analogue of inherent
bellman error (Munos & Szepesvári, 2008). Intuitively, it measures the degree to which a policy in Π
can best approximate the bellman operator in an average sense with respect to the state distribution
induced by a policy from Π.
Deﬁnition 9 (Policy Completeness). For any initial state distribution µ, deﬁne"
ALGORITHM & MAIN RESULTS,0.22559652928416485,"Eµ(Π, Π) = max
π∈Π min
π∗∈Π Es∼dπ
µ"
ALGORITHM & MAIN RESULTS,0.227765726681128,"
max
a∈A Qπ(s, a) −Qπ(s, ·)⊤π∗(·|s)

."
ALGORITHM & MAIN RESULTS,0.2299349240780911,"The following notion of the distribution mismatch coefﬁcient is often useful to characterize the
exploration problem faced by policy optimization algorithms.
Deﬁnition 10 (Distribution Mismatch). Let π∗= arg maxπ V π, and ν a ﬁxed initial state distribu-
tion (see section 2). Deﬁne the following distribution mismatch coefﬁcients:1"
ALGORITHM & MAIN RESULTS,0.23210412147505424,"C∞(Π) = max
π∈Π dπ∗ dπ"
ALGORITHM & MAIN RESULTS,0.23427331887201736,"∞
,
D∞=

dπ∗ ν ∞
."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.23644251626898047,"4.1
RL BOOSTING VIA WEAK SUPERVISED LEARNING"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2386117136659436,"We give the main RL boosting algorithm, assuming supervised weak learners. We use a simple
sub-routine for choosing a step size, provided in the appendix."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.24078091106290672,Algorithm 1 RL Boosting via Weak Supervised Learning
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.24295010845986983,"1: Input parameters T, N, M, P, µ. Initialize a policy π0 ∈ΠW arbitrarily.
2: for t = 1 to T do
3:
Set ρt,0 to be an arbitrary policy in ΠW .
4:
for n = 1 to N do
5:
Execute πt−1 for M episodes with initial state distribution µ via Algorithm 2, to get"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.24511930585683298,"Dt,n = {(si, c
Qi)m
i=1}."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2472885032537961,"6:
Modify Dt,n to produce a new dataset D′
t,n = {(si, fi)}m
i=1, such that for all i ∈[m]:"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.24945770065075923,"fi = −∇FG,β[−bQi](ρt,n(·|si))"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.25162689804772237,".
7:
Let At,n be the policy chosen by the weak learning oracle when given data set D′
t,n.
8:
Update ρt,n = (1 −η2,n)ρt,n−1 + η2,n"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.25379609544468545,"α At,n −η2,n
  1"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2559652928416486,"α −1

πr.
9:
end for
10:
Declare π′
t = Γ [ρt,N].
11:
Choose η1,t = min{1, 2C∞"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.25813449023861174,"t
} if µ = d0 else η1,t = StepChooser(πt−1, π′
t, µ, P).
12:
Update πt = (1 −η1,t)πt−1 + η1,tπ′
t.
13: end for
14: Output ¯π=πT if µ = d0 else output πt−1 with the smallest ηt."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2603036876355748,"1For brevity, We use the shorthand C∞where clear from context."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.26247288503253796,Under review as a conference paper at ICLR 2022
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2646420824295011,"Theorem 11. Algorithm 1 samples T(MN + P) episodes of length ˜O(
1
1−γ ) with probability 1 −δ."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2668112798264642,"In the episodic model, for T = O

C2
∞
(1−γ)3ε

, N =

16|A|C∞
(1−γ)2αϵ
2
, M = m

(1−γ)2αε"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.26898047722342733,"C∞|A| ,
δ
NT

,µ = d0,
P = 0, with probability 1 −δ,"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.27114967462039047,"V ∗−V π ≤C∞
E(Π, Π)"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.27331887201735355,"1 −γ
+ ε."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2754880694143167,"In the ν-reset model, for T
=
8D2
∞
(1−γ)6ε2 , N
=

16|A|D∞
(1−γ)3αϵ
2
, P
=
˜O( 200|A|2D2
∞
(1−γ)6ε2 ), M
="
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.27765726681127983,"m

(1−γ)3αε"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.279826464208243,"8|A|D∞,
δ
2NT

,µ = ν, with probability 1 −δ,"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.28199566160520606,"V ∗−V π ≤D∞
Eν(Π, Π)"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2841648590021692,(1 −γ)2 + ε.
TRAJECTORY SAMPLER,0.28633405639913234,"4.2
TRAJECTORY SAMPLER"
TRAJECTORY SAMPLER,0.2885032537960954,"In Algorithm 2 we describe an episodic sampling procedure, that is used in our sample-based RL
boosting algorithms described above. For a ﬁxed initial state distribution µ, and any given policy
π, we apply the following sampling procedure: start at an initial state s0 ∼µ, and continue to
act thereafter in the MDP according to any policy π, until termination. With this process, it is
straightforward to both sample from the state visitation distribution s ∼dπ, and to obtain unbiased
samples of Qπ(s, ·); see Algorithm 2 for the detailed process."
TRAJECTORY SAMPLER,0.29067245119305857,"Algorithm 2 Trajectory Sampler: s ∼dπ, unbiased estimate of Qπ
s
1: Sample state s0 ∼µ, and action a′ ∼U(A) uniformly.
2: Sample s ∼dπ as follows: at every timestep h, with probability γ, act according to π; else,
accept sh as the sample and proceed to Step 3.
3: Take action a′ at state sh, then continue to execute π, and use a termination probability of 1−γ.
Upon termination, set R(sh, a′) as the undiscounted sum of rewards from time h onwards."
TRAJECTORY SAMPLER,0.2928416485900217,"4: Deﬁne the vector d
Qπsh, such that for all a ∈A, d
Qπsh(a) = |A| · R(sh, a′) · Ia=a′."
TRAJECTORY SAMPLER,0.2950108459869848,"5: return (sh, d
Qπsh)."
TRAJECTORY SAMPLER,0.29718004338394793,"5
ANALYSIS – PROOF SKETCH"
TRAJECTORY SAMPLER,0.2993492407809111,"We sketch the high-level ideas of the proof of our main result, stated in Theorem 11, and refer the
reader to the appendix for the formal proof. Throughout the analysis, we use the notation ∇πV π to
denote the gradient of the value function with respect to the |S| × |A|-sized representation of the
policy π, namely the functional gradient of V π."
TRAJECTORY SAMPLER,0.30151843817787416,"We establish an equivalence between the outlined algorithm and an abstraction of the Frank-Wolfe
algorithm (Algorithm D) from optimization theory. This variant of the Frank-Wolfe (FW) algo-
rithm operates over non-convex and gradient dominated functions to obtain the following novel
convergence guarantees. We establish the necessary gradient domination results from the policy
completeness results.
Theorem 12. Let f : K →R be L-smooth in some norm ∥· ∥∗, H-bounded, and the diameter of K
in ∥· ∥∗be D. Then, for a (ϵ0, K2)-linear optimization oracle, the output ¯x of Algorithm D satisﬁes"
TRAJECTORY SAMPLER,0.3036876355748373,"max
u∈K2 ∇f(¯x)⊤(u −¯x) ≤ r 2HLD2"
TRAJECTORY SAMPLER,0.30585683297180044,"T
+ 3ϵ + ϵ0."
TRAJECTORY SAMPLER,0.3080260303687636,"Furthermore, if f is (κ, τ, K1, K2)-locally gradient-dominated and x0, . . . xT ∈K1, then it holds"
TRAJECTORY SAMPLER,0.31019522776572667,"max
x∗∈K f(x∗) −f(¯x) ≤2κ2 max{LD2, H}"
TRAJECTORY SAMPLER,0.3123644251626898,"T
+ τ + κϵ0."
TRAJECTORY SAMPLER,0.31453362255965295,"The Frank-Wolfe algorithm utilizes an inner gradient optimization oracle as a subroutine. To imple-
ment this oracle using approximate optimizers, we utilize yet another variant of the FW method as
“internal-boosting” for the weak learners (by employing an adapted analysis of Theorem 13)."
TRAJECTORY SAMPLER,0.31670281995661603,Under review as a conference paper at ICLR 2022
INTERNAL-BOOSTING WEAK LEARNERS,0.3188720173535792,"5.1
INTERNAL-BOOSTING WEAK LEARNERS"
INTERNAL-BOOSTING WEAK LEARNERS,0.3210412147505423,"We utilize a variant of the Frank-Wolfe method as a form “internal-boosting” for the weak learners,
by employing an adapted analysis of previous work that is stated below."
INTERNAL-BOOSTING WEAK LEARNERS,0.3232104121475054,"Note that bQπ(s, ·) produced by Algorithm 2 satisﬁes ∥bQπ(s, ·)∥=
|A|
1−γ . We can now borrow the
following result on boosting for statistical learning from (Hazan & Singh, 2021), specializing the
decision set to be ∆A. Let Dt be the distribution induced by the trajectory sampler in round t."
INTERNAL-BOOSTING WEAK LEARNERS,0.32537960954446854,"Theorem 13 ((Hazan & Singh, 2021)). Let β =
q"
INTERNAL-BOOSTING WEAK LEARNERS,0.3275488069414317,"1
αN , and η2,n = min{ 2"
INTERNAL-BOOSTING WEAK LEARNERS,0.3297180043383948,"n, 1}. Then, for any t, π′
t
produced by Algorithm 1 satisﬁes with probability 1 −δ that"
INTERNAL-BOOSTING WEAK LEARNERS,0.3318872017353579,"max
π∈Π E(s,Q)∼Dt

Q⊤π(s)

−E(s,Q)∼Dt

Q⊤π′
t(s)

≤
2|A|
(1 −γ)α  2
√"
INTERNAL-BOOSTING WEAK LEARNERS,0.33405639913232105,"N
+ ε
"
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.3362255965292842,"5.2
FROM WEAK LEARNING TO LINEAR OPTIMIZATION"
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.3383947939262473,"In the following Lemma, we give an important observation which allows us to re-state the guarantee
in the previous subsection in terms of linear optimization over functional gradients.
Lemma 14. Applying Algorithm 2 for any given policy π, yields an unbiased estimate of the gradi-
ent, such that for any π′,"
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.3405639913232104,"(∇πV π
µ )⊤π′ =
1
1 −γ E(s, c
Qπ(s,·))∼D
h
c
Qπ(s, ·)⊤π′(·|s)
i
,
(2)"
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.34273318872017355,"where π′(·|s) ∈∆A, and D is the distribution induced on the outputs of Algorithm 2, for a given
policy π and initial state distribution µ."
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.34490238611713664,"Proof. Recall ∇πV π denotes the gradient with respect to the |S| × |A|-sized representation of the
policy π – the functional gradient. Then, using the policy gradient theorem (Williams, 1992; Sutton
et al., 2000), it is given by,
∂V π
µ
∂π(a|s) =
1
1 −γ dπ
µ(s)Qπ(s, a).
(3)"
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.3470715835140998,"The following sources of randomness are at play in the sampling algorithm (Algorithm 2): the
distribution dπ (which encompasses the discount-factor-based random termination, the transition
probability, and the stochasticity of π), and the uniform sampling over A. For a ﬁxed s, π, denote by
Qπ
s as the distribution over c
Qπ(s, ·) ∈RA, induced by all the aforementioned randomness sources.
To conclude the claim, observe that by construction"
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.3492407809110629,"EQπ(s,·)[ c
Qπ(s, ·)|π, s] = Qπ(s, ·).
(4)"
CONCLUSIONS,0.351409978308026,"6
CONCLUSIONS"
CONCLUSIONS,0.35357917570498915,"Building on recent advances in boosting for online convex optimization and bandits, we have de-
scribed a boosting algorithm for reinforcement learning over large state spaces with provable guaran-
tees. We see this as a ﬁrst attempt at using a tried-and-tested methodology from supervised learning
in RL, and many challenges remain."
CONCLUSIONS,0.3557483731019523,"First and foremost, our notion of weak learner optimizes a linear function over policy space. A more
natural weak learner would be an RL agent with multiplicative optimality guarantee, and it would
be interesting to extend our methodology to this notion of weak learnability."
CONCLUSIONS,0.3579175704989154,"Another important aspect that is not discussed in our paper is that of state-space exploration. Poten-
tially boosting can be combined with state-space exploration techniques to give stronger guarantees
independent of distribution mismatch C∞, D∞factors."
CONCLUSIONS,0.3600867678958785,"Finally, a feature of our method is that it produces nonlinear aggregations of weak learners as per a
two layer neural network. Are simpler aggregations with provable guarantees possible?"
CONCLUSIONS,0.36225596529284165,Under review as a conference paper at ICLR 2022
REFERENCES,0.3644251626898048,REFERENCES
REFERENCES,0.3665943600867679,"Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradi-
ent methods: Optimality, approximation, and distribution shift. arXiv preprint arXiv:1908.00261,
2019."
REFERENCES,0.368763557483731,"Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed explo-
ration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020a."
REFERENCES,0.37093275488069416,"Naman Agarwal, Nataly Brukhim, Elad Hazan, and Zhou Lu. Boosting for control of dynamical
systems. In International Conference on Machine Learning, pp. 96–103. PMLR, 2020b."
REFERENCES,0.37310195227765725,"Noga Alon, Alon Gonen, Elad Hazan, and Shay Moran. Boosting simple learners. arXiv preprint
arXiv:2001.11704, 2020."
REFERENCES,0.3752711496746204,"J Andrew Bagnell, Sham Kakade, Andrew Y Ng, and Jeff G Schneider. Policy search by dynamic
programming. In Advances in Neural Information Processing Systems, 2003."
REFERENCES,0.3774403470715835,"Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit
algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International
Conference on Artiﬁcial Intelligence and Statistics, pp. 19–26. JMLR Workshop and Conference
Proceedings, 2011."
REFERENCES,0.3796095444685466,"Alina Beygelzimer, Satyen Kale, and Haipeng Luo. Optimal and adaptive algorithms for online
boosting. In International Conference on Machine Learning, pp. 2323–2331, 2015."
REFERENCES,0.38177874186550975,"Nataly Brukhim and Elad Hazan. Online boosting with bandit feedback. In Algorithmic Learning
Theory, pp. 397–420. PMLR, 2021."
REFERENCES,0.3839479392624729,"Nataly Brukhim, Xinyi Chen, Elad Hazan, and Shay Moran. Online agnostic boosting via regret
minimization. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.38611713665943603,"Lin Chen, Christopher Harshaw, Hamed Hassani, and Amin Karbasi. Projection-free online opti-
mization with stochastic gradient: From convexity to submodularity. In International Conference
on Machine Learning, pp. 814–823, 2018."
REFERENCES,0.3882863340563991,"Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu. An online boosting algorithm with theoretical
justiﬁcations. In Proceedings of the 29th International Coference on International Conference on
Machine Learning, pp. 1873–1880, 2012."
REFERENCES,0.39045553145336226,"Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu. Boosting with online binary learners for the
multiclass bandit problem. In International Conference on Machine Learning, pp. 342–350, 2014."
REFERENCES,0.3926247288503254,"John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efﬁcient projections onto
the l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference
on Machine learning, pp. 272–279, 2008."
REFERENCES,0.3947939262472885,"Hamed Hassani, Mahdi Soltanolkotabi, and Amin Karbasi. Gradient methods for submodular max-
imization. In Advances in Neural Information Processing Systems, pp. 5841–5851, 2017."
REFERENCES,0.3969631236442516,"Elad Hazan. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019."
REFERENCES,0.39913232104121477,"Elad Hazan and Karan Singh.
Boosting for online convex optimization.
arXiv preprint
arXiv:2102.09305, 2021."
REFERENCES,0.40130151843817785,"Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efﬁcient maximum entropy
exploration. In International Conference on Machine Learning, pp. 2681–2691. PMLR, 2019."
REFERENCES,0.403470715835141,"Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International
Conference on Machine Learning, pp. 427–435. PMLR, 2013."
REFERENCES,0.40563991323210413,"Young Hun Jung and Ambuj Tewari. Online boosting algorithms for multi-label ranking. In Inter-
national Conference on Artiﬁcial Intelligence and Statistics, pp. 279–287, 2018."
REFERENCES,0.4078091106290672,Under review as a conference paper at ICLR 2022
REFERENCES,0.40997830802603036,"Young Hun Jung, Jack Goetz, and Ambuj Tewari. Online multiclass boosting. In Advances in neural
information processing systems, pp. 919–928, 2017."
REFERENCES,0.4121475054229935,"Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
In Proc. 19th International Conference on Machine Learning. Citeseer, 2002."
REFERENCES,0.41431670281995664,"Varun Kanade and Adam Kalai. Potential-based agnostic boosting. In Advances in neural informa-
tion processing systems, pp. 880–888, 2009."
REFERENCES,0.4164859002169197,"Alessandro Lazaric and Rémi Munos. Hybrid stochastic-adversarial on-line learning. In Conference
on Learning Theory, 2009."
REFERENCES,0.41865509761388287,"Christian Leistner, Amir Saffari, Peter M Roth, and Horst Bischof.
On robustness of on-line
boosting-a competitive study. In IEEE 12th International Conference on Computer Vision Work-
shops, ICCV Workshops, pp. 1362–1369. IEEE, 2009."
REFERENCES,0.420824295010846,"Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Stochastic conditional gradient methods:
From convex minimization to submodular maximization. arXiv preprint arXiv:1804.09554, 2018."
REFERENCES,0.4229934924078091,"Rémi Munos and Csaba Szepesvári. Finite-time bounds for ﬁtted value iteration. Journal of Machine
Learning Research, 9(5), 2008."
REFERENCES,0.42516268980477223,"Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Stochastic and con-
strained adversaries. arXiv preprint arXiv:1104.5070, 2011."
REFERENCES,0.42733188720173537,"Robert E Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012."
REFERENCES,0.42950108459869846,"Bruno Scherrer and Matthieu Geist. Local policy search in a convex space and conservative pol-
icy iteration as boosted policy search. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pp. 35–50. Springer, 2014."
REFERENCES,0.4316702819956616,"Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour.
Policy gradi-
ent methods for reinforcement learning with function approximation.
In S. Solla, T. Leen,
and K. Müller (eds.), Advances in Neural Information Processing Systems, volume 12.
MIT Press, 2000.
URL https://proceedings.neurips.cc/paper/1999/file/
464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf."
REFERENCES,0.43383947939262474,"Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229–256, 1992."
REFERENCES,0.4360086767895879,"Jiahao Xie, Zebang Shen, Chao Zhang, Hui Qian, and Boyu Wang. Stochastic recursive gradient-
based methods for projection-free online learning. arXiv preprint arXiv:1910.09396, 2019."
REFERENCES,0.43817787418655096,Under review as a conference paper at ICLR 2022
REFERENCES,0.4403470715835141,"A
APPENDIX"
REFERENCES,0.44251626898047725,"It is important that the policy that the boosting algorithm outputs can be evaluated efﬁciently. To-
wards that end, we give the following claim."
REFERENCES,0.44468546637744033,"Claim 15. For any π ∈Π(Π, N, T), π(·|s) for any s ∈S can be evaluated using TN base policy
evaluations and O(T × (NA + A log A)) arithmetic and logical operations."
REFERENCES,0.44685466377440347,"Proof. Since π ∈Π(Π, N, T), it is composed of TN base policies. Producing each aggregated
function takes NA additions and multiplications; there are T of these. Each projection takes time
equivalent to sorting |A| numbers, due to a water-ﬁlling algorithm (Duchi et al., 2008); these are
also T in number. The ﬁnal linear transformation takes an additional TA operations."
REFERENCES,0.4490238611713666,"B
STEP-SIZE SUBROUTINE"
REFERENCES,0.4511930585683297,"Below we give an algorithm for choosing step sizes used in both of the RL boosting methods (for
online, and supervised, weak learners)."
REFERENCES,0.45336225596529284,"Algorithm 3 StepChooser(πt−1, π′
t, µ, P)"
REFERENCES,0.455531453362256,"1: Execute πt−1 for P episodes with initial state distribution µ via Algorithm 2, to get"
REFERENCES,0.45770065075921906,"D = {(si, c
Qi)P
i=1}."
REFERENCES,0.4598698481561822,"2: For any policy π, let c
Gπ = 1"
REFERENCES,0.46203904555314534,"P
PP
p=1 c
Qi
⊤π(·|si).
3: Return"
REFERENCES,0.4642082429501085,"η1,t = clip[0,1]"
REFERENCES,0.46637744034707157,(1 −γ)2 2
REFERENCES,0.4685466377440347,"d
Gπ′
t −\
Gπt−1
"
REFERENCES,0.47071583514099785,"C
RL BOOSTING VIA WEAK ONLINE LEARNING"
REFERENCES,0.47288503253796094,Algorithm 4 RL Boosting via Weak Online Learning
REFERENCES,0.4750542299349241,"1: Initialize a policy π0 ∈ΠW arbitrarily.
2: for t = 1 to T do
3:
Initialize online weak learners W1, . . . WN.
4:
for m = 1 to M do
5:
Execute πt−1 once with initial state distribution µ via Algorithm 2, to get (st,m, bQt,m).
6:
Choose ρt,m,0 ∈ΠW arbitrarily.
7:
for n = 1 to N do
8:
Set ρt,m,n = (1 −η2,n)ρt,m,n−1 + η2,n"
REFERENCES,0.4772234273318872,"α Wn −η2,n
  1"
REFERENCES,0.4793926247288503,"α −1

πr.
9:
end for
10:
Pass to each Wn the following loss linear ft,m,n:"
REFERENCES,0.48156182212581344,"ft,m,n = −∇FG,β[−bQt,m](ρt,m,n(·|si))"
REFERENCES,0.4837310195227766,"11:
end for
12:
Declare π′
t =
1
M
PM
m=1 Γ [ρt,m,N]."
REFERENCES,0.48590021691973967,"13:
Choose η1,t = min{1, 2C∞(Π)"
REFERENCES,0.4880694143167028,"t
} if µ = d0 else set η1,t = StepChooser(πt−1, π′
t, µ, P).
14:
Update πt = (1 −η1,t)πt−1 + η1,tπ′
t.
15: end for
16: Output ¯π= πT if µ = d0 else output πt−1 with the smallest ηt."
REFERENCES,0.49023861171366595,"Theorem 16. Algorithm 4 samples T(M +P) episodes of length
1
1−γ log T (M+P )"
REFERENCES,0.4924078091106291,"δ
with probability"
REFERENCES,0.4945770065075922,"1−δ. In the episodic model, Algorithm 4 guarantees as long as T = 16C2
∞(Π)
(1−γ)3ε , N =

16|A|C∞(Π)"
REFERENCES,0.4967462039045553,"(1−γ)2αϵ
2
,"
REFERENCES,0.49891540130151846,Under review as a conference paper at ICLR 2022
REFERENCES,0.5010845986984815,"M = max
n
1000|A|2C2
∞(Π)
(1−γ)4ε2α2
log2 Tδ, 8|A|C∞(Π)RW(M)"
REFERENCES,0.5032537960954447,"(1−γ)2αε
o
,µ = d0, we have with probability 1 −δ"
REFERENCES,0.5054229934924078,"V ∗−V π ≤C∞(Π)E(Π, Π)"
REFERENCES,0.5075921908893709,"1 −γ
+ ε"
REFERENCES,0.5097613882863341,"In the ν-reset model, Algorithm 1 guarantees as long as T =
100D2
∞
(1−γ)6ε2 , N =

20|A|D∞
(1−γ)3αϵ
2
,"
REFERENCES,0.5119305856832972,"P = 250D2
∞|A|2"
REFERENCES,0.5140997830802603,(1−γ)6ε2 log2 T
REFERENCES,0.5162689804772235,"δ , M = max

40|A|D∞
(1−γ)3αε log T"
REFERENCES,0.5184381778741866,"δ
2
, 10|A|D∞RW(M)"
REFERENCES,0.5206073752711496,(1−γ)3αε
REFERENCES,0.5227765726681128,"
,µ = ν, we have with"
REFERENCES,0.5249457700650759,probability 1 −δ
REFERENCES,0.527114967462039,"V ∗−V π ≤D∞
Eν(Π, Π)"
REFERENCES,0.5292841648590022,(1 −γ)2 + ε
REFERENCES,0.5314533622559653,"If RW(M) =
p"
REFERENCES,0.5336225596529284,"M log |W| for some measure of weak learning complexity |W|, the algorithm"
REFERENCES,0.5357917570498916,"samples ˜O

C4
∞(Π)|A|2 log |W|"
REFERENCES,0.5379609544468547,"(1−γ)7α2ε3

episodes in the episodic model, and ˜O

D4
∞|A|2 log |W|
(1−γ)12α2ε4

in the ν-
reset model."
REFERENCES,0.5401301518438177,"D
NON-CONVEX FRANK-WOLFE"
REFERENCES,0.5422993492407809,"In this section, we give an abstract high-level procedural template that the previously introduced RL
boosters operate in. This is based on a variant of the Frank-Wolfe optimization technique, adapted
to non-convex and gradient dominated function classes (see Deﬁnition 1)."
REFERENCES,0.544468546637744,"The Frank-Wolfe (FW) method assumes oracle access to a black-box linear optimizer, denoted O,
and utilizes it by iteratively making oracle calls with modiﬁed objectives, in order to solve the
harder task of convex optimization. Analogously, boosting algorithms often assume oracle access
to a ”weak” learner, which are utilized by iteratively making oracle calls with modiﬁed objective,
in order to obtain a ”strong” learner, with boosted performance. In the RL setting, the objective is
in fact non-convex, but exhibits gradient domination. By adapting Frank-Wolfe technique to this
setting, we will in subsequent section obtain guarantees for the algorithms given in Section 4."
REFERENCES,0.5466377440347071,"Setting.
Denote by O a black-box oracle to an (ϵ0, K2)-approximate linear optimizer over a con-
vex set K ⊆Rd such that for any given v ∈Rd, we have"
REFERENCES,0.5488069414316703,"v⊤O(v) ≥max
u∈K2 v⊤u −ϵ0."
REFERENCES,0.5509761388286334,Algorithm 5 Non-convex Frank-Wolfe
REFERENCES,0.5531453362255966,"1: Input: T > 0, objective f, linear optimization oracle O
2: Choose x0 arbitrarily.
3: for t = 1, . . . , T do
4:
Call zt = O(∇t−1), where ∇t−1 = ∇f(xt−1).
5:
Choose ηt = min{1, 2κ"
REFERENCES,0.5553145336225597,"t } in the gradient-dominated case, else choose ηt so that"
REFERENCES,0.5574837310195228,"|LD2ηt −∇⊤
t−1(zt −xt−1)| ≤ϵ."
REFERENCES,0.559652928416486,"6:
Set xt = (1 −ηt)xt−1 + ηtzt.
7: end for
8: return ¯x = xT in the gradient-dominated case, else xt−1 with the smallest ηt."
REFERENCES,0.561822125813449,"Theorem 17. Let f : K →R be L-smooth in some norm ∥· ∥∗, H-bounded, and the diameter of K
in ∥· ∥∗be D. Then, for a (ϵ0, K2)-linear optimization oracle, the output ¯x of Algorithm D satisﬁes"
REFERENCES,0.5639913232104121,"max
u∈K2 ∇f(¯x)⊤(u −¯x) ≤ r 2HLD2"
REFERENCES,0.5661605206073753,"T
+ 3ϵ + ϵ0."
REFERENCES,0.5683297180043384,"Furthermore, if f is (κ, τ, K1, K2)-locally gradient-dominated and x0, . . . xT ∈K1, then it holds"
REFERENCES,0.5704989154013015,"max
x∗∈K f(x∗) −f(¯x) ≤2κ2 max{LD2, H}"
REFERENCES,0.5726681127982647,"T
+ τ + κϵ0."
REFERENCES,0.5748373101952278,Under review as a conference paper at ICLR 2022
REFERENCES,0.5770065075921909,"E
ANALYSIS FOR BOOSTING WITH SUPERVISED LEARNING (PROOF OF
THEOREM 11)"
REFERENCES,0.579175704989154,"Theorem (Formal version of Theorem 11). Algorithm 1 samples T(MN + P) episodes of length
1
1−γ log T (MN+P )"
REFERENCES,0.5813449023861171,"δ
with probability 1 −δ. In the episodic model, Algorithm 1 guarantees as long as"
REFERENCES,0.5835140997830802,"T = 16C2
∞(Π)
(1−γ)3ε , N =

16|A|C∞(Π)"
REFERENCES,0.5856832971800434,"(1−γ)2αϵ
2
, M = m

(1−γ)2αε
8C∞(Π)|A|,
δ
NT

,µ = d0, we have with probability
1 −δ"
REFERENCES,0.5878524945770065,"V ∗−V π ≤C∞(Π)E(Π, Π)"
REFERENCES,0.5900216919739696,"1 −γ
+ ε"
REFERENCES,0.5921908893709328,"In the ν-reset model, Algorithm 1 guarantees as long as T =
8D2
∞
(1−γ)6ε2 , N =

16|A|D∞
(1−γ)3αϵ
2
, P ="
REFERENCES,0.5943600867678959,"200|A|2D2
∞
(1−γ)6ε2 log 2T N"
REFERENCES,0.596529284164859,"δ , M = m

(1−γ)3αε"
REFERENCES,0.5986984815618221,"8|A|D∞,
δ
2NT

,µ = ν, we have with probability 1 −δ"
REFERENCES,0.6008676789587852,"V ∗−V π ≤D∞
Eν(Π, Π)"
REFERENCES,0.6030368763557483,(1 −γ)2 + ε
REFERENCES,0.6052060737527115,"If m(ε, δ) = log |W|"
REFERENCES,0.6073752711496746,"ε2
log 1"
REFERENCES,0.6095444685466378,"δ for some measure of weak learning complexity |W|, the algorithm samples
˜O

C6
∞(Π)|A|4 log |W|"
REFERENCES,0.6117136659436009,"(1−γ)11α4ε5

episodes in the episodic model, and ˜O

D6
∞|A|4 log |W|
(1−γ)18α4ε6

in the ν-reset model."
REFERENCES,0.613882863340564,"Proof of Theorem 11. The broad scheme here is to utilize an equivalence between Algorithm 1 and
Algorithm D on the function V π (or V π
ν in the ν-reset model), to which Theorem 17 applies."
REFERENCES,0.6160520607375272,"To this end, ﬁrstly, note V π is
1
1−γ -bounded. Deﬁne a norm ∥· ∥∞,1 : R|S|×|A| →R as ∥x∥1,∞=
maxs∈S
P"
REFERENCES,0.6182212581344902,"a∈A |xs,a|. Further, observe that for any policy π : S →∆A, ∥π∥∞,1 = 1. The
following lemma speciﬁes the smoothness of V π in this norm."
REFERENCES,0.6203904555314533,"Lemma 18. V π is
2γ
(1−γ)3 -smooth in the ∥· ∥∞,1 norm."
REFERENCES,0.6225596529284165,"To be able to interpret Algorithm 1 as an instantiation of the algorithmic template Algorithm D
presents, we advance two claims: one, the step-size choices of the two algorithms conincide; two,
π′
t (Line 3-10) serves as an approximate linear optimizers for ∇V πt−1. Together, these imply that
the iterates produced by the two algorithms conincide. The ﬁrst of these, which provides a value of
ϵ to use in the statement of Theorem 17, is established below."
REFERENCES,0.6247288503253796,"Claim 19. Upon every invocation of StepChooser, the output η1,t satisﬁes with probability 1−δ

2η1,t
(1 −γ)3 −(∇V πt−1
µ
)⊤(π′
t −πt−1)
 ≤
16|A|
(1 −γ)2√"
REFERENCES,0.6268980477223427,"P
log 1 δ"
REFERENCES,0.6290672451193059,"Next, we move onto the linear optimization equivalence. Indeed, Claim 20 demonstrates that π′
t
serves a linear optimizer over gradients of the function V π; the suboptimality speciﬁes ϵ0."
REFERENCES,0.631236442516269,"Claim 20. Let β =
q"
REFERENCES,0.6334056399132321,"1
αN , and η2,n = min{ 2"
REFERENCES,0.6355748373101953,"n, 1}. Then, for any t, π′
t produced by Algorithm 1
satisﬁes with probability 1 −δ"
REFERENCES,0.6377440347071583,"max
π∈Π(∇V πt−1
µ
)⊤(π −π′
t) ≤
2|A|
(1 −γ)2α  2
√"
REFERENCES,0.6399132321041214,"N
+ εW "
REFERENCES,0.6420824295010846,"Finally, observe that it is by construction that πt ∈Π. Therefore, in terms of the previous section, K
is the class of all policies, K1 = Π, K2 = Π."
REFERENCES,0.6442516268980477,"In the episodic model, we wish to invoke the second part of Theorem 17. The next lemma establishes
gradient-domination properties of V π to support this."
REFERENCES,0.6464208242950108,"Lemma 21. V π is

C∞(Π),
1
1−γ C∞(Π)E(Π, Π), Π, Π

-gradient dominated, i.e. for any π ∈Π:"
REFERENCES,0.648590021691974,"V ∗−V π ≤C∞(Π)

1
1 −γ E(Π, Π) + max
π′∈Π(∇V π)⊤(π′ −π)
"
REFERENCES,0.6507592190889371,Under review as a conference paper at ICLR 2022
REFERENCES,0.6529284164859002,"Deriving κ, τ from the above lemma along with ϵ0 from Claim 20 and ϵ from Claim 19, as a conse-
quence of the second part of Theorem 17, we have with probability 1 −NTδ"
REFERENCES,0.6550976138828634,"V ∗−V ¯π ≤C∞(Π)E(Π, Π)"
REFERENCES,0.6572668112798264,"1 −γ
+ 4C2
∞(Π)
(1 −γ)3T +
4|A|C∞(Π)
(1 −γ)2α
√"
REFERENCES,0.6594360086767896,"N
+ 2|A|C∞(Π)"
REFERENCES,0.6616052060737527,(1 −γ)2α εW .
REFERENCES,0.6637744034707158,"Similarly, in the ν-reset model, the ﬁrst part of Theorem 17 provides a local-optimality guarantee for
V π
ν . Lemma 22 provides a bound on the function-value gap (on V π) provided such local-optimality
conditions."
REFERENCES,0.665943600867679,"Lemma 22. For any π ∈Π, we have"
REFERENCES,0.6681127982646421,"V ∗−V π ≤
1
1 −γ D∞"
REFERENCES,0.6702819956616052,"
1
1 −γ Eν(Π, Π) + max
π′∈Π(∇V π
ν )⊤(π′ −π)
"
REFERENCES,0.6724511930585684,"Again, using the bound on maxπ′∈Π(∇V ¯π
ν )⊤(π′ −¯π) Theorem 17 provides, we have that with
probability 1 −2NTδ"
REFERENCES,0.6746203904555315,"V ∗−V ¯π ≤D∞Eν(Π, Π)"
REFERENCES,0.6767895878524945,"(1 −γ)2
+
2D∞
(1 −γ)3√"
REFERENCES,0.6789587852494577,"T
+ 2|A|D∞"
REFERENCES,0.6811279826464208,"(1 −γ)3α  2
√"
REFERENCES,0.6832971800433839,"N
+ εW"
REFERENCES,0.6854663774403471,"
+
48|A|D∞
(1 −γ)3√"
REFERENCES,0.6876355748373102,"P
log 1 δ"
REFERENCES,0.6898047722342733,"F
ANALYSIS FOR BOOSTING WITH ONLINE LEARNING (PROOF OF
THEOREM 16)"
REFERENCES,0.6919739696312365,"Proof of Theorem 16. Similar to the proof of Theorem 11, we establish an equivalence between
Algorithm 1 and Algorithm D on the function V π (or V π
ν in the ν-reset model), to which Theorem 17
applies provided smoothness (see Lemma 18)."
REFERENCES,0.6941431670281996,"Indeed, Claim 23 demonstrates π′
t serves a linear optimizer over gradients of the function V π, and
provides a bound on ϵ0. Claim 19 ensures that that the step size choices (and hence iterates) of the
two algorithms coincide. As before, observe that it is by construction that πt ∈Π."
REFERENCES,0.6963123644251626,"Claim 23. Let β =
q"
REFERENCES,0.6984815618221258,"1
αN , and η2,n = min{ 2"
REFERENCES,0.7006507592190889,"n, 1}. Then, for any t, π′
t produced by Algorithm 4
satisﬁes with probability 1 −δ"
REFERENCES,0.702819956616052,"max
π∈Π(∇V πt−1
µ
)⊤(π −π′
t) ≤
2|A|
(1 −γ)2α 2
√"
REFERENCES,0.7049891540130152,"N
+ RW(M) M
+ r"
REFERENCES,0.7071583514099783,16 log δ−1 M !
REFERENCES,0.7093275488069414,"In the episodic model, one may combine the second part of Theorem 17, which provides a bound
on function-value gap for gradient dominated functions, which Lemma 21 guarantees, to conclude
with probability 1 −Tδ"
REFERENCES,0.7114967462039046,"V ∗−V ¯π ≤C∞(Π)E(Π, Π)"
REFERENCES,0.7136659436008677,"1 −γ
+ 4C2
∞(Π)
(1 −γ)3T +
4|A|C∞(Π)
(1 −γ)2α
√"
REFERENCES,0.7158351409978309,"N
+ 2|A|C∞(Π)"
REFERENCES,0.7180043383947939,"(1 −γ)2α
RW(M)"
REFERENCES,0.720173535791757,"M
+ 8|A|C∞(Π) log δ−1"
REFERENCES,0.7223427331887202,"(1 −γ)2α
√ M
."
REFERENCES,0.7245119305856833,"Similarly, in the ν-reset model, Lemma 22 provides a bound on the function-value gap provided
local-optimality conditions, which the ﬁrst part of Theorem 17 provides for. Again, with probability
1 −Tδ"
REFERENCES,0.7266811279826464,"V ∗−V ¯π ≤D∞Eν(Π, Π)"
REFERENCES,0.7288503253796096,"(1 −γ)2
+
2D∞
(1 −γ)3  1
√"
REFERENCES,0.7310195227765727,"T
+ |A| α  2
√"
REFERENCES,0.7331887201735358,"N
+ RW(M)"
REFERENCES,0.735357917570499,"M
+ 4 log δ−1 √ M"
REFERENCES,0.737527114967462,"
+ 24|A|
√"
REFERENCES,0.7396963123644251,"P
log 1 δ "
REFERENCES,0.7418655097613883,Under review as a conference paper at ICLR 2022
REFERENCES,0.7440347071583514,"G
PROOFS OF SUPPORTING CLAIMS"
REFERENCES,0.7462039045553145,"G.1
NON-CONVEX FRANK-WOLFE METHOD (THEOREM 17)"
REFERENCES,0.7483731019522777,"Proof of Theorem 17. Non-convex case. Note that for any timestep t, it holds due to smoothness
that
f(xt) = f(xt−1 + ηt(zt −xt−1))"
REFERENCES,0.7505422993492408,"≥f(xt−1) + ηt∇⊤
t−1(zt −xt−1) −η2
t
L 2 D2"
REFERENCES,0.7527114967462039,"= f(xt−1) −
1
2LD2
 
LD2ηt −∇⊤
t−1(zt −xt−1)
2 + (∇⊤
t−1(zt −xt−1))2"
REFERENCES,0.754880694143167,"2LD2
Using the step-size deﬁnition to bound on the middle term, and telescoping this inequality over
function-value differences across successive iterates, we have"
REFERENCES,0.7570498915401301,"min
t (∇⊤
t−1(zt −xt−1))2 ≤1 T T
X"
REFERENCES,0.7592190889370932,"t=1
(∇⊤
t−1(zt −xt−1))2 ≤2LD2H"
REFERENCES,0.7613882863340564,"T
+ ϵ2"
REFERENCES,0.7635574837310195,"Let t′ = arg mint ηt and t∗= arg mint(∇⊤
t−1(zt −xt−1))2. Then"
REFERENCES,0.7657266811279827,"∇⊤
t′−1(zt′ −xt′−1) ≤LD2ηt′ + ϵ ≤LD2ηt∗+ ϵ"
REFERENCES,0.7678958785249458,"≤∇⊤
t∗−1(zt∗−xt∗−1) + 2ϵ ≤ r 2LD2H"
REFERENCES,0.7700650759219089,"T
+ ϵ2 + 2ϵ"
REFERENCES,0.7722342733188721,"To conclude the claim for the non-convex part, observe
√"
REFERENCES,0.7744034707158352,"a + b ≤√a +
√"
REFERENCES,0.7765726681127982,"b for a, b > 0, and that
since zt′ = O(∇t′−1), it follows by oracle deﬁnition that"
REFERENCES,0.7787418655097614,"max
u∈K2 ∇⊤
t′−1u ≤∇⊤
t′−1zt′ + ϵ0."
REFERENCES,0.7809110629067245,"Gradient-Dominated Case.
Deﬁne x∗= arg maxx∈K f(x) and ht = f(x∗) −f(xt)."
REFERENCES,0.7830802603036876,"ht ≤ht−1 −ηt∇⊤
t−1(zt −xt−1) + η2
t
L"
REFERENCES,0.7852494577006508,"2 D2
smoothness"
REFERENCES,0.7874186550976139,"≤ht−1 −ηt max
y∈K2 ηt∇⊤
t−1(y −xt−1) + η2
t
L"
REFERENCES,0.789587852494577,"2 D2 + ηtϵ0
oracle"
REFERENCES,0.7917570498915402,≤ht−1 −ηt
REFERENCES,0.7939262472885033,"κ (f(x∗) −f(xt−1)) + η2
t
L"
REFERENCES,0.7960954446854663,"2 D2 + ηt

ϵ0 + τ κ"
REFERENCES,0.7982646420824295,"
gradient domination"
REFERENCES,0.8004338394793926,"=

1 −ηt κ"
REFERENCES,0.8026030368763557,"
ht−1 + η2
t
L"
REFERENCES,0.8047722342733189,"2 D2 + ηt

ϵ0 + τ κ "
REFERENCES,0.806941431670282,The theorem now follows from the following claim.
REFERENCES,0.8091106290672451,Claim 24. Let C ≥1. Let gt be a H-bounded positive sequence such that
REFERENCES,0.8112798264642083,"gt ≤

1 −σt C"
REFERENCES,0.8134490238611713,"
gt−1 + σ2
t D + σtE."
REFERENCES,0.8156182212581344,"Then choosing σt = min{1, 2C"
REFERENCES,0.8177874186550976,"t } implies gt ≤
2C2 max{2D,H}"
REFERENCES,0.8199566160520607,"t
+ CE."
REFERENCES,0.8221258134490239,"G.2
SMOOTHNESS OF VALUE FUNCTION (LEMMA 18)"
REFERENCES,0.824295010845987,"Proof of Lemma 18. Consider any two policies π, π′. Using the Performance Difference Lemma
(Lemma 3.2 in (Agarwal et al., 2019), e.g.) and Equation 2, we have"
REFERENCES,0.8264642082429501,|V π′ −V π −∇V π(π′ −π)|
REFERENCES,0.8286334056399133,"=
1
1 −γ"
REFERENCES,0.8308026030368764,"Es∼dπ′ 
Qπ(·|s)⊤(π′(·|s) −π(·|s)

−Es∼dπ 
Qπ(·|s)⊤(π′(·|s) −π(·|s)
"
REFERENCES,0.8329718004338394,"≤
1
(1 −γ)2 ∥dπ′ −dπ∥1∥π′ −π∥∞,1"
REFERENCES,0.8351409978308026,Under review as a conference paper at ICLR 2022
REFERENCES,0.8373101952277657,"The last inequality uses the fact that maxs,a Qπ(s, a)
≤
1
1−γ .
It sufﬁces to show ∥dπ′ −
dπ∥1
≤
γ
1−γ ∥π′ −π∥∞,1.
To establish this, consider the Markov operator P π(s′|s) =
P"
REFERENCES,0.8394793926247288,"a∈A P(s′|s, a)π(a|s) induced by a policy π on MDP M. For any distribution d supported on
S, we have"
REFERENCES,0.841648590021692,"∥(P π′ −P π)d∥1 =
X s′  X"
REFERENCES,0.8438177874186551,"s,a
P(s′|s, a)d(s)(π′(a|s) −π(a|s)  ≤
X"
REFERENCES,0.8459869848156182,"s′
P(s′|s, a)∥d∥1∥π′ −π∥∞,1 ≤∥π′ −π∥∞,1"
REFERENCES,0.8481561822125814,"Using sub-additivity of the l1 norm and applying the above observation t times, we have for any t"
REFERENCES,0.8503253796095445,"∥((P π′)t −(P π)t)d∥1 ≤t∥π′ −π∥∞,1."
REFERENCES,0.8524945770065075,"Finally, observe that"
REFERENCES,0.8546637744034707,"∥dπ′ −dπ∥1 ≤(1 −γ) ∞
X"
REFERENCES,0.8568329718004338,"t=0
γt∥((P π′)t −(P π)t)d0∥1"
REFERENCES,0.8590021691973969,"≤∥π′ −π∥∞,1(1 −γ) ∞
X"
REFERENCES,0.8611713665943601,"t=0
tγt =
γ
1 −γ ∥π′ −π∥∞,1"
REFERENCES,0.8633405639913232,"G.3
STEP-SIZE GUARANTEE (CLAIM 19)"
REFERENCES,0.8655097613882863,"Proof of Claim 19. Let D be the distribution induced by Algorithm 2 upon being given πt−1. Due
to Lemma 14, it sufﬁces to demonstrate that for any π ∈{π′
t, πt−1} the following claim holds with
probability 1 −δ"
REFERENCES,0.8676789587852495,"2. The claim in turn follows from Hoeffding’s inequality, while noting \
Qπt−1(s, ·)
is
|A|
1−γ -bounded in the l∞norm.
 c
Gπ −E(s, \
Qπt−1(s,·))∼D"
REFERENCES,0.8698481561822126,"h
\
Qπt−1(s, ·)⊤π(·|s)
i ≤
8|A|
(1 −γ)
√"
REFERENCES,0.8720173535791758,"P
log 1 2δ"
REFERENCES,0.8741865509761388,"G.4
GRADIENT DOMINATION (LEMMA 21 AND LEMMA 22)"
REFERENCES,0.8763557483731019,"Proof of Lemma 21. Invoking Lemma 4.1 from (Agarwal et al., 2019) with µ = d0, we have"
REFERENCES,0.8785249457700651,"V ∗−V π ≤

dπ∗ dπ"
REFERENCES,0.8806941431670282,"∞
max
π0 (∇V π)⊤(π0 −π)"
REFERENCES,0.8828633405639913,"≤C∞(Π)(max
π0 (∇V π)⊤π0 −max
π′∈Π(∇V π)⊤π′ + max
π′∈Π(∇V π)⊤(π′ −π))"
REFERENCES,0.8850325379609545,"Finally, with the aid of Equation 2, observe that"
REFERENCES,0.8872017353579176,"max
π0 (∇V π)⊤π0 −max
π′∈Π(∇V π)⊤π′ = min
π′∈Π
1
1 −γ Es∼dπ
h
max
a
Qπ(s, a) −Qπ(·|s)⊤π′i"
REFERENCES,0.8893709327548807,"≤
1
1 −γ E(Π, Π)"
REFERENCES,0.8915401301518439,"Proof of Lemma 22. Invoking Lemma 4.1 from (Agarwal et al., 2019) with µ = ν, we have"
REFERENCES,0.8937093275488069,"V ∗−V π ≤
1
1 −γ dπ∗ ν"
REFERENCES,0.89587852494577,"∞
max
π0 (∇V π
ν )⊤(π0 −π)"
REFERENCES,0.8980477223427332,"≤
1
1 −γ D∞(max
π0 (∇V π
ν )⊤π0 −max
π′∈Π(∇V π
ν )⊤π′ + max
π′∈Π(∇V π
ν )⊤(π′ −π))"
REFERENCES,0.9002169197396963,Under review as a conference paper at ICLR 2022
REFERENCES,0.9023861171366594,"Again, with the aid of Equation 2, observe that"
REFERENCES,0.9045553145336226,"max
π0 (∇V π
ν )⊤π0 −max
π′∈Π(∇V π
ν )⊤π′ = min
π′∈Π
1
1 −γ Es∼dπ
ν
h
max
a
Qπ(s, a) −Qπ(·|s)⊤π′i"
REFERENCES,0.9067245119305857,"≤
1
1 −γ Eν(Π, Π)"
REFERENCES,0.9088937093275488,"G.5
SUPERVISED LINEAR OPTIMIZATION GUARANTEES (CLAIM 20)"
REFERENCES,0.911062906724512,"Proof of Claim 20. The subroutine presented in lines 3-10 (which culminate in π′
t) is an instantiation
of Algorithm 3 from (Hazan & Singh, 2021), specializing the decision set to be ∆A. To note the
equivalence, note that in (Hazan & Singh, 2021) the algorithm is stated assuming that the center-of-
mass of the decision set is at the origin (after a coordinate transform); correspondingly, the update
rule in Algorithm 1 can be written as"
REFERENCES,0.913232104121475,"(ρt,n −πr) = (1 −η2,n)(ρt,n−1 −πr) + η2,n"
REFERENCES,0.9154013015184381,"α (At,n −πr)."
REFERENCES,0.9175704989154013,"For any state s, πr(·|s) =
1
A1|A| corresponds to the center-of-masss of ∆A. Finally, note that
maximizing f ⊤x over x ∈K is equivalent to minimizing (−f)⊤x over the same domain. Therefore,
we can borrow the following result on boosting for statistical learning from (Hazan & Singh, 2021)
(Theorem 13). Note that c
Qπ(s, ·) produced by Algorithm 2 satisﬁes ∥c
Qπ(s, ·)∥=
|A|
1−γ . Let Dt be
the distribution induced by the trajectory sampler in round t."
REFERENCES,0.9197396963123644,"Theorem 25 ((Hazan & Singh, 2021)). Let β =
q"
REFERENCES,0.9219088937093276,"1
αN , and η2,n = min{ 2"
REFERENCES,0.9240780911062907,"n, 1}. Then, for any t, π′
t
produced by Algorithm 1 satisﬁes with probability 1 −δ that"
REFERENCES,0.9262472885032538,"max
π∈Π E(s,Q)∼Dt

Q⊤π(s)

−E(s,Q)∼Dt

Q⊤π′
t(s)

≤
2|A|
(1 −γ)α  2
√"
REFERENCES,0.928416485900217,"N
+ ε
"
REFERENCES,0.93058568329718,"Lemma 14 allows us to restate the guarantees in the previous subsection in terms of linear optimiza-
tion over functional gradients. The conclusion thus follows immediately by combining Lemma 14
and Theorem 25."
REFERENCES,0.9327548806941431,"G.6
ONLINE LINEAR OPTIMIZATION GUARANTEES (CLAIM 23)"
REFERENCES,0.9349240780911063,"Proof of Claim 23. In a similar vein to the proof of Claim 20, here we state the a result on boosting
for online convex optimization (OCO) from (Hazan & Singh, 2021) (Theorem 6), the counterpart of
Theorem 13 for the online weak learning case."
REFERENCES,0.9370932754880694,"Theorem 26 ((Hazan & Singh, 2021)). Let β =
q"
REFERENCES,0.9392624728850325,"1
αN , and η2,n = min{ 2"
REFERENCES,0.9414316702819957,"n, 1}. Then, for any t,
Γ[ρt,m,N] produced by Algorithm 4 satisﬁes"
REFERENCES,0.9436008676789588,"max
π∈Π M
X m=1"
REFERENCES,0.9457700650759219,"h
ˆQ⊤
t,mπ(st,m)
i
− M
X m=1"
REFERENCES,0.9479392624728851,"h
ˆQ⊤
t,mΓ[ρt,m,N](st,m)
i
≤
2|A|
(1 −γ)α"
REFERENCES,0.9501084598698482," 2M
√"
REFERENCES,0.9522776572668112,"N
+ RW(M)
"
REFERENCES,0.9544468546637744,"Next we invoke online-to-batch conversions. Note that in Algorithm 4, (st,m, ˆQt,m) for any ﬁxed
t is sampled i.i.d. from the same distribution. Therefore, we can apply online-to-batch results, i.e.
Theorem 9.5 in (Hazan, 2019), on Theorem 26 to get"
REFERENCES,0.9566160520607375,"max
π∈Π E(s,Q)∼Dt

Q⊤π(s)

−E(s,Q)∼Dt

Q⊤π′
t(s)

≤
2|A|
(1 −γ)α 2
√"
REFERENCES,0.9587852494577006,"N
+ RW(M) M
+ r"
REFERENCES,0.9609544468546638,16 log δ−1 M !
REFERENCES,0.9631236442516269,We ﬁnally invoke Lemma 14.
REFERENCES,0.96529284164859,Under review as a conference paper at ICLR 2022
REFERENCES,0.9674620390455532,"G.7
REMAINING PROOFS (CLAIM 24)"
REFERENCES,0.9696312364425163,"Proof of Claim 24. Let T ∗= arg maxt{t : t ≤2C}. For any t ≤T ∗, we have σt = 1 and
gt ≤H ≤
2C2H"
REFERENCES,0.9718004338394793,"t
. For t ≥T ∗, we proceed by induction. The base case (t = T ∗) is true by the"
REFERENCES,0.9739696312364425,"previous display. Now, assume gt−1 ≤
2C2 max{2D,H}"
REFERENCES,0.9761388286334056,"t−1
+ CE for some t > T ∗."
REFERENCES,0.9783080260303688,"gt ≤

1 −2 t"
REFERENCES,0.9804772234273319," 2C2 max{2D, H}"
REFERENCES,0.982646420824295,"t −1
+ CE

+ 4C2D"
REFERENCES,0.9848156182212582,"t2
+ 2CE t"
REFERENCES,0.9869848156182213,"≤CE + 2C2 max{2D, H}

1
t −1"
REFERENCES,0.9891540130151844,"
1 −2 t 
+ 1 t2 "
REFERENCES,0.9913232104121475,"= CE + 2C2 max{2D, H}t2 −2t + t −1"
REFERENCES,0.9934924078091106,t2(t −1)
REFERENCES,0.9956616052060737,"≤CE + 2C2 max{2D, H} t(t −1)"
REFERENCES,0.9978308026030369,t2(t −1)
