Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002036659877800407,"We propose a new general model called IPNN – Indeterminate Probability Neural
1"
ABSTRACT,0.004073319755600814,"Network, which combines neural network and probability theory together. In the
2"
ABSTRACT,0.006109979633401222,"classical probability theory, the calculation of probability is based on the occurrence
3"
ABSTRACT,0.008146639511201629,"of events, which is hardly used in current neural networks. In this paper, we propose
4"
ABSTRACT,0.010183299389002037,"a new general probability theory, which is an extension of classical probability
5"
ABSTRACT,0.012219959266802444,"theory, and makes classical probability theory a special case to our theory. With
6"
ABSTRACT,0.014256619144602852,"this new theory, some intractable probability problems have now become tractable
7"
ABSTRACT,0.016293279022403257,"(analytical solution). Besides, for our proposed neural network framework, the
8"
ABSTRACT,0.018329938900203666,"output of neural network is defined as probability events, and based on the statistical
9"
ABSTRACT,0.020366598778004074,"analysis of these events, the inference model for classification task is deduced.
10"
ABSTRACT,0.02240325865580448,"IPNN shows new property: It can perform unsupervised clustering while doing
11"
ABSTRACT,0.024439918533604887,"classification. Besides, IPNN is capable of making very large classification with
12"
ABSTRACT,0.026476578411405296,"very small neural network, e.g. model with 100 output nodes can classify 10 billion
13"
ABSTRACT,0.028513238289205704,"categories. Theoretical advantages are reflected in experimental results.
14"
INTRODUCTION,0.03054989816700611,"1
Introduction
15"
INTRODUCTION,0.032586558044806514,"Humans can distinguish at least 30,000 basic object categories [1], classification of all these would
16"
INTRODUCTION,0.034623217922606926,"have two challenges: It requires huge well-labeled images; Model with softmax for large scaled
17"
INTRODUCTION,0.03665987780040733,"datasets is computationally expensive. Zero-Shot Learning – ZSL [2, 3] method provides an idea
18"
INTRODUCTION,0.038696537678207736,"for solving the first problem, which is an attribute-based classification method. ZSL performs object
19"
INTRODUCTION,0.04073319755600815,"detection based on a human-specified high-level description of the target object instead of training
20"
INTRODUCTION,0.04276985743380855,"images, like shape, color or even geographic information. But labelling of attributes still needs great
21"
INTRODUCTION,0.04480651731160896,"efforts and expert experience. Hierarchical softmax can solve the computationally expensive problem,
22"
INTRODUCTION,0.04684317718940937,"but the performance degrades as the number of classes increase [4].
23"
INTRODUCTION,0.048879837067209775,"Probability theory has not only achieved great successes in the classical area, such as Naïve Bayesian
24"
INTRODUCTION,0.05091649694501019,"method [5], but also in deep neural networks (VAE [6], ZSL, etc.) over the last years. However, both
25"
INTRODUCTION,0.05295315682281059,"have their shortages: Classical probability can not extract features from samples; For neural networks,
26"
INTRODUCTION,0.054989816700611,"the extracted features are usually abstract and cannot be directly used for numerical probability
27"
INTRODUCTION,0.05702647657841141,"calculation. What if we combine them?
28"
INTRODUCTION,0.059063136456211814,"There are already some combinations of neural network and bayesian approach, such as probability
29"
INTRODUCTION,0.06109979633401222,"distribution recognition [7, 8], Bayesian approach are used to improve the accuracy of neural
30"
INTRODUCTION,0.06313645621181263,"modeling [9], etc. However, current combinations do not take advantages of ZSL method.
31"
INTRODUCTION,0.06517311608961303,"We propose an approach to solve the mentioned problems, and our contributions are as follows:
32"
INTRODUCTION,0.06720977596741344,"• We propose a new general probability theory – indeterminate probability theory, which is
33"
INTRODUCTION,0.06924643584521385,"an extension of classical probability theory, and makes classical probability theory a special
34"
INTRODUCTION,0.07128309572301425,"case to our theory. The proposed general tractable Equation (12) is analytical solutions even
35"
INTRODUCTION,0.07331975560081466,"for some intractable probability calculation problems.
36"
INTRODUCTION,0.07535641547861507,"• With this new theory, CIPNN [10] has found the analytical solution for the posterior
37"
INTRODUCTION,0.07739307535641547,"calculation of continuous latent variables, which was regarded as intractable [6, 11]. Besides,
38"
INTRODUCTION,0.07942973523421588,"CIPNN applied our theory and proposed a general auto encoder (CIPAE), the decoder part
39"
INTRODUCTION,0.0814663951120163,"is not a neural network and uses a fully probabilistic inference model for the first time.
40"
INTRODUCTION,0.0835030549898167,"• We propose a novel unified combination of (indeterminate) probability theory and deep
41"
INTRODUCTION,0.0855397148676171,"neural network. The neural network is used to extract attributes which are defined as discrete
42"
INTRODUCTION,0.08757637474541752,"random variables, and the inference model for classification task is derived. Besides, these
43"
INTRODUCTION,0.08961303462321792,"attributes do not need to be labeled in advance.
44"
INTRODUCTION,0.09164969450101833,"The rest of this paper is organized as follows: In Section 2, related works are discussed. In Section 3,
45"
INTRODUCTION,0.09368635437881874,"we first introduce a coin toss game as example of human cognition to explain the core idea of
46"
INTRODUCTION,0.09572301425661914,"IPNN. In Section 4, the indeterminate probability theory and IPNN is proposed. In Section 5, the
47"
INTRODUCTION,0.09775967413441955,"training strategy is discussed. In Section 6, we evaluate IPNN and make an impact analysis on its
48"
INTRODUCTION,0.09979633401221996,"hyper-parameters. Finally, we conclude the paper in Section 7.
49"
RELATED WORK,0.10183299389002037,"2
Related Work
50"
RELATED WORK,0.10386965376782077,"Tractable Probabilistic Models.
There are a large family of tractable models including probabilistic
51"
RELATED WORK,0.10590631364562118,"circuits [12, 13], arithmetic circuits [14, 15], sum-product networks [16], cutset networks [17], and-or
52"
RELATED WORK,0.1079429735234216,"search spaces [18], and probabilistic sentential decision diagrams [19]. The analytical solution of
53"
RELATED WORK,0.109979633401222,"a probability calculation is defined as occurrence, P(A = a) = number of event (A=a) occurs"
RELATED WORK,0.1120162932790224,"number of random experiments , which is
54"
RELATED WORK,0.11405295315682282,"however not focused in these models. Our proposed IPNN is fully based on event occurrence and is
55"
RELATED WORK,0.11608961303462322,"an analytical solution.
56"
RELATED WORK,0.11812627291242363,"Deep Latent Variable Models.
DLVMs are probabilistic models and can refer to the use of neural
57"
RELATED WORK,0.12016293279022404,"networks to perform latent variable inference [20]. Currently, the posterior calculation of continuous
58"
RELATED WORK,0.12219959266802444,"latent variables is regarded as intractable [11], VAEs [6, 21–23] use variational inference method [24]
59"
RELATED WORK,0.12423625254582485,"as approximate solutions. Our proposed IPNN is one DLVM with discrete latent variables and the
60"
RELATED WORK,0.12627291242362526,"intractable posterior calculation is now analytically solved with our proposed theory.
61"
BACKGROUND,0.12830957230142567,"3
Background
62"
BACKGROUND,0.13034623217922606,"Let’s first introduce a small game – coin toss: a child and an adult are observing the outcomes of
63"
BACKGROUND,0.13238289205702647,"each coin toss and record the results independently (heads or tails), the child can’t always record the
64"
BACKGROUND,0.13441955193482688,"results correctly and the adult can record it correctly, in addition, the records of the child are also
65"
BACKGROUND,0.1364562118126273,"observed by the adult. After several coin tosses, the question now is, suppose the adult is not allowed
66"
BACKGROUND,0.1384928716904277,"to watch the next coin toss, what is the probability of his inference outcome of next coin toss via the
67"
BACKGROUND,0.14052953156822812,"child’s record?
68"
BACKGROUND,0.1425661914460285,"Heads
Tails
Heads
Tails X A
Y"
BACKGROUND,0.1446028513238289,Figure 1: Example of coin toss game.
BACKGROUND,0.14663951120162932,Table 1: Example of 10 times coin toss outcomes
BACKGROUND,0.14867617107942974,"Experiment
Truth
A
Y
X = x1
hd
A = hd
Y = hd
X = x2
hd
A = hd
Y = hd
X = x3
hd
A = hd
Y = hd
X = x4
hd
A = hd
Y = hd
X = x5
hd
A = tl
Y = hd
X = x6
tl
A = tl
Y = tl
X = x7
tl
A = tl
Y = tl
X = x8
tl
A = tl
Y = tl
X = x9
tl
A = tl
Y = tl
X = x10
tl
A = tl
Y = tl
X = x11
hd
A =?
Y =?"
BACKGROUND,0.15071283095723015,"As shown in Figure 1, random variables X is the random experiment itself, and X = xk represent the
69"
BACKGROUND,0.15274949083503056,"kth random experiment. Y and A are defined to represent the adult’s record and the child’s record,
70"
BACKGROUND,0.15478615071283094,"respectively. And hd, tl is for heads and tails. For example, after 10 coin tosses, the records are
71"
BACKGROUND,0.15682281059063136,"shown in Table 1.
72"
BACKGROUND,0.15885947046843177,"We formulate X compactly with the ground truth, as shown in Table 2.
73"
BACKGROUND,0.16089613034623218,Table 2: The adult’s and child’s records: P(Y |X) and P(A|X)
BACKGROUND,0.1629327902240326,"#(Y,X)"
BACKGROUND,0.164969450101833,"#(X)
Y = hd
Y = tl
#(A,X)"
BACKGROUND,0.1670061099796334,"#(X)
A = hd
A = tl"
BACKGROUND,0.1690427698574338,"X = hd
5/5
0
X = hd
4/5
1/5
X = tl
0
5/5
X = tl
0
5/5"
BACKGROUND,0.1710794297352342,"Through the adult’s record Y and the child’s records A, we can calculate P(Y |A), as shown in
74"
BACKGROUND,0.17311608961303462,"Table 3. We define this process as observation phase.
75"
BACKGROUND,0.17515274949083504,"For next coin toss (X = x11), the question of this game is formulated as calculation of the probability
76"
BACKGROUND,0.17718940936863545,"P A(Y |X), superscript A indicates that Y is inferred via record A, not directly observed by the adult.
77"
BACKGROUND,0.17922606924643583,"For example, given the next coin toss X = hd = x11, the child’s record has then two situations:
78"
BACKGROUND,0.18126272912423624,"P(A = hd|X = hd = x11) = 4/5 and P(A = tl|X = hd = x11) = 1/5. With the adult’s
79"
BACKGROUND,0.18329938900203666,"observation of the child’s records, we have P(Y = hd|A = hd) = 4/4 and P(Y = hd|A = tl) =
80"
BACKGROUND,0.18533604887983707,"1/6. Therefore, given next coin toss X = hd = x11, P A(Y = hd|X = hd = x11) is the summation
81"
BACKGROUND,0.18737270875763748,of these two situations: 4 5 · 4 4 + 1 5 · 1
BACKGROUND,0.1894093686354379,"6. Table 3 answers the above mentioned question.
82"
BACKGROUND,0.19144602851323828,Table 3: Results of observation and inference phase: P(Y |A) and P A(Y |X)
BACKGROUND,0.1934826883910387,"#(Y,A)"
BACKGROUND,0.1955193482688391,"#(A)
Y = hd
Y = tl
P"
BACKGROUND,0.1975560081466395,"A

#(A,X)"
BACKGROUND,0.19959266802443992,"#X
· #(Y,A)"
BACKGROUND,0.20162932790224034,"#A

Y = hd
Y = tl"
BACKGROUND,0.20366598778004075,"A = hd
4/4
0
X = hd = x11
4
5 · 4 4 + 1 5 · 1"
BACKGROUND,0.20570264765784113,"6
4
5 · 0 + 1 5 · 5"
BACKGROUND,0.20773930753564154,"6
A = tl
1/6
5/6
X = tl = x11
0 · 4 4 + 5 5 · 1"
BACKGROUND,0.20977596741344195,"6
0 · 0 + 5 5 · 5 6"
BACKGROUND,0.21181262729124237,"Let’s go one step further, we can find that even the child’s record is written in unknown language
83"
BACKGROUND,0.21384928716904278,"(e.g. A ∈{ZHENG, FAN}), Table 3 can still be calculated by the man. The same is true if the
84"
BACKGROUND,0.2158859470468432,"child’s record is written from the perspective of attributes, such as color, shape, etc.
85"
BACKGROUND,0.21792260692464357,"Hence, if we substitute the child with a neural network and regard the adult’s record as the sample
86"
BACKGROUND,0.219959266802444,"labels, although the representation of the model outputs is unknown, the labels of input samples can
87"
BACKGROUND,0.2219959266802444,"still be inferred from these outputs. This is the core idea of IPNN.
88"
INDETERMINATE PROBABILITY THEORY,0.2240325865580448,"4
Indeterminate Probability Theory
89"
INDETERMINATE PROBABILITY THEORY,0.22606924643584522,"In this section, we propose a new general probability theory, which is derived from IPNN – a neural
90"
INDETERMINATE PROBABILITY THEORY,0.22810590631364563,"network with discrete deep latent variables.
91"
IPNN MODEL ARCHITECTURE,0.23014256619144602,"4.1
IPNN Model Architecture
92"
IPNN MODEL ARCHITECTURE,0.23217922606924643,"Let X ∈{x1, x2, . . . , xn} be training samples (X = xk is understood as kth random experiment
93"
IPNN MODEL ARCHITECTURE,0.23421588594704684,"– select one train sample.) and Y ∈{y1, y2, . . . , ym} consists of m discrete labels (or classes),
94"
IPNN MODEL ARCHITECTURE,0.23625254582484725,"P(yl|xk) = yl(k) ∈{0, 1} describes the label of sample xk. For prediction, we calculate the posterior
95"
IPNN MODEL ARCHITECTURE,0.23828920570264767,"of the label for a given new input sample xn+1, it is formulated as P A (yl | xn+1), superscript A
96"
IPNN MODEL ARCHITECTURE,0.24032586558044808,"stands for the medium – model outputs, via which we can infer label yl, l = 1, 2, . . . , m. After
97"
IPNN MODEL ARCHITECTURE,0.24236252545824846,"P A (yl | xn+1) is calculated, the yl with maximum posterior is the predicted label.
98"
IPNN MODEL ARCHITECTURE,0.24439918533604887,"Figure 2a shows IPNN model architecture, the output neurons of a general neural network
99"
IPNN MODEL ARCHITECTURE,0.24643584521384929,"(FFN, CNN, Resnet [25], Transformer [26], Pretrained-Models [27], etc.)
is split into N un-
100"
IPNN MODEL ARCHITECTURE,0.2484725050916497,"equal/equal parts, the split shape is marked as Equation (1), hence, the number of output neu-
101"
IPNN MODEL ARCHITECTURE,0.2505091649694501,"rons is the summation of the split shape PN
j=1 Mj. Next, each split part is passed to ‘softmax’,
102"
IPNN MODEL ARCHITECTURE,0.2525458248472505,"so the output neurons can be defined as discrete random variable Aj ∈
n
aj
1, aj
2, . . . , aj
Mj"
IPNN MODEL ARCHITECTURE,0.2545824847250509,"o
, j =
103 𝑎𝑖1 1"
IPNN MODEL ARCHITECTURE,0.25661914460285135,"𝑠𝑜𝑓𝑡𝑚𝑎𝑥𝑠𝑜𝑓𝑡𝑚𝑎𝑥
𝑠𝑜𝑓𝑡𝑚𝑎𝑥
…"
IPNN MODEL ARCHITECTURE,0.25865580448065173,"N-dimensional 
Joint Sample Space"
IPNN MODEL ARCHITECTURE,0.2606924643584521,"𝑦1
…
…"
IPNN MODEL ARCHITECTURE,0.26272912423625255,𝑃(𝑦𝑙|𝑎𝑖1
IPNN MODEL ARCHITECTURE,0.26476578411405294,"1 , 𝑎𝑖2"
IPNN MODEL ARCHITECTURE,0.2668024439918534,"2 , … , 𝑎𝑖𝑁 𝑁)"
IPNN MODEL ARCHITECTURE,0.26883910386965376,Joint Sample Point (𝑎𝑖1
IPNN MODEL ARCHITECTURE,0.2708757637474542,"1 , 𝑎𝑖2"
IPNN MODEL ARCHITECTURE,0.2729124236252546,"2 , … , 𝑎𝑖𝑁 𝑁) 𝐴1 𝐴2 𝐴𝑁"
IPNN MODEL ARCHITECTURE,0.27494908350305497,"SPLIT into N-Parts 𝑎𝑖2 2
𝑎𝑖𝑁"
IPNN MODEL ARCHITECTURE,0.2769857433808554,"𝑁
𝐴1
𝐴2
𝐴𝑁"
IPNN MODEL ARCHITECTURE,0.2790224032586558,"…
Neural 
Network …"
IPNN MODEL ARCHITECTURE,0.28105906313645623,"𝑥1
𝑥𝑛
𝑥𝑘
…
…"
IPNN MODEL ARCHITECTURE,0.2830957230142566,Random Variable
IPNN MODEL ARCHITECTURE,0.285132382892057,Sample Space …
IPNN MODEL ARCHITECTURE,0.28716904276985744,"𝑦2
𝑦𝑙
𝑦𝑚"
IPNN MODEL ARCHITECTURE,0.2892057026476578,(a) model architecture 𝑋 𝐴1 𝑌
IPNN MODEL ARCHITECTURE,0.29124236252545826,"𝐴2
𝐴𝑁
…"
IPNN MODEL ARCHITECTURE,0.29327902240325865,(b) observation phase 𝑋 𝐴1 𝑌
IPNN MODEL ARCHITECTURE,0.2953156822810591,"𝐴2
𝐴𝑁
…"
IPNN MODEL ARCHITECTURE,0.2973523421588595,(c) inference phase
IPNN MODEL ARCHITECTURE,0.29938900203665986,"Figure 2: IPNN. (a) P
 
yl|a1
i1, a2
i2, . . . , aN
iN

is statistically calculated, not model weights. (b, c)
Independence illustration with Bayesian network."
IPNN MODEL ARCHITECTURE,0.3014256619144603,"1, 2, . . . , N, and each neuron in Aj is regarded as an event. After that, all the random variables
104"
IPNN MODEL ARCHITECTURE,0.3034623217922607,"together form the N-dimensional joint sample space, marked as A = (A1, A2, . . . , AN), and
105"
IPNN MODEL ARCHITECTURE,0.3054989816700611,"all the joint sample points are fully connected with all labels Y ∈{y1, y2, . . . , ym} via condi-
106"
IPNN MODEL ARCHITECTURE,0.3075356415478615,"tional probability P
 
Y = yl|A1 = a1
i1, A2 = a2
i2, . . . , AN = aN
iN

, or more compactly written as
107"
IPNN MODEL ARCHITECTURE,0.3095723014256619,"P
 
yl|a1
i1, a2
i2, . . . , aN
iN
1.2
108"
IPNN MODEL ARCHITECTURE,0.31160896130346233,"Split shape := {M1, M2, . . . , MN}
(1)"
DEFINITION OF INDETERMINATE PROBABILITY,0.3136456211812627,"4.2
Definition of Indeterminate Probability
109"
DEFINITION OF INDETERMINATE PROBABILITY,0.31568228105906315,"In classical probability theory, perform a random experiment (or given a sample xk), the event or
110"
DEFINITION OF INDETERMINATE PROBABILITY,0.31771894093686354,"joint event has only two states: happened or not happened. However, for IPNN, the model only
111"
DEFINITION OF INDETERMINATE PROBABILITY,0.319755600814664,"outputs the probability of an event state and its state is indeterminate, that’s why this paper is called
112"
DEFINITION OF INDETERMINATE PROBABILITY,0.32179226069246436,"IPNN. This difference makes the calculation of probability (especially joint probability) also different.
113"
DEFINITION OF INDETERMINATE PROBABILITY,0.32382892057026474,"Equation (2) and Equation (3) will later formulate this difference.
114"
DEFINITION OF INDETERMINATE PROBABILITY,0.3258655804480652,"Given an input sample xk (perform the kth random experiment), with Assumption 1 the indeterminate
115"
DEFINITION OF INDETERMINATE PROBABILITY,0.32790224032586557,"probability (model outputs) is defined as:
116"
DEFINITION OF INDETERMINATE PROBABILITY,0.329938900203666,"P

aj
ij | xk

= αj
ij(k)
(2)"
DEFINITION OF INDETERMINATE PROBABILITY,0.3319755600814664,"Assumption 1. Given an input sample X = xk, IF PMj
ij=1αj
ij(k) = 1 and αj
ij(k) ∈[0, 1], k =
117"
DEFINITION OF INDETERMINATE PROBABILITY,0.3340122199592668,"1, 2, . . . , n. THEN,
n
aj
1, aj
2, . . . , aj
Mj"
DEFINITION OF INDETERMINATE PROBABILITY,0.3360488798370672,"o
can be regarded as collectively exhaustive and exclusive
118"
DEFINITION OF INDETERMINATE PROBABILITY,0.3380855397148676,"events set, they are partitions of the sample space of random variable Aj, j = 1, 2, . . . , N.
119"
DEFINITION OF INDETERMINATE PROBABILITY,0.34012219959266804,"In classical probability, αj
ij(k) ∈{0, 1}, which indicates the state of event is 0 or 1.
120"
DEFINITION OF INDETERMINATE PROBABILITY,0.3421588594704684,"For joint event, given xk, using Assumption 2 and Equation (2), the joint indeterminate probability is
121"
DEFINITION OF INDETERMINATE PROBABILITY,0.34419551934826886,"formulated as:
122"
DEFINITION OF INDETERMINATE PROBABILITY,0.34623217922606925,"1All the probability is formulated compactly in this paper.
2Reading symbols see Appendix G."
DEFINITION OF INDETERMINATE PROBABILITY,0.34826883910386963,"P
 
a1
i1, a2
i2, . . . , aN
iN | xk

= QN
j=1αj
ij(k)
(3)"
DEFINITION OF INDETERMINATE PROBABILITY,0.35030549898167007,"Assumption 2. Given an input sample X = xk, A1, A2, . . . , AN is mutually independent.
123"
DEFINITION OF INDETERMINATE PROBABILITY,0.35234215885947046,"Where it can be easily proved,
124 P"
DEFINITION OF INDETERMINATE PROBABILITY,0.3543788187372709,"A
QN
j=1αj
ij(k)

= 1, k = 1, 2, . . . , n.
(4)"
DEFINITION OF INDETERMINATE PROBABILITY,0.3564154786150713,"In classical probability, QN
j=1αj
ij(k) ∈{0, 1}, which indicates the state of joint event is 0 or 1.
125"
DEFINITION OF INDETERMINATE PROBABILITY,0.35845213849287166,"Equation (2) and Equation (3) describes the uncertainty of the state of event

Aj = aj
ij

and joint
126"
DEFINITION OF INDETERMINATE PROBABILITY,0.3604887983706721,"event
 
A1 = a1
i1, A2 = a2
i2, . . . , AN = aN
iN

.
127"
OBSERVATION PHASE,0.3625254582484725,"4.3
Observation Phase
128"
OBSERVATION PHASE,0.3645621181262729,"In observation phase, the relationship between all random variables A1, A2, . . . , AN and Y is
129"
OBSERVATION PHASE,0.3665987780040733,"established after the whole observations, it is formulated as:
130"
OBSERVATION PHASE,0.36863543788187375,"P
 
yl | a1
i1, a2
i2, . . . , aN
iN

= P
 
yl, a1
i1, a2
i2, . . . , aN
iN
"
OBSERVATION PHASE,0.37067209775967414,"P
 
a1
i1, a2
i2, . . . , aN
iN

(5)"
OBSERVATION PHASE,0.3727087576374745,"Because the state of joint event is not determinate in IPNN, we cannot count its occurrence like
131"
OBSERVATION PHASE,0.37474541751527496,"classical probability. Hence, the joint probability is calculated according to total probability theorem
132"
OBSERVATION PHASE,0.37678207739307534,"over all samples X = (x1, x2, . . . , xn), and with Equation (3) we have:
133"
OBSERVATION PHASE,0.3788187372708758,"P
 
a1
i1, a2
i2, . . . , aN
iN

= Pn
k=1
 
P
 
a1
i1, a2
i2, . . . , aN
iN | xk

· P(xk)
"
OBSERVATION PHASE,0.38085539714867617,"= Pn
k=1
QN
j=1P

aj
ij | xk

· P(xk)

="
OBSERVATION PHASE,0.38289205702647655,"Pn
k=1
QN
j=1αj
ij(k)
 n (6)"
OBSERVATION PHASE,0.384928716904277,"Because Y = yl is sample label and Aj = aj
ij comes from model, it means Aj and Y come from
134"
OBSERVATION PHASE,0.3869653767820774,"different observer, so we can have Assumption 3 (see Figure 2c).
135"
OBSERVATION PHASE,0.3890020366598778,"Assumption 3. Given an input sample X = xk, Aj and Y is mutually independent in observation
136"
OBSERVATION PHASE,0.3910386965376782,"phase, j = 1, 2, . . . , N.
137"
OBSERVATION PHASE,0.39307535641547864,"Therefore, according to total probability theorem, Equation (3) and the above assumption, we derive:
138"
OBSERVATION PHASE,0.395112016293279,"P
 
yl, a1
i1, a2
i2, . . . , aN
iN

= Pn
k=1
 
P
 
yl, a1
i1, a2
i2, . . . , aN
iN | xk

· P(xk)
"
OBSERVATION PHASE,0.3971486761710794,"= Pn
k=1

P (yl | xk) · QN
j=1P

aj
ij | xk

· P(xk)
 ="
OBSERVATION PHASE,0.39918533604887985,"Pn
k=1

yl(k) · QN
j=1αj
ij(k)
 n (7)"
OBSERVATION PHASE,0.40122199592668023,"Substitute Equation (6) and Equation (7) into Equation (5), we have:
139"
OBSERVATION PHASE,0.40325865580448067,"P
 
yl|a1
i1, a2
i2, . . . , aN
iN

="
OBSERVATION PHASE,0.40529531568228105,"Pn
k=1

yl(k) · QN
j=1αj
ij(k)
"
OBSERVATION PHASE,0.4073319755600815,"Pn
k=1
QN
j=1αj
ij(k)

(8)"
OBSERVATION PHASE,0.4093686354378819,"Where it can be proved,
140"
OBSERVATION PHASE,0.41140529531568226,"Pm
l=1P
 
yl | a1
i1, a2
i2, . . . , aN
iN

= 1
(9)"
INFERENCE PHASE,0.4134419551934827,"4.4
Inference Phase
141"
INFERENCE PHASE,0.4154786150712831,"Given Aj, with Equation (8) (passed experience) label yl can be inferred, this inferred yl has no
142"
INFERENCE PHASE,0.4175152749490835,"pointing to any specific sample xk, incl. also new input sample xn+1, see Figure 2b. So we can have
143"
INFERENCE PHASE,0.4195519348268839,"following assumption:
144"
INFERENCE PHASE,0.4215885947046843,"Assumption 4. Given Aj, X and Y is mutually independent in inference phase, j = 1, 2, . . . , N.
145"
INFERENCE PHASE,0.42362525458248473,"Therefore, given a new input sample X = xn+1, according to total probability theorem over joint
146"
INFERENCE PHASE,0.4256619144602851,"sample space
 
a1
i1, a2
i2, . . . , aN
iN

∈A, with Assumption 4, Equation (3) and Equation (8), we have:
147"
INFERENCE PHASE,0.42769857433808556,"P A (yl | xn+1) =
X A"
INFERENCE PHASE,0.42973523421588594," 
P
 
yl, a1
i1, a2
i2, . . . , aN
iN | xn+1
 =
X A"
INFERENCE PHASE,0.4317718940936864," 
P
 
yl | a1
i1, a2
i2, . . . , aN
iN

· P
 
a1
i1, a2
i2, . . . , aN
iN | xn+1
 =
X A  "
INFERENCE PHASE,0.43380855397148677,"Pn
k=1

yl(k) · QN
j=1αj
ij(k)
"
INFERENCE PHASE,0.43584521384928715,"Pn
k=1
QN
j=1αj
ij(k)

· N
Y"
INFERENCE PHASE,0.4378818737270876,"j=1
αj
ij(n + 1)   (10)"
INFERENCE PHASE,0.439918533604888,"And the maximum posterior is the predicted label of an input sample:
148"
INFERENCE PHASE,0.4419551934826884,"ˆy :=
arg max
l∈{1,2,...,m}
P A (yl | xn+1)
(11)"
SUMMARY,0.4439918533604888,"4.5
Summary
149"
SUMMARY,0.4460285132382892,"Our most important contribution is that we propose a new general tractable probability Equation (10),
150"
SUMMARY,0.4480651731160896,"rewritten as:
151"
SUMMARY,0.45010183299389,P A (Y = yl | X = xn+1) = X A 
SUMMARY,0.45213849287169044,"








 nP k=1 "
SUMMARY,0.45417515274949083,"P (Y = yl | X = xk) ·
NQ"
SUMMARY,0.45621181262729127,"j=1
P

Aj = aj
ij | X = xk
! nP k=1 NQ"
SUMMARY,0.45824847250509165,"j=1
P

Aj = aj
ij | X = xk
!"
SUMMARY,0.46028513238289204,"|
{z
}
Observation phase · N
Y"
SUMMARY,0.4623217922606925,"j=1
P

Aj = aj
ij | X = xn+1
 "
SUMMARY,0.46435845213849286,"








"
SUMMARY,0.4663951120162933,"|
{z
}
Inference phase
(12)"
SUMMARY,0.4684317718940937,"Where X is random variable and X = xk denote the kth random experiment (or model input sample
152"
SUMMARY,0.47046843177189407,"xk), Y and A1:N are different discrete or continuous [10] random variables. This equation can be
153"
SUMMARY,0.4725050916496945,"applied to any random experiment, as long as the outcomes of random experiments are detected by
154"
SUMMARY,0.4745417515274949,"some observers (neural networks, humans, or others).
155"
SUMMARY,0.47657841140529533,"Our proposed theory is derived from three our proposed conditional mutual independency assumptions,
156"
SUMMARY,0.4786150712830957,"see Assumption 2 Assumption 3 and Assumption 4. However, in our opinion, these assumptions can
157"
SUMMARY,0.48065173116089616,"neither be proved nor falsified, and we do not find any exceptions until now. Since this theory can not
158"
SUMMARY,0.48268839103869654,"be mathematically proved, we can only validate it through experiment.
159"
SUMMARY,0.4847250509164969,"Finally, our proposed indeterminate probability theory is an extension of classical probability theory,
160"
SUMMARY,0.48676171079429736,"and classical probability theory is one special case to our theory. More details to understand our
161"
SUMMARY,0.48879837067209775,"theory intuitively, see Appendix B.
162"
TRAINING,0.4908350305498982,"5
Training
163"
TRAINING STRATEGY,0.49287169042769857,"5.1
Training Strategy
164"
TRAINING STRATEGY,0.49490835030549896,"Given an input sample xt from a mini batch, with a minor modification of Equation (10):
165"
TRAINING STRATEGY,0.4969450101832994,"P A (yl | xt) ≈
X A "
TRAINING STRATEGY,0.4989816700610998,"max(H + h(¯t), ϵ)"
TRAINING STRATEGY,0.5010183299389002,"max(G + g(¯t), ϵ) · N
Y"
TRAINING STRATEGY,0.5030549898167006,"j=1
αj
ij(t) "
TRAINING STRATEGY,0.505091649694501,"
(13)"
TRAINING STRATEGY,0.5071283095723014,"h(¯t) = Pb·¯t
k=b·(¯t−1)+1

yl(k) · QN
j=1αj
ij(k)

(14)"
TRAINING STRATEGY,0.5091649694501018,"g(¯t) = Pb·¯t
k=b·(¯t−1)+1
QN
j=1αj
ij(k)

(15)"
TRAINING STRATEGY,0.5112016293279023,"H = P¯t−1
k=max(1,¯t−T )h(k), for ¯t = 2, 3, . . .
(16)"
TRAINING STRATEGY,0.5132382892057027,"G = P¯t−1
k=max(1,¯t−T )g(k), for ¯t = 2, 3, . . .
(17)"
TRAINING STRATEGY,0.515274949083503,"Algorithm 1 IPNN training
Input: A sample xt from mini-batch
Parameter: Split shape, forget number T, ϵ, learning rate η.
Output: Posterior P A (yl | xt)"
TRAINING STRATEGY,0.5173116089613035,"1: Declare default variables: H, G, hList, gList
2: for ¯t = 1, 2, . . . Until Convergence do
3:
Compute h, g with Equation (14) and Equation (15)
4:
Record: hList.append(h), gList.append(g)
5:
if ¯t > T then
6:
Forget: H = H −hList[0], G = G −gList[0]
7:
Remove first element from hList, gList
8:
end if
9:
Compute posterior with Equation (13): P A (yl | xt)
10:
Compute loss with Equation (18): L(θ)
11:
Update model parameter: θ = θ −η∇L(θ)
12:
Update for next loop: H = H + h, G = G + g
13: end for
14: return model and the posterior"
TRAINING STRATEGY,0.5193482688391039,"Where b is for batch size, ¯t
=
166
 t"
TRAINING STRATEGY,0.5213849287169042,"b

, t
=
1, 2, . . . , n.
Hyper-
167"
TRAINING STRATEGY,0.5234215885947047,"parameter T is for forgetting use, i.e.,
168"
TRAINING STRATEGY,0.5254582484725051,"H and G are calculated from the re-
169"
TRAINING STRATEGY,0.5274949083503055,"cent T batches. Hyper-parameter T
170"
TRAINING STRATEGY,0.5295315682281059,"is introduced because at beginning of
171"
TRAINING STRATEGY,0.5315682281059063,"training phase the calculated result
172"
TRAINING STRATEGY,0.5336048879837068,"with Equation (8) is not good yet. And
173"
TRAINING STRATEGY,0.5356415478615071,"the ϵ on the denominator is to avoid di-
174"
TRAINING STRATEGY,0.5376782077393075,"viding zero, the ϵ on the numerator is
175"
TRAINING STRATEGY,0.539714867617108,"to have an initial value of 1. Besides,
176"
TRAINING STRATEGY,0.5417515274949084,"H and G are not needed for gradi-
177"
TRAINING STRATEGY,0.5437881873727087,"ent updating during back-propagation.
178"
TRAINING STRATEGY,0.5458248472505092,"The detailed algorithm implementa-
179"
TRAINING STRATEGY,0.5478615071283096,"tion is shown in Algorithm 1.
180"
TRAINING STRATEGY,0.5498981670061099,"We use cross entropy as loss function:
181"
TRAINING STRATEGY,0.5519348268839104,"L = −Pm
l=1
 
yl(k) · log P A (yl | xt)
 (18)"
TRAINING STRATEGY,0.5539714867617108,"With Equation (13) we can get that P A (yl | x1) = 1 for the first input sample if yl is the ground truth
182"
TRAINING STRATEGY,0.5560081466395111,"and batch size is 1. Therefore, for IPNN the loss may increase at the beginning and fall back again
183"
TRAINING STRATEGY,0.5580448065173116,"while training.
184"
TRAINING STRATEGY,0.560081466395112,"5.2
Multi-degree Classification (Optional)
185"
TRAINING STRATEGY,0.5621181262729125,"In IPNN, the model outputs N different random variables A1, A2, . . . , AN, if we use part of them to
186"
TRAINING STRATEGY,0.5641547861507128,"form sub-joint sample spaces, we are able of doing sub classification task, the sub-joint spaces are
187"
TRAINING STRATEGY,0.5661914460285132,"defined as Λ1 ⊂A, Λ2 ⊂A, . . . The number of sub-joint sample spaces is:
188 N
X j=1 N
j 
= N
X j=1"
TRAINING STRATEGY,0.5682281059063137,"
N!
j!(N −j)!"
TRAINING STRATEGY,0.570264765784114,"
(19)"
TRAINING STRATEGY,0.5723014256619144,"If the input samples are additionally labeled for part of sub-joint sample spaces3, defined as Y τ ∈
189"
TRAINING STRATEGY,0.5743380855397149,"{yτ
1, yτ
2, . . . , yτ
mτ }. The sub classification task can be represented as

X, Λ1, Y 1
,

X, Λ2, Y 2
, . . .
190"
TRAINING STRATEGY,0.5763747454175153,"With Equation (18) we have,
191"
TRAINING STRATEGY,0.5784114052953157,Lτ = −Pmτ
TRAINING STRATEGY,0.5804480651731161,"l=1

yτ
l (k) · log P Λτ (yτ
l | xt)

, τ = 1, 2, . . .
(20)"
TRAINING STRATEGY,0.5824847250509165,"Together with the main loss, the overall loss is L + L1 + L2 + . . . In this way, we can perform
192"
TRAINING STRATEGY,0.5845213849287169,"multi-degree classification task. The additional labels can guide the convergence of the joint sample
193"
TRAINING STRATEGY,0.5865580448065173,"spaces and speed up the training process, as discussed later in Appendix D.1.
194"
TRAINING STRATEGY,0.5885947046843177,"3It is labelling of input samples, not sub-joint sample points."
MULTI-DEGREE UNSUPERVISED CLUSTERING,0.5906313645621182,"5.3
Multi-degree Unsupervised Clustering
195"
MULTI-DEGREE UNSUPERVISED CLUSTERING,0.5926680244399185,"If there are no additional labels for the sub-joint sample spaces, the model are actually doing
196"
MULTI-DEGREE UNSUPERVISED CLUSTERING,0.594704684317719,"unsupervised clustering while training. And every sub-joint sample space describes one kind of
197"
MULTI-DEGREE UNSUPERVISED CLUSTERING,0.5967413441955194,"clustering result, we have Equation (19) number of clustering situations in total.
198"
DESIGNATION OF JOINT SAMPLE SPACE,0.5987780040733197,"5.4
Designation of Joint Sample Space
199"
DESIGNATION OF JOINT SAMPLE SPACE,0.6008146639511202,"As in Appendix C proved, we have following proposition:
200"
DESIGNATION OF JOINT SAMPLE SPACE,0.6028513238289206,"Proposition 1. For P(yl|xk) = yl(k) ∈{0, 1} hard label case, IPNN converges to global minimum
201"
DESIGNATION OF JOINT SAMPLE SPACE,0.604887983706721,"only when P
 
yl|a1
i1, a2
i2, . . . , aN
iN

= 1, for QN
j=1 αj
ij(t) > 0, ij = 1, 2, . . . , Mj. In other word,
202"
DESIGNATION OF JOINT SAMPLE SPACE,0.6069246435845214,"each joint sample point corresponds to an unique category. However, a category can correspond to
203"
DESIGNATION OF JOINT SAMPLE SPACE,0.6089613034623218,"one or more joint sample points.
204"
DESIGNATION OF JOINT SAMPLE SPACE,0.6109979633401222,"Corollary 1. The necessary condition of achieving the global minimum is when the split shape
205"
DESIGNATION OF JOINT SAMPLE SPACE,0.6130346232179226,"defined in Equation (1) satisfies: QN
j=1Mj ≥m, where m is the number of classes. That is, for a
206"
DESIGNATION OF JOINT SAMPLE SPACE,0.615071283095723,"classification task, the number of all joint sample points is greater than the classification classes.
207"
DESIGNATION OF JOINT SAMPLE SPACE,0.6171079429735234,"Theoretically, if model with 100 output nodes are split into 10 equal parts, it can classify 10 billion
208"
DESIGNATION OF JOINT SAMPLE SPACE,0.6191446028513238,"categories, validation result see Appendix D.1. Besides, the unsupervised clustering (Section 5.3)
209"
DESIGNATION OF JOINT SAMPLE SPACE,0.6211812627291242,"depends on the input sample distributions, the split shape shall not violate from multi-degree clustering.
210"
DESIGNATION OF JOINT SAMPLE SPACE,0.6232179226069247,"For example, if the main attributes of one dataset shows three different colors, and your split shape is
211"
DESIGNATION OF JOINT SAMPLE SPACE,0.6252545824847251,"{2, 2, . . . }, this will hinder the unsupervised clustering, in this case, the shape of one random variable
212"
DESIGNATION OF JOINT SAMPLE SPACE,0.6272912423625254,"is better set to 3. And as in Appendix D also analyzed, there are two local minimum situations,
213"
DESIGNATION OF JOINT SAMPLE SPACE,0.6293279022403259,"improper split shape will make IPNN go to local minimum.
214"
DESIGNATION OF JOINT SAMPLE SPACE,0.6313645621181263,"In addition, the latter part from Proposition 1 also implies that IPNN may be able of doing further
215"
DESIGNATION OF JOINT SAMPLE SPACE,0.6334012219959266,"unsupervised classification task, this is beyond the scope of this discussion.
216"
EXPERIMENTS AND RESULTS,0.6354378818737271,"6
Experiments and Results
217"
UNSUPERVISED CLUSTERING,0.6374745417515275,"6.1
Unsupervised Clustering
218"
UNSUPERVISED CLUSTERING,0.639511201629328,"0
1
2
3
4
5
6
7
8
9
true labels"
UNSUPERVISED CLUSTERING,0.6415478615071283,"0
1
cluster id"
UNSUPERVISED CLUSTERING,0.6435845213849287,"7
89
5.6
7.2
92
8.7
10
95
15
94"
UNSUPERVISED CLUSTERING,0.6456211812627292,"93
11
94
93
7.6
91
90
5
85
6.3 0 20 40 60 80 100"
UNSUPERVISED CLUSTERING,0.6476578411405295,percentage [%]
UNSUPERVISED CLUSTERING,0.6496945010183299,percentage =
UNSUPERVISED CLUSTERING,0.6517311608961304,"1
round ·"
UNSUPERVISED CLUSTERING,0.6537678207739308,"round
X i=1"
UNSUPERVISED CLUSTERING,0.6558044806517311,"number of samples with label l
in one cluster at ith round
number of samples with label l"
UNSUPERVISED CLUSTERING,0.6578411405295316,"Figure 3: Unsupervised clustering results on MNIST: test accuracy 95.1 ± 0.4, ϵ = 2, batch size
b = 64, forget number T = 5, epoch is 5 per round. The test was repeated for 876 rounds with same
configuration (different random seeds) in order to check the stability of clustering performance, each
round clustering result is aligned using Jaccard similarity [28]."
UNSUPERVISED CLUSTERING,0.659877800407332,"As in Section 5.3 discussed, IPNN is able of performing unsupervised clustering, we evaluate it
219"
UNSUPERVISED CLUSTERING,0.6619144602851323,"on MNIST. The split shape is set to {2, 10}, it means we have two random variables, and the first
220"
UNSUPERVISED CLUSTERING,0.6639511201629328,"random variable is used to divide MNIST labels 0, 1, . . . 9 into two clusters. The cluster results is
221"
UNSUPERVISED CLUSTERING,0.6659877800407332,"shown in Figure 3.
222"
UNSUPERVISED CLUSTERING,0.6680244399185336,"We find only when ϵ in Equation (13) is set to a relative high value that IPNN prefers to put number
223"
UNSUPERVISED CLUSTERING,0.670061099796334,"1,4,7,9 into one cluster and the rest into another cluster, otherwise, the clustering results is always
224"
UNSUPERVISED CLUSTERING,0.6720977596741344,"different for each round training. The reason is unknown, our intuition is that high ϵ makes that each
225"
UNSUPERVISED CLUSTERING,0.6741344195519349,"category catch the free joint sample point more harder, categories have similar attributes together will
226"
UNSUPERVISED CLUSTERING,0.6761710794297352,"be more possible to catch the free joint sample point.
227"
HYPER-PARAMETER ANALYSIS,0.6782077393075356,"6.2
Hyper-parameter Analysis
228"
HYPER-PARAMETER ANALYSIS,0.6802443991853361,"IPNN has two import hyper-parameters: split shape and forget number T. In this section, we have
229"
HYPER-PARAMETER ANALYSIS,0.6822810590631364,"analyzed it with test on MNIST, batch size is set to 64, ϵ = 10−6. As shown in Figure 4a, if the
230"
HYPER-PARAMETER ANALYSIS,0.6843177189409368,"number of joint sample points is smaller than 10, IPNN is not able of making a full classification and
231"
HYPER-PARAMETER ANALYSIS,0.6863543788187373,"its test accuracy is proportional to number of joint sample points, as number of joint sample points
232"
HYPER-PARAMETER ANALYSIS,0.6883910386965377,"increases over 10, IPNN goes to global minimum for both 3 cases, this result is consistent with our
233"
HYPER-PARAMETER ANALYSIS,0.6904276985743381,"analysis. However, we have exceptions, the accuracy of split shape with {2, 5} and {2, 6} is not high.
234"
HYPER-PARAMETER ANALYSIS,0.6924643584521385,"From Figure 3 we know that for the first random variable, IPNN sometimes tends to put number
235"
HYPER-PARAMETER ANALYSIS,0.6945010183299389,"1,4,7,9 into one cluster and the rest into another cluster, so this cluster result request that the split
236"
HYPER-PARAMETER ANALYSIS,0.6965376782077393,"shape need to be set minimums to {2, ≥6} in order to have enough free joint sample points. That’s
237"
HYPER-PARAMETER ANALYSIS,0.6985743380855397,"why the accuracy of split shape with {2, 5} is not high. (For {2, 6} case, only three numbers are in
238"
HYPER-PARAMETER ANALYSIS,0.7006109979633401,"one cluster.)
239"
HYPER-PARAMETER ANALYSIS,0.7026476578411406,"Another test in Figure 4b shows that IPNN will go to local minimum as forget number T increases
240"
HYPER-PARAMETER ANALYSIS,0.7046843177189409,"and cannot go to global minimum without further actions, hence, a relative small forget number T
241"
HYPER-PARAMETER ANALYSIS,0.7067209775967414,"shall be found with try and error.
242"
HYPER-PARAMETER ANALYSIS,0.7087576374745418,"5
10
15
20
25
number of joint sample points 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
HYPER-PARAMETER ANALYSIS,0.7107942973523421,accuracy
D SPLIT SHAPE,0.7128309572301426,"1D Split Shape
2D Split Shape
3D Split Shape"
D SPLIT SHAPE,0.714867617107943,(a) split shape
D SPLIT SHAPE,0.7169042769857433,"0
200
400
600
800
forget number T 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
D SPLIT SHAPE,0.7189409368635438,accuracy
D SPLIT SHAPE,0.7209775967413442,accuracy
D SPLIT SHAPE,0.7230142566191446,(b) forget number T
D SPLIT SHAPE,0.725050916496945,"Figure 4: (a) Impact Analysis of split shape with MNIST: 1D split shape is for {τ}, τ = 2, 3, . . . , 24.
2D split shape is for {2, τ}, τ = 2, 3, . . . , 12. 3D split shape is for {2, 2, τ}, τ = 2, 3, . . . , 6. The
x-axis is the number of joint sample points calculated with QN
j=1Mj, see Equation (1).
(b) Impact Analysis of forget number T with MNIST: Split shape is {10}."
EVALUATION ON DATASETS,0.7270875763747454,"6.3
Evaluation on Datasets
243"
EVALUATION ON DATASETS,0.7291242362525459,"Table 4: Test accuracy: split shape for all these
datasets is set to {2, 2, 5}; backbone is FCN for
MNIST and Fashion-MNIST, Resnet50 [25] for
CIFAR10 and STL10."
EVALUATION ON DATASETS,0.7311608961303462,"Dataset
IPNN
Simple-Softmax"
EVALUATION ON DATASETS,0.7331975560081466,"MNIST
95.8 ± 0.5
97.6 ± 0.2
Fashion-
MNIST
84.5 ± 1.0
87.8 ± 0.2"
EVALUATION ON DATASETS,0.7352342158859471,"CIFAR10
83.6 ± 0.5
85.7 ± 0.9
STL10
91.6 ± 4.0
94.7 ± 0.7"
EVALUATION ON DATASETS,0.7372708757637475,"Further results on MNIST [29],
Fashion-
244"
EVALUATION ON DATASETS,0.7393075356415478,"MNIST [30], CIFAR10 [31] and STL10 [32]
245"
EVALUATION ON DATASETS,0.7413441955193483,"show that our proposed indeterminate probabil-
246"
EVALUATION ON DATASETS,0.7433808553971487,"ity theory is valid, the backbone between IPNN
247"
EVALUATION ON DATASETS,0.745417515274949,"and ‘Simple-Softmax’ is the same, the last layer
248"
EVALUATION ON DATASETS,0.7474541751527495,"of the latter one is connected to softmax func-
249"
EVALUATION ON DATASETS,0.7494908350305499,"tion. Although IPNN does not reach any SOTA,
250"
EVALUATION ON DATASETS,0.7515274949083504,"the results are very important evidences to our
251"
EVALUATION ON DATASETS,0.7535641547861507,"proposed mutual independence assumptions, see
252"
EVALUATION ON DATASETS,0.7556008146639511,"Assumption 2 Assumption 3 and Assumption 4.
253"
CONCLUSION,0.7576374745417516,"7
Conclusion
254"
CONCLUSION,0.7596741344195519,"For a classification task, we proposed an approach to extract the attributes of input samples as random
255"
CONCLUSION,0.7617107942973523,"variables, and these variables are used to form a large joint sample space. After IPNN converges
256"
CONCLUSION,0.7637474541751528,"to global minimum, each joint sample point will correspond to an unique category, as discussed in
257"
CONCLUSION,0.7657841140529531,"Proposition 1. As the joint sample space increases exponentially, the classification capability of IPNN
258"
CONCLUSION,0.7678207739307535,"will increase accordingly.
259"
CONCLUSION,0.769857433808554,"We can then use the advantages of classical probability theory, for example, for very large joint
260"
CONCLUSION,0.7718940936863544,"sample space, we can use the Bayesian network approach or mutual independence among variables
261"
CONCLUSION,0.7739307535641547,"(see Appendix E) to simplify the model and improve the inference efficiency, in this way, a more
262"
CONCLUSION,0.7759674134419552,"complex Bayesian network could be built for more complex reasoning task.
263"
REFERENCES,0.7780040733197556,"References
264"
REFERENCES,0.780040733197556,"[1] Irving Biederman. Recognition-by-components: a theory of human image understanding. In
265"
REFERENCES,0.7820773930753564,"Psychological review, pages 115–147, 1987. doi: 10.1037/0033-295X.94.2.115.
266"
REFERENCES,0.7841140529531568,"[2] Christoph H. Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen
267"
REFERENCES,0.7861507128309573,"object classes by between-class attribute transfer. In 2009 IEEE Conference on Computer Vision
268"
REFERENCES,0.7881873727087576,"and Pattern Recognition, pages 951–958, 2009. doi: 10.1109/CVPR.2009.5206594.
269"
REFERENCES,0.790224032586558,"[3] Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue, Leonid Sigal, and Shaogang Gong.
270"
REFERENCES,0.7922606924643585,"Recent advances in zero-shot recognition: Toward data-efficient understanding of visual content.
271"
REFERENCES,0.7942973523421588,"IEEE Signal Processing Magazine, 35(1):112–125, 2018. doi: 10.1109/MSP.2017.2763441.
272"
REFERENCES,0.7963340122199593,"[4] Abdul Arfat Mohammed and Venkatesh Umaashankar. Effectiveness of hierarchical softmax in
273"
REFERENCES,0.7983706720977597,"large scale classification tasks. In 2018 International Conference on Advances in Computing,
274"
REFERENCES,0.8004073319755601,"Communications and Informatics (ICACCI), pages 1090–1094, 2018. doi: 10.1109/ICACCI.
275"
REFERENCES,0.8024439918533605,"2018.8554637.
276"
REFERENCES,0.8044806517311609,"[5] Yonghui Cao. Study of the bayesian networks. In 2010 International Conference on E-Health
277"
REFERENCES,0.8065173116089613,"Networking Digital Ecosystems and Technologies (EDT), volume 1, pages 172–174, 2010. doi:
278"
REFERENCES,0.8085539714867617,"10.1109/EDT.2010.5496612.
279"
REFERENCES,0.8105906313645621,"[6] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114,
280"
REFERENCES,0.8126272912423625,"2014.
281"
REFERENCES,0.814663951120163,"[7] Chan Su and Chia-Jen Chou. A neural network-based approach for statistical probability
282"
REFERENCES,0.8167006109979633,"distribution recognition. Quality Engineering, 18:293 – 297, 2006.
283"
REFERENCES,0.8187372708757638,"[8] Ozan Kocada˘glı and Barı¸s A¸sıkgil. Nonlinear time series forecasting with bayesian neural
284"
REFERENCES,0.8207739307535642,"networks. Expert Systems with Applications, 41(15):6596–6610, 2014. ISSN 0957-4174.
285"
REFERENCES,0.8228105906313645,"doi: https://doi.org/10.1016/j.eswa.2014.04.035. URL https://www.sciencedirect.com/
286"
REFERENCES,0.824847250509165,"science/article/pii/S0957417414002589.
287"
REFERENCES,0.8268839103869654,"[9] Jorge Morales and Wen Yu. Improving neural network’s performance using bayesian infer-
288"
REFERENCES,0.8289205702647657,"ence. Neurocomputing, 461:319–326, 2021. ISSN 0925-2312. doi: https://doi.org/10.1016/j.
289"
REFERENCES,0.8309572301425662,"neucom.2021.07.054. URL https://www.sciencedirect.com/science/article/pii/
290"
REFERENCES,0.8329938900203666,"S0925231221011309.
291"
REFERENCES,0.835030549898167,"[10] Anonymous. Continuous indeterminate probability neural network. ICCV 2023 Submission ID
292"
REFERENCES,0.8370672097759674,"4297, Supplied as additional material cipnn.pdf.
293"
REFERENCES,0.8391038696537678,"[11] Diederik P. Kingma and Max Welling. 2019.
294"
REFERENCES,0.8411405295315683,"[12] YooJung Choi, Antonio Vergari, and Guy Van den Broeck. Probabilistic circuits: A unifying
295"
REFERENCES,0.8431771894093686,"framework for tractable probabilistic models. oct 2020. URL http://starai.cs.ucla.edu/
296"
REFERENCES,0.845213849287169,"papers/ProbCirc20.pdf.
297"
REFERENCES,0.8472505091649695,"[13] Meihua Dang, Anji Liu, and Guy Van den Broeck. Sparse probabilistic circuits via pruning and
298"
REFERENCES,0.8492871690427699,"growing. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,
299"
REFERENCES,0.8513238289205702,"Advances in Neural Information Processing Systems, volume 35, pages 28374–28385. Curran
300"
REFERENCES,0.8533604887983707,"Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/
301"
REFERENCES,0.8553971486761711,"2022/file/b6089408f4893289296ad0499783b3a6-Paper-Conference.pdf.
302"
REFERENCES,0.8574338085539714,"[14] Adnan Darwiche. A logical approach to factoring belief networks. In Proceedings of the Eights
303"
REFERENCES,0.8594704684317719,"International Conference on Principles of Knowledge Representation and Reasoning, KR’02,
304"
REFERENCES,0.8615071283095723,"page 409–420, San Francisco, CA, USA, 2002. Morgan Kaufmann Publishers Inc. ISBN
305"
REFERENCES,0.8635437881873728,"1558605541.
306"
REFERENCES,0.8655804480651731,"[15] Daniel Lowd and Pedro Domingos. Learning arithmetic circuits. In Proceedings of the Twenty-
307"
REFERENCES,0.8676171079429735,"Fourth Conference on Uncertainty in Artificial Intelligence, UAI’08, page 383–392, Arlington,
308"
REFERENCES,0.869653767820774,"Virginia, USA, 2008. AUAI Press. ISBN 0974903949.
309"
REFERENCES,0.8716904276985743,"[16] Hoifung Poon and Pedro Domingos. Sum-product networks: A new deep architecture. In
310"
REFERENCES,0.8737270875763747,"2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), pages
311"
REFERENCES,0.8757637474541752,"689–690, 2011. doi: 10.1109/ICCVW.2011.6130310.
312"
REFERENCES,0.8778004073319755,"[17] Tahrima Rahman, Prasanna Kothalkar, and Vibhav Gogate. Cutset networks: A simple, tractable,
313"
REFERENCES,0.879837067209776,"and scalable approach for improving the accuracy of chow-liu trees. In Machine Learning and
314"
REFERENCES,0.8818737270875764,"Knowledge Discovery in Databases, page 630–645, Berlin, Heidelberg, 2014. Springer-Verlag.
315"
REFERENCES,0.8839103869653768,"ISBN 978-3-662-44850-2. doi: 10.1007/978-3-662-44851-9_40. URL https://doi.org/
316"
REFERENCES,0.8859470468431772,"10.1007/978-3-662-44851-9_40.
317"
REFERENCES,0.8879837067209776,"[18] Radu Marinescu and Rina Dechter. And/or branch-and-bound for graphical models. In Pro-
318"
REFERENCES,0.890020366598778,"ceedings of the 19th International Joint Conference on Artificial Intelligence, IJCAI’05, page
319"
REFERENCES,0.8920570264765784,"224–229, San Francisco, CA, USA, 2005. Morgan Kaufmann Publishers Inc.
320"
REFERENCES,0.8940936863543788,"[19] Doga Kisa, Guy Van den Broeck, Arthur Choi, and Adnan Darwiche. Probabilistic sentential
321"
REFERENCES,0.8961303462321792,"decision diagrams. In Proceedings of the Fourteenth International Conference on Principles of
322"
REFERENCES,0.8981670061099797,"Knowledge Representation and Reasoning, KR’14, page 558–567. AAAI Press, 2014. ISBN
323"
REFERENCES,0.90020366598778,"1577356578.
324"
REFERENCES,0.9022403258655805,"[20] Yoon Kim, Sam Wiseman, and Alexander M. Rush. A tutorial on deep latent variable models
325"
REFERENCES,0.9042769857433809,"of natural language, 2018.
326"
REFERENCES,0.9063136456211812,"[21] Michalis Titsias and Miguel Lázaro-Gredilla. Doubly stochastic variational bayes for non-
327"
REFERENCES,0.9083503054989817,"conjugate inference.
In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st
328"
REFERENCES,0.9103869653767821,"International Conference on Machine Learning, volume 32 of Proceedings of Machine
329"
REFERENCES,0.9124236252545825,"Learning Research, pages 1971–1979, Bejing, China, 22–24 Jun 2014. PMLR.
URL
330"
REFERENCES,0.9144602851323829,"https://proceedings.mlr.press/v32/titsias14.html.
331"
REFERENCES,0.9164969450101833,"[22] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation
332"
REFERENCES,0.9185336048879837,"and approximate inference in deep generative models. In Proceedings of the 31st International
333"
REFERENCES,0.9205702647657841,"Conference on International Conference on Machine Learning - Volume 32, ICML’14, page
334"
REFERENCES,0.9226069246435845,"II–1278–II–1286. JMLR.org, 2014.
335"
REFERENCES,0.924643584521385,"[23] Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep
336"
REFERENCES,0.9266802443991853,"autoregressive networks. ArXiv, abs/1310.8499, 2013.
337"
REFERENCES,0.9287169042769857,"[24] Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An
338"
REFERENCES,0.9307535641547862,"introduction to variational methods for graphical models. Mach. Learn., 37(2):183–233, nov
339"
REFERENCES,0.9327902240325866,"1999. ISSN 0885-6125. doi: 10.1023/A:1007665907178. URL https://doi.org/10.1023/
340"
REFERENCES,0.9348268839103869,"A:1007665907178.
341"
REFERENCES,0.9368635437881874,"[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
342"
REFERENCES,0.9389002036659878,"recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
343"
REFERENCES,0.9409368635437881,"pages 770–778, 2016. doi: 10.1109/CVPR.2016.90.
344"
REFERENCES,0.9429735234215886,"[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
345"
REFERENCES,0.945010183299389,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st
346"
REFERENCES,0.9470468431771895,"International Conference on Neural Information Processing Systems, NIPS’17, page 6000–6010,
347"
REFERENCES,0.9490835030549898,"Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.
348"
REFERENCES,0.9511201629327902,"[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
349"
REFERENCES,0.9531568228105907,"deep bidirectional transformers for language understanding. ArXiv, abs/1810.04805, 2019.
350"
REFERENCES,0.955193482688391,"[28] Edward Raff and Charles Nicholas. An alternative to ncd for large sequences, lempel-ziv
351"
REFERENCES,0.9572301425661914,"jaccard distance. In Proceedings of the 23rd ACM SIGKDD International Conference on
352"
REFERENCES,0.9592668024439919,"Knowledge Discovery and Data Mining, KDD ’17, page 1007–1015, New York, NY, USA, 2017.
353"
REFERENCES,0.9613034623217923,"Association for Computing Machinery. ISBN 9781450348874. doi: 10.1145/3097983.3098111.
354"
REFERENCES,0.9633401221995926,"URL https://doi.org/10.1145/3097983.3098111.
355"
REFERENCES,0.9653767820773931,"[29] Li Deng. The mnist database of handwritten digit images for machine learning research [best of
356"
REFERENCES,0.9674134419551935,"the web]. IEEE Signal Processing Magazine, 29(6):141–142, 2012. doi: 10.1109/MSP.2012.
357"
REFERENCES,0.9694501018329938,"2211477.
358"
REFERENCES,0.9714867617107943,"[30] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for
359"
REFERENCES,0.9735234215885947,"benchmarking machine learning algorithms, 2017.
360"
REFERENCES,0.9755600814663951,"[31] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
361"
REFERENCES,0.9775967413441955,"2009.
362"
REFERENCES,0.9796334012219959,"[32] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsu-
363"
REFERENCES,0.9816700610997964,"pervised feature learning. In Geoffrey Gordon, David Dunson, and Miroslav Dudík, editors,
364"
REFERENCES,0.9837067209775967,"Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,
365"
REFERENCES,0.9857433808553971,"volume 15 of Proceedings of Machine Learning Research, pages 215–223, Fort Lauderdale, FL,
366"
REFERENCES,0.9877800407331976,"USA, 11–13 Apr 2011. PMLR. URL https://proceedings.mlr.press/v15/coates11a.
367"
REFERENCES,0.9898167006109979,"html.
368"
REFERENCES,0.9918533604887984,"[33] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activa-
369"
REFERENCES,0.9938900203665988,"tion. In Proceedings of the 31st International Conference on Neural Information Processing
370"
REFERENCES,0.9959266802443992,"Systems, NIPS’17, page 597–607, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN
371"
REFERENCES,0.9979633401221996,"9781510860964.
372"
