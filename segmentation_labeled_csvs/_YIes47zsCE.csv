Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0031446540880503146,"We address the challenge of identifying multiple change points in a group of independent
time series, assuming these change points occur simultaneously in all series and their number is
unknown. The search for the best segmentation can be expressed as a minimization problem
over a given cost function. We focus on dynamic programming algorithms that solve this
problem exactly. When the number of changes is proportional to data length, an inequality-based
pruning rule encoded in the PELT algorithm leads to a linear time complexity. Another type of
pruning, called functional pruning, gives a close-to-linear time complexity whatever the number
of changes, but only for the analysis of univariate time series. We propose a few extensions of
functional pruning for multiple independent time series based on the use of simple geometric
shapes (balls and hyperrectangles). We focus on the Gaussian case, but some of our rules can be
easily extended to the exponential family. In a simulation study we compare the computational
efficiency of different geometric-based pruning rules. We show that for a small number of time
series some of them ran significantly faster than inequality-based approaches in particular when
the underlying number of changes is small compared to the data length."
ABSTRACT,0.006289308176100629,"Keywords: multivariate time series, multiple change point detection, dynamic programming, func-
tional pruning, computational geometry"
OTHER,0.009433962264150943,Contents
OTHER,0.012578616352201259,"Introduction
2"
OTHER,0.015723270440251572,"1
Functional Pruning for Multiple Time Series
3
1.1
Model and Cost
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Functional Pruning Optimal Partitioning Algorithm . . . . . . . . . . . . . . . . . .
4
1.3
Geometric Formulation of Functional Pruning . . . . . . . . . . . . . . . . . . . . .
6"
OTHER,0.018867924528301886,"2
Geometric Functional Pruning Optimal Partitioning
8
2.1
General Principle of GeomFPOP . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8"
OTHER,0.0220125786163522,1Corresponding author: liudmila.pishchagina@univ-evry.fr
OTHER,0.025157232704402517,"3
Approximation Operators ⋂̃𝑍and ⧵̃𝑍
10
3.1
S-type Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
3.2
R-type Approximation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11"
OTHER,0.02830188679245283,"4
Simulation Study of GeomFPOP
13
4.1
The Number of Change Point Candidates stored over Time . . . . . . . . . . . . . .
14
4.2
Empirical Time Complexity of GeomFPOP . . . . . . . . . . . . . . . . . . . . . . .
15
4.3
Empirical Time Complexity of a Randomized GeomFPOP . . . . . . . . . . . . . . .
15
4.4
Empirical Complexity of the Algorithm as a Function of 𝑝
. . . . . . . . . . . . . .
16
4.5
Run Time as a Function of the Number of Segments . . . . . . . . . . . . . . . . . .
17"
OTHER,0.031446540880503145,"Acknowledgments
18"
OTHER,0.03459119496855346,"5
Supplements
18
5.1
Examples of Likelihood-Based Cost Functions
. . . . . . . . . . . . . . . . . . . . .
18
5.2
Intersection and Inclusion of Two p-balls . . . . . . . . . . . . . . . . . . . . . . . .
18
5.3
Intersection and Inclusion Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
5.4
Proof of Proposition 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
5.5
Optimization Strategies for GeomFPOP (R-type) . . . . . . . . . . . . . . . . . . . .
21
5.6
The number of change point candidates in time: GeomFPOP vs. PELT . . . . . . . .
22
5.7
Run time of the algorithm by multivariate time series with changes in subset of
dimension
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22"
OTHER,0.03773584905660377,"References
22"
INTRODUCTION,0.040880503144654086,Introduction
INTRODUCTION,0.0440251572327044,"A National Research Council report (Data et al. 2013) has identified change point detection as one
of the “inferential giants” in massive data analysis. Detecting change points, either a posteriori or
online, is important in areas as diverse as bioinformatics (Olshen et al. 2004; Picard et al. 2005),
econometrics (Bai and Perron 2003; Aue et al. 2006), medicine (Bosc et al. 2003; Staudacher et al.
2005; Malladi, Kalamangalam, and Aazhang 2013), climate and oceanography (Reeves et al. 2007;
Ducré-Robitaille, Vincent, and Boulet 2003; Killick, Fearnhead, and Eckley 2012; Naoki and Kurths
2010), finance (Andreou and Ghysels 2002; Fryzlewicz 2014), autonomous driving (Galceran et al.
2017), entertainment (Rybach et al. 2009; Radke et al. 2005; Davis, Lee, and Rodriguez-Yam 2006),
computer vision (Ranganathan 2012) or neuroscience (Jewell, Fearnhead, and Witten 2019). The most
common and prototypical change point detection problem is that of detecting changes in mean of a
univariate Gaussian signal and a large number of approaches have been proposed to perform this task
(see among many others (Yao 1984; Lebarbier 2005; Harchaoui and Lévy-Leduc 2010; Frick, Munk,
and Sieling 2013; Anastasiou and Fryzlewicz 2022) and the reviews (Truong, Oudre, and Vayatis 2020;
Aminikhanghahi and Cook 2017))."
INTRODUCTION,0.04716981132075472,"Penalized cost methods. Some of these methods optimize a penalized cost function (see for example
(Lebarbier 2005; Auger and Lawrence 1989; Jackson et al. 2005; Killick, Fearnhead, and Eckley 2012;
Rigaill 2015; Maidstone et al. 2017). These methods have good statistical guarantees (Yao 1984;
Lavielle and Moulines 2000; Lebarbier 2005) and have shown good performances in benchmark
simulation (Fearnhead, Maidstone, and Letchford 2018) and on many applications (Lai et al. 2005;
Liehrmann, Rigaill, and Hocking 2021). From a computational perspective, they rely on dynamic
programming algorithms that are at worst quadratic in the size of the data, 𝑛. However using
inequality-based and functional pruning techniques (Rigaill 2015; Killick, Fearnhead, and Eckley
2012; Maidstone et al. 2017) the average run times are typically much smaller allowing to process"
INTRODUCTION,0.050314465408805034,"very large profiles (𝑛> 105) in a matter of seconds or minutes. In detail, for one time series:"
INTRODUCTION,0.05345911949685535,"• if the number of change points is proportional to 𝑛both PELT (Killick, Fearnhead, and Eckley
2012) (a version of OP which uses inequality-based pruning) and FPOP (Maidstone et al. 2017)
(a version of OP which uses functional pruning as in (Rigaill 2015)) are on average linear
(Killick, Fearnhead, and Eckley 2012; Maidstone et al. 2017);
• if the number of change points is fixed, FPOP is quasi-linear (on simulations) while PELT is
quadratic (Maidstone et al. 2017)."
INTRODUCTION,0.05660377358490566,"Multivariate extensions. This paper focuses on identifying multiple change points in a multivariate
independent time series. We assume that changes occur simultaneously in all dimensions, their
number is unknown, and the cost function or log-likelihood of a segment (denoted as 𝒞) can be
expressed as a sum across all dimensions 𝑝. Informally, that is,"
INTRODUCTION,0.059748427672955975,𝒞(𝑠𝑒𝑔𝑚𝑒𝑛𝑡) =
INTRODUCTION,0.06289308176100629,"𝑝
∑
𝑘=1
𝒞(𝑠𝑒𝑔𝑚𝑒𝑛𝑡, time series 𝑘) ."
INTRODUCTION,0.0660377358490566,"In this context, the PELT algorithm can easily be extended for multiple time series. However, as for
the univariate case, it will be algorithmically efficient only if the number of non-negligible change
points is comparable to 𝑛. In this paper, we study the extension of functional pruning techniques
(and more specifically FPOP) to the multivariate case."
INTRODUCTION,0.06918238993710692,"At each iteration, FPOP updates the set of parameter values for which a change position 𝜏is optimal.
As soon as this set is empty the change is pruned. For univariate time series, this set is a union of
intervals in ℝ. For parametric multivariate models, this set is equal to the intersection and difference
of convex sets in ℝ𝑝(Runge 2020). It is typically non-convex, hard to update, and deciding whether
it is empty or not is not straightforward."
INTRODUCTION,0.07232704402515723,"In this work, we present a new algorithm, called Geometric Functional Pruning Optimal Partitioning
(GeomFPOP). The idea of our method consists in approximating the sets that are updated at each
iteration of FPOP using simpler geometric shapes. Their simplicity of description and simple updating
allow for a quick emptiness test."
INTRODUCTION,0.07547169811320754,"The paper has the following structure. In Section 1 we introduce the penalized optimization problem
for segmented multivariate time series in case where the number of changes is unknown. We then
review the existing pruned dynamic programming methods for solving this problem. We define the
geometric problem that occurs when using functional pruning. The new method, called GeomFPOP,
is described in Section 2 and based on approximating intersection and exclusion set operators. In
Section 3 we introduce two approximation types (sphere-like and rectangle-like) and define the
approximation operators for each of them. We then compare in Section 4 the empirical efficiency of
GeomFPOP with PELT on simulated data."
IMPLEMENTATION/METHODS,0.07861635220125786,"1
Functional Pruning for Multiple Time Series"
IMPLEMENTATION/METHODS,0.08176100628930817,"1.1
Model and Cost"
IMPLEMENTATION/METHODS,0.08490566037735849,"We consider the problem of change point detection in multiple independent time series of length 𝑛
and dimension 𝑝, while assuming simultaneous changes in all univariate time series and an unknown
number of changes. Our aim is to partition data into segments, such that in each segment the
parameter associated to each time series is constant. For a time series 𝑦we write 𝑦= 𝑦1∶𝑛=
(𝑦1, … , 𝑦𝑛) ∈(ℝ𝑝)𝑛with 𝑦𝑘
𝑖the 𝑘-th component of the 𝑝-dimensional point 𝑦𝑖∈ℝ𝑝in position 𝑖
in vector 𝑦1∶𝑛. We also use the notation 𝑦𝑖∶𝑗= (𝑦𝑖, … , 𝑦𝑗) to denote points from index 𝑖to 𝑗. If we"
IMPLEMENTATION/METHODS,0.0880503144654088,"assume that there are 𝑀change points in a time series, this corresponds to time series splits into
𝑀+ 1 distinct segments. The data points of each segment 𝑚∈{1, … , 𝑀+ 1} are generated by
independent random variables from a multivariate distribution with the segment-specific parameter
𝜃𝑚= (𝜃1𝑚, … , 𝜃𝑝
𝑚) ∈ℝ𝑝. A segmentation with 𝑀change points is defined by the vector of integers
𝜏= (𝜏0 = 0, 𝜏1, … , 𝜏𝑀, 𝜏𝑀+1 = 𝑛). Segments are given by the sets of indices {𝜏𝑖+ 1, … , 𝜏𝑖+1} with 𝑖in
{0, 1, … , 𝑀}."
IMPLEMENTATION/METHODS,0.09119496855345911,"We define the set 𝑆𝑀
𝑛of all possible change point locations related to the segmentation of data points
between positions 1 to 𝑛in 𝑀+ 1 segments as"
IMPLEMENTATION/METHODS,0.09433962264150944,"𝑆𝑀
𝑛= {𝜏= (𝜏0, 𝜏1, … , 𝜏𝑀, 𝜏𝑀+1) ∈ℕ𝑀+2|0 = 𝜏0 < 𝜏1 < ⋯< 𝜏𝑀< 𝜏𝑀+1 = 𝑛} ."
IMPLEMENTATION/METHODS,0.09748427672955975,"For any segmentation 𝜏in 𝑆𝑀
𝑛we define its size as |𝜏| = 𝑀. We denote 𝒮∞
𝑛as the set of all possible
segmentations of 𝑦1∶𝑛:"
IMPLEMENTATION/METHODS,0.10062893081761007,"𝒮∞
𝑛= ⋃
𝑀<𝑛
𝑆𝑀
𝑛,"
IMPLEMENTATION/METHODS,0.10377358490566038,"and take the convention that 𝑆∞−1
𝑛
= 𝑆∞
𝑛. In our case the number of changes 𝑀is unknown, and has
to be estimated."
IMPLEMENTATION/METHODS,0.1069182389937107,"Many approaches to detecting change points define a cost function for segmentation using the
negative log-likelihood (times two). Here the negative log-likelihood (times two) calculated at the
data point 𝑦𝑗is given by function 𝜃↦Ω(𝜃, 𝑦𝑗), where 𝜃= (𝜃1, … , 𝜃𝑝) ∈ℝ𝑝. Over a segment from 𝑖to
𝑡, the parameter remains the same and the segment cost 𝒞is given by"
IMPLEMENTATION/METHODS,0.11006289308176101,"𝒞(𝑦𝑖∶𝑡) = min
𝜃∈ℝ𝑝"
IMPLEMENTATION/METHODS,0.11320754716981132,"𝑡
∑
𝑗=𝑖
Ω(𝜃, 𝑦𝑗) = min
𝜃∈ℝ𝑝"
IMPLEMENTATION/METHODS,0.11635220125786164,"𝑡
∑
𝑗=𝑖
("
IMPLEMENTATION/METHODS,0.11949685534591195,"𝑝
∑
𝑘=1
𝜔(𝜃𝑘, 𝑦𝑘
𝑗)) ,
(1)"
IMPLEMENTATION/METHODS,0.12264150943396226,"with 𝜔the atomic likelihood function associated with Ω for each univariate time series. This
decomposition is made possible by the independence hypothesis between univariate time series}.
Notice that it could have been dimension-dependent with a mixture of different distributions (Gauss,
Poisson, negative binomial, etc.). In our study, we use the same data model for all dimensions."
IMPLEMENTATION/METHODS,0.12578616352201258,"In summary, the methodology we propose relies on the assumption that:"
IMPLEMENTATION/METHODS,0.1289308176100629,"1. the cost is point additive (see first equality in equation (1));
2. the per-point cost Ω has a simple decomposition : Ω(𝜃) = ∑𝑝𝜔(𝜃𝑝);
3. the 𝜔is convex."
IMPLEMENTATION/METHODS,0.1320754716981132,"We get that for any 𝜏∈𝒮∞
𝑛its segmentation cost is the sum of segment cost functions:"
IMPLEMENTATION/METHODS,0.13522012578616352,"|𝜏|
∑
𝑖=0
𝒞(𝑦(𝜏𝑖+1)∶𝜏𝑖+1) ."
IMPLEMENTATION/METHODS,0.13836477987421383,"We consider a penalized version of the segment cost by a penalty 𝛽> 0, as the zero penalty case would
lead to segmentation with 𝑛segments. The optimal penalized cost associated with our segmentation
problem is then defined by"
IMPLEMENTATION/METHODS,0.14150943396226415,̂𝑄𝑛= min
IMPLEMENTATION/METHODS,0.14465408805031446,"𝜏∈𝑆∞
𝑛"
IMPLEMENTATION/METHODS,0.14779874213836477,"|𝜏|
∑
𝑖=0
{𝒞(𝑦(𝜏𝑖+1)∶𝜏𝑖+1) + 𝛽} .
(2)"
IMPLEMENTATION/METHODS,0.1509433962264151,The optimal segmentation 𝜏is obtained by the argminimum in equation (2).
IMPLEMENTATION/METHODS,0.1540880503144654,"Various penalty forms have been proposed in the literature (Yao 1984; Killick, Fearnhead, and Eckley
2012; Zhang and Siegmund 2007; Lebarbier 2005; Verzelen et al. 2020). Summing over all segments
in Equation (2), we end up with a global penalty of the form 𝛽(𝑀+ 1). Hence, our model only allows
penalties that are proportional to the number of segments (Yao 1984; Killick, Fearnhead, and Eckley
2012). Penalties such as (Zhang and Siegmund 2007; Lebarbier 2005; Verzelen et al. 2020) cannot be
considered with our algorithm."
IMPLEMENTATION/METHODS,0.15723270440251572,"By default, we set the penalty 𝛽for 𝑝-variate time series of length 𝑛using the Schwarz Information
Criterion from (Yao 1984) (calibrated to the 𝑝dimensions), as 𝛽= 2𝑝𝜎2 log 𝑛. In practice, if the
variance 𝜎2 is unknown, it is replaced by an appropriate estimation (e.g. (Hampel 1974; Hall, Kay,
and Titterington 1990) as in (Lavielle and Lebarbier 2001; Liehrmann et al. 2023))."
IMPLEMENTATION/METHODS,0.16037735849056603,"1.2
Functional Pruning Optimal Partitioning Algorithm"
IMPLEMENTATION/METHODS,0.16352201257861634,"The idea of the Optimal Partitioning (OP) method (Jackson et al. 2005) is to search for the last
change point defining the last segment in data 𝑦1∶𝑡at each iteration (with 𝑄0 = 0), which leads to the
recursion:"
IMPLEMENTATION/METHODS,0.16666666666666666,"𝑄𝑡=
min
𝑖∈{0,…,𝑡−1} (𝑄𝑖+ 𝒞(𝑦(𝑖+1∶𝑡) + 𝛽) ."
IMPLEMENTATION/METHODS,0.16981132075471697,"The Pruned Exact Linear Time (PELT) method, introduced in (Killick, Fearnhead, and Eckley 2012),
uses inequality-based pruning. It essentially relies on the assumption that splitting a segment in two
is always beneficial in terms of cost, this is 𝐶(𝑦(𝑖+1)∶𝑗) + 𝐶(𝑦(𝑗+1)∶𝑡) ≤𝐶(𝑦(𝑖+1)∶𝑡). This assumption is
always true in our setting. PELT considers each change point candidate sequentially and decides
whether 𝑖can be excluded from the set of changepoint candidates if ̂𝑄𝑖+ 𝒞(𝑦(𝑖+1)∶𝑡) ≥
̂𝑄𝑡, as 𝑖cannot
appear as the optimal change point in future iterations."
IMPLEMENTATION/METHODS,0.17295597484276728,"Functional description. In the FPOP method we introduce a last segment parameter 𝜃= (𝜃1, … , 𝜃𝑝) in
ℝ𝑝and define a functional cost 𝜃↦𝑄𝑡(𝜃) depending on 𝜃, that takes the following form:"
IMPLEMENTATION/METHODS,0.1761006289308176,"𝑄𝑡(𝜃) = min
𝜏∈𝑆𝑡
(
𝑀−1
∑
𝑖=0
{𝒞(𝑦(𝜏𝑖+1)∶𝜏𝑖+1) + 𝛽} +
𝑡
∑
𝑗=𝜏𝑀+1
Ω(𝜃, 𝑦𝑗) + 𝛽) ."
IMPLEMENTATION/METHODS,0.1792452830188679,"As explained in (Maidstone et al. 2017), we can compute the function 𝑄𝑡+1(⋅) based only on the
knowledge of 𝑄𝑡(⋅) for each integer 𝑡from 0 to 𝑛−1. We have:"
IMPLEMENTATION/METHODS,0.18238993710691823,"𝑄𝑡+1(𝜃) = min{𝑄𝑡(𝜃), ̂𝑄𝑡+ 𝛽} + Ω(𝜃, 𝑦𝑡+1) ,
(3)"
IMPLEMENTATION/METHODS,0.18553459119496854,"for all 𝜃∈ℝ𝑝, with
̂𝑄𝑡= min𝜃𝑄𝑡(𝜃) (𝑡≥1) and the initialization 𝑄0(𝜃) = 𝛽,
̂𝑄0 = 0 so that
𝑄1(𝜃) = Ω(𝜃, 𝑦1) + 𝛽. By looking closely at this relation, we see that each function 𝑄𝑡is a piece-wise
continuous function consisting of at most 𝑡different functions on ℝ𝑝, denoted 𝑞𝑖𝑡:"
IMPLEMENTATION/METHODS,0.18867924528301888,"𝑄𝑡(𝜃) =
min
𝑖∈{1,…,𝑡} {𝑞𝑖𝑡(𝜃)} ,"
IMPLEMENTATION/METHODS,0.1918238993710692,where the 𝑞𝑖𝑡functions are given by explicit formulas:
IMPLEMENTATION/METHODS,0.1949685534591195,"𝑞𝑖𝑡(𝜃) =
̂𝑄𝑖−1 + 𝛽+
𝑡
∑
𝑗=𝑖
Ω(𝜃, 𝑦𝑗) ,
𝜃∈ℝ𝑝,
𝑖= 1, … , 𝑡. and"
IMPLEMENTATION/METHODS,0.19811320754716982,"̂𝑄𝑖−1 = min
𝜃∈ℝ𝑝𝑄𝑖−1(𝜃) =
min
𝑗∈{1,…,𝑖−1} {min
𝜃∈ℝ𝑝𝑞𝑗
𝑖−1(𝜃)} .
(4)"
IMPLEMENTATION/METHODS,0.20125786163522014,"It is important to notice that each 𝑞𝑖𝑡function is associated with the last change point 𝑖−1 and the
last segment is given by indices from 𝑖to 𝑡. Consequently, the last change point at step 𝑡in 𝑦1∶𝑡is
denoted as ̂𝜏𝑡( ̂𝜏𝑡≤𝑡−1) and is given by"
IMPLEMENTATION/METHODS,0.20440251572327045,"̂𝜏𝑡= 𝐴𝑟𝑔min
𝑖∈{1,…,𝑡}
{min
𝜃∈ℝ𝑝𝑞𝑖𝑡(𝜃)} −1."
IMPLEMENTATION/METHODS,0.20754716981132076,"Backtracking. Knowing the values of ̂𝜏𝑡for all 𝑡= 1, … , 𝑛, we can always restore the optimal
segmentation at time 𝑛for 𝑦1∶𝑛. This procedure is called backtracking. The vector 𝑐𝑝(𝑛) of ordered
change points in the optimal segmentation of 𝑦1∶𝑛is determined recursively by the relation 𝑐𝑝(𝑛) =
(𝑐𝑝( ̂𝜏𝑛), ̂𝜏𝑛) with stopping rule 𝑐𝑝(0) = ∅."
IMPLEMENTATION/METHODS,0.21069182389937108,"Parameter space description. Applying functional pruning requires a precise analysis of the recursion
(3) that depends on the property of the cost function Ω. In what follows we consider three choices
based on a Gaussian, Poisson, and negative binomial distribution for data distribution. The exact
formulas of these cost functions are given in Section 5.1."
IMPLEMENTATION/METHODS,0.2138364779874214,We denote the set of parameter values for which the function 𝑞𝑖𝑡(⋅) is optimal as:
IMPLEMENTATION/METHODS,0.2169811320754717,"𝑍𝑖𝑡= {𝜃∈ℝ𝑝|𝑄𝑡(𝜃) = 𝑞𝑖𝑡(𝜃)} ,
𝑖= 1, … , 𝑡."
IMPLEMENTATION/METHODS,0.22012578616352202,"also called the living zone. The key idea behind functional pruning is that the 𝑍𝑖𝑡are nested (𝑍𝑖
𝑡+1 ⊂𝑍𝑖𝑡)
thus as soon as we can prove the emptiness of one set 𝑍𝑖𝑡, we delete its associated 𝑞𝑖𝑡function and
do not have to consider its minimum anymore at any further iteration (proof in Section 1.3). In
dimension 𝑝= 1 this is reasonably easy. In this case, the sets 𝑍𝑖𝑡(𝑖= 1, … , 𝑡) are unions of intervals
and an efficient functional pruning rule is possible by updating a list of these intervals for 𝑄𝑡. This
approach is implemented in FPOP (Maidstone et al. 2017)."
IMPLEMENTATION/METHODS,0.22327044025157233,"In dimension 𝑝≥2 it is not so easy anymore to keep track of the emptiness of the sets 𝑍𝑖𝑡. We illustrate
the dynamics of the 𝑍𝑖𝑡sets in Figure 1 in the bivariate Gaussian case. Each color is associated with a
set 𝑍𝑖𝑡(corresponding to a possible change at 𝑖−1) for 𝑡equal 1 to 5. This plot shows in particular
that sets 𝑍𝑖𝑡can be non-convex."
IMPLEMENTATION/METHODS,0.22641509433962265,"1.3
Geometric Formulation of Functional Pruning"
IMPLEMENTATION/METHODS,0.22955974842767296,"To build an efficient pruning strategy for dimension 𝑝≥2 we need to test the emptiness of the sets
𝑍𝑖𝑡at each iteration. Note that to get 𝑍𝑖𝑡we need to compare the functional cost 𝑞𝑖𝑡with any other
functional cost 𝑞𝑗
𝑡, 𝑗= 1, … , 𝑡, 𝑗≠𝑖. This leads to the definition of the following sets."
IMPLEMENTATION/METHODS,0.23270440251572327,"Definition 1.1. We define 𝑆-type set 𝑆𝑖
𝑗using the function Ω as"
IMPLEMENTATION/METHODS,0.2358490566037736,"𝑆𝑖
𝑗= {𝜃∈ℝ𝑝|"
IMPLEMENTATION/METHODS,0.2389937106918239,"𝑗−1
∑
𝑢=𝑖
Ω(𝜃, 𝑦𝑢) ≤
̂𝑄𝑗−1 −̂𝑄𝑖−1} , when 𝑖< 𝑗"
IMPLEMENTATION/METHODS,0.24213836477987422,"Figure 1: The sets 𝑍𝑖𝑡over time for the bivariate independent Gaussian model on time series without
change 𝑦= ((0.29, 1.93), (1.86, −0.02), (0.9, 2.51), (−1.26, 0.91), (1.22, 1.11)). From left to right we
represent at time 𝑡= 1, 2, 3, 4, and 5 the parameter space (𝜃1, 𝜃2). Each 𝑍𝑖𝑡is represented by a color.
The change 1 associated with quadratics 2 is pruned at 𝑡= 3. Notice that each time sequence of 𝑍𝑖𝑡
with 𝑖fixed is a nested sequence of sets."
IMPLEMENTATION/METHODS,0.24528301886792453,"and 𝑆𝑖
𝑖= ℝ𝑝. We denote the set of all possible S-type sets as S."
IMPLEMENTATION/METHODS,0.24842767295597484,"To ease some of our calculations, we now introduce some additional notations. For 𝜃= (𝜃1, … , 𝜃𝑝) in
ℝ𝑝, 1 ≤𝑖< 𝑗≤𝑛we define 𝑝univariate functions 𝜃𝑘↦𝑠𝑘
𝑖𝑗(𝜃𝑘) associated to the 𝑘-th time series as"
IMPLEMENTATION/METHODS,0.25157232704402516,"𝑠𝑘
𝑖𝑗(𝜃𝑘) ="
IMPLEMENTATION/METHODS,0.25471698113207547,"𝑗−1
∑
𝑢=𝑖
𝜔(𝜃𝑘, 𝑦𝑘𝑢),
𝑘= 1, … , 𝑝.
(5)"
IMPLEMENTATION/METHODS,0.2578616352201258,"We introduce a constant Δ𝑖𝑗and a function 𝜃↦𝑠𝑖𝑗(𝜃): ⎧ ⎨
⎩"
IMPLEMENTATION/METHODS,0.2610062893081761,"Δ𝑖𝑗=
̂𝑄𝑗−1 −̂𝑄𝑖−1 ,"
IMPLEMENTATION/METHODS,0.2641509433962264,𝑠𝑖𝑗(𝜃) =
IMPLEMENTATION/METHODS,0.2672955974842767,"𝑝
∑
𝑘=1
𝑠𝑘
𝑖𝑗(𝜃𝑘) −Δ𝑖𝑗,
(6)"
IMPLEMENTATION/METHODS,0.27044025157232704,"where ̂𝑄𝑖−1 and ̂𝑄𝑗−1 are defined as in (4). The sets 𝑆𝑖
𝑗for 𝑖< 𝑗can thus be written as"
IMPLEMENTATION/METHODS,0.27358490566037735,"𝑆𝑖
𝑗= 𝑠−1
𝑖𝑗(−∞, 0] .
(7)"
IMPLEMENTATION/METHODS,0.27672955974842767,"In Figure 2 we present the level curves for three different parametric models given by 𝑠−1
𝑖𝑗({𝑤}) with
𝑤a real number. Each of these curves encloses an S-type set, which, according to the definition of
the function 𝜔, is convex."
IMPLEMENTATION/METHODS,0.279874213836478,"At time 𝑡= 1, … , 𝑛we define the following sets associated to the last change point index 𝑖−1:"
IMPLEMENTATION/METHODS,0.2830188679245283,-past set 𝒫𝑖
IMPLEMENTATION/METHODS,0.2861635220125786,"𝒫𝑖= {𝑆𝑢
𝑖, 𝑢= 1, … , 𝑖−1} ."
IMPLEMENTATION/METHODS,0.2893081761006289,"-future set ℱ𝑖(𝑡)
ℱ𝑖(𝑡) = {𝑆𝑖𝑣, 𝑣= 𝑖, … , 𝑡} ."
IMPLEMENTATION/METHODS,0.29245283018867924,"We denote the cardinal of a set 𝒜as |𝒜|. Using these two sets of sets, the 𝑍𝑖𝑡have the following
description."
IMPLEMENTATION/METHODS,0.29559748427672955,"Proposition 1.1. At iteration 𝑡, the living zones 𝑍𝑖𝑡(𝑖= 1, … , 𝑡) are defined by the functional cost 𝑄𝑡(⋅),
with each of them being formed as the intersection of sets in ℱ𝑖(𝑡) excluding the union of sets in 𝒫𝑖."
IMPLEMENTATION/METHODS,0.29874213836477986,"Figure 2: Three examples of the level curves of a function 𝑠𝑖𝑗for bivariate time series {𝑦1, 𝑦2}. We use
the following simulations for univariate time series : (a) 𝑦1 ∼𝒩(0, 1), 𝑦2 ∼𝒩(0, 1), (b) 𝑦1 ∼𝒫(1),
𝑦2 ∼𝒫(3), (c) 𝑦1 ∼𝒩ℬ(0.5, 1), 𝑦2 ∼𝒩ℬ(0.8, 1)."
IMPLEMENTATION/METHODS,0.3018867924528302,𝑍𝑖𝑡= ( ⋂
IMPLEMENTATION/METHODS,0.3050314465408805,"𝑆∈ℱ𝑖(𝑡)
𝑆) ⧵(∪𝑆∈𝒫𝑖𝑆) ,
𝑖= 1, … , 𝑡.
(8)"
IMPLEMENTATION/METHODS,0.3081761006289308,"Proof. Based on the definition of the set 𝑍𝑖𝑡, the proof is straightforward. Parameter value 𝜃is in 𝑍𝑖𝑡
if and only if 𝑞𝑖𝑡(𝜃) ≤𝑞𝑢𝑡(𝜃) for all 𝑢≠𝑖; these inequalities define the past set (when 𝑢< 𝑖) and the
future set (when 𝑢≥𝑖)."
IMPLEMENTATION/METHODS,0.3113207547169811,"Proposition 1.1 states that regardless of the value of i, the living zone 𝑍𝑖𝑡is formed through intersection
and elimination operations on 𝑡S-type sets. Notably, one of these sets, 𝑆𝑖
𝑖, always represents the
entire space ℝ𝑝."
IMPLEMENTATION/METHODS,0.31446540880503143,Corollary 1.1. The sequence 𝜁𝑖= (𝑍𝑖𝑡)𝑡≥𝑖is a nested sequence of sets.
IMPLEMENTATION/METHODS,0.31761006289308175,"Indeed, 𝑍𝑖
𝑡+1 is equal to 𝑍𝑖𝑡with an additional intersection in the future set. Based on Corollary 1.1,
as soon as we prove that the set 𝑍𝑖𝑡, is empty, we delete its associated 𝑞𝑖𝑡function and, consequently,
we can prune the change point 𝑖−1. In this context, functional and inequality-based pruning have a
simple geometric interpretation."
IMPLEMENTATION/METHODS,0.32075471698113206,"Functional pruning geometry. The position 𝑖−1 is pruned at step 𝑡, in 𝑄𝑡(⋅), if the intersection set of
⋂𝑆∈ℱ𝑖(𝑡) 𝑆is covered by the union set ∪𝑆∈𝒫𝑖𝑆."
IMPLEMENTATION/METHODS,0.3238993710691824,"Inequality-based pruning geometry. The inequality-based pruning of PELT is equivalent to the
geometric rule: position 𝑖−1 is pruned at step 𝑡if the set 𝑆𝑖𝑡is empty. In that case, the intersection set
⋂𝑆∈ℱ𝑖(𝑡) 𝑆is empty, and therefore 𝑍𝑖𝑡is also empty using (8). This shows that if a change is pruned"
IMPLEMENTATION/METHODS,0.3270440251572327,"using inequality-based pruning it is also pruned using functional pruning. For the dimension 𝑝= 1
this claim was theoretically proved in (Maidstone et al. 2017)."
IMPLEMENTATION/METHODS,0.330188679245283,"According to Proposition 1.1, beginning with 𝑍𝑖
𝑖= ℝ𝑝, the set 𝑍𝑖𝑡is derived by iteratively applying
two types of operations: intersection with an S-type set 𝑆from ℱ𝑖(𝑡) or subtraction of an S-type set
𝑆from 𝒫𝑖. The construction of set 𝑍𝑖𝑡using Proposition 1.1 is illustrated in Figure 3 for a bivariate
independent Gaussian case: we have the intersection of three S-type sets and the subtraction of three
S-type sets. This simple example highlights that the set 𝑍𝑖𝑡is typically non-convex, posing challenge
in studying its emptiness."
IMPLEMENTATION/METHODS,0.3333333333333333,"Figure 3: Examples of building a living zone 𝑍𝑖𝑡with |𝒫𝑖| = |ℱ𝑖(𝑡)| = 3 for the Gaussian case in 2-D
(𝜇= 0, 𝜎= 1). The green disks are S-type sets of the past set 𝒫𝑖. The blue disks are S-type sets of
the future set ℱ𝑖(𝑡). The shaded area is the set 𝑍𝑖𝑡."
IMPLEMENTATION/METHODS,0.33647798742138363,"2
Geometric Functional Pruning Optimal Partitioning"
IMPLEMENTATION/METHODS,0.33962264150943394,"2.1
General Principle of GeomFPOP"
IMPLEMENTATION/METHODS,0.34276729559748426,"Rather than considering an exact representation of the 𝑍𝑖𝑡, our idea is to consider a hopefully slightly
larger set that is easier to update. To be specific, for each 𝑍𝑖𝑡we introduce
̃𝑍𝑖𝑡, called testing set,
such that 𝑍𝑖𝑡⊂
̃𝑍𝑖𝑡. If at time 𝑡
̃𝑍𝑖𝑡is empty thus is 𝑍𝑖𝑡and thus change 𝑖−1 can be pruned. From
Proposition 1.1 we have that starting from 𝑍𝑖
𝑖= ℝ𝑝the set 𝑍𝑖𝑡is obtained by successively applying
two types of operations: intersection with an S-type set 𝑆(𝑍⋂𝑆) or subtraction of an S-type set 𝑆
(𝑍⧵𝑆). Similarly, starting from
̃𝑍𝑖
𝑖= ℝ𝑝we obtain
̃𝑍𝑖𝑡by successively applying approximation of
these intersection and subtraction operations. Intuitively, the complexity of the resulting algorithm
is a combination of the efficiency of the pruning and the easiness of updating the testing set."
IMPLEMENTATION/METHODS,0.34591194968553457,"A Generic Formulation of GeomFPOP. In what follows we will generically describe GeomFPOP, that
is, without specifying the precise structure of the testing set ̃𝑍𝑖𝑡. We call ̃Z the set of all possible ̃𝑍𝑖𝑡
and assume the existence of two operators ⋂̃𝑍and ⧵̃𝑍. We have the following assumptions for these
operators."
IMPLEMENTATION/METHODS,0.3490566037735849,Definition 2.1. The two operators ⋂̃𝑍and ⧵̃𝑍are such that:
IMPLEMENTATION/METHODS,0.3522012578616352,"1. the left input is a ̃𝑍-type set (that is an element of ̃Z);
2. the right input is a 𝑆-type set;"
IMPLEMENTATION/METHODS,0.3553459119496855,"3. the output is a ̃𝑍-type set;
4.
̃𝑍⋂𝑆⊂
̃𝑍⋂̃𝑍𝑆and ̃𝑍⧵𝑆⊂
̃𝑍⧵̃𝑍𝑆."
IMPLEMENTATION/METHODS,0.3584905660377358,"We give a proper description of two types of testing sets and their approximation operators in
Section 3."
IMPLEMENTATION/METHODS,0.36163522012578614,"At each iteration 𝑡GeomFPOP will construct ̃𝑍𝑖𝑡(with 𝑖< 𝑡) from ̃𝑍𝑖
𝑡−1, 𝒫𝑖and ℱ𝑖(𝑡) iteratively using
the two operators ⋂̃𝑍and ⧵̃𝑍. To be specific, we define 𝑆𝐹
𝑗the j-th element of ℱ𝑖(𝑡) and 𝑆𝑗
𝑃the j-th
element of 𝒫𝑖, we use the following iterations:"
IMPLEMENTATION/METHODS,0.36477987421383645,"{
𝐴0 =
̃𝑍𝑖𝑡,
𝐴𝑗= 𝐴𝑗−1 ⋂
̃𝑍
𝑆𝐹
𝑗,
𝑗= 1, … , |ℱ𝑖(𝑡)| ,"
IMPLEMENTATION/METHODS,0.36792452830188677,"𝐵0 = 𝐴|ℱ𝑖(𝑡)| ,
𝐵𝑗= 𝐵𝑗−1 ⧵̃𝑍𝑆𝑗
𝑃,
𝑗= 1, … , |𝒫𝑖| ,"
IMPLEMENTATION/METHODS,0.3710691823899371,"and define ̃𝑍𝑖𝑡= 𝐵|𝒫𝑖|. Using the fourth property of Definition 2.1 and Proposition 1.1, we get that at
any time of the algorithm ̃𝑍𝑖𝑡contains 𝑍𝑖𝑡."
IMPLEMENTATION/METHODS,0.3742138364779874,"The pseudo-code of this procedure is described in Algorithm 1. The select(𝒜) step in Algorithm 1,
where 𝒜⊂S, returns a subset of 𝒜in S. By default, select(𝒜) ∶= 𝒜."
IMPLEMENTATION/METHODS,0.37735849056603776,Algorithm 1 Geometric update rule of ̃𝑍𝑖𝑡
IMPLEMENTATION/METHODS,0.3805031446540881,"procedure updateZone( ̃𝑍𝑖
𝑡−1, 𝒫𝑖, ℱ𝑖(𝑡), 𝑖< 𝑡)
̃𝑍𝑖𝑡←
̃𝑍𝑖
𝑡−1
for 𝑆∈select(ℱ𝑖(𝑡−1)) do"
IMPLEMENTATION/METHODS,0.3836477987421384,"̃𝑍𝑖𝑡←
̃𝑍𝑖𝑡⋂̃𝑍𝑆
for 𝑆∈select(𝒫𝑖) do"
IMPLEMENTATION/METHODS,0.3867924528301887,"̃𝑍𝑖𝑡←
̃𝑍𝑖𝑡⧵̃𝑍𝑆
return ̃𝑍𝑖𝑡"
IMPLEMENTATION/METHODS,0.389937106918239,"We denote the set of candidate change points at time 𝑡as 𝜏𝑡. Note that for any (𝑖−1) ∈𝜏𝑡the sum of |𝒫𝑖|
and |ℱ𝑖(𝑡)| is |𝜏𝑡|. With the default select procedure we do 𝒪(𝑝|𝜏𝑡|) operations in the updateZone
procedure. By limiting the number of elements returned by select we can reduce the complexity of
the updateZone procedure."
IMPLEMENTATION/METHODS,0.39308176100628933,"Remark. For example, if the operator 𝒜↦select(𝒜), regardless of |𝒜|, always returns a subset of
constant size, then the overall complexity of GeomFPOP is at worst ∑𝑛
𝑡=1 𝒪(𝑝|𝜏𝑡|)."
IMPLEMENTATION/METHODS,0.39622641509433965,"Using this updateZone procedure we can now informally describe the GeomFPOP algorithm. At
each iteration the algorithm will"
IMPLEMENTATION/METHODS,0.39937106918238996,"1. find the minimum value for 𝑄𝑡, 𝑚𝑡and the best position for last change point ̂𝜏𝑡(note that this
step is standard: as in the PELT algorithm we need to minimize the cost of the last segment
defined in equation (1));
2. compute all sets ̃𝑍𝑖𝑡using ̃𝑍𝑖
𝑡−1, 𝒫𝑖, and ℱ𝑖(𝑡) with the updateZone procedure;
3. remove changes such that ̃𝑍𝑖𝑡is empty."
IMPLEMENTATION/METHODS,0.4025157232704403,"To simplify the pseudo-code of GeomFPOP, we also define the following operators:"
IMPLEMENTATION/METHODS,0.4056603773584906,"1. bestCost&Tau(𝑡) operator returns two values: the minimum value of 𝑄𝑡, 𝑚𝑡, and the best
position for last change point ̂𝜏𝑡at time 𝑡(see Section 1.2);
2. getPastFutureSets(𝑖, 𝑡) operator returns a pair of sets (𝒫𝑖, ℱ𝑖(𝑡)) for change point candidate
𝑖−1 at time 𝑡;
3. backtracking( ̂𝜏, 𝑛) operator returns the optimal segmentation for 𝑦1∶𝑛."
IMPLEMENTATION/METHODS,0.4088050314465409,The pseudo-code of GeomFPOP is presented in Algorithm 2.
IMPLEMENTATION/METHODS,0.4119496855345912,Algorithm 2 GeomFPOP algorithm
IMPLEMENTATION/METHODS,0.41509433962264153,"procedure GeomFPOP(𝑦, Ω(⋅, ⋅), 𝛽)"
IMPLEMENTATION/METHODS,0.41823899371069184,"̂𝑄0 ←0,
𝑄0(𝜃) ←𝛽,
𝜏0 ←∅,
{ ̃𝑍𝑖
𝑖}𝑖∈{1,…,𝑛} ←ℝ𝑝"
IMPLEMENTATION/METHODS,0.42138364779874216,"for 𝑡= 1, … , 𝑛do"
IMPLEMENTATION/METHODS,0.42452830188679247,"𝑄𝑡(𝜃) ←min{𝑄𝑡−1(𝜃), ̂𝑄𝑡−1 + 𝛽} + Ω(𝜃, 𝑦𝑡)
( ̂𝑄𝑡, ̂𝜏𝑡) ←bestCost&Tau(𝑡)
for 𝑖−1 ∈𝜏𝑡−1 do"
IMPLEMENTATION/METHODS,0.4276729559748428,"(𝒫𝑖, ℱ𝑖(𝑡)) ←getPastFutureSets(𝑖, 𝑡)"
IMPLEMENTATION/METHODS,0.4308176100628931,"̃𝑍𝑖𝑡←updateZone( ̃𝑍𝑖
𝑡−1, 𝒫𝑖, ℱ𝑖(𝑡), 𝑖, 𝑡)
if ̃𝑍𝑖𝑡= ∅then"
IMPLEMENTATION/METHODS,0.4339622641509434,"𝜏𝑡−1 ←𝜏𝑡−1\{𝑖−1}
𝜏𝑡←(𝜏𝑡−1, 𝑡−1)
return 𝑐𝑝(𝑛) ←backtracking( ̂𝜏= ( ̂𝜏1, … , ̂𝜏𝑛), 𝑛)"
IMPLEMENTATION/METHODS,0.4371069182389937,"Remark. Whatever the number of elements returned by the select operator for computing ̃𝑍𝑖𝑡, we
can guarantee the exactness of the GeomFPOP algorithm, since the approximate living zone (the
testing set) includes the living zone (8), as we consider less intersections and set subtractions."
IMPLEMENTATION/METHODS,0.44025157232704404,"3
Approximation Operators ⋂̃𝑍and ⧵̃𝑍"
IMPLEMENTATION/METHODS,0.44339622641509435,"The choice of the geometric structure and the way it is constructed directly affects the computational
cost of the algorithm. We consider two types of testing set ̃𝑍∈̃Z, a S-type set ̃𝑆∈S (see Definition 1.1)
and a hyperrectangle ̃𝑅∈R defined below."
IMPLEMENTATION/METHODS,0.44654088050314467,"Definition 3.1. Given two vectors in ℝ𝑝, ̃𝑙and ̃𝑟we define the set ̃𝑅, called hyperrectangle, as:"
IMPLEMENTATION/METHODS,0.449685534591195,"̃𝑅= [ ̃𝑙1, ̃𝑟1] × ⋯× [ ̃𝑙𝑝, ̃𝑟𝑝] ."
IMPLEMENTATION/METHODS,0.4528301886792453,We denote the set of all possible sets ̃𝑅as R.
IMPLEMENTATION/METHODS,0.4559748427672956,"To update the testing sets we need to give a strict definition of the operators ⋂̃𝑍and ⧵̃𝑍for each
type of testing set. To facilitate the following discussion, we rename them. For the first type of
geometric structure, we rename the testing set
̃𝑍as ̃𝑆, the operators ⋂̃𝑍and ⧵̃𝑍as ⋂𝑆and ⧵𝑆and
̃𝑍-type approximation as S-type approximation. And, likewise, we rename the testing set
̃𝑍as
̃𝑅,
the operators ⋂̃𝑍and ⧵̃𝑍as ⋂𝑅and ⧵𝑅and ̃𝑍-type approximation as R-type approximation for the
second type of geometric structure."
IMPLEMENTATION/METHODS,0.4591194968553459,"3.1
S-type Approximation"
IMPLEMENTATION/METHODS,0.46226415094339623,"With this approach, our goal is to keep track of the fact that at time 𝑡= 1, … , 𝑛there is a pair of
changes (𝑢1, 𝑢2), with 𝑢1 < 𝑖< 𝑢2 ≤𝑡such that 𝑆𝑖𝑢2 ⊂𝑆𝑢1
𝑖
or there is a pair of changes (𝑣1, 𝑣2), with
𝑖< 𝑣1 < 𝑣2 ≤𝑡such that 𝑆𝑖𝑣1 ⋂𝑆𝑖𝑣2 is empty. If at time 𝑡at least one of these conditions is met, we can
guarantee that the set ̃𝑆is empty, otherwise, we propose to keep as the result of approximation the
last future S-type set 𝑆𝑖𝑡, because it always includes the set 𝑍𝑖𝑡. This allows us to quickly check and
prove (if ̃𝑆= ∅) the emptiness of set 𝑍𝑖𝑡."
IMPLEMENTATION/METHODS,0.46540880503144655,"We consider two generic S-type sets, 𝑆and ̃𝑆from S, described as in Definition 1.1 by the functions 𝑠
and ̃𝑠:"
IMPLEMENTATION/METHODS,0.46855345911949686,𝑠(𝜃) =
IMPLEMENTATION/METHODS,0.4716981132075472,"𝑝
∑
𝑘=1
𝑠𝑘(𝜃𝑘) −Δ ,
̃𝑠(𝜃) ="
IMPLEMENTATION/METHODS,0.4748427672955975,"𝑝
∑
𝑘=1
̃𝑠𝑘(𝜃𝑘) −̃Δ ."
IMPLEMENTATION/METHODS,0.4779874213836478,Definition 3.2. For all 𝑆and ̃𝑆in S we define the operators ⋂𝑆and ⧵𝑆as:
IMPLEMENTATION/METHODS,0.4811320754716981,"̃𝑆⋂
𝑆
𝑆
= {
∅,
if ̃𝑆⋂𝑆= ∅,
̃𝑆,
otherwise ."
IMPLEMENTATION/METHODS,0.48427672955974843,"̃𝑆⧵𝑆𝑆
= {
∅,
if ̃𝑆⊂𝑆,
̃𝑆,
otherwise ."
IMPLEMENTATION/METHODS,0.48742138364779874,"As a consequence, we only need an easy way to detect any of these two geometric configurations:
̃𝑆⋂𝑆and ̃𝑆⊂𝑆."
IMPLEMENTATION/METHODS,0.49056603773584906,"In the Gaussian case, the S-type sets are 𝑝-balls and an easy solution exists based on comparing radii
(see Section 5.2 for details). In the case of other models (as Poisson or negative binomial), intersection
and inclusion tests can be performed based on a solution using separative hyperplanes and iterative
algorithms for convex problems (see Section 5.3). We propose another type of testing set solving all
types of models with the same method."
IMPLEMENTATION/METHODS,0.4937106918238994,"3.2
R-type Approximation"
IMPLEMENTATION/METHODS,0.4968553459119497,"Here, we approximate the sets 𝑍𝑖𝑡by hyperrectangles ̃𝑅𝑖𝑡∈R. A key insight of this approximation is
that given a hyperrectangle 𝑅and an S-type set 𝑆we can efficiently (in 𝒪(𝑝) using Proposition 3.2)
recover the best hyperrectangle approximation of 𝑅∪𝑆and 𝑅⧵𝑆. Formally we define these operators
as follows."
IMPLEMENTATION/METHODS,0.5,"Definition 3.3. For all 𝑅, ̃𝑅∈R and 𝑆∈S we define the operators ⋂𝑅and ⧵𝑅as:"
IMPLEMENTATION/METHODS,0.5031446540880503,"𝑅⋂
𝑅
𝑆=
⋂"
IMPLEMENTATION/METHODS,0.5062893081761006,"{ ̃𝑅|𝑅⋂𝑆⊂R} ̃𝑅,"
IMPLEMENTATION/METHODS,0.5094339622641509,"𝑅⧵𝑅𝑆=
⋂"
IMPLEMENTATION/METHODS,0.5125786163522013,{ ̃𝑅|𝑅⧵𝑆⊂R} ̃𝑅.
IMPLEMENTATION/METHODS,0.5157232704402516,"We now explain how we compute these two operators. First, we note that they can be recovered by
solving 2𝑝one-dimensional optimization problems."
IMPLEMENTATION/METHODS,0.5188679245283019,"Proposition 3.1. The 𝑘-th minimum coordinates ̃𝑙𝑘and maximum coordinates ̃𝑟𝑘of ̃𝑅= 𝑅⋂𝑅𝑆(resp.
̃𝑅= 𝑅⧵𝑅𝑆) is obtained as"
IMPLEMENTATION/METHODS,0.5220125786163522,"̃𝑙𝑘or ̃𝑟𝑘=
⎧⎪
⎨⎪⎩"
IMPLEMENTATION/METHODS,0.5251572327044025,"min
𝜃𝑘∈ℝor max
𝜃𝑘∈ℝ𝜃𝑘,"
IMPLEMENTATION/METHODS,0.5283018867924528,"subject to 𝜀𝑠(𝜃) ≤0 ,"
IMPLEMENTATION/METHODS,0.5314465408805031,"𝑙𝑗≤𝜃𝑗≤𝑟𝑗,
𝑗= 1, … , 𝑝, (9)"
IMPLEMENTATION/METHODS,0.5345911949685535,with 𝜀= 1 (resp. 𝜀= −1).
IMPLEMENTATION/METHODS,0.5377358490566038,"To solve the previous problems (𝜀= 1 or −1), we define the following characteristic points."
IMPLEMENTATION/METHODS,0.5408805031446541,"Definition 3.4. Let 𝑆∈S, described by function 𝑠(𝜃) = ∑𝑝
𝑘=1 𝑠𝑘(𝜃𝑘) −Δ from the family of functions
(6), with 𝜃∈ℝ𝑝. We define the minimal point c ∈ℝ𝑝of 𝑆as:"
IMPLEMENTATION/METHODS,0.5440251572327044,"c = {c𝑘}𝑘=1,…,𝑝,
with
c𝑘= 𝐴𝑟𝑔min
𝜃𝑘∈ℝ
{𝑠𝑘(𝜃𝑘)} .
(10)"
IMPLEMENTATION/METHODS,0.5471698113207547,"Moreover, with 𝑅∈R defined through vectors 𝑙, 𝑟∈ℝ𝑝, we define two points of 𝑅, the closest point
m ∈ℝ𝑝and the farthest point M ∈ℝ𝑝relative to 𝑆as"
IMPLEMENTATION/METHODS,0.550314465408805,"m = {m𝑘}𝑘=1,…,𝑝,
with
m𝑘= 𝐴𝑟𝑔min
𝑙𝑘≤𝜃𝑘≤𝑟𝑘{𝑠𝑘(𝜃𝑘)} ,"
IMPLEMENTATION/METHODS,0.5534591194968553,"M = {M𝑘}𝑘=1,…,𝑝,
with
M𝑘= 𝐴𝑟𝑔max
𝑙𝑘≤𝜃𝑘≤𝑟𝑘{𝑠𝑘(𝜃𝑘)} ."
IMPLEMENTATION/METHODS,0.5566037735849056,"Remark. In the Gaussian case, 𝑆is a ball in ℝ𝑝and"
IMPLEMENTATION/METHODS,0.559748427672956,"• c is the center of the ball;
• m is the closest point to c inside 𝑅;
• M is the farthest point to c in 𝑅."
IMPLEMENTATION/METHODS,0.5628930817610063,"Figure 4: Three examples of minimal point c, closest point m and farthest point M for bivariate
Gaussian case: (a) 𝑅⊂𝑆; (b) 𝑅⋂𝑆≠∅; (c) 𝑅⋂𝑆= ∅."
IMPLEMENTATION/METHODS,0.5660377358490566,"Proposition 3.2. Let ̃𝑅= 𝑅⋂𝑅𝑆(resp. 𝑅⧵𝑅𝑆), with 𝑅∈R and 𝑆∈S. We compute the boundaries
( ̃𝑙, ̃𝑟) of ̃𝑅using the following rule:"
IMPLEMENTATION/METHODS,0.5691823899371069,"1. We define the point ̃𝜃∈ℝ𝑝as the closest point m (resp. farthest M). For all 𝑘= 1, … 𝑝we find the
roots 𝜃𝑘1 and 𝜃𝑘2 of the one-variable (𝜃𝑘) equation"
IMPLEMENTATION/METHODS,0.5723270440251572,"𝑠𝑘(𝜃𝑘) + ∑
𝑗≠𝑘
𝑠𝑗( ̃𝜃𝑗) −Δ = 0 ."
IMPLEMENTATION/METHODS,0.5754716981132075,"If the roots are real-valued we consider that 𝜃𝑘1 ≤𝜃𝑘2, otherwise we write [𝜃𝑘1, 𝜃𝑘2] = ∅."
IMPLEMENTATION/METHODS,0.5786163522012578,2. We compute the boundary values ̃𝑙𝑘and ̃𝑟𝑘of ̃𝑅as:
IMPLEMENTATION/METHODS,0.5817610062893082,"• For 𝑅⋂𝑅𝑆(𝑘= 1, … , 𝑝):"
IMPLEMENTATION/METHODS,0.5849056603773585,"[ ̃𝑙𝑘, ̃𝑟𝑘] = [𝜃𝑘1, 𝜃𝑘2] ⋂[𝑙𝑘, 𝑟𝑘] .
(11)"
IMPLEMENTATION/METHODS,0.5880503144654088,"• For 𝑅⧵𝑅𝑆(𝑘= 1, … , 𝑝):"
IMPLEMENTATION/METHODS,0.5911949685534591,"[ ̃𝑙𝑘, ̃𝑟𝑘] = {
[𝑙𝑘, 𝑟𝑘] ⧵[𝜃𝑘1, 𝜃𝑘2] ,
if
[𝜃𝑘1, 𝜃𝑘2] ⊄[𝑙𝑘, 𝑟𝑘] ,"
IMPLEMENTATION/METHODS,0.5943396226415094,"[𝑙𝑘, 𝑟𝑘] ,
otherwise ."
IMPLEMENTATION/METHODS,0.5974842767295597,"If there is a dimension 𝑘for which [ ̃𝑙𝑘, ̃𝑟𝑘] = ∅, then the set ̃𝑅is empty."
IMPLEMENTATION/METHODS,0.60062893081761,The proof of Proposition 3.2 is presented in Section 5.4.
IMPLEMENTATION/METHODS,0.6037735849056604,"As a partial conclusion to this theoretical study, those ideas could be extended to some other models
with missing values or dependencies between dimensions (e.g. piece-wise constant regression).
However, it would require introducing new approximation operators of potential high complexity."
RESULTS/EXPERIMENTS,0.6069182389937107,"4
Simulation Study of GeomFPOP"
RESULTS/EXPERIMENTS,0.610062893081761,"In this section, we study the efficiency of GeomFPOP using simulations of multivariate independent
time series. For this, we implemented GeomFPOP (with S and R types) and PELT for the Multivariate
Independent Gaussian Model in the R-package ‘GeomFPOP’ https://github.com/lpishchagina/Geom
FPOP written in R/C++. By default, the value of penalty 𝛽for each simulation was defined by the
Schwarz Information Criterion proposed in (Yao 1984) as 𝛽= 2𝑝𝜎2 log 𝑛with 𝜎= 1 known. As long
as the per-dimension variance is known (or appropriately estimated) we can make this assumption
(𝜎= 1 known) without loss of generality by rescaling the data by the standard deviation."
RESULTS/EXPERIMENTS,0.6132075471698113,"Overview of our simulations. First, for 2 ≤𝑝≤10 we generated 𝑝-variate independent time series
(multivariate independent Gaussian model with fixed variance) with 𝑛= 104 data points and number
of segments: 1, 5, 10, 50 and 100. The segment-specific parameter (mean) was set to 1 for even
segments, and 0 for odd segments. As a quality control measure, we verified that PELT and GeomFPOP
produced identical outputs on these simulated profiles. Second, we studied cases where the PELT
approach is not efficient, that is when the data has no or few changes relative to 𝑛. Indeed, it was
shown in (Killick, Fearnhead, and Eckley 2012) and (Maidstone et al. 2017) that the run time of
PELT is close to 𝒪(𝑛2) in such cases. So we considered simulations of multivariate time series
without change (only one segment). By these simulations we evaluated the pruning efficiency of
GeomFPOP (using S and R types) for dimension 2 ≤𝑝≤10 (see Figure 5 in Section 4.1). For small
dimensions we also evaluated the run time of GeomFPOP and PELT and compare them (see Figure 6
in Section 4.2). In addition, we considered another approximation of the 𝑍𝑖𝑡where we applied our
⋂𝑅and ⧵𝑅operators only for a randomly selected subset of the past and future balls. In practice,
this strategy turned out to be faster computationally than the full/original GeomFPOP and PELT
(see Figure 7 in Section 4.3). For this strategy we also generated time series of a fixed size (106 data
points) and varying number of segments and evaluated how the run time vary with the number of
segments for small dimensions (2 ≤𝑝≤4). Our empirical results confirmed that the GeomFPOP
(R-type: random/random) approach is computationally comparable to PELT when the number of
changes is large (see Figure 9 in Section 4.5)."
RESULTS/EXPERIMENTS,0.6163522012578616,"4.1
The Number of Change Point Candidates stored over Time"
RESULTS/EXPERIMENTS,0.6194968553459119,"We evaluate the functional pruning efficiency of the GeomFPOP method using 𝑝-variate independent
Gaussian noise of length 𝑛= 104 data points. For such series, PELT typically does not pruned (e.g. for
𝑡= 104, 𝑝= 2 it stores almost always 𝑡candidates)."
RESULTS/EXPERIMENTS,0.6226415094339622,"We report in Figure 5 the percentage of candidates that are kept by GeomFPOP as a function of 𝑛, 𝑝
and the type of pruning (R or S). Regardless of the type of approximation and contrary to PELT, we"
RESULTS/EXPERIMENTS,0.6257861635220126,"observe that there is some pruning. However when increasing the dimension 𝑝, the quality of the
pruning decreases."
RESULTS/EXPERIMENTS,0.6289308176100629,"Comparing the left plot of Figure 5 with the right plot we see that for dimensions 𝑝= 2 to 𝑝= 5
R-type prunes more than the S-type, while for larger dimensions the S-type prunes more than the
R-type. For example, for 𝑝= 2 at time 𝑡= 104 by GeomFPOP (R-type) the number of candidates
stored over 𝑡does not exceed 1% versus 3% by GeomFPOP (S-type). This intuitively makes sense.
One the one hand, the R-type approximation of a sphere deteriorates as the dimension increases. On
the other hand with R-type approximation every new approximation is included in the previous one.
For small dimensions this memory effect outweighs the roughness of the approximation."
RESULTS/EXPERIMENTS,0.6320754716981132,"Figure 5: Percentage of candidate change points stored over time by GeomFPOP with R (left) or S
(right) type pruning for dimension 𝑝= 2, … , 10. Averaged over 100 data sets."
RESULTS/EXPERIMENTS,0.6352201257861635,"Based on these results we expect that R-type pruning GeomFPOP will be more efficient than S-type
pruning for small dimensions."
RESULTS/EXPERIMENTS,0.6383647798742138,"4.2
Empirical Time Complexity of GeomFPOP"
RESULTS/EXPERIMENTS,0.6415094339622641,"We studied the run time of GeomFPOP (S and R-type) and compared it to PELT for small dimensions.
We simulated data generated by a 𝑝-variate i.i.d. Gaussian noise and saved their run times with a three
minutes limit. The results are presented in Figure 6. We observe that GeomFPOP is faster than PELT
only for 𝑝= 2. For 𝑝= 3 run times are comparable and for 𝑝= 4 GeomFPOP is slower. This is not in
line with the fact that GeomFPOP prunes more than PELT. However, as explained in Section 2.1, the
computational complexity of GeomFPOP and PELT is affected by both the efficiency of pruning and
the number of comparisons conducted at each step. For PELT at time 𝑡, all candidates are compared
to the last change, resulting in a complexity of order 𝒪(𝑝|𝜏𝑃𝐸𝐿𝑇
𝑡
|). On the other hand, GeomFPOP
compares all candidates to each other (refer to Algorithm 1 and the remark from Section 2.1), leading
to a complexity of order 𝒪(𝑝|𝜏𝐺𝑒𝑜𝑚𝐹𝑃𝑂𝑃
𝑡
|2). In essence, the complexity of GeomFPOP is governed
by the square of the number of candidates. Therefore, GeomFPOP is expected to be more efficient
than PELT only if its square number of candidates is smaller than the number of candidates for
PELT. Based on the information presented in} Figure 6, we argue that this condition holds true only
for dimensions 𝑝= 2 and 3. Indeed, analysis of the number of comparisons between PELT and
GeomFPOP (see Section 5.6) supports this claim, revealing that GeomFPOP (S-type) outperforms
PELT only when 𝑝≤2 and GeomFPOP (R-type) outperforms PELT only when 𝑝≤3} (see Figure 12
in Section 5.6). This leads us to consider a randomized version of GeomFPOP."
RESULTS/EXPERIMENTS,0.6446540880503144,"Figure 6: Run time of GeomFPOP (S and R types) and PELT using multivariate time series without
change points. The maximum run time of the algorithms is 3 minutes. Averaged over 100 data sets."
RESULTS/EXPERIMENTS,0.6477987421383647,"4.3
Empirical Time Complexity of a Randomized GeomFPOP"
RESULTS/EXPERIMENTS,0.6509433962264151,"R-type GeomFPOP is designed in such a way that at each iteration we need to consider all past and
future spheres of change 𝑖. In practice, it is often sufficient to consider just a few of them to get an
empty set. Having this in mind, we propose a further approximation of the 𝑍𝑖𝑡where we apply our
⋂𝑅and ⧵𝑅operators only for a randomly selected subset of the past and future sets. In detail, we
propose to redefine the output of the select() function in Algorithm 1 for any sets 𝒫𝑖and ℱ𝑖(𝑡) as:"
RESULTS/EXPERIMENTS,0.6540880503144654,"• select(𝒫𝑖) returns one random set from 𝒫𝑖.
• select(ℱ𝑖(𝑡)) returns the last set 𝑆𝑖𝑡and one random set from ℱ𝑖(𝑡)."
RESULTS/EXPERIMENTS,0.6572327044025157,"Thus, we consider the following geometric update rule:"
RESULTS/EXPERIMENTS,0.660377358490566,• (random/random) At time 𝑡we update hyperrectangle:
RESULTS/EXPERIMENTS,0.6635220125786163,"1. by only two intersection operations: one with the last S-type set 𝑆𝑖𝑡from ℱ𝑖(𝑡), and one
with a random S-type set from ℱ𝑖(𝑡);
2. by only one exclusion operation with a random S-type set from 𝒫𝑖."
RESULTS/EXPERIMENTS,0.6666666666666666,"In this approach, at time 𝑡we need no more than three operations to update the testing set ̃𝑍𝑖𝑡for
each (𝑖−1) ∈𝜏𝑡. As can be seen in Figure Figure 11 of Section 5.5, by making less comparisons, we
prune less change points than in the general GeomFPOP (R-type) case, but still more than PELT.
It is important to note that in this randomization, we compare each change point candidate with
only two other change point candidates (rather than all in the general case of GeomFPOP (R-type)).
Therefore, informally our complexity at time step 𝑡is only 𝒪(𝑝|𝜏𝐺𝑒𝑜𝑚𝐹𝑃𝑂𝑃(random/random)
𝑡
|). According
to the remark from Section 2.1 and the discussion in Section 4.2, even with large values of 𝑝, the
overall complexity of GeomFPOP should not be worse than that of PELT. We investigated other
randomized strategies (see Section 5.5) but this simple one was sufficient to significantly improve
run times. The run time of our optimization approach and PELT in dimension (𝑝= 2, … , 10, 100) are
presented in Figure 7. As in Section 4.2, run times were limited to three minutes and were recorded
for simulations of length ranging 𝑛from 210 to 223 data points (𝑝-variate i.i.d. Gaussian noise)."
RESULTS/EXPERIMENTS,0.6698113207547169,"Although the (random/random) approach reduces the quality of pruning (see Section 5.5), it gives a
significant gain in run time compared to PELT in small dimensions. To be specific, with a run time of
five minutes GeomFPOP, on average, processes a time series with a length of about 8 × 106, 106 and
2, 5 × 105 data points in the dimensions 𝑝= 2, 3 and 4, respectively. At the same time, PELT manages
to process time series with a length of at most 6, 5 × 104 data points in these dimensions."
RESULTS/EXPERIMENTS,0.6729559748427673,"Figure 7: Run time of the (random/random) approach of { GeomFPOP} (R-type) and PELT using
p-variate time series without change points (𝑝= 2, … , 10, 100). The maximum run time of the
algorithms is 3 minutes. Averaged over 100 data sets."
RESULTS/EXPERIMENTS,0.6761006289308176,"4.4
Empirical Complexity of the Algorithm as a Function of 𝑝"
RESULTS/EXPERIMENTS,0.6792452830188679,"We also evaluate the slope coefficient 𝛼of the run time curve of GeomFPOP with random sampling
of the past and future candidates for all considered dimensions. In Figure 8 we can see that already
for 𝑝≥7 𝛼is close to 2."
RESULTS/EXPERIMENTS,0.6823899371069182,"Figure 8: Run time dependence of (random/random) approach of GeomFPOP (R-type) on dimension
𝑝."
RESULTS/EXPERIMENTS,0.6855345911949685,"4.5
Run Time as a Function of the Number of Segments"
RESULTS/EXPERIMENTS,0.6886792452830188,"For small dimensions we also generated time series with 𝑛= 106 data points with increasing number
of segments. We have considered the following number of segments: (1, 2, 5) × 10𝑖(for 𝑖= 0, … , 3)
and 104. The mean was equal to 1 for even segments, and 0 for odd segments. In Figure 9 we can
see the run time dependence of the (random/random) approach of GeomFPOP (R-type) and PELT on
the number of segments for this type of time series. For smaller number of segments (the threshold
between small and large numbers of segments is around 5 × 103 for all considered dimensions 𝑝)
GeomFPOP (random/random) is an order of magnitude faster. But for large number of segments, it
can be seen that the run times (both PELT and GeomFPOP) are larger. This might be a bit counter-"
RESULTS/EXPERIMENTS,0.6918238993710691,"intuitive. However, it is essential to recall that a similar trend of increased run time for a large number
of segments was already noted in the one-dimensional case, as demonstrated in (Maidstone et al.
2017). This observation is explained as follows. When the number of segments becomes excessively
large, the algorithm (both PELT and GeomFPOP) tends to interpret this abundance as an indication
of no change, resulting in reduced pruning. As a conclusion of this simulation study, in Section 5.7
we make a similar analysis, but using time series in which changes are present only in a subset of
dimensions. We observe that in this case GeomFPOP (random/random) will be slightly less effective
but no worse than no change (see Figure 13)."
RESULTS/EXPERIMENTS,0.6949685534591195,"Figure 9: Run time dependence of (random/random) approach of GeomFPOP (R-type) on the number
of segments in time series with 106 data points."
OTHER,0.6981132075471698,Acknowledgments
OTHER,0.7012578616352201,We thank Paul Fearnhead for fruitful discussions.
OTHER,0.7044025157232704,"5
Supplements"
OTHER,0.7075471698113207,"5.1
Examples of Likelihood-Based Cost Functions"
OTHER,0.710691823899371,"We define a cost function for segmentation as in Equation 1 by the function Ω(⋅, ⋅) (the opposite log-
likelihood (times two)). Below is the expression of this function linked to data point 𝑦𝑖= (𝑦1
𝑖, … , 𝑦𝑝
𝑖)
in ℝ𝑝for three examples of parametric multivariate models:"
OTHER,0.7138364779874213,"Ω(𝜃, 𝑦𝑖) = ⎧
⎪⎪⎪"
OTHER,0.7169811320754716,"⎨
⎪⎪⎪
⎩"
OTHER,0.720125786163522,"𝑝
∑
𝑘=1
(𝑦𝑘
𝑖−𝜃𝑘)2 ,
if 𝑦𝑖∼𝒩𝑝(𝜃, 𝜎2𝕀𝑝) , 2"
OTHER,0.7232704402515723,"𝑝
∑
𝑘=1
{𝜃𝑘−log ((𝜃𝑘)𝑦𝑘
𝑖"
OTHER,0.7264150943396226,"𝑦𝑘
𝑖!
)} ,
if 𝑦𝑖∼𝒫(𝜃) , −2"
OTHER,0.7295597484276729,"𝑝
∑
𝑘=1
log ((𝜃𝑘)𝑦𝑘
𝑖(1 −𝜃𝑘)𝜙(𝑦𝑘
𝑖+ 𝜙−1
𝑦𝑘
𝑖
)) ,
if 𝑦𝑖∼𝒩ℬ(𝜃, 𝜙) . (12)"
OTHER,0.7327044025157232,"We suppose that the over-dispersion parameter 𝜙of the multivariate negative binomial distribution
is known."
OTHER,0.7358490566037735,"5.2
Intersection and Inclusion of Two p-balls"
OTHER,0.7389937106918238,"We define two 𝑝-balls, 𝑆and 𝑆′ in ℝ𝑝using their centers 𝑐, 𝑐′ ∈ℝ𝑝and radius 𝑅, 𝑅′ ∈ℝ+ as"
OTHER,0.7421383647798742,"𝑆= {𝑥∈ℝ𝑝, ||𝑥−𝑐||2 ≤𝑅2} and 𝑆′ = {𝑥∈ℝ𝑝, ||𝑥−𝑐′||2 ≤𝑅′2},"
OTHER,0.7452830188679245,"where ||𝑥−𝑐||2 = ∑𝑝
𝑘=1(𝑥𝑘−𝑐𝑘)2, with 𝑥= (𝑥1, ..., 𝑥𝑝) ∈ℝ𝑝, is the Euclidean norm. The distance
between centers 𝑐and 𝑐′ is defined as 𝑑(𝑐, 𝑐′) = √||𝑐−𝑐′||2. We have the following simple results:"
OTHER,0.7484276729559748,"𝑆∩𝑆′ = ∅⟺𝑑(𝑐, 𝑐′) > 𝑅+ 𝑅′ ,"
OTHER,0.7515723270440252,"𝑆⊂𝑆′ or 𝑆′ ⊂𝑆⟺𝑑(𝑐, 𝑐′) ≤|𝑅−𝑅′| ."
OTHER,0.7547169811320755,"5.3
Intersection and Inclusion Tests"
OTHER,0.7578616352201258,"Remark. For any 𝑆𝑖
𝑗∈S its associated function 𝑠can be redefine after normalization by constant
𝑗−𝑖+ 1 as:"
OTHER,0.7610062893081762,"𝑠(𝜃) = 𝑎(𝜃) + ⟨𝑏, 𝜃⟩+ 𝑐,"
OTHER,0.7641509433962265,"with 𝑎(⋅) is some convex function depending on 𝜃, 𝑏= {𝑏𝑘}𝑘=1,…,𝑝∈ℝ𝑝and 𝑐∈ℝ."
OTHER,0.7672955974842768,"For example, in the Gaussian case, the elements have the following form:"
OTHER,0.7704402515723271,"𝑎∶𝜃↦𝜃2 ,
𝑏𝑘= 2 ̄𝑌𝑘
𝑖∶𝑗,
𝑐= ̄𝑌2
𝑖∶𝑗−Δ𝑖𝑗,"
OTHER,0.7735849056603774,"where ̄𝑌𝑘
𝑖∶𝑗=
1
𝑗−𝑖+1 ∑𝑗
𝑢=𝑖+1 𝑦𝑘𝑢and ̄𝑌2
𝑖∶𝑗=
1
𝑗−𝑖+1 ∑𝑗
𝑢=𝑖+1 ∑𝑝
𝑘=1(𝑦𝑘𝑢)2."
OTHER,0.7767295597484277,"Definition 5.1. For all 𝜃∈ℝ𝑝and 𝑆1, 𝑆2 ∈S with their associated functions, 𝑠1 and 𝑠2, we define a
function ℎ12 and a hyperplane 𝐻12 as:"
OTHER,0.779874213836478,"ℎ12(𝜃) ∶= 𝑠2(𝜃) −𝑠1(𝜃) ,
𝐻12 ∶= {𝜃∈ℝ𝑝|ℎ12(𝜃) = 0} ."
OTHER,0.7830188679245284,"We denote by 𝐻+
12 ∶= {𝜃∈ℝ𝑝|ℎ12(𝜃) > 0} and 𝐻−
12 ∶= {𝜃∈ℝ𝑝|ℎ12(𝜃) < 0} the positive and negative
half-spaces of 𝐻12, respectively. We call H the set of hyperplanes."
OTHER,0.7861635220125787,For all 𝑆∈S and 𝐻∈H we introduce a half −space operator.
OTHER,0.789308176100629,Definition 5.2. The operator half −space is such that:
OTHER,0.7924528301886793,"1. the left input is an S-type set 𝑆;
2. the right input is a hyperplane 𝐻;
3. the output is the half-spaces of 𝐻, such that 𝑆lies in those half-spaces."
OTHER,0.7955974842767296,"Definition 5.3. We define the output of half −space(𝑆, 𝐻) by the following rule:"
OTHER,0.7987421383647799,"1. We find two points, 𝜃1, 𝜃2 ∈ℝ𝑝, as: ⎧⎪ ⎨⎪
⎩"
OTHER,0.8018867924528302,"𝜃1 =
𝐴𝑟𝑔min 𝑠(𝜃),"
OTHER,0.8050314465408805,"𝜃2 =
{
𝐴𝑟𝑔min
𝜃∈𝑆ℎ(𝜃),
if 𝜃1 ∈𝐻+,"
OTHER,0.8081761006289309,"𝐴𝑟𝑔max
𝜃∈𝑆ℎ(𝜃),
if 𝜃1 ∈𝐻−."
OTHER,0.8113207547169812,2. We have:
OTHER,0.8144654088050315,"half −space(𝑆, 𝐻) =
⎧ ⎨
⎩"
OTHER,0.8176100628930818,"{𝐻+},
if 𝜃1, 𝜃2 ∈𝐻+,"
OTHER,0.8207547169811321,"{𝐻−},
if 𝜃1, 𝜃2 ∈𝐻−,"
OTHER,0.8238993710691824,"{𝐻+, 𝐻−},
otherwise."
OTHER,0.8270440251572327,"Lemma 5.1. 𝑆1 ⊂𝐻−
12 ⇔𝜕𝑆1 ⊂𝐻−
12, where 𝜕(⋅) denote the frontier operator."
OTHER,0.8301886792452831,The proof of Lemma 5.1 follows from the convexity of 𝑆1.
OTHER,0.8333333333333334,"Lemma 5.2. 𝑆1 ⊂𝑆2 (resp. 𝑆2 ⊂𝑆1) ⇔𝑆1, 𝑆2 ⊂𝐻−
12 (resp. 𝑆1, 𝑆2 ⊂𝐻+
12)."
OTHER,0.8364779874213837,"Proof. We have the hypothesis ℋ0 ∶{𝑆1 ⊂𝑆2}, then"
OTHER,0.839622641509434,"∀𝜃∈𝜕𝑆1
{𝑠1(𝜃) = 0,
[by Definition 1.1]"
OTHER,0.8427672955974843,"𝑠2(𝜃) ≤0,
[by ℋ0]
⇒𝜃∈𝐻−
12
⇒𝜕𝑆1 ⊂𝐻−
12."
OTHER,0.8459119496855346,"Thus, according to Lemma 5.1, 𝑆1 ⊂𝐻−
12."
OTHER,0.8490566037735849,"We have now the hypothesis ℋ0 ∶{𝑆1, 𝑆2 ⊂𝐻−
12}, then"
OTHER,0.8522012578616353,"∀𝜃∈𝑆1
{ 𝑠1(𝜃) ≤0,
[by Definition 1.1]"
OTHER,0.8553459119496856,"ℎ12(𝜃) < 0,
[by ℋ0, Definitions 5.1 and 1.1]
⇒𝜃∈𝑆2
⇒𝑆1 ⊂𝑆2."
OTHER,0.8584905660377359,"Similarly, it is easy to show that 𝑆2 ⊂𝑆1 ⇔𝑆1, 𝑆2 ⊂𝐻+
12."
OTHER,0.8616352201257862,Lemma 5.3. 𝑆1 ∩𝑆2 = ∅⇔𝐻12 is a separating hyperplane of 𝑆1 and 𝑆2.
OTHER,0.8647798742138365,"Proof. We have the hypothesis ℋ0 ∶{𝑆1 ⊂𝐻+
12, 𝑆2 ⊂𝐻−
12}. Thus, 𝐻12 is a separating hyperplane of
𝑆1 and 𝑆2 then, according to its definition, 𝑆1 ∩𝑆2 = ∅."
OTHER,0.8679245283018868,We have now the hypothesis ℋ0 ∶{𝑆1 ∩𝑆2 = ∅} then
OTHER,0.8710691823899371,"∀𝜃∈𝑆1
{𝑠1(𝜃) ≤0,
[by Definition 1.1]"
OTHER,0.8742138364779874,"𝑠2(𝜃) > 0,
[by ℋ0, Definition 1.1]
⇒𝜃∈𝐻+
12."
OTHER,0.8773584905660378,"∀𝜃∈𝑆2
{𝑠1(𝜃) > 0,
[by ℋ0, Definition 1.1]"
OTHER,0.8805031446540881,"𝑠2(𝜃) ≤0,
[by Definition 1.1]
⇒𝜃∈𝐻−
12."
OTHER,0.8836477987421384,"Consequently, 𝐻12 is a separating hyperplane of 𝑆1 and 𝑆2."
OTHER,0.8867924528301887,"Proposition 5.1. To detect set inclusion 𝑆1 ⊂𝑆2 and emptiness of set intersection 𝑆1 ∩𝑆2, it is necessary:"
OTHER,0.889937106918239,"1. build the hyperplane 𝐻12;
2. apply the half −space operator for couples (𝑆1, 𝐻12) and (𝑆2, 𝐻12) to know in which half-space(s)
𝑆1 and 𝑆2 are located;
3. check the conditions in Lemmas 5.2 and 5.3."
OTHER,0.8930817610062893,"5.4
Proof of Proposition 3.2"
OTHER,0.8962264150943396,"Proof. Let c = {c𝑘}𝑘=1,…,𝑝is the minimal point of 𝑆, defined as in Equation 10. In the intersection case,
we consider solving the optimization problem (9) for the boundaries ̃𝑙𝑘and ̃𝑟𝑘, removing constraint
𝑙𝑘≤𝜃𝑘≤𝑟𝑘. If 𝑅intersects 𝑆, the optimal solution 𝜃𝑘belongs to the boundary of 𝑆due to our simple
(axis-aligned rectangular) inequality constraints and we get"
OTHER,0.89937106918239,"𝑠𝑘(𝜃𝑘) = −∑
𝑗≠𝑘
𝑠𝑗(𝜃𝑗) + Δ .
(13)"
OTHER,0.9025157232704403,We are looking for minimum and maximum values in 𝜃𝑘for this equation with constraints 𝑙𝑗≤𝜃𝑗≤𝑟𝑗
OTHER,0.9056603773584906,"(𝑗≠𝑘). Using the convexity of 𝑠𝑘and 𝑠𝑗, we need to maximize the quantity in the right-hand side.
Thus, the solution ̃𝜃𝑗for each 𝜃𝑗is the minimal value of ∑𝑗≠𝑘𝑠𝑗(𝜃𝑗) under constraint 𝑙𝑗≤𝜃𝑗≤𝑟𝑗and
the result can only be 𝑙𝑗, 𝑟𝑗or c𝑗. Looking at all coordinates at the same time, the values for ̃𝜃∈ℝ𝑝"
OTHER,0.9088050314465409,"corresponds to the closest point m = {m𝑘}𝑘=1,…,𝑝. Having found 𝜃𝑘1 and 𝜃𝑘2 using ̃𝜃the result in
Equation 11 is obvious considering current boundaries 𝑙𝑘and 𝑟𝑘."
OTHER,0.9119496855345912,"In exclusion case, we remove from 𝑅the biggest possible rectangle included into 𝑆∩{𝑙𝑗≤𝜃𝑗≤𝑟𝑗, 𝑗≠𝑘},
which correspond to minimizing the right hand side of Equation 13, that is maximizing ∑𝑗≠𝑘𝑠𝑗(𝜃𝑗)
under constraint 𝑙𝑗≤𝜃𝑗≤𝑟𝑗(𝑗≠𝑘). In that case, the values for ̃𝜃correspond to the greatest value
returned by ∑𝑗≠𝑘𝑠𝑗(𝜃𝑗) on interval boundaries. With convex functions 𝑠𝑗, it corresponds to the
farthest point M = {M𝑘}𝑘=1,…,𝑝."
OTHER,0.9150943396226415,"5.5
Optimization Strategies for GeomFPOP (R-type)"
OTHER,0.9182389937106918,"In GeomFPOP(R-type) at each iteration, we need to consider all past and future spheres of change 𝑖.
As it was said in Section 4, in practice it is often sufficient to consider just a few of them to get an
empty set. Thus, we propose to limit the number of operations ∩𝑅no more than two:"
OTHER,0.9213836477987422,"• last. At time 𝑡we update hyperrectangle by only one operation, this is an intersection with
the last S-type set 𝑆𝑖𝑡from ℱ𝑖(𝑡).
• random. At time 𝑡we update the hyperrectangle by only two operations. First, this is an
intersection with the last S-type set 𝑆𝑖𝑡from ℱ𝑖(𝑡), and second, this is an intersection with
other random S-type set from ℱ𝑖(𝑡)."
OTHER,0.9245283018867925,The number of operations ⧵𝑅we limit no more than one:
OTHER,0.9276729559748428,"• empty. At time 𝑡we do not perform ⧵𝑅operations.
• random. At time 𝑡we update hyperrectangle by only one operation: exclusion with a random
S-type set from 𝒫𝑖."
OTHER,0.9308176100628931,"According to these notations, the approach presented in the original GeomFPOP (R-type) has the
form (all/all). We show the impact of introduced limits on the number of change point candidates
retained over time and evaluate their run times. The results are presented in Figures 10 and 11."
OTHER,0.9339622641509434,"Even though the (random/random) approach reduces the quality of pruning in dimensions 𝑝= 2, 3
and 4, it gives a significant gain in the run time compared to the original GeomFPOP (R-type) and is
at least comparable to the (last/random) approach."
OTHER,0.9371069182389937,"Figure 10: Ratio number of candidate change point over time by different optimization approaches of
GeomFPOP (R-type) in dimension 𝑝= 2, 3 and 4. Averaged over 100 data sets without changes with
104 data points."
OTHER,0.940251572327044,"Figure 11: Run time of different optimization approaches of GeomFPOP (R-type) using multivariate
time series without change points. The maximum run time of the algorithms is 3 minutes. Averaged
over 100 data sets."
OTHER,0.9433962264150944,"5.6
The number of change point candidates in time: GeomFPOP vs. PELT"
OTHER,0.9465408805031447,"In this appendix we compare the square of the number of candidates stored by GeomFPOP (S and
R-type) to the corresponding number of candidates stored by PELT over time. Indeed, the complexity
of GeomFPOP at each time step is a function of the square of the number of candidates, while, for
PELT, of the number of candidates (see Section 4.2). Figure 12 shows the ratios of these computed
quantities for dimension 2 ≤𝑝≤10. It is noteworthy that for both S-type and R-type for 𝑝= 2
this ratio is almost always less than 1 and decreases with time. This is coherent with the fact that
GeomFPOP is faster than PELT (see Figure 6). At 𝑝= 3 for the R-type this ratio is approximately 1,
while for the S-type it is greater than 1 and continues to increase with increasing 𝑡value. For sizes
3 < 𝑝≤10, this ratio remains consistently greater than 1 for both S-type and R-type, showing a
continuous increasing trend with time. This is coherent with the fact that GeomFPOP is almost as
fast as PELT for 𝑝= 3 and slower than PELT for 𝑝≤4 (see Figure 6)."
OTHER,0.949685534591195,"5.7
Run time of the algorithm by multivariate time series with changes in subset
of dimension"
OTHER,0.9528301886792453,"We expect GeomFPOP (random/random) to be slightly less effective (but no worse than in the absence
of changes) if changes are only present in a subset of dimensions. To this end, in this appendix for"
OTHER,0.9559748427672956,"Figure 12: The ratio of the square of the number of candidates stored in GeomFPOP to the number of
candidates stored in PELT over time. The horizontal black line corresponds to the value 1."
OTHER,0.9591194968553459,"dimension 2 ≤𝑝≤4 we examine the run time of GeomFPOP (random/random) as in Section 4.5
(see Figure 9) but removing all changes in the last 𝑘dimensions (with 𝑘= 0, … , 𝑝−1). The results
are presented in Figure 13. There are two regimes. For a small number of segments (the threshold
between small and large numbers of segments is around 2 × 103 for all considered dimensions 𝑝),
the run time decreases with the number of segments and the difference between the run time of
GeomFPOP (random/random) for 𝑘= 0 (this case corresponds to changes in all dimensions) and
𝑘> 0 is very small. For larger number of segments, the run time increases with the number of
segments, as in Section 4.5 and also increases with 𝑘. Importantly, in this regime the run time is
never lower than for 1 segment."
OTHER,0.9622641509433962,"p = 2
p = 3
p = 4"
OTHER,0.9654088050314465,"100
101
102
103
104 100
101
102
103
104 100
101
102
103
104 101 102 103"
OTHER,0.9685534591194969,Number of segments into a time series with 10⁶ data points
OTHER,0.9716981132075472,Seconds
OTHER,0.9748427672955975,"k
0
1
2
3"
OTHER,0.9779874213836478,"Figure 13: Dependence of the run time of the (random/random) approach of GeomFPOP (R-type) on
the number of segments in a 𝑝-variable time series with 106 data points where all changes in the last
𝑘dimensions have been removed."
REFERENCES,0.9811320754716981,References
REFERENCES,0.9842767295597484,"Aminikhanghahi, Samaneh, and Diane J Cook. 2017. “A Survey of Methods for Time Series Change
Point Detection.” Knowledge and Information Systems 51 (2): 339–67.
Anastasiou, Andreas, and Piotr Fryzlewicz. 2022. “Detecting Multiple Generalized Change-Points by"
REFERENCES,0.9874213836477987,"Isolating Single Ones.” Metrika 85 (February). https://doi.org/10.1007/s00184-021-00821-6.
Andreou, Elena, and Eric Ghysels. 2002. “Detecting Multiple Breaks in Financial Market Volatility
Dynamics.” Journal of Applied Econometrics 17 (5): 579–600. http://www.jstor.org/stable/4129273.
Aue, Alexander, Lajos Horváth, Marie Hušková, and Piotr Kokoszka. 2006. “Change-Point Monitoring
in Linear Models.” The Econometrics Journal 9 (3): 373–403. http://www.jstor.org/stable/23114925.
Auger, Ivan E., and Charles E. Lawrence. 1989. “Algorithms for the Optimal Identification of Segment
Neighborhoods.” Bulletin of Mathematical Biology 51 (1): 39–54. https://doi.org/10.1007/BF0245
8835.
Bai, Jushan, and Pierre Perron. 2003. “Computation and Analysis of Multiple Structural-Change.”
Journal of Applied Econometrics 18 (January).
Bosc, Marcel, Fabrice Heitz, Jean-Paul Armspach, Izzie Namer, Daniel Gounot, and Lucien Rumbach.
2003. “Automatic Change Detection in Multimodal Serial MRI: Application to Multiple Sclerosis
Lesion Evolution.” NeuroImage 20(2), 643–56. https://doi.org/https://doi.org/10.1016/S1053-
8119(03)00406-3.
Data, Committee, Committee Statistics, Board Applications, Division Sciences, and National Council.
2013. Frontiers in Massive Data Analysis. Frontiers in Massive Data Analysis. The National
Academies Press. https://doi.org/10.17226/18374.
Davis, Richard A., Thomas C. M. Lee, and Gabriel A. Rodriguez-Yam. 2006. “Structural Break
Estimation for Nonstationary Time Series Models.” Journal of the American Statistical Association
101: 223–39. https://EconPapers.repec.org/RePEc:bes:jnlasa:v:101:y:2006:p:223-239.
Ducré-Robitaille, Jean-François, Lucie A. Vincent, and Gilles Boulet. 2003. “Comparison of Techniques
for Detection of Discontinuities in Temperature Series.” International Journal of Climatology 23.
Fearnhead, Paul, Robert Maidstone, and Adam Letchford. 2018. “Detecting Changes in Slope with an
L0 Penalty.” Journal of Computational and Graphical Statistics, 1–11.
Frick, Klaus, Axel Munk, and Hannes Sieling. 2013. “Multiscale Change-Point Inference.” arXiv."
REFERENCES,0.9905660377358491,"https://doi.org/10.48550/ARXIV.1301.7212.
Fryzlewicz, Piotr. 2014. “Wild Binary Segmentation for Multiple Change-Point Detection.” The
Annals of Statistics 42 (6). https://doi.org/10.1214/14-aos1245.
Galceran, Enric, Alexander Cunningham, Ryan Eustice, and Edwin Olson. 2017. “Multipolicy
Decision-Making for Autonomous Driving via Changepoint-Based Behavior Prediction: Theory
and Experiment.” Autonomous Robots 41 (August). https://doi.org/10.1007/s10514-017-9619-z.
Hall, Peter, J. W. Kay, and D. M. Titterington. 1990. “Asymptotically Optimal Difference-Based
Estimation of Variance in Nonparametric Regression.” Biometrika 77 (3): 521–28. http://www.js
tor.org/stable/2336990.
Hampel, Frank R. 1974. “The Influence Curve and Its Role in Robust Estimation.” Journal of the
American Statistical Association 69 (346): 383–93. http://www.jstor.org/stable/2285666.
Harchaoui, Z., and C. Lévy-Leduc. 2010. “Multiple Change-Point Estimation with a Total Variation
Penalty.” Journal of the American Statistical Association. 105 (492): 1480–93. http://www.jstor.or
g/stable/27920180.
Jackson, Brad, Jeffrey D Scargle, David Barnes, Sundararajan Arabhi, Alina Alt, Peter Gioumousis,
Elyus Gwin, Paungkaew Sangtrakulcharoen, Linda Tan, and Tun Tao Tsai. 2005. “An Algorithm
for Optimal Partitioning of Data on an Interval.” IEEE Signal Processing Letters 12 (2): 105–8.
Jewell, Sean, Paul Fearnhead, and Daniela Witten. 2019. “Testing for a Change in Mean After
Changepoint Detection.” arXiv. https://doi.org/10.48550/ARXIV.1910.04291.
Killick, Rebecca, Paul Fearnhead, and Idris A. Eckley. 2012. “Optimal Detection of Changepoints with
a Linear Computational Cost.” Journal of the American Statistical Association 107 (500): 1590–98.
Lai, Weil R, Mark D Johnson, Raju Kucherlapati, and Peter J Park. 2005. “Comparative Analysis of
Algorithms for Identifying Amplifications and Deletions in Array CGH Data.” Bioinformatics 21
(19): 3763–70.
Lavielle, Marc, and Émilie Lebarbier. 2001. “An Application of MCMC Methods for the Multiple"
REFERENCES,0.9937106918238994,"Change-Points Problem.” Signal Processing 81: 39–53. https://api.semanticscholar.org/CorpusID:
9866087.
Lavielle, Marc, and Eric Moulines. 2000. “Least-Squares Estimation of an Unknown Number of Shifts
in a Time Series.” Journal of Time Series Analysis 21 (1): 33–59.
Lebarbier, Emilie. 2005. “Detecting Multiple Change-Points in the Mean of Gaussian Process by Model
Selection.” Signal Processing 85 (April): 717–36. https://doi.org/10.1016/j.sigpro.2004.11.012.
Liehrmann, Arnaud, Etienne Delannoy, Alexandra Launay-Avon, Elodie Gilbault, Olivier Loudet,
Benoît Castandet, and Guillem Rigaill. 2023. “DiffSegR: an RNA-seq data driven method for
differential expression analysis using changepoint detection.” NAR Genomics and Bioinformatics
5 (4): lqad098. https://doi.org/10.1093/nargab/lqad098.
Liehrmann, Arnaud, Guillem Rigaill, and Toby Dylan Hocking. 2021. “Increased Peak Detection
Accuracy in over-Dispersed ChIP-Seq Data with Supervised Segmentation Models.” BMC Bioin-
formatics 22 (1): 1–18.
Maidstone, Robert, Toby Hocking, Guillem Rigaill, and Paul Fearnhead. 2017. “On Optimal Multiple
Changepoint Algorithms for Large Data.” Statistics and Computing 27 (2): 519–33.
Malladi, Rakesh, Giridhar P. Kalamangalam, and Behnaam Aazhang. 2013. “Online Bayesian Change
Point Detection Algorithms for Segmentation of Epileptic Activity.” 2013 Asilomar Conference on
Signals, Systems and Computers, 1833–37.
Naoki, Itoh, and Juergen Kurths. 2010. “Change-Point Detection of Climate Time Series by Nonpara-
metric Method.” Lecture Notes in Engineering and Computer Science 2186 (October).
Olshen, Adam, E. S. Venkatraman, Robert Lucito, and Michael Wigler. 2004. “Circular Binary
Segmentation for the Analysis of Array-Based DNA Copy Number Data.” Biostatistics (Oxford,
England) 5 (November): 557–72. https://doi.org/10.1093/biostatistics/kxh008.
Picard, Franck, Stephane Robin, Marc Lavielle, Christian Vaisse, and Jean-Jacques Daudin. 2005.
“A Statistical Approach for Array CGH Data Analysis.” BMC Bioinformatics 6: np. https:
//doi.org/10.1186/1471-2105-6-27.
Radke, R. J., S. Andra, O. Al-Kofahi, and B. Roysam. 2005. “Image Change Detection Algorithms: A
Systematic Survey.” IEEE Transactions on Image Processing 14 (3): 294–307. https://doi.org/10.110
9/TIP.2004.838698.
Ranganathan, Ananth. 2012. “PLISS: Labeling Places Using Online Changepoint Detection.” Auton.
Robots 32 (4): 351–68. https://doi.org/10.1007/s10514-012-9273-4.
Reeves, Jaxk, Jien Chen, Xiaolan L. Wang, Robert Lund, and Qi Qi Lu. 2007. “A Review and Compari-
son of Changepoint Detection Techniques for Climate Data.” Journal of Applied Meteorology and
Climatology 46 (6): 900–915. https://doi.org/10.1175/JAM2493.1.
Rigaill, Guillem. 2015. “A Pruned Dynamic Programming Algorithm to Recover the Best Segmenta-
tions with 1 to 𝐾𝑚𝑎𝑥Change-Points.” Journal de La Société Française de Statistique 156 (4): 180–205.
http://www.numdam.org/item/JSFS_2015__156_4_180_0/.
Runge, Vincent. 2020. “Is a Finite Intersection of Balls Covered by a Finite Union of Balls in Euclidean
Spaces?” Journal of Optimization Theory and Applications 187 (2): 431–47.
Rybach, David, Christian Gollan, Ralf Schluter, and Hermann Ney. 2009. “Audio Segmentation for
Speech Recognition Using Segment Features.” In 2009 IEEE International Conference on Acoustics,
Speech and Signal Processing, 4197–4200. https://doi.org/10.1109/ICASSP.2009.4960554.
Staudacher, Martin, Stefan Telser, Anton Amann, Hartmann Hinterhuber, and Monika Ritsch-Marte.
2005. “A New Method for Change-Point Detection Developed for on-Line Analysis of the Heart
Beat Variability During Sleep.” Physica A-Statistical Mechanics and Its Applications 349: 582–96.
Truong, Charles, Laurent Oudre, and Nicolas Vayatis. 2020. “Selective Review of Offline Change
Point Detection Methods.” Signal Processing 167: 107299.
Verzelen, Nicolas, Magalie Fromont, Matthieu Lerasle, and Patricia Reynaud-Bouret. 2020. “Optimal
Change-Point Detection and Localization.” arXiv. https://doi.org/10.48550/ARXIV.2010.11470.
Yao, Yi-Ching. 1984. “Estimation of a Noisy Discrete-Time Step Function: Bayes and Empirical Bayes"
REFERENCES,0.9968553459119497,"Approaches.” The Annals of Statistics 12 (4): 1434–47. https://doi.org/10.1214/aos/1176346802.
Zhang, Nancy, and David Siegmund. 2007. “A Modified Bayes Information Criterion with Applications
to the Analysis of Comparative Genomic Hybridization Data.” Biometrics 63 (April): 22–32.
https://doi.org/10.1111/j.1541-0420.2006.00662.x."
