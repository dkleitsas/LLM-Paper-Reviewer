Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017730496453900709,"Adversarial Training (AT) is the de-facto standard for improving robustness against
1"
ABSTRACT,0.0035460992907801418,"adversarial examples. This usually involves a multi-step adversarial attack applied
2"
ABSTRACT,0.005319148936170213,"on each example during training. In this paper, we explore only constructing
3"
ABSTRACT,0.0070921985815602835,"Adversarial Examples (AEs) on a subset of the training examples. That is, we
4"
ABSTRACT,0.008865248226950355,"split the training set in two subsets A and B, train models on both (A ‚à™B) but
5"
ABSTRACT,0.010638297872340425,"construct AEs only for examples in A. Starting with A containing only a single
6"
ABSTRACT,0.012411347517730497,"class, we systematically increase the size of A and consider splitting by class and by
7"
ABSTRACT,0.014184397163120567,"examples. We observe that: (i) adv. robustness transfers by difficulty and to classes
8"
ABSTRACT,0.015957446808510637,"in B that have never been adv. attacked during training, (ii) we observe a tendency
9"
ABSTRACT,0.01773049645390071,"for hard examples to provide better robustness transfer than easy examples, yet find
10"
ABSTRACT,0.01950354609929078,"this tendency to diminish with increasing complexity of datasets (iii) generating
11"
ABSTRACT,0.02127659574468085,"AEs on only 50% of training data is sufficient to recover most of the baseline AT
12"
ABSTRACT,0.02304964539007092,"performance even on ImageNet. We observe similar transfer properties across tasks,
13"
ABSTRACT,0.024822695035460994,"where generating AEs on only 30% of data can recover baseline robustness on the
14"
ABSTRACT,0.026595744680851064,"target task. We evaluate our subset analysis on a wide variety of image datasets
15"
ABSTRACT,0.028368794326241134,"like CIFAR-10, CIFAR-100, ImageNet-200 and show transfer to SVHN, Oxford-
16"
ABSTRACT,0.030141843971631204,"Flowers-102 and Caltech-256. In contrast to conventional practice, our experiments
17"
ABSTRACT,0.031914893617021274,"indicate that the utility of computing AEs varies by class and examples and that
18"
ABSTRACT,0.03368794326241135,"weighting examples from A higher than B provides high transfer performance.
19"
INTRODUCTION,0.03546099290780142,"1
Introduction
20"
INTRODUCTION,0.03723404255319149,"Imperceptible changes in the input can change the output of a well performing model dramatically.
21"
INTRODUCTION,0.03900709219858156,"These so-called Adversarial Examples (AEs) have been the focus of a large body on deep learning
22"
INTRODUCTION,0.040780141843971635,"vulnerabilities of works since its discovery [1]. To date, Adversarial Training (AT) [2, 3] and its
23"
INTRODUCTION,0.0425531914893617,"variants [4‚Äì6] is the de-facto state-of-the-art in improving the robustness against AEs. Essentially, AT
24"
INTRODUCTION,0.044326241134751775,"generates adversarial perturbations for all examples seen during training. While adversarial training
25"
INTRODUCTION,0.04609929078014184,"is known to transfer robustness to downstream tasks [7‚Äì9] and that robustness is distributed unevenly
26"
INTRODUCTION,0.047872340425531915,"across classes [10, 11], common practice dictates that AT ‚Äúsees‚Äù adversarial examples corresponding
27"
INTRODUCTION,0.04964539007092199,"to the whole training data, including all classes and concepts therein. This is independent of whether
28"
INTRODUCTION,0.051418439716312055,"only adversarial robustness is optimized or a trade-off between robustness and clean performance
29"
INTRODUCTION,0.05319148936170213,"is desired [12]. This also holds for variants that treat individual examples differently [13‚Äì15] or
30"
INTRODUCTION,0.0549645390070922,"adaptively select subsets to attack during training to reduce computational overhead [16, 17]. It is
31"
INTRODUCTION,0.05673758865248227,"largely unclear how adversarial robustness is affected when training is limited to seeing adversarial
32"
INTRODUCTION,0.05851063829787234,"examples only on specific subsets of the training data.
33"
INTRODUCTION,0.06028368794326241,"To shed light on this issue, we consider the adversarial training setup depicted in figure 1, called
34"
INTRODUCTION,0.06205673758865248,"Subset Adversarial Training (SAT), where we split the training data into two subsets A and B, train
35"
INTRODUCTION,0.06382978723404255,"the model conventionally on the union (A‚à™B), but generate AEs only on examples from A (indicated
36 ùúñ"
INTRODUCTION,0.06560283687943262,"Figure 1: Adversarial robustness transfers among classes. Using Subset Adversarial Training (SAT),
during which only a subset of all training examples (A) are attacked, we show that robust training
even on a single class provides robustness transfer to all other, non adv. trained, classes (B). E.g.,
SAT for A=cat, we observe an robust accuracy of 37.8% on B. Noteworthy is the difference of
transfer utility between classes. I.e. A=car provides very little transfer to B (17.1%). We investigate
this transfer among classes and provide new insights for robustness transfer to downstream tasks."
INTRODUCTION,0.0673758865248227,"by the emoji). For example, we can split training data by class, with A = {car} or A = {cat} and
37"
INTRODUCTION,0.06914893617021277,"B = Ac, and investigate how adversarial robustness transfers. Surprisingly, we observe significant
38"
INTRODUCTION,0.07092198581560284,"adversarial robustness on Bval at test time, the degree of which depends on the class(es) in A. Of
39"
INTRODUCTION,0.0726950354609929,"course, A and B can be arbitrary partitions of the training data. For example, we could put only
40"
INTRODUCTION,0.07446808510638298,"‚Äúdifficult‚Äù examples in A during training. At test time, we evaluate overall adversarial robustness
41"
INTRODUCTION,0.07624113475177305,"(since there is no natural split into Aval or Bval). These experiments reveal a rather complex interaction
42"
INTRODUCTION,0.07801418439716312,"of adversarial robustness between classes and examples.
43"
INTRODUCTION,0.0797872340425532,"Our analysis provides a set of contributions revealing a surprising generalizability of robustness
44"
INTRODUCTION,0.08156028368794327,"towards non-adv. trained classes and examples even under scarce training data setups. First, selecting
45"
INTRODUCTION,0.08333333333333333,"subsets of whole classes, we find that SAT provides transfer of adversarial robustness to classes which
46"
INTRODUCTION,0.0851063829787234,"have never been attacked during training. E.g. only generating adversarial examples for class car on
47"
INTRODUCTION,0.08687943262411348,"CIFAR-10, achieves a non-trivial robust accuracy of 17.1% on all remaining CIFAR-10 classes (see
48"
INTRODUCTION,0.08865248226950355,"figure 1, right). Secondly, we observe classes and examples that are hard to classify do generally
49"
INTRODUCTION,0.09042553191489362,"provide better robustness transfer than easier ones. I.e. class cat achieves more than twice the robust
50"
INTRODUCTION,0.09219858156028368,"accuracy on the remaining classes (37.8%) over class car (17.1%). Thirdly, SAT with 50% of
51"
INTRODUCTION,0.09397163120567376,"training data is sufficient to recover the baseline performance with vanilla AT even on hard datasets
52"
INTRODUCTION,0.09574468085106383,"like ImageNet. Lastly, we observe similar transfer properties of SATed models to downstream tasks.
53"
INTRODUCTION,0.0975177304964539,"In this setting, exposing the model to only 30% of AEs during training, can recover baseline AT
54"
INTRODUCTION,0.09929078014184398,"performance on the target task.
55"
RELATED WORK,0.10106382978723404,"2
Related Work
56"
RELATED WORK,0.10283687943262411,"Since their discovery [1], robustness against adversarial examples has mainly been tackled using
57"
RELATED WORK,0.10460992907801418,"adversarial training [18, 2, 4]. Among many others, prior work proposed adversarial training variants
58"
RELATED WORK,0.10638297872340426,"working with example-dependent threat models [19, 13‚Äì15], acknowledging that examples can have
59"
RELATED WORK,0.10815602836879433,"different difficulties. Some works also mine hard examples [16] or progressively prune a portion of
60"
RELATED WORK,0.1099290780141844,"the training examples throughout training [17, 20]. However, all of these methods generally assume
61"
RELATED WORK,0.11170212765957446,"access to adversarial examples on the whole training set. That is, while individual examples can
62"
RELATED WORK,0.11347517730496454,"be dropped during training or are treated depending on difficulty, the model can see adversarial
63"
RELATED WORK,0.11524822695035461,"perturbations for these examples if deemed necessary. Adversarial training is also known to transfer
64"
RELATED WORK,0.11702127659574468,"robustness to downstream tasks [8, 9, 7] and adversarially robust representations can be learned
65"
RELATED WORK,0.11879432624113476,"in a self-supervised fashion [21]. Here, a robust backbone is often adapted to the target task by
66"
RELATED WORK,0.12056737588652482,"re-training a shallow classifier ‚Äì sometimes in an adversarial fashion.
It is generally not studied
67"
RELATED WORK,0.12234042553191489,"whether seeing adversarial examples on the whole training set is required for good transfer. This is
68"
RELATED WORK,0.12411347517730496,"despite evidence that achieving adversarial robustness is easier for some classes/concepts than for
69"
RELATED WORK,0.12588652482269502,"others [22, 23, 11, 10], also for robustness transfer [24]. Complementing these works, we consider
70"
RELATED WORK,0.1276595744680851,"only constructing adversarial examples on a pre-defined subset of the training set, not informed by
71"
RELATED WORK,0.12943262411347517,"the model or training procedure, and study how robustness transfers across examples and tasks.
72"
BACKGROUND AND METHOD,0.13120567375886524,"3
Background and Method
73"
BACKGROUND AND METHOD,0.13297872340425532,"3.1
Adversarial Training (AT)
74"
BACKGROUND AND METHOD,0.1347517730496454,"It is a well known fact that conventional deep networks are vulnerable to small, often imperceptible,
75"
BACKGROUND AND METHOD,0.13652482269503546,"changes in the input. As mitigation, AT is a common approach to extend the empirical risk minimiza-
76"
BACKGROUND AND METHOD,0.13829787234042554,"tion framework [2]. Let (x, y) ‚ààDtrain be a training set of example and label pairs and Œ∏ be trainable
77"
BACKGROUND AND METHOD,0.1400709219858156,"parameters, then AT is defined as:
78"
BACKGROUND AND METHOD,0.14184397163120568,"min
Œ∏
E(x,y)‚ààDtrain"
BACKGROUND AND METHOD,0.14361702127659576,"
max
||Œ¥||2‚â§œµ L(x + Œ¥, y; Œ∏)

,
(1)"
BACKGROUND AND METHOD,0.1453900709219858,"where Œ¥ is a perturbation that maximizes the training loss L and thus training error. The idea being
79"
BACKGROUND AND METHOD,0.14716312056737588,"that, simultaneously to minimizing the training loss, the loss is also optimized to be stable within a
80"
BACKGROUND AND METHOD,0.14893617021276595,"small space œµ around each training example ||Œ¥||2 ‚â§œµ (we consider the L2 norm). This additional
81"
BACKGROUND AND METHOD,0.15070921985815602,"inner maximization is solved by an iterative loop; conventionally consisting of 7 or more steps. In
82"
BACKGROUND AND METHOD,0.1524822695035461,"some settings [18, 12, 4], the robust loss is combined with the corresponding loss on clean examples
83"
BACKGROUND AND METHOD,0.15425531914893617,"in a weighted fashion to control the trade-off between adversarial robustness and clean performance.
84"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.15602836879432624,"3.2
AT without Perturbing all Training Examples
85"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.15780141843971632,"Most proposed AT methodologies generate AEs on the whole training set. This being also valid for
86"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.1595744680851064,"methods which adaptively select subsets [16, 17] during training or more traditional AT in which
87"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.16134751773049646,"only a subset per batch is adversarially attacked. These methods do not guarantee the exclusion of
88"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.16312056737588654,"examples, that is, the model is likely to see an AE for every example in the training set. From a broader
89"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.16489361702127658,"perspective, the necessity to generate AEs exhaustively for all classes appears unfortunate though.
90"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.16666666666666666,"Ideally, we desire robust models to be scalable, i.e. transfer flexibly from few examples and across
91"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.16843971631205673,"classes to unseen ones [25]. We propose SAT to investigate to what extent AT provides this utility.
92"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.1702127659574468,"To formalize, let A be a training subset and B contain the complement: A ‚äÇDtrain, B = Dtrain \ A.
93"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.17198581560283688,"Then SAT applies the inner maximization loop of AT on the subset A only; on B the conventional
94"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.17375886524822695,"empirical risk is minimized:
95"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.17553191489361702,"min
Œ∏
E(x,y)‚ààDtrain "
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.1773049645390071,"wA1(x,y)‚ààA max
||Œ¥||2‚â§œµ L(x + Œ¥, y; Œ∏) + wB1(x,y)‚ààBL(x, y; Œ∏)"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.17907801418439717,"
,
(2)"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.18085106382978725,"where 1(x,y)‚ààA is 1 when the training example is in A and 0 otherwise. wA and wB define optional
96"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.18262411347517732,"weights, which are by default both set to 1. Note that this is different from balancing robust and clean
97"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.18439716312056736,"loss as discussed in [18, 12, 4], where the model still encounters adversarial examples on the whole
98"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.18617021276595744,"training set.
99"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.1879432624113475,"Loss balancing. The formulation in equation 2 implies an imbalance between left and right loss as
100"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.18971631205673758,"soon as the training split is not even (|A| Ã∏= |B|). To counteract, we assign different values to wA and
101"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.19148936170212766,"wB based on their subset size. E.g., to equalize the loss between both subsets, we assign wB = 1
102"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.19326241134751773,"and wA = |B|/|A|. We will utilize this loss balancing to improve robustness for transfer learning in
103"
AT WITHOUT PERTURBING ALL TRAINING EXAMPLES,0.1950354609929078,"section 4.3.
104"
TRAINING AND EVALUATION RECIPES,0.19680851063829788,"3.3
Training and evaluation recipes
105"
TRAINING AND EVALUATION RECIPES,0.19858156028368795,"Consider the depiction of SAT in figure 1. Prior to training, the training set is split into A and B
106"
TRAINING AND EVALUATION RECIPES,0.20035460992907803,"(left). For evaluation (middle), we split the validation set into a corresponding split of Aval and Bval,
107"
TRAINING AND EVALUATION RECIPES,0.20212765957446807,"if possible. For Class-subset Adversarial Training (CSAT), this split aligns with the classes on the
108"
TRAINING AND EVALUATION RECIPES,0.20390070921985815,"dataset: A and B are all training examples corresponding to two disjoint sets of classes while Aval
109"
TRAINING AND EVALUATION RECIPES,0.20567375886524822,"and Bval are the corresponding test examples of these classes. As experimenting with all possible
110"
TRAINING AND EVALUATION RECIPES,0.2074468085106383,"splits of classes is infeasible, we motivate splits by class difficulty where we measure difficulty by
111"
TRAINING AND EVALUATION RECIPES,0.20921985815602837,"the average entropy of predictions per class ‚Äì introduced as HC in the next paragraph.
In contrast,
112"
TRAINING AND EVALUATION RECIPES,0.21099290780141844,"we can also split based on individual example difficulty. We provide empirical support for this
113"
TRAINING AND EVALUATION RECIPES,0.2127659574468085,"approach in the experimental section 4. Additionally, example difficulty has been frequently linked to
114"
TRAINING AND EVALUATION RECIPES,0.21453900709219859,"proximity between decision boundary and example [26, 13, 15, 16, 27]. The closer the example is
115"
TRAINING AND EVALUATION RECIPES,0.21631205673758866,"to the boundary, the harder it is likely to classify. The hypothesis: hard examples provide a larger
116"
TRAINING AND EVALUATION RECIPES,0.21808510638297873,"contribution to training robust models, since they optimize for large margins [13, 14]. We refer to
117"
TRAINING AND EVALUATION RECIPES,0.2198581560283688,"this experiment as Example-subset Adversarial Training (ESAT). In contrast to CSAT, however,
118"
TRAINING AND EVALUATION RECIPES,0.22163120567375885,"there is no natural split of the test examples into Aval and Bval such that we evaluate robustness on
119"
TRAINING AND EVALUATION RECIPES,0.22340425531914893,"the whole test set (i.e., Dval).
120"
TRAINING AND EVALUATION RECIPES,0.225177304964539,"As difficulty metric, we utilize entropy over softmax, which we empirically find to be as suitable as
121"
TRAINING AND EVALUATION RECIPES,0.22695035460992907,"alternative metrics (discussed in the supplement). Consider a training set example x ‚ààDtrain and a
122"
TRAINING AND EVALUATION RECIPES,0.22872340425531915,"classifier f mapping from input space to logit space with N logits. Then the entropy of example x is
123"
TRAINING AND EVALUATION RECIPES,0.23049645390070922,"determined by H(f(x)) and of a whole class C ‚äÇDtrain is determined by HC(f) ‚Äì the average over
124"
TRAINING AND EVALUATION RECIPES,0.2322695035460993,"all examples in C:
125"
TRAINING AND EVALUATION RECIPES,0.23404255319148937,"H(f(x)) = ‚àí N
X"
TRAINING AND EVALUATION RECIPES,0.23581560283687944,"i=1
œÉi(f(x)) ¬∑ log œÉi(f(x)),
HC(f) =
1
|C| X"
TRAINING AND EVALUATION RECIPES,0.2375886524822695,"x‚ààC
H(f(x)),"
TRAINING AND EVALUATION RECIPES,0.2393617021276596,"where œÉ denotes the softmax function. For our SAT setting, we rank examples prior to adversarial
126"
TRAINING AND EVALUATION RECIPES,0.24113475177304963,"training. This requires a classifier pretrained on Dtrain enabling the calculation of the entropy. To
127"
TRAINING AND EVALUATION RECIPES,0.2429078014184397,"strictly separate the effects between entropy and AT, we determine the entropy using a non-robust
128"
TRAINING AND EVALUATION RECIPES,0.24468085106382978,"classifier trained without AT. Similar to [27], we aggregate the classifier states at multiple epochs
129"
TRAINING AND EVALUATION RECIPES,0.24645390070921985,"during training and average the entropies. Let f1, f2, ...fM be snapshots of the classifier from multiple
130"
TRAINING AND EVALUATION RECIPES,0.24822695035460993,"epochs during training, where M denotes the number of training epochs. Then the average entropy
131"
TRAINING AND EVALUATION RECIPES,0.25,"for an example is given by H(x) and for a class by HC(f):
132"
TRAINING AND EVALUATION RECIPES,0.25177304964539005,"H(x) = 1 M M
X"
TRAINING AND EVALUATION RECIPES,0.25354609929078015,"e=1
H(fe(x)),
HC = 1 M M
X"
TRAINING AND EVALUATION RECIPES,0.2553191489361702,"e=1
HC(fe).
(3)"
EXPERIMENTS,0.2570921985815603,"4
Experiments
133"
EXPERIMENTS,0.25886524822695034,"As aforementioned, common practice performs AT for the whole training set. In the following, we
134"
EXPERIMENTS,0.26063829787234044,"explore CSAT and ESAT, which splits the training set in two subsets A and B and only constructs AEs
135"
EXPERIMENTS,0.2624113475177305,"for A such that the model never sees AEs for B. We start with single-class CSAT ‚Äì A contains only
136"
EXPERIMENTS,0.2641843971631206,"examples of a single class ‚Äì and increase the size of A (section 4.1) by utilizing the entropy ranking
137"
EXPERIMENTS,0.26595744680851063,"of classes HC (equation 3). ESAT, which splits into example subsets is discussed in section 4.2.
138"
EXPERIMENTS,0.26773049645390073,"Both SAT variants reveal complex interactions between classes and examples while indicating that
139"
EXPERIMENTS,0.2695035460992908,"few AEs can provide high transfer performance to downstream tasks when weighted appropriately
140"
EXPERIMENTS,0.2712765957446808,"(section 4.3).
141"
EXPERIMENTS,0.2730496453900709,"Training and evaluation details. Since AT is prone to overfitting [28], it is common practice to stop
142"
EXPERIMENTS,0.274822695035461,"training when robust accuracy on a hold-out set is at its peak. This typically happens after a learning
143"
EXPERIMENTS,0.2765957446808511,"rate decay. We adopt this ‚Äúearly stopping‚Äù for all our experiments by following the methodology
144"
EXPERIMENTS,0.2783687943262411,"in [28] but utilize Auto Attack (AA) to evaluate robust accuracy. Throughout the course of the training,
145"
EXPERIMENTS,0.2801418439716312,"we evaluate AA on 10% of the validation data Dval after each learning rate decay and perform final
146"
EXPERIMENTS,0.28191489361702127,"evaluation with the model providing the highest robust accuracy. This final evaluation is performed
147"
EXPERIMENTS,0.28368794326241137,"on the remaining 90% of validation data. This AA split is fixed throughout experiments to provide
148"
EXPERIMENTS,0.2854609929078014,"consistency. If not specified otherwise, we generate adversarial examples during training with PGD-7
149"
EXPERIMENTS,0.2872340425531915,"within an L2 epsilon ball of œµ = 0.5 (all CIFAR variants) or œµ = 3.0 (all ImageNet variants) ‚Äì typical
150"
EXPERIMENTS,0.28900709219858156,"configurations found in related work. We train all models from scratch and use ResNet-18 [29] for all
151"
EXPERIMENTS,0.2907801418439716,"CIFAR-10 and CIFAR-100 [30] experiments and ResNet-50 for all ImageNet-200 experiments. Here,
152"
EXPERIMENTS,0.2925531914893617,"ImageNet-200 corresponds to the ImageNet-A subset [31] to render random baseline experiments
153"
EXPERIMENTS,0.29432624113475175,"tractable (to reduce training time). This ImageNet-200 dataset, contains 200 classes that retain the
154"
EXPERIMENTS,0.29609929078014185,"class variety and breadth of regular ImageNet, but remove classes that are similar to each other
155"
EXPERIMENTS,0.2978723404255319,"(e.g. fine-grained dog types). We use all training and validation examples from ImageNet [32] that
156"
EXPERIMENTS,0.299645390070922,"correspond to this subset classes. All training details can be found in the supplement.
157"
CLASS SUBSET SPLITS,0.30141843971631205,"4.1
Class subset splits
158"
CLASS SUBSET SPLITS,0.30319148936170215,"We start by investigating the interactions between individual classes in A using CSAT on CIFAR-10,
159"
CLASS SUBSET SPLITS,0.3049645390070922,"followed by an investigation on increasing the number of classes. Single-class subsets (CSAT). We
160"
CLASS SUBSET SPLITS,0.3067375886524823,"train all possible, single class CSAT runs (10) and evaluate robust accuracies on the adv. trained
161"
CLASS SUBSET SPLITS,0.30851063829787234,"class (A) and the non-adv. trained classes (B). The results are shown in figure 2, left. Each rows
162"
CLASS SUBSET SPLITS,0.3102836879432624,"20
40
60
80
100"
CLASS SUBSET SPLITS,0.3120567375886525,Robust accuracy (L2 = 0.5) plane car bird cat deer dog frog horse ship truck
CLASS SUBSET SPLITS,0.31382978723404253,ATed class
CLASS SUBSET SPLITS,0.31560283687943264,+25.7%
CLASS SUBSET SPLITS,0.3173758865248227,+17.1%
CLASS SUBSET SPLITS,0.3191489361702128,+34.3%
CLASS SUBSET SPLITS,0.32092198581560283,+37.8%
CLASS SUBSET SPLITS,0.32269503546099293,+34.2%
CLASS SUBSET SPLITS,0.324468085106383,+34.9%
CLASS SUBSET SPLITS,0.3262411347517731,+28.4%
CLASS SUBSET SPLITS,0.3280141843971631,+27.3%
CLASS SUBSET SPLITS,0.32978723404255317,+24.5%
CLASS SUBSET SPLITS,0.33156028368794327,+21.9% = 0%
CLASS SUBSET SPLITS,0.3333333333333333,"no AT
full AT
ATed class
Non-ATed classes
A
B plane car bird cat deer dog frog horse ship truck"
CLASS SUBSET SPLITS,0.3351063829787234,Robust accuracy plane car bird cat deer dog frog horse ship truck
CLASS SUBSET SPLITS,0.33687943262411346,ATed class
CLASS SUBSET SPLITS,0.33865248226950356,"89
57
15
5
10
16
16
38
28
46"
CLASS SUBSET SPLITS,0.3404255319148936,"28
94
9
3
6
9
8
32
24
35"
CLASS SUBSET SPLITS,0.3421985815602837,"33
57
80
12
12
24
28
45
41
56"
CLASS SUBSET SPLITS,0.34397163120567376,"41
61
28
50
16
25
28
50
37
53"
CLASS SUBSET SPLITS,0.34574468085106386,"36
62
24
8
84
24
26
41
35
52"
CLASS SUBSET SPLITS,0.3475177304964539,"34
55
28
11
16
61
30
49
36
54"
CLASS SUBSET SPLITS,0.34929078014184395,"27
58
17
10
11
18
93
42
27
44"
CLASS SUBSET SPLITS,0.35106382978723405,"31
55
17
6
10
22
24
89
33
47"
CLASS SUBSET SPLITS,0.3528368794326241,"34
56
12
5
9
10
12
35
92
47"
CLASS SUBSET SPLITS,0.3546099290780142,"33
35
15
3
9
18
14
37
32
92"
CLASS SUBSET SPLITS,0.35638297872340424,"Full AT
77
85
53
41
60
53
77
79
81
81"
CLASS SUBSET SPLITS,0.35815602836879434,"Figure 2: CSAT on a single CIFAR-10 class A (blue), we observe non-trivial transfer to the non-adv.
trained classes B (green). Classes considered hard in CIFAR-10 (cat) offer best generalization
(+37.8% gain on non-adv. trained), while easy classes offer the worst (car, +17.1% gained). Note
that without AT, robust accuracy is close to 0% for all classes (orange). Right: same as left, but robust
accuracy is evaluated per class (along columns). Here, we observe an unexpected transfer property:
hard classes provide better transfer to seemingly unrelated classes (cat ‚Üítruck: 53%) than related
classes (car ‚Üítruck: 35%)."
CLASS SUBSET SPLITS,0.3599290780141844,"represents a different training run. Note that the baseline robust accuracy, trained without AT achieves
163"
CLASS SUBSET SPLITS,0.3617021276595745,"practically 0% (indicated by red line). Most importantly, we observe non-trivial robustness gains for
164"
CLASS SUBSET SPLITS,0.36347517730496454,"all classes that have never been attack during training (B-sets). That is, irrespective of the chosen
165"
CLASS SUBSET SPLITS,0.36524822695035464,"class, we gain at least 17.1% robust accuracy (A=car) on the remaining classes and can gain up to
166"
CLASS SUBSET SPLITS,0.3670212765957447,"37.8% robust accuracy when A=cat. These robustness gains are unexpectedly good, given many
167"
CLASS SUBSET SPLITS,0.36879432624113473,"features of the non-adv. trained classes can be assumed to not be trained robustly.
168 plane car bird cat deer dog frog horse ship truck 0.0 0.04 0.08 0.12"
CLASS SUBSET SPLITS,0.37056737588652483,Class error 0.00 0.13 0.27 0.40
CLASS SUBSET SPLITS,0.3723404255319149,Entropy HC
CLASS SUBSET SPLITS,0.374113475177305,"Figure
3:
The
hardest
classes (blue) have the high-
est entropy (green)."
CLASS SUBSET SPLITS,0.375886524822695,"To investigate this phenomenon further, we analyze robust gains for
169"
CLASS SUBSET SPLITS,0.3776595744680851,"each individual class and present robust accuracies in the matrix in
170"
CLASS SUBSET SPLITS,0.37943262411347517,"figure 2, right, where training runs are listed in rows and robust accura-
171"
CLASS SUBSET SPLITS,0.38120567375886527,"cies per class are listed in columns. Blue cells denote the adv. trained
172"
CLASS SUBSET SPLITS,0.3829787234042553,"class and green cells denote non-adv. trained classes. While we see
173"
CLASS SUBSET SPLITS,0.38475177304964536,"some expected transfer properties, e.g. CSAT on car provides greater
174"
CLASS SUBSET SPLITS,0.38652482269503546,"robust accuracy on the related class truck (46%) than unrelated animal
175"
CLASS SUBSET SPLITS,0.3882978723404255,"classes bird, cat, deer, dog (between 5% and 16%), the reverse is not
176"
CLASS SUBSET SPLITS,0.3900709219858156,"straight-forward. CSAT on bird provides 56% robust accuracy on the
177"
CLASS SUBSET SPLITS,0.39184397163120566,"seemingly unrelated class truck, 10%-points more than CSAT on car.
178"
CLASS SUBSET SPLITS,0.39361702127659576,"More generally, animal classes provide stronger robustness throughout
179"
CLASS SUBSET SPLITS,0.3953900709219858,"all classes than inanimate classes. We observe, that these classes are
180"
CLASS SUBSET SPLITS,0.3971631205673759,"also harder to classify and have a higher entropy HC as shown in figure 3.
181"
CLASS SUBSET SPLITS,0.39893617021276595,"Many-class subsets (CSAT). To increase the number of classes in A while maintaining a minimal
182"
CLASS SUBSET SPLITS,0.40070921985815605,"computational complexity, we utilize the average class entropy HC proposed in equation 3 to inform
183"
CLASS SUBSET SPLITS,0.4024822695035461,"us which ranking to select from. To improve clarity, we begin with a reduced set of experiments
184"
CLASS SUBSET SPLITS,0.40425531914893614,"on CIFAR-10 before transitioning to larger datasets. We utilize the observed correlation between
185"
CLASS SUBSET SPLITS,0.40602836879432624,"class difficulty, average class entropy and robustness transfer HC to rank classes and construct 4
186"
CLASS SUBSET SPLITS,0.4078014184397163,"adv. trained subsets. Ranked by class entropy HC, we select 4 subsets showing in figure 4, left. As
187"
CLASS SUBSET SPLITS,0.4095744680851064,"observed before, cat and dog are hardest and thus first chosen to be in subset A. Truck and car on
188"
CLASS SUBSET SPLITS,0.41134751773049644,"the other hand are easiest and thus last. To gauge the utility of this ranking, we provide a robust and
189"
CLASS SUBSET SPLITS,0.41312056737588654,"clean accuracy comparison with a random baseline in figure 4, center and right. I.e., for each subset
190"
CLASS SUBSET SPLITS,0.4148936170212766,"A we select 10 random subsets and report mean and std. deviation (red line and shaded area). Similar
191"
CLASS SUBSET SPLITS,0.4166666666666667,"to the single-class setup, we observe subsets of the hardest classes to consistently outperform the
192"
CLASS SUBSET SPLITS,0.41843971631205673,"random baseline (upper middle plot), up until a subset size of |A| = 8, when it draws even. Also
193"
CLASS SUBSET SPLITS,0.42021276595744683,"note that the robust accuracy on Bval is improved across all splits, thus providing support that harder
194"
CLASS SUBSET SPLITS,0.4219858156028369,"classes ‚Äì as initially observed on animate vs inanimate classes ‚Äì offer greater robustness transfer.
195"
CLASS SUBSET SPLITS,0.4237588652482269,Data splits
CLASS SUBSET SPLITS,0.425531914893617,"cat
dog"
CLASS SUBSET SPLITS,0.42730496453900707,"cat
dog
bird
deer"
CLASS SUBSET SPLITS,0.42907801418439717,"cat
dog
bird
deer
plane frog"
CLASS SUBSET SPLITS,0.4308510638297872,"cat
dog
bird
deer
plane"
CLASS SUBSET SPLITS,0.4326241134751773,"frog
horse ship truck car horse"
CLASS SUBSET SPLITS,0.43439716312056736,"ship
truck car plane"
CLASS SUBSET SPLITS,0.43617021276595747,"frog
horse"
CLASS SUBSET SPLITS,0.4379432624113475,"ship
truck car"
CLASS SUBSET SPLITS,0.4397163120567376,"bird
deer
plane"
CLASS SUBSET SPLITS,0.44148936170212766,"frog
horse"
CLASS SUBSET SPLITS,0.4432624113475177,"ship
truck car A B"
CLASS SUBSET SPLITS,0.4450354609929078,"2
4
6
8
|A|"
CLASS SUBSET SPLITS,0.44680851063829785,Performance on
CLASS SUBSET SPLITS,0.44858156028368795,CIFAR-10val 40 50 60 70
CLASS SUBSET SPLITS,0.450354609929078,Rob. accuracy
CLASS SUBSET SPLITS,0.4521276595744681,"2
4
6
8
10
Number of adv. trained classes (|A|) 85.0 87.5 90.0 92.5 95.0"
CLASS SUBSET SPLITS,0.45390070921985815,Clean accuracy
CLASS SUBSET SPLITS,0.45567375886524825,"full AT
split as left
random class split"
CLASS SUBSET SPLITS,0.4574468085106383,"Performance on
Non-adv. trained classes (Bval) 0 25 50 75"
CLASS SUBSET SPLITS,0.4592198581560284,Rob. accuracy
CLASS SUBSET SPLITS,0.46099290780141844,"2
4
6
8
10
Number of adv. trained classes (|A|) 80 85 90 95 100"
CLASS SUBSET SPLITS,0.4627659574468085,Clean accuracy
CLASS SUBSET SPLITS,0.4645390070921986,"no AT
split as left
random class split"
CLASS SUBSET SPLITS,0.46631205673758863,"Figure 4: Ranking CIFAR10 classes by difficulty (using entropy as proxy), we perform CSAT with an
increasing size of adv. trained classes in A. Class splits used for training (A and B) are stated on the
left. The resulting robust and clean accuracies on the validation set is shown on the right, separated
into performance on Bval and all. Compared with a random baseline of random class ranking (red),
we find the ranking by difficulty to have consistently better transfer to non-adv. trained classes (B).
Overall, this results in an improved robust accuracy on average over all classes."
CLASS SUBSET SPLITS,0.46808510638297873,"For our experiments on larger datasets like CIFAR-100 and ImageNet-200, we additionally evaluate
196"
CLASS SUBSET SPLITS,0.4698581560283688,"a third ranking strategy. Beside selecting at random and selecting the hardest first, we additionally
197"
CLASS SUBSET SPLITS,0.4716312056737589,"compare with selecting the easiest (inverting the entropy ranking). We construct 9 subsets per type of
198"
CLASS SUBSET SPLITS,0.4734042553191489,"ranking (instead of 4) and report robust accuracies for selecting the easiest classes as well. Results are
199"
CLASS SUBSET SPLITS,0.475177304964539,"presented in three columns in figure 5; one dataset per column. As before, we show robust accuracies
200"
CLASS SUBSET SPLITS,0.4769503546099291,"on the tested dataset (upper row) and robust accuracies on Bval (lower row). For CIFAR-10, we
201"
CLASS SUBSET SPLITS,0.4787234042553192,"calculate mean and std. dev. over 10 runs, for CIFAR-100 over 5 runs and for ImageNet-200 over
202"
CLASS SUBSET SPLITS,0.4804964539007092,"3 runs. Selecting hardest first (highest entropy) is marked as a solid line and easiest first (lowest
203"
CLASS SUBSET SPLITS,0.48226950354609927,"entropy) as a dashed line. First and foremost, we observe that irrespective of the dataset and the
204"
CLASS SUBSET SPLITS,0.48404255319148937,"size of A, we see robustness transfer to Bval. This transfer remains greatest with classes we consider
205"
CLASS SUBSET SPLITS,0.4858156028368794,"hard, while easy classes provide the least. Nonetheless, we see diminishing returns of such an
206"
CLASS SUBSET SPLITS,0.4875886524822695,"informed ranking when dataset complexity is increased. E.g. the gap between dashed and solid line
207"
CLASS SUBSET SPLITS,0.48936170212765956,"on ImageNet-200 is small and random class selection is on-par with the best. The results are similar
208"
CLASS SUBSET SPLITS,0.49113475177304966,"on CIFAR-100, as shown in figure 5, middle). Based on these results, entropy ranking and selecting
209"
CLASS SUBSET SPLITS,0.4929078014184397,"classes provides only slight improvements in general. Importantly though, we continue to see the
210"
CLASS SUBSET SPLITS,0.4946808510638298,"tendency of increased robustness transfer to Bval, which we will come back to in section 4.3.
211"
CLASS SUBSET SPLITS,0.49645390070921985,"On Bval
On whole dataset"
CLASS SUBSET SPLITS,0.49822695035460995,CIFAR-10 40 60
CLASS SUBSET SPLITS,0.5,Rob. accuracy
CLASS SUBSET SPLITS,0.50177304964539,"1
2
3
4
5
6
7
8
9
10
Number of adv. trained classes (|A|) 0 25 50 75"
CLASS SUBSET SPLITS,0.5035460992907801,Rob. accuracy
CLASS SUBSET SPLITS,0.5053191489361702,"CSAT-hardest Ô¨Årst
CSAT-easiest Ô¨Årst"
CLASS SUBSET SPLITS,0.5070921985815603,"CSAT-random
one std. dev."
CLASS SUBSET SPLITS,0.5088652482269503,"full AT
no AT"
CLASS SUBSET SPLITS,0.5106382978723404,CIFAR-100 20 30 40
CLASS SUBSET SPLITS,0.5124113475177305,Rob. accuracy
CLASS SUBSET SPLITS,0.5141843971631206,"10
20
30
40
50
60
70
80
90
100
Number of adv. trained classes (|A|) 0 20 40 60"
CLASS SUBSET SPLITS,0.5159574468085106,Rob. accuracy
CLASS SUBSET SPLITS,0.5177304964539007,"CSAT-hardest Ô¨Årst
CSAT-easiest Ô¨Årst"
CLASS SUBSET SPLITS,0.5195035460992907,"CSAT-random
one std. dev."
CLASS SUBSET SPLITS,0.5212765957446809,"full AT
no AT"
CLASS SUBSET SPLITS,0.5230496453900709,ImageNet-200 20 40
CLASS SUBSET SPLITS,0.524822695035461,Rob. accuracy
CLASS SUBSET SPLITS,0.526595744680851,"20
40
60
80
100 120 140 160 180 200
Number of adv. trained classes (|A|) 0 20 40 60"
CLASS SUBSET SPLITS,0.5283687943262412,Rob. accuracy
CLASS SUBSET SPLITS,0.5301418439716312,"CSAT-hardest Ô¨Årst
CSAT-easiest Ô¨Årst"
CLASS SUBSET SPLITS,0.5319148936170213,"CSAT-random
one std. dev."
CLASS SUBSET SPLITS,0.5336879432624113,"full AT
no AT"
CLASS SUBSET SPLITS,0.5354609929078015,"Figure 5: Class-subset Adversarial Training (CSAT) produces non-trivial robustness on classes that
have never been attacked during training (Bval). Along the x-axes we increase the class subset size of
A on which AEs are constructed and compare three different class-selection strategies: select hardest
first (solid lines), select easiest first (dashed line) and select at random (red). On average, random
selection performs as well as informed ranking (upper row), while the robustness transfer to Bval is
best for the hardest classes (lower row). AT on a single class provides already much greater robust
accuracies than without AT (orange)."
CLASS SUBSET SPLITS,0.5372340425531915,"4.2
Example subset splits (ESAT)
212"
CLASS SUBSET SPLITS,0.5390070921985816,"Considering that splits along classes are inefficient in terms of reaching the full potential of adversarial
213"
CLASS SUBSET SPLITS,0.5407801418439716,"robustness, we investigate ranking examples across the whole dataset (ESAT). We follow with the
214"
CLASS SUBSET SPLITS,0.5425531914893617,"same setup as before but rank examples ‚Äì and not classes ‚Äì by entropy H. Since it is not feasible to
215"
CLASS SUBSET SPLITS,0.5443262411347518,"construct corresponding rankings on the validation set, we cannot gauge robustness transfer to Bval.
216"
CLASS SUBSET SPLITS,0.5460992907801419,"Instead, we will test transfer performance to downstream tasks in section 4.3. We consequently report
217"
CLASS SUBSET SPLITS,0.5478723404255319,"robust accuracy and clean accuracy on the whole validation set in figure 6.
218"
CLASS SUBSET SPLITS,0.549645390070922,"Firstly, note that the increase in robust accuracy is more rapid than with CSAT w.r.t. the size of
219"
CLASS SUBSET SPLITS,0.5514184397163121,"A. AT only on 50% of training data (25k examples on CIFAR and 112k on ImageNet-200) and the
220"
CLASS SUBSET SPLITS,0.5531914893617021,"resulting average robust accuracy is very close to the baseline AT performance (gray line). Secondly,
221"
CLASS SUBSET SPLITS,0.5549645390070922,"note that gap between hard (solid line) and easy example selection (dashed line) has substantially
222"
CLASS SUBSET SPLITS,0.5567375886524822,"widened. In practice, it is therefore possible to accidentally select poor performing subsets, although
223"
CLASS SUBSET SPLITS,0.5585106382978723,"the chance appears to be low given the narrow variance of random rankings (red). To some extent,
224"
CLASS SUBSET SPLITS,0.5602836879432624,"this observation supports the hypothesis that examples far from the decision border (the easiest to
225"
CLASS SUBSET SPLITS,0.5620567375886525,"classify) provide the least contribution to robustness gains. This is also supported by the reverse
226"
CLASS SUBSET SPLITS,0.5638297872340425,"gap in clean accuracy (bottom row in figure 6). That is, easiest-first-selection results in higher clean
227"
CLASS SUBSET SPLITS,0.5656028368794326,"accuracies than hardest-first, while robust accuracies are much lower. In contrast however, we observe
228"
CLASS SUBSET SPLITS,0.5673758865248227,"random rankings (red) to achieve similar performances to hard rankings (solid lines) on all datasets
229"
CLASS SUBSET SPLITS,0.5691489361702128,"and subset sizes. This is somewhat unexpected, especially on small sizes of A (e.g. 5k). Given
230"
CLASS SUBSET SPLITS,0.5709219858156028,"the results, we conjecture that the proximity to the decision boundary plays a subordinate role to
231"
CLASS SUBSET SPLITS,0.5726950354609929,"increasing robustness. Instead, it is plausible to assume that diversity in the training data has a large
232"
CLASS SUBSET SPLITS,0.574468085106383,"impact on learning robust features, also indicated by [33].
233"
CLASS SUBSET SPLITS,0.5762411347517731,On whole dataset
CLASS SUBSET SPLITS,0.5780141843971631,CIFAR-10 40 60
CLASS SUBSET SPLITS,0.5797872340425532,Rob. accuracy
K,0.5815602836879432,"5k
10k 15k 20k 25k 30k 35k 40k 45k 50k
Number of adv. trained examples (|A|) 90 92 94"
K,0.5833333333333334,Clean accuracy
K,0.5851063829787234,"ESAT-hardest Ô¨Årst
ESAT-easiest Ô¨Årst"
K,0.5868794326241135,"ESAT-random
one std. dev."
K,0.5886524822695035,full AT
K,0.5904255319148937,CIFAR-100 10 20 30 40
K,0.5921985815602837,Rob. accuracy
K,0.5939716312056738,"5k
10k 15k 20k 25k 30k 35k 40k 45k 50k
Number of adv. trained examples (|A|) 70 75"
K,0.5957446808510638,Clean accuracy
K,0.5975177304964538,"ESAT-hardest Ô¨Årst
ESAT-easiest Ô¨Årst"
K,0.599290780141844,"ESAT-random
one std. dev."
K,0.601063829787234,full AT
K,0.6028368794326241,ImageNet-200 20 40
K,0.6046099290780141,Rob. accuracy
K,0.6063829787234043,22k 44k 67k 89k 112k 134k 156k 179k 201k 224k
K,0.6081560283687943,Number of adv. trained examples (|A|) 75.0 77.5 80.0 82.5
K,0.6099290780141844,Clean accuracy
K,0.6117021276595744,"ESAT-hardest Ô¨Årst
ESAT-easiest Ô¨Årst"
K,0.6134751773049646,"ESAT-random
one std. dev."
K,0.6152482269503546,full AT
K,0.6170212765957447,"Figure 6: Example-subset Adversarial Training (ESAT) on CIFAR datasets and ImageNet-200,
provide quick convergence to a full AT baseline (gray line and dot) with increasing size of A. We
report robust accuracy (upper row) and clean accuracy (lower row) and observe similar characteristics
as with CSAT (figure 5). I.e., selecting the hardest examples first (solid line) provide higher rob.
accuracy than easy ones (dashed line), although the gap substantially widens. Random example
selection (red) provides competitive performance on average. Across all datasets, we see the common
clean accuracy decrease while robust accuracy increases [34]."
TRANSFER TO DOWNSTREAM TASKS,0.6187943262411347,"4.3
Transfer to downstream tasks
234"
TRANSFER TO DOWNSTREAM TASKS,0.6205673758865248,"Previous experiments on ESAT could not provide explicit robust accuracies on the non-adv. trained
235"
TRANSFER TO DOWNSTREAM TASKS,0.6223404255319149,"subset Bval since training and testing splits do not align naturally ‚Äì recall the evaluation recipe outlined
236"
TRANSFER TO DOWNSTREAM TASKS,0.624113475177305,"in section 3.3. In order to test transfer performance regardless, we make use of the fixed-feature
237"
TRANSFER TO DOWNSTREAM TASKS,0.625886524822695,"task transfer setting proposed in [7]. The recipe just slightly changes: split the data into A and B as
238"
TRANSFER TO DOWNSTREAM TASKS,0.6276595744680851,"usual and perform SAT. Fix all features, replace the last classification layer with a 1-hidden layered
239"
TRANSFER TO DOWNSTREAM TASKS,0.6294326241134752,"classifier and finetune only the new classifier on the target task. Importantly, neither training nor
240"
TRANSFER TO DOWNSTREAM TASKS,0.6312056737588653,"validation set for the target task are split. We consider CIFAR-100 and ImageNet-200 and transfer
241"
TRANSFER TO DOWNSTREAM TASKS,0.6329787234042553,"to CIFAR-10, SVHN, Caltech-256 [35] and Flowers102 [36]. We call SAT trained for transfer
242"
TRANSFER TO DOWNSTREAM TASKS,0.6347517730496454,"Source-task Subset Adversarial Training (S-SAT), to emphasize that the subset training is performed
243"
TRANSFER TO DOWNSTREAM TASKS,0.6365248226950354,"on the source-task dataset.
244"
TRANSFER TO DOWNSTREAM TASKS,0.6382978723404256,"In this section, we consider models that have ‚Äúseen‚Äù only a fraction of AEs on the source task and
245"
TRANSFER TO DOWNSTREAM TASKS,0.6400709219858156,"investigate the robustness transfer capabilities to tasks on which they have not explicitly adversarially
246"
TRANSFER TO DOWNSTREAM TASKS,0.6418439716312057,"On adv. trained
CIFAR-100 classes (Aval)"
TRANSFER TO DOWNSTREAM TASKS,0.6436170212765957,"On non-adv. trained
CIFAR-100 classes (Bval)"
TRANSFER TO DOWNSTREAM TASKS,0.6453900709219859,"On non-adv. trained
downstream task CIFAR-10val 10 20 30 40"
TRANSFER TO DOWNSTREAM TASKS,0.6471631205673759,Rob. accuracy
TRANSFER TO DOWNSTREAM TASKS,0.648936170212766,"10
20
30
40
50
60
70
80
90
100
Number of adv. trained classes (|A|) 50 55 60 65"
TRANSFER TO DOWNSTREAM TASKS,0.650709219858156,Clean accuracy
TRANSFER TO DOWNSTREAM TASKS,0.6524822695035462,"vanilla
wA = |B|/|A|"
TRANSFER TO DOWNSTREAM TASKS,0.6542553191489362,wA = 2 ‚Üí10|B|/|A|
TRANSFER TO DOWNSTREAM TASKS,0.6560283687943262,full AT 20 40 60
TRANSFER TO DOWNSTREAM TASKS,0.6578014184397163,Rob. accuracy
TRANSFER TO DOWNSTREAM TASKS,0.6595744680851063,"10
20
30
40
50
60
70
80
90
100
Number of adv. trained classes (|A|) 60 80"
TRANSFER TO DOWNSTREAM TASKS,0.6613475177304965,Clean accuracy
TRANSFER TO DOWNSTREAM TASKS,0.6631205673758865,"vanilla
wA = |B|/|A|"
TRANSFER TO DOWNSTREAM TASKS,0.6648936170212766,wA = 2 ‚Üí10|B|/|A| 20 30 40
TRANSFER TO DOWNSTREAM TASKS,0.6666666666666666,Rob. accuracy
TRANSFER TO DOWNSTREAM TASKS,0.6684397163120568,CIFAR10
TRANSFER TO DOWNSTREAM TASKS,0.6702127659574468,"10
20
30
40
50
60
70
80
90
10
20
30
40
50
60
70
80
90
10
20
30
40
50
60
70
80
90
# of adv. trained classes (|A|) on CIFAR100 75 80"
TRANSFER TO DOWNSTREAM TASKS,0.6719858156028369,Clean accuracy
TRANSFER TO DOWNSTREAM TASKS,0.6737588652482269,CIFAR10
TRANSFER TO DOWNSTREAM TASKS,0.675531914893617,"vanilla
wA = |B|/|A|"
TRANSFER TO DOWNSTREAM TASKS,0.6773049645390071,wA = 2 ‚Üí10|B|/|A|
TRANSFER TO DOWNSTREAM TASKS,0.6790780141843972,full AT on CIFAR100
TRANSFER TO DOWNSTREAM TASKS,0.6808510638297872,"Figure 7: Impact of cross-entropy weighting on robustness transfer. For subset AT, we test different
weighting strategies for sets A and B given they are of unequal size. We observe that vanilla cross-
entropy (circle) offers the worst robustness transfer to CIFAR-10 (right). The best transfer (plus)
is provided when loss weights are chosen such that training is overemphasized on A, indicated by
dropping robust accuracies on B (compare left and center)."
TRANSFER TO DOWNSTREAM TASKS,0.6826241134751773,"trained on. We find unexpectedly strong transfer performances for models that have both low clean
247"
TRANSFER TO DOWNSTREAM TASKS,0.6843971631205674,"and robust accuracy, only by putting more weight on the AEs.
248"
TRANSFER TO DOWNSTREAM TASKS,0.6861702127659575,"Loss balancing improves robustness transfer. In contrast to the previously explored setting, we
249"
TRANSFER TO DOWNSTREAM TASKS,0.6879432624113475,"observe the transfer setting to benefit from loss balancing. Recall equation 2 in section 3.2 in which
250"
TRANSFER TO DOWNSTREAM TASKS,0.6897163120567376,"wA and wB can be assigned different values to balance the loss when |A| Ã∏= |B|. We show that the
251"
TRANSFER TO DOWNSTREAM TASKS,0.6914893617021277,"vanilla configuration wA = wB = 1 transfers robustness to downstream tasks poorly, that balancing
252"
TRANSFER TO DOWNSTREAM TASKS,0.6932624113475178,"the loss with wB = 1, wA = |B|/|A| lacks transfer performance for small |B| and that weighting
253"
TRANSFER TO DOWNSTREAM TASKS,0.6950354609929078,"examples from A higher results in improved robustness transfer. We present results for all three
254"
TRANSFER TO DOWNSTREAM TASKS,0.6968085106382979,"weightings in figure 7. The figure is organized in three columns, all reporting robust accuracy. The
255"
TRANSFER TO DOWNSTREAM TASKS,0.6985815602836879,"first column reports the robust accuracy on subset Aval, the second on subset Bval and the third reports
256"
TRANSFER TO DOWNSTREAM TASKS,0.700354609929078,"the robust accuracy on the downstream task. Here, we train on CIFAR-100 and transfer to CIFAR-10.
257"
TRANSFER TO DOWNSTREAM TASKS,0.7021276595744681,"The vanilla loss is indicated by circles and a solid line, the balanced loss wA = |B|/|A| by squares
258"
TRANSFER TO DOWNSTREAM TASKS,0.7039007092198581,"and a dotted line and the loss overemphasizing A by a plus and a dashed line.
259"
TRANSFER TO DOWNSTREAM TASKS,0.7056737588652482,"First and foremost, note that the robustness transfer for the vanilla configuration is substantially worse
260"
TRANSFER TO DOWNSTREAM TASKS,0.7074468085106383,"than both alternatives (robust accuracy in top right). Transfer improves with use of loss balancing, e.g.
261"
TRANSFER TO DOWNSTREAM TASKS,0.7092198581560284,"for |A| = 10, robust accuracy improves from 8% to 30%, but does not converge to the baseline AT
262"
TRANSFER TO DOWNSTREAM TASKS,0.7109929078014184,"performance (gray line). This is an unwanted side effect of equalizing the weight between A and B.
263"
TRANSFER TO DOWNSTREAM TASKS,0.7127659574468085,"When A is much smaller than B, less weight is assigned to the AEs constructed for A and robustness
264"
TRANSFER TO DOWNSTREAM TASKS,0.7145390070921985,"reduces. Note, this effect can also be seen on Aval (top left in figure). Instead, we find it beneficial
265"
TRANSFER TO DOWNSTREAM TASKS,0.7163120567375887,"to overemphasize on the AEs (plus with dashed line). This configuration assigns wA = 2|B|/|A| for
266"
TRANSFER TO DOWNSTREAM TASKS,0.7180851063829787,"|A| = 10 and increases the weight to wA = 10|B|/|A| for |A| = 90. This results in improved robust
267"
TRANSFER TO DOWNSTREAM TASKS,0.7198581560283688,"accuray on Aval, but low robust and clean accuracy on Bval. Interestingly, while the generalization to
268"
TRANSFER TO DOWNSTREAM TASKS,0.7216312056737588,"Bval is low, robustness transfer to CIFAR-10 is very high. We use this loss weighting for all following
269"
TRANSFER TO DOWNSTREAM TASKS,0.723404255319149,"task transfer experiments.
270"
TRANSFER TO DOWNSTREAM TASKS,0.725177304964539,"Robustness transfer from example subsets. Using the weighted loss, we focus in the following
271"
TRANSFER TO DOWNSTREAM TASKS,0.7269503546099291,"on S-ESAT on two source tasks: CIFAR-100 and ImageNet-200, and train on three downstream
272"
TRANSFER TO DOWNSTREAM TASKS,0.7287234042553191,"tasks. Similar results for S-CSAT and SVHN as additional downstream task can be found in the
273"
TRANSFER TO DOWNSTREAM TASKS,0.7304964539007093,"supplement. Figure 8 presents results for three settings: CIFAR-100 ‚ÜíCIFAR-10 and ImageNet-200
274"
TRANSFER TO DOWNSTREAM TASKS,0.7322695035460993,"‚ÜíCaltech-256, Oxford-Flowers-102. The first and second row show robust and clean accuracy on
275"
TRANSFER TO DOWNSTREAM TASKS,0.7340425531914894,"the downstream task respectively. As before, we compare with a random (red) and a full AT baseline
276"
TRANSFER TO DOWNSTREAM TASKS,0.7358156028368794,"(gray line). Selecting A to contain the hardest examples first (highest entropy) is marked by a solid
277"
TRANSFER TO DOWNSTREAM TASKS,0.7375886524822695,"line; selecting easiest is marked by a dashed line.
278"
TRANSFER TO DOWNSTREAM TASKS,0.7393617021276596,"In line with the improvements seen using the appropriate loss weighting, we see similarly fast
279"
TRANSFER TO DOWNSTREAM TASKS,0.7411347517730497,"recovery of baseline AT performance across all dataset. In fact, |A| containing only 30% of training
280"
TRANSFER TO DOWNSTREAM TASKS,0.7429078014184397,"data (15k and 70k) is sufficient to reach near baseline performance. On CIFAR-100 ‚ÜíCIFAR-10
281"
TRANSFER TO DOWNSTREAM TASKS,0.7446808510638298,"and ImageNet-200 ‚ÜíFlowers-102 even slightly outperforming the same with a further increase
282"
TRANSFER TO DOWNSTREAM TASKS,0.7464539007092199,"in size. Similar to the non-transfer settings tested before, we also see similar interactions between
283"
TRANSFER TO DOWNSTREAM TASKS,0.74822695035461,CIFAR-100 ‚ÜíCIFAR-10 20 30 40
TRANSFER TO DOWNSTREAM TASKS,0.75,Rob. accuracy
TRANSFER TO DOWNSTREAM TASKS,0.75177304964539,CIFAR10
K,0.7535460992907801,"5k
10k 15k 20k 25k 30k 35k 40k 45k
5k
10k 15k 20k 25k 30k 35k 40k 45k
5k
10k 15k 20k 25k 30k 35k 40k 45k
# of adv. trained examples (|A|) on CIFAR100 65 70 75 80"
K,0.7553191489361702,Clean accuracy
K,0.7570921985815603,CIFAR10
K,0.7588652482269503,"S-ESAT hardest Ô¨Årst
S-ESAT easiest Ô¨Årst"
K,0.7606382978723404,"S-ESAT-random
full AT on CIFAR100"
K,0.7624113475177305,ImageNet-200 ‚ÜíCaltech-256 30 40
K,0.7641843971631206,Rob. accuracy
K,0.7659574468085106,Caltech256
K,0.7677304964539007,"23k 47k 70k 94k 117k 140k 164k 187k 211k
23k 47k 70k 94k 117k 140k 164k 187k 211k
23k 47k 70k 94k 117k 140k 164k 187k 211k
# of adv. trained examples (|A|) on ImageNet200 60 65 70"
K,0.7695035460992907,Clean accuracy
K,0.7712765957446809,Caltech256
K,0.7730496453900709,"S-ESAT hardest Ô¨Årst
S-ESAT easiest Ô¨Årst"
K,0.774822695035461,"S-ESAT-random
full AT on ImageNet200"
K,0.776595744680851,ImageNet-200 ‚ÜíFlowers-102 30 35 40 45 50
K,0.7783687943262412,Rob. accuracy
K,0.7801418439716312,Flowers102
K,0.7819148936170213,"23k 47k 70k 94k 117k 140k 164k 187k 211k
23k 47k 70k 94k 117k 140k 164k 187k 211k
23k 47k 70k 94k 117k 140k 164k 187k 211k
# of adv. trained examples (|A|) on ImageNet200 70 75"
K,0.7836879432624113,Clean accuracy
K,0.7854609929078015,Flowers102
K,0.7872340425531915,"S-ESAT hardest Ô¨Årst
S-ESAT easiest Ô¨Årst"
K,0.7890070921985816,"S-ESAT-random
full AT on ImageNet200"
K,0.7907801418439716,"Figure 8: Transfer from S-ESAT to three different downstream tasks. S-ESAT is trained on source
dataset CIFAR-100 (left) and ImageNet-200 (middle and right). We report robust (top row) and clean
(bottom) accuracies for increasing size of A. Similar to our investigation on transfer from A to B, we
find that hard examples provide better robustness transfer than easy ones, but random selections (red)
achieve competitive performances. Most importantly, ‚Äúseeing‚Äù only few AEs (here 30% of source
data) recovers baseline AT performance (gray line)."
K,0.7925531914893617,"subset selection strategies. I.e. hardest examples (solid line) provide greater robustness transfer
284"
K,0.7943262411347518,"than easiest (dashed line) while a random baseline (red) achieves competitive performances. The
285"
K,0.7960992907801419,"latter consistently outperforming entropy selection on ImageNet-200 ‚ÜíFlowers-102, supporting our
286"
K,0.7978723404255319,"observation in section 4.2: with increasing dataset complexity, informed subset selection provides
287"
K,0.799645390070922,"diminishing returns. Note that all robust accuracy increases proportionally correlate to an increase in
288"
K,0.8014184397163121,"clean accuracy as well. This is in stark contrast to the inverse relationship in previous settings. C.f.
289"
K,0.8031914893617021,"figure 5 and 6, for which clean accuracy decreases. This interaction during transfer is similar to what
290"
K,0.8049645390070922,"is reported in [8]: increased robustness of the source model results in increased clean accuracy on the
291"
K,0.8067375886524822,"target task (over a non-robust model). Intriguingly though, with appropriate weighting, the biggest
292"
K,0.8085106382978723,"robustness gains on the downstream task happen under fairly small A. This is a promising outlook
293"
K,0.8102836879432624,"for introducing robustness in the foundational setting [37], where models are generally trained on
294"
K,0.8120567375886525,"very large datasets, for which AT is multiple factors more expensive to train. Note that our results
295"
K,0.8138297872340425,"generalize to single-step attacks like fast gradient sign method (FGSM) [18, 38] as well. We provide
296"
K,0.8156028368794326,"evaluations in the supplement. While we consider the fixed-feature transfer only, recent work has
297"
K,0.8173758865248227,"shown this to be a reliable indicator for utility on full-network transfer [8, 39].
298"
CONCLUSION,0.8191489361702128,"5
Conclusion
299"
CONCLUSION,0.8209219858156028,"In this paper, we presented an analysis of how adversarial robustness transfers between classes,
300"
CONCLUSION,0.8226950354609929,"examples and tasks. To this end, we proposed the use of Subset Adversarial Training (SAT), which
301"
CONCLUSION,0.824468085106383,"splits the training data into A and B and constructs AEs on A only. Trained on CIFAR-10, CIFAR-
302"
CONCLUSION,0.8262411347517731,"100 and ImageNet-200, SAT revealed a surprising generalizability of robustness between subsets,
303"
CONCLUSION,0.8280141843971631,"which we found to be based on the following observations: (i) adv. robustness transfers among
304"
CONCLUSION,0.8297872340425532,"classes even if some or most classes have never been attacked during training and (ii) hard classes
305"
CONCLUSION,0.8315602836879432,"and examples provide better robustness transfer than easy ones. These observations remained largely
306"
CONCLUSION,0.8333333333333334,"valid in the transfer to downstream tasks like Flowers-102 and Caltech-256 for which we found that
307"
CONCLUSION,0.8351063829787234,"overemphasizing loss minimization of AEs in A provided fast convergence to baseline AT robust
308"
CONCLUSION,0.8368794326241135,"accuracies, even though transfer to B was severely reduced. Specifically, it appears that only few AEs
309"
CONCLUSION,0.8386524822695035,"(A containing 30% of the training set) learn all of the robust features which generalize to downstream
310"
CONCLUSION,0.8404255319148937,"tasks. This finding could be particularly interesting for AT in the foundational setting, in which very
311"
CONCLUSION,0.8421985815602837,"large datasets render training computationally demanding.
312"
CONCLUSION,0.8439716312056738,"More broadly, improving adversarial robustness remains one of the most important problems to
313"
CONCLUSION,0.8457446808510638,"solve in deep learning, especially in high-stake decision making like autonomous driving or medical
314"
CONCLUSION,0.8475177304964538,"diagnostics. Our findings shed new light onto the properties of adversarial training and may lead to
315"
CONCLUSION,0.849290780141844,"more efficient robustness transfer approaches which would allow easier deployment of robust models.
316"
CONCLUSION,0.851063829787234,"We provided an account on a broad variety of datasets and used models commonly evaluated in
317"
CONCLUSION,0.8528368794326241,"related work. It needs to be seen whether our findings generalize to other threat models [40] as well.
318"
REFERENCES,0.8546099290780141,"References
319"
REFERENCES,0.8563829787234043,"[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good-
320"
REFERENCES,0.8581560283687943,"fellow, and Rob Fergus. Intriguing properties of neural networks. ICLR, 2014.
321"
REFERENCES,0.8599290780141844,"[2] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
322"
REFERENCES,0.8617021276595744,"Towards deep learning models resistant to adversarial attacks. ICLR, 2018.
323"
REFERENCES,0.8634751773049646,"[3] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
324"
REFERENCES,0.8652482269503546,"Tsipras, Ian J Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial
325"
REFERENCES,0.8670212765957447,"robustness. ICLR, 2019.
326"
REFERENCES,0.8687943262411347,"[4] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
327"
REFERENCES,0.8705673758865248,"Theoretically principled trade-off between robustness and accuracy. ICML, 2019.
328"
REFERENCES,0.8723404255319149,"[5] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
329"
REFERENCES,0.874113475177305,"data improves adversarial robustness. NeurIPS, 2019.
330"
REFERENCES,0.875886524822695,"[6] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust
331"
REFERENCES,0.8776595744680851,"generalization. NeurIPS, 2020.
332"
REFERENCES,0.8794326241134752,"[7] Ali Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer, David Jacobs, and
333"
REFERENCES,0.8812056737588653,"Tom Goldstein. Adversarially robust transfer learning. ICLR, 2020.
334"
REFERENCES,0.8829787234042553,"[8] Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do
335"
REFERENCES,0.8847517730496454,"adversarially robust imagenet models transfer better? NeurIPS, 2020.
336"
REFERENCES,0.8865248226950354,"[9] Yutaro Yamada and Mayu Otani. Does robustness on imagenet transfer to downstream tasks?
337"
REFERENCES,0.8882978723404256,"CVPR, 2022.
338"
REFERENCES,0.8900709219858156,"[10] Qi Tian, Kun Kuang, Kelu Jiang, Fei Wu, and Yisen Wang. Analysis and applications of
339"
REFERENCES,0.8918439716312057,"class-wise robustness in adversarial training. SIGKDD, 2021.
340"
REFERENCES,0.8936170212765957,"[11] Zhikang Xia, Bin Chen, Tao Dai, and Shu-Tao Xia. Class aware robust training. ICASSP, 2021.
341"
REFERENCES,0.8953900709219859,"[12] David Stutz, Matthias Hein, and Bernt Schiele. Disentangling adversarial robustness and
342"
REFERENCES,0.8971631205673759,"generalization. CVPR, 2019.
343"
REFERENCES,0.898936170212766,"[13] Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Mma training:
344"
REFERENCES,0.900709219858156,"Direct input space margin maximization through adversarial training. ICLR, 2020.
345"
REFERENCES,0.9024822695035462,"[14] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving
346"
REFERENCES,0.9042553191489362,"adversarial robustness requires revisiting misclassified examples. ICLR, 2020.
347"
REFERENCES,0.9060283687943262,"[15] Minseon Kim, Jihoon Tack, Jinwoo Shin, and Sung Ju Hwang. Entropy weighted adversarial
348"
REFERENCES,0.9078014184397163,"training. ICML Workshop on Adversarial Machine Learning, 2021.
349"
REFERENCES,0.9095744680851063,"[16] Weizhe Hua, Yichi Zhang, Chuan Guo, Zhiru Zhang, and G Edward Suh. Bullettrain: Acceler-
350"
REFERENCES,0.9113475177304965,"ating robust neural network training via boundary example mining. NeurIPS, 2021.
351"
REFERENCES,0.9131205673758865,"[17] Hadi M Dolatabadi, Sarah Erfani, and Christopher Leckie. l-inf-robustness and beyond: Un-
352"
REFERENCES,0.9148936170212766,"leashing efficient adversarial training. ECCV, 2022.
353"
REFERENCES,0.9166666666666666,"[18] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adver-
354"
REFERENCES,0.9184397163120568,"sarial examples. ICLR, 2015.
355"
REFERENCES,0.9202127659574468,"[19] Yogesh Balaji, Tom Goldstein, and Judy Hoffman. Instance adaptive adversarial training:
356"
REFERENCES,0.9219858156028369,"Improved accuracy tradeoffs in neural nets. arXiv preprint, 2019.
357"
REFERENCES,0.9237588652482269,"[20] Maximilian Kaufmann, Yiren Zhao, Ilia Shumailov, Robert Mullins, and Nicolas Papernot.
358"
REFERENCES,0.925531914893617,"Efficient adversarial training with data pruning. arXiv preprint, 2022.
359"
REFERENCES,0.9273049645390071,"[21] Sven Gowal, Po-Sen Huang, A√§ron van den Oord, Timothy A. Mann, and Pushmeet Kohli.
360"
REFERENCES,0.9290780141843972,"Self-supervised adversarial robustness for the low-label, high-data regime. In ICLR, 2021.
361"
REFERENCES,0.9308510638297872,"[22] Philipp Benz, Chaoning Zhang, Adil Karjauv, and In So Kweon. Robustness may be at odds
362"
REFERENCES,0.9326241134751773,"with fairness: An empirical study on class-wise accuracy. NeurIPS Workshop on Pre-registration
363"
REFERENCES,0.9343971631205674,"in Machine Learning, 2020.
364"
REFERENCES,0.9361702127659575,"[23] Vedant Nanda, Samuel Dooley, Sahil Singla, Soheil Feizi, and John P Dickerson. Fairness
365"
REFERENCES,0.9379432624113475,"through robustness: Investigating robustness disparity in deep learning. ACM FAccT, 2021.
366"
REFERENCES,0.9397163120567376,"[24] Saachi Jain, Hadi Salman, Alaa Khaddaj, Eric Wong, Sung Min Park, and Aleksander Madry.
367"
REFERENCES,0.9414893617021277,"A data-based perspective on transfer learning. CVPR, 2023.
368"
REFERENCES,0.9432624113475178,"[25] Mohamed Omran and Bernt Schiele. Towards systematic robustness for scalable visual recogni-
369"
REFERENCES,0.9450354609929078,"tion. ICML Shift Happens Workshop, 2022.
370"
REFERENCES,0.9468085106382979,"[26] Robert Baldock, Hartmut Maennel, and Behnam Neyshabur. Deep learning through the lens of
371"
REFERENCES,0.9485815602836879,"example difficulty. NeurIPS, 2021.
372"
REFERENCES,0.950354609929078,"[27] Chirag Agarwal, Daniel D‚Äôsouza, and Sara Hooker. Estimating example difficulty using variance
373"
REFERENCES,0.9521276595744681,"of gradients. CVPR, 2022.
374"
REFERENCES,0.9539007092198581,"[28] Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning.
375"
REFERENCES,0.9556737588652482,"ICML, 2020.
376"
REFERENCES,0.9574468085106383,"[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
377"
REFERENCES,0.9592198581560284,"recognition. CVPR, 2016.
378"
REFERENCES,0.9609929078014184,"[30] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
379"
REFERENCES,0.9627659574468085,"2009.
380"
REFERENCES,0.9645390070921985,"[31] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural
381"
REFERENCES,0.9663120567375887,"adversarial examples. CVPR, 2021.
382"
REFERENCES,0.9680851063829787,"[32] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
383"
REFERENCES,0.9698581560283688,"hierarchical image database. CVPR, 2009.
384"
REFERENCES,0.9716312056737588,"[33] Paul Gavrikov and Janis Keuper. Adversarial robustness through the lens of convolutional filters.
385"
REFERENCES,0.973404255319149,"CVPR, 2022.
386"
REFERENCES,0.975177304964539,"[34] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
387"
REFERENCES,0.9769503546099291,"Robustness may be at odds with accuracy. ICLR, 2019.
388"
REFERENCES,0.9787234042553191,"[35] Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007.
389"
REFERENCES,0.9804964539007093,"[36] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large
390"
REFERENCES,0.9822695035460993,"number of classes. ICVGIP, 2008.
391"
REFERENCES,0.9840425531914894,"[37] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von
392"
REFERENCES,0.9858156028368794,"Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the
393"
REFERENCES,0.9875886524822695,"opportunities and risks of foundation models. arXiv preprint, 2021.
394"
REFERENCES,0.9893617021276596,"[38] Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial
395"
REFERENCES,0.9911347517730497,"training. ICLR, 2020.
396"
REFERENCES,0.9929078014184397,"[39] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better?
397"
REFERENCES,0.9946808510638298,"CVPR, 2019.
398"
REFERENCES,0.9964539007092199,"[40] Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense
399"
REFERENCES,0.99822695035461,"against unseen threat models. ICLR, 2021.
400"
