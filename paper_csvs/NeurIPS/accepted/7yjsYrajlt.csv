Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017123287671232876,"We propose the Target Charging Technique (TCT), a unified privacy analysis
framework for interactive settings where a sensitive dataset is accessed multi-
ple times using differentially private algorithms. Unlike traditional composition,
where privacy guarantees deteriorate quickly with the number of accesses, TCT
allows computations that don’t hit a specified target, often the vast majority, to be
essentially free (while incurring instead a small overhead on those that do hit their
targets). TCT generalizes tools such as the sparse vector technique and top-k se-
lection from private candidates and extends their remarkable privacy enhancement
benefits from noisy Lipschitz functions to general private algorithms."
INTRODUCTION,0.003424657534246575,"1
Introduction"
INTRODUCTION,0.005136986301369863,"In many practical settings of data analysis and optimization, the dataset D is accessed multiple times
interactively via different algorithms (Ai), so that Ai depends on the transcript of prior responses
(Aj(D))j<i. When each Ai is privacy-preserving, we are interested in tight end-to-end privacy
analysis. We consider the standard statistical framework of differential privacy introduced in [11].
Composition theorems [14] are a generic way to do that and achieve overall privacy cost that scales
linearly or (via “advanced” composition) with square-root dependence in the number of private
computations. We aim for a broad understanding of scenarios where the overall privacy bounds
can be lowered significantly via the following paradigm: Each computation is specified by a private
algorithm Ai together with a target ⊤i, that is a subset of its potential outputs. The total privacy cost
depends only on computations where the output hits its target, that is Ai(D) ∈⊤i. This paradigm
is suitable and can be highly beneficial when (i) the specified targets are a good proxy for the actual
privacy exposure and (ii) we expect the majority of computations to not hit their target, and thus
essentially be “free” in terms of privacy cost."
INTRODUCTION,0.00684931506849315,"The Sparse Vector Technique (SVT) [12, 30, 18, 34] is a classic special case. SVT is designed
for computations that have the form of approximate threshold tests applied to Lipschitz functions.
Concretely, each such AboveThreshold test is specified by a 1-Lipschitz function f and a threshold
value t and we wish to test whether f(D) ≳t. The textbook SVT algorithm compares a noisy
value with a noisy threshold (independent Laplace noise for the values and threshold noise that
can be updated only after positive responses). Remarkably, the overall privacy cost depends only
on positive responses: Roughly, composition is applied to twice the number of positive responses
instead of to the total number of computations. In our terminology, the target of each test is a
positive response. SVT privacy analysis benefits when the majority of AboveThreshold test results
are negative (and hence “free”). This makes SVT a key ingredient in a range of methods [13]:
private multiplicative weights [18], Propose-Test-Release [10], fine privacy analysis via distance-to-
stability [33], model-agnostic private learning [1],and designing streaming algorithms that are robust
to adaptive inputs [19, 6]."
INTRODUCTION,0.008561643835616438,"We aim to extend such target-hits privacy analysis to interactive applications of general private al-
gorithms (that is, algorithms that provide privacy guarantees but have no other assumptions): private
tests, where we would hope to incur privacy cost only for positive responses, and private algo-
rithms that return more complex outputs, e.g., vector average, cluster centers, a sanitized dataset,
or a trained ML model, where the goal is to incur privacy cost only when the output satisfies some
criteria. Textbook SVT, however, is less amenable to such extensions: First, SVT departs from
the natural paradigm of applying private algorithms to the dataset and reporting the output. A nat-
ural implementation of private AboveThreshold tests would add Laplace noise to the value and
compare with the threshold. Instead, SVT takes as input the Lipschitz output of the non-private al-
gorithms with threshold value and the privacy treatment is integrated (added noise both to values and
threshold). The overall utility and privacy of the complete interaction are analyzed with respect to
the non-private values, which is not suitable when the algorithms are already private. Moreover, the
technique of using a hidden shared threshold noise across multiple AboveThreshold tests is spe-
cific for Lipschitz functions, introduces dependencies between responses, and more critically, results
in separate privacy costs for reporting noisy values (that is often required by analytics tasks [22])."
INTRODUCTION,0.010273972602739725,"Consider private tests. The natural paradigm is to sequentially choose a test, apply it, and report the
result. The hope is to incur privacy loss only on positive responses. Private testing was considered
in prior works [21, 7] but in ways that departed from this paradigm: [21] processed the private tests
so that a positive answer is returned only when the probability p of a positive response by the private
test is very close to 1. This seems unsatisfactory: If the design goal of the private testing algorithm
was to report only very high probabilities, then this could have been more efficiently integrated into
the design, and if otherwise, then we miss out on acceptable positive responses with moderately high
probabilities (e.g. 95%)."
INTRODUCTION,0.011986301369863013,"Consider now Top-k selection, which is a basic subroutine in data analysis, where input algorithms
(Ai)i∈[m] (aka candidates) that return results with quality scores are provided in a batch (i.e., non
interactively). The selection returns the k candidates with highest quality scores on our dataset.
The respective private construct, where the data is sensitive and the algorithms are private, had
been intensely studied [23, 17, 32]. The top-k candidates can be viewed as target-hits and we
might hope for privacy cost that is close to a composition over k private computations, instead of
over m ≫k. The natural approach for top-k is one-shot (Algorithm 3), where each algorithm is
applied once and the responses with top-k scores are reported. Prior works on private selection
that achieve this analysis goal include those [9, 29] that use the natural one-shot selection but are
tailored to Lipschitz functions (apply the Exponential Mechanism [24] or the Report-Noise-Max
paradigm [13]) and works [21, 28, 7] that do apply with general private algorithms but significantly
depart from the natural one-shot approach: They make a randomized number of computations that
is generally much larger than m, with each Ai invoked multiple times or none. The interpretation of
the selection deviates from top-1 and does not naturally extend to top-k. We seek privacy analysis
that applies to one-shot top-k selection with candidates that are general private algorithms."
INTRODUCTION,0.0136986301369863,"The natural interactive paradigm and one-shot selection are simple, interpretable, and general. The
departures made in prior works were made for a reason: Simple arguments (that apply with both
top-1 one-shot private selection [21] and AboveThreshold tests) seem to preclude efficient target-
charging privacy analysis: With pure-DP, if we perform m computations that are ε-DP (that is,
m candidates or m tests), then the privacy parameter value for a pure DP bound is Ω(m)ε. With
approximate-DP, and even a single “hit,” the parameter values are (Ω(ε log(1/δ)), δ). The latter
suggests a daunting overhead of O(log(1/δ)) instead of O(1) per “hit.” We circumvent the men-
tioned limitations by taking approximate DP to be a reasonable relaxation and additionally, aim
for application regimes where many private computations are performed on the same dataset and
we expect multiple, say Ω(log(1/δ)), “target hits” (e.g. positive tests and sum of the k-values of
selections). With these relaxations in place, we seek a unified target-charging analysis (e.g. privacy
charge that corresponds to O(1) calls per “target hit”) that applies with the natural paradigm across
interactive calls and top-k selections."
OVERVIEW OF CONTRIBUTIONS,0.015410958904109588,"2
Overview of Contributions"
OVERVIEW OF CONTRIBUTIONS,0.017123287671232876,"We overview our contributions (proofs and details are provided in the Appendix). We introduce the
Target-Charging Technique (TCT) for privacy analysis over interactive private computations (see
Algorithm 1). Each computation performed on the sensitive dataset D is specified by a pair of"
OVERVIEW OF CONTRIBUTIONS,0.018835616438356163,"private algorithm Ai and target ⊤i. The interaction is halted after a pre-specified number τ of
computations satisfy Ai(D) ∈⊤i. We define targets as follows:
Definition 2.1 (q-Target). Let M : Xn →Y be a randomized algorithm. For q ∈(0, 1] and ε > 0,
we say that a subset ⊤⊆Y of outcomes is a q-Target of M, if the following holds: For any pair D0
and D1 of neighboring data sets, there exist p ∈[0, 1], and three distributions C, B0 and B1 such
that"
OVERVIEW OF CONTRIBUTIONS,0.02054794520547945,1. The distributions M(D0) and M(D1) can be written as the following mixtures:
OVERVIEW OF CONTRIBUTIONS,0.02226027397260274,"M(D0) ≡p · C + (1 −p) · B0,"
OVERVIEW OF CONTRIBUTIONS,0.023972602739726026,M(D1) ≡p · C + (1 −p) · B1.
OVERVIEW OF CONTRIBUTIONS,0.025684931506849314,"2. B0, B1 are (ε, 0)-indistinguishable,"
OVERVIEW OF CONTRIBUTIONS,0.0273972602739726,"3. min(Pr[B0 ∈⊤], Pr[B1 ∈⊤]) ≥q."
OVERVIEW OF CONTRIBUTIONS,0.02910958904109589,"The effectiveness of a target as a proxy of the actual privacy cost is measured by its q-value where
q ∈(0, 1]. We interpret 1/q as the overhead factor of the actual privacy exposure per target hit, that
is, the number of private accesses that correspond to a single target hit. Note that an algorithm with
a q-target for ε > 0 must be (ε, 0)-DP and that any (ε, 0)-DP algorithm has a 1-target, as the set of
all outcomes ⊤= Y is a 1-target (and hence also a q-target for any q ≤1). More helpful targets are
“smaller” (so that we are less likely to be charged) with a larger q (so that the overhead per charge
is smaller). We establish the following privacy bounds.
Lemma 2.2 (simplified meta privacy cost of target-charging). The privacy parameters of Algo-
rithm 1 are (ε′, δ) where ε′ ≈τ"
OVERVIEW OF CONTRIBUTIONS,0.030821917808219176,q ε and δ = e−Ω(τ).
OVERVIEW OF CONTRIBUTIONS,0.032534246575342464,"Alternatively, we obtain parameter values (ε′, δ′) = (fε(r, ε), fδ(r, ε) + e−Ω(τ)) where r ≈τ/q
and (fε(r, ε), fδ(r, ε)) are privacy parameter values for advanced composition [14] of r ε-DP com-
putations."
OVERVIEW OF CONTRIBUTIONS,0.03424657534246575,"Proof details for a more general statement that also applies with approximate DP algorithms are
provided in Section B (in which case the δ parameters of all calls simply add up). The idea is
simple: We compare the execution of Algorithm 4 on two neighboring data sets D0, D1. Given a
request (A, ⊤), let p, C, B, B0, B1 be the decomposition of A w.r.t. D0, D1 given by Definition 2.1.
Then, running A on D0, D1 can be implemented in the following equivalent way: we flip a p-biased
coin. With probability p, the algorithm samples from C and returns the result, without accessing
D0, D1 at all (!). Otherwise, the algorithm needs to sample from B0 or B1, depending on whether
the private data is D0 or D1. However, by Property 3 in Definition 2.1, there is a probability of
at least q that Algorithm 1 will “notice” the privacy-leaking computation by observing a result in
the target set ⊤. If this indeed happens, the algorithm increments the counter. On average, each
counter increment corresponds to 1"
OVERVIEW OF CONTRIBUTIONS,0.03595890410958904,"q accesses to the private data. Therefore we use the number of
target hits (multiplied by 1/q) as a proxy for the actual privacy leak. Finally, we apply concentration
inequalities to obtain high confidence bounds on the probability that the actual number of accesses
significantly exceeds its expectation of τ/q. The multiplicative error decreases when the number τ of
target hits is larger. In the regime τ > ln(1/δ), we amortize the mentioned O(log(1/δ)) overhead of
the natural paradigm so that each target hit results in privacy cost equivalent to O(1/q) calls. In the
regime of very few target hits (e.g., few private tests or private selections), we still have to effectively
“pay” for the larger τ = Ω(ln(1/δ)), but TCT still has some advantages over alternative approaches,
due to its use of the natural paradigm and its applicability with general private algorithms."
OVERVIEW OF CONTRIBUTIONS,0.03767123287671233,"TCT is simple but turns out to be surprisingly powerful due to natural targets with low overhead.
We present an expansive toolkit that is built on top of TCT and describe application scenarios."
NOTPRIOR TARGETS,0.039383561643835614,"2.1
NotPrior targets"
NOTPRIOR TARGETS,0.0410958904109589,"A NotPrior target of an ε-DP algorithm is specified by any outcome of our choice (the “prior”) that
we denote by ⊥. The NotPrior target is the set of all outcomes except ⊥. Surprisingly perhaps,
this is an effective target (See Section C for the proof that applies also with approximate-DP):
Lemma 2.3 (Property of a NotPrior target). Let A : X →Y ∪{⊥}, where ⊥̸∈Y, be an ε-DP
algorithm. Then the set of outcomes Y constitutes an
1
eε+1-target for A."
NOTPRIOR TARGETS,0.04280821917808219,Algorithm 1: Target Charging
NOTPRIOR TARGETS,0.04452054794520548,"Input: Dataset D = {x1, . . . , xn} ∈Xn. Integer τ ≥1 (Upper limit on the number of target hits).
Fraction q ∈[0, 1].
C ←0
// Initialize target hit counter
while C < τ do
// Main loop
Receive (A, ⊤) where A is an ε-DP mechanism, and ⊤is a q-target for A
r ←A(D)
Publish r
if r ∈⊤then C ←C + 1
// outcome is a target hit"
NOTPRIOR TARGETS,0.046232876712328765,"Note that for small ε, we have q approaching 1/2 and thus the overhead factor is close to 2. The
TCT privacy analysis is beneficial over plain composition when the majority of all outcomes in our
interaction match their prior ⊥. We describe application scenarios for NotPrior targets. For most of
these scenarios, TCT is the only method we are aware of that provides the stated privacy guarantees
in the general context."
NOTPRIOR TARGETS,0.04794520547945205,"Private testing
A private test is a private algorithm with a Boolean output. By specifying our
prior to be a negative response, we obtain (for small ε) an overhead of 2 for positive responses,
which matches SVT. TCT is the only method we are aware of that provides SVT-like guarantees
with general private tests."
NOTPRIOR TARGETS,0.04965753424657534,"Pay-only-for-change
When we have a prior on the result of each computation and expect the
results of most computations to agree with their respective prior, we set ⊥to be our prior. We report
all results but pay only for those that disagree with the prior. We describe some use cases where
paying only for change can be very beneficial (i) the priors are results of the same computations on an
older dataset, so they are likely to remain the same (ii) In streaming or dynamic graph algorithms, the
input is a sequence of updates where typically the number of changes to the output is much smaller
than the number of updates. Differential privacy was used to obtain algorithms that are robust to
adaptive inputs [19, 2] by private aggregation of non-robust copies. The pay-only-for-change allows
for number of changes to output (instead of the much larger number of updates) that is quadratic in
the number of copies. Our result enables such gain with any private aggregation algorithm (that is
not necessarily in the form of AboveThreshold tests)."
CONDITIONAL RELEASE,0.05136986301369863,"2.2
Conditional Release"
CONDITIONAL RELEASE,0.053082191780821915,"We have a private algorithm A : X →Y but are interested in the output A(D) only when a certain
condition holds (i.e., when the output is in ⊤⊆Y). The condition may depend on the interaction
transcript thus far (depend on prior computations and outputs). We expect most computations not
to meet their release conditions and want to be “charged” only for the ones that do. Recall that
with differential privacy, not reporting a result also leaks information on the dataset, so this is not
straightforward. We define A⊤:= ConditionalRelease(A, ⊤) as the operation that inputs a
dataset D, computes y ←A(D). If y ∈⊤, then publish y and otherwise publish ⊥. We show
that this operation can be analysed in TCT as a call with the algorithm and NotPrior target pair
(A⊤, ⊤), that is, a target hit occurs if and only if y ∈⊤:"
CONDITIONAL RELEASE,0.0547945205479452,"Lemma 2.4 (ConditionalRelease privacy analysis). A⊤satisfies the privacy parameters of A
and ⊤is a NotPrior target of A⊤."
CONDITIONAL RELEASE,0.05650684931506849,"Proof. A⊤processes the output of the private algorithm A and thus from post processing property
is also private with the same privacy parameter values. Now note that ⊤is a NotPrior target of A,
with respect to prior ⊥."
CONDITIONAL RELEASE,0.05821917808219178,We describe some example use-cases:
CONDITIONAL RELEASE,0.059931506849315065,"(i) Private learning of models from the data (clustering, regression, average, ML model) but we are
interested in the result only when its quality is sufficient, say above a specified threshold, or when
some other conditions hold."
CONDITIONAL RELEASE,0.06164383561643835,"(ii) Greedy coverage or representative selection type applications, where we incur privacy cost only
for selected items. To do so, we condition the release on the “coverage” of past responses. For
example, when greedily selecting a subset of features that are most relevant or a subset of centers
that bring most value."
CONDITIONAL RELEASE,0.06335616438356165,"(iii) Approximate AboveThreshold tests on Lipschitz functions, with release of above-threshold
noisy values: As mentioned, SVT incurs additional privacy cost for the reporting whereas TCT
(using ConditionalRelease) does not, so TCT benefits in the regime of sufficiently many target
hits."
CONDITIONAL RELEASE,0.06506849315068493,"(iv) AboveThreshold tests with sketch-based approximate distinct counts:
Distinct counting
sketches [16, 15, 5] meet the privacy requirement by the built-in sketch randomness [31]. We apply
ConditionalRelease and set ⊤to be above threshold values. In comparison, despite the function
(distinct count) being 1-Lipschitz, the use of SVT for this task incurs higher overheads in utility
(approximation quality) and privacy: Even for the goal of just testing, a direct use of SVT treats
the approximate value as the non-private input, which reduces accuracy due to the additional added
noise. Treating the reported value as a noisy Lipschitz still incurs accuracy loss due to the threshold
noise, threshold noise introduces bias, and analysis is complicated by the response not following a
particular noise distribution. For releasing values, SVT as a separate distinct-count sketch is needed
to obtain an independent noisy value [22], which increases both storage and privacy costs."
CONDITIONAL RELEASE WITH REVISIONS,0.06678082191780822,"2.3
Conditional Release with Revisions"
CONDITIONAL RELEASE WITH REVISIONS,0.0684931506849315,"We present an extension of Conditional Release that allows for followup revisions of the target. The
initial ConditionalRelease and the followup ReviseCR calls are described in Algorithm 2. The
ConditionalRelease call specifies a computation identifier h for later reference, an algorithm and
a target pair (A, ⊤). It draws rh ∼A(D) and internally stores rh and a current target ⊤h ←⊤.
When rh ∈⊤then rh is published and a charge is made. Otherwise, ⊥is published. Each (followup)
ReviseCR call specifies an identifier h and a disjoint extension ⊤′ to its current target ⊤h. If
rh ∈⊤′, then rh is published and a charge is made. Otherwise, ⊥is published. The stored current
target for computation h is augmented to include ⊤′. Note that a target hit occurs at most once in a
sequence of (initial and followup revise) calls and if and only if the result of the initial computation
rh is in the final target ⊤h."
CONDITIONAL RELEASE WITH REVISIONS,0.0702054794520548,Algorithm 2: Conditional Release and Revise Calls
CONDITIONAL RELEASE WITH REVISIONS,0.07191780821917808,"// Initial Conditional Release call:
Analysed in TCT as a (ε, δ)-DP algorithm A⊤and NotPrior target
⊤
Function ConditionalRelease(h, A, ⊤):
// unique identifier h, an (ε, δ)-DP algorithm A →Y,
⊤⊂Y"
CONDITIONAL RELEASE WITH REVISIONS,0.07363013698630137,"⊤h ←⊤
// Current target for computation h
TCT Charge for δ
// If δ > 0, see Section B
rh ←A(D)
// Result for computation h
if rh ∈⊤h then
// publish and charge only if outcome is in ⊤h
Publish rh
TCT Charge for a NotPrior target hit of an ε-DP algorithm
else"
CONDITIONAL RELEASE WITH REVISIONS,0.07534246575342465,Publish ⊥
CONDITIONAL RELEASE WITH REVISIONS,0.07705479452054795,"// Revise call:
Analysed in TCT as a 2ε-DP Algorithm (A | ¬⊤h)⊤′ and NotPrior target ⊤′
Function ReviseCR(h, ⊤′):
// Revise target to include ⊤′
Input: An identifier h of a prior ConditionalRelease call, target extension ⊤′ where ⊤′ ∩⊤h = ∅
if rh ∈⊤′ then
// Result is in current target, publish and charge
Publish rh
TCT Charge for a NotPrior target hit of an 2ε-DP algorithm
else"
CONDITIONAL RELEASE WITH REVISIONS,0.07876712328767123,Publish ⊥
CONDITIONAL RELEASE WITH REVISIONS,0.08047945205479452,"⊤h ←⊤h ∪⊤′
// Update the target to include extension"
CONDITIONAL RELEASE WITH REVISIONS,0.0821917808219178,We show the following (Proof provided in Section D):
CONDITIONAL RELEASE WITH REVISIONS,0.0839041095890411,"Lemma 2.5 (Privacy analysis for Algorithm 2). Each ReviseCR call can be analysed in TCT as a
call to a 2ε-DP algorithm with a NotPrior target ⊤′."
CONDITIONAL RELEASE WITH REVISIONS,0.08561643835616438,"Thus, the privacy cost of conditional release followed by a sequence of revise calls is within a factor
of 2 (due to the doubled privacy parameter on revise calls) of a single ConditionalRelease call
made with the final target. The revisions extension of conditional release facilitates our results for
private selection, which are highlighted next."
PRIVATE TOP-K SELECTION,0.08732876712328767,"2.4
Private Top-k Selection"
PRIVATE TOP-K SELECTION,0.08904109589041095,"Consider the nature one-shot top-k selection procedure as shown in Algorithm 3: We call each al-
gorithm once and report the k responses with the highest quality scores. We establish the following:"
PRIVATE TOP-K SELECTION,0.09075342465753425,"Lemma 2.6 (Privacy of One-Shot Top-k Selection). Consider one-shot top-k selection (Algo-
rithm 3) on a dataset D where {Ai} are (ε, δi)-DP. This selection can be simulated exactly in
TCT by a sequence of calls to (2ε, δ)-DP algorithms with NotPrior targets that has k target hits."
PRIVATE TOP-K SELECTION,0.09246575342465753,"As a corollary, assuming ε < 1, Algorithm 3 is (O(ε
p"
PRIVATE TOP-K SELECTION,0.09417808219178082,"k log(1/δ)), 2−Ω(k) + δ + P"
PRIVATE TOP-K SELECTION,0.0958904109589041,"i δi)-DP for
every δ > 0."
PRIVATE TOP-K SELECTION,0.0976027397260274,"To the best of our knowledge, our result is the first such bound for one-shot selection from general
private candidates. For the case when the only computation performed on D is a single top-1 se-
lection, we match the “bad example” in [21] (see Theorem J.1). In the regime where k > log(1/δ)
our bounds generalize those specific to Lipschitz functions in [9, 29] (see Section J). Moreover,
Lemma 2.6 allows for a unified privacy analysis of interactive computations that are interleaved
with one-shot selections. We obtain O(1) overhead per target hit when there are Ω(log(1/δ)) hits in
total."
PRIVATE TOP-K SELECTION,0.09931506849315068,"Algorithm 3: One-Shot Top-k Selection
Input: A dataset D. Candidate algorithms A1, . . . , Am. Parameter k ≤m.
S ←∅
for i = 1, . . . , m do"
PRIVATE TOP-K SELECTION,0.10102739726027397,"(yi, si) ←Ai(D)
S ←S ∪{(i, yi, si)}
return L ←the top-k triplets from S, by decreasing si"
PRIVATE TOP-K SELECTION,0.10273972602739725,"The proofs of Lemma 2.6 and implications to selection tasks are provided in Section J. The proof
utilizes Conditional Release with revisions (Section 2.3)."
SELECTION USING CONDITIONAL RELEASE,0.10445205479452055,"2.4.1
Selection using Conditional Release"
SELECTION USING CONDITIONAL RELEASE,0.10616438356164383,"We analyze private selection procedures using conditional release (see Section J for details). First
note that ConditionalRelease calls (without revising) suffice for one-shot above-threshold selec-
tion (release all results with a quality score that exceeds a pre-specified threshold t), with target hits
only on what was released: We simply specify the release condition to be si > t. What is missing
in order to implement one-shot top-k selection is an ability to find the “right” threshold (a value t
so that exactly k candidates have quality scores above t), while incurring only k target hits. The
revise calls provide the functionality of lowering the threshold of previous conditional release calls
(lowering the threshold amounts to augmenting the target). This functionality allows us to simulate a
sweep of the m results of the batch in the order of decreasing quality scores. We can stop the sweep
when a certain condition is met (the condition must be based on the prefix of the ordered sequence
that we viewed so far) and we incur target hits only for the prefix. To simulate a sweep, we run a
high threshold conditional release of all m candidates and then incrementally lower the threshold
using sets of m revise calls (one call per candidate). The released results are in decreasing order of
quality scores. To prove Lemma 2.6 we observe that the one-shot top-k selection (Algorithm 3) is
simulated exactly by such a sweep that halts after k scores are released (the sweep is only used for
analysis)."
SELECTION USING CONDITIONAL RELEASE,0.10787671232876712,"As mentioned, with this approach we can apply any stopping condition that depends on the prefix.
This allows us to use data-dependent selection criteria. One natural such criteria (instead of using"
SELECTION USING CONDITIONAL RELEASE,0.1095890410958904,"a rigid value of k) is to choose k when there is a large gap in the quality scores, that the (k + 1)st
quality score is much lower than the kth score [35]. This criterion can be implemented using a one-
shot algorithm and analyzed in the same way using an equivalent sweep. Data-dependent criteria
are also commonly used in applications such as clustering (choose “the right” number of clusters
according to gap in clustering cost) and greedy selection of representatives."
BEST OF MULTIPLE TARGETS,0.1113013698630137,"2.5
Best of multiple targets"
BEST OF MULTIPLE TARGETS,0.11301369863013698,"Multi-target charging is a simple but useful extension of Algorithm 1 (that is “single target”). With
k-TCT, queries have the form
 
A, (⊤i)i∈[k]

where ⊤i for i ∈[k] are q-targets (we allow targets to
overlap). The algorithm maintains k counters (Ci)i∈[k]. For each query, for each i, we increment Ci
if r ∈⊤i. We halt when mini Ci = τ."
BEST OF MULTIPLE TARGETS,0.11472602739726027,"The multi-target extension allows us to flexibly reduce the total privacy cost to that of the “best”
among k target indices in retrospect (the one that is hit the least number of times). Interestingly,
this extension is almost free in terms of privacy cost: The number of targets k only multiplies the δ
privacy parameter (see Section B.1 for details)."
BETWEENTHRESHOLDS IN TCT,0.11643835616438356,"2.6
BetweenThresholds in TCT"
BETWEENTHRESHOLDS IN TCT,0.11815068493150685,"The BetweenThresholds classifier refines the AboveThreshold test: The goal is to report if the
noisy Lipschitz value is below, between, or above two thresholds tl < tr. We aim for privacy
loss that only depends on between outcomes. An SVT-based BetweenThresholds was proposed
by [3] (with noisy value and thresholds). Their analysis required the gap size to satisfy tr −tl ≥
(12/ε)(log(10/ε) + log(1/δ) + 1)."
BETWEENTHRESHOLDS IN TCT,0.11986301369863013,"We consider the “natural” BetweenThresholds classifier that compares the Lipschitz value with
added Lap(1/ε) noise to the two threshold values and reports the result. This is ε-DP and we show
(see Section G) that the between outcome is a target with q ≥(1−e−(tr−tl)ε)·
1
eε+1. This q value is
at most that of a NotPrior target (
1
eε+1) and degrades smoothly as the gap decreases. Importantly,
there is almost no degradation for fairly small gaps: When tr −tl ≥2/ε (resp. 3/ϵ) the q-value is
≈
0.87
eε+1 (resp 0.95"
BETWEENTHRESHOLDS IN TCT,0.12157534246575342,eε+1).
BETWEENTHRESHOLDS IN TCT,0.1232876712328767,"One benefit of TCT BetweenThresholds is that it applies with much smaller gaps tr −tl compared
with [3], also asymptotically. Another benefit that holds even for large gaps (where the SVT variant
is applicable) is that the natural algorithm requires lower privacy noise for a given accuracy. In
Section H we demonstrate this improvement numerically in that we can answer ×6-×95 fold more
queries for the same accuracy requirements compared to [3]. This bring BetweenThresholds into
the practical regime."
BETWEENTHRESHOLDS IN TCT,0.125,"We can compare an AboveThreshold test with a threshold t with a BetweenThresholds classifier
with tl = t −1/ε and tr = t + 1/ε. Surprisingly perhaps, despite BetweenThresholds being
more informative than AboveThreshold, as it provides more granular information on the value, its
privacy cost is lower, and is much lower when values are either well above or well below the thresh-
olds. Somehow, the addition of the third between outcome to the test tightened the privacy analysis!
A natural question is whether we can extend this benefit more generally – inject a “boundary out-
come” as a target when our private algorithm does not have one, then tighten the privacy analysis
when queries are “far” from the boundary. We introduce next a method that achieves this goal."
THE BOUNDARY WRAPPER METHOD,0.1267123287671233,"2.7
The Boundary Wrapper Method"
THE BOUNDARY WRAPPER METHOD,0.1284246575342466,"When the algorithm is a tester or a classifier, the result is most meaningful when one outcome
dominates the distribution A(D). Moreover, when performing a sequence of tests or classification
tasks we might expect most queries to have high confidence labels (e.g., [27, 1]). Our hope then
is to incur privacy cost that depends only on the “uncertainty,” captured by the probability of non-
dominant outcomes. When we have for each computation a good prior on which outcome is most
likely, this goal can be achieved via NotPrior targets (Section 2.1). When we expect the whole
sequence to be dominated by one type of outcome, even when we don’t know which one it is, this
goal can be achieved via NotPrior with multiple targets (Section 2.5). But these approaches do not
apply when a dominant outcome exists in most computations but we have no handle on it."
THE BOUNDARY WRAPPER METHOD,0.13013698630136986,"For a private test A, can we choose a moving target per computation to be the value with the smaller
probability arg minb∈{0,1} Pr[A(D) = b]? More generally, with a private classifier, can we some-
how choose the target to be all outcomes except for the most likely one? Our boundary wrapper,
described in Algorithm 4, achieves that goal. The privacy wrapper W takes any private algorithm A,
such as a tester or a classifier, and wraps it to obtain algorithm W(A). The wrapped algorithm has
its outcome set augmented to include one boundary outcome ⊤that is designed to be a q-target. The
wrapper returns ⊤with some probability that depends on the distribution of A(D) and otherwise
returns a sample from A(D) (that is, the output we would get when directly applying A to D). We
then analyse the wrapped algorithm in TCT."
THE BOUNDARY WRAPPER METHOD,0.13184931506849315,"Note that the probability of the wrapper A returning ⊤is at most 1/3 and is roughly proportional
to the probability of sampling an outcome other than the most likely from A(D). When there is no
dominant outcome the ⊤probability tops at 1/3. Also note that a dominant outcome (has probability
p ∈[1/2, 1] in A(D)) has probability p/(2 −p) to be reported. This is at least 1/3 when p = 1/2
and is close to 1 when p is close to 1. For the special case of A being a private test, there is always
a dominant outcome."
THE BOUNDARY WRAPPER METHOD,0.13356164383561644,"A wrapped AboveThreshold test provides the benefit of BetweenThresholds discussed in Sec-
tion 2.6 where we do not pay privacy cost for values that are far from the threshold (on either side).
This is achieved mechanically without the need to explicitly introduce two thresholds around the
given one and defining a different algorithm."
THE BOUNDARY WRAPPER METHOD,0.13527397260273974,Algorithm 4: Boundary Wrapper
THE BOUNDARY WRAPPER METHOD,0.136986301369863,"Input: Dataset D = {x1, . . . , xn} ∈Xn, a private algorithm A
r∗←arg maxr Pr[A(D) = r]
// The most likely outcome of A(D)
π(D) ←1 −Pr[A(D) = r∗]
// Probability that A does not return the most likely outcome"
THE BOUNDARY WRAPPER METHOD,0.1386986301369863,"c ∼Ber(min
n
1
3,
π
1+π
o
)
// Coin toss for boundary"
THE BOUNDARY WRAPPER METHOD,0.1404109589041096,"if c = 1 then Return ⊤else Return A(D)
// return boundary or value"
THE BOUNDARY WRAPPER METHOD,0.1421232876712329,"We show (proofs provided in Section E) that the wrapped algorithm is nearly as private as its base-
line:"
THE BOUNDARY WRAPPER METHOD,0.14383561643835616,"Lemma 2.7 (Privacy of a wrapped algorithm). If A is ε-DP then Algorithm 4 applied to A is t(ε)-
DP where t(ε) ≤4 3ε."
THE BOUNDARY WRAPPER METHOD,0.14554794520547945,"Lemma 2.8 (q-value of the boundary target). The outcome ⊤of a boundary wrapper (Algorithm 4)
of an ε-DP algorithm is a
et(ε)−1
2(eε+t(ε)−1)-target."
THE BOUNDARY WRAPPER METHOD,0.14726027397260275,For small ε we obtain q ≈t(ε)/(2(ε + t(ε)). Substituting t(ε) = 4
THE BOUNDARY WRAPPER METHOD,0.14897260273972604,3ε we obtain q ≈2
SINCE THE,0.1506849315068493,"7. Since the
target ⊤has probability at most 1/3, this is a small loss of efficiency (1/6 factor overhead) compared
with composition in the worst case when there are no dominant outcomes."
SINCE THE,0.1523972602739726,"The boundary wrapper yields light-weight privacy analysis that pays only for the “uncertainty” of the
response distribution A(D) and can be an alternative to more complex approaches based on smooth
sensitivity (the stability of A(D) to changes in D) [25, 10, 33].
Note that the boundary-wrapper
method assumes availability of the probability of the most dominant outcome in the distribution
A(D), when it is large enough. The probability can always be computed without incurring pri-
vacy costs (only computation cost) and is readily available with the Exponential Mechanism [24] or
when applying known noise distributions for AboveThreshold, BetweenThresholds, and Report-
Noise-Max [9]. In Section F we propose a boundary-wrapper that only uses sampling access to
A(D)."
APPLICATIONS TO PRIVATE LEARNING USING NON-PRIVACY-PRESERVING MODELS,0.1541095890410959,"2.7.1
Applications to Private Learning using Non-privacy-preserving Models"
APPLICATIONS TO PRIVATE LEARNING USING NON-PRIVACY-PRESERVING MODELS,0.1558219178082192,"Methods that achieve private learning through training non-private models include Private Aggre-
gation of Teacher Ensembles (PATE) [26, 27] and Model-Agnostic private learning [1]. The private
dataset D is partitioned into k parts D = D1 ⊔· · · ⊔Dk and a model is trained (non-privately) on
each part. For multi-class classification with c labels, the trained models can be viewed as functions
{fi : X →[c]}i∈[k]. Note that changing one sample in D can only change the training set of one
of the models. To privately label an example x drawn from a public distribution, we compute the"
APPLICATIONS TO PRIVATE LEARNING USING NON-PRIVACY-PRESERVING MODELS,0.15753424657534246,"predictions of all the models {fi(x)}i∈[k] and consider the counts nj = P
i∈[k] 1{fi(x) = j} (the
number of models that gave label j to example x) for j ∈[c]. We then privately aggregate to obtain
a private label, for example using the Exponential Mechanism [24] or Report-Noisy-Max [9, 29].
This setup is used to process queries (label examples) until the privacy budget is exceeded. In PATE,
the new privately-labeled examples are used to train a new student model (and {fi} are called teacher
models). In these applications we seek tight privacy analysis. Composition over all queries – for
O(1) privacy, only allows for O(k2) queries. We aim to replace this with O(k2) “target hits.” These
works used a combination of methods including SVT, smooth sensitivity, distance-to-instability, and
propose-test-release [10, 33]. The TCT toolkit can streamline the analysis:"
APPLICATIONS TO PRIVATE LEARNING USING NON-PRIVACY-PRESERVING MODELS,0.15924657534246575,"(i) It was noted in [1, 27] that when the teacher models are sufficiently accurate, we can expect that
nj ≫k/2 on the ground truth label j on most queries. High-agreement examples are also more
useful for training the student model. Moreover, agreement implies stability and lower privacy cost
(when accounted through the mentioned methods) is lower. Instead, to gain from this stability, we
can apply the boundary wrapper (Algorithm 4) on top of the Exponential Mechanism. Then use ⊤
as our target. Agreement queries, where maxj nj ≫k/2 (or more finely, when h = arg maxj nj
and nh ≫maxj∈[k]\{h} nj) are very unlikely to be target hits."
APPLICATIONS TO PRIVATE LEARNING USING NON-PRIVACY-PRESERVING MODELS,0.16095890410958905,"(ii) If we expect most queries to be either high agreement maxj nj ≫k/2 or no agreement
maxj nj ≪k/2 and would like to avoid privacy charges also with no agreement, we can apply
AboveThreshold test to maxj nj. If above, we apply the exponential mechanism. Otherwise, we
report “Low.” The wrapper applied to the combined algorithm returns a label in [c], “Low,” or ⊤.
Note that “Low” is a dominant outcome with no-agreement queries (where the actual label is not
useful anyway) and a class label in [c] is a dominant outcome with high agreement. We therefore
incur privacy loss only on weak agreements."
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.16267123287671234,"2.8
SVT with individual privacy charging"
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.1643835616438356,"Our TCT privacy analysis simplifies and improves the analysis of SVT with individual privacy charg-
ing, introduced by Kaplan et al [20]. The input is a dataset D ∈X n and an online sequence of linear
queries that are specified by predicate and threshold value pairs (fi, Ti). For each query, the algo-
rithms reports noisy AboveThreshold test results P"
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.1660958904109589,"x∈D fi(x) ≳T. Compared with the standard
SVT, which halts after reporting τ positive responses, SVT with individual charging maintains a
separate budget counter Cx for each item x. For each query with a positive response, the algorithm
only charges items that contribute to this query (namely, all the x’s such that fi(x) = 1). Once an
item x contributes to τ hits (that is, Cx = τ), it is removed from the data set. This finer privacy
charging improves utility with the same privacy budget, as demonstrated by several recent works
[20, 6]. We establish the following:"
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.1678082191780822,"Theorem 2.9 (Privacy of Algorithm 5). Assume ε < 1. Algorithm 5 is (O(
p"
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.1695205479452055,"τ log(1/δ)ε, 2−Ω(τ) +
δ)-DP for every δ ∈(0, 1)."
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.17123287671232876,"Compared with prior work [20]:
Algorithm 5 simply adds Laplace noise to obtain ˆfi
=
 P"
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.17294520547945205,"x∈D fi(x)

+ Lap(1/ε) and then tests whether ˆfi ≥T, whereas [20] adds two independent
Laplace noises and publishes the approximate sum ˆfi for “Above-Threshold” without incurring ad-
ditional privacy loss. Our analysis is much simpler (few lines instead of several pages) and sig-
nificantly tighter, also asymptotically: For the privacy bound in the statement of Theorem 5, our
additive error is lower by a log(1/ε)
p"
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.17465753424657535,"log(1/δ) factor. Importantly, our improvement aligns the
bounds of SVT with individual privacy charging with those of standard SVT, bringing the former
into the practical regime. See Section I for details."
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.17636986301369864,"Conclusion
We introduced the Target Charging Technique (TCT), a versatile unified privacy anal-
ysis framework that is particularly suitable when a sensitive dataset is accessed multiple times via
differentially private algorithms. We provide an expansive toolkit and demonstrate significant im-
provement over prior work for basic tasks such as private testing and one-shot selection, describe
use cases, and list challenges for followup works. TCT is simple with low overhead and we hope
will be adopted in practice."
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.1780821917808219,"Algorithm 5: SVT with Individual Privacy Charging
Input: Private data set D ∈X n; privacy budget τ > 0; Privacy parameter ε > 0.
foreach x ∈D do Cx ←0
// Initialize a counter for item x
for i = 1, 2, . . . , do
// Receive queries
Receive a predicate fi : X →[0, 1] and threshold Ti ∈R
ˆfi ←
 P"
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.1797945205479452,"x∈D fi(x)

+ Lap(1/ε)
// Add Laplace noise to count
if ˆfi ≥Ti then
// Compare with threshold
Publish ˆfi
foreach x ∈D such that f(x) > 0 do"
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.1815068493150685,"Cx ←Cx + 1
if Cx = τ then Remove x from D"
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.1832191780821918,else Publish ⊥
SVT WITH INDIVIDUAL PRIVACY CHARGING,0.18493150684931506,"Acknowledgement
Edith Cohen is partially supported by Israel Science Foundation (grant no.
1156/23)"
REFERENCES,0.18664383561643835,References
REFERENCES,0.18835616438356165,"[1] Raef Bassily, Om Thakkar, and Abhradeep Guha Thakurta. Model-agnostic private learning.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, edi-
tors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.,
2018."
REFERENCES,0.19006849315068494,"[2] Amos Beimel, Haim Kaplan, Yishay Mansour, Kobbi Nissim, Thatchaphol Saranurak, and Uri
Stemmer. Dynamic algorithms against an adaptive adversary: Generic constructions and lower
bounds. CoRR, abs/2111.03980, 2021."
REFERENCES,0.1917808219178082,"[3] Mark Bun, Thomas Steinke, and Jonathan Ullman. Make Up Your Mind: The Price of Online
Queries in Differential Privacy, pages 1306–1325. 2017."
REFERENCES,0.1934931506849315,"[4] H. Chernoff. A measure of the asymptotic efficiency for test of a hypothesis based on the sum
of observations. Annals of Math. Statistics, 23:493–509, 1952."
REFERENCES,0.1952054794520548,"[5] E. Cohen. Hyperloglog hyper extended: Sketches for concave sublinear frequency statistics.
In KDD. ACM, 2017. full version: https://arxiv.org/abs/1607.06517."
REFERENCES,0.1969178082191781,"[6] Edith Cohen, Xin Lyu, Jelani Nelson, Tam´as Sarl´os, Moshe Shechner, and Uri Stemmer. On
the robustness of countsketch to adaptive inputs. In Proceedings of the 39th International
Conference on Machine Learning (ICML), 2022."
REFERENCES,0.19863013698630136,"[7] Edith Cohen, Xin Lyu, Jelani Nelson, Tam´as Sarl´os, and Uri Stemmer. Generalized Private
Selection and Testing with High Confidence. In 14th Innovations in Theoretical Computer Sci-
ence Conference (ITCS 2023), volume 251 of Leibniz International Proceedings in Informatics
(LIPIcs), Dagstuhl, Germany, 2023. Schloss Dagstuhl – Leibniz-Zentrum f¨ur Informatik."
REFERENCES,0.20034246575342465,"[8] Edith Cohen, Xin Lyu, Jelani Nelson, Tam´as Sarl´os, and Uri Stemmer. Optimal differentially
private learning of thresholds and quasi-concave optimization. In Proceedings of the 55th
Annual ACM Symposium on Theory of Computing, STOC 2023, page 472–482, New York,
NY, USA, 2023. Association for Computing Machinery."
REFERENCES,0.20205479452054795,"[9] David Durfee and Ryan M. Rogers. Practical differentially private top-k selection with pay-
what-you-get composition. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Flo-
rence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Informa-
tion Processing Systems 32: Annual Conference on Neural Information Processing Systems
2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 3527–3537, 2019."
REFERENCES,0.20376712328767124,"[10] Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. STOC ’09, New York,
NY, USA, 2009. Association for Computing Machinery."
REFERENCES,0.2054794520547945,"[11] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensi-
tivity in private data analysis. In TCC, 2006."
REFERENCES,0.2071917808219178,"[12] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N. Rothblum, and Salil Vadhan. On the
complexity of differentially private data release: Efficient algorithms and hardness results. In
Proceedings of the Forty-First Annual ACM Symposium on Theory of Computing, STOC ’09,
page 381–390, New York, NY, USA, 2009. Association for Computing Machinery."
REFERENCES,0.2089041095890411,"[13] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Found.
Trends Theor. Comput. Sci., 9(3–4):211–407, aug 2014."
REFERENCES,0.2106164383561644,"[14] Cynthia Dwork, Guy N. Rothblum, and Salil P. Vadhan. Boosting and differential privacy.
In 51th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2010, October
23-26, 2010, Las Vegas, Nevada, USA, pages 51–60. IEEE Computer Society, 2010."
REFERENCES,0.21232876712328766,"[15] P. Flajolet, E. Fusy, O. Gandouet, and F. Meunier. Hyperloglog: The analysis of a near-optimal
cardinality estimation algorithm. In Analysis of Algorithms (AofA). DMTCS, 2007."
REFERENCES,0.21404109589041095,"[16] P. Flajolet and G. N. Martin. Probabilistic counting algorithms for data base applications.
Journal of Computer and System Sciences, 31:182–209, 1985."
REFERENCES,0.21575342465753425,"[17] Arik Friedman and Assaf Schuster. Data mining with differential privacy. In Proceedings of
the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
KDD ’10, page 493–502, New York, NY, USA, 2010. Association for Computing Machinery."
REFERENCES,0.21746575342465754,"[18] Moritz Hardt and Guy N. Rothblum.
A multiplicative weights mechanism for privacy-
preserving data analysis. In 51th Annual IEEE Symposium on Foundations of Computer Sci-
ence, FOCS 2010, October 23-26, 2010, Las Vegas, Nevada, USA, pages 61–70. IEEE Com-
puter Society, 2010."
REFERENCES,0.2191780821917808,"[19] Avinatan Hassidim, Haim Kaplan, Yishay Mansour, Yossi Matias, and Uri Stemmer. Adversar-
ially robust streaming algorithms via differential privacy. In Annual Conference on Advances
in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.2208904109589041,"[20] Haim Kaplan, Yishay Mansour, and Uri Stemmer. The sparse vector technique, revisited.
In Mikhail Belkin and Samory Kpotufe, editors, Conference on Learning Theory, COLT 2021,
15-19 August 2021, Boulder, Colorado, USA, volume 134 of Proceedings of Machine Learning
Research, pages 2747–2776. PMLR, 2021."
REFERENCES,0.2226027397260274,"[21] Jingcheng Liu and Kunal Talwar. Private selection from private candidates. In Moses Charikar
and Edith Cohen, editors, Proceedings of the 51st Annual ACM SIGACT Symposium on Theory
of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019, pages 298–309. ACM, 2019."
REFERENCES,0.2243150684931507,"[22] Min Lyu, Dong Su, and Ninghui Li. Understanding the sparse vector technique for differential
privacy. Proc. VLDB Endow., 10(6):637–648, 2017."
REFERENCES,0.22602739726027396,"[23] Frank McSherry and Ilya Mironov. Differentially private recommender systems: Building pri-
vacy into the netflix prize contenders. In Proceedings of the 15th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, KDD ’09, page 627–636, New York,
NY, USA, 2009. Association for Computing Machinery."
REFERENCES,0.22773972602739725,"[24] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In 48th Annual
IEEE Symposium on Foundations of Computer Science (FOCS 2007), October 20-23, 2007,
Providence, RI, USA, Proceedings, pages 94–103. IEEE Computer Society, 2007."
REFERENCES,0.22945205479452055,"[25] Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Smooth sensitivity and sampling in
private data analysis. In Proceedings of the Thirty-Ninth Annual ACM Symposium on Theory
of Computing, STOC ’07, page 75–84, New York, NY, USA, 2007. Association for Computing
Machinery."
REFERENCES,0.23116438356164384,"[26] Nicolas Papernot, Mart´ın Abadi, ´Ulfar Erlingsson, Ian J. Goodfellow, and Kunal Talwar. Semi-
supervised knowledge transfer for deep learning from private training data. In 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Con-
ference Track Proceedings. OpenReview.net, 2017."
REFERENCES,0.2328767123287671,"[27] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and ´Ulfar
Erlingsson. Scalable private learning with PATE. In 6th International Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings. OpenReview.net, 2018."
REFERENCES,0.2345890410958904,"[28] Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with R´enyi differential privacy.
In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event,
April 25-29, 2022. OpenReview.net, 2022."
REFERENCES,0.2363013698630137,"[29] Gang Qiao, Weijie J. Su, and Li Zhang. Oneshot differentially private top-k selection. In
Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on
Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of
Machine Learning Research, pages 8672–8681. PMLR, 2021."
REFERENCES,0.238013698630137,"[30] Aaron Roth and Tim Roughgarden.
Interactive privacy via the median mechanism.
In
Leonard J. Schulman, editor, Proceedings of the 42nd ACM Symposium on Theory of Com-
puting, STOC 2010, Cambridge, Massachusetts, USA, 5-8 June 2010, pages 765–774. ACM,
2010."
REFERENCES,0.23972602739726026,"[31] Adam Smith, Shuang Song, and Abhradeep Thakurta. The flajolet-martin sketch itself pre-
serves differential privacy: Private counting with minimal space. In Proceedings of the 34th
International Conference on Neural Information Processing Systems, NIPS’20, 2020."
REFERENCES,0.24143835616438356,"[32] Thomas Steinke and Jonathan R. Ullman. Tight lower bounds for differentially private se-
lection. In Chris Umans, editor, 58th IEEE Annual Symposium on Foundations of Computer
Science, FOCS 2017, Berkeley, CA, USA, October 15-17, 2017, pages 552–563. IEEE Com-
puter Society, 2017."
REFERENCES,0.24315068493150685,"[33] Abhradeep Guha Thakurta and Adam Smith. Differentially private feature selection via sta-
bility arguments, and the robustness of the lasso. In Shai Shalev-Shwartz and Ingo Steinwart,
editors, Proceedings of the 26th Annual Conference on Learning Theory, volume 30 of Pro-
ceedings of Machine Learning Research, pages 819–850, Princeton, NJ, USA, 12–14 Jun 2013.
PMLR."
REFERENCES,0.24486301369863014,"[34] Salil P. Vadhan. The complexity of differential privacy. In Yehuda Lindell, editor, Tutorials on
the Foundations of Cryptography, pages 347–450. Springer International Publishing, 2017."
REFERENCES,0.2465753424657534,"[35] Yuqing Zhu and Yu-Xiang Wang. Adaptive private-k-selection with adaptive k and application
to multi-label pate. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors,
Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, vol-
ume 151 of Proceedings of Machine Learning Research, pages 5622–5635. PMLR, 28–30 Mar
2022."
REFERENCES,0.2482876712328767,"A
Preliminaries"
REFERENCES,0.25,"Notation.
We say that a function f over datasets is t-Lipschitz if for any two neighboring datasest
D0, D1, it holds that |f(D1) −f(D0)| ≤t. For two reals a, b ≥0 and ε > 0, we write a ≈ε b if
e−εb ≤a ≤eεb."
REFERENCES,0.2517123287671233,"For two random variables X0, X1, we say that they are ε-indistinguishable, denoted X0 ≈ε X1,
if their max-divergence and symmetric counterpart are both at most ε. That is, for b ∈{0, 1},"
REFERENCES,0.2534246575342466,"maxS⊆supp(Xb) ln
h
Pr[Xb∈S]
Pr[X1−b∈S]
i
≤ε."
REFERENCES,0.2551369863013699,"We similarly say that for δ > 0, the random variables are (ε, δ)-indistinguishable, denoted X0 ≈ε,δ
X1, if for b ∈{0, 1}"
REFERENCES,0.2568493150684932,"max
S⊆supp(Xb) ln
Pr[Xb ∈S] −δ"
REFERENCES,0.2585616438356164,"Pr[X1−b ∈S] 
≤ε."
REFERENCES,0.2602739726027397,"For two probability distributions, B0, B1 We extend the same notation and write B0 ≈ε B1 and
B0 ≈ε,δ B1 when this holds for random variables drawn from the respective distributions."
REFERENCES,0.261986301369863,"The following relates (ε, 0) and (ε, δ)-indistinguishability with δ = 0 and δ > 0."
REFERENCES,0.2636986301369863,"Lemma A.1. Let B0, B1 be two distributions. Then B0 ≈ε,δ B1 if and only if we can express them
as mixtures
Bb ≡(1 −δ) · Nb + δ · Eb ,"
REFERENCES,0.2654109589041096,where N0 ≈ε N1.
REFERENCES,0.2671232876712329,"We treat random variables interchangeably as distributions, and in particular, for a randomized al-
gorithms A and input D we use A(D) to denote both the random variable and the distribution."
REFERENCES,0.2688356164383562,"For a space of datasets equipped with a symmetric neighboring relation, we say an algorithm A is ε-
DP (pure differential privacy), if for any two neighboring datasets D and D′, A(D) ≈ε A(D′). Sim-
ilarly, we say A is (ε, δ)-DP (approximate differential privacy) if for any two neighboring datasets
D, D′, it holds that A(D) ≈ε,δ A(D′) [11]. We refer to ε, δ as the privacy parameters."
REFERENCES,0.2705479452054795,"A private test is a differentially private algorithm with Boolean output (say in {0, 1})."
REFERENCES,0.2722602739726027,"Remark A.2. The literature in differential privacy uses different definitions of neighboring datasets.
TCT, and properties in these preliminaries, apply with any symmetric relation over the space of
possible input datasets."
REFERENCES,0.273972602739726,The following is immediate from Lemma A.1:
REFERENCES,0.2756849315068493,"Corollary A.3 (Decomposition of an approximate DP Algorithm). An algorithm A is (ε, δ)-DP if
and only if for any two neighboring datasets D0 and D1 we can represent each distribution A(Db)
(b ∈{0, 1}) as a mixture"
REFERENCES,0.2773972602739726,"A(Db) ≡(1 −δ) · Nb + δ · Eb ,"
REFERENCES,0.2791095890410959,where N0 ≈ε N1.
REFERENCES,0.2808219178082192,"Differential privacy satisfies the post-processing property (post-processing of the output of a private
algorithm remains private with the same parameter values) and also has nice composition theorems:"
REFERENCES,0.2825342465753425,"Lemma A.4 (DP composition [11, 14]). An interactive sequence of r executions of ε-DP algorithms
satisfies (ε′, δ)-DP for"
REFERENCES,0.2842465753424658,"• ε′ = rε and δ = 0 by basic composition [11], or"
REFERENCES,0.285958904109589,"• for any δ > 0,"
REFERENCES,0.2876712328767123,ε′ = 1
REFERENCES,0.2893835616438356,"2rε2 + ε
p"
REFERENCES,0.2910958904109589,2r log(1/δ) .
REFERENCES,0.2928082191780822,by advanced composition [14].
REFERENCES,0.2945205479452055,"A.1
Simulation-based privacy analysis"
REFERENCES,0.2962328767123288,"Privacy analysis of an algorithm A via simulations is performed by simulating the original algorithm
A on two neighboring datasets D0, D1. The simulator does not know which of the datasets is the
actual input (but knows everything about the datasets). Another entity called the ”data holder” has
the 1-bit information b ∈{0, 1} on which dataset it is. We perform privacy analysis with respect to
what the holder discloses to the simulator regarding the private bit b (taking the maximum over all
choices of D0,D1). The privacy analysis is worst case over the choices of two neighboring datasets.
This is equivalent to performing privacy analysis for A."
REFERENCES,0.2979452054794521,"Lemma A.5 (Simulation-based privacy analysis). [8] Let A be an algorithm whose input is a
dataset. If there exist a pair of interactive algorithms S and H satisfying the following 2 prop-
erties, then algorithm A is (ε, δ)-DP."
REFERENCES,0.2996575342465753,"1. For every two neighboring datasets D0, D1 and for every bit b ∈{0, 1} it holds that
 
S(D0, D1) ↔H(D0, D1, b)

≡A(Db)."
REFERENCES,0.3013698630136986,"Here
 
S(D0, D1) ↔H(D0, D1, b)

denotes the outcome of S after interacting with H."
REFERENCES,0.3030821917808219,"2. Algorithm H is (ε, δ)-DP w.r.t. the input bit b."
REFERENCES,0.3047945205479452,"A.2
Privacy Analysis with Failure Events"
REFERENCES,0.3065068493150685,Privacy analysis of a randomized algorithm A using designated failure events is as follows:
REFERENCES,0.3082191780821918,1. Designate some runs of the algorithm as failure events.
REFERENCES,0.3099315068493151,"2. Compute an upper bound on the maximum probability, over datasets D, of a transcript with
a failure designation."
REFERENCES,0.3116438356164384,3. Analyse the privacy of the interaction transcript conditioned on no failure designation.
REFERENCES,0.3133561643835616,"Note that the failure designation is only used for the purpose of analysis. The output on failure runs
is not restricted (e.g., could be the dataset D)"
REFERENCES,0.3150684931506849,"Lemma A.6 (Privacy analysis with privacy failure events). Consider privacy analysis of A with
failure events. If the probability of a failure event is bounded by δ∗∈[0, 1] and the transcript
conditioned on non-failure is (ε′, δ′)-DP then the algorithm A is (ε, δ + δ∗)-DP."
REFERENCES,0.3167808219178082,"Proof. Let D0 and D1 be neighboring datasets. From our assumptions, for b ∈{0, 1}, we can
represent A(Db) as the mixture A(Db) ≡(1−δb)·Zb +δb ·Fb, where Z0 ≈ε′,δ′ Z1, and δ(b) ≤δ∗.
From Lemma A.1, we have Zb ≡(1 −δ′) · Nb + δ′ · Eb, where N0 ≈ε′ N1. Then"
REFERENCES,0.3184931506849315,A(Db) = (1 −δ(b)) · Zb + δ(b) · F(b)
REFERENCES,0.3202054794520548,= (1 −δ∗) · Zb + (δ∗−δ(b)) · Zb + δ(b) · Fb
REFERENCES,0.3219178082191781,"= (1 −δ∗) · Zb + δ∗·

(1 −δ(b)/δ∗) · Zb + δ(b) · Fb"
REFERENCES,0.3236301369863014,"= (1 −δ∗)(1 −δ′) · Nb + (1 −δ∗)δ′ · Eb + δ∗·

(1 −δ(b)/δ∗) · Zb + δ(b) · Fb"
REFERENCES,0.3253424657534247,"= (1 −δ∗−δ′) · Nb + δ′δ∗· N + (1 −δ∗)δ′ · Eb + δ∗·

(1 −δ(b)/δ∗) · Zb + δ(b) · Fb"
REFERENCES,0.3270547945205479,The claim follows from Corollary A.3.
REFERENCES,0.3287671232876712,"Using simulation-based privacy analysis we can treat an interactive sequence of approximate-DP
algorithms (optionally with designated failure events) as a respective interactive sequence of pure-
DP algorithms where the δ parameters are anlaysed through failure events. This simplifies analysis:"
REFERENCES,0.3304794520547945,"We can relate the privacy of a composition of approximate-DP algorithms to that of a composition
of corresponding pure-DP algorithms:"
REFERENCES,0.3321917808219178,"Corollary A.7 (Composition of approximate-DP algorithms). An interactive sequence of (εi, δi)-
DP algorithms (i ∈[k]) has privacy parameter values (ε′, δ′ + Pk
i=1 δi), where (ε′, δ′) are privacy
parameter values of a composition of pure (εi, 0)-DP algorithms i ∈[k]."
REFERENCES,0.3339041095890411,"Proof. We perform simulation-based analysis. Fix two neighboring datasets D0, D1. For an (εi, δi)-
DP algorithm, we can consider the mixtures as in Corollary A.3. We draw c ∼Ber(δi) and if c = 1
designate the output as failure and return r ∼E(b). Otherwise, we return r ∼N(b). The overall
failure probability is bounded by 1 −Q"
REFERENCES,0.3356164383561644,i(1 −δi) ≤P
REFERENCES,0.3373287671232877,"i δi. The output conditioned on non-failure is
a composition of (εi, 0)-DP algorithms (i ∈[k]). The claim follows using Lemma A.6."
REFERENCES,0.339041095890411,"B
The Target-Charging Technique"
REFERENCES,0.3407534246575342,"We extend the definition of q-targets (Definition 2.1) so that it applies with approximate DP algo-
rithms:"
REFERENCES,0.3424657534246575,"Definition B.1 (q-target with (ε, δ) of a pair of distributions). Let A →Y be a randomized algo-
rithm. Let Z0 and Z1 be two distributions with support Y. We say that ⊤⊆Y is a q-target of
(Z0, Z1) with (ε, δ), where ε > 0 and δ ∈[0, 1), if there exist p ∈[0, 1] and five distributions C,
Bb, and Eb (for b ∈{0, 1}) such that Z0 and Z1 can be written as the mixtures"
REFERENCES,0.3441780821917808,Z0 ≡(1 −δ) · (p · C + (1 −p) · B0) + δ · E0
REFERENCES,0.3458904109589041,Z1 ≡(1 −δ) · (p · C + (1 −p) · B1) + δ · E1
REFERENCES,0.3476027397260274,"where B0 ≈ε B1, and min(Pr[B0 ∈⊤], Pr[B1 ∈⊤]) ≥q."
REFERENCES,0.3493150684931507,"Definition B.2 (q-target with (ε, δ) of a randomized algorithm). Let A →Y be a randomized
algorithm. We say that ⊤⊆Y is a q-target of A with (ε, δ), where ε > 0 and δ ∈[0, 1), if for any
pair D0, D1 of neighboring datasets, ⊤is a q-target with (ε, δ) of A(D0) and A(D1)."
REFERENCES,0.351027397260274,"We can relate privacy of an algorithms or indistinguishability of two distributions to existence of
q-targets:"
REFERENCES,0.3527397260273973,"Lemma B.3. (i) If (Z0, Z1) have a q-target with (ε, δ) then Z0 ≈ε,δ Z1. Conversely, if Z0 ≈ε,δ Z1"
REFERENCES,0.3544520547945205,"then (Z0, Z1) have a 1-target with (ε, δ) (the full support is a 1-target)."
REFERENCES,0.3561643835616438,"(ii) If an algorithm A has a q-target with (ε, δ) then A is (ε, δ)-DP. Conversely, if an algorithm A is
(ε, δ)-DP then it has a 1-target (the set Y) with (ε, δ)."
REFERENCES,0.3578767123287671,"Proof. If two distributions B0, B1 have a q-target with (ε, δ) than from Definition B.1 they can be
represented as mixtures. Now observe the if B0 ≈ε B1 then the mixtures also satisfy p · C + (1 −
p) · B0 ≈ε p · C + (1 −p) · B0. Using Lemma A.1, we get Z0 ≈ε,δ Z1."
REFERENCES,0.3595890410958904,"For (ii) consider A and two neighboring datasets D0 and D1. Using Definition B.2 and applying the
argument above we obtain A(D0) ≈ε,δ A(D1). The claim follows using Corollary A.3."
REFERENCES,0.3613013698630137,"Now for the converse. If Z0 ≈ε,δ Z1 then consider the decomposition as in Lemma A.1. Now we
set p = 0 and Bb ←Nb to obtain the claim with q = 1 and the target being the full support."
REFERENCES,0.363013698630137,"For (ii), if A →Y is (ε, δ)-DP then consider neighboring {D0, D1}. We have A(D0) ≈ε,δ A(D1).
We proceed as with the distributions."
REFERENCES,0.3647260273972603,"Algorithm 6 is an extension of Algorithm 1 that permits calls to approximate DP algorithms. The
extension also inputs a bound τ on the number of target hits and a bound τδ on the cummulative δ
parameter values of the algorithms that were called. We apply adaptively a sequence of (ε, δ)-DP
algorithms with specified q-targets to the input data set D and publish the results. We halt when the
first of the following happens (1) the respective target sets are hit for a specified τ number of times
(2) the accumulated δ-values exceed the specified limit τδ."
REFERENCES,0.3664383561643836,"The privacy cost of Target-Charging is as follows (This is a precise and more general statement of
Lemma 2.2):"
REFERENCES,0.3681506849315068,Algorithm 6: Target Charging with Approximate DP
REFERENCES,0.3698630136986301,"Input: Dataset D = {x1, . . . , xn} ∈Xn. Integer τ ≥1 (Upper limit on the number of target hits).
τδ ≥0 (upper limit on cumulative δ parameter). Fraction q ∈[0, 1].
C ←0, Cδ ←0
// Initialize target hit and failure counters
for i = 1, . . . do
// Main loop
Receive (Ai, ⊤i) where Ai is an (ε, δi)-DP mechanism, and ⊤i is a q-target with (ε, δi) for A
r ←Ai(D)
if Cδ + δi > τδ then Halt
Cδ ←Cδ + δ
// TCT charge for δi
Publish r
if r ∈⊤then
// TCT Charge for a q-target hit with ε
C ←C + 1
if C = τ then Halt"
REFERENCES,0.3715753424657534,Algorithm 7: Simulation of Target Charging
REFERENCES,0.3732876712328767,"Input: Two neighboring datasets D0, D1, private b ∈{0, 1}, τ ∈N, τδ ∈R≥0, q ∈[0, 1], α > 0.
C ←0, Cδ ←0, h ←0
// Initialize; h is a counter on the number of non-fail calls to data holder
for i = 1, . . . do
// Main loop
Receive (Ai, ⊤i) where Ai is an (ε, δi)-DP mechanism, and ⊤i is a q-target with (ε, δi) for A
if Cδ + δi > τδ then Halt
Cδ ←Cδ + δ
Let p ∈[0, 1], C, B0 ≈ε B1, and Eb (for b ∈{0, 1}) such that
A(Db) ≡(1 −δ) · (p · C + (1 −p) · Bb) + δ · Eb
// By Definition B.2
if Ber(δ) ≡1 then
// Non-private Data Holder call with Failure
Fail
Publish r ∼Eb
else"
REFERENCES,0.375,if Ber(p) ≡1 then
REFERENCES,0.3767123287671233,"Publish r ∼C
// No access to data holder
else"
REFERENCES,0.3784246575342466,"Publish r ∼Bb
// ε-DP Data Holder Call
h ←h + 1
// counter of ε-private data holder calls
if h > (1 + α)τ/q then
// Number of Holder calls exceeded limit
Fail
if r ∈⊤then
// outcome is a target hit
C ←C + 1
if C = τ then Halt"
REFERENCES,0.3801369863013699,"Theorem B.4 (Privacy of Target-Charging). Algorithm 6 satisfies the following approximate DP
privacy bounds:

(1 + α)τ"
REFERENCES,0.3818493150684932,"q ε, Cδ + δ∗(τ, α)

,
for any α > 0;
1"
REFERENCES,0.3835616438356164,2(1 + α)τ
REFERENCES,0.3852739726027397,"q ε2 + ε
r"
REFERENCES,0.386986301369863,(1 + α)τ
REFERENCES,0.3886986301369863,"q log(1/δ), δ + Cδ + δ∗(τ, α)

,
for any δ > 0, α > 0."
REFERENCES,0.3904109589041096,"where δ∗(τ, α) ≤e−
α2
2(1+α) τ and Cδ ≤τδ is as computed by the algorithm."
REFERENCES,0.3921232876712329,"Proof. We apply the simulation-based privacy analysis in Lemma A.5 and use privacy analysis with
failure events (Lemma A.6)."
REFERENCES,0.3938356164383562,"The simulation is described in Algorithm 7. Fix two neighboring data sets D0 and D1. The simulator
initializes the target hit counter C ←0 and the cumulative δ-values tracker Cδ ←0. For i ≥1 it
proceeds as follows. It receives (Ai, ⊤i) where Ai is (ε, δi)-DP. If Cδ + δi > τδ it halts. Since ⊤i
is a q-target for Ai, there are p, C, B0, B1, E0 and E1 as in Definition B.2. The simulator flips a
biased coin c′ ∼Ber(δ). If c′ = 1 it outputs r ∼Eb and the execution is designated as Fail. In this"
REFERENCES,0.3955479452054795,"case there is an interaction with the data holder but also a failure designation. The simulator flips
a biased coin c ∼Ber(p). If c = 1, then the simulator publishes a sample r ∼C (this does not
require an interaction with the data holder). Otherwise, the data holder is called. The data holder
publishes r ∼Bb. We track the number h of calls to the data holder. If h exceeds (1 + α)τ/q, we
designate the execution as Fail. If r ∈⊤i then C is incremented. If C = τ, the algorithm halts."
REFERENCES,0.3972602739726027,"The correctness of the simulation (faithfully simulating Algorithm 1 on the dataset Db) is straight-
forward. We analyse the privacy cost. We will show that"
REFERENCES,0.398972602739726,"(i) the simulation designated a failure with probability at most Cδ + δ∗(τ, α)."
REFERENCES,0.4006849315068493,"(ii) Conditioned on no failure designation, the simulation performed at most r = (1 + α) τ"
REFERENCES,0.4023972602739726,"q
adaptive calls to (ε, 0)-DP algorithms"
REFERENCES,0.4041095890410959,"Observe that (ii) is immediate from the simulation declaring failure when h > r. We will establish
(i) below."
REFERENCES,0.4058219178082192,"The statement of the Theorem follows from Lemma A.6 and when applying the DP composition
bounds (Lemma A.4). The first bounds follow using basic composition and the second follow using
advanced composition [14]."
REFERENCES,0.4075342465753425,"This analysis yields the claimed privacy bounds with respect to the private bit b. From Lemma A.5
this is the privacy cost of the algorithm."
REFERENCES,0.4092465753424658,"It remains to show bound the failure probability. There are two ways in which a failure can occur.
The first is on each call, with probability δi. This probability is bounded by 1−Q"
REFERENCES,0.410958904109589,i δi ≤P
REFERENCES,0.4126712328767123,"i δi ≤Cδ.
The second is when the number h of private accesses to the data holder exceeds the limit. We show
that the probability that the algorithm halts with failure due to that is at most δ∗."
REFERENCES,0.4143835616438356,"We consider a process that continues until τ charges are made. The privacy cost of the simulation
(with respect to the private bit b) depends on the number of times that the data holder is called. Let
X be the random variable that is the number of calls to the data holder. Each call is ε-DP with
respect to the private b. In each call, there is probability at least q for a “charge” (increment of C)."
REFERENCES,0.4160958904109589,"A failure is the event that the number of calls to data holder exceeds (1 + α)τ/q before τ charges
are made. We show that this occurs with probability at most δ∗(τ, α):"
REFERENCES,0.4178082191780822,"Pr

X > (1 + α)τ q"
REFERENCES,0.4195205479452055,"
≤δ∗(τ, α) .
(1)"
REFERENCES,0.4212328767123288,"To establish (1), we first observe that the distribution of the random variable X is dominated by a
random variable X′ that corresponds to a process of drawing i.i.d. Ber(q) until we get τ successes
(Domination means that for all m, Pr[X′ > m] ≥Pr[X > m]). Therefore, it suffices to establish
that"
REFERENCES,0.4229452054794521,"Pr

X′ > (1 + α)τ q"
REFERENCES,0.4246575342465753,"
≤δ∗(τ, α) ."
REFERENCES,0.4263698630136986,"Let Y be the random variable that is a sum of m = 1 +
j
(1 + α) τ"
REFERENCES,0.4280821917808219,"q
k
i.i.d. Ber(q) random variables.
Note that"
REFERENCES,0.4297945205479452,"Pr

X′ > (1 + α)τ q"
REFERENCES,0.4315068493150685,"
= Pr[Y < τ] ."
REFERENCES,0.4332191780821918,"We bound Pr[Y < τ] using multiplicative Chernoff bounds [4]1. The expectation is µ = mq and
we bound the probability that the sum of Bernoulli random variables is below
1
1+αµ = (1 −
α
1+α)µ.
Using the simpler form of the bounds we get using µ = mq ≥(1 + α)τ"
REFERENCES,0.4349315068493151,"Pr[Y < τ] = Pr[Y < (1 −
α
1 + α)µ] ≤e
−
α2"
REFERENCES,0.4366438356164384,"2(1+α)2 µ ≤e−
α2
2(1+α) τ ."
REFERENCES,0.4383561643835616,1Bound can be tightened when using precise tail probability values.
REFERENCES,0.4400684931506849,"Remark B.5 (Number of target hits). The TCT privacy analysis has a tradeoff between the final
“ε” and “δ” privacy parameters. There is multiplicative factor of (1 + α) (√1 + α with advanced
composition) on the “ε” privacy parameter. But when we use a smaller α we need a larger value of
τ to keep the “δ” privacy parameter small. For a given α, δ∗> 0, we can calculate a bound on the
smallest value of τ that works. We get"
REFERENCES,0.4417808219178082,τ ≥21 + α
REFERENCES,0.4434931506849315,"α2
· ln(1/δ∗)
(simplified Chernoff)"
REFERENCES,0.4452054794520548,"τ ≥
1
(1 + α) ln
 
eα/(1+α)(1 + α)−1/(1+α) · ln(1/δ∗)
(raw Chernoff)"
REFERENCES,0.4469178082191781,"For α = 0.5 we get τ > 10.6 · ln(1/δ∗). For α = 1 we get τ > 3.26 · ln(1/δ∗). For α = 5 we get
τ > 0.31 · ln(1/δ∗)."
REFERENCES,0.4486301369863014,"Remark B.6 (Mix-and-match TCT). TCT analysis can be extended to the case where we use al-
gorithms with varied privacy guarantees εi and varied qi values.2 In this case the privacy cost
depends on P"
REFERENCES,0.4503424657534247,"i|Ai(D)∈⊤i
εi
qi . The analysis relies on tail bounds on the sum of random variables, is
more complex. Varied ε values means the random variables have different size supports. A simple
coarse bound is according to the largest support, which allows us to use a simple counter for target
hits, but may be lossy with respect to precise bounds. The discussion concerns the (analytical or
numerical) derivation of tail bounds is non-specific to TCT and is tangential to our contribution."
REFERENCES,0.4520547945205479,"B.1
Multi-Target TCT"
REFERENCES,0.4537671232876712,Multi-target charging is described in Algorithm 8. We show the following
REFERENCES,0.4554794520547945,"Lemma B.7 (Privacy of multi-TCT). Algorithm 8 satisfies (ε′, kδ′)-approximate DP bounds, where
(ε′, δ′) are privacy bounds for single-target charging (Algorithm 1)."
REFERENCES,0.4571917808219178,"Specifically, when we expect that one (index) of multiple outcomes ⊥1, . . . , ⊥k will dominate our
interaction but can not specify which one it is in advance, we can use k-TCT with NotPrior targets
with priors ⊥1, . . . , ⊥k. From Lemma B.7, the overall privacy cost depends on the number of times
that the reported output is different than the most dominant outcome. More specifically, for private
testing, when we expect that one type of outcome would dominate the sequence but we do not know
if it is 0 or 1, we can apply 2-TCT. The total number of target hits corresponds to the less dominant
outcome. The total number of privacy charges (on average) is at most (approximately for small ε)
double that, and therefore is always comparable or better to composition (can be vastly lower when
there is a dominant outcome)."
REFERENCES,0.4589041095890411,"Algorithm 8: Multi-Target Charging
Input: Dataset D = {x1, . . . , xn} ∈Xn. Integer τ ≥1 (charging limit). Fraction q ∈[0, 1],
k ≥1 (number of targets).
for i ∈[k] do Ci ←0
// Initialize charge counters
while mini∈[k] Ci < τ do
// Main loop
Receive (A, (⊤i)i∈[k]) where A is an ε-DP mechanism, and ⊤i is a q-target for A
r ←A(D)
Publish r
for i ∈[k] do"
REFERENCES,0.4606164383561644,"if r ∈⊤i then Ci ←Ci + 1
// outcome is in q-target ⊤i"
REFERENCES,0.4623287671232877,"Proof of Lemma B.7 (Privacy of multi-Target TCT). 3Let (ε, δ) be the privacy bounds for Mi that
is single-target TCT with (Ai, ⊤i). Let M be the k-target algorithm. Let ⊤j
i be the ith target in step
j."
REFERENCES,0.464041095890411,"2One of our applications of revise calls to conditional release (see Section D applies TCT with both ε-DP
and 2ε-DP algorithms even for base ε-DP algorithm)
3We note that the claim generally holds for online privacy analysis with the best of multiple methods. We
provide a proof specific to multi-target charging below."
REFERENCES,0.4657534246575342,"We say that an outcome sequence R = (rj)h
j=1 ∈R is valid for i ∈[k] if and only if Mi would halt
with this output sequence, that is, Ph
j=1 1{rj ∈⊤j
i} = τ and rh ∈⊤h
i . We define G(R) ⊂[k] to
be all i ∈[k] for which R is valid."
REFERENCES,0.4674657534246575,"Consider a set of sequences H. Partition H into k + 1 sets Hi so that H0 = {R ∈H | G(R) = ∅}
and Hi may only include R ∈H for which i ∈G(R). That is, H0 contains all sequences that are
not valid for any i and Hi may contain only sequences that are valid for i."
REFERENCES,0.4691780821917808,"Pr[M(D) ∈H] = k
X"
REFERENCES,0.4708904109589041,"i=1
Pr[M(D) ∈Hi] = k
X"
REFERENCES,0.4726027397260274,"i=1
Pr[Mi(D) ∈Hi] ≤ k
X i=1"
REFERENCES,0.4743150684931507," 
eε · Pr[Mi(D′) ∈Hi] + δ

= eε · k
X"
REFERENCES,0.476027397260274,"i=1
Pr[Mi(D′) ∈Hi] + k · δ"
REFERENCES,0.4777397260273973,= eε Pr[M(D′) ∈H] + k · δ.
REFERENCES,0.4794520547945205,"C
Properties of NotPrior targets"
REFERENCES,0.4811643835616438,"Recall that a NotPrior target of an (ε, δ)-DP algorithm is specified by any potential outcome (of
our choice) that we denote by ⊥. The NotPrior target is the set of all outcomes except ⊥. In this
Section we prove (a more general statement of) Lemma 2.3:"
REFERENCES,0.4828767123287671,"Lemma C.1 (Property of a NotPrior target). Let M : X →Y ∪{⊥}, where ⊥̸∈Y, be an
(ε, δ)-DP algorithm. Then the set of outcomes Y constitutes an
1
eε+1-target with (ε, δ) for M."
REFERENCES,0.4845890410958904,We will use the following lemma:
REFERENCES,0.4863013698630137,"Lemma C.2. If two distributions Z0, Z1 with support Y ∪{⊥} satisfy Z0 ≈ε Z1 then Y constitutes
an
1
eε+1-target with (ε, 0) for (Z0, Z1)."
REFERENCES,0.488013698630137,"Proof of Lemma 2.3. From Definition B.2, it suffices to show that for any two neighboring datasets,
D0 and D1, the set Y is an
1
eε+1-target with (ε, δ) for (M(D0), M(D1)) (as in Definition B.1)."
REFERENCES,0.4897260273972603,"Consider two neighboring datasets. We have M(D0) ≈ε,δ M(D1). Using Lemma A.1, for b ∈
{0, 1} we can have
M(Db) = (1 −δ) · Nb + δ · Eb,
(2)"
REFERENCES,0.4914383561643836,"where N0 ≈ε N1. From Lemma C.2, Y is a
1
eε+1-target with (ε, 0) for (N0, N1). From Defini-
tion B.1 and (2), this means that Y is a
1
eε+1-target with (ε, δ) for (M(D0), M(D1))."
REFERENCES,0.4931506849315068,"C.1
Proof of Lemma C.2"
REFERENCES,0.4948630136986301,"We first prove Lemma C.2 for the special case of private testing (when the support is {0, 1}):"
REFERENCES,0.4965753424657534,"Lemma C.3 (target for private testing). Let Z0 and Z1 with support {0, 1} satisfy Z0 ≈ε Z1 Then
⊤= {1} (or ⊤= {0}) is an
1
eε+1-target with (ε, 0) for (Z0, Z1)."
REFERENCES,0.4982876712328767,"Proof. We show that Definition B.1 is satisfied with ⊤= {1}, q =
1
eε+1 and (ε, 0), and Z0, Z1."
REFERENCES,0.5,π = Pr[Z0 ∈⊤]
REFERENCES,0.5017123287671232,π′ = Pr[Z1 ∈⊤]
REFERENCES,0.5034246575342466,"be the probabilities of ⊤outcome in Z0 and Z1 respectively. Assume without loss of generality
(otherwise we switch the roles of Z0 and Z1) that π′ ≥π. If π ≥
1
eε+1, the choice of p = 0 and
Bb = Zb (and any C) trivially satisfies the conditions of Definition 2.1. Generally, (also for all
π <
1
eε+1): • Let"
REFERENCES,0.5051369863013698,p = 1 −π′eε −π
REFERENCES,0.5068493150684932,"eε −1
."
REFERENCES,0.5085616438356164,"Note that since Z0 ≈ε Z1 it follows that π′ ≈ε π and (1 −π′) ≈ε (1 −π) and therefore
p ∈[0, 1] for any applicable 0 ≤π ≤π′ ≤1."
REFERENCES,0.5102739726027398,• Let C be the distribution with point mass on ⊥= {0}.
REFERENCES,0.511986301369863,"• Let B0 = Ber(1 −
π′−π
π′−e−επ) = Ber( π−πe−ε"
REFERENCES,0.5136986301369864,π′−e−επ)
REFERENCES,0.5154109589041096,"• Let B1 = Ber(1 −
π′−π
eεπ′−π) = Ber( eεπ′−π′"
REFERENCES,0.5171232876712328,eεπ′−π )
REFERENCES,0.5188356164383562,"We show that this choice satisfies Definition 2.1 with q =
1
eε+1."
REFERENCES,0.5205479452054794,"• We show that for both b ∈{0, 1}. Zb ≡p · C + (1 −p) · Bb: It suffices to show that the
probability of ⊥is the same for the distributions on both sides. For b = 0, the probability
of ⊥in the right hand side distribution is"
REFERENCES,0.5222602739726028,"p + (1 −p) ·
π′ −π
π′ −e−επ = 1 −π′eε −π"
REFERENCES,0.523972602739726,"eε −1
+ π′eε −π"
REFERENCES,0.5256849315068494,"eε −1
·
π′ −π
π′ −e−επ = 1 −π ."
REFERENCES,0.5273972602739726,"For b = 1, the probability is"
REFERENCES,0.5291095890410958,p + (1 −p) · π′ −π
REFERENCES,0.5308219178082192,eεπ′ −π = 1 −π′eε −π
REFERENCES,0.5325342465753424,"eε −1
+ π′eε −π"
REFERENCES,0.5342465753424658,"eε −1
· π′ −π"
REFERENCES,0.535958904109589,eεπ′ −π
REFERENCES,0.5376712328767124,= 1 −π′eε −π eε −1
REFERENCES,0.5393835616438356,"
1 −π′ −π"
REFERENCES,0.541095890410959,eεπ′ −π 
REFERENCES,0.5428082191780822,= 1 −π′eε −π
REFERENCES,0.5445205479452054,"eε −1
· eεπ′ −π −π′ + π"
REFERENCES,0.5462328767123288,"eεπ′ −π
= 1 −π′ ."
REFERENCES,0.547945205479452,"• We show that for b ∈{0, 1}, Pr[Bb ∈⊤] ≥
1
eε+1."
REFERENCES,0.5496575342465754,Pr[B0 ∈⊤] = π −e−επ
REFERENCES,0.5513698630136986,π′ −e−επ
REFERENCES,0.553082191780822,"≥
π −e−επ
eεπ −e−επ = eε −1"
REFERENCES,0.5547945205479452,"e2ε −1 =
1
eε + 1 ."
REFERENCES,0.5565068493150684,Pr[B1 ∈⊤] = π′(eε −1)
REFERENCES,0.5582191780821918,π′eε −π
REFERENCES,0.559931506849315,≥π(eε −1)
REFERENCES,0.5616438356164384,πe2ε −π = eε −1
REFERENCES,0.5633561643835616,"e2ε −1 =
1
eε + 1
Note that the inequalities are tight when π′ = π (and are tighter when π′ is closer to π).
This means that our selected q is the largest possible that satisfies the conditions for the
target being ⊤."
REFERENCES,0.565068493150685,"• We show that B0 and B1 are ε-indistinguishable, that is"
REFERENCES,0.5667808219178082,"Ber(1 −
π′ −π
π′ −e−επ ) ≈ε Ber(1 −π′ −π"
REFERENCES,0.5684931506849316,eεπ′ −π ).
REFERENCES,0.5702054794520548,Recall that Ber(a) ≈ε Ber(b) if and only if a ≈ε b and (1−a) ≈ε (1−b). First note that
REFERENCES,0.571917808219178,"e−ε ·
π′ −π
π′ −e−επ =
π′ −π
eεπ′ −π
Hence
π′ −π
π′ −e−επ ≈ε
π′ −π
eεπ′ −π ."
REFERENCES,0.5736301369863014,It also holds that 1 ≤
REFERENCES,0.5753424657534246,"π−e−επ
π′−e−επ
π′(1−e−ε)"
REFERENCES,0.577054794520548,"π′−e−επ
= π′"
REFERENCES,0.5787671232876712,π ≤eε.
REFERENCES,0.5804794520547946,"Proof of Lemma C.2. The proof is very similar to that of Lemma C.3, with a few additional details
since ⊤= Y can have more than one element (recall that ⊥is a single element)."
REFERENCES,0.5821917808219178,Assume (otherwise we switch roles) that Pr[Z0 = ⊥] ≥Pr[Z1 = ⊥]. Let
REFERENCES,0.583904109589041,π = Pr[Z0 ∈Y]
REFERENCES,0.5856164383561644,π′ = Pr[Z1 ∈Y] .
REFERENCES,0.5873287671232876,Note that π′ ≥π.
REFERENCES,0.589041095890411,"We choose p, C, B0, B1 as follows. Note that when π ≥
1
eε+1, then the choice of p = 0 and
Bb = Zb satisfies the conditions. Generally, • Let"
REFERENCES,0.5907534246575342,p = 1 −π′eε −π
REFERENCES,0.5924657534246576,"eε −1
."
REFERENCES,0.5941780821917808,• Let C be the distribution with point mass on ⊥.
REFERENCES,0.5958904109589042,"• Let B0 be ⊥with probability
π′−π
π′−e−επ and otherwise (with probability
π−πe−ε
π′−e−επ) be Z0"
REFERENCES,0.5976027397260274,conditioned on the outcome being in Y.
REFERENCES,0.5993150684931506,"• Let B1 be ⊥with probability
π′−π
eεπ′−π and otherwise (with probability eεπ′−π′"
REFERENCES,0.601027397260274,"eεπ′−π ) be Z1 con-
ditioned on the outcome being in Y."
REFERENCES,0.6027397260273972,It remains to show that these choices satisfy Definition 2.1:
REFERENCES,0.6044520547945206,"The argument for Pr[Bb ∈Y] ≥
eε−1
e2ε−1 is identical to Lemma C.3 (with Y = ⊤)."
REFERENCES,0.6061643835616438,"We next verify that for b ∈{0, 1}: Zb ≡p · C + (1 −p) · Bb. The argument for the probability of
⊥is identical to Lemma C.3. The argument for y ∈Y follows from the probability of being in Y
being the same and that proportions are maintained."
REFERENCES,0.6078767123287672,"For b = 0, the probability of y ∈Y in the right hand side distribution is"
REFERENCES,0.6095890410958904,(1 −p) · π −πe−ε
REFERENCES,0.6113013698630136,π′ −e−επ · Pr[Z0 = y]
REFERENCES,0.613013698630137,Pr[Z0 ∈Y] = π · Pr[Z0 = y]
REFERENCES,0.6147260273972602,Pr[Z0 ∈Y] = Pr[Z0 = y].
REFERENCES,0.6164383561643836,"For b = 1, the probability of y ∈Y in the right hand side distribution is"
REFERENCES,0.6181506849315068,(1 −p) · eεπ′ −π′
REFERENCES,0.6198630136986302,eεπ′ −π · Pr[Z1 = y]
REFERENCES,0.6215753424657534,Pr[Z1 ∈Y] = π′ · Pr[Z1 = y]
REFERENCES,0.6232876712328768,Pr[Z1 ∈Y]
REFERENCES,0.625,= Pr[Z1 = y].
REFERENCES,0.6267123287671232,"Finally, we verify that B0 and B1 are ε-indistinguishable. Let W ⊂Y. We have"
REFERENCES,0.6284246575342466,Pr[B0 ∈W] = π(1 −e−ε)
REFERENCES,0.6301369863013698,π′ −e−επ · Pr[Z0 ∈W]
REFERENCES,0.6318493150684932,"π
=
eε −1
π′eε −π Pr[Z0 ∈W]"
REFERENCES,0.6335616438356164,Pr[B1 ∈W] = π′(eε −1)
REFERENCES,0.6352739726027398,eεπ′ −π · Pr[Z1 ∈W]
REFERENCES,0.636986301369863,"π′
=
eε −1
π′eε −π Pr[Z1 ∈W] ."
REFERENCES,0.6386986301369864,Therefore
REFERENCES,0.6404109589041096,"Pr[B0 ∈W]
Pr[B1 ∈W] = Pr[Z0 ∈W]"
REFERENCES,0.6421232876712328,Pr[Z1 ∈W]
REFERENCES,0.6438356164383562,"and we use Z0 ≈ε Z1. The case of W = ⊥is identical to the proof of Lemma C.3. The case ⊥∈W
follows."
REFERENCES,0.6455479452054794,"D
Conditional Release with Revisions"
REFERENCES,0.6472602739726028,"In this section we analyze an extension to conditional release that allows for revision calls to be made
with respect to previous computations. This extension was presented in Section 2.3 and described
in Algorithm 2. A conditional release applies a private algorithm A →Y with respect to a subset of
outcomes ⊤⊂Y. It draws y ∼A(D) and returns y if y ∈⊤and ⊥otherwise. Each revise calls
effectively expands the target to ⊤h ∪⊤′, when ⊤h is the prior target and ⊤′ a disjoint extension.
If the (previously) computed result hits the expanded target (y ∈⊤′), the value y is reported and
charged. Otherwise, additional revise calls can be performed. The revise calls can be interleaved
with other TCT computations at any point in the interaction."
REFERENCES,0.648972602739726,"D.1
Preliminaries"
REFERENCES,0.6506849315068494,"For a distribution Z with support Y and W ⊂Y we denote by ZW the distribution with support
W ∪{⊥} where outcomes not in W are “replaced” by ⊥. That is, for y ∈W, Pr[ZW = y] :=
Pr[Z = y] and Pr[ZW = ⊥] := Pr[Z ̸∈W]."
REFERENCES,0.6523972602739726,"For a distribution Z with support Y and W ⊂Y we denote by Z | W the conditional distribution of
Z on W. That is, for y ∈W, Pr[(Z | W) = y] := Pr[Z = y]/ Pr[Z ∈W]."
REFERENCES,0.6541095890410958,"Lemma D.1. If B0 ≈ε,δ B1 then B0
W ≈ε,δ B1
W ."
REFERENCES,0.6558219178082192,"Lemma D.2. Let B0, B1 be probability distributions with support Y such that B0 ≈ε B1. Let
W ⊂Y. Then B0 | W ≈2ε B1 | W."
REFERENCES,0.6575342465753424,"We extend these definitions to a randomized algorithm A, where AW (D) has distribution A(D)W
and (A | W)(D) has distribution A(D) | W. The claims in Lemma D.1 and Lemma D.2 then
transfer to privacy of the algorithms."
REFERENCES,0.6592465753424658,"D.2
Analysis"
REFERENCES,0.660958904109589,"To establish correctness, it remains to show that each ConditionalRelease call with an (ε, δ)-DP
algorithm A can be casted in TCT as a call to an (ε, δ)-DP algorithm with a NotPrior target and
each ReviseCR call cap be casted as a call to an 2ε-DP algorithm with a NotPrior target."
REFERENCES,0.6626712328767124,"Proof of Lemma 2.5. The claim for ConditionalRelease was established in Lemma 2.4: Con-
ditional release ConditionalRelease (A, ⊤) calls the algorithm A⊤with target ⊤.
From
Lemma D.1, A⊤is (ε, δ)-DP when A is (ε, δ)-DP. ⊤constitutes a NotPrior target for A⊤with
respect to prior ⊥."
REFERENCES,0.6643835616438356,"We next consider revision calls as described in Algorithm 2. We first consider the case of a pure-DP
A (δ = 0)."
REFERENCES,0.666095890410959,"When ConditionalRelease publishes ⊥, the internally stored value rh conditioned on published
⊥is a sample from the conditional distribution A(D) | ¬⊤."
REFERENCES,0.6678082191780822,"We will show by induction that this remains true after ReviseCR calls, that is the distribution of
rh conditioned on ⊥being returned in all previous calls is A(D) | ¬⊤h where ⊤h is the current
expanded target."
REFERENCES,0.6695205479452054,"An ReviseCR call with respect to current target ⊤h and extension ⊤′ can be equivalently framed
as drawing r ∼A(D) | ¬⊤h. From Lemma D.2, if A is ε-DP then A | ¬⊤h is 2ε-DP. If r ∈⊤′
we publish it and otherwise we publish ⊥. This is a conditional release computation with respect
to the 2ε-DP algorithm A | ¬⊤h and the target ⊤′. Equivalently, it is a call to the 2ε-DP algorithm
(A | ¬⊤h)⊤′ with a NotPrior target ⊤′."
REFERENCES,0.6712328767123288,"Following the ReviseCR call, the conditional distribution of rh conditioned on ⊥returned in the
previous calls is A(D) | ¬(⊤h ∪⊤′) as claimed. We then update ⊤h ←⊤h ∪⊤′."
REFERENCES,0.672945205479452,"It remains to handle the case δ > 0. We consider ReviseCR calls for the case where A is (ε, δ)-
DP (approximate DP). In this case, we want to show that we charge for the δ value once, only on
the original ConditionalRelease call. We apply the simulation-based analysis in the proof of
Theorem B.4 with two fixed neighboring datasets. Note that this can be viewed as each call being"
REFERENCES,0.6746575342465754,"with a pair of distributions with an appropriate q-target (that in our case is always a NotPrior
target)."
REFERENCES,0.6763698630136986,"The first ConditionalRelease call uses the distributions A(D0) and A(D1). From Lemma A.1
they can be expressed as respective mixtures of pure N0 ≈ε N1 part (with probability 1 −δ) and
non-private parts. The non-private draw is designated failure with probability δ. Effectively, the call
in the simulation is then applied to the pair (N0
⊤, N1
⊤) with target ⊤."
REFERENCES,0.678082191780822,"A followup ReviseCR call is with respect to the previous target ⊤h and target extension ⊤′. The
call is with the distributions (Nb | ¬⊤h)⊤′ that using Lemma D.1 and Lemma D.2 satisfy (N0 |
¬⊤h)⊤′ ≈2ε (N1 | ¬⊤h)⊤′."
REFERENCES,0.6797945205479452,"E
Boundary Wrapper Analysis"
REFERENCES,0.6815068493150684,"In this section we provide details for the boundary wrapper method including proofs of Lemma 2.7
and Lemma 2.8. For instructive reasons, we first consider the special case of private testing and then
outline the extensions to private classification."
REFERENCES,0.6832191780821918,"Algorithm 4 when specialized for tests first computes π(D) = min{Pr[A(D) = 0], 1−Pr[A(D) =
0]}, returns ⊤with probability π/(1 + π) and otherwise (with probability 1/(1 + π)) return A(D).
Overall, we return the less likely outcome with probability π/(1 + π), and the more likely one with
probability (1 −π)/(1 + π)."
REFERENCES,0.684931506849315,"Lemma E.1 (Privacy of wrapped test). If the test is ε-DP then the wrapper test is t(ε)-DP where
t(ε) ≤4 3ε."
REFERENCES,0.6866438356164384,"Proof. Working directly with the definitions, t(ε) is the maximum of"
REFERENCES,0.6883561643835616,"max
π∈(0,1/2)"
REFERENCES,0.690068493150685,"ln
1 −e−επ"
REFERENCES,0.6917808219178082,1 + e−επ · 1 + π 1 −π
REFERENCES,0.6934931506849316, ≤4
REFERENCES,0.6952054794520548,"3ε
(3)"
REFERENCES,0.696917808219178,"max
π∈(0,1/2)"
REFERENCES,0.6986301369863014,"ln

e−επ
1 + e−επ · 1 + π π"
REFERENCES,0.7003424657534246," ≤ε
(4)"
REFERENCES,0.702054794520548,"max
π∈( e−ε"
REFERENCES,0.7037671232876712,"2
,
1
1+eε )"
REFERENCES,0.7054794520547946,"ln

π
1 + π · 2 −eεπ eεπ"
REFERENCES,0.7071917808219178," ≤ε
(5)"
REFERENCES,0.708904109589041,"max
π∈( e−ε"
REFERENCES,0.7106164383561644,"2
,
1
1+eε )"
REFERENCES,0.7123287671232876,"ln
1 −π"
REFERENCES,0.714041095890411,1 + π · 2 −eεπ
REFERENCES,0.7157534246575342,1 −eεπ
REFERENCES,0.7174657534246576, ≤4
REFERENCES,0.7191780821917808,"3ε
(6)"
REFERENCES,0.7208904109589042,"max
π∈( e−ε"
REFERENCES,0.7226027397260274,"2
,
1
1+eε )"
REFERENCES,0.7243150684931506,"ln

π
1 + π · 2 −eεπ"
REFERENCES,0.726027397260274,1 −eεπ
REFERENCES,0.7277397260273972," ≤ε
(7)"
REFERENCES,0.7294520547945206,"Inequality (3) bounds the ratio change in the probably of the larger probability outcome when it
remains the same and (4) the ratio change in the probability of the smaller probability outcome when
it remains the same between the neighboring datasets. When the less probable outcome changes
between the neighboring datasets it suffices to consider the case where the probability of the initially
less likely outcome changes to eεπ > 1/2 so that eεπ < 1 −π, that is the change is from π to eεπ
where π ∈( e−ε"
REFERENCES,0.7311643835616438,"2 ,
1
1+eε ). Inequalities 5 and 6 correspond to this case. The wrapped probabilities of
the ⊤outcome are the same as the less probably outcome in the case that it is the same in the two
databases. Inequality 7 corresponds to the case when there is change."
REFERENCES,0.7328767123287672,We now show that ⊤is a target for the wrapped test.
REFERENCES,0.7345890410958904,"Lemma E.2 (q-value of the boundary target). The outcome ⊤of a boundary wrapper of an ε-DP
test is a
et(ε)−1
2(eε+t(ε)−1)-target."
REFERENCES,0.7363013698630136,"Proof. Consider two neighboring datasets where the same outcome is less likely for both and π ≤
π′. Suppose without loss of generality that 0 is the less likely outcome."
REFERENCES,0.738013698630137,The common distribution (C) has point mass on 1.
REFERENCES,0.7397260273972602,"The distribution B0 is a scaled part of M(D0) that includes all 0 and ⊤outcomes (probability
π/(1 + π) each) and probability of ∆
et(ε)"
REFERENCES,0.7414383561643836,"et(ε)−1 of the 1 outcomes, where ∆=
2π′
1+π′ −
2π
1+π."
REFERENCES,0.7431506849315068,"The distribution B1 is a scaled part of M(D1) that includes all 0 and ⊤outcomes (probability
π′/(1 + π′) each) and probability of ∆
1
et(ε)−1 of the 1 outcomes."
REFERENCES,0.7448630136986302,It is easy to verify that B0 ≈t(ε) B1 and that
REFERENCES,0.7465753424657534,"1 −p =
2π′"
REFERENCES,0.7482876712328768,"1 + π′ + ∆
1
et(ε) −1 =
2π
1 + π + ∆
et(ε)"
REFERENCES,0.75,"et(ε) −1 =
2π′"
REFERENCES,0.7517123287671232,"1 + π′
et(ε)"
REFERENCES,0.7534246575342466,"et(ε) −1 −
2π
1 + π
1
et(ε) −1"
REFERENCES,0.7551369863013698,"=
2
et(ε) −1(et(ε)
π′"
REFERENCES,0.7568493150684932,"1 + π′ −
π
1 + π )"
REFERENCES,0.7585616438356164,"Using
π
1+π ≤
π′
1+π′ and π′ 1+π′"
REFERENCES,0.7602739726027398,"π
1+π ≤eε we obtain q ≥"
REFERENCES,0.761986301369863,"π
1+π
1 −p"
REFERENCES,0.7636986301369864,= et(ε) −1 2
REFERENCES,0.7654109589041096,"1
et(ε) ·
π′
1+π′ · 1+π π
−1 !"
REFERENCES,0.7671232876712328,≥et(ε) −1
REFERENCES,0.7688356164383562,"2
1
et(ε)+ε −1."
REFERENCES,0.7705479452054794,"Extension to Private Classification
To extension from Lemma E.1 to Lemma 2.7 follows by
noting that the same arguments also hold respectively for sets of outcomes and also cover the case
when there is no dominant outcome and when there is a transition between neighboring datasets
from no dominant outcome to a dominant outcome. The extension from Lemma E.1 to Lemma 2.7
is also straightforward by also noting the cases above (that only make the respective ∆smaller), and
allowing C to be empty when there is no dominant outcome."
REFERENCES,0.7722602739726028,"F
Boundary wrapping without a probability oracle"
REFERENCES,0.773972602739726,"We present a boundary-wrapping method that does not assume a probability oracle. This method
accesses the distribution A(D) in a blackbox fashion."
REFERENCES,0.7756849315068494,"At a very high level, we show that one can run an (ε, 0)-DP algorithm A twice and observe both
outcomes. Then, denote by Y the range of the algorithm A. We can show that E = {(y, y′) : y ̸=
y′} ⊆Y × Y is an Ω(1)-target of this procedure. That is, if the analyst observes the same outcome
twice, she learns the outcome “for free”. If the two outcomes are different, the analyst pays O(ε) of
privacy budget, but she will be able to access both outcomes, which is potentially more informative
than a single execution of the algorithm.
Lemma F.1. Suppose A : X ∗→Y is an (ε, 0)-DP algorithm where |Y| < ∞. Denote by A ◦A
the following algorithm: on input D, independently run A twice and publish both outcomes. Define
E := {(y, y′) : y ̸= y′} ⊆Y × Y. Then, A ◦A is a (2ε, 0)-DP algorithm, and E is a f(ε)-target
for A ◦A, where
f(ε) = 1 −
p"
REFERENCES,0.7773972602739726,e2ε/(1 + e2ε).
REFERENCES,0.7791095890410958,"Proof. A ◦A is (2ε, 0)-DP by the basic composition theorem. Next, we verify the second claim."
REFERENCES,0.7808219178082192,"Identify elements of Y as 1, 2, . . . , m = |Y|. Let D, D′ be two adjacent data sets. For each i ∈[m],
let
pi = Pr[A(D) = i],
p′
i = Pr[A(D′) = i]."
REFERENCES,0.7825342465753424,"We define a distribution C. For each i ∈[m], define qi to be the largest real such that"
REFERENCES,0.7842465753424658,"p2
i −qi ∈[e−2ε(p′
i
2 −qi), e2ε(p′
i
2 −qi)]."
REFERENCES,0.785958904109589,"Then, we define C to be a distribution over {(i, i) : i ∈[m]} where Pr[C = (i, i)] =
qi
P"
REFERENCES,0.7876712328767124,j qj .
REFERENCES,0.7893835616438356,"We can then write (A ◦A)(D) = α · C + (1 −α) · N0 and (A ◦A)(D′) = α · C + (1 −α) · N1,
where α = P"
REFERENCES,0.791095890410959,"i qi, and N0 and N1 are 2ε-indistinguishable."
REFERENCES,0.7928082191780822,"Next, we consider lower-bounding Pr[N0 = (y, y′) : y ̸= y′]. The lower bound of Pr[N0 =
(y, y′) : y ̸= y′] will follow from the same argument."
REFERENCES,0.7945205479452054,"Indeed, we have
Pr[N0 = (y, y′) : y ̸= y′]"
REFERENCES,0.7962328767123288,"Pr[N0 = (y, y)]
=
P"
REFERENCES,0.797945205479452,"i pi(1 −pi)
P"
REFERENCES,0.7996575342465754,"i p2
i −qi
."
REFERENCES,0.8013698630136986,"We claim that
p2
i −qi ≤1 −p′
i
2."
REFERENCES,0.803082191780822,"The inequality is trivially true if p2
i ≤1−p′
i
2. Otherwise, we can observe that for q := pi2+p′
i
2−1 >
0, we have pi2 −q = 1 −p′
i
2 and p′
i
2 −q = 1 −pi2. Since 1 −p′
i
2 ∈[e−2ε(1 −p2
i ), e2ε(1 −p2
i )],
this implies that qi can only be larger than q."
REFERENCES,0.8047945205479452,"Since we also trivially have that p2
i −qi ≤pi2, we conclude that"
REFERENCES,0.8065068493150684,"Pr[N0 = (y, y′) : y ̸= y′]"
REFERENCES,0.8082191780821918,"Pr[N0 = (y, y)]
≥
P
i pi(1 −pi)
P"
REFERENCES,0.809931506849315,"i min(p2
i , 1 −p′
i
2)
≥
P
i pi(1 −pi)
P"
REFERENCES,0.8116438356164384,"i min(p2
i , e2ε(1 −p2
i ))."
REFERENCES,0.8133561643835616,"Next, it is straightforward to show that, for every p ∈[0, 1], one has"
REFERENCES,0.815068493150685,"p(1 −p)
min(p2, e2ε(1 −p2)) = min
1 −p"
REFERENCES,0.8167808219178082,"p
,
p
e2ε(1 + p)"
REFERENCES,0.8184931506849316,"
≥1 −
p"
REFERENCES,0.8202054794520548,"e2ε/(1 + e2ε)
p"
REFERENCES,0.821917808219178,"e2ε/(1 + e2ε)
."
REFERENCES,0.8236301369863014,"Consequently,"
REFERENCES,0.8253424657534246,"Pr[N0 = (y, y′) : y ̸= y′] =
Pr[N0 = (y, y′) : y ̸= y′]
Pr[N0 = (y, y′) : y ̸= y′] + Pr[N0 = (y, y)] ≥1−
p"
REFERENCES,0.827054794520548,"e2ε/(1 + e2ε),"
REFERENCES,0.8287671232876712,as desired.
REFERENCES,0.8304794520547946,"Remark F.2. For a typical use case where ϵ = 0.1, we have f(ε) ≈0.258. Then, by applying
Theorem B.4, on average we pay ≈8ε privacy cost for each target hit. Improving the constant of 8
is a natural question for future research. We also note that while the overhead is more significant
compared to the boundary wrapper of Algorithm 4, the output is more informative as it includes two
independent responses of the core algorithm whereas Algorithm 4 returns one or none (when ⊤is
returned). We expect that it is possible to design less-informative boundary wrappers for the case of
blackbox access (no probability oracle) that have a lower overhead. We leave this as an interestion
question for followup work."
REFERENCES,0.8321917808219178,"G
q value for BetweenThresholds"
REFERENCES,0.833904109589041,"We
provide
details
for
the
BetweenThresholds
classifier
(see
Section
2.6).
The
BetweenThresholds classifier is a refinement of AboveThreshold.
It is specified by a 1-
Lipschitz function f, two thresholds tℓ< tr, and a privacy parameter ε. We compute ˜f(D) =
f(D) + Lap(1/ε), where Lap is the Laplace distribution. If ˜f(D) < tℓwe return L. If ˜f(D) > tr
we return H. Otherwise, we return ⊤.
Lemma G.1 (Effectiveness of the “between” target). The ⊤outcome is an (1 −e−(tr−tl)ε) · eε−1"
REFERENCES,0.8356164383561644,"e2ε−1-
target for BetweenThresholds."
REFERENCES,0.8373287671232876,Proof. Without loss of generality we assume that tℓ= 0 and tr = t/ε for a parameter t.
REFERENCES,0.839041095890411,"Consider two neighboring data sets D0 and D1 and the respective f(D0) and f(D1). Since f is
1-Lipschitz, we can assume without loss of generality (otherwise we switch the roles of the two data"
REFERENCES,0.8407534246575342,"sets) that f(D0) ≤f(D1) ≤f(D0) + 1. Consider the case f(D1) ≤0. The case f(D0) ≥t/ε is
symmetric and the cases where one or both of f(Db) are in (0, t/ε) make ⊥a more effective target."
REFERENCES,0.8424657534246576,"πb
L := Pr[f(Db) + Lap(1/ε) < tℓ= 0] = 1 −1"
REFERENCES,0.8441780821917808,2e−|f(Db)|ε
REFERENCES,0.8458904109589042,"πb
H := Pr[f(Db) + Lap(1/ε) > tr = t/ε] = 1"
REFERENCES,0.8476027397260274,2e−(|f(Db)|ε−t
REFERENCES,0.8493150684931506,"πb
⊤:= Pr[f(Db) + Lap(1/ε) ∈(0, t/ε)] = 1 2"
REFERENCES,0.851027397260274,"
e−|f(Db)|ε −e−(|f(Db)|ε−t
= 1"
REFERENCES,0.8527397260273972,2e−|f(Db)|ε(1 −e−t)
REFERENCES,0.8544520547945206,"Note that π0
L ≈ε π1
L and π1
H ≈ε π0
H, π0
L ≥π1
L and π1
H ≥π0
H
We set
p = (π1
L −
1
eε −1(π0
L −π1
L)) + (π0
H −
1
eε −1(π1
H −π0
H))"
REFERENCES,0.8561643835616438,"and the distribution C to be L with probability (π1
L −
1
eε−1(π0
L −π1
L))/p and H otherwise."
REFERENCES,0.8578767123287672,"We specify p and the distributions Bb and C as we did for NotPrior (Lemma 2.3) with respect to
“prior” L. (We can do that and cover also the case where f(D0) > t/ε where the symmetric prior
would be H because the target does not depend on the values being below or above the threshold)."
REFERENCES,0.8595890410958904,"The only difference is that our target is smaller, and includes only ⊤rather than ⊤and H. Because
of that, the calculated q value is reduced by a factor of"
REFERENCES,0.8613013698630136,"πb
⊤
πb
⊤+ πb
H
="
REFERENCES,0.863013698630137,"1
2e−|f(Db)|ε(1 −e−t)"
REFERENCES,0.8647260273972602,"1
2e−|f(Db)|ε
= (1 −e−t) ."
REFERENCES,0.8664383561643836,"H
Numerical Comparison of BetweenThresholds"
REFERENCES,0.8681506849315068,"We demonstrate numerically the benefits of our TCT-based BetweenThresholds (Section 2.6)
compared with the SVT-based prior work of [3]."
REFERENCES,0.8698630136986302,"Consider a sensitive dataset D = {x1, . . . , xn} of size n and thresholds tl < tr. We process a
sequence of queries where each is specified by a function f : X →[0, 1]. We aim to process
each query by performing a noisy comparison of 1"
REFERENCES,0.8715753424657534,"n
Pn
i=1 f(xi) to tl and tr with the goal of having
a error of at most α on our responses to queries with confidence at least β = 1 −1/n. That
is, for below responses it holds that 1"
REFERENCES,0.8732876712328768,"n
Pn
i=1 f(xi) ≤tl + α, for above responses it holds that
1
n
Pn
i=1 f(xi) ≥tr −α, and for between responses it holds that tl −α ≤1"
REFERENCES,0.875,"n
Pn
i=1 f(xi) ≤tr + α."
REFERENCES,0.8767123287671232,"We then consider how many ⊤(that is, between) responses we can incur under the requirement that
the whole sequence is (ε, δ)-DP with ε ≤1 and δ = 1/n."
REFERENCES,0.8784246575342466,"TCT: For a query f, we compare the noisy count 1"
REFERENCES,0.8801369863013698,"n
Pn
i=1 f(xi) + Lap(
α
log(n)) with the thresholds
tl, tr and report below, between (⊤), or above accordingly. To achieve the desired accuracy and
confidence of α and β = 1 −1/n, we set the Laplace noise parameter to
α
log(n). This ensures that
with confidence at least (1 −1"
REFERENCES,0.8818493150684932,"n), the noisy count is within relative error α of the true count. The
sensitivity is 1/n and each call is ε′-DP with ε′ = log n αn ."
REFERENCES,0.8835616438356164,For tr −tl ≥2
REFERENCES,0.8852739726027398,"ε′ , each call has a q-target with q = 1−e−2"
REFERENCES,0.886986301369863,"1+eε′ . We can then compute the overall privacy
cost for a number of target hits using basic or advanced composition according to Theorem B.4
using the appropriate q values and δ. We solve for the number of target hits that keeps the overall
privacy parameter values below (1, 1/n)."
REFERENCES,0.8886986301369864,"SVT-based [3]: Through the analysis in [3], we obtain that we need ε′ ≥16·log(n)"
REFERENCES,0.8904109589041096,"αn
. An “optimistic”
analysis that we performed implied that we can not get better than ε′ ≥4·log(n) αn
."
REFERENCES,0.8921232876712328,"Figure 1 shows the dependence of the number of between responses (y axis) on the number of data
points n (x axis) for TCT and for the provided and optimistic bounds for [3]. TCT has a factor of
95 gain with respect to the provided bound and a factor of 6 gain over the optimistic bound."
REFERENCES,0.8938356164383562,"100000
200000
300000
400000
500000
600000
Number of Data Points 101 102 103"
REFERENCES,0.8955479452054794,Number of Queries
REFERENCES,0.8972602739726028,Comparison for BetweenThreshold
REFERENCES,0.898972602739726,"TCT
BSU - Optimistic
BSU - Original"
REFERENCES,0.9006849315068494,"Figure 1: Number of BetweenThresholds target hits for data set size. δ = β = 1/n, α = 0.01"
REFERENCES,0.9023972602739726,"Finally, the gains reported in the figure assume that both algorithms use the same gap that is large
enough to satisfy the requirement of [3] that |tr −tl| >= 12(log(1/ε′)+log(1/δ))"
REFERENCES,0.9041095890410958,"ε′
. But in situations
where we are interested in a narrow gap around a single threshold, TCT with the smaller effective
gap also benefits from much fewer target hits."
REFERENCES,0.9058219178082192,"I
Analysis of SVT with individual privacy charging"
REFERENCES,0.9075342465753424,Our improved SVT with individual charging is described in Algorithm 5 (see Section 2.8).
REFERENCES,0.9092465753424658,We establish the privacy guarantee:
REFERENCES,0.910958904109589,"Proof of Theorem 2.9. We apply simulation-based privacy analysis (see Section A.5). Consider two
neighboring datasets D and D′ = D ∪{x}. The only queries where potentially f(D) ̸= f(D′) and
we may need to call the data holder are those with f(x) ̸= 0. Note that for every x′ ∈D, the counter
Cx′ is the same during the execution of Algorithm 5 on either D or D′. This is because the update of
Cx′ depends only on the published results and fi(x′), both of which are public information. Hence,
we can think of the processing of Cx′ as a post-processing when we analyze the privacy property
between D and D′."
REFERENCES,0.9126712328767124,"After x is removed, the response on D and D′ is the same, and the data holder does not need to be
called. Before x is removed from D′, we need to consider the queries such that f(x) ̸= 0 while
Cx < τ. Note that this is equivalent to a sequence of AboveThreshold tests to linear queries, we
apply TCT analysis with ConditionalRelease applied with above threshold responses. The claim
follows from Theorem B.4."
REFERENCES,0.9143835616438356,"We also add that Algorithm 5 can be implemented with BetweenThresholds test (see Section 2.6),
the extension is straightforward with the respective privacy bounds following from Lemma G.1 (q
value for target hit)."
REFERENCES,0.916095890410959,"J
Private Selection"
REFERENCES,0.9178082191780822,"In this section we provide proofs and additional details for private selection in TCT (Sections 2.4
and 2.4.1). Let A1, . . . , Am be of m private algorithms that return results with quality scores. The
private selection task asks us to select the best algorithm from the m candidates. The one-shot
selection described in Algorithm 3 (with k = 1) runs each algorithm once and returns the response
with highest quality."
REFERENCES,0.9195205479452054,"It is shown in [21] that if each Ai is (ε, 0)-DP then the one-shot selection algorithm degrades the
privacy bound to (mε, 0)-DP. However, if we relax the requirement to approximate DP, we can
show that one-shot selection is (O(log(1/δ)ε), δ)-DP, which is independent of m (the number of
candidates). Moreover, in light of a lower-bound example by [21], Theorem J.1 is tight up to constant
factors."
REFERENCES,0.9212328767123288,"Formally, our theorem can be stated as
Theorem J.1. Suppose ε < 1. Let A1, . . . , Am : Xn →Y × R be a list of (ε, δi)-DP al-
gorithms, where the output of Ai consists of a solution y ∈Y and a score s ∈R. Denote by
Best(A1, . . . , Am) the following algorithm (Algorithm 3 with k = 1): run each A1, . . . , Am once,
get m results (y1, s1), . . . , (ym, sm), and output (yi∗, si∗) where i∗= arg maxi si."
REFERENCES,0.922945205479452,"Then, for every δ ∈(0, 1), Best(A1, . . . , Am) satisfies (ε′, δ′)-DP where ε′ = O(ε log(1/δ)), δ′ =
δ + P i δi."
REFERENCES,0.9246575342465754,"Proof. Discrete scores. We start by considering the case that the output scores from A1, . . . , Am
always lie in a finite set X ⊆R. The case with continuous scores can be analyzed by a discretization
argument."
REFERENCES,0.9263698630136986,"Fix D0, D1 to be a pair of adjacent data sets. We consider the following implementation of the
vanilla private selection."
REFERENCES,0.928082191780822,"Algorithm 9: Private Selection: A Simulation
Input: Private data set D. The set X defined above.
for i = 1, . . . , m do"
REFERENCES,0.9297945205479452,"(yi, si) ←Ai(D)
for ˆs ∈X in the decreasing order do"
REFERENCES,0.9315068493150684,"for i = 1, . . . , m do"
REFERENCES,0.9332191780821918,if si ≥ˆs then
REFERENCES,0.934931506849315,"return (yi, si)"
REFERENCES,0.9366438356164384,"Assuming the score of Ai(D) always lies in the set X, it is easy to see that Algorithm 9 simulates the
top-1 one-shot selection algorithm (Algorithm 3 with k = 1) perfectly. Namely, Algorithm 9 first
runs each Ai(D) once and collects m results. Then, the algorithm searches for the lowest ˆs ∈X
such that there is a pair (yi, si) with a score of at least si ≥ˆs. The algorithm then publishes this
score."
REFERENCES,0.9383561643835616,"On the other hand, we note that Algorithm 9 can be implemented by the conditional release with
revisions framework (Algorithm 2). Namely, Algorithm 9 first runs each private algorithm once
and stores all the outcomes. Then the algorithm gradually extends the target set (namely, when the
algorithm is searching for the threshold ˆs, the target set is {(y, s) : s ≥ˆs}), and tries to find an
outcome in the target. Therefore, it follows from Lemma 2.5 and Theorem B.4 that Algorithm 9 is
(O(ε log(1/δ)), δ + P"
REFERENCES,0.940068493150685,i δi)-DP.
REFERENCES,0.9417808219178082,"Continuous scores.
We then consider the case that the distributions of the scores of
A1(D), . . . , AK(D) are continuous over R. We additionally assume that the distribution has no
“point mass”. This is to say, for every i ∈[m] and ˆs ∈R, it holds that"
REFERENCES,0.9434931506849316,"lim
∆→0
Pr
(yi,si)∼Ai(D)[ˆs −∆≤s ≤ˆs + ∆] = 0."
REFERENCES,0.9452054794520548,"This assumption is without loss of generality because we can always add a tiny perturbation to the
original output score of Ai(D)."
REFERENCES,0.946917808219178,"Fix D, D′ as two neighboring data sets. We show that the vanilla selection algorithm preserves
differential privacy between D and D′."
REFERENCES,0.9486301369863014,Let η > 0 be an arbitrarily small real number. Set M = 10·m4
REFERENCES,0.9503424657534246,"η
. For each ℓ∈[1, M], let qℓ∈R be
the unique real such that"
REFERENCES,0.952054794520548,"Pr
i∼[m],(yi,si)∼Ai(D)[si ≥qℓ] =
ℓ
M + 1."
REFERENCES,0.9537671232876712,"Similarly we define q′
ℓwith respect to Ai(D′). Let X = {qℓ, q′
ℓ}."
REFERENCES,0.9554794520547946,"Now, consider running Algorithm 9 with the set X and candidate algorithms A1, . . . , AK on D or
D′. Sort elements of X in the increasing order, which we denote as X = {ˆq1 ≤· · · ≤ˆqm}. After"
REFERENCES,0.9571917808219178,"sampling Ai(D) for each i ∈[m], Algorithm 9 fails to return the best outcome only if one of the
following events happens."
REFERENCES,0.958904109589041,"• The best outcome (y∗, s∗) satisfies that s∗< ˆq1."
REFERENCES,0.9606164383561644,"• There are two outcomes (yi, si) and (yj, sj) such that si, sj ∈[ˆqℓ, ˆqℓ+1) for some ℓ∈[n]."
REFERENCES,0.9623287671232876,"If Item 1 happens, Algorithm 9 does not output anything. If Item 2 happens, then it might be possible
that i < j, si > sj, but Algorithm 9 outputs si."
REFERENCES,0.964041095890411,It is easy to see that Event 1 happens with probability at most m2
REFERENCES,0.9657534246575342,"M
≤η by the construction of
X. Event 2 happens with probability at most M · m4"
REFERENCES,0.9674657534246576,"M 2 ≤η. Therefore, the output distribution of
Algorithm 9 differs from the true best outcome by at most O(η) in the statistical distance. Taking
the limit η →0 completes the proof."
REFERENCES,0.9691780821917808,"Remark J.2. Theorem J.1 shows that there is a factor of log(1/δ) overhead when we run top-1 one-
shot private selection (Algorithm 9) only once. Nevertheless, we observe that if we compose top-1
one-shot selection with other algorithms under the TCT framework (e.g., compose multiple top-1
one-shot selections, generalized private testing, or any other applications mentioned in this paper)),
then on-average we only pay 4ε privacy cost (one NotPrior target hit with a 2ε-DP algorithm) per
top-1 selection (assuming ε is sufficiently small so that eε ≈1). In particular, adaptively performing
c executions of top-1 selection is (ε′, δ)-DP where ε′ = ε · (4
p"
REFERENCES,0.9708904109589042,c log(1/δ) + o(√c)).
REFERENCES,0.9726027397260274,"Liu and Talwar [21] established a lower bound of 2ε on the privacy of a more relaxed top-1 selection
task. Hence, there is a factor of 2 gap between this lower bound and our privacy analysis. Note that
for the simpler task of one-shot above threshold score (discussed in Section 2.4.1), where the goal
is to return a response that is above the threshold if there is one, can be implemented using a single
target hit on Conditional Release call (without revise) and this matches the lower bound of 2ε. We
therefore suspect that it might be possible to tighten the privacy analysis of top-1 one-shot selection.
We leave it as an interesting question for followup work."
REFERENCES,0.9743150684931506,"J.1
One-Shot Top-k Selection"
REFERENCES,0.976027397260274,"In this section, we prove our results for top-k selection."
REFERENCES,0.9777397260273972,"We consider the natural one-shot algorithm for top-k selection described in Algorithm 3, which (as
mentioned in the introduction) generalizes the results presented in [9, 29], which were tailored for
selecting from 1-Lipschitz functions, using the Exponential Mechanism or the Report-Noise-Max
paradigm."
REFERENCES,0.9794520547945206,"We prove the following privacy theorem for Algorithm 3.
Theorem J.3. Suppose ε < 1. Assume that each Ai is (ε, 0)-DP. Then, for every δ ∈(0, 1),"
REFERENCES,0.9811643835616438,"Algorithm 3 is (ε · O(
q"
REFERENCES,0.9828767123287672,k log( 1
REFERENCES,0.9845890410958904,δ ) + log( 1
REFERENCES,0.9863013698630136,"δ )), δ)-DP."
REFERENCES,0.988013698630137,"Remark J.4. The constant hidden in the big-Oh depends on ε. For the setting that ε is close
to zero so that eε ≈1 and δ ≥2o(k), the privacy bound is roughly (ε′, δ)-DP where ε′ = ε ·
(4
p"
REFERENCES,0.9897260273972602,"k log(1/δ) + o(
√"
REFERENCES,0.9914383561643836,"k)).
Remark J.5. We can take Ai as the Laplace mechanism applied to a 1-Lipschisz quality function fi
(namely, Ai(D) outputs a pair (i, fi(D) + Lap(1/ε)), where i denotes the ID of the i-th candidate,
and fi(D) + Lap(1/ε) is the noisy quality score of Candidate i with respect to the data D). In this
way, Theoerem J.3 recovers the main result of [29]."
REFERENCES,0.9931506849315068,"Moreover, Theorem J.3 improves over [29] from three aspects: Firstly, Theorem J.3 allows us to
report the noisy quality scores of selected candidates for free, while [29] needs to run one addi-
tional round of Laplace mechanism to publish the quality scores. Second, our privacy bound has no
dependence on m, while the bound in the prior work [29] was (O(ε
p"
REFERENCES,0.9948630136986302,"k log(m/δ)), δ)-DP. Lastly,
Theorem J.3 applies more generally to any private-preserving algorithms, instead of the classic
Laplace mechanism."
REFERENCES,0.9965753424657534,"Proof. The proof is similar to that of Theorem J.1. Namely, we run each Ai(D) once and store
all results. Then we maintain a threshold T, which starts with T = ∞. We gradually decrease T,"
REFERENCES,0.9982876712328768,"and use Algorithm 2 (Conditional Release with Revised Calls) to find outcomes with a quality score
larger than T. We keep this process until we identify k largest outcomes. The claimed privacy bound
now follows from Lemma 2.5 and Theorem B.4."
