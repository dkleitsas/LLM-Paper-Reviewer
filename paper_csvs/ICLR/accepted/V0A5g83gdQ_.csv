Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002277904328018223,"Transformers are neural network architectures that achieve remarkable performance
in many areas. However, the core component of Transformers, multi-head self-
attention (MHSA), is mainly derived from heuristics, and the interactions across its
components are not well understood. To address the problem, we Ô¨Årst introduce a
mathematically rigorous and yet intuitive tensor diagram representation of MHSA.
Guided by tensor diagram representations, we propose a novel design, namely
Tunable Transformers (Tuformers), by allowing data-driven weights across heads,
whereas MHSA adopts pre-deÔ¨Åned and Ô¨Åxed weights across heads, as will be
explained in our paper. Tuformers naturally reveal a Ô¨Çexible design space that
a user, depending on the needs, can choose a structure that has either improved
performance (generalization error) or higher model efÔ¨Åciency. Any pre-trained
Transformer can be an initialization of the corresponding Tuformer with trainable
number of heads for efÔ¨Åcient training and Ô¨Åne-tuning. Tuformers universally
outperform Transformers on various tasks across multiple domains under a wide
range of model sizes.1"
INTRODUCTION,0.004555808656036446,"1
INTRODUCTION"
INTRODUCTION,0.00683371298405467,"Transformer models are Ô¨Årst introduced by Vaswani et al. (2017) in the context of sequence modelling.
They have demonstrated impressive results on a wide variety of tasks in many Ô¨Åelds, such as
language model pre-training (Sanh et al., 2019), speech recognition (Moritz et al., 2020), image
classiÔ¨Åcation (Dosovitskiy et al., 2020), and generation (Parmar et al., 2018). The core component of
Transformer models is the multi-head self-attention (MHSA) which is extended from the standard
attention mechanism (Bahdanau et al., 2014). Each attention head in MHSA has a global receptive
Ô¨Åeld, i.e., each token‚Äôs representation is updated by attending to all other tokens, and H attention
heads are computed in parallel and concatenated together."
INTRODUCTION,0.009111617312072893,"The current MHSA design is mainly derived from empirical studies or heuristics, leaving some
unresolved challenges. (1) Lack of solid theoretical understanding. The theory behind MHSA is
only starting to catch up with practice. The role of the components and their interactions in MHSA
are not well understood rigorously from a theoretical perspective, which may lead to inefÔ¨Åciencies in
the design. For example, Michel et al. (2019) show that most attention heads can be removed in the
testing phase without much performance compromise, while Cordonnier et al. (2020) Ô¨Ånd empirical
evidence of redundancy in key/query projections and propose a re-parameterization scheme. These
works focus more on practical solutions, leading to questions about whether theoretical patterns exist
in these designs. (2) The number of heads is not trainable. Intuitively, the heads in multi-heads"
INTRODUCTION,0.011389521640091117,1code available at : https://github.com/umd-huang-lab/tuformer
INTRODUCTION,0.01366742596810934,Published as a conference paper at ICLR 2022
INTRODUCTION,0.015945330296127564,"are expected to capture different context information through each head. However, the number of
heads is Ô¨Åxed in training. Thus, although we could tune the hyper-parameter, an exhaustive search
would be time-consuming or not practical for large-scale tasks. (3) Hard to analyze the expressive
power. Analyzing the expressive power (as will be deÔ¨Åned in DeÔ¨Ånition 3) of a neural network, i.e.,
proving that some architectures are more expressive than others, is a non-trivial and challenging task.
Such analysis has been done in Convolutional Neural Networks (CNNs) (LeCun et al., 1995; Cohen
et al., 2016), Recurrent Neural Networks (RNNs) (Mikolov et al., 2010; Khrulkov et al., 2018), but
no such work exists for interpreting MHSA or guiding the structural design."
INTRODUCTION,0.018223234624145785,"In response to the above challenges, we Ô¨Årst interpret MHSA from a tensor representation perspective
using the intuitive graphical tool, the tensor diagram. Current prevalent descriptions of MHSA use
Ô¨Çow charts to convey high-level intuitions, which could cause ambiguities. Therefore, it is inevitable
to pair those Ô¨Çow charts with mathematical formulas to understand the mechanism precisely. However,
these two separated descriptions create difÔ¨Åculties for interpretations and inspections of the operations
implemented in MHSA. To address this issue, we propose a graphical representation of MHSA,
which is both semantically intuitive and mathematically rigorous. SpeciÔ¨Åcally, we modify and
extend the vanilla tensor diagrams (Penrose, 1971), which conveniently allow for rigorous graphical
representation of multi-linear operations between multi-dimensional arrays (i.e., higher-order tensors)
to represent nonlinear operations."
INTRODUCTION,0.02050113895216401,"Using tensor diagram representation, we project the current design of MHSA into its tensor form,
which renders a holistic view of the weights in MHSA for better interpretation of the information Ô¨Çow
and exchange among the components of MHSA. We then propose a novel data-driven structure,
namely Tunable-Head Self-Attention (THSA), which is a re-parameterization of the weight matrices
to allow learnable weight across heads. Transformers with THSA, named Tunable Transformers
(Tuformers), have several advantages compared against vanilla Transformers: (1) A guaranteed higher
expressive power. We prove that MHSA is a special case of THSA. (2) The number of heads is
trainable. The concept of the number of heads in THSA generalizes to the stable rank of the core
matrix, allowing data-driven implicit training. (3) Tuformers allow initialization from pre-trained
Transformers such as BERT (Devlin et al., 2019) and its variants."
INTRODUCTION,0.022779043280182234,"More importantly, we formulate a Ô¨Çexible design space for MHSA where we explore the trade-off
between generalization and efÔ¨Åciency, which can be adjusted by the expressive power if there is no
over-Ô¨Åtting. Users can choose structures with either higher model efÔ¨Åciency or better generalization
that satisfy their own needs. The proposed design space further allows improving the expressive power
of the network by Ô¨Ånding better tensor representations of the weights through tensor representation
theory in future followup works."
INTRODUCTION,0.025056947608200455,"We experiment Tuformers with several tasks across multiple domains, from language modeling, ma-
chine translation to image generation under a wide range of model sizes. We demonstrate competitive
results not only on Tuformers but also in cases where Tuformers are initialized with pre-trained
Transformers for other downstream tasks, when combined with Linear Transformer (Katharopoulos
et al., 2020) on the image generation task and their efÔ¨Åcient variants in the design space."
INTRODUCTION,0.02733485193621868,"Summary of Contributions:
(1) We propose a mathematically rigorous and semantically intuitive tensor diagram representation
of the multi-head self-attention, introducing a new tool to the ML community for future studies on
interpretation and improvements of Transformers.
(2) We propose a novel design of the MHSA, Tunable-Head Self-Attention (THSA), resulting in
Tuformers. Tuformers are structurally more expressive and are showing improved generalization
error experimentally.
(3) We formulate a Ô¨Çexible design space for attention unit design where users can choose to design
structures, depending on the needs, that have better generalization or efÔ¨Åciency."
TENSOR DIAGRAM REPRESENTATION,0.029612756264236904,"2
TENSOR DIAGRAM REPRESENTATION"
TENSOR DIAGRAM REPRESENTATION,0.03189066059225513,"Notations. We use lower case letters (e.g., v) to denote vectors, upper case letters (e.g., M) to
denote matrices, and curly letters (e.g., T ) to denote general tensors. For a tensor T ‚ààRI1√ó¬∑¬∑¬∑IM ,
we refer to the number of indices as order, each individual index as mode, and the length of one mode
as dimension. For instance, T is an M th order tensor that has dimension Im at its mth mode. We
reserve superscripts to distinguish similar arrays (e.g., W Q, W K, W V are query/key/value weight"
TENSOR DIAGRAM REPRESENTATION,0.03416856492027335,Published as a conference paper at ICLR 2022
TENSOR DIAGRAM REPRESENTATION,0.03644646924829157,"matrices), and subscripts to index the elements in an array (e.g., Wij is the (i, j)th element of W ).
We use colon : to slice an array (e.g., Wi,: denotes the ith row of W )."
TENSOR DIAGRAM REPRESENTATION,0.0387243735763098,"We propose to use tensor diagrams (Penrose, 1971), a commonly used rigorous/precise and intuitive
graphical representation for multi-linear operations among higher-order arrays (i.e., tensors), to
represent multi-head self-attention (MHSA). Since MHSA consists of multilinear operations and a
nonlinear softmax function, we will introduce tensor diagrams and our novel design extending tensor
diagrams to denote the composition of nonlinear and multilinear operations in MHSA."
TENSOR DIAGRAM BASICS,0.04100227790432802,"2.1
TENSOR DIAGRAM BASICS v
M
ùì£"
TENSOR DIAGRAM BASICS,0.04328018223234624,"Vector   
order 1 tensor"
TENSOR DIAGRAM BASICS,0.04555808656036447,"Matrix  
order 2 tensor"
DIMENSIONAL ARRAY,0.04783599088838269,3 dimensional array
DIMENSIONAL ARRAY,0.05011389521640091,order 3 tensor
DIMENSIONAL ARRAY,0.05239179954441914,"A
B
A
A
B C"
DIMENSIONAL ARRAY,0.05466970387243736,"Figure 1: Arrays in tensor diagram. A vector is de-
noted as a node with 1 leg, a matrix as a node with 2
legs and an N-dimensional array as a node with N legs."
DIMENSIONAL ARRAY,0.05694760820045558,"Arrays denoted as nodes with legs. An array
is represented as a node with leg(s) in a tensor
diagram as shown in Figure 1. We denote the
order (the number of dimensions) of the array
by the number of legs extending from the node.
Each labeled leg represents one mode of a tensor.
Every mode of the tensor needs to be uniquely
labeled. We usually use the dimension of the
mode as the label (i.e., an associated positive
integer written on top of each leg). The legs
do not need to be straight lines, and their orientations do not matter. Matrices M ‚ààRA√óB and
M ‚ä§‚ààRB√óA can be represented via the same tensor diagram as long as the M node has two legs
(to denote that it is a matrix, wherever the legs extend to), labeled as A and B (to denote its size)."
DIMENSIONAL ARRAY,0.05922551252847381,"A
M
K
B
N
C
M
N
= K
X"
DIMENSIONAL ARRAY,0.06150341685649203,"k=1
AmkBkn = Cmn"
DIMENSIONAL ARRAY,0.06378132118451026,"Œ±
A
M
N
C
M
N
="
DIMENSIONAL ARRAY,0.06605922551252848,"exp(Œ± ¬∑ Amn)
PN
n‚Ä≤=1 exp(Œ± ¬∑ Amn‚Ä≤)
= Cmn"
DIMENSIONAL ARRAY,0.0683371298405467,"C
M
N
="
DIMENSIONAL ARRAY,0.07061503416856492,"R
A
M
B N R"
DIMENSIONAL ARRAY,0.07289293849658314,AmrBnr = Cnmr
DIMENSIONAL ARRAY,0.07517084282460136,"Figure 2: Tensor diagrams for atomic operations. (1) (left) Contraction is an operation that generalizes
the matrix multiplication. It sums element-wise products on a mode in object A and a corresponding mode
(with the same dimension) in object B (i.e., along a leg in node A and a corresponding leg in node B). In the
tensor diagram, multiplying two matrices (or higher-order tensors with more than 2 legs) corresponds to ‚Äúgluing‚Äù
their corresponding legs (on a certain mode). (2) (middle) Softmax is an element-wise exponential function
normalized along a certain mode. We propose to denote the Œ±-scaled softmax function softmax(Œ±A) on A as a
dotted box with a labeled Ô¨Ålled ball (to distinguish itself from tensor objects, i.e., nodes which are blank circles)
attached to one leg. (3) (right) Batch multiplication is an element-wise product along the connected legs."
DIMENSIONAL ARRAY,0.0774487471526196,"Operations in tensor diagrams. There are three types of operations in the calculation of MHSA:,
contraction, softmax, and batch multiplication, as shown in Figure 2 and explained in its caption."
DIMENSIONAL ARRAY,0.07972665148063782,"Evaluation of tensor diagrams: (1) Evaluation order. We can evaluate a tensor diagram in any
pair-wise order except for the nodes in the softmax box. Since the softmax function is nonlinear, we
must Ô¨Årst evaluate the nodes in the softmax box with arbitrary order before those outside the box.
(2) Reading a tensor diagram. We can easily identify the output shape by the dangling edges. For
instant, in Section 2.1, there are three dangling legs M, N, R. Thus the output is a 3rd order tensor
with dimensions M, N, R. Note that the softmax function does not change the shape of the tensor."
DIMENSIONAL ARRAY,0.08200455580865604,"Advantages of tensor diagrams: (1) Tensor diagram is orientation invariant, meaning that we can
represent X and X‚ä§using the same tensor diagram. Thus in multi-head, we obtain a universal
graphical representation regardless of whether we represent an embedding of each token in the
sequence as rows or columns of the input embedding matrix X. (2) Multi-linear operations are
concisely represented and interpreted. (3) The representation is both precise in math and intuitive,
and the information Ô¨Çow is clear, making analysis of network structure more accessible. In addition,
with the labels of the legs, we can read the model complexity explicitly."
DIMENSIONAL ARRAY,0.08428246013667426,We include a comprehensive introduction to tensor diagram representation in Appendix A.
DIMENSIONAL ARRAY,0.08656036446469248,Published as a conference paper at ICLR 2022
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.0888382687927107,"2.2
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.09111617312072894,"The core of Transformer models is a multi-head self-attention (MHSA) module which allows the
model to jointly attend to information from different representation sub-spaces at different posi-
tions (Vaswani et al., 2017). To distinguish inputs for query, key, value matrices, we use XQ ‚ààRN√óF ,
XK, XV ‚ààRM√óF respectively. The MHSA module outputs a matrix M ‚ààRN√óF as
Q[h] = XQW Q
[h]; K[h] = XKW K
[h]; V[h] = XV W V
[h],
(1a)"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.09339407744874716,"head[h] = softmax

Q[h]K‚ä§
[h]
‚àö"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.09567198177676538,"D

V[h],
(1b)"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.0979498861047836,"M =

head[1], head[2], ¬∑ ¬∑ ¬∑ , head[H]

W O.
(1c)"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.10022779043280182,"In the above equations, we see how MHSA computes its output in three steps:"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.10250569476082004,"(1a) Linear transformation on input embedding to get latent features. For each h, the query/key/value
matrices Q[h], K[h], V[h] are linear transformations, parameterized by W Q
[h], W K
[h], and W V
[h] ‚àà
RF √óD respectively, of the input embedding.
(1b) Scaled dot-product attention. Each head[h] ‚ààRN√óD computes a scaled dot-product be-
tween a latent feature matrix Q[h] ‚ààRN√óD (i.e., a query matrix) and another latent feature matrix
K[h] ‚ààRM√óD (i.e., a key matrix) along D, and applies a softmax function to obtain the weights to
be multiplied with a third latent feature matrix V[h] ‚ààRM√óD (i.e., a value matrix).
(1c) Concatenation and contraction for MHSA. After concatenation of the H heads, a linear trans-
formation of the concatenated heads, parameterized by W O ‚ààRHD√óF , is implemented to get the
outcome M of the MHSA as a RN√óF object. N M F
F F 1 D
D D WQ"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.10478359908883828,"[h]
WK [h] WV [h] XQ XV XK"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.1070615034168565,"(a) Single-head Self-Attention M N
F
F 1 D D"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.10933940774487472,"H
Head Contraction"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.11161731207289294,"Latent Contraction F
F
D"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.11389521640091116,Latent Contraction
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.11617312072892938,"XQ
ùí≤Q
ùí≤K
XK"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.11845102505694761,"ùí≤O
ùí≤V
XV"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.12072892938496584,"(b) Multi-Head Self-Attention
Multi-head self-attention in tensor diagram. The concatenation of H matrices {head[h]}H
h=1 can
be viewed as constructing a three dimensional array with the third mode being the number of heads,
i.e., three-legged nodes in tensor diagram notation. This way, the weight matrices {W Q
[h]}H
h=1 are"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.12300683371298406,"stacked to be an order 3 tensor WQ. It is the same with other weight matrices. Since in MHSA,
the attention calculation among key, query and value matrices are done separately within each head,
batch multiplication along mode H is a proper way to denote these ‚Äúhead-wise‚Äù operations. Finally,
we connect the resultant tensor with WO to obtain MHSA as in Figure 3b. See Appendix B for more
details and the proof."
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.1252847380410023,"3
TUFORMERS: DATA-DRIVEN DESIGN WITH HIGHER EXPRESSIVE POWER"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.1275626423690205,"3.1
MOTIVATION: INTERPRETATION OF MHSA"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.12984054669703873,"In MHSA, Q[h] and K[h] can only contribute to head[h], ‚àÄh. In order to characterize the information
Ô¨Çow in MHSA, we stack the weight matrices W Q
[h], ‚àÄh together (similarly for W K
[h] and W V
[h]) and
introduce a matrix C to interpret MHSA. Guided by its tensor diagram representation in Figure 3b,
C is clearly characterized as in Proposition 1."
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.13211845102505695,"Proposition 1 (Interpretation of MHSA).
With weight matrices stacked as W
Q
:=
[W Q
[1], ¬∑ ¬∑ ¬∑ , W Q
[H]], W
K := [W K
[1], ¬∑ ¬∑ ¬∑ , W K
[H]], W
V
:= [W V
[1], ¬∑ ¬∑ ¬∑ , W V
[H]] ‚ààRF √óDH, and a
core matrix C := IH ‚äó(1D1‚ä§
D) ‚ààRDH√óDH ( i.e., a Kronecker product of an all-ones matrix
1D1‚ä§
D ‚ààRD√óD and an identity matrix IH ‚ààRH√óH), MHSA is equivalent to"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.13439635535307518,Published as a conference paper at ICLR 2022
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.1366742596810934,"Q = XQW
Q; K = XKW
K; V = XV W
V ,
(2a)"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.13895216400911162,"headr = softmax DH
X"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.14123006833712984,"s=1
Crs QsK
‚ä§
s
‚àö D !"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.14350797266514806,"V r,
(2b) M =
"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.14578587699316628,"head1, head2, ¬∑ ¬∑ ¬∑ , headDH

W O.
(2c)"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.1480637813211845,"In tensor diagram notation, MHSA is equivalent to Figure 4a. Here, we use Qr, Kr, V r ‚ààRF ,
headr ‚ààRN to denote the rth column of the corresponding matrices, and Crs to denote the (r, s)th
element of C."
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.15034168564920272,"Proposition 1 is proved in Appendix D.1. Note that {head1, ¬∑ ¬∑ ¬∑ , headD} correspond to head[1],
{headD+1, ¬∑ ¬∑ ¬∑ , head2D} correspond to head[2] and so forth. The intuition of interpreting MHSA
using Proposition 1 comes from the tensor diagram graphical representation of MHSA structure.
From the tensor diagram of MHSA as shown in Figure 3b, we see two types of contractions: latent
contractions along D and head contractions along H. The former corresponds to (1D1‚ä§
D) part of C
and the latter corresponds to IH part of C as shown in Figure 4a."
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.15261958997722094,"Motivation for an improved design: from MHSA (a preset C) to THSA (a learnable C). In
MHSA, with the preset speciÔ¨Åc C = IH ‚äó(1D1‚ä§
D), it restricts the possible weight sharing. In
other words, Q[h] and K[h] can only contribute to head[h], ‚àÄh. However, there is no motivation why
such a preset and Ô¨Åxed C is desirable or why we do not allow Q[h] or K[h] to contribute to other
heads. Our motivation for an improved design is simple: rather than presetting the core matrix as
C = IH ‚äó(1D1‚ä§
D), we allow a data-driven learnable C, and we call such a design Tunable-head
self-attention (THSA). Now with the learnable C in THSA, we allow Q[h] and K[h] to contribute to
other heads head[h‚Ä≤], where h‚Ä≤ Ã∏= h. In other words, THSA allows learnable weights across heads. N M F
F F
F 1"
TENSOR DIAGRAM REPRESENTATION OF MULTI-HEAD SELF-ATTENTION,0.1548974943052392,"R
ùí≤Q
ùí≤K ùí≤O
ùí≤V IH XQ XV XK"
D,0.1571753986332574,1D
D,0.15945330296127563,1D 1 D D H H
D,0.16173120728929385,C = Ih ‚äó(1‚ä§ D1D)
D,0.16400911161731208,"(a) Multi-head Self-Attention
(MHSA)
C = IH ‚äó(1‚ä§
D1D) M N
F
F 1 R
R F
F"
D,0.1662870159453303,"C
Fully Trainable WQ
WK WO
WV XQ
XK XV"
D,0.16856492027334852,"(b) Tunable-Head Self-Attention
(THSA)
C fully trainable N M F
F F
F 1"
D,0.17084282460136674,"R
ùí≤Q
ùí≤K ùí≤O
ùí≤V C1 XQ XV XK C2 C3 1 D D H H"
D,0.17312072892938496,C = C1 ‚äó(C‚ä§ 2C3)
D,0.17539863325740318,"(c) C as a design space: an
example.
C = C1 ‚äó(C‚ä§
2 C3)."
D,0.1776765375854214,"Figure 4: MHSA as a special case of THSA. Figure 4a (left) represents MHSA. Figure 4b (middle) represents
THSA. As shown in Proposition 1, weight matrices in MHSA collaborate in a very speciÔ¨Åc way. When
C = IH ‚äó(1‚ä§
D1D) in Proposition 1, THSA degrades to to MHSA. Therefore, it is natural to make C fully
trainable for higher expressive power. THSA also reveals a design space. For example, we can extend MHSA by
allowing trainable components C1, C2 and C3 as in Figure 4c."
D,0.17995444191343962,"3.2
A NOVEL DESIGN: TUNABLE-HEAD SELF-ATTENTION (THSA)"
D,0.18223234624145787,"As shown in Figure 4b, a Tunable-head self-attention (THSA) 2 module has the same input and
output domains as an MHSA, i.e., it takes the input embedding XQ ‚ààRN√óF and XK, XV ‚àà
RM√óF as inputs and returns a matrix T ‚ààRN√óF as output. A THSA, with Ô¨Åve weight matrices
W Q, W K, W V ‚ààRF √óR, W O ‚ààRR√óF , and C ‚ààRR√óR, is mathematically described as"
D,0.1845102505694761,"Q = XQW Q; K = XKW K; V = XV W V ,
(3a)"
D,0.1867881548974943,"‚Ñ¶r = softmax R
X"
D,0.18906605922551253,"s=1
Crs QsK‚ä§
s
‚àö R !"
D,0.19134396355353075,"Vr,
(3b)"
D,0.19362186788154898,"T = ‚Ñ¶W O = [‚Ñ¶1, ‚Ñ¶2, ¬∑ ¬∑ ¬∑ , ‚Ñ¶R] W O.
(3c)"
D,0.1958997722095672,"The intermediate result ‚Ñ¶r ‚ààRN is the rth column of the matrix ‚Ñ¶. In MHSA, the softmax function
is scaled, which normalizes its input by
‚àö"
D,0.19817767653758542,"D, i.e., the square root of the latent dimension. The scaling"
D,0.20045558086560364,"2THSA improves MHSA and only modiÔ¨Åes the intra-layer structure. It does not change the inter-layer
structure outside MHSA."
D,0.20273348519362186,Published as a conference paper at ICLR 2022
D,0.20501138952164008,"ensures that the distribution of the attention matrix approximates a standard Gaussian distribution. To
achieve a similar effect, in THSA, we change the constant to
‚àö"
D,0.2072892938496583,R accordingly.
D,0.20956719817767655,"THSA generalizes MHSA. THSA has learnable C. If C = IH ‚äó(1D1‚ä§
D), i.e., a Kronecker product
of an all-ones matrix 1D1‚ä§
D ‚ààRD√óD and an identity matrix IH ‚ààRH√óH, the THSA reduces to an
MHSA with H heads and latent dimension D, as illustrated in Figure 4a. We call a Transformer with
THSA module as Tunable Transformers (Tuformers). The tensor diagram representation of THSA is
given in Figure 4b. It has an learnable core matrix C compared to MHSA in Figure 4a which has a
preset core matrix."
D,0.21184510250569477,"Tuformers can be initialized with Pre-trained Transformers. Although different in the structure,
Tuformers can always be initialized with pre-trained Transformer models through re-parameterization.
MHSA is updated by weight matrices {W K
[h]}H
h=1, {W Q
[h]}H
h=1, {W V
[h]}H
h=1 and W O while in THSA
we have W Q, W K, W V , W O and C. To initialize THSA with MHSA, we follow the scheme
proposed in Proposition 1. C is the kronecker product between a D √ó D all-one matrix and an
H √ó H identity matrix, where D is the latent dimension and H is the number of heads in MHSA.
W O remains the same as that of MHSA. As for W Q, W K and W V ‚ààRF √óR, we concatenate
{W K
[h]}H
h=1, {W Q
[h]}H
h=1, {W V
[h]}H
h=1 ‚ààRF √óD along mode D."
D,0.214123006833713,"The notion of heads in THSA. Due to the learnable C and data-dependent weight sharing mecha-
nism, the notion of heads in THSA is not as obvious as in MHSA. Although we have an upper bound
R of the number of heads, which is achieved when the learned C decides not to share any weights
across heads, the actual effective number of heads depends on C. As a result, we deÔ¨Åne the number
of heads in THSA as the stable rank of the core matrix C, which is P"
D,0.2164009111617312,"i œÉ2
i

maxi œÉi, where œÉi is the
singular value of C. We use the stable rank because it is largely unaffected by tiny singular values."
D,0.21867881548974943,"THSA as a Ô¨Çexible design space: C in THSA ( Figure 4b ) naturally reveals a design space. In
MHSA, C is completely Ô¨Åxed and in THSA C is fully trainable. We can also have Ô¨Çexible C designs
for user-speciÔ¨Åc needs of either increasing generalization or improving efÔ¨Åciency. N M F
F F
F 1"
D,0.22095671981776766,"R
ùí≤Q
ùí≤K ùí≤O
ùí≤V IH XQ XV XK C2 C3 1 D D H H"
D,0.22323462414578588,C = Ih ‚äó(C‚ä§ 2C3)
D,0.2255125284738041,"(a) skip-left
C = IH ‚äó(C‚ä§
2 C3) N M F
F F
F 1 R C1 XQ XV XK"
D,0.22779043280182232,1D
D,0.23006833712984054,"1D 1 D D H H ùí≤Q
ùí≤K ùí≤O
ùí≤V"
D,0.23234624145785876,C = C1 ‚äó(1‚ä§ D1D)
D,0.23462414578587698,"(b) skip-right
C = C1 ‚äó(1‚ä§
D1D) N M F
F F
F 1"
D,0.23690205011389523,"R
WQ
WK
XQ XV XK H WV
WO C1
H"
D,0.23917995444191345,C = C1
D,0.24145785876993167,"(c) trainable heads-only
C = C1 N M F
F F
F 1"
D,0.2437357630979499,"R
WQ
WK
XQ XV XK H WV
WO"
D,0.2460136674259681,C = Ih
D,0.24829157175398633,"(d) heads-only
C = IH"
D,0.2505694760820046,"Figure 5: C naturally reveals a design space. We can obtain better generalization or better efÔ¨Åciency by allowing
fully trainable C, partially trainable C, pre-set Ô¨Åxed C and etc. We demonstrate some of them extending from
the structure of MHSA where C takes the form of a Kronecker product as shown in Figure 4c. The relationship
between the generalization and efÔ¨Åciency of these designs is summarized in Figure 6."
D,0.2528473804100228,More Generalization
D,0.255125284738041,More Efficiency THSA
D,0.25740318906605925,"skip-left
skip-right"
D,0.25968109339407747,"MHSA
Heads-only
Heads-only -"
D,0.2619589977220957,trainable
D,0.2642369020501139,"Figure 6: We can divide all structures listed
in Figure 4 and Figure 5 into different groups
by their expressive power. The higher group
a structure is in, the higher expressive it has.
Better generalization can be obtained in more
expressive structures. We can also go for very
efÔ¨Åcient ones if we are willing to sacriÔ¨Åce some
generalization. More details in Section 4."
D,0.26651480637813213,"Better generalization. Better generalization can be
obtained by increasing the expressive power if there is
no over-Ô¨Åtting. A naive way to increase the expressive
power is to add more parameters in the weight matrices
in MHSA, such as increasing the latent dimension D or
the number of heads H. We Ô¨Ånd that a more effective
way to increase the expressive power for better gen-
eralization is to Ô¨Ånd more expressive structures. We
demonstrate a few of them in Figure 5 and Figure 4b.
In MHSA, C is pre-set Ô¨Åxed where C = IH ‚äó(1‚ä§
D1D).
In Figure 5a (skip-left C = IH ‚äó(C‚ä§
2 C3) ) and Fig-
ure 5b (skip-right C = IH ‚äó(C‚ä§
2 C3) ), we maintain
the form of a Kronecker product but allow either C1 or
C2, C3 trainable. We can think of skip-left and skip-
right as skip-connection designs in C. In THSA, C is
fully trainable. It is easy to observe an increasing in
expressive power from MHSA to skip-left, skip-right,
and Ô¨Ånally THSA, together with an increasing in generalization in the same order. Our empirical study"
D,0.26879271070615035,Published as a conference paper at ICLR 2022
D,0.27107061503416857,"also show that these structurally more expressive models more effectively improve the generalization
than naively increase the number of parameters."
D,0.2733485193621868,"More EfÔ¨Åciency. We can obtain different levels of efÔ¨Åcient structures in the design space by adjusting
the expressive power. Compared to MHSA, skip-left and skip-right have more generalization in
various tasks but almost the same number of parameters. Being special cases of skip-right, Figure 5c
( trainable heads-only C = C1 ) and Figure 5d ( heads-only C = IH ) are less expressive. However,
trainable heads-only only has around 6% of the parameters compared to MHSA while heads-only
only has 1.5% with most performance maintained."
D,0.275626423690205,"There is a trade-off between generalization and efÔ¨Åciency in the design space C as demonstrated in
Figure 6. We empirically verify that without over-Ô¨Åtting, generalization and efÔ¨Åciency have similar
hierarchy as expressive power. Thus we can design structures in different level of expressive power,
tailoring needs for either better generalization or more efÔ¨Åciency. THSA has the best generalization,
but 125% parameters compared to MHSA. Since skip-left and skip-right are special cases of THSA,
it is natural that their expressive power and generalization are lower. However, they are more efÔ¨Åcient.
With almost the same number of parameters, they have better generalization than MHSA. Heads-only
and trainable heads-only are very efÔ¨Åciency sacriÔ¨Åcing an acceptable amount generalization. More
details can be found in Section 4."
EXPERIMENTS,0.27790432801822323,"4
EXPERIMENTS"
EXPERIMENTS,0.28018223234624146,"Datasets, tasks and evaluation metrics. We evaluate Tuformers on 7 datasets for 5 tasks: word-level
Language Modeling (LM) on Penn Treebank (PTB) (Marcus et al., 1993), Neural Machine Translation
(NMT) on WMT16 ‚ÄòEnglish-German‚Äô (Sennrich et al., 2016), Automatic Speech Recognition (ASR)
on LibriSpeech (Panayotov et al., 2015), Natural Language Inference (NLI) on MNLI (Williams
et al., 2018) and QNLI (Wang et al., 2018) and Image Generation on CIFAR10 (Krizhevsky, 2009)
and MNIST (Deng, 2012) datasets."
EXPERIMENTS,0.2824601366742597,"Baselines. We compare the performance of MHSA module with THSA module in the following 5
backbone architectures: (1) a vanilla Transformer model (Vaswani et al., 2017), (2) an Transformer-
XL model (Dai et al., 2019), (3) a BERT-large model (Devlin et al., 2019), (4) a RoBERTa-large
model (Liu et al., 2019), and (5) an ALBERT model (Lan et al., 2019). In all baselines, we adopt
all default settings used in the original paper (hyperparameters speciÔ¨Åed in Appendix E), and only
replace/re-parameterize the MHSA to THSA."
EXPERIMENTS,0.2847380410022779,"4.1
EMPIRICAL RESULTS OF TUFORMERS."
EXPERIMENTS,0.2870159453302961,"In this part, we Ô¨Årst show that THSA (used in Tuformers) universally outperforms MHSA (used in
Transformers) diverse tasks under different model scales and THSA is simple to use in practice."
EXPERIMENTS,0.28929384965831434,"(1) THSA consistently outperforms MHSA on diverse datasets for a variety of tasks under different
model scales. In LM, NMT and ASR tasks, we compare THSA against the MHSA models under
different model scales. MHSA is parameterized by four weight matrices W Q, W K, W V and W O
whereas THSA adds an additional core matrix C of the same size. Adopting the size of the four
original weight matrices in MHSA to THSA incurs an 25% increase in the number of parameters.
For a fair comparison, we Ô¨Årst compare MHSA against THSA with the same number of parameters
(thus with smaller query/key/value/output weight matrices). To get a full picture, we also compare
MHSA against THSA with the same sized four original weight matrices. As shown in Figure 7,
under the same number of parameters, our THSA (blue curve) universally outperforms MHSA (red
curve) under all model scales. By increasing the number of parameters of the attention units by 25%
3 (i.e., adding the core matrix C while keeping the size of the query/key/value/output weight matrices
the same), our THSA (purple curve) further improves the performance universally under all model
scales."
EXPERIMENTS,0.29157175398633256,(2) Tuformers initialized with pre-trained strong models further improve the performance.
EXPERIMENTS,0.2938496583143508,"In NLI task, we initialize Tuformers with pre-trained BERT-large, RoBERTa-large and ALBERT
models, and perform the same Ô¨Åne-tuning tasks. As shown in Table 1, THSA is compatible with"
EXPERIMENTS,0.296127562642369,"3As will be discussed later, the overall model size is not drastically increased."
EXPERIMENTS,0.2984054669703872,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.30068337129840544,"10%
20%
50%
100%
150%
200% 52 54 56 58 60"
EXPERIMENTS,0.30296127562642367,model scale
EXPERIMENTS,0.3052391799544419,test PPL (the lower the better)
EXPERIMENTS,0.30751708428246016,Language Modeling on Penn Treebank
EXPERIMENTS,0.3097949886104784,"MHSA (same num of param)
THSA (same num of param)
THSA (25% more param)"
EXPERIMENTS,0.3120728929384966,"10%
20%
50%
100%
150%
200% 26 28 30"
EXPERIMENTS,0.3143507972665148,model scale
EXPERIMENTS,0.31662870159453305,BLEU (the higher the better)
EXPERIMENTS,0.31890660592255127,Machine Translation on WMT-16 English-German
EXPERIMENTS,0.3211845102505695,"MHSA (same num of param)
THSA (same num of param)
THSA (25% more param)"
EXPERIMENTS,0.3234624145785877,"10%
20%
50%
100%
150%
200% 4 6 8 10 12"
EXPERIMENTS,0.32574031890660593,model scale
EXPERIMENTS,0.32801822323462415,WER (the lower the better)
EXPERIMENTS,0.33029612756264237,Automatic Speech Recognition on Librispeech(Test Clean)
EXPERIMENTS,0.3325740318906606,"MHSA (same num of parameters)
THSA (same num of parameters)
THSA (25% more parameters)"
EXPERIMENTS,0.3348519362186788,"Figure 7: Performance comparison of THSA and MHSA in (left) language modeling on Penn Treebank
dataset on TransformerXL, (middle) neural machine translation on WMT16 English-German dataset and (right)
automatic speech recognition on Librispeech on Transformers. Under the same number of parameters, our THSA
universally outperforms MHSA under all model sizes. The error bars displayed in shades are generated using 10
random runs in LM and NMT and 3 random runs in ASR.
Table 1: (Left) THSA initialized with pre-trained models. Comparison of accuracies obtained for language
inference task on MNLI and QNLI datasets, the higher the better. Results show that Tuformers can be
conveniently initialized with pre-trained models for downstream tasks, and Ô¨Åne-tuning obtains impressive
performance. (Right) THSA +kernels. Comparison of bits per dimension (BPD) for image generation task on
MNIST and CIFAR-10 datasets, the lower the better. Results show that Tuformers can be extended to image
generation tasks and improve the performance of other efÔ¨Åcient designs with linear computational and memory
complexities."
EXPERIMENTS,0.33712984054669703,"Models
MNLI
QNLI"
EXPERIMENTS,0.33940774487471526,"BERT (Devlin et al., 2019)
84.1%
92.1%"
EXPERIMENTS,0.3416856492027335,"BERT + THSA
84.9%
92.8%"
EXPERIMENTS,0.3439635535307517,"RoBERTa (Liu et al., 2019)
89.4%
94.3%"
EXPERIMENTS,0.3462414578587699,"RoBERTa + THSA
90.2%
94.7%"
EXPERIMENTS,0.34851936218678814,"ALBERT (Lan et al., 2019)
89.1%
97.9%"
EXPERIMENTS,0.35079726651480636,"ALBERT + THSA
89.5%
98.1%"
EXPERIMENTS,0.3530751708428246,"Models
MNIST
CIFAR 10
ELU kernel (Katharopoulos et al., 2020)
0.72
3.51"
EXPERIMENTS,0.3553530751708428,"ELU kernel + THSA
0.65
3.42"
EXPERIMENTS,0.357630979498861,"Polynomial kernel (Tsai et al., 2019)
0.64
3.47"
EXPERIMENTS,0.35990888382687924,"Polynomial kernel THSA
0.57
3.37"
EXPERIMENTS,0.3621867881548975,"existing pre-trained Transformer models and there is no need to train Tuformers from scratch: Ô¨Åne-
tuning THSA initialized from pre-trained BERT and its variants further improves their performance."
EXPERIMENTS,0.36446469248291574,"(3) Combing THSA with kernels to obtain linear computation and memory complexities in sequence
length. Tuformers can also be incorporated into efÔ¨Åcient variants of Transformers for lower computa-
tion and memory complexities. We experiment on image generation tasks on CIFAR10 (Krizhevsky,
2009) and MNIST (Deng, 2012) datasets as in Table 1 and show that Tuformers obtain strong
performance and linear space complexity when combined with kernels."
FLEXIBLE DESIGN SPACE,0.36674259681093396,"4.2
FLEXIBLE DESIGN SPACE"
FLEXIBLE DESIGN SPACE,0.3690205011389522,"(1) THSA reveals a Ô¨Çexible design space C for Transformer. We empirically demonstrate that there
exists a trade-off between generalization and efÔ¨Åciency in this design space, which could be adjusted
by the expressive power. By making C fully-trainable, partially trainable or pre-set Ô¨Åxed as shown in
Figure 5 and Figure 4, we demonstrate that some structures in C improves the generalization while
some other designs improve the efÔ¨Åciency of MHSA."
FLEXIBLE DESIGN SPACE,0.3712984054669704,Table 2: Performance in the Ô¨Çexible design space.
FLEXIBLE DESIGN SPACE,0.3735763097949886,"Models
NMT (BLEU)
# params
Train
Test"
FLEXIBLE DESIGN SPACE,0.37585421412300685,"MHSA
28.9
100%
10.6
3.2"
FLEXIBLE DESIGN SPACE,0.37813211845102507,"MHSA with 125% params
29.0
125%
11.2
3.6"
FLEXIBLE DESIGN SPACE,0.3804100227790433,"MHSA with 1.5% params
20.2
1.5%
6.4
2.5"
FLEXIBLE DESIGN SPACE,0.3826879271070615,"THSA
29.4
125%
11.5
3.9"
FLEXIBLE DESIGN SPACE,0.38496583143507973,"skip-right
C = C1 ‚äó(1‚ä§
D1D)
29.3
101%
10.6
3.2"
FLEXIBLE DESIGN SPACE,0.38724373576309795,"skip-left
C = IH ‚äó(C‚ä§
2 C3)
29.2
100%
10.6
3.2"
FLEXIBLE DESIGN SPACE,0.3895216400911162,"trainable heads-only C = C1
27.4
6%
7.4
2.8"
FLEXIBLE DESIGN SPACE,0.3917995444191344,"heads-only C = IH
27.1
1.5%
6.1
2.3"
FLEXIBLE DESIGN SPACE,0.3940774487471526,"(2) Models with higher expressive power in the de-
sign space have better generalization. As shown in
Table 2 and Figure 7, we compare two ways that in-
crease the expressive power for better generalization:
naively increasing the number of parameters and de-
signing more expressive structures. Both approaches
increase generalization. However, naively increas-
ing the number of parameters is not as effective as
adopting more expressive structures. With the same
number of parameters, more expressive structures,
such as THSA, skip-left and skip-right, universally
outperform MHSA."
FLEXIBLE DESIGN SPACE,0.39635535307517084,Published as a conference paper at ICLR 2022
FLEXIBLE DESIGN SPACE,0.39863325740318906,"(3) Models can also be made very efÔ¨Åcient. Moreover, in Figure 5c ( trainable heads-only C = C1 ),
we trade generalization for efÔ¨Åciency. With 6% parameters, we maintain 95% performance in the
NMT task. In a more extreme case, if we only maintain the head contraction as shown in Figure 5d,
the number of parameters goes down to 1.5% with a 42% faster training and 30% faster testing.
Surprisingly, almost 94% performance is obtained with this light-weighted model."
FLEXIBLE DESIGN SPACE,0.4009111617312073,"4.3
ABLATION STUDY: CHECK-POINTING TO ALLEVIATE MEMORY OVERHEAD WITHOUT
KERNELS."
FLEXIBLE DESIGN SPACE,0.4031890660592255,"Although an effective design with guaranteed higher expressive power, Tuformers come with an extra
memory overhead when calculating values within the softmax. In Tuformers, if done naively, the
memory overhead is MNR where N, M denote the sequence length and R = DH is the rank of the
core tensor, incurring a D times larger memory overhead than vanilla Transformers in calculating
softmax. Combining Tuformers with some state-of-the-art kernel-based efÔ¨Åcient Transformers such
as Linear Transformers using ELU (Katharopoulos et al., 2020) and polynomial (Tsai et al., 2019)
kernels, we reduce the computation and memory complexities of Tuformers to be linear in the
sequence length by removing the nonlinearity constrain in the softmax."
FLEXIBLE DESIGN SPACE,0.4054669703872437,"We can also alleviate the D times larger memory overhead compared with vanilla Transformers
in calculating softmax without using kernels.SpeciÔ¨Åcally, we use check-pointing to eliminate the
memory overhead in THSA. Since no intermediate results are saved in the forward pass using check-
pointing, we recalculate the intermediate results of the softmax box in the back-propagation, which
introduces some computational overhead. We observe that applying check-pointing to Tuformers
leads to only a slight increase in training time while relieving the memory overhead. We can reduce
or even avoid the repetitive calculations by developing a proper backward module, although such
development is beyond the scope of this paper and deferred for future works. See Figure 12 for more
details."
RELATED WORKS,0.40774487471526194,"5
RELATED WORKS"
RELATED WORKS,0.41002277904328016,"Analysis of MHSA. Given the popularity of Transformers in a wide range of domains, a line of
works focuses on understanding and improving MHSA. Voita et al. (2019) proposed a practical
scheme to prune less informative heads. Michel et al. (2019) also showed that most attention heads
are removable in testing without much performance compromise. Cordonnier et al. (2020) found
empirical evidence of redundancy in key/query projections and proposed a re-parameterization
scheme. These works bring valuable insights into MHSA, but a consistent theoretical analysis for
these Ô¨Åndings is missing. Our work differs from these works in that we focus on the theoretical
understanding of MHSA in terms of expressive power, and we provide theoretical guarantees for a
novel design, Tuformers, with higher expressive power and data-driven trainable heads."
RELATED WORKS,0.4123006833712984,"EfÔ¨Åcient Transformers. MHSA has a global receptive Ô¨Åeld, i.e., each token‚Äôs representation is
updated by attending to all other tokens, therefore incurring a quadratic memory and computation
complexities concerning the sequence length. An extensive line of works focusing on reliving such the
dependencies on the sequence length, such as Performer (Choromanski et al., 2020), Reformer (Kitaev
et al., 2020), Linformer (Wang et al., 2020) and Linear Transformer (Katharopoulos et al., 2020).
Our proposed model, Tuformers, can be incorporated into some kernel-based efÔ¨Åcient Transformers,
which applies kernel-based methods to MHSA to remove the nonlinearity constraint brought by the
softmax function, achieving the same state-of-the-art linear computation and memory complexities in
the sequence length."
CONCLUSION,0.4145785876993166,"6
CONCLUSION"
CONCLUSION,0.4168564920273349,"This paper introduces a mathematically rigorous yet intuitive tensor diagram representation of MHSA,
formulates a design space where we can analyze the expressive power of MHSA and its variants and
proposes Tuformers, a novel model design with a guaranteed higher expressive power. Furthermore,
Tuformers have a data-driven structure where heads can be trained implicitly and initialized with
pre-trained Transformer models. The introduction of the tensor diagram representations and the
theory presented in this paper provide new tools and open new directions for future research searching
for expressive and efÔ¨Åcient architectures."
CONCLUSION,0.4191343963553531,Published as a conference paper at ICLR 2022
REFERENCES,0.4214123006833713,REFERENCES
REFERENCES,0.42369020501138954,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014."
REFERENCES,0.42596810933940776,"Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea
Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al.
Rethinking attention with performers. In International Conference on Learning Representations,
2020."
REFERENCES,0.428246013667426,"Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor
analysis. In Conference on learning theory, pp. 698‚Äì728. PMLR, 2016."
REFERENCES,0.4305239179954442,"Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. Multi-head attention: Collaborate
instead of concatenate. arXiv preprint arXiv:2006.16362, 2020."
REFERENCES,0.4328018223234624,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-xl: Attentive language models beyond a Ô¨Åxed-length context. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pp. 2978‚Äì2988, 2019."
REFERENCES,0.43507972665148065,"Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine, 29(6):141‚Äì142, 2012."
REFERENCES,0.43735763097949887,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171‚Äì4186, 2019."
REFERENCES,0.4396355353075171,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. In International Conference
on Learning Representations, 2020."
REFERENCES,0.4419134396355353,"Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran√ßois Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. In International Conference on Machine
Learning, pp. 5156‚Äì5165. PMLR, 2020."
REFERENCES,0.44419134396355353,"Valentin Khrulkov, Alexander Novikov, and Ivan Oseledets. Expressive power of recurrent neural
networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.44646924829157175,"Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efÔ¨Åcient transformer. In
International Conference on Learning Representations, 2020."
REFERENCES,0.44874715261959,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009."
REFERENCES,0.4510250569476082,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942, 2019."
REFERENCES,0.4533029612756264,"Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series. The
handbook of brain theory and neural networks, 3361(10):1995, 1995."
REFERENCES,0.45558086560364464,"Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010."
REFERENCES,0.45785876993166286,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.4601366742596811,"Jerry Ma and Denis Yarats. On the adequacy of untuned warmup for adaptive optimization. In
Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, 2021."
REFERENCES,0.4624145785876993,"Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: the penn treebank. Computational Linguistics, 19(2):313‚Äì330, 1993."
REFERENCES,0.4646924829157175,Published as a conference paper at ICLR 2022
REFERENCES,0.46697038724373574,"Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances in
Neural Information Processing Systems, 32:14014‚Äì14024, 2019."
REFERENCES,0.46924829157175396,"Tomas Mikolov, M. KaraÔ¨Å√°t, L. Burget, J. Cernock√Ω, and S. Khudanpur. Recurrent neural network
based language model. In INTERSPEECH, 2010."
REFERENCES,0.4715261958997722,"Niko Moritz, Takaaki Hori, and Jonathan Le. Streaming automatic speech recognition with the
transformer model. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 6074‚Äì6078. IEEE, 2020."
REFERENCES,0.47380410022779046,"Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguistics
(Demonstrations), pp. 48‚Äì53, 2019."
REFERENCES,0.4760820045558087,"Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus
based on public domain audio books. In 2015 IEEE international conference on acoustics, speech
and signal processing (ICASSP), pp. 5206‚Äì5210. IEEE, 2015."
REFERENCES,0.4783599088838269,"Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In International Conference on Machine Learning, pp. 4055‚Äì4064.
PMLR, 2018."
REFERENCES,0.4806378132118451,"Roger Penrose. Applications of negative dimensional tensors. Combinatorial mathematics and its
applications, 1:221‚Äì244, 1971."
REFERENCES,0.48291571753986334,"Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019."
REFERENCES,0.48519362186788156,"Rico Sennrich, Barry Haddow, and Alexandra Birch. Edinburgh neural machine translation systems
for wmt 16. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared
Task Papers, pp. 371‚Äì376, 2016."
REFERENCES,0.4874715261958998,"Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhut-
dinov. Transformer dissection: An uniÔ¨Åed understanding for transformer‚Äôs attention via the lens of
kernel. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
2019."
REFERENCES,0.489749430523918,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998‚Äì6008, 2017."
REFERENCES,0.4920273348519362,"Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head
self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pp. 5797‚Äì5808, 2019."
REFERENCES,0.49430523917995445,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint
arXiv:1804.07461, 2018."
REFERENCES,0.49658314350797267,"Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768, 2020."
REFERENCES,0.4988610478359909,"Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers), pp. 1112‚Äì1122. Association for Computational Linguistics, 2018.
URL http://aclweb.org/anthology/N18-1101."
REFERENCES,0.5011389521640092,Published as a conference paper at ICLR 2022
REFERENCES,0.5034168564920274,"A
SUPPLEMENTARY MATERIAL FOR TENSOR DIAGRAMS"
REFERENCES,0.5056947608200456,"In this section, we provide a comprehensive introduction to the tensor diagram representation."
REFERENCES,0.5079726651480638,"A.1
COMPONENTS OF TENSOR DIAGRAM: ARRAYS AS NODES WITH LEG(S)"
REFERENCES,0.510250569476082,"Arrays denoted as Nodes with Legs. An array is represented as a node with leg(s) in a tensor
diagram as shown in Figure 1. The order (the number of dimensions) of the array is denoted by the
number of legs extending from the node. The legs do not need to be straight lines ,and the orientation
does not matter. For example, matrices A and A‚ä§are equivalent in this notation. Each labeled leg
represents one mode of a tensor. Every mode of the tensor needs to be uniquely labeled. We usually
use the dimension of the mode as the label (i.e., an associated positive integer written on top of each
leg). If multiple modes of the same tensor have the same length, we use subscripts to differentiate
them."
REFERENCES,0.5125284738041003,"A.2
OPERATIONS IN TENSOR DIAGRAM"
REFERENCES,0.5148063781321185,"A
M
K
B
N
C
M
N
= K
X"
REFERENCES,0.5170842824601367,"k=1
AmkBkn = Cmn"
REFERENCES,0.5193621867881549,"Œ±
A
M
N
C
M
N
="
REFERENCES,0.5216400911161732,"exp(Œ± ¬∑ Amn)
PN
n‚Ä≤=1 exp(Œ± ¬∑ Amn‚Ä≤)
= Cmn"
REFERENCES,0.5239179954441914,"C
M
N
="
REFERENCES,0.5261958997722096,"R
A
M
B N R"
REFERENCES,0.5284738041002278,AnrBmr = Cnmr
REFERENCES,0.530751708428246,"Figure 8: (1) Contraction as shown in Appendix A.2 is an operation that generalizes the matrix multiplication.
It does summation on element-wise products along a mode in object 1 and a corresponding mode (with the same
dimension) in object 2 (i.e., in tensor diagram language, along a leg in node 1 and a corresponding leg in node 2).
In tensor diagram, multiplying two matrices (or higher-order tensors with more than 2 legs) corresponds
to ‚Äúgluing‚Äù their corresponding legs (along a certain mode). (2) Softmax as shown in Appendix A.2 is an
element-wise exponential function normalized along a certain mode. Tensor diagram has a convention to denote
the contraction, but not the softmax. We propose to denote the Œ±-scaled softmax function softmax(Œ±A) on A
as a dotted box with a labeled Ô¨Ålled ball (to distinguish itself from tensor objects, i.e., nodes which are blank
circles) attached to one leg. (3) Batch Multiplication, as shown in appendix A.2, is an elementwise product
along the connected legs."
REFERENCES,0.5330296127562643,"There are three types of operations:contraction, softmax and batch multiplication. Contraction
is an operation that does summation on element-wise products along a mode in object 1 and a
corresponding mode (with the same dimension) in object 2 (i.e., in tensor diagram language, along a
leg in node 1 and a corresponding leg in node 2), whereas softmax is an element-wise exponential
function normalized along a certain mode. Tensor diagram has a convention to denote the contraction,
but not the softmax. We will Ô¨Årst introduce contraction in tensor diagram notation and then propose
our design of softmax in tensor diagram language."
REFERENCES,0.5353075170842825,"Contractions denoted as edges connecting the node legs. In tensor diagram, multiplying two
matrices (or higher-order tensors with more than 2 legs) corresponds to ‚Äúgluing‚Äù their corresponding
legs (along a certain mode) as shown in Appendix A.2, and it is called tensor contraction. Since
the representation of arrays in the tensor diagram is orientation invariant, contractions are also
orientation invariant. This is especially useful in representing the self-attention unit. In some cases,
the data object are represented as row vectors while in some other scenarios they are denoted as
column vectors. If we use mathematical formula, we need to take care of the ordering of the matrix
multiplication because A‚ä§B, AB‚ä§, BA‚ä§, B‚ä§A and more are all different. In contrast, tensor
diagram provides a universal graphical representation as long as the corresponding legs are connected
correctly."
REFERENCES,0.5375854214123007,"Nonlinear activation denoted as dotted box with a labelled Ô¨Ålled ball attached. We propose to
denote the Œ±-scaled softmax function softmax(Œ±A) on A as a dotted box with a labeled Ô¨Ålled ball
(to distinguish itself from tensor objects, i.e., nodes which are blank circles) attached to one leg. The
label right above the Ô¨Ålled ball denote the scaling parameter Œ±, and the leg that the ball is attached to"
REFERENCES,0.5398633257403189,Published as a conference paper at ICLR 2022
REFERENCES,0.5421412300683371,"indicate the mode where normalization is implemented along in the softmax operation. As shown in
Appendix A.2, let A denote the resultant of some operations (for example Q[h]K‚ä§
[h]), the
1
‚àö"
REFERENCES,0.5444191343963554,"D-scaled
softmax on A which normalize along leg N is depicted. Softmax does not change the shape of the
input. That is why we introduce a dotted box, which does not change the shape either, to denote it."
REFERENCES,0.5466970387243736,"Batch Multiplication Given A and B, the batch multiplication is an element-wise product along
the connected legs. The resultant tensor C is obtained by merging connected nodes, maintaining the
dangling legs M, N and R. Since in MHSA, the attention calculation among key, query and value
matrices are done separately within each head, batch multiplication is a proper way to denote these
‚Äúhead-wise‚Äù operations."
REFERENCES,0.5489749430523918,"A.3
ADVANTAGES OF TENSOR DIAGRAM"
REFERENCES,0.55125284738041,"Tensor diagram representations enjoy a few advantages compared with using Ô¨Çow charts together
with math formulas."
REFERENCES,0.5535307517084282,"(1) The notation is orientation invariant. The input embedding matrix X can be represented as
either RN√óF or RF √óN with rows or columns being the embedding of each token in the sequence
respectively. Although we take the former convention in our mathematical notations in Equation B.1,
there are papers that use the latter, creating discrepancies between notations. However, in tensor
diagram, since the node legs are orientation invariant, we obtain a universal graphical representation
of multi-head self-attention, irrespective of how the input embedding matrix is represented, as shown
in Figure 9c."
REFERENCES,0.5558086560364465,"(2) The representation is both precise in math and intuitive, making analysis easier. The tensor
diagram itself is mathematically rigorous."
REFERENCES,0.5580865603644647,"With the labels of the legs, the model complexity (number of parameters) is explicitly displayed,
requiring no supplementary information such as the size of the parameters W Q, W K and W V as in
mathematical formulas. In addition,the resultant from an operation or a sequence of operations (i.e.,
a connected sub-graph of the entire tensor diagram graph) can be treated as ‚Äúmerging‚Äù any connected
nodes in the sub-graph. This is particularly convenient to obtain the size of the resultant: any the
dangling leg becomes a leg of the resultant."
REFERENCES,0.5603644646924829,"(3) Multi-linear operations are concisely represented and interpreted. Using tensor diagram, we
can concisely represent the multi-head self-attention rigorously in one diagram as shown in Figure 9c.
More important, the multi-linear interaction between the latent features Q[h], K[h] and V[h], which
essentially attribute to the multi-linear interaction between the weight tensors W Q, W K and W B,
is clearly illustrated in the tensor diagram. This illustration, which is not achieved by any existing
representations, is crucial for our principled understanding of the multi-head self-attention."
REFERENCES,0.5626423690205011,"(4) The information Ô¨Çow is clear. For example, in Figure 9a there is no direct information pass from
W K
[h], W Q
[h] to W V
[h] while in Figure 9c, such exchange is done through edge Ds and Hs. It is such
observation (there are multiple paths that connect the weight parameter matrices) that leads to our
idea of comparing expressive power between different structures."
REFERENCES,0.5649202733485194,Published as a conference paper at ICLR 2022
REFERENCES,0.5671981776765376,"B
SUPPLEMENTARY MATERIAL FOR MULTI-HEAD SELF-ATTENTION"
REFERENCES,0.5694760820045558,"In the section, we prove the equivalence between the tensor diagram and the equations for multi-head
self-attention (MHSA). To establish such an equivalence, we will need to show that both tensor
diagram and the matrix equations lead to the same result in element-wise notation. N M F
F F 1 D
D D WQ"
REFERENCES,0.571753986332574,"[h]
WK [h] WV [h] XQ XV XK"
REFERENCES,0.5740318906605922,"(a) Single-Head Self-Attention N M F
F F
F 1 D
D H
H H
H ùí≤Q
ùí≤K"
REFERENCES,0.5763097949886105,"ùí≤O
ùí≤V
D XQ XV XK"
REFERENCES,0.5785876993166287,"(b) Single-Head to Multi-Head N M F
F F
F ùí≤Q
ùí≤K ùí≤O
ùí≤V 1 D
D D H XQ XV XK"
REFERENCES,0.5808656036446469,(c) Multi-head self-attention
REFERENCES,0.5831435079726651,"Figure 9: Tensor diagrams from single-head self-attention to multi-head self-attention. Figure 9a is the
tensor diagram representation of a single-head self-attention. Figure 9c is the tensor-diagram representation of a
multi-head self-attention."
REFERENCES,0.5854214123006833,"For convenience of reference, we recap both representations in Equation (B.1) and Figure 9."
REFERENCES,0.5876993166287016,"Q[h] = XQW Q
[h]; K[h] = XKW K
[h]; V[h] = XV W V
[h].
(B.1a)"
REFERENCES,0.5899772209567198,"head[h] = softmax

Q[h]K‚ä§
[h]
‚àö"
REFERENCES,0.592255125284738,"D

V[h],
(B.1b)"
REFERENCES,0.5945330296127562,"M =

head[1], head[2], ¬∑ ¬∑ ¬∑ , head[H]

W O,
(B.1c)"
REFERENCES,0.5968109339407744,"Note that the query weight tensor WQ comes from a concatenation of H query weight matrices
{W Q
[h]}H
h=1 such that WQ
h,:,: = W Q
[h]. Similarly, WK, WV are concatenated from {W K
[h]}H
h=1,"
REFERENCES,0.5990888382687927,"{WV [h]}H
h=1 such that WK
h,:,: = W K
[h], WV
h,:,: = W V
[h]. Furthermore, we denote Q, K, V as the"
REFERENCES,0.6013667425968109,"reshaped tensors from {Q[h]}H
h=1, {K[h]}H
h=1, {V[h]}H
h=1. Finally, WO is reshaped from W O."
REFERENCES,0.6036446469248291,"To differentiate between equations and tensor diagrams in the proof, we add an over-line to every
object in the tensor diagram throughout this section."
REFERENCES,0.6059225512528473,"B.1
SINGLE-HEAD SELF-ATTENTION (SHSA)"
REFERENCES,0.6082004555808656,We will prove that the tensor diagram in Figure 9a is equivalent to Equations (B.1a) and (B.1b).
REFERENCES,0.6104783599088838,"Proof. For simplicity, we omit the subscript [h] in this subsection."
REFERENCES,0.6127562642369021,"We Ô¨Årst show that a contraction of the nodes within the softmax box is equivalent to QK‚ä§. Let
A = QK‚ä§= XQW Q(W KXK)‚ä§, where XQ ‚ààRN√óF , XK ‚ààRM√óF , W Q, W K ‚ààRF √óD.
We use subscripts in the element-wise notation to differentiate edges when multiple edges with same
dimension appear in the same equation. Qnd = F
X"
REFERENCES,0.6150341685649203,"f1=1
XQ
nf1W Q
f1d,
(B.2a) Kmd = F
X"
REFERENCES,0.6173120728929385,"f2=1
XK
mf2W K
f2d,
(B.2b) Anm = D
X"
REFERENCES,0.6195899772209568,"d=1
QndKdm = D
X d=1 F
X f1=1 F
X"
REFERENCES,0.621867881548975,"f2=1
XQ
nf1W Q
f1dW K
f2dXK
mf2.
(B.2c)"
REFERENCES,0.6241457858769932,"Denote be the result of the subset of nodes in the softmax box as A. Anm = D
X d=1 F
X f1=1 F
X f2=1"
REFERENCES,0.6264236902050114,"X
Q
nf1W
Q
f1dW
K
f2dX
K
mf2.
(B.3)"
REFERENCES,0.6287015945330297,Published as a conference paper at ICLR 2022
REFERENCES,0.6309794988610479,"Comparing Equations (B.2c) and (B.3), we have A = A. Let B = softmax(A) and B =
softmax(A), we further have B = softmax(A) = softmax(A) = B."
REFERENCES,0.6332574031890661,"Let M be the result of the tensor diagram and M be the matrix representation of the single-head
attention. We have Mnd = N
X m=1 F
X"
REFERENCES,0.6355353075170843,"f=1
BnmXV
mfW V
fd.
(B.4)"
REFERENCES,0.6378132118451025,"M nd = N
X m=1 F
X f=1"
REFERENCES,0.6400911161731208,"BnmX
V
mfW
V
fd.
(B.5)"
REFERENCES,0.642369020501139,"Comparing Equations (B.4) and (B.5), we have C is equivalent to C, which completes the proof."
REFERENCES,0.6446469248291572,"B.2
MULTI-HEAD SELF-ATTENTION (MHSA)"
REFERENCES,0.6469248291571754,We now prove that tensor diagram in Figure 9c is equivalent to Equation (1c).
REFERENCES,0.6492027334851936,"Proof. Let H be the number of heads. Let A ‚ààRH√óN√óM be the result of a contraction of the nodes
within the softmax box. From the last section, we know that Ah,:,: = A[h] = Q[h]K‚ä§
[h]. Applying
the softmax function B = softmax(A), we have Bh,:,: = softmax(Q[h]K‚ä§
[h])."
REFERENCES,0.6514806378132119,Let T be result of the tensor diagram and T be that of Equation (B.1c).
REFERENCES,0.6537585421412301,"Tnf4 =
X"
REFERENCES,0.6560364464692483,"m,h,d1,d2
softmax(Q[h],nd1Q[h],md1)V[h],md2W O
[h],d2f4.
(B.6)"
REFERENCES,0.6583143507972665,"T n1f4 =
X"
REFERENCES,0.6605922551252847,"m,h,d1,d2
softmax(Qhnd1Khmd1)Vhmd2W
O
hmd2.
(B.7)"
REFERENCES,0.662870159453303,"Comparing T and T , we complete the proof."
REFERENCES,0.6651480637813212,"C
CLOSER INVESTIGATION OF MHSA AND ITS COMPARISON TO THSA"
REFERENCES,0.6674259681093394,"Rethinking MHSA. There are two types contractions in MHSA as shown in Figure 4a: Head-
Contraction, a global contraction among all weights WQ, WK, WV and WO along mode H, and
Latent-Contraction, a local contraction between WQ and WK as well as between WV and WO"
REFERENCES,0.6697038724373576,"both along mode D. If we remove the Head-Contraction and maintain the Latent-Contraction only,
multi-head is reduced to single-head with an additional contraction with the weight matrix WO. To
understand the role of the two types of contractions, we propose ablation studies by removing one
of them and study the changes in the structure and the expressive power. It turns out that in both
situation, MHSA falls into a special case of THSA."
REFERENCES,0.6719817767653758,"Repeated Proposition 1 with two special cases. Given R = DH and other hyperparameters being
the same, THSA module includes MHSA as a special case. SpeciÔ¨Åcally,
(1) If C = 1D1‚ä§
D, i.e., an D √ó D all-ones matrix, the THSA module reduces to an MHSA with a
single head and latent dimension D (i.e., a single-head self-attention with latent dimension D).
(2) If C = IH, i.e., an H √ó H identity matrix, the THSA reduces to a MHSA with H heads and latent
dimension 1 (i.e., a heads-only self-attention with H heads).
(3) If C = IH ‚äó(1D1‚ä§
D), i.e., a Kronecker product of an all-ones matrix 1D1‚ä§
D ‚ààRD√óD and an
identity matrix IH ‚ààRH√óH, the THSA reduces to an MHSA with H heads and latent dimension D."
REFERENCES,0.6742596810933941,The proof of the above propositions is given in Appendix D.
REFERENCES,0.6765375854214123,"DeÔ¨Ånition 2 (Stable Rank). Let A ‚ààRN√óM, the stable rank of A of is deÔ¨Åned as P"
REFERENCES,0.6788154897494305,"i œÉ2
i

maxi œÉi
where œÉi is the singular value of A."
REFERENCES,0.6810933940774487,"Published as a conference paper at ICLR 2022 N M F
F F
F 1 D D
XQ XV XK D 1"
REFERENCES,0.683371298405467,"stable rank = 1 WQ
WK WO
WV"
D,0.6856492027334852,1D
D,0.6879271070615034,"1D N M F
F F
F 1 H WQ
WK WO
WV"
D,0.6902050113895216,stable rank = H IH XQ XV XK
D,0.6924829157175398,"Figure 10: (Left) THSA includes a single-head self-attention. In this case, the stable rank of the core tensor
C is 1. It is interesting to observe that this is a single-head self-attention and this structure can be obtained
by removing the head contraction from Figure 4a. (Right) THSA includes a MHSA with H heads and latent
dimension = 1. In this case, the stable rank equals to the rank of the identical matrix, which is H, corresponding
to the number of heads of MHSA. Also, the structure on the right can be obtained by removing the latent
contraction edges D from Figure 4a. N M F
F F
F 1"
D,0.6947608200455581,"R
ùí≤Q
ùí≤K ùí≤O
ùí≤V IH XQ XV XK"
D,0.6970387243735763,1D
D,0.6993166287015945,1D 1 D D H H
D,0.7015945330296127,C = Ih ‚äó(1‚ä§ D1D)
D,0.7038724373576309,"Figure 11: This Ô¨Ågure shows how to initialize THSA with MHSA. MHSA is a special case of THSA when the
core tensor C takes the Kronecker product of a D √ó D all-one matrix and a H √ó H identity matrix. Note that
in this case, the number of heads H in MHSA is equivalent to the stable rank of C."
D,0.7061503416856492,"The notion of heads in THSA.
The concept of the number of heads in THSA generalizes to the
stable rank of the core matrix, allowing a data-driven implicit training. The number of heads H in
MHSA corresponds to the stable rank of the core matrix C, deÔ¨Åned as P"
D,0.7084282460136674,"i œÉ2
i

maxi œÉi where œÉi is
the singular value. It is a useful surrogate for the rank because it is largely unaffected by tiny singular
values."
D,0.7107061503416856,"As shown in Appendix C when C is constructed by the outer product of two vectors, the stable
rank of C is 1, while in this case, the whole structure is equivalent to a single-head self-attention.
When C is an identical matrix as shown in appendix C, the stable rank of C equals to the rank of the
identical matrix, which is H, corresponding to the number of heads of MHSA. Also, the structure on
the right can be obtained by removing the latent contraction edges D from Figure 4a. When C is
the Kronecker product of the all-one matrix and the identical matrix, which corresponds to MHSA,
the stable rank of C is H, and this number does not change in the training because that C is a Ô¨Åxed
matrix in MHSA."
D,0.7129840546697038,"D
SUPPLEMENTARY MATERIAL FOR TUFORMER AND PROOFS"
D,0.715261958997722,We Ô¨Årst formally describe how to compare the expressive power between two models.
D,0.7175398633257403,"DeÔ¨Ånition 3 (Expressive Power). Suppose we have two function classes F, G with the same source
domain X and target domain Y, i.e., each function f ‚ààF (or g ‚ààG) is a mapping from X to
Y. We say G is more expressive than F if G ‚äáF: for any f ‚ààF, there exists g ‚ààG such that
g(x) = f(x), ‚àÄx ‚ààX. Furthermore, we say G is strictly more expressive than F if G ‚äÉF: besides
G ‚äáF, there exists g ‚ààG such that given any f there exists x ‚ààX and g(x) Ã∏= f(x)."
D,0.7198177676537585,Published as a conference paper at ICLR 2022
D,0.7220956719817767,"According to the deÔ¨Ånition, expressive power is a partial order set. Therefore, expressive power of
different models is usually not comparable as two sets might have overlap and non-overlap regions.
However, when one is a superset or subset of the other, we can unambiguously compare their
expressive power. In the following theorem, we formally show that a Tunable-head self-attention
(THSA) module has a higher expressive than multi-head self-attention (MHSA) if the rank R in
THSA is equal to the product of the number of heads H and the latent dimension D in MHSA."
D,0.724373576309795,"Theorem 4 (THSA is more expressive than MHSA). Given R = DH and other hyper-parameters
being the same, a Tunable-head self-attention (THSA) module with rank R is more expressive than a
multi-head self-attention (MHSA) module with H heads and latent dimension D."
D,0.7266514806378133,"Theorem 4, proved in Appendix D.2, shows that any function realizable by MHSA can be realized by
THSA."
D,0.7289293849658315,"In this section, we will prove Proposition 1 and Theorem 4 in Section 3. In both theorems, we assume
the rank in Tunable-head self-attention (THSA) equals to the product of the number of heads H and
the latent dimension D in multi-head self-attention (MHSA), i.e., R = DH."
D,0.7312072892938497,"For convenience of reference, we recap the mathematical expressions and tensor diagrams for both
multi-head self-attention (MHSA) and Tunable-head self-attention (THSA) in the following."
D,0.7334851936218679,"Multi-head Self-Attention (MHSA). An MHSA module has its learnable parameters as H sets
of (query, key, value) weight matrices {W Q
[h], W K
[h], W V
[h]}H
h=1 (with W Q
[h], W K
[h], W V
[h] ‚ààRF √óD"
D,0.7357630979498861,"for each h) and an output weight matrix W O ‚ààRHD√óF . The module maps three input matrices
XQ ‚ààRN√óF , XK, XV ‚ààRM√óF into an output matrix M ‚ààRN√óF ."
D,0.7380410022779044,"Q[h] = XQW Q
[h]; K[h] = XKW K
[h]; V[h] = XV W V
[h],
(D.1a)"
D,0.7403189066059226,"head[h] = softmax

Q[h]K‚ä§
[h]

V[h],
(D.1b)"
D,0.7425968109339408,"M =

head[1], head[2], ¬∑ ¬∑ ¬∑ , head[H]

W O.
(D.1c)"
D,0.744874715261959,"In Equation (D.1a), Q[h] ‚ààRN√óD, K[h], V[h] ‚ààRM√óD are query, key, value matrices respectively.
In Equation (D.1b), we omit the scaling factor 1/
‚àö"
D,0.7471526195899773,"D for the softmax function (the scalar can be
merged into the learnable parameters), and head[h] ‚ààRN√óD is the resulted matrix for head h."
D,0.7494305239179955,"Tunable-Head Self-Attention. A THSA module is parameterized by Ô¨Åve matrices: a query weight
matrix W
Q ‚ààRF √óR, a key weight matrix W
K ‚ààRF √óR, a value weight matrix W
V ‚ààRF √óR,
an output weight matrix W
O ‚ààRR√óF , and an additional core matrix C ‚ààRR√óR. The module
has the same input and output domains as in MHSA, i.e., it takes three matrices XQ ‚ààRN√óF ,
XK, XV ‚ààRM√óF as inputs and returns a matrix T ‚ààRN√óF as output."
D,0.7517084282460137,"Q = XQW
Q; K = XKW
K; V = XV W
V ,
(D.2a)"
D,0.7539863325740319,"headr = softmax R
X"
D,0.7562642369020501,"s=1
Crs QsK
‚ä§
s !"
D,0.7585421412300684,"V r,
(D.2b)"
D,0.7608200455580866,"T = head W
O =
"
D,0.7630979498861048,"head1, head2, ¬∑ ¬∑ ¬∑ , headR

.
(D.2c)"
D,0.765375854214123,"To distinguish THSA from MHSA, we use over-scored symbols whenever needed. In Equation (D.2a),
Q ‚ààRN√óR, K, V ‚ààRM√óR are query, key, value matrices respectively. In Equation (D.2b), we
again omit the scaling factor 1/
‚àö"
D,0.7676537585421412,"R in the softmax function. Equation (D.2b) leads to a head matrix
head ‚ààRF √óN, where headr ‚ààRF denotes the rth column of the matrix head."
D,0.7699316628701595,Published as a conference paper at ICLR 2022
D,0.7722095671981777,"D.1
PROOF OF PROPOSITION 1"
D,0.7744874715261959,"Proof of Proposition 1. We constructively prove that THSA reduces to three cases of MHSA if the
query/key/value weight matrices in THSA concatenate the corresponding matrices in MHSA:"
D,0.7767653758542141,"W
Q =
h
W Q
[1], W Q
[2], ¬∑ ¬∑ ¬∑ , W Q
[H]
i
,
(D.3a)"
D,0.7790432801822323,"W
K =
h
W K
[1], W K
[2], ¬∑ ¬∑ ¬∑ , W K
[H]
i
,
(D.3b)"
D,0.7813211845102506,"W
V =
h
W V
[1], W V
[2], ¬∑ ¬∑ ¬∑ , W V
[H]
i
.
(D.3c)"
D,0.7835990888382688,"In addition, we set W
O = W O. We have W
Q, W
K, W
V ‚ààRF √óR, and W
O ‚ààRR√óF ."
D,0.785876993166287,"(1) For C = 1R1‚ä§
R, i.e. an R √ó R all-ones matrix, we aim to prove that the THSA module reduces
to a single-head self-attention with H = 1 and D = R. Since there is only one head in the module,
we have W
Q = W Q
[1], W
K = W K
[1], and W
V = W V
[1]. As an immediate result,"
D,0.7881548974943052,"Q = XQW
Q = XQW Q
[1] = Q[1],
(D.4a)"
D,0.7904328018223234,"K = XKW
K= XKW K
[1] = K[1],
(D.4b)"
D,0.7927107061503417,"V = XV W
V = XV W V
[1] = V[1].
(D.4c)"
D,0.7949886104783599,"Since C is an all-ones matrix, i.e., Crs = 1, ‚àÄr, s, we rewrite Equation (D.2b) as:"
D,0.7972665148063781,"headr = softmax R
X s=1"
D,0.7995444191343963,"QsK
‚ä§
s !"
D,0.8018223234624146,"V r = softmax
"
D,0.8041002277904328,"Q K
‚ä§"
D,0.806378132118451,"V r.
(D.5)"
D,0.8086560364464692,"Notice that the equation holds for each column r, we further write all R equations jointly as:"
D,0.8109339407744874,"head = softmax
"
D,0.8132118451025057,"Q K
‚ä§"
D,0.8154897494305239,"V = softmax

Q[1]K‚ä§
[1]

V[1] = head[1].
(D.6)"
D,0.8177676537585421,"Finally, we express the output matrix as:"
D,0.8200455580865603,"T = head W
O = head[1]W O = M,
(D.7)"
D,0.8223234624145785,which concludes the reduction for the special case (1).
D,0.8246013667425968,"(2) For C = IR, an R √ó R identity matrix, we aim to prove that the THSA module reduces to a
heads-only self-attention with H = R and D = 1. Since the latent dimension D = 1, i.e., each W Q
[h]
(or W K
[h], W V
[h]) is a vector, we have W
Q
r = W Q
[r], W
K
r = W K
[r], and W
V
r = W V
[r]. Therefore,"
D,0.826879271070615,"Qr = XQW
Q
r = XQW Q
[r] = Q[r],
(D.8a)"
D,0.8291571753986332,"Kr = XKW
K
r = XKW K
[r] = K[r],
(D.8b)"
D,0.8314350797266514,"V r = XV W
V
r = XV W V
[r] = V[r].
(D.8c)"
D,0.8337129840546698,"Since C is an identity matrix, i.e., Crr = 1 and Crs = 0, ‚àÄr Ã∏= s, we rewrite Equation (D.2b) as:"
D,0.835990888382688,"headr = softmax
"
D,0.8382687927107062,"QrK
‚ä§
r
"
D,0.8405466970387244,"V r = softmax

Q[r]K‚ä§
[r]

V[r] = head[r].
(D.9)"
D,0.8428246013667426,"Finally, we express the output matrix as:"
D,0.8451025056947609,"T = head W
O =

head[1], head[2], ¬∑ ¬∑ ¬∑ ; head[R]

W O = M,
(D.10)"
D,0.8473804100227791,which concludes the reduction for the special case (2).
D,0.8496583143507973,"(3) For C = IH ‚äó(1D1‚ä§
D), i.e., a Kronecker product between an identity matrix IH ‚ààRH√óH and
an all-ones matrix 1D1‚ä§
D ‚ààRD√óD, we aim to prove that the THSA module reduces to a multi-head"
D,0.8519362186788155,Published as a conference paper at ICLR 2022
D,0.8542141230068337,"self-attention (MHSA) module with H heads and latent dimension D. In this general case, we have"
D,0.856492027334852,"Q = XQW
Q = XQ h
W Q
[1], W Q
[2], ¬∑ ¬∑ ¬∑ , W Q
[H]
i
=

Q[1], Q[2], ¬∑ ¬∑ ¬∑ , Q[H]

(D.11)"
D,0.8587699316628702,"K = XKW
K= XK h
W K
[1], W K
[2], ¬∑ ¬∑ ¬∑ , W K
[H]
i
=

K[1], K[2], ¬∑ ¬∑ ¬∑ , K[H]

(D.12)"
D,0.8610478359908884,"V = XV W
V = XV h
W V
[1], W V
[2], ¬∑ ¬∑ ¬∑ , W V
[H]
i
=

V[1], V[2], ¬∑ ¬∑ ¬∑ , V[H]

(D.13)"
D,0.8633257403189066,"Notice that C is a block diagonal matrix blkdiag(1D1‚ä§
D, ¬∑ ¬∑ ¬∑ , 1D1‚ä§
D), where each block is an all-ones
matrix. Therefore we can rewrite Equation (D.2b) for r = (h ‚àí1)D + d as:"
D,0.8656036446469249,"head(h‚àí1)D+d = softmax R
X"
D,0.8678815489749431,"s=1
1(h‚àí1)D<s‚â§hD
"
D,0.8701594533029613,"QsK
‚ä§
s
!"
D,0.8724373576309795,"V r
(D.14)"
D,0.8747152619589977,"= softmax D
X"
D,0.876993166287016,"d=1
Q[h],dK‚ä§
[h],d !"
D,0.8792710706150342,"V[h],d
(D.15)"
D,0.8815489749430524,"= softmax

Q[h]K‚ä§
[h]

V[h],d.
(D.16)"
D,0.8838268792710706,"Since Equation (D.16) holds for each column d, we write all D equations jointly as:
"
D,0.8861047835990888,"head(h‚àí1)D+d, ¬∑ ¬∑ ¬∑ , headhD

= softmax

Q[h]K‚ä§
[h]

V[h] = head[h].
(D.17)"
D,0.8883826879271071,"Again, since Equation (D.17) holds for each block h, we combine all H equations as:"
D,0.8906605922551253,"head =

head[1], head[2], ¬∑ ¬∑ ¬∑ , head[H]

.
(D.18)"
D,0.8929384965831435,"Finally, we express the output matrix as:"
D,0.8952164009111617,"T = head W
O =

head[1], head[2], ¬∑ ¬∑ ¬∑ , head[H]

W O = M,
(D.19)"
D,0.89749430523918,which concludes the reduction for the case (3).
D,0.8997722095671982,"D.2
PROOF OF THEOREM 4"
D,0.9020501138952164,"Proof of Theorem 4. We have shown that THSA reduces to MHSA when C takes speciÔ¨Åc forms.
This shows that THSA is more expressive than MHSA ‚Äî any mapping by an MHSA module can
also be realized by a THSA module."
D,0.9043280182232346,"To prove that THSA is strictly more expressive than MHSA, we need Ô¨Ånd a mapping by a THSA
module that can not be realized by an MHSA module. It sufÔ¨Åces to show that the matrix M is
rank-deÔ¨Åcient, while the matrix T can be full-rank. We assume MH < min(N, F)."
D,0.9066059225512528,"For convenience, we divide the matrix W O for MHSA into H blocks."
D,0.908883826879271,"W O =
h
W O
[1]
‚ä§, W O
[2]
‚ä§, ¬∑ ¬∑ ¬∑ , W O
[H]
i‚ä§
,
(D.20)"
D,0.9111617312072893,"where W O
[h] ‚ààRD√óF is the matrix for the hth head. We now rewrite the output matrix M as: M = H
X"
D,0.9134396355353075,"h=1
softmax

Q[h]K‚ä§
[h]
"
D,0.9157175398633257,"|
{z
}
RN√óM"
D,0.9179954441913439,"
V[h]W O
[h]
"
D,0.9202733485193622,"|
{z
}
RM√óF"
D,0.9225512528473804,".
(D.21)"
D,0.9248291571753986,"Since the matrix inside summation is a product of two matrices of size RN√óM and RM√óF , its rank is
at most M. Use the property rank(A + B) ‚â§rank(A) + rank(B), we have rank(M) ‚â§MH <
min(N, F). Similarly, we divide the matrix W
O for THSA into R rows and rewrite the output
matrix T as: T = R
X"
D,0.9271070615034168,"r=1
softmax R
X"
D,0.929384965831435,"s=1
Crs Q:,rK
‚ä§
:,r !"
D,0.9316628701594533,"|
{z
}
RN√óM "
D,0.9339407744874715,"V :,rW
O
r,:
"
D,0.9362186788154897,"|
{z
}
RM√óF"
D,0.9384965831435079,".
(D.22)"
D,0.9407744874715261,"Using the same argument, we have rank(T ) ‚â§MR. Since H < R (in fact, R = DH), we can
always Ô¨Ånd an example such that rank(T ) > rank(M). This completes the proof."
D,0.9430523917995444,Published as a conference paper at ICLR 2022
D,0.9453302961275627,"E
SUPPLEMENTARY MATERIAL FOR EXPERIMENTS"
D,0.9476082004555809,All the experiments are run on computing nodes with 4 NVIDIA GeForce GPUs.
D,0.9498861047835991,"(1) Language Modeling on Penn Treebank. Language modeling is the task of computing the
probability of a sentence or a sequence of words. The model performance is measured by per-word
Perplexity (PPL) which is the lower the better."
D,0.9521640091116174,"Penn Treebank (Marcus et al., 1993) is under the LDC User Agreement for Non-Members. We adopt
the default settings as in the vanilla Transformer models(Vaswani et al., 2017). SpeciÔ¨Åcally, we have
N (length of the sequence) to be 2048, F (length of the initial embedding) to be 512, 8 number
of heads, 6 layers encoder-decoder structure, D (latent feature dimension) to be 64. We use Adam
optimizer and have 4000 warm-up steps. We also use label smoothing(0.1). The model is trained for
50 epochs."
D,0.9544419134396356,"(2) Neural Machine Translation on WMT16. The goal of NMT is to generate a corresponding
sequence in one language given a sequence in another. The performance is measured by BLEU scores.
(the higher, the better)."
D,0.9567198177676538,"WMT16 English-German dataset (Sennrich et al., 2016) is under the MIT license. In this experiment,
we use a Transformer model with 8 layers and we have F = 512 and R = 1024. The learning rate is
1e‚àí5 and 4000 warmups. We also set label-smooting to be 0.1. To speed up the experiment, we use
the mix-precision trick with an optimization level being O1."
D,0.958997722095672,"(3) Image Generation on MNIST and CIFAR-10. The image generation task is to predict an image
pixel by pixel. The performance is evaluated by bits per dimension (BPD). The image generation task
is chosen to demonstrate, Ô¨Årstly, that Tuformers work beyond the language domain, and secondly,
Tuformers can improve other efÔ¨Åcient designs."
D,0.9612756264236902,"CIFAR10 (Krizhevsky, 2009) dataset is under the MIT license (MIT), MNIST(LeCun et al., 2010)
is under the Creative Commons Attribution-Share Alike 3.0 license. The reason why we choose
to evaluate Tuformer in image generation tasks on these two datasets is that we want to show the
compatibility of our model to other efÔ¨Åcient designs. Katharopoulos et al. (2020) introduces a linear
transformer that has a state-of-the-art complexity. As a result, we evaluate our design on the same
experiments as they do in their paper. We adopt basically the same settings except for GPU resources.
Concretely, we use a 8 layer transformer model with 8 heads. The embedding size F is 256 an D is 32.
N is set to be 1024. We also use ‚ÄúRAdam‚Äù (Ma & Yarats, 2021) optimizer for a state-of-the-art result.
The model is trained for 250 epochs for MNIST dataset and 50 epochs for CIFAR10 dataset. The
batch size is 10 in MNIST training and 4 in CIFAR training. We additionally use he mix-precision
trick with an optimization level set to be O1 to speed up the training."
D,0.9635535307517085,"(4) Automatic Speech Recognition on LibriSpeech Dataset ASR consists of transcribing audio
speech segments into text. The performance is measured by Word Error Rate (WER). (the lower, the
better). We adopt the default settings as in Fairseq (Ott et al., 2019), except that we replace MHSA
with THSA."
D,0.9658314350797267,"(5) Natural Language Inference on MNLI and QNLI. NLI is a task of determining whether the
given ‚Äúhypothesis‚Äù and ‚Äúpremise‚Äù logically follow (entailment) or unfollow (contradiction) or are
undetermined (neutral) to each other. The results are evaluated by accuracy. Using MNLI dataset, the
model tries to predict whether sentence A entails or contradicts B while in QNLI dataset, the model
is trained to answer whether sentence B contains answers to the question in sentence A. We use a
batch size of 32 for 10 epochs, with a learning rate 1e‚àí5."
D,0.9681093394077449,"Memory Overhead in THSA. As mentioned in Section 4.3, naive Tuformers come with an extra
memory overhead when calculating values within the softmax. In Tuformers, if done naively, the
memory overhead is MNR where N, M denote the sequence length and R = DH is the rank of the
core tensor, incurring a D times larger memory overhead than vanilla Transformers in calculating
softmax. We argue that combining Tuformers with kernel-based efÔ¨Åcient variants such as Linear
Transformers (Katharopoulos et al., 2020) and polynomial kernels (Tsai et al., 2019) kernels could not
only cancel the memory overhead but also achieve the same linear memory complexity by removing
the nonlinear constrain in the softmax."
D,0.9703872437357631,Published as a conference paper at ICLR 2022
D,0.9726651480637813,"Furthermore, the extra memory overhead can be eliminated from the engineering side using the
check-pointing technique: intermediate values are not saved in the memory but recalculated when
needed. We compare the runtime in both training and inference phase with check-pointing. As shown
in Ô¨Åg. 12, under the same number of parameters, there is only slight increase in the training and
basically the same runtime in the inference phase. With a direct transformation from Transformer
to Tuformer (for example, when initialized with pre-trained models), the runtime is still under a
reasonable range."
D,0.9749430523917996,"10%
20%
50%
100%
150%
200% 1.5 2 2.5 3 3.5"
D,0.9772209567198178,model scale
D,0.979498861047836,second per iteration
D,0.9817767653758542,Language Modeling on Penn Treebank
D,0.9840546697038725,"MHSA
THSA
THSA with checkpointing"
D,0.9863325740318907,"10%
20%
50%
100%
150%
200% 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
D,0.9886104783599089,model scale
D,0.9908883826879271,second per iteration
D,0.9931662870159453,Language Modeling on Penn Treebank
D,0.9954441913439636,"MHSA
THSA
THSA with checkpointing"
D,0.9977220956719818,"Figure 12: (Left) Training and (Right) inference run-time comparison between MHSA, THSA with the same
number of parameters and THSA with the same number of parameters with check-pointing. The batch size is
256."
