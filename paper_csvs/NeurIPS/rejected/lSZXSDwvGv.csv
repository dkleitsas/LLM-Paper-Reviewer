Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018552875695732839,"We study whether multiple large language models (LLMs) can autonomously
1"
ABSTRACT,0.0037105751391465678,"improve each other in a negotiation game by playing, reflecting, and criticizing.
2"
ABSTRACT,0.0055658627087198514,"We are interested in this question because if LLMs were able to improve each other,
3"
ABSTRACT,0.0074211502782931356,"it would imply the possibility of creating strong AI agents with minimal human
4"
ABSTRACT,0.00927643784786642,"intervention. We ask two LLMs to bargain with each other, playing the roles of a
5"
ABSTRACT,0.011131725417439703,"buyer and a seller, respectively. They aim to reach a deal with the buyer targeting
6"
ABSTRACT,0.012987012987012988,"a lower price and the seller a higher one. A third language model, playing the
7"
ABSTRACT,0.014842300556586271,"critic, provides feedback to a player to improve the player’s negotiation strategies.
8"
ABSTRACT,0.016697588126159554,"We let the two agents play multiple rounds, using previous negotiation history
9"
ABSTRACT,0.01855287569573284,"and AI feedback as in-context demonstrations to improve the model’s negotiation
10"
ABSTRACT,0.02040816326530612,"strategy iteratively. We use different LLMs (GPT and Claude) for different roles
11"
ABSTRACT,0.022263450834879406,"and use the deal price as the evaluation metric. Our experiments reveal multiple
12"
ABSTRACT,0.02411873840445269,"intriguing findings: (1) Only a subset of the language models we consider can
13"
ABSTRACT,0.025974025974025976,"self-play and improve the deal price from AI feedback, weaker models either do
14"
ABSTRACT,0.027829313543599257,"not understand the game’s rules or cannot incorporate AI feedback for further
15"
ABSTRACT,0.029684601113172542,"improvement. (2) Models’ abilities to learn from the feedback differ when playing
16"
ABSTRACT,0.03153988868274583,"different roles. For example, it is harder for Claude-instant to improve as the
17"
ABSTRACT,0.03339517625231911,"buyer than as the seller. (3) When unrolling the game to multiple rounds, stronger
18"
ABSTRACT,0.03525046382189239,"agents can consistently improve their performance by meaningfully using previous
19"
ABSTRACT,0.03710575139146568,"experiences and iterative AI feedback, yet have a higher risk of breaking the
20"
ABSTRACT,0.03896103896103896,"deal. We hope our work provides insightful initial explorations of having models
21"
ABSTRACT,0.04081632653061224,"autonomously improve each other with game playing and AI feedback.
22"
INTRODUCTION,0.04267161410018553,"1
Introduction
23"
INTRODUCTION,0.04452690166975881,"We study whether multiple Large Language Models (LLMs) can improve each other in a negotiation
24"
INTRODUCTION,0.04638218923933209,"game with minimal human intervention, in the fashion of AlphaGo Zero [31] where AI agents improve
25"
INTRODUCTION,0.04823747680890538,"themselves by continuously playing competitive games under well-defined rules. The answers to this
26"
INTRODUCTION,0.05009276437847866,"research question have profound implications. On the positive side, if the agents were able to improve
27"
INTRODUCTION,0.05194805194805195,"autonomously, strong agents might be created with very few human annotations, which greatly saves
28"
INTRODUCTION,0.05380333951762523,"the cost compared to today’s data-hungry LLM training [8, 17]. On the risky side, it also implies
29"
INTRODUCTION,0.055658627087198514,"strong agents with limited human oversight [4]. In this work, we ask two language models (a seller
30"
INTRODUCTION,0.0575139146567718,"and a buyer) to bargain about a product. The seller is asked to sell the product at a higher price, while
31"
INTRODUCTION,0.059369202226345084,"the buyer aims to purchase it at a lower price (Fig. 1A). After reaching a deal, we ask a third language
32"
INTRODUCTION,0.061224489795918366,"model to play as the critic and give feedback to a player. Then we play the game again, asking the
33"
INTRODUCTION,0.06307977736549165,"player to improve their strategy using AI feedback provided by the critic LLM.
34"
INTRODUCTION,0.06493506493506493,"We choose the bargaining game because it comes with well-defined rules described in text, and a
35"
INTRODUCTION,0.06679035250463822,"clear and measurable objective (a lower/ higher deal price) for strategic negotiation. Although the
36"
INTRODUCTION,0.0686456400742115,"A. We ask two agents to play a negotiation game and have an AI critic to give suggestions to one of the player to improve 
its negotiation strategy. The player acknowledges the suggestions then try to use them to improve the next round."
INTRODUCTION,0.07050092764378478,"C. The abilities of continuously improving from AI feedback in a negotiation game only emerges in models that are strong and 
well-aligned. Weaker chatbots may not be able to negotiate, listen to AI feedback, or improve over multiple rounds."
INTRODUCTION,0.07235621521335807,Seller
INTRODUCTION,0.07421150278293136,Round 1. A seller and a buyer bargain
INTRODUCTION,0.07606679035250463,about a product.
INTRODUCTION,0.07792207792207792,"Hi, how much is the"
INTRODUCTION,0.07977736549165121,balloon?
INTRODUCTION,0.08163265306122448,It’s a good balloon and
INTRODUCTION,0.08348794063079777,its $20 …
INTRODUCTION,0.08534322820037106,AI feedback: a critic reads the dialog history
INTRODUCTION,0.08719851576994433,and  give suggestions for improvements
INTRODUCTION,0.08905380333951762,To sell it to a high price
INTRODUCTION,0.09090909090909091,you may want to ...
INTRODUCTION,0.09276437847866419,"Understood. I will 
use ... to sell a higher price"
INTRODUCTION,0.09461966604823747,Round 2. Seller improves bargaining
INTRODUCTION,0.09647495361781076,strategy based on AI feedback
INTRODUCTION,0.09833024118738404,"Hi, how much is the"
INTRODUCTION,0.10018552875695733,balloon?
INTRODUCTION,0.10204081632653061,It's a royal product
INTRODUCTION,0.1038961038961039,"with top-notch 
craftsmanship ...
Buyer
Seller
Critic
Seller
Buyer
…
…"
INTRODUCTION,0.10575139146567718,"C1. Chitchat in 
casual scenario"
INTRODUCTION,0.10760667903525047,"C2. Role playing and 
negotiation"
INTRODUCTION,0.10946196660482375,"C3. Improve next round 
from AI feedback"
INTRODUCTION,0.11131725417439703,"C4. Continuously improve 
over multiple rounds"
INTRODUCTION,0.11317254174397032,"GPT-4
Claude-v1.3
GPT-4
GPT-3.5-Turbo"
INTRODUCTION,0.1150278293135436,Claude-v1.3
INTRODUCTION,0.11688311688311688,Claude-instant-v1.0 GPT-4
INTRODUCTION,0.11873840445269017,GPT-3.5-Turbo
INTRODUCTION,0.12059369202226346,Claude-v1.3
INTRODUCTION,0.12244897959183673,Claude-instant-v1.0
INTRODUCTION,0.12430426716141002,Jurassic-2-instruct
INTRODUCTION,0.1261595547309833,Cohere-command GPT-4
INTRODUCTION,0.1280148423005566,GPT-3.5-Turbo
INTRODUCTION,0.12987012987012986,Claude-v1.3
INTRODUCTION,0.13172541743970315,Claude-instant-v1.0
INTRODUCTION,0.13358070500927643,Jurassic-2-instruct
INTRODUCTION,0.13543599257884972,GPT-3.5-Turbo
INTRODUCTION,0.137291280148423,Buyer Critic:
INTRODUCTION,0.1391465677179963,"Employ the ""flinch"" technique: when 
the seller offers a counteroffer, the 
buyer should display a degree of 
surprise or disappointment
Buyer’s Improvement:"
INTRODUCTION,0.14100185528756956,"Oh! That's higher than I expected. I 
saw a similar balloon at another 
store for $14. Can you match that 
price?"
INTRODUCTION,0.14285714285714285,Buyer Critic:
INTRODUCTION,0.14471243042671614,"Use the power of silence: The buyer 
can employ the power of silence in 
the negotiation process by pausing 
longer before responding to the 
seller's offer."
INTRODUCTION,0.14656771799628943,Buyer’s Improvement:
INTRODUCTION,0.14842300556586271,"*pause* ...  Alright, I'll take the 
balloon for $13."
INTRODUCTION,0.150278293135436,Seller Critic:
INTRODUCTION,0.15213358070500926,"Utilize split-the-difference: In 
situations where a small price 
difference remains, propose to split 
the difference with the buyer."
INTRODUCTION,0.15398886827458255,Seller’s Improvement:
INTRODUCTION,0.15584415584415584,"I understand, how about we split the 
difference and make it $16.75 to 
accommodate your budget?"
INTRODUCTION,0.15769944341372913,Seller Critic:
INTRODUCTION,0.15955473098330242,"Use anchoring technique: Begin by 
emphasizing the high starting price 
and then offer a slightly lower price"
INTRODUCTION,0.1614100185528757,Seller’s Improvement:
INTRODUCTION,0.16326530612244897,"This high-quality, long-lasting balloon 
is really worth $25, but I'm offering 
it for $20."
INTRODUCTION,0.16512059369202226,"Buyer proposes $15, seller calls $18
Context:"
INTRODUCTION,0.16697588126159554,"B1. The “flinch” technique
B2. The power of silence
B3. Split-the-difference
B4. The anchoring technique"
INTRODUCTION,0.16883116883116883,B. Example feedback from AI critic and how a GPT-4 player improves from it.
INTRODUCTION,0.17068645640074212,"Figure 1: Settings of our negotiation game. A. We ask two LLM agents to play a bargaining game as
the seller and the buyer. Their goals are to sell/ buy the product at a higher/ lower price. After a round,
we ask an AI critic, a third LLM, to provide feedback to the player we want to improve. Then we ask
the player to improve their negotiation strategies based on the feedback. We repeat this process for
multiple rounds and study if models can continuously improve. See Fig. 2 for an example run. B.
Bargaining techniques that we observed from the AI Critic and how the player incorporates these
techniques into the negotiation strategy. C. Abilities that are required in our game (C2 - negotiation,
C3 - AI feedback, and C4 - continuous improvements) classify models into different tiers. We find out
that only strong and well-aligned models (like gpt-4 and claude-v1.3) can continuously improve
from iterative AI feedback (see Fig. 3 for example models that do not exhibit these abilities)."
INTRODUCTION,0.1725417439703154,"game seems easy at first glance, it requires non-trivial capabilities of the language models, as the
37"
INTRODUCTION,0.17439703153988867,"model needs to: (1) clearly understand and strictly follow the textual rules of the negotiation game
38"
INTRODUCTION,0.17625231910946196,"(2) correspond to the textual feedback provided by the critic LM and improve based on it iteratively
39"
INTRODUCTION,0.17810760667903525,"(see example feedback in Fig 1B); (3) reflect upon the strategy and feedback over the long term
40"
INTRODUCTION,0.17996289424860853,"and improve over multiple rounds. We will see that not all models we considered show all these
41"
INTRODUCTION,0.18181818181818182,"abilities (Fig. 1C), and only models that can (1) understand negotiation rules and strategies (capable)
42"
INTRODUCTION,0.1836734693877551,"and (2) respond to AI instructions (well-aligned) can continuously improve from AI feedback over
43"
INTRODUCTION,0.18552875695732837,"multiple rounds (in our experiments, only gpt-3.5-turbo, gpt-4, and claude-v1.3) meet these
44"
INTRODUCTION,0.18738404452690166,"requirements). We have also tried more complicated textual games including board games and textual
45"
INTRODUCTION,0.18923933209647495,"RPG games in the preliminary experiments, but they are more challenging for current agents to
46"
INTRODUCTION,0.19109461966604824,"understand and follow the rules.
47"
INTRODUCTION,0.19294990723562153,"We call our approach In-Context Learning from AI Feedback (ICL-AIF). Specifically, we use the
48"
INTRODUCTION,0.19480519480519481,"feedback from the AI critic as well as the previous rounds of dialog history as in-context demonstra-
49"
INTRODUCTION,0.19666048237476808,"tions [5]. By doing this, the critic’s suggestions for improvements and the player’s actual improvement
50"
INTRODUCTION,0.19851576994434136,"in the previous rounds effectively become the few-shot prompts for the next round of negotiation.
51"
INTRODUCTION,0.20037105751391465,"We use in-context learning for two reasons: (1) tuning large language models with reinforcement
52"
INTRODUCTION,0.20222634508348794,"learning is prohibitively expensive [25, 15] and the base model [24] may not be accessible to a wide
53"
INTRODUCTION,0.20408163265306123,"range of the community; (2) in-context learning is recently shown to be closely related to gradient
54"
INTRODUCTION,0.20593692022263452,"descent [11, 1, 32], such that the conclusions we draw is fairly likely to generalize when one actually
55"
INTRODUCTION,0.2077922077922078,"finetunes the model (if resources permit). One notable difference between our ICL-AIF and the
56"
INTRODUCTION,0.20964749536178107,"mainstream Reinforcement Learning from Human Feedback (RLHF) is that in RL the reward is a
57"
INTRODUCTION,0.21150278293135436,Claude-instant-v1.0 v.s. GPT-3.5-Turbo 2023-05-10
INTRODUCTION,0.21335807050092764,"Hi, this is a good balloon and its 
price is $20
Claude-ins-1.0 | Seller"
INTRODUCTION,0.21521335807050093,GPT-3.5-T | Buyer Would you consider selling it for $10?
INTRODUCTION,0.21706864564007422,"No, that price is too low.
Claude-ins-1.0 | Seller"
INTRODUCTION,0.2189239332096475,GPT-3.5-T | Buyer What about $15?
INTRODUCTION,0.22077922077922077,"$15 is better, but still not enough.
Claude-ins-1.0 | Seller"
INTRODUCTION,0.22263450834879406,GPT-3.5-T | Buyer What is the lowest price you
INTRODUCTION,0.22448979591836735,can offer?
INTRODUCTION,0.22634508348794063,"My lowest price is $18.
Claude-ins-1.0 | Seller"
INTRODUCTION,0.22820037105751392,GPT-3.5-T | Buyer I understand. Could you do $16?
INTRODUCTION,0.2300556586270872,That's the highest I can go.
INTRODUCTION,0.23191094619666047,"$16 will work for today. 
Congratulations, you got a good 
deal!"
INTRODUCTION,0.23376623376623376,Claude-ins-1.0 | Seller
INTRODUCTION,0.23562152133580705,"Round 1, before AI Feedback"
INTRODUCTION,0.23747680890538034,AI Feedback to seller
INTRODUCTION,0.23933209647495363,Claude-ins-1.0 | Seller Critic
FOCUS ON THE QUALITIES AND BENEFITS,0.24118738404452691,"1. Focus on the qualities and benefits 
of the balloon, not just the price. 
Highlight how rare and special it is."
FOCUS ON THE QUALITIES AND BENEFITS,0.24304267161410018,Claude-ins-1.0 | Seller
FOCUS ON THE QUALITIES AND BENEFITS,0.24489795918367346,"Sure, I will try to improve my 
negotiation strategy based on the 
feedback from the critic."
FOCUS ON THE QUALITIES AND BENEFITS,0.24675324675324675,"Round 2, After AI Feedback"
FOCUS ON THE QUALITIES AND BENEFITS,0.24860853432282004,"Hi, this is a good balloon and its price is $20
Claude-ins-1.0 | Seller"
FOCUS ON THE QUALITIES AND BENEFITS,0.2504638218923933,GPT-3.5-T | Buyer Would you consider selling it for $10?
FOCUS ON THE QUALITIES AND BENEFITS,0.2523191094619666,"No, $10 is much too low. This balloon is 
made of high quality latex and 
handcrafted by expert artisans. It's 
unique one-of-a-kind piece."
FOCUS ON THE QUALITIES AND BENEFITS,0.2541743970315399,Claude-in-1.0 | Seller
FOCUS ON THE QUALITIES AND BENEFITS,0.2560296846011132,GPT-3.5-T | Buyer What would be the lowest price you
FOCUS ON THE QUALITIES AND BENEFITS,0.25788497217068646,"could offer me for this balloon?
Well, because I trust you and value your 
business, I will offer you this special 
balloon at $17. This price is only 
available to valued customers like 
yourself today."
FOCUS ON THE QUALITIES AND BENEFITS,0.2597402597402597,Claude-ins-1.0 | Seller
FOCUS ON THE QUALITIES AND BENEFITS,0.26159554730983303,"GPT-3.5-T | Buyer Can you give me a better deal, like $15?"
FOCUS ON THE QUALITIES AND BENEFITS,0.2634508348794063,"I apologize, but $15 does not even cover 
my costs. I have to charge at least $17 
for this unique, high-quality balloon."
FOCUS ON THE QUALITIES AND BENEFITS,0.2653061224489796,Claude-ins-1.0 | Seller
FOCUS ON THE QUALITIES AND BENEFITS,0.26716141001855287,"GPT-3.5-T | Buyer Alright, you've got a deal. I will take the"
FOCUS ON THE QUALITIES AND BENEFITS,0.2690166975881262,special  balloon for $17.
FOCUS ON THE QUALITIES AND BENEFITS,0.27087198515769945,"And I will try to sell it at a higher 
price (higher than $16.0) than the 
previous round."
FOCUS ON THE QUALITIES AND BENEFITS,0.2727272727272727,"3. Be willing to walk away if needed. 
Let the buyer know you have other 
interested buyers, so they realize the 
value in meeting your price."
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.274582560296846,"2. Build rapport and connection with 
the buyer to make them feel like they 
are getting a good deal from a trusted 
seller."
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.2764378478664193,"A. Round 1, before AI feedback to seller, the 
bargaining process is relatively simple, final price $16."
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.2782931354359926,B. AI critic give feedbacks to the seller based on
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.28014842300556586,"the previous round, then seller acknowledge."
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.2820037105751391,"C. Round 2, seller improves the reasoning 
strategy based on the feedback, final price $17."
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.28385899814471244,"Figure 2: An example of playing the negotiation game and then improving from AI feedback. A:
claude-instant-v1.0 plays the seller and gpt-3.5-turbo the buyer, bargaining about a balloon.
B: then we use a claude-instant-v1.0 critic to provide feedback. C: upon receiving the feedback,
the seller improves its strategy based on the suggestions. Note that colored phrases like “high quality
latex and handcrafted by expert artisans” correspond to previous AI feedback “how rare and special it
is”. We measure the final price as the proxy of the effectiveness of the strategy because the overall
goal is to get a better price. In this case, it improves from $16 to $17."
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.2857142857142857,"scalar [25, 15] while in ICL the feedback is in natural language. We study AI feedback (rather than
58"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.287569573283859,"rely on human intervention after each round) because it is more scalable and can allow models to
59"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.2894248608534323,"self-improve automatically.
60"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.2912801484230056,"Our experiments lead to several intriguing findings: (1) The requirements of our bargaining game
61"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.29313543599257885,"effectively serve as a testbed for assessing the abilities of LLMs (Fig. 1C): although most models can
62"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.2949907235621521,"do chitchat in a casual scenario, as of our experiment date (May 2023), cohere-command [10] model
63"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.29684601113172543,"does not understand the rule of bargaining (Fig. 3A), ai21-jurassic [18] model does not respond
64"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.2987012987012987,"to AI feedback (Fig. 3B), claude-instant-v1.0 can at most improve one round (Fig. 5), and only
65"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.300556586270872,"gpt-3.5-turbo, gpt-4, and claude-v1.3 can continuously improve over multiple rounds. (2)
66"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.30241187384044527,"Models behave differently upon receiving feedback when playing different roles. Models playing the
67"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.3042671614100185,"buyer role may be harder to improve than when in the seller role (Fig. 4). (3) It is indeed possible
68"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.30612244897959184,"for strong agents like gpt-4 to continuously improve meaningfully using previous experiences and
69"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.3079777365491651,"online iterative AI feedback, yet the attempt to sell at a higher price (or buy at a lower price) comes
70"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.3098330241187384,"with the risk of failing to reach a deal at all (Fig. 6). We further show evidence of the model being
71"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.3116883116883117,"able to negotiation in a less verbose but more strategic (thus more effective) way (Fig. 7). Overall,
72"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.313543599257885,"we hope our work serves as a meaningful initiative for improving language models’ negotiation in a
73"
"BUILD RAPPORT AND CONNECTION WITH 
THE BUYER TO MAKE THEM FEEL LIKE THEY 
ARE GETTING A GOOD DEAL FROM A TRUSTED",0.31539888682745826,"game setting using AI feedback.
74"
PROBLEM SETTING,0.3172541743970315,"2
Problem Setting
75"
PROBLEM SETTING,0.31910946196660483,"Our goal is to study whether LLMs can improve each other by playing a negotiation game and
76"
PROBLEM SETTING,0.3209647495361781,"incorporating AI feedback, as shown in Fig. 1A. We set the product being bargained as a balloon (and
77"
PROBLEM SETTING,0.3228200371057514,"our results hold when changing the balloon to other items). We use different combinations of backend
78"
PROBLEM SETTING,0.3246753246753247,"LLM engines: cohere-command [10], AI21’s jurassic-2 [18], OpenAI’s gpt-3.5-turbo and
79"
PROBLEM SETTING,0.32653061224489793,"gpt-4 [24], Anthropic’s claude-instant-v1.0 (which supposedly matches gpt-3.5-turbo [14])
80"
PROBLEM SETTING,0.32838589981447125,"and claude-v1.3 (which is supposed to be slightly worse but close to gpt-4 [14]). throughout our
81"
PROBLEM SETTING,0.3302411873840445,"experiments, we provide feedback to improve only one of the two players, while its rival receives no
82"
PROBLEM SETTING,0.3320964749536178,"feedback, clears the negotiation history of previous rounds, and restarts. We vary the engines for the
83"
PROBLEM SETTING,0.3339517625231911,"model being improved while fixing its rival’s engine to be gpt-3.5-turbo. Essentially, our game
84"
PROBLEM SETTING,0.3358070500927644,"is gpt-3.5-turbo vs. all other engines. We keep the LM engine behind the critic is always the
85"
PROBLEM SETTING,0.33766233766233766,"same as the player it provides feedback to. One example setting is a gpt-4 seller playing against a
86"
PROBLEM SETTING,0.3395176252319109,"gpt-3.5-turbo buyer, with a gpt-4 critic. After one round, the gpt-4 critic provides feedback to
87"
PROBLEM SETTING,0.34137291280148424,"the gpt-4 seller such that the seller can improve in the next round while its rival gpt-3.5-turbo
88"
PROBLEM SETTING,0.3432282003710575,"buyer clears its dialog history and restarts.
89"
PROBLEM SETTING,0.3450834879406308,"Process of the Game
Before the game begins, the rules of the negotiation game are explained
90"
PROBLEM SETTING,0.3469387755102041,"to the models through textual instructions with the objective of selling/ buying at a higher/ lower
91"
PROBLEM SETTING,0.34879406307977734,"price. We set the deal price to [$10, $20] for easier evaluation, since other the deal price may vary
92"
PROBLEM SETTING,0.35064935064935066,"in a wide range according to the observations from our preliminary experiments. To achieve this,
93"
PROBLEM SETTING,0.3525046382189239,"we hard code the seller to kick off the negotiation with “This is a good balloon and its price is $20.”
94"
PROBLEM SETTING,0.35435992578849723,"Similarly, the buyer always opens with “Would you consider selling it for $10?” When both players
95"
PROBLEM SETTING,0.3562152133580705,"strictly follow the game rules, the deal price would be between $10 and $20. We let the models play
96"
PROBLEM SETTING,0.3580705009276438,"multiple runs and measure the average deal price before and after AI feedback. During the game, the
97"
PROBLEM SETTING,0.35992578849721707,"seller’s output is used to prompt the buyer and vice versa, conditioning on the entire conversation
98"
PROBLEM SETTING,0.36178107606679033,"history. This process is repeated till a terminal state is reached. Fig. 2A shows an example round. We
99"
PROBLEM SETTING,0.36363636363636365,"define three game states: (1) ON-GOING: the negotiation between the two players is still ongoing;
100"
PROBLEM SETTING,0.3654916512059369,"(2) DEAL: the negotiation has concluded and the two players have reached a deal; (3) NO DEAL: the
101"
PROBLEM SETTING,0.3673469387755102,"players cannot agree on a price and have failed to reach a deal. To track the game states, we set an
102"
PROBLEM SETTING,0.3692022263450835,"additional moderator (powered by a fourth LLM, in our case, gpt-3.5-turbo) to read the current
103"
PROBLEM SETTING,0.37105751391465674,"dialog and classify the states (we will discuss more details about the moderator later). We measure
104"
PROBLEM SETTING,0.37291280148423006,"the performance of the players based on the final deal price.
105"
PROBLEM SETTING,0.3747680890538033,"Critic
A round is finished when the negotiation reaches a terminating state, either a DEAL or
106"
PROBLEM SETTING,0.37662337662337664,"NO DEAL. After each round, the critic LM is asked to provide constructive feedback to the player we
107"
PROBLEM SETTING,0.3784786641929499,"aim to improve. This player’s dialog history from all past rounds and all feedback it has received are
108"
PROBLEM SETTING,0.3803339517625232,"used to prompt the critic LM (Fig. 2B). The critic model is instructed to provide three suggestions to
109"
PROBLEM SETTING,0.3821892393320965,"the player, in order to improve its negotiation strategies to achieve a more favorable price in the next
110"
PROBLEM SETTING,0.38404452690166974,"game. Before the next round, the player being improved receives the critic’s feedback as a textual
111"
PROBLEM SETTING,0.38589981447124305,"prompt, while its rival clears its negotiation history and restarts.
112"
PROBLEM SETTING,0.3877551020408163,"The Moderator
The game state is classified by prompting a gpt-3.5-turbo moderator using
113"
PROBLEM SETTING,0.38961038961038963,"few-shot demonstrations. The moderator reads the most recent four rounds (as well as in-context
114"
PROBLEM SETTING,0.3914656771799629,"examples of different dialog states) and determines the state of the negotiation. Empirically, we found
115"
PROBLEM SETTING,0.39332096474953615,"that four rounds of conversations are sufficient for the moderator to determine the negotiation state.
116"
PROBLEM SETTING,0.39517625231910947,"One key challenge here is detecting no-deals as the model seems to be better at recognizing DEAL
117"
PROBLEM SETTING,0.3970315398886827,"than NO DEAL. We mitigate this issue by playing multiple runs, inspect failure cases manually,
118"
PROBLEM SETTING,0.39888682745825604,"and add them to the prompt with corrected labels. We find this method an effective side product
119"
PROBLEM SETTING,0.4007421150278293,"recommend it as a technique for prompt optimization for generic classification tasks.
120"
PROBLEM SETTING,0.4025974025974026,"Playing for Multiple Rounds
Finally, we would like to explore whether the players can continu-
121"
PROBLEM SETTING,0.4044526901669759,"ously improve from AI feedback in a game over multiple rounds. Intuitively, the more rounds the
122"
PROBLEM SETTING,0.40630797773654914,"players play, the more challenging to keep improving because the (already improved) price from the
123"
PROBLEM SETTING,0.40816326530612246,"previous round becomes the baseline for the next round. In the experiments, we will show that only
124"
PROBLEM SETTING,0.4100185528756957,"gpt-4 can improve over 5 rounds while other models’ improvements may saturate at about 3 rounds.
125"
RELATED WORK,0.41187384044526903,"3
Related Work
126"
RELATED WORK,0.4137291280148423,"Game Playing and AlphaGo Zero
Our setting is strongly inspired by AlphaGo Zero [31] where
127"
RELATED WORK,0.4155844155844156,"two agents play the game of Go and improve each other with minimal human intervention. Here we
128"
RELATED WORK,0.4174397031539889,"would like to explore its counterpart in natural language. Our work is similar to AlphaGo Zero in the
129"
RELATED WORK,0.41929499072356213,"sense that we also have AI agents (large language models) playing competitive games (bargaining)
130"
RELATED WORK,0.42115027829313545,"and try to improve with little human supervision. Yet there is an important difference between our
131"
RELATED WORK,0.4230055658627087,"work and AlphaGo Zero: we have a third agent, the critic, to give feedback helping its player to
132"
RELATED WORK,0.424860853432282,"improve. This is a cooperative relationship that does not exist in AlphaGo Zero. On the NLP side, the
133"
RELATED WORK,0.4267161410018553,"closest related work is Lewis et al. [19] where they have (small) RNN [9] language models to bargain,
134"
RELATED WORK,0.42857142857142855,"and our work can be viewed as a more developed version of them since we change the engine to
135"
RELATED WORK,0.43042671614100186,"be large language models. In general, our work is broadly under the area of AI negotiation [6, 6],
136"
RELATED WORK,0.4322820037105751,"strategic reasoning [12], and general game playing [30].
137"
RELATED WORK,0.43413729128014844,"Large Language Models as Generative Agents
Large language models have demonstrated
138"
RELATED WORK,0.4359925788497217,"incredible multi-dimensional capabilities [33, 24], especially in complex reasoning [34, 28, 13] and
139"
RELATED WORK,0.437847866419295,"multi-round dialog [15, 2, 3], which serve as the foundation of this work. Our work is related to
140"
RELATED WORK,0.4397031539888683,"concurrent works like Generative Agents [26] and CAMEL [20] as they also study the behavior of
141"
RELATED WORK,0.44155844155844154,"LLMs in a multi-agent game setting. The core difference between our work and theirs is that we
142"
RELATED WORK,0.44341372912801486,Cohere command-xlarge-nightly 2023-04-22 v.s. human
RELATED WORK,0.4452690166975881,Cohere | Buyer Would you consider selling it for $15?
RELATED WORK,0.44712430426716143,"User | Seller
Yes, I can sell the balloon for $15."
RELATED WORK,0.4489795918367347,Cohere | Buyer Would you consider selling it for $17?
RELATED WORK,0.45083487940630795,# Asking $ 17 after seller agreed on $15
RELATED WORK,0.45269016697588127,Cohere command-nightly 2023-05-11 v.s. human
RELATED WORK,0.45454545454545453,"User | Buyer
Would you consider selling it for $10?"
RELATED WORK,0.45640074211502785,"Cohere | Seller 
No. This is a good baloon and its price is $20."
RELATED WORK,0.4582560296846011,"User | Buyer
Understood. What is the lowest price you 
would accept for the balloon?
Cohere | Seller 
I would accept $8 for the balloon."
RELATED WORK,0.4601113172541744,# Accept $8 while reject $10
RELATED WORK,0.4619666048237477,"A. Examples where model does not understand bargaining
B. Examples where model does not incorporate feedback"
RELATED WORK,0.46382189239332094,AI21 j2-jumbo-instruct 2023-05-09 v.s. GPT-3.5-Turbo
RELATED WORK,0.46567717996289426,"AI21 | Seller 
I can't sell it for $10, but I can consider selling it for $12
GPT-3.5-Turbo | Buyer 
How about $11?"
RELATED WORK,0.4675324675324675,"AI21 | Seller 
I can consider selling it for $11
GPT-3.5-Turbo | Buyer 
Thank you, I'll take it for $11."
RELATED WORK,0.46938775510204084,AI21 | Seller Critic Feedback: 1. Emphasizing its unique characteristics or qualities
CREATE A SENSE OF URGENCY,0.4712430426716141,"2. Create a sense of urgency
               3. Encourage the buyer to elaborate on their needs"
CREATE A SENSE OF URGENCY,0.47309833024118736,"AI21 | Seller 
I can't sell it for $10, but I can consider selling it for $12"
CREATE A SENSE OF URGENCY,0.4749536178107607,"# Repeat previous strategy, does not listen to critic
GPT-3.5-Turbo | Buyer 
How about $11? That's the most I can offer.
AI21 | Seller 
I can consider selling it for $11."
CREATE A SENSE OF URGENCY,0.47680890538033394,# Does not defend its position
CREATE A SENSE OF URGENCY,0.47866419294990725,Round 1
CREATE A SENSE OF URGENCY,0.4805194805194805,AI Feedback
CREATE A SENSE OF URGENCY,0.48237476808905383,Round 2
CREATE A SENSE OF URGENCY,0.4842300556586271,"GPT-3.5-Turbo | Buyer 
Great, it’s a deal then. Thank you!"
CREATE A SENSE OF URGENCY,0.48608534322820035,"Figure 3:
Not all models can play bargaining. A. As of May 2023, the cohere model does not
understand the rule of bargaining and agrees on irrational prices. B. The AI21 Jurrasic-2 model,
although understanding the rule of bargaining, does not incorporate the feedback from the critic.
Since these models are consistently being updated, we include the timestamp and note future versions
may have improved performance."
CREATE A SENSE OF URGENCY,0.48794063079777367,"have a clear objective (the deal price) for the model to improve through competition and cooperation,
143"
CREATE A SENSE OF URGENCY,0.4897959183673469,"while their work studies the generic social behavior of LLMs.
144"
CREATE A SENSE OF URGENCY,0.49165120593692024,"Learning from AI Feedback
Our method is also strongly inspired by constitutional AI [3] as
145"
CREATE A SENSE OF URGENCY,0.4935064935064935,"we both use AI feedback, while the difference is that our feedback is directly in natural language
146"
CREATE A SENSE OF URGENCY,0.49536178107606677,"(not a scalar from a reward model). There are also related/ concurrent works demonstrating the
147"
CREATE A SENSE OF URGENCY,0.4972170686456401,"effectiveness of natural language feedback [29, 27, 22] and self-refinement [7, 23]. Our work further
148"
CREATE A SENSE OF URGENCY,0.49907235621521334,"confirms the effectiveness of AI feedback in the strategic negotiation game setting.
149"
EXPERIMENTS,0.5009276437847866,"4
Experiments
150"
EXPERIMENTS,0.5027829313543599,"In our experiments, we consider three stages that gradually deepen our exploration of learning from
151"
EXPERIMENTS,0.5046382189239332,"AI feedback: (1) We first set up the basics of the game (Sec. 4.2), showing that only a few models can
152"
EXPERIMENTS,0.5064935064935064,"improve from AI critics, in which case AI feedback can be comparable (but more scalable) as human
153"
EXPERIMENTS,0.5083487940630798,"feedback. Other models either do not understand/ follow the rule of bargaining, or cannot incorporate
154"
EXPERIMENTS,0.5102040816326531,"AI feedback for improvements. (2) Then we study the models’ behaviors when playing different roles
155"
EXPERIMENTS,0.5120593692022264,"(Sec. 4.3). We discover the intriguing result that buyers are in general harder to improve than sellers.
156"
EXPERIMENTS,0.5139146567717996,"(3) Finally, we study whether models can continuously improve over multiple rounds (Sec. 4.4), and
157"
EXPERIMENTS,0.5157699443413729,"show a tradeoff of deal price versus success rate: although some models can continuously improve
158"
EXPERIMENTS,0.5176252319109462,"the deal price, it comes with a higher risk of breaking a deal. We further show evidence of negotiation
159"
EXPERIMENTS,0.5194805194805194,"in a more strategic way: both gpt-4 and claude-v1.3’s responses become longer after multiple
160"
EXPERIMENTS,0.5213358070500927,"rounds of AI feedback (note that verbosity is a straightforward negotiation strategy), yet gpt-4 is less
161"
EXPERIMENTS,0.5231910946196661,"verbose than claude-v1.3 but achieves higher deal price and deal rate, meaning that its responses,
162"
EXPERIMENTS,0.5250463821892394,"although using fewer words, are more strategic and effective.
163"
EXPERIMENT SETUP,0.5269016697588126,"4.1
Experiment Setup
164"
EXPERIMENT SETUP,0.5287569573283859,"Model Engines
The minimum requirement for models to enter our game is that they should be a
165"
EXPERIMENT SETUP,0.5306122448979592,"chatbot. All models we consider (cohere-command, AI21’s jurassic-2, OpenAI’s gpt and An-
166"
EXPERIMENT SETUP,0.5324675324675324,"thropic’s claude) can be accessed by API calls. Amoung them, gpt-4 is the most expensive one and
167"
EXPERIMENT SETUP,0.5343228200371057,"running 500 rounds of negotiation costs about $120 and gpt-3.5-turbo costs about $10. Other mod-
168"
EXPERIMENT SETUP,0.536178107606679,"els are beta testing (as of May 2023) and do not charge money. For reference, the approximate rank
169"
EXPERIMENT SETUP,0.5380333951762524,"of these models, from benchmarks like chain-of-thought hub [14] and HeLM [21], is that gpt-4 and
170"
EXPERIMENT SETUP,0.5398886827458256,"claude-v1.3 are approximately similar, better than gpt-3.5-turbo and claude-instant-v1.0,
171"
EXPERIMENT SETUP,0.5417439703153989,"and better than cohere-command and j2-jumbo-instruct. We will consider more models in the
172"
EXPERIMENT SETUP,0.5435992578849722,"future, such as Google’s PaLM-2 [16].
173"
EXPERIMENT SETUP,0.5454545454545454,"We let all models compete with gpt-3.5-turbo, effectively making it a baseline for all other models.
174"
EXPERIMENT SETUP,0.5473098330241187,"We will show that, aligning with other concurrent model rankings [14, 21], gpt-3.5-turbo is a
175"
EXPERIMENT SETUP,0.549165120593692,"middle-level powerful engine (worse than gpt-4, better than claude-instant-v1.0). For a given
176"
EXPERIMENT SETUP,0.5510204081632653,"model engine (say claude-v1.3), we run it as the seller (with gpt-3.5-turbo as the buyer) and
177"
EXPERIMENT SETUP,0.5528756957328386,"Table 1: Seller performance using AI feedback vs. randomly selected human feedback from a
pre-defined pool. Recall that the buyer is fixed to be gpt-3.5-turbo and has no access to previous
rounds. AI’s feedback is comparable to human’s, but is more scalable, as the two both induce similar
price increases."
EXPERIMENT SETUP,0.5547309833024119,"GPT-3.5-Turbo
Claude-instant-v1.0
Claude-v1.3"
EXPERIMENT SETUP,0.5565862708719852,"Before feedback
16.26
14.74
15.40
Random sampled human feedback
16.83 (+0.57)
16.33 (+1.59)
16.89 (+1.49)
AI feedback
17.03 (+0.77)
15.98 (+1.24)
16.98 (+1.58)"
EXPERIMENT SETUP,0.5584415584415584,"as a buyer (with gpt-3.5-turbo now as the seller) We first let the models to play one round and
178"
EXPERIMENT SETUP,0.5602968460111317,"manually inspect if they understand the rules of bargaining. If they do, we let them play two rounds
179"
EXPERIMENT SETUP,0.562152133580705,"to see if they could respond to AI feedback. For the critic model, we set its engine the same as its
180"
EXPERIMENT SETUP,0.5640074211502782,"player. We repeat the game 500 times to compute the average deal price before and after AI feedback.
181"
EXPERIMENT SETUP,0.5658627087198516,"If they do improve one round, we let them play multiple rounds and see if they could continuously
182"
EXPERIMENT SETUP,0.5677179962894249,"improve their strategy. We repeat the game 200 times with 5 max rounds to compute the average
183"
EXPERIMENT SETUP,0.5695732838589982,"deal price for each round. When decoding from the model engines, we use sampling with default
184"
EXPERIMENT SETUP,0.5714285714285714,"temperature (1.0 for gpt and claude, 0.75 for cohere and 0.7 for j2).
185"
EXPERIMENT SETUP,0.5732838589981447,"Prompt Engineering
In this work, we only had to manually optimize the prompts for the
186"
EXPERIMENT SETUP,0.575139146567718,"moderator because the player may reach/ break a deal with very diverse expressions, and we would
187"
EXPERIMENT SETUP,0.5769944341372912,"like to make sure the moderator correctly recognizes all of them. As mentioned above, we identify the
188"
EXPERIMENT SETUP,0.5788497217068646,"errors made by the moderator in identifying deals and keep adding them as in-context demonstrations
189"
EXPERIMENT SETUP,0.5807050092764379,"until the model reaches a sufficiently high accuracy (about 90+ by manual inspection). For the players
190"
EXPERIMENT SETUP,0.5825602968460112,"and the critic, we do not do prompt engineering and keep the instructions the same for all engines (but
191"
EXPERIMENT SETUP,0.5844155844155844,"the format may be different, e.g., claude requires two linebreaks before “HUMAN:” and j2 requires
192"
EXPERIMENT SETUP,0.5862708719851577,"two “##” after each dialog round). Code and Prompts will be released publicly on publication.
193"
BASIC EXPERIMENTS,0.588126159554731,"4.2
Basic Experiments
194"
BASIC EXPERIMENTS,0.5899814471243042,"In this section, we first study the minimal requirements for models to participle in our game, namely
195"
BASIC EXPERIMENTS,0.5918367346938775,"(1) understanding the rule of bargaining and (2) responding to AI feedback. Then we consider basic
196"
BASIC EXPERIMENTS,0.5936920222634509,"comparison between AI and human feedback, showing that AI feedback can be comparable to human
197"
BASIC EXPERIMENTS,0.5955473098330241,"feedback, but more scalable.
198"
BASIC EXPERIMENTS,0.5974025974025974,"Conversational ability does not guarantee ability to negotiate or learning from feedback
We
199"
BASIC EXPERIMENTS,0.5992578849721707,"study whether conversational models can understand the rule of bargaining by manually checking
200"
BASIC EXPERIMENTS,0.601113172541744,"traces of the dialog, and found that cohere-command fails to understand the rules, as is shown in
201"
BASIC EXPERIMENTS,0.6029684601113172,"Fig 3A. We observe that it does not realize what price is a better deal. For example, when playing
202"
BASIC EXPERIMENTS,0.6048237476808905,"seller, it rejects a proposal of $10 but accepts $8. We also observe that AI21’s j2-jumbo-instruct
203"
BASIC EXPERIMENTS,0.6066790352504638,"model, although understanding the rule of bargaining, cannot incorporate AI feedback, as is shown in
204"
BASIC EXPERIMENTS,0.608534322820037,"Fig. 3B. Generally, when instructed with AI feedback, the model keeps the same strategy as before,
205"
BASIC EXPERIMENTS,0.6103896103896104,"without any improvements.
206"
BASIC EXPERIMENTS,0.6122448979591837,"After ruling out the cohere-command and j2-jumbo-instruct models, we consider the three
207"
BASIC EXPERIMENTS,0.614100185528757,"remaining models: gpt-3.5-turbo, claude-instant-v1.0 and claude-v1.3. For these three
208"
BASIC EXPERIMENTS,0.6159554730983302,"engines, we do not observe the problems in Fig. 3. This means that these models can be used for our
209"
BASIC EXPERIMENTS,0.6178107606679035,"multi-round games.
210"
BASIC EXPERIMENTS,0.6196660482374768,"AI Feedback can be comparable to human feedback
Now we consider some initial comparison
211"
BASIC EXPERIMENTS,0.62152133580705,"between AI and human feedback. We emphasize that our goal is not to show which one is better –
212"
BASIC EXPERIMENTS,0.6233766233766234,"a similar level of effectiveness would suffice our study (to see if LLMs can continuously improve
213"
BASIC EXPERIMENTS,0.6252319109461967,"through self-play and AI feedback). For the human feedback, we manually write done a pool of 10
214"
BASIC EXPERIMENTS,0.62708719851577,"suggestions. Then we play 500 runs of the game, computing the deal price before and after feedback.
215"
BASIC EXPERIMENTS,0.6289424860853432,"After 500 runs, we compare the improvements after: (1) randomly sampling 3 suggestions from
216"
BASIC EXPERIMENTS,0.6307977736549165,"the predefined pool and (2) asking the AI critic to write down 3 suggestions. We note that this may
217"
BASIC EXPERIMENTS,0.6326530612244898,"underestimate the performance of human feedback, yet it would be unpractical to ask human to write
218"
BASIC EXPERIMENTS,0.634508348794063,"done 3 suggestions for all 1500 runs (while AI feedback does not have this problem). The results
219"
BASIC EXPERIMENTS,0.6363636363636364,"are shown in Table 1 where we see that all three models (gpt-3.5-turbo, claude-instant-v1.0
220"
BASIC EXPERIMENTS,0.6382189239332097,"and claude-v1.3) exhibit comparable improvements over human and AI feedback.
221"
BASIC EXPERIMENTS,0.640074211502783,"A1. Claude-instant-v1.0 seller
A2. GPT-3.5-Turbo seller"
BASIC EXPERIMENTS,0.6419294990723562,A. AI feedback moves the distribution of sellers’ deal price towards a higher range
BASIC EXPERIMENTS,0.6437847866419295,B1. Claude-instant-v1.0 buyer
BASIC EXPERIMENTS,0.6456400742115028,B. Buyers are harder to improve than sellers: AI feedback does not quite move buyers’ deal price distribution
BASIC EXPERIMENTS,0.647495361781076,"C1. Claude-v1.3 buyer
C2. GPT-4 buyer"
BASIC EXPERIMENTS,0.6493506493506493,"C. Stronger agents (Claude-v1.3 and GPT-4), when playing buyers, can still improve from AI feedback"
BASIC EXPERIMENTS,0.6512059369202227,B2. GPT-3.5-Turbo buyer
BASIC EXPERIMENTS,0.6530612244897959,"Figure 4: Binned deal price frequencies of 500 games, before v.s. after feedback. Effective feedback
should move the distribution towards a lower/ higher price range. X-axis: intervals of deals from
$10 (buyers’ initial price) to $20 (sellers’ asking price). Y-axis: the frequency of the price. A and B:
for weaker agents like claude-instant-v1.0 and gpt-3.5-turbo, improving from AI feedback
as the seller is easier than as buyer. For sellers, AI feedback moves the deal distribution to a higher
range (rightward), but does not move buyers’ deal distribution much. Consequently, the change in
average deal price when playing as buyers (-0.2 and -0.5) is clearly smaller than those as sellers (+1.0
and +1.7) C. Stronger agents (claude-v1.3/ gpt-4), can still improve from AI feedback even as
buyers, with larger changes in average deal price (-1.2 and -3.0)."
BEHAVIORS OF DIFFERENT LLM BACKEND,0.6549165120593692,"4.3
Behaviors of Different LLM Backend
222"
BEHAVIORS OF DIFFERENT LLM BACKEND,0.6567717996289425,"So far we have established that our game setting is valid for stronger LLM engines. Now we
223"
BEHAVIORS OF DIFFERENT LLM BACKEND,0.6586270871985158,"consider the detailed behavior comparisons using different engines for different roles. Specifically,
224"
BEHAVIORS OF DIFFERENT LLM BACKEND,0.660482374768089,"we use claude-instant-v1.0, claude-v1.3, gpt-3.5-turbo, and gpt-4 to play the seller/
225"
BEHAVIORS OF DIFFERENT LLM BACKEND,0.6623376623376623,"buyer (against a gpt-3.5-turbo buyer/ seller respectively), then study the deal price distribution
226"
BEHAVIORS OF DIFFERENT LLM BACKEND,0.6641929499072357,"before/ after AI feedback (also recall that the AI critic is powered by the same engine as its player).
227"
BEHAVIORS OF DIFFERENT LLM BACKEND,0.6660482374768089,"The results are visualized in Fig. 4. When claude-instant-v1.0 and gpt-3.5-turbo play the
228"
BEHAVIORS OF DIFFERENT LLM BACKEND,0.6679035250463822,"seller, they are able to improve their average deal price after AI feedback (Fig. 4A). But when they
229"
BEHAVIORS OF DIFFERENT LLM BACKEND,0.6697588126159555,"play the buyer role, their average deal price does not improve, which indicates that buyers tend to be
230"
BEHAVIORS OF DIFFERENT LLM BACKEND,0.6716141001855288,"a harder role than sellers (Fig. 4B). Yet this observation does not hold for engines like gpt-4 and
231"
BEHAVIORS OF DIFFERENT LLM BACKEND,0.673469387755102,"claude-v1.3, as they can still improve from AI feedback even playing buyers. Overall, this set of
232"
BEHAVIORS OF DIFFERENT LLM BACKEND,0.6753246753246753,"experiments reveal the nuanced capability differences between the four engines we consider.
233"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.6771799628942486,"4.4
Towards Continuous Improvements from Iterative AI Feedback
234"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.6790352504638218,"Now we unroll the game to multiple rounds and see if models can continuously improve from
235"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.6808905380333952,"previous dialog history and iterative AI feedback. Specifically, we let gpt-3.5-turbo, gpt-4,
236"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.6827458256029685,"claude-instant-v1.0, and claude-v1.3 play as the seller/ buyer respectively against a rival
237"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.6846011131725418,"powered by gpt-3.5-turbo. As mentioned before, the critic shares the same engine as the player it
238"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.686456400742115,"GPT-3.5-Turbo
Claude-instant-v1.0"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.6883116883116883,"Criticize seller
Criticize buyer"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.6901669758812616,A1. GPT-3.5-Turbo seller continuously improves
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.6920222634508348,from AI feedback
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.6938775510204082,A2. GPT-3.5-Turbo buyer continuously improves
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.6957328385899815,from AI feedback
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.6975881261595547,B1. Claude-instant-v1.0 seller only improves 1 round
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.699443413729128,from AI feedback
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7012987012987013,B2. Claude-instant-v1.0 buyer does not improve
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7031539888682746,from AI feedback
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7050092764378478,"Figure 5: In the multi-round setting, different engines have different behavior when playing seller/
buyer. Line plots are the average price over 200 runs and bar plots represent the price distribution. A1
v.s. B1. When playing sellers, gpt-3.5-turbo can improve from AI feedback in multiple rounds,
while claude-instant-v1.0 only improves the first round. A2 v.s. B2. When playing buyers,
gpt-3.5-turbo can improve in multiple rounds, whild claude-instant-v1.0 cannot."
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7068645640074211,"helps with. We play 200 runs of the game, and unroll each game to be 5 rounds. We compute the
239"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7087198515769945,"final deal price and the deal success rate and see if the price can be continuously improved.
240"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7105751391465677,"Fig. 5 shows gpt-3.5-turbo and claude-instant-v1.0 playing different roles. For a given
241"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.712430426716141,"engine, improvements over one round may not necessarily extrapolate to multiple rounds, as we
242"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7142857142857143,"observe that gpt-3.5-turbo can improve over multiple rounds, but claude-instant-v1.0 only
243"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7161410018552876,"improves at most one round.
244"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7179962894248608,"Now we consider the tradeoff between the tendency of achieving a higher deal price versus the rick
245"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7198515769944341,"of breaking a deal, as is shown in Fig 6. We see that when playing sellers, all four model engines can
246"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7217068645640075,"improve over at least one round, but this comes at the cost of decreasing deal success ratio. When
247"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7235621521335807,"playing buyers, there are models that cannot improve (claude-instant-v1.0), or saturate over 3
248"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.725417439703154,"rounds (claude-v1.3), while gpt-4 and gpt-3.5-turbo can continuously improve, and gpt-4
249"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7272727272727273,"achieves better (lower) deal price and higher deal rate than gpt-3.5-turbo.
250"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7291280148423006,"Finally, we study how iterative AI feedback influences the language complexity used by the agents
251"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7309833024118738,"by plotting the average response length (measured in number of characters) after each round, as is
252"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7328385899814471,"shown in Fig. 7. We see that both claude-v1.3 and gpt-4 become more verbose after iterative AI
253"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7346938775510204,"feedback with a continuously increasing response length. This is intuitive because being verbosity is
254"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7365491651205937,"a straightforward strategy in negotiation. Yet for claude-v1.3, the verbosity does not translate to
255"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.738404452690167,"better negotiation strategy, as its improvement saturates after three rounds (Fig. 6B1). In comparison,
256"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7402597402597403,"gpt-4’s increase verbosity is more strategic, as it use less words than claude-v1.3, but achieves
257"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7421150278293135,"better deal price and deal success rate (Fig. 6B). This observation serve as strong evidence that AI
258"
TOWARDS CONTINUOUS IMPROVEMENTS FROM ITERATIVE AI FEEDBACK,0.7439703153988868,"feedback improves players’ response towards a word-tuned, strategic direction.
259"
CONCLUSIONS,0.7458256029684601,"5
Conclusions
260"
CONCLUSIONS,0.7476808905380334,"In this work, we study whether multiple large language models can autonomously improve each other
261"
CONCLUSIONS,0.7495361781076066,"in a negotiation game by role-playing and learning from AI feedback. Our experiments show that
262"
CONCLUSIONS,0.75139146567718,"certain models can indeed improve by continuously playing competition games with iterative AI
263"
CONCLUSIONS,0.7532467532467533,"feedback, under well-defined rules in an AlphaGo Zero fashion. We also show the tradeoff between
264"
CONCLUSIONS,0.7551020408163265,"next-round price improvement and success rate, as a better deal price also comes with a higher risk
265"
CONCLUSIONS,0.7569573283858998,"Average deal price over 5 rounds
Deal success ratio over 5 rounds"
CONCLUSIONS,0.7588126159554731,Criticize
CONCLUSIONS,0.7606679035250464,Seller
CONCLUSIONS,0.7625231910946196,Criticize Buyer
CONCLUSIONS,0.764378478664193,"Claude-instant-v1.0
GPT-3.5-Turbo
Claude-v1.3
GPT-4"
CONCLUSIONS,0.7662337662337663,"A1. When playing seller, most models can"
CONCLUSIONS,0.7680890538033395,improve at least one round.
CONCLUSIONS,0.7699443413729128,A2. Yet the deal success ratios continue to decrease
CONCLUSIONS,0.7717996289424861,over 5 rounds (mostly < 50% in the last round).
CONCLUSIONS,0.7736549165120594,"B1. When playing buyer, GPT models are better at"
CONCLUSIONS,0.7755102040816326,improving from AI feedback than Claude models.
CONCLUSIONS,0.7773654916512059,"B2. The deal success ratio continues to decrease, but"
CONCLUSIONS,0.7792207792207793,overall higher (mostly >50%) than criticizing sellers.
CONCLUSIONS,0.7810760667903525,"Figure 6: Performance of GPT and Claude models in multi-round games and their success rate of
getting a deal. A1 and A2: when playing the seller, most models can improve over multiple rounds.
Yet higher prices also mean that it is more likely the seller may break the deal, as shown in the
continuously decreasing curve of A2. B1 and B2: when playing buyer, claude-instant-v1.0
cannot improve over multiple rounds while others can. Again, a better buying price also comes with
a higher chance of running away from a deal. We see that GPT-4 achieves the best trade-off here: it
gets the best price over multiple rounds with a higher success rate of reaching a deal."
CONCLUSIONS,0.7829313543599258,Round 1. How about we meet in the middle at $15?
CONCLUSIONS,0.7847866419294991,"Round 2. This high-quality balloon is made from durable material, and I can offer a slight"
CONCLUSIONS,0.7866419294990723,discount at $18.
CONCLUSIONS,0.7884972170686456,"Round 3. Hi there! I hope you're having a fantastic day. This one-of-a-kind balloon, made from durable"
CONCLUSIONS,0.7903525046382189,"material, is priced at $20."
CONCLUSIONS,0.7922077922077922,Round 4: This special balloon can bring an extra touch of joy to any event and create lasting
CONCLUSIONS,0.7940630797773655,"memories, making it a worthwhile purchase at $20."
CONCLUSIONS,0.7959183673469388,"Round 5.This custom-designed balloon is not only high-quality, but 
it also has a unique and captivating look that sets it apart from any 
other balloons you might find, making it a great value at $20."
CONCLUSIONS,0.7977736549165121,Response length in number of characters
CONCLUSIONS,0.7996289424860853,Claude-v1.3 seller
CONCLUSIONS,0.8014842300556586,GPT-4 seller
CONCLUSIONS,0.8033395176252319,"Figure 7: The average response length increases as the model learns from multiple rounds. Here
we show examples of the seller’s response when being asked the buyer’s initial query “Would you
consider selling it for $10?” After multiple rounds of negotiation, the seller’s responses become more
verbose and word-tuned. Yet verbosity does not mean better strategy: claude-v1.3 is more verbose
(higher curve) than gpt-4, but it has a worse success rate and deal price (recall Fig. 6). This indicates
that gpt-4’s verbosity is more strategic."
CONCLUSIONS,0.8051948051948052,"of deal breaking. This suggests future research may consider global optimization for improving the
266"
CONCLUSIONS,0.8070500927643784,"overall gain over multiple rounds. We further show evidence of improved language from iterative
267"
CONCLUSIONS,0.8089053803339518,"AI feedback: in a multi-round game, one model may be less verbose than another, but be better
268"
CONCLUSIONS,0.8107606679035251,"word-tuned, thus more effective in getting a better deal.
269"
CONCLUSIONS,0.8126159554730983,"We believe our results have profound implications for AI research: on the positive side, it indicates
270"
CONCLUSIONS,0.8144712430426716,"the possibility of continuously improving language models with minimal human intervention. On
271"
CONCLUSIONS,0.8163265306122449,"the risky side, it might be more challenging to oversight the model behavior in our framework
272"
CONCLUSIONS,0.8181818181818182,"because models are acting autonomously, which calls for future alignment and safety research in the
273"
CONCLUSIONS,0.8200371057513914,"multi-agent game setting. Overall, we believe our work provides a meaningful initial exploration for
274"
CONCLUSIONS,0.8218923933209648,"large language models’ learning from game-playing and iterative AI feedback.
275"
REFERENCES,0.8237476808905381,"References
276"
REFERENCES,0.8256029684601113,"[1] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
What
277"
REFERENCES,0.8274582560296846,"learning algorithm is in-context learning? investigations with linear models. arXiv preprint
278"
REFERENCES,0.8293135435992579,"arXiv:2211.15661, 2022.
279"
REFERENCES,0.8311688311688312,"[2] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy
280"
REFERENCES,0.8330241187384044,"Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a
281"
REFERENCES,0.8348794063079777,"laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.
282"
REFERENCES,0.8367346938775511,"[3] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
283"
REFERENCES,0.8385899814471243,"Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai:
284"
REFERENCES,0.8404452690166976,"Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.
285"
REFERENCES,0.8423005565862709,"[4] Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile
286"
REFERENCES,0.8441558441558441,"Lukosuite, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable
287"
REFERENCES,0.8460111317254174,"oversight for large language models. arXiv preprint arXiv:2211.03540, 2022.
288"
REFERENCES,0.8478664192949907,"[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
289"
REFERENCES,0.849721706864564,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
290"
REFERENCES,0.8515769944341373,"few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
291"
REFERENCES,0.8534322820037106,"[6] Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale Lucas, Jonathan May, and Jonathan Gratch.
292"
REFERENCES,0.8552875695732839,"Casino: A corpus of campsite negotiation dialogues for automatic negotiation systems. arXiv
293"
REFERENCES,0.8571428571428571,"preprint arXiv:2103.15721, 2021.
294"
REFERENCES,0.8589981447124304,"[7] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language
295"
REFERENCES,0.8608534322820037,"models to self-debug. arXiv preprint arXiv:2304.05128, 2023.
296"
REFERENCES,0.862708719851577,"[8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
297"
REFERENCES,0.8645640074211502,"Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
298"
REFERENCES,0.8664192949907236,"Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
299"
REFERENCES,0.8682745825602969,"[9] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation
300"
REFERENCES,0.8701298701298701,"of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555,
301"
REFERENCES,0.8719851576994434,"2014.
302"
REFERENCES,0.8738404452690167,"[10] Cohere. Cohere command models. Cohere website, 2023. URL https://docs.cohere.com/
303"
REFERENCES,0.87569573283859,"docs/models.
304"
REFERENCES,0.8775510204081632,"[11] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn
305"
REFERENCES,0.8794063079777366,"in-context? language models secretly perform gradient descent as meta optimizers. arXiv
306"
REFERENCES,0.8812615955473099,"preprint arXiv:2212.10559, 2022.
307"
REFERENCES,0.8831168831168831,"[12] Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam Brown, Emily
308"
REFERENCES,0.8849721706864564,"Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan
309"
REFERENCES,0.8868274582560297,"Hu, et al. Human-level play in the game of diplomacy by combining language models with
310"
REFERENCES,0.8886827458256029,"strategic reasoning. Science, 378(6624):1067–1074, 2022.
311"
REFERENCES,0.8905380333951762,"[13] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based
312"
REFERENCES,0.8923933209647495,"prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022.
313"
REFERENCES,0.8942486085343229,"[14] Yao Fu, Litu Ou, Mingyu Chen, and Yuhao Wan. Measuring llms’ reasoning performance.
314"
REFERENCES,0.8961038961038961,"Github, 2023. URL https://github.com/FranxYao/chain-of-thought-hub.
315"
REFERENCES,0.8979591836734694,"[15] Amelia Glaese, Nat McAleese, Maja Tr˛ebacz, John Aslanides, Vlad Firoiu, Timo Ewalds,
316"
REFERENCES,0.8998144712430427,"Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment
317"
REFERENCES,0.9016697588126159,"of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.
318"
REFERENCES,0.9035250463821892,"[16] Google. Palm 2 technical report. ArXiv, 2023.
319"
REFERENCES,0.9053803339517625,"[17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
320"
REFERENCES,0.9072356215213359,"Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
321"
REFERENCES,0.9090909090909091,"Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
322"
REFERENCES,0.9109461966604824,"[18] AI21 Labs. Announcing jurassic-2 and task-specific apis. AI21 Blog, 2023. URL https:
323"
REFERENCES,0.9128014842300557,"//www.ai21.com/blog/introducing-j2.
324"
REFERENCES,0.9146567717996289,"[19] Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal?
325"
REFERENCES,0.9165120593692022,"end-to-end learning for negotiation dialogues. arXiv preprint arXiv:1706.05125, 2017.
326"
REFERENCES,0.9183673469387755,"[20] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard
327"
REFERENCES,0.9202226345083488,"Ghanem. Camel: Communicative agents for"" mind"" exploration of large scale language model
328"
REFERENCES,0.922077922077922,"society. arXiv preprint arXiv:2303.17760, 2023.
329"
REFERENCES,0.9239332096474954,"[21] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,
330"
REFERENCES,0.9257884972170687,"Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of
331"
REFERENCES,0.9276437847866419,"language models. arXiv preprint arXiv:2211.09110, 2022.
332"
REFERENCES,0.9294990723562152,"[22] Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. Languages are rewards: Hindsight finetuning
333"
REFERENCES,0.9313543599257885,"using human feedback. arXiv preprint arXiv:2302.02676, 2023.
334"
REFERENCES,0.9332096474953617,"[23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
335"
REFERENCES,0.935064935064935,"Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement
336"
REFERENCES,0.9369202226345084,"with self-feedback. arXiv preprint arXiv:2303.17651, 2023.
337"
REFERENCES,0.9387755102040817,"[24] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.
338"
REFERENCES,0.9406307977736549,"[25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
339"
REFERENCES,0.9424860853432282,"Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to
340"
REFERENCES,0.9443413729128015,"follow instructions with human feedback. Advances in Neural Information Processing Systems,
341"
REFERENCES,0.9461966604823747,"35:27730–27744, 2022.
342"
REFERENCES,0.948051948051948,"[26] Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and
343"
REFERENCES,0.9499072356215214,"Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv
344"
REFERENCES,0.9517625231910947,"preprint arXiv:2304.03442, 2023.
345"
REFERENCES,0.9536178107606679,"[27] Ethan Perez, Sam Ringer, Kamil˙e Lukoši¯ut˙e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig
346"
REFERENCES,0.9554730983302412,"Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model
347"
REFERENCES,0.9573283858998145,"behaviors with model-written evaluations. arXiv preprint arXiv:2212.09251, 2022.
348"
REFERENCES,0.9591836734693877,"[28] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan,
349"
REFERENCES,0.961038961038961,"Fei Huang, and Huajun Chen. Reasoning with language model prompting: A survey. arXiv
350"
REFERENCES,0.9628942486085343,"preprint arXiv:2212.09597, 2022.
351"
REFERENCES,0.9647495361781077,"[29] Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho,
352"
REFERENCES,0.9666048237476809,"and Ethan Perez. Training language models with natural language feedback. arXiv preprint
353"
REFERENCES,0.9684601113172542,"arXiv:2204.14146, 2022.
354"
REFERENCES,0.9703153988868275,"[30] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-
355"
REFERENCES,0.9721706864564007,"che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-
356"
REFERENCES,0.974025974025974,"tering the game of go with deep neural networks and tree search. nature, 529(7587):484–489,
357"
REFERENCES,0.9758812615955473,"2016.
358"
REFERENCES,0.9777365491651205,"[31] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur
359"
REFERENCES,0.9795918367346939,"Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of
360"
REFERENCES,0.9814471243042672,"go without human knowledge. nature, 550(7676):354–359, 2017.
361"
REFERENCES,0.9833024118738405,"[32] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mord-
362"
REFERENCES,0.9851576994434137,"vintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient
363"
REFERENCES,0.987012987012987,"descent. arXiv preprint arXiv:2212.07677, 2022.
364"
REFERENCES,0.9888682745825603,"[33] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani
365"
REFERENCES,0.9907235621521335,"Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large
366"
REFERENCES,0.9925788497217068,"language models. arXiv preprint arXiv:2206.07682, 2022.
367"
REFERENCES,0.9944341372912802,"[34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
368"
REFERENCES,0.9962894248608535,"Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
369"
REFERENCES,0.9981447124304267,"arXiv:2201.11903, 2022.
370"
