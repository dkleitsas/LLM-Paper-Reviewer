Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004524886877828055,"Principal component analysis (PCA) is an important method for dimensionality
reduction in data science and machine learning. But, it is expensive for large
matrices when only a few principal components are needed. Existing fast PCA al-
gorithms typically assume the user will supply the number of components needed,
but in practice, they may not know this number beforehand. Thus, it is important
to have fast PCA algorithms depending on a tolerance. For m × n matrices where
a few principal components explain most of the variance in the data, we develop
one such algorithm that runs in O(mnl) time, where l ≪min(m, n) is a small
multiple of the number of principal components. We provide approximation error
bounds that are within a constant factor away from optimal and demonstrate its
utility with data from a variety of applications."
INTRODUCTION,0.00904977375565611,"1
INTRODUCTION"
THE TRUNCATED SVD AND PCA,0.013574660633484163,"1.1
THE TRUNCATED SVD AND PCA"
THE TRUNCATED SVD AND PCA,0.01809954751131222,"Let A be an m×n real matrix and A = UΣV T its singular value decomposition (SVD). The rank-k
truncated SVD (rank-k TSVD) of A is the matrix Ak := UkΣkV T
K , where Uk and Vk are the first
k columns of U and V , respectively, and Σk is the leading k × k block of Σ. The columns uj and
vj of Uk and Vk are the left and right singular vectors of A, respectively, and the diagonal entries
σ1(A) ≥· · · ≥σk(A) ≥0 are the singular values of A."
THE TRUNCATED SVD AND PCA,0.02262443438914027,"One common use of TSVD is in principal component analysis (PCA). This is a dimensionality
reduction technique that aims to find the directions in which the data varies the most. It turns out
that these directions are given by the top k right singular vectors vj, 1 ≤j ≤k. Projecting the
original data onto these directions then transforms the data from a high-dimensional space into a
lower-dimensional one. Typically, k is chosen so that these principal directions explain a certain
amount of variance in the data. For example, if the user wanted to explain 99% of the variance, they
would choose k so that Pk
i=1 σi(A)2/ Pn
i=1 σi(A)2 ≥0.99. PCA is used to compress data in a
variety of settings such as images, training data for machine learning algorithms, and even neural
network weight matrices (Xue et al., 2013)."
THE TRUNCATED SVD AND PCA,0.027149321266968326,"In this work, we will be interested in a slightly different version of PCA. Let ε be a user-prescribed
tolerance, and instead choose k so that σk(A) ≥ε ≥σk+1(A). Since the ith principal component
explains σi(A)2/ Pn
i=1 σi(A)2 fraction of the total variance, we are essentially ignoring any com-
ponents that explain less than ε2/ Pn
i=1 σi(A)2 fraction of the total variance. This can be interpreted
as discarding principal components corresponding to noise, where ε describes the size of the noise."
THE TRUNCATED SVD AND PCA,0.03167420814479638,"Despite its utility and importance, the main drawback of using TSVD for PCA is that it is expen-
sive, especially when the user needs only the top few singular values/vectors. Thus, a large body
of research has been devoted to finding faster ways of computing it without sacrificing too much
accuracy."
PRIOR WORK,0.03619909502262444,"1.2
PRIOR WORK"
PRIOR WORK,0.04072398190045249,"The literature on fast, approximate TSVD algorithms typically assume the user knows what rank k
to use. Recent work uses randomization to reduce the run time while still maintaining a high level of"
PRIOR WORK,0.04524886877828054,Under review as a conference paper at ICLR 2022
PRIOR WORK,0.049773755656108594,"accuracy. See, for example, Rokhlin et al. (2010) and Halko et al. (2010) and the references therein
for typical examples of these types of algorithms. In Musco & Musco (2015), the authors present a
randomized algorithm based on block Krylov subspace methods to compute an approximate TSVD.
For a matrix A, rank k, and tolerance ε, the algorithm produces a matrix Z whose columns approxi-
mate the top k left singular vectors of A and such that
A −ZZT A

2 ≤(1 + ε) ∥A −Ak∥2. They
also prove stronger bounds on the quality of the singular vectors, which is important for PCA. This
algorithm is especially suited to sparse matrices, which can be multiplied quickly."
PRIOR WORK,0.05429864253393665,"In some cases, the user may not know ahead of time what k to use, so it is useful to consider
algorithms which accept a desired precision ε as input rather than rank. Algorithms in this vein
incrementally build a matrix Q with orthogonal columns and another matrix B until ∥A −QB∥< ε.
Typically, the number of columns of Q (or the number of rows of B) is quite small so that QB is a
compact approximation of the original data A. An approximate TSVD of A can then be produced
from the SVD of B. Recent work again uses randomization to reduce the run time. We present a
prototypical example of this style of algorithm in Algorithm 1 (Yu et al., 2018). See Halko et al.
(2010) and Martinsson & Voronin (2016) for more examples. While these algorithms guarantee a
small approximation error, there are no guarantees on the accuracy of the singular values or vectors.
We will compare the accuracy for Algorithm 1 to the proposed algorithm in Experiments."
PRIOR WORK,0.058823529411764705,Algorithm 1 The randQB EI algorithm for the fixed-precision problem
PRIOR WORK,0.06334841628959276,"Input: an m × n matrix A; desired accuracy tolerance ε; block size b
Output: Q, B such that ∥A −QB∥F < ε
Q = [ ]; B = [ ];
E = ∥A∥2
F
for i = 1, 2, 3, . . . do"
PRIOR WORK,0.06787330316742081,"Ωi = randn(n, b)
Qi = orth(AΩi −Q(BΩi))
Qi = orth(Qi −Q(QT Qi))
Bi = QT
i A
Q = [Q, Qi]"
PRIOR WORK,0.07239819004524888,"B =

B
Bi "
PRIOR WORK,0.07692307692307693,"E = E −∥Bi∥2
F
if E < ε2 then stop
end for"
OUR WORK,0.08144796380090498,"1.3
OUR WORK"
OUR WORK,0.08597285067873303,"In this work, we propose an algorithm that, for a matrix A, accuracy tolerance δ, and singular value
tolerance ε, produces an approximate TSVD ˜Ak satisfying:"
OUR WORK,0.09049773755656108,"1. The rank k of ˜Ak does not exceed the true rank of A, determined by the tolerance ε as
described above,"
OUR WORK,0.09502262443438914,"2. σj( ˜Ak) ≥(1 −δ)σj(A) for 1 ≤j ≤k, 3."
OUR WORK,0.09954751131221719,"A −˜Ak

2 ≤1+δ"
OUR WORK,0.10407239819004525,"1−δε ≈(1 + 2δ)ε, and"
OUR WORK,0.1085972850678733,"4. If k coincides with the true rank, then
A −˜Ak

2 ≤(1+δ)σk+1(A) = (1+δ) ∥A −Ak∥2,
i.e. the truncation error is a factor of 1 + δ from optimal."
OUR WORK,0.11312217194570136,"These properties are verified in the Appendix. The algorithm thus yields a high-quality approxi-
mation to TSVD and can be used in applications as an approximate PCA. The algorithm is fast for
matrices whose singular values decay quickly when ε is set so that k will be relatively small."
OUR WORK,0.11764705882352941,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.12217194570135746,"2
PRELIMINARIES"
PRELIMINARIES,0.12669683257918551,"Unless otherwise stated, we will consider matrices with more rows than columns. For a matrix with
more columns than rows, apply the algorithm to its transpose."
FLIP-FLOP SPECTRUM REVEALING QR,0.13122171945701358,"2.1
FLIP-FLOP SPECTRUM REVEALING QR"
FLIP-FLOP SPECTRUM REVEALING QR,0.13574660633484162,"The proposed algorithm is essentially a tolerance-based version of Flip-Flop Spectrum Revealing
QR (FFQR) (Feng et al., 2019). For a matrix A and integers k ≤l, FFQR produces an approximation
to the rank-k TSVD Ak whose accuracy depends on the ratio σk+1(A)/σl+1(A). Thus, if A has
rapidly decaying singular values, FFQR will be close to TSVD."
FLIP-FLOP SPECTRUM REVEALING QR,0.14027149321266968,"FFQR is computed as follows. Let A be an m × n matrix (m ≥n) and k ≤l. Perform l steps of
Randomized QR with Column Pivoting (RQRCP) (Duersch & Gu, 2017) to get the factorization"
FLIP-FLOP SPECTRUM REVEALING QR,0.14479638009049775,"AΠ = QR = Q

R11
R12
0
R22 
,"
FLIP-FLOP SPECTRUM REVEALING QR,0.1493212669683258,"where Π is an n × n permutation matrix, Q is an m × m orthogonal matrix, R is m × n, and R11 is
an l × l upper triangular matrix."
FLIP-FLOP SPECTRUM REVEALING QR,0.15384615384615385,"The next phase of FFQR involves performing extra “spectrum-revealing” column swaps on R
and using Givens rotations to restore its upper trapezoidal form. These swaps ensure ∥R22∥2 =
O(σl(A)). See Xiao et al. (2017) for more details."
FLIP-FLOP SPECTRUM REVEALING QR,0.1583710407239819,"Next, perform l steps of QR on RT to get"
FLIP-FLOP SPECTRUM REVEALING QR,0.16289592760180996,"RT = PLT = (P1
P2)

L11
0
L21
L22 T
,"
FLIP-FLOP SPECTRUM REVEALING QR,0.167420814479638,"where P is an n × n orthogonal matrix, P1 is its leading l columns, L is an m × n matrix, and L11
is l × l lower triangular. Putting the above together yields"
FLIP-FLOP SPECTRUM REVEALING QR,0.17194570135746606,"A = QRΠT = Q

L11
0
L21
L22"
FLIP-FLOP SPECTRUM REVEALING QR,0.17647058823529413,"
P T ΠT ."
FLIP-FLOP SPECTRUM REVEALING QR,0.18099547511312217,"Discard L22 (as in truncated QRCP) and approximate

L11
L21"
FLIP-FLOP SPECTRUM REVEALING QR,0.18552036199095023,"
with its rank-k TSVD ˆUk ˆΣk ˆV T
k :"
FLIP-FLOP SPECTRUM REVEALING QR,0.19004524886877827,"Q

L11
0
L21
L22"
FLIP-FLOP SPECTRUM REVEALING QR,0.19457013574660634,"
P T ΠT ≈Q

L11
L21"
FLIP-FLOP SPECTRUM REVEALING QR,0.19909502262443438,"
P T
1 ΠT ≈Q( ˆUk ˆΣk ˆV T
k )P T
1 ΠT"
FLIP-FLOP SPECTRUM REVEALING QR,0.20361990950226244,"Setting ˜Uk := Q ˆUk, ˜Σk := ˆΣk, ˜Vk := ΠP1 ˆVk gives the rank-k approximation A ≈˜Uk ˜Σk ˜V T
k ."
FLIP-FLOP SPECTRUM REVEALING QR,0.2081447963800905,"In Feng et al. (2019), the authors prove the following bounds for FFQR. Given ε > 0 and g > 1,"
FLIP-FLOP SPECTRUM REVEALING QR,0.21266968325791855,"there are matrix-dependent quantities g1 ≤
q"
FLIP-FLOP SPECTRUM REVEALING QR,0.2171945701357466,"1+ε
1−ε, g2 ≤g, τ ≤g1g2
p"
FLIP-FLOP SPECTRUM REVEALING QR,0.22171945701357465,"(l + 1)(n −l), and ˆτ ≤"
FLIP-FLOP SPECTRUM REVEALING QR,0.22624434389140272,"g1g2
p"
FLIP-FLOP SPECTRUM REVEALING QR,0.23076923076923078,"l(n −l) such that for 1 ≤j ≤k,"
FLIP-FLOP SPECTRUM REVEALING QR,0.23529411764705882,"σj(Σk) ≥
σj(A)"
S,0.2398190045248869,"4
s"
S,0.24434389140271492,"1 + min

2ˆτ 4, τ 4(2 + 4ˆτ 4)

σl+1(A)"
S,0.248868778280543,"σj(A)
4"
S,0.25339366515837103,"and
A −˜Uk ˜Σk ˜V T
k

2 ≤σk+1(A)
4
s"
S,0.2579185520361991,"1 + 2τ 4
 σl+1(A)"
S,0.26244343891402716,"σk+1(A) 4
."
THE QLP DECOMPOSITION,0.2669683257918552,"2.2
THE QLP DECOMPOSITION"
THE QLP DECOMPOSITION,0.27149321266968324,"The basis of FFQR is the QLP decomposition (Stewart, 1999). Let A be an m × n matrix. Perform
QRCP on A to obtain AΠ = QR and then perform QRCP on RT to get RT Π1 = PLT , where"
THE QLP DECOMPOSITION,0.27601809954751133,Under review as a conference paper at ICLR 2022
THE QLP DECOMPOSITION,0.28054298642533937,"L is lower triangular. Putting these together yields A = QΠ1LP T ΠT . This is the pivoted QLP
decomposition of A. Stewart observed that the diagonal entries Lii of L closely track the singular
values of A."
THE QLP DECOMPOSITION,0.2850678733031674,"For the proposed algorithm, we choose not to pivot when factoring RT . In this case, Lii will not
track σi(A) as well. We will discuss a partial remedy for this below. The advantage of not pivoting
is that, just as in FFQR, we do not have to finish computing R before computing L. Once we have
performed l steps of QRCP on A, we can compute the first l rows of L and thus have access to its
first l diagonal entries."
THE QLP DECOMPOSITION,0.2895927601809955,"3
A FAST, APPROXIMATE, TOLERANCE-BASED PCA ALGORITHM"
BLOCKED FFQR,0.29411764705882354,"3.1
BLOCKED FFQR"
BLOCKED FFQR,0.2986425339366516,"Since we do not know what l is beforehand, we compute R and L incrementally in blocks. Select a
block size b and perform b steps of RQRCP to get"
BLOCKED FFQR,0.3031674208144796,AΠ1 = Q1
BLOCKED FFQR,0.3076923076923077,"R(b)
11
R(b)
12
0
R(b)
22 ! ,"
BLOCKED FFQR,0.31221719457013575,"where R(b)
11 is b × b upper triangular. The first b rows of R are essentially done since subsequent
steps of RQRCP will only permute the columns of R(b)
12 . Perform QR on them (to keep the notation
simple, we write this as an LQ factorization):

R(b)
11
R(b)
12

= (L11
0) P T
1 ,"
BLOCKED FFQR,0.3167420814479638,"where L11 is b × b lower triangular, and P1 is orthogonal. We have just computed the first b rows
of L and know the first b diagonal entries. For the next block, continue RQRCP for another b steps.
The permutation matrix Π2 in this block will affect only columns b + 1 through n, leaving the first"
BLOCKED FFQR,0.3212669683257919,"b columns untouched. Thus, Π2 can be written in block form as Π2 =
Ib
0
0
˜Π2"
BLOCKED FFQR,0.3257918552036199,"
, where Ib is the"
BLOCKED FFQR,0.33031674208144796,b × b identity matrix and ˜Π2 is an (n −b) × (n −b) permutation matrix. We now have
BLOCKED FFQR,0.334841628959276,AΠ1Π2 = Q2Q1 
BLOCKED FFQR,0.3393665158371041,"

R(b)
11
R(b)
12 ˜Π2
0
R(2b)
11
R(2b)
12
0
0
R(2b)
22  
,"
BLOCKED FFQR,0.3438914027149321,"where R(2b)
11
is b × b upper triangular. Since the first b rows have changed, we must account for this
in the previous LQ:

R(b)
11
R(b)
12 ˜Π2

=

R(b)
11
R(b)
12

Π2 = (L11
0) P T
1 Π2."
BLOCKED FFQR,0.34841628959276016,"Now apply the matrix ΠT
2 P1 to the newly completed rows

0
R(2b)
11
R(2b)
12

and perform LQ on
the last n −b columns to get

0
R(2b)
11
R(2b)
12

ΠT
2 P1 = (L21
L22
0) P T
2 ,"
BLOCKED FFQR,0.35294117647058826,where L22 is b × b lower triangular.
BLOCKED FFQR,0.3574660633484163,The orthogonal matrix P2 affects only the last n −b columns and can therefore be written in block
BLOCKED FFQR,0.36199095022624433,"form as P2 =
Ib
0
0
˜P2"
BLOCKED FFQR,0.3665158371040724,"
. Hence, (L11
0) = (L11
0) P T
2 and"
BLOCKED FFQR,0.37104072398190047,"R(b)
11
R(b)
12 ˜Π2
0
R(2b)
11
R(2b)
12 !"
BLOCKED FFQR,0.3755656108597285,"=

L11
0
0
L21
L22
0"
BLOCKED FFQR,0.38009049773755654,"
P T
2 P T
1 Π2,"
BLOCKED FFQR,0.38461538461538464,"showing that we have computed the first 2b rows of L. We can continue this procedure, computing
b rows of L at a time. Once we decide to stop, we finish the remaining rows of L by applying the"
BLOCKED FFQR,0.3891402714932127,Under review as a conference paper at ICLR 2022
BLOCKED FFQR,0.3936651583710407,"orthogonal matrices from all previous LQ factorizations to the last rows of R. For example, if we
wanted to stop after 2 blocks, apply ΠT
2 P1P2 to

0
0
R(2b)
22

to get

0
0
R(2b)
22

ΠT
2 P1P2 = (L31
L32
L33)"
BLOCKED FFQR,0.39819004524886875,and the partial QLP decomposition
BLOCKED FFQR,0.40271493212669685,AΠ1Π2 = Q2Q1 
BLOCKED FFQR,0.4072398190045249,"

R(b)
11
R(b)
12 ˜Π2
0
R(2b)
11
R(2b)
12
0
0
R(2b)
22 "
BLOCKED FFQR,0.4117647058823529,"
= Q2Q1"
BLOCKED FFQR,0.416289592760181,"L11
0
0
L21
L22
0
L31
L32
L33 !"
BLOCKED FFQR,0.42081447963800905,"P T
2 P T
1 Π2."
BLOCKED FFQR,0.4253393665158371,"Afterwards, spectrum-revealing swaps can be performed if desired. For each swap and upper-
trapezoidal restoration, some nonzero entries will appear above the diagonal in L. These are easily
eliminated with Givens rotations."
DETERMINING L,0.4298642533936652,"3.2
DETERMINING l"
DETERMINING L,0.4343891402714932,"We now derive a criterion to determine when to stop factoring in blocked FFQR and to find l. Let
ε be the tolerance parameter, and define the rank k by σk+1(A) ≤ε ≤σk(A). One could use the
bounds derived in Feng et al. (2019), using the diagonal entries of L to estimate the singular values
of A and stopping when σl+1(A)/σk+1(A) is sufficiently small. However, the dimension-dependent
bounds for τ and ˆτ are impractical, so we will use a different bound."
DETERMINING L,0.43891402714932126,"In Feng et al. (2019), the authors prove that σj(A)4 ≤σj(Σk)4+2 ∥R22∥4
2 , 1 ≤j ≤k. Rearranging
this inequality gives"
DETERMINING L,0.4434389140271493,"σj(Σk) ≥σj(A)
4
s"
DETERMINING L,0.4479638009049774,"1 −2∥R22∥4
2
σj(A)4 ,
1 ≤j ≤k."
DETERMINING L,0.45248868778280543,They also prove the following bound on the truncation error:
DETERMINING L,0.45701357466063347,"A −˜Uk ˜Σk ˜V T
k

2 ≤σk+1(A)
4
s"
DETERMINING L,0.46153846153846156,"1 + 2 ∥R22∥4
2
σk+1(A)4 .
(1)"
DETERMINING L,0.4660633484162896,"These bounds hold even without spectrum-revealing swaps. So, if ∥R22∥2 /σk+1(A), is small, then
the leading k singular values of A will be revealed up to a certain number of digits and ˜Uk ˜Σk ˜V T
k will
be a nearly optimal rank-k approximation. In practice, the above two bounds are sufficient because
∥R22∥2 = O(σl(A)) already, without extra swaps. The earlier bounds still have theoretical value in
that they show the algorithm works well when A has rapidly decaying singular values."
DETERMINING L,0.47058823529411764,"The factors
4q"
DETERMINING L,0.4751131221719457,"1 −2 ∥R22∥4
2
σj(A)4 and
4q"
DETERMINING L,0.4796380090497738,"1 + 2 ∥R22∥4
2
σk+1(A)4 are equal to 1 −1"
DETERMINING L,0.4841628959276018,"2
∥R22∥4
2
σj(A)4 and 1 + 1"
DETERMINING L,0.48868778280542985,"2
∥R22∥4
2
σk+1(A)4 ,"
DETERMINING L,0.49321266968325794,"respectively, up to first order. Introduce an accuracy parameter δ, and say we have 1"
DETERMINING L,0.497737556561086,"2
∥R22∥4
2
σk+1(A)4 ≤δ."
DETERMINING L,0.502262443438914,"Then we have σj(˜Σk) ≥σj(A)(1 −δ), 1 ≤j ≤k, and
A −˜Uk ˜Σk ˜V T
k

2 ≤σk+1(A)(1 + δ) up to
first order. This means that ≈−log δ digits of the top k singular values of A and optimal truncation
error have been computed correctly. We can rewrite 1"
DETERMINING L,0.5067873303167421,"2
∥R22∥4
2
σk+1(A)4 ≤δ as ∥R22∥2 ≤σk+1(A)
4√"
DETERMINING L,0.5113122171945701,"2δ.
This is the tolerance-based criterion to determine l."
DETERMINING L,0.5158371040723982,"3.2.1
ESTIMATING σk+1 AND ∥R22∥2"
DETERMINING L,0.5203619909502263,"To estimate ∥R22∥2 and σk+1(A) accurately, we use Stewart’s observation that the diagonal entries
Lii of L closely track the singular values σi(A) of A. As stated above, Lii will not track σi(A) as
well because we are not pivoting when factoring RT . A partial remedy is simply to sort the Lii’s.
We show below that the resulting tracking behavior is similar in quality to that of fully pivoted QLP."
DETERMINING L,0.5248868778280543,"Let L(j) be the j-th largest diagonal entry of L in magnitude, i.e.
L(1) ≥
L(2) ≥· · · ≥
L(n). In
light of Stewart’s observation, we will assume that there are constants α and β such that α
L(j) ≤
σj(A) ≤β
L(j), 1 ≤j ≤n. The values of α and β will be estimated empirically below. A simple"
DETERMINING L,0.5294117647058824,Under review as a conference paper at ICLR 2022
DETERMINING L,0.5339366515837104,"way to interpret these inequalities is that for each diagonal entry Ljj, there is a singular value of
A in the interval [α |Ljj| , β |Ljj|]. Consider {Ljj : β |Ljj| ≤ε}. This just corresponds to all the
intervals [α |Ljj| , β |Ljj|] contained in (−∞, ε]. For each Ljj in the set, there is a singular value
σi(A) in the corresponding interval. Thus σi(A) ≤ε. Since σk+1(A) is the largest singular value of
A less than or equal to ε, we must have σi(A) ≤σk+1(A), which then implies α |Ljj| ≤σk+1(A).
This yields a lower bound on σk+1(A), namely max{α |Ljj| : β |Ljj| ≤ε}."
DETERMINING L,0.5384615384615384,"Since we will not know all the Ljj’s, we can obtain only a sub-optimal lower bound sk+1 on
σk+1(A). Initialize sk+1 = 0. After i blocks of blocked FFQR, update sk+1 = max{α |Ljj| :
β |Ljj| ≤ε and j ≤ib}."
DETERMINING L,0.5429864253393665,"To estimate ∥R22∥2 = σ1(R22), we will use the first diagonal entry L11 of L in the fully pivoted
QLP factorization. First, consider a general matrix A, and perform QRCP: AΠ = QR. Then in
fully pivoted QLP, |L11| is just the largest row norm of R. As noted in Stewart (1999), the largest
row of R is usually among the first few rows. Thus we can estimate ∥A∥2 using max
1≤i≤q ∥R(i, :)∥2,"
DETERMINING L,0.5475113122171946,for some small integer q. This requires only q steps of QRCP.
DETERMINING L,0.5520361990950227,"We can apply this idea to estimate ∥R22∥2. After i steps of QRCP on A, the R factor has the form
 
R(i)
1
0
R(i)
22 !"
DETERMINING L,0.5565610859728507,", where R(i)
1 is i×n upper triangular. The R factor in the QRCP factorization of R(i)
22 is"
DETERMINING L,0.5610859728506787,"just R(n)
1 (i+1 : m, i+1 : n). Thus, QRCP-factoring A automatically yields the QRCP factorizations
of all the trailing blocks R(i)
22 . Using the 2-norm estimation scheme in the previous paragraph, after"
DETERMINING L,0.5656108597285068,"j steps of QRCP, we have the upper bound
R(i)
22

2 ≤β
max
i≤ι≤i+q−1"
DETERMINING L,0.5701357466063348,"R(j)
1 (ι, :)

2 := β
R(i)
22

j,q for"
DETERMINING L,0.5746606334841629,1 ≤i ≤j −q + 1.
DETERMINING L,0.579185520361991,"Putting these estimates together will give us the final stopping criterion. After each block, we first
update sk+1 with the newly computed Ljj’s and then check if
R(i)
22

j,q ≤1"
DETERMINING L,0.583710407239819,"β sk+1
4√"
DETERMINING L,0.5882352941176471,2δ for some i.
DETERMINING L,0.5927601809954751,The smallest i for which this inequality holds will be l + 1.
DETERMINING L,0.5972850678733032,"Algorithm 2 Approximate, tolerance-based PCA"
DETERMINING L,0.6018099547511312,"Inputs: A, tolerance ε, accuracy δ, block size b, number of rows q, oversampling size p for
RQRCP
Outputs: Rank k, ˜Uk, ˜Σk, ˜Vk
c ←0, sk+1 ←0
while c < n do"
DETERMINING L,0.6063348416289592,"Perform steps c + 1 to c + b of RQRCP on A; update Q, R, and Π
Compute rows c + 1 to c + b of L; update P
for j = c + 1 : c + b do"
DETERMINING L,0.6108597285067874,if |Ljj| ≤ε/β and α |Ljj| ≥sk+1 then
DETERMINING L,0.6153846153846154,"sk+1 ←α |Ljj|
end if
end for
for i = 1 : c + b −q + 1 do"
DETERMINING L,0.6199095022624435,"if
R(i)
22

c+b,q ≤1"
DETERMINING L,0.6244343891402715,"β sk+1
4√"
DETERMINING L,0.6289592760180995,2δ then
DETERMINING L,0.6334841628959276,"l ←i −1
exit while loop
end if
end for
c ←c + b
end while
Compute rows c + b + 1 to m of L.
Compute TSVD ˆUk ˆΣk ˆVk of L(:, 1 : l), where k satisfies σk(L(:, 1 : l)) ≥ε ≥σk+1(L(:, 1 : l))."
DETERMINING L,0.6380090497737556,"˜Uk ←Q ˆUk, ˜Σk ←ˆΣk, ˜Vk ←ΠP1 ˆVk"
DETERMINING L,0.6425339366515838,Under review as a conference paper at ICLR 2022
DETERMINING L,0.6470588235294118,Figure 1: Singular value distributions of the test matrices
DETERMINING L,0.6515837104072398,Table 1: α and β values for the test matrices under the three schemes.
DETERMINING L,0.6561085972850679,"Unpivoted
Sorted
Pivoted
α
β
α
β
α
β
Random
0.710
1.39
0.742
1.33
0.745
1.32
Data
0.523
2.75
0.793
1.17
0.817
1.17
Video
0.321
2.85
0.838
1.85
0.840
1.58
Kernel
0.635
1.60
0.802
1.32
0.817
1.31"
EXPERIMENTS,0.6606334841628959,"4
EXPERIMENTS"
EXPERIMENTS,0.665158371040724,We use the following test matrices for our experiments:
EXPERIMENTS,0.669683257918552,"1. A random 3000×3000 matrix with singular values decaying geometrically from 1 down to
10−12. We generate a 3000×3000 matrix with entries from a standard normal distribution,
compute its SVD UΣV T , and replace the diagonal of Σ with the desired singular value
distribution."
EXPERIMENTS,0.6742081447963801,"2. A 2003×2003 data matrix (bcsstk13) from the SuiteSparse matrix collection (Davis & Hu,
2011). This matrix arises from a computational fluid dynamics problem. Its singular values
decay from ≈1012 down to ≈102."
EXPERIMENTS,0.6787330316742082,"3. A 19200×5322 matrix generated from a video from the UCF-Crime dataset (Sultani et al.,
2018). The original video was a 240 × 320 RGB video consisting of 5322 frames. We
resized the video by half to 120 × 160, converted it to grayscale, flattened each frame into
a 19200 × 1 column vector, and then stacked these horizontally to form the final matrix."
EXPERIMENTS,0.6832579185520362,"4. A 5000×5000 kernel matrix generated from 5000 data points from the MNIST handwritten
digits dataset (Lecun et al., 1998). We used the kernel function k(x, x′) = e−γ∥x−x′∥
2
,
where γ = 1/(median of pairwise distances between data points)2."
EXPERIMENTS,0.6877828054298643,The singular values of each test matrix are plotted in Figure 1.
EXPERIMENTS,0.6923076923076923,"4.1
ESTIMATING α AND β"
EXPERIMENTS,0.6968325791855203,"For each matrix A, we tested three singular value estimation schemes. See Table 1. For the first two
columns (“Unpivoted”), we ran RQRCP to get AΠ = QR and then QR-factored RT = PL. We
recorded the minimum (α) and maximum (β) values of σi(A)/ |Lii|. For the second two (“Sorted”),
we recorded the minimum and maximum values of σi(A)/
L(i), where L(i) is the ith largest diago-
nal entry of L in magnitude. For the last two columns (“Pivoted”), we QRCP-factored RT Π1 = PL
and then recorded the minimum and maximum values of σi(A)/ |Lii|."
EXPERIMENTS,0.7013574660633484,"We observed that the tracking behavior can break down when σi(A) is smaller than machine preci-
sion and thus ignored ratios corresponding to such σi(A) when computing the minimum and max-
imum values. Therefore, it is recommended that the tolerance ε be set at least a small factor above
machine epsilon."
EXPERIMENTS,0.7058823529411765,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.7104072398190046,"Figure 2: Relative singular value errors for the test matrices.
Our algorithm is the bold line,
randQB EI is the thin one."
EXPERIMENTS,0.7149321266968326,"“Sorted” and “Pivoted” have similar α and β values, with the latter slightly better overall, while
“Unpivoted” tends to be worse than the other two. Based on the middle two columns, it seems that
α ≈0.7 and β ≈2 are reasonable values."
COMPARISON TO TSVD AND RANDQB EI,0.7194570135746606,"4.2
COMPARISON TO TSVD AND RANDQB EI"
COMPARISON TO TSVD AND RANDQB EI,0.7239819004524887,"Here we compare the proposed algorithm to TSVD and randQB EI. Tests were coded in Fortran and
run on a laptop with a 2.00 GHz Intel i7-4510U CPU with 16.0 GB of RAM."
COMPARISON TO TSVD AND RANDQB EI,0.7285067873303167,"First, we compare the proposed algorithm to TSVD. To compute the latter, the LAPACK routine
dgesdd routine is used to compute the full SVD, which is then truncated based on the tolerance ε.
For the random, data, and kernel matrices, tolerances corresponding to 99% explained variance are
chosen. For the video matrix, we choose one corresponding to 99.9% explained variance because
the first principal component already accounts for 99% of the variance."
COMPARISON TO TSVD AND RANDQB EI,0.7330316742081447,"It is not so simple to translate a Frobenius norm tolerance to a corresponding 2-norm tolerance,
but we find that for matrices with geometrically decaying singular values, 99% explained variance
roughly corresponds to a tolerance of 0.1 ∥A∥2. The 2-norm of A can be estimated after the first
block of FFQR. For experimental purposes, we computed the SVD of each matrix and then selected
the tolerance."
COMPARISON TO TSVD AND RANDQB EI,0.7375565610859729,"We set δ = 10−4 for all test matrices because machine learning algorithms typically only need a
few digits of accuracy. But the larger singular values are computed with more accuracy because the
accuracy of the jth singular value depends on ∥R22∥2 /σj(A). Finally, for all matrices, we set the
block size b = 64, number of rows q = 5, and oversampling size p = 5."
COMPARISON TO TSVD AND RANDQB EI,0.7420814479638009,"The results are listed in Table 2. The proposed algorithm detects the rank k correctly for each test
matrix and is much faster than dgesdd. The column REOTE contains the relative error in the optimal"
COMPARISON TO TSVD AND RANDQB EI,0.746606334841629,"truncation error

∥A−˜
Ak∥2
∥A−Ak∥2 −1
. This is always bounded above by δ, but we see that in practice this"
COMPARISON TO TSVD AND RANDQB EI,0.751131221719457,relative error is much smaller.
COMPARISON TO TSVD AND RANDQB EI,0.755656108597285,"Figure 2 plots the relative errors in the approximate singular values
1 −σj(˜Σk)"
COMPARISON TO TSVD AND RANDQB EI,0.7601809954751131,"σj(A)
, 1 ≤j ≤k for
each of the test matrices. These are again bounded by δ. As expected, the larger singular values
are computed more accurately. We also plot the relative errors in the singular values computed by
randQB EI, using block size 64; ε =
√"
COMPARISON TO TSVD AND RANDQB EI,0.7647058823529411,"0.01 ∥A∥F for the random, data, and kernel matrices; and
ε =
√"
COMPARISON TO TSVD AND RANDQB EI,0.7692307692307693,"0.001 ∥A∥F for the video matrix. The authors of randQB EI also include a power parameter
P in their implementation. We set P = 1 as in their paper."
COMPARISON TO TSVD AND RANDQB EI,0.7737556561085973,"Although we did not analyze the error in the singular vectors (in general, the error depends on
the gap between the singular values), we compute the angles θ(vj, ˜vj) between the right singular
vectors and their approximations for both our algorithm and randQB EI, and plot them in Figure
3. For our algorithm, the angles are all quite small, so the proposed algorithm finds good-quality
approximations to the true singular vectors/principal directions."
COMPARISON TO TSVD AND RANDQB EI,0.7782805429864253,"Accuracy-wise, our algorithm performs better than randQB EI, except on the kernel matrix. We
found that setting P = 0 causes the accuracy of randQB EI to drop below ours. The first few"
COMPARISON TO TSVD AND RANDQB EI,0.7828054298642534,Under review as a conference paper at ICLR 2022
COMPARISON TO TSVD AND RANDQB EI,0.7873303167420814,"Figure 3: Angle between the top k right singular vectors vj and their approximations ˜vj for the test
matrices. Our algorithm is the bold line, randQB EI is the thin one."
COMPARISON TO TSVD AND RANDQB EI,0.7918552036199095,Table 2: Comparison of the proposed algorithm to TSVD.
COMPARISON TO TSVD AND RANDQB EI,0.7963800904977375,"dgesdd
Proposed algorithm
Matrix
ε
k
Time(s)
l
k
Time (s)
REOTE
Speed-up
Random
1 × 10−1
250
14.9
804
250
2.76
1.11 × 10−16
5.39×
Data
1 × 1011
128
4.41
759
128
1.38
2.22 × 10−16
3.19×
Video
5.6 × 103
36
161
1692
36
50.2
9.55 × 10−15
3.21×
Kernel
7.68 × 101
7
72.7
370
7
2.75
1.11 × 10−15
26.4×"
COMPARISON TO TSVD AND RANDQB EI,0.8009049773755657,"singular values of the kernel matrix are extremely large compared to the rest; thus, performing even
just one power iteration effectively enhances the accuracy of randQB EI on this matrix."
CONCLUSION,0.8054298642533937,"5
CONCLUSION"
CONCLUSION,0.8099547511312217,"In this work, we developed an efficient algorithm for computing an approximate truncated SVD.
In contrast to much of the literature, this algorithm truncates according to a tolerance rather than
a fixed rank. We have also demonstrated that it provides high-quality approximations to both the
singular values and vectors of the original matrix, thus making it suitable for use in applications as
an approximate PCA."
CONCLUSION,0.8144796380090498,ACKNOWLEDGMENTS
CONCLUSION,0.8190045248868778,We would like to thank Jed Duersch for providing his code for RQRCP.
REFERENCES,0.8235294117647058,REFERENCES
REFERENCES,0.8280542986425339,"Timothy A. Davis and Yifan Hu. The university of florida sparse matrix collection. ACM Transac-
tions on Mathematical Software, 38(1):1–25, 2011."
REFERENCES,0.832579185520362,"Jed A. Duersch and Ming Gu. Randomized qr with column pivoting. SIAM Journal on Scientific
Computing, 39(4):C263–C291, 2017."
REFERENCES,0.8371040723981901,"Yuehua Feng, Jianwei Xiao, and Ming Gu. Low-rank matrix approximations with flip-flop spectrum-
revealing qr factorization. Electronic Transactions on Numerical Analysis, 51:469–494, 2019."
REFERENCES,0.8416289592760181,"Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions, 2010."
REFERENCES,0.8461538461538461,"Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791."
REFERENCES,0.8506787330316742,"Per-Gunnar Martinsson and Sergey Voronin. A randomized blocked algorithm for efficiently com-
puting rank-revealing factorizations of matrices. SIAM Journal on Scientific Computing, 38(5):
S485–S507, 2016."
REFERENCES,0.8552036199095022,Under review as a conference paper at ICLR 2022
REFERENCES,0.8597285067873304,"Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and faster
approximate singular value decomposition.
In Corinna Cortes, Neil D. Lawrence, Daniel D.
Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural Information Processing
Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-
12, 2015, Montreal, Quebec, Canada, pp. 1396–1404, 2015."
REFERENCES,0.8642533936651584,"Vladimir Rokhlin, Arthur Szlam, and Mark Tygert. A randomized algorithm for principal compo-
nent analysis. SIAM Journal on Matrix Analysis and Applications, 31(3):1100–1124, 2010."
REFERENCES,0.8687782805429864,"G. W. Stewart.
The qlp approximation to the singular value decomposition.
SIAM Journal on
Scientific Computing, 20(4):1336–1348, 1999."
REFERENCES,0.8733031674208145,"Waqas Sultani, Chen Chen, and Mubarak Shah.
Real-world anomaly detection in surveillance
videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018."
REFERENCES,0.8778280542986425,"Jianwei Xiao, Ming Gu, and Julien Langou. Fast parallel randomized qr with column pivoting algo-
rithms for reliable low-rank matrix approximations. In 2017 IEEE 24th International Conference
on High Performance Computing (HiPC), pp. 233–242, 2017. doi: 10.1109/HiPC.2017.00035."
REFERENCES,0.8823529411764706,"Jian Xue, Jinyu Li, and Yifan Gong. Restructuring of deep neural network acoustic models with
singular value decomposition. In Interspeech, January 2013."
REFERENCES,0.8868778280542986,"Wenjian Yu, Yu Gu, and Yaohang Li. Efficient randomized algorithms for the fixed-precision low-
rank matrix approximation. SIAM Journal on Matrix Analysis and Applications, 39(3):1339–
1359, 2018."
REFERENCES,0.8914027149321267,"A
APPENDIX"
REFERENCES,0.8959276018099548,"A.1
VERIFICATION OF PROPERTIES 1-4"
REFERENCES,0.9004524886877828,"In this subsection, we denote the true rank as ktrue, the rank detected by our algorithm as ˜k, and
the matrix output by our algorithm ˜A˜k. Recall ktrue is defined by σktrue+1(A) ≤ε ≤σktrue(A).
The detected rank ˜k is determined as follows. We run Blocked FFQR until the trailing block of R
is small enough and then take l to be the smallest integer such that ∥R(l + 1 : m, l + 1 : n)∥2 ≤
σktrue+1(A)
4√"
REFERENCES,0.9049773755656109,"2δ. Denote R(l + 1 : m, l + 1 : n) by R22 for short. Afterwards, compute the SVD
of L1 := L(:, 1 : l) and define ˜k by σ˜k+1(L1) ≤ε ≤σ˜k(L1)."
REFERENCES,0.9095022624434389,"First, note that ˜k ≤ktrue. To see this, first observe that ktrue = #{j : σj(A) > ε}. By the Cauchy
Interlacing Theorem, σj(L1) ≤σj(A), 1 ≤j ≤l. Thus we can only shift the singular values of A
downward, which will not increase the size of the above set. This proves Property 1."
REFERENCES,0.9140271493212669,"For Property 2, it follows from computations in Feng et al. (2019) that σj(A)4 ≤σ4
j (L1)+2 ∥R22∥4
2,
or"
REFERENCES,0.918552036199095,"σj(L1) ≥σj(A)
4
s"
REFERENCES,0.9230769230769231,"1 −2∥R22∥4
2
σj(A)4 ≈σj(A)  1 −1"
REFERENCES,0.9276018099547512,"2
∥R22∥4
2
σj(A)4 !"
REFERENCES,0.9321266968325792,",
1 ≤j ≤l."
REFERENCES,0.9366515837104072,"Plugging in ∥R22∥2 ≤σktrue+1(A)
4√"
REFERENCES,0.9411764705882353,"2δ gives σj(L1) ≥σj(A)(1 −δ) for 1 ≤j ≤ktrue + 1 and
in particular for 1 ≤j ≤˜k. This is Property 2."
REFERENCES,0.9457013574660633,"For the last two properties, we refer to Equation 1. In the notation for this section, it reads:"
REFERENCES,0.9502262443438914,"A −˜A˜k

2 ≤σ˜k+1(A)
4
s"
REFERENCES,0.9547511312217195,"1 + 2 ∥R22∥4
2
σ˜k+1(A)4 ≈σ˜k+1(A)  1 + 1"
REFERENCES,0.9592760180995475,"2
∥R22∥4
2
σ˜k+1(A)4 ! ."
REFERENCES,0.9638009049773756,"Again, plugging in ∥R22∥2 ≤σktrue+1(A)
4√"
REFERENCES,0.9683257918552036,"2δ and using the fact that ˜k + 1 ≤ktrue + 1 gives
A −˜A˜k

2 ≤σ˜k+1(A)(1 + δ). We see that if ˜k = ktrue, then this inequality is Property 4."
REFERENCES,0.9728506787330317,Under review as a conference paper at ICLR 2022
REFERENCES,0.9773755656108597,"Finally, from the proof of Property 2 above, we have σ˜k+1(L1) ≥σ˜k+1(A)(1−δ). Thus, σ˜k+1(A) ≤"
REFERENCES,0.9819004524886877,"1
1−δσ˜k+1(L1) and
A −˜A˜k

2 ≤1+δ"
REFERENCES,0.9864253393665159,1−δσ˜k+1(L1) ≤1+δ
REFERENCES,0.9909502262443439,"1−δε, which is Property 3."
REFERENCES,0.995475113122172,"Note that ˜k < ktrue only when there are singular values slightly above the tolerance. The tolerance-
based criterion ensures that up to first order σj(L1) ≥σj(A)(1 −δ). So only singular values
satisfying σj(A) ≥ε ≥σj(A)(1 −δ) can be perturbed below ε and decrease the rank."
