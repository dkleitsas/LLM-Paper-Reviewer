Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002840909090909091,"Crowdsourcing is a quick and easy way to collect labels for large datasets, involving many
workers. However, workers often disagree with each other. Sources of error can arise from the
workers’ skills, but also from the intrinsic difficulty of the task. We present peerannot: a Python
library for managing and learning from crowdsourced labels for classification. Our library allows
users to aggregate labels from common noise models or train a deep learning-based classifier
directly from crowdsourced labels. In addition, we provide an identification module to easily
explore the task difficulty of datasets and worker capabilities."
ABSTRACT,0.005681818181818182,"Keywords: crowdsourcing, label noise, task difficulty, worker ability, classification"
OTHER,0.008522727272727272,Contents
OTHER,0.011363636363636364,"1
Introduction: crowdsourcing in image classification
2"
OTHER,0.014204545454545454,"2
Notation and package structure
3
2.1
Crowdsourcing notation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2.2
Storing crowdsourced datasets in peerannot . . . . . . . . . . . . . . . . . . . . . .
4"
OTHER,0.017045454545454544,"3
Aggregation strategies in crowdsourcing
6
3.1
Classical models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
3.1.1
Majority vote (MV) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
3.1.2
Naive soft (NS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
3.1.3
Dawid and Skene (DS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
3.1.4
Variations around the DS model . . . . . . . . . . . . . . . . . . . . . . . . .
8
3.1.5
Generative model of Labels, Abilities, and Difficulties (GLAD) . . . . . . . .
8
3.1.6
Aggregation strategies in peerannot . . . . . . . . . . . . . . . . . . . . . .
9
3.2
Experiments and evaluation of label aggregation strategies . . . . . . . . . . . . . .
9
3.2.1
Simulated independent mistakes
. . . . . . . . . . . . . . . . . . . . . . . .
10
3.2.2
Simulated correlated mistakes . . . . . . . . . . . . . . . . . . . . . . . . . .
11
3.3
More on confusion matrices in simulation settings . . . . . . . . . . . . . . . . . . .
12"
OTHER,0.019886363636363636,1Corresponding author: tanguy.lefort@umontpellier.fr
OTHER,0.022727272727272728,"4
Learning from crowdsourced tasks
13
4.1
Popular models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
4.1.1
CrowdLayer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
4.1.2
CoNAL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
4.2
Prediction error when learning from crowdsourced tasks . . . . . . . . . . . . . . .
14
4.3
Use case with peerannot on real datasets . . . . . . . . . . . . . . . . . . . . . . . .
14"
OTHER,0.02556818181818182,"5
Identifying tasks difficulty and worker abilities
15
5.1
Exploring tasks’ difficulty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
5.1.1
CIFAR-1OH dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
5.1.2
LabelMe dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
5.2
Identification of worker reliability and task difficulty
. . . . . . . . . . . . . . . . .
17
5.2.1
CIFAR-10H
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
5.2.2
LabelMe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19"
OTHER,0.028409090909090908,"6
Conclusion
20"
OTHER,0.03125,"7
Appendix
20
7.1
Supplementary simulation: Simulated mistakes with discrete difficulty levels on tasks 20
7.2
Comparison with other libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
7.3
Examples of images in CIFAR-10H and Labelme . . . . . . . . . . . . . . . . . . . .
23
7.4
Case study with bird sound classification . . . . . . . . . . . . . . . . . . . . . . . .
23"
INTRODUCTION,0.03409090909090909,"1
Introduction: crowdsourcing in image classification"
INTRODUCTION,0.036931818181818184,"Image datasets widely use crowdsourcing to collect labels, involving many workers who can annotate
images for a small cost (or even free for instance in citizen science) and faster than using expert
labeling. Many classical datasets considered in machine learning have been created with human
intervention to create labels, such as CIFAR-10, (Krizhevsky and Hinton 2009), ImageNet (Deng et
al. 2009) or Pl@ntnet (Garcin et al. 2021) in image classification, but also COCO (Lin et al. 2014),
solar photovoltaic arrays (Kasmi et al. 2023) or even macro litter (Chagneux et al. 2023) in image
segmentation and object counting."
INTRODUCTION,0.03977272727272727,Crowdsourced datasets induce at least three major challenges to which we contribute with peerannot:
INTRODUCTION,0.04261363636363636,"1) How to aggregate multiple labels into a single label from crowdsourced tasks? This
occurs for example when dealing with a single dataset that has been labeled by multiple
workers with disagreements. This is also encountered with other scoring issues such as polls,
reviews, peer-grading, etc. In our framework this is treated with the aggregate command,
which given multiple labels, infers a label. From aggregated labels, a classifier can then be
trained using the train command.
2) How to learn a classifier from crowdsourced datasets? Where the first question is bound
by aggregating multiple labels into a single one, this considers the case where we do not need
a single label to train on, but instead train a classifier on the crowdsourced data, with the
motivation to perform well on a testing set. This end-to-end vision is common in machine
learning, however, it requires the actual tasks (the images, texts, videos, etc.) to train on – and in
crowdsourced datasets publicly available, they are not always available. This is treated with the
aggregate-deep command that runs strategies where the aggregation has been transformed
into a deep learning optimization problem.
3) How to identify good workers in the crowd and difficult tasks? When multiple answers
are given to a single task, looking for who to trust for which type of task becomes necessary"
INTRODUCTION,0.045454545454545456,"to estimate the labels or later train a model with as few noise sources as possible. The module
identify uses different scoring metrics to create a worker and/or task evaluation. This is
particularly relevant considering the gamification of crowdsourcing experiments (Servajean et
al. 2016)"
INTRODUCTION,0.048295454545454544,"The library peerannot addresses these practical questions within a reproducible setting. Indeed,
the complexity of experiments often leads to a lack of transparency and reproducible results for
simulations and real datasets. We propose standard simulation settings with explicit implementation
parameters that can be shared. For real datasets, peerannot is compatible with standard neural net-
work architectures from the Torchvision (Marcel and Rodriguez 2010) library and Pytorch (Paszke
et al. 2019), allowing a flexible framework with easy-to-share scripts to reproduce experiments."
INTRODUCTION,0.05113636363636364,"Figure 1: From crowdsourced labels to training a classifier neural network, the learning pipeline
using the peerannot library. An optional preprocessing step using the identify command allows
us to remove the worst-performing workers or images that can not be classified correctly (very bad
quality for example). Then, from the cleaned dataset, the aggregate command may generate a single
label per task from a prescribed strategy. From the aggregated labels we can train a neural network
classifier with the train command. Otherwise, we can directly train a neural network classifier that
takes into account the crowdsourcing setting in its architecture using aggregate-deep."
LIT REVIEW,0.05397727272727273,"2
Notation and package structure"
LIT REVIEW,0.056818181818181816,"2.1
Crowdsourcing notation"
LIT REVIEW,0.05965909090909091,"Let us consider the classical supervised learning classification framework. A training set 𝒟=
{(𝑥𝑖, 𝑦⋆
𝑖)}𝑛task
𝑖=1 is composed of 𝑛task tasks 𝑥𝑖∈𝒳(the feature space) with (unknown) true label 𝑦⋆
𝑖∈
[𝐾] = {1, … , 𝐾} one of the 𝐾possible classes. In the following, the tasks considered are generally
RGB images. We use the notation 𝜎(⋅) for the softmax function. In particular, given a classifier 𝒞
with logits outputs, 𝜎(𝒞(𝑥𝑖))[1] represents the largest probability and we can sort the probabilities
as 𝜎(𝒞(𝑥𝑖))[1] ≥𝜎(𝒞(𝑥𝑖))[2] ≥⋯≥𝜎(𝒞(𝑥𝑖))[𝐾]. The indicator function is denoted 1(⋅). We use
the 𝑖index notation to range over the different tasks and the 𝑗index notation for the workers
in the crowdsourcing experiment. Note that indices start at position 1 in the equation to follow
mathematical standard notation but it should be noted that, as this is a Python library, in the code"
LIT REVIEW,0.0625,indices start at the 0 position.
LIT REVIEW,0.06534090909090909,"With crowdsourced data the true label of a task 𝑥𝑖, denoted 𝑦⋆
𝑖is unknown, and there is no single
label that can be trusted as in standard supervised learning (even on the train set!). Instead, there is a
crowd of 𝑛worker workers from which multiple workers (𝑤𝑗)𝑗propose a label (𝑦(𝑗)
𝑖)𝑗. These proposed
labels are used to estimate the true label. The set of workers answering the task 𝑥𝑖is denoted by"
LIT REVIEW,0.06818181818181818,"𝒜(𝑥𝑖) = {𝑗∈[𝑛worker] ∶𝑤𝑗answered 𝑥𝑖} .
(1)"
LIT REVIEW,0.07102272727272728,"The cardinal |𝒜(𝑥𝑖)| is called the feedback effort on the task 𝑥𝑖. Note that the feedback effort can not
exceed the total number of workers 𝑛worker. Similarly, one can adopt a worker point of view: the set
of tasks answered by a worker 𝑤𝑗is denoted"
LIT REVIEW,0.07386363636363637,"𝒯(𝑤𝑗) = {𝑖∈[𝑛task] ∶𝑤𝑗answered 𝑥𝑖} .
(2)"
LIT REVIEW,0.07670454545454546,The cardinal |𝒯(𝑤𝑗)| is called the workload of 𝑤𝑗. The final dataset can then be decomposed as:
LIT REVIEW,0.07954545454545454,"𝒟train ∶=
⋃
𝑖∈[𝑛task]
{(𝑥𝑖, (𝑦(𝑗)
𝑖)) for 𝑗∈𝒜(𝑥𝑖)} =
⋃
𝑗∈[𝑛worker]
{(𝑥𝑖, (𝑦(𝑗)
𝑖)) for 𝑖∈𝒯(𝑤𝑗)} ."
LIT REVIEW,0.08238636363636363,"In this article, we do not address the setting where workers report their self-confidence (Yasmin et al.
2022), nor settings where workers are presented a trapping set – i.e., a subset of tasks where the true
label is known to evaluate them with known labels (Khattak 2017)."
LIT REVIEW,0.08522727272727272,"2.2
Storing crowdsourced datasets in peerannot"
LIT REVIEW,0.08806818181818182,"Crowdsourced datasets come in various forms. To store crowdsourcing datasets efficiently and in a
standardized way, peerannot proposes the following structure, where each dataset corresponds to a
folder. Let us set up a toy dataset example to understand the data structure and how to store it."
LIT REVIEW,0.09090909090909091,"Listing 1 Dataset storage tree structure.
datasetname
 train

 ...

 images

 ...
 val
 test
 metadata.json
 answers.json"
LIT REVIEW,0.09375,"The answers.json file stores the different votes for each task as described in Figure 2. This .json
is the rosetta stone between the task ids and the images. It contains the tasks’ id, the workers’s
id and the proposed label for each given vote. Furthermore, storing labels in a dictionary is more
memory-friendly than having an array of size (n_task,n_worker) and writing 𝑦(𝑗)
𝑖
= −1 when the
worker 𝑤𝑗did not see the task 𝑥𝑖and 𝑦(𝑗)
𝑖
∈[𝐾] otherwise."
LIT REVIEW,0.09659090909090909,"In Figure 2, there are three tasks, 𝑛worker = 4 workers and 𝐾= 2 classes. Any available task should
be stored in a single file whose name follows the convention described in Listing 1. These files are
spread into a train, val and test subdirectories as in ImageFolder datasets from torchvision"
LIT REVIEW,0.09943181818181818,"Finally, a metadata.json file includes relevant information related to the crowdsourcing experiment
such as the number of workers, the number of tasks, etc. For example, a minimal metadata.json file
for the toy dataset presented in Figure 2 is:"
LIT REVIEW,0.10227272727272728,"Figure 2: Data storage for the toy-data crowdsourced dataset, a binary classification problem (𝐾= 2,
smiling/not smiling) on recognizing smiling faces. (left: how data is stored in peerannot in a file
answers.json, right: data collected) {"
LIT REVIEW,0.10511363636363637,"""name"": ""toy-data"",
""n_classes"": 2,
""n_workers"": 4,
""n_tasks"": 3
}"
LIT REVIEW,0.10795454545454546,"The toy-data example dataset is available as an example in the peerannot repository. Classical
datasets in crowdsourcing such as CIFAR-10H (Peterson et al. 2019) and LabelMe (Rodrigues, Pereira,
and Ribeiro 2014) can be installed directly using peerannot. To install them, run the install
command from peerannot:"
LIT REVIEW,0.11079545454545454,"For both CIFAR-10H and LabelMe, the dataset was was originally released for standard supervised
learning (classification). Both datasets has been reannotated by a crowd or workers. These labels are
used as true labels in evaluations and visualizations. Examples of CIFAR-10H images are available in
Figure 16, and LabelMe examples in Figure 17 in Appendix. Crowdsourcing votes, however, bring
information about possible confusions (see Figure 3 for an example with CIFAR-10H and Figure 4
with LabelMe). bird car cat deer dog frog horse plane ship truck label 0% 25% 50% 75% 100%"
LIT REVIEW,0.11363636363636363,voting distribution bird car cat deer dog frog horse plane ship truck label bird car cat deer dog frog horse plane ship truck label bird car cat deer dog frog horse plane ship truck label bird car cat deer dog frog horse plane ship truck label
LIT REVIEW,0.11647727272727272,"Figure 3: Example of crowdsourced images from CIFAR-10H. Each task has been labeled by multiple
workers. We display the associated voting distribution over the possible classes. coast"
LIT REVIEW,0.11931818181818182,forest
LIT REVIEW,0.12215909090909091,highway
LIT REVIEW,0.125,insidecity
LIT REVIEW,0.1278409090909091,mountain
LIT REVIEW,0.13068181818181818,opencountry
LIT REVIEW,0.13352272727272727,street
LIT REVIEW,0.13636363636363635,tallbuilding label 0% 25% 50% 75% 100%
LIT REVIEW,0.13920454545454544,voting distribution coast
LIT REVIEW,0.14204545454545456,forest
LIT REVIEW,0.14488636363636365,highway
LIT REVIEW,0.14772727272727273,insidecity
LIT REVIEW,0.15056818181818182,mountain
LIT REVIEW,0.1534090909090909,opencountry
LIT REVIEW,0.15625,street
LIT REVIEW,0.1590909090909091,tallbuilding label coast
LIT REVIEW,0.16193181818181818,forest
LIT REVIEW,0.16477272727272727,highway
LIT REVIEW,0.16761363636363635,insidecity
LIT REVIEW,0.17045454545454544,mountain
LIT REVIEW,0.17329545454545456,opencountry
LIT REVIEW,0.17613636363636365,street
LIT REVIEW,0.17897727272727273,tallbuilding label coast
LIT REVIEW,0.18181818181818182,forest
LIT REVIEW,0.1846590909090909,highway
LIT REVIEW,0.1875,insidecity
LIT REVIEW,0.1903409090909091,mountain
LIT REVIEW,0.19318181818181818,opencountry
LIT REVIEW,0.19602272727272727,street
LIT REVIEW,0.19886363636363635,tallbuilding label coast
LIT REVIEW,0.20170454545454544,forest
LIT REVIEW,0.20454545454545456,highway
LIT REVIEW,0.20738636363636365,insidecity
LIT REVIEW,0.21022727272727273,mountain
LIT REVIEW,0.21306818181818182,opencountry
LIT REVIEW,0.2159090909090909,street
LIT REVIEW,0.21875,tallbuilding label
LIT REVIEW,0.2215909090909091,"Figure 4: Example of crowdsourced images from LabelMe. Each task has been labeled by multiple
workers. We display the associated voting distribution over the possible classes."
IMPLEMENTATION/METHODS,0.22443181818181818,"3
Aggregation strategies in crowdsourcing"
IMPLEMENTATION/METHODS,0.22727272727272727,"The first question we address with peerannot is: How to aggregate multiple labels into a single label
from crowdsourced tasks? The aggregation step can lead to two types of learnable labels ̂𝑦𝑖∈Δ𝐾
(where Δ𝐾is the simplex of dimension 𝐾−1: Δ𝐾= {𝑝∈ℝ𝐾∶∑𝐾
𝑘=1 𝑝𝑘= 1, 𝑝𝑘≥0} ) depending on
the use case for each task 𝑥𝑖, 𝑖= 1, … , 𝑛task:"
IMPLEMENTATION/METHODS,0.23011363636363635,"• a hard label: ̂𝑦𝑖is a Dirac distribution, this can be encoded as a classical label in [𝐾],
• a soft label:
̂𝑦𝑖∈Δ𝐾can represent any probability distribution on [𝐾]. In that case, each
coordinate of the 𝐾-dimensional vector ̂𝑦𝑖represents the probability of belonging to the given
class."
IMPLEMENTATION/METHODS,0.23295454545454544,"Learning from soft labels has been shown to improve learning performance and make the classifier
learn the task ambiguity (Zhang et al. 2018; Peterson et al. 2019; Park and Caragea 2022). However,
crowdsourcing is often used as a stepping stone to create a new dataset. We usually expect a
classification dataset to associate a task 𝑥𝑖to a single label and not a full probability distribution. In
this case, we recommend releasing the anonymous answered labels and the aggregation strategy used
to reach a consensus on a single label. With peerannot, both soft and hard labels can be produced."
IMPLEMENTATION/METHODS,0.23579545454545456,"Note that when a strategy produces a soft label, a hard label can be easily induced by taking the
mode, i.e., the class achieving the maximum probability."
IMPLEMENTATION/METHODS,0.23863636363636365,"3.1
Classical models"
IMPLEMENTATION/METHODS,0.24147727272727273,We list below the most classical aggregation strategies used in crowdsourcing.
IMPLEMENTATION/METHODS,0.24431818181818182,"3.1.1
Majority vote (MV)"
IMPLEMENTATION/METHODS,0.2471590909090909,"The most intuitive way to create a label from multiple answers for any type of crowdsourced task is
to take the majority vote (MV). Yet, this strategy has many shortcomings (James 1998) – there is no"
IMPLEMENTATION/METHODS,0.25,"noise model, no worker reliability estimated, no task difficulty involved and especially no way to
remove poorly performing workers. This standard choice can be expressed as:"
IMPLEMENTATION/METHODS,0.2528409090909091,"̂𝑦MV
𝑖
= argmax
𝑘∈[𝐾]
∑
𝑗∈𝒜(𝑥𝑖)
1{𝑦(𝑗)
𝑖=𝑘} ."
IMPLEMENTATION/METHODS,0.2556818181818182,"3.1.2
Naive soft (NS)"
IMPLEMENTATION/METHODS,0.2585227272727273,"One pitfall with MV is that the label produced is hard, hence the ambiguity is discarded by construction.
A simple remedy consists in using the Naive Soft (NS) labeling, i.e., output the empirical distribution
as the task label:"
IMPLEMENTATION/METHODS,0.26136363636363635,"̂𝑦NS
𝑖
= (
1
|𝒜(𝑥𝑖)|
∑
𝑗∈𝒜(𝑥𝑖)
1{𝑦(𝑗)
𝑖=𝑘})
𝑗∈[𝐾]
."
IMPLEMENTATION/METHODS,0.26420454545454547,"With the NS label, we keep the ambiguity, but all workers and all tasks are put on the same level. In
practice, it is known that each worker comes with their abilities, thus modeling this knowledge can
produce better results."
IMPLEMENTATION/METHODS,0.26704545454545453,"3.1.3
Dawid and Skene (DS)"
IMPLEMENTATION/METHODS,0.26988636363636365,"Refining the aggregation, researchers have proposed a noise model to take into account the workers’
abilities. The Dawid and Skene’s (DS) model (Dawid and Skene 1979) is one of the most studied
(Gao and Zhou 2013) and applied (Servajean et al. 2017; Rodrigues and Pereira 2018). These types of
models are most often optimized using EM-based procedures. Assuming the workers are answering
tasks independently, this model boils down to model pairwise confusions between each possible
class. Each worker 𝑤𝑗is assigned a confusion matrix 𝜋(𝑗) ∈ℝ𝐾×𝐾as described in Section 3. The
model assumes that for a task 𝑥𝑖, conditionally on the true label 𝑦⋆
𝑖= 𝑘the label distribution of the
worker’s answer follows a multinomial distribution with probabilities 𝜋(𝑗)
𝑘,⋅for each worker. Each
class has a prevalence 𝜌𝑘= ℙ(𝑦⋆
𝑖= 𝑘) to appear in the dataset. Using the independence between
workers, we obtain the following likelihood to maximize, with latent variables 𝜌, 𝜋= {𝜋(𝑗)}𝑗and"
IMPLEMENTATION/METHODS,0.2727272727272727,"unobserved variables (𝑦(𝑗)
𝑖)𝑖,𝑗:"
IMPLEMENTATION/METHODS,0.2755681818181818,"arg max
𝜌,𝜋
∏
𝑖∈[𝑛task]
∏
𝑘∈[𝐾]
[𝜌𝑘
∏
𝑗∈[𝑛worker]
∏
ℓ∈[𝐾]
(𝜋(𝑗)
𝑘,ℓ)
1
{𝑦(𝑗)
𝑖
=ℓ}]."
IMPLEMENTATION/METHODS,0.2784090909090909,"When the true labels are not available, the data comes from a mixture of categorical distributions.
To retrieve ground truth labels and be able to estimate these parameters, Dawid and Skene (1979)
have proposed to consider the true labels as additional unknown parameters. In this case, denoting
𝑇𝑖,𝑘= 1{𝑦⋆
𝑖=𝑘} the vectors of label class indicators for each task, the likelihood with known true labels
is:"
IMPLEMENTATION/METHODS,0.28125,"arg max
𝜌,𝜋,𝑇
∏
𝑖∈[𝑛task]
∏
𝑘∈[𝐾]
[𝜌𝑘
∏
𝑗∈[𝑛worker]
∏
ℓ∈[𝐾]
(𝜋(𝑗)
𝑘,ℓ)
1
{𝑦(𝑗)
𝑖
=ℓ}]
𝑇𝑖,𝑘
."
IMPLEMENTATION/METHODS,0.2840909090909091,"This framework allows to estimate 𝜌, 𝜋, 𝑇with an EM algorithm as follows:"
IMPLEMENTATION/METHODS,0.2869318181818182,"• With the MV strategy, get an initial estimate of the true labels 𝑇.
• Estimate 𝜌and 𝜋knowing 𝑇using maximum likelihood estimators.
• Update 𝑇knowing 𝜌and 𝜋using Bayes formula.
• Repeat until convergence of the likelihood."
IMPLEMENTATION/METHODS,0.2897727272727273,"The final aggregated soft labels are ̂𝑦DS
𝑖
= 𝑇𝑖,⋅. Note that DS also provides the estimated confusion
matrices ̂𝜋(𝑗) for each worker 𝑤𝑗."
IMPLEMENTATION/METHODS,0.29261363636363635,Figure 5: Bayesian plate notation for the DS model
IMPLEMENTATION/METHODS,0.29545454545454547,"3.1.4
Variations around the DS model"
IMPLEMENTATION/METHODS,0.29829545454545453,"Many variants of the DS model have been proposed in the literature, using Dirichlet priors on the
confusion matrices (Passonneau and Carpenter 2014), using 1 ≤𝐿≤𝑛worker clusters of workers
(Imamura, Sato, and Sugiyama 2018) (DSWC) or even faster implementation that produces only hard
labels (Sinha, Rao, and Balasubramanian 2018)."
IMPLEMENTATION/METHODS,0.30113636363636365,"In particular, the DSWC strategy (Dawid and Skene with Worker Clustering) highly reduces the
dimension of the parameters in the DS model. In the original model, there are 𝐾2×𝑛worker parameters
to be estimated for the confusion matrices only. The DSWC model reduces them to 𝐾2 × 𝐿+ 𝐿
parameters. Indeed, there are 𝐿confusion matrices Λ = {Λ1, … , Λ𝐿} and the confusion matrix of a
cluster is assumed drawn from a multinomial distribution with weights (𝜏1, … , 𝜏𝐿) ∈Δ𝐿over Λ, such
that ℙ(𝜋(𝑗) = Λℓ) = 𝜏ℓ."
IMPLEMENTATION/METHODS,0.3039772727272727,"3.1.5
Generative model of Labels, Abilities, and Difficulties (GLAD)"
IMPLEMENTATION/METHODS,0.3068181818181818,"Finally, we present the GLAD model (Whitehill et al. 2009) that not only takes into account the
worker’s ability, but also the task difficulty in the noise model. The likelihood is optimized using an
EM algorithm to recover the soft label ̂𝑦GLAD
𝑖
."
IMPLEMENTATION/METHODS,0.3096590909090909,Figure 6: Bayesian plate notation for the GLAD model
IMPLEMENTATION/METHODS,0.3125,"Denoting 𝛼𝑗∈ℝthe worker ability (the higher the better) and 𝛽𝑖∈ℝ+⋆the task’s difficulty (the higher
the easier), the model noise is:"
IMPLEMENTATION/METHODS,0.3153409090909091,"ℙ(𝑦(𝑗)
𝑖
= 𝑦⋆
𝑖|𝛼𝑗, 𝛽𝑖) =
1
1 + exp(−𝛼𝑗𝛽𝑖) ."
IMPLEMENTATION/METHODS,0.3181818181818182,"GLAD’s model also assumes that the errors are uniform across wrong labels, thus:"
IMPLEMENTATION/METHODS,0.3210227272727273,"∀𝑘∈[𝐾], ℙ(𝑦(𝑗)
𝑖
= 𝑘|𝑦⋆
𝑖≠𝑘, 𝛼𝑗, 𝛽𝑖) =
1
𝐾−1 (1 −
1
1 + exp(−𝛼𝑗𝛽𝑖)) ."
IMPLEMENTATION/METHODS,0.32386363636363635,This results in estimating 𝑛worker + 𝑛task parameters.
IMPLEMENTATION/METHODS,0.32670454545454547,"3.1.6
Aggregation strategies in peerannot"
IMPLEMENTATION/METHODS,0.32954545454545453,"All of these aggregation strategies – and more – are available in the peerannot library from the
peerannot.models module. Each model is a class object in its own Python file. It inherits from the
CrowdModel template class and is defined with at least two methods:"
RESULTS/EXPERIMENTS,0.33238636363636365,"• run: includes the optimization procedure to obtain needed weights (e.g., the EM algorithm for
the DS model),
• get_probas: returns the soft labels output for each task."
RESULTS/EXPERIMENTS,0.3352272727272727,"3.2
Experiments and evaluation of label aggregation strategies"
RESULTS/EXPERIMENTS,0.3380681818181818,"One way to evaluate the label aggregation strategies is to measure their accuracy. This means that
the underlying ground truth must be known – at least for a representative subset. This is the case in
simulation settings where the ground truth is available. As the set of 𝑛task can be seen as a training
set for a future classifier, we denote this metric AccTrain on a dataset 𝒟for some given aggregated
label ( ̂𝑦𝑖)𝑖as:"
RESULTS/EXPERIMENTS,0.3409090909090909,"AccTrain(𝒟) =
1
|𝒟|"
RESULTS/EXPERIMENTS,0.34375,"|𝒟|
∑
𝑖=1
1{𝑦⋆
𝑖=argmax𝑘∈[𝐾]( ̂𝑦𝑖)𝑘} ."
RESULTS/EXPERIMENTS,0.3465909090909091,"In the following, we write AccTrain for AccTrain(𝒟train) as we only consider the full training set
so there is no ambiguity. The AccTrain computes the number of correctly predicted labels by the
aggregation strategy knowing a ground truth. While this metric is useful, in practice there are a few
arguable issues:"
RESULTS/EXPERIMENTS,0.3494318181818182,"• the AccTrain metric does not consider the ambiguity of the soft label, only the most probable
class, whereas in some contexts ambiguity can be informative,
• in supervised learning one objective is to identify difficult or mislabeled tasks (Pleiss et al.
2020; Lefort et al. 2022), pruning those tasks can easily artificially improve the AccTrain, but
there is no guarantee over the predictive performance of a model based on the newly pruned
dataset,
• in practice, true labels are unknown, thus this metric would not be computable."
RESULTS/EXPERIMENTS,0.3522727272727273,"We first consider classical simulation settings in the literature that can easily be created and repro-
duced using peerannot. For each dataset, we present the distribution of the number of workers per
task (|𝒜(𝑥𝑖)|)𝑖=1,…,𝑛task Equation 1 on the right and the distribution of the number of tasks per worker
(|𝒯(𝑤𝑗)|)𝑗=1,…,𝑛worker Equation 2 on the left."
RESULTS/EXPERIMENTS,0.35511363636363635,"3.2.1
Simulated independent mistakes"
RESULTS/EXPERIMENTS,0.35795454545454547,"The independent mistakes setting considers that each worker 𝑤𝑗answers follows a multinomial
distribution with weights given at the row 𝑦⋆
𝑖of their confusion matrix 𝜋(𝑗) ∈ℝ𝐾×𝐾. Each confusion
row in the confusion matrix is generated uniformly in the simplex. Then, we make the matrix
diagonally dominant (to represent non-adversarial workers) by switching the diagonal term with
the maximum value by row. Answers are independent of one another as each matrix is generated
independently and each worker answers independently of other workers. In this setting, the DS
model is expected to perform better with enough data as we are simulating data from its assumed
noise model."
RESULTS/EXPERIMENTS,0.36079545454545453,"We simulate 𝑛task = 200 tasks and 𝑛worker = 30 workers with 𝐾= 5 possible classes. Each task 𝑥𝑖
receives |𝒜(𝑥𝑖)| = 10 labels. With 200 tasks and 30 workers, asking for 10 leads to around 200×10"
RESULTS/EXPERIMENTS,0.36363636363636365,"30
≃67
tasks per worker (with variations due to randomness in the affectations)."
RESULTS/EXPERIMENTS,0.3664772727272727,"57
60
63
66
69
72
75
78
81
| (wj)| 0% 2% 4% 6% 8% 10% 12% 14%"
RESULTS/EXPERIMENTS,0.3693181818181818,Percent
RESULTS/EXPERIMENTS,0.3721590909090909,"8
9
10
11
12
|
(xi)| 0% 20% 40% 60% 80% 100%"
RESULTS/EXPERIMENTS,0.375,Percent
RESULTS/EXPERIMENTS,0.3778409090909091,"Figure 7: Distribution of number of tasks given per worker (left) and number of labels per task (right)
in the independent mistakes setting."
RESULTS/EXPERIMENTS,0.3806818181818182,"With the obtained answers, we can look at the aforementioned aggregation strategies performance.
The peerannot aggregate command takes as input the path to the data folder and the aggregation
--strategy/-s to perform. Other arguments are available and described in the --help description."
RESULTS/EXPERIMENTS,0.3835227272727273,"Table 1: AccTrain metric on simulated independent mistakes considering classical feature-blind label
aggregation strategies"
RESULTS/EXPERIMENTS,0.38636363636363635,Table 1
RESULTS/EXPERIMENTS,0.38920454545454547,"MV
GLAD
DS
DSWC[L=5]
DSWC[L=10]
NS"
RESULTS/EXPERIMENTS,0.39204545454545453,"AccTrain
0.760
0.775
0.890
0.775
0.770
0.760"
RESULTS/EXPERIMENTS,0.39488636363636365,"As expected by the simulation framework, Table 1 fits the DS model, thus leading to better accuracy
in retrieving the simulated labels for the DS strategy. The MV and NS aggregations do not consider
any worker-ability scoring or the task’s difficulty and perform the worst."
RESULTS/EXPERIMENTS,0.3977272727272727,"Remark: peerannot can also simulate datasets with an imbalanced number of votes chosen uniformly
at random between 1 and the number of workers available. For example:"
RESULTS/EXPERIMENTS,0.4005681818181818,"96
99
102 105 108 111 114"
RESULTS/EXPERIMENTS,0.4034090909090909,| (wj)| 0% 2% 5% 8% 10% 12% 15% 18%
RESULTS/EXPERIMENTS,0.40625,Percent
RESULTS/EXPERIMENTS,0.4090909090909091,"8
9
10
11
12
|
(xi)| 0% 2% 4% 6% 8%"
RESULTS/EXPERIMENTS,0.4119318181818182,Percent
RESULTS/EXPERIMENTS,0.4147727272727273,"Figure 8: Distribution of the number of tasks given per worker (left) and of the number of labels per
task (right) in the independent mistakes setting with voting imbalance enabled."
RESULTS/EXPERIMENTS,0.41761363636363635,"With the obtained answers, we can look at the aforementioned aggregation strategies performance:"
RESULTS/EXPERIMENTS,0.42045454545454547,"Table 2: AccTrain metric on simulated independent mistakes with an imbalanced number of votes
per task considering classical feature-blind label aggregation strategies"
RESULTS/EXPERIMENTS,0.42329545454545453,Table 2
RESULTS/EXPERIMENTS,0.42613636363636365,"MV
GLAD
DS
DSWC[L=5]
DSWC[L=10]
NS"
RESULTS/EXPERIMENTS,0.4289772727272727,"AccTrain
0.825
0.810
0.895
0.845
0.840
0.830"
RESULTS/EXPERIMENTS,0.4318181818181818,"While more realistic, working with an imbalanced number of votes per task can lead to disrupting
orders of performance for some strategies (here GLAD is outperformed by other strategies)."
RESULTS/EXPERIMENTS,0.4346590909090909,"3.2.2
Simulated correlated mistakes"
RESULTS/EXPERIMENTS,0.4375,"The correlated mistakes are also known as the student-teacher or junior-expert setting (Cao et al.
(2019)). Consider that the crowd of workers is divided into two categories: teachers and students
(with 𝑛teacher +𝑛student = 𝑛worker). Each student is randomly assigned to one teacher at the beginning
of the experiment. We generate the (diagonally dominant as in Section 3.2.1) confusion matrices of
each teacher and the students share the same confusion matrix as their associated teacher. Hence,
clustering strategies are expected to perform best in this context. Then, they all answer independently,
following a multinomial distribution with weights given at the row 𝑦⋆
𝑖of their confusion matrix
𝜋(𝑗) ∈ℝ𝐾×𝐾."
RESULTS/EXPERIMENTS,0.4403409090909091,"We simulate 𝑛task = 200 tasks and 𝑛worker = 30 with 80% of students in the crowd. There are 𝐾= 5
possible classes. Each task receives |𝒜(𝑥𝑖)| = 10 labels."
RESULTS/EXPERIMENTS,0.4431818181818182,"51
54
57
60
63
66
69
72
75
| (w )| 0% 5% 10% 15% 20%"
RESULTS/EXPERIMENTS,0.4460227272727273,Percent
RESULTS/EXPERIMENTS,0.44886363636363635,"8
9
10
11
12
|
(x )| 0% 20% 40% 60% 80% 100%"
RESULTS/EXPERIMENTS,0.45170454545454547,Percent
RESULTS/EXPERIMENTS,0.45454545454545453,"Table 3: AccTrain metric on simulated correlated mistakes considering classical feature-blind label
aggregation strategies"
RESULTS/EXPERIMENTS,0.45738636363636365,Table 3
RESULTS/EXPERIMENTS,0.4602272727272727,"MV
GLAD
DS
DSWC[L=5]
DSWC[L=6]
DSWC[L=10]
NS"
RESULTS/EXPERIMENTS,0.4630681818181818,"AccTrain
0.705
0.645
0.755
0.795
0.780
0.815
0.690"
RESULTS/EXPERIMENTS,0.4659090909090909,"With Table 3, we see that with correlated data (24 students and 6 teachers), using 5 confusion matrices
with DSWC[L=5] outperforms the vanilla DS strategy that does not consider the correlations. The
best-performing method here estimates only 10 confusion matrices (instead of 30 for the vanilla DS
model)."
RESULTS/EXPERIMENTS,0.46875,"To summarize our simulations, we see that depending on workers answering strategies, different
latent variable models perform best. However, these are unknown outside of a simulation framework,
thus if we want to obtain labels from multiple responses, we need to investigate multiple models.
This can be done easily with peerannot as we demonstrated using the aggregate module. However,
one might not want to generate a label, simply learn a classifier to predict labels on unseen data.
This leads us to another module part of peerannot."
RESULTS/EXPERIMENTS,0.4715909090909091,"3.3
More on confusion matrices in simulation settings"
RESULTS/EXPERIMENTS,0.4744318181818182,"Moreover, the concept of confusion matrices has been commonly used to represent worker abilities.
Let us remind that a confusion matrix 𝜋(𝑗) ∈ℝ𝐾×𝐾of a worker 𝑤𝑗is defined such that 𝜋(𝑗)
𝑘,ℓ= ℙ(𝑦(𝑗)
𝑖
=
ℓ|𝑦⋆
𝑖= 𝑘). These quantities need to be estimated since no true label is available in a crowd-sourced
scenario. In practice, the confusion matrices of each worker is estimated via an aggregation strategy
like Dawid and Skene’s (Dawid and Skene 1979) presented in Section 3.1. 0 1 2 3 4"
RESULTS/EXPERIMENTS,0.4772727272727273,Proposed label 0 1 2 3 4
RESULTS/EXPERIMENTS,0.48011363636363635,True label
RESULTS/EXPERIMENTS,0.48295454545454547,Spammer worker 0 1 2 3 4
RESULTS/EXPERIMENTS,0.48579545454545453,Proposed label
RESULTS/EXPERIMENTS,0.48863636363636365,Common worker 0 1 2 3 4
RESULTS/EXPERIMENTS,0.4914772727272727,Proposed label
RESULTS/EXPERIMENTS,0.4943181818181818,Expert worker
RESULTS/EXPERIMENTS,0.4971590909090909,"Figure 10: Three types of profiles of worker confusion matrices simulated with peerannot. The
spammer answers independently of the true label. Expert workers identify classes without mistakes.
In practice common workers are good for some classes but might confuse two (or more) labels. All
workers are simulated using the peerannot simulate command presented in Section 3.2."
RESULTS/EXPERIMENTS,0.5,"In Figure 10, we illustrate multiple workers’ profile (as reflected by their confusion matrix) on
a simulate scenario where the ground truth is available. For that we generate toy datasets with
the simulate command from peerannot. In particular, we display a type of worker that can hurt"
RESULTS/EXPERIMENTS,0.5028409090909091,"data quality: the spammer. Raykar and Yu (2011) defined a spammer as a worker that answers
independently of the true label:"
RESULTS/EXPERIMENTS,0.5056818181818182,"∀𝑘∈[𝐾], ℙ(𝑦(𝑗)
𝑖
= 𝑘|𝑦⋆
𝑖) = ℙ(𝑦(𝑗)
𝑖
= 𝑘) .
(3)"
RESULTS/EXPERIMENTS,0.5085227272727273,"Each row of the confusion matrix represents the label’s probability distribution given a true label.
Hence, the spammer has a confusion matrix with near-identical rows. Apart from the spammer,
common mistakes often involve workers mixing up one or several classes. Expert workers have a
confusion matrix close to the identity matrix."
IMPLEMENTATION/METHODS,0.5113636363636364,"4
Learning from crowdsourced tasks"
IMPLEMENTATION/METHODS,0.5142045454545454,"Commonly, tasks are crowdsourced to create a large annotated training set as modern machine
learning models require more and more data. The aggregation step then simply becomes the first step
in the complete learning pipeline. However, instead of aggregating labels, modern neural networks
are directly trained end-to-end from multiple noisy labels."
IMPLEMENTATION/METHODS,0.5170454545454546,"4.1
Popular models"
IMPLEMENTATION/METHODS,0.5198863636363636,"In recent years, directly learning a classifier from noisy labels was introduced. Two of the most
used models: CrowdLayer (Rodrigues and Pereira 2018) and CoNAL (Chu, Ma, and Wang 2021), are
directly available in peerannot. These two learning strategies directly incorporate a DS-inspired
noise model in the neural network’s architecture."
IMPLEMENTATION/METHODS,0.5227272727272727,"4.1.1
CrowdLayer"
IMPLEMENTATION/METHODS,0.5255681818181818,"CrowdLayer trains a classifier with noisy labels as follows. Let the scores (logits) output by a given
classifier neural network 𝒞be 𝑧𝑖= 𝒞(𝑥𝑖). Then CrowdLayer adds as a last layer 𝜋∈ℝ𝑛worker×𝐾×𝐾, the
tensor of all 𝜋(𝑗)’s such that the crossentropy loss (CE) is adapted to the crowdsourcing setting into
ℒCrowdLayer
𝐶𝐸
and computed as:"
IMPLEMENTATION/METHODS,0.5284090909090909,"ℒCrowdLayer
𝐶𝐸
(𝑥𝑖) =
∑
𝑗∈𝒜(𝑥𝑖)
CE (𝜎(𝜋(𝑗)𝜎(𝑧𝑖)) , 𝑦(𝑗)
𝑖) ,"
IMPLEMENTATION/METHODS,0.53125,"where the crossentropy loss for two distribution 𝑢, 𝑣∈Δ𝐾is defined as CE(𝑢, 𝑣) = ∑𝑘∈[𝐾] 𝑣𝑘log(𝑢𝑘)."
IMPLEMENTATION/METHODS,0.5340909090909091,"Where DS modeled workers as confusion matrices, CrowdLayer adds a layer of 𝜋(𝑗)s into the backbone
architecture as a new tensor layer to transform the output probabilities. The backbone classifier
predicts a distribution that is then corrupted through the added layer to learn the worker-specific
confusion. The weights in the tensor layer of 𝜋(𝑗)s are learned during the optimization procedure."
IMPLEMENTATION/METHODS,0.5369318181818182,"4.1.2
CoNAL"
IMPLEMENTATION/METHODS,0.5397727272727273,"For some datasets, it was noticed that global confusion occurs between the proposed classes. It is the
case for example in the LabelMe dataset (Rodrigues et al. 2017) where classes overlap. In this case,
Chu, Ma, and Wang (2021) proposed to extend the CrowdLayer model by adding global confusion
matrix 𝜋𝑔∈ℝ𝐾×𝐾to the model on top of each worker’s confusion."
IMPLEMENTATION/METHODS,0.5426136363636364,"Given the output 𝑧𝑖= 𝒞(𝑥𝑖) ∈ℝ𝐾of a given classifier and task, CoNAL interpolates between the
prediction corrected by local confusions 𝜋(𝑗)𝑧𝑖and the prediction corrected by a global confusion"
IMPLEMENTATION/METHODS,0.5454545454545454,𝜋𝑔𝑧𝑖. The loss function is computed as follows:
IMPLEMENTATION/METHODS,0.5482954545454546,"ℒCoNAL
𝐶𝐸
(𝑥𝑖) =
∑
𝑗∈𝒜(𝑥𝑖)
CE(ℎ(𝑗)
𝑖, 𝑦(𝑗)
𝑖) ,"
IMPLEMENTATION/METHODS,0.5511363636363636,"with ℎ(𝑗)
𝑖
= 𝜎((𝜔(𝑗)
𝑖𝜋𝑔+ (1 −𝜔(𝑗)
𝑖)𝜋(𝑗))𝑧𝑖) ."
IMPLEMENTATION/METHODS,0.5539772727272727,"The interpolation weight 𝜔(𝑗)
𝑖
is unobservable in practice. So, to compute ℎ(𝑗)
𝑖, the weight is obtained
through an auxiliary network. This network takes as input the image and worker information
and outputs a task-related vector 𝑣𝑖and a worker-related vector 𝑢𝑗of the same dimension. Finally,"
IMPLEMENTATION/METHODS,0.5568181818181818,"𝑤(𝑗)
𝑖
= (1 + exp(−𝑢⊤
𝑗𝑣𝑖))−1."
IMPLEMENTATION/METHODS,0.5596590909090909,"Both CrowdLayer and CoNAL model worker confusions directly in the classifier’s weights to learn
from the noisy collected labels and are available in peerannot as we will see in the following."
IMPLEMENTATION/METHODS,0.5625,"4.2
Prediction error when learning from crowdsourced tasks"
IMPLEMENTATION/METHODS,0.5653409090909091,"The AccTrain metric presented in Section 3.2 might no longer be of interest when training a classifier.
Classical error measurements involve a test dataset to estimate the generalization error. To do so, we
present hereafter two error metrics. Assuming we trained our classifier 𝒞on a training set and that
there is a test set available with known true labels:"
IMPLEMENTATION/METHODS,0.5681818181818182,"• the test accuracy is computed as
1
𝑛test ∑𝑛test
𝑖=1 1{𝑦⋆
𝑖= ̂𝑦𝑖}.
• the expected calibration error (Guo et al. 2017) over 𝑀equally spaced bins 𝐼1, … , 𝐼𝑀partition-
ning the interval [0, 1], is computed as:"
IMPLEMENTATION/METHODS,0.5710227272727273,"ECE =
𝑀
∑
𝑚=1"
IMPLEMENTATION/METHODS,0.5738636363636364,"|𝐵𝑚|
𝑛task
|acc(𝐵𝑚) −conf(𝐵𝑚)| ,"
IMPLEMENTATION/METHODS,0.5767045454545454,"with 𝐵𝑚= {𝑥𝑖|𝒞(𝑥𝑖)[1] ∈𝐼𝑚} the tasks with predicted probability in the 𝑚-th bin, acc(𝐵𝑚)
the accuracy of the network for the samples in 𝐵𝑚and conf(𝐵𝑚) the associated empirical
confidence. More precisely:"
IMPLEMENTATION/METHODS,0.5795454545454546,"acc(𝐵𝑚) =
1
|𝐵𝑚| ∑
𝑖∈𝐵𝑚
1( ̂𝑦𝑖= 𝑦⋆
𝑖)
and
conf(𝐵𝑚) =
1
|𝐵𝑚| ∑
𝑖∈𝐵𝑚
𝜎(𝒞(𝑥𝑖))[1] ."
IMPLEMENTATION/METHODS,0.5823863636363636,"The accuracy represents how well the classifier generalizes, and the expected calibration error (ECE)
quantifies the deviation between the accuracy and the confidence of the classifier. Modern neural
networks are known to often be overconfident in their predictions (Guo et al. 2017). However, it has
also been remarked that training on crowdsourced data, depending on the strategy, mitigates this
confidence issue. That is why we propose to compare them both in our coming experiments. Note
that the ECE error estimator is known to be biased (Gruber and Buettner 2022). Smaller training
sets are known to have a higher ECE estimation error. And in the crowdsourcing setting, openly
available datasets are often quite small."
RESULTS/EXPERIMENTS,0.5852272727272727,"4.3
Use case with peerannot on real datasets"
RESULTS/EXPERIMENTS,0.5880681818181818,"Few real crowdsourcing experiments have been released publicly. Among the available ones,
CIFAR-10H (Peterson et al. 2019) is one of the largest with 10000 tasks labeled by workers (the
testing set of CIFAR-10). The main limitation of CIFAR-10H is that there are few disagreements
between workers and a simple majority voting already leads to a near-perfect AccTrain error. Hence,
comparing the impact of aggregation and end-to-end strategies might not be relevant (Peterson et al."
RESULTS/EXPERIMENTS,0.5909090909090909,"2019; Aitchison 2021), it is however a good benchmark for task difficulty identification and worker
evaluation scoring. Each of these dataset contains a test set, with known ground truth. Thus, we can
train a classifier from the crowdsourced data, and compare predictive performance on the test set."
RESULTS/EXPERIMENTS,0.59375,"The LabelMe dataset was extracted from crowdsourcing segmentation experiments and a subset of
𝐾= 8 classes was released in Rodrigues et al. (2017)."
RESULTS/EXPERIMENTS,0.5965909090909091,"Let us use peerannot to train a VGG-16 with two dense layers on the LabelMe dataset. Note that
this modification was introduced to reach state-of-the-art performance in (Chu, Ma, and Wang
2021). Other models from the torchvision library can be used, such as Resnets, Alexnet etc. The
aggregate-deep command takes as input the path to the data folder, --output-name/-o is the name
for the output file, --n-classes/-K the number of classes, --strategy/-s the learning strategy to
perform (e.g., CrowdLayer or CoNAL), the backbone classifier in --model and then optimization
hyperparameters for pytorch described with more details using the peerannot aggregate-deep
--help command."
RESULTS/EXPERIMENTS,0.5994318181818182,"Table 4: Generalization performance on LabelMe dataset depending on the learning strategy from
the crowdsourced labels. The network used is a VGG-16 with two dense layers for all methods."
RESULTS/EXPERIMENTS,0.6022727272727273,Table 4
RESULTS/EXPERIMENTS,0.6051136363636364,"method
AccTest
ECE"
RESULTS/EXPERIMENTS,0.6079545454545454,"0
CoNAL[scale=0]
81.061
0.189
1
DS
85.606
0.143
2
CrowdLayer
86.448
0.136
3
CoNAL[scale=1e-4]
87.205
0.117
4
GLAD
87.542
0.124
5
NS
88.468
0.115
6
MV
88.889
0.112"
RESULTS/EXPERIMENTS,0.6107954545454546,"As we can see, CoNAL strategy performs best. In this case, it is expected behavior as CoNAL
was created for the LabelMe dataset. However, using peerannot we can look into why modeling
common confusion returns better results with this dataset. To do so, we can explore the
datasets from two points of view: worker-wise or task-wise in Section 5."
IMPLEMENTATION/METHODS,0.6136363636363636,"5
Identifying tasks difficulty and worker abilities"
IMPLEMENTATION/METHODS,0.6164772727272727,"If a dataset requires crowdsourcing to be labeled, it is because expert knowledge is long and costly to
obtain. In the era of big data, where datasets are built using web scraping (or using a platform like
Amazon Mechanical Turk), citizen science is popular as it is an easy way to produce many labels."
IMPLEMENTATION/METHODS,0.6193181818181818,"However, mistakes and confusions happen during these experiments. Sometimes involuntarily
(e.g., because the task is too hard or the worker is unable to differentiate between two classes) and
sometimes voluntarily (e.g., the worker is a spammer)."
IMPLEMENTATION/METHODS,0.6221590909090909,"Underlying all the learning models and aggregation strategies, the cornerstone of crowdsourcing
is evaluating the trust we put in each worker depending on the presented task. And with the
gamification of crowdsourcing (Servajean et al. 2016; Tinati et al. 2017), it has become essential to
find scoring metrics both for workers and tasks to keep citizens in the loop so to speak. This is the
purpose of the identification module in peerannot."
IMPLEMENTATION/METHODS,0.625,"Our test cases are both the CIFAR-10H dataset and the LabelMe dataset to compare the worker and
task evaluation depending on the number of votes collected. Indeed, the LabelMe dataset has only
up to three votes per task whereas CIFAR-10H accounts for nearly fifty votes per task."
IMPLEMENTATION/METHODS,0.6278409090909091,"5.1
Exploring tasks’ difficulty"
IMPLEMENTATION/METHODS,0.6306818181818182,"To explore the tasks’ intrinsic difficulty, we propose to compare three scoring metrics:"
IMPLEMENTATION/METHODS,0.6335227272727273,"• the entropy of the NS distribution: the entropy measures the inherent uncertainty of the
distribution to the possible outcomes. It is reliable with a big enough and not adversarial crowd.
More formally:"
IMPLEMENTATION/METHODS,0.6363636363636364,"∀𝑖∈[𝑛task], Entropy( ̂𝑦𝑁𝑆
𝑖
) = −∑
𝑘∈[𝐾]
(𝑦𝑁𝑆
𝑖
)𝑘log ((𝑦𝑁𝑆
𝑖
)𝑘) ."
IMPLEMENTATION/METHODS,0.6392045454545454,"• GLAD’s scoring: by construction, Whitehill et al. (2009) introduced a scalar coefficient to score
the difficulty of a task.
• the Weighted Area Under the Margins (WAUM): introduced by Lefort et al. (2022), this weighted
area under the margins indicates how difficult it is for a classifier 𝒞to learn a task’s label. This
procedure is done with a budget of 𝑇> 0 epochs. Given the crowdsourced labels and the trust
we have in each worker denoted 𝑠(𝑗)(𝑥𝑖) > 0, the WAUM of a given task 𝑥𝑖∈𝒳and a set of
crowdsourced labels {𝑦(𝑗)
𝑖}𝑗∈[𝐾]|𝒜(𝑥𝑖)| is defined as:"
IMPLEMENTATION/METHODS,0.6420454545454546,"WAUM(𝑥𝑖) ∶=
1
|𝒜(𝑥𝑖)|
∑
𝑗∈𝒜(𝑥𝑖)
𝑠(𝑗)(𝑥𝑖) {1 𝑇"
IMPLEMENTATION/METHODS,0.6448863636363636,"𝑇
∑
𝑡=1
𝜎(𝒞(𝑥𝑖))𝑦(𝑗)
𝑖
−𝜎(𝒞(𝑥𝑖))[2]} ,"
IMPLEMENTATION/METHODS,0.6477272727272727,"where we remind that 𝒞(𝑥𝑖))[2] is the second largest probability output by the classifier 𝒞for
the task 𝑥𝑖."
IMPLEMENTATION/METHODS,0.6505681818181818,The weights 𝑠(𝑗)(𝑥𝑖) are computed à la Servajean et al. (2017):
IMPLEMENTATION/METHODS,0.6534090909090909,"∀𝑗∈[𝑛worker], ∀𝑖∈[𝑛task], 𝑠(𝑗)(𝑥𝑖) = ⟨𝜎(𝒞(𝑥𝑖)), diag(𝜋(𝑗))⟩,"
IMPLEMENTATION/METHODS,0.65625,"where ̂𝜋(𝑗) is the estimated confusion matrix of worker 𝑤𝑗(by default, the estimation provided by
DS)."
IMPLEMENTATION/METHODS,0.6590909090909091,"The WAUM is a generalization of the AUM by Pleiss et al. (2020) to the crowdsourcing setting. A
high WAUM indicates a high trust in the task classification by the network given the crowd labels. A
low WAUM indicates difficulty for the network to classify the task into the given classes (taking into
consideration the trust we have in each worker for the task considered). Where other methods only
consider the labels and not directly the tasks, the WAUM directly considers the learning trajectories
to identify ambiguous tasks. One pitfall of the WAUM is that it is dependent on the architecture used."
IMPLEMENTATION/METHODS,0.6619318181818182,"Note that each of these statistics could prove useful in different contexts. The entropy is irrelevant in
settings with few labels per task (small |𝒜(𝑥𝑖)|). For instance, it is uninformative for LabelMe dataset.
The WAUM can handle any number of labels, but the larger the better. However, as it uses a deep
learning classifier, the WAUM needs the tasks (𝑥𝑖)𝑖in addition to the proposed labels while the other
strategies are feature-blind."
IMPLEMENTATION/METHODS,0.6647727272727273,"5.1.1
CIFAR-1OH dataset"
IMPLEMENTATION/METHODS,0.6676136363636364,"First, let us consider a dataset with a large number of tasks, annotations and workers: the CIFAR-10H
dataset by Peterson et al. (2019)."
IMPLEMENTATION/METHODS,0.6704545454545454,Unable to display output for mime type(s): text/html
IMPLEMENTATION/METHODS,0.6732954545454546,"Most difficult tasks sorted by class from MV aggregation identified depending on the strategy used
(entropy, GLAD or WAUM) using a Resnet34."
IMPLEMENTATION/METHODS,0.6761363636363636,Unable to display output for mime type(s): text/html
IMPLEMENTATION/METHODS,0.6789772727272727,"The entropy, GLAD’s difficulty, and WAUM’s difficulty each show different images as exhibited in
the interactive Figure. While the entropy and GLAD output similar tasks, in this case, the WAUM
often differs. We can also observe an ambiguity induced by the labels in the truck category, with the
presence of a trailer that is technically a mixup between a car and a truck."
IMPLEMENTATION/METHODS,0.6818181818181818,"5.1.2
LabelMe dataset"
IMPLEMENTATION/METHODS,0.6846590909090909,"As for the LabelMe dataset, one difficulty in evaluating tasks’ intrinsic difficulty is that there is a
limited amount of votes available per task. Hence, the entropy in the distribution of the votes is no
longer a reliable metric, and we need to rely on other models."
IMPLEMENTATION/METHODS,0.6875,"Now, let us compare the tasks’ difficulty distribution depending on the strategy considered using
peerannot."
IMPLEMENTATION/METHODS,0.6903409090909091,Unable to display output for mime type(s): text/html
IMPLEMENTATION/METHODS,0.6931818181818182,"Most difficult tasks sorted by class from MV aggregation identified depending on the strategy used
(entropy, GLAD or WAUM) using a VGG-16 with two dense layers."
IMPLEMENTATION/METHODS,0.6960227272727273,"Note that in this experiment, because the number of labels given per task is in {1, 2, 3}, the entropy
only takes four values. In particular, tasks with only one label all have a null entropy, so not just
consensual tasks. The MV is also not suited in this case because of the low number of votes per task."
IMPLEMENTATION/METHODS,0.6988636363636364,"The underlying difficulty of these tasks mainly comes from the overlap in possible labels. For example,
tallbuildings are most often found insidecities, and so are streets. In the opencountry we
find forests, river-coasts and mountains."
RESULTS/EXPERIMENTS,0.7017045454545454,"5.2
Identification of worker reliability and task difficulty"
RESULTS/EXPERIMENTS,0.7045454545454546,"From the labels, we can explore different worker evaluation scores. GLAD’s strategy estimates a
reliability scalar coefficient 𝛼𝑗per worker. With strategies looking to estimate confusion matrices,
we investigate two scoring rules for workers:"
RESULTS/EXPERIMENTS,0.7073863636363636,"• The trace of the confusion matrix: the closer to 𝐾the better the worker.
• The closeness to spammer metric (Raykar and Yu 2011) (also called spammer score) that is the
Frobenius norm between the estimated confusion matrix ̂𝜋(𝑗) and the closest rank-1 matrix.
The further to zero the better the worker. On the contrary, the closer to zero, the more likely it
is for the worker to be a spammer. This score separates spammers from common workers and
experts (with profiles as in Figure 10)."
RESULTS/EXPERIMENTS,0.7102272727272727,"When the tasks are available, confusion-matrix-based deep learning models can also be used. We
thus add to the comparison the trace of the confusion matrices with CrowdLayer and CoNAL on
the LabelMe datasets. For CoNAL, we only consider the trace of the confusion matrix 𝜋(𝑗) in the
pairwise comparison. Moreover, for CrowdLayer and CoNAL we show in Figure 12 the weights
learned without the softmax operation by row to keep the comparison as simple as possible with the
actual outputs of the model."
RESULTS/EXPERIMENTS,0.7130681818181818,"Comparisons in Figure 11 and Figure 12 are plotted pairwise between the evaluated metrics. Each
point represents a worker. Each off-diagonal plot shows the joint distribution between the scores of
the y-axis row and the x-axis column. They allow us to visualize the relationship between these two
variables. The main diagonal represents the (smoothed) marginal distribution of the score of the
considered column."
RESULTS/EXPERIMENTS,0.7159090909090909,"5.2.1
CIFAR-10H"
RESULTS/EXPERIMENTS,0.71875,"The CIFAR-10H dataset has few disagreements among workers. However, these strategies disagree
on the ranking of good against best workers as they do not measure the same properties. 0.0 0.2 0.4 0.6 0.8 1.0"
RESULTS/EXPERIMENTS,0.7215909090909091,spam_score
RESULTS/EXPERIMENTS,0.7244318181818182,"2.5
5.0
7.5
10.0
Trace DS 0 1 2 glad"
RESULTS/EXPERIMENTS,0.7272727272727273,"0.00
0.25
0.50
0.75
1.00
spam_score"
RESULTS/EXPERIMENTS,0.7301136363636364,"1
0
1
2
3
glad"
RESULTS/EXPERIMENTS,0.7329545454545454,"Figure 11: Comparison of ability scores by workers for the CIFAR-10H dataset. All metrics computed
identify the same poorly performing workers. A mass of good and expert workers can be seen as the
dataset presents few disagreements, thus few data to discriminate expert workers from the otherss."
RESULTS/EXPERIMENTS,0.7357954545454546,"From Figure 11, we can see that in this dataset, different methods easily separate the worst workers
from the rest of the crowd (workers in the left tail of the distribution)."
RESULTS/EXPERIMENTS,0.7386363636363636,"5.2.2
LabelMe"
RESULTS/EXPERIMENTS,0.7414772727272727,"Finally, let us evaluate workers for the LabelMe dataset. Because of the lack of data (up to 3 labels
per task), ranking workers is more difficult than in the CIFAR-10H dataset. 0.0 0.2 0.4 0.6 0.8"
RESULTS/EXPERIMENTS,0.7443181818181818,Spam score 0.0 0.2 0.4 0.6 0.8 1.0 1.2 glad 6 7 8 9 10 11 12
RESULTS/EXPERIMENTS,0.7471590909090909,Trace CrowdLayer
RESULTS/EXPERIMENTS,0.75,"0
5
10
Trace DS 80 60 40 20 0 20"
RESULTS/EXPERIMENTS,0.7528409090909091,Trace CoNAL[scale=1e-4]
RESULTS/EXPERIMENTS,0.7556818181818182,"0.0
0.5
1.0
Spam score"
RESULTS/EXPERIMENTS,0.7585227272727273,"0
1
glad"
RESULTS/EXPERIMENTS,0.7613636363636364,"5
10
Trace CrowdLayer"
RESULTS/EXPERIMENTS,0.7642045454545454,"100
0
Trace CoNAL[scale=1e-4]"
RESULTS/EXPERIMENTS,0.7670454545454546,"Figure 12: Comparison of ability scores by workers for the LabelMe dataset. With few labels per
task, workers are more difficult to rank. It is more difficult to separate workers with their abilities in
this crowd. Hence the importance of investigating the generalization performance of the methods
presented in the previous section."
RESULTS/EXPERIMENTS,0.7698863636363636,"We can see in Figure 12 that the number of labels available by task highly impacts the worker
evaluation scores. The spam score, DS model and CoNAL all show similar results in the distribution
shape (bimodal distribution) whereas GLAD and CrowdLayer are more concentrated. However, this
does not account for the ranking of a given worker by the methods considered. The exploration of
the dataset lets us look at different scores, but generalization performance presented in Section 4.3
should also be considered in crowdsourcing. This difference in worker evaluation scores indeed
further highlights the importance of using multiple test metrics to compare the model’s prediction
performance in crowdsourcing. Poorly performing workers could be removed from the dataset with"
RESULTS/EXPERIMENTS,0.7727272727272727,"naive strategies like MV or NS. However, some label aggregation strategies like DS or GLAD can
sometimes use adversarial votes as information – for example in binary classification, with a worker
answering always the opposite label the confusion matrix retrieves the true label. We have seen that
the library peerannot allows users to explore the datasets, both in terms of tasks and workers, and
easily compare predictive performance in this setting."
RESULTS/EXPERIMENTS,0.7755681818181818,"In practice, the data exploration step can be used to detect possible ambiguities in the dataset’s tasks,
but also remove answers from spammers to improve the data quality as shown in Figure 1. The easy
access to the different strategies allows the user to decide if, for their collected dataset, there is a
need for more recent deep-learning-based strategies to improve the results. This is the case for the
LabelMe dataset. Otherwise, the user can decide that standard aggregation-based crowdsourcing
strategies are sufficient and for example, if there are plenty of votes per task like in CIFAR-10H, that
the entropy of the vote distribution is a criterion that identified enough ambiguous tasks for their
case. As often, not a single strategy works best for all datasets, hence the need to perform easy
comparisons with peerannot."
CONCLUSION/DISCUSSION,0.7784090909090909,"6
Conclusion"
CONCLUSION/DISCUSSION,0.78125,"We introduced peerannot, a library to handle crowdsourced datasets. This library enables both
easy label aggregation and direct training strategies with classical state-of-the-art classifiers. The
identification module of the library allows exploring the collected data from both the tasks and the
workers’ point of view for better scorings and data cleaning procedures. Our library also comes
with templated datasets to better share crowdsourced datasets. Going beyond templating, it helps
the crowdsourcing community to have openly accessible strategies to test, compare and improve to
develop common strategies to analyze more and more common crowdsourced datasets."
CONCLUSION/DISCUSSION,0.7840909090909091,"We hope that this library helps reproducibility in the crowdsourcing community and also standardizes
training from crowdsourced datasets. New strategies can easily be incorporated into the open-source
code available on GitHub. Finally, as peerannot is mostly directed to handle classification datasets,
one of our future works would be to consider other peerannot modules to handle crowdsourcing for
object detection, segmentation and even worker evaluation in other contexts like peer-grading."
OTHER,0.7869318181818182,"7
Appendix"
OTHER,0.7897727272727273,"7.1
Supplementary simulation: Simulated mistakes with discrete difficulty levels
on tasks"
OTHER,0.7926136363636364,"For an additional simulation setting, we consider the so-called discrete difficulty presented in Whitehill
et al. (2009). Contrary to other simulations, we here consider that workers belong to two levels of
abilities: good or bad, and tasks have two levels of difficulty: easy or hard. The keyword ratio-diff
indicates the prevalence of each level of difficulty, it is defined as the ratio of easy tasks over hard
tasks:"
OTHER,0.7954545454545454,ratio-diff = ℙ(easy)
OTHER,0.7982954545454546,ℙ(hard) with ℙ(easy) + ℙ(hard) = 1 .
OTHER,0.8011363636363636,"Difficulties are then drawn at random. Tasks that are easy are answered correctly by every worker.
Tasks that are hard are answered following the confusion matrix assigned to each worker (as in
Section 3.2.1). Each worker then answers independently to the presented tasks."
OTHER,0.8039772727272727,"We simulate 𝑛task = 500 tasks and 𝑛worker = 100 with 35% of good workers in the crowd and 50% of
easy tasks. There are 𝐾= 5 possible classes. Each task receives |𝒜(𝑥𝑖)| = 10 labels."
OTHER,0.8068181818181818,"12
15
18
21
24
27
30
| (wj)| 0% 5% 10% 15% 20%"
OTHER,0.8096590909090909,Percent
OTHER,0.8125,"8
9
10
11
12
|
(xi)| 0% 20% 40% 60% 80% 100%"
OTHER,0.8153409090909091,Percent
OTHER,0.8181818181818182,"Figure 13: Distribution of the number of tasks given per worker (left) and of the number of labels per
task (right) in the setting with simulated discrete difficulty levels."
OTHER,0.8210227272727273,"With the obtained answers, we can look at the aforementioned aggregation strategies performance:"
OTHER,0.8238636363636364,"Table 5: AccTrain metric on simulated mistakes made when tasks are associated with a difficulty
level considering classical feature-blind label aggregation strategies."
OTHER,0.8267045454545454,Table 5
OTHER,0.8295454545454546,"MV
GLAD
DS
DSWC[L=2]
DSWC[L=5]
NS"
OTHER,0.8323863636363636,"AccTrain
0.780
0.845
0.810
0.600
0.660
0.790"
OTHER,0.8352272727272727,"Finally, in this setting involving task difficulty coefficients, the only strategy that involves a latent
variable for the task difficulty, knowing GLAD, outperforms the other strategies (see Table 5). Note
that in this case, creating clusters of answers leads to worse decisions than an MV aggregation."
OTHER,0.8380681818181818,"7.2
Comparison with other libraries"
OTHER,0.8409090909090909,"In this section, we provide several comparisons with the Ustalov, Pavlichenko, and Tseitlin (2023)
library."
OTHER,0.84375,"• Framework: peerannot focuses on image classification problems with categorical answers.
crowd-kit also considers textual responses and image segmentation with three aggregation
strategies for each field.
• Data storage: peerannot introduces this .json storage that can handle large datasets. crowd-
kit stores the collected data in a .csv file with columns task, worker, label.
• Identification module: one of the major differences between the two libraries resides in
the identification module of peerannot. This module allows us to explore the dataset
and detect poorly performing workers / difficult tasks easily. crowd-kit only allows us to
explore workers with the accuracy_on_aggregation metric that computes the accuracy of
a worker given aggregated hard labels. peerannot, as demonstrated in Section 5, proposes
several metrics such as the spam score, GLAD’s worker ability coefficient and the trace of the"
OTHER,0.8465909090909091,"confusion matrices. As for the task side, peerannot proposes the different popular metrics
in crowd-kit accompanied with the WAUM (and also the AUMC) metrics from Lefort et al.
(2022) and GLAD’s difficulty coefficients.
• Training: peerannot lets users directly train a neural network architecture from the aggre-
gated labels. This feature is not proposed by crowd-kit.
• Simulation: peerannot created a simulate module to check strategies on. This feature is also
not in the crowd-kit library."
OTHER,0.8494318181818182,"Finally, to compare different strategies across libraries, we implemented a crowdsourcing benchmark
in the Benchopt (Moreau et al. (2022)) library. The Benchopt library allows users to easily compare
and reproduce optimization problem benchmarks between multiple frameworks. After running each
strategy, we measure the cumulated time taken to reach the optimum during the optimization steps.
The metric measured on the y-axis is the AccTrain. Each strategy is run 5 times until convergence.
The differences in results across iterations for the MV strategy come from the randomness in the
choice in case of equalities. We provide a clone of the crowdsourcing benchmark and the results are
obtained by running the following command:"
OTHER,0.8522727272727273,benchopt run ./benchmark_crowdsourcing
OTHER,0.8551136363636364,"First, let us see the performance on the Bluebirds dataset, a small dataset with 39 workers, 108 tasks
and 𝐾= 2 classes."
OTHER,0.8579545454545454,"Figure 14: Aggregation strategies computational time during optimization procedure for the BlueBirds
dataset with K=2."
OTHER,0.8607954545454546,"We see in Figure 14 that the DS strategy from peerannot is the first to reach the optimum, followed
by the Fast-DS strategy and then crowd-kit DS. Other strategies do not lead to better accuracy on
this dataset and DS seems to be the best fitting strategy."
OTHER,0.8636363636363636,"For the LabelMe dataset, DS strategy is also the best aggregation strategy, faster for crowd-kit. The
sensitivity of GLAD’s method to the priors on 𝛼and 𝛽parameters can lead to large performance
differences for real datasets as we see in Figure 15. Note that crowd-kit’s KOS strategy is not
available for this dataset as it is only made for binary classification datasets."
OTHER,0.8664772727272727,"Figure 15: Aggregation strategies computational time during optimization procedure for the LabelMe
dataset with K=8"
OTHER,0.8693181818181818,"7.3
Examples of images in CIFAR-10H and Labelme"
OTHER,0.8721590909090909,"In this section, we provide examples of images from the CIFAR-10H and LabelMe datasets. Both of
these datasets came with known true labels. For CIFAR-10H, the true labels were from the original
CIFAR-10 dataset. For LabelMe, the true labels were determined by the authors at release."
OTHER,0.875,"7.4
Case study with bird sound classification"
OTHER,0.8778409090909091,"We shared our results on the classical CIFAR-10H and LabelMe datasets. More recently, Lehikoinen et
al. (2023) developed a platform for bird sound classification. They released the data for the following
crowdsourcing experiment. Given the sample of the audio of a species (denoted as a letter on their
web portal), users were presented with a new audio sample (the candidate). The question is as follows:
Is the species vocalizing in the candidate the same as the species in the letter¿‘ The answer is a binary yes
or no. In total, 𝑛worker = 205 workers labeled 𝑛task = 79 592 candidates. Each task received between
1 and 77 annotations. Workers answered between 1 and 30 759 tasks (only one worker achieved that
record, and 23% of the workers answered 100 tasks). There is no test set available as is in the original
dataset. However, to have an idea of the level of performance of the label aggregation strategies, we
use the fact that workers reported their level of expertise between 1 and 4. The latter corresponds to
“I am bird researcher or professional birdwatcher”. This generates a test set of 13 041 tasks where the
expert label is used as the current truth. This test set is only used to compute the AccTrain metric.
Note that we do not perform deep-learning methods as the tasks of comparing the birds from two
audio files and designing specific architectures to match this framework is out of the scope of this
paper."
OTHER,0.8806818181818182,"We then can run our aggregation strategies, and from Table 6 we see that strategies reach the same
levels of label recovery, however naive they are. Indeed, most tasks have very few disagreements.
Note that NS and MV performance difference comes from the random tie-breakers in case of equalities."
OTHER,0.8835227272727273,"bird
car
cat
deer
dog"
OTHER,0.8863636363636364,"Figure 16: Example of images from CIFAR-10H. We display images row-wise according to the true
label given initially in CIFAR-10."
OTHER,0.8892045454545454,"coast
forest
highway
insidecity
mountain"
OTHER,0.8920454545454546,"Figure 17: Example of images from LabelMe. We display images row-wise according to the true label
given with the crowdsourced data. 0 4000 8000 12000 16000 20000 24000 28000 32000"
OTHER,0.8948863636363636,| (wj)| 100 101
OTHER,0.8977272727272727,Percent
OTHER,0.9005681818181818,"0
10
20
30
40
50
60
70
80
|
(xi)| 10
2 10
1 100 101 102"
OTHER,0.9034090909090909,Percent
OTHER,0.90625,"Figure 18: Distribution of the number of tasks given per worker (left) and of the number of labels per
task (right) in the Audio Birds letters dataset."
OTHER,0.9090909090909091,"Table 6: AccTrain metric on birds audio dataset considering classical feature-blind label aggregation
strategies."
OTHER,0.9119318181818182,Table 6
OTHER,0.9147727272727273,"MV
DS
GLAD
NS"
OTHER,0.9176136363636364,"AccTrain
0.954
0.946
0.950
0.960"
OTHER,0.9204545454545454,"We can explore what tasks lead to the most disagreements depending on the entropy criterion or
GLAD’s difficulty-estimated latent variable."
OTHER,0.9232954545454546,"Using the entropy criterion, the most difficult tasks (highest entropy) and GLAD’s difficulty, we
recover the index of the most ambiguous tasks."
OTHER,0.9261363636363636,"Highest entropy tasks index: [28272, 25827, 40989, 2771, 55559]
Highest GLAD difficulty index: [2347, 435, 8710, 8992, 51700]"
OTHER,0.9289772727272727,"• Entropy: we obtain the candidate MRG18_20180514_000000_203.mp3 that was to be compared
with the letter HLO15_20180515_021439_31.mp3 (one worker agrees and another disagrees):"
OTHER,0.9318181818181818,./datasets/birds_audio/bird_sound_training_data/audio_files/MRG18_20180514_000000_203.mp3
OTHER,0.9346590909090909,./datasets/birds_audio/bird_sound_training_data/audio_files/HLO15_20180515_021439_31.mp3
OTHER,0.9375,"And the candidate MRG24_20180512_000000_437.mp3 that was to be compared with the letter
HLO12_20180511_150153_42.mp3 (one worker agrees and another disagrees):"
OTHER,0.9403409090909091,./datasets/birds_audio/bird_sound_training_data/audio_files/MRG24_20180512_000000_437.mp3
OTHER,0.9431818181818182,./datasets/birds_audio/bird_sound_training_data/audio_files/HLO12_20180511_150153_42.mp3
OTHER,0.9460227272727273,"• GLAD: we obtain the candidate HLO04_20180511_034424_15.mp3 that was to be compared
with the letter MRG11_20180519_000000_506.mp3 (53 votes, 29 aggreeing and 24 disagreeing):"
OTHER,0.9488636363636364,./datasets/birds_audio/bird_sound_training_data/audio_files/HLO04_20180511_034424_15.mp3
OTHER,0.9517045454545454,./datasets/birds_audio/bird_sound_training_data/audio_files/MRG11_20180519_000000_506.mp3
OTHER,0.9545454545454546,"And the candidate MRG27_20180512_000000_597.mp3 that was to be compared with the letter
HLO01_20180601_080126_30.mp3 (43 votes, 23 aggreeing and 20 disagreeing):"
OTHER,0.9573863636363636,./datasets/birds_audio/bird_sound_training_data/audio_files/MRG27_20180512_000000_597.mp3
OTHER,0.9602272727272727,./datasets/birds_audio/bird_sound_training_data/audio_files/HLO01_20180601_080126_30.mp3
OTHER,0.9630681818181818,"In this dataset, a single task with two different votes has the highest entropy. GLAD’s coefficient lets
us explore tasks with multiple votes where workers were split."
OTHER,0.9659090909090909,"We can also explore the dataset from a worker’s point of view and visualize workers’ performance
and how many are identified as poorly performing. This gives us an idea of the level of noise in the
answers. 0.0 0.2 0.4 0.6 0.8 1.0"
OTHER,0.96875,spam_score
OTHER,0.9715909090909091,"0
1
2
Trace DS 0.0 0.5 1.0 1.5 2.0 2.5 3.0 glad"
OTHER,0.9744318181818182,"0.0
0.5
1.0
spam_score"
OTHER,0.9772727272727273,"0
1
2
3
glad"
OTHER,0.9801136363636364,"Figure 19: Comparison of ability scores by workers for the birds audio dataset. Most workers do
seem to perform similarly, with very little noise voluntarily induced."
OTHER,0.9829545454545454,"From Figure 19, we notice that very few workers are identified as spammers and that different"
OTHER,0.9857954545454546,"worker identification strategies seem to perform similarly. Here we show the worse workers’ indices
depending on each strategy."
OTHER,0.9886363636363636,"Worse workers using GLAD [94, 80, 109, 35, 45]
Worse workers using DS trace [69, 94, 172, 109, 35]
Worse workers using Spam Score [69, 109, 172, 35, 130]"
OTHER,0.9914772727272727,"One of the closing statements of Lehikoinen et al. (2023) is “we learned lessons for how to better
implement similar citizen science projects in the future”. On one hand, identifying the most ambiguous
tasks can help by saving only these tasks to the most expert workers and acquiring better data. On
the other hand, combining the task difficulty with the worker ability performance metrics could help
to create personal feeds of tasks to label and generate more worker participation. Finally, the label
aggregation step can lead to training classifiers with better labels. We hope that allowing easy access
thanks to the peerannot library to each of those steps can indeed help to better implement citizen
science projects and use the collected data."
REFERENCES,0.9943181818181818,"Aitchison, L. 2021. “A Statistical Theory of Cold Posteriors in Deep Neural Networks.” In ICLR.
Cao, P, Y Xu, Y Kong, and Y Wang. 2019. “Max-MIG: An Information Theoretic Approach for Joint
Learning from Crowds.” In ICLR.
Chagneux, M, S LeCorff, P Gloaguen, C Ollion, O Lepâtre, and A Bruge. 2023. “Macrolitter Video
Counting on Riverbanks Using State Space Models and Moving Cameras.” Computo, February.
https://computo.sfds.asso.fr/published-202301-chagneux-macrolitter.
Chu, Z, J Ma, and H Wang. 2021. “Learning from Crowds by Modeling Common Confusions.” In
AAAI, 5832–40.
Dawid, AP, and AM Skene. 1979. “Maximum Likelihood Estimation of Observer Error-Rates Using
the EM Algorithm.” J. R. Stat. Soc. Ser. C. Appl. Stat. 28 (1): 20–28.
Deng, J., W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. “ImageNet: A Large-Scale Hierarchical
Image Database.” In CVPR.
Gao, G, and D Zhou. 2013. “Minimax Optimal Convergence Rates for Estimating Ground Truth from
Crowdsourced Labels.” arXiv Preprint arXiv:1310.5764.
Garcin, C., A. Joly, P. Bonnet, A. Affouard, J.-C. Lombardo, M. Chouet, M. Servajean, T. Lorieul, and
J. Salmon. 2021. “Pl@ntNet-300K: A Plant Image Dataset with High Label Ambiguity and a
Long-Tailed Distribution.” In Proceedings of the Neural Information Processing Systems Track on
Datasets and Benchmarks.
Gruber, S G, and F Buettner. 2022. “Better Uncertainty Calibration via Proper Scores for Classification
and Beyond.” In Advances in Neural Information Processing Systems.
Guo, C, G Pleiss, Y Sun, and KQ Weinberger. 2017. “On Calibration of Modern Neural Networks.” In
ICML, 1321.
Imamura, H, I Sato, and M Sugiyama. 2018. “Analysis of Minimax Error Rate for Crowdsourcing and
Its Application to Worker Clustering Model.” In ICML, 2147–56.
James, GM. 1998. “Majority Vote Classifiers: Theory and Applications.” PhD thesis, Stanford
University.
Kasmi, G, Y-M Saint-Drenan, D Trebosc, R Jolivet, J Leloux, B Sarr, and L Dubus. 2023. “A Crowd-
sourced Dataset of Aerial Images with Annotated Solar Photovoltaic Arrays and Installation
Metadata.” Scientific Data 10 (1): 59.
Khattak, FK. 2017. “Toward a Robust and Universal Crowd Labeling Framework.” PhD thesis,
Columbia University.
Krizhevsky, A, and G Hinton. 2009. “Learning Multiple Layers of Features from Tiny Images.”
University of Toronto.
Lefort, T, B Charlier, A Joly, and J Salmon. 2022. “Identify Ambiguous Tasks Combining Crowdsourced
Labels by Weighting Areas Under the Margin.” arXiv Preprint arXiv:2209.15380."
REFERENCES,0.9971590909090909,"Lehikoinen, P., M. Rannisto, U. Camargo, A. Aintila, P. Lauha, E. Piirainen, P. Somervuo, and O.
Ovaskainen. 2023. “A Successful Crowdsourcing Approach for Bird Sound Classification.” Citizen
Science: Theory and Practice 8 (1): 16. https://doi.org/10.5334/cstp.556.
Lin, Tsung-Yi, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollá r, and C. Lawrence Zitnick. 2014. “Microsoft COCO:
Common Objects in Context.” CoRR abs/1405.0312. http://arxiv.org/abs/1405.0312.
Marcel, S, and Y Rodriguez. 2010. “Torchvision the Machine-Vision Package of Torch.” In Proceedings
of the 18th ACM International Conference on Multimedia, 1485–88. MM ’10. New York, NY, USA:
Association for Computing Machinery.
Moreau, Thomas, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier,
Benjamin Charlier, Mathieu Dagréou, et al. 2022. “Benchopt: Reproducible, Efficient and Collab-
orative Optimization Benchmarks.” In NeurIPS. https://arxiv.org/abs/2206.13424.
Park, Seo Yeon, and Cornelia Caragea. 2022. “On the Calibration of Pre-Trained Language Models
Using Mixup Guided by Area Under the Margin and Saliency.” In ACML, 5364–74.
Passonneau, R J., and B Carpenter. 2014. “The Benefits of a Model of Annotation.” Transactions of the
Association for Computational Linguistics 2: 311–26.
Paszke, A, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, et al. 2019. “PyTorch: An
Imperative Style, High-Performance Deep Learning Library.” In NeurIPS, 8024–35.
Peterson, J C., R M. Battleday, T L. Griffiths, and O Russakovsky. 2019. “Human Uncertainty Makes
Classification More Robust.” In ICCV, 9617–26.
Pleiss, G, T Zhang, E R Elenberg, and K Q Weinberger. 2020. “Identifying Mislabeled Data Using the
Area Under the Margin Ranking.” In NeurIPS.
Raykar, V C, and S Yu. 2011. “Ranking Annotators for Crowdsourced Labeling Tasks.” In NeurIPS,
1809–17.
Rodrigues, F, M Lourenco, B Ribeiro, and F C Pereira. 2017. “Learning Supervised Topic Models for
Classification and Regression from Crowds.” IEEE Transactions on Pattern Analysis and Machine
Intelligence 39 (12): 2409–22.
Rodrigues, F, and F Pereira. 2018. “Deep Learning from Crowds.” In AAAI. Vol. 32.
Rodrigues, F, F Pereira, and B Ribeiro. 2014. “Gaussian Process Classification and Active Learning
with Multiple Annotators.” In ICML, 433–41. PMLR.
Servajean, M, A Joly, D Shasha, J Champ, and E Pacitti. 2016. “ThePlantGame: Actively Training
Human Annotators for Domain-Specific Crowdsourcing.” In Proceedings of the 24th ACM In-
ternational Conference on Multimedia, 720–21. MM ’16. New York, NY, USA: Association for
Computing Machinery.
———. 2017. “Crowdsourcing Thousands of Specialized Labels: A Bayesian Active Training Approach.”
IEEE Transactions on Multimedia 19 (6): 1376–91.
Sinha, V B, S Rao, and V N Balasubramanian. 2018. “Fast Dawid-Skene: A Fast Vote Aggregation
Scheme for Sentiment Classification.” arXiv Preprint arXiv:1803.02781.
Tinati, R, M Luczak-Roesch, E Simperl, and W Hall. 2017. “An Investigation of Player Motivations in
Eyewire, a Gamified Citizen Science Project.” Computers in Human Behavior 73: 527–40.
Ustalov, Dmitry, Nikita Pavlichenko, and Boris Tseitlin. 2023. “Learning from Crowds with Crowd-
Kit.” arXiv. https://arxiv.org/abs/2109.08584.
Whitehill, J, T Wu, J Bergsma, J Movellan, and P Ruvolo. 2009. “Whose Vote Should Count More:
Optimal Integration of Labels from Labelers of Unknown Expertise.” In NeurIPS. Vol. 22.
Yasmin, R, M Hassan, J T Grassel, H Bhogaraju, A R Escobedo, and O Fuentes. 2022. “Improving
Crowdsourcing-Based Image Classification Through Expanded Input Elicitation and Machine
Learning.” Frontiers in Artificial Intelligence 5: 848056.
Zhang, H, M Cissé, Y N. Dauphin, and D Lopez-Paz. 2018. “Mixup: Beyond Empirical Risk Minimiza-
tion.” In ICLR."
