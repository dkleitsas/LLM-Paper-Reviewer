Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004,"Federated Learning (FL) is a machine learning paradigm that allows decentral-
ized clients to learn collaboratively without sharing their private data. However,
excessive computation and communication demands pose challenges to current
FL frameworks, especially when training large-scale models. To prevent these
issues from hindering the deployment of FL systems, we propose a lightweight
framework where clients jointly learn to fuse the representations generated by
multiple ﬁxed pre-trained models rather than training a large-scale model from
scratch. This leads us to a more practical FL problem by considering how to capture
more client-speciﬁc and class-relevant information from the pre-trained models
and jointly improve each client’s ability to exploit those off-the-shelf models. In
this work, we design a Federated Prototype-wise Contrastive Learning (FedPCL)
approach which shares knowledge across clients through their class prototypes
and builds client-speciﬁc representations in a prototype-wise contrastive manner.
Sharing prototypes rather than learnable model parameters allows each client to
fuse the representations in a personalized way while keeping the shared knowledge
in a compact form for efﬁcient communication. We perform a thorough evaluation
of the proposed FedPCL in the lightweight framework, measuring and visualizing
its ability to fuse various pre-trained models on popular FL datasets."
INTRODUCTION,0.008,"1
Introduction"
INTRODUCTION,0.012,"Federated learning (FL) is a promising ﬁeld of machine learning that allows multiple clients to train
together without sharing their private data [1]. Vanilla FL aims to train a single global model over
all participating clients by periodically synchronizing their model parameters. However, the learned
model usually does not perform well on all clients due to the statistical heterogeneity among local
datasets [2, 3, 4]. Personalized federated learning (PFL) is proposed to solve this problem by training a
personalized model for each client. Recent studies on PFL leverage various techniques to enable more
common underlying information shared across different clients [5, 6, 7]. So far, FL and PFL have
been widely used in computer vision [8], natural language processing [9], graph data mining [10, 11],
and some practical applications, e.g., healthcare [12], ﬁnance [13], mobile Internet [14, 15], etc."
INTRODUCTION,0.016,"However, the models in real-world applications are usually large-scale neural networks which incur
high computation costs and require high communication bandwidth when trained from scratch. This
can make it infeasible to train such models in some practical FL scenarios, e.g., low-resource device-
based federated learning. To alleviate the above issues, we propose a lightweight FL framework that
uses multiple ﬁxed pre-trained backbones as the encoder, followed by learnable layers to fuse the rep-
resentations generated by the backbones for each client. The proposed framework is capable of fusing
the representations generated by pre-trained models with various architectures or obtained from vari-"
INTRODUCTION,0.02,"ous source data, expanding the scope of federated learning by integrating off-the-shelf foundation mod-
els. Also, it makes it possible to utilize large-scale pre-trained models, e.g., VisionTransformer [16]
and Swin Transformer [17], in a computation resource-constrained case to enhance the overall
performance. Using the pre-trained foundation models as the ﬁxed encoder can efﬁciently reduce
costs because neither complicated backward propagation computation nor large-scale neural network
transmission between the server and clients is needed during the training stage. As shown in Table 1,
compared to training a ResNet18 [18] from scratch using FedAvg [1], a much smaller number of pa-
rameters are communicated per round when learning to fuse the representations generated by ﬁxed pre-
trained models. Besides, higher performance can be achieved when using pre-trained models after the
same communication rounds. Compared with using a single pre-trained model, using multiple models
pre-trained on diverse datasets can provide a more comprehensive view for an input sample [19]."
INTRODUCTION,0.024,"Table 1: Compared with training a single ResNet18 from
scratch, less training time per round and fewer learnable parame-
ters are needed when using multiple ﬁxed ResNet18 pre-trained
on different datasets [20]. Experiments are implemented with
FedAvg [1] on Digit-5 dataset [21] under feature shift non-IID
setting [3]. The number of communication rounds is 50."
INTRODUCTION,0.028,"Mode
Param.↓
Time↓
Acc↑"
INTRODUCTION,0.032,"Training from scratch
11M
0.95s
31.75(3.07)"
INTRODUCTION,0.036,"Using 1 pre-trained model
0.13M
0.31s
38.65(2.65)
Using 3 pre-trained models
0.40M
0.64s
40.47(3.05)"
INTRODUCTION,0.04,"To enable a better personalized represen-
tation ability for each client under this
lightweight FL framework, we need to
select an appropriate information carrier
to share common underlying knowledge
across clients. Motivated by [22, 23, 24],
class-wise prototypes, deﬁned as “a repre-
sentative embedding” for a speciﬁc class,
can be an effective information carrier
for communication between the server
and clients. Sharing prototypes allows
for better knowledge sharing across vari-
ous learning domains, which has been proved in transfer learning [25] and multi-task learning [26]
scenarios. To efﬁciently extract the useful shared information learned from the pre-trained mod-
els via prototypes, we design an algorithm called Federated Prototype-wise Contrastive Learning
(FedPCL) where both local prototypes and global prototypes are used for knowledge sharing in a
supervised contrastive manner. By maximizing the agreement between the fused representation and
its corresponding prototypes with contrastive learning, class-relevant information and semantically
meaningful knowledge are captured by each client. Concretely, global prototypes force the fused
representation to be closer to the global class center, while local prototypes force clients to share
more higher-level feature information in a pairwise way. Using prototypes to realize inter-client com-
munication allows for knowledge sharing in a latent space and brings additional beneﬁts. Compared
with directly transmitting the representation of a sample, prototypes can eliminate bias from a single
sample and protect clients’ privacy. Compared with model parameters, prototypes are much smaller
in size, which signiﬁcantly reduces communication costs."
INTRODUCTION,0.044,"We quantitatively evaluate the performance of FedPCL and state-of-the-art FL algorithms under
the lightweight framework based on benchmark foundation models and datasets. We also perform
extensive experiments to validate the effectiveness of FedPCL in fusing representations output by
different backbones and its capability to integrate knowledge from backbones with different model
architectures. Our main contributions are summarized as follows:"
INTRODUCTION,0.048,"• We take the ﬁrst step towards integrating pre-trained models into federated learning to form
a lightweight FL framework, which substantially reduces computation and communication
costs of current FL frameworks.
• We further propose FedPCL, a novel algorithm that uses class prototypes as the information
carrier and conducts contrastive learning during local updates, which allows clients to share
more class-relevant knowledge from both local and global prototypes.
• Experiments are conducted on a variety of benchmark foundation models and datasets to
measure FedPCL’s ability to fuse various pre-trained models for each client. The results
indicate that FedPCL outperforms baselines with a better personalization and knowledge
integration ability."
RELATED WORK,0.052,"2
Related Work"
RELATED WORK,0.056,"Personalized Federated Learning. Personalized Federated Learning (PFL) aims to train a personal-
ized model for each client in the FL framework so that the model can achieve better performance
on the local dataset. Existing approaches for PFL are based on various techniques. [27, 28, 29] add"
RELATED WORK,0.06,"an additional term to the original loss function of each client to produce better personalized models
according to the private data. [3, 7] share part of the model and keep personalized layers private
to achieve personalization. [30] proposes to use a central hypernetwork to generate personalized
models for clients. [31] enables a more ﬂexible personalization by adaptively weighted aggregation.
[5, 32] study PFL from a meta-learning perspective where a meta-model is learned to generate the
initialized local model for each client."
RELATED WORK,0.064,"Contrastive Learning. Contrastive learning has been widely applied to self-supervised learning
scenarios in recent years, achieving state-of-the-art performance in the unsupervised training of deep
image models [33, 34] and graph models [35, 36, 37]. A number of works are focused on learning an
encoder where the embeddings of the same sample are pulled closer and those of different samples
are pushed apart [38, 39, 40, 41]. [42] extends contrastive learning from self-supervised settings to
fully supervised settings, enabling us to better exploit label information with contrastive learning.
There are also some works incorporate contrastive learning into federated learning to assist local
training to achieve higher model performance [43, 44, 45]."
RELATED WORK,0.068,"Prototype Learning. Prototypes have been widely used in a variety of tasks in transfer learning [25],
multi-task learning [26], and few-shot learning [22, 46, 47]. It is usually deﬁned as the mean feature
vectors of samples within the same class [48, 49]. The authors in [50] represent task-agnostic
information by prototypes for distributed machine learning systems and propose a multi-task model
fusion method that integrates prototypes for a new task. Since the prototype has the ability to
generalize semantic knowledge from similar samples, it is used to assist the local training in federated
learning by several studies [51, 24, 44, 43]."
RELATED WORK,0.072,"Pre-Trained Foundation Model. Due to the huge number of parameters and the broad data available
for training, pre-trained foundation models can better capture knowledge for downstream tasks and
lead to green AI [52, 53]. Recently, pre-trained models (e.g., ViT [16], DETR [54], BERT [55]) have
been widely investigated in both vision and natural language processing (NLP) tasks [56]. Extracting
task-speciﬁc knowledge from the pre-trained models has the potential to achieve the state-of-the-art
performance due to their generality and adaptability to different tasks [57, 58, 59]. However, how to
integrate the pre-trained models into a federated learning paradigm and keep the whole framework
lightweight still remains an open problem."
PROBLEM FORMULATION,0.076,"3
Problem Formulation
In this section, we formulate our proposed lightweight FL framework which integrates off-the-shelf
pre-trained models as ﬁxed backbones and learns to fuse them adaptively. We start from the general
framework of FL, and then explicitly deﬁne the problem and explain the proposed global objective."
PROBLEM FORMULATION,0.08,"General FL Framework. Formally, the global objective of general FL across m clients is"
PROBLEM FORMULATION,0.084,"min
(w1,w2,··· ,wm)
1
m m
X i=1 |Di|"
PROBLEM FORMULATION,0.088,"N Li (wi; Di)
(1)"
PROBLEM FORMULATION,0.092,"where Li and wi are the local loss function and model parameters for client i, respectively. Di is the
private dataset of the i-th client. N is the total number of samples among all clients."
PROBLEM FORMULATION,0.096,"The objective of general FL, such as vanilla FedAvg and its variants, is to learn an optimal global
model w across m clients, where w = w1 = w2 = · · · = wm [1]. This can be achieved by
periodically synchronizing the model parameters of all clients at the server. However, parameter
synchronization sometimes deteriorates the performance on local datasets in the presence of data
heterogeneity [7]. Some recent studies [6, 60] also explore personalized FL by applying various
constraints and regularization terms, which allows clients to keep different models wi, i ∈[1, m] to
achieve higher performance on their local datasets."
PROBLEM FORMULATION,0.1,"The Proposed Lightweight FL Framework. Similar to most FL frameworks, there are m clients
and a central server involved in the multiple pre-trained backbone-based framework. Each client
i ∈[1, m] owns K shared and ﬁxed backbones and a private dataset Di that cannot be shared with
each other. Each learning model can be seen as a combination of at least two parts: (i) Feature
Encoder r(·; Φ∗) : Rd →RK×de, comprising K ﬁxed pre-trained backbones, each of which maps
the raw sample x of size d to a representation vector of size de. K representation vectors are
concatenated together as the output r(x; Φ∗), denoted as rx for short. (ii) Projection Network
h (·; θi) : RK×de →Rdh, which fuses the K representation vectors and maps rx from one latent
space to another for further representation learning. The formal deﬁnition is provided as follows."
PROBLEM FORMULATION,0.104,Backbones 𝑟! ...
PROBLEM FORMULATION,0.108,Client 1
PROBLEM FORMULATION,0.112,Server ... |ℂ| { avg ... ... ... 𝐿!
PROBLEM FORMULATION,0.116,"Eq. (6) 𝑪""
𝑪# avg ... ... 𝑪"" 𝑪# ... ℂ ... 𝐿$"
PROBLEM FORMULATION,0.12,"Client 𝑚 𝐿! ... ... 𝑪"" 𝑪# ... ℂ ... 𝐿$"
PROBLEM FORMULATION,0.124,Backbones 𝑟!
PROBLEM FORMULATION,0.128,Projection
PROBLEM FORMULATION,0.132,Network
PROBLEM FORMULATION,0.136,"ℎ(&; 𝜃"")"
PROBLEM FORMULATION,0.14,Projection
PROBLEM FORMULATION,0.144,Network
PROBLEM FORMULATION,0.148,ℎ(&; 𝜃#) 𝑧(𝐱) 𝑧(𝐱)
PROBLEM FORMULATION,0.152,"Local prototype
Global prototype 𝐱 𝐱"
PROBLEM FORMULATION,0.156,"Figure 1: An overview of the proposed lightweight federated learning framework. This example assumes
that for each client, there are three pre-trained backbones, with the block in different colors illustrating their
backbone-speciﬁc representation."
PROBLEM FORMULATION,0.16,"Deﬁnition 3.1. Let φ∗
k, where k ∈[1, K], denote the optimal parameter of the k-th backbone pre-
trained on a speciﬁc dataset, rk be the embedding function of the k-th backbone, and x denote an
instance sampled from a local dataset. We deﬁne the concatenated representation output by the
feature encoder as
r(x; Φ∗) := concat (r1(x; φ∗
1), . . . , rK(x; φ∗
K)) .
(2)"
PROBLEM FORMULATION,0.164,"For client i, the projection network h, parameterized by θi, aims to fuse the representations output by
multiple backbones into another abstract space. The output of the projection network is computed as"
PROBLEM FORMULATION,0.168,"z(x) = h (rx; θi) .
(3)"
PROBLEM FORMULATION,0.172,"Based on the above deﬁnition, we formulate the global objective of the lightweight FL framework as"
PROBLEM FORMULATION,0.176,"min
{θ1,θ2,...,θm} m
X i=1 |Di|"
PROBLEM FORMULATION,0.18,"N E(x,y)∈Di[Li (θi; z (x) , y)]"
PROBLEM FORMULATION,0.184,"s.t. z(x) = h (r(x; Φ∗); θi) where Φ∗= {φ∗
1, φ∗
2, · · · , φ∗
K}. (4)"
PROBLEM FORMULATION,0.188,"The target of the framework is to learn the personalized projection network {θi}m
i=1 for each client.
Given an input sample x, the representations output by pre-trained backbones are concatenated
together as r(x; Φ∗). Then, the projection network h(θi) optimized for each client converts the
concatenated representation to z(x)."
PROBLEM FORMULATION,0.192,"The proposed multi-backbone setting is friendly to most FL methods. The learnable parameter θ in
the projection network can be fully or partly shared across clients for different purposes. For example,
FedAvg [1] and its variants [61, 62, 63, 64] can synchronize θ every round, and personalized FL
methods can also adapt to this framework without much modiﬁcation [27, 7, 65]."
PROBLEM FORMULATION,0.196,"4
Federated Prototype-wise Contrastive Learning (FedPCL)"
PROBLEM FORMULATION,0.2,"In this section, we elaborate our proposed algorithm Federated Prototype-wise Contrastive Learning
(FedPCL), which is illustrated in Figure 1. In each client, z(x) is generated by the projection network
which fuses the representations from multiple backbones. Then, to share the common underlying
knowledge across clients, we employ a prototype-based communication scheme to transmit and
aggregate prototype sets between the server and clients. With the prototypes returned from the
server, we perform local optimization via well-crafted prototype-wise contrastive loss function, which
extracts class-relevant information while sharing more inter-client knowledge in the latent space. We
provide detailed illustration for each procedure in the rest of this section."
PROBLEM FORMULATION,0.204,"Prototype as the Information Carrier. To capture more class-relevant information and semantically
meaningful knowledge, we propose to transmit prototypes between the server and clients. Compared
with transmitting the learnable model parameters, there are several advantages brought by transmitting
class-wise prototypes. Firstly, prototype is more compact in form, which signiﬁcantly decreases
communication costs required during the training process. Secondly, non-parametric communication"
PROBLEM FORMULATION,0.208,"allows each client to learn a more customized local model without synchronizing parameters with
others. Thirdly, prototypes are high-level statistic information rather than raw features, which raise
no additional privacy concerns to the system and are robust to gradient-based attacks [66, 67]."
PROBLEM FORMULATION,0.212,"To extract useful class-relevant information from the local side, we construct a local prototype as
the information carrier for knowledge uploading. Speciﬁcally, it is deﬁned in the latent space of
projection network’s output by the mean of the fused representations within the same class j,"
PROBLEM FORMULATION,0.216,"C(j)
i
:=
1
|Di,j| X"
PROBLEM FORMULATION,0.22,"(x,y)∈Di,j
z (x) ,
(5)"
PROBLEM FORMULATION,0.224,"where Di,j refers to the subset of Di composed of all instances belonging to class j, and Ci denotes
the local prototype set of the i-th client. After the above computation, the local prototype set of each
client is sent to the central server for knowledge aggregation, which shares the local class-relevant
information extracted on each speciﬁc client based on its local dataset."
PROBLEM FORMULATION,0.228,"Server Aggregation. After receiving local prototype sets {Ci}m
i=1 from all participating clients, the
server ﬁrst computes the global prototype as"
PROBLEM FORMULATION,0.232,"¯C(j) :=
1
|Nj| X i∈Nj"
PROBLEM FORMULATION,0.236,"|Di,j|"
PROBLEM FORMULATION,0.24,"Nj
C(j)
i
,
(6)"
PROBLEM FORMULATION,0.244,"where Nj denotes the set of clients that own instances of class j, and Nj denotes the number
of instances belonging to class j over all clients. The global prototype set is denoted as C =
{ ¯C(1), ¯C(2), . . . }. With such an aggregation mechanism, the global prototype set summarizes coarse-
grained class-relevant knowledge shared by all clients, which provides a high-level perspective for
representation learning."
PROBLEM FORMULATION,0.248,"After aggregation, the server sends both the global prototype set and the full local prototype sets
collected from all clients back to every clients. For the situation where only a few classes in each
client in some non-IID cases, we introduce a prototype padding procedure in the server to ensure that
each local prototype set contains prototypes corresponding to all classes:"
PROBLEM FORMULATION,0.252,"C(j)
i
="
PROBLEM FORMULATION,0.256,"(
C(j)
i
,
i ∈Nj
¯C(j),
i /∈Nj
.
(7)"
PROBLEM FORMULATION,0.26,"Prototype-based communication and aggregation allow each client to own a unique projection network
that is able to fuse the general representations in a customized way. The returned local prototype sets
can encourage a mutual learning from a client-relevant perspective while the global prototype set,
where each element indicates a class center in the overall data, provides a chance to learn from a
highly summarized client-irrelevant perspective."
PROBLEM FORMULATION,0.264,"Local Training. After receiving the prototype sets from the server, the main target of local training
is to efﬁciently extract useful knowledge from the local and the global prototypes, respectively, so
as to maximally beneﬁt local representation learning. To achieve that, we propose a prototype-wise
supervised contrastive loss that consists of two terms, i.e., global term and local term."
PROBLEM FORMULATION,0.268,"To force the fused representation z(x) generated by the local projection network to be closer to its
corresponding global class center so as to extract more class-relevant but client-irrelevant information,
we deﬁne the global prototype-based loss term as"
PROBLEM FORMULATION,0.272,"Lg =
X"
PROBLEM FORMULATION,0.276,"(x,y)∈Di
−log
exp

zx · ¯C(y)/τ
 P"
PROBLEM FORMULATION,0.28,"ya∈A(y) exp
 
zx · ¯C(ya)/τ
.
(8)"
PROBLEM FORMULATION,0.284,"where zx represents z(x) for short, A(y) := {ya ∈[1, |C|] : ya ̸= y} is the set of labels distinct from
y, τ is the temperature that can adjust the tolerance for feature difference [68, 41, 42]. For a speciﬁc
instance x sampled from Di, we use an inner dot product to measure the similarity between the fused
representation zx and prototypes."
PROBLEM FORMULATION,0.288,"Apart from the global term, to align z(x) with each client’s local prototypes by alternate client-wise
contrastive learning in the latent space and enable more inter-client knowledge sharing, we deﬁne the
local prototype-based loss term as"
PROBLEM FORMULATION,0.292,"Lp =
X"
PROBLEM FORMULATION,0.296,"(x,y)∈Di
−1 m m
X"
PROBLEM FORMULATION,0.3,"p=1
log
exp

zx · C(y)
p /τ
 P"
PROBLEM FORMULATION,0.304,"ya∈A(y) exp

zx · C(ya)
p
/τ
.
(9)"
PROBLEM FORMULATION,0.308,"Given the ground-truth label y of x, a prototype set can be divided as one positive prototype C(y)"
PROBLEM FORMULATION,0.312,"and a set of negative prototypes C(ya). Both Lg and Lp force the representation of a sample to be
closer to those positive prototypes and apart from those negative prototypes. ¯C(y) in Lg and C(y) in
Lp summarize abstract class-relevant information to different granularity, which provides guidance
for optimizing the local projection network from different perspectives. For the i-th client, the local
objective function in Eq. (4) is deﬁned as a combination of Lg and Lp in the following form,"
PROBLEM FORMULATION,0.316,"L

θi; z (x) , y, C, {Cp}m
p=1

= Lg (θi; z (x) , y, C) + Lp(θi; z (x) , y, {Cp}m
p=1).
(10)"
PROBLEM FORMULATION,0.32,"Algorithm 1 FedPCL
Input: Di, θi, i = 1, · · · , m, and K pre-trained back-
bones with parameters φ∗
1, φ∗
2, · · · , φ∗
K, respectively.
Server executes:"
PROBLEM FORMULATION,0.324,"1: Initialize prototype sets {Cp}m
p=1.
2: for each round T = 1, 2, ... do
3:
for each client i in parallel do
4:
Ci ←LocalUpdate(i, C, {Cp}m
p=1)
5:
end for
6:
Update global prototype by Eq. (6).
7: end for
LocalUpdate(i, C, {Cp}m
p=1):
1: for each local epoch do
2:
for each batch in Di do
3:
Compute Lg by Eq. (8) with global prototypes.
4:
Compute Lp by Eq. (9) with local prototypes.
5:
Update θi by Eq. (10).
6:
end for
7: end for
8: Compute local prototypes by Eq. (5).
9: return Ci"
PROBLEM FORMULATION,0.328,"At the end of the local training in each
round, clients upload their local prototype
set Ci to the server. We present the Fed-
PCL algorithm in detail in Algorithm 1."
PROBLEM FORMULATION,0.332,"Prototype-based Inference.
After ade-
quate optimization, in each client, the local
prototypes generated by projection network
not only contain compact local class-wise
information but also absorb general knowl-
edge from all participating clients. Consid-
ering their powerful representation capabil-
ity, we leverage the local prototypes to pre-
dict unknown labels in the inference stage.
Concretely, for a test sample, we ﬁrst calcu-
late the similarity scores between the out-
put of projection network and every local
prototypes. Then, the predicted result is the
class with the maximum similarity score."
PROBLEM FORMULATION,0.336,"Generalization Bound.
We provide in-
sights into the performance analysis of Fed-
PCL. A detailed description and derivations
can be found in Appendix C."
PROBLEM FORMULATION,0.34,"Theorem 4.1. (Generalization Bound of FedPCL.) Consider an FL system with m clients. Let
D1, D2, · · · , Dm be the true data distribution and bD1, bD2, · · · , bDm be the empirical data distribution.
Denote the projection network h as the hypothesis from H and d be the VC-dimension of H. The total
number of samples over all clients is N. Then, with probability at least 1 −δ:"
PROBLEM FORMULATION,0.344,"max
(θ1,θ2,...,θm)  m
X i=1 |Di|"
PROBLEM FORMULATION,0.348,"N LDi

θi; C, {Cp}m
p=1

− m
X i=1 |Di|"
PROBLEM FORMULATION,0.352,"N L b
Di"
PROBLEM FORMULATION,0.356,"
θi; C, {Cp}m
p=1
 ≤ r N"
PROBLEM FORMULATION,0.36,"2 log (m + 1)|C| δ
+ r"
PROBLEM FORMULATION,0.364,"d
N log eN d . (11)"
PROBLEM FORMULATION,0.368,"Theorem 4.1 indicates that compared with the model trained on an ideal data distribution, the
performance of the empirically trained model is associated with the VC-dimension of H and the size
of the prototype set C. An expected performance gap can be achieved by using appropriate projection
network and number of classes. The generalization bound is also important to ensure a satisfying
performance of FedPCL, especially when applying it to some safety-critical scenarios [60, 69, 70]."
EXPERIMENTS,0.372,"5
Experiments"
EXPERIMENTAL SETUP,0.376,"5.1
Experimental Setup"
EXPERIMENTAL SETUP,0.38,"Datasets and Non-IID Settings. We evaluate our proposed framework on the following three
benchmark datasets: Digit-5 [21], Ofﬁce-10 [71], and DomainNet dataset [72]. Digit-5 [21] is a
benchmark dataset for digit recognition, including ﬁve benchmark datasets, namely SVHN, USPS,
SynthDigits, MNIST-M, and MNIST. Ofﬁce-10 [71] is a standard benchmark dataset consisting
of four datasets, namely Amazon, Caltech, DSLR and WebCam. DomainNet [73] is a large-scale
dataset consisting of six datasets, namely Clipart, Info, Painting, Quickdraw, Real, and Sketch."
EXPERIMENTAL SETUP,0.384,"Table 2: Test accuracy under the feature shift non-IID setting. In the column labeled BB (short for backbone), s
is for a single pre-trained backbone and m is for multiple pre-trained backbones. # of Comm Params refers to the
average number of parameters sent from a client to the server per round."
EXPERIMENTAL SETUP,0.388,"BB
Method
MNIST
SVHN
USPS
Synth
MNIST-M
Avg
# of Comm
Params s"
EXPERIMENTAL SETUP,0.392,"FedAvg
70.65(1.15)
17.10(0.20)
70.24(1.62)
32.90(0.75)
29.33(1.18)
44.04(0.98)
133,632
pFedMe
71.13(3.63)
13.18(1.78)
69.20(0.30)
36.25(3.35)
25.25(2.25)
43.00(2.26)
133,632
PerFedAvg
52.68(7.03)
16.28(1.23)
53.66(6.58)
29.05(3.45)
24.38(2.38)
35.21(4.13)
133,632
FedRep
64.00(2.20)
17.88(1.08)
70.44(1.27)
36.50(1.55)
31.90(0.05)
44.14(2.03)
131,072
FedProto
80.40(2.75)
17.03(0.38)
88.47(0.91)
40.90(1.10)
32.85(0.75)
51.93(1.18)
2,560
Solo
60.40(2.25)
15.60(0.20)
75.28(4.48)
34.65(0.05)
28.48(0.53)
42.88(1.50)
-
Ours
82.75(0.40)
18.12(0.42)
88.82(0.15)
41.40(0.60)
33.05(0.95)
52.83(0.21)
2,560 m"
EXPERIMENTAL SETUP,0.396,"FedAvg
71.68(2.93)
18.45(0.45)
72.95(0.86)
37.35(1.35)
33.70(2.55)
46.83(1.63)
395,776
pFedMe
67.45(2.70)
15.43(0.38)
65.66(7.20)
33.55(4.60)
31.80(0.20)
42.78(3.01)
395,776
PerFedAvg
56.03(2.73)
17.03(0.63)
57.55(0.27)
34.90(2.80)
30.98(1.53)
39.30(1.59)
395,776
FedRep
77.25(1.75)
16.40(0.50)
80.25(0.32)
37.63(2.18)
36.53(0.28)
49.61(1.05)
393,216
FedProto
83.78(0.83)
17.90(0.10)
91.74(0.00)
43.70(2.45)
36.43(1.58)
54.71(0.99)
2,560
Solo
70.43(4.63)
15.00(0.40)
84.90(0.24)
37.18(2.73)
34.35(2.20)
48.37(2.04)
-
Ours
84.65(0.15)
19.38(0.63)
90.74(0.53)
44.73(0.37)
37.25(0.28)
55.34(0.34)
2,560"
EXPERIMENTAL SETUP,0.4,"To mimic non-IID scenarios in a more general way, we investigate three different non-IID settings:
(i) Feature shift non-IID: The datasets owned by clients have the same label distribution but different
feature distributions. (ii) Label shift non-IID: The datasets owned by clients have the same feature
distribution but different label distributions which is simulated by Dirichlet distribution with parameter
α [69]. (iii) Feature & Label shift non-IID: The datasets owned by clients are different in both label
distribution and feature distribution, which is more common but challenging in real-world scenarios.
Details about the non-IID settings can be found in Appendix A.1."
EXPERIMENTAL SETUP,0.404,"Baselines and Implementation. We compare our proposed method with popular FL algorithms
including FedAvg [1], pFedMe [27], PerFedAvg [5], FedRep [7], FedProto [24], and Solo, i.e.,
training independently within each client. In feature shift and label shift non-IID setting, the number
of clients is 5. In feature & label shift setting, the number of clients is 5, 4, 6 for Digit-5, Ofﬁce-10,
and DomainNet, respectively."
EXPERIMENTAL SETUP,0.408,"Similar to [19, 20], we use the ResNet18 [18] pre-trained on Quick Draw, Aircraft, and CU-Birds [20]
as the backbones. For all baseline methods, the backbone module is followed by two fully connected
layers, corresponding to projection network and classiﬁer, respectively, whereas for our method, the
backbone module is followed by only one fully connected layer as the projection network. The output
dimension of each backbone and the projection network are 512 and 256, respectively. Note that for
a fair comparison, all baselines use the same network architecture on top of the frozen backbones as
FedPCL except their essential classiﬁer."
EXPERIMENTAL SETUP,0.412,"We use a batch size of 32, and an Adam [74] optimizer with weight decay 1e-4 and learning rate
0.001. The default setting for local update epochs is E = 1 and the temperature τ is 0.07. We
implement all the methods using PyTorch and conduct all experiments on one NVIDIA Tesla V100
GPU. Details about the model and each dataset can be found in Appendix A.1."
PERFORMANCE COMPARISON,0.416,"5.2
Performance Comparison"
PERFORMANCE COMPARISON,0.42,"Table 3: Test accuracy under the feature & label
shift non-IID setting for Ofﬁce-10, under the
label shift non-IID setting for DomainNet."
PERFORMANCE COMPARISON,0.424,"Method
Ofﬁce-10
Domainnet"
PERFORMANCE COMPARISON,0.428,"FedAvg
33.84(4.59)
28.09(2.91)
pFedMe
30.00(1.41)
32.65(0.72)
Per-FedAvg
26.04(1.46)
34.64(0.54)
FedRep
37.24(1.54)
48.82(0.55)
FedProto
34.54(2.65)
44.48(0.58)
Solo
36.38(0.54)
46.70(0.75)
Ours
41.40(1.19)
52.92(3.47)"
PERFORMANCE COMPARISON,0.432,"Table 2 and Table 3 report the results of our method
and baselines in mean (std) format over clients with
three independent runs. Table 2 shows the results of
two cases: (1) s: a single backbone is available; (2) m:
multiple backbones are available."
PERFORMANCE COMPARISON,0.436,"The results suggest that: (1) compared with single
backbone cases, multiple pre-trained backbones lead to
higher test accuracy in most cases and about a 1%−4%
test accuracy improvement for our method; (2) apart
from locally training each client (Solo), our method
achieves relatively smaller deviation across different
runs compared with most baselines, which demon-
strates that FedPCL is able to fuse the representations in a more stable way; (3) the number of
communicated parameters in prototype-based method is much lower than that of the model parameter-
based methods."
PERFORMANCE COMPARISON,0.44,"10
5
2
1
0.5
Non-iid level 10 20 30 40 50 60"
PERFORMANCE COMPARISON,0.444,Avg test acc
PERFORMANCE COMPARISON,0.448,"FedRep
Solo
FedPCL (a)"
PERFORMANCE COMPARISON,0.452,"5
10
20
40
80
Num of clients 10 20 30 40 50"
PERFORMANCE COMPARISON,0.456,Avg test acc
PERFORMANCE COMPARISON,0.46,"FedAvg
FedPCL"
PERFORMANCE COMPARISON,0.464,"(b)
Figure 2: Test accuracy on Digit-5 under varying non-IID
levels (left) and varying numbers of clients (right). Both are
conducted in the label shift non-IID setting. In (a), non-IID
level is controlled by α, ranging from 0.5 to 10. In (b), the
number of clients (m) ranges from 5 to 80."
PERFORMANCE COMPARISON,0.468,"Robustness to varying levels of hetero-
geneity.
In the label shift non-IID set-
ting, we use Dirichlet distribution with
parameter α > 0 to simulate the hetero-
geneity level over 10 clients.
As α be-
comes smaller, the label distribution be-
comes more heterogeneous. We evaluate
our proposed FedPCL with FedRep and
Solo by varying α ∈{0.5, 1, 2, 5, 10} on
10 clients. Figure 2(a) suggests that Fed-
PCL achieves the best performance for
all values of α. Moreover, FedPCL has
the smallest deviation across several runs,
which implies the stable convergence and
robustness of FedPCL."
PERFORMANCE COMPARISON,0.472,"Effect of varying numbers of participating clients. We also compare FedPCL with the vanilla
FedAvg by varying the number of clients (m) from 5 to 80 as shown in Figure 2(b). The non-IID
level remains the same when more clients participate in the training procedure. The results show
that the average test accuracy of FedPCL is higher than that of FedAvg by about 15%. Furthermore,
the deviation of FedPCL across three runs is much lower than that of FedAvg, especially when m
becomes larger. This indicates the scalability of the lightweight framework and the ability of FedPCL
to adapt to a large-scale FL system."
INTEGRATING BACKBONES WITH VARIOUS ARCHITECTURES,0.476,"5.3
Integrating Backbones with Various Architectures"
INTEGRATING BACKBONES WITH VARIOUS ARCHITECTURES,0.48,"Table 4: Integrating backbones with various architec-
tures into the proposed framework. Experiments are
implemented with FedPCL under feature & label shift
non-IID setting."
INTEGRATING BACKBONES WITH VARIOUS ARCHITECTURES,0.484,"Backbone-Domain
Digit-5
Ofﬁce-10"
INTEGRATING BACKBONES WITH VARIOUS ARCHITECTURES,0.488,"[ResNet18-QuickDraw,
ResNet18-Aircraft,
ResNet18-Birds]"
INTEGRATING BACKBONES WITH VARIOUS ARCHITECTURES,0.492,42.87(1.47) 41.40(1.19)
INTEGRATING BACKBONES WITH VARIOUS ARCHITECTURES,0.496,"[MLP-ImageNet,
AlexNet-ImageNet,
VGG11-ImageNet]"
INTEGRATING BACKBONES WITH VARIOUS ARCHITECTURES,0.5,55.80(2.09) 70.11(1.78)
INTEGRATING BACKBONES WITH VARIOUS ARCHITECTURES,0.504,"[tiny-ViT-ImageNet,
small-ViT-ImageNet,
base-ViT-ImageNet]"
INTEGRATING BACKBONES WITH VARIOUS ARCHITECTURES,0.508,40.96(2.87) 84.63(2.57)
INTEGRATING BACKBONES WITH VARIOUS ARCHITECTURES,0.512,"The above experiments use backbones with the
same architecture but pre-trained on different
source data, showing the capability of FedPCL
on integrating knowledge from different source
domains. In Table 4, we also show that our
proposed lightweight framework is capable of
(i) using pre-trained models with different archi-
tectures, i.e., a two-layer CNN, AlexNet [75],
and VGGNet [76]; (ii) integrating large-scale
pre-trained models, i.e., ViT [16, 77], to en-
hance the performance without the need of huge
computation resources to train or ﬁne-tune them.
Pre-trained models with different representation
abilities should be adaptively leveraged for dif-
ferent tasks to achieve expected performance.
They can be selected based on a small set of
data before large-scale training."
ABLATION STUDY,0.516,"5.4
Ablation Study"
ABLATION STUDY,0.52,"In this section, we present the results for various ablation experiments to test the effect of global/local
prototypes and the contrastive loss. More results can be found in Appendix A.2.3."
ABLATION STUDY,0.524,"Table 5: Comparison between the cases when dif-
ferent local losses are used for local training. Ex-
periments are conducted on Digit-5 dataset under
the feature & label shift non-IID setting where the
Dirichlet parameter α is 1, the number of clients is
5, and the number of pre-trained backbones is 3."
ABLATION STUDY,0.528,"Loss Type
Acc"
ABLATION STUDY,0.532,"Cross Entropy
32.98(3.44)
Cross Entropy + ProtoDist [24]
43.94(3.02)
Supervised Contrastive [42]
42.18(3.25)
Ours
45.22(3.65)"
ABLATION STUDY,0.536,"Effect of the contrastive loss. We test the perfor-
mance of the local loss function in different forms:
(1) Cross Entropy loss; (2) Cross Entropy loss + Pro-
toDist term, which takes the distance between proto-
type and fused representation as an additional term
to regularize the cross entropy loss; (3) Supervised
Contrastive loss that only uses local embedding for
contrastive learning; (4) Our prototype-wise con-
trastive loss that uses both global and local proto-
types for local contrastive learning. As shown in
Table 5, our prototype-wise supervised contrastive
loss outperforms others."
ABLATION STUDY,0.54,"Effect of prototypes. To verify the effectiveness of different kinds of prototypes used for local
training in our proposed FedPCL, we compare the following three cases: (i) Only global prototypes
computed at the server are used for local training; (ii) Only local prototypes aggregated at the server
are used for local training; (iii) Both global and local prototypes are used for local training. All these
three cases are implemented under three non-IID settings. As shown in Table 6, without global or
local prototypes being used for local supervised contrastive learning, the performance drops 0.3%-2%,
indicating that the knowledge conveyed by global prototypes and local prototypes can beneﬁt the
local learning framework from different perspectives."
ABLATION STUDY,0.544,"Table 6: Comparison between the cases when only global prototypes are used for local training, only local
prototypes from all clients are used for local training, and both of them are used for local training. Experiments
are conducted on Digit-5 dataset under three non-IID settings where the Dirichlet parameter α is 1, the number
of clients is 5, and the number of pre-trained backbones is 3."
ABLATION STUDY,0.548,"Prototypes
Feature shift non-IID
Label shift non-IID
Feature & Label shift non-IID"
ABLATION STUDY,0.552,"Global only
54.69(0.14)
44.75(1.77)
43.79(4.09)
Local only
55.01(0.10)
44.52(1.73)
43.08(3.87)
Global and local
55.34(0.34)
45.35(1.58)
45.22(3.65)"
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.556,"5.5
Visualizing the Fusing Results of FedPCL MNIST SVHN USPS Synth"
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.56,MNIST-M
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.564,Quickdraw
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.568,Aircraft Birds
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.572,0.4 0.5 0.5 0.6 0.5
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.576,0.4 0.5 0.4 0.3 0.5
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.58,"0.2
0
0.1 0.1 0.1 0.0 0.2 0.4 0.6"
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.584,(a) Digit-5
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.588,Amazon
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.592,Caltech DSLR
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.596,Webcam
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.6,Quickdraw
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.604,Aircraft Birds
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.608,0.5 0.5 0.5 0.6
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.612,0.5 0.4 0.4 0.3
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.616,"0
0.1 0.2 0.1 0.0 0.2 0.4 0.6"
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.62,"(b) Ofﬁce-10
Figure 3: Similarity scores generated on two datasets: (a) Digit-5
and (b) Ofﬁce-10. Rows correspond to the pre-trained backbones
rk(x) and columns correspond to different local datasets with
feature shift."
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.624,"To better understand how FedPCL fuses
the representation output by backbones,
we visualize the normalized similarity
scores under the feature shift non-IID
setting. The heatmap in Figure 3 sum-
marizes the values of the normalized
similarity scores, computed by the in-
ner product between local prototypes in
the single backbone case and the multi-
backbone case. For example, in Fig-
ure 3(a), the element 0.5 on row 1 and
column 2 represents the similarity score
between the local prototype computed when only one backbone pre-trained in Quickdraw is available
and the local prototype computed when three backbones pre-trained in Quickdraw, Aircraft, and
Birds are available."
VISUALIZING THE FUSING RESULTS OF FEDPCL,0.628,"We ﬁnd that the backbone pre-trained on Quickdraw contributes more to the fused prototype, while
the backbone pre-trained on Birds contributes the least. Clients who own a speciﬁc dataset show
different levels of utilization when fusing the representations from various backbones."
INCORPORATING WITH PRIVACY-PRESERVING TECHNIQUES,0.632,"5.6
Incorporating with Privacy-Preserving Techniques
To further eliminate concerns about privacy leakage, we also incorporate FedPCL with privacy-
preserving techniques to observe the variation in performance. Concretely, we add various random
noise to the prototypes to be communicated and the original images, respectively. The results show that
the performance of FedPCL remains high after noise injection. Details are given in Appendix A.2.4."
"CONCLUSION
TO ADDRESS THE PROBLEMS ON EXCESSIVE COMPUTATION AND COMMUNICATION DEMANDS IN CURRENT FEDERATED",0.636,"6
Conclusion
To address the problems on excessive computation and communication demands in current federated
learning frameworks, we propose a lightweight framework that leverages multiple neural networks as
ﬁxed pre-trained backbones to replace the learnable feature extractor. To customize the general rep-
resentations generated by these backbones for each client, class-wise prototypes are shared across the
clients and the server. To efﬁciently extract the shared knowledge from the prototypes, we develop Fed-
PCL algorithm that uses contrastive learning at the client-side during the local update. Extensive exper-
iments are conducted to show the superiority of FedPCL under the proposed lightweight framework."
"CONCLUSION
TO ADDRESS THE PROBLEMS ON EXCESSIVE COMPUTATION AND COMMUNICATION DEMANDS IN CURRENT FEDERATED",0.64,"This research has great potential to bring existing FL frameworks to a new paradigm and provide
more options on local neural network architectures. However, we mainly focus on the vision task
in this work. Methods for language tasks can be further explored in future studies. Considering the
evident motivation and effectiveness of the lightweight framework and FedPCL, we believe this work
is intriguing for future studies."
REFERENCES,0.644,References
REFERENCES,0.648,"[1] H Brendan McMahan, Eider Moore, Daniel Ramage, et al. Communication-efﬁcient learning of deep
networks from decentralized data. In AISTATS, 2017."
REFERENCES,0.652,"[2] Peter Kairouz, H Brendan McMahan, et al. Advances and open problems in federated learning. Foundations
and Trends in Machine Learning, 14(1–2):1–210, 2021."
REFERENCES,0.656,"[3] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning on
non-IID features via local batch normalization. In ICLR, 2020."
REFERENCES,0.66,"[4] Chen Chen, Yuchen Liu, Xingjun Ma, and Lingjuan Lyu. CalFAT: Calibrated federated adversarial training
with label skewness. In NeurIPS, 2022."
REFERENCES,0.664,"[5] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-learning
approach. In NeurIPS, 2020."
REFERENCES,0.668,"[6] Canh T Dinh, Nguyen Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau
envelopes. In NeurIPS, 2020."
REFERENCES,0.672,"[7] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations
for personalized federated learning. In ICML, 2021."
REFERENCES,0.676,"[8] Sangjoon Park, Gwanghyun Kim, Jeongsol Kim, Boah Kim, and Jong Chul Ye. Federated split task-
agnostic vision transformer for COVID-19 CXR diagnosis. In NeurIPS, 2021."
REFERENCES,0.68,"[9] Bill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin Wang, Yufen Huang, Mahdi Soltanolkotabi, Xiang
Ren, and Salman Avestimehr. FedNLP: A research platform for federated learning in natural language
processing. arXiv preprint arXiv:2104.08815, 2021."
REFERENCES,0.684,"[10] Han Xie, Jing Ma, Li Xiong, and Carl Yang. Federated graph classiﬁcation over non-IID graphs. In
NeurIPS, 2021."
REFERENCES,0.688,"[11] Fengwen Chen, Guodong Longr, Zonghan Wu, Tianyi Zhou, and Jing Jiang. Personalized federated
learning with structure. In IJCAI, 2022."
REFERENCES,0.692,"[12] Guodong Long, Tao Shen, Yue Tan, Leah Gerrard, Allison Clarke, and Jing Jiang. Federated learning
for privacy-preserving open innovation future on digital health. In Humanity Driven AI, pages 113–133.
Springer, 2022."
REFERENCES,0.696,"[13] Guodong Long, Yue Tan, Jing Jiang, and Chengqi Zhang. Federated learning for open banking. In
Federated learning, pages 240–254. Springer, 2020."
REFERENCES,0.7,"[14] Yuzheng Ren, Renchao Xie, F. Richard Yu, Tao Huang, and Yunjie Liu. Quantum collective learning and
many-to-many matching game in the metaverse for connected and autonomous vehicles. IEEE TVT, pages
1–12, 2022."
REFERENCES,0.704,"[15] Jing Jiang, Shaoxiong Ji, and Guodong Long. Decentralized knowledge acquisition for mobile internet
applications. World Wide Web, 23(5):2653–2669, 2020."
REFERENCES,0.708,"[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. In ICLR, 2020."
REFERENCES,0.712,"[17] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021."
REFERENCES,0.716,"[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In CVPR, 2016."
REFERENCES,0.72,"[19] Lu Liu, William L Hamilton, Guodong Long, Jing Jiang, and Hugo Larochelle. A universal representation
transformer layer for few-shot image classiﬁcation. In ICLR, 2020."
REFERENCES,0.724,"[20] Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, et al. Meta-
dataset: A dataset of datasets for learning to learn from few examples. In ICLR, 2020."
REFERENCES,0.728,"[21] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel domains
for domain generalization. In ECCV, 2020."
REFERENCES,0.732,"[22] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In NeurIPS,
2017."
REFERENCES,0.736,"[23] Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng. No fear of heterogeneity:
Classiﬁer calibration for federated learning with Non-IID data. In NeurIPS, 2021."
REFERENCES,0.74,"[24] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. FedProto:
Federated prototype learning over heterogeneous devices. In AAAI, 2022."
REFERENCES,0.744,"[25] Ariadna Quattoni, Michael Collins, and Trevor Darrell. Transfer learning for image classiﬁcation with
sparse prototype representations. In CVPR, 2008."
REFERENCES,0.748,"[26] Zhuoliang Kang, Kristen Grauman, and Fei Sha. Learning with whom to share in multi-task feature
learning. In ICML, 2011."
REFERENCES,0.752,"[27] Canh T Dinh, Nguyen H Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau
envelopes. In NeurIPS, 2020."
REFERENCES,0.756,"[28] Filip Hanzely, Slavomír Hanzely, Samuel Horváth, and Peter Richtárik. Lower bounds and optimal
algorithms for personalized federated learning. In NeurIPS, 2020."
REFERENCES,0.76,"[29] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning
through personalization. In ICML, 2021."
REFERENCES,0.764,"[30] Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. Personalized federated learning using
hypernetworks. In ICML, 2021."
REFERENCES,0.768,"[31] Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M Alvarez. Personalized federated
learning with ﬁrst order model optimization. In ICLR, 2020."
REFERENCES,0.772,"[32] Yihan Jiang, Jakub Koneˇcn`y, Keith Rush, and Sreeram Kannan. Improving federated learning personaliza-
tion via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019."
REFERENCES,0.776,"[33] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-supervised
learning: Generative or contrastive. IEEE TKDE, 2021."
REFERENCES,0.78,"[34] Enze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang Xu, Peize Sun, Zhenguo Li, and Ping Luo.
Detco: Unsupervised contrastive learning for object detection. In ICCV, 2021."
REFERENCES,0.784,"[35] Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, and Shirui Pan. Towards unsupervised
deep graph structure learning. In The Web Conference, 2022."
REFERENCES,0.788,"[36] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. Anomaly detection on
attributed networks via contrastive self-supervised learning. IEEE TNNLS, 33(6):2378–2392, 2021."
REFERENCES,0.792,"[37] Yizhen Zheng, Shirui Pan, Vincent Cs Lee, Yu Zheng, and Philip S Yu. Rethinking and scaling up graph
contrastive learning: An extremely efﬁcient approach with group discrimination. In NeurIPS, 2022."
REFERENCES,0.796,"[38] Yizhen Zheng, Ming Jin, Shirui Pan, Yuan-Fang Li, Hao Peng, Ming Li, and Zhao Li. Towards graph
self-supervised learning with contrastive adjusted zooming. arXiv preprint arXiv:2111.10698, 2021."
REFERENCES,0.8,"[39] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler,
and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization.
In ICLR, 2018."
REFERENCES,0.804,"[40] Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding learning via invariant
and spreading instance feature. In CVPR, 2019."
REFERENCES,0.808,"[41] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In ICML, 2020."
REFERENCES,0.812,"[42] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot,
Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In NeurIPS, 2020."
REFERENCES,0.816,"[43] Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In CVPR, 2021."
REFERENCES,0.82,"[44] Xutong Mu, Yulong Shen, Ke Cheng, Xueli Geng, Jiaxuan Fu, Tao Zhang, and Zhiwei Zhang. Fedproc:
Prototypical contrastive federated learning on non-IID data. arXiv preprint arXiv:2109.12273, 2021."
REFERENCES,0.824,"[45] Fengda Zhang, Kun Kuang, Zhaoyang You, Tao Shen, Jun Xiao, Yin Zhang, Chao Wu, Yueting Zhuang,
and Xiaolin Li. Federated unsupervised representation learning. arXiv preprint arXiv:2010.08982, 2020."
REFERENCES,0.828,"[46] Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. Learning to propagate for graph
meta-learning. In NeurIPS, 2019."
REFERENCES,0.832,"[47] Bingcong Li, Bo Han, Zhuowei Wang, Jing Jiang, and Guodong Long. Confusable learning for large-class
few-shot classiﬁcation. In ECML PKDD, 2020."
REFERENCES,0.836,"[48] John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. Towards universal paraphrastic sentence
embeddings. In ICLR, 2015."
REFERENCES,0.84,"[49] Artem Babenko and Victor Lempitsky. Aggregating local deep features for image retrieval. In ICCV, 2015."
REFERENCES,0.844,"[50] Nghia Hoang, Thanh Lam, Bryan Kian Hsiang Low, and Patrick Jaillet. Learning task-agnostic embedding
of multiple black-box experts for multi-task model fusion. In ICML, 2020."
REFERENCES,0.848,"[51] Umberto Michieli and Mete Ozay. Prototype guided federated learning of visual feature representations.
arXiv preprint arXiv:2105.08982, 2021."
REFERENCES,0.852,"[52] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang,
Wentao Han, Minlie Huang, et al. Pre-trained models: Past, present and future. AI Open, 2021."
REFERENCES,0.856,"[53] Yuzheng Ren, Renchao Xie, Fei Richard Yu, Tao Huang, and Yunjie Liu. Green intelligence networking
for connected and autonomous vehicles in smart cities. IEEE Transactions on Green Communications and
Networking, 6(3):1591–1603, 2022."
REFERENCES,0.86,"[54] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020."
REFERENCES,0.864,"[55] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In NAACL, 2019."
REFERENCES,0.868,"[56] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of
foundation models. arXiv preprint arXiv:2108.07258, 2021."
REFERENCES,0.872,"[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. arXiv preprint arXiv:2103.00020, 2021."
REFERENCES,0.876,"[58] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014."
REFERENCES,0.88,"[59] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint
arXiv:1504.00325, 2015."
REFERENCES,0.884,"[60] Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao Xu, and Feijie Wu. Parameterized
knowledge transfer for personalized federated learning. In NeurIPS, 2021."
REFERENCES,0.888,"[61] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. In MLSys, 2020."
REFERENCES,0.892,"[62] Guodong Long, Ming Xie, Tao Shen, Tianyi Zhou, Xianzhi Wang, and Jing Jiang. Multi-center federated
learning: clients clustering for better personalization. World Wide Web, 2022."
REFERENCES,0.896,"[63] Jie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu, Jianghe Xu, Shouhong Ding, and Chao
Wu. A practical data-free approach to one-shot federated learning with heterogeneity. arXiv preprint
arXiv:2112.12371, 2021."
REFERENCES,0.9,"[64] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efﬁcient framework for clustered
federated learning. In NeurIPS, 2020."
REFERENCES,0.904,"[65] Jie Ma, Guodong Long, Tianyi Zhou, Jing Jiang, and Chengqi Zhang. On the convergence of clustered
federated learning. arXiv preprint arXiv:2202.06187, 2022."
REFERENCES,0.908,"[66] Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In NeurIPS, 2019."
REFERENCES,0.912,"[67] Chen Chen, Lingjuan Lyu, Han Yu, and Gang Chen. Practical attribute reconstruction attack against
federated learning. IEEE Transactions on Big Data, 2022."
REFERENCES,0.916,"[68] Oliver Zhang, Mike Wu, Jasmine Bayrooti, and Noah Goodman. Temperature as uncertainty in contrastive
learning. arXiv preprint arXiv:2110.04403, 2021."
REFERENCES,0.92,"[69] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model
fusion in federated learning. In NeurIPS, 2020."
REFERENCES,0.924,"[70] Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personaliza-
tion with applications to federated learning. arXiv:2002.10619, 2020."
REFERENCES,0.928,"[71] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic ﬂow kernel for unsupervised domain
adaptation. In CVPR, 2012."
REFERENCES,0.932,"[72] Xingchao Peng and Kate Saenko. Synthetic to real adaptation with generative correlation alignment
networks. In WACV, 2018."
REFERENCES,0.936,"[73] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for
multi-source domain adaptation. In ICCV, 2019."
REFERENCES,0.94,"[74] Diederik P Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.944,"[75] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In NeurIPS, 2012."
REFERENCES,0.948,"[76] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.952,"[77] Liangqiong Qu, Yuyin Zhou, Paul Pu Liang, Yingda Xia, Feifei Wang, Li Fei-Fei, Ehsan Adeli, and Daniel
Rubin. Rethinking architecture design for tackling data heterogeneity in federated learning. In CVPR,
2022."
REFERENCES,0.956,"[78] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated
learning with matched averaging. In ICLR, 2020."
REFERENCES,0.96,"[79] Jonas Jongejan, Henry Rowley, Takashi Kawashima, Jongmin Kim, and Nick Fox-Gieg. The quick,
draw!-AI experiment. Mount View, CA, accessed Feb, 17(2018):4, 2016."
REFERENCES,0.964,"[80] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual
classiﬁcation of aircraft. arXiv preprint arXiv:1306.5151, 2013."
REFERENCES,0.968,"[81] Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning.
In ICLR, 2019."
REFERENCES,0.972,"[82] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama.
Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NeurIPS, 2018."
REFERENCES,0.976,"[83] Zhuowei Wang, Jing Jiang, Bo Han, Lei Feng, Bo An, Gang Niu, and Guodong Long. SemiNLL: A
framework of noisy-label learning by semi-supervised learning. Transactions on Machine Learning
Research, 2022."
REFERENCES,0.98,"[84] Zhuowei Wang, Tianyi Zhou, Guodong Long, Bo Han, and Jing Jiang. FedNoiL: A simple two-level
sampling method for federated learning with noisy labels. arXiv preprint arXiv:2205.10110, 2022."
REFERENCES,0.984,"[85] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,
Chao Xu, and Wen Gao. Pre-trained image processing transformer. In CVPR, 2021."
REFERENCES,0.988,"[86] Kaichao You, Yong Liu, Jianmin Wang, and Mingsheng Long. Logme: Practical assessment of pre-trained
models for transfer learning. In ICML, 2021."
REFERENCES,0.992,"[87] Gary Cheng, Karan Chadha, and John Duchi. Federated asymptotics: a model to compare federated
learning algorithms. arXiv preprint arXiv:2108.07313, 2021."
REFERENCES,0.996,"[88] Lin Zhang, Li Shen, Liang Ding, Dacheng Tao, and Ling-Yu Duan. Fine-tuning global model via data-free
knowledge distillation for non-IID federated learning. In CVPR, 2022."
