Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0020920502092050207,"Drawing upon recent advances in language model alignment, we formulate offline
Reinforcement Learning as a two-stage optimization problem: First pretraining
expressive generative policies on reward-free behavior datasets, then fine-tuning
these policies to align with task-specific annotations like Q-values. This strategy
allows us to leverage abundant and diverse behavior data to enhance generalization
and enable rapid adaptation to downstream tasks using minimal annotations. In par-
ticular, we introduce Efficient Diffusion Alignment (EDA) for solving continuous
control problems. EDA utilizes diffusion models for behavior modeling. However,
unlike previous approaches, we represent diffusion policies as the derivative of a
scalar neural network with respect to action inputs. This representation is critical
because it enables direct density calculation for diffusion models, making them
compatible with existing LLM alignment theories. During policy fine-tuning, we
extend preference-based alignment methods like Direct Preference Optimization
(DPO) to align diffusion behaviors with continuous Q-functions. Our evaluation
on the D4RL benchmark shows that EDA exceeds all baseline methods in overall
performance. Notably, EDA maintains about 95% of performance and still outper-
forms several baselines given only 1% of Q-labelled data during fine-tuning. Code:
https://github.com/thu-ml/Efficient-Diffusion-Alignment"
INTRODUCTION,0.0041841004184100415,"1
Introduction"
INTRODUCTION,0.006276150627615063,"Learning diverse behaviors is generative modeling; transforming them into optimized policies is
reinforcement learning. Recent studies have identified diffusion policies as a powerful tool for
representing heterogeneous behavior datasets [21, 38]. However, these behavior policies incorporate
suboptimal decisions in datasets, making them unsuitable for direct deployment in downstream tasks.
To get optimized policies, typical methods involve either augmenting the behavior policy with an
additional guidance/evaluation network [21, 32, 15] or training a new evaluation policy supervised
by the behavior policy [13, 4]. While functional, these methods fail to leverage the full potential of
pretrained behaviors as they require constructing some new policy models from scratch. This raises
the question: Can we directly fine-tune pretrained diffusion behaviors into optimized policies?"
INTRODUCTION,0.008368200836820083,"Recent advances in Large Language Model (LLM) alignment techniques [57, 37, 43] offer valuable
insights for fine-tuning diffusion behavior policies due to the fundamental similarity of the issues they
aim to address (Fig. 1). While pretrained LLMs accurately imitate language patterns from web-scale
corpus, they also capture toxic or unwanted content within the dataset. Alignment algorithms, such
as Direct Preference Optimization (DPO, [41]), are designed to remove harmful or useless content
learned during pretraining. They enable quick adaptations of pretrained LLMs to human intentions"
INTRODUCTION,0.010460251046025104,âˆ—The corresponding author.
INTRODUCTION,0.012552301255230125,"Web-scale corpus
LLM Pretraining"
INTRODUCTION,0.014644351464435146,Diverse behavior
INTRODUCTION,0.016736401673640166,Diffusion Modeling
INTRODUCTION,0.01882845188284519,Help me solve this math â€¦
INTRODUCTION,0.02092050209205021,"A   :Math is â€¦ B   :Sure, â€¦"
INTRODUCTION,0.02301255230125523,"State ð’”
Action ð‘Ž1 ð‘Ž2 ð‘Ž3"
INTRODUCTION,0.02510460251046025,"Q-value estimation B
A
> ð‘Ž1 ð‘Ž2"
INTRODUCTION,0.027196652719665274,"ð‘Ž3
1.3 -0.1 3.7"
INTRODUCTION,0.029288702928870293,"Preference 
Alignment"
INTRODUCTION,0.03138075313807531,"Q-function
Alignment"
INTRODUCTION,0.03347280334728033,Enhanced Human
INTRODUCTION,0.03556485355648536,Value Alignment
INTRODUCTION,0.03765690376569038,Optimized policy
INTRODUCTION,0.0397489539748954,"Language
Continuous Control"
INTRODUCTION,0.04184100418410042,Figure 1: Comparison between alignment strategies for LLMs and diffusion policies (ours).
INTRODUCTION,0.043933054393305436,"by fine-tuning them on a small dataset annotated with human preference labels. These strategies, due
to their simplicity and effectiveness, have seen widespread applications in academia and industry."
INTRODUCTION,0.04602510460251046,"Despite the high similarity in problem formulation and the immense potential of LLM alignment
techniques, they cannot be directly applied to fine-tune diffusion policy in domains like continuous
control. This is primarily because LLMs employ Categorical models to deal with discrete actions
(tokens). Their alignment relies on computing data probabilities for maximum likelihood training
(Sec. 2.2). However, diffusion models lack a tractable probability calculation method in continuous
action space [5]. Additionally, the data annotation method differs significantly between two areas:
LM alignment uses binary preference labels for comparing responses, while continuous control uses
scalar Q-functions for evaluating actions (Fig. 1)."
INTRODUCTION,0.04811715481171548,"To allow aligning diffusion behavior models with Q-functions for policy optimization, we introduce
Efficient Diffusion Alignment (EDA). EDA consists of two stages: behavior pretraining and policy
fine-tuning. During the pretraining stage, we learn a conditional diffusion behavior model on reward-
free datasets. Different from previous work which constructs diffusion models as an end-to-end
network, we represent diffusion policies as the derivative of a scalar neural network with respect to
action inputs. This representation is critical because it enables direct density calculation for diffusion
policies. We demonstrate that the scalar network exactly outputs the unnormalized density of behavior
distributions, making diffusion policies compatible with existing LLM alignment theories."
INTRODUCTION,0.0502092050209205,"During the fine-tuning stage, we propose a novel algorithm that directly fine-tunes pretrained behavior
models into optimized diffusion policies. The training objective is strictly derived by constructing a
classification task to predict the optimal action using log-probability ratios between the policy and
the behavior model. Our approach innovatively expands DPO by allowing fine-tuning on an arbitrary
number of actions annotated with explicit Q-values, beyond just the typical binary preference data."
INTRODUCTION,0.05230125523012552,"One main advantage of EDA is that it enables fast and data-efficient adaptations of behavior models
in downstream tasks. Our experiments on the D4RL benchmark [10] show that EDA maintains 95 %
of its performance and still surpasses baselines like IQL [25] with just 1% of Q-labelled data relative
to the pretraining phase. Besides, EDA exhibits rapid convergence during fine-tuning, requiring only
about 20K gradient steps (about 2% of the typical 1M policy training steps) to achieve convergence.
Finally, EDA outperforms all reference baselines in overall performance with access to the full
datasets. We attribute the high efficiency of EDA to its exploitation of the diffusion behavior modelsâ€™
generalization ability acquired during pretraining."
INTRODUCTION,0.05439330543933055,"Our key contributions: 1. We represent diffusion policies as the derivative of a scalar value network to
allow direct density estimation. This makes diffusion policies compatible with the existing alignment
framework. 2. We extend preference-based alignment methods and propose EDA to align diffusion
behaviors with scalar Q-functions, showcasing its vast potential in continuous control."
BACKGROUND,0.056485355648535567,"2
Background"
OFFLINE REINFORCEMENT LEARNING,0.058577405857740586,"2.1
Offline Reinforcement Learning"
OFFLINE REINFORCEMENT LEARNING,0.060669456066945605,"Offline RL aims to tackle decision-making problems by solely utilizing a pre-collected behavior
dataset. Consider a typical Markov Decision Process (MDP) described by the tuple âŸ¨S, A, P, r, Î³âŸ©.
S is the state space, A is the action space, P(sâ€²|s, a) is the transition function, r(s, a) is the
reward function and Î³ is the discount factor. Given a static dataset DÂµ := {s, a, r, sâ€²} representing"
OFFLINE REINFORCEMENT LEARNING,0.06276150627615062,"interaction history between an implicit policy Âµ and the MDP environment, our goal is to learn a new
policy that maximizes cumulative rewards in this MDP while staying close to the behavior policy Âµ."
OFFLINE REINFORCEMENT LEARNING,0.06485355648535565,"Offline RL can be formalized as a constrained policy optimization problem [26, 35, 53]:
max
Ï€
Esâˆ¼DÂµ,aâˆ¼Ï€(Â·|s)Q(s, a) âˆ’Î²DKL [Ï€(Â·|s)||Âµ(Â·|s)] ,
(1)"
OFFLINE REINFORCEMENT LEARNING,0.06694560669456066,"where Q(s, a) is an action evaluation network that can be learned from DÂµ. Î² is a temperature
coefficient. Previous work [40, 39] proves that the optimal solution for solving Eq. 1 is:"
OFFLINE REINFORCEMENT LEARNING,0.06903765690376569,"Ï€âˆ—(a|s) =
1
Z(s)Âµ(a|s)eQ(s,a)/Î².
(2)"
OFFLINE REINFORCEMENT LEARNING,0.07112970711297072,"In this paper, we focus on how to efficiently learn parameterized policies for modeling Ï€âˆ—."
DIRECT PREFERENCE OPTIMIZATION FOR LANGUAGE MODEL ALIGNMENT,0.07322175732217573,"2.2
Direct Preference Optimization for Language Model Alignment"
DIRECT PREFERENCE OPTIMIZATION FOR LANGUAGE MODEL ALIGNMENT,0.07531380753138076,"Direct Preference Optimization (DPO, [41]) is a fine-tuning technique for aligning pretrained LLMs
with human feedback. Suppose we already have a pretrained LLM model Âµ(a|s), where s rep-
resents user instructions, and a represents generated responses. The goal is to align ÂµÏ• with
some implicit evaluation rewards rLM(s, a) that reflect human preference. Our target model is
Ï€âˆ—(a|s) âˆÂµÏ•(a|s)erLM(s,a)/Î²."
DIRECT PREFERENCE OPTIMIZATION FOR LANGUAGE MODEL ALIGNMENT,0.07740585774058577,"DPO assumes we only have access to some pairwise preference data {s â†’(aw > al)} and the
preference probability is influenced by rLM(s, a). Formally,"
DIRECT PREFERENCE OPTIMIZATION FOR LANGUAGE MODEL ALIGNMENT,0.0794979079497908,"p(aw â‰»al|s) :=
erLM(s,aw)"
DIRECT PREFERENCE OPTIMIZATION FOR LANGUAGE MODEL ALIGNMENT,0.08158995815899582,"erLM(s,al) + erLM(s,aw) = Ïƒ(rLM(s, aw) âˆ’rLM(s, al)),
(3)"
DIRECT PREFERENCE OPTIMIZATION FOR LANGUAGE MODEL ALIGNMENT,0.08368200836820083,where Ïƒ is the sigmoid function.
DIRECT PREFERENCE OPTIMIZATION FOR LANGUAGE MODEL ALIGNMENT,0.08577405857740586,"In order to learn Ï€Î¸ â‰ˆÏ€âˆ—(a|s) âˆÂµÏ•(a|s)erLM(s,a)/Î², DPO first parameterizes a reward model using
the log-probability ratio between Ï€Î¸ and ÂµÏ•, and then optimizes this reward model through maximum
likelihood training:
LDPO = âˆ’E{s,awâ‰»al} log Ïƒ(rLM
Î¸ (s, aw) âˆ’rLM
Î¸ (s, al)),
(4)"
DIRECT PREFERENCE OPTIMIZATION FOR LANGUAGE MODEL ALIGNMENT,0.08786610878661087,"where
rLM
Î¸ (s, a) := Î² log Ï€Î¸(a|s)"
DIRECT PREFERENCE OPTIMIZATION FOR LANGUAGE MODEL ALIGNMENT,0.0899581589958159,"ÂµÏ•(a|s)
The key insight behind DPOâ€™s loss function is the equivalence and mutual convertibility between
the policy model and the reward model. This offers a new perspective for solving generative policy
optimization problems by applying discriminative classification loss."
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.09205020920502092,"2.3
Diffusion Modeling for Estimating Behavior Score Functions"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.09414225941422594,"Recent studies show that diffusion models [45, 20, 49] excel at representing heterogeneous behavior
policies in continuous control [21, 5, 38]. To train a diffusion behavior model, we first gradually
inject Gaussian noise into action points according to the forward diffusion process:
at = Î±ta + ÏƒtÏµ,
(5)
where t âˆˆ[0, 1], and Ïµ is standard Gaussian noise. Î±t, Ïƒt âˆˆ[0, 1] are manually defined so that at time
t = 0, we have at = a and at time t = 1, we have at â‰ˆÏµ. When a is sampled from the behavior
policy Âµ(a|s), the marginal distribution of at at various time t satisfies"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.09623430962343096,"Âµt(at|s, t) =
Z
N(at|Î±ta, Ïƒ2
t I)Âµ(a|s, t)da.
(6)"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.09832635983263599,"Intuitively, the diffusion training objective predicts the noise added to the original behavior actions:
min
Ï• Et,Ïµ,s,aâˆ¼Âµ(Â·|s)

âˆ¥ÏµÏ•(at|s, t) âˆ’Ïµâˆ¥2
2
"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.100418410041841,"at=Î±ta+ÏƒtÏµ .
(7)"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.10251046025104603,"More formally, it can be proved that the learned ""noise predictor"" ÏµÏ• actually represents the score
function âˆ‡at log Âµt(at|s, t) of the diffused behavior distribution Âµt [49]:
âˆ‡at log Âµt(at|s, t) = âˆ’Ïµâˆ—(at|s, t)/Ïƒt â‰ˆâˆ’ÏµÏ•(at|s, t)/Ïƒt.
(8)
With such a score-function estimator, we can employ existing numerical solvers [46, 33] to reverse
the diffusion process, and sample actions from the learned behavior policy ÂµÏ•. init"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.10460251046025104,"ð‘Ž1
Inference"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.10669456066945607,"State ð‘ , time ð‘¡"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.1087866108786611,"Back-propagation
(1) Behavior Pretraining
(2) Policy Fine-tuning ð‘Ž2 ð‘Žð¾ ð‘Žð‘¡ 1 ð‘Žð‘¡ 2 ð‘Žð‘¡ ð¾ Î”ð‘“ðœƒð‘Žð‘¡ 1 Î”ð‘“ðœƒð‘Žð‘¡ 2 Î”ð‘“ðœƒð‘Žð‘¡ ð¾"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.1108786610878661,"ð‘„(ð‘ , ð‘Ž2)"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.11297071129707113,"ð‘„(ð‘ , ð‘Žð¾)"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.11506276150627615,Sample from ðœ‡ðœ™â‹…Èð‘ 
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.11715481171548117,Diffusion loss
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.1192468619246862,Bottleneck value â€¦ â€¦
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.12133891213389121,"ð‘„(ð‘ , ð‘Ž1)"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.12343096234309624,"ð‘“ðœ™
È
ð‘Žð‘¡ð‘ , ð‘¡"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.12552301255230125,"ð›»ð‘Žð‘¡ð‘“ðœ™
È
ð‘Žð‘¡ð‘ , ð‘¡ ðœ‡ðœ™ ðœ‹ðœƒ"
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.12761506276150628,Cross-entropy loss â€¦ â€¦
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.1297071129707113,Q-model
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.13179916317991633,add noise ðœ€ð‘¡ 1 ðœ€ð‘¡ 2 ðœ€ð‘¡ ð¾
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.13389121338912133,Behavioral actions ð‘“ðœ™
DIFFUSION MODELING FOR ESTIMATING BEHAVIOR SCORE FUNCTIONS,0.13598326359832635,"Figure 2:
Algorithm overview. Left: In behavior pretraining, the diffusion behavior model is
represented as the derivative of a scalar neural network with respect to action inputs. The scalar
outputs of the network can later be utilized to estimate behavior density. Right: In policy fine-tuning,
we predict the optimality of actions in a contrastive manner among K candidates. The prediction
logit for each action is the density gap between the learned policy model and the frozen behavior
model. We use cross-entropy loss to align prediction logits â–³fÎ¸ := f Ï€
Î¸ âˆ’f Âµ
Î¸ with dataset Q-labels."
METHOD,0.13807531380753138,"3
Method"
METHOD,0.1401673640167364,"We decompose the policy optimization problem into two stages: behavior pretraining (Sec. 3.1) and
policy alignment (Sec. 3.2)."
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.14225941422594143,"3.1
Bottleneck Diffusion Models for Efficient Behavior Density Estimation"
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.14435146443514643,"Recent advances in alignment techniques cannot be readily applied to continuous control tasks. Their
successful applications in LLM fine-tuning require two essential prerequisites:"
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.14644351464435146,1. A powerful foundation model capable of capturing diverse behaviors within datasets.
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.14853556485355648,2. A tractable probability calculation method that allows direct density estimation (Eq. 4).
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.1506276150627615,"Language models primarily deal with discrete actions (tokens) defined by a vocabulary set V, and
thus employ Categorical models. This modeling method enables easy calculation of data probability
through a softmax operation and is capable of representing any discrete distribution. In contrast, for
continuous action space, direct density estimation is not so feasible. Diffusion policies only estimate
the gradient field (a.k.a., score) of data density instead of the density value itself [49], making it
impossible to directly apply LLM alignment techniques [41, 9, 3]. Conventional Gaussian policies
have a tractable probability formulation but lack enough expressivity and multimodality needed to
accurately model behavior datasets [52, 13, 5], and catastrophically fail in our initial experiments."
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.15271966527196654,"To address the above limitation of diffusion models, we propose a new diffusion modeling technique
to enable direct density estimation. Normally, a conditional diffusion policy ÏµÏ•(at|s, t) : AÃ—SÃ—R â†’
R|A| maps noisy actions at to predicted noises Ïµ âˆˆR|A|. In our approach, we redefine ÏµÏ• as the
derivative of a scalar network fÏ•(at|s, t) : A Ã— S Ã— R â†’R with respect to input at:"
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.15481171548117154,"ÏµÏ•(at|s, t) := âˆ’Ïƒtâˆ‡atfÏ•(at|s, t).
(9)"
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.15690376569037656,"Given that fÏ• is a parameterized network, its gradient computation can be conveniently performed by
auto-differential libraries. The new training objective for fÏ• can then be reformulated from Eq. 7:"
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.1589958158995816,"min
Ï• LÂµ(Ï•) = Et,Ïµ,(s,a)âˆ¼DÂµ 
âˆ¥Ïƒtâˆ‡atfÏ•(at|s, t) + Ïµâˆ¥2
2
"
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.16108786610878661,"at=Î±ta+ÏƒtÏµ .
(10)"
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.16317991631799164,"As noted by [49], with unlimited model capacity, the optimal solution for solving Eq. 10 is:"
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.16527196652719664,"Ïµâˆ—(at|s, t) = âˆ’Ïƒtâˆ‡at log Âµt(at|s, t) =â‡’f âˆ—(at|s, t) = log Âµt(at|s, t) + C(s, t).
(11)"
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.16736401673640167,"An illustration is provided in Figure 2 (left). Intuitively, our proposed modeling method first
compresses the input action into a scalar value with one single dimension. Then, this bottleneck value
is expanded back to R|A| through back-propagation. We thus refer to fÏ• as Bottleneck Diffusion
Models (BDMs)."
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.1694560669456067,"t = 1.0
t = 0.3
t = 0.2
t = 0.0"
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.17154811715481172,"Dataset
Pretrained f Âµ
Ï•
fine-tuned f Ï€
Î¸
QÎ¸:=f Ï€
Î¸ âˆ’f Âµ
Ï•"
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.17364016736401675,(a) Value-based optimization
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.17573221757322174,"Dataset
Pretrained f Âµ
Ï•
fine-tuned f Ï€
Î¸
QÎ¸:=f Ï€
Î¸ âˆ’f Âµ
Ï•"
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.17782426778242677,(b) Preference-based optimization
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.1799163179916318,"Q-value
Dispreferred
Preferred
Density"
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.18200836820083682,"Figure 3: Experimental results of EDA in 2D bandit settings at different diffusion times. Column
1: Visualization of diversified behavior datasets. Each dot represents a two-dimensional behavioral
action. Its color reflects the actionâ€™s Q-value. Column 2 & 3: Density maps of the action distribution
as estimated by the pretrained or fine-tuned BDM models. The density for low-Q-value actions has
been effectively decreased after fine-tuning. Column 4: The predicted action Q-values, calculated by
Eq. 13, align with dataset Q-values in Column 1. See appendix B for complete results."
BOTTLENECK DIFFUSION MODELS FOR EFFICIENT BEHAVIOR DENSITY ESTIMATION,0.18410041841004185,"The primary advantage of BDMs is their ability to efficiently estimate behavior densities in a
single forward pass. Moreover, BDMs are fully compatible with existing diffusion-based codebases
regarding training and sampling procedures, inheriting their key benefits such as training stability and
model expressivity. BDMs can also be viewed as a diffused version of Energy-Based Models (EBMs,
[8]). We refer interested readers to Appendix A for a detailed discussion."
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.18619246861924685,"3.2
Policy Optimization by Aligning Diffusion Behaviors with Q-functions"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.18828451882845187,"In this section, our goal is to learn a new policy Ï€Î¸ âˆÂµÏ•eQ by fine-tuning the previously pretrained
behavior policy ÂµÏ• on a new dataset annotated by an existing Q-function Q(s, a). We show that
this policy optimization problem can actually be transformed into a simple classification task for
predicting the optimal action among multiple candidates. We elaborate on our method below."
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.1903765690376569,"Dataset construction. For any state s, we draw K > 1 independent action samples a1:K from
ÂµÏ•(Â·|s). Assume we already have an Q-function Q(s, a) that evaluates input state-action pairs in
scalar values, our dataset is formed as Df := {s, a1:K, Q(s, ak)|kâˆˆ1:K}."
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.19246861924686193,"Action optimality. We first introduce a formal notion of action optimality. We draw from the control-
as-probabilistic-inference framework [29] and define a random optimality variable OK, which is a
one-hot vector of length K. The k-th index of OK being 1 indicates that ak is the optimal action
within K action candidates a1:K. We have"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.19456066945606695,"p(Ok
K = 1|s, a1:K) =
eQ(s,ak)
PK
i=1 eQ(s,ai) ."
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.19665271966527198,"The optimality probability of a behavioral action a is proportional to the exponential of its Q-value,
aligning with the optimal policy definition Ï€âˆ—(a|s) âˆÂµ(a|s)eQ(s,a)/Î²."
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.19874476987447698,"From policy optimization to action classification. We construct a classification task by predicting
the optimal action among K candidates. This requires learning a Q-model termed QÎ¸ first. Drawing
inspiration from DPO (Sec. 2.2), we parameterize QÎ¸ as the log probability ratio between Ï€Î¸ and ÂµÏ•:"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.200836820083682,"QÎ¸(s, a) := Î² log Ï€Î¸(a|s)"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.20292887029288703,"ÂµÏ•(a|s) + Î² log Z(s),"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.20502092050209206,"This parameterization allows us to directly optimize Ï€Î¸ during training because Ï€Î¸(a|s) =
1
Z(s)ÂµÏ•(a|s)eQÎ¸(s,a)/Î² constantly holds. Since Q-values in datasets define the probability of being
the optimal action, the training objective can be derived by applying cross-entropy loss:"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.20711297071129708,"max
Î¸
LÏ€(Î¸) = E(s,a1:K)âˆ¼Df "" K
X k=1"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.20920502092050208,"eQ(s,ak)
PK
i=1 eQ(s,ai)
|
{z
}
optimality probability"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.2112970711297071,"log
e
Î² log
Ï€Î¸(ak|s)"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.21338912133891214,"ÂµÏ•(ak|s)hhhhh
+Î² log Z(s)"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.21548117154811716,"PK
i=1 e
Î² log
Ï€Î¸(ai|s)"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.2175732217573222,"ÂµÏ•(ai|s)hhhhh
+Î² log Z(s)
|
{z
}
predicted probability #"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.2196652719665272,".
(12)"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.2217573221757322,"The unknown partition function Z(s) automatically cancels out during division. Î² is a hyperparameter
that can be tuned to control how far Ï€Î¸ deviates from ÂµÏ•."
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.22384937238493724,"Expanding to bottleneck diffusion behavior. A reliable behavior density estimation of ÂµÏ•(a|s) is
critical and is a main challenge for applying Eq. 12. Our initial experiments tested with Gaussian
policies drastically failed and even underperformed vanilla behavior cloning. This highlights the
necessity of adopting a much more powerful generative policy, such as the BDM model (Sec. 3.1)."
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.22594142259414227,"Diffusion policies define a series of distributions Ï€t(at|s, t) =
R
N(at|Î±ta0, Ïƒ2
t I)Ï€0(a0|s, t)da0
at different timesteps t âˆˆ[0, 1], rather than just a single distribution Ï€ = Ï€0. Consequently,
instead of directly predicting the optimal action given raw actions a1:K, we perturb all actions
with K independent Gaussian noises according to the diffusion forward process by applying ak
t =
Î±tak + ÏƒtÏµk. Then we predict action optimality given K noisy action a1:K
t
:"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.2280334728033473,"QÎ¸(s, at, t) :=Î² log Ï€t,Î¸(at|s, t)"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.2301255230125523,"Âµt,Ï•(at|s, t) + Î² log Z(s)"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.23221757322175732,"=Î²[f Ï€
Î¸ (at|s, t) âˆ’f Âµ
Ï• (at|s, t)] + Î²[log Z(s, t) âˆ’CÏ€(s, t) + CÂµ(s, t)],
(13)"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.23430962343096234,"Similar to Eq. 12, all unknown terms above automatically cancel out in the training objective:"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.23640167364016737,"max
Î¸
Lf(Î¸) = Et,Ïµ1:K,(s,a1:K)âˆ¼Df "" K
X k=1"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.2384937238493724,"eQ(s,ak)
PK
i=1 eQ(s,ai)
|
{z
}
optimality probability"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.2405857740585774,"log
eÎ²[f Ï€
Î¸ (ak
t |s,t)âˆ’f Âµ
Ï• (ak
t |s,t)]
PK
i=1 eÎ²[f Ï€
Î¸ (ai
t|s,t)âˆ’f Âµ
Ï• (ai
t|s,t)]
|
{z
}
predicted probability on noisy actions #"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.24267782426778242,. (14)
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.24476987447698745,"Proposition 3.1. (Proof in Appendix C) Let f âˆ—
Î¸ be the optimal solution of Problem 14 and Ï€âˆ—
t,Î¸ âˆef âˆ—
Î¸
be the optimal diffusion policy. Assuming unlimited model capacity and data samples, we have the
following results:"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.24686192468619247,"(a) Optimality Guarantee. At time t = 0, the learned policy Ï€âˆ—
Î¸ converges to the optimal target policy."
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.2489539748953975,"Ï€âˆ—
Î¸(a|s) = Ï€âˆ—
t=0,Î¸(a|s) âˆÂµÏ•(a|s)eQ(s,a)/Î²"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.2510460251046025,"(b) Diffusion Consistency. At time t > 0, Ï€t>0,Î¸ models the diffused distribution of Ï€âˆ—
Î¸:"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.25313807531380755,"Ï€âˆ—
t,Î¸(a|s, t) =
Z
N(at|Î±ta, Ïƒ2
t I)Ï€âˆ—
Î¸(a0|s)da0"
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.25523012552301255,"asymptotically holds when K â†’âˆžand Î² = 1, satisfying the definition of diffusion process (Eq. 6)."
POLICY OPTIMIZATION BY ALIGNING DIFFUSION BEHAVIORS WITH Q-FUNCTIONS,0.25732217573221755,"Fine-tuning Efficiency. In practice, the policy and the behavior model share the same architecture, so
we can initialize Î¸ = Ï• to fully exploit the generalization capabilities acquired during the pretraining
phase. This fine-tuning technique allows us to perform policy optimization with an incredibly small
amount of Q-labelled data (e.g., 10k samples) and optimization steps (e.g., 20K gradient steps).
These are less than 5% of previous approaches (Sec. 5.2)."
RELATED WORK,0.2594142259414226,"4
Related Work"
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.2615062761506276,"4.1
Diffusion Modeling for Offline Continuous Control"
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.26359832635983266,"Recent advancements in offline RL have identified diffusion models as an effective approach for
behavior modeling [38, 16], which excels at representing complex and multimodal distributions"
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.26569037656903766,"Environment
Dataset
BCQ
CQL
IQL
DT
D-Diffuser
IDQL
Diffusion-QL
QGPO
BDM (Ours)"
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.26778242677824265,"HalfCheetah
Medium-Expert
64.7
91.6
86.7
86.8
90.6
95.9
96.8
93.5
93.2 Â± 1.2
HalfCheetah
Medium
40.7
44.0
47.4
42.6
49.1
51.0
51.1
54.1
57.0 Â± 0.5
HalfCheetah
Medium-Replay
38.2
45.5
44.2
36.6
39.3
45.9
47.8
47.6
51.6 Â± 0.9"
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.2698744769874477,"Hopper
Medium-Expert
100.9
105.4
91.5
107.6
111.8
108.6
111.1
108.0
104.9 Â± 7.4
Hopper
Medium
54.5
58.5
66.3
67.6
79.3
65.4
90.5
98.0
98.4 Â± 3.9
Hopper
Medium-Replay
33.1
95.0
94.7
82.7
100.0
92.1
100.7
96.9
92.7 Â± 10.0"
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.2719665271966527,"Walker2d
Medium-Expert
57.5
108.8
109.6
108.1
108.8
112.7
110.1
110.7
111.1 Â± 0.7
Walker2d
Medium
53.1
72.5
78.3
74.0
82.5
82.5
87.0
86.0
87.4 Â± 1.1
Walker2d
Medium-Replay
15.0
77.2
73.9
66.6
75.0
85.1
95.5
84.4
89.2 Â± 5.5"
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.27405857740585776,"Average (D4RL Locomotion)
50.9
77.6
76.9
74.7
81.8
82.1
88.0
86.6
87.3"
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.27615062761506276,"AntMaze
Umaze
73.0
74.0
87.5
59.2
-
94.0
93.4
96.4
93.0 Â± 4.5
AntMaze
Umaze-Diverse
61.0
84.0
62.2
53.0
-
80.2
66.2
74.4
81.0 Â± 7.4
AntMaze
Medium-Play
0.0
61.2
71.2
0.0
-
84.5
76.6
83.6
79.0 Â± 4.2
AntMaze
Medium-Diverse
8.0
53.7
70.0
0.0
-
84.8
78.6
83.8
84.0 Â± 8.2"
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.27824267782426776,"Average (D4RL Locomotion)
35.5
68.2
72.7
28.1
-
85.9
78.7
84.6
84.3"
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.2803347280334728,"Kitchen
Complete
8.1
43.8
62.5
-
-
-
84.0
62.8
81.5 Â± 7.3
Kitchen
Partial
18.9
49.8
46.3
-
57.0
-
60.5
66.0
69.3 Â± 4.6
Kitchen
Mixed
8.1
51.0
51.0
-
65.0
-
62.6
45.5
65.3 Â± 2.2"
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.2824267782426778,"Average (D4RL Locomotion)
11.7
48.2
53.3
-
-
-
69.0
58.1
72.0"
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.28451882845188287,"Average (D4RL Overall)
39.7
69.7
71.4
-
-
-
82.1
80.7
83.7"
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.28661087866108786,"Table 1: Evaluation results of D4RL benchmarks (normalized according to [10]). We report mean Â±
standard deviation of algorithm performance across 5 random seeds at the end of training. Numbers
within 5 % of the maximum are highlighted."
DIFFUSION MODELING FOR OFFLINE CONTINUOUS CONTROL,0.28870292887029286,"compared with other modeling methods like Gaussians [54, 55, 24, 35, 53] or VAEs [11, 26, 12].
However, optimizing diffusion models can be a bit more tricky due to the unavailability of a tractable
probability calculation method [5, 51]. Existing approaches include learning a separate guidance
network to guide the sampling process during evaluation [21, 32], applying classifier-free guidance
[1, 7], backpropagating sampled actions through the diffusion policy model to maximize Q-values
[52, 22], distilling new policies from diffusion behaviors [13, 4], using rejection sampling to filter out
behavioral actions with low Q-values [5, 15] and applying planning-based techniques [30, 36, 16].
Our proposed method differs from all previous work in that it directly fine-tunes the pretrained
behavior to align with task-specific annotations, instead of learning a new downstream policy."
PREFERENCE-BASED ALIGNMENT IN OFFLINE REINFORCEMENT LEARNING,0.2907949790794979,"4.2
Preference-based Alignment in Offline Reinforcement Learning"
PREFERENCE-BASED ALIGNMENT IN OFFLINE REINFORCEMENT LEARNING,0.2928870292887029,"Existing alignment algorithms are largely preference-based methods. Preference-based Reinforcement
Learning (PbRL) assumes the reward function is unknown, and must be learned from data. Previous
work usually applies inverse RL to learn a reward model first and then uses this reward model for
standard RL training [19, 57, 28, 44, 34]. This separate learning of a reward model adds complexity
to algorithm implementation and thus limits its application. To address this issue, recent work like
OPPO [23], DPO [41], and CPL [17] respectively proposes methods to align Gaussian or Categorical
policies directly with human preference. Despite their simplicity and effectiveness, these techniques
require calculating model probability, and thus cannot be applied to diffusion policies. Existing
diffusion alignment strategies [7, 56, 2, 51] are incompatible with mainstream PbRL methods. Our
work effectively closes this gap by introducing bottleneck diffusion models. We also extend existing
PbRL methods to align with continuous Q-functions instead of just binary preference data."
EXPERIMENTS,0.29497907949790797,"5
Experiments"
EXPERIMENTS,0.29707112970711297,We conduct experiments to answer the following questions:
EXPERIMENTS,0.29916317991631797,â€¢ How does EDA perform compared with other baselines in standard benchmarks? (Sec. 5.1)
EXPERIMENTS,0.301255230125523,"â€¢ Is the alignment stage data-efficient and training-efficient? How many training steps and
annotated data does EDA require for aligning pretrained behavior models? (Sec. 5.2)"
EXPERIMENTS,0.303347280334728,â€¢ Does value-based alignment outperform preference-based alignment? (Sec. 5.3)
EXPERIMENTS,0.3054393305439331,â€¢ How do contrastive action number K and other choices affect the performance? (Sec. 5.4)
EXPERIMENTS,0.3075313807531381,"5.1
D4RL Evaluation"
EXPERIMENTS,0.30962343096234307,"Benchmark. In Table 1, we evaluate the performance of EDA in D4RL benchmarks [10]. All
evaluation tasks have a continuous action space and can be broadly categorized into three types:
MuJoCo locomotion are tasks for controlling legged robots to move forward, where datasets are
generated by a variety of policies, including expert, medium, and mixed levels. Antmaze is about an
ant robot navigating itself in a maze and requires both low-level control and high-level navigation.
FrankaKitchen are manipulation tasks containing real-world datasets. It is critical to faithfully
imitate human behaviors in these tasks."
EXPERIMENTS,0.3117154811715481,"Experimental setup. Throughout our experiments, we set the contrastive action number K = 16.
For each task, we train an action evaluation model QÏˆ(s, a) for annotating behavioral data during
the fine-tuning stage. Implicit Q-learning [25] is employed for training QÏˆ due to its simplicity and
orthogonality to policy training. We compare with other critic training methods in Figure 4. The rest
of the implementation details are in Appendix D."
EXPERIMENTS,0.3138075313807531,"Baselines. We mainly consider diffusion-based RL methods with various optimization techniques.
Decision Diffuser [1] employs classifier-free guidance for optimizing behavior models. QGPO
employs energy guidance. IDQL [15] does not optimize the behavior policy and simply uses rejection
sampling during evaluation. Diffusion-QL [52] has no explicit behavior model, but instead adopts
a diffusion regularization loss. We also reference well-studied conventional algorithms for different
classes of generative policies. BCQ [11] features a VAE-based policy. DT [6] has a transformer-based
architecture. CQL [27] and IQL [25] targets Gaussian policies."
EXPERIMENTS,0.3158995815899582,"+ Online QL
+ Softmax QL
+ Implicit QL
Q-learning method 70 80 90 100"
EXPERIMENTS,0.3179916317991632,Locomotion scores
EXPERIMENTS,0.3200836820083682,TD3+BC
EXPERIMENTS,0.32217573221757323,Diffusion-QL EDA
EXPERIMENTS,0.32426778242677823,Gaussian
EXPERIMENTS,0.3263598326359833,behavior
EXPERIMENTS,0.3284518828451883,"QGPO
EDA IQL IDQL"
EXPERIMENTS,0.3305439330543933,Reported
EXPERIMENTS,0.33263598326359833,"Gaussian baseline
Diffusion baseline
Ours"
EXPERIMENTS,0.33472803347280333,"Figure 4: Average performance of EDA combined
with different Q-learning methods in Locomotion
tasks."
EXPERIMENTS,0.3368200836820084,"Result analysis. From table 1, we find that
EDA surpasses all referenced baselines regard-
ing overall performance and provides a compet-
itive number in each D4RL task. To ascertain
whether this improvement stems from the align-
ment algorithm rather than from a superior critic
model or policy class, we conduct additional
controlled experiments. As outlined in Figure 4,
we evaluate three variants of EDA using differ-
ent Q-learning approaches and compare these
against both diffusion and Gaussian baselines.
The experimental results highlight the superior-
ity of diffusion policies and further validate the
effectiveness of our proposed method."
EXPERIMENTS,0.3389121338912134,"1%
2%
10%
20%
50%
100%"
EXPERIMENTS,0.3410041841004184,Fine-tunning data numbers (% of pretraining) 20 40 60 80
EXPERIMENTS,0.34309623430962344,Normalized scores
EXPERIMENTS,0.34518828451882844,95% score
EXPERIMENTS,0.3472803347280335,Locomotion average (9 tasks)
EXPERIMENTS,0.3493723849372385,"Ours
QGPO
IQL"
EXPERIMENTS,0.3514644351464435,(a) Sample Efficiency
EXPERIMENTS,0.35355648535564854,"0
500
1000
1500
Gradient steps (k) 0 20 40"
EXPERIMENTS,0.35564853556485354,Normalized scores
EXPERIMENTS,0.3577405857740586,halfcheetah-medium-replay-v2
EXPERIMENTS,0.3598326359832636,"Ours
QGPO
D-QL
IQL
0
50
0 50"
EXPERIMENTS,0.3619246861924686,"0
500
1000
1500
Gradient steps (k) 0 50 100"
EXPERIMENTS,0.36401673640167365,Normalized scores
EXPERIMENTS,0.36610878661087864,hopper-medium-expert-v2
EXPERIMENTS,0.3682008368200837,"Ours
QGPO
D-QL
IQL
0
50
0 100"
EXPERIMENTS,0.3702928870292887,(b) Training Efficiency.
EXPERIMENTS,0.3723849372384937,Figure 5: Aligning pretrained diffusion behaviors with task Q-functions is fast and data-efficient.
FINE-TUNING EFFICIENCY,0.37447698744769875,"5.2
Fine-tuning Efficiency"
FINE-TUNING EFFICIENCY,0.37656903765690375,"The success of the pretraining, fine-tuning paradigm in language models is largely due to its high
fine-tuning efficiency, allowing pretrained models to adapt quickly to various downstream tasks.
Similarly, we aim to explore data efficiency and training efficiency during EDAâ€™s fine-tuning phase."
FINE-TUNING EFFICIENCY,0.3786610878661088,"To investigate EDAâ€™s data efficiency, we reduce the training data used for aligning with pretrained
Q-functions by randomly excluding a portion of the available dataset (Figure 5 (a)). We compare
this with IQL, which uses the same Q-model as EDA but extracts a Gaussian policy via weighted
regression. We also compare with QGPO, which shares our pretrained diffusion behavior models"
FINE-TUNING EFFICIENCY,0.3807531380753138,"but employs a separate guidance network to augment the behavior model during evaluation, instead
of directly fine-tuning the pretrained policy. Experimental results reveal that EDA is significantly
more data-efficient than the QGPO and IQL baselines. Notably, EDA maintains about 95% of its
performance in locomotion tasks when using only 1% of the Q-labeled data for policy fine-tuning.
This even surpasses several baselines that use the full dataset for policy training."
FINE-TUNING EFFICIENCY,0.38284518828451886,"In Figure 5 (b), we plot EDAâ€™s performance throughout the fine-tuning phase. EDA rapidly converges
in roughly 20K gradient steps, a negligible count compared to the typical 1M steps used for behavior
pretraining. Note that behavior modeling and task-oriented Q-definition are largely orthogonal in
offline RL. The high fine-tuning efficiency of EDA demonstrates the vast potential of pretraining on
large-scale diversified behavior data and quickly adapting to individual downstream tasks."
FINE-TUNING EFFICIENCY,0.38493723849372385,"5.3
Value Optimization v.s. Preference Optimization"
FINE-TUNING EFFICIENCY,0.38702928870292885,"A significant difference between EDA and existing preference-based RL methods such as DPO is
that EDA is tailored for alignment with scalar Q-values instead of just preference data."
FINE-TUNING EFFICIENCY,0.3891213389121339,"For preference data without explicit Q-labels, we can similarly derive an alignment loss:"
FINE-TUNING EFFICIENCY,0.3912133891213389,"max
Î¸
Lpref
f (Î¸) = Et,Ïµ1:K,(s,a1:K)âˆ¼Df """
FINE-TUNING EFFICIENCY,0.39330543933054396,"log
eÎ²[f Ï€
Î¸ (aw
t |s,t)âˆ’f Âµ
Ï• (aw
t |s,t)]
PK
i=1 eÎ²[f Ï€
Î¸ (ai
t|s,t)âˆ’f Âµ
Ï• (ai
t|s,t)] #"
FINE-TUNING EFFICIENCY,0.39539748953974896,".
(15)"
FINE-TUNING EFFICIENCY,0.39748953974895396,"Here aw represents the most preferred action among a1:K. In practice, we select aw as the action
with the highest Q-value but abandon the absolute number. Weâ€™d like to note that when K = 2, the
above objective becomes exactly the DPO objective (Eq. 4) in preference learning:"
FINE-TUNING EFFICIENCY,0.399581589958159,"max
Î¸
LDPO
f
(Î¸) = Et,Ïµ{w,l},s,awâ‰»al log Ïƒ

Î²[f Ï€
Î¸ (aw
t |s, t)âˆ’f Âµ
Ï• (aw
t |s, t)]âˆ’Î²[f Ï€
Î¸ (al
t|s, t)âˆ’f Âµ
Ï• (al
t|s, t)]
 (16)"
FINE-TUNING EFFICIENCY,0.401673640167364,"K=2
K=4
K=16
K=128
Contrastive action numbers 70 80 90"
FINE-TUNING EFFICIENCY,0.40376569037656906,Locomotion scores DPO Ours
FINE-TUNING EFFICIENCY,0.40585774058577406,"Preference-based optimization
Value-based optimization"
FINE-TUNING EFFICIENCY,0.40794979079497906,"Figure 6: Ablation of action numbers K and opti-
mization methods."
FINE-TUNING EFFICIENCY,0.4100418410041841,"We ablate different choices of K and compare
our proposed value-based alignment with prefer-
ence methods in 9 D4RL Locomotion tasks (Fig-
ure 6). Results show that EDA generally outper-
forms preference-based alignment approaches.
We attribute this improvement to its ability to
exploit the Q-value information provided by the
pretrained Q-model. Besides, we notice the
performance gap between the two methods be-
comes larger as K increases. This is expected
because preference-based optimization greedily
follows a single action with the highest Q-value.
However, as more action candidates are sampled
from the behavior model, the final selected action will have a higher probability of being out-of-
behavior-distribution data. This further hurts performance. In contrast, EDA is a softer version of
preference learning. This leads to greater tolerance for K."
ABLATION STUDIES,0.4121338912133891,"5.4
Ablation Studies"
ABLATION STUDIES,0.41422594142259417,"We study the impact of varying temperature Î² on algorithm performance in Appendix E. We also
illustratively compare our proposed diffusion models with other generative models for behavior
modeling in 2D settings in Appendix A."
CONCLUSION,0.41631799163179917,"6
Conclusion"
CONCLUSION,0.41841004184100417,"We propose Efficient Diffusion Alignment (EDA) for solving offline continuous control tasks. EDA
allows leveraging abundant and diverse behavior data to enhance generalization through behavior
pretraining and enables rapid adaptation to downstream tasks using minimal annotations. Our
experimental results show that EDA exceeds numerous baseline methods in D4RL tasks. It also
demonstrates high sample efficiency during the fine-tuning stage. This indicates its vast potential in
learning from large-scale behavior datasets and efficiently adapting to individual downstream tasks."
CONCLUSION,0.4205020920502092,Acknowledgments and Disclosure of Funding
CONCLUSION,0.4225941422594142,"We would like to thank Chengyang Ying and Cheng Lu for discussion. This work was supported
by NSFC Projects (Nos. 62350080, 92248303, 92370124, 62276149, 62061136001), BNRist
(BNR2022RC01006), Tsinghua Institute for Guo Qiang, and the High Performance Computing
Center, Tsinghua University. J. Zhu was also supported by the XPlorer Prize."
REFERENCES,0.4246861924686193,References
REFERENCES,0.42677824267782427,"[1] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S. Jaakkola, and Pulkit
Agrawal. Is conditional generative modeling all you need for decision making? In The Eleventh
International Conference on Learning Representations, 2023."
REFERENCES,0.42887029288702927,"[2] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion
models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023."
REFERENCES,0.4309623430962343,"[3] Huayu Chen, Guande He, Hang Su, and Jun Zhu. Noise contrastive alignment of language
models with explicit rewards. arXiv preprint arXiv:2402.05369, 2024."
REFERENCES,0.4330543933054393,"[4] Huayu Chen, Cheng Lu, Zhengyi Wang, Hang Su, and Jun Zhu. Score regularized policy
optimization through diffusion behavior. arXiv preprint arXiv:2310.07297, 2023."
REFERENCES,0.4351464435146444,"[5] Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu. Offline reinforcement learning
via high-fidelity generative behavior modeling. In The Eleventh International Conference on
Learning Representations, 2023."
REFERENCES,0.4372384937238494,"[6] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
via sequence modeling. In Advances in Neural Information Processing Systems, 2021."
REFERENCES,0.4393305439330544,"[7] Zibin Dong, Yifu Yuan, Jianye HAO, Fei Ni, Yao Mu, YAN ZHENG, Yujing Hu, Tangjie
Lv, Changjie Fan, and Zhipeng Hu.
Aligndiff: Aligning diverse human preferences via
behavior-customisable diffusion model. In The Twelfth International Conference on Learning
Representations, 2024."
REFERENCES,0.44142259414225943,"[8] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models.
Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.4435146443514644,"[9] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto:
Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024."
REFERENCES,0.4456066945606695,"[10] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for
deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.4476987447698745,"[11] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning
without exploration. In Proceedings of the 36th International Conference on Machine Learning,
2019."
REFERENCES,0.4497907949790795,"[12] Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu.
Emaq:
Expected-max q-learning operator for simple yet effective offline and online rl. In International
Conference on Machine Learning, pages 3682â€“3691. PMLR, 2021."
REFERENCES,0.45188284518828453,"[13] Wonjoon Goo and Scott Niekum. Know your boundaries: The necessity of explicit behavioral
cloning in offline rl. arXiv preprint arXiv:2206.00695, 2022."
REFERENCES,0.45397489539748953,"[14] Ulf Grenander and Michael I Miller. Representations of knowledge in complex systems. Journal
of the Royal Statistical Society: Series B (Methodological), 56(4):549â€“581, 1994."
REFERENCES,0.4560669456066946,"[15] Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey
Levine. Idql: Implicit q-learning as an actor-critic method with diffusion policies. arXiv preprint
arXiv:2304.10573, 2023."
REFERENCES,0.4581589958158996,"[16] Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao,
and Xuelong Li. Diffusion model is an effective planner and data synthesizer for multi-task
reinforcement learning. Advances in neural information processing systems, 36, 2024."
REFERENCES,0.4602510460251046,"[17] Joey Hejna and Dorsa Sadigh. Inverse preference learning: Preference-based rl without a reward
function. Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.46234309623430964,"[18] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771â€“1800, 2002."
REFERENCES,0.46443514644351463,"[19] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in
neural information processing systems, volume 29, pages 4565â€“4573, 2016."
REFERENCES,0.4665271966527197,"[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In
Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.4686192468619247,"[21] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for
flexible behavior synthesis. In International Conference on Machine Learning, 2022."
REFERENCES,0.4707112970711297,"[22] Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan. Efficient diffusion policies
for offline reinforcement learning. arXiv preprint arXiv:2305.20081, 2023."
REFERENCES,0.47280334728033474,"[23] Yachen Kang, Diyuan Shi, Jinxin Liu, Li He, and Donglin Wang. Beyond reward: Offline
preference-guided policy optimization. arXiv preprint arXiv:2305.16217, 2023."
REFERENCES,0.47489539748953974,"[24] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement
learning with fisher divergence critic regularization. In International Conference on Machine
Learning, pages 5774â€“5783. PMLR, 2021."
REFERENCES,0.4769874476987448,"[25] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit
Q-learning. In International Conference on Learning Representations, 2022."
REFERENCES,0.4790794979079498,"[26] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-
policy q-learning via bootstrapping error reduction. Advances in Neural Information Processing
Systems, 2019."
REFERENCES,0.4811715481171548,"[27] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for
offline reinforcement learning. Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.48326359832635984,"[28] Kimin Lee, Laura Smith, Anca Dragan, and Pieter Abbeel. B-pref: Benchmarking preference-
based reinforcement learning.
In Conference on Neural Information Processing Systems
Datasets and Benchmarks Track (round 1), 2021."
REFERENCES,0.48535564853556484,"[29] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and
review. arXiv preprint arXiv:1805.00909, 2018."
REFERENCES,0.4874476987447699,"[30] Wenhao Li, Xiangfeng Wang, Bo Jin, and Hongyuan Zha. Hierarchical diffusion for offline
decision making. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference
on Machine Learning, volume 202 of Proceedings of Machine Learning Research. PMLR,
23â€“29 Jul 2023."
REFERENCES,0.4895397489539749,"[31] Zengyi Li, Yubei Chen, and Friedrich T Sommer. Learning energy-based models in high-
dimensional spaces with multiscale denoising-score matching. Entropy, 25(10):1367, 2023."
REFERENCES,0.4916317991631799,"[32] Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, and Jun Zhu. Contrastive
energy prediction for exact energy-guided diffusion sampling in offline reinforcement learning.
In Proceedings of the 40th International Conference on Machine Learning, 2023."
REFERENCES,0.49372384937238495,"[33] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A
fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint
arXiv:2206.00927, 2022."
REFERENCES,0.49581589958158995,"[34] MichaÃ«l Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray
Jiang, Tom Le Paine, Richard Powell, Konrad Ë™ZoÅ‚na, Julian Schrittwieser, et al. Alphastar
unplugged: Large-scale offline reinforcement learning. arXiv preprint arXiv:2308.03526, 2023."
REFERENCES,0.497907949790795,"[35] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online
reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020."
REFERENCES,0.5,"[36] Fei Ni, Jianye Hao, Yao Mu, Yifu Yuan, Yan Zheng, Bin Wang, and Zhixuan Liang. Metadiffuser:
Diffusion model as conditional planner for offline meta-rl. In International Conference on
Machine Learning, pages 26087â€“26105. PMLR, 2023."
REFERENCES,0.502092050209205,"[37] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to
follow instructions with human feedback. Advances in Neural Information Processing Systems,
35:27730â€“27744, 2022."
REFERENCES,0.50418410041841,"[38] Tim Pearce, Tabish Rashid, Anssi Kanervisto, David Bignell, Mingfei Sun, Raluca Georgescu,
Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, et al. Imitating
human behaviour with diffusion models. In Deep Reinforcement Learning Workshop NeurIPS,
2022.
[39] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
[40] Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In Twenty-
Fourth AAAI Conference on Artificial Intelligence, 2010.
[41] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.
In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
[42] Saeed Saremi, Arash Mehrjou, Bernhard SchÃ¶lkopf, and Aapo HyvÃ¤rinen.
Deep energy
estimator networks. arXiv preprint arXiv:1805.08306, 2018.
[43] John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan
Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Optimizing
language models for dialogue. OpenAI blog, 2022.
[44] Daniel Shin and Daniel S Brown. Offline preference-based apprenticeship learning. arXiv
preprint arXiv:2107.09251, 2021.
[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
vised learning using nonequilibrium thermodynamics. In International Conference on Machine
Learning, pages 2256â€“2265, 2015.
[46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In
International Conference on Learning Representations, 2021.
[47] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data
distribution. Advances in neural information processing systems, 32, 2019.
[48] Yang Song and Diederik P Kingma. How to train your energy-based models. arXiv preprint
arXiv:2101.03288, 2021.
[49] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-based generative modeling through stochastic differential equations. In
International Conference on Learning Representations, 2021.
[50] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural
computation, 23(7):1661â€“1674, 2011.
[51] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam,
Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using
direct preference optimization. arXiv preprint arXiv:2311.12908, 2023.
[52] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive
policy class for offline reinforcement learning. In The Eleventh International Conference on
Learning Representations, 2023.
[53] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E
Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, and Nando de Freitas.
Critic regularized regression. In Advances in Neural Information Processing Systems, 2020.
[54] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement
learning. arXiv preprint arXiv:1911.11361, 2019.
[55] Haoran Xu, Xianyuan Zhan, Jianxiong Li, and Honglei Yin. Offline reinforcement learning
with soft behavior regularization. arXiv preprint arXiv:2110.07395, 2021.
[56] Zhilong Zhang, Yihao Sun, Junyin Ye, Tian-Shuo Liu, Jiaji Zhang, and Yang Yu. Flow to better:
Offline preference-based reinforcement learning via preferred trajectory generation. In The
Twelfth International Conference on Learning Representations, 2024.
[57] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,
Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences.
arXiv preprint arXiv:1909.08593, 2019."
REFERENCES,0.5062761506276151,"A
Comparing Bottleneck Diffusion Models with Energy-Based Models"
REFERENCES,0.5083682008368201,"Our proposed Bottleneck Diffusion Models (BDMs) can be viewed as a diffused variant of Energy-
Based Models (EBMs, [8]). Both methods aim to model data distributionâ€™s unnormalized log
probability:
pÎ¸(x) âˆeâˆ’EÎ¸(x).
Despite their conceptual similarity, the sampling and training approaches differ between diffusion
models and EBMs. A prevalent sampling method for EBMs is Langevin MCMC [14], which employs
the energy gradient âˆ‡xEÎ¸(x) to progressively transform Gaussian noise into data samples. Langevin
MCMC generally necessitates hundreds or thousands of iterative steps to achieve convergence, a
significantly higher number compared with 15-50 steps required by diffusion models (Figure A)."
REFERENCES,0.5104602510460251,"Furthermore, the maximum-likelihood training of EBMs (e.g., Contrastive Divergence [18]) is more
computationally expensive, as it involves online data sampling from the model during the training
process [48]. To avoid online data sampling, subsequent research [50, 47] has shifted away from
directly modeling EÎ¸(x) towards developing score-based models defined as sÎ¸(x) := âˆ’âˆ‡xEÎ¸(x).
The score-matching objectives utilized in training these score-based models have subsequently been
adapted for training diffusion models [49]. Our work is inspired by some prior work [42, 47, 31] that
employed such score-matching objectives for training EBMs."
REFERENCES,0.5125523012552301,"(a) Ground truth
(b) Gaussians
(c) VAEs
(d) EBMs (100 steps) (e) EBMs (1k steps)
(f) Ours (25 steps)"
REFERENCES,0.5146443514644351,Figure 7: Comparison of various generative modeling methods in 2D modeling and sampling.
REFERENCES,0.5167364016736402,"B
Complete 2D-bandit Experiment Results"
REFERENCES,0.5188284518828452,"t = 1.0
t = 0.3
t = 0.1
t = 0.0
t = 1.0
t = 0.3
t = 0.15
t = 0.0
t = 1.0
t = 0.3
t = 0.2
t = 0.0"
REFERENCES,0.5209205020920502,"Dataset
Pretrained f Âµ
Ï•
fine-tuned f Ï€
Î¸
QÎ¸:=f Ï€
Î¸ âˆ’f Âµ
Ï•"
REFERENCES,0.5230125523012552,(a) Value-based optimization
REFERENCES,0.5251046025104602,"Dataset
Pretrained f Âµ
Ï•
fine-tuned f Ï€
Î¸
QÎ¸:=f Ï€
Î¸ âˆ’f Âµ
Ï•"
REFERENCES,0.5271966527196653,(b) Preference-based optimization
REFERENCES,0.5292887029288703,Figure 8: Illustration of EDAâ€™s performance in 2D bandit settings at different diffusion times.
REFERENCES,0.5313807531380753,"C
Theoretical Analysis"
REFERENCES,0.5334728033472803,"In this section, we present the theoretical proof for Proposition 3.1."
REFERENCES,0.5355648535564853,"Our proof is based on the theoretical framework of Contrastive Energy Prediction (CEP) for diffusion
energy guidance [32]. For the ease of readers, we incorporate the relevant theories from their work as
lemmas below."
REFERENCES,0.5376569037656904,"Lemma C.1. Let Ë†Q(s, a) : AÃ—S â†’R be a scalar function approximator. Consider the optimization
problem"
REFERENCES,0.5397489539748954,"max
Ë†
Q
EÂµ(s) QK
i=1 Âµ(ai|s) "" K
X k=1"
REFERENCES,0.5418410041841004,"eQ(s,ak)
PK
i=1 eQ(s,ai) log
e Ë†
Q(s,ak)
PK
i=1 e Ë†
Q(s,ai) #"
REFERENCES,0.5439330543933054,".
(17)"
REFERENCES,0.5460251046025104,The solution for Problem 17 satisfies
REFERENCES,0.5481171548117155,"Ë†Qâˆ—(s, a) = Q(s, a) + C(s),"
REFERENCES,0.5502092050209205,where C(s) can be arbitrary scalar functions conditioned on s.
REFERENCES,0.5523012552301255,Proof. The proof is quite straightforward. Consider two discrete distributions P :=
REFERENCES,0.5543933054393305,"(
eQ(s,a1)
PK
i=1 eQ(s,ai) ,
eQ(s,a2)
PK
i=1 eQ(s,a2) ...,
eQ(s,aK)
PK
i=1 eQ(s,ai) ) Ë†P :="
REFERENCES,0.5564853556485355,"(
e Ë†
Q(s,a1)
PK
i=1 e Ë†
Q(s,ai) ,
e Ë†
Q(s,a2)
PK
i=1 e Ë†
Q(s,ai) ...,
e Ë†
Q(s,aK)
PK
i=1 e Ë†
Q(s,ai) )"
REFERENCES,0.5585774058577406,"For any s and any a1:K, we have"
REFERENCES,0.5606694560669456,"EÂµ(s) QK
i=1 Âµ(ai|s) "" K
X k=1"
REFERENCES,0.5627615062761506,"eQ(s,ak)
PK
i=1 eQ(s,ai) log
e Ë†
Q(s,ak)
PK
i=1 e Ë†
Q(s,ai) #"
REFERENCES,0.5648535564853556,"=EÂµ(s) QK
i=1 Âµ(ai|s) âˆ’DKL(P|| Ë†P) âˆ’H(P)"
REFERENCES,0.5669456066945606,"â‰¤EÂµ(s) QK
i=1 Âµ(ai|s) âˆ’H(P)"
REFERENCES,0.5690376569037657,"According to the properties of KL divergence, the equality holds if and only if P = Ë†P for any s and
a1:K. This implies that
Ë†Qâˆ—(s, a) = Q(s, a) + C(s),"
REFERENCES,0.5711297071129707,constantly holds.
REFERENCES,0.5732217573221757,"Lemma C.2. Let Ë†Q(s, at, t) : A Ã— S Ã— R â†’R be a scalar function approximator. p(at|a, t) is any
conditional transition probability. Consider the optimization problem"
REFERENCES,0.5753138075313807,"max
Ë†
Q
EÂµ(s) QK
i=1 Âµ(ai|s)p(ai
t|ai,t) "" K
X"
REFERENCES,0.5774058577405857,"k=1
eQ(s,ak) log
e Ë†
Q(s,ak
t ,t)
PK
i=1 e Ë†
Q(s,ai
t,t) #"
REFERENCES,0.5794979079497908,".
(18)"
REFERENCES,0.5815899581589958,The solution for Problem 18 satisfies
REFERENCES,0.5836820083682008,"Ë†Qâˆ—(s, at, t) = log EÂµt(a|at,s,t)eQ(s,a) + C(s),"
REFERENCES,0.5857740585774058,"where Âµt(a|at, s, t) = Âµ(a|s)p(at|a, t)/Âµt(at|s, t) is the posterior action distribution, C(s) can
be arbitrary scalar functions conditioned on s."
REFERENCES,0.5878661087866108,Proof.
REFERENCES,0.5899581589958159,"EÂµ(s) QK
i=1 Âµ(ai|s)p(ai
t|ai,t) "" K
X"
REFERENCES,0.5920502092050209,"k=1
eQ(s,ak) log
e Ë†
Q(s,ak
t ,t)
PK
i=1 e Ë†
Q(s,ai
t,t) #"
REFERENCES,0.5941422594142259,"=EÂµ(s) QK
i=1 Âµt(ai
t|s)Âµt(ai|ai
t,s,t) "" K
X"
REFERENCES,0.5962343096234309,"k=1
eQ(s,ak) log
e Ë†
Q(s,ak
t ,t)
PK
i=1 e Ë†
Q(s,ai
t,t) # ."
REFERENCES,0.5983263598326359,"=EÂµ(s) QK
i=1 Âµt(ai
t|s) "" K
X"
REFERENCES,0.600418410041841,"k=1
EÂµt(ak|ak
t ,s,t)eQ(s,ak) log
e Ë†
Q(s,ak
t ,t)
PK
i=1 e Ë†
Q(s,ai
t,t) # ."
REFERENCES,0.602510460251046,"=EÂµ(s) QK
i=1 Âµâ€²
t(ai
t|s) "" K
X k=1"
REFERENCES,0.604602510460251,"EÂµt(ak|ak
t ,s,t)eQ(s,ak)
PK
i=1 EÂµt(ai|ai
t,s,t)eQ(s,ai) log
e Ë†
Q(s,ak
t ,t)
PK
i=1 e Ë†
Q(s,ai
t,t) # ."
REFERENCES,0.606694560669456,"According to Lemma C.1, for any state a and any diffused action at at time t, the optimal solution
satisfies
Ë†Qâˆ—(s, at, t) = log EÂµt(a|at,s,t)eQ(s,a) + C(s)."
REFERENCES,0.608786610878661,"Lemma C.3. Consider the behavior distribution Âµ(a|s) and the policy distribution Ï€âˆ—(a|s) âˆ
Âµ(a|s)eQ(s,a). Their diffused distribution at time t are both defined by the forward diffusion process
(Eq. 5). Let p(at|a, t) := N(at|Î±ta, Ïƒ2
t I), such that"
REFERENCES,0.6108786610878661,"Âµt(at|s, t) =
Z
N(at|Î±ta, Ïƒ2
t I)Âµ(a|s, t)da,"
REFERENCES,0.6129707112970711,"and
Ï€âˆ—
t (at|s, t) =
Z
N(at|Î±ta, Ïƒ2
t I)Ï€âˆ—(a|s, t)da."
REFERENCES,0.6150627615062761,"Then the relationship between Ï€âˆ—
t and Âµt can be derived as"
REFERENCES,0.6171548117154811,"Ï€âˆ—
t (at|s, t) âˆÂµt(at|s, t)eQt(s,at,t),"
REFERENCES,0.6192468619246861,"where Qt(s, at, t) := log EÂµt(a|at,s,t)eQ(s,a)."
REFERENCES,0.6213389121338913,"Proof. According to the definition, we have"
REFERENCES,0.6234309623430963,"Ï€âˆ—
t (a|s) =
Z
N(at|Î±ta, Ïƒ2
t I)Ï€âˆ—(a|s, t)da"
REFERENCES,0.6255230125523012,"=
Z
N(at|Î±ta, Ïƒ2
t I)Âµ(a|s)eQ(s,a)"
REFERENCES,0.6276150627615062,Z(s) da
REFERENCES,0.6297071129707112,"=
Z
p(at|a, t)Âµ(a|s)eQ(s,a)"
REFERENCES,0.6317991631799164,Z(s) da
REFERENCES,0.6338912133891214,"=
Z
Âµt(a|at, s, t)Âµt(at|s, t)eQ(s,a)"
REFERENCES,0.6359832635983264,Z(s) da
REFERENCES,0.6380753138075314,"=
1
Z(s)Âµt(at|s, t)
Z
Âµt(a|at, s, t)eQ(s,a)da"
REFERENCES,0.6401673640167364,"âˆÂµt(at|s, t)eQt(s,at,t)"
REFERENCES,0.6422594142259415,"Proposition C.4. Let f âˆ—
Î¸ be the optimal solution of Problem 14 and Ï€âˆ—
t,Î¸ âˆef âˆ—
Î¸ be the optimal
diffusion policy. Assuming unlimited model capacity and data samples, we have the following results:"
REFERENCES,0.6443514644351465,"(a) Optimality Guarantee. At time t = 0, the learned policy Ï€âˆ—
Î¸ converges to the optimal target policy."
REFERENCES,0.6464435146443515,"Ï€âˆ—
Î¸(a|s) = Ï€âˆ—
t=0,Î¸(a|s) âˆÂµÏ•(a|s)eQ(s,a)/Î²"
REFERENCES,0.6485355648535565,"(b) Diffusion Consistency. At time t > 0, Ï€t>0,Î¸ models the diffused distribution of Ï€âˆ—
Î¸:"
REFERENCES,0.6506276150627615,"Ï€âˆ—
t,Î¸(a|s, t) =
Z
N(at|Î±ta, Ïƒ2
t I)Ï€âˆ—
Î¸(a0|s)da0"
REFERENCES,0.6527196652719666,"asymptotically holds when K â†’âˆžand Î² = 1, satisfying the definition of diffusion process (Eq. 6)."
REFERENCES,0.6548117154811716,Proof. We first rewrite Problem 14 below:
REFERENCES,0.6569037656903766,"max
Î¸
Lf(Î¸) = EÂµ(s) QK
i=1 Âµ(ai|s)p(ai
t|ai,t) "" K
X k=1"
REFERENCES,0.6589958158995816,"eQ(s,ak)
PK
i=1 eQ(s,ai) log
eÎ²[f Ï€
Î¸ (ak
t |s,t)âˆ’f Âµ
Ï• (ak
t |s,t)]
PK
i=1 eÎ²[f Ï€
Î¸ (ai
t|s,t)âˆ’f Âµ
Ï• (ai
t|s,t)] # ."
REFERENCES,0.6610878661087866,"(19)
(a) Optimality Guarantee. At time t = 0, we have p(at|a, t) = N(at|Î±ta, Ïƒ2
t I) = N(at|a, 0I)
such that at = a."
REFERENCES,0.6631799163179917,"Define Ë†Qâˆ—(s, a) := Î²[f Ï€
Î¸ (ak
t |s, t = 0) âˆ’f Âµ
Ï• (ak
t |s, t = 0)]. Since we assume unlimited model
capacity for fÎ¸, Ë†Qâˆ—can be arbitrary scalar functions. Lemma C.1 can then be applied:"
REFERENCES,0.6652719665271967,"Ï€âˆ—
Î¸(a|s) =Ï€âˆ—
t=0,Î¸(a|s)"
REFERENCES,0.6673640167364017,"âˆef Ï€
Î¸ (ai
t|s,t=0)"
REFERENCES,0.6694560669456067,"=ef Âµ
Ï• (ai
t|s,t=0)+ Ë†
Qâˆ—(s,a)/Î²"
REFERENCES,0.6715481171548117,"=ef Âµ
Ï• (ai
t|s,t=0)e[Q(s,a)+C(s)]/Î²"
REFERENCES,0.6736401673640168,"âˆÂµÏ•(a|s)eQ(s,a)/Î²"
REFERENCES,0.6757322175732218,"(b) Diffusion Consistency. When K â†’âˆž, we PK
i=1 eQ(s,ai) = EÂµ(a|s)eQ(s,a) becomes constant
and can be removed. Set Î² = 1, the optimization problem equation 14 becomes"
REFERENCES,0.6778242677824268,"max
Î¸
Lf(Î¸) = EÂµ(s) QK
i=1 Âµ(ai|s)p(ai
t|ai,t) "" K
X"
REFERENCES,0.6799163179916318,"k=1
eQ(s,ak) log
e [f Ï€
Î¸ (ak
t |s,t)âˆ’f Âµ
Ï• (ak
t |s,t)]
PK
i=1 e[f Ï€
Î¸ (ai
t|s,t)âˆ’f Âµ
Ï• (ai
t|s,t)] #"
REFERENCES,0.6820083682008368,".
(20)"
REFERENCES,0.6841004184100419,We can then similarly apply Lemma C.2 and get
REFERENCES,0.6861924686192469,"log
Ï€âˆ—
t,Î¸(at|s, t)
Âµt(at|s, t) = f Ï€
Î¸ (at|s, t) âˆ’f Âµ
Ï• (at|s, t) = log EÂµt(a|at,s,t)eQ(s,a) + Z(s)"
REFERENCES,0.6882845188284519,"Ï€âˆ—
t,Î¸(at|s, t) âˆÂµt(at|s, t)EÂµt(a|at,s,t)eQ(s,a)"
REFERENCES,0.6903765690376569,"According to Lemma C.3, we have"
REFERENCES,0.6924686192468619,"Ï€âˆ—
t,Î¸(a|s, t) =
Z
N(at|Î±ta, Ïƒ2
t I)Ï€âˆ—
Î¸(a0|s)da0"
REFERENCES,0.694560669456067,"D
Implementation Details for D4RL Tasks"
REFERENCES,0.696652719665272,We use NVIDIA A40 GPU cards to run all experiments.
REFERENCES,0.698744769874477,"Behavior pretraining. For pretraining the bottleneck diffusion model, we extract a reward-free
behavior dataset {s, a} from DÂµ := {s, a, r, sâ€²}. We adopt the model architecture used by IDQL
[15] and SRPO [4] but sum up the final |A|-dimensional output to form a scalar network. The
resulting model is basically a 6-layer MLP with residual connections, layer normalizations, and
dropout regularization. We train the behavior network for 1M steps to ensure convergence. The batch
size is 2048. The optimizer is Adam with a learning rate of 3e-4. We adopt default VPSDE [49]
hyperparameters as the diffusion data perturbation method."
REFERENCES,0.700836820083682,"Constructing alignment dataset. First, we leverage DÂµ and use existing methods such as IQL [25]
to learn a critic network that will later be utilized for data annotation. Then, for a random portion
of state s in DÂµ, we leverage the pretrained behavior model to generate K = 16 action samples.
The original action in DÂµ is thrown away. We use the critic model to annotate each state-action pair
and store them together as the alignment dataset Df := {s, a1:K, Q(s, ak)|kâˆˆ1:K}. We use 2-layer
MLPs with 256 hidden states. Critic training details is exactly the same with previous work [25].
Other critic learning methods used in ablation studies are also consistent with respective prior work
[15, 32]."
REFERENCES,0.702928870292887,"Policy fine-tuning. The policy network is initialized to be the behavior network. Throughout
the training, we fix the behavior model and only optimize the policy model. Behavior weights
are frozen. The optimizer is Adam and the learning rate is 5e-5. All policy models are trained
for 200k gradient steps though we observe convergence at 20K steps in most tasks. We do not
employ dropout regularization during fine-tuning because we find it harms performance. For the
temperature coefficient, we sweep over Î² âˆˆ{0.1, 0.2, 0.3, 0.5, 0.8, 1.0, 2.0} (Figure 9&10). Similarly
to [11, 15, 5, 15], we find the performance can be improved by adopting a rejection sampling technique.
We select the action with the highest Q-value among 4 candidates during evaluation. We do not use
such techniques in experimental plots (Figure 5&11) to better reflect policy improvement. We use
20 test seeds in all experiments and report numbers at the end of training. We train all experiments
independently with 5 seeds in our main experiments and 3-5 seeds in ablation studies."
REFERENCES,0.7050209205020921,"E
Additional Experiment Results"
REFERENCES,0.7071129707112971,"0.1
0.2
0.3
0.5
0.8 1.0
2.0 20 40 60 80"
REFERENCES,0.7092050209205021,Normalized Scores
REFERENCES,0.7112970711297071,Halfcheetah
REFERENCES,0.7133891213389121,"Medium-Expert
Medium-Replay
Medium"
REFERENCES,0.7154811715481172,"0.1
0.2
0.3
0.5
0.8 1.0
2.0 0 50 100"
REFERENCES,0.7175732217573222,Normalized Scores
REFERENCES,0.7196652719665272,Hopper
REFERENCES,0.7217573221757322,"Medium-Expert
Medium-Replay
Medium"
REFERENCES,0.7238493723849372,"0.1
0.2
0.3
0.5
0.8 1.0
2.0 25 50 75 100"
REFERENCES,0.7259414225941423,Normalized Scores
REFERENCES,0.7280334728033473,Walker2d
REFERENCES,0.7301255230125523,"Medium-Expert
Medium-Replay
Medium"
REFERENCES,0.7322175732217573,"0.3
0.5
0.8 1.0
2.0
3.0
5.0
0 25 50 75 100"
REFERENCES,0.7343096234309623,Normalized Scores
REFERENCES,0.7364016736401674,Antmaze
REFERENCES,0.7384937238493724,"Medium-Diverse
Medium-Replay
Umaze-Diverse
Umaze"
REFERENCES,0.7405857740585774,"0.3
0.5
0.81.0
2.0
3.0
5.0 0 25 50 75"
REFERENCES,0.7426778242677824,Normalized Scores
REFERENCES,0.7447698744769874,Kitchen
REFERENCES,0.7468619246861925,"Complete
Mixed
Partial"
REFERENCES,0.7489539748953975,Figure 9: Ablation of the temperature coefficient Î² in D4RL benchmarks.
REFERENCES,0.7510460251046025,"0.1
0.2
0.3
0.5
0.8 1.0
2.0 40 60 80"
REFERENCES,0.7531380753138075,Normalized Scores
REFERENCES,0.7552301255230126,Halfcheetah    (w/o RS)
REFERENCES,0.7573221757322176,"Medium-Expert
Medium-Replay
Medium"
REFERENCES,0.7594142259414226,"0.1
0.2
0.3
0.5
0.8 1.0
2.0 0 50 100"
REFERENCES,0.7615062761506276,Normalized Scores
REFERENCES,0.7635983263598326,Hopper    (w/o RS)
REFERENCES,0.7656903765690377,"Medium-Expert
Medium-Replay
Medium"
REFERENCES,0.7677824267782427,"0.1
0.2
0.3
0.5
0.8 1.0
2.0 50 75 100 125"
REFERENCES,0.7698744769874477,Normalized Scores
REFERENCES,0.7719665271966527,Walker2d    (w/o RS)
REFERENCES,0.7740585774058577,"Medium-Expert
Medium-Replay
Medium"
REFERENCES,0.7761506276150628,"0.3
0.5
0.8 1.0
2.0
3.0
5.0
0 25 50 75 100"
REFERENCES,0.7782426778242678,Normalized Scores
REFERENCES,0.7803347280334728,Antmaze    (w/o RS)
REFERENCES,0.7824267782426778,"Medium-Diverse
Medium-Replay
Umaze-Diverse
Umaze"
REFERENCES,0.7845188284518828,"0.3
0.5
0.81.0
2.0
3.0
5.0
0 25 50 75"
REFERENCES,0.7866108786610879,Normalized Scores
REFERENCES,0.7887029288702929,Kitchen    (w/o RS)
REFERENCES,0.7907949790794979,"Complete
Mixed
Partial"
REFERENCES,0.7928870292887029,Figure 10: Ablation of the temperature coefficient Î² without rejection sampling.
REFERENCES,0.7949790794979079,"Halfcheetah
Medium-expert
Medium
Medium-Replay
2.0
0.1
0.2"
REFERENCES,0.797071129707113,"Hopper
Medium-expert
Medium
Medium-Replay
2.0
0.2
0.2"
REFERENCES,0.799163179916318,"Walker2d
Medium-expert
Medium
Medium-Replay
0.3
0.5
0.1"
REFERENCES,0.801255230125523,"AntMaze
Umaze
Umanze-diverse
Medium (both)
0.5
5.0
1.0"
REFERENCES,0.803347280334728,"Kitchen
Complete
Mixed
Partial
2.0
3.0
3.0
Table 2: Temperature coefficient Î² for every individual task."
REFERENCES,0.805439330543933,"F
Additional Training Curves"
REFERENCES,0.8075313807531381,"0
500
1000
1500
Gradient steps (k) 0 25 50 75 100"
REFERENCES,0.8096234309623431,Normalized scores
REFERENCES,0.8117154811715481,halfcheetah-medium-expert-v2
REFERENCES,0.8138075313807531,"Ours
QGPO
D-QL
IQL
0
50
0 50"
REFERENCES,0.8158995815899581,"0
500
1000
1500
Gradient steps (k) 0 50 100"
REFERENCES,0.8179916317991632,Normalized scores
REFERENCES,0.8200836820083682,hopper-medium-expert-v2
REFERENCES,0.8221757322175732,"Ours
QGPO
D-QL
IQL
0
50
0 100"
REFERENCES,0.8242677824267782,"0
500
1000
1500
Gradient steps (k) 0 50 100"
REFERENCES,0.8263598326359832,Normalized scores
REFERENCES,0.8284518828451883,walker2d-medium-expert-v2
REFERENCES,0.8305439330543933,"Ours
QGPO
D-QL
IQL
0
50
0 100"
REFERENCES,0.8326359832635983,"0
500
1000
1500
Gradient steps (k) 0 20 40"
REFERENCES,0.8347280334728033,Normalized scores
REFERENCES,0.8368200836820083,halfcheetah-medium-v2
REFERENCES,0.8389121338912134,"Ours
QGPO
D-QL
IQL
0
50
0 50"
REFERENCES,0.8410041841004184,"0
500
1000
1500
Gradient steps (k) 25 50 75 100"
REFERENCES,0.8430962343096234,Normalized scores
REFERENCES,0.8451882845188284,hopper-medium-v2
REFERENCES,0.8472803347280334,"Ours
QGPO
D-QL
IQL
0
50"
REFERENCES,0.8493723849372385,"25
50
75"
REFERENCES,0.8514644351464435,"0
500
1000
1500
Gradient steps (k) 0 20 40 60 80"
REFERENCES,0.8535564853556485,Normalized scores
REFERENCES,0.8556485355648535,walker2d-medium-v2
REFERENCES,0.8577405857740585,"Ours
QGPO
D-QL
IQL
0
50 25 50 75"
REFERENCES,0.8598326359832636,"0
500
1000
1500
Gradient steps (k) 0 20 40"
REFERENCES,0.8619246861924686,Normalized scores
REFERENCES,0.8640167364016736,halfcheetah-medium-replay-v2
REFERENCES,0.8661087866108786,"Ours
QGPO
D-QL
IQL
0
50
0 50"
REFERENCES,0.8682008368200836,"0
500
1000
1500
Gradient steps (k) 25 50 75 100"
REFERENCES,0.8702928870292888,Normalized scores
REFERENCES,0.8723849372384938,hopper-medium-replay-v2
REFERENCES,0.8744769874476988,"Ours
QGPO
D-QL
IQL
0
50 25 50 75"
REFERENCES,0.8765690376569037,"0
500
1000
1500
Gradient steps (k) 25 50 75 100"
REFERENCES,0.8786610878661087,Normalized scores
REFERENCES,0.8807531380753139,walker2d-medium-replay-v2
REFERENCES,0.8828451882845189,"Ours
QGPO
D-QL
IQL
0
50"
REFERENCES,0.8849372384937239,"25
50
75"
REFERENCES,0.8870292887029289,Figure 11: Training curves of EDA (ours) and several baselines.
REFERENCES,0.8891213389121339,"G
Reproducibility"
REFERENCES,0.891213389121339,"To ensure that our work is reproducible, we submit the source code as supplementary material. The
code will go open-source upon publication. Reported experimental numbers are averaged under
multiple random seeds. We provide implementation details of our work in Appendix D."
REFERENCES,0.893305439330544,"H
Limitations and Broader Impacts"
REFERENCES,0.895397489539749,"Limitations. EDA employs a diffusion policy, which is a natural fit for continuous control problems.
However, it cannot be directly applied in decision-making tasks with a discrete action space. Addi-
tionally, EDA places restrictions on the diffusion model design. The diffusion policy has to be the
gradient of a scalar neural network with respect to action inputs to enable direct density estimation.
Lastly,as task reward is human-defined and varies from task to task, achieving optimal performance
with EDA in downstream tasks requires tailored network architecture design and hyperparameter
tuning. How to ensure that the same set of algorithm parameters can be applicable across various
tasks remains a topic worthy of further research."
REFERENCES,0.897489539748954,"Broader Impacts. EDA is a rather theoretical paper and all experiments are done in simulation
environments. However, its further application could assist the deployment of highly optimized AI
systems in critical real-world applications, such as autonomous driving or healthcare robots. This
could have unintended consequences if the systems fail or produce incorrect results. Robust testing
and validation are essential to mitigate these risks. There is also a risk that EDA could be misused for
malicious purposes. For instance, optimized AI policies could be employed in autonomous systems
for surveillance or other invasive activities without proper regulation and oversight."
REFERENCES,0.899581589958159,NeurIPS Paper Checklist
CLAIMS,0.9016736401673641,1. Claims
CLAIMS,0.9037656903765691,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: The main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope
Guidelines:"
CLAIMS,0.9058577405857741,"â€¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.9079497907949791,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Appendix H
Guidelines:"
CLAIMS,0.9100418410041841,"â€¢ The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
â€¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors"
CLAIMS,0.9121338912133892,"should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢ The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢ If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.9142259414225942,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Appendix C.
Guidelines:"
CLAIMS,0.9163179916317992,"â€¢ The answer NA means that the paper does not include theoretical results.
â€¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢ Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
CLAIMS,0.9184100418410042,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: yes we have.
Guidelines:"
CLAIMS,0.9205020920502092,"â€¢ The answer NA means that the paper does not include experiments.
â€¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same"
CLAIMS,0.9225941422594143,"dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢ While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
CLAIMS,0.9246861924686193,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]"
CLAIMS,0.9267782426778243,"Justification: Source code is in supplemental material. We will open-source it upon publica-
tion.
Guidelines:"
CLAIMS,0.9288702928870293,"â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
â€¢ While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
â€¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
CLAIMS,0.9309623430962343,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Appendix D"
CLAIMS,0.9330543933054394,Guidelines:
CLAIMS,0.9351464435146444,"â€¢ The answer NA means that the paper does not include experiments.
â€¢ The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢ The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9372384937238494,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9393305439330544,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9414225941422594,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9435146443514645,Justification: See Sec. 5.1 and Appendix E.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9456066945606695,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9476987447698745,"â€¢ The answer NA means that the paper does not include experiments.
â€¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢ For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9497907949790795,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9518828451882845,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9539748953974896,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9560669456066946,Justification: Appendix D and Sec. 5.2
EXPERIMENTS COMPUTE RESOURCES,0.9581589958158996,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9602510460251046,"â€¢ The answer NA means that the paper does not include experiments.
â€¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper)."
CODE OF ETHICS,0.9623430962343096,9. Code Of Ethics
CODE OF ETHICS,0.9644351464435147,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9665271966527197,Answer: [Yes]
CODE OF ETHICS,0.9686192468619247,"Justification: Yes.
Guidelines:"
CODE OF ETHICS,0.9707112970711297,"â€¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
CODE OF ETHICS,0.9728033472803347,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Appendix H.
Guidelines:"
CODE OF ETHICS,0.9748953974895398,"â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
â€¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢ If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
CODE OF ETHICS,0.9769874476987448,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper is highly theoretical, thereâ€™s little risk the released training code is
misused.
Guidelines:"
CODE OF ETHICS,0.9790794979079498,"â€¢ The answer NA means that the paper poses no such risks.
â€¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9811715481171548,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9832635983263598,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Yes, we have.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9853556485355649,"â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢ The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢ If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢ If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.9874476987447699,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: Yes, we have.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9895397489539749,"â€¢ The answer NA means that the paper does not release new assets.
â€¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢ At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.9916317991631799,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing or research with human subjects.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9937238493723849,"â€¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢ Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
LICENSES FOR EXISTING ASSETS,0.99581589958159,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.997907949790795,"â€¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢ For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
