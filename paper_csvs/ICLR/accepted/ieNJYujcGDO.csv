Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024096385542168677,"In the Mixup training paradigm, a model is trained using convex combinations of
data points and their associated labels. Despite seeing very few true data points
during training, models trained using Mixup seem to still minimize the original
empirical risk and exhibit better generalization and robustness on various tasks
when compared to standard training. In this paper, we investigate how these beneÔ¨Åts
of Mixup training rely on properties of the data in the context of classiÔ¨Åcation.
For minimizing the original empirical risk, we compute a closed form for the
Mixup-optimal classiÔ¨Åcation, which allows us to construct a simple dataset on
which minimizing the Mixup loss can provably lead to learning a classiÔ¨Åer that
does not minimize the empirical loss on the data. On the other hand, we also give
sufÔ¨Åcient conditions for Mixup training to also minimize the original empirical
risk. For generalization, we characterize the margin of a Mixup classiÔ¨Åer, and use
this to understand why the decision boundary of a Mixup classiÔ¨Åer can adapt better
to the full structure of the training data when compared to standard training. In
contrast, we also show that, for a large class of linear models and linearly separable
datasets, Mixup training leads to learning the same classiÔ¨Åer as standard training."
INTRODUCTION,0.004819277108433735,"1
INTRODUCTION"
INTRODUCTION,0.007228915662650603,"Mixup (Zhang et al., 2018) is a modiÔ¨Åcation to the standard supervised learning setup which involves
training on convex combinations of pairs of data points and their labels instead of the original data
itself. In the original paper, Zhang et al. (2018) demonstrated that training deep neural networks using
Mixup leads to better generalization performance, as well as greater robustness to adversarial attacks
and label noise on image classiÔ¨Åcation tasks. The empirical advantages of Mixup training have been
afÔ¨Årmed by several follow-up works (He et al., 2019; Thulasidasan et al., 2019; Lamb et al., 2019;
Arazo et al., 2019; Guo, 2020). The idea of Mixup has also been extended beyond the supervised
learning setting, and been applied to semi-supervised learning (Berthelot et al., 2019; Sohn et al.,
2020), contrastive learning (Verma et al., 2021; Lee et al., 2020), privacy-preserving learning (Huang
et al., 2021), and learning with fairness constraints (Chuang & Mroueh, 2021)."
INTRODUCTION,0.00963855421686747,"However, from a theoretical perspective, Mixup training is still mysterious even in the basic multi-
class classÔ¨Åcation setting ‚Äì why should the output of a linear mixture of two training samples be the
same linear mixture of their labels, especially when considering highly nonlinear models? Despite
several recent theoretical results (Guo et al., 2019; Carratino et al., 2020; Zhang et al., 2020; 2021),
there is still not a complete understanding of why Mixup training actually works in practice. In this
paper, we try to understand why Mixup works by Ô¨Årst understanding when Mixup works: in particular,
how the properties of Mixup training rely on the structure of the training data."
INTRODUCTION,0.012048192771084338,"We consider two properties for classiÔ¨Åers trained with Mixup. First, even though Mixup training
does not observe many original data points during training, it usually can still correctly classify all of
the original data points (empirical risk minimization (ERM)). Second, the aforementioned empirical
works have shown how classiÔ¨Åers trained with Mixup often have better adversarial robustness and
generalization than standard training. In this work, we show that both of these properties can rely
heavily on the data used for training, and that they need not hold in general."
INTRODUCTION,0.014457831325301205,"Main Contributions and Related Work. The idea that Mixup can potentially fail to minimize
the original risk is not new; Guo et al. (2019) provide examples of how Mixup labels can conÔ¨Çict
with actual data point labels. However, their theoretical results do not characterize the data and"
INTRODUCTION,0.016867469879518072,Correspondence to Muthu Chidambaram (muthu@cs.duke.edu).
INTRODUCTION,0.01927710843373494,Published as a conference paper at ICLR 2022
INTRODUCTION,0.021686746987951807,"model conditions under which this failure can provably happen when minimizing the Mixup loss. In
Section 2 of this work, we provide a concrete classiÔ¨Åcation dataset on which continuous approximate-
minimizers of the Mixup loss can fail to minimize the empirical risk. We also provide sufÔ¨Åcient
conditions for Mixup to minimize the original risk, and show that these conditions hold approximately
on standard image classiÔ¨Åcation benchmarks."
INTRODUCTION,0.024096385542168676,"With regards to generalization and robustness, the parallel works of Carratino et al. (2020) and Zhang
et al. (2020) showed that Mixup training can be viewed as minimizing the empirical loss along with
a data-dependent regularization term. Zhang et al. (2020) further relate this term to the adversarial
robustness and Rademacher complexity of certain function classes learned with Mixup. In Section
3, we take an alternative approach to understanding generalization and robustness by analyzing
the margin of Mixup classiÔ¨Åers. Our perspective can be viewed as complementary to that of the
aforementioned works, as we directly consider the properties exhibited by a Mixup-optimal classiÔ¨Åer
instead of considering what properties are encouraged by the regularization effects of the Mixup loss.
In addition to our margin analysis, we also show that for the common setting of linear models trained
on high-dimensional Gaussian features both Mixup (for a large class of mixing distributions) and
ERM with gradient descent learn the same classiÔ¨Åer with high probability."
INTRODUCTION,0.02650602409638554,"Finally, we note the related works that are beyond the scope of our paper; namely the many Mixup-like
training procedures such as Manifold Mixup (Verma et al., 2019), Cut Mix (Yun et al., 2019), Puzzle
Mix (Kim et al., 2020), and Co-Mixup (Kim et al., 2021)."
MIXUP AND EMPIRICAL RISK MINIMIZATION,0.02891566265060241,"2
MIXUP AND EMPIRICAL RISK MINIMIZATION"
MIXUP AND EMPIRICAL RISK MINIMIZATION,0.03132530120481928,"The goal of this section is to understand when Mixup training can also minimize the empirical risk.
Our main technique for doing so is to derive a closed-form for the Mixup-optimal classiÔ¨Åer over a
sufÔ¨Åciently powerful function class, which we do in Section 2.2 after introducing the basic setup in
Section 2.1. We use this closed form to motivate a concrete example on which Mixup training does
not minimize the empirical risk in Section 2.3, and show under mild nondegeneracy conditions that
Mixup will minimize the emprical risk in Section 2.4."
SETUP,0.033734939759036145,"2.1
SETUP"
SETUP,0.03614457831325301,"We consider the problem of k-class classiÔ¨Åcation where the classes 1, ..., k correspond to compact
disjoint sets X1, ..., Xk ‚äÇRn with an associated probability measure PX supported on X =
Sk
i=1 Xi. We use C to denote the set of all functions g : Rn ‚Üí[0, 1]k satisfying the property that
Pk
i=1 gi(x) = 1 for all x (where gi represents the i-th coordinate function of g). We refer to a
function g ‚ààC as a classiÔ¨Åer, and say that g classiÔ¨Åes x as class j if j = argmaxi gi(x). The
cross-entropy loss associated with such a classiÔ¨Åer g is then:"
SETUP,0.03855421686746988,"J(g, PX) = ‚àí k
X i=1 Z"
SETUP,0.04096385542168675,"Xi
log gi(x)dPX(x)"
SETUP,0.043373493975903614,"The goal of standard training is to learn a classiÔ¨Åer h ‚ààargming‚ààC J(g, PX). Any such classiÔ¨Åer h
will necessarily satisfy hi(x) = 1 on Xi since the Xi are disjoint."
SETUP,0.04578313253012048,"Mixup. In the Mixup version of our setup, we are interested in minimizing the cross-entropy of
convex combinations of the original data and their classes. These convex combinations are determined
according to a probability measure Pf whose support is [0, 1], and we assume this measure has a
density f. For two points s, t ‚ààX, we let zst(Œª) = Œªs + (1 ‚àíŒª)t (and use zst when Œª is understood)
and deÔ¨Åne the Mixup cross-entropy on s, t with respect to a classiÔ¨Åer g as:"
SETUP,0.04819277108433735,"‚Ñìmix(g, s, t, Œª) =
‚àílog gi(zst)
s, t ‚ààXi
‚àí
 
Œª log gi(zst) + (1 ‚àíŒª) log gj(zst)

s ‚ààXi, t ‚ààXj"
SETUP,0.05060240963855422,"Having deÔ¨Åned ‚Ñìmix as above, we may write the component of the full Mixup cross-entropy loss
corresponding to mixing points from classes i and j as:"
SETUP,0.05301204819277108,"Ji,j
mix(g, PX, Pf) =
Z"
SETUP,0.05542168674698795,"Xi√óXj√ó[0,1]
‚Ñìmix(g, s, t, Œª) d(PX √ó PX √ó Pf)(s, t, Œª)"
SETUP,0.05783132530120482,Published as a conference paper at ICLR 2022
SETUP,0.060240963855421686,"The Ô¨Ånal Mixup cross-entropy loss is then the sum of Ji,j
mix over all i, j ‚àà{1, ..., k} (corresponding
to all possible mixings between classes, including themselves):"
SETUP,0.06265060240963856,"Jmix(g, PX, Pf) = k
X i=1 k
X"
SETUP,0.06506024096385542,"j=1
Ji,j
mix(g, PX, Pf)"
SETUP,0.06746987951807229,"Relation to Prior Work. We have opted for a more general deÔ¨Ånition of the Mixup loss (at least
when constrained to multi-class classiÔ¨Åcation) than prior works. This is not generality for generality‚Äôs
sake, but rather because many of our results apply to any mixing distribution supported on [0, 1].
One obtains the original Mixup formulation of Zhang et al. (2018) for multi-class classiÔ¨Åcation on
a Ô¨Ånite dataset by taking the Xi to be Ô¨Ånite sets, and choosing PX to be the normalized counting
measure (corresponding to a discrete uniform distribution). Additionally, Pf is chosen to have density
Beta(Œ±, Œ±), where Œ± is a hyperparameter."
MIXUP-OPTIMAL CLASSIFIER,0.06987951807228916,"2.2
MIXUP-OPTIMAL CLASSIFIER"
MIXUP-OPTIMAL CLASSIFIER,0.07228915662650602,"Given our setup, we now wish to characterize the behavior of a Mixup-optimal classiÔ¨Åer at a point
x ‚ààRn. However, if the optimization of Jmix is considered over the class of functions C, this
is intractable (to the best of our knowledge) due to the lack of regularity conditions imposed on
functions in C. We thus wish to constrain the optimization of Jmix to a class of functions that is
sufÔ¨Åciently powerful (so as to include almost all practical settings) while still allowing for local
analysis. To do so, we will need the following deÔ¨Ånitions, which will also be referenced throughout
the results in this section and the next:"
MIXUP-OPTIMAL CLASSIFIER,0.0746987951807229,"Ai,j
x,œµ = {(s, t, Œª) ‚ààXi √ó Xj √ó [0, 1] : Œªs + (1 ‚àíŒª)t ‚ààBœµ(x)}"
MIXUP-OPTIMAL CLASSIFIER,0.07710843373493977,"Ai,j
x,œµ,Œ¥ = {(s, t, Œª) ‚ààXi √ó Xj √ó [0, 1 ‚àíŒ¥] : Œªs + (1 ‚àíŒª)t ‚ààBœµ(x)}"
MIXUP-OPTIMAL CLASSIFIER,0.07951807228915662,"Xmix = Ô£±
Ô£≤"
MIXUP-OPTIMAL CLASSIFIER,0.0819277108433735,"Ô£≥x ‚ààRn :
["
MIXUP-OPTIMAL CLASSIFIER,0.08433734939759036,"i,j
Ai,j
x,œµ has positive measure for every œµ > 0 Ô£º
Ô£Ω Ô£æ"
MIXUP-OPTIMAL CLASSIFIER,0.08674698795180723,"Œæi,j
x,œµ =
Z"
MIXUP-OPTIMAL CLASSIFIER,0.0891566265060241,"Ai,j
x,œµ
d(PX √ó PX √ó Pf)(s, t, Œª)"
MIXUP-OPTIMAL CLASSIFIER,0.09156626506024096,"Œæi,j
x,œµ,Œª =
Z"
MIXUP-OPTIMAL CLASSIFIER,0.09397590361445783,"Ai,j
x,œµ
Œª d(PX √ó PX √ó Pf)(s, t, Œª)"
MIXUP-OPTIMAL CLASSIFIER,0.0963855421686747,"The set Ai,j
x,œµ represents all points in Xi √ó Xj that have lines between them intersecting an œµ-
neighborhood of x, while the set Ai,j
x,œµ,Œ¥ represents the restriction of Ai,j
x,œµ to only those points whose
connecting line segments intersect an œµ-neighborhood of x with Œª values bounded by 1 ‚àíŒ¥ (used in
Section 3). The set Xmix corresponds to all points for which every neighborhood factors into Jmix.
The Œæi,j
x,œµ term represents the measure of the set Ai,j
x,œµ while Œæi,j
x,œµ,Œª represents the expectation of Œª over
the same set. To provide better intuition for these deÔ¨Ånitions, we provide visualizations in Section B
of the appendix. We can now deÔ¨Åne the subset of C to which we will constrain our optimization of
Jmix."
MIXUP-OPTIMAL CLASSIFIER,0.09879518072289156,"DeÔ¨Ånition 2.1. Let C‚àóto be the subset of C for which every h ‚ààC‚àósatisÔ¨Åes h(x) =
limœµ‚Üí0 argminŒ∏‚àà[0,1]k Jmix(Œ∏)|Bœµ(x) for all x ‚ààXmix when the limit exists. Here Jmix(Œ∏)|Bœµ(x)
represents the Mixup loss for a constant function with value Œ∏ with the restriction of each term in
Jmix to the set Ai,j
x,œµ."
MIXUP-OPTIMAL CLASSIFIER,0.10120481927710843,We immediately justify this deÔ¨Ånition with the following proposition.
MIXUP-OPTIMAL CLASSIFIER,0.10361445783132531,"Proposition 2.2. Any function h ‚ààargming‚ààC‚àóJmix(g, PX, Pf) satisÔ¨Åes Jmix(h) ‚â§Jmix(g) for
any continuous g ‚ààC."
MIXUP-OPTIMAL CLASSIFIER,0.10602409638554217,"Proof Sketch. We can argue directly from deÔ¨Ånitions by considering points in Xmix for which h
and g differ."
MIXUP-OPTIMAL CLASSIFIER,0.10843373493975904,Published as a conference paper at ICLR 2022
MIXUP-OPTIMAL CLASSIFIER,0.1108433734939759,"Proposition 2.2 demonstrates that optimizing over C‚àóis at least as good as optimizing over the subset
of C consisting of continuous functions, so we cover most cases of practical interest (i.e. optimizing
deep neural networks). As such, the term ‚ÄúMixup-optimal‚Äù is intended to mean optimal with respect
to C‚àóthroughout the rest of the paper. We may now characterize the classiÔ¨Åcation of a Mixup-optimal
classiÔ¨Åer on Xmix."
MIXUP-OPTIMAL CLASSIFIER,0.11325301204819277,"Lemma 2.3. For any point x ‚ààXmix and œµ > 0, there exists a continuous function hœµ satisfying:"
MIXUP-OPTIMAL CLASSIFIER,0.11566265060240964,"hi
œµ(x) =
Œæi,i
x,œµ + P"
MIXUP-OPTIMAL CLASSIFIER,0.1180722891566265,"jÃ∏=i
 
Œæi,j
x,œµ,Œª + (Œæj,i
x,œµ ‚àíŒæj,i
x,œµ,Œª)
"
MIXUP-OPTIMAL CLASSIFIER,0.12048192771084337,"Pk
q=1"
MIXUP-OPTIMAL CLASSIFIER,0.12289156626506025,"
Œæq,q
x,œµ + P"
MIXUP-OPTIMAL CLASSIFIER,0.12530120481927712,"jÃ∏=q
 
Œæq,j
x,œµ,Œª + (Œæj,q
x,œµ ‚àíŒæj,q
x,œµ,Œª)

(1)"
MIXUP-OPTIMAL CLASSIFIER,0.12771084337349398,"With the property that limœµ‚Üí0 hœµ(x) = h(x) for every h ‚ààargming‚ààC‚àóJmix(g, PX, Pf) when the
limit exists."
MIXUP-OPTIMAL CLASSIFIER,0.13012048192771083,"Proof Sketch. We set hœµ = argminŒ∏‚àà[0,1]k Jmix(Œ∏)|Bœµ(x) and show that this is well-deÔ¨Åned, contin-
uous, and has the above form using the strict convexity of the minimization problem."
MIXUP-OPTIMAL CLASSIFIER,0.13253012048192772,"Remark 2.4. For the important case of Ô¨Ånite datasets, it will be shown that the limit above always
exists as part of the proof of Theorem 3.2."
MIXUP-OPTIMAL CLASSIFIER,0.13493975903614458,"The expression for hi
œµ just represents the expected location of the point x on all lines between class i
and other classes, normalized by the sum of the expected locations for all classes. It can be simpliÔ¨Åed
signiÔ¨Åcantly if Pf is assumed to be symmetric; we give this as a corollary after the proof in Section C
of the Appendix. Importantly, we note that while hœµ as deÔ¨Åned in Lemma 2.3 is continuous for every
œµ > 0, its pointwise limit h need not be, which we demonstrate below."
MIXUP-OPTIMAL CLASSIFIER,0.13734939759036144,"Proposition 2.5. Let X1 = {(0, 1), (0, ‚àí1)} and let X2 = {(1, 0), (‚àí1, 0)}, with PX being discrete
uniform over X1‚à™X2 and Pf being continuous uniform over [0, 1]. Then the Mixup-optimal classiÔ¨Åer
h is discontinuous at (0, 0)."
MIXUP-OPTIMAL CLASSIFIER,0.13975903614457832,"Proof Sketch. One may explicitly compute for x = (0, 0) that h1(x) = h2(x) = 1 2."
MIXUP-OPTIMAL CLASSIFIER,0.14216867469879518,"Proposition 2.5 illustrates our Ô¨Årst signiÔ¨Åcant difference between Mixup training and standard training:
there always exists a minimizer of the empirical cross-entropy J that can be extended to a continuous
function (since a minimizer is constant on the class supports and not constrained elsewhere), whereas
depending on the data the minimizer of Jmix can be discontinuous."
A MIXUP FAILURE CASE,0.14457831325301204,"2.3
A MIXUP FAILURE CASE"
A MIXUP FAILURE CASE,0.14698795180722893,"With that in mind, several model classes popular in practical applications consist of continuous
functions. For example, neural networks with ReLU activations are continuous, and several works
have noted that they are Lipschitz continuous with shallow networks having approximately small
Lipschitz constant (Scaman & Virmaux, 2019; Fazlyab et al., 2019; Latorre et al., 2020). Given the
regularity of such models, we are motivated to consider the continuous approximations hœµ in Lemma
2.3 and see if it is possible to construct a dataset on which hœµ (for a Ô¨Åxed œµ) can fail to classify the
original points correctly. We thus consider the following dataset:"
A MIXUP FAILURE CASE,0.1493975903614458,"DeÔ¨Ånition 2.6. [3-Point Alternating Line] We deÔ¨Åne X 2
3 to be the binary classiÔ¨Åcation dataset
consisting of the points {0, 1, 2} classiÔ¨Åed as {1, 2, 1}. In our setup, this corresponds to X1 = {0, 2}
and X2 = {1} with PX = 1"
A MIXUP FAILURE CASE,0.15180722891566265,"31{0,1,2}."
A MIXUP FAILURE CASE,0.15421686746987953,"Intuitively, the reason why Mixup can fail on X 2
3 is that, for choices of Pf that concentrate about 1"
A MIXUP FAILURE CASE,0.1566265060240964,"2,
we will have by Lemma 2.3 that the Mixup-optimal classiÔ¨Åcation in a neighborhood of point 1 should
skew towards class 1 instead of class 2 due to the sandwiching of point 1 between points 0 and 2.
The canonical choice of Pf corresponding to a mixing density of Beta(Œ±, Œ±) is one such choice:"
A MIXUP FAILURE CASE,0.15903614457831325,"Theorem 2.7. Let Pf have associated density Beta(Œ±, Œ±). Then for any classiÔ¨Åer hœµ on X 2
3 (as
deÔ¨Åned in Lemma 2.3), we may choose Œ± such that hœµ does not achieve 0 classiÔ¨Åcation error on X 2
3 ."
A MIXUP FAILURE CASE,0.1614457831325301,"Proof Sketch. For any œµ > 0, we can bound the Œæ terms in Equation 1 using the fact that Beta(Œ±, Œ±)
is strictly subgaussian (Marchal & Arbel, 2017), and then choose Œ± appropriately."
A MIXUP FAILURE CASE,0.163855421686747,Published as a conference paper at ICLR 2022
A MIXUP FAILURE CASE,0.16626506024096385,"(a) Œ± = 1
(b) Œ± = 32
(c) Œ± = 128"
A MIXUP FAILURE CASE,0.1686746987951807,"Figure 1: Training error for Mixup and regular training on X 2
3 . Each curve corresponds to the mean
of 10 training runs, and the area around each curve represents a region of one standard deviation."
A MIXUP FAILURE CASE,0.1710843373493976,"Experiments. The result of Theorem 2.7 leads us to believe that the Mixup training of a continuous
model should fail on X 2
3 for appropriately chosen Œ±. To verify that the theory predicts the experiments,
we train a two-layer feedforward neural network with 512 hidden units and ReLU activations on X 2
3
with and without Mixup. The implementation of Mixup training does not differ from the theoretical
setup; we uniformly sample pairs of data points and train on their mixtures. Our implementation uses
PyTorch (Paszke et al., 2019) and is based heavily on the open source implementation of Manifold
Mixup (Verma et al., 2019) by Shivam Saboo. Results for training using (full-batch) Adam (Kingma
& Ba, 2015) with the suggested (and common) hyperparameters of Œ≤1 = 0.9, Œ≤2 = 0.999 and a
learning rate of 0.001 are shown in Figure 1. The class 1 probabilities for each point in the dataset
outputted by the learned Mixup classiÔ¨Åers from Figure 1 are shown in Table 1 below:"
A MIXUP FAILURE CASE,0.17349397590361446,"h
0
1
2
Œ± = 1
0.995
0.156
0.979
Œ± = 32
1.000
0.603
0.997
Œ± = 128
1.000
0.650
0.997"
A MIXUP FAILURE CASE,0.17590361445783131,"Table 1: Mixup model evaluations on X 2
3 for different choices of Œ±."
A MIXUP FAILURE CASE,0.1783132530120482,"We see from Figure 1 and Table 1 that Mixup training fails to correctly classify the points in X 2
3 for
Œ± = 32, and this misclassiÔ¨Åcation becomes more exacerbated as we increase Œ±. The choice of Œ± for
which misclassiÔ¨Åcations begin to happen is largely superÔ¨Åcial; we show in Section D of the Appendix
that it is straightforward to construct datasets in the style of X 2
3 for which Mixup training will fail
even for the very mild choice of Œ± = 1. We focus on the case of X 2
3 here to simplify the theory. The
key takeaway is that, for datasets that exhibit (approximately) collinear structure amongst points, it is
possible for inappropriately chosen mixing distributions to cause Mixup training to fail to minimize
the original empirical risk."
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.18072289156626506,"2.4
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK"
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.18313253012048192,"The natural follow-up question to the results of the previous subsection is: under what conditions
on the data can this failure case be avoided? In other words, when can the Mixup-optimal classiÔ¨Åer
classify the original data points correctly while being continuous at those points?"
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.1855421686746988,"Prior to answering that question, we Ô¨Årst point out that if discontinuous functions are allowed, then
Mixup training always minimizes the original risk on Ô¨Ånite datasets:"
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.18795180722891566,"Proposition 2.8. Consider k-class classiÔ¨Åcation where the supports X1, ..., Xk are Ô¨Ånite and PX
corresponds to the discrete uniform distribution. Then for every h ‚ààargming‚ààC‚àóJmix(g, PX, Pf),
we have that hi(x) = 1 on Xi."
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.19036144578313252,"Proof Sketch. Only the Œæi,i
x,œµ term doesn‚Äôt vanish in hi
œµ(x) as œµ ‚Üí0, as the mixing distribution is
continuous and cannot assign positive measure to x alone when mixing two points that are not x."
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.1927710843373494,"Note that Proposition 2.8 holds for any continuous mixing distribution Pf supported on [0, 1] - we
just need a rich enough model class."
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.19518072289156627,Published as a conference paper at ICLR 2022
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.19759036144578312,"(a) MNIST
(b) CIFAR-10
(c) CIFAR-100"
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.2,"Figure 2: Mean and single standard deviation of 5 training runs for Mixup (Œ± = 1024) and ERM on
the original training data. Mixup achieves near-identical (within 1%) training accuracy to ERM."
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.20240963855421687,"In order to obtain the result of Proposition 2.8 with the added restriction of continuity of h on each of
the Xi, we need to further assume that the collinearity of different class points that occurred in the
previous section does not happen.
Assumption 2.9. For any point x ‚ààXi, there do not exist u ‚ààX and v ‚ààXj for j Ã∏= i such that
there is a Œª > 0 for which x = Œªu + (1 ‚àíŒª)v."
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.20481927710843373,"A visualization of Assumption 2.9 is provided in Section B of the appendix. With this assumption
in hand, we obtain the following result as a corollary of Theorem 3.2 which is proved in the next
section:
Theorem 2.10. We consider the same setting as Proposition 2.8 and further suppose that Assumption
2.9 is satisÔ¨Åed. Then for every h ‚ààargming‚ààC‚àóJmix(g, PX, Pf), we have that hi(x) = 1 on Xi and
that h is continuous on X."
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.20722891566265061,"Application of SufÔ¨Åcient Conditions. The practical take-away of Theorem 2.10 is that if a dataset
does not exhibit approximate collinearity between points of different classes, then Mixup training
should achieve near-identical training error on the original training data when compared to ERM. We
validate this by training ResNet-18 (He et al., 2015) (using the popular implementation of Kuang Liu)
on MNIST (LeCun, 1998), CIFAR-10, and CIFAR-100 (Krizhevsky, 2009) with and without Mixup
for 50 epochs with a batch size of 128 and otherwise identical settings to the previous subsection. For
Mixup, we consider mixing using Beta(Œ±, Œ±) for Œ± = 1, 32, 128, and 1024 to cover a wide band of
mixing distributions. Our experimental results for the ‚Äúworst case‚Äù of Œ± = 1024 (the other choices
are strictly closer to ERM in training accuracy) are shown in Figure 2, while the other experiments
can be found in Section D of the Appendix."
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.20963855421686747,"We now check that the theory can predict the results of our experiments by verifying Assumption 2.9
approximately (exact veriÔ¨Åcation is too expensive). We sample one epoch‚Äôs worth of Mixup points (to
simulate training) from a downsampled version of each train dataset, and then compute the minimum
distances between each Mixup point and points from classes other than the two mixed classes. The
minimum over these distances corresponds to an estimate of œµ in Assumption 2.9. We compute the
distances for both training and test data, to see whether good training but poor test performance can
be attributed to test data conÔ¨Çicting with mixed training points. Results are shown below in Table 2."
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.21204819277108433,"Comparison Type
MNIST
CIFAR-10
CIFAR-100
Mixup/Train
11.433
17.716
12.936
Mixup/Test
11.897
22.133
15.076"
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.21445783132530122,"Table 2: Minimum Euclidean distance results using our approximation procedure with Mixup
points generated using Beta(1024, 1024). We downsample all datasets to 20%, to compare to the
experiments of Guo et al. (2019)."
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.21686746987951808,"To interpret the estimated œµ values in Table 2, we note that unless œµ ‚â™1/L (where L is the Lipschitz
constant of the model being considered), a Mixup point cannot conÔ¨Çict with an original point (since
the function has enough Ô¨Çexibility to Ô¨Åt both). Due to the estimated large Lipschitz constants of deep
networks (Scaman & Virmaux, 2019), our œµ values certainly do not fall in this regime, explaining
how the near-identical performance to ERM is possible on the original datasets. We remark that our
results challenge an implication of Guo et al. (2019), which was that training/test performance on"
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.21927710843373494,Published as a conference paper at ICLR 2022
SUFFICIENT CONDITIONS FOR MINIMIZING THE ORIGINAL RISK,0.2216867469879518,"the above benchmarks degrades with high values of Œ± due to conÔ¨Çicts between original points and
Mixup points."
THE RATE OF EMPIRICAL RISK MINIMIZATION USING MIXUP,0.22409638554216868,"2.5
THE RATE OF EMPIRICAL RISK MINIMIZATION USING MIXUP"
THE RATE OF EMPIRICAL RISK MINIMIZATION USING MIXUP,0.22650602409638554,"Another striking aspect of the experiments in Figure 2 is that Mixup training minimizes the original
empirical risk at a very similar rate to that of direct empirical risk minimization. A priori, there is
no reason to expect that Mixup should be able to do this - a simple calculation shows that Mixup
training only sees one true data point per epoch in expectation (each pair of points is sampled with
probability
1
m2 and there are m true point pairs and m pairs seen per epoch, where m is the dataset
size). The experimental results are even more surprising given that we are training using Œ± = 1024,
which essentially corresponds to training using the midpoints of the original data points. This seems
to imply that it is possible to recover the classiÔ¨Åcations of the original data points from the midpoints
alone (not including the midpoint of a point and itself), and similar phenomena has indeed been
observed in recent empirical work (Guo, 2021). We make this rigorous with the following result:
Theorem 2.11. Suppose {x1, ..., xm} with m ‚â•6 are sampled from X according to PX, and that
PX has a density. Then with probability 1, we can uniquely determine the points {x1, ..., xm} given
only the
 m
2

midpoints {xi,j}1‚â§i<j‚â§m."
THE RATE OF EMPIRICAL RISK MINIMIZATION USING MIXUP,0.2289156626506024,"Proof Sketch. The idea is to represent the problem as a linear system, and show using rank arguments
that the sets of m points that cannot be uniquely determined are a measure zero set."
THE RATE OF EMPIRICAL RISK MINIMIZATION USING MIXUP,0.23132530120481928,"Theorem 2.11 shows, in an information-theoretic sense, that it is possible to obtain the original data
points (and therefore also their labels) from only their midpoints. While this gives more theoretical
backing as to why it is possible for Mixup training using Beta(1024, 1024) to recover the original
data point classiÔ¨Åcations with very low error, it does not explain why this actually happens in practice
at the rate that it does. A full theoretical analysis of this phenomenon would necessarily require
analyzing the training dynamics of neural networks (or another model of choice) when trained only
on midpoints of the original data, which is outside the intended scope of this work. That being said,
we hope that such analysis will be a fruitful line of investigation for future work."
GENERALIZATION PROPERTIES OF MIXUP CLASSIFIERS,0.23373493975903614,"3
GENERALIZATION PROPERTIES OF MIXUP CLASSIFIERS"
GENERALIZATION PROPERTIES OF MIXUP CLASSIFIERS,0.236144578313253,"Having discussed how Mixup training differs from standard empirical risk minimization with regards
to the original training data, we now consider how a learned Mixup classiÔ¨Åer can differ from one
learned through empirical risk minimization on unseen test data. To do so, we analyze the per-class
margin of Mixup classiÔ¨Åers, i.e. the distance one can move from a class support Xi while still being
classiÔ¨Åed as class i."
THE MARGIN OF MIXUP CLASSIFIERS,0.2385542168674699,"3.1
THE MARGIN OF MIXUP CLASSIFIERS"
THE MARGIN OF MIXUP CLASSIFIERS,0.24096385542168675,"Intuitively, if a point x falls only on line segments between Xi and some other classes Xj, ..., and if
x always falls closer to Xi than the other classes, we can expect x to be classiÔ¨Åed according to class
i by the Mixup-optimal classiÔ¨Åer due to Lemma 2.3. To make this rigorous, we introduce another
assumption that generalizes Assumption 2.9 to points outside of the class supports:
Assumption 3.1. For a class i and a point x ‚ààXmix, suppose there exists an œµ > 0 and a 0 < Œ¥ < 1"
THE MARGIN OF MIXUP CLASSIFIERS,0.2433734939759036,"2
such that Ai,j
x,œµ‚Ä≤,Œ¥ and Aj,q
x,œµ‚Ä≤ have measure zero for all œµ‚Ä≤ ‚â§œµ and j, q Ã∏= i, and the measure of Ai,j
x,œµ is
at least that of Aj,i
x,œµ."
THE MARGIN OF MIXUP CLASSIFIERS,0.2457831325301205,"Here the measure zero conditions are codifying the ideas that the point x falls closer to Xi than
any other class on every line segment that intersects it, and there are no line segments between
non-i classes that intersect x. The condition that the measure of Ai,j
x,œµ is at least that of Aj,i
x,œµ handles
asymmetric mixing distributions that concentrate on pathological values of Œª. A visualization of
Assumption 3.1 is provided in Section B of the Appendix. Now we can prove:
Theorem 3.2. Consider k-class classiÔ¨Åcation where the supports X1, ..., Xk are Ô¨Ånite and PX
corresponds to the discrete uniform distribution. If a point x satisÔ¨Åes Assumption 3.1 with respect to
a class i, then for every h ‚ààargming‚ààC‚àóJmix(g, PX, Pf), we have that h classiÔ¨Åes x as class i and
that h is continuous at x."
THE MARGIN OF MIXUP CLASSIFIERS,0.24819277108433735,Published as a conference paper at ICLR 2022
THE MARGIN OF MIXUP CLASSIFIERS,0.25060240963855424,"Figure 3: Decision boundary plots for standard and Mixup training on the two moons dataset of
Pezeshki et al. (2020) with a class separation of 0.5. Each boundary represents the average of 10
training runs of 1500 epochs."
THE MARGIN OF MIXUP CLASSIFIERS,0.25301204819277107,"Proof Sketch. The limit in Lemma 2.3 can be shown to exist using the Lebesgue differentiation
theorem, and we can bound the limit below since the Ai,j
x,œµ‚Ä≤,Œ¥ have measure zero."
THE MARGIN OF MIXUP CLASSIFIERS,0.25542168674698795,"Assumption 2.9 implies Assumption 3.1 with respect to each class, and hence we get Theorem
2.10 as a corollary of Theorem 3.2 as mentioned in Section 2. To use Theorem 3.2 to understand
generalization, we make the observation that a point x can satisfy Assumption 3.1 while being a
distance of up to minj d(Xi,Xj)"
THE MARGIN OF MIXUP CLASSIFIERS,0.25783132530120484,"2
from some class i. This distance can be signiÔ¨Åcantly farther than, for
example, the optimal linear separator in a linearly separable dataset."
THE MARGIN OF MIXUP CLASSIFIERS,0.26024096385542167,"Experiments. To illustrate that Mixup can lead to more separation between points than a linear
decision boundary, we consider the two moons dataset (Buitinck et al., 2013), which consists of two
classes of points supported on semicircles with added Gaussian noise. Our motivation for doing so
comes from the work of Pezeshki et al. (2020), in which it was noted that neural network models
trained on a separated version of the two moons dataset essentially learned a linear separator while
ignoring the curvature of the class supports. While Pezeshki et al. (2020) introduced an explicit
regularizer to encourage a nonlinear decision boundary, we expect due to Theorem 3.2 that Mixup
training will achieve a similar result without any additional modiÔ¨Åcations."
THE MARGIN OF MIXUP CLASSIFIERS,0.26265060240963856,"To verify this empirically, we train a two-layer neural network with 500 hidden units with and without
Mixup, to have a 1-to-1 comparison with the setting of Pezeshki et al. (2020). We use Œ± = 1 and
Œ± = 1024 for Mixup to capture a wide band of mixing densities. The version of the two moons
dataset we use is also identical to that of the one used in the experiments of Pezeshki et al. (2020), and
we are grateful to the authors for releasing their code under the MIT license. We do full-batch training
with all other training, implementation, and compute details remaining the same as the previous
section. Results are shown in Figure 3."
THE MARGIN OF MIXUP CLASSIFIERS,0.26506024096385544,"Our results afÔ¨Årm the observations of Pezeshki et al. (2020) and previous work (des Combes et al.,
2018) that neural network training dynamics may ignore salient features of the dataset; in this case the
‚ÄúBase Model‚Äù learns to differentiate the two classes essentially based on the x-coordinate alone. On
the other hand, the models trained using Mixup have highly nonlinear decision boundaries. Further
experiments for different class separations and values of Œ± are included in Section F of the Appendix."
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.2674698795180723,"3.2
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER"
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.26987951807228916,"The experiments and theory of the previous sections have shown how a Mixup classiÔ¨Åer can differ
signiÔ¨Åcantly from one learned through standard training. In this subsection, we now consider the
opposing question - when is the Mixup classiÔ¨Åer the same as the one learned through standard
training? Prior work (Archambault et al., 2019) has considered when Mixup training coincides with
certain adversarial training, and our results complement this line of work. The motivation for our
results comes from the fact that a practitioner need not spend compute on Mixup training in addition
to standard training in settings where the performance will be provably the same."
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.27228915662650605,"We consider the case of binary classiÔ¨Åcation using a linear model Œ∏‚ä§x on high-dimensional Gaussian
data, which is a setting that arises naturally when training using Gaussian kernels. SpeciÔ¨Åcally, we
consider the dataset X to consist of n points in Rd distributed according to N(0, Id) with d > n
(to be made more precise shortly). We also consider the mixing distribution to be any symmetric
distribution supported on [0, 1] (thereby including as a special case Beta(Œ±, Œ±)). We let the labels of"
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.2746987951807229,Published as a conference paper at ICLR 2022
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.27710843373493976,"points in X be ¬±1 (so that the sign of Œ∏‚ä§x is the classiÔ¨Åcation), and use X1 and X‚àí1 to denote the
individual class points. We will show that in this setting, the optimal Mixup classiÔ¨Åer is the same (up
to rescaling of Œ∏) as the ERM classiÔ¨Åer learned using gradient descent with high probability. To do so
we need some additional deÔ¨Ånitions."
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.27951807228915665,"DeÔ¨Ånition 3.3. We say ÀÜŒ∏ is an interpolating solution, if there exists k > 0 such that"
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.2819277108433735,"ÀÜŒ∏‚ä§xi = ‚àíÀÜŒ∏‚ä§zj = k ‚àÄxi ‚ààX1, ‚àÄzj ‚ààX‚àí1."
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.28433734939759037,DeÔ¨Ånition 3.4. The maximum margin solution ÀúŒ∏ is deÔ¨Åned through:
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.28674698795180725,"ÀúŒ∏ := argmax
‚à•Œ∏‚à•2=1"
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.2891566265060241,"
min
xi‚ààX1,zj‚ààX‚àí1"
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.29156626506024097,"
Œ∏‚ä§xi, ‚àíŒ∏‚ä§zj
	"
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.29397590361445786,"When the maximum margin solution coincides with an interpolating solution for the dataset X (i.e.
all the points are support vectors), we have that Mixup training leads to learning the max margin
solution (up to rescaling)."
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.2963855421686747,"Theorem 3.5. If the maximum margin solution for X is also an interpolating solution for X, then any
Œ∏ that lies in the span of X and minimizes the Mixup loss Jmix for a symmetric mixing distribution
Pf is a rescaling of the maximum margin solution."
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.2987951807228916,"Proof Sketch. It can be shown that Œ∏ is an interpolating solution using a combination of the strict
convexity of Jmix as a function of Œ∏ and the symmetry of the mixing distribution."
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.30120481927710846,"Remark 3.6. For every Œ∏, we can decompose it as Œ∏ = Œ∏X + Œ∏X‚ä•where Œ∏X is the projection of
Œ∏ onto the subspace spanned by X. By deÔ¨Ånition we have that Œ∏X‚ä•is orthogonal to all possible
mixings of points in X. Hence, Œ∏X‚ä•does not affect the Mixup loss or the interpolating property, so
for simplicity we may just assume Œ∏ lies in the span of X."
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.3036144578313253,"To characterize the conditions on X under which the maximum margin solution interpolates the data,
we use a key result of Muthukumar et al. (2020), restated below. Note that Muthukumar et al. (2020)
actually provide more settings in their paper, but we constrain ourselves to the one stated below for
simplicity."
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.3060240963855422,"Lemma 3.7. [Theorem 1 in Muthukumar et al. (2020), Rephrased] Assuming d > 10n ln n + n ‚àí1,
then with probability at least 1 ‚àí2/n, the maximum margin solution for X is also an interpolating
solution."
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.30843373493975906,"To tie the optimal Mixup classiÔ¨Åer back to the classiÔ¨Åer learned through standard training, we appeal
to the fact that minimizing the empirical cross-entropy of a linear model using gradient descent
leads to learning the maximum margin solution on linearly separable data (Soudry et al., 2018; Ji &
Telgarsky, 2018). From this we obtain the desired result of this subsection:"
WHEN MIXUP TRAINING LEARNS THE SAME CLASSIFIER,0.3108433734939759,"Corollary 3.8. Under the same conditions as Lemma 3.7, the optimal Mixup classiÔ¨Åer has the same
direction as the classiÔ¨Åer learned through minimizing the empirical cross-entropy using gradient
descent with high probability."
CONCLUSION,0.3132530120481928,"4
CONCLUSION"
CONCLUSION,0.3156626506024096,"The main contribution of our work has been to provide a theoretical framework for analyzing how
Mixup training can differ from empirical risk minimization. Our results characterize a practical
failure case of Mixup, and also identify conditions under which Mixup can provably minimize the
original risk. They also show in the sense of margin why the generalization of Mixup classiÔ¨Åers
can be superior to those learned through empirical risk minimization, while again identifying model
classes and datasets for which the generalization of a Mixup classiÔ¨Åer is no different (with high
probability). We also emphasize that the generality of our theoretical framework allows most of our
results to hold for any continuous mixing distribution. Our hope is that the tools developed in this
work will see applications in future works concerned with analyzing the relationship between beneÔ¨Åts
obtained from Mixup training and properties of the training data."
CONCLUSION,0.3180722891566265,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.3204819277108434,"5
ETHICS STATEMENT"
ETHICS STATEMENT,0.3228915662650602,"We do not anticipate any direct misuses of this work due to its theoretical nature. That being said,
the failure case of Mixup discussed in Section 2 could serve as a way for an adversary to potentially
exploit a model trained using Mixup to classify data incorrectly. However, as this requires knowledge
of the mixing distribution and other hyperparameters of the model, we do not Ô¨Çag this as a signiÔ¨Åcant
concern - we would just like to point it out for completeness."
REPRODUCIBILITY STATEMENT,0.3253012048192771,"6
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.327710843373494,"Full proofs for all results in the main body of the paper can be found in Sections C and E of the
Appendix. All of the code used to generate the plots and experimental results in this paper can be
found at: https://github.com/2014mchidamb/Mixup-Data-Dependency. We have
tried our best to organize the code to be easy to use and extend. Detailed instructions for how to run
each type of experiment are provided in the README Ô¨Åle included in the GitHub repository."
REPRODUCIBILITY STATEMENT,0.3301204819277108,ACKNOWLEDGEMENTS
REPRODUCIBILITY STATEMENT,0.3325301204819277,"Rong Ge, Muthu Chidambaram, Xiang Wang, and Chenwei Wu are supported in part by NSF
Award DMS-2031849, CCF-1704656, CCF-1845171 (CAREER), CCF-1934964 (Tripods), a Sloan
Research Fellowship, and a Google Faculty Research Award. Muthu would like to thank Michael Lin
for helpful discussions during the early stages of this project."
REFERENCES,0.3349397590361446,REFERENCES
REFERENCES,0.3373493975903614,"Eric Arazo, Diego Ortego, Paul Albert, Noel E O‚ÄôConnor, and Kevin McGuinness. Unsupervised
label noise modeling and loss correction. arXiv preprint arXiv:1904.11238, 2019."
REFERENCES,0.3397590361445783,"Guillaume P. Archambault, Yongyi Mao, Hongyu Guo, and Richong Zhang. Mixup as directional
adversarial training. CoRR, abs/1906.06875, 2019. URL http://arxiv.org/abs/1906.
06875."
REFERENCES,0.3421686746987952,"David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and
Colin A Raffel.
Mixmatch:
A holistic approach to semi-supervised learning.
In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf."
REFERENCES,0.344578313253012,"Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel,
Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake
VanderPlas, Arnaud Joly, Brian Holt, and Ga√´l Varoquaux. API design for machine learning
software: experiences from the scikit-learn project. CoRR, abs/1309.0238, 2013. URL http:
//arxiv.org/abs/1309.0238."
REFERENCES,0.3469879518072289,"Luigi Carratino, Moustapha Ciss√©, Rodolphe Jenatton, and Jean-Philippe Vert. On mixup regulariza-
tion, 2020."
REFERENCES,0.3493975903614458,"Ching-Yao Chuang and Youssef Mroueh.
Fair mixup:
Fairness via interpolation.
CoRR,
abs/2103.06503, 2021. URL https://arxiv.org/abs/2103.06503."
REFERENCES,0.35180722891566263,"Remi Tachet des Combes, Mohammad Pezeshki, Samira Shabanian, Aaron C. Courville, and Yoshua
Bengio. On the learning dynamics of deep neural networks. CoRR, abs/1809.06848, 2018. URL
http://arxiv.org/abs/1809.06848."
REFERENCES,0.3542168674698795,"Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George J. Pappas. EfÔ¨Åcient
and accurate estimation of lipschitz constants for deep neural networks, 2019."
REFERENCES,0.3566265060240964,Published as a conference paper at ICLR 2022
REFERENCES,0.35903614457831323,"Hongyu Guo. Nonlinear mixup: Out-of-manifold data augmentation for text classiÔ¨Åcation. In The
Thirty-Fourth AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI 2020, The Thirty-Second Innovative
Applications of ArtiÔ¨Åcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on
Educational Advances in ArtiÔ¨Åcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12,
2020, pp. 4044‚Äì4051. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/
AAAI/article/view/5822."
REFERENCES,0.3614457831325301,"Hongyu Guo. Midpoint regularization: from high uncertainty training to conservative classiÔ¨Åcation,
2021."
REFERENCES,0.363855421686747,"Hongyu Guo, Yongyi Mao, and Richong Zhang. Mixup as locally linear out-of-manifold regulariza-
tion. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 33, pp. 3714‚Äì3722,
2019."
REFERENCES,0.36626506024096384,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition, 2015."
REFERENCES,0.3686746987951807,"Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image
classiÔ¨Åcation with convolutional neural networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019."
REFERENCES,0.3710843373493976,"Yangsibo Huang, Zhao Song, Kai Li, and Sanjeev Arora. Instahide: Instance-hiding schemes for
private distributed learning, 2021."
REFERENCES,0.37349397590361444,"Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. CoRR,
abs/1803.07300, 2018. URL http://arxiv.org/abs/1803.07300."
REFERENCES,0.3759036144578313,"Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local
statistics for optimal mixup. In International Conference on Machine Learning, pp. 5275‚Äì5285.
PMLR, 2020."
REFERENCES,0.3783132530120482,"JangHyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint
mixup with supermodular diversity. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=gvxJzw8kW4b."
REFERENCES,0.38072289156626504,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980."
REFERENCES,0.38313253012048193,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.3855421686746988,"Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated adversarial training:
Achieving robust neural networks without sacriÔ¨Åcing too much accuracy. In Proceedings of the
12th ACM Workshop on ArtiÔ¨Åcial Intelligence and Security, pp. 95‚Äì103, 2019."
REFERENCES,0.38795180722891565,"Fabian Latorre, Paul Rolland, and Volkan Cevher. Lipschitz constant estimation of neural networks
via sparse polynomial optimization, 2020."
REFERENCES,0.39036144578313253,"Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998."
REFERENCES,0.3927710843373494,"Kibok Lee, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin, and Honglak Lee. i-mix: A
strategy for regularizing contrastive representation learning. CoRR, abs/2010.08887, 2020. URL
https://arxiv.org/abs/2010.08887."
REFERENCES,0.39518072289156625,"Olivier Marchal and Julyan Arbel. On the sub-gaussianity of the beta and dirichlet distributions.
Electronic Communications in Probability, 22(none), Jan 2017. ISSN 1083-589X. doi: 10.1214/
17-ecp92. URL http://dx.doi.org/10.1214/17-ECP92."
REFERENCES,0.39759036144578314,"Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel Hsu, and Anant
Sahai. ClassiÔ¨Åcation vs regression in overparameterized regimes: Does the loss function matter?
arXiv preprint arXiv:2005.08054, 2020."
REFERENCES,0.4,Published as a conference paper at ICLR 2022
REFERENCES,0.40240963855421685,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In Advances in Neural Information Processing Systems, volume 32, pp.
8026‚Äì8037. Curran Associates, Inc., 2019."
REFERENCES,0.40481927710843374,"Mohammad Pezeshki, S√©kou-Oumar Kaba, Yoshua Bengio, Aaron C. Courville, Doina Precup,
and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. CoRR,
abs/2011.09468, 2020. URL https://arxiv.org/abs/2011.09468."
REFERENCES,0.4072289156626506,"Kevin Scaman and Aladin Virmaux. Lipschitz regularity of deep neural networks: analysis and
efÔ¨Åcient estimation, 2019."
REFERENCES,0.40963855421686746,"Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Do-
gus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning
with consistency and conÔ¨Ådence. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 596‚Äì608. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf."
REFERENCES,0.41204819277108434,"Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822‚Äì2878, 2018."
REFERENCES,0.41445783132530123,"Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.
On mixup training: Improved calibration and predictive uncertainty for deep neural networks.
Advances in Neural Information Processing Systems, 32:13888‚Äì13899, 2019."
REFERENCES,0.41686746987951806,"Vikas Verma, Alex Lamb, Christopher Beckham, Amir NajaÔ¨Å, Ioannis Mitliagkas, David Lopez-Paz,
and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In
International Conference on Machine Learning, pp. 6438‚Äì6447. PMLR, 2019."
REFERENCES,0.41927710843373495,"Vikas Verma, Minh-Thang Luong, Kenji Kawaguchi, Hieu Pham, and Quoc V. Le. Towards domain-
agnostic contrastive learning, 2021."
REFERENCES,0.42168674698795183,"Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classiÔ¨Åers with localizable features. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pp. 6023‚Äì6032, 2019."
REFERENCES,0.42409638554216866,"Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations, 2018."
REFERENCES,0.42650602409638555,"Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup
help with robustness and generalization?, 2020."
REFERENCES,0.42891566265060244,"Linjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou. When and how mixup improves
calibration, 2021."
REFERENCES,0.43132530120481927,Published as a conference paper at ICLR 2022
REFERENCES,0.43373493975903615,"A
REVIEW OF DEFINITIONS AND ASSUMPTIONS"
REFERENCES,0.43614457831325304,"For convenience, we Ô¨Årst recall the deÔ¨Ånitions and assumptions stated throughout the paper below."
REFERENCES,0.43855421686746987,"‚Ñìmix(g, s, t, Œª) =
‚àílog gi(zst)
s, t ‚ààXi
‚àí
 
Œª log gi(zst) + (1 ‚àíŒª) log gj(zst)

s ‚ààXi, t ‚ààXj"
REFERENCES,0.44096385542168676,"Ji,j
mix(g, PX, Pf) =
Z"
REFERENCES,0.4433734939759036,"Xi√óXj√ó[0,1]
‚Ñìmix(g, s, t, Œª) d(PX √ó PX √ó Pf)(s, t, Œª)"
REFERENCES,0.4457831325301205,"Jmix(g, PX, Pf) = k
X i=1 k
X"
REFERENCES,0.44819277108433736,"j=1
Ji,j
mix(g, PX, Pf)"
REFERENCES,0.4506024096385542,"Ai,j
x,œµ = {(s, t, Œª) ‚ààXi √ó Xj √ó [0, 1] : Œªs + (1 ‚àíŒª)t ‚ààBœµ(x)}"
REFERENCES,0.4530120481927711,"Ai,j
x,œµ,Œ¥ = {(s, t, Œª) ‚ààXi √ó Xj √ó [0, 1 ‚àíŒ¥] : Œªs + (1 ‚àíŒª)t ‚ààBœµ(x)}"
REFERENCES,0.45542168674698796,"Xmix = Ô£±
Ô£≤"
REFERENCES,0.4578313253012048,"Ô£≥x ‚ààRn :
["
REFERENCES,0.4602409638554217,"i,j
Ai,j
x,œµ has positive measure for every œµ > 0 Ô£º
Ô£Ω Ô£æ"
REFERENCES,0.46265060240963857,"Œæi,j
x,œµ =
Z"
REFERENCES,0.4650602409638554,"Ai,j
x,œµ
d(PX √ó PX √ó Pf)(s, t, Œª)"
REFERENCES,0.4674698795180723,"Œæi,j
x,œµ,Œª =
Z"
REFERENCES,0.46987951807228917,"Ai,j
x,œµ
Œª d(PX √ó PX √ó Pf)(s, t, Œª)"
REFERENCES,0.472289156626506,"DeÔ¨Ånition 2.1. Let C‚àóto be the subset of C for which every h ‚ààC‚àósatisÔ¨Åes h(x) =
limœµ‚Üí0 argminŒ∏‚àà[0,1]k Jmix(Œ∏)|Bœµ(x) for all x ‚ààXmix when the limit exists. Here Jmix(Œ∏)|Bœµ(x)
represents the Mixup loss for a constant function with value Œ∏ with the restriction of each term in
Jmix to the set Ai,j
x,œµ."
REFERENCES,0.4746987951807229,"DeÔ¨Ånition 2.6. [3-Point Alternating Line] We deÔ¨Åne X 2
3 to be the binary classiÔ¨Åcation dataset
consisting of the points {0, 1, 2} classiÔ¨Åed as {1, 2, 1}. In our setup, this corresponds to X1 = {0, 2}
and X2 = {1} with PX = 1"
REFERENCES,0.4771084337349398,"31{0,1,2}."
REFERENCES,0.4795180722891566,"Assumption 2.9. For any point x ‚ààXi, there do not exist u ‚ààX and v ‚ààXj for j Ã∏= i such that
there is a Œª > 0 for which x = Œªu + (1 ‚àíŒª)v."
REFERENCES,0.4819277108433735,"Assumption 3.1. For a class i and a point x ‚ààXmix, suppose there exists an œµ > 0 and a 0 < Œ¥ < 1"
REFERENCES,0.4843373493975904,"2
such that Ai,j
x,œµ‚Ä≤,Œ¥ and Aj,q
x,œµ‚Ä≤ have measure zero for all œµ‚Ä≤ ‚â§œµ and j, q Ã∏= i, and the measure of Ai,j
x,œµ is
at least that of Aj,i
x,œµ."
REFERENCES,0.4867469879518072,"DeÔ¨Ånition 3.3. We say ÀÜŒ∏ is an interpolating solution, if there exists k > 0 such that"
REFERENCES,0.4891566265060241,"ÀÜŒ∏‚ä§xi = ‚àíÀÜŒ∏‚ä§zj = k ‚àÄxi ‚ààX1, ‚àÄzj ‚ààX‚àí1."
REFERENCES,0.491566265060241,DeÔ¨Ånition 3.4. The maximum margin solution ÀúŒ∏ is deÔ¨Åned through:
REFERENCES,0.4939759036144578,"ÀúŒ∏ := argmax
‚à•Œ∏‚à•2=1"
REFERENCES,0.4963855421686747,"
min
xi‚ààX1,zj‚ààX‚àí1"
REFERENCES,0.4987951807228916,"
Œ∏‚ä§xi, ‚àíŒ∏‚ä§zj
	"
REFERENCES,0.5012048192771085,"B
VISUALIZATIONS OF DEFINITIONS AND ASSUMPTIONS"
REFERENCES,0.5036144578313253,"Due to the technical nature of the deÔ¨Ånitions and assumptions above, we provide several visualizations
in Figures 4 to 7 to help aid the reader‚Äôs intuition for our main results."
REFERENCES,0.5060240963855421,"C
FULL PROOFS FOR SECTION 2"
REFERENCES,0.5084337349397591,"We now prove all results found in Section 2 of the main body of the paper in the order that they
appear."
REFERENCES,0.5108433734939759,Published as a conference paper at ICLR 2022
REFERENCES,0.5132530120481927,"Figure 4: A visualization of Xmix and Ai,j
x,œµ."
REFERENCES,0.5156626506024097,"Figure 5: A visualization of the X 2
3 dataset and how the Mixup sandwiching works."
REFERENCES,0.5180722891566265,Published as a conference paper at ICLR 2022
REFERENCES,0.5204819277108433,"Figure 6: A visualization of Assumption 2.9, i.e. the ‚Äúno collinearity‚Äù assumption."
REFERENCES,0.5228915662650603,"Figure 7: A visualization of Assumption 3.1. Once again, the key idea is that x falls at most Œ¥ away
from Xi on every line between Xi and Xj that intersects it."
REFERENCES,0.5253012048192771,Published as a conference paper at ICLR 2022
REFERENCES,0.5277108433734939,"C.1
PROOFS FOR PROPOSITIONS, LEMMAS, AND THEOREMS 2.2 - 2.10"
REFERENCES,0.5301204819277109,"Proposition 2.2. Any function h ‚ààargming‚ààC‚àóJmix(g, PX, Pf) satisÔ¨Åes Jmix(h) ‚â§Jmix(g) for
any continuous g ‚ààC."
REFERENCES,0.5325301204819277,"Proof. Intuitively, the idea is that when h and g differ at a point x ‚ààXmix, there must be a
neighborhood of x for which the constant function that takes value h(x) has lower loss than g due to
the continuity constraint on g. We formalize this below."
REFERENCES,0.5349397590361445,"Let h be an arbitrary function in argming‚ààC‚àóJmix(g, PX, Pf) and let g be a continuous function in
C. Consider a point x ‚ààXmix such that the limit in DeÔ¨Ånition 2.1 exists and that h(x) Ã∏= g(x) (if
such an x did not exist, we would be done). Now let Œ∏h and Œ∏g be the constant functions whose values
are h(x) and g(x) (respectively) on all of Rn, and further let Œ∏hŒ¥ = argminŒ∏‚àà[0,1]k Jmix(Œ∏)|BŒ¥(x)
(this is shown to be a single value in the proof of Lemma 2.3 below). Finally, as a convenient abuse
of notation, we will use Jmix(œµ‚Ä≤)|BŒ¥(x) to indicate the result of replacing all log gi terms in the
integrands of Jmix(g)|BŒ¥(x) with œµ‚Ä≤, as shown below (note that in doing so, we can combine the Œª
and 1 ‚àíŒª terms from mixing classes i and j; this simpliÔ¨Åes the Ji,j
mix expression obtained in the proof
of Lemma 2.3)."
REFERENCES,0.5373493975903615,"Jmix(œµ‚Ä≤)|BŒ¥(x) = ‚àíœµ‚Ä≤
k
X i=1 k
X"
REFERENCES,0.5397590361445783,"j=1
Œæi,j
x,œµ"
REFERENCES,0.5421686746987951,"Since Œ∏h Ã∏= Œ∏g, we have that there exists a Œ¥‚Ä≤ > 0 such that for Œ¥ ‚â§Œ¥‚Ä≤ we have |Œ∏hŒ¥ ‚àíŒ∏g| = œµ > 0.
From this we get that there exists œµ‚Ä≤ > 0 depending only on œµ and Œ∏g such that:"
REFERENCES,0.5445783132530121,Jmix(Œ∏g)|BŒ¥(x) ‚àíJmix(Œ∏h)|BŒ¥(x) ‚â•Jmix(œµ‚Ä≤)|BŒ¥(x)
REFERENCES,0.5469879518072289,"Now by the continuity of g (and thus the continuity of log gi), we may choose Œ¥ ‚â§Œ¥‚Ä≤ such that
Jmix(g)|BŒ¥(x) ‚ààJmix(Œ∏g)|BŒ¥(x) ¬± Jmix(œµ‚Ä≤)|BŒ¥(x). This implies Jmix(g)|BŒ¥(x) ‚â•Jmix(Œ∏h)|BŒ¥(x),
and since x was arbitrary (within the initially mentioned constraints, as outside of them h is uncon-
strained) we have the desired result."
REFERENCES,0.5493975903614458,"Lemma 2.3. For any point x ‚ààXmix and œµ > 0, there exists a continuous function hœµ satisfying:"
REFERENCES,0.5518072289156627,"hi
œµ(x) =
Œæi,i
x,œµ + P"
REFERENCES,0.5542168674698795,"jÃ∏=i
 
Œæi,j
x,œµ,Œª + (Œæj,i
x,œµ ‚àíŒæj,i
x,œµ,Œª)
"
REFERENCES,0.5566265060240964,"Pk
q=1"
REFERENCES,0.5590361445783133,"
Œæq,q
x,œµ + P
jÃ∏=q
 
Œæq,j
x,œµ,Œª + (Œæj,q
x,œµ ‚àíŒæj,q
x,œµ,Œª)

(1)"
REFERENCES,0.5614457831325301,"With the property that limœµ‚Üí0 hœµ(x) = h(x) for every h ‚ààargming‚ààC‚àóJmix(g, PX, Pf) when the
limit exists."
REFERENCES,0.563855421686747,"Proof. Firstly, the condition that x ‚ààXmix is necessary, since if S"
REFERENCES,0.5662650602409639,"i,j Ai,j
x,œµ has measure zero the
LHS of Equation 1 is not even deÔ¨Åned."
REFERENCES,0.5686746987951807,"Now we simply take hœµ(x) = argminŒ∏‚àà[0,1]k Jmix(Œ∏)|Bœµ(x) as in DeÔ¨Ånition 2.1 and show that hœµ is
well-deÔ¨Åned."
REFERENCES,0.5710843373493976,"Since the argmin is over constant functions, we may unpack the deÔ¨Ånition of Jmix|Bœµ(x) and pull
each of the log Œ∏i(zst(Œª)) terms out of the integrands and rewrite them simply as log Œ∏i. Doing so,
we obtain:"
REFERENCES,0.5734939759036145,"Ji,j
mix = ‚àí
 
Œæi,j
x,œµ,Œª log Œ∏i + (Œæi,j
x,œµ ‚àíŒæi,j
x,œµ,Œª) log Œ∏j"
REFERENCES,0.5759036144578313,"Plugging the above back into Jmix and collecting the log Œ∏i terms as i = 1, ..., k we get:"
REFERENCES,0.5783132530120482,"Jmix(g, PX, Pf)|Bœµ(x) = ‚àí k
X i=1"
REFERENCES,0.5807228915662651,"
Œæi,i
x,œµ +
X jÃ∏=i"
REFERENCES,0.5831325301204819," 
Œæi,j
x,œµ,Œª + (Œæj,i
x,œµ ‚àíŒæj,i
x,œµ,Œª)

log Œ∏i"
REFERENCES,0.5855421686746988,Published as a conference paper at ICLR 2022
REFERENCES,0.5879518072289157,"Where the Ô¨Årst part of the summation above is from mixing Xi with itself, and the second part of
the summation corresponds to the Œª and (1 ‚àíŒª) components of mixing Xi with Xj. Discarding
the terms for which the coefÔ¨Åcients above are 0 (the associated Œ∏i terms are taken to be 0, as
anything else is suboptimal due to the summation constraint), we are left with a linear combination
of ‚àílog Œ∏i1, ‚àílog Œ∏i2, ..., ‚àílog Œ∏im, where the set {i1, i2, ..., im} is a subset of {1, ..., k}. If the
aforementioned set consists of only a single index i, then the unique optimizer is of course Œ∏i = 1.
On the other hand, if there are multiple Œ∏ip, we cannot minimize Jmix|Bœµ(x) by choosing any of
the Œ∏ip to be 0, and as a consequence we also cannot choose any of the Œ∏ip to be 1 since they are
constrained to add to 1."
REFERENCES,0.5903614457831325,"As such, we may consider each of the Œ∏ip ‚àà[Œ¥, 1 ‚àíŒ¥] for some Œ¥ > 0. With this consideration in
mind, Jmix|Bœµ(x) is strictly convex in terms of the Œ∏ip, since the Hessian of a linear combination
of ‚àílog Œ∏ip will be diagonal with positive entries when the arguments of the log terms are strictly
greater than 0 and less than 1. Thus, as [Œ¥, 1 ‚àíŒ¥]m is compact, there exists a unique solution to
hœµ(x) = argminŒ∏‚àà[0,1]k Jmix(Œ∏)|Bœµ(x), justifying the use of equality. This unique solution is easily
computed via Lagrange multipliers, and the solution is given in Equation 1."
REFERENCES,0.5927710843373494,"We have thus far deÔ¨Åned hœµ on Xmix, and it remains to check that this construction of hœµ corresponds
to the restriction of a continuous function from Rn ‚Üí[0, 1]k. To do so, we Ô¨Årst note that Xmix is
closed, since any limit point x‚Ä≤ of Xmix must necessarily either be contained in one of the supports
Xi (due to compactness) or on a line segment between/within two supports (else it has positive
distance from Xmix), and every œµ‚Ä≤-neighborhood of x‚Ä≤ contains points x for which S"
REFERENCES,0.5951807228915663,"i,j Ai,j
x,œµ has
positive measure for every œµ (immediately implying that S"
REFERENCES,0.5975903614457831,"i,j Ai,j
x‚Ä≤,œµ‚Ä≤ has positive measure)."
REFERENCES,0.6,"Now we can check that hœµ is continuous on Xmix as follows. Consider a sequence xm ‚Üíx‚àó; we
wish to show that hœµ(xm) ‚Üíhœµ(x‚àó). Since the codomain of hœµ is compact, the sequence hœµ(xm)
must have a limit point h‚àó. Furthermore, since each hœµ(xm) is the unique minimizer of Jmix|Bœµ(xm),
we must have that h‚àóis the unique minimizer of Jmix|Bœµ(x‚àó), implying that h‚àó= hœµ(x‚àó). We have
thus established the continuity of hœµ on Xmix, and since Xmix is closed (in fact, compact), we have
by the Tietze Extension Theorem that hœµ can be extended to a continuous function on all of Rn."
REFERENCES,0.6024096385542169,"Finally, if limœµ‚Üí0 hœµ(x) exists, then it is by deÔ¨Ånition h(x) for any h ‚ààC‚àóand therefore for any
h ‚ààargming‚ààC‚àóJmix(g, PX, Pf)."
REFERENCES,0.6048192771084338,"Corollary C.1 (Symmetric Version). If Pf is symmetric, then we may simplify hi
œµ to be:"
REFERENCES,0.6072289156626506,"hi
œµ(x) =
Œæi,i
x,œµ + 2 P
jÃ∏=i Œæi,j
x,œµ,Œª
Pk
q=1
 
Œæq,q
x,œµ + 2 P"
REFERENCES,0.6096385542168675,"jÃ∏=q Œæq,j
x,œµ,Œª
"
REFERENCES,0.6120481927710844,"Proof. In the symmetric case, we have Œæi,j
x,œµ,Œª = Œæj,i
x,œµ ‚àíŒæj,i
x,œµ,Œª."
REFERENCES,0.6144578313253012,"Proposition 2.5. Let X1 = {(0, 1), (0, ‚àí1)} and let X2 = {(1, 0), (‚àí1, 0)}, with PX being discrete
uniform over X1‚à™X2 and Pf being continuous uniform over [0, 1]. Then the Mixup-optimal classiÔ¨Åer
h is discontinuous at (0, 0)."
REFERENCES,0.6168674698795181,"Proof. Considering the point x = (0, 0), we note that only Œæ1,1
x,œµ and Œæ2,2
x,œµ are non-zero for all œµ > 0.
Furthermore, we have that Œæ1,1
x,œµ = Œæ2,2
x,œµ =
œµ
4 since the Pf-measure of Bœµ( 1"
REFERENCES,0.619277108433735,2) is 2œµ and the sets
REFERENCES,0.6216867469879518,"A1,1
x,œµ, A2,2
x,œµ have PX-measure 1"
REFERENCES,0.6240963855421687,"8. From this we have that the limit limœµ‚Üí0 hœµ(x) =
 1"
REFERENCES,0.6265060240963856,"2
1
2
‚ä§is the
Mixup-optimal value at x."
REFERENCES,0.6289156626506024,"On the other hand, every other point on the line segment connecting the points in X1 will have an
œµ-neighborhood disjoint from the line segment between the points in X2 (and vice versa), so we will
have h1
œµ‚Ä≤(x) = 1 for all œµ‚Ä≤ ‚â§œµ. This implies that hœµ‚Ä≤(x) ‚Üí[1 0] (and an identical result for points
on the X2 line segment), so we have that the Mixup-optimal classiÔ¨Åer is discontinuous at (0, 0) as
desired."
REFERENCES,0.6313253012048192,"Theorem 2.7. Let Pf have associated density Beta(Œ±, Œ±). Then for any classiÔ¨Åer hœµ on X 2
3 (as
deÔ¨Åned in Lemma 2.3), we may choose Œ± such that hœµ does not achieve 0 classiÔ¨Åcation error on X 2
3 ."
REFERENCES,0.6337349397590362,Published as a conference paper at ICLR 2022
REFERENCES,0.636144578313253,"Proof. Fix a classiÔ¨Åer hœµ as deÔ¨Åned in Lemma 2.3 on X 2
3 . Now for Y ‚àºBeta(Œ±, Œ±) we have by the
fact that Beta(Œ±, Œ±) is strictly subgaussian that P
 Y ‚àí1"
REFERENCES,0.6385542168674698,"2
 ‚â§œµ

‚â•1 ‚àí2 exp
 
‚àíœµ2/(2œÉ2)

where"
REFERENCES,0.6409638554216868,"œÉ2 =
1
4Œ±+2 is the variance of Beta(Œ±, Œ±). As a result, we can choose Œ± > 1"
REFERENCES,0.6433734939759036,"2

log 4"
REFERENCES,0.6457831325301204,"œµ2
‚àí1

to guarantee"
REFERENCES,0.6481927710843374,"that P
 Y ‚àí1"
REFERENCES,0.6506024096385542,"2
 ‚â§œµ

> 1"
REFERENCES,0.653012048192771,"2 and therefore that Œæ1,1
1,œµ > 1"
REFERENCES,0.655421686746988,"9 = Œæ2,2
1,œµ ."
REFERENCES,0.6578313253012048,"Now we have by Lemma 2.3 (or more precisely, Corollary C.1) that:"
REFERENCES,0.6602409638554216,"h1
œµ(1) =
Œæ1,1
1,œµ + 2Œæ1,2
1,œµ,Œª
Œæ1,1
1,œµ + 4Œæ1,2
1,œµ,Œª + Œæ2,2
1,œµ
> 1 2"
REFERENCES,0.6626506024096386,"Thus, we have shown that hœµ will classify the point 1 as class 1 despite it belonging to class 2."
REFERENCES,0.6650602409638554,"Proposition 2.8. Consider k-class classiÔ¨Åcation where the supports X1, ..., Xk are Ô¨Ånite and PX
corresponds to the discrete uniform distribution. Then for every h ‚ààargming‚ààC‚àóJmix(g, PX, Pf),
we have that hi(x) = 1 on Xi."
REFERENCES,0.6674698795180722,"Proof. The full proof is not much more than the proof sketch in the main body. For x ‚ààXi, we
have that Œæj,q
x,œµ ‚Üí0 and Œæj,q
x,œµ,Œª ‚Üí0 as œµ ‚Üí0 for every (j, q) Ã∏= (i, i), while Œæi,i
x,œµ ‚Üí
1
|
S"
REFERENCES,0.6698795180722892,"i Xi|
2 and"
REFERENCES,0.672289156626506,"Œæi,i
x,œµ,Œª ‚Üí
EPf [Œª]
|
S"
REFERENCES,0.6746987951807228,"i Xi|
2 . As a result, we have limœµ‚Üí0 hi
œµ(x) = 1 as desired."
REFERENCES,0.6771084337349398,"Theorem 2.10. We consider the same setting as Proposition 2.8 and further suppose that Assumption
2.9 is satisÔ¨Åed. Then for every h ‚ààargming‚ààC‚àóJmix(g, PX, Pf), we have that hi(x) = 1 on Xi and
that h is continuous on X."
REFERENCES,0.6795180722891566,Proof. Obtained as a corollary of Theorem 3.2.
REFERENCES,0.6819277108433734,"C.2
PROOF FOR THEOREM 2.11"
REFERENCES,0.6843373493975904,"Prior to proving Theorem 2.11, we Ô¨Årst introduce some additional notation as well as a lemma that
will be necessary for the proof."
REFERENCES,0.6867469879518072,"Notation: Throughout the following m corresponds to the number of data points being considered
(as in Theorem 2.11), and as a shorthand we use [m] to indicate {1, ..., m}. Additionally, for two
matrices A and B, we use the notation [A, B] to indicate the matrix formed by the concatenation of
the columns of B to A. We use ei to denote the i-th basis vector in Rm, and use e‚Ä≤
i to denote the i-th
basis vector in R(
m
2). Let A ‚ààR(
m
2)√óm be the ‚ÄúMixup matrix‚Äù where each row has two 1s and the
other entries are 0, representing a mixture of the two data points whose associated indices have a 1.
The
 m
2

rows enumerate all the possible mixings of the m data points. In this way, A is uniquely
deÔ¨Åned up to a permutation of rows. We can pick any representative as our A matrix, and prove the
following lemma."
REFERENCES,0.689156626506024,"Lemma C.2. Assume m > 6, and P ‚ààR(
m
2)√ó(
m
2) is a permutation matrix. If PA is not a
permutation of the columns of A, then the rank of [A, PA] is larger than m."
REFERENCES,0.691566265060241,Using this lemma we can prove Theorem 2.11.
REFERENCES,0.6939759036144578,"Theorem 2.11. Suppose {x1, ..., xm} with m ‚â•6 are sampled from X according to PX, and that
PX has a density. Then with probability 1, we can uniquely determine the points {x1, ..., xm} given
only the
 m
2

midpoints {xi,j}1‚â§i<j‚â§m."
REFERENCES,0.6963855421686747,"Proof. We only need to show that the set of samples that cannot be uniquely determined from their
midpoints has Lebesgue measure zero, since PX is absolutely continuous with respect to the Lebesgue
measure. It sufÔ¨Åces to show this for the Ô¨Årst entry (dimension) of the xi‚Äôs, as the result then follows
for all dimensions. Let {x‚Ä≤
i} be another sample of m points. For convenience, we group the Ô¨Årst
entries of the data points {xi}m
i=1 into a vector w‚àó‚ààRm, and similarly obtain w ‚ààRm from {x‚Ä≤
i}m
i=1.
Suppose w‚àó‚ààRm is not a permutation of w ‚ààRm but that they have the same set of Mixup points.
We only need to show that the set of such w‚àóhas measure zero."
REFERENCES,0.6987951807228916,Published as a conference paper at ICLR 2022
REFERENCES,0.7012048192771084,"Suppose A is a Mixup matrix and P is a permutation matrix. Suppose PA is not a permutation of the
columns in A. We would need Aw‚àó= PAw, which is equivalent to [A, PA][(w‚àó)‚ä§, ‚àíw‚ä§]‚ä§= 0
(here [(w‚àó)‚ä§, ‚àíw‚ä§]‚ä§indicates the vector resulting from concatenating ‚àíw to w‚àó). According to
Lemma C.2, we know the rank of [A, PA] is at least m + 1, which implies that the solution set of w‚àó
is at most m ‚àí1."
REFERENCES,0.7036144578313253,"So Ô¨Åxing A and P, the set of non-recoverable w‚àóhas measure zero. There are only a Ô¨Ånite number of
combinations of A and P. Thus, considering all of these A and PA, the full set of non-recoverable
w‚àóstill has measure zero."
REFERENCES,0.7060240963855422,"C.2.1
PROOF OF SUPPORTING LEMMA"
REFERENCES,0.708433734939759,"Lemma C.2. Assume m > 6, and P ‚ààR(
m
2)√ó(
m
2) is a permutation matrix. If PA is not a
permutation of the columns of A, then the rank of [A, PA] is larger than m."
REFERENCES,0.7108433734939759,"Proof. First, we show that both the ranks of A and PA are m. For all i ‚àà[m ‚àí1], deÔ¨Åne ui =
e1 + ei+1, and deÔ¨Åne um = e2 + e3. Note that these m vectors are all rows of A. The Ô¨Årst (m ‚àí1)
vectors {ui}m‚àí1
i=1 are linearly independent because each ui has a unique direction ei+1 that is not a
linear combination of any other vectors in {ui}m‚àí1
i=1 . Besides, we know that the span of {ui}m‚àí1
i=1 is a
subspace of {v ‚ààRm : Pm
i=2 v(i) = v(1)} where v(i) is the i-th entry of v. Therefore, um doesn‚Äôt
lie in the span of {ui}m‚àí1
i=1 , which implies that these n vectors {ui}n
i=1 are linearly independent. This
shows that A is at least rank m. Since A only has m columns, we know that the rank of A is m. The
matrix PA is also rank m because P is full rank."
REFERENCES,0.7132530120481928,"Therefore, to show that the rank of [A, PA] is larger than m, we only need to Ô¨Ånd a vector v that
lies in the column span of A but is not in the column span of PA. To do this, we need the following
claim:"
REFERENCES,0.7156626506024096,"Claim 1. There exists a row index subset I ‚äÜ[
 m
2

] with size (m ‚àí1) such that the sub-matrix
composed by the rows of A with indices in I has a column that is all-one vector, but every column of
the sub-matrix composed by the rows of PA with indices in I is not all-one vector."
REFERENCES,0.7180722891566265,"Proof of Claim 1. By the deÔ¨Ånition of A, each column of A has only (m ‚àí1) ones and the other
entries are zero. Therefore, the position of (m ‚àí1) ones in a column of A can uniquely determine
that column vector. Besides, for an index set I with size (m ‚àí1), the sub-matrix composed by the
rows of A with indices in I cannot have two columns that are both all-one vector. This is because
otherwise A will have duplicate rows, which contradicts the deÔ¨Ånition of A. Therefore, there are m
possible choices of I, each of which corresponds to the positions of all ones in a column of A."
REFERENCES,0.7204819277108434,"Assume by contradiction that for these m choices of I, there exist a column of the sub-matrix
composed by the rows of PA with indices in I that is all-one vector. Then these m choices of I also
correspond to the positions of all ones in a column of PA. This means that the columns of A and
PA are the same up to permutations, which contradicts the assumption of our theorem. This Ô¨Ånishes
the proof of this claim."
REFERENCES,0.7228915662650602,"Now deÔ¨Åne B1 as the sub-matrix composed by the rows of A with indices in I, and C1 as the
sub-matrix composed by the rows of PA with indices in I. Without loss of generality, suppose
I = [m ‚àí1], and suppose the Ô¨Årst column of B1 is all-one vector. Let u = ‚àíe1 + Pm
i=2 ei ‚ààRm,"
REFERENCES,0.7253012048192771,"we know that Au = 2 P(
m
2)
i=m e‚Ä≤
i, i.e., the Ô¨Årst (m ‚àí1) entries of Au are 0, and the other entries are
2. DeÔ¨Åne v ‚âúAu, we are going to show that v is not in the column span of PA. Let C2 be the
sub-matrix in PA consisting of the rows that are not in C1, then the following claim shows that C2
has full column rank."
REFERENCES,0.727710843373494,Claim 2. rank(C2) = m.
REFERENCES,0.7301204819277108,"Proof of Claim 2. In this proof, we will consider each column of C2 as a vertex. Since each row of
C2 has only two 1s, we view each row as an edge connecting the two columns which correspond to
the two 1s in that row. From the deÔ¨Ånition of A we know that the graph we constructed is a simple
undirected graph. Then we are going to show that we can select m ‚Äúedges‚Äù from C2 which are
linearly independent."
REFERENCES,0.7325301204819277,Published as a conference paper at ICLR 2022
REFERENCES,0.7349397590361446,"There are
 m
2

‚àí(m ‚àí1) edges in C2. From m > 6 we know that
 m
2

‚àí(m ‚àí1) > m2"
REFERENCES,0.7373493975903614,"4 , so
from Tur√°n‚Äôs theorem, C2 contains at least one triangle. Assume this triangle is (i, j, k), then we
select edges (i, j), (j, k) and (i, k) and deÔ¨Åne E3 = {i, j, k}. ‚àÄr ‚àà[m], r ‚â•3, we select an edge
connecting Er with [m] \ Er. Assume that edge is (s, t) where s ‚ààEr and t /‚ààEr, we then add t to
Er, i.e., Er+1 = Er ‚à™{t}. In the next two paragraphs, we are going to show that there are always
edges between Er and [m] \ Er (so we can successfully select m edges in total), and the m edges we
selected are linearly independent."
REFERENCES,0.7397590361445783,"In matrix PA, there are r(m ‚àír) edges between Er and [m] \ Er. Since C2 is constructed by
deleting (m ‚àí1) edges from PA, the number of edges left between Er and [m] \ Er is at least
r(m ‚àír) ‚àí(m ‚àí1). When 3 ‚â§r ‚â§m ‚àí2, we have r(m ‚àír) ‚àí(m ‚àí1) > 0. When r = m ‚àí1,
the only case where there is no edge between Er and [m] \ Er is when all edges from vertex [m] \ Er
is in C1, which means that C1 has a column that is all-one vector and is a contradiction. Therefore,
there are always edges between Er and [m] \ Er and we can successfully select m edges."
REFERENCES,0.7421686746987952,"Then we only need to show that these m selected edges are linearly independent. We use {ui}m
i=1 to
denote the vectors that correspond to these edges, i.e., u1 = ei + ej, u2 = ej + ek, u3 = ei + ek, ¬∑ ¬∑ ¬∑
Assume by contradiction that they are linearly independent, then there exists x ‚ààRm, x Ã∏= 0 such
that Pm
i=1 x(i)ui = 0. By the selection process of the edges, we know that ‚àÄr ‚â•4, ur has a unique
direction, so x(r) = 0. Therefore, x(1)u1 + x(2)u2 + x(3)u3 = 0. Since {u1, u2, u3} are linearly
independent, we have x = 0, which is a contradiction. Thus, these m selected edges are linearly
independent, proving the claim."
REFERENCES,0.744578313253012,"Now assume by contradiction that v is in the column span of PA, then ‚àÉw ‚ààRm such that v = PAw.
Let v2 ‚ààR(
m
2)‚àí(m‚àí1) be the bottom
 m
2

‚àí(m ‚àí1) entries of v, then v2 = C2w. DeÔ¨Åne w0 to be
the all-one vector in Rm, we know that w0 is a valid solution to v2 = C2w. Since C2 has full column
rank, w0 must be the unique solution to v2 = C2w. This implies that v = PAw0. However, we"
REFERENCES,0.7469879518072289,"know that PAw0 = 2 P(
m
2)
i=1 e‚Ä≤
i Ã∏= v, which is a contradiction. Thus, v is not in the column span of A,
which Ô¨Ånishes the proof of the lemma."
REFERENCES,0.7493975903614458,"D
ADDITIONAL EXPERIMENTS FOR SECTION 2"
REFERENCES,0.7518072289156627,"D.1
EXPERIMENTS FOR SECTION 2.3"
REFERENCES,0.7542168674698795,"As noted in the main paper, it is not difÔ¨Åcult to extend DeÔ¨Ånition 2.6 to construct datasets on which
Mixup training empirically fails for small values of Œ±. The observation to make is that a major part of
the proof of Theorem 2.7 is the fact that the same-point mixing probability of point 1 is small relative
to the same-class mixing probability of class 0. As the former probability decreases quadratically
with the dataset size m, we are motivated to consider extensions of X 2
3 consisting of more points and
more classes. Towards that end, we consider the following generalization of X 2
3 :"
REFERENCES,0.7566265060240964,"DeÔ¨Ånition D.1 (m-Point k-Class Alternating Line). We deÔ¨Åne X k
m to be the k-class classiÔ¨Åcation
dataset consisting of the points {0, 1, ..., m ‚àí1} classiÔ¨Åed according to their value mod k incremented
by 1. As before, PX is the normalized counting measure on X k
m."
REFERENCES,0.7590361445783133,"We now consider training a two-layer feedforward network on X 2
10 and X 10
10 using the same procedure
and hyperparameters as in the main paper. The results are shown in Figure 8 below."
REFERENCES,0.7614457831325301,"Here we see that even at Œ± = 1, Mixup training fails to minimize the original empirical risk on
X 2
10 and X 10
10 , whereas in the main paper we noted that at Œ± = 1 Mixup training had no issues
minimizing the original risk on X 2
3 . Interestingly, we Ô¨Ånd that even standard training does not
completely minimize the original empirical risk on X 2
10, once again perhaps a result of the regularity
of the two-layer network model (although this is merely a hypothesis, and we do not analyze this
phenomenon further here)."
REFERENCES,0.763855421686747,"D.2
EXPERIMENTS FOR SECTION 2.4"
REFERENCES,0.7662650602409639,"Here we include the additional experiments mentioned in Section 2.4, namely the results of training
ResNet-18 on MNIST, CIFAR-10, and CIFAR-100 with and without Mixup for Œ± = 1, 32, and 128
(with the other hyperparameters being as described in Section 2.4). The results are shown in Figure 9."
REFERENCES,0.7686746987951807,Published as a conference paper at ICLR 2022
REFERENCES,0.7710843373493976,"(a) X 2
10, Œ± = 1
(b) X 2
10, Œ± = 32"
REFERENCES,0.7734939759036145,"(c) X 10
10 , Œ± = 1
(d) X 10
10 , Œ± = 32"
REFERENCES,0.7759036144578313,"Figure 8: Training error plots for Mixup and regular training on X 2
10 and X 10
10 . Each curve corresponds
to the mean of 10 training runs, and the area around each curve represents a region of one standard
deviation. Note that the ERM curves appear slightly different across Œ± values due to changes in
y-axis scale."
REFERENCES,0.7783132530120482,"As mentioned in the main body, these choices of Œ± only lead to Mixup performing more similarly to
ERM than Œ± = 1024."
REFERENCES,0.7807228915662651,"E
FULL PROOFS FOR SECTION 3"
REFERENCES,0.7831325301204819,"E.1
PROOF OF THEOREM 3.2"
REFERENCES,0.7855421686746988,"Theorem 3.2. Consider k-class classiÔ¨Åcation where the supports X1, ..., Xk are Ô¨Ånite and PX
corresponds to the discrete uniform distribution. If a point x satisÔ¨Åes Assumption 3.1 with respect to
a class i, then for every h ‚ààargming‚ààC‚àóJmix(g, PX, Pf), we have that h classiÔ¨Åes x as class i and
that h is continuous at x."
REFERENCES,0.7879518072289157,"Proof. For two points p, q in the supports X1, ..., Xk with a line segment between them intersecting
x, let Œª(p, q, x) denote the value of Œª for which Œªp + (1 ‚àíŒª)q = x. Since the supports are Ô¨Ånite, we
have that there exists œµ1 > 0 such that for all œµ ‚â§œµ1 we have that:"
REFERENCES,0.7903614457831325,"Œæi,j
x,œµ =
X p X q Z"
REFERENCES,0.7927710843373494,"Bœµ(Œª(p,q,x))
dPf"
REFERENCES,0.7951807228915663,"Where the summations are over all points p, q with line segments containing x. Now we have by
the Lebesgue differentiation theorem applied to the integral term in the summations above (this"
REFERENCES,0.7975903614457831,Published as a conference paper at ICLR 2022
REFERENCES,0.8,"(a) MNIST (Œ± = 1)
(b) CIFAR-10 (Œ± = 1)
(c) CIFAR-100 (Œ± = 1)"
REFERENCES,0.8024096385542169,"(d) MNIST (Œ± = 32)
(e) CIFAR-10 (Œ± = 32)
(f) CIFAR-100 (Œ± = 32)"
REFERENCES,0.8048192771084337,"(g) MNIST (Œ± = 128)
(h) CIFAR-10 (Œ± = 128)
(i) CIFAR-100 (Œ± = 128)"
REFERENCES,0.8072289156626506,"Figure 9: Mean and single standard deviation of 5 training runs for Mixup (Œ± = 1, 32, 128) and ERM
on the original training data."
REFERENCES,0.8096385542168675,"is possible because Pf has a density) that the following limit (as well as each limit for the other
coordinate functions) exists:"
REFERENCES,0.8120481927710843,"lim
œµ‚Üí0 hi
œµ = lim
œµ‚Üí0
Œæi,i
x,œµ + P"
REFERENCES,0.8144578313253013,"jÃ∏=i
 
Œæi,j
x,œµ,Œª + (Œæj,i
x,œµ ‚àíŒæj,i
x,œµ,Œª)
"
REFERENCES,0.8168674698795181,"Pk
q=1"
REFERENCES,0.8192771084337349,"
Œæq,q
x,œµ + P"
REFERENCES,0.8216867469879519,"jÃ∏=q
 
Œæq,j
x,œµ,Œª + (Œæj,q
x,œµ ‚àíŒæj,q
x,œµ,Œª)
"
REFERENCES,0.8240963855421687,"= lim
œµ‚Üí0"
REFERENCES,0.8265060240963855,"1
œµ Œæi,i
x,œµ + 1"
REFERENCES,0.8289156626506025,"œµ
P
jÃ∏=i
 
Œæi,j
x,œµ,Œª + (Œæj,i
x,œµ ‚àíŒæj,i
x,œµ,Œª)
"
REFERENCES,0.8313253012048193,"1
œµ
Pk
q=1"
REFERENCES,0.8337349397590361,"
Œæq,q
x,œµ + P"
REFERENCES,0.8361445783132531,"jÃ∏=q
 
Œæq,j
x,œµ,Œª + (Œæj,q
x,œµ ‚àíŒæj,q
x,œµ,Œª)
"
REFERENCES,0.8385542168674699,"Now by Assumption 3.1, we need only consider the Œæi,i
x,œµ, Œæi,j
x,œµ, Œæi,j
x,œµ,Œª terms (as there are no other line
segments that contain x). Furthermore, we have that there exists œµ2 > 0 such that for all œµ ‚â§œµ2 we
have Œª(p, q, x) ‚àíœµ ‚â•1"
REFERENCES,0.8409638554216867,"2 for p ‚ààXi and q ‚ààXj, and that the measure of Ai,j
x,œµ is at least that of Aj,i
x,œµ.
From this we get that for all œµ ‚â§min(œµ1, œµ2):"
REFERENCES,0.8433734939759037,"hi
œµ =
Œæi,i
x,œµ + P"
REFERENCES,0.8457831325301205,"jÃ∏=i
 
Œæi,j
x,œµ,Œª + (Œæj,i
x,œµ ‚àíŒæj,i
x,œµ,Œª)
"
REFERENCES,0.8481927710843373,"Œæi,i
x,œµ + P"
REFERENCES,0.8506024096385543,"jÃ∏=i
 
Œæi,j
x,œµ,Œª + Œæj,i
x,œµ,Œª + (Œæj,i
x,œµ ‚àíŒæj,i
x,œµ,Œª) + (Œæi,j
x,œµ ‚àíŒæi,j
x,œµ,Œª)
"
REFERENCES,0.8530120481927711,"‚â•
Œæi,i
x,œµ + P
jÃ∏=i
 
Œæi,j
x,œµ,Œª + (Œæj,i
x,œµ ‚àíŒæj,i
x,œµ,Œª)
"
REFERENCES,0.8554216867469879,"Œæi,i
x,œµ + 2 P"
REFERENCES,0.8578313253012049,"jÃ∏=i
 
Œæi,j
x,œµ,Œª + (Œæj,i
x,œµ ‚àíŒæj,i
x,œµ,Œª)
 ‚â•1 2"
REFERENCES,0.8602409638554217,Published as a conference paper at ICLR 2022
REFERENCES,0.8626506024096385,"Since the above also holds for a sufÔ¨Åciently small neighborhood about x, we have the desired result."
REFERENCES,0.8650602409638555,"E.2
PROOF OF THEOREM 3.5"
REFERENCES,0.8674698795180723,"Let us Ô¨Årst recall the setting of Theorem 3.5, since it is more specialized than that of the previous
results."
REFERENCES,0.8698795180722891,"Setting. We consider the case of binary classiÔ¨Åcation using a linear model Œ∏‚ä§x on high-dimensional
Gaussian data, which is a setting that arises naturally when training using Gaussian kernels. SpeciÔ¨Å-
cally, we consider the dataset X to consist of n points in Rd distributed according to N(0, Id) with
d > n (to be made more precise shortly). We let the labels of points in X be ¬±1 (so that the sign of
Œ∏‚ä§x is the classiÔ¨Åcation), and use X1 and X‚àí1 to denote the individual class points. Additionally,
we let n1 = |X1| and n2 = |X‚àí1|."
REFERENCES,0.8722891566265061,"Before introducing the proof of Theorem 3.5, we Ô¨Årst present a lemma that will be necessary in the
proof.
Lemma E.1. [Strict Convexity of Jmix on Data Span] Suppose n1 = n2 = 1, i.e. there are two data
points x, z with opposite labels. If x and z are linearly independent, then Jmix is strictly convex with
respect to Œ∏ on the span of x and z."
REFERENCES,0.8746987951807229,"The reason this lemma focuses on the two data point case is that in the proof of Theorem 3.5 we will
break up the Mixup loss into the sum over these cases. With this lemma, we may now prove Theorem
3.5. Before doing so, we point out that the version of Jmix considered here is after composition with
the logistic loss (since we are considering binary classiÔ¨Åcation with a linear classiÔ¨Åer).
Theorem 3.5. If the maximum margin solution for X is also an interpolating solution for X, then any
Œ∏ that lies in the span of X and minimizes the Mixup loss Jmix for a symmetric mixing distribution
Pf is a rescaling of the maximum margin solution."
REFERENCES,0.8771084337349397,"Proof. Since d > n, we have that all of the points {xi}n1
i=1, {zj}n2
j=1 are linearly independent
with probability one. We will break the proof into two parts. In doing so, we make the following
important observation: it sufÔ¨Åces to prove the result for mixings of distinct points. This is because
an interpolating solution (as given in DeÔ¨Ånition 3.3) is immediately seen to be optimal for the ERM
part of Jmix (the terms corresponding to mixing points with themselves). Thus, in what follows, we
disclude these ERM terms from Jmix to simplify the presentation."
REFERENCES,0.8795180722891566,"Part I: n1 = n2 = 1. Denote Œ∏‚ä§x1 = u and Œ∏‚ä§z1 = ‚àív, then"
REFERENCES,0.8819277108433735,"Jmix = EŒª [Œª log(1 + exp(‚àíŒªu + (1 ‚àíŒª)v)) + (1 ‚àíŒª) log(1 + exp(Œªu ‚àí(1 ‚àíŒª)v))] .
(2)"
REFERENCES,0.8843373493975903,"Where Œª is distributed according to Pf which is symmetric and has full support on [0, 1]. Therefore,
we can do the change of variables Œª := 1 ‚àíŒª, and"
REFERENCES,0.8867469879518072,"Jmix = EŒª [Œª log(1 + exp(‚àíŒªv + (1 ‚àíŒª)u)) + (1 ‚àíŒª) log(1 + exp(Œªv ‚àí(1 ‚àíŒª)u))] .
(3)"
REFERENCES,0.8891566265060241,"Combining Eq.(2) and Eq.(3), we know if (u, ‚àív) is a global minimum of Jmix, then so is (v, ‚àíu).
But the strict convexity in Lemma E.1 implies such a global minimum is unique, so we must have
u = v = k(Pf), where k(Pf) is a constant that only depends on the density of Pf. Furthermore,
‚àÄŒª ‚àà[0, 1], deÔ¨Åne"
REFERENCES,0.891566265060241,"hŒª(k) = Œª log(1 + exp((1 ‚àí2Œª)k)) + (1 ‚àíŒª) log(1 + exp((2Œª ‚àí1)k)),
(4)"
REFERENCES,0.8939759036144578,"then ‚àÄk > 0,"
REFERENCES,0.8963855421686747,hŒª(‚àík) ‚àíhŒª(k) = (1 ‚àí2Œª) log(1 + exp((1 ‚àí2Œª)k)) + (2Œª ‚àí1) log(1 + exp((2Œª ‚àí1)k))
REFERENCES,0.8987951807228916,"= (1 ‚àí2Œª) log
1 + exp((1 ‚àí2Œª)k)"
REFERENCES,0.9012048192771084,1 + exp((2Œª ‚àí1)k)
REFERENCES,0.9036144578313253,"
‚â•0,
(5) and"
REFERENCES,0.9060240963855422,"h‚Ä≤
Œª(k)|k=0 = ‚àí1"
REFERENCES,0.908433734939759,"2(1 ‚àí2Œª)2 ‚â§0.
(6)"
REFERENCES,0.9108433734939759,Published as a conference paper at ICLR 2022
REFERENCES,0.9132530120481928,"Hence, we must have k(Pf) > 0."
REFERENCES,0.9156626506024096,"Part II: General n1 and n2. For the general case, we extend the observation we made prior to the
proof of Part I. Namely, if we can show that an interpolating solution is optimal for mixing across the
two classes, it follows immediately that the solution is optimal for all of Jmix (it is not hard to see
that the calculation for mixing points from the same class is essentially no different from the ERM
case in this context, as the Œª and 1 ‚àíŒª terms can be combined). We thus focus only on mixing across
classes, and overload the Ji,j
mix notation to indicate mixing of points xi and zj, so that we may write
the loss in consideration as:"
REFERENCES,0.9180722891566265,"Jmix(Œ∏) =
1
n1n2 n1
X i=1 n2
X"
REFERENCES,0.9204819277108434,"j=1
Ji,j
mix(Œ∏).
(7)"
REFERENCES,0.9228915662650602,"By the proof in the previous part we know if Ji,j
mix is minimized, then we must have Œ∏‚ä§xi = ‚àíŒ∏‚ä§zj =
k(Pf) > 0. On the other hand, if Ji,j
mix(Œ∏) are minimized simultaneously for all pairs (i, j), then
clearly Jmix(Œ∏) is also minimized. This is possible since the data points are linearly independent, so
there exists Œ∏ ‚ààRd, such that"
REFERENCES,0.9253012048192771,"Œ∏‚ä§xi = ‚àíŒ∏‚ä§zj = k(Pf) > 0
‚àÄi ‚àà[n1], ‚àÄj ‚àà[n2].
(8)"
REFERENCES,0.927710843373494,"Now we can conclude that any Œ∏ that minimizes the Mixup loss Jmix is an interpolating solution.
Restricting Œ∏ to the span of X Ô¨Ånishes the proof."
REFERENCES,0.9301204819277108,"E.2.1
PROOF OF SUPPORTING LEMMA"
REFERENCES,0.9325301204819277,"Lemma E.1. [Strict Convexity of Jmix on Data Span] Suppose n1 = n2 = 1, i.e. there are two data
points x, z with opposite labels. If x and z are linearly independent, then Jmix is strictly convex with
respect to Œ∏ on the span of x and z."
REFERENCES,0.9349397590361446,"Proof. We note again that it sufÔ¨Åces to prove the strict convexity with respect to only the mixings
of different points, as the ERM part is clearly strictly convex and the sum of two strictly convex
functions remains strictly convex. Denote"
REFERENCES,0.9373493975903614,"f(Œª) = Œªx + (1 ‚àíŒª)z,
(9)"
REFERENCES,0.9397590361445783,then Jmix can be expressed as
REFERENCES,0.9421686746987952,"Jmix(Œ∏) = EŒª

Œª log
 
1 + exp
 
‚àíŒ∏‚ä§f(Œª)

+ (1 ‚àíŒª) log
 
1 + exp

Œ∏‚ä§f(Œª)
	"
REFERENCES,0.944578313253012,"= EŒª

log
 
1 + exp
 
‚àíŒ∏‚ä§f(Œª)

+ (1 ‚àíŒª)Œ∏‚ä§f(Œª)

.
(10)"
REFERENCES,0.946987951807229,"Where again Œª ‚àºPf. Note that the second term in Eq.(10) is linear in Œ∏, hence the Hessian of Jmix
can be written as"
REFERENCES,0.9493975903614458,‚àá2Jmix(Œ∏) = EŒª
REFERENCES,0.9518072289156626,"""
exp
 
‚àíŒ∏‚ä§f(Œª)
"
REFERENCES,0.9542168674698795,"(1 + exp(‚àíŒ∏‚ä§f(Œª)))2 f(Œª)f(Œª)‚ä§
# (11)"
REFERENCES,0.9566265060240964,:= EŒªg(Œª).
REFERENCES,0.9590361445783132,"DeÔ¨Åne B := Span{x, z}. To show Jmix is strictly convex on B, it sufÔ¨Åces to show for every non-zero
vector a ‚ààB, we always have"
REFERENCES,0.9614457831325302,"a‚ä§‚àá2Jmix(Œ∏)a > 0.
(12)"
REFERENCES,0.963855421686747,"Note that g(Œª) is continuous w.r.t. Œª and that Pf has full support on [0, 1], it sufÔ¨Åces to show either"
REFERENCES,0.9662650602409638,"a‚ä§f(0)f(0)‚ä§a > 0 or a‚ä§f(1)f(1)‚ä§a > 0,
(13)"
REFERENCES,0.9686746987951808,which is equivalent to either
REFERENCES,0.9710843373493976,"a‚ä§x Ã∏= 0 or a‚ä§z Ã∏= 0.
(14)"
REFERENCES,0.9734939759036144,"This is obvious since a is a non-zero vector in B, and that x and z are linearly independent."
REFERENCES,0.9759036144578314,Published as a conference paper at ICLR 2022
REFERENCES,0.9783132530120482,"Figure 10: Decision boundary plots for Œ± = 32, 64 and a class separation of 0.5."
REFERENCES,0.980722891566265,"Figure 11: Decision boundary plots for Œ± = 128, 512 and a class separation of 0.5."
REFERENCES,0.983132530120482,Published as a conference paper at ICLR 2022
REFERENCES,0.9855421686746988,"F
ADDITIONAL EXPERIMENTS FOR SECTION 3"
REFERENCES,0.9879518072289156,"In this section, we consider different class separations and choices of the mixing parameter Œ± when
training on the two moons dataset, with all other experimental settings being the same as in Section
3.1."
REFERENCES,0.9903614457831326,"Upon decreasing the class separation to 0.1, we note that even standard training captures more of the
nonlinear aspects of the data, as was observed in the prior work of Pezeshki et al. (2020)."
REFERENCES,0.9927710843373494,"Figure 12: Decision boundary plots for Œ± = 1, 1024 and a class separation of 0.1."
REFERENCES,0.9951807228915662,"Figure 13: Decision boundary plots for Œ± = 32, 64 and a class separation of 0.1."
REFERENCES,0.9975903614457832,"Figure 14: Decision boundary plots for Œ± = 128, 512 and a class separation of 0.1."
