Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0037313432835820895,"Neural Machine Translation (NMT) models often use subword-level vocabular-
ies to deal with rare or unknown words. Although some studies have shown the
effectiveness of purely character-based models, these approaches have resulted
in highly expensive models in computational terms. In this work, we explore
the advantages of quasi character-level Transformers for low-resource NMT, as
well as their ability to mitigate the catastrophic forgetting problem.
We ﬁrst
present an empirical study on the effectiveness of these models as a function of
the size of the training set. As a result, we found that for data-poor environ-
ments, quasi character-level Transformers present a competitive advantage over
their large subword-level versions. Similarly, we study the generalization of this
phenomenon in different languages, domains, and neural architectures. Finally,
we conclude this work by studying the ability of these models to mitigate the ef-
fects of catastrophic forgetting in machine translation. Our work suggests that
quasi character-level Transformers have a competitive advantage in data-poor en-
vironments and, although they do not mitigate the catastrophic forgetting problem,
they greatly help to achieve greater consistency between domains."
INTRODUCTION,0.007462686567164179,"1
INTRODUCTION"
INTRODUCTION,0.011194029850746268,"Neural machine translation (NMT) has become the dominant paradigm in the ﬁeld of machine trans-
lation due to the impressive results achieved with encoder-decoder architectures (Sutskever et al.
(2014); Cho et al. (2014a); Wu et al. (2016); Vaswani et al. (2017))."
INTRODUCTION,0.014925373134328358,"Despite these advances, the translation of rare or unknown words has turned out to be a more difﬁcult
problem to tackle than was initially thought. For which reason, authors have proposed different
approaches to deal with this problem, which can be grouped into three categories: i) Models based on
character-level representations. ii) Hybrid NMT Models. iii) Models based on word segmentation."
INTRODUCTION,0.018656716417910446,"Character-based models are the direct approach as they can naturally deal with rare or unseen words.
However, these models have historically resulted in unsatisfactory results (Vilar et al. (2007); Neubig
et al. (2013)) or highly expensive models in computational terms (Luong & Manning (2016b)). To
balance the beneﬁts of word- and character-based models, Hybrid NMT models were invented (Lu-
ong & Manning (2016a)). The idea behind these models is to translate mainly at the word level and
only query character components for rare words when necessary. However, these models tend to be
a bit cumbersome as they generally need two models to do the back-off. Finally, word segmentation
approaches such as BPE (Sennrich et al. (2016)), Unigram (Kudo (2018)), or SentencePiece (Kudo
& Richardson (2018)), emerged to address many of the problems mentioned above, by allowing the
encoding of rare words using a ﬁxed-vocabulary of subwords."
INTRODUCTION,0.022388059701492536,"Despite the success of word segmentation models, it is still not clear how to determine the vocabulary
size. The most common strategy is to choose an arbitrarily large value (i.e. 32k), under the premise
that large vocabularies tend to work well (Gowda & May (2020)). Despite this heuristic, we think
that there must be a better way of choosing this value depending on the dataset properties."
INTRODUCTION,0.026119402985074626,"Motivated by these ideas, we decided to study whether NMT character-based models had any advan-
tages compared to models with large subword vocabularies (i.e 32k tokens). We theorize that since
words follow a Zipﬁan distribution, models with large vocabularies would require large datasets to
learn the long-tailed distribution of subwords. In contrast, character-based models would be able"
INTRODUCTION,0.029850746268656716,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.033582089552238806,"to learn it more easily in data-poor environments as their subword distribution is less pronounced
than the distribution for large vocabularies. One of the main criticisms of character-level models is
that they involve longer sequences and therefore, more long-term dependencies. To alleviate some
of these problems, we decided to study quasi character-level models, which are essentially small
subword-level vocabularies (i.e 100-400 tokens)."
INTRODUCTION,0.03731343283582089,"Furthermore, we decided to study if character-based models can mitigate the effects of catastrophic
forgetting based on the idea that, if this phenomenon depends on the vocabulary’s domain, when a
vocabulary is shared between domains, these effects should be mitigated."
INTRODUCTION,0.041044776119402986,The contributions of this paper are fourfold:
INTRODUCTION,0.04477611940298507,"• Quasi character-level Transformers seem to consistently outperform large subword-level
Transformers on low-resource environments. (Section 5.1.1)."
INTRODUCTION,0.048507462686567165,"• When LSTMs and CNNs are used, this phenomenon is still present but it is more sensitive
on the chosen hyperparameters. (Section 5.1.3)."
INTRODUCTION,0.05223880597014925,"• Quasi character-level Transformers appear to be more susceptible to catastrophic forgetting
than their large subword-level counterparts. (Section 5.2)."
INTRODUCTION,0.055970149253731345,"• However, quasi character-level Transformers achieved better cross-domain consistencies.
(Section 5.2)."
RELATED WORK,0.05970149253731343,"2
RELATED WORK"
RELATED WORK,0.06343283582089553,"Character-based models have been well-studied in the Natural Language Processing (NLP) ﬁeld
to deal with the open-vocabulary problem. One of the ﬁrst character-based models was proposed
by Vilar et al. (2007), who treated the source and target sentences as a string of letters. Similarly,
Neubig et al. (2013) viewed translation as a single transduction between character strings in the
source. However, despite the relevance of their contributions, their results were not satisfactory as
their models generally performed worse than their word-based counterparts."
RELATED WORK,0.06716417910447761,"Consequently, authors began to propose strategies to get the best of word- and character-level mod-
els. These approaches were mostly based on Hybrid NMT models or subword-level models with
vocabularies built through word segmentation techniques. Luong & Manning (2016a) achieved open
vocabulary NMT through hybrid models, which translate at the word level and consult the character
components only for rare words; and Sennrich et al. (2016) developed a model to segment words
based on the Byte-Pair encoding compression algorithm proposed by Gage (1994). Later, more
word segmentation models were proposed based on this idea, such as Unigram by Kudo (2018) or
SentencePiece by Kudo & Richardson (2018)."
RELATED WORK,0.0708955223880597,"Chung et al. (2016) demonstrated that a NMT model with a character-based decoder can outperform
NMT models with subword-level decoders. Similarly, Luong & Manning (2016b) and Costa-juss`a
& Fonollosa (2016) showed that competitive purely character-based NMT models were possible.
Despite their relative success, these models were extremely slow to train and at runtime."
RELATED WORK,0.07462686567164178,"Recurrent neural architectures have been the dominant neural architecture for NLP tasks (Cho et al.
(2014b); Wu et al. (2016); Sundermeyer et al. (2014)). However, in recent years we have seen
convolutional neural models outperform recurrent models on some NLP tasks. Gehring et al. (2017)
proposed a competitive sequence to sequence architecture based entirely on convolutional neural
networks, and Conneau et al. (2016) achieved state-of-the-art machine translation with a purely
character-level NMT model via deep convolutional stacks."
RELATED WORK,0.07835820895522388,"Recently, the attention mechanism (Bahdanau et al. (2015); Luong et al. (2015)) along with the
Transformer architecture by Vaswani et al. (2017), has led to signiﬁcant progress in the ﬁeld of NLP,
due to the parallelizable nature of the Transformer architectures and its ability to deal with long-term
dependencies."
RELATED WORK,0.08208955223880597,"This paper ends with a brief discussion on the ability of quasi character-based Transformers to
mitigate the catastrophic forgetting problem in machine translation. As far as we know, this is the
ﬁrst work to address this issue from the perspective of (quasi) character-level models, since most
of the existing alternatives are based on regularization (i.e. LwF by Li & Hoiem (2016), EWC"
RELATED WORK,0.08582089552238806,Under review as a conference paper at ICLR 2022
RELATED WORK,0.08955223880597014,"by Kirkpatrick et al. (2016)), dynamic architectures (Rusu et al. (2016); Draelos et al. (2016)) or
Complementary Learning Systems (CLS) and Memory Replay (MR) (Kemker & Kanan (2017);
Lopez-Paz & Ranzato (2017))."
NEURAL MACHINE TRANSLATION,0.09328358208955224,"3
NEURAL MACHINE TRANSLATION"
NEURAL ARCHITECTURES FOR MACHINE TRANSLATION,0.09701492537313433,"3.1
NEURAL ARCHITECTURES FOR MACHINE TRANSLATION"
NEURAL ARCHITECTURES FOR MACHINE TRANSLATION,0.10074626865671642,"The goal of any translation system is to transform an input sequence in a given language into an
output sequence in a target language."
NEURAL ARCHITECTURES FOR MACHINE TRANSLATION,0.1044776119402985,"Nowadays, this is usually done using neural models based on the encoder-decoder architecture;
also know as seq2seq models in the machine translation community (Sutskever et al. (2014)). The
encoder part transforms the input sequence into an internal representation, and then the decoder
transforms this internal representation into the output sequence."
NEURAL ARCHITECTURES FOR MACHINE TRANSLATION,0.10820895522388059,"In this work, we focus our study on the Transformer architecture, although recurrent and fully con-
volutional architectures are also explored for completeness."
NEURAL ARCHITECTURES FOR MACHINE TRANSLATION,0.11194029850746269,"Recurrent architectures (RNNs) were the ﬁrst to be successfully applied in an encoder-decoder setup
for machine translation. Even though there are many types of RNNs, most of them chain sequen-
tially a series of unit cells to process temporal sequences. We decided to use LSTMs (Hochreiter &
Schmidhuber (1997)) because their units cells are explicitly designed to deal with long-term depen-
dencies."
NEURAL ARCHITECTURES FOR MACHINE TRANSLATION,0.11567164179104478,"Convolution-based architectures (CNN) do not contain any recurrent elements. They can do this,
because the idea behind this architecture is that the convolutional ﬁlters can slide through the se-
quence of tokens, from beginning to end (Gehring et al. (2017)). Initially, the convolutional layers
were intended to operate on 2-dimensional images so, in order to apply them to text data, we have
to imagine the sequence of tokens as if it were a 1xm image, where the height of the ﬁlter is equal to
1, and the width equal to k. Being k the number of consecutive tokens that the ﬁlter can observe at a
given time."
NEURAL ARCHITECTURES FOR MACHINE TRANSLATION,0.11940298507462686,"Finally, we have Transformer architecture by Vaswani et al. (2017), a state-of-the-art architecture
based entirely on the concept of attention (Bahdanau et al. (2015); Luong et al. (2015)) to draw
global dependencies between the input and output. This architecture does not use any recurrence or
convolutional layer to process temporal sequences. Instead, it makes use of linear and normalization
layers, residual connections, attention mechanisms, and some tricks to encode temporal information."
THE OPEN VOCABULARY PROBLEM,0.12313432835820895,"3.2
THE OPEN VOCABULARY PROBLEM"
THE OPEN VOCABULARY PROBLEM,0.12686567164179105,"In the written language it is common to ﬁnd alternative spellings (i.e. color-colour) and typos (i.e
acknowledge-acknowlege) that slightly modify the spelling of a word but do not prevent us, the
humans, from knowing its meaning. However, if a model is using a word-level representation, it will
stop knowing a known word at the very ﬁrst moment that it is slightly modiﬁed and this modiﬁcation
is not in its vocabulary. Furthermore, it must be taken into account that most languages make use
of agglutination and compounding mechanisms to form new words, which in itself makes it very
inefﬁcient to build a vocabulary with all possible words of a language."
THE OPEN VOCABULARY PROBLEM,0.13059701492537312,"Because of this, researchers have proposed multiple approaches to deal with the open vocabulary
problem. These approaches can be mostly grouped into three categories: i) Character-based models
ii) Hybrid models iii) Subword-based models."
THE OPEN VOCABULARY PROBLEM,0.13432835820895522,"Character-based models contain the minimum possible vocabulary with which to form all possible
words. Consequently, these models can translate rare or even unseen words character-by-character,
but at the same time, these models tend to be much slower and harder to train than word-based
models. This is because they have to deal with longer sequences and therefore, longer long-term
dependencies."
THE OPEN VOCABULARY PROBLEM,0.13805970149253732,"Hybrid models tend to translate primarily at word-level but fall back to character-level translation
when a rare or unseen word appears."
THE OPEN VOCABULARY PROBLEM,0.1417910447761194,Under review as a conference paper at ICLR 2022
THE OPEN VOCABULARY PROBLEM,0.1455223880597015,"Finally, subword-based models are ﬁxed close-vocabularies that use subwords instead of full words.
These models allow us to represent infrequent or even unknown words as a sequence of subwords,
but without the performance and memory issues that typically imply character-based models."
THE OPEN VOCABULARY PROBLEM,0.14925373134328357,"In this work, we always work at the subword level but distinguishing two types of subword vocabu-
laries. The ﬁrst one is the quasi character-level vocabulary with 100-400 tokens, and the second one
is a large subword-level vocabulary with around 32,000 tokens."
THE OPEN VOCABULARY PROBLEM,0.15298507462686567,"To build our subword-level vocabularies we chose a word segmentation model known as Byte-Pair
Encoding (BPE) by Sennrich et al. (2016). We did also consider Unigram (Kudo (2018)) and Sen-
tencePiece (Kudo & Richardson (2018)) but eventually, we decided to use BPE due to its popularity.
The idea behind BPE is to start with a set of characters and iteratively extend it with the most
frequent n-grams pairs in the corpus."
EXPERIMENTAL SETUP,0.15671641791044777,"4
EXPERIMENTAL SETUP"
DATASETS,0.16044776119402984,"4.1
DATASETS"
DATASETS,0.16417910447761194,"The data used in this work comes mostly from the WMT tasks. In table 1 we ﬁnd all the datasets
used in this work."
DATASETS,0.16791044776119404,Table 1: Datasets partitions
DATASETS,0.17164179104477612,"Dataset
Train
Val
Test
Europarl v7 (ES-EN)
1.9M / 100k / 50k
5000
5000
Europarl v7 (DE-EN)
1,8M / 100k / 50k
5000
5000
Europarl v7 (CS-EN)
635k / 100k / 50k
5000
5000
CommonCrawl (ES-EN)
1.8M / 100k
5000
5000
SciELO (ES-EN)
575k / 120k
4961
4961
SciELO (PT-EN)
116k
3738
3738
NewsCommentary (DE-EN)
357k / 35k
5000
5000
IWLST’16 (DE-EN)
196k
993
1305
Multi30K (DE-EN)
29k
1014
1000"
DATASETS,0.17537313432835822,All the values in this Table indicate the number of sentences.
DATASETS,0.1791044776119403,"Europarl contains parallel sentences extracted from the European Parliament website; Common-
Crawl consists of parallel texts mined from the Common Crawl dataset; SciELO is a parallel corpus
made up of scientiﬁc publications for the biomedical domain (health and biological); NewCommen-
tary contains political and economic commentaries, crawled from the website Project Syndicate;
Multi30k is a small dataset from WMT 2016 multimodal task, also known as Flickr30k; and the
IWLST’16 dataset comes from the IWSLT 2016 TED talk translation task."
DATASETS,0.1828358208955224,"In addition to this, we created two additional versions for each training set, with 100k and 50k
sentences (randomly sampled).1 The validation and test sets were left untouched."
TRAINING DETAILS,0.1865671641791045,"4.2
TRAINING DETAILS"
TRAINING DETAILS,0.19029850746268656,"First, we normalized and tokenized each dataset with Sacremoses, a Python port of the Moses to-
kenizer by Koehn et al. (2007). Then, we created two vocabularies for each dataset with different
subword granularities. The ﬁrst one was made up of 32k subwords, while the other was limited
to 100-400 subwords (characters + common subwords)2. Both were built using FastBPE, a tool to
learn and apply BPE codes. The large subword-level vocabulary was achieved using 32k merge
operations, while the small subword-level vocabulary only had 64 merge operations."
TRAINING DETAILS,0.19402985074626866,"We used Fairseq v1.0.0a0 (Ott et al. (2019)) as our sequence modeling toolkit. For our experimenta-
tion, we use the following base architectures: LSTMModel, FConvModel and TransformerModel.3."
TRAINING DETAILS,0.19776119402985073,"1Later we added more versions (120k, 35k, etc) to perform additional experiments
2The exact amount of tokens depended on each dataset
3Long Short-Term Memory model (LSTMModel), Fully convolutional model (FConvModel), and the
Transformer model (TransformerModel)"
TRAINING DETAILS,0.20149253731343283,Under review as a conference paper at ICLR 2022
TRAINING DETAILS,0.20522388059701493,"In addition to this, we tried two versions of the Transformer architecture: the Standard Transformer
with 93M parameters and a smaller version with 28M parameters (see Table 2). Interestingly, both
versions performed quite similarly in terms of accuracy (±1 BLEU). Also, the smaller version was
notably faster than the standard version, so we end up using the smaller version for most of our
experimentation."
TRAINING DETAILS,0.208955223880597,"Regarding the comparison of the models, we tried to make the comparison as fair a possible by only
comparing models with a similar number of parameters (see Table 2). Similarly, we shared as many
of the training hyperparameters as possible in order to reduce training biases. Nonetheless, this will
be discussed in detail in see Section 5.1.3, as this comparison is tricky due to the impact of the
chosen hyperparameters on the ﬁnal result."
TRAINING DETAILS,0.2126865671641791,Table 2: Neural architectures
TRAINING DETAILS,0.21641791044776118,"Model
Parameters
Hyperparameters"
TRAINING DETAILS,0.22014925373134328,"LSTM
6.8M (S) — 27.8M (L)
3 layers / 256 emb / 256 hid / bidir+attn
Fully CNN
6.5M (S) — 27.4M (L)
6x(256, 3) (enc/dec)
Transformer small
4.1M (S) — 25.0M (L)
3 layers / 8 heads / 256 dim / 512 ffnn
Transformer standard
44.9M (S) — 92.7 (L)
6 layers / 8 heads / 512 dim / 2048 ffnn"
TRAINING DETAILS,0.22388059701492538,"Common hyperparamters: Loss function: CrossEntropy (without label smoothing).
Optimizer:
Adam (Kingma & Ba (2015)) or NAG. Batch size: 4096 tokens/batch, Clip-norm: 0.1. Maximum
epochs: 100 epochs with early stopping. Beam width: 5."
TRAINING DETAILS,0.22761194029850745,Note: S/L stands for Small/Large subword vocabulary
EVALUATION METRICS,0.23134328358208955,"4.3
EVALUATION METRICS"
EVALUATION METRICS,0.23507462686567165,"NMT models are generally evaluated qualitatively and quantitatively. The qualitative part is carried
out by humans, while the quantitative evaluation is carried out with automatic metrics such as BLEU
(Papineni et al. (2002)), chrF (Popovi´c (2015)), or TER (Snover et al. (2006))."
EVALUATION METRICS,0.23880597014925373,"These automatic metrics compute the quality of a model by comparing its output with a reference
translation written by a human. To do so, we computed the BLEU, chrF, and TER scores using
Sacrebleu (Post (2018)) with its default arguments.4"
RESULTS,0.24253731343283583,"5
RESULTS"
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.2462686567164179,"5.1
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS"
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.25,"In this section, we propose a series of experiments to demonstrate that under a series of conditions,
quasi character-based models can obtain signiﬁcant advantages over models based on large vocabu-
laries."
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.2537313432835821,"Words follow a Zipﬁan distribution, which implies that in a large enough corpus, the most frequent
word will occur twice as often as the second most frequent word, three times as often as the third
most frequent word, and so on. Starting from this idea, we theorized that models with large sub-
word vocabularies would require large datasets to learn the long-tailed distribution of subwords (see
Figure 1). In contrast, we thought that models with small subword vocabularies would learn their
subword distribution more easily in data-poor environments than their counterparts because since
the long-tail distribution of small subword-level vocabularies is shorter, the average relative fre-
quency per token is should be higher which would lead to more opportunities to learn each token
in a more homogeneous distribution. Similarly, we argue that the advantages of character-based
models should be lost when there is enough data from which to learn the low-frequent subwords of
the tail due to the issues related to the longer long-tail dependencies of character-based models."
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.2574626865671642,"4Even though we evaluated each model using BLEU, chrF, and TER, all of them behaved extremely simi-
larly, so we end up reporting just the BLEU metric for clarity."
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.26119402985074625,"Under review as a conference paper at ICLR 2022 the , . of to and in that is a for I on this be we are have it not as with which"
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.26492537313432835,European The @-@ will by has Mr -
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.26865671641791045,Commission an at would also
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.27238805970149255,should all
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.27611940298507465,President
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.2798507462686567,&apos;s from We our must
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.2835820895522388,&apos; but been Union
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.2873134328358209,Parliament you It can was
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.291044776119403,Member more or its
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.2947761194029851,report
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.29850746268656714,States
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.30223880597014924,Council their there do like very (
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.30597014925373134,Europe ) EU This these In they one
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.30970149253731344,countries so about what other us need who
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.31343283582089554,important : my
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.31716417910447764,people only time new
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.3208955223880597,policy
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.3246268656716418,because if no up ? am out now
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.3283582089552239,support such
ON THE EFFECTIVENESS OF CHARACTER-BASED MODELS,0.332089552238806,"0.00
500.00K"
M,0.3358208955223881,"1.00M
1.50M
2.00M
2.50M
3.00M
3.50M
4.00M
4.50M
5.00M"
M,0.33955223880597013,Frequency
M,0.34328358208955223,"Large subword vocab distribution (32k tokens) e a@@ e@@ i@@ c@@ t t@@ u@@ m@@ s@@ the p@@ o@@ b@@ d@@ l@@ r@@ s re@@ , g@@ w@@ f@@ n@@ y . er@@ th@@ of v@@ in@@ to ti@@ ou@@ h@@ ar@@ or@@ and en@@ is on@@ ing in ed an@@ is@@ es it@@ d h al@@ on a er om@@ at@@ si@@ al l ec@@ that ic@@ an k@@ men@@ r st@@ ro@@ ly ve ha@@ o ac@@ li@@ wh@@ pro@@ E@@ C@@ su@@ y@@ m es@@ en il@@ no@@ for I M@@ le@@ con@@ pe@@ Th@@ le ation us@@ de@@ ts pos@@ as P@@"
M,0.34701492537313433,"0.00
500.00K"
M,0.35074626865671643,"1.00M
1.50M
2.00M
2.50M
3.00M
3.50M
4.00M
4.50M
5.00M"
M,0.35447761194029853,Frequency
M,0.3582089552238806,Quasi character-level vocab distribution (350 tokens)
M,0.3619402985074627,"Figure 1: The frequency of the ﬁrst 100 tokens of a large subword vocabulary (32k tokens) and a
small subword vocabulary (350 tokens). The ﬁrst distribution has a much longer tail than the second
distribution, which translates to a lower average relative frequency per token (see ﬁrst Figure)."
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.3656716417910448,"5.1.1
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS"
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.3694029850746269,"To test our hypothesis, we created smaller versions of our datasets by limiting the amount of training
data to 100k and 50k sentences. As expected, we see in Figure 2 that as we limit the size of a
training set, the performance of quasi character-level Transformer increases compared to their large
subword-level versions. The right column corresponds to the full datasets for the Spanish-English,
German-English, and Czech-English pairs, while the middle and left columns correspond to the
same datasets but limited to 100k and 50k sentences. Our experimentation shows that for each
of the original datasets, the Transformers with large subword vocabularies outperform every quasi
character-level Transformer by around 5 points in BLEU. However, when we trained them with the
smaller versions of those datasets (middle and left columns), every quasi character-level Transformer
outperformed each of their large subword-level versions."
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.373134328358209,Europarl Large
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.376865671641791,(2M; es-en) 0 10 20 30 40 50 BLEU 44.5 39.6
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.3805970149253731,Europarl-2M (es-en)
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.3843283582089552,"Large subword vocab
Quasi char-level vocab"
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.3880597014925373,Europarl Small
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.3917910447761194,(100K; es-en) 0 10 20 30 40 50 BLEU
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.39552238805970147,"33.0
35.2"
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.39925373134328357,Europarl-100k (es-en)
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.40298507462686567,"Large subword vocab
Quasi char-level vocab"
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.40671641791044777,Europarl Small
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.41044776119402987,(50K; es-en) 0 10 20 30 40 50 BLEU 24.4 31.7
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.4141791044776119,Europarl-50k (es-en)
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.417910447761194,"Large subword vocab
Quasi char-level vocab"
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.4216417910447761,Europarl Large
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.4253731343283582,(2M; de-en) 0 10 20 30 40 50 BLEU 36.6 30.7
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.4291044776119403,Europarl-2M (de-en)
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.43283582089552236,"Large subword vocab
Quasi char-level vocab"
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.43656716417910446,Europarl Small
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.44029850746268656,(100K; de-en) 0 10 20 30 40 50 BLEU
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.44402985074626866,"24.0
26.4"
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.44776119402985076,Europarl-100k (de-en)
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.45149253731343286,"Large subword vocab
Quasi char-level vocab"
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.4552238805970149,Europarl Small
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.458955223880597,(50K; de-en) 0 10 20 30 40 50 BLEU 16.6 23.3
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.4626865671641791,Europarl-50k (de-en)
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.4664179104477612,"Large subword vocab
Quasi char-level vocab"
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.4701492537313433,Europarl Large
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.47388059701492535,(630K; cs-en) 0 10 20 30 40 50 BLEU 42.5 37.1
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.47761194029850745,Europarl-630k (cs-en)
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.48134328358208955,"Large subword vocab
Quasi char-level vocab"
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.48507462686567165,Europarl Small
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.48880597014925375,(100K; cs-en) 0 10 20 30 40 50 BLEU
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.4925373134328358,"31.0
33.2"
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.4962686567164179,Europarl-100k (cs-en)
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.5,"Large subword vocab
Quasi char-level vocab"
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.503731343283582,Europarl Small
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.5074626865671642,(50K; cs-en) 0 10 20 30 40 50 BLEU 22.6 28.6
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.5111940298507462,Europarl-50k (cs-en)
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.5149253731343284,"Large subword vocab
Quasi char-level vocab"
ADVANTAGES OF TRANSFORMERS FOR DATA-POOR ENVIRONMENTS,0.5186567164179104,"Figure 2: As we limit the training data (left to right), we see that quasi character-level Transformers
performs better than their large subword vocabularies versions. Similarly, this phenomenon seems
to occur regardless of the language (top to bottom)."
LANGUAGE AND DOMAIN GENERALIZATION,0.5223880597014925,"5.1.2
LANGUAGE AND DOMAIN GENERALIZATION"
LANGUAGE AND DOMAIN GENERALIZATION,0.5261194029850746,"Initially, we performed the previous experiment on the Spanish-English pair, but in order to account
for potential language biases, we decided to repeat the experiment but using different languages."
LANGUAGE AND DOMAIN GENERALIZATION,0.5298507462686567,Under review as a conference paper at ICLR 2022
LANGUAGE AND DOMAIN GENERALIZATION,0.5335820895522388,"Our go-to alternative language was German since it is well-known for its propensity for nominal
compounding and agglutination. Similarly, we added the Czech-English pair to our experimentation
as it keeps more distance with the other two pairs (see Figure 2), as well as other pairs for com-
pleteness (i.e. Portuguese-English, Spanish-English). This phenomenon was consistent across all
language pairs we tested, which could imply that this is a language-agnostic phenomenon. Nonethe-
less, we would like to repeat these experiments with more distant pairs (e.g. Chinese-Russian,
Japanese-Spanish, etc.) so that we could conﬁdently assert that this is indeed a language-agnostic
phenomenon."
LANGUAGE AND DOMAIN GENERALIZATION,0.5373134328358209,"Since all the previous datasets came from the Europarl domain, we wanted to explore whether this
phenomenon could be caused by domain bias. To do so, we repeated the same experiment but
on different domains, such as crawled data, political and economic news, health and biological
sciences, transcripted talks, and multimodal transcriptions. These domains came from the following
datasets: CommonCrawl, NewsCommentary, SciELO (health and biological datasets), IWLST’16,
and Multi30k. See Figure 3."
LANGUAGE AND DOMAIN GENERALIZATION,0.5410447761194029,CommonCrawl Large
LANGUAGE AND DOMAIN GENERALIZATION,0.5447761194029851,(1.8M; es-en)
LANGUAGE AND DOMAIN GENERALIZATION,0.5485074626865671,CommonCrawl Small
LANGUAGE AND DOMAIN GENERALIZATION,0.5522388059701493,(100K; es-en) 0 10 20 30 40 50 BLEU 30.7 15.6 26.5 22.6
LANGUAGE AND DOMAIN GENERALIZATION,0.5559701492537313,CommonCrawl
LANGUAGE AND DOMAIN GENERALIZATION,0.5597014925373134,"Large subword vocab
Quasi char-level vocab"
LANGUAGE AND DOMAIN GENERALIZATION,0.5634328358208955,Biomedical Large
LANGUAGE AND DOMAIN GENERALIZATION,0.5671641791044776,(570K; es-en)
LANGUAGE AND DOMAIN GENERALIZATION,0.5708955223880597,Biomedical Small
LANGUAGE AND DOMAIN GENERALIZATION,0.5746268656716418,(120K; es-en) 0 10 20 30 40 50 BLEU 35.6
LANGUAGE AND DOMAIN GENERALIZATION,0.5783582089552238,"28.7
35.2
33.3"
LANGUAGE AND DOMAIN GENERALIZATION,0.582089552238806,SciELO (Health domain)
LANGUAGE AND DOMAIN GENERALIZATION,0.585820895522388,"Large subword vocab
Quasi char-level vocab"
LANGUAGE AND DOMAIN GENERALIZATION,0.5895522388059702,NewsCommentary Large
LANGUAGE AND DOMAIN GENERALIZATION,0.5932835820895522,(360K; es-en)
LANGUAGE AND DOMAIN GENERALIZATION,0.5970149253731343,NewsCommentary Small
LANGUAGE AND DOMAIN GENERALIZATION,0.6007462686567164,(35K; es-en) 0 10 20 30 40 50 BLEU 45.4 21.5 42.6 30.1
LANGUAGE AND DOMAIN GENERALIZATION,0.6044776119402985,NewsCommentary
LANGUAGE AND DOMAIN GENERALIZATION,0.6082089552238806,"Large subword vocab
Quasi char-level vocab"
LANGUAGE AND DOMAIN GENERALIZATION,0.6119402985074627,"Figure 3: The advantages of quasi character-level Transformers in data-poor environments are con-
sistent when models are trained on different languages and domains. From left to right we have the
results from models trained on the CommonCrawl, SciELO, and NewCommentary datasets, which
belong to different domains. In each of those ﬁgures, we see that when the training data is lim-
ited (two columns on the left of each ﬁgure), quasi character-level models outperform their large
subword-level counterparts, regardless of the domain."
LANGUAGE AND DOMAIN GENERALIZATION,0.6156716417910447,"As we see in Figure 3, the phenomenon here studied is present when we change the training domain
to crawled data (CommonCrawl), scientiﬁc literature (SciELO), and, political and economic com-
mentaries (NewsCommentaries). In addition to these domains, we also experiment with transcripted
TED talks (IWLST’16) and image descriptions (Multi30k). As expected, quasi character-level mod-
els kept outperforming their large subword-level versions when we limited the training sets. They
improved the BLEU score in 6.2pts for the IWLST’16 dataset and 2.3pts for the Multi30k dataset."
NEURAL ARCHITECTURES GENERALIZATION,0.6194029850746269,"5.1.3
NEURAL ARCHITECTURES GENERALIZATION"
NEURAL ARCHITECTURES GENERALIZATION,0.6231343283582089,"As all the previous studies were done using the Transformer architecture, so we decided to study
whether the phenomenon described above generalizes to other neural architectures for machine
translation, such as LSTMs or CNNs. Speciﬁcally, we focused our research on bidirectional LSTMs
with attention mechanisms and fully convolutional architectures like the one described in Gehring
et al. (2017). To have a fairer comparison, we only compared models that had a similar number of
parameters for a given vocabulary (i.e. 25-30M parameters for vocabularies of 32k subwords)."
NEURAL ARCHITECTURES GENERALIZATION,0.6268656716417911,"Since the performance of a model does not depend solely on the number parameters but on its
architecture, how it was trained, etc. We decided to standardize the training hyperparameters we
used amongst all models as much as possible. In addition to this, we performed multiple runs for
each model with slight variations in their hyperparameters and initializations to account for this
performance variability."
NEURAL ARCHITECTURES GENERALIZATION,0.6305970149253731,"From our experimentation, we observed that as expected, when large subword-level models were
trained with enough data (unlimited training sets), they outperformed their quasi character-level
counterparts. This happened for all three architectures (LSTM, CNN, and Transformer). How-
ever, when the training sets were limited to 100k or 50k sentences, quasi character-based models
performed better than their large subword-level versions in most cases (see Figure 4)."
NEURAL ARCHITECTURES GENERALIZATION,0.6343283582089553,"When Transformers were used, the ones with quasi character-level vocabularies consistently out-
performed the ones with large subword-level vocabularies. In contrast, when LSTMs or CNNs,"
NEURAL ARCHITECTURES GENERALIZATION,0.6380597014925373,Under review as a conference paper at ICLR 2022
NEURAL ARCHITECTURES GENERALIZATION,0.6417910447761194,"their performance was more dependent on their hyperparameters. In the case of LSTMs, the quasi
character-level versions always end up outperforming the large subword vocabularies, but the im-
provements were not as impressive as the ones seen with the Transformer architecture. Finally, we
found CNN-based architectures to be particularly sensitive to the chosen hyperparameters, so the
advantages of quasi character-level CNNs are not as clear as to us compared to the other models.
Nevertheless, with enough computational time and the right hyperparameters, quasi character-CNNs
probably end up outperforming consistently their large subword counterparts."
NEURAL ARCHITECTURES GENERALIZATION,0.6455223880597015,"Figure 4: The green lines refer to the best and worst runs of the models with large subword-level
vocabularies, while the blue lines refer to the best run of the quasi character-level models. In the left
ﬁgure, we see that quasi character-level Transformers consistently outperform the ones with large
subword vocabularies. Then, in the middle ﬁgure we see that for the LSTMs, this phenomenon is not
as strong as with Transformers. Finally, in the right ﬁgure, we can see how quasi character-CNNs
do not present these advantages as clearly as in the other models."
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.6492537313432836,"5.2
EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.6529850746268657,"In this section, we study whether quasi character-based Transformers can help to mitigate the catas-
trophic forgetting problem, whereby neural networks forget previously learned information after
learning new information."
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.6567164179104478,"To do this, we have designed an experiment in which we train a model in a domain A and evaluate
it in domains A and B to establish the baselines. Next, we ﬁne-tune the model trained in domain
A with data from the new domain B, and then, it is evaluate it in domains A and B. In theory, the
model trained in domain A should perform well in the domain A, and poorly in the unseen domain
B. Similarly, after ﬁne-tune it on domain B, it should perform worse in A and better in domain B
than the original model trained only on domain A."
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.6604477611940298,"In Figure 5a we see that quasi character-level Transformer trained on the health domain (SciELO)
obtained a BLEU of 33.3pts on its domain (health) and a BLEU of 14.3pts in the other domain
(biological). Then, when we ﬁne-tune this model on the Biological domain (SciELO), the BLEU
obtained on this domain increased from 14.3 to 31.7pts, while BLEU for the health domain fell
from 33.3 to 21.0pts. In Figure5b we see that something similar happened for the large subword-
vocabulary, but the effects of the catastrophic forgetting problem were not as strong as in the other
model, because the BLEU score went from 28.7 to 28.0pts."
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.664179104477612,"Health
(Voc. domain=H)"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.667910447761194,"H
B
(Voc. domain=H) 0 20 40 BLEU 33.3"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.6716417910447762,"21.0
14.3 31.7"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.6753731343283582,Vocabulary size: ~350 tokens
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.6791044776119403,"Health
Biological"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.6828358208955224,(a) Quasi character-level vocabulary
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.6865671641791045,"Health
(Voc. domain=H)"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.6902985074626866,"H
B
(Voc. domain=H) 0 20 40 BLEU"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.6940298507462687,"28.7
28.0 16.0 23.0"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.6977611940298507,Vocabulary size: ~32k tokens
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7014925373134329,"Health
Biological"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7052238805970149,(b) Large subword-level vocabulary
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7089552238805971,"Figure 5: Vocabularies seem to have a strong impact on the catastrophic forgetting effects. While
the quasi character-level model lost 12.3pts (see Figure 5a), the large subword-level model only lost
0.7pts (see Figure 5b)"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7126865671641791,Under review as a conference paper at ICLR 2022
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7164179104477612,"From Figure 5, we can infer that vocabularies seem to have a strong impact on the effects of catas-
trophic forgetting because character-level vocabularies seem to make models more susceptible to the
catastrophic forgetting problem than large subword-level vocabularies."
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7201492537313433,"To further explore this problem, we repeated the previous experiment but taking into account the
vocabulary domain. As a result, we discovered that the vocabulary domain has a stronger impact
on the model’s performance than we thought. As shown in Figure 6, quasi character-level Trans-
formers present a high consistency between domains, while large subword-level Transformers seem
particularly sensitive to their vocabulary’s domain, to the point of achieving opposite results between
domains (see right column of the Figure 6)."
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7238805970149254,"Health
(Voc. domain=H)"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7276119402985075,"Biological
(Voc. domain=H)"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7313432835820896,"H
B
(Voc. domain=H) 0 10 20 30 40 50 BLEU 33.3"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7350746268656716,"18.7
21.0 14.3"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7388059701492538,"31.3
31.7"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7425373134328358,Voc size: ~350 tok | Voc. domain: Health
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.746268656716418,"Health
Biological"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.75,"Health
(Voc. domain=H)"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.753731343283582,"Biological
(Voc. domain=H)"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7574626865671642,"H
B
(Voc. domain=H) 0 10 20 30 40 50 BLEU 28.7 24.5 28.0 16.0"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7611940298507462,"21.3
23.0"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7649253731343284,Voc. size: ~32k tok | Voc. domain: Health
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7686567164179104,"Health
Biological"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7723880597014925,"Health
(Voc. domain=B)"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7761194029850746,"Biological
(Voc. domain=B)"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7798507462686567,"H
B
(Voc. domain=B) 0 10 20 30 40 50 BLEU 29.1"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7835820895522388,"14.6
16.2
20.3"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7873134328358209,"35.8
37.1"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7910447761194029,Voc size: ~350 tok | Voc. domain: Biological
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7947761194029851,"Health
Biological"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.7985074626865671,"Health
(Voc. domain=B)"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.8022388059701493,"Biological
(Voc. domain=B)"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.8059701492537313,"H
B
(Voc. domain=B) 0 10 20 30 40 50 BLEU"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.8097014925373134,"20.2
18.2
19.9
23.7"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.8134328358208955,"31.2
33.7"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.8171641791044776,Voc. size: ~32k tok | Voc. domain: Biological
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.8208955223880597,"Health
Biological"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.8246268656716418,"Figure 6: Transformers with quasi character-level vocabularies (left ﬁgures) appear to be more con-
sistent between domains than Transformers with large subword-level vocabularies (right ﬁgures)"
"EFFECTS OF QUASI CHARACTER-BASED TRANSFORMERS ON THE CATASTROPHIC
FORGETTING PROBLEM",0.8283582089552238,"Even though quasi character-level Transformers achieve better consistency across domains, they
suffer more severely the catastrophic forgetting problem than Transformers with large subword vo-
cabularies. We believe that by using specially designed regularization techniques to address this
issue such as LwF (Li & Hoiem (2016)) or EWC (Kirkpatrick et al. (2016)) these problems could
be mitigated, leading to more robust and consistent models."
CONCLUSION,0.832089552238806,"6
CONCLUSION"
CONCLUSION,0.835820895522388,"In this paper, we have empirically studied the effectiveness of quasi character-level models and their
ability to tackle the catastrophic forgetting problem."
CONCLUSION,0.8395522388059702,"Our studies reveal that for low-resource scenarios, quasi character-level Transformers are superior
in terms of accuracy compared to Transformers with large subword vocabularies. Although this
phenomenon seems to generalize with language and domain, it also appears to be dependent on
neural architecture. When LSTMs were used, the improvements were not as impressive as with the
Transformer architecture, but when we experimented with CNNs, we noticed that the outcome was
more dependent on the chosen hyperparameters than with the other models."
CONCLUSION,0.8432835820895522,"Finally, we showed that even though quasi character-level vocabularies do not seem to mitigate the
effects of the catastrophic forgetting problem, they achieved a higher consistency between domains,
which might lead to substantial improvements if regularization techniques are applied to deal with
catastrophic forgetting."
CONCLUSION,0.8470149253731343,ACKNOWLEDGMENT
CONCLUSION,0.8507462686567164,"Work supported by the Horizon 2020 - European Commission (H2020) under the SELENE project
(grant agreement no 871467) and the project Deep learning for adaptive and multimodal interac-
tion in pattern recognition (DeepPattern) (grant agreement PROMETEO/2019/121). We gratefully
acknowledge the support of NVIDIA Corporation with the donation of a GPU used for part of this
research."
CONCLUSION,0.8544776119402985,Under review as a conference paper at ICLR 2022
REFERENCES,0.8582089552238806,REFERENCES
REFERENCES,0.8619402985074627,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2015."
REFERENCES,0.8656716417910447,"Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties
of neural machine translation: Encoder–decoder approaches. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103–111. Association
for Computational Linguistics, 2014a."
REFERENCES,0.8694029850746269,"Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of the 2014 Conference on EMNLP, pp. 1724–
1734, 2014b."
REFERENCES,0.8731343283582089,"Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio. A character-level decoder without explicit
segmentation for neural machine translation. CoRR, abs/1603.06147, 2016."
REFERENCES,0.8768656716417911,"Alexis Conneau, Holger Schwenk, Lo¨ıc Barrault, and Yann LeCun. Very deep convolutional net-
works for natural language processing. CoRR, abs/1606.01781, 2016."
REFERENCES,0.8805970149253731,"Marta R. Costa-juss`a and Jos´e A. R. Fonollosa. Character-based neural machine translation. In
Proceedings of the 54th Annual Meeting of the ACL (Volume 2: Short Papers), pp. 357–361,
2016."
REFERENCES,0.8843283582089553,"Timothy J. Draelos, Nadine E. Miner, Christopher C. Lamb, Craig M. Vineyard, Kristofor D.
Carlson, Conrad D. James, and James B. Aimone.
Neurogenesis deep learning.
CoRR,
abs/1612.03770, 2016."
REFERENCES,0.8880597014925373,"Philip Gage. A new algorithm for data compression. C Users J., 12(2):23–38, 1994. ISSN 0898-
9788."
REFERENCES,0.8917910447761194,"Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional
sequence to sequence learning. In Proceedings of the 34th International Conference on Machine
Learning - Volume 70, ICML’17, pp. 1243–1252, 2017."
REFERENCES,0.8955223880597015,"Thamme Gowda and Jonathan May. Finding the optimal vocabulary size for neural machine trans-
lation. In Findings of the ACL: EMNLP 2020, pp. 3955–3964, 2020."
REFERENCES,0.8992537313432836,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9:1735–
80, 1997."
REFERENCES,0.9029850746268657,"Ronald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning.
CoRR, abs/1711.10563, 2017."
REFERENCES,0.9067164179104478,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015."
REFERENCES,0.9104477611940298,"James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic for-
getting in neural networks. CoRR, abs/1612.00796, 2016."
REFERENCES,0.914179104477612,"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar,
Alexandra Constantin, and Evan Herbst.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, ACL ’07, pp. 177–180, 2007."
REFERENCES,0.917910447761194,"Taku Kudo. Subword regularization: Improving neural network translation models with multiple
subword candidates. In Proceedings of the 56th Annual Meeting of the ACL (Volume 1: Long
Papers), pp. 66–75, 2018."
REFERENCES,0.9216417910447762,Under review as a conference paper at ICLR 2022
REFERENCES,0.9253731343283582,"Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing.
In Eduardo Blanco and Wei Lu (eds.),
Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 - November 4, 2018, pp.
66–71, 2018. doi: 10.18653/v1/d18-2012."
REFERENCES,0.9291044776119403,"Zhizhong Li and Derek Hoiem. Learning without forgetting. CoRR, abs/1606.09282, 2016."
REFERENCES,0.9328358208955224,"David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continuum learning.
CoRR, abs/1706.08840, 2017."
REFERENCES,0.9365671641791045,"Minh-Thang Luong and Christopher D. Manning. Achieving open vocabulary neural machine trans-
lation with hybrid word-character models. In Proceedings of the 54th Annual Meeting of the ACL
(Volume 1: Long Papers), pp. 1054–1063, 2016a."
REFERENCES,0.9402985074626866,"Minh-Thang Luong and Christopher D. Manning. Achieving open vocabulary neural machine trans-
lation with hybrid word-character models. In Proceedings of the 54th Annual Meeting of the ACL
(Volume 1: Long Papers), pp. 1054–1063, 2016b."
REFERENCES,0.9440298507462687,"Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based
neural machine translation. In Proceedings of the 2015 Conference on EMNLP, pp. 1412–1421,
2015."
REFERENCES,0.9477611940298507,"Graham Neubig, Taro Watanabe, Shinsuke Mori, and Tatsuya Kawahara. Substring-based machine
translation. Machine Translation, 27(2):139–166, 2013. ISSN 0922-6567."
REFERENCES,0.9514925373134329,"Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. CoRR, abs/1904.01038,
2019."
REFERENCES,0.9552238805970149,"Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting on ACL, ACL ’02,
pp. 311–318, 2002."
REFERENCES,0.9589552238805971,"Maja Popovi´c. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the
Tenth WMT, pp. 392–395, 2015."
REFERENCES,0.9626865671641791,"Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pp. 186–191, 2018."
REFERENCES,0.9664179104477612,"Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. CoRR,
abs/1606.04671, 2016."
REFERENCES,0.9701492537313433,"Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual Meeting of the ACL (Volume 1: Long Papers),
pp. 1715–1725, 2016."
REFERENCES,0.9738805970149254,"Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. A study of
translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the
AMTA: Technical Papers, pp. 223–231, 2006."
REFERENCES,0.9776119402985075,"Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker, and Hermann Ney. Translation modeling
with bidirectional recurrent neural networks. In Proceedings of the 2014 Conference on EMNLP,
pp. 14–25, 2014."
REFERENCES,0.9813432835820896,"Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger (eds.), NIPS,
volume 27, 2014."
REFERENCES,0.9850746268656716,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st NeurIPS,
NIPS’17, pp. 6000–6010, 2017. ISBN 9781510860964."
REFERENCES,0.9888059701492538,Under review as a conference paper at ICLR 2022
REFERENCES,0.9925373134328358,"David Vilar, Jan-T. Peter, and Hermann Ney. Can we translate letters? In Proceedings of the Second
WMT, StatMT ’07, pp. 33–39, 2007."
REFERENCES,0.996268656716418,"Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin John-
son, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa,
Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa,
Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural
machine translation system: Bridging the gap between human and machine translation. CoRR,
abs/1609.08144, 2016."
