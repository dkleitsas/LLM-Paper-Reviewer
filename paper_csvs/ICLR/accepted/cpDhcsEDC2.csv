Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002785515320334262,"Unsupervised large-scale vision-language pre-training has shown promising ad-
vances on various downstream tasks. Existing methods often model the cross-
modal interaction either via the similarity of the global feature of each modality
which misses sufÔ¨Åcient information, or Ô¨Åner-grained interactions using cross/self-
attention upon visual and textual tokens. However, cross/self-attention suffers
from inferior efÔ¨Åciency in both training and inference. In this paper, we intro-
duce a large-scale Fine-grained Interactive Language-Image Pre-training (FILIP)
to achieve Ô¨Åner-level alignment through a cross-modal late interaction mecha-
nism, which uses a token-wise maximum similarity between visual and textual
tokens to guide the contrastive objective. FILIP successfully leverages the Ô¨Åner-
grained expressiveness between image patches and textual words by modifying
only contrastive loss, while simultaneously gaining the ability to pre-compute im-
age and text representations ofÔ¨Çine at inference, keeping both large-scale training
and inference efÔ¨Åcient. Furthermore, we construct a new large-scale image-text
pair dataset called FILIP300M for pre-training. Experiments show that FILIP
achieves state-of-the-art performance on multiple downstream vision-language
tasks including zero-shot image classiÔ¨Åcation and image-text retrieval. The visu-
alization on word-patch alignment further shows that FILIP can learn meaningful
Ô¨Åne-grained features with promising localization ability."
INTRODUCTION,0.005571030640668524,"1
INTRODUCTION"
INTRODUCTION,0.008356545961002786,"Large-scale Vision-Language Pre-training (VLP) models like CLIP (Radford et al., 2021) and
ALIGN (Jia et al., 2021) have recently demonstrated success across various downstream tasks. They
learn visual and textual representations from millions of image-text pairs collected from the Internet
and show superior zero-shot ability and robustness. The core technique of these models lies in the
global contrastive alignment of the images and texts through a dual-stream model. Such architecture
is inference-efÔ¨Åcient for downstream tasks like retrieval because the encoders for the two modalities
can be decoupled and the image or text representations can be pre-computed ofÔ¨Çine. However, CLIP
and ALIGN model the cross-modal interaction via solely the similarity of the global feature of each
modality, lacking the ability of capturing Ô¨Åner-level information like the relationship between visual
objects and textual words. In this paper, we develop a simple yet efÔ¨Åcient cross-modal Ô¨Åner-grained
interaction mechanism for large-scale VLP."
INTRODUCTION,0.011142061281337047,"To achieve Ô¨Åner-grained cross-modal interaction, previous methods mainly exploited two kinds of
methods. (1) One line of work (Chen et al., 2020; Li et al., 2020b; Dong et al., 2021; Li et al., 2021b;
Zhang et al., 2021; Zhan et al., 2021) uses a pre-trained object detector to extract region-of-interest
(ROI) features from images, and then fuses it with the paired text through a VLP model. This design
complicates the pre-training due to pre-computing and storing a large number of ROI features. In
addition, the zero-shot ability of these approaches is usually limited by the predeÔ¨Åned number of
classes and their performance is also restricted by the quality of the detector. (2) Another line of
work (Li et al., 2021a; Kim et al., 2021) enforces the token-wise or patch-wise representations from"
INTRODUCTION,0.013927576601671309,"‚àóEqual contribution
‚Ä†Corresponding authors: xu.hang@huawei.com, xdliang328@gmail.com"
INTRODUCTION,0.016713091922005572,Published as a conference paper at ICLR 2022
INTRODUCTION,0.019498607242339833,"both modalities into the same space and models these Ô¨Åner-grained interactions via cross-attention
(Li et al., 2021a) or self-attention (Kim et al., 2021). However, these methods are usually less ef-
Ô¨Åcient in terms of both training and inference. In particular, during training, cross-attention in (Li
et al., 2021a) requires to be performed in an encoder-decoder structure, while the complexity of the
self-attention (Kim et al., 2021) grows quadratically with the length of the prolonged concatenated
sequences of both modalities. During inference, the data from both modalities are intertwined to
compute the cross-attention or self-attention, and can not be pre-computed ofÔ¨Çine as dual-stream
models like CLIP and ALIGN. This can be less efÔ¨Åcient for downstream tasks like image/text re-
trieval and image classiÔ¨Åcation."
INTRODUCTION,0.022284122562674095,"In this paper, we propose a large-scale Fine-grained Interactive Language-Image Pre-training frame-
work named FILIP to address these limitations. Inspired by Khattab & Zaharia (2020), we model
the Ô¨Åne-grained semantic alignment through a novel cross-modal late interaction mechanism in the
contrastive loss, instead of using cross or self-attention. SpeciÔ¨Åcally, our Ô¨Åne-grained contrastive
learning uses a token-wise maximum similarity between visual and textual tokens to guide the con-
trastive objective. In this way, FILIP successfully leverages the Ô¨Åner-grained expressiveness among
image patches and textual words while simultaneously gaining the ability to pre-compute image and
text representations ofÔ¨Çine. Unlike Khattab & Zaharia (2020), we discard the padded tokens and
use average instead summation of token-wise maximum similarities when computing the image-text
alignment, which enhances the cross-modal representation learning and stabilizes training. Further-
more, we construct a large-scale pre-training dataset named FILIP300M from the Internet. Data
cleaning and image-text data augmentation are also explored and proved useful in this work."
INTRODUCTION,0.025069637883008356,"Extensive experiments show that by effectively learning Ô¨Åne-grained representations, FILIP achieves
state-of-the-art performance on multiple downstream tasks, including zero-shot image classiÔ¨Åcation
and image-text retrieval. For example, FILIP reaches 77.1% top-1 accuracy for zero-shot ImageNet
classiÔ¨Åcation, surpassing CLIP with less training data. Visualizations on word-patch alignment
further show that FILIP learns meaningful Ô¨Åner-grained features with promising localization ability."
RELATED WORK,0.027855153203342618,"2
RELATED WORK"
RELATED WORK,0.03064066852367688,"Vision-Language Pre-training Models.
The pre-train-and-Ô¨Åne-tune scheme has achieved great
success in the domains of natural language processing (Devlin et al., 2019; Brown et al., 2020) and
computer vision (Dosovitskiy et al., 2020). It is then naturally extended to a joint cross-modal do-
main of Vision-and-Language Pre-training (VLP). The pre-training datasets of recent VLP models
include publically available datasets like YFCC100M (Thomee et al., 2016) and CC12M (Chang-
pinyo et al., 2021), as well as larger-scale datasets with more than 100M samples in CLIP (Radford
et al., 2021) and ALIGN (Jia et al., 2021), which are shown to be even more powerful. The pre-
training tasks of VLP models can be categorized into two categories: image-text contrastive learning
task and Language Modeling (LM) based tasks: (i) CLIP (Radford et al., 2021), ALIGN (Jia et al.,
2021) and UNIMO (Li et al., 2021b) make use of cross-modal contrastive learning which aligns
the textual and visual information into a uniÔ¨Åed semantic space; (ii) VisualBERT (Li et al., 2019),
UNITER (Chen et al., 2020), M6 (Lin et al., 2021), and DALL-E (Ramesh et al., 2021) employ
LM-like objectives, including both masked LM (e.g., Masked Language/Region Modeling), and au-
toregressive LM (e.g., image captioning, text-grounded image generation). On the other hand, some
methods rely on a pre-trained object detection model such as Faster-RCNN (Ren et al., 2015) to
extract image regional features ofÔ¨Çine, which requires extra labeled bounding-box data and makes
the approach less scalable. Recent efforts such as SOHO (Huang et al., 2021) and SimVLM (Wang
et al., 2021) try to eliminate this burden via visual dictionary or PreÔ¨ÅxLM (Raffel et al., 2020).
In this paper, we directly learn Ô¨Åne-grained vision-language representations in an end-to-end and
simpler manner while maintaining the beneÔ¨Åt of inference efÔ¨Åciency."
RELATED WORK,0.033426183844011144,"Multi-Modality Interaction Mechanism.
The core of vision-language pre-training models lies
in modeling the interaction between the two modalities. There are mainly two types of cross-modal
interaction architectures: Single-stream models like VisualBERT (Li et al., 2019) and ViLT (Kim
et al., 2021) directly concatenate the patch-wise or regional visual features and textual embeddings
and feed them to the transformer-based model. Dual-stream models such as ViLBERT (Lu et al.,
2019) and CLIP (Radford et al., 2021) have separate encoders for different modalities. This allows
Ô¨Çexible use of different models for different modalities, and efÔ¨Åcient inference for downstream tasks
like image-text retrieval, through the ability of decoupling the encoders and pre-compute image/text"
RELATED WORK,0.036211699164345405,Published as a conference paper at ICLR 2022
MEAN,0.03899721448467967,"2. Mean
1. Max"
MAX,0.04178272980501393,1. Max
MEAN,0.04456824512534819,2. Mean
MEAN,0.04735376044568245,"<BOS> 
cat
in
the
basin
<EOS> *"
MEAN,0.05013927576601671,Visual Token
MEAN,0.052924791086350974,Textual Token
MEAN,0.055710306406685235,Token Embedding
MEAN,0.0584958217270195,"Positional
Embedding"
MEAN,0.06128133704735376,Cross-modal Late Interaction
MEAN,0.06406685236768803,"Image Encoder
Text Encoder ùëõ! ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ùëõ!√óùëõ"" ùêº! ùêº"" ùêº#"
MEAN,0.06685236768802229,"ùëá! ùëá""
ùëá# ‚Ä¶ ‚Ä¶ ùë†$,& '
ùëá! ùëá"" ùëá#"
MEAN,0.06963788300835655,"ùêº!
ùêº""
ùêº# ‚Ä¶ ‚Ä¶ ùë†$,& ("
MEAN,0.07242339832869081,Image-to-text
MEAN,0.07520891364902507,Contrastive
MEAN,0.07799442896935933,Text-to-image
MEAN,0.0807799442896936,Contrastive A
MEAN,0.08356545961002786,Token-wise Similarity
MEAN,0.08635097493036212,"Image-to-text
Text-to-image"
MEAN,0.08913649025069638,"0
0
1
2
3
4
5
1
2
3
4
5
ùëõ"""
MEAN,0.09192200557103064,Linear Projection
MEAN,0.0947075208913649,"Figure 1: Overall architecture of FILIP, a dual-stream model with Transformer-based image and
text encoders. On top of the image and text encoders, the representations of textual tokens and
visual tokens are linearly projected to the multi-modal joint space. A novel Ô¨Åne-grained contrastive
learning equipped with cross-modal late interaction is proposed, which uses a token-wise maximum
similarity between visual and textual tokens."
MEAN,0.09749303621169916,"features ofÔ¨Çine. SCAN (Lee et al., 2018) considers latent alignments between image regions and
words. However, it is based on Triplet loss with a bottom-Up attention via a Faster-RCNN to extract
object features while we try to directly learn to localize Ô¨Åne-grained object from patches. In this
paper, while following the dual-stream approach for its Ô¨Çexible and efÔ¨Åcient inference, we further
propose a new multi-modal interaction mechanism to capture the Ô¨Åne-grained representations."
METHOD,0.10027855153203342,"3
METHOD"
METHOD,0.10306406685236769,"In this paper, we propose a new cross-modal pre-training model that excels in Ô¨Åne-grained interac-
tion between image encoder and text encoder for mining more detailed semantic alignment, named
as FILIP, as shown in Figure 1. Particularly, FILIP is a dual-stream model with Transformer-based
image and text encoders. For the visual modality, the image encoder is a Vision Transformer (Doso-
vitskiy et al., 2020) which takes the concatenation of an extra [CLS] token embedding and linearly
projected image patches as input. For the textual modality, following Radford et al. (2021), we use
the lower-cased byte pair encoding (BPE) (Sennrich et al., 2016b) with a vocabulary size of 49,408
to tokenize the text. Each text sequence starts with [BOS] token and ends with [EOS] token. After
the word embedding layer, the token embeddings are fed into a modiÔ¨Åed decoder-only Transformer
model as in (Radford et al., 2019). On top of the image and text encoders, the representations of
textual tokens and visual tokens are linearly projected to the multi-modal common space, and are
separately L2-normalized. Different from existing dual-stream models (e.g., CLIP and ALIGN)
which models cross-modal interaction via only the global features of the entire image and text se-
quence, we introduce a novel Ô¨Åne-grained contrastive learning objective equipped with cross-modal
late interaction which takes into account the Ô¨Åne-grained interaction between image patches and
textual tokens, detailed in Section 3.1."
FINE-GRAINED CONTRASTIVE LEARNING,0.10584958217270195,"3.1
FINE-GRAINED CONTRASTIVE LEARNING"
FINE-GRAINED CONTRASTIVE LEARNING,0.10863509749303621,"Contrastive representation learning has recently been found to learn better representations than its
predictive counterpart in both visual (Tian et al., 2020) and vision-language cross-modal pre-training
(Radford et al., 2021). Under a general formulation of cross-modal contrastive learning (Radford
et al., 2021), we want to learn encoders fŒ∏ for image data I and gœÜ for text data T such that, given
an image xI ‚ààI, and a text xT ‚ààT , the encoded representations fŒ∏(xI) and gœÜ(xT ) are close
if they are related and far apart if not, under a distance metric. In each training batch, we sample b
image-text pairs {xI
k, xT
k }b
k=1, For image xI
k in image-text pair {xI
k, xT
k }, xT
k is its positive, while"
FINE-GRAINED CONTRASTIVE LEARNING,0.11142061281337047,Published as a conference paper at ICLR 2022
FINE-GRAINED CONTRASTIVE LEARNING,0.11420612813370473,"the other texts will be used as in-batch negatives. The image-to-text contrastive loss LI
k for xI
k can
then be formulated as"
FINE-GRAINED CONTRASTIVE LEARNING,0.116991643454039,"LI
k(xI
k, {xT
j }b
j=1) = ‚àí1"
FINE-GRAINED CONTRASTIVE LEARNING,0.11977715877437325,"b log
exp(sI
k,k)
P"
FINE-GRAINED CONTRASTIVE LEARNING,0.12256267409470752,"j exp(sI
k,j),"
FINE-GRAINED CONTRASTIVE LEARNING,0.12534818941504178,"where sI
k,j denotes the similarity of the k-th image to the j-th text. Similarly, the text-to-image
contrastive loss for xT
k is"
FINE-GRAINED CONTRASTIVE LEARNING,0.12813370473537605,"LT
k (xT
k , {xI
j}b
j=1) = ‚àí1"
FINE-GRAINED CONTRASTIVE LEARNING,0.1309192200557103,"b log
exp(sT
k,k)
P"
FINE-GRAINED CONTRASTIVE LEARNING,0.13370473537604458,"j exp(sT
j,k)."
FINE-GRAINED CONTRASTIVE LEARNING,0.13649025069637882,The total loss of this mini-batch can be represented by L = 1 2 Xb
FINE-GRAINED CONTRASTIVE LEARNING,0.1392757660167131,"k=1(LI
k + LT
k ).
(1)"
CROSS-MODAL LATE INTERACTION,0.14206128133704735,"3.1.1
CROSS-MODAL LATE INTERACTION"
CROSS-MODAL LATE INTERACTION,0.14484679665738162,"From the contrastive loss (1), the cross-modal interaction is reÔ¨Çected in how we compute the sim-
ilarities sI
i,j and sT
i,j for the i-th image and j-th text. Previous methods like CLIP (Radford et al.,
2021) and ALIGN (Jia et al., 2021) simply encode each image or text separately to a global feature
i.e., fŒ∏(xI
i ) ‚ààRd and gœÜ(xT
j ) ‚ààRd, and compute these two similarities as"
CROSS-MODAL LATE INTERACTION,0.14763231197771587,"sI
i,j = sT
i,j = fŒ∏(xI
i )‚ä§gœÜ(xT
j ),
(2)"
CROSS-MODAL LATE INTERACTION,0.15041782729805014,"neglecting Ô¨Åner-grained interactions (e.g., word-patch alignment) between the two modalities. To
alleviate this problem, while simultaneously maintain the training and inference efÔ¨Åciency of dual-
stream models, we apply a cross-modal late interaction inspired by Khattab & Zaharia (2020) to
model the token-wise cross-modal interaction."
CROSS-MODAL LATE INTERACTION,0.1532033426183844,"SpeciÔ¨Åcally, denote n1 and n2 as the number of (non-padded) tokens of the i-th image and j-th text,
respectively, and the corresponding encoded features are fŒ∏(xI
i ) ‚ààRn1√ód and gœÜ(xT
j ) ‚ààRn2√ód.
For the k-th visual token, we compute its similarities with all textual tokens of xT
j , and use the
largest one
max
0‚â§r<n2[fŒ∏(xI
i )]‚ä§
k [gœÜ(xT
j )]r
(3)"
CROSS-MODAL LATE INTERACTION,0.15598885793871867,"as its token-wise maximum similarity with xT
j . We then use the average token-wise maximum
similarity of all non-padded tokens in the image (resp. text) as the similarity of an image to a text
(resp. a text to an image). The similarity of the i-th image to the j-th text can thus be formulated as:"
CROSS-MODAL LATE INTERACTION,0.15877437325905291,"sI
i,j(xI
i , xT
j ) = 1 n1 n1
X"
CROSS-MODAL LATE INTERACTION,0.1615598885793872,"k=1
[fŒ∏(xI
i )]‚ä§
k [gœÜ(xT
j )]mI
k,
(4)"
CROSS-MODAL LATE INTERACTION,0.16434540389972144,"where mI
k = arg max0‚â§r<n2[fŒ∏(xI
i )]‚ä§
k [gœÜ(xT
j )]r. Similarly, the similarity of the j-th text to the
i-th image is"
CROSS-MODAL LATE INTERACTION,0.1671309192200557,"sT
i,j(xI
i , xT
j ) = 1 n2 n2
X"
CROSS-MODAL LATE INTERACTION,0.16991643454038996,"k=1
[fŒ∏(xI
i )]‚ä§
mT
k [gœÜ(xT
j )]k,
(5)"
CROSS-MODAL LATE INTERACTION,0.17270194986072424,"where mT
k = arg max0‚â§r<n1[fŒ∏(xI
i )]‚ä§
r [gœÜ(xT
j )]k. Note that sI
i,j(xI
i , xT
j ) in Equation (4) does not
necessarily equal sT
i,j(xI
i , xT
j ) in Equation (5)."
CROSS-MODAL LATE INTERACTION,0.17548746518105848,"Remark 1 Intuitively, the token-wise maximum similarity in Equation (3) means that for each image
patch, we Ô¨Ånd its most similar textual token. Similarly, for each textual token, we also Ô¨Ånd its closest
image patch. By applying this to the similarity calculation in (4) and (5) for contrastive loss (1), the
dual-stream model learns Ô¨Åne-grained alignment between image patches and textual tokens."
CROSS-MODAL LATE INTERACTION,0.17827298050139276,"The original late interaction mechanism in (Khattab & Zaharia, 2020) computes the relevance score
of a document to a query padded with mask tokens, as a sum of token-wise maximum similari-
ties, and is optimized via a pairwise softmax cross-entropy loss. Though inspired from Khattab
& Zaharia (2020), our proposed cross-modal late interaction differs in several aspects. Firstly, we"
CROSS-MODAL LATE INTERACTION,0.181058495821727,Published as a conference paper at ICLR 2022
CROSS-MODAL LATE INTERACTION,0.18384401114206128,"exclude the padded textual tokens when computing the similarity, as they harm the performance.
We speculate that this is because these padded tokens also learn textual representations and will
mislead the model to align image patches to these meaningless padded tokens rather than mean-
ingful non-padded words. Secondly, when computing similarities (4) and (5), we use the average
of the token-wise maximum similarities instead of summation in (Khattab & Zaharia, 2020). This
is because the number of non-padded tokens varies from text to text, and this summation over all
non-padded tokens can have quite different magnitudes, leading to less stabilized training and worse
Ô¨Ånal performance. These two modiÔ¨Åcations are crucial to not only the downstream tasks‚Äô perfor-
mance, but also the quality of the word-patch alignment. A more detailed discussion can be found in
Appendix A.7. Thirdly, we optimize the late interaction mechanism via a contrastive loss (1) which
is found powerful vision-language pre-training (Radford et al., 2021) instead of the original pairwise
loss in (Khattab & Zaharia, 2020)."
CROSS-MODAL LATE INTERACTION,0.18662952646239556,"Training EfÔ¨Åciency. Though the cross-modal late interaction is able to capture Ô¨Åner-grained fea-
tures compared with the original loss, it relies on the token-wise representations of both modalities,
and can be inefÔ¨Åcient in terms of communication, memory and computation, especially when the
batch size is large. To alleviate this problem, we utilize several methods. Firstly, we reduce the
embedding size to 256. Besides, we reduce the precision of the last-layer features of both modalities
from fp32 to fp16 before node communication in a distributed learning setting, and perform the mul-
tiplication in Equations (4) and (5) under the reduced precision. In addition, since the complexity of
similarity calculation scales with the sequence length of textual tokens and image patches, for each
image (resp. text), we select the 25% tokens with the highest token-wise maximum similarity score
(Equation (3)) among all texts (resp. images) in the same local worker before node communica-
tion, based on the intuition that each sample can be represented by a few of the most representative
tokens. Effects of these modiÔ¨Åcations are studied in Section 4.4."
PROMPT ENSEMBLE AND TEMPLATES,0.1894150417827298,"3.1.2
PROMPT ENSEMBLE AND TEMPLATES"
PROMPT ENSEMBLE AND TEMPLATES,0.19220055710306408,"Due to the problem of polysemy and inconsistency with the pre-training process, following Radford
et al. (2021), we also use prompt templates to augment the original label for some downstream tasks.
For visualizations, for simplicity, we use only one prompt template across the paper, i.e. ‚Äúa photo of
a {label}.‚Äù as Radford et al. (2021). For other experiments, we report results using prompt ensemble
following Radford et al. (2021). When multiple prompts are allowed, the token-wise representations
of different prompt templates for the same class label are different, and can not be summed together
to form a mean textual representation as in (Radford et al., 2021). Thus, instead of ensembling
different prompt templates by their mean textual representation, we ensemble them by their mean
token-wise similarity. SpeciÔ¨Åcally, suppose there are C prompt templates, each label is augmented
to C different texts xT
1 , xT
2 , ¬∑ ¬∑ ¬∑ , xT
C. The similarity between an image xI and this label is computed
as 1"
PROMPT ENSEMBLE AND TEMPLATES,0.19498607242339833,"C
PC
c=1 sI
¬∑,¬∑(xI, xT
c ), where sI
¬∑,¬∑ is deÔ¨Åned in Equation (4)."
PROMPT ENSEMBLE AND TEMPLATES,0.1977715877437326,"We use a uniÔ¨Åed rule-based method inspired by Radford et al. (2018) to construct prompt templates
for image classiÔ¨Åcation tasks. SpeciÔ¨Åcally, each template consists of four components:"
PROMPT ENSEMBLE AND TEMPLATES,0.20055710306406685,"[preÔ¨Åx] {label}, [category description]. [sufÔ¨Åx].
(6)
Here, the ‚Äú[preÔ¨Åx]‚Äù is an in-context description like ‚Äúa photo of a‚Äù similar as Radford et al. (2021);
‚Äúlabel‚Äù is a class label of the dataset; ‚Äú[category description]‚Äù describes the category which is found
helpful for some Ô¨Åne-grained image classiÔ¨Åcation datasets (Radford et al., 2021), e.g., ‚Äú a type of
pet‚Äù for dataset Oxford-IIIT Pets. An interesting Ô¨Ånding is that, adding a sufÔ¨Åx that includes the
reference word ‚Äúit‚Äù (e.g., ‚ÄúI like it.‚Äù) at the end of the prompt empirically improves the zero-shot
classiÔ¨Åcation performance of the proposed model. We speculate this is because the reference word
‚Äúit‚Äù strengthens the Ô¨Åne-grained cross-modal alignment, as it can also be aligned to image patches
of the target object. Detailed prompt templates for different datasets can be found in Appendix A.5."
IMAGE AND TEXT AUGMENTATION,0.20334261838440112,"3.2
IMAGE AND TEXT AUGMENTATION"
IMAGE AND TEXT AUGMENTATION,0.20612813370473537,"To obtain better generalization and data-efÔ¨Åciency of the model, we perform data augmentation on
both images and texts during the pre-training phase to construct more image-text pairs. We apply
AutoAugment (Krizhevsky et al., 2012; Sato et al., 2015; Cubuk et al., 2019; Hoffer et al., 2020)
for image augmentation, following the SOTA vision recognition methods (Touvron et al., 2021; Xie
et al., 2020b). To ensure the augmented texts are semantically similar as the original one, for text"
IMAGE AND TEXT AUGMENTATION,0.20891364902506965,Published as a conference paper at ICLR 2022
IMAGE AND TEXT AUGMENTATION,0.2116991643454039,"Table 1: Top-1 accuracy(%) of zero-shot image classiÔ¨Åcation on 12 datasets. Our FILIP can boost
3‚àº5% accuracy on average."
IMAGE AND TEXT AUGMENTATION,0.21448467966573817,CIFAR10
IMAGE AND TEXT AUGMENTATION,0.21727019498607242,CIFAR100
IMAGE AND TEXT AUGMENTATION,0.2200557103064067,Caltech101
IMAGE AND TEXT AUGMENTATION,0.22284122562674094,StanfordCars
IMAGE AND TEXT AUGMENTATION,0.22562674094707522,Flowers102
IMAGE AND TEXT AUGMENTATION,0.22841225626740946,Food101
IMAGE AND TEXT AUGMENTATION,0.23119777158774374,SUN397 DTD
IMAGE AND TEXT AUGMENTATION,0.233983286908078,Aircrafts
IMAGE AND TEXT AUGMENTATION,0.23676880222841226,OxfordPets
IMAGE AND TEXT AUGMENTATION,0.2395543175487465,EuroSAT
IMAGE AND TEXT AUGMENTATION,0.24233983286908078,ImageNet
IMAGE AND TEXT AUGMENTATION,0.24512534818941503,Average
IMAGE AND TEXT AUGMENTATION,0.2479108635097493,"CLIP-ViT-B/32
91.3
65.1
87.9
59.4
66.7
84.4
63.2
44.5
21.2
87.0
49.4
63.2
65.3
FILIPbase-ViT-B/32
86.9
65.5
91.9
55.4
85.3
82.8
69.1
49.3
57.2
88.1
49.9
68.8
70.9+5.6"
IMAGE AND TEXT AUGMENTATION,0.25069637883008355,"CLIP-ViT-L/14
96.2
77.9
92.6
77.3
78.7
92.9
67.7
55.3
36.1
93.5
59.9
75.3
75.3
FILIPlarge-ViT-L/14
95.7
75.3
93.0
70.8
90.1
92.2
73.1
60.7
60.2
92
59.2
77.1
78.3+3.0"
IMAGE AND TEXT AUGMENTATION,0.25348189415041783,"augmentation, we rewrite the original text using back-translation (Xie et al., 2020a; Sennrich et al.,
2016a). SpeciÔ¨Åcally, the texts are Ô¨Årst translated to the target language and then translated back to
the source language. We choose German and Russian as the target language and get extra two texts
for each image-text pair. When constructing a batch of image-text pairs during the pre-training, the
text of each image-text pair is randomly sampled from the three candidate texts, i.e., the original text
and two back-translated texts."
PRE-TRAINING DATASET,0.2562674094707521,"3.3
PRE-TRAINING DATASET"
PRE-TRAINING DATASET,0.2590529247910863,"A sufÔ¨Åciently large image-text dataset is a prerequisite for vision-language pre-training. Recent
CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) construct datasets with 400M and 1800M
image-text pairs, respectively. In this work, we also collect a large-scale dataset called FILIP300M
from the Internet, which consists of 300M image-text pairs and covers board vision and language
concepts. For image-based Ô¨Åltering, we remove the images whose shorter dimension is smaller
than 200 pixels and the aspect ratio is larger than 3. For text-based Ô¨Åltering, we keep only English
texts, and exclude the meaningless ones, e.g., img 0.jpg. We also discard image-text pairs whose
texts are repeated for over 10 times. Besides, we also use 3 public datasets, including Conceptual
Captions 3M (CC3M) (Sharma et al., 2018), Conceptual 12M (CC12M) (Changpinyo et al., 2021)
and Yahoo Flickr Creative Commons 100M (YFCC100M) (Thomee et al., 2016). We apply the
same Ô¨Åltering rules on YFCC100M. Finally, we use about 340M image-text pairs for pre-training.
Despite using a smaller training dataset than CLIP and ALIGN, our models still outperform them in
most down-steam tasks (see Section 4)."
EXPERIMENTS,0.2618384401114206,"4
EXPERIMENTS"
EXPERIMENTAL SETUP,0.2646239554317549,"4.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.26740947075208915,"Model Architectures. We train two models from scratch, i.e., FILIPbase and FILIPlarge. The model
architectures follow CLIP (Radford et al., 2021), i.e., the image encoder is ViT-B/32 for FILIPbase
and ViT-L/14 for FILIPlarge. More details can be found in Appendix A.3."
EXPERIMENTAL SETUP,0.27019498607242337,"Pre-training Details. To save memory and scale up the batch size, automatic mixed-precision
(Micikevicius et al., 2018) and gradient checkpoint (Griewank & Walther, 2000; Chen et al., 2016)
are used The input images are resized to 224 √ó 224 resolution during pre-training and the maximum
length of the text is limited to 77 tokens following Radford et al. (2021). The training is mainly
conducted on Nvidia V100 GPUs and Ascend Cards. FILIPbase is trained on 128 cards about 9
days and FILIPlarge takes about 24 days to train on 192 cards. Unless otherwise speciÔ¨Åed, we use
FILIPlarge to compare with other methods and FILIPbase for ablation. We train both models using
the LAMB optimizer (You et al., 2020) and cosine learning rate schedule (Loshchilov & Hutter,
2016) with a linear warmup. Weight decay regularization is applied to all parameters except bias,
layer normalization, token embedding, positional embedding and temperature in contrastive loss.
Detailed values of hyperparameters for different datasets and models can be found in Appendix A.3."
IMAGE CLASSIFICATION,0.27298050139275765,"4.2
IMAGE CLASSIFICATION"
IMAGE CLASSIFICATION,0.2757660167130919,"In this section, we compare our FILIP with CLIP (Radford et al., 2021) on 12 downstream image
classiÔ¨Åcation datasets."
IMAGE CLASSIFICATION,0.2785515320334262,Published as a conference paper at ICLR 2022
IMAGE CLASSIFICATION,0.28133704735376047,"Table 2: Results of zero-shot image-text retrieval on Flickr30K and MSCOCO datasets. The last
two rows (marked with *) report the zero-shot results on Flickr30K dataset of model Ô¨Åne-tuned on
MSCOCO dataset, following the setting of ALBEF (Li et al., 2021a)."
IMAGE CLASSIFICATION,0.2841225626740947,"Flickr30K
MSCOCO
image-to-text
text-to-image
image-to-text
text-to-image
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
Unicoder-VL 64.3
85.8
92.3
48.4
76.0
85.2
‚àí
‚àí
‚àí
‚àí
‚àí
‚àí
ImageBERT
70.7
90.2
94.0
54.3
79.6
87.5
44.0
71.2
80.4
32.3
59.0
70.2
UNITER
83.6
95.7
97.7
68.7
89.2
93.9
‚àí
‚àí
‚àí
‚àí
‚àí
‚àí
CLIP
88.0
98.7
99.4
68.7
90.6
95.2
58.4
81.5
88.1
37.8
62.4
72.2
ALIGN
88.6
98.7
99.7
75.7
93.8
96.8
58.6
83.0
89.7
45.6
69.8
78.6
FILIP
89.8
99.2
99.8
75.0
93.4
96.3
61.3
84.3
90.4
45.9
70.6
79.3
ALBEF*
94.1
99.5
99.7
82.8
96.3
98.1
‚àí
‚àí
‚àí
‚àí
‚àí
‚àí
FILIP*
95.4
99.8
100.0
84.7
97.0
98.7
‚àí
‚àí
‚àí
‚àí
‚àí
‚àí"
IMAGE CLASSIFICATION,0.28690807799442897,"Zero-shot ClassiÔ¨Åcation.
As in Section 3.1.2, we apply a set of prompts (Appendix A.5) for each
dataset and ensemble them to get the Ô¨Ånal results. Table 1 shows the results on 12 datasets. Despite
using less training data (340M vs. 400M), both FILIPbase and FILIPlarge considerably outperform
their CLIP counterparts in terms of average top-1 accuracy over 12 datasets, i.e., achieving absolute
improvements of 5.6% and 3.0%, respectively. In particular, our FILIP surpasses CLIP on ImageNet,
the largest dataset among 12 datasets. FILIP also achieves substantial performance gains on some
domain-speciÔ¨Åc datasets like Aircrafts. We speculate this is because, unlike CLIP which aggregates
the information of the whole image into the [CLS] token, our proposed FILIP focuses more on
the target object by directly aligning the image patches of the target object with the textual tokens
corresponding to the class label (visualizations of word-patch alignment are in Section 4.5)."
IMAGE CLASSIFICATION,0.28969359331476324,"Linear Probe.
Table 14 in Appendix A.6 shows the linear probe results, and FILIP again outper-
forms CLIP by 1.2‚àº1.8% points on average. More details can be found in Appendix A.6."
IMAGE-TEXT RETRIEVAL,0.2924791086350975,"4.3
IMAGE-TEXT RETRIEVAL"
IMAGE-TEXT RETRIEVAL,0.29526462395543174,"Image-text retrieval consists of two sub-tasks: image-to-text retrieval and text-to-image retrieval.
We evaluate our FILIP model on two retrieval benchmark datasets: Flickr30K (Plummer et al.,
2015) and MSCOCO (Lin et al., 2014), under both zero-shot and Ô¨Åne-tuned settings. More details
of experimental setting can be found in Appendix A.3."
IMAGE-TEXT RETRIEVAL,0.298050139275766,"Tables 2 and 3 show the results of zero-shot and Ô¨Åne-tuned image-text retrieval, respectively. We
compare our FILIP model against methods with complex attention layers including Unicoder-VL (Li
et al., 2020a), ImageBERT (Qi et al., 2020), UNITER (Chen et al., 2020), VILLA (Gan et al., 2020),
ERNIE-ViL (Yu et al., 2021), Oscar (Li et al., 2020b), VinVL (Zhang et al., 2021), ALBEF (Li et al.,
2021a), and methods trained on larger-scale image-text datasets including CLIP (Radford et al.,
2021) and ALIGN (Jia et al., 2021). As we can see, FILIP achieves state-of-the-art performances
under all metrics on both Flickr30K and MSCOCO datasets, except for zero-shot text-to-image
retrieval on Flickr30K, where FILIP achieves competitive performance with SOTA. For zero-shot
image-to-text retrieval on MSCOCO dataset, the absolute R@1 of our proposed FILIP is 2.7% higher
than ALIGN, which is trained on a much larger dataset."
ABLATION STUDY,0.3008356545961003,"4.4
ABLATION STUDY"
ABLATION STUDY,0.30362116991643456,"Effectiveness of Each Component. We study the effectiveness of each component in FILIP, i.e.,
image/text augmentations and cross-modal late interaction. Experiments are conducted on FILIPbase,
with a Ô¨Åltered subset of YFCC100M as the training dataset (as described in Section 3.3), on both
zero-shot retrieval and classiÔ¨Åcation tasks. We measure models‚Äô performance on MSCOCO zero-
shot image-text retrieval and ImageNet zero-shot classiÔ¨Åcation, which are two effective indicators
for the quality of the learned vision-language representations."
ABLATION STUDY,0.3064066852367688,"Table 4 reports the results. As can be seen, all three components are beneÔ¨Åcial for both tasks.
Despite the simple design, cross-modal late interaction brings signiÔ¨Åcant performance improvements
over the baseline (the vanilla CLIP ViT-B/32), with an absolute R@1 gain of 5.5% (resp. 3.8%)
for image-to-text (resp. text-to-image) retrieval on MSCOCO and an absolute top-1 accuracy gain"
ABLATION STUDY,0.30919220055710306,Published as a conference paper at ICLR 2022
ABLATION STUDY,0.31197771587743733,Table 3: Results of Ô¨Åne-tuned image-text retrieval on Flickr30K and MSCOCO datasets.
ABLATION STUDY,0.3147632311977716,"Flickr30K
MSCOCO
image-to-text
text-to-image
image-to-text
text-to-image
R@1
R@5
R@10
R@1
R@5
R@10
R@1
R@5
R@10
R@1
R@5
R@10
Unicoder-VL
86.2
96.3
99.0
71.5
90.9
94.9
62.3
87.1
92.8
48.4
76.7
85.9
ImageBERT
87.0
97.6
99.2
73.1
92.6
96.0
66.4
89.8
94.4
50.5
78.7
87.1
UNITER
87.3
98.0
99.2
75.6
94.1
96.8
65.7
88.6
93.8
52.9
79.9
88.0
VILLA
87.9
97.5
98.8
76.3
94.2
96.8
‚àí
‚àí
‚àí
‚àí
‚àí
‚àí
ERNIE-ViL
88.1
98.0
99.2
76.7
93.6
96.4
‚àí
‚àí
‚àí
‚àí
‚àí
‚àí
Oscar
‚àí
‚àí
‚àí
‚àí
‚àí
‚àí
73.5
92.2
96.0
57.5
82.8
89.8
VinVL
‚àí
‚àí
‚àí
‚àí
‚àí
‚àí
75.4
92.9
96.2
58.8
83.5
90.3
ALIGN
95.3
99.8
100.0
84.9
97.4
98.6
77.0
93.5
96.9
59.9
83.3
89.8
ALBEF
95.9
99.8
100.0
85.6
97.5
98.9
77.6
94.3
97.2
60.7
84.3
90.5
Our FILIP
96.6
100.0
100.0
87.1
97.7
99.1
78.9
94.4
97.4
61.2
84.3
90.6"
ABLATION STUDY,0.31754874651810583,"Table 4: Ablation study of different components on pre-training subset of YFCC100M. I2T and T2I
are abbreviations for image-to-text and text-to-image retrieval, respectively. ‚ÄúZS‚Äù means zero-shot
performance. Underlined numbers have the highest improvements for the corresponding metrics."
ABLATION STUDY,0.3203342618384401,"Model
MSCOCO
ImageNet
I2T R@1
I2T R@5
T2I R@1
T2I R@5
ZS Top1"
ABLATION STUDY,0.3231197771587744,"Baseline (ViT-B/32)
25.0
49.5
14.7
34.7
30.4
w/ image augmentation
26.1
51.8
16.5
37.5
32.5
w/ back translation
29.2
55.0
17.9
39.8
33.9
w/ cross-modal late interaction
30.5
55.3
18.5
40.0
34.3
Our FILIPbase
33.4
60.1
23.0
46.2
37.8"
ABLATION STUDY,0.32590529247910865,"Table 5: EfÔ¨Åciency study of the cross-modal late interaction. ‚Äúorig‚Äù and ‚Äúlate‚Äù stand for the con-
trastive loss based on the original cosine similarity in CLIP and our proposed cross-modal late
interaction, respectively. ‚ÄúZS‚Äù means zero-shot performance. We report results for ViT-B/32 trained
on Ô¨Åltered YFCC100M with 8 V100 GPUs, with a batch size of 512 per GPU. Training time and
memory consumption are tested using the same gradient checkpoint conÔ¨Åguration. * denotes our
Ô¨Ånal setting used in other experiments."
ABLATION STUDY,0.3286908077994429,"Loss
Embed
Embed
Token
Training time
Memory
ImageNet
dim
precision
%
(sec/iter)
(MB)
ZS Top1"
ABLATION STUDY,0.33147632311977715,"orig (baseline)
512
fp32
-
1.31
14300
30.4
late
512
fp32
100%
2.85
26000
34.6
late
512
fp16
100%
2.67
23468
34.5
late
256
fp16
100%
2.31
22382
35.2
late
256
fp16
50%
1.61
16336
34.5
late*
256
fp16
25%
1.39
16100
34.3"
ABLATION STUDY,0.3342618384401114,"of 3.9% for zero-shot classiÔ¨Åcation on ImageNet. Further improvements are observed when all
components are combined together."
ABLATION STUDY,0.3370473537604457,"EfÔ¨Åciency Study of Cross-modal Late Interaction. Since the late interaction mechanism in Sec-
tion 3.1.1 requires to calculate the similarity between all visual and textual tokens, its efÔ¨Åciency can
be a problem when employed in large-scale distributed training. As described in Section 3.1.1, we
make several attempts to address the issue. Table 5 shows the efÔ¨Åciency improvement on zero-shot
classiÔ¨Åcation on ImageNet when these attempts are applied. As can be seen, these attempts improve
the efÔ¨Åciency of late interaction without accuracy drop. Combining all three attempts achieves only
slightly slower training and larger memory consumption than the original loss in CLIP."
VISUALIZATION OF FINE-GRAINED ALIGNMENT,0.3398328690807799,"4.5
VISUALIZATION OF FINE-GRAINED ALIGNMENT"
VISUALIZATION OF FINE-GRAINED ALIGNMENT,0.3426183844011142,"In this section, we visualize FILIP‚Äôs capability of capturing Ô¨Åne-grained cross-modal correspon-
dence using the method of word-patch alignment. To make a fair comparison, we use our FILIPbase
trained on YFCC100M and CLIP‚Äôs ViT-B/32, which are of the same size, for visualization. Each
image is patchiÔ¨Åed to 7√ó7 image patches. More visualization results can be found in Appendix A.4."
VISUALIZATION OF FINE-GRAINED ALIGNMENT,0.34540389972144847,Published as a conference paper at ICLR 2022
VISUALIZATION OF FINE-GRAINED ALIGNMENT,0.34818941504178275,"Raw image
FILIP
CLIP
Raw image
FILIP
CLIP"
VISUALIZATION OF FINE-GRAINED ALIGNMENT,0.35097493036211697,"Raw image
FILIP
CLIP
Raw image
FILIP
CLIP"
VISUALIZATION OF FINE-GRAINED ALIGNMENT,0.35376044568245124,"(a) Balloon (5)
(b) Lifeboat (5)"
VISUALIZATION OF FINE-GRAINED ALIGNMENT,0.3565459610027855,"(c) Small white butterfly (5, 6, 7)
(d) Electric locomotive (5,6)"
VISUALIZATION OF FINE-GRAINED ALIGNMENT,0.3593314763231198,"Figure 2: Visualizations of word-patch alignment for 4 classes of the ImageNet dataset and ‚Äúa photo
of a {label}.‚Äù is the prompt. Numbers in the parentheses after the class label indicate the location
indices of the class label in the tokenized textual sequence. The correct predictions are highlighted
by opaque patches with the class label indices in red."
VISUALIZATION OF FINE-GRAINED ALIGNMENT,0.362116991643454,"Visualization Method. The word-patch alignment is performed based on the token-wise similarity
between the image patches and textual tokens. SpeciÔ¨Åcally, for the k-th image patch, the location
index of textual token with the largest similarity with it (mI
k in Equation (4)) is considered as its
predicted label, and is placed at the center of it. Take class ‚Äúballoon‚Äù as an example. There are
8 tokens in the tokenized textual sequence ‚Äú[BOS] a photo of a balloon. [EOS]‚Äù, and the location
index of the class label ‚Äúballoon‚Äù is ‚Äú5‚Äù. Note that one class label may be tokenized to more than
one token. Location indices of textual tokens corresponding to the class label are highlighted in red,
while the others are marked in white. A desired model that learns Ô¨Åne-grained representations would
predict image patches of the target object to red indices."
VISUALIZATION OF FINE-GRAINED ALIGNMENT,0.3649025069637883,"Observations. Figure 2 shows the word-patch alignment results for FILIP and CLIP on 4 classes
from the ImageNet dataset. As can be seen, FILIP exhibits the Ô¨Åner-grained understanding of an
image in the following aspects. (i) A single object: From the visualization of class ‚Äúsmall white but-
terÔ¨Çy‚Äù, the image patches covering the object are all classiÔ¨Åed correctly; (ii) Same object in different
shapes: From the visualizations of class ‚Äúballoon‚Äù and ‚Äúlifeboat‚Äù, image patches corresponding to
all target objects with different shapes and locations are correctly classiÔ¨Åed; (iii) Key Components
of an object: For class ‚Äúelectric locomotive‚Äù, there are two key components crucial to correctly
classifying the image, i.e., ‚Äúelectric‚Äù and ‚Äúlocomotive‚Äù, whose corresponding textual token indices
are ‚Äú5‚Äù and ‚Äú6‚Äù, respectively. As can be seen, image patches matching these two key components
are respectively correctly classiÔ¨Åed. On the other hand, CLIP can not correctly align image patches
with corresponding textual tokens. Compared with Kim et al. (2021) which uses an extra optimal
transport to align the textual word and image patch distributions, the word-patch alignment can be
simply automatically learned by our method."
CONCLUSION AND FUTURE WORK,0.36768802228412256,"5
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.37047353760445684,"This paper introduces FILIP, a simple yet generic framework towards Ô¨Åne-grained vision-language
pre-training. By using a token-wise maximum similarity, our method learns Ô¨Åne-grained represen-
tation for patches in the images and words in the sentences. While it achieves competitive results
against several large-scale multi-modal pre-training on various downstream tasks, both its archi-
tecture and training procedure can still be optimized to improve its performance. In the future, a
more advanced image encoder as well as a well-designed interaction layer can be used to boost the
performance. Furthermore, we can further add more masked language/image loss to support more
generation tasks. To this end, we hope to extend FILIP as a generic and uniÔ¨Åed interface for solving
a large variety of vision-language tasks."
CONCLUSION AND FUTURE WORK,0.3732590529247911,Published as a conference paper at ICLR 2022
REFERENCES,0.37604456824512533,REFERENCES
REFERENCES,0.3788300835654596,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. In Advances in neural information processing systems, 2020."
REFERENCES,0.3816155988857939,"Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-
scale image-text pre-training to recognize long-tail visual concepts. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2021."
REFERENCES,0.38440111420612816,"Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear
memory cost. preprint arXiv:1604.06174, 2016."
REFERENCES,0.3871866295264624,"Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and
Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on
computer vision, pp. 104‚Äì120. Springer, 2020."
REFERENCES,0.38997214484679665,"Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 113‚Äì123, 2019."
REFERENCES,0.39275766016713093,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Annual Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics, 2019."
REFERENCES,0.3955431754874652,"Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou,
Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers.
arXiv preprint arXiv:2105.13290, 2021."
REFERENCES,0.3983286908077994,"Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by
context prediction. In Proceedings of the IEEE international conference on computer vision, pp.
1422‚Äì1430, 2015."
REFERENCES,0.4011142061281337,"Xiao Dong, Xunlin Zhan, Yangxin Wu, Yunchao Wei, Xiaoyong Wei, Minlong Lu, and Xiaodan
Liang. M5product: A multi-modal pretraining benchmark for e-commercial product downstream
tasks. Preprint arXiv:2109.04275, 2021."
REFERENCES,0.403899721448468,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An im-
age is worth 16x16 words: Transformers for image recognition at scale. In International Confer-
ence on Learning Representations, 2020."
REFERENCES,0.40668523676880225,"Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial
training for vision-and-language representation learning. arXiv preprint arXiv:2006.06195, 2020."
REFERENCES,0.40947075208913647,"Andreas Griewank and Andrea Walther. Algorithm 799: revolve: an implementation of check-
pointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on
Mathematical Software (TOMS), 26(1):19‚Äì45, 2000."
REFERENCES,0.41225626740947074,"Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten HoeÔ¨Çer, and Daniel Soudry. Aug-
ment your batch: Improving generalization through instance repetition. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8129‚Äì8138, 2020."
REFERENCES,0.415041782729805,"Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. Seeing
out of the box: End-to-end pre-training for vision-language representation learning. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 12976‚Äì12985, 2021."
REFERENCES,0.4178272980501393,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448‚Äì456.
PMLR, 2015."
REFERENCES,0.4206128133704735,"Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan
Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning
with noisy text supervision. In International Conference on Machine Learning, 2021."
REFERENCES,0.4233983286908078,Published as a conference paper at ICLR 2022
REFERENCES,0.42618384401114207,"Omar Khattab and Matei Zaharia. Colbert: EfÔ¨Åcient and effective passage search via contextualized
late interaction over bert. In International ACM SIGIR Conference on Research and Development
in Information Retrieval, pp. 39‚Äì48, 2020."
REFERENCES,0.42896935933147634,"Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convo-
lution or region supervision. In International Conference on Machine Learning, 2021."
REFERENCES,0.43175487465181056,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiÔ¨Åcation with deep convo-
lutional neural networks. In Advances in neural information processing systems, volume 25, pp.
1097‚Äì1105, 2012."
REFERENCES,0.43454038997214484,"Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for
image-text matching. In Proceedings of the European Conference on Computer Vision (ECCV),
pp. 201‚Äì216, 2018."
REFERENCES,0.4373259052924791,"Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. Unicoder-vl: A universal encoder
for vision and language by cross-modal pre-training. In Proceedings of the AAAI Conference on
ArtiÔ¨Åcial Intelligence, volume 34, pp. 11336‚Äì11344, 2020a."
REFERENCES,0.4401114206128134,"Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, ShaÔ¨Åq Joty, and Steven Xiong,
Caiming andHoi. Align before fuse: Vision and language representation learning with momentum
distillation. Technical Report arXiv:2107.07651, 2021a."
REFERENCES,0.4428969359331476,"Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple
and performant baseline for vision and language. Preprint arXiv:1908.03557, 2019."
REFERENCES,0.4456824512534819,"Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang.
Unimo: Towards uniÔ¨Åed-modal understanding and generation via cross-modal contrastive learn-
ing. In Annual Meeting of the Association for Computational Linguistics, 2021b."
REFERENCES,0.44846796657381616,"Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong
Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language
tasks. In European Conference on Computer Vision, pp. 121‚Äì137. Springer, 2020b."
REFERENCES,0.45125348189415043,"Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang,
Le Jiang, Xianyan Jia, et al. M6: A chinese multimodal pretrainer. Preprint arXiv:2103.00823,
2021."
REFERENCES,0.45403899721448465,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Doll¬¥ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740‚Äì755. Springer, 2014."
REFERENCES,0.4568245125348189,"I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. ICLR 2017 (5th
International Conference on Learning Representations), 2016."
REFERENCES,0.4596100278551532,"Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: pretraining task-agnostic visiolin-
guistic representations for vision-and-language tasks.
In International Conference on Neural
Information Processing Systems, pp. 13‚Äì23, 2019."
REFERENCES,0.4623955431754875,"Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision
training. In International Conference on Learning Representations, 2018."
REFERENCES,0.46518105849582175,"Fabian Pedregosa, Ga¬®el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
Machine learning in python. the Journal of machine Learning research, 12:2825‚Äì2830, 2011."
REFERENCES,0.467966573816156,"Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svet-
lana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-
to-sentence models. In IEEE international conference on computer vision, pp. 2641‚Äì2649, 2015."
REFERENCES,0.47075208913649025,"Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. Imagebert: Cross-modal
pre-training with large-scale weak-supervised image-text data. arXiv preprint arXiv:2001.07966,
2020."
REFERENCES,0.4735376044568245,Published as a conference paper at ICLR 2022
REFERENCES,0.4763231197771588,"Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training. 2018."
REFERENCES,0.479108635097493,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.4818941504178273,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. Preprint arXiv:2103.00020, 2021."
REFERENCES,0.48467966573816157,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniÔ¨Åed text-to-text
transformer. Journal of Machine Learning Research, 21:1‚Äì67, 2020."
REFERENCES,0.48746518105849584,"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. Preprint arXiv:2102.12092, 2021."
REFERENCES,0.49025069637883006,"Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. Advances in neural information processing systems, 28:
91‚Äì99, 2015."
REFERENCES,0.49303621169916434,"Ikuro Sato, Hiroki Nishimura, and Kensuke Yokoi. Apac: Augmented pattern classiÔ¨Åcation with
neural networks. Technical Report arXiv:1505.03229, 2015."
REFERENCES,0.4958217270194986,"Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In IEEE international conference on computer vision, pp. 618‚Äì626, 2017."
REFERENCES,0.4986072423398329,"Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models
with monolingual data.
In Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pp. 86‚Äì96, 2016a."
REFERENCES,0.5013927576601671,"Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), pp. 1715‚Äì1725, 2016b."
REFERENCES,0.5041782729805014,"Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pp. 2556‚Äì2565, 2018."
REFERENCES,0.5069637883008357,"Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland,
Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications
of the ACM, 59(2):64‚Äì73, 2016."
REFERENCES,0.5097493036211699,"Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer
Vision‚ÄìECCV 2020: 16th European Conference, pp. 776‚Äì794. Springer, 2020."
REFERENCES,0.5125348189415042,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Herv¬¥e J¬¥egou.
Training data-efÔ¨Åcient image transformers & distillation through attention.
In
International Conference on Machine Learning, pp. 10347‚Äì10357. PMLR, 2021."
REFERENCES,0.5153203342618384,"Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple
visual language model pretraining with weak supervision. Preprint arXiv:2108.10904, 2021."
REFERENCES,0.5181058495821727,"Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation
for consistency training. In Advances in Neural Information Processing Systems, volume 33,
2020a."
REFERENCES,0.520891364902507,"Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student
improves imagenet classiÔ¨Åcation. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 10687‚Äì10698, 2020b."
REFERENCES,0.5236768802228412,Published as a conference paper at ICLR 2022
REFERENCES,0.5264623955431755,"Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
Large batch optimization for deep
learning: Training bert in 76 minutes. In International Conference on Learning Representations,
2020."
REFERENCES,0.5292479108635098,"Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowl-
edge enhanced vision-language representations through scene graphs. In Proceedings of the AAAI
Conference on ArtiÔ¨Åcial Intelligence, volume 35, pp. 3208‚Äì3216, 2021."
REFERENCES,0.532033426183844,"Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, and
Xiaodan Liang. Product1m: Towards weakly supervised instance-level product retrieval via cross-
modal pretraining. In International Conference on Computer Vision, 2021."
REFERENCES,0.5348189415041783,"Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and
Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 5579‚Äì5588, 2021."
REFERENCES,0.5376044568245125,Published as a conference paper at ICLR 2022
REFERENCES,0.5403899721448467,"A
APPENDIX"
REFERENCES,0.5431754874651811,"A.1
ETHICAL ISSUES IN DATA COLLECTION"
REFERENCES,0.5459610027855153,"When collecting the large-scale image-text pairs from the Internet, we perform person-name substi-
tutions as in Changpinyo et al. (2021), in order to protect the privacy of the individuals appearing in
the text. SpeciÔ¨Åcally, we replace each person name appeared in the text with a special < person >
token. Besides, we also disgard image-text pairs whose text contains sensitive words."
REFERENCES,0.5487465181058496,"A.2
DATASETS SUMMARY"
REFERENCES,0.5515320334261838,Table 6 shows the number of image-text pairs of each datasets used in different pre-training methods.
REFERENCES,0.5543175487465181,"Table 6: Number of image-text pairs used in the pre-training of FILIP, CLIP and ALIGN."
REFERENCES,0.5571030640668524,"FILIP
CLIP
ALIGN
CC3M
CC12M
YFCC100M
FILIP300M
(Radford et al., 2021)
(Jia et al., 2021)
#
3M
10M
26M
300M
400M
1800M"
REFERENCES,0.5598885793871866,"A.3
DETAILED EXPERIMENTAL SETTINGS"
REFERENCES,0.5626740947075209,Table 7: The architecture parameters for FILIP models.
REFERENCES,0.5654596100278552,"Model
Embedding
Input
Image Encoder
Text Encoder
dimension
resolution
#layers
width
#heads
#layers
width
#heads"
REFERENCES,0.5682451253481894,"FILIPbase
256
224 √ó 224
12
768
12
12
512
8
FILIPlarge
256
224 √ó 224
24
1024
16
12
768
12"
REFERENCES,0.5710306406685237,"Model Architectures.
We follow the same architecture design as CLIP, for both FILIPbase and
FILIPlarge, except that we reduce the embedding dimension from 512/768 to 256 for the efÔ¨Åciency
of loss computation. Table 7 describes the details of architectures."
REFERENCES,0.5738161559888579,"Details for Pre-training and Hyperparameters.
For the implementation of the contrastive loss,
following CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), we also set the temperature
in the softmax function to be a learnable parameter and initialize it as 0.07. For the pre-training,
we use the LAMB optimizer implemented by the cybertronai‚Äôs open-source repository (https:
//github.com/cybertronai/pytorch-lamb). For the learning rate scheduler, we Ô¨Årst
assign a base learning rate and then linearly warm it up to the peak learning rate according to the"
REFERENCES,0.5766016713091922,"effective total batch size by a square root strategy, peak lr = base lr √ó
q"
REFERENCES,0.5793871866295265,total bs
REFERENCES,0.5821727019498607,"512
. We note that a
large weight decay is crucial to stabilize training and improve generalization. SpeciÔ¨Åcally, we found
that the training stability is a challenging issue when applying mix-precision training to large-scale
models, i.e., the training is extremely unstable and the NaN loss easily happens. Recent works"
REFERENCES,0.584958217270195,Table 8: Common hyperparameters used for FILIP pre-training.
REFERENCES,0.5877437325905293,"Hyperparameter
Value"
REFERENCES,0.5905292479108635,"Vocabulary size
49408
Initial temperature
0.07
LAMB Œ≤1
0.9
LAMB Œ≤2
0.999
LAMB œµ
10‚àí4
Warm-up iters
3000
Training epochs
30"
REFERENCES,0.5933147632311978,Published as a conference paper at ICLR 2022
REFERENCES,0.596100278551532,"Table 9: Model- and dataset-speciÔ¨Åc hyperparameters used for FILIP pre-training. Numbers in batch
size represent the total batch size across all workers and are calculated as: batch size per GPU √ó
#GPUs. FILIP340M is the combination of FILIP300M, YFCC100M, CC12M and CC3M."
REFERENCES,0.5988857938718662,"Model
Dataset
Batch size
Base LR
Weight decay"
REFERENCES,0.6016713091922006,"FILIPbase
YFCC100M
1024 √ó 8
6 √ó 10‚àí3
3e-2
FILIPbase
FILIP340M
320 √ó 128
2 √ó 10‚àí3
3e-3"
REFERENCES,0.6044568245125348,"FILIPlarge
FILIP340M
160 √ó 192
1.5 √ó 10‚àí3
3e-3"
REFERENCES,0.6072423398328691,"DALL-E (Ramesh et al., 2021) and Cogview (Ding et al., 2021) also notice this issue and provide
their solutions. However, we found that simply increasing the weight decay and applying the trick
of removing the weight decay of speciÔ¨Åc parameters as described in Section 4.1 work for our case.
The base learning rate and weight decay are selected manually via observing the performance at
the early training stage. Table 8 summarizes the common hyperparameters and Table 9 shows the
model- and dataset-speciÔ¨Åc hyperparameters for FILIP pre-training."
REFERENCES,0.6100278551532033,"Details for Image-text Retrieval.
Following previous works (Jia et al., 2021; Li et al., 2021a),
for Flickr30K, we test on the 1K test set with or without Ô¨Åne-tuning on the 30K training set, while
for MSCOCO, we test on the 5K test set with or without Ô¨Åne-tuning on the 113K training set. We
use the similarity between image and text for ranking and use the contrastive loss for Ô¨Åne-tuning.
Since there are multiple texts for each image in these two datasets, we change the ground-truth label
of contrastive loss to consider multiple positives, by assigning a probability of 1/#positive to each
positive following ALBEF (Li et al., 2021a). Besides, we also use prompts during evaluation for
both datasets, see Appendix A.5 for details. Table 10 shows the hyperparameters for image-text
retrieval Ô¨Åne-tuning."
REFERENCES,0.6128133704735376,Table 10: Hyperparameters used for image-text retrieval Ô¨Åne-tuning.
REFERENCES,0.6155988857938719,"Hyperparameter
Value"
REFERENCES,0.6183844011142061,"Image size
392 √ó 392
Training epochs
3
Optimizer
LAMB
Batch size
5120
Base LR
2 √ó 10‚àí4"
REFERENCES,0.6211699164345403,"Weight decay
3 √ó 10‚àí4"
REFERENCES,0.6239554317548747,"A.4
MORE VISUALIZATIONS OF WORD-PATCH ALIGNMENT AND GRAD-CAM HEATMAPS"
REFERENCES,0.6267409470752089,"In Figure 3, we visualize the cross-modal alignment of the proposed method for more images, in
terms of both word-patch alignment as described in Section 4.5 and Grad-CAM heatmaps (Selvaraju
et al., 2017). We compute the Grad-CAM heatmaps based on the average self-attention maps over
the image patches classiÔ¨Åed to targeted textual tokens (i.e., the textual token(s) corresponding to the
class label in the ImageNet dataset) in the last layer of the image encoder. We average the heatmaps
over all attention heads. As can be seen, our proposed model learns meaningful alignment between
image patches and textual tokens."
REFERENCES,0.6295264623955432,"A.5
PROMPT TEMPLATES FOR DOWNSTREAM TASKS"
REFERENCES,0.6323119777158774,"Image ClassiÔ¨Åcation.
Table 11 shows the prompt templates for different image classiÔ¨Åcation
datasets in the form of ‚Äú [preÔ¨Åx] {label}, [category description]. [sufÔ¨Åx]. ‚Äù in Equation (6). There
are three components to be determined in the template, i.e., the preÔ¨Åx, the category description and
the sufÔ¨Åx. For each component, we select several well-performed ones for each dataset. Then we
use the full combinations of all three components as the set of prompt templates for ensemble. For
instance, we use 5 preÔ¨Åxes, no category descriptions, and 6 sufÔ¨Åxes for dataset ImageNet. Then the
total number of prompt templates for this dataset is: 5 √ó 1 √ó 6 = 30."
REFERENCES,0.6350974930362117,Published as a conference paper at ICLR 2022
REFERENCES,0.637883008356546,"Bald eagle
(5,6)"
REFERENCES,0.6406685236768802,"Killer whale
(5,6)"
REFERENCES,0.6434540389972145,"Bee eater
(5,6)"
REFERENCES,0.6462395543175488,"Damselfly
(5,6,7)"
REFERENCES,0.649025069637883,"Bullock cart
(5,6)"
REFERENCES,0.6518105849582173,"Fireboat
(5,6)"
REFERENCES,0.6545961002785515,Ski (5)
REFERENCES,0.6573816155988857,Mosque (5)
REFERENCES,0.6601671309192201,"Yellow 
lady‚Äôs 
slipper
(5,6,7,8)"
REFERENCES,0.6629526462395543,"Necklace 
(5)"
REFERENCES,0.6657381615598886,"Raw image
Patch prediction
GradCAM
Patch prediction
GradCAM
Raw image"
REFERENCES,0.6685236768802229,"Figure 3: More visualizations on different classes of ImageNet dataset. Numbers in the parentheses
after the class label indicate the location indices of class label in the tokenized textual sequence."
REFERENCES,0.6713091922005571,Published as a conference paper at ICLR 2022
REFERENCES,0.6740947075208914,Table 11: Prompt templates used for 12 downstream image classiÔ¨Åcation tasks.
REFERENCES,0.6768802228412256,"Dataset
PreÔ¨Åx
Category
description
SufÔ¨Åx"
REFERENCES,0.6796657381615598,CIFAR10
REFERENCES,0.6824512534818942,"‚Äúa photo of a‚Äù, ‚Äúa jpeg photo of
a‚Äù, ‚Äúa painting of a‚Äù, ‚Äúitap of a‚Äù,
‚ÄúgrafÔ¨Åti of a‚Äù, ‚Äúa cartoon‚Äù, ‚Äúa
doodle‚Äù None"
REFERENCES,0.6852367688022284,"None, ‚ÄúIt‚Äôs common in daily
life‚Äù, ‚ÄúIt‚Äôs cute‚Äù, ‚ÄúIt‚Äôs ugly‚Äù,
‚ÄúIt‚Äôs weird‚Äù, ‚ÄúHope you like it‚Äù"
REFERENCES,0.6880222841225627,CIFAR100
REFERENCES,0.6908077994428969,"‚Äúa jpeg photo of a‚Äù, ‚Äúa painting
of a‚Äù, ‚Äúa good photo of a‚Äù, ‚Äúa
bad photo of a‚Äù, ‚Äúa photo of a‚Äù,
‚Äúitap of a‚Äù, ‚Äúa rendering of a‚Äù None"
REFERENCES,0.6935933147632312,"None, ‚ÄúIt‚Äôs common in daily
life‚Äù,
‚ÄúIt‚Äôs
beautiful‚Äù,
‚ÄúIt‚Äôs
ugly‚Äù, ‚ÄúI like it‚Äù, ‚ÄúI take it to-
day‚Äù"
REFERENCES,0.6963788300835655,Caltech101
REFERENCES,0.6991643454038997,"‚Äúa photo of a‚Äù, ‚Äúa cropped
photo of a‚Äù, ‚Äúa good photo of
a‚Äù, ‚Äúa bad photo of a‚Äù"
REFERENCES,0.7019498607242339,"None
None, ‚ÄúI like it‚Äù, ‚ÄúI hate it‚Äù,
‚ÄúIt‚Äôs ugly‚Äù, ‚ÄúIt‚Äôs cute‚Äù"
REFERENCES,0.7047353760445683,"Stanford-
Car"
REFERENCES,0.7075208913649025,"‚Äúa photo of a‚Äù, ‚Äúa close-up
photo of a‚Äù, ‚Äúa good photo of
a‚Äù, ‚Äúa bad photo of a‚Äù"
REFERENCES,0.7103064066852368,"‚Äúa type of car‚Äù,
‚Äúa type of auto-
mobile‚Äù"
REFERENCES,0.713091922005571,"‚ÄúI like it‚Äù, ‚ÄúIt belongs to my
friend‚Äù, ‚ÄúIt‚Äôs brand new‚Äù, ‚ÄúIt‚Äôs
popular recently‚Äù, ‚ÄúIt‚Äôs impor-
tant to me‚Äù, ‚ÄúI take it today‚Äù"
REFERENCES,0.7158774373259053,Flowers102
REFERENCES,0.7186629526462396,"‚Äúa photo of a (many) ‚Äù, ‚Äúa ren-
dering of a (many) ‚Äù, ‚Äúitap of a
(many) ‚Äù"
REFERENCES,0.7214484679665738,"‚Äúa
type
of
Ô¨Çower‚Äù, ‚Äúa type
of bloom‚Äù"
REFERENCES,0.724233983286908,"‚ÄúIt‚Äôs beautiful‚Äù, ‚ÄúIt‚Äôs from my
best friend‚Äù, ‚ÄúIt gives out a
sweet perfume/fragrance‚Äù"
REFERENCES,0.7270194986072424,ImageNet
REFERENCES,0.7298050139275766,"‚Äúa photo of a‚Äù, ‚Äùa good photo of
a‚Äù, ‚Äúa bad photo of a‚Äù, ‚Äúa close-
up photo of a‚Äù, ‚Äúitap of a‚Äù None"
REFERENCES,0.7325905292479109,"‚ÄúI like it‚Äù, ‚ÄúIt‚Äôs common in
daily life‚Äù, ‚ÄúIt‚Äôs not common
in daily life‚Äù, ‚ÄúIt‚Äôs ugly‚Äù, ‚ÄúIt‚Äôs
cute‚Äù, ‚ÄúIt‚Äôs beautiful‚Äù"
REFERENCES,0.7353760445682451,"Food101
‚Äúa photo of my‚Äù, ‚Äúa close-up
photo of my‚Äù, ‚Äúitap of my‚Äù"
REFERENCES,0.7381615598885793,"‚Äúa
type
of
food‚Äù, ‚Äúa type
of
nourish-
ment‚Äù"
REFERENCES,0.7409470752089137,"‚ÄúI made it today‚Äù, ‚ÄúI like it‚Äù,
‚ÄúI hate it‚Äù, ‚ÄúIt‚Äôs delicious‚Äù, ‚ÄúIt‚Äôs
with nice Ô¨Çavour‚Äù, ‚ÄúIt‚Äôs with
terrible Ô¨Çavour‚Äù, ‚ÄúIt‚Äôs popular
recently‚Äù"
REFERENCES,0.7437325905292479,SUN397
REFERENCES,0.7465181058495822,"‚Äúa photo of a‚Äù, ‚Äúa good photo of
a‚Äù, ‚Äúa bad photo of a‚Äù, ‚Äúa bright
photo of a‚Äù, a dark photo of a‚Äù,
‚Äúa black and white photo of a‚Äù,
‚Äúa nice scene of a‚Äù, ‚Äúa terrible
scene of a‚Äù None"
REFERENCES,0.7493036211699164,"None, ‚ÄúI like it‚Äù, ‚ÄúI hate it‚Äù,
‚ÄúIt‚Äôs beautiful‚Äù, ‚ÄúIt‚Äôs common
in daily life‚Äù, ‚ÄúIt‚Äôs important to
me‚Äù"
REFERENCES,0.7520891364902507,"DTD
‚Äúitap of a‚Äù, ‚Äúa close-up photo of
a‚Äù"
REFERENCES,0.754874651810585,"‚Äútexture‚Äù,
‚Äúsurface‚Äù,
‚Äúmaterial‚Äù"
REFERENCES,0.7576601671309192,"None, ‚ÄúIt‚Äôs out of style‚Äù, ‚ÄúIt‚Äôs
popular in old days‚Äù,
‚ÄúIt‚Äôs
ugly‚Äù, ‚ÄúIt‚Äôs beautiful‚Äù"
REFERENCES,0.7604456824512534,Aircrafts
REFERENCES,0.7632311977715878,"‚Äúa photo of the‚Äù, ‚Äúa close-up
photo of the‚Äù, ‚Äúa good photo of
the ‚Äù, ‚Äúa pixelated photo of the‚Äù"
REFERENCES,0.766016713091922,"‚Äúa
type
of
plane‚Äù, ‚Äúa type
of aircraft‚Äù, ‚Äúa
type of airliner‚Äù"
REFERENCES,0.7688022284122563,"None,‚ÄúI like it‚Äù, ‚ÄúIt‚Äôs important
to me‚Äù, ‚ÄúI take it today‚Äù, ‚ÄúHope
you like it‚Äù"
REFERENCES,0.7715877437325905,Oxford Pet
REFERENCES,0.7743732590529248,"‚Äúa photo of my‚Äù, ‚Äúa low reso-
lution photo of my‚Äù, ‚Äúa good
photo of my‚Äù"
REFERENCES,0.7771587743732591,"‚Äúa type of pet‚Äù,
‚Äúa type of dog
or cat‚Äù"
REFERENCES,0.7799442896935933,"None, ‚ÄúIt‚Äôs cute‚Äù, ‚ÄúIt‚Äôs impor-
tant to me‚Äù, ‚ÄúI like it‚Äù, ‚ÄúIt‚Äôs
beautiful‚Äù"
REFERENCES,0.7827298050139275,EuroSAT
REFERENCES,0.7855153203342619,"‚Äúa photo of a‚Äù, ‚Äúa painting of a‚Äù,
‚Äúa cropped photo of a‚Äù, ‚Äúa good
photo of a‚Äù, ‚Äúa blurry photo of
a‚Äù"
REFERENCES,0.7883008356545961,"None, ‚Äúan ex-
ample of aerial
or satellite im-
ages‚Äù"
REFERENCES,0.7910863509749304,"None, ‚ÄúI like it‚Äù, ‚ÄúIt‚Äôs taken
from an aircraft or some Ô¨Çying
object‚Äù, ‚ÄúIt‚Äôs collected by imag-
ing satellites‚Äù"
REFERENCES,0.7938718662952646,Published as a conference paper at ICLR 2022
REFERENCES,0.7966573816155988,"Table 12: Prompt templates used for zero-shot image-text retrieval on Flickr30K and MSCOCO
datasets."
REFERENCES,0.7994428969359332,"Dataset
Task
PreÔ¨Åx
SufÔ¨Åx"
REFERENCES,0.8022284122562674,"Flickr30K
image-to-text retrieval
‚Äúa good photo of the‚Äù
‚ÄúI hate it.‚Äù
text-to-image retrieval
‚Äúa good photo of‚Äù
None"
REFERENCES,0.8050139275766016,"MSCOCO
image-to-text retrieval
‚Äúa good photo of‚Äù
‚ÄúIt is ugly.‚Äù
text-to-image retrieval
None
None"
REFERENCES,0.807799442896936,"Image-text Retrieval.
Following CLIP (Radford et al., 2021), we use prompt in zero-shot image-
text retrieval for both Flickr30K and MSCOCO datasets. The prompt is selected by the same rule as
described in Section 3.1.2, except that we do not use ‚Äú[category description]‚Äù here. Table 12 shows
the prompt templates for zero-shot image-text retrieval on Flickr30K and MSCOCO datasets."
REFERENCES,0.8105849582172702,"A.6
LINEAR PROBE ON IMAGE CLASSIFICATION"
REFERENCES,0.8133704735376045,"In this section, we evaluate FILIP on the linear probe for image classiÔ¨Åcation. Following common
linear probe setting, we freeze the whole backbone network and only Ô¨Ånetune the last linear classi-
Ô¨Åer. Since we remove the ‚Äú[CLS]‚Äù token in our vision encoder, we apply a mean pooling over all
the other visual tokens to aggregate them into a global image representation which is then fed into
the linear classiÔ¨Åer."
REFERENCES,0.8161559888579387,"Setting.
Following CLIP, we train the logistic regression classiÔ¨Åer using scikit-learn‚Äôs L-BFGS
implementation (Pedregosa et al., 2011), with maximum 1,000 iterations on those 11 datasets except
ImageNet. For ImageNet, we use a pytorch-based codebase to accelerate the training with GPU.
Following Doersch et al. (2015), we adopt a Batch Normalization (Ioffe & Szegedy, 2015) layer
before the linear classiÔ¨Åer which is beneÔ¨Åcial to stabilize the mixed-precision training. Random
resized crop and horizontal Ô¨Çipping are used to augment training data. We use the cosine learning
rate scheduler with a linear warmup of 10 epochs. More hyperparameters used in linear probe on
ImageNet are shown in Table 13."
REFERENCES,0.8189415041782729,Table 13: Hyperparameters used for linear probe image classiÔ¨Åcation on ImageNet.
REFERENCES,0.8217270194986073,"Hyperparameter
Value"
REFERENCES,0.8245125348189415,"Image size
224 √ó 224
Training epochs
90
Optimizer
SGD
Batch size
4096
Base LR
0.1
Weight decay
0"
REFERENCES,0.8272980501392758,"Results.
Table 14 compares the linear probe performance of our proposed FILIP with CLIP over
12 datasets. Our FILIPbase (resp. FILIPlarge) achieves 85.5% (resp. 91.0%) average Top-1 accuracy
over 12 downstream tasks, which provides noticeable improvements, i.e., 1.8% (resp. 1.2%) higher,
compared to its CLIP‚Äôs counterpart. This implies that our FILIP learns more powerful vision features
which may potentially facilitate border downstream vision tasks."
REFERENCES,0.83008356545961,"A.7
COMPARISON WITH KHATTAB & ZAHARIA (2020)"
REFERENCES,0.8328690807799443,"As is stated in Section 3.1, compared to Khattab & Zaharia (2020), besides being the Ô¨Årst to ap-
ply the late interaction to contrastive learning for vision-language pre-training, we make two other
modiÔ¨Åcations, i.e., removing padded tokens and using average over non-padded tokens instead of
summation. In the following, we show that these two modiÔ¨Åcations are crucial to the performance,
and the quality of Ô¨Åner-granular word-patch alignment."
REFERENCES,0.8356545961002786,"For comparison, we replace the proposed cross-modal late interaction in FILIPbase with the original
late interaction in Khattab & Zaharia (2020). Following the setting in Section 4.4, we pre-train on"
REFERENCES,0.8384401114206128,Published as a conference paper at ICLR 2022
REFERENCES,0.841225626740947,"Table 14: Top-1 accuracy(%) of linear probe on image classiÔ¨Åcation on 12 datasets. Our FILIP
outperforms CLIP by 1.2‚àº1.8% points on average."
REFERENCES,0.8440111420612814,CIFAR10
REFERENCES,0.8467966573816156,CIFAR100
REFERENCES,0.8495821727019499,Caltech101
REFERENCES,0.8523676880222841,StanfordCars
REFERENCES,0.8551532033426184,Flowers102
REFERENCES,0.8579387186629527,Food101
REFERENCES,0.8607242339832869,SUN397 DTD
REFERENCES,0.8635097493036211,Aircrafts
REFERENCES,0.8662952646239555,OxfordPets
REFERENCES,0.8690807799442897,EuroSAT
REFERENCES,0.871866295264624,ImageNet
REFERENCES,0.8746518105849582,Average
REFERENCES,0.8774373259052924,"CLIP-ViT-B/32
95.1
80.5
93.0
81.8
96.6
88.8
76.6
76.5
52.0
90.0
97.0
76.1
83.7
FILIPbase
95.3
80.3
95.0
78.6
98.7
86.2
77.9
78.1
76.6
88.0
95.9
75.8
85.5+1.8"
REFERENCES,0.8802228412256268,"CLIP-ViT-L/14
98.0
87.5
96.5
90.9
99.2
95.2
81.8
82.1
69.4
95.1
98.2
83.9
89.8
FILIPlarge
97.9
87.0
97.2
89.0
99.6
94.6
83.2
83.9
84.8
93.5
97.3
84.5
91.0+1.2"
REFERENCES,0.883008356545961,"the Ô¨Åltered YFCC100M with mixed-precision using 8 V100 GPUs. The batch size per GPU is 512
and the dimension of the token feature is 256. We report results with the top 25% tokens (selected
using the method in Section 3.1) during training. Note that the original late interaction in Khattab &
Zaharia (2020) is sensitive to the temperature in the softmax function, and we report the best result
among several initialization values of the temperature."
REFERENCES,0.8857938718662952,"Effect to Performance.
Table 15 shows the comparison on zero-shot ImageNet classiÔ¨Åcation.
When these two modiÔ¨Åcations are removed, the zero-shot Top-1 accuracy of ImageNet drops from
34.3 to 32.7."
REFERENCES,0.8885793871866295,"Table 15: Comparison of Top-1 Accuracy(%) between the proposed cross-modal late interaction
loss and Khattab & Zaharia (2020) on zero-shot ImageNet classiÔ¨Åcation."
REFERENCES,0.8913649025069638,"ours
late interaction in Khattab & Zaharia (2020)
34.3
32.7"
REFERENCES,0.8941504178272981,"Effect to the Word-patch Alignment
In Figure 4, we compare the word-patch alignment using
the models trained with the proposed cross-modal late interaction and the late interaction in Khattab
& Zaharia (2020). According to the visualizations, using the original late interaction in Khattab &
Zaharia (2020) leads to less accurate word-patch alignment. SpeciÔ¨Åcally, the object patches are often
aligned to the padded tokens instead of class names. We speculate this is because the padded tokens
learn similar representations as existing key textual tokens, similar to the Ô¨Ånding in Section 3.2 of
Khattab & Zaharia (2020) that padding with masked tokens (which is called ‚Äúquery augmentation‚Äù in
Khattab & Zaharia (2020)) tend to ‚Äúre-weigh existing terms based on their importance for matching
the query‚Äù."
REFERENCES,0.8969359331476323,"A.8
ABLATION ON THE FULL PRE-TRAINING DATASET"
REFERENCES,0.8997214484679665,"In Table 16, we compare the proposed cross-modal late interaction loss with the original CLIP
loss (Radford et al., 2019) on the full pre-training dataset introduced in Section 3.3. In Table 16,
CLIP denotes the results reported by CLIP paper, CLIPrep is our reproduced CLIP version with
the original contrastive loss using exactly the same architecture on the same pre-training dataset as
FILIPbase. As can be seen, the FILIPbase has 6.7 points higher average accuracy than the CLIPrep
over 12 datasets. This further veriÔ¨Åes that the performance gain of FILIP comes from the proposed
cross-modal late interaction, rather than the data or architecture."
REFERENCES,0.9025069637883009,"A.9
INFERENCE TIME OF IMAGE-TEXT RETRIEVAL"
REFERENCES,0.9052924791086351,"Setting.
In this section, we test the inference time of both image retrieval and text retrieval on the
test set of Flickr30K and MSCOCO. We compare our proposed model FILIPlarge against SCAN
(Lee et al., 2018) and CLIP (ViT-L/14) (Radford et al., 2021) . We test the inference time of CLIP
and SCAN using their released code. For image retrieval, we precompute the image features and
report the inference time for one text query, which contains (i) the time to extract the feature of
one text query, and (ii) the time of similarity calculation with all images and ranking. Similarly, for"
REFERENCES,0.9080779944289693,Published as a conference paper at ICLR 2022
REFERENCES,0.9108635097493036,"Bald eagle
(5,6)"
REFERENCES,0.9136490250696379,"Killer whale
(5,6)"
REFERENCES,0.9164345403899722,"Bee eater
(5,6)"
REFERENCES,0.9192200557103064,"Damselfly
(5,6,7)"
REFERENCES,0.9220055710306406,"Bullock cart
(5,6)"
REFERENCES,0.924791086350975,"Fireboat
(5,6)"
REFERENCES,0.9275766016713092,Ski (5)
REFERENCES,0.9303621169916435,Mosque (5)
REFERENCES,0.9331476323119777,"Yellow 
lady‚Äôs 
slipper
(5,6,7,8)"
REFERENCES,0.935933147632312,"Necklace 
(5)"
REFERENCES,0.9387186629526463,"Raw image
Patch pred. (FILIP)
Raw image
Patch pred. (colbert)
Patch pred. (FILIP)
Patch pred. (colbert)"
REFERENCES,0.9415041782729805,"Figure 4: Comparison of word-patch alignment between the proposed cross-modal late interaction
and that in ColBERT (Khattab & Zaharia, 2020). ‚Äúa photo of a {label}.‚Äù is the prompt. Numbers in
the parentheses after the class label indicate the location indices of the class label in the tokenized
textual sequence. The correct predictions to the class labels are highlighted by opaque patches with
the class label indices in red. Incorrect predictions to the padded tokens are highlighted by opaque
patches with the padded token indices in blue."
REFERENCES,0.9442896935933147,Published as a conference paper at ICLR 2022
REFERENCES,0.947075208913649,"Table 16: Top-1 accuracy(%) on image classiÔ¨Åcation on 12 datasets. CLIPrep is our reproduced
CLIP trained with the same training data and evaluated with the same prompts as our FILIP. With the
same backbone architecture, our FILIP signiÔ¨Åcantly improves the zero-shot Top-1 average accuracy
over 12 datasets."
REFERENCES,0.9498607242339833,CIFAR10
REFERENCES,0.9526462395543176,CIFAR100
REFERENCES,0.9554317548746518,Caltech101
REFERENCES,0.958217270194986,StanfordCars
REFERENCES,0.9610027855153204,Flowers102
REFERENCES,0.9637883008356546,Food101
REFERENCES,0.9665738161559888,SUN397 DTD
REFERENCES,0.9693593314763231,Aircrafts
REFERENCES,0.9721448467966574,OxfordPets
REFERENCES,0.9749303621169917,EuroSAT
REFERENCES,0.9777158774373259,ImageNet
REFERENCES,0.9805013927576601,Average
REFERENCES,0.9832869080779945,"CLIP
91.3
65.1
87.9
59.4
66.7
84.4
63.2
44.5
21.2
87
49.4
63.2
65.3
CLIPrep
82.0
57.5
89.9
45.1
80.7
75.1
63.6
46.7
33.7
82.7
49.0
64.2
64.2
FILIPbase
86.9
65.5
91.9
55.4
85.3
82.8
69.1
49.3
57.2
88.1
49.9
68.8
70.9"
REFERENCES,0.9860724233983287,"Table 17: Comparison on performance and inference time of image-text retrieval on Flickr30K and
MSCOCO datasets."
REFERENCES,0.9888579387186629,"Recall
Inference time
Flickr30K
MSCOCO
Flickr30K
MSCOCO
image->text
text->image
image->text
text->image
i-to-t
t-to-i
i-to-t
t-to-i
R@1
R@5
R@10
R@1
R@5
R@10
R@1
R@5
R@10
R@1
R@5
R@10"
REFERENCES,0.9916434540389972,"SCAN
67.4
90.3
95.8
48.6
77.7
85.2
50.4
82.2
90.0
38.6
69.3
80.4
4.47s
7ms
21.3s
26ms
CLIP
88.0
98.7
99.4
68.7
90.6
95.2
58.4
81.5
88.1
37.8
62.4
72.2
23ms
8ms
24ms
9ms
FILIP
96.6
100.0
100.0
87.1
97.7
99.1
78.9
94.4
97.4
61.2
84.3
90.6
24ms
8ms
26ms
9ms"
REFERENCES,0.9944289693593314,"text retrieval, we precompute the text features and report the inference time for one image query,
which contains (i) the time to extract the feature of one image query, and (ii) the time of similarity
calculation with all texts and ranking. The test set of Flickr30k contains 1000 images and 5000 texts,
while the test set of COCO contains 5000 images and 25000 texts. The time is averaged over 1000
runs."
REFERENCES,0.9972144846796658,"Results.
The inference time of retrieval is shown in Table 17. BeneÔ¨Åtting from the efÔ¨Åciency opti-
mizations (i.e., FP16 quantization and reduced feature dimension) in Section 3.1, the inference time
of FILIP is close to CLIP. In image retrieval, SCAN is slightly faster than FILIP on Flickr30K with
1000 images, because SCAN uses a lightweight GRU as the text encoder. However, SCAN is much
slower than FILIP (i.e., about 17ms slower per query) on MSCOCO with more (i.e., 5000) images
because of the slower computation involved in the two-stage stacked cross-attention when comput-
ing the similarity. For text retrieval, SCAN is much slower than FILIP and its own image retrieval,
mainly due to three reasons: (i) the image encoder is a Faster RCNN which is more expensive than
the lightweight GRU text encoder; (ii) the text candidates are 5 times more than the image candi-
dates in image retrieval; and (ii) the similarity computation of SCAN relies on the cross-attention
computation, which is not straightforward to be paralleled, even in their ofÔ¨Åcial code; while our
FILIP‚Äôs similarity computation is simply a matrix multiplication and is readily optimized on most
modern hardwares."
