Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001692047377326565,"We investigate the entity alignment (EA) problem with unlabeled dangling cases,
meaning that partial entities have no counterparts in the other knowledge graph
(KG), yet these entities are unlabeled. The problem arises when the source and tar-
get graphs are of different scales, and it is much cheaper to label the matchable pairs
than the dangling entities. To address this challenge, we propose the framework
Lambda for dangling detection and entity alignment. Lambda features a GNN-
based encoder called KEESA with a spectral contrastive learning loss for EA and a
positive-unlabeled learning algorithm called iPULE for dangling detection. Our
dangling detection module offers theoretical guarantees of unbiasedness, uniform
deviation bounds, and convergence. Experimental results demonstrate that each
component contributes to overall performances that are superior to baselines, even
when baselines additionally exploit 30% of dangling entities labeled for training."
INTRODUCTION,0.00338409475465313,"1
Introduction"
INTRODUCTION,0.005076142131979695,"Entity alignment is a problem that seeks entities referring to the same real-world identity across
different knowledge graphs (KGs), and is widely deployed in Ô¨Åelds such as knowledge fusion,
question-answering, web mining, etc. To address the issue, embedding-based methods have been
proposed to capture entity similarity in the embedding space through translation-based [24, 42, 20]
or graph neural network (GNN)-based [41, 37, 27, 26, 35] models. Particularly, if the entities do not
have counterparts on another KG, the entities are referred to as dangling entities, as shown in Fig. 1."
INTRODUCTION,0.00676818950930626,"In many real-world scenarios, the labels for the dangling entities on KGs are often missing, as those
labels are much harder to acquire. For example, in KG plagiarism detection, it is relatively easy to
align entity pairs that both exist in KGs, but one would have to traverse all possible pairs to conclude
an entity is not paired. Hence EA with unlabeled dangling entities is a hard but realistic problem.
The problem even worsens in EA on KGs of different scales where the dangling entities take a large
proportion of all nodes."
INTRODUCTION,0.008460236886632826,"Despite that many works have been investigating the EA problem with dangling entities, few have
focused on EA with unlabeled dangling cases. We list closely related works in Table 1. The work of
[33] extends the conventional EA methods MTransE [5] and AliNet [37] to the case with dangling"
INTRODUCTION,0.01015228426395939,"‚àóCorresponding author: Liyao Xiang, xiangliyao08@sjtu.edu.cn."
INTRODUCTION,0.011844331641285956,"entities, and thus require a portion of labeled dangling entities for training. Based on their works,
MHP [19] has improved performance with additional knowledge, i.e., the high-order proximities
information for alignment. Both UED [24] and SoTead [23] are unsupervised schemes that rely on
side information such as entity name/attribute as global alignment information. Different from prior
works, we consider a stricter case where neither side information nor any labeled dangling entities
are known, as side information often leads to name-bias [21, 44] while labels for dangling entities are
hard to obtain."
INTRODUCTION,0.01353637901861252,"dangling entity in 
source KG"
INTRODUCTION,0.015228426395939087,"dangling entity in
target KG"
INTRODUCTION,0.01692047377326565,matchable entity in target KG
INTRODUCTION,0.018612521150592216,Figure 1: Examples of dangling entities.
INTRODUCTION,0.02030456852791878,"Method
Side Info
Dangling Labels
[33] w/ MTransE


[33] w/ AliNet


UED [24]


SoTead [23]


MHP [19]

 + high-order info
Our Work

"
INTRODUCTION,0.021996615905245348,Table 1: Different EA models with dangling cases.
INTRODUCTION,0.023688663282571912,"The EA with unlabeled dangling entities faces unique challenges: Ô¨Årst, the unlabeled dangling entities
would cause erroneous information to propagate through neighborhood aggregation if applying
conventional GNN-based embedding methods, negatively affecting the dangling detection and
alignment of matchable entities. Second, the absence of labeled dangling entities makes its feature
distribution non-observable, requiring the model to distinguish potential dangling entities while
learning the representation of matchable entities. Hence the EA problem has to be solved with mere
positive (matchable entities with labels) and unlabeled samples, yet without any prior knowledge of
the distribution of the nodes."
INTRODUCTION,0.025380710659898477,"We tackle the Ô¨Årst challenge by proposing a novel GNN-based EA framework. To eliminate the
‚Äòpollution‚Äô of dangling entities, the adaptive dangling indicator has been applied globally for selective
aggregation. Relation projection attention is designed to combine both entity and relation information
for a more comprehensive representation. The designed spectral contrastive learning loss disentangles
the matchable entities from dangling ones while portraying a uniÔ¨Åed embedding space for entity
alignment."
INTRODUCTION,0.02707275803722504,"As to the second challenge, we Ô¨Årst derive an unbiased risk estimator and a tighter uniform deviation
bound for the positive-unlabeled (PU) learning loss. However, such an estimator still requires prior
knowledge of the proportion of positive entities among all nodes. Thus we develop an iterative
strategy to estimate such prior knowledge while training the classiÔ¨Åer with a PU learning loss. The
prior estimation could also be seen as dangling entity detection; if too few entities are determined to
be matchable, one can stop all subsequent procedures and decide the two KGs cannot be aligned."
INTRODUCTION,0.028764805414551606,"Our framework Lambda is provided in Fig. 2 where there are two phases corresponding to two
trained models ‚Äî dangling detection and entity alignment. Both phases share one GNN-based
encoder and the spectral contrastive learning loss. The dangling detection additionally uses a positive-
unlabeled learning loss. The GNN-based encoder contains both the intra-graph and the cross-graph
representation learning modules. After the dangling detection, the estimated proportion of matchable
entities is Ô¨Ågured for judging whether two KGs could be aligned. Only aligned KGs are sent for EA
model training, and then only Ô¨Årst-phase predicted matchable entity embeddings are obtained from
the EA model for inference. Finally, we select pairs of entities that are mutually nearest by their
embeddings as aligned pairs."
INTRODUCTION,0.030456852791878174,"Highlights of our contributions are as follows: we raise the challenging problem of EA with unlabeled
dangling entities for the Ô¨Årst time. To resolve the issue, we propose the framework Lambda featured
by a GNN-based encoder called KEESA with spectral contrastive learning for EA and a positive-
unlabeled learning algorithm for dangling detection called iPULE. We provide a theoretical analysis
of PU learning on the unbiasedness, uniform deviation bound, and convergence. Experiments on a
variety of real-world datasets have demonstrated our alignment performance is superior to baselines,
even the baselines with 30% labeled dangling entities. Our code is available on github2."
INTRODUCTION,0.032148900169204735,"2https://github.com/Handon112358/NeurIPS_2024_Learning-Matchable-Prior-For-Entity-Alignment-with-
Unlabeled-Dangling-Cases t
G s
G pu
ÔÅå"
INTRODUCTION,0.0338409475465313,Iterative Positive Unlabeled Learning
INTRODUCTION,0.03553299492385787,"MLP 
classifier"
INTRODUCTION,0.03722504230118443,"gate
proxy
eh 2"
INTRODUCTION,0.038917089678511,"proxy
eh 1"
EH,0.04060913705583756,"1
eh"
EH,0.04230118443316413,"2
eh"
EH,0.043993231810490696,"f
ei
h"
EH,0.04568527918781726,Dangling Detection
EH,0.047377326565143825,"Entity Alignment
Stop early"
EH,0.049069373942470386,Remove        Dangling
EH,0.050761421319796954,"t
G
s
G
seed alignment anchor Input )
,
( 2
1"
EH,0.05245346869712352,"f
e
f
e h
h"
EH,0.05414551607445008,KG entity encoder with selective aggregation:
EH,0.05583756345177665,"Intra-graph 
Cross-graph 
&"
E,0.05752961082910321,"1e q ÔÄ´
p ÔÄ≠
p"
E,0.05922165820642978,"ÔÄ≠
p
2e"
E,0.06091370558375635,Spectral Contrastive Learning
E,0.06260575296108291,"info
ÔÅå"
E,0.06429780033840947,embedding space
E,0.06598984771573604,1e
E,0.0676818950930626,2e
EH,0.06937394247038917,"1
eh"
EH,0.07106598984771574,"2
eh"
EH,0.0727580372250423,"concantenate ÔÉóÔÉóÔÉó l
eh 1 1 1"
EH,0.07445008460236886,"ÔÄ´
l
eh ÔÉóÔÉóÔÉó 1 1"
EH,0.07614213197969544,"ÔÄ≠
l
eh"
EH,0.077834179357022,detection model
EH,0.07952622673434856,alignment model
EH,0.08121827411167512,positive
EH,0.0829103214890017,labeled
EH,0.08460236886632826,"negative
negative"
EH,0.08629441624365482,"decision boundary
Prior 
Estimation"
EH,0.08798646362098139,Cold  Start
EH,0.08967851099830795,"PU 
Training"
EH,0.09137055837563451,"E step
M step"
EH,0.09306260575296109,Algorithm flow
EH,0.09475465313028765,"Trainable
Product
Neighbour Aggregate"
EH,0.09644670050761421,Intra-graph Representation Learning
EH,0.09813874788494077,Indicator
EH,0.09983079526226735,Relation
EH,0.10152284263959391,"Entity k
j j"
EH,0.10321489001692047,"k
r
r
e
e
r
h
W
r
h
ÔÄΩ
ÔÇÆ ijk
ÔÅ°"
EH,0.10490693739424704,Attention
EH,0.1065989847715736,"1
ÔÄ´
l
ei
h
...
......
..."
EH,0.10829103214890017,"ÔÄæ
ÔÄº
j
k
i
e
r
e
,
,
Triplet ie je
kr ..."
EH,0.10998307952622674,"...
... ..."
EH,0.1116751269035533,Figure 2: The illustration of our framework.
PRELIMINARIES AND RELATED WORK,0.11336717428087986,"2
Preliminaries and Related Work"
ENTITY ALIGNMENT,0.11505922165820642,"2.1
Entity Alignment"
ENTITY ALIGNMENT,0.116751269035533,"Embedding-based entity alignment methods have evolved rapidly and are gradually becoming the
mainstream approach of EA in recent years due to their Ô¨Çexibility and effectiveness [17], which aim
to encode KGs into low-dimensional embedding space to capture the similarities of entities [18, 10].
It could be divided into translation-based [24, 42, 20] and GNN-based [41, 37, 27, 26, 35]."
ENTITY ALIGNMENT,0.11844331641285956,"Previous EA methods mostly assume a one-to-one correspondence between two KGs, but such an
assumption does not always hold and thus leads to a performance drop in real-world cases [38].
Notably, Sun et al. [33] as a pioneering work modeled it upon a supervised setting, i.e., a small set
of aligned entities and labeled dangling entities. On this basis, MHP [19] employed more dangling
information concerning high-order proximities in both training and inference. UED [24] and SoTead
[22] propose an unsupervised translation-based method for joint entity alignment and dangling entity
detection without labeled dangling entities, while the practical problem of matching cost is ignored."
POSITIVE-UNLABLED LEARNING,0.12013536379018612,"2.2
Positive-Unlabled Learning"
POSITIVE-UNLABLED LEARNING,0.1218274111675127,"Let X ‚ààRd, d ‚ààN, and Y ‚àà{¬±1} be the input and output random variables. We also deÔ¨Åne
p(x, y) to be the joint probability density of (X, Y ), pp(x) = p(x | y = +1), pn(x) = p(x | y = ‚àí1)
to be the P (Positive) and N (Negative) marginals (a.k.a., class-conditional densities), and pu(x) be
the U (Unlabeled) marginal. The class-prior probability is expressed as œÄp = p(y = +1), which is
assumed to be known throughout the paper, and can be estimated from known datasets [13]."
POSITIVE-UNLABLED LEARNING,0.12351945854483926,"The PU learning problem setting is as follows: the positive and unlabeled data are sampled inde-
pendently from pp(x) and pu(x) as Xp = {xp
i }np
i=1 ‚àºpp(x) and Xu = {xu
i }nu
i=1 ‚àºpu(x), and a
classiÔ¨Åer is trained from Xp and Xu, in contrast to learning a classiÔ¨Åer telling negative samples apart
from positive ones. The general assumption of the previous work is to let the unlabeled distribution
be equal to the overall data distribution, i.e., pu(x) = p(x) since pu(x) cannot be obtained, but the
assumption hardly holds in many real-world scenarios, for example transductive learning, making
methods in [13, 30] infeasible."
SELECTIVE AGGREGATION WITH SPECTRAL CONTRASTIVE LEARNING,0.12521150592216582,"3
Selective Aggregation with Spectral Contrastive Learning"
SELECTIVE AGGREGATION WITH SPECTRAL CONTRASTIVE LEARNING,0.12690355329949238,"Notation: Source and target KG Gs = (Es, Rs, Ts), Gt = (Et, Rt, Tt) stored in triples <entity,
relation, entity>: entities E, relations R, and triples T ‚äÜE √óR√óE, Es = Ds ‚à™Ms, Et = Dt ‚à™Mt,"
SELECTIVE AGGREGATION WITH SPECTRAL CONTRASTIVE LEARNING,0.12859560067681894,"where D denotes dangling and M denotes matchable. A set of pre-aligned anchor node pairs are
S = {(u, v)|u ‚ààMs, v ‚ààMt, u ‚â°v}. (see appendix A for more details)."
SELECTIVE AGGREGATION WITH SPECTRAL CONTRASTIVE LEARNING,0.13028764805414553,We start by introducing the KEESA (KG Entity Encoder with Selective Aggregation).
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.1319796954314721,"3.1
KG Entity Encoder with Selective Aggregation"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.13367174280879865,"Adaptive Dangling Indicator & Relation Projection Attention. Real-world EA tasks often involve
graphs with dangling distortion [39, 3]. Conventional GNN aggregation will ‚Äòpollute‚Äô matchable
entities‚Äô embeddings with dangling. However, a hard dangling indicator for the entity is over-
conÔ¨Ådent as only approximate results can be obtained without labels. Incorrect indicators may lead
to inappropriate aggregation and thus destruction of the KG structure. Instead, we apply a learnable
scalar weight rej for each ei‚Äôs neighboring message:"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.1353637901861252,"hl+1
ei
= œÉ Ô£´ Ô£¨
Ô£≠
X"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.13705583756345177,"ej‚ààNei‚à™{ei}
tanh(rej)
|
{z
}
adaptive dangling indicator"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.13874788494077833,"Œ±i,jW l+1hl
ej Ô£∂"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.1404399323181049,"Ô£∑
Ô£∏,
(1)"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.14213197969543148,"where tanh serves to normalize rej to the range of [‚àí1, 1]. The initialization of rej is critical, please
see the implementation details for more."
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.14382402707275804,"As compressed feature of ej ‚Äî rej is a plain scalar, we link relation rk‚Äôs embedding hrk to the
associated entity ej by h‚Üíej
rk
for capturing more comprehensive attention. A matrix Wr ‚ààRd√ód
with an orthogonal regularizer Lo is applied to hrk to perform projection while preserving its norm
for better convergence:"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.1455160744500846,"h‚Üíej
rk
= rejWrhrk
and
Lo =
W ‚ä§
r Wr ‚àíId√ód
2 2 ."
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.14720812182741116,"The attention coefÔ¨Åcient is obtained by the following equation, where v‚ä§is the attention vector:"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.14890016920473773,"Œ±l
ijk =
exp(v‚ä§h‚Üíej
rk
)
P
em‚ààNei,<ei,rn,em>‚ààTs‚à™Tt exp(v‚ä§h‚Üíem
rn
)
(2)"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.1505922165820643,"Intra- & Cross-Graph Representation Learning. Based on the above, we can express the embed-
ding of ei at the (l + 1)-th layer hl+1
ei
as Eq. 1, where W l+1 is speciÔ¨Åed as W l+1 = Id√ód ‚àí2hrkh‚ä§
rk
by the triplet < ei, rk, ej > inclusive relation embedding hrk. We adopt the tanh(¬∑) as the activation
function. The Householder transformation W l+1 is applied on the last layer embedding hl
ei to restore
the useful relative positions of KG entities at each layer recursively."
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.15228426395939088,"Overall, the intra-graph representation hei of ei is obtained by concatenating embeddings from all
layers. Its cross-graph representation hproxy
ei
can be described by hei and proxy qj, where the latter
is generated by Proxy Matching Attention Layer [25] to align the embeddings across two graphs.
With Sp representing the set of proxy vectors, and sim(¬∑) denoting the cosine similarity, we have:"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.15397631133671744,"hei = [h0
ei‚à•h1
ei‚à•...‚à•hl
ei]
and
hproxy
ei
=
X qj‚ààSp"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.155668358714044,"exp(sim(hei, qj))
P"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.15736040609137056,"qk‚ààSp exp(sim(hei, qk))(hei ‚àíqj)."
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.15905245346869712,"Finally, we employ a gating mechanism [31] to integrate both intra-graph representation hei and
cross-graph representation hproxy
ei
into hf
ei:"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.16074450084602368,"Œ∏ei = sigmoid(Wghproxy
ei
+ b),
hf
ei = [(Œ∏ei ¬∑ hei + (1 ‚àíŒ∏ei) ¬∑ hproxy
ei
)‚à•rei],"
KG ENTITY ENCODER WITH SELECTIVE AGGREGATION,0.16243654822335024,"where Wg and b are the gate weight and gate bias, respectively. The learnable weight of ei is also
attached to the embedding. It is worth noticing that for each entity on either Gs or Gt, they are
encoded by one shared KEESA with below spectral contrastive learning for a uniÔ¨Åed representation
space."
SPECTRAL CONTRASTIVE LEARNING,0.16412859560067683,"3.2
Spectral Contrastive Learning"
SPECTRAL CONTRASTIVE LEARNING,0.1658206429780034,"In this part, we propose the spectral contrastive learning loss Linfo with high-quality negative
sample mining, which serves both tasks (entity alignment and dangling detection) at the same time."
SPECTRAL CONTRASTIVE LEARNING,0.16751269035532995,"SpeciÔ¨Åcally, given a pre-aligned matchable entity ei ‚ààXp, let there be a paired positive sample entity
ei
+ ‚ààXp, such that (ei, ei
+) ‚ààS, and N sampled entity {ei
j}N as negative samples (ei, ei
j) /‚ààS. The
spectral contrastive learning loss is one speciÔ¨Åc form of alignment loss H(¬∑):"
SPECTRAL CONTRASTIVE LEARNING,0.1692047377326565,"Linfo =
X"
SPECTRAL CONTRASTIVE LEARNING,0.17089678510998307,"ei‚ààXp
log Ô£Æ Ô£∞1 + N
X"
SPECTRAL CONTRASTIVE LEARNING,0.17258883248730963,"j
exp(Œª H(ei, ei
+, ei
j)) Ô£π"
SPECTRAL CONTRASTIVE LEARNING,0.17428087986463622,"Ô£ª.
(3)"
SPECTRAL CONTRASTIVE LEARNING,0.17597292724196278,"UniÔ¨Åed Representation for Entity Alignment. We expect a uniÔ¨Åed feature space where the distance
between aligned anchor node pairs is as close as possible, while the unaligned is on the contrary. To
satisfy this intuition, we introduce an alignment loss:"
SPECTRAL CONTRASTIVE LEARNING,0.17766497461928935,"H(ei, ei
+, ei
j) = [sim(ei, ei
j) ‚àísim(ei, ei
+) + Œ≥]+,
(4)"
SPECTRAL CONTRASTIVE LEARNING,0.1793570219966159,"where [x]+ represents max(0, x) and sim(¬∑) indicates L2-norm distance between the embeddings. A
soft margin Œ≥ is involved to discourage trivial solutions, e.g., sim(ei, ei
j) = sim(ei, ei
+) = 0."
SPECTRAL CONTRASTIVE LEARNING,0.18104906937394247,"Discrimination for Dangling Detection. For our proposed dangling detection, the vital task is
to discriminate the dangling from the matchable ones with unlabeled dangling entities. Hence
unsupervised method of spectral clustering is exploited to separate two types of entities. We design
the loss function according to Lemma 1 to achieve its equivalent effect."
SPECTRAL CONTRASTIVE LEARNING,0.18274111675126903,"Lemma 1. Given one positive sample p+ for q, and N negative samples {p‚àí
j }N (node set: {q, p+}‚à™
{p‚àí
j }N), employing the following loss function is equivalent to conducting spectral clustering on
similarity graph œÄ with the temperature hyper-parameter Œª:"
SPECTRAL CONTRASTIVE LEARNING,0.1844331641285956,"infoNCE(q, p+, {p‚àí}N) = ‚àílog
exp(Œª sim(q, p+))"
SPECTRAL CONTRASTIVE LEARNING,0.18612521150592218,"exp(Œª sim(q, p+)) + PN
j exp(Œª sim(q, p‚àí
j ))
.
(5)"
SPECTRAL CONTRASTIVE LEARNING,0.18781725888324874,"The equivalence has been discussed in previous studies [40, 1, 6, 32]. Regarding our proposed
problem, the positive samples are the corresponding pairs whereas the negative samples are those
sampled unaligned pairs. The equivalence is derived as follows:"
SPECTRAL CONTRASTIVE LEARNING,0.1895093062605753,"infoNCE(q, p+, {p‚àí}N) = log
exp(Œªsim(q, p+)) + PN
j exp(Œªsim(q, p‚àí
j ))"
SPECTRAL CONTRASTIVE LEARNING,0.19120135363790186,"exp(Œª sim(q, p+))"
SPECTRAL CONTRASTIVE LEARNING,0.19289340101522842,= log[1 +
SPECTRAL CONTRASTIVE LEARNING,0.19458544839255498,"PN
j exp(Œª sim(q, p‚àí
j ))"
SPECTRAL CONTRASTIVE LEARNING,0.19627749576988154,"exp(Œª sim(q, p+))
] = log Ô£Æ Ô£∞1 + N
X"
SPECTRAL CONTRASTIVE LEARNING,0.19796954314720813,"j
exp(Œª sim(q, p‚àí
j ) ‚àíŒª sim(q, p+)) Ô£π Ô£ª."
SPECTRAL CONTRASTIVE LEARNING,0.1996615905245347,The spectral contrastive learning loss could be obtained by replacing the exponent term with Eq. 4.
SPECTRAL CONTRASTIVE LEARNING,0.20135363790186125,"Remark. In the alignment loss function, we observe that dangling entities are only in the negative
samples, and the entities in the positive samples are all matchable. Such a stark asymmetry provides
an advantage in distinguishing between dangling and matchable entities."
SPECTRAL CONTRASTIVE LEARNING,0.20304568527918782,We also prove that Linfo (Eq. 3) can promote model learning by Lemma 2 (see appendix B for proof).
SPECTRAL CONTRASTIVE LEARNING,0.20473773265651438,"Lemma 2. The loss Linfo can mine high-quality negative samples, which we show has an equivalent
effect to truncated uniform negative sampling (TUNS) in [35]."
SPECTRAL CONTRASTIVE LEARNING,0.20642978003384094,"Minimizing the spectral contrastive loss of Eq. (3) maps matchable and dangling entities into a uniÔ¨Åed
but distinguishable feature space for improved entity alignment while facilitating dangling detection.
In practice, we adopt the loss normalization trick [8] on H(¬∑) to speed up training."
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.20812182741116753,"4
Iterative Positive-Unlabeled Learning for Dangling Detection"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.2098138747884941,"We expect to avoid any additional computational overhead for EA if few entities are matchable for the
source and target KG. Thus, we address a more challenging problem in EA: given partial pre-aligned"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.21150592216582065,"matchable entities as positive samples (i.e., Xp), how to jointly predict the proportion of matchable
entities in the unlabeled nodes (i.e., œÄu
p) and identify those entities? If œÄu
p could be predicted, it could
serve as an indicator whether we should proceed to EA. We propose to address the problem by PU
learning."
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.2131979695431472,"Unbiased Risk Estimator. First, we propose a new unbiased estimation for PU learning without any
constraint (i.e., pu(x) = p(x) in [13, 30]) on unlabeled samples distribution pu(x) concerning the
overall distribution p(x). Assuming that œÄp and œÄu
p are known (estimation strategy would be given
later), we have:"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.21489001692047377,"Theorem 1. Suppose that g ‚ààG : Rd ‚ÜíR is a binary classiÔ¨Åer, and ‚Ñì: R √ó {¬±1} ‚ÜíR is the loss
function by which ‚Ñì(t, y) means the loss incurred by predicting an output t when the ground truth is
y. bRpu(g) is the unbiased risk estimator of R(g):"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.21658206429780033,"bRpu(g) = œÄp bR+
p (g) + œÄn"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.2182741116751269,"œÄun
¬∑
h
bR‚àí
u (g) ‚àíœÄu
p bR‚àí
p (g)
i
,
(6)"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.21996615905245348,"where œÄn = p(y = ‚àí1) and œÄu
n = pu(y = ‚àí1) are estimable class priors given œÄp and œÄu
p, R+
p (g) =
EX‚àºpp(x)[‚Ñì(g(X), +1)] and R‚àí
n (g) = EX‚àºpn(x)[‚Ñì(g(X), ‚àí1)] (see appendix C for proof)."
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.22165820642978004,"By our proof, bRpu(g) is an unbiased risk bound for the PU learning. More importantly, the bound
provided by Thm. 2 is a tighter uniform deviation bound than the classic Non-negative Risk Estimator
[13]:"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.2233502538071066,"Theorem 2. Let Var(R) denote the uniform deviation bound of risk estimator R, and Non-negative
Risk Estimator be c
R‚Ä≤pu(g), then:Var( bRpu(g)) < Var(c
R‚Ä≤pu(g))(see appendix D for proof)."
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.22504230118443316,"Positive Unlabeled Loss Function. Since it is evident that all negative samples exist in unlabeled
data, we have œÄn"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.22673434856175972,"œÄu
n < 1 and thus we apply a hyper-parameter Œ± = œÄu
n
œÄn to scale œÄp bR+
p (g) equivalently
and max(¬∑) to restrict the estimated œÄnR‚àí
n (g) ‚â•0. The PU learning loss function is formulated as:"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.22842639593908629,"Lpu = Œ±œÄp bR+
p (g) + max{0, bR‚àí
u (g) ‚àíœÄu
p bR‚àí
p (g)},"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.23011844331641285,We specify the corresponding risk function using cross-entropy losses as below:
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.23181049069373943,"bR+
p (g) =
1
|Xp| X"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.233502538071066,"ei‚ààXp
log ÀÜyi(+1), bR‚àí
u (g) =
1
|Xu| X"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.23519458544839256,"ei‚ààXu
log ÀÜyi(‚àí1), bR‚àí
p (g) =
1
|Xp| X"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.23688663282571912,"ei‚ààXp
log ÀÜyi(‚àí1)"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.23857868020304568,"where the output logit for ei ‚ààEs ‚à™Et, being labeled as a state u ‚àà{+1, ‚àí1}, is ÀÜyi(u) =
softmax(MLP(hf
ei)), based on KEESA output hf
ei. Hence each term in the Ô¨Ånal loss can be
calculated or estimated without the negative labels."
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.24027072758037224,"Iterative PU Learning with Prior Estimator How could we estimate prior œÄp and œÄu
p? Inspired
by [11], we introduce a hidden variable in the model as well as an iterative approach. We adopt a
variational approximation strategy and a warm-up phase to tackle the cold start problem, as shown
in Alg. 1. First, we estimate and Ô¨Åx the class prior œÄp and œÄu
p by the ratio of the anchor points
in the training set. Linfo is optimized together with Lpu for a discriminative embedding space in
the warm-up phase. Finally, we minimize Lpu to update the class prior and the model parameters
alternately till convergence."
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.24196277495769883,"The convergence guarantee is provided in Thm. 3, which mostly follows the convergence of EM
algorithm. We collect the proof in appendix E."
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.2436548223350254,"Theorem 3. Given the assumptions of marginalization in Eq. 16 and Eq. 17, the objective function
of ‚àíLpu is the same as the expectation function Q of Eq. 13 where the loss function is the cross en-
tropy function CE(¬Øyi, ÀÜyi) = ‚àí¬Øyi(+1) log ÀÜyi(+1) ‚àí¬Øyi(‚àí1) log ÀÜyi(‚àí1) on the preference condition:
P"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.24534686971235195,"j‚ààU
1
|U| log ÀÜyj(+1)"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.2470389170896785,ÀÜyj(‚àí1) ‚âàP
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.24873096446700507,"i‚ààP
1
|P| log ÀÜyi(+1)"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.25042301184433163,ÀÜyi(‚àí1).
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.2521150592216582,"The iterative process of our method is a special case of the EM algorithm. We hold the same
assumptions as the EM algorithm and we further assume the training of f is able to Ô¨Ånd the globally
optimal Œ∏. Although the assumptions seem to be too strict, the algorithm typically converges in
practice as we veriÔ¨Åed in the experimental section."
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.25380710659898476,Algorithm 1 iPULE (iterative PU Learning with Prior Estimator)
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.25549915397631134,"Require: Gs and Gt are treated as one input graph G = (V, E), positive-node set P = Xp,
unlabeled-node set U = Xu, classiÔ¨Åer f with initial parameters Œ∏0, KEESA Enc(G, œà) with
initial parameters œà0 and warm-up epoch N. L represents training loss.
Ensure: Model parameters Œ∏, œà and estimated prior ÀÜœÄp and ÀÜœÄu
p
1: l ‚Üê‚àû,
ÀÜœÄu
p ‚ÜêÀÜœÄp ‚Üê
|P|
|P|+|U|,
i ‚Üê0,
Œ≤ = Œ≤0;
//Initial value"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.2571912013536379,"2: L ‚ÜêŒ≤ ¬∑ Linfo + (1 ‚àíŒ≤) ¬∑ Lpu;
//Loss of warm-up
3: repeat
4:
X ‚ÜêEnc(G, œà);
//Entity embedding matrix X
5:
Œ∏, œà ‚Üêarg minŒ∏,œà L(Œ∏; X, y, P, U);
//Optimize Enc(¬∑) and f jointly
6:
l ‚ÜêL(Œ∏; X, y, P, U);
7: until N epochs is over
//Warm-up phase to solve cold start
8: L ‚ÜêLpu;
9: repeat
10:
X ‚ÜêEnc(G, œà),
ÀÜyi ‚Üêf(X, i; Œ∏) for all i ‚ààV;"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.25888324873096447,"11:
ÀÜœÄu
p ‚Üê|U|‚àí1 P"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.26057529610829105,"i‚ààU I[ÀÜyi(+1) > 0.5],
ÀÜœÄp ‚Üê
|P|+|U|¬∑ÀÜœÄu
p
|P|+|U| ;
//E step"
ITERATIVE POSITIVE-UNLABELED LEARNING FOR DANGLING DETECTION,0.2622673434856176,"12:
l‚Ä≤ ‚Üêl,
l ‚ÜêL(Œ∏; X, y, P, U);
13:
Œ∏, œà ‚Üêarg maxŒ∏,œà ‚àíL(Œ∏; X, y, P, U);
//M step
14: until |l‚Ä≤ ‚àíl| converge OR ÀÜœÄp converge
15: return"
EXPERIMENTS,0.2639593908629442,"5
Experiments"
EXPERIMENTS,0.2656514382402707,"Our method is evaluated on datasets GA16K, DBP2.0, and GA-DBP15K. DBP2.0 and GA-DBP15K
are used for the veriÔ¨Åcation of iPULE. To address incomparability caused by inconsistent metrics,
we adopt the GA16K dataset to enable compromised comparison of the Dangling-Entities-Unaware
EA method. We further compare our method with dangling aware baselines on DBP2.0. Statistics of
experimental dataset in appendix F, and additional experiment in appendix G."
EXPERIMENTS,0.2673434856175973,"Datasets. The training/test sets for each dataset are generated using a Ô¨Åxed random seed. For entity
alignment, 30% of matchable entity pairs constitute the training set, while the remaining form the test
set. For dangling entity detection, we did not utilize any labeled dangling entity data, in contrast to
prior work which labels an extra 30% of the dangling entities for training [33, 19]."
EXPERIMENTS,0.26903553299492383,"Baselines. Since our work does not take advantage of any side information, we emphasize its
comparison with the previous methods purely depending on graph structures. These works majorly
incorporate two types:"
EXPERIMENTS,0.2707275803722504,"Dangling-Entities-Unaware. We include advanced entity alignment methods in recent years: GCN-
Align [41], RSNs [9], MuGNN [4], KECG [16]. Methods with bootstrapping to generate semi-
supervised structure data are also adopted: BootEA [35], TransEdge [36], MRAEA [26], AliNet [37],
and Dual-AMN [25]."
EXPERIMENTS,0.272419627749577,"Dangling-Entities-Aware. To the best of our knowledge, the method of [33] is the most fairly
comparable baseline which is based on MTransE [5] and AliNet [37]. Because MHP [19] over-
emphasized more use of labeled dangling data like high-order similarity information which is
also based on the above two methods, while SoTead [22] and UED [24] utilize additional side-
information. SoTead [22] and UED [24] can only execute the degraded version on DBP2.0 cause
no side-information is available on that. We exclude them from baselines for our methods. [33]
introduces three techniques to address the dangling entity issue: nearest neighbor (NN) classiÔ¨Åcation,
marginal ranking (MR), and background ranking (BR)."
EXPERIMENTS,0.27411167512690354,"Implementation Detail. We use the Keras framework for developing our approach. Our experiments
are conducted on a workstation with an NVIDIA Tesla A100 GPU, and 80GB memory."
EXPERIMENTS,0.27580372250423013,"By default, the embedding dimension is set to 128 with the depth of GNN set to 2 and a dropout rate
of 0.3. A total of 64 proxy vectors are used and margin Œ≥ = 1. RMSprop optimizer is adopted with a
learning rate of 0.005 and batch size of 5120. Œª is set to be 30. Œ≤ is set as 1e-3 for all datasets. CSLS
[14] is adopted as the distance metric for alignment. As we found, the tanh function changes rapidly"
EXPERIMENTS,0.27749576988155666,"in the region close to 0 but stays stable in the region beyond [‚àí3, 3]. Hence we initialize the rej to 1
to prevent gradients oscillation or near-zero gradients."
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.27918781725888325,"5.1
Experiments of iPULE Convergence and Class Prior Estimation"
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.2808798646362098,"DBP2.0 œÄp between 20%-50% contains more entities to be aligned, trained by pre-aligned 9%-15%
nodes then judged by iPULE as aligned KGs. GA-DBP15K œÄp between 10%-25% are treated as
unaligned KGs ignoring the pre-aligned part, trained by all pre-aligned 10%-25% nodes. We get
accurate estimation and convergence results as shown in Fig. 3. As iPULE progresses, the estimated
class prior gradually approaches the true value as the Ô¨Årst row for GA-DBP15K and the second for
DBP2.0. œÄu
p = 0 for GA-DBP15K while DBP2.0‚Äôs are given by red dotted line respectively. The œÄp
for GA-DBP15K is stably consistent as pre-aligned proportion due to accurate estimation of its œÄu
p.
As common in PU learning [43], iPULE treats more nodes as positive when œÄp ‚âà50% in FR-EN."
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.2825719120135364,"GA-EN
GA-FR
GA-JA
GA-ZH 4e-4 9e-4"
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.28426395939086296,"0
4
8
0 4e-5 8e-5"
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.2859560067681895,1.2e-4 Epoch Prior
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.2876480541455161,Pre-aligned 10% 4e-3 9e-3
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.2893401015228426,"0
6
12
18
2e-4 7e-4"
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.2910321489001692,1.2e-3 Epoch Prior
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.2927241962774958,Pre-aligned 15% 1e-2 2e-2
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.29441624365482233,"0
4
8
1e-3 3e-3 5e-3 Epoch Prior"
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.2961082910321489,Pre-aligned 20% 4e-2 9e-2
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.29780033840947545,"0
2
4
6
2e-3 4e-3 6e-3 8e-3 Epoch Prior"
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.29949238578680204,Pre-aligned 25% p
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.3011844331641286,"u
p
true prior 
p
true prior 
u
p"
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.30287648054145516,"0
1
2
3
Epoch 0.05 0.10 0.15 0.20 Prior"
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.30456852791878175,ZH-EN Pre-aligned 6%
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.3062605752961083,"0
4
8
Epoch 0.1 0.2 0.3 Prior"
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.3079526226734349,JA-EN Pre-aligned 9%
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.3096446700507614,"0
3
6
9
Epoch 0.1 0.2 0.3 0.4 Prior"
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.311336717428088,JA-EN Pre-aligned 12%
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.3130287648054145,"0
4
8
12
Epoch 0.2 0.4 0.6 Prior"
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.3147208121827411,FR-EN Pre-aligned 15%
EXPERIMENTS OF IPULE CONVERGENCE AND CLASS PRIOR ESTIMATION,0.3164128595600677,Figure 3: Prior estimation GA-DBP15K and DBP2.0. (loss convergence in appendix F).
EXPERIMENTS UNAWARE OF DANGLING ENTITIES,0.31810490693739424,"5.2
Experiments Unaware of Dangling Entities"
EXPERIMENTS UNAWARE OF DANGLING ENTITIES,0.3197969543147208,We show the experiments on baselines without considering dangling entities in this section.
EXPERIMENTS UNAWARE OF DANGLING ENTITIES,0.32148900169204736,"Dangling-Entities-Unaware Baselines Comparison. The direct comparison between our method
and the dangling-entities-unaware baselines is unavailable due to inconsistent metrics used. Hence,
we adopt the GA16K dataset as a compromise and do not remove any detected dangling entities
for entity alignment. Thus the ranking list S in Hits@K only contains (matchable) entities in the
source graph since GA16K only contains dangling entities in the target KG. In Tab. 2, Dual-AMN
demonstrates a competitive performance but is inferior to ours at Hits@1. MRAEA performs similarly
to Dual-AMN since the latter is built on the former. TransEdge performs poorly since the method
adopts semi-supervised bootstrapping to mine anchor entities iteratively. The presence of dangling
entities could lead to false anchors and spread of error. Meanwhile, it is also a relation-centric
approach that suffers from insufÔ¨Åcient relation information on GA16K. Other baselines exhibit
up-to-par performance but our method delivers consistently superior or state-of-the-art Hits@Ks."
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.32318104906937395,"5.3
Experiments Aware of Dangling Entities"
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.3248730964467005,We provide a comparison of dangling detection and entity alignment with baselines aware of dangling.
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.32656514382402707,"Dangling Entities Detection Performance. We test our method‚Äôs dangling detection performance
compared with baselines aware of dangling entities. The results on DBP2.0 in the consolidated
setting are reported in Tab. 12. Note that the comparison is unfair as we don‚Äôt use 30% of the labeled a
b c
d"
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.32825719120135366,"Figure 4: Visualization of entity representations
learned by our method on GA16K dataset."
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.3299492385786802,"Method
GA16K
H@1
H@10
H@50
BootEA
13.95
37.25
49.08
TransEdge
0.03
0.12
0.14
MRAEA
63.97
76.64
81.06
GCN-Align
29.48
45.64
57.15
RSNs
9.40
42.70
46.70
MuGNN
62.17
76.25
80.87
KECG
44.18
57.73
63.41
AliNet
48.53
67.72
74.50
Dual-AMN
64.49
80.55
84.67
Ours
67.59
80.33
84.35"
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.3316412859560068,"Table 2: Performance comparison with dangling-
entities-unaware baselines on GA16K."
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.3333333333333333,"Methods
ZH-EN
EN-ZH
JA-EN
EN-JA
FR-EN
EN-FR"
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.3350253807106599,"Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1"
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.33671742808798644,AliNet
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.338409475465313,"NNC
.676
.419 .517
.738
.558 .634
.597
.482 .534
.761
.120 .207
.466
.365 .409
.545
.162 .250
MR
.752
.538 .627
.828
.505 .627
.779
.580 .665
.854
.543 .664
.552
.570 .561
.686
.549 .609
BR
.762
.556 .643
.829
.515 .635
.783
.591 .673
.846
.546 .663
.547
.556 .552
.674
.556 .609"
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.3401015228426396,MTransE
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.34179357021996615,"NNC
.604
.485 .538
.719
.511 .598
.622
.491 .549
.686
.506 .583
.459
.447 .453
.557
.543 .550
MR
.781
.702 .740
.866
.675 .759
.799
.708 .751
.864
.653 .744
.482
.575 .524
.639
.613 .625
BR
.811
.728 .767
.892
.700 .785
.816
.733 .772
.888
.731 .801
.539
.686 .604
.692
.735 .713"
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.34348561759729274,"Ours
.763
.925 .836
.844
.909 .875
.807
.836 .821
.880
.809 .843
.615
.772 .685
.732
.749 .740"
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.34517766497461927,Table 3: Dangling detection results on DBP2.0 in the consolidated setting.
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.34686971235194586,"Methods
ZH-EN
EN-ZH
JA-EN
EN-JA
FR-EN
EN-FR"
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.34856175972927245,"Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1"
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.350253807106599,AliNet
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.35194585448392557,"NNC
.121
.193 .149
.085
.138 .105
.113
.146 .127
.067
.208 .101
.126
.148 .136
.086
.161 .112
MR
.207
.299 .245
.159
.320 .213
.231
.321 .269
.178
.340 .234
.195
.190 .193
.160
.200 .178
BR
.203
.286 .238
.155
.308 .207
.223
.306 .258
.170
.321 .222
.183
.181 .182
.164
.200 .180"
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.3536379018612521,MTransE
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.3553299492385787,"NNC
.164
.215 .186
.118
.207 .150
.180
.238 .205
.101
.167 .125
.185
.189 .187
.135
.140 .138
MR
.302
.349 .324
.231
Àô362
.282
.313
.367 .338
.227
.366 .280
.260
.220 .238
.213
.224 .218
BR
.312
.362 .335
.241
.376 .294
.314
.363 .336
.251
.358 .295
.265
.208 .233
.231
.213 .222"
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.3570219966159052,"Ours
.279
.447 .344
.219
.489 .303
.324
.409 .362
.234
.460 .310
.234
.320 .271
.192
.363 .251"
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.3587140439932318,Table 4: Entity alignment results on DBP2.0 in the consolidated setting.
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.3604060913705584,"dangling entities as the baselines. Nevertheless, our approach maintains SOTA performance across
all six datasets, excelling in almost every metric except for a slightly inferior precision."
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.36209813874788493,"Dangling-Entities-Aware Baselines Comparison. Tab. 4 reports the entity alignment performance
comparison in the consolidated setting on DBP2.0. The precision, recall, and F1 scores are computed
according to Eq. (19), (20), (21) in Metric part of appendix F, respectively. We test the entity
alignment performance of our method in comparison with baselines that are aware of dangling
entities. Our method still maintains almost state-of-the-art performance, but there is still a slightly
inferior precision problem. It makes us wonder about the reasons behind it."
EXPERIMENTS AWARE OF DANGLING ENTITIES,0.3637901861252115,"How does our method work?
To understand why our method works and its precision slightly
suffers, we visualized all entity embeddings of GA16K in Fig. 4. As shown above, matchable entities
are denoted as red and green in source and target KG respectively, while dangling as blue. The
distribution of three types of entity in Fig. 4(a) suggests our method maps all nodes into a uniÔ¨Åed
embedding space where matchable entities exhibit considerable overlap and are appropriately aligned
(shown in Fig. 4(b)). Fig. 4(c)(d) depicts that a part of the dangling entities is intertwined with the
matchable ones, suggesting that this part resides at the decision boundary and easily leads to false
positives which explain the lesser precision of our method."
ABLATION STUDIES AND VARYING ANCHOR NODES,0.36548223350253806,"5.4
Ablation Studies and Varying Anchor Nodes"
ABLATION STUDIES AND VARYING ANCHOR NODES,0.36717428087986465,"We conduct ablation studies to show each module‚Äôs impact, and similarly pre-aligned entities‚Äô impact."
ABLATION STUDIES AND VARYING ANCHOR NODES,0.3688663282571912,"Ablation Studies The impact of adaptive dangling indicator and relation projection attention in our
method are investigated. We denote the counterpart removing rei as w/o rei, and replacing h‚Üíej
rk
with the original hrk as w/o h‚Üíej
rk
. Fig. 5 gives the ablation study results on DBP2.0, where ‚ÄòOurs‚Äô
represents an all-inclusive model. We observe that the h‚Üíej
rk
has a more substantial impact than rei
to the alignment performance. As to why the rei has a minor impact on the alignment, we consider
it may be attributed to the lower degrees of dangling entities on DBP2.0. The degrees of dangling
entities are generally lower than that of matchable ones, indicating that the dangling is more isolated
in the graph and thus has less impact on matchable nodes in the neighborhood aggregation. 0.4 0.45 0.5 0.55 0.6 0.65 ZH-EN EN-ZH JA-EN EN-JA FR-EN EN-FR"
ABLATION STUDIES AND VARYING ANCHOR NODES,0.37055837563451777,Hits@1
ABLATION STUDIES AND VARYING ANCHOR NODES,0.37225042301184436,w/o           R w/o E Ours 0.2 0.25 0.3 0.35 0.4 ZH-EN EN-ZH JA-EN EN-JA FR-EN EN-FR F1 0.3 0.35 0.4 0.45 0.5 0.55 ZH-EN EN-ZH JA-EN EN-JA FR-EN EN-FR
ABLATION STUDIES AND VARYING ANCHOR NODES,0.3739424703891709,Recall 0.15 0.2 0.25 0.3 0.35 ZH-EN EN-ZH JA-EN EN-JA FR-EN EN-FR
ABLATION STUDIES AND VARYING ANCHOR NODES,0.3756345177664975,Precision
ABLATION STUDIES AND VARYING ANCHOR NODES,0.377326565143824,ùëüùëüùëíùëíùëñùëñ ‚Ñéùëüùëüùëòùëò
ABLATION STUDIES AND VARYING ANCHOR NODES,0.3790186125211506,"‚Üíùëíùëíùëóùëó
w/o"
ABLATION STUDIES AND VARYING ANCHOR NODES,0.38071065989847713,Figure 5: The ablation study of entity alignment performance in the consolidated setting on DBP2.0. 0.15 0.23 0.31 0.39 0.47 0.55
ABLATION STUDIES AND VARYING ANCHOR NODES,0.3824027072758037,"0.05
0.1
0.2
0.3"
ABLATION STUDIES AND VARYING ANCHOR NODES,0.3840947546531303,Recall
ABLATION STUDIES AND VARYING ANCHOR NODES,0.38578680203045684,Anchor Ratio 0.15 0.2 0.25 0.3 0.35 0.4
ABLATION STUDIES AND VARYING ANCHOR NODES,0.38747884940778343,"0.05
0.1
0.2
0.3 F1"
ABLATION STUDIES AND VARYING ANCHOR NODES,0.38917089678510997,Anchor Ratio 0.1 0.15 0.2 0.25 0.3 0.35
ABLATION STUDIES AND VARYING ANCHOR NODES,0.39086294416243655,"0.05
0.1
0.2
0.3"
ABLATION STUDIES AND VARYING ANCHOR NODES,0.3925549915397631,Precision
ABLATION STUDIES AND VARYING ANCHOR NODES,0.3942470389170897,Anchor Ratio 0.2 0.3 0.4 0.5 0.6 0.7
ABLATION STUDIES AND VARYING ANCHOR NODES,0.39593908629441626,"0.05
0.1
0.2
0.3"
ABLATION STUDIES AND VARYING ANCHOR NODES,0.3976311336717428,Hits@1
ABLATION STUDIES AND VARYING ANCHOR NODES,0.3993231810490694,Anchor Ratio ZH-EN EN-ZH JA-EN EN-JA FR-EN EN-FR
ABLATION STUDIES AND VARYING ANCHOR NODES,0.4010152284263959,Figure 6: The entity alignment performance on varying pre-aligned anchor nodes ratios on DBP2.0.
ABLATION STUDIES AND VARYING ANCHOR NODES,0.4027072758037225,"Varying Anchor Nodes. Pre-aligned entities may be far scarce in reality. The sensitivity of our
method to the proportion variation of anchor nodes is investigated. As the proportion increases, the
alignment performance enhances as provided in Fig. 6."
ABLATION STUDIES AND VARYING ANCHOR NODES,0.40439932318104904,"Notably, even with an anchor ratio as low as 5%, our alignment accuracy still well exceeds 30% on
most datasets except for FR-EN and EN-FR. Cause they contain twice as many entities and triples
as ZH-EN and JA-EN, which introduces intricate dependencies among entities and thus greater
challenges in alignment. Moreover, a larger graph may require a higher dimension of representations
to learn, but the embedding dimension is restricted to merely 96 due to the out-of-memory problem."
CONCLUSION,0.40609137055837563,"6
Conclusion"
CONCLUSION,0.4077834179357022,"We found that previous EA methods suffer from great performance decline if dangling entities
are considered. Our goal is to address the EA problem with unlabeled dangling entities. A novel
framework Lambda for detecting dangling entities and then pairing alignment is proposed. The core
idea is to perform selective aggregation with spectral contrastive learning and to adopt theoretically
guaranteed PU learning to relieve the dependence on the labeled dangling entities. Experimental
results on multiple representative datasets demonstrate the effectiveness of our proposed approach.
This work also has important implications for real-world applications, such as EA of different scales,
KG plagiarism detection, etc."
CONCLUSION,0.40947546531302875,Acknowledgments and Disclosure of Funding
CONCLUSION,0.41116751269035534,"The research was supported in part by NSF China (No. 61960206002, 62272306, 62032020,
62136006)."
CONCLUSION,0.4128595600676819,"The authors would like to thank the reviewers for their constructive comments and appreciate the
Student Innovation Center of SJTU for providing GPUs. Hang Yin personally thanks Chenyu Liu,
Yuting Feng, Jingyuan Zhou, and Qingyang Liu for feedbacks on early versions of this paper. Hang
Yin would also like to thank Professor Yuan Luo for his inspiration in the information theory course
(CS7317) of Shanghai Jiao Tong University."
REFERENCES,0.41455160744500846,References
REFERENCES,0.41624365482233505,"[1] Hugues Van Assel, Thibault Espinasse, Julien Chiquet, and Franck Picard. A probabilistic
graph coupling view of dimension reduction. In Adcances in neural information processing
systems (NeurIPS), pages 10696‚Äì10708, 2022."
REFERENCES,0.4179357021996616,"[2] S√∂ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and
Zachary G. Ives. Dbpedia: A nucleus for a web of open data. In international semantic
web conference (ISWC), pages 722‚Äì735, 2007."
REFERENCES,0.4196277495769882,"[3] Khalid Belhajjame and Mohamed-Yassine Mejri. Online maintenance of evolving knowledge
graphs with rdfs-based saturation and why-provenance support. Journal of Web Semantics
(JoWS), 78:100796, 2023."
REFERENCES,0.4213197969543147,"[4] Yixin Cao, Zhiyuan Liu, Chengjiang Li, Juanzi Li, and Tat-Seng Chua. Multi-channel graph neu-
ral network for entity alignment. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1452‚Äì1461, 2019."
REFERENCES,0.4230118443316413,"[5] Muhao Chen, Yingtao Tian, Mohan Yang, and Carlo Zaniolo. Multilingual knowledge graph
embeddings for cross-lingual knowledge alignment. In Proceedings of the 26th International
Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI), pages 1511‚Äì1517, 2017."
REFERENCES,0.42470389170896783,"[6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework
for contrastive learning of visual representations. In International conference on machine
learning (ICML), pages 1597‚Äì1607, 2020."
REFERENCES,0.4263959390862944,"[7] Cheng Deng, Yuting Jia, Hui Xu, Chong Zhang, Jingyao Tang, Luoyi Fu, Weinan Zhang,
Haisong Zhang, Xinbing Wang, and Chenghu Zhou. Gakg: A multimodal geoscience academic
knowledge graph. In Proceedings of the 30th ACM International Conference on Information &
Knowledge Management (CIKM), pages 4445‚Äì4454, 2021."
REFERENCES,0.428087986463621,"[8] Yunjun Gao, Xiaoze Liu, Junyang Wu, Tianyi Li, Pengfei Wang, and Lu Chen. Clusterea:
Scalable entity alignment with stochastic training and normalized mini-batch similarities. In
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD), pages 421‚Äì
431, 2022."
REFERENCES,0.42978003384094754,"[9] Lingbing Guo, Zequn Sun, and Wei Hu. Learning to exploit long-term relational dependencies in
knowledge graphs. In International conference on machine learning (ICML), pages 2505‚Äì2514,
2019."
REFERENCES,0.43147208121827413,"[10] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A survey on
knowledge graphs: Representation, acquisition, and applications. IEEE Transactions on Neural
Networks and learning systems (TNNLS), 33(2):494‚Äì514, 2021."
REFERENCES,0.43316412859560066,"[11] Yibo Jiang and Bryon Aragam. Learning nonparametric latent causal graphs with unknown
interventions. Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.43485617597292725,"[12] Thomas N Kipf and Max Welling. Semi-supervised classiÔ¨Åcation with graph convolutional
networks, 2016."
REFERENCES,0.4365482233502538,"[13] Ryuichi Kiryo, Gang Niu, Marthinus C Du Plessis, and Masashi Sugiyama. Positive-unlabeled
learning with non-negative risk estimator. In Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing Systems (NIPS), pages
1675‚Äì1685, 2017."
REFERENCES,0.43824027072758037,"[14] Guillaume Lample, Alexis Conneau, Marc‚ÄôAurelio Ranzato, Ludovic Denoyer, and Herv√© J√©gou.
Word translation without parallel data. In International Conference on Learning Representations
(ICLR), 2018."
REFERENCES,0.43993231810490696,"[15] Kasper Green Larsen and Jelani Nelson. Optimality of the johnson-lindenstrauss lemma. In 2017
IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 633‚Äì638,
2017."
REFERENCES,0.4416243654822335,"[16] Chengjiang Li, Yixin Cao, Lei Hou, Jiaxin Shi, Juanzi Li, and Tat-Seng Chua. Semi-supervised
entity alignment via joint knowledge embedding model and cross-graph model. In Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing, EMNLP-IJCNLP, pages
2723‚Äì2732, 2019."
REFERENCES,0.4433164128595601,"[17] Yangning Li, Jiaoyan Chen, Yinghui Li, Yuejia Xiang, Xi Chen, and Hai-Tao Zheng. Vision,
deduction and alignment: An empirical study on multi-modal knowledge graph alignment. In
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages
1‚Äì5. IEEE, 2023."
REFERENCES,0.4450084602368866,"[18] Yangning Li, Yinghui Li, Xi Chen, Hai-Tao Zheng, and Ying Shen. Active relation discovery:
Towards general and label-aware open relation extraction, 2023."
REFERENCES,0.4467005076142132,"[19] Juncheng Liu, Zequn Sun, Bryan Hooi, Yiwei Wang, Dayiheng Liu, Baosong Yang, Xiaokui
Xiao, and Muhao Chen. Dangling-aware entity alignment with mixed high-order proximities.
Findings of the Association for Computational Linguistics: NAACL 2022, 2022."
REFERENCES,0.44839255499153974,"[20] Xiao Liu, Haoyun Hong, Xinghao Wang, Zeyi Chen, Evgeny Kharlamov, Yuxiao Dong, and Jie
Tang. Selfkg: Self-supervised entity alignment in knowledge graphs. In ACM Web Conference
(WWW), pages 860‚Äì870, 2022."
REFERENCES,0.4500846023688663,"[21] Xiaoze Liu, Junyang Wu, Tianyi Li, Lu Chen, and Yunjun Gao. Unsupervised entity alignment
for temporal knowledge graphs. In Proceedings of the ACM Web Conference 2023, pages
2528‚Äì2538, 2023."
REFERENCES,0.4517766497461929,"[22] Gongxu Luo, Jianxin Li, Hao Peng, Carl Yang, Lichao Sun, Philip S. Yu, and Lifang He.
Graph entropy guided node embedding dimension selection for graph neural networks. In
Proceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence, IJCAI,
pages 2767‚Äì2774, 2021."
REFERENCES,0.45346869712351945,"[23] Shengxuan Luo, Pengyu Cheng, and Sheng Yu. Semi-constraint optimal transport for entity
alignment with dangling cases, 2022."
REFERENCES,0.45516074450084604,"[24] Shengxuan Luo and Sheng Yu. An accurate unsupervised method for joint entity alignment and
dangling entity detection. In Findings of the Association for Computational Linguistics (ACL),
pages 2330‚Äì2339, 2022."
REFERENCES,0.45685279187817257,"[25] Xin Mao, Wenting Wang, Yuanbin Wu, and Man Lan. Boosting the speed of entity alignment
10√ó: Dual attention matching network with normalized hard sample mining. In Proceedings of
the Web Conference 2021 (WWW), pages 821‚Äì832, 2021."
REFERENCES,0.45854483925549916,"[26] Xin Mao, Wenting Wang, Huimin Xu, Man Lan, and Yuanbin Wu. Mraea: an efÔ¨Åcient and
robust entity alignment approach for cross-lingual knowledge graph. In Proceedings of the 13th
International Conference on Web Search and Data Mining (WSDM), pages 420‚Äì428, 2020."
REFERENCES,0.4602368866328257,"[27] Xin Mao, Wenting Wang, Huimin Xu, Yuanbin Wu, and Man Lan. Relational reÔ¨Çection entity
alignment. In ACM International Conference on Information & Knowledge Management
(CIKM), pages 1095‚Äì1104, 2020."
REFERENCES,0.4619289340101523,"[28] Xinnian Mao, Wenting Wang, Yuanbin Wu, and Man Lan. Lightea: A scalable, robust, and
interpretable entity alignment framework via three-view label propagation. In Proceedings of
the 2022 Conference on Empirical Methods in Natural Language Processing, pages 825‚Äì838,
2022."
REFERENCES,0.46362098138747887,"[29] Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012."
REFERENCES,0.4653130287648054,"[30] Gang Niu, Marthinus Christoffel Du Plessis, Tomoya Sakai, Yao Ma, and Masashi Sugiyama.
Theoretical comparisons of positive-unlabeled learning against positive-negative learning. In Ad-
vances in Neural Information Processing Systems 29: Annual Conference on Neural Information
Processing Systems (NeurIPS), pages 1199‚Äì1207, 2016."
REFERENCES,0.467005076142132,"[31] Rupesh Kumar Srivastava, Klaus Greff, and J√ºrgen Schmidhuber. Highway networks, 2015."
REFERENCES,0.4686971235194585,"[32] Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao Wang, and
Yichen Wei. Circle loss: A uniÔ¨Åed perspective of pair similarity optimization. In Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 6398‚Äì
6407, 2020."
REFERENCES,0.4703891708967851,"[33] Zequn Sun, Muhao Chen, and Wei Hu. Knowing the no-match: Entity alignment with dangling
cases. In Proceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing
(ACL-IJCNLP), pages 3582‚Äì3593, 2021."
REFERENCES,0.4720812182741117,"[34] Zequn Sun, Wei Hu, and Chengkai Li. Cross-lingual entity alignment via joint attribute-
preserving embedding. In International Semantic Web Conference (ISWC), pages 628‚Äì644,
2017."
REFERENCES,0.47377326565143824,"[35] Zequn Sun, Wei Hu, Qingheng Zhang, and Yuzhong Qu. Bootstrapping entity alignment
with knowledge graph embedding. In International Joint Conference on ArtiÔ¨Åcial Intelligence
(IJCAI), volume 18, pages 4396‚Äì4402, 2018."
REFERENCES,0.4754653130287648,"[36] Zequn Sun, Jiacheng Huang, Wei Hu, Muhao Chen, Lingbing Guo, and Yuzhong Qu. Transedge:
Translating relation-contextualized embeddings for knowledge graphs. In The Semantic Web‚Äì
ISWC 2019: 18th International Semantic Web Conference (ISWC), pages 612‚Äì629, 2019."
REFERENCES,0.47715736040609136,"[37] Zequn Sun, Chengming Wang, Wei Hu, Muhao Chen, Jian Dai, Wei Zhang, and Yuzhong Qu.
Knowledge graph alignment network with gated multi-hop neighborhood aggregation. In AAAI
Conference on ArtiÔ¨Åcial Intelligence (AAAI), volume 34, pages 222‚Äì229, 2020."
REFERENCES,0.47884940778341795,"[38] Zequn Sun, Qingheng Zhang, Wei Hu, Chengming Wang, Muhao Chen, Farahnaz Akrami,
and Chengkai Li. A benchmarking study of embedding-based entity alignment for knowledge
graphs. Proceedings of the VLDB Endowment, 13(12), 2020."
REFERENCES,0.4805414551607445,"[39] Anil Surisetty, Deepak Chaurasiya, Nitish Kumar, Alok Singh, Gaurav Dhama, Aakarsh Mal-
hotra, Ankur Arora, and Vikrant Dey. Reps: Relation, position and structure aware entity
alignment. In Companion Proceedings of the Web Conference 2022 (WWW), pages 1083‚Äì1091,
2022."
REFERENCES,0.48223350253807107,"[40] Zhiquan Tan, Yifan Zhang, Jingqin Yang, and Yang Yuan. Contrastive learning is spectral
clustering on similarity graph, 2023."
REFERENCES,0.48392554991539766,"[41] Zhichun Wang, Qingsong Lv, Xiaohan Lan, and Yu Zhang. Cross-lingual knowledge graph
alignment via graph convolutional networks. In Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 349‚Äì357, 2018."
REFERENCES,0.4856175972927242,"[42] Kun Xu, Liwei Wang, Mo Yu, Yansong Feng, Yan Song, Zhiguo Wang, and Dong Yu. Cross-
lingual knowledge graph alignment via graph matching neural network. In Conference of the
Association for Computational Linguistics (ACL), pages 3156‚Äì3161, 2019."
REFERENCES,0.4873096446700508,"[43] Jaemin Yoo, Junghun Kim, Hoyoung Yoon, Geonsoo Kim, Changwon Jang, and U Kang.
Graph-based pu learning for binary and multiclass classiÔ¨Åcation without class prior. Knowledge
and Information Systems, 64(8):2141‚Äì2169, 2022."
REFERENCES,0.4890016920473773,"[44] Ziheng Zhang, Hualuo Liu, Jiaoyan Chen, Xi Chen, Bo Liu, Yuejia Xiang, and Yefeng Zheng.
An industry evaluation of embedding-based entity alignment. In Proceedings of the 28th
International Conference on Computational Linguistics: Industry Track, pages 179‚Äì189, 2020."
REFERENCES,0.4906937394247039,Appendix
REFERENCES,0.49238578680203043,"A
Notation"
REFERENCES,0.494077834179357,"A.1
DeÔ¨Ånitions"
REFERENCES,0.4957698815566836,"DeÔ¨Ånition 1 (Knowledge graph). Knowledge graph (KG) is a directed graph G = (E, R, T)
comprising three distinct sets: entities E, relations R, and triples T ‚äÜE √ó R √ó E. KG is stored in
the form of triples <entity, relation, entity>, with entities denoted by nodes and the relation between
entities deÔ¨Åned by edges.
DeÔ¨Ånition 2 (Entity alignment). Given source KG and target KG, corresponding to Gs =
(Es, Rs, Ts) and Gt = (Et, Rt, Tt) respectively, and A = {(u, v)|u ‚ààEs, v ‚ààEt, u ‚â°v} a set
of pre-aligned anchor node pairs, where ‚â°indicates equivalence, the goal of entity alignment is to
identify additional pairs of potentially equivalent entities using information from Gs, Gt, and A. This
task typically assumes a one-to-one correspondence between Es and Et.
DeÔ¨Ånition 3 (Entity alignment with dangling cases). Let entities in the source and target graphs be
composed of two types of nodes: Es = Ds ‚à™Ms, Et = Dt ‚à™Mt, where Ds, Dt denote dangling
sets that contain entities that have no counterparts, and Ms, Mt are matchable sets. A set of pre-
aligned anchor node pairs are S = {(u, v)|u ‚ààMs, v ‚ààMt, u ‚â°v}. The task seeks to discover the
remaining aligned entities given Gs, Gt, and S."
REFERENCES,0.49746192893401014,"A.2
Transductive Learning:"
REFERENCES,0.49915397631133673,"Transductive learning models are trained from observed, speciÔ¨Åc (training) cases to speciÔ¨Åc (test)
cases, employing both training and test information except for test labels. In contrast, the inductive
learning model is reasoning from observed training cases to general rules, which are then applied to
the test cases. Let‚Äôs get down to the EA task. As KG structure is accessible through given triples,
which can accurately describe the connections between entities. When we attempt to Ô¨Ågure out entity
alignment tasks, the KG structure information of the whole source and target KGs is what we could
exploit, covering potentially (test) equivalent entities‚Äô relative positions. That‚Äôs why we conÔ¨Åne the
problem Ô¨Åeld to transductive learning."
REFERENCES,0.5008460236886633,"A.3
Graph Convolutional Networks"
REFERENCES,0.5025380710659898,"For graph convolutional networks (GCN) [12], the embedding hl+1
ei
of node ei at the l + 1-th layer is
updated iteratively by aggregating node features of the neighboring nodes Nei from the prior layer:"
REFERENCES,0.5042301184433164,"hl+1
ei
= œÉ Ô£´ Ô£≠
X"
REFERENCES,0.505922165820643,"ej‚ààNei‚à™{ei}
Œ±i,jW l+1hl
ej Ô£∂"
REFERENCES,0.5076142131979695,"Ô£∏,
(7)"
REFERENCES,0.5093062605752962,"where each embedding hl
ei represents the d-dimensional embedding vector of ei, Œ±i,j denotes the
weight coefÔ¨Åcient between ei and ej, W l+1 being the transformation matrix of the (l + 1)-th GNN
layer, and œÉ being the activation function."
REFERENCES,0.5109983079526227,"B
Proof for Lemma 2"
REFERENCES,0.5126903553299492,"Proof. The Linfo also has a good effect on mining high-quality negative samples, which we show has
an equivalent effect to truncated uniform negative sampling (TUNS) in [35]. TUNS points out that
negative samples obtained by uniform random sampling are highly redundant since only high-quality
negative samples improve the model. Thus TUNS chooses the K-nearest neighbors of the ei as the
negative samples, which are most challenging to distinguish. In the special case of K = 1, the loss
can be written as LTUNS = P"
REFERENCES,0.5143824027072758,"ei‚ààXp maxj(H(ei, ei
+, ei
j)). If we approximate the max function by
the LogSumExp, the contrastive loss function turns to"
REFERENCES,0.5160744500846024,"LTUNS ‚âà
X ei‚ààXp"
REFERENCES,0.5177664974619289,"1
Œª log Ô£´ Ô£≠
N
X"
REFERENCES,0.5194585448392555,"j
exp(Œª H(ei, ei
+, ei
j)) Ô£∂"
REFERENCES,0.5211505922165821,"Ô£∏,
(8)"
REFERENCES,0.5228426395939086,"minimizing which is equivalent to minimizing Eq. (3). Hence our contrastive learning loss is actually a
special form of TUNS. For randomly sampled negative samples, Linfo can play a role in preferentially
optimizing high-quality negative samples."
REFERENCES,0.5245346869712352,"C
Proof for Theorem 1"
REFERENCES,0.5262267343485617,"Proof. The risk of g is R(g) = E(X,Y )‚àºp(x,y)[‚Ñì(g(X), Y )] = œÄpR+
p (g) + œÄnR‚àí
n (g) in posi-
tive negative learning problems.
In positive-unlabeled learning where Xn is unavailable, we
can only approximate R(g) by positive samples and unlabeled samples. We represent the un-
labeled distribution as pu(x) = œÄu
npn(x) + œÄu
ppp(x), so that the negative distribution can be
written as œÄnpn(x) =
œÄn
œÄu
n ¬∑

pu(x) ‚àíœÄu
ppp(x)

. Provided R‚àí
p (g) = EX‚àºpp(x)[‚Ñì(g(X), ‚àí1)] and
R‚àí
u (g) = EX‚àºpu(x)[‚Ñì(g(X), ‚àí1)], we obtain that"
REFERENCES,0.5279187817258884,"œÄnR‚àí
n (g) = œÄn"
REFERENCES,0.5296108291032149,"œÄun
¬∑

R‚àí
u (g) ‚àíœÄu
pR‚àí
p (g)

,
(9)"
REFERENCES,0.5313028764805414,"and
bRpu(g) = œÄp bR+
p (g) + œÄn"
REFERENCES,0.5329949238578681,"œÄun
¬∑
h
bR‚àí
u (g) ‚àíœÄu
p bR‚àí
p (g)
i
.
(10)"
REFERENCES,0.5346869712351946,"SpeciÔ¨Åcally, œÄn = 1 ‚àíœÄp, œÄu
n = 1 ‚àíœÄu
p could be derived as class-prior probability given œÄp and œÄu
p.
In particular, the ratio of labeled positive samples could be precisely Ô¨Ågured out as œÄtr
p in transductive"
REFERENCES,0.5363790186125211,"learning, given which œÄu
p =
œÄp‚àíœÄtr
p
1‚àíœÄtr
p , œÄu
n = 1 ‚àíœÄu
p could be derived as class-prior probability."
REFERENCES,0.5380710659898477,"D
Proof for Theorem 2"
REFERENCES,0.5397631133671743,"Proof. Rn,q is deÔ¨Åned as the Rademacher complexity of the class of classiÔ¨Åers G for the sampling of
size n from distribution q(x). From [30] we have that with probability at least 1 ‚àíŒ¥/2, the uniform
deviation bounds below hold separately:"
REFERENCES,0.5414551607445008,"sup
g‚ààG
| bR+(g) ‚àíR+(g)| ‚â§2L‚ÑìRn+,pp(G) + s"
REFERENCES,0.5431472081218274,ln(4/Œ¥) 2n+
REFERENCES,0.544839255499154,"‚âúM+ > 0,"
REFERENCES,0.5465313028764806,"sup
g‚ààG
| bRu,‚àí(g) ‚àíRu,‚àí(g)| ‚â§2L‚ÑìRnu,pu(G) + s"
REFERENCES,0.5482233502538071,ln(4/Œ¥)
NU,0.5499153976311336,2nu
NU,0.5516074450084603,"‚âúM‚àí> 0,"
NU,0.5532994923857868,"where L‚Ñìis the Lipschitz constant of loss ‚Ñìin its Ô¨Årst parameter and n+ is the number of positive
samples while nu is that of unlabeled. Following the symmetric condition assumption in [30], it is
obviously holds that:"
NU,0.5549915397631133,"Var(c
R
‚Ä≤
pu(g)) = 2œÄpM+ + M‚àí, while"
NU,0.55668358714044,"Var( bRpu(g)) = (œÄp + œÄn ¬∑ œÄu
p
œÄun
)M+ + œÄn"
NU,0.5583756345177665,"œÄun
M‚àí"
NU,0.560067681895093,holds in our setting. Then it is evident that œÄn
NU,0.5617597292724196,"œÄp < œÄu
n
œÄu
p , i.e.,
œÄn¬∑œÄu
p
œÄu
n
< œÄp and œÄn"
NU,0.5634517766497462,"œÄu
n < 1."
NU,0.5651438240270727,"Consequently, comparing the coefÔ¨Åcients of M+ and M‚àíleads to the conclusion that bRpu(g) could
possess tighter uniform deviation bound than that of Non-negative Risk Estimator [13]."
NU,0.5668358714043993,"E
Convergence Proof"
NU,0.5685279187817259,"The expectation‚Äìmaximization (EM) algorithm is an iterative approach to maximize the likelihood
p(y|X; Œ∏) of target variables y over input variables X and parameters Œ∏. The EM algorithm works
iteratively, and each iteration consists of an expectation (E) step and a maximization (M) step. The E
step computes the expectation of the log-likelihood concerning the conditional distribution of the
latent variable z given the current parameters Œ∏(t) at the t-th iteration:"
NU,0.5702199661590525,"Q(Œ∏ | Œ∏(t)) = Ez‚àºp(z|X,y,Œ∏(t))[log p(y, z | X, Œ∏)].
(11)"
NU,0.571912013536379,"Then, the M step Ô¨Ånds a set of parameters that maximizes the computed expectation:"
NU,0.5736040609137056,"Œ∏(t+1) = arg max
Œ∏
Q(Œ∏ | Œ∏(t)).
(12)"
NU,0.5752961082910322,"Due to the maximization step, we get the following inequality naturally:"
NU,0.5769881556683587,"Q(Œ∏(t+1) | Œ∏(t)) ‚àíQ(Œ∏(t) | Œ∏(t)) ‚â•0.
(13)"
NU,0.5786802030456852,"Lemma 3 (Convergence of EM algorithm [29]). It is guaranteed that the EM algorithm always im-
proves log p(y | X, Œ∏) by increasing the value of Q(Œ∏ | Œ∏(t)). Since log p(y | X, Œ∏) is monotonically
bounded, the EM must converge."
NU,0.5803722504230119,Proof. The following equation holds for any z:
NU,0.5820642978003384,"log p(y | X, Œ∏) = log p(y, z | X, Œ∏) ‚àílog p(z | X, y, Œ∏)."
NU,0.583756345177665,"We take the expectation over p(z | X, y, Œ∏(t)) for both sides as follows:"
NU,0.5854483925549916,"Ep(z|X,y,Œ∏(t))[log p(y | X, Œ∏)]"
NU,0.5871404399323181,"= Ep(z|X,y,Œ∏(t))[log p(y, z | X, Œ∏)] ‚àíEp(z|X,y,Œ∏(t))[log p(z | X, y, Œ∏)]"
NU,0.5888324873096447,"= Q(Œ∏ | Œ∏(t)) + H(p(z | X, y, Œ∏) | p(z | X, y, Œ∏(t)))"
NU,0.5905245346869712,‚âúQ(Œ∏ | Œ∏(t)) + H(pŒ∏ | pŒ∏(t))
NU,0.5922165820642978,"where H stands for the entropy. If we substitute Œ∏(t) for Œ∏, we get the following:"
NU,0.5939086294416244,"log p(y | X, Œ∏(t)) = Q(Œ∏(t) | Œ∏(t)) + H(pŒ∏(t) | pŒ∏(t)).
(14)"
NU,0.5956006768189509,"Gibbs‚Äô inequality states that H(q | p) ‚àíH(p | p) ‚â•0 always holds for any distribution p and q.
Hence we have:"
NU,0.5972927241962775,"log p(y | X, Œ∏(t+1)) ‚àílog p(y | X, Œ∏(t))"
NU,0.5989847715736041,= Q(Œ∏(t+1) | Œ∏(t)) + H(pŒ∏(t+1) | pŒ∏(t)) ‚àíQ(Œ∏(t) | Œ∏(t)) ‚àíH(pŒ∏(t) | pŒ∏(t))
NU,0.6006768189509306,= Q(Œ∏(t+1) | Œ∏(t)) ‚àíQ(Œ∏(t) | Œ∏(t)) + H(pŒ∏(t+1) | pŒ∏(t)) ‚àíH(pŒ∏(t) | pŒ∏(t))
NU,0.6023688663282571,‚â•Q(Œ∏(t+1) | Œ∏(t)) ‚àíQ(Œ∏(t) | Œ∏(t)) = Q(Œ∏(t+1) | Œ∏(t)) ‚àíQ(Œ∏(t)) ‚â•0.
NU,0.6040609137055838,Now we provide the proof for Theorem 3.
NU,0.6057529610829103,"Proof. According to Lemma 3, we have that the EM algorithm converges. Next, we model our
problem and give the approximate equivalence between our algorithm and the EM algorithm to prove
our algorithm converges."
NU,0.6074450084602369,"The latent variables z represent the true label distribution of unlabeled samples, where ÀÜyi(u) =
f(X, i; Œ∏) is the probability of node i being labeled as u ‚àà{+1, ‚àí1} by the current classiÔ¨Åer f.
Given œÄu
p, the label distribution of all unlabeled samples is:"
NU,0.6091370558375635,"p(zi) =
ÀÜœÄu
p
if
zi = +1,
1 ‚àíÀÜœÄu
p
if
zi = ‚àí1.
(15)"
NU,0.61082910321489,"First, the conditional distribution p(z | X, y, Œ∏(t)) of latent variables given the current parameters
Œ∏(t) is approximated by:
p(z | X, y, Œ∏(t)) ‚âà
Y"
NU,0.6125211505922166,"i‚ààU
p(zi).
(16)"
NU,0.6142131979695431,"Second, the joint distribution p(y, z | X, Œ∏) of labeled and unlabeled nodes with new parameters Œ∏ is
approximated by the classiÔ¨Åer f, which is also considered as a marginalization function that gives the
label distribution of each node based on all given information:"
NU,0.6159052453468697,"p(y, z | X, Œ∏) ‚âà
Y"
NU,0.6175972927241963,"i‚ààP
ÀÜyi(+1)
Y"
NU,0.6192893401015228,"j‚ààU
ÀÜyj(zj)
(17)"
NU,0.6209813874788495,"We propose to use the average log-likelihood differences to measure the classiÔ¨Åcation preference
of the model. It can be transformed into the following form by ‚àÜU and ‚àÜP, representing that on
domain U and P:
‚àÜU = 1 |U| X"
NU,0.622673434856176,"j‚ààU
log ÀÜyj(+1) ‚àílog ÀÜyj(‚àí1)"
NU,0.6243654822335025,"‚àÜP =
1
|P| X"
NU,0.626057529610829,"i‚ààP
log ÀÜyi(+1) ‚àílog ÀÜyi(‚àí1)"
NU,0.6277495769881557,"If the mean value is positive, it means that the logarithmic probability of positive classes is higher
than that of negative classes in the given domain, and vice versa. When we ignore the preference of
the classiÔ¨Åcation model, this actually describes the category feature bias in certain domains."
NU,0.6294416243654822,"Let‚Äôs rethink the meaning of preference condition. If the model has a similar classiÔ¨Åcation preference
in domain U and P, we can express as ‚àÜU ‚âà‚àÜP. This condition can be arranged as the mathematical
expression of the condition given by Theorem 3 through the properties of the log function."
NU,0.6311336717428088,"‚àÜU ‚âà‚àÜP ‚â°
X j‚ààU"
NU,0.6328257191201354,"1
|U| log ÀÜyj(+1)"
NU,0.6345177664974619,"ÀÜyj(‚àí1) ‚âà
X i‚ààP"
NU,0.6362098138747885,"1
|P| log ÀÜyi(+1)"
NU,0.637901861252115,ÀÜyi(‚àí1)
NU,0.6395939086294417,"We derive ‚àíLpu from Eq.( 7), since the goal of training is to minimize the objective function:"
NU,0.6412859560067682,"1
N Ez‚àºp(z|X,y,Œ∏(t))[log p(y, z | X, Œ∏)] = 1 N X"
NU,0.6429780033840947,"z
p(z | X, y, Œ∏(t)) log p(y, z | X, Œ∏) ‚âà1 N X"
NU,0.6446700507614214,"z
p(z | X, y, Œ∏(t))(
X"
NU,0.6463620981387479,"i‚ààP
log ÀÜyi(+1) +
X"
NU,0.6480541455160744,"j‚ààU
log ÀÜyj(zj)) = 1 N X"
NU,0.649746192893401,"i‚ààP
log ÀÜyi(+1) + 1 N X j‚ààU X"
NU,0.6514382402707276,"zj‚àà¬±1
p(zj) log ÀÜyj(zj) = |P| N ¬∑ 1 |P| X"
NU,0.6531302876480541,"i‚ààP
log ÀÜyi(+1) + |U| N ¬∑ 1 |U| X j‚ààU"
NU,0.6548223350253807," 
ÀÜœÄu
p log ÀÜyj(+1) + (1 ‚àíÀÜœÄu
p) log ÀÜyj(‚àí1)
 = |P| N ¬∑ 1 |P| X"
NU,0.6565143824027073,"i‚ààP
log ÀÜyi(+1) + |U| N ¬∑ 1 |U| X"
NU,0.6582064297800339,"j‚ààU
log ÀÜyj(‚àí1) + ÀÜœÄu
p|U| N
‚àÜU"
NU,0.6598984771573604,"We replaced ‚àÜU with ‚àÜP, and this process is completed by the above preference condition. ‚âà|P| N ¬∑ 1 |P| X"
NU,0.6615905245346869,"i‚ààP
log ÀÜyi(+1) + |U| N ¬∑ 1 |U| X"
NU,0.6632825719120136,"j‚ààU
log ÀÜyj(‚àí1) + |U|ÀÜœÄu
p
N
¬∑ 1 |P| X"
NU,0.6649746192893401,"i‚ààP
log ÀÜyi(+1) ‚àílog ÀÜyi(‚àí1)"
NU,0.6666666666666666,"= ‚àí|P| + |U| ¬∑ ÀÜœÄu
p
|P| + |U| X i‚ààP"
NU,0.6683587140439933,"1
|P| log ÀÜyi(+1) ‚àí"
NU,0.6700507614213198,"|N |
|P|+|U| |N | |U| Ô£´ Ô£≠X j‚ààU"
NU,0.6717428087986463,"1
|U| log ÀÜyj(‚àí1) ‚àíÀÜœÄu
p
X i‚ààP"
NU,0.6734348561759729,"1
|P| log ÀÜyi(‚àí1) !"
NU,0.6751269035532995,"Datasets
# Entities
# Rel.
# Triples
# Dang
# Align"
NU,0.676818950930626,"DBP2.0ZH‚àíEN Chinese
84,996
3,706
286,067
51,813
33,183
English
118,996
3,402
586,868
85,813
DBP2.0JA‚àíEN Japanese
100,860
3,243
347,204
61,090
39,770
English
139,304
3,396
668,341
99,534
DBP2.0FR‚àíEN French
221,327
2,841
802,678
97,375
123,952
English
278,411
4,598
1,287,231
154,459
DBP15KZH‚àíENChinese
19,388
1,701
70,414
4,388
15,000
English
19,572
1,323
95,142
4,572
DBP15KJA‚àíENJapanese
19,814
1,299
77,214
4,814
15,000
English
19,780
1,153
93,484
4,780
DBP15KFR‚àíENFrench
19,661
903
105,998
4,661
15,000
English
19,993
1,208
115,722
4,993
GA16K
None
6,208
8
68,534
0
6,208
None
16,363
12
151,662
10,155"
NU,0.6785109983079526,"Table 5: Statistics of DBP2.0, DBP15K and GA16K."
NU,0.6802030456852792,"Datasets
# Entities
# Rel.
# Triples
# Dang
# Align"
NU,0.6818950930626058,"DBP2.0ZH‚àíENPlus Chinese
69,386
3,455
241,588
36,302
33,084
English
94,026
3,131
470,284
60,942"
NU,0.6835871404399323,"DBP2.0JA‚àíENPlus Japanese
82,192
3,011
291,406
42,588
39,604
English
110,362
3,054
532,988
70,758"
NU,0.6852791878172588,"DBP2.0ZH‚àíENMinusChinese
72,252
3,351
200,400
54,594
17,658
English
107,853
3,140
421,597
90,195"
NU,0.6869712351945855,"DBP2.0JA‚àíENMinusJapanese
86,241
3,014
236,546
64,841
21,400
English
126,558
3,166
485,133
105,158"
NU,0.688663282571912,Table 6: Statistics of DBP2.0-Plus and DBP2.0-Minus
NU,0.6903553299492385,"Denote œÄp =
|P|+|U|¬∑ÀÜœÄu
p
|P|+|U| , œÄn =
|N|
|P|+|U| and œÄu
n = |N |"
NU,0.6920473773265652,|U| . The above formula is equivalent to:
NU,0.6937394247038917,"arg max
Œ∏
1
N Ez‚àºp(z|X,y,Œ∏(t))[log p(y, z | X, Œ∏)]"
NU,0.6954314720812182,"= arg max
Œ∏
‚àíœÄp bR+
p (g) ‚àíœÄn"
NU,0.6971235194585449,"œÄun
¬∑
h
bR‚àí
u (g) ‚àíœÄu
p bR‚àí
p (g)
i"
NU,0.6988155668358714,"|
{z
}"
NU,0.700507614213198,"‚àíb
Rpu(g)"
NU,0.7021996615905245,"= arg min
Œ∏
bRpu(g) = arg max
Œ∏
‚àíLpu (18)"
NU,0.7038917089678511,"F
Statistics of Experimental Dataset and Baselines"
NU,0.7055837563451777,"Datasets. The training/test sets for each dataset are generated using a Ô¨Åxed random seed. For entity
alignment, 30% of matchable entity pairs constitute the training set, while the remaining form the test
set. For dangling entity detection, we did not utilize any labeled dangling entity data, in contrast to
prior work which labels 30% of the dangling entities and matchable pairs respectively for training
[33]. Hence our method imposes minimal restrictions on annotated data. All datasets are brieÔ¨Çy
introduced in the following and some statistics are provided in Tab. 7."
NU,0.7072758037225042,"In addition to the existing datasets, we also constructed DBP2.0-minus & -plus as supplementary
to DBP2.0, GA16K enabling comparison between Dangling-Entities-Unaware baselines, and GA-
DBP15K for evaluation of iPULE."
NU,0.7089678510998308,"DBP15K3 [34]: DBP15K consists of three cross-lingual subsets constructed from DBpedia: English-
French(DBPFR‚àíEN), English-Chinese (DBPZH‚àíEN), English-Japanese(DBPJA‚àíEN). Each subset"
NU,0.7106598984771574,3https://paperswithcode.com/dataset/dbp15k
NU,0.7123519458544839,"GA-DBP15K
Entities
Dang
Align"
NU,0.7140439932318104,"GA ‚àíEN
GA
16,363
16,363 - Align
16,363‚àóc%
EN-share
19,388 + Align
19,388
GA ‚àíZH
GA
16,363
16,363 - Align
16,363‚àóc%
ZH-share
19,572 + Align
19,572
GA ‚àíJA
GA
16,363
16,363 - Align
16,363‚àóc%
JA-share
19,814 + Align
19,814
GA ‚àíFR
GA
19,388
16,363 - Align
16,363‚àóc%
FR-share
19,661 + Align
19,661"
NU,0.7157360406091371,"Table 7: Statistics of GA-DBP15K. c = [25%,20%,15%,10%]."
NU,0.7174280879864636,"contains 15,000 pre-aligned entity pairs. This dataset includes a small proportion of dangling entity
samples which is yet mostly ignored in previous entity alignment tasks."
NU,0.7191201353637902,"DBP2.04 [33]: DBP2.0 is an entity alignment dataset with a considerable proportion of dangling
entities, constructed from the multilingual Infobox Data of DBpedia [2]. The dataset contains three
pairs of crosslingual KGs, ZH-EN (Chinese to English), JA-EN (Japanese to English), and FR-EN
(French to English). Since there are dangling nodes in both the source and target graphs, we separately
test source-to-target and target-to-source alignment, consistent with the established approach. A
representative feature of the dataset is that the matchable and dangling entities exhibit similar degree
distributions which are hard to distinguish, displaying a real-world challenge in aligning knowledge
graphs. Based on DBP2.0, we extend the following -minus & -plus datasets for veriÔ¨Åcation of iPULE
on different positive proportions between 20%-50%."
NU,0.7208121827411168,"DBP2.0-plus: In the construction of the plus dataset, our goal is to construct the dataset that has a
higher œÄp, and we realize this by reducing a few existing dangling entities on ZH-EN and JA-EN.
We randomly delete dangling entities from both source and target KG equally and remove triples
containing them. The constructed DBP2.0-plus are reindexed and thus obtain a higher œÄp value than
the original dataset."
NU,0.7225042301184433,"DBP2.0-minus: In contrast, to lower the œÄp value. Given the constraint of preventing new dangling
entities that could introduce false information to the KG, we can only reduce the number of matchable
entities. Given source and target KG, removing one entity from a pair makes the remaining entity
dangling. We randomly delete matchable entities from one side of the pair on both source and target
KG uniformly. The constructed DBP2.0-minus are reindexed and thus obtain a lower œÄp value than
the original dataset."
NU,0.7241962774957699,"GA16K: This dataset constructed by us exclusively contains dangling nodes in the target graph,
facilitating a comparison between our work and baselines that neglect dangling entities. GA16K is
extracted from GAKG5 [7], a Geoscience Academic Knowledge Graph. We Ô¨Årst order each type of
entity in GAKG according to their degrees and select the entities with a large degree into the entity
set. A total of 16,363(16K) separate entities and their relations were extracted to compose the target
graph. Then we extract 6,208 entities from the target graph to comprise the source graph. Hence there
are 6,208 ground-truth matchable pairs between the source and the target. The remaining 10,155
entities in the target graph are regarded as dangling entities."
NU,0.7258883248730964,"GA-DBP15K: The GA-DBP15K dataset is derived from a subset of entities within GA16K, along
with their associated triples, which are then concatenated with the DBP15K dataset, such as EN,
resulting in a new dataset pair that shares a proportion of common entities. To achieve the goal, we
Ô¨Årst extract a certain proportion of triples from GA16K. We then reindex all the entities from the
extricated GA16K and DBP15K datasets. Finally, we update the entity and relation indices in the
triples, replacing them with the newly assigned indices."
NU,0.727580372250423,"Baselines. Since our work does not take advantage of any side information, we emphasize its
comparison with the previous methods purely depending on graph structures. These works majorly
incorporate two types:"
NU,0.7292724196277496,"Dangling-Entities-Unaware. We include advanced entity alignment methods in recent years: GCN-
Align [41], RSNs [9], MuGNN [4], KECG [16]. Methods with bootstrapping to generate semi-"
NU,0.7309644670050761,"4https://github.com/nju-websoft/OpenEA/tree/master/dbp2.0
5https://github.com/davendw49/gakg"
NU,0.7326565143824028,"supervised structure data are also adopted: BootEA [35], TransEdge [36], MRAEA [26], AliNet [37],
and Dual-AMN [25]."
NU,0.7343485617597293,"Dangling-Entities-Aware. To the best of our knowledge, the method of [33] is the most fairly
comparable baseline which is based on MTransE [5] and AliNet [37]. Because MHP [19] over-
emphasized more use of labeled dangling data like high-order similarity information which is
also based on the above two methods, while SoTead [22] and UED [24] utilize additional side-
information. SoTead [22] and UED [24] can only execute the degraded version on DBP2.0 cause
no side-information is available on that. We exclude them from baselines for our methods. [33]
introduces three techniques to address the dangling entity issue: nearest neighbor (NN) classiÔ¨Åcation,
marginal ranking (MR), and background ranking (BR)."
NU,0.7360406091370558,"Metrics are set for the dangling entity detection task and the entity alignment task separately. For
the entity detection, we evaluate the detection performance by the standard precision, recall, and F1
score. To align the previous dangling detection baselines, we detect dangling entities as ‚Äòpositive‚Äô
samples and align matchable entities for entity alignment."
NU,0.7377326565143824,"For the entity alignment, the metrics slightly differ in the dangling-entities-unaware and dangling-
entities-aware settings. We evaluate the baselines unaware of the dangling entities by following their
assumptions and using their metric Hits@K (K= 1, 10, 50, H@K for short) on the ranking list S.
This setting is referred to as relaxed setting when S is composed of all ground-truth entities without
the dangling ones:"
NU,0.739424703891709,"Hits@K = 1 |S| |S|
X"
NU,0.7411167512690355,"k=1
1(ranki ‚â§k)."
NU,0.7428087986463621,"In contrast, we refer to a consolidated setting for baselines aware of dangling entities. In this setting,
the ranking list S also contains all dangling entities. We use H@K in the consolidated setting to
evaluate the performance of baselines aware of but not removing dangling entities in the alignment.
For baselines where dangling entities are detected and removed before alignment, the direct use of
H@K to evaluate entity alignment may not be precise, since errors are introduced in the detection
phase. Thus we follow the convention of [33] to apply a set of metrics for evaluating the accuracy of
entity alignment in the consolidated setting. Each of them is derived and introduced as follows."
NU,0.7445008460236887,The standard precision and recall is given as
NU,0.7461928934010152,"precision =
TP
TP + FP , recall =
TP
TP + FN"
NU,0.7478849407783418,"for dangling entity detection. We denote the dangling entities as 0 and matchable ones as 1. The
subscript t1y suggests that an entity with ground truth y is classiÔ¨Åed as matchable. Likewise, ty1
represents a matchable entity that is classiÔ¨Åed as y by the detection classiÔ¨Åer. If a source entity
is dangling but not identiÔ¨Åed by the detection module, its alignment result is always considered
incorrect, i.e., H@Kt10 = 0. Hence we have the precision for entity alignment as"
NU,0.7495769881556683,"H@1t1y = precision ¬∑ H@1t11 + (1 ‚àíprecision) ¬∑ H@1t10
= precision ¬∑ H@1t11.
(19)"
NU,0.751269035532995,"Similarly, if a matchable entity is falsely excluded by the dangling detection module, this test case
is also regarded as incorrect H@Kt01 = 0 since the alignment model has no chance to search for
alignment. Hence we have the recall for entity alignment as"
NU,0.7529610829103215,"H@1ty1 = recall ¬∑ H@1t11 + (1 ‚àírecall) ¬∑ H@1t01
= recall ¬∑ H@1t11.
(20)"
NU,0.754653130287648,"For the methods that are aware of dangling entities, we use H@1t1y and H@1ty1 to denote the
precision and recall of the entity alignment task. Similarly, we deÔ¨Åne the F1 score of the entity
alignment as the harmonic average of precision and recall:"
NU,0.7563451776649747,F1 = 2 ¬∑ H@1t1y ¬∑ H@1ty1
NU,0.7580372250423012,"H@1t1y + H@1ty1
.
(21)"
NU,0.7597292724196277,"Later, H@1t1y and H@1ty1 are referred to as Prec. and Rec. in reporting alignment performance."
NU,0.7614213197969543,"G
Additional Experiment"
NU,0.7631133671742809,"RQ1: How do current network alignment methods perform in unlabeled dangling cases? (see
appendix G.1)"
NU,0.7648054145516074,RQ2: Loss convergence on GA-DBP15K and DBP2.0. (see appendix G.2)
NU,0.766497461928934,RQ3: How do we select the embedding dimensions? (see appendix G.3)
NU,0.7681895093062606,RQ4: What is the actual efÔ¨Åciency of our approach? (see appendix G.4)
NU,0.7698815566835872,RQ5: Baseline comparison under different pre-aligned seeds? (see appendix G.5)
NU,0.7715736040609137,RQ6: Additional experiments involved LightEA as a strong baseline? (see appendix G.6)
NU,0.7732656514382402,"G.1
The Non-Negligibility of dangling Problem (RQ1)."
NU,0.7749576988155669,"We investigated the performance degradation of various existing EA methods in the face of the
dangling problem, which shows that this problem is worth considering."
NU,0.7766497461928934,"Method
DBP15KZH‚àíEN
DBP15KJA‚àíEN
DBP15KFR‚àíEN
H@1
H@10
H@50
H@1
H@10
H@50
H@1
H@10
H@50
BootEA
31.30‚Üì20.96
59.70‚Üì16.18
71.51‚Üì12.91
33.77‚Üì15.27
62.66‚Üì11.64
73.09‚Üì10.29
23.11‚Üì26.72
58.39‚Üì18.77
71.54‚Üì14.00
TransEdge
49.91‚Üì15.21
76.62‚Üì9.79
83.44‚Üì7.16
54.07‚Üì13.42
78.01‚Üì8.25
84.00‚Üì6.21
48.23‚Üì17.34
79.32‚Üì9.70
86.69‚Üì6.24
MRAEA
59.45‚Üì5.62
83.04‚Üì2.53
88.68‚Üì1.56
61.60‚Üì4.45
83.48‚Üì2.21
88.65‚Üì1.50
61.55‚Üì6.62
85.85‚Üì2.61
90.79‚Üì1.69
GCN-Align
31.99‚Üì10.70
62.21‚Üì6.45
71.93‚Üì4.31
32.08‚Üì10.08
61.04‚Üì5.86
70.34‚Üì3.52
30.71‚Üì10.50
61.64‚Üì7.07
72.45‚Üì5.55
RSNs
43.00‚Üì8.50
62.90‚Üì8.00
69.70‚Üì7.00
20.60‚Üì31.60
44.60‚Üì26.60
53.20‚Üì23.60
36.30‚Üì15.30
63.30‚Üì10.10
71.70‚Üì7.80
MuGNN
34.66‚Üì14.75
68.48‚Üì9.32
80.53‚Üì5.69
32.93‚Üì14.68
66.68‚Üì8.82
78.63‚Üì5.67
34.93‚Üì14.02
68.88‚Üì9.69
81.67‚Üì5.32
KECG
35.92‚Üì12.87
65.70‚Üì10.35
76.44‚Üì8.06
32.31‚Üì15.48
63.19‚Üì11.96
74.42‚Üì9.29
32.84‚Üì15.47
64.78‚Üì11.98
76.70‚Üì8.35
AliNet
53.84‚Üì0.66
73.73‚Üì3.16
80.30‚Üì1.59
52.69‚Üì1.30
74.01‚Üì2.60
80.91‚Üì1.90
54.01‚Üì0.58
76.19‚Üì2.74
83.25‚Üì1.40
Dual-AMN
60.72‚Üì12.20
83.93‚Üì5.22
89.45‚Üì3.54
62.29‚Üì10.62
83.38‚Üì5.35
88.80‚Üì3.21
65.33‚Üì10.48
87.76‚Üì4.17
92.47‚Üì2.24"
NU,0.7783417935702199,"Table 8: Network alignment performance on DBP15K in the consolidated setting. The blue numbers
suggest the drop from the relaxed setting (as with their original implementation)."
NU,0.7800338409475466,"We reproduce the baselines unaware of dangling entities on DBP15K in the relaxed setting. On the
same dataset, we rerun their methods but in a consolidated setting that takes the dangling entities into
account. Even though DBP15K only comprises a small percentage of dangling entities, the drop in
the consolidated setting is signiÔ¨Åcant, as shown in Tab. 8."
NU,0.7817258883248731,"The reason behind such a performance drop is mainly because most previous works remove dangling
entities from the ground truth in measuring their alignment performance. In particular, Dual-AMN
takes advantage of the bootstrapping module by incorporating labeled pairs in training. In the relaxed
setting, such labeled pairs are ground-truth aligned pairs, but in the consolidated setting, the dangling
entities could bring in erroneous alignment which contaminates the alignment of other pairs."
NU,0.7834179357021996,"G.2
Class Prior Estimation Supplementary Experiment (RQ2)."
NU,0.7851099830795262,"We hope further to verify the estimation and convergence results of iPULE of loss convergence.
We list the corresponding loss convergence results in Fig. 7. The losses under different pre-aligned
proportions (0.25, 0.2, 0.15, 0.1) on the GA-DBP15K constitute a group of statistical data, and the
corresponding loss mean and standard deviation of this set of statistical data are displayed."
NU,0.7868020304568528,"On the other hand, the loss difference is a direct indication of convergence in iPULE‚Äôs implementation.
Thus, we plot the histogram Ô¨Ågure of the DBP2.0 (w/ -minus & -plus) of the corresponding loss
difference for more comprehensive. With the progress of the algorithm, and the statistical number of
the difference of the smaller loss function occupied the maximum. This shows the convergence of
iPULE in this data set from another aspect."
NU,0.7884940778341794,"It is worth noticing that, there are performance Ô¨Çuctuations during the constitution of an ideal
embedding space during the cold start stage. The Ô¨Ågure plotted covers only the cold start subsequent
procedure."
NU,0.7901861252115059,"G.3
Embedding Dimension Selection (RQ3)."
NU,0.7918781725888325,"Although a higher embedding dimension may encode richer information, an overly high dimension
leads to performance decline. We select the GNN dimension according to the principle of [22]. Let"
NU,0.7935702199661591,"0
10
20
Epoch 0.1 0.2 0.3 0.4 0.5 Loss GA-EN"
NU,0.7952622673434856,"0
20
40
Epoch 0.1 0.2 0.3 0.4 0.5 Loss GA-FR"
NU,0.7969543147208121,"0
10
20
Epoch 0.1 0.2 0.3 0.4 0.5 Loss GA-JA"
NU,0.7986463620981388,"0
10
20
30
Epoch 0.1 0.2 0.3 0.4 0.5 Loss GA-ZH"
NU,0.8003384094754653,"mean
variance"
NU,0.8020304568527918,"0.000
0.025
0.050
0.075
Epoch 0 10 20 30 40 Loss FR-EN"
NU,0.8037225042301185,"0.00
0.02
0.04
0.06
Epoch 0 20 40 60 Loss JA-EN"
NU,0.805414551607445,"0.00
0.05
0.10
Epoch 0 10 20 30 Loss ZH-EN"
NU,0.8071065989847716,Figure 7: Visualization of loss convergence on DBP2.0 and GA-DBP15K.
NU,0.8087986463620981,"the dimension of embedding be d and the number of entities is N. According to the feature entropy in
[22], it holds that d > 8.33 log N by the Johnson-Lindenstrauss lemma [15] that the vector dimension
is at O(log N) order. In most of our settings, N is approximately 105, and thus d is set to 128."
NU,0.8104906937394247,"Dimension
ZH-EN
EN-ZH
JA-EN
EN-JA
FR-EN
EN-FR"
NU,0.8121827411167513,"Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1 Ours"
NU,0.8138747884940778,"64
.278
.446 .342
.224
.501 .310
.325
.410 .362
.239
.470 .317
.164
.224 .189
.135
.257 .178
96
.281
.451 .346
.226
.505 .312
.329
.415 .367
.241
.474 .320
.172
.235 .199
.143
.271 .187
128
.280
.448 .344
.225
.502 .311
.330
.416 .368
.243
.477 .322
.177
.242 .204
.151
.287 .198"
NU,0.8155668358714044,Table 9: The entity alignment performance over different embedding dimensions on DBP2.0.
NU,0.817258883248731,"As shown in Tab. 9, due to the varying number of entities in datasets, the embedding dimension at the
optimal performance varies. For example, the top performance is achieved on ZH-EN and EN-ZH
when the embedding dimension is 96 but is obtained on JA-EN, EN-JA, FR-EN, and EN-FR with an
embedding dimension of 128. As a compromise, we simplify the embedding representation of edges
to enable FR-EN and EN-FR to run with an embedding dimension of 128 with limited memory (For
more details, please check our open source code). A higher alignment performance can be achieved
if no compromise is made. As we observe, the optimal performance is typically achieved at the
theoretically chosen d. This also indicates our approach has a memory cost at the order of O(log N)."
NU,0.8189509306260575,"G.4
EfÔ¨Åciency (RQ4)"
NU,0.8206429780033841,"The previous works concerning the dangling problem have not analyzed its efÔ¨Åciency in their
experiments. Thus we only report the efÔ¨Åciency of our methods without baseline comparison. We
evaluate the efÔ¨Åciency of our work on Tab. 10 including alignment search time as ‚ÄòInference Time‚Äô,
KEESA average training time as ‚ÄòAverage Training Time‚Äô, and GPU memory cost on three different
datasets of DBP2.0. Data obtained from these three datasets with the top three node numbers is a
robust indicator of the efÔ¨Åciency of our method."
NU,0.8223350253807107,"We gathered the mean value of 5 inference time costs for each dataset with the corresponding CPU
and GPU memory consumption. Meanwhile, the average training time for each period from early
to late is calculated. We enumerate the average training time of epochs 1-20, 21-25, 26-30, 31-35,
36-40, and 41-45."
NU,0.8240270727580372,"Cause GPU is employed for not only model training but also inference, as shown on Tab. 10, the
inference speed is still very impressive. SpeciÔ¨Åcally, we split the large similarity matrix into multiple
independent row blocks to perform the nearest searches within each block, which are well suited for
GPU parallel processing."
NU,0.8257191201353637,"It‚Äôs noteworthy that the average training time correspondingly increases as training progresses from
early to late stages. More quasi-supervised information incorporated by us accounts for that. To
be speciÔ¨Åc, as the training deepens, we repeatedly conduct preliminary alignment tests while we
gather more and more entity pairs mutually closest under a given metric. The entity pairs serve as the
pre-aligned anchor nodes, i.e., the quasi-supervisory information mentioned above."
NU,0.8274111675126904,"Besides, we list the CPU and GPU memory consumption required for our work. Memory consumption
is inÔ¨Çuenced by various factors such as complex allocation algorithms, model parameter scales, and
hyperparameters. In this problem, we put more attention on triples which characterize one KG,
revealing an approximate proportionality between the number of triples and memory consumption."
NU,0.8291032148900169,"Datasets
Triples
Inference Time
Average Training Time (from 1 to 45 training epochs)
CPU Memory GPU Memory
1-20
21-25
26-30
31-35
36-40
41-45"
NU,0.8307952622673435,"DBP2.0ZH‚àíEN
872,935
48.78s
11.21s/it
21.16s/it
25.67s/it
28.17s/it 29.21s/it 30.14s/it
10.8GB
32.5GB
DBP2.0JA‚àíEN 1,015,545
120.76s
28.14s/it
53.99s/it
63.43s/it
68.27s/it 70.61s/it 72.80s/it
11.9GB
32.6GB
DBP2.0FR‚àíEN 2,089,909
382.48s
90.18s/it 158.18s/it 190.65s/it
-
-
-
27.7GB
60.2GB"
NU,0.8324873096446701,"Table 10: EfÔ¨Åciency performance of our work on DBP2.0. The measurement of average training time
is ‚Äòs/it‚Äô, which indicates seconds per iteration. One iteration here represents one training epoch. ‚Äò-‚Äô
indicates the absence of data due to training termination."
NU,0.8341793570219966,"G.5
Baseline Comparison Under Different Ratios of Pre-aligned Seeds (RQ5)."
NU,0.8358714043993232,"Comparing the proposed method with strong baseline models under different ratios of pre-aligned
seeds would better demonstrate Lambda‚Äôs superiority. The experimental baseline includes MtransE
w/ BR the SOTA method in previous works, which is also the only open-source method. The results
are shown in the Table. 11."
NU,0.8375634517766497,"Methods
ZH-EN
EN-ZH
JA-EN
EN-JA
FR-EN
EN-FR"
NU,0.8392554991539763,"Ratios Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1
Prec. Rec.
F1"
NU,0.8409475465313029,"MtransE w/ BR
10%
.161
.141 .148
.105
.127 .114
.102
.102 .128
.102
.128 .113
.133
.085 .102
.096
.072 .083
20%
.264
.267 .265
.186
.251 .213
.179
.180 .251
.180
.251 .210
.215
.151 .179
.167
.138 .150
30%
.312
.362 .335
.241
.376 .294
.314
.363 .336
.251
.358 .295
.265
.208 .233
.231
.213 .222"
NU,0.8426395939086294,"Lambda
10%
.236
.346 .280
.197
.385 .261
.262
.315 .286
.206
.360 .262
.179
.230 .201
.153
.260 .193
20%
.262
.399 .316
.215
.446 .290
.300
.368 .330
.226
.417 .293
.217
.286 .247
.182
.324 .233
30%
.279
.447 .344
.219
.489 .303
.324
.409 .362
.234
.460 .310
.234
.320 .271
.192
.363 .251"
NU,0.8443316412859561,"Table 11: Performance of Lambda and MtransE w/ BR under different ratios of pre-aligned of 10%,
20%, and 30%. Bold indicates optimal performance."
NU,0.8460236886632826,"G.6
LightEA as Strong Baseline for Comparison (RQ6)."
NU,0.8477157360406091,"LightEA [28] is recommended as a strong baseline for Lambda. We Ô¨Åxed LightEA‚Äôs code to include
dangling entities into the alignment candidates and evaluated its performance on DBP2.0. Hits@1
and Hits@10 are evaluated in a similar way to the dangling-unaware methods in our paper, as listed
below. In comparison, Lambda still outperforms LightEA."
NU,0.8494077834179357,"Methods
ZH-EN
JA-EN
FR-EN
H@1
H@10
H@1
H@10
H@1
H@10
LightEA 60.5%
82.9%
61.4% 84.1%
-
-
Lambda
62.6% 84.7% 62.1% 84.0% 44.1% 69.3%"
NU,0.8510998307952623,"Table 12: Comparison of Lambda and LightEA under relaxed setting. ‚Äò-‚Äô indicates the absence of
data due to out of time."
NU,0.8527918781725888,"H
Discussion"
NU,0.8544839255499154,"H.1
Alignment Direction"
NU,0.856175972927242,"As we found, the alignment problem with dangling cases has a deeper issue concerning the classiÔ¨Åca-
tion of imbalanced datasets. It originated from the observation that the alignment performance from
the source to the target is different from the other direction. The work of [33] has observed that on
DBP2.0, choosing the alignment direction from a less populated KG (e.g., ZH, JA, FR) to a more
populated KG (e.g., EN) enjoys a higher alignment accuracy but the other way around would lead to
a noticeable performance drop. Meanwhile, the dangling entity detection on EN-XX has a higher F1
score than XX-EN, as shown in Tab. 13."
NU,0.8578680203045685,"By analysis, we think it may be attributed to an improper indication of the dangling entity detection
power on imbalanced datasets. This error in removing the predicted dangling entity would accumulate
hurting the alignment task. To verify the point, we introduce a trivial classiÔ¨Åer that makes a simple
choice to classify all entities as dangling (positive) ones, and the detection results are reported in"
NU,0.8595600676818951,"Datasets
Dangling Detection
Entity Alignment
Our Work
Trivial
Our Work
Prec.
Rec.
F1
Prec.
Rec.
F1
Prec.
Rec.
F1"
NU,0.8612521150592216,"ZH-EN
.763
.925
.836
.583
1
.736
.279
.447
.344
EN-ZH
.844
.909
.875
.609
1
.756
.219
.489
.303"
NU,0.8629441624365483,"JA-EN
.807
.836
.821
.580
1
.734
.324
.409
.362
EN-JA
.880
.809
.843
.605
1
.753
.234
.320
.271"
NU,0.8646362098138748,"FR-EN
.615
.772
.685
.439
1
.610
.234
.320
.271
EN-FR
.732
.749
.740
.554
1
.715
.192
.363
.251
Table 13: Dangling entities detection by our classiÔ¨Åer v.s. a trivial one on DBP2.0."
NU,0.8663282571912013,"Tab. 13. As all unlabeled entities are trivially classiÔ¨Åed as dangling ones, the detection metrics of
the trivial classiÔ¨Åer are all falsely high. The more populated source KG usually has more dangling
entities (positives) and thus yields a higher precision in detection. Meanwhile, since the detection
classiÔ¨Åer actually is not working, more dangling entities participate in the alignment phase, resulting
in poor alignment performance. This has explained why EN-XX has a higher dangling detection
performance but a lower alignment accuracy compared to the other direction."
NU,0.868020304568528,"The root of this issue is that matchable and dangling entities comprise imbalanced categories in
the classiÔ¨Åcation task, but the corresponding metric is inappropriate. Hence boosting the detection
performance does not necessarily improve the alignment performance. We believe more practical
indicators of imbalanced datasets should be introduced to the alignment problem."
NU,0.8697123519458545,"H.2
The Similarity between Dual-AMN and Lambda"
NU,0.871404399323181,The differences between the proposed GNN and Dual-AMN include:
NU,0.8730964467005076,Aggregation:
NU,0.8747884940778342,1. The adaptive dangling indicator rej is included in Lambda for eliminating dangling pollution.
NU,0.8764805414551607,2. The indicator rej is concatenated as a part of the entity feature.
NU,0.8781725888324873,Attention:
NU,0.8798646362098139,1. The attention is scaled by rej to Ô¨Ålter dangling information.
NU,0.8815566835871405,"2. Relation rk‚Äôs embedding hrk is linked to the adaptive dangling indicator of the associated entity
rej, and thus the attention in Eq. (2) models the relationship between the relation and the entity."
NU,0.883248730964467,NeurIPS Paper Checklist
CLAIMS,0.8849407783417935,1. Claims
CLAIMS,0.8866328257191202,"Question: Do the main claims made in the abstract and introduction accurately reÔ¨Çect the
paper‚Äôs contributions and scope?
Answer: [Yes]
JustiÔ¨Åcation: see Abstract and Introduction. 1.
Guidelines:"
CLAIMS,0.8883248730964467,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢ The claims made should match theoretical and experimental results, and reÔ¨Çect how
much the results can be expected to generalize to other settings.
‚Ä¢ It is Ô¨Åne to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.8900169204737732,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
JustiÔ¨Åcation: We explained the trade-off of our method in How does our method work? 5.3
for a slightly inferior precision reported in Tab. 12 and Tab. 4.
Guidelines:"
CLAIMS,0.8917089678510999,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-speciÔ¨Åcation, asymptotic approximations only holding locally). The authors
should reÔ¨Çect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢ The authors should reÔ¨Çect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢ The authors should reÔ¨Çect on the factors that inÔ¨Çuence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢ The authors should discuss the computational efÔ¨Åciency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be speciÔ¨Åcally instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.8934010152284264,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]"
CLAIMS,0.8950930626057529,"JustiÔ¨Åcation: According to the order of appearance, we sort out and give the speciÔ¨Åc proof
in the appendix."
CLAIMS,0.8967851099830795,"‚Ä¢ The problem setting about PU learning as sec. 2.2
‚Ä¢ Proof for Lemma 3.
‚Ä¢ Proof for Theorem 1.
‚Ä¢ Proof for Theorem 2.
‚Ä¢ Proof for Theorem 3.
‚Ä¢ Proof for Lemma 2."
CLAIMS,0.8984771573604061,Guidelines:
CLAIMS,0.9001692047377327,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9018612521150592,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9035532994923858,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9052453468697124,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9069373942470389,"JustiÔ¨Åcation: We introduce the method proposed in this paper in detail in two sections,
Selective Aggregation with Spectral Contrastive Learning 3 and Iterative Positive-
Unlabeled Learning for Dangling Detection 4, and use the Alg. 1 to describe the latter in
pseudocode. Meanwhile, we gave implementation details at the beginning of the Experiment
5."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9086294416243654,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9103214890016921,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or veriÔ¨Åable.
‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might sufÔ¨Åce, or if the contribution is a speciÔ¨Åc model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9120135363790186,"(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9137055837563451,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9153976311336718,"Question: Does the paper provide open access to the data and code, with sufÔ¨Åcient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9170896785109983,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9187817258883249,"JustiÔ¨Åcation: We provide the code and data in supplemental material which is described in a
documented readme.md Ô¨Åle."
OPEN ACCESS TO DATA AND CODE,0.9204737732656514,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.922165820642978,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9238578680203046,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9255499153976311,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9272419627749577,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9289340101522843,"JustiÔ¨Åcation: We gave the main implementation details at the beginning of the Experiment 5.
Statistics of the experimental dataset and baselines in appendix F and additional experiment
in appendix G also cover that including dataset construction details and hyperparameter
selection criteria."
OPEN ACCESS TO DATA AND CODE,0.9306260575296108,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9323181049069373,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
material."
OPEN ACCESS TO DATA AND CODE,0.934010152284264,7. Experiment Statistical SigniÔ¨Åcance
OPEN ACCESS TO DATA AND CODE,0.9357021996615905,"Question: Does the paper report error bars suitably and correctly deÔ¨Åned or other appropriate
information about the statistical signiÔ¨Åcance of the experiments?
Answer: [Yes]
JustiÔ¨Åcation: We provide the corresponding mean and standard deviation curves in Fig. 7
by calculating the loss function of different alignment ratios 0.25, 0.2, 0.15, 0.1, and the
corresponding mean and standard deviation are drawn. Other experimental data have also
been measured many times to take the mean value.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.937394247038917,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, conÔ¨Å-
dence intervals, or statistical signiÔ¨Åcance tests, at least for the experiments that support
the main claims of the paper.
‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not veriÔ¨Åed.
‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
Ô¨Ågures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding Ô¨Ågures or tables in the text.
8. Experiments Compute Resources"
OPEN ACCESS TO DATA AND CODE,0.9390862944162437,"Question: For each experiment, does the paper provide sufÔ¨Åcient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
JustiÔ¨Åcation: We gave GPU and CPU resources needed for the experiment in Implementa-
tion Detail part at the beginning of the Experiment 5. Additionally, time of execution such
as training & inference time is provided in EfÔ¨Åciency. G.4.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9407783417935702,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper).
9. Code Of Ethics"
OPEN ACCESS TO DATA AND CODE,0.9424703891708968,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
JustiÔ¨Åcation: The dataset construction and usage do not contain any information that
endangers personal privacy, and it is licensed.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9441624365482234,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9458544839255499,10. Broader Impacts
BROADER IMPACTS,0.9475465313028765,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.949238578680203,Answer: [NA]
BROADER IMPACTS,0.9509306260575296,JustiÔ¨Åcation: There is no societal impact of the work performed.
BROADER IMPACTS,0.9526226734348562,Guidelines:
BROADER IMPACTS,0.9543147208121827,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake proÔ¨Åles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact speciÔ¨Åc
groups), privacy considerations, and security considerations.
‚Ä¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efÔ¨Åciency and accessibility of ML)."
SAFEGUARDS,0.9560067681895094,11. Safeguards
SAFEGUARDS,0.9576988155668359,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9593908629441624,Answer: [NA]
SAFEGUARDS,0.961082910321489,JustiÔ¨Åcation: The paper poses no such risks.
SAFEGUARDS,0.9627749576988156,Guidelines:
SAFEGUARDS,0.9644670050761421,"‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety Ô¨Ålters.
‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9661590524534687,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9678510998307953,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9695431472081218,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9712351945854484,"JustiÔ¨Åcation: In this paper, we give all the sufÔ¨Åcient reference materials. We provide the
code and data in the supplemental material and describe them in a documented readme.md
Ô¨Åle, where more required information is clariÔ¨Åed."
LICENSES FOR EXISTING ASSETS,0.9729272419627749,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9746192893401016,"‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators."
NEW ASSETS,0.9763113367174281,13. New Assets
NEW ASSETS,0.9780033840947546,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9796954314720813,Answer: [Yes]
NEW ASSETS,0.9813874788494078,"JustiÔ¨Åcation: We introduce the dataset GA16K, GA-DBP15K and DBP2.0-minus & -plus
in detail in the appendix F. We provide the code and data in the supplemental material and
describe them in a documented readme.md Ô¨Åle, where more required information is clariÔ¨Åed."
NEW ASSETS,0.9830795262267343,Guidelines:
NEW ASSETS,0.9847715736040609,"‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip Ô¨Åle."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864636209813875,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988155668358714,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9898477157360406,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9915397631133672,JustiÔ¨Åcation: The paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932318104906938,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9949238578680203,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Including this information in the supplemental material is Ô¨Åne, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966159052453468,"‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
JustiÔ¨Åcation: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983079526226735,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢ We recognize that the procedures for this may vary signiÔ¨Åcantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
