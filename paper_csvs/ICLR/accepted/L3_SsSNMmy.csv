Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0018867924528301887,"Vision Transformer (ViT) attains state-of-the-art performance in visual recognition,
and the variant, Local Vision Transformer, makes further improvements. The major
component in Local Vision Transformer, local attention, performs the attention
separately over small local windows. We rephrase local attention as a channel-wise
locally-connected layer and analyze it from two network regularization manners,
sparse connectivity and weight sharing, as well as dynamic weight computation.
We point out that local attention resembles depth-wise convolution and its dynamic
variants in sparse connectivity: there is no connection across channels, and each
position is connected to the positions within a small local window. The main
differences lie in (i) weight sharing - depth-wise convolution shares connection
weights (kernel weights) across spatial positions and attention shares the connection
weights across channels, and (ii) dynamic weight computation manners - local
attention is based on dot-products between pairwise positions in the local window,
and dynamic convolution is based on linear projections conducted on the center
representation or the globally pooled representation.
The connection between local attention and dynamic depth-wise convolution is
empirically veriﬁed by the ablation study about weight sharing and dynamic weight
computation in Local Vision Transformer and (dynamic) depth-wise convolu-
tion. We empirically observe that the models based on depth-wise convolution
and the dynamic variants with lower computation complexity perform on-par
with or slightly better than Swin Transformer, an instance of Local Vision Trans-
former, for ImageNet classiﬁcation, COCO object detection and ADE seman-
tic segmentation. Code is available at https://github.com/Atten4Vis/
DemystifyLocalViT."
INTRODUCTION,0.0037735849056603774,"1
INTRODUCTION"
INTRODUCTION,0.005660377358490566,"Vision Transformer (Chu et al., 2021b; d’Ascoli et al., 2021; Dosovitskiy et al., 2021; Guo et al.,
2021; Han et al., 2020; Khan et al., 2021; Touvron et al., 2020; Wang et al., 2021b; Wu et al., 2021;
Xu et al., 2021; Yuan et al., 2021b) has shown promising performance in ImageNet classiﬁcation.
The improved variants, Local Vision Transformer (Chu et al., 2021a; Liu et al., 2021b; Vaswani et al.,
2021), adopt the local attention mechanism, which partitions the image space into a set of small
windows, and conducts the attention over the windows simultaneously. Local attention leads to great
improvement in memory and computation efﬁciency and makes the extension to downstream tasks
easier and more efﬁcient, such as object detection and semantic segmentation."
INTRODUCTION,0.007547169811320755,"We exploit the network regularization schemes (Goodfellow et al., 2016), sparse connectivity that
controls the model complexity, and weight sharing that relaxes the requirement of increasing the
training data scale and reduces the model parameters, as well as dynamic weight prediction that
increases the model capability, to study the local attention mechanism. We rephrase local attention
as a channel-wise spatially-locally connected layer with dynamic connection weights. The main
properties are summarized as follows. (i) Sparse connectivity: there is no connection across channels,
and each output position is only connected to the input positions within a local window. (ii) Weight"
INTRODUCTION,0.009433962264150943,"∗Equal contribution
†Corresponding author. wangjingdong@outlook.com"
INTRODUCTION,0.011320754716981131,Published as a conference paper at ICLR 2022
INTRODUCTION,0.013207547169811321,"sharing: the connection weights are shared across channels or within each group of channels. (iii)
Dynamic weight: the connection weights are dynamically predicted according to each image instance."
INTRODUCTION,0.01509433962264151,"We connect local attention with depth-wise convolution (Chollet, 2017; Howard et al., 2017) and its
dynamic variants that are also a channel-wise spatially-locally connected layer with optional dynamic
connection weights. They are similar in sparse connectivity. The main differences lie in (i) weight
sharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions
and attention shares the connection weights across channels, and (ii) dynamic weight computation
manners - local attention is based on dot-products between pairwise positions in the local window,
and dynamic convolution is based on linear projections conducted on the center representation or the
globally pooled representation."
INTRODUCTION,0.016981132075471698,"We further present the empirical veriﬁcation for the connection. We take the recently-developed Local
Vision Transformer, Swin Transformer (Liu et al., 2021b), as an example, and study the empirical
performance of local attention and (dynamic) depth-wise convolution in the same training settings as
Swin Transformer. We replace the local attention layer with the (dynamic) depth-wise convolution
layer, keeping the overall structure unchanged."
INTRODUCTION,0.018867924528301886,"The results show that the (dynamic) depth-wise convolution-based approaches achieve comparable or
slightly higher performance for ImageNet classiﬁcation and two downstream tasks, COCO object
detection and ADE semantic segmentation, and (dynamic) depth-wise convolution takes lower com-
putation complexity. The ablation studies imply that weight sharing and dynamic weight improves the
model capability. Speciﬁcally, (i) for Swin Transformer, weight sharing across channels is beneﬁcial
mainly for reducing the parameter (attention weight) complexity, and the attention-based dynamic
weight scheme is advantageous in learning instance-speciﬁc weights and block-translation equivalent
representations; (ii) for depth-wise convolution, weight sharing across positions is beneﬁcial for
reducing the parameter complexity as well as learning translation equivalent representations, and the
linear projection-based dynamic weight scheme learns instance-speciﬁc weights."
CONNECTING LOCAL ATTENTION AND DEPTH-WISE CONVOLUTION,0.020754716981132074,"2
CONNECTING LOCAL ATTENTION AND DEPTH-WISE CONVOLUTION"
LOCAL ATTENTION,0.022641509433962263,"2.1
LOCAL ATTENTION
Vision Transformer (Dosovitskiy et al., 2021) forms a network by repeating the attention layer and
the subsequent point-wise MLP (point-wise convolution). The local Vision Transformer, such as
Swin Transformer (Liu et al., 2021b) and HaloNet (Vaswani et al., 2021), adopts the local attention
layer, which partitions the space into a set of small windows and performs the attention operation
within each window simultaneously, to improve the memory and computation efﬁciency."
LOCAL ATTENTION,0.024528301886792454,"The local attention mechanism forms the keys and values in a window that the query lies in. The
attention output for the query xi ∈RD at the position i is the aggregation of the corresponding
values in the local window, {xi1, xi2, . . . , xiNk}, weighted by the corresponding attention weights
{ai1, ai2, . . . , aiNk}1:"
LOCAL ATTENTION,0.026415094339622643,"yi =
XNk"
LOCAL ATTENTION,0.02830188679245283,"j=1 aijxij,
(1)"
LOCAL ATTENTION,0.03018867924528302,"where Nk = Kw × Kh is the size of the local window. The attention weight aij is computed as the
softmax normalization of the dot-product between the query xi and the key xij:"
LOCAL ATTENTION,0.03207547169811321,"aij = e 1
√"
LOCAL ATTENTION,0.033962264150943396,"D x⊤
i xij"
LOCAL ATTENTION,0.035849056603773584,"Zi
where Zi =
XNk j=1 e 1
√"
LOCAL ATTENTION,0.03773584905660377,"D x⊤
i xij.
(2)"
LOCAL ATTENTION,0.03962264150943396,"The multi-head version partitions the D-dimensional query, key and value vectors into M subvectors
(each with D"
LOCAL ATTENTION,0.04150943396226415,"M dimensions), and conducts the attention process M times, each over the corresponding
subvector. The whole output is the concatenation of M outputs, yi = [y⊤
i1 y⊤
i2 . . . y⊤
iM]⊤. The mth
output yim is calculated by"
LOCAL ATTENTION,0.04339622641509434,"yim =
XNk"
LOCAL ATTENTION,0.045283018867924525,"j=1 aijmxijm,
(3)"
LOCAL ATTENTION,0.04716981132075472,"where xijm is the mth value subvector and aijm is the attention weight computed from the mth head
in the same way as Equation 2."
LOCAL ATTENTION,0.04905660377358491,"1For presentation convenience, we ignore the linear projections conducted to the queries, the keys and the
values. In vision applications, the value and the corresponding key are from the same feature possibly with
different linear projections, and we denote them using the same symbol xij."
LOCAL ATTENTION,0.0509433962264151,Published as a conference paper at ICLR 2022 (a)
LOCAL ATTENTION,0.052830188679245285,Spatial
LOCAL ATTENTION,0.05471698113207547,Channel (b)
LOCAL ATTENTION,0.05660377358490566,Spatial
LOCAL ATTENTION,0.05849056603773585,Channel (c)
LOCAL ATTENTION,0.06037735849056604,Spatial
LOCAL ATTENTION,0.062264150943396226,Channel (d)
LOCAL ATTENTION,0.06415094339622641,Spatial
LOCAL ATTENTION,0.0660377358490566,Channel (e)
LOCAL ATTENTION,0.06792452830188679,Spatial
LOCAL ATTENTION,0.06981132075471698,Channel
LOCAL ATTENTION,0.07169811320754717,"Figure 1: Illustration of connectivity for (a) convolution, (b) global attention and spatial mixing MLP, (c) local
attention and depth-wise convolution, (d) point-wise MLP or 1 × 1 convolution, and (e) MLP (fully-connected
layer). In the spatial dimension, we use 1D to illustrate the local-connectivity pattern for clarity."
LOCAL ATTENTION,0.07358490566037736,"2.2
SPARSE CONNECTIVITY, WEIGHT SHARING, AND DYNAMIC WEIGHT"
LOCAL ATTENTION,0.07547169811320754,"We give a brief introduction of two regularization forms, sparse connectivity and weight sharing, and
dynamic weight, and their beneﬁts. We will use the three forms to analyze local attention and connect
it to dynamic depth-wise convolution."
LOCAL ATTENTION,0.07735849056603773,"Sparse connectivity means that there are no connections between some output neurons (variables)
and some input neurons in a layer. It reduces the model complexity without decreasing the number of
neurons, e.g., the size of the (hidden) representations."
LOCAL ATTENTION,0.07924528301886792,"Weight sharing indicates that some connection weights are equal. It lowers the number of model
parameters and increases the network size without requiring a corresponding increase in training
data (Goodfellow et al., 2016)."
LOCAL ATTENTION,0.08113207547169811,"Dynamic weight refers to learning specialized connection weights for each instance. It generally
aims to increase the model capacity. If regarding the learned connection weights as hidden variables,
dynamic weight can be viewed as introducing second-order operations that increase the capability of
the network. The connection to Hopﬁeld networks is discussed in (Ramsauer et al., 2020)."
ANALYZING LOCAL ATTENTION,0.0830188679245283,"2.3
ANALYZING LOCAL ATTENTION"
ANALYZING LOCAL ATTENTION,0.08490566037735849,"We show that local attention is a channel-wise spatially-locally connected layer with dynamic weight
computation, and discuss its properties. Figure 1 (c) illustrates the connectivity pattern."
ANALYZING LOCAL ATTENTION,0.08679245283018867,"The aggregation processes (Equation 1 and Equation 3) for local attention can be rewritten equivalently
in a form of element-wise multiplication:"
ANALYZING LOCAL ATTENTION,0.08867924528301886,"yi =
XNk"
ANALYZING LOCAL ATTENTION,0.09056603773584905,"j=1 wij ⊙xij,
(4)"
ANALYZING LOCAL ATTENTION,0.09245283018867924,"where ⊙is the element-wise multiplication operator, and wij ∈RD is the weight vector formed
from the attention weight aij or {aij1, aij2, . . . , aijM}."
ANALYZING LOCAL ATTENTION,0.09433962264150944,"Sparse connectivity. The local attention layer is spatially sparse: each position is connected to the Nk
positions in a small local window. There are also no connections across channels. The element-wise
multiplication in Equation 4 indicates that given the attention weights, each output element, e.g., yid
(the ith position for the dth channel), is only dependent on the corresponding input elements from the
same channel in the window, {xi1d, xi2d, . . . , xiNkd}, and not related to other channels."
ANALYZING LOCAL ATTENTION,0.09622641509433963,"Weight sharing. The weights are shared with respect to channels. In the single-head attention case, all
the elements {wij1, wij2, . . . , wijD} in the weight vector wij are the same: wijd = aij, 1 ⩽d ⩽D.
In the multi-head attention case, the weight vector wij is group-wise same: wij is partitioned to M
subvectors each corresponding to one attention head, {wij1, wij2, . . . , wijM}, and the elements in
each subvector wijm are the same and are equal to the mth attention weight, aijm."
ANALYZING LOCAL ATTENTION,0.09811320754716982,"Dynamic weight. The weights, {wi1, wi2, . . . , wiNk}, are dynamically predicted from the query xi
and the keys {xi1, xi2, . . . , xiNk} in the local window as shown in Equation 2. We rewrite it as:"
ANALYZING LOCAL ATTENTION,0.1,"{wi1, wi2, . . . , wiNk} = f(xi; xi1, xi2, . . . , xiNk).
(5)"
ANALYZING LOCAL ATTENTION,0.1018867924528302,"Each weight may obtain the information across all the channels in one head, and serves as a bridge to
deliver the across-channel information to each output channel."
ANALYZING LOCAL ATTENTION,0.10377358490566038,"Translation equivalence. Different from convolution which satisﬁes translation equivalence through
sharing weights across positions, the equivalence to translation for local attention, depends if the
keys/values are changed, i.e., the attention weights are changed, when the feature map is translated."
ANALYZING LOCAL ATTENTION,0.10566037735849057,Published as a conference paper at ICLR 2022
ANALYZING LOCAL ATTENTION,0.10754716981132076,"In the case of sparsely-sampled window (for run-time efﬁciency), e.g., (Hu et al., 2019; Liu et al.,
2021b; Ramachandran et al., 2019; Vaswani et al., 2021), local attention is equivalent to block-wise
translation, i.e., the translation is a block or multiple blocks with the block size same as the window
size Kw × Kh, and otherwise not equivalent (as keys/values are changed). In the case that the
windows are densely sampled (e.g., (Zhao et al., 2020)), local attention is equivalent to translation."
ANALYZING LOCAL ATTENTION,0.10943396226415095,"Set representation. The keys/values for one query are collected as a set with the spatial-order
information lost. This leads to that the spatial correspondence between the keys/values across
windows is not exploited. The order information loss is partially remedied by encoding the positions
as embeddings (Dosovitskiy et al., 2021; Touvron et al., 2020), or learning a so-called relative
position embedding (e.g., (Liu et al., 2021b)) in which the spatial-order information is preserved as
the keys/values in a local window are collected as a vector."
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.11132075471698114,"2.4
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION
Depth-wise convolution is a type of convolution that applies a single convolutional ﬁlter for each
channel: ¯Xd = Cd ⊗Xd, where Xd and ¯Xd are the dth input and output channel maps, Cd ∈RNk
is the corresponding kernel weight, and ⊗is the convolution operation. It can be equivalently written
in the form of element-wise multiplication for each position:"
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.11320754716981132,"yi =
XNk"
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.11509433962264151,"j=1 woﬀset(i,j) ⊙xij.
(6)"
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.1169811320754717,"Here, oﬀset(i, j) is the relative offset, from the 2D coordinate of the position j to the 2D coordinate
of the central position i. The weights {woﬀset(i,j) ∈RD; j = 1, 2, . . . , Nk} are reshaped from
C1, C2, . . . , CD. The Nk weight vectors are model parameters and shared for all the positions."
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.11886792452830189,"We also consider two dynamic variants of depth-wise convolution: homogeneous and inhomoge-
neous2. The homogeneous dynamic variant predicts the convolution weights using linear projections
from a feature vector that is obtained by globally-pooling the feature maps:"
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.12075471698113208,"{w1, w2, . . . , wNk} = g(GAP(x1, x2, . . . , xN)).
(7)"
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.12264150943396226,"Here, {x1, x2, . . . , xN} are the image responses. GAP() is the global average pooling operator. g()
is a function based on linear projection: a linear projection layer to reduce the channel dimension
with BN and ReLU, followed by another linear projection to generate the connection weights."
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.12452830188679245,"The inhomogeneous dynamic variant predicts the convolution weights separately for each position
from the feature vector xi at the position (the center of the window):"
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.12641509433962264,"{wi1, wi2, . . . , wiNk } = g(xi).
(8)"
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.12830188679245283,"This means that the weights are not shared across positions. We share the weights across the channels
in a way similar to the multi-head attention mechanism to reduce the complexity."
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.13018867924528302,"We describe the similarities and differences between (dynamic) depth-wise convolution and local
attention. Figure 1 (c) illustrates the connectivity patterns and Table 1 shows the properties between
local attention and depth-wise convolution , and various other modules."
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.1320754716981132,"Similarity. Depth-wise convolution resembles local attention in sparse connectivity. There are no
connections across channels. Each position is only connected to the positions in a small local window
for each channel."
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.1339622641509434,"Difference. One main difference lies in weight sharing: depth-wise convolution shares the connection
weights across spatial positions, while local attention shares the weights across channels or within
each group of channels. Local attention uses proper weight sharing across channels to get better
performance. Depth-wise convolution beneﬁts from the weight sharing across positions to reduce the
parameter complexity and increase the network capability."
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.13584905660377358,"The second difference is that the connection weights for depth-wise convolution are static and learned
as model parameters, while the connection weights for local attention are dynamic and predicted from
each instance. The dynamic variants of depth-wise convolution also beneﬁt from the dynamic weight."
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.13773584905660377,"2The homogeneous version follows and applies dynamic convolution to depth-wise convolution. The
inhomogeneous version is close to involution (Li et al., 2021) and lightweight depth-wise convolution (Wu et al.,
2019)."
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.13962264150943396,Published as a conference paper at ICLR 2022
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.14150943396226415,"Table 1: The comparison of attention, local MLP (non-dynamic version of local attention, the attention weights
are learned as static model parameters), local attention, convolution, depth-wise convolution (DW-Conv.) and
the dynamic variant (D-DW-Conv.) in terms of the patterns of sparse connectivity, weight sharing, and dynamic
weight. Please refer to Figure 1 for the connectivity pattern illustration."
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.14339622641509434,"Sparse between positions
Sparse between
Weight sharing across
Dynamic
non-local
full
channels
position
channel
weight
Local MLP


♭"
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.14528301886792452,"Local attention


♭

DW-Conv.



D-DW-Conv.




Conv.

"
CONNECTION TO DYNAMIC DEPTH-WISE CONVOLUTION,0.1471698113207547,"One more difference lies in window representation. Local attention represents the positions in
a window by utilizing a set form with spatial-order information lost. It explores the spatial-order
information implicitly using the positional embedding or explicitly using the learned so-called relative
positional embedding. Depth-wise convolution exploits a vector form: aggregate the representations
within a local window with the weights indexed by the relative position (see Equation 6); keep
spatial correspondence between the positions for different windows, thus exploring the spatial-order
information explicitly."
EXPERIMENTAL STUDY,0.1490566037735849,"3
EXPERIMENTAL STUDY"
EXPERIMENTAL STUDY,0.1509433962264151,"We conduct empirical comparisons between local attention and depth-wise convolutions on three
visual recognition tasks: ImageNet classiﬁcation, COCO object detection, and ADE semantic
segmentation. We follow the structure of Swin Transformer to build the depth-wise convolution-
based networks. We apply the same training and evaluation settings from Swin Transformer to our
models. In addition, we study the effects of weight sharing and dynamic weight in the two structures.
The results for large scale pre-training are given in the appendix."
"ARCHITECTURES
WE USE THE RECENTLY-DEVELOPED SWIN TRANSFORMER AS THE EXAMPLE OF LOCAL ATTENTION-BASED NETWORKS",0.15283018867924528,"3.1
ARCHITECTURES
We use the recently-developed Swin Transformer as the example of local attention-based networks
and study the performance over the tiny and base networks: Swin-T and Swin-B, provided by the
authors (Liu et al., 2021b) We follow the tiny and base networks to build two depth-wise convolution-
based networks, DW-Conv.-T and DW-Conv.-B so that the overall architectures are the same, making
the comparison fair. We also build two dynamic versions, D-DW-Conv. and I-D-DW-Conv., by
predicting the dynamic weights as described in Section 2.4. We simply replace local attention in
Swin Transformer by depth-wise convolution of the same window size, where the pre- and post-
linear projections over the values are replaced by 1 × 1 convolutions. We adopt the convolutional
network design pattern to append BN (Ioffe & Szegedy, 2015) and ReLU (Nair & Hinton, 2010) to
the convolution. The details are available in the Appendix. In terms of parameter and computation
complexity, the depth-wise convolution-based networks are lower (Table 2) because there are linear
projections for keys and values in local attention."
DATASETS AND IMPLEMENTATION DETAILS,0.15471698113207547,"3.2
DATASETS AND IMPLEMENTATION DETAILS
ImageNet classiﬁcation. The ImageNet-1K recognition dataset (Deng et al., 2009) contains 1.28M
training images and 50K validation images with totally 1,000 classes. We use the exactly-same
training setting as Swin Transformer (Liu et al., 2021b). The AdamW (Loshchilov & Hutter, 2019)
optimizer for 300 epochs is adopted, with a cosine decay learning rate scheduler and 20 epochs of
linear warm-up. The weight decay is 0.05, and the initial learning rate is 0.001. The augmentation
and regularization strategies include RandAugment (Cubuk et al., 2020), Mixup (Zhang et al., 2018a),
CutMix (Yun et al., 2019), stochastic depth (Huang et al., 2016), etc."
DATASETS AND IMPLEMENTATION DETAILS,0.15660377358490565,"COCO object detection. The COCO 2017 dataset (Lin et al., 2014) contains 118K training and 5K
validation images. We follow Swin Transformer to adopt Cascade Mask R-CNN (Cai & Vasconcelos,
2019) for comparing backbones. We use the training and test settings from Swin Transformer:
multi-scale training - resizing the input such that the shorter side is between 480 and 800 and the
longer side is at most 1333; AdamW optimizer with the initial learning rate 0.0001; weight decay -
0.05; batch size - 16; and epochs - 36."
DATASETS AND IMPLEMENTATION DETAILS,0.15849056603773584,"ADE semantic segmentation. The ADE20K (Zhou et al., 2017) dataset contains 25K images, 20K
for training, 2K for validation, and 3K for testing, with 150 semantic categories. The same setting"
DATASETS AND IMPLEMENTATION DETAILS,0.16037735849056603,Published as a conference paper at ICLR 2022
DATASETS AND IMPLEMENTATION DETAILS,0.16226415094339622,"Table 2: ImageNet classiﬁcation comparison for ResNet, Mixer and ResMLP, ViT and DeiT, Swin (Swin
Transformer), DW-Conv. (depth-wise convolution), and D-DW-Conv. (dynamic depth-wise convolution)."
DATASETS AND IMPLEMENTATION DETAILS,0.1641509433962264,"method
img. size #param. FLOPs throughput (img. / s) top-1 acc. real acc.
Bottleneck: convolution with low rank
ResNet-50 (He et al., 2016)
2242
26M
4.1G
1128.3
76.2
82.5
ResNet-101 (He et al., 2016)
2242
45M
7.9G
652.0
77.4
83.7
ResNet-152 (He et al., 2016)
2242
60M
11.6G
456.7
78.3
84.1
Channel and spatial separable MLP, spatial separable MLP = point-wise 1 × 1 convolution
Mixer-B/16 (Tolstikhin et al., 2021)
2242
46M
-
-
76.4
82.4
Mixer-L/16 (Tolstikhin et al., 2021)
2242
189M
-
-
71.8
77.1
ResMLP-12 (Touvron et al., 2021)
2242
15M
3.0G
-
76.6
83.3
ResMLP-24 (Touvron et al., 2021)
2242
30M
6.0G
-
79.4
85.3
ResMLP-36 (Touvron et al., 2021)
2242
45M
8.9G
-
79.7
85.6
Global attention: dynamic channel separable MLP + spatial separable MLP
ViT-B/16 (Dosovitskiy et al., 2021)
3842
86M
55.4G
83.4
77.9
83.6
ViT-L/16 (Dosovitskiy et al., 2021)
3842
307M 190.7G
26.5
76.5
82.2
DeiT-S (Touvron et al., 2020)
2242
22M
4.6G
947.3
79.8
85.7
DeiT-B (Touvron et al., 2020)
2242
86M
17.5G
298.2
81.8
86.7
DeiT-B (Touvron et al., 2020)
3842
86M
55.4G
82.7
83.1
87.7
Local MLP: perform static separable MLP in local small windows
Swin-Local MLP-T
2242
26M
3.8G
861.0
80.3
86.1
Swin-Local MLP-B
2242
79M
12.9G
321.2
82.2
86.9
Local attention: perform attention in local small windows
Swin-T (Liu et al., 2021b)
2242
28M
4.5G
713.5
81.3
86.6
Swin-B (Liu et al., 2021b)
2242
88M
15.4G
263.0
83.3
87.9
Depth-wise convolution + point-wise 1 × 1 convolution
DW-Conv.-T
2242
24M
3.8G
928.7
81.3
86.8
DW-Conv.-B
2242
74M
12.9G
327.6
83.2
87.9
D-DW-Conv.-T
2242
51M
3.8G
897.0
81.9
87.3
D-DW-Conv.-B
2242
162M
13.0G
322.4
83.2
87.9
I-D-DW-Conv.-T
2242
26M
4.4G
685.3
81.8
87.1
I-D-DW-Conv.-B
2242
80M
14.3G
244.9
83.4
88.0
as Swin Transformer (Liu et al., 2021b) is adopted. UPerNet (Xiao et al., 2018) is used as the
segmentation framework. Details are provided in the Appendix."
RESULTS,0.1660377358490566,"3.3
RESULTS
ImageNet classiﬁcation. The comparison for ImageNet classiﬁcation is given in Table 2. One
can see that the local attention-based networks, Swin Transformer, and the depth-wise convolution-
based networks, perform on par (with a slight difference of 0.1) in terms of top-1 accuracy and real
accuracy (Beyer et al., 2020) for both tiny and base models. In the tiny model case, the two dynamic
depth-wise convolution-based networks perform higher. In particular, the depth-wise convolution-
based networks are more efﬁcient in parameters and computation complexities. In the tiny model
case, the parameters and computation complexities are reduced by 14.2% and 15.5%, respectively.
Similarly, in the base model case, the two costs are reduced by 15.9% and 16.2%, respectively. The
homogeneous dynamic variant takes more parameters but with almost the same complexity efﬁciency,
and the inhomogeneous dynamic variant take advantage of weight sharing across channels that reduce
the model parameters.
COCO object detection. The comparisons between local attention (Swin Transformer), depth-wise
convolution, and two versions of dynamic depth-wise convolution are shown in Table 3. Depth-wise
convolution performs a little lower than local attention, and dynamic depth-wise convolution performs
better than the static version and on par with local attention.
ADE semantic Segmentation. The comparisons of single scale testing on ADE semantic segmenta-
tion are shown in Table 3. In the tiny model case, (dynamic) depth-wise convolution is ~1.0% higher
than local attention. In the base model case, the performances are similar3."
RESULTS,0.16792452830188678,"3We conducted an additional experiment by changing the ending learning rate from 0 to 1e −6. The base
model with depth-wise convolutions achieves a higher mIoU score: 48.9."
RESULTS,0.16981132075471697,Published as a conference paper at ICLR 2022
RESULTS,0.17169811320754716,Table 3: Comparison results on COCO object detection and ADE semantic segmentation.
RESULTS,0.17358490566037735,"COCO Object Detection
ADE20K Semantic Segmentation
#param. FLOPs
APbox
APbox
50
APbox
75
APmask
#param. FLOPs
mIoU
Swin-T
86M
747G
50.5
69.3
54.9
43.7
60M
947G
44.5
DW Conv.-T
82M
730G
49.9
68.6
54.3
43.4
56M
928G
45.5
D-DW Conv.-T
108M
730G
50.5
69.5
54.6
43.7
83M
928G
45.7
I-D-DW Conv.-T
84M
741G
50.8
69.5
55.3
44.0
58M
939G
46.2
Swin-B
145M
986G
51.9
70.9
56.5
45.0
121M
1192G
48.1
DW Conv.-B
132M
924G
51.1
69.6
55.4
44.2
108M
1129G
48.3
D-DW Conv.-B
219M
924G
51.2
70.0
55.4
44.4
195M
1129G
48.0
I-D-DW Conv.-B
137M
948G
51.8
70.3
56.1
44.8
114M
1153G
47.8
Table 4: Effects of weight sharing across channels and positions. The results are reported on the
ImageNet top-1 accuracy. SC = Sharing across channels. SP = sharing across positions."
RESULTS,0.17547169811320754,"SC
SP
Acc.
#param.
SC
SP
Acc.
#param."
RESULTS,0.17735849056603772,Local MLP
RESULTS,0.1792452830188679,"

80.2
35.3M"
RESULTS,0.1811320754716981,DW Conv.
RESULTS,0.1830188679245283,"

81.3
24.2M


80.3
26.2M


80.3
26.2M


80.3
24.3M


81.1
23.9M"
EMPIRICAL ANALYSIS,0.18490566037735848,"3.4
EMPIRICAL ANALYSIS
Local and channel-separable connection has been shown to be helpful for visual recognition. The
empirical results in Table 2, e.g., local attention performs better than global attention (local connection)
and depth-wise convolution performs better than convolution (channel-separable connection), also
verify it. In the following, we present empirical results for weight sharing and dynamic weight by
taking the tiny models as examples."
EMPIRICAL ANALYSIS,0.18679245283018867,"Weight sharing. We study how the performance is affected by the number of channels in each group
across which the weights are shared (the numbers of attention heads at each stage are accordingly
changed) for local attention and local MLP (learn the weights in each window as model parameters
and not shared across windows). Figure 2 shows the effect for (a) local MLP - static weights, and
(b) local attention - dynamic weights. One can see that for local attention, too many channels and
too few channels in each group perform similarly, but do not lead to the best. For local MLP, weight
sharing signiﬁcantly reduces model parameters. These indicate proper weight sharing across channels
is helpful for both local attention and local MLP."
EMPIRICAL ANALYSIS,0.18867924528301888,"We further study the effect of combining the weight sharing pattern for local MLP and depth-wise
convolution. For local MLP, Weight sharing across positions means the connection weight is shared
for different spatial blocks in local MLP. For convolution, the scheme of sharing weights across
channels is similar to the multi-head manner in local attention. The results in Table 4 suggest that:
(i) for local MLP, sharing weight across channels reduces the model parameters and sharing across
spatial blocks do not have big impact; (ii) For depth-wise convolution, sharing weight across channels
does not have big impact, but sharing weight across positions signiﬁcantly increase the performance."
EMPIRICAL ANALYSIS,0.19056603773584907,"The window sampling scheme for local MLP and depth-wise convolution is different: local MLP
sparsely samples the windows using the way in Swin Transformer, for reducing the high memory cost,
and depth-wise convolution densely sample the windows. Weight sharing across positions in local
MLP is insufﬁcient for learning translation-equivalent representation, explaining why local MLP
with weight sharing across both channels and positions performs lower than depth-wise convolution
with additional weight sharing across channels."
EMPIRICAL ANALYSIS,0.19245283018867926,"Dynamic weight. We study how dynamic weight in local attention affects performance. As seen
from Table 2, local MLP achieves, the static version, 80.3% and 82.2% for tiny and base models,
lower than Swin, the dynamic version, 81.3% and 83.3%. This implies that dynamic weight is helpful.
The improvements from dynamic weight are also observed for depth-wise convolution (Table 2)."
EMPIRICAL ANALYSIS,0.19433962264150945,"We further study the effects of the attention scheme and the linear-projection scheme for dynamic
weight computation. The observations from in Table 5 include: the attention mechanism for shifted
and sliding window sampling performs similarly; the inhomogeneous dynamic weight computa-
tion way is better than the attention mechanism (81.8 vs 81.4). We think that the reasons for the
latter observation include: for the attention mechanism the representation is only block translation
equivalent other than any translation equivalent; the linear projection-based dynamic weight scheme"
EMPIRICAL ANALYSIS,0.19622641509433963,Published as a conference paper at ICLR 2022 (a)
EMPIRICAL ANALYSIS,0.19811320754716982,1 / 95M
EMPIRICAL ANALYSIS,0.2,2 / 59M
EMPIRICAL ANALYSIS,0.2018867924528302,6 / 36M
EMPIRICAL ANALYSIS,0.2037735849056604,16 / 28M
EMPIRICAL ANALYSIS,0.20566037735849058,32 / 26M
EMPIRICAL ANALYSIS,0.20754716981132076,48 / 25M
EMPIRICAL ANALYSIS,0.20943396226415095,96 / 25M 79.7 80 80.5 (b)
EMPIRICAL ANALYSIS,0.21132075471698114,6 / 28M
EMPIRICAL ANALYSIS,0.21320754716981133,16 / 28M
EMPIRICAL ANALYSIS,0.21509433962264152,32 / 28M
EMPIRICAL ANALYSIS,0.2169811320754717,48 / 28M
EMPIRICAL ANALYSIS,0.2188679245283019,96 / 28M
EMPIRICAL ANALYSIS,0.22075471698113208,"80
80.5"
EMPIRICAL ANALYSIS,0.22264150943396227,"81
81.5"
EMPIRICAL ANALYSIS,0.22452830188679246,"Figure 2: Effect of #channels sharing the weights on ImageNet classiﬁcation. X-axis: #channels within each
group / #param. Y-axis: ImageNet classiﬁcation accuracy. (a) Local MLP: the static version of Swin transformer.
(b) Local attention: Swin transformer. Results is reported for tiny model on ImageNet dataset.
Table 5: Comparison of different dynamic weight manners. The results are reported on the ImageNet top-1
accuracy. Shifted window sampling (Win. samp.) means the way in Swin Transformer and sliding means
the densely-sampling manner. The result of Sliding local MLP is from (Liu et al., 2021b). homo. dyna. =
homogeneous dynamic weight. inhomo. dyna. = inhomogeneous dynamic weight."
EMPIRICAL ANALYSIS,0.22641509433962265,"Win. samp. #param. FLOPs Acc.
Win. samp. #param. FLOPs Acc.
Local MLP
shifted
26M
3.8G
80.3
DW Conv.
sliding
24M
3.8G
81.3
w/ attention
shifted
28M
4.5G
81.3
w/ homo. dyna.
sliding
51M
3.8G
81.9
w/ attention
sliding
28M
4.5G
81.4 w/ inhomo. dyna.
sliding
26M
4.4G
81.8"
EMPIRICAL ANALYSIS,0.22830188679245284,"(vector representation for the window) learns better weights than the attention-based scheme (set
representation for the window). We also observe that such inﬂuence is eliminated for large models
and detection tasks."
EMPIRICAL ANALYSIS,0.23018867924528302,"Set representation. Local attention represents the positions in a window as a set with the spatial-
order information lost. Swin Transformer learns relative positional embeddings where the positions
in a window are actually described as a vector keeping the spatial-order information. It is reported
in (Liu et al., 2021b) that removing the relative positional embeddings leads to a 1.2% accuracy drop,
indicating the spatial-order information is important."
EMPIRICAL ANALYSIS,0.2320754716981132,"Concurrent works. We give the comparison between inhomogeneous dynamic depth-wise convo-
lution (I-D-DW Conv.) and concurrent local attention-based works (Chu et al., 2021a; Wang et al.,
2021a; Huang et al., 2021; Xu et al., 2021) in Table 6. We follow Shufﬂe Transformer and add
an extra DW Conv. before FFN in I-D-DW Conv, and the performance is improved by 0.5. The
performance is on par with these concurrent works except the Twins-SVT (81.9%, 2.9G) which uses
interleaved attention and additional depth-wise convolutions."
RELATED WORK,0.2339622641509434,"4
RELATED WORK"
RELATED WORK,0.2358490566037736,"Sparse connectivity. Sparse connection across channels is widely explored for removing redun-
dancy in the channel domain. The typical schemes are depth-wise convolution adopted by Mo-
bileNet (Howard et al., 2017; Sandler et al., 2018), ShufﬂeNetV2 (Ma et al., 2018) and IGCv3 (Sun
et al., 2018), and group convolution adopted by ResNeXt (Xie et al., 2017), merge-and-run (Zhao
et al., 2018), ShufﬂeNetV1 (Zhang et al., 2018b), and IGC (Zhang et al., 2017)."
RELATED WORK,0.23773584905660378,"The self-attention unit4 in Vision Transformer, its variants (Chen et al., 2020; Chu et al., 2021b;
Dosovitskiy et al., 2021; Han et al., 2021; Heo et al., 2021; Li et al., 2021; Liu et al., 2021b; Pan
et al., 2021; Touvron et al., 2020; Vaswani et al., 2021; Wang et al., 2021b; Wu et al., 2021; Yuan
et al., 2021a;b; Zhang et al., 2021; Zhao et al., 2020; Zhou et al., 2021), and the spatial information
fusion unit (e.g., token-mixer in MLP-Mixer (Tolstikhin et al., 2021) and ResMLP (Touvron et al.,
2021)) have no connections across channels."
RELATED WORK,0.23962264150943396,"1 × 1 (point-wise) convolution (in ShufﬂeNetV2 (Ma et al., 2018), MobileNet (Howard et al., 2017;
Sandler et al., 2018), IGC (Zhang et al., 2017), ViT (Dosovitskiy et al., 2021), local ViT (Liu et al.,
2021b; Vaswani et al., 2021), MLP-Mixer (Tolstikhin et al., 2021), ResMLP (Touvron et al., 2021))
has no connections across spatial positions. The convolutions with other kernel sizes and local
attention (Zhao et al., 2020; Liu et al., 2021b; Vaswani et al., 2021) have connections between each
position and the positions within a small local window, respectively."
RELATED WORK,0.24150943396226415,"4The pre- and post- linear projections for values can be regarded as 1 × 1 convolutions. The attention weights
generated from keys and values with linear projections in some sense mix the information across channels."
RELATED WORK,0.24339622641509434,Published as a conference paper at ICLR 2022
RELATED WORK,0.24528301886792453,Table 6: Comparison with concurrent works on ImageNet classiﬁcation with tiny models.
RELATED WORK,0.24716981132075472,"#param.
FLOPs
top-1 acc.
Twins-PCPVT (Chu et al., 2021a)
24M
3.8G
81.2
Twins-SVT (Chu et al., 2021a)
24M
2.9G
81.7
CoaT-Lite (Xu et al., 2021)
20M
4.0G
81.9
CoaT (Xu et al., 2021)
22M
12.6G
82.1
PVT-v2 (Wang et al., 2021a)
25M
4.0G
82.0
Shufﬂe Transformer (Huang et al., 2021)
29M
4.6G
82.5
I-D-DW Conv.
26M
4.4G
81.8
I-D-DW Conv. + DW
27M
4.4G
82.3"
RELATED WORK,0.2490566037735849,"Weight sharing. Weight sharing across spatial positions is mainly used in convolution, including
normal convolution, depth-wise convolution and point-wise convolution. Weight sharing across
channels is adopted in the attention unit (Vaswani et al., 2017), its variants (Chu et al., 2021a;b;
Dosovitskiy et al., 2021; Li et al., 2021; Liu et al., 2021b; Touvron et al., 2020; Vaswani et al.,
2021; Wang et al., 2021b; Wu et al., 2021; Yuan et al., 2021b), and token-mixer MLP in MLP-
mixer (Tolstikhin et al., 2021) and ResMLP (Touvron et al., 2021)."
RELATED WORK,0.2509433962264151,"Dynamic weight. Predicting the connection weights is widely studied in convolutional networks.
There are basically two types. One is to learn homogeneous connection weights, e.g., SENet (Hu
et al., 2018b), dynamic convolution (Jia et al., 2016). The other is to learn the weights for each region
or each position (GENet (Hu et al., 2018a), Lite-HRNet (Yu et al., 2021), Involution (Li et al., 2021)).
The attention unit in ViT or local ViT learns dynamic connection weights for each position."
RELATED WORK,0.2528301886792453,"Networks built with depth-wise separable convolutions. There are many networks built upon
depth-wise separable convolution or its variants, such as MobileNet (Howard et al., 2017; Sandler
et al., 2018), ShufﬂeNet (Ma et al., 2018), IGC (Zhang et al., 2017), Xception (Chollet, 2017), and
EfﬁcientNet (Tan & Le, 2019; 2021). In this paper, our goal is to connect dynamic depth-wise
convolution with local attention."
RELATED WORK,0.25471698113207547,"Convolution vs Transformer. The study in (Cordonnier et al., 2020) shows that a multi-head self-
attention layer can simulate a convolutional layer by developing additional carefully-designed relative
positional embeddings with the attention part dropped. Differently, we connect (dynamic) depth-wise
convolution and local self-attention by connecting the attention weights for self-attention and the
dynamic weights for convolution (as well as studying weight sharing). In (Andreoli, 2019), the
mathematical connection (in terms of the tensor form) between convolution and attention is presented.
The opinion that convolution and attention are essentially about the model complexity control is
similar to ours, and we make the detailed analysis and report empirical studies."
RELATED WORK,0.25660377358490566,"The concurrently-developed work in NLP (Tay et al., 2021) empirically compares lightweight depth-
wise convolution (Wu et al., 2019) to Transformer for NLP tasks, and reaches a conclusion similar to
ours for vision tasks: convolution and Transformer obtain on-par results. Differently, we attempt to
understand why they perform on par from three perspectives: sparse connectivity, weight sharing and
dynamic weight, and discuss their similarities and differences."
CONCLUSION,0.25849056603773585,"5
CONCLUSION"
CONCLUSION,0.26037735849056604,"The connections between local attention and dynamic depth-wise convolution are summarized as
follows. (i) Same with dynamic depth-wise convolution, local attention beneﬁts from two sparse
connectivity forms: local connection and no connection across channels. (ii) Weight sharing across
channels in local attention is helpful for reducing the parameter (attention weight) complexity and
slightly boosting the performance, and weight sharing across positions in depth-wise convolution is
helpful for reducing the parameter complexity and learning translation-equivalent representations
and thus boosting the performance. (iii) The attention-based dynamic weight computation for
local attention is beneﬁcial for learning image-dependent weights and block-translation equivalent
representations, and the linear projection-based dynamic weight computation for (in)homogeneous
dynamic depth-wise convolution is beneﬁcial for learning image-dependent weights. Inhomogeneous
dynamic depth-wise convolution is superior over local attention for ImageNet classiﬁcation and
segmentation in the case of tiny models, and on par for larger models and detection tasks. In addition,
the better downstream performance for local attention and depth-wise convolution stems from the
larger kernel size (7 × 7 vs 3 × 3), which is also observed in Yuan et al. (2021d)."
CONCLUSION,0.2622641509433962,Published as a conference paper at ICLR 2022
REFERENCES,0.2641509433962264,REFERENCES
REFERENCES,0.2660377358490566,"Jean-Marc Andreoli.
Convolution, attention and structure embedding.
arXiv preprint
arXiv:1905.01289, 2019."
REFERENCES,0.2679245283018868,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016."
REFERENCES,0.269811320754717,"Lucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are
we done with imagenet? arXiv preprint arXiv:2006.07159, 2020."
REFERENCES,0.27169811320754716,"Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: high quality object detection and instance
segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 2019."
REFERENCES,0.27358490566037735,"Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chun-
jing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. arXiv preprint
arXiv:2012.00364, 2020."
REFERENCES,0.27547169811320754,"François Chollet. Xception: Deep learning with depthwise separable convolutions. In IEEE Conf.
Comput. Vis. Pattern Recog., pp. 1251–1258, 2017."
REFERENCES,0.27735849056603773,"Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and
Chunhua Shen. Twins: Revisiting spatial attention design in vision transformers. arXiv preprint
arXiv:2104.13840, 2021a."
REFERENCES,0.2792452830188679,"Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. Do we really need explicit
position encodings for vision transformers? arXiv preprint arXiv:2102.10882, 2021b."
REFERENCES,0.2811320754716981,"MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and
benchmark. https://github.com/open-mmlab/mmsegmentation, 2020."
REFERENCES,0.2830188679245283,"Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-
attention and convolutional layers. In Int. Conf. Learn. Represent., 2020."
REFERENCES,0.2849056603773585,"Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated
data augmentation with a reduced search space. In IEEE Conf. Comput. Vis. Pattern Recog., pp.
702–703, 2020."
REFERENCES,0.28679245283018867,"Stéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun.
Convit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint
arXiv:2103.10697, 2021."
REFERENCES,0.28867924528301886,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 248–255. Ieee, 2009."
REFERENCES,0.29056603773584905,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.
In Int. Conf. Learn. Represent., 2021."
REFERENCES,0.29245283018867924,"Shang-Hua Gao, Qi Han, Duo Li, Pai Peng, Ming-Ming Cheng, and Pai Peng. Representative batch
normalization with feature calibration. In IEEE Conf. Comput. Vis. Pattern Recog., 2021."
REFERENCES,0.2943396226415094,"Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016."
REFERENCES,0.2962264150943396,"Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, and Shi-Min Hu. Beyond self-attention: External
attention using two linear layers for visual tasks. arXiv preprint arXiv:2105.02358, 2021."
REFERENCES,0.2981132075471698,"Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang,
An Xiao, Chunjing Xu, Yixing Xu, et al.
A survey on visual transformer.
arXiv preprint
arXiv:2012.12556, 2020."
REFERENCES,0.3,"Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in
transformer. arXiv preprint arXiv:2103.00112, 2021."
REFERENCES,0.3018867924528302,Published as a conference paper at ICLR 2022
REFERENCES,0.30377358490566037,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 770–778, 2016."
REFERENCES,0.30566037735849055,"Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh.
Rethinking spatial dimensions of vision transformers. arXiv preprint arXiv:2103.16302, 2021."
REFERENCES,0.30754716981132074,"Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017."
REFERENCES,0.30943396226415093,"Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.
In Int. Conf. Comput. Vis., pp. 3464–3473, 2019."
REFERENCES,0.3113207547169811,"Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea Vedaldi. Gather-excite: Exploiting feature
context in convolutional neural networks. In Adv. Neural Inform. Process. Syst., 2018a."
REFERENCES,0.3132075471698113,"Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In IEEE Conf. Comput. Vis.
Pattern Recog., pp. 7132–7141, 2018b."
REFERENCES,0.3150943396226415,"Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with
stochastic depth. In Eur. Conf. Comput. Vis., pp. 646–661. Springer, 2016."
REFERENCES,0.3169811320754717,"Lang Huang, Yuhui Yuan, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang. Interlaced
sparse self-attention for semantic segmentation. CoRR, abs/1907.12273, 2019a."
REFERENCES,0.31886792452830187,"Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:
Criss-cross attention for semantic segmentation. In Int. Conf. Comput. Vis., pp. 603–612, 2019b."
REFERENCES,0.32075471698113206,"Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu. Shufﬂe transformer:
Rethinking spatial shufﬂe for vision transformer. arXiv preprint arXiv:2106.03650, 2021."
REFERENCES,0.32264150943396225,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Int. Conf. Mach. Learn., pp. 448–456. PMLR, 2015."
REFERENCES,0.32452830188679244,"Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc Van Gool. Dynamic ﬁlter networks. In Adv.
Neural Inform. Process. Syst., 2016."
REFERENCES,0.3264150943396226,"Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and
Mubarak Shah. Transformers in vision: A survey. arXiv preprint arXiv:2101.01169, 2021."
REFERENCES,0.3283018867924528,"Duo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, Lei Zhu, Tong Zhang, and Qifeng Chen.
Involution: Inverting the inherence of convolution for visual recognition. In IEEE Conf. Comput.
Vis. Pattern Recog., 2021."
REFERENCES,0.330188679245283,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Eur. Conf.
Comput. Vis., pp. 740–755. Springer, 2014."
REFERENCES,0.3320754716981132,"Hanxiao Liu, Zihang Dai, David R So, and Quoc V Le. Pay attention to mlps. arXiv preprint
arXiv:2105.08050, 2021a."
REFERENCES,0.3339622641509434,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint
arXiv:2103.14030, 2021b."
REFERENCES,0.33584905660377357,"Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Int. Conf. Learn.
Represent. OpenReview.net, 2019."
REFERENCES,0.33773584905660375,"Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufﬂenet v2: Practical guidelines for
efﬁcient cnn architecture design. In Eur. Conf. Comput. Vis., pp. 116–131, 2018."
REFERENCES,0.33962264150943394,"Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In
Int. Conf. Mach. Learn., 2010."
REFERENCES,0.34150943396226413,Published as a conference paper at ICLR 2022
REFERENCES,0.3433962264150943,"Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jianfei Cai. Scalable visual transformers with
hierarchical pooling. arXiv preprint arXiv:2103.10619, 2021."
REFERENCES,0.3452830188679245,"Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon
Shlens. Stand-alone self-attention in vision models. In Adv. Neural Inform. Process. Syst., pp.
68–80, 2019."
REFERENCES,0.3471698113207547,"Hubert Ramsauer, Bernhard Schäﬂ, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler,
Lukas Gruber, Markus Holzleitner, Milena Pavlovi´c, Geir Kjetil Sandve, et al. Hopﬁeld networks
is all you need. arXiv preprint arXiv:2008.02217, 2020."
REFERENCES,0.3490566037735849,"Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In IEEE Conf. Comput. Vis. Pattern Recog.,
pp. 4510–4520, 2018."
REFERENCES,0.35094339622641507,"Ke Sun, Mingjie Li, Dong Liu, and Jingdong Wang. Igcv3: Interleaved low-rank group convolutions
for efﬁcient deep neural networks. In Brit. Mach. Vis. Conf., 2018."
REFERENCES,0.35283018867924526,"Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for
human pose estimation. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 5693–5703, 2019."
REFERENCES,0.35471698113207545,"Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.
In Int. Conf. Mach. Learn., pp. 6105–6114. PMLR, 2019."
REFERENCES,0.35660377358490564,"Mingxing Tan and Quoc V Le. Efﬁcientnetv2: Smaller models and faster training. arXiv preprint
arXiv:2104.00298, 2021."
REFERENCES,0.3584905660377358,"Yi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen Qin, and Donald Metzler. Are
pre-trained convolutions better than pre-trained transformers? arXiv preprint arXiv:2105.03322,
2021."
REFERENCES,0.360377358490566,"Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-
terthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An
all-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021."
REFERENCES,0.3622641509433962,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé
Jégou. Training data-efﬁcient image transformers & distillation through attention. arXiv preprint
arXiv:2012.12877, 2020."
REFERENCES,0.3641509433962264,"Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard
Grave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Hervé Jégou. Resmlp: Feedforward
networks for image classiﬁcation with data-efﬁcient training. arXiv preprint arXiv:2105.03404,
2021."
REFERENCES,0.3660377358490566,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Adv. Neural Inform. Process. Syst., pp.
5998–6008, 2017."
REFERENCES,0.36792452830188677,"Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon
Shlens. Scaling local self-attention for parameter efﬁcient visual backbones. In IEEE Conf. Comput.
Vis. Pattern Recog., 2021."
REFERENCES,0.36981132075471695,"Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong
Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep high-resolution representation
learning for visual recognition. IEEE Trans. Pattern Anal. Mach. Intell., 2020."
REFERENCES,0.37169811320754714,"Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,
and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv preprint
arXiv:2106.13797, 2021a."
REFERENCES,0.37358490566037733,"Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,
and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without
convolutions. arXiv preprint arXiv:2102.12122, 2021b."
REFERENCES,0.3754716981132076,Published as a conference paper at ICLR 2022
REFERENCES,0.37735849056603776,"Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, and Michael Auli. Pay less attention with
lightweight and dynamic convolutions. In Int. Conf. Learn. Represent., 2019."
REFERENCES,0.37924528301886795,"Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:
Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021."
REFERENCES,0.38113207547169814,"Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uniﬁed perceptual parsing for
scene understanding. In Eur. Conf. Comput. Vis., pp. 418–434, 2018."
REFERENCES,0.38301886792452833,"Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 1492–
1500, 2017."
REFERENCES,0.3849056603773585,"Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers.
arXiv preprint arXiv:2104.06399, 2021."
REFERENCES,0.3867924528301887,"Changqian Yu, Bin Xiao, Changxin Gao, Lu Yuan, Lei Zhang, Nong Sang, and Jingdong Wang.
Lite-hrnet: A lightweight high-resolution network. In IEEE Conf. Comput. Vis. Pattern Recog.,
2021."
REFERENCES,0.3886792452830189,"Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating
convolution designs into visual transformers. arXiv preprint arXiv:2103.11816, 2021a."
REFERENCES,0.3905660377358491,"Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and
Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv
preprint arXiv:2101.11986, 2021b."
REFERENCES,0.39245283018867927,"Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan. Volo: Vision outlooker for visual
recognition, 2021c."
REFERENCES,0.39433962264150946,"Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and Jingdong Wang.
Hrformer: High-resolution transformer for dense prediction. Adv. Neural Inform. Process. Syst.,
34, 2021d."
REFERENCES,0.39622641509433965,"Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In Int. Conf.
Comput. Vis., pp. 6023–6032, 2019."
REFERENCES,0.39811320754716983,"Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In Int. Conf. Learn. Represent., 2018a."
REFERENCES,0.4,"Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao.
Multi-scale vision longformer: A new vision transformer for high-resolution image encoding.
arXiv preprint arXiv:2103.15358, 2021."
REFERENCES,0.4018867924528302,"Qinglong Zhang and Yubin Yang. Rest: An efﬁcient transformer for visual recognition. arXiv
preprint arXiv:2105.13677, 2021."
REFERENCES,0.4037735849056604,"Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. Interleaved group convolutions. In Int. Conf.
Comput. Vis., pp. 4373–4382, 2017."
REFERENCES,0.4056603773584906,"Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufﬂenet: An extremely efﬁcient
convolutional neural network for mobile devices. In IEEE Conf. Comput. Vis. Pattern Recog., pp.
6848–6856, 2018b."
REFERENCES,0.4075471698113208,"Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In
IEEE Conf. Comput. Vis. Pattern Recog., June 2020."
REFERENCES,0.40943396226415096,"Liming Zhao, Mingjie Li, Depu Meng, Xi Li, Zhaoxiang Zhang, Yueting Zhuang, Zhuowen Tu, and
Jingdong Wang. Deep convolutional neural networks with merge-and-run mappings. In Jérôme
Lang (ed.), IJCAI, pp. 3170–3176, 2018."
REFERENCES,0.41132075471698115,"Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmenta-
tion. In Assoc. Adv. Artif. Intell., volume 34, pp. 13001–13008, 2020."
REFERENCES,0.41320754716981134,Published as a conference paper at ICLR 2022
REFERENCES,0.41509433962264153,"Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene
parsing through ade20k dataset. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 633–641, 2017."
REFERENCES,0.4169811320754717,"Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, and Jiashi Feng.
Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021."
REFERENCES,0.4188679245283019,Published as a conference paper at ICLR 2022
REFERENCES,0.4207547169811321,APPENDIX
REFERENCES,0.4226415094339623,"A
RELATION GRAPH"
REFERENCES,0.42452830188679247,"We present a relation graph in Figure 3 to describe the relation between convolution, depth-wise
separable convolution (depth-wise convolution + 1 × 1 convolution), Vision Transformer, Local
Vision Transformer, as well as multilayer perceptron (MLP), Separable MLP in terms of sparse
connectivity, weight sharing, and dynamic weight. Table 7"
REFERENCES,0.42641509433962266,"Multilayer perceptron (MLP) is a fully-connected layer: each neuron (an element at each position
and each channel) in one layer is connected with all the neurons in the previous layer5. Convolution
and separable MLP are sparse versions of MLP. The connection weights can be formulated as a
tensor (e.g., 3D tensor, two dimension for space and one dimension for channel) and the low-rank
approximation of the tensor can be used to regularize the MLP."
REFERENCES,0.42830188679245285,"Convolution is a locally-connected layer, formed by connecting each neuron to the neurons in a small
local window with the weights shared across the spatial positions. Depth-wise separable convolution
is formed by decomposing the convolution into two components: one is point-wise 1 × 1 convolution,
mixing the information across channels, and the other is depth-wise convolution, mixing the spatial
information. Other variants of convolution, such as bottleneck, multi-scale convolution or pyramid,
can be regarded as low-rank variants."
REFERENCES,0.43018867924528303,"Separable MLP (e.g., MLP-Mixer and ResMLP) reshapes the 3D tensor into a 2D format with the
spatial dimension and channel dimension. Separable MLP consists of two sparse MLP along the two
dimensions separately, which are formed by separating the input neurons into groups. Regarding
channel sparsity, the neurons in the same channel form a group, and an MLP is performed over each
group with the MLP parameters shared across groups, forming the ﬁrst sparse MLP (spatial/token
mixing). A similar process is done by viewing the neurons at the same position into a group, forming
the second sparse MLP (channel mixing)."
REFERENCES,0.4320754716981132,"Vision Transformer is a dynamic version of separable MLP. The weights in the ﬁrst sparse MLP
(spatial/token mixing) are dynamically predicted from each instance. Local Vision Transformer is a
spatially-sparser version of Vision Transformer: each output neuron is connected to the input neurons
in a local window. PVT (Wang et al., 2021b) is a pyramid (spatial sampling/ low-rank) variant of
Vision Transformer."
REFERENCES,0.4339622641509434,"Depth-wise separable convolution can also be regarded as a spatially-sparser version of separable
MLP. In the ﬁrst sparse MLP (spatial/token mixing), each output neuron is only dependent on the
input neurons in a local window, forming depth-wise convolution. In addition, the connection weights
are shared across spatial positions, instead of across channels."
REFERENCES,0.4358490566037736,"B
MATRIX FORM EXPLANATION"
REFERENCES,0.4377358490566038,"We use the matrix form to explain sparsity connectivity in various layers and how they are obtained
by modifying the MLP."
REFERENCES,0.439622641509434,"MLP. The term MLP, Multilayer Perceptron, is used ambiguously, sometimes loosely to any feedfor-
ward neural network. We adopt one of the common deﬁnitions, and use it to refer to fully-connected
layers. Our discussion is based on a single fully-connected layer, and can be easily generalized to
two or more fully-connected layers. One major component, except the nonlinear units and others, is a
linear transformation:"
REFERENCES,0.44150943396226416,"y = Wx,
(9)"
REFERENCES,0.44339622641509435,"where x represents the input neurons, y represents the output neurons, and W represents the
connection weights, e.g., W ∈RNC×NC, where N is the number of positions, and C is the number
of channels."
REFERENCES,0.44528301886792454,"Convolution. Considering the 1D case with a single channel (the 2D case is similar), the connection
weight matrix W ∈RN×N is in the following sparse form, also known as the Toeplitz matrix (We"
REFERENCES,0.44716981132075473,5We use the widely-used deﬁnition for the term MLP: fully-connected layer. There might be other deﬁnitions.
REFERENCES,0.4490566037735849,Published as a conference paper at ICLR 2022
REFERENCES,0.4509433962264151,regularization
REFERENCES,0.4528301886792453,"Pyramid
MS Conv."
REFERENCES,0.4547169811320755,Locality Sep.
REFERENCES,0.45660377358490567,Dim. Sep.
REFERENCES,0.45849056603773586,Locality Sep.
REFERENCES,0.46037735849056605,Dim. Sep.
REFERENCES,0.46226415094339623,Spatial
REFERENCES,0.4641509433962264,Spatial LR
REFERENCES,0.4660377358490566,Channel LR ViT MLP Conv.
REFERENCES,0.4679245283018868,Sparse Connection
REFERENCES,0.469811320754717,Dynamic Weight
REFERENCES,0.4716981132075472,Low-Rank
REFERENCES,0.47358490566037736,Bottleneck
REFERENCES,0.47547169811320755,"DW-S Conv.
Local ViT"
REFERENCES,0.47735849056603774,Sep. MLP
REFERENCES,0.47924528301886793,LR MLP
REFERENCES,0.4811320754716981,Dim. LR
REFERENCES,0.4830188679245283,Dynamic PVT
REFERENCES,0.4849056603773585,Locality Sep.
REFERENCES,0.4867924528301887,Spatial LR
REFERENCES,0.48867924528301887,"Dynamic
Local-S MLP"
REFERENCES,0.49056603773584906,"Inhomogeneous
Vec. Atten."
REFERENCES,0.49245283018867925,"Dynamic
DW-S Conv."
REFERENCES,0.49433962264150944,Vec. Atten.
REFERENCES,0.4962264150943396,"Figure 3: Relation graph for convolution (Conv.), depth-wise separable convolution (DW-S Conv.), Vision
Transformer (ViT) building block, local ViT building block, Sep. MLP (e.g., MLP-Mixer and ResMLP),
dynamic depth-wise separable convolution (Dynamic DW-S Conv.), as well as dynamic local separable MLP
( e.g., involution (Li et al., 2021) and inhomogeneous dynamic depth-wise convolution) in terms of sparse
connectivity and dynamic weight. Dim. = dimension including spatial and channel, Sep. = separable, LR = low
rank, MS Conv. = multi-scale convolution, PVT = pyramid vision transformer."
REFERENCES,0.4981132075471698,"Table 7: The comparison of attention, local MLP (non-dynamic version of local attention, the attention weights
are learned as static model parameters), local attention, convolution, depth-wise convolution (DW-Conv.) and the
dynamic variant (D-DW-Conv.), as well as MLP and MLP variants in terms of the patterns of sparse connectivity,
weight sharing, and dynamic weight. †Spatial-mixing MLP (channel-separable MLP) corresponds to token-mixer
MLP. ‡1 × 1 Conv. is also called point-wise (spatial-separable) MLP. ♭The weights might be shared within each
group of channels. Please refer to Figure 1 for the connectivity pattern illustration."
REFERENCES,0.5,"Sparse between positions
Sparse between
Weight sharing across
Dynamic
non-local
full
channels
position
channel
weight
Local MLP


♭"
REFERENCES,0.5018867924528302,"Local attention


♭

DW-Conv.



D-DW-Conv.




Conv.


MLP
Attention

♭

Spatial-mixing MLP†


1 × 1 Conv.‡

"
REFERENCES,0.5037735849056604,use the window size 3 as an example): W =  
REFERENCES,0.5056603773584906,"a2
a3
0
0
· · ·
0
a1
a1
a2
a3
0
· · ·
0
0
...
...
...
...
...
...
...
a3
0
0
0
· · ·
a1
a2 "
REFERENCES,0.5075471698113208,"
.
(10)"
REFERENCES,0.5094339622641509,"For the C-channel case, we organize the input into a vector channel by channel: [x⊤
1 x⊤
2 . . . x⊤
C]⊤,
and accordingly the connection weight matrix channel by channel for the coth output channel,
Wco = [Wco1 Wco2 . . . WcoC] (the form of Wcoi is the same as Equation 10). The whole form
could be written as
 "
REFERENCES,0.5113207547169811,"y1
y2
...
yC "
REFERENCES,0.5132075471698113,"
=  "
REFERENCES,0.5150943396226415,"W1
W2
...
WC    "
REFERENCES,0.5169811320754717,"x1
x2
...
xC "
REFERENCES,0.5188679245283019,"
.
(11)"
REFERENCES,0.5207547169811321,Published as a conference paper at ICLR 2022
REFERENCES,0.5226415094339623,"Sep. MLP. Sep. MLP, e.g., ResMLP and MLP-Mixer, is formed with two kinds of block-sparse
matrices: one for channel-mixing and the other for spatial-mixing. In the case that the input is
organized channel by channel (the neurons in each channel form a group), x = [x⊤
1 x⊤
2 . . . x⊤
C]⊤,
the connection weight matrix is in a block-sparse form: W =  "
REFERENCES,0.5245283018867924,"Wc
0
· · ·
0
0
0
Wc
· · ·
0
0
...
...
...
...
...
0
0
· · ·
0
Wc "
REFERENCES,0.5264150943396226,"
,
(12)"
REFERENCES,0.5283018867924528,"where the block matrices Wc ∈RN×N are shared across all the channels, and the sharing pattern
can be modiﬁed to share weights within each group of channels."
REFERENCES,0.530188679245283,"The input can be reshaped position by position (the neurons at each position forms a group): x =
[x⊤
1 x⊤
2 . . . x⊤
N]⊤, and similarly one more connection weight matrix can be formulated in a block-
sparse form (it is essentially a 1 × 1 convolution, Wp ∈RC×C): W′ =  "
REFERENCES,0.5320754716981132,"Wp
0
· · ·
0
0
0
Wp
· · ·
0
0
...
...
...
...
...
0
0
· · ·
0
Wp "
REFERENCES,0.5339622641509434,"
.
(13)"
REFERENCES,0.5358490566037736,"The forms of block-sparsity are studied in interleaved group convolutions (Zhang et al., 2017) without
sharing the weights across groups."
REFERENCES,0.5377358490566038,"Sep. MLP can also be regarded as using Kronecker product to approximate the connection matrix,"
REFERENCES,0.539622641509434,"Wx = vec(A mat(x)B).
(14)"
REFERENCES,0.5415094339622641,"Here, W = B⊤⊗A = W⊤
c ⊗Wp. and ⊗is the Kronecker product operator. mat(x) reshapes
the vector x in a 2D matrix form, while vec(x) reshapes the 2D matrix into a vector form. In Sep.
MLP, the 2D matrix, mat(x) ∈RC×N, is organized so that each row corresponds to one channel
and each column corresponds to one spatial position. CCNet (Huang et al., 2019b) and interlaced
self-attention (Huang et al., 2019a) use Kronecker product to approximate the spatial connection: the
former reshapes the vector in a 2D matrix form along the x and y axes, and the latter reshapes the
vector windows by windows."
REFERENCES,0.5433962264150943,"Vision Transformer (ViT). The matrix form is similar to Sep. MLP. The difference is that the matrix
Wc is predicted from each image instance. The weight prediction manner in ViT has a beneﬁt: handle
an arbitrary number of input neurons."
REFERENCES,0.5452830188679245,"Depth-wise separable convolution. There are two basic components: depth-wise convolution, and
1 × 1 convolution that is the same as channel-mixing MLP in Sep. MLP. Depth-wise convolution can
be written in the matrix form:
 "
REFERENCES,0.5471698113207547,"y1
y2
...
yC "
REFERENCES,0.5490566037735849,"
=  "
REFERENCES,0.5509433962264151,"W11
0
· · ·
0
0
W22
· · ·
0
...
...
...
...
0
0
· · ·
WCC    "
REFERENCES,0.5528301886792453,"x1
x2
...
xC "
REFERENCES,0.5547169811320755,"
,
(15)"
REFERENCES,0.5566037735849056,where the form of Wcc is the same as Equation 10.
REFERENCES,0.5584905660377358,"Local ViT. In the non-overlapping window partition case, local ViT simply repeats ViT over each
window separately with the linear projections, applied to keys, values, and queries, shared across
windows. In the overlapping case, the form is a little complicated, but the intuition is the same. In the"
REFERENCES,0.560377358490566,Published as a conference paper at ICLR 2022
REFERENCES,0.5622641509433962,"extreme case, the partition is the same as convolution, and the form is as the following:
 "
REFERENCES,0.5641509433962264,"y1
y2
...
yC "
REFERENCES,0.5660377358490566,"
= "
REFERENCES,0.5679245283018868,
REFERENCES,0.569811320754717,"Wd
0
· · ·
0
0
Wd
· · ·
0
...
...
...
...
0
0
· · ·
Wd "
REFERENCES,0.5716981132075472,  
REFERENCES,0.5735849056603773,"x1
x2
...
xC "
REFERENCES,0.5754716981132075,"
,
(16)"
REFERENCES,0.5773584905660377,where the dynamic weight matrix Wd is like the form below: Wd =  
REFERENCES,0.5792452830188679,"a12
a13
0
0
· · ·
0
a11
a21
a22
a23
0
· · ·
0
0
...
...
...
...
...
...
...
aN3
0
0
0
· · ·
aN1
aN2 "
REFERENCES,0.5811320754716981,"
.
(17)"
REFERENCES,0.5830188679245283,"Low-rank MLP. Low-rank MLP approximates the connection weight matrix W ∈RDo×Di in
Equation 9 using the product of two low-rank matrix:"
REFERENCES,0.5849056603773585,"W ←WDorWrDi,
(18)"
REFERENCES,0.5867924528301887,where r is a number smaller than Di and Do
REFERENCES,0.5886792452830188,"Pyramid. The downsampling process in the pyramid networks can be regarded as spatial low
rank: W(∈RNC×NC) →W′(∈RN′C×N′C), where N ′ is equal to N"
IN THE CASE THAT THE,0.590566037735849,"4 in the case that the
resolution is reduced by 1"
IN THE CASE THAT THE,0.5924528301886792,"2. If the numbers of input and output channels are different, it becomes
W(∈RNC′×NC) →W′(∈RN′C′×N ′C)."
IN THE CASE THAT THE,0.5943396226415094,"Multi-scale parallel convolution. Multi-scale parallel convolution used in HRNet (Wang et al.,
2020; Sun et al., 2019) can also be regarded as spatial low rank. Consider the case with four scales,
multi-scale parallel convolution can be formed as as the following, W →  "
IN THE CASE THAT THE,0.5962264150943396,W1 ∈RNC1
IN THE CASE THAT THE,0.5981132075471698,W2 ∈RNC2
IN THE CASE THAT THE,0.6,W3 ∈RNC3
IN THE CASE THAT THE,0.6018867924528302,W4 ∈RNC4  →  
IN THE CASE THAT THE,0.6037735849056604,"W′
1 ∈RNC1"
IN THE CASE THAT THE,0.6056603773584905,"W′
2 ∈R
N 4 C2"
IN THE CASE THAT THE,0.6075471698113207,"W′
3 ∈R
N
16 C3"
IN THE CASE THAT THE,0.6094339622641509,"W′
4 ∈R
N
64 C4 "
IN THE CASE THAT THE,0.6113207547169811,"
,
(19)"
IN THE CASE THAT THE,0.6132075471698113,"where C1, C2, C3, and C4 are the numbers of the channels in four resolutions."
IN THE CASE THAT THE,0.6150943396226415,"C
LOCAL ATTENTION VS CONVOLUTION: DYNAMIC WEIGHTS"
IN THE CASE THAT THE,0.6169811320754717,"We take the 1D case with the window size 2K + 1 as an example to illustrate the dynamic weight
prediction manner. Let {xi−K, . . . , xi, . . . , xi+k} correspond to the (2K + 1) positions in the ith
window, and {wi−K, . . . , wi, . . . , wi+K} be the corresponding dynamic weights for updating the
representation of the ith (center) position. The discussion can be easily extended to multiple weights
for each positions, like the M-head attention and updating the representations for other positions."
IN THE CASE THAT THE,0.6188679245283019,"Inhomogeneous dynamic convolution. We use the case using only a single linear projection to
illustrate inhomogeneous dynamic convolution. The properties we will discuss are similar for more
linear projections. The dynamic weights are predicted as the following:
"
IN THE CASE THAT THE,0.620754716981132,
IN THE CASE THAT THE,0.6226415094339622,"wi−K
...
wi
...
wi+K "
IN THE CASE THAT THE,0.6245283018867924,
IN THE CASE THAT THE,0.6264150943396226,= Θxi = 
IN THE CASE THAT THE,0.6283018867924528,
IN THE CASE THAT THE,0.630188679245283,"θ⊤
−K
..."
IN THE CASE THAT THE,0.6320754716981132,"θ⊤
0
... θ⊤
K "
IN THE CASE THAT THE,0.6339622641509434,
IN THE CASE THAT THE,0.6358490566037736,"xi.
(20)"
IN THE CASE THAT THE,0.6377358490566037,Published as a conference paper at ICLR 2022
IN THE CASE THAT THE,0.6396226415094339,"It can be seen that dynamic convolution learns the weights for each position through the parameters
that are different for different positions, e.g., θk corresponds to wi+k. It regards the positions in the
window as the vector form, keeping the spatial order information."
IN THE CASE THAT THE,0.6415094339622641,"Dot-product attention. The dot-product attention mechanism in the single-head case predicts the
weights as the following6: "
IN THE CASE THAT THE,0.6433962264150943,
IN THE CASE THAT THE,0.6452830188679245,"wi−K
...
wi
...
wi+K "
IN THE CASE THAT THE,0.6471698113207547, = 
IN THE CASE THAT THE,0.6490566037735849,
IN THE CASE THAT THE,0.6509433962264151,(xi−K)⊤
IN THE CASE THAT THE,0.6528301886792452,"...
(xi)⊤"
IN THE CASE THAT THE,0.6547169811320754,"...
(xi+K)⊤ "
IN THE CASE THAT THE,0.6566037735849056,
IN THE CASE THAT THE,0.6584905660377358,"P⊤
k Pqxi.
(21)"
IN THE CASE THAT THE,0.660377358490566,"Dot-product attention uses the same parameters P⊤
k Pq for all the positions. The weight depends
on the features at the same position, e.g., wi−k corresponds to xi−k. It in some sense regards the
positions in the window as a set form, losing the spatial order information."
IN THE CASE THAT THE,0.6622641509433962,We rewrite it as the following Θd = 
IN THE CASE THAT THE,0.6641509433962264,
IN THE CASE THAT THE,0.6660377358490566,(xi−K)⊤
IN THE CASE THAT THE,0.6679245283018868,"...
(xi)⊤"
IN THE CASE THAT THE,0.6698113207547169,"...
(xi+K)⊤ "
IN THE CASE THAT THE,0.6716981132075471,
IN THE CASE THAT THE,0.6735849056603773,"P⊤
k Pq,
(22)"
IN THE CASE THAT THE,0.6754716981132075,"from which we can see that the parameters Θd is dynamically predicted. In other words, dot-product
attention can be regarded as a two-level dynamic scheme."
IN THE CASE THAT THE,0.6773584905660377,"Relative position embeddings is equivalent to adding static weights that keeps the spatial order
information:
"
IN THE CASE THAT THE,0.6792452830188679,
IN THE CASE THAT THE,0.6811320754716981,"wi−K
...
wi
...
wi+K "
IN THE CASE THAT THE,0.6830188679245283,
IN THE CASE THAT THE,0.6849056603773584,= Θdxi + 
IN THE CASE THAT THE,0.6867924528301886,
IN THE CASE THAT THE,0.6886792452830188,"β−K
...
β0
...
βK "
IN THE CASE THAT THE,0.690566037735849,
IN THE CASE THAT THE,0.6924528301886792,".
(23)"
IN THE CASE THAT THE,0.6943396226415094,"A straightforward variant is a combination of the static Θ and the dynamic Θd:
"
IN THE CASE THAT THE,0.6962264150943396,
IN THE CASE THAT THE,0.6981132075471698,"wi−K
...
wi
...
wi+K "
IN THE CASE THAT THE,0.7,
IN THE CASE THAT THE,0.7018867924528301,"= (Θd + Θ)xi.
(24)"
IN THE CASE THAT THE,0.7037735849056603,"Convolutional attention. We introduce a convolutional attention framework so that it enjoys the
beneﬁts of dynamic convolution and dot-product attention: keep the spatial order information and
two-level dynamic weight prediction."
IN THE CASE THAT THE,0.7056603773584905,"6For presentation clarity, we omit the softmax normalization and the scale in dot-product. What we discuss
still holds if softmax and scale are included."
IN THE CASE THAT THE,0.7075471698113207,Published as a conference paper at ICLR 2022
IN THE CASE THAT THE,0.7094339622641509,The post-convolutional attention mechanism left-multiplies a matrix (with the kernel size being 3): Θd =  
IN THE CASE THAT THE,0.7113207547169811,"a2
a3
0
0
· · ·
0
a1
a1
a2
a3
0
· · ·
0
0
...
...
...
...
...
...
...
a3
0
0
0
· · ·
a1
a2   "
IN THE CASE THAT THE,0.7132075471698113,
IN THE CASE THAT THE,0.7150943396226415,(xi−K)⊤
IN THE CASE THAT THE,0.7169811320754716,"...
(xi)⊤"
IN THE CASE THAT THE,0.7188679245283018,"...
(xi+K)⊤ "
IN THE CASE THAT THE,0.720754716981132,
IN THE CASE THAT THE,0.7226415094339622,"P⊤
k Pq.
(25)"
IN THE CASE THAT THE,0.7245283018867924,"This can be reviewed as a variant of relative positional embeddings (Equation 23). In the simpliﬁed
case that the left matrix is diagonal, it can be regarded as the product version of relative positional
embeddings (Equation 23 is an addition version)."
IN THE CASE THAT THE,0.7264150943396226,"We can perform a convolution with the kernel size being 3, the kernel weights shared across channels
(it is also ﬁne not to share weights), and then do dot-product attention. This is called pre-convolutional
attention: perform convolutions on the representations. The two processes are can be written as
follows (omit BN and ReLU that follow the convolution), "
IN THE CASE THAT THE,0.7283018867924528,
IN THE CASE THAT THE,0.730188679245283,"wi−K
...
wi
...
wi+K "
IN THE CASE THAT THE,0.7320754716981132, = 
IN THE CASE THAT THE,0.7339622641509433,
IN THE CASE THAT THE,0.7358490566037735,"a1
a2
a3
· · ·
0
0
0
0
a1
a1
· · ·
0
0
0
...
...
...
...
...
...
...
0
0
0
· · ·
a2
a3
0
0
0
0
· · ·
a1
a2
a3 "
IN THE CASE THAT THE,0.7377358490566037, 
IN THE CASE THAT THE,0.7396226415094339,
IN THE CASE THAT THE,0.7415094339622641,(xi−K−1)⊤
IN THE CASE THAT THE,0.7433962264150943,(xi−K)⊤
IN THE CASE THAT THE,0.7452830188679245,"...
(xi)⊤"
IN THE CASE THAT THE,0.7471698113207547,"...
(xi+K)⊤"
IN THE CASE THAT THE,0.7490566037735849,(xi+K+1)⊤ 
IN THE CASE THAT THE,0.7509433962264151,
IN THE CASE THAT THE,0.7528301886792453,"P⊤
k Pq [xi−1
xi
xi+1] "
IN THE CASE THAT THE,0.7547169811320755,"
a1
a2
a3  . (26)"
IN THE CASE THAT THE,0.7566037735849057,It can be generalized to using normal convolution: 
IN THE CASE THAT THE,0.7584905660377359,
IN THE CASE THAT THE,0.7603773584905661,"wi−K
...
wi
...
wi+K "
IN THE CASE THAT THE,0.7622641509433963, = C′ 
IN THE CASE THAT THE,0.7641509433962265,
IN THE CASE THAT THE,0.7660377358490567,"xi−K−1
xi−K−1
· · ·
xi−K−1
xi−K
xi−K
· · ·
xi−K
...
...
...
...
xi
xi
· · ·
xi
...
...
...
...
xi+K
xi+K
· · ·
xi+K
xi+K+1
xi+K+1
· · ·
xi+K+1 "
IN THE CASE THAT THE,0.7679245283018868,
IN THE CASE THAT THE,0.769811320754717,"P⊤
k PqC3 "
IN THE CASE THAT THE,0.7716981132075472,"
xi−1
xi
xi+1 "
IN THE CASE THAT THE,0.7735849056603774,".
(27)"
IN THE CASE THAT THE,0.7754716981132076,"Here, C’ is a (2K + 1)-row matrix and can be easily derived from the convolutional kernel C3.
The (2K + 1) weights, {wi−1, wi, wi+1}, correspond to the (2K + 1) rows in C, respectively. This
means that the three positions are differentiated and the same position in each window corresponds to
the same row. This explains why the positional embeddings are not necessary when convolutions are
adopted (Wu et al., 2021). Using different pairs (Wq, Wk) leads to more weights for each position,
e.g., M pairs correspond to M-head attention."
IN THE CASE THAT THE,0.7773584905660378,"D
ARCHITECTURE DETAILS"
IN THE CASE THAT THE,0.779245283018868,"Overall structures. Following local vision transformer, Swin Transformer (Liu et al., 2021b), we
build two depth-wise convolution-based networks, namely DW-Conv.-T and DW-Conv.-B. The
corresponding dynamic versions are D-DW-Conv.-T, D-DW-Conv.-B, I-D-DW-Conv.-T, and I-D-
DW-Conv.-B. The depth-wise convolution-based networks follow the overall structure of Swin
Transformer. We replace local self attention by depth-wise convolution with the same window size.
We use batch normalization (Ioffe & Szegedy, 2015) and ReLU (Nair & Hinton, 2010) instead of
layer normalization (Ba et al., 2016) in the convolution blocks."
IN THE CASE THAT THE,0.7811320754716982,Published as a conference paper at ICLR 2022
IN THE CASE THAT THE,0.7830188679245284,"Table 8: Architectures details of Swin Transformer and depth-wise convolution-based network (DW
Conv.) for the tiny model. The architectures for the base model can be easily obtained."
IN THE CASE THAT THE,0.7849056603773585,"downsp. rate
(output size)
Swin
DW Conv."
IN THE CASE THAT THE,0.7867924528301887,"stage 1
4×
(56×56)"
IN THE CASE THAT THE,0.7886792452830189,"concat 4×4, linear 96-d, LN
concat 4×4, linear 96-d, LN
"
IN THE CASE THAT THE,0.7905660377358491,
IN THE CASE THAT THE,0.7924528301886793,"LN, linear 96x3-d"
IN THE CASE THAT THE,0.7943396226415095,"local sa. 7×7, head 3"
IN THE CASE THAT THE,0.7962264150943397,linear 96-d
IN THE CASE THAT THE,0.7981132075471699,"LN, linear 384-d"
IN THE CASE THAT THE,0.8,"GELU, linear 96-d "
IN THE CASE THAT THE,0.8018867924528302, × 2 
IN THE CASE THAT THE,0.8037735849056604,
IN THE CASE THAT THE,0.8056603773584906,"linear 96-d, BN, ReLU"
IN THE CASE THAT THE,0.8075471698113208,"depthwise conv. 7×7, BN, ReLU"
IN THE CASE THAT THE,0.809433962264151,"linear 96-d, BN"
IN THE CASE THAT THE,0.8113207547169812,"BN, linear 384-d"
IN THE CASE THAT THE,0.8132075471698114,"GELU, linear 96-d "
IN THE CASE THAT THE,0.8150943396226416, × 2
IN THE CASE THAT THE,0.8169811320754717,"stage 2
8×
(28×28)"
IN THE CASE THAT THE,0.8188679245283019,"concat 2×2, linear 192-d , LN
concat 2×2, linear 192-d , LN
"
IN THE CASE THAT THE,0.8207547169811321,
IN THE CASE THAT THE,0.8226415094339623,"LN, linear 192x3-d"
IN THE CASE THAT THE,0.8245283018867925,"local sa. 7×7, head 6"
IN THE CASE THAT THE,0.8264150943396227,linear 192-d
IN THE CASE THAT THE,0.8283018867924529,"LN, linear 768-d"
IN THE CASE THAT THE,0.8301886792452831,"GELU, linear 192-d "
IN THE CASE THAT THE,0.8320754716981132, × 2 
IN THE CASE THAT THE,0.8339622641509434,
IN THE CASE THAT THE,0.8358490566037736,"linear 192-d, BN, ReLU"
IN THE CASE THAT THE,0.8377358490566038,"depthwise conv. 7×7, BN, ReLU"
IN THE CASE THAT THE,0.839622641509434,"linear 192-d, BN"
IN THE CASE THAT THE,0.8415094339622642,"BN, linear 768-d"
IN THE CASE THAT THE,0.8433962264150944,"GELU, linear 192-d "
IN THE CASE THAT THE,0.8452830188679246, × 2
IN THE CASE THAT THE,0.8471698113207548,"stage 3
16×
(14×14)"
IN THE CASE THAT THE,0.8490566037735849,"concat 2×2, linear 384-d , LN
concat 2×2, linear 384-d , LN
"
IN THE CASE THAT THE,0.8509433962264151,
IN THE CASE THAT THE,0.8528301886792453,"LN, linear 384x3-d"
IN THE CASE THAT THE,0.8547169811320755,"local sa. 7×7, head 12"
IN THE CASE THAT THE,0.8566037735849057,linear 384-d
IN THE CASE THAT THE,0.8584905660377359,"LN, linear 1536-d"
IN THE CASE THAT THE,0.8603773584905661,"GELU, linear 384-d "
IN THE CASE THAT THE,0.8622641509433963, × 6 
IN THE CASE THAT THE,0.8641509433962264,
IN THE CASE THAT THE,0.8660377358490566,"linear 384-d, BN, ReLU"
IN THE CASE THAT THE,0.8679245283018868,"depthwise conv. 7×7, BN, ReLU"
IN THE CASE THAT THE,0.869811320754717,"linear 384-d, BN"
IN THE CASE THAT THE,0.8716981132075472,"BN, linear 1536-d"
IN THE CASE THAT THE,0.8735849056603774,"GELU, linear 384-d "
IN THE CASE THAT THE,0.8754716981132076, × 6
IN THE CASE THAT THE,0.8773584905660378,"stage 4
32×
(7×7)"
IN THE CASE THAT THE,0.879245283018868,"concat 2×2, linear 768-d , LN
concat 2×2, linear 768-d , LN
"
IN THE CASE THAT THE,0.8811320754716981,
IN THE CASE THAT THE,0.8830188679245283,"LN, linear 768x3-d"
IN THE CASE THAT THE,0.8849056603773585,"local sa. 7×7, head 24"
IN THE CASE THAT THE,0.8867924528301887,linear 768-d
IN THE CASE THAT THE,0.8886792452830189,"LN, linear 3072-d"
IN THE CASE THAT THE,0.8905660377358491,"GELU, linear 768-d "
IN THE CASE THAT THE,0.8924528301886793, × 2 
IN THE CASE THAT THE,0.8943396226415095,
IN THE CASE THAT THE,0.8962264150943396,"linear 768-d, BN, ReLU"
IN THE CASE THAT THE,0.8981132075471698,"depthwise conv. 7×7, BN, ReLU"
IN THE CASE THAT THE,0.9,"linear 768-d, BN"
IN THE CASE THAT THE,0.9018867924528302,"BN, linear 3072-d"
IN THE CASE THAT THE,0.9037735849056604,"GELU, linear 768-d "
IN THE CASE THAT THE,0.9056603773584906, × 2
IN THE CASE THAT THE,0.9075471698113208,"stage 4
1×1
LN, AvgPool. 1×1
LN, AvgPool. 1×1"
IN THE CASE THAT THE,0.909433962264151,"linear classiﬁer
linear classiﬁer"
IN THE CASE THAT THE,0.9113207547169812,"Table 8 shows the architecture details of Swin Transformer and depth-wise convolution-based
networks for the tiny model. Normalizations are performed within the residual block, same as Swin
Transformer. The base model is similarly built by following Swin Transformer to change the number
of channels and the depth of the third stage."
IN THE CASE THAT THE,0.9132075471698113,"Dynamic depth-wise convolution. Dynamic depth-wise convolution generates the connection
weights according to the instance. As described in Section 2.4, for the homogeneous version, we
conduct the global average pooling operation to get a vector, and adopt two linear projections: the ﬁrst
one reduces the dimension by 1/4, followed by BN and ReLU, and then generate the kernel weights
and shared for all spatial positions. Unlike SENet (Hu et al., 2018b), we currently do not use the
Sigmoid activation function for generating the weights. For the inhomogeneous version, we generate
unshared dynamic weight for each spatial position using the corresponding feature. The connection
weights are shared across channels to reduce the model parameters and computation complexity.
Speciﬁcally, we share 3 and 4 channels in each group of channels for tiny and base models. Thus the
number of model parameters and computation complexity are similar to Swin Transformer."
IN THE CASE THAT THE,0.9150943396226415,Published as a conference paper at ICLR 2022
IN THE CASE THAT THE,0.9169811320754717,"Table 9: ImageNet classiﬁcation comparison for ResNet, HRNet, Mixer and ResMLP and gMLP, ViT and
DeiT, Swin (Swin Transformer), DW-Conv. (depth-wise convolution), and D-DW-Conv. (dynamic depth-wise
convolution). † means that ResNet is built by using two 3 × 3 convolutions to form the residual units. Table 7
presents the comparison for representative modules in terms of spare connectivity, weight sharing and dynamic
weight."
IN THE CASE THAT THE,0.9188679245283019,"method
img. size #param.
FLOPs throughput (img. / s) top-1 acc. real acc.
Convolution: local connection
ResNet-38 † (Wang et al., 2020)
2242
28M
3.8G
2123.7
75.4
-
ResNet-72 † (Wang et al., 2020)
2242
48M
7.5G
623.0
76.7
-
ResNet-106 † (Wang et al., 2020)
2242
65M
11.1G
452.8
77.3
-
Bottleneck: convolution with low rank
ResNet-50 (He et al., 2016)
2242
26M
4.1G
1128.3
76.2
82.5
ResNet-101 (He et al., 2016)
2242
45M
7.9G
652.0
77.4
83.7
ResNet-152 (He et al., 2016)
2242
60M
11.6G
456.7
78.3
84.1
Pyramid: convolution with pyramid (spatial low rank) features.
HRNet-W18 (Wang et al., 2020)
2242
21M
4.0G
-
76.8
-
HRNet-W32 (Wang et al., 2020)
2242
41M
8.3G
-
78.5
-
HRNet-W48 (Wang et al., 2020)
2242
78M
16.1G
-
79.3
-
Channel and spatial separable MLP, spatial separable MLP = point-wise 1 × 1 convolution
Mixer-B/16 (Tolstikhin et al., 2021)
2242
46M
-
-
76.4
82.4
Mixer-L/16 (Tolstikhin et al., 2021)
2242
189M
-
-
71.8
77.1
ResMLP-12 (Touvron et al., 2021)
2242
15M
3.0G
-
76.6
83.3
ResMLP-24 (Touvron et al., 2021)
2242
30M
6.0G
-
79.4
85.3
ResMLP-36 (Touvron et al., 2021)
2242
45M
8.9G
-
79.7
85.6
gMLP-Ti (Liu et al., 2021a)
2242
6M
1.4G
-
72.0
-
gMLP-S (Liu et al., 2021a)
2242
20M
4.5G
-
79.4
-
gMLP-B (Liu et al., 2021a)
2242
73M
15.8G
-
81.6
-
Global attention: dynamic channel separable MLP + spatial separable MLP
ViT-B/16 (Dosovitskiy et al., 2021)
3842
86M
55.4G
83.4
77.9
83.6
ViT-L/16 (Dosovitskiy et al., 2021)
3842
307M 190.7G
26.5
76.5
82.2
DeiT-S (Touvron et al., 2020)
2242
22M
4.6G
947.3
79.8
85.7
DeiT-B (Touvron et al., 2020)
2242
86M
17.5G
298.2
81.8
86.7
DeiT-B (Touvron et al., 2020)
3842
86M
55.4G
82.7
83.1
87.7
Pyramid attention: perform attention with spatial low rank
PVT-S (Wang et al., 2021b)
2242
25M
3.8G
-
79.8
-
PVT-M (Wang et al., 2021b)
2242
44M
6.7G
-
81.2
-
PVT-L (Wang et al., 2021b)
2242
61M
9.8G
-
81.7
-
Local MLP: perform static separable MLP in local small windows
Swin-Local MLP-T
2242
26M
3.8G
861.0
80.3
86.1
Swin-Local MLP-B
2242
79M
12.9G
321.2
82.2
86.9
Local attention: perform attention in local small windows
Swin-T (Liu et al., 2021b)
2242
28M
4.5G
713.5
81.3
86.6
Swin-B (Liu et al., 2021b)
2242
88M
15.4G
263.0
83.3
87.9
Depth-wise convolution + point-wise 1 × 1 convolution
DW-Conv.-T
2242
24M
3.8G
928.7
81.3
86.8
DW-Conv.-B
2242
74M
12.9G
327.6
83.2
87.9
D-DW-Conv.-T
2242
51M
3.8G
897.0
81.9
87.3
D-DW-Conv.-B
2242
162M
13.0G
322.4
83.2
87.9
I-D-DW-Conv.-T
2242
26M
4.4G
685.3
81.8
87.1
I-D-DW-Conv.-B
2242
80M
14.3G
244.9
83.4
88.0"
IN THE CASE THAT THE,0.9207547169811321,"E
SETTING DETAILS"
IN THE CASE THAT THE,0.9226415094339623,"ImageNet pretraining. We use the identical training setting with Swin Transformer in ImageNet pre-
training for fair comparison. The default input size is 224 × 224. The AdamW optimizer (Loshchilov
& Hutter, 2019), with the initial learning rate 0.001 and the weight decay 0.05, is used for 300 epochs."
IN THE CASE THAT THE,0.9245283018867925,Published as a conference paper at ICLR 2022
IN THE CASE THAT THE,0.9264150943396227,"The learning rate is scheduled by a cosine decay schema and warm-up with linear schema for the
ﬁrst 20 epochs. We train the model on 8 GPUs with the total batch size 1024. The augmentation and
regularization strategies are same as Swin Transformer, which includes RandAugment (Cubuk et al.,
2020), Mixup (Zhang et al., 2018a), CutMix (Yun et al., 2019), random erasing (Zhong et al., 2020)
and stochastic depth (Huang et al., 2016). The stochastic depth rate is employed as 0.2 and 0.5 for
the tiny and base models, respectively, the same as Swin Transformer."
IN THE CASE THAT THE,0.9283018867924528,"COCO object detection. We follow Swin Transformer to adopt Cascade Mask R-CNN (Cai &
Vasconcelos, 2019) for comparing backbones. We use the training and test settings from Swin
Transformer: multi-scale training - resizing the input such that the shorter side is between 480 and
800 and the longer side is at most 1333; AdamW optimizer with the initial learning rate 0.0001;
weight decay - 0.05; batch size - 16; and epochs - 36."
IN THE CASE THAT THE,0.930188679245283,"ADE semantic segmentation. Following Swin Transformer, we use UPerNet (Xiao et al., 2018)
as the segmentation framework. We use the same setting as the Swin Transformer: the AdamW
optimizer with initial learning rate 0.00006; weight decay 0.01; linear learning rate decay; 160,000
iterations with warm-up for 1500 iterations; 8 GPUs with mini-batch 2 per GPU. We use the same
data augmentation as Swin Transformer based on MMSegmentation (Contributors, 2020). The
experimental results are reported as single scale testing."
IN THE CASE THAT THE,0.9320754716981132,"Static version of Swin Transformer - Local MLP. We remove the linear projections applied to
keys and queries, accordingly dot production and softmax normalization. The connection weights
(corresponding to attention weights in the dynamic version) are set as static model parameters which
are learnt during the training and shared for all the images."
IN THE CASE THAT THE,0.9339622641509434,"Retraining on 384 × 384. We retrain the depth-wise convolution-based network on the ImageNet
dataset with 384 × 384 input images from the model trained with 224 × 224 images. We use learning
rate 10−5, weight decay 10−8 and stochastic depth ratio 0.1 for 30 epochs for both 7 × 7 and 12 × 12
windows."
IN THE CASE THAT THE,0.9358490566037736,"F
ADDITIONAL EXPERIMENTS AND ANALYSIS"
IN THE CASE THAT THE,0.9377358490566038,"More results on ImageNet classiﬁcation. We give more experimental results with different sparse
connection strategies, as shown in Table 9. These results also verify that locality-based sparsity pattern
(adopted in depth-wise convolution and local attention) besides sparsity between channels/spatial
positions still facilitates the network training for ImageNet-1K."
IN THE CASE THAT THE,0.939622641509434,"Results on large scale pre-training. Transformers (Liu et al., 2021b; Dosovitskiy et al., 2021)
show higher performance compared with the previous convolutional networks with large scale pre-
training. We further study the performance on ImageNet-22K pre-training. We ﬁrst train the model
on ImageNet-22K dataset which has about 14.2 million images, and then ﬁne-tune the model on
ImageNet-1K classiﬁcation, downstream detection and segmentation tasks. The same training settings
with Swin transformer are used in all tasks. The ﬁne-tuning results in Table 10 and Table 11 indicate
the (dynamic) depth-wise convolution based networks could get the performance comparable to Swin
transformer with large scale pre-training."
IN THE CASE THAT THE,0.9415094339622642,"Cooperating with different normalization functions. Transformers usually use the layer normal-
ization to stabilize the training, while convolutional architectures adopt batch normalization. We
verify different combinations of backbones (Swin and DW Conv.) and normalization functions. The
popular used layer normalization (LN), batch normalization (BN), and the dynamic version of batch"
IN THE CASE THAT THE,0.9433962264150944,Table 10: Comparison on ImageNet-1K classiﬁcation with ImageNet-22K pre-training.
IN THE CASE THAT THE,0.9452830188679245,"ImageNet-1K ﬁne-tuning
#param.
FLOPs
top-1 acc.
Swin-B
88M
15.4G
85.2
DW-Conv.-B
74M
12.9G
84.8
D-DW-Conv.-B
162M
13.0G
85.0
I-D-DW-Conv.-B
80M
14.3G
85.2"
IN THE CASE THAT THE,0.9471698113207547,Published as a conference paper at ICLR 2022
IN THE CASE THAT THE,0.9490566037735849,"Table 11: Comparison results on COCO object detection and ADE semantic segmentation with
ImageNet-22k pre-training."
IN THE CASE THAT THE,0.9509433962264151,"COCO ﬁne-tuning
ADE20K ﬁne-tuning
#param.
FLOPs
APbox
APbox
50
APbox
75
APmask
#param.
FLOPs
mIoU
Swin-B
145M
986G
53.4
72.1
58.1
46.1
121M
1192G
49.4
DW Conv.-B
132M
924G
52.0
70.4
56.3
45.0
108M
1129G
50.1
D-DW Conv.-B
219M
924G
51.9
70.7
56.2
45.0
195M
1129G
49.6
I-D-DW Conv.-B
137M
948G
52.9
71.2
57.2
45.8
114M
1153G
51.3"
IN THE CASE THAT THE,0.9528301886792453,"Table 12: Exploring normalization schemes of Swin Transformer and depth-wise convolution based
networks (DW Conv.) for the tiny model. The results are reported on the ImageNet top-1 accuracy."
IN THE CASE THAT THE,0.9547169811320755,"Layer Norm.
Batch Norm.
Centering calibrated Batch Norm.
Top-1 Acc.
Swin

81.3
Swin

80.9
Swin

81.2
DW Conv.

81.2
DW Conv.

81.3
DW Conv.

81.7"
IN THE CASE THAT THE,0.9566037735849057,"normalization - centering calibrated batch normalization (Gao et al., 2021) (CC. BN) are veriﬁed in
the experiments. Table 12 shows the results on ImageNet classiﬁcation."
IN THE CASE THAT THE,0.9584905660377359,"Depth-wise convolution with other architectures. We conduct experiments on other local attention
designs, such as SVT (Chu et al., 2021a) and VOLO (Yuan et al., 2021c) whose implementations
are publicly available. SVT uses local self attention as a basic spatial feature fusion operation, while
VOLO proposes a new attention module named Vision Outlooker. We replace the local self attention
with depth-wise convolution in SVT same as the paper, and replace Vision Outlooker with 7 × 7 local
self attention and 7 × 7 depth-wise convolution, respectively. The remaining structures are unchanged
and the same training setting is used as the original papers. The experimental results are shown in
Tab 13 and the observations are the same as the Swin Transformer design."
IN THE CASE THAT THE,0.960377358490566,"Retraining on 384 × 384 images. Similar to (Liu et al., 2021b), we study the performance of ﬁne-
tuning the models: ﬁrst learn with 224 × 224 images, then ﬁne-tune on large images of 384 × 384.
We study two cases: (1) keep the window size 7 × 7 unchanged; and (2) upsample the kernel weights
from 7×7 to 12×12 as done in (Liu et al., 2021b) for upsampling the relative positional embeddings."
IN THE CASE THAT THE,0.9622641509433962,"The results are in Table 147. In the case of keeping the window size 7 × 7 unchanged, depth-wise
convolution (DW) performs better. When using a larger window size 12 ×12, depth-wise convolution
performs worse than 7×7. We suspect that this is because upsampling the kernel weights is not a good
starting for ﬁne-tuning. In Swin Transformer, using a larger window size improves the performance.
We believe that this is because the local attention mechanism is suitable for variable window sizes."
IN THE CASE THAT THE,0.9641509433962264,"Cooperating with SE. Squeeze-and-excitation (Hu et al., 2018b) (SE) is a parameter- and
computation-efﬁcient dynamic module, initially designed for improving the ResNet performance.
The results in Table 15 show that depth-wise convolution (DW), a static module, beneﬁts from the SE
module, while Swin Transformer, already a dynamic module, does not beneﬁt from dynamic module
SE. The reason is still unclear, and might lie in the optimization."
IN THE CASE THAT THE,0.9660377358490566,"G
POTENTIAL STUDIES"
IN THE CASE THAT THE,0.9679245283018868,"Complexity balance between point-wise (1 × 1) convolution and depth-wise (spatial) convolu-
tion. Depth-wise convolution takes only about 2% computation in the depth-wise convolution-based
architecture. The major computation complexity comes from 1 × 1 convolutions. The solutions to
this issue could be: group 1 × 1 convolution studied in IGC (Zhang et al., 2017; Sun et al., 2018), and"
IN THE CASE THAT THE,0.969811320754717,"7Swin Transformer takes slightly higher FLOPs for 7 × 7 than 12 × 12. The higher computation cost comes
from larger padding than 12 × 12."
IN THE CASE THAT THE,0.9716981132075472,Published as a conference paper at ICLR 2022
IN THE CASE THAT THE,0.9735849056603774,"Table 13: Comparison between local attention and depth-wise convolution in VOLO (Yuan et al.,
2021c) and SVT (Chu et al., 2021a) architecture. Results are reported on ImageNet classiﬁcation
with tiny model."
IN THE CASE THAT THE,0.9754716981132076,"#param.
FLOPs
top-1 acc.
VOLO-d1 (Yuan et al., 2021c)
27M
7.0G
84.1
VOLO (Local SA)-d1
27M
7.2G
84.2
DW Conv.-d1
26M
6.9G
84.2
SVT-S (Chu et al., 2021a)
24M
2.8G
81.7
DW Conv.-S
22M
2.7G
81.9"
IN THE CASE THAT THE,0.9773584905660377,Table 14: Retrain on larger images.
IN THE CASE THAT THE,0.9792452830188679,"model
ws.
#param. FLOPs Acc."
IN THE CASE THAT THE,0.9811320754716981,"Swin
7×7
28M
14.4G
81.8
12×12
28M
14.2G
82.4"
IN THE CASE THAT THE,0.9830188679245283,"DW Conv.
7×7
24M
11.1G
82.2
12×12
25M
11.5G
82.1"
IN THE CASE THAT THE,0.9849056603773585,Table 15: Cooperate with SE.
IN THE CASE THAT THE,0.9867924528301887,"model
SE
#param.
FLOPs
Acc."
IN THE CASE THAT THE,0.9886792452830189,"Swin
28M
4.5G
81.3

29M
4.5G
81.2"
IN THE CASE THAT THE,0.9905660377358491,"DW Conv.
24M
3.8G
81.3

24M
3.8G
81.7"
IN THE CASE THAT THE,0.9924528301886792,"channel-wise weighting (like SENet) studied in Lite-HRNet (Yu et al., 2021) and EfﬁcientNet (Tan &
Le, 2019; 2021), or simply add more depth-wise (spatial) convolutions."
IN THE CASE THAT THE,0.9943396226415094,"Attention weights as channel maps. Attention weights in attention can be regarded as channel maps.
The operations, such as convolution or simple weighting, can be applied to the attention weights. The
resT approach (Zhang & Yang, 2021) performs 1 × 1 convolutions over the attention weight maps."
IN THE CASE THAT THE,0.9962264150943396,"Dynamic weights. In Swin Transformer and our developed dynamic depth-wise convolution net-
works, only the spatial part, attention and depth-wise convolution, explores dynamic weights. Lite-
HRNet instead studies dynamic weight for point-wise (1 × 1) convolution. It is interesting to explore
dynamic weight for both parts."
IN THE CASE THAT THE,0.9981132075471698,"Convolution-style MLP weights. The weights of the spatial-mixing MLP in MLP-Mixer and
ResMLP could be modiﬁed in the convolution-like style with more weights (some like the relative
position embeddings used in local attention, larger than the image window size) so that it could be
extended to larger images and downstream tasks with different image sizes."
