Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0020491803278688526,"Ofﬂine imitation learning (IL) with imperfect data has garnered increasing attention
1"
ABSTRACT,0.004098360655737705,"due to the scarcity of expert data in many real-world domains. A fundamental prob-
2"
ABSTRACT,0.006147540983606557,"lem in this scenario is how to extract good behaviors from noisy demonstrations.
3"
ABSTRACT,0.00819672131147541,"In general, current approaches to the problem build upon state-action similarity to
4"
ABSTRACT,0.010245901639344262,"the expert, neglecting the valuable information in (potentially abundant) diverse be-
5"
ABSTRACT,0.012295081967213115,"haviors that deviate from given expert demonstrations. In this paper, we introduce
6"
ABSTRACT,0.014344262295081968,"a simple yet effective data selection method that identiﬁes the positive behavior
7"
ABSTRACT,0.01639344262295082,"based on its resultant state, which is a more informative criterion that enables ex-
8"
ABSTRACT,0.018442622950819672,"plicit utilization of dynamics information and the extraction of both expert-like and
9"
ABSTRACT,0.020491803278688523,"beneﬁcial diverse behaviors. Further, we devise a lightweight constrained behavior
10"
ABSTRACT,0.022540983606557378,"cloning algorithm capable of leveraging the expert and selected data correctly.
11"
ABSTRACT,0.02459016393442623,"We term our proposed method iLID and evaluate it on a suite of complex and
12"
ABSTRACT,0.02663934426229508,"high-dimensional ofﬂine IL benchmarks, including MuJoCo and Adroit tasks. The
13"
ABSTRACT,0.028688524590163935,"results demonstrate that iLID achieves state-of-the-art performance, signiﬁcantly
14"
ABSTRACT,0.030737704918032786,"outperforming existing methods often by 2-5x while maintaining a comparable
15"
ABSTRACT,0.03278688524590164,"runtime to behavior cloning (BC).
16"
INTRODUCTION,0.03483606557377049,"1
Introduction
17"
INTRODUCTION,0.036885245901639344,"Ofﬂine imitation learning (IL) is the study of learning from demonstrated behaviors without rein-
18"
INTRODUCTION,0.0389344262295082,"forcement signals or further interaction with the environment. It has been deemed as a promising
19"
INTRODUCTION,0.040983606557377046,"solution for safety-sensitive applications, such as autonomous driving and healthcare, where manually
20"
INTRODUCTION,0.0430327868852459,"identifying a reward function is difﬁcult but historical human demonstrations are readily available.
21"
INTRODUCTION,0.045081967213114756,"Traditionally, ofﬂine IL methods such as behavior cloning (BC) (Pomerleau, 1988) often require an
22"
INTRODUCTION,0.0471311475409836,"expert dataset with sufﬁcient coverage over state-action spaces to combat error compounding (Ross
23"
INTRODUCTION,0.04918032786885246,"and Bagnell, 2010; Jarrett et al., 2020; Chan and van der Schaar, 2021), which can be prohibitively
24"
INTRODUCTION,0.05122950819672131,"expensive for many real-world domains. Instead, a more realistic scenario might allow for a small
25"
INTRODUCTION,0.05327868852459016,"expert dataset, combined with a large amount of imperfect data sampled from unknown policies (Wu
26"
INTRODUCTION,0.055327868852459015,"et al., 2019; Xu et al., 2022a; Yu et al., 2022). For example, autonomous vehicle companies may have
27"
INTRODUCTION,0.05737704918032787,"limited high-quality data from experienced drivers but can obtain a wealth of mixed-quality data from
28"
INTRODUCTION,0.05942622950819672,"ordinary drivers. Clearly, effective utilization of these imperfect demonstrations would signiﬁcantly
29"
INTRODUCTION,0.06147540983606557,"enhance the robustness and generalization of ofﬂine IL.
30"
INTRODUCTION,0.06352459016393443,"A fundamental question raised in this scenario is: how can we extract good behaviors from noisy
31"
INTRODUCTION,0.06557377049180328,"data? To answer this question, several prior works have attempted to explore and imitate the imperfect
32"
INTRODUCTION,0.06762295081967214,"behaviors that resemble expert ones (as in Xu et al. (2022a); Sasaki and Yamashina (2020)). However,
33"
INTRODUCTION,0.06967213114754098,"due to the scarcity of expert data, such methods are ill-equipped to leverage valuable knowledge
34"
INTRODUCTION,0.07172131147540983,"in (potentially abundant) diverse behaviors that deviate from limited expert demonstrations. Of
35"
INTRODUCTION,0.07377049180327869,"course, a natural solution to incorporate these behaviors is inferring a reward function and labeling
36"
INTRODUCTION,0.07581967213114754,"Expert behaviors
Imperfect behaviors
Beneficial behaviors"
INTRODUCTION,0.0778688524590164,"A state unobserved
from expert data"
INTRODUCTION,0.07991803278688525,"Leading to an
expert state"
INTRODUCTION,0.08196721311475409,Figure 1: A cartoon illustration of the beneﬁcial behaviors in imperfect data.
INTRODUCTION,0.08401639344262295,"all imperfect data, followed by an ofﬂine reinforcement learning (RL) progress (as in Zolna et al.
37"
INTRODUCTION,0.0860655737704918,"(2020); Chang et al. (2022); Yue et al. (2023)). Unfortunately, it is highly challenging to deﬁne and
38"
INTRODUCTION,0.08811475409836066,"learn meaningful reward functions without environmental interaction. As a result, current ofﬂine
39"
INTRODUCTION,0.09016393442622951,"reward learning methods typically rely on complex adversarial optimization using a learned dynamics
40"
INTRODUCTION,0.09221311475409837,"model. They easily suffer from hyperparameter sensitivity, learning instability, and limited scalability
41"
INTRODUCTION,0.0942622950819672,"in high-dimensional environments (Yu et al., 2022; Arjovsky et al., 2017; Garg et al., 2021).
42"
INTRODUCTION,0.09631147540983606,"In this paper, we introduce a simpler data selection method along with a lightweight policy learning
43"
INTRODUCTION,0.09836065573770492,"algorithm to fully exploit both expert-like and positive diverse behaviors in imperfect demonstrations
44"
INTRODUCTION,0.10040983606557377,"without indirect reward learning procedures. Speciﬁcally, instead of examining a behavior’s similarity
45"
INTRODUCTION,0.10245901639344263,"to expert demonstrations in and of itself, we assess its value based on whether its resultant state, to
46"
INTRODUCTION,0.10450819672131148,"which environment transitions after performing that behavior, falls within the expert data manifold.
47"
INTRODUCTION,0.10655737704918032,"In other words, we (properly) select the state-actions that can lead to expert states, even if they bear
48"
INTRODUCTION,0.10860655737704918,"no similarity to expert demonstrations. As illustrated in Fig. 1 and supported by the theoretical results
49"
INTRODUCTION,0.11065573770491803,"in Section 3.1, the underlying rationale is: when the agent encounters a state unobserved in expert
50"
INTRODUCTION,0.11270491803278689,"demonstrations, compared to taking a random action, a more reasonable way is to return to the states
51"
INTRODUCTION,0.11475409836065574,"where it knows expert behaviors; otherwise, it may keep making mistakes and remain out-of-expert-
52"
INTRODUCTION,0.1168032786885246,"distribution for the remainder of time steps. Notably, the resultant state is more informative than
53"
INTRODUCTION,0.11885245901639344,"the state-action similarity, as it can explicitly utilize the dynamics information and identify both
54"
INTRODUCTION,0.12090163934426229,"expert-like and beneﬁcial diverse state-actions.
55"
INTRODUCTION,0.12295081967213115,"Drawing on this insight, we ﬁrst train a state-only discriminator to distinguish expert and non-expert
56"
INTRODUCTION,0.125,"states in imperfect demonstrations. Based on the identiﬁed expert-like states, we appropriately select
57"
INTRODUCTION,0.12704918032786885,"their causal state-actions and build a complementary training dataset. In light of the suboptimality of
58"
INTRODUCTION,0.1290983606557377,"the complementary data, we further devise a lightweight constrained BC algorithm to mitigate the
59"
INTRODUCTION,0.13114754098360656,"potential interference among behaviors. We term our proposed method Ofﬂine Imitation Learning
60"
INTRODUCTION,0.13319672131147542,"with Imperfect Demonstrations (iLID) and evaluate it on a suite of ofﬂine IL benchmarks, including
61"
INTRODUCTION,0.13524590163934427,"widely-used MuJoCo tasks as well as more complex and high-dimensional Adroit tasks. iLID
62"
INTRODUCTION,0.13729508196721313,"achieves state-of-the-art performance, signiﬁcantly outperforming existing baseline methods often
63"
INTRODUCTION,0.13934426229508196,"by 2-5x while maintaining a comparable runtime to BC. In a nutshell, the main contributions of this
64"
INTRODUCTION,0.1413934426229508,"paper are as follows:
65"
INTRODUCTION,0.14344262295081966,"• We introduce a simple yet effective method to select potentially useful behaviors in noisy data. It
66"
INTRODUCTION,0.14549180327868852,"can explicitly exploit the dynamics information and extract both expert-like and positive diverse
67"
INTRODUCTION,0.14754098360655737,"behaviors, achieving a signiﬁcant improvement in the utilization of imperfect demonstrations.
68"
INTRODUCTION,0.14959016393442623,"• To avoid behavior interference induced by the suboptimality of complementary behaviors, we
69"
INTRODUCTION,0.15163934426229508,"propose a constrained BC algorithm that can correctly leverage the expert and extracted behaviors.
70"
INTRODUCTION,0.15368852459016394,"• Extensive experiments on complex and high-dimensional domains corroborate that iLID can surpass
71"
INTRODUCTION,0.1557377049180328,"the existing baseline methods in terms of performance and computational cost.
72"
PRELIMINARIES,0.15778688524590165,"2
Preliminaries
73"
PRELIMINARIES,0.1598360655737705,"Episodic Markov decision process (MDP) can be speciﬁed by M .= hS, A, T, H, r, µi, consisting of
74"
PRELIMINARIES,0.16188524590163936,"state space S, action space A, transition dynamics T : S ⇥A ! P(S), episode horizon H, reward
75"
PRELIMINARIES,0.16393442622950818,"function r : S ⇥A ! [0, 1], and initial state distribution µ : S ! [0, 1]. A stationary stochastic
76"
PRELIMINARIES,0.16598360655737704,"policy maps states to distributions over actions, denoted as ⇡: S ! P(A). The policy value of ⇡
77"
PRELIMINARIES,0.1680327868852459,"is deﬁned as the expected cumulative reward, V ⇡.= E[PH"
PRELIMINARIES,0.17008196721311475,"h=1 r(sh, ah)], where the expectation is
78"
PRELIMINARIES,0.1721311475409836,"computed w.r.t. the distribution over trajectories induced by rolling out ⇡in the environment. The
79"
PRELIMINARIES,0.17418032786885246,"objective of reinforcement learning (RL) can be expressed as max⇡2⇧V ⇡, where ⇧is the set of
80"
PRELIMINARIES,0.1762295081967213,"all stationary stochastic policies taking actions in A given states in S. We denote the average state
81"
PRELIMINARIES,0.17827868852459017,distribution of policy ⇡as ⇢⇡(s) .= 1 H PH
PRELIMINARIES,0.18032786885245902,"h=1 Pr(sh = s|⇡, T, µ), where Pr(sh = s|⇡, T, µ) denotes
82"
PRELIMINARIES,0.18237704918032788,"the probability of visiting s at time step h by rolling out ⇡with M. When clear from context, we
83"
PRELIMINARIES,0.18442622950819673,"overload notation and denote the average state-action distribution as ⇢⇡(s, a) .= ⇢⇡(s)⇡(a|s).
84"
PRELIMINARIES,0.1864754098360656,"Ofﬂine IL with imperfect demonstrations is the setting where the algorithm is neither allowed to
85"
PRELIMINARIES,0.1885245901639344,"interact with the environment nor provided ground-truth reward signals. Rather, it has access to an
86"
PRELIMINARIES,0.19057377049180327,"expert dataset and a mix-quality imperfect/noisy dataset, collected from unknown expert policy ⇡e and
87"
PRELIMINARIES,0.19262295081967212,"(perhaps highly suboptimal) behavior policy ⇡s, respectively. To be speciﬁc, the expert and imperfect
88"
PRELIMINARIES,0.19467213114754098,datasets are denoted by De .= {⌧j}ne
PRELIMINARIES,0.19672131147540983,j=1 and Ds .= {⌧i}ns
PRELIMINARIES,0.1987704918032787,"i=1, where ⌧i .= (si,1, ai,1, . . . , si,H, ai,H)
89"
PRELIMINARIES,0.20081967213114754,"represents a trajectory. Our goal is to learn the best policy with regard to optimizing V ⇡from static
90"
PRELIMINARIES,0.2028688524590164,"ofﬂine data Do .= De [ Ds without querying the expert or interacting with the environment.
91"
PRELIMINARIES,0.20491803278688525,"Behavior cloning (BC) is a classical ofﬂine IL approach, which seeks to learn a policy via supervised
92"
PRELIMINARIES,0.2069672131147541,"learning. The standard objective of BC is to maximize the negative log-likelihood over De:
93 max"
PRELIMINARIES,0.20901639344262296,"⇡2⇧E(s,a)⇠De ⇥"
PRELIMINARIES,0.21106557377049182,"log(⇡(a|s)) ⇤ .
(1)"
PRELIMINARIES,0.21311475409836064,"However, standard BC does not utilize the information in Ds. Due to the limited state coverage of
94"
PRELIMINARIES,0.2151639344262295,"De, the learned policy may suffer from severe compounding errors, i.e., the inability for the policy to
95"
PRELIMINARIES,0.21721311475409835,"get back on track if it encounters a state not seen in the expert demonstrations.
96"
PRELIMINARIES,0.2192622950819672,"3
Ofﬂine imitation learning with imperfect demonstrations
97"
PRELIMINARIES,0.22131147540983606,"In this section, we provide a detailed description of our approach. We begin by presenting the
98"
PRELIMINARIES,0.22336065573770492,"theoretical ﬁndings on the beneﬁts of utilizing diverse transitions. Building on the theoretical insights,
99"
PRELIMINARIES,0.22540983606557377,"we then design our data selection and policy learning methods.
100"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.22745901639344263,"3.1
How to extract good behaviors from noisy data
101"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.22950819672131148,"To discard low-quality demonstrations from Ds, existing approaches often rely on the state-action
102"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.23155737704918034,"dissimilarity between Ds and De. For example, Xu et al. (2022a); Zolna et al. (2020); Kim et al.
103"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.2336065573770492,"(2022) propose to learn a weighting function f(s, a) by pushing up its value on (s, a) 2 De while
104"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.23565573770491804,"pushing down that on (s, a) 2 Ds. Based on f(s, a), they perform weighed BC to implicitly
105"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.23770491803278687,"select expert-like state-actions, i.e., max⇡2⇧E(s,a)⇠Do[f(s, a) log(⇡(a|s))]. However, due to the
106"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.23975409836065573,"limitation of expert demonstrations, the learned f(s, a) can be overly conservative and neglect the
107"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.24180327868852458,"useful information in diverse state-actions. Therefore, it calls for a more informative criterion to
108"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.24385245901639344,"assess the value of imperfect behaviors.
109"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.2459016393442623,"Before preceding, we ﬁrst provide the following theoretical results under deterministic transition
110"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.24795081967213115,"dynamics to gain insights into this problem. Denote Sh(D) as the set of h-step visited states of D
111"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.25,and S(D) .= SH
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.2520491803278688,"h=1 Sh(D) all the states thereof. Assume that ⇡e is optimal and deterministic, and
112"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.2540983606557377,"there exists a supplementary dataset consisting of transition tuples from initial states to given expert
113"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.25614754098360654,"states, i.e., ˜D .= {(si, ai, s0"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.2581967213114754,"i) | si ⇠µ, s0"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.26024590163934425,"i ⇠S(De), T(si, ai) = s0"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.26229508196721313,"i, i = 1, 2, . . . , ˜n}. Consider a
114"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.26434426229508196,"policy ˜⇡such that in expert states S(De), it takes the corresponding expert actions, and in states
115"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.26639344262295084,"S1( ˜D)\S1(De), it takes the actions in ˜D, that is,
116"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.26844262295081966,˜⇡(a|s) .=
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.27049180327868855,"8
>
>
>
<"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.2725409836065574,">
>
>
: P"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.27459016393442626,"(˜s,˜a)2De
((˜s,˜a)=(s,a))
P"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.2766393442622951,"˜s2S(De)
(˜s=s)
,
if s 2 S(De);
P"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.2786885245901639,"(˜s,˜a)2 ˜
D
((˜s,˜a)=(s,a))
P"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.2807377049180328,"˜s2S( ˜
D)
(˜s=s)
,
if s 2 S1( ˜D)\S1(De);"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.2827868852459016,"1
|A|,
else. (2)"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.2848360655737705,"We bound the suboptimality gap and sample complexity of ˜⇡in the next theorem and corollary.
117"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.28688524590163933,"Theorem 3.1. For any ﬁnite and episodic MDP with deterministic transition dynamics and µ = U(S),
118"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.2889344262295082,"the following fact holds:
119"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.29098360655737704,V ⇡e −E ⇥ V ˜⇡⇤ 
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.2930327868852459,✓1 + δ
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.29508196721311475,"2
+ 1 −δ H2 ◆"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.29713114754098363,"H✏,
(3)"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.29918032786885246,"where ✏.= E[Es1⇠µ[ (s1 /2 S1(De))]] and δ .= E[Es1⇠µ[ (s1 /2 S1( ˜D))]] represent the missing
120"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3012295081967213,"mass over the initial distribution w.r.t. S1(De) and S1( ˜D)). U(S) is the uniform distribution over S.
121"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.30327868852459017,"Proof Sketch. Note that the error stems from the initial states that are not covered by S1(De). We
122"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.305327868852459,"bound the errors generated from the states not in S(De) [ S( ˜D) and from the states in S( ˜D)\S(De)
123"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3073770491803279,"by Hδ✏and (H/2 + 1/H)(1 −δ)✏, respectively. Combining these two errors yields the result. For a
124"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3094262295081967,"detailed proof, please refer to Appendix B.
125"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3114754098360656,"Building on Theorem 3.1, we can obtain the following sample complexity result (where we retain the
126"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3135245901639344,constant 1
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3155737704918033,"2 in the asymptotic result to highlight the improvement over BC).
127"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3176229508196721,"Corollary 3.2. Suppose ˜D is sufﬁciently large. For any ﬁnite and episodic MDP with deterministic
128"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.319672131147541,"transition dynamics and µ = U(S), to obtain an ""-optimal policy, V ⇡e −E[V ˜⇡] "", ˜⇡requires at
129"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.32172131147540983,"most O(|S|H/(2 · "")) expert trajectories.
130"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3237704918032787,"Proof. Invoking Xu et al. (2021, Theorem 2) yields the bounds for the missing mass:
131 E  Es1⇠µ h"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.32581967213114754,(s1 /2 S1( ˜D)) i. |S|
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.32786885245901637,"e| ˜D| ,
E h Es1⇠µ ⇥"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.32991803278688525,(s1 /2 S1(De) ⇤i
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3319672131147541,"
|S|
e|De|,"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.33401639344262296,"where e is the Euler’s number. If ˜D is sufﬁciently large, then δ ! 0. Using Theorem 3.1, the result
132"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3360655737704918,"can be easily derived.
133"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.33811475409836067,"Remarks. It is worth noting that the minimax suboptimality of BC is limited to H✏in this setting
134"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3401639344262295,"(Rajaraman et al., 2020), and beating the O(H) barrier is unattainable. The reason is that when the
135"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3422131147540984,"agent encounters a state beyond given demonstrations during the interaction with the environment,
136"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3442622950819672,"it has no prior knowledge about the expert. As a result, the agent is essentially forced to take an
137"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3463114754098361,"arbitrary action in these states, potentially leading to mistakes for H time steps. Whereas, as revealed
138"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3483606557377049,"by Theorem 3.1 and Corollary 3.2, ˜⇡provably alleviates the error compounding and reduces the
139"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.35040983606557374,"sample complexity bound of BC (which is O(|S|H/"")) by approximately half. The reason behind is
140"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3524590163934426,"that ˜D can empower ˜⇡to recover from mistakes. Combined with Eq. (2), this provides an important
141"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.35450819672131145,"insight for us: in the states uncovered by De, if an action can lead to known expert states, mimicking
142"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.35655737704918034,"it can beneﬁt the performance of imitation policy.
143"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.35860655737704916,"Thus motivated, we propose to assess the imperfect behavior based on its resultant states rather
144"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.36065573770491804,"than the state-action in and of itself. For example, if there exists (s1, a1, s2, a2, s3) 2 Ds such that
145"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.36270491803278687,"s0 2 De, one can select (s1, a1) and (s2, a2) (or only (s2, a2)), even if these behaviors do not bear
146"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.36475409836065575,"similarity to any (s, a) 2 De. To this end, we consider learning a state-only discriminator to contrast
147"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3668032786885246,"expert and non-expert states in Ds, e.g.,
148 max"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.36885245901639346,"d
Es⇠De ⇥"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3709016393442623,log d(s) ⇤
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3729508196721312,+ Es⇠Ds ⇥
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.375,"log(1 −d(s)) ⇤ .
(4)"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3770491803278688,"However, optimizing Problem (4) can lead to the problem of false negative, where the learned discrim-
149"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3790983606557377,"inator assigns 1 to all transitions from De and 0 to all transitions from Ds. This problem is analogous
150"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.38114754098360654,"to the positive-unlabeled (PU) classiﬁcation problem (Elkan and Noto, 2008), where both positive
151"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3831967213114754,"(expert) and negative (imperfect) samples exist in the unlabeled data (imperfect demonstrations).
152"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.38524590163934425,"Akin to Xu et al. (2022a); Zolna et al. (2020), we adopt the reweighting method from PU learning to
153"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.38729508196721313,"address this issue:
154"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.38934426229508196,d⇤= arg max d
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.39139344262295084,⌘· Es⇠De ⇥
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.39344262295081966,log d(s) ⇤
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.39549180327868855,+ Es⇠Ds ⇥
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.3975409836065574,log(1 −d(s)) ⇤
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.39959016393442626,−⌘· Es⇠De ⇥
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4016393442622951,"log(1 −d(s)) ⇤ ,
(5)"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4036885245901639,"where ⌘> 0 is a reweighting parameter, corresponding to the proportion of expert states to imperfect
155"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4057377049180328,"states. Intuitively, the third term in Eq. (5) could avoid d⇤(s) of the states from S(Ds) but similar to
156"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4077868852459016,"S(De) becoming 0.
157"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4098360655737705,"An imperfect traj.
· · ·
· · ·"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.41188524590163933,Selected causal state-actions
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4139344262295082,Rollback steps: 2
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.41598360655737704,"sh−3
ah−3
sh
sh−2
ah−2
sh−1
ah−1
ah
An imperfect traj."
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4180327868852459,Figure 2: An illustration of the data selection procedure. sh represents an identiﬁed expert state.
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.42008196721311475,"Data selection. The learned discriminator d⇤is able to identify the expert states in Ds. Based on
158"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.42213114754098363,"these states, we in turn select their causal states and actions to construct a complementary dataset
159"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.42418032786885246,"˜D. Speciﬁcally, given threshold σ 2 [0, 1] and rollback steps K ≥1, if there exist h > 1 and
160"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4262295081967213,"i 2 {1, . . . , ns} (where ns represents the number of trajectories in Ds) such that d(si,h) ≥σ, we
161"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.42827868852459017,"include K causal state-action pairs from si,h into ˜D.:
162"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.430327868852459,˜D  ˜D [ /
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4323770491803279,"(k, si,h−k, ai,h−k) "
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4344262295081967,"k=1:min{h−1,K} .
(6)"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4364754098360656,"We iterate the above process for all identiﬁed expert-like states. To clarify, we illustrate the process in
163"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4385245901639344,"Fig. 2. It is evident that ˜D comprises of both the positive diverse state-actions in Ds and those similar
164"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4405737704918033,"to De therein. This highlights that using resultant states is a more informative way to extract useful
165"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4426229508196721,"behaviors.
166"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.444672131147541,"Behavior interference. After obtaining ˜D, a natural solution to learn an imitation policy is carrying
167"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.44672131147540983,"out BC from the union of De and ˜D. However, due to the suboptimality of ˜D, this naïve solution
168"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4487704918032787,"will suffer from potential interference among behaviors. That is, for a selected (s, a, s0), if s, s0 2 De
169"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.45081967213114754,"but a 6= ⇡e(s), action a will affect mimicking the expert behavior in expert state s when learning via
170"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.45286885245901637,"the naïve solution. Furthermore, this interference issue also exists in the states of complementary
171"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.45491803278688525,"dataset ˜D, but in a more subtle manner. Consider a state s 2 ˜D where two actions a1, a2 are selected,
172"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4569672131147541,"i.e., (k1, s, a1), (k2, s, a2) 2 ˜D. Owing to the stochasticity of MDPs, if k1 < k2, one may prefer a1
173"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.45901639344262296,"over a2, whereas the naïve solution will imitate both actions equally. In Section 3.2, we address this
174"
HOW TO EXTRACT GOOD BEHAVIORS FROM NOISY DATA,0.4610655737704918,"problem and propose a lightweight algorithm to correctly learn from De and ˜D.
175"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.46311475409836067,"3.2
How to learn an imitation policy from expert and extracted data
176"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.4651639344262295,"Due to the suboptimality of Ds and the stochasticity of MDPs, direct cloning the behaviors in ˜D [ De
177"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.4672131147540984,"can lead to the interference issue. In fact, the solution has been implied in Eq. (2), which suggests
178"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.4692622950819672,"that the policy should be constrained to De(·|s) in the known expert states. Accordingly, we cast the
179"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.4713114754098361,"problem of learning policy from De and ˜D as follows:
180"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.4733606557377049,"min
⇡2⇧E(k,s,a)⇠˜
D h"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.47540983606557374,−γk log ⇡(a|s) i
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.4774590163934426,s.t. Es⇠De ⇥
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.47950819672131145,DKL(˜⇡e(·|s)k⇡(·|s)) ⇤
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.48155737704918034,"< ✏
(7)"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.48360655737704916,"where ˜⇡e = arg max⇡2⇧E(s,a)⇠De[log(⇡(a|s))] is the BC policy learned on De, and ✏≥0 is the
181"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.48565573770491804,"threshold. In Eq. (7), we use discount factor γ 2 (0, 1] to mitigate the impact of stochasticity of MDPs.
182"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.48770491803278687,"It is easy to see that with a sufﬁciently small ✏, the optimal solution of Problem (7) enjoys at least
183"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.48975409836065575,"the same theoretical guarantee of BC in general stochastic MDPs, i.e., suboptimality upper-bound
184"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.4918032786885246,"O(|S|H2 log ne/ne) compared to V ⇡e (Rajaraman et al., 2020).
185"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.49385245901639346,"Problem (7) is a convex optimization problem. From Slater’s condition, the strong duality holds, and
186"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.4959016393442623,"thus the optimization is equal to
187 max"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.4979508196721312,↵>0 min
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5,"⇡2⇧−Ek,s,a⇠˜
D ⇥"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5020491803278688,γk log ⇡(a|s) ⇤ −↵ ⇣
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5040983606557377,"Es,a⇠De ⇥"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5061475409836066,log ⇡(a|s) ⇤
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5081967213114754,"+ ˜H + ✏ ⌘ ,
(8)"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5102459016393442,"where ↵is the dual variable, and ˜H is the expected entropy of the empirical expert policy, which is
188"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5122950819672131,"derived from:
189 Es⇠De ⇥"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.514344262295082,DKL(˜⇡e(·|s)k⇡(·|s)) ⇤
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5163934426229508,= Es⇠De h
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5184426229508197,Ea⇠˜⇡e(·|s) ⇥
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5204918032786885,log ˜⇡e(a|s) ⇤
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5225409836065574,−Ea⇠˜⇡e(·|s) ⇥
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5245901639344263,log ⇡(a|s) ⇤i
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5266393442622951,"= E(s,a)⇠De ⇥"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5286885245901639,log ˜⇡e(a|s) ⇤
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5307377049180327,"|
{z
}
.=−˜
H"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5327868852459017,"−E(s,a)⇠De ⇥"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5348360655737705,"log ⇡(a|s) ⇤ .
(9)"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5368852459016393,"Algorithm 1: Ofﬂine Imitation Learning with Imperfect Demonstrations (iLID)
Require: expert data De, imperfect data Ds, learning rate λ, parameter K, ✏"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5389344262295082,1 Train discriminator d using De and Ds based on Eq. (5);
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5409836065573771,2 Select data from Ds and build complementary dataset ˜D based on Eq. (6);
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5430327868852459,"3 Train BC policy ˜⇡e only using De and compute expected entropy ˜H .= −Es,a⇠De[log ˜⇡e(a|s)];"
HOW TO LEARN AN IMITATION POLICY FROM EXPERT AND EXTRACTED DATA,0.5450819672131147,4 Initialize policy ⇡✓and dual variable ↵;
WHILE NOT DONE DO,0.5471311475409836,5 while not done do
WHILE NOT DONE DO,0.5491803278688525,"6
Sample a training batch from De and ˜D;"
WHILE NOT DONE DO,0.5512295081967213,"7
Update policy parameter ✓ ✓−λ ˜rL(✓) based on Eq. (10);"
WHILE NOT DONE DO,0.5532786885245902,"8
Update dual variable ↵ ↵−λ ˜rL(↵) based on Eq. (11);"
END WHILE,0.555327868852459,9 end while
END WHILE,0.5573770491803278,"Parameterize the learned policy by ✓and denote the loss functions for ✓and ↵as follows:
190"
END WHILE,0.5594262295081968,"L(✓) .= −Ek,s,a⇠˜
D ⇥"
END WHILE,0.5614754098360656,γk log ⇡✓(a|s) ⇤
END WHILE,0.5635245901639344,"−↵Es,a⇠De ⇥"
END WHILE,0.5655737704918032,log ⇡✓(a|s) ⇤
END WHILE,0.5676229508196722,",
(10)"
END WHILE,0.569672131147541,"L(↵) .= Es,a⇠De ⇥"
END WHILE,0.5717213114754098,log ⇡✓(a|s) ⇤
END WHILE,0.5737704918032787,"+ ˜H + ✏.
(11)"
END WHILE,0.5758196721311475,"Problem (8) can be optimized by approximating dual gradient descent that alternates between the
191"
END WHILE,0.5778688524590164,"gradient steps w.r.t. L(✓) and L(↵), which has been shown to converge under convexity assumptions
192"
END WHILE,0.5799180327868853,"(Boyd and Vandenberghe, 2004) and work very well in the case of nonlinear function approximators
193"
END WHILE,0.5819672131147541,"such as neural networks (Haarnoja et al., 2018).
194"
END WHILE,0.5840163934426229,"Our algorithm, named Ofﬂine Imitation Learning with Imperfect Demonstrations (iLID), is outlined in
195"
END WHILE,0.5860655737704918,"Algorithm 1. Notably, while iLID pretrains a discriminator and a BC policy (using De), the progress
196"
END WHILE,0.5881147540983607,"can converge within a small number of gradient steps, especially when De is limited. In light of the
197"
END WHILE,0.5901639344262295,"negligible cost in updating ↵, iLID is indeed computationally cheap.
198"
EXPERIMENTS,0.5922131147540983,"4
Experiments
199"
EXPERIMENTS,0.5942622950819673,"In this section, we use experimental studies to test the proposed method and answer the following
200"
EXPERIMENTS,0.5963114754098361,"questions: 1) Can iLID effectively utilize imperfect demonstrations? 2) What is the convergence
201"
EXPERIMENTS,0.5983606557377049,"properity of iLID? 3) How does iLID perform given different numbers of expert demonstrations or
202"
EXPERIMENTS,0.6004098360655737,"different qualities of imperfect demonstrations? 4) What is the impact of the rollback steps? 5) What
203"
EXPERIMENTS,0.6024590163934426,"is the runtime of iLID? 6) Is the constrained BC an overkill?
204"
EXPERIMENTS,0.6045081967213115,"Baselines. We evaluate our method against ﬁve strong baseline methods in the ofﬂine IL setting:
205"
EXPERIMENTS,0.6065573770491803,"1) Behavior Cloning with Expert Data (BCE), the standard BC trained only on the expert dataset
206"
EXPERIMENTS,0.6086065573770492,"(Pomerleau, 1988); 2) Behavior Cloning with Union Data (BCU), BC on both the expert and diverse
207"
EXPERIMENTS,0.610655737704918,"datasets; 3) Discriminator-Weighted Behavioral Cloning (DWBC) (Chang et al., 2022), a recent ofﬂine
208"
EXPERIMENTS,0.6127049180327869,"IL algorithm capable of leveraging suboptimal demonstrations; 4) Using Imperfect Demonstration via
209"
EXPERIMENTS,0.6147540983606558,"Stationary Distribution Correction Estimation (DemoDICE) (Kim et al., 2022), another recent ofﬂine
210"
EXPERIMENTS,0.6168032786885246,"IL algorithm that can leverage suboptimal demonstrations; 5) Conservative offLine model-bAsed
211"
EXPERIMENTS,0.6188524590163934,"Reward lEarning (CLARE) (Yue et al., 2023), a recent model-based ofﬂine inverse RL algorithm
212"
EXPERIMENTS,0.6209016393442623,"trained from both expert and imperfect datasets.
213"
EXPERIMENTS,0.6229508196721312,"Datasets. We conduct experiments on both widely-used MuJoCo tasks (includ-
214"
EXPERIMENTS,0.625,"ing HalfCheetah, Walker2d, Hopper, and Ant) and more complex and high-
215"
EXPERIMENTS,0.6270491803278688,"dimensional Adroit tasks (including Pen, Hammer, Relocate, and Door, shown
216"
EXPERIMENTS,0.6290983606557377,"on the right). We use the D4RL datasets (Fu et al., 2020) and utilize the random
217"
EXPERIMENTS,0.6311475409836066,"and expert data for each MuJoCo task, and cloned and expert data for Adroit
218"
EXPERIMENTS,0.6331967213114754,"tasks.1 Similar to Xu et al. (2022a); Kim et al. (2022), we generate De and Ds as
219"
EXPERIMENTS,0.6352459016393442,"follows:
220"
EXPERIMENTS,0.6372950819672131,1Experimental details is elaborated in Appendix A.
EXPERIMENTS,0.639344262295082,"Table 1: Performance of different algorithms. The numbers of expert trajectories are 1 for MuJoCo
tasks and 10 for Adroit. The results correspond to the mean and standard deviation of normalized
scores over 5 random seeds. low and high represent the qualities of imperfect data."
EXPERIMENTS,0.6413934426229508,"Task
Data quality
BCE
BCU
DWBC
ClARE
DemoDICE
iLID (ours)"
EXPERIMENTS,0.6434426229508197,"Ant
low
-11.1 ± 9.7
31.4 ± 0.1
30.6 ± 9.7
29.7 ± 6.4
74.3 ± 11.0
79.8 ± 11.8
high
-11.1 ± 9.7
32.4 ± 7.1
34.6 ± 8.7
22.4 ± 4.7
88.1 ± 8.9
88.2 ± 7.9"
EXPERIMENTS,0.6454918032786885,"HalfCheetah
low
0.2 ± 0.9
2.2 ± 0.0
1.1 ± 1.1
1.1 ± 0.9
2.2 ± 0.0
25.4 ± 4.1
high
0.2 ± 0.9
2.3 ± 0.0
0.8 ± 1.2
2.2 ± 0.9
5.9 ± 2.8
29.3 ± 6.3"
EXPERIMENTS,0.6475409836065574,"Hopper
low
17.0 ± 4.2
7.6 ± 5.7
76.0 ± 9.4
8.9 ± 5.2
58.3 ± 13.8
95.0 ± 10.9
high
17.0 ± 4.2
3.7 ± 1.6
60.6 ± 18.6
3.5 ± 0.5
72.2 ± 13.6
104.8 ± 7.1"
EXPERIMENTS,0.6495901639344263,"Walker2d
low
8.0 ± 5.7
0.3 ± 0.1
61.1 ± 13.9
1.9 ± 0.8
96.7 ± 7.5
97.0 ± 8.0
high
8.0 ± 5.7
0.3 ± 0.0
49.9 ± 26.5
1.4 ± 0.5
102.6 ± 6.3
97.0 ± 10.3"
EXPERIMENTS,0.6516393442622951,"Hammer
low
6.8 ± 5.6
0.2 ± 0.0
11.0 ± 8.8
7.2 ± 8.3
10.1 ± 12.3
66.0 ± 17.8
high
6.8 ± 5.6
0.2 ± 0.0
13.2 ± 7.1
3.9 ± 4.4
9.1 ± 12.5
109.4 ± 10.0"
EXPERIMENTS,0.6536885245901639,"Pen
low
-0.1 ± 0.0
2.1 ± 6.9
43.7 ± 14.2
7.5 ± 5.9
41.3 ± 13.9
90.2 ± 19.4
high
-0.1 ± 0.0
1.6 ± 3.4
57.1 ± 13.6
6.4 ± 6.6
48.6 ± 25.3
65.7 ± 7.5"
EXPERIMENTS,0.6557377049180327,"Relocate
low
-0.1 ± 0.0
-0.1 ± 0.0
-0.1 ± 0.0
0.0 ± 0.0
12.0 ± 5.6
29.1 ± 5.6
high
-0.1 ± 0.0
0.0 ± 0.0
-0.1 ± 0.1
0.0 ± 0.0
26.0 ± 10.6
41.5 ± 12.1"
EXPERIMENTS,0.6577868852459017,"Door
low
1.0 ± 1.2
-0.1 ± 0.0
0.5 ± 1.0
-0.1 ± 0.0
-0.1 ± 0.1
0.3 ± 0.4
high
1.0 ± 1.2
-0.1 ± 0.1
0.3 ± 0.7
-0.2 ± 0.1
-0.1 ± 0.0
0.4 ± 0.4"
EXPERIMENTS,0.6598360655737705,"• Expert datasets: For MuJoCo, we sample 1 trajectory (including less than 1000 state-action pairs)
221"
EXPERIMENTS,0.6618852459016393,"from the expert D4RL dataset to constitute expert datasets. For Adroit, we sample 10 expert
222"
EXPERIMENTS,0.6639344262295082,"trajectories (each includes less than 100 state-action pairs) to form the datasets.
223"
EXPERIMENTS,0.6659836065573771,"• Imperfect datasets: For MuJoCo tasks, we sample 1000 random trajectories mixed with 10 and 20
224"
EXPERIMENTS,0.6680327868852459,"expert trajectories to constitute the low-quality and high-quality imperfect datasets. Regarding
225"
EXPERIMENTS,0.6700819672131147,"Adroit tasks, we sample 1000 cloned trajectories mixed with 100 and 200 expert trajectories to
226"
EXPERIMENTS,0.6721311475409836,"constitute the low-quality and high-quality datasets.
227"
EXPERIMENTS,0.6741803278688525,"Comparative results. To answer the ﬁrst and second questions, we show the comparative results
228"
EXPERIMENTS,0.6762295081967213,"under both low-quality and high-quality imperfect data in Section 4 and their corresponding learning
229"
EXPERIMENTS,0.6782786885245902,"curves in Fig. 4. iLID outperforms baseline algorithms on most of the tasks (13 out of 16) often by
230"
EXPERIMENTS,0.680327868852459,"a wide margin and reaches near-expert scores on many tasks. It indicates that iLID can effectively
231"
EXPERIMENTS,0.6823770491803278,"extract and leverage positive behaviors from imperfect demonstrations over the approaches based on
232"
EXPERIMENTS,0.6844262295081968,"state-action similarity such as DWBC and DemoDICE. Unsurprisingly, BCE fails to fulﬁll most of
233"
EXPERIMENTS,0.6864754098360656,"the tasks, while BCU learns a mediocre policy. CLARE also performs poorly because the learned
234"
EXPERIMENTS,0.6885245901639344,"reward function could become too pessimistic due to the scarcity of expert demonstrations. Clearly,
235"
EXPERIMENTS,0.6905737704918032,"the model-based approach struggle in high-dimensional environments.
236"
EXPERIMENTS,0.6926229508196722,"Expert demonstrations. To answer the third question, we vary the numbers of expert trajectories
237"
EXPERIMENTS,0.694672131147541,"from 1 to 50 and present the results on Fig. 3(a). iLID reaches the expert with sufﬁcient expert data.
238"
EXPERIMENTS,0.6967213114754098,"(a) Expert demonstrations.
(b) Rollback steps.
(c) Ablation Study."
EXPERIMENTS,0.6987704918032787,"Figure 3: Performance of iLID under varying numbers of expert demonstrations and rollback steps
along with the ablation study for the constrained BC procedure."
EXPERIMENTS,0.7008196721311475,"Figure 4: Convergence properties of different algorithms. The solid curve corresponds to the mean
and the shaded region to the standard derivative across ﬁve random seeds."
EXPERIMENTS,0.7028688524590164,"Albeit with very limited expert trajectories, iLID also achieves strong performance, revealing its
239"
EXPERIMENTS,0.7049180327868853,"advantages in extracting good behaviors. DemoDICE performs relatively poorly with larger ne. The
240"
EXPERIMENTS,0.7069672131147541,"reason is that it learns on both expert and random data, whereas the random data of HalfCheetah is
241"
EXPERIMENTS,0.7090163934426229,"highly suboptimal.
242"
EXPERIMENTS,0.7110655737704918,"Rollback steps. To answer the fourth question, we vary the rollback steps from 1 to 20 and show the
243"
EXPERIMENTS,0.7131147540983607,"corresponding results in Fig. 3(b). With larger K, the performance increases at the beginning. This
244"
EXPERIMENTS,0.7151639344262295,"is due to more positive diverse data included. An excessively large K may have a negative impact
245"
EXPERIMENTS,0.7172131147540983,"due to the dynamics stochasticity and behavior interference. However, it is worth noting that, as
246"
EXPERIMENTS,0.7192622950819673,"K increases further, the performance does not signiﬁcantly deteriorate. This is because we apply a
247"
EXPERIMENTS,0.7213114754098361,"discount factor to penalize the potential uncertainty in the resulting states, capable of mitigating the
248"
EXPERIMENTS,0.7233606557377049,"issue. In practice, K can be treated as a hyper-parameter to tune. Intuitively, it can be set relatively
249"
EXPERIMENTS,0.7254098360655737,"smaller in a more stochastic environment.
250"
EXPERIMENTS,0.7274590163934426,"Ablation study. We compare iLID to the naïve solution mentioned in Section 3.1, i.e., directly
251"
EXPERIMENTS,0.7295081967213115,"imitating the union of expert and select data. Fig. 3(c) validates the necessity of the constrained BC
252"
EXPERIMENTS,0.7315573770491803,"procedure. The result of direct imitation is passable as we select a number of positive data. However,
253"
EXPERIMENTS,0.7336065573770492,"it fails to deal with the behavior interference issue caused by the suboptimality of imperfect data.
254"
EXPERIMENTS,0.735655737704918,"Runtime. We evaluate the runtime of iLID compared with baseline
255"
EXPERIMENTS,0.7377049180327869,"algorithms for 250,000 training steps, utilizing the same network
256"
EXPERIMENTS,0.7397540983606558,"size and batch size. We reproduce the reported results in Xu et al.
257"
EXPERIMENTS,0.7418032786885246,"(2022a) on an NVIDIA V100 GPU. As illustrated by the ﬁgure
258"
EXPERIMENTS,0.7438524590163934,"on the right, the runtime of iLID is nearly the same as BC. It
259"
EXPERIMENTS,0.7459016393442623,"substantiates that the iLID is indeed a lightweight method. Due
260"
EXPERIMENTS,0.7479508196721312,"to the cooperation training between the discriminator and policy,
261"
EXPERIMENTS,0.75,"DWBC requires additional computation than iLID. CLARE is
262"
EXPERIMENTS,0.7520491803278688,"costly due to the effort to solve an intermediate ofﬂine RL problem.
263"
RELATED WORK,0.7540983606557377,"5
Related work
264"
RELATED WORK,0.7561475409836066,"Ofﬂine IL deals with training an agent to mimic the actions of a demonstrator in an entirely ofﬂine
265"
RELATED WORK,0.7581967213114754,"fashion. BC (Ross and Bagnell, 2010) is an intrinsically ofﬂine solution, but it is prone to covariate
266"
RELATED WORK,0.7602459016393442,"shift and inevitably suffers from error compounding, i.e., there is no way for the policy to learn how
267"
RELATED WORK,0.7622950819672131,"to recover if it deviates from the expert behavior to a state not seen in the expert demonstrations
268"
RELATED WORK,0.764344262295082,"(Levine et al., 2020). Considerable research has been devoted to developing new ofﬂine IL methods
269"
RELATED WORK,0.7663934426229508,"to remedy this problem, e.g., Jarrett et al. (2020); Chan and van der Schaar (2021); Garg et al. (2021);
270"
RELATED WORK,0.7684426229508197,"Klein et al. (2011, 2012); Piot et al. (2014); Herman et al. (2016); Kostrikov et al. (2019); Swamy
271"
RELATED WORK,0.7704918032786885,"et al. (2021); Florence et al. (2022). However, since these methods imitate all given demonstrations,
272"
RELATED WORK,0.7725409836065574,"they often require a large amount of clean expert data, which can be expensive for real-world tasks.
273"
RELATED WORK,0.7745901639344263,"Recently, there has been growing interest in exploring how to effectively leverage imperfect data in
274"
RELATED WORK,0.7766393442622951,"ofﬂine IL (Xu et al., 2022a; Yu et al., 2022; Sasaki and Yamashina, 2020; Kim et al., 2022). Sasaki
275"
RELATED WORK,0.7786885245901639,"and Yamashina (2020) analyze why the imitation policy trained by BC deteriorates its performance
276"
RELATED WORK,0.7807377049180327,"when using noisy demonstrations. They reuse an ensemble of policies learned from the previous
277"
RELATED WORK,0.7827868852459017,"iteration as the weight of the original BC objective to extract the expert behaviors. However, this
278"
RELATED WORK,0.7848360655737705,"requires that expert data occupies the majority proportion of the ofﬂine dataset, otherwise the policy
279"
RELATED WORK,0.7868852459016393,"will be misguided to imitate the suboptimal data. Kim et al. (2022) retroﬁt the BC objective with an
280"
RELATED WORK,0.7889344262295082,"additional KL-divergence term to regularize the learned policy to stay close to the behavior policy.
281"
RELATED WORK,0.7909836065573771,"Although it can implicitly extract the behaviors that bear similarity to the expert demonstrations, it
282"
RELATED WORK,0.7930327868852459,"easily fails to achieve satisfactory performance when the diverse data is highly suboptimal. Xu et al.
283"
RELATED WORK,0.7950819672131147,"(2022a) cope with this issue by introducing an additional discriminator, the outputs of which serve
284"
RELATED WORK,0.7971311475409836,"as the weights of the original BC loss, so as to imitate demonstrations selectively. Unfortunately, it
285"
RELATED WORK,0.7991803278688525,"selects behaviors building on state-action similarity, which does not sufﬁce to leverage the dynamics
286"
RELATED WORK,0.8012295081967213,"information and diverse behaviors. In ofﬂine RL, Yu et al. (2022) propose to utilize unlabeled data by
287"
RELATED WORK,0.8032786885245902,"applying zero rewards, but this method necessitates a large amount of labeled ofﬂine data. In contrast,
288"
RELATED WORK,0.805327868852459,"this paper focuses on the setting with no access to any reward signals.
289"
RELATED WORK,0.8073770491803278,"Ofﬂine inverse reinforcement learning (IRL) explicitly learns a reward function from ofﬂine datasets,
290"
RELATED WORK,0.8094262295081968,"aiming to comprehend and generalize the underlying intentions behind expert actions (Lee et al.,
291"
RELATED WORK,0.8114754098360656,"2019). Zolna et al. (2020) propose ORIL that constructs a reward function that discriminates expert
292"
RELATED WORK,0.8135245901639344,"and exploratory trajectories, followed by an ofﬂine RL progress. Chan and van der Schaar (2021)
293"
RELATED WORK,0.8155737704918032,"use a variational method to jointly learn an approximate posterior distribution over the reward and
294"
RELATED WORK,0.8176229508196722,"policy. Garg et al. (2021) propose to learn a soft Q-function that implicitly represents both reward
295"
RELATED WORK,0.819672131147541,"and policy, which can stabilize the training. To cope with the reward extrapolation error, Chang
296"
RELATED WORK,0.8217213114754098,"et al. (2022) introduce a model-based ofﬂine IRL algorithms that uses a model inaccuracy estimate
297"
RELATED WORK,0.8237704918032787,"to penalize the learned reward function on out-of-distribution state-actions. Recently, Yue et al.
298"
RELATED WORK,0.8258196721311475,"(2023) also propose a model-based ofﬂine IRL approach, named CLARE. In contrast to Chang et al.
299"
RELATED WORK,0.8278688524590164,"(2022), they compute a conservative element-wise weight to implicitly penalize out-of-distribution
300"
RELATED WORK,0.8299180327868853,"behaviors. However, it is highly challenging to deﬁne and learn meaningful reward functions without
301"
RELATED WORK,0.8319672131147541,"environmental interaction (Xu et al., 2022b). The model-based approaches often struggle to scale in
302"
RELATED WORK,0.8340163934426229,"high-dimensional environments, and their min-max progress usually causes training to be unstable
303"
RELATED WORK,0.8360655737704918,"and inefﬁcient.
304"
CONCLUSION,0.8381147540983607,"6
Conclusion
305"
CONCLUSION,0.8401639344262295,"In this paper, we introduce a simple yet effective data selection method along with a lightweight
306"
CONCLUSION,0.8422131147540983,"behavior cloning algorithm to fully leverage the imperfect demonstrations in ofﬂine IL. In contrast
307"
CONCLUSION,0.8442622950819673,"to the prior methods, we exploit the resultant states to access the value of behaviors, which is an
308"
CONCLUSION,0.8463114754098361,"informative criterion that enables explicit utilization of dynamics information and the extraction of
309"
CONCLUSION,0.8483606557377049,"both expert-like and beneﬁcial diverse behaviors. We provide necessary theoretical guarantees for the
310"
CONCLUSION,0.8504098360655737,"proposed method, and extensive experiments corroborate that iLID outperforms existing methods
311"
CONCLUSION,0.8524590163934426,"in continuous, high-dimensional environments by a signiﬁcant margin. In future work, we plan to
312"
CONCLUSION,0.8545081967213115,"establish theoretical guarantees for iLID in the general stochastic MDPs and explore whether the
313"
CONCLUSION,0.8565573770491803,"proposed methods can beneﬁt ofﬂine RL in terms of data selection and policy optimization.
314"
REFERENCES,0.8586065573770492,"References
315"
REFERENCES,0.860655737704918,"Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Proc. of NeurIPS,
316"
REFERENCES,0.8627049180327869,"1988.
317"
REFERENCES,0.8647540983606558,"Stéphane Ross and Drew Bagnell. Efﬁcient reductions for imitation learning. In Proc. of AISTATS,
318"
REFERENCES,0.8668032786885246,"pages 661–668, 2010.
319"
REFERENCES,0.8688524590163934,"Daniel Jarrett, Ioana Bica, and Mihaela van der Schaar. Strictly batch imitation learning by energy-
320"
REFERENCES,0.8709016393442623,"based distribution matching. Proc. of NeurIPS, pages 7354–7365, 2020.
321"
REFERENCES,0.8729508196721312,"Alex J Chan and M van der Schaar. Scalable bayesian inverse reinforcement learning. In Proc. of
322"
REFERENCES,0.875,"ICLR, 2021.
323"
REFERENCES,0.8770491803278688,"Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama.
324"
REFERENCES,0.8790983606557377,"Imitation learning from imperfect demonstration. In Proc. of ICML, pages 6818–6827, 2019.
325"
REFERENCES,0.8811475409836066,"Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin. Discriminator-weighted ofﬂine imitation
326"
REFERENCES,0.8831967213114754,"learning from suboptimal demonstrations. In Proc. of ICML, pages 24725–24742, 2022a.
327"
REFERENCES,0.8852459016393442,"Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and Sergey Levine. How
328"
REFERENCES,0.8872950819672131,"to leverage unlabeled data in ofﬂine reinforcement learning. In Proc. of ICML, pages 25611–25635,
329"
REFERENCES,0.889344262295082,"2022.
330"
REFERENCES,0.8913934426229508,"Fumihiro Sasaki and Ryota Yamashina. Behavioral cloning from noisy demonstrations. In Proc. of
331"
REFERENCES,0.8934426229508197,"ICLR, 2020.
332"
REFERENCES,0.8954918032786885,"Konrad Zolna, Alexander Novikov, Ksenia Konyushkova, Caglar Gulcehre, Ziyu Wang, Yusuf
333"
REFERENCES,0.8975409836065574,"Aytar, Misha Denil, Nando de Freitas, and Scott Reed. Ofﬂine learning from demonstrations and
334"
REFERENCES,0.8995901639344263,"unlabeled experience. In Proc. of NeurIPS Workshop, 2020.
335"
REFERENCES,0.9016393442622951,"Jonathan Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating
336"
REFERENCES,0.9036885245901639,"covariate shift in imitation learning via ofﬂine data with partial coverage. Proc. of NeurIPS, pages
337"
REFERENCES,0.9057377049180327,"965–979, 2022.
338"
REFERENCES,0.9077868852459017,"Sheng Yue, Guanbo Wang, Wei Shao, Zhaofeng Zhang, Sen Lin, Ju Ren, and Junshan Zhang. Clare:
339"
REFERENCES,0.9098360655737705,"Conservative model-based reward learning for ofﬂine inverse reinforcement learning. In Proc. of
340"
REFERENCES,0.9118852459016393,"ICLR, 2023.
341"
REFERENCES,0.9139344262295082,"Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks.
342"
REFERENCES,0.9159836065573771,"In Proc. of ICML, pages 214–223, 2017.
343"
REFERENCES,0.9180327868852459,"Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. Iq-learn:
344"
REFERENCES,0.9200819672131147,"Inverse soft-q learning for imitation. Proc. of NeurIPS, pages 4028–4039, 2021.
345"
REFERENCES,0.9221311475409836,"Geon-Hyeong Kim, Seokin Seo, Jongmin Lee, Wonseok Jeon, HyeongJoo Hwang, Hongseok
346"
REFERENCES,0.9241803278688525,"Yang, and Kee-Eung Kim. Demodice: Ofﬂine imitation learning with supplementary imperfect
347"
REFERENCES,0.9262295081967213,"demonstrations. In Proc. of ICLR, 2022.
348"
REFERENCES,0.9282786885245902,"Tian Xu, Ziniu Li, Yang Yu, and Zhi-Quan Luo. On generalization of adversarial imitation learning
349"
REFERENCES,0.930327868852459,"and beyond. arXiv preprint arXiv:2106.10424, 2021.
350"
REFERENCES,0.9323770491803278,"Nived Rajaraman, Lin Yang, Jiantao Jiao, and Kannan Ramchandran. Toward the fundamental limits
351"
REFERENCES,0.9344262295081968,"of imitation learning. Proc. of NeurIPS, pages 2914–2924, 2020.
352"
REFERENCES,0.9364754098360656,"Charles Elkan and Keith Noto. Learning classiﬁers from only positive and unlabeled data. In Proc. of
353"
REFERENCES,0.9385245901639344,"KDD, pages 213–220, 2008.
354"
REFERENCES,0.9405737704918032,"Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
355"
REFERENCES,0.9426229508196722,"Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
356"
REFERENCES,0.944672131147541,"Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al.
Soft actor-critic algorithms and
357"
REFERENCES,0.9467213114754098,"applications. arXiv preprint arXiv:1812.05905, 2018.
358"
REFERENCES,0.9487704918032787,"Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
359"
REFERENCES,0.9508196721311475,"data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
360"
REFERENCES,0.9528688524590164,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tutorial,
361"
REFERENCES,0.9549180327868853,"review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
362"
REFERENCES,0.9569672131147541,"Edouard Klein, Matthieu Geist, and Olivier Pietquin. Batch, off-policy and model-free apprenticeship
363"
REFERENCES,0.9590163934426229,"learning. In Proc. of EWRL, pages 285–296, 2011.
364"
REFERENCES,0.9610655737704918,"Edouard Klein, Matthieu Geist, Bilal Piot, and Olivier Pietquin. Inverse reinforcement learning
365"
REFERENCES,0.9631147540983607,"through structured classiﬁcation. Proc. of NeurIPS, 2012.
366"
REFERENCES,0.9651639344262295,"Bilal Piot, Matthieu Geist, and Olivier Pietquin. Boosted and reward-regularized classiﬁcation for
367"
REFERENCES,0.9672131147540983,"apprenticeship learning. In Proc. of AAMAS, pages 1249–1256, 2014.
368"
REFERENCES,0.9692622950819673,"Michael Herman, Tobias Gindele, Jörg Wagner, Felix Schmitt, and Wolfram Burgard. Inverse
369"
REFERENCES,0.9713114754098361,"reinforcement learning with simultaneous estimation of rewards and dynamics.
In Proc. of
370"
REFERENCES,0.9733606557377049,"AISTATS, pages 102–110, 2016.
371"
REFERENCES,0.9754098360655737,"Ilya Kostrikov, Oﬁr Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution
372"
REFERENCES,0.9774590163934426,"matching. In Proc. of ICLR, 2019.
373"
REFERENCES,0.9795081967213115,"Gokul Swamy, Sanjiban Choudhury, J Andrew Bagnell, and Steven Wu. Of moments and matching:
374"
REFERENCES,0.9815573770491803,"A game-theoretic framework for closing the imitation gap. In Proc. of ICML, pages 10022–10032,
375"
REFERENCES,0.9836065573770492,"2021.
376"
REFERENCES,0.985655737704918,"Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs, Adrian
377"
REFERENCES,0.9877049180327869,"Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. In Proc.
378"
REFERENCES,0.9897540983606558,"of CoRL, pages 158–168, 2022.
379"
REFERENCES,0.9918032786885246,"Donghun Lee, Srivatsan Srinivasan, and Finale Doshi-Velez. Truly batch apprenticeship learning
380"
REFERENCES,0.9938524590163934,"with deep successor features. In Proc. of IJCAI, 2019.
381"
REFERENCES,0.9959016393442623,"Tian Xu, Ziniu Li, Yang Yu, and Zhi-Quan Luo. Understanding adversarial imitation learning in
382"
REFERENCES,0.9979508196721312,"small sample regime: A stage-coupled analysis. arXiv preprint arXiv:2208.01899, 2022b.
383"
