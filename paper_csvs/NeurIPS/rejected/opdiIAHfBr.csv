Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010582010582010583,"We study the intriguing connection between visual data, deep networks, and the
1"
ABSTRACT,0.0021164021164021165,"brain. Our method creates a universal channel alignment by using brain voxel
2"
ABSTRACT,0.0031746031746031746,"fMRI response prediction as the training objective. We discover that deep net-
3"
ABSTRACT,0.004232804232804233,"works, trained with different objectives, share common feature channels across
4"
ABSTRACT,0.005291005291005291,"various models. These channels can be clustered into recurring sets, correspond-
5"
ABSTRACT,0.006349206349206349,"ing to distinct brain regions, indicating the formation of visual concepts. Tracing
6"
ABSTRACT,0.007407407407407408,"the clusters of channel responses onto the images, we see semantically meaning-
7"
ABSTRACT,0.008465608465608466,"ful object segments emerge, even without any supervised decoder. Furthermore,
8"
ABSTRACT,0.009523809523809525,"the universal feature alignment and the clustering of channels produce a picture
9"
ABSTRACT,0.010582010582010581,"and quantification of how visual information is processed through the different
10"
ABSTRACT,0.01164021164021164,"network layers, which produces precise comparisons between the networks.
11"
INTRODUCTION,0.012698412698412698,"1
Introduction
12"
INTRODUCTION,0.013756613756613757,"Introducing a novel approach, Yang et al. (2024) has successfully established a method of computing
13"
INTRODUCTION,0.014814814814814815,"a mapping between the brain and deep-nets, effectively linking two black boxes. The brain fMRI
14"
INTRODUCTION,0.015873015873015872,"prediction task allows for visualizing information flow from layer to layer, using the brain as an
15"
INTRODUCTION,0.016931216931216932,"analysis tool.
16"
INTRODUCTION,0.01798941798941799,channel (768D)
INTRODUCTION,0.01904761904761905,activation
INTRODUCTION,0.020105820105820106,visual brain
INTRODUCTION,0.021164021164021163,"Linear
transform LH
RH"
INTRODUCTION,0.022222222222222223,"Figure 1: Transform the hidden channel activation
of deep-nets into visual brain voxels’ response."
INTRODUCTION,0.02328042328042328,"If a picture is worth a thousand words, the
17"
INTRODUCTION,0.02433862433862434,"main idea is that the brain’s thousands of voxels
18"
INTRODUCTION,0.025396825396825397,"can be thought of as alphabets for these words
19"
INTRODUCTION,0.026455026455026454,"that describe an image. Just as alphabets must
20"
INTRODUCTION,0.027513227513227514,"be combined to form words and phrases with
21"
INTRODUCTION,0.02857142857142857,"meanings, we need to find the grouping of brain
22"
INTRODUCTION,0.02962962962962963,"voxels and their network channel counterparts
23"
INTRODUCTION,0.030687830687830688,"to understand their meaning (Figure 1).
24"
INTRODUCTION,0.031746031746031744,"Our main discovery is that while the network
25"
INTRODUCTION,0.0328042328042328,"layer structure differs, channel feature corre-
26"
INTRODUCTION,0.033862433862433865,"spondence exists across networks with a shared
27"
INTRODUCTION,0.03492063492063492,"encoding of reoccurring visual concepts. This paper builds upon the idea of ‘Rosetta stone’ neurons
28"
INTRODUCTION,0.03597883597883598,"(Dravid et al., 2023), which find channels across networks that share similar image responses in bi-
29"
INTRODUCTION,0.037037037037037035,"nary segmentation. If channels are alphabets, ‘Rosetta stone’ provides an alphabet-level translation
30"
INTRODUCTION,0.0380952380952381,"between networks.
31"
INTRODUCTION,0.039153439153439155,"Individual channel-level analysis could miss feature correspondence across networks at finer and
32"
INTRODUCTION,0.04021164021164021,"coarser levels. On a finer level, because the channels are invariant up to a linear transformation, we
33"
INTRODUCTION,0.04126984126984127,"might miss a reconstituted feature constructed from a composition of existing channels. On a coarse
34"
INTRODUCTION,0.042328042328042326,"level, the channels can be combined and clustered to form a bigger ‘Rosetta’ concept.
35"
INTRODUCTION,0.04338624338624339,"To address fine-level channel analysis, we use brain voxel response as a reference signal and linearly
36"
INTRODUCTION,0.044444444444444446,"transform channels for each network into a shared space sufficient for brain fMRI prediction. This
37"
INTRODUCTION,0.0455026455026455,"process produces a universal feature space that aligns channel features across the layers and models.
38"
INTRODUCTION,0.04656084656084656,"To find bigger visual concepts, one can start with Neuroscience knowledge of brain regions (ROIs)
39"
INTRODUCTION,0.047619047619047616,"with specific brain functionality, i.e., V1, V4, and EBA. While tracing the mapping of the ROIs to
40"
INTRODUCTION,0.04867724867724868,"channels can produce visual concepts (Figure 2), brain regions don’t function in isolation.
41"
INTRODUCTION,0.04973544973544974,Image Patch
INTRODUCTION,0.050793650793650794,"upper
left"
INTRODUCTION,0.05185185185185185,center
INTRODUCTION,0.05291005291005291,"lower
right 0 256 512 768 4 2 0 2 4"
INTRODUCTION,0.05396825396825397,"Brain ROI's
Top Channels"
INTRODUCTION,0.05502645502645503,"V1
V4
EBA"
INTRODUCTION,0.056084656084656084,"V1
V4
EBA
all"
INTRODUCTION,0.05714285714285714,"Figure 2: From the 768D feature on CLIP layer-6, we extract different levels of segmentation by
restricting the use of a subset of channels. Left: Channel activation on example image patches. The
ordering of channels is sorted from the early brain to the late brain by their weights for brain voxels.
Right: Spectral clustering on each subset of channels filtered by each brain ROI (V1, V4, EBA),
image pixels colored by 3D spectral-tSNE of top 10 eigenvectors."
INTRODUCTION,0.0582010582010582,"Instead of searching through all possible channel grouping combinations, our first insight is that
42"
INTRODUCTION,0.05925925925925926,"we can create a channel grouping hypothesis by examining channels from each pixel’s perspective.
43"
INTRODUCTION,0.06031746031746032,"Think of the pixels and channels forming a bipartite graph; each channel produces a per-pixel re-
44"
INTRODUCTION,0.061375661375661375,"sponse (image activation map), defining the graph edge between the pixels and channels. Taking the
45"
INTRODUCTION,0.06243386243386243,"perspective of pixels, one can collect graph edges incident on each pixel into a vector, which can be
46"
INTRODUCTION,0.06349206349206349,"thresholded to produce a hypothesis grouping over channels.
47"
INTRODUCTION,0.06455026455026455,"Our second insight is that if a channel grouping hypothesis repeats across images, layers, and mod-
48"
INTRODUCTION,0.0656084656084656,"els, it is highly unlikely to be accidental and, therefore, signals meaningful visual concepts.
49"
INTRODUCTION,0.06666666666666667,"We formulate this clustering problem as a graph partition task. The graph nodes are the product
50"
INTRODUCTION,0.06772486772486773,"space of pixels and layers. We apply spectral clustering to produce k-top eigenvectors. We take
51"
INTRODUCTION,0.06878306878306878,"advantage of two properties of spectral clustering: it makes 1) soft-cluster embedding space in the
52"
INTRODUCTION,0.06984126984126984,"form of eigenvectors and 2) hierarchical clustering by varying the number of eigenvectors.
53"
INTRODUCTION,0.07089947089947089,"We made the following discoveries. First, shared channel sets, reoccurring across layers and models,
54"
INTRODUCTION,0.07195767195767196,"predict response in distinct brain regions. By tracing the channel activation to the known brain ROI
55"
INTRODUCTION,0.07301587301587302,"properties, we observe that the channel cluster encodes visual concepts at various levels of visual
56"
ABSTRACT,0.07407407407407407,"abstraction.
57"
ABSTRACT,0.07513227513227513,"Second, meaningful object segments can emerge by tracing the channel cluster responses onto each
58"
ABSTRACT,0.0761904761904762,"image. We observed that some channel clusters produce figure/ground separation while others pro-
59"
ABSTRACT,0.07724867724867725,"duce fine-grained category classification. Our image segmentation requires no additional segmenta-
60"
ABSTRACT,0.07830687830687831,"tion decoder and uses only a simple distance measure over the eigenvectors.
61"
ABSTRACT,0.07936507936507936,"Finally, the universal feature alignment and the spectral clustering of channels produce a picture and
62"
ABSTRACT,0.08042328042328042,"quantification of how visual information is processed through the different network layers.
63"
ABSTRACT,0.08148148148148149,"While these discoveries are promising, there are two main technical hurdles to overcome to verify
64"
ABSTRACT,0.08253968253968254,"them on a large scale. Our method rests upon a crucial assumption: the channels across the different
65"
ABSTRACT,0.0835978835978836,"layers and models can be mapped into a shared space. While brain prediction over thousands of
66"
ABSTRACT,0.08465608465608465,"voxels can provide strong guidance for this alignment, an additional constraint would be needed
67"
ABSTRACT,0.08571428571428572,"when the shared space has a large dimension (suitable for expressiveness). We use clustering as
68"
ABSTRACT,0.08677248677248678,"a constraint, ensuring alignment linear transformation preserves spectral clustering eigenvectors.
69"
ABSTRACT,0.08783068783068783,"Furthermore, the graph size is enormous as it is a product space over pixels, layers, images, and
70"
ABSTRACT,0.08888888888888889,"models; therefore, computing eigenvectors over their pairwise affinity matrix can be computationally
71"
ABSTRACT,0.08994708994708994,"infeasible. We developed a Nystrom-like approximation to ensure efficient computation.
72"
ABSTRACT,0.091005291005291,"In summary, our key contributions are:
73"
WE CONSTRUCTED A UNIVERSAL CHANNEL-ALIGNED SPACE USING BRAIN ENCODING AS SUPERVISION AND SPEC-,0.09206349206349207,"1. We constructed a universal channel-aligned space using brain encoding as supervision and spec-
74"
WE CONSTRUCTED A UNIVERSAL CHANNEL-ALIGNED SPACE USING BRAIN ENCODING AS SUPERVISION AND SPEC-,0.09312169312169312,"tral clustering eigenvector constraints to ensure minimal channel signal loss. Brain encoding asso-
75"
WE CONSTRUCTED A UNIVERSAL CHANNEL-ALIGNED SPACE USING BRAIN ENCODING AS SUPERVISION AND SPEC-,0.09417989417989418,"ciates the aligned channel space to brain regions and gives them meanings.
76"
WE CONSTRUCTED A UNIVERSAL CHANNEL-ALIGNED SPACE USING BRAIN ENCODING AS SUPERVISION AND SPEC-,0.09523809523809523,"2. Models trained with different objectives learned similar visual concepts: corresponding channel
77"
WE CONSTRUCTED A UNIVERSAL CHANNEL-ALIGNED SPACE USING BRAIN ENCODING AS SUPERVISION AND SPEC-,0.0962962962962963,"patterns exist across different models. The resulting visual concepts can be validated by unsuper-
78"
WE CONSTRUCTED A UNIVERSAL CHANNEL-ALIGNED SPACE USING BRAIN ENCODING AS SUPERVISION AND SPEC-,0.09735449735449736,"vised segmentation benchmarks on ImageNet-segmentation and PASCAL VOC.
79"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.09841269841269841,"3. Models show divergent computation paths over the visual concept space formed by the top-k
80"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.09947089947089947,"spectral eigenvectors. Different models differ in trajectories and pace of movement layer-to-layer.
81"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.10052910052910052,"2
Methods: AlignedCut
82"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.10158730158730159,"0
4
8
0
4
8
0
4"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.10264550264550265,"0
4
8
0
4
8
0
4
8"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.1037037037037037,before channel align
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.10476190476190476,"0
4
8
0
4
8
0
4
8
8
CLIP 
DINO 
MAE"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.10582010582010581,"0
4
8
0
4
8
0
4
8 Layer"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.10687830687830688,after channel align 0.6 0.4 0.2 0.0 0.2 0.4 0.6
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.10793650793650794,"CLIP
DINO
MAE
Layer"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.10899470899470899,"Figure 3: Cosine similarity of channel activa-
tion on the same image inputs."
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.11005291005291006,"Just as human languages might consist of distinct
83"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.1111111111111111,"alphabets, features across different models appear
84"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.11216931216931217,"superficially in embedding spaces as almost mutu-
85"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.11322751322751323,"ally orthogonal (Figure 3). However, the underly-
86"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.11428571428571428,"ing information that they represent can be similar.
87"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.11534391534391535,"To jointly analyze features across models and lay-
88"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.1164021164021164,"ers, we proposed the channel align transform that
89"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.11746031746031746,"linearly projects features to a universal space.
90"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.11851851851851852,"The learning signal for the channel align transform
91"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.11957671957671957,"is provided by brain response prediction. Learn-
92"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.12063492063492064,"ing from brain prediction offers two advantages.
93"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.12169312169312169,"First, brain response covers rich representations from all levels of semantics; the channel alignment
94"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.12275132275132275,"removes irrelevant information while preserving the necessary and sufficient visual image features.
95"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.12380952380952381,"Second, knowledge of brain regions provides an interpretable understanding of their corresponding
96"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.12486772486772486,"channels derived from the alignment.
97"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.1259259259259259,"Our visual concept discovery is formulated as a graph partitioning task using spectral clustering.
98"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.12698412698412698,"We term our approach for this channel align and graph partitioning as AlignedCut. Furthermore, a
99"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.12804232804232804,"major challenge in applying spectral clustering to large graphs is the complexity scaling issue. To
100"
MODELS SHOW DIVERGENT COMPUTATION PATHS OVER THE VISUAL CONCEPT SPACE FORMED BY THE TOP-K,0.1291005291005291,"address this, we developed a Nystrom-like approximation to reduce the computational complexity.
101"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.13015873015873017,"2.1
Brain-Guided Universal Channel Align
102"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.1312169312169312,"Brain Dataset
We used the Algonauts competition (Gifford et al., 2023) release of Nature Scenes
103"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.13227513227513227,"Dataset (NSD) (Allen et al., 2022). Briefly, NSD provides an fMRI brain scan when watching
104"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.13333333333333333,"COCO images. Each subject viewed 10,000 images over 40 hours of scanning. We used the first
105"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.1343915343915344,"subject’s publicly shared pre-processed and denoised (Prince et al., 2022) data.
106"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.13544973544973546,"Channel Align
Let V = {V1, V2, · · · , Vn|Vi ∈RP ×Di} be the set of image features, extracted
107"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.1365079365079365,"from each layer of pre-trained ViT models, where P = (H×W+1) is image patches and class token,
108"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.13756613756613756,"Di is the hidden dimension. In particular, we used the attention layer output for each Vi without
109"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.13862433862433862,"adding residual connections from previous layers. Let V′ be the channel-aligned features; the goal
110"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.13968253968253969,"of channel alignment is to learn a set of linear transform W = {W1, W2, · · · , Wn|Wi ∈RDi×D′}.
111"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.14074074074074075,"In the new D′ dimensional space, channels are aligned.
112"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.14179894179894179,"V′ = V ⊙W = {V1W1, V2W2, · · · , VnWn|ViWi ∈RP ×D′}
(1)"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.14285714285714285,"Brain Prediction
To produce a learning signal for channel align W, features from V′ are summed
113"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.1439153439153439,"(not concatenated) to do brain prediction. Let Y ∈R1×N be the brain prediction target, where N is
114"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.14497354497354498,"the number of flattened 3D brain voxels, and 1 indicates that each voxel’s response is a scalar value.
115"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.14603174603174604,"Let Fθ : RP ×D′ ⇒R1×N be the learned brain encoding model; without loss of generalizability, we
116"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.14708994708994708,"set Fθ as global average pooling then linear weight βθ ∈RD′×N and bias ϵθ ∈R1×N:
117 """
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.14814814814814814,"Avg Pool
p∈P
( 1 n n
X"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.1492063492063492,"i=1
(ViWi)) × βθ + ϵθ #"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.15026455026455027,"⇒Y
(2)"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.15132275132275133,"Channel in the Brain’s Space
Let B = {B1, B2, · · · , Bn|Bi ∈RP ×N} be the set of channel
118"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.1523809523809524,"activations in the brain’s space. By defining Bi := ViWi×βθ, we have the brain response prediction
119"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.15343915343915343,Y = Avg Poolp∈P ( 1
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.1544973544973545,"n
Pn
i=1 Bi) + ϵθ (Eq. (2)). Intuitively, we linearly transformed the activation
120"
BRAIN-GUIDED UNIVERSAL CHANNEL ALIGN,0.15555555555555556,"to the brain’s space, such that the activation from all slots sum up to the brain response prediction.
121"
GRAPH SPECTRAL CLUSTERING,0.15661375661375662,"2.2
Graph Spectral Clustering
122"
GRAPH SPECTRAL CLUSTERING,0.15767195767195769,"Spectral Clustering
We use spectral clustering for visual concepts discovery and image-channel
123"
GRAPH SPECTRAL CLUSTERING,0.15873015873015872,"analysis; it provides 1) soft-cluster embedding space and 2) unsupervised hierarchical image seg-
124"
GRAPH SPECTRAL CLUSTERING,0.15978835978835979,"mentation. Normalized Cut (Shi and Malik, 2000) partitions the graph into sub-graphs with minimal
125"
GRAPH SPECTRAL CLUSTERING,0.16084656084656085,"cost of breaking edges. It embeds the graph into a lower dimensional eigenvector representation,
126"
GRAPH SPECTRAL CLUSTERING,0.1619047619047619,"where each eigenvector is a hierarchical sub-graph assignment.
127"
GRAPH SPECTRAL CLUSTERING,0.16296296296296298,"Let A ∈RM×M be the symmetric affinity matrix, where M denotes the total number of image
128"
GRAPH SPECTRAL CLUSTERING,0.164021164021164,"patches. Given channel aligned features V ′ ∈RM×D′, we define Aij := exp(cos(V ′
i , V ′
j ) −1)
129"
GRAPH SPECTRAL CLUSTERING,0.16507936507936508,"such that Aij > 0 measures the similarity between data i and j. The spectral clustering embedding
130"
GRAPH SPECTRAL CLUSTERING,0.16613756613756614,"X ∈RM×C is solved by the top C eigenvectors of the following generalized eigenproblem:
131"
GRAPH SPECTRAL CLUSTERING,0.1671957671957672,"(D−1/2AD−1/2)X = XΛ
(3)"
GRAPH SPECTRAL CLUSTERING,0.16825396825396827,where D is the diagonal degree matrix Dii = P
GRAPH SPECTRAL CLUSTERING,0.1693121693121693,"j Aij, Λ is diagonal eigenvalue matrix.
132"
GRAPH SPECTRAL CLUSTERING,0.17037037037037037,"Nystrom-like Approximation
Computing eigenvectors for A ∈RM×M is prohibitively expen-
133"
GRAPH SPECTRAL CLUSTERING,0.17142857142857143,"sive for enormous M with a time complexity of O(M 3). The original Nystrom approximation
134"
GRAPH SPECTRAL CLUSTERING,0.1724867724867725,"method (Fowlkes et al., 2004) reduced the time complexity to O(m3 + m2M) by solving eigenvec-
135"
GRAPH SPECTRAL CLUSTERING,0.17354497354497356,"tors on sub-sampled graph A′ ∈Rm×m, where m ≪M. In particular, the orthogonalization step
136"
GRAPH SPECTRAL CLUSTERING,0.1746031746031746,"of eigenvectors introduced the time complexity of O(m2M). Because our Nystrom-like approxi-
137"
GRAPH SPECTRAL CLUSTERING,0.17566137566137566,"mation trades the O(m2M) orthogonalization term with the K-nearest neighbor, our Nystrom-like
138"
GRAPH SPECTRAL CLUSTERING,0.17671957671957672,"approximation reduced the time complexity to O(m3 + mM).
139"
GRAPH SPECTRAL CLUSTERING,0.17777777777777778,"Our Nystrom-like Approximation first solves the eigenvector X′ ∈Rm×C on a sub-sampled graph
140"
GRAPH SPECTRAL CLUSTERING,0.17883597883597885,"A′ ∈Rm×m using Equation (3), then propagates the eigenvector from the sub-graph m nodes
141"
GRAPH SPECTRAL CLUSTERING,0.17989417989417988,"to the full-graph M nodes. Let ˜
X ∈RM×C be the approximation ˜
X ≈X. The eigenvector
142"
GRAPH SPECTRAL CLUSTERING,0.18095238095238095,"approximation ˜
Xi of full-graph node i ≤M is assigned by averaging the top K-nearest neighbors’
143"
GRAPH SPECTRAL CLUSTERING,0.182010582010582,"eigenvector X′
k from the sub-graph nodes k ≤m:
144"
GRAPH SPECTRAL CLUSTERING,0.18306878306878308,"Ki = KNN(A∗i; m, K) = arg max
k≤m K
X"
GRAPH SPECTRAL CLUSTERING,0.18412698412698414,"k=1
Aki"
GRAPH SPECTRAL CLUSTERING,0.18518518518518517,"˜
Xi =
1
P"
GRAPH SPECTRAL CLUSTERING,0.18624338624338624,k∈Ki Aki X
GRAPH SPECTRAL CLUSTERING,0.1873015873015873,"k∈Ki
AkiX′
k (4)"
GRAPH SPECTRAL CLUSTERING,0.18835978835978837,"where KNN(A∗i; m, K) denotes KNN from full-graph node i ≤M to sub-graph nodes k ≤m.
145"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.18941798941798943,"2.3
Affinity Eigen-constraints as Regularization for Channel Align
146"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.19047619047619047,"Table 1: Affinity eigen-constraints improved
brain score (R2: variance explained)."
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.19153439153439153,"ROI Brain Score R2 (± 0.001)
λeigen
V1
V4
EBA
all"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.1925925925925926,"1.0
0.170
0.181
0.295
0.196
0.1
0.167
0.179
0.294
0.193
0
0.155
0.166
0.296
0.188"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.19365079365079366,"While brain prediction can provide strong supervi-
147"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.19470899470899472,"sion for the learned channel align operation, we ob-
148"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.19576719576719576,"served that the quality of unsupervised segmentation
149"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.19682539682539682,"dropped after the channel alignment. To address this
150"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.19788359788359788,"issue, a regularization term is added:
151"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.19894179894179895,"Leigen = ∥XbXT
b −XaXT
a ∥
(5)"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.2,"where Xb and Xa ∈R ˜m×c are affinity matrix
152"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.20105820105820105,"eigenvectors before and after channel alignment, re-
153"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.2021164021164021,"spectively; ˜m = 100 are randomly sampled nodes
154"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.20317460317460317,"in a mini-batch and c = 6 are the top eigenvectors. The eigen-constraint preserves spectral clus-
155"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.20423280423280424,"tering eigenvectors in dot-product space, invariant to random rotations in eigenvectors. We found
156"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.2052910052910053,"adding eigen-constraints improved both the performance of segmentation (Figure 5) and the brain
157"
AFFINITY EIGEN-CONSTRAINTS AS REGULARIZATION FOR CHANNEL ALIGN,0.20634920634920634,"prediction score (Table 1).
158"
RESULTS,0.2074074074074074,"3
Results
159"
RESULTS,0.20846560846560847,"Our spectral clustering analysis aims to discover visual concepts that share the same pattern of
160"
RESULTS,0.20952380952380953,"channel activation across different models and layers. However, implementing spectral clustering
161"
RESULTS,0.2105820105820106,"analysis comes with two main challenges. First, the models sit in different feature spaces, so direct
162"
RESULTS,0.21164021164021163,"clustering will not reveal their overlap and similarities. Second, when scaling up to a large graph,
163"
RESULTS,0.2126984126984127,"spectral clustering is computationally expensive.
164"
RESULTS,0.21375661375661376,"To address the first challenge, we developed our channel align transform to align features into a
165"
RESULTS,0.21481481481481482,"universal space. We extracted features from all 12 layers of the CLIP (ViT-B, OpenAI) (Radford
166"
RESULTS,0.21587301587301588,"et al., 2021), DINOv2 (ViT-B with registers) (Darcet et al., 2024), and MAE (ViT-B) (He et al.,
167"
RESULTS,0.21693121693121692,"2022) and then transformed features from each layer into the universal feature space.
168"
RESULTS,0.21798941798941798,"To address the second challenge, we developed our Nystrom-like approximation to reduce the com-
169"
RESULTS,0.21904761904761905,"putational complexity. We extracted features from 1000 ImageNet (Deng et al., 2009) images, with
170"
RESULTS,0.2201058201058201,"each image consisting of 197 patches per layer. The entire product space of all images and fea-
171"
RESULTS,0.22116402116402117,"tures totaled M = 7e+6 nodes, from which we applied our Nystrom-like approximation with sub-
172"
RESULTS,0.2222222222222222,"sampled m = 5e+4 nodes and KNN K = 100, computing the top 20 eigenvectors.
173"
RESULTS,0.22328042328042327,"To visualize the affinity eigenvectors, the top 20 eigenvectors were reduced to a 3-dimensional space
174"
RESULTS,0.22433862433862434,"by t-SNE, and a color value was assigned to each node by the RGB cube. We call this approach
175"
RESULTS,0.2253968253968254,"AlignedCut color.
176 CLIP"
RESULTS,0.22645502645502646,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
RESULTS,0.2275132275132275,"DINO
MAE
CLIP"
RESULTS,0.22857142857142856,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
RESULTS,0.22962962962962963,"DINO
MAE
CLIP"
RESULTS,0.2306878306878307,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
RESULTS,0.23174603174603176,"DINO
MAE"
RESULTS,0.2328042328042328,"Figure 4: Spectral clustering in the universal channel aligned feature space. The image pixels are
colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of
the top 20 eigenvectors. The coloring is consistent across all images, layers, and models."
RESULTS,0.23386243386243386,"In Figure 4, we displayed the analysis, AlignedCut color, and made the following observations:
177"
RESULTS,0.23492063492063492,"1. In CLIP layer-5, DINO layer-6, and MAE layer-8, there is class-agnostic figure-ground sepa-
178"
RESULTS,0.23597883597883598,"ration, with foreground objects from different categories grouped into the same AlignedCut color.
179"
RESULTS,0.23703703703703705,"2. In CLIP layer-9, there is a class-specific separation of foreground objects, with foreground
180"
RESULTS,0.23809523809523808,"objects grouped into AlignedCut colors with associated semantic categories.
181"
RESULTS,0.23915343915343915,"3. Before layer-3, CLIP and DINO produce the same AlignedCut color regardless of the image
182"
RESULTS,0.2402116402116402,"input. From layer-4 onwards, the AlignedCut color smoothly changes over layers.
183"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.24126984126984127,"3.1
Figure-ground representation emerge before categories
184"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.24232804232804234,"In this section, we benchmark each layer in CLIP with unsupervised segmentation. The key findings
185"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.24338624338624337,"from this benchmarking are: 1) The figure-ground representation emerges at CLIP layer-4 and is
186"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.24444444444444444,"preserved in subsequent layers; 2) Categories emerge over layers, peaking at layer-9 and layer-10.
187"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2455026455026455,"0
1
2
3
4
5
6
7
8
9 10 11
Layer 0.2 0.4 0.6 mIoU"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.24656084656084656,ImageNet-segmentation
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.24761904761904763,"before channel align
after channel align
  (w/o eigen constraints)"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.24867724867724866,"0
1
2
3
4
5
6
7
8
9 10 11
Layer 0.2 0.4 mIoU"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.24973544973544973,PASCAL VOC (val2012)
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2507936507936508,"Figure 5: Unsupervised segmentation scores from spectral clustering on each CLIP layer. ImageNet-
segmentation dataset is used with binary figure-ground labels, and the mIoU score peaks plateau
from layer-4 to layer-10. In PASCAL VOC with 20 class labels, the mIoU score peaks at layer-9."
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2518518518518518,"From which layers did the figure-ground and category representations emerge? We conducted
188"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2529100529100529,"experiments that compared the unsupervised segmentation scores across layers, tracing how well
189"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.25396825396825395,"each representation is encoded at each layer. We used two datasets: a) ImageNet-segmentation
190"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.25502645502645505,"(Guillaumin et al., 2014) with binary figure-ground labels, and b) PASCAL VOC (Everingham et al.,
191"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2560846560846561,"2010) with 20 category labels. The results are presented in Figure 5. On the ImageNet-segmentation
192"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2571428571428571,"benchmark, the score peaks at layer-4 (mIoU=0.6) and plateaus in subsequent layers, suggesting that
193"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2582010582010582,"the figure-ground representation is encoded and preserved from layer-4 onwards. On the PASCAL
194"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.25925925925925924,"VOC benchmark, the score peaks at layer-9 and layer-10 (mIoU=0.5) even though it is low at layer-4
195"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.26031746031746034,"(mIoU=0.2), indicating that category information is encoded at layer-9 and layer-10. Overall, we
196"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2613756613756614,"conclude that the figure-ground representation emerges before the category representation.
197"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2624338624338624,"3.2
Visual concepts: class-agnostic figure-ground
198"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2634920634920635,"In this section, we use brain activation heatmaps and image similarity heatmaps to describe figure-
199"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.26455026455026454,"ground visual concepts. The key findings from these heatmaps are: 1) The figure vs. ground pixels
200"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2656084656084656,"activate different channels; 2) The figure-ground visual concept is class-agnostic; 3) The figure-
201"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.26666666666666666,"ground visual concept is consistent across models.
202"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2677248677248677,"foreground
background ref. bg
fg"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2687830687830688,"Figure 6: The figure-ground visual concepts in CLIP layer-5. Left: Mean activation of foreground
or background pixels, linearly transformed to the brain’s space. Right: Cosine similarity from one
reference pixel marked. The figure-ground visual concepts are agnostic to image categories."
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2698412698412698,"How can the channel activation patterns of the figure-ground visual concept be described? We
203"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2708994708994709,"averaged the channel activations from foreground and background pixels, using the ground-truth
204"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.27195767195767195,"labels from the ImageNet-segmentation dataset. The averaged channel activations were transformed
205"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.273015873015873,"into the brain’s space. In Figure 6, foreground pixels exhibit positive activation in early visual brain
206"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2740740740740741,"ROIs (V1 to V4) and the face-selective ROI (FFA), while negatively activating place-selective ROIs
207"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2751322751322751,"(OPA and PPA). Interestingly, background pixels activate the reverse pattern compared to foreground
208"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2761904761904762,"pixels. Overall, the figure and ground pixels activate distinct brain ROIs.
209"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.27724867724867724,"Is the figure-ground visual concept class-agnostic? We manually selected one pixel and computed
210"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2783068783068783,"the cosine similarity to all of the other image pixels. In Figure 6, the results demonstrate that one
211"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.27936507936507937,"pixel (on the human) could segment out foreground objects from all other classes (shark, dog, cat,
212"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2804232804232804,"rabbit). The same result holds true for one background pixel. We conclude that the figure-ground
213"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2814814814814815,"visual concept is class-agnostic.
214"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.28253968253968254,"Figure 7: The same figure-ground visual concepts are found in CLIP, DINO and MAE. Left: Mean
activation of all foreground (top) and background (bottom) pixels; the three models exhibit similar
activation patterns. Right: AlignedCut, pixels colored by 3D spectral-tSNE of the top 20 eigenvec-
tors; the three models show similar grouping colors for foreground pixels."
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.28359788359788357,"Is the figure-ground visual concept consistent across models? We performed the channel analysis
215"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.28465608465608466,"for CLIP, DINO, and MAE. In Figure 7, the foreground or background pixels activates similar
216"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2857142857142857,"brain ROIs across the three models. Additionally, spectral clustering grouped the representations of
217"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2867724867724868,"foreground objects into similar colors for CLIP and DINO (light blue), the grouping for MAE is less
218"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2878306878306878,"similar (dark blue). Overall, the figure-ground visual concept is consistent across models.
219"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.28888888888888886,"3.3
Visual concepts: categories
220"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.28994708994708995,"In this section we use AlignedCut to discover category visual concepts. The key findings from the
221"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.291005291005291,"category visual concepts are: 1) Class-specific visual concepts activate diverse brain regions; 2)
222"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2920634920634921,"Visual concepts with higher channel activation values are more consistent.
223"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2931216931216931,"Figure 8: Category visual concepts in CLIP layer-9. Left: Mean activation of all pixels within an
Euclidean sphere centered at the visual concept in the 3D spectral-tSNE space; the concepts activate
different brain regions. Middle: The standard deviation negatively correlates with absolute mean
activations. Right: AlignedCut, pixels colored by 3D spectral-tSNE of the top 20 eigenvectors."
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.29417989417989415,"How does each class-specific concept activate the channels? To answer this question, we sam-
224"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.29523809523809524,"pled class-specific concepts from CLIP layer-9. First, we used farthest point sampling to identify
225"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2962962962962963,"candidate centers in the 3D spectral-tSNE space. Then, each candidate center was grouped with its
226"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.29735449735449737,"neighboring pixels within an Euclidean sphere in the spectral-tSNE space. Finally, the channel acti-
227"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.2984126984126984,"vations of the grouped pixels were averaged to produce the mean channel activation for each visual
228"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.29947089947089944,"concept. In Figure 8, Concept 1 (duck, goose) negatively activates late brain regions; Concept 2
229"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.30052910052910053,"(snake, turtle) positively activates early brain regions and also FFA; Concept 3 (dog) negatively ac-
230"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.30158730158730157,"tivates early brain regions. Overall, category-specific visual concepts activate diverse brain regions.
231"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.30264550264550266,"How do we quantify the consistency of each visual concept? Qualitatively, Concept 1 exhibits more
232"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.3037037037037037,"consistent coloring (Figure 8, pink) than Concept 3 (purple). To further quantify this observation, we
233"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.3047619047619048,"computed the mean and standard deviation of channel activations for each Euclidean sphere centered
234"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.3058201058201058,"on a concept. In Figure 8, there is a reverse U-shape relation between magnitude and standard
235"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.30687830687830686,"deviation. The reverse U-shape implies that larger absolute mean channel activation corresponds
236"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.30793650793650795,"to lower standard deviation. Overall, higher channel activation magnitudes suggest more consistent
237"
FIGURE-GROUND REPRESENTATION EMERGE BEFORE CATEGORIES,0.308994708994709,"visual concepts.
238"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3100529100529101,"3.4
Transition of visual concepts over layers
239"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3111111111111111,"In this section, instead of using 3D spectral-tSNE, we use 2D spectral-tSNE to trace the layer-to-
240"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.31216931216931215,"layer feature computation. The key findings of spectral-tSNE in 2D are: 1) The figure vs. ground
241"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.31322751322751324,"pixels are encoded in separate spaces in late layers; 2) The representations for foreground and back-
242"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3142857142857143,"ground bifurcate at CLIP layer-4 and DINO layer-5.
243 Layer"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.31534391534391537,"person
unsupervised"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3164021164021164,"horse
car
grass"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.31746031746031744,"sky
road"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.31851851851851853,"Figure 9: Trajectory of feature progression in layers for six example pixels. Left: 2D spectral-tSNE
plot of the top 20 eigenvectors, jointly clustered across all models; the foreground and background
pixels bifurcate at CLIP layer-4 and DINO layer-5. Right: Pixels colored by unsupervised segmen-
tation."
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.31957671957671957,"How does the network encode figure and ground pixels in each layer? We performed spectral
244"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.32063492063492066,"clustering and 2D t-SNE on the top 20 eigenvectors to project all layers into a 2D spectral-tSNE
245"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3216931216931217,"space. In Figure 9, we found that all foreground and background pixels are grouped together in
246"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.32275132275132273,"each early layer. Each early layer (dark dots) forms an isolated cluster separate from other layers,
247"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3238095238095238,"while late layers (bright dots) are grouped in the center. In the late layers, there is a separation
248"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.32486772486772486,"where foreground pixels occupy the upper part of 2D spectral-tSNE space, while background pixels
249"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.32592592592592595,"occupy the middle part. Overall, foreground and background pixels are encoded in separate spaces
250"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.326984126984127,"in late layers.
251"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.328042328042328,"How does the network process each pixel from layer to layer? In the 2D spectral-tSNE plot,
252"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3291005291005291,"we traced the trajectory for each pixel from layer-3 to the last layer. In Figure 9, we found that
253"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.33015873015873015,"the trajectories for foreground and background pixels bifurcate: foreground pixels (person, horse,
254"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.33121693121693124,"car) traverse to the upper side and remain within the upper side; background pixels (grass, road,
255"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3322751322751323,"sky) jump between the middle right and left sides. The same bifurcation is consistently observed
256"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3333333333333333,"for CLIP from layer-3 to layer-4 and DINO from layer-4 to layer-5. Furthermore, to quantify the
257"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3343915343915344,"bifurcation for foreground and background pixels, we first sampled 5 visual concepts from CLIP
258"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.33544973544973544,"layer-3 and layer-4. Then, we measured the transition probability between visual concepts, defined
259"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.33650793650793653,"as the proportion of pixels that transited from an Euclidean circle around concept A to a circle
260"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.33756613756613757,"around concept B. In Figure 10, the transition probability of foreground pixels to the upper side
261"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3386243386243386,"(A1 to B0) is higher than that of background pixels (0.44 vs. 0.16), while the transition probability
262"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3396825396825397,"of background pixels to the right side (A4 to B4) is higher than that of foreground pixels (0.36 vs.
263"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.34074074074074073,"0.06). Overall, this suggests a bifurcation of figure and ground pixel representations at the middle
264"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3417989417989418,"layers of both CLIP and DINO.
265"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.34285714285714286,"B0
B1
B2
B3
B4
CLIP Layer 4 (in)"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3439153439153439,"A0
A1
A2
A3
A4
CLIP Layer 3 (out)"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.344973544973545,"0.04
0.10
0.05
0.10
0.00"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.346031746031746,"0.16
0.26
0.03
0.01
0.00"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3470899470899471,"0.06
0.25
0.23
0.02
0.00"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.34814814814814815,"0.00
0.00
0.12
0.57
0.00"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3492063492063492,"0.00
0.01
0.10
0.00
0.36"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3502645502645503,Transition Probability (background pixels)
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3513227513227513,Transition Probability
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3523809523809524,"B0
B1
B2
B3
B4
CLIP Layer 4 (in)"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.35343915343915344,"A0
A1
A2
A3
A4
CLIP Layer 3 (out)"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3544973544973545,"0.28
0.07
0.01
0.01
0.00"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.35555555555555557,"0.44
0.08
0.01
0.00
0.00"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3566137566137566,"0.23
0.16
0.03
0.00
0.00"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.3576719576719577,"0.04
0.01
0.04
0.11
0.00"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.35873015873015873,"0.03
0.02
0.12
0.00
0.06"
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.35978835978835977,Transition Probability (foreground pixels)
TRANSITION OF VISUAL CONCEPTS OVER LAYERS,0.36084656084656086,"Figure 10: Transition probability of visual concepts from CLIP layer-3 to layer-4. Left: Five visual
concepts sampled from CLIP layer-3 and layer-4. Right: Transition probability measured separately
for foreground and background pixels; a bifurcation occurs where foreground pixels have more
traffic to concept B0, while background pixels have more traffic to concepts B3 and B4."
RELATED WORK,0.3619047619047619,"4
Related Work
266"
RELATED WORK,0.362962962962963,"Mechanistic Interpretability is a field of study that intends to understand and explain the inner
267"
RELATED WORK,0.364021164021164,"working mechanisms of deep networks. One approach is to interpret individual neurons (Bau et al.,
268"
RELATED WORK,0.36507936507936506,"2017; Dravid et al., 2023) and circuit connections between neurons (Olah et al., 2020). Another ap-
269"
RELATED WORK,0.36613756613756615,"proach is to interpret transformer attention heads (Gandelsman et al., 2024) and circuit connections
270"
RELATED WORK,0.3671957671957672,"between attention heads (Wang et al., 2023a). Other approaches also looked into the role of patch
271"
RELATED WORK,0.3682539682539683,"tokens (Sun et al., 2024). These approaches made the assumption that channels are aligned within
272"
RELATED WORK,0.3693121693121693,"the same model; we compare across models by actively aligning the channels to a universal space.
273"
RELATED WORK,0.37037037037037035,"Spectral Clustering is a graphical method to analyze data grouping in the eigenvector space. Spec-
274"
RELATED WORK,0.37142857142857144,"tral methods have been widely used for unsupervised image segmentation (Shi and Malik, 2000;
275"
RELATED WORK,0.3724867724867725,"von Luxburg, 2007; Wu et al., 2018; Wang et al., 2023b). One major challenge for applying spectral
276"
RELATED WORK,0.37354497354497357,"clustering to large graphs is the complexity scaling issue. To solve the scaling issue, the Nystrom
277"
RELATED WORK,0.3746031746031746,"approximation (Fowlkes et al., 2004) approaches solve eigenvectors on sub-sampled graphs and then
278"
RELATED WORK,0.37566137566137564,"propagate to the full graph. Another approach is the gradient-based eigenvector solver (Zhang et al.,
279"
RELATED WORK,0.37671957671957673,"2023), which solves the eigenvectors in mini-batches. Our proposed Nystrom-like approximation
280"
RELATED WORK,0.37777777777777777,"achieves a computational speedup over the original Nystrom approximation, albeit at the expense of
281"
RELATED WORK,0.37883597883597886,"weakened orthogonality of the eigenvectors.
282"
RELATED WORK,0.3798941798941799,"Brain Encoding Model is widely used by the computational neuroscience community (Kriegeskorte
283"
RELATED WORK,0.38095238095238093,"and Douglas, 2018). They have been using deep nets to explain the brain’s function. One approach
284"
RELATED WORK,0.382010582010582,"is to use the gradient of the brain encoding model to find the most salient image features (Sarch
285"
RELATED WORK,0.38306878306878306,"et al., 2023). Another approach generate text caption for brain activation (Luo et al., 2024). Other
286"
RELATED WORK,0.38412698412698415,"approaches compare brain prediction performance for different models (Schrimpf et al., 2020). The
287"
RELATED WORK,0.3851851851851852,"field focused on using deep nets as a tool to explain the brain’s function; we go in the opposite
288"
RELATED WORK,0.3862433862433862,"direction by using the brain to explain deep nets.
289"
CONCLUSION AND LIMITATIONS,0.3873015873015873,"5
Conclusion and Limitations
290"
CONCLUSION AND LIMITATIONS,0.38835978835978835,"We present a novel approach to interpreting deep neural networks by leveraging brain data. Our
291"
CONCLUSION AND LIMITATIONS,0.38941798941798944,"fundamental innovation is twofold: First, we use brain prediction as guidance to align channels from
292"
CONCLUSION AND LIMITATIONS,0.3904761904761905,"different models into a universal feature space; Second, we developed a Nystrom-like approximation
293"
CONCLUSION AND LIMITATIONS,0.3915343915343915,"to scale up the spectral clustering analysis. Our key discovery is that recurring visual concepts exist
294"
CONCLUSION AND LIMITATIONS,0.3925925925925926,"across networks and layers; such concepts correspond to different levels of objects, ranging from
295"
CONCLUSION AND LIMITATIONS,0.39365079365079364,"figure-ground to categories. Additionally, we quantified the information flow from layer to layer,
296"
CONCLUSION AND LIMITATIONS,0.39470899470899473,"where we found a bifurcation of figure-ground visual concepts.
297"
CONCLUSION AND LIMITATIONS,0.39576719576719577,"Limitations. While the learned channel align transformation projects all features onto a universal
298"
CONCLUSION AND LIMITATIONS,0.3968253968253968,"feature space, the nature of learned transformation does not preserve all the information. There
299"
CONCLUSION AND LIMITATIONS,0.3978835978835979,"is a small drop in unsupervised segmentation performance after channel alignment, which is not
300"
CONCLUSION AND LIMITATIONS,0.39894179894179893,"fully addressed by our proposed eigen-constraint regularization. Secondly, as a trade-off for faster
301"
CONCLUSION AND LIMITATIONS,0.4,"computation, our Nystrom-like approximation does not produce strictly orthogonal eigenvectors. To
302"
CONCLUSION AND LIMITATIONS,0.40105820105820106,"produce expressive eigenvectors, our approximation relies on using larger sub-sample sizes than the
303"
CONCLUSION AND LIMITATIONS,0.4021164021164021,"original Nystrom method.
304"
REFERENCES,0.4031746031746032,"References
305"
REFERENCES,0.4042328042328042,"Allen, E. J., St-Yves, G., Wu, Y., Breedlove, J. L., Prince, J. S., Dowdle, L. T., Nau, M., Caron,
306"
REFERENCES,0.4052910052910053,"B., Pestilli, F., Charest, I., Hutchinson, J. B., Naselaris, T., and Kay, K. (2022). A massive 7T
307"
REFERENCES,0.40634920634920635,"fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nature Neuroscience,
308"
REFERENCES,0.4074074074074074,"25(1):116–126. Number: 1 Publisher: Nature Publishing Group. 3
309"
REFERENCES,0.4084656084656085,"Bau, D., Zhou, B., Khosla, A., Oliva, A., and Torralba, A. (2017). Network Dissection: Quantifying
310"
REFERENCES,0.4095238095238095,"Interpretability of Deep Visual Representations. In Computer Vision and Pattern Recognition. 9
311"
REFERENCES,0.4105820105820106,"Darcet, T., Oquab, M., Mairal, J., and Bojanowski, P. (2024). Vision Transformers Need Registers.
312"
REFERENCES,0.41164021164021164,"In The Twelfth International Conference on Learning Representations. 5
313"
REFERENCES,0.4126984126984127,"Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009).
ImageNet: A large-
314"
REFERENCES,0.41375661375661377,"scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern
315"
REFERENCES,0.4148148148148148,"Recognition, pages 248–255. 5
316"
REFERENCES,0.4158730158730159,"Dravid, A., Gandelsman, Y., Efros, A. A., and Shocher, A. (2023). Rosetta Neurons: Mining the
317"
REFERENCES,0.41693121693121693,"Common Units in a Model Zoo. In Proceedings of the IEEE/CVF International Conference on
318"
REFERENCES,0.41798941798941797,"Computer Vision (ICCV), pages 1934–1943. 1, 9
319"
REFERENCES,0.41904761904761906,"Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., and Zisserman, A. (2010). The Pascal
320"
REFERENCES,0.4201058201058201,"Visual Object Classes (VOC) Challenge. International Journal of Computer Vision, 88(2):303–
321"
REFERENCES,0.4211640211640212,"338. 6
322"
REFERENCES,0.4222222222222222,"Fowlkes, C., Belongie, S., Chung, F., and Malik, J. (2004). Spectral grouping using the Nystrom
323"
REFERENCES,0.42328042328042326,"method. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):214–225. 4, 9
324"
REFERENCES,0.42433862433862435,"Gandelsman, Y., Efros, A. A., and Steinhardt, J. (2024). Interpreting CLIP’s Image Representation
325"
REFERENCES,0.4253968253968254,"via Text-Based Decomposition. In The Twelfth International Conference on Learning Represen-
326"
REFERENCES,0.4264550264550265,"tations. 9
327"
REFERENCES,0.4275132275132275,"Gifford, A. T., Lahner, B., Saba-Sadiya, S., Vilas, M. G., Lascelles, A., Oliva, A., Kay, K., Roig, G.,
328"
REFERENCES,0.42857142857142855,"and Cichy, R. M. (2023). The Algonauts Project 2023 Challenge: How the Human Brain Makes
329"
REFERENCES,0.42962962962962964,"Sense of Natural Scenes. arXiv:2301.03198 [cs, q-bio]. 3
330"
REFERENCES,0.4306878306878307,"Guillaumin, M., K¨uttel, D., and Ferrari, V. (2014). ImageNet Auto-Annotation with Segmentation
331"
REFERENCES,0.43174603174603177,"Propagation. International Journal of Computer Vision, 110(3):328–348. 6
332"
REFERENCES,0.4328042328042328,"He, K., Chen, X., Xie, S., Li, Y., Doll´ar, P., and Girshick, R. (2022). Masked Autoencoders Are
333"
REFERENCES,0.43386243386243384,"Scalable Vision Learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and
334"
REFERENCES,0.43492063492063493,"Pattern Recognition (CVPR), pages 16000–16009. 5
335"
REFERENCES,0.43597883597883597,"Kriegeskorte, N. and Douglas, P. K. (2018). Cognitive computational neuroscience. Nature Neuro-
336"
REFERENCES,0.43703703703703706,"science, 21(9):1148–1160. Publisher: Nature Publishing Group. 9
337"
REFERENCES,0.4380952380952381,"Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Dollar, P. (2017). Focal Loss for Dense Object
338"
REFERENCES,0.43915343915343913,"Detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV). 14
339"
REFERENCES,0.4402116402116402,"Luo, A., Henderson, M. M., Tarr, M. J., and Wehbe, L. (2024). BrainSCUBA: Fine-Grained Natural
340"
REFERENCES,0.44126984126984126,"Language Captions of Visual Cortex Selectivity. In The Twelfth International Conference on
341"
REFERENCES,0.44232804232804235,"Learning Representations. 9
342"
REFERENCES,0.4433862433862434,"Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. (2020). Zoom In: An
343"
REFERENCES,0.4444444444444444,"Introduction to Circuits. Distill, 5(3):e00024.001. 9
344"
REFERENCES,0.4455026455026455,"Prince, J. S., Charest, I., Kurzawski, J. W., Pyles, J. A., Tarr, M. J., and Kay, K. N. (2022). Im-
345"
REFERENCES,0.44656084656084655,"proving the accuracy of single-trial fMRI response estimates using GLMsingle. eLife, 11:e77599.
346"
REFERENCES,0.44761904761904764,"Publisher: eLife Sciences Publications, Ltd. 3
347"
REFERENCES,0.4486772486772487,"Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,
348"
REFERENCES,0.4497354497354497,"Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. (2021). Learning Transferable Visual Models
349"
REFERENCES,0.4507936507936508,"From Natural Language Supervision. In Meila, M. and Zhang, T., editors, Proceedings of the 38th
350"
REFERENCES,0.45185185185185184,"International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning
351"
REFERENCES,0.45291005291005293,"Research, pages 8748–8763. PMLR. 5
352"
REFERENCES,0.45396825396825397,"Sarch, G. H., Tarr, M. J., Fragkiadaki, K., and Wehbe, L. (2023). Brain Dissection: fMRI-trained
353"
REFERENCES,0.455026455026455,"Networks Reveal Spatial Selectivity in the Processing of Natural Images. In Thirty-seventh Con-
354"
REFERENCES,0.4560846560846561,"ference on Neural Information Processing Systems. 9
355"
REFERENCES,0.45714285714285713,"Schrimpf, M., Kubilius, J., Lee, M. J., Murty, N. A. R., Ajemian, R., and DiCarlo, J. J. (2020). Inte-
356"
REFERENCES,0.4582010582010582,"grative Benchmarking to Advance Neurally Mechanistic Models of Human Intelligence. Neuron.
357 9
358"
REFERENCES,0.45925925925925926,"Shi, J. and Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on
359"
REFERENCES,0.4603174603174603,"Pattern Analysis and Machine Intelligence, 22(8):888–905. 4, 9
360"
REFERENCES,0.4613756613756614,"Sun, M., Chen, X., Kolter, J. Z., and Liu, Z. (2024). Massive Activations in Large Language Models.
361"
REFERENCES,0.4624338624338624,"arXiv:2402.17762 [cs]. 9
362"
REFERENCES,0.4634920634920635,"von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and Computing, 17(4):395–416.
363 9
364"
REFERENCES,0.46455026455026455,"Wang, K. R., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. (2023a). Interpretabil-
365"
REFERENCES,0.4656084656084656,"ity in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small. In The Eleventh
366"
REFERENCES,0.4666666666666667,"International Conference on Learning Representations. 9
367"
REFERENCES,0.4677248677248677,"Wang, X., Girdhar, R., Yu, S. X., and Misra, I. (2023b). Cut and learn for unsupervised object
368"
REFERENCES,0.4687830687830688,"detection and instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer
369"
REFERENCES,0.46984126984126984,"Vision and Pattern Recognition, pages 3124–3134. 9
370"
REFERENCES,0.4708994708994709,"Wu, Z., Xiong, Y., Stella, X. Y., and Lin, D. (2018). Unsupervised Feature Learning via Non-
371"
REFERENCES,0.47195767195767196,"Parametric Instance Discrimination. In Proceedings of the IEEE Conference on Computer Vision
372"
REFERENCES,0.473015873015873,"and Pattern Recognition. 9
373"
REFERENCES,0.4740740740740741,"Yang, H., Gee, J., and Shi, J. (2024). Brain Decodes Deep Nets. arXiv:2312.01280 [cs]. 1
374"
REFERENCES,0.47513227513227513,"Zhang, X., Yunis, D., and Maire, M. (2023). Deciphering ’What’ and ’Where’ Visual Pathways
375"
REFERENCES,0.47619047619047616,"from Spectral Clustering of Layer-Distributed Neural Representations. arXiv:2312.06716 [cs]. 9
376"
REFERENCES,0.47724867724867726,"A
Appendix overview
377"
REFERENCES,0.4783068783068783,"1. Appendix B summarizes background of brain ROIs.
378"
APPENDIX C IS IMPLEMENTATION DETAILS,0.4793650793650794,"2. Appendix C is implementation details
379"
ADDITIONAL REGULARIZATION TERMS,0.4804232804232804,"2.1. Additional regularization terms
380"
BRAIN ENCODING MODEL TRAINING LOSS FUNCTION,0.48148148148148145,"2.2. Brain encoding model training loss function
381"
UNSUPERVISED SEGMENTATION EVALUATION PIPELINE,0.48253968253968255,"2.3. Unsupervised segmentation evaluation pipeline
382"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.4835978835978836,"2.4. Nystrom-like approximation for t-SNE
383"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.4846560846560847,"3. Appendix D lists more image examples from the 3D spectral-tSNE.
384"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.4857142857142857,"4. Appendix E lists figure-ground channel activation for every model and layer.
385"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.48677248677248675,"5. Appendix F lists more example category-specific visual concepts.
386"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.48783068783068784,"6. Appendix G lists more example pixels from the 2D spectral-tSNE information flow.
387"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.4888888888888889,"B
Brain Region Background Knowledge
388"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.48994708994708996,"Figure 11: Brain Region of Interests (ROIs). V1v: ventral stream, V1d: dorsal stream."
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.491005291005291,Table 2: Known function and selectivity of brain region of interests (ROIs).
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.49206349206349204,"ROI name
V1 V2 V3
V4
EBA FBA
OFA FFA
OPA
PPA
OWFA VWFA
Known Function/Selectivity
primary visual
mid-level
body
face
navigation
scene
words"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.4931216931216931,"This section briefly summarizes the known functions of key brain regions of interest (ROIs). Fig-
389"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.49417989417989416,"ure 11 provides an overview of these brain ROIs. Table 2 lists the known functions and selectivities
390"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.49523809523809526,"for each ROI.
391"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.4962962962962963,"In brief, V1 to V3 are the primary visual stream, which is further divided into ventral (lower) and dor-
392"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.4973544973544973,"sal (upper) streams. V4 is a mid-level visual area. EBA (extrastriate body area) and FBA (fusiform
393"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.4984126984126984,"body area) are selectively responsive to bodies, while FFA (fusiform face area) and OFA (occipital
394"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.49947089947089945,"face area) show selectivity for faces. OWFA (occipital word form area) and VWFA (visual word
395"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5005291005291005,"form area) are selective for written words. PPA (parahippocampal place area) exhibits selectivity for
396"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5015873015873016,"scenes and places, and OPA (occipital place area) is involved in navigation and spatial reasoning.
397"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5026455026455027,"Visual information processing in the brain follows a hierarchical, feedforward organization. Be-
398"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5037037037037037,"ginning in the primary visual cortex (V1) and progressing through higher visual areas like V2, V3,
399"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5047619047619047,"and V4, neurons exhibit increasingly large receptive fields and represent increasingly abstract visual
400"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5058201058201058,"concepts. While neurons in V1 encode low-level features like edges and orientations within a small
401"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5068783068783069,"portion of the visual field, neurons in V4 synthesize more complex patterns and object representa-
402"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5079365079365079,"tions across a larger area of the visual input.
403"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.508994708994709,"C
Implementation Details
404"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5100529100529101,"C.1
Additional Regularization for Channel Align Transformation
405"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5111111111111111,"Additional Regularization are added to the channel align transform to ensure good properties of the
406"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5121693121693122,"aligned features: 1) zero-centered, 2) small covariance between channels, and 3) focal loss.
407"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5132275132275133,"Zero-centered regularization. We did not apply z-score normalization to the extracted features;
408"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5142857142857142,"instead, we added a regularization term to ensure the transformed features are zero-centered. Recall
409"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5153439153439153,"that the channel-aligned transformed feature V ′ ∈RM×D′, where M is the number of data points
410"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5164021164021164,"and D′ is the hidden dimension. The zero-center loss is defined as:
411"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5174603174603175,Lzero = 1
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5185185185185185,"D′
1
M X"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5195767195767196,"i≤M,j≤D′
v′
ij
(6)"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5206349206349207,"Covariance regularization. We used the covariance loss to minimize the off-diagonal elements in
412"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5216931216931217,"the covariance matrix of the transformed feature C(V ′), aiming to bring them close to 0. Recall
413"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5227513227513227,"that channel align transformed feature V ′ ∈RM×D′, where M is number of data, D′ is the hidden
414"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5238095238095238,"dimension. The covariance loss is defined as:
415"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5248677248677248,"Lcov = 1 D′
X"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5259259259259259,"i̸=j
[C(V ′)]2
i,j, where C(V ′) =
1
M −1 M
X i=1"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.526984126984127," 
v′
i −¯v′  
v′
i −¯v′T , ¯v′ = 1 M M
X"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5280423280423281,"i=1
v′
i."
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5291005291005291,"(7)
Focal Loss. Lin et al. (2017) introduced focal loss, which dynamically assigns smaller weights to
416"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5301587301587302,"the loss function for hard-to-classify classes. In our scenario, we apply spectral clustering on the
417"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5312169312169313,"affinity matrix Aa ∈RM×M after performing the channel alignment transform, where M represents
418"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5322751322751322,"the number of data points. Due to the characteristics of spectral clustering, disconnected edges
419"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5333333333333333,"play a more critical role than connected edges. Adding an edge between disconnected clusters
420"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5343915343915344,"significantly reshapes the eigenvectors, while adding edges to connected clusters has only a minor
421"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5354497354497354,"impact. Therefore, we aim to assign larger weights to disconnected edges in the loss function:
422"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5365079365079365,"Leigen = ∥(XbXT
b −XaXT
a ) ∗exp(−Ab)∥
(8)"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5375661375661376,"where Ab ∈RM×M is the affinity matrix before the channel alignment transform, element wise dot-
423"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5386243386243387,"product to exp(−Ab) assigned larger wights for disconnected edges. Xb ∈RM×C, Xa ∈RM×C
424"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5396825396825397,"are eigenvectors before and after channel align transform, respectively.
425"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5407407407407407,"C.2
Brain Encoding Model Training Loss
426"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5417989417989418,"Let Y ∈R1×N represent the brain prediction target, where N is the number of flattened 3D brain
427"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5428571428571428,"voxels, and the 1 indicates that each voxel’s response is a scalar value. ˆY is the model’s predicted
428"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5439153439153439,"brain response. The brain encoding model training loss is the L1 loss:
429"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.544973544973545,"Lbrain = ∥Y −ˆY ∥
(9)"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.546031746031746,"C.3
Total Training Loss
430"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5470899470899471,"The total training loss is a combination of the following components: 1) brain encoding model loss,
431"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5481481481481482,"2) eigen-constraint regularization, 3) zero-centered regularization, and 4) covariance regularization:
432"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5492063492063493,"L = Lbrain + λeigenLeigen + λzeroLzero + λcovLcov
(10)"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5502645502645502,"where we set λeigen = 1, λzero = 0.01, λcov = 0.01.
433"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5513227513227513,"C.4
Oracle-based Unsupervised Segmentation Evaluation Pipeline
434"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5523809523809524,"Our unsupervised segmentation pipeline aims to benchmark and compare the performance across
435"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5534391534391534,"each single layer of the CLIP model. The evaluation pipeline is oracle-based:
436"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5544973544973545,"1. Apply spectral clustering jointly across all images, taking the top 10 eigenvectors.
437"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5555555555555556,"2. For each class of object (plus one background class), use ground-truth labels from the dataset
438"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5566137566137566,"to mask out the pixels and their eigenvectors, and then use the mean of the eigenvectors to define a
439"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5576719576719577,"center for each class.
440"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5587301587301587,"3. Compute the cosine similarity of each pixel to all class centers.
441"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5597883597883598,"4. For each pixel, if the maximum similarity to all classes is less than a threshold value, assign
442"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5608465608465608,"this pixel to the background class.
443"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5619047619047619,"5. Assign pixels (with a similarity greater than the threshold value) to the class with the maximum
444"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.562962962962963,"similarity.
445"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.564021164021164,"There’s one hyper-parameter, the threshold value that requires different optimal value for each layer
446"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5650793650793651,"of CLIP. To ensure a fair comparison across all layers, the threshold value is grid-searched from 10
447"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5661375661375662,"evenly spaced values between 0 and 1, the maximum mIoU score in the grid search is taken for each
448"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5671957671957671,"layer.
449"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5682539682539682,"C.5
Nystrom-like approximation for t-SNE
450"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5693121693121693,"To visualize the eigenvectors, we applied t-SNE to the eigenvectors X ∈RM×C, where the number
451"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5703703703703704,"of data points M span the product space of models, layers, pixels, and images. Due to the enormous
452"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5714285714285714,"size of M = 7e+6 nodes, t-SNE suffered from complexity scaling issues. We again applied our
453"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5724867724867725,"Nystrom-like approximation to t-SNE, with sub-sampled m = 10e+4 nodes and KNN K = 1.
454"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5735449735449736,"It’s worth noting that, since the non-linear distance adjustment in t-SNE, it’s crucial to use only one
455"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5746031746031746,"nearest neighbor K = 1 for t-SNE.
456"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5756613756613757,"C.6
Computation Resource
457"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5767195767195767,"All of our experiments are performed on one consumer-grade RTX 4090 GPU. The brain encoding
458"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5777777777777777,"model training took 3 hours on 4GB of VRAM, spectral clustering eigen-decomposition on large
459"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5788359788359788,"graph took 10 minutes on 10GB of VRAM and 60GB of CPU RAM.
460"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.5798941798941799,"C.7
Code Release
461"
NYSTROM-LIKE APPROXIMATION FOR T-SNE,0.580952380952381,"Our code will be publicly released upon publication.
462"
D SPECTRAL-TSNE,0.582010582010582,3D Spectral-tSNE
D SPECTRAL-TSNE,0.5830687830687831,"D
3D spectral-tSNE
463 CLIP"
D SPECTRAL-TSNE,0.5841269841269842,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.5851851851851851,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.5862433862433862,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.5873015873015873,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.5883597883597883,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.5894179894179894,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.5904761904761905,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.5915343915343916,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.5925925925925926,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.5936507936507937,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.5947089947089947,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.5957671957671957,"DINO
MAE"
D SPECTRAL-TSNE,0.5968253968253968,"Figure 12: Spectral clustering in the universal channel aligned feature space. The image pixels are
colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of
the top 20 eigenvectors. The coloring is consistent across all images, layers, and models."
D SPECTRAL-TSNE,0.5978835978835979,3D Spectral-tSNE CLIP
D SPECTRAL-TSNE,0.5989417989417989,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6010582010582011,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6021164021164022,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6031746031746031,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6042328042328042,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6052910052910053,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6063492063492063,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6074074074074074,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6084656084656085,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6095238095238096,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6105820105820106,"DINO
MAE"
D SPECTRAL-TSNE,0.6116402116402117,"Figure 13: Spectral clustering in the universal channel aligned feature space. The image pixels are
colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of
the top 20 eigenvectors. The coloring is consistent across all images, layers, and models."
D SPECTRAL-TSNE,0.6126984126984127,3D Spectral-tSNE CLIP
D SPECTRAL-TSNE,0.6137566137566137,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6148148148148148,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6158730158730159,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6169312169312169,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.617989417989418,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6190476190476191,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6201058201058202,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6211640211640211,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6222222222222222,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6232804232804233,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6243386243386243,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6253968253968254,"DINO
MAE"
D SPECTRAL-TSNE,0.6264550264550265,"Figure 14: Spectral clustering in the universal channel aligned feature space. The image pixels are
colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of
the top 20 eigenvectors. The coloring is consistent across all images, layers, and models."
D SPECTRAL-TSNE,0.6275132275132275,3D Spectral-tSNE CLIP
D SPECTRAL-TSNE,0.6285714285714286,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6296296296296297,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6306878306878307,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6317460317460317,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6328042328042328,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6338624338624339,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6349206349206349,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.635978835978836,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6370370370370371,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.638095238095238,"DINO
MAE
CLIP"
D SPECTRAL-TSNE,0.6391534391534391,"Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11"
D SPECTRAL-TSNE,0.6402116402116402,"DINO
MAE"
D SPECTRAL-TSNE,0.6412698412698413,"Figure 15: Spectral clustering in the universal channel aligned feature space. The image pixels are
colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of
the top 20 eigenvectors. The coloring is consistent across all images, layers, and models."
D SPECTRAL-TSNE,0.6423280423280423,Figure-ground Channel Activation
D SPECTRAL-TSNE,0.6433862433862434,"E
Figure-ground Channel Activation from All Layers and Models
464"
D SPECTRAL-TSNE,0.6444444444444445,foreground
D SPECTRAL-TSNE,0.6455026455026455,"CLIP L0
CLIP L1
CLIP L2
CLIP L3
CLIP L4
CLIP L5
CLIP L6
CLIP L7
CLIP L8
CLIP L9
CLIP L10
CLIP L11"
D SPECTRAL-TSNE,0.6465608465608466,foreground
D SPECTRAL-TSNE,0.6476190476190476,"DINO L0
DINO L1
DINO L2
DINO L3
DINO L4
DINO L5
DINO L6
DINO L7
DINO L8
DINO L9
DINO L10
DINO L11"
D SPECTRAL-TSNE,0.6486772486772486,foreground
D SPECTRAL-TSNE,0.6497354497354497,"MAE L0
MAE L1
MAE L2
MAE L3
MAE L4
MAE L5
MAE L6
MAE L7
MAE L8
MAE L9
MAE L10
MAE L11"
D SPECTRAL-TSNE,0.6507936507936508,background
D SPECTRAL-TSNE,0.6518518518518519,"CLIP L0
CLIP L1
CLIP L2
CLIP L3
CLIP L4
CLIP L5
CLIP L6
CLIP L7
CLIP L8
CLIP L9
CLIP L10
CLIP L11"
D SPECTRAL-TSNE,0.6529100529100529,background
D SPECTRAL-TSNE,0.653968253968254,"DINO L0
DINO L1
DINO L2
DINO L3
DINO L4
DINO L5
DINO L6
DINO L7
DINO L8
DINO L9
DINO L10
DINO L11"
D SPECTRAL-TSNE,0.6550264550264551,background
D SPECTRAL-TSNE,0.656084656084656,"MAE L0
MAE L1
MAE L2
MAE L3
MAE L4
MAE L5
MAE L6
MAE L7
MAE L8
MAE L9
MAE L10
MAE L11"
D SPECTRAL-TSNE,0.6571428571428571,"Figure 16: Mean activation of foreground or background pixels at each layer of CLIP, DINO and
MAE. Channel activations are linearly transformed to the brain’s space. Large absolute activation
value means more consistent visual concepts."
D SPECTRAL-TSNE,0.6582010582010582,Visual Concepts: Category-specific
D SPECTRAL-TSNE,0.6592592592592592,"F
Visual Concepts: Categories
465"
D SPECTRAL-TSNE,0.6603174603174603,"Figure 17: Category visual concepts in CLIP Layer 9. Left: Mean activation of all pixels within an
Euclidean sphere centered at the visual concept in the 3D spectral-tSNE space; the concepts activate
different brain regions. Middle: The standard deviation negatively correlates with absolute mean
activations. Right: Spectral clustering, colored by 3D spectral-tSNE of the top 20 eigenvectors."
D SPECTRAL-TSNE SPACE INFORMATION FLOW,0.6613756613756614,2D Spectral-tSNE Space Information Flow
D SPECTRAL-TSNE SPACE INFORMATION FLOW,0.6624338624338625,"G
Layer-to-Layer Feature Computation Flow in 2D spectral-tSNE space
466"
D SPECTRAL-TSNE SPACE INFORMATION FLOW,0.6634920634920635,"Figure 18: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space.
Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsu-
pervised segmentation."
D SPECTRAL-TSNE SPACE INFORMATION FLOW,0.6645502645502646,2D Spectral-tSNE Space Information Flow
D SPECTRAL-TSNE SPACE INFORMATION FLOW,0.6656084656084656,"Figure 19: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space.
Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsu-
pervised segmentation."
D SPECTRAL-TSNE SPACE INFORMATION FLOW,0.6666666666666666,2D Spectral-tSNE Space Information Flow
D SPECTRAL-TSNE SPACE INFORMATION FLOW,0.6677248677248677,"Figure 20: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space.
Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsu-
pervised segmentation."
D SPECTRAL-TSNE SPACE INFORMATION FLOW,0.6687830687830688,2D Spectral-tSNE Space Information Flow
D SPECTRAL-TSNE SPACE INFORMATION FLOW,0.6698412698412698,"Figure 21: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space.
Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsu-
pervised segmentation."
D SPECTRAL-TSNE SPACE INFORMATION FLOW,0.6708994708994709,2D Spectral-tSNE Space Information Flow
D SPECTRAL-TSNE SPACE INFORMATION FLOW,0.671957671957672,"Figure 22: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space.
Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsu-
pervised segmentation."
D SPECTRAL-TSNE SPACE INFORMATION FLOW,0.6730158730158731,"NeurIPS Paper Checklist
467"
CLAIMS,0.674074074074074,"1. Claims
468"
CLAIMS,0.6751322751322751,"Question: Do the main claims made in the abstract and introduction accurately reflect the
469"
CLAIMS,0.6761904761904762,"paper’s contributions and scope?
470"
CLAIMS,0.6772486772486772,"Answer: [Yes]
471"
CLAIMS,0.6783068783068783,"Justification:
472"
CLAIMS,0.6793650793650794,"Guidelines:
473"
CLAIMS,0.6804232804232804,"• The answer NA means that the abstract and introduction do not include the claims
474"
CLAIMS,0.6814814814814815,"made in the paper.
475"
CLAIMS,0.6825396825396826,"• The abstract and/or introduction should clearly state the claims made, including the
476"
CLAIMS,0.6835978835978836,"contributions made in the paper and important assumptions and limitations. A No or
477"
CLAIMS,0.6846560846560846,"NA answer to this question will not be perceived well by the reviewers.
478"
CLAIMS,0.6857142857142857,"• The claims made should match theoretical and experimental results, and reflect how
479"
CLAIMS,0.6867724867724868,"much the results can be expected to generalize to other settings.
480"
CLAIMS,0.6878306878306878,"• It is fine to include aspirational goals as motivation as long as it is clear that these
481"
CLAIMS,0.6888888888888889,"goals are not attained by the paper.
482"
LIMITATIONS,0.68994708994709,"2. Limitations
483"
LIMITATIONS,0.691005291005291,"Question: Does the paper discuss the limitations of the work performed by the authors?
484"
LIMITATIONS,0.692063492063492,"Answer: [Yes]
485"
LIMITATIONS,0.6931216931216931,"Justification:
486"
LIMITATIONS,0.6941798941798942,"Guidelines:
487"
LIMITATIONS,0.6952380952380952,"• The answer NA means that the paper has no limitation while the answer No means
488"
LIMITATIONS,0.6962962962962963,"that the paper has limitations, but those are not discussed in the paper.
489"
LIMITATIONS,0.6973544973544974,"• The authors are encouraged to create a separate ”Limitations” section in their paper.
490"
LIMITATIONS,0.6984126984126984,"• The paper should point out any strong assumptions and how robust the results are to
491"
LIMITATIONS,0.6994708994708995,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
492"
LIMITATIONS,0.7005291005291006,"model well-specification, asymptotic approximations only holding locally). The au-
493"
LIMITATIONS,0.7015873015873015,"thors should reflect on how these assumptions might be violated in practice and what
494"
LIMITATIONS,0.7026455026455026,"the implications would be.
495"
LIMITATIONS,0.7037037037037037,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
496"
LIMITATIONS,0.7047619047619048,"only tested on a few datasets or with a few runs. In general, empirical results often
497"
LIMITATIONS,0.7058201058201058,"depend on implicit assumptions, which should be articulated.
498"
LIMITATIONS,0.7068783068783069,"• The authors should reflect on the factors that influence the performance of the ap-
499"
LIMITATIONS,0.707936507936508,"proach. For example, a facial recognition algorithm may perform poorly when image
500"
LIMITATIONS,0.708994708994709,"resolution is low or images are taken in low lighting. Or a speech-to-text system might
501"
LIMITATIONS,0.71005291005291,"not be used reliably to provide closed captions for online lectures because it fails to
502"
LIMITATIONS,0.7111111111111111,"handle technical jargon.
503"
LIMITATIONS,0.7121693121693121,"• The authors should discuss the computational efficiency of the proposed algorithms
504"
LIMITATIONS,0.7132275132275132,"and how they scale with dataset size.
505"
LIMITATIONS,0.7142857142857143,"• If applicable, the authors should discuss possible limitations of their approach to ad-
506"
LIMITATIONS,0.7153439153439154,"dress problems of privacy and fairness.
507"
LIMITATIONS,0.7164021164021164,"• While the authors might fear that complete honesty about limitations might be used by
508"
LIMITATIONS,0.7174603174603175,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
509"
LIMITATIONS,0.7185185185185186,"limitations that aren’t acknowledged in the paper. The authors should use their best
510"
LIMITATIONS,0.7195767195767195,"judgment and recognize that individual actions in favor of transparency play an impor-
511"
LIMITATIONS,0.7206349206349206,"tant role in developing norms that preserve the integrity of the community. Reviewers
512"
LIMITATIONS,0.7216931216931217,"will be specifically instructed to not penalize honesty concerning limitations.
513"
THEORY ASSUMPTIONS AND PROOFS,0.7227513227513227,"3. Theory Assumptions and Proofs
514"
THEORY ASSUMPTIONS AND PROOFS,0.7238095238095238,"Question: For each theoretical result, does the paper provide the full set of assumptions and
515"
THEORY ASSUMPTIONS AND PROOFS,0.7248677248677249,"a complete (and correct) proof?
516"
THEORY ASSUMPTIONS AND PROOFS,0.725925925925926,"Answer: [NA]
517"
THEORY ASSUMPTIONS AND PROOFS,0.726984126984127,"Justification:
518"
THEORY ASSUMPTIONS AND PROOFS,0.728042328042328,"Guidelines:
519"
THEORY ASSUMPTIONS AND PROOFS,0.7291005291005291,"• The answer NA means that the paper does not include theoretical results.
520"
THEORY ASSUMPTIONS AND PROOFS,0.7301587301587301,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
521"
THEORY ASSUMPTIONS AND PROOFS,0.7312169312169312,"referenced.
522"
THEORY ASSUMPTIONS AND PROOFS,0.7322751322751323,"• All assumptions should be clearly stated or referenced in the statement of any theo-
523"
THEORY ASSUMPTIONS AND PROOFS,0.7333333333333333,"rems.
524"
THEORY ASSUMPTIONS AND PROOFS,0.7343915343915344,"• The proofs can either appear in the main paper or the supplemental material, but if
525"
THEORY ASSUMPTIONS AND PROOFS,0.7354497354497355,"they appear in the supplemental material, the authors are encouraged to provide a
526"
THEORY ASSUMPTIONS AND PROOFS,0.7365079365079366,"short proof sketch to provide intuition.
527"
THEORY ASSUMPTIONS AND PROOFS,0.7375661375661375,"• Inversely, any informal proof provided in the core of the paper should be comple-
528"
THEORY ASSUMPTIONS AND PROOFS,0.7386243386243386,"mented by formal proofs provided in appendix or supplemental material.
529"
THEORY ASSUMPTIONS AND PROOFS,0.7396825396825397,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
530"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7407407407407407,"4. Experimental Result Reproducibility
531"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7417989417989418,"Question: Does the paper fully disclose all the information needed to reproduce the main
532"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7428571428571429,"experimental results of the paper to the extent that it affects the main claims and/or conclu-
533"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7439153439153439,"sions of the paper (regardless of whether the code and data are provided or not)?
534"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.744973544973545,"Answer: [Yes]
535"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.746031746031746,"Justification: Experimental details are in the appendix
536"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7470899470899471,"Guidelines:
537"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7481481481481481,"• The answer NA means that the paper does not include experiments.
538"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7492063492063492,"• If the paper includes experiments, a No answer to this question will not be perceived
539"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7502645502645503,"well by the reviewers: Making the paper reproducible is important, regardless of
540"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7513227513227513,"whether the code and data are provided or not.
541"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7523809523809524,"• If the contribution is a dataset and/or model, the authors should describe the steps
542"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7534391534391535,"taken to make their results reproducible or verifiable.
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7544973544973544,"• Depending on the contribution, reproducibility can be accomplished in various ways.
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7555555555555555,"For example, if the contribution is a novel architecture, describing the architecture
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7566137566137566,"fully might suffice, or if the contribution is a specific model and empirical evaluation,
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7576719576719577,"it may be necessary to either make it possible for others to replicate the model with
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7587301587301587,"the same dataset, or provide access to the model. In general. releasing code and data
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7597883597883598,"is often one good way to accomplish this, but reproducibility can also be provided via
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7608465608465609,"detailed instructions for how to replicate the results, access to a hosted model (e.g., in
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7619047619047619,"the case of a large language model), releasing of a model checkpoint, or other means
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.762962962962963,"that are appropriate to the research performed.
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.764021164021164,"• While NeurIPS does not require releasing code, the conference does require all sub-
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.765079365079365,"missions to provide some reasonable avenue for reproducibility, which may depend
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7661375661375661,"on the nature of the contribution. For example
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7671957671957672,"(a) If the contribution is primarily a new algorithm, the paper should make it clear
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7682539682539683,"how to reproduce that algorithm.
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7693121693121693,"(b) If the contribution is primarily a new model architecture, the paper should describe
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7703703703703704,"the architecture clearly and fully.
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7714285714285715,"(c) If the contribution is a new model (e.g., a large language model), then there should
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7724867724867724,"either be a way to access this model for reproducing the results or a way to re-
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7735449735449735,"produce the model (e.g., with an open-source dataset or instructions for how to
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7746031746031746,"construct the dataset).
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7756613756613756,"(d) We recognize that reproducibility may be tricky in some cases, in which case au-
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7767195767195767,"thors are welcome to describe the particular way they provide for reproducibility.
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7777777777777778,"In the case of closed-source models, it may be that access to the model is limited in
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7788359788359789,"some way (e.g., to registered users), but it should be possible for other researchers
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7798941798941799,"to have some path to reproducing or verifying the results.
568"
OPEN ACCESS TO DATA AND CODE,0.780952380952381,"5. Open access to data and code
569"
OPEN ACCESS TO DATA AND CODE,0.782010582010582,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
570"
OPEN ACCESS TO DATA AND CODE,0.783068783068783,"tions to faithfully reproduce the main experimental results, as described in supplemental
571"
OPEN ACCESS TO DATA AND CODE,0.7841269841269841,"material?
572"
OPEN ACCESS TO DATA AND CODE,0.7851851851851852,"Answer: [No]
573"
OPEN ACCESS TO DATA AND CODE,0.7862433862433862,"Justification: The data is provided as open-source from another study; our code is not yet
574"
OPEN ACCESS TO DATA AND CODE,0.7873015873015873,"released, it will be released upon publication.
575"
OPEN ACCESS TO DATA AND CODE,0.7883597883597884,"Guidelines:
576"
OPEN ACCESS TO DATA AND CODE,0.7894179894179895,"• The answer NA means that paper does not include experiments requiring code.
577"
OPEN ACCESS TO DATA AND CODE,0.7904761904761904,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
578"
OPEN ACCESS TO DATA AND CODE,0.7915343915343915,"public/guides/CodeSubmissionPolicy) for more details.
579"
OPEN ACCESS TO DATA AND CODE,0.7925925925925926,"• While we encourage the release of code and data, we understand that this might not
580"
OPEN ACCESS TO DATA AND CODE,0.7936507936507936,"be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
581"
OPEN ACCESS TO DATA AND CODE,0.7947089947089947,"including code, unless this is central to the contribution (e.g., for a new open-source
582"
OPEN ACCESS TO DATA AND CODE,0.7957671957671958,"benchmark).
583"
OPEN ACCESS TO DATA AND CODE,0.7968253968253968,"• The instructions should contain the exact command and environment needed to run to
584"
OPEN ACCESS TO DATA AND CODE,0.7978835978835979,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
585"
OPEN ACCESS TO DATA AND CODE,0.798941798941799,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
586"
OPEN ACCESS TO DATA AND CODE,0.8,"• The authors should provide instructions on data access and preparation, including how
587"
OPEN ACCESS TO DATA AND CODE,0.801058201058201,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
588"
OPEN ACCESS TO DATA AND CODE,0.8021164021164021,"• The authors should provide scripts to reproduce all experimental results for the new
589"
OPEN ACCESS TO DATA AND CODE,0.8031746031746032,"proposed method and baselines. If only a subset of experiments are reproducible, they
590"
OPEN ACCESS TO DATA AND CODE,0.8042328042328042,"should state which ones are omitted from the script and why.
591"
OPEN ACCESS TO DATA AND CODE,0.8052910052910053,"• At submission time, to preserve anonymity, the authors should release anonymized
592"
OPEN ACCESS TO DATA AND CODE,0.8063492063492064,"versions (if applicable).
593"
OPEN ACCESS TO DATA AND CODE,0.8074074074074075,"• Providing as much information as possible in supplemental material (appended to the
594"
OPEN ACCESS TO DATA AND CODE,0.8084656084656084,"paper) is recommended, but including URLs to data and code is permitted.
595"
OPEN ACCESS TO DATA AND CODE,0.8095238095238095,"6. Experimental Setting/Details
596"
OPEN ACCESS TO DATA AND CODE,0.8105820105820106,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
597"
OPEN ACCESS TO DATA AND CODE,0.8116402116402116,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
598"
OPEN ACCESS TO DATA AND CODE,0.8126984126984127,"results?
599"
OPEN ACCESS TO DATA AND CODE,0.8137566137566138,"Answer: [Yes]
600"
OPEN ACCESS TO DATA AND CODE,0.8148148148148148,"Justification: Experimental details are in the appendix
601"
OPEN ACCESS TO DATA AND CODE,0.8158730158730159,"Guidelines:
602"
OPEN ACCESS TO DATA AND CODE,0.816931216931217,"• The answer NA means that the paper does not include experiments.
603"
OPEN ACCESS TO DATA AND CODE,0.817989417989418,"• The experimental setting should be presented in the core of the paper to a level of
604"
OPEN ACCESS TO DATA AND CODE,0.819047619047619,"detail that is necessary to appreciate the results and make sense of them.
605"
OPEN ACCESS TO DATA AND CODE,0.8201058201058201,"• The full details can be provided either with the code, in appendix, or as supplemental
606"
OPEN ACCESS TO DATA AND CODE,0.8211640211640212,"material.
607"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8222222222222222,"7. Experiment Statistical Significance
608"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8232804232804233,"Question: Does the paper report error bars suitably and correctly defined or other appropri-
609"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8243386243386244,"ate information about the statistical significance of the experiments?
610"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8253968253968254,"Answer: [Yes]
611"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8264550264550264,"Justification: We provided standard deviation in Table 1, measured over training with 3
612"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8275132275132275,"random seed.
613"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8285714285714286,"Guidelines:
614"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8296296296296296,"• The answer NA means that the paper does not include experiments.
615"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8306878306878307,"• The authors should answer ”Yes” if the results are accompanied by error bars, confi-
616"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8317460317460318,"dence intervals, or statistical significance tests, at least for the experiments that support
617"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8328042328042328,"the main claims of the paper.
618"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8338624338624339,"• The factors of variability that the error bars are capturing should be clearly stated (for
619"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.834920634920635,"example, train/test split, initialization, random drawing of some parameter, or overall
620"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8359788359788359,"run with given experimental conditions).
621"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.837037037037037,"• The method for calculating the error bars should be explained (closed form formula,
622"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8380952380952381,"call to a library function, bootstrap, etc.)
623"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8391534391534392,"• The assumptions made should be given (e.g., Normally distributed errors).
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8402116402116402,"• It should be clear whether the error bar is the standard deviation or the standard error
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8412698412698413,"of the mean.
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8423280423280424,"• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8433862433862434,"ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8444444444444444,"Normality of errors is not verified.
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8455026455026455,"• For asymmetric distributions, the authors should be careful not to show in tables or
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8465608465608465,"figures symmetric error bars that would yield results that are out of range (e.g. negative
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8476190476190476,"error rates).
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8486772486772487,"• If error bars are reported in tables or plots, The authors should explain in the text how
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8497354497354498,"they were calculated and reference the corresponding figures or tables in the text.
634"
EXPERIMENTS COMPUTE RESOURCES,0.8507936507936508,"8. Experiments Compute Resources
635"
EXPERIMENTS COMPUTE RESOURCES,0.8518518518518519,"Question: For each experiment, does the paper provide sufficient information on the com-
636"
EXPERIMENTS COMPUTE RESOURCES,0.852910052910053,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
637"
EXPERIMENTS COMPUTE RESOURCES,0.8539682539682539,"the experiments?
638"
EXPERIMENTS COMPUTE RESOURCES,0.855026455026455,"Answer: [Yes]
639"
EXPERIMENTS COMPUTE RESOURCES,0.8560846560846561,"Justification:
640"
EXPERIMENTS COMPUTE RESOURCES,0.8571428571428571,"Guidelines:
641"
EXPERIMENTS COMPUTE RESOURCES,0.8582010582010582,"• The answer NA means that the paper does not include experiments.
642"
EXPERIMENTS COMPUTE RESOURCES,0.8592592592592593,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
643"
EXPERIMENTS COMPUTE RESOURCES,0.8603174603174604,"or cloud provider, including relevant memory and storage.
644"
EXPERIMENTS COMPUTE RESOURCES,0.8613756613756614,"• The paper should provide the amount of compute required for each of the individual
645"
EXPERIMENTS COMPUTE RESOURCES,0.8624338624338624,"experimental runs as well as estimate the total compute.
646"
EXPERIMENTS COMPUTE RESOURCES,0.8634920634920635,"• The paper should disclose whether the full research project required more compute
647"
EXPERIMENTS COMPUTE RESOURCES,0.8645502645502645,"than the experiments reported in the paper (e.g., preliminary or failed experiments
648"
EXPERIMENTS COMPUTE RESOURCES,0.8656084656084656,"that didn’t make it into the paper).
649"
CODE OF ETHICS,0.8666666666666667,"9. Code Of Ethics
650"
CODE OF ETHICS,0.8677248677248677,"Question: Does the research conducted in the paper conform, in every respect, with the
651"
CODE OF ETHICS,0.8687830687830688,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
652"
CODE OF ETHICS,0.8698412698412699,"Answer: [Yes]
653"
CODE OF ETHICS,0.870899470899471,"Justification:
654"
CODE OF ETHICS,0.8719576719576719,"Guidelines:
655"
CODE OF ETHICS,0.873015873015873,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
656"
CODE OF ETHICS,0.8740740740740741,"• If the authors answer No, they should explain the special circumstances that require a
657"
CODE OF ETHICS,0.8751322751322751,"deviation from the Code of Ethics.
658"
CODE OF ETHICS,0.8761904761904762,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
659"
CODE OF ETHICS,0.8772486772486773,"eration due to laws or regulations in their jurisdiction).
660"
BROADER IMPACTS,0.8783068783068783,"10. Broader Impacts
661"
BROADER IMPACTS,0.8793650793650793,"Question: Does the paper discuss both potential positive societal impacts and negative
662"
BROADER IMPACTS,0.8804232804232804,"societal impacts of the work performed?
663"
BROADER IMPACTS,0.8814814814814815,"Answer: [NA]
664"
BROADER IMPACTS,0.8825396825396825,"Justification:
665"
BROADER IMPACTS,0.8835978835978836,"Guidelines:
666"
BROADER IMPACTS,0.8846560846560847,"• The answer NA means that there is no societal impact of the work performed.
667"
BROADER IMPACTS,0.8857142857142857,"• If the authors answer NA or No, they should explain why their work has no societal
668"
BROADER IMPACTS,0.8867724867724868,"impact or why the paper does not address societal impact.
669"
BROADER IMPACTS,0.8878306878306879,"• Examples of negative societal impacts include potential malicious or unintended uses
670"
BROADER IMPACTS,0.8888888888888888,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
671"
BROADER IMPACTS,0.8899470899470899,"(e.g., deployment of technologies that could make decisions that unfairly impact spe-
672"
BROADER IMPACTS,0.891005291005291,"cific groups), privacy considerations, and security considerations.
673"
BROADER IMPACTS,0.8920634920634921,"• The conference expects that many papers will be foundational research and not tied
674"
BROADER IMPACTS,0.8931216931216931,"to particular applications, let alone deployments. However, if there is a direct path to
675"
BROADER IMPACTS,0.8941798941798942,"any negative applications, the authors should point it out. For example, it is legitimate
676"
BROADER IMPACTS,0.8952380952380953,"to point out that an improvement in the quality of generative models could be used to
677"
BROADER IMPACTS,0.8962962962962963,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
678"
BROADER IMPACTS,0.8973544973544973,"that a generic algorithm for optimizing neural networks could enable people to train
679"
BROADER IMPACTS,0.8984126984126984,"models that generate Deepfakes faster.
680"
BROADER IMPACTS,0.8994708994708994,"• The authors should consider possible harms that could arise when the technology is
681"
BROADER IMPACTS,0.9005291005291005,"being used as intended and functioning correctly, harms that could arise when the
682"
BROADER IMPACTS,0.9015873015873016,"technology is being used as intended but gives incorrect results, and harms following
683"
BROADER IMPACTS,0.9026455026455027,"from (intentional or unintentional) misuse of the technology.
684"
BROADER IMPACTS,0.9037037037037037,"• If there are negative societal impacts, the authors could also discuss possible mitiga-
685"
BROADER IMPACTS,0.9047619047619048,"tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
686"
BROADER IMPACTS,0.9058201058201059,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
687"
BROADER IMPACTS,0.9068783068783068,"feedback over time, improving the efficiency and accessibility of ML).
688"
SAFEGUARDS,0.9079365079365079,"11. Safeguards
689"
SAFEGUARDS,0.908994708994709,"Question: Does the paper describe safeguards that have been put in place for responsible
690"
SAFEGUARDS,0.91005291005291,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
691"
SAFEGUARDS,0.9111111111111111,"image generators, or scraped datasets)?
692"
SAFEGUARDS,0.9121693121693122,"Answer: [NA]
693"
SAFEGUARDS,0.9132275132275133,"Justification:
694"
SAFEGUARDS,0.9142857142857143,"Guidelines:
695"
SAFEGUARDS,0.9153439153439153,"• The answer NA means that the paper poses no such risks.
696"
SAFEGUARDS,0.9164021164021164,"• Released models that have a high risk for misuse or dual-use should be released with
697"
SAFEGUARDS,0.9174603174603174,"necessary safeguards to allow for controlled use of the model, for example by re-
698"
SAFEGUARDS,0.9185185185185185,"quiring that users adhere to usage guidelines or restrictions to access the model or
699"
SAFEGUARDS,0.9195767195767196,"implementing safety filters.
700"
SAFEGUARDS,0.9206349206349206,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
701"
SAFEGUARDS,0.9216931216931217,"should describe how they avoided releasing unsafe images.
702"
SAFEGUARDS,0.9227513227513228,"• We recognize that providing effective safeguards is challenging, and many papers do
703"
SAFEGUARDS,0.9238095238095239,"not require this, but we encourage authors to take this into account and make a best
704"
SAFEGUARDS,0.9248677248677248,"faith effort.
705"
LICENSES FOR EXISTING ASSETS,0.9259259259259259,"12. Licenses for existing assets
706"
LICENSES FOR EXISTING ASSETS,0.926984126984127,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
707"
LICENSES FOR EXISTING ASSETS,0.928042328042328,"the paper, properly credited and are the license and terms of use explicitly mentioned and
708"
LICENSES FOR EXISTING ASSETS,0.9291005291005291,"properly respected?
709"
LICENSES FOR EXISTING ASSETS,0.9301587301587302,"Answer: [Yes]
710"
LICENSES FOR EXISTING ASSETS,0.9312169312169312,"Justification:
711"
LICENSES FOR EXISTING ASSETS,0.9322751322751323,"Guidelines:
712"
LICENSES FOR EXISTING ASSETS,0.9333333333333333,"• The answer NA means that the paper does not use existing assets.
713"
LICENSES FOR EXISTING ASSETS,0.9343915343915344,"• The authors should cite the original paper that produced the code package or dataset.
714"
LICENSES FOR EXISTING ASSETS,0.9354497354497354,"• The authors should state which version of the asset is used and, if possible, include a
715"
LICENSES FOR EXISTING ASSETS,0.9365079365079365,"URL.
716"
LICENSES FOR EXISTING ASSETS,0.9375661375661376,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
717"
LICENSES FOR EXISTING ASSETS,0.9386243386243386,"• For scraped data from a particular source (e.g., website), the copyright and terms of
718"
LICENSES FOR EXISTING ASSETS,0.9396825396825397,"service of that source should be provided.
719"
LICENSES FOR EXISTING ASSETS,0.9407407407407408,"• If assets are released, the license, copyright information, and terms of use in the pack-
720"
LICENSES FOR EXISTING ASSETS,0.9417989417989417,"age should be provided. For popular datasets, paperswithcode.com/datasets has
721"
LICENSES FOR EXISTING ASSETS,0.9428571428571428,"curated licenses for some datasets. Their licensing guide can help determine the li-
722"
LICENSES FOR EXISTING ASSETS,0.9439153439153439,"cense of a dataset.
723"
LICENSES FOR EXISTING ASSETS,0.944973544973545,"• For existing datasets that are re-packaged, both the original license and the license of
724"
LICENSES FOR EXISTING ASSETS,0.946031746031746,"the derived asset (if it has changed) should be provided.
725"
LICENSES FOR EXISTING ASSETS,0.9470899470899471,"• If this information is not available online, the authors are encouraged to reach out to
726"
LICENSES FOR EXISTING ASSETS,0.9481481481481482,"the asset’s creators.
727"
NEW ASSETS,0.9492063492063492,"13. New Assets
728"
NEW ASSETS,0.9502645502645503,"Question: Are new assets introduced in the paper well documented and is the documenta-
729"
NEW ASSETS,0.9513227513227513,"tion provided alongside the assets?
730"
NEW ASSETS,0.9523809523809523,"Answer: [NA]
731"
NEW ASSETS,0.9534391534391534,"Justification:
732"
NEW ASSETS,0.9544973544973545,"Guidelines:
733"
NEW ASSETS,0.9555555555555556,"• The answer NA means that the paper does not release new assets.
734"
NEW ASSETS,0.9566137566137566,"• Researchers should communicate the details of the dataset/code/model as part of their
735"
NEW ASSETS,0.9576719576719577,"submissions via structured templates. This includes details about training, license,
736"
NEW ASSETS,0.9587301587301588,"limitations, etc.
737"
NEW ASSETS,0.9597883597883597,"• The paper should discuss whether and how consent was obtained from people whose
738"
NEW ASSETS,0.9608465608465608,"asset is used.
739"
NEW ASSETS,0.9619047619047619,"• At submission time, remember to anonymize your assets (if applicable). You can
740"
NEW ASSETS,0.9629629629629629,"either create an anonymized URL or include an anonymized zip file.
741"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.964021164021164,"14. Crowdsourcing and Research with Human Subjects
742"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9650793650793651,"Question: For crowdsourcing experiments and research with human subjects, does the pa-
743"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9661375661375662,"per include the full text of instructions given to participants and screenshots, if applicable,
744"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9671957671957672,"as well as details about compensation (if any)?
745"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9682539682539683,"Answer: [NA]
746"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693121693121693,"Justification:
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9703703703703703,"Guidelines:
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9714285714285714,"• The answer NA means that the paper does not involve crowdsourcing nor research
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724867724867725,"with human subjects.
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9735449735449735,"• Including this information in the supplemental material is fine, but if the main contri-
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9746031746031746,"bution of the paper involves human subjects, then as much detail as possible should
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9756613756613757,"be included in the main paper.
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9767195767195768,"• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9777777777777777,"tion, or other labor should be paid at least the minimum wage in the country of the
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9788359788359788,"data collector.
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798941798941799,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9809523809523809,"Subjects
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.982010582010582,"Question: Does the paper describe potential risks incurred by study participants, whether
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830687830687831,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9841269841269841,"approvals (or an equivalent approval/review based on the requirements of your country or
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851851851851852,"institution) were obtained?
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9862433862433863,"Answer: [NA]
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873015873015873,"Justification:
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883597883597883,"Guidelines:
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9894179894179894,"• The answer NA means that the paper does not involve crowdsourcing nor research
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904761904761905,"with human subjects.
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9915343915343915,"• Depending on the country in which research is conducted, IRB approval (or equiva-
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925925925925926,"lent) may be required for any human subjects research. If you obtained IRB approval,
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936507936507937,"you should clearly state this in the paper.
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947089947089947,"• We recognize that the procedures for this may vary significantly between institutions
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957671957671957,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968253968253968,"guidelines for their institution.
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978835978835979,"• For initial submissions, do not include any information that would break anonymity
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989417989417989,"(if applicable), such as the institution conducting the review.
775"
