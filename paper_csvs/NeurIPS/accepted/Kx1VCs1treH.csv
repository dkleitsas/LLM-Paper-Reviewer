Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.006369426751592357,"Differentiable simulations of optical systems can be combined with deep learning-
based reconstruction networks to enable high performance computational imaging
via end-to-end (E2E) optimization of both the optical encoder and the deep decoder.
This has enabled imaging applications such as 3D localization microscopy, depth
estimation, and lensless photography via the optimization of local optical encoders.
More challenging computational imaging applications, such as 3D snapshot mi-
croscopy which compresses 3D volumes into single 2D images, require a highly
non-local optical encoder. We show that existing deep network decoders have
a locality bias which prevents the optimization of such highly non-local optical
encoders. We address this with a decoder based on a shallow neural network
architecture using global kernel Fourier convolutional neural networks (Fourier-
Nets). We show that FourierNets surpass existing deep network based decoders at
reconstructing photographs captured by the highly non-local DiffuserCam optical
encoder. Further, we show that FourierNets enable E2E optimization of highly
non-local optical encoders for 3D snapshot microscopy. By combining Fourier-
Nets with a large-scale multi-GPU differentiable optical simulation, we are able
to optimize non-local optical encoders 170× to 7372× larger than prior state of
the art, and demonstrate the potential for ROI-type specific optical encoding with a
programmable microscope."
INTRODUCTION,0.012738853503184714,"1
Introduction"
INTRODUCTION,0.01910828025477707,"Modern computational optics relies on the end-to-end (E2E) optimization of the optical system
(optical encoder) and the computational image reconstruction algorithm (computational decoder).
This has been enabled by the development of differentiable physics-based simulations of optical
encoders, paired with deep network based decoders. This E2E approach has been successfully applied
to the estimation of depth from defocus [1, 2, 3, 4], lensless photography [5], and particle localization
microscopy [6, 7]. Despite this wide success, only the space of local optical encoders has been
explored in this fashion due to the computational expense of physics-based simulation of highly
non-local optical encoders [3]. This is particularly the case for 3D snapshot microscopy which is
well-known to require highly non-local optical encoders, and for which no E2E solutions have yet
been demonstrated."
INTRODUCTION,0.025477707006369428,"We show that there are actually two issues blocking the E2E optimization of highly non-local optical
encoders. First, the computational expense of physics-based simulation of non-local optical encoders
is significantly greater, since they are orders of magnitude larger than local encoders. And second, the
locality bias inherent to deep network architectures currently used for computational imaging prevents
decoding from non-local solutions. In this paper, we primarily focus on developing a computational
framework for engineering highly non-local optical encoders by solving both issues. We solve the
first issue by developing a large-scale multi-GPU differentiable optical simulation, and the second by
developing FourierNet: a new decoder network architecture based on Fourier convolutions."
INTRODUCTION,0.03184713375796178,"In 3D snapshot microscopy, all existing methods [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 5, 19]
employ a highly non-local optical encoder to optically transform a 3D volume into a single 2D camera
image, which is computationally decoded to reconstruct the 3D volume. Fast volumetric imaging
is invaluable across biology, including for imaging neural activity throughout the whole brain of an
animal [20, 21, 22]. Here, we focus on developing a 3D snapshot microscope for imaging neuronal
activity across the whole brain of the larval zebrafish (Danio rerio) at camera rates exceeding 100
Hz. This is two orders of magnitude faster than the fastest conventional microscopes — light sheet
imaging of whole brain neural activity can only achieve 0.5 Hz - 2 Hz volume rates [20]. This
improvement of temporal resolution is essential for imaging with fast calcium indicators [23] and
voltage indicators [24]."
INTRODUCTION,0.03821656050955414,"Newly developed programmable microscopes with up to 106 free parameters, e.g. pixels on a spatial
light modulator (SLM), enable the implementation of a rich space of optical encodings and present
the possibility of direct optimization of snapshot microscope parameters specifically for particular
ROI types and imaging tasks. While our paper primarily focuses on addressing the computational
challenges of E2E optimization of highly non-local optical encoders, we also demonstrate a proof-of-
concept that an SLM-based programmable microscope can implement such an engineered non-local
optical encoder, paving the way to implementing multiple ROI-type and task specific optical encoders
in a single physical microscope. In contrast, 3D snapshot imaging has classically been performed
using fixed optical encoders based on microlens arrays [8, 9, 10, 11, 12, 13, 14, 15], pseudorandom
diffusers/masks [16, 17], and designed or optimized diffusers/masks [18, 5, 19], with some of these
methods using deep learning for reconstruction [11, 15]."
INTRODUCTION,0.044585987261146494,"Problem statement We define an optical encoder as Mϕ parameterized by ϕ and a computational
decoder as Rθ parameterized by θ. Optical encoders can usually be modeled as linear transfer
functions implemented by the optics. In special cases, such as for the optical systems explored
in this paper, they can be represented as a linear convolution filter called a “point spread function”
with coefficients computed by a wave optics simulation, dependent on the physical parameters ϕ of
the optical system (Appendix A.1). We wish to develop a reconstruction network architecture for
decoding non-local encodings in 2D images c produced by Mϕ and also to enable the end-to-end
optimization of ϕ to produce non-local encodings. We’ll consider two applications to investigate
these optimization problems: (1) lensless photography (Figure 2), where we optimize only the
reconstruction network Rθ to reconstruct images of natural scenes from camera images captured
by the DiffuserCam [25], and (2) 3D snapshot microscopy using an SLM-based programmable
microscope (Figure 1A), where we perform E2E optimization of both the reconstruction network Rθ
and the optical encoding Mϕ based on the SLM parameters in order to image 3D volumes v and
reconstruct 3D volumes ˆv. While we focus on solving the computational challenges involved in this
E2E optimization of a large highly non-local optical encoder, we also demonstrate a proof-of-concept
implementation of our optimized optical encoder using an SLM-based programmable microscope."
OUR CONTRIBUTIONS,0.050955414012738856,"1.1
Our contributions"
OUR CONTRIBUTIONS,0.05732484076433121,"1. We developed a large-scale, parallel, multi-GPU differentiable wave optics simulation of a
programmable microscope, based on a 4f optical model with a phase mask (ϕ) implemented
using a spatial light modulator (SLM), described further in Appendix A.1. SLMs (ϕ) can
have over 106 optimizable parameters, which we can feasibly simulate and optimize to
produce PSFs with 170× to 7372× more unique voxels than previous attempts at deep
learning PSF optimization using single GPUs [6, 3] (Appendix A.5)."
OUR CONTRIBUTIONS,0.06369426751592357,"2. We collected a large dataset of high resolution 3D confocal volumes of zebrafish larvae for
the purpose of ROI-type specific end-to-end optimization of optical encoders."
"WE INTRODUCE AN EFFICIENT FOURIERNET RECONSTRUCTION NETWORK ARCHITECTURE FOR DECODING FROM
NON-LOCAL OPTICAL ENCODERS USING VERY LARGE GLOBAL CONVOLUTIONS IMPLEMENTED VIA FOURIER",0.07006369426751592,"3. We introduce an efficient FourierNet reconstruction network architecture for decoding from
non-local optical encoders using very large global convolutions implemented via Fourier
convolutions.
4. We show that our networks outperform the state-of-the-art deep decoders for DiffuserCam
based lensless photography [25] and for 3D snapshot microscopy.
5. Our method enables, for the first time, direct end-to-end optimization of highly non-local
optical encoders in the space of spatial light modulator (SLM) pixels with over 106 parame-
ters. In simulation, we demonstrate the potential for significant improvements in imaging
resulting from ROI-type specific optimization of optical encoders."
PRIOR WORK,0.07643312101910828,"1.2
Prior work"
PRIOR WORK,0.08280254777070063,"Neural network architectures for computational imaging have all used convolution layers with
small filters. End-to-end optimization of optical encoders have largely been performed with UNet-
based architectures [1, 2, 3, 4, 5, 7], with one method using a ResNet-based architecture [6]. Such
optimization has always led to local optical encodings. End-to-end optimization has never been
attempted for large-field of view 3D snapshot microscopy due to the difficulty of simulating and
reconstructing from non-local encoders. However, small filter convolutional deep networks have been
used in a non-end-to-end manner to reconstruct volumes from 3D snapshot microscopes designed
using microlens arrays [15, 26, 27]. One recent hybrid approach combines a UNet with a differentiable
approximate inverse method (Wiener filter) to handle non-local spatially varying (non-convolutional)
optical encoders [27]. For photography and MRI, another hybrid approach of deep learning combined
with unrolling iterations of traditional deconvolution algorithms provides the benefits of fast amortized
optimization and higher quality reconstructions due to learning of structural priors [28, 29, 25, 26].
We note that these hybrid approaches typically require measurement of the optical encoder of the
system, whereas our method does not and can learn to produce high quality reconstructions using
only pairs of ground truth and system images."
PRIOR WORK,0.08917197452229299,"In our work, we demonstrate that convolution layers with large filters implemented efficiently in the
Fourier domain enable the end-to-end learning of highly non-local optical encoders for 3D snapshot
microscopy. Large convolution filters have been shown to be helpful for other computer vision
applications such as semantic segmentation and salient object detection [30, 31], and the Fourier
domain parameterization of small filters has been described previously [32]."
PRIOR WORK,0.09554140127388536,"A pioneering strategy in 3D snapshot microscopy is light field microscopy [8], which employs a
microlens array at the microscope’s image plane to create subimages encoding both the amplitude
and phase of light [8, 33]. A variety of microlens-array-based light field microscopes have been
used to perform whole brain imaging [8, 11, 14, 22, 13, 12, 9, 34, 19]. [19] optimizes the placement
of microlenses, but not in an end-to-end manner. Despite variation in design, microlens-based
microscopes have, to various degrees, four main limitations that can be improved: 1) blocking or
scattering light between microlenses, causing light inefficiency, 2) not making use of all pixels on the
camera to encode a 3D volume, leading to inefficient compression and suboptimal reconstructions ˆv,
3) aliasing at some planes with generally nonuniform axial resolution, and 4) a fixed optical encoding
scheme."
PRIOR WORK,0.10191082802547771,"An alternative to using microlenses is to implement a coded detection strategy using a phase mask or
diffuser to spread light broadly across the camera sensor [16, 18, 35, 5, 10, 17, 19]. The designed
phase masks can be implemented either by manufacturing a custom optical element or using a
programmable SLM [2, 3, 4, 5, 19, 6]. Using a programmable element allows different microscope
parameters to be used for different sample and ROI types."
METHODS,0.10828025477707007,"2
Methods"
METHODS,0.11464968152866242,"We show our network architecture and an overview of autoencoder training both the microscope
parameters ϕ and reconstruction network parameters θ in Figure 1A. The programmable microscope
is simulated by a differentiable implementation of a wave-optics model of light propagation. We
have selected a programmable microscope design based on pupil-plane phase modulation with a
programmable spatial light modulator, for which imaging is well-approximated by a computationally-
efficient convolution [36]. A detailed description of our simulation is provided in Appendix A.1. For
lensless photography, there is no optical simulation because the images have been collected on a real
camera."
D SNAPSHOT MICROSCOPY,0.12101910828025478,3D snapshot microscopy
D SNAPSHOT MICROSCOPY,0.12738853503184713,FourierNet
D RECONSTRUCTION,0.1337579617834395,"3D reconstruction
3D sample"
D IMAGE,0.14012738853503184,2D image
D IMAGE,0.1464968152866242,"reconstruction network
(computational decoder)
programmable microscope
(optical encoder)"
D IMAGE,0.15286624203821655,phase mask
D IMAGE,0.1592356687898089,"conv
convs"
D IMAGE,0.16560509554140126,Fourier convolution
D IMAGE,0.17197452229299362,"Figure 1: Overview of our problem setup and our proposed network architectures. Top row (A) shows
the problem of 3D snapshot microscopy, where we computationally reconstruct a 3D volume from a
2D image. Bottom row (B) shows our proposed FourierNet architecture, which includes a Fourier
convolution layer that enables efficient computation of global features."
FOURIERNET FOR DECODING NON-LOCAL OPTICAL ENCODERS,0.17834394904458598,"2.1
FourierNet for decoding non-local optical encoders"
FOURIERNET FOR DECODING NON-LOCAL OPTICAL ENCODERS,0.18471337579617833,"Because images created by optical encoders can potentially encode signals from the incoming light
field to any location in the camera image in a non-local manner, it is essential that a reconstruction
network have global context. Existing multi-scale architectures such as the UNet [37] can achieve
global context, but at the expense of many computation layers with small convolution kernels which
we speculate to have a local information bias that is inappropriate for computational optics. In
this paper, we introduce relatively shallow architectures for computational imaging, which rely on
convolution layers with very large filters, implemented efficiently in the Fourier domain [32]."
FOURIERNET FOR DECODING NON-LOCAL OPTICAL ENCODERS,0.1910828025477707,"FourierNet We propose a simple three layer convolutional network architecture with very large
global convolutions at the very first layer, followed by two standard local convolutional layers (Figure
1B). We define a global convolution as a convolution with kernel size equal to the input image. Such
a convolution achieves global context in a single step but is computationally expensive. Global
convolutions are implemented more efficiently in the Fourier domain, yielding a speed up of two
orders of magnitude over direct convolution. Due to the use of Fourier convolutions to enable global
context, we call our architecture FourierNet. In contrast to a typical UNet which can contain many
tens of convolution layers, the FourierNet is only three layers deep, which requires backpropagation
through fewer layers compared to a typical UNet with the same receptive field."
FOURIERNET FOR DECODING NON-LOCAL OPTICAL ENCODERS,0.19745222929936307,"Fourier domain convolutions It is well-known that large kernel convolutions can be implemented
more efficiently in the Fourier domain [38, 39, 40]. A naive implementation of global convolution
requires O(N 2) operations, where N is the number of pixels in both the image and the kernel. An
alternative global convolution implementation is to Fourier transform (F) the input x and convolution
kernel w, perform element-wise multiplication in Fourier space, and finally inverse Fourier transform
(F−1), requiring only O(N log N) operations [39, 40]. Following [32], we store and optimize the
weights W in Fourier space. This over-parameterization costs 8× the memory of an equivalent real
valued large filter but saves the computational cost of Fourier transforming the real-valued weights
(Appendix A.6). Thus a Fourier convolution is defined:
Re{F−1 {W ⊙F {x}}}
(1)
For image and kernel sizes of 256 × 256 pixels, our implementation leads to nearly 500× speedup: a
standard PyTorch convolution takes 2860 ms, while a Fourier convolution takes 5.92 ms on a TITAN
X. We also show how we can naturally extend our Fourier convolutions and FourierNet to a multiscale
version using cropping in the Fourier domain in Appendix A.2."
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE ENCODER AND
RECONSTRUCTION NETWORK DECODER",0.20382165605095542,"2.2
Physics-based autoencoder for simultaneous engineering of microscope encoder and
reconstruction network decoder"
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE ENCODER AND
RECONSTRUCTION NETWORK DECODER",0.21019108280254778,"We describe the imaging process as the following transformation from the 3D intensity volume v to
the 2D image formed on the camera c:
µc = Mϕ(v)
(2)
c = max ([µc + √µcϵ] , 0), ϵ ∼N(0, 1)
(3)"
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE ENCODER AND
RECONSTRUCTION NETWORK DECODER",0.21656050955414013,"where Mϕ denotes the microscope parameterized by a 2D phase mask, ϕ. This phase mask ϕ
describes the 3D-to-2D encoding of this microscope model completely. A Poisson distribution
with mean rate µc describes the physics of photon detection at the camera, but sampling from this
distribution is not differentiable. We approximate the noise distribution with a rectified Gaussian.
We include details on Mϕ in Appendix A.1 [36]. Jointly training reconstruction networks and
microscope parameters involves image simulation, reconstruction, then gradient backpropagation to
update the reconstruction network and microscope parameters. Our parallelization strategy enables
optimization of phase masks with millions of parameters in a feasible amount of time and memory
per GPU. This also enables us to produce PSFs with multiple orders of magnitude more unique
voxels than previous attempts (Appendix A.4). Details on parallelization, planewise reconstruction
networks, and planewise sparse gradients are provided in Appendix A.4, Figure 12."
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE ENCODER AND
RECONSTRUCTION NETWORK DECODER",0.2229299363057325,"We use the normalized mean squared error (NMSE) as the basis for our loss function. Since we care
more about the high spatial frequency content of the image which contains mostly cells, and less
about the low spatial frequencies which contain mostly background, we implemented a two part loss
function. LHNMSE measures the NMSE between the high pass filtered volume and high pass filtered
reconstruction. This is combined with the unfiltered NMSE LNMSE between the original volume and
its reconstruction. Formally, our loss function L(v, ˆv) for all snapshot microscopy reconstruction
problems is defined as:
L(v, ˆv) = LHNMSE(v, ˆv) + βLNMSE(v, ˆv)
(4)"
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE ENCODER AND
RECONSTRUCTION NETWORK DECODER",0.22929936305732485,"LHNMSE(v, ˆv) = E

(H(v) −H(ˆv))2"
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE ENCODER AND
RECONSTRUCTION NETWORK DECODER",0.2356687898089172,"E(H(v)2)
, LNMSE(v, ˆv) = E

(v −ˆv)2"
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE ENCODER AND
RECONSTRUCTION NETWORK DECODER",0.24203821656050956,"E(v2)
(5)"
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE ENCODER AND
RECONSTRUCTION NETWORK DECODER",0.2484076433121019,"where H(·) denotes high pass filtering and E(·) denotes the mean over pixels and sample volumes.
Both loss terms are normalized as shown to reduce variance in L, which can otherwise cause large
magnitude fluctuations based on the brightness variation across training volumes resulting in training
instability. For our experiments, we set the weight β for the LNMSE term to 0.1."
RESULTS,0.25477707006369427,"3
Results"
RESULTS,0.2611464968152866,"DiffuserCam Lensless Mirflickr Dataset We test reconstruction performance on experimental
computational photography data1 from [25] (Figure 2). This is a dataset constructed by displaying
RGB color natural images from the MIRFlickr dataset on a monitor and then capturing diffused
images by the DiffuserCam lensless camera. The dataset contains 24,000 pairs of DiffuserCam and
ground truth images. The goal of the dataset is to learn to reconstruct the ground truth images from
the diffused images. As in [25], we train on 23,000 paired diffused and ground truth images, and test
on 999 held-out pairs of images."
RESULTS,0.267515923566879,"Larval Zebrafish Snapshot Microscopy Dataset We show all our results for snapshot microscopy
using our simulation of a snapshot microscope, and our engineered optical encoders have not been
experimentally tested for reconstruction performance on a programmable microscope. We simulate
snapshot imaging using high resolution confocal imaging volumes of zebrafish. These are volumes of
transgenic larval zebrafish whole brains expressing the nuclear-restricted GCaMP6 calcium indicator
in all neurons. These images are representative of brain-wide activity imaging. We train on 58
different zebrafish volumes (which we augment heavily) and test on 10 held-out volumes. For all
experiments, we downsample the high resolution confocal data to (1.0 µm z, 1.625 µm y, 1.625 µm
x). We created 4 datasets from these scanned volumes corresponding to imaging different fields of
view or regions of interest (ROIs) for our experiments. Full specifications for these datasets are in
Table 6 (Appendix A.4). For Figure 3, we restrict the field of view to (200 µm z, 416 µm y, 416 µm
x) with a tall cylinder cutout of diameter 193 µm and height 200 µm and image with 256×256 pixels
on the simulated camera sensor. Figure 4 and Table 3 show our larger experiments with 512 × 512
simulated camera pixels, with a field of view of (250 µm z, 832 µm y, 832 µm x)."
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.27388535031847133,"3.1
FourierNets outperform state-of-the-art for reconstructing natural images captured by
DiffuserCam lensless camera"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.2802547770700637,"We compare our FourierNet architecture to the best learned method from [25] using unrolled ADMM
and a denoising UNet, as well as to a vanilla UNet from [25]. Architecture details are in Appendix"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.28662420382165604,1Publicly available: https://waller-lab.github.io/LenslessLearning/dataset.html
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.2929936305732484,"ground truth
DiﬀuserCam"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.29936305732484075,"image
Le-ADMM-U"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.3057324840764331,"(MSE + LPIPS)
UNet"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.31210191082802546,"(MSE + LPIPS)
FourierNet ()"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.3184713375796178,(MSE + LPIPS) 1 1 1 1 1 1 2 2 1 1 2 2
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.3248407643312102,"Figure 2: Comparisons of our method (third row) to state-of-the-art learned reconstruction methods
on lensless diffused images of natural scenes. Regions labeled 1⃝indicate missing details, either
resolution or textures in backgrounds. Regions labeled 2⃝indicate hallucinated textures. Note that
the previous state-of-the-art solution (fourth row) [25] exhibits both issues more often compared to
our method."
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.33121019108280253,"Table 1: Quality of natural image reconstruction on the DiffuserCam Lensless Mirflickr Dataset
(mean ± s.e.m., n = 999). Superscripts denote loss function: 1 MSE, 2 MSE+LPIPS."
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.3375796178343949,"Method
MSE ↓(×10−2) LPIPS ↓
MS-SSIM ↑
PSNR ↑
Time ↓(ms)"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.34394904458598724,"FourierNet1
0.39 ± 0.007
0.20 ± 0.00
0.882 ± 0.001
24.8 ± 0.09
35.54
FourierNet2
0.54 ± 0.010
0.16 ± 0.00
0.868 ± 0.001
23.4 ± 0.09
35.54"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.3503184713375796,"Le-ADMM-U2 [25] 0.75 ± 0.021
0.19 ± 0.00
0.865 ± 0.002
22.1 ± 0.09
48.59
UNet2 [25]
1.68 ± 0.060
0.24 ± 0.00
0.818 ± 0.002
19.2 ± 0.11
06.97"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL IMAGES CAPTURED BY
DIFFUSERCAM LENSLESS CAMERA",0.35668789808917195,"A.6, A.7. We can see that FourierNet visually outperforms the methods from [25] in Figure 2, and
quantitatively in Table 1 across all quality metrics. The Le-ADMM-U and UNet results in Table
1 were reported by [25] using a combined MSE + LPIPS loss. Unlike [25], we find that training
FourierNets with the MSE loss alone provides reconstructions visually similar to the ground truth as
shown in Figure 13 (Appendix A.7). Timings in Table 1 are for only the forward pass on a single
TITAN Xp GPU."
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.3630573248407643,"3.2
FourierNets outperform UNets for engineering non-local optical encoders"
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.36942675159235666,"Table 2: Quality of reconstructed volumes after optimizing microscope parameters to image zebrafish
on 256 × 256 pixel camera (mean ± s.e.m., n = 10)"
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.37579617834394907,"Microscope
Reconstruction
LHNMSE ↓
MS-SSIM ↑
PSNR ↑
Time ↓(s)"
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.3821656050955414,"FourierNet2D
FourierNet3D
0.6093 ± 0.0209
0.955 ± 0.004
34.89 ± 0.88
0.38
FourierNet2D
UNet3D
0.7298 ± 0.0151
0.923 ± 0.008
30.16 ± 0.94
0.96
Wiener + UNet
Wiener + UNet
0.7223 ± 0.0179
0.957 ± 0.003
34.49 ± 0.91
0.73
UNet2D
UNet3D
0.7109 ± 0.0161
0.913 ± 0.009
29.17 ± 1.13
0.96"
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.3885350318471338,"We compare optimizing microscope parameters ϕ with three neural networks: 1) using our FourierNet
with 2D convolutions (FourierNet2D), 2) using a vanilla UNet with 2D convolutions (UNet2D),
and 3) using a Wiener deconvolution as an approximate inverse combined with a refining UNet, as x
y z
x x
y 50 μm"
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.39490445859872614,100 μm
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.4012738853503185,FourierNet
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.40764331210191085,ground truth UNet z y x
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.4140127388535032,xy max projection
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.42038216560509556,xz max projection
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.4267515923566879,xy max projection
S,0.43312101910828027,3.00 s
S,0.4394904458598726,"0.01 s
0.01 s"
S,0.445859872611465,light propagation
S,0.45222929936305734,"simulated camera image
reconstructed volume"
S,0.4585987261146497,Wiener + UNet
S,0.46496815286624205,0.01 s
S,0.4713375796178344,"30
1040
30
637
30
432
30
645"
S,0.47770700636942676,"3104
0
1104
0
2104
0"
S,0.4840764331210191,"Figure 3: Comparing simulated camera images (0.01 second expected acquisition time) and corre-
sponding reconstructions of a volume captured using our FourierNet (left) versus UNet (middle) and
Wiener + UNet [3] (right) optimized microscopes. Top row shows simulated 256 × 256 pixel camera
images; bottom right of camera image shows approximate acquisition time (given a reasonable
number of simulated photons per pixel, i.e. SNR). Ground truth has no corresponding camera image,
because the 3D volume is imaged directly via slow high resolution confocal microscopy (3 second
acquisition time). Colored arrows in left column show projection axis for each row. White arrows
show individual neurons clearly visible for FourierNet, but not for UNet. Wiener + UNet [3] has
detail in some planes, but not consistently throughout the whole volume, and also uses fewer camera
pixels."
S,0.49044585987261147,"in [3]. Training is a two stage process in which a plane-wise reconstruction network with fewer
parameters and 2D convolutions is used during optimization of ϕ, then once ϕ is fixed a more
powerful reconstruction network with 3D convolutions is used, except for the Wiener + UNet [3]
method which is trained in a single stage as in [3]. Table 2 shows the type of network used for
the first stage to train the optical encoder Mϕ in the first column, and the type of network used
in the second stage for training the reconstruction network only (given a fixed ϕ) in the second
column. We find this scheme achieves better reconstruction quality because the reconstruction
network does not need to constantly adapt to a changing optical encoding (Appendix A.6). We train
these microscopes and reconstruction networks on ROIs which are tall cylindrical cutouts of zebrafish
with diameter 193 µm and height 200 µm. Sample volumes are imaged on a camera with 256 × 256
pixels (Figure 3). FourierNet2D has 2 convolution layers (with 99.8% of its kernel parameters in
the initial Fourier convolution layer), while UNet2D has 32 convolution layers (kernel parameters
approximately uniformly distributed per layer). UNet2D was designed to have a global receptive field.
FourierNet2D and UNet2D both have ∼4×107 parameters; FourierNet3D has ∼6×107 parameters
vs. ∼108 for UNet3D. The Wiener + UNet method [3] has ∼8 × 107 parameters. Architecture
details are in Appendix A.6, A.8."
S,0.4968152866242038,"Simulated camera images (Figure 3) show that the UNet microscope does not make sufficient use
of camera pixels, producing only a single view of the volume. We speculate this is due to a local
information prior in the small kernels of UNets. Adding a Wiener deconvolution as an approximate"
S,0.5031847133757962,"Table 3: ROI specific microscope parameter optimization for 3 types of zebrafish volumes (mean
PSNR (top), MS-SSIM (bottom) ± s.e.m., n = 10). Green shows regions of interest."
S,0.5095541401273885,"Microscope parameters optimized for
Type A
Type B
Type C"
S,0.5159235668789809,Tested on
S,0.5222929936305732,"Type A
49.76 ± 1.35
0.998 ± 0.000
46.03 ± 1.33
0.996 ± 0.001
42.67 ± 1.14
0.992 ± 0.002
Type B
35.56 ± 1.41
0.965 ± 0.004
37.34 ± 0.96
0.972 ± 0.003
35.38 ± 1.16
0.967 ± 0.003
Type C
30.87 ± 1.15
0.912 ± 0.007
31.48 ± 0.93
0.920 ± 0.006
33.79 ± 0.90
0.935 ± 0.006"
S,0.5286624203821656,"inverse before a UNet also does not result in a microscope that makes full use of camera pixels, but is
better than the UNet alone. The FourierNet microscope uses more camera pixels and performs better
than the UNet microscope for reconstruction (Figure 3, quantified in Table 2). Timings in Table 2
are for only the forward pass on a single TITAN Xp GPU; one training iteration on 8 GPUs takes
∼0.4 seconds for FourierNet3D, ∼0.8 seconds for Wiener + UNet, and ∼0.8 seconds for UNet3D
(Appendix A.6). Both reconstruction networks must reconstruct from images that have a compressed
encoding of 3D information, but the FourierNet2D is clearly more effective than the UNet2D at
optimizing this encoding."
S,0.535031847133758,"3.3
FourierNets outperform UNets for 3D snapshot microscopy volume reconstruction"
S,0.5414012738853503,"We can determine which architecture is better for volume reconstruction by choosing fixed microscope
parameters and varying the architecture, except for the Wiener + UNet method [3] which is trained in
one stage. In Table 2, we compare results using a FourierNet with 3D convolutions (FourierNet3D)
and a vanilla UNet with 3D convolutions (UNet3D). UNet3D was also designed to have a global
receptive field. Architecture details are in Appendix A.6, A.8."
S,0.5477707006369427,"Reconstruction results in Table 2 compare normalized MSE LHNMSE between the high pass fil-
tered volume and high pass filtered reconstruction, the multiscale structural similarity MS −SSIM
between the true volume and its reconstruction, and finally the peak signal-to-noise ratio PSNR.
We also visualize reconstruction results for a volume in the head of a zebrafish in Figure 3. The
UNet3D reconstruction networks (using either microscope) fall significantly behind the FourierNet3D
reconstruction network in all metrics, despite their global receptive field. The Wiener + UNet [3]
network achieves similar MS −SSIM and PSNR as the FourierNet but a worse LHNMSE due to the
inconsistent detail, though the reconstructions are slightly better than the UNet3D for certain regions
of the volume.
3.4
Engineered optical encoders can be optimized for a region of interest"
S,0.554140127388535,"To explore the effect of ROI size on optimized ϕ and the resulting reconstruction performance, we
optimized ϕs for three different regions of interest: 1) Type A, ROIs with a short cylinder cutout of
386 µm diameter and 25 µm height, 2) Type B, ROIs with a tall cylinder cutout of 386 µm diameter
and 250 µm height, and 3) Type C, ROIs without any cutout of dimension (250 µm z × 832 µm y ×
832 µm x) (Table 3). All ROI types were imaged with 512 × 512 pixels on the simulated camera. We
then tested reconstruction performance on all combinations of optimized ϕ and ROI type, as shown
in Table 3 and visualized in Figure 4 for Type B. We include architecture details in Appendix A.6,
A.9."
S,0.5605095541401274,"We see in Table 3 that for all types, highest performance is achieved using the phase mask optimized
for that particular type. These results show that there is potentially a large benefit to optimizing
type-specific optical encoders, which can be easily implemented in a programmable microscope."
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.5668789808917197,"3.5
Engineered optical encoders can be implemented on a programmable microscope"
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.5732484076433121,"We demonstrate that the optical encoders engineered by simulations can be implemented on real
hardware using a prototype programmable microscope. Our engineered phase masks were displayed x
y z x 50 μm"
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.5796178343949044,100 μm
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.5859872611464968,"ground truth (Type B)
optimized for Type A
optimized for Type B
optimized for Type C"
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.5923566878980892,simulated camera image
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.5987261146496815,"30
1026
30
518
30
582
30
516"
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.6050955414012739,"4 10
0
2 10
0
1 10
0"
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.6114649681528662,"Figure 4: Reconstructed volumes resulting from imaging Type B ROIs by microscopes optimized
for Types A, B, and C. Imaging Type B ROIs with microscope parameters optimized for Type B
ROIs yields the best reconstructions. Top to bottom: xy max projection, xz max projection, simulated
512 × 512 camera image."
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.6178343949044586,"on a spatial light modulator (SLM) located in the pupil plane. We observe a good qualitative match
between the features of the simulated and experimental optical encoders (point spread functions) in
Figure 5, technical details in Appendix A.10. There is also a good qualitative match between the
simulated camera images generated by both optical encoders. These results demonstrate the potential
for programmable microscopy with spatial light modulators."
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.6242038216560509,"We also note that our optimized optical encoders result in pencil-like elements, qualitatively similar
to lenslet-based approaches [19, 10, 9]. However as we show in our supplement, our optimized phase
masks (Figures 11) appear qualitatively different from lenslet-based phase masks, with different
regions of the pupil contributing to the same pencil. This suggests a qualitatively different, and
perhaps more light efficient, mechanism for generating high resolution projections of the volume
along different axes."
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.6305732484076433,100 μm
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.6369426751592356,"Simulated optical encoder
Measured optical encoder x
y z x"
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.643312101910828,"5105
0
1106
0"
ENGINEERED OPTICAL ENCODERS CAN BE IMPLEMENTED ON A PROGRAMMABLE MICROSCOPE,0.6496815286624203,"Figure 5: Simulated versus measured optical encoders (point spread functions) with corresponding
simulated camera images using an example from our Type B zebrafish dataset. Zoom in for best
viewing."
"DISCUSSION
SUMMARY WE HAVE PRESENTED THE FOURIERNET ARCHITECTURE AS A DEEP NETWORK BASED DECODER FOR",0.6560509554140127,"4
Discussion
Summary We have presented the FourierNet architecture as a deep network based decoder for
computational imaging with highly non-local optical encoders. We demonstrated the superiority
of FourierNets for lensless photography on the DiffuserCam dataset, and also for the end-to-end
optimization of highly non-local optical encoders (where UNets fail) for 3D snapshot microscopy.
FourierNets are many orders of magnitude faster than traditional iterative reconstruction algorithms
[25] and generate higher quality reconstructions by learning image priors. Generally, our global
Fourier-domain convolution architecture could be applicable to other problems where global integra-
tion of features is necessary, though we have focused on computational imaging applications where
physics-based optical encoders induce global mixing of information. Our work solves two important
computational problems preventing E2E optimization of large highly non-local optical encoders: the
computational complexity of simulating large non-local encoders, and the effective decoding from
such encoders. Our contributions are primarily computational, however we also demonstrate the
potential for implementing 3D snapshot microscopy with an E2E optimized non-local optical encoder
using an SLM-based programmable microscope. And in simulation, we demonstrate the potential for
ROI-specific optimization of optical encoders for 3D snapshot microscopy, which can be efficiently
implemented in a programmable microscope."
"DISCUSSION
SUMMARY WE HAVE PRESENTED THE FOURIERNET ARCHITECTURE AS A DEEP NETWORK BASED DECODER FOR",0.6624203821656051,"Limitations While we have fully demonstrated that our framework now enables computational E2E
optimization of large non-local optical encoders, we do not focus on their hardware implementation.
The specific claims regarding the potential benefits of ROI-specific optimization of optical encoders
are only evaluated in simulation, and have only been evaluated for whole brain larval zebrafish data."
"DISCUSSION
SUMMARY WE HAVE PRESENTED THE FOURIERNET ARCHITECTURE AS A DEEP NETWORK BASED DECODER FOR",0.6687898089171974,"Reproducibility We train on 8 Quadro RTX 8000 GPUs for the largest experiments, and have
described our pre-processing, training, and testing procedures in Appendix A.4, A.6, A.8. We have
made our simulation software, training scripts, and our datasets available at https://github.com/
TuragaLab/snapshotscope."
"DISCUSSION
SUMMARY WE HAVE PRESENTED THE FOURIERNET ARCHITECTURE AS A DEEP NETWORK BASED DECODER FOR",0.6751592356687898,"Broader Impact End-to-end optimization of optics can improve image quality but also enable
multiplexed imaging not currently possible with standard optics. Our PyTorch-based optical modeling
library as well as our neural network architectures are publicly available and could enable new
experiments in neuroscience via whole brain imaging at an order of magnitude greater temporal
resolution. Our training procedure does require long optimization periods with many GPUs, which
poses a carbon footprint and barrier to usage compared to conventional microscopy."
"DISCUSSION
SUMMARY WE HAVE PRESENTED THE FOURIERNET ARCHITECTURE AS A DEEP NETWORK BASED DECODER FOR",0.6815286624203821,Acknowledgement
"DISCUSSION
SUMMARY WE HAVE PRESENTED THE FOURIERNET ARCHITECTURE AS A DEEP NETWORK BASED DECODER FOR",0.6878980891719745,"We would like to thank William Bishop, Roman Vaxenburg, Gert-Jan Both, Janne Lappalainen,
Nathan Klapoetke, Richard Xu, Lu Mi, Sridhama Prakhya, and Jinyao Yan for invaluable feedback
and discussions. We thank Howard Hughes Medical Institute and a Simons Foundation grant (Simons
Collaboration on the Global Brain, 542943SPI, Ahrens) for their funding. We also thank Huazhong
University of Science and Technology and the China Scholarship Council for supporting the work of
Zhenfei Jiao. We additionally thank the Janelia Visiting Scientist Program for supporting the work of
Ruth Sims."
REFERENCES,0.6942675159235668,References
REFERENCES,0.7006369426751592,"[1] J. Chang and G. Wetzstein, “Deep optics for monocular depth estimation and 3d object detection,”
arXiv:1904.08601 [cs, eess], Apr 2019. arXiv: 1904.08601.
[2] X. Dun, X. Dun, H. Ikoma, G. Wetzstein, Z. Wang, Z. Wang, X. Cheng, X. Cheng, X. Cheng,
Y. Peng, and et al., “Learned rotationally symmetric diffractive achromat for full-spectrum
computational imaging,” Optica, vol. 7, p. 913–922, Aug 2020.
[3] H. Ikoma, C. M. Nguyen, C. A. Metzler, Y. Peng, and G. Wetzstein, “Depth from defocus with
learned optics for imaging and occlusion-aware depth estimation,” in 2021 IEEE International
Conference on Computational Photography (ICCP), p. 1–12, May 2021.
[4] Y. Wu, V. Boominathan, H. Chen, A. Sankaranarayanan, and A. Veeraraghavan, “Phasecam3d
— learning phase masks for passive single view depth estimation,” in 2019 IEEE International
Conference on Computational Photography (ICCP), p. 1–12, May 2019."
REFERENCES,0.7070063694267515,"[5] Y. Hua, S. Nakamura, M. S. Asif, and A. C. Sankaranarayanan, “Sweepcam — depth-aware
lensless imaging using programmable masks,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 42, p. 1606–1617, Jul 2020."
REFERENCES,0.7133757961783439,"[6] E. Nehme, D. Freedman, R. Gordon, B. Ferdman, L. E. Weiss, O. Alalouf, T. Naor, R. Orange,
T. Michaeli, and Y. Shechtman, “Deepstorm3d: dense 3d localization microscopy and psf design
by deep learning,” Nature Methods, vol. 17, p. 734–740, Jul 2020."
REFERENCES,0.7197452229299363,"[7] H. Ikoma, T. Kudo, Y. Peng, M. Broxton, and G. Wetzstein, “Deep learning multi-shot 3d
localization microscopy using hybrid optical-electronic computing,” Optics Letters, vol. 46,
p. 6023–6026, Dec 2021."
REFERENCES,0.7261146496815286,"[8] M. Levoy, R. Ng, A. Adams, M. Footer, and M. Horowitz, “Light field microscopy,” in ACM
SIGGRAPH 2006 Papers, SIGGRAPH ’06, p. 924–934, Association for Computing Machinery,
Jul 2006."
REFERENCES,0.732484076433121,"[9] L. Cong, Z. Wang, Y. Chai, W. Hang, C. Shang, W. Yang, L. Bai, J. Du, K. Wang, and Q. Wen,
“Rapid whole brain imaging of neural activity in freely behaving larval zebrafish (danio rerio),”
eLife, vol. 6, p. e28158, Sep 2017."
REFERENCES,0.7388535031847133,"[10] F. Linda Liu, G. Kuo, N. Antipa, K. Yanny, and L. Waller, “Fourier diffuserscope: single-shot
3d fourier light field microscopy with a diffuser,” Optics Express, vol. 28, p. 28969, Sep 2020."
REFERENCES,0.7452229299363057,"[11] N. C. Pégard, H.-Y. Liu, N. Antipa, M. Gerlock, H. Adesnik, and L. Waller, “Compressive
light-field microscopy for 3d neural activity recording,” Optica, vol. 3, p. 517–524, May 2016."
REFERENCES,0.7515923566878981,"[12] R. Prevedel, Y.-G. Yoon, M. Hoffmann, N. Pak, G. Wetzstein, S. Kato, T. Schrödel, R. Raskar,
M. Zimmer, E. S. Boyden, and et al., “Simultaneous whole-animal 3d imaging of neuronal
activity using light-field microscopy,” Nature Methods, vol. 11, p. 727–730, Jul 2014."
REFERENCES,0.7579617834394905,"[13] O. Skocek, T. Nöbauer, L. Weilguny, F. Martínez Traub, C. N. Xia, M. I. Molodtsov, A. Grama,
M. Yamagata, D. Aharoni, D. D. Cox, and et al., “High-speed volumetric imaging of neuronal
activity in freely moving rodents,” Nature Methods, vol. 15, p. 429–432, Jun 2018."
REFERENCES,0.7643312101910829,"[14] K. Yanny, N. Antipa, R. Ng, L. Waller, and L. Waller, “Miniature 3d fluorescence microscope
using random microlenses,” in Biophotonics Congress: Optics in the Life Sciences Congress
2019 (BODA,BRAIN,NTM,OMA,OMP) (2019), paper BT3A.4, p. BT3A.4, Optical Society of
America, Apr 2019."
REFERENCES,0.7707006369426752,"[15] N. Wagner, F. Beuttenmueller, N. Norlin, J. Gierten, J. C. Boffi, J. Wittbrodt, M. Weigert,
L. Hufnagel, R. Prevedel, and A. Kreshuk, “Deep learning-enhanced light-field imaging with
continuous validation,” Nature Methods, vol. 18, p. 557–563, May 2021."
REFERENCES,0.7770700636942676,"[16] N. Antipa, G. Kuo, R. Heckel, B. Mildenhall, E. Bostan, R. Ng, and L. Waller, “Diffusercam:
lensless single-exposure 3d imaging,” Optica, vol. 5, p. 1–9, Jan 2018."
REFERENCES,0.7834394904458599,"[17] F. L. Liu, V. Madhavan, N. Antipa, G. Kuo, S. Kato, and L. Waller, “Single-shot 3d fluorescence
microscopy with fourier diffusercam,” in Biophotonics Congress: Optics in the Life Sciences
Congress 2019 (BODA,BRAIN,NTM,OMA,OMP) (2019), paper NS2B.3, p. NS2B.3, Optical
Society of America, Apr 2019."
REFERENCES,0.7898089171974523,"[18] M. S. Asif, A. Ayremlou, A. Veeraraghavan, R. Baraniuk, and A. Sankaranarayanan, “Flatcam:
Replacing lenses with masks and computation,” in 2015 IEEE International Conference on
Computer Vision Workshop (ICCVW), p. 663–666, Dec 2015."
REFERENCES,0.7961783439490446,"[19] K. Yanny, N. Antipa, W. Liberti, S. Dehaeck, K. Monakhova, F. L. Liu, K. Shen, R. Ng, and
L. Waller, “Miniscope3d: optimized single-shot miniature 3d fluorescence microscopy,” Light:
Science & Applications, vol. 9, p. 171, Oct 2020."
REFERENCES,0.802547770700637,"[20] M. B. Ahrens, M. B. Orger, D. N. Robson, J. M. Li, and P. J. Keller, “Whole-brain func-
tional imaging at cellular resolution using light-sheet microscopy,” Nature Methods, vol. 10,
p. 413–420, May 2013."
REFERENCES,0.8089171974522293,"[21] Y. Mu, D. V. Bennett, M. Rubinov, S. Narayan, C.-T. Yang, M. Tanimoto, B. D. Mensh, L. L.
Looger, and M. B. Ahrens, “Glia accumulate evidence that actions are futile and suppress
unsuccessful behavior,” Cell, vol. 178, pp. 27–43.e19, Jun 2019."
REFERENCES,0.8152866242038217,"[22] W. Yang and R. Yuste, “In vivo imaging of neural activity,” Nature Methods, vol. 14, p. 349–359,
Apr 2017."
REFERENCES,0.821656050955414,"[23] Y. Zhang, M. Rózsa, Y. Liang, D. Bushey, Z. Wei, J. Zheng, D. Reep, G. J. Broussard, A. Tsang,
G. Tsegaye, S. Narayan, C. J. Obara, J.-X. Lim, R. Patel, R. Zhang, M. B. Ahrens, G. C.
Turner, S. S.-H. Wang, W. L. Korff, E. R. Schreiter, K. Svoboda, J. P. Hasseman, I. Kolb, and
L. L. Looger, “Fast and sensitive gcamp calcium indicators for imaging neural populations,”
p. 2021.11.08.467793, Nov 2021."
REFERENCES,0.8280254777070064,"[24] A. S. Abdelfattah, T. Kawashima, A. Singh, O. Novak, H. Liu, Y. Shuai, Y.-C. Huang, L. Cam-
pagnola, S. C. Seeman, J. Yu, J. Zheng, J. B. Grimm, R. Patel, J. Friedrich, B. D. Mensh,
L. Paninski, J. J. Macklin, G. J. Murphy, K. Podgorski, B.-J. Lin, T.-W. Chen, G. C. Turner,
Z. Liu, M. Koyama, K. Svoboda, M. B. Ahrens, L. D. Lavis, and E. R. Schreiter, “Bright and
photostable chemigenetic indicators for extended in vivo voltage imaging,” Science, vol. 365,
p. 699–704, Aug 2019."
REFERENCES,0.8343949044585988,"[25] K. Monakhova, J. Yurtsever, G. Kuo, N. Antipa, K. Yanny, and L. Waller, “Learned reconstruc-
tions for practical mask-based lensless imaging,” Optics Express, vol. 27, p. 28075–28090, Sep
2019."
REFERENCES,0.8407643312101911,"[26] Z. Wang, L. Zhu, H. Zhang, G. Li, C. Yi, Y. Li, Y. Yang, Y. Ding, M. Zhen, S. Gao, and et al.,
“Real-time volumetric reconstruction of biological dynamics with light-field microscopy and
deep learning,” Nature Methods, p. 1–6, Feb 2021."
REFERENCES,0.8471337579617835,"[27] K. Yanny, K. Monakhova, R. W. Shuai, and L. Waller, “Deep learning for fast spatially varying
deconvolution,” Optica, vol. 9, p. 96–99, Jan 2022."
REFERENCES,0.8535031847133758,"[28] S. Diamond, V. Sitzmann, F. Heide, and G. Wetzstein, “Unrolled optimization with deep priors,”
arXiv e-prints, vol. 1705, p. arXiv:1705.08041, May 2017."
REFERENCES,0.8598726114649682,"[29] J. Dong, S. Roth, and B. Schiele, “Deep wiener deconvolution: Wiener meets deep learning for
image deblurring,” Advances in Neural Information Processing Systems, vol. 33, p. 1048–1059,
2020."
REFERENCES,0.8662420382165605,"[30] C. Peng, X. Zhang, G. Yu, G. Luo, and J. Sun, “Large kernel matters — improve semantic
segmentation by global convolutional network,” in 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), p. 1743–1751, IEEE, Jul 2017."
REFERENCES,0.8726114649681529,"[31] N. Mu, X. Xu, and X. Zhang, “Salient object detection in low contrast images via global
convolution and boundary refinement,” in 2019 IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops (CVPRW), p. 743–751, IEEE, Jun 2019."
REFERENCES,0.8789808917197452,"[32] O. Rippel, J. Snoek, and R. P. Adams, “Spectral representations for convolutional neural
networks,” in Advances in Neural Information Processing Systems (C. Cortes, N. Lawrence,
D. Lee, M. Sugiyama, and R. Garnett, eds.), vol. 28, Curran Associates, Inc., 2015."
REFERENCES,0.8853503184713376,"[33] E. Adelson and J. Wang, “Single lens stereo with a plenoptic camera,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 14, p. 99–106, Feb 1992."
REFERENCES,0.89171974522293,"[34] L. Grosenick, M. Broxton, C. K. Kim, C. Liston, B. Poole, S. Yang, A. Andalman, E. Scharff,
N. Cohen, O. Yizhar, and et al., “Identification of cellular-activity dynamics across large tissue
volumes in the mammalian brain,” bioRxiv, p. 132688, May 2017."
REFERENCES,0.8980891719745223,"[35] M. Broxton, Volume Reconstruction and Resolution Limits for Three Dimensional Snapshot
Microscopy. PhD thesis, Stanford University, Aug 2017."
REFERENCES,0.9044585987261147,"[36] J. Goodman, Introduction to Fourier Optics. Macmillan Learning, 4 ed., 2017."
REFERENCES,0.910828025477707,"[37] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image
segmentation,” in Medical Image Computing and Computer-Assisted Intervention – MICCAI
2015 (N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, eds.), Lecture Notes in Computer
Science, p. 234–241, Springer International Publishing, 2015."
REFERENCES,0.9171974522292994,"[38] A. V. Oppenheim and A. S. Willsky, Signals and Systems: Pearson New International Edition.
Pearson Education Limited, Jul 2013. Google-Books-ID: ut9oAQAACAAJ."
REFERENCES,0.9235668789808917,"[39] M. Mathieu, M. Henaff, and Y. LeCun, “Fast training of convolutional networks through ffts,”
arXiv:1312.5851 [cs], Mar 2014. arXiv: 1312.5851."
REFERENCES,0.9299363057324841,"[40] N. Vasilache, J. Johnson, M. Mathieu, S. Chintala, S. Piantino, and Y. LeCun, “Fast convolu-
tional nets with fbfft: A gpu performance evaluation,” arXiv:1412.7580 [cs], Apr 2015. arXiv:
1412.7580."
REFERENCES,0.9363057324840764,"[41] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing
internal covariate shift,” arXiv:1502.03167 [cs], Mar 2015. arXiv: 1502.03167.
[42] D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Instance normalization: The missing ingredient for
fast stylization,” arXiv:1607.08022 [cs], Nov 2017. arXiv: 1607.08022."
REFERENCES,0.9426751592356688,Checklist
REFERENCES,0.9490445859872612,1. For all authors...
REFERENCES,0.9554140127388535,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope? [Yes] See our summary in Section 4.
(b) Did you describe the limitations of your work? [Yes] We also discuss limitations in
Section 4.
(c) Did you discuss any potential negative societal impacts of your work? [Yes] We also
discuss broader impacts in Section 4.
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results..."
REFERENCES,0.9617834394904459,"(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments..."
REFERENCES,0.9681528662420382,"(a) Did you include the code, data, and instructions needed to reproduce the main ex-
perimental results (either in the supplemental material or as a URL)? [Yes] See Ap-
pendix A.4, A.6, A.8. We have also released our simulation and optimization library
(along with instructions to download data) at https://github.com/TuragaLab/
snapshotscope.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] See Appendix A.4, A.6, A.8.
(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes] See standard error of the mean values in Table 2.
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] See limitations in Section 4 and
Appendix A.6.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9745222929936306,"(a) If your work uses existing assets, did you cite the creators? [Yes] We have cited UNets
as [37] and DLMD as [25].
(b) Did you mention the license of the assets? [N/A]"
REFERENCES,0.9808917197452229,(c) Did you include any new assets either in the supplemental material or as a URL?
REFERENCES,0.9872611464968153,"[Yes] We have released our simulation and reconstruction network code at https:
//github.com/TuragaLab/snapshotscope.
(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [Yes] See footnote 1 about DiffuserCam dataset. Zebrafish data is our
own.
(e) Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.9936305732484076,"(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]"
