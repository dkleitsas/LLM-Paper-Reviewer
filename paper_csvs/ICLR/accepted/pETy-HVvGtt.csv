Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0027247956403269754,"We propose a framework to analyze how multivariate representations disentan-
gle ground-truth generative factors. A quantitative analysis of disentanglement
has been based on metrics designed to compare how one variable explains each
generative factor. Current metrics, however, may fail to detect entanglement that
involves more than two variables, e.g., representations that duplicate and rotate
generative factors in high dimensional spaces. In this work, we establish a frame-
work to analyze information sharing in a multivariate representation with Par-
tial Information Decomposition and propose a new disentanglement metric. This
framework enables us to understand disentanglement in terms of uniqueness, re-
dundancy, and synergy. We develop an experimental protocol to assess how in-
creasingly entangled representations are evaluated with each metric and conﬁrm
that the proposed metric correctly responds to entanglement. Through experi-
ments on variational autoencoders, we ﬁnd that models with similar disentangle-
ment scores have a variety of characteristics in entanglement, for each of which a
distinct strategy may be required to obtain a disentangled representation."
INTRODUCTION,0.005449591280653951,"1
INTRODUCTION"
INTRODUCTION,0.008174386920980926,"Disentanglement is a guiding principle for designing a learned representation separable into parts
that individually capture the underlying factors of variation. The concept is originally concerned as
an inductive bias towards obtaining representations aligned with the underlying factors of variation
in data (Bengio et al., 2013) and has been applied to controlling otherwise unstructured represen-
tations of data from several domains, e.g., images (Karras et al., 2019; Esser et al., 2019), text (Hu
et al., 2017), and audio (Hsu et al., 2019) to name just a few."
INTRODUCTION,0.010899182561307902,"While the concept is appealing, deﬁning disentanglement is not clear. After Higgins et al. (2017),
generative learning methods with regularized total correlation have been proposed (Kim & Mnih,
2018; Chen et al., 2018); however, it is still not clear if independence of latent variables is essential
for better disentanglement (Higgins et al., 2018). Furthermore, it is not obvious to measure disen-
tanglement given true generative factors. Towards understanding disentanglement, it is crucial to
deﬁne disentanglement metrics, for which several attempts have been made (Higgins et al., 2017;
Kim & Mnih, 2018; Chen et al., 2018; Eastwood & Williams, 2018; Do & Tran, 2020; Zaidi et al.,
2020); however, there are still problems to be solved."
INTRODUCTION,0.013623978201634877,"Current disentanglement metrics may fail to detect entanglement involving more than two variables.
In these metrics, one ﬁrst measures how each variable explains one generative factor and then com-
pares or contrasts them among variables. With such a procedure, we may overlook multiple variables
conveying information of one generative factor. For example, let z = (z1, z2) be a representation
consisting of two vectors, where each variable in z1 disentangles a distinct generative factor and z2
is a rotation of z1 not axis-aligned with the factors. Since any dimension of z2 alone may convey
little information of one generative factor, these metrics do not detect that multiple variables en-
code one generative factor redundantly. Although this is a simple example, this kind of information
sharing may arise in learned representations as well, if not the variables are linearly correlated."
INTRODUCTION,0.01634877384196185,"In this work, we present a disentanglement analysis framework aware of interactions among multiple
variables. Our key idea is to decompose the information of representation into entangled and disen-
tangled components using Partial Information Decomposition (PID), which is a framework in mod-
ern information theory to analyze information sharing among multiple random variables (Williams"
INTRODUCTION,0.01907356948228883,Published as a conference paper at ICLR 2022
INTRODUCTION,0.021798365122615803,"R(u; v1, v2)
U(u; v1 \ v2)
U(u; v2 \ v1)"
INTRODUCTION,0.02452316076294278,"C(u; v1, v2)"
INTRODUCTION,0.027247956403269755,"I(u; v1)
I(u; v2)"
INTRODUCTION,0.02997275204359673,"I(u; v1, v2)"
INTRODUCTION,0.0326975476839237,"U: unique information
R: redundant information
C: complementary information"
INTRODUCTION,0.035422343324250684,"Figure 1: Information diagram of three variable system in PID. Each circle represents mutual infor-
mation, and each area separated by them represents a decomposed term in PID. When we substitute
a generative factor for u, a latent variable for v1, and the other latent variables for v2, the unique
information U(u; v1 \v2) represents the information of the factor disentangled by the latent variable.
See Figure 5 in Appendix for the alternative form similar to the ones we will use in Section 3."
INTRODUCTION,0.03814713896457766,"& Beer, 2010). As illustrated in Figure 1, the mutual information I(u; v1, v2) = E
h
log
p(u,v1,v2)
p(u)p(v1,v2)
i"
INTRODUCTION,0.04087193460490463,"between a random variable u and a pair of random variables (v1, v2) is decomposed into four
nonnegative terms: unique information U(u; v1 \ v2) and U(u; v2 \ v1)1, redundant information
R(u; v1, v2), and complementary (or synergistic) information C(u; v1, v2). While these partial in-
formation terms have no agreed-upon concrete deﬁnitions yet (Bertschinger et al., 2014; Finn &
Lizier, 2018; Lizier et al., 2018; Finn & Lizier, 2020; Sigtermans, 2020), we can derive universal
lower and upper bounds of the partial information terms only with well-deﬁned mutual information
terms. We apply the PID framework to representations learned from data by letting u be a generative
factor and v1, v2 be one and the remaining latent variables, respectively. The unique information of
a latent variable intuitively corresponds to the amount of disentangled information, while the redun-
dant and complementary information correspond to different types of entangled information. We
can quantify disentanglement and multiple types of entanglement through the framework, which
enriches our understanding on disentangled representations."
INTRODUCTION,0.043596730245231606,Our contributions are summarized as follows.
INTRODUCTION,0.04632152588555858,"• PID-based disentanglement analysis framework: We propose a disentanglement anal-
ysis framework that captures interactions among multiple variables with PID. With this
framework, one can distinguish two different types of entanglement, namely redundancy
and synergy, which provide insights on how a representation entangles generative factors."
INTRODUCTION,0.04904632152588556,"• Tractable bounds of partial information terms: We derive lower and upper bounds of
partial information terms. We formulate a disentanglement metric, called UNIBOUND, us-
ing the lower bound of unique information. We design entanglement attacks, which inject
entanglement to a given disentangled representation, and conﬁrm through experiments us-
ing them that UNIBOUND effectively captures entanglement involving multiple variables."
INTRODUCTION,0.051771117166212535,"• Detailed analyses of learned representations: We analyze representations obtained by
variational autoencoders (VAEs). We observe that UNIBOUND sometimes disagrees with
other metrics, which indicates multi-variable interactions may dominate learned represen-
tations. We also observe that different types of entanglement arise in models learned with
different methods. This observation provides us an insight that we may require distinct
approaches to remove them for disentangled representation learning."
INTRODUCTION,0.05449591280653951,PROBLEM FORMULATION AND NOTATIONS
INTRODUCTION,0.05722070844686648,"Let x be a random variable representing a data point, drawn uniformly from a dataset D. Assume
that the true generative factors y = (y1, . . . , yK)⊤are available for each data point; in other words,
we can access the subset D(y) ⊂D of the data points with any ﬁxed generative factors y. Let z =
(z1, . . . , zL)⊤be a latent representation consisting of L random variables. An inference model is
provided as the conditional distribution p(z|x). Our goal is to evaluate how well the latent variables z
disentangle each generative factor in y. The inference model can integrate out the input as p(·|y) =
Ep(x|y)[p(·|x)] =
1
|D(y)|
P"
INTRODUCTION,0.05994550408719346,"x∈D(y) p(·|x); therefore, we only use y and z in most of our discussions."
INTRODUCTION,0.06267029972752043,"1Note that this \ is not a set difference operator. It is just a common notation used in the PID literature to
emphasize the unique information is not symmetric and resembles the set difference as depicted in Fig.1."
INTRODUCTION,0.0653950953678474,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0681198910081744,"We denote the mutual information between random variables u and v by I(u; v) = E
h
log
p(u,v)
p(u)p(v)
i
,"
INTRODUCTION,0.07084468664850137,"the entropy of u by H(u) = −E[log p(u)], and the conditional entropy of u given v by H(u|v) =
−E[log p(u|v)]. We denote a vector of zeros by 0, a vector of ones by 1, and an identity matrix by
I. We denote by N(µ, Σ) the Gaussian distribution with mean µ and covariance Σ and denote its
density function by N(·; µ, Σ)."
RELATED WORK,0.07356948228882834,"2
RELATED WORK"
RELATED WORK,0.07629427792915532,"The importance of representing data with multiple variables conveying distinct information has been
recognized at least since the ’80s (Barlow, 1989; Barlow et al., 1989; Schmidhuber, 1992). The
minimum entropy coding principle (Watanabe, 1981), which aims at representing data by random
variables z with the sum of minimum marginal entropies P"
RELATED WORK,0.07901907356948229,"ℓH(zℓ), is found to be useful for un-
supervised learning to remove the inherent redundancy in the sensory stimuli. The resulting rep-
resentation minimizes the total correlation and is called factorial coding. Recent advancements in
disentangled representation learning based on VAEs (Kingma & Welling, 2014) are guided by the
same principle as minimum entropy coding (Kim & Mnih, 2018; Chen et al., 2018; Gao et al., 2019)."
RELATED WORK,0.08174386920980926,"Understanding better representations, which is tackled from the coding side as above, is also ap-
proached from the generative perspective. It is often expected that data are generated from genera-
tive factors through a process that entangles them into high dimensional sensory space (DiCarlo &
Cox, 2007). As generative factors are useful as the basis of downstream learning tasks, obtaining
disentangled representations from data is a hot topic of representation learning (Bengio et al., 2013).
Towards learning disentangled representations, it is arguably important to quantitatively measure
disentanglement. In that regard, Higgins et al. (2017) established a standard evaluation procedure
using controlled datasets with balanced and fully-annotated ground-truth factors. A variety of met-
rics have then been proposed on the basis of the procedure. Among them, Higgins et al. (2017) and
Kim & Mnih (2018) propose metrics based on the deviation of each latent variable conditioned by
a generative factor. In contrast, Mutual Information Gap (MIG) (Chen et al., 2018) and its variants
(Do & Tran, 2020; Zaidi et al., 2020) are based on mutual information between a latent variable and
a generative factor. We extend the latter direction, considering multi-variable interactions."
RELATED WORK,0.08446866485013624,"Barlow (1989) discussed redundancy by comparing the population and the individual variables by
their entropies, i.e., total correlation. It is though less trivial to measure redundancy as an informa-
tion quantity. The PID framework (Williams & Beer, 2010) provides an approach to understanding
redundancy among multiple random variables as a constituent of mutual information. The frame-
work provides some desirable relationships between decomposed information terms, while it leaves
some degrees of freedom to determine all of them, for which several deﬁnitions have been proposed
(Williams & Beer, 2010; Bertschinger et al., 2014; Finn & Lizier, 2018; 2020; Sigtermans, 2020)."
RELATED WORK,0.08719346049046321,"The PID framework has been applied to machine learning models. For example, Tax et al. (2017)
measured the PID terms for restricted Boltzmann machines using the deﬁnition of Williams & Beer
(2010). Yu et al. (2021) took an alternative route, similar to our approach, where they measured lin-
ear combinations of PID terms by corresponding linear combinations of mutual information terms.
These work aims at analyzing the learning dynamics of models in supervised settings. In contrast,
we use the PID framework for analyzing disentanglement in unsupervised representation learning."
PARTIAL INFORMATION DECOMPOSITION FOR DISENTANGLEMENT,0.08991825613079019,"3
PARTIAL INFORMATION DECOMPOSITION FOR DISENTANGLEMENT"
PARTIAL INFORMATION DECOMPOSITION FOR DISENTANGLEMENT,0.09264305177111716,"In this section, we analyze the current metrics and introduce our framework. In Section 3.1, we
introduce PID of the system we concern. In Section 3.2, we investigate the current metrics in terms
of multi-variable interactions. In Section 3.3 and 3.4, we construct our disentanglement metric with
bounds for PID terms. We provide a method of computing the bounds in Section 3.5."
"PARTIAL INFORMATION DECOMPOSITION
WE TACKLE THE PROBLEM OF EVALUATING DISENTANGLEMENT OF A LATENT REPRESENTATION Z RELATIVE TO THE TRUE",0.09536784741144415,"3.1
PARTIAL INFORMATION DECOMPOSITION
We tackle the problem of evaluating disentanglement of a latent representation z relative to the true
generative factors y from an information-theoretic perspective. Let us consider evaluating how one
generative factor yk is captured by the latent representation z. The information of yk captured by z
is measured using mutual information I(yk; z) = H(z) −H(z|yk)."
"PARTIAL INFORMATION DECOMPOSITION
WE TACKLE THE PROBLEM OF EVALUATING DISENTANGLEMENT OF A LATENT REPRESENTATION Z RELATIVE TO THE TRUE",0.09809264305177112,"In a desirably disentangled representation, we expect one of the latent variables zℓto exclusively
capture the information of the factor yk. To evaluate a given representation, we are interested in"
"PARTIAL INFORMATION DECOMPOSITION
WE TACKLE THE PROBLEM OF EVALUATING DISENTANGLEMENT OF A LATENT REPRESENTATION Z RELATIVE TO THE TRUE",0.1008174386920981,Published as a conference paper at ICLR 2022
"PARTIAL INFORMATION DECOMPOSITION
WE TACKLE THE PROBLEM OF EVALUATING DISENTANGLEMENT OF A LATENT REPRESENTATION Z RELATIVE TO THE TRUE",0.10354223433242507,"understanding how the information is distributed between a latent variable zℓand the remaining
representation z\ℓ= (zℓ′)ℓ′̸=ℓ. This is best described by the PID framework, where the mutual
information is decomposed into the following four terms.
I(yk; z) = R(yk; zℓ, z\ℓ) + U(yk; zℓ\ z\ℓ) + U(yk; z\ℓ\ zℓ) + C(yk; zℓ, z\ℓ).
(1)
Here, the decomposed terms represent the following non-negative quantities."
"PARTIAL INFORMATION DECOMPOSITION
WE TACKLE THE PROBLEM OF EVALUATING DISENTANGLEMENT OF A LATENT REPRESENTATION Z RELATIVE TO THE TRUE",0.10626702997275204,"• Redundant information R(yk; zℓ, z\ℓ) is the information of yk held by both zℓand z\ℓ."
"PARTIAL INFORMATION DECOMPOSITION
WE TACKLE THE PROBLEM OF EVALUATING DISENTANGLEMENT OF A LATENT REPRESENTATION Z RELATIVE TO THE TRUE",0.10899182561307902,"• Unique information U(yk; zℓ\ z\ℓ) is the information of yk held by zℓand not held by z\ℓ.
The opposite term U(yk; z\ℓ\ zℓ) is also deﬁned by exchanging the roles of zℓand z\ℓ."
"PARTIAL INFORMATION DECOMPOSITION
WE TACKLE THE PROBLEM OF EVALUATING DISENTANGLEMENT OF A LATENT REPRESENTATION Z RELATIVE TO THE TRUE",0.11171662125340599,"• Complementary information (or synergistic information) C(yk; zℓ, z\ℓ) is the information
of yk held by z = (zℓ, z\ℓ) that is not held by either zℓor z\ℓalone."
"PARTIAL INFORMATION DECOMPOSITION
WE TACKLE THE PROBLEM OF EVALUATING DISENTANGLEMENT OF A LATENT REPRESENTATION Z RELATIVE TO THE TRUE",0.11444141689373297,"The following identities, combined with Eq.1, partially characterize each term.
I(yk; zℓ) = R(yk; zℓ, z\ℓ) + U(yk; zℓ\ z\ℓ), I(yk; z\ℓ) = R(yk; zℓ, z\ℓ) + U(yk; z\ℓ\ zℓ). (2)
The decomposition of this system is illustrated in Figure 2a."
"PARTIAL INFORMATION DECOMPOSITION
WE TACKLE THE PROBLEM OF EVALUATING DISENTANGLEMENT OF A LATENT REPRESENTATION Z RELATIVE TO THE TRUE",0.11716621253405994,"We expect disentangled representations to concentrate the information of yk to a single latent vari-
able zℓ, and to let the other variables z\ℓnot convey the information in either unique, redundant,
or synergistic ways. This is understood in terms of PID as maximizing the unique information
U(yk; zℓ\ z\ℓ) while minimizing the other parts of the decomposition."
"PARTIAL INFORMATION DECOMPOSITION
WE TACKLE THE PROBLEM OF EVALUATING DISENTANGLEMENT OF A LATENT REPRESENTATION Z RELATIVE TO THE TRUE",0.11989100817438691,"The above formulation is incomplete as one degree of freedom remains to determine the four terms
with the three equalities. Instead of stepping into searching for suitable deﬁnitions of these terms, we
build discussions applicable to any such deﬁnitions that fulﬁll the above incomplete requirements2."
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.1226158038147139,"3.2
UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
Current disentanglement metrics are typically designed to measure how each latent variable captures
a factor and compare it among latent variables, i.e., output a high score when only one latent variable
captures the factor well."
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.12534059945504086,"For example, the BetaVAE metric (Higgins et al., 2017) is computed by estimating the mean abso-
lute difference (MAD) of two i.i.d. variables following p(zℓ|yk) = Ep(x|yk)p(zℓ|x) for each ℓby
Monte-Carlo sampling and training a linear classiﬁer that predicts k from the noisy estimations of
the differences. The FactorVAE metric (Kim & Mnih, 2018) is computed similarly, except that the
MAD is replaced with variance (following normalization by population), and a majority-vote clas-
siﬁer is used to eliminate a failure mode of ignoring one of the factors and to avoid depending on
hyperparameters. These metrics have the same goal of ﬁnding a mapping between ℓand k by com-
paring the deviation of zℓwhen ﬁxing yk. Since the deviation is computed for each zℓseparately,
these metrics do not count how each latent variable zℓinteracts with the other variables z\ℓ."
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.12806539509536785,"Another example is MIG (Chen et al., 2018), which compares mutual information I(yk; zℓ) for all
ℓand uses the gap between the maximum and the second maximum among them. More precisely,
MIG is deﬁned by the following formula."
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.1307901907356948,"MIG = 1 K K
X k=1"
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.1335149863760218,"1
H(yk) max
ℓ
min
ℓ′̸=ℓ(I(yk; zℓ) −I(yk; zℓ′)).
(3)"
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.1362397820163488,"Here, dividing each summand by H(yk) balances the contribution of each factor when they are
discrete. The difference in mutual information is rewritten as a difference in unique information as
I(yk; zℓ) −I(yk; zℓ′) = U(yk; zℓ\ zℓ′) −U(yk; zℓ′ \ zℓ) ≤U(yk; zℓ\ zℓ′).
(4)
In that sense, MIG effectively captures the pairwise interactions between latent variables. This
metric still ignores interplays between more than two variables. Figure 2b reveals that some of
the redundant information R(yk; zℓ, z\ℓ) is positively evaluated in MIG, which should have been
considered as a signal of entanglement. Note that there are several extensions to MIG (Do & Tran,
2020; Li et al., 2020; Zaidi et al., 2020); see Appendix B for detailed discussions on them."
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.13896457765667575,"2Most studies on PID only deal with discrete systems, while deep representations often include continuous
variables. There have been attempts to deﬁne and analyze PID for continuous systems (Barrett, 2015; Schick-
Poland et al., 2021; Pakman et al., 2021). Note that the domain of variables, which our framework depends on,
is not limited with the PID framework."
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.14168937329700274,Published as a conference paper at ICLR 2022
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.1444141689373297,I(yk; z)
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.14713896457765668,I(yk; zℓ)
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.14986376021798364,I(yk; z\ℓ)
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.15258855585831063,"U(yk; zℓ\ z\ℓ)
R(yk; zℓ, z\ℓ)
U(yk; z\ℓ\ zℓ)
C(yk; zℓ, z\ℓ)"
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.1553133514986376,"(a) PID for systems with yk, zℓ, and z\ℓ"
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.15803814713896458,"MIG
UNIBOUND"
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.16076294277929154,I(yk; z)
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.16348773841961853,I(yk; zℓ)
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.16621253405994552,I(yk; z\ℓ)
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.16893732970027248,I(yk; zℓ′)
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.17166212534059946,"U(yk; zℓ\ zℓ′)
R(yk; zℓ, zℓ′) U(yk; zℓ′ \ zℓ)
U(yk; zℓ\ z\ℓ)
R(yk; zℓ, z\ℓ)
U(yk; z\ℓ\ zℓ)
C(yk; zℓ, z\ℓ)"
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.17438692098092642,(b) Side-by-side comparison of positive and negative terms in UNIBOUND and MIG
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.1771117166212534,"Figure 2: Information diagrams depicted by bands (the style borrowed from Figure 8.1 of MacKay
(2003)). White boxes represent mutual information, which we can compute. (a) The bands depict the
decomposition used in the PID-based disentanglement evaluation. (b) This diagram superposes the
decomposition for systems with (yk, zℓ, z\ℓ) and (yk, zℓ, zℓ′) where zℓ′ is the latent variable chosen
by MIG evaluation. The green boxes are positively evaluated in MIG (the top colored line) and
UNIBOUND (the bottom colored line), while the red boxes are negatively evaluated in them. Observe
that MIG positively evaluates a part of the redundancy, namely R(yk; zℓ, z\ℓ) −R(yk; zℓ, zℓ′), as it
does not take into account the interactions among strict supersets of {zℓ, zℓ′}."
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.17983651226158037,"3.3
UNIBOUND: NOVEL DISENTANGLEMENT METRIC
We can lower bound the unique information in any possible PID deﬁnitions by computable compo-
nents, as we did in Eq.4. To bound U(yk; zℓ\z\ℓ) instead of U(yk; zℓ\zℓ′), we replace zℓ′ with z\ℓ,
obtaining"
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.18256130790190736,"U(yk; zℓ\ z\ℓ) ≥

U(yk; zℓ\ z\ℓ) −U(yk; z\ℓ\ zℓ)
"
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.18528610354223432,"+ =

I(yk; zℓ) −I(yk; z\ℓ)
 +
(5)"
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.1880108991825613,"where we use [·]+ = max{0, ·} as the difference in mutual information is not guaranteed to be non-
negative. The decomposed terms evaluated by the bound is illustrated in the lower part of Figure
2b. It effectively excludes, from the positive term, the effect of interaction between zℓand any other
latent variables. In a similar way to MIG, we summarize this bound over all the generative factors
to obtain the metric we call UNIBOUND."
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.1907356948228883,"UNIBOUND := 1 K K
X k=1"
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.19346049046321526,"1
H(yk) max
ℓ

I(yk; zℓ) −I(yk; z\ℓ)
"
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.19618528610354224,"+.
(6)"
"UNDERSTANDING DISENTANGLEMENT METRICS FROM INTERACTION PERSPECTIVE
CURRENT DISENTANGLEMENT METRICS ARE TYPICALLY DESIGNED TO MEASURE HOW EACH LATENT VARIABLE CAPTURES",0.1989100817438692,"Dividing each summand by the entropy H(yk) has the same role as in MIG; it makes the evaluation
fair between factors and eases the comparison as the metric is normalized when yk is discrete."
OTHER BOUNDS FOR PARTIAL INFORMATION TERMS,0.2016348773841962,"3.4
OTHER BOUNDS FOR PARTIAL INFORMATION TERMS
The UNIBOUND metric is a handy quantity to compare representations by a single scalar, while PID
itself may provide more ideas on how a given representation entangles or disentangles the factors.
To fully leverage the potential, we derive bounds for all the terms of interest, including redundancy
and synergy terms, from both lower and upper sides."
OTHER BOUNDS FOR PARTIAL INFORMATION TERMS,0.20435967302452315,"Let II(yk; zℓ; z\ℓ) = I(yk; zℓ) + I(yk; z\ℓ) −I(yk; z) be the interaction information of a triple
(yk, zℓ, z\ℓ). Using nonnegativity of PID terms, we can derive the following bounds from Eq.1-2.

I(yk; zℓ) −I(yk; z\ℓ)
"
OTHER BOUNDS FOR PARTIAL INFORMATION TERMS,0.20708446866485014,"+ ≤U(yk; zℓ\ z\ℓ) ≤I(yk; zℓ) −

II(yk; zℓ; z\ℓ)
"
OTHER BOUNDS FOR PARTIAL INFORMATION TERMS,0.2098092643051771,"+,

II(yk; zℓ; z\ℓ)
"
OTHER BOUNDS FOR PARTIAL INFORMATION TERMS,0.2125340599455041,"+ ≤R(yk; zℓ, z\ℓ) ≤min{I(yk; zℓ), I(yk; z\ℓ)},

−II(yk; zℓ; z\ℓ)
"
OTHER BOUNDS FOR PARTIAL INFORMATION TERMS,0.21525885558583105,"+ ≤C(yk; zℓ, z\ℓ) ≤min{I(yk; zℓ), I(yk; z\ℓ)} −II(yk; zℓ; z\ℓ). (7)"
OTHER BOUNDS FOR PARTIAL INFORMATION TERMS,0.21798365122615804,Published as a conference paper at ICLR 2022
OTHER BOUNDS FOR PARTIAL INFORMATION TERMS,0.22070844686648503,"Note that all six bounds are computed by arithmetics on I(yk; zℓ), I(yk; z\ℓ), and I(yk; z). We can
summarize each lower bound in a similar way as we did in Eq.6 and summarize the corresponding
upper bound using the same ℓfor each k as the lower bound. While these bounds only determine
the terms as intervals, they provide us enough insight into the type of entanglement dominant in the
representation (redundancy or synergy)."
"ESTIMATING BOUNDS BY EXACT LOG-MARGINAL DENSITIES
WHEN THE DATASET IS FULLY ANNOTATED WITH DISCRETE GENERATIVE FACTORS AND THE INFERENCE DISTRIBUTION",0.22343324250681199,"3.5
ESTIMATING BOUNDS BY EXACT LOG-MARGINAL DENSITIES
When the dataset is fully annotated with discrete generative factors and the inference distribution
p(z|x) and its marginals p(zℓ|x), p(z\ℓ|x) are all tractable (e.g., mean ﬁeld variational models), we
can compute the bounds in a similar way as is done by Chen et al. (2018) for MIG. Let zS be either of
zℓ, z\ℓor z. We denote by D(yk) the subset of the dataset D with a speciﬁc value of yk. The mutual
information I(yk; zS) = −H(zS|yk) + H(zS) can then be computed by the following formula."
"ESTIMATING BOUNDS BY EXACT LOG-MARGINAL DENSITIES
WHEN THE DATASET IS FULLY ANNOTATED WITH DISCRETE GENERATIVE FACTORS AND THE INFERENCE DISTRIBUTION",0.22615803814713897,"I(yk; zS) = Ep(yk,zS) "
"ESTIMATING BOUNDS BY EXACT LOG-MARGINAL DENSITIES
WHEN THE DATASET IS FULLY ANNOTATED WITH DISCRETE GENERATIVE FACTORS AND THE INFERENCE DISTRIBUTION",0.22888283378746593,"log
1
|D(yk)| X"
"ESTIMATING BOUNDS BY EXACT LOG-MARGINAL DENSITIES
WHEN THE DATASET IS FULLY ANNOTATED WITH DISCRETE GENERATIVE FACTORS AND THE INFERENCE DISTRIBUTION",0.23160762942779292,"x∈D(yk)
p(zS|x) "
"ESTIMATING BOUNDS BY EXACT LOG-MARGINAL DENSITIES
WHEN THE DATASET IS FULLY ANNOTATED WITH DISCRETE GENERATIVE FACTORS AND THE INFERENCE DISTRIBUTION",0.23433242506811988,"−Ep(yk,zS) "" log 1 |D| X"
"ESTIMATING BOUNDS BY EXACT LOG-MARGINAL DENSITIES
WHEN THE DATASET IS FULLY ANNOTATED WITH DISCRETE GENERATIVE FACTORS AND THE INFERENCE DISTRIBUTION",0.23705722070844687,"x∈D
p(zS|x) # ,
(8)"
"ESTIMATING BOUNDS BY EXACT LOG-MARGINAL DENSITIES
WHEN THE DATASET IS FULLY ANNOTATED WITH DISCRETE GENERATIVE FACTORS AND THE INFERENCE DISTRIBUTION",0.23978201634877383,"Assuming that each generative factor yk is discrete and uniform, we employ stratiﬁed sampling
over p(yk, zS). We approximate the expectation over p(zS|yk) by sampling x from the subset
D(yk) and then sampling zS from p(z|x) to avoid quadratic computational cost. Following Chen
et al. (2018), we used the sample size of 10000 in experiments. We use log-sum-exp function to
compute log P p(zS|x) = log P exp(log p(zS|x)) for numerical stability. The PID bounds and the
UNIBOUND metric are computed by combining these estimations."
"ESTIMATING BOUNDS BY EXACT LOG-MARGINAL DENSITIES
WHEN THE DATASET IS FULLY ANNOTATED WITH DISCRETE GENERATIVE FACTORS AND THE INFERENCE DISTRIBUTION",0.24250681198910082,"When the inference distribution p(z|x) is factorized, its log marginal log p(zS|x) is computed by
just adding up the log marginal of each variable as P"
"ESTIMATING BOUNDS BY EXACT LOG-MARGINAL DENSITIES
WHEN THE DATASET IS FULLY ANNOTATED WITH DISCRETE GENERATIVE FACTORS AND THE INFERENCE DISTRIBUTION",0.2452316076294278,"ℓ∈S log p(zℓ|x). Otherwise, we need to ex-
plicitly derive the marginal distribution and compute the log density. For example, when p(z|x) is
a Gaussian distribution with mean µ and non-diagonal covariance Σ, p(zS|x) is a Gaussian distri-
bution with mean (µi)i∈S and covariance (Σij)i,j∈S. Such a case arises for the attacked model we
will describe in the next section."
"ESTIMATING BOUNDS BY EXACT LOG-MARGINAL DENSITIES
WHEN THE DATASET IS FULLY ANNOTATED WITH DISCRETE GENERATIVE FACTORS AND THE INFERENCE DISTRIBUTION",0.24795640326975477,"Let M be the sample size for the expectations, and N = |D| be the size of the dataset. Then, the
computation of Eq.8 requires O(MN) evaluations of the conditional density p(zS|x)."
ENTANGLEMENT ATTACKS,0.2506811989100817,"4
ENTANGLEMENT ATTACKS"
ENTANGLEMENT ATTACKS,0.25340599455040874,"To conﬁrm that the proposed framework effectively captures interactions among multiple latent
variables, we apply it to adversarial representations that entangle any generative factors. Instead
of making a single artiﬁcial model, we modify a given model that disentangles factors well into a
noised version that entangles the original variables. We call this process an entanglement attack."
ENTANGLEMENT ATTACKS,0.2561307901907357,"Let z ∈RL be the representation deﬁned by a given model. Our goal is to design a transform from
z to an attacked representation ˜z so that disentanglement metrics fail to capture entanglement unless
they correctly handle multi-variable interactions."
ENTANGLEMENT ATTACKS,0.25885558583106266,"As we mentioned in Section 3, metrics that do not take into account the interaction among multiple
latent variables may underestimate redundant information. To crystallize such a situation, we ﬁrst
design an entanglement attack to inject redundant information into multiple variables. For complete-
ness, we also design a similar attack to inject synergistic information as well."
REDUNDANCY ATTACK,0.2615803814713896,"4.1
REDUNDANCY ATTACK
Let U be an L × L orthonormal matrix and ϵ ∈RL be a random vector following the standard
normal distribution N(0, I). Using a hyperparameter α ≥0, we deﬁne the redundancy attack by"
REDUNDANCY ATTACK,0.26430517711171664,"˜zred =

z
αUz + ϵ"
REDUNDANCY ATTACK,0.2670299727520436,"
.
(9)"
REDUNDANCY ATTACK,0.26975476839237056,"The coefﬁcient α adjusts the degree of entanglement. When α = 0, the new representation just
appends noise elements to a given representation, which does not affect the disentanglement. In-
creasing α makes the additional dimensions less noisy, resulting in ˜zred redundantly encoding the
information of factors conveyed by the original representation. The mixing matrix U chooses how
the information of individual variables in z is distributed to the additional dimensions. If we choose"
REDUNDANCY ATTACK,0.2724795640326976,Published as a conference paper at ICLR 2022
REDUNDANCY ATTACK,0.27520435967302453,"U = I, the dimensions are not mixed; thus considering one-to-one interaction between pairs of
variables is enough to capture the redundancy. To mix the dimensions, we can use U = I −2"
REDUNDANCY ATTACK,0.2779291553133515,"L11⊤
instead, which is an orthonormal matrix that mixes each variable with the others."
REDUNDANCY ATTACK,0.28065395095367845,"To evaluate mutual information terms for MIG and UNIBOUND metrics after the attack, we need
an explicit formula for the inference distribution p(˜zred|x). When the original model has a Gaussian
inference distribution p(z|x) = N(z; µ(x), Σ(x)), the attacked model is a summation of linear
transformation of z and a standard normal noise, which results in a Gaussian distribution"
REDUNDANCY ATTACK,0.28337874659400547,"p(˜zred|x) = N

˜zred;

µ(x)
αUµ(x)"
REDUNDANCY ATTACK,0.28610354223433243,"
,

Σ(x)
αΣ(x)U⊤"
REDUNDANCY ATTACK,0.2888283378746594,"αUΣ(x)
I + α2UΣ(x)U⊤ 
."
SYNERGY ATTACK,0.29155313351498635,"4.2
SYNERGY ATTACK
For completeness, we also deﬁne an attack that entangles representation by increasing synergistic
information. Using the same setting with an L × L orthonormal matrix U and a noise vector ϵ ∼
N(0, I), we construct the synergy attack by"
SYNERGY ATTACK,0.29427792915531337,"˜zsyn =

αUϵ + z
ϵ"
SYNERGY ATTACK,0.2970027247956403,"
.
(10)"
SYNERGY ATTACK,0.2997275204359673,"Here again, the coefﬁcient α adjusts the degree of entanglement. When α = 0, the attacked version
just extends the original representation with independent noise elements. By increasing α, the noise
are mixed with the parts conveying the information of factors. The attacked vector ˜zsyn fully conveys
the information of factors in z regardless of α, as we can recover the original representation by z =
˜zsyn
1:L−αU˜zsyn
L+1:2L. Note that most existing metrics correctly react to this attack since the information
of individual variables is destroyed by the noise. The upper bound of the unique information is one
of the quantities that positively evaluate synergistic information and is expected to ignore the attack."
EVALUATION,0.3024523160762943,"5
EVALUATION"
EVALUATION,0.30517711171662126,"We evaluated the metrics in a toy model in Section 5.1 and in VAEs trained on datasets with the true
generative factors in Section 5.2. We also performed detailed analyses of VAEs by plotting the PID
terms of each factor, which is deferred to Appendix E due to the limited space."
EXACT ANALYSIS WITH TOY MODEL,0.3079019073569482,"5.1
EXACT ANALYSIS WITH TOY MODEL
We consider a toy model with attacks deﬁned in Section 4 as a sanity check. Suppose that data are
generated by factors y ∼N(0, I), and we have a latent representation that disentangles them up to
noise as z|y ∼N(y, σ2I), where σ > 0. With this simple setting, we can analytically compute MIG
and UNIBOUND. For example, when we set σ = 0.1 and K = 5, we can derive the scores after the
redundancy attack as MIG = 1"
EXACT ANALYSIS WITH TOY MODEL,0.3106267029972752,"c log

101 × 1+0.65α2"
EXACT ANALYSIS WITH TOY MODEL,0.3133514986376022,"1+1.01α2

and UNIBOUND = 1"
EXACT ANALYSIS WITH TOY MODEL,0.31607629427792916,"c log

101 × 1+0.01α2"
EXACT ANALYSIS WITH TOY MODEL,0.3188010899182561,"1+1.01α2

,"
EXACT ANALYSIS WITH TOY MODEL,0.3215258855585831,"where c = log(2πe). As a function of α, UNIBOUND decreases faster than MIG. The difference
comes from the information distributed among the added dimensions of the attacked vector; while
UNIBOUND counts all the entangled information, MIG only deals with one of the added dimensions.
Indeed, the amount of the untracked entanglement remains in MIG after taking the limit of α →∞
as MIG →1"
EXACT ANALYSIS WITH TOY MODEL,0.3242506811989101,"c log 65, while UNIBOUND →0."
EXACT ANALYSIS WITH TOY MODEL,0.32697547683923706,"We can also compute the scores after the synergy attack as MIG
=
UNIBOUND
="
C LOG,0.329700272479564,"1
c log

1 +
1
α2+0.01

, where both metrics correctly capture the injected synergy. Refer to Appendix
C for the derivations of the exact scores for arbitrary parameter choice."
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.33242506811989103,"5.2
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS
We used the DSPRITES and 3DSHAPES datasets for our analysis. The DSPRITES dataset consists
of 737, 280 binary images generated from ﬁve generative factors (shape, size, rotation, and x/y co-
ordinates). The 3DSHAPES dataset consists of 480, 000 color images generated from six generative
factors (ﬂoor/wall/object hues, scale, shape, and orientation). All the images have 64 × 64 pixels.
We used all the factors for y, encoded as a set of discrete (categorical) random variables."
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.335149863760218,"We used variants of VAEs as methods for disentangled representation learning: β-VAE (Higgins
et al., 2017), FactorVAE (Kim & Mnih, 2018), β-TCVAE (Chen et al., 2018), and JointVAE
(Dupont, 2018). We trained all the models with six latent variables, one of which in JointVAE is"
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.33787465940054495,Published as a conference paper at ICLR 2022
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.3405994550408719,"DSPRITES
3DSHAPES"
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.34332425068119893,"(a) Disentanglement scores
(b) PID terms
(c) Redundancy attacks"
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.3460490463215259,"Figure 3: Experimental results for VAEs trained with DSPRITES (top row) and 3DSHAPES (bottom
row). (a) Disentanglement scores. (b) Estimated PID terms. Three orange bars in each plot represent
the possible values of unique (U), redundant (R), and complementary (C) information, respectively.
The top and bottom of each orange area correspond to the upper and lower bounds of the term,
computed with Eq.7. (c) Disentanglement scores of β-VAE and β-TCVAE after redundancy attack
with varying strength. See Appendix H for a larger version of the plots."
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.34877384196185285,"a three-way categorical variable for DSPRITES and a four-way categorical variable for 3DSHAPES.
We trained each model eight times with different random seeds and chose the best half of them for
each metric to avoid cluttered results due to training instability. We optimized network weights with
Adam (Kingma & Ba, 2015). We used the standard convolutional networks used in the literature for
the encoder and the decoder. See Appendix D for the details of architectures and hyperparameters."
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.35149863760217986,"For disentanglement metrics, we compare BetaVAE metric (Higgins et al., 2017), FactorVAE metric
(Kim & Mnih, 2018), MIG (Chen et al., 2018), and the proposed UNIBOUND metric."
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.3542234332425068,"We ﬁrst compared the models with each metric as shown in Figure 3a. The trend is basically similar
between UNIBOUND and the other metrics, while they disagree in some cases. For example, Factor-
VAE achieves a higher MIG score for DSPRITES than β-VAE, while its UNIBOUND score is low. As
we saw in Figure 2b, such a case occurs when a part of the redundancy R(yk; zℓ, z\ℓ)−R(yk; zℓ, zℓ′)
is large. This observation indicates that FactorVAE effectively forces each variable to encode infor-
mation of distinct factors (i.e., one-vs-one redundancy is small), while it fails to avoid entangling
the information over multiple variables (i.e., one-vs-all redundancy is large)."
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.3569482288828338,"We can conﬁrm that the redundancy is indeed large in FactorVAE by computing the PID bounds. In
Figure 3b, we plot the aggregated bounds of U(yk; zℓ\ z\ℓ), R(yk; zℓ, z\ℓ), and C(yk; zℓ, z\ℓ). The
plot reveals that FactorVAE tends to encode the factors redundantly into multiple latent variables."
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.35967302452316074,"To understand the tasks and models more deeply, we evaluated the PID bounds for each factor in
Figure 4. We can see that the factors are captured by the models in various ways. For example, β-
TCVAE succeeds to disentangle the position and scale factors in DSPRITES, while it encodes orien-
tation synergistically. It may reﬂect the inherent difﬁculty of disentangling this factor in DSPRITES,
as the image of each shape corresponding to 0◦orientation is chosen arbitrarily. These observations
help us to choose what kind of inductive biases to introduce into the models."
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.36239782016348776,Published as a conference paper at ICLR 2022
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.3651226158038147,"DSPRITES
3DSHAPES"
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.3678474114441417,"(a) β-VAE
(b) FactorVAE
(c) β-TCVAE
(d) JointVAE"
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.37057220708446864,"Figure 4: Estimated PID terms of each factor in DSPRITES and 3DSHAPES. As in Figure 3b, three
orange bars in each plot show the range between lower and upper bound estimations of unique
information (U), redundant information (R), and complementary information (C). See Figure 6 in
Appendix for a larger version and Table 5 for qualitative interpretations."
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.37329700272479566,"FactorVAE, on the other hand, tends to encode factors redundantly. This indicates FactorVAE suc-
ceeds to make individual variables encode each factor, while it fails to prevent other variables from
encoding the same information. JointVAE also encodes factors redundantly. While it fails to disen-
tangle all the factors, this is the only model that encodes the shape and orientation to single latent
variables. This can be viewed as the effect of introducing a discrete variable into the representation."
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.3760217983651226,"We can understand large redundancy as the models failing to make variables independent enough.
The lower bound of redundancy in Eq.7 is related to independence as"
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.3787465940054496,"[II(yk; zℓ; z\ℓ)]+ = [I(zk; z\ℓ) −I(zℓ; z\ℓ|yk)]+ ≤I(zk; z\ℓ).
(11)"
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.3814713896457766,"Therefore, a high redundancy lower bound indicates large mutual information I(zk; z\ℓ); i.e., the
latent variables are highly dependent. We conjecture that FactorVAE, which approximates the total
correlation DKL(p(z)∥Q"
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.38419618528610355,"ℓp(zℓ)) by a critic, fails to make the critic capture the dependency in z
enough in our experiments. Since the MIG score is relatively high, the critic succeeds to capture
pairwise dependency, while it fails to capture higher dimensional dependency. The high redundancy
in JointVAE can also be explained by the lack of independence; see Appendix F for details."
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.3869209809264305,"As in Figure 3a, some metrics have large deviations from the median. This is caused by the random-
ness in training rather than in evaluation; see Appendix I for detailed analyses."
EMPIRICAL ANALYSIS WITH ANNOTATED DATASETS,0.3896457765667575,"As we did for the toy model, we performed entanglement attacks in Figure 3c to assess its effect on
each metric in learned representations. We selected the best training trials of β-VAE and β-TCVAE.
Here, we only plot the results with the redundancy attack, as each metric already behaves well
against the synergy attack. The plot reveals that BetaVAE and FactorVAE metrics do not detect the
redundancy injected by the attack. MIG slightly decreases with the attack, which is not signiﬁcant
against the score variation between learning methods as we observed in Figure 3a. UNIBOUND
strongly reacts against the attack, indicating that it effectively detects the injected redundancy."
CONCLUSION,0.3923705722070845,"6
CONCLUSION"
CONCLUSION,0.39509536784741145,"We established a framework of disentanglement analysis using Partial Information Decomposition.
We formulated a new disentanglement metric, UNIBOUND, using the unique information bounds,
and conﬁrmed with entanglement attacks that the metric correctly responses to entanglement caused
by multi-variable interactions which are not captured by other metrics. UNIBOUND sometimes
disagrees with other metrics on VAEs trained with controlled datasets, which indicates that multi-
variable interactions arise not only in artiﬁtial settings but in learned representations. We found that
VAEs trained with different methods induce representations with a variety of ratios between PID
terms, even if their disentanglement scores are close. It is a major future work to develop learning
methods of disentangled representations on the basis of these observations."
CONCLUSION,0.3978201634877384,Published as a conference paper at ICLR 2022
CONCLUSION,0.40054495912806537,"ACKNOWLEDGMENTS
We thank members of Issei Sato Laboratory and researchers in Preferred Networks for fruitful dis-
cussions, and thank the reviewers for helpful comments to improve the work."
REFERENCES,0.4032697547683924,REFERENCES
REFERENCES,0.40599455040871935,"H.B. Barlow. Unsupervised Learning. Neural Computation, 1(3):295–311, 1989. ISSN 0899-7667.
doi: 10.1162/neco.1989.1.3.295."
REFERENCES,0.4087193460490463,"H.B. Barlow, T.P. Kaushal, and G.J. Mitchison. Finding Minimum Entropy Codes. Neural Compu-
tation, 1(3):412–423, 1989. ISSN 0899-7667. doi: 10.1162/neco.1989.1.3.412."
REFERENCES,0.4114441416893733,"Adam B. Barrett. Exploration of synergistic and redundant information sharing in static and dynam-
ical gaussian systems. Phys. Rev. E, 91:052802, 2015. doi: 10.1103/PhysRevE.91.052802."
REFERENCES,0.4141689373297003,"Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828,
2013. doi: 10.1109/TPAMI.2013.50."
REFERENCES,0.41689373297002724,"Nils Bertschinger, Johannes Rauh, Eckehard Olbrich, J¨urgen Jost, and Nihat Ay. Quantifying unique
information. Entropy, 16(4):2161–2183, 2014. ISSN 1099-4300. doi: 10.3390/e16042161."
REFERENCES,0.4196185286103542,"Ricky T. Q. Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disen-
tanglement in variational autoencoders. In Advances in Neural Information Processing Systems,
volume 31, 2018."
REFERENCES,0.4223433242506812,"James J. DiCarlo and David D. Cox. Untangling invariant object recognition. Trends in Cognitive
Sciences, 11(8):333–341, 2007. ISSN 1364-6613. doi: 10.1016/j.tics.2007.06.010."
REFERENCES,0.4250681198910082,"Kien Do and Truyen Tran. Theory and evaluation metrics for learning disentangled representations.
In International Conference on Learning Representations, 2020."
REFERENCES,0.42779291553133514,"Emilien Dupont. Learning disentangled joint continuous and discrete representations. In Advances
in Neural Information Processing Systems, volume 31, 2018."
REFERENCES,0.4305177111716621,"Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of
disentangled representations. In International Conference on Learning Representations, 2018."
REFERENCES,0.4332425068119891,"Patrick Esser, Johannes Haux, and Bj¨orn Ommer. Unsupervised robust disentangling of latent char-
acteristics for image synthesis. In Proceedings of the Intl. Conf. on Computer Vision (ICCV),
2019."
REFERENCES,0.4359673024523161,"Conor Finn and Joseph T. Lizier. Pointwise partial information decomposition using the speciﬁcity
and ambiguity lattices. Entropy, 20(4), 2018. ISSN 1099-4300. doi: 10.3390/e20040297."
REFERENCES,0.43869209809264303,"Conor Finn and Joseph T. Lizier. Generalised measures of multivariate information content. Entropy,
22(2), 2020. ISSN 1099-4300. doi: 10.3390/e22020216."
REFERENCES,0.44141689373297005,"Shuyang Gao, Rob Brekelmans, Greg Ver Steeg, and Aram Galstyan. Auto-encoding total corre-
lation explanation. In Proceedings of the Twenty-Second International Conference on Artiﬁcial
Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pp. 1157–
1166, 2019."
REFERENCES,0.444141689373297,"Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner.
beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
2017."
REFERENCES,0.44686648501362397,"Irina Higgins, David Amos, David Pfau, S´ebastien Racani`ere, Lo¨ıc Matthey, Danilo J. Rezende,
and Alexander Lerchner.
Towards a deﬁnition of disentangled representations.
CoRR,
abs/1812.02230, 2018."
REFERENCES,0.44959128065395093,Published as a conference paper at ICLR 2022
REFERENCES,0.45231607629427795,"Wei-Ning Hsu, Yu Zhang, Ron J. Weiss, Yu-An Chung, Yuxuan Wang, Yonghui Wu, and James
Glass. Disentangling correlated speaker and noise for speech synthesis via data augmentation and
adversarial factorization. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 5901–5905, 2019.
doi: 10.1109/ICASSP.2019.
8683561."
REFERENCES,0.4550408719346049,"Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. Toward con-
trolled generation of text. In Proceedings of the 34th International Conference on Machine Learn-
ing, volume 70, pp. 1587–1596, 2017."
REFERENCES,0.45776566757493187,"Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks.
In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2019."
REFERENCES,0.4604904632152589,"Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In Proceedings of the 35th Interna-
tional Conference on Machine Learning, volume 80, pp. 2649–2658, 2018."
REFERENCES,0.46321525885558584,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Interna-
tional Conference on Learning Representations, ICLR, 2015."
REFERENCES,0.4659400544959128,"Diederik P. Kingma and Max Welling.
Auto-encoding variational bayes.
In 2nd International
Conference on Learning Representations, ICLR, 2014."
REFERENCES,0.46866485013623976,"Zhiyuan Li, Jaideep Vitthal Murkute, Prashnna Kumar Gyawali, and Linwei Wang. Progressive
learning and disentanglement of hierarchical representations.
In International Conference on
Learning Representations, 2020."
REFERENCES,0.4713896457765668,"Joseph T. Lizier, Nils Bertschinger, J¨urgen Jost, and Michael Wibral. Information decomposition of
target effects from multi-source interactions: Perspectives on previous, current and future work.
Entropy, 20(4), 2018. ISSN 1099-4300. doi: 10.3390/e20040307."
REFERENCES,0.47411444141689374,"David JC MacKay. Information theory, inference and learning algorithms. Cambridge university
press, 2003."
REFERENCES,0.4768392370572207,"Ari Pakman, Dar Gilboa, and Elad Schneidman. Estimating the unique information of continuous
variables in recurrent networks. CoRR, abs/2102.00218, 2021."
REFERENCES,0.47956403269754766,"Kyle Schick-Poland, Abdullah Makkeh, Aaron J. Gutknecht, Patricia Wollstadt, Anja Sturm, and
Michael Wibral.
A partial information decomposition for discrete and continuous variables.
CoRR, abs/2106.12393, 2021."
REFERENCES,0.4822888283378747,"J¨urgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation,
4(6):863–879, 1992. doi: 10.1162/neco.1992.4.6.863."
REFERENCES,0.48501362397820164,"David Sigtermans. A path-based partial information decomposition. Entropy, 22(9), 2020. ISSN
1099-4300. doi: 10.3390/e22090952."
REFERENCES,0.4877384196185286,"Tycho M.S. Tax, Pedro A.M. Mediano, and Murray Shanahan. The partial information decom-
position of generative neural network models. Entropy, 19(9), 2017. ISSN 1099-4300. doi:
10.3390/e19090474."
REFERENCES,0.4904632152588556,"Satosi Watanabe. Pattern recognition as a quest for minimum entropy. Pattern Recognition, 13(5):
381–387, 1981. ISSN 0031-3203. doi: 10.1016/0031-3203(81)90094-7."
REFERENCES,0.49318801089918257,"Paul L. Williams and Randall D. Beer. Nonnegative decomposition of multivariate information.
CoRR, abs/1004.2515, 2010."
REFERENCES,0.49591280653950953,"Shujian Yu, Kristoffer Wickstrøm, Robert Jenssen, and Jos´e C. Pr´ıncipe. Understanding convo-
lutional neural networks with information theory: An initial exploration.
IEEE Transactions
on Neural Networks and Learning Systems, 32(1):435–442, 2021. doi: 10.1109/TNNLS.2020.
2968509."
REFERENCES,0.4986376021798365,"Julian Zaidi, Jonathan Boilard, Ghyslain Gagnon, and Marc-Andr´e Carbonneau. Measuring disen-
tanglement: A review of metrics. CoRR, abs/2012.09276, 2020."
REFERENCES,0.5013623978201635,Published as a conference paper at ICLR 2022
REFERENCES,0.5040871934604905,"A
ADDITIONAL DIAGRAMS"
REFERENCES,0.5068119891008175,"I(u; v1, v2)"
REFERENCES,0.5095367847411444,I(u; v1)
REFERENCES,0.5122615803814714,I(u; v2)
REFERENCES,0.5149863760217984,"U(u; v1 \ v2)
R(u; v1, v2)
U(u; v2 \ v1)
C(u; v1, v2)"
REFERENCES,0.5177111716621253,Figure 5: Alternative diagram of Figure 1 in the band style following Figure 8.1 of MacKay (2003).
REFERENCES,0.5204359673024523,"B
RELATIONSHIPS TO OTHER INFORMATION-THEORETIC METRICS"
REFERENCES,0.5231607629427792,"There have been several metrics proposed in the literature for information-theoretic measurement of
disentanglement."
REFERENCES,0.5258855585831063,"For example, Do & Tran (2020) proposed four metrics, namely WSEPIN, WINDIN, RMIG, and
JEMMIG. Among them, WSEPIN is computed based on the information gap I(x; zℓ|z\ℓ) =
I(x; z) −I(x; zℓ). In the PID perspective, this quantity upper bounds the unique information of
x held by zℓ, i.e., I(x; zℓ|z\ℓ) ≥U(x; zℓ\ z\ℓ). It is similar to the upper bound of unique informa-
tion that we derived in Eq.7, as"
REFERENCES,0.5286103542234333,"I(yk; zℓ) −[II(yk; zℓ; z\ℓ)]+ ≤I(yk; zℓ) −II(yk; zℓ; z\ℓ) = I(yk; z) −I(yk; z\ℓ).
(12)"
REFERENCES,0.5313351498637602,"In that sense, WSEPIN is an upper bound of unique information, where yk is replaced with x and the
nonnegativity of redundant information is ignored. As WSEPIN does not handle generative factors
separately, it is useful when the generative factors are unknown, while our approach provides more
detailed information of (dis)entanglement when the generative factors are available."
REFERENCES,0.5340599455040872,"RMIG is an extension to MIG, where the conditioning between yk and x is inverted so that one can
compute the quantity with uncontrolled dataset. This approach may be extended for our framework
as well, in a similar way to applying the inversion to MIG. We did not use this extension as we only
deal with controlled datasets in this paper to contrast our approach with a wider variety of metrics."
REFERENCES,0.5367847411444142,"JEMMIG is also an extention to MIG, which provides a measurement of how each variable captures
only one generative factor. This aspect is also studied by Eastwood & Williams (2018) and Li
et al. (2020). As we used a set of simple latent variables each of which is either a one-dimensional
real variable or a categorical variable with a small number of arms (three or four depending on the
dataset), we expect that each variable does not capture much information of multiple generative
factors. Indeed, we did not observe any latent variable selected for more than two generative factors,
and when a latent variable is selected for two generative factors, the overall disentanglement score
is low so that the effect of duplicated selection is ignorable. We still consider it an interesting future
work to extend our approach for analyzing entanglement of multiple generative factors into a single
latent variable."
REFERENCES,0.5395095367847411,"C
DERIVATIONS FOR THE EXACT ANALYSIS WITH THE TOY MODEL"
REFERENCES,0.5422343324250681,"In this section, we provide a sketch of analytically computing the metrics for the toy model we used
in Section 5.1. We use the following formulae."
REFERENCES,0.5449591280653951,"• Let u be a Gaussian vector with covariance Σ. Then, its entropy is given by H(u) =
1
2 log(2πe|Σ|), where |·| is the matrix determinant. When we remove the ℓ-th element
from u, the remaining vector u\ℓfollows a Gaussian distribution whose covariance matrix
Σ\ℓis obtained by removing the ℓ-th row and column from Σ. The determinant of Σ\ℓ
is the cofactor of Σ at the ℓ-th diagonal element, i.e.,
Σ\ℓ
 = |Σ|(Σ−1)ℓℓ. With this
formula, we can derive the entropy of u\ℓas H(u\ℓ) = H(u) + 1"
REFERENCES,0.547683923705722,2 log(Σ−1)ℓℓ.
REFERENCES,0.5504087193460491,"• Suppose that an invertible matrix Σ is written by blocks as Σ =

A
B
C
D"
REFERENCES,0.553133514986376,"
. If A and its"
REFERENCES,0.555858310626703,"Schur complement S := D −CA−1B are invertible, the determinant and inverse of Σ are"
REFERENCES,0.55858310626703,Published as a conference paper at ICLR 2022
REFERENCES,0.5613079019073569,Table 1: Exact values of metrics for toy model. We use U = I −2
REFERENCES,0.5640326975476839,"K 11⊤for the attacks. We do not
normalize the metrics by H(yk) as y is isotropic and continuous; normalization by it just scales the
results by a constant scalar. For the attacked models, we show the limit of α →∞to understand
how well each metric reacts to completely entangled representations."
REFERENCES,0.5667574931880109,"Metric
z
˜zred
˜zsyn"
REFERENCES,0.5694822888283378,"MIG
1
2 log
 
1 +
1
σ2

1
2 log
(1+σ2)(1+α2(1+σ2−(1−2"
REFERENCES,0.5722070844686649,K )2))
REFERENCES,0.5749318801089919,σ2(1+α2(1+σ2)) →1
LOG,0.5776566757493188,"2 log

1+
1−(1−2 K )2 σ2"
LOG,0.5803814713896458,"
1
2 log

1 +
1
α2+σ2

→0"
LOG,0.5831062670299727,"U lower bound
(UNIBOUND)"
LOG,0.5858310626702997,"1
2 log
 
1 +
1
σ2

1
2 log (1+σ2)(1+α2σ2)"
LOG,0.5885558583106267,"σ2(1+α2(1+σ2)) →0
1
2 log

1 +
1
α2+σ2

→0"
LOG,0.5912806539509536,"U upper bound
1
2 log
 
1 +
1
σ2

1
2 log (1+σ2)(1+α2σ2)"
LOG,0.5940054495912807,"σ2(1+α2(1+σ2)) →0
1
2 log

1 +
1
α2+σ2

→0"
LOG,0.5967302452316077,written as
LOG,0.5994550408719346,"|Σ| = |A| · |S|,
Σ−1 =

A−1 + A−1BS−1CA−1
−A−1BS−1"
LOG,0.6021798365122616,"−S−1CA−1
S−1"
LOG,0.6049046321525886,"
.
(13)"
LOG,0.6076294277929155,"Similarly, if D and its Schur complement T := A −BD−1C are invertible, then"
LOG,0.6103542234332425,"|Σ| = |D| · |T|,
Σ−1 =

T−1
−T−1BD−1"
LOG,0.6130790190735694,"−D−1CT−1
D−1 + D−1CT−1BD−1"
LOG,0.6158038147138964,"
.
(14)"
LOG,0.6185286103542235,"Provided that z is a Gaussian vector with covariance Σ, we can derive the entropies of the attacked
vectors. Recall that the redundancy and synergy attacks are deﬁned as follows."
LOG,0.6212534059945504,"˜zred =

I
0
αU
I"
LOG,0.6239782016348774," 
z
ϵ"
LOG,0.6267029972752044,"
,
˜zsyn =

I
αU
0
I"
LOG,0.6294277929155313," 
z
ϵ "
LOG,0.6321525885558583,"where U is an orthonormal matrix, α > 0, and ϵ ∼N(0, I). Here, 0 is the zero matrix. These
again follow Gaussian distributions, whose covariance matrices are computed as follows."
LOG,0.6348773841961853,"Cov(˜zred) =

I
0
αU
I"
LOG,0.6376021798365122," 
Σ
0
0
I"
LOG,0.6403269754768393," 
I
αU⊤
0
I"
LOG,0.6430517711171662,"
=

Σ
αΣU⊤"
LOG,0.6457765667574932,"αUΣ
I + α2UΣU⊤ 
,"
LOG,0.6485013623978202,"Cov(˜zsyn) =

I
αU
0
I"
LOG,0.6512261580381471," 
Σ
0
0
I"
LOG,0.6539509536784741," 
I
0
αU⊤
I"
LOG,0.6566757493188011,"
=

α2I + Σ
αU
αU⊤
I 
."
LOG,0.659400544959128,"Using Eq.13 and Eq.14, we obtain their determinants and inverses."
LOG,0.662125340599455,"Cov(˜zred)
 = |Σ|,
Cov(˜zred)−1 =

α2I + Σ−1
−αU⊤
−αU
I 
,"
LOG,0.6648501362397821,"|Cov(˜zsyn)| = |Σ|,
Cov(˜zsyn)−1 =

Σ−1
−αΣ−1U
−αU⊤Σ−1
I + α2U⊤Σ−1U 
."
LOG,0.667574931880109,"Therefore, we obtain the following entropies for each ℓ∈{1, . . . , L}."
LOG,0.670299727520436,"H(˜zred
ℓ) = 1"
LOG,0.6730245231607629,"2 log(2πeΣℓℓ),
H(˜zred
\ℓ) = 1"
LOG,0.6757493188010899,"2 log(2πe|Σ|(α2 + (Σ−1)ℓℓ)),"
LOG,0.6784741144414169,"H(˜zred
L+ℓ) = 1"
LOG,0.6811989100817438,"2 log(2πe(1 + α2u⊤
ℓΣuℓ)),
H(˜zred
\L+ℓ) = 1"
LOG,0.6839237057220708,"2 log(2πe|Σ|),"
LOG,0.6866485013623979,"H(˜zsyn
ℓ) = 1"
LOG,0.6893732970027248,"2 log(2πe(α2 + Σℓℓ)),
H(˜zsyn
\ℓ) = 1"
LOG,0.6920980926430518,"2 log(2πe|Σ|(Σ−1)ℓℓ),"
LOG,0.6948228882833788,"H(˜zsyn
L+ℓ) = 1"
LOG,0.6975476839237057,"2 log(2πe),
H(˜zsyn
\L+ℓ) = 1"
LOG,0.7002724795640327,"2 log(2πe|Σ|(1 + α2v⊤
ℓΣ−1vℓ)),"
LOG,0.7029972752043597,H(˜zred) = H(˜zsyn) = 1
LOG,0.7057220708446866,"2 log(2πe|Σ|).
(15)
Here, uℓand vℓare the ℓ-th columns of U⊤and U, respectively. When we use U = I −2"
LOG,0.7084468664850136,"K 11⊤,
these are written as uℓ= vℓ= eℓ−2"
LOG,0.7111716621253406,"K 1, where (e1, . . . , eL) is the standard basis of RL."
LOG,0.7138964577656676,Published as a conference paper at ICLR 2022
LOG,0.7166212534059946,"Table 2: Encoder and decoder architectures used in the DSPRITES and 3DSHAPES experiments.
Data ﬂow top to bottom. Conv4x4s2p1 represents spatial convolution layer with kernel size 4x4,
stride 2x2, and 1 pixel padding at each side of the image. ConvT represents the transposed convo-
lution layer. FC stands for a fully-connected layer. The last fully-connected layer of the encoder
outputs the parameters of the variational posterior distribution, the number of which depends on the
model deﬁnition as follows. We used six latent variables in all models, which are all Gaussian ex-
cept for JointVAE where one of them is replaced with a categorical variable. As Gaussian variables
are parameterized by mean and standard deviation while the categorical variables are parameter-
ized by logits, the ﬁnal feature dimensionality is 12 for the models except for JointVAE where the
dimensionality is 13 for DSPRITE and 14 for 3DSHAPES."
LOG,0.7193460490463215,"Encoder
Decoder
Conv4x4s2p1, 32 channels
FC, 256 features
Conv4x4s2p1, 32 channels
FC, 64x4x4 features
Conv4x4s2p1, 64 channels
ConvT4x4s2p1, 64 channels
Conv4x4s2p1, 64 channels
ConvT4x4s2p1, 32 channels
FC, 256 features
ConvT4x4s2p1, 32 channels
FC, * features
ConvT4x4s2p1, 1 or 3 channels"
LOG,0.7220708446866485,"Recall that, in the toy model, the representation z is drawn from N(y, σ2I), where the generative
factors y follow the standard Gaussian distribution. Therefore, the marginal distribution of the
representation is p(z) = N(z; 0, (1 + σ2)I). When the representation is conditioned by a single
factor yk, it follows p(z|yk) = N(z; ykek, (1 + σ2)I −eke⊤
k ). By substituting the covariance
matrices of these distributions to Σ in Eq.15 and computing their differences, we obtain the mutual
information terms as follows."
LOG,0.7247956403269755,"I(yk; ˜zred
ℓ) ="
LOG,0.7275204359673024,"(
1
2 log 1+σ2"
LOG,0.7302452316076294,"σ2
(k = ℓ),
0
(k ̸= ℓ),
I(yk; ˜zred
L+ℓ) = 
 "
LOG,0.7329700272479565,"1
2 log
1+α2(1+σ2)
1+α2(1+σ2−(1−2"
LOG,0.7356948228882834,"K )2)
(k = ℓ),"
LOG,0.7384196185286104,"1
2 log
1+α2(1+σ2)
1+α2(1+σ2−
4
K2 )
(k ̸= ℓ),"
LOG,0.7411444141689373,"I(yk; ˜zred
\ℓ) ="
LOG,0.7438692098092643,"(
1
2 log 1+α2(1+σ2)"
LOG,0.7465940054495913,"1+α2σ2
(k = ℓ),"
LOG,0.7493188010899182,"1
2 log 1+σ2"
LOG,0.7520435967302452,"σ2
(k ̸= ℓ),
I(yk; ˜zred
\L+ℓ) = 1"
LOG,0.7547683923705722,"2 log 1 + σ2 σ2
,"
LOG,0.7574931880108992,"I(yk; ˜zsyn
ℓ) ="
LOG,0.7602179836512262,"(
1
2 log 1+σ2+α2"
LOG,0.7629427792915532,"σ2+α2
(k = ℓ),
0
(k ̸= ℓ),
I(yk; ˜zsyn
L+ℓ) = 0,
(16)"
LOG,0.7656675749318801,"I(yk; ˜zsyn
\ℓ) ="
LOG,0.7683923705722071,"(
0
(k = ℓ),
1
2 log 1+σ2"
LOG,0.771117166212534,"σ2
(k ̸= ℓ),
I(yk; ˜zsyn
\L+ℓ) = 
 "
LOG,0.773841961852861,"1
2 log
(1+σ2)(1+σ2+α2)
σ2(1+σ2+α2)+α2(1−2"
LOG,0.776566757493188,"K )2
(k = ℓ),"
LOG,0.779291553133515,"1
2 log
(1+σ2)(1+σ2+α2)
σ2(1+σ2+α2)+α2
4
K2
(k ̸= ℓ)"
LOG,0.782016348773842,I(yk; ˜zred) = I(yk; ˜zsyn) = 1
LOG,0.784741144414169,"2 log 1 + σ2 σ2
."
LOG,0.7874659400544959,"The metrics in Table 1 are computed from these quantities. In addition, we can compute other partial
information terms as follows."
LOG,0.7901907356948229,"R(yk; ˜zred
k , ˜zred
\k) = 1"
LOG,0.7929155313351499,2 log 1 + α2(1 + σ2)
LOG,0.7956403269754768,"1 + α2σ2
,
C(yk; ˜zred
k , ˜zred
\k) = 0,"
LOG,0.7983651226158038,"R(yk; ˜zsyn
k , ˜zsyn
\k ) = 0,
C(yk; ˜zsyn
k , ˜zsyn
\k ) = 1"
LOG,0.8010899182561307,2 log (1 + σ2)(σ2 + α2)
LOG,0.8038147138964578,σ2(1 + σ2 + α2) .
LOG,0.8065395095367848,"We can observe that the redundancy and synergy terms effectively increase with the corresponding
attacks."
LOG,0.8092643051771117,"D
MODEL ARCHITECTURES AND TRAINING HYPERPARAMETERS"
LOG,0.8119891008174387,"The architectures of the encoder and decoder used in the experiments are listed in Table 2. We
used ReLU nonlinearity at each convolutional layer except for the ﬁnal output of each network. In
addition, we used for the critic in FactorVAE a feedforward network with ﬁve hidden layers each of
which consists of 1,000 leaky ReLU units with the slope coefﬁcient of 0.2."
LOG,0.8147138964577657,Published as a conference paper at ICLR 2022
LOG,0.8174386920980926,Table 3: Training hyperparameters used for DSPRITES experiments.
LOG,0.8201634877384196,"Model
β-VAE
FactorVAE
JointVAE
β-TCVAE
Batch size
64
64
64
2,048
Iterations
300,000
300,000
300,000
30,000
Adam α
5 × 10−4
1 × 10−4
5 × 10−4
1 × 10−3"
LOG,0.8228882833787466,"Adam β1, β2
0.9, 0.999
0.9, 0.999
0.9, 0.999
0.9, 0.999
Critic Adam α
-
1 × 10−4
-
-
Critic Adam β1, β2
-
0.5, 0.9
-
-
Discrete variable capacity Cc
-
-
1.1
-
Continuous variable capacity Cz
-
-
40
-
Regularization coefﬁcient
β = 4
γ = 35
γ = 150
β = 6"
LOG,0.8256130790190735,Table 4: Training hyperparameters used for 3DSHAPES experiments.
LOG,0.8283378746594006,"Model
β-VAE
FactorVAE
JointVAE
β-TCVAE
Batch size
64
64
64
2,048
Iterations
500,000
500,000
500,000
50,000
Adam α
1 × 10−4
1 × 10−4
1 × 10−4
1 × 10−3"
LOG,0.8310626702997275,"Adam β1, β2
0.9, 0.999
0.9, 0.999
0.9, 0.999
0.9, 0.999
Critic Adam α
-
1 × 10−5
-
-
Critic Adam β1, β2
-
0.5, 0.9
-
-
Discrete variable capacity Cc
-
-
1.1
-
Continuous variable capacity Cz
-
-
40
-
Regularization coefﬁcient
β = 4
γ = 20
γ = 150
β = 4"
LOG,0.8337874659400545,"The hyperparameters used in training are listed in Table 3 and Table 4. For hyperparameters not
listed in the tables, we used the values suggested in the original papers."
LOG,0.8365122615803815,"E
FULL RESULTS FOR FACTOR-WISE PID ANALYSES"
LOG,0.8392370572207084,"We illustrate the estimated bounds of PID terms for each factor in Figure 6, whose small version
appeared in Figure 4. We summarize these results in Table 5. This table is made by observing and
categorizing the plots in Figure 6 into some patterns as follows. Note that we ignore the error bars
here."
IF THE LOWER BOUND OF THE UNIQUE INFORMATION IS LARGER THAN THE UPPER BOUNDS OF THE REDUN-,0.8419618528610354,"1. If the lower bound of the unique information is larger than the upper bounds of the redun-
dant and complementary information, mark the plot as disentangled. In this case, the model
is determined as successfully disentangling the factor regardless of the concrete deﬁnition
of PID. Note that the model does not necessarily learn the factor completely; see the ﬁgure
for how much the information of the factor is uniquely captured by a latent variable."
IF THE UPPER AND LOWER BOUNDS OF THE REDUNDANT INFORMATION ARE LARGER THAN THOSE OF THE,0.8446866485013624,"2. If the upper and lower bounds of the redundant information are larger than those of the
complementary information by a margin, mark the plot as redundant. In this case, the
model entangles the factor in a redundant way. As we analyzed in Section 5.2, this indicates
that the latent variables are highly dependent."
IF THE UPPER AND LOWER BOUNDS OF THE COMPLEMENTARY INFORMATION ARE LARGER THAN THOSE OF,0.8474114441416893,"3. If the upper and lower bounds of the complementary information are larger than those of
the redundant information by a margin, mark the plot as synergistic. In this case, the model
entangles the factor in a synergistic way. This may occur even if the latent variables are
independent. We may require additional inductive biases to help the model disentangle the
factor."
IF THE UPPER AND LOWER BOUNDS OF THE COMPLEMENTARY INFORMATION ARE LARGER THAN THOSE OF,0.8501362397820164,"4. Otherwise, mark the plot as ﬂat."
IF THE UPPER AND LOWER BOUNDS OF THE COMPLEMENTARY INFORMATION ARE LARGER THAN THOSE OF,0.8528610354223434,"F
POSSIBLE EXPLANATIONS FOR HIGH REDUNDANCY IN JOINTVAE"
IF THE UPPER AND LOWER BOUNDS OF THE COMPLEMENTARY INFORMATION ARE LARGER THAN THOSE OF,0.8555858310626703,"We observed in Table 5 and Figure 6 that JointVAE suffers from high redundancy in all the factors.
To explain this phenomenon, we review the training scheme of JointVAE (Dupont, 2018). The"
IF THE UPPER AND LOWER BOUNDS OF THE COMPLEMENTARY INFORMATION ARE LARGER THAN THOSE OF,0.8583106267029973,Published as a conference paper at ICLR 2022
IF THE UPPER AND LOWER BOUNDS OF THE COMPLEMENTARY INFORMATION ARE LARGER THAN THOSE OF,0.8610354223433242,"β-VAE
FactorVAE
β-TCVAE
JointVAE"
IF THE UPPER AND LOWER BOUNDS OF THE COMPLEMENTARY INFORMATION ARE LARGER THAN THOSE OF,0.8637602179836512,"(a) DSPRITES
(b) 3DSHAPES"
IF THE UPPER AND LOWER BOUNDS OF THE COMPLEMENTARY INFORMATION ARE LARGER THAN THOSE OF,0.8664850136239782,Figure 6: Large version of Figure 4.
IF THE UPPER AND LOWER BOUNDS OF THE COMPLEMENTARY INFORMATION ARE LARGER THAN THOSE OF,0.8692098092643051,Published as a conference paper at ICLR 2022
IF THE UPPER AND LOWER BOUNDS OF THE COMPLEMENTARY INFORMATION ARE LARGER THAN THOSE OF,0.8719346049046321,"Table 5: Qualitative summary of PID decomposition for model-factor pairs. Each pair is explained
by the following terms: disentangled: the unique information is larger than other terms; synergistic,
redundant: the corresponding PID term is large; ﬂat: no term exceeds others much, which indicates
that all three terms are small (i.e., multiple variables contain distinct information of the factor) or
both redundancy and synergy are large. Note that these qualitative analyses are only applicable to
our experimental settings. In particular, high redundancy of JointVAE (marked by ∗in the table)
may be caused by a capacity hyperparameter. See Appendix G for details."
IF THE UPPER AND LOWER BOUNDS OF THE COMPLEMENTARY INFORMATION ARE LARGER THAN THOSE OF,0.8746594005449592,"Dataset
Factor
β-VAE
FactorVAE
β-TCVAE
JointVAE"
IF THE UPPER AND LOWER BOUNDS OF THE COMPLEMENTARY INFORMATION ARE LARGER THAN THOSE OF,0.8773841961852861,DSPRITES
IF THE UPPER AND LOWER BOUNDS OF THE COMPLEMENTARY INFORMATION ARE LARGER THAN THOSE OF,0.8801089918256131,"shape
ﬂat
redundant
ﬂat
redundant∗
scale
synergistic
redundant
disentangled
redundant∗
orientation
synergistic
ﬂat
synergistic
redundant∗
position x
synergistic
redundant
disentangled
redundant∗
position y
synergistic
redundant
disentangled
redundant∗"
DSHAPES,0.8828337874659401,3DSHAPES
DSHAPES,0.885558583106267,"ﬂoor hue
disentangled
disentangled
disentangled
redundant∗
wall hue
redundant
redundant
redundant
redundant∗
object hue
redundant
redundant
redundant
redundant∗
scale
ﬂat
ﬂat
synergistic
redundant∗
shape
ﬂat
ﬂat
synergistic
redundant∗
orientation
ﬂat
redundant
disentangled
redundant∗"
DSHAPES,0.888283378746594,"Table 6: KL terms and total correlation of latent variables learned by JointVAE. The values in
parentheses show the standard deviation."
DSHAPES,0.8910081743869209,"Dataset
DKL(p(z1|x)∥U(z1))
DKL(p(z2:L|x)∥N(z2:L; 0, I))
Total correlation of z"
DSHAPES,0.8937329700272479,"DSPRITES
1.10 (±0.00)
40.00 (±0.04)
25.02 (±0.47)
3DSHAPES
1.10 (±0.00)
39.99 (±0.05)
25.98 (±0.56)"
DSHAPES,0.896457765667575,"JointVAE model consists of one categorical variable z1 and L−1 Gaussian variables z2:L. Let U(z1)
be the uniform categorical distribution and pd(x|z) be the decoder to be learned simultaneously.
Then, the objective function of JointVAE is"
DSHAPES,0.8991825613079019,"L = Ep(z|x)[log pd(x|z)]−γ|DKL(p(z1|x)∥U(z1)) −c1|−γ|DKL(p(z2:L|x)∥N(z2:L; 0, I)) −c2|,"
DSHAPES,0.9019073569482289,"which involves three hyperparameters: the regularization coefﬁcient γ, the capacity of the discrete
variable c1, and the capacity of continuous variables c2. Throughout training, γ is kept constant,
while the capacities c1 and c2 are gradually increased and saturated at the predeﬁned maximum
values (Cc and Cz, respectively) in the middle of training. As the capacities are positive at the end
of training, this indicates that the KL terms, which include the total correlation of the latent variables
(Kim & Mnih, 2018; Chen et al., 2018), are large in the trained model. Therefore, as we discussed
in Section 5.2, this model does not add pressure on the representation to be less redundant, which
may explain the high redundancy3. See Appendix G for the results with varying Cz, whose results
also support the above hypothesis as low capacities induce lower redundancy. This training scheme
is designed to align the amount of information captured by discrete and continuous variables, which
seem to be successful as we observed, i.e., it effectively captures the shape factor in both datasets
compared to the other methods."
DSHAPES,0.9046321525885559,"G
INFLUENCE OF REGULARIZATION HYPERPARAMETERS ON
DISENTANGLEMENT"
DSHAPES,0.9073569482288828,"We trained each model with varying hyperparameters (β for β-VAE and β-TCVAE, γ for Factor-
VAE, and the capcity of continuous variables Cz for JointVAE) and investigated how the metrics as
well as PID terms are affected by the regularization strengths4."
DSHAPES,0.9100817438692098,"3To conﬁrm that large capacity actually causes large dependency, we measured the KL terms and the total
correlation of the trained JointVAE models. We summarize the results in Table 6. These values indicate that
the KL terms are actually close to the capacity hyperparameters (see Table 3 and Table 4), and more than a half
of them are occupied by the total correlation.
4For JointVAE, we chose the ﬁnal capacity of continuous variables instead of the regularization coefﬁcient
as we expect the capacity to be more relevant to disentanglement. The former controls the KL divergence"
DSHAPES,0.9128065395095368,Published as a conference paper at ICLR 2022
DSHAPES,0.9155313351498637,(a) β-VAE. The KL term coefﬁcient β is varied.
DSHAPES,0.9182561307901907,(b) FactorVAE. The multiplier of total correlation γ is varied.
DSHAPES,0.9209809264305178,(c) β-TCVAE. The multiplier of total correlation β is varied.
DSHAPES,0.9237057220708447,"(d) JointVAE. The ﬁnal capacity of continuous variables Cz is varied. Note that higher capacity induces lower
regularization effect."
DSHAPES,0.9264305177111717,"Figure 7: Metrics and PID estimations for varying regularization coefﬁcients on DSPRITES. For each
model, the left panel shows how each metric reacts to changing the hyperparameter. The right panel
shows the PID esitmations of resulting representations. In all plots, the horizontal axis corresponds
to the regularization hyperparameter."
DSHAPES,0.9291553133514986,Published as a conference paper at ICLR 2022
DSHAPES,0.9318801089918256,(a) β-VAE. The KL term coefﬁcient β is varied.
DSHAPES,0.9346049046321526,(b) FactorVAE. The multiplier of total correlation γ is varied.
DSHAPES,0.9373297002724795,(c) β-TCVAE. The multiplier of total correlation β is varied.
DSHAPES,0.9400544959128065,"(d) JointVAE. The ﬁnal capacity of continuous variables Cz is varied. Note that higher capacity induces lower
regularization effect."
DSHAPES,0.9427792915531336,Figure 8: Metrics and PID estimations for varying regularization coefﬁcients on 3DSHAPES.
DSHAPES,0.9455040871934605,Published as a conference paper at ICLR 2022
DSHAPES,0.9482288828337875,"The results for DSPRITES and 3DSHAPES are illustrated in Fig.7 and Fig.8, respectively. From the
left panels, we can observe that the UNIBOUND metric is positively correlated with the regular-
ization strength5. It indicates that the regularization method introduced by each model positively
contributes to disentanglement in the PID perspective. We also estimated PID bounds in the right
panels. They show some interesting effects of regularization against the types of entanglement. For
example, in JointVAE, the representation has low redundancy when the capacity of continuous vari-
ables is small, and the redundancy grows signiﬁcantly when we increase the capacity. It indicates
that a large capacity makes the model enforce each latent variable to capture details of the input
images, ignoring how the information is redundantly captured by other variables."
DSHAPES,0.9509536784741145,"H
LARGE FIGURES OF EXPERIMENTAL RESULTS"
DSHAPES,0.9536784741144414,"We put large versions of Figure 3 and Figure 4 in Figure 9 and Figure 6, respectively, for ﬁner
rendering."
DSHAPES,0.9564032697547684,"I
TRAINING AND EVALUATION STABILITY"
DSHAPES,0.9591280653950953,"We illustrate the disentanglement scores of models trained with eight training seeds in Figure 10 and
Figure 11. As each disentanglement metric involves sampling during evaluation, the evaluated score
has some randomness even if we ﬁx the training random seed. This plot reveals that the deviation
caused by randomness in evaluating disentanglement metrics is much smaller than the deviation
caused by randomness in training each model."
DSHAPES,0.9618528610354223,"between the aggregated posterior of continuous variables and their prior at the end of training, while the latter
controls the strength of enforcing the KL divergence to be close to the capacity. See the original paper (Dupont,
2018) for more details.
5In JointVAE, the hyperparameter controls the ﬁnal KL term; hence, a smaller hyperparameter should in-
duce more disentangled representation."
DSHAPES,0.9645776566757494,Published as a conference paper at ICLR 2022
DSHAPES,0.9673024523160763,(a) Disentanglement scores
DSHAPES,0.9700272479564033,(b) PID terms
DSHAPES,0.9727520435967303,(c) Redundancy attacks
DSHAPES,0.9754768392370572,Figure 9: Large versions of Fig.3. (Left) Results for DSPRITES. (Right) Results for 3DSHAPES.
DSHAPES,0.9782016348773842,Published as a conference paper at ICLR 2022
DSHAPES,0.9809264305177112,"(a) β-VAE
(b) FactorVAE"
DSHAPES,0.9836512261580381,"(c) β-TCVAE
(d) JointVAE"
DSHAPES,0.9863760217983651,"Figure 10: Disentanglement scores before aggregating across different training seeds for DSPRITES.
We optimized parameters for each model eight times with different random seeds, whose scores are
illustrated by the eight boxes in each plot."
DSHAPES,0.989100817438692,Published as a conference paper at ICLR 2022
DSHAPES,0.9918256130790191,"(a) β-VAE
(b) FactorVAE"
DSHAPES,0.9945504087193461,"(c) β-TCVAE
(d) JointVAE"
DSHAPES,0.997275204359673,Figure 11: Disentanglement scores before aggregating across different training seeds for 3DSHAPES.
