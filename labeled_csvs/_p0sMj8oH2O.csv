Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008097165991902834,"Graph databases (GDBs) enable processing and analysis of unstructured, complex,
rich, and usually vast graph datasets. Despite the large signiﬁcance of GDBs
in both academia and industry, little effort has been made into integrating them
with the predictive power of graph neural networks (GNNs). In this work, we
show how to seamlessly combine nearly any GNN model with the computational
capabilities of GDBs. For this, we observe that the majority of these systems
are based on, or support, a graph data model called the Labeled Property Graph
(LPG), where vertices and edges can have arbitrarily complex sets of labels and
properties. We then develop LPG2vec, an encoder that transforms an arbitrary LPG
dataset into a representation that can be directly used with a broad class of GNNs,
including convolutional, attentional, message-passing, and even higher-order or
spectral models. In our evaluation, we show that the rich information represented
as LPG labels and properties is properly preserved by LPG2vec, and it increases
the accuracy of predictions regardless of the targeted learning task or the used
GNN model, by up to 34% compared to graphs with no LPG labels/properties. In
general, LPG2vec enables combining predictive power of the most powerful GNNs
with the full scope of information encoded in the LPG model, paving the way for
neural graph databases, a class of systems where the vast complexity of maintained
data will beneﬁt from modern and future graph machine learning methods."
INTRODUCTION,0.0016194331983805667,"1
Introduction"
INTRODUCTION,0.0024291497975708503,"Graph databases are a class of systems that enable storing, processing, analyzing, and the overall
management of large and rich graph datasets [26]. They are heavily used in computational biology
and chemistry, medicine, social network analysis, recommendation and online purchase infrastructure,
and many others [26]. A plethora of such systems exist, for example Neo4j [100] (a leading industry
graph database)1, TigerGraph [141, 142], JanusGraph [138], Azure Cosmos DB [99], Amazon
Neptune [4], Virtuoso [110], ArangoDB [9–11], OrientDB [37, 136], and others [30, 38, 40, 47, 51,
58, 108, 109, 111, 119, 133]. Graph databases differ from other classes of graph-related systems
and workloads such as graph streaming frameworks [19] in that they deal with transactional support,
persistence, physical/logical data independence, data integrity, consistency, and complex graph data
models where both vertices and edges may be of different classes and may be associated with arbitrary
properties."
INTRODUCTION,0.0032388663967611335,"An established data model used in the majority of graph databases is called the Labeled Property
Graph (LPG) [26]. It is the model of choice for the leading industry Neo4j graph database system.
LPG has several advantages over other graph data models, such as heterogeneous graphs [153, 160,
163] or the Resource Description Framework (RDF) [87] graphs, often referred to as knowledge"
INTRODUCTION,0.004048582995951417,1According to the DB engines ranking (https://db-engines.com/en/ranking/graph+dbms)
INTRODUCTION,0.004858299595141701,Neural Graph Databases
INTRODUCTION,0.005668016194331984,"Example labeled property
            graph (LPG)
Example plain
       graph"
INTRODUCTION,0.006477732793522267,"Example heterogeneous
              graph"
INTRODUCTION,0.0072874493927125505,"Several
classes of
vertices"
INTRODUCTION,0.008097165991902834,"Several
classes of
edges"
INTRODUCTION,0.008906882591093117,"Vertex (e.g.,
a publication)"
INTRODUCTION,0.009716599190283401,"Edge (e.g.,
a citation)"
INTRODUCTION,0.010526315789473684,"Task-specific
vertex data
(e.g., abstract)"
INTRODUCTION,0.011336032388663968,"Task-specific
edge data (e.g.,
type of citation)"
INTRODUCTION,0.012145748987854251,"Arbitrary set of
properties attached
to a vertex / edge"
INTRODUCTION,0.012955465587044534,"Vertex with an
arbitrary set of
labels"
INTRODUCTION,0.013765182186234818,Vertex Edge
INTRODUCTION,0.014574898785425101,"Example RDF
      dataset"
INTRODUCTION,0.015384615384615385,"Edge with an
arbitrary set of
labels"
INTRODUCTION,0.016194331983805668,connects to
INTRODUCTION,0.01700404858299595,connects to
INTRODUCTION,0.017813765182186234,connects to
INTRODUCTION,0.01862348178137652,connects to
INTRODUCTION,0.019433198380566803,abstract of
INTRODUCTION,0.020242914979757085,abstract of
INTRODUCTION,0.021052631578947368,data of
INTRODUCTION,0.02186234817813765,data of
INTRODUCTION,0.022672064777327937,"Triple:
Subject
Predicate Object
Vertex
Edge
Abstract
Data
Edge
Vertex ... ... ... ..."
INTRODUCTION,0.02348178137651822,"Figure 1: Overview of the Labeled Property Graph (LPG) data model used in graph databases, vs. plain and heterogeneous graphs used in
broad graph processing and graph machine learning, and RDF triples."
INTRODUCTION,0.024291497975708502,"graphs (see Figure 1). First, while heterogeneous graphs support different classes of vertices and
edges, LPGs offer arbitrary sets of labels as well as key-value property pairs that can be attached
to vertices and edges. This facilitates modeling very rich and highly complex data. For example,
when modeling publications with graph vertices, one can use labels to model an arbitrarily complex
hierarchy of types of publications (journal, conference, workshop papers; best papers, best student
papers, best paper runner-ups, etc.). We discuss this example further in Section 2. Second, LPG
explicitly stores the neighborhood structure of the graph, very often in the form of adjacency lists [26].
Hence, it enables very fast accesses to vertex neighborhoods and consequently fast and scalable graph
algorithms and graph queries. This may be more difﬁcult to achieve in data representations such as
sets of triples. First, any possible relation between any two entities in a graph (i.e., edges, vertices,
and any other data) is explicitly maintained as a separate triple (see Figure 1). Second, any entity
is fundamentally the same “resource”, where “vertex” or “edge” are just roles assigned to a given
resource; these roles can differ in different triples (i.e., one resource can be both a vertex and an edge,
depending on a speciﬁc triple). Hence, RDF graphs may need more storage, and they may require
more complex indexing structures for vertex neighborhoods, than in the corresponding LPGs."
INTRODUCTION,0.025101214574898785,"Graph neural networks (GNNs) have recently become an established part of the machine learning
(ML) landscape [20, 23, 35, 41, 62, 65, 66, 129, 159, 170, 172]. Example applications are node,
link, or graph classiﬁcation or regression in social sciences, bioinformatics, chemistry, medicine,
cybersecurity, linguistics, transportation, and others. GNNs have been successfully used to provide
cost-effective and fast placement of chips [101], improve the accuracy of protein folding predic-
tion [75], simulate complex physics [114, 127], or guide mathematical discoveries [48]. The versatility
of GNNs brings a promise of enhanced analytics capabilities in the graph database landscape."
INTRODUCTION,0.025910931174089068,"Recently, Neo4j Inc., Amazon, and others have started to investigate harnessing graph ML capabil-
ities into their graph database architectures. However, current efforts only enable limited learning
functionalities that do not take advantage of the full richness of data enabled by LPG. For example,
Neo4j’s Graph Data Science module [71] supports obtaining embeddings and using them for node
or graph classiﬁcation. However, these embeddings are based on the graph structure, with limited
support for taking advantage of the full scope of information provided by LPG labels and properties."
INTRODUCTION,0.026720647773279354,"Combining LPG-based graph databases with GNNs could facilitate reaching new frontiers in ana-
lyzing complex unstructured datasets, and it could also illustrate the potential of GNNs for broad
industry. In this work, we ﬁrst broadly investigate both the graph database setting and GNNs to
ﬁnd the best approach for combining these two. As a result, we develop LPG2vec, an encoder that
enables harnessing the predictive power of GNNs for LPG graph databases. In LPG2vec, we treat
labels and properties attached to a vertex v as an additional source of information that should be
integrated with v’s input feature vectors. For this, we show how to encode different forms of data
provided in such labels/properties. This data is transformed into embeddings that can seamlessly be
used with different GNN models. LPG2vec is orthogonal to the software design and can also be used
with any GNN framework or graph database implementation."
INTRODUCTION,0.027530364372469637,Neural Graph Databases
INTRODUCTION,0.02834008097165992,"We combine LPG2vec with three established GNN models (GCN [85], GIN [161], and GAT [147]),
and we show that the information preserved by LPG2vec consistently enhances the accuracy of graph
ML tasks, i.e., classiﬁcation or regression of nodes and edges, by up to 34%. Moreover, LPG2vec
supports the completion of missing labels and properties in often noisy LPGs. Overall, it enables
Neural Graph Databases: the ﬁrst learning architecture that enables harnessing both the structure
and rich data (labels, properties) of LPG for highly accurate predictions in graph databases."
LIT REVIEW,0.029149797570850202,"2
Background"
LIT REVIEW,0.029959514170040485,We ﬁrst introduce fundamental concepts and notation for the LPG model and GNNs.
LIT REVIEW,0.03076923076923077,"2.1
Labeled Property Graph Data Model"
LIT REVIEW,0.031578947368421054,"Labeled Property Graph Model (LPG) [26] (also called the property graph [5]) is a primary established
data model used in graph databases. We focus on LPG because it is supported by the majority of
systems, and is a model of choice in many leading ones [26] (see Section 1)."
LIT REVIEW,0.032388663967611336,":Journal
:WorkshopProc"
LIT REVIEW,0.03319838056680162,:PhDThesis
LIT REVIEW,0.0340080971659919,":ConferenceProc
:TechReport"
LIT REVIEW,0.034817813765182185,":TechReport
:Journal
:WorkshopProc"
LIT REVIEW,0.03562753036437247,":VancouverStyle
:ParentheticalStyle"
LIT REVIEW,0.03643724696356275,:VancouverStyle
LIT REVIEW,0.03724696356275304,:VancouverStyle
LIT REVIEW,0.03805668016194332,:ParentheticalStyle
LIT REVIEW,0.038866396761133605,:VancouverStyle
LIT REVIEW,0.03967611336032389,":SelfCitation
:AdvisorCitation
:AdvisorCitation"
LIT REVIEW,0.04048582995951417,:AdvisorCitation
LIT REVIEW,0.04129554655870445,:AdvisorCitation
LIT REVIEW,0.042105263157894736,"title = ""Enabling...""
#authors = 4
#equations = 21
#pages = 12"
LIT REVIEW,0.04291497975708502,"#count = 4
section = 1
title = ""Neural...""
#authors = 2
#equations = 5
#pages = 20"
LIT REVIEW,0.0437246963562753,:BestPaper
LIT REVIEW,0.044534412955465584,"#count = 1
section = 1"
LIT REVIEW,0.045344129554655874,"#count = 1
section = 8"
LIT REVIEW,0.046153846153846156,"#count = 3
section = 1"
LIT REVIEW,0.04696356275303644,"#count = 2
section = 1"
LIT REVIEW,0.04777327935222672,"#count = 4
section = 1"
LIT REVIEW,0.048582995951417005,"title = ""Enabling...""
#authors = 4
#equations = 21
#pages = 12"
LIT REVIEW,0.04939271255060729,"title = ""Enabling...""
#authors = 4
#equations = 21
#pages = 12"
LIT REVIEW,0.05020242914979757,"title = ""Enabling...""
#authors = 4
#equations = 21
#pages = 12"
LIT REVIEW,0.05101214574898785,Example labeled property graph
LIT REVIEW,0.051821862348178135,"A vertex models
a publication"
LIT REVIEW,0.05263157894736842,"An edge models
a citation"
LIT REVIEW,0.05344129554655871,"Properties may be
associated with both
vertices and edges"
LIT REVIEW,0.05425101214574899,"There may be arbitrarily
many properties attached"
LIT REVIEW,0.05506072874493927,"There may
be arbitrarily
many labels
attached"
LIT REVIEW,0.055870445344129556,"There may
be arbitrarily
many label
values attached"
LIT REVIEW,0.05668016194331984,"Labels may
be associated
with vertices
and edges"
LIT REVIEW,0.05748987854251012,:InvitedPaper
LIT REVIEW,0.058299595141700404,"Labels in this example:
➼ Publication type (a journal paper, a conference paper, a workshop paper, a report, a thesis)
➼ Publication highlight (best paper, distinguished paper, invited paper)
➼ Citation variant (self citation, advisor citation)
➼ Citation style (Vancouver, Parenthetical)"
LIT REVIEW,0.05910931174089069,:ConferenceProc
LIT REVIEW,0.05991902834008097,Figure 2: An example of an LPG graph modeling publications and citations between them.
LIT REVIEW,0.06072874493927125,"At its core, LPG is based on the
plain graph model G = (V, E),
where V is a set of vertices and
E ⊆V × V is a set of edges;
|V | = n and |E| = m.
An
edge e = (u, v) ∈E is a tu-
ple of the out-vertex u (origin)
and the in-vertex v (target). If
G is undirected, then an edge
e = {u, v} ∈E is a set of u and
v. Ni and di denote the neigh-
bors and the degree of a given
vertex i (Ni ⊂V ); d is G’s max-
imum degree. LPG then adds
arbitrary labels and properties
to vertices and edges. An LPG
is formally modeled as a tuple
(V, E, L, l, K, W, p). L is a set
of labels. l : V ∪E 7→P(L)
is a labeling function, mapping
– respectively – each vertex and
each edge to a subset of labels,
where P(L) is the power set of
L, containing all possible subsets
of L. In addition to labels, each
vertex and edge can have arbitrar-
ily many properties (sometimes
referenced as attributes). A prop-
erty is a (key, value) pair, with key being an identiﬁer and value being a corresponding value. Here,
K is a set with all possible keys and W is a set with all possible values. For any property, we have
key ∈K and value ∈W. Then, p : (V ∪E) × K 7→W is a mapping function from vertices/edges
to property values. Speciﬁcally, p(u, key) and p(e, key) assign – respectively – a value to a property
indexed with a key key, of a vertex u of an edge e. Note that one can assign multiple properties with
the same key to vertices and edges (i.e., only the pair (key, value) must be unique)."
LIT REVIEW,0.06153846153846154,We illustrate an example of an LPG graph in Figure 2.
LIT REVIEW,0.062348178137651825,"2.2
Graph Neural Networks"
LIT REVIEW,0.06315789473684211,"Graph neural networks (GNNs) are a class of neural networks that enable learning over irregular
graph datasets [129]. Each vertex (and often each edge) of the input graph usually comes with an
input feature vector that encodes the semantics of a given task. For example, when vertices and
edges model publications and citations between these papers, then a vertex input feature vector"
LIT REVIEW,0.06396761133603239,Neural Graph Databases
LIT REVIEW,0.06477732793522267,"is a encoding of the publication abstract (e.g., a one-hot bag-of-words encoding specifying which
words are present). Input feature vectors are transformed in a series of GNN layers. In this process,
intermediate hidden latent vectors are created. The last GNN layer produces output feature vectors,
which are then used for the downstream ML tasks such as node classiﬁcation or graph classiﬁcation."
LIT REVIEW,0.06558704453441296,"Many GNN models exist [23, 39, 42, 128, 139, 157, 159, 168, 170, 172]. Most of such models
consist of a series of GNN layers, and a single layer has two stages: (1) the aggregation stage that
– for each vertex – combines the features of the neighbors of that vertex, and (2) the neural stage
that combines the results of the aggregation with the vertex score from the previous layer into a new
score. One may also explicitly distinguish stage (3), a non-linear activation over feature vectors (e.g.,
ReLU [85]) and/or normalization. We illustrate a simpliﬁed view of a GNN layer in Figure 3. ReLU"
LIT REVIEW,0.06639676113360324,"Input
samples"
LIT REVIEW,0.06720647773279352,"Stage (1): graph
operaon (e.g.,
graph convoluon)"
LIT REVIEW,0.0680161943319838,"Stage (2): neural
operaon,
(e.g., MLP)"
LIT REVIEW,0.06882591093117409,"Next
GNN
layer"
LIT REVIEW,0.06963562753036437,"Normalizaon,
non-linearity
(oponal)
A single
GNN Layer"
LIT REVIEW,0.07044534412955465,"Each vertex is
associated with
a feature vector"
LIT REVIEW,0.07125506072874493,Figure 3: Overview of a single GNN layer.
LIT REVIEW,0.07206477732793522,"The input, output, and hidden
feature vector of a vertex i
are denoted with, respectively,
xi, yi, hi. We have xi ∈Rk and
yi, hi ∈RO(k), k is the dimen-
sionality of vertex input feature
vectors. “(l)” denotes the l-th
GNN layer; h(l)
i
are latent features in layer l."
LIT REVIEW,0.0728744939271255,"Formally, the graph aggregation stage of a GNN layer can be described using two functions, ψ and
L. First, the feature vector of each neighbor of i is transformed by a function ψ. Then, the resulting
neighbor feature vectors are aggregated using a function L, such as sum or max. The outcome of L"
LIT REVIEW,0.07368421052631578,"is then processed using a third function, ϕ, that models the neural operation and non-linearity. This
gives the latent feature vector hi in the next GNN layer. Combined, we have"
LIT REVIEW,0.07449392712550608,"h(l+1)
i
= ϕ "
LIT REVIEW,0.07530364372469636,"h(l)
i ,
M"
LIT REVIEW,0.07611336032388664,"j∈N(i)
ψ

h(l)
i , h(l)
j

 
(1)"
LIT REVIEW,0.07692307692307693,"This is a generic form of GNNs, which can be used to deﬁne three major classes of GNNs [34]:
convolutional GNNs (C-GNNs; examples are GCN [85], GraphSAGE [67], GIN [161], and Comm-
Net [134]), attentional GNNs (A-GNNs; examples are MoNet [103], GAT [147], and AGNN [139]),
and the most generic message-passing GNNs (MP-GNNs; examples are G-GCN [33], Edge-
Conv [155], MPNN [64], and GraphNets [13])."
LIT REVIEW,0.07773279352226721,"To avoid confusion, we always use a term “label” to denote an LPG label, while a term “class”
indicates a prediction target in classiﬁcation tasks."
IMPLEMENTATION/METHODS,0.07854251012145749,"3
Marrying Graph Databases and Graph Neural Networks"
IMPLEMENTATION/METHODS,0.07935222672064778,"We ﬁrst investigate how LPG-based graph databases and GNNs can be combined to reach new
frontiers of complex graph data analytics."
IMPLEMENTATION/METHODS,0.08016194331983806,"3.1
How to Use GNNs with GDBs?"
IMPLEMENTATION/METHODS,0.08097165991902834,"It is not immediately clear on how to use GNNs in combination with the LPG data model. Speciﬁcally,
labels and properties of a vertex v are often seen as additional “vertices” attached to v [106, 116].
From this perspective, it would seem natural to use them during the aggregation phase of a GNN
computation, together with the neighbors of v. Similarly, one could consider incorporating attentional
GNNs [34], by attending to individual labels and properties. In general, there can be many different
approaches for integrating GNNs and LPGs."
IMPLEMENTATION/METHODS,0.08178137651821862,"Here, we ﬁrst extensively investigated both the graph database (GDB) and the GNN settings. The
goal was to determine the best approach for using GNNs with GDBs in order to beneﬁt the maximum
number of different GDB workloads while ensuring a seamless integration with as many GNN models
as possible. We consider all major classes of GDB workloads: online transactional, analytical, and
serving processing (respectively, OLTP, OLAP, OLSP) [26], and the fundamental GNN model classes
(e.g., C-GNN, A-GNN, MP-GNN) [34], for a total of more than 280 analyzed publications or reports."
IMPLEMENTATION/METHODS,0.0825910931174089,"Our analysis indicated that the most versatile approach for extracting the information from LPG
labels and properties is based on encoding labels and properties directly into the input feature vectors,"
IMPLEMENTATION/METHODS,0.08340080971659919,Neural Graph Databases
IMPLEMENTATION/METHODS,0.08421052631578947,"and subsequently feeding such vectors into a selected GNN model. First, this approach only requires
modiﬁcations to the input feature vectors, which makes LPG2vec fully compatible with any C-
GNN, A-GNN, or MP-GNN model (and many others). Second, this approach is very similar in
its workﬂow to schemes such as positional encodings: is is based on preprocessing and feeding
additional information into input feature vectors. Hence, it is straightforward to integrate into existing
GNN infrastructures."
IMPLEMENTATION/METHODS,0.08502024291497975,"3.2
Use Cases and Advantages"
IMPLEMENTATION/METHODS,0.08582995951417004,"The ﬁrst advantage of combining graph databases with GNNs is enhancing the accuracy of traditional
GNN tasks: classiﬁcation and regression of nodes, edges, and graphs (note that tasks such as
clustering or link prediction can be expressed as node/edge classiﬁcation/regression). This is because
LPG labels and properties, when incorporated into input feature vectors, carry additional information.
This is similar to how different classes of vertices/edges in heterogeneous graphs enhance prediction
tasks [160, 163]. However, the challenge is how to incorporate the full rich set of LPG information,
i.e., multiple labels and properties, into the learning workﬂow, while achieving high accuracy and
without exacerbating running times or memory pressure."
IMPLEMENTATION/METHODS,0.08663967611336032,"GNNs can also be used to deliver novel prediction tasks suited for LPG, namely label prediction
and property prediction. In the former, one is interested in assessing whether a given vertex or
edge potentially has a speciﬁed label, i.e., whether label ∈l(v) or label ∈l(e), where label ∈
L, v ∈V, e ∈E are – respectively – a label, a vertex, and an edge of interest, and l is a labeling
function. In the latter, one analogously asks whether a given vertex or edge potentially has a speciﬁed
property, and – if yes – what its value is, i.e., whether property ∈p(v) or property ∈p(e), where
p = (k, w), k ∈K, w ∈W, v ∈V, e ∈E are – respectively – a property, a vertex, and an edge of
interest, and p maps v and e to their corresponding properties."
IMPLEMENTATION/METHODS,0.0874493927125506,"Here, we observe that predicting new labels can be seamlessly resolved with node/edge classiﬁcation,
with the target learned label being l. Similarly, property prediction is effectively node/edge regression,
where w is the learned value. Thus, it means that one can easily use existing GNN models for LPG
graph completion tasks, i.e., ﬁnding missing labels or properties in the often noisy datasets."
IMPLEMENTATION/METHODS,0.08825910931174089,"3.3
LPG2vec + GNN: Towards A Neural Graph Database"
IMPLEMENTATION/METHODS,0.08906882591093117,"Our architecture for neural graph databases can be seen as an encoder combined with a selected GNN
model. An overview is provided in Figure 4."
IMPLEMENTATION/METHODS,0.08987854251012145,"In the ﬁrst step to construct an embedding of an LPG, we apply one-hot encoding for labels and
properties of each vertex and edge. For labels, the encoding is {0, 1}|L|, where “1” indicates that
a given ith label is attached to a given vertex/edge. For properties, the encoding details depend
on the property type: If a property can have discretely many (C) values, then we encode it using
a plain one-hot vector with C entries. A continuous scalar property is normalized to [0; 1] or,
alternatively, discretized and encoded as a one-hot vector. Importantly, one must use the same norm or
discretization for all property instances for a given property type. A numerical vector is standardized
and normalized. Finally, for properties that contain a string of text, we use Sentence Transformers,
based on sentence-BERT [120], to embed such a property. String embeddings are usually much
longer than other numerical properties to preserve most information in strings."
IMPLEMENTATION/METHODS,0.09068825910931175,"After encoding, labels and properties are concatenated into input feature vectors for each vertex
and for each edge. Importantly, the concatenation is done after ordering the elements of a set
Labels ∪Property keys (i.e., L ∪K) and applying the same ordering for each vertex and for each
edge. This ensures that the embeddings of labels/properties follow the same order in each feature
vector and that the lengths of feature vectors for, respectively, vertices and edges, are the same."
IMPLEMENTATION/METHODS,0.09149797570850203,"3.4
Seamless Integration with GNN Models and Encodings"
IMPLEMENTATION/METHODS,0.09230769230769231,"Both vertex and edge information is straightforwardly harnessed by LPG2vec by ﬁrst encoding
the input vertex or edge labels/properties within LPG2vec. Then, we feed such vertex and edge
encodings, as input feature vectors xi (for any vertex i) and xij (for any edge (i, j)), into a selected
GNN model. Here, LPG2vec enables seamless integration with virtually any GNN model, encoding,
or architecture. This is due to the simplicity of our solution: all LPG2vec does is providing “enriched”
input feature vectors xi and xij. Vectors xi can be directly fed to any convolutional, attentional, or"
IMPLEMENTATION/METHODS,0.0931174089068826,Neural Graph Databases
IMPLEMENTATION/METHODS,0.09392712550607288,"Main task
semantics"
IMPLEMENTATION/METHODS,0.09473684210526316,"Graph
remainder"
IMPLEMENTATION/METHODS,0.09554655870445344,":ConferenceProc
:TechReport"
IMPLEMENTATION/METHODS,0.09635627530364373,:BestPaper 1 1 0 0 ... 0 1 1 0 ...
IMPLEMENTATION/METHODS,0.09716599190283401,"Abstract: We establish
a general motif prediction
problem and we propose..."
IMPLEMENTATION/METHODS,0.09797570850202429,"title = ""Enabling...""
#authors = 4
#equations = 21
#pages = 12 0.1 0.9 0.3 0.7 ..."
IMPLEMENTATION/METHODS,0.09878542510121457,"A selected
encoding for
the abstract"
IMPLEMENTATION/METHODS,0.09959514170040486,"Use one-hot
encoding for labels"
IMPLEMENTATION/METHODS,0.10040485829959514,"Normalize numeric
properties, encode
string properties with
text transformers"
IMPLEMENTATION/METHODS,0.10121457489878542,Encoding input LPG graphs (LPG2vec)
IMPLEMENTATION/METHODS,0.1020242914979757,Concatenate
IMPLEMENTATION/METHODS,0.10283400809716599,Use as feature vectors in the first GNN layer
IMPLEMENTATION/METHODS,0.10364372469635627,:VancouverStyle
IMPLEMENTATION/METHODS,0.10445344129554655,":SelfCitation
:AdvisorCitation"
IMPLEMENTATION/METHODS,0.10526315789473684,"#count = 4
section = 1"
IMPLEMENTATION/METHODS,0.10607287449392712,Vertex labels
IMPLEMENTATION/METHODS,0.10688259109311742,"Edge
labels
Vertex
properties"
IMPLEMENTATION/METHODS,0.1076923076923077,"Edge
properties 0 0 0 1 ... 0.3 0.8 0.1 0.2 ..."
IMPLEMENTATION/METHODS,0.10850202429149798,"Normalize numeric
properties, encode
string properties with
text transformers"
IMPLEMENTATION/METHODS,0.10931174089068826,Concatenate
IMPLEMENTATION/METHODS,0.11012145748987855,Vertex
IMPLEMENTATION/METHODS,0.11093117408906883,"Edge
Use one-hot
encoding
for labels"
IMPLEMENTATION/METHODS,0.11174089068825911,"Information
used in homo-
geneous graphs"
IMPLEMENTATION/METHODS,0.1125506072874494,"Information
coming from
LPG labels
& properties"
IMPLEMENTATION/METHODS,0.11336032388663968,Input graph + LPG embeddings 1
IMPLEMENTATION/METHODS,0.11417004048582996,"2
LPG rich attached information is preserved
in the form of vertex and edge embeddings."
IMPLEMENTATION/METHODS,0.11497975708502024,"3
GNN Computation"
IMPLEMENTATION/METHODS,0.11578947368421053,"Seamless integration with
existing GNN models (GCN,
GAT, GIN; A-GNNs, MP-GNNs,
GraphNets, higher-order models, ...)"
IMPLEMENTATION/METHODS,0.11659919028340081,"PyG
Neo4j"
IMPLEMENTATION/METHODS,0.11740890688259109,"Execute
within"
IMPLEMENTATION/METHODS,0.11821862348178137,"Final sizes of
edge/vertex
feature vectors
may differ"
IMPLEMENTATION/METHODS,0.11902834008097166,"Dimensionalities are
specified by the user"
IMPLEMENTATION/METHODS,0.11983805668016194,"Figure 4: An overview of LPG2vec in the context of processing LPG graphs with GNNs. First (“ 1 ”), the graph data is loaded from disk and
encoded using LPG2vec. Here, we differentiate the additional data usually used with homogeneous graphs, that determines the task semantics
(in this case, publication abstracts), from the LPG-related additional data (labels, properties). Note that, in practice, encoding the abstract could
be just implemented as encoding an additional property. The encoding process gives a graph dataset (“ 2 ”) that is ready for the actual GNN
computation that can be executed in a dedicated module of a graph database (e.g., Neo Graph Data Science) or in a dedicated ML framework
(e.g., PyG). Importantly, LPG2vec preserves all the rich LPG information in the form of vertex and edge embeddings. Thus, the actual input to
the GNN computation is a homogeneous graph structure together with the embeddings. This makes the integration with existing GNN models
straightforward (“ 3 ”). The computation itself is conducted in a dedicated module of a graph database (e.g., Neo4j’s Graph Data Science), but
- thanks to the seamless LPG2vec design (i.e., the fact that the output of LPG2vec is a homogeneous graph with enhanced feature vectors) - it
can also be conducted in a standalone GNN framework (e.g., PyG)."
IMPLEMENTATION/METHODS,0.12064777327935222,"message-passing GNN model, as the input vertex feature vectors. Vectors xij are fed into any model
that also incorporates edge feature vectors."
IMPLEMENTATION/METHODS,0.1214574898785425,"Moreover, LPG2vec also enables easy integration with encoding schemes such as MPGNNs-
LSPE [53]. This can be achieved by, for example, concatenating the LPG2vec vectors with any
additional encodings, and then using the resulting feature vectors with the selected GNN model."
RESULTS/EXPERIMENTS,0.12226720647773279,"4
Evaluation"
RESULTS/EXPERIMENTS,0.12307692307692308,"Our main goal in the evaluation is to show that LPG2vec successfully harnesses the label and property
information from the LPG graph datasets to offer more accurate predictions in graph ML tasks. Our
analysis comes with a large evaluation space. Thus, we show selected representative results; full data
is in the appendix due to space constraints."
RESULTS/EXPERIMENTS,0.12388663967611337,"4.1
Experimental Setup"
RESULTS/EXPERIMENTS,0.12469635627530365,"An important part of the experimental setup is ﬁnding the appropriate graph datasets that have
many labels and properties. First, we use the Microsoft Academic Knowledge Graph (MAKG) [56].
The original graph is in the RDF format. We extracted data from RDF triples describing consecutive
vertices, and we built LPG vertex entries containing the gathered data; single triples containing
edges were parsed directly into the LPG format. Due to the huge size of MAKG, we extracted two
subgraphs. For this, we consider the following LPG labels of vertices: :Paper, :Author, :Afﬁliation,
:ConferenceSeries, :ConferenceInstance, :FieldOfStudy, as well as the links between them. Then, we
additionally limit the number of the considered research areas (and thus vertices) in the :FieldOfStudy
ﬁeld (four for a small MAKG dataset, 25 for a large MAKG dataset); they form classes to be predicted."
RESULTS/EXPERIMENTS,0.12550607287449392,Neural Graph Databases
RESULTS/EXPERIMENTS,0.12631578947368421,"For diversiﬁed analysis, we make sure that these two datasets differ in their degree distributions,
implying different connectivity structure. Second, we use example LPG graphs provided by Neo4j2;
While these datasets are small, they are original excerpts from industry LPG databases. Most
importantly, we use a “citations” network (modeling publications and citations between them), a
“Twitter trolls” network (modeling anonymized Twitter trolls and the interaction of retweets), and a
network modeling crime investigations. The details of datasets are in Table 1; the appendix provides a
full speciﬁcation of the associated labels/properties in selected datasets, as well as additional results."
RESULTS/EXPERIMENTS,0.12712550607287448,"While there are many heterogeneous graphs available online, they have usually single labels (often
called types) per vertex or edge. We considered some of these graphs; we ﬁrst convert them
appropriately into the LPG model by transforming certain information from the graph structure into
labels and properties. Note that we do not compete with heterogeneous representations, datasets,
and the associated heterogeneous GNN models (they are outside the scope of this work); instead, we
focus on LPG because this is the main established graph data model in graph databases."
RESULTS/EXPERIMENTS,0.12793522267206478,"Dataset
#vertices #edges #labels #properties size
Prediction target & ML task details"
RESULTS/EXPERIMENTS,0.12874493927125505,"[MAKG] (small)
3.06M
12.3M 20
28
1.2 GB
Publication area (node classiﬁcation, 4 classes)
[MAKG] (large)
50.7M
190M
20
28
19.5 GB Publication area (node classiﬁcation, 25 classes)
[Neo4j] citations
132k
221k
5
6
51 MB
Citation count (node regression)
[Neo4j] Twitter trolls
281k
493k
13
14
79 MB
Retweet count (node regression)
[Neo4j] crime investigations 61.5k
106k
28
29
17 MB
Crime type (node classiﬁcation)"
RESULTS/EXPERIMENTS,0.12955465587044535,"Table 1: Considered LPG datasets & ML tasks. [Neo4j]: provided by the Neo4j online repository, [MAKG]: extracted from MS Academic
Knowledge Graph. Additional results for all the datasets are provided in the appendix."
RESULTS/EXPERIMENTS,0.13036437246963561,"We consider different established GNN models: GCN [85] (a seminal convolutional GNN model),
GAT [147] (a seminal attentional GNN model), and GIN [161] (a seminal model having more
expressive power than GCN or GAT). We test these models with and without the LPG2vec encoding
scheme. Then, when considering models enhanced with LPG2vec, we test variants that harness
the additional LPG information coming from only labels, only properties, and from both labels and
properties. Our goal is to investigate how exactly the rich additional LPG information inﬂuences the
accuracy of the established graph ML tasks, focusing on node classiﬁcation (assigning each vertex to
one of a given number of classes) and node regression (predicting a real value for each vertex) [159]."
RESULTS/EXPERIMENTS,0.1311740890688259,"We split the datasets into train, val, and test by the ratio of [0.8, 0.1, 0.1]. We set the mini-batch size to
32, use the Adam optimizer [82], the learning rate of 0.01 augmented with the cosine annealing decay,
and we train for 100 epochs. The node mini-batch sampling is conducted using the GraphSAINT
established scheme [167]. We use the cross-entropy and MSE loss functions for classiﬁcation and
regression, respectively. In the design of used GNN models (GCN, GAT, GIN), following the
established practice [165], we incorporate one preprocessing MLP layer, followed by two actual
GNN layers, and then one additional post-processing MLP layer. We use the PReLU non-linearity."
RESULTS/EXPERIMENTS,0.1319838056680162,"Our implementation is integrated into PyG [57]. We use GraphGym [165] as well as Weights &
Biases [29] for managing experiments."
RESULTS/EXPERIMENTS,0.13279352226720648,"4.2
Improving the Accuracy with LPG Labels and Properties"
RESULTS/EXPERIMENTS,0.13360323886639677,"We ﬁrst analyze how LPG2vec appropriately harnesses the rich information from LPG labels/proper-
ties, enabling accuracy improvements for different GNN models. Example results are in Figure 5,
showing both node classiﬁcation and node regression, with MAKG and Neo4j datasets. We plot the
the ﬁnal test accuracy (with the standard deviation) for classiﬁcation and the mean absolute error
(MAE) for node regression. The task is to predict the research area of the publication (for MAKG)
and the citation count of a paper (for Neo4j citations). In the results, the baseline with no LPG
labels/properties (i.e., only the neighborhood structure) consistently delivers the lowest accuracy
(MAKG), or – in some cases such as for the GCN/GAT models and Neo4j citations dataset – is unable
to converge. Then, for MAKG, including, respectively, labels (describing paper types), a property
(paper title), and both the labels and the title property, steadily improves the accuracy, reaching
nearly 35% for GCN. The trend is similar across all the studied models, and they achieve similarly
high accuracy, which indicates that harnessing the appropriate labels/properties is very relevant and -
when this information is present - different GNN models will perform similarly well. Neo4j citations"
RESULTS/EXPERIMENTS,0.13441295546558704,2Available at https://github.com/neo4j-graph-examples
RESULTS/EXPERIMENTS,0.13522267206477734,Neural Graph Databases
RESULTS/EXPERIMENTS,0.1360323886639676,"Node regression
Node classiﬁcation"
RESULTS/EXPERIMENTS,0.1368421052631579,"GCN
GAT
GIN
0 10 20 30 40 50 60 70 80"
RESULTS/EXPERIMENTS,0.13765182186234817,Test Accuracy [%]
RESULTS/EXPERIMENTS,0.13846153846153847,MAKG (small)
RESULTS/EXPERIMENTS,0.13927125506072874,"none
labels"
RESULTS/EXPERIMENTS,0.14008097165991903,"title
labels,title"
RESULTS/EXPERIMENTS,0.1408906882591093,"GCN
GAT
GIN
0 5 10 15 20 25 30 35"
RESULTS/EXPERIMENTS,0.1417004048582996,Test Accuracy [%]
RESULTS/EXPERIMENTS,0.14251012145748987,MAKG (large)
RESULTS/EXPERIMENTS,0.14331983805668017,"none
labels"
RESULTS/EXPERIMENTS,0.14412955465587043,"title
labels,title"
RESULTS/EXPERIMENTS,0.14493927125506073,"GCN
GAT
GIN
0 20 40 60 80 100 120"
RESULTS/EXPERIMENTS,0.145748987854251,Test MAE
RESULTS/EXPERIMENTS,0.1465587044534413,Neo4j (citations)
RESULTS/EXPERIMENTS,0.14736842105263157,"none
labels"
RESULTS/EXPERIMENTS,0.14817813765182186,"year
labels,year"
RESULTS/EXPERIMENTS,0.14898785425101216,"GCN
GAT
GIN
0 10 20 30 40 50 60 70"
RESULTS/EXPERIMENTS,0.14979757085020243,Test MAE
RESULTS/EXPERIMENTS,0.15060728744939272,Neo4j (Twitter trolls)
RESULTS/EXPERIMENTS,0.151417004048583,"none
labels"
RESULTS/EXPERIMENTS,0.1522267206477733,"follower_cnt
labels,follower_cnt"
RESULTS/EXPERIMENTS,0.15303643724696356,"the lower the better
the lower the better
the higher the better
the higher the better"
RESULTS/EXPERIMENTS,0.15384615384615385,"Figure 5: Advantages of preserving the information encoded in LPG labels and properties, for node classiﬁcation (the MAKG datasets in the
left panel; 4 classes for small and 25 classes for large) and node regression (the Neo4j datasets in the right panel)."
RESULTS/EXPERIMENTS,0.15465587044534412,"and Twitter trolls are similar (note the different metric as this is a node regression task). The main
difference is that, for GIN and the Twitter dataset, combining the labels and the follower count
leads to worse results than only using one of these two individually. This illustrates that certain
combinations of LPG information might not always enhance the accuracy; we study this in more
detail in Sec. 4.3. Another interesting effect takes place when considering the bare graph structure on
the small MAKG. Here, GCN performs worst, GAT is somewhat better, while GIN delivers much
higher accuracy than GAT. We conjecture this is because GIN is provably highly expressive in the
Weisfeiler-Lehman sense (when considering the bare graph structure) [161]. Overall, the results show
the importance of including both the labels and properties when analyzing LPG graphs."
RESULTS/EXPERIMENTS,0.15546558704453442,"4.3
Selecting the Right Labels and Properties"
RESULTS/EXPERIMENTS,0.1562753036437247,"In some experiments, we observed that selecting certain properties was not improving the accuracy.
Moreover, in certain cases, the accuracy was actually diminishing. We analyze this effect in more
detail in Figure 6 for the node classiﬁcation and regression on MAKG small and Neo4j Twitter trolls,
with the GIN model, plotting both train and test accuracy. The plots show the impact of using each of
the many available properties on the ﬁnal prediction accuracy. For example, on the small MAKG,
using the title property signiﬁcantly improves the accuracy, and the majority of other properties
also increase it, although by much smaller (often negligible) factor. Still, using the publication date
property in many cases decreases the accuracy (see the bottom-left plot). We further analyze this
effect with heatmaps, by considering each possible pair of properties, and how using this pair impacts
the results. The accuracy is almost always enhanced, when using title together with nearly any other
property. Some properties, such as entity id, have no effect. Many pair combinations result in slight
accuracy improvements. However, in the Neo4j Twitter case (the bottom panel), in the test accuracy,
while using many individual properties signiﬁcantly enhances the accuracy, most combinations of
property pairs decrease it. Interestingly, this only happens for the GIN model; the GCN models and
GAT models are able to extract useful knowledge from most property pairs (these results are provided
in the appendix, see Figures 12 and 13). This illustrates that it is important to understand the data and
select the right encoded LPG information and the model for a given selected graph ML task."
CONCLUSION/DISCUSSION ,0.15708502024291499,"5
Related Work and Discussion"
CONCLUSION/DISCUSSION ,0.15789473684210525,"Our work touches on many areas. We now brieﬂy discuss related works. We do not compare LPG2vec
to non-GNN baselines because our main goal is to illustrate how to integrate GNN capabilities into
GDBs, and not to argue that neural methods outperform those of traditional non-GNN baselines.
Hence, we do not focus on experiments with traditional GDB non-neural tasks such as BFS or
Connected Components."
CONCLUSION/DISCUSSION ,0.15870445344129555,"Graph Neural Networks and Graph Machine Learning Graph neural networks (GNNs) emerged
as a highly successful part of the graph machine learning ﬁeld [66]. Numerous GNN models have been
developed [20, 23, 35, 41, 62, 66, 129, 159, 170, 172], including convolutional [67, 85, 134, 156, 161],"
CONCLUSION/DISCUSSION ,0.15951417004048582,Neural Graph Databases
CONCLUSION/DISCUSSION ,0.16032388663967612,"Neo4j Twitter, node regression (the lower the better), GIN model"
CONCLUSION/DISCUSSION ,0.16113360323886639,"MAKG small, node classification (the higher the better), GIN model 0 10 20 30 40 50 60 70"
CONCLUSION/DISCUSSION ,0.16194331983805668,Train Accuracy [%]
CONCLUSION/DISCUSSION ,0.16275303643724695,est_cite_cnt
CONCLUSION/DISCUSSION ,0.16356275303643725,pub_date
CONCLUSION/DISCUSSION ,0.16437246963562754,"ref_cnt
volume"
CONCLUSION/DISCUSSION ,0.1651821862348178,"rank
entity_id"
CONCLUSION/DISCUSSION ,0.1659919028340081,cite_cnt
CONCLUSION/DISCUSSION ,0.16680161943319838,"created
end_page
paper_cnt
start_page"
CONCLUSION/DISCUSSION ,0.16761133603238867,issue_id
CONCLUSION/DISCUSSION ,0.16842105263157894,"doi
name
publisher title 58 60 62 64 66 68 70 0 10 20 30 40 50 60 70"
CONCLUSION/DISCUSSION ,0.16923076923076924,Test Accuracy [%]
CONCLUSION/DISCUSSION ,0.1700404858299595,est_cite_cnt
CONCLUSION/DISCUSSION ,0.1708502024291498,pub_date
CONCLUSION/DISCUSSION ,0.17165991902834007,"ref_cnt
volume
cite_cnt"
CONCLUSION/DISCUSSION ,0.17246963562753037,created
CONCLUSION/DISCUSSION ,0.17327935222672064,"doi
end_page"
CONCLUSION/DISCUSSION ,0.17408906882591094,"entity_id
paper_cnt
start_page"
CONCLUSION/DISCUSSION ,0.1748987854251012,issue_id
CONCLUSION/DISCUSSION ,0.1757085020242915,"rank
publisher name"
CONCLUSION/DISCUSSION ,0.17651821862348177,"title
58 60 62 64 66 68 70 0 5 10 15 20 25 30"
CONCLUSION/DISCUSSION ,0.17732793522267207,Train MAE
CONCLUSION/DISCUSSION ,0.17813765182186234,"userkey
scr_name verif"
CONCLUSION/DISCUSSION ,0.17894736842105263,"tag
retweeted"
CONCLUSION/DISCUSSION ,0.1797570850202429,"exp_url
timezone"
CONCLUSION/DISCUSSION ,0.1805668016194332,"loc
name
favs_cnt
friend_cnt
follower_cnt"
CONCLUSION/DISCUSSION ,0.1813765182186235,"lang
listed_cnt"
CONCLUSION/DISCUSSION ,0.18218623481781376,stat_cnt
CONCLUSION/DISCUSSION ,0.18299595141700406,"desc
fav_cnt text 10 12 14 16 18 20 22 0 5 10 15 20 25"
CONCLUSION/DISCUSSION ,0.18380566801619433,Test MAE desc
CONCLUSION/DISCUSSION ,0.18461538461538463,"text
timezone name"
CONCLUSION/DISCUSSION ,0.1854251012145749,"lang
userkey
scr_name verif"
CONCLUSION/DISCUSSION ,0.1862348178137652,"tag
retweeted"
CONCLUSION/DISCUSSION ,0.18704453441295546,exp_url
CONCLUSION/DISCUSSION ,0.18785425101214576,"loc
favs_cnt"
CONCLUSION/DISCUSSION ,0.18866396761133603,stat_cnt
CONCLUSION/DISCUSSION ,0.18947368421052632,"fav_cnt
friend_cnt
follower_cnt"
CONCLUSION/DISCUSSION ,0.1902834008097166,listed_cnt 5 10 15 20 25 30 35 40 45
CONCLUSION/DISCUSSION ,0.1910931174089069,"est_cite_cnt
pub_date
ref_cnt
none
volume
rank
entity_id
cite_cnt
created
end_page
paper_cnt
start_page
issue_id
doi
name
publisher
title"
CONCLUSION/DISCUSSION ,0.19190283400809716,"none
est_cite_cnt
pub_date
ref_cnt
volume
cite_cnt
created
doi
end_page
entity_id
paper_cnt
start_page
issue_id
rank
publisher
name
title"
CONCLUSION/DISCUSSION ,0.19271255060728745,"est_cite_cnt
pub_date
ref_cnt
volume
rank
entity_id
cite_cnt
created
end_page
paper_cnt
start_page
issue_id
doi
name
publisher
title"
CONCLUSION/DISCUSSION ,0.19352226720647772,"est_cite_cnt
pub_date
ref_cnt
volume
cite_cnt
created
doi
end_page
entity_id
paper_cnt
start_page
issue_id
rank
publisher
name
title"
CONCLUSION/DISCUSSION ,0.19433198380566802,"none
userkey
scr_name
verif
tag
retweeted
exp_url
timezone
loc
name
favs_cnt
friend_cnt
follower_cnt
lang
listed_cnt
stat_cnt
desc
fav_cnt
text"
CONCLUSION/DISCUSSION ,0.1951417004048583,"userkey
scr_name
verif
tag
retweeted
exp_url
timezone
loc
name
favs_cnt
friend_cnt
follower_cnt
lang
listed_cnt
stat_cnt
desc
fav_cnt
text"
CONCLUSION/DISCUSSION ,0.19595141700404858,"desc
text
none
timezone
name
lang
userkey
scr_name
verif
tag
retweeted
exp_url
loc
favs_cnt
stat_cnt
fav_cnt
friend_cnt
follower_cnt
listed_cnt"
CONCLUSION/DISCUSSION ,0.19676113360323888,"desc
text
timezone
name
lang
userkey
scr_name
verif
tag
retweeted
exp_url
loc
favs_cnt
stat_cnt
fav_cnt
friend_cnt
follower_cnt
listed_cnt"
CONCLUSION/DISCUSSION ,0.19757085020242915,Neural Graph Databases
CONCLUSION/DISCUSSION ,0.19838056680161945,"attentional [103, 139, 147], message-passing [13, 33, 64, 127, 155], or – more recently – higher-order
ones [1, 2, 14, 31, 104, 124, 125]. Moreover, a large number of software frameworks [57, 72, 74, 89,
93, 144, 148–151, 154, 158, 169, 171, 173], and even hardware accelerators [60, 83, 84, 90, 162] for
processing GNNs have been introduced over the last years. LPG2vec enables using all these designs
together with the LPG graphs and consequently with LPG-based graph databases. This is because
of the fact that the information within LPG labels and properties is encoded into the input features
vectors, which can then be seamlessly used with essentially any GNN model or framework of choice."
CONCLUSION/DISCUSSION ,0.19919028340080971,"Graph Databases (GDBs) [26] are systems used to manage, process, analyze, and store vast amounts
of rich and complex graph datasets. GDBs have a long history of development and focus in both
academia and in the industry, and there has been signiﬁcant work on them [6, 7, 49, 59, 68, 78, 86].
A lot of research has been dedicated to graph query languages [5, 5, 32], GDB management [32, 76,
100, 113, 115], compression in GDBs and data models [16, 22, 25, 28, 92, 94, 105], execution in
novel environments such as the serverless setting [45, 96, 143], and others. Many GDBs exist [4, 8–
11, 30, 37, 38, 40, 47, 51, 52, 55, 58, 79, 80, 97–99, 107–111, 118, 119, 121, 133, 136–138, 141,
142, 166, 174, 175]. We enhance the learning capabilities of graph databases by illustrating how to
harness all the information encoded in Labeled Property Graph (LPG), a data model underlying the
majority of graph databases, and use it for graph ML tasks such as node classiﬁcation."
CONCLUSION/DISCUSSION ,0.2,"Resource Description Framework and Knowledge Graphs Resource Description Framework
(RDF) [87] is a standard originally developed as a data model for metadata. It consists of triples,
i.e., 3-tuples used to encode any information in a graph. Hence, while it has been used to encode
knowledge in ontological models and in frameworks called RDF stores [69, 102, 112], it is less
common in graph databases that focus on achieving high performance, low latency, and large scale.
The reason is that LPG facilitates explicit storage of graph structure, and thus makes it easier to achieve
high performance of different graph algorithms and complex business-intelligence graph queries that
commonly require accessing graph neighborhoods [26]. We focus on graph databases built on top of
LPG, and thus RDF and the associated graph ML models such as RDF2vec [77, 117, 122, 123] are
outside the scope of this work. Note that the notions of label and property prediction are analogous to
the concepts of knowledge graph completion [3, 12, 43, 46, 91, 91, 130, 131, 135, 145, 146, 152, 164]."
CONCLUSION/DISCUSSION ,0.20080971659919028,"Dynamic, Temporal, and Streaming Graph Processing Frameworks There also exist systems
for processing dynamic, temporal, and streaming graphs [15, 19, 44, 126]. Their setting partially
overlaps with graph databases, because they also focus on high-performance graph processing
and on solving (in a dynamic setting) graph problems such as Betweenness Centrality [95, 132],
Graph Traversals [21, 24, 36, 81], Connected Components, Graph Coloring, Matchings, and many
others [17, 18, 27, 50, 54, 61, 63, 70, 73, 88, 140]. However, the rate of updates in such systems is
much higher than in graph databases, thus requiring usually signiﬁcantly different system designs
and architectures. More importantly, such frameworks usually do not focus on rich data and do not
use the LPG model. Hence, these systems differ fundamentally from graph databases, and are outside
the focus of this paper."
CONCLUSION/DISCUSSION ,0.20161943319838058,"6
Conclusion"
CONCLUSION/DISCUSSION ,0.20242914979757085,"Graph databases (GDBs), despite being an important part of the graph analytics landscape, have still
not embraced the full predictive capabilities of graph neural networks (GNNs). To address this, we
ﬁrst observe that the majority of graph databases use, or support, the Labeled Property Graph (LPG)
as their data model. In LPG, the graph structure, stored explicitly in the compressed-sparse row
format, is combined with labels and key-value properties that can be attached, in any conﬁguration,
to vertices and edges. To integrate GDBs with graph machine learning capabilities, we develop
LPG2vec, an encoder that converts LPG labels and properties into input vertex and edge embeddings.
This enables seamless integration of any GDB with any GNN model of interest."
CONCLUSION/DISCUSSION ,0.20323886639676114,"Our evaluation shows that incorporating labels and properties into GNN models consistently improves
accuracy. For example, GCN, GAT, and GIN models achieve even up to 34% better accuracy in node
classiﬁcation for the LPG representation of the Microsoft Academic Knowledge Graph, compared to a
setting without LPG labels and properties. We conclude that LPG2vec will facilitate the development
of neural graph databases, a learning architecture that harnesses both the structure and rich data
(labels, properties) of LPG for highly accurate predictions in graph databases. It will lead to the wider
adoption of GNNs in the broad graph database industry setting."
CONCLUSION/DISCUSSION ,0.2040485829959514,Neural Graph Databases
REFERENCES,0.2048582995951417,References
REFERENCES,0.20566801619433198,"[1] Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr Haru-
tyunyan, Greg Ver Steeg, and Aram Galstyan. 2019. MixHop: Higher-Order Graph Convolutional
Architectures via Sparsiﬁed Neighborhood Mixing. arXiv:1905.00067 [cs, stat] (June 2019). arXiv:cs,
stat/1905.00067"
REFERENCES,0.20647773279352227,"[2] Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr Harutyun-
yan, Greg Ver Steeg, and Aram Galstyan. 2019. Mixhop: Higher-order graph convolutional architectures
via sparsiﬁed neighborhood mixing. In international conference on machine learning. PMLR, 21–29."
REFERENCES,0.20728744939271254,"[3] Farahnaz Akrami, Mohammed Samiul Saeef, Qingheng Zhang, Wei Hu, and Chengkai Li. 2020. Realistic
re-evaluation of knowledge graph completion methods: An experimental study. In Proceedings of the
2020 ACM SIGMOD International Conference on Management of Data. 1995–2010."
REFERENCES,0.20809716599190284,[4] Amazon. 2018. Amazon Neptune. Available at https://aws.amazon.com/neptune/.
REFERENCES,0.2089068825910931,"[5] Renzo Angles, Marcelo Arenas, Pablo Barceló, Aidan Hogan, Juan Reutter, and Domagoj Vrgoˇc. 2017.
Foundations of Modern Query Languages for Graph Databases. in ACM Comput. Surv. 50, 5, Article 68
(2017), 40 pages. https://doi.org/10.1145/3104031"
REFERENCES,0.2097165991902834,"[6] Renzo Angles and Claudio Gutierrez. 2008. Survey of Graph Database Models. in ACM Comput. Surv.
40, 1, Article 1 (2008), 39 pages. https://doi.org/10.1145/1322432.1322433"
REFERENCES,0.21052631578947367,"[7] Renzo Angles and Claudio Gutierrez. 2018. An Introduction to Graph Data Management. In Graph Data
Management, Fundamental Issues and Recent Developments. 1–32."
REFERENCES,0.21133603238866397,[8] Apache. 2018. Apache Mormotta. Available at http://marmotta.apache.org/.
REFERENCES,0.21214574898785424,"[9] ArangoDB Inc. 2018.
ArangoDB.
Available at https://docs.arangodb.com/3.3/Manual/
DataModeling/Concepts.html."
REFERENCES,0.21295546558704453,"[10] ArangoDB
Inc.
2018.
ArangoDB:
Index
Free
Adjacency
or
Hybrid
In-
dexes
for
Graph
Databases.
Available
at
https://www.arangodb.com/2016/04/
index-free-adjacency-hybrid-indexes-graph-databases/."
REFERENCES,0.21376518218623483,"[11] ArangoDB Inc. 2018. ArangoDB Starter Tool. Available at https://docs.arangodb.com/devel/
Manual/Tutorials/Starter/."
REFERENCES,0.2145748987854251,"[12] Siddhant Arora. 2020. A survey on graph neural networks for knowledge graph completion. arXiv
preprint arXiv:2007.12374 (2020)."
REFERENCES,0.2153846153846154,"[13] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. 2018.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261 (2018)."
REFERENCES,0.21619433198380567,"[14] Austin R Benson et al. 2018. Simplicial closure and higher-order link prediction. Proceedings of the
National Academy of Sciences 115, 48 (2018), E11221–E11230."
REFERENCES,0.21700404858299596,"[15] Maciej Besta. 2021. Enabling High-Performance Large-Scale Irregular Computations. Ph.D. Dissertation.
ETH Zurich."
REFERENCES,0.21781376518218623,"[16] Maciej Besta et al. 2019. Slim Graph: Practical Lossy Graph Compression for Approximate Graph
Processing, Storage, and Analytics. , Article 35 (2019), 25 pages.
https://doi.org/10.1145/
3295500.3356182"
REFERENCES,0.21862348178137653,"[17] Maciej Besta et al. 2021. GraphMineSuite: Enabling High-Performance and Programmable Graph Mining
Algorithms with Set Algebra. VLDB."
REFERENCES,0.2194331983805668,"[18] Maciej Besta et al. 2021. SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-
in-Memory Systems. arXiv preprint arXiv:2104.07582 (2021)."
REFERENCES,0.2202429149797571,"[19] Maciej Besta et al. 2022. Practice of Streaming Processing of Dynamic Graphs: Concepts, Models, and
Systems. IEEE TPDS (2022)."
REFERENCES,0.22105263157894736,"[20] Maciej Besta, Raphael Grob, Cesare Miglioli, Nicola Bernold, Grzegorz Kwasniewski, Gabriel Gjini,
Raghavendra Kanakagiri, Saleh Ashkboos, Lukas Gianinazzi, Nikoli Dryden, et al. 2022. Motif Prediction
with Graph Neural Networks, In ACM KDD. arXiv preprint arXiv:2106.00761."
REFERENCES,0.22186234817813766,"[21] Maciej Besta and Torsten Hoeﬂer. 2015. Accelerating Irregular Computations with Hardware Trans-
actional Memory and Active Messages. In Proc. of the Intl. Symp. on High-Perf. Par. and Dist. Comp.
(HPDC ’15). 161–172."
REFERENCES,0.22267206477732793,"[22] Maciej Besta and Torsten Hoeﬂer. 2018. Survey and taxonomy of lossless graph compression and
space-efﬁcient graph representations. arXiv preprint arXiv:1806.01799 (2018)."
REFERENCES,0.22348178137651822,"[23] Maciej Besta and Torsten Hoeﬂer. 2022. Parallel and Distributed Graph Neural Networks: An In-Depth
Concurrency Analysis. arXiv preprint arXiv:2205.09702 (2022)."
REFERENCES,0.2242914979757085,Neural Graph Databases
REFERENCES,0.2251012145748988,"[24] Maciej Besta, Florian Marending, Edgar Solomonik, and Torsten Hoeﬂer. 2017. Slimsell: A vectorizable
graph representation for breadth-ﬁrst search. In IEEE IPDPS. IEEE, 32–41."
REFERENCES,0.22591093117408906,"[25] Maciej Besta, Cesare Miglioli, Paolo Sylos Labini, Jakub Tˇetek, Patrick Iff, Raghavendra Kanakagiri,
Saleh Ashkboos, Kacper Janda, Michal Podstawski, Grzegorz Kwasniewski, et al. 2022. ProbGraph: High-
Performance and High-Accuracy Graph Mining with Probabilistic Set Representations. In ACM/IEEE
Supercomputing."
REFERENCES,0.22672064777327935,"[26] Maciej Besta, Emanuel Peter, Robert Gerstenberger, Marc Fischer, Michał Podstawski, Claude Barthels,
Gustavo Alonso, and Torsten Hoeﬂer. 2019. Demystifying Graph Databases: Analysis and Taxonomy of
Data Organization, System Designs, and Graph Queries. arXiv preprint arXiv:1910.09017 (2019)."
REFERENCES,0.22753036437246962,"[27] Maciej Besta, Michał Podstawski, Linus Groner, Edgar Solomonik, and Torsten Hoeﬂer. 2017. To push
or to pull: On reducing communication and synchronization in graph computations. In ACM HPDC."
REFERENCES,0.22834008097165992,"[28] Maciej Besta, Dimitri Stanojevic, Tijana Zivic, Jagpreet Singh, Maurice Hoerold, and Torsten Hoeﬂer.
2018. Log (graph): a near-optimal high-performance graph representation.. In PACT (Limassol, Cyprus).
ACM, Article 7, 13 pages. https://doi.org/10.1145/3243176.3243198"
REFERENCES,0.2291497975708502,"[29] Lukas Biewald. 2020. Experiment Tracking with Weights and Biases.
https://www.wandb.com/
Software available from wandb.com."
REFERENCES,0.22995951417004049,[30] Blazegraph. 2018. BlazeGraph DB. Available at https://www.blazegraph.com/.
REFERENCES,0.23076923076923078,"[31] Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter, Guido F Montufar, Pietro Lio, and Michael
Bronstein. 2021. Weisfeiler and lehman go topological: Message passing simplicial networks. In
International Conference on Machine Learning. PMLR, 1026–1037."
REFERENCES,0.23157894736842105,"[32] Angela Bonifati, George Fletcher, Hannes Voigt, and Nikolay Yakovets. 2018. Querying graphs. Synthesis
Lectures on Data Management 10, 3 (2018), 1–184."
REFERENCES,0.23238866396761135,"[33] Xavier Bresson and Thomas Laurent. 2017.
Residual gated graph convnets.
arXiv preprint
arXiv:1711.07553 (2017)."
REFERENCES,0.23319838056680162,"[34] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c. 2021. Geometric deep learning:
Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478 (2021)."
REFERENCES,0.2340080971659919,"[35] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. 2017. Ge-
ometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine 34, 4 (2017),
18–42."
REFERENCES,0.23481781376518218,"[36] Aydın Buluç and John R Gilbert. 2011. The Combinatorial BLAS: Design, implementation, and applica-
tions. IJHPCA 25, 4 (2011), 496–509."
REFERENCES,0.23562753036437248,"[37] Callidus Software Inc. 2018. OrientDB: Lightweight Edges. Available at https://orientdb.com/
docs/3.0.x/java/Lightweight-Edges.html."
REFERENCES,0.23643724696356275,"[38] Cambridge Semantics. 2018. AnzoGraph. Available at https://www.cambridgesemantics.com/
product/anzograph/."
REFERENCES,0.23724696356275304,"[39] Wenming Cao, Zhiyue Yan, Zhiquan He, and Zhihai He. 2020. A comprehensive survey on geometric
deep learning. IEEE Access 8 (2020), 35929–35949."
REFERENCES,0.2380566801619433,"[40] Cayley. 2018.
CayleyGraph.
Available at https://cayley.io/ and https://github.com/
cayleygraph/cayley."
REFERENCES,0.2388663967611336,"[41] Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher Ré, and Kevin Murphy. 2020. Machine
learning on graphs: A model and comprehensive taxonomy. arXiv preprint arXiv:2005.03675 (2020)."
REFERENCES,0.23967611336032388,"[42] Zhiqian Chen et al. 2020. Bridging the gap between spatial and spectral domains: A survey on graph
neural networks. arXiv preprint arXiv:2002.11867 (2020)."
REFERENCES,0.24048582995951417,"[43] Zhe Chen, Yuehan Wang, Bin Zhao, Jing Cheng, Xin Zhao, and Zongtao Duan. 2020. Knowledge graph
completion: A review. Ieee Access 8 (2020), 192435–192456."
REFERENCES,0.24129554655870444,"[44] Sutanay Choudhury, Khushbu Agarwal, Sumit Purohit, Baichuan Zhang, Meg Pirrung, Will Smith, and
Mathew Thomas. 2017. Nous: Construction and querying of dynamic knowledge graphs. In IEEE ICDE.
1563–1565."
REFERENCES,0.24210526315789474,"[45] Marcin Copik, Grzegorz Kwasniewski, Maciej Besta, Michal Podstawski, and Torsten Hoeﬂer.
2020. SeBS: A Serverless Benchmark Suite for Function-as-a-Service Computing. arXiv preprint
arXiv:2012.14132 (2020)."
REFERENCES,0.242914979757085,"[46] Yuanfei Dai, Shiping Wang, Neal N Xiong, and Wenzhong Guo. 2020. A survey on knowledge graph
embedding: Approaches, applications and benchmarks. Electronics 9, 5 (2020), 750."
REFERENCES,0.2437246963562753,"[47] DataStax, Inc. 2018. DSE Graph (DataStax). Available at https://www.datastax.com/."
REFERENCES,0.24453441295546557,Neural Graph Databases
REFERENCES,0.24534412955465587,"[48] Alex Davies, Petar Veliˇckovi´c, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomašev, Richard
Tanburn, Peter Battaglia, Charles Blundell, András Juhász, et al. 2021. Advancing mathematics by
guiding human intuition with AI. Nature 600, 7887 (2021), 70–74."
REFERENCES,0.24615384615384617,"[49] Ali Davoudian, Liu Chen, and Mengchi Liu. 2018. A survey on NoSQL stores. ACM Computing Surveys
(CSUR) 51, 2, Article 40 (2018), 43 pages. https://doi.org/10.1145/3158661"
REFERENCES,0.24696356275303644,"[50] Camil Demetrescu, David Eppstein, Zvi Galil, and Giuseppe F Italiano. 2009. Dynamic graph algorithms.
In Algorithms and Theory of Computation Handbook, Volume 1. Chapman and Hall/CRC, 235–262."
REFERENCES,0.24777327935222673,"[51] Dgraph Labs, Inc. 2018. DGraph. Available at https://dgraph.io/, https://docs.dgraph.io/
design-concepts."
REFERENCES,0.248582995951417,"[52] Ayush Dubey, Greg D Hill, Robert Escriva, and Emin Gün Sirer. 2016. Weaver: a high-performance,
transactional graph database based on reﬁnable timestamps. Proceedings of the VLDB Endowment 9, 11
(2016), 852–863."
REFERENCES,0.2493927125506073,"[53] Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.
2021. Graph neural networks with learnable structural and positional representations. arXiv preprint
arXiv:2110.07875 (2021)."
REFERENCES,0.25020242914979757,"[54] David Eppstein, Zvi Galil, and Giuseppe F Italiano. 1999. Dynamic graph algorithms. Algorithms and
theory of computation handbook 1 (1999), 9–1."
REFERENCES,0.25101214574898784,[55] FactNexus. 2018. GraphBase. Available at https://graphbase.ai/.
REFERENCES,0.25182186234817816,"[56] Michael Färber. 2019. The Microsoft Academic Knowledge Graph: A Linked Data Source with 8 Billion
Triples of Scholarly Data. In Proceedings of the 18th International Semantic Web Conference (Auckland,
New Zealand) (ISWC’19). 113–129. https://doi.org/10.1007/978-3-030-30796-7_8"
REFERENCES,0.25263157894736843,"[57] Matthias Fey and Jan Eric Lenssen. 2019. Fast graph representation learning with PyTorch Geometric.
arXiv preprint arXiv:1903.02428 (2019)."
REFERENCES,0.2534412955465587,[58] Franz Inc. 2018. AllegroGraph. Available at https://franz.com/agraph/allegrograph/.
REFERENCES,0.25425101214574897,[59] Santhosh Kumar Gajendran. 2012. A survey on NoSQL databases. University of Illinois (2012).
REFERENCES,0.2550607287449393,"[60] Tong Geng, Ang Li, Runbin Shi, Chunshu Wu, Tianqi Wang, Yanfei Li, Pouya Haghi, Antonino Tumeo,
Shuai Che, Steve Reinhardt, et al. 2020. AWB-GCN: A graph convolutional network accelerator with
runtime workload rebalancing. In IEEE/ACM MICRO."
REFERENCES,0.25587044534412956,"[61] Lukas Gianinazzi, Maciej Besta, Yannick Schaffner, and Torsten Hoeﬂer. 2021. Parallel Algorithms for
Finding Large Cliques in Sparse Graphs. In Proceedings of the 33rd ACM Symposium on Parallelism in
Algorithms and Architectures. 243–253."
REFERENCES,0.25668016194331983,"[62] Lukas Gianinazzi, Maximilian Fries, Nikoli Dryden, Tal Ben-Nun, and Torsten Hoeﬂer. 2021. Learning
Combinatorial Node Labeling Algorithms. arXiv preprint arXiv:2106.03594 (2021)."
REFERENCES,0.2574898785425101,"[63] Lukas Gianinazzi, Pavel Kalvoda, Alessandro De Palma, Maciej Besta, and Torsten Hoeﬂer. 2018.
Communication-avoiding parallel minimum cuts and connected components, In ACM SIGPLAN Notices.
ACM SIGPLAN Notices 53, 1, 219–232. https://doi.org/10.1145/3200691.3178504"
REFERENCES,0.2582995951417004,"[64] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. Neural
message passing for quantum chemistry. In International Conference on Machine Learning. PMLR,
1263–1272."
REFERENCES,0.2591093117408907,"[65] William L Hamilton. 2020. Graph representation learning. Synthesis Lectures on Artiﬁcal Intelligence
and Machine Learning 14, 3 (2020), 1–159."
REFERENCES,0.25991902834008096,"[66] William L Hamilton et al. 2017. Representation learning on graphs: Methods and applications. arXiv
preprint arXiv:1709.05584 (2017)."
REFERENCES,0.26072874493927123,"[67] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation learning on large
graphs. In NeurIPS."
REFERENCES,0.26153846153846155,"[68] Jing Han, E Haihong, Guan Le, and Jian Du. 2011. Survey on NoSQL database. In 2011 6th international
conference on pervasive computing and applications. IEEE, 363–366."
REFERENCES,0.2623481781376518,"[69] Steve Harris, Nick Lamb, Nigel Shadbolt, et al. 2009. 4store: The design and implementation of a
clustered RDF store. In 5th International Workshop on Scalable Semantic Web Knowledge Base Systems
(SSWS2009), Vol. 94."
REFERENCES,0.2631578947368421,"[70] Monika R Henzinger and Valerie King. 1999. Randomized fully dynamic graph algorithms with polylog-
arithmic time per operation. Journal of the ACM (JACM) 46, 4 (1999), 502–516."
REFERENCES,0.2639676113360324,"[71] Amy E Hodler and Mark Needham. 2022. Graph Data Science Using Neo4j. In Massive Graph Analytics.
Chapman and Hall/CRC, 433–457."
REFERENCES,0.2647773279352227,"[72] Yuwei Hu et al. 2020. Featgraph: A ﬂexible and efﬁcient backend for graph neural network systems.
arXiv preprint arXiv:2008.11359 (2020)."
REFERENCES,0.26558704453441295,Neural Graph Databases
REFERENCES,0.2663967611336032,"[73] Zoran Ivkovi´c and Errol L Lloyd. 1993. Fully dynamic maintenance of vertex cover. In International
Workshop on Graph-Theoretic Concepts in Computer Science. Springer, 99–111."
REFERENCES,0.26720647773279355,"[74] Zhihao Jia et al. 2020. Improving the accuracy, scalability, and performance of graph neural networks
with roc. MLSys (2020)."
REFERENCES,0.2680161943319838,"[75] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn
Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. 2021. Highly accurate protein
structure prediction with AlphaFold. Nature 596, 7873 (2021), 583–589."
REFERENCES,0.2688259109311741,"[76] Martin Junghanns, André Petermann, Martin Neumann, and Erhard Rahm. 2017. Management and
analysis of big graph data: current systems and open challenges. In Handbook of Big Data Technologies.
Springer, 457–505."
REFERENCES,0.26963562753036435,"[77] Matthias Jurisch and Bodo Igler. 2018. RDF2Vec-based classiﬁcation of ontology alignment changes.
arXiv preprint arXiv:1805.09145 (2018)."
REFERENCES,0.2704453441295547,[78] R. Kumar Kaliyar. 2015. Graph databases: A survey. In ICCCA. 785–790.
REFERENCES,0.27125506072874495,"[79] U. Kang, Hanghang Tong, Jimeng Sun, Ching-Yung Lin, and Christos Faloutsos. 2012. Gbase: An
Efﬁcient Analysis Platform for Large Graphs. In PVLDB 21, 5 (2012), 637–650. https://doi.org/10.
1007/s00778-012-0283-9"
REFERENCES,0.2720647773279352,"[80] Chathura Kankanamge, Siddhartha Sahu, Amine Mhedbhi, Jeremy Chen, et al. 2017. Graphﬂow: An
Active Graph Database. In ACM SIGMOD (Chicago, Illinois, USA). 1695–1698. https://doi.org/
10.1145/3035918.3056445"
REFERENCES,0.2728744939271255,"[81] Jeremy Kepner, Peter Aaltonen, David Bader, Aydin Buluç, Franz Franchetti, John Gilbert, Dylan Hutchi-
son, Manoj Kumar, Andrew Lumsdaine, Henning Meyerhenke, et al. 2016. Mathematical foundations of
the GraphBLAS. In 2016 IEEE High Performance Extreme Computing Conference (HPEC). IEEE, 1–9."
REFERENCES,0.2736842105263158,"[82] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)."
REFERENCES,0.2744939271255061,"[83] Kevin Kiningham, Philip Levis, and Christopher Ré. 2020. GReTA: Hardware Optimized Graph Process-
ing for GNNs. In ReCoML."
REFERENCES,0.27530364372469635,"[84] Kevin Kiningham, Christopher Re, and Philip Levis. 2020. GRIP: a graph neural network accelerator
architecture. arXiv preprint arXiv:2007.13828 (2020)."
REFERENCES,0.2761133603238866,"[85] Thomas N Kipf and Max Welling. 2016. Semi-supervised classiﬁcation with graph convolutional networks.
arXiv preprint arXiv:1609.02907 (2016)."
REFERENCES,0.27692307692307694,"[86] Vijay Kumar and Anjan Babu. 2015. Domain Suitable Graph Database Selection: A Preliminary Report.
In 3rd International Conference on Advances in Engineering Sciences & Applied Mathematics, London,
UK. 26–29."
REFERENCES,0.2777327935222672,"[87] Ora Lassila, Ralph R Swick, et al. 1998. Resource description framework (RDF) model and syntax
speciﬁcation. (1998)."
REFERENCES,0.2785425101214575,"[88] Min-Joong Lee, Sunghee Choi, and Chin-Wan Chung. 2016. Efﬁcient algorithms for updating betweenness
centrality in fully dynamic graphs. Information Sciences 326 (2016), 278–296."
REFERENCES,0.2793522267206478,"[89] Shen Li et al. 2020. Pytorch distributed: Experiences on accelerating data parallel training. arXiv preprint
arXiv:2006.15704 (2020)."
REFERENCES,0.28016194331983807,"[90] Shengwen Liang, Ying Wang, Cheng Liu, Lei He, LI Huawei, Dawen Xu, and Xiaowei Li. 2020. Engn:
A high-throughput and energy-efﬁcient accelerator for large graph neural networks. IEEE TOC (2020)."
REFERENCES,0.28097165991902834,"[91] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation
embeddings for knowledge graph completion. In AAAI."
REFERENCES,0.2817813765182186,"[92] Bingqing Lyu, Lu Qin, Xuemin Lin, Lijun Chang, and Jeffrey Xu Yu. 2016. Scalable supergraph search
in large graph databases. In 2016 IEEE 32nd International Conference on Data Engineering (ICDE).
IEEE, 157–168."
REFERENCES,0.28259109311740893,"[93] Lingxiao Ma, Zhi Yang, Youshan Miao, Jilong Xue, Ming Wu, Lidong Zhou, and Yafei Dai. 2019.
Neugraph: parallel deep neural network computation on large graphs. In USENIX ATC."
REFERENCES,0.2834008097165992,"[94] Shuai Ma, Jia Li, Chunming Hu, Xuelian Lin, and Jinpeng Huai. 2016. Big graph search: challenges and
techniques. Frontiers of Computer Science 10, 3 (2016), 387–398."
REFERENCES,0.28421052631578947,"[95] Kamesh Madduri, David Ediger, Karl Jiang, David A Bader, and Daniel Chavarria-Miranda. 2009. A faster
parallel algorithm and efﬁcient multithreaded implementations for evaluating betweenness centrality
on massive datasets. In Parallel & Distributed Processing, 2009. IPDPS 2009. IEEE International
Symposium on. IEEE, 1–8."
REFERENCES,0.28502024291497974,Neural Graph Databases
REFERENCES,0.28582995951417006,"[96] Zhitao Mao, Ruoyu Wang, Haoran Li, Yixin Huang, Qiang Zhang, Xiaoping Liao, and Hongwu Ma.
2022. ERMer: a serverless platform for navigating, analyzing, and visualizing Escherichia coli regulatory
landscape through graph database. Nucleic Acids Research (2022)."
REFERENCES,0.28663967611336033,"[97] Norbert Martínez-Bazan, Victor Muntés-Mulero, Sergio Gómez-Villamor, M.Ángel Águila Lorente,
David Dominguez-Sal, and Josep-L. Larriba-Pey. 2012. Efﬁcient Graph Management Based On Bitmap
Indices. In IDEAS (2012), 110–119. https://doi.org/10.1145/2351476.2351489"
REFERENCES,0.2874493927125506,[98] Memgraph Ltd. 2018. Memgraph. Available at https://memgraph.com/.
REFERENCES,0.28825910931174087,"[99] Microsoft. 2018. Azure Cosmos DB. Available at https://azure.microsoft.com/en-us/services/
cosmos-db/."
REFERENCES,0.2890688259109312,"[100] Justin J Miller. 2013. Graph Database Applications and Concepts with Neo4j. In Proceedings of the
Southern Association for Information Systems Conference, Vol. 2324."
REFERENCES,0.28987854251012146,"[101] Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang,
Young-Joon Lee, Eric Johnson, Omkar Pathak, Azade Nazi, et al. 2021. A graph placement methodology
for fast chip design. Nature 594, 7862 (2021), 207–212."
REFERENCES,0.29068825910931173,"[102] Gianfranco E Modoni, Marco Sacco, and Walter Terkaj. 2014. A survey of RDF store solutions. In 2014
International Conference on Engineering, Technology and Innovation (ICE). IEEE, 1–7."
REFERENCES,0.291497975708502,"[103] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M
Bronstein. 2017. Geometric deep learning on graphs and manifolds using mixture model cnns. In IEEE
CVPR."
REFERENCES,0.2923076923076923,"[104] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan,
and Martin Grohe. 2019. Weisfeiler and leman go neural: Higher-order graph neural networks. In
Proceedings of the AAAI conference on artiﬁcial intelligence, Vol. 33. 4602–4609."
REFERENCES,0.2931174089068826,"[105] Chemseddine Nabti and Hamida Seba. 2017. Querying massive graph data: A compress and search
approach. Future Generation Computer Systems 74 (2017), 63–75."
REFERENCES,0.29392712550607286,"[106] Neo4j, Inc. 2018. Neo4j (3.0 Release).
Available at https://neo4j.com/blog/neo4j-3-0-massive-scale-developer-productivity/."
REFERENCES,0.29473684210526313,[107] Networked Planet Limited. 2018. BrightstarDB. Available at http://brightstardb.com/.
REFERENCES,0.29554655870445345,"[108] Objectivity Inc. 2018.
InﬁniteGraph.
Available at https://www.objectivity.com/products/
infinitegraph/."
REFERENCES,0.2963562753036437,[109] Ontotext. 2018. GraphDB. Available at https://www.ontotext.com/products/graphdb/.
REFERENCES,0.297165991902834,[110] OpenLink. 2018. Virtuoso. Available at https://virtuoso.openlinksw.com/.
REFERENCES,0.2979757085020243,"[111] Oracle. 2018.
Oracle Spatial and Graph.
Available at https://www.oracle.com/database/
technologies/spatialandgraph.html."
REFERENCES,0.2987854251012146,"[112] Nikolaos Papailiou, Ioannis Konstantinou, Dimitrios Tsoumakos, and Nectarios Koziris. 2012. H2RDF:
adaptive query processing on RDF data in the cloud.. In Proceedings of the 21st International Conference
on World Wide Web. 397–400."
REFERENCES,0.29959514170040485,"[113] N.S. Patil, P Kiran, N.P. Kavya, and K.M. Naresh Patel. 2018. A Survey on Graph Database Management
Techniques for Huge Unstructured Data. International Journal of Electrical and Computer Engineering
81, 2 (2018), 1140–1149."
REFERENCES,0.3004048582995951,"[114] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. 2020. Learning
mesh-based simulation with graph networks. arXiv preprint arXiv:2010.03409 (2020)."
REFERENCES,0.30121457489878545,"[115] Jaroslav Pokorny. 2015. Graph databases: their power and limitations. In IFIP International Conference
on Computer Information Systems and Industrial Management. Springer, 58–69."
REFERENCES,0.3020242914979757,"[116] Irene Polikoff. 2018. Knowledge Graphs vs. Property Graphs - Part I.
Available at https://tdan.com/knowledge-graphs-vs-property-graphs-part-1/27140."
REFERENCES,0.302834008097166,"[117] Jan Portisch, Michael Hladik, and Heiko Paulheim. 2020. RDF2Vec Light–A Lightweight Approach for
Knowledge Graph Embeddings. arXiv preprint arXiv:2009.07659 (2020)."
REFERENCES,0.30364372469635625,[118] Proﬁum. 2018. Proﬁum Sense. Available at https://www.profium.com/en/.
REFERENCES,0.3044534412955466,[119] Redis Labs. 2018. RedisGraph. Available at https://oss.redislabs.com/redisgraph/.
REFERENCES,0.30526315789473685,"[120] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-
networks. arXiv preprint arXiv:1908.10084 (2019)."
REFERENCES,0.3060728744939271,"[121] Christopher D. Rickett, Utz-Uwe Haus, James Maltby, and Kristyn J. Maschhoff. 2018. Loading and
Querying a Trillion RDF triples with Cray Graph Engine on the Cray XC. In CUG. Cray Users Group."
REFERENCES,0.3068825910931174,"[122] Petar Ristoski and Heiko Paulheim. 2016. Rdf2vec: Rdf graph embeddings for data mining. In Interna-
tional Semantic Web Conference. Springer, 498–514."
REFERENCES,0.3076923076923077,Neural Graph Databases
REFERENCES,0.308502024291498,"[123] Petar Ristoski, Jessica Rosati, Tommaso Di Noia, Renato De Leone, and Heiko Paulheim. 2019. RDF2Vec:
RDF graph embeddings and their applications. Semantic Web 10, 4 (2019), 721–752."
REFERENCES,0.30931174089068825,"[124] Ryan A. Rossi, Nesreen K. Ahmed, and Eunyee Koh. 2018. Higher-Order Network Representation
Learning. In Companion Proceedings of the The Web Conference 2018 (WWW ’18). International World
Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 3–4.
https:
//doi.org/10.1145/3184558.3186900"
REFERENCES,0.3101214574898785,"[125] Ryan A. Rossi, Nesreen K. Ahmed, Eunyee Koh, Sungchul Kim, Anup Rao, and Yasin Abbasi Yadkori.
2018. HONE: Higher-Order Network Embeddings. arXiv:1801.09303 [cs, stat] (May 2018). arXiv:cs,
stat/1801.09303"
REFERENCES,0.31093117408906884,"[126] Sherif Sakr et al. 2020. The Future is Big Graphs! A Community View on Graph Processing Systems.
arXiv preprint arXiv:2012.06171 (2020)."
REFERENCES,0.3117408906882591,"[127] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia.
2020. Learning to simulate complex physics with graph networks. In ICML."
REFERENCES,0.3125506072874494,"[128] Ryoma Sato. 2020.
A survey on the expressive power of graph neural networks.
arXiv preprint
arXiv:2003.04078 (2020)."
REFERENCES,0.3133603238866397,"[129] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2008.
The graph neural network model. IEEE transactions on neural networks 20, 1 (2008), 61–80."
REFERENCES,0.31417004048582997,"[130] Baoxu Shi and Tim Weninger. 2017. Proje: Embedding projection for knowledge graph completion. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, Vol. 31."
REFERENCES,0.31497975708502024,"[131] Baoxu Shi and Tim Weninger. 2018. Open-world knowledge graph completion. In Proceedings of the
AAAI conference on artiﬁcial intelligence, Vol. 32."
REFERENCES,0.3157894736842105,"[132] Edgar Solomonik, Maciej Besta, Flavio Vella, and Torsten Hoeﬂer. 2017. Scaling betweenness centrality
using communication-efﬁcient sparse matrix multiplication. In ACM/IEEE Supercomputing."
REFERENCES,0.31659919028340083,[133] Stardog Union. 2018. Stardog. Available at https://www.stardog.com/.
REFERENCES,0.3174089068825911,"[134] Sainbayar Sukhbaatar, Rob Fergus, et al. 2016. Learning multiagent communication with backpropagation.
NeurIPS (2016)."
REFERENCES,0.31821862348178137,"[135] Zhiqing Sun, Shikhar Vashishth, Soumya Sanyal, Partha Talukdar, and Yiming Yang. 2019. A re-
evaluation of knowledge graph completion methods. arXiv preprint arXiv:1911.03903 (2019)."
REFERENCES,0.31902834008097164,"[136] Claudio Tesoriero. 2013. Getting started with OrientDB. Packt Publishing Birmingham, England."
REFERENCES,0.31983805668016196,"[137] The Apache Software Foundation. 2021. Apache Jena TBD. Available at https://jena.apache.org/
documentation/tdb/index.html."
REFERENCES,0.32064777327935223,[138] The Linux Foundation. 2018. JanusGraph. Available at http://janusgraph.org/.
REFERENCES,0.3214574898785425,"[139] Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. 2018. Attention-based graph neural
network for semi-supervised learning. arXiv preprint arXiv:1803.03735 (2018)."
REFERENCES,0.32226720647773277,"[140] Mikkel Thorup. 2000. Near-optimal fully-dynamic graph connectivity. In Proceedings of the thirty-second
annual ACM symposium on Theory of computing. 343–350."
REFERENCES,0.3230769230769231,[141] TigerGraph. 2018. TigerGraph. Available at https://www.tigergraph.com/.
REFERENCES,0.32388663967611336,"[142] TigerGraph, Inc. 2022. Using the Linked Data Benchmark Council Social Network Benchmark Methodol-
ogy to Evaluate TigerGraph at 36 Terabytes. White Paper."
REFERENCES,0.32469635627530363,"[143] Lucian Toader, Alexandru Uta, Ahmed Musaaﬁr, and Alexandru Iosup. 2019. Graphless: Toward
serverless graph processing. In 2019 18th International Symposium on Parallel and Distributed Computing
(ISPDC). IEEE, 66–73."
REFERENCES,0.3255060728744939,"[144] Alok Tripathy, Katherine Yelick, and Aydın Buluç. 2020. Reducing communication in graph neural
network training. In ACM/IEEE Supercomputing."
REFERENCES,0.3263157894736842,"[145] Théo Trouillon, Christopher R Dance, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guil-
laume Bouchard. 2017. Knowledge graph completion via complex tensor factorization. arXiv preprint
arXiv:1702.06879 (2017)."
REFERENCES,0.3271255060728745,"[146] Jacopo Urbani, Sourav Dutta, Sairam Gurajada, and Gerhard Weikum. 2016. KOGNAC: efﬁcient
encoding of large knowledge graphs. arXiv preprint arXiv:1604.04795 (2016)."
REFERENCES,0.32793522267206476,"[147] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.
2017. Graph attention networks. arXiv preprint arXiv:1710.10903 (2017)."
REFERENCES,0.3287449392712551,"[148] Roger Waleffe, Jason Mohoney, Theodoros Rekatsinas, and Shivaram Venkataraman. 2022. Marius++:
Large-Scale Training of Graph Neural Networks on a Single Machine. arXiv preprint arXiv:2202.02365
(2022)."
REFERENCES,0.32955465587044536,Neural Graph Databases
REFERENCES,0.3303643724696356,"[149] Cheng Wan, Youjie Li, Ang Li, Nam Sung Kim, and Yingyan Lin. 2022. BNS-GCN: Efﬁcient Full-Graph
Training of Graph Convolutional Networks with Partition-Parallelism and Random Boundary Node
Sampling Sampling. MLSys (2022)."
REFERENCES,0.3311740890688259,"[150] Cheng Wan, Youjie Li, Cameron R Wolfe, Anastasios Kyrillidis, Nam Sung Kim, and Yingyan Lin.
2022. PipeGCN: Efﬁcient full-graph training of graph convolutional networks with pipelined feature
communication. arXiv preprint arXiv:2203.10428 (2022)."
REFERENCES,0.3319838056680162,"[151] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan
Yu, Yu Gai, et al. 2019. Deep graph library: A graph-centric, highly-performant package for graph neural
networks. arXiv:1909.01315 (2019)."
REFERENCES,0.3327935222672065,"[152] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge graph embedding: A survey of
approaches and applications. IEEE TKDE (2017)."
REFERENCES,0.33360323886639676,"[153] Xiao Wang, Deyu Bo, Chuan Shi, Shaohua Fan, Yanfang Ye, and Philip S Yu. 2020. A survey on
heterogeneous graph embedding: methods, techniques, applications and sources. arXiv:2011.14867
(2020)."
REFERENCES,0.334412955465587,"[154] Yuke Wang, Boyuan Feng, Gushu Li, Shuangchen Li, Lei Deng, Yuan Xie, and Yufei Ding. 2020. GNNAd-
visor: An Efﬁcient Runtime System for GNN Acceleration on GPUs. arXiv preprint arXiv:2006.06608
(2020)."
REFERENCES,0.33522267206477735,"[155] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. 2019.
Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics (tog) 38, 5 (2019),
1–12."
REFERENCES,0.3360323886639676,"[156] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. 2019.
Simplifying graph convolutional networks. In International conference on machine learning. PMLR,
6861–6871."
REFERENCES,0.3368421052631579,"[157] Shiwen Wu, Fei Sun, Wentao Zhang, and Bin Cui. 2020. Graph neural networks in recommender systems:
a survey. arXiv preprint arXiv:2011.02260 (2020)."
REFERENCES,0.33765182186234816,"[158] Yidi Wu, Kaihao Ma, Zhenkun Cai, Tatiana Jin, Boyang Li, Chenguang Zheng, James Cheng, and Fan
Yu. 2021. Seastar: vertex-centric programming for graph neural networks. In EuroSys."
REFERENCES,0.3384615384615385,"[159] Zonghan Wu et al. 2020. A comprehensive survey on graph neural networks. IEEE Transactions on
Neural Networks and Learning Systems (2020)."
REFERENCES,0.33927125506072875,"[160] Yu Xie, Bin Yu, Shengze Lv, Chen Zhang, Guodong Wang, and Maoguo Gong. 2021. A survey on
heterogeneous network representation learning. Pattern Recognition 116 (2021), 107936."
REFERENCES,0.340080971659919,"[161] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826 (2018)."
REFERENCES,0.3408906882591093,"[162] Mingyu Yan, Lei Deng, Xing Hu, Ling Liang, Yujing Feng, Xiaochun Ye, Zhimin Zhang, Dongrui Fan,
and Yuan Xie. 2020. Hygcn: A gcn accelerator with hybrid architecture. In IEEE HPCA. IEEE, 15–29."
REFERENCES,0.3417004048582996,"[163] Carl Yang, Yuxin Xiao, Yu Zhang, Yizhou Sun, and Jiawei Han. 2020. Heterogeneous network represen-
tation learning: A uniﬁed framework with survey and benchmark. IEEE TKDE (2020)."
REFERENCES,0.3425101214574899,"[164] Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. KG-BERT: BERT for knowledge graph completion.
arXiv preprint arXiv:1909.03193 (2019)."
REFERENCES,0.34331983805668015,"[165] Jiaxuan You, Zhitao Ying, and Jure Leskovec. 2020. Design space for graph neural networks. Advances
in Neural Information Processing Systems 33 (2020), 17009–17021."
REFERENCES,0.3441295546558704,"[166] Pingpeng Yuan, Pu Liu, Buwen Wu, Hai Jin, Wenya Zhang, and Ling Liu. 2013. TripleBit: a fast and
compact system for large scale RDF data. Proceedings of the VLDB Endowment 6, 7 (2013), 517–528.
https://doi.org/10.14778/2536349.2536352"
REFERENCES,0.34493927125506074,"[167] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. 2019.
Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931 (2019)."
REFERENCES,0.345748987854251,"[168] Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V Chawla. 2019. Heteroge-
neous graph neural network. In KDD. 793–803."
REFERENCES,0.3465587044534413,"[169] Dalong Zhang et al. 2020. Agl: a scalable system for industrial-purpose graph machine learning. arXiv
preprint arXiv:2003.02454 (2020)."
REFERENCES,0.3473684210526316,"[170] Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2020. Deep learning on graphs: A survey. IEEE Transactions
on Knowledge and Data Engineering (2020)."
REFERENCES,0.3481781376518219,"[171] Da Zheng, Xiang Song, Chengru Yang, Dominique LaSalle, Qidong Su, Minjie Wang, Chao Ma, and
George Karypis. 2021. Distributed Hybrid CPU and GPU training for Graph Neural Networks on
Billion-Scale Graphs. arXiv:2112.15345 (2021)."
REFERENCES,0.34898785425101214,Neural Graph Databases
REFERENCES,0.3497975708502024,"[172] Jie Zhou et al. 2020. Graph neural networks: A review of methods and applications. AI Open 1 (2020),
57–81."
REFERENCES,0.35060728744939273,"[173] Rong Zhu et al. 2019. Aligraph: A comprehensive graph neural network platform. arXiv preprint
arXiv:1902.08730 (2019)."
REFERENCES,0.351417004048583,"[174] Xiaowei Zhu et al. 2020. LiveGraph: A Transactional Graph Storage System with Purely Sequential Ad-
jacency List Scans. VLDB 13, 7 (2020), 1020–1034. https://doi.org/10.14778/3384345.3384351"
REFERENCES,0.3522267206477733,"[175] Lei Zou, M. Tamer Özsu, Lei Chen, Xuchuan Shen, Ruizhe Huang, and Dongyan Zhao. 2014. GStore: A
Graph-Based SPARQL Query Engine. VLDB Journal 23, 4 (2014), 565–590. https://doi.org/10.
1007/s00778-013-0337-7"
APPENDIX,0.35303643724696354,Appendix
APPENDIX,0.35384615384615387,"A
Dataset Speciﬁcation"
APPENDIX,0.35465587044534413,We present the details about the used datasets.
APPENDIX,0.3554655870445344,"A.1
MAKG"
APPENDIX,0.3562753036437247,"The dataset MAKG (small) consists of 3′066′782 vertices and 12′314′398 edges. Each vertex
is labeled with either author (55%), paper (44%), afﬁliation (< 1%), conferenceseries (< 1%),
conferenceinstance (< 1%), ﬁeldofstudy (< 1%), or journal (< 1%). Vertices with the label paper
are further subdivided into book, bookchapter, conferencepaper, journalpaper, patentdocument or
others. Vertices labeled with afﬁliation, author, conferenceseries, conferenceinstance, ﬁeldofstudy
and journal do all have the properties rank, name, papercount, citationcount, and created. Some of
them have additional properties, e.g., homepage. All vertices with the label paper have the properties
rank, citationcount, created, title, publicationdate, referencecount, and estimatedcitationcount. Some
of them have the additional properties publisher, volume, issueidentiﬁer, startingpage, endingpage,
or doi. Edges do not have properties but each edge has a label which is either cites (40%), creator
(36%), hasdiscipline (11%), apreasinjournal (6%), memberof (5.7%), appearsinconferenceinstance
(< 1%), appearsinconferenceseries (< 1%), or ispartof (< 1%)."
APPENDIX,0.357085020242915,"A.2
Citations"
APPENDIX,0.35789473684210527,"The citations dataset contains 132′259 vertices and 221′237 edges; we use it mostly for debugging
purposes. Each vertex has one label which is either author (61%), article (39%), or venue (< 1%).
All article-vertices have the properties index (a 32-digit HEX number), title and year (the year in
which the article was published). 85% of the articles have the property abstract and 72% of them
have the property ncitations (the article’s citation count). Vertices labeled with author or venue have
only one property called name. Edges do not have properties but each edge has a label which is either
author (64%), venue (23%), or cited (13%)."
APPENDIX,0.35870445344129553,"A.3
Twitter"
APPENDIX,0.3595141700404858,"The dataset TwitterTrolls contains 281′136 vertices and 493′160 edges. Vertices are labeled with
tweet (82%), url (8%), hashtag (5%), user (5%), trolluser (< 1%), or source (< 1%). Vertices
wit labels hashtag, source, user, and url have a single property each, namely tag, name, userkey,
and expandedurl respectively. Vertices labeled with trolluser do all have the properties sourcename
and userkey. Most of them (> 80%) have additional properties lang (language), veriﬁed (true or
false), name, description, location, timezone, createdat, favoritescount, followerscount, friendscount,
listedcount, and statusescount. Most vertices labeled with tweet (> 80%) have properties createdat,
createdstr and text. About 25% of them have additional properties favoritecount, retweetcount, and
retweeted. Edge do not have properties but each edge has a label which can be posted (41%), hastag
(22%), postedvia (12%), mentions (11%), retweeted (8%), haslink (6%), or inreplyto (< 1%)."
APPENDIX,0.3603238866396761,"A.4
Differences to Traditional GNN Datasets"
APPENDIX,0.3611336032388664,"The main difference between LPG graphs and traditional GNN datasets such as Citeseer or Cora
is that the latter usually do not have extensive sets of labels. Instead, these datasets often have
vertices from different classes, which may be interpreted as a single label (that would encode such"
APPENDIX,0.36194331983805667,Neural Graph Databases
APPENDIX,0.362753036437247,"different classes). Moreover, these datasets often do not have rich sets of attached different properties.
Instead, they may come with extensive feature vectors that encode a single large additional piece of
information, for example a whole abstract. Finally, in the graph database setting, it is less common to
process graphs such as PROTEINS, where the dataset consists of a very large number of relatively
small graph. Instead, it is more common to focus on one large graph dataset."
APPENDIX,0.36356275303643726,"B
Results for Additional Labels, Properties, and Datasets"
APPENDIX,0.3643724696356275,"Figures 7–18 illustrate the impact of using each of the many available properties, and pairs of
properties, on the ﬁnal prediction accuracy. We show results separately for each GNN model and
also aggregated for all thee models, for the completeness of the analysis. To facilitate comparing
the data, we also replot the results for the GIN model for MAKG small and Neo4j Twitter analyses
from Section 4. Finally, Figure 19 shows results for an additional Neo4j dataset modeling crime
investigations."
APPENDIX,0.3651821862348178,"Interestingly, the largest accuracy increase for MAKG is consistently obtained when including
the title property. This is the case for all the considered GNN models. Similarly, when detecting
trolls, including the counts of friends or followers was crucial in consistent accuracy improvements.
This indicates that it is more important to appropriately understand the data and include the right
information in the input feature vectors, and once this is achieved, different GNN models would be
similarly able to extract this information for more accurate outcomes."
APPENDIX,0.3659919028340081,Neural Graph Databases none
APPENDIX,0.3668016194331984,issue_id
APPENDIX,0.36761133603238866,est_cite_cnt
APPENDIX,0.3684210526315789,pub_date
APPENDIX,0.36923076923076925,ref_cnt
APPENDIX,0.3700404858299595,cite_cnt
APPENDIX,0.3708502024291498,created
APPENDIX,0.37165991902834006,end_page
APPENDIX,0.3724696356275304,paper_cnt
APPENDIX,0.37327935222672065,start_page
APPENDIX,0.3740890688259109,"entity_id
rank"
APPENDIX,0.3748987854251012,namedoi
APPENDIX,0.3757085020242915,volume
APPENDIX,0.3765182186234818,"publisher
title 0 10 20 30 40 50 60 70"
APPENDIX,0.37732793522267205,Train Accuracy [%]
APPENDIX,0.3781376518218624,issue_id
APPENDIX,0.37894736842105264,"est_cite_cnt
pub_date
ref_cnt"
APPENDIX,0.3797570850202429,cite_cnt
APPENDIX,0.3805668016194332,created
APPENDIX,0.3813765182186235,end_page
APPENDIX,0.3821862348178138,paper_cnt
APPENDIX,0.38299595141700404,start_page
APPENDIX,0.3838056680161943,"entity_id
rank"
APPENDIX,0.38461538461538464,namedoi
APPENDIX,0.3854251012145749,volume
APPENDIX,0.3862348178137652,"publisher
title"
APPENDIX,0.38704453441295544,"issue_id
est_cite_cnt"
APPENDIX,0.38785425101214577,pub_date
APPENDIX,0.38866396761133604,"ref_cnt
cite_cnt"
APPENDIX,0.3894736842105263,"created
end_page
paper_cnt
start_page"
APPENDIX,0.3902834008097166,entity_id
APPENDIX,0.3910931174089069,"rank
name"
APPENDIX,0.39190283400809717,"doi
volume
publisher"
APPENDIX,0.39271255060728744,"title
58 60 62 64 66 68 70 72 name none"
APPENDIX,0.39352226720647776,cite_cnt
APPENDIX,0.39433198380566803,"created
doi"
APPENDIX,0.3951417004048583,end_page
APPENDIX,0.39595141700404857,entity_id
APPENDIX,0.3967611336032389,paper_cnt
APPENDIX,0.39757085020242916,start_page
APPENDIX,0.39838056680161943,"issue_id
rank"
APPENDIX,0.3991902834008097,publisher
APPENDIX,0.4,est_cite_cnt
APPENDIX,0.4008097165991903,pub_date
APPENDIX,0.40161943319838056,ref_cnt
APPENDIX,0.40242914979757083,"volume
title 0 10 20 30 40 50 60 70"
APPENDIX,0.40323886639676115,Test Accuracy [%] name
APPENDIX,0.4040485829959514,cite_cnt
APPENDIX,0.4048582995951417,"created
doi"
APPENDIX,0.40566801619433196,end_page
APPENDIX,0.4064777327935223,entity_id
APPENDIX,0.40728744939271255,paper_cnt
APPENDIX,0.4080971659919028,"start_page
issue_id
rank"
APPENDIX,0.4089068825910931,publisher
APPENDIX,0.4097165991902834,"est_cite_cnt
pub_date
ref_cnt"
APPENDIX,0.4105263157894737,"volume
title"
APPENDIX,0.41133603238866395,"name
cite_cnt"
APPENDIX,0.4121457489878543,created
APPENDIX,0.41295546558704455,"doi
end_page"
APPENDIX,0.4137651821862348,"entity_id
paper_cnt
start_page"
APPENDIX,0.4145748987854251,issue_id
APPENDIX,0.4153846153846154,"rank
publisher
est_cite_cnt"
APPENDIX,0.4161943319838057,pub_date
APPENDIX,0.41700404858299595,"ref_cnt
volume title 55 60 65 70 75"
APPENDIX,0.4178137651821862,"Figure 7: MAKG small (node classiﬁcation, 4 classes, results aggregated over all three models). Impact from different properties and
their combinations on the accuracy. Green: accuracy is better than that of a graph with no labels/properties; red: the accuracy is worse than
that of a graph with no labels/properties."
APPENDIX,0.41862348178137654,Neural Graph Databases none
APPENDIX,0.4194331983805668,issue_id
APPENDIX,0.4202429149797571,cite_cnt
APPENDIX,0.42105263157894735,created
APPENDIX,0.42186234817813767,end_page
APPENDIX,0.42267206477732794,paper_cnt
APPENDIX,0.4234817813765182,start_page name
APPENDIX,0.4242914979757085,entity_id rank
APPENDIX,0.4251012145748988,"publisher
doi"
APPENDIX,0.42591093117408907,est_cite_cnt
APPENDIX,0.42672064777327934,pub_date
APPENDIX,0.42753036437246966,ref_cnt
APPENDIX,0.42834008097165993,volume title 0 10 20 30 40 50 60 70
APPENDIX,0.4291497975708502,Train Accuracy [%]
APPENDIX,0.42995951417004047,issue_id
APPENDIX,0.4307692307692308,cite_cnt
APPENDIX,0.43157894736842106,created
APPENDIX,0.43238866396761133,end_page
APPENDIX,0.4331983805668016,paper_cnt
APPENDIX,0.4340080971659919,"start_page
name"
APPENDIX,0.4348178137651822,entity_id rank
APPENDIX,0.43562753036437246,"publisher
doi"
APPENDIX,0.43643724696356273,est_cite_cnt
APPENDIX,0.43724696356275305,pub_date
APPENDIX,0.4380566801619433,ref_cnt
APPENDIX,0.4388663967611336,volume title
APPENDIX,0.43967611336032386,"issue_id
cite_cnt"
APPENDIX,0.4404858299595142,"created
end_page
paper_cnt
start_page"
APPENDIX,0.44129554655870445,"name
entity_id"
APPENDIX,0.4421052631578947,"rank
publisher"
APPENDIX,0.44291497975708505,"doi
est_cite_cnt"
APPENDIX,0.4437246963562753,pub_date
APPENDIX,0.4445344129554656,"ref_cnt
volume title 58 60 62 64 66 68 70 none name"
APPENDIX,0.44534412955465585,cite_cnt
APPENDIX,0.4461538461538462,created doi
APPENDIX,0.44696356275303645,end_page
APPENDIX,0.4477732793522267,entity_id
APPENDIX,0.448582995951417,paper_cnt
APPENDIX,0.4493927125506073,start_page
APPENDIX,0.4502024291497976,publisher
APPENDIX,0.45101214574898785,issue_id rank
APPENDIX,0.4518218623481781,est_cite_cnt
APPENDIX,0.45263157894736844,pub_date
APPENDIX,0.4534412955465587,ref_cnt
APPENDIX,0.454251012145749,volume title 0 10 20 30 40 50 60 70 80
APPENDIX,0.45506072874493925,Test Accuracy [%] name
APPENDIX,0.45587044534412957,cite_cnt
APPENDIX,0.45668016194331984,"created
doi"
APPENDIX,0.4574898785425101,end_page
APPENDIX,0.4582995951417004,entity_id
APPENDIX,0.4591093117408907,paper_cnt
APPENDIX,0.45991902834008097,start_page
APPENDIX,0.46072874493927124,publisher
APPENDIX,0.46153846153846156,issue_id rank
APPENDIX,0.46234817813765183,est_cite_cnt
APPENDIX,0.4631578947368421,pub_date
APPENDIX,0.46396761133603237,ref_cnt
APPENDIX,0.4647773279352227,volume title
APPENDIX,0.46558704453441296,"name
cite_cnt"
APPENDIX,0.46639676113360323,created
APPENDIX,0.4672064777327935,"doi
end_page"
APPENDIX,0.4680161943319838,"entity_id
paper_cnt
start_page"
APPENDIX,0.4688259109311741,publisher
APPENDIX,0.46963562753036436,issue_id
APPENDIX,0.47044534412955463,"rank
est_cite_cnt"
APPENDIX,0.47125506072874496,pub_date
APPENDIX,0.4720647773279352,"ref_cnt
volume title 55.0 57.5 60.0 62.5 65.0 67.5 70.0 72.5"
APPENDIX,0.4728744939271255,"Figure 8: MAKG small (node classiﬁcation, 4 classes, GCN-only results). Impact from different properties and their combinations on the
accuracy. Green: the accuracy is better than that of a graph with no labels/properties; red: the accuracy is worse than that of a graph with no
labels/properties."
APPENDIX,0.47368421052631576,Neural Graph Databases name
APPENDIX,0.4744939271255061,est_cite_cnt
APPENDIX,0.47530364372469636,pub_date
APPENDIX,0.4761133603238866,ref_cnt
APPENDIX,0.47692307692307695,cite_cnt
APPENDIX,0.4777327935222672,created
APPENDIX,0.4785425101214575,end_page
APPENDIX,0.47935222672064776,entity_id
APPENDIX,0.4801619433198381,paper_cnt
APPENDIX,0.48097165991902835,"start_page
rank"
APPENDIX,0.4817813765182186,issue_id doi
APPENDIX,0.4825910931174089,volume none
APPENDIX,0.4834008097165992,"publisher
title 0 10 20 30 40 50 60 70"
APPENDIX,0.4842105263157895,Train Accuracy [%] name
APPENDIX,0.48502024291497975,est_cite_cnt
APPENDIX,0.48582995951417,pub_date
APPENDIX,0.48663967611336034,ref_cnt
APPENDIX,0.4874493927125506,cite_cnt
APPENDIX,0.4882591093117409,created
APPENDIX,0.48906882591093115,end_page
APPENDIX,0.4898785425101215,entity_id
APPENDIX,0.49068825910931174,paper_cnt
APPENDIX,0.491497975708502,"start_page
rank"
APPENDIX,0.49230769230769234,"issue_id
doi"
APPENDIX,0.4931174089068826,volume
APPENDIX,0.4939271255060729,"publisher
title"
APPENDIX,0.49473684210526314,"name
est_cite_cnt"
APPENDIX,0.49554655870445347,pub_date
APPENDIX,0.49635627530364373,"ref_cnt
cite_cnt"
APPENDIX,0.497165991902834,"created
end_page"
APPENDIX,0.4979757085020243,"entity_id
paper_cnt
start_page"
APPENDIX,0.4987854251012146,"rank
issue_id"
APPENDIX,0.49959514170040487,"doi
volume
publisher title 58 60 62 64 66 68 70 name"
APPENDIX,0.5004048582995951,issue_id
APPENDIX,0.5012145748987854,cite_cnt
APPENDIX,0.5020242914979757,created doi
APPENDIX,0.5028340080971659,end_page
APPENDIX,0.5036437246963563,entity_id
APPENDIX,0.5044534412955466,paper_cnt rank
APPENDIX,0.5052631578947369,"start_page
none"
APPENDIX,0.5060728744939271,publisher
APPENDIX,0.5068825910931174,est_cite_cnt
APPENDIX,0.5076923076923077,pub_date
APPENDIX,0.5085020242914979,ref_cnt
APPENDIX,0.5093117408906883,volume title 0 10 20 30 40 50 60 70
APPENDIX,0.5101214574898786,Test Accuracy [%] name
APPENDIX,0.5109311740890689,issue_id
APPENDIX,0.5117408906882591,cite_cnt
APPENDIX,0.5125506072874494,"created
doi"
APPENDIX,0.5133603238866397,end_page
APPENDIX,0.5141700404858299,entity_id
APPENDIX,0.5149797570850202,"paper_cnt
rank"
APPENDIX,0.5157894736842106,start_page
APPENDIX,0.5165991902834008,publisher
APPENDIX,0.5174089068825911,est_cite_cnt
APPENDIX,0.5182186234817814,pub_date
APPENDIX,0.5190283400809717,ref_cnt
APPENDIX,0.5198380566801619,volume title
APPENDIX,0.5206477732793522,"name
issue_id
cite_cnt"
APPENDIX,0.5214574898785425,created
APPENDIX,0.5222672064777328,"doi
end_page"
APPENDIX,0.5230769230769231,"entity_id
paper_cnt"
APPENDIX,0.5238866396761134,"rank
start_page"
APPENDIX,0.5246963562753036,"publisher
est_cite_cnt"
APPENDIX,0.5255060728744939,pub_date
APPENDIX,0.5263157894736842,"ref_cnt
volume title 35 40 45 50 55 60 65 70"
APPENDIX,0.5271255060728745,"Figure 9: MAKG small (node classiﬁcation, 4 classes, GAT-only results). Impact from different properties and their combinations on the
accuracy. Green: accuracy is better than that of a graph with no labels/properties; red: the accuracy is worse than that of a graph with no
labels/properties."
APPENDIX,0.5279352226720648,Neural Graph Databases
APPENDIX,0.5287449392712551,est_cite_cnt
APPENDIX,0.5295546558704454,pub_date
APPENDIX,0.5303643724696356,ref_cnt none
APPENDIX,0.5311740890688259,volume rank
APPENDIX,0.5319838056680162,entity_id
APPENDIX,0.5327935222672064,cite_cnt
APPENDIX,0.5336032388663967,created
APPENDIX,0.5344129554655871,end_page
APPENDIX,0.5352226720647774,paper_cnt
APPENDIX,0.5360323886639676,start_page
APPENDIX,0.5368421052631579,issue_id doi name
APPENDIX,0.5376518218623482,"publisher
title 0 10 20 30 40 50 60 70"
APPENDIX,0.5384615384615384,Train Accuracy [%]
APPENDIX,0.5392712550607287,est_cite_cnt
APPENDIX,0.540080971659919,pub_date
APPENDIX,0.5408906882591094,ref_cnt
APPENDIX,0.5417004048582996,volume rank
APPENDIX,0.5425101214574899,entity_id
APPENDIX,0.5433198380566802,cite_cnt
APPENDIX,0.5441295546558704,created
APPENDIX,0.5449392712550607,end_page
APPENDIX,0.545748987854251,paper_cnt
APPENDIX,0.5465587044534413,start_page
APPENDIX,0.5473684210526316,"issue_id
doi name"
APPENDIX,0.5481781376518219,"publisher
title"
APPENDIX,0.5489878542510122,est_cite_cnt
APPENDIX,0.5497975708502024,pub_date
APPENDIX,0.5506072874493927,"ref_cnt
volume"
APPENDIX,0.551417004048583,"rank
entity_id"
APPENDIX,0.5522267206477732,cite_cnt
APPENDIX,0.5530364372469636,"created
end_page
paper_cnt
start_page"
APPENDIX,0.5538461538461539,issue_id
APPENDIX,0.5546558704453441,"doi
name
publisher title 58 60 62 64 66 68 70 none"
APPENDIX,0.5554655870445344,est_cite_cnt
APPENDIX,0.5562753036437247,pub_date
APPENDIX,0.557085020242915,ref_cnt
APPENDIX,0.5578947368421052,volume
APPENDIX,0.5587044534412956,cite_cnt
APPENDIX,0.5595141700404859,created doi
APPENDIX,0.5603238866396761,end_page
APPENDIX,0.5611336032388664,entity_id
APPENDIX,0.5619433198380567,paper_cnt
APPENDIX,0.562753036437247,start_page
APPENDIX,0.5635627530364372,issue_id rank
APPENDIX,0.5643724696356275,publisher name title 0 10 20 30 40 50 60 70
APPENDIX,0.5651821862348179,Test Accuracy [%]
APPENDIX,0.5659919028340081,est_cite_cnt
APPENDIX,0.5668016194331984,pub_date
APPENDIX,0.5676113360323887,ref_cnt
APPENDIX,0.5684210526315789,volume
APPENDIX,0.5692307692307692,cite_cnt
APPENDIX,0.5700404858299595,"created
doi"
APPENDIX,0.5708502024291497,end_page
APPENDIX,0.5716599190283401,entity_id
APPENDIX,0.5724696356275304,paper_cnt
APPENDIX,0.5732793522267207,start_page
APPENDIX,0.5740890688259109,issue_id rank
APPENDIX,0.5748987854251012,publisher name title
APPENDIX,0.5757085020242915,est_cite_cnt
APPENDIX,0.5765182186234817,pub_date
APPENDIX,0.5773279352226721,"ref_cnt
volume
cite_cnt"
APPENDIX,0.5781376518218624,created
APPENDIX,0.5789473684210527,"doi
end_page"
APPENDIX,0.5797570850202429,"entity_id
paper_cnt
start_page"
APPENDIX,0.5805668016194332,issue_id
APPENDIX,0.5813765182186235,"rank
publisher name"
APPENDIX,0.5821862348178137,"title
58 60 62 64 66 68 70"
APPENDIX,0.582995951417004,"Figure 10: MAKG small (node classiﬁcation, 4 classes, GIN-only results). Impact from different properties and their combinations on the
accuracy. Green: accuracy is better than that of a graph with no labels/properties; red: the accuracy is worse than that of a graph with no
labels/properties."
APPENDIX,0.5838056680161944,Neural Graph Databases none
APPENDIX,0.5846153846153846,exp_url tag
APPENDIX,0.5854251012145749,"retweeted
verif"
APPENDIX,0.5862348178137652,userkey
APPENDIX,0.5870445344129555,timezone name
APPENDIX,0.5878542510121457,"scr_name
loc"
APPENDIX,0.588663967611336,friend_cnt
APPENDIX,0.5894736842105263,favs_cnt
APPENDIX,0.5902834008097166,"follower_cnt
stat_cnt"
APPENDIX,0.5910931174089069,"listed_cnt
lang desc text"
APPENDIX,0.5919028340080972,fav_cnt 0 5 10 15 20 25
APPENDIX,0.5927125506072874,Train MAE
APPENDIX,0.5935222672064777,"exp_url
tag"
APPENDIX,0.594331983805668,"retweeted
verif"
APPENDIX,0.5951417004048583,userkey
APPENDIX,0.5959514170040486,"timezone
name"
APPENDIX,0.5967611336032389,"scr_name
loc"
APPENDIX,0.5975708502024292,friend_cnt
APPENDIX,0.5983805668016194,favs_cnt
APPENDIX,0.5991902834008097,"follower_cnt
stat_cnt"
APPENDIX,0.6,"listed_cnt
lang desc text"
APPENDIX,0.6008097165991902,fav_cnt
APPENDIX,0.6016194331983805,exp_url
APPENDIX,0.6024291497975709,"tag
retweeted"
APPENDIX,0.6032388663967612,"verif
userkey
timezone"
APPENDIX,0.6040485829959514,"name
scr_name"
APPENDIX,0.6048582995951417,"loc
friend_cnt"
APPENDIX,0.605668016194332,"favs_cnt
follower_cnt"
APPENDIX,0.6064777327935222,"stat_cnt
listed_cnt"
APPENDIX,0.6072874493927125,"lang
desc"
APPENDIX,0.6080971659919029,"text
fav_cnt 8 10 12 14 16 18 20 desc text none"
APPENDIX,0.6089068825910932,timezone name loc lang
APPENDIX,0.6097165991902834,userkey
APPENDIX,0.6105263157894737,scr_name
APPENDIX,0.611336032388664,exp_url tag
APPENDIX,0.6121457489878542,fav_cnt verif
APPENDIX,0.6129554655870445,retweeted
APPENDIX,0.6137651821862348,favs_cnt
APPENDIX,0.6145748987854251,stat_cnt
APPENDIX,0.6153846153846154,listed_cnt
APPENDIX,0.6161943319838057,friend_cnt
APPENDIX,0.617004048582996,follower_cnt 0 10 20 30 40
APPENDIX,0.6178137651821862,Test MAE desc text
APPENDIX,0.6186234817813765,"timezone
name loc lang"
APPENDIX,0.6194331983805668,userkey
APPENDIX,0.620242914979757,scr_name
APPENDIX,0.6210526315789474,"exp_url
tag"
APPENDIX,0.6218623481781377,fav_cnt verif
APPENDIX,0.622672064777328,retweeted
APPENDIX,0.6234817813765182,favs_cnt
APPENDIX,0.6242914979757085,stat_cnt
APPENDIX,0.6251012145748988,listed_cnt
APPENDIX,0.625910931174089,friend_cnt
APPENDIX,0.6267206477732794,follower_cnt desc
APPENDIX,0.6275303643724697,"text
timezone name"
APPENDIX,0.6283400809716599,"loc
lang
userkey
scr_name"
APPENDIX,0.6291497975708502,exp_url
APPENDIX,0.6299595141700405,"tag
fav_cnt"
APPENDIX,0.6307692307692307,"verif
retweeted"
APPENDIX,0.631578947368421,favs_cnt
APPENDIX,0.6323886639676113,"stat_cnt
listed_cnt
friend_cnt
follower_cnt 10 15 20 25"
APPENDIX,0.6331983805668017,"Figure 11: Neo4j Twitter trolls (node regression, results aggregated over all three models). Impact from different properties and their
combinations on the MAE. Green: MAE is better than that of a graph with no labels/properties; red: the MAE is worse than that of a graph
with no labels/properties."
APPENDIX,0.6340080971659919,Neural Graph Databases none lang
APPENDIX,0.6348178137651822,timezone
APPENDIX,0.6356275303643725,exp_url
APPENDIX,0.6364372469635627,friend_cnt
APPENDIX,0.637246963562753,"listed_cnt
tag name"
APPENDIX,0.6380566801619433,retweeted
APPENDIX,0.6388663967611335,scr_name
APPENDIX,0.6396761133603239,userkey verif loc
APPENDIX,0.6404858299595142,stat_cnt
APPENDIX,0.6412955465587045,"follower_cnt
favs_cnt desc text"
APPENDIX,0.6421052631578947,fav_cnt 0 5 10 15 20
APPENDIX,0.642914979757085,Train MAE lang
APPENDIX,0.6437246963562753,timezone
APPENDIX,0.6445344129554655,exp_url
APPENDIX,0.6453441295546559,friend_cnt
APPENDIX,0.6461538461538462,"listed_cnt
tag name"
APPENDIX,0.6469635627530365,retweeted
APPENDIX,0.6477732793522267,scr_name
APPENDIX,0.648582995951417,"userkey
verif loc"
APPENDIX,0.6493927125506073,stat_cnt
APPENDIX,0.6502024291497975,"follower_cnt
favs_cnt
desc text"
APPENDIX,0.6510121457489878,fav_cnt
APPENDIX,0.6518218623481782,"lang
timezone"
APPENDIX,0.6526315789473685,"exp_url
friend_cnt"
APPENDIX,0.6534412955465587,listed_cnt
APPENDIX,0.654251012145749,"tag
name
retweeted"
APPENDIX,0.6550607287449393,scr_name
APPENDIX,0.6558704453441295,userkey verif
APPENDIX,0.6566801619433198,"loc
stat_cnt
follower_cnt"
APPENDIX,0.6574898785425102,favs_cnt desc
APPENDIX,0.6582995951417004,"text
fav_cnt
5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5 none desc loc lang"
APPENDIX,0.6591093117408907,timezone
APPENDIX,0.659919028340081,"exp_url
tag verif"
APPENDIX,0.6607287449392713,"retweeted
name"
APPENDIX,0.6615384615384615,fav_cnt
APPENDIX,0.6623481781376518,scr_name
APPENDIX,0.6631578947368421,userkey
APPENDIX,0.6639676113360324,"favs_cnt
text"
APPENDIX,0.6647773279352227,stat_cnt
APPENDIX,0.665587044534413,friend_cnt
APPENDIX,0.6663967611336032,listed_cnt
APPENDIX,0.6672064777327935,follower_cnt 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
APPENDIX,0.6680161943319838,Test MAE desc loc lang
APPENDIX,0.668825910931174,timezone
APPENDIX,0.6696356275303643,"exp_url
tag verif"
APPENDIX,0.6704453441295547,"retweeted
name"
APPENDIX,0.671255060728745,fav_cnt
APPENDIX,0.6720647773279352,scr_name
APPENDIX,0.6728744939271255,userkey
APPENDIX,0.6736842105263158,"favs_cnt
text"
APPENDIX,0.674493927125506,stat_cnt
APPENDIX,0.6753036437246963,friend_cnt
APPENDIX,0.6761133603238867,listed_cnt
APPENDIX,0.676923076923077,follower_cnt desc
APPENDIX,0.6777327935222672,"loc
lang
timezone"
APPENDIX,0.6785425101214575,exp_url
APPENDIX,0.6793522267206478,"tag
verif
retweeted"
APPENDIX,0.680161943319838,"name
fav_cnt
scr_name"
APPENDIX,0.6809716599190283,"userkey
favs_cnt"
APPENDIX,0.6817813765182186,"text
stat_cnt
friend_cnt"
APPENDIX,0.682591093117409,"listed_cnt
follower_cnt 5 10 15 20 25"
APPENDIX,0.6834008097165992,"Figure 12: Neo4j Twitter trolls (node regression, GCN-only results). Impact from different properties and their combinations on the MAE.
Green: MAE is better than that of a graph with no labels/properties; red: the MAE is worse than that of a graph with no labels/properties."
APPENDIX,0.6842105263157895,Neural Graph Databases tag verif
APPENDIX,0.6850202429149798,exp_url
APPENDIX,0.68582995951417,"retweeted
name"
APPENDIX,0.6866396761133603,timezone
APPENDIX,0.6874493927125506,stat_cnt
APPENDIX,0.6882591093117408,"friend_cnt
loc"
APPENDIX,0.6890688259109312,userkey
APPENDIX,0.6898785425101215,"follower_cnt
none"
APPENDIX,0.6906882591093118,favs_cnt
APPENDIX,0.691497975708502,listed_cnt
APPENDIX,0.6923076923076923,"scr_name
lang desc text"
APPENDIX,0.6931174089068826,fav_cnt 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
APPENDIX,0.6939271255060728,Train MAE tag verif
APPENDIX,0.6947368421052632,exp_url
APPENDIX,0.6955465587044535,"retweeted
name"
APPENDIX,0.6963562753036437,timezone
APPENDIX,0.697165991902834,stat_cnt
APPENDIX,0.6979757085020243,"friend_cnt
loc"
APPENDIX,0.6987854251012146,userkey
APPENDIX,0.6995951417004048,"follower_cnt
favs_cnt"
APPENDIX,0.7004048582995951,listed_cnt
APPENDIX,0.7012145748987855,"scr_name
lang desc text"
APPENDIX,0.7020242914979757,fav_cnt
APPENDIX,0.702834008097166,"tag
verif
exp_url
retweeted"
APPENDIX,0.7036437246963563,"name
timezone"
APPENDIX,0.7044534412955465,"stat_cnt
friend_cnt"
APPENDIX,0.7052631578947368,"loc
userkey
follower_cnt"
APPENDIX,0.7060728744939271,"favs_cnt
listed_cnt
scr_name"
APPENDIX,0.7068825910931175,"lang
desc"
APPENDIX,0.7076923076923077,"text
fav_cnt
10 12 14 16 18 desc text"
APPENDIX,0.708502024291498,fav_cnt none loc name
APPENDIX,0.7093117408906883,"timezone
lang"
APPENDIX,0.7101214574898785,userkey
APPENDIX,0.7109311740890688,scr_name
APPENDIX,0.7117408906882591,stat_cnt
APPENDIX,0.7125506072874493,favs_cnt
APPENDIX,0.7133603238866397,"exp_url
tag verif"
APPENDIX,0.71417004048583,retweeted
APPENDIX,0.7149797570850203,listed_cnt
APPENDIX,0.7157894736842105,follower_cnt
APPENDIX,0.7165991902834008,friend_cnt 0 10 20 30 40 50 60
APPENDIX,0.7174089068825911,Test MAE desc text
APPENDIX,0.7182186234817813,"fav_cnt
loc name"
APPENDIX,0.7190283400809716,"timezone
lang"
APPENDIX,0.719838056680162,userkey
APPENDIX,0.7206477732793523,scr_name
APPENDIX,0.7214574898785425,stat_cnt
APPENDIX,0.7222672064777328,favs_cnt
APPENDIX,0.7230769230769231,"exp_url
tag verif"
APPENDIX,0.7238866396761133,retweeted
APPENDIX,0.7246963562753036,listed_cnt
APPENDIX,0.725506072874494,follower_cnt
APPENDIX,0.7263157894736842,friend_cnt desc
APPENDIX,0.7271255060728745,"text
fav_cnt"
APPENDIX,0.7279352226720648,"loc
name
timezone"
APPENDIX,0.728744939271255,"lang
userkey
scr_name"
APPENDIX,0.7295546558704453,"stat_cnt
favs_cnt"
APPENDIX,0.7303643724696356,exp_url
APPENDIX,0.7311740890688259,"tag
verif
retweeted"
APPENDIX,0.7319838056680162,"listed_cnt
follower_cnt"
APPENDIX,0.7327935222672065,friend_cnt 10 15 20 25 30 35 40 45
APPENDIX,0.7336032388663968,"Figure 13: Neo4j Twitter trolls (node regression, GAT-only results). Impact from different properties and their combinations on the MAE.
Green: MAE is better than that of a graph with no labels/properties; red: the MAE is worse than that of a graph with no labels/properties."
APPENDIX,0.734412955465587,Neural Graph Databases none
APPENDIX,0.7352226720647773,userkey
APPENDIX,0.7360323886639676,"scr_name
verif tag"
APPENDIX,0.7368421052631579,retweeted
APPENDIX,0.7376518218623481,exp_url
APPENDIX,0.7384615384615385,"timezone
loc name"
APPENDIX,0.7392712550607288,favs_cnt
APPENDIX,0.740080971659919,friend_cnt
APPENDIX,0.7408906882591093,"follower_cnt
lang"
APPENDIX,0.7417004048582996,listed_cnt
APPENDIX,0.7425101214574898,stat_cnt desc
APPENDIX,0.7433198380566801,fav_cnt text 0 5 10 15 20 25 30
APPENDIX,0.7441295546558705,Train MAE
APPENDIX,0.7449392712550608,userkey
APPENDIX,0.745748987854251,"scr_name
verif tag"
APPENDIX,0.7465587044534413,retweeted
APPENDIX,0.7473684210526316,exp_url
APPENDIX,0.7481781376518218,"timezone
loc name"
APPENDIX,0.7489878542510121,favs_cnt
APPENDIX,0.7497975708502024,friend_cnt
APPENDIX,0.7506072874493928,"follower_cnt
lang"
APPENDIX,0.751417004048583,listed_cnt
APPENDIX,0.7522267206477733,stat_cnt desc
APPENDIX,0.7530364372469636,"fav_cnt
text"
APPENDIX,0.7538461538461538,"userkey
scr_name verif"
APPENDIX,0.7546558704453441,"tag
retweeted"
APPENDIX,0.7554655870445344,"exp_url
timezone"
APPENDIX,0.7562753036437248,"loc
name
favs_cnt
friend_cnt
follower_cnt"
APPENDIX,0.757085020242915,"lang
listed_cnt"
APPENDIX,0.7578947368421053,stat_cnt
APPENDIX,0.7587044534412956,"desc
fav_cnt text 10 12 14 16 18 20 22 desc text none"
APPENDIX,0.7595141700404858,timezone name lang
APPENDIX,0.7603238866396761,userkey
APPENDIX,0.7611336032388664,"scr_name
verif tag"
APPENDIX,0.7619433198380566,retweeted
APPENDIX,0.762753036437247,"exp_url
loc"
APPENDIX,0.7635627530364373,favs_cnt
APPENDIX,0.7643724696356275,stat_cnt
APPENDIX,0.7651821862348178,fav_cnt
APPENDIX,0.7659919028340081,friend_cnt
APPENDIX,0.7668016194331984,follower_cnt
APPENDIX,0.7676113360323886,listed_cnt 0 5 10 15 20 25
APPENDIX,0.7684210526315789,Test MAE desc text
APPENDIX,0.7692307692307693,"timezone
name lang"
APPENDIX,0.7700404858299595,userkey
APPENDIX,0.7708502024291498,"scr_name
verif tag"
APPENDIX,0.7716599190283401,retweeted
APPENDIX,0.7724696356275303,"exp_url
loc"
APPENDIX,0.7732793522267206,favs_cnt
APPENDIX,0.7740890688259109,stat_cnt
APPENDIX,0.7748987854251013,fav_cnt
APPENDIX,0.7757085020242915,friend_cnt
APPENDIX,0.7765182186234818,follower_cnt
APPENDIX,0.7773279352226721,listed_cnt desc
APPENDIX,0.7781376518218623,"text
timezone name"
APPENDIX,0.7789473684210526,"lang
userkey
scr_name verif"
APPENDIX,0.7797570850202429,"tag
retweeted"
APPENDIX,0.7805668016194331,exp_url
APPENDIX,0.7813765182186235,"loc
favs_cnt"
APPENDIX,0.7821862348178138,stat_cnt
APPENDIX,0.7829959514170041,"fav_cnt
friend_cnt
follower_cnt"
APPENDIX,0.7838056680161943,listed_cnt 5 10 15 20 25 30 35 40 45
APPENDIX,0.7846153846153846,"Figure 14: Neo4j Twitter trolls (node regression, GIN-only results). Impact from different properties and their combinations on the MAE.
Green: MAE is better than that of a graph with no labels/properties; red: the MAE is worse than that of a graph with no labels/properties."
APPENDIX,0.7854251012145749,Neural Graph Databases
APPENDIX,0.7862348178137651,abstract title index name
APPENDIX,0.7870445344129555,labels year none 0 10 20 30 40 50 60
APPENDIX,0.7878542510121458,Train MAE
APPENDIX,0.7886639676113361,abstract title index name
APPENDIX,0.7894736842105263,labels year
APPENDIX,0.7902834008097166,abstract title index name
APPENDIX,0.7910931174089069,labels
APPENDIX,0.7919028340080971,"year
43 44 45 46 47 48 none"
APPENDIX,0.7927125506072874,labels name index
APPENDIX,0.7935222672064778,abstract title year 0 20 40 60 80 100 120
APPENDIX,0.794331983805668,Test MAE
APPENDIX,0.7951417004048583,labels name index
APPENDIX,0.7959514170040486,abstract title year
APPENDIX,0.7967611336032389,labels name index
APPENDIX,0.7975708502024291,abstract title
APPENDIX,0.7983805668016194,"year
50 60 70 80 90 100"
APPENDIX,0.7991902834008097,"Figure 15: Neo4j citations (node regression, results aggregated over all three models). Impact from different properties and their combi-
nations on the MAE. Green: MAE is better than that of a graph with no labels/properties; red: the MAE is worse than that of a graph with no
labels/properties."
APPENDIX,0.8,Neural Graph Databases
APPENDIX,0.8008097165991903,abstract title index name
APPENDIX,0.8016194331983806,labels year 0 10 20 30 40 50
APPENDIX,0.8024291497975709,Train MAE
APPENDIX,0.8032388663967611,abstract title index name
APPENDIX,0.8040485829959514,labels year
APPENDIX,0.8048582995951417,abstract title index name
APPENDIX,0.805668016194332,labels year 42 44 46 48 50 name
APPENDIX,0.8064777327935223,labels index year
APPENDIX,0.8072874493927126,abstract title 0 20 40 60 80 100 120
APPENDIX,0.8080971659919028,Test MAE name
APPENDIX,0.8089068825910931,labels index year
APPENDIX,0.8097165991902834,abstract title name
APPENDIX,0.8105263157894737,labels index year
APPENDIX,0.8113360323886639,abstract
APPENDIX,0.8121457489878543,"title
40 50 60 70 80 90 100 110"
APPENDIX,0.8129554655870446,"Figure 16: Neo4j citations (node regression, GCN-only results). Impact from different properties and their combinations on the MAE. Here,
we do not use green/red colors, because the baselines with no labels/properties could not converge. Instead, we use only one-color (blue)
shades to indicate relative improvements."
APPENDIX,0.8137651821862348,Neural Graph Databases
APPENDIX,0.8145748987854251,abstract title
APPENDIX,0.8153846153846154,labels year index name 0 10 20 30 40 50 60 70
APPENDIX,0.8161943319838056,Train MAE
APPENDIX,0.8170040485829959,abstract title
APPENDIX,0.8178137651821862,labels year index name
APPENDIX,0.8186234817813766,abstract title
APPENDIX,0.8194331983805668,labels year index name 42 44 46 48 50 52 54 56 year
APPENDIX,0.8202429149797571,labels index name title
APPENDIX,0.8210526315789474,abstract 0 10 20 30 40 50
APPENDIX,0.8218623481781376,Test MAE year
APPENDIX,0.8226720647773279,labels index name title
APPENDIX,0.8234817813765182,abstract year
APPENDIX,0.8242914979757086,labels index name title
APPENDIX,0.8251012145748988,abstract 35 40 45 50 55 60 65
APPENDIX,0.8259109311740891,"Figure 17: Neo4j citations (node regression, GAT-only results). Impact from different properties and their combinations on the MAE. Here,
we do not use green/red colors, because the baselines with no labels/properties could not converge. Instead, we use only one-color (blue)
shades to indicate relative improvements. name index none"
APPENDIX,0.8267206477732794,abstract title year
APPENDIX,0.8275303643724696,labels 0 10 20 30 40 50
APPENDIX,0.8283400809716599,Train MAE name index
APPENDIX,0.8291497975708502,abstract title year
APPENDIX,0.8299595141700404,labels name index
APPENDIX,0.8307692307692308,abstract title year
APPENDIX,0.8315789473684211,"labels
42 44 46 48 50 52 none title"
APPENDIX,0.8323886639676114,abstract
APPENDIX,0.8331983805668016,labels index name year 0 20 40 60 80 100 120 140 160
APPENDIX,0.8340080971659919,Test MAE title
APPENDIX,0.8348178137651822,abstract
APPENDIX,0.8356275303643724,labels index name year title
APPENDIX,0.8364372469635628,abstract
APPENDIX,0.8372469635627531,labels index name year 40 50 60 70 80 90 100
APPENDIX,0.8380566801619433,"Figure 18: Neo4j citations (node regression, GIN-only results). Impact from different properties and their combinations on the MAE. Green:
MAE is better than that of a graph with no labels/properties; red: the MAE is worse than that of a graph with no labels/properties."
APPENDIX,0.8388663967611336,Neural Graph Databases
APPENDIX,0.8396761133603239,"GCN
GAT
GIN
0 20 40 60"
APPENDIX,0.8404858299595142,Test Accuracy [%]
APPENDIX,0.8412955465587044,Neo4j (crime investigation)
APPENDIX,0.8421052631578947,"none
labels"
APPENDIX,0.8429149797570851,"address
postcode"
APPENDIX,0.8437246963562753,"date
code"
APPENDIX,0.8445344129554656,"latitude
longitude"
APPENDIX,0.8453441295546559,"lastoutcome
labels,lastoutcome"
APPENDIX,0.8461538461538461,"Figure 19: Advantages of preserving the information encoded in LPG labels and properties, for node classiﬁcation in the Neo4j crime investi-
gation dataset."
APPENDIX,0.8469635627530364,Neural Graph Databases
APPENDIX,0.8477732793522267,"C
Details of Embedding Construction"
APPENDIX,0.848582995951417,"We provide formal speciﬁcations of the computed LPG2vec encodings for any vertex i (xi) and
for any edge (i, j) (xij). The speciﬁc ﬁelds are as follows: one-hot encoding of the x-th label
(lx) where x ∈{1, ..., L}, one-hot encoding of the y-th property that has Cy potential values
(py,1, py,2, ..., py,Cy) where y ∈{1, ..., P}, and a string encoding (e.g., BERT) of the z-th text
feature that has Tz potential ﬁelds (fz,1, fz,2, ..., fz,Tz) where z ∈{1, ..., F}. This formal description
assumes that all the properties are appropriately discretized and - if needed - normalized. The
encoding for edges is fully analogous (for simplicity, we assume that the set of labels and properties
L ∪P is common for vertices and edges). xi = "
APPENDIX,0.8493927125506073,"





























































"
APPENDIX,0.8502024291497976,"l1
l2
...
lL
p1,1
p1,2
...
p1,C1
p2,1
p2,2
...
p2,C2
...
pP,1
pP,2
...
pP,CP
f1,1
f1,2
...
f1,T1
f2,1
f2,2
...
f2,T2
...
fF,1
fF,2
...
fF,TF "
APPENDIX,0.8510121457489879,"





























































"
APPENDIX,0.8518218623481781,Neural Graph Databases
APPENDIX,0.8526315789473684,"ei,j = "
APPENDIX,0.8534412955465587,"





























































"
APPENDIX,0.854251012145749,"l1
l2
...
lL
p1,1
p1,2
...
p1,C1
p2,1
p2,2
...
p2,C2
...
pP,1
pP,2
...
pP,CP
f1,1
f1,2
...
f1,F1
f2,1
f2,2
...
f2,F2
...
fF,1
fF,2
...
fF,CF "
APPENDIX,0.8550607287449393,"





























































"
APPENDIX,0.8558704453441296,Neural Graph Databases
APPENDIX,0.8566801619433199,"D
Results for Additional Hyperparameters and Models"
APPENDIX,0.8574898785425101,"We also investigate different training split ratios as well as the counts of convolution layers, see
Figures 20 and 21. Adding node features generally improves the accuracy across different GNN
models and splits. Differences in the training split ratio for the MAKG dataset have little effect on the
accuracy. However, in the citations dataset, the accuracy gets worse when it uses more training data.
It indicates that, in this dataset and task, the initial 80% split ratio for the training nodes is too high."
APPENDIX,0.8582995951417004,"10%
20%
40%
80% 55 60 65 70 75 80"
APPENDIX,0.8591093117408907,Test Accuracy [%]
APPENDIX,0.8599190283400809,"none
labels
title
labels,title
GCN"
APPENDIX,0.8607287449392712,"10%
20%
40%
80% 55 60 65 70 75 80"
APPENDIX,0.8615384615384616,Test Accuracy [%]
APPENDIX,0.8623481781376519,"none
labels
title
labels,title
GAT"
APPENDIX,0.8631578947368421,"10%
20%
40%
80%
Training split ratio 55 60 65 70 75 80"
APPENDIX,0.8639676113360324,Test Accuracy [%]
APPENDIX,0.8647773279352227,"none
labels
title
labels,title
GIN"
APPENDIX,0.8655870445344129,"Figure 20: MAKG small (node classiﬁcation, 4 classes). Impact from different split ratios (the higher the better)."
APPENDIX,0.8663967611336032,Neural Graph Databases
APPENDIX,0.8672064777327935,"10%
20%
40%
80% 40 50 60 70 80 90 100"
APPENDIX,0.8680161943319838,Val MAE
APPENDIX,0.8688259109311741,"none
labels
year
labels,year
GCN"
APPENDIX,0.8696356275303644,"10%
20%
40%
80% 30 40 50 60 70 80 90"
APPENDIX,0.8704453441295547,Val MAE
APPENDIX,0.8712550607287449,"none
labels
year
labels,year
GAT"
APPENDIX,0.8720647773279352,"10%
20%
40%
80%
Training split ratio 40 50 60 70 80 90"
APPENDIX,0.8728744939271255,Val MAE
APPENDIX,0.8736842105263158,"none
labels
year
labels,year
GIN"
APPENDIX,0.8744939271255061,Figure 21: Neo4j citations (node regression). Impact from different split ratios (the lower the better).
APPENDIX,0.8753036437246964,"We also vary the number convolution layers, see Figure 22. Adding more layers on its own does not
bring consistent improvements. This is because the structure of the considered graph datasets usually
has a lot of locality and is highly clustered. However, importantly, adding the information from labels
and from properties enhances the accuracy consistency across all tried layer counts."
APPENDIX,0.8761133603238866,Neural Graph Databases
APPENDIX,0.8769230769230769,"layers=2
layers=3
layers=4 55 60 65 70 75"
APPENDIX,0.8777327935222672,Test Accuracy [%]
APPENDIX,0.8785425101214575,"none
labels
title
labels,title
GCN"
APPENDIX,0.8793522267206477,"layers=2
layers=3
layers=4 55 60 65 70 75 80 85"
APPENDIX,0.8801619433198381,Test Accuracy [%]
APPENDIX,0.8809716599190284,"none
labels
title
labels,title
GAT"
APPENDIX,0.8817813765182186,"layers=2
layers=3
layers=4 55 60 65 70 75"
APPENDIX,0.8825910931174089,Test Accuracy [%]
APPENDIX,0.8834008097165992,"none
labels
title
labels,title
GIN"
APPENDIX,0.8842105263157894,"layers=2
layers=3
layers=4
Number of message-passing layers 55 60 65 70 75"
APPENDIX,0.8850202429149797,Test Accuracy [%]
APPENDIX,0.8858299595141701,"none
labels
title
labels,title
SAGEConv"
APPENDIX,0.8866396761133604,"Figure 22: MAKG small (node classiﬁcation, 4 classes). Impact from different counts of convolution layers (the higher the better)."
APPENDIX,0.8874493927125506,"We also investigated different hyperparameters for LPG2vec embeddings. For example, we experi-
mented with the dimensions of the constructed embeddings. For this, we tried to use an additional
MLP to reduce the dimensions of the high dimensional LPG2vec feature vectors, while keeping the
information within the features intact. We use two linear layers combined with a dropout layer and
the Leaky Relu activation. The dimensions were reduced by different rations, between 20 and 5×.
This approach on one hand resulted in much smaller input feature vectors, which could visibly reduce
the memory storage overheads for particularly large graphs. However, we also observed consistent
accuracy losses across all tried datasets and GNN models. We left more extensive experiments into
this direction for future work."
APPENDIX,0.8882591093117409,Neural Graph Databases
APPENDIX,0.8890688259109312,"Finally, we also investigate additional models, GraphSAGE (Figure 23) and plain MLP (Figure 24).
As with GCN, GIN, and GAT, adding more labels and more properties enhances the accuracy. MLP
comes with much lower accuracy than GraphSAGE for most tried settings (i.e., with most of labels
and properties tried). However, interestingly, it becomes only slightly less powerful than GraphSAGE
when including the title property. This further shows the importance of harnessing LPG data - when
the right data is included into the initial embeddings, it may offer very high accuracy even without
considering the graph structure. name none doi"
APPENDIX,0.8898785425101214,entity_id rank
APPENDIX,0.8906882591093117,cite_cnt
APPENDIX,0.891497975708502,created
APPENDIX,0.8923076923076924,end_page
APPENDIX,0.8931174089068826,est_cite_cnt
APPENDIX,0.8939271255060729,issue_id
APPENDIX,0.8947368421052632,paper_cnt
APPENDIX,0.8955465587044534,pub_date
APPENDIX,0.8963562753036437,ref_cnt
APPENDIX,0.897165991902834,start_page
APPENDIX,0.8979757085020242,volume
APPENDIX,0.8987854251012146,"publisher
title 0 10 20 30 40 50 60 70"
APPENDIX,0.8995951417004049,Train Accuracy [%] name doi
APPENDIX,0.9004048582995952,entity_id rank
APPENDIX,0.9012145748987854,cite_cnt
APPENDIX,0.9020242914979757,created
APPENDIX,0.902834008097166,end_page
APPENDIX,0.9036437246963562,est_cite_cnt
APPENDIX,0.9044534412955466,issue_id
APPENDIX,0.9052631578947369,paper_cnt
APPENDIX,0.9060728744939271,pub_date
APPENDIX,0.9068825910931174,ref_cnt
APPENDIX,0.9076923076923077,start_page
APPENDIX,0.908502024291498,volume
APPENDIX,0.9093117408906882,"publisher
title name"
APPENDIX,0.9101214574898785,"doi
entity_id"
APPENDIX,0.9109311740890689,"rank
cite_cnt"
APPENDIX,0.9117408906882591,"created
end_page
est_cite_cnt"
APPENDIX,0.9125506072874494,"issue_id
paper_cnt"
APPENDIX,0.9133603238866397,pub_date
APPENDIX,0.91417004048583,"ref_cnt
start_page"
APPENDIX,0.9149797570850202,"volume
publisher title 58 60 62 64 66 68 70 72 rank none"
APPENDIX,0.9157894736842105,cite_cnt
APPENDIX,0.9165991902834008,created doi
APPENDIX,0.9174089068825911,end_page
APPENDIX,0.9182186234817814,entity_id
APPENDIX,0.9190283400809717,est_cite_cnt
APPENDIX,0.9198380566801619,issue_id
APPENDIX,0.9206477732793522,paper_cnt
APPENDIX,0.9214574898785425,pub_date
APPENDIX,0.9222672064777327,ref_cnt
APPENDIX,0.9230769230769231,start_page
APPENDIX,0.9238866396761134,volume
APPENDIX,0.9246963562753037,publisher name title 0 10 20 30 40 50 60 70
APPENDIX,0.9255060728744939,Test Accuracy [%] rank
APPENDIX,0.9263157894736842,cite_cnt
APPENDIX,0.9271255060728745,"created
doi"
APPENDIX,0.9279352226720647,end_page
APPENDIX,0.928744939271255,entity_id
APPENDIX,0.9295546558704454,est_cite_cnt
APPENDIX,0.9303643724696357,issue_id
APPENDIX,0.9311740890688259,paper_cnt
APPENDIX,0.9319838056680162,pub_date
APPENDIX,0.9327935222672065,ref_cnt
APPENDIX,0.9336032388663967,start_page
APPENDIX,0.934412955465587,volume
APPENDIX,0.9352226720647774,publisher name title
APPENDIX,0.9360323886639677,"rank
cite_cnt"
APPENDIX,0.9368421052631579,created
APPENDIX,0.9376518218623482,"doi
end_page"
APPENDIX,0.9384615384615385,"entity_id
est_cite_cnt"
APPENDIX,0.9392712550607287,"issue_id
paper_cnt"
APPENDIX,0.940080971659919,pub_date
APPENDIX,0.9408906882591093,"ref_cnt
start_page"
APPENDIX,0.9417004048582996,"volume
publisher name"
APPENDIX,0.9425101214574899,"title
35 40 45 50 55 60 65 70"
APPENDIX,0.9433198380566802,"Figure 23: MAKG small (node classiﬁcation, 4 classes, GraphSAGE-only results). Impact from different properties and their combinations
on the accuracy. Green: the accuracy is better than that of a graph with no labels/properties; red: the accuracy is worse than that of a graph
with no labels/properties."
APPENDIX,0.9441295546558705,Neural Graph Databases rank
APPENDIX,0.9449392712550607,entity_id
APPENDIX,0.945748987854251,cite_cnt
APPENDIX,0.9465587044534413,created
APPENDIX,0.9473684210526315,end_page
APPENDIX,0.9481781376518219,est_cite_cnt
APPENDIX,0.9489878542510122,issue_id
APPENDIX,0.9497975708502024,paper_cnt
APPENDIX,0.9506072874493927,pub_date
APPENDIX,0.951417004048583,ref_cnt
APPENDIX,0.9522267206477733,"start_page
doi none"
APPENDIX,0.9530364372469635,volume name
APPENDIX,0.9538461538461539,"publisher
title 0 10 20 30 40 50 60 70"
APPENDIX,0.9546558704453442,Train Accuracy [%] rank
APPENDIX,0.9554655870445344,entity_id
APPENDIX,0.9562753036437247,cite_cnt
APPENDIX,0.957085020242915,created
APPENDIX,0.9578947368421052,end_page
APPENDIX,0.9587044534412955,est_cite_cnt
APPENDIX,0.9595141700404858,issue_id
APPENDIX,0.9603238866396762,paper_cnt
APPENDIX,0.9611336032388664,pub_date
APPENDIX,0.9619433198380567,ref_cnt
APPENDIX,0.962753036437247,"start_page
doi"
APPENDIX,0.9635627530364372,volume name
APPENDIX,0.9643724696356275,"publisher
title"
APPENDIX,0.9651821862348178,"rank
entity_id"
APPENDIX,0.965991902834008,cite_cnt
APPENDIX,0.9668016194331984,"created
end_page
est_cite_cnt"
APPENDIX,0.9676113360323887,"issue_id
paper_cnt"
APPENDIX,0.968421052631579,pub_date
APPENDIX,0.9692307692307692,"ref_cnt
start_page"
APPENDIX,0.9700404858299595,"doi
volume"
APPENDIX,0.9708502024291498,"name
publisher"
APPENDIX,0.97165991902834,"title
56 58 60 62 64 66 68 70 none"
APPENDIX,0.9724696356275304,cite_cnt
APPENDIX,0.9732793522267207,created doi
APPENDIX,0.974089068825911,end_page
APPENDIX,0.9748987854251012,entity_id
APPENDIX,0.9757085020242915,est_cite_cnt
APPENDIX,0.9765182186234818,issue_id
APPENDIX,0.977327935222672,paper_cnt
APPENDIX,0.9781376518218623,pub_date rank
APPENDIX,0.9789473684210527,ref_cnt
APPENDIX,0.979757085020243,start_page
APPENDIX,0.9805668016194332,volume
APPENDIX,0.9813765182186235,publisher name title 0 10 20 30 40 50 60 70 80
APPENDIX,0.9821862348178138,Test Accuracy [%]
APPENDIX,0.982995951417004,cite_cnt
APPENDIX,0.9838056680161943,"created
doi"
APPENDIX,0.9846153846153847,end_page
APPENDIX,0.9854251012145749,entity_id
APPENDIX,0.9862348178137652,est_cite_cnt
APPENDIX,0.9870445344129555,issue_id
APPENDIX,0.9878542510121457,paper_cnt
APPENDIX,0.988663967611336,"pub_date
rank"
APPENDIX,0.9894736842105263,ref_cnt
APPENDIX,0.9902834008097166,start_page
APPENDIX,0.9910931174089069,volume
APPENDIX,0.9919028340080972,publisher name title
APPENDIX,0.9927125506072875,cite_cnt
APPENDIX,0.9935222672064777,created
APPENDIX,0.994331983805668,"doi
end_page"
APPENDIX,0.9951417004048583,"entity_id
est_cite_cnt"
APPENDIX,0.9959514170040485,"issue_id
paper_cnt"
APPENDIX,0.9967611336032388,pub_date
APPENDIX,0.9975708502024292,"rank
ref_cnt
start_page"
APPENDIX,0.9983805668016195,"volume
publisher name title 30 40 50 60 70"
APPENDIX,0.9991902834008097,"Figure 24: MAKG small (node classiﬁcation, 4 classes, MLP-only results). Impact from different properties and their combinations on the
accuracy. Green: the accuracy is better than that of a graph with no labels/properties; red: the accuracy is worse than that of a graph with no
labels/properties."
