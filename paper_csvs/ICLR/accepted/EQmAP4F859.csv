Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0009057971014492754,"To understand how deep learning works, it is crucial to understand the training
dynamics of neural networks. Several interesting hypotheses about these dynam-
ics have been made based on empirically observed phenomena, but there exists a
limited theoretical understanding of when and why such phenomena occur.
In this paper, we consider the training dynamics of gradient ﬂow on kernel least-
squares objectives, which is a limiting dynamics of SGD trained neural networks.
Using precise high-dimensional asymptotics, we characterize the dynamics of the
ﬁtted model in two “worlds”: in the Oracle World the model is trained on the pop-
ulation distribution and in the Empirical World the model is trained on a sampled
dataset. We show that under mild conditions on the kernel and L2 target regression
function the training dynamics undergo three stages characterized by the behav-
iors of the models in the two worlds. Our theoretical results also mathematically
formalize some interesting deep learning phenomena. Speciﬁcally, in our setting
we show that SGD progressively learns more complex functions and that there is
a “deep bootstrap” phenomenon: during the second stage, the test error of both
worlds remain close despite the empirical training error being much smaller. Fi-
nally, we give a concrete example comparing the dynamics of two different kernels
which shows that faster training is not necessary for better generalization."
INTRODUCTION,0.0018115942028985507,"1
INTRODUCTION"
INTRODUCTION,0.002717391304347826,"In order to fundamentally understand how and why deep learning works, there has been much effort
to understand the dynamics of neural networks trained by gradient descent based algorithms. This
effort has led to the discovery of many intriguing empirical phenomena (e.g. Frankle et al. (2020);
Fort et al. (2020); Nakkiran et al. (2019a;b; 2020)) that help shape our conceptual framework for un-
derstanding the learning process in neural networks. Nakkiran et al. (2019b) provides evidence that
SGD starts by ﬁrst learning a linear classiﬁer and over time learns increasingly complex functions.
Nakkiran et al. (2020) introduces the “deep bootstrap” phenomenon: for some deep learning tasks
the empirical world test error remains close to the oracle world error1 for many SGD iterations, even
if the empirical training and test errors display a large gap. To better understand such phenomena, it
is useful to study training dynamics in related but mathematically tractable settings."
INTRODUCTION,0.0036231884057971015,"One approach for theoretical investigation is to study kernel methods, which were recently shown to
have a tight connection with over-parameterized neural networks (Jacot et al., 2018; Du et al., 2018).
Indeed, consider a sequence of neural networks (fN(x; θ))N∈N with the widths of the layers going
to inﬁnity as N →∞. Assuming proper parametrization and initialization, for large N the SGD
dynamics on fN is known to be well approximated by the corresponding dynamics on the ﬁrst-order
Taylor expansion of fN around its initialization θ0,"
INTRODUCTION,0.004528985507246377,"fN,lin(x; θ) = fN(x; θ0) + ⟨∇θfN(x; θ0), θ −θ0⟩."
INTRODUCTION,0.005434782608695652,"Thus, in the large width limit it sufﬁces to study the dynamics on the linearization fN,lin. When
using the squared loss, these dynamics correspond to optimizing a kernel least-squares objective
with the neural tangent kernel KN(x, x′) = ⟨∇θfN(x; θ0), ∇θfN(x′; θ0)⟩."
INTRODUCTION,0.006340579710144928,1Their paper uses “Ideal World” for “Oracle World” and “Real World” for “Empirical World”.
INTRODUCTION,0.007246376811594203,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.008152173913043478,Approximation Error
INTRODUCTION,0.009057971014492754,"Stage 1
Stage 2
Stage 3 Error"
INTRODUCTION,0.009963768115942028,Time t
INTRODUCTION,0.010869565217391304,Oracle World
INTRODUCTION,0.01177536231884058,Empirical World Train
INTRODUCTION,0.012681159420289856,Empirical World Test
INTRODUCTION,0.01358695652173913,R(f or t )
INTRODUCTION,0.014492753623188406,̂Rn( ̂ft)
INTRODUCTION,0.015398550724637682,R( ̂ft)
INTRODUCTION,0.016304347826086956,"Figure 1: A conceptual drawing of empirical and oracle world learning curves. Stage 1: all curves
are together. Stage 2: training error goes to zero while test and oracle error stay together. Stage 3:
test error remains constant while oracle error decays to the RKHS approximation error. See Section
1.1 for a more detailed discussion. (Dotted lines in stage 3 indicate compressed time interval.)"
INTRODUCTION,0.017210144927536232,"Over the past few years, researchers have used kernel machines as a tractable model to investigate
many neural network phenomena including benign overﬁtting, i.e., generalization despite the inter-
polation of noisy data (Bartlett et al., 2020; Liang & Rakhlin, 2020) and double-descent, i.e, risk
curves that are not classically U-shaped (Belkin et al., 2020; Liu et al., 2021). Kernels have also
been studied to better understand certain aspects of neural network architectures such as invariance
and stability (Bietti & Mairal, 2017; Mei et al., 2021b). Although kernel methods cannot be used to
explain some phenomena such as feature learning, they can still be conceptually useful for under-
standing other neural networks properties."
THREE STAGES OF KERNEL DYNAMICS,0.018115942028985508,"1.1
THREE STAGES OF KERNEL DYNAMICS"
THREE STAGES OF KERNEL DYNAMICS,0.019021739130434784,"Despite much classical work in the study of gradient descent training of kernel machines (e.g. Yao
et al. (2007); Raskutti et al. (2014)) there has been limited work understanding the high-dimensional
setting, which is the setting of interest in this paper. Although solving the linear dynamics of gradient
ﬂow is simple, the statistical analysis of the ﬁtted model requires involved random matrix theory
arguments. In our analysis we study the dynamics of the Oracle World, where training is done on
the (usually inaccessible) population risk, and the Empirical World, where training is done on the
empirical risk (as is done in practice). Associated with the oracle world model f or
t and the empirical
world model ˆft are the following quantities of interest: the empirical training error bRn( ˆft), the
empirical test error R( ˆft), and the oracle error R(f or
t ) deﬁned in Eqs. (1), (2), (3) for which we
derive expressions that are accurate in high dimensions."
THREE STAGES OF KERNEL DYNAMICS,0.019927536231884056,"Informally, our main results show that under reasonable conditions on the regression function and
the kernel the training dynamics undergo the following three stages:"
THREE STAGES OF KERNEL DYNAMICS,0.020833333333333332,"• Stage one: the empirical training error, the empirical test error, and the oracle error are all close.
• Stage two: the empirical training error decays to zero, but the empirical test error and the oracle
error stay close and keep approximately constant.
• Stage three: the empirical training error is still zero, the empirical test error stays approximately"
THREE STAGES OF KERNEL DYNAMICS,0.021739130434782608,"constant, but the oracle test error decays to the approximation error."
THREE STAGES OF KERNEL DYNAMICS,0.022644927536231884,"We conceptually illustrate the error curves of the oracle and empirical world in Fig. 1 and provide
intuition for the evolution of the learned models in Fig. 2. The existence of the ﬁrst and third
stages are not unexpected: at the beginning of training the model has not ﬁt the dataset enough to
distinguish the oracle and empirical world and at the end of training an expressive enough model
with inﬁnite samples will outperform one with ﬁnitely many. The most interesting stage is the
second one where the empirical model begins to “overﬁt” the training set while still remaining close
to the non-interpolating oracle model in the L2 sense (see Fig. 2)."
THREE STAGES OF KERNEL DYNAMICS,0.02355072463768116,"In Section 2 we discuss some related work. In Section 3 we elaborate our description of the three
stages and give a mathematical characterization for two particular settings in Theorems 1 and 2."
THREE STAGES OF KERNEL DYNAMICS,0.024456521739130436,Under review as a conference paper at ICLR 2022
THREE STAGES OF KERNEL DYNAMICS,0.025362318840579712,Target
THREE STAGES OF KERNEL DYNAMICS,0.026268115942028984,"Empirical
Oracle fd x y"
THREE STAGES OF KERNEL DYNAMICS,0.02717391304347826,"Stage 1
Stage 2 — start
Stage 2 — end
Stage 3"
THREE STAGES OF KERNEL DYNAMICS,0.028079710144927536,"Figure 2: A conceptual drawing of the evolution of the empirical and oracle models ˆft and f or
t . In
stage 1, ˆft and f or
t
learn the best linear approximation of fd. At the start of stage 2, ˆft and f or
t
learn
the best quadratic approximation. At the end of stage 2, ˆft interpolates the training set but is close
to f or
t
in the L2 sense. Lastly in stage 3, f or
t
learns fd while ˆft stays the same as the end of stage 2."
THREE STAGES OF KERNEL DYNAMICS,0.028985507246376812,"Although the three stages arise fairly generally, we remark that certain stages will vanish if the
problem parameters are chosen in a special way (c.f. Remark 1). We connect our theoretical results
to related empirical deep learning phenomena in Remark 3 and discuss the relation to deep learning
in practice in Remark 4. In Section 4 we provide numerical simulations to illustrate the theory more
concretely and in Section 5 we end with a summary and discussion of the results."
RELATED LITERATURE,0.029891304347826088,"2
RELATED LITERATURE"
RELATED LITERATURE,0.030797101449275364,"The generalization error of the kernel ridge regression (KRR) solution has been well-studied in both
the ﬁxed dimension regime (Wainwright, 2019, Chap. 13), (Caponnetto & De Vito, 2007) and the
high-dimensional regime (El Karoui, 2010; Liang & Rakhlin, 2020; Liu et al., 2021; Ghorbani et al.,
2020; 2021; Mei et al., 2021a;b). Most closely related to our results is the setting of (Ghorbani
et al., 2021; Mei et al., 2021a;b). Analysis of the entire KRR training trajectory has also been done
(Yao et al., 2007; Raskutti et al., 2014; Cao et al., 2019) but only for the ﬁxed dimensional setting.
Classical non-parametric rates are often obtained by specifying a strong regularity assumption on
the target function (e.g. the source condition in Fischer & Steinwart (2020)), whereas in our work
the assumption on the target function is mild."
RELATED LITERATURE,0.03170289855072464,"Another line of work directly studies the dynamics of learning in linear neural networks (Saxe et al.,
2013; Li et al., 2018; Arora et al., 2019; Vaskevicius et al., 2019). Similar to us, these works show
that some notion of complexity (typically effective rank or sparsity) increases in the linear network
over the course of optimization."
RELATED LITERATURE,0.03260869565217391,"The relationship between the speed of iterative optimization and gap between population and empir-
ical quantities has been studied before in the context of algorithmic stability (Bousquet & Elisseeff,
2002; Hardt et al., 2016; Chen et al., 2018). These analyses certify good empirical generalization
by using stability in the ﬁrst few iterations to upper bound the gap between train and test error. In
contrast, our analysis directly computes the errors at an arbitrary time t (c.f. Remark 2). The rela-
tionship between oracle and empirical training dynamics has been considered before in Bottou &
LeCun (2004) and Pillaud-Vivien et al. (2018)."
RESULTS,0.03351449275362319,"3
RESULTS"
RESULTS,0.034420289855072464,"In this section we introduce the problem and present a specialization of our results to two concrete
settings: dot product and group invariant kernels on the sphere (Theorems 1 and 2 respectively). The
more general version of our results is described in Appendix A.3."
PROBLEM SETUP,0.035326086956521736,"3.1
PROBLEM SETUP"
PROBLEM SETUP,0.036231884057971016,"We consider the supervised learning problem where we are given i.i.d. data (xi, yi)i≤n. The covari-
ate vectors (xi)i≤n ∼iid Unif(Sd−1(
√"
PROBLEM SETUP,0.03713768115942029,"d)) and the real-valued noisy responses yi = fd(xi) + εi for
some unknown target function fd ∈L2(Sd−1(
√"
PROBLEM SETUP,0.03804347826086957,"d)) and (εi)i≤n ∼iid N(0, σ2
ε). Given a function"
PROBLEM SETUP,0.03894927536231884,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.03985507246376811,"f ∈L2(Sd−1(
√"
PROBLEM SETUP,0.04076086956521739,"d)), we deﬁne its test error R(f) and its training error bRn(f) as"
PROBLEM SETUP,0.041666666666666664,"R(f) ≡E(xnew,ynew){(ynew −f(xnew))2},
bRn(f) ≡1 n n
X"
PROBLEM SETUP,0.042572463768115944,"i=1
(yi −f(xi))2
(1)"
PROBLEM SETUP,0.043478260869565216,"where (xnew, ynew) is i.i.d. with (xi, yi)i≤n. The test error R(f) measures the ﬁt of f on the
population distribution and the training error bRn(f) measures the ﬁt of f to the training set."
PROBLEM SETUP,0.044384057971014496,"For a kernel function Hd : Sd−1(
√"
PROBLEM SETUP,0.04528985507246377,"d) × Sd−1(
√"
PROBLEM SETUP,0.04619565217391304,"d) →R, we analyse the dynamics of the following
two ﬁtted models indexed by time t: the oracle model f or
t
and the empirical model ˆft, which are
given by the gradient ﬂow on R and bRn over the associated RKHS Hd respectively
d
dtf or
t (x) = −∇R(f or
t (x)) = E[Hd(x, z)(fd(z) −f or
t (z))],
(2)"
PROBLEM SETUP,0.04710144927536232,"d
dt
ˆft(x) = −∇bRn( ˆft(x)) = 1 n n
X"
PROBLEM SETUP,0.04800724637681159,"i=1
Hd(x, xi)(yi −ˆft(xi)),
(3)"
PROBLEM SETUP,0.04891304347826087,"with zero initialization f or
0 ≡ˆf0 ≡0. These dynamics are motivated from the neural tangent kernel
perspective of over-parameterized neural networks (Jacot et al., 2018; Du et al., 2018). A precise
mathematical deﬁnition and derivation of these two dynamics are provided in Appendix E.1."
PROBLEM SETUP,0.049818840579710144,"For our results we make some assumptions on the spectral properties of the kernels Hd similar to
those in Mei et al. (2021a) that are discussed in detail in Section A.2. At a high-level we require that
the diagonal elements of the kernel concentrate, that the kernel eigenvalues obey certain spectral gap
conditions, and that the top eigenfunctions obey a hyperconctractivity condition which says they are
“delocalized”. For the speciﬁc settings of Theorems 1 and 2 we give more speciﬁc conditions on the
kernels that are more easily veriﬁed and imply the required spectral properties."
DOT PRODUCT KERNELS,0.050724637681159424,"3.2
DOT PRODUCT KERNELS"
DOT PRODUCT KERNELS,0.051630434782608696,"In our ﬁrst example, we consider dot product kernels Hd of the form"
DOT PRODUCT KERNELS,0.05253623188405797,"Hd(x1, x2) = hd(⟨x1, x2⟩/d),
∀x1, x2 ∈Sd−1(
√"
DOT PRODUCT KERNELS,0.05344202898550725,"d),
(4)
for some function hd : [−1, 1] →R. Our results apply to general dot product kernels under weak
conditions on hd given in Appendix C.2. In particular they apply to the random feature and neural
tangent kernels associated to certain fully connected neural networks (Jacot et al., 2018)."
DOT PRODUCT KERNELS,0.05434782608695652,"Before presenting our results for this setting we introduce some notation. Denote by P≤ℓthe orthog-
onal projection onto the subspace of L2(Sd−1(
√"
DOT PRODUCT KERNELS,0.0552536231884058,"d)) spanned by polynomials of degree less than or
equal to ℓ. The projectors Pℓand P>ℓare deﬁned analogously (see Appendix G for details). We
use od(·) for standard little-o relations, where the subscript d emphasizes the asymptotic variable.
The statement f(d) = ωd(g(d)) is equivalent to g(d) = od(f(d)). We use od,P(·) in probability
relations. Namely for two sequences of random variables Z1(d) and Z2(d), Z1(d) = od,P(Z2(d)) if
for any ε, Cε > 0 there exists dε ∈Z>0, such that P(|Z1(d)/Z2(d)| < Cε) ≤ε for all d ≥dε. The
asymptotic notations Od, Od,P etc. are deﬁned analogously."
DOT PRODUCT KERNELS,0.05615942028985507,"Theorem 1 (Dot Product Kernels). Let {fd ∈L2(Sd−1(
√"
DOT PRODUCT KERNELS,0.057065217391304345,"d))}d≥1 be a sequence of functions such
that for some η > 0, ∥fd∥L2+η = Od(1), and let {Hd}d≥1 be a sequence of dot product kernels
satisfying Assumption 4. Assume that for some ﬁxed integers j, s ≥0 and some δ > 0 that"
DOT PRODUCT KERNELS,0.057971014492753624,"dj+δ ≤t ≤dj+1−δ,
ds+δ ≤n ≤ds+1−δ.
Then we have the following characterizations,"
DOT PRODUCT KERNELS,0.058876811594202896,(a) (Oracle World) The oracle model learns every degree component of fd as time progresses
DOT PRODUCT KERNELS,0.059782608695652176,"R(f or
t ) =
P>jfd
2
L2 + σ2
ε + od(1),
f or
t −P≤jfd
2
L2 = od(1)."
DOT PRODUCT KERNELS,0.06068840579710145,(b) (Empirical World – Train) Empirical training error follows oracle error then goes to zero
DOT PRODUCT KERNELS,0.06159420289855073,"bRn( ˆft) =
P>jfd
2
L2 + σ2
ε + od,P(1)
if t/n = od(1),"
DOT PRODUCT KERNELS,0.0625,"bRn( ˆft) = od,P(1)
if t/n = ωd(1)."
DOT PRODUCT KERNELS,0.06340579710144928,Under review as a conference paper at ICLR 2022
DOT PRODUCT KERNELS,0.06431159420289854,"(a) Dot prod. kernel, n ≍d2.5
(b) Dot prod. kernel, n ≍d2.99
(c) Dot prod. vs cyclic, n ≍d2.5"
DOT PRODUCT KERNELS,0.06521739130434782,"Figure 3: Schematic drawings of the conclusions of Theorems 1 and 2 for three different noiseless
settings. Panels (3a) and (3b) illustrate the performance of a dot product kernel Hd for two different
scalings n(d). The full three stages appear in (3a), but the second stages disappears in (3b) since
log n/ log d is nearly an integer (see Remark 1). Panel (3c) compares the performance of a dot
product kernel Hd (red) and corresponding cyclic kernel Hd,inv (green) for a cyclic target function
fd. The cyclic kernel in (3c) generalizes better but optimizes more slowly (see Remark 2)."
DOT PRODUCT KERNELS,0.0661231884057971,"(c) (Empirical World – Test) Empirical test error follows oracle error until the empirical model
learns the degree-s component of fd"
DOT PRODUCT KERNELS,0.06702898550724638,"R( ˆft) =
P>min{j,s}fd
2"
DOT PRODUCT KERNELS,0.06793478260869565,"L2 + σ2
ε + od,P(1),
 ˆft −P≤min{j,s}fd

2"
DOT PRODUCT KERNELS,0.06884057971014493,"L2 = od,P(1)."
DOT PRODUCT KERNELS,0.06974637681159421,"The results are conceptually illustrated in an example in Fig. 3a which shows the stair-case phe-
nomenon in high-dimensions and the three learning stages. We see that in both the oracle world and
the empirical world, the prediction model increases in complexity over time. More precisely, the
model learns the best polynomial ﬁt to the target function (in an L2 sense) of increasingly higher
degree. In the empirical world the maximum complexity is determined by the sample size n, which
is in contrast to the oracle world where there are effectively inﬁnite samples."
DOT PRODUCT KERNELS,0.07065217391304347,"The results imply that generally (but not always c.f. Remark 1) there will be three stages of learning.
In the ﬁrst stage the oracle and empirical world models are close in L2 and ﬁt a polynomial with
degree determined by t. The ﬁrst stage lasts from t = 0 to t = nd−ε ≪n for some small ε > 0.
As t approaches n, there is a phase transition and the empirical world training error goes to zero at
t = ndε ≫n. From time nd−ε till at least ds+1 is the second stage where the empirical and oracle
models remain close in L2 but the gap between test and train error can be large. If the sample size
n is not large enough for ˆft to learn the target function, then at some large enough t we will enter
a third stage where f or
t
improves in performance, outperforming ˆft which remains the same. On
synthetic data in ﬁnite dimensions we can see a resemblance of the staircase shape which becomes
sharper with increasing d (c.f. Appendix F)."
DOT PRODUCT KERNELS,0.07155797101449275,"Remark 1 (Degenerate stages of Learning). For special problem parameters we will not observe
the second and/or third stages. The second stage will disappear if n ≍dq for some q ∈N (see Fig.
3b), or if fd is a degree-s polynomial. The third stage will not occur if P>sfd lies in the orthogonal
complement of the RKHS Hd."
GROUP INVARIANT KERNELS,0.07246376811594203,"3.3
GROUP INVARIANT KERNELS"
GROUP INVARIANT KERNELS,0.07336956521739131,"We now consider our second setting which concerns the invariant function estimation problem
introduced in Mei et al. (2021b). As before, we are given i.i.d. data (xi, yi)i≤n where the fea-
ture vectors (xi)i≤n ∼iid Unif(Sd−1(
√"
GROUP INVARIANT KERNELS,0.07427536231884058,"d)) and noisy responses yi = fd(xi) + εi.
We now
assume that the target function fd satisﬁes an invariant property.
We consider a general type
of invariance, deﬁned by a group Gd that is represented as a subgroup of the orthogonal group
in d dimensions.
The group element g ∈Gd acts on a vector x ∈Rd via x 7→g · x.
We say that f⋆is Gd-invariant if f⋆(x) = f⋆(g · x) for all g ∈Gd.
We denote the space
of square integrable Gd-invariant functions by L2(Sd−1(
√"
GROUP INVARIANT KERNELS,0.07518115942028986,"d), Gd). We focus on groups Gd that
are groups of degeneracy α as deﬁned below.
As an example, we consider the cyclic group"
GROUP INVARIANT KERNELS,0.07608695652173914,Under review as a conference paper at ICLR 2022
GROUP INVARIANT KERNELS,0.0769927536231884,"Cycd = {g0, g1, . . . , gd−1} where for any x = (x1, . . . , xd)T ∈Sd−1(
√"
GROUP INVARIANT KERNELS,0.07789855072463768,"d), the group action is
deﬁned by gi · x = (xi+1, xi+2, . . . , xd, x1, x2, . . . , xi)T. The cyclic group has degeneracy 1."
GROUP INVARIANT KERNELS,0.07880434782608696,"Deﬁnition 1 (Groups of degeneracy α). Let Vd,k be the subspace of degree-k polynomials that are
orthogonal to polynomials of degree at most (k −1) in L2(Sd−1(
√"
GROUP INVARIANT KERNELS,0.07971014492753623,"d)), and denote by Vd,k(Gd) the
subspace of Vd,k formed by polynomials that are Gd-invariant. We say that Gd has degeneracy α if
for any integer k ≥α we have dim(Vd,k/Vd,k(Gd)) ≍dα (i.e., there exists 0 < ck ≤Ck < +∞
such that ck ≤dim(Vd,k/Vd,k(Gd)) ≤Ck for any d ≥2)."
GROUP INVARIANT KERNELS,0.0806159420289855,To encode invariance in our kernel we consider Gd-invariant kernels Hd of the form
GROUP INVARIANT KERNELS,0.08152173913043478,"Hd,inv(x1, x2) =
Z"
GROUP INVARIANT KERNELS,0.08242753623188406,"Gd
h(⟨x1, g · x2⟩/d)πd(dg)
(5)"
GROUP INVARIANT KERNELS,0.08333333333333333,"where πd is the Haar measuare on Gd. Such kernels satisfy the following invariance property: for all
g, g′ ∈Gd and for Hd(x1, x2) = Hd(g · x1, g′ · x2) for every x1, x2. For the cyclic group, πd is
the uniform measure. We now present our results for the group invariant setting."
GROUP INVARIANT KERNELS,0.08423913043478261,"Theorem 2 (Group Invariant Kernels). Let Gd be a group of degeneracy α ≤1 according to Def-
inition 1. Let {fd ∈L2(Sd−1(
√"
GROUP INVARIANT KERNELS,0.08514492753623189,"d), Gd)}d≥1 a sequence of Gd-invariant functions such that for
some η > 0, ∥fd∥L2+η = Od(1), and let {Hd}d≥1 be a sequence of Gd-invariant kernels satisfying
Assumption 5. Assume that for some ﬁxed integers j ≥0, s ≥1 and some δ > 0 that"
GROUP INVARIANT KERNELS,0.08605072463768115,"dj+δ ≤t ≤dj+1−δ,
ds−α+δ ≤n ≤ds+1−α−δ."
GROUP INVARIANT KERNELS,0.08695652173913043,"Then we have the following characterizations,"
GROUP INVARIANT KERNELS,0.08786231884057971,(a) (Oracle World) The oracle model learns every degree component of fd as time progresses
GROUP INVARIANT KERNELS,0.08876811594202899,"R(f or
t ) =
P>jfd
2
L2 + σ2
ε + od(1),
f or
t −P≤jfd
2
L2 = od(1)."
GROUP INVARIANT KERNELS,0.08967391304347826,(b) (Empirical World – Train) Empirical training error follows oracle error then goes to zero
GROUP INVARIANT KERNELS,0.09057971014492754,"bRn( ˆft) =
P>jfd
2
L2 + σ2
ε + od,P(1)
if t/n = od(dα),"
GROUP INVARIANT KERNELS,0.09148550724637682,"bRn( ˆft) = od,P(1)
if t/n = ωd(dα)."
GROUP INVARIANT KERNELS,0.09239130434782608,"(c) (Empirical World – Test) Empirical test error follows oracle error until the empirical model
learns the degree-s component of fd"
GROUP INVARIANT KERNELS,0.09329710144927536,"R( ˆft) =
P>min{j,s}fd
2"
GROUP INVARIANT KERNELS,0.09420289855072464,"L2 + σ2
ε + od,P(1),
 ˆft −P≤min{j,s}fd

2"
GROUP INVARIANT KERNELS,0.09510869565217392,"L2 = od,P(1)."
GROUP INVARIANT KERNELS,0.09601449275362318,"With respect to the dot product kernel setting (c.f. Theorem 1), in this setting the behavior of the
oracle world is unchanged, but the empirical world behaves as if it has dα times as many samples.
This is illustrated graphically in Fig. 3c. It can be shown that using an invariant kernel is equivalent
to using a dot product kernel and augmenting the dataset to {(g · xi, yi) : g ∈Gd, i ∈[n]} (c.f. Ap-
pendix E.2). Hence for the cyclic group which has size dα = d, we arrive at the following intriguing
conclusion: if the target function is invariant, then using a dot product kernel and augmenting n i.i.d
samples to nd many samples is asymptotically equivalent to using nd i.i.d samples."
GROUP INVARIANT KERNELS,0.09692028985507246,"Remark 2 (Optimization Speed versus Generalization). Interestingly, training with an invariant
kernel is slower than with a dot product kernel and takes longer to interpolate the dataset despite
eventually generalizing better on invariant function estimation tasks (c.f. Fig. 3c). This conclusion
is not an artifact of the continuous time analysis (c.f. Appendix E.3) and is observed empirically in
Section 4.2 for discrete-time SGD. This example highlights the limitation of stability based analyses
(e.g. Hardt et al. (2016)) which argue that faster SGD training leads to better generalization. While
a faster rate leads to better generalization in the ﬁrst stage when stability can control the gap be-
tween train and test error, our analysis shows that the duration length of the ﬁrst stage also impacts
the ﬁnal generalization error."
GROUP INVARIANT KERNELS,0.09782608695652174,"Remark 3 (Connection with Deep Phenomena). The dynamics of high-dimensional kernel regres-
sion display behaviors that parallel some empirically observed phenomena in deep learning. For"
GROUP INVARIANT KERNELS,0.09873188405797101,Under review as a conference paper at ICLR 2022
GROUP INVARIANT KERNELS,0.09963768115942029,"kernel regression, we have shown that the complexity of the empirical model, measured as the num-
ber of learned eigenfunctions, depends on the time optimized when t ≪n and the sample size when
t ≫n. At a high-level, we also expect a similar story for neural networks but for some other notion
of complexity. It is believed that neural networks ﬁrst learn simple functions and then progressively
more complex ones, until the complexity saturates after interpolating at some time proportional to n
(Nakkiran et al., 2019b). We have also shown that in kernel regression there is a non-trivial “deep
boostrap” phenomenon (Nakkiran et al., 2020) during the second learning stage: the gap between
the oracle world and empirical world test errors is negligible whereas the train and test errors ex-
hibit a substantial gap. The gradient ﬂow results for kernel regression can also provide insight into
the deep bootstrap for random feature networks SGD as these results can approximately predict
their behavior (see Section 4.2)."
GROUP INVARIANT KERNELS,0.10054347826086957,"Remark 4 (Connection with Deep Learning Practice). Although we believe our results conceptually
shed light on some of the interesting behaviors observed in the training dynamics of deep learning,
due to our stylized setting we may not exactly see the predicted phenomena in practice. Accurately
observing the three stages of kernel regression requires sufﬁciently high-dimensional data in order
for the kernel eigenvalues to obey a staircase-like decay and for training to be sufﬁciently long
as the time axis should be in log-scale. Our results hold for regression whereas for classiﬁcation
the empirical model may continue improving after classifying the train set correctly. Despite these
caveats, certain conclusions can be observed in some realistic settings (c.f. Appendix E.4)."
NUMERICAL SIMULATIONS,0.10144927536231885,"4
NUMERICAL SIMULATIONS"
NUMERICAL SIMULATIONS,0.10235507246376811,"As mentioned previously, Fig. 3 is a “cartoon” of the conclusions stated in Theorems 1 and 2. In
this section, we verify the qualitative predictions of our theorems using synthetic data. Concretely,
throughout this section we take d = 400 and n = d1.5 = 8000, and following our theoretical setup
(c.f. Section 3.1) generate covariates (xi)i≤n ∼iid Unif(Sd−1(
√"
NUMERICAL SIMULATIONS,0.10326086956521739,"d)) and responses yi = f⋆(xi)+εi
with (εi)i≤n ∼iid N(0, σ2
ε), for different choices of target function f⋆. All simulations in this
section are for the noiseless case σ2
ε = 0 but a noisy example is given in Appendix F."
NUMERICAL SIMULATIONS,0.10416666666666667,"In Section 4.1, we simulate the gradient ﬂows of kernel least-squares with dot product kernels and
cyclic kernels (Fig. 4) to reproduce the three stages as shown in Fig. 3. In Section 4.2 we show
that SGD of (dot product and cyclic) random-feature models (Fig. 5) exhibit similar three stages
phenomena, in which the second stage behaviors are consistent with the deep bootstrap phenomena
observed in deep learning experiments (Nakkiran et al., 2020). Empirical quantities are averaged
over 10 trials and the shaded regions indicate one standard deviation from the mean."
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.10507246376811594,"4.1
GRADIENT FLOW OF KERNEL LEAST-SQUARES"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.10597826086956522,"Under the synthetic data set-up mentioned earlier, we ﬁrst simulate the oracle world and empirical
world errors curves of gradient ﬂow dynamics using dot product kernels of the form"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.1068840579710145,"H(x1, x2) = Ew∼Sd−1[σ(⟨w, x1⟩)σ(⟨w, x2⟩)]
(6)"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.10778985507246377,"for some activation function σ. We will examine a few different choices of f⋆and kernel H, which
are speciﬁed in the descriptions of each ﬁgure."
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.10869565217391304,"The oracle world error is computed analytically but the empirical world errors require sampling
train and test datasets. We compute empirical world errors by averaging over 10 trials. The results
are visualized both in log-scale and linear-scale on the time axis. The log-scale plots allow for
direct comparison with the cartoons in Fig. 3. The linear-scale plots are zoomed into the region
0 < t ≤nd0.4 since: 1) plotting the full interval squeeze all curves to the left boundary which is
uninformative 2) in practice one would not optimize for very long after interpolation."
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.10960144927536232,In Figs. 4a and 4b we take the target function to be a polynomial of the form
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.1105072463768116,"f⋆(x) = a0 He0(x1) + . . . + ak Hek(x1),
Pjf⋆
2
L2 ≈a2
jj!,
(7)"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.11141304347826086,"where Hei(t) is the ith Hermite polynomial (c.f. Appendix G.4) and the approximate equality in Eq.
(7) holds in high-dimensions. In panel (4a) we consider use the ReLU activation function σ(t) =
max(t, 0), and take f⋆to be a quadratic polynomial with (a0, a1, a2) = (1/2, 1/
√"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.11231884057971014,"2, 1/
√"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.11322463768115942,8). With
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.11413043478260869,Under review as a conference paper at ICLR 2022
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.11503623188405797,"(a)
(b)
(c)"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.11594202898550725,"Figure 4: Top row: Log-scale plot of errors versus training time for dot product kernel (4a, 4b) and
{dot product, cyclic} kernels in (4c). In (4a) σ = ReLU and (a0, a1, a2) = (1/2, 1/
√"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.11684782608695653,"2, 1/
√"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.11775362318840579,"8). In
(4b), σ = ReLU +0.1 He3 and (a0, a1, a2, a3) = (1/2, 1/
√"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.11865942028985507,"2, 0, 1/
√"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.11956521739130435,"24). In (4c), f⋆is Eq. (8) and
σ = ReLU +0.1 He3. Bottom row: Same as the top row but with linear-scale time and zoomed in."
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.12047101449275362,"such a choice of parameters, we can see the three stages phenomenon. In panel (4b) we choose f⋆
to be a cubic polynomial with (a0, a1, a2, a3) = (1/2, 1/
√"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.1213768115942029,"2, 0, 1/
√"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.12228260869565218,"24) and σ(t) = max(t, 0) +
0.1 He3(t) (we need the third Hermite coefﬁcient of σ to be non-zero for stage 3 to occur). This
choice of coefﬁcients for f⋆is such that ∥P>1f⋆∥2
L2 ≈∥P>2f⋆∥2
L2, so that the second stage in (4b)
is longer compared to (4a)."
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.12318840579710146,"In Fig. 4c, we take the target function to be a cubic cyclic polynomial"
GRADIENT FLOW OF KERNEL LEAST-SQUARES,0.12409420289855072,"f⋆(x) =
1
√"
D,0.125,"3d d
X"
D,0.12590579710144928,"i=1
xi + d
X"
D,0.12681159420289856,"i=1
xixi+1 + d
X"
D,0.12771739130434784,"i=1
xixi+1xi+2 ! (8)"
D,0.1286231884057971,"where the subindex addition in xi+k is understood to be taken modulo d. We compare the per-
formance of the dot product kernel H and its invariant version Hinv (c.f. Eq. (5)) with activation
function σ(t) = max(t, 0) + 0.1 He3(t). The kernel Hinv with n samples performs equivalently to
H with nd samples (c.f. Remark 2), but is more computationally efﬁcient since the size of the kernel
matrix is still n × n. Using Hinv elongates the ﬁrst stage by a factor d, delaying the later stages and
ensuring that the empirical world model improves longer."
D,0.12952898550724637,"Although in the simulations, the dimension d is not yet high enough to see a totally sharp staircase
phenomenon as in the illustrations of Fig. 3, even for this d we are still able to clearly see the
three predicted learning stages and deep bootstrap phenomenon across a range of settings. To better
understand the effect of dimension we show similar plots with varying d in Appendix F."
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.13043478260869565,"4.2
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS"
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.13134057971014493,"To more closely relate with deep learning practice and the deep bootstrap phenomenon (Nakkiran
et al., 2020), we simulate the error curves of SGD training on random-feature (RF) models (i.e. two-
layer networks with random ﬁrst-layer weights and trainable second-layer weights), in the same
synthetic data setup as before. In particular, we look at dot product RF models"
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.1322463768115942,"ˆfdot(x; a) =
1
√ N N
X"
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.1331521739130435,"i=1
aiσ(⟨wi, x⟩),
wi ∼iid Unif(Sd−1),"
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.13405797101449277,and cyclic invariant RF models
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.13496376811594202,"ˆfcyc(x; a) =
1
√ N N
X"
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.1358695652173913,"i=1
ai Z"
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.13677536231884058,"Gd
σ(⟨wi, g · x⟩)πd(dg),
wi ∼iid Unif(Sd−1)."
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.13768115942028986,Under review as a conference paper at ICLR 2022
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.13858695652173914,"(a)
(b)
(c)"
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.13949275362318841,"Figure 5: Top Row: SGD dynamics of random-feature models as described in Section 4.2. The tar-
get function and data distribution of (5a), (5b), (5c) are that of (4a), (4b), (4c) respectively. Bottom
Row: The corresponding linear-scale errors of kernel least-squares gradient ﬂow for comparison."
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.1403985507246377,For all following experiments we take the activation σ to be ReLU and N = 4 × 105 ≈n1.4.
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.14130434782608695,"For a given data distribution and RF model we train two ﬁtted functions, one on a ﬁnite dataset
(empirical world) and the other on the data distribution (oracle world). More speciﬁcally, the training
of the empirical model is done using multi-pass SGD on a ﬁnite training set of size n with learning
rate η = 0.1 and batch size b = 50. The training of the oracle model is done using one-pass SGD
with the same learning rate η and batch size b, but at each iteration a fresh batch is sampled from
the population distribution. Both models ˆft, f or
t
are initialized with ai = 0 for i ∈[N]. To speed up
and stabilize optimization we use momentum β = 0.9. Note that if we took N →∞, η →0, and
β = 0 we would be exactly in the dot product kernel gradient ﬂow setting."
SGD FOR TWO-LAYER RANDOM-FEATURE MODELS,0.14221014492753623,"In Fig. 5, the data generating distributions of panels (5a), (5b), (5c) are respectively the same as
that of panels (4a), (4b), and (4c) from Section 4.1. The top row of Fig. 5 shows SGD for {dot
product, cyclic} RF models, and the bottom row shows the corresponding gradient ﬂow for {dot
product, cyclic} kernel least-squares. We see that the corresponding curves in these two rows exhibit
qualitatively the same behaviors. Additionally, the results in panel (5c) show that as predicted, even
for discrete SGD dynamics the dot product RF optimizes faster but fails to generalize, whereas the
invariant RF optimizes slower but generalizes better."
SUMMARY AND DISCUSSION,0.1431159420289855,"5
SUMMARY AND DISCUSSION"
SUMMARY AND DISCUSSION,0.14402173913043478,"In this paper, we used precise asymptotics to study the oracle world and empirical world dynamics
of gradient ﬂow on kernel least-squares objectives for high-dimensional regression problems. Under
reasonable conditions on the target function and kernel, we showed that in this setting there are three
learning stages based on the behaviors of the empirical and oracle models and also connected our
results to some empirical deep learning phenomena."
SUMMARY AND DISCUSSION,0.14492753623188406,"Although our setting already captures some interesting aspects of deep learning training dynamics,
there are some limitations which would be interesting to resolve in future work. We require very
high-dimensional data in order for the asymptotics to be accurate, but real data distributions have
low-dimensional structure. We work in a limiting regime of neural network training where the
dynamics are linear and the step-size is inﬁnitesimal. It is an important direction to extend this
analysis to the non-linear feature learning regime and to consider discrete step-size minibatch SGD,
as these are considered important aspects of network training. Our results hold for the square-loss
in regression problems, but many deep learning problems involve classiﬁcation using cross-entropy
loss. Lastly, our analysis holds speciﬁcally for gradient ﬂow, so it would be also interesting to
consider other iterative learning algorithms such as boosting."
SUMMARY AND DISCUSSION,0.14583333333333334,Under review as a conference paper at ICLR 2022
REFERENCES,0.14673913043478262,REFERENCES
REFERENCES,0.14764492753623187,"Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. Advances in Neural Information Processing Systems, 32:7413–7424, 2019."
REFERENCES,0.14855072463768115,"Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler. Benign overﬁtting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020."
REFERENCES,0.14945652173913043,"Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM
Journal on Mathematics of Data Science, 2(4):1167–1180, 2020."
REFERENCES,0.1503623188405797,"Alberto Bietti and Julien Mairal. Group invariance, stability to deformations, and complexity of
deep convolutional representations. arXiv preprint arXiv:1706.03078, 2017."
REFERENCES,0.151268115942029,"L´eon Bottou and Yann LeCun. Large scale online learning. Advances in neural information pro-
cessing systems, 16:217–224, 2004."
REFERENCES,0.15217391304347827,"Olivier Bousquet and Andr´e Elisseeff. Stability and generalization. The Journal of Machine Learn-
ing Research, 2:499–526, 2002."
REFERENCES,0.15307971014492755,"Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the
spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019."
REFERENCES,0.1539855072463768,"Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics, 7(3):331–368, 2007."
REFERENCES,0.15489130434782608,"Yuansi Chen, Chi Jin, and Bin Yu. Stability and convergence trade-off of iterative optimization
algorithms. arXiv preprint arXiv:1804.01619, 2018."
REFERENCES,0.15579710144927536,"Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018."
REFERENCES,0.15670289855072464,"Noureddine El Karoui. The spectrum of kernel random matrices. The Annals of Statistics, 38(1):
1–50, 2010."
REFERENCES,0.15760869565217392,"Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares algo-
rithms. J. Mach. Learn. Res., 21:205–1, 2020."
REFERENCES,0.1585144927536232,"Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy,
and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape
geometry and the time evolution of the neural tangent kernel. arXiv preprint arXiv:2010.15110,
2020."
REFERENCES,0.15942028985507245,"Jonathan Frankle, David J Schwab, and Ari S Morcos. The early phase of neural network training.
arXiv preprint arXiv:2002.10365, 2020."
REFERENCES,0.16032608695652173,"Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods? arXiv preprint arXiv:2006.13409, 2020."
REFERENCES,0.161231884057971,"Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension. The Annals of Statistics, 49(2):1029–1054, 2021."
REFERENCES,0.1621376811594203,"Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225–1234. PMLR,
2016."
REFERENCES,0.16304347826086957,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018."
REFERENCES,0.16394927536231885,"Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning The-
ory, pp. 2–47. PMLR, 2018."
REFERENCES,0.16485507246376813,"Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev
Arora. Enhanced convolutional neural tangent kernels. arXiv preprint arXiv:1911.00809, 2019."
REFERENCES,0.16576086956521738,Under review as a conference paper at ICLR 2022
REFERENCES,0.16666666666666666,"Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeless” regression can gener-
alize. The Annals of Statistics, 48(3):1329–1347, 2020."
REFERENCES,0.16757246376811594,"Fanghui Liu, Zhenyu Liao, and Johan Suykens. Kernel regression in high dimensions: Reﬁned anal-
ysis beyond double descent. In International Conference on Artiﬁcial Intelligence and Statistics,
pp. 649–657. PMLR, 2021."
REFERENCES,0.16847826086956522,"Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
Generalization error of random fea-
tures and kernel methods: hypercontractivity and kernel matrix concentration. arXiv preprint
arXiv:2101.10588, 2021a."
REFERENCES,0.1693840579710145,"Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random
features and kernel models. arXiv preprint arXiv:2102.13219, 2021b."
REFERENCES,0.17028985507246377,"Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292,
2019a."
REFERENCES,0.17119565217391305,"Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Benjamin Edelman, Tristan Yang, Boaz Barak,
and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances
in Neural Information Processing Systems, 32:3496–3506, 2019b."
REFERENCES,0.1721014492753623,"Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good
online learners are good ofﬂine generalizers. arXiv preprint arXiv:2010.08127, 2020."
REFERENCES,0.17300724637681159,"Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochastic gradi-
ent descent on hard learning problems through multiple passes. arXiv preprint arXiv:1805.10074,
2018."
REFERENCES,0.17391304347826086,"Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Early stopping and non-parametric regression:
an optimal data-dependent stopping rule. The Journal of Machine Learning Research, 15(1):
335–366, 2014."
REFERENCES,0.17481884057971014,"Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013."
REFERENCES,0.17572463768115942,"Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini.
Implicit regularization for optimal
sparse recovery. Advances in Neural Information Processing Systems, 32:2972–2983, 2019."
REFERENCES,0.1766304347826087,"Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019."
REFERENCES,0.17753623188405798,"Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learn-
ing. Constructive Approximation, 26(2):289–315, 2007."
REFERENCES,0.17844202898550723,Under review as a conference paper at ICLR 2022
REFERENCES,0.1793478260869565,"A
GENERAL SETTING"
REFERENCES,0.1802536231884058,"In this section we present our theory for training dynamics of kernel regression in an abstract setting
similar to that of Mei et al. (2021a). We ﬁrst introduce the setting of interest, then state the relevant
assumptions, and ﬁnally we provide our theoretical results. We provide proofs of these results in
Appendix B."
REFERENCES,0.18115942028985507,"A.1
PROBLEM SETUP"
REFERENCES,0.18206521739130435,"Consider a sequence of Polish probability spaces (Xd, νd), where νd is a probability measure on the
conﬁguration space Xd, indexed by an integer d. We denote by L2(Xd) = L2(Xd, νd) the space of
square integrable functions on (Xd, νd). For p ≥1, we denote ∥f∥Lp(Xd) = Ex∼νd[|f(x)p|]1/p the
Lp norm of f. Let Dd ⊆L2(Xd) be a closed linear subspace. In some simple applications we will
consider Dd = L2(Xd), but the extra generality will be useful in certain applications."
REFERENCES,0.18297101449275363,"We are concerned with a supervised learning problem where we are given i.i.d data (yi, xi)i≤n. The
feature vectors xi ∼iid νd are in Xd and the empirical-valued noisy responses yi are given by
yi = fd(xi) + εi
for some unknown target function fd ∈Dd and εi ∼iid N(0, σ2
ε)."
REFERENCES,0.1838768115942029,"We consider a general RKHS deﬁned on (Xd, νd) via the compact self-adjoint positive deﬁnite
operator Hd : Dd →Dd which admits the representation"
REFERENCES,0.18478260869565216,"Hdg(x) =
Z"
REFERENCES,0.18568840579710144,"Xd
Hd(x, x′)g(x′)νd(dx′),"
REFERENCES,0.18659420289855072,"where Hd ∈L2(Xd × Xd) with the property that
R"
REFERENCES,0.1875,"Xd Hd(x, x′)g(x′)νd(dx′) = 0 for g ∈D⊥
d ."
REFERENCES,0.18840579710144928,"By the spectral theorem of compact operators, there exists an orthonormal basis (ψj)j≥1,
span(ψj, j ≥1) = Dd ⊆L2(Xd) and empirical eigenvalues (λd,j)j≥1 with nonincreasing ab-
solute values |λd,1| ≥|λd,2| ≥· · · and P"
REFERENCES,0.18931159420289856,"j≥1 λ2
d,j < ∞such that"
REFERENCES,0.19021739130434784,"Hd(x1, x2) = ∞
X"
REFERENCES,0.1911231884057971,"j=1
λ2
d,jψj(x1)ψj(x2)"
REFERENCES,0.19202898550724637,where convergence holds in L2(Xd × Xd).
REFERENCES,0.19293478260869565,"For S ⊆{1, 2, . . .} we denote PS to be the projection operator from L2(Xd) onto Dd,S :=
span(ψj, j ∈S). We denote Hd,S to be the operator"
REFERENCES,0.19384057971014493,"Hd,S = ∞
X"
REFERENCES,0.1947463768115942,"j=1
λ2
d,jψjψ⋆
j"
REFERENCES,0.1956521739130435,"and Hd,S the corresponding kernel."
REFERENCES,0.19655797101449277,"If S = {j ∈N : j ≤ℓ} we will write as short-hand Hd,≤ℓand analogously for S = {j ∈N : j > ℓ}.
The trace of this operator is given by"
REFERENCES,0.19746376811594202,"Tr(Hd,S) ≡
X"
REFERENCES,0.1983695652173913,"j∈S
λ2
d,j = Ex∼νd[Hd,S(x, x)] < ∞."
REFERENCES,0.19927536231884058,Deﬁne the test error R : L2(Xd) →R and training error bRn : L2(Xd) →R
REFERENCES,0.20018115942028986,"R(f) ≡E(xnew,ynew){(ynew −f(xnew))2},
bRn(f) ≡1 n n
X"
REFERENCES,0.20108695652173914,"i=1
(yi −f(xi))2
(9)"
REFERENCES,0.20199275362318841,"where (x1, y1), . . . , (xn, yn), (xnew, ynew) are iid. For a kernel Hd ∈L2(Xd×Xd) we will consider
the oracle model f or
t
and the empirical model ˆft which satisfy the following gradient ﬂows
d
dtf or
t (x) = −∇R(f or
t (x)) = Ez∼νd[Hd(x, z)(fd(z) −f or
t (z))],
(10)"
REFERENCES,0.2028985507246377,"d
dt
ˆft(x) = −∇bRn( ˆft(x)) = 1 n n
X"
REFERENCES,0.20380434782608695,"i=1
Hd(x, xi)(yi −ˆft(xi)).
(11)"
REFERENCES,0.20471014492753623,Under review as a conference paper at ICLR 2022
REFERENCES,0.2056159420289855,"A.2
GENERAL ASSUMPTIONS"
REFERENCES,0.20652173913043478,"We now state our assumptions on the kernel and the sequence of probability spaces (Xd, νd).
Assumption 1 ({n(d), m(d)}d≥1-Kernel Concentration Property). We say that the sequence of op-
erators {Hd}d≥1 satisﬁes the Kernel Concentration Property (KCP) with respect to the sequence
{n(d), m(d)}d≥1 if there exists a sequence of integers {r(d)}d≥1 with r(d) ≥m(d), such that the
following conditions hold."
REFERENCES,0.20742753623188406,"(a) (Hypercontractivity of ﬁnite eigenspaces.) For any ﬁxed q ≥1, there exists a constant C
such that for any h ∈Dd,≤r(d) = span(ψs, 1 ≤s ≤r(d)), we have"
REFERENCES,0.20833333333333334,∥h∥L2q ≤C∥h∥L2.
REFERENCES,0.20923913043478262,"(b) (Properly decaying eigenvalues) There exists ﬁxed δ0 > 0, such that, for all d large enough,"
REFERENCES,0.21014492753623187,"n(d)2+δ0 ≤
(P∞
j=r(d)+1 λ4
d,j)2
P∞
j=r(d)+1 λ8
d,j
,"
REFERENCES,0.21105072463768115,"n(d)2+δ0 ≤
(P∞
j=r(d)+1 λ2
d,j)2
P∞
j=r(d)+1 λ4
d,j
."
REFERENCES,0.21195652173913043,"(c) (Concentration of diagonal elements of kernel) For (xi)i∈[n(d)] ∼iid νd, we have:"
REFERENCES,0.2128623188405797,"max
i∈[n(d)]"
REFERENCES,0.213768115942029,"Ex∼νd[Hd,>m(d)(xi, x)2] −Ex,x′∼νd[Hd,>m(d)(x, x′)2]
 = od,P(1) · Ex,x′∼νd[Hd,>m(d)(x, x′)2],"
REFERENCES,0.21467391304347827,"max
i∈[n(d)]"
REFERENCES,0.21557971014492755,"Hd,>m(d)(xi, xi) −Ex[Hd,>m(d)(x, x)]
 = od,P(1) · Ex[Hd,>m(d)(x, x)]."
REFERENCES,0.2164855072463768,"Assumption 1(a) can be interpreted as requiring that the top eigenfunctions of Hd are delocalized.
Assumption 1(b) concerns the tail of eigenvalues of Hd and is a mild assumption in high-dimensions.
Lastly, 1(c) essentially requires that “most points” in Xd behave similarly in the sense of having
similar values of the kernel diagonal Hd(x, x).
Assumption 2 (Eigenvalue condition at level {(n(d), m(d))}d≥1). We say that the sequence of
kernel operators {Hd}d≥1 satisﬁes the Eigenvalue Condition at level {(n(d), m(d))}d≥1 if the fol-
lowing conditions hold for all d large enough"
REFERENCES,0.21739130434782608,"(a) There exists a ﬁxed δ0 > 0, such that"
REFERENCES,0.21829710144927536,"n(d)1+δ0 ≤
1
λ4
d,m(d)+1 ∞
X"
REFERENCES,0.21920289855072464,"k=m(d)+1
λ4
d,k,
(12)"
REFERENCES,0.22010869565217392,"n(d)1+δ0 ≤
1
λ2
d,m(d)+1 ∞
X"
REFERENCES,0.2210144927536232,"k=m(d)+1
λ2
d,k.
(13)"
REFERENCES,0.22192028985507245,"(b) There exists a ﬁxed δ0 > 0, such that"
REFERENCES,0.22282608695652173,"n(d)1−δ0 ≥
1
λ2
d,m(d) ∞
X"
REFERENCES,0.223731884057971,"k=m(d)+1
λ2
d,k."
REFERENCES,0.2246376811594203,"(c) There exists a ﬁxed δ0 > 0, such that"
REFERENCES,0.22554347826086957,m(d) ≤n(d)1−δ0.
REFERENCES,0.22644927536231885,"Assumptions 2(a) and 2(b) can be seen as a spectral gap assumption. This ensures a clear separa-
tion between the eigenvalues in the subspace Dd,≤m(d) and the subspace Dd,>m(d). The technical
requirement 2(c) is mild."
REFERENCES,0.22735507246376813,"In the asymptotic setting, we will be interested in understanding the model learned at a time t = t(d)
which scales with the dimension. The next set of assumptions give requirements for a valid scaling."
REFERENCES,0.22826086956521738,Under review as a conference paper at ICLR 2022
REFERENCES,0.22916666666666666,"Assumption 3 (Admissible Time at {t(d), u(d), n(d), m(d)}d≥1). We say that the sequence
{t(d), u(d), n(d), m(d)}d≥1 is an Admissible Time for the sequence of kernel operators {Hd}d≥1 if
the following conditions hold"
REFERENCES,0.23007246376811594,"(a) There exists a ﬁxed δ0 > 0, such that for d large enough
1
λ2
d,u(d)
≤t(d)1−δ0 ≤t(d)1+δ0 ≤
1
λ2
d,u(d)+1
."
REFERENCES,0.23097826086956522,"(b) If u(d) < m(d) for inﬁnitely many d, then"
REFERENCES,0.2318840579710145,"t(d)
n(d) ∞
X"
REFERENCES,0.23278985507246377,"k=m(d)+1
λ2
d,k = od(1)."
REFERENCES,0.23369565217391305,(c) There exists a constant C such that
REFERENCES,0.2346014492753623,"m(d)
X"
REFERENCES,0.23550724637681159,"k=1
λ2
d,k ≤C ∞
X"
REFERENCES,0.23641304347826086,"k=m(d)+1
λ2
d,k."
REFERENCES,0.23731884057971014,"Assumption 3(a) is similar to the spectral gap condition Assumption 2(a), 2(b) for (t(d), u(d))d≥1.
Assumption 3(b) relates the ordering of the indices u(d), m(d) to the relative growth of t(d), n(d).
Assumption 3(c) requires that the eigenvalue tail does not decay too abruptly."
REFERENCES,0.23822463768115942,"A.3
MAIN RESULTS"
REFERENCES,0.2391304347826087,"In this section we give the main theoretical results. Recall the problem set-up and notation from
Appendix A.1. We will characterize the gradient ﬂow dynamics dynamics of the oracle model f or
t
Eq. (10) and the empirical model ˆft Eq. (11) for a general kernel Hd.
Theorem 3 (Oracle World). Let {fd ∈Dd}d≥1 be a sequence of functions and {Hd}d≥1 be a
sequence of kernel operators such that {(Hd, t(d), u(d))}d≥1 satisﬁes Assumption 3(a), then"
REFERENCES,0.24003623188405798,"R(f or
t ) =
P>u(d)fd
2
L2 + σ2
ε + od(1) · ∥fd∥2
L2 ,
f or
t −P≤u(d)fd
2
L2 = od(1) · ∥fd∥2
L2 ,"
REFERENCES,0.24094202898550723,"where P≤u(d) and P>u(d) are the projection operators onto the subspace spanned by the top u(d)
kernel eigenfunctions and the orthogonal complement respectively, as deﬁned in Appendix A.1."
REFERENCES,0.2418478260869565,"The error of the oracle model is determined solely by optimization time t through u(d). Due to the
spectral gap assumption 3(a) learning only occurs along the top u(d) eigenfunctions."
REFERENCES,0.2427536231884058,"The next results describe the empirical model. First we characterize the training error.
Theorem 4 (Empirical World - Train). Let {fd
∈
Dd}d≥1 be a sequence of functions,
(xi)i∈[n(d)] ∼νd independently, and {Hd}d≥1 be a sequence of kernel operators such that
{(Hd, n(d), m(d), t(d), u(d))}d≥1 satisﬁes {(n(d), m(d))}d≥1-KPCP (Assumption 1), eigenvalue
condition at level {(n(d), m(d))}d≥1 (Assumption 2), and {(n(d), m(d), t(d), u(d))}d≥1 is a valid
time (Assumption 3). Deﬁne κH = Tr
 
Hd,>m(d)

and ℓ(d) = min{u(d), m(d)}. Then for any
η > 0 we have,"
REFERENCES,0.24365942028985507,"bRn( ˆft) =
P>ℓ(d)fd
2
L2 + σ2
ε + od,P(1) · (∥fd∥2
L2+η + σ2
ε),
if t = od(n/κH),"
REFERENCES,0.24456521739130435,"bRn( ˆft) = od,P(1) · (∥fd∥2
L2+η + σ2
ε),
if t = ωd(n/κH)."
REFERENCES,0.24547101449275363,"In the early-time regime t ≪n/κH the training error may be non-zero and matches the oracle world
error if also u(d) ≤m(d). In the late-time regime t ≫n/κH the training error is negligible and the
model interpolates the training set. The quantity κH arises since the empirical kernel matrix can be
decomposed as H = H≤m + H>m and the second component is approximately a multiple of the
identity: H>m ≈Tr(H>m) · In = κH · In. This term acts as a self-induced ridge-regularizer."
REFERENCES,0.2463768115942029,Our ﬁnal result characterizes the test error of the empirical model.
REFERENCES,0.24728260869565216,Under review as a conference paper at ICLR 2022
REFERENCES,0.24818840579710144,"Theorem 5 (Empirical World - Test). Let {fd
∈
Dd}d≥1 be a sequence of functions,
(xi)i∈[n(d)] ∼νd independently, and {Hd}d≥1 be a sequence of kernel operators such that
{(Hd, n(d), m(d), t(d), u(d))}d≥1 satisﬁes {(n(d), m(d))}d≥1-KPCP (Assumption 1), eigenvalue
condition at level {(n(d), m(d))}d≥1 (Assumption 2), and {(n(d), m(d), t(d), u(d))}d≥1 is a valid
time (Assumption 3). Deﬁne ℓ(d) = min{u(d), m(d)}. Then for any η > 0 we have,"
REFERENCES,0.24909420289855072,"R( ˆft) =
P>ℓ(d)fd
2
L2 + σ2
ε + od,P(1) · (∥fd∥2
L2+η + σ2
ε),
 ˆft −P≤ℓ(d)fd

2"
REFERENCES,0.25,"L2 = od,P(1) · (∥fd∥2
L2+η + σ2
ε)."
REFERENCES,0.2509057971014493,"The result shows that the empirical model is essentially the projection of the regression function
onto the ﬁrst ℓ(d) eigenfunctions. The quantity ℓ(d) controls the complexity of the model which
increases with time t up until u(d) ≥m(d) after which the complexity is limited by n."
REFERENCES,0.25181159420289856,Under review as a conference paper at ICLR 2022
REFERENCES,0.25271739130434784,"B
PROOF OF GENERAL SETTING"
REFERENCES,0.2536231884057971,"B.1
ORACLE WORLD - PROOF OF THEOREM 3"
REFERENCES,0.2545289855072464,"The oracle model ODE Eq. (10) with initialization f or
0
≡0 can be solved (c.f. Appendix E.1) to
yield the solution
f or
t
= fd −e−tHdfd."
REFERENCES,0.2554347826086957,Therefore the excess risk is given by
REFERENCES,0.2563405797101449,"R(f or
t )−σ2
ε = Ex∼νd(fd(x)−f or
t (x))2 =
Z"
REFERENCES,0.2572463768115942,"Xd
fd(x)e−2tHdfd(x)νd(dx) = ∞
X"
REFERENCES,0.25815217391304346,"k=0
exp
 
−2tλ2
d,k
 ˆf 2
k."
REFERENCES,0.25905797101449274,"where λ2
d,k are the kernel eigenvalues and ˆfk := ⟨fd, ψk⟩L2 are the Fourier coefﬁcients of fd in the
kernel eigenbasis (c.f. Appendix A.1). We can control this quantity as follows,"
REFERENCES,0.259963768115942,"|R(f or
t ) −σ2
ε −∥P>ufd∥2
L2 |/ ∥fd∥2
L2 ≤max

max
k≤u exp
 
−2tλ2
d,k

, max
k≥u+1 1 −exp
 
−2tλ2
d,k
"
REFERENCES,0.2608695652173913,"≤max

max
k≤u exp
 
−Ω(tλ2
d,k)

, max
k≥u+1 O(tλ2
d,k)
"
REFERENCES,0.2617753623188406,"≤max

exp
 
−Ω(tδ0)

, O(t−δ0)
	
= od(1)"
REFERENCES,0.26268115942028986,"where the last inequality follows from Assumption 3(a). This shows the ﬁrst theorem statement,"
REFERENCES,0.26358695652173914,"∥f or
t −P≤ufd∥2
L2 = od(1) · ∥fd∥2
L2 ."
REFERENCES,0.2644927536231884,Now observe that
REFERENCES,0.2653985507246377,"P≤ufd −f or
t
= u
X"
REFERENCES,0.266304347826087,"k=0
e−tλ2
d,k ˆfkψk −
X"
REFERENCES,0.26721014492753625,"k≥u+1
(1 −e−tλ2
d,k) ˆfkψk hence"
REFERENCES,0.26811594202898553,"∥f or
t −P≤ufd∥2
L2 / ∥fd∥2
L2 ≤max

max
k≤u exp
 
−2tλ2
d,k

, max
k≥u+1(1 −e−tλ2
d,k)2
"
REFERENCES,0.26902173913043476,"≤max

max
k≤u exp
 
−2tλ2
d,k

, max
k≥u+1 1 −exp
 
−2tλ2
d,k

= od(1)"
REFERENCES,0.26992753623188404,"where the second inequality follows from the fact that (1 −e−x)2 ≤1 −e−2x and the ﬁnal equality
is from the proof of the ﬁrst part of the theorem. Thus,"
REFERENCES,0.2708333333333333,"∥f or
t −P≤ufd∥2
L2 = od(1) · ∥fd∥2
L2"
REFERENCES,0.2717391304347826,completing the proof.
REFERENCES,0.2726449275362319,"B.2
EMPIRICAL WORLD – PRELIMINARIES"
REFERENCES,0.27355072463768115,We will introduce some useful notations and the objects of analysis for studying the empirical world.
REFERENCES,0.27445652173913043,"B.2.1
TRAINING DYNAMICS"
REFERENCES,0.2753623188405797,"For the training of the empirical world c.f. Eq. (11), if u(t) = ( ˆft(x1), . . . , ˆft(xn)) ∈Rn then from
Eq. (44) if u(0) = 0 then"
REFERENCES,0.276268115942029,"u(t) = y −e−tH/ny = (In −e−tH/n)y,
(14)"
REFERENCES,0.27717391304347827,"where H ∈Rn×n with the (i, j)th element given by Hij = Hd(xi, xj) and y = f + ε ∈Rn with
f = (fd(x1), . . . , fd(xn))T ∈Rn and ε = (ε1, . . . , εn)T ∈Rn."
REFERENCES,0.27807971014492755,Under review as a conference paper at ICLR 2022
REFERENCES,0.27898550724637683,"For x ∈Rd, deﬁne h(x) = (Hd(x1, x), . . . , Hd(xn, x))T ∈Rn. The training and test errors as
deﬁned in Eq. (1) can be written as"
REFERENCES,0.2798913043478261,bRn( ˆft) = 1
REFERENCES,0.2807971014492754,"n∥u(t) −y∥2
2 = 1"
REFERENCES,0.2817028985507246,"nyTe−2tH/ny,"
REFERENCES,0.2826086956521739,"R( ˆft) = Ex

(fd(x) −u(t)TH−1h(x))2
."
REFERENCES,0.28351449275362317,Expanding R( ˆft) yields
REFERENCES,0.28442028985507245,"R( ˆft) = Ex[fd(x)2] −2u(t)TH−1E + u(t)TH−1MH−1u(t)
(15)"
REFERENCES,0.28532608695652173,"where E = (E1, . . . , En)T ∈Rn, M = (Mij)i,j∈[n] ∈Rn×n, and H = (Hij)i,j∈[n] ∈Rn×n
with"
REFERENCES,0.286231884057971,"Ei = Ex[fd(x)Hd(x, xi)],
Mij = Ex[Hd(x, xi)Hd(x, xj)],
Hij = Hd(xi, xj)."
REFERENCES,0.2871376811594203,"B.2.2
DECOMPOSITIONS AND NOTATIONS"
REFERENCES,0.28804347826086957,"In this section we recall some useful decompositions of empirical quantities from Mei et al. (2021a).
As mentioned earlier the eigendecomposition of Hd is given by"
REFERENCES,0.28894927536231885,"Hd(x, y) = ∞
X"
REFERENCES,0.2898550724637681,"k=1
λ2
d,kψk(x)ψk(y)."
REFERENCES,0.2907608695652174,We write the orthogonal decomposition of fd in the basis {ψk}k≥1 as
REFERENCES,0.2916666666666667,"fd(x) = ∞
X"
REFERENCES,0.29257246376811596,"k=1
ˆfd,kψk(x). Deﬁne"
REFERENCES,0.29347826086956524,"ψk = (ψk(x1), . . . , ψk(xn))T ∈Rn,"
REFERENCES,0.29438405797101447,"D≤m = diag(λd,1, λd,2, . . . , λd,m) ∈Rm×m,"
REFERENCES,0.29528985507246375,"Ψ≤m = (ψk(xi))i∈[n],k∈[m] ∈Rn×m,
b
f≤m = ( ˆfd,1, ˆfd,2, . . . , ˆfd,m)T ∈Rm."
REFERENCES,0.296195652173913,"We have the following orthogonal basis decompositions of f, H, E and M"
REFERENCES,0.2971014492753623,"f = f≤m + f>m,
f≤m = Ψ≤m b
f≤m,
f>m = ∞
X"
REFERENCES,0.2980072463768116,"k=m+1
ˆfd,kψk,"
REFERENCES,0.29891304347826086,"H = H≤m + H>m,
H≤m = Ψ≤mD2
≤mΨT
≤m,
H>m = ∞
X"
REFERENCES,0.29981884057971014,"k=m+1
λ2
d,kψkψT
k ,"
REFERENCES,0.3007246376811594,"E = E≤m + E>m,
E≤m = Ψ≤mD2
≤m b
f≤m,
E>m = ∞
X"
REFERENCES,0.3016304347826087,"k=m+1
λ2
d,k ˆfd,kψk,"
REFERENCES,0.302536231884058,"M = M≤m + M>m,
M≤m = Ψ≤mD4
≤mΨT
≤m,
M>m = ∞
X"
REFERENCES,0.30344202898550726,"k=m+1
λ4
d,kψkψT
k . (16)"
REFERENCES,0.30434782608695654,"By Lemma 6 below, under Assumptions 1 and 2(a) the matrices H and M can be written as"
REFERENCES,0.3052536231884058,"H = Ψ≤mD2
≤mΨT
≤m + κH(In + ∆H),
(17)"
REFERENCES,0.3061594202898551,"M = Ψ≤mD4
≤mΨT
≤m + κM(In + ∆M),
(18)"
REFERENCES,0.3070652173913043,Under review as a conference paper at ICLR 2022 where
REFERENCES,0.3079710144927536,"κH = Tr(Hd,>m) = ∞
X"
REFERENCES,0.3088768115942029,"k≥m+1
λ2
d,k,"
REFERENCES,0.30978260869565216,"κM = Tr
 
H2
d,>m

= ∞
X"
REFERENCES,0.31068840579710144,"k≥m+1
λ4
d,k,"
REFERENCES,0.3115942028985507,"and
max{∥∆H∥op , ∥∆M∥op} = od,P(1)."
REFERENCES,0.3125,We will use α as shorthand for the scalar valued dimension dependent quantity e−(t/n)κH and take
REFERENCES,0.3134057971014493,"K≤m := In −αe−(t/n)Ψ≤mD2
≤mΨT
≤m.
(19)"
REFERENCES,0.31431159420289856,We also introduce the shrinkage matrix deﬁned as
REFERENCES,0.31521739130434784,"S≤m =

Im + κH"
REFERENCES,0.3161231884057971,"n D−2
≤m
−1
= diag((sj)j∈[m]) ∈Rm×m,
where sj =
λ2
d,j
λ2
d,j + κH"
REFERENCES,0.3170289855072464,"n
.
(20)"
REFERENCES,0.3179347826086957,"If unspeciﬁed we will typically use ∆, ∆′, etc. to denote matrices with operator norm od,P(1). For
positive integers ℓ< m, deﬁne [ℓ, m] = {ℓ+ 1, . . . , m}. We will use the following notation"
REFERENCES,0.3188405797101449,"Sℓm = diag((sj)j∈[ℓ,m])
for sj deﬁned in Eq. (20)"
REFERENCES,0.3197463768115942,"Ψℓm = (ψk(xi))i∈[n],k∈[ℓ,m] ∈Rn×(m−ℓ)"
REFERENCES,0.32065217391304346,"fℓm =
X"
REFERENCES,0.32155797101449274,"k∈[ℓ,m]"
REFERENCES,0.322463768115942,"ˆfd,kψk"
REFERENCES,0.3233695652173913,"B.2.3
AUXILIARY LEMMAS"
REFERENCES,0.3242753623188406,"Here we collect some lemmas which will be of use to us.
Lemma 1 (Matrix Exponential Perturbation Inequality). For matrix operator norm ∥· ∥, if A, B ∈
Rn×n are symmetric then
eA −eB ≤∥A −B∥max{
eA,
eB}."
REFERENCES,0.32518115942028986,"For general square matrices A, B ∈Rn×n we have
eA −eB ≤∥A −B∥e∥A∥e∥B∥."
REFERENCES,0.32608695652173914,"Lemma 2. Let Z ∈Rm×n, Ψ ∈Rm×p, D ∈Rp×p and t ∈R. Denote A = ZTΨ ∈Rn×p and
B = ΨTΨ ∈Rp×p. Then,
ZTetΨDΨTΨ = AetDB."
REFERENCES,0.3269927536231884,"The notations in Lemmas 3-8 all follow the notations given in Section B.2.2.
Lemma 3 (Lemma 12 from Mei et al. (2021a) with λ = 0). Let Assumptions 1 and 2 hold. Then,
nH−1MH−1 −Ψ≤mS2
≤mΨT
≤m/n

op = od,P(1)."
REFERENCES,0.3278985507246377,"Lemma 4 (Theorem 6(b) from Mei et al. (2021a)). Let Assumptions 1(a), 2(c) hold. Then,
ΨT
≤mΨ≤m/n −Im

op = od,P(1)."
REFERENCES,0.328804347826087,"Lemma 5 (Lemma 13 from Mei et al. (2021a) with λ = 0). Let Assumptions 1 and 2 hold. Then,
ΨT
≤mH−1Ψ≤mD2
≤m −S≤m

op = od,P(1)."
REFERENCES,0.32971014492753625,"Lemma 6 (Theorem 6 from Mei et al. (2021a)). Let Assumptions 1 and 2(a) hold. Then we can
decompose the kernel matrices as follows"
REFERENCES,0.33061594202898553,"H = Ψ≤mD2
≤mΨT
≤m + κH(In + ∆H),"
REFERENCES,0.33152173913043476,"M = Ψ≤mD4
≤mΨT
≤m + κM(In + ∆M),"
REFERENCES,0.33242753623188404,Under review as a conference paper at ICLR 2022 where
REFERENCES,0.3333333333333333,"κH = Tr(Hd,>m) = ∞
X"
REFERENCES,0.3342391304347826,"k≥m+1
λ2
d,k,"
REFERENCES,0.3351449275362319,"κM = Tr
 
H2
d,>m

= ∞
X"
REFERENCES,0.33605072463768115,"k≥m+1
λ4
d,k,"
REFERENCES,0.33695652173913043,"and
max{∥∆H∥op , ∥∆M∥op} = od,P(1)."
REFERENCES,0.3378623188405797,"Lemma 7. Let Assumption 1(a) hold. Let S, T be disjoint subsets of N. Let D ∈R|S|×|S| be a
diagonal matrix. Then we have that for any η > 0, there exists C(η) (independent of d), such that"
REFERENCES,0.338768115942029,"E[ b
f T
T ΨT
T ΨSDΨT
SΨT b
fT ]/n2 ≤C(η) ∥PT fd∥2
L2+η Tr(D)/n,"
REFERENCES,0.33967391304347827,"where the expectation is with respect to the randomness in ΨS, ΨT ."
REFERENCES,0.34057971014492755,"Proof. Let ι : S →[|S|] be the bijection such that ΨS = (ψι−1(k)(xi))i∈[n],k∈[|S|] then we have"
REFERENCES,0.34148550724637683,"E[ b
f T
T ΨT
T ΨSDΨT
SΨT b
fT ]/n2 =
X u,v∈T X s∈S X"
REFERENCES,0.3423913043478261,"i,j∈[n]
Dι(s)ι(s)E[ψu(xi)ψs(xi)ψs(xj)ψv(xj)] ˆfu ˆfv/n2 =
X u,v∈T X s∈S X"
REFERENCES,0.3432971014492754,"i∈[n]
Dι(s)ι(s)E[ψu(xi)ψs(xi)ψs(xi)ψv(xi)] ˆfu ˆfv/n2 = 1 n X"
REFERENCES,0.3442028985507246,"s∈S
Dι(s)ι(s)Ex[(PT fd(x))2ψs(x)2] ≤1 n X"
REFERENCES,0.3451086956521739,"s∈S
Dι(s)ι(s) ∥PT fd∥2
L2+η ∥ψs∥2
L(4+2η)/η"
REFERENCES,0.34601449275362317,"≤C(η) ∥PT fd∥2
L2+η n
X"
REFERENCES,0.34692028985507245,"i=1
Dii/n,"
REFERENCES,0.34782608695652173,"where the second to last inequality is by Holder’s inequality and the last inequality used the hyper-
contractivity assumption as in Assumption 1(a)."
REFERENCES,0.348731884057971,"Lemma 8 (Exponential Kernel Decomposition). Assume the conditions of Lemma 6 hold and as-
sume there exists δ0 > 0, such that (t(d), u(d)) satisﬁes the condition"
REFERENCES,0.3496376811594203,"t(d)1+δ0 ≤
1
λ2
d,u(d)+1
.
(21)"
REFERENCES,0.35054347826086957,"Let j(d) satisfy min{u(d), m(d)} ≤j(d) ≤m(d) then
e−tHd/n −e−(t/n)(Ψ≤jD2
≤jΨT
≤j+κHIn)
op = od,P(1)."
REFERENCES,0.35144927536231885,"Proof. First consider the regime t = Od(n/κH). Recall the decomposition,"
REFERENCES,0.3523550724637681,"H = Ψ≤mD2
≤mΨT
≤m + κH(In + ∆H)"
REFERENCES,0.3532608695652174,"where ∥∆H∥op = od,P(1). By Lemma 1 and Lemma 4,"
REFERENCES,0.3541666666666667,"e−tH/n −e−(t/n)(Ψ≤jD2
≤jΨT
≤j+κHI)
op ≤  m
X"
REFERENCES,0.35507246376811596,"k≥j+1
tλ2
d,kψkψT
k /n op"
REFERENCES,0.35597826086956524,+ (t/n)κH ∥∆h∥op
REFERENCES,0.35688405797101447,"≤(t · max
k≥j+1 ·λ2
d,k)
ΨT
≤mΨ≤m/n

op + od,P(1)"
REFERENCES,0.35778985507246375,"≤Od,P(t · max
k≥j+1 λ2
d,k) + od,P(1)"
REFERENCES,0.358695652173913,"(a)
= Od,P(t−δ0) + od,P(1) = od,P(1),"
REFERENCES,0.3596014492753623,Under review as a conference paper at ICLR 2022
REFERENCES,0.3605072463768116,"where equality (a) holds by assumption Eq. (21). Now if t = ωd(n/κH), then it is easy to see that"
REFERENCES,0.36141304347826086,"max
e−tH/n
op ,
e−(t/n)(Ψ≤jD2
≤jΨT
≤j+κHI)
op"
REFERENCES,0.36231884057971014,"
≤e−ωd,P(1) = od,P(1),"
REFERENCES,0.3632246376811594,"and therefore
e−tH/n −e−(t/n)(Ψ≤jD2
≤jΨT
≤j+κHI)
op"
REFERENCES,0.3641304347826087,"≤2 max
e−tH/n
op ,
e−(t/n)(Ψ≤jD2
≤jΨT
≤j+κHI)
op"
REFERENCES,0.365036231884058,"
= od,P(1)."
REFERENCES,0.36594202898550726,"B.3
EMPIRICAL WORLD - TRAIN"
REFERENCES,0.36684782608695654,"If t = ωd(n/κH), then by Eq. (17) it is easy to see that
e−2(t/n)H
op = od,P(1), hence"
REFERENCES,0.3677536231884058,"bRn( ˆft) = od,P(1) · ∥fd∥2
L2 ."
REFERENCES,0.3686594202898551,"From now on we focus on the case that t = od(n/κH). Let us decompose the training error as
follows
bRn( ˆft) = R1 + R2 + R3
where"
REFERENCES,0.3695652173913043,R1 = 1
REFERENCES,0.3704710144927536,"nf Te−2(t/n)Hf,"
REFERENCES,0.3713768115942029,R2 = 2
REFERENCES,0.37228260869565216,"nεTe−2(t/n)Hf,"
REFERENCES,0.37318840579710144,R3 = 1
REFERENCES,0.3740942028985507,nεTe−2(t/n)Hε.
REFERENCES,0.375,"B.3.1
TERM R1"
REFERENCES,0.3759057971014493,Let us start by analysing R1. We can write
REFERENCES,0.37681159420289856,R1 = 1
REFERENCES,0.37771739130434784,n(f≤ℓ+ f>ℓ)Te−2(t/n)H(f≤ℓ+ f>ℓ)
REFERENCES,0.3786231884057971,= T1 + 2T2 + T3 where
REFERENCES,0.3795289855072464,T1 = 1
REFERENCES,0.3804347826086957,"nf T
≤ℓe−2(t/n)Hf≤ℓ,"
REFERENCES,0.3813405797101449,T2 = 1
REFERENCES,0.3822463768115942,"nf T
>ℓe−2(t/n)Hf≤ℓ,"
REFERENCES,0.38315217391304346,T3 = 1
REFERENCES,0.38405797101449274,"nf T
>ℓe−2(t/n)Hf>ℓ."
REFERENCES,0.384963768115942,"Let us analyse the term T1, T1 ≤1"
REFERENCES,0.3858695652173913,"nf T
≤ℓe−2(t/n)Ψ≤ℓD2
≤ℓΨT
≤ℓf≤ℓ."
REFERENCES,0.3867753623188406,"By Lemma 2, denoting B = ΨT
≤ℓΨ≤ℓ/n,"
REFERENCES,0.38768115942028986,"1
nΨT
≤ℓe−2(t/n)Ψ≤ℓD2
≤ℓΨT
≤ℓΨ≤ℓ= Be−2tD2
≤ℓB."
REFERENCES,0.38858695652173914,"We will show that
Be−2tD2
≤ℓB
op = od,P(1).
(22)"
REFERENCES,0.3894927536231884,Under review as a conference paper at ICLR 2022
REFERENCES,0.3903985507246377,"By Lemma 4, since ∥B∥op = Θd,P(1), for d large B has a positive-deﬁnite square root B1/2 w.h.p.
Therefore we can write"
REFERENCES,0.391304347826087,"Be−2tD2
≤ℓB = B1/2e−2tB1/2D2
≤ℓB1/2B1/2"
REFERENCES,0.39221014492753625,"and bound the operator norm
Be−2tD2
≤ℓB
op ≤∥B∥op
e−2tB1/2D2
≤ℓB1/2
op"
REFERENCES,0.39311594202898553,"= ∥B∥op e−2tλmin(B1/2D2
≤ℓB1/2)"
REFERENCES,0.39402173913043476,"≤∥B∥op e−2tλmax(B)λmin(D2
≤ℓ) = od,P(1)."
REFERENCES,0.39492753623188404,"since by Assumption 3(a), tλmin(D2
≤ℓ) = Ω(tδ0). Therefore T1 = od,P(1) · ∥P≤ℓfd∥2
L2."
REFERENCES,0.3958333333333333,"Now we analyse the term T3. Observe that by the inequality 1 −x ≤e−x ≤1,
1
n∥f>ℓ∥2
2 −(2t/n) 1"
REFERENCES,0.3967391304347826,"nf T
>ℓ(Ψ≤ℓD2
≤ℓΨT
≤ℓ+ κH(In + ∆H))f>ℓ≤T3 ≤1"
REFERENCES,0.3976449275362319,"n∥f>ℓ∥2
2."
REFERENCES,0.39855072463768115,"We have by Lemma 7,"
REFERENCES,0.39945652173913043,"tE[ b
f T
>ℓΨT
>ℓΨ≤ℓD2
≤ℓΨT
≤ℓΨ>ℓb
f>ℓ]/n2 ≤C(η) ∥P>ℓfd∥2
L2+η t
n ℓ
X"
REFERENCES,0.4003623188405797,"s=1
λ2
d,s !"
REFERENCES,0.401268115942029,"where the last quantity is od,P(1) · ∥P>ℓfd∥2
L2+η by Assumption 3(c). Thus we see that"
REFERENCES,0.40217391304347827,"T3 = ∥P>ℓfd∥2
L2 + od,P(1) · ∥P>ℓfd∥2
L2+η ."
REFERENCES,0.40307971014492755,"Observe that by the Cauchy-Schwarz inequality,"
REFERENCES,0.40398550724637683,"T2 ≤(T1T3)1/2 ≤od,P(1) ∥P≤ℓfd∥L2 ∥P>ℓfd∥L2+η ."
REFERENCES,0.4048913043478261,"Putting everything together we see that,"
REFERENCES,0.4057971014492754,"R1 = ∥P>ℓfd∥2
L2 + od,P(1) · (∥fd∥2
L2 + ∥P>ℓfd∥2
L2+η)."
REFERENCES,0.4067028985507246,"B.3.2
TERM R2"
REFERENCES,0.4076086956521739,"Turning to R2, we take the second-moment with respect to ε
1
σ2ε
Eε[R2
2] = 4"
REFERENCES,0.40851449275362317,"n2 Eε[εTe−2(t/n)Hff Te−2(t/n)Hε]/σ2
ε = 4"
REFERENCES,0.40942028985507245,n2 f Te−4(t/n)Hf ≤4
REFERENCES,0.41032608695652173,"n(∥f∥2
2/n)"
REFERENCES,0.411231884057971,"= od,P(1) · ∥fd∥2
L2 ."
REFERENCES,0.4121376811594203,"By Markov’s inequality R2 = od,P(1) · ∥fd∥2
L2 σ2
ε."
REFERENCES,0.41304347826086957,"B.3.3
TERM R3"
REFERENCES,0.41394927536231885,"Now let us analyse R3. Recalling the deﬁnition B = ΨT
≤ℓΨ≤ℓ/n, we can compute the expectation"
REFERENCES,0.4148550724637681,"1
σ2ε
(1 −Eε[R3]) = 1"
REFERENCES,0.4157608695652174,"n Tr

In −e−2(t/n)H"
REFERENCES,0.4166666666666667,"(a)
= 1"
REFERENCES,0.41757246376811596,"ne−κHt/n Tr

In −e−2(t/n)(Ψ≤ℓD2
≤ℓΨT
≤ℓ)
+ od,P(1)"
REFERENCES,0.41847826086956524,"(b)
≤1"
REFERENCES,0.41938405797101447,"ne−κHt/n Tr
 
2(t/n)(Ψ≤ℓD2
≤ℓΨT
≤ℓ)

+ od,P(1)"
REFERENCES,0.42028985507246375,"(c)
= 1"
REFERENCES,0.421195652173913,"ne−κHt/n Tr
 
2tBD2
≤ℓ

+ od,P(1) ≤2t"
REFERENCES,0.4221014492753623,"n e−κHt/n ∥B∥op Tr
 
D2
≤ℓ

+ od,P(1) = od,P(1)."
REFERENCES,0.4230072463768116,Under review as a conference paper at ICLR 2022
REFERENCES,0.42391304347826086,"Equality (a) follows from Lemma 8. For (b) we used the inequality 1−e−x ≤x and that trace is the
sum of eigenvalues. Equality (c) uses the cyclic property of trace. In the last equality we used that by
Lemma 4, ∥B∥op = Od,P(1) and Assumption 3(c) implies that (t/n) Tr
 
D2
≤ℓ

= Od((t/n)κH) =
od(1) since by assumption we consider t = od(n/κH). Turning to the variance"
REFERENCES,0.42481884057971014,"Varε[R3] = Eε[R2
3] −Eε[R3]2 = 1"
REFERENCES,0.4257246376811594,n2 Eε[(εTe−2(t/n)Hε)2] −Eε[R3]2 ≤1
REFERENCES,0.4266304347826087,"n2 Eε[(εTε)2] −σ4
ε(1 + od,P(1))"
REFERENCES,0.427536231884058,"(a)
= O(1/n) · σ4
ε + σ4
ε −σ4
ε(1 + od,P(1))"
REFERENCES,0.42844202898550726,"= od,P(1) · σ4
ε,"
REFERENCES,0.42934782608695654,"where the ﬁrst inequality uses that
e−2(t/n)H
op ≤1 and (a) uses that for εi ∼N(0, σ2
ε), E[ε4
i ] =
3σ4
ε. Therefore by Chebyshev’s inequality, R3 = σ2
ε(1 + od,P(1))."
REFERENCES,0.4302536231884058,Putting everything together yields
REFERENCES,0.4311594202898551,"bRn( ˆft) = R1 + R2 + R3 = ∥P>ℓ∥2
L2 + od,P(1) · (∥fd∥2
L2+η + σ2
ε)."
REFERENCES,0.4320652173913043,"B.4
EMPIRICAL WORLD - TEST"
REFERENCES,0.4329710144927536,"Recalling u(t) from Eq. (14), let
u(t) = v(t) + ε(t) where"
REFERENCES,0.4338768115942029,"v(t) = (In −e−tH/n)f,
(23)"
REFERENCES,0.43478260869565216,"ε(t) = (In −e−tH/n)ε.
(24)"
REFERENCES,0.43568840579710144,"Recall the expansion of the test error from Eq. (15),"
REFERENCES,0.4365942028985507,R( ˆft) = Ex[fd(x)2] −2u(t)TH−1E + u(t)TH−1MH−1u(t)
REFERENCES,0.4375,"= ∥fd∥2
L2 −2T1 + T2 + T3 −2T4 + 2T5, where"
REFERENCES,0.4384057971014493,"T1 = v(t)TH−1E,"
REFERENCES,0.43931159420289856,"T2 = v(t)TH−1MH−1v(t),"
REFERENCES,0.44021739130434784,"T3 = ε(t)TH−1MH−1ε(t),"
REFERENCES,0.4411231884057971,"T4 = ε(t)TH−1E,"
REFERENCES,0.4420289855072464,T5 = ε(t)TH−1MH−1v(t).
REFERENCES,0.4429347826086957,"The proof for the test error is the most involved, but will follow a similar strategy of analysing each
term in the expansion. We ﬁrst begin by analysing T2 in Section B.4.1, then T1 in Section B.4.2,
and ﬁnally terms T3, T4, T5, which are all simpler than the ﬁrst two, in Section B.4.3. At the end of
this Appendix section we present the proof of Theorem 5."
REFERENCES,0.4438405797101449,"B.4.1
TERM T2"
REFERENCES,0.4447463768115942,"As before, we will analyze each term separately. We begin with term T2."
REFERENCES,0.44565217391304346,Proposition 1 (Term T2).
REFERENCES,0.44655797101449274,"T2 = v(t)TH−1MH−1v(t) =
S≤ℓb
f≤ℓ

2"
REFERENCES,0.447463768115942,"2 + od,P(1) · ∥fd∥2
L2+η"
REFERENCES,0.4483695652173913,where we recall the shrinkage matrix S≤m deﬁned in Eq. (20) and v(t) in Eq. (23).
REFERENCES,0.4492753623188406,Under review as a conference paper at ICLR 2022
REFERENCES,0.45018115942028986,Proof of Proposition 1. To analyze T2 we further decompose it into the following terms
REFERENCES,0.45108695652173914,"T2 = (f≤ℓ+ f>ℓ)T(In −e−tH/n)H−1MH−1(In −e−tH/n)(f≤ℓ+ f>ℓ)
= T21 + T22 + T23, where"
REFERENCES,0.4519927536231884,"T21 = f T
≤ℓ(In −e−tH/n)H−1MH−1(In −e−tH/n)f≤ℓ,
(25)"
REFERENCES,0.4528985507246377,"T22 = 2f T
≤ℓ(In −e−tH/n)H−1MH−1(In −e−tH/n)f>ℓ,
(26)"
REFERENCES,0.453804347826087,"T23 = f T
>ℓ(In −e−tH/n)H−1MH−1(In −e−tH/n)f>ℓ.
(27)"
REFERENCES,0.45471014492753625,"Using Lemma 9 and 10 proven below, by the Cauchy-Schwarz inequality,"
REFERENCES,0.45561594202898553,"T22 ≤(2T21T23)1/2 = od,P(1) · ∥P≤ℓfd∥L2 ∥fd∥L2+η
and hence
T2 = T21 + T22 + T23 =
S≤ℓb
f≤ℓ

2"
REFERENCES,0.45652173913043476,"2 + od,P(1) · ∥fd∥2
L2+η ."
REFERENCES,0.45742753623188404,Lemma 9 (Term T21 Eq. (25)).
REFERENCES,0.4583333333333333,"T21 =
S≤ℓb
f≤ℓ

2"
REFERENCES,0.4592391304347826,"2 + od,P(1) · ∥P≤ℓfd∥2
L2"
REFERENCES,0.4601449275362319,Proof of Lemma 9. Recall the notation α and K≤ℓfrom Eq. (19). Deﬁne ∆such that
REFERENCES,0.46105072463768115,"∆:= (In −e−tH/n) −K≤ℓ
which by Lemma 8 satisﬁes ∥∆∥op = od,P(1). Then we can split T21 into"
REFERENCES,0.46195652173913043,"T21 = T211 + T212 + T213 + T214
where"
REFERENCES,0.4628623188405797,"T211 = f T
≤ℓK≤ℓΨ≤ℓS2
≤ℓΨT
≤ℓK≤ℓf≤ℓ/n2,"
REFERENCES,0.463768115942029,"T212 = f T
≤ℓK≤ℓΨℓmS2
ℓmΨT
ℓmK≤ℓf≤ℓ/n2,"
REFERENCES,0.46467391304347827,"T213 = f T
≤ℓK≤ℓ(H−1MH−1 −Ψ≤mS2
≤mΨT
≤m/n2)K≤ℓf≤ℓ,"
REFERENCES,0.46557971014492755,"T214 = f T
≤ℓ∆H−1MH−1K≤ℓf≤ℓ+ f T
≤ℓK≤ℓH−1MH−1∆f≤ℓ+ f T
≤ℓ∆H−1MH−1∆f≤ℓ."
REFERENCES,0.46648550724637683,"We will show that the dominant term is T211 and the others are of lower order. By Lemma 3,
nH−1MH−1 −Ψ≤mS2
≤mΨT
≤m/n

op = od,P(1),"
REFERENCES,0.4673913043478261,"and since K≤ℓ⪯In, T213 = od,P(1) · ∥P≤ℓfd∥2
L2. By Lemma 3, Lemma 4 and since S≤m ⪯Im,
nH−1MH−1
op ≤
Ψ≤mS2
≤mΨT
≤m/n

op + od,P(1) ≤1 + od,P(1)"
REFERENCES,0.4682971014492754,"hence it is easy to see that T214 = od,P(1) · ∥P≤ℓfd∥2
L2. Turning to T212 we have"
REFERENCES,0.4692028985507246,T212 ≤1
REFERENCES,0.4701086956521739,"n2 f T
≤ℓ(In −αe−(t/n)Ψ≤ℓD2
≤ℓΨT
≤ℓ)ΨℓmΨT
ℓm(In −αe−(t/n)Ψ≤ℓD2
≤ℓΨT
≤ℓ)f≤ℓ"
REFERENCES,0.47101449275362317,"(a)
=
1
n2 b
f T
≤ℓ(I −αe−(t/n)BD2
≤ℓ)ΨT
≤ℓΨℓmΨT
ℓmΨ≤ℓ(I −αe−(t/n)BD2
≤ℓ) b
f T
≤ℓ"
REFERENCES,0.47192028985507245,"≤
 b
f≤ℓ

2 2"
REFERENCES,0.47282608695652173,"ΨT
≤ℓΨℓm/n
2"
REFERENCES,0.473731884057971,"op = od,P(1) · ∥P≤ℓfd∥2
L2 ,"
REFERENCES,0.4746376811594203,"where the ﬁrst inequality used S2
ℓm ⪯Im−ℓ, we used Lemma 2 in (a), and the last equality used
Lemma 4 (note that ΨT
≤ℓΨℓm/n is an off-diagonal block of ΨT
≤mΨ≤m/n which corresponds to an
all zeroes submatrix of Im). Finally we look at the main term T211. Deﬁning the matrices"
REFERENCES,0.47554347826086957,A := 1
REFERENCES,0.47644927536231885,"nΨT
≤ℓe−(t/n)Ψ≤ℓD2
≤ℓΨT
≤ℓΨ≤ℓ,"
REFERENCES,0.4773550724637681,B := 1
REFERENCES,0.4782608695652174,"nΨT
≤ℓΨ≤ℓ,"
REFERENCES,0.4791666666666667,Under review as a conference paper at ICLR 2022
REFERENCES,0.48007246376811596,we can write
REFERENCES,0.48097826086956524,T211 = 1
REFERENCES,0.48188405797101447,"n2 b
f T
≤ℓΨT
≤ℓ(In −αe−(t/n)Ψ≤ℓD2
≤ℓΨT
≤ℓ)Ψ≤ℓS2
≤ℓΨT
≤ℓ(In −αe−(t/n)Ψ≤ℓD2
≤ℓΨT
≤ℓ)Ψ≤ℓb
f≤ℓ"
REFERENCES,0.48278985507246375,"= b
f T
≤ℓ(BS2
≤ℓB −2αAS2
≤ℓB + α2AS2
≤ℓA) b
f≤ℓ."
REFERENCES,0.483695652173913,"Now observe that by Lemma 2, A = 1"
REFERENCES,0.4846014492753623,"nΨT
≤ℓe−(t/n)Ψ≤ℓD2
≤ℓΨT
≤ℓΨ≤ℓ= Be−tD2
≤ℓB"
REFERENCES,0.4855072463768116,"and by the same argument used to show Eq. (22), ∥A∥op = od,P(1). Since B = In + ∆′ and
S≤ℓ⪯Iℓ, we have"
REFERENCES,0.48641304347826086,"T211 =
S≤ℓb
f≤ℓ

2"
REFERENCES,0.48731884057971014,"2 + od,P(1) · ∥P≤ℓfd∥2
L2 ,"
REFERENCES,0.4882246376811594,hence combining terms
REFERENCES,0.4891304347826087,"T21 = T211 + T212 + T213 + T214 =
S≤ℓb
f≤ℓ

2"
REFERENCES,0.490036231884058,"2 + od,P(1) · ∥P≤ℓfd∥2
L2 ."
REFERENCES,0.49094202898550726,Lemma 10 (Term T23 Eq. (27)).
REFERENCES,0.49184782608695654,"T23 = f T
>ℓ(In −e−tH/n)H−1MH−1(In −e−tH/n)f>ℓ= od,P(1) · ∥fd∥2
L2+η ."
REFERENCES,0.4927536231884058,Proof of Lemma 10. Let us deﬁne the matrix
REFERENCES,0.4936594202898551,G = (In −e−tH/n)H−1MH−1(In −e−tH/n).
REFERENCES,0.4945652173913043,Using similar reasoning from Lemma 9 we can write G = 1
REFERENCES,0.4954710144927536,"n2 K≤mΨ≤mS2
≤mΨT
≤mK≤m + ∆ = 1"
REFERENCES,0.4963768115942029,"n2 K≤ℓΨ≤mS2
≤mΨT
≤mK≤ℓ+ ∆′."
REFERENCES,0.49728260869565216,"for matrices ∆, ∆′ satisfying max{∥∆∥op , ∥∆′∥op} = od,P(1). We can split T23 into"
REFERENCES,0.49818840579710144,T23 = 1
REFERENCES,0.4990942028985507,"nf T
>ℓGf>ℓ = 1"
REFERENCES,0.5,n(f>m + fℓm)TG(f>m + fℓm) = 1
REFERENCES,0.5009057971014492,"nf T
>mGf>m + 2"
REFERENCES,0.5018115942028986,"nf T
ℓmGf>m + 1"
REFERENCES,0.5027173913043478,"nf T
ℓmGfℓm"
REFERENCES,0.5036231884057971,"= T231 + T232 + T233 + T234
where"
REFERENCES,0.5045289855072463,T231 = 1
REFERENCES,0.5054347826086957,"n2 f T
>mK≤mΨ≤mS2
≤mΨT
≤mK≤mf>m,"
REFERENCES,0.5063405797101449,T232 = 2
REFERENCES,0.5072463768115942,"n2 f T
ℓmGf>m,"
REFERENCES,0.5081521739130435,T233 = 1
REFERENCES,0.5090579710144928,"n2 f T
ℓmK≤ℓΨ≤mS2
≤mΨT
≤mK≤ℓfℓm,"
REFERENCES,0.509963768115942,"T234 = od,P(1) · ∥P>ℓfd∥2
L2 ."
REFERENCES,0.5108695652173914,"Let us ﬁrst start with analysing T231, deﬁning B = ΨT
≤mΨ≤m/n we have"
REFERENCES,0.5117753623188406,T231 = 1
REFERENCES,0.5126811594202898,"n2 f T
>mΨ≤m(Im −αe−(t/n)D2
≤mB)S2
≤m(Im −αe−(t/n)BD2
≤m)ΨT
≤mf>m"
REFERENCES,0.5135869565217391,"(a)
≤(1 + od,P(1))f T
>mΨ≤mΨT
≤mf>m/n2"
REFERENCES,0.5144927536231884,"= od,P(1) · ∥P>mfd∥2
L2+η ,"
REFERENCES,0.5153985507246377,Under review as a conference paper at ICLR 2022
REFERENCES,0.5163043478260869,"where the ﬁrst equality is by Lemma 2 and the last line follows from Lemma 7, Markov’s inequality,
and by the fact that m/n = od(1) by Assumption 2(c). To see that inequality (a) holds note that
Im −αe−(t/n)D2
≤mB
op =
B−1/2(Im −αe−(t/n)B1/2D2
≤mB1/2)B1/2
op"
REFERENCES,0.5172101449275363,"≤
B−1/2
op"
REFERENCES,0.5181159420289855,"B1/2
op ."
REFERENCES,0.5190217391304348,"Since S2
≤m ⪯Im,
(Im −αe−(t/n)D2
≤mB)S2
≤m(Im −αe−(t/n)BD2
≤m)

op ≤
B−1
op ∥B∥op = 1 + od,P(1),"
REFERENCES,0.519927536231884,"where the last equality is by Lemma 4. Now let us turn to T233, which we can further split into"
REFERENCES,0.5208333333333334,"T233 = T2331 + T2332
(28) where"
REFERENCES,0.5217391304347826,T2331 = 1
REFERENCES,0.5226449275362319,"n2 f T
ℓmK≤ℓΨ≤ℓS2
≤ℓΨT
≤ℓK≤ℓfℓm,"
REFERENCES,0.5235507246376812,T2332 = 1
REFERENCES,0.5244565217391305,"n2 f T
ℓmK≤ℓΨℓmS2
ℓmΨT
ℓmK≤ℓfℓm."
REFERENCES,0.5253623188405797,"Redeﬁning B = ΨT
≤ℓΨ≤ℓ/n and using a similar argument as for T231, the ﬁrst term can be seen as"
REFERENCES,0.5262681159420289,T2331 = 1
REFERENCES,0.5271739130434783,"n2 b
f T
ℓmΨT
ℓmΨ≤ℓ(I −αe−(t/n)D2
≤ℓB)S2
≤ℓ(I −αe−(t/n)BD2
≤ℓ)ΨT
≤ℓΨℓm b
fℓm"
REFERENCES,0.5280797101449275,"≤(1 + od,P(1))
 b
fℓm

2 2"
REFERENCES,0.5289855072463768,"ΨT
ℓmΨ≤ℓ/n
2"
REFERENCES,0.529891304347826,"op = od,P(1) · ∥Pℓmfd∥2
L2 .
(29)"
REFERENCES,0.5307971014492754,"Turning to the second term T2332, let"
REFERENCES,0.5317028985507246,A := 1
REFERENCES,0.532608695652174,"nΨT
ℓm

In −e−(t/n)(Ψ≤ℓD2
≤ℓΨT
≤ℓ+κHIn)
Ψℓm,"
REFERENCES,0.5335144927536232,then we can write this term as
REFERENCES,0.5344202898550725,"T2332 = b
f T
ℓmAS2
ℓmA b
fℓm"
REFERENCES,0.5353260869565217,"≤b
f T
ℓmA
2 b
fℓm ≤b
f T
ℓmA b
fℓm + od,P(1) · ∥Pℓmfd∥2
L2 ,
(30)"
REFERENCES,0.5362318840579711,"where the last inequality holds since A ⪯ΨT
ℓmΨℓm/n = Im−ℓ+ ∆by Lemma 4 and so"
REFERENCES,0.5371376811594203,"A
2 = A
1/2A A
1/2"
REFERENCES,0.5380434782608695,"⪯A
1/2(I + ∆)A
1/2"
REFERENCES,0.5389492753623188,"= A + A
1/2∆A
1/2"
REFERENCES,0.5398550724637681,= A + ∆′.
REFERENCES,0.5407608695652174,"By the inequality 1 −e−x ≤x, in the PSD order we see that A ⪯t"
REFERENCES,0.5416666666666666,"n2 ΨT
ℓmΨ≤ℓD2
≤ℓΨT
≤ℓΨℓm + κHt n  1"
REFERENCES,0.542572463768116,"nΨT
ℓmΨℓm 
."
REFERENCES,0.5434782608695652,Therefore from Eq. (30) we have
REFERENCES,0.5443840579710145,T2332 ≤t
REFERENCES,0.5452898550724637,"n2 b
f T
ℓmΨT
ℓmΨ≤ℓD2
≤ℓΨT
≤ℓΨℓm b
fℓm + b
f T
ℓm
κHt n  1"
REFERENCES,0.5461956521739131,"nΨT
ℓmΨℓm"
REFERENCES,0.5471014492753623,"
b
fℓm + od,P(1) · ∥Pℓmfd∥2
L2 ."
REFERENCES,0.5480072463768116,"For the second term on the right, by Assumption 3(b) and by Lemma 4,"
REFERENCES,0.5489130434782609,"b
f T
ℓm
κHt n  1"
REFERENCES,0.5498188405797102,"nΨT
ℓmΨℓm"
REFERENCES,0.5507246376811594,"
b
fℓm = od,P(1) · ∥Pℓmfd∥2
L2 ."
REFERENCES,0.5516304347826086,Under review as a conference paper at ICLR 2022
REFERENCES,0.552536231884058,"For the ﬁrst term by Lemma 7 and Assumptions 3(b), 3(c),
t
n2 E[ b
f T
ℓmΨT
ℓmΨ≤ℓD2
≤ℓΨT
≤ℓΨℓm b
fℓm] ≤(t/n)C(η) ∥Pℓmfd∥2
L2+η Tr
 
D2
≤ℓ
"
REFERENCES,0.5534420289855072,"= od,P(1) · ∥Pℓmfd∥2
L2+η ,"
REFERENCES,0.5543478260869565,therefore by Markov’s inequality
REFERENCES,0.5552536231884058,"T2332 = od,P(1) · ∥Pℓmfd∥2
L2+η ."
REFERENCES,0.5561594202898551,Hence combining terms
REFERENCES,0.5570652173913043,"T233 = T2331 + T2332 = od,P(1) · ∥Pℓmfd∥2
L2+η ."
REFERENCES,0.5579710144927537,By the Cauchy-Schwarz inequality
REFERENCES,0.5588768115942029,T232 = 2
REFERENCES,0.5597826086956522,"n2 f T
ℓmGf>m"
REFERENCES,0.5606884057971014,"≤2
 1"
REFERENCES,0.5615942028985508,"n2 f T
ℓmGfℓm
1
n2 f T
>mGf>m 1/2"
REFERENCES,0.5625,"= 2

T231 + od,P(1) · ∥P>ℓfd∥2
L2
1/2
T233 + od,P(1) · ∥P>ℓfd∥2
L2
1/2"
REFERENCES,0.5634057971014492,"≤od,P(1) · ∥P>ℓfd∥L2+η ∥P>mfd∥L2+η ."
REFERENCES,0.5643115942028986,Putting everything together we see that
REFERENCES,0.5652173913043478,"T23 = T231 + T232 + T233 + T234 = od,P(1) · ∥fd∥2
L2+η ."
REFERENCES,0.5661231884057971,"B.4.2
TERM T1"
REFERENCES,0.5670289855072463,"Now we will analyse term T1.
Proposition 2 (Term T1)."
REFERENCES,0.5679347826086957,"T1 = f T(In −e−tH/n)H−1E =
S1/2
≤ℓb
f≤ℓ

2"
REFERENCES,0.5688405797101449,"2 + od,P(1) · ∥fd∥2
L2+η ."
REFERENCES,0.5697463768115942,Proof of Proposition 2. We break T1 into the following terms
REFERENCES,0.5706521739130435,"T1 = T11 + T12 + T13
where"
REFERENCES,0.5715579710144928,"T11 = f T
≤ℓ(I −e−tH/n)H−1E≤m,
(31)"
REFERENCES,0.572463768115942,"T12 = f T
>ℓ(I −e−tH/n)H−1E≤m,
(32)"
REFERENCES,0.5733695652173914,"T13 = f T(I −e−tH/n)H−1E>m.
(33)"
REFERENCES,0.5742753623188406,"Using Lemma 11 and recalling Lemma 10,"
REFERENCES,0.5751811594202898,"T12 ≤(T23)1/2 b
f≤m

2 = od,P(1) · ∥P≤mfd∥L2 ∥fd∥L2+η ,"
REFERENCES,0.5760869565217391,"where T23 is as given in Eq. (27). From the analysis of T11 in Lemma 12 and T13 in Lemma 13 we
combine everything to get Proposition 2"
REFERENCES,0.5769927536231884,"T1 =
S1/2
≤ℓb
f≤ℓ

2"
REFERENCES,0.5778985507246377,"2 + od,P(1) · ∥fd∥2
L2+η ."
REFERENCES,0.5788043478260869,"Lemma 11. For a vector v ∈Rn,"
REFERENCES,0.5797101449275363,"vTH−1E≤m ≤(vTH−1MH−1v)1/2 b
f≤m

2 =
 1"
REFERENCES,0.5806159420289855,"n2 vTΨ≤mS2
≤mΨT
≤mv + od,P(1) · 1 n∥v∥2"
REFERENCES,0.5815217391304348,"1/2 b
f≤m

2."
REFERENCES,0.582427536231884,Under review as a conference paper at ICLR 2022
REFERENCES,0.5833333333333334,"Proof of Lemma 11. Recall that E≤m = Ψ≤mD2
≤m b
f≤m. By the Cauchy-Schwarz inequality,"
REFERENCES,0.5842391304347826,"vTH−1E≤m = vTH−1Ψ≤mD2
≤m b
f≤m,"
REFERENCES,0.5851449275362319,"≤(vTH−1Ψ≤mD4
≤mΨT
≤mH−1v)1/2 b
f≤m

2,"
REFERENCES,0.5860507246376812,"≤(vTH−1MH−1v)1/2 b
f≤m

2 =
 1"
REFERENCES,0.5869565217391305,"n2 vTΨ≤mS2
≤mΨT
≤mv + od,P(1) · 1 n∥v∥2"
REFERENCES,0.5878623188405797,"1/2 b
f≤m

2,"
REFERENCES,0.5887681159420289,where the last equality follows from Lemma 3.
REFERENCES,0.5896739130434783,Lemma 12 (Term T11 Eq. (31)).
REFERENCES,0.5905797101449275,"T11 =
S1/2
≤ℓb
f≤ℓ

2"
REFERENCES,0.5914855072463768,"2 + od,P(1) · ∥P≤mfd∥2
L2 ."
REFERENCES,0.592391304347826,"Proof of Lemma 12. First note that by Lemma 8 for some ∆such that ∥∆∥op = od,P(1),"
REFERENCES,0.5932971014492754,"T11 = T111 + T112
where"
REFERENCES,0.5942028985507246,"T111 = f T
≤ℓ(I −e−(t/n)(Ψ≤ℓD2
≤ℓΨT
≤ℓ+κHIn))H−1E≤m,"
REFERENCES,0.595108695652174,"T112 = f T
≤ℓ∆H−1E≤m."
REFERENCES,0.5960144927536232,"By Lemma 11,"
REFERENCES,0.5969202898550725,"T112 ≤

∥∆∥2
op (∥f≤ℓ∥2
2/n)
nH−1MH−1
op"
REFERENCES,0.5978260869565217,"1/2 b
f≤m

2 = od,P(1) · ∥P≤mfd∥2
L2 ."
REFERENCES,0.5987318840579711,"We now consider
T111 = T1111 −T1112
where"
REFERENCES,0.5996376811594203,"T1111 = f T
≤ℓH−1E≤m"
REFERENCES,0.6005434782608695,"T1112 = αf T
≤ℓe−(t/n)Ψ≤ℓD2
≤ℓΨT
≤ℓH−1E≤m."
REFERENCES,0.6014492753623188,"Note that by Lemma 5,
ΨT
≤ℓH−1Ψ≤mD2
≤m −[S≤ℓ; 0]

op = od,P(1),"
REFERENCES,0.6023550724637681,"where 0 is a ℓ× (m −ℓ) matrix of zeros. Hence for the ﬁrst term T1111,"
REFERENCES,0.6032608695652174,"T1111 = b
f T
≤ℓΨT
≤ℓH−1Ψ≤mD2
≤m b
f≤m =
S1/2
≤ℓb
f≤ℓ

2"
REFERENCES,0.6041666666666666,"2 + od,P(1) · ∥P≤mfd∥2
L2 ."
REFERENCES,0.605072463768116,"Deﬁne H≤ℓ:= Ψ≤ℓD2
≤ℓΨT
≤ℓ. For the second term T1112, by Lemma 11"
REFERENCES,0.6059782608695652,"T1112 ≤

S + od,P(1) · ∥P≤ℓfd∥2
L2
1/2 b
f≤m

2,
(34)"
REFERENCES,0.6068840579710145,"where
S = 1"
REFERENCES,0.6077898550724637,"n2 f T
≤ℓe−(t/n)H≤ℓΨ≤mS2
≤mΨT
≤me−(t/n)H≤ℓf≤ℓ."
REFERENCES,0.6086956521739131,Deﬁne A := 1
REFERENCES,0.6096014492753623,"nΨT
≤ℓe−(t/n)H≤ℓΨ≤ℓ. We have S ≤1"
REFERENCES,0.6105072463768116,"n2 f T
≤ℓe−(t/n)H≤ℓΨ≤mΨT
≤me−(t/n)H≤ℓf≤ℓ = 1"
REFERENCES,0.6114130434782609,"n2 f T
≤ℓe−(t/n)H≤ℓΨ≤ℓΨT
≤ℓe−(t/n)H≤ℓf≤ℓ+ 1"
REFERENCES,0.6123188405797102,"n2 f T
≤ℓe−(t/n)H≤ℓΨℓmΨT
ℓme−(t/n)H≤ℓf≤ℓ ≤1"
REFERENCES,0.6132246376811594,"n2 f T
≤ℓe−(t/n)H≤ℓΨ≤ℓΨT
≤ℓe−(t/n)H≤ℓf≤ℓ+
 b
f≤ℓ

2"
REFERENCES,0.6141304347826086,"2 ·
ΨT
≤ℓΨℓm/n
2 op"
REFERENCES,0.615036231884058,"= b
f T
≤ℓA2 b
f≤ℓ+ od,P(1) · ∥P≤ℓfd∥2
L2"
REFERENCES,0.6159420289855072,"= od,P(1) · ∥P≤ℓfd∥2
L2 ,"
REFERENCES,0.6168478260869565,Under review as a conference paper at ICLR 2022
REFERENCES,0.6177536231884058,"since as noted before in Eq. (22), ∥A∥op = od,P(1). Therefore"
REFERENCES,0.6186594202898551,"T11 = T1111 + T1112 + T112 =
S1/2
≤ℓb
f≤ℓ

2"
REFERENCES,0.6195652173913043,"2 + od,P(1) · ∥P≤mfd∥2
L2 ."
REFERENCES,0.6204710144927537,Lemma 13 (Term T13 Eq. (33)).
REFERENCES,0.6213768115942029,"T13 = od,P(1) · ∥P>mfd∥L2 ∥fd∥L2 ."
REFERENCES,0.6222826086956522,Proof of Lemma 13. We have
REFERENCES,0.6231884057971014,|T13| = |f T(I −e−tH/n)H−1E>m|
REFERENCES,0.6240942028985508,"≤∥f∥2
H−1
op ∥E>m∥2"
REFERENCES,0.625,"Note that we have E[∥f∥2
2] = n ∥fd∥2
L2. Further by Eq. (17), we have
H−1
op ≤2/κH with high
probability. Finally, recalling the deﬁnition of E>m from Eq. (16), we have"
REFERENCES,0.6259057971014492,"E[∥E>m∥2] = n ∞
X"
REFERENCES,0.6268115942028986,"k=m+1
λ4
d,k ˆf 2
k ≤n

max
k≥m+1 λ4
d,k"
REFERENCES,0.6277173913043478,"
∥P>mfd∥2
L2 ."
REFERENCES,0.6286231884057971,"As a result, we have"
REFERENCES,0.6295289855072463,"|T13| ≤Od,P(1) · ∥P>mfd∥L2 ∥fd∥L2 [n2 max
k≥m+1 λ4
d,k]1/2/κH"
REFERENCES,0.6304347826086957,"= Od,P(1) · ∥P>mfd∥L2 ∥fd∥L2 [n max
k≥m+1 λ2
d,k] /
X"
REFERENCES,0.6313405797101449,"k≥m+1
λ2
d,k"
REFERENCES,0.6322463768115942,"= od,P(1) · ∥P>mfd∥L2 ∥fd∥L2 ,"
REFERENCES,0.6331521739130435,where the last equality used Eq. (13) in Assumption 2(a).
REFERENCES,0.6340579710144928,"B.4.3
TERMS T3, T4, T5"
REFERENCES,0.634963768115942,"To analyse the terms T3, T4, T5 we can adapt the corresponding steps for the proof of Theorem 4 in
Mei et al. (2021a). For the following analysis we recall the deﬁnition of ε(t) from Eq. (24)."
REFERENCES,0.6358695652173914,Lemma 14 (Term T3).
REFERENCES,0.6367753623188406,"T3 = ε(t)TH−1MH−1ε(t) = od,P(1) · σ2
ε"
REFERENCES,0.6376811594202898,Proof of Lemma 14.
REFERENCES,0.6385869565217391,"1
σ2ε
Eε[T3] = Tr

(In −e−tH/n)2H−1MH−1"
REFERENCES,0.6394927536231884,"≤Tr
 
H−1MH−1"
REFERENCES,0.6403985507246377,"(a)
= Tr
 
Ψ≤mS2
≤mΨT
≤m/n2
+ od,P(1)"
REFERENCES,0.6413043478260869,"(b)
≤1"
REFERENCES,0.6422101449275363,"n2 Tr
 
Ψ≤mΨT
≤m

+ od,P(1)"
REFERENCES,0.6431159420289855,"(c)
= 1"
REFERENCES,0.6440217391304348,"n2 nm(1 + od,P(1)) + od,P(1) = od,P(1),"
REFERENCES,0.644927536231884,"where (a) used Lemma 3, (b) used S≤m ⪯Im, and (c) used Lemma 4 and Assumption 2(c). The
lemma then follows from Markov’s inequality."
REFERENCES,0.6458333333333334,Lemma 15 (Term T4).
REFERENCES,0.6467391304347826,"T4 = ε(t)TH−1E = od,P(1) · (σ2
ε + ∥fd∥2
L2)."
REFERENCES,0.6476449275362319,Under review as a conference paper at ICLR 2022
REFERENCES,0.6485507246376812,Proof of Lemma 15.
REFERENCES,0.6494565217391305,"1
σ2ε
Eε[T 2
4 ] = 1"
REFERENCES,0.6503623188405797,"σ2ε
Eε[εT(I −e−tH/n)H−1EETH−1(I −e−tH/n)ε]"
REFERENCES,0.6512681159420289,= ETH−1(I −e−tH/n)2H−1E
REFERENCES,0.6521739130434783,≤ETH−2E
REFERENCES,0.6530797101449275,"Notice that M ⪰Ψ≤LD4
≤LΨT
≤L for any L ∈N, by the decomposition of Eq. (16). Therefore sup
L"
REFERENCES,0.6539855072463768,"D2
≤LΨT
≤LH−2Ψ≤LD2
≤L

op = sup
L"
REFERENCES,0.654891304347826,"H−1Ψ≤LD4
≤LΨT
≤LH−1
op"
REFERENCES,0.6557971014492754,"≤
H−1MH−1
op = od,P(1),
(35)"
REFERENCES,0.6567028985507246,"where the last inequality follows from Lemma 3. Hence,"
REFERENCES,0.657608695652174,"ETH−2E = lim
L→∞ET
≤LH−2ET
≤L"
REFERENCES,0.6585144927536232,"(a)
= lim
L→∞
b
f T
≤L[D2
≤LΨT
≤LH−2Ψ≤LD2
≤L] b
f≤L"
REFERENCES,0.6594202898550725,"(b)
≤lim sup
L→∞"
REFERENCES,0.6603260869565217,"D2
≤LΨT
≤LH−2Ψ≤LD2
≤L

op · lim
L→∞"
REFERENCES,0.6612318840579711,"b
f≤L

2 2"
REFERENCES,0.6621376811594203,"(c)
≤od,P(1) · ∥fd∥2
L2 ,
where (a) follows from the deﬁnition of E≤L, (b) follows from the deﬁnition of operator norm, and
(c) follows from Eq. (35). Therefore we get"
REFERENCES,0.6630434782608695,"T4 = od,P(1) · σε · ∥fd∥L2 = od,P(1) · (σ2
ε + ∥fd∥2
L2)."
REFERENCES,0.6639492753623188,Lemma 16 (Term T5).
REFERENCES,0.6648550724637681,"T5 = ε(t)TH−1MH−1v(t) = od,P(1) · (σ2
ε + ∥fd∥2
L2+η)."
REFERENCES,0.6657608695652174,"Proof of Lemma 16. We can write term T5 as
T5 = T51 + T52
where
T51 = ε(t)TH−1MH−1(In −e−tH/n)f≤ℓ,"
REFERENCES,0.6666666666666666,"T52 = ε(t)TH−1MH−1(In −e−tH/n)f>ℓ.
Note as in Eq. (35), that by Lemma 3 and Lemma 4,
M 1/2H−2M 1/2
op =
H−1MH−1
op = od,P(1),"
REFERENCES,0.667572463768116,"and taking the second moment of T51 yields
1
σ2ε
Eε[T 2
51] ≤1"
REFERENCES,0.6684782608695652,"σ2ε
Eε[εTH−1MH−1(In −e−tH/n)f≤ℓf T
≤ℓ(In −e−tH/n)H−1MH−1ε]"
REFERENCES,0.6693840579710145,"= f T
≤ℓ(In −e−tH/n)[H−1MH−1]2(In −e−tH/n)f≤ℓ"
REFERENCES,0.6702898550724637,"≤
M 1/2H−2M 1/2
op"
REFERENCES,0.6711956521739131,"M 1/2H−1(In −e−tH/n)f≤ℓ

2"
REFERENCES,0.6721014492753623,"2
= od,P(1) · T21"
REFERENCES,0.6730072463768116,"= od,P(1) · ∥P≤ℓfd∥2
L2 ,
where T21 is as given in Eq. (25). Similarly we get that
1
σ2ε
Eε[T 2
52] = od,P(1) · T23 = od,P(1) · ∥fd∥2
L2+η ,"
REFERENCES,0.6739130434782609,where T23 is as given in Eq. (27). By Markov’s inequality we deduce that
REFERENCES,0.6748188405797102,"T5 = od,P(1) · σε · (∥P≤ℓfd∥L2 + ∥fd∥L2+η) = od,P(1) · (σ2
ε + ∥fd∥2
L2+η)."
REFERENCES,0.6757246376811594,Under review as a conference paper at ICLR 2022
REFERENCES,0.6766304347826086,"Finally putting Propositions 1, 2 and Lemmas 14, 15, 16 together for terms T2, T1, T3, T4 and T5
respectively leads to the proof of Theorem 5"
REFERENCES,0.677536231884058,Proof of Theorem 5.
REFERENCES,0.6784420289855072,"R( ˆft) = ∥fd∥2
L2 −2T1 + T2 + T3 −2T4 + 2T5"
REFERENCES,0.6793478260869565,"= ∥P>ℓfd∥2
L2 +
 b
fℓ

2"
REFERENCES,0.6802536231884058,"2 −2
S≤ℓb
f≤ℓ

2
+
S≤ℓb
f≤ℓ

2"
REFERENCES,0.6811594202898551,"+ od,P(1) · (∥fd∥2
L2 + ∥fd∥2
L2+η + σ2
ε)"
REFERENCES,0.6820652173913043,"=
(I −S≤ℓ) b
f≤ℓ

2"
REFERENCES,0.6829710144927537,"2 + ∥P>ℓfd∥2
L2 + od,P(1) · (∥fd∥2
L2 + ∥fd∥2
L2+η + σ2
ε)."
REFERENCES,0.6838768115942029,"By Assumption 2(b), κH/n = od(1) · maxj≤ℓλ2
d,j hence
(I −S≤ℓ) b
f≤ℓ

2"
REFERENCES,0.6847826086956522,"2 = od,P(1) · ∥fd∥2
L2"
REFERENCES,0.6856884057971014,and as a result we obtain the ﬁrst part of the theorem
REFERENCES,0.6865942028985508,"R( ˆft) = ∥P>ℓfd∥2
L2 + od,P(1) · (∥fd∥2
L2 + ∥fd∥2
L2+η + σ2
ε).
(36)
Now observe that similar to Eq. (15) we have the following decomposition
 ˆft −P≤ℓfd

2"
REFERENCES,0.6875,"L2 = ∥P≤ℓfd∥2
L2 −2u(t)TH−1E≤ℓ+ u(t)TH−1MH−1u(t),"
REFERENCES,0.6884057971014492,"where u(t) is given in Eq. (14). Therefore we can write
 ˆft −P≤ℓfd

2"
REFERENCES,0.6893115942028986,"L2 = R( ˆft) −∥P>ℓfd∥2
L2 + 2u(t)TH−1E>ℓ.
(37)"
REFERENCES,0.6902173913043478,"We now focus on the term
u(t)TH−1E>ℓ= v(t)TH−1E>ℓ+ ε(t)TH−1E>ℓ.
By choosing L = ℓin the proof of Lemma 15, it follows that"
REFERENCES,0.6911231884057971,"ε(t)TH−1E≤ℓ= od,P(1) · (σ2
ε + ∥P≤ℓfd∥2
L2),
hence combining with the bound for T4 in Lemma 15 yields
ε(t)TH−1E>ℓ= T4 −ε(t)TH−1E≤ℓ"
REFERENCES,0.6920289855072463,"= od,P(1) · (σ2
ε + ∥fd∥2
L2) −od,P(1) · (σ2
ε + ∥P≤ℓfd∥2
L2)"
REFERENCES,0.6929347826086957,"= od,P(1) · (σ2
ε + ∥fd∥2
L2).
We will now show that
v(t)TH−1E>ℓ= od,P(1) · ∥P>ℓfd∥L2 ∥fd∥L2 .
(38)
If ℓ(d) = m(d), then Eq. (38) follows from Lemma 13. Otherwise, consider the case ℓ(d) = u(d).
Following similar logic to the proof of Lemma 13, because E[∥f∥2
2] = n ∥fd∥2
L2 and"
REFERENCES,0.6938405797101449,"E[∥E>u∥2
2] = n ∞
X"
REFERENCES,0.6947463768115942,"k=u+1
λ4
d,k ˆf 2
k ≤nλ4
d,u+1 ∥P>ufd∥2
L2 ,"
REFERENCES,0.6956521739130435,we also get Eq. (38) since
REFERENCES,0.6965579710144928,|v(t)TH−1E>ℓ| = |f T(I −e−tH/nH−1)E>u|
REFERENCES,0.697463768115942,"≤(t/n)∥f∥2
(I −e−tH/n)(tH/n)−1
op ∥E>u∥2"
REFERENCES,0.6983695652173914,"(a)
≤Od,P(1) · ∥P>ufd∥L2 ∥fd∥L2 tλ2
d,u+1"
REFERENCES,0.6992753623188406,"(b)
= od,P(1) · ∥P>ufd∥L2 ∥fd∥L2 ,
where (a) used the inequality (1 −e−x)/x ≤1 and (b) used Assumption 3(a).
Therefore
u(t)TH−1E>ℓ= od,P(1) · (σ2
ε + ∥fd∥2
L2), hence by Eq. (36) and Eq. (37) we obtain the ﬁnal
part of the theorem
 ˆft −P≤ℓfd

2"
REFERENCES,0.7001811594202898,"L2 = od,P(1) · (∥fd∥2
L2+η + σ2
ε).
(39)"
REFERENCES,0.7010869565217391,Under review as a conference paper at ICLR 2022
REFERENCES,0.7019927536231884,"C
DOT PRODUCT KERNELS ON Sd−1(
√ d)"
REFERENCES,0.7028985507246377,"C.1
SETTING"
REFERENCES,0.7038043478260869,"We now apply our general theorems to the setting of dot product kernels on the sphere. Concretely
we take Xd = Sd−1(
√"
REFERENCES,0.7047101449275363,"d) and νd = Unif(Sd−1(
√"
REFERENCES,0.7056159420289855,"d)) and consider dot product kernels Hd which
take the form of Eq. (4). Note that by Eq. (61) any dot product kernel hd can be decomposed as"
REFERENCES,0.7065217391304348,"hd(⟨x1, x2⟩/d) = Ew∼Unif(Sd−1)[σd(⟨w, x1⟩)σd(⟨w, x2⟩)]"
REFERENCES,0.707427536231884,"for some activation function σd. We state mild assumptions on σd and show that under these condi-
tions we can apply the results in Section A.3."
REFERENCES,0.7083333333333334,"C.2
ASSUMPTIONS"
REFERENCES,0.7092391304347826,"We state our assumptions on σd after some deﬁnitions. See Appendix G for additional background.
Denote by P≤ℓthe orthogonal projection onto the subspace of L2(Sd−1(
√"
REFERENCES,0.7101449275362319,"d)) spanned by polyno-
mials of degree less than or equal to ℓ. The projectors Pℓand P>ℓare deﬁned analogously. Let us
emphasize that the projectors P≤ℓare related but distinct from the P≤m: while P≤ℓprojects onto
the eigenspace of polynomials of degree at most ℓ, P≤m projects onto the top m-eigenfunctions."
REFERENCES,0.7110507246376812,The assumptions given on the activations are the same as Assumption 3 of Mei et al. (2021a).
REFERENCES,0.7119565217391305,"Assumption 4 (Assumptions for Dot Product Kernels at level s ∈N). Let {Hd}d≥1 be a sequence
of dot product kernels with associated activation functions {σd}d≥1 as in Eq. (61). We assume the
following hold"
REFERENCES,0.7128623188405797,"(a) There exists an integer k and constants c1 < 1 and c0 > 0, such that |σd(x)| ≤
c0 exp
 
c1x2/(4k)

."
REFERENCES,0.7137681159420289,(b) We have
REFERENCES,0.7146739130434783,"min
k≤s ds−k Pkσd(⟨e, ·⟩)
2
L2 = Ωd(1),"
REFERENCES,0.7155797101449275,"P>2s+1σd(⟨e, ·⟩)
2
L2 = Ωd(1),"
REFERENCES,0.7164855072463768,where e ∈Sd−1 is a ﬁxed vector (it is easy to see that these quantities do not depend on e).
REFERENCES,0.717391304347826,"Consider t(d), n(d) such that"
REFERENCES,0.7182971014492754,"dj+δ0 ≤t ≤dj+1−δ0,
ds+δ0 ≤n ≤ds+1−δ0"
REFERENCES,0.7192028985507246,"for some j, s ∈N and δ0 > 0. We now verify that if {σd}d≥1 satisﬁes Assumption 4 at level s, then
for an appropriate choice of (u(d), m(d)) the conditions in Appendix A.2 are satisﬁed and lead to
Theorem 1. We set u(d) and m(d) to be the number of eigenvalues associated to spherical harmonics
of degree less than or equal to j and s respectively u = j
X"
REFERENCES,0.720108695652174,"k=0
B(d, k) = Θd(dj),
m = s
X"
REFERENCES,0.7210144927536232,"k=0
B(d, k) = Θd(ds)."
REFERENCES,0.7219202898550725,"The veriﬁcation of Assumption 1 (Kernel Concentration Property) and Assumption 2 (Eigenvalue
Condition) at level {(n(d), m(d)} is the same as the treatment in Theorem 2 of Mei et al. (2021a).
We only need to verify Assumption 3. To see part 3(a), note that 1/λ2
d,u(d) = Θd(dj) and 1/λ2
d,u(d) =
Θd(dj+1). For part 3(b), the condition holds because u(d) < m(d) for large d if and only if j < s.
Assumption 3(c) is easily seen to hold since the trace of the kernel operator Tr(Hd) = Θd(1)."
REFERENCES,0.7228260869565217,Under review as a conference paper at ICLR 2022
REFERENCES,0.7237318840579711,"D
GROUP INVARIANT KERNELS ON Sd−1(
√ d)"
REFERENCES,0.7246376811594203,"D.1
SETTING"
REFERENCES,0.7255434782608695,"We now apply our general theorems to the setting of group invariant kernels on the sphere. Con-
cretely we take Xd = Sd−1(
√"
REFERENCES,0.7264492753623188,"d) and νd = Unif(Sd−1(
√"
REFERENCES,0.7273550724637681,"d)) and consider kernels Hd which take
the form of Eq. (5) for some function h. By Eq. (61), for some activation function σd"
REFERENCES,0.7282608695652174,"Hd(x1, x2) =
Z"
REFERENCES,0.7291666666666666,"Gd
Ex∼Unif(Sd−1)[σd(⟨x1, w⟩)σd(⟨x2, g · w⟩)]πd(dg).
(40)"
REFERENCES,0.730072463768116,"We state mild assumptions on σd and show that under these conditions we can apply the results in
Appendix A.3. For additional technical background refer to Appendix G."
REFERENCES,0.7309782608695652,"D.2
ASSUMPTIONS"
REFERENCES,0.7318840579710145,"We will assume that σd = σ for all d and make the following assumptions on σ which are the same
as Assumption 1 in Mei et al. (2021b).
Assumption 5 (Assumption on Group Invariant Kernel at level s). Let {Hd}d≥1 be a sequence
of invariant kernels with associated activation functions σd = σ as in Eq. (40). We assume the
following conditions hold"
REFERENCES,0.7327898550724637,"(a) For Gd = Cycd, we assume σ to be (s + 1) ∨3 differentiable and there exists constants
c0 > 0 and c1 < 1 such that |σ(k)| ≤c0ec1u2/2 for any 2 ≤k ≤(s + 1) ∨3."
REFERENCES,0.7336956521739131,"For general Gd, we assume that σ is a (ﬁnite degree) polynomial function."
REFERENCES,0.7346014492753623,(b) The Hermite coefﬁcients µk(σ) (c.f. Appendix) verify µk ̸= 0 for any 0 ≤k ≤s.
REFERENCES,0.7355072463768116,(c) We assume that σ is not a polynomial with degrees less than or equal to s.
REFERENCES,0.7364130434782609,"Consider t(d), n(d) such that"
REFERENCES,0.7373188405797102,"dj+δ0 ≤t ≤dj+1−δ0,
ds−α+δ0 ≤n ≤ds−α+1−δ0"
REFERENCES,0.7382246376811594,"for some j, s ∈N and δ0 > 0. We now verify that if σ satisﬁes Assumption 5 at level s, then the
conditions given in Appendix A.2 are satisﬁed for an appropriate choice of (u(d), m(d)) which leads
to Theorem 2. We set u and m to be the number of eigenvalues invariant polynomials of degree less
than or equal to j and s respectively u = j
X"
REFERENCES,0.7391304347826086,"k=0
D(d, k) = Θd(dj−α),
m = s
X"
REFERENCES,0.740036231884058,"k=0
D(d, k) = Θd(ds−α),"
REFERENCES,0.7409420289855072,"where D(d, k) is the dimension of the subspace of invariant polynomials of degree k, c.f. Appendix
G.6. The veriﬁcation of Assumption 1 (Kernel Concentration Property) and Assumption 2 (Eigen-
value Condition) at level {(n(d), m(d)} is exactly the same as in Theorem 1 in Mei et al. (2021b)."
REFERENCES,0.7418478260869565,"We must verify Assumption 3. To see part 3(a), note that 1/λ2
d,u(d) = Θd(dj) and 1/λ2
d,u(d) =
Θd(dj+1). For part 3(b), the condition holds because u(d) < m(d) for large d if and only if j < s in
which case
t(d)
n(d) Tr
 
Hd,m(d)

≤dj+1−δ0"
REFERENCES,0.7427536231884058,ds−α+δ0 Θ(d−α) = O(d−2δ0dj−s+1) = od(1).
REFERENCES,0.7436594202898551,"Assumption 3(c) can be seen to hold from the fact that Tr
 
Hd,>m(d)

= Θ(ds−α) and m
X"
REFERENCES,0.7445652173913043,"j=0
λ2
d,j = s
X"
REFERENCES,0.7454710144927537,"k=0
ξ2
d,kD(d, k) = Θ(sd−α) = Θ(d−α)"
REFERENCES,0.7463768115942029,"from which it follows that for some constant C
m
X"
REFERENCES,0.7472826086956522,"j=0
λ2
d,j ≤C ∞
X"
REFERENCES,0.7481884057971014,"j>m
λ2
d,j."
REFERENCES,0.7490942028985508,Under review as a conference paper at ICLR 2022
REFERENCES,0.75,"E
AUXILIARY RESULTS"
REFERENCES,0.7509057971014492,"E.1
SOLUTION TO KERNEL DYNAMICS"
REFERENCES,0.7518115942028986,"Recall that we are interested in the following dynamics given in Eqs. (2), (3),"
REFERENCES,0.7527173913043478,"d
dtf or
t (x) = E[Hd(x, z)(fd(z) −f or
t (z))],"
REFERENCES,0.7536231884057971,"d
dt
ˆft(x) = 1 n n
X"
REFERENCES,0.7545289855072463,"i=1
Hd(x, xi)(yi −ˆft(xi))"
REFERENCES,0.7554347826086957,"with zero initialization f or
0 ≡ˆf0 ≡0. In this section we clarify the derivation, validity, and solution
of these dynamics. Let us consider the maps R : Hd →R and bRn : Hd →R deﬁned in Eq. (1)."
REFERENCES,0.7563405797101449,"R(f) =
Z"
REFERENCES,0.7572463768115942,"Xd
(f(x) −fd(x))2 dνd (x) + σ2
ε,"
REFERENCES,0.7581521739130435,"bRn(f) = 1 n n
X"
REFERENCES,0.7590579710144928,"i=1
(f(xi) −yi)2."
REFERENCES,0.759963768115942,"First, we recall the deﬁnition of the Fr´echet derivative of a functional V : Hd →R at f. The Fr´echet
derivative DV (f) is the linear functional such that for g ∈Hd,"
REFERENCES,0.7608695652173914,"lim
∥g∥Hd→0
|V (f + g) −V (f) −DV (f)(g)|"
REFERENCES,0.7617753623188406,"∥g∥Hd
= 0."
REFERENCES,0.7626811594202898,The gradient ∇V (f) ∈Hd is deﬁned such that
REFERENCES,0.7635869565217391,"⟨∇V (f), g⟩Hd = DV (f)(g)"
REFERENCES,0.7644927536231884,exists uniquely by the Riesz representation theorem. The gradients of the risk functionals are
REFERENCES,0.7653985507246377,"∇R(f) = Hd(f −fd),"
REFERENCES,0.7663043478260869,"∇bRn(f) = 1 n n
X"
REFERENCES,0.7672101449275363,"i=1
(f(xi) −yi)Hxi,"
REFERENCES,0.7681159420289855,"where Hxi(x) := Hd(xi, x) and Hd is the kernel operator as in Section A.1. A proof of this fact
is given in Proposition 2.1 of Yao et al. (2007). Taking f or
t (x) as shorthand for f or(t, x) where
f or(t, ·) ∈Hd is the oracle model at time t and similarly for ˆft(x), the following gradient ﬂows
with zero initialization are well-deﬁned for t ≥0,"
REFERENCES,0.7690217391304348,"d
dtf or
t
= −∇R(f or
t ) = −Hd(f or
t −fd) = Ez[Hd(·, z)(fd(z) −f or
t (z)]
(41)"
REFERENCES,0.769927536231884,"d
dt
ˆft = −∇bRn( ˆft) = −1 n n
X"
REFERENCES,0.7708333333333334,"i=1
( ˆft(xi) −yi)Hxi = 1 n n
X"
REFERENCES,0.7717391304347826,"i=1
Hd(·, xi)(yi −ˆft(xi)).
(42)"
REFERENCES,0.7726449275362319,The oracle model ODE Eq. (41) is simply a linear differential equation which has the following
REFERENCES,0.7735507246376812,"solution involving the operator exponential exp(A) :=
∞
P"
REFERENCES,0.7744565217391305,"k=0
Ak/k!"
REFERENCES,0.7753623188405797,"f or
t
= fd + exp(−tHd)(f or
0 −fd) = fd −exp(−tHd)fd.
(43)"
REFERENCES,0.7762681159420289,"For the empirical model ODE Eq. (42) we ﬁrst consider the system of scalar differential equations
induced at the points {(xi, yi)}i∈[n]. Letting u(t) = ( ˆft(x1), . . . , ˆft(xn))T, y = (y1, . . . , yn)T,
and H = (Hd(xi, xj))i,j∈[n] we have"
REFERENCES,0.7771739130434783,"d
dtu(t) = −1"
REFERENCES,0.7780797101449275,"nH(u(t) −y),"
REFERENCES,0.7789855072463768,"with initial condition u(0) = 0. As this a linear ODE, the solution is given by"
REFERENCES,0.779891304347826,"u(t) = y + e−tH/n(u(0) −y) = (In −e−tH/n)y.
(44)"
REFERENCES,0.7807971014492754,Under review as a conference paper at ICLR 2022
REFERENCES,0.7817028985507246,"For x ∈Rd, deﬁne h(x) = (Hd(x, x1), . . . , Hd(x, xn))T ∈Rn. Let a(t) = H−1u(t) ∈Rn. We
will show that the function ˆft(·) := ⟨h(·), a(t)⟩∈Hd, which satisﬁes ( ˆft(x1), . . . , ˆft(xn))T =
u(t), satisﬁes the following equation"
REFERENCES,0.782608695652174,"d
dt
ˆft(x) = 1"
REFERENCES,0.7835144927536232,"n⟨h(x), y −u(t)⟩= 1"
REFERENCES,0.7844202898550725,"n⟨h(x), e−tH/ny⟩,"
REFERENCES,0.7853260869565217,"which is Eq. (42) at point x. Indeed, by the chain rule"
REFERENCES,0.7862318840579711,"d
dt
ˆft(x) = d"
REFERENCES,0.7871376811594203,"dt⟨h(x), a(t)⟩"
REFERENCES,0.7880434782608695,"= ⟨h(x), d"
REFERENCES,0.7889492753623188,dta(t)⟩
REFERENCES,0.7898550724637681,"= ⟨h(x), H−1 1"
REFERENCES,0.7907608695652174,nHe−tH/ny⟩ = 1
REFERENCES,0.7916666666666666,"n⟨h(x), e−tH/ny⟩"
REFERENCES,0.792572463768116,which is what we wanted to show.
REFERENCES,0.7934782608695652,"E.2
EQUIVALENCE BETWEEN INVARIANT KERNELS AND DATA AUGMENTATION"
REFERENCES,0.7943840579710145,"In this section we will show an equivalence between the (time rescaled) gradient ﬂows for training
invariant kernels and using an augmented dataset. Speciﬁcally consider a group G and a kernel H
that is G-equivariant, that is"
REFERENCES,0.7952898550724637,"H(g · x1, g · x2) = H(x1, x2)
∀g ∈G, ∀x1, x2 ∈X."
REFERENCES,0.7961956521739131,"Given a G-equivariant kernel H, we deﬁne a G-invariant kernel Hinv as the group averaged kernel"
REFERENCES,0.7971014492753623,"Hinv =
Z"
REFERENCES,0.7980072463768116,"G
H(x1, g · x2)π(dg)"
REFERENCES,0.7989130434782609,"for the Haar measure π on G (c.f. Eq. (5)). Note that any dot product kernel is G-equivariant for G a
subgroup the orthogonal group e.g. the cyclic group Cyc (c.f. Section 3.3)."
REFERENCES,0.7998188405797102,"Given a dataset (X, y) = {(xi, yi) : i ∈[n]} consider the augmented dataset"
REFERENCES,0.8007246376811594,"(XG, yG) = {(g · xi, yi) : g ∈G, i ∈[n]}."
REFERENCES,0.8016304347826086,"We consider the (rescaled c.f. Remark 5) empirical dynamics Eq. (3) of the gradient ﬂow on (X, y)
using Hinv which we denote ˆft,inv"
REFERENCES,0.802536231884058,"d
dt
ˆft,inv(x) = −m∇bRn( ˆft,inv) = −m n n
X"
REFERENCES,0.8034420289855072,"i=1
( ˆft,inv(xi) −yi)Hinv(xi, x)"
REFERENCES,0.8043478260869565,"and the empirical dynamics of the gradient ﬂow on (XG, yG) using H which we denote ˆft,aug"
REFERENCES,0.8052536231884058,"d
dt
ˆft,aug(x) = −1 n X g∈G n
X"
REFERENCES,0.8061594202898551,"i=1
( ˆft,aug(g · xi) −yi)H(g · xi, x)."
REFERENCES,0.8070652173913043,"Proposition 3. Let G be a ﬁnite group with m elements. Given a G-equivariant kernel H, if π is the
uniform measure on G then
ˆft,inv ≡ˆft,aug,
∀t ≥0."
REFERENCES,0.8079710144927537,"Proof. Let G = {g1, . . . , gm} where g1 is the identity. Deﬁne the output vectors"
REFERENCES,0.8088768115942029,"uinv(t) := ( ˆft,inv(x1), . . . , ˆft,inv(xn))T ∈Rn,"
REFERENCES,0.8097826086956522,"ug(t) := ( ˆft,aug(g · x1), . . . , ˆft,aug(g · xn))T ∈Rn,"
REFERENCES,0.8106884057971014,"uaug(t) := (ug1(t), . . . , ugm(t))T ∈Rmn."
REFERENCES,0.8115942028985508,Under review as a conference paper at ICLR 2022
REFERENCES,0.8125,"Furthermore, deﬁne the kernel matrices"
REFERENCES,0.8134057971014492,"Hg,g′ := [H(g · xi, g′ · xj)]i,j∈[n] ∈Rn×n for g, g′ ∈G,"
REFERENCES,0.8143115942028986,"Haug := [Hg,g′]g,g′∈G ∈Rmn×mn,
Hinv = [Hinv(xi, xj)]i,j∈[n]."
REFERENCES,0.8152173913043478,"Note that by deﬁnition Hinv = 1 m m
P"
REFERENCES,0.8161231884057971,"j=1
Hg1,gj. We will show that"
REFERENCES,0.8170289855072463,"uaug(t) = (uinv(t), uinv(t), . . . , uinv(t)) for all t ≥0.
(45)"
REFERENCES,0.8179347826086957,"From this the result follows by Theorem 4.1 in Li et al. (2019) since ˆft,inv, ˆft,aug are given by kernel
regressions with targets uinv(t), uaug(t) and kernels Hinv, H respectively."
REFERENCES,0.8188405797101449,"By Eq. (42), we can write"
REFERENCES,0.8197463768115942,"uinv(t) = (I −exp(−tmHinv/n))y,
uaug(t) = (I −exp(−tHaug/n))yG,"
REFERENCES,0.8206521739130435,"where yG = (y, . . . , y) ∈Rmn. By expanding the matrix exponential series and using linearity, it
sufﬁces to show that"
REFERENCES,0.8215579710144928,"Hk
augyG = (mkHk
invy, mkHk
invy, . . . , mkHk
invy) for all k ∈N."
REFERENCES,0.822463768115942,"in order to show Eq. (45) holds. We prove the above by induction on k. For k = 1, observe that"
REFERENCES,0.8233695652173914,"HaugyG =  
m
X"
REFERENCES,0.8242753623188406,"j=1
Hgi,gjy   m i=1 =  
m
X"
REFERENCES,0.8251811594202898,"j=1
Hg1,gjy   m"
REFERENCES,0.8260869565217391,"i=1
= (mHinvy)m
i=1
where the second equality follows from G-equivariance of H. Assume the inductive hypothesis
holds for k. Then"
REFERENCES,0.8269927536231884,"Hk+1
aug yG = HaugHk
augyG = "
REFERENCES,0.8278985507246377,"mk
m
X"
REFERENCES,0.8288043478260869,"j=1
Hgi,gjHk
invy   m i=1 =  
 m
X"
REFERENCES,0.8297101449275363,"j=1
Hg1,gj  
m
X"
REFERENCES,0.8306159420289855,"j′=1
Hg1,gj′   k y  
 m i=1 =  
  
m
X"
REFERENCES,0.8315217391304348,"j=1
Hg1,gj   k+1 y  
 m"
REFERENCES,0.832427536231884,"i=1
= (mk+1Hk+1
inv y)m
i=1
where the second equality applies the induction hypothesis and the third equality uses equivariance.
Thus the inductive claim is proved and the proof is complete."
REFERENCES,0.8333333333333334,"Remark 5. The scaling factor m in the gradient ﬂow for ˆft,inv, leads to a natural comparison with
ˆft,aug as elaborated in Appendix E.3. As argued in that section, in the gradient descent discretiza-
tion, it is natural to take a step-size inversely proportional to the maximum kernel eigenvalue. In the
case of high-dimensional invariant kernels, note that"
REFERENCES,0.8342391304347826,λmax(Haug) = mλmax(H) ∼mλmax(Hinv)
REFERENCES,0.8351449275362319,hence the step-size for the invariant kernel ﬂow should be m times larger.
REFERENCES,0.8360507246376812,Under review as a conference paper at ICLR 2022
REFERENCES,0.8369565217391305,"E.3
DISCRETIZING TIME"
REFERENCES,0.8378623188405797,"Comparing different “speeds” of optimization algorithms only makes sense for discrete-time algo-
rithms. Consider the following gradient descent dynamics with step-size η, obtained as the dis-
cretization of the empirical gradient ﬂow Eq. (42)"
REFERENCES,0.8387681159420289,"ˆfk+1 = ˆfk −η∇bRn( ˆfk) = ˆfk −η 1 n n
X"
REFERENCES,0.8396739130434783,"i=1
( ˆfk(xi) −yi)H(·, xi),
k = 0, 1, . . .
(46)"
REFERENCES,0.8405797101449275,We will argue that it is natural to take η ∼n/λmax(H) where H is the kernel matrix.
REFERENCES,0.8414855072463768,"Deﬁne the sampling operator S : Hd →Rn by S(f) = (f(xi))n
i=1 ∈Rn and let S∗: Rn →Hd"
REFERENCES,0.842391304347826,"be its adjoint, deﬁned by S∗(y) = 1 n nP"
REFERENCES,0.8432971014492754,"i=1
yiHxi (see Yao et al. (2007) Appendix B for more details)."
REFERENCES,0.8442028985507246,"Then we can rewrite the gradient descent equation Eq. (46) as
ˆfk+1 = ˆfk −η(S∗S ˆft −S∗y),
k = 0, 1, . . .
(47)"
REFERENCES,0.845108695652174,"Let T
= S∗S and deﬁne H
:= SS∗=
1
nH to be the normalized kernel matrix.
Let"
REFERENCES,0.8460144927536232,"b := S∗H
−1y ∈Hd and note that since Tb = S∗y, we can rewrite Eq. (47) as
ˆfk+1 = ˆfk −ηT( ˆfk −b),
k = 0, 1, . . .
(48)"
REFERENCES,0.8469202898550725,"Denote the eigenvalues of H as λ1 ≥λ2 ≥. . . ≥λn > 0. By the spectral theorem, there exists"
REFERENCES,0.8478260869565217,"a set of orthornomal eigenvectors φ1, . . . , φn ∈Hd such that T =
nP"
REFERENCES,0.8487318840579711,"i=1
λiφiφ∗
i . Deﬁne αi(k) ="
REFERENCES,0.8496376811594203,"⟨ˆfk −b, φi⟩∈R. By taking Eq. (48) then subtracting b and taking the inner product with φi on both
sides, we get the coordinate evolution equations for i = 1, . . . , n"
REFERENCES,0.8505434782608695,"αi(k + 1) = ⟨(id −ηT)( ˆfk −b), φi⟩"
REFERENCES,0.8514492753623188,"= ⟨( ˆfk −b), (id −ηT)φi⟩
= (1 −ηλi)αi(k)
where the second equality holds since T is self-adjoint and the last equality is since φi is an eigen-
vector of T. It is easy to see that αi(k) = (1 −ηλi)kαi(0). Therefore we see that gradient descent
Eq. (46) is guaranteed to converge if η < 1/(2λ1) and may not otherwise. Therefore it is natural to
choose the step-size η to scale asymptotically as η ∼1/λmax(H)."
REFERENCES,0.8523550724637681,"For a dot product kernel H and its corresponding invariant kernel Hinv the kernel matrices have
operator norms of the same order
λmax(H) ∼λmax(Hinv)
hence no time rescaling is need to compare the corresponding optimization speeds asymptotically."
REFERENCES,0.8532608695652174,"E.4
SIMILARITIES WITH EMPIRICAL PHENOMENA"
REFERENCES,0.8541666666666666,"In this section we elaborate upon Remark 4 and mention some connections with empirical observa-
tions in Nakkiran et al. (2020). Although the metric in our setting is the squared loss, we can still
observe three stages in classiﬁcation problems when measuring the soft error. In Fig. 6a taken from
Nakkiran et al. (2020) we can observe stage 1 and stage 2. Either training has not continued long
enough to observe stage 3 or n is large enough so that the models have converged to the approxima-
tion error of the neural network class (c.f. Remark 1). In Fig. 6b taken from Nakkiran et al. (2020),
although the train errors are not plotted, by extrapolating from Fig. 6a, presumably for each n stage
1 and stage 2 occur. From the dimmer curves in Fig. 6b we can see that for n < 50000 stage 3
occurs as well."
REFERENCES,0.855072463768116,"In Fig. 7a, we see a parallel between the use of cyclic versus dot product kernels and the use data
augmentation versus not for a Resnet-18 trained on CIFAR-5m (note that using a cyclic kernel is
equivalent to using a dot product kernel with data-augmentation c.f. Appendix E.2). In both our
theoretical results and in the empirical results of Nakkiran et al. (2020) we observe that the ideal
world optimization speed of augmented and non-augmented training are the same, but for augmented
training the real world training speed is slowed down, eventually leading to better generalization for
long enough training."
REFERENCES,0.8559782608695652,Under review as a conference paper at ICLR 2022
REFERENCES,0.8568840579710145,"(a)
(b)"
REFERENCES,0.8577898550724637,"Figure 6: Soft-error curves for Resnet-18 trained on CIFAR-5m taken from ref. Nakkiran et al.
(2020). Panel (6a): n = 5 × 104 (Fig. 6 in ref). Panel (6b) Varying n (Fig. 4a in ref)."
REFERENCES,0.8586956521739131,"(a)
(b)"
REFERENCES,0.8596014492753623,"Figure 7: Panel (7a): Data-augmentation for Resnet-18 on CIFAR-5m. (Fig. 5a from Nakkiran et al.
(2020)). Panel (7b): Cyclic versus dot product kernel (Fig. 5c from this work)"
REFERENCES,0.8605072463768116,Under review as a conference paper at ICLR 2022
REFERENCES,0.8614130434782609,"F
ADDITIONAL FIGURES"
REFERENCES,0.8623188405797102,"To see the effects of varying the dimension d, in Fig. 8 we replicate the log-scale plots of kernel gra-
dient ﬂow with dot product kernels from Fig. 4. We take n = d1.5 and vary d ∈{50, 100, 200, 400}.
Each plot is averaged over 10 runs with the shaded region representing one standard deviation around
the mean. We can see that as d increasing the standard deviation decreases and the curves approach
the theoretical high-dimensional prediction."
REFERENCES,0.8632246376811594,"(a) d = 50
(b) d = 50
(c) d = 50"
REFERENCES,0.8641304347826086,"(d) d = 100
(e) d = 100
(f) d = 100"
REFERENCES,0.865036231884058,"(g) d = 200
(h) d = 200
(i) d = 200"
REFERENCES,0.8659420289855072,"(j) d = 400
(k) d = 400
(l) d = 400"
REFERENCES,0.8668478260869565,"Figure 8:
Each column replicates a kernel gradient ﬂow experiment from Fig. 4 over d ∈
{50, 100, 200, 400}. Column 1: Replicates Fig. 4a Column 2: Replicates Fig. 4a but with σ2
ε = 0.2.
Column 3: Replicates Fig. 4b. Averaged over 10 trials."
REFERENCES,0.8677536231884058,Under review as a conference paper at ICLR 2022
REFERENCES,0.8686594202898551,"G
TECHNICAL BACKGROUND"
REFERENCES,0.8695652173913043,"G.1
NOTATIONS"
REFERENCES,0.8704710144927537,"For a positive integer, we denote by [n] the set {1, 2, . . . , n}. For vectors u, v ∈Rd, we denote
⟨u, v⟩= u1v1 + . . . + udvd their scalar product, and ∥u∥2 = ⟨u, u⟩1/2 the ℓ2 norm. Given a
matrix A ∈Rn×m, we denote ∥A∥op = max∥u∥2=1 ∥Au∥2 its operator norm and by ∥A∥F =
  P"
REFERENCES,0.8713768115942029,"i,j A2
ij
1/2 its Frobenius norm. If A ∈Rn×n is a square matrix, the trace of A is denoted by
Tr(A) = P"
REFERENCES,0.8722826086956522,i∈[n] Aii.
REFERENCES,0.8731884057971014,"We use Od( · ) (resp. od( · )) for the standard big-O (resp. little-o) relations, where the subscript d
emphasizes the asymptotic variable. Furthermore, we write f = Ωd(g) if g(d) = Od(f(d)), and
f = ωd(g) if g(d) = od(f(d)). Finally, f = Θd(g) if we have both f = Od(g) and f = Ωd(g)."
REFERENCES,0.8740942028985508,"We use Od,P( · ) (resp. od,P( · )) the big-O (resp. little-o) in probability relations. Namely, for h1(d)
and h2(d) two sequences of random variables, h1(d) = Od,P(h2(d)) if for any ε > 0, there exists
Cε > 0 and dε ∈Z>0, such that
P(|h1(d)/h2(d)| > Cε) ≤ε,
∀d ≥dε,
and respectively: h1(d) = od,P(h2(d)), if h1(d)/h2(d) converges to 0 in probability. Similarly, we
will denote h1(d) = Ωd,P(h2(d)) if h2(d) = Od,P(h1(d)), and h1(d) = ωd,P(h2(d)) if h2(d) =
od,P(h1(d)). Finally, h1(d) = Θd,P(h2(d)) if we have both h1(d) = Od,P(h2(d)) and h1(d) =
Ωd,P(h2(d))."
REFERENCES,0.875,"G.2
FUNCTIONAL SPACES OVER THE SPHERE"
REFERENCES,0.8759057971014492,"For d ≥3, we let Sd−1(r) = {x ∈Rd : ∥x∥2 = r} denote the sphere with radius r in Rd.
We will mostly work with the sphere of radius
√"
REFERENCES,0.8768115942028986,"d, Sd−1(
√"
REFERENCES,0.8777173913043478,"d) and will denote by τd the uniform
probability measure on Sd−1(
√"
REFERENCES,0.8786231884057971,"d). All functions in this section are assumed to be elements of
L2(Sd−1(
√"
REFERENCES,0.8795289855072463,"d), τd), with scalar product and norm denoted as ⟨· , · ⟩L2 and ∥· ∥L2:"
REFERENCES,0.8804347826086957,"⟨f, g⟩L2 ≡
Z"
REFERENCES,0.8813405797101449,"Sd−1(
√"
REFERENCES,0.8822463768115942,"d)
f(x) g(x) τd(dx) .
(49)"
REFERENCES,0.8831521739130435,"For ℓ∈Z≥0, let ˜Vd,ℓbe the space of homogeneous harmonic polynomials of degree ℓon Rd (i.e.
homogeneous polynomials q(x) satisfying ∆q(x) = 0), and denote by Vd,ℓthe linear space of
functions obtained by restricting the polynomials in ˜Vd,ℓto Sd−1(
√"
REFERENCES,0.8840579710144928,"d). With these deﬁnitions, we
have the following orthogonal decomposition"
REFERENCES,0.884963768115942,"L2(Sd−1(
√"
REFERENCES,0.8858695652173914,"d), τd) = ∞
M"
REFERENCES,0.8867753623188406,"ℓ=0
Vd,ℓ.
(50)"
REFERENCES,0.8876811594202898,The dimension of each subspace is given by
REFERENCES,0.8885869565217391,"dim(Vd,ℓ) = B(d, ℓ) = 2ℓ+ d −2 d −2"
REFERENCES,0.8894927536231884,"ℓ+ d −3
ℓ"
REFERENCES,0.8903985507246377,"
.
(51)"
REFERENCES,0.8913043478260869,"For each ℓ∈Z≥0, the spherical harmonics {Y (d)
ℓ,j }1≤j≤B(d,ℓ) form an orthonormal basis of Vd,ℓ:"
REFERENCES,0.8922101449275363,"⟨Y (d)
ki , Y (d)
sj ⟩L2 = δijδks.
Note that our convention is different from the more standard one, that deﬁnes the spherical harmonics
as functions on Sd−1(1). It is immediate to pass from one convention to the other by a simple scaling.
We will drop the superscript d and write Yℓ,j = Y (d)
ℓ,j whenever clear from the context."
REFERENCES,0.8931159420289855,"We denote by Pk the orthogonal projections to Vd,k in L2(Sd−1(
√"
REFERENCES,0.8940217391304348,"d), τd). This can be written in
terms of spherical harmonics as"
REFERENCES,0.894927536231884,Pkf(x) ≡
REFERENCES,0.8958333333333334,"B(d,k)
X"
REFERENCES,0.8967391304347826,"l=1
⟨f, Ykl⟩L2Ykl(x).
(52)"
REFERENCES,0.8976449275362319,"We also deﬁne P≤ℓ≡Pℓ
k=0 Pk, P>ℓ≡I−P≤ℓ= P∞
k=ℓ+1 Pk, and P<ℓ≡P≤ℓ−1, P≥ℓ≡P>ℓ−1."
REFERENCES,0.8985507246376812,Under review as a conference paper at ICLR 2022
REFERENCES,0.8994565217391305,"G.3
GEGENBAUER POLYNOMIALS"
REFERENCES,0.9003623188405797,"The ℓ-th Gegenbauer polynomial Q(d)
ℓ
is a polynomial of degree ℓ. Consistently with our convention
for spherical harmonics, we view Q(d)
ℓ
as a function Q(d)
ℓ
: [−d, d] →R. The set {Q(d)
ℓ}ℓ≥0 forms
an orthogonal basis on L2([−d, d], ˜τ 1
d), where ˜τ 1
d is the distribution of
√"
REFERENCES,0.9012681159420289,"d⟨x, e1⟩when x ∼τd,
satisfying the normalization condition:"
REFERENCES,0.9021739130434783,"⟨Q(d)
k (
√"
REFERENCES,0.9030797101449275,"d⟨e1, ·⟩), Q(d)
j (
√"
REFERENCES,0.9039855072463768,"d⟨e1, ·⟩)⟩L2(Sd−1(
√"
REFERENCES,0.904891304347826,"d)) =
1
B(d, k) δjk .
(53)"
REFERENCES,0.9057971014492754,"In particular, these polynomials are normalized so that Q(d)
ℓ(d) = 1. As above, we will omit the
superscript (d) in Q(d)
ℓ
when clear from the context."
REFERENCES,0.9067028985507246,"Gegenbauer polynomials are directly related to spherical harmonics as follows. Fix v ∈Sd−1(
√"
REFERENCES,0.907608695652174,"d)
and consider the subspace of Vℓformed by all functions that are invariant under rotations in Rd that
keep v unchanged. It is not hard to see that this subspace has dimension one, and coincides with the
span of the function Q(d)
ℓ(⟨v, · ⟩)."
REFERENCES,0.9085144927536232,We will use the following properties of Gegenbauer polynomials
REFERENCES,0.9094202898550725,"1. For x, y ∈Sd−1(
√ d)"
REFERENCES,0.9103260869565217,"⟨Q(d)
j (⟨x, ·⟩), Q(d)
k (⟨y, ·⟩)⟩L2 =
1
B(d, k)δjkQ(d)
k (⟨x, y⟩).
(54)"
REFERENCES,0.9112318840579711,"2. For x, y ∈Sd−1(
√ d)"
REFERENCES,0.9121376811594203,"Q(d)
k (⟨x, y⟩) =
1
B(d, k)"
REFERENCES,0.9130434782608695,"B(d,k)
X"
REFERENCES,0.9139492753623188,"i=1
Y (d)
ki (x)Y (d)
ki (y).
(55)"
REFERENCES,0.9148550724637681,"These properties imply that, up to a constant, Q(d)
k (⟨x, y⟩) is a representation of the projector onto
the subspace of degree-k spherical harmonics"
REFERENCES,0.9157608695652174,"(Pkf)(x) = B(d, k)
Z"
REFERENCES,0.9166666666666666,"Sd−1(
√"
REFERENCES,0.917572463768116,"d)
Q(d)
k (⟨x, y⟩) f(y) τd(dy) .
(56)"
REFERENCES,0.9184782608695652,"For a function σ ∈L2([−
√ d,
√"
REFERENCES,0.9193840579710145,"d], τ 1
d) (where τ 1
d is the distribution of ⟨e1, x⟩when x ∼
Unif(Sd−1(
√"
REFERENCES,0.9202898550724637,"d))), denoting its spherical harmonics coefﬁcients ξd,k(σ) to be"
REFERENCES,0.9211956521739131,"ξd,k(σ) =
Z [−
√ d,
√"
REFERENCES,0.9221014492753623,"d]
σ(x)Q(d)
k (
√"
REFERENCES,0.9230072463768116,"dx)τ 1
d(dx),
(57)"
REFERENCES,0.9239130434782609,"then we have the following equation holds in L2([−
√ d,
√"
REFERENCES,0.9248188405797102,"d], τ 1
d) sense"
REFERENCES,0.9257246376811594,"σ(x) = ∞
X"
REFERENCES,0.9266304347826086,"k=0
ξd,k(σ)B(d, k)Q(d)
k (
√ dx)."
REFERENCES,0.927536231884058,"For any rotationally invariant kernel Hd(x1, x2)
=
hd(⟨x1, x2⟩/d), with hd(
√"
REFERENCES,0.9284420289855072,"d · )
∈
L2([−
√ d,
√"
REFERENCES,0.9293478260869565,"d], τ 1
d), we can associate a self adjoint operator Hd : L2(Sd−1(
√"
REFERENCES,0.9302536231884058,"d)) →L2(Sd−1(
√ d))"
REFERENCES,0.9311594202898551,"Hdf(x) ≡
Z"
REFERENCES,0.9320652173913043,"Sd−1(
√"
REFERENCES,0.9329710144927537,"d)
hd(⟨x, x1⟩/d) f(x1) τd(dx1) .
(58)"
REFERENCES,0.9338768115942029,"By rotational invariance, the space Vk of homogeneous polynomials of degree k is an eigenspace
of Hd, and we will denote the corresponding eigenvalue by ξd,k(hd). In other words Hdf(x) ≡
P∞
k=0 ξd,k(hd)Pkf. The eigenvalues can be computed via"
REFERENCES,0.9347826086956522,"ξd,k(hd) =
Z [−
√ d,
√"
REFERENCES,0.9356884057971014,"d]
hd
 
x/
√"
REFERENCES,0.9365942028985508,"d

Q(d)
k (
√"
REFERENCES,0.9375,"dx)τ 1
d(dx) .
(59)"
REFERENCES,0.9384057971014492,Under review as a conference paper at ICLR 2022
REFERENCES,0.9393115942028986,"For a dot product kernel Hd(x, y) = hd(⟨x, y⟩/d) consider the Gegenbauer expansion of hd in
L2([−
√ d,
√"
REFERENCES,0.9402173913043478,"d], τ 1
d)"
REFERENCES,0.9411231884057971,"hd(⟨x, y⟩/d) = ∞
X"
REFERENCES,0.9420289855072463,"k=0
ξk,d(hd)B(d, k)Q(d)
k (⟨x, y⟩).
(60)"
REFERENCES,0.9429347826086957,"Using Eq. (54) we can equivalently write the kernel as an expectation over random features for some
activation σd
hd(⟨x, y⟩/d) = Ew∼Unif(Sd−1)[σd(⟨w, x⟩)σd(⟨w, y⟩)]
(61)"
REFERENCES,0.9438405797101449,by taking
REFERENCES,0.9447463768115942,"σd(x) = ∞
X"
REFERENCES,0.9456521739130435,"k=0
ξd,k(hd)1/2B(d, k)Q(d)
k (
√"
REFERENCES,0.9465579710144928,"dx).
(62)"
REFERENCES,0.947463768115942,"Note that σd ∈L2([−
√ d,
√"
REFERENCES,0.9483695652173914,"d], τ 1
d) as long as h(1) < ∞."
REFERENCES,0.9492753623188406,"G.4
HERMITE POLYNOMIALS"
REFERENCES,0.9501811594202898,"The Hermite polynomials {Hek}k≥0 form an orthogonal basis of L2(R, γ), where γ(dx) =
e−x2/2dx/
√"
REFERENCES,0.9510869565217391,"2π is the standard Gaussian measure, and Hek has degree k. We will follow the classi-
cal normalization (here and below, expectation is with respect to G ∼N(0, 1)):"
REFERENCES,0.9519927536231884,"E

Hej(G) Hek(G)
	
= k! δjk .
(63)"
REFERENCES,0.9528985507246377,"As a consequence, for any function g ∈L2(R, γ), we have the decomposition"
REFERENCES,0.9538043478260869,"g(x) = ∞
X k=0 µk(g)"
REFERENCES,0.9547101449275363,"k!
Hek(x) ,
µk(g) ≡E

g(G) Hek(G)} .
(64)"
REFERENCES,0.9556159420289855,"The Hermite polynomials can be obtained as high-dimensional limits of the Gegenbauer polyno-
mials introduced in the previous section. Indeed, the Gegenbauer polynomials (up to a
√"
REFERENCES,0.9565217391304348,"d scaling
in domain) are constructed by Gram-Schmidt orthogonalization of the monomials {xk}k≥0 with
respect to the measure ˜τ 1
d, while Hermite polynomial are obtained by Gram-Schmidt orthogonaliza-
tion with respect to γ. Since ˜τ 1
d ⇒γ (here ⇒denotes weak convergence), it is immediate to show
that, for any ﬁxed integer k,"
REFERENCES,0.957427536231884,"lim
d→∞Coeﬀ{Q(d)
k (
√"
REFERENCES,0.9583333333333334,"dx) B(d, k)1/2} = Coeﬀ

1
(k!)1/2 Hek(x)

.
(65)"
REFERENCES,0.9592391304347826,"Here and below, for P a polynomial, Coeﬀ{P(x)} is the vector of the coefﬁcients of P. As a
consequence, for any ﬁxed integer k, we have"
REFERENCES,0.9601449275362319,"µk(σ) = lim
d→∞ξd,k(σ)(B(d, k)k!)1/2,
(66)"
REFERENCES,0.9610507246376812,"where µk(σ) and ξd,k(σ) are given in Eq. (64) and Eq. (57)."
REFERENCES,0.9619565217391305,"G.5
THE INVARIANT FUNCTION CLASS AND THE SYMMETRIZATION OPERATOR"
REFERENCES,0.9628623188405797,"Let Gd be a group that is isomorphic to a subgroup of O(d), the orthogonal group in d dimension.
That means, each element of Gd can be identiﬁed with a matrix in O(d) ⊆Rd×d, and the group
addition operation in Gd can be regarded as matrix multiplications in O(d). For any x ∈Sd−1(
√"
REFERENCES,0.9637681159420289,"d)
and g ∈Gd, we deﬁne group action g · x to be the multiplication of matrix representation of g
with the vector x. We equip Gd with a probability measure πd, which is the uniform probability
measure on Gd. More speciﬁcally, the Borel sigma algebra on Gd is deﬁned as the Borel sigma
algebra of O(d) restricted on Gd. The uniform probability measure πd satisﬁes the property that, for
any Borel-measurable set B ⊆Gd and any g ∈Gd, we have"
REFERENCES,0.9646739130434783,πd(B) = πd(gB).
REFERENCES,0.9655797101449275,Under review as a conference paper at ICLR 2022
REFERENCES,0.9664855072463768,"Let L2(Sd−1(
√"
REFERENCES,0.967391304347826,"d)) be the class of L2 functions on Sd−1(
√"
REFERENCES,0.9682971014492754,"d) equipped with uniform probability
measure Unif(Sd−1(
√"
REFERENCES,0.9692028985507246,d)). We deﬁne the invariant function class to be
REFERENCES,0.970108695652174,"L2(Sd−1(
√"
REFERENCES,0.9710144927536232,"d), Gd) =
n
f ∈L2(Sd−1(
√"
REFERENCES,0.9719202898550725,"d)) : f(x) = f(g · x), ∀x ∈Sd−1(
√"
REFERENCES,0.9728260869565217,"d), ∀g ∈Gd
o
."
REFERENCES,0.9737318840579711,"We deﬁne the symmetrization operator S : L2(Sd−1(
√"
REFERENCES,0.9746376811594203,"d)) →L2(Sd−1(
√"
REFERENCES,0.9755434782608695,"d), Gd) to be"
REFERENCES,0.9764492753623188,"(Sf)(x) =
Z"
REFERENCES,0.9773550724637681,"Gd
f(g · x)πd(dg)."
REFERENCES,0.9782608695652174,"G.6
ORTHOGONAL POLYNOMIALS ON INVARIANT FUNCTION CLASS"
REFERENCES,0.9791666666666666,"We deﬁne Vd,≤k ⊆L2(Sd−1(
√"
REFERENCES,0.980072463768116,"d)) to be the subspace spanned by all the degree ℓpolynomials,
Vd,>k ≡V ⊥
d,≤k ⊆L2(Sd−1(
√"
REFERENCES,0.9809782608695652,"d)) to be the orthogonal complement of Vd,≤k, and Vd,k = Vd,≤k ∩
V ⊥
d,≤k−1. In words, Vd,k contains all degree k polynomials that orthogonal to all polynomials of
degree at most k −1. We further deﬁne Vd,<k = Vd,≤k−1 and Vd,≥k = Vd,>k−1."
REFERENCES,0.9818840579710145,"Let P≤ℓto be the projection operator on L2(Sd−1(
√"
REFERENCES,0.9827898550724637,"d), Unif) that project a function onto Vd,≤ℓ,
the space spanned by all the degree ℓpolynomials. Then it is easy to see that P≤ℓand S operator
commute. This means, for any f ∈L2(Sd−1(
√"
REFERENCES,0.9836956521739131,"d)), we have"
REFERENCES,0.9846014492753623,P≤ℓ[S(f)] = S[P≤ℓ(f)].
REFERENCES,0.9855072463768116,"Similarly, we can deﬁne Pℓ, P<ℓ, P>ℓ, P≥ℓ, which commute with S. We denote Vd,ℓ(Gd) ≡
Pℓ(Sd−1(
√"
REFERENCES,0.9864130434782609,"d), Gd) to be the space of polynomials in the images of PℓS. Then we have"
REFERENCES,0.9873188405797102,"Pℓ(Ad, Gd) = Pℓ(L2(Sd−1(
√"
REFERENCES,0.9882246376811594,"d), Gd)) = S[Pℓ(L2(Ad))]."
REFERENCES,0.9891304347826086,"We denote D(d; k) = D(Sd−1(
√"
REFERENCES,0.990036231884058,"d); Gd; k) ≡dim(Pk(Sd−1(
√"
REFERENCES,0.9909420289855072,"d), Gd)) to be the dimension of"
REFERENCES,0.9918478260869565,"Pk(Sd−1(
√"
REFERENCES,0.9927536231884058,"d), Gd). We denote {Y
(d)
kl }l∈[D(Sd−1(
√"
REFERENCES,0.9936594202898551,d);k)] to be a set of orthonormal polynomial basis
REFERENCES,0.9945652173913043,"in Pk(Sd−1(
√"
REFERENCES,0.9954710144927537,"d), Gd). That means"
REFERENCES,0.9963768115942029,"Ex∼Unif(Sd−1(
√"
REFERENCES,0.9972826086956522,"d))[Y
(d)
k1l1(x)Y
(d)
k2l2(x)] = 1{k1 = k2, l1 = l2}, and"
REFERENCES,0.9981884057971014,"Y
(d)
kl (x) = Y
(d)
kl (g · x), ∀x ∈Sd−1(
√"
REFERENCES,0.9990942028985508,"d), ∀g ∈Gd."
