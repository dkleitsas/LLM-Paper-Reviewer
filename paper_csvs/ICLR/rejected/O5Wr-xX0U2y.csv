Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00423728813559322,"Motivated by the application of equal-risk pricing and hedging of a ﬁnancial
derivative, where two operationally meaningful hedging portfolio policies needs
to be found that minimizes coherent risk measures, we propose in this paper a
novel deep reinforcement learning algorithm for solving risk-averse dynamic de-
cision making problems. Prior to our work, such hedging problems can either only
be solved based on static risk measures, leading to time-inconsistent policies, or
based on dynamic programming solution schemes that are impracticable in re-
alistic settings. Our work extends for the ﬁrst time the deep deterministic policy
gradient algorithm, an off-policy actor-critic reinforcement learning (ACRL) algo-
rithm, to solving dynamic problems formulated based on time-consistent dynamic
expectile risk measure. Our numerical experiments conﬁrm that the new ACRL
algorithm produces high quality solutions to equal-risk pricing and hedging prob-
lems and that its hedging strategy outperforms the strategy produced using a static
risk measure when the risk is evaluated at later points of time."
INTRODUCTION,0.00847457627118644,"1
INTRODUCTION"
INTRODUCTION,0.012711864406779662,"This paper considers solving risk-averse dynamic decision making problems arising from applica-
tions where risk needs to be evaluated according to risk measures that are coherent. In particular, we
draw our motivation from the ﬁnancial application of equal-risk pricing (ERP) and hedging (Guo &
Zhu (2017)), where two dynamic hedging problems need to be solved, one for the buyer and one for
the seller of a ﬁnancial derivative (a.k.a option), for determining a fair transaction price that would
expose both parties to the same amount of hedging risk. The need to meaningfully model each
party’s best hedging decision in a ﬁnancial market, namely that no arbitrage is allowed, and to have
a meaningful comparison between the two parties’ risk exposures, namely that the risks should be
measured in the same units, has led to the use of coherent risk measures for capturing both parties’
hedging risks in this application (see Marzban et al. (2020))."
INTRODUCTION,0.01694915254237288,"To this date, most solution methods proposed for solving risk-averse dynamic decision making prob-
lems under a coherent risk measure have either relied on traditional dynamic programming (DP),
which suffers from the curse of dimensionality and assumes the knowledge of a stochastic model
that precisely captures the dynamics of the decision environment, or on the use of a static risk mea-
sure, i.e., that disregards the temporal structure of the random variable (e.g. Marzban et al. (2020),
Carbonneau & Godin (2020), and Carbonneau & Godin (2021) in the case of the ERP application).
The latter raises the serious issue that the resulting policy could be time inconsistent, i.e. that the ac-
tions prescribed by the policy may be considered signiﬁcantly sub-optimal once the state is visited.
In an application such as ERP, this issue implies that policies obtained based on static risk measures
will not be implemented in practice, raising the need to consider dynamic risk measures."
INTRODUCTION,0.0211864406779661,"Focusing on deep reinforcement learning (DRL) methods, while there has been a large number of
approaches proposed to address risk averse Markov decision processes (MDPs) using coherent risk
measures, to the best of our knowledge, all of them, except for two exceptions, consider a static
risk measure (see Prashanth & Ghavamzadeh (2013); Chow & Ghavamzadeh (2014); Castro et al.
(2019); Singh et al. (2020); Urp´ı et al. (2021)) and therefore suffer from time-inconsistency. The"
INTRODUCTION,0.025423728813559324,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.029661016949152543,"two exceptions consist of Tamar et al. (2015) and Huang et al. (2021) who propose actor-critic re-
inforcement learning (ACRL) algorithms to deal with a general dynamic law-invariant coherent risk
measures. Unfortunately, the two algorithms respectively either assume that it is possible to generate
samples from a perturbed version of the dynamics, or rely on training three neural networks (namely
a state distribution reweighting network, a transition perturbation network, and a Lagrangean penal-
isation network) concurrently with the actor and critic networks. Furthermore, only Huang et al.
(2021) actually implemented their method. This was done on a toy tabular problem involving 12
states and 4 actions where it produced questionable performances1."
INTRODUCTION,0.03389830508474576,"In this paper, we develop a new model-free ACRL algorithm for solving a time-consistent risk
averse MDP under a dynamic expectile risk measure.2 Overall, we may summarize the contribution
as follows:"
INTRODUCTION,0.038135593220338986,• Our ACRL algorithm is the ﬁrst to naturally extend the popular model-free deep deter-
INTRODUCTION,0.0423728813559322,"ministic policy gradient algorithm (DDPG) (see Lillicrap et al. (2015)) to a risk averse
setting where a time consistent coherent risk measure is used. Unlike the ACRL proposed
in Huang et al. (2021), which employs ﬁve neural networks, our algorithm will only re-
quire an actor and a critic network. While our policy network will be trained following
a stochastic gradient procedure similar to Silver et al. (2014), we are the ﬁrst to leverage
the elicitability property of expectile risk measures to propose a procedure for training the
“risk-to-go” deep Q-network that is also based on stochastic gradient descent.
• Our ACRL is the ﬁrst model-free DRL-based algorithm capable of identifying optimal risk"
INTRODUCTION,0.046610169491525424,"averse option hedging strategies that are time-consistent with respect to a dynamic coherent
risk measure, and of computing their associated equal risk prices. A side beneﬁt of time-
consistency will be that after training for an option with a given maturity, one obtains equal
risk prices and hedging strategies for any other shorter maturities. While our algorithm
certainly has a broader set of applications, we believe that ERP constitutes an original and
fertile application in which to develop and test new risk averse DRL methods.
• We evaluate the training efﬁciency and the quality of solution, in terms of quality of option"
INTRODUCTION,0.05084745762711865,"hedging strategies and of estimated equal risk prices, obtained using our ACRL algorithm
on a synthetic multi-asset geometric Brownian motion market model. These experiments
constitute the ﬁrst real application of a risk averse DRL algorithm that employs a dynamic
coherent risk measure."
INTRODUCTION,0.05508474576271186,"The rest of this paper is organized as follows. Section 2 introduces equal risk pricing and its asso-
ciated DP equations. Section 3 proposes the new ACRL algorithm for general ﬁnite horizon risk
averse MDP with dynamic expectile measures. Finally, Section 4 presents and discusses our numer-
ical experiments. We note that a reader only interested in the ACRL algorithm can skip right ahead
to Section 3."
INTRODUCTION,0.059322033898305086,"2
APPLICATION: EQUAL RISK PRICING AND HEDGING UNDER DYNAMIC
EXPECTILE RISK MEASURES"
INTRODUCTION,0.0635593220338983,"As described in Marzban et al. (2020), the problem of ERP can be formalized as follows. Consider
a frictionless market, i.e. no transaction cost, tax, etc, that contains m risky assets, and a risk-free
bank account with zero interest rate. Let St : ⌦! Rm denote the values of the risky assets adapted
to a ﬁltered probability space (⌦, F, F := {Ft}T"
INTRODUCTION,0.06779661016949153,"t=0, P), i.e. each St is Ft measurable. It is assumed
that St is a locally bounded real-valued semi-martingale process and that the set of equivalent local
martingale measures is non-empty (i.e. no arbitrage opportunity). The set of all admissible self-
ﬁnancing hedging strategies with the initial capital p0 2 R is shown by X(p0):"
INTRODUCTION,0.07203389830508475,X(p0) = (
INTRODUCTION,0.07627118644067797,X : ⌦! RT
INTRODUCTION,0.08050847457627118,"""""""""""9{⇠t}T −1"
INTRODUCTION,0.0847457627118644,"t=0 ,
Xt = p0 + t−1
X t0=0 ⇠>"
INTRODUCTION,0.08898305084745763,"t0 ∆St0+1,
8t = 1, . . . , T ) ,"
INTRODUCTION,0.09322033898305085,"1At the time of writing this paper, the risk averse implementation of this algorithm reported in Huang et al.
(2021) is unable to recommend an optimal policy in a deterministic setting, while the risk neutral implementa-
tion produces policies that are outperformed by risk averse ones in a stochastic setting."
INTRODUCTION,0.09745762711864407,"2Our ACRL algorithm exploits the elicitabilty property of expectile risk measures, which is the only elic-
itable coherent risk measure."
INTRODUCTION,0.1016949152542373,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.1059322033898305,"where ∆St+1 := St+1 −St, the hedging strategy ⇠t 2 Rm is a vector of random variables adapted
to the ﬁltration F and captures the number of shares of each risky asset held in the portfolio during
the period [t, t + 1], and Xt is the accumulated wealth."
INTRODUCTION,0.11016949152542373,Let F({St}T
INTRODUCTION,0.11440677966101695,"t=1) denote the payoff of a derivative. Throughout this paper, we assume F({St}T"
INTRODUCTION,0.11864406779661017,"t=1)
admits the formulation of F(ST , YT ) where Yt is an auxiliary ﬁxed-dimensional stochastic process
that is Ft-measurable. This class of payoff functions is common in the literature, (see for example
Bertsimas et al. (2001) and Marzban et al. (2020)). The problem of ERP is deﬁned based on the
following two hedging problems that seek to minimize the risk of hedging strategies, one is for the
writer and the other is for the buyer of the derivative:"
INTRODUCTION,0.1228813559322034,"(Writer)
%w(p0) =
inf
X2X(p0) ⇢w(F(ST , YT ) −XT )
(1)"
INTRODUCTION,0.1271186440677966,"(Buyer)
%b(p0) =
inf
X2X(−p0) ⇢b(−F(ST , YT ) −XT ) ,
(2)"
INTRODUCTION,0.13135593220338984,"where ⇢w and ⇢b are two risk measures that capture respectively the writer and the buyer’s risk
aversion. In words, equation (1) describes a writer that is receiving p0 as the initial payment and
implements an optimal hedging strategy for the liability F(ST , YT ). On the other hand, in (2) the
buyer is assumed to borrow p0 in order to pay for the option and then to manage a portfolio that
will minimize the risks associated to his ﬁnal wealth. With equations (1) and (2), ERP deﬁnes a
fair price p⇤"
INTRODUCTION,0.13559322033898305,"0 as the value of an initial capital that leads to the same risk exposure to both parties, i.e.
%w(p⇤"
INTRODUCTION,0.13983050847457626,0) = %b(p⇤ 0).
INTRODUCTION,0.1440677966101695,"In particular, based on Proposition 3.1 and the examples presented in section 3.3 of Marzban et al.
(2020), together with the fact that both ⇢w and ⇢b are dynamic recursive law invariant risk measures,
a Markovian assumption allows us to conclude that the ERP can be calculated using two sets of
dynamic programming equations.
Assumption 2.1. [Markov property] There exists a sufﬁcient statistic process  t adapted to
F such that {(St, Yt,  t)}T"
INTRODUCTION,0.1483050847457627,"t=0 is a Markov process relative to the ﬁltration F.
Namely,
P((St+s, Yt+s,  t+s) 2 A|Ft) = P((St+s, Yt+s,  t+s) 2 A|St, Yt,  t) for all t, for all s ≥0,
and all sets A."
INTRODUCTION,0.15254237288135594,"Speciﬁcally, on the writer side, we can deﬁne V w"
INTRODUCTION,0.15677966101694915,"T (ST , YT ,  T ) := F(ST , YT ), and recursively V w"
INTRODUCTION,0.16101694915254236,"t (St, Yt,  t) := inf"
INTRODUCTION,0.1652542372881356,⇠t ¯⇢(−⇠>
INTRODUCTION,0.1694915254237288,t ∆St+1 + V w
INTRODUCTION,0.17372881355932204,"t+1(St + ∆St+1, Yt + ∆Yt+1,  t+1)|St, Yt,  t) ,"
INTRODUCTION,0.17796610169491525,"where ¯⇢(·|St, Yt,  t) is a law invariant risk measure that uses P(·|St, Yt,  t).
This leads to
considering %w(0) = V w"
INTRODUCTION,0.18220338983050846,"0 (S0, Y0,  0).
On the other hand, for the buyer we similarly deﬁne:
V b"
INTRODUCTION,0.1864406779661017,"T (ST , YT ,  T ) := −F(ST , YT ) and V b"
INTRODUCTION,0.1906779661016949,"t (St, Yt,  t) := inf"
INTRODUCTION,0.19491525423728814,⇠t ¯⇢(−⇠>
INTRODUCTION,0.19915254237288135,t ∆St+1 + V b
INTRODUCTION,0.2033898305084746,"t+1(St + ∆St+1, Yt + ∆Yt+1,  t+1)|St, Yt,  t) ,"
INTRODUCTION,0.2076271186440678,with %b(0) = V b
INTRODUCTION,0.211864406779661,"0 (S0, Y0,  0). The following lemma summarizes how DP can be used to compute
the ERP.
Lemma 2.1 (Marzban et al. (2020)). Under Assumption 2.1, the ERP that employs dynamic expec-
tile risk measure can be computed as: p⇤"
INTRODUCTION,0.21610169491525424,0 = (V w
INTRODUCTION,0.22033898305084745,"0 (S0, Y0,  0) −V b"
INTRODUCTION,0.2245762711864407,"0 (S0, Y0,  0))/2."
INTRODUCTION,0.2288135593220339,"In what follows, we will further assume that the risk measure is a dynamic expectile risk measure.
Deﬁnition 2.1. A dynamic expectile risk measure takes the form: ⇢(X) := ¯⇢0(¯⇢1(. . . ¯⇢T −1(X)))
where each ¯⇢(·) is an expectile risk measure that employs the conditional distribution based on Ft.
Namely, ¯⇢t(Xt+1) := arg minq ⌧E ⇥"
INTRODUCTION,0.2330508474576271,(q −Xt+1)2 +|Ft ⇤
INTRODUCTION,0.23728813559322035,+ (1 −⌧)E ⇥
INTRODUCTION,0.24152542372881355,(q −Xt+1)2 −|Ft ⇤
INTRODUCTION,0.2457627118644068,"where Xt+1
is a random liability measureable on Ft+1."
INTRODUCTION,0.25,"Like conditional value at risk, the expectile measure (see Bellini & Bignozzi (2015)) covers the
range of risk attitudes from risk neutrality, when ⌧= 1/2, to worst-case risk, when ⌧! 1."
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.2542372881355932,"3
A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.2584745762711864,"With the dynamic programming equations in hand, it now becomes apparent that each option hedg-
ing problem in ERP can be formulated as a ﬁnite horizon Markov Decision Process (MDP) described"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.2627118644067797,Under review as a conference paper at ICLR 2022
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.2669491525423729,"with (S, A, r, P). In this regard, the agent (i.e. the writer or buyer) interacts with a stochastic en-
vironment by taking an action at ⌘⇠t 2 [−1, 1]m after observing the state st 2 S, which in-
cludes St, Yt, and  t. Note that to simplify exposition, in this section we drop the reference to
the speciﬁc identity (i.e. w or b) of the agent in our notation. The action taken at each time t re-
sults in the immediate stochastic reward that takes the shape of the immediate hedging portfolio
return, i.e. rt(st, at, st+1) := ⇠>"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.2711864406779661,"t ∆St+1 when t < T and otherwise of the option liability/payout
rT (sT , aT , sT +1) := F(ST , YT )(1 −2 · 1{agent=writer}), which is insensitive to sT +1. Fi-
nally, the Markovian exogeneous dynamics described in Assumption 2.1 are modeled using P as
P(st+1|st, at) = P(St+1, Yt+1,  t+1|St, Yt,  t). Overall, each of the two dynamic derivative
hedging problems presented in Section 2 reduce to a version of the following general risk averse
reinforcement learning problem:"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.2754237288135593,"%(0) = V0(S0, Y0,  0) = min ⇡Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.2796610169491525,"0(¯s0, ⇡0(¯s0)) ,"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.2838983050847458,"where ¯s0 := (S0, Y0,  0) is the initial state in which the option is priced while Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.288135593220339,"t (st, at) :=
¯⇢(−rt(st, at, st+1) + Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.2923728813559322,"t+1(st+1, ⇡t+1(st+1))|st), Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.2966101694915254,"T (sT , aT ) := −rT (sT , aT , sT ), and where ¯⇢
is an expectile risk measure, i.e. ¯⇢(X) := arg minq E ⇥"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3008474576271186,⌧(q −X)2
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3050847457627119,+ + (1 −⌧)(q −X)2 − ⇤
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3093220338983051,". Equipped
with these deﬁnitions, we can now motivate our proposed extension of the model-free off-policy
deterministic ACRL algorithm to the general ﬁnite horizon risk-averse MDP setting. In doing so,
we start with a proposition (see Appendix A.1 for a proof) that will provide the motivation for
a stochastic gradient scheme to optimize the actor network, while the optimization of the critic
network will follow from the elicitability property of the expectile risk measure."
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3135593220338983,"Proposition 3.1. Let ¯⇡be an arbitrary reference policy and µ an arbitrary distribution over the
initial state, such that there is a strictly positive probability of reaching all of S for all t ≥1.3 For
any ⇡⇤that satisﬁes"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3177966101694915,⇡⇤2 arg min
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3220338983050847,"⇡E ˜t⇠{0,...,T },s0⇠µ"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.326271186440678,"st+1⇠P (·|st,¯⇡t(st)) [Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3305084745762712,"˜t (s˜t, ⇡˜t(s˜t))]
(3)"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3347457627118644,"where ˜t is uniformly drawn, we necessarily have that ⇡⇤2 arg min Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3389830508474576,"0(¯s0, ⇡0(¯s0)) hence %(0) =
Q⇡⇤"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3432203389830508,"0 (¯s0, ⇡⇤"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3474576271186441,0(¯s0)).
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3516949152542373,"In the context of a deep reinforcement learning approach, we can employ a procedure based on off-
policy deterministic policy gradient (Silver et al., 2014) to optimize (3). Speciﬁcally, given a policy
network ⇡✓, we wish to optimize: min"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3559322033898305,"✓
E
˜t⇠{0,...,T −1}
st+1⇠P (·|st,¯⇡t(st)) [Q⇡✓"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3601694915254237,"˜t (s˜t, ⇡✓"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3644067796610169,"˜t (s˜t))] ,"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3686440677966102,"using a stochastic gradient algorithm. In doing so, we rely on the fact that:"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3728813559322034,"r✓E
˜t⇠{0,...,T −1}
st+1⇠P (·|st,¯⇡t(st)) [Q⇡✓"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3771186440677966,"˜t (s˜t, ⇡✓(s˜t))]"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3813559322033898,"= E
˜t⇠{0,...,T −1}
st+1⇠P (·|st,¯⇡t(st))  r✓Q⇡✓"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3855932203389831,"˜t (s˜t, a) """""" a=⇡✓"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3898305084745763,˜t (s˜t) + raQ⇡✓
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3940677966101695,"˜t (s˜t, a)r✓⇡✓"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.3983050847457627,"˜t (s˜t) """""" a=⇡✓"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4025423728813559,˜t (s˜t) (
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4067796610169492,"⇡E
˜t⇠{0,...,T −1}
st+1⇠P (·|st,¯⇡t(st))  raQ⇡✓"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4110169491525424,"˜t (s˜t, a)r✓⇡✓"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4152542372881356,"˜t (s˜t) """""" a=⇡✓"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4194915254237288,˜t (s˜t) ( .
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.423728813559322,"Note that in the above equation, we have dropped the term that depends on r✓Q⇡✓"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4279661016949153,"˜t
as is commonly
done in off-policy deterministic gradient methods and usually motivated by a result of Degris et al.
(2012), who argue that this approximation preserves the set of local optima in a risk neutral setting,
i.e. ¯⇢(·) := E[·]. While we do consider as an important subject of future research to extend this
motivation to more general risk measures, our numerical experiments (see Section 4.3) will conﬁrm
empirically that the quality of this approximation permits the identiﬁcation of nearly optimal hedging
policies."
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4322033898305085,"3In our option hedging problem, given that st is entirely exogenous, the distribution of st+1 is unaffected
by ¯⇡, which can therefore be chosen arbitrarily. Moreover, µ can put all the mass on ¯s0."
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4364406779661017,Under review as a conference paper at ICLR 2022
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4406779661016949,Given that we do not have access to an exact expression for Q⇡✓
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4449152542372881,"˜t (s˜t, a), this operator needs to be
estimated directly from the training data. Exploiting the fact that ⇢is a utility-based shortfall risk
measure, we get that: Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4491525423728814,"t (st, at) 2 arg min"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4533898305084746,"q
Est+1⇠P (·|st,at)[`(q + rt(st, at, st+1) −Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4576271186440678,"t+1(st+1, ⇡t+1(st+1)))]"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.461864406779661,"where `(y) := (⌧1{y > 0}−(1−⌧)1{y 0})y2 is the score function associated to the ⌧-expectile
risk measure. As explained in Theorem 3.2 of Shen et al. (2014), in a tabular MDP environment one
can apply the following stochastic gradient step:"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4661016949152542,"ˆQt(st, at)  ˆQt(st, at) −↵@`( ˆQt(st, at) + rt(st, at, st+1) −ˆQt+1(st+1, ⇡t+1(st+1)) ,"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4703389830508475,"where @`(y) := 2(⌧max(0, y) −(1 −⌧) max(0, −y)) is the derivative of `(y), within a properly
designed Q-learning algorithm and have the guarantee that ˆQt(st, at) will almost surely converge to
Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4745762711864407,"t (st, at) for all t, st, and at."
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4788135593220339,"In the non-tabular setting, we replace ˆQ⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4830508474576271,"t (st, at) with two estimators: i.e.
the “main” net-
work Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4872881355932203,"t (st, at|✓Q) for the immediate conditional risk and the “target” network Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4915254237288136,"t (st, at|✓Q0)
for the next period’s conditional risk.
The procedure consists in iterating between a step that
attempts to make the main network Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.4957627118644068,"t (st, at|✓Q) a good estimator of ⇢(−r(st, at, st+1) +
Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.5,"t+1(st+1, at+1|✓Q0)) and a step that replaces the target network Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.5042372881355932,"t (st, at|✓Q0) with a network
more similar to the main one Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.5084745762711864,"t (st, at|✓Q). The former is achieved, similarly as with the policy
network, by searching for the optimal ✓Q according to: min"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.5127118644067796,"✓Q E
˜t⇠{0,...,T −1}
st+1⇠P (·|st,¯⇡t(st)) [`(Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.5169491525423728,"˜t (s˜t, ¯⇡˜t(s˜t)|✓Q)+rt(s˜t, ¯⇡˜t(s˜t), s˜t+1)−Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.5211864406779662,"˜t+1(s˜t+1, ⇡˜t+1(s˜t+1)|✓Q0))] ,"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.5254237288135594,"which suggests a stochastic gradient update of the form ✓Q  ✓Q −↵∆, where ∆is @`(Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.5296610169491526,"˜t (s˜t, ¯⇡˜t(s˜t)|✓Q)+rt(s˜t, ¯⇡˜t(s˜t), s˜t+1)−Q⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.5338983050847458,"˜t+1(s˜t+1, ⇡˜t+1(s˜t+1)|✓Q0))r✓QQ⇡"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.538135593220339,"˜t (s˜t, ¯⇡˜t(s˜t)|✓Q) ."
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.5423728813559322,"These two types of updates are integrated in our proposed expectile-based actor-critic deep RL (a.k.a.
ACRL) algorithm. A ﬁrst version, Algorithm 1, is designed for a simulation-based environment.
One may note that in each episode, the reference policy ¯⇡t is updated to be a perturbed version of the
main policy network in order to focus the accuracy of the main critic network’s value and derivatives
on actions that are more likely to be produced by the main policy network. We also choose to update
the target networks using convex combinations operations as is done in Lillicrap et al. (2015) in
order to improve stability of learning. A second more general version of ACRL, which mimics the
original DDPG, by generating minibatches using a replay buffer can also be found in Appendix A.2."
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.5466101694915254,"We
ﬁnally
note
that
in
our
problem,
P(st+1|st, at)
=
P(st+1|st, a0"
"A NOVEL ACTOR-CRITIC ALGORITHM FOR RISK AVERSE MDP UNDER A
DYNAMIC EXPECTILE RISK MEASURE",0.5508474576271186,"t)
=
P(St+1, Yt+1,  t+1|St, Yt,  t), meaning that the action is not affecting the distribution of
state in the next period. This is a direct consequence of using a translation invariant risk measure,
which eliminates the need to keep track of the accumulated wealth in the set of state variables as
explained in Marzban et al. (2020) and allows the reward function to provide an immediate signal
regarding the quality of implemented actions. In the context of our deep reinforcement learning
approach, we observed that convergence speed is signiﬁcantly improved in training due to this
property (see Figure 4 in Appendix)."
EXPERIMENTAL RESULTS,0.5550847457627118,"4
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.559322033898305,"In this section we provide two different sets of experiments that are run over one vanilla and one
basket option. We will compare both algorithmic efﬁciency and quality, in terms of pricing and
hedging strategies, of the dynamic risk model (DRM), which employs a dynamic expectile risk
measure and is solved using our new ACRL algorithm, and the static risk model (SRM), which
employs a static expectile measure and is solved using an AORL algorithm similar to Carbonneau &
Godin (2021). All experiments are done using simulated price processes of ﬁve risky assets: AAPL,
AMZN, FB, JPM, and GOOGL. The price paths are simulated using correlated Brownian motions
considering the empirical mean, variance, and the correlation matrix of ﬁve reference stocks (AAPL,
AMZN, FB, KPM, and GOOGL) over the period that spans from January 2019 to January 2021. In"
EXPERIMENTAL RESULTS,0.5635593220338984,Under review as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.5677966101694916,"Algorithm 1: The actor-critic RL algorithm for the dynamic recursive expectile option hedging
problem (ACRL)"
EXPERIMENTAL RESULTS,0.5720338983050848,"Randomly initialize the main actor and critic networks’ parameters ✓⇡and ✓Q;
Initialize the target actor, ✓⇡0  ✓⇡, and critic, ✓Q0  ✓Q, networks;
for j = 1 : #Episodes do"
EXPERIMENTAL RESULTS,0.576271186440678,"Randomly select t 2 {0, 1, ..., T −1};
Sample a minibatch of N triplets {(si t, ai t, si"
EXPERIMENTAL RESULTS,0.5805084745762712,t+1)}N
EXPERIMENTAL RESULTS,0.5847457627118644,"i=1 from P(·|st, ¯⇡t(st)), where
¯⇡t(st) := ⇡t(st|✓⇡) + N(0, σ);
Set the realized losses yi"
EXPERIMENTAL RESULTS,0.5889830508474576,"t := −rt(si t, ai t, si"
EXPERIMENTAL RESULTS,0.5932203389830508,t+1) + Qt+1(si
EXPERIMENTAL RESULTS,0.597457627118644,"t+1, ⇡t+1(si"
EXPERIMENTAL RESULTS,0.6016949152542372,"t+1|✓⇡0)|✓Q0);
Update the main critic network:"
EXPERIMENTAL RESULTS,0.6059322033898306,✓Q  ✓Q −↵1 N
EXPERIMENTAL RESULTS,0.6101694915254238,"N
X
X
X i=1"
EXPERIMENTAL RESULTS,0.614406779661017,"@`(Qt(si t, ai"
EXPERIMENTAL RESULTS,0.6186440677966102,t|✓Q) −yj
EXPERIMENTAL RESULTS,0.6228813559322034,"t )r✓QQt(si t, ai"
EXPERIMENTAL RESULTS,0.6271186440677966,t|✓Q) ;
EXPERIMENTAL RESULTS,0.6313559322033898,Update the main actor network:
EXPERIMENTAL RESULTS,0.635593220338983,"✓⇡ ✓⇡−↵1 N N
X i=1"
EXPERIMENTAL RESULTS,0.6398305084745762,raQt(si
EXPERIMENTAL RESULTS,0.6440677966101694,"t, a|✓Q)|a=⇡t(si"
EXPERIMENTAL RESULTS,0.6483050847457628,t|✓⇡)r✓⇡⇡t(si
EXPERIMENTAL RESULTS,0.652542372881356,t|✓⇡) ;
EXPERIMENTAL RESULTS,0.6567796610169492,Update the target networks:
EXPERIMENTAL RESULTS,0.6610169491525424,"✓Q0  ↵✓Q + (1 −↵)✓Q0,
✓⇡0  ↵✓⇡+ (1 −↵)✓⇡0 ;
(4) end"
EXPERIMENTAL RESULTS,0.6652542372881356,"both experiments, the maturity of the option will be one year and the hedging portfolios will be
rebalanced on a monthly basis. Table 3 in the appendix provides the descriptive statistics of our
underlying hidden stochastic process."
EXPERIMENTAL RESULTS,0.6694915254237288,"In what follows, we ﬁrst explain the architectures of our ACRL model. Then, the training pro-
cedure of the networks under the dynamic risk measurement is elaborated. Finally, the main nu-
merical results of the paper are presented for pricing and hedging a vanilla, where the precision
of our approach will be empirically demonstrated, and a basket option. All codes are available at
https://anonymous.4open.science/r/ERP-Dynamic-Expectile-RM-4BEA."
ACTOR AND CRITIC NETWORK ARCHITECTURE,0.673728813559322,"4.1
ACTOR AND CRITIC NETWORK ARCHITECTURE"
ACTOR AND CRITIC NETWORK ARCHITECTURE,0.6779661016949152,"Our implementation of the ACRL algorithm involves two simple networks presented in Figure 1.
Since the underlying assets follow a Brownian motion, the actor and critic networks can deﬁne
the input state as the logarithm of the cumulative returns of each asset and the time remaining to
maturity (i.e. dimension = m + 1). The actor network is composed of three fully connected layers
where the number of neurons are considered to be k = 32 in the ﬁrst two layers and then maps back
to the number of assets in the last layer to generate the investment policy accordingly for each asset.
The activation functions in our networks are considered to be tanh functions. In the last layer, this
implies that the actions will lie in [−1, 1]m. The critic network only concatenates the m dimensional
action information vector after its third layer. In the case of SRM, only the actor network is used."
TRAINING PROCEDURE AND LEARNING CURVES,0.6822033898305084,"4.2
TRAINING PROCEDURE AND LEARNING CURVES"
TRAINING PROCEDURE AND LEARNING CURVES,0.6864406779661016,"Recall that in an SRM setting, overﬁtting of any DRL algorithm can be controlled by measuring the
performance of the trained policy on a validation data set using an empirical estimate of the risk-
averse objective as validation score. Unfortunately, this is no longer possible in the case of DRMs
since the risk measure relies on conditional risk measurements of the trajectories produced by our
policy. In theory, estimates of such conditional measurements could be obtained by training a new
critic network using the validation set (while maintaining the policy ﬁxed to the trained one). In
practice, this is highly computationally demanding to perform in the training stage and raises a new"
TRAINING PROCEDURE AND LEARNING CURVES,0.690677966101695,Under review as a conference paper at ICLR 2022
TRAINING PROCEDURE AND LEARNING CURVES,0.6949152542372882,Figure 1: The architecture of the actor and critic networks in ACRL algorithm.
TRAINING PROCEDURE AND LEARNING CURVES,0.6991525423728814,"issue of how to control overﬁtting of the validation score estimate. Our solution for this problem is
to rely on using static risk measures as validation score, namely a set of static expectiles at risk levels
that are larger or equal to the risk level of the DRM. Figures 2 and 3 in the appendix show examples
of learning curves for the validation performance of DRM and SRM approaches on vanilla and
basket options at a risk level of ⌧= 90%, with a maturity T = 12. SRM appeared to have a faster
rate of convergence than DRM, due to its simpler architecture. Being a time-inconsistent model,
SRM must however be retrained whenever the maturity of the option is modiﬁed. When comparing
convergence rates between vanilla and basket options, we observed similar behavior, which indicates
that the training time might not be very sensitive to the number of assets, thus suffering less from
the curse of dimensionality. We ﬁnally note that both our training and validation sets included 1000
trajectories from the underlying geometric Brownian motion process, implying that the procedure
can be applied in settings where only historical data is available."
VANILLA OPTION HEDGING AND PRICING,0.7033898305084746,"4.3
VANILLA OPTION HEDGING AND PRICING"
VANILLA OPTION HEDGING AND PRICING,0.7076271186440678,"In our ﬁrst set of experiments, we consider pricing and hedging an at-the-money vanilla call option
on AAPL. In this setting, it is possible to obtain (approximately) optimal solutions by dynamic
programming via discretization of the state space. The initial price of AAPL is set to 78.81 and
options with time to maturity ranging from one month to one year are considered. Both DRM and
SRM are trained using a one year maturity/horizon."
VANILLA OPTION HEDGING AND PRICING,0.711864406779661,"With the trained DRM and SRM policy networks, we can evaluate the writer and the buyer’s (out-
of-sample) risk exposure over a pre-speciﬁed time horizon so as to calculate the corresponding ERP.
We consider the following three metrics for measuring the realized risk under different hedging
policy and explain the methods used for calculating the metrics:"
VANILLA OPTION HEDGING AND PRICING,0.7161016949152542,"• Out-of-sample static expectile risk: Given a trained policy, use the test data to calculate the"
VANILLA OPTION HEDGING AND PRICING,0.7203389830508474,static expectile risk. This is the metric that should be minimized by the SRM.
VANILLA OPTION HEDGING AND PRICING,0.7245762711864406,"• RL based out-of-sample dynamic expectile risk estimation: Given the trained policy, use the"
VANILLA OPTION HEDGING AND PRICING,0.7288135593220338,"test data to only train a critic network using ACRL to produce an estimate of out-of-sample
dynamic expectile risk. This is an estimate of the metric minimized by the DRM."
VANILLA OPTION HEDGING AND PRICING,0.7330508474576272,"• DP based out-of-sample dynamic expectile risk estimation: Given a trained policy, evaluate"
VANILLA OPTION HEDGING AND PRICING,0.7372881355932204,"the “true” dynamic expectile risk by solving the dynamic programming equations using a
high precision discretization of the states, actions, and transitions.4 This serves as the true
metric minimized by the DRM."
VANILLA OPTION HEDGING AND PRICING,0.7415254237288136,"We note that our RL based estimate of out-of-sample dynamic risk is a novel approach, which tackles
the important challenge of policy evaluation in RL with dynamic risk measures."
VANILLA OPTION HEDGING AND PRICING,0.7457627118644068,"Table 1 summarizes the evaluations of out-of-sample dynamic risk for DRM policies trained for 1
year maturity at risk level ⌧= 90% then applied to options of different maturities ranging from 12"
VANILLA OPTION HEDGING AND PRICING,0.75,4Note that this metric is available neither for the case of basket option nor in a data-driven environment.
VANILLA OPTION HEDGING AND PRICING,0.7542372881355932,Under review as a conference paper at ICLR 2022
VANILLA OPTION HEDGING AND PRICING,0.7584745762711864,"Table 1: The out-of-sample dynamic and static 90%-expectile risk imposed to the two sides of
vanilla at-the-money call options over AAPL."
VANILLA OPTION HEDGING AND PRICING,0.7627118644067796,"Time to maturity
Policy
Est.†
12
11
10
9
8
· · ·
4
3
2
1
Dynamic 90%-expectile risk"
VANILLA OPTION HEDGING AND PRICING,0.7669491525423728,"Writer’s DRM
RL
0.77
0.73
0.69
0.65
0.62
· · ·
0.45
0.38
0.29
0.23
DP
0.75
0.71
0.68
0.65
0.61
· · ·
0.43
0.38
0.31
0.23"
VANILLA OPTION HEDGING AND PRICING,0.7711864406779662,"Buyer’s DRM
RL
-0.22
-0.21
-0.20
-0.19
-0.18
· · ·
-0.11
-0.09
-0.07
-0.05
DP
-0.23
-0.22
-0.21
-0.20
-0.18
· · ·
-0.12
-0.11
-0.08
-0.06
Static 90%-expectile risk
Writer’s SRM
ED
0.55
0.54
0.54
0.53
0.53
· · ·
0.48
0.46
0.41
0.31
Writer’s DRM
ED
0.56
0.54
0.52
0.50
0.47
· · ·
0.36
0.33
0.29
0.24
Buyer’s SRM
ED
-0.35
-0.33
-0.30
-0.27
-0.23
· · ·
-0.09
-0.07
-0.07
-0.06
Buyer’s SRM
ED
-0.36
-0.34
-0.32
-0.30
-0.28
· · ·
-0.18
-0.14
-0.11
-0.06
Equal risk prices with DRM
True ERP
0.49
0.47
0.45
0.42
0.40
· · ·
0.28
0.24
0.19
0.14
DRM
RL
0.50
0.47
0.45
0.42
0.40
· · ·
0.28
0.24
0.18
0.14
SRM
RL
0.49
0.46
0.44
0.43
0.40
· · ·
0.30
0.27
0.24
0.22"
VANILLA OPTION HEDGING AND PRICING,0.7754237288135594,"† Estimation (Est.) is either made based on reinforcement learning (RL), discretized dynamic programming
(DP), or the empirical distribution (ED)."
VANILLA OPTION HEDGING AND PRICING,0.7796610169491526,"months to 1 months. One can observe that the risk of the writer decreases monotonically for options
of shorter maturities, whereas the risk of the buyer increases monotonically. This is consistent
with the fact that there is less uncertainty for a shorter hedging horizon, which favors the writer’s
risk exposure more than the buyer’s when considering an at-the-money option. This also provides
the evidence that the DRM policies, albeit only trained based on the longest time to maturity, i.e.
one year, can be well applied to hedge options with shorter time to maturity and be used to draw
consistent conclusion. Another important observation one can make is that the RL based out-of-
sample dynamic risk estimate is generally very close to the DP based estimate across all conditions."
VANILLA OPTION HEDGING AND PRICING,0.7838983050847458,"Table 1 also reports the out-of-sample static risk for both SRM policies and DRM policies. The
results are interesting and perhaps surprising. First, the DRM policies outperform SRM policies in
terms of static risk exposure for short maturities, even though they were trained using a different
risk measure. Second, unlike with DRM, we observed at other risk levels (see Figure 6(e) and (f)
in Appendix) that the static risk of SRM policies for the seller (resp. buyer) can increase (resp.
decrease) when hedging an option with shorter maturity. The possibility that a seller’s policy may
actually increase risk when applied to an option with shorter maturity is clearly problematic here as
it is inconsistent with the fact that there is less uncertainty (and lower expected value) regarding the
payout of such options. Both observed phenomenon are consequences of the fact that SRM violates
the time consistency property. We suspect that the possibility that SRM policies may not account
properly for risk aversion at some future time point or for other range of option maturities should
seriously hinder their use in practice."
VANILLA OPTION HEDGING AND PRICING,0.788135593220339,"Finally, Table 1 reports the equal risk prices calculated based on RL based out-of-sample dynamic
risk estimate and based on the discretized DP (referred as True ERP).5 One ﬁrst conﬁrms that the RL
based estimate is of high quality, with a maximum approximation error of 0.01 over all maturities.
Moreover, we can see that the prices for the SRM polices are generally higher than the prices for
the DRM polices, perhaps due to the fact that it is the writer that beneﬁts most from the improved
DRM policy than the buyer, as he is more exposed to tail risks in this transaction. We further refer
the reader to section B.2 of the appendix for additional results regarding the performance of SRM
and DRM in this vanilla option setting."
BASKET OPTION HEDGING AND PRICING,0.7923728813559322,"4.4
BASKET OPTION HEDGING AND PRICING"
BASKET OPTION HEDGING AND PRICING,0.7966101694915254,"In our second set of experiments, we extend the application of ERP pricing framework to the case
of basket options where traditionnal DP solution schemes are not computationally tractable. In"
BASKET OPTION HEDGING AND PRICING,0.8008474576271186,"5Note that in a real data-driven setting, the ERP could either be estimated using the in-sample trained critic
network, or by calculating our RL based estimate using some freshly reserved data to reduce statistical biases."
BASKET OPTION HEDGING AND PRICING,0.8050847457627118,Under review as a conference paper at ICLR 2022
BASKET OPTION HEDGING AND PRICING,0.809322033898305,"particular, we consider an at-the-money basket option with the strike price of 753$ on ﬁve underlying
assets: AAPL, AMZN, FB, JPM, and GOOGL, where the option payoff is determined by the average
price of the underlyings. In this section, dynamic risk is only estimated using the RL based estimator
deﬁned in Section 4.3, given that exact DP resolution has become intractable."
BASKET OPTION HEDGING AND PRICING,0.8135593220338984,"Table 2: The out-of-sample dynamic and static 90%-expectile risk imposed to the two sides of basket
at-the-money call options. Associated ERPs under the DRM are also compared."
BASKET OPTION HEDGING AND PRICING,0.8177966101694916,"Time to maturity
Policy
Est.†
12
11
10
9
8
· · ·
4
3
2
1
Dynamic 90%-expectile risk
Writer’s DRM
RL
3.92
3.62
3.38
3.15
2.95
· · ·
2.00
1.70
1.39
1.10
Buyer’s DRM
RL
-0.48
-0.49
-0.51
-0.52
-0.50
· · ·
-0.47
-0.37
-0.33
-0.29
Static 90%-expectile risk
Writer’s SRM
ED
2.43
2.36
2.28
2.16
2.08
· · ·
1.61
1.45
1.26
0.94
Writer’s DRM
ED
2.38
2.28
2.18
2.06
1.96
· · ·
1.51
1.39
1.20
0.92
Buyer’s SRM
ED
-1.31
-1.24
-1.15
-1.01
-0.94
· · ·
-0.56
-0.48
-0.36
-0.22
Buyer’s SRM
ED
-1.39
-1.32
-1.24
-1.13
-1.07
· · ·
-0.66
-0.56
-0.40
-0.23
Equal risk prices with DRM
DRM
RL
2.20
2.06
1.95
1.84
1.73
· · ·
1.24
1.04
0.86
0.70
SRM
RL
2.23
2.10
2.01
1.91
1.79
· · ·
1.21
1.03
0.92
0.82"
BASKET OPTION HEDGING AND PRICING,0.8220338983050848,"† Estimation (Est.) is either made based on reinforcement learning (RL), or the empirical distribution (ED)."
BASKET OPTION HEDGING AND PRICING,0.826271186440678,"Table 2 presents the dynamic risk obtained from training the DRM policy for a one year maturity
option and applying it on the test data for maturity ranging from 1 to 12 months. Similar to the
vanilla option case, the dynamic risk of the writer is monotonically decreasing as we get closer
to the maturity of the option, while for the writer the monotonic behavior seems to be slightly
perturbed by estimation error. The table also compares the static risk under DRM and SRM. One
can ﬁrst recognize the same monotone convergence to zero of the two sides of the options. However,
contrary to the case of the vanilla option, the difference between the static risk performance of
DRM and SRM policies are rather similar for all maturity times. It therefore appears that in these
experiments with a basket option, both SRM and DRM produce more similar polices. One possible
reason could be that the range of “optimal” risk averse investment plans, whether using DRM or
SRM, is more limited. Indeed, while for the vanilla option, we observed that the optimal policies
generated investments in the range [0, 1] and [-1, 0] for the writer and the buyer respectively, for
the basket option we observed wealth allocations that are more concentrated around 0.20 (i.e. the
uniform portfolio known for its risk hedging properties) and -0.20 for each of the 5 assets asset
respectively. Finally, Table 2 presents the equal risk prices computed based on our RL based out-of-
sample dynamic risk estimator. Once again, the higher ERP price for the SRM policy are notable,
which again can be attributed to the better performing (in terms of dynamic risk) hedging policy
produced by ACRL for the DRM, compared to the policy produced by AORL for the SRM. Further
details are presented in section B.3 of the Appendix."
CONCLUSION,0.8305084745762712,"5
CONCLUSION"
CONCLUSION,0.8347457627118644,"Motivated by the application of ERP, in this paper we considered solving risk averse MDP problems
formulated based on dynamic expectile risk measures, and proposed a novel ACRL algorithm that
extends the model-free off-policy deterministic ACRL algorithm to a general ﬁnite horizon risk-
averse MDP setting. In comparison to existing model-free deep RL methods for solving risk-averse
MDP formulated based on dynamic risk measures, our method is more amenable to practical im-
plementation, allowing for tackling real applications such as the ERP problem. Indeed, as a natural
risk-averse extension of the popular model-free DDPG, our method can easily accommodate any
ﬁnite horizon MDP applications solved by DDPG. More in-depth studies of these other applications
are left for future work. The extension of our method to an inﬁnite horizon MDP setting is also
worth investigating further. Finally, the exploration of our method to accommodate other utility-
based shortfall risk measures should also be of great interest for future study."
CONCLUSION,0.8389830508474576,Under review as a conference paper at ICLR 2022
REFERENCES,0.8432203389830508,REFERENCES
REFERENCES,0.847457627118644,"Fabio Bellini and Valeria Bignozzi. On elicitable risk measures. Quantitative Finance, 15(5):725–"
REFERENCES,0.8516949152542372,"733, 2015."
REFERENCES,0.8559322033898306,"Dimitris Bertsimas, Leonid Kogan, and Andrew W Lo. Hedging derivative securities and incomplete"
REFERENCES,0.8601694915254238,"markets: an ✏-arbitrage approach. Operations research, 49(3):372–397, 2001."
REFERENCES,0.864406779661017,Alexandre Carbonneau and Fr´ed´eric Godin. Equal risk pricing of derivatives with deep hedging.
REFERENCES,0.8686440677966102,"Quantitative Finance, pp. 1–16, 2020."
REFERENCES,0.8728813559322034,Alexandre Carbonneau and Fr´ed´eric Godin. Deep equal risk pricing of ﬁnancial derivatives with
REFERENCES,0.8771186440677966,"multiple hedging instruments. arXiv preprint arXiv:2102.12694, 2021."
REFERENCES,0.8813559322033898,"Dotan Di Castro, J. Oren, and Shie Mannor. Practical risk measures in reinforcement learning."
REFERENCES,0.885593220338983,"ArXiv, abs/1908.08379, 2019."
REFERENCES,0.8898305084745762,Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for CVaR optimization in MDPs. Ad-
REFERENCES,0.8940677966101694,"vances in neural infor- mation processing systems, abs/1406.3339:3509–3517, 2014."
REFERENCES,0.8983050847457628,"Thomas Degris, Martha White, and Richard S. Sutton. Off-policy actor-critic. In Proceedings of the"
REFERENCES,0.902542372881356,"29th International Coference on International Conference on Machine Learning, ICML’12, pp.
179–186, Madison, WI, USA, 2012. Omnipress."
REFERENCES,0.9067796610169492,Ivan Guo and Song-Ping Zhu. Equal risk pricing under convex trading constraints. Journal of
REFERENCES,0.9110169491525424,"Economic Dynamics and Control, 76:136–151, 2017."
REFERENCES,0.9152542372881356,"Audrey Huang, Liu Leqi, Zachary C. Lipton, and Kamyar Azizzadenesheli. On the convergence and"
REFERENCES,0.9194915254237288,"optimality of policy gradient for markov coherent risk, 2021."
REFERENCES,0.923728813559322,"Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,"
REFERENCES,0.9279661016949152,"David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015."
REFERENCES,0.9322033898305084,"Saeed Marzban, Erick Delage, and Jonathan Yumeng Li. Equal risk pricing and hedging of ﬁnancial"
REFERENCES,0.9364406779661016,"derivatives with convex risk measures. arXiv preprint arXiv:2002.02876, 2020."
REFERENCES,0.940677966101695,L.A. Prashanth and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive MDPs.
REFERENCES,0.9449152542372882,"Advances in neural infor- mation processing systems, abs/1406.3339:252–260, 2013."
REFERENCES,0.9491525423728814,Alexander Shapiro. Interchangeability principle and dynamic equations in risk averse stochastic
REFERENCES,0.9533898305084746,"programming. Operations Research Letters, 45(4):377–381, 2017."
REFERENCES,0.9576271186440678,"Yun Shen, Michael J. Tobia, Tobias Sommer, and Klaus Obermayer. Risk-sensitive reinforcement"
REFERENCES,0.961864406779661,"learning. Neural Computation, 26(7):1298–1328, 2014."
REFERENCES,0.9661016949152542,"David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller."
REFERENCES,0.9703389830508474,"Deterministic policy gradient algorithms. In International conference on machine learning, pp.
387–395. PMLR, 2014."
REFERENCES,0.9745762711864406,"Rahul Singh, Qinsheng Zhang, and Yongxin Chen. Improving robustness via risk averse distribu-"
REFERENCES,0.9788135593220338,"tional reinforcement learning. In Alexandre M. Bayen, Ali Jadbabaie, George Pappas, Pablo A.
Parrilo, Benjamin Recht, Claire Tomlin, and Melanie Zeilinger (eds.), Proceedings of the 2nd
Conference on Learning for Dynamics and Control, volume 120 of Proceedings of Machine
Learning Research, pp. 958–968, 2020."
REFERENCES,0.9830508474576272,"Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy gradient for co-"
REFERENCES,0.9872881355932204,"herent risk measures. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.),
Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015."
REFERENCES,0.9915254237288136,"N´uria Armengol Urp´ı, Sebastian Curi, and Andreas Krause. Risk-averse ofﬂine reinforcement learn-"
REFERENCES,0.9957627118644068,"ing. In ICLR 2021: The Ninth International Conference on Learning Representations, 2021."
