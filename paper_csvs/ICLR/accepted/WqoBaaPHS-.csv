Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0010964912280701754,"We propose a new notion of multiclass calibration called top-label calibration. A
classiﬁer is said to be top-label calibrated if the reported probability for the pre-
dicted class label—the top-label—is calibrated, conditioned on the top-label. This
conditioning is essential for practical utility of the calibration property, since the
top-label is always reported and we must condition on what is reported. How-
ever, the popular notion of conﬁdence calibration erroneously skips this condi-
tioning. Furthermore, we outline a multiclass-to-binary (M2B) reduction frame-
work that uniﬁes conﬁdence, top-label, and class-wise calibration, among others.
As its name suggests, M2B works by reducing multiclass calibration to differ-
ent binary calibration problems; various types of multiclass calibration can then
be achieved using simple binary calibration routines. We instantiate the M2B
framework with the well-studied histogram binning (HB) binary calibrator, and
prove that the overall procedure is multiclass calibrated without making any as-
sumptions on the underlying data distribution. In an empirical evaluation with
four deep net architectures on CIFAR-10 and CIFAR-100, we ﬁnd that the M2B
+ HB procedure achieves lower top-label and class-wise calibration error than
other approaches such as temperature scaling. Code for this work is available at
https://github.com/aigen/df-posthoc-calibration."
INTRODUCTION,0.0021929824561403508,"1
INTRODUCTION"
INTRODUCTION,0.003289473684210526,"Machine learning models often make probabilistic predictions. The ideal prediction is the true con-
ditional distribution of the output given the input. However, nature never reveals true probability
distributions, making it infeasible to achieve this ideal in most situations. Instead, there is signiﬁ-
cant interest towards designing models that are calibrated, which is often feasible. We motivate the
deﬁnition of calibration using a standard example of predicting the probability of rain. Suppose a
meteorologist claims that the probability of rain on a particular day is 0.7. Regardless of whether it
rains on that day or not, we cannot know if 0.7 was the underlying probability of rain. However, we
can test if the meteorologist is calibrated in the long run, by checking if on the D days when 0.7 was
predicted, it indeed rained on around 0.7D days (and the same is true for other probabilities)."
INTRODUCTION,0.0043859649122807015,"This example is readily converted to a formal binary calibration setting. Denote a random (feature,
label)-pair as pX, Y q P X ˆ t0, 1u, where X is the feature space. A probabilistic predictor h : X Ñ
r0, 1s is said to be calibrated if for every prediction q P r0, 1s, PrpY “ 1 | hpXq “ qq “ q (almost
surely). Arguably, if an ML classiﬁcation model produces such calibrated scores for the classes,
downstream users of the model can reliably use its predictions for a broader set of tasks."
INTRODUCTION,0.005482456140350877,"Our focus in this paper is calibration for multiclass classiﬁcation, with L ě 3 classes and Y P
rLs :“ t1, 2, . . . , L ě 3u. We assume all (training and test) data is drawn i.i.d. from a ﬁxed
distribution P, and denote a general point from this distribution as pX, Y q „ P. Consider a typical
multiclass predictor, h : X Ñ ∆L´1, whose range ∆L´1 is the probability simplex in RL. A
natural notion of calibration for h, called canonical calibration is the following: for every l P rLs,
PpY “ l | hpXq “ qq “ ql (ql denotes the l-th component of q). However, canonical calibration
becomes infeasible to achieve or verify once L is even 4 or 5 (Vaicenavicius et al., 2019). Thus,
there is interest in studying statistically feasible relaxations of canonical notion, such as conﬁdence
calibration (Guo et al., 2017) and class-wise calibration (Kull et al., 2017)."
INTRODUCTION,0.006578947368421052,Published as a conference paper at ICLR 2022
INTRODUCTION,0.007675438596491228,"In particular, the notion of conﬁdence calibration (Guo et al., 2017) has been popular recently.
A model is conﬁdence calibrated if the following is true: “when the reported conﬁdence for the
predicted class is q P r0, 1s, the accuracy is also q”. In any practical setting, the conﬁdence q is
never reported alone; it is always reported along with the actual class prediction l P rLs. One
may expect that if a model is conﬁdence calibrated, the following also holds: “when the class l is
predicted with conﬁdence q, the probability of the actual class being l is also q”? Unfortunately, this
expectation is rarely met—there exist conﬁdence calibrated classiﬁer for whom the latter statement
is grossly violated for all classes (Example 1). On the other hand, our proposed notion of top-label
calibration enforces the latter statement. It is philosophically more coherent, because it requires
conditioning on all relevant reported quantities (both the predicted top label and our conﬁdence in
it). In Section 2, we argue further that top-label calibration is a simple and practically meaningful
replacement of conﬁdence calibration."
INTRODUCTION,0.008771929824561403,"In Section 3, we unify top-label, conﬁdence, and a number of other popular notions of multiclass
calibration into the framework of multiclass-to-binary (M2B) reductions. The M2B framework re-
lies on the simple observation that each of these notions internally veriﬁes binary calibration claims.
As a consequence, each M2B notion of calibration can be achieved by solving a number of binary
calibration problems. With the M2B framework at our disposal, all of the rich literature on binary
calibration can now be used for multiclass calibration. We illustrate this by instantiating the M2B
framework with the binary calibration algorithm of histogram binning or HB (Zadrozny and Elkan,
2001; Gupta and Ramdas, 2021). The M2B + HB procedure achieves state-of-the-art results with
respect to standard notions of calibration error (Section 4). Further, we show that our procedure is
provably calibrated for arbitrary data-generating distributions. The formal theorems are delayed to
Appendices B, C (due to space limitations), but an informal result is presented in Section 4."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.009868421052631578,"2
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.010964912280701754,"Let c : X Ñ rLs denote a classiﬁer or top-label predictor and h : X Ñ r0, 1s a function that
provides a conﬁdence or probability score for the top-label cpXq. The predictor pc, hq is said to be
conﬁdence calibrated (for the data-generating distribution P) if"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.01206140350877193,"PpY “ cpXq | hpXqq “ hpXq.
(1)"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.013157894736842105,"In other words, when the reported conﬁdence hpXq equals p P r0, 1s, then the fraction of instances
where the predicted label is correct also approximately equals p. Note that for an L-dimensional
predictor h : X Ñ ∆L´1, one would use cp¨q “ arg maxlPrLs hlp¨q and hp¨q “ hcp¨qp¨q; ties are
broken arbitrarily. Then h is conﬁdence calibrated if the corresponding pc, hq satisﬁes (1)."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.01425438596491228,"Conﬁdence calibration is most applicable in high-accuracy settings where we trust the label pre-
diction cpxq. For instance, if a high-accuracy cancer-grade-prediction model predicts a patient as
having “95% grade III, 3% grade II, and 2% grade I”, we would suggest the patient to undergo
an invasive treatment. However, we may want to know (and control) the number of non-grade-III
patients that were given this suggestion incorrectly. In other words, is Prpcancer is not grade III |
cancer is predicted to be of grade III with conﬁdence 95%q equal to 5%? It would appear that by
focusing on the the probability of the predicted label, conﬁdence calibration enforces such control."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.015350877192982455,"However, as we illustrate next, conﬁdence calibration fails at this goal by providing a guarantee that
is neither practically interpretable, nor actionable. Translating the probabilistic statement (1) into
words, we ascertain that conﬁdence calibration leads to guarantees of the form: “if the conﬁdence
hpXq in the top-label is 0.6, then the accuracy (frequency with which Y equals cpXq) is 0.6”. Such
a guarantee is not very useful. Suppose a patient P is informed (based on their symptoms X), that
they are most likely to have a certain disease D with probability 0.6. Further patient P is told that
this score is conﬁdence calibrated. P can now infer the following: “among all patients who have
probability 0.6 of having some unspeciﬁed disease, the fraction who have that unspeciﬁed disease
is also 0.6.” However, P is concerned only about disease D, and not about other diseases. That is,
P wants to know the probability of having D among patients who were predicted to have disease D
with conﬁdence 0.6, not among patients who were predicted to have some disease with conﬁdence
0.6. In other words, P cares about the occurrence of D among patients who were told the same thing
that P has been told. It is tempting to wish that the conﬁdence calibrated probability 0.6 has any
bearing on what P cares about. However, this faith is misguided, as the above reasoning suggests,
and further illustrated through the following example."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.01644736842105263,Published as a conference paper at ICLR 2022
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.017543859649122806,"Example 1. Suppose the instance space is pX, Y q P ta, bu ˆ t1, 2, . . .u. (X can be seen as the
random patient, and Y as the disease they are suffering from.) Consider a predictor pc, hq and let the
values taken by pX, Y, c, hq be as follows:"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.01864035087719298,"Feature x
PpX “ xq
Class prediction cpxq
Conﬁdence hpxq
PpY “ cpXq | X “ xq
a
0.5
1
0.6
0.2
b
0.5
2
0.6
1.0"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.019736842105263157,"The
table
speciﬁes
only
the
probabilities
PpY “ cpXq | X “ xq;
the
probabilities
PpY “ l | X “ xq, l ‰ cpxq, can be set arbitrarily. We verify that pc, hq is conﬁdence calibrated:
PpY “ cpXq | hpXq “ 0.6q “ 0.5pPpY “ 1 | X “ aq ` PpY “ 2 | X “ bqq “ 0.5p0.2 ` 1q “ 0.6.
However, whether the actual instance is X “ a or X “ b, the probabilistic claim of 0.6 bears
no correspondence with reality.
If X “ a, hpXq “ 0.6 is extremely overconﬁdent since
PpY “ 1 | X “ aq “ 0.2. Contrarily, if X “ b, hpXq “ 0.6 is extremely underconﬁdent."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.020833333333333332,"The reason for the strange behavior above is that the probability PpY “ cpXq | hpXqq is not
interpretable from a decision-making perspective. In practice, we never report just the conﬁdence
hpXq, but also the class prediction cpXq (obviously!). Thus it is more reasonable to talk about the
conditional probability of Y “ cpXq, given what is reported, that is both cpXq and hpXq. We make
a small but critical change to (1); we say that pc, hq is top-label calibrated if
PpY “ cpXq | hpXq, cpXqq “ hpXq.
(2)
(See the disambiguating Remark 2 on terminology.) Going back to the patient-disease example,
top-label calibration would tell patient P the following: “among all patients, who (just like you)
are predicted to have disease D with probability 0.6, the fraction who actually have disease D is
also 0.6.” Philosophically, it makes sense to condition on what is reported—both the top label and
its conﬁdence—because that is what is known to the recipient of the information; and there is no
apparent justiﬁcation for not conditioning on both."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.021929824561403508,"A commonly used metric for quantifying the miscalibration of a model is the expected-calibration-
error (ECE) metric. The ECE associated with conﬁdence calibration is deﬁned as
conf-ECEpc, hq :“ EX |PpY “ cpXq | hpXqq ´ hpXq| .
(3)
We deﬁne top-label-ECE (TL-ECE) in an analogous fashion, but also condition on cpXq:
TL-ECEpc, hq :“ EX |PpY “ cpXq | cpXq, hpXqq ´ hpXq| .
(4)
Higher values of ECE indicate worse calibration performance. The predictor in Example 1 has
conf-ECEpc, hq “ 0. However, it has TL-ECEpc, hq “ 0.4, revealing its miscalibration. More gen-
erally, it can be deduced as a straightforward consequence of Jensen’s inequality that conf-ECEpc, hq
is always smaller than the TL-ECEpc, hq (see Proposition 4 in Appendix H). As illustrated by Exam-
ple 1, the difference can be signiﬁcant. In the following subsection we illustrate that the difference
can be signiﬁcant on a real dataset as well. First, we make a couple of remarks.
Remark 1 (ECE estimation using binning). Estimating the ECE requires estimating probabilities
conditional on some prediction such as hpxq. A common strategy to do this is to bin together nearby
values of hpxq using binning schemes (Nixon et al., 2020, Section 2.1), and compute a single esti-
mate for the predicted and true probabilities using all the points in a bin. The calibration method
we espouse in this work, histogram binning (HB), produces discrete predictions whose ECE can
be estimated without further binning. Based on this, we use the following experimental protocol:
we report unbinned ECE estimates while assessing HB, and binned ECE estimates for all other
compared methods, which are continuous output methods (deep-nets, temperature scaling, etc). It
is commonly understood that binning leads to underestimation of the effective ECE (Vaicenavicius
et al., 2019; Kumar et al., 2019). Thus, using unbinned ECE estimates for HB gives HB a disadvan-
tage compared to the binned ECE estimates we use for other methods. (This further strengthens our
positive results for HB.) The binning scheme we use is equal-width binning, where the interval r0, 1s
is divided into B equal-width intervals. Equal-width binning typically leads to lower ECE estimates
compared to adaptive-width binning (Nixon et al., 2020).
Remark 2 (Terminology). The term conf-ECE was introduced by Kull et al. (2019). Most works
refer to conf-ECE as just ECE (Guo et al., 2017; Nixon et al., 2020; Mukhoti et al., 2020; Kumar
et al., 2018). However, some papers refer to conf-ECE as top-label-ECE (Kumar et al., 2019; Zhang
et al., 2020), resulting in two different terms for the same concept. We call the older notion as
conf-ECE, and our deﬁnition of top-label calibration/ECE (4) is different from previous ones."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.023026315789473683,Published as a conference paper at ICLR 2022
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.02412280701754386,"(a)
Conﬁdence
reliability
diagram
(points
marked ‹) and top-label reliability diagram
(points marked `) for a ResNet-50 model on
the CIFAR-10 dataset; see further details in
points (a) and (b) below. The gray bars denote
the fraction of predictions in each bin.
The
conﬁdence
reliability
diagram
(mistakenly)
suggests better calibration than the top-label
reliability diagram."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.025219298245614034,"(b) Class-wise and zoomed-in version of Figure 1a for bin
6 (top) and bin 10 (bottom); see further details in point (c)
below. The ‹ markers are in the same position as Figure 1a,
and denote the average predicted and true probabilities. The
colored points denote the predicted and true probabilities
when seen class-wise. The histograms on the right show the
number of test points per class within bins 6 and 10."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.02631578947368421,Figure 1: Conﬁdence reliability diagrams misrepresent the effective miscalibration.
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.027412280701754384,"2.1
AN ILLUSTRATIVE EXPERIMENT WITH RESNET-50 ON CIFAR-10"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.02850877192982456,"We now compare conﬁdence and top-label calibration using ECE estimates and reliability diagrams
(Niculescu-Mizil and Caruana, 2005). This experiment can be seen as a less malignant version of
Example 1. Here, conﬁdence calibration is not completely meaningless, but can nevertheless be
misleading. Figure 1 illustrates the (test-time) calibration performance of a ResNet-50 model (He
et al., 2016) on the CIFAR-10 dataset (Krizhevsky, 2009). In the following summarizing points, the
pc, hq correspond to the ResNet-50 model."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.029605263157894735,"(a) The ‹ markers in Figure 1a form the conﬁdence reliability diagram (Guo et al., 2017), con-
structed as follows. First, the hpxq values on the test set are binned into one of B “ 10 bins,
r0, 0.1q, r0.1, 0.2q, . . . , r0.9, 1s, depending on the interval to which hpxq belongs. The gray bars
in Figure 1a indicate the fraction of hpxq values in each bin—nearly 92% points belong to bin
r0.9, 1s and no points belong to bin r0, 0.1q. Next, for every bin b, we plot ‹ “ pconfb, accbq,
which are the plugin estimates of E rhpXq | hpXq P Bin bs and PpY “ cpXq | hpXq P Bin bq
respectively. The dashed X “ Y line indicates perfect conﬁdence calibration."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.03070175438596491,"(b) The ` markers in Figure 1a form the top-label reliability diagram. Unlike the conﬁdence
reliability diagram, the top-label reliability diagram shows the average miscalibration across
classes in a given bin. For a given class l and bin b, deﬁne"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.03179824561403509,"∆b,l :“ | pPpY “ cpXq | cpXq “ l, hpXq P Bin bq ´ pE rhpXq | cpXq “ l, hpXq P Bin bs |,"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.03289473684210526,"where pP, pE denote empirical estimates based on the test data. The overall miscalibration is then"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.03399122807017544,"∆b :“ Weighted-averagep∆b,lq “ ř
lPrLs pPpcpXq “ l | hpXq P Bin bq ∆b,l."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.03508771929824561,"Note that ∆b is always non-negative and does not indicate whether the overall miscalibration
occurs due to under- or over-conﬁdence; also, if the absolute-values were dropped from ∆b,l,
then ∆b would simply equal accb ´ confb. In order to plot ∆b in a reliability diagram, we obtain
the direction for the corresponding point from the conﬁdence reliability diagram. Thus for every
‹ “ pconfb, accbq, we plot ` “ pconfb, confb`∆bq if accb ą confb and ` “ pconfb, confb´∆bq
otherwise, for every b. This scatter plot of the `’s gives us the top-label reliability diagram.
Figure 1a shows that there is a visible increase in miscalibration when going from conﬁdence
calibration to top-label calibration. To understand why this change occurs, Figure 1b zooms
into the sixth bin (hpXq P r0.5, 0.6q) and bin 10 (hpXq P r0.9, 1.0s), as described next."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.03618421052631579,"(c) Figure 1b displays the class-wise top-label reliability diagrams for bins 6 and 10. Note that for
bin 6, the ‹ marker is nearly on the X “ Y line, indicating that the overall accuracy matches the"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.03728070175438596,Published as a conference paper at ICLR 2022
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.03837719298245614,"Base model top-label-ECE
Base model conf-ECE"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.039473684210526314,"Temperature scaling top-label-ECE
Temperature scaling conf-ECE"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.04057017543859649,"Histogram binning top-label-ECE
Histogram binning  conf-ECE"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.041666666666666664,"5
10
15
20
25
Number of bins"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.04276315789473684,0.0075
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.043859649122807015,0.0100
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.044956140350877194,0.0125
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.046052631578947366,0.0150
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.047149122807017545,0.0175
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.04824561403508772,0.0200
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.049342105263157895,0.0225
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.05043859649122807,0.0250
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.051535087719298246,0.0275
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.05263157894736842,Estimated ECE
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.0537280701754386,ResNet-50
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.05482456140350877,"5
10
15
20
25
Number of bins 0.005 0.010 0.015 0.020 0.025 0.030"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.05592105263157895,Estimated ECE
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.05701754385964912,ResNet-110
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.0581140350877193,"5
10
15
20
25
Number of bins 0.005 0.010 0.015 0.020 0.025"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.05921052631578947,Estimated ECE
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.06030701754385965,Wide-ResNet-26-10
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.06140350877192982,"5
10
15
20
25
Number of bins 0.005 0.010 0.015 0.020 0.025 0.030"
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.0625,Estimated ECE
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.06359649122807018,DenseNet-121
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.06469298245614036,"Figure 2: Conf-ECE (dashed lines) and TL-ECE (solid lines) of four deep-net architectures on
CIFAR-10, as well as with recalibration using histogram binning and temperature scaling. The TL-
ECE is often 2-3 times the conf-ECE, depending on the number of bins used to estimate ECE, and
the architecture. Top-label histogram binning typically performs better than temperature scaling."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.06578947368421052,"overall conﬁdence of 0.545. However, the true accuracy when class 1 was predicted is « 0.2 and
the true accuracy when class 8 was predicted is « 0.9 (a very similar scenario to Example 1). For
bin 10, the ‹ marker indicates a miscalibration of « 0.01; however, when class 4 was predicted
(roughly 8% of all test-points) the miscalibration is « 0.05."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.0668859649122807,"Figure 2 displays the aggregate effect of the above phenomenon (across bins and classes) through
estimates of the conf-ECE and TL-ECE. The precise experimental setup is described in Section 4.
These plots display the ECE estimates of the base model, as well as the base model when recalibrated
using temperature scaling (Guo et al., 2017) and our upcoming formulation of top-label histogram
binning (Section 3). Since ECE estimates depend on the number of bins B used (see Roelofs et al.
(2020) for empirical work around this), we plot the ECE estimate for every value B P r5, 25s in order
to obtain clear and unambiguous results. We ﬁnd that the TL-ECE is signiﬁcantly higher than the
conf-ECE for most values of B, the architectures, and the pre- and post- recalibration models. This
ﬁgure also previews the performance of our forthcoming top-label histogram binning algorithm.
Top-label HB has smaller estimated TL-ECE than temperature scaling for most values of B and the
architectures. Except for ResNet-50, the conf-ECE estimates are also better."
MODIFYING CONFIDENCE CALIBRATION TO TOP-LABEL CALIBRATION,0.06798245614035088,"To summarize, top-label calibration captures the intuition of conﬁdence calibration by focusing on
the predicted class. However, top-label calibration also conditions on the predicted class, which is
always part of the prediction in any practical setting. Further, TL-ECE estimates can be substantially
different from conf-ECE estimates. Thus, while it is common to compare predictors based on the
conf-ECE, the TL-ECE comparison is more meaningful, and can potentially be different."
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.06907894736842106,"3
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.07017543859649122,"In this section, we unify a number of notions of multiclass calibration as multiclass-to-binary (or
M2B) notions, and propose a general-purpose calibration algorithm that achieves the corresponding
M2B notion of calibration. The M2B framework yields multiple novel post-hoc calibration algo-
rithms, each of which is tuned to a speciﬁc M2B notion of calibration."
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.0712719298245614,"3.1
MULTICLASS-TO-BINARY (M2B) NOTIONS OF CALIBRATION"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.07236842105263158,"In Section 2, we deﬁned conﬁdence calibration (1) and top-label calibration (2). These notions
verify calibration claims for the highest predicted probability. Other popular notions of calibration
verify calibration claims for other entries in the full L-dimensional prediction vector. A predictor
h “ ph1, h2, . . . , hLq is said to be class-wise calibrated (Kull et al., 2017) if"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.07346491228070176,"(class-wise calibration)
@l P rLs, PpY “ l | hlpXqq “ hlpXq.
(5)"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.07456140350877193,"Another recently proposed notion is top-K conﬁdence calibration (Gupta et al., 2021). For some
l P rLs, let cplq : X Ñ rLs denote the l-th highest class prediction, and let hplq : X Ñ rLs denote
the conﬁdence associated with it (c “ cp1q and h “ hp1q are special cases). For a given K ď L,"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.0756578947368421,"(top-K-conﬁdence calibration)
@k P rKs, PpY “ cpkqpXq | hpkqpXqq “ hpkqpXq.
(6)"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.07675438596491228,Published as a conference paper at ICLR 2022
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.07785087719298246,"Calibration notion
Quantiﬁer
Prediction (predpXq)
Binary calibration statement
Conﬁdence
-
hpXq
PpY “ cpXq | predpXqq “ hpXq
Top-label
-
cpXq, hpXq
PpY “ cpXq | predpXqq “ hpXq
Class-wise
@l P rLs
hlpXq
PpY “ l | predpXqq “ hlpXq
Top-K-conﬁdence
@k P rKs
hpkqpXq
PpY “ cpkqpXq | predpXqq “ hpkqpXq
Top-K-label
@k P rKs
cpkqpXq, hpkqpXq
PpY “ cpkqpXq | predpXqq “ hpkqpXq"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.07894736842105263,"Table 1: Multiclass-to-binary (M2B) notions internally verify one or more binary calibration state-
ments/claims. The statements in the rightmost column are required to hold almost surely."
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.0800438596491228,"As we did in Section 2 for conﬁdenceÑtop-label, top-K-conﬁdence calibration can be modiﬁed to
the more interpretable top-K-label calibration by further conditioning on the predicted labels:"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.08114035087719298,"(top-K-label calibration)
@k P rKs, PpY “ cpkqpXq | hpkqpXq, cpkqpXqq “ hpkqpXq.
(7)"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.08223684210526316,"Each of these notions reduce multiclass calibration to one or more binary calibration requirements,
where each binary calibration requirement corresponds to verifying if the distribution of Y , condi-
tioned on some prediction predpXq, satisﬁes a single binary calibration claim associated with
predpXq. Table 1 illustrates how the calibration notions discussed so far internally verify a number
of binary calibration claims, making them M2B notions. For example, for class-wise calibration, for
every l P rLs, the conditioning is on predpXq “ hlpXq, and a single binary calibration statement
is veriﬁed: PpY “ l | predpXqq “ hlpXq. Based on this property, we call each of these notions
multiclass-to-binary or M2B notions."
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.08333333333333333,"The notion of canonical calibration mentioned in the introduction is not an M2B notion. Canonical
calibration is discussed in detail in Appendix G. Due to the conditioning on a multi-dimensional
prediction, non-M2B notions of calibration are harder to achieve or verify. For the same reason, it is
possibly easier for humans to interpret binary calibration claims when taking decisions/actions."
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.08442982456140351,"3.2
ACHIEVING M2B NOTIONS OF CALIBRATION USING M2B CALIBRATORS"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.08552631578947369,"The M2B framework illustrates how multiclass calibration can typically be viewed via a reduction to
binary calibration. The immediate consequence of this reduction is that one can now solve multiclass
calibration problems by leveraging the well-developed methodology for binary calibration."
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.08662280701754387,"The upcoming M2B calibrators belong to the standard recalibration or post-hoc calibration setting.
In this setting, one starts with a ﬁxed pre-learnt base model g : X Ñ ∆L´1. The base model g
can correspond to a deep-net, a random forest, or any 1-v-all (one-versus-all) binary classiﬁcation
model such as logistic regression. The base model is typically optimized for classiﬁcation accuracy
and may not be calibrated. The goal of post-hoc calibration is to use some given calibration data
D “ pX1, Y1q, pX2, Y2q, . . . , pXn, Ynq P pX ˆ rLsqn, typically data on which g was not learnt, to
recalibrate g. In practice, the calibration data is usually the same as the validation data."
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.08771929824561403,"To motivate M2B calibrators, suppose we want to verify if g is calibrated on a certain test set,
based on a given M2B notion of calibration. Then, the verifying process will split the test data
into a number of sub-datasets, each of which will verify one of the binary calibration claims. In
Appendix A.2, we argue that the calibration data can also be viewed as a test set, and every step in
the veriﬁcation process can be used to provide a signal for improving calibration."
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.08881578947368421,"M2B calibrators take the form of wrapper methods that work on top of a given binary calibrator.
Denote an arbitrary black-box binary calibrator as At0,1u : r0, 1sX ˆpX ˆt0, 1uq‹ Ñ r0, 1sX , where
the ﬁrst argument is a mapping X Ñ r0, 1s that denotes a (miscalibrated) binary predicor, and the
second argument is a calibration data sequence of arbitrary length. The output is a (better calibrated)
binary predictor. Examples of At0,1u are histogram binning (Zadrozny and Elkan, 2001), isotonic
regression (Zadrozny and Elkan, 2002), and Platt scaling (Platt, 1999). In the upcoming descriptions,
we use the indicator function 1 ta “ bu P t0, 1u which takes the value 1 if a “ b, and 0 if a ‰ b."
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.08991228070175439,"The general formulation of our M2B calibrator is delayed to Appendix A since the description is a
bit involved. To ease readability and adhere to the space restrictions, in the main paper we describe
the calibrators corresponding to top-label, class-wise, and conﬁdence calibration (Algorithms 1–3).
Each of these calibrators are different from the classical M2B calibrator (Algorithm 4) that has been
used by Zadrozny and Elkan (2002), Guo et al. (2017), Kull et al. (2019), and most other papers"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.09100877192982457,Published as a conference paper at ICLR 2022
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.09210526315789473,M2B calibrators: Post-hoc multiclass calibration using binary calibrators
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.09320175438596491,"Input in each case: Binary calibrator At0,1u : r0, 1sX ˆ pX ˆ t0, 1uq‹ Ñ r0, 1sX , base multiclass
predictor g : X Ñ ∆L´1, calibration data D “ pX1, Y1q, . . . , pXn, Ynq."
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.09429824561403509,Algorithm 1: Conﬁdence calibrator
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.09539473684210527,1 c Ð classiﬁer or top-class based on g;
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.09649122807017543,2 g Ð top-class-probability based on g;
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.09758771929824561,"3 D1 Ð tpXi, 1 tYi “ cpXiquq : i P rnsu;"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.09868421052631579,"4 h Ð At0,1upg, D1q;"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.09978070175438597,"5 return pc, hq;"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.10087719298245613,Algorithm 2: Top-label calibrator
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.10197368421052631,1 c Ð classiﬁer or top-class based on g;
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.10307017543859649,2 g Ð top-class-probability based on g;
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.10416666666666667,3 for l Ð 1 to L do
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.10526315789473684,"4
Dl Ð tpXi, 1 tYi “ luq : cpXiq “ lqu;"
CALIBRATION ALGORITHMS FROM CALIBRATION METRICS,0.10635964912280702,"5
hl Ð At0,1upg, Dlq;"
END,0.1074561403508772,6 end
END,0.10855263157894737,7 hp¨q Ð hcp¨qp¨q (predict hlpxq if cpxq “ l);
END,0.10964912280701754,"8 return pc, hq;"
END,0.11074561403508772,Algorithm 3: Class-wise calibrator
END,0.1118421052631579,"1 Write g “ pg1, g2, . . . , gLq;"
END,0.11293859649122807,2 for l Ð 1 to L do
END,0.11403508771929824,"3
Dl Ð tpXi, 1 tYi “ luq : i P rnsu;"
END,0.11513157894736842,"4
hl Ð At0,1upgl, Dlq;"
END,0.1162280701754386,5 end
END,0.11732456140350878,"6 return ph1, h2, . . . , hLq;"
END,0.11842105263157894,Algorithm 4: Normalized calibrator
END,0.11951754385964912,"1 Write g “ pg1, g2, . . . , gLq;"
END,0.1206140350877193,2 for l Ð 1 to L do
END,0.12171052631578948,"3
Dl Ð tpXi, 1 tYi “ luq : i P rnsu;"
END,0.12280701754385964,"4
rhl Ð At0,1upgl, Dlq;"
END,0.12390350877192982,5 end
END,0.125,"6 Normalize: for every l P rLs,
hlp¨q :“ rhlp¨q{ řL
k“1 rhkp¨q;"
END,0.12609649122807018,"7 return ph1, h2, . . . , hLq;"
END,0.12719298245614036,"we are aware of, with the most similar one being Algorithm 3. Top-K-label and top-K-conﬁdence
calibrators are also explicitly described in Appendix A (Algorithms 6 and 7)."
END,0.12828947368421054,"Top-label calibration requires that for every class l P rLs, PpY “ l | cpXq “ l, hpXqq “ hpXq.
Thus, to achieve top-label calibration, we must solve L calibration problems. Algorithm 2 constructs
L datasets tDl : l P rLsu (line 4). The features in Dl are the Xi’s for which cpXiq “ l, and the
labels are 1 tYi “ lu. Now for every l P rLs, we calibrate g to hl : X Ñ r0, 1s using Dl and any
binary calibrator. The ﬁnal probabilistic predictor is hp¨q “ hcp¨qp¨q (that is, it predicts hlpxq if
cpxq “ l). The top-label predictor c does not change in this process. Thus the accuracy of pc, hq is
the same as the accuracy of g irrespective of which At0,1u is used. Unlike the top-label calibrator,
the conﬁdence calibrator merges all classes together into a single dataset D1 “ Ť"
END,0.12938596491228072,lPrLs Dl.
END,0.13048245614035087,"To achieve class-wise calibration, Algorithm 3 also solves L calibration problems, but these corre-
spond to satisfying PpY “ l | hlpXqq “ hlpXq. Unlike top-label calibration, the dataset Dl for
class-wise calibration contains all the Xi’s (even if cpXiq ‰ l), and hl is passed to At0,1u instead
of h. Also, unlike conﬁdence calibration, Yi is replaced with 1 tYi “ lu instead of 1 tYi “ cpXiqu.
The overall process is similar to reducing multiclass classiﬁcation to L 1-v-all binary classiﬁcation
problem, but our motivation is intricately tied to the notion of class-wise calibration."
END,0.13157894736842105,"Most popular empirical works that have discussed binary calibrators for multiclass calibration have
done so using the normalized calibrator, Algorithm 4. This is almost identical to Algorithm 3, except
that there is an additional normalization step (line 6 of Algorithm 4). This normalization was ﬁrst
proposed by Zadrozny and Elkan (2002, Section 5.2), and has been used unaltered by most other
works1 where the goal has been to simply compare direct multiclass calibrators such as temperature
scaling, Dirichlet scaling, etc., to a calibrator based on binary methods (for instance, see Section
4.2 of Guo et al. (2017)). In contrast to these papers, we investigate multiple M2B reductions in an
effort to identify the right reduction of multiclass calibration to binary calibration."
END,0.13267543859649122,"To summarize, the M2B characterization immediately yields a novel and different calibrator for
every M2B notion. In the following section, we instantiate M2B calibrators on the binary calibrator
of histogram binning (HB), leading to two new algorithms: top-label-HB and class-wise-HB, that
achieve strong empirical results and satisfy distribution-free calibration guarantees."
END,0.1337719298245614,"1the only exception we are aware of is the recent work of Patel et al. (2021) who also suggest skipping
normalization (see their Appendix A1); however they use a common I-Max binning scheme across classes,
whereas in Algorithm 3 the predictor hl for each class is learnt completely independently of other classes"
END,0.13486842105263158,Published as a conference paper at ICLR 2022
END,0.13596491228070176,"Metric
Dataset
Architecture
Base
TS
VS
DS
N-HB
TL-HB"
END,0.13706140350877194,"Top-
label-
ECE"
END,0.13815789473684212,CIFAR-10
END,0.13925438596491227,"ResNet-50
0.025
0.022
0.020
0.019
0.018
0.020
ResNet-110
0.029
0.022
0.021
0.021
0.020
0.021
WRN-26-10
0.023
0.023
0.019
0.021
0.012
0.018
DenseNet-121
0.027
0.027
0.020
0.020
0.019
0.021"
END,0.14035087719298245,CIFAR-100
END,0.14144736842105263,"ResNet-50
0.118
0.114
0.113
0.322
0.081
0.143
ResNet-110
0.127
0.121
0.115
0.353
0.093
0.145
WRN-26-10
0.103
0.103
0.100
0.304
0.070
0.129
DenseNet-121
0.110
0.110
0.109
0.322
0.086
0.139"
END,0.1425438596491228,"Top-
label-
MCE"
END,0.14364035087719298,CIFAR-10
END,0.14473684210526316,"ResNet-50
0.315
0.305
0.773
0.282
0.411
0.107
ResNet-110
0.275
0.227
0.264
0.392
0.195
0.077
WRN-26-10
0.771
0.771
0.498
0.325
0.140
0.071
DenseNet-121
0.289
0.289
0.734
0.294
0.345
0.087"
END,0.14583333333333334,CIFAR-100
END,0.14692982456140352,"ResNet-50
0.436
0.300
0.251
0.619
0.397
0.291
ResNet-110
0.313
0.255
0.277
0.557
0.266
0.257
WRN-26-10
0.273
0.255
0.256
0.625
0.287
0.280
DenseNet-121
0.279
0.231
0.235
0.600
0.320
0.289"
END,0.14802631578947367,"Table 2: Top-label-ECE and top-label-MCE for deep-net models (above: ‘Base’) and various post-
hoc calibrators: temperature-scaling (TS), vector-scaling (VS), Dirichlet-scaling (DS), top-label-HB
(TL-HB), and normalized-HB (N-HB). Best performing method in each row is in bold."
END,0.14912280701754385,"Metric
Dataset
Architecture
Base
TS
VS
DS
N-HB
CW-HB"
END,0.15021929824561403,"Class-
wise-
ECE
ˆ102"
END,0.1513157894736842,CIFAR-10
END,0.1524122807017544,"ResNet-50
0.46
0.42
0.35
0.35
0.50
0.28
ResNet-110
0.59
0.50
0.42
0.38
0.53
0.27
WRN-26-10
0.44
0.44
0.35
0.39
0.39
0.28
DenseNet-121
0.46
0.46
0.36
0.36
0.48
0.36"
END,0.15350877192982457,CIFAR-100
END,0.15460526315789475,"ResNet-50
0.22
0.20
0.20
0.66
0.23
0.16
ResNet-110
0.24
0.23
0.21
0.72
0.24
0.16
WRN-26-10
0.19
0.19
0.18
0.61
0.20
0.14
DenseNet-121
0.20
0.21
0.19
0.66
0.24
0.16"
END,0.15570175438596492,"Table 3: Class-wise-ECE for deep-net models and various post-hoc calibrators. All methods are
same as Table 2, except TL-HB is replaced with class-wise-HB (CW-HB)."
END,0.15679824561403508,"4
EXPERIMENTS: M2B CALIBRATION WITH HISTOGRAM BINNING"
END,0.15789473684210525,"Histogram binning or HB was proposed by Zadrozny and Elkan (2001) with strong empirical results
for binary calibration. In HB, a base binary calibration model g : X Ñ r0, 1s is used to partition the
calibration data into a number of bins so that each bin has roughly the same number of points. Then,
for each bin, the probability of Y “ 1 is estimated using the empirical distribution on the calibration
data. This estimate forms the new calibrated prediction for that bin. Recently, Gupta and Ramdas
(2021) showed that HB satisﬁes strong distribution-free calibration guarantees, which are otherwise
impossible for scaling methods (Gupta et al., 2020)."
END,0.15899122807017543,"Despite these results for binary calibration, studies for multiclass calibration have reported that HB
typically performs worse than scaling methods such as temperature scaling (TS), vector scaling
(VS), and Dirichlet scaling (DS) (Kull et al., 2019; Roelofs et al., 2020; Guo et al., 2017). In our
experiments, we ﬁnd that the issue is not HB but the M2B wrapper used to produce the HB baseline.
With the right M2B wrapper, HB beats TS, VS, and DS. A number of calibrators have been proposed
recently (Zhang et al., 2020; Rahimi et al., 2020; Patel et al., 2021; Gupta et al., 2021), but VS and
DS continue to remain strong baselines which are often close to the best in these papers. We do not
compare to each of these calibrators; our focus is on the M2B reduction and the message that the
baselines dramatically improve with the right M2B wrapper."
END,0.1600877192982456,"We use three metrics for comparison: the ﬁrst is top-label-ECE or TL-ECE (deﬁned in (4)), which
we argued leads to a more meaningful comparison compared to conf-ECE. Second, we consider
the more stringent maximum-calibration-error (MCE) metric that assesses the worst calibration
across predictions (see more details in Appendix E.3). For top-label calibration MCE is given by
TL-MCEpc, hq :“ maxlPrLs suprPRangephq |PpY “ l | cpXq “ l, hpXq “ rq ´ r|. To assess class-
wise calibration, we use class-wise-ECE deﬁned as the average calibration error across classes:"
END,0.1611842105263158,Published as a conference paper at ICLR 2022
END,0.16228070175438597,"CW-ECEpc, hq :“ L´1 řL
l“1 EX |PpY “ l | hlpXqq ´ hlpXq|. All ECE/MCE estimation is per-
formed as described in Remark 1. For further details, see Appendix E.2."
END,0.16337719298245615,"Formal algorithm and theoretical guarantees. Top-label-HB (TL-HB) and class-wise-HB (CW-
HB) are explicitly stated in Appendices B and C respectively; these are instantiations of the top-label
calibrator and class-wise calibrator with HB. N-HB is the the normalized calibrator (Algorithm 4)
with HB, which is the same as CW-HB, but with an added normalization step. In the Appendix,
we extend the binary calibration guarantees of Gupta and Ramdas (2021) to TL-HB and CW-HB
(Theorems 1 and 2). We informally summarize one of the results here: if there are at least k cali-
bration points-per-bin, then the expected-ECE is bounded as: E r(TL-) or (CW-) ECEs ď
a"
END,0.16447368421052633,"1{2k,
for TL-HB and CW-HB respectively. The outer E above is an expectation over the calibration data,
and corresponds to the randomness in the predictor learnt on the calibration data. Note that the ECE
itself is an expected error over an unseen i.i.d. test-point pX, Y q „ P."
END,0.16557017543859648,"Experimental details. We experimented on the CIFAR-10 and CIFAR-100 datasets, which have
10 and 100 classes each. The base models are deep-nets with the following architectures: ResNet-
50, Resnet-110, Wide-ResNet-26-10 (WRN) (Zagoruyko and Komodakis, 2016), and DenseNet-
121 (Huang et al., 2017). Both CIFAR datasets consist of 60K (60,000) points, which are split as
45K/5K/10K to form the train/validation/test sets. The validation set was used for post-hoc calibra-
tion and the test set was used for evaluation through ECE/MCE estimates. Instead of training new
models, we used the pre-trained models of Mukhoti et al. (2020). We then ask: “which post-hoc
calibrator improves the calibration the most?” We used their Brier score and focal loss models in
our experiments (Mukhoti et al. (2020) report that these are the empirically best performing loss
functions). All results in the main paper are with Brier score, and results with focal loss are in
Appendix E.4. Implementation details for TS, VS, and DS are in Appendix E."
END,0.16666666666666666,"Findings. In Table 2, we report the binned ECE and MCE estimates when B “ 15 bins are used by
HB, and for ECE estimation. We make the following observations:"
END,0.16776315789473684,"(a) For TL-ECE, N-HB is the best performing method for both CIFAR-10 and CIFAR-100. While
most methods perform similarly across architectures for CIFAR-10, there is high variation in
CIFAR-100. DS is the worst performing method on CIFAR-100, but TL-HB also performs
poorly. We believe that this could be because the data splitting scheme of the TL-calibrator (line
4 of Algorithm 2) splits datasets across the predicted classes, and some classes in CIFAR-100
occur very rarely. This is further discussed in Appendix E.6.
(b) For TL-MCE, TL-HB is the best performing method on CIFAR-10, by a huge margin. For
CIFAR-100, TS or VS perform slightly better than TL-HB. Since HB ensures that each bin gets
roughly the same number of points, the predictions are well calibrated across bins, leading to
smaller TL-MCE. A similar observation was also made by Gupta and Ramdas (2021).
(c) For CW-ECE, CW-HB is the best performing method across the two datasets and all four ar-
chitectures. The N-HB method which has been used in many CW-ECE baseline experiments
performs terribly. In other words, skipping the normalization step leads to a large improvement
in CW-ECE. This observation is one of our most striking ﬁndings. To shed further light on
this, we note that the distribution-free calibration guarantees for CW-HB shown in Appendix C
no longer hold post-normalization. Thus, both our theory and experiments indicate that skipping
normalization improves CW-ECE performance."
END,0.16885964912280702,"Additional experiments in the Appendix. In Appendix E.5, we report each of the results in Ta-
bles 2 and 3 with the number of bins taking every value in the range r5, 25s. Most observations
remain the same under this expanded study. In Appendix B.2, we consider top-label calibration for
the class imbalanced COVTYPE-7 dataset, and show that TL-HB adapts to tail/infrequent classes."
CONCLUSION,0.1699561403508772,"5
CONCLUSION"
CONCLUSION,0.17105263157894737,"We make two contributions to the study of multiclass calibration: (i) deﬁning the new notion of
top-label calibration which enforces a natural minimal requirement on a multiclass predictor—the
probability score for the top class prediction should be calibrated; (ii) developing a multiclass-to-
binary (M2B) framework which posits that various notions of multiclass calibration can be achieved
via reduction to binary calibration, balancing practical utility with statistically tractability. Since it
is important to identify appropriate notions of calibration in any structured output space (Kuleshov
et al., 2018; Gneiting et al., 2007), we anticipate that the philosophy behind the M2B framework
could ﬁnd applications in other structured spaces."
CONCLUSION,0.17214912280701755,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.17324561403508773,"6
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.17434210526315788,"Some reproducibility desiderata, such as external code and libraries that were used are summarized
in Appendix E.1. All code to generate results with the CIFAR datasets is attached in the supple-
mentary material. Our base models were pre-trained deep-net models generated by Mukhoti et al.
(2020), obtained from www.robots.ox.ac.uk/„viveka/focal calibration/ (corre-
sponding to ‘brier score’ and ‘focal loss adaptive 53’ at the above link). By avoiding training of
new deep-net models with multiple hyperparameters, we also consequently avoided selection biases
that inevitably creep in due to test-data-peeking. The predictions of the pre-trained models were
obtained using the code at https://github.com/torrvision/focal calibration."
ETHICS STATEMENT,0.17543859649122806,"7
ETHICS STATEMENT"
ETHICS STATEMENT,0.17653508771929824,"Post-hoc calibration is a post-processing step that can be applied on top of miscalibrated machine
learning models to increase their reliability. As such, we believe our work should improve the
transparency and explainability of machine learning models. However, we outline a few limita-
tions. Post-hoc calibration requires keeping aside a fresh, representative dataset, that was not used
for training. If this dataset is too small, the resulting calibration guarantee can be too weak to be
meaningful in practice. Further, if the test data distribution shifts in signiﬁcant ways, additional
corrections may be needed to recalibrate (Gupta et al., 2020; Podkopaev and Ramdas, 2021). A
well calibrated classiﬁer is not necessarily an accurate or a fair one, and vice versa (Kleinberg et al.,
2017). Deploying calibrated models in critical applications like medicine, criminal law, banking,
etc. does not preclude the possibility of the model being frequently wrong or unfair."
ETHICS STATEMENT,0.17763157894736842,ACKNOWLEDGEMENTS
ETHICS STATEMENT,0.1787280701754386,"This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is
supported by National Science Foundation grant number ACI-1548562 (Towns et al., 2014). Specif-
ically, it used the Bridges-2 system, which is supported by NSF award number ACI-1928147, at the
Pittsburgh Supercomputing Center (PSC). CG’s research was supported by the generous Bloomberg
Data Science Ph.D. Fellowship. CG would like to thank Saurabh Garg and Youngseog Chung for
interesting discussions, and Viveka Kulharia for help with the focal calibration repository. Finally,
we thank Zack Lipton, the ICLR reviewers, and the ICLR area chair, for excellent feedback that
helped improve the writing of the paper."
REFERENCES,0.17982456140350878,REFERENCES
REFERENCES,0.18092105263157895,"Jock A Blackard and Denis J Dean. Comparative accuracies of artiﬁcial neural networks and dis-
criminant analysis in predicting forest cover types from cartographic variables. Computers and
electronics in agriculture, 24(3):131–151, 1999."
REFERENCES,0.18201754385964913,"Leo Breiman, Jerome H Friedman, Richard A Olshen, and Charles J Stone. Classiﬁcation and
regression trees. Routledge, 2017."
REFERENCES,0.18311403508771928,"Luc Devroye. The equivalence of weak, strong and complete convergence in L1 for kernel density
estimates. The Annals of Statistics, 11(3):896–904, 1983."
REFERENCES,0.18421052631578946,"Luc Devroye. Automatic pattern recognition: A study of the probability of error. IEEE Transactions
on pattern analysis and machine intelligence, 10(4):530–543, 1988."
REFERENCES,0.18530701754385964,"Tilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery. Probabilistic forecasts, calibration
and sharpness. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2):
243–268, 2007."
REFERENCES,0.18640350877192982,"Louis Gordon and Richard A Olshen.
Almost surely consistent nonparametric regression from
recursive partitioning schemes. Journal of Multivariate Analysis, 15(2):147–163, 1984."
REFERENCES,0.1875,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, 2017."
REFERENCES,0.18859649122807018,Published as a conference paper at ICLR 2022
REFERENCES,0.18969298245614036,"Chirag Gupta and Aaditya Ramdas. Distribution-free calibration guarantees for histogram binning
without sample splitting. In International Conference on Machine Learning, 2021."
REFERENCES,0.19078947368421054,"Chirag Gupta, Aleksandr Podkopaev, and Aaditya Ramdas. Distribution-free binary classiﬁcation:
prediction sets, conﬁdence intervals and calibration. In Advances in Neural Information Process-
ing Systems, 2020."
REFERENCES,0.19188596491228072,"Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu,
and Richard Hartley. Calibration of neural networks using splines. In International Conference
on Learning Representations, 2021."
REFERENCES,0.19298245614035087,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2016."
REFERENCES,0.19407894736842105,"Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017."
REFERENCES,0.19517543859649122,"Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determi-
nation of risk scores. In Innovations in Theoretical Computer Science, 2017."
REFERENCES,0.1962719298245614,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical Report, Univer-
sity of Toronto, 2009."
REFERENCES,0.19736842105263158,"Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning
using calibrated regression. In International Conference on Machine Learning, 2018."
REFERENCES,0.19846491228070176,"Meelis Kull, Telmo M. Silva Filho, and Peter Flach.
Beyond sigmoids: How to obtain well-
calibrated probabilities from binary classiﬁers with beta calibration. Electronic Journal of Statis-
tics, 11(2):5052–5080, 2017."
REFERENCES,0.19956140350877194,"Meelis Kull, Miquel Perello-Nieto, Markus K¨angsepp, Hao Song, and Peter Flach. Beyond tem-
perature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration. In
Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.20065789473684212,"Ananya Kumar, Percy S Liang, and Tengyu Ma. Veriﬁed uncertainty calibration. In Advances in
Neural Information Processing Systems, 2019."
REFERENCES,0.20175438596491227,"Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks
from kernel mean embeddings. In International Conference on Machine Learning, 2018."
REFERENCES,0.20285087719298245,"G´abor Lugosi and Andrew Nobel. Consistency of data-driven histogram methods for density esti-
mation and classiﬁcation. Annals of Statistics, 24(2):687–706, 1996."
REFERENCES,0.20394736842105263,"Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip HS Torr, and Puneet K
Dokania. Calibrating deep neural networks using focal loss. In Advances in Neural Information
Processing Systems, 2020."
REFERENCES,0.2050438596491228,"Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learn-
ing. In International Conference on Machine Learning, 2005."
REFERENCES,0.20614035087719298,"Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measur-
ing calibration in deep learning. arXiv preprint arXiv:1904.01685, 2020."
REFERENCES,0.20723684210526316,"Andrew Nobel. Histogram regression estimation using data-dependent partitions. The Annals of
Statistics, 24(3):1084–1105, 1996."
REFERENCES,0.20833333333333334,"Kanil Patel, William Beluch, Bin Yang, Michael Pfeiffer, and Dan Zhang. Multi-class uncertainty
calibration via mutual information maximization-based binning. In International Conference on
Learning Representations, 2021."
REFERENCES,0.20942982456140352,"John C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. In Advances in Large Margin Classiﬁers, pages 61–74. MIT Press, 1999."
REFERENCES,0.21052631578947367,Published as a conference paper at ICLR 2022
REFERENCES,0.21162280701754385,"Aleksandr Podkopaev and Aaditya Ramdas. Distribution-free uncertainty quantiﬁcation for classi-
ﬁcation under label shift. In Uncertainty in Artiﬁcial Intelligence, 2021."
REFERENCES,0.21271929824561403,"Jian Qian, Ronan Fruit, Matteo Pirotta, and Alessandro Lazaric. Concentration inequalities for
multinoulli random variables. arXiv preprint arXiv:2001.11595, 2020."
REFERENCES,0.2138157894736842,"Amir Rahimi, Amirreza Shaban, Ching-An Cheng, Richard Hartley, and Byron Boots. Intra order-
preserving functions for calibration of multi-class neural networks. In Advances in Neural Infor-
mation Processing Systems, 2020."
REFERENCES,0.2149122807017544,"Rebecca Roelofs, Nicholas Cain, Jonathon Shlens, and Michael C Mozer. Mitigating bias in cali-
bration error estimation. arXiv preprint arXiv:2012.08668, 2020."
REFERENCES,0.21600877192982457,"J. Towns, T. Cockerill, M. Dahan, I. Foster, K. Gaither, A. Grimshaw, V. Hazlewood, S. Lathrop,
D. Lifka, G. D. Peterson, R. Roskies, J. Scott, and N. Wilkins-Diehr. XSEDE: Accelerating
Scientiﬁc Discovery. Computing in Science & Engineering, 16(5):62–74, 2014."
REFERENCES,0.21710526315789475,"Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and
Thomas B Sch¨on. Evaluating model calibration in classiﬁcation. In International Conference
on Artiﬁcial Intelligence and Statistics, 2019."
REFERENCES,0.21820175438596492,"Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger. In-
equalities for the L1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep,
2003."
REFERENCES,0.21929824561403508,"David Widmann, Fredrik Lindsten, and Dave Zachariah. Calibration tests in multi-class classiﬁca-
tion: a unifying framework. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.22039473684210525,"Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees
and naive Bayesian classiﬁers. In International Conference on Machine Learning, 2001."
REFERENCES,0.22149122807017543,"Bianca Zadrozny and Charles Elkan. Transforming classiﬁer scores into accurate multiclass proba-
bility estimates. In International Conference on Knowledge Discovery and Data Mining, 2002."
REFERENCES,0.2225877192982456,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
In British Machine Vision
Conference, 2016."
REFERENCES,0.2236842105263158,"Jize Zhang, Bhavya Kailkhura, and T Han. Mix-n-match: Ensemble and compositional methods for
uncertainty calibration in deep learning. In International Conference on Machine Learning, 2020."
REFERENCES,0.22478070175438597,Published as a conference paper at ICLR 2022
REFERENCES,0.22587719298245615,"Algorithm 5: Post-hoc calibrator for a given M2B calibration notion C
Input: Base (uncalibrated) multiclass predictor g, calibration data
D “ pX1, Y1q, . . . , pXn, Ynq, binary calibrator
At0,1u : r0, 1sX ˆ pX ˆ t0, 1uq‹ Ñ r0, 1sX"
REFERENCES,0.22697368421052633,1 K Ð number of distinct calibration claims that C veriﬁes;
FOR EACH CLAIM K P RKS DO,0.22807017543859648,2 for each claim k P rKs do
FOR EACH CLAIM K P RKS DO,0.22916666666666666,"3
From g, infer prc, rgq Ð plabel-predictor, probability-predictorq corresponding to claim k;"
FOR EACH CLAIM K P RKS DO,0.23026315789473684,"4
Dk Ð tpXi, Ziqu, where Zi Ð 1 tYi “ rcpXiqu;"
IF CONDITIONING DOES NOT INCLUDE CLASS PREDICTION RC THEN,0.23135964912280702,"5
if conditioning does not include class prediction rc then"
IF CONDITIONING DOES NOT INCLUDE CLASS PREDICTION RC THEN,0.2324561403508772,"6
— (conﬁdence, top-K-conﬁdence, and class-wise calibration) —"
IF CONDITIONING DOES NOT INCLUDE CLASS PREDICTION RC THEN,0.23355263157894737,"7
hk Ð At0,1uprg, Dkq;"
END,0.23464912280701755,"8
end"
ELSE,0.23574561403508773,"9
else"
ELSE,0.23684210526315788,"10
— (top-label and top-K-label calibration) —"
FOR L P RLS DO,0.23793859649122806,"11
for l P rLs do"
FOR L P RLS DO,0.23903508771929824,"12
Dk,l Ð tpXi, Ziq P Dk : rcpXiq “ lqu;"
FOR L P RLS DO,0.24013157894736842,"13
hk,l Ð At0,1uprg, Dk,lq;"
END,0.2412280701754386,"14
end"
END,0.24232456140350878,"15
hkp¨q Ð hk,rcp¨qp¨q (hk predicts hk,lpxq if rcpxq “ l);"
END,0.24342105263157895,"16
end"
END,0.24451754385964913,17 end
END,0.24561403508771928,18 — (the new predictor replaces each rg with the corresponding hk) —
END,0.24671052631578946,"19 return plabel-predictor, hkq corresponding to each claim k P rKs;"
END,0.24780701754385964,"Input for Algorithms 6 and 7: base multiclass predictor g : X Ñ ∆L´1, calibration data D “
pX1, Y1q, . . . , pXn, Ynq, binary calibrator At0,1u : r0, 1sX ˆ pX ˆ t0, 1uq‹ Ñ r0, 1sX ."
END,0.24890350877192982,Algorithm 6: Top-K-label calibrator
END,0.25,"1 For every k P rKs, infer from g the k-th
largest class predictor cpkq and the
associated probability gpkq;"
END,0.25109649122807015,2 for k Ð 1 to K do
END,0.25219298245614036,"3
for l Ð 1 to L do"
END,0.2532894736842105,"4
Dk,l Ð tpXi, 1 tYi “ luq :
cpkqpXiq “ lu;"
END,0.2543859649122807,"5
hpk,lq Ð At0,1upgpkq, Dk,lq;"
END,0.25548245614035087,"6
end"
END,0.2565789473684211,"7
hpkq Ð hpk,cpkqp¨qqp¨q;"
END,0.2576754385964912,8 end
END,0.25877192982456143,"9 return php1q, hp2q, . . . , hpKqq;"
END,0.2598684210526316,Algorithm 7: Top-K-conﬁdence calibrator
END,0.26096491228070173,"1 For every k P rKs, infer from g the k-th
largest class predictor cpkq and the
associated probability gpkq;"
END,0.26206140350877194,2 for k Ð 1 to K do
END,0.2631578947368421,"3
Dk Ð tpXi, 1 tYi “ luq : i P rnsu;"
END,0.2642543859649123,"4
hpkq Ð At0,1upgpkq, Dkq;"
END,0.26535087719298245,5 end
END,0.26644736842105265,"6 return php1q, hp2q, . . . , hpKqq;"
END,0.2675438596491228,"A
ADDENDUM TO SECTION 3 “CALIBRATION ALGORITHMS FROM
CALIBRATION METRICS”"
END,0.26864035087719296,"In Section 3, we introduced the concept of M2B calibration, and showed that popular calibration
notions are in fact M2B notions (Table 1). We showed how the calibration notions of top-label,
class-wise, and conﬁdence calibration can be achieved using a corresponding M2B calibrator. In
the following subsection, we present the general-purpose wrapper Algorithm 5 that can be used to
derive an M2B calibrator from any given M2B calibration notion that follows the rubric speciﬁed by
Table 1. In Appendix A.2, we illustrate the philosophy of M2B calibration using a simple example
with a dataset that contains 6 points. This example also illustrates the top-label-calibrator, the class-
wise-calibrator, and the conﬁdence-calibrator."
END,0.26973684210526316,Published as a conference paper at ICLR 2022
END,0.2708333333333333,"(a) Predictions of a ﬁxed base model
g : X Ñ ∆3 on calibration/test data
D “ tpa, 3q, pb, 4q, . . . , pf, 1qu."
END,0.2719298245614035,(b) Conﬁdence calibration
END,0.2730263157894737,(c) Top-label calibration
END,0.2741228070175439,(d) Class-wise calibration
END,0.27521929824561403,(e) Canonical calibration
END,0.27631578947368424,"Figure 3: Illustrative example for Section A.2. The numbers in plot (a) correspond to the predictions
made by g on a dataset D. If D were a test set, plots (b–e) show how it should be used to verify if g
satisﬁes the corresponding notion of calibration. Consequently, we argue that if D were a calibration
set, and we want to achieve one of the notions (b–e), then the data shown in the corresponding plots
should be the data used to calibrate g as well."
END,0.2774122807017544,"A.1
GENERAL-PURPOSE M2B CALIBRATOR"
END,0.27850877192982454,"Denote some M2B notion of calibration as C. Suppose C corresponds to K binary calibration claims.
The outer for-loop in Algorithm 5, runs over each such claim in C. For example, for class-wise
calibration, K “ L and for conﬁdence and top-label calibration, K “ 1. Corresponding to each
claim, there is a probability-predictor that the conditioning is to be done on, such as g or gl or gpkq.
Additionally, there may be conditioning on the label predictor such as c or cpkq. These are denoted
as prc, rgq in Algorithm 5. For conﬁdence and top-label calibration, rc “ c, the top-label-conﬁdence.
For class-wise calibration, when rg “ gl, we have rcp¨q “ l."
END,0.27960526315789475,"If there is no label conditioning in the calibration notion, such as in conﬁdence, top-K-conﬁdence,
and class-wise calibration, then we enter the if-condition inside the for-loop. Here hk is learnt using
a single calibration dataset and a single call to At0,1u. Otherwise, if there is label conditioning, such
as in top-label and top-K-label calibration, we enter the else-condition, where we learn a separate
hk,l for every l P rLs, using a different part of the dataset Dl in each case. Then hkpxq equals
hk,lpxq if rcpxq “ l."
END,0.2807017543859649,"Finally, since C is verifying a sequence of claims, the output of Algorithm 5 is a sequence of pre-
dictors. Each original prediction prc, rgq corresponding to the C is replaced with prc, hkq. This is the
output of the M2B calibrator. Note that the rc values are not changed. This output appears abstract,
but normally, it can be represented in an interpretable way. For example, for class-wise calibration,
the output is just a sequence of predictors, one for each class: ph1, h2, . . . , hLq."
END,0.2817982456140351,"This general-purpose M2B calibrators can be used to achieve any M2B calibration notion: top-
label calibration (Algorithm 2), class-wise calibration (Algorithm 3), conﬁdence calibration (Algo-
rithm 1), top-K-label calibration (Algorithm 6), and top-K-conﬁdence calibration (Algorithm 7)."
END,0.28289473684210525,"A.2
AN EXAMPLE TO ILLUSTRATE THE PHILOSOPHY OF M2B CALIBRATION"
END,0.28399122807017546,"Figure 3a shows the predictions of a given base model g on a given dataset D. Suppose D is a
test set, and we are testing conﬁdence calibration. Then the only predictions that matter are the
top-predictions corresponding to the shaded values. These are stripped out and shown in Figure 3b,
in the gp¨q row. Note that the indicator 1 tY “ cp¨qu is sufﬁcient to test conﬁdence calibration and
given this, the cpXq are not needed. Thus the second row in Figure 3b only shows these indicators."
END,0.2850877192982456,Published as a conference paper at ICLR 2022
END,0.28618421052631576,"Algorithm 8: Top-label histogram binning
Input: Base multiclass predictor g, calibration data D “ pX1, Y1q, . . . , pXn, Ynq
Hyperparameter: # points per bin k P N (say 50), tie-breaking parameter δ ą 0 (say 10´10)
Output: Top-label calibrated predictor pc, hq"
END,0.28728070175438597,1 c Ð classiﬁer or top-class based on g;
END,0.2883771929824561,2 g Ð top-class-probability based on g;
END,0.2894736842105263,3 for l Ð 1 to L do
END,0.2905701754385965,"4
Dl Ð tpXi, 1 tYi “ luq : cpXiq “ lqu and nl Ð |Dl|;"
END,0.2916666666666667,"5
hl Ð Binary-histogram-binningpg, Dl, tnl{ku , δq;"
END,0.29276315789473684,6 end
END,0.29385964912280704,7 hp¨q Ð hcp¨qp¨q;
END,0.2949561403508772,"8 return pc, hq;"
END,0.29605263157894735,"Verifying top-label calibration is similar (Figure 3c), but in addition to the predictions gp¨q, we also
retain the values of cp¨q. Thus the gp¨q and 1 tY “ cp¨qu are shown, but split across the 4 classes.
Class-wise calibration requires access to all the predictions, however, each class is considered sep-
arately as indicated by Figure 3d. Canonical calibration looks at the full prediction vector in each
case. However, in doing so, it becomes unlikely that gpxq “ gpyq for any x, y since the number of
values that g can take is now exponential."
END,0.29714912280701755,"Let us turn this around and suppose that D were a calibration set instead of a test set. We argue that D
should be used in the same way, whether testing or calibrating. Thus, if conﬁdence calibration is to
be achieved, we should focus on the pg, 1 tY “ cp¨quq corresponding to g. If top-label calibration is
to be achieved, we should use the pc, gq values. If class-wise calibration is to be achieved, we should
look at each gl separately and solve L different problems. Finally, for canonical calibration, we must
look at the entire g vector as a single unit. This is the core philosophy behind M2B calibrators: if
binary claims are being veriﬁed, solve binary calibration problems."
END,0.2982456140350877,"B
DISTRIBUTION-FREE TOP-LABEL CALIBRATION USING HISTOGRAM
BINNING"
END,0.2993421052631579,"In this section, we formally describe histogram binning (HB) with the top-label-calibrator (Algo-
rithm 2) and provide methodological insights through theory and experiments."
END,0.30043859649122806,"B.1
FORMAL ALGORITHM AND THEORETICAL GUARANTEES"
END,0.30153508771929827,"Algorithm 8 describes the top-label calibrator formally using HB as the binary calibration algorithm.
The function called in line 5 is Algorithm 2 of Gupta and Ramdas (2021). The ﬁrst argument in the
call is the top-label conﬁdence predictor, the second argument is the dataset to be used, the third
argument is the number of bins to be used, and the fourth argument is a tie-breaking parameter
(described shortly). While previous empirical works on HB ﬁxed the number of bins per class, the
analysis of Gupta and Ramdas (2021) suggests that a more principled way of choosing the number
of bins is to ﬁx the number of points per bin. This is parameter k of Algorithm 8. Given k, the
number of bins is decided separately for every class as tnl{ku where nl is the number of points
predicted as class l. This choice is particularly relevant for top-label calibration since nl can be
highly non-uniform (we illustrate this empirically in Section B.2). The tie-breaking parameter δ
can be arbitrarily small (like 10´10), and its signiﬁcance is mostly theoretical—it is used to ensure
that outputs of different bins are not exactly identical by chance, so that conditioning on a calibrated
probability output is equivalent to conditioning on a bin; this leads to a cleaner theoretical guarantee."
END,0.3026315789473684,"HB recalibrates g to a piecewise constant function h that takes one value per bin. Consider a speciﬁc
bin b; the h value for this bin is computed as the average of the indicators t1 tYi “ cpXiqu : Xi P
Bin bu. This is an estimate of the bias of the bin PpY “ cpXq | X P Bin bq. A concentration
inequality can then be used to bound the deviation between the estimate and the true bias to prove
distribution-free calibration guarantees. In the forthcoming Theorem 1, we show high-probability
and in-expectation bounds on the the TL-ECE of HB. Additionally, we show marginal and condi-"
END,0.30372807017543857,Published as a conference paper at ICLR 2022
END,0.3048245614035088,"tional top-label calibration bounds, deﬁned next. These notions were proposed in the binary cal-
ibration setting by Gupta et al. (2020) and Gupta and Ramdas (2021). In the deﬁnition below, A
refers to any algorithm that takes as input calibration data D and an initial classiﬁer g to produce a
top-label predictor c and an associated probability map h. Algorithm 8 is an example of A."
END,0.3059210526315789,"Deﬁnition 1 (Marginal and conditional top-label calibration). Let ε, α P p0, 1q be some given levels
of approximation and failure respectively. An algorithm A : pg, Dq ÞÑ pc, hq is"
END,0.30701754385964913,"(a) pε, αq-marginally top-label calibrated if for every distribution P over X ˆ rLs,"
END,0.3081140350877193,"P
´
|PpY “ cpXq | cpXq, hpXqq ´ hpXq| ď ε
¯
ě 1 ´ α.
(8)"
END,0.3092105263157895,"(b) pε, αq-conditionally top-label calibrated if for every distribution P over X ˆ rLs,"
END,0.31030701754385964,"P
´
@ l P rLs, r P Rangephq, |PpY “ cpXq | cpXq “ l, hpXq “ rq ´ r| ď ε
¯
ě 1 ´ α.
(9)"
END,0.31140350877192985,"To clarify, all probabilities are taken over the test point pX, Y q „ P, the calibration data D „ P n,
and any other inherent algorithmic randomness in A; these are all implicit in pc, hq “ ApD, gq.
Marginal calibration asserts that with high probability, on average over the distribution of D, X,
PpY “ cpXq | cpXq, hpXqq is at most ε away from hpXq. In comparison, TL-ECE is the average of
these deviations over X. Marginal calibration may be a more appropriate metric for calibration than
TL-ECE if we are somewhat agnostic to probabilistic errors less than some ﬁxed threshold ε (like
0.05). Conditional calibration is a strictly stronger deﬁnition that requires the deviation to be at most
ε for every possible prediction pl, rq, including rare ones, not just on average over predictions. This
may be relevant in medical settings where we want the prediction on every patient to be reasonably
calibrated. Algorithm 8 satisﬁes the following calibration guarantees."
END,0.3125,"Theorem 1. Fix hyperparameters δ ą 0 (arbitrarily small) and points per bin k ě 2, and assume
nl ě k for every l P rLs. Then, for any α P p0, 1q, Algorithm 8 is pε1, αq-marginally and pε2, αq-
conditionally top-label calibrated for ε1 “ d"
END,0.31359649122807015,"logp2{αq
2pk ´ 1q ` δ,
and
ε2 “ d"
END,0.31469298245614036,logp2n{kαq
END,0.3157894736842105,"2pk ´ 1q
` δ.
(10)"
END,0.3168859649122807,"Further, for any distribution P over X ˆ rLs, we have PpTL-ECEpc, hq ď ε2q ě 1 ´ α, and
E rTL-ECEpc, hqs ď
a"
END,0.31798245614035087,1{2k ` δ.
END,0.3190789473684211,"The proof in Appendix H is a multiclass top-label adaption of the guarantee in the binary setting by
Gupta and Ramdas (2021). The rOp1{
?"
END,0.3201754385964912,"kq dependence of the bound relies on Algorithm 8 delegating
at least k points to every bin. Since δ can be chosen to be arbitrarily small, setting k “ 50 gives
roughly ED rTL-ECEphqs ď 0.1. Base on this, we suggest setting k P r50, 150s in practice."
END,0.32127192982456143,"B.2
TOP-LABEL HISTOGRAM BINNING ADAPTS TO CLASS IMBALANCED DATASETS"
END,0.3223684210526316,"The principled methodology of ﬁxing the number of points per bin reaps practical beneﬁts. Fig-
ure 4 illustrates this through the performance of HB for the class imbalanced COVTYPE-7 dataset
(Blackard and Dean, 1999) with class ratio approximately 36% for class 1 and 49% for class 2. The
entire dataset has 581012 points which is divided into train-test in the ratio 70:30. Then, 10% of the
training points are held out for calibration (n “ |D| “ 40671). The base classiﬁer is a random forest
(RF) trained on the remaining training points (it achieves around 95% test accuracy). The RF is then
recalibrated using HB. The top-label reliability diagrams in Figure 4a illustrate that the original RF
(in orange) is underconﬁdent on both the most likely and least likely classes. Additional ﬁgures in
Appendix F show that the RF is always underconﬁdent no matter which class is predicted as the
top-label. HB (in green) recalibrates the RF effectively across all classes. Validity plots (Gupta and
Ramdas, 2021) estimate how the LHS of condition (8), denoted as V pεq, varies with ε. We observe
that for all ε, V pεq is higher for HB. The rightmost barplot compares the estimated TL-ECE for all
classes, and also shows the class proportions. While the original RF is signiﬁcantly miscalibrated for"
END,0.32346491228070173,Published as a conference paper at ICLR 2022
END,0.32456140350877194,"Random forest
Histogram binning
Class ratio"
END,0.3256578947368421,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.3267543859649123,True probability
END,0.32785087719298245,Class 2 reliability diagram
END,0.32894736842105265,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.3300438596491228,Class 2 validity plot
END,0.33114035087719296,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.33223684210526316,True probability
END,0.3333333333333333,Class 4 reliability diagram
END,0.3344298245614035,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.3355263157894737,Class 4 validity plot 1 2 3 4 5 6 7 Class 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
END,0.3366228070175439,Top-label ECE 0.0 0.1 0.2 0.3 0.4 0.5
END,0.33771929824561403,Class ratio
END,0.33881578947368424,"(a) Top-label histogram binning (Algorithm 8) with k “ 100 points per bin. Class 4 has only 183 calibration
points. Algorithm 8 adapts and uses only a single bin to ensure that the TL-ECE on class 4 is comparable to the
TL-ECE on class 2. Overall, the random forest classiﬁer has signiﬁcantly higher TL-ECE for the least likely
classes (4, 5, and 6), but the post-calibration TL-ECE using binning is quite uniform."
END,0.3399122807017544,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.34100877192982454,True probability
END,0.34210526315789475,Class 2 reliability diagram
END,0.3432017543859649,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.3442982456140351,Class 2 validity plot
END,0.34539473684210525,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.34649122807017546,True probability
END,0.3475877192982456,Class 4 reliability diagram
END,0.34868421052631576,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.34978070175438597,Class 4 validity plot 1 2 3 4 5 6 7 Class 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
END,0.3508771929824561,Top-label ECE 0.0 0.1 0.2 0.3 0.4 0.5
END,0.3519736842105263,Class ratio
END,0.3530701754385965,"(b) Histogram binning with B “ 50 bins for every class. Compared to Figure 4a, the post-calibration TL-ECE
for the most likely classes decreases while the TL-ECE for the least likely classes increases."
END,0.3541666666666667,"Figure 4: Recalibration of a random forest using histogram binning on the class imbalanced
COVTYPE-7 dataset (class 2 is roughly 100 times likelier than class 4). By ensuring a ﬁxed num-
ber of calibration points per bin, Algorithm 8 obtains relatively uniform top-label calibration across
classes (Figure 4a). In comparison, if a ﬁxed number of bins are chosen for all classes, the perfor-
mance deteriorates for the least likely classes (Figure 4b)."
END,0.35526315789473684,"the less likely classes, HB has a more uniform miscalibration across classes. Figure 4b considers a
slightly different HB algorithm where the number of points per class is not adapted to the number of
times the class is predicted, but is ﬁxed beforehand (this corresponds to replacing tnl{ku in line 5 of
Algorithm 8 with a ﬁxed B P N). While even in this setting there is a drop in the TL-ECE compared
to the RF model, the ﬁnal proﬁle is less uniform compared to ﬁxing the number of points per bin."
END,0.35635964912280704,"The validity plots and top-label reliability diagrams for all the 7 classes are reported in Figure 9 in
Appendix F, along with some additional observations."
END,0.3574561403508772,"C
DISTRIBUTION-FREE CLASS-WISE CALIBRATION USING HISTOGRAM
BINNING"
END,0.35855263157894735,"In this section, we formally describe histogram binning (HB) with the class-wise-calibrator (Algo-
rithm 3) and provide theoretical guarantees for it. The overall procedure is called class-wise-HB.
Further details and background on HB are contained in Appendix B, where top-label-HB is de-
scribed."
END,0.35964912280701755,"C.1
FORMAL ALGORITHM"
END,0.3607456140350877,"To achieve class-wise calibration using binary routines, we learn each component function hl in a 1-
v-all fashion as described in Algorithm 3. Algorithm 9 contains the pseudocode with the underlying
routine as binary HB. To learn hl, we use a dataset Dl, which unlike top-label HB (Algorithm 8),
contains Xi even if cpXiq ‰ l. However the Yi is replaced with 1 tYi “ lu. The number of points
per bin kl can be different for different classes, but generally one would set k1 “ . . . “ kL “ k P N.
Larger values of kl will lead to smaller εl and δl in the guarantees, at loss of sharpness since the
number of bins tn{klu would be smaller."
END,0.3618421052631579,Published as a conference paper at ICLR 2022
END,0.36293859649122806,Algorithm 9: Class-wise histogram binning
END,0.36403508771929827,"Input: Base multiclass predictor g : X Ñ ∆L´1, calibration data D “ pX1, Y1q, . . . , pXn, Ynq
Hyperparameter: # points per bin k1, k2, . . . , kl P NL (say each kl “ 50), tie-breaking
parameter δ ą 0 (say 10´10)
Output: L class-wise calibrated predictors h1, h2, . . . , hL"
END,0.3651315789473684,1 for l Ð 1 to L do
END,0.36622807017543857,"2
Dl Ð tpXi, 1 tYi “ luq : i P rnsqu;"
END,0.3673245614035088,"3
hl Ð Binary-histogram-binningpgl, Dl, tn{klu , δq;"
END,0.3684210526315789,4 end
END,0.36951754385964913,"5 return ph1, h2, . . . , hLq;"
END,0.3706140350877193,"C.2
CALIBRATION GUARANTEES"
END,0.3717105263157895,"A general algorithm A for class-wise calibration takes as input calibration data D and an initial
classiﬁer g to produce an approximately class-wise calibrated predictor h : X Ñ r0, 1sL. Deﬁne
the notation ε “ pε1, ε2, . . . , εLq P p0, 1qL and α “ pα1, α2, . . . , αLq P p0, 1qL.
Deﬁnition 2 (Marginal and conditional class-wise calibration). Let ε, α P p0, 1qL be some given
levels of approximation and failure respectively. An algorithm A : pg, Dq ÞÑ h is"
END,0.37280701754385964,"(a) pε, αq-marginally class-wise calibrated if for every distribution P over X ˆ rLs and for every
l P rLs
P
´
|PpY “ l | hlpXqq ´ hlpXq| ď εl
¯
ě 1 ´ αl.
(11)"
END,0.37390350877192985,"(b) pε, αq-conditionally class-wise calibrated if for every distribution P over X ˆ rLs and for every
l P rLs,"
END,0.375,"P
´
@r P Rangephlq, |PpY “ l | hlpXq “ rq ´ r| ď εl
¯
ě 1 ´ αl.
(12)"
END,0.37609649122807015,"Deﬁnition 2 requires that each hl is pεl, αlq calibrated in the binary senses deﬁned by Gupta et al.
(2021, Deﬁnitions 1 and 2). From Deﬁnition 2, we can also uniform bounds that hold simultaneously
over every l P rLs. Let α “ řL
l“1 αl and ε “ maxlPrLs εl. Then (11) implies"
END,0.37719298245614036,"P
´
@l P rLs, |PpY “ l | hlpXqq ´ hlpXq| ď ε
¯
ě 1 ´ α,
(13)"
END,0.3782894736842105,and (12) implies
END,0.3793859649122807,"P
´
@l P rLs, r P Rangephlq, |PpY “ l | hlpXq “ rq ´ r| ď ε
¯
ě 1 ´ α.
(14)"
END,0.38048245614035087,"The choice of not including the uniformity over L in Deﬁnition 2 reveals the nature of our class-wise
HB algorithm and the upcoming theoretical guarantees: (a) we learn the hl’s separately for each l
and do not combine the learnt functions in any way (such as normalization), (b) we do not combine
the calibration inequalities for different rLs in any other way other than a union bound. Thus the
only way we can show (13) (or (14)) is by using a union bound over (11) (or (12))."
END,0.3815789473684211,"We now state the distribution-free calibration guarantees satisﬁed by Algorithm 9.
Theorem 2. Fix hyperparameters δ ą 0 (arbitrarily small) and points per bin k1, k2, . . . , kl ě 2,
and assume nl ě kl for every l P rLs. Then, for every l P rLs, for any αl P p0, 1q, Algorithm 9 is
pεp1q, αq-marginally and pεp2q, αq-conditionally class-wise calibrated with"
END,0.3826754385964912,"εp1q
l
“ d"
END,0.38377192982456143,logp2{αlq
END,0.3848684210526316,"2pkl ´ 1q ` δ,
and
εp2q
l
“ d"
END,0.38596491228070173,logp2n{klαlq
END,0.38706140350877194,"2pkl ´ 1q
` δ.
(15)"
END,0.3881578947368421,"Further, for any distribution P over X ˆ rLs,"
END,0.3892543859649123,"(a) PpCW-ECEpc, hq ď maxlPrLs εp2q
l
q ě 1 ´ ř"
END,0.39035087719298245,"lPrLs αl, and"
END,0.39144736842105265,"(b) E rCW-ECEpc, hqs ď maxlPrLs
a"
END,0.3925438596491228,1{2kl ` δ.
END,0.39364035087719296,Published as a conference paper at ICLR 2022
END,0.39473684210526316,"Theorem 2 is proved in Appendix H. The proof follows by using the result of Gupta and Ramdas
(2021, Theorem 2), derived in the binary calibration setting, for each hl separately. Gupta and
Ramdas (2021) proved a more general result for general ℓp-ECE bounds. Similar results can also be
derived for the suitably deﬁned ℓp-CW-ECE."
END,0.3958333333333333,"As discussed in Section 3.2, unlike previous works (Zadrozny and Elkan, 2002; Guo et al., 2017;
Kull et al., 2019), Algorithm 9 does not normalize the hl’s. We do not know how to derive Theorem 2
style results for a normalized version of Algorithm 9."
END,0.3969298245614035,"D
FIGURES FOR APPENDIX E"
END,0.3980263157894737,"Appendix E begins on page 23. The relevant ﬁgures for Appendix E are displayed on the following
pages."
END,0.3991228070175439,Published as a conference paper at ICLR 2022
END,0.40021929824561403,"5
10
15
20
25
Number of bins 0.014 0.016 0.018 0.020 0.022 0.024 0.026"
END,0.40131578947368424,Estimated ECE
END,0.4024122807017544,ResNet-50
END,0.40350877192982454,"5
10
15
20
25
Number of bins"
END,0.40460526315789475,0.0125
END,0.4057017543859649,0.0150
END,0.4067982456140351,0.0175
END,0.40789473684210525,0.0200
END,0.40899122807017546,0.0225
END,0.4100877192982456,0.0250
END,0.41118421052631576,0.0275
END,0.41228070175438597,0.0300
END,0.4133771929824561,Estimated ECE
END,0.4144736842105263,ResNet-110
END,0.4155701754385965,"5
10
15
20
25
Number of bins"
END,0.4166666666666667,0.0100
END,0.41776315789473684,0.0125
END,0.41885964912280704,0.0150
END,0.4199561403508772,0.0175
END,0.42105263157894735,0.0200
END,0.42214912280701755,0.0225
END,0.4232456140350877,0.0250
END,0.4243421052631579,0.0275
END,0.42543859649122806,Estimated ECE
END,0.42653508771929827,Wide-ResNet-26-10
END,0.4276315789473684,"5
10
15
20
25
Number of bins"
END,0.42872807017543857,0.0150
END,0.4298245614035088,0.0175
END,0.4309210526315789,0.0200
END,0.43201754385964913,0.0225
END,0.4331140350877193,0.0250
END,0.4342105263157895,0.0275
END,0.43530701754385964,0.0300
END,0.43640350877192985,Estimated ECE
END,0.4375,DenseNet-121
END,0.43859649122807015,"(a) TL-ECE estimates on CIFAR-10 with Brier score. TL-HB is close to the best in each case. While CW-HB
performs the best at B “ 15, the ECE estimate may not be reliable since it is highly variable across bins."
END,0.43969298245614036,"5
10
15
20
25
Number of bins 0.10 0.15 0.20 0.25 0.30"
END,0.4407894736842105,Estimated ECE
END,0.4418859649122807,ResNet-50
END,0.44298245614035087,"5
10
15
20
25
Number of bins 0.10 0.15 0.20 0.25 0.30 0.35"
END,0.4440789473684211,Estimated ECE
END,0.4451754385964912,ResNet-110
END,0.44627192982456143,"5
10
15
20
25
Number of bins 0.10 0.15 0.20 0.25 0.30"
END,0.4473684210526316,Estimated ECE
END,0.44846491228070173,Wide-ResNet-26-10
END,0.44956140350877194,"5
10
15
20
25
Number of bins 0.10 0.15 0.20 0.25 0.30"
END,0.4506578947368421,Estimated ECE
END,0.4517543859649123,DenseNet-121
END,0.45285087719298245,"(b) TL-ECE estimates on CIFAR-100 with Brier score. N-HB is the best performing method, while DS is the
worst performing method, across different numbers of bins. TL-HB performs worse than TS and VS."
END,0.45394736842105265,"5
10
15
20
25
Number of bins 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
END,0.4550438596491228,Estimated ECE
END,0.45614035087719296,ResNet-50
END,0.45723684210526316,"5
10
15
20
25
Number of bins 0.1 0.2 0.3 0.4 0.5"
END,0.4583333333333333,Estimated ECE
END,0.4594298245614035,ResNet-110
END,0.4605263157894737,"5
10
15
20
25
Number of bins 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
END,0.4616228070175439,Estimated ECE
END,0.46271929824561403,Wide-ResNet-26-10
END,0.46381578947368424,"5
10
15
20
25
Number of bins 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
END,0.4649122807017544,Estimated ECE
END,0.46600877192982454,DenseNet-121
END,0.46710526315789475,"(c) TL-MCE estimates on CIFAR-10 with Brier score. The only reliably and consistently well-performing
method is TL-HB."
END,0.4682017543859649,"5
10
15
20
25
Number of bins 0.2 0.3 0.4 0.5 0.6"
END,0.4692982456140351,Estimated ECE
END,0.47039473684210525,ResNet-50
END,0.47149122807017546,"5
10
15
20
25
Number of bins 0.2 0.3 0.4 0.5 0.6"
END,0.4725877192982456,Estimated ECE
END,0.47368421052631576,ResNet-110
END,0.47478070175438597,"5
10
15
20
25
Number of bins 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
END,0.4758771929824561,Estimated ECE
END,0.4769736842105263,Wide-ResNet-26-10
END,0.4780701754385965,"5
10
15
20
25
Number of bins 0.2 0.3 0.4 0.5 0.6"
END,0.4791666666666667,Estimated ECE
END,0.48026315789473684,DenseNet-121
END,0.48135964912280704,"(d) TL-MCE estimates on CIFAR-100 with Brier score. DS is the worst performing method. Other methods
perform across different values of B."
END,0.4824561403508772,"Figure 5: Table 2 style results with the number of bins varied as B P r5, 25s. See Appendix E.5 for
further details. The captions summarize the ﬁndings in each case. In most cases, the ﬁndings are
similar to those with B “ 15. The notable exception is that performance of N-HB on CIFAR-10 for
TL-ECE while very good at B “ 15, is quite inconsistent when seen across different bins. In some
cases, the blue base model line and the orange temperature scaling line coincide. This occurs since
the optimal temperature on the calibration data was learnt to be T “ 1, which corresponds to not
changing the base model at all."
END,0.48355263157894735,Published as a conference paper at ICLR 2022
END,0.48464912280701755,"5
10
15
20
25
Number of bins 0.010 0.015 0.020 0.025 0.030 0.035 0.040 0.045"
END,0.4857456140350877,Estimated ECE
END,0.4868421052631579,ResNet-50
END,0.48793859649122806,"5
10
15
20
25
Number of bins 0.015 0.020 0.025 0.030 0.035 0.040"
END,0.48903508771929827,Estimated ECE
END,0.4901315789473684,ResNet-110
END,0.49122807017543857,"5
10
15
20
25
Number of bins 0.015 0.020 0.025 0.030 0.035 0.040 0.045 0.050"
END,0.4923245614035088,Estimated ECE
END,0.4934210526315789,Wide-ResNet-26-10
END,0.49451754385964913,"5
10
15
20
25
Number of bins 0.015 0.020 0.025 0.030 0.035 0.040 0.045 0.050 0.055"
END,0.4956140350877193,Estimated ECE
END,0.4967105263157895,DenseNet-121
END,0.49780701754385964,"(a) TL-ECE estimates on CIFAR-10 with focal loss. TL-HB is close to the best in each case. While CW-HB
performs the best at B “ 15, the ECE estimate may not be reliable since it is highly variable across bins."
END,0.49890350877192985,"5
10
15
20
25
Number of bins 0.05 0.10 0.15 0.20 0.25 0.30"
END,0.5,Estimated ECE
END,0.5010964912280702,ResNet-50
END,0.5021929824561403,"5
10
15
20
25
Number of bins 0.05 0.10 0.15 0.20 0.25 0.30"
END,0.5032894736842105,Estimated ECE
END,0.5043859649122807,ResNet-110
END,0.5054824561403509,"5
10
15
20
25
Number of bins 0.10 0.15 0.20 0.25 0.30"
END,0.506578947368421,Estimated ECE
END,0.5076754385964912,Wide-ResNet-26-10
END,0.5087719298245614,"5
10
15
20
25
Number of bins 0.05 0.10 0.15 0.20 0.25 0.30"
END,0.5098684210526315,Estimated ECE
END,0.5109649122807017,DenseNet-121
END,0.5120614035087719,"(b) TL-ECE estimates on CIFAR-100 with focal loss. N-HB is the best performing method, while DS is the
worst performing method, across different numbers of bins. TL-HB performs worse than TS and VS."
END,0.5131578947368421,"5
10
15
20
25
Number of bins 0.0 0.2 0.4 0.6 0.8"
END,0.5142543859649122,Estimated ECE
END,0.5153508771929824,ResNet-50
END,0.5164473684210527,"5
10
15
20
25
Number of bins 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
END,0.5175438596491229,Estimated ECE
END,0.518640350877193,ResNet-110
END,0.5197368421052632,"5
10
15
20
25
Number of bins 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
END,0.5208333333333334,Estimated ECE
END,0.5219298245614035,Wide-ResNet-26-10
END,0.5230263157894737,"5
10
15
20
25
Number of bins 0.1 0.2 0.3 0.4 0.5"
END,0.5241228070175439,Estimated ECE
END,0.5252192982456141,DenseNet-121
END,0.5263157894736842,"(c) TL-MCE estimates on CIFAR-10 with focal loss. The only reliably and consistently well-performing
method is TL-HB."
END,0.5274122807017544,"5
10
15
20
25
Number of bins 0.2 0.3 0.4 0.5 0.6 0.7"
END,0.5285087719298246,Estimated ECE
END,0.5296052631578947,ResNet-50
END,0.5307017543859649,"5
10
15
20
25
Number of bins 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
END,0.5317982456140351,Estimated ECE
END,0.5328947368421053,ResNet-110
END,0.5339912280701754,"5
10
15
20
25
Number of bins 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
END,0.5350877192982456,Estimated ECE
END,0.5361842105263158,Wide-ResNet-26-10
END,0.5372807017543859,"5
10
15
20
25
Number of bins 0.2 0.3 0.4 0.5 0.6"
END,0.5383771929824561,Estimated ECE
END,0.5394736842105263,DenseNet-121
END,0.5405701754385965,"(d) TL-MCE estimates on CIFAR-100 with focal loss. DS is the worst performing method. Other methods
perform across different values of B."
END,0.5416666666666666,"Figure 6: Table 4 style results with the number of bins varied as B P r5, 25s. See Appendix E.5 for
further details. The captions summarize the ﬁndings in each case. In most cases, the ﬁndings are
similar to those with B “ 15. In some cases, the blue base model line and the orange temperature
scaling line coincide. This occurs since the optimal temperature on the calibration data was learnt
to be T “ 1, which corresponds to not changing the base model at all."
END,0.5427631578947368,Published as a conference paper at ICLR 2022
END,0.543859649122807,"5
10
15
20
25
Number of bins 0.003 0.004 0.005 0.006 0.007"
END,0.5449561403508771,Estimated ECE
END,0.5460526315789473,ResNet-50
END,0.5471491228070176,"5
10
15
20
25
Number of bins 0.002 0.003 0.004 0.005 0.006 0.007 0.008"
END,0.5482456140350878,Estimated ECE
END,0.5493421052631579,ResNet-110
END,0.5504385964912281,"5
10
15
20
25
Number of bins"
END,0.5515350877192983,0.0025
END,0.5526315789473685,0.0030
END,0.5537280701754386,0.0035
END,0.5548245614035088,0.0040
END,0.555921052631579,0.0045
END,0.5570175438596491,0.0050
END,0.5581140350877193,0.0055
END,0.5592105263157895,Estimated ECE
END,0.5603070175438597,Wide-ResNet-26-10
END,0.5614035087719298,"5
10
15
20
25
Number of bins 0.002 0.003 0.004 0.005 0.006 0.007"
END,0.5625,Estimated ECE
END,0.5635964912280702,DenseNet-121
END,0.5646929824561403,"(a) CW-ECE estimates on CIFAR-10 with Brier score. CW-HB is the best performing method across bins, and
N-HB is quite unreliable."
END,0.5657894736842105,"5
10
15
20
25
Number of bins 0.001 0.002 0.003 0.004 0.005 0.006"
END,0.5668859649122807,Estimated ECE
END,0.5679824561403509,ResNet-50
END,0.569078947368421,"5
10
15
20
25
Number of bins 0.001 0.002 0.003 0.004 0.005 0.006 0.007"
END,0.5701754385964912,Estimated ECE
END,0.5712719298245614,ResNet-110
END,0.5723684210526315,"5
10
15
20
25
Number of bins 0.001 0.002 0.003 0.004 0.005 0.006"
END,0.5734649122807017,Estimated ECE
END,0.5745614035087719,Wide-ResNet-26-10
END,0.5756578947368421,"5
10
15
20
25
Number of bins 0.001 0.002 0.003 0.004 0.005 0.006"
END,0.5767543859649122,Estimated ECE
END,0.5778508771929824,DenseNet-121
END,0.5789473684210527,"(b) CW-ECE estimates on CIFAR-100 with Brier score. CW-HB is the best performing method. DS and N-HB
are the worst performing methods."
END,0.5800438596491229,"Figure 7: Table 3 style results with the number of bins varied as B P r5, 25s. The captions summarize
the ﬁndings in each case, which are consistent with those in the table. See Appendix E.5 for further
details."
END,0.581140350877193,"5
10
15
20
25
Number of bins 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.010"
END,0.5822368421052632,Estimated ECE
END,0.5833333333333334,ResNet-50
END,0.5844298245614035,"5
10
15
20
25
Number of bins 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009"
END,0.5855263157894737,Estimated ECE
END,0.5866228070175439,ResNet-110
END,0.5877192982456141,"5
10
15
20
25
Number of bins 0.002 0.004 0.006 0.008 0.010"
END,0.5888157894736842,Estimated ECE
END,0.5899122807017544,Wide-ResNet-26-10
END,0.5910087719298246,"5
10
15
20
25
Number of bins 0.004 0.006 0.008 0.010"
END,0.5921052631578947,Estimated ECE
END,0.5932017543859649,DenseNet-121
END,0.5942982456140351,"(a) CW-ECE estimates on CIFAR-10 with focal loss. CW-HB is the best performing method across bins, and
N-HB is quite unreliable."
END,0.5953947368421053,"5
10
15
20
25
Number of bins 0.001 0.002 0.003 0.004 0.005 0.006"
END,0.5964912280701754,Estimated ECE
END,0.5975877192982456,ResNet-50
END,0.5986842105263158,"5
10
15
20
25
Number of bins 0.001 0.002 0.003 0.004 0.005 0.006"
END,0.5997807017543859,Estimated ECE
END,0.6008771929824561,ResNet-110
END,0.6019736842105263,"5
10
15
20
25
Number of bins 0.001 0.002 0.003 0.004 0.005 0.006"
END,0.6030701754385965,Estimated ECE
END,0.6041666666666666,Wide-ResNet-26-10
END,0.6052631578947368,"5
10
15
20
25
Number of bins 0.001 0.002 0.003 0.004 0.005 0.006"
END,0.606359649122807,Estimated ECE
END,0.6074561403508771,DenseNet-121
END,0.6085526315789473,"(b) CW-ECE estimates on CIFAR-100 with focal loss. CW-HB is the best performing method. DS and N-HB
are the worst performing methods."
END,0.6096491228070176,"Figure 8: Table 5 style results with the number of bins varied as B P r5, 25s. The captions summarize
the ﬁndings in each case, which are consistent with those in the table. See Appendix E.5 for further
details."
END,0.6107456140350878,Published as a conference paper at ICLR 2022
END,0.6118421052631579,"E
ADDITIONAL EXPERIMENTAL DETAILS AND RESULTS FOR CIFAR-10 AND
CIFAR-100"
END,0.6129385964912281,"We present additional details and results to supplement the experiments with CIFAR-10 and CIFAR-
100 in Sections 2 and 4 of the main paper."
END,0.6140350877192983,"E.1
EXTERNAL LIBRARIES USED"
END,0.6151315789473685,"All our base models were pre-trained deep-net models generated by Mukhoti et al. (2020), obtained
from www.robots.ox.ac.uk/„viveka/focal calibration/ and used along with the
code at https://github.com/torrvision/focal calibration to obtain base predic-
tions. We focused on the models trained with Brier score and focal loss, since it was found to perform
the best for calibration. All reports in the main paper are with the Brier score; in Appendix E.4, we
report corresponding results with focal loss."
END,0.6162280701754386,"We also used the code at https://github.com/torrvision/focal calibration for
temperature scaling (TS). For vector scaling (VS) and Dirichlet scaling (DS), we used the code of
Kull et al. (2019), hosted at https://github.com/dirichletcal/dirichlet python.
For VS, we used the ﬁle dirichletcal/calib/vectorscaling.py, and for DS, we used
the ﬁle dirichletcal/calib/fulldirichlet.py. No hyperparameter tuning was per-
formed in any of our histogram binning experiments or baseline experiments; default settings were
used in every case. The random seed was ﬁxed so that every run of the experiment gives the same
result. In particular, by relying on pre-trained models, we avoid training new deep-net models with
multiple hyperparameters, thus avoiding any selection biases that may arise due to test-data peeking
across multiple settings."
END,0.6173245614035088,"E.2
FURTHER COMMENTS ON BINNING FOR ECE ESTIMATION"
END,0.618421052631579,"As mentioned in Remark 1, ECE estimates for all methods except TL-HB and CW-HB was done
using ﬁxed-width bins r0, 1{Bq, r1{B, 2{Bq, . . . r1 ´ 1{B, 1s for various values of B P r5, 25s. For
TL-HB and CW-HB, B is the number of bins used for each call to binary HB. For TL-HB, note that
we actually proposed that the number of bins-per-class should be ﬁxed; see Section B.2. However,
for ease of comparison to other methods, we simply set the number of bins to B for each call to
binary HB. That is, in line 5, we replace tnl{ku with B. For CW-HB, we described Algorithm 9
with different values of kl corresponding to the number of bins per class. For the CIFAR-10 and
CIFAR-100 comparisons, we set each k1 “ k2 “ . . . “ kL “ k, where k P N satisﬁes tn{ku “ B."
END,0.6195175438596491,"Tables 2,3, 4, and 5 report estimates with B “ 15, which has been commonly used in many works
(Guo et al., 2017; Kull et al., 2019; Mukhoti et al., 2020). Corresponding to each table, we have a
ﬁgure where ECE estimates with varying B are reported to strengthen conclusions: these are Fig-
ure 5,7, 6, and 8 respectively. Plugin estimates of the ECE were used, same as Guo et al. (2017).
Further binning was not done for TL-HB and CW-HB since the output is already discrete and suf-
ﬁciently many points take each of the predicted values. Note that due to Jensen’s inequality, any
further binning will only decrease the ECE estimate (Kumar et al., 2019). Thus, using unbinned
estimates may give TL-HB and CW-HB a disadvantage."
END,0.6206140350877193,"E.3
SOME REMARKS ON MAXIMUM-CALIBRATION-ERROR (MCE)"
END,0.6217105263157895,"Guo et al. (2017) deﬁned MCE with respect to conﬁdence calibration, as follows:"
END,0.6228070175438597,"conf-MCEpc, hq :“
sup
rPRangephq
|PpY “ cpXq | hpXq “ rq ´ r| .
(16)"
END,0.6239035087719298,"Conf-MCE suffers from the same issue illustrated in Figure 2 for conf-ECE. In Figure 1b, we looked
at the reliability diagram within two bins. These indicate two of the values over which the supremum
is taken in equation (16): these are the Y-axis distances between the ‹ markers and the X “ Y line
for bins 6 and 10 (both are less than 0.02). On the other hand, the effective maximum miscalibration
for bin 6 is roughly 0.15 (for class 1), and roughly 0.045 (for class 4), and the maximum should be
taken with respect to these values across all bins. To remedy the underestimation of the effective"
END,0.625,Published as a conference paper at ICLR 2022
END,0.6260964912280702,"Metric
Dataset
Architecture
Base
TS
VS
DS
N-HB
TL-HB"
END,0.6271929824561403,"Top-
label-
ECE"
END,0.6282894736842105,CIFAR-10
END,0.6293859649122807,"ResNet-50
0.022
0.023
0.018
0.019
0.023
0.019
ResNet-110
0.025
0.024
0.022
0.021
0.020
0.020
WRN-26-10
0.024
0.019
0.016
0.017
0.019
0.018
DenseNet-121
0.023
0.023
0.021
0.021
0.025
0.021"
END,0.6304824561403509,CIFAR-100
END,0.631578947368421,"ResNet-50
0.109
0.107
0.107
0.332
0.086
0.148
ResNet-110
0.124
0.117
0.105
0.316
0.115
0.153
WRN-26-10
0.100
0.100
0.101
0.293
0.074
0.135
DenseNet-121
0.106
0.108
0.105
0.312
0.091
0.147"
END,0.6326754385964912,"Top-
label-
MCE"
END,0.6337719298245614,CIFAR-10
END,0.6348684210526315,"ResNet-50
0.298
0.443
0.368
0.472
0.325
0.082
ResNet-110
0.378
0.293
0.750
0.736
0.535
0.089
WRN-26-10
0.741
0.582
0.311
0.363
0.344
0.075
DenseNet-121
0.411
0.411
0.243
0.391
0.301
0.099"
END,0.6359649122807017,CIFAR-100
END,0.6370614035087719,"ResNet-50
0.289
0.355
0.234
0.640
0.322
0.273
ResNet-110
0.293
0.265
0.274
0.633
0.366
0.272
WRN-26-10
0.251
0.227
0.256
0.663
0.229
0.270
DenseNet-121
0.237
0.225
0.239
0.597
0.327
0.248"
END,0.6381578947368421,"Table 4: Top-label-ECE and top-label-MCE for deep-net models and various post-hoc calibrators.
All methods are same as Table 2. Best performing method in each row is in bold."
END,0.6392543859649122,"MCE, we can consider the top-label-MCE, deﬁned as"
END,0.6403508771929824,"TL-MCEpc, hq :“ max
lPrLs
sup
rPRangephq
|PpY “ l | cpXq “ l, hpXq “ rq ´ r| .
(17)"
END,0.6414473684210527,"Interpreted in words, the TL-MCE assesses the maximum deviation between the predicted and true
probabilities across all predictions and all classes. Following the same argument as in the proof of
Proposition 4, it can be shown that for any c, h, conf-MCEpc, hq ď TL-MCEpc, hq. The TL-MCE
is closely related to conditional top-label calibration (Deﬁnition 1b). Clearly, an algorithm is pε, αq-
conditionally top-label calibrated if and only if for every distribution P, PpTL-MCEpc, hq ď εq ě
1 ´ α. Thus the conditional top-label calibration guarantee of Theorem 1 implies a high probability
bound on the TL-MCE as well."
END,0.6425438596491229,"E.4
TABLE 2 AND 3 STYLE RESULTS WITH FOCAL LOSS"
END,0.643640350877193,"Results for top-label-ECE and top-label-MCE with the base deep net model being trained using
focal loss are reported in Table 4. Corresponding results for class-wise-ECE are reported in Table 5.
The observations are similar to the ones reported for Brier score:"
END,0.6447368421052632,"1. For TL-ECE, TL-HB is either the best or close to the best performing method on CIFAR-
10, but suffers on CIFAR-100. This phenomenon is discussed further in Appendix E.6.
N-HB is the best or close to the best for both CIFAR-10 and CIFAR-100.
2. For TL-MCE, TL-HB is the best performing method on CIFAR-10, by a huge margin. For
CIFAR-100, TS or VS perform better than TL-HB, but not by a huge margin.
3. For CW-ECE, CW-HB is the best performing method across the two datasets and all four
architectures."
END,0.6458333333333334,"E.5
ECE AND MCE ESTIMATES WITH VARYING NUMBER OF BINS"
END,0.6469298245614035,"Corresponding to each entry in Tables 2 and 4, we perform an ablation study with the number of
bins varying as B P r5, 25s. This is in keeping with the ﬁndings of Roelofs et al. (2020) that the
ECE/MCE estimate can vary with different numbers of bins, along with the relative performance of
the various models."
END,0.6480263157894737,"The results are reported in Figure 5 (ablation of Table 2) and Figure 7 (ablation of Table 3). The
captions of these ﬁgures contain further details on the ﬁndings. Most ﬁndings are similar to those
in the main paper, but the ﬁndings in the tables are strengthened through this ablation. The same
ablations are performed for focal loss as well. The results are reported in Figure 6 (ablation of"
END,0.6491228070175439,Published as a conference paper at ICLR 2022
END,0.6502192982456141,"Metric
Dataset
Architecture
Base
TS
VS
DS
N-HB
CW-HB"
END,0.6513157894736842,"Class-
wise-
ECE
ˆ102"
END,0.6524122807017544,CIFAR-10
END,0.6535087719298246,"ResNet-50
0.42
0.42
0.35
0.37
0.52
0.35
ResNet-110
0.48
0.44
0.36
0.35
0.51
0.29
WRN-26-10
0.41
0.31
0.31
0.35
0.49
0.27
DenseNet-121
0.41
0.41
0.40
0.39
0.63
0.30"
END,0.6546052631578947,CIFAR-100
END,0.6557017543859649,"ResNet-50
0.22
0.20
0.20
0.66
0.23
0.16
ResNet-110
0.24
0.23
0.21
0.72
0.24
0.16
WRN-26-10
0.19
0.19
0.18
0.61
0.20
0.14
DenseNet-121
0.20
0.21
0.19
0.66
0.24
0.16"
END,0.6567982456140351,"Table 5: Class-wise-ECE for deep-net models and various post-hoc calibrators. All methods are
same as Table 2, except top-label-HB is replaced with class-wise-HB or Algorithm 3 (CW-HB).
Best performing method in each row is in bold."
END,0.6578947368421053,"Table 4) and Figure 8 (ablation of Table 5). The captions of these ﬁgures contain further details on
the ﬁndings. The ablation results in the ﬁgures support those in the tables."
END,0.6589912280701754,"E.6
ANALYZING THE POOR PERFORMANCE OF TL-HB ON CIFAR-100"
END,0.6600877192982456,"CIFAR-100 is an imbalanced dataset with 100 classes and 5000 points for validation/calibration
(as per the default splits). Due to random subsampling, the validation split we used had one of
the classes predicted as the top-label only 31 times. Thus, based on Theorem 1, we do not expect
HB to have small TL-ECE. This is conﬁrmed by the empirical results presented in Tables 2/4, and
Figures 5b/6b. We observe that HB has higher estimated TL-ECE than all methods except DS, for
most values of the number of bins. The performance of TL-HB for TL-MCE however is much much
closer to the other methods since HB uses the same number of points per bin, ensuring that the
predictions are somewhat equally calibrated across bins (Figures 5d/6d). In comparison, for CW-
ECE, CW-HB is the best performing method. This is because in the class-wise setting, 5000 points
are available for recalibration irrespective of the class, which is sufﬁcient for HB."
END,0.6611842105263158,"The deterioration in performance of HB when few calibration points are available was also observed
in the binary setting by Gupta and Ramdas (2021, Appendix C). Niculescu-Mizil and Caruana (2005)
noted in the conclusion of their paper that Platt scaling (Platt, 1999), which is closely related to TS,
performs well when the data is small, but another nonparametric binning method, isotonic regression
(Zadrozny and Elkan, 2002) performs better when enough data is available. Kull et al. (2019, Section
4.1) compared HB to other calibration techniques for class-wise calibration on 21 UCI datasets, and
found that HB performs the worst. On inspecting the UCI repository, we found that most of the
datasets they used had fewer than 5000 (total) data points, and many contain fewer than 500."
END,0.6622807017543859,"Overall, comparing our results to previous empirical studies, we believe that if sufﬁciently many
points are available for recalibration, or the number of classes is small, then HB performs quite well.
To be more precise, we expect HB to be competitive if at least 200 points per class can be held out
for recalibration, and the number of points per bin is at least k ě 20."
END,0.6633771929824561,"F
ADDITIONAL EXPERIMENTAL DETAILS AND RESULTS FOR COVTYPE-7"
END,0.6644736842105263,"We present additional details and results for the top-label HB experiment of Section B.2. The base
classiﬁer is an RF learnt using sklearn.ensemble import RandomForestClassifier
with default parameters. The base RF is a nearly continuous base model since most predictions are
unique. Thus, we need to use binning to make reliability diagrams, validity plots, and perform ECE
estimation, for the base model. To have a fair comparison, instead of having a ﬁxed binning scheme
to assess the base model, the binning scheme was decided based on the unique predictions of top-
label HB. Thus for every l, and r P Rangephlq, the bins are deﬁned as tx : cpxq “ l, hlpxq “ ru.
Due to this, while the base model in Figures 4a and 4b are the same, the reliability diagrams and
validity plots in orange are different. As can be seen in the bar plots in Figure 4, the ECE estimation
is not affected signiﬁcantly."
END,0.6655701754385965,"When k “ 100, the total number of bins chosen by Algorithm 8 was 403, which is roughly 57.6
bins per class. The choice of B “ 50 for the ﬁxed bins per class experiment was made on this basis."
END,0.6666666666666666,Published as a conference paper at ICLR 2022
END,0.6677631578947368,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.668859649122807,True probability
END,0.6699561403508771,Class 1 reliability diagram
END,0.6710526315789473,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.6721491228070176,Class 1 validity plot
END,0.6732456140350878,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.6743421052631579,True probability
END,0.6754385964912281,Class 2 reliability diagram
END,0.6765350877192983,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.6776315789473685,Class 2 validity plot
END,0.6787280701754386,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.6798245614035088,True probability
END,0.680921052631579,Class 3 reliability diagram
END,0.6820175438596491,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.6831140350877193,Class 3 validity plot
END,0.6842105263157895,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.6853070175438597,True probability
END,0.6864035087719298,Class 4 reliability diagram
END,0.6875,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.6885964912280702,Class 4 validity plot
END,0.6896929824561403,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.6907894736842105,True probability
END,0.6918859649122807,Class 5 reliability diagram
END,0.6929824561403509,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.694078947368421,Class 5 validity plot
END,0.6951754385964912,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.6962719298245614,True probability
END,0.6973684210526315,Class 6 reliability diagram
END,0.6984649122807017,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.6995614035087719,Class 6 validity plot
END,0.7006578947368421,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.7017543859649122,True probability
END,0.7028508771929824,Class 7 reliability diagram
END,0.7039473684210527,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.7050438596491229,Class 7 validity plot
END,0.706140350877193,(a) Top-label HB with k “ 100 points per bin.
END,0.7072368421052632,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.7083333333333334,True probability
END,0.7094298245614035,Class 1 reliability diagram
END,0.7105263157894737,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.7116228070175439,Class 1 validity plot
END,0.7127192982456141,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.7138157894736842,True probability
END,0.7149122807017544,Class 2 reliability diagram
END,0.7160087719298246,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.7171052631578947,Class 2 validity plot
END,0.7182017543859649,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.7192982456140351,True probability
END,0.7203947368421053,Class 3 reliability diagram
END,0.7214912280701754,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.7225877192982456,Class 3 validity plot
END,0.7236842105263158,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.7247807017543859,True probability
END,0.7258771929824561,Class 4 reliability diagram
END,0.7269736842105263,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.7280701754385965,Class 4 validity plot
END,0.7291666666666666,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.7302631578947368,True probability
END,0.731359649122807,Class 5 reliability diagram
END,0.7324561403508771,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.7335526315789473,Class 5 validity plot
END,0.7346491228070176,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.7357456140350878,True probability
END,0.7368421052631579,Class 6 reliability diagram
END,0.7379385964912281,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.7390350877192983,Class 6 validity plot
END,0.7401315789473685,"0.00
0.25
0.50
0.75
1.00
Predicted probability 0.0 0.2 0.4 0.6 0.8 1.0"
END,0.7412280701754386,True probability
END,0.7423245614035088,Class 7 reliability diagram
END,0.743421052631579,"0.00
0.05
0.10
0.15 0.0 0.2 0.4 0.6 0.8 1.0 V( )"
END,0.7445175438596491,Class 7 validity plot
END,0.7456140350877193,(b) Top-label HB with B “ 50 bins per class.
END,0.7467105263157895,"Figure 9: Top-label histogram binning (HB) calibrates a miscalibrated random-forest on the class
imbalanced COVTYPE-7 dataset. For the less likely classes (4, 5, and 6), the left column is bet-
ter calibrated than the right column. Similar observations are made on other datasets, and so we
recommend adaptively choosing a different number of bins per class, as Algorithm 8 does."
END,0.7478070175438597,Published as a conference paper at ICLR 2022
END,0.7489035087719298,"Figure 9 supplements Figure 4 in the main paper by presenting reliability diagrams and validity
plots of top-label HB for all classes. Figure 9a presents the plots with adaptive number of bins per
class (Algorithm 8), and Figure 9b presents these for ﬁxed number of bins per class. We make the
following observations."
END,0.75,"(a) For every class l P rLs, the RF is overconﬁdent. This may seem surprising at ﬁrst since we
generally expect that models may be overconﬁdent for certain classes and underconﬁdent
for others. However, note that all our plots assess top-label calibration, that is, we are
assessing the predicted and true probabilities of only the predicted class. It is possible that
a model is overconﬁdent for every class whenever that class is predicted to be the top-label.
(b) For the most likely classes, namely classes 1 and 2, the number of bins in the adaptive case
is higher than 50. Fewer bins leads to better calibration (at the cost of sharpness). This can
be veriﬁed through the validity plots for classes 1 and 2—the validity plots in the ﬁxed bins
case is slightly above the validity plot in the adaptive bin case. However both validity plots
are quite similar.
(c) The opposite is true for the least likely classes, namely classes 4, 5, 6. The validity plot
in the ﬁxed bins case is below the validity plot in the adaptive bins case, indicating higher
TL-ECE in the ﬁxed bins case. The difference between the validity plots is high. Thus if
a ﬁxed number of bins per class is pre-decided, the performance for the least likely classes
signiﬁcantly suffers."
END,0.7510964912280702,"Based on these observations, we recommend adaptively choosing the number of bins per class, as
done by Algorithm 8."
END,0.7521929824561403,"G
BINNING-BASED CALIBRATORS FOR CANONICAL MULTICLASS
CALIBRATION"
END,0.7532894736842105,"Canonical calibration is a notion of calibration that does not fall in the M2B category. To deﬁne
canonical calibration, we use Y to denote the output as a 1-hot vector. That is, Yi “ eYi P
∆L´1, where el corresponds to the l-th canonical basis vector in Rd. Recall that a predictor h “
ph1, h2, . . . , hLq is said to be canonically calibrated if PpY “ l | hpXqq “ hlpXq for every l P rLs.
Equivalently, this can be stated as E rY | hpXqs “ hpXq. Canonical calibration implies class-wise
calibration:
Proposition 1. If E rY | hpXqs “ hpXq, then for every l P rLs, PpY “ l | hlpXqq “ hlpXq."
END,0.7543859649122807,"The proof in Appendix H is straightforward, but the statement above is illuminating, because there
exist predictors that are class-wise calibrated but not canonically calibrated (Vaicenavicius et al.,
2019, Example 1)."
END,0.7554824561403509,"Canonical calibration is not an M2B notion since the conditioning occurs on the L-dimensional
prediction vector predpXq “ hpXq, and after this conditioning, each of the L statements PpY “
l | predpXqq “ hlpXq should simultaneously be true. On the other hand, M2B notions verify only
individual binary calibration claims for every such conditioning. Since canonical calibration does
not fall in the M2B category, Algorithm 5 does not lead to a calibrator for canonical calibration. In
this section, we discuss alternative binning-based approaches to achieving canonical calibration."
END,0.756578947368421,"For binary calibration, there is a complete ordering on the interval r0, 1s, and this ordering is lever-
aged by binning based calibration algorithms. However, ∆L´1, for L ě 3 does not have such a
natural ordering. Hence, binning algorithms do not obviously extend for multiclass classiﬁcation.
In this section, we brieﬂy discuss some binning-based calibrators for canonical calibration. Our
descriptions are for general L ě 3, but we anticipate these algorithms to work reasonably only for
small L, say if L ď 5."
END,0.7576754385964912,"As usual, denote g : X Ñ ∆L´1 as the base model and h : X Ñ ∆L´1 as the model learnt us-
ing some post-hoc canonical calibrator. For canonical calibration, we can surmise binning schemes
that directly learn h by partitioning the prediction space ∆L´1 into bins and estimating the dis-
tribution of Y in each bin. A canonical calibration guarantee can be showed for such a binning
scheme using multinomial concentration (Podkopaev and Ramdas, 2021, Section 3.1). However,
since Volp∆L´1q “ 2ΘpLq, there will either be a bin whose volume is 2ΩpLq (meaning that h would"
END,0.7587719298245614,Published as a conference paper at ICLR 2022
END,0.7598684210526315,"not be sharp), or the number of bins will be 2ΩpLq, entailing 2ΩpLq requirements on the sample
complexity—a curse of dimensionality. Nevertheless, let us consider some binning schemes that
could work if L is small."
END,0.7609649122807017,"Formally, a binning scheme corresponds to a partitioning of ∆L´1 into B ě 1 bins. We denote
this binning scheme as B : ∆L´1 Ñ rBs, where Bpsq corresponds to the bin to which s P ∆L´1
belongs. To learn h, the calibration data is binned to get sets of data-point indices that belong to
each bin, depending on the gpXiq values:
for every b P rBs, Tb :“ ti : BpgpXiqq “ bu, nb “ |Tb| .
We then compute the following estimates for the label probabilities in each bin:"
END,0.7620614035087719,"for every pl, bq P rLs ˆ rBs, pΠl,b :“ ř"
END,0.7631578947368421,iPTb 1 tYi “ lu
END,0.7642543859649122,"nb
if nb ą 0 else pΠl,b “ 1{B."
END,0.7653508771929824,The binning predictor h : X Ñ ∆L´1 is now deﬁned component-wise as follows:
END,0.7664473684210527,"for every l P rLs, hlpxq “ pΠl,Bpxq."
END,0.7675438596491229,"In words, for every bin b P rBs, h predicts the empirical distribution of the Y values in bin b."
END,0.768640350877193,"Using a multinomial concentration inequality (Devroye, 1983; Qian et al., 2020; Weissman et al.,
2003), calibration guarantees can be shown for the learnt h. Podkopaev and Ramdas (2021, Theorem
3) show such a result using the Bretagnolle-Huber-Carol inequality. All of these concentration
inequality give bounds that depend inversely on nb or ?nb."
END,0.7697368421052632,"In the following subsections, we describe some binning schemes which can be used for canoni-
cal calibration based on the setup illustrated above. First we describe ﬁxed schemes that are not
adaptive to the distribution of the data: Sierpinski binning (Appendix G.1) and grid-style binning
(Appendix G.2). These are analogous to ﬁxed-width binning for L “ 2. Fixed binning schemes
are not adapted to the calibration data and may have highly imbalanced bins leading to poor esti-
mation of the distribution of Y in bins with small nb. In the binary case, this issue is remedied
using histogram binning to ensure that each bin has nearly the same number of calibration points
(Gupta and Ramdas, 2021). While histogram binning uses the order of the scalar gpXiq values,
there is no obvious ordering for the multi-dimensional gpXiq values. In Appendix G.3 we describe
a projection based histogram binning scheme that circumvents this issue and ensures that each nb
is reasonably large. In Appendix G.4, we present some preliminary experimental results using our
proposed binning schemes."
END,0.7708333333333334,"Certain asymptotic consistency results different from calibration have been established for histogram
regression and classiﬁcation in the nonparametric statistics literature (Nobel, 1996; Lugosi and No-
bel, 1996; Gordon and Olshen, 1984; Breiman et al., 2017; Devroye, 1988); further extensive refer-
ences can be found within these works. The methodology of histogram regression and classiﬁcation
relies on binning and is very similar to the one we propose here. The main difference is that these
works consider binning the feature space X directly, unlike the post-hoc setting where we are essen-
tially interested in binning ∆L´1. In terms of theory, the results these works target are asymptotic
consistency for the (Bayes) optimal classiﬁcation and regression functions, instead of canonical cal-
ibration. It would be interesting to consider the (ﬁnite-sample) canonical calibration properties of
the various algorithms proposed in the context of histogram classiﬁcation. However, such a study is
beyond the scope of this paper."
END,0.7719298245614035,"G.1
SIERPINSKI BINNING"
END,0.7730263157894737,"First, we describe Sierpinski binning for L “ 3. The probability simplex for L “ 3, ∆2, is a triangle
with vertices e1 “ p1, 0, 0q, e2 “ p0, 1, 0q, and e3 “ p0, 0, 1q. Sierpinski binning is a recursive
partitioning of this triangle based on the fractal popularly known as the Sierpinski triangles. Some
Sierpinski bins for L “ 3 are shown in Figure 10. Formally, we deﬁne Sierpinski binning recursively
based on a depth parameter q P N. Given an x P X, let s “ gpxq. For q “ 1, the number of bins is
B “ 4, and the binning scheme B is deﬁned as:"
END,0.7741228070175439,"Bpsq “ $
’
& ’
%"
END,0.7752192982456141,"1
if s1 ą 0.5
2
if s2 ą 0.5
3
if s3 ą 0.5
4
otherwise. (18)"
END,0.7763157894736842,Published as a conference paper at ICLR 2022
END,0.7774122807017544,"∆2
q “ 1
q “ 2
q “ 3"
END,0.7785087719298246,"Figure 10: Sierpinski binning for L “ 3. The leftmost triangle represents the probability simplex
∆2. Sierpinski binning divides ∆2 recursively based on a depth parameter q P N."
END,0.7796052631578947,"Note that since s1 ` s2 ` s3 “ 1, only one of the above conditions can be true. It can be veriﬁed
that each of the bins have volume equal to p1{4q-th the volume of the probability simplex ∆2. If a
ﬁner resolution of ∆2 is desired, B can be increased by further dividing the partitions above. Note
that each partition is itself a triangle; thus each triangle can be mapped to ∆2 to recursively deﬁne
the sub-partitioning. For i P r4s, deﬁne the bins bi “ ts : Bpsq “ iu. Consider the bin b1. Let us
reparameterize it as pt1, t2, t3q “ p2s1 ´ 1, 2s2, 2s3q. It can be veriﬁed that"
END,0.7807017543859649,"b1 “ tpt1, t2, t3q : s1 ą 0.5u “ tpt1, t2, t3q : t1 ` t2 ` t3 “ 1, t1 P p0, 1s, t2 P r0, 1q, t3 P r0, 1qu."
END,0.7817982456140351,"Based on this reparameterization, we can recursively sub-partition b1 as per the scheme (18), replac-
ing s with t. Such reparameterizations can be deﬁned for each of the bins deﬁned in (18):"
END,0.7828947368421053,"b2 “ tps1, s2, s3q : s2 ą 0.5u : pt1, t2, t3q “ p2s1, 2s2 ´ 1, 2s3q,
b3 “ tps1, s2, s3q : s3 ą 0.5u : pt1, t2, t3q “ p2s1, 2s2, 2s3 ´ 1q,
b4 “ tps1, s2, s3q : si ď 0.5 for all iu : pt1, t2, t3q “ p1 ´ 2s1, 1 ´ 2s2, 1 ´ 2s3q,"
END,0.7839912280701754,"and thus every bin can be recursively sub-partitioned as per (18). As illustrated in Figure 10, for
Sierpinski binning, we sub-partition only the bins b1, b2, b3 since the bin b4 corresponds to low
conﬁdence for all labels, where ﬁner calibration may not be needed. (Also, in the L ą 3 case
described shortly, the corresponding version of b4 is geometrically different from ∆L´1, and the
recursive partitioning cannot be deﬁned for it.) If at every depth, we sub-partition all bins except
the corresponding b4 bins, then it can be shown using simple algebra that the total number of bins
is p3q`1 ´ 1q{2. For example, in Figure 10, when q “ 2, the number of bins is B “ 14, and when
q “ 3, the number of bins is B “ 40."
END,0.7850877192982456,"As in the case of L “ 3, Sierpinski binning for general L is deﬁned through a partitioning function
of ∆L´1 into L ` 1 bins, and a reparameterization of the partitions so that they can be further
sub-partitioned. The L ` 1 bins at depth q “ 1 are deﬁned as"
END,0.7861842105263158,"Bpsq “
""
l
if sl ą 0.5,
L ` 1
otherwise.
(19)"
END,0.7872807017543859,"While this looks similar to the partitioning (18), the main difference is that the bin bL`1 has a
larger volume than other bins. Indeed for l P rLs, volpblq “ volp∆L´1q{2L´1, while volpbL`1q “
volp∆L´1qp1 ´ L{2L´1q ě volp∆L´1q{2L´1, with equality only occuring for L “ 3. Thus the
bin bL`1 is larger than the other bins. If gpxq P bL`1, then the prediction for x may be not be
very sharp, compared to if gpxq were in any of the other bins. On the other hand, if gpxq P bL`1,
the score for every class is smaller than 0.5, and sharp calibration may often not be desired in this
region."
END,0.7883771929824561,"In keeping with this understanding, we only reparameterize the bins b1, b2, . . . , bL so that they can
be further divided:"
END,0.7894736842105263,"bl “ tps1, s2, . . . , sLq : sl ą 0.5u : pt1, t2, . . . , tLq “ p2s1, . . . , 2sl ´ 1, . . . , 2sLq."
END,0.7905701754385965,"For every l P rLs, under the reparameterization above, it is straightforward to verify that"
END,0.7916666666666666,"tpt1, t2, . . . , tLq : sl ą 0.5u “ tpt1, t2, . . . , tLq :
ÿ"
END,0.7927631578947368,"uPrLs
tu “ 1, tl P p0, 1s, tu P r0, 1q @u ‰ lu."
END,0.793859649122807,"Thus every bin can be recursively sub-partitioned following (19). For Sierpinski binning with L
labels, the number of bins at depth q is given by pLq`1 ´ 1q{pL ´ 1q."
END,0.7949561403508771,"Published as a conference paper at ICLR 2022 0
1 0 0.25 0.5 0.75 1
0 1"
END,0.7960526315789473,"0.25
0.5"
END,0.7971491228070176,Class 2
END,0.7982456140350878,Class 3
END,0.7993421052631579,Class 1 0.75 0.25 0.5 0.75
END,0.8004385964912281,"K “ 4, τ “ 0.25"
END,0.8015350877192983,"0
0.2
0.4
0.6
0.8
1 0 0.2 0.4 0.6 0.8 1
0 0.2 0.4 0.6 0.8 1"
END,0.8026315789473685,Class 2
END,0.8037280701754386,Class 3
END,0.8048245614035088,Class 1
END,0.805921052631579,"K “ 5, τ “ 0.2"
END,0.8070175438596491,Figure 11: Grid-style binning for L “ 3.
END,0.8081140350877193,"G.2
GRID-STYLE BINNING"
END,0.8092105263157895,"Grid-style binning is motivated from the 2D reliability diagrams of Widmann et al. (2019, Figure 1),
where they partitioned ∆2 into multiple equi-volume bins in order to assess canonical calibration on
a 3-class version of CIFAR-10. For L “ 3, ∆2 can be divided as shown in Figure 11. This corre-
sponds to gridding the space ∆2, just the way we think of gridding the real hyperplane. However,
the mathematical description of this grid for general L is not apparent from Figure 11. We describe
grid-style binning formally for general L ě 3."
END,0.8103070175438597,"Consider some τ ą 0 such that K :“ 1{τ P N. For every tuple k “ pk1, k2, . . . , kLq in the set"
END,0.8114035087719298,"I “ tk P NL : maxpL, K ` 1q ď
ÿ"
END,0.8125,"lPrLs
kl ď K ` pL ´ 1qu,
(20)"
END,0.8135964912280702,"deﬁne the bins
bk :“ ts P ∆L´1 : for every l P rLs, slK P rkl ´ 1, kls.
(21)"
END,0.8146929824561403,"These bins are not mutually disjoint, but intersections can only occur at the edges. That is, for
every s that belongs to more than one bin, at least one component sl satisies slK P N. In order
to identify a single bin s, ties can be broken arbitrarily. One strategy is to use some ordering on
NL; say for k1 ‰ k2 P N, k1 ă k2 if and only if for the ﬁrst element of k1 that is unequal to
the corresponding element of k2 the one corresponding to k1 is smaller. Then deﬁne the binning
function B : ∆L´1 Ñ |I| as Bpsq “ mintk : s P bku. The following propositions prove that a)
each s belongs to at least one bin, and b) that every bin is an L ´ 1 dimensional object (and thus a
meaningful partition of ∆L´1)."
END,0.8157894736842105,Proposition 2. The bins tbk : k P Iu deﬁned by (21) mutually exhaust ∆L´1.
END,0.8168859649122807,"Proof. Consider any s P ∆L´1. For slK R N “ t1, 2, . . .u, set kl “ maxp1, rslKsq ą slK.
Consider the condition
C : for all l such that slK R N, slK “ 0."
END,0.8179824561403509,"If C is true, then for l such that slK P N, set kl “ slK. If C is not true, then for l such that slK P N,
set exactly one kl “ slK ` 1, and for the rest set kl “ slK. Based on this setting of k, it can be
veriﬁed that s P bk."
END,0.819078947368421,"Further, note that for every l, kl ě slK, and there exists at least one l such that kl ą slK. Thus we
have: L
ÿ"
END,0.8201754385964912,"l“1
kl ą L
ÿ"
END,0.8212719298245614,"l“1
slK “ K."
END,0.8223684210526315,"Since řL
l“1 kl P N, we must have řL
l“1 kl ě K ` 1. Further since each kl P N, řL
l“1 kl ě L."
END,0.8234649122807017,"Next, note that for every l, kl ď slK ` 1. If C is true, then there is at least one l such that slK P N,
and for this l, we have set kl “ slK ă slK ` 1. If C is not true, then either there exists at least one
l such that slK R N Y t0u for which kl “ rslKs ă slK ` 1, or every slK P N, in which case we"
END,0.8245614035087719,Published as a conference paper at ICLR 2022
END,0.8256578947368421,"have set kl “ slK for one of them. In all cases, note that there exists an l such that kl ă slK ` 1.
Thus, L
ÿ"
END,0.8267543859649122,"l“1
kl ă L
ÿ"
END,0.8278508771929824,"l“1
pslK ` 1q"
END,0.8289473684210527,“ K ` L.
END,0.8300438596491229,"Since řL
l“1 kl P N, we must have řL
l“1 kl ď K ` L ´ 1."
END,0.831140350877193,"Next, we show that each bin indexed by k P I contains a non-zero volume subset of ∆L´1, where
volume is deﬁned with respect to the Lebesgue measure in RL´1. This can be shown by arguing
that bk contains a scaled and translated version of ∆L“1.
Proposition 3. For every k P I, there exists some u P RL and v ą 0 such that u ` v∆L´1 Ď bk."
END,0.8322368421052632,"Proof. Fix some k P I. By condition (20), řL
l“1 kl P rmaxpL, K ` 1q, K ` L ´ 1s. Based on this,
we claim that there exists a τ P r0, 1q such that L
ÿ"
END,0.8333333333333334,"l“1
pkl ´ 1q ` τL ` p1 ´ τq “ K.
(22)"
END,0.8344298245614035,"Indeed, note that for τ “ 0, řL
l“1pkl ´ 1q ` τL ` p1 ´ τq ď pK ´ 1q ` 1 “ K and for τ “ 1,
řL
l“1pkl ´ 1q ` τL ` p1 ´ τq “ řL
l“1 kl ą K. Thus, there exists a τ that satisﬁes (22) by the
intermediate value theorem."
END,0.8355263157894737,"Next, deﬁne u “ K´1pk ` pτ ´ 1q1Lq and v “ K´1p1 ´ τq ą 0, where 1L denotes the vector
in RL with each component equal to 1. Consider any s P u ` v∆L´1. Note that for every l P rLs,
slK P rkl ´ 1, kls and by property (22), L
ÿ"
END,0.8366228070175439,"l“1
slK “ ˜ L
ÿ"
END,0.8377192982456141,"l“1
pkl ` pτ ´ 1qq ¸ ` v “ L
ÿ"
END,0.8388157894736842,"l“1
pkl ´ 1q ` τL ` p1 ´ τq “ K."
END,0.8399122807017544,"Thus, s P ∆L´1 and by the deﬁnition of bk, s P bk. This completes the proof."
END,0.8410087719298246,"The previous two propositions imply that B satisﬁes the property we require of a reasonable binning
scheme. For L “ 3, grid-style binning gives equi-volume bins as illustrated in Figure 11; however
this is not true for L ą 3. We now describe a histogram binning based partitioning scheme."
END,0.8421052631578947,"G.3
PROJECTION BASED HISTOGRAM BINNING FOR CANONICAL CALIBRATION"
END,0.8432017543859649,"Some of the bins deﬁned by Sierpinski binning and grid-style binning may have very few calibration
points nb, leading to poor estimation of pΠ. In the binary calibration case, this can be remedied using
histogram binning which strongly relies on the scoring function g taking values in a fully ordered
space r0, 1s. To ensure that each bin contains Ωpn{Bq points, we estimate the quantiles of gpXq
and created the bins as per these quantiles. However, there are no natural quantiles for unordered
prediction spaces such as ∆L´1 (L ě 3). In this section, we develop a histogram binning scheme
for ∆L´1 that is semantically interpretable and has desirable statistical properties."
END,0.8442982456140351,"Our algorithm takes as input a prescribed number of bins B and an arbitrary sequence of vectors
q1, q2, . . . , qB´1 P RL with unit ℓ2-norm: }qi}2 “ 1. Each of these vectors represents a direction
on which we will project ∆L´1 in order to induce a full order on ∆L´1. Then, for each direction,
we will use an order statistics on the induced full order to identify a bin with exactly tpn ` 1q{Bu ´
1 calibration points (except the last bin, which may have more points). The formal algorithm is
described in Algorithm 10. It uses the following new notation: given m vectors v1, v2, . . . , vm P RL,
a unit vector u, and an index j P rms, let order-statisticsptv1, v2, . . . , vmu, u, jq denote the j-th
order-statistics of tvT
1 u, vT
2 u, . . . , vT
muu."
END,0.8453947368421053,Published as a conference paper at ICLR 2022
END,0.8464912280701754,Algorithm 10: Projection histogram binning for canonical calibration
END,0.8475877192982456,"Input: Base multiclass predictor g : X Ñ ∆L´1, calibration data
D “ tpX1, Y1q, pX2, Y2q, . . . , pXn, Ynqu
Hyperparameter: number of bins B, unit vectors q1, q2, . . . , qB P RL,
Output: Approximately calibrated scoring function h"
END,0.8486842105263158,"1 S Ð tgpX1q, gpX2q, . . . , gpXnqu;"
END,0.8497807017543859,2 T Ð empty array of size B;
END,0.8508771929824561,3 c Ð t n`1 B u;
END,0.8519736842105263,4 for b Ð 1 to B ´ 1 do
END,0.8530701754385965,"5
Tb Ð order-statisticspS, qb, cq;"
END,0.8541666666666666,"6
S Ð Sztv P S : vT qb ď Tbu;"
END,0.8552631578947368,7 end
END,0.856359649122807,8 TB Ð 1.01;
END,0.8574561403508771,9 Bpgp¨qq Ð mintb P rBs : gp¨qT qb ă Tbu;
END,0.8585526315789473,10 pΠ Ð empty matrix of size B ˆ L;
END,0.8596491228070176,11 for b Ð 1 to B do
END,0.8607456140350878,"12
for l Ð 1 to L do"
END,0.8618421052631579,"13
pΠb,l Ð Meant1 tYi “ lu : BpgpXiqq “ b and @s P rBs, gpXiqT qs ‰ Tsu;"
END,0.8629385964912281,"14
end"
END,0.8640350877192983,15 end
END,0.8651315789473685,16 for l Ð 1 to L do
END,0.8662280701754386,"17
hlp¨q Ð pΠBpgp¨qq,l;"
END,0.8673245614035088,18 end
END,0.868421052631579,19 return h;
END,0.8695175438596491,"We now brieﬂy describe some values computed by Algorithm 10 in words to build intuition. The
array T, which is learnt on the data, represents the identiﬁed thresholds for the directions given
by q. Each pqb, Tbq pair corresponds to a hyperplane that cuts ∆L´1 into two subsets given by
tx P ∆L´1 : xT qb ă Tbu and tx P ∆L´1 : xT qb ě Tbu. The overall partitioning of ∆L´1 is
created by merging these cuts sequentially. This deﬁnes the binning function B. By construction,
the binning function is such that each bin contains at least t n`1"
END,0.8706140350877193,"B u ´ 1 many points in its interior. As
suggested by Gupta and Ramdas (2021), we do not include the points that lie on the boundary, that
is, points Xi that satisfy gpXiqT qs “ Ts for some s P rBs. The interior points bins are then used to
estimate the bin biases pΠ."
END,0.8717105263157895,"No matter how the q-vectors are chosen, the bins created by Algorithm 10 have at least
X n"
END,0.8728070175438597,"B
\
´1 points
for bias estimation. However, we discuss some simple heuristics for setting q that are semantically
meaningful. For some intuition, note that the binary version of histogram binning Gupta and Ramdas
(2021, Algorithm 1) is essentially recovered by Algorithm 10 if L “ 2 by setting each qb as e2 (the
vector r0, 1s). Equivalently, we can set each qb to ´e1 since both are equivalent for creating a
projection-based order on ∆2. Thus for L ě 3, a natural strategy for the q-vectors is to rotate
between the canonical basis vectors: q1 “ ´e1, q2 “ ´e2, . . . , qL “ ´eL, qL`1 “ ´e1, . . . ,
and so on. Projecting with respect to ´el focuses on the class l by forming a bin corresponding to
the largest values of glpXiq among the remaining Xi’s which have not yet been binned. (On the
other hand, projecting with respect to el will correspond to forming a bin with the smallest values
of glpXiq.)"
END,0.8739035087719298,"The q-vectors can also be set adaptively based on the training data (without seeing the calibration
data). For instance, if most points belong to a speciﬁc class l P rLs, we may want more sharpness
for this particular class. In that case, we can choose a higher ratio of the q-vectors to be ´el."
END,0.875,"G.4
EXPERIMENTS WITH THE COVTYPE DATASET"
END,0.8760964912280702,"In Figure 12 we illustrate the binning schemes proposed in this section on a 3-class version of the
COVTYPE-7 dataset considered in Section B.2. As noted before, this is an imbalanced dataset
where classes 1 and 2 dominate. We created a 3 class problem with the classes 1, 2, and other (as"
END,0.8771929824561403,Published as a conference paper at ICLR 2022
END,0.8782894736842105,(a) Calibration using Sierpinski binning at depth q “ 2.
END,0.8793859649122807,"(b) Calibration using grid-style binning with K “ 5, τ “ 0.2."
END,0.8804824561403509,"(c) Projection-based HB with B “ 27 projections: q1 “ ´e1, q2 “ ´e2, . . . , q4, ´e1, . . ., and so on."
END,0.881578947368421,(d) Projection-based HB with B “ 27 random projections (qi drawn uniformly from the ℓ2-unit-ball in R3).
END,0.8826754385964912,"Figure 12: Canonical calibration using ﬁxed and histogram binning on a 3-class version of the
COVTYPE-7 dataset. The reliability diagrams (left) indicate that all forms of binning improve the
calibration of the base logistic regression model. The recalibration diagrams (right) are a scatter plot
of the predictions gpXq on the test data with the points colored in 10 different colors depending on
their bin. For every bin, the arrow-tail indicates the mean probability predicted by the base model g
whereas the arrow-head indicates the probability predicted by the updated model h."
END,0.8837719298245614,"class 3). The entire dataset has 581012 points and the ratio of the classes is approximately 36%,
49%, 15% respectively. The dataset was split into train-test in the ratio 70:30. The training data
was further split into modeling-calibration in the ratio 90:10. A logistic regression model g using
sklearn.linear model.LogisticRegression was learnt on the modeling data, and g
was recalibrated on the calibration data."
END,0.8848684210526315,"The plots on the right in Figure 12 are recalibration diagrams. The base predictions gpXq on the
test-data are displayed as a scatter plot on ∆2. Points in different bins are colored using one of 10"
END,0.8859649122807017,Published as a conference paper at ICLR 2022
END,0.8870614035087719,"different colors (since the number of bins is larger than 10, some colors correspond to more than one
bin). For each bin, an arrow is drawn, where the tail of the arrow corresponds to the average gpXq
predictions in the bin and the head of the arrow corresponds to the recalibrated hpXq prediction for
the bin. For bins that contained very few points, the arrows are suppressed for visual clarity."
END,0.8881578947368421,"The plots on the left in Figure 12 are validity plots (Gupta and Ramdas, 2021). Validity plots display
estimates of
V pεq “ Ptest-data p}E rY | gpXqs ´ gpXq}1 ď εq ,
as ε varies (g corresponds to the validity plot for logistic regression; replacing g with h above gives
plots for the binning based classiﬁer h). For logistic regression, the same binning scheme as the one
provided by h is used to estimate V pεq."
END,0.8892543859649122,"Overall, Figure 12 shows that all of the binning approaches improve the calibration of the original
logistic regression model across different ε. However, the recalibration does not change the original
model signiﬁcantly. Comparing the different binning methods to each other, we ﬁnd that they all
perform quite similarly. It would be interesting to further study these and other binning methods for
post-hoc canonical calibration."
END,0.8903508771929824,"H
PROOFS"
END,0.8914473684210527,"Proofs appear in separate subsections, in the same order as the corresponding results appear in the
paper and Appendix. Proposition 4 was stated informally, so we state it formally as well."
END,0.8925438596491229,"H.1
STATEMENT AND PROOF OF PROPOSITION 4"
END,0.893640350877193,"Proposition 4. For any predictor pc, hq, conf-ECEpc, hq ď TL-ECEpc, hq."
END,0.8947368421052632,"Proof. To avoid confusion between the the conditioning operator and the absolute value operator |¨|,
we use abs p¨q to denote absolute values below. Note that,"
END,0.8958333333333334,"abs pPpY “ cpXq | hpXqq ´ hpXqq “ abs pE r1 tY “ cpXqu | hpXqs ´ hpXqq
“ abs pE r1 tY “ cpXqu ´ hpXq | hpXqsq
“ abs pE rE r1 tY “ cpXqu ´ hpXq | hpXq, cpXqs | hpXqsq
ď E rabs pE r1 tY “ cpXqu ´ hpXq | hpXq, cpXqsq | hpXqs
(by Jensen’s inequality)
“ E rabs pPpY “ cpXq | hpXq, cpXqq ´ hpXqq | hpXqs . Thus,"
END,0.8969298245614035,"conf-ECEpc, hq “ E rabs pPpY “ cpXq | hpXqq ´ hpXqqs
ď E rE rabs pPpY “ cpXq | hpXq, cpXqq ´ hpXqq | hpXqss
“ E rabs pPpY “ cpXq | hpXq, cpXqq ´ hpXqqs
“ TL-ECEpc, hq."
END,0.8980263157894737,"H.2
PROOF OF THEOREM 1"
END,0.8991228070175439,"The proof strategy is as follows. First, we use the bound of Gupta and Ramdas (2021, Theorem 4)
(henceforth called the GR21 bound), derived in the binary calibration setting, to conclude marginal,
conditional, and ECE guarantees for each hl. Then, we show that the binary guarantees for the
individual hl’s leads to a top-label guarantee for the overall predictor pc, hq."
END,0.9002192982456141,"Consider any l P rLs. Let Pl denote the conditional distribution of pX, 1 tY “ luq given cpXq “ l.
Clearly, Dl is a set of nl i.i.d. samples from Pl, and hl is learning a binary calibrator with respect
to Pl using binary histogram binning. The number of data-points is nl and the number of bins is
Bl “ tnl{ku bins. We now apply the GR21 bounds on hl. First, we verify that the condition they
require is satisﬁed:
nl ě k tnl{ku ě 2Bl."
END,0.9013157894736842,Published as a conference paper at ICLR 2022
END,0.9024122807017544,"Thus their marginal calibration bound for hl gives, P ˜"
END,0.9035087719298246,"|PpY “ l | cpXq “ l, hlpXqq ´ hlpXq| ď δ ` d"
END,0.9046052631578947,"logp2{αq
2ptnl{Blu ´ 1q | cpXq “ l ¸"
END,0.9057017543859649,ě 1 ´ α.
END,0.9067982456140351,"Note that since tnl{Blu “ tnl{ tnl{kuu ě k,"
END,0.9078947368421053,ε1 “ δ ` d
END,0.9089912280701754,"logp2{αq
2pk ´ 1q ě δ ` d"
END,0.9100877192982456,"logp2{αq
2ptnl{Blu ´ 1q."
END,0.9111842105263158,Thus we have
END,0.9122807017543859,"P p|PpY “ l | cpXq “ l, hlpXqq ´ hlpXq| ď ε1 | cpXq “ lq ě 1 ´ α."
END,0.9133771929824561,"This is satisﬁed for every l. Using the law of total probability gives us the top-label marginal cali-
bration guarantee for pc, hq:"
END,0.9144736842105263,"Pp|PpY “ cpXq | cpXq, hpXqq ´ hpXq| ď ε1q “ L
ÿ"
END,0.9155701754385965,"l“1
PpcpXq “ lqPp|PpY “ cpXq | cpXq, hpXqq ´ hpXq| ď ε1 | cpXq “ lq"
END,0.9166666666666666,"(law of total probability) “ L
ÿ"
END,0.9177631578947368,"l“1
PpcpXq “ lqPp|PpY “ l | cpXq “ l, hlpXqq ´ hlpXq| ď ε1 | cpXq “ lq"
END,0.918859649122807,"(by construction, if cpxq “ l, hpxq “ hlpxq) ě L
ÿ"
END,0.9199561403508771,"l“1
PpcpXq “ lqp1 ´ αq"
END,0.9210526315789473,“ 1 ´ α.
END,0.9221491228070176,"Similarly, the in-expectation ECE bound of GR21, for p “ 1, gives for every l,"
END,0.9232456140350878,"E |PpY “ l | cpXq “ l, hlpXqq ´ hlpXq | cpXq “ l| ď
a"
END,0.9243421052631579,"Bl{2nl ` δ “
a"
END,0.9254385964912281,"tnl{ku {2nl ` δ ď
a"
END,0.9265350877192983,"1{2k ` δ. Thus,"
END,0.9276315789473685,"E|PpY “ cpXq | cpXq, hlpXqq ´ hpXq| “ L
ÿ"
END,0.9287280701754386,"l“1
PpcpXq “ lqE|PpY “ l | cpXq “ l, hlpXqq ´ hlpXq| | cpXq “ l ď L
ÿ"
END,0.9298245614035088,"l“1
PpcpXq “ lqp
a"
END,0.930921052631579,"1{2k ` δq “
a"
END,0.9320175438596491,1{2k ` δ.
END,0.9331140350877193,"Next, we show the top-label conditional calibration bound. Let B “ řL
l“1 Bl and αl “ αBl{B.
Note that B ď řL
l“1 nl{k “ n{k. The binary conditional calibration bound of GR21 gives P ˜"
END,0.9342105263157895,"@r P Rangephlq, |PpY “ l | cpXq “ l, hlpXq “ rq ´ r| ď δ ` d"
END,0.9353070175438597,"logp2Bl{αlq
2ptnl{Blu ´ 1q | cpXq “ l ¸"
END,0.9364035087719298,ě 1 ´ αl.
END,0.9375,"Note that
d"
END,0.9385964912280702,"logp2Bl{αlq
2ptnl{Blu ´ 1q “ d"
END,0.9396929824561403,"logp2B{αq
2ptnl{Blu ´ 1q"
END,0.9407894736842105,Published as a conference paper at ICLR 2022 ď d
END,0.9418859649122807,"logp2n{kαq
2ptnl{Blu ´ 1q
(since B ď n{k) ď d"
END,0.9429824561403509,logp2n{kαq
END,0.944078947368421,"2pk ´ 1q
(since k ď tnl{Blu)."
END,0.9451754385964912,"Thus for every l P rLs,"
END,0.9462719298245614,"Pp@r P Rangephlq, |PpY “ l | cpXq “ l, hlpXq “ rq ´ r| ď ε2q ě 1 ´ αl."
END,0.9473684210526315,"By construction of h, conditioning on cpXq “ l and hlpXq “ r is the same as conditioning on
cpXq “ l and hpXq “ r. Taking a union bound over all L gives"
END,0.9484649122807017,"Pp@l P rLs, r P Rangephq, |PpY “ cpXq | cpXq “ l, hpXq “ rq ´ r|q ď ε2q ě 1 ´ L
ÿ"
END,0.9495614035087719,"l“1
αl “ 1 ´ α,"
END,0.9506578947368421,"proving the conditional calibration result. Finally, note that if for every l P rLs, r P Rangephq,"
END,0.9517543859649122,"|PpY “ cpXq | cpXq “ l, hpXq “ rq ´ r| ď ε2,"
END,0.9528508771929824,"then also
TL-ECEpc, hq “ E|PpY “ cpXq | hpXq, cpXqq ´ hpXq| ď ε2.
This proves the high-probability bound for the TL-ECE.
Remark 3. Gupta and Ramdas (2021) proved a more general result for general ℓp-ECE bounds.
Similar results can also be derived for the suitably deﬁned ℓp-TL-ECE. Additionally, it can be shown
that with probability 1 ´ α, the TL-MCE of pc, hq is bounded by ε2. (TL-MCE is deﬁned in Ap-
pendix E, equation (17).)"
END,0.9539473684210527,"H.3
PROOF OF PROPOSITION 1"
END,0.9550438596491229,"Consider a speciﬁc l P rLs. We use hl to denote the l-th component function of h and Yl “
1 tY “ lu. Canonical calibration implies"
END,0.956140350877193,"PpY “ l | hpXqq “ E rYl | hpXqs “ hlpXq.
(23)"
END,0.9572368421052632,We can then use the law of iterated expectations (or tower rule) to get the ﬁnal result:
END,0.9583333333333334,"E rYl | hlpXqs “ E rE rYl | hpXqs | hlpXqs
“ E rhlpXq | hlpXqs
(by the canonical calibration property (23))
“ hlpXq."
END,0.9594298245614035,"H.4
PROOF OF THEOREM 2"
END,0.9605263157894737,"We use the bounds of Gupta and Ramdas (2021, Theorem 4) (henceforth called the GR21 bounds),
derived in the binary calibration setting, to conclude marginal, conditional, and ECE guarantees for
each hl. This leads to the class-wise results as well."
END,0.9616228070175439,"Consider any l P rLs. Let Pl denote the distribution of pX, 1 tY “ luq. Clearly, Dl is a set of n i.i.d.
samples from Pl, and hl is learning a binary calibrator with respect to Pl using binary histogram
binning. The number of data-points is n and the number of bins is Bl “ tn{klu bins. We now apply
the GR21 bounds on hl. First, we verify that the condition they require is satisﬁed:"
END,0.9627192982456141,n ě kl tn{klu ě 2Bl.
END,0.9638157894736842,"Thus the GR21 marginal calibration bound gives that for every l P rLs, hl satisﬁes P ˜"
END,0.9649122807017544,|PpY “ l | hlpXqq ´ hlpXq| ď δ ` d
END,0.9660087719298246,"logp2{αlq
2ptn{Blu ´ 1q ¸"
END,0.9671052631578947,ě 1 ´ αl.
END,0.9682017543859649,Published as a conference paper at ICLR 2022
END,0.9692982456140351,"The class-wise marginal calibration bound of Theorem 2 follows since tn{Blu “ tn{ tn{kluu ě kl,
and so"
END,0.9703947368421053,"εp1q
l
ě δ ` d"
END,0.9714912280701754,"logp2{αlq
2ptn{Blu ´ 1q."
END,0.9725877192982456,"Next, the GR21 conditional calibration bound gives for every l P rLs, hl satisﬁes P ˜"
END,0.9736842105263158,"@r P Rangephlq, |PpY “ l | hlpXq “ rq ´ r| ď δ ` d"
END,0.9747807017543859,"logp2Bl{αlq
2ptn{Blu ´ 1q ¸"
END,0.9758771929824561,ě 1 ´ αl.
END,0.9769736842105263,"The class-wise marginal calibration bound of Theorem 2 follows since Bl “ tn{klu ď n{kl and
tn{Blu “ tn{ tn{kluu ě kl, and so"
END,0.9780701754385965,"εp2q
l
ě δ ` d"
END,0.9791666666666666,"logp2Bl{αlq
2ptn{Blu ´ 1q."
END,0.9802631578947368,"Let k “ minlPrLs kl. The in-expectation ECE bound of GR21, for p “ 1, gives for every l,"
END,0.981359649122807,"E rbinary-ECE-for-class-l phlqs ď
a"
END,0.9824561403508771,"Bl{2nl ` δ “
a"
END,0.9835526315789473,"tn{klu {2n ` δ ď
a"
END,0.9846491228070176,"1{2kl ` δ ď
a"
END,0.9857456140350878,"1{2k ` δ. Thus,"
END,0.9868421052631579,"E rCW-ECEpc, hqs “ E «"
END,0.9879385964912281,"L´1
L
ÿ"
END,0.9890350877192983,"l“1
binary-ECE-for-class-l phlq ﬀ"
END,0.9901315789473685,"ď L´1
L
ÿ"
END,0.9912280701754386,"l“1
p
a"
END,0.9923245614035088,"1{2k ` δq “
a"
END,0.993421052631579,"1{2k ` δ,"
END,0.9945175438596491,"as required for the in-expectation CW-ECE bound of Theorem 2. Finally, for the high probability
CW-ECE bound, let ε “ maxlPrLs εp2q
l
and α “ řL
l“1 αl. By taking a union bound over the the
conditional calibration bounds for each hl, we have, with probability 1 ´ α, for every l P rLs and
r P Rangephq,
|PpY “ l | cpXq “ l, hpXq “ rq ´ r| ď εp2q
l
ď ε."
END,0.9956140350877193,"Thus, with probability 1 ´ α,"
END,0.9967105263157895,"CW-ECEpc, hq “ L´1
L
ÿ"
END,0.9978070175438597,"l“1
E|PpY “ l | hlpXqq ´ hlpXq| ď ε."
END,0.9989035087719298,This proves the high-probability bound for the CW-ECE.
