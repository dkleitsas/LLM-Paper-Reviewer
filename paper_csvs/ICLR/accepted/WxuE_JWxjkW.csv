Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0026954177897574125,"Researchers are using deep learning models to explore the emergence of language
in various language games, where agents interact and develop an emergent language
to solve tasks. We focus on the factors that determine the expressivity of emergent
languages, which reﬂects the amount of information about input spaces those
languages are capable of encoding. We measure the expressivity of emergent
languages based on the generalisation performance across different games, and
demonstrate that the expressivity of emergent languages is a trade-off between
the complexity and unpredictability of the context those languages emerged from.
Another contribution of this work is the discovery of message type collapse, i.e.
the number of unique messages is less than that of inputs. We also show that using
the contrastive loss proposed by Chen et al. (2020) can alleviate this problem."
INTRODUCTION,0.005390835579514825,"1
INTRODUCTION"
INTRODUCTION,0.008086253369272238,"Language games were ﬁrst introduced by Wittgenstein (1954) to explore the meanings of language
utterances. Instantiating this concept with the signalling game design from Lewis (1969) enables
linguists to explore the emergence of linguistic structure (Kirby, 2001; Kirby & Hurford, 2002) where
artiﬁcial languages are represented as symbolic systems. The success of deep learning (DL) models
on complicated cognitive tasks (Krizhevsky et al., 2012; LeCun et al., 2015; Silver et al., 2016) then
inspired researchers to apply DL-based models to language games to investigate the agents’ ability to
invent communication protocols without preset rules (e.g. Lee et al., 2018; Lazaridou et al., 2018)."
INTRODUCTION,0.01078167115902965,"In the existing works (e.g. Lee et al., 2018; Lazaridou et al., 2018; Graesser et al., 2019; Zhang et al.,
2019; Yuan et al., 2020), there are usually two types of agents, speakers that emit messages based on
their observations (i.e. input target objects) and listeners that receive messages and act accordingly,
as illustrated in Figure 1. Based on the goals for listeners, we can categorise most of the games into
the following three types: 1) referential games in which listeners need to select the target object
observed by the speaker among a set of candidate objects (candidates for short) (e.g. Lazaridou et al.,
2017; Ren et al., 2020); 2) reconstruction games in which listeners need to reconstruct the speaker’s
observation (e.g. Chaabouni et al., 2020; Kharitonov et al., 2020); and 3) navigation games in which
listeners need to go to the location speciﬁed by speakers (e.g. Lowe et al., 2017; Kaji´c et al., 2020).
We focus on referential games, illustrated in Figure 1a, which have been well investigated in both
the linguistic community (e.g. Kirby et al., 2014; Winters et al., 2018) and emergent communication
community (e.g. Lazaridou et al., 2018; Li & Bowling, 2019)."
INTRODUCTION,0.013477088948787063,"It is reasonable to assume that listeners in referential games can differentiate the target object from the
other distractors in the context as long as the speaker’s message encodes some unique feature of the
target. Therefore, the speaker’s messages must convey a different amount of information for listeners
to complete their task in games where candidates are similar in different degrees. For example,
to distinguish two very similar candidates, e.g. blue wide-stripped shirt and blue narrow-stripped
shirt, it requires more information about the target to be encoded by the speaker’s message than to
distinguish a shirt from a cup. Experiments with human participants show that emergent languages
are sensitive to the requirements of the communicative tasks for which they are used, with languages
developing in which only the necessary discriminatory information is encoded (Winters et al., 2018)."
INTRODUCTION,0.016172506738544475,"˚Correspondence author, e-mail address: s.guo@ed.ac.uk"
INTRODUCTION,0.018867924528301886,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0215633423180593,"Speaker
Listener"
INTRODUCTION,0.02425876010781671,Message
INTRODUCTION,0.026954177897574125,"(a) Illustration of the referential game we investi-
gate in this work. Observations are concatenations
of two 5-digits-long one-hot vectors. The objective
of the listener is to select among a set of candidate
inputs (the context)."
INTRODUCTION,0.029649595687331536,Speaker's Encoder
INTRODUCTION,0.03234501347708895,"(MLP)
Speaker's Message Generator (LSTM)"
INTRODUCTION,0.03504043126684636,Listener's Message Encoder (LSTM)
INTRODUCTION,0.03773584905660377,Speaker Modules
INTRODUCTION,0.04043126684636118,Listener Modules
INTRODUCTION,0.0431266846361186,"Listener's
Candidate"
INTRODUCTION,0.04582210242587601,Encoder (MLP) Input
INTRODUCTION,0.04851752021563342,target
INTRODUCTION,0.05121293800539083,distractor
INTRODUCTION,0.05390835579514825,Listener's Action Module (Softmax)
INTRODUCTION,0.05660377358490566,Message
INTRODUCTION,0.05929919137466307,output distribution
INTRODUCTION,0.06199460916442048,Candidate set
INTRODUCTION,0.0646900269541779,(context)
INTRODUCTION,0.0673854447439353,"(b) Illustration of the agent architecture we implement
for referential games. Blue modules are components of
the speaker, and green modules belong to the listener.
Orange and grey indicate the vectors generated by agents
and input samples, respectively."
INTRODUCTION,0.07008086253369272,Figure 1: Diagram of referential game and the architecture of our agents.
INTRODUCTION,0.07277628032345014,"Following Kirby et al. (2015), we refer to such ability to encode information about input space (e.g.
structure, dimensionality, distribution of samples) as expressivity. In this work, we explore the
factors that inﬂuence expressivity in the framework of DL-based referential games, and we argue that
it is important to understand these factors such that agents could develop languages that are effective
enough for completing various tasks."
INTRODUCTION,0.07547169811320754,"Our contribution in this work is threefold. First, we propose and verify a hypothesis about the
determining factors of expressivity of emergent languages under the DL-based framework. Following
Winters et al. (2018), we show that context size (i.e. the number of candidates the listener must select
among when interpreting the speaker’s message) inﬂuence both the complexity and unpredictability
of that context. Complexity refers to the similarity between the items in the context: in order for
the listener to select the target, more information needs to be encoded in the speaker’s signal if the
context contains more similar distractors for a given target. Unpredictability refers to the extent to
which the information that needs to be encoded by the speaker is stable across contexts in successive
episodes of communication: in games where the information required by the listener differs from
trial (training epoch for DL agents) to trial, the speaker needs to encode more information on every
trial in order to be sure to encode the information the listener needs.1 As we show in Section 3.1,
complexity and unpredictability are affected differently by the context size: as context size increases,
complexity increases but unpredictability decreases. Therefore, we propose and verify the following
hypothesis about the determining factors of expressivity:"
INTRODUCTION,0.07816711590296496,"[HYPOTHESIS] The expressivity of emergent languages is a trade-off between the complexity
and the unpredictability of context in language games.
[1]"
INTRODUCTION,0.08086253369272237,"Our second contribution is a novel measure of expressivity based on partial ordering of languages
in terms of their generalisation performances across tasks. Although expressivity is related to the
amount of information encoded in a language, we illustrate that mutual information (MI) is not an
appropriate measurement for expressivity in Section 3.2. Considering that one of our objectives is to
facilitate a universally useful emergent language, we propose to measure expressivity of a language
based on the generalisation performance of listening agents trained with that language across different
games. Since it is more challenging to quantitatively and directly measure expressivity, we focus on
the partial ordering between languages in this ﬁrst step towards understanding expressivity."
INTRODUCTION,0.08355795148247978,"Our ﬁnal contribution is the discovery of message type collapse, i.e. the number of unique messages
is signiﬁcantly lower than the size of input space in relatively easy games, which can lead to highly
ambiguous emergent languages. To overcome the technical limitations imposed by GPU memory
size on large-scale referential games, we introduce the contrastive loss proposed by Chen et al. (2020)
in referential games. While comparing the behaviour of the contrastive loss and the loss function
used in previous works (e.g Havrylov & Titov, 2017; Li & Bowling, 2019; Ren et al., 2020; Guo
et al., 2020), we ﬁnd that this contrastive loss can greatly alleviate the collapse of message types,
leading to more disambiguous and potentially more expressive emergent languages."
INTRODUCTION,0.0862533692722372,"1Context, complexity, and unpredictability are deﬁned and discussed in depth in Section 3."
INTRODUCTION,0.0889487870619946,Published as a conference paper at ICLR 2022
INTRODUCTION,0.09164420485175202,"2
GAMES, AGENTS AND LEARNING METHODS"
GAME ENVIRONMENT AND ARCHITECTURE OF AGENTS,0.09433962264150944,"2.1
GAME ENVIRONMENT AND ARCHITECTURE OF AGENTS"
GAME ENVIRONMENT AND ARCHITECTURE OF AGENTS,0.09703504043126684,"Environment Components As shown in Figure 1b, formally speaking, the environment in our
referential games consists of: 1) input space X from which the speaker’s observations are drawn,
which consists of 10, 000 possible inputs2 where each x P X is a concatenation of 4 one-hot
vectors whose length is 10; 2) message space M consists of 106 6-tokens-long sequences m (called
as a message type) where the size of the token inventory is 10; note that the tokens are initially
meaningless, and meaning emerges through the negotiation of a communication protocol between
speakers and listeners; 3) candidate set C which contains a target input xt and |C| ´ 1 distractors
txd1, xd2, . . . , xd|C|´1u sampled uniformly at random from X without replacement."
GAME ENVIRONMENT AND ARCHITECTURE OF AGENTS,0.09973045822102426,"Agents The architecture of the agents we implemented is also illustrated in Figure 1b: 4) speaker S
consists of both a multi layer perceptron (MLP) for encoding the 40-digits-long target input xt onto
a 256-digits-long embedding hS, and a decoder for generating a message m based on long-short
term memory (LSTM) network (Hochreiter & Schmidhuber, 1997). 5) listener L is constituted by an
LSTM-based sequence encoder which encodes m into a 256-digits-long embedding hLpmq, and an
MLP for encoding the inputs which can be denoted by f L
cencp¨q."
GAME ENVIRONMENT AND ARCHITECTURE OF AGENTS,0.10242587601078167,"Deﬁnition of an emergent language Following e.g. Ren et al. (2020), we also deﬁne an emergent
language L as a mapping function from the input space X to the message space M, i.e. L : X ÞÑ M,
thus it can be represented as tpxi, Lpxiqq|@xi P Xu. Note that L could be a non-injective mapping
function, i.e. it is possible for two different inputs to be mapped to an identical message thus the
number of message types might be less than the number of unique inputs."
LEARNING METHODS OF AGENTS,0.10512129380053908,"2.2
LEARNING METHODS OF AGENTS"
LEARNING METHODS OF AGENTS,0.1078167115902965,"In the previous works (e.g. Havrylov & Titov, 2017), the action module of L is usually a softmax
function which produces a categorical distribution over the candidates based on the distance between
hLpmq and embeddings of each candidate (f L
cencpxiq) obtained through listener’s candidate encoder
f L
cencp¨q. By comparing the loss function used in Havrylov & Titov (2017) and the contrastive loss
function proposed by Hadsell et al. (2006), we ﬁnd that the aim of both functions is the same, i.e.
to make the distance between positive pairs (xt and m) closer while keeping the negative pairs
(xd and m) as far apart as possible. To effectively validate our hypothesis, we need to make the
whole input space X as candidate set C in which case the batch is too large to be stored in GPU
memory (since the space complexity is Op|X|2q). Therefore, to overcome this issue, we adapt the
contrastive learning method proposed by Chen et al. (2020) to referential games. Suppose that we
have a batch of input samples B “ tx1, x2, . . . , xi, . . . , x|B|u and the corresponding messages from
the speaker are tmB1, mB2, . . . , mBi, . . . , mB|B|u (note that it is possible that mBi “ mBj for
different i, j) which would be further encoded into thL
mB1, hL
mB2, . . . , hL
mBi, . . . , hL
mB|B|u by the
listener L. Then the loss function for a sample xi is the deﬁned on a softmax function:"
LEARNING METHODS OF AGENTS,0.1105121293800539,"ℓi “ ´ log
exp
´
hL
mi
J ¨ f L
cencpxiq
¯"
LEARNING METHODS OF AGENTS,0.11320754716981132,"ř|B|
j“1 exp
´
hLmi
J ¨ f L
cencpxjq
¯.
(1)"
LEARNING METHODS OF AGENTS,0.11590296495956873,"In this way, the number of candidates |C| is exactly the batch size |B| during training. More details
about the comparison between referential loss and this contrastive loss can be found in Appendix A."
LEARNING METHODS OF AGENTS,0.11859838274932614,"As for updating the parameters, we use the Adam algorithm introduced by Kingma & Ba (2015), and
the learning rate is set to 10´4. To allow the gradients being propagated through the discrete channel
to overcome the sampling issue of messages, we apply the Gumbel-Softmax trick proposed by Jang
et al. (2020), and the temperature hyper-parameter τ is set to 1.0."
LEARNING METHODS OF AGENTS,0.12129380053908356,"Our implementation3 of games is based on the framework EGG developed by Kharitonov et al. (2019).
The experiments across 6 random seeds, 18 source games, 11 target games took 4, 216 hours in total,
on Nvidia Tesla P100. Deﬁnition of “source/target games” are given in the following Section 4.2."
LEARNING METHODS OF AGENTS,0.12398921832884097,"2According to the results from Chaabouni et al. (2020), an input space containing 10, 000 possible inputs is
large enough for neural models to achieve ą 99% generalisation accuracy.
3Codes are released at https://github.com/uoe-agents/Expressivity-of-Emergent-Languages."
LEARNING METHODS OF AGENTS,0.12668463611859837,Published as a conference paper at ICLR 2022
CONTEXT AND EXPRESSIVITY,0.1293800539083558,"3
CONTEXT AND EXPRESSIVITY"
CONTEXT AND EXPRESSIVITY,0.1320754716981132,"3.1
CONTEXT, AND ITS COMPLEXITY AND UNPREDICTABILITY IN DIFFERENT GAMES"
CONTEXT AND EXPRESSIVITY,0.1347708894878706,"Context Communication between humans always takes place w.r.t. some context (Winters et al.,
2018). For humans, that context includes: 1) features of the environment that can be directly perceived
by the dialogue participants and used for interpreting an utterance (e.g. “cup” is interpreted as an
object in the immediate physical context); and 2) information that cannot be directly perceived but
can affect the interpretation of utterances (e.g. using and interpreting “cup” assumes some shared
knowledge of what counts as a “cup”; once a particular cup has been established as salient in particular
dialogue, further utterances of “the cup” or “it” will be interpreted as referring to that speciﬁc cup)."
CONTEXT AND EXPRESSIVITY,0.13746630727762804,"Inspired by this conclusion from pragmatics, in the DL-based language games, we deﬁne context as
the supervision information involved in the calculation of losses, i.e. the context of a speciﬁc target
xt, denoted as Cpxtq, is the space of samples involved in the calculation of loss. As for in referential
games, the cross entropy loss is calculated based on the distance between the message embedding
hLpmBiq and candidate embedding f L
cencpxiq where xi P C, thus Cpxtq “ C Ď X in referential
games."
CONTEXT AND EXPRESSIVITY,0.14016172506738545,"Complexity of context We assume that the goal of communication is to distinguish a target object
from other possibilities in the context (as deﬁned above). It therefore follows that the similarity of
distractors in the context to the target inﬂuences the communicative precision required, and that
greater precision is required to distinguish the target from a more similar distractor. For example, it is
relatively easy in natural language to distinguish e.g. a cat from a table (a relatively general label like
""cat"" or ""animal"" would sufﬁce), but harder to make ﬁne-grained distinctions between very similar
objects e.g. a Scottish Fold Cat and an American Shorthair Cat (a specialist vocabulary or a lengthy
description is necessary)."
CONTEXT AND EXPRESSIVITY,0.14285714285714285,"Following the assumption (described verbally above) that a context which contains more similar
objects makes the game harder because there are fewer unique features that sufﬁce to distinguish
the target from the distractors, we ﬁrst deﬁne a neighbour set in k-th degree of a given target xt
as Nkpxtq “ txj : dpxt, xjq ď ku where d is a distance function that can properly capture the
similarity between inputs, e.g Hamming distance in our setting. The complexity of Cpxtq is then
deﬁned as the expectation of the probability that Cpxtq contains an object from Nkpxtq, i.e."
CONTEXT AND EXPRESSIVITY,0.14555256064690028,"Ext rg pCpxtq, Nkpxtqqs ,where g pCpxtq, Nkpxtqq “
""1,
if Dxi P Cpxtq s.t. xi P Nkpxtq
0,
otherwise
(2)"
CONTEXT AND EXPRESSIVITY,0.14824797843665768,"In our referential games, since the sampling procedure is independent Bernoulli without replacement,"
CONTEXT AND EXPRESSIVITY,0.1509433962264151,"the value of the above expectation is then 1´
´
|X|´1´|Nkpxtq|"
CONTEXT AND EXPRESSIVITY,0.15363881401617252,"|X|´1
¯|C|
which is a monotonically increasing
function w.r.t |C| and a ﬁxed k. That said, larger contexts are more complex since they are more
likely to include items which are very similar to the target."
CONTEXT AND EXPRESSIVITY,0.15633423180592992,"Unpredictability of context Our notion of unpredictability comes from experimental work with
human participants, e.g. Winters et al. (2018). Suppose the aim is to distinguish a striped t-shirt
from a number of distractors, and there are two sequences of context: 1) three runs of games where
distractors are all cups; 2) three runs of games where distractors are a cup, a plain t-shirt, and a
pencil. In the ﬁrst sequence of games, participants would become certain that “t-shirt” is enough
for distinguishing the target, whereas in the second sequence participants would learn that a more
overspeciﬁed utterance (e.g. ""striped t-shirt"") is necessary to guarantee comprehension after a failure
on the trial involving the plain t-shirt distractor. That is, the context in the ﬁrst sequence is more
predictable than the second. Winters et al. (2018) show that human participants are sensitive to this
kind of unpredictability, and adapt their communicative strategies accordingly."
CONTEXT AND EXPRESSIVITY,0.15902964959568733,"In DL-based games, we refer to the context at the e-th trial, i.e. the e-th training epoch, of a target
xt as Cepxtq. Following the above example, the unpredictability of context is then deﬁned as the"
CONTEXT AND EXPRESSIVITY,0.16172506738544473,Published as a conference paper at ICLR 2022
CONTEXT AND EXPRESSIVITY,0.16442048517520216,"proportion of Ce`1pxtq that are not from Cepxtq, i.e. Ext »"
CONTEXT AND EXPRESSIVITY,0.16711590296495957,"–
1
|Ce`1pxtq| ÿ"
CONTEXT AND EXPRESSIVITY,0.16981132075471697,"xjPCe`1pxtq
I pxj R Cepxtqq ﬁ ﬂ
(3)"
CONTEXT AND EXPRESSIVITY,0.1725067385444744,"In our referential games, since the sampling procedure is independent Bernoulli without replacement,
the proportion of objects not from Cepxtq in Ce`1pxtq is then simply 1 ´ |Cepxtq|"
CONTEXT AND EXPRESSIVITY,0.1752021563342318,"|X|
“ 1 ´ |C|"
CONTEXT AND EXPRESSIVITY,0.1778975741239892,"|X| (i.e.
smaller contexts are more unpredictable, since contexts on successive trials are more likely to differ
in their composition)."
CONTEXT AND EXPRESSIVITY,0.18059299191374664,More details about the above discussion are provided in Appendix B.
MEASURING EXPRESSIVITY IN LANGUAGE GAMES,0.18328840970350405,"3.2
MEASURING EXPRESSIVITY IN LANGUAGE GAMES"
MEASURING EXPRESSIVITY IN LANGUAGE GAMES,0.18598382749326145,"Following the intuition that the different types of games require messages to convey different amount
of information about input space X, MI may seem to be a good candidate measure for expressivity.
However, we found that even for two emergent languages that are both deterministic and bijective
mapping functions, i.e. they have the same MI value logp|X|q, their expressivity can still be different.
Further discussion about MI can be found in Appendix D."
MEASURING EXPRESSIVITY IN LANGUAGE GAMES,0.18867924528301888,"Since it is more challenging to directly quantify the expressivity of emergent languages, we instead
focus on the partial orders between them in this work. Considering that the purpose of the emergent
communication is to complete tasks, we therefore propose to measure the partial order between
expressivity based on game performance. This leads us to a common practice in the transfer learning
community (e.g. Pan & Yang, 2009), i.e. train models on source domain but evaluate them on different
target domains. Inspired by this practice, here we propose a deﬁnition of the partial order between
the expressivity of two emergent languages LA and LB, i.e. ELA and ELB:"
MEASURING EXPRESSIVITY IN LANGUAGE GAMES,0.19137466307277629,"Deﬁnition 3.1 (EG
LA ą EG
LB). Let g be an instance of language game, and G be a set of language
game instances. Then, given G, if the generalisation performance of LA is better than LB on some
language games G1 with statistically signiﬁcant difference, i.e. @g P G1, Pg
LA ą Pg
LB, where
∅Ă G1 Ď G and P denotes the generalisation performance, while there is no statistically signiﬁcant
difference between the converged generalisation performance of LA and LB on the remaining games,
i.e. Pg1"
MEASURING EXPRESSIVITY IN LANGUAGE GAMES,0.1940700808625337,LA « Pg1
MEASURING EXPRESSIVITY IN LANGUAGE GAMES,0.1967654986522911,"LB@g1 P GzG1 4, we then say that expressivity of LA is higher than LB on G, i.e.
EG
LA ą EG
LB. The metric of generalisation performance could be varied according to different types
of games."
MEASURING EXPRESSIVITY IN LANGUAGE GAMES,0.19946091644204852,"Brieﬂy speaking, given a set of games, if one language performs better than the other on some games
while they perform approximately the same on the remaining games, we then say the expressivity of
that language is higher than the other. Following this deﬁnition, there are several possible relationships
between the expressivity of two languages LA and LB: 1) EG
LA ą EG
LB, deﬁned as above; 2)
EG
LA “ EG
LB, if Pg
LA « Pg
LB, @g P G; 3) EG
LB ą EG
LA, deﬁned as above; 4) EG
LA is incomparable"
MEASURING EXPRESSIVITY IN LANGUAGE GAMES,0.20215633423180593,"to EG
LB (ELA£ELA), if propositions Dg1 P G, s.t. Pg1"
MEASURING EXPRESSIVITY IN LANGUAGE GAMES,0.20485175202156333,LA ą Pg1
MEASURING EXPRESSIVITY IN LANGUAGE GAMES,0.20754716981132076,"LB and Dg2 P G, s.t. Pg2"
MEASURING EXPRESSIVITY IN LANGUAGE GAMES,0.21024258760107817,LB ą Pg2
MEASURING EXPRESSIVITY IN LANGUAGE GAMES,0.21293800539083557,"LA are
both true."
PREDICTIONS AND EXPERIMENT DESIGNS,0.215633423180593,"4
PREDICTIONS AND EXPERIMENT DESIGNS"
PREDICTIONS FROM OUR HYPOTHESIS,0.2183288409703504,"4.1
PREDICTIONS FROM OUR HYPOTHESIS"
PREDICTIONS FROM OUR HYPOTHESIS,0.2210242587601078,"As we illustrated in Section 3.1, higher complexity indicates more similar distractors to targets,
thus requires messages conveying more ﬁne-grained features of inputs. This leads to the following
prediction:"
HIGHER COMPLEXITY IN GAMES WITH SAME CONTEXTUAL UNPREDICTABILITY CAN FACILITATE EMERGENT,0.22371967654986524,"1.
higher complexity in games with same contextual unpredictability can facilitate emergent
languages having higher expressivity;"
HIGHER COMPLEXITY IN GAMES WITH SAME CONTEXTUAL UNPREDICTABILITY CAN FACILITATE EMERGENT,0.22641509433962265,"4Note that « in this work means that there is no statistically signiﬁcant difference between means of two
sequences."
HIGHER COMPLEXITY IN GAMES WITH SAME CONTEXTUAL UNPREDICTABILITY CAN FACILITATE EMERGENT,0.22911051212938005,Published as a conference paper at ICLR 2022
HIGHER COMPLEXITY IN GAMES WITH SAME CONTEXTUAL UNPREDICTABILITY CAN FACILITATE EMERGENT,0.23180592991913745,"Meanwhile, following the conclusion from Winters et al. (2018) that higher contextual unpredictability
leads to higher signal autonomy, i.e. more comprehensive information about input space is encoded
in the emergent languages, we then make the following prediction:"
HIGHER UNPREDICTABILITY IN GAMES HAVING SAME CONTEXTUAL COMPLEXITY CAN FACILITATE EMERGENT,0.23450134770889489,"2.
higher unpredictability in games having same contextual complexity can facilitate emergent
languages having higher expressivity;
Following the above two predictions, both higher complexity and higher unpredictability can unilater-
ally lead to better expressivity. However, with an input space whose size is ﬁnite, changing context
size always affects the two factors in an inverse way, which leads to our Hypothesis 1 and gives the
following prediction:"
HIGHER UNPREDICTABILITY IN GAMES HAVING SAME CONTEXTUAL COMPLEXITY CAN FACILITATE EMERGENT,0.2371967654986523,"3.
if the size of X is ﬁxed across games, a language which emerges in a game with moderate
complexity and unpredictability should have the highest expressivity."
LANGUAGE TRANSFER EXPERIMENT,0.2398921832884097,"4.2
LANGUAGE TRANSFER EXPERIMENT"
LANGUAGE TRANSFER EXPERIMENT,0.24258760107816713,"To verify the above three predictions and thus our hypothesis, we evaluate the generalisation perfor-
mance of languages emerging in various games following the Procedure 1. Brieﬂy speaking, we ﬁrst
train a pair of speaker and listener to complete a referential game, i.e. a “source game” gs, and obtain
an emergent language. We then train a new listener in another referential game, i.e. a “target game”
gt, with only 90% of the input-message pairs, and use the game performance of this listener with the
remaining 10% pairs as the generalisation performance of gs on gt."
LANGUAGE TRANSFER EXPERIMENT,0.24528301886792453,"Procedure 1: Procedure for the language emergence and transfer experiment
Input: A set of source game Gs, a set of target game Gt
for every game gi
s in Gs do
1. initialise a new speaker and listener for gi
s, and train them to play gi
s with the whole X;
2. after the agents converge on gi
s, record L “ tpx, mq|x P Xu;
3. randomly shufﬂe and split L into 2 disjoint sets Ltrain and Ltest s.t. |Ltrain| “ 90% ¨ |L|;
4. for every game gj
t in Gt do
1. initialise a new listener for gj
t ;
2. train the listener with Ltrain to complete gj
t ;
3. record the accuracy of listener on Ltest as the generalisation performance of gi
s on
gj
t ;
end
end"
LANGUAGE TRANSFER EXPERIMENT,0.24797843665768193,"Since our hypothesis concerns the size of C, we propose to experiment on a series of referential games
with varying candidate set size |C| (“referX” where X“ |C|). Meanwhile, as these referential games
have both different complexity and unpredictability, we also implement a series of referential games
with different |C| but the batches are ﬁxed during training such that there is no unpredictability of
the context (“referXf” where X“ |C|). To sum up, in our experiment, Gs “ t“refer2/f”, “refer10/f”,
“refer100/f”, “refer1000/f”, “refer2500/f”, “refer5000/f”, “refer7500/f”, “refer10000”u5. Since lan-
guage games with ﬁxed context are not good simulation of human communication , we keep only the
ones with varying batches from Gs in Gt."
LANGUAGE TRANSFER EXPERIMENT,0.25067385444743934,"As a naming convention to allow us to refer easily to the factors that inﬂuence a language’s expressivity,
we refer to emergent languages by the parameters of their source game, e.g. ‘refer10’6 represents
a language which emerges in referential games whose |C| “ 10 and context varies across epochs.
More details about the naming of games and languages can be found in Appendix E.1."
RESULTS & DISCUSSION,0.25336927223719674,"5
RESULTS & DISCUSSION"
RESULTS & DISCUSSION,0.2560646900269542,"To alleviate the effect from randomness as much as possible, all the following results are obtained
with 6 different random seeds. All values are averaged across the seeds, and the corresponding
standard deviation (SD) are also provided (either in this section or appendices)."
RESULTS & DISCUSSION,0.2587601078167116,"5There is no “refer10000f” since no unpredictability is possible when |B| “ |C| “ 10, 000 “ |X|
6“referX” is a game while ‘referX’ is an emergent language from it."
RESULTS & DISCUSSION,0.261455525606469,Published as a conference paper at ICLR 2022
RESULTS & DISCUSSION,0.2641509433962264,"5.1
HIGHER COMPLEXITY ` SAME UNPREDICTABILITY Ñ HIGHER EXPRESSIVITY"
RESULTS & DISCUSSION,0.2668463611859838,"To verify the Prediction 1, we compare the generalisation performance of languages from ﬁxed
context, i.e. the unpredictability of source games’ context are the same, on Gt, and the results
are shown in Figure 2. Languages which emerge in source games whose contexts have higher
complexity consistently perform better on all games than those which emerge in source games with
lower-complexity contexts, since all lines keep growing with the increase of values on x-axis (i.e.
complexity). That is, the generalisation performance is a monotonically increasing function of |C| if
there is no unpredictability in the context, which matches with the prediction of our hypothesis."
RESULTS & DISCUSSION,0.2695417789757412,"refer2f
refer10f
refer100f
refer1000f
refer2500f
refer5000f
refer10000
Source Games 0.70 0.75 0.80 0.85 0.90 0.95 1.00"
RESULTS & DISCUSSION,0.2722371967654987,Accuracy
RESULTS & DISCUSSION,0.2749326145552561,Target Games
RESULTS & DISCUSSION,0.2776280323450135,"refer2
refer10
refer100
refer1000
refer2500
refer5000
refer7500
refer10000"
RESULTS & DISCUSSION,0.2803234501347709,"Figure 2: Generalisation performance of languages from ﬁxed context with varying complexity. The
x-axis indicates the source games, and y is the accuracy. Lines in different colours represent the
generalisation performance on different target games. We plot only the means of the converged
performance to keep the ﬁgure readable (SD values are provided in Table 3 in Appendix E.2), and
remove ‘refer7500f’ from the x-axis since it is a mix of |C| “ 7, 500 and |C| “ 2, 500."
RESULTS & DISCUSSION,0.2830188679245283,"5.2
HIGHER UNPREDICTABILITY ` SAME COMPLEXITY Ñ HIGHER EXPRESSIVITY"
RESULTS & DISCUSSION,0.2857142857142857,"To verify the Prediction 2, we compare the difference between the generalisation performance of
language with varying context (‘referX’) and their corresponding ﬁxed-context twins (‘referXf’).
As indicated by that all points in Figure 3 are positive, languages from context with varying objects
always perform better on all target games. Since all “referXf” has lower unpredictability of context
than the corresponding “referX” games, this indicates that higher unpredictability can unilaterally
lead to better generalisation performance of emergent language across various target games, which
matches with the prediction of our hypothesis."
RESULTS & DISCUSSION,0.2884097035040431,"refer2
refer10
refer100
refer1000
refer2500
refer5000
refer7500
Source Games 0.1 0.0 0.1 0.2 0.3 0.4"
RESULTS & DISCUSSION,0.29110512129380056,Accuracy Difference
RESULTS & DISCUSSION,0.29380053908355797,"Figure 3: Generalisation performance gaps between languages from varying context and their
corresponding ﬁxed-context twins on all target games. The x-axis indicates the types of source
games, and the y-axis indicates the distribution of difference between the generalisation performance
of ‘referX’ and ‘referXf’ on all target games (i.e. ‘referX’ - ‘referXf’). Each point represent the
difference on a speciﬁc kind of target game, and the boxes show the corresponding values of medians,
25th-percentile, and 75-th percentile. “refer10000” is not plotted since it doesn’t have a ﬁxed-context
twin game."
RESULTS & DISCUSSION,0.29649595687331537,Published as a conference paper at ICLR 2022
"EXPRESSIVITY IS A TRADE-OFF BETWEEN CONTEXTUAL COMPLEXITY AND
UNPREDICTABILITY",0.2991913746630728,"5.3
EXPRESSIVITY IS A TRADE-OFF BETWEEN CONTEXTUAL COMPLEXITY AND
UNPREDICTABILITY"
"EXPRESSIVITY IS A TRADE-OFF BETWEEN CONTEXTUAL COMPLEXITY AND
UNPREDICTABILITY",0.3018867924528302,"To verify the Prediction 3, we compare performance for all source games with varying context on all
target games, and the results are shown in Figure 4 (see also Appendix E.3 for more details)."
"EXPRESSIVITY IS A TRADE-OFF BETWEEN CONTEXTUAL COMPLEXITY AND
UNPREDICTABILITY",0.3045822102425876,"refer2
refer10
refer100
refer1000
refer2500
refer5000
refer7500
refer10000
Source Games 0.70 0.75 0.80 0.85 0.90 0.95 1.00"
"EXPRESSIVITY IS A TRADE-OFF BETWEEN CONTEXTUAL COMPLEXITY AND
UNPREDICTABILITY",0.30727762803234504,Accuracy
"EXPRESSIVITY IS A TRADE-OFF BETWEEN CONTEXTUAL COMPLEXITY AND
UNPREDICTABILITY",0.30997304582210244,Target Games
"EXPRESSIVITY IS A TRADE-OFF BETWEEN CONTEXTUAL COMPLEXITY AND
UNPREDICTABILITY",0.31266846361185985,"refer2
refer10
refer100
refer1000
refer2500
refer5000
refer7500
refer10000"
"EXPRESSIVITY IS A TRADE-OFF BETWEEN CONTEXTUAL COMPLEXITY AND
UNPREDICTABILITY",0.31536388140161725,"Figure 4: Generalisation performance of language from different source games on all target games.
Each line represents a speciﬁc kind of target game. The x-axis indicates the type of source games,
and the y-axis is the accuracy on target games. To keep the ﬁgure readable, we remove the error bars,
and instead provide the corresponding SD values in Table 4. We limit the range of y-axis to r0.7, 1.0s
to emphasise the difference between languages on complex games, refer to Figure 8 for more values."
"EXPRESSIVITY IS A TRADE-OFF BETWEEN CONTEXTUAL COMPLEXITY AND
UNPREDICTABILITY",0.31805929919137466,"As shown in Figure 4, on whichever type of game, the peaks of all curves are always among
“refer1000”, “refer2500”, or “refer5000”, which means that the best expressivity always emerges
in these 3 types of games. Considering that the complexity of context in referential game is a
monotonically increasing function of the |C|"
"EXPRESSIVITY IS A TRADE-OFF BETWEEN CONTEXTUAL COMPLEXITY AND
UNPREDICTABILITY",0.32075471698113206,"|X| while the contextual unpredictability is a monotonically
decreasing of it, the results shown in Figure 4 support our hypothesis, i.e. the expressivity is indeed a
trade-off between the contextual complexity and unpredictability."
"EXPRESSIVITY IS A TRADE-OFF BETWEEN CONTEXTUAL COMPLEXITY AND
UNPREDICTABILITY",0.32345013477088946,"To be speciﬁc, although context in “refer2”, “refer10”, and “refer100” have high unpredictability,
their complexity is low thus the languages have lower expressivity. On the other hand, context
of “refer7500” and “refer10000” have high complexity, but expressivity of neither language is the
highest due to the high predictability of the context in these games. This supports our argument that
unpredictability of context incentivises the agents to encode more information into the messages
such that the messages could overcome the uncertainty of contexts and be interpreted in various
contexts. Therefore, the three languages (‘refer1000’, ‘refer2500’, and ‘refer5000’) from context
with both moderate complexity and unpredictability perform better than the others. To make it
more straightforward to compare the expressivity between these languages, we also provide another
view of the above results in Figure 9 in Appendix E.3, which shows that ‘refer1000’£‘refer2500’ «
‘refer5000’ ą ‘refer7500’ « ‘refer10000’ ą ‘refer100’ ą ‘refer10’ ą ‘refer2’."
"EXPRESSIVITY IS A TRADE-OFF BETWEEN CONTEXTUAL COMPLEXITY AND
UNPREDICTABILITY",0.3261455525606469,"To eliminate the possibility that the above phenomena are caused by the hyperparameters we used
in our games, we also run the language transfer experiment with varying communication channel
capacity and agent capacity (i.e. the size of hidden layers). As shown in the ﬁgures in Appendix F,
both ‘refer1000’ and ‘refer2500’ perform better than ‘refer10000’ across varying hyperparameters,
which shows that the trade-off phenomenon is consistent over game settings and agent capacity."
THE COLLAPSE OF MESSAGE TYPES,0.3288409703504043,"5.4
THE COLLAPSE OF MESSAGE TYPES"
THE COLLAPSE OF MESSAGE TYPES,0.33153638814016173,"As we use a contrastive loss instead of the conventional referential loss applied by (e.g. Havrylov
& Titov, 2017; Ren et al., 2020), we also compare the behaviours of both loss functions on G1
s “
G1
t “ t“refer2”, “refer10”, “refer100”u. We discover that the number of message types is some times
greatly less than the size of input space. We refer such phenomenon as “message type collapse”, and
the mappings from several different inputs to a same message type as “degeneracy components”."
THE COLLAPSE OF MESSAGE TYPES,0.33423180592991913,"While comparing the two loss functions, we observe that contrastive loss is more efﬁcient on removing
degenerate components in large-scale games (where |C| ě 100), and can better avoid the collapse of"
THE COLLAPSE OF MESSAGE TYPES,0.33692722371967654,Published as a conference paper at ICLR 2022
THE COLLAPSE OF MESSAGE TYPES,0.33962264150943394,"the number of message types. To be speciﬁc, as shown in Figure 5a, the numbers of message types
collapses to less than 500 on games with conventional referential loss. They, however, would only
decrease to roughly 5, 000 on ‘refer10’ and ‘refer1000’ with contrastive loss as shown in Figure 5b.
The contrastive loss can also make the training of referential game more stable, as illustrated by the
smaller variance in Figure 5b."
THE COLLAPSE OF MESSAGE TYPES,0.3423180592991914,"Our results indicate that it is necessary to establish a referential game with candidate sets containing
enough many distractors to guarantee that the emergent language will be relatively unambiguous. As
far as we know, the existing works do not investigate the effects of the size of candidate sets, e.g. Lee
et al. (2018) set |C| “ 2, Li & Bowling (2019) set |C| “ 5, and |C| ď 20 is used by Lazaridou et al.
(2018). Given our results, our heuristic is that |C| should be greater than either 1, 000 for large input
spaces or
1
10|X| for smaller ones."
THE COLLAPSE OF MESSAGE TYPES,0.3450134770889488,"We also explore more about the degeneracy degree (as well as structure degree) of the mappings
constituting different languages, and the results and discussion are provided in Appendix C."
THE COLLAPSE OF MESSAGE TYPES,0.3477088948787062,"0
100
200
300
400
500
Epochs 0 2000 4000 6000 8000 10000"
THE COLLAPSE OF MESSAGE TYPES,0.3504043126684636,Number of Messages
THE COLLAPSE OF MESSAGE TYPES,0.353099730458221,Source Games
THE COLLAPSE OF MESSAGE TYPES,0.3557951482479784,"refer2
refer10
refer100"
THE COLLAPSE OF MESSAGE TYPES,0.3584905660377358,(a) With conventional referential loss
THE COLLAPSE OF MESSAGE TYPES,0.3611859838274933,"0
100
200
300
400
500
Epochs 0 2000 4000 6000 8000 10000"
THE COLLAPSE OF MESSAGE TYPES,0.3638814016172507,Number of Messages
THE COLLAPSE OF MESSAGE TYPES,0.3665768194070081,Source Games
THE COLLAPSE OF MESSAGE TYPES,0.3692722371967655,"refer2
refer10
refer100"
THE COLLAPSE OF MESSAGE TYPES,0.3719676549865229,(b) With contrastive loss
THE COLLAPSE OF MESSAGE TYPES,0.3746630727762803,"Figure 5: How number of message types change over training epochs with different loss functions.
The lines are the means over 6 runs and shadow areas indicate the corresponding variance."
CONCLUSION & FUTURE WORK,0.37735849056603776,"6
CONCLUSION & FUTURE WORK"
CONCLUSION & FUTURE WORK,0.38005390835579517,"To our knowledge, this is the ﬁrst work exploring the expressivity of emergent languages from
language games played by DL-based agents that have different context size. We ﬁrst proposed
a hypothesis about the factors inﬂuencing the expressivity of emergent languages, and deﬁned a
partial ordering between expressivity of different emergent languages based on the generalisation
performance across a set of games. Through a series of experiments on referential games with varying
size of context, we then validated the three predictions of our hypothesis, which furthers indicates
that expressivity is indeed a trade-off between the complexity and unpredictability of context in
language games. To overcome the memory inefﬁciency issue caused by the conventional referential
loss function, we introduced a contrastive loss into our implementation. In the comparison between
this contrastive loss and the referential loss, we found the message type collapse problem, and also
showed that using this contrastive loss can alleviate it."
CONCLUSION & FUTURE WORK,0.38274932614555257,"Since this is a ﬁrst step towards deﬁning and understanding the expressivity of emergent languages,
there are still some questions unanswered by this work. For example, some properties of our emergent
languages are surprising given the simplicity of some of our referential games. In the simplest setting
(|C| “ 2), we see emergent languages with more than 2, 000 message types, whereas an optimal
strategy requires a language consists of only 40 unique messages (since there are only 40 different
values for all properties in X and two items in the context always differ on at least one property).
Thus, it shows that further investigation about how redundancy helps during the communication
between agents is necessary. More importantly, we lack understanding of the structure of mappings
constituting a language, especially when the languages are generated or encoded by neural networks.
We believe that the expressivity of languages still needs further exploration in order to facilitate
languages that are universally effective across various tasks."
CONCLUSION & FUTURE WORK,0.38544474393531,Published as a conference paper at ICLR 2022
CONCLUSION & FUTURE WORK,0.3881401617250674,ACKNOWLEDGEMENT
CONCLUSION & FUTURE WORK,0.3908355795148248,"This project has also received funding from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme under grant agreement No. 681942, held
by K. S."
REFERENCES,0.3935309973045822,REFERENCES
REFERENCES,0.39622641509433965,"Jacob Andreas. Measuring compositionality in representation learning. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
HJz05o0qK7."
REFERENCES,0.39892183288409705,"Henry Brighton and Simon Kirby. Understanding linguistic evolution by visualizing the emergence
of topographic mappings. Artiﬁcial life, 12(2):229–242, 2006."
REFERENCES,0.40161725067385445,"Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and Marco Baroni.
Compositionality and generalization in emergent languages. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pp. 4427–4442, Online, July 2020.
Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.407. URL https:
//aclanthology.org/2020.acl-main.407."
REFERENCES,0.40431266846361186,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning, pp.
1597–1607. PMLR, 2020."
REFERENCES,0.40700808625336926,"Laura Harding Graesser, Kyunghyun Cho, and Douwe Kiela. Emergent linguistic phenomena in
multi-agent communication games. In Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP), pp. 3700–3710, Hong Kong, China, November 2019. Association
for Computational Linguistics. doi: 10.18653/v1/D19-1384. URL https://aclanthology.
org/D19-1384."
REFERENCES,0.40970350404312667,"Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural
Information Processing Systems, 33:21271–21284, 2020."
REFERENCES,0.4123989218328841,"Shangmin Guo, Yi Ren, Serhii Havrylov, Stella Frank, Ivan Titov, and Kenny Smith. The emergence
of compositional languages for numeric concepts through iterated learning in neural agents. The
13th International Conference on the Evolution of Language, 2020."
REFERENCES,0.41509433962264153,"Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
(CVPR’06), volume 2, pp. 1735–1742. IEEE, 2006."
REFERENCES,0.41778975741239893,"Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to
communicate with sequences of symbols. In Advances in Neural Information Processing Systems,
2017."
REFERENCES,0.42048517520215634,"Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.42318059299191374,"Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=rkE3y85ee."
REFERENCES,0.42587601078167114,"Ivana Kaji´c, Eser Aygün, and Doina Precup. Learning to cooperate: Emergent communication in
multi-agent navigation. arXiv preprint arXiv:2004.01097, 2020."
REFERENCES,0.42857142857142855,"Eugene Kharitonov, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni. EGG: a toolkit
for research on Emergence of lanGuage in Games. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference"
REFERENCES,0.431266846361186,Published as a conference paper at ICLR 2022
REFERENCES,0.4339622641509434,"on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations. Association for
Computational Linguistics, 2019. doi: 10.18653/v1/D19-3010. URL https://www.aclweb.
org/anthology/D19-3010."
REFERENCES,0.4366576819407008,"Eugene Kharitonov, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni. Entropy minimization
in emergent languages. In International Conference on Machine Learning, pp. 5220–5230. PMLR,
2020."
REFERENCES,0.4393530997304582,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015. URL https://openreview.net/forum?
id=8gmWwjFyLj."
REFERENCES,0.4420485175202156,"Simon Kirby. Spontaneous evolution of linguistic structure-an iterated learning model of the emer-
gence of regularity and irregularity. IEEE Transactions on Evolutionary Computation, 5(2):
102–110, 2001."
REFERENCES,0.444743935309973,"Simon Kirby and James R Hurford. The emergence of linguistic structure: An overview of the
iterated learning model. In Simulating the evolution of language, pp. 121–147. Springer, 2002."
REFERENCES,0.4474393530997305,"Simon Kirby, Tom Grifﬁths, and Kenny Smith. Iterated learning and the evolution of language.
Current opinion in neurobiology, 28:108–114, 2014."
REFERENCES,0.4501347708894879,"Simon Kirby, Monica Tamariz, Hannah Cornish, and Kenny Smith. Compression and communication
in the cultural evolution of linguistic structure. Cognition, 141:87–102, 2015."
REFERENCES,0.4528301886792453,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.4555256064690027,"Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the
emergence of (natural) language. In International Conference on Learning Representations, 2017."
REFERENCES,0.4582210242587601,"Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. Emergence of linguistic
communication from referential games with symbolic and pixel input. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
HJGv1Z-AW."
REFERENCES,0.4609164420485175,"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444,
2015."
REFERENCES,0.4636118598382749,"Jason Lee, Kyunghyun Cho, Jason Weston, and Douwe Kiela. Emergent translation in multi-agent
communication. In International Conference on Learning Representations, 2018."
REFERENCES,0.46630727762803237,"David Lewis. Convention: A philosophical study. John Wiley & Sons, 1969."
REFERENCES,0.46900269541778977,"Fushan Li and Michael Bowling. Ease-of-teaching and language structure from emergent com-
munication.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
b0cf188d74589db9b23d5d277238a929-Paper.pdf."
REFERENCES,0.4716981132075472,"Ryan Lowe, YI WU, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch.
Multi-agent actor-critic for mixed cooperative-competitive environments.
In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
68a9750337a418a86fe06c1991a1d64c-Paper.pdf."
REFERENCES,0.4743935309973046,"Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345–1359, 2009."
REFERENCES,0.477088948787062,"Yi Ren, Shangmin Guo, Matthieu Labeau, Shay B. Cohen, and Simon Kirby. Compositional
languages emerge in a neural iterated learning model. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=HkePNpVKPB."
REFERENCES,0.4797843665768194,Published as a conference paper at ICLR 2022
REFERENCES,0.48247978436657685,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016."
REFERENCES,0.48517520215633425,"James Winters, Simon Kirby, and Kenny Smith. Contextual predictability shapes signal autonomy.
Cognition, 176:15–30, 2018."
REFERENCES,0.48787061994609165,"Ludwig Wittgenstein. Philosophical investigations. John Wiley & Sons, 1954."
REFERENCES,0.49056603773584906,"Luyao Yuan, Zipeng Fu, Jingyue Shen, Lu Xu, Junhong Shen, and Song-Chun Zhu. Emergence of
pragmatics from referential game between theory of mind agents. arXiv preprint arXiv:2001.07752,
2020."
REFERENCES,0.49326145552560646,"Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Efﬁcient communication in multi-agent reinforcement
learning via variance based control. Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.49595687331536387,Published as a conference paper at ICLR 2022
REFERENCES,0.49865229110512127,"A
MORE DETAILS ABOUT COMPARISON BETWEEN DIFFERENT LOSS
FUNCTIONS"
REFERENCES,0.5013477088948787,"In this section, we give more details about the comparison between the referential loss used by
previous works and the contrastive loss we use in Section 2.2."
REFERENCES,0.5040431266846361,"Following the same notations used in Section 2.2, the referential loss used by (e.g. Havrylov & Titov,
2017) can be written as:"
REFERENCES,0.5067385444743935,"ppci|xi, mq9 exp EphL
m, f L
cencpxiqq “ exp
´
hL
m
J ¨ f L
cencpxiq
¯
(4)"
REFERENCES,0.5094339622641509,"where hL
m is the embedding for the message m obtained from the sequence encoder of listener L,
and f L
cencp¨q is the candidate encoding function of L. From the above equation, we can see that if a
candidate has an embedding closer to message m, the probability mass for predicting it would then
be higher. Hence, the predicated probability mass is inversely proportional to the distances, e.g. the
dot product in the above equation, between the embeddings of m and all candidates."
REFERENCES,0.5121293800539084,The contrastive loss function proposed by Chen et al. (2020) with τ “ 1 is given as follow:
REFERENCES,0.5148247978436657,"ℓi,j “ ´
exp psimpxi, xjqq
ř"
REFERENCES,0.5175202156334232,"k Irk‰is exp psimpxi, xkqq
(5)"
REFERENCES,0.5202156334231806,"where I is an indicator function. By comparing the above loss function with Equation 4, we ﬁnd
that the aim of them is the same, i.e. make the distance between positive pairs (targets and their
corresponding messages) closer while keeping the embeddings for negative pairs (distractors and
the messages) as far apart as possible. While negative sampling has been deeply investigated in the
self-supervised learning (SSL) community, (e.g. Chen et al., 2020; Grill et al., 2020), it has not been
discussed in the emergent communication community yet, thus we simply use all randomly sampled
distractors as negative examples. As mentioned in Section 2.2, we need to make the whole input
space as candidate set, thus the above contrastive loss which has lower space complexity is preferred."
REFERENCES,0.522911051212938,"In the following, we give a bit further derivation to show that Equation 1 is computationally equivalent
to Equation 5. By substituting k in the denominator of Equation 5 with j, we can see that:"
REFERENCES,0.5256064690026954,"ℓi,j “ ´ log
exp psimpxi, xjqq
ř"
REFERENCES,0.5283018867924528,"j Irj‰is exp psimpxi, xkqq “ log ř"
REFERENCES,0.5309973045822103,"j Irj‰is exp psimpxi, xkqq"
REFERENCES,0.5336927223719676,"exp psimpxi, xjqq “ log ř"
REFERENCES,0.5363881401617251,"j Irj‰is exp psimpxi, xkqq"
REFERENCES,0.5390835579514824,"exp psimpxi, xjqq
` log exp psimpxi, xiqq"
REFERENCES,0.5417789757412399,"exp psimpxi, xiqq
loooooooooooomoooooooooooon “0 “ log ř"
REFERENCES,0.5444743935309974,"j exp psimpxi, xjqq"
REFERENCES,0.5471698113207547,"exp psimpxi, xjqq"
REFERENCES,0.5498652291105122,"“ ´ log
exp psimpxi, xjqq
ř"
REFERENCES,0.5525606469002695,"j exp psimpxi, xjqq (6)"
REFERENCES,0.555256064690027,"B
MORE DETAILS ABOUT CONTEXTUAL COMPLEXITY/UNPREDICTABILITY"
REFERENCES,0.5579514824797843,"In this section, we give more details about the deﬁnition of context and its complexity/unpredictability."
REFERENCES,0.5606469002695418,"B.1
CONTEXT"
REFERENCES,0.5633423180592992,"As deﬁned in Section 3.1, the context for a target object xt is the space of samples involved in
the calculation of its loss. We know that the prediction of the listener is calculated based on the
distance between the message received (mpxtq) which is a function of the target input (xt), i.e.
softmax pd1 pmpxtq, xdqq where xd is a distractor and d1 is a distance function for embeddings.
Cross entropy loss is then calculated based on softmax function. Thus, we can also write the loss
function as Lpmpxtq, txdu; θq. As shown in the equation, the calculation is based on samples from
candidate set C. Therefore, in referential game, Cpxtq “ txtu Y txdu “ C Ď X Ă O."
REFERENCES,0.5660377358490566,Published as a conference paper at ICLR 2022
REFERENCES,0.568733153638814,"B.2
COMPLEXITY OF CONTEXT"
REFERENCES,0.5714285714285714,"As deﬁned in Section 3.1, the k-th degree neighbour set is Nkpxtq “ txj : dpxt, xjq ď ku where
k P Z`. As described in Section 2.1, there are 4 features in our synthetic data set and each of them
can take 10 different values. Therefore, for a speciﬁc target xt, the k in the deﬁnition of neighbour
set can take only the following 4 values:"
REFERENCES,0.5741239892183289,"• k “ 1: samples in N1pxtq differ only in 1 feature from xt. For the only different feature,
there are only 9 possible values a distractor can take. Thus, |N1pxtq| “ 4 ˆ 9 “ 36."
REFERENCES,0.5768194070080862,"• k “ 2: samples from N1pxtq and samples that differ at exactly 2 features from xt. For the
later case, there are ﬁrst
`4
2
˘
“ 12 possible combinations of such two features, and there are
9ˆ9 “ 81 possible samples for each combination. Thus, |N2pxtq| “ 12ˆ81`36 “ 1008."
REFERENCES,0.5795148247978437,"• k “ 3: samples from N2pxtq and samples that differ at exactly 3 features from xt. For
the later case, there are ﬁrst
`4
3
˘
“ 4 possible combinations of such three features, and
there are 9 ˆ 9 ˆ 9 “ 729 possible samples for each combination. Thus, |N3pxtq| “
4 ˆ 729 ` 1008 “ 3924."
REFERENCES,0.5822102425876011,"• k “ 4: all samples in X except xt are in this neighbour set, thus |N4pxtq| “ 10000 ´ 1 “
9999"
REFERENCES,0.5849056603773585,"Since we use independent Bernoulli sampling, for each draw, the probability to get a sample from
Nkpxtq is simply Nkpxtq"
REFERENCES,0.5876010781671159,"|X|´1 where |X| ´ 1 is due to excluding xt from X. Meanwhile, since we deﬁne
Cpxtq as the candidate set in referential games. Therefore, for |Cpxtq| “ |C| draws, the probability
of having at one sample from Nkpxtq is:"
REFERENCES,0.5902964959568733,"1 ´
ˆ
1 ´ |Nkpxtq|"
REFERENCES,0.5929919137466307,|X| ´ 1
REFERENCES,0.5956873315363881,"˙|C|
“ 1 ´
ˆ|X| ´ 1 ´ |Nkpxtq|"
REFERENCES,0.5983827493261455,|X| ´ 1
REFERENCES,0.601078167115903,"˙|C|
."
REFERENCES,0.6037735849056604,"B.3
UNPREDICTABILITY OF CONTEXT"
REFERENCES,0.6064690026954178,"In this section, we ﬁrst derive our measure of unpredictability following Winters et al. (2018). In
their experiments, the input space consists of 16 referents. There are 2 different attributes, colour and
shape, and each attribute has 4 different values. By setting the size of candidate set to 4, they deﬁne
the following two types of context:"
REFERENCES,0.6091644204851752,"• shape-different: all the 3 different distractors have identical colour to the target, but
different shapes. Since there are only 4 shapes and candidate set size is also 4, in this setting,
the distractors for a given target will never change during the games. This is exactly the
same as our “ﬁxed-batch” games deﬁned in Appendix E.1."
REFERENCES,0.6118598382749326,"• mixed: all the 3 different distractors are sampled uniformly at random from the input space,
thus they may have different shapes as well as different colours to the target. In this case,
the distractors for a given target vary across different trials during the games."
REFERENCES,0.6145552560646901,"By comparing the two above cases, it is straightforward to see that it is more possible to see different
distractors for a given target in the games with mixed context. Following this heuristic, we proposed
to measure the unpredictability of context by the expected chance of observing different distractors for
targets across training epochs, which further leads us to the deﬁnition given in Equation 3. Note that
there is also complexity (under our deﬁnition) involved in the above two types of context, we focus
only on the variability part of the context in our deﬁnition of unpredictability since the complexity
part has been taken into consideration by the neighbourhood set deﬁned in Section 3.1."
REFERENCES,0.6172506738544474,"We then give further explanation about Equation 3 and its simpliﬁed form. Since we use Bernoulli
sampling for each individual object in candidate set, the overall sampling of candidate set is thus a
binomial procedure. As the probability of getting a sample from Cepxtq in X is |Cepxtq|"
REFERENCES,0.6199460916442049,"|X|
, the overall"
REFERENCES,0.6226415094339622,"proportion of objects from Cepxtq in Ce`1pxtq is the same, i.e. |Cepxtq|"
REFERENCES,0.6253369272237197,"|X|
. Therefore, the probability of"
REFERENCES,0.628032345013477,"the complementary event, i.e. getting an object not from Cepxtq in Ce`1pxtq, is 1 ´ |Cepxtq| |X|
."
REFERENCES,0.6307277628032345,Published as a conference paper at ICLR 2022
REFERENCES,0.633423180592992,"C
MORE RESULTS AND DISCUSSION ON EXPRESSIVITY"
REFERENCES,0.6361185983827493,"In this section, we provide two more intermediate properties of emergent languages that are also
inﬂuenced by complexity and unpredictability of context, and thua may inﬂuence the expressivity."
REFERENCES,0.6388140161725068,"C.1
DEGREE OF DEGENERACY AFFECTS BUT DOES NOT DETERMINE THE EXPRESSIVITY"
REFERENCES,0.6415094339622641,"Inspired by Kirby et al. (2015), the ﬁrst intermediate factor we explore is the degree of degeneracy
(i.e. the degree of ambiguity) of the languages. As a language is deﬁned as a mapping function
L : X ÞÑ M, the degree of its degeneracy could be measured by the number of distinct message
types."
REFERENCES,0.6442048517520216,"As shown in the Table 1, in referential languages, the degeneracy degree decreases rapidly with the
increase of the contextual complexity of the source games, and reaches a ceiling around |D| ě 1, 000.
There is a positive correlation between the number of message types and expressivity for the languages
with low complexity, which indicates that degree of degeneracy could be an intermediate factor that
is correlated with the expressivity of these languages. However, the fact that the average number of
message types on refer10000 is higher than refer1000, while the expressivity of ‘refer1000’ is higher
than ‘refer10000’, i.e. emergent languages having lower level of degeneracy does not necessarily
have higher expressivity, indicates that degeneracy degree of one language cannot determine its
expressivity."
REFERENCES,0.6469002695417789,"Game
refer2
refer10
refer100
refer1000
refer2500
refer5000
refer7500
refer10000
Mean
2872.18
8565.69
9547.58
9801.60
9839.33
9841.27
9840.34
9856.96
25th-percentile
2629.0
8325.0
9501.5
9778.75
9825.0
9837.5
9819.0
9845.0
75th-percentile
3195.0
8795.5
9639.25
9823.25
9854.25
9864.25
9866.0
9871.0"
REFERENCES,0.6495956873315364,"Table 1: Mean, 25th-percentile, and 75th-percentile of the distributions of message types numbers
from different source game."
REFERENCES,0.6522911051212938,"C.2
STRUCTURE DEGREE OF MAPPINGS OF DIFFERENT LANGUAGES"
REFERENCES,0.6549865229110512,"Since the degenerate degree is not the only factor determining expressivity, another factor to be
considered is how the mappings of a language L are structured after the training procedure of agents
on source games. According to the cardinality of the mappings of L, we can divide them into
two disjoint sets: i) one-to-one mappings, i.e. an input is mapped to a unique message; and ii)
many-to-one mappings (a.k.a degenerate components), i.e. several inputs are mapped to the same
message type. Between this two types, the structure of one-to-one mappings has been explored under
the view of compositionality in the previous works, e.g. (Brighton & Kirby, 2006; Andreas, 2019).
However, as illustrated by Chaabouni et al. (2020), for emergent languages from DL-based agents, a
good generalisability does not require a good compositionality, which means that measurements for
compositionality are not good predictors for expressivity either. In this work, we focus on only the
many-to-one mappings."
REFERENCES,0.6576819407008087,"Given the objectives of games, for the degenerate components in an emergent language, a message m
being mapped to several closely-related inputs could imply that it captures common features among
these inputs. Suppose that several inputs XK “ tx1, . . . , xKu are mapped to the same message
m in a converged language L, we hypothesise that the average distances between every pair of
inputs, i.e.
1
|XK|
ř
i,jPt1,...,Ku dpxi, xjq, would become smaller after training, compared with their
random initial values. Therefore, we compare the averaged distances between meanings of degenerate
components as well as the frequency of degenerate components at the beginning and end of training.
To make the plots more readable, we only plot the statistics about the top-10 most frequent ambiguous
message types, where the frequency of a message type is deﬁned as the number of inputs that are
mapped to it in a languages. Results on ‘refer1000’ (one of the most expressive language in our
experiments) are shown in Figure 6."
REFERENCES,0.660377358490566,"By comparing the data of beginning and end of training in Figure 6b, we can easily observe that the
averaged distance decreased throughout the training, which indicates that the degenerate components
actually became more although not fully structured. Another observation we have limited under-
standing of is that ‘refer1000’ becomes more ambiguous after training (shown by that the red bars"
REFERENCES,0.6630727762803235,Published as a conference paper at ICLR 2022
REFERENCES,0.6657681940700808,"1
2
3
4
5
6
7
8
9
10
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0"
REFERENCES,0.6684636118598383,Frequency
REFERENCES,0.6711590296495957,"beginning
end"
REFERENCES,0.6738544474393531,(a) Frequencies of degeneracy components
REFERENCES,0.6765498652291105,"1
2
3
4
5
6
7
8
9
10
0 1 2 3 4 5"
REFERENCES,0.6792452830188679,Distance
REFERENCES,0.6819407008086253,"beginning
end"
REFERENCES,0.6846361185983828,(b) Distance between corresponding inputs
REFERENCES,0.6873315363881402,"Figure 6: Statistics about frequency of Top-10 most frequent message type and the average Hamming
distances between their meanings at the beginning and end of training on ‘refer1000’. The bars
indicate the frequency and the Hamming distance, and the black lines indicate the corresponding SD."
REFERENCES,0.6900269541778976,"representing the statistics at the end of training are lower than the bars representing the statistics at
the beginning of training), although its accuracy does improve to almost 100%. However, limited by
the scope of this work, we will leave further exploration of it to future works."
REFERENCES,0.692722371967655,"D
MUTUAL INFORMATION IS INSUFFICIENT"
REFERENCES,0.6954177897574124,"In Section 3.2, we mentioned that MI cannot completely reﬂect the expressivity of an emergent
language. Here, we provide more detailed explanations. On one hand, a higher MI value does not
necessarily mean more task-relevant information, e.g. encoding colour in messages is unnecessary
for tasks involving only the shapes of inputs although it provides more information. On the other
hand, not only the determinacy of the mappings matters for generalising them, but also the structures,
e.g. it would be easier for humans to extrapolate compositional languages to novel concepts than
holistic ones although they are both unambiguous and therefore have the same MI. In the following
paragraphs, we will give more reasoning and empirical evidence to support our claim."
REFERENCES,0.6981132075471698,"Following the mapping function deﬁnition of emergent languages illustrated in Section 2.1, suppose
that we get a message type collection M Ă M in some training epoch, the calculation of the mutual
information between M and X is given as follows:"
REFERENCES,0.7008086253369272,"IpM|Xq “
ÿ xPX ÿ"
REFERENCES,0.7035040431266847,"mPM
ppx, mq log ppx, mq"
REFERENCES,0.706199460916442,ppxqppmq
REFERENCES,0.7088948787061995,"ppx, mq “
1
|X|
“
ÿ xPX ÿ mPM1"
REFERENCES,0.7115902964959568,"1
|X| log ppx, mq"
REFERENCES,0.7142857142857143,ppxqppmq
REFERENCES,0.7169811320754716,"“
1
|X| ÿ xPX ÿ"
REFERENCES,0.7196765498652291,"mPM
log ppx, mq"
REFERENCES,0.7223719676549866,ppxqppmq
REFERENCES,0.7250673854447439,"“
1
|X| ÿ xPX ÿ"
REFERENCES,0.7277628032345014,"mPM
plog ppx, mq ´ log ppxq ´ log ppmqq"
REFERENCES,0.7304582210242587,"ppxq “
1
|X|
“
1
N ÿ xPX ÿ mPM"
REFERENCES,0.7331536388140162,"ˆ
log 1"
REFERENCES,0.7358490566037735,|X| ´ log 1
REFERENCES,0.738544474393531,"|X| ´ log ppmq
˙"
REFERENCES,0.7412398921832885,"“
1
|X| ÿ xPX ÿ"
REFERENCES,0.7439353099730458,"mPM
p´ log ppmqq “ ´
ÿ"
REFERENCES,0.7466307277628033,"mPM
log ppmq"
REFERENCES,0.7493261455525606,"Published as a conference paper at ICLR 2022 “ ´
ÿ"
REFERENCES,0.7520215633423181,"mPM
log |Lpmq| |M| “ ´
ÿ"
REFERENCES,0.7547169811320755,"mPM
plog |Lpmq| ´ log |M|q “
ÿ"
REFERENCES,0.7574123989218329,"mPM
plog |M| ´ log |Lpmq|q"
REFERENCES,0.7601078167115903,"where ppxq “
1
|X| as x follows a uniform distribution over the input space, ppx, mq “
1
|X| since
every x is mapped to a message, and |Lpmq| is the appearance number of m in one language."
REFERENCES,0.7628032345013477,"1
20
40
60
80 100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400 420 440 460 480 500"
REFERENCES,0.7654986522911051,Epochs 50000 60000 70000 80000 90000
REFERENCES,0.7681940700808625,Mutual Information
REFERENCES,0.77088948787062,Source Games
REFERENCES,0.7735849056603774,"refer10
refer100
refer1000
refer2500
refer5000
refer7500
refer10000"
REFERENCES,0.7762803234501348,"Figure 7: Mutual information curves over training epochs in different types of games. The converged
values are provided in Table 2"
REFERENCES,0.7789757412398922,"By observing the above equation, it is straightforward to see that the MI value is decided by the number
of message types, i.e. the degeneracy degree of one language. Therefore, if two emergent languages
have approximately the same number of messages types (as shown in Table 1 and degeneracy
degree, the MI values for these two languages should also be the same. However, as we illustrate in
Section C.1 that degree of degeneracy cannot determine the expressivity of emergent languages, we
can then deduce that MI cannot fully measure the expressivity of emergent languages."
REFERENCES,0.7816711590296496,"To provide more empirical evidence to support our claim, we tracked the MI values over training
epochs on a subset of Gs, and the results are shown in the following Figure 7. As we can see, there is
no statistically signiﬁcant difference between the MI values of ‘referX’ (where Xě 1, 000) while it
has been shown in Section 5.3 that their expressivity is different."
REFERENCES,0.784366576819407,"Game
refer10
refer100
refer1000
refer2500
refer5000
refer7500
refer10000
Mean
78269.22
86823.66
89823.16
90447.69
90504.40
90378.89
90533.06
SD
3760.05
1464.69
443.12
213.76
412.37
306.85
139.42"
REFERENCES,0.7870619946091644,Table 2: Converged MI from different source games. Both mean and standard deviation are given.
REFERENCES,0.7897574123989218,"E
LANGUAGE TRANSFER EXPERIMENT RESULTS"
REFERENCES,0.7924528301886793,"E.1
NAMING OF GAMES AND LANGUAGES"
REFERENCES,0.7951482479784366,"In this section, we give more details about the naming of games and corresponding languages we
used in Section 5 as follows:"
REFERENCES,0.7978436657681941,Published as a conference paper at ICLR 2022
REFERENCES,0.8005390835579514,"• “referX”: this refers to the referential games with candidate sets whose size is X (which
itself is an integer), note that we use double quotation marks to indicate that this is a speciﬁc
type of game. For example, “refer2500” refers to referential games with 2, 500 candidates
for listeners.
• ‘referX’: this refers to a emergent languages from “referX”, note that we use apostrophes
here in order to differentiate it from the notations of the corresponding game. For example,
‘refer2500’ refers to emergent languages from referential games with 2, 500 candidates for
listeners.
• “referXf”: this refers to the referential games with candidate sets whose size is X, but the
partition of batches is identical over epochs, thus giving the game a ﬁxed context. For
example, “refer2500f” refers to referential games with 2, 500 candidates for listeners, but
the batches of data are kept identical through the whole training procedure.
• ‘referXf’: this refers to the emergent languages from “referXf”. For example, ‘refer2500f’
refers to emergent languages from referential games with 2, 500 candidates for listeners
with ﬁxed context. Regarding the meaning of “ﬁxed-batch” or “ﬁxed context”, we give an
example below where the whole input space is ta, b, c, du and batch size is 2:"
REFERENCES,0.8032345013477089,"– varying batch/context: ta, bu, tc, du is the partition for the 1st epoch; then ta, cu, tb, du
is used for the 2nd epoch; and random partitions are used in the following epochs;
– ﬁxed batch/context: whichever epoch it is, ta, bu, tc, du is always used as the partition
of context;
• “referX/f”: this equals a “referX” and a “referXf”, both of them have same complexity but
different unpredictability. To be more speciﬁc, contextual unpredictability of “referX” is
higher than “referXf”.
• Gs: the universal set of source games which is deﬁned as t“refer2”, “refer10”, “refer100”,
“refer1000”, “refer2500”, “refer5000”, “refer7500”, “refer10000”, “refer2f”, “refer10f”,
“refer100f”, “refer1000f”, “refer2500f”, “refer5000f”, “refer7500f”, u.
• Gt: the universal set of target games which is deﬁned as t“refer2”, “refer10”, “refer100”,
“refer1000”, “refer2500”, “refer5000”, “refer7500”, “refer10000”u."
REFERENCES,0.8059299191374663,"E.2
RAW RESULTS OF COMPLEXITY EXPERIMENT"
REFERENCES,0.8086253369272237,"In this section, we provide the raw data of Figure 2 in the following Table 3. Not that although
“refer1000f” performs the best on “refer10” than all other languages, we argue this outlier doesn’t
inﬂuence conclusion of our prediction considering the simplicity of “refer10”."
REFERENCES,0.8113207547169812,"Source
Statistics
refer2
refer10
refer100
refer1000
refer2500
refer5000
refer7500
refer10000"
REFERENCES,0.8140161725067385,"refer2f
mean
0.9596
0.7423
0.2244
0.0322
0.0168
0.0163
0.0156
0.0163"
REFERENCES,0.816711590296496,"σ
0.0236
0.1493
0.1308
0.0203
0.0103
0.0105
0.0111
0.0119"
REFERENCES,0.8194070080862533,"refer10f
mean
0.9979
0.9823
0.8175
0.3259
0.2028
0.1918
0.1909
0.1879"
REFERENCES,0.8221024258760108,"σ
0.0007
0.0044
0.0404
0.0518
0.0461
0.0446
0.0500
0.0418"
REFERENCES,0.8247978436657682,"refer100f
mean
0.9983
0.9958
0.9655
0.7428
0.5988
0.5933
0.5946
0.5871"
REFERENCES,0.8274932614555256,"σ
0.0010
0.0029
0.0106
0.0573
0.0738
0.0685
0.0719
0.0663"
REFERENCES,0.8301886792452831,"refer1000f
mean
0.9989
0.9977
0.9881
0.8794
0.7849
0.7793
0.7565
0.7686"
REFERENCES,0.8328840970350404,"σ
0.0008
0.0006
0.0032
0.0163
0.0250
0.0242
0.0664
0.0237"
REFERENCES,0.8355795148247979,"refer2500f
mean
0.9992
0.9987
0.9892
0.8871
0.8044
0.7960
0.7788
0.7880"
REFERENCES,0.8382749326145552,"σ
0.0008
0.0005
0.0032
0.0199
0.0259
0.0239
0.0267
0.0264"
REFERENCES,0.8409703504043127,"refer5000f
mean
0.9988
0.9980
0.9897
0.8916
0.8094
0.7980
0.7941
0.7928"
REFERENCES,0.8436657681940701,"σ
0.0007
0.0006
0.0023
0.0077
0.0099
0.0143
0.0127
0.0122"
REFERENCES,0.8463611859838275,"refer10000
mean
0.9993
0.9980
0.9897
0.8864
0.8095
0.7991
0.7943
0.7950"
REFERENCES,0.8490566037735849,"σ
0.0005
0.0007
0.0021
0.0084
0.0142
0.0095
0.0123
0.0107"
REFERENCES,0.8517520215633423,"Table 3: Mean and standard deviation σ of generalisation performance of languages from ﬁxed-context
on various target games."
REFERENCES,0.8544474393530997,Published as a conference paper at ICLR 2022
REFERENCES,0.8571428571428571,"E.3
MORE RESULTS OF TRADE-OFF EXPERIMENT"
REFERENCES,0.8598382749326146,"In this section, we ﬁrst provide the complete version of Figure 4, i.e. Figure 8. Since the performance
of ‘refer2’, ‘refer10’, and ‘refer100’ are too bad on complex target games, it is very clear that their
expressivity are much lower that the rest. Therefore, to emphasise the difference between the other
languages, we limit the range of y-axis to r0.7, 1.0s in Figure 8."
REFERENCES,0.862533692722372,"refer2
refer10
refer100
refer1000
refer2500
refer5000
refer7500
refer10000
Source Games 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8652291105121294,Accuracy
REFERENCES,0.8679245283018868,Target Games
REFERENCES,0.8706199460916442,"refer2
refer10
refer100
refer1000
refer2500
refer5000
refer7500
refer10000"
REFERENCES,0.8733153638814016,"Figure 8: Same to Figure 4, but with wider range for y-axis."
REFERENCES,0.876010781671159,"We also illustrate the partial order between the expressivity of different languages in Figure 9. It is
very clear that the ‘refer2’, ‘refer10’, and ‘refer100’ performs worse than the others on referential
games with more candidates than their sources. Combining this fact with the fact that ‘refer7500’ and
‘refer10000’ perform worse than ‘refer1000’, ‘refer2500’, and ‘refer5000’ shows that increasing the
size of candidate set |C| would not always beneﬁt the expressivity. As for ‘refer1000’, ‘refer2500’,
and ‘refer5000’, there is no statistically signiﬁcant difference between their performance on the
simpler referential games, and they intersect on some games, thus their expressivity are either
incomparable or approximately the same."
REFERENCES,0.8787061994609164,"At last, we give the precise values of both Figure 9 and 4 in the following Table 4. All the results are
obtained by multiple runs, thus both mean and standard deviation (σ) are given."
REFERENCES,0.8814016172506739,"To better support our claim, we also provide p-values of testing the statistical signiﬁcance between
the generalisation performance of two different source games across all types of target games in the
following Table 5."
REFERENCES,0.8840970350404312,"F
VARYING THE CAPACITY OF CHANNEL AND NETWORK"
REFERENCES,0.8867924528301887,"To verify that our ﬁndings are robust to the capacity of the communication channel and agents, we
also run some extra experiments on the following different conﬁgurations of referential games:"
REFERENCES,0.889487870619946,"1. larger channel capacity: set the length of messages to 8 and size of token inventory to 20,
thus the size of message space becomes 208;"
REFERENCES,0.8921832884097035,"2. larger agent capacity: set the hidden layer size to 512, thus the capacity of both speaker and
listener become larger than the original setting (256);"
REFERENCES,0.894878706199461,3. larger channel&agent capacity: do the above two changes at the same time.
REFERENCES,0.8975741239892183,"Since our key ﬁndings are about the generalisation performance between refer1000, re-
fer2500, and refer10000, we just run these extra experiments on a game set Gextra
“
t‘refer1000’, ‘refer2500’, ‘refer10000’u with multiple runs. The results of conﬁguration 1, 2, and 3
are given in Figure 10, 11, and 12 respectively. It can be observed in all ﬁgures that ‘refer1000’ or"
REFERENCES,0.9002695417789758,Published as a conference paper at ICLR 2022
REFERENCES,0.9029649595687331,"Figure 9: Generalisation performance of language from the same source game over different target
games, where each line represents a source game. Since the plot would become not readable if we
plot error bars here, we provide SD in Table 4 below. The x-axis indicates the type of target games,
and the y-axis is the accuracy on target games. If a line lies above the other over all the target games,
it means that the corresponding source language has a higher expressivity than the other. On the other
hand, if two lines intersect, e.g. ‘refer1000’ and ‘refer2500’ above, then the expressivity of these
languages is incomparable."
REFERENCES,0.9056603773584906,"Source
Statistics
refer2
refer10
refer100
refer1000
refer2500
refer5000
refer7500
refer10000"
REFERENCES,0.9083557951482479,"refer2
mean
0.9936
0.9543
0.6043
0.1203
0.0668
0.0616
0.0592
0.0565"
REFERENCES,0.9110512129380054,"σ
0.0014
0.0077
0.0394
0.0146
0.0105
0.0078
0.0090
0.0047"
REFERENCES,0.9137466307277629,"refer10
mean
0.9980
0.9909
0.9076
0.4896
0.3434
0.3318
0.3282
0.3188"
REFERENCES,0.9164420485175202,"σ
0.0007
0.0036
0.0245
0.0833
0.0809
0.0801
0.0778
0.0752"
REFERENCES,0.9191374663072777,"refer100
mean
0.9986
0.9978
0.9812
0.8298
0.7162
0.7121
0.7078
0.7001"
REFERENCES,0.921832884097035,"σ
0.0007
0.0009
0.0035
0.0171
0.0299
0.0312
0.0293
0.0350"
REFERENCES,0.9245283018867925,"refer1000
mean
0.9989
0.9979
0.9881
0.9003
0.8256
0.8139
0.8153
0.8033"
REFERENCES,0.9272237196765498,"σ
0.0007
0.0009
0.0015
0.0143
0.0151
0.0165
0.0226
0.0183"
REFERENCES,0.9299191374663073,"refer2500
mean
0.9988
0.9981
0.9892
0.9036
0.8235
0.8178
0.8160
0.8059"
REFERENCES,0.9326145552560647,"σ
0.0006
0.0009
0.0012
0.0121
0.0099
0.0158
0.0148
0.0197"
REFERENCES,0.9353099730458221,"refer5000
mean
0.9990
0.9975
0.9890
0.8991
0.8212
0.8141
0.8125
0.8013"
REFERENCES,0.9380053908355795,"σ
0.0004
0.0004
0.0022
0.0050
0.0140
0.0095
0.0117
0.0140"
REFERENCES,0.9407008086253369,"refer7500
mean
0.9990
0.9975
0.9890
0.8893
0.7963
0.7895
0.7906
0.7812"
REFERENCES,0.9433962264150944,"σ
0.0005
0.0004
0.0034
0.0156
0.0206
0.0185
0.0223
0.0215"
REFERENCES,0.9460916442048517,"refer10000
mean
0.9988
0.9980
0.9887
0.8916
0.8044
0.7930
0.7940
0.7828"
REFERENCES,0.9487870619946092,"σ
0.0007
0.0005
0.0023
0.0077
0.0099
0.0143
0.0127
0.0122"
REFERENCES,0.9514824797843666,"Table 4: Mean and standard deviation σ of generalisation performance of language games on each
other."
REFERENCES,0.954177897574124,"‘refer2500’ always perform better than ‘refer10000’ on all kinds of target games, i.e. ‘refer1000’
or ‘refer2500’ both have higher expressivity than ‘refer10000’ . Therefore, we believe the key
observation from Figure 9 holds across conﬁgurations of communication channel and capacity of
agents, which means that our conclusions are robust to the conﬁguration of language games."
REFERENCES,0.9568733153638814,Published as a conference paper at ICLR 2022
REFERENCES,0.9595687331536388,"Sources
refer2
refer10
refer100
refer1000
refer2500
refer5000
refer7500
refer10000
refer2 vs refer10
2.73e-06
5.27e-04
1.63e-06
1.27e-04
5.05e-04
5.92e-04
5.51e-04
5.56e-04
refer2 vs refer100
7.44e-06
3.82e-04
6.70e-06
1.57e-13
3.78e-09
2.12e-08
6.69e-07
1.00e-07
refer2 vs refer1000
4.62e-06
3.47e-04
6.21e-06
5.75e-16
7.36e-14
2.71e-12
2.51e-10
4.47e-10
refer2 vs refer2500
6.46e-06
3.50e-04
6.04e-06
7.39e-15
1.08e-14
5.08e-10
1.01e-10
5.59e-08
refer2 vs refer5000
2.40e-06
3.42e-04
6.11e-06
1.13e-13
5.47e-15
3.57e-15
1.43e-15
2.62e-11
refer2 vs refer7500
1.53e-06
3.53e-04
6.14e-06
6.99e-16
7.59e-12
9.73e-11
3.09e-10
1.98e-09
refer2 vs refer10000
7.69e-06
3.42e-04
6.11e-06
2.45e-11
5.66e-17
2.63e-14
2.72e-14
4.85e-12
refer10 vs refer100
1.71e-03
2.24e-04
5.99e-04
1.58e-04
4.44e-05
3.71e-05
1.64e-05
1.83e-05
refer10 vs refer1000
3.33e-04
7.23e-05
3.90e-04
8.50e-05
2.47e-05
2.59e-05
1.33e-05
1.51e-05
refer10 vs refer2500
3.23e-04
9.75e-05
3.54e-04
8.36e-05
3.11e-05
2.29e-05
2.04e-05
1.15e-05
refer10 vs refer5000
1.66e-04
4.92e-05
3.65e-04
9.31e-05
2.86e-05
3.16e-05
2.65e-05
2.03e-05
refer10 vs refer7500
1.81e-04
9.81e-05
3.70e-04
9.65e-05
2.70e-05
2.60e-05
1.77e-05
1.56e-05
refer10 vs refer10000
7.58e-04
5.61e-05
3.64e-04
1.09e-04
4.14e-05
3.76e-05
3.01e-05
2.73e-05
refer100 vs refer1000
2.88e-02
7.18e-04
1.00e-04
1.37e-04
1.10e-04
2.77e-04
1.26e-03
4.50e-04
refer100 vs refer2500
1.70e-03
4.17e-04
1.78e-05
1.08e-04
1.92e-04
1.89e-04
1.49e-03
3.87e-04
refer100 vs refer5000
2.50e-02
1.85e-03
2.43e-05
2.12e-04
1.70e-04
3.78e-04
1.99e-03
6.50e-04
refer100 vs refer7500
4.87e-02
1.39e-03
4.27e-05
4.11e-04
8.38e-04
1.09e-03
3.96e-03
1.97e-03
refer100 vs refer10000
5.08e-02
5.88e-04
2.51e-05
7.31e-04
7.31e-04
1.21e-03
4.03e-03
2.14e-03
refer1000 vs refer2500
4.93e-01
2.70e-01
2.78e-01
6.41e-01
9.84e-01
5.97e-01
9.07e-01
8.31e-01
refer1000 vs refer5000
1.85e-01
9.86e-01
1.14e-01
7.32e-01
7.58e-01
9.28e-01
8.53e-01
8.39e-01
refer1000 vs refer7500
4.98e-01
2.87e-01
3.86e-01
1.90e-01
3.74e-02
9.23e-02
1.13e-01
1.12e-01
refer1000 vs refer10000
4.55e-01
3.33e-01
1.15e-01
1.79e-01
4.96e-02
6.98e-02
1.05e-01
7.06e-02
refer2500 vs refer5000
5.03e-02
2.92e-01
9.26e-01
8.44e-01
7.33e-01
6.04e-01
6.95e-01
6.82e-01
refer2500 vs refer7500
1.55e-01
3.66e-01
7.53e-01
7.06e-02
2.85e-02
4.47e-02
6.60e-02
1.04e-01
refer2500 vs refer10000
1.50e-01
9.86e-01
7.56e-01
5.12e-02
1.98e-02
3.31e-02
4.02e-02
8.05e-02
refer5000 vs refer7500
5.21e-01
2.87e-01
6.05e-01
6.59e-02
4.95e-02
5.91e-02
8.63e-02
1.19e-01
refer5000 vs refer10000
6.08e-01
3.53e-01
7.58e-01
2.25e-02
5.91e-02
2.73e-02
3.71e-02
5.61e-02
refer7500 vs refer10000
9.22e-01
5.01e-02
4.89e-01
7.82e-01
4.31e-01
8.47e-01
7.67e-01
8.92e-01"
REFERENCES,0.9622641509433962,"Table 5: p-values of testing the statistical signiﬁcance between two different types of source languages
across various target games. We take 0.05 as the threshold for our p-values, and bold all the signiﬁcant
results. For the comparison between t‘refer1000’, ‘refer2500’, ‘refer5000’u and t‘refer7500’,
‘refer10000’u, we also make the signiﬁcant results be in red colour."
REFERENCES,0.9649595687331537,"refer1000
refer2500
refer10000
Target Games 0.80 0.82 0.84 0.86 0.88 0.90 0.92 0.94 0.96"
REFERENCES,0.967654986522911,Performance
REFERENCES,0.9703504043126685,"refer1000
refer2500
refer10000"
REFERENCES,0.9730458221024259,"Figure 10: Results of language transfer with larger channel capacity. Error bars indicate the standard
deviation of the mean values."
REFERENCES,0.9757412398921833,Published as a conference paper at ICLR 2022
REFERENCES,0.9784366576819407,"refer1000
refer2500
refer10000
Target Games 0.80 0.82 0.84 0.86 0.88 0.90 0.92 0.94"
REFERENCES,0.9811320754716981,Performance
REFERENCES,0.9838274932614556,"refer1000
refer2500
refer10000"
REFERENCES,0.9865229110512129,"Figure 11: Results of language transfer with larger agent capacity. Error bars indicate the standard
deviation of the mean values."
REFERENCES,0.9892183288409704,"refer1000
refer2500
refer10000
Target Games 0.84 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00"
REFERENCES,0.9919137466307277,Performance
REFERENCES,0.9946091644204852,"refer1000
refer2500
refer10000"
REFERENCES,0.9973045822102425,"Figure 12: Results of language transfer with larger channel and agent capacity. Error bars indicate
the standard deviation of the mean values."
