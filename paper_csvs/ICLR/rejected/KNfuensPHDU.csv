Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.006097560975609756,"Recent developments on the robustness of neural networks have primarily
emphasized the notion of worst-case adversarial robustness in both veriﬁ-
cation and robust training. However, often looser constraints are needed
and some margin of error is allowed. We instead consider the task of proba-
bilistic robustness, which assumes the input follows a known probabilistic
distribution and seeks to bound the probability of a given network failing
against the input. We focus on developing an eﬃcient robustness veriﬁcation
algorithm by extending a bound-propagation-based approach. Our proposed
algorithm improves upon the robustness certiﬁcate of this algorithm by up
to 8× while with no additional computational cost. In addition, we perform
a case study on incorporating the probabilistic robustness veriﬁcation during
training for the ﬁrst time."
INTRODUCTION,0.012195121951219513,"1
Introduction"
INTRODUCTION,0.018292682926829267,"Neural networks have found great success in a wide variety of applications. For many of
these applications, understanding when and how a neural network can fail is crucial. Szegedy
et al. (2014) found that almost visually imperceptible perturbations of input images could
drastically change the output of a neural network. This realization set oﬀa large area of
research, both in ﬁnding ways of attacking neural networks and developing defense and
certiﬁcation methods against said attacks. The most common setting which is considered
is the worst-case robustness given an lp norm. There have been a number of adversarial
robustness certiﬁcation methods which operate by ﬁnding provable lp balls for which the
output of the neural network is guaranteed to be constant. In other words, they ﬁnd lower
bounds on the minimum lp radius for which an adversarial attack exists. This is done
by relaxing the network for a bounded domain. Convex relaxations are typically used
(Salman et al., 2019), although some works also consider quadratic program formulations
(Raghunathan et al., 2018). Note that these are input-speciﬁc, so they do not give guarantees
about the entire input domain."
INTRODUCTION,0.024390243902439025,"As opposed to the worst-case adversarial robustness, there is also great interest in the threat
model where the primary concern is natural noise and corruption rather than a malicious
adversary and thus the perturbations follow some probability distribution. Typical neural
networks have been found to be vulnerable to common corruptions (Dodge & Karam, 2016;
Geirhos et al., 2018). We distinguish between these two notions of robustness as adversarial
robustness and probabilistic robustness. We note that the terms such as corruption robustness,
probability of violation, and adversarial density have also been used to refer to the general
concept of probabilistic robustness. For the threat model of natural noise and corruption,
there has not been as much work developed as for worst-case adversarial robustness. One
of the very ﬁrst probabilistic veriﬁcation algorithms without imposing assumptions on the
decision boundary is known as the PROVEN algorithm (Weng et al., 2019). In PROVEN,
the authors derive probabilistic veriﬁcation bounds based on existing worst-case robustness
veriﬁcation such as Fast-Lin and CROWN (Weng et al., 2018; Zhang et al., 2018)."
INTRODUCTION,0.03048780487804878,"Contributions. In this work, we generalize the PROVEN algorithm proposed by Weng
et al. (2019) to inﬁnite support and greatly improve the tightness of the robustness certiﬁcate
without additional computation cost. We name our algorithm I-PROVEN, as the proposed"
INTRODUCTION,0.036585365853658534,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.042682926829268296,"algorithm is a signiﬁcantly improved version of PROVEN algorithm. The I-PROVEN
algorithm can achieve signiﬁcant improvements (2×-8× tighter) on the tightness of prob-
abilistic robustness certiﬁcate for both MNIST and CIFAR-10 models without additional
computation cost, and it also enables the certiﬁcation of probability distributions with
inﬁnite support. Based on our proposed algorithm, we conduct a case study on augmenting
an existing training pipeline with probabilistic robustness veriﬁcation bounds, and we ﬁnd
mixed results for the training. We examine potential causes and implications."
BACKGROUND AND RELATED WORKS,0.04878048780487805,"2
Background and related works"
BACKGROUND AND RELATED WORKS,0.054878048780487805,"Notation. In order to describe related work in depth, we will lay out some notation.
We deﬁne a K-layer feed-forward ReLU neural network with n0 inputs and nK outputs
f : Rn0 →RnK as"
BACKGROUND AND RELATED WORKS,0.06097560975609756,f(x) = f (K)(x)
BACKGROUND AND RELATED WORKS,0.06707317073170732,f (i+1)(x) = W (i+1)σ(f (i)(x)) + b(i+1)
BACKGROUND AND RELATED WORKS,0.07317073170731707,f (1)(x) = W (1)x + b(1)
BACKGROUND AND RELATED WORKS,0.07926829268292683,"In other words, f (i)(x) denotes the vector of pre-activation values in the ith layer. Generally,
we work in the setting of image classiﬁers where a class c is classiﬁed over a class i if
fc(x) −fi(x) > 0. To simplify notation, we assume that the neural networks f which we
are working with have already had this margin function applied to it for some given c, i. In
other words, we assume nK = 1 for convenience and we are interested in when f(x) > 0."
ADVERSARIAL ROBUSTNESS VERIFICATION,0.08536585365853659,"2.1
Adversarial robustness verification"
ADVERSARIAL ROBUSTNESS VERIFICATION,0.09146341463414634,"Adversarial robustness veriﬁcation asks, given a neural network f and a region R(ϵ) in the
input space, does there exist an x ∈R(ϵ) such that f(x) ≤0? To solve this problem, we can
formulate it as an equivalent optimization problem: minx∈R(ϵ) f(x). If no such x such that
f(x) ≤0 exists, or equivalently, if the minimum f(x) is positive, then f is robust for region
R(ϵ). If we can prove that f is robust on regions R(ϵ) for all ϵ ≤ϵ, then the robustness
certiﬁcate is ϵ. The robustness certiﬁcate is a lower bound for the true minimum distortion
ϵ∗."
ADVERSARIAL ROBUSTNESS VERIFICATION,0.0975609756097561,"The regions R(ϵ) of general interest are Lp balls Bp(x0, ϵ) for a given image or input x0.
This arises from the interpretation that an adversary is perturbing x0 by at most ϵ under a
given Lp norm. Note that certiﬁcation only informs robustness about a single image. As far
as we know, it is infeasible to certify an entire dataset other than processing it image by
image."
ADVERSARIAL ROBUSTNESS VERIFICATION,0.10365853658536585,"Convex relaxation for provable veriﬁcation. These methods ﬁnd a convex relaxation
of a neural network in order to ﬁnd provable certiﬁcations for its adversarial robustness. We
will discuss these methods in detail as our method builds on them in certain ways. There
are a number of works following these methods (Weng et al., 2018; Singh et al., 2018; Zhang
et al., 2018; Singh et al., 2019b), and a general framework for them is described in (Salman
et al., 2019). We will use the setup used in CROWN (Zhang et al., 2018). In these methods,
inequalities on pre-activation neurons are recursively computed"
ADVERSARIAL ROBUSTNESS VERIFICATION,0.10975609756097561,"l(j) ≤A(j)
L x + b(j)
L ≤f (j)(x) ≤A(j)
U x + b(j)
U ≤u(j)"
ADVERSARIAL ROBUSTNESS VERIFICATION,0.11585365853658537,"for each layer j. Note that these are element-wise bounds, l(j) and u(j) are scalar vectors, and
A(j)
L , b(j)
L , A(j)
U , b(j)
U
are linear transformations of inputs x. These linear bounds are obtained
by relaxing the non-linear activation functions to linear lower and upper bounds given that
the inputs to the activation functions are within some interval found from the inequalities
applied to earlier layers, l(i), u(i), i < j. These inequalities are propagated backwards through
the network until the original input is reached. Under the typical lp ball threat model,
Hölder’s inequality can give scalar bounds on these layers and the process can continue to
the ﬁnal outputs. (Singh et al., 2019a; Tjandraatmadja et al., 2020) have made progress
on improving these bounds beyond the convex relaxation gap pointed out by Salman et al.
(2019) by considering the activation functions on multiple neurons jointly."
ADVERSARIAL ROBUSTNESS VERIFICATION,0.12195121951219512,Under review as a conference paper at ICLR 2022
PROBABILISTIC ROBUSTNESS,0.12804878048780488,"2.2
Probabilistic robustness"
PROBABILISTIC ROBUSTNESS,0.13414634146341464,"For probabilistic robustness, we are considering a known probability distribution D : Rn →
[0, 1] which the inputs x are sampled from. We will focus on additive iid uniform noise which
we denote, in an abuse of notation, as B∞(x, ϵ). In other words, B∞(x, ϵ) is the distribution
generated by sampling points evenly from the hyperrectangle [x1 −ϵ, x1 + ϵ] × [x2 −ϵ, x2 +
ϵ] × · · · × [xn −ϵ, xn + ϵ]."
PROBABILISTIC ROBUSTNESS,0.1402439024390244,Then the problem of probabilistic robustness veriﬁcation is to verify that
PROBABILISTIC ROBUSTNESS,0.14634146341463414,"Prx∼D [f(x) > 0] ≥1 −Q
(1)"
PROBABILISTIC ROBUSTNESS,0.1524390243902439,for some given failure probability Q.
PROBABILISTIC ROBUSTNESS,0.15853658536585366,"We can deﬁne the robustness certiﬁcate similarly to how it was done for adversarial robustness.
Weng et al. (2019) and Anderson & Sojoudi (2020) particularly consider the maximum ϵ
parameterizing D for which the above holds and we also provide such results. This is found
by binary searching over ϵ, as empirically we ﬁnd that the robustness is monotonic in ϵ for
the distributions we consider, but we note that there are no theoretical guarantees for this."
PROBABILISTIC ROBUSTNESS,0.16463414634146342,"Sampling methods. Sampling gives well-established statistical guarantees which can be
applied to the problem of probabilistic robustness. By using the neural network essentially as
a black-box, Chernoﬀbounds can estimate the probability of the neural network giving the
incorrect classiﬁcation given a distribution. This has the advantage of making no assumptions
on the model or the distribution, but requires a large number of samples to achieve a high
degree of accuracy and there is an inherent uncertainty present in the application of such an
algorithm. Baluta et al. (2021) notes for example, that proving that the probability is between
[0.1 −0.5 × 10−4, 0.1 + 0.5 × 10−4] with a conﬁdence of 0.9 would require 5.5 × 106 samples.
To overcome this, they propose a framework which reduces the number of samples necessary,
although this is dependent on the true probability. Anderson & Sojoudi (2020) also provide
a method that can ﬁnd upper bounds on the probability that a model is incorrect with a
small number of samples. Webb et al. (2018) uses a clever sampling method that leverages
the layered structure of common architectures. They require upwards of 107 samples but are
able to obtain precise estimations. Though they are unable to provide theoretical guarantees,
they show that empirically, their estimations agree with naive Monte Carlo estimates with
as many as 1010 samples."
TRAINING ADVERSARIALLY-ROBUST MODELS,0.17073170731707318,"2.3
Training adversarially-robust models"
TRAINING ADVERSARIALLY-ROBUST MODELS,0.17682926829268292,"Training methods for improving adversarial robustness have generally taken two paths. The
ﬁrst augments the data with adversarial attacks in order to strengthen a model’s resistance
to such attacks (Madry et al., 2017). The second approach adds loss regularization terms
that help the model learn robust features of the data. Xiao et al. (2018) identiﬁes weight
sparsity and ReLU stability as important factors in a model’s adversarial robustness and
builds a training framework which incorporates these. Other works use certiﬁcation methods
as regularization terms in order to improve the certiﬁable robustness of a model. Interval
bound propagation (IBP) has found great success in this despite being a relatively loose
certiﬁcation method (Gowal et al., 2019). In particular, the eﬃciency of IBP has made it
amenable to training. A number of other works have made progress in closing this eﬃciency
gap (Zhang et al., 2019; Xu et al., 2020; Shi et al., 2021; Boopathy et al., 2021)."
OUR MAIN RESULTS,0.18292682926829268,"3
Our main results"
OUR MAIN RESULTS,0.18902439024390244,"In section 3.1, we illustrate the idea of deriving tighter probabilistic robustness certiﬁcate
and provide the details of our I-PROVEN algorithm. We remark on alternative setups in
section 3.2. In section 3.3, we conduct a case study on including our proposed probabilistic
veriﬁcation bounds into standard training. All experimental results are reported in section 4."
OUR MAIN RESULTS,0.1951219512195122,Under review as a conference paper at ICLR 2022
OUR MAIN RESULTS,0.20121951219512196,"3.1
I-PROVEN: Improving the tightness of PROVEN algorithm"
OUR MAIN RESULTS,0.2073170731707317,"In this subsection, we will show how we could build on top of the state-of-the-art PROVEN
algorithm (Weng et al., 2019) to derive tighter probabilistic robustness certiﬁcate in I-
PROVEN. Using convex relaxation methods, one can ﬁnd linear bounds with respect to
the input such that,"
OUR MAIN RESULTS,0.21341463414634146,"A(K)
L
x + b(K)
L
≤f(x) ≤A(K)
U
x + b(K)
U
, ∀x ∈supp(D).
(2)"
OUR MAIN RESULTS,0.21951219512195122,PROVEN argues that by applying probabilistic bounds
OUR MAIN RESULTS,0.22560975609756098,"Pr
x∼D[A(K)
L
x + b(K)
L
≥0] ≥1 −q,
(3)"
OUR MAIN RESULTS,0.23170731707317074,"we can conclude that f(x) ≥0 with probability at least 1 −q.
Notably, probabilistic
inequalities are only considered in the ﬁnal layer. Thus, they are able to give a probabilistic
robustness certiﬁcation."
OUR MAIN RESULTS,0.23780487804878048,"To extend this method, we must look into how these linear bounds AL, bL, AU, bU were
obtained. This is done recursively: every layer’s pre-activation neurons are bounded by linear
bounds with respect to the input. Then, scalar bounds are obtained using Hölder’s inequality
on the linear bounds and the support of the input. These scalar bounds deﬁne intervals in
the domain of each activation function for the next layer. This allows linear bounds to be
calculated over the activation function and for the next layer to continue propagating linear
bounds."
OUR MAIN RESULTS,0.24390243902439024,"The key observation is that these linear bounds in previous layers can also apply with some
probability rather than strictly and that the failure probabilities accumulate linearly. Assume
that we have some q′ for which"
OUR MAIN RESULTS,0.25,"Pr
x∼D[f(x) ≥A(K)
L
x + b(K)
L
] ≥1 −q′.
(4)"
OUR MAIN RESULTS,0.25609756097560976,"Then a simple union bound gives that the probability of f(x) ≥0 is at least 1 −q −q′. This
is a simple scenario which can be extended to involve all layers of the network.
Theorem 1. In the convex-relaxation framework, for each scalar inequality i (l ≤ALx + bL
or u ≥AUx + bU), denote qi as the probability that this inequality is violated with respect
to the probability distribution D which x is sampled from. Then the probability of the ﬁnal
output of the convex-relaxation algorithm holding for a given x ∼D is ≥1 −P i qi."
OUR MAIN RESULTS,0.2621951219512195,"Proof. The convex-relaxation framework operates by making a series of m inequalities which
ultimately lead to the output layer. We can label these L1, L2, . . . , Lm. When we apply
probabilistic bounds, these inequalities may not be guaranteed. We will denote Ej to be
the event that Lj is correct. Even though there is the chance of failure, we will operate
assuming that all inequalities were correct. If they are indeed all correct for some x, then we
can conclude that f(x) ≥l(K) for this particular x as expected. Then it suﬃces to ﬁnd a
lower bound on the probability of the intersection of the Ej’s. We have"
OUR MAIN RESULTS,0.2682926829268293,"Pr
x∼D"
OUR MAIN RESULTS,0.27439024390243905,"h
f(x) ≥l(K)i
≥Pr
x∼D  \ j
Ej "
OUR MAIN RESULTS,0.2804878048780488,"= 1−Pr
x∼D  [ j Ej "
OUR MAIN RESULTS,0.2865853658536585,"≥1−
X"
OUR MAIN RESULTS,0.2926829268292683,"j
Pr
x∼D
"
OUR MAIN RESULTS,0.29878048780487804,"Ej

= 1−
X"
OUR MAIN RESULTS,0.3048780487804878,"j
qj (5)"
OUR MAIN RESULTS,0.31097560975609756,as desired.
OUR MAIN RESULTS,0.3170731707317073,"Now we will take advantage of this theorem.
Assign every scalar inequality i (either
ALx + bL ≥l or AUx + bU ≤u) a failure probability qi which sum to Q altogether. We can
invert the probabilistic inequalities to ﬁnd scalars l and u such that these hold with given
failure probability qi. In particular, say that functions γL(a, b) and γU(a, b) are such that"
OUR MAIN RESULTS,0.3231707317073171,"γL(a, b) ≤Pr
x∼D[ax + b > 0] ≤γU(a, b)"
OUR MAIN RESULTS,0.32926829268292684,"for any a, b. We want to choose l such that Pr[ALx + bL < l] ≤qi. Thus, it suﬃces to solve
for l such that γL(AL, bL −l) = 1 −qi."
OUR MAIN RESULTS,0.3353658536585366,Under review as a conference paper at ICLR 2022
OUR MAIN RESULTS,0.34146341463414637,"Algorithm 1 I-PROVEN for B∞(x, ϵ)"
OUR MAIN RESULTS,0.3475609756097561,"1: procedure PrIneq(x, ϵ, AL, bL, AU, bU, q)
2:
lstrict →ALx + bL −ϵ||AL||1
3:
ustrict →AUx + bU + ϵ||AU||1"
OUR MAIN RESULTS,0.35365853658536583,"4:
lprob →ALx + bL −ϵ
q"
OUR MAIN RESULTS,0.3597560975609756,2 ln 1
OUR MAIN RESULTS,0.36585365853658536,q ||AL||2
OUR MAIN RESULTS,0.3719512195121951,"5:
uprob →AUx + bU + ϵ
q"
OUR MAIN RESULTS,0.3780487804878049,2 ln 1
OUR MAIN RESULTS,0.38414634146341464,q ||AU||2
OUR MAIN RESULTS,0.3902439024390244,"6:
return max(lstrict, lprob), min(ustrict, uprob)
7: end procedure
8: procedure IPROVEN(x, ϵ, f, Q)
9:
q →Q/(2 × number of neurons)
10:
l(1), u(1) →PrIneq(x, ϵ, W1, b1, W1, b1, q)
11:
for i in [2, K] do"
OUR MAIN RESULTS,0.39634146341463417,"12:
A(i)
L , b(i)
L , A(i)
U , b(i)
U →GetLinearBounds(l(:i), u(:i), A(:i)
L , b(:i)
L , A(:i)
U , b(:i)
U )"
OUR MAIN RESULTS,0.4024390243902439,"13:
l(i), u(i) →PrIneq(x, ϵ, A(i)
L , b(i)
L , A(i)
U , b(i)
U , q)
14:
end for
15:
return l(n)"
OUR MAIN RESULTS,0.40853658536585363,16: end procedure
OUR MAIN RESULTS,0.4146341463414634,"For the uniform distribution B∞(x, ϵ), such functions γL and γU can be derived from
Hoeﬀding’s Inequality as seen in Corollary 3.2 of (Weng et al., 2019). Inverting them, we
obtain"
OUR MAIN RESULTS,0.42073170731707316,"l = ALx + bL −ϵ
r"
OUR MAIN RESULTS,0.4268292682926829,2 ln 1
OUR MAIN RESULTS,0.4329268292682927,"qi
||AL||2.
(6)"
OUR MAIN RESULTS,0.43902439024390244,"Performing the forward and backward bound propagation methods ultimately yields a ﬁnal
scalar lower bound l(n). By theorem 1, we have that"
OUR MAIN RESULTS,0.4451219512195122,"Pr
x∼D[f(x) ≥l(n)] ≥1 −
X"
OUR MAIN RESULTS,0.45121951219512196,"i
qi = 1 −Q."
OUR MAIN RESULTS,0.4573170731707317,"We choose a simple scheme in assigning the failure probabilities. We select some subset of
layers S and assign all inequalities pertaining to these layers equal qi’s. All other qi’s are set
to 0. So qi ="
OUR MAIN RESULTS,0.4634146341463415,"(
Q
2× number of neurons within layers of S
if i ∈S
0
if i ̸∈S
(7)"
OUR MAIN RESULTS,0.4695121951219512,"qi = 0 indicates using the strict bounds and is only possible when D has bounded support,
as with B∞(x, ϵ). In this case, the Hölder inequality for the l∞norm is used. Note that the
original PROVEN algorithm is equivalent to only selecting the last layer."
OUR MAIN RESULTS,0.47560975609756095,"We found that more complex assignment strategies such as optimizing over q with sum
equal to 1 did not lend themselves to any signiﬁcant improvements compared to this simple
method, especially given their additional run-time cost. However, there are a few factors to
keep in mind when choosing the subset of layers S. In the case of uniform bounded noise, if
the non-zero qi are small, it is possible for the strict inequalities, which ﬁnd"
OUR MAIN RESULTS,0.4817073170731707,"lstrict = ALx + bL −ϵ||AL||1,
ustrict = AUx + bU + ϵ||AU||1
(8)"
OUR MAIN RESULTS,0.4878048780487805,to give tighter intervals than the probabilistic inequalities.
OUR MAIN RESULTS,0.49390243902439024,"The diﬀerent weight norms also create some discrepancy in the eﬀectiveness of the probabilistic
bounds. In particular, although the matrices A in the above equations are not directly
the network weights, we found that I-PROVEN performed worse on models with sparse
weights. To take these into account, we return the tightest intervals using either the strict or
probabilistic bounds, although we do not update the q’s to incorporate our choice."
OUR MAIN RESULTS,0.5,Under review as a conference paper at ICLR 2022
OTHER DISTRIBUTIONS AND CERTIFIERS,0.5060975609756098,"3.2
Other distributions and certifiers"
OTHER DISTRIBUTIONS AND CERTIFIERS,0.5121951219512195,"I-PROVEN can support distributions with inﬁnite support.
The only requirement
which I-PROVEN has on the distribution D is that we must have lower and upper bounds
on Prx∼D[ax + b > 0] for a, b ∈R. This may include distributions with inﬁnite support. For
example, for additive iid Gaussian noise with standard deviation ϵ, we can obtain"
OTHER DISTRIBUTIONS AND CERTIFIERS,0.5182926829268293,"l = ALx + bL −ϵ erf−1(1 −2qi) ||AL||2
(9)"
OTHER DISTRIBUTIONS AND CERTIFIERS,0.524390243902439,"from basic facts about Gaussian distributions. Similar formulas can be found for Gaussian
mixtures when such a distribution is known and relevant. Note that qi must be non-zero for
all inequalities when dealing with distributions with inﬁnite support, as strict alternatives
no longer exist."
OTHER DISTRIBUTIONS AND CERTIFIERS,0.5304878048780488,"I-PROVEN can be used with any linear-relaxation-based certiﬁer with no addi-
tional computational cost.
As I-PROVEN only requires changing the evaluation of
the scalar bounds, it has no additional time complexity cost to whatever method it is being
used with. It can be incorporated in any such linear-relaxation method. For a network
with K layers and n neurons at each layer, the entire method when used with CROWN is
O(n3K2) (Zhang et al., 2018), an additional factor of K compared to a pass of the network."
PROBABILISTIC VERIFICATION BASED TRAINING,0.5365853658536586,"3.3
Probabilistic verification based training"
PROBABILISTIC VERIFICATION BASED TRAINING,0.5426829268292683,"Our training method is simply a substitution of PROVEN into CROWN-IBP (Zhang et al.,
2019). It requires two forward passes. The ﬁrst computes strict bounds on each pre-activation
neuron using IBP. The second performs linear bound propagations to compute linear bounds
on the output. Then probabilistic bounds are applied to obtain the ﬁnal scalar lower and
upper bounds which are used in the loss function as described in CROWN-IBP’s original
paper. Note that this eﬀectively means that only the original PROVEN is applied in this
training method."
EXPERIMENTS,0.5487804878048781,"4
Experiments"
IMPLEMENTATION DETAILS,0.5548780487804879,"4.1
Implementation details"
IMPLEMENTATION DETAILS,0.5609756097560976,"We performed experiments on the MNIST and CIFAR-10 datasets (LeCun et al., 2010;"
IMPLEMENTATION DETAILS,0.5670731707317073,"Krizhevsky, 2009). For our veriﬁcation experiments, we used pretrained models provided in
(Weng et al., 2018) and (Zhang et al., 2018) and the images from each dataset were scaled so
that their values were in [−0.5, 0.5]. For our training experiments, we used the architectures
described in (Gowal et al., 2019) and the images in the dataset were scaled to be in [0, 1].
Our MNIST models were trained for 100 epochs with a batch size of 100 and with a warm-up
period for the ﬁrst 5 epochs and a ramp-up period for the next 45. We followed the β and κ
schedule used in (Zhang et al., 2019) for the PROVEN-IBP loss and we increase our ϵ from
0 to 4 × 10−1 during the ramp-up period. We evaluate the validation set using ϵ = 3 × 10−1.
All code was written in Python with the use of the PyTorch library (Paszke et al., 2019) and
training was conducted on a NVIDIA Tesla V100 GPU."
VERIFICATION RESULTS,0.573170731707317,"4.2
Verification results"
VERIFICATION RESULTS,0.5792682926829268,"We compare the original PROVEN bounds with our improved PROVEN on various MNIST
and CIFAR-10 classiﬁer in table 1. The classiﬁer are k-layer MLPs with n neurons in each
layer, denoted as k × [n]. In I-PROVEN, qi are all non-zero and equal across all layers. It
achieves much better performance than the original PROVEN (Weng et al., 2019) with a
600% increase for 99.99% probabilistic robustness. Note that Q = 0 is simply the adversarial
robustness certiﬁcate."
VERIFICATION RESULTS,0.5853658536585366,"I-PROVEN’s improvement over PROVEN can be observed from intermediate layers’ bounds.
In table 2, the average interval gap of the neurons in a layer is tabulated. The model in
particular is a 4-layer MNIST MLP (layer 0 indicates the input, which is why the interval
length is simply 2ϵ). PROVEN and I-PROVEN are each evaluated on 10 images for various"
VERIFICATION RESULTS,0.5914634146341463,Under review as a conference paper at ICLR 2022
VERIFICATION RESULTS,0.5975609756097561,"Table 1: I-PROVEN versus original PROVEN for probabilistic robustness certiﬁcate on
MNIST and CIFAR-10 models. I-PROVEN achieve signiﬁcant improvements (1.6×-8.5×)
on the tightness of probabilistic robustness certiﬁcate without additional computation cost."
VERIFICATION RESULTS,0.6036585365853658,"Q
0
0.0001
0.01
0.05
0.25
0.5"
VERIFICATION RESULTS,0.6097560975609756,"MNIST
2 × [1024]
PROVEN
0.0247
0.0325
0.0333
0.0337
0.0342
0.0345
I-PROVEN
0.0247
0.0864
0.1013
0.1087
0.1181
0.1229"
VERIFICATION RESULTS,0.6158536585365854,"% improvement
0
166
204
223
245
256"
VERIFICATION RESULTS,0.6219512195121951,"MNIST
3 × [1024]
PROVEN
0.0194
0.0223
0.0226
0.0227
0.0228
0.0229
I-PROVEN
0.0194
0.0640
0.0745
0.0797
0.0860
0.0893"
VERIFICATION RESULTS,0.6280487804878049,"% improvement
0
187
230
251
277
289"
VERIFICATION RESULTS,0.6341463414634146,"MNIST
4 × [1024]
PROVEN
0.0079
0.0083
0.0084
0.0084
0.0084
0.0084
I-PROVEN
0.0079
0.0299
0.0347
0.0370
0.0398
0.0413"
VERIFICATION RESULTS,0.6402439024390244,"% improvement
0
259
315
342
374
391"
VERIFICATION RESULTS,0.6463414634146342,"CIFAR-10
5 × [2048]
PROVEN
0.0017
0.0018
0.0018
0.0018
0.0018
0.0018
I-PROVEN
0.0017
0.0123
0.0141
0.0150
0.0161
0.0166"
VERIFICATION RESULTS,0.6524390243902439,"% improvement
0
574
673
720
776
804"
VERIFICATION RESULTS,0.6585365853658537,"CIFAR-10
7 × [1024]
PROVEN
0.0018
0.0018
0.0018
0.0018
0.0018
0.0018
I-PROVEN
0.0018
0.0128
0.0147
0.0157
0.0168
0.0174"
VERIFICATION RESULTS,0.6646341463414634,"% improvement
0
606
716
764
826
857"
VERIFICATION RESULTS,0.6707317073170732,"Figure 1: I-PROVEN veriﬁcation results for a small 3-layer MNIST model considering
the uniform distribution B∞(x, ϵ) and isotropic Gaussian distribution N(x, ϵ), respectively.
Error bars are based on 3× stdev."
VERIFICATION RESULTS,0.676829268292683,"ϵ, Q. The scalar intervals l, u which give bounds on each neuron in both methods are averaged
within each layer. The smaller the interval size (that is, the tighter the interval), the better.
I-PROVEN’s intervals are noticeably tighter from the ﬁrst layer onwards as it invests a
portion of the total failure probability Q to tighten these earlier inequalities. This does
mean PROVEN’s ﬁnal inequalities are sharper than I-PROVEN’s, which we can notice by
observing the ratio of interval widths between Layer 3 and Layer 4. For ϵ = 0.001, Q = 0.1
for example, PROVEN goes from 0.045 to 0.564, a 12.5× increase, while I-PROVEN goes
from 0.09 to 0.203, a 22.5× increase. However, the improvements I-PROVEN makes in
the earlier layers means it still ends up with tighter ﬁnal bounds than PROVEN."
VERIFICATION RESULTS,0.6829268292682927,"Note that we can our method is also able to handle additive Gaussian noise with only small
changes to our probabilistic inequalities. Notably, we do not need to truncate the Gaussian
distribution as (Weng et al., 2019) did. We plot our results for both additive uniform noise
and additive Gaussian noise in ﬁg. 1."
VERIFICATION RESULTS,0.6890243902439024,Under review as a conference paper at ICLR 2022
VERIFICATION RESULTS,0.6951219512195121,Table 2: Average interval lengths per layer
VERIFICATION RESULTS,0.7012195121951219,"Layer 0
Layer 1
Layer 2
Layer 3
Layer 4"
VERIFICATION RESULTS,0.7073170731707317,"ϵ = 0.001, Q = 0.01
PROVEN
0.002
0.049
0.043
0.045
0.564
I-PROVEN
0.002
0.011
0.009
0.009
0.203"
VERIFICATION RESULTS,0.7134146341463414,"ϵ = 0.005, Q = 0.01
PROVEN
0.010
0.247
0.324
0.522
7.478
I-PROVEN
0.010
0.054
0.052
0.055
1.146"
VERIFICATION RESULTS,0.7195121951219512,"ϵ = 0.001, Q = 0.0001
PROVEN
0.002
0.049
0.043
0.045
0.588
I-PROVEN
0.002
0.013
0.011
0.011
0.237"
COMPARISON TO OTHER METHODS,0.725609756097561,"4.3
Comparison to other methods"
COMPARISON TO OTHER METHODS,0.7317073170731707,"In the earlier section, we compared I-PROVEN to PROVEN in terms of their robustness
certiﬁcates. Now, we will show comparisons to IBP and a simple Monte Carlo approach
based on (Anderson & Sojoudi, 2020). IBP does not generally perform well for arbitrary
networks, so we use IBP-trained models in our experiment. In particular, we trained three
CIFAR-10 CNN models A, B, and C. Model A was trained with standard loss, Model B was
trained with an IBP loss term with ϵ ramping up to 2.2/255, and Model C was trained with
an IBP loss term with ϵ ramping up to 8.8/255. We use two diﬀerent ϵ, 2/255 and 8/255,
for each model and certiﬁer. Q is ﬁxed at 1%. We apply these certiﬁers on 1000 images from
the validation dataset."
COMPARISON TO OTHER METHODS,0.7378048780487805,"For the Monte Carlo approach, we take T ln 1/u samples from the uniform B∞(x, ϵ) distri-
bution where u = 0.0001 and T = 100, rounding to 921 samples per image x. Then if every
sample is correctly classiﬁed, we conclude that at least 1 −1/T = 99% of the distribution is
correctly classiﬁed with false positive rate less than u. This Monte Carlo approach has a
higher false negative rate than other sampling approaches, but we chose this one beecause it
requires the lowest number of samples as far as we are aware."
COMPARISON TO OTHER METHODS,0.7439024390243902,"Table 3: IBP, PROVEN, I-PROVEN, and the Monte Carlo method on a CIFAR-10 classiﬁer.
The entries indicate the portion of images which each certiﬁer is unable to certify as robust
(lower is better). Standard (error) denotes the portion of images which is classiﬁed incorrectly
without any perturbations."
COMPARISON TO OTHER METHODS,0.75,"IBP
PROVEN
I-PROVEN
Monte Carlo
Standard"
COMPARISON TO OTHER METHODS,0.7560975609756098,"ϵ = 2/255
Model A
1.000
0.996
0.808
0.429
0.409
Model B
0.715
0.643
0.598
0.511
0.483
Model C
0.621
0.604
0.613
0.576
0.552"
COMPARISON TO OTHER METHODS,0.7621951219512195,"ϵ = 8/255
Model A
1.000
1.000
1.000
0.491
0.409
Model B
0.993
0.978
0.952
0.588
0.483
Model C
0.806
0.797
0.801
0.634
0.552"
COMPARISON TO OTHER METHODS,0.7682926829268293,"As the results in table 3 show, I-PROVEN performs better than IBP across all models, but
this gap diminishes greatly in the two IBP-trained models, particularly for Model C. Similarly,
we see the gap between PROVEN and I-PROVEN close and PROVEN even outperforms
I-PROVEN in Model C. We see a similar phenomena in the next section, section 4.4, and
provide an explanation."
COMPARISON TO OTHER METHODS,0.774390243902439,"Unsurprisingly, the Monte Carlo approach obtains better results than any of the relaxation-
based approaches. Furthermore, in terms of timing, PROVEN/I-PROVEN took 128-131s
for all 1000 images per model, while the Monte Carlo approach consistently took around 4s.
Evaluating the standard error and IBP error took under a second. However, the exact details
on timing do depend somewhat on the situation. In this experiment, I-PROVEN could
not be batched at all as the memory used was too expensive, while Monte Carlo methods
can easily be batched."
COMPARISON TO OTHER METHODS,0.7804878048780488,Under review as a conference paper at ICLR 2022
A CASE STUDY ON TRAINING WITH I-PROVEN,0.7865853658536586,"4.4
A case study on training with I-PROVEN"
A CASE STUDY ON TRAINING WITH I-PROVEN,0.7926829268292683,"We examine I-PROVEN’s veriﬁcation compared with IBP’s on models trained with IBP and
PROVEN-IBP, respectively, on the small CNN from (Gowal et al., 2019) and the veriﬁcation
algorithms are considering B∞(x, 0.3). Note that IBP is considering the model’s adversarial
robustness while I-PROVEN is considering the probabilistic robustness for Q = 1 × 10−2
(equivalently, for 99% of the ball). We found that I-PROVEN does not obtain signiﬁcantly
better results than IBP in either case, mirroring our results on the IBP-trained models in
table 3. We conjecture that this is due to the weight sparsity induced by IBP’s involvement
in the training for both pure IBP training and PROVEN-IBP, and that this sparsity is also
present in the linear bounds for I-PROVEN. Weight sparsity is beneﬁcial for adversarial
robustness, particularly for the l∞norm (Xiao et al., 2018). However, as far as we are aware,
there is no reason to expect weight sparsity to help a model’s probabilistic robustness and as
noted in section 3.1, I-PROVEN’s probabilistic inequalities prefer more evenly distributed
matrices."
A CASE STUDY ON TRAINING WITH I-PROVEN,0.7987804878048781,"This weight sparsity also explains why PROVEN outperforms I-PROVEN in Model C
of table 3. When the model weights are sparse, the strict inequality is tighter than the
probabilistic inequalities for very small qi and so I-PROVEN does not perform as well as
usual by distributing Q evenly among the inequalities."
A CASE STUDY ON TRAINING WITH I-PROVEN,0.8048780487804879,"To test our hypothesis, we compared the last layer’s linear lower bounds AL between a CNN
trained in a standard manner, and a CNN trained with an IBP loss as in (Gowal et al.,
2019). We show this for an image from the validation set in ﬁg. 2. These AL’s came from
I-PROVEN applied with ϵ = 0.3, Q = 1 × 10−2, and the failure probabilities in the last
two layers non-zero and equal. Each 28 × 28 grid corresponds to a logit in the output. The
grid for 3 is all 0 as this is speciﬁcally considering the margin function between each class
with the true class, 3. The absolute values of the matrix are scaled to ﬁt in [0, 1]. Evidently,
the values from the IBP model are far more sparse. Further examples are included in the
appendix."
A CASE STUDY ON TRAINING WITH I-PROVEN,0.8109756097560976,"Figure 2: AL from a CNN trained in a standard manner (top row), and a CNN trained with
IBP respectively (bottom roow)."
CONCLUSION,0.8170731707317073,"5
Conclusion"
CONCLUSION,0.823170731707317,"In this paper, we present I-PROVEN, an algorithm that can eﬃciently verify the prob-
abilistic robustness of a neural network. We show strong improvements compared to the
prior method used against this problem: we remove the assumptions of bounded support and
signiﬁcantly improve the tightness of robustness certiﬁcate without any additional cost. Fur-
thermore, we present a training framework for probabilistic robustness and demonstrate its
shortcomings. By taking a closer look at these results, we make steps towards understanding
the relation between adversarial certiﬁed defense methods and our own."
REFERENCES,0.8292682926829268,References
REFERENCES,0.8353658536585366,"Brendon G Anderson and Somayeh Sojoudi. Certifying neural network robustness to random
input noise from samples. arXiv preprint arXiv:2010.07532, 2020. 3, 8"
REFERENCES,0.8414634146341463,Under review as a conference paper at ICLR 2022
REFERENCES,0.8475609756097561,"Teodora Baluta, Zheng Leong Chua, Kuldeep S Meel, and Prateek Saxena.
Scalable
quantitative veriﬁcation for deep neural networks. In 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE), pp. 312–323. IEEE, 2021. 3"
REFERENCES,0.8536585365853658,"Akhilan Boopathy, Tsui-Wei Weng, Sijia Liu, Pin-Yu Chen, Gaoyuan Zhang, and Luca
Daniel. Fast training of provably robust neural networks by singleprop. arXiv preprint
arXiv:2102.01208, 2021. 3"
REFERENCES,0.8597560975609756,"Samuel Dodge and Lina Karam. Understanding how image quality aﬀects deep neural
networks. In 2016 eighth international conference on quality of multimedia experience
(QoMEX), pp. 1–6. IEEE, 2016. 1"
REFERENCES,0.8658536585365854,"Robert Geirhos, Carlos R Medina Temme, Jonas Rauber, Heiko H Schütt, Matthias Bethge,
and Felix A Wichmann. Generalisation in humans and deep neural networks. arXiv
preprint arXiv:1808.08750, 2018. 1"
REFERENCES,0.8719512195121951,"Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin,
Jonathan Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. Scalable
veriﬁed training for provably robust image classiﬁcation. ICCV, 2019. 3, 6, 9"
REFERENCES,0.8780487804878049,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report,
2009. 6"
REFERENCES,0.8841463414634146,"Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs
[Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. 6"
REFERENCES,0.8902439024390244,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083, 2017. 3"
REFERENCES,0.8963414634146342,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-
gory
Chanan,
Trevor
Killeen,
Zeming
Lin,
Natalia
Gimelshein,
Luca
Antiga,
Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala.
Pytorch:
An imperative style, high-performance deep learning
library.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.
8024–8035. Curran Associates, Inc., 2019.
URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf. 6"
REFERENCES,0.9024390243902439,"Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semideﬁnite relaxations for certifying
robustness to adversarial examples. arXiv preprint arXiv:1811.01057, 2018. 1"
REFERENCES,0.9085365853658537,"Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex
relaxation barrier to tight robustness veriﬁcation of neural networks. arXiv preprint
arXiv:1902.08722, 2019. 1, 2"
REFERENCES,0.9146341463414634,"Zhouxing Shi, Yihan Wang, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Fast certiﬁed robust
training via better initialization and shorter warmup. arXiv preprint arXiv:2103.17268,
2021. 3"
REFERENCES,0.9207317073170732,"Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Martin T Vechev.
Fast and eﬀective robustness certiﬁcation. NeurIPS, 1(4):6, 2018. 2"
REFERENCES,0.926829268292683,"Gagandeep Singh, Rupanshu Ganvir, Markus Püschel, and Martin Vechev. Beyond the single
neuron convex barrier for neural network certiﬁcation. Advances in Neural Information
Processing Systems, 32:15098–15109, 2019a. 2"
REFERENCES,0.9329268292682927,"Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. An abstract domain
for certifying neural networks. Proceedings of the ACM on Programming Languages, 3
(POPL):1–30, 2019b. 2"
REFERENCES,0.9390243902439024,Under review as a conference paper at ICLR 2022
REFERENCES,0.9451219512195121,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus. Intriguing properties of neural networks. ICLR, 2014. 1"
REFERENCES,0.9512195121951219,"Christian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Patel, and
Juan Pablo Vielma. The convex relaxation barrier, revisited: Tightened single-neuron
relaxations for neural network veriﬁcation. arXiv preprint arXiv:2006.14076, 2020. 2"
REFERENCES,0.9573170731707317,"Stefan Webb, Tom Rainforth, Yee Whye Teh, and M Pawan Kumar. A statistical approach
to assessing neural network robustness. arXiv preprint arXiv:1811.07209, 2018. 3"
REFERENCES,0.9634146341463414,"Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane
Boning, and Inderjit Dhillon. Towards fast computation of certiﬁed robustness for relu
networks. In International Conference on Machine Learning, pp. 5276–5285. PMLR, 2018.
1, 2, 6"
REFERENCES,0.9695121951219512,"Lily Weng, Pin-Yu Chen, Lam Nguyen, Mark Squillante, Akhilan Boopathy, Ivan Oseledets,
and Luca Daniel. Proven: Verifying robustness of neural networks with a probabilistic
approach. In International Conference on Machine Learning, pp. 6727–6736. PMLR, 2019.
1, 3, 4, 5, 6, 7"
REFERENCES,0.975609756097561,"Kai Y Xiao, Vincent Tjeng, Nur Muhammad Shaﬁullah, and Aleksander Madry. Training
for faster adversarial robustness veriﬁcation via inducing relu stability. arXiv preprint
arXiv:1809.03008, 2018. 3, 9"
REFERENCES,0.9817073170731707,"Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya
Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable
certiﬁed robustness and beyond. Advances in Neural Information Processing Systems, 33,
2020. 3"
REFERENCES,0.9878048780487805,"Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Eﬃcient
neural network robustness certiﬁcation with general activation functions. arXiv preprint
arXiv:1811.00866, 2018. 1, 2, 6"
REFERENCES,0.9939024390243902,"Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane
Boning, and Cho-Jui Hsieh. Towards stable and eﬃcient training of veriﬁably robust
neural networks. arXiv preprint arXiv:1906.06316, 2019. 3, 6"
