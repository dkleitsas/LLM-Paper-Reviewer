Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0021008403361344537,"The works of (Daskalakis et al., 2009, 2022; Jin et al., 2022; Deng et al., 2023)
indicate that computing Nash equilibria in multi-player Markov games is a compu-
tationally hard task. This fact raises the question of whether or not computational
intractability can be circumvented if one focuses on specific classes of Markov
games. One such example is two-player zero-sum Markov games, in which ef-
ficient ways to compute a Nash equilibrium are known. Inspired by zero-sum
polymatrix normal-form games (Cai et al., 2016), we define a class of zero-sum
multi-agent Markov games in which there are only pairwise interactions described
by a graph that changes per state. For this class of Markov games, we show that an
ϵ-approximate Nash equilibrium can be found efficiently. To do so, we generalize
the techniques of (Cai et al., 2016), by showing that the set of coarse-correlated
equilibria collapses to the set of Nash equilibria. Afterwards, it is possible to
use any algorithm in the literature that computes approximate coarse-correlated
equilibria Markovian policies to get an approximate Nash equilibrium."
INTRODUCTION,0.004201680672268907,"1
Introduction"
INTRODUCTION,0.0063025210084033615,"Multi-agent reinforcement learning (MARL) is the discipline that is concerned with strategic interac-
tions between agents who find themselves in a dynamically changing environment. Early aspects of
MARL can be traced back to, as early as, the initial text on two-player zero-sum stochastic/Markov
games (Shapley, 1953). Today, Markov games have been established as the theoretical framework for
MARL (Littman, 1994). The connection between game theory and MARL has lead to several recent
cornerstone results in benchmark domains in AI (Bowling et al., 2015; Brown and Sandholm, 2019,
2018; Brown et al., 2020; Silver et al., 2017; Moravˇcík et al., 2017; Perolat et al., 2022; Vinyals et al.,
2019). The majority of the aforementioned breakthroughs relied on computing Nash equilibria (Nash,
1951) in a scalable and often decentralized manner. Although the theory of single agent reinforcement
learning (RL) has witnessed an outstanding progress (e.g., see (Agarwal et al., 2020; Bertsekas, 2000;
Jin et al., 2018; Li et al., 2021; Luo et al., 2019; Panait and Luke, 2005; Sidford et al., 2018; Sutton
and Barto, 2018), and references therein), the landscape of multi-agent settings eludes a thorough
understanding. In fact, guarantees for provably efficient computation of Nash equilibria remain
limited to either environments in which agents strive to coordinate towards a shared goal (Chen
et al., 2022; Claus and Boutilier, 1998; Ding et al., 2022; Fox et al., 2022; Leonardos et al., 2021;
Maheshwari et al., 2022; Wang and Sandholm, 2002; Zhang et al., 2021) or fully competitive such as
two-player zero-sum games (Cen et al., 2021; Condon, 1993; Daskalakis et al., 2020; Sayin et al.,
2021, 2020; Wei et al., 2021) to name a few. Part of the lack of efficient algorithmic results in MARL
is the fact that computing approximate Nash equilibria in (general-sum) games is computationally
intractable (Daskalakis et al., 2009; Rubinstein, 2017; Chen et al., 2009; Etessami and Yannakakis,
2010) even when the games have a single state, i.e., normal-form two-player games."
INTRODUCTION,0.008403361344537815,"We aim at providing a theoretical framework that captures an array of real-world applications with
multiple agents — which admittedly correspond to a big portion of all modern applications. A
recent contribution that computes NE efficiently in a setting that combines both collaboration and
competition, (Kalogiannis et al., 2022), concerns adversarial team Markov games, or competition
between an adversary and a group of uncoordinated agents with common rewards. Efficient algorithms
for computing Nash equilibria in settings that include both cooperation and competition are far fewer
and tend to impose assumptions that are restrictive and difficult to meet in most applications (Bowling,
2000; Hu and Wellman, 2003). The focus of our work is centered around the following question:"
INTRODUCTION,0.01050420168067227,"Are there any other settings of Markov games that encompass both competition and
coordination while mantaining the tractability of Nash equilibrium computation?
(⋆)"
INTRODUCTION,0.012605042016806723,"Inspired by contemporary works in algorithmic game theory and specifically zero-sum polymatrix
normal-form games (Cai et al., 2016), we focus on the problem of computing Nash equilibria in
zero-sum polymatrix Markov games. Informally, a polymatrix Markov game is a multi-agent Markov
decision process with n agents, state-space S, action space Ak for agent k, a transition probability
model P and is characterized by a graph Gs(V, Es) which is potentially different in every state s. For
a fixed state s, the nodes of the graph V correspond to the agents, and the edges Es of the graph are
two-player normal-form games (different per state). Every node/agent k has a fixed set of actions Ak,
and chooses a strategy from this set to play in all games corresponding to adjacent edges. Given an
action profile of all the players, the node’s reward is the sum of its rewards in all games on the edges
adjacent to it. The game is globally zero-sum if, for all strategy profiles, the rewards of all players
add up to zero. Afterwards, the process transitions to a state s′ according to P. In a more high-level
description, the agents interact over a network whose connections change at every state."
INTRODUCTION,0.014705882352941176,"Our results.
We consider a zero-sum polymatrix Markov game with the additional property that
a single agent (not necessarily the same) controls the transition at each state, i.e., the transition
model is affected by a single agent’s actions for each state s. These games are known as switching
controller Markov games. We show that we can compute in time poly(|S|, n, maxi∈[n] |Ai|, 1/ϵ)
an ϵ-approximate Nash equilibrium. The proof relies on the fact that zero-sum polymatrix Markov
games with a switching controller have the following important property: the marginals of a coarse-
correlated equilibrium constitute a Nash equilibrium (see Section 3.2). We refer to this phenomenon
as equilibrium collapse. This property was already known for zero-sum polymatrix normal-form
games by Cai et al. (2016) and our results generalize the aforementioned work for Markov games. As
a corollary, we get that any algorithm in the literature that guarantees convergence to approximate
coarse-correlated equilibria Markovian policies—e.g., (Daskalakis et al., 2022)—can be used to get
approximate Nash equilibria. Our contribution also unifies previous results that where otherwise only
applicable to the settings of single and switching-control two-player zero-sum games, or zero-sum
polymatrix normal-form games. Finally, we show that the equilibrium collapsing phenomenon does
not carry over if there are two or more controllers per state (see Section 3.3)."
INTRODUCTION,0.01680672268907563,"Technical overview.
In order to prove our results, we rely on nonlinear programming and, in
particular, nonlinear programs whose optima coincide with the Nash equilibria for a particular
Markov game (Filar et al., 1991; Filar and Vrieze, 2012). Our approach is analogous to the one used
by (Cai et al., 2016) which uses linear programming to prove the collapse of the set of CCE to the
set of NE. Nevertheless, using the duality of linear programming in our case is not possible since
a Markov game introduces nonlinear terms in the program. It is noteworthy that we do not need to
invoke (Lagrangian) duality or an argument that relies on stationary points of a Lagrangian function.
Rather, we use the structure of the zero-sum polymatrix Markov games with a switching controller to
conclude the relation between a correlated policy and the individual policies formed by its marginals
in terms of the individual utilities of the game."
IMPORTANCE OF ZERO-SUM POLYMATRIX MARKOV GAMES,0.018907563025210083,"1.1
Importance of zero-sum polymatrix Markov games"
IMPORTANCE OF ZERO-SUM POLYMATRIX MARKOV GAMES,0.02100840336134454,"Strategic interactions of agents over a network is a topic of research in multiple disciplines that
span computer science (Easley and Kleinberg, 2010), economics (Schweitzer et al., 2009), control
theory (Tipsuwan and Chow, 2003), and biology (Szabó and Fath, 2007) to name a few."
IMPORTANCE OF ZERO-SUM POLYMATRIX MARKOV GAMES,0.023109243697478993,"In many environments where multiple agents interact with each other, they do so in a localized manner.
That is, every agent is affected by the set of agents that belong to their immediate “neighborhood”.
Further, it is quite common that these agents will interact independently with each one of their"
IMPORTANCE OF ZERO-SUM POLYMATRIX MARKOV GAMES,0.025210084033613446,"neighbors; meaning that the outcome of their total interactions is a sum of pairwise interactions rather
than interactions that depend on joint actions. Finally, players might remain indifferent to actions of
players are not their neighbors."
IMPORTANCE OF ZERO-SUM POLYMATRIX MARKOV GAMES,0.0273109243697479,"To illustrate this phenomenon we can think of multiplayer e-games (e.g., CS:GO, Fortnite, League
of Legends, etc) where each player interacts through the same move only with players that are
present on their premises and, in general, the neighbors cannot combine their actions into something
that is not a mere sum of their individual actions (i.e., they rarely can “multiply” the effect of the
individual actions). In other scenarios, such as strategic games played on social networks (e.g.,
opinion dynamics) agents clearly interact in a pairwise manner with agents that belong to their
neighborhood and are somewhat oblivious to the actions of agents who they do not share a connection
with."
IMPORTANCE OF ZERO-SUM POLYMATRIX MARKOV GAMES,0.029411764705882353,"With the proposed model we provide the theoretical framework needed to reason about such strategic
interactions over dynamically changing networks."
RELATED WORK,0.031512605042016806,"1.2
Related work"
RELATED WORK,0.03361344537815126,"From the literature of Markov games, we recognize the settings of single controller (Filar and Ragha-
van, 1984; Sayin et al., 2022; Guan et al., 2016; Qiu et al., 2021) and switching controller (Vrieze
et al., 1983) Markov games to be one of the most related to ours. In these settings, all agents’ actions
affect individual rewards, but in every state one particular player (single controller), or respectively
a potentially different one (switching controller), controls the transition of the environment to a
new state. To the best of our knowledge, prior to our work, the only Markov games that have been
examined under this assumption are either zero-sum or potential games."
RELATED WORK,0.03571428571428571,"Further, we manage to go beyond the dichotomy of absolute competition or absolute collaboration
by generalizing zero-sum polymatrix games to their Markovian counterpart. In this sense, our work
is related to previous works of Cai et al. (2016); Anagnostides et al. (2022); Ao et al. (2022) which
show fast convergence to Nash equilibria in zero-sum polymatrix normal-form games for various
no-regret learning algorithms including optimistic gradient descent."
PRELIMINARIES,0.037815126050420166,"2
Preliminaries"
PRELIMINARIES,0.03991596638655462,"Notation.
We define [n] := {1, · · · , n}. Scalars are denoted using lightface variables, while, we
use boldface for vectors and matrices. For simplicity in the exposition, we use O(·) to suppress
dependencies that are polynomial in the parameters of the game. Additionally, given a collection x
of policies or strategies for players [n], x−k denotes the policies of every player excluding k."
MARKOV GAMES,0.04201680672268908,"2.1
Markov games"
MARKOV GAMES,0.04411764705882353,"In its most general form, a Markov game (MG) with a finite number of n players is defined as a tuple
Γ(H, S, {Ak}k∈[n], P, {rk}k∈[n], γ, ρ). Namely,"
MARKOV GAMES,0.046218487394957986,"• H ∈N+ denotes the time horizon, or the length of each episode,
• S, with cardinality S := |S|, stands for the state space,
• {Ak}k∈[n] is the collection of every player’s action space, while A := A1 × · · · × An
denotes the joint action space; further, an element of that set —a joint action— is generally
noted as a = (a1, . . . , an) ∈A,
• P := {Ph}h∈[H] is the set of all transition matrices, with Ph : S × A →∆(S); further,
Ph(·|s, a) marks the probability of transitioning to every state given that the joint action a
is selected at time h and state s — in infinite-horizon games P does not depend on h and the
index is dropped,
• rk := {rk,h} is the reward function of player k at time h; rk,h : S, A →[−1, 1] yields the
reward of player k at a given state and joint action — in infinite-horizon games, rk,h is the
same for every h and the index is dropped,
• a discount factor γ > 0, which is generally set to 1 when H < ∞, and γ < 1 when
H →∞,
• an initial state distribution ρ ∈∆(S)."
MARKOV GAMES,0.04831932773109244,"Policies and value functions.
We will define stationary and nonstationary Markov policies. When
the horizon H is finite, a stationary policy equilibrium need not necessarily exist even for a single-
agent MG, i.e., a Markov decision process; in this case, we seek nonstationary policies. For the case
of infinite-horizon games, it is folklore that a stationary Markov policy Nash equilibrium always
exists."
MARKOV GAMES,0.05042016806722689,"We note that a policy is Markovian when it depends on the present state only. A nonstationary Markov
policy πk for player k is defined as πk := {πk,h : S →∆(Ak), ∀h ∈[H]}. It is a sequence of
mappings of states s to a distribution over actions ∆(Ak) for every timestep h. By πk,h(a|s) we will
denote the probability of player k taking action a in timestep h and state s. A Markov policy is said
to be stationary in the case that it outputs an identical probability distribution over actions whenever
a particular state is visited regardless of the corresponding timestep h."
MARKOV GAMES,0.052521008403361345,"Further, we define a nonstationary Markov joint policy σ := {πh, ∀h ∈[H]} to be a sequence of
mappings from states to distributions over joint actions ∆(A) ≡∆(A1 ×· · ·×An) for all times steps
h in the time horizon. In this case, the players can be said to share a common source of randomness,
or that the joint policy is correlated."
MARKOV GAMES,0.0546218487394958,"A joint policy π will be said to be a product policy if there exist policies πk : [H] × S →
∆(Ak), ∀k ∈[n] such that πh = π1,h × · · · × πn,h, ∀h ∈[H]. Moreover, given a joint policy π
we let a joint policy π−k stand for the marginal joint policy excluding player k, i.e.,"
MARKOV GAMES,0.05672268907563025,"π−k,h(a|s) =
X"
MARKOV GAMES,0.058823529411764705,"a′∈Ak
πh(a′, a|s), ∀h ∈[H], ∀s ∈S, ∀a ∈A−k."
MARKOV GAMES,0.06092436974789916,"By fixing a joint policy π we can define the value function of any given state s and timestep h for
every player k as the expected cumulative reward they get from that state and timestep h onward,"
MARKOV GAMES,0.06302521008403361,"V π
k,h(s1) = Eπ "" H
X"
MARKOV GAMES,0.06512605042016807,"τ=h
γτ−1rk,τ(sτ, aτ)
s1 #"
MARKOV GAMES,0.06722689075630252,"= e⊤
s1 H
X τ=h "
MARKOV GAMES,0.06932773109243698,"γτ−1
h
Y"
MARKOV GAMES,0.07142857142857142,"τ=h
Pτ(πτ) !"
MARKOV GAMES,0.07352941176470588,"rk,τ(πτ)."
MARKOV GAMES,0.07563025210084033,"Depending on whether the game is of finite or infinite horizon we get the followin displays,"
MARKOV GAMES,0.07773109243697479,"• In finite-horizon games, γ = 1, the
value function reads,"
MARKOV GAMES,0.07983193277310924,"• In infinite-horizon games, the value
function of each state is,"
MARKOV GAMES,0.0819327731092437,"V π
k,h(s1) = e⊤
s1 H
X τ=h τY"
MARKOV GAMES,0.08403361344537816,"τ ′=h
Pτ ′(πτ ′) !"
MARKOV GAMES,0.0861344537815126,"rk,τ(πτ),
V π
k (s1) = e⊤
s1 (I −γ P(π))−1 r(π)."
MARKOV GAMES,0.08823529411764706,"Where Ph(πh), P(π) and rh(πh), r(π) denote the state-to-state transition probability matrix and
expected per-state reward vector for a given policy πh or π accordingly. Additionally, es1 is an all-
zero vector apart of a value of 1 in its s1-th position. Also, we denote V π
k,h(ρ) = P"
MARKOV GAMES,0.09033613445378151,"s∈S ρ(s)V π
k,h(s)."
MARKOV GAMES,0.09243697478991597,"Best-response policies.
Given an arbitrary joint policy σ, we define the best-response policy of a
player k to be a policy π†
k := {π†
k,h, ∀h ∈[H]}, such that it is a maximizer of maxπ′
k V π′
k×σ−k
k,1
(s1)."
MARKOV GAMES,0.09453781512605042,"Additionally, we will use the following notation V †,σ−k
k,h
(s) := maxπ′
k V π′
k×σ−k
k,h
(s)."
MARKOV GAMES,0.09663865546218488,"Equilibrium notions.
Having defined what a best-response is, it is then quite direct to define
different notions of equilibria for Markov games.
Definition 2.1 (CCE). We will say that a joint (potentially correlated) policy σ ∈∆(A)H×S is an
ϵ-approximate coarse-correlated equilibrium if it holds that, for an ϵ > 0,"
MARKOV GAMES,0.09873949579831932,"V †,σ−k
k,1
(s1) −V σ
k,1(s1) ≤ϵ, ∀k ∈[n].
(CCE)"
MARKOV GAMES,0.10084033613445378,"Further, we will define a Nash equilibrium policy,
Definition 2.2 (NE). A joint, product policy π ∈Q
k∈[n] ∆(Ak)H×S is an ϵ-approximate Nash
equilibrium if it holds that, for an ϵ > 0,"
MARKOV GAMES,0.10294117647058823,"V †,π−k
k,1
(s1) −V π
k,1(s1) ≤ϵ, ∀k ∈[n].
(NE)"
MARKOV GAMES,0.10504201680672269,"It is quite evident that an approximate Nash equilibrium is also an approximate coarse-correlated
equilibrium while the converse is not generally true. For infinite-horizon games the definitions are
analogous and are deferred to the appendix."
OUR SETTING,0.10714285714285714,"2.2
Our setting"
OUR SETTING,0.1092436974789916,"We focus on the setting of zero-sum polymatrix switching-control Markov games. This setting
encompasses two major assumptions related to the reward functions in every state {rk}k∈[n] and the
transition kernel P. The first assumption imposes a zero-sum, polymatrix structure on {rk}k∈[n] for
every state and directly generalizes zero-sum polymatrix games for games with multiple states."
OUR SETTING,0.11134453781512606,"Assumption 1 (Zero-sum polymatrix games). The reward functions of every player in any state s are
characterized by a zero-sum, polymatrix structure."
OUR SETTING,0.1134453781512605,"Polymatrix structure.
For every state s there exists an undirected graph Gs(V, Es) where,"
OUR SETTING,0.11554621848739496,"• the set of nodes V coincides with the set of agents [n]; the k-th node is the k-th agent,"
OUR SETTING,0.11764705882352941,"• the set of edges Es stands for the set of pair-wise interactions; each edge e = (k, j), k, j ∈
[n], k ̸= j stands for a general-sum normal-form game played between players k, j and"
OUR SETTING,0.11974789915966387,"which we note as

rkj(s, ·, ·), rjk(s, ·, ·)

with rkj, rjk : S × Ak × Aj →[−1, 1]."
OUR SETTING,0.12184873949579832,"Moreover, we define adj(s, k) := {j ∈[n] | (k, j) ∈Es} ⊆[n] to be the set of all neighbors of an
arbitrary agent k in state s. The reward of agent k at state s given a joint action a depends solely on
interactions with their neighbors,"
OUR SETTING,0.12394957983193278,"rk,h(s, a) =
X"
OUR SETTING,0.12605042016806722,"j∈adj(k)
rkj,h(s, ak, aj), ∀h ∈[H], ∀s ∈S, ∀a ∈A."
OUR SETTING,0.12815126050420167,"Further, the zero-sum assumption implies that,
X"
OUR SETTING,0.13025210084033614,"k
rk,h(s, a) = 0,
∀h ∈[H], ∀s ∈S, ∀a ∈A.
(1)"
OUR SETTING,0.1323529411764706,"In the infinite-horizon setting, the subscript h can be dropped."
OUR SETTING,0.13445378151260504,"A further assumption (switching-control) is necessary in order to ensure the desirable property of
equilibrium collapse."
OUR SETTING,0.13655462184873948,"Assumption 2 (Switching-control). In every state s ∈S, there exists a single player (not necessarily
the same), or controller, whose actions determine the probability of transitioning to a new state."
OUR SETTING,0.13865546218487396,"The function argctrl : S →[n] returns the index of the player who controls the transition probability
at a given state s. On the other hand, the function ctrl : S × A →Aargctrl(s) gets an input of a joint
action a, for a particular state s, and returns the action of the controller of that state, aargctrl(s).
Remark 1. It is direct to see that Markov games with a single controller and turn-based Markov
games (Daskalakis et al., 2022), are special case of Markov games with switching controller."
MAIN RESULTS,0.1407563025210084,"3
Main results"
MAIN RESULTS,0.14285714285714285,"In this section we provide the main results of this paper. We shall show the collapsing phenomenon
of coarse-correlated equilibria to Nash equilibria in the case of zero-sum, single switching controller
polymatrix Markov games. Before we proceed, we provide a formal definition of the notion of
collapsing."
MAIN RESULTS,0.14495798319327732,"Definition 3.1 (CCE collapse to NE). Let σ be any ϵ-CCE policy of a Markov game. Moreover, let
the marginal policy πσ := (πσ
1 , ..., πσ
n) be defined as:"
MAIN RESULTS,0.14705882352941177,"πσ
k (a|s) =
X"
MAIN RESULTS,0.14915966386554622,"a−k∈A−k
σ(a, a−k|s), ∀k, ∀s ∈S, ∀a ∈Ak."
MAIN RESULTS,0.15126050420168066,"If πσ is a O(ϵ)-NE equilibrium for every σ then we say the set of approximate CCE’s collapses to
that of approximate NE’s."
MAIN RESULTS,0.15336134453781514,"We start with the warm-up result that the set of CCE’s collapses to the set of NE’s for two-player
zero-sum Markov games."
MAIN RESULTS,0.15546218487394958,"3.1
Warm-up: equilibrium collapse in two-player zero-sum MG’s"
MAIN RESULTS,0.15756302521008403,"Since we focus on two-player zero-sum Markov games, we simplify the notation by using V ·
h=1(s) :=
V ·
2,1(s)—i.e., player 1 is the minimizing player and player 2 is the maximizer. We show the following
theorem:"
MAIN RESULTS,0.15966386554621848,"Theorem 3.1 (Collapse in two-player zero-sum MG’s). Let a two-player zero-sum Markov game Γ′
and an ϵ-approximate CCE policy of that game σ. Then, the marginalized product policies πσ
1 , πσ
2
form a 2ϵ-approximate NE."
MAIN RESULTS,0.16176470588235295,"Proof. Since σ is an ϵ-approximate CCE joint policy, by definition it holds that for any π1 and any
π2,"
MAIN RESULTS,0.1638655462184874,"V σ−2×π2
h=1
(s1) −ϵ ≤V σ
h=1(s1) ≤V π1×σ−1
h=1
(s1) + ϵ."
MAIN RESULTS,0.16596638655462184,"Due to Claim A.1, the latter is equivalent to the following inequality,"
MAIN RESULTS,0.16806722689075632,"V πσ
1 ×π2
h=1
(s1) −ϵ ≤V σ
h=1(s1) ≤V π1×πσ
2
h=1
(s1) + ϵ."
MAIN RESULTS,0.17016806722689076,"Plugging in πσ
1, πσ
2 alternatingly, we get the inequalities:
(
V πσ
1 ×π2
h=1
(s1) −ϵ ≤V σ
h=1(s1) ≤V πσ
1 ×πσ
2
h=1
(s1) + ϵ
V πσ
1 ×πσ
2
h=1
(s1) −ϵ ≤V σ
h=1(s1) ≤V π1×πσ
2
h=1
(s1) + ϵ"
MAIN RESULTS,0.1722689075630252,"The latter leads us to conclude that for any π1 and any π2,"
MAIN RESULTS,0.17436974789915966,"V πσ
1 ×π2
h=1
(s1) −2ϵ ≤V πσ
1 ×πσ
2
h=1
(s1) ≤V π1×πσ
2
h=1
(s1) + 2ϵ,"
MAIN RESULTS,0.17647058823529413,which is the definition of a NE in a zero-sum game.
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.17857142857142858,"3.2
Equilibrium collapse in finite-horizon polymatrix Markov games"
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.18067226890756302,"In this section, we focus on the more challenging case of polymatrix Markov games which is the
main focus of this paper. For any finite horizon Markov game, we define (PNE) to be the following
nonlinear program with variables π, w: (PNE) min
P k∈[n]"
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.18277310924369747,"
wk,1(s1) −e⊤
s1 H
P h=1  hQ"
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.18487394957983194,"τ=1
Pτ(πτ)

rk,h(πh)
"
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.1869747899159664,"s.t. wk,h(s) ≥rk,h(s, a, π−k,h) + Ph(s, a, π−k,h)wk,h+1,
∀s ∈S, ∀h ∈[H], ∀k ∈[n], ∀a ∈Ak;
wk,H(s) = 0,
∀k ∈[n], ∀s ∈S;
πk,h(s) ∈∆(Ak),
∀s ∈S, ∀h ∈[H], ∀k ∈[n], ∀a ∈Ak."
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.18907563025210083,"Using the following theorem, we are able to use (PNE) to argue about equilibrium collapse."
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.19117647058823528,"Theorem 3.2 (NE and global optima of (PNE)). If (π⋆, w⋆) yields an ϵ-approximate global minimum
of (PNE), then π⋆is an nϵ-approximate NE of the zero-sum polymatrix switching controller MG,
Γ. Conversely, if π⋆is an ϵ-approximate NE of the MG Γ with corresponding value function vector
w⋆such that w⋆
k,h(s) = V π⋆
k,h(s)∀(k, h, s) ∈[n] × [H] × S, then (π⋆, w⋆) attains an ϵ-approximate
global minimum of (PNE)."
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.19327731092436976,"Following, we are going to use (PNE) in proving the collapse of CCE’s to NE’s. We observe that the
latter program is nonlinear and in general nonconvex. Hence, duality cannot be used in the way it
was used in (Cai et al., 2016) to prove equilibrium collapse. Nevertheless, we can prove that given
a CCE policy σ, the marginalized, product policy×k∈[n] πσ
k along with an appropriate vector wσ"
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.1953781512605042,"achieves a global minimum in the nonlinear program (PNE). More precisely, our main result reads as
the following statement."
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.19747899159663865,"Theorem 3.3 (CCE collapse to NE in polymatrix MG). Let a zero-sum polymatrix switching-
control Markov game, i.e., a Markov game for which Assumptions 1 and 2 hold. Further, let an
ϵ-approximate CCE of that game σ. Then, the marginal product policy πσ, with πσ
k,h(a|s) =
P
a−k∈A−k σh(a, a−k), ∀k ∈[n], ∀h ∈[H] is an nϵ-approximate NE."
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.19957983193277312,"Proof. Let an ϵ-approximate CCE policy, σ, of game Γ. Moreover, let the best-response value-vectors
of each agent k to joint policy σ−k, w†
k."
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.20168067226890757,"Now, we observe that due to Assumption 1,"
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.20378151260504201,"w†
k,h(s) ≥rk,h(s, a, σ−k,h) + Ph(s, a, σ−k,h)w†
k,h+1 =
X"
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.20588235294117646,"j∈adj(k)
r(k,j),h(s, a, πσ
j ) + Ph(s, a, σ−k,h)w†
k,h+1."
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.20798319327731093,"Further, due to Assumption 2,"
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.21008403361344538,"Ph(s, a, σ−k,h)w†
k,h+1 = Ph(s, a, πσ
argctrl(s),h)w†
k,h+1, or,"
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.21218487394957983,"Ph(s, a, σ−k,h)w†
k,h+1 = Ph(s, a, πσ)w†
k,h+1."
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.21428571428571427,"Putting these pieces together, we reach the conclusion that (πσ, w†) is feasible for the nonlinear
program (PNE)."
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.21638655462184875,"What is left is to prove that it is also an ϵ-approximate global minimum. Indeed, if P"
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.2184873949579832,"k w†
k,h(s1)≤ϵ
(by assumption of an ϵ-approximate CCE), then the objective function of (PNE) will attain an
ϵ-approximate global minimum. In turn, due to Theorem 3.2 the latter implies that πσ is an nϵ-
approximate NE."
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.22058823529411764,"We can now conclude that due to the algorithm introduced in (Daskalakis et al., 2022) for CCE
computation in general-sum MG’s, the next statement holds true."
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.22268907563025211,"Corollary 3.1 (Computing a NE—finite-horizon). Given a finite-horizon switching control zero-
sum polymatrix Markov game, we can compute an ϵ-approximate Nash equilibrium policy that is
Markovian with probability at least 1 −δ in time poly
 
n, H, S, maxk |Ak|, 1"
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.22478991596638656,"ϵ , log(1/δ)

."
EQUILIBRIUM COLLAPSE IN FINITE-HORIZON POLYMATRIX MARKOV GAMES,0.226890756302521,"In the next section, we discuss the necessity of the assumption of switching control using a counter-
example of non-collapsing equilibria."
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.22899159663865545,"3.3
No equilibrium collapse with more than one controllers per-state"
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.23109243697478993,"Although Assumption 1 is sufficient for the collapse of any CCE to a NE in single-state (i.e., normal-
form) games, we will prove that Assumption 2 is indispensable in guaranteeing such a collapse in
zero-sum polymatrix Markov games. That is, if more than one players affect the transition probability
from one state to another, a CCE is not guaranteed to collapse to a NE."
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.23319327731092437,"Example 1. We consider the following 3-player Markov game that takes place for a time horizon
H = 3. There exist three states, s1, s2, and s3 and the game starts at state s1. Player 3 has a
single action in every state, while players 1 and 2 have two available actions {a1, a2} and {b1, b2}
respectively in every state."
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.23529411764705882,"Reward functions.
If player 1 (respectively, player 2) takes action a1 (resp., b1), in either of the
states s1 or s2, they get a reward equal to
1
20. In state s3, both players get a reward equal to −1"
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.23739495798319327,"2
regardless of the action they select. Player 3 always gets a reward that is equal to the negative sum
of the reward of the other two players. This way, the zero-sum polymatrix property of the game is
ensured (Assumption 1)."
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.23949579831932774,"Transition probabilities.
If players 1 and 2 select the joint action (a1, b1) in state s1, the game
will transition to state s2. In any other case, it will transition to state s3. The converse happens if
in state s2 they take joint action (a1, b1); the game will transition to state s3. For any other joint
action, it will transition to state s1. From state s3, the game transitions to state s1 or s2 uniformly at
random."
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.2415966386554622,"At this point, it is important to notice that two players control the transition probability from one state
to another. In other words, Assumption 2 does not hold. s1
s2 s3"
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.24369747899159663,"1/2
1/2"
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.24579831932773108,1 −π1(a1|s1)π2(b1|s1)
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.24789915966386555,"π1(a1|s1)π2(b1|s1)
π1(a1|s2)π2(b1|s2)"
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.25,1 −π1(a1|s2)π2(b1|s2)
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.25210084033613445,"Figure 1: A graph of the state space with transition probabilities parametrized with respect to the
policy of each player."
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.2542016806722689,"Next, we consider the joint policy σ,"
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.25630252100840334,σ(s1) = σ(s2) =
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.25840336134453784,"b1
b2


a1
0
1/2"
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.2605042016806723,"a2
1/2
0
."
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.26260504201680673,Claim 3.1. The joint policy σ that assigns probability 1
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.2647058823529412,"2 to the joint actions (a1, b2) and (a2, b1) in
both states s1, s2 is a CCE and V σ
1,1(s1) = V σ
2,1(s1) =
1
20."
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.2668067226890756,"Yet, the marginalized product policy of σ which we note as πσ
1 × πσ
2 does not constitute a NE. The
components of this policy are,







"
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.2689075630252101,"





"
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.2710084033613445,"πσ
1 (s1) = πσ
1 (s2) ="
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.27310924369747897,"a1
a2
 

1/2
1/2 ,"
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.27521008403361347,"πσ
2 (s1) = πσ
2 (s2) ="
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.2773109243697479,"b1
b2
 

1/2
1/2 ."
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.27941176470588236,"I.e., the product policy πσ
1 × πσ
2 selects any of the two actions of each player in states s1, s2
independently and uniformally at random. With the following claim, it can be concluded that in
general when more than one player control the transition the set of equilibria do not collapse.
Claim 3.2. The product policy πσ
1 × πσ
2 is not a NE."
NO EQUILIBRIUM COLLAPSE WITH MORE THAN ONE CONTROLLERS PER-STATE,0.2815126050420168,"In conclusion, Assumption 1 does not suffice to ensure equilibrium collapse.
Theorem 3.4. There exists a zero-sum polymatrix Markov game (Assumption 2 is not satisfied) that
has a CCE which does not collapse to a NE."
EQUILIBRIUM COLLAPSE IN INFINITE-HORIZON POLYMATRIX MARKOV GAMES,0.28361344537815125,"3.4
Equilibrium collapse in infinite-horizon polymatrix Markov games"
EQUILIBRIUM COLLAPSE IN INFINITE-HORIZON POLYMATRIX MARKOV GAMES,0.2857142857142857,"In proving equilibrium collapse for infinite-horizon polymatrix Markov games, we use similar
arguments and the following nonlinear program with variables π, w,"
EQUILIBRIUM COLLAPSE IN INFINITE-HORIZON POLYMATRIX MARKOV GAMES,0.28781512605042014,"(P′
NE) min
X"
EQUILIBRIUM COLLAPSE IN INFINITE-HORIZON POLYMATRIX MARKOV GAMES,0.28991596638655465,"k∈[n]
ρ⊤ 
wk −(I −γ P(π))−1rk(π)
"
EQUILIBRIUM COLLAPSE IN INFINITE-HORIZON POLYMATRIX MARKOV GAMES,0.2920168067226891,"s.t. wk(s) ≥rk(s, a, π−k) + γ P(s, a, π−k)wk,
∀s ∈S, ∀k ∈[n], ∀a ∈Ak;
πk(s) ∈∆(Ak),
∀s ∈S, ∀k ∈[n], ∀a ∈Ak."
EQUILIBRIUM COLLAPSE IN INFINITE-HORIZON POLYMATRIX MARKOV GAMES,0.29411764705882354,"We note that Example 1 can be properly adjusted to show that the switching-control assumption is
necessary for equilibrium collapse in infinite-horizon games as well. Compared to finite-horizon
games, infinite-horizon games cannot be possibly solved using backward induction. They pose a
genuine computational challenge and, in that sense, the importance of the property of equilibrium
collapse gets highlighted."
EQUILIBRIUM COLLAPSE IN INFINITE-HORIZON POLYMATRIX MARKOV GAMES,0.296218487394958,"Computational implications.
Equilibrium collapse in infinite-horizon MG’s allows us to use the
CCE computation technique found in (Daskalakis et al., 2022) in order to compute an ϵ-approximate
NE. Namely, given an accuracy threshold ϵ, we truncate the infinite-horizon game to its effective
horizon H :=
log(1/ϵ)"
EQUILIBRIUM COLLAPSE IN INFINITE-HORIZON POLYMATRIX MARKOV GAMES,0.29831932773109243,"1−γ
. Then, we define reward functions that depend on the time-step h, i.e.,
rk,h = γh−1rk. Finally,
Corollary 3.2. (Computing a NE—infinite-horizon) Given an infinite-horizon switching control
zero-sum polymatrix game Γ, it is possible to compute a Nash equilibrium policy that is Markovian
and nonstationary with probability at least 1 −δ in time poly

n,
1
1−γ , S, maxk |Ak|, 1"
EQUILIBRIUM COLLAPSE IN INFINITE-HORIZON POLYMATRIX MARKOV GAMES,0.3004201680672269,"ϵ , log(1/δ)

."
CONCLUSION AND OPEN PROBLEMS,0.3025210084033613,"4
Conclusion and open problems"
CONCLUSION AND OPEN PROBLEMS,0.30462184873949577,"In this paper, we unified switching-control Markov games and zero-sum polymatrix normal-form
games. We highlighted how numerous applications can be modeled using this framework and we
focused on the phenomenon of equilibrium collapse from the set of coarse-correlated equilibria to
that of Nash equilibria. This property holds implications for computing approximate Nash equilibria
in switching control zero-sum polymatrix Markov games; it ensures that it can be done efficiently."
CONCLUSION AND OPEN PROBLEMS,0.3067226890756303,"Open problems.
In light of the proposed problem and our results there are multiple interesting
open questions:"
CONCLUSION AND OPEN PROBLEMS,0.3088235294117647,"• Is it possible to use a policy optimization algorithm similar to those of (Erez et al., 2022;"
CONCLUSION AND OPEN PROBLEMS,0.31092436974789917,"Zhang et al., 2022) in order to converge to an approximate Nash equilibrium? We note
that the question can be settled in one of two ways; either extend the current result of
equilibrium collapse to policies that are non-Markovian or guarantee convergence to
Markovian policies. The notion of regret in (Erez et al., 2022) gives rise to the computation
of a CCE that is a non-Markovian policy in the sense that the policy at every timestep
depends on the policy sampled from the history of no-regret play and not only the given
state.
• We conjecture that a convergence rate of O( 1"
CONCLUSION AND OPEN PROBLEMS,0.3130252100840336,"T ) to a NE is possible, i.e., there exists an
algorithm with running time O(1/ϵ) that computes an ϵ-approximate NE.
• Are there more classes of Markov games in which computing Nash equilibria is computa-
tionally tractable?"
REFERENCES,0.31512605042016806,References
REFERENCES,0.3172268907563025,"Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. In Conference on Learning Theory,
COLT 2020, 9-12 July 2020, volume 125 of Proceedings of Machine Learning Research, pages
64–66. PMLR, 2020."
REFERENCES,0.31932773109243695,"Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On last-iterate
convergence beyond zero-sum games. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning,
ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine
Learning Research, pages 536–581. PMLR, 2022. URL https://proceedings.mlr.press/
v162/anagnostides22a.html."
REFERENCES,0.32142857142857145,"Ruicheng Ao, Shicong Cen, and Yuejie Chi. Asynchronous gradient play in zero-sum multi-agent
games, 2022."
REFERENCES,0.3235294117647059,"D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientific, 2nd edition, 2000.
ISBN 1886529094."
REFERENCES,0.32563025210084034,"Michael Bowling. Convergence problems of general-sum multiagent reinforcement learning. In
In Proceedings of the Seventeenth International Conference on Machine Learning, pages 89–94.
Morgan Kaufmann, 2000."
REFERENCES,0.3277310924369748,"Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up limit hold’em
poker is solved. Science, 347(6218):145–149, 2015. doi: 10.1126/science.1259433."
REFERENCES,0.32983193277310924,"Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top
professionals. Science, 359(6374):418–424, 2018. doi: 10.1126/science.aao1733."
REFERENCES,0.3319327731092437,"Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):
885–890, 2019. doi: 10.1126/science.aay2400."
REFERENCES,0.33403361344537813,"Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. Combining deep reinforcement
learning and search for imperfect-information games. In Advances in Neural Information Process-
ing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, 2020."
REFERENCES,0.33613445378151263,"Yang Cai, Ozan Candogan, Constantinos Daskalakis, and Christos Papadimitriou. Zero-sum polyma-
trix games: A generalization of minmax. Mathematics of Operations Research, 41(2):648–655,
2016."
REFERENCES,0.3382352941176471,"Shicong Cen, Yuting Wei, and Yuejie Chi. Fast policy extragradient methods for competitive games
with entropy regularization. In Advances in Neural Information Processing Systems 34: Annual
Conference on Neural Information Processing Systems 2021, NeurIPS 2021, pages 27952–27964,
2021."
REFERENCES,0.3403361344537815,"Dingyang Chen, Qi Zhang, and Thinh T. Doan. Convergence and price of anarchy guarantees of
the softmax policy gradient in markov potential games. In Decision Awareness in Reinforcement
Learning Workshop at ICML 2022, 2022."
REFERENCES,0.34243697478991597,"Xi Chen, Xiaotie Deng, and Shang-Hua Teng. Settling the complexity of computing two-player nash
equilibria. J. ACM, 56(3):14:1–14:57, 2009. doi: 10.1145/1516512.1516516."
REFERENCES,0.3445378151260504,"Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent
systems. In Proceedings of the Fifteenth National Conference on Artificial Intelligence and Tenth
Innovative Applications of Artificial Intelligence Conference, AAAI 98, pages 746–752. AAAI
Press / The MIT Press, 1998."
REFERENCES,0.34663865546218486,"Anne Condon. On algorithms for simple stochastic games. In Advances in Computational Complexity
Theory, volume 13 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science,
pages 51–73. American Mathematical Society, 1993."
REFERENCES,0.3487394957983193,"Qiwen Cui, Kaiqing Zhang, and Simon Du. Breaking the curse of multiagents in a large state space:
Rl in markov games with independent linear function approximation. In The Thirty Sixth Annual
Conference on Learning Theory, pages 2651–2652. PMLR, 2023."
REFERENCES,0.35084033613445376,"Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. The complexity of
computing a nash equilibrium. SIAM Journal on Computing, 39(1):195–259, 2009."
REFERENCES,0.35294117647058826,"Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods
for competitive reinforcement learning. Advances in neural information processing systems, 33:
5527–5540, 2020."
REFERENCES,0.3550420168067227,"Constantinos Daskalakis, Noah Golowich, and Kaiqing Zhang. The complexity of markov equilibrium
in stochastic games. arXiv preprint arXiv:2204.03991, 2022."
REFERENCES,0.35714285714285715,"Xiaotie Deng, Ningyuan Li, David Mguni, Jun Wang, and Yaodong Yang. On the complexity of
computing markov perfect equilibrium in general-sum stochastic games. National Science Review,
10(1):nwac256, 2023."
REFERENCES,0.3592436974789916,"Dongsheng Ding, Chen-Yu Wei, Kaiqing Zhang, and Mihailo R Jovanovi´c. Independent policy
gradient for large-scale markov potential games: Sharper rates, function approximation, and
game-agnostic convergence. arXiv preprint arXiv:2202.04129, 2022."
REFERENCES,0.36134453781512604,"David Easley and Jon Kleinberg. Networks, crowds, and markets: Reasoning about a highly connected
world. Cambridge university press, 2010."
REFERENCES,0.3634453781512605,"Liad Erez, Tal Lancewicki, Uri Sherman, Tomer Koren, and Yishay Mansour. Regret minimization
and convergence to equilibria in general-sum markov games, 2022."
REFERENCES,0.36554621848739494,"Kousha Etessami and Mihalis Yannakakis. On the complexity of nash equilibria and other fixed
points. SIAM J. Comput., 39(6):2531–2597, 2010. doi: 10.1137/080720826."
REFERENCES,0.36764705882352944,"Jerzy Filar and Koos Vrieze. Competitive Markov decision processes. Springer Science & Business
Media, 2012."
REFERENCES,0.3697478991596639,"Jerzy A Filar and TES Raghavan. A matrix game solution of the single-controller stochastic game.
Mathematics of Operations Research, 9(3):356–362, 1984."
REFERENCES,0.37184873949579833,"Jerzy A Filar, Todd A Schultz, Frank Thuijsman, and OJ Vrieze. Nonlinear programming and
stationary equilibria in stochastic games. Mathematical Programming, 50(1-3):227–237, 1991."
REFERENCES,0.3739495798319328,"Roy Fox, Stephen M. McAleer, Will Overman, and Ioannis Panageas. Independent natural policy
gradient always converges in markov potential games. In International Conference on Artificial
Intelligence and Statistics, AISTATS 2022, volume 151 of Proceedings of Machine Learning
Research, pages 4414–4425. PMLR, 2022."
REFERENCES,0.3760504201680672,"Peng Guan, Maxim Raginsky, Rebecca Willett, and Daphney-Stavroula Zois. Regret minimization
algorithms for single-controller zero-sum stochastic games. In 2016 IEEE 55th Conference on
Decision and Control (CDC), pages 7075–7080. IEEE, 2016."
REFERENCES,0.37815126050420167,"Junling Hu and Michael P. Wellman. Nash q-learning for general-sum stochastic games. J. Mach.
Learn. Res., 4:1039–1069, 2003."
REFERENCES,0.3802521008403361,"Chi Jin, Zeyuan Allen-Zhu, Sébastien Bubeck, and Michael I. Jordan. Is q-learning provably
efficient? In Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018, NeurIPS 2018, pages 4868–4878, 2018."
REFERENCES,0.38235294117647056,"Yujia Jin, Vidya Muthukumar, and Aaron Sidford. The complexity of infinite-horizon general-
sum stochastic games. CoRR, abs/2204.04186, 2022. doi: 10.48550/arXiv.2204.04186. URL
https://doi.org/10.48550/arXiv.2204.04186."
REFERENCES,0.38445378151260506,"Fivos Kalogiannis, Ioannis Anagnostides, Ioannis Panageas, Emmanouil-Vasileios Vlatakis-
Gkaragkounis, Vaggos Chatziafratis, and Stelios Stavroulakis. Efficiently computing nash equilibria
in adversarial team markov games. arXiv preprint arXiv:2208.02204, 2022."
REFERENCES,0.3865546218487395,"Stefanos Leonardos, Will Overman, Ioannis Panageas, and Georgios Piliouras. Global convergence
of multi-agent policy gradient in markov potential games. arXiv preprint arXiv:2106.01969, 2021."
REFERENCES,0.38865546218487396,"Yuanzhi Li, Ruosong Wang, and Lin F. Yang. Settling the horizon-dependence of sample complexity
in reinforcement learning. In 62nd IEEE Annual Symposium on Foundations of Computer Science,
FOCS 2021, pages 965–976. IEEE, 2021. doi: 10.1109/FOCS52979.2021.00097."
REFERENCES,0.3907563025210084,"Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pages 157–163. Elsevier, 1994."
REFERENCES,0.39285714285714285,"Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
framework for model-based deep reinforcement learning with theoretical guarantees. In 7th
International Conference on Learning Representations, ICLR 2019. OpenReview.net, 2019."
REFERENCES,0.3949579831932773,"Chinmay Maheshwari, Manxi Wu, Druv Pai, and Shankar Sastry. Independent and decentralized
learning in markov potential games, 2022."
REFERENCES,0.39705882352941174,"Matej Moravˇcík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor
Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial
intelligence in heads-up no-limit poker. Science, 356(6337):508–513, 2017. doi: 10.1126/science.
aam6960."
REFERENCES,0.39915966386554624,"John Nash. Non-cooperative games. Annals of mathematics, pages 286–295, 1951."
REFERENCES,0.4012605042016807,"Gergely Neu and Ciara Pike-Burke. A unifying view of optimism in episodic reinforcement learning.
Advances in Neural Information Processing Systems, 33:1392–1403, 2020."
REFERENCES,0.40336134453781514,"L. Panait and S. Luke. Cooperative Multi-Agent Learning: The State of the Art. Autonomous Agents
and Multi-Agent Systems, 11(3):387–434, Nov 2005. doi: 10.1007/s10458-005-2631-2."
REFERENCES,0.4054621848739496,"Julien Perolat, Bart de Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer,
Paul Muller, Jerome T. Connor, Neil Burch, Thomas Anthony, Stephen McAleer, Romuald Elie,
Sarah H. Cen, Zhe Wang, Audrunas Gruslys, Aleksandra Malysheva, Mina Khan, Sherjil Ozair,
Finbarr Timbers, Toby Pohlen, Tom Eccles, Mark Rowland, Marc Lanctot, Jean-Baptiste Lespiau,
Bilal Piot, Shayegan Omidshafiei, Edward Lockhart, Laurent Sifre, Nathalie Beauguerlange, Remi
Munos, David Silver, Satinder Singh, Demis Hassabis, and Karl Tuyls. Mastering the game of
stratego with model-free multiagent reinforcement learning, 2022."
REFERENCES,0.40756302521008403,"Shuang Qiu, Xiaohan Wei, Jieping Ye, Zhaoran Wang, and Zhuoran Yang. Provably efficient fictitious
play policy optimization for zero-sum markov games with structured transitions. In International
Conference on Machine Learning, pages 8715–8725. PMLR, 2021."
REFERENCES,0.4096638655462185,"Aviad Rubinstein. Settling the complexity of computing approximate two-player nash equilibria.
SIGecom Exch., 15(2):45–49, 2017. doi: 10.1145/3055589.3055596."
REFERENCES,0.4117647058823529,"Muhammed Sayin, Kaiqing Zhang, David Leslie, Tamer Basar, and Asuman Ozdaglar. Decentralized
q-learning in zero-sum markov games. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang,
and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,
pages 18320–18334. Curran Associates, Inc., 2021."
REFERENCES,0.41386554621848737,"Muhammed O Sayin, Francesca Parise, and Asuman Ozdaglar. Fictitious play in zero-sum stochastic
games. arXiv preprint arXiv:2010.04223, 2020."
REFERENCES,0.41596638655462187,"Muhammed O Sayin, Kaiqing Zhang, and Asuman Ozdaglar. Fictitious play in markov games with
single controller. In Proceedings of the 23rd ACM Conference on Economics and Computation,
pages 919–936, 2022."
REFERENCES,0.4180672268907563,"Frank Schweitzer, Giorgio Fagiolo, Didier Sornette, Fernando Vega-Redondo, Alessandro Vespignani,
and Douglas R White. Economic networks: The new challenges. science, 325(5939):422–425,
2009."
REFERENCES,0.42016806722689076,"Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):
1095–1100, 1953."
REFERENCES,0.4222689075630252,"Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye. Near-optimal time and sample
complexities for solving markov decision processes with a generative model. In Advances in
Neural Information Processing Systems 31: Annual Conference on Neural Information Processing
Systems 2018, pages 5192–5202, 2018."
REFERENCES,0.42436974789915966,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan
Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the
game of go without human knowledge. Nat., 550(7676):354–359, 2017. doi: 10.1038/nature24270."
REFERENCES,0.4264705882352941,"R. S. Sutton and A. G. Barto.
Reinforcement Learning: An Introduction.
A Bradford Book,
Cambridge, MA, USA, 2018. ISBN 0262039249."
REFERENCES,0.42857142857142855,"György Szabó and Gabor Fath. Evolutionary games on graphs. Physics reports, 446(4-6):97–216,
2007."
REFERENCES,0.43067226890756305,"Yodyium Tipsuwan and Mo-Yuen Chow. Control methodologies in networked control systems.
Control engineering practice, 11(10):1099–1111, 2003."
REFERENCES,0.4327731092436975,"Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung
Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan,
Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max
Jaderberg, Alexander Sasha Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David
Budden, Yury Sulsky, James Molloy, Tom Le Paine, Çaglar Gülçehre, Ziyu Wang, Tobias Pfaff,
Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith,
Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David
Silver. Grandmaster level in starcraft II using multi-agent reinforcement learning. Nat., 575(7782):
350–354, 2019. doi: 10.1038/s41586-019-1724-z."
REFERENCES,0.43487394957983194,"OJ Vrieze, SH Tijs, TES Raghavan, and JA Filar. A finite algorithm for the switching control
stochastic game. Or Spektrum, 5(1):15–24, 1983."
REFERENCES,0.4369747899159664,"Xiaofeng Wang and Tuomas Sandholm. Reinforcement learning to play an optimal nash equilibrium
in team markov games. In Advances in Neural Information Processing Systems 15 [Neural
Information Processing Systems, NIPS 2002, December 9-14, 2002, pages 1571–1578. MIT Press,
2002."
REFERENCES,0.43907563025210083,"Yuanhao Wang, Qinghua Liu, Yu Bai, and Chi Jin. Breaking the curse of multiagency: Provably effi-
cient decentralized multi-agent rl with function approximation. arXiv preprint arXiv:2302.06606,
2023."
REFERENCES,0.4411764705882353,"Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of
decentralized optimistic gradient descent/ascent in infinite-horizon competitive markov games.
In Mikhail Belkin and Samory Kpotufe, editors, Proceedings of Thirty Fourth Conference on
Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 4259–4299.
PMLR, 15–19 Aug 2021."
REFERENCES,0.4432773109243697,"Runyu Zhang, Zhaolin Ren, and Na Li. Gradient play in stochastic games: stationary points,
convergence, and sample complexity. arXiv preprint arXiv:2106.00198, 2021."
REFERENCES,0.44537815126050423,"Runyu Zhang, Qinghua Liu, Huan Wang, Caiming Xiong, Na Li, and Yu Bai. Policy optimization for
markov games: Unified framework and faster convergence, 2022."
REFERENCES,0.4474789915966387,"A
Missing statements and proofs"
REFERENCES,0.4495798319327731,"A.1
Statements for Section 3.1"
REFERENCES,0.45168067226890757,"Claim A.1. Let a two-player Markov game where both players affect the transition. Further, consider
a correlated policy σ and its corresponding marginalized product policy πσ = πσ
1 × πσ
2 . Then, for
any π′
1, π′
2,"
REFERENCES,0.453781512605042,"V π′
1,σ−1
k,1
(s1) = V π′
1,πσ
2
k,1
(s1),"
REFERENCES,0.45588235294117646,"V σ−2,π′
2
k,2
(s1) = V πσ
1 ,π′
2
k,2
(s1)."
REFERENCES,0.4579831932773109,"Proof. We will effectively show that the problem of best-responding to a correlated policy σ is
equivalent to best-responding to the marginal policy of σ for the opponent. The proof follows from
the equivalence of the two MDPs."
REFERENCES,0.46008403361344535,"As a reminder,"
REFERENCES,0.46218487394957986,"π1,h(a|s) =
X"
REFERENCES,0.4642857142857143,"b∈A2
σh(a, b|s)"
REFERENCES,0.46638655462184875,"π2,h(b|s) =
X"
REFERENCES,0.4684873949579832,"a∈A1
σh(a, b|s)"
REFERENCES,0.47058823529411764,"As we have seen in Section 2.1, in the case of unilateral deviation from joint policy σ, an agent
faces a single agent MDP. More specifically, agent 2, best-responds by optimizing a reward function
¯r2,h(s, b) under a transition kernel ¯P2 for which,
¯r2,h(s, b) = Eb∼σ [r2,h(s, a, b)] = Eb∼πσ
1 [r2,h(s, a, b)] = r2,h(s, πσ
1 , b).
Similarly,
¯r1,h(s, b) = r1,h(s, a, πσ
2 )."
REFERENCES,0.4726890756302521,"Analogously, for each of the transition kernels,
¯P2,h(s′|s, b) = Ea∼σ [P2,h(s′|s, a, b)] = Ea∼πσ
2 [P2,h(s′|s, a, b)] = P2,h(s′|s, πσ
1 , b),
as for agent 1,
¯P1,h(s′|s, a) = P1,h(s′|s, a, πσ
2 )."
REFERENCES,0.47478991596638653,"Hence,
it follows that,
V σ−2×π′
2
2,1
(s1)
=
V πσ
1 ×π′
2
2,1
(s1),
∀π′
2
and
V π′
1×σ−1
1,1
(s1)
="
REFERENCES,0.47689075630252103,"V π′
1×πσ
2
1,1
(s1), ∀π′
2."
REFERENCES,0.4789915966386555,"A.2
Proof of Theorem 3.2"
REFERENCES,0.4810924369747899,"The best-response program.
First, we state the following lemma that will prove useful for several
of our arguments,
Lemma A.1 (Best-response LP). Let a (possibly correlated) joint policy ˆσ. Consider the following
linear program with variables w ∈Rn×H×S , (PBR) min
P"
REFERENCES,0.4831932773109244,"k∈[n]
wk,s(s1) −e⊤
s1 H
P h=1  hQ"
REFERENCES,0.4852941176470588,"τ=1
Pτ(ˆστ)

rk,h(ˆσh)"
REFERENCES,0.48739495798319327,"s.t. wk,h(s) ≥rk,h(s, a, ˆσ−k,h) + Ph(s, a, ˆσ−k,h)wk,h+1,
∀s ∈S, ∀h ∈[H], ∀k ∈[n], ∀a ∈Ak;
wk,H(s) = 0, ∀k ∈[n], ∀s ∈S."
REFERENCES,0.4894957983193277,"The optimal solution w† of the program is unique and corresponds to the value function of each
player k ∈[n] when player k best-responds to ˆσ."
REFERENCES,0.49159663865546216,"Proof. We observe that the program is separable to n independent linear programs, each with
variables wk ∈Rn×H,
min wk,1(s1)
s.t. wk,h(s) ≥rk,h(s, a, ˆσ−k,h) + Ph(s, a, ˆσ−k,h)wk,h+1,
∀s ∈S, ∀h ∈[H], ∀a ∈Ak;
wk,H(s) = 0, ∀k ∈[n], ∀s ∈S.
Each of these linear programs describes the problem of a single agent MDP (Neu and Pike-Burke,
2020, Section 2) —that agent being k— which, as we have seen in Best-response policies, is
equivalent to the problem of finding a best-response to ˆσ−k. It follows that the optimal w†
k for every
program is unique (each program corresponds to a set of Bellman optimality equations)."
REFERENCES,0.49369747899159666,"Properties of the NE program.
Second, we need to prove that the minimum value of the objective
function of the program is nonnegative.
Lemma A.2 (Feasibility of (P′
NE) and global optimum). The nonlinear program (P′
NE) is feasible,
has a nonnegative objective value, and its global minimum is equal to 0."
REFERENCES,0.4957983193277311,"Proof. Analogously to the finite-horizon case, for the feasibility of the nonlinear program, we invoke
the theorem of the existence of a Nash equilibrium. We let a NE product policy, π⋆, and a vector
w⋆∈Rn×S such that w⋆
k(s) = V
†,π⋆
−k
k
(s), ∀k ∈[n] × S."
REFERENCES,0.49789915966386555,"By Lemma A.1, we know that (π⋆, w⋆) satisfies all the constraints of (PNE). Additionaly, because"
REFERENCES,0.5,"π⋆is a NE, V π⋆
k,h(s1) = V
†,π⋆
−k
k,h
(s1) for all k ∈[n]. Observing that,"
REFERENCES,0.5021008403361344,"w⋆
k,1(s1) −e⊤
s1 H
X h=1 h
Y"
REFERENCES,0.5042016806722689,"τ=1
Pτ(π⋆
τ) !"
REFERENCES,0.5063025210084033,"rk,h(π⋆
h) = V
†,π⋆
−k
k,h
(s1) −V π⋆
k,h(s1) = 0,"
REFERENCES,0.5084033613445378,concludes the argument that a NE attains an objective value equal to 0.
REFERENCES,0.5105042016806722,"Continuing, we observe that due to (1) the objective function can be equivalently rewritten as, X k∈[n] "
REFERENCES,0.5126050420168067,"wk,1(s1) −e⊤
s1 H
X h=1 h
Y"
REFERENCES,0.5147058823529411,"τ=1
Pτ(πτ) !"
REFERENCES,0.5168067226890757,"rk,h(πh) ! =
X"
REFERENCES,0.5189075630252101,"k∈[n]
wk,1(s1) −e⊤
s1 H
X h=1 h
Y"
REFERENCES,0.5210084033613446,"τ=1
Pτ(πτ) ! X"
REFERENCES,0.523109243697479,"k∈[n]
rk,h(πh) =
X"
REFERENCES,0.5252100840336135,"k∈[n]
wk,1(s1)."
REFERENCES,0.5273109243697479,"Next, we focus on the inequality constraint
wk,h(s) ≥rk,h(s, a, π−k,h) + Ph(s, a, π−k,h)wk,h+1
which holds for all s ∈S, all players k ∈[n], all a ∈Ak, and all timesteps h ∈[H −1]."
REFERENCES,0.5294117647058824,"By summing over a ∈Ak while multiplying each term with a corresponding coefficient πk,h(a|s),
the display written in an equivalent element-wise vector inequality reads:
wk,h ≥rk,h(πh) + Ph(πh)wk,h+1.
Finally, after consecutively substituting wk,h+1 with the element-wise lesser term rk,h+1(πh+1) +
Ph+1(πh+1)wk,h+2, we end up with the inequality:"
REFERENCES,0.5315126050420168,"wk,1 ≥ H
X h=1 h
Y"
REFERENCES,0.5336134453781513,"τ=1
Pτ(πτ) !"
REFERENCES,0.5357142857142857,"rk,h(πh).
(5)"
REFERENCES,0.5378151260504201,"Summing over k, it holds for the s1-th entry of the inequality, X"
REFERENCES,0.5399159663865546,"k∈[n]
wk,1 ≥
X k∈[n] H
X h=1 h
Y"
REFERENCES,0.542016806722689,"τ=1
Pτ(πτ) !"
REFERENCES,0.5441176470588235,"rk,h(πh) = 0."
REFERENCES,0.5462184873949579,"Where the equality holds due to the zero-sum property, (1)."
REFERENCES,0.5483193277310925,"An approximate NE is an approximate global minimum.
We show that an ϵ-approximate NE,
π⋆, achieves an nϵ-approximate global minimum of the program. Utilizing Lemma A.1, setting
w⋆
k(s1) = V
†,π⋆
−k
k,1
(s1), and the definition of an ϵ-approximate NE we see that, X k∈[n] "
REFERENCES,0.5504201680672269,"w⋆
k,1(s1) −e⊤
s1 H
X h=1 h
Y"
REFERENCES,0.5525210084033614,"τ=1
Pτ(π⋆
τ) !"
REFERENCES,0.5546218487394958,"rk,h(π⋆
h) ! =
X k∈[n]"
REFERENCES,0.5567226890756303,"
w⋆
k,1(s1) −V π⋆
k,1 (s1)
 ≤
X"
REFERENCES,0.5588235294117647,"k∈[n]
ϵ = nϵ."
REFERENCES,0.5609243697478992,"Indeed, this means that π⋆, w⋆is an nϵ-approximate global minimizer of (PNE)."
REFERENCES,0.5630252100840336,"An approximate global minimum is an approximate NE.
For the opposite direction, we let a
feasible ϵ-approximate global minimizer of the program (PNE), (π⋆, w⋆). Because a global minimum
of the program is equal to 0, an ϵ-approximate global optimum must be at most ϵ > 0. We observe
that for every k ∈[n],"
REFERENCES,0.5651260504201681,"w⋆
k,1(s1) ≥e⊤
s1 H
X h=1 h
Y"
REFERENCES,0.5672268907563025,"τ=1
Pτ(π⋆
τ) !"
REFERENCES,0.569327731092437,"rk,h(π⋆
h),
(6)"
REFERENCES,0.5714285714285714,which follows from induction on the inequality constraint over all h similar to (5).
REFERENCES,0.5735294117647058,"Consequently, the assumption that ϵ≥
X k∈[n] "
REFERENCES,0.5756302521008403,"w⋆
k,1(s1) −e⊤
s1 H
X h=1 h
Y"
REFERENCES,0.5777310924369747,"τ=1
Pτ(π⋆
τ) !"
REFERENCES,0.5798319327731093,"rk,h(π⋆
h) ! ,"
REFERENCES,0.5819327731092437,"and Equation (6), yields the fact that"
REFERENCES,0.5840336134453782,"ϵ ≥w⋆
k,1(s1) −e⊤
s1 H
X h=1 h
Y"
REFERENCES,0.5861344537815126,"τ=1
Pτ(π⋆
τ) !"
REFERENCES,0.5882352941176471,"rk,h(π⋆
h)"
REFERENCES,0.5903361344537815,"≥V
†,π⋆
−k
k,1
(s1) −V π⋆
k,1 (s1),"
REFERENCES,0.592436974789916,"where the second inequality holds from the fact that w⋆is feasible for (PBR). The latter concludes
the proof, as the display coincides with the definition of an ϵ-approximate NE."
REFERENCES,0.5945378151260504,"A.3
Proof of Claim 3.1"
REFERENCES,0.5966386554621849,Proof. The value function of s1 for h = 1 of players 1 and 2 read:
REFERENCES,0.5987394957983193,"V σ
1,1(s1) = e⊤
s1 (r1(σ) + P(σ)r1(σ))"
REFERENCES,0.6008403361344538,"= −9σ(a1, b1|s1)"
REFERENCES,0.6029411764705882,"20
+ σ(a1, b2|s1)"
REFERENCES,0.6050420168067226,"20
+ (1 −σ(a1, b1|s1)) (σ(a1, b1|s2) + σ(a1, b2|s2)) 20
, and,"
REFERENCES,0.6071428571428571,"V σ
2,1(s1) = e⊤
s1 (r2(σ) + P(σ)r2(σ))"
REFERENCES,0.6092436974789915,"= −9σ(a1, b1|s1)"
REFERENCES,0.6113445378151261,"20
+ σ(a2, b2|s1)"
REFERENCES,0.6134453781512605,"20
+ (1 −σ(a1, b1|s1)) (σ(a1, b1|s2) + σ(a2, b1|s2)) 20
."
REFERENCES,0.615546218487395,"We are indifferent to the corresponding value function of player 3 as they only have one available
action per state and hence, cannot affect their rewards. For the joint policy σ, the corresponding
value functions of both players 1 and 2 are V σ
1,1(s1) = V σ
2,1(s1) =
1
20."
REFERENCES,0.6176470588235294,"Deviations.
We will now prove that no deviation of player 1 manages to accumulate a reward
greater than
1
20. The same follows for player 2 due to symmetry."
REFERENCES,0.6197478991596639,"When a player deviates unilaterally from a joint policy, they experience a single agent Markov
decision process (MDP). It is well-known that MDPs always have a deterministic optimal policy.
As such, it suffices to check whether V π1,σ−1
1,1
(s1) is greater than
1
20 for any of the four possible
deterministic policies:"
REFERENCES,0.6218487394957983,"• π1(s1) = π1(s2) = (1
0),"
REFERENCES,0.6239495798319328,"• π1(s1) = π1(s2) = (0
1),"
REFERENCES,0.6260504201680672,"• π1(s1) = (1
0) , π1(s2) = (0
1),"
REFERENCES,0.6281512605042017,"• π1(s1) = (0
1) , π1(s2) = (1
0)."
REFERENCES,0.6302521008403361,"Finally, the value function of any deviation π′
1 writes,"
REFERENCES,0.6323529411764706,"V π′
1×σ−1
1,1
(s1) = −π′
1(a1|s1)"
REFERENCES,0.634453781512605,"5
−π′
1(a1|s2) (π′
1(a1|s1) −2)
40
."
REFERENCES,0.6365546218487395,"We can now check that for all deterministic policies V π′
1×σ−1
1,1
(s1) ≤
1
20. By symmetry, it follows"
REFERENCES,0.6386554621848739,"that V π′
2×σ−2
2,1
(s1) ≤
1
20 and as such σ is indeed a CCE."
REFERENCES,0.6407563025210085,"A.4
Proof of Claim 3.2"
REFERENCES,0.6428571428571429,"Proof. In general, the value functions of each player 1 and 2 are:"
REFERENCES,0.6449579831932774,"V π1×π2
1,1
(s1) = −π1(a1|s1)π2(b1|s1)"
REFERENCES,0.6470588235294118,"2
+ π1(a1|s1)"
REFERENCES,0.6491596638655462,"20
−π1(a1|s2) (π1(a1|s1)π2(b1|s1) −1) 20
, and"
REFERENCES,0.6512605042016807,"V π1×π2
2,1
(s1) = −π1(a1|s1)π2(b1|s1)"
REFERENCES,0.6533613445378151,"2
+ π1(b1|s1)"
REFERENCES,0.6554621848739496,"20
−π1(b1|s2) (π1(a1|s1)π2(b1|s1) −1) 20
."
REFERENCES,0.657563025210084,"Plugging in πσ
1, πσ
2 yields V πσ
1 ×πσ
2
1,1
(s1) = V πσ
1 ×πσ
2
2,1
(s1) = −13"
REFERENCES,0.6596638655462185,"160. But, if player 1 deviates to say
π′
1(s1) = π′
1(s2) = (0
1), they get a value equal to 0 which is clearly greater than −13"
REFERENCES,0.6617647058823529,"160. Hence,
πσ
1 × πσ
2 is not a NE."
REFERENCES,0.6638655462184874,"A.5
Proof of Theorem 3.4"
REFERENCES,0.6659663865546218,"Proof. The proof follows from the game of Example 1, and Claims 3.1 and 3.2."
REFERENCES,0.6680672268907563,"B
Proofs for infinite-horizon Zero-Sum Polymatrix Markov Games"
REFERENCES,0.6701680672268907,"In this section we will explicitly state definitions, theorems and proofs relating to the infinite-horizon
discounted zero-sum polymatrix Markov games."
REFERENCES,0.6722689075630253,"B.1
Definitions of equilibria for the infinite-horizon"
REFERENCES,0.6743697478991597,"Let us restate the definition specifically for infinite-horizon Markov games. They are defined as a
tuple Γ(H, S, {Ak}k∈[n], P, {rk}k∈[n], γ, ρ)."
REFERENCES,0.6764705882352942,• H = ∞denotes the time horizon
REFERENCES,0.6785714285714286,"• S, with cardinality S := |S|, stands for the state space,"
REFERENCES,0.680672268907563,"• {Ak}k∈[n] is the collection of every player’s action space, while A := A1 × · · · × An
denotes the joint action space; further, an element of that set —a joint action— is generally
noted as a = (a1, . . . , an) ∈A,"
REFERENCES,0.6827731092436975,"• P : S × A →∆(S) is the transition probability function,"
REFERENCES,0.6848739495798319,"• rk : S, A →[−1, 1] yields the reward of player k at a given state and joint action,"
REFERENCES,0.6869747899159664,"• a discount factor 0 < γ < 1,"
REFERENCES,0.6890756302521008,• an initial state distribution ρ ∈∆(S).
REFERENCES,0.6911764705882353,"Policies and value functions.
In infinite-horizon Markov games policies can still be distinguished
in two main ways, Markovian/non-Markovian and stationary/nonstationary. Moreover, a joint policy
can be a correlated policy or a product policy."
REFERENCES,0.6932773109243697,"Markovian policies attribute a probability over the simplex of actions solely depending on the running
state s of the game. On the other hand, non-Markovian policies attribute a probability over the
simplex of actions that depends on any subset of the history of the game. I.e., they can depend on any
sub-sequence of actions and states up until the running timestep of the horizon."
REFERENCES,0.6953781512605042,"Stationary policies are those that will attribute the same probability distribution over the simplex
of actions for every timestep of the horizon. Nonstationary policies, on the contrary can change
depending on the timestep of the horizon."
REFERENCES,0.6974789915966386,"A joint Markovian stationary policy σ is said to be correlated when for every state s ∈S, attributes
a probability distribution over the simplex of joint actions A for all players, i.e., σ(s) ∈∆(A).
A Markovian stationary policy π is said to be a product policy when for every s ∈S, π(s) ∈
Qn
k=1 ∆(Ak). It is rather easy to define correlated/product policies for the case of non-Markovian
and nonstationary policies."
REFERENCES,0.6995798319327731,"Given a Markovian stationary policy π, the value function for an infinite-horizon discounted game is
defined as,"
REFERENCES,0.7016806722689075,"V π
k (s1) = Eπ "" H
X"
REFERENCES,0.7037815126050421,"h=1
γh−1rk,h(sh, ah)
s1 #"
REFERENCES,0.7058823529411765,"= e⊤
s1 H
X h=1 "
REFERENCES,0.707983193277311,"γh−1
h
Y"
REFERENCES,0.7100840336134454,"τ=1
Pτ(πτ) !"
REFERENCES,0.7121848739495799,"rk,h(πh)."
REFERENCES,0.7142857142857143,"It is possible to express the value function of each player k in the following way,"
REFERENCES,0.7163865546218487,"V π
k (s1) = e⊤
s1 (I −γ P(π))−1 r(π)."
REFERENCES,0.7184873949579832,"Where I is the identity matrix of appropriate dimensions. Also, when the initial state is drawn from
the initial state distribution, we denote, the value function reads V π
k (ρ) = ρ⊤(I −γ P(π))−1 r(π)."
REFERENCES,0.7205882352941176,"Best-response policies.
Given an arbitrary joint policy σ (which can be either a correlated or
product policy), a best-response policy of a player k is defined to be π†
k ∈∆(Ak)S such that"
REFERENCES,0.7226890756302521,"π†
k ∈arg maxπ′
k V π′
k×σ−k
k
(s). Also, we will denote V †,σ−k
k
(s) = maxπ′
k V π′
k,σ−k
k
(s). It is rather
straightforward to see that the problem of computing a best-response to a given policy is equivalent
to solving a single-agent MDP problem."
REFERENCES,0.7247899159663865,"Notions of equilibria.
Now that best-response policies have been defined, it is straightforward to
define the different notions of equilibria. First, we define the notion of a coarse-correlated equilibrium.
Definition B.1 (CCE—infinite-horizon). A joint (potentially correlated) policy σ ∈∆(A)S is an
ϵ-approximate coarse-correlated equilibrium if it holds that for an ϵ,"
REFERENCES,0.726890756302521,"V †,σ−k
k
(ρ) −V σ
k (ρ) ≤ϵ, ∀k ∈[n]."
REFERENCES,0.7289915966386554,"Second, we define the notion of a Nash equilibrium. The main difference of the definition of the
coarse-correlated equilibrium, is the fact that a NE Markovian stationary policy is a product policy.
Definition B.2 (NE—infinite-horizon). A joint (potentially correlated) policy π ∈Q"
REFERENCES,0.7310924369747899,k∈[n] ∆(Ak)S
REFERENCES,0.7331932773109243,"is an ϵ-approximate coarse-correlated equilibrium if it holds that for an ϵ,"
REFERENCES,0.7352941176470589,"V †,π−k
k
(ρ) −V π
k (ρ) ≤ϵ, ∀k ∈[n]."
REFERENCES,0.7373949579831933,"As it is folklore by now, infinite-horizon discounted Markov games have a stationary Markovian Nash
equilibrium."
REFERENCES,0.7394957983193278,"C
Main results for infinite-horizon games"
REFERENCES,0.7415966386554622,"The workhorse of our arguments in the following results is still the following nonlinear program with
variables π, w,"
REFERENCES,0.7436974789915967,"(P′
NE) min
X"
REFERENCES,0.7457983193277311,"k∈[n]
ρ⊤ 
wk −(I −γ P(π))−1rk(π)
"
REFERENCES,0.7478991596638656,"s.t. wk(s) ≥rk(s, a, π−k) + γ P(s, a, π−k)wk,
∀s ∈S, ∀k ∈[n], ∀a ∈Ak;
πk(s) ∈∆(Ak),
∀s ∈S, ∀k ∈[n], ∀a ∈Ak."
REFERENCES,0.75,"As we will prove, approximate NE’s correspond to approximate global minima of (P′
NE) and vice-
versa. Before that, we need some intermediate lemmas. The first lemma we prove is about the
best-response program."
REFERENCES,0.7521008403361344,"The best-response program.
Even for the infinite-horizon, we can define a linear program for the
best-responses of all players. That program is the following, with variables w,"
REFERENCES,0.7542016806722689,"(P′
BR) min
X"
REFERENCES,0.7563025210084033,"k∈[n]
ρ⊤ 
wk −(I −γ P(ˆσ))−1rk(ˆσ)
"
REFERENCES,0.7584033613445378,"s.t. wk(s) ≥rk(s, a, ˆσ−k) + P(s, a, ˆσ−k)wk,
∀s ∈S, ∀k ∈[n], ∀a ∈Ak."
REFERENCES,0.7605042016806722,"Lemma C.1 (Best-response LP—infinite-horizon). Let a (possibly correlated) joint policy ˆσ. Con-
sider the linear program (P′
BR). The optimal solution w† of the program is unique and corresponds
to the value function of each player k ∈[n] when player k best-responds to ˆσ."
REFERENCES,0.7626050420168067,"Proof. We observe that the program is separable to n independent linear programs, each with
variables wk ∈Rn,"
REFERENCES,0.7647058823529411,"min ρ⊤wk
s.t. wk(s) ≥rk(s, a, ˆσ−k) + γ P(s, a, ˆσ−k)wk,
∀s ∈S, ∀a ∈Ak."
REFERENCES,0.7668067226890757,"Each of these linear programs describes the problem of a single agent MDP —that agent being k.
It follows that the optimal w†
k for every program is unique (each program corresponds to a set of
Bellman optimality equations)."
REFERENCES,0.7689075630252101,"Properties of the NE program.
Second, we need to prove that the minimum value of the objective
function of the program is nonnegative."
REFERENCES,0.7710084033613446,"Lemma C.2 (Feasibility of (P′
NE) and global optimum). The nonlinear program (P′
NE) is feasible,
has a nonnegative objective value, and its global minimum is equal to 0."
REFERENCES,0.773109243697479,"Proof. For the feasibility of the nonlinear program, we invoke the theorem of the existence of
a Nash equilibrium. i.e., let a NE product policy, π⋆, and a vector w⋆∈Rn×H×S such that
w⋆
k,s(s) = V
†,π⋆
−k
k
(s), ∀k ∈[n] × S."
REFERENCES,0.7752100840336135,"By Lemma C.1, we know that (π⋆, w⋆) satisfies all the constraints of (P′
NE). Additionally, because"
REFERENCES,0.7773109243697479,"π⋆is a NE, V π⋆
k
(ρ) = V
†,π⋆
−k
k
(ρ) for all k ∈[n]. Observing that,"
REFERENCES,0.7794117647058824,"ρ⊤ 
w⋆
k −(I −γ P(π⋆))−1rk(π⋆)

= V
†,π⋆
−k
k
(ρ) −V π⋆
k
(ρ) = 0,"
REFERENCES,0.7815126050420168,concludes the argument that a NE attains an objective value equal to 0.
REFERENCES,0.7836134453781513,"Continuing, we observe that due to (1) the objective function can be equivalently rewritten as,
X k∈[n]"
REFERENCES,0.7857142857142857," 
ρ⊤wk −ρ⊤(I −γ P(π))−1rk(π)
 =
X"
REFERENCES,0.7878151260504201,"k∈[n]
ρ⊤wk −ρ⊤(I −γ P(π))−1 X"
REFERENCES,0.7899159663865546,"k∈[n]
rk(πh) =
X"
REFERENCES,0.792016806722689,"k∈[n]
ρ⊤wk."
REFERENCES,0.7941176470588235,"Next, we focus on the inequality constraint"
REFERENCES,0.7962184873949579,"wk(s) ≥rk(s, a, π−k) + γ P(s, a, π−k)wk"
REFERENCES,0.7983193277310925,"which holds for all s ∈S, all players k ∈[n], and all a ∈Ak."
REFERENCES,0.8004201680672269,"By summing over a ∈Ak while multiplying each term with a corresponding coefficient πk(a|s), the
display written in an equivalent element-wise vector inequality reads:"
REFERENCES,0.8025210084033614,"wk ≥rk,h(π) + γ P(π)wk."
REFERENCES,0.8046218487394958,"Finally, after consecutively substituting wk with the element-wise lesser term rk(π) + γ P( π)wk,
we end up with the inequality:"
REFERENCES,0.8067226890756303,"wk ≥(I −γ P(π))−1 rk(π).
(9)"
REFERENCES,0.8088235294117647,We note that I + γ P(π) + γ2 P2(π) + · · · = (I −γ P(π))−1.
REFERENCES,0.8109243697478992,"Summing over k, it holds for the s1-th entry of the inequality,
X"
REFERENCES,0.8130252100840336,"k∈[n]
wk ≥
X"
REFERENCES,0.8151260504201681,"k∈[n]
(I −γ P(π))−1 rk(π) = (I −γ P(π))−1 X"
REFERENCES,0.8172268907563025,"k∈[n]
rk(π) = 0."
REFERENCES,0.819327731092437,"Where the equality holds due to the zero-sum property, (1)."
REFERENCES,0.8214285714285714,"Theorem C.1 (NE and global optima of (P′
NE)—infinite-horizon). If (π⋆, w⋆) yields an ϵ-
approximate global minimum of (P′
NE), then π⋆is an nϵ-approximate NE of the infinite-horizon
zero-sum polymatrix switching controller MG, Γ. Conversely, if π⋆is an ϵ-approximate NE of the
MG Γ with corresponding value function vector w⋆such that w⋆
k(s) = V π⋆
k
(s)∀(k, s) ∈[n] × S,
then (π⋆, w⋆) attains an ϵ-approximate global minimum of (P′
NE)."
REFERENCES,0.8235294117647058,"Proof.
An approximate NE is an approximate global minimum. We show that an ϵ-approximate NE,
π⋆, achieves an nϵ-approximate global minimum of the program. Utilizing Lemma C.1 by setting
w⋆
k = V†,π⋆
−k(ρ), feasibility , and the definition of an ϵ-approximate NE we see that, X k∈[n]"
REFERENCES,0.8256302521008403,"
ρ⊤w⋆
k −ρ⊤(I −γ P(π⋆))−1 rk(π⋆)

=
X k∈[n]"
REFERENCES,0.8277310924369747,"
ρ⊤w⋆
k −V π⋆
k
(ρ)
 ≤
X"
REFERENCES,0.8298319327731093,"k∈[n]
ϵ = nϵ."
REFERENCES,0.8319327731092437,"Indeed, this means that π⋆, w⋆is an nϵ-approximate global minimizer of (P′
NE)."
REFERENCES,0.8340336134453782,"An approximate global minimum is an approximate NE. For this direction, we let a feasible
ϵ-approximate global minimizer of the program (P′
NE), (π⋆, w⋆). Because a global minimum of the
program is equal to 0, an ϵ-approximate global optimum must be at most ϵ > 0. We observe that for
every k ∈[n],"
REFERENCES,0.8361344537815126,"ρ⊤w⋆
k ≥ρ⊤(I −γ P(π⋆))−1 rk(π⋆),
(10)
which follows from induction on the inequality constraint (9)."
REFERENCES,0.8382352941176471,"Consequently, the assumption that"
REFERENCES,0.8403361344537815,"ϵ≥ρ⊤w⋆
k −ρ⊤(I −γ P(π⋆))−1 rk(π⋆)
and Equation (10), yields the fact that"
REFERENCES,0.842436974789916,"ϵ ≥ρ⊤w⋆
k −ρ⊤(I −γ P(π⋆))−1 rk(π⋆)"
REFERENCES,0.8445378151260504,"≥V
†,π⋆
−k
k
(ρ) −V π⋆
k
(ρ),
where the second inequality holds from the fact that w⋆is also feasible for (P′
BR). The latter
concludes the proof, as the display coincides with the definition of an ϵ-approximate NE."
REFERENCES,0.8466386554621849,"Theorem C.2 (CCE collapse to NE in polymatrix MG—infinite-horizon). Let a zero-sum polymatrix
switching-control Markov game, i.e., a Markov game for which Assumptions 1 and 2 hold. Further,
let an ϵ-approximate CCE of that game σ. Then, the marginal product policy πσ, with πσ
k (a|s) =
P"
REFERENCES,0.8487394957983193,"a−k∈A−k σ(a, a−k), ∀k ∈[n] is an nϵ-approximate NE."
REFERENCES,0.8508403361344538,"Proof. Let an ϵ-approximate CCE policy, σ, of game Γ. Moreover, let the best-response value-vectors
of each agent k to joint policy σ−k, w†
k."
REFERENCES,0.8529411764705882,"Now, we observe that due to Assumption 1,"
REFERENCES,0.8550420168067226,"w†
k(s) ≥rk(s, a, σ−k) + γ Ph(s, a, σ−k)w†
k
=
X"
REFERENCES,0.8571428571428571,"j∈adj(k)
r(k,j),h(s, a, πσ
j ) + γ P(s, a, σ−k)w†
k."
REFERENCES,0.8592436974789915,"Further, due to Assumption 2,"
REFERENCES,0.8613445378151261,"P(s, a, σ−k)w†
k = P(s, a, πσ
argctrl(s))w†
k,
or,"
REFERENCES,0.8634453781512605,"P(s, a, σ−k)w†
k = P(s, a, πσ)w†
k."
REFERENCES,0.865546218487395,"Putting these pieces together, we reach the conclusion that (πσ, w†) is feasible for the nonlinear
program (P′
NE)."
REFERENCES,0.8676470588235294,"What is left is to prove that it is also an ϵ-approximate global minimum. Indeed, if P"
REFERENCES,0.8697478991596639,"k ρ⊤w†
k≤ϵ
(by assumption of an ϵ-approximate CCE), then the objective function of (P′
NE) will attain an
ϵ-approximate global minimum. In turn, due to Theorem C.1 the latter implies that πσ is an
nϵ-approximate NE."
REFERENCES,0.8718487394957983,"C.1
No equilibrium collapse with more than one controllers per-state"
REFERENCES,0.8739495798319328,"Example 2. We consider the following 3-player Markov game that takes place for a time horizon
H = 3. There exist three states, s1, s2, and s3 and the game starts at state s1. Player 3 has a
single action in every state, while players 1 and 2 have two available actions {a1, a2} and {b1, b2}
respectively in every state. The initial state distribution ρ is the uniform probability distribution over
S."
REFERENCES,0.8760504201680672,"Reward functions.
If player 1 (respectively, player 2) takes action a1 (resp., b1), in either of the
states s1 or s2, they get a reward equal to
1
20. In state s3, both players get a reward equal to −1"
REFERENCES,0.8781512605042017,"2
regardless of the action they select. Player 3 always gets a reward that is equal to the negative sum
of the reward of the other two players. This way, the zero-sum polymatrix property of the game is
ensured (Assumption 1)."
REFERENCES,0.8802521008403361,"Transition probabilities.
If players 1 and 2 select the joint action (a1, b1) in state s1, the game
will transition to state s2. In any other case, it will transition to state s3. The converse happens if
in state s2 they take joint action (a1, b1); the game will transition to state s3. For any other joint
action, it will transition to state s1. From state s3, the game transition to state s1 or s2 uniformally
at random."
REFERENCES,0.8823529411764706,"At this point, it is important to notice that two players control the transition probability from one state
to another. In other words, Assumption 2 does not hold. s1
s2 s3"
REFERENCES,0.884453781512605,"1/2
1/2"
REFERENCES,0.8865546218487395,1 −π1(a1|s1)π2(b1|s1)
REFERENCES,0.8886554621848739,"π1(a1|s1)π2(b1|s1)
π1(a1|s2)π2(b1|s2)"
REFERENCES,0.8907563025210085,1 −π1(a1|s2)π2(b1|s2)
REFERENCES,0.8928571428571429,"Figure 2: A graph of the state space with transition probabilities parametrized with respect to the
policy of each player."
REFERENCES,0.8949579831932774,"Next, we consider the joint policy σ,"
REFERENCES,0.8970588235294118,σ(s1) = σ(s2) =
REFERENCES,0.8991596638655462,"b1
b2


a1
0
1/2"
REFERENCES,0.9012605042016807,"a2
1/2
0
."
REFERENCES,0.9033613445378151,Claim C.1. The joint policy σ that assigns probability 1
REFERENCES,0.9054621848739496,"2 to the joint actions (a1, b2) and (a2, b1) in
both states s1, s2 is a CCE and V σ
1 (ρ) = V σ
2 (ρ) = −1 10."
REFERENCES,0.907563025210084,Proof.
REFERENCES,0.9096638655462185,"V σ
1 (ρ) = ρ⊤(I −γ P(σ))−1 r1(σ) =
  1"
REFERENCES,0.9117647058823529,"3
1
3
1
3

 "
REFERENCES,0.9138655462184874,"9
5
6
5
0
6
5
9
5
0
1
1
1    "
REFERENCES,0.9159663865546218,"1
40
1
40
−1 2   = −1 10."
REFERENCES,0.9180672268907563,"We check every deviation,"
REFERENCES,0.9201680672268907,"• π1(s1) = π1(s2) = (1
0) , V π1×σ−1(ρ) = −2 5,"
REFERENCES,0.9222689075630253,"• π1(s1) = π1(s2) = (0
1) , V π1×σ−1(ρ) = −1 6,"
REFERENCES,0.9243697478991597,"• π1(s1) = (1
0) , π1(s2) = (0
1) , V π1×σ−1(ρ) = −5 16,"
REFERENCES,0.9264705882352942,"• π1(s1) = (0
1) , π1(s2) = (1
0) , V π1×σ−1(ρ) = −5 16."
REFERENCES,0.9285714285714286,For every such deviation the value of player 1 is smaller than −1
REFERENCES,0.930672268907563,"10. For player 2, the same follows by
symmetry. Hence, σ is indeed a CCE."
REFERENCES,0.9327731092436975,"Yet, the marginalized product policy of σ which we note as πσ
1 × πσ
2 does not constitute a NE. The
components of this policy are,







"
REFERENCES,0.9348739495798319,"





"
REFERENCES,0.9369747899159664,"πσ
1 (s1) = πσ
1 (s2) ="
REFERENCES,0.9390756302521008,"a1
a2
 

1/2
1/2 ,"
REFERENCES,0.9411764705882353,"πσ
2 (s1) = πσ
2 (s2) ="
REFERENCES,0.9432773109243697,"b1
b2
 

1/2
1/2 ."
REFERENCES,0.9453781512605042,"I.e., the product policy πσ
1 × πσ
2 selects any of the two actions of each player in states s1, s2
independently and uniformally at random. With the following claim, it can be concluded that in
general when more than one player control the transition the set of equilibria do not collapse.
Claim C.2. The product policy πσ
1 × πσ
2 is not a NE."
REFERENCES,0.9474789915966386,"Proof. For πσ = πσ
1 × πσ
2 we get,"
REFERENCES,0.9495798319327731,"V πσ
1
= ρ⊤(I −γ P(πσ))−1 r1(πσ) =
  1"
REFERENCES,0.9516806722689075,"3
1
3
1
3

 "
REFERENCES,0.9537815126050421,"34
21
20
21
3
7
20
21
34
21
3
7
6
7
6
7
9
7    "
REFERENCES,0.9558823529411765,"1
40
1
40
−1 2   = −3 10."
REFERENCES,0.957983193277311,"But, for the deviation π1(a1|s1) = π1(a1|s2) = 0, the value funciton of player 1, is equal to −1"
REFERENCES,0.9600840336134454,"6.
Hence, πσ is not a NE."
REFERENCES,0.9621848739495799,"In conclusion, Assumption 1 does not suffice to ensure equilibrium collapse.
Theorem C.3 (No collapse—infinite-horizon). There exists a zero-sum polymatrix Markov game
(Assumption 2 is not satisfied) that has a CCE which does not collapse to a NE."
REFERENCES,0.9642857142857143,"Proof. The proof follows from the game of Example 2, and Claims C.1 and C.2."
REFERENCES,0.9663865546218487,"D
An algorithm for approximating Markovian CCE"
REFERENCES,0.9684873949579832,"In this section we describe the algorithm in (Daskalakis et al., 2022) used for the computation
a of Markovian CCE. We note that M, Nvisit, p are parameters that affect the accuracy of the
approximation and we kindly ask the reader to refer to (Daskalakis et al., 2022) for further details.
This algorithm computes an ϵ-approximate CCE in time ˜O(ϵ−3). Newer works have improved the
dependence on ϵ to ˜O(ϵ−2), see (Wang et al., 2023; Cui et al., 2023)."
REFERENCES,0.9705882352941176,"As soon as an ϵ-approximate CCE is computed, what is left to do is to compute the marginal policy
of every player, πσ
k,h(a|s) = P"
REFERENCES,0.9726890756302521,"a−k∈A−k σ(a, a−k|s)."
REFERENCES,0.9747899159663865,"Algorithm 1 SPoCMAR (Daskalakis et al., 2022)"
REFERENCES,0.976890756302521,"1: procedure SPoCMAR(n, S, A, H, M, Nvisit, p)
2:
Set V = ∅.
3:
For each h ∈[H], s ∈S, set σcover
h,s
=⊥.
4:
for q ≥1 and while τ = 0 do
5:
Set τ = 1
#Terminate flag.
6:
Set Πq
h := {σcover
h,s
: s ∈S} for each h ∈[H].
7:
for h = H, H −1, . . . , 1 do
8:
Set k = 0, and ¯V q
i,H+1(s) = 0 for all s ∈S and i ∈[m].
9:
Each player i initializes an adversarial bandit for all (s, h) ∈S × [H].
10:
for each σ ∈Πq
h ∪σU do
# σU chooses actions uniformly at random
11:
for a total of M times do
12:
k ←k + 1.
13:
Let ¯σ be the policy which follows σ for the first h −1 steps and plays
according to the bandit algorithm for the state visited at step h (and acts arbitrarily for steps
h′ > h).
14:
Draw a joint trajectory (s1,m, a1,m, r1,m, . . . , sH,m, aH,m, rH,m) from ¯σ.
15:
if (h, sh,m) ∈V then
16:
Each
i
updates
its
bandit
alg.
at
(h, sh,m)
with"
REFERENCES,0.9789915966386554,"(ai,h,m,
H−ri,h,m−¯V q
i,h+1(sh+1,m)
H
).
17:
else
18:
Each i updates its bandit alg. at (h, sh,m) with (ai,h,m, H−(H+1−h)"
REFERENCES,0.9810924369747899,"H
).
19:
end if
20:
end for
21:
end for
22:
For each s ∈S, and j ≥1, let mj,h,s ∈[M + 1] denote the jth smallest value of k
so that sh,m = s, or M + 1 if such a jth smallest value does not exist.
23:
For each s ∈S, let Jh,s denote the largest integer j so that kj,h,s ≤M."
REFERENCES,0.9831932773109243,"24:
Define ˜σq
h ∈∆(A)S to be the 1-step policy: ˜σq
h(a|s) =
1
Jh,s
PJh,s
j=1 1[a =
ah,mj,h,s].
25:
Set"
REFERENCES,0.9852941176470589,"¯V q
i,h(s) :="
REFERENCES,0.9873949579831933,"(
1
Jh,s
PJh,s
j=1

ri,h,kj,h,s + ¯V q
i,h+1(sh+1,kj,h,s)

: (h, s) ∈V"
REFERENCES,0.9894957983193278,"(H + 1 −h)
: (h, s) ̸∈V."
REFERENCES,0.9915966386554622,"26:
end for
27:
Define the joint policy ˜σq, which follows ˜σq
h′ at each step h′ ∈[H]."
REFERENCES,0.9936974789915967,"28:
Call EstVisitation(˜σq, Nvisit) (Alg. 2) to obtain estimates ˆdq
h′ ∈∆(S) for each
h′ ∈[H].
29:
for each s ∈S and h′ ∈[H] do
30:
if ˆdq
h′(s) ≥p and (h′, s) ̸∈V then
31:
Set σcover
h′,s ←˜σq.
32:
Add (h′, s) to V.
33:
Set τ ←0.
34:
end if
35:
end for
36:
end for
37:
return the policy ˆσ := ˜σq.
38: end procedure"
REFERENCES,0.9957983193277311,Algorithm 2 EstVisitation
REFERENCES,0.9978991596638656,"1: procedure EstVisitation(σ, N)
2:
for 1 ≤n ≤N do
3:
Draw a trajectory from σ, and let (sn
1, . . . , sn
H) denote the sequence of states observed.
4:
end for
5:
for h ∈[H] do
6:
Let ˆdh ∈∆(S) denote the empirical distribution over (s1
h, . . . , sN
h ).
7:
end for
8:
return ( ˆd1, . . . , ˆdH).
9: end procedure"
