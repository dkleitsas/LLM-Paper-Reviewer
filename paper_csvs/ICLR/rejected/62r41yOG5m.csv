Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0038910505836575876,"Humans can decompose previous experiences into skills and reuse them to enable
fast learning in the future. Inspired by this process, we propose a new model called
Option-Controller Network (OCN), which is a bi-level recurrent policy network
composed of a high-level controller and a pool of low-level options. The options
are disconnected from any task-speciﬁc information to model task-agnostic skills.
The controller use options to solve a given task, and it calls one option at a time and
waits until the option return. With the isolation of information and the synchronous
calling mechanism, we can impose a division of works between the controller
and options in an end-to-end training regime. In experiments, we ﬁrst perform
behavior cloning from unstructured demonstrations coming from different tasks.
We then freeze the learned options and learn a new controller with an RL algorithm
to solve a new task. Extensive results on discrete and continuous environments
show that OCN can jointly learn to decompose unstructured demonstrations into
skills and model each skill with separate options. The learned options provide
a good temporal abstraction, allowing OCN to quickly transfer to tasks with
a novel combination of learned skills even with sparse reward, while previous
methods either suffer from the delayed reward problem due to the lack of temporal
abstraction or a complicated option controlling mechanism that increases the
complexity of exploration."
INTRODUCTION,0.007782101167315175,"1
INTRODUCTION"
INTRODUCTION,0.011673151750972763,"Acquiring primitive skills from demonstrations and reusing them to solve a novel long-horizon task
is a hallmark in human intelligence. For example, after learning the necessary skills (e.g., steering
wheel, changing lanes) at a driving school, one could be capable of driving across the country by
recombining the learned skills."
INTRODUCTION,0.01556420233463035,"In hierarchical reinforcement learning, different temporal abstractions (Sutton et al., 1999; Dietterich,
2000; Parr & Russell, 1998; Nakanishi et al., 2004) are proposed to achieve structured exploration
and transfer to a long-horizon task. However, when learning from scratch, these pure HRL methods
share an exploration problem: it takes a signiﬁcant amount of samples for random walk to induce a
good temporal abstraction that leads to positive rewards at the beginning of training. To circumvent
this issue, recent works (Le et al., 2018; Levy et al., 2018; Gupta et al., 2019; Jiang et al., 2019)
have focused on learning useful skills in a pretraining phase ﬁrst, and then reusing these skills when
ﬁnetuning with HRL in the new environment. However, these methods either assume the existence
of goal-conditioned policies or require intensive interaction with environments , which limits the
practical values of these approaches. One general approach is to leverage the additional unstructured
demonstrations during pretraining, e.g., compILE (Kipf et al., 2019) pretrains a VAE (Kingma &
Welling, 2013) on the demonstrations and uses an action decoder for ﬁnetuning. Our work is in this
line of research."
INTRODUCTION,0.019455252918287938,"In this paper, we propose Option-Control Network (OCN). Inspired by the option framework (Sutton
et al., 1999), OCN is a two-level recurrent policy network including a high-level controller and a
pool of low-level options. At each time step, a selected low-level option outputs an action and a
termination probability, and the high-level controller selects a new option whenever the old option is
terminated. Inspired by Lu et al. (2021), we enforce a special recurrent hidden state updating rule
to enforce the hierarchical constraint between the controller and the options so that the high-level"
INTRODUCTION,0.023346303501945526,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.027237354085603113,"controller is updated less frequently than the low-level options while keeping the model end-to-end
differentiable. As shown in Figure 1, OCN can jointly learn options and controllers with multitask
behavior cloning from unstructured demonstrations. When given a new task, one could perform
HRL ﬁnetuning by re-initializing the controller and freezing the options. This enables our model
to generalize combinatorially to unforeseen conjunctions (Denil et al., 2017). Unlike previous
works, our method does not require generative models (Eysenbach et al., 2018), goal-conditioned
policies (Gupta et al., 2019), pre-speciﬁed policy sketch (Shiarlis et al., 2018) or constraints on the
number of segments (Kipf et al., 2019), making our approach conceptually simple and general."
INTRODUCTION,0.0311284046692607,Demonstrations
INTRODUCTION,0.03501945525291829,Option
INTRODUCTION,0.038910505836575876,Option
INTRODUCTION,0.042801556420233464,Option
INTRODUCTION,0.04669260700389105,Option
INTRODUCTION,0.05058365758754864,Controller
INTRODUCTION,0.054474708171206226,GetWood
INTRODUCTION,0.058365758754863814,GoFactory
INTRODUCTION,0.0622568093385214,GetGrass
INTRODUCTION,0.06614785992217899,GetIron
INTRODUCTION,0.07003891050583658,Controller
INTRODUCTION,0.07392996108949416,"Behavior Cloning
HRL Finetune"
INTRODUCTION,0.07782101167315175,Re-initialize
INTRODUCTION,0.08171206225680934,GetWood
INTRODUCTION,0.08560311284046693,GoFactory
INTRODUCTION,0.08949416342412451,GetGrass
INTRODUCTION,0.0933852140077821,GetIron
INTRODUCTION,0.09727626459143969,Controller
INTRODUCTION,0.10116731517509728,GetWood
INTRODUCTION,0.10505836575875487,GoFactory
INTRODUCTION,0.10894941634241245,GetGrass
INTRODUCTION,0.11284046692607004,GetIron
INTRODUCTION,0.11673151750972763,Controller
INTRODUCTION,0.12062256809338522,"Figure 1: The training pipeline of OCN. Our model is composed of a controller (circle) and a options
pool (rectangles). The controller and options are randomly initialized, which means each option
does not correspond to a meaningful subtask. After behavior cloning, both options and controllers
are induced (marked blue) and the options correspond to meaningful subtasks from demonstrations
(e.g., get wood). Then we freeze the parameters in the options and re-initialize the controller. The
controller is trained to adapt to the new environment with HRL (marked red)."
INTRODUCTION,0.1245136186770428,"We perform experiments in Craft (Andreas et al., 2017), a grid-world environment focusing on
navigation and collecting objects, and Dial (Shiarlis et al., 2018), a robotic setting where a JACO
6DoF manipulator interact with a large number pad. Our results show that with unstructured
demonstrations, OCN can jointly learn to segment the trajectories into meaningful skills as well as
model this rich set of skills with our pool of low-level options. During HRL ﬁnetuning, we show
that OCN achieves better performance in more complex long-horizon tasks with either sparse or
dense reward compared with existing baselines. We also provide further visualization to show the
discovered options are reused during ﬁnetuning. Our contributions can be concluded as:"
INTRODUCTION,0.12840466926070038,"1. We propose Option-Controller Network, a bi-level recurrent policy network that uses a
special recurrent hidden state updating rule to enforce the hierarchical constraints.
2. We show that the OCN can discover the optimum options decomposition from unstructured
demonstration via multitasking behavior cloning.
3. We also demonstrate that the learned options can be coupled with a new controller to solve
an unseen long-horizon task via interaction with the environment."
RELATED WORK,0.13229571984435798,"2
RELATED WORK"
RELATED WORK,0.13618677042801555,"Learning to solve temporally extended tasks is an important question for Hierarchical Reinforcement
Learning (HRL), including option frameworks (Sutton et al., 1999), HAM (Parr & Russell, 1998)
and max-Q (Dietterich, 2000). With the popularity of neural nets, recent works propose to use a
bi-level neural network such as option critics (Bacon et al., 2017), feudal networks (Vezhnevets et al.,
2017), generative models with latents (Nachum et al., 2018), and modulated networks (Pashevich
et al., 2018). These models can be furthered combined with hindsight memory (Levy et al., 2018) to
increase the sample efﬁciency. Our work can also be viewed as designing a speciﬁc neural architecture
for HRL."
RELATED WORK,0.14007782101167315,"However, as discussed in Section 1, a pure HRL method suffers from serious exploration challenges
when learning from scratch (Gupta et al., 2019). A general approach to tackle this problem is to
introduce a pretraining phase to “warm up” the policy. Recent works propose to pretrain the policy"
RELATED WORK,0.14396887159533073,Under review as a conference paper at ICLR 2022
RELATED WORK,0.14785992217898833,"with an intrinsic diversity reward (Eysenbach et al., 2018) or language abstraction (Jiang et al.,
2019), which is shown to be useful in the HRL. However, assuming access to an environment in the
pretrain phase might be infeasible in many tasks. A principal approach is to leverage the additional
unstructured expert demonstrations and performs imitation learning during pretraining. Our work can
be viewed as solving the “cold start” problem for the option framework (Sutton et al., 1999)."
RELATED WORK,0.1517509727626459,"Recent works build upon this “imitation - ﬁnetune” paradigm. With the prevalence of goal-conditioned
policies (Schaul et al., 2015; Kaelbling, 1993; Levy et al., 2018) in robotics, these methods leverage
demonstrations with relabelling technique to pretrain the low-level policy (Gupta et al., 2019) or
a generative model (Lynch et al., 2020). However, they exploit the fact that the ending state of a
trajectory segment can be described as a point in the goal space. Hence it is difﬁcult to apply them
beyond goal-conditioned policies. CompILE (Kipf et al., 2019) treats the segment boundaries as
latent variables, and their model can be trained end-to-end with soft trajectory masking. However,
CompILE requires specifying the number of segments, while OCN only requires that the number
of options to be large enough (Appendix A.5. Following a similar VAE setting, SPiRL (Pertsch
et al., 2020) includes a skill prior and a skill autoencoder. They encoder skills into latent variables
and use a skill prior to generate high-level instructions – a sequence of latent variables. However,
SPiRL deﬁne skills as a sequence of actions with a ﬁx horizon H, which could prevent the model
from learning skills with clear semantics. Modular policy networks (Andreas et al., 2017; Shiarlis
et al., 2018) are also used in this paradigm, where each subtask corresponds to a single modular
policy. However, in this setting, the demonstration needs to be segmented beforehand, which requires
additional human labor. On the contrary, our work focused on using unstructured demonstrations.
OptionGAN (Henderson et al., 2018) proposes a Mixture-of-Expert (MoE) formulation and performs
IRL on the demonstration. However, without an explicit termination function, the learnt expert
networks do not provide time-extended actions for the high-level controller. As a result, this method
still suffers from problems of exploration with sparse rewards (as also seen in our experimental
comparison with an MoE baseline)."
RELATED WORK,0.1556420233463035,"Extracting meaningful trajectory segments from the unstructured demonstration is the focus of
Hierarchical Imitation Learning (HIL). These works can be summarized as ﬁnding the optimal
behavior hierarchy so that the behavior can be better predicted (Solway et al., 2014). DDO (Fox
et al., 2017) proposes an iterative EM-like algorithm to discover multiple levels of options, and
it is applied in the continuous action space (Krishnan et al., 2017) and program modelling (Fox
et al., 2018). VALOR (Achiam et al., 2018) extends this idea by incorporating powerful inference
methods like VAE (Kingma & Welling, 2013). Directed-Info GAIL (Sharma et al., 2018) extracts
meaning segments by maximizing the mutual information between the subtask latent variables and
the generated trajectory. Ordered Memory Policy Network (OMPN) (Lu et al., 2021) proposes a
hierarchical inductive bias to infer the skill boundaries. The above works mainly focus on skill
extraction, so it is unclear how to use the segmented skills for RL ﬁnetuning. Although OCN shares a
similar inductive bias with OMPN, OCN replaces the continuous hidden states communication with a
softmax distribution over multiple low-level modules (options). This enables OCN to model different
subtasks with different options and to effectively reuse them in a new task."
METHODOLOGY,0.15953307392996108,"3
METHODOLOGY"
METHODOLOGY,0.16342412451361868,"An Option-Controller Network (OCN) includes a set of N options {o1, ..., oN} and a controller
c. As shown in ﬁgure 1, the OCN starts by using the controller to choose an option to execute the
ﬁrst subtask. Once the subtask is done, the controller will choose another option to execute the
second subtask, and so on, until the goal of the task is achieved. OCN shares a similar inductive bias
with Ordered Memory Policy Network (OMPN) (Lu et al., 2021), that the lower-level components
(options) execute more frequently than the higher-level components (controllers). The inductive bias
enables OCN to induce the temporal hierarchical structure of unstructured demonstrations."
OPTION AND CONTROLLER,0.16731517509727625,"3.1
OPTION AND CONTROLLER"
OPTION AND CONTROLLER,0.17120622568093385,"Option
As shown in the middle of Figure 2, an option oi models a skill that can solve one speciﬁc
subtask, for example get wood, get iron or make at workbench. It can be described as:"
OPTION AND CONTROLLER,0.17509727626459143,"po
i,t, ho
i,t, ei,t = oi(xt, ho
i,t−1)
(1)"
OPTION AND CONTROLLER,0.17898832684824903,Under review as a conference paper at ICLR 2022
OPTION AND CONTROLLER,0.1828793774319066,"where xt is the observation at time step t, and ho
i,t−1 is the hidden state of the respective option at
time step t −1; ho
i,t is the hidden state of oi at time step t; ei,t is a scalar between 0 and 1, represents
the probability that the current option is done; po
i,t is a distribution of actions, including move up,
move left and use. These actions are the smallest elementary operations that an agent can execute.
During the execution of an option, if probability ei,t is 0, the option will keep executing the current
subtask; if ei,t is 1, the option will stop the execution and return to the controller for the next subtask.
In our work, each option maintains a separate set of parameters."
OPTION AND CONTROLLER,0.1867704280155642,"Figure 2: An example of OCN. The controller c models the task make bridge. Three options
separately model subtasks get iron, get wood or make at factory."
OPTION AND CONTROLLER,0.19066147859922178,"Controller
As shown at the top of Figure 2, a controller c models a higher level task, like make
bed, make axe, or make bridge. Each of these tasks can be decompose to a sequence of subtasks.
For example, make bridge can be decompose to 3 steps: 1) get iron, 2) get wood, 3) make at factory.
Thus a controller can also be represented as:"
OPTION AND CONTROLLER,0.19455252918287938,"pc
t , hc
t , ec
t = c(xt, hc
t−1)
(2)"
OPTION AND CONTROLLER,0.19844357976653695,"where pc
t is a distribution over the set of options {oi}, hc
t is the hidden state for controller, ec
t is the
probability that the current task is done. In this OCN architecture, we don’t need the ec
t, since the
environment will provide signal(reward) once the task is done. However, OCN can be easily expanded
to a multi-level model. In this multi-levels model, a set of multiple controllers become options for a
higher-level controller, and their respective tasks become subtasks for a more complicated task."
OPTION AND CONTROLLER,0.20233463035019456,"Cell Network
In OCN, options and controllers share the same format for input and output. Thus,
we parameterize them with the same neural network architecture. To model the policy of controllers
and options, we proposed the following cell network:"
OPTION AND CONTROLLER,0.20622568093385213,"ˆht = MLP ([xt, ht−1])
(3)"
OPTION AND CONTROLLER,0.21011673151750973,"pt = softmax(Wactˆht + bact)
(4)"
OPTION AND CONTROLLER,0.2140077821011673,"ht = tanh(Whidˆht + bhid)
(5)"
OPTION AND CONTROLLER,0.2178988326848249,"et = sigmoid(wendˆht + bend)
(6)"
OPTION AND CONTROLLER,0.22178988326848248,"xt is the raw observation, the shape of the vector depends on the environment. ht−1 is the recurrent
hidden state of size dhid, it allows the model to remember important information from previous time
steps. MLP is a multi-layer neural network of Depth lMLP and hidden size dMLP. We use tanh as
activation function for MLP. ˆht is a vector of size dMLP. Wact is a matrix of size nact × dMLP,
where nact is number of actions. Whid is a matrix of size dhid × dMLP. wend is a vector of size
dMLP. Following the fast and slow learning idea proposed in Madan et al. (2021), we introduce a
temperature term T to controller’s softmax function:"
OPTION AND CONTROLLER,0.22568093385214008,"pc
t = softmax"
OPTION AND CONTROLLER,0.22957198443579765,Wactˆht + bact T ! (7)
OPTION AND CONTROLLER,0.23346303501945526,"A large temperature T allows the option to output smoother distribution at the beginning of training.
It also reduces the scale of gradient backpropagated into the controller. This results in the controller
changes and updates slower than options. We found T makes OCN become more stable in imitation
learning and converge to a better hierarchical structure."
OPTION-CONTROLLER FRAMEWORK,0.23735408560311283,"3.2
OPTION-CONTROLLER FRAMEWORK"
OPTION-CONTROLLER FRAMEWORK,0.24124513618677043,"Given the deﬁnition for options and controllers, we can further formulate OCN. As shown in Figure
3a, at the ﬁrst time step, the controller computes a probability distribution over options for the ﬁrst"
OPTION-CONTROLLER FRAMEWORK,0.245136186770428,Under review as a conference paper at ICLR 2022
OPTION-CONTROLLER FRAMEWORK,0.2490272373540856,"(a) First time step
(b) Inside a subtask
(c) Switching between subtasks"
OPTION-CONTROLLER FRAMEWORK,0.2529182879377432,"Figure 3: The three different phase of OCN: (a) At the ﬁrst time step, the controller selects an option
oi; The option oi outputs the ﬁrst action a1. (b) If the previous option oi predict that the subtask is
not ﬁnish; The option oi then continue outputs action at; The controller hidden state is copied from
previous time step. (c) If the previous option oi predict that the subtask is done; The controller then
selects a new option oj and updates the controller hidden state; The new option oj outputs action
at. Blue arrows represent probability distributions output by controller and options. Red arrows
represent recurrent hidden states between time steps."
OPTION-CONTROLLER FRAMEWORK,0.25680933852140075,"subtask, and options execute their ﬁrst steps:"
OPTION-CONTROLLER FRAMEWORK,0.2607003891050584,"pc
1, hc
1 = c(x1, hc
0)
(8)
po
i,1, ho
i,1, ei,1 = oi(x1, ho
i,0)
(9)"
OPTION-CONTROLLER FRAMEWORK,0.26459143968871596,"pa
1 =
X"
OPTION-CONTROLLER FRAMEWORK,0.26848249027237353,"i
pc
1,ipo
i,1
(10)"
OPTION-CONTROLLER FRAMEWORK,0.2723735408560311,"where hc
0 and ho
i,0 are initial hidden states for controller and options, pa
1 is a distribution for actions.
The output pa
1 is formulated as a mixture of experts, where experts are options and the gating model
is the controller."
OPTION-CONTROLLER FRAMEWORK,0.27626459143968873,"At time steps t > 1, the options ﬁrst execute one step to decide whether this subtask is done. If the
subtask is unﬁnished, the option then outputs an action distribution, as shown in Figure 3b:"
OPTION-CONTROLLER FRAMEWORK,0.2801556420233463,"ˆpo
i,t, ˆho
i,t, ei,t = oi(xt, ho
i,t−1)
(11)"
OPTION-CONTROLLER FRAMEWORK,0.2840466926070039,"et =
X"
OPTION-CONTROLLER FRAMEWORK,0.28793774319066145,"i
pc
t−1,iei,t
(12)"
OPTION-CONTROLLER FRAMEWORK,0.2918287937743191,"ˆpa
i,t =
X"
OPTION-CONTROLLER FRAMEWORK,0.29571984435797666,"i
pc
t−1,iˆpo
i,t
(13)"
OPTION-CONTROLLER FRAMEWORK,0.29961089494163423,"where et is the probability that the previous subtask is done and ˆpa
i,t is the action distribution if the
subtask is not done. If the previous subtask is done, the controller c need to select a new option
distribution for the next subtask and reinitialize the option, as shown in Figure 3c:"
OPTION-CONTROLLER FRAMEWORK,0.3035019455252918,"p′c
t , h′c
t = c(xt, hc
t−1)
(14)"
OPTION-CONTROLLER FRAMEWORK,0.30739299610894943,"p′o
i,t, h′o
i,t, e′
i,t = oi(xt, ho
i,0)
(15)"
OPTION-CONTROLLER FRAMEWORK,0.311284046692607,"p′a
t =
X"
OPTION-CONTROLLER FRAMEWORK,0.3151750972762646,"i
p′c
t,ip′o
i,t
(16)"
OPTION-CONTROLLER FRAMEWORK,0.31906614785992216,"where h′c
t , h′c
t , p′c
t and p′c
t,i are hidden states and distributions for the next subtask if the previous
subtask is done. Thus, we can formulate the output at time step t as a weighted sum of the two
situations:
 "
OPTION-CONTROLLER FRAMEWORK,0.3229571984435798,"hc
t
pc
t
ho
t
pa
t "
OPTION-CONTROLLER FRAMEWORK,0.32684824902723736,= et  
OPTION-CONTROLLER FRAMEWORK,0.33073929961089493,"h′c
t
p′c
t
h′o
t
p′a
t "
OPTION-CONTROLLER FRAMEWORK,0.3346303501945525,+ (1 −et)  
OPTION-CONTROLLER FRAMEWORK,0.33852140077821014,"hc
t−1
pc
t−1
ˆho
t
ˆpa
t "
OPTION-CONTROLLER FRAMEWORK,0.3424124513618677,"
(17)"
OPTION-CONTROLLER FRAMEWORK,0.3463035019455253,"The equation 17 provides OCN an internal hierarchical inductive bias, that a higher-level component
(c) only update its recurrent hidden state and output a new command (p′c
t ) when its current functioning
subordinate (oi) reports “done”."
OPTION-CONTROLLER FRAMEWORK,0.35019455252918286,Under review as a conference paper at ICLR 2022
INDUCING AND REUSING SKILLS,0.3540856031128405,"3.3
INDUCING AND REUSING SKILLS"
INDUCING AND REUSING SKILLS,0.35797665369649806,"Imitation Learning and Inducing Skills
OCN imitates and induces skills from unstructured
demonstrations. In the rest of this paper, d represents an unstructured demonstration {(xt, at)}T
t=1
D represents a set of demonstrations of different tasks [(d1, τ1), (d2, τ2), ...], where τi are task ids,
belongs to a shared task set T."
INDUCING AND REUSING SKILLS,0.36186770428015563,"Given a demonstration d, OCN can perform behavior cloning with a negative log-likelihood loss:"
INDUCING AND REUSING SKILLS,0.3657587548638132,"loss = averaget (NLLLoss(pa
t , at))
(18)"
INDUCING AND REUSING SKILLS,0.36964980544747084,"For different tasks τ, we can use two different methods to model their associated controllers. The ﬁrst
method is to assign one controller cτ and a initial hidden state hc
τ,0 to each τ. The second method is to
share the controller c, but assign a different initial hidden state hc
τ,0 to each τ. We choose the second
method in this work because sharing c could avoid the risk that different controllers choose to model
the same subtask with options. During the imitation learning, OCN allows gradient backpropagation
through all probabilities p. Thus, the gradient descent will try to induce an optimal set of options that
can best increase the likelihood of the data."
INDUCING AND REUSING SKILLS,0.3735408560311284,"Reinforcement Learning and Reusing Skills
Given the induced options from imitation learning,
our model can learn to solve a new task by reusing these skills via reinforcement learning. For example,
after training on demonstrations of task 1 and task 2, OCN induce N options {o1, ..., oN}. Given a
new task 3 without demonstrations, we can initialize a new controller c3, that takes observations as
input and outputs a probability distribution over N induced options. To learn c3, we freeze all options
and use PPO (Schulman et al., 2017) algorithm to learn c3 from interactions with the environment.
During the training, once the controller outputs an option distribution pc, OCN samples from the
distribution, the sampled option will rollout until it’s done, then the process will repeat until the
task is solved. Thus, in the RL phase, our model only needs to explore at options space, which
signiﬁcantly reduces the number of interaction steps to solve the new tasks. We outline the process in
Appendix A.1."
EXPERIMENTS,0.377431906614786,"4
EXPERIMENTS"
EXPERIMENTS,0.38132295719844356,"We test OCN in two environments. For the discrete action space, we use a grid world environment
called Craft adapted from Andreas et al. (2017). For the continuous action space, we have a robotic
setting called Dial (Shiarlis et al., 2018), where a JACO 6DoF manipulator interacts with a number
pad. We compare OCN with three baselines including task decomposition methods and hierarchical
methods: (1) compILE (Kipf et al., 2019), which leverages Variational Auto-Encoder to recover
the subtask boundaries and models the subtasks with different options. (2) OMPN (Lu et al.,
2021) which studies inductive bias and discovers hierarchical structure from demonstrations. (3)
Mixture-of-Experts (MOE), which uses a similar architecture as OCN, but the controller predicts
a new distribution over options at every time step. This baseline is designed following the MoE
framework proposed in Henderson et al. (2018). Implementation details for OCN and baselines can
be found in Appendix A.2 and A.3."
CRAFT,0.3852140077821012,"4.1
CRAFT"
CRAFT,0.38910505836575876,"In this environment, an agent can move in a 2D grid map with actions (up, down, left, right) and
interact with the objects with the action use. The environment includes 4 subtasks: A) get wood,
B) get gold, C) get iron, D) get grass. They require the agent to locate and collect a speciﬁc type
of object. For example, subtask A, get wood, requires the agent to ﬁrst navigate to the block that
contains wood, then execute a use action to collect one unit of wood. A task requires the agent to
ﬁnish a sequence of subtasks in the given order. The environment can provide either sparse reward or
dense reward. In the dense reward, the agent receives rewards after completing each subtask while in
the sparse reward, the agent only receives rewards after completing all subtasks."
CRAFT,0.39299610894941633,"S1: Transferring from Single Agent
In this setting, the training task set is {AC, CD, DA}. Dur-
ing the imitation phase, we pretrain an OCN with one controller c1 and three options {o1, o2, o3}
to imitate these demonstrations. During the ﬁne-tuning phase, the model needs to solve three new
tasks: {ADC, CAD, DCA}. We initialize a new controller for each while freezing the parameters"
CRAFT,0.3968871595330739,Under review as a conference paper at ICLR 2022
CRAFT,0.40077821011673154,0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
CRAFT,0.4046692607003891,Million Frames 0.0 0.5 1.0 1.5 2.0
ADC-DENSE,0.4085603112840467,"2.5
ADC-dense"
ADC-DENSE,0.41245136186770426,0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
ADC-DENSE,0.4163424124513619,Million Frames 0.0 0.5 1.0 1.5 2.0
CAD-DENSE,0.42023346303501946,"2.5
CAD-dense"
CAD-DENSE,0.42412451361867703,0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
CAD-DENSE,0.4280155642023346,Million Frames 0.0 0.5 1.0 1.5 2.0
DCA-DENSE,0.43190661478599224,"2.5
DCA-dense"
DCA-DENSE,0.4357976653696498,0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
DCA-DENSE,0.4396887159533074,Million Frames 0.0 0.2 0.4 0.6 0.8
DCA-DENSE,0.44357976653696496,ADC-sparse
DCA-DENSE,0.4474708171206226,0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
DCA-DENSE,0.45136186770428016,Million Frames 0.0 0.2 0.4 0.6 0.8
DCA-DENSE,0.45525291828793774,CAD-sparse
DCA-DENSE,0.4591439688715953,0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
DCA-DENSE,0.46303501945525294,Million Frames 0.0 0.2 0.4 0.6 0.8
DCA-DENSE,0.4669260700389105,DCA-sparse
DCA-DENSE,0.4708171206225681,"MoE
OCN
OMPN
compile"
DCA-DENSE,0.47470817120622566,"Figure 4: The learning curve of different methods on three ﬁnetuning tasks of S1. dense means
dense reward setting. sparse means sparse reward setting."
DCA-DENSE,0.4785992217898833,"of options. This is the classical setting where an agent is required to learn skills from short expert
demonstrations and to transfer to long-horizon tasks."
DCA-DENSE,0.48249027237354086,"As is shown in Figure 4, our method converges faster and achieves higher performance than baselines
in both dense and sparse reward settings. With dense rewards, our method achieves double the returns
than the strongest baseline. In the sparse reward setting, our method can get an average return of 0.7
with the maximum being 1, while other baselines struggle. We ﬁnd that MoE fails to achieve similar
performance even with a very similar architecture as OCN. The only difference is that MoE does
not model the termination of an option and the controller selects a new option every time step. This
result shows that exploring in option space is more efﬁcient than other schemes, provided the new
task is expressible as a combination of previous observed subtasks."
DCA-DENSE,0.48638132295719844,"0.0
0.5
1.0
1.5
2.0
2.5
Million Frames 0.5 1.0 1.5 2.0 2.5 3.0"
DCA-DENSE,0.490272373540856,BADC-dense
DCA-DENSE,0.49416342412451364,"0.0
0.5
1.0
1.5
2.0
2.5
Million Frames 0.5 1.0 1.5 2.0 2.5"
ACBD-DENSE,0.4980544747081712,"3.0
ACBD-dense"
ACBD-DENSE,0.5019455252918288,"0.0
0.5
1.0
1.5
2.0
2.5
Million Frames 0.5 1.0 1.5 2.0 2.5 3.0"
CABD-DENSE,0.5058365758754864,"3.5
CABD-dense"
CABD-DENSE,0.5097276264591439,"0.0
0.5
1.0
1.5
2.0
2.5
Million Frames 0.0 0.1 0.2 0.3 0.4"
BADC-SPARSE,0.5136186770428015,"0.5
BADC-sparse"
BADC-SPARSE,0.5175097276264592,"0.0
0.5
1.0
1.5
2.0
2.5
Million Frames 0.0 0.1 0.2 0.3 0.4"
BADC-SPARSE,0.5214007782101168,ACBD-sparse
BADC-SPARSE,0.5252918287937743,"0.0
0.5
1.0
1.5
2.0
2.5
Million Frames 0.0 0.1 0.2 0.3 0.4 0.5"
BADC-SPARSE,0.5291828793774319,CABD-sparse
BADC-SPARSE,0.5330739299610895,"MoE
OCN
compile"
BADC-SPARSE,0.5369649805447471,"Figure 5: The learning curve of different methods on three ﬁnetuning tasks of S2. OMPN is not
included because it does not learn an explicit set of options."
BADC-SPARSE,0.5408560311284046,Under review as a conference paper at ICLR 2022
BADC-SPARSE,0.5447470817120622,"S2: Transferring from Multiple Agents
In this setting, we have two disjoint task sets. The
ﬁrst set is {AB, BA} and the second task set is {CD, DC}. We train two separate OCN models.
Each model includes a controller and two options. Thus, at the end of imitation phase, we ob-
tain four options {o1, ..., o4}. Then we initialize three new controllers to solve three new tasks:
{BADC, ACBD, CABD}."
BADC-SPARSE,0.5486381322957199,"This setting is related to the problem of data islands and federated learning (Yang et al., 2019), where
two companies could each pretrain models on their separate datasets, merge the induced options, and
share the controller ﬁnetuned on more challenging tasks. This is made possible because of the highly
modularized design of our architecture."
BADC-SPARSE,0.5525291828793775,"The results are shown in Figure 5. We show that OCN can still reuse merged option pools, while other
baseline methods fail at this setting. CompILE uses a continuous latent variable for communication
between the controller and the action decoder, which causes compatibility issues while merging skills
from different models. The MoE method still suffers from the long horizon problem. Overall, this
result highlights the ﬂexibility of OCN and its promise in maintaining data privacy for collaborative
machine learning applications."
DIAL,0.556420233463035,"4.2
DIAL"
DIAL,0.5603112840466926,"0
2
4
6
8
10
Million Frames 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
DIAL,0.5642023346303502,"[1, 4, 8]-dense"
DIAL,0.5680933852140078,"0
2
4
6
8
10
Million Frames 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00"
DIAL,0.5719844357976653,"[8, 4, 2]-dense"
DIAL,0.5758754863813229,"0
2
4
6
8
10
Million Frames 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
DIAL,0.5797665369649806,"2.00
[4, 1, 2]-dense"
DIAL,0.5836575875486382,"0
2
4
6
8
10
Million Frames 0.00 0.05 0.10 0.15 0.20"
DIAL,0.5875486381322957,"[1, 4, 8]-sparse"
DIAL,0.5914396887159533,"0
2
4
6
8
10
Million Frames 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
DIAL,0.5953307392996109,"[8, 4, 2]-sparse"
DIAL,0.5992217898832685,"0
2
4
6
8
10
Million Frames 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
DIAL,0.603112840466926,"[4, 1, 2]-sparse"
DIAL,0.6070038910505836,"MOE
OCN
OMPN"
DIAL,0.6108949416342413,"Figure 6: The learning curve of different methods on three ﬁnetuning tasks of Dial. dense means
dense reward setting. sparse means sparse reward setting."
DIAL,0.6147859922178989,"In this experiment, the task requires the agent to move the robotic arm to dial a PIN – a sequence of
digits that should be pressed in the given order. We choose a set S = (1, 2, 4, 8) of 4 digits as the
environment. Demonstrations contain state-action trajectories that the robotic arm press a 2-digits
pin randomly sampled from S. Demonstrations are generated from a PID controller that moves the
robotic arm to predeﬁned joint angles for each digit. The ﬁne-tuning task is dialing a 3-digits PIN
sampled from S. The environment can provide either sparse rewards or dense rewards. In the dense
reward setting, the agent can receive a reward after pressing each digit in the PIN. In the sparse reward
setting, the agent can only receive a reward if all the digits in the PIN are pressed. The state has
39-dimensions which containing the 9 joint angles and the distance from each digit on the dial pad
to the ﬁnger of the robotic arm in three dimensions. We compare OCN with two baseline methods:
OMPN and MoE."
DIAL,0.6186770428015564,"Figure 6 shows the learning curve of three different ﬁne-tuning tasks. We ﬁnd that OCN consistently
outperforms baseline methods. It’s worth noting that, although OCN is designed to learn a discrete
transition between subtasks, it is still capable of providing a good solution for continuous settings."
DIAL,0.622568093385214,Under review as a conference paper at ICLR 2022
MODEL ANALYSIS,0.6264591439688716,"4.3
MODEL ANALYSIS"
MODEL ANALYSIS,0.6303501945525292,"Visualization
Appendix A.6 shows several trajectories produced by OCN, including imitation
learning trajectories and reinforcement learning trajectories. As shown in Figure 10 and 11, OCN can
accurately segment the demonstration into different subtasks and unambiguously associate options
with subtasks. At the reinforcement learning phase, the discovered options are reused to solve their
assigned tasks when coupled with a new controller to solve a long horizon task."
MODEL ANALYSIS,0.6342412451361867,"0
1
2
3
4
5
Million Frames 0.70 0.75 0.80 0.85 0.90 0.95 1.00"
MODEL ANALYSIS,0.6381322957198443,F1_Tol1
MODEL ANALYSIS,0.642023346303502,"0
1
2
3
4
5
Million Frames 0.6 0.7 0.8 0.9 1.0"
MODEL ANALYSIS,0.6459143968871596,Align_Succ
MODEL ANALYSIS,0.6498054474708171,"0
1
2
3
4
5
Million Frames 0.0 0.2 0.4 0.6 0.8 1.0 NMI"
MODEL ANALYSIS,0.6536964980544747,"OCN
OCN,T=1
OMPN"
MODEL ANALYSIS,0.6575875486381323,"Figure 7: Comparison of unsupervised trajectory parsing results during the imitation phase with
OMPN (Lu et al., 2021). The F1 score with tolerance (Left) and Task Alignment (Center) show the
quality of learned task boundaries. The normalized mutual information (Right) between the emerged
option selection pc
t and the ground-truth shows that OCN learns to associate each option to one
subtask. T=1 means that the temperature term in the controller is removed."
MODEL ANALYSIS,0.6614785992217899,"Table 1: The success rate of each option when
testing on different subtasks."
MODEL ANALYSIS,0.6653696498054474,"Option
subtask
A
C
D"
MODEL ANALYSIS,0.669260700389105,"1
0.96
0.07
0.03
2
0.00
0.02
0.95
3
0.01
0.98
0.01"
MODEL ANALYSIS,0.6731517509727627,"Quantitative Analysis
Figure 7 shows the perfor-
mances of parsing and option-subtask correlation
during the imitation phase. We ﬁnd that OCN can
converge faster and achieve better parsing perfor-
mance than the OMPN model. The NMI ﬁgure
in Figure 7 shows that, during the imitation phase,
randomly initialized options slowly converged to
model different subtasks. At the end of imitation,
OCN shows strong alignment between options and
subtasks. In 4 out of 5 runs, OCN actually achieves
NMI=1, which means that the alignment between
option and subtask is perfect. On the other hand, if we remove the temperature term (i.e. set T = 1) in
the controller, the NMI drops signiﬁcantly. This result suggests that the fast and slow learning schema
is important for the model to learn the correct alignment between options and subtasks. Furthermore,
Table 1 shows the success rate of using each option to solve each subtask. We ﬁnd that there is a
one-to-one correspondence between subtasks and learned options."
MODEL ANALYSIS,0.6770428015564203,"In the Appendix A.5, we show that OCN is not sensitive to the number of options N, as long as it’s
larger than the intrinsic number of skills in demonstrations. If N is too small, then the performance
of skill transfer may decrease. So it’s always recommended to choose a larger N."
CONCLUSION,0.6809338521400778,"5
CONCLUSION"
CONCLUSION,0.6848249027237354,"In this paper, we proposed a novel framework: Option-Controller Network(OCN). It is composed of
a controller and a set of options. After training on a set of demonstrations, OCN can automatically
discover the temporal hierarchical structure of training tasks and assign different options to solve
different subtasks in the hierarchy. Experiment results show that our model can effectively associate
subtasks with options. And a newly initialized controller can leverage previously learned options to
solve complicated long horizon tasks via interaction with the environment. In this ﬁnetuning process,
OCN shows strong combinatorial generalization. It outperforms previous baseline methods by a large
margin. Overall, this method provides a simple and effective framework for hierarchical imitation
learning and reinforcement learning in discrete space."
CONCLUSION,0.688715953307393,Under review as a conference paper at ICLR 2022
REFERENCES,0.6926070038910506,REFERENCES
REFERENCES,0.6964980544747081,"Joshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option discovery
algorithms. arXiv preprint arXiv:1807.10299, 2018. 3"
REFERENCES,0.7003891050583657,"Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy
sketches. In International Conference on Machine Learning, pp. 166–175. PMLR, 2017. 2, 3, 6"
REFERENCES,0.7042801556420234,"Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, 2017. 2"
REFERENCES,0.708171206225681,"Misha Denil, Sergio Gómez Colmenarejo, Serkan Cabi, David Saxton, and Nando de Freitas. Pro-
grammable agents. arXiv preprint arXiv:1706.06383, 2017. 2"
REFERENCES,0.7120622568093385,"Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decomposi-
tion. Journal of artiﬁcial intelligence research, 13:227–303, 2000. 1, 2"
REFERENCES,0.7159533073929961,"Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine.
Diversity is all you
need: Learning skills without a reward function.
In International Conference on Learning
Representations, 2018. 2, 3"
REFERENCES,0.7198443579766537,"Roy Fox, Sanjay Krishnan, Ion Stoica, and Ken Goldberg. Multi-level discovery of deep options.
arXiv preprint arXiv:1703.08294, 2017. 3"
REFERENCES,0.7237354085603113,"Roy Fox, Richard Shin, Sanjay Krishnan, Ken Goldberg, Dawn Song, and Ion Stoica. Parametrized
hierarchical procedures for neural programming. ICLR, 2018. 3"
REFERENCES,0.7276264591439688,"Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy
learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint
arXiv:1910.11956, 2019. 1, 2, 3"
REFERENCES,0.7315175097276264,"Peter Henderson, Wei-Di Chang, Pierre-Luc Bacon, David Meger, Joelle Pineau, and Doina Precup.
Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforcement
learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018. 3, 6"
REFERENCES,0.7354085603112841,"Yiding Jiang, Shixiang Gu, Kevin Murphy, and Chelsea Finn. Language as an abstraction for
hierarchical deep reinforcement learning. arXiv preprint arXiv:1906.07343, 2019. 1, 3"
REFERENCES,0.7392996108949417,"Leslie Pack Kaelbling. Learning to achieve goals. In IJCAI, pp. 1094–1099. Citeseer, 1993. 3"
REFERENCES,0.7431906614785992,"Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
arXiv:1312.6114, 2013. 1, 3"
REFERENCES,0.7470817120622568,"Thomas Kipf, Yujia Li, Hanjun Dai, Vinicius Zambaldi, Alvaro Sanchez-Gonzalez, Edward Grefen-
stette, Pushmeet Kohli, and Peter Battaglia. Compile: Compositional imitation learning and
execution. In International Conference on Machine Learning, pp. 3418–3428. PMLR, 2019. 1, 2,
3, 6"
REFERENCES,0.7509727626459144,"Sanjay Krishnan, Roy Fox, Ion Stoica, and Ken Goldberg. Ddco: Discovery of deep continuous
options for robot learning from demonstrations. In Conference on Robot Learning, pp. 418–437.
PMLR, 2017. 3"
REFERENCES,0.754863813229572,"Hoang Le, Nan Jiang, Alekh Agarwal, Miroslav Dudík, Yisong Yue, and Hal Daumé. Hierarchical
imitation and reinforcement learning. In International Conference on Machine Learning, pp.
2917–2926. PMLR, 2018. 1"
REFERENCES,0.7587548638132295,"Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical reinforcement learning with hindsight.
arXiv preprint arXiv:1805.08180, 2018. 1, 2, 3"
REFERENCES,0.7626459143968871,"Yuchen Lu, Yikang Shen, Siyuan Zhou, Aaron Courville, Joshua B. Tenenbaum, and Chuang
Gan.
Learning task decomposition with ordered memory policy network.
In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=vcopnwZ7bC. 1, 3, 6, 9"
REFERENCES,0.7665369649805448,Under review as a conference paper at ICLR 2022
REFERENCES,0.7704280155642024,"Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre
Sermanet. Learning latent plans from play. In Conference on Robot Learning, pp. 1113–1132.
PMLR, 2020. 3"
REFERENCES,0.77431906614786,"Kanika Madan, Rosemary Nan Ke, Anirudh Goyal, Bernhard Bernhard Schölkopf, and Yoshua Ben-
gio. Fast and slow learning of recurrent independent mechanisms. arXiv preprint arXiv:2105.08710,
2021. 4"
REFERENCES,0.7782101167315175,"Oﬁr Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Data-efﬁcient hierarchical reinforce-
ment learning. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, pp. 3307–3317, 2018. 2"
REFERENCES,0.7821011673151751,"Jun Nakanishi, Jun Morimoto, Gen Endo, Gordon Cheng, Stefan Schaal, and Mitsuo Kawato.
Learning from demonstration and adaptation of biped locomotion. Robotics and autonomous
systems, 47(2-3):79–91, 2004. 1"
REFERENCES,0.7859922178988327,"Ronald Parr and Stuart J Russell. Reinforcement learning with hierarchies of machines. In Advances
in neural information processing systems, pp. 1043–1049, 1998. 1, 2"
REFERENCES,0.7898832684824902,"Alexander Pashevich, Danijar Hafner, James Davidson, Rahul Sukthankar, and Cordelia Schmid.
Modulated policy hierarchies. arXiv preprint arXiv:1812.00025, 2018. 2"
REFERENCES,0.7937743190661478,"Karl Pertsch, Youngwoon Lee, and Joseph J Lim. Accelerating reinforcement learning with learned
skill priors. arXiv preprint arXiv:2010.11944, 2020. 3"
REFERENCES,0.7976653696498055,"Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators.
In International conference on machine learning, pp. 1312–1320. PMLR, 2015. 3"
REFERENCES,0.8015564202334631,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 6"
REFERENCES,0.8054474708171206,"Mohit Sharma, Arjun Sharma, Nicholas Rhinehart, and Kris M Kitani. Directed-info gail: Learning
hierarchical policies from unsegmented demonstrations using directed information. In International
Conference on Learning Representations, 2018. 3"
REFERENCES,0.8093385214007782,"Kyriacos Shiarlis, Markus Wulfmeier, Sasha Salter, Shimon Whiteson, and Ingmar Posner. Taco:
Learning task decomposition via temporal alignment for control. In International Conference on
Machine Learning, pp. 4654–4663, 2018. 2, 3, 6"
REFERENCES,0.8132295719844358,"Alec Solway, Carlos Diuk, Natalia Córdova, Debbie Yee, Andrew G Barto, Yael Niv, and Matthew M
Botvinick. Optimal behavioral hierarchy. PLOS Comput Biol, 10(8):e1003779, 2014. 3"
REFERENCES,0.8171206225680934,"Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework
for temporal abstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-2):181–211, 1999.
1, 2, 3"
REFERENCES,0.8210116731517509,"Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David
Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. arXiv
preprint arXiv:1703.01161, 2017. 2"
REFERENCES,0.8249027237354085,"Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and
applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1–19, 2019.
8"
REFERENCES,0.8287937743190662,Under review as a conference paper at ICLR 2022
REFERENCES,0.8326848249027238,"A
APPENDIX"
REFERENCES,0.8365758754863813,"A.1
REINFORCEMENT LEARNING ALGORITHM"
REFERENCES,0.8404669260700389,"Algorithm 1: PPO, Adapt OCN to a new task"
REFERENCES,0.8443579766536965,"Initialize controller c;
Freeze all options {o1...N};
for iterations=1,2,... do"
REFERENCES,0.8482490272373541,"for actor=1,2,... do"
REFERENCES,0.8521400778210116,"for step t=1,2,...,T do"
REFERENCES,0.8560311284046692,"pc = c(xt, hc
t−1);
i = sample(pc);
Rollout oi until sample(ei) = 1;
end
Compute advantage estimates ˆA1, ..., ˆAT ;
end
Optimize surrogate L wrt c, with K epochs and minibatch size B;
end"
REFERENCES,0.8599221789883269,"A.2
IMPLEMENTATION DETAILS"
REFERENCES,0.8638132295719845,"We train all imitation learning methods by utilizing behaviour cloning with a batch size of 512 and a
learning rate of 0.001. For each task, we sample 6000 demonstrations and split 80% for training and
20% for validation. For reinforcement learning, we use PPO algorithm with a batch size of 1024 and
a learning rate of 0.0003. We use Adam optimizer and a linear schedule to adjust the learning rate.
The hidden state size dhid of OCN and baselines is 128. The depth lMLP of cell network is 2. The
temperature T for controller’s softmax function is 10. The hyparameters used in IL and RL are listed
in table 2 and 3. We perform imitation learning and reinforcement learning phase with a Tesla V100
GPU."
REFERENCES,0.867704280155642,Table 2: Imitation Learning Parameters
REFERENCES,0.8715953307392996,"Hyparameter
Value"
REFERENCES,0.8754863813229572,"batch size
512
learning rate
0.001
train episodes
1500
hidden size
128
optimizer
Adam
temperature for Softmax
10"
REFERENCES,0.8793774319066148,Table 3: Reinforcement Learning Parameters
REFERENCES,0.8832684824902723,"Hyparameter
Value"
REFERENCES,0.8871595330739299,"batch size
256
learning rate
0.0003
hidden size
128
entropy
0.001
ppo epoch
4
gamma
0.97
optimizer
Adam"
REFERENCES,0.8910505836575876,"A.3
BASELINES"
REFERENCES,0.8949416342412452,"compILE
We modify the encoder and decoder of compILE so that the model can adapt to the
observation of our environment. We use the discrete latent as the original paper describes. We
re-initialize the encoder to predict latent for the new tasks and freeze the decoder which predicts the
actions in RL."
REFERENCES,0.8988326848249028,"MoE
This baseline has a similar architecture with OCN, but it doesn’t have a terminate signal and
has to predict options distribution and select options at each step. We pretrain MoE with a controller
and options in the imitation phase and re-initialize a new controller for the new tasks while freezing
options as we do with OCN."
REFERENCES,0.9027237354085603,"OMPN
We can directly use this baseline in the imitation phase. However, since OMPN has
strong connections(e.g., bottom-up and top-down recurrence) between the higher-level model and
lower-level model, we don’t re-initialize the higher-level model in RL."
REFERENCES,0.9066147859922179,Under review as a conference paper at ICLR 2022
REFERENCES,0.9105058365758755,"A.4
TASK ALIGNMENT EVALUATION"
REFERENCES,0.914396887159533,"We use three metrics to evaluate the performance of parsing and option-subtask correlation: a) task
align accuracy, b) F1 scores with tolerance and c) normalized mutual information scores(NMI). The
ﬁrst two metrics we use are the same as those deﬁned in OMPN. The NMI is a normalization of the
mutual information score between two clusterings, which are subtasks and options in our experiment.
We give the formulation of NMI between two clusterings U, V as:"
REFERENCES,0.9182879377431906,"NMI(U, V ) =
MI(U, V )
mean(H(U), H(V ))"
REFERENCES,0.9221789883268483,"where MI(U, V ) is the mutual information and H is the entropy. The values of NMI is from 0 to 1.
The higher values mean the higher correlation between subtasks and options."
REFERENCES,0.9260700389105059,"A.5
HYPERPARAMETERS ANALYSIS"
REFERENCES,0.9299610894941635,"Our model does not require the assumption about the number of skills. We analyze the effect of the
number of options K. As shown in Figure 8, when K is larger than or equal to the number of skills,
which is 3 in this experiment, our model basically remains similar results at three metrics: Align Acc,
F1 Tol1, and NMI and achieve almost 1. When K = 2, which means K is smaller than the number
of skills, one of the options must execute two different skills, which is contrary to our assumption and
only achieves 0.4 at NMI. We also compare the prediction accuracy of the actions and the returns
in Figure 9. Our performance isn’t inﬂuenced by the number of skills when K is larger than the
number of skills."
REFERENCES,0.933852140077821,"0
1
2
3
4
5
Million Frames 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00"
REFERENCES,0.9377431906614786,F1_Tol1
REFERENCES,0.9416342412451362,"0
1
2
3
4
5
Million Frames 0.70 0.75 0.80 0.85 0.90 0.95 1.00"
REFERENCES,0.9455252918287937,Align_Acc
REFERENCES,0.9494163424124513,"0
1
2
3
4
5
Million Frames 0.0 0.2 0.4 0.6 0.8 1.0 NMI"
REFERENCES,0.953307392996109,"K=2
K=3
K=4
K=5"
REFERENCES,0.9571984435797666,"Figure 8: Comparison of parsing results during different K at F1 scores with tolerance, task align
accuracy and NMI."
REFERENCES,0.9610894941634242,"0
1
2
3
4
5
Million Frames 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Acc"
REFERENCES,0.9649805447470817,"0
1
2
3
4
5
Million Frames 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
RETURNS,0.9688715953307393,"2.00
Returns"
RETURNS,0.9727626459143969,"K=2
K=3
K=4
K=5"
RETURNS,0.9766536964980544,Figure 9: Comparison of prediction accuracy of actions and the returns during different K
RETURNS,0.980544747081712,Under review as a conference paper at ICLR 2022 Agent Grass Wood Iron Gold Agent Grass Wood Iron Gold
RETURNS,0.9844357976653697,"Figure 10: Visualizations of different tasks in S1. Different colored lines correspond to different
subtasks. Agent Grass Wood Iron Gold"
RETURNS,0.9883268482490273,Figure 11: Visualization of a task in S2.
RETURNS,0.9922178988326849,"A.6
VISUALIZATION"
RETURNS,0.9961089494163424,"We show more visualization results in Figure 10 and 11. switch indicates what time the previous
option ends and switches to the new option. The options distribution is computed with pc
t. In
Figure 10, the model is trained on different tasks to learn skills(A,C,D) which can be reused in the
new tasks(ADC,CAD). In Figure 11, there are two separate OCN models trained on two disjoints
tasks sets(AB,BA,CD,DC). The four options can be obtained and reused to solve a new task(ACBD)."
