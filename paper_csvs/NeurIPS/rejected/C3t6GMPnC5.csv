Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010050251256281408,"Mamba [22] state-space models (SSMs) have recently outperformed state-of-the-
1"
ABSTRACT,0.0020100502512562816,"art (SOTA) Transformer large language models (LLMs) in various tasks and been
2"
ABSTRACT,0.003015075376884422,"widely adapted. However, Mamba’s downstream learning capabilities remain ei-
3"
ABSTRACT,0.004020100502512563,"ther unexplored–e.g., mixed-precision (MPFT) and parameter-efficient fine-tuning
4"
ABSTRACT,0.005025125628140704,"(PEFT)–or under-evaluated–e.g., in-context learning (ICL). For the latter, recent
5"
ABSTRACT,0.006030150753768844,"works [45, 19] reported Mamba’s ICL rivals SOTA Transformer LLMs using non-
6"
ABSTRACT,0.007035175879396985,"standard benchmarks. In contrast, we show that on standard benchmarks, pretrained
7"
ABSTRACT,0.008040201005025126,"Mamba models achieve only 38% of the ICL performance improvements (over
8"
ABSTRACT,0.009045226130653266,"zero-shot) of comparable Transformers.
9"
ABSTRACT,0.010050251256281407,"Enabling MPFT and PEFT in Mamba architectures is challenging due to recurrent
10"
ABSTRACT,0.011055276381909548,"dynamics and highly customized CUDA kernels, respectively. However, we prove
11"
ABSTRACT,0.012060301507537688,"that Mamba’s recurrent dynamics are robust to small input changes using dynamical
12"
ABSTRACT,0.01306532663316583,"systems theory. Empirically, we show that performance changes in Mamba’s
13"
ABSTRACT,0.01407035175879397,"inference and fine-tuning due to mixed-precision align with Transformer LLMs.
14"
ABSTRACT,0.01507537688442211,"Furthermore, we show that targeting key memory buffers in Mamba’s customized
15"
ABSTRACT,0.016080402010050253,"CUDA kernels for low-rank adaptation regularizes SSM parameters, thus achieving
16"
ABSTRACT,0.017085427135678392,"parameter efficiency while retaining speedups. We show that combining MPFT and
17"
ABSTRACT,0.018090452261306532,"PEFT enables up to 2.15 times more tokens-per-second and 65.5% reduced per-
18"
ABSTRACT,0.019095477386934675,"token-memory compared to full Mamba fine-tuning, while achieving up to 81.5%
19"
ABSTRACT,0.020100502512562814,"of the ICL performance improvements (over zero-shot) of comparably fine-tuned
20"
ABSTRACT,0.021105527638190954,"Transformers.
21"
INTRODUCTION,0.022110552763819097,"1
Introduction
22"
INTRODUCTION,0.023115577889447236,"Innovating on previous state-space models (SSMs) [23, 11], Mamba [22] has been recently proposed
23"
INTRODUCTION,0.024120603015075376,"as an accurate, sub-quadratic alternative to Transformer large language models (LLMs). Mamba was
24"
INTRODUCTION,0.02512562814070352,"initially shown to greatly outperform comparable Transformer LLMs [5] across a large number of
25"
INTRODUCTION,0.02613065326633166,"standard natural language benchmarks. Subsequently, pretrained Mamba models have been widely
26"
INTRODUCTION,0.027135678391959798,"adapted across different data modalities [42, 65, 36, 46, 37], tasks [60, 62, 48, 63, 57, 37, 2], and
27"
INTRODUCTION,0.02814070351758794,"architectures [1, 45, 40].
28"
INTRODUCTION,0.02914572864321608,"However, despite such rapid and widespread adaptation, evaluation of Mamba’s ability to perform
29"
INTRODUCTION,0.03015075376884422,"standard downstream learning abilities exhibited by Transformer-based LLMs have either not been
30"
INTRODUCTION,0.031155778894472363,"extensively conducted on standard natural benchmarks or are completely lacking. For instance, while
31"
INTRODUCTION,0.032160804020100506,"recent works [45, 19, 30] have evaluated Mamba’s ability to perform in-context learning (ICL), such
32"
INTRODUCTION,0.033165829145728645,"studies focused extensively on either non-natural tasks [30, 17] or non-standard benchmarks [25].
33"
INTRODUCTION,0.034170854271356785,*Equal Contribution
INTRODUCTION,0.035175879396984924,"Furthermore, evaluation of Mamba’s mixed-precision fine-tuning (MPFT) and performance efficient
34"
INTRODUCTION,0.036180904522613064,"fine-tuning (PEFT) capabilities are currently lacking. For the former, MPFT (and, by extension,
35"
INTRODUCTION,0.0371859296482412,"mixed-precision inference) are made difficult due to potential sensitivities of Mamba’s recurrent
36"
INTRODUCTION,0.03819095477386935,"dynamics, where [21, 29] suggest full precision (FP32) is required to perform stable training. For
37"
INTRODUCTION,0.03919597989949749,"the latter, PEFT via standard low-rank adaptation (LoRA) [28] is made difficult within Mamba’s
38"
INTRODUCTION,0.04020100502512563,"SSM layer (referred to herein as the MambaBlock) due highly customized SSM CUDA kernels which
39"
INTRODUCTION,0.04120603015075377,"provide competitive performance to attention-based speedups [10] at the cost of standard adapter
40"
INTRODUCTION,0.04221105527638191,"support. However, PEFT and MPFT are arguably two of the most widely utilized techniques for LLM
41"
INTRODUCTION,0.04321608040201005,"alignment [53] and customization [55], and are typically combined to drastically decrease hardware
42"
INTRODUCTION,0.044221105527638194,"demands needed to fine-tune modern LLMs [12].
43"
INTRODUCTION,0.04522613065326633,"Herein, we extensively explore Mamba’s downstream learning capabilities across standard natural
44"
INTRODUCTION,0.04623115577889447,"benchmarks. For ICL, we show that, in contrast to recent non-standard studies showing Mamba
45"
INTRODUCTION,0.04723618090452261,"models rival state-of-the-art (SOTA) LLMs of similar parameter counts, the pretrained benefits of
46"
INTRODUCTION,0.04824120603015075,"Mamba few-shot learning are significantly less than comparable Transformer LLMs across
47"
INTRODUCTION,0.04924623115577889,"standard natural benchmarks; averaged across the benchmarks and parameter counts in Table 1,
48"
INTRODUCTION,0.05025125628140704,"Mamba models only achieve 38% of the performance improvements (relative to zero-shot)
49"
INTRODUCTION,0.05125628140703518,"of comparable Transformer models from the Pythia suite [5]. However, we show in the sequel
50"
INTRODUCTION,0.05226130653266332,"that Mamba models can more than halve this gap through efficient fine-tuning, achieving as
51"
INTRODUCTION,0.053266331658291456,"much as 81.5% of the average few-shot learning improvement (relative to zero-shot) of comparable
52"
INTRODUCTION,0.054271356783919596,"Transformers.
53"
INTRODUCTION,0.05527638190954774,"For MPFT, we leverage theory from dynamical systems to show that small input changes in a
54"
INTRODUCTION,0.05628140703517588,"MambaBlock do not lead to exponentially deviating outputs. Empirically, we validate this theoretical
55"
INTRODUCTION,0.05728643216080402,"result; compared to full-precision, deviations due to mixed-precision for Mamba inference and
56"
INTRODUCTION,0.05829145728643216,"fine-tuning are on par with those demonstrated by Transformer LLMs (Section 6). For PEFT, we
57"
INTRODUCTION,0.0592964824120603,"show that by targeting the largest memory buffer exploited by Mamba’s highly customized CUDA
58"
INTRODUCTION,0.06030150753768844,"kernels, LoRA may be used for extremely efficient fine-tuning, while simultaneously regularizing
59"
INTRODUCTION,0.061306532663316586,"the majority of Mamba’s SSM parameters via weight tying. We show that this leads to extremely
60"
INTRODUCTION,0.062311557788944726,"efficient PEFT, resulting in up to 2.15 times faster training and 65.5% reduced memory compared to
61"
INTRODUCTION,0.06331658291457286,"the largest evaluated Mamba model without MPFT or PEFT.
62"
BACKGROUND,0.06432160804020101,"2
Background
63"
BACKGROUND,0.06532663316582915,"Downstream learning for LLMs. Since the release of the Transformer architecture [54], attention-
64"
BACKGROUND,0.06633165829145729,"based LLMs have exhibited several downstream learning abilities–in particular, PEFT, MPFT, and
65"
BACKGROUND,0.06733668341708543,"ICL–which allow the rapid adaptation of foundation models towards specific applications. PEFT using
66"
BACKGROUND,0.06834170854271357,"adapters [24] allows a large pretrained model to be efficiently adapted for a particular downstream
67"
BACKGROUND,0.06934673366834171,"task by freezing the full model and training only a small number of extra parameters. Arguably the
68"
BACKGROUND,0.07035175879396985,"most widely used such PEFT method is LoRA [28], which injects trainable low-rank matrices into
69"
BACKGROUND,0.07135678391959799,"Transformer layers to approximate weight updates.
70"
BACKGROUND,0.07236180904522613,"To further decrease the computational demands necessary for LLM fine-tuning and inference, MPFT
71"
BACKGROUND,0.07336683417085427,"via mixed-precision (i.e., FP16 or BF16) [31, 43] and quantized low-precision [12] have proven
72"
BACKGROUND,0.0743718592964824,"effective strategies to reduce GPU memory and runtime requirements without deleterious effects on
73"
BACKGROUND,0.07537688442211055,"downstream performance [12, 59]. Additionally, mixed-precision approaches have paved the way for
74"
BACKGROUND,0.0763819095477387,"hardware-aware optimizations within the self-attention module [10], greatly mitigating the quadratic
75"
BACKGROUND,0.07738693467336684,"complexity of Transformer LLMs. Together, PEFT and MPFT have created a rich ecosystem with
76"
BACKGROUND,0.07839195979899498,"which varying combinations of these approaches may be used to meet the computational constraints
77"
BACKGROUND,0.07939698492462312,"of a given training system. We note that post-fine-tuning quantization approaches [13] may be further
78"
BACKGROUND,0.08040201005025126,"used to decrease Transformer LLM computational demands, but such approaches are not considered
79"
BACKGROUND,0.0814070351758794,"in this work.
80"
BACKGROUND,0.08241206030150754,"ICL provides an adaptable alternative to fine-tuning. Rather than fine-tune the LLM directly, ICL
81"
BACKGROUND,0.08341708542713568,"augments a prompt with n relevant examples (called shots) preceding the query of interest. Given
82"
BACKGROUND,0.08442211055276382,"sufficiently large models and pretraining data [8, 58], Transformer LLMs have proven adept at
83"
BACKGROUND,0.08542713567839195,"learning new concepts on the fly provided such few-shot prompting. However, it is worth noting
84"
BACKGROUND,0.0864321608040201,"that ICL inference time increases dramatically as the number of shots grows (due to self-attention’s
85"
BACKGROUND,0.08743718592964825,"quadratic complexity) and PEFT (when possible) is known to produce more accurate downstream
86"
BACKGROUND,0.08844221105527639,"learning results [8, 41].
87"
BACKGROUND,0.08944723618090453,"Table 1: In-context learning performance for pretrained Mamba and Pythia models. Models are
collected into parameter classes for head-to-head comparison using the groupings in [22]. Model
checkpoints were evaluated on all benchmarks and few-shot settings using the LM evaluation harness
from Eleuther AI [16]. LAMBADA zero-shot is more effective for the model sizes considered (further
discussed in [61, 8]) and thus excluded from few-shot performance averages. Highlighted in bold is
the top-performing few-shot learner per benchmark and model grouping."
BACKGROUND,0.09045226130653267,"Model
N-shot LAMBADA LAMBADA HellaSwag PIQA Arc-E Arc-C WinoGrande 0-shot incr.
ppl Ó
acc Ò
acc Ò
acc Ò
acc Ò
acc Ò
acc Ò
Mean % Ò"
BACKGROUND,0.0914572864321608,"Mamba
130M"
BACKGROUND,0.09246231155778895,"0
16.07
44.3
35.3
64.5
48.0
24.2
44.8
–
1
19.34
38.3
35.2
64.3
47.1
23.5
51.3
-1.4
3
23.13
35.4
35.1
65.1
49.0
24.0
50.7
-0.2
5
24.38
36.2
34.8
64.9
49.2
23.9
50.5
-0.5"
BACKGROUND,0.09346733668341708,"Pythia
160M"
BACKGROUND,0.09447236180904522,"0
38.20
32.7
30.2
61.8
43.4
23.8
51.0
–
1
47.21
28.2
30.6
62.2
43.4
23.7
49.3
-0.4
3
63.70
24.7
30.5
61.9
44.8
22.9
51.3
0.1
5
66.30
25.3
30.4
62.6
43.4
23.1
50.8
-0.2"
BACKGROUND,0.09547738693467336,"Mamba
370M"
BACKGROUND,0.0964824120603015,"0
8.14
55.6
46.5
69.5
55.0
27.9
55.5
–
1
9.74
49.8
45.9
69.3
57.4
26.5
54.6
-0.8
3
10.89
48.5
46.2
69.6
58.7
28.5
53.6
1.0
5
11.36
48.5
46.2
69.4
58.3
28.0
56.0
1.3"
BACKGROUND,0.09748743718592964,"Pythia
410M"
BACKGROUND,0.09849246231155778,"0
10.83
51.5
40.6
66.9
52.0
24.1
53.4
–
1
12.26
47.1
40.5
68.0
53.8
25.6
52.4
1.8
3
14.39
43.2
40.9
67.9
55.1
26.9
54.0
4.2
5
14.62
44.1
40.8
68.1
54.6
26.6
53.4
3.5"
BACKGROUND,0.09949748743718594,"Mamba
790M"
BACKGROUND,0.10050251256281408,"0
6.01
61.7
55.1
72.1
61.2
29.6
56.0
–
1
7.06
56.2
54.5
72.5
63.3
30.1
56.9
1.4
3
8.05
54.8
54.2
72.2
63.4
31.6
57.2
2.4
5
8.83
53.4
54.6
72.5
64.6
32.1
57.5
3.4"
BACKGROUND,0.10150753768844221,"Pythia
1B"
BACKGROUND,0.10251256281407035,"0
7.92
56.3
47.2
70.7
57.0
27.0
53.4
–
1
8.99
51.8
47.3
70.7
57.1
28.2
53.4
1.0
3
10.48
48.2
47.5
71.2
59.2
28.0
54.3
2.2
5
10.86
48.4
47.3
71.4
58.7
28.4
53.1
1.9"
BACKGROUND,0.1035175879396985,"Mamba
1.4B"
BACKGROUND,0.10452261306532663,"0
5.04
65.0
59.1
74.2
65.5
32.9
58.6
–
1
5.83
60.6
58.20
74.7
64.5
33.0
61.2
-0.5
3
6.62
58.9
58.8
73.7
66.1
34.4
60.9
0.6
5
6.98
58.4
59.0
74.0
66.4
35.5
60.5
1.4"
BACKGROUND,0.10552763819095477,"Pythia
1.4B"
BACKGROUND,0.10653266331658291,"0
6.09
61.7
52.1
70.9
60.5
28.5
57.4
–
1
6.96
56.3
52.1
71.4
62.0
29.5
57.5
1.4
3
7.89
54.4
52.6
70.9
63.9
31.1
56.8
2.9
5
8.02
54.4
52.8
71.0
63.2
31.3
57.8
3.3"
BACKGROUND,0.10753768844221105,"Mamba
2.8B"
BACKGROUND,0.10854271356783919,"0
4.23
69.2
66.2
75.2
69.7
36.3
63.4
–
1
5.01
63.9
65.7
75.5
69.8
37.2
63.7
0.6
3
5.53
63.0
65.5
75.2
70.8
38.1
64.8
1.6
5
5.70
62.7
66.2
76.2
70.9
38.3
64.6
2.1"
BACKGROUND,0.10954773869346733,"Pythia
2.8B"
BACKGROUND,0.11055276381909548,"0
5.04
64.7
59.3
73.9
64.2
32.9
59.8
–
1
5.66
60.9
59.4
73.8
66.8
34.8
59.0
1.7
3
6.20
59.1
59.9
74.7
67.4
34.9
60.8
2.9
5
6.52
59.1
60.2
74.5
67.1
35.0
61.3
3.1"
BACKGROUND,0.11155778894472362,"State-space Models. Structured state-space sequence (S4) models [23, 14] are SSMs which leverage
88"
BACKGROUND,0.11256281407035176,"linear time-invariant (LTI) systems to combine the computational advantages of Transformers–i.e.,
89"
BACKGROUND,0.1135678391959799,"highly parallelizable training–and recurrent neural networks (RNNs)–i.e., subquadratic autoregressive
90"
BACKGROUND,0.11457286432160804,"inference using recurrency. Within the S4 layer, an input signal is discretized and LTI parameters
91"
BACKGROUND,0.11557788944723618,"representing the input’s latent dynamics are learned. Owing to the S4 block’s latent dynamics being
92"
BACKGROUND,0.11658291457286432,"LTI, the S4 block’s output may be thus compactly represented as a single convolution between the
93"
BACKGROUND,0.11758793969849246,"input and an SSM convolution kernel (a matrix whose entries are products of LTI learnable parameters
94"
BACKGROUND,0.1185929648241206,"resulting from unrolling the state-space equations). However, despite hardware efficiency and
95"
BACKGROUND,0.11959798994974874,"long-dependency-modeling improvements, LTI-based S4 models remained inferior to Transformers
96"
BACKGROUND,0.12060301507537688,"of comparable parameter-sizes for natural language tasks, even when augmenting S4 layers with
97"
BACKGROUND,0.12160804020100502,"attention-layers for hybrid architectures [22].
98"
BACKGROUND,0.12261306532663317,"Innovating on these previous S4 approaches, Mamba utilizes time-varying parameters to model
99"
BACKGROUND,0.12361809045226131,"latent dynamics, thus broadening the ability to capture nuanced changes evolving in discrete-time.
100"
BACKGROUND,0.12462311557788945,"Without LTI dynamics, however, the input-output representation via the SSM convolution kernel is no
101"
BACKGROUND,0.12562814070351758,"longer applicable, thus voiding previous hardware-aware S4 optimizations [14]. To enable hardware
102"
BACKGROUND,0.12663316582914572,"efficiency with time-varying SSM parameters, [22] thus introduced extensively customized CUDA
103"
BACKGROUND,0.12763819095477386,"kernels which implement highly parallelized prefix sums to compute recurrent states.
104"
MAMBA STATE-SPACE MODELS,0.12864321608040202,"3
Mamba state-space models
105"
MAMBA STATE-SPACE MODELS,0.12964824120603016,"For model dimension d and maximum input sequence length T, the MambaBlock defines state-space
106"
MAMBA STATE-SPACE MODELS,0.1306532663316583,"parameters A, Bt, Ct, ∆t P Rdˆd for t P t1, . . . , Tu. The matrix ∆t controls the discrete step-
107"
MAMBA STATE-SPACE MODELS,0.13165829145728644,"size. Given an input sequence u1, . . . , uT P Rd, the following linear mapping through latent states
108"
MAMBA STATE-SPACE MODELS,0.13266331658291458,"x1, . . . , xT P Rd is used to produce the output y1, . . . , yT P Rd:
109"
MAMBA STATE-SPACE MODELS,0.13366834170854272,"xt “ ¯Atxt´1 ` ¯Btut
(1)"
MAMBA STATE-SPACE MODELS,0.13467336683417086,"yt “ ¯Ctxt,
(2)"
MAMBA STATE-SPACE MODELS,0.135678391959799,"where ¯∆t “ softpluspLinearp∆tqq P Rdˆd, ¯At “ exp p ¯∆tAq and ¯Bt “ A´1p ¯A ´ IqBt. In
110"
MAMBA STATE-SPACE MODELS,0.13668341708542714,"practice, A, Bt, Ct and ∆t are diagonal matrices.
111"
MAMBA STATE-SPACE MODELS,0.13768844221105528,"Hardware-aware optimizations. As matrices Bt, Ct and ∆t are time-varying, S4 optimizations via
112"
MAMBA STATE-SPACE MODELS,0.13869346733668342,"the SSM convolution kernel [11] are no longer applicable. However, by diagonality, each dimension
113"
MAMBA STATE-SPACE MODELS,0.13969849246231156,"may be computed in parallel. Furthermore, the recurrence along every dimension is a prefix sum (also
114"
MAMBA STATE-SPACE MODELS,0.1407035175879397,"called a scan), which is highly parallelizable [7]. [15] thus capitalizes on this through extensively
115"
MAMBA STATE-SPACE MODELS,0.14170854271356784,"customized CUDA kernels wherein the majority of temporal variables are carefully laid out in a large
116"
MAMBA STATE-SPACE MODELS,0.14271356783919598,"buffer of GPU memory and manipulated. Instantiated as a PyTorch linear layer’s weight matrix, this
117"
MAMBA STATE-SPACE MODELS,0.14371859296482412,"memory buffer W P Rnˆ3d is used to store and access the diagonal elements of Bt, Ct and ∆t for
118"
MAMBA STATE-SPACE MODELS,0.14472361809045226,"all t P t1, . . . , Tu, such that
119"
MAMBA STATE-SPACE MODELS,0.1457286432160804,"Wrt ´ 1, : ds “ diagp∆tq, Wrt ´ 1, d : 2ds “ diagpBtq, Wrt ´ 1, 2d : 3ds “ diagpCtq, (3)"
MAMBA STATE-SPACE MODELS,0.14673366834170853,"where Wr0, : ds “ diagp∆1q, Wrn ´ 1, d : 2ds “ diagpBT q, and so on.
120"
MAMBA STATE-SPACE MODELS,0.14773869346733667,"The customized Mamba prefix scan kernel heavily relies on this memory layout to optimize the
121"
MAMBA STATE-SPACE MODELS,0.1487437185929648,"access pattern of W in Equations 5 and 6.We note that, rather than adjusting Mamba’s low-level
122"
MAMBA STATE-SPACE MODELS,0.14974874371859295,"CUDA kernels themselves to integrate LoRA within the highly optimized prefix scan, we can instead
123"
MAMBA STATE-SPACE MODELS,0.1507537688442211,"directly target W. Doing so, we have the following, where the proof is available in Appendix A.
124"
MAMBA STATE-SPACE MODELS,0.15175879396984926,"Theorem 1. Consider the weight matrix W of a MambaBlock from Equation 3. Targeting W for
125"
MAMBA STATE-SPACE MODELS,0.1527638190954774,"LoRA during fine-tuning ties adaptation weights across Bt, Ct and ∆t.
126"
STABLE DYNAMICS IN THE MAMBABLOCK,0.15376884422110554,"4
Stable dynamics in the MambaBlock
127"
STABLE DYNAMICS IN THE MAMBABLOCK,0.15477386934673368,"The Mamba foundation models were pretrained in full FP32 precision. Consequently, official Mamba
128"
STABLE DYNAMICS IN THE MAMBABLOCK,0.15577889447236182,"implementations have cautioned against fine-tuning or training in reduced precision [21, 29], with
129"
STABLE DYNAMICS IN THE MAMBABLOCK,0.15678391959798996,"potential sensitivities of MambaBlock recurrent dynamics remaining an open question. We answer
130"
STABLE DYNAMICS IN THE MAMBABLOCK,0.1577889447236181,"the latter using theory from dynamical systems. For Mamba’s discrete dynamic system in Equations 5
131"
STABLE DYNAMICS IN THE MAMBABLOCK,0.15879396984924624,"and 6, define
132"
STABLE DYNAMICS IN THE MAMBABLOCK,0.15979899497487438,"xt “ Fθpxt´1, utq,
(4)"
STABLE DYNAMICS IN THE MAMBABLOCK,0.16080402010050251,"where θ denotes the time-varying parameters described in Section 3. For input sequence u1, . . . , uT
133"
STABLE DYNAMICS IN THE MAMBABLOCK,0.16180904522613065,"and initial latent state vector x0, we thus write
134"
STABLE DYNAMICS IN THE MAMBABLOCK,0.1628140703517588,"xT “ FθpFθp. . . Fθpx0, u1qqq – F T ´1
θ
px0, u1q."
STABLE DYNAMICS IN THE MAMBABLOCK,0.16381909547738693,"The rate of divergence between two scalar ε-close inputs to a discrete dynamical system is bounded
135"
STABLE DYNAMICS IN THE MAMBABLOCK,0.16482412060301507,"by the system’s maximal Lyapunov exponent λmax [44]. Given λmax and two initial values px0, u1q
136"
STABLE DYNAMICS IN THE MAMBABLOCK,0.1658291457286432,"and px0 ` ε, u1 ` εq, the maximum deviation between these points grows as [33, 50]:
137"
STABLE DYNAMICS IN THE MAMBABLOCK,0.16683417085427135,"max |F N
θ px0, u1q ´ F N
θ px0 ` ε, u1 ` εq| P Opε exp pNλmaxqq."
STABLE DYNAMICS IN THE MAMBABLOCK,0.1678391959798995,"Thus, when λmax ą 0, nearby trajectories exponentially separate and, when λmax ď 0, nearby
138"
STABLE DYNAMICS IN THE MAMBABLOCK,0.16884422110552763,"trajectories ultimately converge to the same fixed point or periodic cycles.
139"
STABLE DYNAMICS IN THE MAMBABLOCK,0.16984924623115577,"The maximal Lyapunov exponent is defined as
140"
STABLE DYNAMICS IN THE MAMBABLOCK,0.1708542713567839,"λmax – lim
T Ñ8
1
T log ››››› T
ź t“0"
STABLE DYNAMICS IN THE MAMBABLOCK,0.17185929648241205,"Bxt
Bxt´1"
STABLE DYNAMICS IN THE MAMBABLOCK,0.1728643216080402,"›››››
2
,"
STABLE DYNAMICS IN THE MAMBABLOCK,0.17386934673366833,"where }}2 denotes the spectral norm for matrices. For an arbitrary MambaBlock, we prove the
141"
STABLE DYNAMICS IN THE MAMBABLOCK,0.1748743718592965,"following:
142"
STABLE DYNAMICS IN THE MAMBABLOCK,0.17587939698492464,"Theorem 2. Let pxt´1, utq be the latent state and input at an arbitrary time t P t1, . . . , Tu within a
143"
STABLE DYNAMICS IN THE MAMBABLOCK,0.17688442211055277,"MambaBlock. Then small changes pxt´1`ε, ut`εq produce deviations which are exponentially non-
144"
STABLE DYNAMICS IN THE MAMBABLOCK,0.17788944723618091,"increasing over discrete-time. That is, max |F N
θ pxt´1, utq´F N
θ pxt´1`ε, ut`εq| P Opε exp pNζqq,
145"
STABLE DYNAMICS IN THE MAMBABLOCK,0.17889447236180905,"for some scalar ζ ď 0.
146"
STABLE DYNAMICS IN THE MAMBABLOCK,0.1798994974874372,"The proof of Theorem 2 is available in Appendix B, where the maximal Lyapunov exponent for an
147"
STABLE DYNAMICS IN THE MAMBABLOCK,0.18090452261306533,"arbitrary MambaBlock is first proven to be non-positive. The main result subsequently follows.
148"
STABLE DYNAMICS IN THE MAMBABLOCK,0.18190954773869347,"Consequences for automatic mixed-precision. During a forward pass, automatic mixed-precision
149"
STABLE DYNAMICS IN THE MAMBABLOCK,0.1829145728643216,"(AMP) saves time and memory by computing forward activations in half-precision (FP16 or BF16).
150"
STABLE DYNAMICS IN THE MAMBABLOCK,0.18391959798994975,"During a backward pass, AMP computes gradients in half-precision and up-casts to full-precision
151"
STABLE DYNAMICS IN THE MAMBABLOCK,0.1849246231155779,"prior to updating. In contrast to full-precision fine-tuning, MPFT within the MambaBlock thus results
152"
STABLE DYNAMICS IN THE MAMBABLOCK,0.18592964824120603,"in small differences to the inputs u1, . . . , uT fed into the SSM scan (which are passed through a
153"
STABLE DYNAMICS IN THE MAMBABLOCK,0.18693467336683417,"SwiGLU), ¯∆t (which is passed through a softplus), and the gradients calculated during training.
154"
STABLE DYNAMICS IN THE MAMBABLOCK,0.1879396984924623,"For a discrete dynamical system with λmax ą 0, changes due to AMP compound after repeated
155"
STABLE DYNAMICS IN THE MAMBABLOCK,0.18894472361809045,"expansion of the recurrent state, thus leading to exponential deviations between quantities calculated
156"
STABLE DYNAMICS IN THE MAMBABLOCK,0.1899497487437186,"using mixed- versus full-precision. We note that Transformers are not recurrent, and thus not
157"
STABLE DYNAMICS IN THE MAMBABLOCK,0.19095477386934673,"susceptible to such issues. Yet, just as differences introduced by quantization/mixed-precision produce
158"
STABLE DYNAMICS IN THE MAMBABLOCK,0.19195979899497487,"output differences in Transformer results, differences are expected in Mamba results using different
159"
STABLE DYNAMICS IN THE MAMBABLOCK,0.192964824120603,"precision strategies. However, by Theorem 2, such differences do not exponentially compound over
160"
STABLE DYNAMICS IN THE MAMBABLOCK,0.19396984924623115,"discrete-time within the MambaBlock.
161"
RELATED WORK,0.19497487437185929,"5
Related Work
162"
RELATED WORK,0.19597989949748743,"Several recent works [45, 19, 30, 40] have studied Mamba’s ability to perform ICL. However, none
163"
RELATED WORK,0.19698492462311556,"of these have extensively studied Mamba’s ICL capabilities either on standard NLP benchmarks or on
164"
RELATED WORK,0.19798994974874373,"pure MambaBlock foundation models. In particular, foundational Mamba models’ ICL abilities were
165"
RELATED WORK,0.19899497487437187,"tested in [45] to learn simple function classes (e.g., logistic regression and decision trees [17]) and in
166"
RELATED WORK,0.2,"[19] to learn non-standard NLP benchmarks (i.e., task vectors [25]). While [45, 19] report Mamba’s
167"
RELATED WORK,0.20100502512562815,"ICL abilities rival SOTA Transformers, their utilized benchmarks were proposed as supplemental
168"
RELATED WORK,0.2020100502512563,"ICL studies after Transformer LLMs’ success on standard NLP benchmarks [8]. Indeed, direct
169"
RELATED WORK,0.20301507537688443,"evaluation of Mamba foundation models on standard NLP benchmarks does not lead to higher gains
170"
RELATED WORK,0.20402010050251257,"over zero-shot performance relative to comparable Transformer LLMs (demonstrated in Table 1).
171"
RELATED WORK,0.2050251256281407,"Lyapunov exponents have previously been considered for classic RNN structures (e.g., vanilla
172"
RELATED WORK,0.20603015075376885,"RNNs, LSTMs, GRUs, PLRNNs, etc.) [44, 56], to determine when such models exhibit chaotic
173"
RELATED WORK,0.207035175879397,"dynamics and the impact on the exploding/vanishing gradient phenomena*. For more recent S4 neural
174"
RELATED WORK,0.20804020100502513,"*We note that this continues a long line of research exploring RNNs sensitivity to initial conditions and their
subsequent ability to produce chaotic output [47, 34, 3, 4], although previous work did not leverage Lyapunov
exponents."
RELATED WORK,0.20904522613065327,"models, [18] used Hurwitz matrices to characterize the numerical stability of linear time-invariant
175"
RELATED WORK,0.2100502512562814,"(LTI) S4 models. However, such analysis is not applicable to time-varying models, such as Mamba,
176"
RELATED WORK,0.21105527638190955,"nor does it characterize the effects of sensitive dependence on initial conditions (e.g., divergence of
177"
RELATED WORK,0.21206030150753769,"two ε close inputs). To the best of our knowledge, no previous works have used Lyapunov exponents
178"
RELATED WORK,0.21306532663316582,"to explore the effects of mixed-precision on recurrent neural models or Mamba architectures.
179"
RELATED WORK,0.21407035175879396,"As in [22], the majority of subsequent Mamba works have focused on pretraining MambaBlocks using
180"
RELATED WORK,0.2150753768844221,"full precision [65, 62, 1, 40]. Notably, the official implementation of Jamba [40], the Transformer-
181"
RELATED WORK,0.21608040201005024,"Mamba hybrid, supports mixed- and 8-bit precision, but avoids MambaBlocks when applying such
182"
RELATED WORK,0.21708542713567838,"quantization [32]. Similarly, the official Mamba sources advise using full precision within the
183"
RELATED WORK,0.21809045226130652,"MambaBlock [29, 21], cautioning against using mixed-precision due to potential recurrent sensitivities.
184"
RELATED WORK,0.21909547738693466,"To the best of our knowledge, no existing works have either theoretically explored the effects small
185"
RELATED WORK,0.2201005025125628,"input changes (e.g., due to mixed-precision) have on Mamba’s recurrent dynamics, empirically
186"
RELATED WORK,0.22110552763819097,"explored such effects downstream impact on fine-tuning and inference, or explored pure Mamba
187"
RELATED WORK,0.2221105527638191,"networks fine-tuning abilities relative to Transformer LLMs.
188"
EXPERIMENTS,0.22311557788944725,"6
Experiments
189"
EXPERIMENTS,0.2241206030150754,"To demonstrate the implications of Theorem 2, we explore the performance difference between
190"
EXPERIMENTS,0.22512562814070353,"running inference with full-precision pretrained weights and using mixed-precision (FP16 and BF16)
191"
EXPERIMENTS,0.22613065326633167,"weights. Model performance is measured as percent accuracy using the MMLU [26] dataset.
192"
EXPERIMENTS,0.2271356783919598,"The difference in model performance is reported as the mean divergence (i.e., absolute difference)
193"
EXPERIMENTS,0.22814070351758794,"between the original full-precision and respective mixed-precision model, averaged over {0, 1, 3,
194"
EXPERIMENTS,0.22914572864321608,"5}-shot percent accuracy. Thus, a divergence greater than one denotes an average difference
195"
EXPERIMENTS,0.23015075376884422,"greater than one entire percentage of accuracy.
196"
EXPERIMENTS,0.23115577889447236,"Mamba pretrained checkpoints are compared to pretrained Transformer models of similar parameter
197"
EXPERIMENTS,0.2321608040201005,"counts and no more than „300B total pretraining tokens (Pythia [5], OLMo [20] 336B-token
198"
EXPERIMENTS,0.23316582914572864,"checkpoint, and Phi 1.5 [39]). We note that Pythia and Mamba models were both pretrained using
199"
EXPERIMENTS,0.23417085427135678,"the same corpus [15], allowing the fairest comparison between SSMs and Transformers. To limit
200"
EXPERIMENTS,0.23517587939698492,"extraneous numerical effects within experiments (e.g., due to parameter aggregation across multiple
201"
EXPERIMENTS,0.23618090452261306,"GPUs), all models were run using a single GPU (Nvidia A10G, 24 GB total memory). All models
202"
EXPERIMENTS,0.2371859296482412,"were evaluated using the LM evaluation harness from Eleuther AI [16]. Further experimental details
203"
EXPERIMENTS,0.23819095477386934,"are available in Appendix C. The results are available in Table 2.
204"
EXPERIMENTS,0.23919597989949748,"Table 2: Mean full-precision (FP32) divergence in MMLU performance for mixed-precision inference.
Divergence is averaged over {0, 1, 3, 5}-shot performance. Pretrained checkpoints are used for
Mamba (M), Pythia (P), OLMo [20], and Phi-1.5 [39] (Phi) models."
EXPERIMENTS,0.24020100502512562,"Model
M
P
M
P
M
P
OLMo
M
P
Phi
M
P"
EXPERIMENTS,0.24120603015075376,"Size
130m
160m
370m
410m
790m
1b
1.4b
1.5b
2.8b"
EXPERIMENTS,0.2422110552763819,"FP16 µ
0.03
0.35
0.05
0.06
0.21
0.05
0.04
0.04
0.07
0.03
0.15
0.12
BF16 µ
0.05
1.45
0.20
0.20
0.66
0.16
0.13
0.31
0.13
1.05
1.17
0.11"
EXPERIMENTS,0.24321608040201004,"From Table 2, inferencing in Pythia using FP16 and BF16 result in an average 0.13 and 0.41 full-
205"
EXPERIMENTS,0.2442211055276382,"precision divergence, respectively. Mamba displays similar averages in comparison: inferencing in
206"
EXPERIMENTS,0.24522613065326634,"Mamba using FP16 and BF16 result in an average 0.10 and 0.48 divergence, respectively. Interestingly,
207"
EXPERIMENTS,0.24623115577889448,"both SSM and Transformer architectures exhibit large divergence spikes–i.e., mean divergence greater
208"
EXPERIMENTS,0.24723618090452262,"than a percentage point–when using BF16, which occurs once for Mamba and Phi 1.5 models and
209"
EXPERIMENTS,0.24824120603015076,"twice for Pythia models. In the following, we show that such spikes may be mitigated for Mamba
210"
EXPERIMENTS,0.2492462311557789,"SSMs by combining mixed-precision with parameter-efficient adapters during fine-tuning.
211"
EXPERIMENTS,0.25025125628140704,"Non-divergent Mamba fine-tuning. We next explore the implications of Theorem 2 on fine-tuning,
212"
EXPERIMENTS,0.25125628140703515,"wherein mixed-precision is especially critical; MPFT combined with PEFT adapters have been shown
213"
EXPERIMENTS,0.2522613065326633,"to drastically reduce Transformer fine-tuning times [12]. We are thus interested in the divergence
214"
EXPERIMENTS,0.25326633165829143,"between Mamba models fully fine-tuned (i.e., no adapters, all model weights are trained) in full-
215"
EXPERIMENTS,0.2542713567839196,"precision and models fine-tuned using mixed-precision and/or PEFT adapters. We focus on utilizing
216"
EXPERIMENTS,0.2552763819095477,"LoRA [28], which is arguably the most widely used PEFT framework for LLMs.
217"
EXPERIMENTS,0.2562814070351759,"Mamba 130M
Pythia 160M
Mamba 370M
Pythia 410M
Mamba 790M
Pythia 1B
Model 0.0 0.5 1.0 1.5"
EXPERIMENTS,0.25728643216080405,Mean Per-shot FP32 Divergence
EXPERIMENTS,0.25829145728643216,"FP16 Full
BF16 Full
FP16 ALL LoRA
BF16 ALL LoRA
FP16 SLL LoRA
BF16 SLL LoRA"
EXPERIMENTS,0.2592964824120603,"Figure 1: Mean full-precision (FP32) divergence in MMLU performance for Mamba and Pythia
models. Models are fine-tuned over the Alpaca dataset [51] using different combinations of MPFT
and PEFT. Full fine-tuning (i.e., no PEFT adapters) is denoted as Full."
EXPERIMENTS,0.26030150753768844,"Using the Alpaca dataset [51], Mamba 160M, 410M, and 790M models are fine-tuned for three epochs
218"
EXPERIMENTS,0.2613065326633166,"with a maximum sequence length of 512. We denote the targeting of all linear layers (ALL) for LoRA
219"
EXPERIMENTS,0.2623115577889447,"as ALL LoRA, the targeting of a subset of linear layers (SLL) for LoRA as SLL LoRA, and no adapters
220"
EXPERIMENTS,0.2633165829145729,"as Full (i.e., full fine-tuning). Both ALL and SLL LoRA adapt the large memory buffer described in
221"
EXPERIMENTS,0.264321608040201,"Theorem 1.
222"
EXPERIMENTS,0.26532663316582916,"Each fine-tuning run occurred on a single A10G GPU. To further limit extraneous numerical effects,
223"
EXPERIMENTS,0.2663316582914573,"the same batch size is used for all FP32, FP16, and BF16 experiments for a given model size. While
224"
EXPERIMENTS,0.26733668341708544,"this leads to hardware underutilization (i.e., non-saturated GPU memory for mixed-precision and
225"
EXPERIMENTS,0.26834170854271355,"LoRA experiments), this is necessary to guarantee no divergence is due to differences in parameter
226"
EXPERIMENTS,0.2693467336683417,"update schedules. For comparison, Pythia 160M, 410M, and 1B models are fine-tuned using the
227"
EXPERIMENTS,0.27035175879396983,"same experimental setup. The training recipe for all models was adapted from [53], with the
228"
EXPERIMENTS,0.271356783919598,"AdamW_torch optimizer and a cosine annealing schedule. Further experimental details are
229"
EXPERIMENTS,0.2723618090452261,"available in Appendix C.
230"
EXPERIMENTS,0.2733668341708543,"For each Mamba and Pythia model, Figure 1 shows the mean divergence calculated between the
231"
EXPERIMENTS,0.2743718592964824,"respective FP32 Full and mixed-precision ALL/SLL LoRA fine-tuned models, averaged over {0, 1, 3,
232"
EXPERIMENTS,0.27537688442211056,"5}-shot MMLU accuracy. Across mixed-precisions and adapter settings, Mamba displays comparable
233"
EXPERIMENTS,0.27638190954773867,"divergences to Pythia models. E.g., for FP16, Mamba demonstrates an average divergence of 0.1,
234"
EXPERIMENTS,0.27738693467336684,"compared to 0.14 for Pythia. Similarly, for BF16, Mamba demonstrates an average divergence
235"
EXPERIMENTS,0.27839195979899495,"of 0.18, compared to 0.28 for Pythia. Importantly, Mamba models do not exhibit large deviation
236"
EXPERIMENTS,0.2793969849246231,"spikes after fine-tuning (in contrast to Pythia models).
237"
EXPERIMENTS,0.2804020100502513,"Hardware throughput and memory-utilization improvements. With comparable divergences
238"
EXPERIMENTS,0.2814070351758794,"to Transformers and stable dynamics, we show that MPFT and PEFT may be used to significantly
239"
EXPERIMENTS,0.28241206030150756,"increase GPU-training throughput for Mamba SSMs. To demonstrate such improvements, we utilize
240"
EXPERIMENTS,0.2834170854271357,"the previous fine-tuning settings for the Alpaca dataset. However, we now adjust the batch size to
241"
EXPERIMENTS,0.28442211055276384,"maximize throughput per MPFT and PEFT configuration.
242"
EXPERIMENTS,0.28542713567839195,"For each MPFT and PEFT configuration, the average tokens-per-second (ATPS) is calculated as the
243"
EXPERIMENTS,0.2864321608040201,"total tokens used for fine-tuning divided by total training time, and the maximum memory-per-token
244"
EXPERIMENTS,0.28743718592964823,"(MMPT) is calculated as the maximum GPU memory utilization incurred (over the entire fine-tuning
245"
EXPERIMENTS,0.2884422110552764,"run) divided by the total number of tokens in each mini-batch. Results are plotted in Figure 6.
246"
EXPERIMENTS,0.2894472361809045,"Both throughput and memory utilization improve as the number of Mamba parameters increases
247"
EXPERIMENTS,0.2904522613065327,"in Figure 6. Compared to the full-precision full fine-tuning of Mamba 790M (the largest model
248"
EXPERIMENTS,0.2914572864321608,"supported by an A10G’s memory capacity), evaluated MPFT and PEFT combinations result in
249"
EXPERIMENTS,0.29246231155778896,"an average 2.15 times more training tokens-per-second while reducing per-token memory
250"
EXPERIMENTS,0.29346733668341707,"utilization by an average 62.7%. Across all model sizes, evaluated MPFT and PEFT combinations
251"
EXPERIMENTS,0.29447236180904524,"result in an average 1.74 times more training tokens-per-second while reducing per-token memory
252"
EXPERIMENTS,0.29547738693467335,"utilization by an average 47.2% compared to respective full-precision fine-tuned runs.
253"
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.2964824120603015,"6.1
Fine-tuning narrows the ICL gap between Mamba and Transformers
254"
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.2974874371859296,"We next explore how MPFT and PEFT affect Mamba ICL performance. All Mamba pretrained
255"
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.2984924623115578,"models are instruction fine-tuned using ALL LoRA and the OpenHermes dataset [52] (which consists
256"
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.2994974874371859,"of 242,000 supervised samples). We use the training recipe of [53], which includes BF16 utilization.
257"
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.3005025125628141,"Mamba 130m
Mamba 370m
Mamba 790m
Model 0 1 2 3"
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.3015075376884422,Tokens/second 1e4
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.30251256281407035,"fp32 Full
fp16 ALL LoRA
bf16 ALL LoRA
fp16 SLL LoRA
bf16 SLL LoRA"
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.3035175879396985,(a) Average tokens-per-second Ò.
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.30452261306532663,"Mamba 130m
Mamba 370m
Mamba 790m
Model 0 2 4 6 8"
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.3055276381909548,Max-memory/token (MB)
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.3065326633165829,"fp32 Full
fp16 ALL LoRA
bf16 ALL LoRA
fp16 SLL LoRA
bf16 SLL LoRA"
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.3075376884422111,(b) Maximum memory-per-token Ó.
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.3085427135678392,"Figure 2: Timing and memory usage calculated Mamba model-sizes and PEFT combinations. Each
model was trained using the Alpaca dataset [51] dataset for three epochs and maximum sequence
length 512. For each PEFT combination, the batch size was tuned to maximize GPU occupancy."
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.30954773869346736,"Performance is evaluated using the datasets from Table 1–HellaSwag [64], PIQA [6], Arc-E [9],
258"
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.31055276381909547,"Arc-C [9], and WinoGrande [49]–and report the average improvement percentage of {1, 3, 5}-shot
259"
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.31155778894472363,"versus 0-shot (AIPSS). For comparison, Pythia pretrained models are instruction fine-tuned using the
260"
FINE-TUNING NARROWS THE ICL GAP BETWEEN MAMBA AND TRANSFORMERS,0.31256281407035175,"same training recipe and ALL LoRA (i.e., all Pythia linear layers are adapted)."
-SHOT,0.3135678391959799,"0-Shot
1-Shot
3-Shot
5-Shot 0 2 4"
-SHOT,0.314572864321608,% 0-shot ICL Improvement
B,0.3155778894472362,"2.8B
1.4B
790M
370M
130M"
B,0.3165829145728643,(a) Mamba Pretrained
-SHOT,0.31758793969849247,"0-Shot
1-Shot
3-Shot
5-Shot 0 1 2 3"
-SHOT,0.3185929648241206,"4
2.8B
1.4B
1B
410M
160M"
-SHOT,0.31959798994974875,(b) Pythia Pretrained
-SHOT,0.32060301507537686,"0-Shot
1-Shot
3-Shot
5-Shot
0 2 4"
-SHOT,0.32160804020100503,% 0-shot ICL Improvement
B,0.32261306532663314,"2.8B
1.4B
790M
370M
130M"
B,0.3236180904522613,(c) Mamba ALL LoRA
-SHOT,0.3246231155778894,"0-Shot
1-Shot
3-Shot
5-Shot
0 2 4"
B,0.3256281407035176,"2.8B
1.4B
1B
410M
160M"
B,0.32663316582914576,(d) Pythia ALL LoRA
B,0.32763819095477387,"Figure 3: Fine-tuning narrows the ICL gap between Mamba and Pythia. ALL LoRA models were
instruction fine-tuned on the OpenHermes [52] dataset for one epoch. Performance is reported as the
average improvement percentage of {1, 3, 5}-shot versus 0-shot over five standard benchmarks. 261"
B,0.32864321608040203,"Figure 3 displays AIPSS for pretrained and instruction fine-tuned Mamba and Pythia models. As
262"
B,0.32964824120603015,"previously noted, pretrained Mamba models do not display similar ICL ability as comparable Pythia
263"
B,0.3306532663316583,"models on the evaluated standard NLP benchmarks. In particular, Mamba 2.8B, the largest pretrained
264"
B,0.3316582914572864,"Mamba model, displays inconsistent zero-shot improvements as the number of shots increase.
265"
B,0.3326633165829146,"However, after fine-tuning, all Mamba models larger than Mamba 130M consistently improve in ICL
266"
B,0.3336683417085427,"performance as the number of shots increase. Compared to Mamba pretrained models, which are only
267"
B,0.33467336683417087,"capable of 38% of the AIPSS compared to similar pretrained Pythia models, fine-tuned ALL LoRA
268"
B,0.335678391959799,"Mamba models are capable of 81.5% of the AIPSS compared to similarly fine-tuned Pythia models.
269"
B,0.33668341708542715,"Fine-tuning robustness. We show that Mamba is robust to the choice of PEFT hyperparemters. We
270"
B,0.33768844221105526,"conduct an extensive hyperparameter search across the learning rate, LoRA dimension, and number of
271"
B,0.33869346733668343,"warmup steps. From the Cartesian-product of these three parameters, 150 hyperparameter configura-
272"
B,0.33969849246231154,"tions were sampled and used to fine-tune Mamba 370M over the Openhermes dataset. For comparison,
273"
B,0.3407035175879397,"Pythia 410M is similarly fine-tuned using the same set of 150 hyperparameter configurations.
274"
B,0.3417085427135678,"10
7
10
6
10
5
10
4
10
3"
B,0.342713567839196,Learning Rate 0.24 0.26
B,0.3437185929648241,MMLU 5-Shot Acc.
B,0.34472361809045227,"LoRA Dim
16
32
64
128
256"
B,0.3457286432160804,(a) Mamba 370M
B,0.34673366834170855,"10
7
10
6
10
5
10
4
10
3"
B,0.34773869346733666,Learning Rate 0.24 0.26
B,0.3487437185929648,"LoRA Dim
16
32
64
128
256 0 1000 2000"
B,0.349748743718593,(b) Pythia 410M
B,0.3507537688442211,"Figure 4: Fine-tuning hyperparameter search for OpenHermes. Each point is a different hyperparam-
eter configuration. SLL LoRA was used for both models. The x-axis is the learning rate, the y-axis
is resulting MMLU 5-shot performance, bubble size is the LoRA dimension, and the color is the
number of warmup steps P t0, 1k, 2ku."
B,0.35175879396984927,"The MMLU 5-shot performance for each of the 150 Mamba and Pythia fine-tuned models is displayed
275"
B,0.3527638190954774,"in 6.1. Pythia 410M is capable of higher performance than Mamba 370M, where the average accuracy
276"
B,0.35376884422110555,"for the former and the latter are 26.5% and 24.8%, respectively. However, Mamba 370M is much more
277"
B,0.35477386934673366,"robust to the choice of hyperparameters, with a difference of 1.5% between the minimum (23.3%)
278"
B,0.35577889447236183,"and maximum (24.8%). In contrast, Pythia 410M fine-tuned models display a large performance
279"
B,0.35678391959798994,"difference of 4.7% between the minimum (22.9%) and maximum (27.6%).
280"
DISCUSSION,0.3577889447236181,"7
Discussion
281"
DISCUSSION,0.3587939698492462,"We’ve extensively explored Mamba’s downstream learning capabilities. Using dynamical systems
282"
DISCUSSION,0.3597989949748744,"theory, we’ve shown that Mamba’s recurrent dynamics are robust to small input perturbations (contrary
283"
DISCUSSION,0.3608040201005025,"to the current understanding of Mamba’s recurrent sensitivities). We’ve extensively confirmed this
284"
DISCUSSION,0.36180904522613067,"result, showing that: a) Mamba inference is robust to changes due to mixed-precision, (b) Mamba
285"
DISCUSSION,0.3628140703517588,"inference differences due to mixed-precision align with Transformers, (c) Mamba fine-tuning is robust
286"
DISCUSSION,0.36381909547738694,"to changes due to mixed-precision and PEFT, and (d) differences in downstream performance for
287"
DISCUSSION,0.36482412060301506,"Mamba due to MPFT and PEFT can be more robust than Transformers. Using both MPFT and PEFT,
288"
DISCUSSION,0.3658291457286432,"we’ve shown that instruction fine-tuning Mamba SSMs greatly narrows the previously observed ICL
289"
DISCUSSION,0.36683417085427134,"gap, going from only 38% (post pretraining) up to 81.5% (post fine-tuning) of the ICL abilities of
290"
DISCUSSION,0.3678391959798995,"similar Transformers. Furthermore, we’ve shown that combining MPFT and PEFT can more than
291"
DISCUSSION,0.3688442211055276,"halve training time and nearly triple memory efficiency for Mamba models.
292"
DISCUSSION,0.3698492462311558,"There are significant avenues for future work. In particular, adapting Mamba’s CUDA kernels to
293"
DISCUSSION,0.3708542713567839,"support more aggressive low-precision PEFT methods [12] would further decrease the hardware
294"
DISCUSSION,0.37185929648241206,"needed to train Mamba models, while providing additional speedups. Furthermore, while the largest
295"
DISCUSSION,0.37286432160804023,"pure Mamba model contains 2.8B parameters, the training speedups and improved memory utilization
296"
DISCUSSION,0.37386934673366834,"described herein may be applied to more efficiently pretrain larger pure Mamba SSMs (e.g., 7B
297"
DISCUSSION,0.3748743718592965,"parameters and greater), where Mamba models may better manifest emergent abilities previously
298"
DISCUSSION,0.3758793969849246,"displayed by Transformers (or even manifest previously unobserved abilities).
299"
DISCUSSION,0.3768844221105528,"Limitations. While we explored the use of LoRA for Mamba models, many other PEFT adapters
300"
DISCUSSION,0.3778894472361809,"exist [41, 38, 27, 35]. Furthermore, while mixed-precision using FP16 and BF16 were explored,
301"
DISCUSSION,0.37889447236180906,"lower-precision methods exist [12] (which may be enabled by adapting Mamba’s highly customized
302"
DISCUSSION,0.3798994974874372,"CUDA kernels). Both are interesting directions for future work. Finally, our timing and memory
303"
DISCUSSION,0.38090452261306534,"usage experiments using Alpaca did not consider the largest two Mamba models (1.4B and 2.8B) due
304"
DISCUSSION,0.38190954773869346,"to their exceeding A10G memory capacity for FP32 full fine-tuning.
305"
DISCUSSION,0.3829145728643216,"Broader Impact. The Mamba models considered are all LLMs, and thus have the same potential
306"
DISCUSSION,0.38391959798994973,"positive and negative societal impacts as other LLMs (e.g., hallucinations). Furthermore, fine-tuning
307"
DISCUSSION,0.3849246231155779,"is known to possibly erode existing LLM guardrails, and thus our methods may be adapted for this
308"
DISCUSSION,0.385929648241206,"fine-tuning use case (as is the case for all PEFT and MPFT methods). However, our work improves the
309"
DISCUSSION,0.3869346733668342,"quality of Mamba models for downstream applications, which may be adapted for all positive LLM
310"
DISCUSSION,0.3879396984924623,"applications in society (e.g., personal assistants, task automation, code completion, etc.). Finally, our
311"
DISCUSSION,0.38894472361809046,"work decreases the computational constraints required to train and inference Mamba SSMs, which
312"
DISCUSSION,0.38994974874371857,"has implications for green ML (e.g., decreased CO2 emissions, positive climate change impact, etc.).
313"
DISCUSSION,0.39095477386934674,"410 GPU days were used to produce the results for this paper.
314"
REFERENCES,0.39195979899497485,"References
315"
REFERENCES,0.392964824120603,"[1] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba: Mixture of experts
316"
REFERENCES,0.39396984924623113,"for state-space models. arXiv preprint arXiv:2402.01771, 2024.
317"
REFERENCES,0.3949748743718593,"[2] Ali Behrouz and Farnoosh Hashemi. Graph mamba: Towards learning on graphs with state space models.
318"
REFERENCES,0.39597989949748746,"arXiv preprint arXiv:2402.08678, 2024.
319"
REFERENCES,0.3969849246231156,"[3] Nils Bertschinger and Thomas Natschläger. Real-time computation at the edge of chaos in recurrent neural
320"
REFERENCES,0.39798994974874374,"networks. Neural computation, 16(7):1413–1436, 2004.
321"
REFERENCES,0.39899497487437185,"[4] Nils Bertschinger, Thomas Natschläger, and Robert Legenstein. At the edge of chaos: Real-time computa-
322"
REFERENCES,0.4,"tions and self-organized criticality in recurrent neural networks. Advances in neural information processing
323"
REFERENCES,0.40100502512562813,"systems, 17, 2004.
324"
REFERENCES,0.4020100502512563,"[5] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric
325"
REFERENCES,0.4030150753768844,"Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia:
326"
REFERENCES,0.4040201005025126,"A suite for analyzing large language models across training and scaling. In International Conference on
327"
REFERENCES,0.4050251256281407,"Machine Learning (ICML), pages 2397–2430. PMLR, 2023.
328"
REFERENCES,0.40603015075376886,"[6] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical common-
329"
REFERENCES,0.40703517587939697,"sense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34,
330"
REFERENCES,0.40804020100502514,"pages 7432–7439, 2020.
331"
REFERENCES,0.40904522613065325,"[7] Guy E Blelloch. Prefix sums and their applications. 1990.
332"
REFERENCES,0.4100502512562814,"[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
333"
REFERENCES,0.41105527638190953,"Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
334"
REFERENCES,0.4120603015075377,"Advances in neural information processing systems, 33:1877–1901, 2020.
335"
REFERENCES,0.4130653266331658,"[9] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind
336"
REFERENCES,0.414070351758794,"Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint
337"
REFERENCES,0.4150753768844221,"arXiv:1803.05457, 2018.
338"
REFERENCES,0.41608040201005025,"[10] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient
339"
REFERENCES,0.41708542713567837,"exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359,
340"
REFERENCES,0.41809045226130653,"2022.
341"
REFERENCES,0.4190954773869347,"[11] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré. Hungry hungry
342"
REFERENCES,0.4201005025125628,"hippos: Towards language modeling with state space models. In Proceedings of the 11th International
343"
REFERENCES,0.421105527638191,"Conference on Learning Representations (ICLR), 2023.
344"
REFERENCES,0.4221105527638191,"[12] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of
345"
REFERENCES,0.42311557788944726,"quantized llms. Advances in Neural Information Processing Systems, 36, 2024.
346"
REFERENCES,0.42412060301507537,"[13] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization
347"
REFERENCES,0.42512562814070354,"for generative pre-trained transformers. 2023.
348"
REFERENCES,0.42613065326633165,"[14] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry
349"
REFERENCES,0.4271356783919598,"hungry hippos: Towards language modeling with state space models. In International Conference on
350"
REFERENCES,0.42814070351758793,"Learning Representations (ICLR), 2023.
351"
REFERENCES,0.4291457286432161,"[15] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,
352"
REFERENCES,0.4301507537688442,"Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language
353"
REFERENCES,0.4311557788944724,"modeling. arXiv preprint arXiv:2101.00027, 2020.
354"
REFERENCES,0.4321608040201005,"[16] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,
355"
REFERENCES,0.43316582914572865,"Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris
356"
REFERENCES,0.43417085427135677,"Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang,
357"
REFERENCES,0.43517587939698493,"Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation,
358"
REFERENCES,0.43618090452261304,"12 2023.
359"
REFERENCES,0.4371859296482412,"[17] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-
360"
REFERENCES,0.4381909547738693,"context? a case study of simple function classes. Advances in Neural Information Processing Systems
361"
REFERENCES,0.4391959798994975,"(NeurIPS), 35:30583–30598, 2022.
362"
REFERENCES,0.4402010050251256,"[18] Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. It’s raw! audio generation with state-space
363"
REFERENCES,0.44120603015075377,"models. In International Conference on Machine Learning, pages 7616–7633. PMLR, 2022.
364"
REFERENCES,0.44221105527638194,"[19] Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, and Frank Hutter. Is mamba capable of
365"
REFERENCES,0.44321608040201005,"in-context learning? arXiv preprint arXiv:2402.03170, 2024.
366"
REFERENCES,0.4442211055276382,"[20] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh
367"
REFERENCES,0.4452261306532663,"Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language
368"
REFERENCES,0.4462311557788945,"models. arXiv preprint arXiv:2402.00838, 2024.
369"
REFERENCES,0.4472361809045226,"[21] Albert Gu and Tri Dao. Mamba Precision Guidance. ""https://github.com/state-spaces/mamba#
370"
REFERENCES,0.4482412060301508,"precision"", 2023. ""Accessed: 2024-04-25"".
371"
REFERENCES,0.4492462311557789,"[22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint
372"
REFERENCES,0.45025125628140705,"arXiv:2312.00752, 2023.
373"
REFERENCES,0.45125628140703516,"[23] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state
374"
REFERENCES,0.45226130653266333,"spaces. In International Conference on Learning Representations (ICLR), 2022.
375"
REFERENCES,0.45326633165829144,"[24] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified
376"
REFERENCES,0.4542713567839196,"view of parameter-efficient transfer learning. In International Conference on Learning Representations
377"
REFERENCES,0.4552763819095477,"(ICLR), 2021.
378"
REFERENCES,0.4562814070351759,"[25] Roee Hendel, Mor Geva, and Amir Globerson. In-context learning creates task vectors. In The 2023
379"
REFERENCES,0.457286432160804,"Conference on Empirical Methods in Natural Language Processing, 2023.
380"
REFERENCES,0.45829145728643217,"[26] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
381"
REFERENCES,0.4592964824120603,"Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning
382"
REFERENCES,0.46030150753768845,"Representations, 2020.
383"
REFERENCES,0.46130653266331656,"[27] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Ges-
384"
REFERENCES,0.4623115577889447,"mundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International
385"
REFERENCES,0.46331658291457284,"conference on machine learning, pages 2790–2799. PMLR, 2019.
386"
REFERENCES,0.464321608040201,"[28] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
387"
REFERENCES,0.4653266331658292,"Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,
388"
REFERENCES,0.4663316582914573,"2021.
389"
REFERENCES,0.46733668341708545,"[29] Huggingface. Mamba PEFT. ""https://huggingface.co/docs/transformers/en/model_doc/
390"
REFERENCES,0.46834170854271356,"mamba#peft-finetuning"", 2024. ""Accessed: 2024-04-25"".
391"
REFERENCES,0.46934673366834173,"[30] Samy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. Repeat after me: Transformers
392"
REFERENCES,0.47035175879396984,"are better than state space models at copying. arXiv preprint arXiv:2402.01032, 2024.
393"
REFERENCES,0.471356783919598,"[31] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth
394"
REFERENCES,0.4723618090452261,"Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of
395"
REFERENCES,0.4733668341708543,"bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019.
396"
REFERENCES,0.4743718592964824,"[32] AI 21 Labs. Jamba PEFT. ""https://huggingface.co/ai21labs/Jamba-v0.1"", 2024. ""Accessed:
397"
REFERENCES,0.47537688442211057,"2024-04-25"".
398"
REFERENCES,0.4763819095477387,"[33] Tanguy Laffargue, Khanh-Dang Nguyen Thu Lam, Jorge Kurchan, and Julien Tailleur. Large deviations of
399"
REFERENCES,0.47738693467336685,"lyapunov exponents. Journal of Physics A: Mathematical and Theoretical, 46(25):254002, 2013.
400"
REFERENCES,0.47839195979899496,"[34] Thomas Laurent and James von Brecht. A recurrent neural network without chaos. In Proceedings of the
401"
REFERENCES,0.4793969849246231,"11th International Conference on Learning Representations (ICLR), 2017.
402"
REFERENCES,0.48040201005025124,"[35] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning.
403"
REFERENCES,0.4814070351758794,"arXiv preprint arXiv:2104.08691, 2021.
404"
REFERENCES,0.4824120603015075,"[36] Kai Li and Guo Chen. Spmamba: State-space model is all you need in speech separation. arXiv preprint
405"
REFERENCES,0.4834170854271357,"arXiv:2404.02063, 2024.
406"
REFERENCES,0.4844221105527638,"[37] Lincan Li, Hanchen Wang, Wenjie Zhang, and Adelle Coster. Stg-mamba: Spatial-temporal graph learning
407"
REFERENCES,0.48542713567839196,"via selective state space model. arXiv preprint arXiv:2403.12418, 2024.
408"
REFERENCES,0.4864321608040201,"[38] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv
409"
REFERENCES,0.48743718592964824,"preprint arXiv:2101.00190, 2021.
410"
REFERENCES,0.4884422110552764,"[39] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.
411"
REFERENCES,0.4894472361809045,"Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.
412"
REFERENCES,0.4904522613065327,"[40] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked
413"
REFERENCES,0.4914572864321608,"Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: A hybrid transformer-mamba language
414"
REFERENCES,0.49246231155778897,"model. arXiv preprint arXiv:2403.19887, 2024.
415"
REFERENCES,0.4934673366834171,"[41] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A
416"
REFERENCES,0.49447236180904525,"Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances
417"
REFERENCES,0.49547738693467336,"in Neural Information Processing Systems, 35:1950–1965, 2022.
418"
REFERENCES,0.4964824120603015,"[42] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan
419"
REFERENCES,0.49748743718592964,"Liu. Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166, 2024.
420"
REFERENCES,0.4984924623115578,"[43] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris
421"
REFERENCES,0.4994974874371859,"Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In
422"
REFERENCES,0.5005025125628141,"International Conference on Learning Representations (ICLR), 2018.
423"
REFERENCES,0.5015075376884423,"[44] Jonas Mikhaeil, Zahra Monfared, and Daniel Durstewitz. On the difficulty of learning chaotic dynamics
424"
REFERENCES,0.5025125628140703,"with rnns. Advances in Neural Information Processing Systems, 35:11297–11312, 2022.
425"
REFERENCES,0.5035175879396985,"[45] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook
426"
REFERENCES,0.5045226130653266,"Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative study on in-context
427"
REFERENCES,0.5055276381909548,"learning tasks. International Conference on Machine Learning (ICML), 2024.
428"
REFERENCES,0.5065326633165829,"[46] Changsheng Quan and Xiaofei Li. Multichannel long-term streaming neural speech enhancement for static
429"
REFERENCES,0.507537688442211,"and moving speakers. arXiv preprint arXiv:2403.07675, 2024.
430"
REFERENCES,0.5085427135678392,"[47] Antônio H Ribeiro, Koen Tiels, Luis A Aguirre, and Thomas Schön. Beyond exploding and vanishing
431"
REFERENCES,0.5095477386934674,"gradients: analysing rnn training using attractors and smoothness. In International conference on artificial
432"
REFERENCES,0.5105527638190954,"intelligence and statistics (AISTATS), pages 2370–2380. PMLR, 2020.
433"
REFERENCES,0.5115577889447236,"[48] Jiacheng Ruan and Suncheng Xiang. Vm-unet: Vision mamba unet for medical image segmentation. arXiv
434"
REFERENCES,0.5125628140703518,"preprint arXiv:2402.02491, 2024.
435"
REFERENCES,0.5135678391959799,"[49] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial
436"
REFERENCES,0.5145728643216081,"winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.
437"
REFERENCES,0.5155778894472361,"[50] Hiroki Sayama. Introduction to the modeling and analysis of complex systems. Open SUNY Textbooks,
438"
REFERENCES,0.5165829145728643,"2015.
439"
REFERENCES,0.5175879396984925,"[51] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
440"
REFERENCES,0.5185929648241207,"and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.
441"
REFERENCES,0.5195979899497487,"com/tatsu-lab/stanford_alpaca, 2023.
442"
REFERENCES,0.5206030150753769,"[52] Teknium. Openhermes. ""https://huggingface.co/datasets/teknium/openhermes"", 2024. ""Ac-
443"
REFERENCES,0.521608040201005,"cessed: 2024-04-25"".
444"
REFERENCES,0.5226130653266332,"[53] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada,
445"
REFERENCES,0.5236180904522613,"Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation
446"
REFERENCES,0.5246231155778894,"of lm alignment. arXiv preprint arXiv:2310.16944, 2023.
447"
REFERENCES,0.5256281407035176,"[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
448"
REFERENCES,0.5266331658291458,"Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,
449"
REFERENCES,0.5276381909547738,"30, 2017.
450"
REFERENCES,0.528643216080402,"[55] Kushala VM, Harikrishna Warrier, Yogesh Gupta, et al. Fine tuning llm for enterprise: Practical guidelines
451"
REFERENCES,0.5296482412060302,"and recommendations. arXiv preprint arXiv:2404.10779, 2024.
452"
REFERENCES,0.5306532663316583,"[56] Ryan Vogt, Maximilian Puelma Touzel, Eli Shlizerman, and Guillaume Lajoie. On lyapunov exponents
453"
REFERENCES,0.5316582914572864,"for rnns: Understanding information propagation using dynamical systems tools. Frontiers in Applied
454"
REFERENCES,0.5326633165829145,"Mathematics and Statistics, 8:818799, 2022.
455"
REFERENCES,0.5336683417085427,"[57] Chloe Wang, Oleksii Tsepa, Jun Ma, and Bo Wang. Graph-mamba: Towards long-range graph sequence
456"
REFERENCES,0.5346733668341709,"modeling with selective state spaces. arXiv preprint arXiv:2402.00789, 2024.
457"
REFERENCES,0.535678391959799,"[58] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
458"
REFERENCES,0.5366834170854271,"Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.
459"
REFERENCES,0.5376884422110553,"Transactions on Machine Learning Research, 2022.
460"
REFERENCES,0.5386934673366834,"[59] Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization for
461"
REFERENCES,0.5396984924623116,"deep learning inference: Principles and empirical evaluation. arXiv preprint arXiv:2004.09602, 2020.
462"
REFERENCES,0.5407035175879397,"[60] Jianhao Xie, Ruofan Liao, Ziang Zhang, Sida Yi, Yuesheng Zhu, and Guibo Luo. Promamba: Prompt-
463"
REFERENCES,0.5417085427135678,"mamba for polyp segmentation. arXiv preprint arXiv:2403.13660, 2024.
464"
REFERENCES,0.542713567839196,"[61] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning
465"
REFERENCES,0.5437185929648242,"as implicit bayesian inference. In International Conference on Learning Representations (ICLR), 2021.
466"
REFERENCES,0.5447236180904522,"[62] Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, and Lei Zhu. Segmamba: Long-range sequential modeling
467"
REFERENCES,0.5457286432160804,"mamba for 3d medical image segmentation. arXiv preprint arXiv:2401.13560, 2024.
468"
REFERENCES,0.5467336683417086,"[63] Yijun Yang, Zhaohu Xing, and Lei Zhu. Vivim: a video vision mamba for medical video object segmenta-
469"
REFERENCES,0.5477386934673367,"tion. arXiv preprint arXiv:2401.14168, 2024.
470"
REFERENCES,0.5487437185929648,"[64] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really
471"
REFERENCES,0.549748743718593,"finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational
472"
REFERENCES,0.5507537688442211,"Linguistics, pages 4791–4800, 2019.
473"
REFERENCES,0.5517587939698493,"[65] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision
474"
REFERENCES,0.5527638190954773,"mamba: Efficient visual representation learning with bidirectional state space model. International
475"
REFERENCES,0.5537688442211055,"Conference on Machine Learning (ICML), 2024.
476"
REFERENCES,0.5547738693467337,"A
Proof of weight-tying using LoRA in the MambaBlock
477"
REFERENCES,0.5557788944723618,"Due to the low-level nature of Mamba’s prefix scan optimizations (discussed in Section 3), standard
478"
REFERENCES,0.5567839195979899,"use of LoRA adapters is made difficult within Mamba’s SSM-layer. E.g., while Bt, Ct and ∆t are
479"
REFERENCES,0.5577889447236181,"conceptually PyTorch linear layers, their bundling in a contiguous memory block and careful manip-
480"
REFERENCES,0.5587939698492462,"ulation makes appending a LoRA adapter on any of these invidiual matrices non-trivial (particularly,
481"
REFERENCES,0.5597989949748744,"while respecting the highly specialized layout of each LoRA adapters targeted layer). However, we
482"
REFERENCES,0.5608040201005026,"note that the overall design of the MambaBlock’s hardware optimizations may be leveraged to both
483"
REFERENCES,0.5618090452261306,"efficiently learn the parameter-space for the majority of time-varying parameters (thus achieving
484"
REFERENCES,0.5628140703517588,"PEFT) and regularize parameters during training (thus improving fine-tuning generalization).
485"
REFERENCES,0.563819095477387,"Theorem 1. Consider the weight matrix W of a MambaBlock from Equation 3. Targeting W for
486"
REFERENCES,0.5648241206030151,"LoRA during fine-tuning ties adaptation weights across Bt, Ct and ∆t.
487"
REFERENCES,0.5658291457286432,"Proof. Let r be the specified LoRA dimension. Targeting this matrix for LoRA results in the adapter
488"
REFERENCES,0.5668341708542713,"˜
W “W ` W1"
REFERENCES,0.5678391959798995,"“W ` UV,"
REFERENCES,0.5688442211055277,"where U P Rnˆr, V P Rrˆ3d, and W is frozen during fine-tuning. Thus, for index ri, js,
489"
REFERENCES,0.5698492462311557,"W1ri, js “ r´1
ÿ"
REFERENCES,0.5708542713567839,"k“0
Uri, ksVrk, js."
REFERENCES,0.5718592964824121,"Recall the form of W:
490"
REFERENCES,0.5728643216080402,"Wrt ´ 1, : ds “ diagp∆tq, Wrt ´ 1, d : 2ds “ diagpBtq, Wrt ´ 1, 2d : 3ds “ diagpCtq,"
REFERENCES,0.5738693467336683,"where Wr0, : ds “ diagp∆1q, Wrn ´ 1, d : 2ds “ diagpBT q, and so on. For index rt ´ 1, js, we
491"
REFERENCES,0.5748743718592965,"thus have
492"
REFERENCES,0.5758793969849246,"˜
Wrt ´ 1, js “Wrt ´ 1, js ` W1rt ´ 1, js"
REFERENCES,0.5768844221105528,"“Wrt ´ 1, js ` r´1
ÿ"
REFERENCES,0.5778894472361809,"k“0
Urt ´ 1, ksVrk, js."
REFERENCES,0.578894472361809,"Thus, the weights Urt ´ 1, :s are tied for any parameter ˜
Wrt ´ 1, js, j P t1, . . . , 3du, which are used
493"
REFERENCES,0.5798994974874372,"to adapt parameters ∆1, Bt, and Ct.
494 495"
REFERENCES,0.5809045226130654,"B
Mamba stable dynamics proof
496"
REFERENCES,0.5819095477386935,"Recall the state-space parameters and equations for the MambaBlock; A, Bt, Ct, ∆t P Rdˆd for
497"
REFERENCES,0.5829145728643216,"t P t1, . . . , nu “ rns. Given an input sequence u1, . . . , un P Rd, the following linear mapping
498"
REFERENCES,0.5839195979899497,"through latent states x1, . . . , xn P Rd is used to produce the output y1, . . . , yn P Rd:
499"
REFERENCES,0.5849246231155779,"xt “ ¯Atxt´1 ` ¯Btut
(5)"
REFERENCES,0.5859296482412061,"yt “ ¯Ctxt,
(6)"
REFERENCES,0.5869346733668341,"where ¯∆t “ softpluspLinearp∆tqq P dˆd, ¯At “ exp p ¯∆tAq, ¯Bt “ A´1p ¯A ´ IqBt, and is the
500"
REFERENCES,0.5879396984924623,"set of non-negative real numbers. In practice, A, Bt, Ct and ∆t are diagonal matrices.
501"
REFERENCES,0.5889447236180905,"Furthermore, recall the following definitions:
502"
REFERENCES,0.5899497487437186,"xt “ Fθpxt´1, utq
where θ denotes the aforementioned time-varying parameters. For input sequence ut, . . . , uT and
503"
REFERENCES,0.5909547738693467,"initial latent state value x0, we thus write
504"
REFERENCES,0.5919597989949749,"xT “ FθpFθp. . . Fθpx0, u1qqq – F T ´1
θ
px0, u1q."
REFERENCES,0.592964824120603,"We first prove that, given two scalar ε-close inputs to a MambaBlock, their deviations do not grow
505"
REFERENCES,0.5939698492462312,"exponentially as the number of recurrences increases (Lemma 1). The main result in the paper is
506"
REFERENCES,0.5949748743718593,"subsequently proved.
507"
REFERENCES,0.5959798994974874,"Lemma 1. For input px0, u1q to a MambaBlock, small changes px0 ` ε, u1 ` εq produce deviations
508"
REFERENCES,0.5969849246231156,"which are exponentially non-increasing over discrete-time. That is, max |F N
θ px0, u1q ´ F N
θ px0 `
509"
REFERENCES,0.5979899497487438,"ε, u1 ` εq| P Opε exp pNζqq, for some scalar ζ ď 0.
510"
REFERENCES,0.5989949748743718,"Proof. Firstly, we note that within the MambaBlock, A is stored in log-space followed by a negative
511"
REFERENCES,0.6,"exponentiation prior to use. Thus, A P dˆd, where is the set of non-positive real numbers.
512"
REFERENCES,0.6010050251256281,"Recall that for the maximum deviation, we have:
513"
REFERENCES,0.6020100502512563,"max |F N
θ px0, u1q ´ F N
θ px0 ` ε, u1 ` εq| P Opε exp pNλmaxqq.
where the maximal Lyapunov exponent λmax is defined as:
514"
REFERENCES,0.6030150753768844,"λmax – lim
T Ñ8
1
T log ››››› T
ź t“0"
REFERENCES,0.6040201005025125,"Bxt
Bxt´1"
REFERENCES,0.6050251256281407,"›››››
2
,"
REFERENCES,0.6060301507537689,"and }}2 denotes the spectral norm for matrices.
515"
REFERENCES,0.607035175879397,"Thus, to complete the proof, it suffices to show that λmax ď 0. Recall that A and ¯∆t are diagonal.
516"
REFERENCES,0.6080402010050251,"From Equation 5, we thus have
517"
REFERENCES,0.6090452261306533,"λmax “ lim
T Ñ8
1
T log ››››› T
ź t“0"
REFERENCES,0.6100502512562814,"Bxt
Bxt´1"
REFERENCES,0.6110552763819096,"›››››
2"
REFERENCES,0.6120603015075377,"“ lim
T Ñ8
1
T log ››››› T
ź"
REFERENCES,0.6130653266331658,"t“0
exp p ¯∆tAq"
REFERENCES,0.614070351758794,"›››››
2"
REFERENCES,0.6150753768844222,"“ lim
T Ñ8
1
T log"
REFERENCES,0.6160804020100502,›››››exp Tÿ
REFERENCES,0.6170854271356784,"t“0
p ¯∆tAq"
REFERENCES,0.6180904522613065,"›››››
2"
REFERENCES,0.6190954773869347,"Let i be the dimension which corresponds to the output of the spectral norm, i.e., i
“
518"
REFERENCES,0.6201005025125628,"argmaxj“1,...,dtexp řT
t“0p ¯∆trj, jsArj, jsqu. We thus have
519"
REFERENCES,0.6211055276381909,"λmax “ lim
T Ñ8
1
T log"
REFERENCES,0.6221105527638191,›››››exp Tÿ
REFERENCES,0.6231155778894473,"t“0
p ¯∆tAq"
REFERENCES,0.6241206030150753,"›››››
2"
REFERENCES,0.6251256281407035,"“ lim
T Ñ8
1
T log exp Tÿ"
REFERENCES,0.6261306532663317,"t“0
p ¯∆tri, isAri, isq"
REFERENCES,0.6271356783919598,"“ Ari, is lim
T Ñ8
1
T Tÿ"
REFERENCES,0.628140703517588,"t“0
¯∆tri, is"
REFERENCES,0.629145728643216,"Ari, is is non-positive and limT Ñ8 1"
REFERENCES,0.6301507537688442,"T
řT
t“0 ¯∆tri, is ě 0, since ¯∆tri, is P @t. Thus, λmax ď 0.
520"
REFERENCES,0.6311557788944724,"Theorem 2. Let pxt´1, utq be the latent state and input at an arbitrary time t P r1, Ts within a
521"
REFERENCES,0.6321608040201006,"MambaBlock. Then small changes pxt´1 ` ε, ut ` εq produce deviations which are exponentially
522"
REFERENCES,0.6331658291457286,"decreasing over discrete-time, i.e., max |F N
θ px0, u1q ´ F N
θ px0 ` ε, u1 ` εq| P Opε exp pNζqq, for
523"
REFERENCES,0.6341708542713568,"some scalar ζ ď 0.
524"
REFERENCES,0.6351758793969849,"Proof. Let τptq be a function that maps time values such that τptq P r1, T ´ ts and τptq “ 1, τpt `
525"
REFERENCES,0.6361809045226131,"1q “ 2, . . . , τpt ` Tq “ T ´ t. Then Bτptq, Cτptq, ∆τptq define a new MambaBlock with inputs
526"
REFERENCES,0.6371859296482412,"uτptq, . . . , uτpt`T q and subsequent recurrent states xτptq, . . . , xτpt`T q. Applying Lemma 1 to this
527"
REFERENCES,0.6381909547738693,"MambaBlock with pxτptq´1, uτptqq completes the proof.
528"
REFERENCES,0.6391959798994975,"C
Experimental Details
529"
REFERENCES,0.6402010050251257,"All model checkpoints were evaluated on all benchmarks and few-shot settings using the LM
530"
REFERENCES,0.6412060301507537,"evaluation harness from Eleuther AI [16], version 0.4.2. Pythia and Mamba Huggingface check-
531"
REFERENCES,0.6422110552763819,"points were used for all inference and fine-tuning experiments, e.g., EleutherAI/pythia-160m
532"
REFERENCES,0.6432160804020101,"and state-spaces/mamba-130m-hf for the smallest respective models. All fine-tuning experi-
533"
REFERENCES,0.6442211055276382,"ments were run using package versions Transformers 4.40.0.dev0, Accelerate 0.28.0, TRL
534"
REFERENCES,0.6452261306532663,"0.8.1, PyTorch 2.2.1+cu121, and PEFT 0.10.0.
535"
REFERENCES,0.6462311557788945,"For MPFT, Flash Attention 2.0 [10] via flash_attn 2.5.7 was used for Pythia mod-
536"
REFERENCES,0.6472361809045226,"els.
For FP16 and BF16 inference results, Flash Attention 2.0 was used for both Pythia
537"
REFERENCES,0.6482412060301508,"and OLMo models.
For OLMo results, the 336B-token checkpoint was used by specifying
538"
REFERENCES,0.6492462311557788,"revision=step80000-tokens336B.
539"
REFERENCES,0.650251256281407,"Outside of the OpenHermes hyperparameter search, all Alpaca and OpenHermes fine-tuning exper-
540"
REFERENCES,0.6512562814070352,"iments used the following training recipe (adapted from [53]): AdamW_torch optimizer, cosine
541"
REFERENCES,0.6522613065326633,"annealing schedule, no gradient accumulation, maximum norm of 1.0 for gradient clipping, and no
542"
REFERENCES,0.6532663316582915,"warmup steps. Training epochs used for all Alpaca and OpenHermes experiments were three and
543"
REFERENCES,0.6542713567839196,"one, respectively. For both Pythia and Mamba models, the learning rate and LoRA dimension r were
544"
REFERENCES,0.6552763819095477,"scaled to improve performance of smaller models (per-model values listed in Table 3).
545"
REFERENCES,0.6562814070351759,"For SLL LoRA, targeted Mamba layers were {x_proj, embeddings, in_proj, out_proj};
546"
REFERENCES,0.6572864321608041,"x_proj
is
the
large
MambaBlock
memory
buffer
which,
when
targeted
547"
REFERENCES,0.6582914572864321,"by
LoRA,
regularizes
the
majority
of
SSM
parameters
during
fine-tuning
548"
REFERENCES,0.6592964824120603,"through
weight
tying
(Theorem
1).
Pythia
targeted
SLL LoRA
layers
were
549"
REFERENCES,0.6603015075376885,"{dense, embed_in, query_key_value, dense_h_to_4h,dense_4h_to_h},
chosen
to
550"
REFERENCES,0.6613065326633166,"balance performance across model sizes.
551"
REFERENCES,0.6623115577889447,"All experiments in Tables 1 and 2, Figures 1 and 6 were run using a signle-GPU Nvidia A10G (24
552"
REFERENCES,0.6633165829145728,"GB total memory). For Pythia and Mamba ALL LoRA experiments in Figure 3, all experiments were
553"
REFERENCES,0.664321608040201,"run on an A10G, except for Mamba 2.8B, which exceeded A10G memory capacity and was run on
554"
REFERENCES,0.6653266331658292,an Nvidia H100 (80 GB total memory).
REFERENCES,0.6663316582914572,Table 3: Learning rate and LoRA dimension r values
REFERENCES,0.6673366834170854,"Mamba size
Pythia size
learning rate
LoRA r
130M
160M
1.0e-5
8
370M
410M
5.0e-5
16
790M
1B
1.0e-6
32
1.4B
1.4B
5.0e-6
64
2.8B
2.8B
5.0e-7
128 555"
REFERENCES,0.6683417085427136,"For the hyperparameter search results in Figure 6.1, all experiments were run using 8 H100 GPUs.
556"
REFERENCES,0.6693467336683417,"SLL LoRA was used for Mamba and Pythia models. The range of hyperparameter values was as
557"
REFERENCES,0.6703517587939698,"follows:
558"
REFERENCES,0.671356783919598,"• learning rate P t1e ´ 7, 2e ´ 7, 5e ´ 7, 1e ´ 6, 2e ´ 6, 5e ´ 6, 1e ´ 5, 2e ´ 5, 5e ´ 5, 1e ´
559"
REFERENCES,0.6723618090452261,"4, 2e ´ 4, 5e ´ 4, 1e ´ 3, 2e ´ 3, 5e ´ 3u
560"
REFERENCES,0.6733668341708543,"• LoRA dimension r P t16, 32, 64, 128, 256u
561"
REFERENCES,0.6743718592964824,"• warmup steps P t0, 1000, 2000u
562"
REFERENCES,0.6753768844221105,"All other hyperparameters followed previous experiments.
563"
REFERENCES,0.6763819095477387,"The Alpaca dataset is freely available for download at ttps://huggingface.co/datasets/
564"
REFERENCES,0.6773869346733669,"tatsu-lab/alpaca under open-source license CC-by-NC 4.0. The OpenHermes dataset is freely
565"
REFERENCES,0.678391959798995,"available for download at https://huggingface.co/datasets/teknium/OpenHermes-2.5 un-
566"
REFERENCES,0.6793969849246231,"der open-source license MIT, Apache 2.0, CC.
567"
REFERENCES,0.6804020100502512,"NeurIPS Paper Checklist
568"
CLAIMS,0.6814070351758794,"1. Claims
569"
CLAIMS,0.6824120603015076,"Question: Do the main claims made in the abstract and introduction accurately reflect the
570"
CLAIMS,0.6834170854271356,"paper’s contributions and scope?
571"
CLAIMS,0.6844221105527638,"Answer: [Yes]
572"
CLAIMS,0.685427135678392,"Justification: All claims made in the abstract and introduction are directly derived from
573"
CLAIMS,0.6864321608040201,"theoretical and experimental results presented in the main paper.
574"
CLAIMS,0.6874371859296482,"Guidelines:
575"
CLAIMS,0.6884422110552764,"• The answer NA means that the abstract and introduction do not include the claims
576"
CLAIMS,0.6894472361809045,"made in the paper.
577"
CLAIMS,0.6904522613065327,"• The abstract and/or introduction should clearly state the claims made, including the
578"
CLAIMS,0.6914572864321608,"contributions made in the paper and important assumptions and limitations. A No or
579"
CLAIMS,0.6924623115577889,"NA answer to this question will not be perceived well by the reviewers.
580"
CLAIMS,0.6934673366834171,"• The claims made should match theoretical and experimental results, and reflect how
581"
CLAIMS,0.6944723618090453,"much the results can be expected to generalize to other settings.
582"
CLAIMS,0.6954773869346733,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
583"
CLAIMS,0.6964824120603015,"are not attained by the paper.
584"
LIMITATIONS,0.6974874371859296,"2. Limitations
585"
LIMITATIONS,0.6984924623115578,"Question: Does the paper discuss the limitations of the work performed by the authors?
586"
LIMITATIONS,0.699497487437186,"Answer: [Yes]
587"
LIMITATIONS,0.700502512562814,"Justification: Limitations of experimental results are described in the limitations section,
588"
LIMITATIONS,0.7015075376884422,"under Discussion.
589"
LIMITATIONS,0.7025125628140704,"Guidelines:
590"
LIMITATIONS,0.7035175879396985,"• The answer NA means that the paper has no limitation while the answer No means that
591"
LIMITATIONS,0.7045226130653266,"the paper has limitations, but those are not discussed in the paper.
592"
LIMITATIONS,0.7055276381909548,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
593"
LIMITATIONS,0.7065326633165829,"• The paper should point out any strong assumptions and how robust the results are to
594"
LIMITATIONS,0.7075376884422111,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
595"
LIMITATIONS,0.7085427135678392,"model well-specification, asymptotic approximations only holding locally). The authors
596"
LIMITATIONS,0.7095477386934673,"should reflect on how these assumptions might be violated in practice and what the
597"
LIMITATIONS,0.7105527638190955,"implications would be.
598"
LIMITATIONS,0.7115577889447237,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
599"
LIMITATIONS,0.7125628140703517,"only tested on a few datasets or with a few runs. In general, empirical results often
600"
LIMITATIONS,0.7135678391959799,"depend on implicit assumptions, which should be articulated.
601"
LIMITATIONS,0.714572864321608,"• The authors should reflect on the factors that influence the performance of the approach.
602"
LIMITATIONS,0.7155778894472362,"For example, a facial recognition algorithm may perform poorly when image resolution
603"
LIMITATIONS,0.7165829145728643,"is low or images are taken in low lighting. Or a speech-to-text system might not be
604"
LIMITATIONS,0.7175879396984924,"used reliably to provide closed captions for online lectures because it fails to handle
605"
LIMITATIONS,0.7185929648241206,"technical jargon.
606"
LIMITATIONS,0.7195979899497488,"• The authors should discuss the computational efficiency of the proposed algorithms
607"
LIMITATIONS,0.7206030150753768,"and how they scale with dataset size.
608"
LIMITATIONS,0.721608040201005,"• If applicable, the authors should discuss possible limitations of their approach to
609"
LIMITATIONS,0.7226130653266332,"address problems of privacy and fairness.
610"
LIMITATIONS,0.7236180904522613,"• While the authors might fear that complete honesty about limitations might be used by
611"
LIMITATIONS,0.7246231155778895,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
612"
LIMITATIONS,0.7256281407035176,"limitations that aren’t acknowledged in the paper. The authors should use their best
613"
LIMITATIONS,0.7266331658291457,"judgment and recognize that individual actions in favor of transparency play an impor-
614"
LIMITATIONS,0.7276381909547739,"tant role in developing norms that preserve the integrity of the community. Reviewers
615"
LIMITATIONS,0.7286432160804021,"will be specifically instructed to not penalize honesty concerning limitations.
616"
THEORY ASSUMPTIONS AND PROOFS,0.7296482412060301,"3. Theory Assumptions and Proofs
617"
THEORY ASSUMPTIONS AND PROOFS,0.7306532663316583,"Question: For each theoretical result, does the paper provide the full set of assumptions and
618"
THEORY ASSUMPTIONS AND PROOFS,0.7316582914572864,"a complete (and correct) proof?
619"
THEORY ASSUMPTIONS AND PROOFS,0.7326633165829146,"Answer: [Yes]
620"
THEORY ASSUMPTIONS AND PROOFS,0.7336683417085427,"Justification: All theoretical results list any underlying assumptions in the main text and full
621"
THEORY ASSUMPTIONS AND PROOFS,0.7346733668341708,"proofs are available in the supplementary.
622"
THEORY ASSUMPTIONS AND PROOFS,0.735678391959799,"Guidelines:
623"
THEORY ASSUMPTIONS AND PROOFS,0.7366834170854272,"• The answer NA means that the paper does not include theoretical results.
624"
THEORY ASSUMPTIONS AND PROOFS,0.7376884422110552,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
625"
THEORY ASSUMPTIONS AND PROOFS,0.7386934673366834,"referenced.
626"
THEORY ASSUMPTIONS AND PROOFS,0.7396984924623116,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
627"
THEORY ASSUMPTIONS AND PROOFS,0.7407035175879397,"• The proofs can either appear in the main paper or the supplemental material, but if
628"
THEORY ASSUMPTIONS AND PROOFS,0.7417085427135678,"they appear in the supplemental material, the authors are encouraged to provide a short
629"
THEORY ASSUMPTIONS AND PROOFS,0.742713567839196,"proof sketch to provide intuition.
630"
THEORY ASSUMPTIONS AND PROOFS,0.7437185929648241,"• Inversely, any informal proof provided in the core of the paper should be complemented
631"
THEORY ASSUMPTIONS AND PROOFS,0.7447236180904523,"by formal proofs provided in appendix or supplemental material.
632"
THEORY ASSUMPTIONS AND PROOFS,0.7457286432160805,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7467336683417085,"4. Experimental Result Reproducibility
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7477386934673367,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7487437185929648,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.749748743718593,"of the paper (regardless of whether the code and data are provided or not)?
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7507537688442211,"Answer: [Yes]
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7517587939698492,"Justification: Relevant experimental results are detailed in the main text, with extensive
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7527638190954774,"details for all experiments further elaborated upon in the supplementary.
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7537688442211056,"Guidelines:
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7547738693467336,"• The answer NA means that the paper does not include experiments.
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7557788944723618,"• If the paper includes experiments, a No answer to this question will not be perceived
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75678391959799,"well by the reviewers: Making the paper reproducible is important, regardless of
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7577889447236181,"whether the code and data are provided or not.
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7587939698492462,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7597989949748744,"to make their results reproducible or verifiable.
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7608040201005025,"• Depending on the contribution, reproducibility can be accomplished in various ways.
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7618090452261307,"For example, if the contribution is a novel architecture, describing the architecture fully
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7628140703517587,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7638190954773869,"be necessary to either make it possible for others to replicate the model with the same
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7648241206030151,"dataset, or provide access to the model. In general. releasing code and data is often
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7658291457286432,"one good way to accomplish this, but reproducibility can also be provided via detailed
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7668341708542713,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7678391959798995,"of a large language model), releasing of a model checkpoint, or other means that are
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7688442211055276,"appropriate to the research performed.
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7698492462311558,"• While NeurIPS does not require releasing code, the conference does require all submis-
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.770854271356784,"sions to provide some reasonable avenue for reproducibility, which may depend on the
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.771859296482412,"nature of the contribution. For example
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7728643216080402,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7738693467336684,"to reproduce that algorithm.
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7748743718592965,"(b) If the contribution is primarily a new model architecture, the paper should describe
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7758793969849246,"the architecture clearly and fully.
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7768844221105528,"(c) If the contribution is a new model (e.g., a large language model), then there should
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7778894472361809,"either be a way to access this model for reproducing the results or a way to reproduce
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7788944723618091,"the model (e.g., with an open-source dataset or instructions for how to construct
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7798994974874371,"the dataset).
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7809045226130653,"(d) We recognize that reproducibility may be tricky in some cases, in which case
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7819095477386935,"authors are welcome to describe the particular way they provide for reproducibility.
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7829145728643216,"In the case of closed-source models, it may be that access to the model is limited in
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7839195979899497,"some way (e.g., to registered users), but it should be possible for other researchers
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7849246231155779,"to have some path to reproducing or verifying the results.
672"
OPEN ACCESS TO DATA AND CODE,0.785929648241206,"5. Open access to data and code
673"
OPEN ACCESS TO DATA AND CODE,0.7869346733668342,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
674"
OPEN ACCESS TO DATA AND CODE,0.7879396984924623,"tions to faithfully reproduce the main experimental results, as described in supplemental
675"
OPEN ACCESS TO DATA AND CODE,0.7889447236180904,"material?
676"
OPEN ACCESS TO DATA AND CODE,0.7899497487437186,"Answer: [No]
677"
OPEN ACCESS TO DATA AND CODE,0.7909547738693468,"Justification: While we currently answer no, and provide enough detail to reproduce our
678"
OPEN ACCESS TO DATA AND CODE,0.7919597989949749,"experiments, we are actively working towards packaging our code for release. All datasets
679"
OPEN ACCESS TO DATA AND CODE,0.792964824120603,"are already open source, with licenses listed in the supplementary material.
680"
OPEN ACCESS TO DATA AND CODE,0.7939698492462312,"Guidelines:
681"
OPEN ACCESS TO DATA AND CODE,0.7949748743718593,"• The answer NA means that paper does not include experiments requiring code.
682"
OPEN ACCESS TO DATA AND CODE,0.7959798994974875,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
683"
OPEN ACCESS TO DATA AND CODE,0.7969849246231155,"public/guides/CodeSubmissionPolicy) for more details.
684"
OPEN ACCESS TO DATA AND CODE,0.7979899497487437,"• While we encourage the release of code and data, we understand that this might not be
685"
OPEN ACCESS TO DATA AND CODE,0.7989949748743719,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
686"
OPEN ACCESS TO DATA AND CODE,0.8,"including code, unless this is central to the contribution (e.g., for a new open-source
687"
OPEN ACCESS TO DATA AND CODE,0.8010050251256281,"benchmark).
688"
OPEN ACCESS TO DATA AND CODE,0.8020100502512563,"• The instructions should contain the exact command and environment needed to run to
689"
OPEN ACCESS TO DATA AND CODE,0.8030150753768844,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
690"
OPEN ACCESS TO DATA AND CODE,0.8040201005025126,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
691"
OPEN ACCESS TO DATA AND CODE,0.8050251256281407,"• The authors should provide instructions on data access and preparation, including how
692"
OPEN ACCESS TO DATA AND CODE,0.8060301507537688,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
693"
OPEN ACCESS TO DATA AND CODE,0.807035175879397,"• The authors should provide scripts to reproduce all experimental results for the new
694"
OPEN ACCESS TO DATA AND CODE,0.8080402010050252,"proposed method and baselines. If only a subset of experiments are reproducible, they
695"
OPEN ACCESS TO DATA AND CODE,0.8090452261306532,"should state which ones are omitted from the script and why.
696"
OPEN ACCESS TO DATA AND CODE,0.8100502512562814,"• At submission time, to preserve anonymity, the authors should release anonymized
697"
OPEN ACCESS TO DATA AND CODE,0.8110552763819096,"versions (if applicable).
698"
OPEN ACCESS TO DATA AND CODE,0.8120603015075377,"• Providing as much information as possible in supplemental material (appended to the
699"
OPEN ACCESS TO DATA AND CODE,0.8130653266331658,"paper) is recommended, but including URLs to data and code is permitted.
700"
OPEN ACCESS TO DATA AND CODE,0.8140703517587939,"6. Experimental Setting/Details
701"
OPEN ACCESS TO DATA AND CODE,0.8150753768844221,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
702"
OPEN ACCESS TO DATA AND CODE,0.8160804020100503,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
703"
OPEN ACCESS TO DATA AND CODE,0.8170854271356784,"results?
704"
OPEN ACCESS TO DATA AND CODE,0.8180904522613065,"Answer: [Yes]
705"
OPEN ACCESS TO DATA AND CODE,0.8190954773869347,"Justification: All datasets are open source, and all experimental hyperparameters are specified
706"
OPEN ACCESS TO DATA AND CODE,0.8201005025125628,"in the paper. All results are fully reproducible with these details.
707"
OPEN ACCESS TO DATA AND CODE,0.821105527638191,"Guidelines:
708"
OPEN ACCESS TO DATA AND CODE,0.8221105527638191,"• The answer NA means that the paper does not include experiments.
709"
OPEN ACCESS TO DATA AND CODE,0.8231155778894472,"• The experimental setting should be presented in the core of the paper to a level of detail
710"
OPEN ACCESS TO DATA AND CODE,0.8241206030150754,"that is necessary to appreciate the results and make sense of them.
711"
OPEN ACCESS TO DATA AND CODE,0.8251256281407036,"• The full details can be provided either with the code, in appendix, or as supplemental
712"
OPEN ACCESS TO DATA AND CODE,0.8261306532663316,"material.
713"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8271356783919598,"7. Experiment Statistical Significance
714"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.828140703517588,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
715"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8291457286432161,"information about the statistical significance of the experiments?
716"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8301507537688442,"Answer: [No]
717"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8311557788944723,"Justification: The paper does not report statistical significance.
718"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8321608040201005,"Guidelines:
719"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8331658291457287,"• The answer NA means that the paper does not include experiments.
720"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8341708542713567,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
721"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8351758793969849,"dence intervals, or statistical significance tests, at least for the experiments that support
722"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8361809045226131,"the main claims of the paper.
723"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8371859296482412,"• The factors of variability that the error bars are capturing should be clearly stated (for
724"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8381909547738694,"example, train/test split, initialization, random drawing of some parameter, or overall
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8391959798994975,"run with given experimental conditions).
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8402010050251256,"• The method for calculating the error bars should be explained (closed form formula,
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8412060301507538,"call to a library function, bootstrap, etc.)
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.842211055276382,"• The assumptions made should be given (e.g., Normally distributed errors).
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.84321608040201,"• It should be clear whether the error bar is the standard deviation or the standard error
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8442211055276382,"of the mean.
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8452261306532663,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8462311557788945,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8472361809045226,"of Normality of errors is not verified.
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8482412060301507,"• For asymmetric distributions, the authors should be careful not to show in tables or
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8492462311557789,"figures symmetric error bars that would yield results that are out of range (e.g. negative
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8502512562814071,"error rates).
737"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8512562814070351,"• If error bars are reported in tables or plots, The authors should explain in the text how
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8522613065326633,"they were calculated and reference the corresponding figures or tables in the text.
739"
EXPERIMENTS COMPUTE RESOURCES,0.8532663316582915,"8. Experiments Compute Resources
740"
EXPERIMENTS COMPUTE RESOURCES,0.8542713567839196,"Question: For each experiment, does the paper provide sufficient information on the com-
741"
EXPERIMENTS COMPUTE RESOURCES,0.8552763819095477,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
742"
EXPERIMENTS COMPUTE RESOURCES,0.8562814070351759,"the experiments?
743"
EXPERIMENTS COMPUTE RESOURCES,0.857286432160804,"Answer: [Yes]
744"
EXPERIMENTS COMPUTE RESOURCES,0.8582914572864322,"Justification: The paper details (at length) the hardware requirements necessary to run each
745"
EXPERIMENTS COMPUTE RESOURCES,0.8592964824120602,"experiment. Environmental requirements are available as experimental details both in the
746"
EXPERIMENTS COMPUTE RESOURCES,0.8603015075376884,"main text and supplementary.
747"
EXPERIMENTS COMPUTE RESOURCES,0.8613065326633166,"Guidelines:
748"
EXPERIMENTS COMPUTE RESOURCES,0.8623115577889447,"• The answer NA means that the paper does not include experiments.
749"
EXPERIMENTS COMPUTE RESOURCES,0.8633165829145729,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
750"
EXPERIMENTS COMPUTE RESOURCES,0.864321608040201,"or cloud provider, including relevant memory and storage.
751"
EXPERIMENTS COMPUTE RESOURCES,0.8653266331658291,"• The paper should provide the amount of compute required for each of the individual
752"
EXPERIMENTS COMPUTE RESOURCES,0.8663316582914573,"experimental runs as well as estimate the total compute.
753"
EXPERIMENTS COMPUTE RESOURCES,0.8673366834170855,"• The paper should disclose whether the full research project required more compute
754"
EXPERIMENTS COMPUTE RESOURCES,0.8683417085427135,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
755"
EXPERIMENTS COMPUTE RESOURCES,0.8693467336683417,"didn’t make it into the paper).
756"
CODE OF ETHICS,0.8703517587939699,"9. Code Of Ethics
757"
CODE OF ETHICS,0.871356783919598,"Question: Does the research conducted in the paper conform, in every respect, with the
758"
CODE OF ETHICS,0.8723618090452261,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
759"
CODE OF ETHICS,0.8733668341708543,"Answer: [Yes]
760"
CODE OF ETHICS,0.8743718592964824,"Justification: The work detailed in the paper conforms to all aspect of the NeurIPS Code of
761"
CODE OF ETHICS,0.8753768844221106,"Ethics.
762"
CODE OF ETHICS,0.8763819095477386,"Guidelines:
763"
CODE OF ETHICS,0.8773869346733668,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
764"
CODE OF ETHICS,0.878391959798995,"• If the authors answer No, they should explain the special circumstances that require a
765"
CODE OF ETHICS,0.8793969849246231,"deviation from the Code of Ethics.
766"
CODE OF ETHICS,0.8804020100502512,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
767"
CODE OF ETHICS,0.8814070351758794,"eration due to laws or regulations in their jurisdiction).
768"
BROADER IMPACTS,0.8824120603015075,"10. Broader Impacts
769"
BROADER IMPACTS,0.8834170854271357,"Question: Does the paper discuss both potential positive societal impacts and negative
770"
BROADER IMPACTS,0.8844221105527639,"societal impacts of the work performed?
771"
BROADER IMPACTS,0.8854271356783919,"Answer: [Yes]
772"
BROADER IMPACTS,0.8864321608040201,"Justification: The societal impact of this work is addressed in the Discussion section.
773"
BROADER IMPACTS,0.8874371859296483,"Guidelines:
774"
BROADER IMPACTS,0.8884422110552764,"• The answer NA means that there is no societal impact of the work performed.
775"
BROADER IMPACTS,0.8894472361809045,"• If the authors answer NA or No, they should explain why their work has no societal
776"
BROADER IMPACTS,0.8904522613065327,"impact or why the paper does not address societal impact.
777"
BROADER IMPACTS,0.8914572864321608,"• Examples of negative societal impacts include potential malicious or unintended uses
778"
BROADER IMPACTS,0.892462311557789,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
779"
BROADER IMPACTS,0.893467336683417,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
780"
BROADER IMPACTS,0.8944723618090452,"groups), privacy considerations, and security considerations.
781"
BROADER IMPACTS,0.8954773869346734,"• The conference expects that many papers will be foundational research and not tied
782"
BROADER IMPACTS,0.8964824120603015,"to particular applications, let alone deployments. However, if there is a direct path to
783"
BROADER IMPACTS,0.8974874371859296,"any negative applications, the authors should point it out. For example, it is legitimate
784"
BROADER IMPACTS,0.8984924623115578,"to point out that an improvement in the quality of generative models could be used to
785"
BROADER IMPACTS,0.8994974874371859,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
786"
BROADER IMPACTS,0.9005025125628141,"that a generic algorithm for optimizing neural networks could enable people to train
787"
BROADER IMPACTS,0.9015075376884422,"models that generate Deepfakes faster.
788"
BROADER IMPACTS,0.9025125628140703,"• The authors should consider possible harms that could arise when the technology is
789"
BROADER IMPACTS,0.9035175879396985,"being used as intended and functioning correctly, harms that could arise when the
790"
BROADER IMPACTS,0.9045226130653267,"technology is being used as intended but gives incorrect results, and harms following
791"
BROADER IMPACTS,0.9055276381909547,"from (intentional or unintentional) misuse of the technology.
792"
BROADER IMPACTS,0.9065326633165829,"• If there are negative societal impacts, the authors could also discuss possible mitigation
793"
BROADER IMPACTS,0.907537688442211,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
794"
BROADER IMPACTS,0.9085427135678392,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
795"
BROADER IMPACTS,0.9095477386934674,"feedback over time, improving the efficiency and accessibility of ML).
796"
SAFEGUARDS,0.9105527638190954,"11. Safeguards
797"
SAFEGUARDS,0.9115577889447236,"Question: Does the paper describe safeguards that have been put in place for responsible
798"
SAFEGUARDS,0.9125628140703518,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
799"
SAFEGUARDS,0.91356783919598,"image generators, or scraped datasets)?
800"
SAFEGUARDS,0.914572864321608,"Answer: [No]
801"
SAFEGUARDS,0.9155778894472362,"Justification: The work does not aim to release pretrained models or datasets.
802"
SAFEGUARDS,0.9165829145728643,"Guidelines:
803"
SAFEGUARDS,0.9175879396984925,"• The answer NA means that the paper poses no such risks.
804"
SAFEGUARDS,0.9185929648241206,"• Released models that have a high risk for misuse or dual-use should be released with
805"
SAFEGUARDS,0.9195979899497487,"necessary safeguards to allow for controlled use of the model, for example by requiring
806"
SAFEGUARDS,0.9206030150753769,"that users adhere to usage guidelines or restrictions to access the model or implementing
807"
SAFEGUARDS,0.9216080402010051,"safety filters.
808"
SAFEGUARDS,0.9226130653266331,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
809"
SAFEGUARDS,0.9236180904522613,"should describe how they avoided releasing unsafe images.
810"
SAFEGUARDS,0.9246231155778895,"• We recognize that providing effective safeguards is challenging, and many papers do
811"
SAFEGUARDS,0.9256281407035176,"not require this, but we encourage authors to take this into account and make a best
812"
SAFEGUARDS,0.9266331658291457,"faith effort.
813"
LICENSES FOR EXISTING ASSETS,0.9276381909547738,"12. Licenses for existing assets
814"
LICENSES FOR EXISTING ASSETS,0.928643216080402,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
815"
LICENSES FOR EXISTING ASSETS,0.9296482412060302,"the paper, properly credited and are the license and terms of use explicitly mentioned and
816"
LICENSES FOR EXISTING ASSETS,0.9306532663316583,"properly respected?
817"
LICENSES FOR EXISTING ASSETS,0.9316582914572864,"Answer: [Yes]
818"
LICENSES FOR EXISTING ASSETS,0.9326633165829146,"Justification: Extensive lengths were made to cite all original authors for any and all utilized
819"
LICENSES FOR EXISTING ASSETS,0.9336683417085427,"code/data/work.
820"
LICENSES FOR EXISTING ASSETS,0.9346733668341709,"Guidelines:
821"
LICENSES FOR EXISTING ASSETS,0.935678391959799,"• The answer NA means that the paper does not use existing assets.
822"
LICENSES FOR EXISTING ASSETS,0.9366834170854271,"• The authors should cite the original paper that produced the code package or dataset.
823"
LICENSES FOR EXISTING ASSETS,0.9376884422110553,"• The authors should state which version of the asset is used and, if possible, include a
824"
LICENSES FOR EXISTING ASSETS,0.9386934673366835,"URL.
825"
LICENSES FOR EXISTING ASSETS,0.9396984924623115,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
826"
LICENSES FOR EXISTING ASSETS,0.9407035175879397,"• For scraped data from a particular source (e.g., website), the copyright and terms of
827"
LICENSES FOR EXISTING ASSETS,0.9417085427135679,"service of that source should be provided.
828"
LICENSES FOR EXISTING ASSETS,0.942713567839196,"• If assets are released, the license, copyright information, and terms of use in the
829"
LICENSES FOR EXISTING ASSETS,0.9437185929648241,"package should be provided. For popular datasets, paperswithcode.com/datasets
830"
LICENSES FOR EXISTING ASSETS,0.9447236180904522,"has curated licenses for some datasets. Their licensing guide can help determine the
831"
LICENSES FOR EXISTING ASSETS,0.9457286432160804,"license of a dataset.
832"
LICENSES FOR EXISTING ASSETS,0.9467336683417086,"• For existing datasets that are re-packaged, both the original license and the license of
833"
LICENSES FOR EXISTING ASSETS,0.9477386934673366,"the derived asset (if it has changed) should be provided.
834"
LICENSES FOR EXISTING ASSETS,0.9487437185929648,"• If this information is not available online, the authors are encouraged to reach out to
835"
LICENSES FOR EXISTING ASSETS,0.949748743718593,"the asset’s creators.
836"
NEW ASSETS,0.9507537688442211,"13. New Assets
837"
NEW ASSETS,0.9517587939698492,"Question: Are new assets introduced in the paper well documented and is the documentation
838"
NEW ASSETS,0.9527638190954774,"provided alongside the assets?
839"
NEW ASSETS,0.9537688442211055,"Answer: [NA]
840"
NEW ASSETS,0.9547738693467337,"Justification: The paper currently does not release source code. However, as previously
841"
NEW ASSETS,0.9557788944723619,"mentioned, we are actively working to remedy this.
842"
NEW ASSETS,0.9567839195979899,"Guidelines:
843"
NEW ASSETS,0.9577889447236181,"• The answer NA means that the paper does not release new assets.
844"
NEW ASSETS,0.9587939698492463,"• Researchers should communicate the details of the dataset/code/model as part of their
845"
NEW ASSETS,0.9597989949748744,"submissions via structured templates. This includes details about training, license,
846"
NEW ASSETS,0.9608040201005025,"limitations, etc.
847"
NEW ASSETS,0.9618090452261306,"• The paper should discuss whether and how consent was obtained from people whose
848"
NEW ASSETS,0.9628140703517588,"asset is used.
849"
NEW ASSETS,0.963819095477387,"• At submission time, remember to anonymize your assets (if applicable). You can either
850"
NEW ASSETS,0.964824120603015,"create an anonymized URL or include an anonymized zip file.
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9658291457286432,"14. Crowdsourcing and Research with Human Subjects
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9668341708542714,"Question: For crowdsourcing experiments and research with human subjects, does the paper
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9678391959798995,"include the full text of instructions given to participants and screenshots, if applicable, as
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9688442211055276,"well as details about compensation (if any)?
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9698492462311558,"Answer: [NA]
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9708542713567839,"Justification: [TODO]
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9718592964824121,"Guidelines:
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9728643216080402,"• The answer NA means that the paper does not involve crowdsourcing nor research with
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9738693467336683,"human subjects.
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9748743718592965,"• Including this information in the supplemental material is fine, but if the main contribu-
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758793969849247,"tion of the paper involves human subjects, then as much detail as possible should be
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9768844221105528,"included in the main paper.
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9778894472361809,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978894472361809,"or other labor should be paid at least the minimum wage in the country of the data
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798994974874372,"collector.
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9809045226130654,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9819095477386934,"Subjects
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9829145728643216,"Question: Does the paper describe potential risks incurred by study participants, whether
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9839195979899498,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9849246231155779,"approvals (or an equivalent approval/review based on the requirements of your country or
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985929648241206,"institution) were obtained?
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9869346733668342,"Answer: [NA]
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9879396984924623,"Justification: [TODO]
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9889447236180905,"Guidelines:
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9899497487437185,"• The answer NA means that the paper does not involve crowdsourcing nor research with
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9909547738693467,"human subjects.
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9919597989949749,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992964824120603,"may be required for any human subjects research. If you obtained IRB approval, you
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9939698492462311,"should clearly state this in the paper.
880"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9949748743718593,"• We recognize that the procedures for this may vary significantly between institutions
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9959798994974874,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969849246231156,"guidelines for their institution.
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979899497487437,"• For initial submissions, do not include any information that would break anonymity (if
884"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989949748743718,"applicable), such as the institution conducting the review.
885"
