Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002551020408163265,"Privacy-preserving nerual network inference has been well studied while homo-
1"
ABSTRACT,0.00510204081632653,"morphic CNN training still remains an open challenging task. In this paper, we
2"
ABSTRACT,0.007653061224489796,"present a practical solution to implement privacy-preserving CNN training based
3"
ABSTRACT,0.01020408163265306,"on mere Homomorphic Encryption (HE) technique. To our best knowledge, this
4"
ABSTRACT,0.012755102040816327,"is the ﬁrst attempt successfully to crack this nut and no work ever before has
5"
ABSTRACT,0.015306122448979591,"achieved this goal. Several techniques combine to accomplish the task:: (1) with
6"
ABSTRACT,0.017857142857142856,"transfer learning, privacy-preserving CNN training can be reduced to homomor-
7"
ABSTRACT,0.02040816326530612,"phic neural network training, or even multiclass logistic regression (MLR) train-
8"
ABSTRACT,0.02295918367346939,"ing; (2) via a faster gradient variant called Quadratic Gradient, an enhanced
9"
ABSTRACT,0.025510204081632654,"gradient method for MLR with a state-of-the-art performance in convergence
10"
ABSTRACT,0.02806122448979592,"speed is applied in this work to achieve high performance; (3) we employ the
11"
ABSTRACT,0.030612244897959183,"thought of transformation in mathematics to transform approximating Softmax
12"
ABSTRACT,0.03316326530612245,"function in the encryption domain to the approximation of the Sigmoid function.
13"
ABSTRACT,0.03571428571428571,"A new type of loss function termed Squared Likelihood Error has been de-
14"
ABSTRACT,0.03826530612244898,"veloped alongside to align with this change.; and (4) we use a simple but ﬂexible
15"
ABSTRACT,0.04081632653061224,"matrix-encoding method named Volley Revolver to manage the data ﬂow in
16"
ABSTRACT,0.04336734693877551,"the ciphertexts, which is the key factor to complete the whole homomorphic CNN
17"
ABSTRACT,0.04591836734693878,"training. The complete, runnable C++ code to implement our work can be found
18"
ABSTRACT,0.04846938775510204,"at: https://anonymous.4open.science/r/HE-CNNtraining-B355/.
19"
ABSTRACT,0.05102040816326531,"We select REGNET_X_400MF as our pre-trained model for transfer learning. We
20"
ABSTRACT,0.05357142857142857,"use the ﬁrst 128 MNIST training images as training data and the whole MNIST
21"
ABSTRACT,0.05612244897959184,"testing dataset as the testing data. The client only needs to upload 6 ciphertexts to
22"
ABSTRACT,0.058673469387755105,"the cloud and it takes ∼21 mins to perform 2 iterations on a cloud with 64 vCPUs,
23"
ABSTRACT,0.061224489795918366,"resulting in a precision of 21.49%.
24"
INTRODUCTION,0.06377551020408163,"1
Introduction
25"
BACKGROUND,0.0663265306122449,"1.1
Background
26"
BACKGROUND,0.06887755102040816,"Applying machine learning to problems involving sensitive data requires not only accurate predictions
27"
BACKGROUND,0.07142857142857142,"but also careful attention to model training. Legal and ethical requirements might limit the use of
28"
BACKGROUND,0.07397959183673469,"machine learning solutions based on a cloud service for such tasks. As a particular encryption scheme,
29"
BACKGROUND,0.07653061224489796,"homomorphic encryption provides the ultimate security for these machine learning applications and
30"
BACKGROUND,0.07908163265306123,"ensures that the data remains conﬁdential since the cloud does not need private keys to decrypt it.
31"
BACKGROUND,0.08163265306122448,"However, it is a big challenge to train the machine learning model, such as neural networks or even
32"
BACKGROUND,0.08418367346938775,"convolution neural networks, in such encrypted domains. Nonetheless, we will demonstrate that
33"
BACKGROUND,0.08673469387755102,"cloud services are capable of applying neural networks over the encrypted data to make encrypted
34"
BACKGROUND,0.08928571428571429,"training, and also return them in encrypted form.
35"
RELATED WORK,0.09183673469387756,"1.2
Related work
36"
RELATED WORK,0.09438775510204081,"Several studies on machine learning solutions are based on homomorphic encryption in the cloud
37"
RELATED WORK,0.09693877551020408,"environment. Since Gilad-Bachrach et al. [1] ﬁrstly considered privacy-preserving deep learning
38"
RELATED WORK,0.09948979591836735,"prediction models and proposed the private evaluation protocol CryptoNets for CNN, many other
39"
RELATED WORK,0.10204081632653061,"approaches [2, 3, 4, 5] for privacy-preserving deep learning prediction based on HE or its combination
40"
RELATED WORK,0.10459183673469388,"with other techniques have been developed. Also, there are several studies [6, 7, 8, 9] working on
41"
RELATED WORK,0.10714285714285714,"logistic regression models based on homomorphic encryption.
42"
RELATED WORK,0.1096938775510204,"However, to our best knowledge, no work ever before based on mere HE techique has presented an
43"
RELATED WORK,0.11224489795918367,"solution to successfully perform homomorphic CNN training.
44"
CONTRIBUTIONS,0.11479591836734694,"1.3
Contributions
45"
CONTRIBUTIONS,0.11734693877551021,"Our speciﬁc contributions in this paper are as follows:
46"
CONTRIBUTIONS,0.11989795918367346,"1. with various techniques, we initiate to propose a practical solution for privacy-preserving
47"
CONTRIBUTIONS,0.12244897959183673,"CNN training, demonstrating the feasibility of homomorphic CNN training.
48"
CONTRIBUTIONS,0.125,"2. We suggest a new type of loss function, Squared Likelihood Error (SLE), which is
49"
CONTRIBUTIONS,0.12755102040816327,"friendly to pervacy-perserving manner. As a result, we can use the Sigmoid function to
50"
CONTRIBUTIONS,0.13010204081632654,"replace the Softmax function which is too diffuclt to calculate in the encryption domain due
51"
CONTRIBUTIONS,0.1326530612244898,"to its uncertainty.
52"
CONTRIBUTIONS,0.13520408163265307,"3. We develop a new algorithm with SLE loss function for MLR using quadratic gradient.
53"
CONTRIBUTIONS,0.1377551020408163,"Experiments show that this HE-friendly algorithm has a state-of-the-art performance in
54"
CONTRIBUTIONS,0.14030612244897958,"convergence speed.
55"
PRELIMINARIES,0.14285714285714285,"2
Preliminaries
56"
PRELIMINARIES,0.14540816326530612,"We adopt “⊗” to denote the kronecker product and “⊙” to denote the component-wise multiplication
57"
PRELIMINARIES,0.14795918367346939,"between matrices.
58"
FULLY HOMOMORPHIC ENCRYPTION,0.15051020408163265,"2.1
Fully Homomorphic Encryption
59"
FULLY HOMOMORPHIC ENCRYPTION,0.15306122448979592,"Homomorphic Encryption (HE) is one type of encryption scheme with a special characteristic called
60"
FULLY HOMOMORPHIC ENCRYPTION,0.1556122448979592,"Homomorphic, which allows to compute on encrypted data without having access to the secret key.
61"
FULLY HOMOMORPHIC ENCRYPTION,0.15816326530612246,"Fully HE means that the scheme is fully homomorphic, namely, homomorphic with regards to both
62"
FULLY HOMOMORPHIC ENCRYPTION,0.16071428571428573,"addition and multiplication, and that it allows arbitrary computation on encrypted data. Since Gentry
63"
FULLY HOMOMORPHIC ENCRYPTION,0.16326530612244897,"proposed the ﬁrst fully HE scheme [10] in 2009, some technological progress on HE has been made.
64"
FULLY HOMOMORPHIC ENCRYPTION,0.16581632653061223,"For example, Brakerski, Gentry and Vaikuntanathan [11] present a novel way of constructing leveled
65"
FULLY HOMOMORPHIC ENCRYPTION,0.1683673469387755,"fully homomorphic encryption schemes (BGV) and Smart and Vercauteren [12] introduced one of the
66"
FULLY HOMOMORPHIC ENCRYPTION,0.17091836734693877,"most important features of HE systems, a packing technique based on polynomial-CRT called Single
67"
FULLY HOMOMORPHIC ENCRYPTION,0.17346938775510204,"Instruction Multiple Data (aka SIMD) to encrypt multiple values into a single ciphertext. Another
68"
FULLY HOMOMORPHIC ENCRYPTION,0.1760204081632653,"great progress in terms of machine learning applications is the rescaling procedure [13], which can
69"
FULLY HOMOMORPHIC ENCRYPTION,0.17857142857142858,"manage the magnitude of plaintext effectively.
70"
FULLY HOMOMORPHIC ENCRYPTION,0.18112244897959184,"Modern fully HE schemes, such as HEAAN, usually support seveal common homomorphic opera-
71"
FULLY HOMOMORPHIC ENCRYPTION,0.1836734693877551,"tions: the encryption algorithm Enc encrypting a vector, the decryption algorithm Dec decrypting
72"
FULLY HOMOMORPHIC ENCRYPTION,0.18622448979591838,"a ciphertext, the homomorphic addition Add and multiplication Mult between two ciphertexts, the
73"
FULLY HOMOMORPHIC ENCRYPTION,0.18877551020408162,"multiplication cMult of a contant vector with a ciphertext, the rescaling operation ReScale to reduce
74"
FULLY HOMOMORPHIC ENCRYPTION,0.1913265306122449,"the magnitude of a plaintext to an appropriate level, the rotation operation Rot generating a new
75"
FULLY HOMOMORPHIC ENCRYPTION,0.19387755102040816,"ciphertext encrypting the shifted plaintext vector, and the bootstrapping operation bootstrap to
76"
FULLY HOMOMORPHIC ENCRYPTION,0.19642857142857142,"refresh a ciphertext usually with a small ciphertext modulus.
77"
DATABASE ENCODING METHOD,0.1989795918367347,"2.2
Database Encoding Method
78"
DATABASE ENCODING METHOD,0.20153061224489796,"For a given database Z, Kim et al. [6] ﬁrst developed an efﬁcient database encoding method, in order
79"
DATABASE ENCODING METHOD,0.20408163265306123,"to make full use of the HE computation and storage resources. They ﬁrst expand the matrix database
80"
DATABASE ENCODING METHOD,0.2066326530612245,"to a vector form V in a row-by-row manner and then encrypt this vector V to obtain a ciphertext
81"
DATABASE ENCODING METHOD,0.20918367346938777,"Z = Enc(V ). Also, based on this database encoding, they mentioned two simple operations via
82"
DATABASE ENCODING METHOD,0.21173469387755103,"shifting the encrypted vector by two different positions, respectively: the complete row shifting
83"
DATABASE ENCODING METHOD,0.21428571428571427,"and the incomplete column shifting. These two operations performing on the matrix Z output the
84"
DATABASE ENCODING METHOD,0.21683673469387754,"matrices Z
′ and Z
′′, as follows:
85 Z =  "
DATABASE ENCODING METHOD,0.2193877551020408,"x10
x11
. . .
x1d
x20
x21
. . .
x2d
...
...
...
...
xn0
xn1
. . .
xnd "
DATABASE ENCODING METHOD,0.22193877551020408,",
Z
′ = Enc  "
DATABASE ENCODING METHOD,0.22448979591836735,"x20
x21
. . .
x2d
...
...
...
...
xn0
xn1
. . .
xnd
x10
x11
. . .
x1d  ,"
DATABASE ENCODING METHOD,0.22704081632653061,"Z
′′ = Enc  "
DATABASE ENCODING METHOD,0.22959183673469388,"x11
. . .
x1d
x20
x21
. . .
x2d
x30
...
...
...
...
xn1
. . .
xnd
x10 "
DATABASE ENCODING METHOD,0.23214285714285715,",
Z
′′′ = Enc  "
DATABASE ENCODING METHOD,0.23469387755102042,"x11
. . .
x1d
x10
x21
. . .
x2d
x20
...
...
...
...
xn1
. . .
xnd
xn0  ."
DATABASE ENCODING METHOD,0.2372448979591837,"The complete column shifting to obtain the matrix Z
′′′ can also be achieved by two Rot, two cMult,
86"
DATABASE ENCODING METHOD,0.23979591836734693,"and an Add.
87"
DATABASE ENCODING METHOD,0.2423469387755102,"Other works [14, 4] using the same encoding method also developed some other procedures, such
88"
DATABASE ENCODING METHOD,0.24489795918367346,"as SumRowVec and SumColVec to calculate the summation of each row and column, respectively.
89"
DATABASE ENCODING METHOD,0.24744897959183673,"Such basic common and simple operations consisting of a series of HE operations are signiﬁcantly
90"
DATABASE ENCODING METHOD,0.25,"important for more complex calculations such as the homomorphic evaluation of gradient.
91"
CONVOLUTIONAL NEURAL NETWORK,0.25255102040816324,"2.3
Convolutional Neural Network
92"
CONVOLUTIONAL NEURAL NETWORK,0.25510204081632654,"Inspired by biological processes, Convolutional Neural Networks (CNN) are a type of artiﬁcial neural
93"
CONVOLUTIONAL NEURAL NETWORK,0.2576530612244898,"network most commonly used to analyze visual images. CNNs play a signiﬁcant role in image
94"
CONVOLUTIONAL NEURAL NETWORK,0.2602040816326531,"recognition due to their powerful performance. It is also worth mentioning that the CNN model is
95"
CONVOLUTIONAL NEURAL NETWORK,0.2627551020408163,"one of a few deep learning models built with reference to the visual organization of the human brain.
96"
TRANSFER LEARNING,0.2653061224489796,"2.3.1
Transfer Learning
97"
TRANSFER LEARNING,0.26785714285714285,"Transfer learning in machine learning is a class of methods in which a pretrained model can be used
98"
TRANSFER LEARNING,0.27040816326530615,"as an optimization for a new model on a related task, allowing rapid progress in modeling the new
99"
TRANSFER LEARNING,0.2729591836734694,"task. In real-world applications, very few researchers train entire convolutional neural networks
100"
TRANSFER LEARNING,0.2755102040816326,"from scratch for image processing-related tasks. Instead, it is common to use a well-trained CNN
101"
TRANSFER LEARNING,0.2780612244897959,"as a ﬁxed feature extractor for the task of interest. In our case, we freeze all the weights of the
102"
TRANSFER LEARNING,0.28061224489795916,"selected pre-trained CNN except that of the ﬁnal fully-connected layer. We then replace the last
103"
TRANSFER LEARNING,0.28316326530612246,"fully-connected layer with a new layer with random weights (such as zeros) and only train this layer.
104"
TRANSFER LEARNING,0.2857142857142857,"REGNET_X_400MF
To use transfer learning in our privacy-preserving CNN training, we adopt
105"
TRANSFER LEARNING,0.288265306122449,"a new network design paradigm called RegNet, recently introduced by Facebook AI researchers,
106"
TRANSFER LEARNING,0.29081632653061223,"as our pre-trained model. RegNet is a low-dimensional design space consisting of simple, regular
107"
TRANSFER LEARNING,0.29336734693877553,"networks. In particular, we apply REGNET_X_400MF as a ﬁxed feature extractor and replaced the ﬁnal
108"
TRANSFER LEARNING,0.29591836734693877,"fully connected layer with a new one of zero weights. CNN training in this case can be simpliﬁed
109"
TRANSFER LEARNING,0.29846938775510207,"to multiclass logistic regression training. Since REGNET_X_400MF only receive color images of size
110"
TRANSFER LEARNING,0.3010204081632653,"224×224, the grayscale images will be stacked threefold and images of different sizes will be resized
111"
TRANSFER LEARNING,0.30357142857142855,"to the same size in advance. These two transformations can be done by using PyTorch.
112"
DATASETS,0.30612244897959184,"2.3.2
Datasets
113"
DATASETS,0.3086734693877551,"We adopt three common datasets in our experiments: MNIST, USPS, and CIFAR10. Table 1 describes
114"
DATASETS,0.3112244897959184,"the three datasets.
115"
TECHNICAL DETAILS,0.3137755102040816,"3
Technical details
116"
MULTICLASS LOGISTIC REGRESSION,0.3163265306122449,"3.1
Multiclass Logistic Regression
117"
MULTICLASS LOGISTIC REGRESSION,0.31887755102040816,"Multiclass Logistic Regression, or Multinomial Logistic Regression, can be seen as an extension of
118"
MULTICLASS LOGISTIC REGRESSION,0.32142857142857145,"logistic regression for multi-class classiﬁcation problems. Supposing that the matrix X ∈Rn×(1+d),
119"
MULTICLASS LOGISTIC REGRESSION,0.3239795918367347,Table 1: Characteristics of the several datasets used in our experiments
MULTICLASS LOGISTIC REGRESSION,0.32653061224489793,"Dataset
No. Samples
(training)"
MULTICLASS LOGISTIC REGRESSION,0.32908163265306123,"No. Samples
(testing)
No. Features
No. Classes"
MULTICLASS LOGISTIC REGRESSION,0.33163265306122447,"USPS
7,291
2,007
16×16
10
MNIST
60,000
10,000
28×28
10
CIFAR-10
50,000
10,000
3×32×32
10"
MULTICLASS LOGISTIC REGRESSION,0.33418367346938777,"the column vector Y ∈Nn×1, the matrix ¯Y ∈Rn×c, and the matrix W ∈Rc×(1+d) represent
120"
MULTICLASS LOGISTIC REGRESSION,0.336734693877551,"the dataset, class labels, the one-hot encoding of the class labels, and the MLR model parameter,
121"
MULTICLASS LOGISTIC REGRESSION,0.3392857142857143,"respectively:
122 X =  "
MULTICLASS LOGISTIC REGRESSION,0.34183673469387754,"x1
x2
...
xn  =  "
MULTICLASS LOGISTIC REGRESSION,0.34438775510204084,"x[1][0]
x[1][1]
· · ·
x[1][d]
x[2][0]
x[2][1]
· · ·
x[2][d]
...
...
...
...
x[n][0]
x[n][1]
· · ·
x[n][d]  , Y =  "
MULTICLASS LOGISTIC REGRESSION,0.3469387755102041,"y1
y2
...
yn "
MULTICLASS LOGISTIC REGRESSION,0.3494897959183674,"
one-hot encoding
7−−−−−−−−−→¯Y =  "
MULTICLASS LOGISTIC REGRESSION,0.3520408163265306,"¯y1
¯y2...
¯yn  =  "
MULTICLASS LOGISTIC REGRESSION,0.35459183673469385,"y[1][1]
y[1][2]
· · ·
y[1][c−1]
y[2][1]
y[2][2]
· · ·
y[2][c−1]
...
...
...
...
y[n][1]
y[n][2]
· · ·
y[n][c−1]  , W =  "
MULTICLASS LOGISTIC REGRESSION,0.35714285714285715,"w[0]
w[1]
...
w[c−1]  =  "
MULTICLASS LOGISTIC REGRESSION,0.3596938775510204,"w[0][0]
w[0][1]
· · ·
w[0][d]
w[1][0]
w[1][1]
· · ·
w[1][d]
...
...
...
...
w[c−1][0]
w[c−1][1]
· · ·
w[c−1][d]  ."
MULTICLASS LOGISTIC REGRESSION,0.3622448979591837,"MLR aims to maxsize L or ln L: L = n
Y i=1"
MULTICLASS LOGISTIC REGRESSION,0.3647959183673469,"exp(xi · w⊺
[yi])
Pc−1
k=0 exp(xi · w⊺
[k])
7−−→ln L = n
X"
MULTICLASS LOGISTIC REGRESSION,0.3673469387755102,"i=1
[xi · w⊺
[yi] −ln c−1
X"
MULTICLASS LOGISTIC REGRESSION,0.36989795918367346,"k=0
exp(xi · w⊺
[k])]."
MULTICLASS LOGISTIC REGRESSION,0.37244897959183676,"The loss function ln L is a multivariate function of [(1 + c)(1 + d)] variables, which has its column-
123"
MULTICLASS LOGISTIC REGRESSION,0.375,"vector gradient ∇of size [(1 + c)(1 + d)] and Hessian square matrix ∇2 of order [(1 + c)(1 + d)] as
124"
MULTICLASS LOGISTIC REGRESSION,0.37755102040816324,"follows:
125"
MULTICLASS LOGISTIC REGRESSION,0.38010204081632654,∇= ∂ln L
MULTICLASS LOGISTIC REGRESSION,0.3826530612244898,"∂π
=
h∂ln L"
MULTICLASS LOGISTIC REGRESSION,0.3852040816326531,"∂w[0]
, ∂ln L"
MULTICLASS LOGISTIC REGRESSION,0.3877551020408163,"∂w[1]
, . . . , ∂ln L"
MULTICLASS LOGISTIC REGRESSION,0.3903061224489796,"∂w[c−1] i⊺
, ∇2 = "
MULTICLASS LOGISTIC REGRESSION,0.39285714285714285,
MULTICLASS LOGISTIC REGRESSION,0.39540816326530615,"∂2 ln L
∂w[0]∂w[0]
∂2 ln L
∂w[0]∂w[1]
· · ·
∂2 ln L
∂w[0]∂w[c−1]
∂2 ln L
∂w[1]∂w[0]
∂2 ln L
∂w[1]∂w[1]
· · ·
∂2 ln L
∂w[1]∂w[c−1]
...
...
...
...
∂2 ln L
∂w[c−1]∂w[0]
∂2 ln L
∂w[c−1]∂w[1]
· · ·
∂2 ln L
∂w[c−1]∂w[c−1] "
MULTICLASS LOGISTIC REGRESSION,0.3979591836734694,"
."
MULTICLASS LOGISTIC REGRESSION,0.4005102040816326,"Nesterov’s Accelerated Gradient
With ∇or ∇2, ﬁrst-order gradient algorithms or second-order
126"
MULTICLASS LOGISTIC REGRESSION,0.4030612244897959,"Newton–Raphson method are commonly applied in MLE to maxmise ln L. In particular, Nesterov’s
127"
MULTICLASS LOGISTIC REGRESSION,0.40561224489795916,"Accelerated Gradient (NAG) is a practical solution for homomorphic MLR without frequent inversion
128"
MULTICLASS LOGISTIC REGRESSION,0.40816326530612246,"operations. It seems plausible that the NAG method is probably the best choice for privacy-preserving
129"
MULTICLASS LOGISTIC REGRESSION,0.4107142857142857,"model training.
130"
MULTICLASS LOGISTIC REGRESSION,0.413265306122449,"3.2
Chiang’s Quadratic Gradient
131"
MULTICLASS LOGISTIC REGRESSION,0.41581632653061223,"Chiang's Quadratic Gradient (CQG) [15, 16, 9] is a faster, promising gradient variant that can
132"
MULTICLASS LOGISTIC REGRESSION,0.41836734693877553,"combine the ﬁrst-order gradient descent/ascent algorithms and the second-order Newton–Raphson
133"
MULTICLASS LOGISTIC REGRESSION,0.42091836734693877,"method, accelerating the raw Newton–Raphson method with various gradient algorithms and probably
134"
MULTICLASS LOGISTIC REGRESSION,0.42346938775510207,"helpful to build super-quadratic algorithms. For a function F(x) with its gradient g and Hessian
135"
MULTICLASS LOGISTIC REGRESSION,0.4260204081632653,"matrix H, to build CQG, we ﬁrst construct a diagonal matrix ¯B from the Hessian H itself:
136 ¯B =  "
MULTICLASS LOGISTIC REGRESSION,0.42857142857142855,"1
ε+Pd
i=0 |¯h0i|
0
. . .
0"
MULTICLASS LOGISTIC REGRESSION,0.43112244897959184,"0
1
ε+Pd
i=0 |¯h1i|
. . .
0
...
...
...
...
0
0
. . .
1
ε+Pd
i=0 |¯hdi| "
MULTICLASS LOGISTIC REGRESSION,0.4336734693877551,"
,"
MULTICLASS LOGISTIC REGRESSION,0.4362244897959184,"where ¯hji is the elements of the matrix H and ε is a small constant positive number.
137"
MULTICLASS LOGISTIC REGRESSION,0.4387755102040816,"CQG for the function F(x), deﬁned as G = ¯B · g, has the same dimension as the raw gradient g. To
138"
MULTICLASS LOGISTIC REGRESSION,0.4413265306122449,"apply CQG in practice, we can use it in the same way as the ﬁrst-order gradient algorithms, except
139"
MULTICLASS LOGISTIC REGRESSION,0.44387755102040816,"that we need to replace the naive gradient with the quadratic gradient and adopt a new learning rate
140"
MULTICLASS LOGISTIC REGRESSION,0.44642857142857145,"(usually by increasing 1 to the original learning rate).
141"
MULTICLASS LOGISTIC REGRESSION,0.4489795918367347,"For efﬁciency in applying CQG, a good bound matrix should be attempted to obtain in order to
142"
MULTICLASS LOGISTIC REGRESSION,0.45153061224489793,"replace the Hessian itself. Chiang has proposed the enhanced NAG method via CQG for MLR with a
143"
MULTICLASS LOGISTIC REGRESSION,0.45408163265306123,"ﬁxed Hessian [17, 7, 18] substitute built from 1"
MULTICLASS LOGISTIC REGRESSION,0.45663265306122447,"2X⊺X.
144"
APPROXIMATING SOFTMAX FUNCTION,0.45918367346938777,"3.3
Approximating Softmax Function
145"
APPROXIMATING SOFTMAX FUNCTION,0.461734693877551,"It might be impractical to perfectly approximate Softmax function in the privacy-preserving domain
146"
APPROXIMATING SOFTMAX FUNCTION,0.4642857142857143,"due to its uncertainty. To address this issue, we employ the thought of transformation from mathemat-
147"
APPROXIMATING SOFTMAX FUNCTION,0.46683673469387754,"ics: transforming one tough problem into another easier one. That is, instead of trying to approximate
148"
APPROXIMATING SOFTMAX FUNCTION,0.46938775510204084,"the Softmax function, we attempt to approximate the Sigmoid function in the encryption domain,
149"
APPROXIMATING SOFTMAX FUNCTION,0.4719387755102041,"which has been well-studied by several works using the least-square method.
150"
APPROXIMATING SOFTMAX FUNCTION,0.4744897959183674,"In line with standard practice of the log-likelihood loss function involving the Softmax function, we
should try to maximize the new loss function L1 = n
Y i=1"
APPROXIMATING SOFTMAX FUNCTION,0.4770408163265306,"1
1 + exp(−xi · w⊺
[yi])."
APPROXIMATING SOFTMAX FUNCTION,0.47959183673469385,We can prove that ln L1 is concave and deduce that 1
APPROXIMATING SOFTMAX FUNCTION,0.48214285714285715,"4E ⊗X⊺X can be used to build the CQG for
151"
APPROXIMATING SOFTMAX FUNCTION,0.4846938775510204,"ln L1. However, the performance of this loss function ln L1 is not ideal, probably because for the
152"
APPROXIMATING SOFTMAX FUNCTION,0.4872448979591837,"individual example its gradient and Hessian contain no information about any other class weights not
153"
APPROXIMATING SOFTMAX FUNCTION,0.4897959183673469,"related to this example.
154"
APPROXIMATING SOFTMAX FUNCTION,0.4923469387755102,"Squared Likelihood Error
After many attempts to ﬁnding a proper loss function, we develop
a novel loss function that can have a competitive performance to the log-likelihood loss function,
which we term Squared Likelihood Error (SLE): L2 = n
Y i=1 c−1
Y"
APPROXIMATING SOFTMAX FUNCTION,0.49489795918367346,"j=0
(¯yi −Sigmoid(xi · w⊺
[yi])2 7−−→ln L2 = n
X i=1 c−1
X"
APPROXIMATING SOFTMAX FUNCTION,0.49744897959183676,"j=0
ln |¯yi −Sigmoid(xi · w⊺
[yi])|."
APPROXIMATING SOFTMAX FUNCTION,0.5,We can also prove that ln L2 is concave and that 1
APPROXIMATING SOFTMAX FUNCTION,0.5025510204081632,"4E ⊗X⊺X can be used to build the CQG for ln L2.
155"
APPROXIMATING SOFTMAX FUNCTION,0.5051020408163265,"The loss function SLE might be related to Mean Squared Error (MSE): the MSE loss function sums
156"
APPROXIMATING SOFTMAX FUNCTION,0.5076530612244898,"all the squared errors while SLE calculates the cumulative product of all the squared likelihood errors.
157"
APPROXIMATING SOFTMAX FUNCTION,0.5102040816326531,"Combining together all the techniques above, we now have the enhanced NAG method with the SLE
158"
APPROXIMATING SOFTMAX FUNCTION,0.5127551020408163,"loss function for MLR training, described in detail in Algorithm 1.
159"
APPROXIMATING SOFTMAX FUNCTION,0.5153061224489796,"Performance Evaluation
We test the convergence speed of the raw NAG method with log-
160"
APPROXIMATING SOFTMAX FUNCTION,0.5178571428571429,"likelihood loss function (denoted as RawNAG), the NAG method with SLE loss function (denoted
161"
APPROXIMATING SOFTMAX FUNCTION,0.5204081632653061,"as SigmoidNAG), and the enhanced NAG method via CQG with SLE loss function (denoted as
162"
APPROXIMATING SOFTMAX FUNCTION,0.5229591836734694,"SigmoidNAGQG) on the three datasets described above: USPS, MNIST, and CIFAR10. Since two
163"
APPROXIMATING SOFTMAX FUNCTION,0.5255102040816326,"different types of loss functions are used in these three methods, the loss function directly measuring
164"
APPROXIMATING SOFTMAX FUNCTION,0.5280612244897959,"the performance of various methods will not be selected as the indicator. Instead, we select precision
165"
APPROXIMATING SOFTMAX FUNCTION,0.5306122448979592,"as the only indicator in the following Python experiments. Note that we use REGNET_X_400MF to in
166"
APPROXIMATING SOFTMAX FUNCTION,0.5331632653061225,Algorithm 1 The Enhanced NAG method with the SLE loss function for MLR Training
APPROXIMATING SOFTMAX FUNCTION,0.5357142857142857,"Input: training dataset X ∈Rn×(1+d); one-hot encoding training label Y ∈Rn×c; and the number
κ of iterations;
Output: the parameter matrix V ∈Rc×(1+d) of the MLR"
APPROXIMATING SOFTMAX FUNCTION,0.5382653061224489,1: Set ¯H ←−1
APPROXIMATING SOFTMAX FUNCTION,0.5408163265306123,"4X⊺X
▷¯H ∈R(1+d)×(1+d)"
APPROXIMATING SOFTMAX FUNCTION,0.5433673469387755,"2: Set V ←0, W ←0, ¯B ←0
▷V ∈Rc×(1+d), W ∈Rc×(1+d), ¯B ∈Rc×(1+d)"
APPROXIMATING SOFTMAX FUNCTION,0.5459183673469388,"3: for j := 0 to d do
4:
¯B[0][j] ←ε
▷ε is a small positive constant such as 1e −10
5:
for i := 0 to d do
6:
¯B[0][j] ←¯B[0][j] + | ¯H[i][j]|
7:
end for
8:
for i := 1 to c −1 do
9:
¯B[i][j] ←¯B[0][j]
10:
end for
11:
for i := 0 to c −1 do
12:
¯B[i][j] ←1.0/ ¯B[i][j]
13:
end for
14: end for
15: Set α0 ←0.01, α1 ←0.5 × (1 +
p"
APPROXIMATING SOFTMAX FUNCTION,0.548469387755102,"1 + 4 × α2
0)
16: for count := 1 to κ do
17:
Set Z ←X × V ⊺
▷Z ∈Rn×c and V ⊺means the transpose of matrix V
18:
for i := 1 to n do
▷Z is going to store the inputs to the Sigmoid function
19:
for j := 0 to d do
20:
Z[i][j] ←1/(1 + e−Z[i][j])
21:
end for
22:
end for
23:
Set g ←(Y −Z)⊺× X
▷g ∈Rc×(1+d)"
APPROXIMATING SOFTMAX FUNCTION,0.5510204081632653,"24:
Set G ←0
25:
for i := 0 to c −1 do
26:
for j := 0 to d do
27:
G[i][j] ←¯B[i][j] × g[i][j]
28:
end for
29:
end for
30:
Set η ←(1 −α0)/α1, γ ←1/(n × count)
▷n is the size of training data
31:
wtemp ←W + (1 + γ) × G
32:
W ←(1 −η) × wtemp + η × V
33:
V ←wtemp
34:
α0 ←α1, α1 ←0.5 × (1 +
p"
APPROXIMATING SOFTMAX FUNCTION,0.5535714285714286,"1 + 4 × α2
0)
35: end for
36: return W"
APPROXIMATING SOFTMAX FUNCTION,0.5561224489795918,"advance extract the features of USPS, MNIST, and CIFAR10, resulting in a new same-size dataset
167"
APPROXIMATING SOFTMAX FUNCTION,0.5586734693877551,"with 401 features of each example. Figure 1 shows that our enhanced methods all converge faster
168"
APPROXIMATING SOFTMAX FUNCTION,0.5612244897959183,"than other algorithms on the three datasets.
169"
DOUBLE VOLLEY REVOLVER,0.5637755102040817,"3.4
Double Volley Revolver
170"
DOUBLE VOLLEY REVOLVER,0.5663265306122449,"Unlike those efﬁcient, complex encoding methods [3], Volley Revolver is a simple, ﬂexible
171"
DOUBLE VOLLEY REVOLVER,0.5688775510204082,"matrix-encoding method specialized for privacy-preserving machine-learning applications, whose
172"
DOUBLE VOLLEY REVOLVER,0.5714285714285714,"basic idea in a simple version is to encrypt the transpose of the second matrix for two matrices to
173"
DOUBLE VOLLEY REVOLVER,0.5739795918367347,"perform multiplication. Figure 2 describes a simple case for the algorithm adopted in this encoding
174"
DOUBLE VOLLEY REVOLVER,0.576530612244898,"method.
175"
DOUBLE VOLLEY REVOLVER,0.5790816326530612,"The encoding method actually plays a signiﬁcant role in implementing privacy-preserving CNN
176"
DOUBLE VOLLEY REVOLVER,0.5816326530612245,"training. Just as Chiang mentioned in [4], we show that Volley Revolver can indeed be used to
177"
DOUBLE VOLLEY REVOLVER,0.5841836734693877,"implement homomorphic CNN training. This simple encoding method can help to control and
178"
DOUBLE VOLLEY REVOLVER,0.5867346938775511,"manage the data ﬂow through ciphertexts.
179"
DOUBLE VOLLEY REVOLVER,0.5892857142857143,"0
100
200
300 0.5 1"
DOUBLE VOLLEY REVOLVER,0.5918367346938775,Iteration Number
DOUBLE VOLLEY REVOLVER,0.5943877551020408,"RawNAG
SigmoidNAG
SigmoidNAGQG"
DOUBLE VOLLEY REVOLVER,0.5969387755102041,(a) USPS Training
DOUBLE VOLLEY REVOLVER,0.5994897959183674,"0
100
200
300
0 0.2 0.4 0.6 0.8 1"
DOUBLE VOLLEY REVOLVER,0.6020408163265306,Iteration Number
DOUBLE VOLLEY REVOLVER,0.6045918367346939,"RawNAG
SigmoidNAG
SigmoidNAGQG"
DOUBLE VOLLEY REVOLVER,0.6071428571428571,(b) USPS Testing
DOUBLE VOLLEY REVOLVER,0.6096938775510204,"0
100
200
300 0.2 0.4 0.6 0.8 1"
DOUBLE VOLLEY REVOLVER,0.6122448979591837,Iteration Number
DOUBLE VOLLEY REVOLVER,0.6147959183673469,"RawNAG
SigmoidNAG
SigmoidNAGQG"
DOUBLE VOLLEY REVOLVER,0.6173469387755102,(c) MNIST Training
DOUBLE VOLLEY REVOLVER,0.6198979591836735,"0
100
200
300 0.2 0.4 0.6 0.8 1"
DOUBLE VOLLEY REVOLVER,0.6224489795918368,Iteration Number
DOUBLE VOLLEY REVOLVER,0.625,"RawNAG
SigmoidNAG
SigmoidNAGQG"
DOUBLE VOLLEY REVOLVER,0.6275510204081632,(d) MNIST Testing
DOUBLE VOLLEY REVOLVER,0.6301020408163265,"0
100
200
300 0.2 0.4 0.6 0.8"
DOUBLE VOLLEY REVOLVER,0.6326530612244898,Iteration Number
DOUBLE VOLLEY REVOLVER,0.6352040816326531,"RawNAG
SigmoidNAG
SigmoidNAGQG"
DOUBLE VOLLEY REVOLVER,0.6377551020408163,(e) CIFAR10 Training
DOUBLE VOLLEY REVOLVER,0.6403061224489796,"0
100
200
300 0.2 0.4 0.6 0.8"
DOUBLE VOLLEY REVOLVER,0.6428571428571429,Iteration Number
DOUBLE VOLLEY REVOLVER,0.6454081632653061,"RawNAG
SigmoidNAG
SigmoidNAGQG"
DOUBLE VOLLEY REVOLVER,0.6479591836734694,(f) CIFAR10 Testing
DOUBLE VOLLEY REVOLVER,0.6505102040816326,"Figure 1: Training and Testing precision results for raw NAG vs. NAG with SLE vs. The enhanced
NAG with SLE ·"
DOUBLE VOLLEY REVOLVER,0.6530612244897959,"a0
a1
b0
b2"
DOUBLE VOLLEY REVOLVER,0.6556122448979592,"a2
a3
b1
b3"
DOUBLE VOLLEY REVOLVER,0.6581632653061225,"a4
a5
b0
b2"
DOUBLE VOLLEY REVOLVER,0.6607142857142857,"a6
a7
b1
b3 × a0
a1"
DOUBLE VOLLEY REVOLVER,0.6632653061224489,"a2
a3
b0
b1"
DOUBLE VOLLEY REVOLVER,0.6658163265306123,"a4
a5
b2
b3 a6
a7 0
0 0
0 0
0 0
0 ·"
DOUBLE VOLLEY REVOLVER,0.6683673469387755,"a0
a1
b0
b2"
DOUBLE VOLLEY REVOLVER,0.6709183673469388,"a2
a3
b1
b3"
DOUBLE VOLLEY REVOLVER,0.673469387755102,"a4
a5
b0
b2"
DOUBLE VOLLEY REVOLVER,0.6760204081632653,"a6
a7
b1
b3"
DOUBLE VOLLEY REVOLVER,0.6785714285714286,"c0 = a0 · b0 + a1 · b2
c3 = a2 · b1 + a3 · b3 c0
c0 c3
c3 c4
c4 c7
c7"
DOUBLE VOLLEY REVOLVER,0.6811224489795918,"c4 = a4 · b0 + a5 · b2
c7 = a6 · b1 + a7 · b3 c0
0 0
c3 c4
0 0
c7 ·"
DOUBLE VOLLEY REVOLVER,0.6836734693877551,"a0
a1
b1
b3"
DOUBLE VOLLEY REVOLVER,0.6862244897959183,"a2
a3
b0
b2"
DOUBLE VOLLEY REVOLVER,0.6887755102040817,"a4
a5
b1
b3"
DOUBLE VOLLEY REVOLVER,0.6913265306122449,"a6
a7
b0
b2"
DOUBLE VOLLEY REVOLVER,0.6938775510204082,"c1 = a0 · b1 + a1 · b3
c2 = a2 · b0 + a3 · b2 c1
c1 c2
c2 c5
c5 c6
c6"
DOUBLE VOLLEY REVOLVER,0.6964285714285714,"c5 = a4 · b1 + a5 · b3
c6 = a6 · b0 + a7 · b2 0
c1 c2
0 0
c5 c6
0"
DOUBLE VOLLEY REVOLVER,0.6989795918367347,Encrypt
DOUBLE VOLLEY REVOLVER,0.701530612244898,Encoding
DOUBLE VOLLEY REVOLVER,0.7040816326530612,Rot(0)
DOUBLE VOLLEY REVOLVER,0.7066326530612245,Rot(2)
DOUBLE VOLLEY REVOLVER,0.7091836734693877,"SumColVec(·)
Clean up the"
DOUBLE VOLLEY REVOLVER,0.7117346938775511,redundant values
DOUBLE VOLLEY REVOLVER,0.7142857142857143,"SumColVec(·)
Clean up the"
DOUBLE VOLLEY REVOLVER,0.7168367346938775,"redundant values ⊕
⊕"
DOUBLE VOLLEY REVOLVER,0.7193877551020408,"Figure 2: The matrix multiplication algorithm of Volley Revolver for the 4 × 2 matrix A and the
matrix B of size 2 × 2"
DOUBLE VOLLEY REVOLVER,0.7219387755102041,"However, we don’t need to stick to encrypting the transpose of the second matrix. Instead, either of
180"
DOUBLE VOLLEY REVOLVER,0.7244897959183674,"the two matrices is transposed would do the trick: we could also encrypt the transpose of the ﬁrst
181"
DOUBLE VOLLEY REVOLVER,0.7270408163265306,"matrix, and the corresponding multiplication algorithm due to this change is similar to the Algorithm
182"
DOUBLE VOLLEY REVOLVER,0.7295918367346939,"2 from [4].
183"
DOUBLE VOLLEY REVOLVER,0.7321428571428571,"Also, if each of the two matrices are too large to be encrypted into a single ciphertext, we could also
184"
DOUBLE VOLLEY REVOLVER,0.7346938775510204,"encrypt the two matrices into two teams A and B of multiple ciphertexts. In this case, we can see this
185"
DOUBLE VOLLEY REVOLVER,0.7372448979591837,"encoding method as Double Volley Revolver, which has two loops: the outside loop deals with
186"
DOUBLE VOLLEY REVOLVER,0.7397959183673469,"the calculations between ciphertexts from two teams while the inside loop literally calculates two
187"
DOUBLE VOLLEY REVOLVER,0.7423469387755102,"sub-matrices encrypted by two ciphertexts A[i] and B[j] using the raw algorithm of Volley Revolver.
188"
PRIVACY-PRESERVING CNN TRAINING,0.7448979591836735,"4
Privacy-preserving CNN Training
189"
POLYNOMIAL APPROXIMATION,0.7474489795918368,"4.1
Polynomial Approximation
190"
POLYNOMIAL APPROXIMATION,0.75,"Although Algorithm 1 enables us to avoid computing the Softmax function in the encryption domain,
191"
POLYNOMIAL APPROXIMATION,0.7525510204081632,"we still need to calculate the Sigmoid function using HE technique. This problem has been well
192"
POLYNOMIAL APPROXIMATION,0.7551020408163265,"studied by several works and we adopt a simple one [19], that is (1) we ﬁrst use the least-square method
193"
POLYNOMIAL APPROXIMATION,0.7576530612244898,"to perfectly approximate the sigmoid function over the range [−8, +8], obtaining a polynomial Z11
194"
POLYNOMIAL APPROXIMATION,0.7602040816326531,"of degree 11; and (2) we use a polynomial Z3 of degree 3 to approximate the Sigmoid by minimizing
195"
POLYNOMIAL APPROXIMATION,0.7627551020408163,"the cost function F including the squared gradient difference:
196"
POLYNOMIAL APPROXIMATION,0.7653061224489796,"F = λ0 ·
Z +8"
POLYNOMIAL APPROXIMATION,0.7678571428571429,"−8
(Z11 −Z3)2dx + λ1 ·
Z +8"
POLYNOMIAL APPROXIMATION,0.7704081632653061,"−8
(Z
′
11 −Z
′
3)2dx,"
POLYNOMIAL APPROXIMATION,0.7729591836734694,"where λ0 and λ1 are two positive ﬂoat numbers to control the shape of the polynomial to approximate.
197"
POLYNOMIAL APPROXIMATION,0.7755102040816326,"Setting λ0 = 128 and λ1 = 1 would result in the polynomial we used in our privacy-preserving CNN
198"
POLYNOMIAL APPROXIMATION,0.7780612244897959,"training:Z3 = 0.5 + 0.106795345032 · x −0.000385032598 · x3.
199"
HOMOMORPHIC EVALUATION,0.7806122448979592,"4.2
Homomorphic Evaluation
200"
HOMOMORPHIC EVALUATION,0.7831632653061225,"Before the homomorphic CNN training starts, the client needs to encrypt the dataset X, the data
201"
HOMOMORPHIC EVALUATION,0.7857142857142857,"labels ¯Y , the matrix ¯B and the weight W into ciphertexts Enc(X), Enc( ¯Y ), Enc( ¯B) and Enc(W),
202"
HOMOMORPHIC EVALUATION,0.7882653061224489,"respectively, and upload them to the cloud. For simplicity in presentation, we can just regard
203"
HOMOMORPHIC EVALUATION,0.7908163265306123,"the whole pipeline of homomorphic evaluation of Algorithm 1 as updating the weight ciphertext:
204"
HOMOMORPHIC EVALUATION,0.7933673469387755,"W = W + ¯B ⊙( ¯Y −Z3(X × W ⊺))⊺× X, regardless of the subtle control of the enhanced NAG
205"
HOMOMORPHIC EVALUATION,0.7959183673469388,"method with the SLE loss function.
206"
HOMOMORPHIC EVALUATION,0.798469387755102,"Since Volley Revolver only needs one of the two matrices to be transposed ahead before en-
207"
HOMOMORPHIC EVALUATION,0.8010204081632653,"cryption and ( ¯Y −Z3(X × W ⊺))⊺× X happened to sufﬁce this situation between any matrix
208"
HOMOMORPHIC EVALUATION,0.8035714285714286,"multiplication, we can complete the homomorphic evaluation of CQG for MLR.
209"
EXPERIMENTS,0.8061224489795918,"5
Experiments
210"
EXPERIMENTS,0.8086734693877551,"The C++ source code to implement the experiments in this section is openly available at:
211"
EXPERIMENTS,0.8112244897959183,"https://anonymous.4open.science/r/HE-CNNtraining-B355/ .
212"
EXPERIMENTS,0.8137755102040817,"Implementation
We implement the enhanced NAG with the SLE loss function based on HE with
213"
EXPERIMENTS,0.8163265306122449,"the library HEAAN. All the experiments on the ciphertexts were conducted on a public cloud with 64
214"
EXPERIMENTS,0.8188775510204082,"vCPUs and 192 GB RAM.
215"
EXPERIMENTS,0.8214285714285714,"We adopt the ﬁrst 128 MNIST training images as the training data and the whole test dataset as the
216"
EXPERIMENTS,0.8239795918367347,"testing data. Both the training images and testing images have been processed in advance with the
217"
EXPERIMENTS,0.826530612244898,"pre-trained model REGNET_X_400MF, resulting in a new dataset with each example of size 401.
218"
PARAMETERS,0.8290816326530612,"5.1
Parameters
219"
PARAMETERS,0.8316326530612245,"The parameters of HEAAN we selected are: logN = 16, logQ = 990, logp = 45, slots = 32768,
220"
PARAMETERS,0.8341836734693877,"which ensure the security level λ = 128. Refer [6] for the details of these parameters. We didn’t
221"
PARAMETERS,0.8367346938775511,"use bootstrapping to refresh the weight ciphertexts and thus it can only perform 2 iterations of our
222"
PARAMETERS,0.8392857142857143,"algorithm. Each iteration takes ∼11mins. The maximum runtime memory in this case is ∼18 GB.
223"
PARAMETERS,0.8418367346938775,"The 128 MNIST training images are encrypted into 2 ciphertexts. The client who own the private data
224"
PARAMETERS,0.8443877551020408,"has to upload these two ciphertexts, two ciphertexts encrypting the one-hot labels ¯Y , one ciphertext
225"
PARAMETERS,0.8469387755102041,"encrypting the ¯B and one ciphertext encrypting the weight W to the cloud. The inticial weight matrix
226"
PARAMETERS,0.8494897959183674,"W0 we adopted is the zero matrix. The resulting MLR model after 2-iteration training has reached a
227"
PARAMETERS,0.8520408163265306,"pricision of 21.49% and obtain the loss of −147206, which are consistent with the Python simulation
228"
PARAMETERS,0.8545918367346939,"experiment.
229"
CONCLUSION,0.8571428571428571,"6
Conclusion
230"
CONCLUSION,0.8596938775510204,"In this work, we initiated to implement privacy-persevering CNN training based on mere HE tech-
231"
CONCLUSION,0.8622448979591837,"niques by presenting a faster HE-friendly algorithm.
232"
CONCLUSION,0.8647959183673469,"The HE operation bootstrapping could be adopted to refresh the weight ciphertexts. Python exper-
233"
CONCLUSION,0.8673469387755102,"iments imitating the privacy-preserving CNN training using Z3 as Sigmoid substitution showed
234"
CONCLUSION,0.8698979591836735,"that using a large amount of data such as 8,192 images to train the MLE model for hundreds of
235"
CONCLUSION,0.8724489795918368,"iterations would ﬁnally reach 95% precision. The real experiments over ciphertexts conducted on a
236"
CONCLUSION,0.875,"high-performance cloud with many vCPUs would take weeks to complete this test, if not months.
237"
REFERENCES,0.8775510204081632,"References
238"
REFERENCES,0.8801020408163265,"[1] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John
239"
REFERENCES,0.8826530612244898,"Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and
240"
REFERENCES,0.8852040816326531,"accuracy. In International conference on machine learning, pages 201–210. PMLR, 2016.
241"
REFERENCES,0.8877551020408163,"[2] Hervé Chabanne, Amaury De Wargny, Jonathan Milgram, Constance Morel, and Emmanuel
242"
REFERENCES,0.8903061224489796,"Prouff. Privacy-preserving classiﬁcation on deep neural network. Cryptology ePrint Archive,
243"
REFERENCES,0.8928571428571429,"2017.
244"
REFERENCES,0.8954081632653061,"[3] Xiaoqian Jiang, Miran Kim, Kristin Lauter, and Yongsoo Song. Secure outsourced matrix
245"
REFERENCES,0.8979591836734694,"computation and application to neural networks. In Proceedings of the 2018 ACM SIGSAC
246"
REFERENCES,0.9005102040816326,"Conference on Computer and Communications Security, pages 1209–1222, 2018.
247"
REFERENCES,0.9030612244897959,"[4] John Chiang. A novel matrix-encoding method for privacy-preserving neural networks (infer-
248"
REFERENCES,0.9056122448979592,"ence). arXiv preprint arXiv:2201.12577, 2022.
249"
REFERENCES,0.9081632653061225,"[5] Florian Bourse, Michele Minelli, Matthias Minihold, and Pascal Paillier. Fast homomorphic
250"
REFERENCES,0.9107142857142857,"evaluation of deep discretized neural networks. In Advances in Cryptology–CRYPTO 2018:
251"
REFERENCES,0.9132653061224489,"38th Annual International Cryptology Conference, Santa Barbara, CA, USA, August 19–23,
252"
REFERENCES,0.9158163265306123,"2018, Proceedings, Part III 38, pages 483–512. Springer, 2018.
253"
REFERENCES,0.9183673469387755,"[6] Andrey Kim, Yongsoo Song, Miran Kim, Keewoo Lee, and Jung Hee Cheon. Logistic regression
254"
REFERENCES,0.9209183673469388,"model training based on the approximate homomorphic encryption. BMC medical genomics,
255"
REFERENCES,0.923469387755102,"11(4):83, 2018.
256"
REFERENCES,0.9260204081632653,"[7] Charlotte Bonte and Frederik Vercauteren. Privacy-preserving logistic regression training. BMC
257"
REFERENCES,0.9285714285714286,"medical genomics, 11(4):86, 2018.
258"
REFERENCES,0.9311224489795918,"[8] Miran Kim, Yongsoo Song, Shuang Wang, Yuhou Xia, and Xiaoqian Jiang. Secure logistic
259"
REFERENCES,0.9336734693877551,"regression based on homomorphic encryption: Design and evaluation. JMIR medical informatics,
260"
REFERENCES,0.9362244897959183,"6(2):e19, 2018.
261"
REFERENCES,0.9387755102040817,"[9] John Chiang. Privacy-preserving logistic regression training with a faster gradient variant. arXiv
262"
REFERENCES,0.9413265306122449,"preprint arXiv:2201.10838, 2022.
263"
REFERENCES,0.9438775510204082,"[10] Craig Gentry. Fully homomorphic encryption using ideal lattices. In Proceedings of the
264"
REFERENCES,0.9464285714285714,"forty-ﬁrst annual ACM symposium on Theory of computing, pages 169–178, 2009.
265"
REFERENCES,0.9489795918367347,"[11] Zvika Brakerski, Craig Gentry, and Vinod Vaikuntanathan. (leveled) fully homomorphic
266"
REFERENCES,0.951530612244898,"encryption without bootstrapping. ACM Transactions on Computation Theory (TOCT), 6(3):1–
267"
REFERENCES,0.9540816326530612,"36, 2014.
268"
REFERENCES,0.9566326530612245,"[12] N.P. Smart and F. Vercauteren. Fully homomorphic simd operations. Cryptology ePrint Archive,
269"
REFERENCES,0.9591836734693877,"Report 2011/133, 2011. https://ia.cr/2011/133.
270"
REFERENCES,0.9617346938775511,"[13] Jung Hee Cheon, Andrey Kim, Miran Kim, and Yongsoo Song. Homomorphic encryption for
271"
REFERENCES,0.9642857142857143,"arithmetic of approximate numbers. In International Conference on the Theory and Application
272"
REFERENCES,0.9668367346938775,"of Cryptology and Information Security, pages 409–437. Springer, 2017.
273"
REFERENCES,0.9693877551020408,"[14] Kyoohyung Han, Seungwan Hong, Jung Hee Cheon, and Daejun Park. Logistic regression on
274"
REFERENCES,0.9719387755102041,"homomorphic encrypted data at scale. In Proceedings of the AAAI Conference on Artiﬁcial
275"
REFERENCES,0.9744897959183674,"Intelligence, volume 33, pages 9466–9471, 2019.
276"
REFERENCES,0.9770408163265306,"[15] John Chiang. Multinomial logistic regression algorithms via quadratic gradient, 2023.
277"
REFERENCES,0.9795918367346939,"[16] John Chiang. Quadratic gradient: Uniting gradient algorithm and newton method as one. arXiv
278"
REFERENCES,0.9821428571428571,"preprint arXiv:2209.03282, 2022.
279"
REFERENCES,0.9846938775510204,"[17] Dankmar Böhning and Bruce G Lindsay. Monotonicity of quadratic-approximation algorithms.
280"
REFERENCES,0.9872448979591837,"Annals of the Institute of Statistical Mathematics, 40(4):641–663, 1988.
281"
REFERENCES,0.9897959183673469,"[18] Dankmar Böhning. Multinomial logistic regression algorithm. Annals of the institute of
282"
REFERENCES,0.9923469387755102,"Statistical Mathematics, 44(1):197–200, 1992.
283"
REFERENCES,0.9948979591836735,"[19] John Chiang.
On polynomial approximation of activation function.
arXiv preprint
284"
REFERENCES,0.9974489795918368,"arXiv:2202.00004, 2022.
285"
