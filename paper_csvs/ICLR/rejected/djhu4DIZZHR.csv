Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002277904328018223,"Logical reasoning over natural text is an important capability towards human level
intelligence. Existing datasets are either limited and inadequate to train and evaluate
logical reasoning capability (e.g., LogiQA and ReClor), or not oriented for logical
reasoning (e.g., SQuAD and HotpotQA). In this paper, we focus on a speciﬁc
category of logical reasoning, named na¨ıve logical reasoning, and propose a new
large-scale benchmark, named NAIL, aiming to help models learn and evaluate
na¨ıve logical reasoning capability. NAIL is sourced from standardized exams such
as Chinese National Civil Servants Examination and Law School Admission Test.
Furthermore, to collect more data, we propose to imitate examples of standardized
exams rather than designing them from scratch. NAIL is available in both Chinese
and English containing a total of 10, 296 ∗2 instances. Empirical results show
that state-of-the-art neural models struggle on NAIL with very poor accuracy (the
best result is 30.1% for NAIL and 36.2% for Chinese NAIL), while human experts
can perform 100% accuracy. Further results indicate that human imitations can
signiﬁcantly help models learn na¨ıve logical reasoning ability."
INTRODUCTION,0.004555808656036446,"1
INTRODUCTION"
INTRODUCTION,0.00683371298405467,"Current deep models have achieved near human-level performance on many tasks in NLP (Devlin
et al., 2019; Liu et al., 2019), and more often than not, superﬁcial knowledge sufﬁces to solve the
problems. To move towards human intelligence, we need to equip the models with logical reasoning
capabilities (e.g., ability to draw logical conclusions from given statements), which is also a long
sought-after goal of AI (Newell & Simon, 1956; McCarthy et al., 1960). One related task is natural
language inference whose goal is to assign the logical relationships (contradicted, neutral
and entailment) to sentence pairs. To push the development of models in logical reasoning, the
researchers have focused on more challenging reading comprehension tasks, which often require
more complex reasoning as well as longer input. However, most existing reading comprehension
datasets are not oriented for the logical reasoning (e.g., SQuAD and HotpotQA), with the exception
of LogiQA (Liu et al., 2020), ReClor (Yu et al., 2020) and LR-LSAT (Wang et al., 2021)."
INTRODUCTION,0.009111617312072893,"Above datasets (LogiQA, ReClor and LR-LSAT) are limited and inadequate to train and evaluate
logical reasoning capability. The reason is that all of the three datasets involve diverse types of logical
reasoning, such as drawing an alternate conclusion to the argument, ﬁnding necessary/sufﬁcient
assumptions, whether statements strengthen/weaken the argument or explain/resolve the situation.
Mixing multiple types of logical reasoning may pose the following challenges. a). From the perspec-
tive of human cognition, different types of logical reasoning correspond to different problem-solving
ideas. But in practice we usually train a model on the whole dataset with the same idea, which makes
it more limited for models to learn different logical reasoning capability. b). From the perspective
of machine learning, if there are many reasoning types mixed in a dataset, then there will be less
data for each reasoning type, which is inadequate to train and evaluate logical reasoning capability
(demonstrated in our experiments). Furthermore, when the model does not work, it is difﬁcult to
determine which reasoning type is the bottleneck (no reasoning type annotation in the dataset), which
may hinder the design of better models."
INTRODUCTION,0.011389521640091117,"To tackle the challenges, we focus on a more ﬁne-grained type of logical reasoning, named na¨ıve
logical reasoning (Section 2), which is to infer the logical conclusion from statements that describe
triples (subject, predicate, object) and the relationships among them. A typical example of na¨ıve"
INTRODUCTION,0.01366742596810934,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.015945330296127564,"Paragraph: A，B，C，D为四位漂亮女生。她们喜欢穿漂亮衣服。某天，她们穿的衣服颜色各不相同，有黄色，绿色，蓝色
和红色四种，在问到她们各自衣服的颜色时，A说：“B的衣服不是黄色的。”B说：“C的衣服不是绿色的。”C说：“D的衣服不
是蓝色的。”D说：“A，B，C三人中有一个人的衣服是绿色的，而且只有这个人说的是实话。” (There are four pretty girls, 
namely A, B, C, and D, all of whom love to dress beautifully. One day, four girls get dressed in different colors: yellow, green, blue, and
red. When asked about the color, A said that B did not dress in yellow; B said that C did not dress in green; C said that D did not dress 
in blue; D said that among A, B, and C, only one girl dressed in green and she was the only one who told the truth.)"
INTRODUCTION,0.018223234624145785,Color of clothes
INTRODUCTION,0.02050113895216401,"Y
G
B
R"
INTRODUCTION,0.022779043280182234,"A
-
-
-
-"
INTRODUCTION,0.025056947608200455,"B
-
-
-
-
C
-
-
-
-"
INTRODUCTION,0.02733485193621868,"D
-
0
-
- S P O"
INTRODUCTION,0.029612756264236904,Color of clothes
INTRODUCTION,0.03189066059225513,"Y
G
B
R
A
0"
INTRODUCTION,0.03416856492027335,"B
0/1
1
0
0"
INTRODUCTION,0.03644646924829157,"C
0
D
0 S P O"
INTRODUCTION,0.0387243735763098,A told the truth
INTRODUCTION,0.04100227790432802,B told the truth
INTRODUCTION,0.04328018223234624,C told the truth
INTRODUCTION,0.04555808656036447,Color of clothes
INTRODUCTION,0.04783599088838269,"Y
G
B
R
A
0
1
0
0 B
0
0"
INTRODUCTION,0.05011389521640091,"C
0/1
D
0 S P O"
INTRODUCTION,0.05239179954441914,Color of clothes
INTRODUCTION,0.05466970387243736,"Y
G
B
R"
INTRODUCTION,0.05694760820045558,"A
0
0
1
0
B
1
0
0
0"
INTRODUCTION,0.05922551252847381,"C
0
1
0
0"
INTRODUCTION,0.06150341685649203,"D
0
0
0
1"
INTRODUCTION,0.06378132118451026,Cghv+HGhSJu/Rl3/o2TNgtPTBwOde7pkTJwp7TjfVmVldW19o7pb23v7O7V9g86Kk4loW0S81j2AqwoZ4K2NdOc9hJcRw2g0mt4XfaJSsVg86mlC/QiPBAsZwdpInu1FWI8J5tlDPqjVnYzA1ombknqUKI1qH15w5ikERWacKxU3US7WdYakY4zW0vVTBZIJHtG+owBFVfjbLnKMTowxRGEvzhEYz9fdGhiOlplFgJouIatErxP+8fqrDaz9jIk1FWR+KEw50jEqCkBDJinRfGoIJpKZrIiMscREm5psU4K7+OVl0jlruJeN8/uLevOmrKMKR3AMp+DCFThDlrQBgIJPMrvFmp9WK9Wx/z0YpV7hzCH1ifP8LSkYU=</latexit>S P
INTRODUCTION,0.06605922551252848,O2iy09cDA4Zx7uWdOmHKmtOt+W5WV1bX1jeqmvbW9s7vn7B+0VZJQlsk4YnshlhRzgRtaY57aS4jktBObwu/80SlYol41JOUBjEeChYxgrWRfNuPsR4RzP7ad+puXV3BrRMvJLUoESz73z5g4RkMRWacKxUz3NTHeRYakY4ndp+pmiKyRgPac9QgWOqgnyWeYpOjDJAUSLNExrN1N8bOY6VmsShmSwiqkWvEP/zepmOroOciTVJD5oSjSCeoKAANmKRE84khmEhmsiIywhITbWqyTQne4peXSfus7l3Wzx8uao2bso4qHMExnIHV9CAO2hCwik8Ayv8GZl1ov1bn3MRytWuXMIf2B9/gC8vpGB</latexit>O
INTRODUCTION,0.0683371298405467,"Q: 如果D说的是实话那么以下说法中正确的是？(If D told the truth, then which of the 
following statements is correct?)
A. C的衣服是绿色的，D的衣服是红色的(C is dressed in green, and D is dressed in red.)
B. A的衣服是绿色的，B的衣服是红色的(A is dressed in green, and B is dressed in red.)
C. B的衣服是蓝色的，C的衣服是红色的(B is dressed in blue, and C is dressed in red.)
D. D的衣服是绿色的，A的衣服是红色的(D is dressed in green, and A is dress in red.)"
INTRODUCTION,0.07061503416856492,"Figure 1: An example of NAIL requiring na¨ıve logical reasoning. Highlighted option A is the correct answer.
Tables depict the reasoning process from the human perspective. - indicates uncertain state. 0 and 1 indicate
false and true respectively. Red grid with “0/1” means there is a conﬂict."
INTRODUCTION,0.07289293849658314,"logical reasoning is shown in Figure 1. To answer this query, we need to iteratively derive conclusions
according to the conditions, and stop the searching branch if a conﬂict occurs. Speciﬁcally, in the
initial table, we only know that “D is not green” from “D told the truth”. Assume that “A told
the truth”, and infer that “A is green, C is non-green”. Next derive “C is green” from “C told the
lie”. Then C is derived as both green and non-green, thus causing a conﬂict. Similar processes for
assuming “B told the truth” and “C told the truth”. It takes extensive training and practice for human
brains to cope with such complex logical reasoning."
INTRODUCTION,0.07517084282460136,"Inspired by the datasets extracted from standardized examinations (Lai et al., 2017; Clark et al., 2018;
Liu et al., 2020), we build a new large-scale benchmark, NAIL, by selecting na¨ıve logical reasoning
examples from standardized exams such as the National Civil Servants Examination of China and
Law School Admission Test. However, such examples are limited in their number, as it takes efforts
for human experts to design these questions. To collect more data, we propose to imitate examples of
standardized exams rather than designing them from scratch. Unlike simple data augmentation (e.g.,
substitution, paraphrasing), human imitation aims at getting more diverse examples while maintaining
the underlying logic (Figure 1). NAIL is available in both Chinese and English, containing a total of
10, 296 ∗2 instances."
INTRODUCTION,0.0774487471526196,"Empirical results show that state-of-the-art neural models struggle on NAIL with very poor accuracy
(the best result is 30.1% for NAIL and 36.2% for Chinese NAIL), while human experts can perform
100% accuracy. Further results indicate that human imitations can signiﬁcantly help models learn
na¨ıve logical ability from natural text."
INTRODUCTION,0.07972665148063782,"2
NA¨IVE LOGICAL REASONING"
INTRODUCTION,0.08200455580865604,"The na¨ıve logical reasoning is a more ﬁne-grained type of logical reasoning. Formally, we give the
deﬁnition as follows.
Deﬁnition 1. The subject is a described resource, usually an entity, such as person or location. The
predicate indicates an attribute of the subject or indicates some kind of relationship between the
subject and the object. When denoting an attribute, the object is the attribute value, usually a literal
value, e.g., (Jacket, Color, Red), otherwise the object is an entity, e.g., (Beijing, Capital, China).
Deﬁnition 2. Assuming the set of subjects S, the set of predicates P, the set of objects O and several
statements, each statement describes a triple (subject, predicate, object) or some logical relationship"
INTRODUCTION,0.08428246013667426,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.08656036446469248,"between triples. The naive logical reasoning is the process of reasoning from these statements to
reach a logical conclusion. 1"
INTRODUCTION,0.0888382687927107,"In this work, we explore na¨ıve logical reasoning in the form of reading comprehension. A typical
example and detailed reasoning process is shown in Figure 1. Similar to the format of multiple-choice
reading comprehension, it contains a context, a query and four options with only one correct answer.
To solve the problem, the model needs to understand the logical connections between the subjects,
predicates and objects, and then derive a valid option."
INTRODUCTION,0.09111617312072894,"3
NAIL: DATA COLLECTION AND ANALYSIS"
INTRODUCTION,0.09339407744874716,"NAIL is a carefully designed benchmark for na¨ıve logical reasoning similar to the format of multiple-
choice reading comprehension. Inspired by the datasets extracted from standardized examinations (Lai
et al., 2017; Clark et al., 2018; Liu et al., 2020), we ﬁrst collect a small amount of examples from
examinations, denoted as NAIL-E (Section 3.1). Then we propose to imitate examples of NAIL-E
to collect more data, denoted as NAIL-I (Section 3.2). Finally we provide a detailed analysis of the
proposed NAIL (Section 3.3)."
COLLECTION FROM EXAMINATION,0.09567198177676538,"3.1
COLLECTION FROM EXAMINATION"
COLLECTION FROM EXAMINATION,0.0979498861047836,"We searched for such examples widely from two different types of public examinations: Chinese
National Civil Servants Examination (CNCSE) and Law School Admission Test (LSAT). 2 CNCSE
is a once-a-year competitive examination in China, and there are overall 120-140 examples per exam
per year. But only 1-4 examples belong to the scope of na¨ıve logical reasoning, which is also the
most difﬁcult type of problems for candidates within the given 120 minutes. And the LSAT is a
standardized test for prospective law school candidates in the United States, Canada, and a growing
number of other countries. Logical Reasoning is a multiple-choice section of LSAT, containing
24-26 questions under the limitation of 35 minutes, where about 2-4 problems fall into na¨ıve logical
reasoning category."
COLLECTION FROM EXAMINATION,0.10022779043280182,"We artiﬁcially selected examples that belong to the na¨ıve logical reasoning category from the above
two examinations. Finally we obtained 488 examples from the last 25 years of CNCSE and LSAT
in the last 30 years, denoted as NAIL-E. These two exams are from countries with different native
languages. The former is expressed in Chinese, and the latter is expressed in English. And note that
there are slight difference in language style between them. And for diversity and fairness, in later
step we get all examples in both Chinese and English through translation."
COLLECTION FROM IMITATION,0.10250569476082004,"3.2
COLLECTION FROM IMITATION"
COLLECTION FROM IMITATION,0.10478359908883828,"After collecting a small amount of NAIL-E, we expect to expand the number of the dataset. Designing
examples from scratch requires a huge effort from human experts. One simple solution is data
augmentation, which artiﬁcially scales up data by creating modiﬁed data from existing data, such as
word/sentence shufﬂing, word replacement and syntactic variation. However, the data augmented
in this way is highly correlated with the original data, and the model easily captures these semantic
surface correlations, which makes it limited to train and evaluate logical reasoning capability. To
alleviate this limitation, we propose to imitate examples of NAIL-E, to create more examples with a
diverse semantic surface while keeping the underlying logic of the original example. Furthermore, in
the process of human imitation, we design strict strategies to control the quality of imitation.
Imitation Example
See Figure 2, the context of an example from NAIL-E consists of 6 sentences
and a query (split by blank lines), each of which can be represented as a logical template (see
“backbone template”). The example focus on the description of a scenario: picking Prince
Charming (m1), which involves ﬁve entities: Li Na (s1), Wang Wei (p1), Wu Gang (p2),
Li Qiang (p3), Liu Dawei (p4), and three noun/adjective properties: tall (a1), handsome
(a2), a PhD (a3). An accepted imitation needs to keep original underlying logic but have seman-
tically different groundings, i.e., to describe a completely different scenario. Imitation 1 and 2"
COLLECTION FROM IMITATION,0.1070615034168565,"1The term naive refers to the fact that this logical reasoning process of human in this task is spontaneous,
intuitive and unsystematic.
2https://www.lsac.org/lsat/taking-lsat/test-format/logical-reasoning"
COLLECTION FROM IMITATION,0.10933940774487472,Under review as a conference paper at ICLR 2022
COLLECTION FROM IMITATION,0.11161731207289294,"𝒔𝟏心中的𝒎𝟏有如下特征：
𝒂𝟏, 𝒂𝟐, and 𝒂𝟑.
(𝒔𝟏’s 𝒎𝟏is supposed to 
be: 𝒂𝟏, 𝒂𝟐, and 𝒂𝟑.)"
COLLECTION FROM IMITATION,0.11389521640091116,"李娜心中的白马王子有如下
特征：高个子、相貌英俊、
博士。
(Li Na's Prince Charming 
is supposed to be: tall, 
handsome, and a PhD.)"
COLLECTION FROM IMITATION,0.11617312072892938,"孙怡心中的白马王子有如下
特征：高个子、相貌英俊、
博士。(Sun Yi's Prince 
Charming is supposed to 
be: tall, handsome, and a 
PhD.)"
COLLECTION FROM IMITATION,0.11845102505694761,"孙怡心中的白马王子有如下
特征：有钱、有房、有车。
(Sun Yi's Prince 
Charming is supposed to 
be: rich, has a house, and 
has a car.)"
COLLECTION FROM IMITATION,0.12072892938496584,"小丽心中的理想礼物有如下
特征：昂贵，有收藏价值，
美观。(Xiao Li’s ideal gift 
is supposed to be: 
expensive, valuable to 
collect, and nice-looking.)"
COLLECTION FROM IMITATION,0.12300683371298406,"她认识𝒑𝟏, 𝒑𝟐, 𝒑𝟑, 𝒑𝟒4（量
词）（𝒑的类别）,其中有一
符合𝒔𝟏所要求的全部条件。
(She knows 4 type_of(𝒑𝒊), 
𝒑𝟏, 𝒑𝟐, 𝒑𝟑, 𝒑𝟒, and only one 
meets all 𝒔𝟏’s
requirements.)"
COLLECTION FROM IMITATION,0.1252847380410023,"她认识王威、吴刚、李强、
刘大伟4位男士,其中有一位
符合她所要求的全部条件。
(She knows 4 men, Wang 
Wei, Wu Gang, Li Qiang
and Liu Dawei, and only 
one meets all her 
requirements.)"
COLLECTION FROM IMITATION,0.1275626423690205,"她认识瞿衡、梅震、季凡、
张磊4位男士,其中有一位符
合她所要求的全部条件。
(She knows 4 men, Qu 
Heng, Mei Zhen, Ji Fan, 
and Zhang Lei, and only 
one meets all her 
requirements.)"
COLLECTION FROM IMITATION,0.12984054669703873,"她认识瞿衡、梅震、季凡、
张磊4位男士,其中有一位符
合她所要求的全部条件。
(She knows 4 men, Qu 
Heng, Mei Zhen, Ji Fan, 
and Zhang Lei, and only 
one meets all her 
requirements.)"
COLLECTION FROM IMITATION,0.13211845102505695,"她知道手链、手表、手镯、
戒指这4个礼物,其中有一个
符合她所要求的全部条件。
(She knows 4 gifts, 
bracelets, watches, 
bangles, and rings, and 
only one meets all her 
requirements.)"
COLLECTION FROM IMITATION,0.13439635535307518,"4（量词）（𝒑的类别）中,有
3（量词）是𝒂𝟏,有2（量词）
是𝒂𝟑, 有1（量词）是𝒂𝟐。
(Among the 4 type_of(𝒑𝒊), 
𝟑are 𝒂𝟏, 𝟐are 𝒂𝟑, and 𝟏is 
𝒂𝟐.)"
COLLECTION FROM IMITATION,0.1366742596810934,"(1)4位男士中,有3个高个
子,2名博士,1人长相英俊。
(Among the 4 men, 3 are 
tall, 2 are PhDs and 1 is 
handsome.)"
COLLECTION FROM IMITATION,0.13895216400911162,"(1) 4位男士中,有3个高个
子,2名博士,1人长相英俊。
(Among the 4 men, 3 are 
tall, 2 are PhDs and 1 is 
handsome.)"
COLLECTION FROM IMITATION,0.14123006833712984,"(1) 4位男士中,有3个有钱,2
人有车,1人有房。(Among 
the 4 men, 3 are rich, 2 
have a car and 1  has a 
house.)"
COLLECTION FROM IMITATION,0.14350797266514806,"(1) 4个礼物中,有3个很昂
贵,2个很美观,1个具有收藏
价值；(Among the 4 gifts, 
3 are expensive, 2 are 
nice-looking and 1 is 
valuable to collect;)"
COLLECTION FROM IMITATION,0.14578587699316628,"𝒑𝟏和𝒑𝟐都是𝒂𝟑。
( 𝒑𝟏and 𝒑𝟐are both 𝒂𝟑.)"
COLLECTION FROM IMITATION,0.1480637813211845,"(2)王威和吴刚都是博士。
(Wang Wei and Wu Gang 
are both PhDs.)"
COLLECTION FROM IMITATION,0.15034168564920272,"(2) 瞿衡和梅震都是博士。
(Qu Heng and Mei Zhen 
are both PhDs.)"
COLLECTION FROM IMITATION,0.15261958997722094,"(2) 瞿衡和梅震都有车。(Qu 
Heng and Mei Zhen both 
have a car.)"
COLLECTION FROM IMITATION,0.1548974943052392,"(2) 手链和手表都很美。
(Bracelets and watches 
are both nice-looking.)"
COLLECTION FROM IMITATION,0.1571753986332574,"𝒑𝟒和𝒑𝟑关于𝒂𝟏的属性相同。
(𝒑𝟒and 𝒑𝟑are of the same 
attribute_of 𝒂𝟏.)"
COLLECTION FROM IMITATION,0.15945330296127563,"(3)刘大伟和李强身高相同。
(Liu Dawei and Li Qiang
are of the same height.)"
COLLECTION FROM IMITATION,0.16173120728929385,"(3) 张磊和季凡身高相同。
(Zhang Lei and Ji Fan are 
of the same height.)"
COLLECTION FROM IMITATION,0.16400911161731208,"(3) 张磊和季凡有同等程度
的钱。(Zhang Lei and Ji 
Fan are of the same 
wealth.)"
COLLECTION FROM IMITATION,0.1662870159453303,"(3) 戒指和手镯价格相同。
(Rings and bangles are of 
the same price.)"
COLLECTION FROM IMITATION,0.16856492027334852,"𝒑𝟑和𝒑𝟏并非都是𝒂𝟏。
(Either 𝒑𝟑or 𝒑𝟏𝐢𝐬𝒂𝟏.)"
COLLECTION FROM IMITATION,0.17084282460136674,"(4)李强和王威并非都是高个
子。(Either Li Qiang or 
Wang Wei is tall.)"
COLLECTION FROM IMITATION,0.17312072892938496,"(4) 季凡和瞿衡并非都是高
个子。(Either Ji Fan or Qu 
Heng is tall.)"
COLLECTION FROM IMITATION,0.17539863325740318,"(4) 季凡和瞿衡并非都很富
有。(Either Ji Fan or Qu 
Heng is rich.)"
COLLECTION FROM IMITATION,0.1776765375854214,"(4) 手镯和手链并非都是昂
贵的。(Either Bangles or 
bracelets is expensive.)"
COLLECTION FROM IMITATION,0.17995444191343962,"请问谁符合𝒔𝟏要求的全部条
件？
(Who meets all the 
requirements of 𝒔𝟏?)"
COLLECTION FROM IMITATION,0.18223234624145787,"请问谁符合李娜要求的全部
条件?
(Who meets all the 
requirements of Li Na?)"
COLLECTION FROM IMITATION,0.1845102505694761,"请问谁符合孙怡要求的全部
条件?
(Who meets all the 
requirements of Sun Yi?)"
COLLECTION FROM IMITATION,0.1867881548974943,"请问谁符合孙怡要求的全部
条件?
(Who meets all the 
requirements of Sun Yi?)"
COLLECTION FROM IMITATION,0.18906605922551253,"请问哪个符合小丽要求的全
部条件?
(Which meets all the 
requirements of Xiao Li?)"
COLLECTION FROM IMITATION,0.19134396355353075,"A. 𝒑𝟒
B. 𝒑𝟑
C. 𝒑𝟐
D. 𝒑𝟏"
COLLECTION FROM IMITATION,0.19362186788154898,"A.刘大伟(Liu Dawei)
B.李强(Li Qiang)
C.吴刚(Wu Gang)
D.王威(Wang Wei)"
COLLECTION FROM IMITATION,0.1958997722095672,"A.张磊(Zhang Lei)
B.季凡(Ji Fan)
C.梅震(Mei Zhen)
D.瞿衡(Qu Heng)"
COLLECTION FROM IMITATION,0.19817767653758542,"A.张磊(Zhang Lei)
B.季凡(Ji Fan)
C.梅震(Mei Zhen)
D.瞿衡(Qu Heng)"
COLLECTION FROM IMITATION,0.20045558086560364,"A.戒指(Rings)
B.手镯(Bangles)
C.手表(Watches)
D.手链(Bracelets)"
COLLECTION FROM IMITATION,0.20273348519362186,"Original Example
Imitation 1
Imitation 2
Imitation 3
Backbone Template"
COLLECTION FROM IMITATION,0.20501138952164008,"Figure 2: An original example from CNCSE and its three imitations, which share the same backbone template.
Imitation 1 is subject-level. Imitation 2 is subject-and-object-level. And Imitation 3 is subject-and-predicate-and-
object-level."
COLLECTION FROM IMITATION,0.2072892938496583,"are unqualiﬁed imitations. Since Imitation 1 only conducts subject-level imitations (S imitations),
i.e., only superﬁcially substitute the ﬁve entities (s1:Li Na→Sun Yi, p1:Wang Wei→Qu Heng,
p2:Wu Gang→Mei Zhen, p3:Li Qiang→Ji Fan, p4:Liu Dawei→Zhang Lei). Further, Im-
itation 2 conducts subject-and-object-level imitations (SO imitations), i.e., not only substituting
the entities, but also altering the corresponding objects (property values).
(a1:tall→rich,
a2:handsome→having a house, a3:a PhD→having a car). The difference between Imi-
tation 2 and the original example is much greater than that between Imitation 1 and and the raw
problem, however, Imitation 2 is still not an ideal imitation. Furthermore, Imitation 3 changes the
scenario m1 into picking ideal gift. Imitation 3 is an expected imitative writing, which
conducts subject-and-predicate-and-object-level imitations (SPO imitations). Note that all of the
three imitations share the same logic templates. And p2 is the answer for all of the original example
and above three imitations."
COLLECTION FROM IMITATION,0.20956719817767655,"Imitation Process
We ﬁrst select a group of people from a variety of occupations: professional
editors, legal practitioners, in-service civil servants and college students (from different majors).
Empirically, people in these occupations have strong logical and verbal skills. These people are
asked to conduct SPO imitations based on original examples from NAIL-E, and reasonable imitations
should meet two requirements: logic invariance and semantic diversity. We trained these candidate
people how to conduct SPO imitations such as Figure 2. We then conducted a trial phase before the
ofﬁcial imitation phase, in which process we eliminated some people of poor quality. Finally we
employed 82 qualiﬁed people3 to imitatively construct problems based on NAIL-E, and they are paid
RMB¥2.8 per imitation4. Averagely, it costs a trained person about 3-4 minutes to ﬁnish an imitation:
starting from coming up a scenario, then replacing the subjects, predicates, and objects of the raw"
COLLECTION FROM IMITATION,0.21184510250569477,"3Consisting of 42 native Chinese speakers and 40 native English speakers. Native speakers of a language
will be assigned imitation tasks expressed by that language.
4A part-time employee can produce 15-20 imitations per hour, where he/she can get RMB¥42-RMB¥56,
while the local minimum wage is RMB¥23 per hour."
COLLECTION FROM IMITATION,0.214123006833713,Under review as a conference paper at ICLR 2022
COLLECTION FROM IMITATION,0.2164009111617312,"problem with those scenario-related while keeping invariant logic, and ﬁnally smoothing the new
sentence with some transition words if necessary. For each original example in NAIL-E, we expect at
least 20 imitations (except for extremely difﬁcult cases). Overall we use 813 paid work hours in total
to build the NAIL-I."
COLLECTION FROM IMITATION,0.21867881548974943,"Imitation Quality Control
We adopt following strategies to ensure the quality of imitations:
1. As mentioned above, we conducted a trial phase before the ofﬁcial imitation phase. In this phase,
we asked them to imitate a small number of problems. Although we do not necessarily need them
to write the backbone template, we will check the logic and give feedback to help them understand
the task. This process was iterated for three rounds. Only those who passed the trial phase could
participate in the ofﬁcial imitation.
2. During the ofﬁcial imitation, we set up an online chat room to communicate with employees and
answer their questions timely.
3. To embrace semantic diversity, each original example is shown to at least 5 people, that is, one
can only conduct 4 imitations based on one original example. People who are assigned to the same
example imitate independently without interference from each other, to ensure varied inspiration for
imitation.
4. To ensure logic invariance, we adopt a double-checking strategy:"
COLLECTION FROM IMITATION,0.22095671981776766,"• Cross Checking: Everyday, for each employee, we sample 5 imitations from all of his/her daily
imitations. And the sampled imitation is assigned to other 2 employees for cross-checking.
The imitation will be qualiﬁed only if they both approved, and the criterion for approval is that
the imitation share the same logics with the original example. If any one of the 5 imitations
produced by one employee fail, then all imitations of that employee for that day will be returned
to re-check and repair.
• Post Checking: To further ensure that the underlying logic do not deviate during imitation,
we introduce another team of experts to solve the imitative examples. The team consists of 20
experienced experts. 10 of them speaks Chinese as native language and have passed CNCSE,
while the other 10 speaks English and have passed LSAT. 5 Each imitative example was presented
to 3 experts randomly, who are allowed to select one and only choice from “A”, “B”, “C”, “D”,
otherwise, “UNABLE TO ANSWER” if bugs exist in the example, causing no correct choice or
multiple correct choices. As long as one of the 3 experts pointed out “UNABLE TO ANSWER”,
then the imitator of this problem should recheck the logic, until each of these 3 experts could
give a choice from “A”, “B”, “C”, “D”. Note that in the post checking process, we broke up the
imitations together and shufﬂe randomly, otherwise if a person is faced with imitations from
same original example, he/she is prone to give a shortcut option with speculation."
COLLECTION FROM IMITATION,0.22323462414578588,"Translation Quality Control
After collecting high-quality mono-collections, we ﬁrst adopted
Google Translation to translate Chinese/English collections into another language, and then employed
10 professional bilingual experts in Chinese and English for manual correction. Bilingual experts
were asked to pay attention to logic-invariance and faithfulness during translation. Next, to ensure
translation quality, we also adopted the post checking strategy. That is, we asked the 20 human
experts mentioned above to solve the translated examples. Each translated example was presented
to 3 experts randomly. Since human experts excel in solving na¨ıve logical reasoning problems, (i.e.
achieve 100% accuracy on mono-collections), if any expert made a mistake on a translated sample
or pointed out “UNABLE TO ANSWER”, the translated instance is sent back to the bilingual experts
for revision. Finally, we asked 50 native speakers to read through all paragraphs of the translation
parts in NAIL and mark “0”/“1” for each, where “1” stands for a translated sample is idiomatic, and
“0” otherwise. Then for all samples marked with “0” (about 20%), the bilingual experts and native
speakers will work together to polish them and conform to the target language norms."
COLLECTION FROM IMITATION,0.2255125284738041,"Human Evaluation
As mentioned above, an imitation is ﬁnally regarded as qualiﬁed only if the
sampled 3 experts could all solve the example. For any original example in NAIL-E, we also ask
3 experts in the team to solve it. Since the gold answers to examples in NAIL-E are provided in
public by the examination committee, and corresponding imitations in NAIL-I share the same answer
with the original example. Therefore, we calculated the mean accuracy of these three submissions
on the overall NAIL, denoted as the performance of human experts. To better demonstrate the"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.22779043280182232,"5Experts who are native English speakers will be assigned English problems and experts who are native
Chinese speakers will be assigned Chinese problems."
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.23006833712984054,Under review as a conference paper at ICLR 2022
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.23234624145785876,"Paragraph: 某省举行“文明城市”评比。4位评委对大家普遍看好的A、B、C等3城市获得“文明城市”称号的可能性进行了分析
预测。评委甲说：“要么A市能获得，要么C市能获得。”评委乙说：“如果A市与C市能获得，则B市也能获得。”评委丙说：
“只有当B市不能获得或者C市能获得时，A市才不能获得。”评委丁说：“我看B市能获得的可能性为零，而A市与C市一定能
获得。”评比结束后发现，4位评委中只有一人预测成立。(Three cities, namely City A, B, and C, took part in a competition to
win the title of ""a civilized city"", and four judges were making their predictions. Judge 1 said that either A or C would win.
Judge 2 said that if A and C had won, B would win too. Judge 3 said that only if B could not win or C won, A would not win.
Judge 4 said that it was impossible for B to win, and A & C was sure to win. After the results came out, only one judge
predicted correctly.)
Q: 据此可以推出能获得“文明城市”称号的城市是? (Which city won the title?)
A. A市(City A)
B. B市(City B)
C. C市(City C)
D. A市，B市和C市(City A,B and C)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.23462414578587698,"Real/Fake
Mixture
(36.03%)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.23690205011389523,"Paragraph + Query + Answers
Reasoning Type"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.23917995444191345,"Paragraph: 甲、乙和丙，一位是山东人，一位是河南人，一位是湖北人。现在只知道：丙比湖北人年龄大，甲和河南人不同岁，
河南人比乙年龄小。(A, B, and C come from different places: Shandong province, Henan province, and Hubei province.
Now the following information is given: C is older than the one who comes from Hubei; A and the one who comes from Henan
are different in their age; the one who comes from Henan is younger than B.)
Q: 由此可以推知: (Which of the following statements can be inferred from the passage?)
A. 甲不是湖北人(A is not from Hubei.)
B. 河南人比甲年龄小(The one from Henan is younger than A.)
C. 河南人比山东人年龄大(The one from Henan is older than that from Shandong.)
D. 湖北人年龄最小(The one from Hubei is the youngest.)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.24145785876993167,"Ordering
(20.40%)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.2437357630979499,"Paragraph: 甲、乙、丙在北京、南京和成都工作，他们的职业是医生、演员和教师。已知：甲不在北京工作，乙不在南京工作；
在北京工作的不是教师；在南京工作的是医生；乙不是演员。(A, B, and C have different jobs (Beijing, Nanjing, and Chengdu)
in different cities (a doctor, an actor, and a teacher), and now the following information is given. A does not work in Beijing, B
does not work in Nanjing; The one who works in Beijing is not a teacher; The one who works in Nanjing is a doctor; B is not an
actor.)
Q: 那么,甲,乙,丙分别在哪里工作? (So, where do A, B and C work respectively?)
A. 南京,成都和北京(Nanjing, Chengdu and Beijing)
B. 成都,北京和南京(Chengdu, Beijing and Nanjing)
C. 南京,北京和成都(Nanjing, Beijing and Chengdu)
D. 成都,南京和北京(Chengdu, Nanjing and Beijing)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.2460136674259681,"Matching
(29.26)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.24829157175398633,"Paragraph: S市一所小学的学生户籍情况比较复杂，所有三年级学生的户籍都在本市，有些二年级学生的户籍也在本市，有些
一年级学生是农民工子弟，而农民工子弟的户籍都不在本市。(An elementary school in City S has complicated Hukou (a
system of household registration in China) issues. All students in grade 3 are registered in the City S. Some of the students in
grade 2 are registered in the City S. The parents of some students in grade 1 are migrant workers, who are not registered in
the city S. )
Q: 据此，可以推出：(Which of the following statements can be inferred from the material?)
A. 所有二年级学生都不是农民工子弟(all students in grade 2 are not children of migrant workers)
B. 有些农民工子弟是三年级学生(some migrant workers' children are in grade 3)
C. 有些户籍在本市的学生是三年级学生(some students registered in this city are in grade 3)
D. 有些一年级学生不是农民工子弟(some students in grade 1 are not children of migrant workers)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.2505694760820046,"Set
Operation
(14.31%)"
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.2528473804100228,Figure 3: The percentage and representative examples of each na¨ıve logical reasoning type.
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.255125284738041,"difﬁculty of NAIL, we also selected another team consisting of 20 ﬁrst-year college students. Same as
above, an example is shown to 3 students randomly and they have to give the answer independently.
We calculated the overall mean accuracy as the performance of human baseline.The evaluation of
human baseline is paid separately. Each person can receive RMB¥1.5 for answering each example,
which generally cost the person 2-3 minutes."
EXPERTS WHO ARE NATIVE ENGLISH SPEAKERS WILL BE ASSIGNED ENGLISH PROBLEMS AND EXPERTS WHO ARE NATIVE,0.25740318906605925,"Since we hire part-time people to write imitatively rather than using existing crowd-sourcing platforms,
we build our own website, where the quality and overall progress can be viewed at any time."
DATA ANALYSIS,0.25968109339407747,"3.3
DATA ANALYSIS"
DATA ANALYSIS,0.2619589977220957,"As mentioned above, NAIL-E and NAIL-I together compose NAIL. NAIL-E, NAIL-I and NAIL are
divided into training set, validation set and test set respectively. The overall statistics of NAIL are
summarized in Table 1. It is worth noting that an original example in NAIL-E and its corresponding
imitations will only be in the same data split. We analyze and manually annotate the ﬁne-grained
types of examples in the NAIL-E and group them into 4 categories including real/fake mixture,
ordering, matching and set operation, whose percentages and representative examples are shown in
Figure 3. Each of these types of examples requires na¨ıve logical reasoning."
DATA ANALYSIS,0.2642369020501139,"Train(NAIL)
Dev(NAIL)
Test(NAIL)
NAIL-E
NAIL-I
NAIL-E
NAIL-I
NAIL-E
NAIL-I"
DATA ANALYSIS,0.26651480637813213,"# Example
292
5906
97
1904
99
1998
# Ave./Max. of paragraph
68.7 / 155
70.2 / 188
67.3 / 151
70.3 / 212
67.0 / 149
73.3 / 185
# Ave./Max. of query
8.1 / 74
6.9 / 81
9.3 / 71
9.3 / 134
7.9 / 92
7.6 / 71
# Ave./Max. of option
9.2 / 74
9.7 / 130
9.1 / 65
8.5 / 134
8.6 / 76
7.7 / 97"
DATA ANALYSIS,0.26879271070615035,Table 1: Data split and corresponding statistics.
DATA ANALYSIS,0.27107061503416857,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2733485193621868,"4
EXPERIMENTS"
EXPERIMENTS,0.275626423690205,"We adopt the accuracy as the evaluation metric. Simple rule-based methods and strong neural-
based methods are included as our baseline. Rule-based methods involve text matching and sliding
window. Neural-based methods include BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019).
Detailed descriptions (e.g., hyper-parameters) of the baseline models and the results on the validation
set are listed in the Appendix A. 6"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.27790432801822323,"4.1
RESULTS AND ANALYSIS
Model
Input
NAIL
Chinese NAIL"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.28018223234624146,"Dev
Test
Dev
Test"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.2824601366742597,"Random
(C, Q, A)
25.0
25.0
25.0
25.0"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.2847380410022779,"Word Matching
(Q, A)
23.1
24.6
23.3
24.9
(C, Q, A)
21.6
25.8
22.2
24.2"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.2870159453302961,"Sliding Window
(Q, A)
23.8
24.2
24.1
24.7
(C, Q, A)
21.9
22.1
21.6
22.5"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.28929384965831434,"BERT LARGE
(A)
26.8
26.7
27.2
26.7
(Q, A)
27.3
27.1
27.4
26.8
(C, Q, A)
29.0
29.4
29.3
27.7"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.29157175398633256,"RoBERTa LARGE
(A)
27.3
26.5
29.4
27.7
(Q, A)
27.8
27.9
32.6
30.5
(C, Q, A)
34.6
30.1
37.3
36.2"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.2938496583143508,"Human baseline
(C, Q, A)
70.1
71.3
75.5
76.4
Human expert
(C, Q, A)
100.0
100.0
100.0
100.0"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.296127562642369,"Table 2: Main results on NAIL (accuracy%). The column Input
means whether to input context (C), query (Q) and answer options
(A)."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.2984054669703872,"The performance of all baselines on
the NAIL is presented in Table 2. In
particular, the human baseline is 71.3
and 76.4 on the test set of NAIL and
Chinese NAIL separately, while the
human expert is 100.0 since ambigu-
ous examples are not included in the
dataset. In comparison, all of the
baselines perform much worse than
humans, demonstrating that these
methods are very limited in na¨ıve log-
ical reasoning. In addition, the results
are relatively similar on the English
and Chinese datasets. The rule-based
approaches achieve an accuracy of
25.8 and 24.2, which is similar to
random guess, indicating that word
correlation can not be used to help im-
prove performance. Pre-trained mod-
els have relatively good performance with about 4-10 points improvement compared to the rule-based
approaches, showing that pre-trained models have a certain degree of commonsense and logical
reasoning capabilities (Huang et al., 2019b). However, the best result by RoBERTa LARGE is 36.2 on
the testing data of Chinese NAIL, which is still far below the human performance. This indicates that
the knowledge in the pre-trained model is quite weak in logical reasoning."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.30068337129840544,"Different Input Settings
We conduct experiments with different input settings. The setting of
questions and answer options (Q, A) does not lead to signiﬁcant improvements compared to the
input setting of only answer options (A). One likely reason is that the queries usually do not provide
much information, e.g., According to this, it can be deduced that? Further adding context yields a
noticeable boost, showing that the context is highly informative."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.30296127562642367,"Transfer Learning
We conduct a set of transfer learning experiments to understand the degree of
overlap in terms of necessary knowledge for solving problems in our dataset and existing datasets."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3052391799544419,"What are the results if pre-trained model is ﬁrst trained on existing reading comprehension datasets,
and then ﬁne-tuned on NAIL? Table 3 shows the results where LogiQA, ReClor and RACE are
adopted. 7 Overall, we observe that when LogiQA, ReClor or RACE is regarded as extra training
resource for RoBERTa, the performance on NAIL will increase (30.1→34.5/33.4/31.0), since models
can learn some degree of general reasoning ability when learning other comprehension tasks. While
interestingly, if models are trained only on LogiQA, ReClor, or RACE, and then zero-shot to directly
test on NAIL, the performance is poor and close to random guess. And as for RACE, the zero-shot
performance on NAIL (22.0) is even far below than that of RoBERTa (23.4). We attribute this to the"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.30751708428246016,"6We also try seq2seq-based models (i.e. T5 (Raffel et al., 2019)) and well-designed models for other related
tasks (i.e., DAGN (Huang et al., 2021) for LogiQA and ReClor, HGN (Fang et al., 2020) for HotpotQA (Yang
et al., 2018), and QDGAT (Chen et al., 2020) for DROP (Dua et al., 2019)) on our English version of NAIL.
Details and results are shown in Appendix B.
7For fair comparison, in all cross-benchmarks experiments in this paper, we removed samples in the training
set that are duplicates of those in the test set."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3097949886104784,Under review as a conference paper at ICLR 2022
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3120728929384966,(zero-shot)
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3143507972665148,"NAIL
(zero-shot)"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.31662870159453305,"*
(finetune)"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.31890660592255127,"*
NAIL
(finetune)
Training data 20 30 40 50 60 70 80"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3211845102505695,Accuracy
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3234624145785877,Zero-shot/Finetuned Results
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.32574031890660593,"LogiQA
ReClor
RACE"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.32801822323462415,"Figure 4: Transfer learning results when evaluating
on the test split of LogiQA, ReClor and RACE. ∅
is the zero-shot performance of RoBERTa. * is a
placeholder for these three datasets."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.33029612756264237,"Evaluate on →
NAIL
NAIL-E
NAIL-I
Train on ↓"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3325740318906606,"∅
23.4
20.8
23.5
LogiQA
25.4
39.6
24.7
ReClor
24.7
19.8
25.0
RACE
22.0
29.2
21.6"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3348519362186788,"NAIL
30.1
37.4
29.7
LogiQA→NAIL
34.5
38.5
34.3
ReClor→NAIL
33.6
33.3
33.4
RACE→NAIL
31.0
32.3
31.0"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.33712984054669703,"Table 3: Transfer learning results when evaluating
on the test split of NAIL, NAIL-E, NAIL-I. RACE
→NAIL denotes ﬁnetuned on RACE ﬁrst and then
ﬁnetuned on NAIL."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.33940774487471526,"fact that, na¨ıve logical reasoning is a typical category of reasoning which should be signiﬁcantly
distinguished from RACE. Fine-tuning on RACE makes the parameters ﬁt the RACE data, which can
lead to side effects on NAIL. We believe that completely different categories of reasoning deserve
to be explored separately since they may have different forms or strategies of reasoning, especially
when training data is insufﬁcient."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3416856492027335,"What are the results if NAIL is used as extra training resource for existing reading comprehension
tasks? Figure 4 shows answers to this question. Generally, we can draw the conclusion that using
NAIL as a pre-training step can signiﬁcantly improve the supervised-learning performance for other
tasks, such as LogiQA (35.3→36.9 for test), ReClor (62.6→64.4),8 and RACE (83.2→85.2). This
indicates that NAIL can bring na¨ıve logical reasoning ability to the model, which is a basic reasoning
ability and can be reﬂected into other comprehension tasks. An interesting thing is that for zero-shot
evaluating on RACE, NAIL seems to have a side effect in the pretraining process (22.6<27.8). This is
because of a similar reason as mentioned above, that NAIL and RACE consist of completely different
categories of examples. Results of more datasets are shown in Appendix D.2."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3439635535307517,"Fine-grained Types
We further analyze the model performance with respect to different types of
na¨ıve logical reasoning (Figure 5). We ﬁnd that language models perform well on set operation
problems, while struggle on matching and ordering. We think that language models can provide good
representation of set object, even if models do not really reason derived from the context."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3462414578587699,"Real/Fake
Ordering
Matching Set operation 0 10 20 30 40 50 60"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.34851936218678814,Accuracy
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.35079726651480636,"29.2
28.5
29.4"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3530751708428246,"35.4
34.7 24.1 27.9 37.8"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3553530751708428,"Trained on NAIL, Evaluated on NAIL"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.357630979498861,"BERT
RoBERTa"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.35990888382687924,"Real/Fake
Ordering
Matching Set operation 0 10 20 30 40 50 60"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3621867881548975,Accuracy 38.2 27.6 42.1 28.6 38.2 34.5 36.8 50
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.36446469248291574,"Trained on NAIL, Evaluated on NAIL-E"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.36674259681093396,"BERT
RoBERTa"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3690205011389522,"Real/Fake
Ordering
Matching Set operation 0 10 20 30 40 50 60"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3712984054669704,Accuracy
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3735763097949886,"28.8
28.5
28.8"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.37585421412300685,"35.7
34.6 23.6 27.5 37.1"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.37813211845102507,"Trained on NAIL, Evaluated on NAIL-I"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3804100227790433,"BERT
RoBERTa"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3826879271070615,Figure 5: Accuracy against reasoning types evaluated on different testing sets.
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.38496583143507973,"Error Analysis
We further perform detailed analysis of human-baseline errors and model errors,
while from Table 2, we can observe that human baseline on NAIL (around 70%) is much lower than
that of human expert (100%), and RoBERTa performs much worse (above 30%). We measure the
accuracy against several factors on the test set of NAIL: number of sentences in the backbone template
of the context , number of possible worlds, 9 and context length."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.38724373576309795,"See Figure 6(a), as the number of sentences in the backbone template decreases, the accuracy rate
of human baseline increases signiﬁcantly, and when reduced to 2 and below, human baseline does"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3895216400911162,"8We report the result on the validation set of ReClor, since gold answers for test are not acquired.
9The number of possible worlds is denoted as, the number of subjects × the number of predicates × the
number of objects."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3917995444191344,Under review as a conference paper at ICLR 2022
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.3940774487471526,"1
2
3
4
5
6
7"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.39635535307517084,Number of sentences in the backbone template 0 100 200 300 400 500 600
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.39863325740318906,Number of samples in test set
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4009111617312073,"100.0
100.0
98.8 92.3 78.3 59.8 55.4"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4031890660592255,"23.8
60.3
36.9 30.1 27.4 25.2 31.2"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4054669703872437,"human baseline:wrong
human baseline:correct
RoBERTa-large:wrong
RoBERTa-large:correct"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.40774487471526194,"[0,10) #917"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.41002277904328016,"[10,20) #564"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4123006833712984,"[20,30) #357"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4145785876993166,"[30,40) #21"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4168564920273349,"[40,50) #42"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4191343963553531,"[50,60) #42"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4214123006833713,"[60,70) #103"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.42369020501138954,"[70,100)"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.42596810933940776,"#8
Number of possible worlds in the context 0 20 40 60 80 100"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.428246013667426,Accuracy
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4305239179954442,"human baseline
RoBERTa-large model"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4328018223234624,"0
50
100
150
200
250
300
Context length 0 5 10 15 20"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.43507972665148065,Number of mistakes
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.43735763097949887,"human baseline
RoBERTa-large model"
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4396355353075171,"Figure 6: (a). X-axis: the number of sentences in the backbone template. The height of bars: Number of samples
in the test set grouped by X-axis. The number in bars: The accuracy against X-axis. (b). Accuracy of human
baseline and RoBERTa against the number of possible worlds in the context.(“[a, b)#k” in the x-axis denotes
there are k samples in the test set where a ≤number of possible worlds<b. (c). Number of mistakes made
by human baseline and RoBERTa model against various context length."
"RESULTS AND ANALYSIS
MODEL
INPUT
NAIL
CHINESE NAIL",0.4419134396355353,"not make any mistakes, while performance of RoBERTa does not show any trends, even when the
number equals to 1, the model still makes mistakes in a large percentage of cases. See Figure 6(b),
as the number of possible worlds in the context increases, the accuracy of human baseline tends to
decrease signiﬁcantly. This is because the more possible worlds that need to be considered, the more
judgments humans need to perform, and humans tend to overlook certain conﬂicts, which leads to
wrong decisions for problems. However there is no signiﬁcant correlation between the performance
of the model and number of possible worlds. This indicates that the model does not really ﬁlter and
judge the possible worlds. See Figure 6(c), the model makes mistakes at a variety of context lengths,
while humans perform perfectly for problems with context length ≤73."
RELATED WORK,0.44419134396355353,"5
RELATED WORK"
RELATED WORK,0.44646924829157175,"There are abundant datasets in reading comprehension, which could facilitate the development of
the ﬁeld. MCTest (Richardson et al., 2013) is a multiple-choice reading comprehension dataset that
contains 500 ﬁctional stories and 2k questions. Rajpurkar et al. (2016) proposes the ﬁrst large-scale
reading comprehension dataset SQuAD (100k+ questions), where the answer to each question is a
span of text from the passage. Recently more datasets requiring more complicated reasoning types are
introduced, such as multi-document (Joshi et al., 2017; Dunn et al., 2017), multi-hop reasoning (Yang
et al., 2018; Welbl et al., 2018; Talmor & Berant, 2018), numerical discrete reasoning (Dua et al.,
2019) and commonsense reasoning (Mihaylov et al., 2018; Zhang et al., 2018; Huang et al., 2019a).
However, these datasets cannot test the logical reasoning ability of the models. To ﬁll this gap,
LogiQA (Liu et al., 2020), ReClor (Yu et al., 2020) and LR-LSAT (Wang et al., 2021) was proposed."
RELATED WORK,0.44874715261959,"LogiQA is collected from National Civil Servants Examination. ReClor and LR-LSAT are extracted
from Law School Admission Test. These datasets require logical reasoning to answer the questions.
However, from human experience, there are different forms of reasoning strategies in answering
different logical questions. We believe that completely different categories of logical reasoning
deserve to be explored separately (Rudinger et al., 2020). Different from these datasets, we inductively
deﬁne a typical class of logical reasoning, named na¨ıve logical reasoning, and then create a new
benchmark targeting the task, named NAIL. Compared with LogiQA, Reclor and LR-LSAT, NAIL
focus on the more ﬁne-grained logical reasoning type (na¨ıve logical reasoning)."
RELATED WORK,0.4510250569476082,"There have been many datasets extracted from human examinations, such as LogiQA and ReClor
mentioned above. Besides, RACE dataset (Lai et al., 2017) is collected from English exams for middle
and high school Chinese students. ARC dataset (Clark et al., 2018) consists of 7,787 science exam
questions drawn from a variety of sources.DREAM (Sun et al., 2019) is dialogue-based multiple-
choice reading comprehension dataset collected from English as a Foreign Language examinations
which contains 10,197 questions for 6,444 dialogues."
CONCLUSION,0.4533029612756264,"6
CONCLUSION"
CONCLUSION,0.45558086560364464,"We introduce a more ﬁne-grained logical reasoning, na¨ıve logical reasoning, and We propose a new
large-scale benchmark, NAIL, aiming to help models learn and evaluate na¨ıve logical reasoning
capability. NAIL is sourced from standardized exams and human imitation. Preliminary results show
that there is still a long way to go to equip deep models with true logical reasoning capability."
CONCLUSION,0.45785876993166286,Under review as a conference paper at ICLR 2022
REFERENCES,0.4601366742596811,REFERENCES
REFERENCES,0.4624145785876993,"Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh
Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based
formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), pp. 2357–2367, 2019."
REFERENCES,0.4646924829157175,"Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan
Qi, and Wei Chu. Question directed graph attention network for numerical reasoning over text.
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 6759–6768, 2020."
REFERENCES,0.46697038724373574,"Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and
Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.
arXiv preprint arXiv:1803.05457, 2018."
REFERENCES,0.46924829157175396,"Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu.
Pre-training with whole word masking for chinese bert. arXiv preprint arXiv:1906.08101, 2019."
REFERENCES,0.4715261958997722,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019."
REFERENCES,0.47380410022779046,"Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.
Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In
Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.
2368–2378, 2019."
REFERENCES,0.4760820045558087,"Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho.
Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint
arXiv:1704.05179, 2017."
REFERENCES,0.4783599088838269,"Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, and Jingjing Liu. Hierarchical graph
network for multi-hop question answering. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp. 8823–8838, 2020."
REFERENCES,0.4806378132118451,"Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos QA: Machine reading
comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), pp. 2391–2401, Hong Kong, China, November
2019a. Association for Computational Linguistics. doi: 10.18653/v1/D19-1243. URL https:
//www.aclweb.org/anthology/D19-1243."
REFERENCES,0.48291571753986334,"Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos qa: Machine reading
comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP), pp. 2391–2401, 2019b."
REFERENCES,0.48519362186788156,"Yinya Huang, Meng Fang, Yu Cao, Liwei Wang, and Xiaodan Liang. Dagn: Discourse-aware graph
network for logical reasoning. arXiv preprint arXiv:2103.14349, 2021."
REFERENCES,0.4874715261958998,"Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–
1611, 2017."
REFERENCES,0.489749430523918,"Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding
comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pp. 785–794, Copenhagen, Denmark, September
2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https:
//www.aclweb.org/anthology/D17-1082."
REFERENCES,0.4920273348519362,Under review as a conference paper at ICLR 2022
REFERENCES,0.49430523917995445,"Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A challenge
dataset for machine reading comprehension with logical reasoning, 2020."
REFERENCES,0.49658314350797267,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.4988610478359909,"John McCarthy et al. Programs with common sense. RLE and MIT computation center, 1960."
REFERENCES,0.5011389521640092,"Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,
2018."
REFERENCES,0.5034168564920274,"Allen Newell and Herbert Simon. The logic theory machine–a complex information processing
system. IRE Transactions on information theory, 2(3):61–79, 1956."
REFERENCES,0.5056947608200456,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019."
REFERENCES,0.5079726651480638,"Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016. Association for
Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.org/
anthology/D16-1264."
REFERENCES,0.510250569476082,"Matthew Richardson, Christopher JC Burges, and Erin Renshaw. Mctest: A challenge dataset for the
open-domain machine comprehension of text. In Proceedings of the 2013 conference on empirical
methods in natural language processing, pp. 193–203, 2013."
REFERENCES,0.5125284738041003,"Rachel Rudinger, Vered Shwartz, Jena D Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan
Le Bras, Noah A Smith, and Yejin Choi. Thinking like a skeptic: Defeasible inference in natural
language. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing: Findings, pp. 4661–4675, 2020."
REFERENCES,0.5148063781321185,"Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. Dream: A challenge data
set and models for dialogue-based reading comprehension. Transactions of the Association for
Computational Linguistics, 7:217–231, 2019."
REFERENCES,0.5170842824601367,"Alon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions.
In Proceedings of the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 641–651,
2018."
REFERENCES,0.5193621867881549,"Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, and
Nan Duan. From lsat: The progress and challenges of complex reasoning. arXiv preprint
arXiv:2108.00648, 2021."
REFERENCES,0.5216400911161732,"Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop reading
comprehension across documents. Transactions of the Association for Computational Linguistics,
6:287–302, 2018."
REFERENCES,0.5239179954441914,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von
Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama
Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language
processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-
cessing: System Demonstrations, pp. 38–45, Online, October 2020. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6."
REFERENCES,0.5261958997722096,"Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,
and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question
answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, pp. 2369–2380, 2018."
REFERENCES,0.5284738041002278,Under review as a conference paper at ICLR 2022
REFERENCES,0.530751708428246,"Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: A reading comprehension dataset
requiring logical reasoning. In International Conference on Learning Representations (ICLR),
April 2020."
REFERENCES,0.5330296127562643,"Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.
Record: Bridging the gap between human and machine commonsense reading comprehension.
arXiv preprint arXiv:1810.12885, 2018."
REFERENCES,0.5353075170842825,Under review as a conference paper at ICLR 2022
REFERENCES,0.5375854214123007,"A
IMPLEMENTATION DETAILS OF BASELINES"
REFERENCES,0.5398633257403189,"Rule-based Methods
Following LogiQA (Liu et al., 2020), we adopt two simple rule-based
methods based on text matching. Speciﬁcally, word matching is to measure the degree of unigram
overlap between the candidate answer and the given paragraph-query pair; sliding window takes into
account the n-gram when calculating the matching score."
REFERENCES,0.5421412300683371,"Neural-based Methods
Pre-trained neural models, such as BERT (Devlin et al., 2019), RoBERTa
(Liu et al., 2019), have achieved impressive performance on reading comprehension. The input to
the pre-trained model is the concatenation of the paragraph, the query and the candidate answer,
separated by [SEP] tokens, denoted as: [CLS], paragraph, [SEP], query, [SEP], answer. After
the encoding of the pre-trained model, the representation of [CLS] token followed by an multiple
layer perceptron (MLP) is used for scoring."
REFERENCES,0.5444191343963554,"We re-implement the rule-based methods following LogiQA (Liu et al., 2020). For pre-trained
models, we modify the code of Transformers of HuggingFace (Wolf et al., 2020) to implement them
on NAIL. We take the off-the-shelf model BERT and RoBERTa for NAIL, and Chinese BERT and
Chinese RoBERTa (Cui et al., 2019) for Chinese NAIL. All hyper-parameters are selected by the
model performance on the development sets."
REFERENCES,0.5466970387243736,"B
MORE STRONG BASELINES"
REFERENCES,0.5489749430523918,"We also try seq2seq-based pre-trained models (i.e. T5 (Raffel et al., 2019)) and well-designed models
for other related tasks (i.e. DAGN (Huang et al., 2021) for LogiQA and Reclor, HGN (Fang et al.,
2020) for HotpotQA (Yang et al., 2018), and QDGAT (Chen et al., 2020) for DROP (Dua et al.,
2019)) on our English version of NAIL."
REFERENCES,0.55125284738041,"A problem is regarded as four samples to T5 during training. The input of a sample is: con-
text+query+one of the four choices. And we let the model generate the label for the sample: if the
choice is the correct answer, then generate “True”, otherwise generate “False”. And during inference,
for a problem, we choose the option with the highest probability of generating “True” among the
four options as the prediction. We set input max len = 512 and ﬁnetuned the off-the-shelf T5-base
model on the training set of NAIL, results are shown in Table 4."
REFERENCES,0.5535307517084282,"LogiQA and ReClor are two logical benchmarks introduced in the text of the paper to test models’
various reasoning abilities. HotpotQA is a multiple-choice QA dataset featuring natural, multi-hop
questions, with strong supervision for supporting facts to test models’ multi-hop factual reasoning
ability. DROP is a 96k-question benchmark to test models’ numerical discrete reasoning ability over
paragraphs (such as addition, counting, or sorting)."
REFERENCES,0.5558086560364465,"We choose three state-of-the-art and open-sourced models designed for the above benchmarks: DAGN
for LogiQA and Reclor, HGN for HotpotQA, and QDGAT for DROP respectively.We reproduced
these three models with the code released by authors10 and NAIL as training data. Results on NAIL
are shown as Table 4."
REFERENCES,0.5580865603644647,"Model
Encoder
NAIL"
REFERENCES,0.5603644646924829,"Dev
Test"
REFERENCES,0.5626423690205011,"RoBERTa LARGE
RoBERTa LARGE
34.6
30.1
RoBERTa BASE
RoBERTa BASE
27.6
26.4"
REFERENCES,0.5649202733485194,"T5 BASE
T5 BASE
27.1
26.3
DAGN
RoBERTa LARGE
36.0
32.3
HGN
RoBERTa LARGE
30.1
28.7
QDGAT
RoBERTa LARGE
28.3
28.7"
REFERENCES,0.5671981776765376,Table 4: Results on NAIL (accuracy%).
REFERENCES,0.5694760820045558,"10DAGN:
https://github.com/Eleanor-H/DAGN,
QDGAT:
https://github.com/
emnlp2020qdgat/QDGAT, HGN: https://github.com/yuwfan/HGN"
REFERENCES,0.571753986332574,Under review as a conference paper at ICLR 2022
REFERENCES,0.5740318906605922,"We observe that DAGN achieves the best results on NAIL, since ReClor and LogiQA are much
closer to NAIL, the well-designed discourse-aware graph network DAGN is more applicable to
NAIL. However there is still very much space for models’ improvement compared to humans. HGN
is a hierarchical graph network for multi-hop question answering, which is designed to aggregate
heterogeneous information, and QDGAT is a question directed graph attention network, which is
dedicated to identifying numbers and the computation between them, which are both of minimal
help to NAIL. T5 is a very powerful generative model, but it seems that T5 lacks the na¨ıve reasoning
capability as well. Due to computational resource limitation, we did not try a T5 model with larger
number of parameters."
REFERENCES,0.5763097949886105,"C
THE EFFECT OF NAIL, NAIL-E AND NAIL-I"
REFERENCES,0.5785876993166287,"To investigate the effect of NAIL, NAIL-E and NAIL-I, we train a RoBERTa LARGE model on NAIL,
NAIL-E and NAIL-I separately and evaluate on the corresponding set of all tests (Table 5). When
trained on NAIL-I and evaluated on NAIL-E, the model achieves the best accuracy 38.5%, showing
that human imitations can signiﬁcantly help models learn logic from natural text. When trained on
NAIL-E and evaluated on NAIL-E, the model achieves the worst accuracy 22.9%. The possible reason
is that the amount of data of NAIL-E is too small to train a deep model. Corresponding results on the
validation set are shown in Table 6."
REFERENCES,0.5808656036446469,"Trained on →
NAIL
NAIL-E
NAIL-I
Evaluated on ↓"
REFERENCES,0.5831435079726651,"NAIL
30.1
26.4
33.9
NAIL-E
37.4
22.9
38.5
NAIL-I
29.7
26.6
33.7"
REFERENCES,0.5854214123006833,Table 5: Performance of RoBERTa on different training sets and test sets.
REFERENCES,0.5876993166287016,"Trained on →
NAIL
NAIL-E
NAIL-I
Evaluated on ↓"
REFERENCES,0.5899772209567198,"NAIL
34.6
24.9
32.0
NAIL-E
43.3
37.4
33.0
NAIL-I
34.2
24.3
31.9"
REFERENCES,0.592255125284738,Table 6: Performance of RoBERTa on different training sets and validation sets.
REFERENCES,0.5945330296127562,"D
TRANSFER LEARNING"
REFERENCES,0.5968109339407744,"D.1
OTHER DATASETS AS EXTRA TRAINING RESOURCE"
REFERENCES,0.5990888382687927,"What are the results if pre-trained model is ﬁrst trained on existing reading comprehension datasets,
and then ﬁne-tuned on NAIL? Table 3 shows the results on the test set of NAIL,NAIL-E, NAIL-I
where LogiQA, ReClor11 and RACE are adopted. And here in Table 7 we give the results on the
validation splits of NAIL,NAIL-E, NAIL-I."
REFERENCES,0.6013667425968109,"D.2
NAIL AS EXTRA TRAINING RESOURCE"
REFERENCES,0.6036446469248291,"What are the results if NAIL is used as extra training resource for existing reading comprehension
tasks? Using NAIL as extra training resource, we conduct plenty of experiments on existing
benchmarks. Besides LogiQA, ReClor and RACE (see in Figure 4), more results are shown in"
REFERENCES,0.6059225512528473,"11For fair comparison, in all cross-benchmarks experiments in this paper, we removed samples in the training
set that are duplicates of those in the test set. For example, when using ReClor or LogiQA as extra training
resource when testing on NAIL, we remove problems in the training set of ReClor or LogiQA if they appear in
the test set of NAIL."
REFERENCES,0.6082004555808656,Under review as a conference paper at ICLR 2022
REFERENCES,0.6104783599088838,"Evaluate on →
NAIL
NAIL-E
NAIL-I
Train on ↓"
REFERENCES,0.6127562642369021,"∅
23.6
24.2
23.6
LogiQA
33.0
45.1
32.4
ReClor
28.4
25.3
28.5
RACE
26.4
27.5
26.4"
REFERENCES,0.6150341685649203,"NAIL
34.6
43.3
34.2
LogiQA→NAIL
37.9
47.3
37.5
ReClor→NAIL
35.2
39.6
35.9
RACE→NAIL
35.4
36.3
35.4"
REFERENCES,0.6173120728929385,"Table 7: Transfer learning results when evaluating on the validation split of NAIL, NAIL-E, NAIL-I
(accuracy%)."
REFERENCES,0.6195899772209568,"Table 8, 9. We adopted MathQA (Amini et al., 2019), and HotpotQA (Yang et al., 2018). For
MathQA, we include a prediction head for multiple choice based on the RoBERTa-large model, and
for HotpotQA,12 we include a prediction head for question answering. For MathQA, we concat the
problem and annotated formula as input."
REFERENCES,0.621867881548975,"Evaluate on →
MathQA
Train on ↓
Test"
REFERENCES,0.6241457858769932,"MathQA
39.8"
REFERENCES,0.6264236902050114,"NAIL →MathQA
41.2"
REFERENCES,0.6287015945330297,"Table
8:
Evaluating
on
MathQA
after
RoBERTa-large pretrained on MathQA/NAIL
training set."
REFERENCES,0.6309794988610479,"Evaluate on →
HotpotQA
Train on ↓
Ans F1"
REFERENCES,0.6332574031890661,"HotpotQA
69.8"
REFERENCES,0.6355353075170843,"NAIL →HotpotQA
72.6"
REFERENCES,0.6378132118451025,"Table 9:
Evaluating on HotpotQA after
RoBERTa-large pretrained on HotpotQA/NAIL
training set."
REFERENCES,0.6400911161731208,"Experimental results show that, if we use NAIL as extra training resource, the supervised-learning
results on all of these six datasets: LogiQA, ReClor, RACE, MathQA, HotpotQA will improve.
This indicates that equipping models with na¨ıve logical reasoning ability can help solve math
word problems (MathQA), improve multi-hop factual reasoning skills (HotpotQA), solve various
logical reasoning problems (LogiQA and ReClor), and enhance general understanding and reading
comprehension capability (RACE). In the future, we will test on more known datasets, to verify that
na¨ıve logical reasoning is a basic capability and is helpful for other tasks."
REFERENCES,0.642369020501139,"E
ANALYSIS OF FINE-GRAINED TYPES"
REFERENCES,0.6446469248291572,"In Section 4, we analyze the model performance with respect to different types of na¨ıve logical
reasoning (Figure 5). Figure 8 shows the accuracy against ﬁne-grained reasoning types on the test set
of NAIL, NAIL-E, NAIL-I when training on NAIL, NAIL-E, NAIL-I respectively. And Figure 7 shows
the corresponding results on the validation set."
REFERENCES,0.6469248291571754,"From above ﬁgures we can ﬁnd that language models perform well on set operation problems, while
struggle on matching and ordering. We think that language models can provide good representation
of set object, even if models do not really reason derived from the context."
REFERENCES,0.6492027334851936,"F
DISCUSSIONS ABOUT FUTURE DIRECTIONS"
REFERENCES,0.6514806378132119,We have some ideas for future directions for models to solve NAIL.
REFERENCES,0.6537585421412301,"The ﬁrst point is to identify deterministic information or information with low uncertainty. Generally,
we need to detect the atomic information that can be used directly without reasoning, which is
the key to solving the problem. Different conditions should not be considered with equal priority,"
REFERENCES,0.6560364464692483,"12Due to computational resource limitations, we only trained 3 epochs for experiments involved HotpotQA."
REFERENCES,0.6583143507972665,Under review as a conference paper at ICLR 2022
REFERENCES,0.6605922551252847,Real/Fake
REFERENCES,0.662870159453303,Ordering
REFERENCES,0.6651480637813212,Matching
REFERENCES,0.6674259681093394,Set operation 0 10 20 30 40 50 60
REFERENCES,0.6697038724373576,Accuracy
REFERENCES,0.6719817767653758,"26.4
25.9 22.4"
REFERENCES,0.6742596810933941,"36.3
38.5 34"
REFERENCES,0.6765375854214123,"37.8
37.3"
REFERENCES,0.6788154897494305,"Trained on NAIL, Evaluated on NAIL"
REFERENCES,0.6810933940774487,"BERT
RoBERTa"
REFERENCES,0.683371298405467,Real/Fake
REFERENCES,0.6856492027334852,Ordering
REFERENCES,0.6879271070615034,Matching
REFERENCES,0.6902050113895216,Set operation 0 10 20 30 40 50 60
REFERENCES,0.6924829157175398,Accuracy 30.4 41.2 20 36.8 43.5 52.9 50 36.8
REFERENCES,0.6947608200455581,"Trained on NAIL, Evaluated on NAIL-E"
REFERENCES,0.6970387243735763,"BERT
RoBERTa"
REFERENCES,0.6993166287015945,Real/Fake
REFERENCES,0.7015945330296127,Ordering
REFERENCES,0.7038724373576309,Matching
REFERENCES,0.7061503416856492,Set operation 0 10 20 30 40 50 60
REFERENCES,0.7084282460136674,Accuracy
REFERENCES,0.7107061503416856,"26.1
25.1 22.5"
REFERENCES,0.7129840546697038,"36.3
38.2 33"
REFERENCES,0.715261958997722,"37.3
37.4"
REFERENCES,0.7175398633257403,"Trained on NAIL, Evaluated on NAIL-I"
REFERENCES,0.7198177676537585,"BERT
RoBERTa"
REFERENCES,0.7220956719817767,Real/Fake
REFERENCES,0.724373576309795,Ordering
REFERENCES,0.7266514806378133,Matching
REFERENCES,0.7289293849658315,Set operation 0 10 20 30 40 50 60
REFERENCES,0.7312072892938497,Accuracy
REFERENCES,0.7334851936218679,"24.8
24.4
23.6 28.6"
REFERENCES,0.7357630979498861,"23.7
24.4
23.3 27.1"
REFERENCES,0.7380410022779044,"Trained on NAIL-E, Evaluated on NAIL"
REFERENCES,0.7403189066059226,"BERT
RoBERTa"
REFERENCES,0.7425968109339408,Real/Fake
REFERENCES,0.744874715261959,Ordering
REFERENCES,0.7471526195899773,Matching
REFERENCES,0.7494305239179955,Set operation 0 10 20 30 40 50 60
REFERENCES,0.7517084282460137,Accuracy 17.4 23.5
REFERENCES,0.7539863325740319,"30
31.6 52.2 35.3 20 26.3"
REFERENCES,0.7562642369020501,"Trained on NAIL-E, Evaluated on NAIL-E"
REFERENCES,0.7585421412300684,"BERT
RoBERTa"
REFERENCES,0.7608200455580866,Real/Fake
REFERENCES,0.7630979498861048,Ordering
REFERENCES,0.765375854214123,Matching
REFERENCES,0.7676537585421412,Set operation 0 10 20 30 40 50 60
REFERENCES,0.7699316628701595,Accuracy
REFERENCES,0.7722095671981777,"25.2
24.4
23.3"
REFERENCES,0.7744874715261959,"28.4
30.6 24.9 18.6 22.4"
REFERENCES,0.7767653758542141,"Trained on NAIL-E, Evaluated on NAIL-I"
REFERENCES,0.7790432801822323,"BERT
RoBERTa"
REFERENCES,0.7813211845102506,Real/Fake
REFERENCES,0.7835990888382688,Ordering
REFERENCES,0.785876993166287,Matching
REFERENCES,0.7881548974943052,Set operation 0 10 20 30 40 50 60
REFERENCES,0.7904328018223234,Accuracy 36
REFERENCES,0.7927107061503417,"26.7
28.1 42.4 35.8 32 36.2 42.4"
REFERENCES,0.7949886104783599,"Trained on NAIL-I, Evaluated on NAIL"
REFERENCES,0.7972665148063781,"BERT
RoBERTa"
REFERENCES,0.7995444191343963,Real/Fake
REFERENCES,0.8018223234624146,Ordering
REFERENCES,0.8041002277904328,Matching
REFERENCES,0.806378132118451,Set operation 0 10 20 30 40 50 60
REFERENCES,0.8086560364464692,Accuracy 34.8 38.2 30 47.4 39.1 29.4 40 36.8
REFERENCES,0.8109339407744874,"Trained on NAIL-I, Evaluated on NAIL-E"
REFERENCES,0.8132118451025057,"BERT
RoBERTa"
REFERENCES,0.8154897494305239,Real/Fake
REFERENCES,0.8177676537585421,Ordering
REFERENCES,0.8200455580865603,Matching
REFERENCES,0.8223234624145785,Set operation 0 10 20 30 40 50 60
REFERENCES,0.8246013667425968,Accuracy 36.1
REFERENCES,0.826879271070615,"26.1
28 42.1 35.6 32.2 36.02 42.63"
REFERENCES,0.8291571753986332,"Trained on NAIL-I, Evaluated on NAIL-I"
REFERENCES,0.8314350797266514,"BERT
RoBERTa"
REFERENCES,0.8337129840546698,Figure 7: Accuracy against the reasoning types based on different training set and development set.
REFERENCES,0.835990888382688,"i.e., the more certain a condition we start from, the fewer possible worlds that the condition yields.
Speciﬁcally, for real/fake mixture problems, we also need to promptly identify two atomic statements
that yield a contradictory, that is, the two statements cannot be both true (there must be one false)
or both false (there must be one true), which is often the breakthrough in solving real/fake mixture
problems."
REFERENCES,0.8382687927107062,"The second is to detect informative subjects/objects. Generally, the more frequently a subject/object
is mentioned in context, the more relevant information it carries. Tables and bi(/tri)partite diagrams
can help to clarify the correspondence between given subjects and objects. For example, in the case
of Figure 1, tables play an effective role in solving the problems. And as for the third case in Figure
3, which is categorized as a matching problem, a tri-partite diagram could help, that is, three disjoint
and independent sets U, V and W represent {A,B,C}, {Beijing, Nanjing, Chengdu}, {a doctor, an
actor, and a teacher} respectively, an edge connecting a vertex in one set to one in another set denotes
the “is a” predicate."
REFERENCES,0.8405466970387244,"The third direction is allowing models to better utilize elimination. On the one hand, models can
perform forward elimination, i.e., according to the context, each time we draw a deﬁnite conclusion,
we can retain options that are logically consistent and exclude those that do not ﬁt the derived
conclusion. On the other hand, models can also perform backward elimination. For example, when
sometimes it is not easy to draw exact inferences directly from the context, then we can substitute the
options into the context. If substituting an option creates a contradiction within the context, then the
option should be excluded. Forward and backward elimination facilitate the model in arriving at the
correct answer."
REFERENCES,0.8428246013667426,Under review as a conference paper at ICLR 2022
REFERENCES,0.8451025056947609,"Real/Fake
Ordering
Matching Set operation 0 10 20 30 40 50 60"
REFERENCES,0.8473804100227791,Accuracy
REFERENCES,0.8496583143507973,"29.2
28.5
29.4"
REFERENCES,0.8519362186788155,"35.4
34.7 24.1 27.9 37.8"
REFERENCES,0.8542141230068337,"Trained on NAIL, Evaluated on NAIL"
REFERENCES,0.856492027334852,"BERT
RoBERTa"
REFERENCES,0.8587699316628702,"Real/Fake
Ordering
Matching Set operation 0 10 20 30 40 50 60"
REFERENCES,0.8610478359908884,Accuracy 38.2 27.6 42.1 28.6 38.2 34.5 36.8 50
REFERENCES,0.8633257403189066,"Trained on NAIL, Evaluated on NAIL-E"
REFERENCES,0.8656036446469249,"BERT
RoBERTa"
REFERENCES,0.8678815489749431,"Real/Fake
Ordering
Matching Set operation 0 10 20 30 40 50 60"
REFERENCES,0.8701594533029613,Accuracy
REFERENCES,0.8724373576309795,"28.8
28.5
28.8"
REFERENCES,0.8747152619589977,"35.7
34.6 23.6 27.5 37.1"
REFERENCES,0.876993166287016,"Trained on NAIL, Evaluated on NAIL-I"
REFERENCES,0.8792710706150342,"BERT
RoBERTa"
REFERENCES,0.8815489749430524,Real/Fake
REFERENCES,0.8838268792710706,Ordering
REFERENCES,0.8861047835990888,Matching
REFERENCES,0.8883826879271071,Set operation 0 10 20 30 40 50 60
REFERENCES,0.8906605922551253,Accuracy
REFERENCES,0.8929384965831435,"26.9
26 22.4 29.4 23.9"
REFERENCES,0.8952164009111617,"26
26.5
28.2"
REFERENCES,0.89749430523918,"Trained on NAIL-E, Evaluated on NAIL"
REFERENCES,0.8997722095671982,"BERT
RoBERTa"
REFERENCES,0.9020501138952164,Real/Fake
REFERENCES,0.9043280182232346,Ordering
REFERENCES,0.9066059225512528,Matching
REFERENCES,0.908883826879271,Set operation 0 10 20 30 40 50 60
REFERENCES,0.9111617312072893,Accuracy 14.7 17.2 21.1
REFERENCES,0.9134396355353075,"35.7
32.4 20.7"
REFERENCES,0.9157175398633257,"15.8
14.3"
REFERENCES,0.9179954441913439,"Trained on NAIL-E, Evaluated on NAIL-E"
REFERENCES,0.9202733485193622,"BERT
RoBERTa"
REFERENCES,0.9225512528473804,Real/Fake
REFERENCES,0.9248291571753986,Ordering
REFERENCES,0.9271070615034168,Matching
REFERENCES,0.929384965831435,Set operation 0 10 20 30 40 50 60
REFERENCES,0.9316628701594533,Accuracy
REFERENCES,0.9339407744874715,"27.5
26.4 22.5"
REFERENCES,0.9362186788154897,"28.9
29.3 24.7 28 21.4"
REFERENCES,0.9384965831435079,"Trained on NAIL-E, Evaluated on NAIL-I"
REFERENCES,0.9407744874715261,"BERT
RoBERTa"
REFERENCES,0.9430523917995444,Real/Fake
REFERENCES,0.9453302961275627,Ordering
REFERENCES,0.9476082004555809,Matching
REFERENCES,0.9498861047835991,Set operation 0 10 20 30 40 50 60
REFERENCES,0.9521640091116174,Accuracy 34.5 29.1 20.5
REFERENCES,0.9544419134396356,"35.7
37.8 31.6 24.8 41.8"
REFERENCES,0.9567198177676538,"Trained on NAIL-I, Evaluated on NAIL"
REFERENCES,0.958997722095672,"BERT
RoBERTa"
REFERENCES,0.9612756264236902,Real/Fake
REFERENCES,0.9635535307517085,Ordering
REFERENCES,0.9658314350797267,Matching
REFERENCES,0.9681093394077449,Set operation 0 10 20 30 40 50 60
REFERENCES,0.9703872437357631,Accuracy 38.2 13.8 42.1
REFERENCES,0.9726651480637813,"35.7
32.4 34.5 21.1"
REFERENCES,0.9749430523917996,"57.1
Trained on NAIL-I, Evaluated on NAIL-E"
REFERENCES,0.9772209567198178,"BERT
RoBERTa"
REFERENCES,0.979498861047836,Real/Fake
REFERENCES,0.9817767653758542,Ordering
REFERENCES,0.9840546697038725,Matching
REFERENCES,0.9863325740318907,Set operation 0 10 20 30 40 50 60
REFERENCES,0.9886104783599089,Accuracy 34.3 29.9 19.5
REFERENCES,0.9908883826879271,"35.7
38.1 31.5 25 41.1"
REFERENCES,0.9931662870159453,"Trained on NAIL-I, Evaluated on NAIL-I"
REFERENCES,0.9954441913439636,"BERT
RoBERTa"
REFERENCES,0.9977220956719818,Figure 8: Accuracy against the reasoning types based on different training set and testing set.
