Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0015432098765432098,"Deep discriminative approaches like random forests and deep neural networks have
1"
ABSTRACT,0.0030864197530864196,"recently found applications in many important real-world scenarios. However, de-
2"
ABSTRACT,0.004629629629629629,"ploying these learning algorithms in safety-critical applications raises concerns, par-
3"
ABSTRACT,0.006172839506172839,"ticularly when it comes to ensuring confidence calibration for both in-distribution
4"
ABSTRACT,0.007716049382716049,"and out-of-distribution data points. Many popular methods for in-distribution (ID)
5"
ABSTRACT,0.009259259259259259,"calibration, such as isotonic and Platt’s sigmoidal regression, exhibit excellent ID
6"
ABSTRACT,0.010802469135802469,"calibration performance. However, these methods are not calibrated for the entire
7"
ABSTRACT,0.012345679012345678,"feature space, leading to overconfidence in the case of out-of-distribution (OOD)
8"
ABSTRACT,0.013888888888888888,"samples. On the other end of the spectrum, existing out-of-distribution (OOD)
9"
ABSTRACT,0.015432098765432098,"calibration methods generally exhibit poor in-distribution (ID) calibration. In this
10"
ABSTRACT,0.016975308641975308,"paper, we address ID and OOD calibration problems jointly. We leveraged the
11"
ABSTRACT,0.018518518518518517,"fact that deep models, including both random forests and deep-nets, learn internal
12"
ABSTRACT,0.020061728395061727,"representations which are unions of polytopes with affine activation functions to
13"
ABSTRACT,0.021604938271604937,"conceptualize them both as partitioning rules of the feature space. We replace the
14"
ABSTRACT,0.023148148148148147,"affine function in each polytope populated by the training data with a Gaussian
15"
ABSTRACT,0.024691358024691357,"kernel. Our experiments on both tabular and vision benchmarks show that the
16"
ABSTRACT,0.026234567901234566,"proposed approaches obtain well-calibrated posteriors while mostly preserving or
17"
ABSTRACT,0.027777777777777776,"improving the classification accuracy of the original algorithm for ID region, and
18"
ABSTRACT,0.029320987654320986,"extrapolate beyond the training data to handle OOD inputs appropriately.
19"
INTRODUCTION,0.030864197530864196,"1
Introduction
20"
INTRODUCTION,0.032407407407407406,"Machine learning methods, specially deep neural networks and random forests have shown excellent
21"
INTRODUCTION,0.033950617283950615,"performance in many real-world tasks, including drug discovery, autonomous driving and clinical
22"
INTRODUCTION,0.035493827160493825,"surgery [1–3]. However, calibrating confidence over the whole feature space for these approaches
23"
INTRODUCTION,0.037037037037037035,"remains a key challenge in the field [4]. Calibrated confidence within the training or in-distribution
24"
INTRODUCTION,0.038580246913580245,"(ID) region as well as in the out-of-distribution (OOD) region is crucial for safety critical applications
25"
INTRODUCTION,0.040123456790123455,"like autonomous driving and computer-assisted surgery, where any aberrant reading should be
26"
INTRODUCTION,0.041666666666666664,"detected and taken care of immediately [4, 5].
27"
INTRODUCTION,0.043209876543209874,"The approaches to calibrate OOD confidence for learning algorithms described in the literature can
28"
INTRODUCTION,0.044753086419753084,"be roughly divided into two groups: discriminative and generative. Intuitively, the easiest solution for
29"
INTRODUCTION,0.046296296296296294,"OOD confidence calibration is to learn a function that gives higher scores for in-distribution samples
30"
INTRODUCTION,0.047839506172839504,"and lower scores for OOD samples [6]. The discriminative approaches try to either modify the loss
31"
INTRODUCTION,0.04938271604938271,"function [7–9] or train the network exhaustively on OOD datasets to calibrate on OOD samples [10, 4].
32"
INTRODUCTION,0.05092592592592592,"Recently, Hein et al. [4] showed ReLU networks produce arbitrarily high confidence as the inference
33"
INTRODUCTION,0.05246913580246913,"point moves far away from the training data. Therefore, calibrating ReLU networks for the whole
34"
INTRODUCTION,0.05401234567901234,"OOD region is not possible without fundamentally changing the network architecture. As a result, all
35"
INTRODUCTION,0.05555555555555555,"of the aforementioned algorithms are unable to provide any guarantee about the performance of the
36"
INTRODUCTION,0.05709876543209876,"network throughout the whole feature space. The other group tries to learn generative models for the
37"
INTRODUCTION,0.05864197530864197,"in-distribution as well as the out-of-distribution samples. The general idea is to do likelihood ratio
38"
INTRODUCTION,0.06018518518518518,"test for a particular sample using the generative models [11], or threshold the ID likelihoods to detect
39"
INTRODUCTION,0.06172839506172839,"OOD samples. However, it is not obvious how to control likelihoods far away from the training data
40"
INTRODUCTION,0.06327160493827161,"for powerful generative models like variational autoencoders (VAEs) [12] and generative adversarial
41"
INTRODUCTION,0.06481481481481481,"networks (GAN) [13]. Moreover, Nalisnick et al. [14] and Hendrycks et al. [10] showed VAEs and
42"
INTRODUCTION,0.06635802469135803,"GANs can also yield overconfident likelihoods far away from the training data.
43"
INTRODUCTION,0.06790123456790123,"The algorithms described so far are concerned with OOD confidence calibration for deep-nets only.
44"
INTRODUCTION,0.06944444444444445,"However, we show that other approaches which partition the feature space, for example random forest,
45"
INTRODUCTION,0.07098765432098765,"can also suffer from poor confidence calibration both in the ID and the OOD regions. Moreover, the
46"
INTRODUCTION,0.07253086419753087,"algorithms described above are concerned about the confidence in the OOD region only and do not
47"
INTRODUCTION,0.07407407407407407,"address the confidence calibration within the ID region at all. This issue is addressed separately in
48"
INTRODUCTION,0.07561728395061729,"a different group of literature [15–20]. Instead, we consider both calibration problems jointly and
49"
INTRODUCTION,0.07716049382716049,"propose an approach that achieves good calibration throughout the whole feature space.
50"
INTRODUCTION,0.0787037037037037,"In this paper, we conceptualize both random forest and ReLU networks as partitioning rules with an
51"
INTRODUCTION,0.08024691358024691,"affine activation over each polytope. We consider replacing the affine functions learned over the
52"
INTRODUCTION,0.08179012345679013,"polytopes with Gaussian kernels. We propose two novel kernel density estimation techniques named
53"
INTRODUCTION,0.08333333333333333,"Kernel Density Forest (KDF) and Kernel Density Network (KDN). Our proposed approach completely
54"
INTRODUCTION,0.08487654320987655,"excludes the need for training on OOD examples for the model (unsupervised OOD calibration). We
55"
INTRODUCTION,0.08641975308641975,"conduct several simulation and real data studies that show both KDF and KDN are well-calibrated for
56"
INTRODUCTION,0.08796296296296297,"OOD samples while they maintain good performance in the ID region.
57"
RELATED WORKS AND OUR CONTRIBUTIONS,0.08950617283950617,"2
Related Works and Our Contributions
58"
RELATED WORKS AND OUR CONTRIBUTIONS,0.09104938271604938,"There are a number of approaches in the literature which attempt to learn a generative model and
59"
RELATED WORKS AND OUR CONTRIBUTIONS,0.09259259259259259,"control the likelihoods far away from the training data. For example, Ren et al. [11] employed
60"
RELATED WORKS AND OUR CONTRIBUTIONS,0.0941358024691358,"likelihood ratio test for detecting OOD samples. Wan et al. [8] modified the training loss so that the
61"
RELATED WORKS AND OUR CONTRIBUTIONS,0.09567901234567901,"downstream projected features follow a Gaussian distribution. However, there is no guarantee of
62"
RELATED WORKS AND OUR CONTRIBUTIONS,0.09722222222222222,"performance for OOD detection for the above methods. To the best of our knowledge, apart from
63"
RELATED WORKS AND OUR CONTRIBUTIONS,0.09876543209876543,"us, only Meinke et al. [5] has proposed an approach to guarantee asymptotic performance for OOD
64"
RELATED WORKS AND OUR CONTRIBUTIONS,0.10030864197530864,"detection. Compared to the aforementioned methods, our approach differs in several ways:
65"
RELATED WORKS AND OUR CONTRIBUTIONS,0.10185185185185185,"• We address the confidence calibration problem for both ReLU-nets and random forests.
66"
RELATED WORKS AND OUR CONTRIBUTIONS,0.10339506172839506,"• We address ID and OOD calibration problem as a continuum.
67"
RELATED WORKS AND OUR CONTRIBUTIONS,0.10493827160493827,"• We provide an algorithm for OOD confidence calibration for both tabular and vision datatsets
68"
RELATED WORKS AND OUR CONTRIBUTIONS,0.10648148148148148,"whereas most of the existing methods are tailor-made for vision problems.
69"
RELATED WORKS AND OUR CONTRIBUTIONS,0.10802469135802469,"• We propose an unsupervised post-hoc OOD calibration approach.
70"
TECHNICAL BACKGROUND,0.1095679012345679,"3
Technical Background
71"
SETTING,0.1111111111111111,"3.1
Setting
72"
SETTING,0.11265432098765432,"Consider a supervised learning problem with independent and identically distributed training samples
73"
SETTING,0.11419753086419752,"{(xi, yi)}n
i=1 such that (X, Y ) ∼PX,Y , where X ∼PX is a X ⊆RD valued input and Y ∼PY is a
74"
SETTING,0.11574074074074074,"Y = {1, · · · , K} valued class label. Let S be the high density region of the marginal, PX, thus S ⊊
75"
SETTING,0.11728395061728394,"X. Here the goal is to learn a confidence score, g : RD →[0, 1]K, g(x) = [g1(x), g2(x), . . . , gK(x)]
76"
SETTING,0.11882716049382716,"such that,
77"
SETTING,0.12037037037037036,"gy(x) =
PY |X(y|x),
if x ∈S
PY (y),
if x /∈S ,
∀y ∈Y
(1)"
SETTING,0.12191358024691358,"where PY |X(y|x) is the posterior probability for class y given by the Bayes formula:
78"
SETTING,0.12345679012345678,"PY |X(y|x) =
PX|Y (x|y)PY (y)
PK
k=1 PX|Y (x|k)PY (k)
,
∀y ∈Y.
(2)"
SETTING,0.125,"Here PX|Y (x|y) is the class conditional density which we will refer as fy(x) hereafter for brevity.
79"
MAIN IDEA,0.12654320987654322,"3.2
Main Idea
80"
MAIN IDEA,0.12808641975308643,"Deep discriminative networks partition the feature space Rd into a union of p affine polytopes Qr
81"
MAIN IDEA,0.12962962962962962,"such that Sp
r=1 Qr = Rd, and learn an affine function over each polytope [4, 21]. Mathematically,
82"
MAIN IDEA,0.13117283950617284,"the unnormalized class-conditional density for the label y estimated by these deep discriminative
83"
MAIN IDEA,0.13271604938271606,"models at a particular point x can be expressed as:
84"
MAIN IDEA,0.13425925925925927,"ˆfy(x) = p
X"
MAIN IDEA,0.13580246913580246,"r=1
(a⊤
r x + br)1(x ∈Qr).
(3)"
MAIN IDEA,0.13734567901234568,"For example, in the case of a decision tree, ar = 0, i.e., decision tree assumes uniform distribution
85"
MAIN IDEA,0.1388888888888889,"for the class-conditional densities over the leaf nodes. Among these polytopes, the ones that lie on
86"
MAIN IDEA,0.1404320987654321,"the boundary of the training data extend to the whole feature space and hence encompass all the OOD
87"
MAIN IDEA,0.1419753086419753,"samples. Since the posterior probability for a class is determined by the affine activation over each of
88"
MAIN IDEA,0.14351851851851852,"these polytopes, the algorithms tend to be overconfident when making predictions on the OOD inputs.
89"
MAIN IDEA,0.14506172839506173,"Moreover, there exist some polytopes that are not populated with training data. These unpopulated
90"
MAIN IDEA,0.14660493827160495,"polytopes serve to interpolate between the training sample points. If we replace the affine activation
91"
MAIN IDEA,0.14814814814814814,"function of the populated polytopes with Gaussian kernels and prune the unpopulated ones, the tail of
92"
MAIN IDEA,0.14969135802469136,"the kernel will help interpolate between the training sample points while assigning lower likelihood to
93"
MAIN IDEA,0.15123456790123457,"the low density or unpopulated polytope regions of the feature space. This results in better confidence
94"
MAIN IDEA,0.1527777777777778,"calibration for the proposed modified approach.
95"
PROPOSED APPROACH,0.15432098765432098,"3.3
Proposed Approach
96"
PROPOSED APPROACH,0.1558641975308642,"We will call the above discriminative approaches as the ‘parent approach’ hereafter. Consider the
97"
PROPOSED APPROACH,0.1574074074074074,"collection of polytope indices P from the parent approach which are populated by the training data.
98"
PROPOSED APPROACH,0.15895061728395063,"We replace the affine functions over the populated polytopes with Gaussian kernels G(·; ˆµr, ˆΣr). For
99"
PROPOSED APPROACH,0.16049382716049382,"a particular inference point x, we consider the Gaussian kernel with the minimum distance from the
100"
PROPOSED APPROACH,0.16203703703703703,"center of the kernel to the corresponding point:
101"
PROPOSED APPROACH,0.16358024691358025,"r∗
x = argmin
r
∥µr −x∥,
(4)"
PROPOSED APPROACH,0.16512345679012347,"where ∥· ∥denotes a distance. As we will show later, the type of distance metric considered in
102"
PROPOSED APPROACH,0.16666666666666666,"Equation 4 highly impacts the performance of the proposed model. In short, we modify Equation 3
103"
PROPOSED APPROACH,0.16820987654320987,"from the parent ReLU-net or random forest to estimate the class-conditional density (unnormalized):
104"
PROPOSED APPROACH,0.1697530864197531,˜fy(x) = 1 ny X
PROPOSED APPROACH,0.1712962962962963,"r∈P
nryG(x; µr, Σr)1(r = r∗
x),
(5)"
PROPOSED APPROACH,0.1728395061728395,"where ny is the total number of samples with label y and nry is the number of samples from class y
105"
PROPOSED APPROACH,0.1743827160493827,"that end up in polytope Qr. We add a small constant to the class conditional density ˜fy:
106"
PROPOSED APPROACH,0.17592592592592593,"ˆfy(x) = ˜fy(x) +
b
log(n).
(6)"
PROPOSED APPROACH,0.17746913580246915,"Note that in Equation 6,
b
log(n) →0 as the total training points, n →∞. The intuition behind the
107"
PROPOSED APPROACH,0.17901234567901234,"added constant will be clarified further later in Proposition 2. The confidence score ˆgy(x) for class y
108"
PROPOSED APPROACH,0.18055555555555555,"given a test point x is estimated using the Bayes rule as:
109"
PROPOSED APPROACH,0.18209876543209877,"ˆgy(x) =
ˆfy(x) ˆPY (y)
PK
k=1 ˆfk(x) ˆPY (k)
,
(7)"
PROPOSED APPROACH,0.18364197530864199,"where ˆPY (y) is the empirical prior probability of class y estimated from the training data. We
110"
PROPOSED APPROACH,0.18518518518518517,"estimate the class for a particular inference point x as:
111"
PROPOSED APPROACH,0.1867283950617284,"ˆy = argmax
y∈Y
ˆgy(x).
(8)"
MODEL PARAMETER ESTIMATION,0.1882716049382716,"4
Model Parameter Estimation
112"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.18981481481481483,"4.1
Gaussian Kernel Parameter Estimation
113"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.19135802469135801,"We fit Gaussian kernel parameters to the samples that end up in the r-th polytope. We set the kernel
114"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.19290123456790123,"center along the d-th dimension:
115"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.19444444444444445,"ˆµd
r = 1 nr n
X"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.19598765432098766,"i=1
xd
i 1(xi ∈Qr),
(9)"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.19753086419753085,"where xd
i is the value of xi along the d-th dimension. We set the kernel variance along the d-th
116"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.19907407407407407,"dimension:
117"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.2006172839506173,"(ˆσd
r)2 = 1 nr
{ n
X"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.2021604938271605,"i=1
1(xi ∈Qr)(xd
i −ˆµd
r)2 + λ},
(10)"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.2037037037037037,"where λ is a small constant that prevents ˆσd
r from being 0. We constrain our estimated Gaussian
118"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.2052469135802469,"kernels to have diagonal covariance.
119"
SAMPLE SIZE RATIO ESTIMATION,0.20679012345679013,"4.2
Sample Size Ratio Estimation
120"
SAMPLE SIZE RATIO ESTIMATION,0.20833333333333334,"For a high dimensional dataset with low training sample size, the polytopes are sparsely populated
121"
SAMPLE SIZE RATIO ESTIMATION,0.20987654320987653,with training samples. For improving the estimate of the ratio nry
SAMPLE SIZE RATIO ESTIMATION,0.21141975308641975,"ny in Equation 5, we incorporate the
122"
SAMPLE SIZE RATIO ESTIMATION,0.21296296296296297,"samples from other polytopes Qs based on the similarity wrs between Qr and Qs as:
123 ˆnry"
SAMPLE SIZE RATIO ESTIMATION,0.21450617283950618,"ˆny
=
P"
SAMPLE SIZE RATIO ESTIMATION,0.21604938271604937,"s∈P
Pn
i=1 wrs1(xi ∈Qs)1(yi = y)
P r∈P
P"
SAMPLE SIZE RATIO ESTIMATION,0.2175925925925926,"s∈P
Pn
i=1 wrs1(xi ∈Qs)1(yi = y).
(11)"
SAMPLE SIZE RATIO ESTIMATION,0.2191358024691358,"As n →∞, the estimated weights wrs should satisfy the condition:
124"
SAMPLE SIZE RATIO ESTIMATION,0.22067901234567902,"wrs →
0,
if Qr ̸= Qs
1,
if Qr = Qs.
(12)"
SAMPLE SIZE RATIO ESTIMATION,0.2222222222222222,"For simplicity, we will describe the estimation procedure for wrs in the next sections. Note that if we
125"
SAMPLE SIZE RATIO ESTIMATION,0.22376543209876543,"satisfy Condition 12, then we have ˆnry"
SAMPLE SIZE RATIO ESTIMATION,0.22530864197530864,ˆny →nry
SAMPLE SIZE RATIO ESTIMATION,0.22685185185185186,"ny as n →∞. Therefore, we modify Equation 5 as:
126"
SAMPLE SIZE RATIO ESTIMATION,0.22839506172839505,ˆfy(x) = 1 ˆny X
SAMPLE SIZE RATIO ESTIMATION,0.22993827160493827,"r∈P
ˆnryG(x; ˆµr, ˆΣr)1(r = ˆr∗
x),
(13)"
SAMPLE SIZE RATIO ESTIMATION,0.23148148148148148,"where ˆr∗
x = argminr ∥ˆµr −x∥. Now we use ˆfy(x) estimated using (13) in Equation (6), (7) and (8),
127"
SAMPLE SIZE RATIO ESTIMATION,0.2330246913580247,"respectively. Below, we describe how we estimate wrs for KDF and KDN .
128"
FOREST KERNEL,0.2345679012345679,"4.3
Forest Kernel
129"
FOREST KERNEL,0.2361111111111111,"Consider T number of decision trees in a random forest trained on n iid training samples
130"
FOREST KERNEL,0.23765432098765432,"{(xi, yi)}n
i=1. Each tree t partitions the feature space into pt polytopes resulting in a set of polytopes:
131"
FOREST KERNEL,0.23919753086419754,"{{Qt,r}pt
r=1}T
t=1. The intersection of these polytopes gives a new set of polytopes {Qr}p
r=1 for the
132"
FOREST KERNEL,0.24074074074074073,"forest. For any two points x ∈Qr and x′ ∈Qs, we define the kernel K(r, s) as:
133"
FOREST KERNEL,0.24228395061728394,"K(r, s) = trs"
FOREST KERNEL,0.24382716049382716,"T ,
(14)"
FOREST KERNEL,0.24537037037037038,"where trs is the total number of trees, x and x′ end up in the same leaf node. Here, 0 ≤K(r, s) ≤1.
134"
FOREST KERNEL,0.24691358024691357,"If the two samples end up in the same leaf in all the trees, i.e., K(r, s) = 1, they belong to the same
135"
FOREST KERNEL,0.24845679012345678,"polytope, i.e. r = s. In short, K(r, s) is the fraction of total trees where the two samples follow the
136"
FOREST KERNEL,0.25,"same path from the root to a leaf node. We exponentiate K(r, s) so that Condition 12 is satisfied:
137"
FOREST KERNEL,0.2515432098765432,"wrs = K(r, s)k log n.
(15)"
FOREST KERNEL,0.25308641975308643,"We choose k using grid search on a hold-out dataset.
138"
NETWORK KERNEL,0.25462962962962965,"4.4
Network Kernel
139"
NETWORK KERNEL,0.25617283950617287,"Consider a fully connected L layer ReLU-net trained on n iid training samples {(xi, yi)}n
i=1. We
140"
NETWORK KERNEL,0.25771604938271603,"have the set of all nodes denoted by Nl at a particular layer l. We can randomly pick a node nl ∈Nl
141"
NETWORK KERNEL,0.25925925925925924,"at each layer l, and construct a sequence of nodes starting at the input layer and ending at the output
142"
NETWORK KERNEL,0.26080246913580246,"layer which we call an activation path: m = {nl ∈Nl}L
l=1. Note that there are N = ΠL
i=1|Nl|
143"
NETWORK KERNEL,0.2623456790123457,"possible activation paths for a sample in the ReLU-net. We index each path by a unique identifier
144"
NETWORK KERNEL,0.2638888888888889,"number z ∈N and construct a sequence of activation paths as: M = {mz}z=1,··· ,N. Therefore, M
145"
NETWORK KERNEL,0.2654320987654321,"contains all possible activation pathways from the input to the output of the network.
146"
NETWORK KERNEL,0.26697530864197533,"While pushing a training sample xi through the network, we define the activation from a ReLU unit
147"
NETWORK KERNEL,0.26851851851851855,"at any node as ‘1’ when it has positive output and ‘0’ otherwise. Therefore, the activation indicates
148"
NETWORK KERNEL,0.2700617283950617,"on which side of the affine function at each node the sample falls. The activation for all nodes in an
149"
NETWORK KERNEL,0.2716049382716049,"activation path mz for a particular sample creates an activation mode az ∈{0, 1}L. If we evaluate
150"
NETWORK KERNEL,0.27314814814814814,"the activation mode for all activation paths in M while pushing a sample through the network, we
151"
NETWORK KERNEL,0.27469135802469136,"get a sequence of activation modes: Ar = {ar
z}N
z=1. Here r is the index of the polytope where the
152"
NETWORK KERNEL,0.2762345679012346,"sample falls in.
153"
NETWORK KERNEL,0.2777777777777778,"If the two sequences of activation modes for two different training samples are identical, they belong
154"
NETWORK KERNEL,0.279320987654321,"to the same polytope. In other words, if Ar = As, then Qr = Qs. This statement holds because the
155"
NETWORK KERNEL,0.2808641975308642,"above samples will lie on the same side of the affine function at each node in different layers of the
156"
NETWORK KERNEL,0.2824074074074074,"network. Now, we define the kernel K(r, s) as:
157"
NETWORK KERNEL,0.2839506172839506,"K(r, s) =
PN
z=1 1(ar
z = as
z)
N
.
(16)"
NETWORK KERNEL,0.2854938271604938,"Note that 0 ≤K(r, s) ≤1. In short, K(r, s) is the fraction of total activation paths which are
158"
NETWORK KERNEL,0.28703703703703703,"identically activated for two samples in two different polytopes r and s. We exponentiate the kernel
159"
NETWORK KERNEL,0.28858024691358025,"using Equation 15. Pseudocodes outlining the two algorithms are provided in Appendix D.
160"
GEODESIC DISTANCE,0.29012345679012347,"4.5
Geodesic Distance
161"
GEODESIC DISTANCE,0.2916666666666667,"Consider Pn = {Q1, Q2, · · · , Qp} as a partition of Rd given by a random forest or a ReLU-net after
162"
GEODESIC DISTANCE,0.2932098765432099,"being trained on n training samples. We measure distance between two points x ∈Qr, x′ ∈Qs
163"
GEODESIC DISTANCE,0.29475308641975306,"using the kernel introduced in Equation 14 and Equation 16, and call it ‘Geodesic’ distance [22]:
164"
GEODESIC DISTANCE,0.2962962962962963,"d(r, s) = −K(r, s) + 1"
GEODESIC DISTANCE,0.2978395061728395,"2(K(r, r) + K(s, s)) = 1 −K(r, s).
(17)"
GEODESIC DISTANCE,0.2993827160493827,"Proposition 1. (Pn, d) is a metric space.
165"
GEODESIC DISTANCE,0.30092592592592593,"Proof. See Appendix A.1 for the proof.
166"
GEODESIC DISTANCE,0.30246913580246915,"We use Geodesic distance to find the nearest polytope to the inference point. As Geodesic distance
167"
GEODESIC DISTANCE,0.30401234567901236,"cannot distinguish between points within the same polytope, it has a resolution similar to the size of
168"
GEODESIC DISTANCE,0.3055555555555556,"the polytope. For discriminating between two points within the same polytope, we fit a Gaussian
169"
GEODESIC DISTANCE,0.30709876543209874,"kernel within the polytope (described above). As hn →0, the resolution for Geodesic distance
170"
GEODESIC DISTANCE,0.30864197530864196,"improves. In Section 5, we will empirically show that using Geodesic distance scales better with
171"
GEODESIC DISTANCE,0.3101851851851852,"higher dimension compared to that of Euclidean distance.
172"
GEODESIC DISTANCE,0.3117283950617284,"Given n training samples {(xi, yi)}n
i=1, we define the distance of an inference point x from the
173"
GEODESIC DISTANCE,0.3132716049382716,"training points as: dx = mini=1,··· ,n ∥x −xi∥, where ∥· ∥denotes Euclidean distance.
174"
GEODESIC DISTANCE,0.3148148148148148,"Proposition 2 (Asymptotic OOD Convergence). Given non-zero and bounded bandwidth of the
175"
GEODESIC DISTANCE,0.31635802469135804,"Gaussians, then we have almost sure convergence for ˆgy as:
176"
GEODESIC DISTANCE,0.31790123456790126,"lim
dx→∞ˆgy(x) = ˆ
PY (y)."
GEODESIC DISTANCE,0.3194444444444444,"Proof. See Appendix A.2 for the proof.
177"
EMPIRICAL RESULTS,0.32098765432098764,"5
Empirical Results
178"
EMPIRICAL RESULTS,0.32253086419753085,"We conduct several experiments on simulated, OpenML-CC18 [23] 1 and vision benchmark datasets
179"
EMPIRICAL RESULTS,0.32407407407407407,"to gain insights on the finite sample performance of KDF and KDN. The details of the simulation
180"
EMPIRICAL RESULTS,0.3256172839506173,"datasets and hyperparameters used for all the experiments are provided in Appendix C. For Trunk
181"
EMPIRICAL RESULTS,0.3271604938271605,"simulation dataset, we follow the simulation setup proposed by Trunk [24] which was designed
182"
EMPIRICAL RESULTS,0.3287037037037037,"to demonstrate ‘curse of dimensionality’. In the Trunk simulation, a binary class dataset is used
183"
EMPIRICAL RESULTS,0.33024691358024694,"where each class is sampled from a Gaussian distribution with higher dimensions having increasingly
184"
EMPIRICAL RESULTS,0.3317901234567901,"less discriminative information. We use both Euclidean and Geodesic distance to detect the nearest
185"
EMPIRICAL RESULTS,0.3333333333333333,"polytope (see Equation (4)) on simulation datasets and use only Geodesic distance for benchmark
186"
EMPIRICAL RESULTS,0.33487654320987653,"datasets. For the simulation setups, we use classification error, Hellinger distance [25, 26] from
187"
EMPIRICAL RESULTS,0.33641975308641975,"the true class conditional posteriors and mean max confidence [4] as performance statistics. While
188"
EMPIRICAL RESULTS,0.33796296296296297,"measuring in-distribution calibration for the datasets in OpenML-CC18 data suite, we used maximum
189"
EMPIRICAL RESULTS,0.3395061728395062,"calibration error as defined by Guo et al. [18] with a fixed bin number of R = 15 across all the datasets.
190"
EMPIRICAL RESULTS,0.3410493827160494,"Given n OOD samples, we define OOD calibration error (OCE) to measure OOD performance for
191"
EMPIRICAL RESULTS,0.3425925925925926,"the benchmark datasets as:
192"
EMPIRICAL RESULTS,0.3441358024691358,"OCE = 1 n n
X i=1"
EMPIRICAL RESULTS,0.345679012345679,"max
y∈Y ( ˆPY |X(y|xi)) −max
y∈Y ( ˆPY (y))
 .
(18)"
EMPIRICAL RESULTS,0.3472222222222222,"For the tabular and the vision datasets, we have used ID calibration approaches, such as Isotonic
193"
EMPIRICAL RESULTS,0.3487654320987654,"[15, 16] and Sigmoid [17] regression, as baselines. Additionally, for the vision benchmark dataset,
194"
EMPIRICAL RESULTS,0.35030864197530864,"we provide results with OOD calibration approaches such as: ACET [4], ODIN [6], OE (outlier exposure)
195"
EMPIRICAL RESULTS,0.35185185185185186,"[10]. For each approach, 70% of the training data was used to fit the model and the rest of the data
196"
EMPIRICAL RESULTS,0.3533950617283951,"was used to calibrate the model.
197"
EMPIRICAL STUDY ON TABULAR DATA,0.3549382716049383,"5.1
Empirical Study on Tabular Data
198"
SIMULATION STUDY,0.35648148148148145,"5.1.1
Simulation Study
199"
SIMULATION STUDY,0.35802469135802467,"Figure 1 leftmost column shows 10000 training samples with 5000 samples per class sampled within
200"
SIMULATION STUDY,0.3595679012345679,"the region [−1, 1] × [−1, 1] from the six simulation setups described in Appendix C. Therefore, the
201"
SIMULATION STUDY,0.3611111111111111,"empty annular region between [−1, 1]×[−1, 1] and [−2, 2]×[−2, 2] is the low density or OOD region
202"
SIMULATION STUDY,0.3626543209876543,"in Figure 1. Figure 1 quantifies the performance of the algorithms which are visually represented
203"
SIMULATION STUDY,0.36419753086419754,"in Appendix Figure 4. KDF and KDN maintain similar classification accuracy to those of their parent
204"
SIMULATION STUDY,0.36574074074074076,"algorithms. We measure hellinger distance from the true distribution for increasing training sample
205"
SIMULATION STUDY,0.36728395061728397,"size within [−1, 1] × [−1, 1] region as a statistics for in-distribution calibration. Column 3 and 6 in
206"
SIMULATION STUDY,0.36882716049382713,"Figure 1 show KDF and KDN are better at estimating the ID region compared to their parent methods.
207"
SIMULATION STUDY,0.37037037037037035,"In all of the simulations, using geodesic distance measure results in better performance compared
208"
SIMULATION STUDY,0.37191358024691357,"to those while using Euclidean distance. For measuring OOD performance, we keep the training
209"
SIMULATION STUDY,0.3734567901234568,"sample size fixed at 1000 and normalize the training data by the maximum of their l2 norm so that
210"
SIMULATION STUDY,0.375,"the training data is confined within a unit circle. For inference, we sample 1000 inference points
211"
SIMULATION STUDY,0.3765432098765432,"uniformly from a circle where the circles have increasing radius and plot mean max posterior for
212"
SIMULATION STUDY,0.37808641975308643,"increasing distance from the origin. Therefore, for distance up to 1 we have in-distribution samples
213"
SIMULATION STUDY,0.37962962962962965,"and distances farther than 1 can be considered as OOD region. As shown in Column 4 and 7 of Figure
214"
SIMULATION STUDY,0.38117283950617287,"1, mean max confidence for KDF and KDN converge to the maximum of the class priors, i.e., 0.5 as we
215"
SIMULATION STUDY,0.38271604938271603,"go farther away from the training data origin.
216"
SIMULATION STUDY,0.38425925925925924,"Row 6 of Figure 1 shows KDF-Geodesic and KDN-Geodesic scale better with higher dimensions
217"
SIMULATION STUDY,0.38580246913580246,"compared to their Euclidean counterpart algorithms respectively.
218"
SIMULATION STUDY,0.3873456790123457,"5.1.2
OpenML-CC18 Data Study
219"
SIMULATION STUDY,0.3888888888888889,"We use OpenML-CC18 data suite for tabular benchmark dataset study. We exclude any dataset
220"
SIMULATION STUDY,0.3904320987654321,"which contains categorical features or NaN values 2 and conduct our experiments on 45 datasets with
221"
SIMULATION STUDY,0.39197530864197533,"varying dimensions and sample sizes. For the OOD experiments, we follow a similar setup as that
222"
SIMULATION STUDY,0.39351851851851855,"of the simulation data. We normalize the training data by their maximum l2 norm and sample 1000
223"
SIMULATION STUDY,0.3950617283950617,"1https://www.openml.org/s/99
2We also excluded the dataset with dataset id 23517 as we could not achieve better than chance accuracy
using RF and DN on that dataset. A. B."
SIMULATION STUDY,0.3966049382716049,"Figure 1: Simulation datasets, Classification error, Hellinger distance from true posteriors, mean
max confidence or posterior for A. five two-dimensional and B. a high dimensional (Trunk)
simulation experiments, visualized for the first two dimensions. The median performance is
shown as a dark curve with shaded region as error bars. 0.1 0.0 0.1"
SIMULATION STUDY,0.39814814814814814,Improvement over parent 0.5 0.0 1.0 0.2 0.0 0.2
SIMULATION STUDY,0.39969135802469136,"10
2
10
3
10
4 0.05 0.00 0.05"
SIMULATION STUDY,0.4012345679012346,Improvement over parent
SIMULATION STUDY,0.4027777777777778,"10
2
10
3
10
4
0.2 0.0 1.0"
SIMULATION STUDY,0.404320987654321,"1
3
5
0.2 0.0 0.8"
SIMULATION STUDY,0.4058641975308642,"Number of Training Samples (log)
Distance"
SIMULATION STUDY,0.4074074074074074,"Classification
ID Calibration
OOD Calibration"
SIMULATION STUDY,0.4089506172839506,"KDF
Isotonic RF
Sigmoid RF
Parent RF"
SIMULATION STUDY,0.4104938271604938,"KDN
Isotonic DN
Sigmoid DN
Parent DN"
SIMULATION STUDY,0.41203703703703703,"Figure 2: Performance summary of KDF and KDN on OpenML-CC18 data suite. The dark curve
in the middle shows the median of performance on 45 datasets with the shaded region as error bar."
SIMULATION STUDY,0.41358024691358025,"testing samples uniformly from hyperspheres where each hypersphere has increasing radius starting
224"
SIMULATION STUDY,0.41512345679012347,"from 1 to 5. For each dataset, we measure improvement with respect to the parent algorithm:
225"
SIMULATION STUDY,0.4166666666666667,Ep −EM
SIMULATION STUDY,0.4182098765432099,"Ep
,
(19)"
SIMULATION STUDY,0.41975308641975306,"A. Circle (ID)
B. Rectange (ID)
C. Ellipse (OOD)"
SIMULATION STUDY,0.4212962962962963,"0
127
255
Color Intensity 0.5 0.8 1.0"
SIMULATION STUDY,0.4228395061728395,Mean Max Conf. D.
SIMULATION STUDY,0.4243827160493827,"KDN
CNN"
SIMULATION STUDY,0.42592592592592593,"0.0
0.5
1.0 E."
SIMULATION STUDY,0.42746913580246915,"0
10
40
Axis Length F."
SIMULATION STUDY,0.42901234567901236,"Figure 3: KDN filters out inference points with different kinds of semantic shifts from the training
data. Simulated images: (A) circle with radius 10, (B) rectangle with sides (20, 50) and out-of-
distribution test points: (C) ellipse with minor and major axis (10, 30). Mean max confidence of KDN
are plotted for semantic shift of the inference points created by (D) changing the color intensity, (E)
taking convex combination of circle and rectangle, (F) changing one of the axes of the ellipse."
SIMULATION STUDY,0.4305555555555556,"where Ep =classification error, MCE or OCE for the parent algorithm and EM represents the perfor-
226"
SIMULATION STUDY,0.43209876543209874,"mance of the approach in consideration. Note that positive improvement implies the corresponding
227"
SIMULATION STUDY,0.43364197530864196,"approach performs better than the parent approach. We report the median of improvement on dif-
228"
SIMULATION STUDY,0.4351851851851852,"ferent datasets along with the error bar in Figure 2. The extended results for each dataset is shown
229"
SIMULATION STUDY,0.4367283950617284,"separately in the appendix. Figure 2 left column shows on average KDF and KDN has nearly similar
230"
SIMULATION STUDY,0.4382716049382716,"or better classification accuracy compared to their respective parent algorithm whereas Isotonic
231"
SIMULATION STUDY,0.4398148148148148,"and Sigmoid regression have lower classification accuracy most of the cases. However, according
232"
SIMULATION STUDY,0.44135802469135804,"to Figure 2 middle column, KDF and KDN have similar in-distribution calibration performance to
233"
SIMULATION STUDY,0.44290123456790126,"the other baseline approaches. Most interestingly, Figure 2 right column shows that KDN and KDF
234"
SIMULATION STUDY,0.4444444444444444,"improves OOD calibration of their respective parent algorithms by a huge margin while the baseline
235"
SIMULATION STUDY,0.44598765432098764,"approaches completely fails to address the OOD calibration problem.
236"
EMPIRICAL STUDY ON VISION DATA,0.44753086419753085,"5.2
Empirical Study on Vision Data
237"
EMPIRICAL STUDY ON VISION DATA,0.44907407407407407,"In vision data, each image pixel contains local information about the neighboring pixels. To extract
238"
EMPIRICAL STUDY ON VISION DATA,0.4506172839506173,"the local information, we use convolutional or vision transformer encoders at the front-end. More
239"
EMPIRICAL STUDY ON VISION DATA,0.4521604938271605,"precisely, we have a front-end encoder, he : RD 7→Rm and typically, m << D. After the
240"
EMPIRICAL STUDY ON VISION DATA,0.4537037037037037,"encoder there is a few fully connected dense layers for discriminating among the K class labels,
241"
EMPIRICAL STUDY ON VISION DATA,0.45524691358024694,"hf : Rm 7→RK. Note that the m-dimensional embedding outputs from the encoder are partitioned
242"
EMPIRICAL STUDY ON VISION DATA,0.4567901234567901,"into polytopes by the dense layers (see Equation (3)) and we fit a KDN on the embedding outputs. The
243"
EMPIRICAL STUDY ON VISION DATA,0.4583333333333333,"above approach results in extraction of better inductive bias by KDN from the parent model and makes
244"
EMPIRICAL STUDY ON VISION DATA,0.45987654320987653,"KDN more scalable with larger parent models and training sample size.
245"
SIMULATION STUDY,0.46141975308641975,"5.2.1
Simulation Study
246"
SIMULATION STUDY,0.46296296296296297,"For the simulation study, we use a simple CNN with one convolutional layer (3 channels with 3 × 3
247"
SIMULATION STUDY,0.4645061728395062,"kernel) followed by two fully connected layers with 10 and 2 nodes in each. We train the CNN on
248"
SIMULATION STUDY,0.4660493827160494,"2000 circle (radius 10) and 2000 rectangle (sides 20, 50) images with their RGB values being fixed at
249"
SIMULATION STUDY,0.4675925925925926,"[127, 127, 127] and their centers randomly sampled within a square with sides 100. The other pixels
250"
SIMULATION STUDY,0.4691358024691358,"in the background where there is no object (circle, rectangle or ellipse) were set to 0.
251"
SIMULATION STUDY,0.470679012345679,"We perform three experiments while inducing semantic shifts in the inference points as shown in
252"
SIMULATION STUDY,0.4722222222222222,"Figure 3. In the first experiment, we randomly sampled data similar to the training points. However,
253"
SIMULATION STUDY,0.4737654320987654,"we added the same shift to all the RGB values of an inference point (shown as color intensity in
254"
SIMULATION STUDY,0.47530864197530864,"Figure 3 D). Therefore, the inference point is ID for color intensity at 127 and otherwise OOD. In the
255"
SIMULATION STUDY,0.47685185185185186,"second experiment, we kept the RGB values fixed at [127, 127, 127] while taking convex combination
256"
SIMULATION STUDY,0.4783950617283951,"of a circle and a rectangle. Let images of circles and rectangles be denoted by Xc and Xr. We derive
257"
SIMULATION STUDY,0.4799382716049383,"an interference point as Xinf:
258"
SIMULATION STUDY,0.48148148148148145,"Xinf = ϵXc + (1 −ϵ)Xr
(20)
Therefore, Xinf is maximally distant from the training points for ϵ = 0.5 and closest to the ID points
259"
SIMULATION STUDY,0.48302469135802467,"at ϵ = {0, 1}. In the third experiment, we sampled ellipse images with the same RGB values as the
260"
SIMULATION STUDY,0.4845679012345679,"training points. However, this time we gradually change one of the ellipse axes from 0.01 to 40 while
261"
SIMULATION STUDY,0.4861111111111111,"keeping the other axis fixed at 10. As a result, the inference point becomes ID for the axis length of
262"
SIMULATION STUDY,0.4876543209876543,"10. As shown in Figure 3 (D, E, F), in all the experiments KDN becomes less confident for the OOD
263"
SIMULATION STUDY,0.48919753086419754,"points while the parent CNN remains overconfident throughout the semantic shifts of the test points.
264"
VISION BENCHMARK DATASETS STUDY,0.49074074074074076,"5.2.2
Vision Benchmark Datasets Study
265"
VISION BENCHMARK DATASETS STUDY,0.49228395061728397,"In this study, we use a V iT_B16 (provided in keras-vit package) vision transformer encoder [27]
266"
VISION BENCHMARK DATASETS STUDY,0.49382716049382713,"pretrained on ImageNet [28] dataset and finetuned on CIFAR-10 [29]. We use the same encoder for
267"
VISION BENCHMARK DATASETS STUDY,0.49537037037037035,"all the baseline algorithms and finetune it with the corresponding loss function without freezing any
268"
VISION BENCHMARK DATASETS STUDY,0.49691358024691357,"weight. As shown in Table 1, pretrained vision transformers are already well-calibrated for ID and
269"
VISION BENCHMARK DATASETS STUDY,0.4984567901234568,"the OOD approaches (ACET, ODIN, OE) degrade ID calibration of the parent model. On the contrary,
270"
VISION BENCHMARK DATASETS STUDY,0.5,"ID calibration approaches (Isotonic, Sigmoid) perform poorly compared to that of KDN in the
271"
VISION BENCHMARK DATASETS STUDY,0.5015432098765432,"OOD region. KDN achieves a compromise between ID and OOD performance while having reduced
272"
VISION BENCHMARK DATASETS STUDY,0.5030864197530864,"confidence on wrongly classified ID samples. The number of populated polytopes (and Gaussians)
273"
VISION BENCHMARK DATASETS STUDY,0.5046296296296297,"for KDN is 9323 ± 353. See Appendix F for the corresponding experiments using Resnet-50.
274"
VISION BENCHMARK DATASETS STUDY,0.5061728395061729,"Table 1: KDN achieves good calibration at both ID and OOD regions whereas other approaches
which excel either in the ID or the OOD region. Notably, KDN has reduced confidence on wrongly
classified ID points. ‘↑’ and ‘↓’ indicate whether higher and lower values are better, respectively.
MMC∗= Mean Max Confidence on wrongly classified ID points."
VISION BENCHMARK DATASETS STUDY,0.5077160493827161,"Dataset
Statistics
Parent
KDN
Isotonic
Sigmoid
ACET
ODIN
OE"
VISION BENCHMARK DATASETS STUDY,0.5092592592592593,"ID
CIFAR-10"
VISION BENCHMARK DATASETS STUDY,0.5108024691358025,"Accuracy(%) ↑
98.06 ± 0.00
97.45 ± 0.00
98.16 ± 0.00
98.10 ± 0.00
98.23 ± 0.00
97.97 ± 0.00
97.94 ± 0.00
MCE ↓
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.01 ± 0.00
0.02 ± 0.00
0.01 ± 0.00
MMC∗↓
0.76 ± 0.01
0.65 ± 0.08
0.74 ± 0.02
0.90 ± 0.01
0.86 ± 0.02
0.97 ± 0.01
0.69 ± 0.01 OOD"
VISION BENCHMARK DATASETS STUDY,0.5123456790123457,"CIFAR-100
OCE ↓
0.47 ± 0.01
0.12 ± 0.01
0.47 ± 0.01
0.69 ± 0.01
0.57 ± 0.01
0.79 ± 0.00
0.29 ± 0.01
SVHN
OCE ↓
0.44 ± 0.06
0.08 ± 0.02
0.34 ± 0.12
0.64 ± 0.16
0.47 ± 0.04
0.75 ± 0.03
0.11 ± 0.02
Noise
OCE ↓
0.28 ± 0.08
0.03 ± 0.02
0.30 ± 0.04
0.56 ± 0.12
0.01 ± 0.00
0.53 ± 0.09
0.07 ± 0.02"
LIMITATIONS,0.5138888888888888,"6
Limitations
275"
LIMITATIONS,0.5154320987654321,"Training time complexity for KDF and KDN is O(n2lf) which is dominated by the Geodesic distance
276"
LIMITATIONS,0.5169753086419753,"calculation. Here lf = total number of leaves in the forest or total nodes in the dense layers of the
277"
LIMITATIONS,0.5185185185185185,"network and n = total training samples. However, the distance calculation can be done in parallel
278"
LIMITATIONS,0.5200617283950617,"using our provided code. Additionally, note that the number of Gaussian kernel used by KDN is
279"
LIMITATIONS,0.5216049382716049,"upper bounded by number of training samples. Therefore, KDN may not scale for really big datasets
280"
LIMITATIONS,0.5231481481481481,"like ImageNet [28]. However, the scaling issue may be solved by selectively pruning neighboring
281"
LIMITATIONS,0.5246913580246914,"polytopes which we will pursue in future.
282"
DISCUSSION,0.5262345679012346,"7
Discussion
283"
DISCUSSION,0.5277777777777778,"In this paper, we demonstrated a simple intuition that renders traditional deep discriminative models
284"
DISCUSSION,0.529320987654321,"into a type of binning and kerneling approach. The bin boundaries are determined by the internal
285"
DISCUSSION,0.5308641975308642,"structure learned by the parent approach and Geodesic distance encodes the low dimensional structure
286"
DISCUSSION,0.5324074074074074,"learned by the model. Moreover, Geodesic distance introduced in this paper may have broader impact
287"
DISCUSSION,0.5339506172839507,"on understanding the internal structure of the deep discriminative models which we will pursue in
288"
DISCUSSION,0.5354938271604939,"future. Our code, including the package and the experiments in this manuscript, will be made publicly
289"
DISCUSSION,0.5370370370370371,"available upon acceptance of the paper.
290"
REFERENCES,0.5385802469135802,"References
291"
REFERENCES,0.5401234567901234,"[1] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
292"
REFERENCES,0.5416666666666666,"networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International
293"
REFERENCES,0.5432098765432098,"Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
294"
REFERENCES,0.5447530864197531,"pages 1321–1330. PMLR, 06–11 Aug 2017.
295"
REFERENCES,0.5462962962962963,"[2] Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being bayesian, even just a bit, fixes
296"
REFERENCES,0.5478395061728395,"overconfidence in ReLU networks. In Hal Daumé III and Aarti Singh, editors, Proceedings
297"
REFERENCES,0.5493827160493827,"of the 37th International Conference on Machine Learning, volume 119 of Proceedings of
298"
REFERENCES,0.5509259259259259,"Machine Learning Research, pages 5436–5446. PMLR, 13–18 Jul 2020.
299"
REFERENCES,0.5524691358024691,"[3] Haoyin Xu, Kaleab A. Kinfu, Will LeVine, Sambit Panda, Jayanta Dey, Michael Ainsworth,
300"
REFERENCES,0.5540123456790124,"Yu-Chung Peng, Madi Kusmanov, Florian Engert, Christopher M. White, Joshua T. Vogelstein,
301"
REFERENCES,0.5555555555555556,"and Carey E. Priebe. When are Deep Networks really better than Decision Forests at small
302"
REFERENCES,0.5570987654320988,"sample sizes, and how? arXiv preprint arXiv:2108.13637, 2021.
303"
REFERENCES,0.558641975308642,"[4] Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield
304"
REFERENCES,0.5601851851851852,"high-confidence predictions far away from the training data and how to mitigate the problem. In
305"
REFERENCES,0.5617283950617284,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
306"
REFERENCES,0.5632716049382716,"41–50, 2019.
307"
REFERENCES,0.5648148148148148,"[5] Alexander Meinke, Julian Bitterwolf, and Matthias Hein. Provably robust detection of out-of-
308"
REFERENCES,0.566358024691358,"distribution data (almost) for free. arXiv preprint arXiv:2106.04260, 2021.
309"
REFERENCES,0.5679012345679012,"[6] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant.
Enhancing the reliability of out-of-
310"
REFERENCES,0.5694444444444444,"distribution image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.
311"
REFERENCES,0.5709876543209876,"[7] Jay Nandy, Wynne Hsu, and Mong Li Lee. Towards maximizing the representation gap between
312"
REFERENCES,0.5725308641975309,"in-domain & out-of-distribution examples. Advances in Neural Information Processing Systems,
313"
REFERENCES,0.5740740740740741,"33:9239–9250, 2020.
314"
REFERENCES,0.5756172839506173,"[8] Weitao Wan, Yuanyi Zhong, Tianpeng Li, and Jiansheng Chen. Rethinking feature distribution
315"
REFERENCES,0.5771604938271605,"for loss functions in image classification. In Proceedings of the IEEE conference on computer
316"
REFERENCES,0.5787037037037037,"vision and pattern recognition, pages 9117–9126, 2018.
317"
REFERENCES,0.5802469135802469,"[9] Terrance DeVries and Graham W Taylor. Learning confidence for out-of-distribution detection
318"
REFERENCES,0.5817901234567902,"in neural networks. arXiv preprint arXiv:1802.04865, 2018.
319"
REFERENCES,0.5833333333333334,"[10] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier
320"
REFERENCES,0.5848765432098766,"exposure. arXiv preprint arXiv:1812.04606, 2018.
321"
REFERENCES,0.5864197530864198,"[11] Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon,
322"
REFERENCES,0.5879629629629629,"and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. Advances in
323"
REFERENCES,0.5895061728395061,"neural information processing systems, 32, 2019.
324"
REFERENCES,0.5910493827160493,"[12] Diederik P Kingma, Max Welling, et al. An introduction to variational autoencoders. Founda-
325"
REFERENCES,0.5925925925925926,"tions and Trends® in Machine Learning, 12(4):307–392, 2019.
326"
REFERENCES,0.5941358024691358,"[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
327"
REFERENCES,0.595679012345679,"Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications
328"
REFERENCES,0.5972222222222222,"of the ACM, 63(11):139–144, 2020.
329"
REFERENCES,0.5987654320987654,"[14] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan.
330"
REFERENCES,0.6003086419753086,"Do deep generative models know what they don’t know? arXiv preprint arXiv:1810.09136,
331"
REFERENCES,0.6018518518518519,"2018.
332"
REFERENCES,0.6033950617283951,"[15] Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision
333"
REFERENCES,0.6049382716049383,"trees and naive bayesian classifiers. In Icml, volume 1, pages 609–616, 2001.
334"
REFERENCES,0.6064814814814815,"[16] R Caruana. Predicting good probabilities with supervised learning. In Proceedings of NIPS
335"
REFERENCES,0.6080246913580247,"2004 Workshop on Calibration and Probabilistic Prediction in Supervised Learning, 2004.
336"
REFERENCES,0.6095679012345679,"[17] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized
337"
REFERENCES,0.6111111111111112,"likelihood methods. Advances in large margin classifiers, 10(3):61–74, 1999.
338"
REFERENCES,0.6126543209876543,"[18] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
339"
REFERENCES,0.6141975308641975,"networks. In International conference on machine learning, pages 1321–1330. PMLR, 2017.
340"
REFERENCES,0.6157407407407407,"[19] Richard Guo, Ronak Mehta, Jesus Arroyo, Hayden Helm, Cencheng Shen, and Joshua T
341"
REFERENCES,0.6172839506172839,"Vogelstein. Estimating information-theoretic quantities with uncertainty forests. arXiv, pages
342"
REFERENCES,0.6188271604938271,"arXiv–1907, 2019.
343"
REFERENCES,0.6203703703703703,"[20] Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song, and Peter
344"
REFERENCES,0.6219135802469136,"Flach. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with
345"
REFERENCES,0.6234567901234568,"dirichlet calibration. Advances in neural information processing systems, 32, 2019.
346"
REFERENCES,0.625,"[21] Haoyin Xu, Kaleab A Kinfu, Will LeVine, Sambit Panda, Jayanta Dey, Michael Ainsworth,
347"
REFERENCES,0.6265432098765432,"Yu-Chung Peng, Madi Kusmanov, Florian Engert, Christopher M White, et al. When are deep
348"
REFERENCES,0.6280864197530864,"networks really better than decision forests at small sample sizes, and how? arXiv preprint
349"
REFERENCES,0.6296296296296297,"arXiv:2108.13637, 2021.
350"
REFERENCES,0.6311728395061729,"[22] Bernhard Schölkopf. The kernel trick for distances. Advances in neural information processing
351"
REFERENCES,0.6327160493827161,"systems, 13, 2000.
352"
REFERENCES,0.6342592592592593,"[23] Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Pieter Gijsbers, Frank Hutter, Michel
353"
REFERENCES,0.6358024691358025,"Lang, Rafael G Mantovani, Jan N van Rijn, and Joaquin Vanschoren. Openml benchmarking
354"
REFERENCES,0.6373456790123457,"suites. arXiv preprint arXiv:1708.03731, 2017.
355"
REFERENCES,0.6388888888888888,"[24] Gerard V Trunk. A problem of dimensionality: A simple example. IEEE Transactions on
356"
REFERENCES,0.6404320987654321,"pattern analysis and machine intelligence, (3):306–307, 1979.
357"
REFERENCES,0.6419753086419753,"[25] Thomas Kailath. The divergence and bhattacharyya distance measures in signal selection. IEEE
358"
REFERENCES,0.6435185185185185,"transactions on communication technology, 15(1):52–60, 1967.
359"
REFERENCES,0.6450617283950617,"[26] C Radhakrishna Rao. A review of canonical coordinates and an alternative to correspondence
360"
REFERENCES,0.6466049382716049,"analysis using hellinger distance. Qüestiió: quaderns d’estadística i investigació operativa,
361"
REFERENCES,0.6481481481481481,"1995.
362"
REFERENCES,0.6496913580246914,"[27] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
363"
REFERENCES,0.6512345679012346,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
364"
REFERENCES,0.6527777777777778,"An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
365"
REFERENCES,0.654320987654321,"arXiv:2010.11929, 2020.
366"
REFERENCES,0.6558641975308642,"[28] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
367"
REFERENCES,0.6574074074074074,"scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern
368"
REFERENCES,0.6589506172839507,"Recognition, pages 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.
369"
REFERENCES,0.6604938271604939,"[29] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced
370"
REFERENCES,0.6620370370370371,"research). URL http://www.cs.toronto.edu/~kriz/cifar.html.
371"
REFERENCES,0.6635802469135802,"[30] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
372"
REFERENCES,0.6651234567901234,"Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural
373"
REFERENCES,0.6666666666666666,"information processing systems, 33:18661–18673, 2020.
374"
REFERENCES,0.6682098765432098,"A
Proofs
375"
REFERENCES,0.6697530864197531,"A.1
Proof of Proposition 1
376"
REFERENCES,0.6712962962962963,"For proving that d is a valid distance metric for Pn, we need to prove the following four statements:
377"
REFERENCES,0.6728395061728395,"1. d(r, s) = 0 when r = s.
378"
REFERENCES,0.6743827160493827,"Proof: By definition, K(r, s) = 1 and d(r, s) = 0 when r = s.
379"
REFERENCES,0.6759259259259259,"2. d(r, s) > 0 when r ̸= s.
380"
REFERENCES,0.6774691358024691,"Proof: By definition, 0 ≤K(r, s) < 1 and d(r, s) > 0 for r ̸= s.
381"
REFERENCES,0.6790123456790124,"3. d is symmetric, i.e., d(r, s) = d(s, r).
382"
REFERENCES,0.6805555555555556,"Proof: By definition, K(r, s) = K(s, r) which implies d(r, s) = d(s, r).
383"
REFERENCES,0.6820987654320988,"4. d follows the triangle inequality, i.e., for any three polytopes Qr, Qs, Qt ∈Pn: d(r, t) ≤
384"
REFERENCES,0.683641975308642,"d(r, s) + d(s, t).
385"
REFERENCES,0.6851851851851852,"Proof: Let Ar denote the set of activation modes in a ReLU-net and the set of leaf nodes
386"
REFERENCES,0.6867283950617284,"in a random forest for a particular polytope r. N is the total number of possible activation
387"
REFERENCES,0.6882716049382716,"paths in a ReLU-net or total trees in a random forest. Below c(·) denotes the cardinality of
388"
REFERENCES,0.6898148148148148,"the set. We can write:
389"
REFERENCES,0.691358024691358,"N ≥c((Ar ∩As) ∪(As ∩At))
(21)
= c(Ar ∩As) + c(As ∩At) −c(Ar ∩As ∩At)
≥c(Ar ∩As) + c(As ∩At) −c(Ar ∩At)."
REFERENCES,0.6929012345679012,"Rearranging the above equation, we get:
390"
REFERENCES,0.6944444444444444,N −c(Ar ∩At) ≤N −c(Ar ∩As) + N −c(As ∩At)
REFERENCES,0.6959876543209876,=⇒1 −c(Ar ∩At)
REFERENCES,0.6975308641975309,"N
≤1 −c(Ar ∩As) N
+ 1"
REFERENCES,0.6990740740740741,−c(As ∩At)
REFERENCES,0.7006172839506173,"N
=⇒d(r, t) ≤d(r, s) + d(s, t).
(22)"
REFERENCES,0.7021604938271605,"A.2
Proof of Proposition 2
391"
REFERENCES,0.7037037037037037,"Note that first we find the nearest polytope to the inference point x using Geodesic distance and use
392"
REFERENCES,0.7052469135802469,"Gaussian kernel locally for x within that polytope. Here the Gaussian kernel uses Euclidean distance
393"
REFERENCES,0.7067901234567902,"from the kernel center to x (within the numerator of the exponent). The value out of the Gaussian
394"
REFERENCES,0.7083333333333334,"kernel decays exponentially with the increasing distance of the inference point from the kernel center.
395"
REFERENCES,0.7098765432098766,"We first expand ˆgy(x):
396"
REFERENCES,0.7114197530864198,"ˆgy(x) =
ˆfy(x) ˆPY (y)
PK
k=1 ˆfk(x) ˆPY (k)"
REFERENCES,0.7129629629629629,"=
˜fy(x) ˆPY (y) +
b
log(n) ˆPY (y)
PK
k=1( ˆfk(x) ˆPY (k) +
b
log(n) ˆPY (k))"
REFERENCES,0.7145061728395061,"As the inference point x becomes more distant from training samples (and more distant from all of
the Gaussian centers), we have that G(x, ˆµr, ˆΣr) becomes smaller. Thus, ∀y, ˜fy(x) shrinks. More
formally, ∀y,"
REFERENCES,0.7160493827160493,"lim
dx→∞
˜fy(x) = 0."
REFERENCES,0.7175925925925926,"We can use this result to then examine the limiting behavior of our posteriors as the inference point x
397"
REFERENCES,0.7191358024691358,"becomes more distant from the training data:
398"
REFERENCES,0.720679012345679,"lim
dx→∞ˆgy(x) =
lim
dx→∞"
REFERENCES,0.7222222222222222,"˜fy(x) ˆPY (y) +
b
log(n) ˆPY (y)
PK
k=1( ˜fk(x) ˆPY (k) +
b
log(n) ˆPY (k))"
REFERENCES,0.7237654320987654,"=
(limdx→∞˜fy(x)) ˆPY (y) +
b
log(n) ˆPY (y)
PK
k=1(limdx→∞˜fk(x)) ˆPY (k) +
b
log(n) ˆPY (k))"
REFERENCES,0.7253086419753086,"=
ˆPY (y)
PK
k=1 ˆPY (k)"
REFERENCES,0.7268518518518519,= ˆPY (y).
REFERENCES,0.7283950617283951,"B
Hardware and Software Configurations
399"
REFERENCES,0.7299382716049383,"• Operating System: Linux (ubuntu 20.04), macOS (Ventura 13.2.1)
400"
REFERENCES,0.7314814814814815,"• VM Size: Azure Standard D96as v4 (96 vcpus, 384 GiB memory)
401"
REFERENCES,0.7330246913580247,"• GPU: Apple M1 Max
402"
REFERENCES,0.7345679012345679,"• Software: Python 3.8, scikit-learn ≥0.22.0, tensorflow-macos≤2.9, tensorflow-metal ≤
403"
REFERENCES,0.7361111111111112,"0.5.0.
404"
REFERENCES,0.7376543209876543,"C
Simulations
405"
REFERENCES,0.7391975308641975,"We construct six types of binary class simulations:
406"
REFERENCES,0.7407407407407407,"• Gaussian XOR is a two-class classification problem with equal class priors. Conditioned
407"
REFERENCES,0.7422839506172839,"on being in class 0, a sample is drawn from a mixture of two Gaussians with means
408"
REFERENCES,0.7438271604938271,"±[0.5, −0.5]⊤and standard deviations of 0.25. Conditioned on being in class 1, a sample is
409"
REFERENCES,0.7453703703703703,"drawn from a mixture of two Gaussians with means ±[0.5, −0.5]⊤and standard deviations
410"
REFERENCES,0.7469135802469136,"of 0.25.
411"
REFERENCES,0.7484567901234568,"• Spiral is a two-class classification problem with the following data distributions: let K
412"
REFERENCES,0.75,be the number of classes and S ∼multinomial( 1
REFERENCES,0.7515432098765432,"K⃗1K, n). Conditioned on S, each feature
413"
REFERENCES,0.7530864197530864,"vector is parameterized by two variables, the radius r and an angle θ. For each sample,
414"
REFERENCES,0.7546296296296297,"r is sampled uniformly in [0, 1]. Conditioned on a particular class, the angles are evenly
415"
REFERENCES,0.7561728395061729,spaced between 4π(k−1)tK
REFERENCES,0.7577160493827161,"K
and 4π(k)tK"
REFERENCES,0.7592592592592593,"K
, where tK controls the number of turns in the
416"
REFERENCES,0.7608024691358025,"spiral. To inject noise along the spirals, we add Gaussian noise to the evenly spaced angles
417"
REFERENCES,0.7623456790123457,"θ′ : θ = θ′ + N(0, 0.09). The observed feature vector is then (r cos(θ), r sin(θ)).
418"
REFERENCES,0.7638888888888888,"• Circle is a two-class classification problem with equal class priors. Conditioned on being
419"
REFERENCES,0.7654320987654321,"in class 0, a sample is drawn from a circle centered at (0, 0) with a radius of r = 0.75.
420"
REFERENCES,0.7669753086419753,"Conditioned on being in class 1, a sample is drawn from a circle centered at (0, 0) with a
421"
REFERENCES,0.7685185185185185,"radius of r = 1, which is cut off by the region bounds. To inject noise along the circles, we
422"
REFERENCES,0.7700617283950617,"add Gaussian noise to the circle radii r′ : r = r′ + N(0, 0.01).
423"
REFERENCES,0.7716049382716049,"• Sinewave is a two-class classification problem based on sine waves. Conditioned on being
424"
REFERENCES,0.7731481481481481,"in class 0, a sample is drawn from the distribution y = cos(πx). Conditioned on being in
425"
REFERENCES,0.7746913580246914,"class 1, a sample is drawn from the distribution y = sin(πx). We inject Gaussian noise to
426"
REFERENCES,0.7762345679012346,"the sine wave heights y′ : y = y′ + N(0, 0.01).
427"
REFERENCES,0.7777777777777778,"• Polynomial is a two-class classification problem with the following data distributions:
428"
REFERENCES,0.779320987654321,"y = xa. Conditioned on being in class 0, a sample is drawn from the distribution y = x1.
429"
REFERENCES,0.7808641975308642,"Conditioned on being in class 1, a sample is drawn from the distribution y = x3. Gaussian
430"
REFERENCES,0.7824074074074074,"noise is added to variables y′ : y = y′ + N(0, 0.01).
431"
REFERENCES,0.7839506172839507,"• Trunk is a two-class classification problem with gradually increasing dimension and equal
432"
REFERENCES,0.7854938271604939,"class priors. The class conditional probabilities are Gaussian:
433"
REFERENCES,0.7870370370370371,"P(X|Y = 0) = G(µ1, I),
P(X|Y = 1) = G(µ2, I),"
REFERENCES,0.7885802469135802,"Figure 4:
Visualization of true and estimated posteriors for class 0 from five binary class
simulation experiments. Column 1: 10,000 training points with 5,000 samples per class sampled
from 6 different simulation setups for binary class classification. Trunk simulation is shown for two
dimensional case. The class labels are indicated by yellow and blue colors. Column 2-8: True and
estimated class conditional posteriors from different approaches. The posteriors estimated from KDN
and KDF are better calibrated for both in- and out-of-distribution regions compared to those of their
parent algorithms."
REFERENCES,0.7901234567901234,"where µ1 = µ, µ2 = −µ, µ is a d dimensional vector whose i-th component is ( 1"
REFERENCES,0.7916666666666666,"i )1/2 and I
434"
REFERENCES,0.7932098765432098,"is d dimensional identity matrix.
435"
REFERENCES,0.7947530864197531,Table 2: Hyperparameters for RF and KDF.
REFERENCES,0.7962962962962963,"Hyperparameters
Value
n_estimators
500
max_depth
∞
min_samples_leaf
1
λ
1 × 10−6"
REFERENCES,0.7978395061728395,"b
exp (−10−7)"
REFERENCES,0.7993827160493827,"D
Pseudocodes
436"
REFERENCES,0.8009259259259259,"We provide the pseudocode for our porposed algorithms in Algorithm 1, 2 and 3.
437"
REFERENCES,0.8024691358024691,"E
Extended Results on OpenML-CC18 data suite
438"
REFERENCES,0.8040123456790124,"See Figure 5, 6, 7 and 8 for extended results on OpenML-CC18 data suite.
439"
REFERENCES,0.8055555555555556,"Algorithm 1 Fit a KDX model.
Input:"
REFERENCES,0.8070987654320988,"(1) θ
▷Parent learner (random forest or deep network model)
(2) Dn = (X, y) ∈Rn×d × {1, . . . , K}n
▷Training data
Output: G
▷a KDX model
1: function KGX.FIT(θ, X, y)
2:
for i = 1, . . . , n do
▷Iterate over the dataset to calculate the weights
3:
for j = 1, . . . , n do
4:
wij ←COMPUTEWEIGHTS(xi, xj, θ)
5:
end for
6:
end for
7:
8:
9:
{Qr, wrs}˜p
r=1 ←GETPOLYTOPES(w)
▷Identify the polytopes by clustering the samples
with similar weight
10:
11:
for r = 1, . . . , ˜p do
▷Iterate over each polytope
12:
G.ˆµr, G.ˆΣr, G.ˆnry ←ESTIMATEPARAMETERS(X, y, {wrs}˜p
s=1) ▷Fit Gaussians using
MLE
13:
end for
14:
return G
15: end function"
REFERENCES,0.808641975308642,"Algorithm 2 Computing weights in KDF
Input:"
REFERENCES,0.8101851851851852,"(1) xi, xj ∈R1×d
▷two input samples to be weighted
(2) θ
▷parent random forest with T trees
Output: wij ∈[0, 1]
▷compute similarity between i and j-th samples.
1: function COMPUTEWEIGHTS(xi, xj, θ)
2:
Ii ←PUSHDOWNTREES(xi, θ) ▷push xi down T trees and get the leaf numbers it end up
in.
3:
Ij ←PUSHDOWNTREES(xj, θ) ▷push xj down T trees and get the leaf numbers it end up
in.
4:
l ←COUNTMATCHES(Ii, Ij) ▷count the number of times the samples end up in the same
leaf
5:
wij ←l"
REFERENCES,0.8117283950617284,"T
6:
return wij
7: end function"
REFERENCES,0.8132716049382716,"Algorithm 3 Computing weights in KDN
Input:"
REFERENCES,0.8148148148148148,"(1) xi, xj ∈R1×d
▷two input samples to be weighted
(2) θ
▷parent deep-net model
Output: wij ∈[0, 1]
▷compute similarity between i and j-th samples.
1: function COMPUTEWEIGHTS(xi, xj, θ)
2:
Ai ←PUSHDOWNNETWORK(xi, θ)
▷get activation modes Ai
3:
Aj ←PUSHDOWNNETWORK(xj, θ)
▷get activation modes Aj
4:
l ←COUNTMATCHES(Ai, Aj)
▷count the number of times the two samples activate the
activation paths in a similar way
5:
wij ←
l
N
▷N is the total number of activation paths
6:
return wij
7: end function"
REFERENCES,0.816358024691358,Table 3: Hyperparameters for ReLU-net and KDNon Tabular data.
REFERENCES,0.8179012345679012,"Hyperparameters
Value
number of hidden layers
4
nodes per hidden layer
1000
optimizer
Adam
learning rate
3 × 10−4"
REFERENCES,0.8194444444444444,"λ
1 × 10−6"
REFERENCES,0.8209876543209876,"b
exp (−10−7)"
REFERENCES,0.8225308641975309,"Table 4: ID approaches (Sigmoid, Isotonic) are bad at OOD calibration and OOD approaches
(ACET, ODIN, OE) are bad at ID calibration. KDN bridges between both ID and OOD calibration
approaches. ‘↑’ and ‘↓’ indicate whether higher and lower values are better, respectively. Bolded
indicates most performant, or within the margin of error of the most performant."
REFERENCES,0.8240740740740741,"Dataset
Statistics
Parent
KDN
Isotonic
Sigmoid
ACET
ODIN
OE"
REFERENCES,0.8256172839506173,"ID
CIFAR-10"
REFERENCES,0.8271604938271605,"Accuracy(%) ↑
77.78 ± 0.00
76.84 ± 0.01
78.25 ± 0.00
76.93 ± 0.00
75.08 ± 0.03
78.00 ± 0.00
73.95 ± 0.00
MCE ↓
0.09 ± 0.00
0.04 ± 0.00
0.03 ± 0.01
0.10 ± 0.01
0.13 ± 0.00
0.09 ± 0.00
0.55 ± 0.00
MMC∗↓
0.47 ± 0.00
0.37 ± 0.01
0.54 ± 0.01
0.43 ± 0.01
0.69 ± 0.00
0.48 ± 0.01
0.13 ± 0.00 OOD"
REFERENCES,0.8287037037037037,"CIFAR-100
OCE ↓
0.30 ± 0.00
0.20 ± 0.01
0.37 ± 0.01
0.29 ± 0.01
0.55 ± 0.00
0.31 ± 0.00
0.01 ± 0.00
SVHN
OCE ↓
0.87 ± 0.00
0.01 ± 0.00
0.85 ± 0.00
0.69 ± 0.01
0.90 ± 0.00
0.87 ± 0.00
0.04 ± 0.01
Noise
OCE ↓
0.90 ± 0.00
0.00 ± 0.00
0.87 ± 0.00
0.71 ± 0.00
0.01 ± 0.01
0.06 ± 0.00
0.00 ± 0.00"
REFERENCES,0.8302469135802469,"F
Extended Results on Vision datasets using Resnet-50
440"
REFERENCES,0.8317901234567902,"In this experiments, we use a Resnet-50 encoder pretrained using contrastive loss [30] as described
441"
REFERENCES,0.8333333333333334,"in http://keras.io/examples/vision/supervised-contrastive-learning. The encoder
442"
REFERENCES,0.8348765432098766,"projects the input images down to a 256 dimensional latent space and we add two dense layers with
443"
REFERENCES,0.8364197530864198,"200 and 10 nodes on top of the encoder. We use the same pretrained encoder for all the baseline
444"
REFERENCES,0.8379629629629629,"algorithms.
445"
REFERENCES,0.8395061728395061,"As shown in Table 4, KDN achieves good calibration for both ID and OOD datasets whereas the ID
446"
REFERENCES,0.8410493827160493,"calibration approaches are poorly calibrated in the OOD regions and the OOD approaches have poor
447"
REFERENCES,0.8425925925925926,"ID calibration.
448"
REFERENCES,0.8441358024691358,"Figure 5: Extended results on OpenML-CC18 datasets. Left: Performance (classification error,
MCE and mean max confidence) of KDF on different Openml-CC18 datasets. Right: Performance
(classification error, MCE and mean max confidence) of KDN on different Openml-CC18 datasets."
REFERENCES,0.845679012345679,"Figure 6: Extended results on OpenML-CC18 datasets (continued). Left: Performance (classi-
fication error, MCE and mean max confidence) of KDF on different Openml-CC18 datasets. Right:
Performance (classification error, MCE and mean max confidence) of KDN on different Openml-CC18
datasets."
REFERENCES,0.8472222222222222,"Figure 7: Extended results on OpenML-CC18 datasets (continued). Left: Performance (classi-
fication error, MCE and mean max confidence) of KDF on different Openml-CC18 datasets. Right:
Performance (classification error, MCE and mean max confidence) of KDN on different Openml-CC18
datasets."
REFERENCES,0.8487654320987654,"Figure 8: Extended results on OpenML-CC18 datasets (continued). Left: Performance (classi-
fication error, MCE and mean max confidence) of KDF on different Openml-CC18 datasets. Right:
Performance (classification error, MCE and mean max confidence) of KDN on different Openml-CC18
datasets."
REFERENCES,0.8503086419753086,"NeurIPS Paper Checklist
449"
CLAIMS,0.8518518518518519,"1. Claims
450"
CLAIMS,0.8533950617283951,"Question: Do the main claims made in the abstract and introduction accurately reflect the
451"
CLAIMS,0.8549382716049383,"paper’s contributions and scope?
452"
CLAIMS,0.8564814814814815,"Answer: [Yes]
453"
CLAIMS,0.8580246913580247,"Justification: We enumerate list of contributions in Section 2 which are also mentioned in
454"
CLAIMS,0.8595679012345679,"the abstract and the introduction.
455"
LIMITATIONS,0.8611111111111112,"2. Limitations
456"
LIMITATIONS,0.8626543209876543,"Question: Does the paper discuss the limitations of the work performed by the authors?
457"
LIMITATIONS,0.8641975308641975,"Answer: [Yes]
458"
LIMITATIONS,0.8657407407407407,"Justification: We discussed the limitations of the work in Section 6 to the best of our
459"
LIMITATIONS,0.8672839506172839,"knowledge.
460"
THEORY ASSUMPTIONS AND PROOFS,0.8688271604938271,"3. Theory Assumptions and Proofs
461"
THEORY ASSUMPTIONS AND PROOFS,0.8703703703703703,"Question: For each theoretical result, does the paper provide the full set of assumptions and
462"
THEORY ASSUMPTIONS AND PROOFS,0.8719135802469136,"a complete (and correct) proof?
463"
THEORY ASSUMPTIONS AND PROOFS,0.8734567901234568,"Answer: [Yes]
464"
THEORY ASSUMPTIONS AND PROOFS,0.875,"Justification: All of our theoretical results are discussed in Section 4.5 and proofs are
465"
THEORY ASSUMPTIONS AND PROOFS,0.8765432098765432,"provided in the Appendix.
466"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8780864197530864,"4. Experimental Result Reproducibility
467"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8796296296296297,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
468"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8811728395061729,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
469"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8827160493827161,"of the paper (regardless of whether the code and data are provided or not)?
470"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8842592592592593,"Answer: [Yes]
471"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8858024691358025,"Justification:
472"
OPEN ACCESS TO DATA AND CODE,0.8873456790123457,"5. Open access to data and code
473"
OPEN ACCESS TO DATA AND CODE,0.8888888888888888,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
474"
OPEN ACCESS TO DATA AND CODE,0.8904320987654321,"tions to faithfully reproduce the main experimental results, as described in supplemental
475"
OPEN ACCESS TO DATA AND CODE,0.8919753086419753,"material?
476"
OPEN ACCESS TO DATA AND CODE,0.8935185185185185,"Answer: [Yes]
477"
OPEN ACCESS TO DATA AND CODE,0.8950617283950617,"Justification: All code that was used to create the experiments has been provided as a part of
478"
OPEN ACCESS TO DATA AND CODE,0.8966049382716049,"the supplementary material, and will be made public after the review process.
479"
OPEN ACCESS TO DATA AND CODE,0.8981481481481481,"6. Experimental Setting/Details
480"
OPEN ACCESS TO DATA AND CODE,0.8996913580246914,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
481"
OPEN ACCESS TO DATA AND CODE,0.9012345679012346,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
482"
OPEN ACCESS TO DATA AND CODE,0.9027777777777778,"results?
483"
OPEN ACCESS TO DATA AND CODE,0.904320987654321,"Answer: [Yes]
484"
OPEN ACCESS TO DATA AND CODE,0.9058641975308642,"Justification: Experimental setting are described adequately in the main paper with more
485"
OPEN ACCESS TO DATA AND CODE,0.9074074074074074,"details discussed in the Appendix.
486"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9089506172839507,"7. Experiment Statistical Significance
487"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9104938271604939,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
488"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9120370370370371,"information about the statistical significance of the experiments?
489"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9135802469135802,"Answer: [Yes]
490"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9151234567901234,"Justification: We have repeated the experiments over several Monte Carlo repetitions and
491"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9166666666666666,"have reported the error bars in the figures and the tables.
492"
EXPERIMENTS COMPUTE RESOURCES,0.9182098765432098,"8. Experiments Compute Resources
493"
EXPERIMENTS COMPUTE RESOURCES,0.9197530864197531,"Question: For each experiment, does the paper provide sufficient information on the com-
494"
EXPERIMENTS COMPUTE RESOURCES,0.9212962962962963,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
495"
EXPERIMENTS COMPUTE RESOURCES,0.9228395061728395,"the experiments?
496"
EXPERIMENTS COMPUTE RESOURCES,0.9243827160493827,"Answer: [Yes]
497"
EXPERIMENTS COMPUTE RESOURCES,0.9259259259259259,"Justification: We have provided the computation platform and resources used in the Appendix
498"
EXPERIMENTS COMPUTE RESOURCES,0.9274691358024691,"B.
499"
CODE OF ETHICS,0.9290123456790124,"9. Code Of Ethics
500"
CODE OF ETHICS,0.9305555555555556,"Question: Does the research conducted in the paper conform, in every respect, with the
501"
CODE OF ETHICS,0.9320987654320988,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
502"
CODE OF ETHICS,0.933641975308642,"Answer: [Yes]
503"
CODE OF ETHICS,0.9351851851851852,"Justification: We did not conduct experiments with human participants and we do not
504"
CODE OF ETHICS,0.9367283950617284,"anticipate any societal harmful consequences.
505"
BROADER IMPACTS,0.9382716049382716,"10. Broader Impacts
506"
BROADER IMPACTS,0.9398148148148148,"Question: Does the paper discuss both potential positive societal impacts and negative
507"
BROADER IMPACTS,0.941358024691358,"societal impacts of the work performed?
508"
BROADER IMPACTS,0.9429012345679012,"Answer: [NA]
509"
BROADER IMPACTS,0.9444444444444444,"Justification: This paper demonstrates a simple intuition that can increase the confidence
510"
BROADER IMPACTS,0.9459876543209876,"calibration for modern deep machine learning approaches. Although the proposed work is
511"
BROADER IMPACTS,0.9475308641975309,"not directly related to any societal impact, down the line we anticipate it will pave the way
512"
BROADER IMPACTS,0.9490740740740741,"for safety critical application of machine learning models.
513"
SAFEGUARDS,0.9506172839506173,"11. Safeguards
514"
SAFEGUARDS,0.9521604938271605,"Question: Does the paper describe safeguards that have been put in place for responsible
515"
SAFEGUARDS,0.9537037037037037,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
516"
SAFEGUARDS,0.9552469135802469,"image generators, or scraped datasets)?
517"
SAFEGUARDS,0.9567901234567902,"Answer: [NA]
518"
SAFEGUARDS,0.9583333333333334,"Justification: The paper poses no such risks.
519"
LICENSES FOR EXISTING ASSETS,0.9598765432098766,"12. Licenses for existing assets
520"
LICENSES FOR EXISTING ASSETS,0.9614197530864198,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
521"
LICENSES FOR EXISTING ASSETS,0.9629629629629629,"the paper, properly credited and are the license and terms of use explicitly mentioned and
522"
LICENSES FOR EXISTING ASSETS,0.9645061728395061,"properly respected?
523"
LICENSES FOR EXISTING ASSETS,0.9660493827160493,"Answer: [Yes]
524"
LICENSES FOR EXISTING ASSETS,0.9675925925925926,"Justification: We only use existing public datasets (appropriately cited in the paper) and
525"
LICENSES FOR EXISTING ASSETS,0.9691358024691358,"synthetic data. No new assets are created from this paper.
526"
NEW ASSETS,0.970679012345679,"13. New Assets
527"
NEW ASSETS,0.9722222222222222,"Question: Are new assets introduced in the paper well documented and is the documentation
528"
NEW ASSETS,0.9737654320987654,"provided alongside the assets?
529"
NEW ASSETS,0.9753086419753086,"Answer: [NA]
530"
NEW ASSETS,0.9768518518518519,"Justification: No assets are created in this paper.
531"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783950617283951,"14. Crowdsourcing and Research with Human Subjects
532"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9799382716049383,"Question: For crowdsourcing experiments and research with human subjects, does the paper
533"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9814814814814815,"include the full text of instructions given to participants and screenshots, if applicable, as
534"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830246913580247,"well as details about compensation (if any)?
535"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845679012345679,"Answer: [NA]
536"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9861111111111112,"Justification: No such study was performed.
537"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876543209876543,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
538"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891975308641975,"Subjects
539"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907407407407407,"Question: Does the paper describe potential risks incurred by study participants, whether
540"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922839506172839,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
541"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938271604938271,"approvals (or an equivalent approval/review based on the requirements of your country or
542"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9953703703703703,"institution) were obtained?
543"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969135802469136,"Answer: [NA]
544"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9984567901234568,"Justification: No such study was performed.
545"
