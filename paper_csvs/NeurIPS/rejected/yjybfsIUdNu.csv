Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016611295681063123,"In many computational science and engineering applications, the output of a
1"
ABSTRACT,0.0033222591362126247,"system of interest corresponding to a given input can be queried at different
2"
ABSTRACT,0.0049833887043189366,"levels of fidelity with different costs. Typically, low-fidelity data is cheap and
3"
ABSTRACT,0.006644518272425249,"abundant, while high-fidelity data is expensive and scarce. In this work we study
4"
ABSTRACT,0.008305647840531562,"the reinforcement learning (RL) problem in the presence of multiple environments
5"
ABSTRACT,0.009966777408637873,"with different levels of fidelity for a given control task. We focus on improving
6"
ABSTRACT,0.011627906976744186,"the RL agent’s performance with multifidelity data. Specifically, a multifidelity
7"
ABSTRACT,0.013289036544850499,"estimator that exploits the cross-correlations between the low- and high-fidelity
8"
ABSTRACT,0.014950166112956811,"returns is proposed to reduce the variance in the estimation of the state-action
9"
ABSTRACT,0.016611295681063124,"value function. The proposed estimator, which is based on the method of control
10"
ABSTRACT,0.018272425249169437,"variates, is used to design a multifidelity Monte Carlo RL (MFMCRL) algorithm that
11"
ABSTRACT,0.019933554817275746,"improves the learning of the agent in the high-fidelity environment. The impacts of
12"
ABSTRACT,0.02159468438538206,"variance reduction on policy evaluation and policy improvement are theoretically
13"
ABSTRACT,0.023255813953488372,"analyzed by using probability bounds. Our theoretical analysis and numerical
14"
ABSTRACT,0.024916943521594685,"experiments demonstrate that for a finite budget of high-fidelity data samples,
15"
ABSTRACT,0.026578073089700997,"our proposed MFMCRL agent attains superior performance compared with that of a
16"
ABSTRACT,0.02823920265780731,"standard RL agent that uses only the high-fidelity environment data for learning
17"
ABSTRACT,0.029900332225913623,"the optimal policy.
18"
INTRODUCTION,0.03156146179401993,"1
Introduction
19"
INTRODUCTION,0.03322259136212625,"Within the computational science and engineering (CSE) community, multifidelity data refers to
20"
INTRODUCTION,0.03488372093023256,"data that comes from different sources with different levels of fidelity. The criteria by which data is
21"
INTRODUCTION,0.036544850498338874,"considered to be low fidelity or high fidelity vary across different applications, but usually low-fidelity
22"
INTRODUCTION,0.03820598006644518,"data is much cheaper to generate than high-fidelity data under some cost metric. In robotics for
23"
INTRODUCTION,0.03986710963455149,"instance, data coming from a robot operating in the real world constitutes high-fidelity data, while
24"
INTRODUCTION,0.04152823920265781,"simulated data of the robot based on first principles is considered to be low-fidelity data. Different
25"
INTRODUCTION,0.04318936877076412,"simulators of the robot can also be designed by increasing the modeling complexity. A simulator
26"
INTRODUCTION,0.044850498338870434,"that takes into account aerodynamic drag is, for instance, of higher fidelity than one that is based
27"
INTRODUCTION,0.046511627906976744,"only on the simple laws of motion. As another example, a neural classifier in deep learning can be
28"
INTRODUCTION,0.04817275747508306,"trained on the full training data for a large number of training epochs, or on a subset of the training
29"
INTRODUCTION,0.04983388704318937,"data for few epochs. Evaluating the trained model on a held-out validation data set in the former
30"
INTRODUCTION,0.05149501661129568,"case yields a higher-fidelity estimate of the classifiers’ performance compared with that in the latter
31"
INTRODUCTION,0.053156146179401995,"case. In general, low-fidelity data serves as an approximation to its high-fidelity counterpart and
32"
INTRODUCTION,0.054817275747508304,"can be generated cheaply and abundantly [24]. Many outer-loop applications that require querying
33"
INTRODUCTION,0.05647840531561462,"the system at many different inputs, including black-box optimization [21], inference [29], and
34"
INTRODUCTION,0.05813953488372093,"uncertainty propagation [19, 27], can exploit the cross-correlations between low- and high-fidelity
35"
INTRODUCTION,0.059800664451827246,"data to solve new problems that would otherwise be prohibitively costly to solve using high-fidelity
36"
INTRODUCTION,0.061461794019933555,"data alone [28, 29].
37"
INTRODUCTION,0.06312292358803986,"Motivated by the advent of multifidelity data sources within CSE, in this work we study the rein-
38"
INTRODUCTION,0.06478405315614617,"forcement learning (RL) problem in the presence of multiple environments with different levels of
39"
INTRODUCTION,0.0664451827242525,"fidelity for a given control task. RL is a popular machine learning paradigm for intelligent sequential
40"
INTRODUCTION,0.0681063122923588,"decision-making under uncertainty, enabling data-driven control of complex systems with scales
41"
INTRODUCTION,0.06976744186046512,"ranging from quantum [18] to cosmological [26]. State-of-the-art model-free RL algorithms have
42"
INTRODUCTION,0.07142857142857142,"indeed demonstrated sheer success for learning complex policies from raw data in single-fidelity
43"
INTRODUCTION,0.07308970099667775,"environments [25, 22, 31, 32, 12]. This success, however, comes at the cost of requiring a large num-
44"
INTRODUCTION,0.07475083056478406,"ber of data samples to solve a control task satisfactorily.1 In the presence of multiple environments
45"
INTRODUCTION,0.07641196013289037,"with different levels of fidelity, new ways arise that could help the agent learn better policies. One
46"
INTRODUCTION,0.07807308970099668,"way that has been well studied in the context of RL is transfer learning (TL). In TL [35, 8, 39], the
47"
INTRODUCTION,0.07973421926910298,"agent first uses the low-fidelity environment to learn a policy that is then transferred (directly or
48"
INTRODUCTION,0.08139534883720931,"indirectly through the transfer of the state-action value function) to the high-fidelity environment
49"
INTRODUCTION,0.08305647840531562,"as a heuristic to bootstrap learning. Essentially, TL attempts to leverage multifidelity environments
50"
INTRODUCTION,0.08471760797342193,"to deal with the exploration-exploitation dilemma that is present within RL, and it works under the
51"
INTRODUCTION,0.08637873754152824,"assumption that the maximum deviation between the optimal low-fidelity state-action value function
52"
INTRODUCTION,0.08803986710963455,"and the optimal high-fidelity state-action value function is bounded with a threshold that is used
53"
INTRODUCTION,0.08970099667774087,"by TL for bootsrapping the high-fidelity value function [9]. In our work we explore an uncharted
54"
INTRODUCTION,0.09136212624584718,"territory and focus on multifidelity estimation in RL and its role in improving the learning of the
55"
INTRODUCTION,0.09302325581395349,"agent. We demonstrate that as long as the low- and high-fidelity state-action value functions for
56"
INTRODUCTION,0.0946843853820598,"any policy are correlated, significant performance improvements can be reaped by leveraging these
57"
INTRODUCTION,0.09634551495016612,"cross-correlations without extra effort in managing the exploration-exploitation process.
58"
INTRODUCTION,0.09800664451827243,"The main contributions of our work are summarized as follows. First, we study a generic multifidelity
59"
INTRODUCTION,0.09966777408637874,"setup in which the RL agent can execute a policy in two environments, a low-fidelity environment
60"
INTRODUCTION,0.10132890365448505,"and a high-fidelity environment. To leverage the cross-correlations between the low- and high-fidelity
61"
INTRODUCTION,0.10299003322259136,"returns, we propose an unbiased reduced-variance multifidelity estimator for the state-action value
62"
INTRODUCTION,0.10465116279069768,"function based on the framework of control variates. Second, a multifidelity Monte Carlo (MC) RL
63"
INTRODUCTION,0.10631229235880399,"algorithm, named MFMCRL, is proposed to improve the learning of the RL agent in the high-fidelity
64"
INTRODUCTION,0.1079734219269103,"environment. For any finite budget of high-fidelity environment interactions, MFMCRL leverages
65"
INTRODUCTION,0.10963455149501661,"low-fidelity data to learn better policies than a standard RL agent that uses only the high-fidelity
66"
INTRODUCTION,0.11129568106312292,"data. Third, we theoretically analyze the impacts of variance reduction in the estimation of the state-
67"
INTRODUCTION,0.11295681063122924,"action value function on policy evaluation and policy improvement using probability bounds. Fourth,
68"
INTRODUCTION,0.11461794019933555,"performance gains of the proposed MFMCRL algorithm are empirically assessed through numerical
69"
INTRODUCTION,0.11627906976744186,"experiments in synthetic multifidelity environments, as well as a neural architecture search (NAS)
70"
INTRODUCTION,0.11794019933554817,"use case.
71"
PRELIMINARIES AND RELATED WORK,0.11960132890365449,"2
Preliminaries and related work
72"
REINFORCEMENT LEARNING,0.1212624584717608,"2.1
Reinforcement learning
73"
REINFORCEMENT LEARNING,0.12292358803986711,"We consider episodic RL problems where the environment Σ is specified by an infinite-horizon
74"
REINFORCEMENT LEARNING,0.12458471760797342,"Markov decision process (MDP) with discounted returns [5]. Specifically, an infinite-horizon MDP
75"
REINFORCEMENT LEARNING,0.12624584717607973,"is defined as a tuple M = (S, A, P, β, R, γ), where S and A are finite sets of states and actions,
76"
REINFORCEMENT LEARNING,0.12790697674418605,"respectively; P : S × A × S →[0, 1] is the environment dynamics; and β : S →[0, 1] is the
77"
REINFORCEMENT LEARNING,0.12956810631229235,"initial distribution over the states, that is, β(s) = Pr(s0 = s), ∀s ∈S. The reward function R is
78"
REINFORCEMENT LEARNING,0.13122923588039867,"bounded and defined as R : S × A →[Rmin, Rmax], where Rmin and Rmax are real numbers. γ is a
79"
REINFORCEMENT LEARNING,0.132890365448505,"discount factor to bound the cumulative rewards and trade off how far- or short-sighted the agent is
80"
REINFORCEMENT LEARNING,0.1345514950166113,"in its decision making. The environment dynamics, P(s′|s, a), ∀s, a, s′ ∈S × A × S, encode the
81"
REINFORCEMENT LEARNING,0.1362126245847176,"stationary transition probability from a state s to a state s′ given that action a is chosen [7, 16]. In the
82"
REINFORCEMENT LEARNING,0.1378737541528239,"episodic setting, there exists at least one terminal state sT such that P(s′|sT , a) = 0, ∀a, s′ ̸= sT and
83"
REINFORCEMENT LEARNING,0.13953488372093023,"P(sT |sT , a) = 1, ∀a, i.e. sT is an absorbing state. Furthermore, β(sT ) = 0 and R(sT , a) = 0, ∀a.
84"
REINFORCEMENT LEARNING,0.14119601328903655,"When the RL agent transitions into a terminal state, all subsequent rewards are zero, and simulation
85"
REINFORCEMENT LEARNING,0.14285714285714285,"is restarted from another state s ∼β.
86"
REINFORCEMENT LEARNING,0.14451827242524917,"The agent’s decision-making process is characterized by π(a|s), which is a Markov stationary policy
87"
REINFORCEMENT LEARNING,0.1461794019933555,"that defines a distribution over the actions a ∈A given a state s ∈S. In the RL problem, P
88"
REINFORCEMENT LEARNING,0.1478405315614618,"1Poor sample complexity of model-free RL algorithms has long motivated developments in model-based RL,
where a predictive model of the environment is learned alongside the policy [14, 30]. Our work is focused on
model-free RL."
REINFORCEMENT LEARNING,0.14950166112956811,"and R are not known to the agent, yet the agent can interact with the environment sequentially
89"
REINFORCEMENT LEARNING,0.1511627906976744,"at discrete time steps, t = 0, 1, 2, · · · , T, by exchanging actions and rewards. Notice that T is a
90"
REINFORCEMENT LEARNING,0.15282392026578073,"random variable and denotes the time step at which the agent transitions into a terminal state. At
91"
REINFORCEMENT LEARNING,0.15448504983388706,"each time step t, the agent observes the environment’s state st = s ∈S, takes action at = a ∼
92"
REINFORCEMENT LEARNING,0.15614617940199335,"π(a|s) ∈A, and receives a reward rt+1 = R(s, a). The environment’s state then evolves to a
93"
REINFORCEMENT LEARNING,0.15780730897009967,"new state st+1 = s′ ∼P(s′|s, a). The state-value function of a state s under a policy π is defined
94"
REINFORCEMENT LEARNING,0.15946843853820597,"as the expected long-term discounted returns starting in state s and following policy π thereafter,
95"
REINFORCEMENT LEARNING,0.1611295681063123,"Vπ(s) = Eat∼π,st∼P"
REINFORCEMENT LEARNING,0.16279069767441862," P∞
t=0 γtR(st, at)|s0 = s

. In addition, the state-action value function of a
96"
REINFORCEMENT LEARNING,0.1644518272425249,"state s and action a under a policy π is defined as Qπ(s, a) = Eat∼π,st∼P"
REINFORCEMENT LEARNING,0.16611295681063123," P∞
t=0 γtR(st, at)|s0 =
97"
REINFORCEMENT LEARNING,0.16777408637873753,"s, a0 = a

. Notice that Vπ(s) = Ea∼π[Qπ(s, a)]. The solution of the RL problem is a policy π∗that
98"
REINFORCEMENT LEARNING,0.16943521594684385,"maximizes the discounted returns from the initial state distribution π∗= argmax
π
Es∼β[Vπ(s)]. It is
99"
REINFORCEMENT LEARNING,0.17109634551495018,"well known that there exists at least one optimal policy π∗such that Vπ∗(s) = max
π
Vπ(s), ∀s ∈S
100"
REINFORCEMENT LEARNING,0.17275747508305647,"and Qπ∗(s, a) = max
π
Qπ(s, a), ∀s, a ∈S × A [2]. Furthermore, a deterministic policy that selects
101"
REINFORCEMENT LEARNING,0.1744186046511628,"the greedy action with respect to Qπ∗(s, a), ∀s ∈S, is an optimal policy.
102"
CONTROL VARIATES,0.1760797342192691,"2.2
Control variates
103"
CONTROL VARIATES,0.1777408637873754,"The method of control variates is a variance reduction technique that leverages the correlation
104"
CONTROL VARIATES,0.17940199335548174,"between random variables (r.vs.) to reduce the variance of an estimator [20]. Let W1, W2, · · · , Wn
105"
CONTROL VARIATES,0.18106312292358803,"be n independent and identically distributed (i.i.d.) r.vs. such that E[Wi] = µW , and E[(Wi −
106"
CONTROL VARIATES,0.18272425249169436,"µW )2] = σ2
W , ∀i ∈[n]. In addition, let Z1, Z2, · · · , Zn be n i.i.d. r.vs. such that E[Zi] = µZ, and
107"
CONTROL VARIATES,0.18438538205980065,"E[(Zi −µZ)2] = σ2
Z, ∀i ∈[n]. Suppose that Wi, Zi are correlated with a correlation coefficient
108"
CONTROL VARIATES,0.18604651162790697,"ρW,Z =
Cov[Zi,Wi]
√"
CONTROL VARIATES,0.1877076411960133,"σ2
Z
√"
CONTROL VARIATES,0.1893687707641196,"σ2
W
, ∀i ∈[n], where Cov[Zi, Wi] = E[ZiWi] −E[Zi]E[Wi] is the covariance
109"
CONTROL VARIATES,0.19102990033222592,"between Zi and Wi. Furthermore, suppose that Wi, Zj are independent and thus uncorrelated ∀i ̸= j.
110"
CONTROL VARIATES,0.19269102990033224,"Using the Cauchy—Schwartz inequality, one can show that |ρW,Z| ≤1.
111"
CONTROL VARIATES,0.19435215946843853,"To estimate µW , we first consider the sample mean estimator, ˆθ1 = 1"
CONTROL VARIATES,0.19601328903654486,"n
Pn
i=1 Wi. ˆθ1 is an unbiased
112"
CONTROL VARIATES,0.19767441860465115,"estimator of µW , in other words, E[ˆθ1] = 1"
CONTROL VARIATES,0.19933554817275748,"n
Pn
i=1 E[Wi] = µW , and has a variance Var[ˆθ1] =
σ2
W
n .
113"
CONTROL VARIATES,0.2009966777408638,"Next, we consider the control-variate-based estimator,
114"
CONTROL VARIATES,0.2026578073089701,"ˆθ2 = 1 n n
X"
CONTROL VARIATES,0.20431893687707642,"i=1
Wi + α(Zi −µZ).
(1)"
CONTROL VARIATES,0.2059800664451827,"ˆθ2 is also an unbiased estimator of µW , i.e., E[ˆθ2] = µW , yet it has a variance Var[ˆθ2] = 1"
CONTROL VARIATES,0.20764119601328904,"nVar[Wi +
115"
CONTROL VARIATES,0.20930232558139536,α(Zi −µZ)] = 1
CONTROL VARIATES,0.21096345514950166,"n
 
Var[Wi]+α2Var[Zi]+2αCov[Zi, Wi]

. The variance of ˆθ2 can be controlled and
116"
CONTROL VARIATES,0.21262458471760798,"minimized by setting α to the minima of Var[Wi] + α2Var[Zi] + 2αCov[Zi, Wi], which is attained
117"
CONTROL VARIATES,0.21428571428571427,"at α∗= −Cov[Zi,Wi]"
CONTROL VARIATES,0.2159468438538206,"σ2
Z
= −ρZ,W
σW"
CONTROL VARIATES,0.21760797342192692,"σZ . Hence, by introducing α(Zi −µZ) as a control variate, the
118"
CONTROL VARIATES,0.21926910299003322,"variance of ˆθ2 is reduced,
119"
CONTROL VARIATES,0.22093023255813954,Var[ˆθ2] = (1 −ρ2
CONTROL VARIATES,0.22259136212624583,"Z,W )Var[ˆθ1].
(2)"
CONTROL VARIATES,0.22425249169435216,"Because ˆθ2 is an unbiased estimator, ˆθ2 has a lower mean squared error (MSE) by the bias-variance
120"
CONTROL VARIATES,0.22591362126245848,"decomposition theorem of the MSE. Applications of the method of control variates extend beyond
121"
CONTROL VARIATES,0.22757475083056478,"variance reduction. For example, the concept of control variates is used in [27] to design a fusion
122"
CONTROL VARIATES,0.2292358803986711,"framework to combine an arbitrary number of surrogate models optimally.
123"
RELATED WORK,0.23089700996677742,"2.3
Related work
124"
RELATED WORK,0.23255813953488372,"In [1], a policy search algorithm is proposed that leverages a crude approximate model ˆP of the true
125"
RELATED WORK,0.23421926910299004,"MDP to quickly learn to perform well on real systems. The proposed algorithm, however, is limited
126"
RELATED WORK,0.23588039867109634,"to the case where P is deterministic, and it assumes that model derivatives are good approximations
127"
RELATED WORK,0.23754152823920266,"of the true derivatives such that policy gradients can be computed by using the approximate model.
128"
RELATED WORK,0.23920265780730898,"In transfer learning (TL) [36, 23], value, model, or policy parameters are transferred in one direction
129"
RELATED WORK,0.24086378737541528,"as a heuristic initialization to bootstrap learning in the high-fidelity environment, with no option
130"
RELATED WORK,0.2425249169435216,"for backtracking. The option for the agent to backtrack and to choose which environment to use is
131"
RELATED WORK,0.2441860465116279,"studied in the multifidelity RL (MFRL) work of [9]. That algorithm is extended in [33] by integrating
132"
RELATED WORK,0.24584717607973422,"function approximation using Gaussian processes [38]. As in TL, both [9] and [33] use the value
133"
RELATED WORK,0.24750830564784054,"function from a lower-fidelity environment as a heuristic to bootstrap learning and guide exploration
134"
RELATED WORK,0.24916943521594684,"in the high-fidelity environment. From an optimization viewpoint, this approach is reasonable only
135"
RELATED WORK,0.25083056478405313,"if the lower-fidelity value function lies in the vicinity of the optimal high-fidelity value function, a
136"
RELATED WORK,0.25249169435215946,"situation that cannot be guaranteed or known a priori in general. Hence, in [9, 33], it is assumed that
137"
RELATED WORK,0.2541528239202658,"the optimal state-action value function in the low- and high-fidelity environments differ by no more
138"
RELATED WORK,0.2558139534883721,"than a small parameter β at every state-action pair, and they require the knowledge of β a priori to
139"
RELATED WORK,0.2574750830564784,"manage exploration-exploitation across multifidelity environments. By contrast, we require only that
140"
RELATED WORK,0.2591362126245847,"the low- and high-fidelity returns are correlated in our work, and the correlation need not be known
141"
RELATED WORK,0.260797342192691,"a priori. The cross-correlation between the low- and high-fidelity returns is used for reducing the
142"
RELATED WORK,0.26245847176079734,"variance in the estimation of the high-fidelity state-action value function, and hence our approach is
143"
RELATED WORK,0.26411960132890366,"complementary to existing TL techniques that use multifidelity environments for guided exploration
144"
RELATED WORK,0.26578073089701,"[9, 33]. We show that as long as the low- and high-fidelity state-action value function of a policy are
145"
RELATED WORK,0.26744186046511625,"correlated, the agent can benefit from the cheap and abundantly available low-fidelity data to improve
146"
RELATED WORK,0.2691029900332226,"its performance, without altering the exploration process.
147"
MULTIFIDELITY ESTIMATION IN RL,0.2707641196013289,"3
Multifidelity estimation in RL
148"
PROBLEM SETUP,0.2724252491694352,"3.1
Problem setup
149"
PROBLEM SETUP,0.27408637873754155,"We consider a multifidelity setup in which the RL agent has access to two environments, Σlo and Σhi,
150"
PROBLEM SETUP,0.2757475083056478,"modeled by the two MDPs Mlo = (Slo, A, Plo, βlo, Rlo, γ), and Mhi = (Shi, A, Phi, βhi, Rhi, γ),
151"
PROBLEM SETUP,0.27740863787375414,"respectively, as shown in Figure 1. Σlo is a low-fidelity environment in which the low-fidelity
152"
PROBLEM SETUP,0.27906976744186046,"reward function Rlo : S × A →[Rlo
min, Rlo
max] and the low-fidelity dynamics Plo are cheap2 to
153"
PROBLEM SETUP,0.2807308970099668,"evaluate/simulate, yet they are potentially inaccurate. On the other hand, Σhi is a high-fidelity
154"
PROBLEM SETUP,0.2823920265780731,"environment in which the high-fidelity reward function Rhi : S × A →[Rhi
min, Rhi
max] and the high-
155"
PROBLEM SETUP,0.2840531561461794,"fidelity dynamics Phi describe the real-world system with the highest accuracy, yet they are expensive
156"
PROBLEM SETUP,0.2857142857142857,"to evaluate/simulate [11]. We stress that (Phi, βhi, Rhi) and (Plo, βlo, Rlo) are unknown to the agent,
157"
PROBLEM SETUP,0.287375415282392,"and interaction with the two environments is only through the exchange of states, actions, next states
158"
PROBLEM SETUP,0.28903654485049834,"and rewards, which is the typical case in RL.
159"
PROBLEM SETUP,0.29069767441860467,"The action space A is the same in both environments, yet the state space may differ. It is assumed
160"
PROBLEM SETUP,0.292358803986711,"that the low-fidelity state space is a subset of the high-fidelity state space, Slo ⊆Shi, in other words,
161"
PROBLEM SETUP,0.29401993355481726,"the states available in the low-fidelity environment are a subset of those available at the high-fidelity
162"
PROBLEM SETUP,0.2956810631229236,"environment, and it is assumed that there exists a known mapping3 T : Shi →Slo as in previous
163"
PROBLEM SETUP,0.2973421926910299,"works [36, 9]. High-fidelity environments usually capture more state information than do low- fidelity
164"
PROBLEM SETUP,0.29900332225913623,"environments so T can be a many-to-one map. Access to the high-fidelity simulator Σhi is restricted
165"
PROBLEM SETUP,0.30066445182724255,"to full episodes τ hi = (shi
0 , a0, rhi
1 , shi
1 , a1, rhi
2 , shi
2 , · · · , shi
T ). On the other hand, Σlo is generative, and
166"
PROBLEM SETUP,0.3023255813953488,"simulation can be started by the agent at any state-action pair [15, 17]. Using T and Σlo, the agent
167"
PROBLEM SETUP,0.30398671096345514,"can map a τ hi to τ lo = (T (shi
0 ), a0, rlo
1 , T (shi
1 ), a1, rlo
2 , T (shi
2 ), · · · , T (shi
T )), and it is assumed that
168"
PROBLEM SETUP,0.30564784053156147,"Pr(τ lo) > 0 under Plo and βlo. It is also assumed that Rlo(T (shi), a) and Rhi(shi, a) are correlated.
169"
PROBLEM SETUP,0.3073089700996678,"Based on this setup, a correlation exits between the low- and high- fidelity trajectories
170"
PROBLEM SETUP,0.3089700996677741,"that can be beneficial for policy learning.
In this work we study how to leverage the
171"
PROBLEM SETUP,0.3106312292358804,"cheaply accessible low-fidelity trajectories from Σlo, to learn an optimal π∗that maximizes
172"
PROBLEM SETUP,0.3122923588039867,"Es∼βhi

Eat∼π,st∼Phi
 P∞
t=0 γtRhi(shi
t , at)|shi
0 = s

; in other words, to learn π∗that is optimal
173"
PROBLEM SETUP,0.313953488372093,"with respect to the high-fidelity environment Σhi.
174"
PROBLEM SETUP,0.31561461794019935,"2Sampling cost is application dependent. It is up to the practitioner to assign cost and determine low- and
high-fidelity sampling budgets.
3T is problem-specific. For instance, if Shi represents a fine grid and Slo represents a coarse grid, then T
will map shi to the closest slo based on a chosen distance metric."
MULTIFIDELITY MONTE CARLO RL,0.31727574750830567,"3.2
Multifidelity Monte Carlo RL
175"
MULTIFIDELITY MONTE CARLO RL,0.31893687707641194,"High-fidelity 
environment"
MULTIFIDELITY MONTE CARLO RL,0.32059800664451826,RL agent
MULTIFIDELITY MONTE CARLO RL,0.3222591362126246,"Low-fidelity 
environment"
MULTIFIDELITY MONTE CARLO RL,0.3239202657807309,"slo 2 Slo
shi 2 Shi
rhi 2 Rhi
rlo 2 Rlo"
MULTIFIDELITY MONTE CARLO RL,0.32558139534883723,"a 2 A
a 2 A"
MULTIFIDELITY MONTE CARLO RL,0.3272425249169435,FJ3UV62vWJdugkVwFVKtr13BjcuK9gFNLJPpB06yYSZG2kJ+RU3LhRx64+482+ctEV8HrhwOde7r3HjzlT4DjvRmFhcWl5pbhaWlvf2Nwyt8stJRJaJMILmTHx4pyFtEmMOC0E0uKQ5/Ttj+6yP32HZWKiegGJjH1QjyIWMAIBi31zLJ7zQYhvk1doGNIuciynlx7PMcx1bVdqb4TSpojkbPfHP7giQhjYBwrFS36sTgpVgCI5xmJTdRNMZkhAe0q2mEQ6q8dHp7Zu1rpW8FQuqKwJqXydSHCo1CX3dGWIYqp9eLv7ldRMIzryURXECNCKzRUHCLRBWHoTVZ5IS4BNMJFM32qRIZaYgI6rpEP4/dP0jq0qyd27apWqR/N4yiXbSHDlAVnaI6ukQN1EQEjdE9ekRPRmY8GM/Gy6y1YMxndtA3GK8fJxWVJA=</latexit>⌃lo
MULTIFIDELITY MONTE CARLO RL,0.3289036544850498,AFJ3UV62vWJdugkVwFVKtr13BjcuK9gFNLJPpB06k4SZG2kJ+RU3LhRx64+482+ctEV8HrhwOde7r3HjzlT4DjvRmFhcWl5pbhaWlvf2Nwyt8stFSWS0CaJeCQ7PlaUs5A2gQGnnVhSLHxO2/7oIvfbd1QqFoU3MImpJ/AgZAEjGLTUM8vuNRsIfJu6QMeQDlmW9cyKY5/nOLaqtjPFb1JBczR65pvbj0giaAiEY6W6VScGL8USGOE0K7mJojEmIzygXU1DLKjy0untmbWvlb4VRFJXCNZU/TqRYqHURPi6U2AYqp9eLv7ldRMIzryUhXECNCSzRUHCLYisPAirzyQlwCeaYCKZvtUiQywxAR1XSYfw/+fpHVoV0/s2lWtUj+ax1FEu2gPHaAqOkV1dIkaqIkIGqN79IiejMx4MJ6Nl1lrwZjP7KBvMF4/ABfVlRo=</latexit>⌃hi
MULTIFIDELITY MONTE CARLO RL,0.33056478405315615,"Figure 1: RL with low- and high-fidelity environ-
ments. Σlo is cheap to evaluate but is potentially
inaccurate. Σhi represents the real world with the
highest accuracy, yet it is expensive to evaluate.
The RL agent leverages the correlations between
the low- and high-fidelity data to learn π∗
hi."
MULTIFIDELITY MONTE CARLO RL,0.33222591362126247,"The Monte Carlo method to solve the RL prob-
176"
MULTIFIDELITY MONTE CARLO RL,0.3338870431893688,"lem is based on the idea of averaging sample
177"
MULTIFIDELITY MONTE CARLO RL,0.33554817275747506,"returns. In the MC method, experience is di-
178"
MULTIFIDELITY MONTE CARLO RL,0.3372093023255814,"vided into episodes. At the end of an episode,
179"
MULTIFIDELITY MONTE CARLO RL,0.3388704318936877,"state-action values are estimated, and the policy
180"
MULTIFIDELITY MONTE CARLO RL,0.34053156146179403,"is updated. For ease of exposition, we consider a
181"
MULTIFIDELITY MONTE CARLO RL,0.34219269102990035,"specific state-action pair (shi, a) in what follows
182"
MULTIFIDELITY MONTE CARLO RL,0.3438538205980066,"and suppress the dependence on (shi, a) from
183"
MULTIFIDELITY MONTE CARLO RL,0.34551495016611294,"the notation to avoid clutter. Consider a sam-
184"
MULTIFIDELITY MONTE CARLO RL,0.34717607973421927,"ple trajectory τ hi that results from the agent’s
185"
MULTIFIDELITY MONTE CARLO RL,0.3488372093023256,"interaction with the high-fidelity environment
186"
MULTIFIDELITY MONTE CARLO RL,0.3504983388704319,"starting at (shi
0 = shi, a0 = a) and following
187"
MULTIFIDELITY MONTE CARLO RL,0.3521594684385382,"π, that is, τ hi : shi
0 , a0, rhi
1 , shi
1 , a1, rhi
2 , · · · , shi
T .
188"
MULTIFIDELITY MONTE CARLO RL,0.3538205980066445,"Note that rhi
t+1 = Rhi(shi
t , at). Let Ghi denote the corresponding long-term discounted return,
189"
MULTIFIDELITY MONTE CARLO RL,0.3554817275747508,"Ghi = P∞
t=0 γtrhi
t+1. The high-fidelity state-action value of the pair (s, a) when the agent follows π
190"
MULTIFIDELITY MONTE CARLO RL,0.35714285714285715,"is
191"
MULTIFIDELITY MONTE CARLO RL,0.3588039867109635,"Qhi
π(shi, a) = Eτ hi

Ghi|shi
0 = shi, a0 = a

.
(3)"
MULTIFIDELITY MONTE CARLO RL,0.36046511627906974,"Notice that Qhi
π(shi, a) is the expectation of an r.v. Ghi with respect to the random trajectory τ hi. Ghi
192"
MULTIFIDELITY MONTE CARLO RL,0.36212624584717606,"is a bounded r.v. with support on the interval [ Rhi
min
1−γ , Rhi
max
1−γ ] and has a finite variance given by
193"
MULTIFIDELITY MONTE CARLO RL,0.3637873754152824,"σ2
hi(shi, a) = Eτ hi
h 
Ghi −Qhi
π(shi, a)
2|s0 = shi, a0 = a
i
.
(4)"
MULTIFIDELITY MONTE CARLO RL,0.3654485049833887,"By interacting with the environment, the agent can sample only a finite number of trajectories, n.
194"
MULTIFIDELITY MONTE CARLO RL,0.36710963455149503,"Let τ hi
1 , τ hi
2 , · · · , τ hi
n be the n sampled trajectories that starts at the pair (shi, a). Furthermore, let
195"
MULTIFIDELITY MONTE CARLO RL,0.3687707641196013,"Ghi
1 , Ghi
2 , · · · , Ghi
n be i.i.d. r.vs. that correspond to the long-term discounted returns of the sampled
196"
MULTIFIDELITY MONTE CARLO RL,0.3704318936877076,"trajectories, τ hi
1 , τ hi
2 , · · · , τ hi
n , respectively. Notice that Eτ hi[Ghi
1 ] = Eτ hi[Ghi
2 ] = · · · = Eτ hi[Ghi
n ] =
197"
MULTIFIDELITY MONTE CARLO RL,0.37209302325581395,"Qhi
π(s, a). The first-visit MC sample average is
198"
MULTIFIDELITY MONTE CARLO RL,0.37375415282392027,"ˆQhi
π,n(shi, a) = 1 n n
X"
MULTIFIDELITY MONTE CARLO RL,0.3754152823920266,"i=1
Ghi
i .
(5)"
MULTIFIDELITY MONTE CARLO RL,0.3770764119601329,"By the weak law of large numbers, lim
n→∞Pr
 
| ˆQhi
π,n(shi, a) −Qhi
π(shi, a)| > ξ

= 0, for any positive
199"
MULTIFIDELITY MONTE CARLO RL,0.3787375415282392,"number ξ. In addition, the variance of this unbiased sample average estimator is
200"
MULTIFIDELITY MONTE CARLO RL,0.3803986710963455,"Var
h
ˆQhi
π,n(shi, a)
i
= σ2
hi(shi, a)"
MULTIFIDELITY MONTE CARLO RL,0.38205980066445183,"n
.
(6)"
MULTIFIDELITY MONTE CARLO RL,0.38372093023255816,"Using the low-fidelity generative environment and the method of control variates, we design an
201"
MULTIFIDELITY MONTE CARLO RL,0.3853820598006645,"unbiased estimator for the expected long-term discounted returns that has a smaller variance than
202"
MULTIFIDELITY MONTE CARLO RL,0.38704318936877075,"(6). Let τ lo
i be the ith low-fidelity trajectory that is obtained from τ hi
i by using T and the generative
203"
MULTIFIDELITY MONTE CARLO RL,0.38870431893687707,"low-fidelity environment to evaluate rlow
t+1 = Rlo(T (shi
t ), at). Let Glo
i be the r.v. which corresponds to
204"
MULTIFIDELITY MONTE CARLO RL,0.3903654485049834,"the long-term discounted return of τ lo
i . Notice that Ghi
i and Glo
i are correlated r.vs. in this multifidelity
205"
MULTIFIDELITY MONTE CARLO RL,0.3920265780730897,"setup. Based on those low-fidelity trajectories, the low-fidelity first-visit MC sample average is
206"
MULTIFIDELITY MONTE CARLO RL,0.39368770764119604,"ˆQlo
π,n(T (shi), a) = 1"
MULTIFIDELITY MONTE CARLO RL,0.3953488372093023,"n
Pn
i=1 Glo
i and has a variance of Var
h
ˆQlo
π,n(T (shi), a)
i
= σ2
lo(T (shi),a)"
MULTIFIDELITY MONTE CARLO RL,0.39700996677740863,"n
, where
207"
MULTIFIDELITY MONTE CARLO RL,0.39867109634551495,"σ2
lo(T (shi), a) = Eτ lo
h 
Glo −Qlo
π(T (shi), a)
2|s0 = T (shi), a0 = a
i
and Qlo
π(T (shi), a) is the true
208"
MULTIFIDELITY MONTE CARLO RL,0.4003322259136213,"population mean.
209"
MULTIFIDELITY MONTE CARLO RL,0.4019933554817276,"Using the method of control variates presented in Subsection 2.2, we propose the following multifi-
210"
MULTIFIDELITY MONTE CARLO RL,0.40365448504983387,"delity MC estimator:
211"
MULTIFIDELITY MONTE CARLO RL,0.4053156146179402,"ˆQMFMC
π,n
(shi, a) = ˆQhi
π,n(shi, a) + α∗
s,a"
MULTIFIDELITY MONTE CARLO RL,0.4069767441860465,"
Qlo
π(T (shi), a) −ˆQlo
π,n(T (shi), a)

,
(7)"
MULTIFIDELITY MONTE CARLO RL,0.40863787375415284,"where
212"
MULTIFIDELITY MONTE CARLO RL,0.41029900332225916,"α∗
s,a = Cov
 ˆQhi
π,n(shi, a), ˆQlo
π,n(T (shi), a)
"
MULTIFIDELITY MONTE CARLO RL,0.4119601328903654,"Var
 ˆQloπ,n(T (shi), a)

.
(8)"
MULTIFIDELITY MONTE CARLO RL,0.41362126245847175,"Notice that the estimator in (7) is unbiased and has a variance of
213"
MULTIFIDELITY MONTE CARLO RL,0.4152823920265781,"Var
h
ˆQMFMC
π,n
(shi, a)
i
=
 
1 −ρ2
s,a

Var
h
ˆQhi
π,n(shi, a)
i
,
(9)"
MULTIFIDELITY MONTE CARLO RL,0.4169435215946844,"where ρs,a is the correlation coefficient between the low-fidelity and high-fidelity long-term dis-
214"
MULTIFIDELITY MONTE CARLO RL,0.4186046511627907,"counted returns:
215"
MULTIFIDELITY MONTE CARLO RL,0.420265780730897,"ρs,a =
Cov
 ˆQhi
π,n(shi, a), ˆQlo
π,n(T (shi), a)
 r"
MULTIFIDELITY MONTE CARLO RL,0.4219269102990033,"Var
h
ˆQhiπ,n(shi, a)
i
Var
h
ˆQloπ,n(T (shi), a)
i.
(10)"
MULTIFIDELITY MONTE CARLO RL,0.42358803986710963,"Therefore, the variance in estimating the value of a state-action pair under a policy π can be reduced
216"
MULTIFIDELITY MONTE CARLO RL,0.42524916943521596,"by a factor of
 
1 −ρ2
s,a

when the low-fidelity data is exploited, although the budget of high-fidelity
217"
MULTIFIDELITY MONTE CARLO RL,0.4269102990033223,"samples remains the same. Notice that
218"
MULTIFIDELITY MONTE CARLO RL,0.42857142857142855,"Cov
 ˆQhi
π,n(shi, a), ˆQlo
π,n(T (shi), a)

= Cov
 1 n n
X"
MULTIFIDELITY MONTE CARLO RL,0.43023255813953487,"i=1
Ghi
i , 1 n n
X"
MULTIFIDELITY MONTE CARLO RL,0.4318936877076412,"i=1
Glo
i

= 1"
MULTIFIDELITY MONTE CARLO RL,0.4335548172757475,"nCov

Ghi
i , Glo
i

,
(11)"
MULTIFIDELITY MONTE CARLO RL,0.43521594684385384,"because Ghi
i , Glo
j
are independent r.vs.
∀i ̸= j.
Hence, Cov
 ˆQhi
π,n(shi, a), ˆQlo
π,n(T (shi), a)

,
219"
MULTIFIDELITY MONTE CARLO RL,0.4368770764119601,"Var
h
ˆQhi
π,n(shi, a)
i
, and Var
h
ˆQlo
π,n(T (shi), a)
i
can all be estimated in practice based on the return
220"
MULTIFIDELITY MONTE CARLO RL,0.43853820598006643,"data samples using the standard unbiased estimators for the variance and covariance.
221"
MULTIFIDELITY MONTE CARLO RL,0.44019933554817275,"The reduced-variance estimator of (7) can be used to design a multifidelity Monte Carlo RL algorithm
222"
MULTIFIDELITY MONTE CARLO RL,0.4418604651162791,"as shown in Algorithm 1 in Appendix A. This algorithm is based on the on-policy first-visit MC
223"
MULTIFIDELITY MONTE CARLO RL,0.4435215946843854,"control algorithm with ϵ-soft policies [34] but uses the multifidelity estimator (7). Algorithm 1 is
224"
MULTIFIDELITY MONTE CARLO RL,0.44518272425249167,"based on the idea of generalized policy iteration. In the policy evaluation step (lines 11–18), the
225"
MULTIFIDELITY MONTE CARLO RL,0.446843853820598,"state-action value function is made consistent with the current policy by updating the estimated
226"
MULTIFIDELITY MONTE CARLO RL,0.4485049833887043,"long-term discounted returns of a state-action pair (st, at) using the control-variate-based estimator
227"
MULTIFIDELITY MONTE CARLO RL,0.45016611295681064,"(7) (line 18). This update requires the estimation of the correlation between the low- and high-
228"
MULTIFIDELITY MONTE CARLO RL,0.45182724252491696,"fidelity returns, which is done in lines 13–17. Next, in the policy improvement step (lines 19–20), the
229"
MULTIFIDELITY MONTE CARLO RL,0.45348837209302323,"policy is made ϵ-greedy with respect to the current state-action value function. In each episode, the
230"
MULTIFIDELITY MONTE CARLO RL,0.45514950166112955,"agent needs to evaluate the policy in the low-fidelity environment to obtain Qlo
π. This can be done in
231"
MULTIFIDELITY MONTE CARLO RL,0.4568106312292359,"practice by collecting a large number of m return samples from the cheap low-fidelity environment
232"
MULTIFIDELITY MONTE CARLO RL,0.4584717607973422,"and setting Qlo
π(T (shi), a) ≈ˆQlo
π,m+n(T (shi), a). The convergence of Algorithm 1 to the optimal
233"
MULTIFIDELITY MONTE CARLO RL,0.4601328903654485,"ϵ-greedy policy, π∗
ϵ−opt, along with its corresponding ˆQMFMC
∗
, is guaranteed under the same conditions
234"
MULTIFIDELITY MONTE CARLO RL,0.46179401993355484,"that guarantee convergence for the on-policy first-visit MC control algorithm with ϵ-soft policies [34].
235"
MULTIFIDELITY MONTE CARLO RL,0.4634551495016611,"In the following subsection, we theoretically analyze the impacts of variance reduction on policy
236"
MULTIFIDELITY MONTE CARLO RL,0.46511627906976744,"evaluation and policy improvement.
237"
THEORETICAL ANALYSIS,0.46677740863787376,"3.3
Theoretical analysis
238"
THEORETICAL ANALYSIS,0.4684385382059801,"In this subsection we analyze the impacts of variance reduction on policy evaluation error and policy
239"
THEORETICAL ANALYSIS,0.4700996677740864,"improvement by introducing two main theorems. Intermediate lemmas along with all the proofs can
240"
THEORETICAL ANALYSIS,0.4717607973421927,"be found in Appendix B.
241"
POLICY EVALUATION,0.473421926910299,"3.3.1
Policy evaluation
242"
POLICY EVALUATION,0.4750830564784053,"In policy evaluation, the task is to estimate the state-action value function of a given policy π.
243"
POLICY EVALUATION,0.47674418604651164,"Trajectory samples are first generated by interacting with the environment using π, and the state-action
244"
POLICY EVALUATION,0.47840531561461797,"value function is then estimated using either the single high-fidelity estimator (5) or the proposed
245"
POLICY EVALUATION,0.48006644518272423,"multifidelity estimator (7). To analyze the impacts of variance reduction on policy evaluation error,
246"
POLICY EVALUATION,0.48172757475083056,"we first derive a a Bernstein-type concentration inequality [6] that relates the deviation between the
247"
POLICY EVALUATION,0.4833887043189369,"sample average and the true mean to the sample size n, estimation accuracy parameters δ, ξ, and the
248"
POLICY EVALUATION,0.4850498338870432,"variance of a r.v. as follows.
249"
POLICY EVALUATION,0.4867109634551495,"Lemma 1 Let X1, X2, · · · , Xn be i.i.d. r.vs. with mean E[Xi] = µX and variance E[(Xi−µX)2] =
250"
POLICY EVALUATION,0.4883720930232558,"σ2
X, ∀i ∈[n]. Furthermore, suppose that Xi, ∀i, are bounded almost surely with a parameter b,
251"
POLICY EVALUATION,0.4900332225913621,"namely, Pr(|Xi −µX| ≤b) = 1, ∀i. Then
252 Pr"
N,0.49169435215946844,"1
n n
X"
N,0.49335548172757476,"i=1
Xi −µX ≥ξ !"
N,0.4950166112956811,"≤2exp
−nξ2 4σ2
X"
N,0.49667774086378735,"
(12)"
N,0.4983388704318937,"for 0 ≤ξ ≤σ2
X/b.
253"
N,0.5,"Next, the concentration bound of Lemma 1 is used to derive the minimum sample size that is required
254"
N,0.5016611295681063,"to ensure that the sample average deviates by no more than ξ from the true mean with high probability
255"
N,0.5033222591362126,"for both the high-fidelity estimator (5) and the multifidelity estimator (7).
256"
N,0.5049833887043189,"Theorem 1 To guarantee that
257"
PR,0.5066445182724253,"1. Pr

| ˆQhi
π,n(shi, a) −Qhi
π(shi, a)| ≤ξ

≥1 −δ, then n ≥4σ2
hi(shi,a)"
PR,0.5083056478405316,"ξ2
log( 2"
PR,0.5099667774086378,"δ ).
258"
PR,0.5116279069767442,"2. Pr

| ˆQMFMC
π,n
(s, a) −Qhi
π(shi, a)| ≤ξ

≥1 −δ, then n ≥
4(1−ρ2
s,a)σ2
hi(shi,a)
ξ2
log( 2"
PR,0.5132890365448505,"δ ).
259"
PR,0.5149501661129569,"The result of Theorem 1 highlights the benefit of using our proposed multifidelity estimator (7) for
260"
PR,0.5166112956810631,"policy evaluation as opposed to using the single high-fidelity estimator of (5). By leveraging the
261"
PR,0.5182724252491694,"correlation between low- and high-fidelity returns ρs,a, the variance of the multifidelity estimator
262"
PR,0.5199335548172758,"is reduced by a factor of (1 −ρ2
s,a), which makes it possible to achieve a low estimation error at a
263"
PR,0.521594684385382,"reduced number of high-fidelity samples.
264"
POLICY IMPROVEMENT,0.5232558139534884,"3.3.2
Policy improvement
265"
POLICY IMPROVEMENT,0.5249169435215947,"In policy improvement, a new policy π′ is constructed by deterministically choosing the greedy
266"
POLICY IMPROVEMENT,0.526578073089701,"action with respect to the state-action value function of the original policy π, Qhi
π(s, a), at every state,
267"
POLICY IMPROVEMENT,0.5282392026578073,"that is, π′(s)
.= argmax
a∈A
Qhi
π(s, a), ∀s ∈S. By the policy improvement theorem, π′ is as good as or
268"
POLICY IMPROVEMENT,0.5299003322259136,"better than π under the assumption that Qhi
π(s, a), ∀s, a is computed exactly. In practice, the MDP is
269"
POLICY IMPROVEMENT,0.53156146179402,"unknown, and the state-action value function is estimated based on a finite number of trajectories.
270"
POLICY IMPROVEMENT,0.5332225913621262,"Moreover, those trajectories are generated by following an exploratory policy, such as an ϵ-soft
271"
POLICY IMPROVEMENT,0.5348837209302325,"policy. Because we are interested in studying how different estimators impact policy improvement,
272"
POLICY IMPROVEMENT,0.5365448504983389,"we consider a target state shi ∈Shi and assume that we have n trajectories for each action a ∈A at
273"
POLICY IMPROVEMENT,0.5382059800664452,"this target state. This assumption basically ensures that all actions at the target state shi have been
274"
POLICY IMPROVEMENT,0.5398671096345515,"explored equally well and enables us to make fair comparisons about estimator performance.
275"
POLICY IMPROVEMENT,0.5415282392026578,"Without loss of generality, suppose that Qhi
π(shi, a1) ≥Qhi
π(shi, a2) ≥· · · Qhi
π(shi, a|A|). Let ∆i =
276"
POLICY IMPROVEMENT,0.5431893687707641,"Qhi
π(shi, a1) −Qhi
π(shi, ai), ∀i ̸= 1. We analyze the probability that a1, which is the greedy action
277"
POLICY IMPROVEMENT,0.5448504983388704,"given the true Qhi
π(shi, a), is the greedy action with respect to the single- and multifidelity estimators
278"
POLICY IMPROVEMENT,0.5465116279069767,"in our next theorem.
279"
POLICY IMPROVEMENT,0.5481727574750831,"Theorem 2 Suppose that the number of trajectories from a state-action pair at a target state shi ∈Shi
280"
POLICY IMPROVEMENT,0.5498338870431894,"is the same for all actions a ∈A and that a1 is the greedy action with respect to the true Qhi
π(shi, a).
281"
POLICY IMPROVEMENT,0.5514950166112956,"Furthermore, suppose that Phi(shi|shi′, a) ≥β(shi), ∀shi ∈Shi. Then
282"
PR,0.553156146179402,"1. Pr
 
a1 = argmax
a∈A
ˆQhi
π,n(shi, a)

≥Q|A|
i=2
∆2
i
∆2
i +Var[ ˆ
Qhi
π,n(shi,a1)]+Var[ ˆ
Qhi
π,n(shi,ai)].
283"
PR,0.5548172757475083,"2. Pr
 
a1 = argmax
a∈A
ˆQMFMC
π,n
(shi, a)

≥Q|A|
i=2
∆2
i
∆2
i +(1−ρ2s,a1)Var[ ˆ
Qhiπ,n(shi,ai)]+(1−ρ2s,ai)Var[ ˆ
Qhiπ,n(shi,ai)].
284"
PR,0.5564784053156147,"Notice that when |ρs,a2| →1, the lower bound in the result of Theorem 2 approaches 1, which
285"
PR,0.5581395348837209,"means that the correct greedy action a1 can be selected with certainty when the reduced-variance
286"
PR,0.5598006644518272,"multifidelity estimator (7) is adopted. Combining the results of Theorems 1 and 2, the proposed
287"
PR,0.5614617940199336,"MFMCRL algorithm is expected to outperform its single high-fidelity Monte Carlo counterpart in terms
288"
PR,0.5631229235880398,"of learning a better policy under a given budget of high-fidelity environment interactions.
289"
NUMERICAL EXPERIMENTS,0.5647840531561462,"4
Numerical experiments
290"
NUMERICAL EXPERIMENTS,0.5664451827242525,"In this section we empirically evaluate the performance of the proposed MFMCRL algorithm on
291"
NUMERICAL EXPERIMENTS,0.5681063122923588,"synthetic MDP problems and on a NAS use case. Our codes and all experimental details can be found
292"
NUMERICAL EXPERIMENTS,0.5697674418604651,"in Appendix C.
293"
SYNTHETIC MDPS,0.5714285714285714,"4.1
Synthetic MDPs
294"
SYNTHETIC MDPS,0.5730897009966778,"We synthesize multifidelity random MDP problems with state space cardinality |S| and action space
295"
SYNTHETIC MDPS,0.574750830564784,"cardinality |A|. The high-fidelity transition and reward functions, Phi and Rhi, respectively, are
296"
SYNTHETIC MDPS,0.5764119601328903,"first generated based on a random process as detailed in Appendix C.2. Next, for a given Phi and
297"
SYNTHETIC MDPS,0.5780730897009967,"Rhi, the corresponding Plow and Rlow are generated by injecting Gaussian noise to meet a desired
298"
SYNTHETIC MDPS,0.579734219269103,"signal-to-noise ratio. Specifically, we generate a random matrix PN of size |S| × |A| × |S| from
299"
SYNTHETIC MDPS,0.5813953488372093,"a normally distributed r.v. with mean 0 and variance σ2
P, and set Plow = Phi + PN. Plow is then
300"
SYNTHETIC MDPS,0.5830564784053156,appropriately normalized so that P
SYNTHETIC MDPS,0.584717607973422,"slo′∈S Plo(slo′|slo, a) = 1. Similarly, we generate a random
301"
SYNTHETIC MDPS,0.5863787375415282,"matrix RN of size |S| × |A| from a normally distributed r.v. with mean 0 and variance σ2
R and set
302"
SYNTHETIC MDPS,0.5880398671096345,"Rlow = Rhi + RN. Phi and Rhi are then encapsulated within a gym-like environment with which
303"
SYNTHETIC MDPS,0.5897009966777409,"the agent can interact by exchanging sample tuples of the form (shi, a, rhi, shi′). Similarly, Plo and
304"
SYNTHETIC MDPS,0.5913621262458472,"Rlo are encapsulated within a gym-like environment to form the low-fidelity environment. In this
305"
SYNTHETIC MDPS,0.5930232558139535,"experiment, both low- and high-fidelity environments share the same state-action space—that is, T is
306"
SYNTHETIC MDPS,0.5946843853820598,"an identity transformation—yet the transition and reward functions of the low-fidelity environment
307"
SYNTHETIC MDPS,0.5963455149501661,"are different since they are corrupted with noise. Notice that even if the agent could draw an infinite
308"
SYNTHETIC MDPS,0.5980066445182725,"number of samples from Plo and Rlo, it would not be able to recover Phi and Rhi since Plo and
309"
SYNTHETIC MDPS,0.5996677740863787,"Rlo underneath the low-fidelity environment themselves are corrupted. This situation mimics what
310"
SYNTHETIC MDPS,0.6013289036544851,"happens in practice when we attempt to learn Plo and Rlo based on real data and build an RL
311"
SYNTHETIC MDPS,0.6029900332225914,"environment off those learned functions to train the agent.
312"
SYNTHETIC MDPS,0.6046511627906976,"After constructing the multifidelity environments, we train an RL agent using the proposed MFMCRL
313"
SYNTHETIC MDPS,0.606312292358804,"algorithm over 10K high-fidelity episodes, where a training episode is defined to be a trajectory that
314"
SYNTHETIC MDPS,0.6079734219269103,"ends at a terminal state. The MFMCRL agent interacts with the low-fidelity environment as shown in
315"
SYNTHETIC MDPS,0.6096345514950167,"Algorithm 1, to generate reduced-variance estimates of the state-action value function. As a baseline
316"
SYNTHETIC MDPS,0.6112956810631229,"for comparison, we train another RL agent (MCRL) using the standard the first-visit MC control
317"
SYNTHETIC MDPS,0.6129568106312292,"algorithm over 10K high-fidelity episodes [34]. We set γ and ϵ to 0.99 and 0.1, respectively. Every 50
318"
SYNTHETIC MDPS,0.6146179401993356,"training episodes, the greedy policy w.r.t to the estimated Q function is used to test the performance
319"
SYNTHETIC MDPS,0.6162790697674418,"of the agent on 200 test episodes. We repeat the whole experiment with 36 different random seeds
320"
SYNTHETIC MDPS,0.6179401993355482,"(to fully leverage our 36 core machine) and report the mean and standard deviation (across different
321"
SYNTHETIC MDPS,0.6196013289036545,"seeds) of the test episode rewards in Figure 2(a). One can observe that for a given budget of high-
322"
SYNTHETIC MDPS,0.6212624584717608,"fidelity episodes, the proposed MFMCRL algorithm outperforms MCRL in terms of policy performance,
323"
SYNTHETIC MDPS,0.6229235880398671,"with performance improving as the RL agent collects more low-fidelity samples (#τ lo refers to the
324"
SYNTHETIC MDPS,0.6245847176079734,"number of low-fidelity trajectories started from a state-action pair). In Figure 2(b), we vary the SNR
325"
SYNTHETIC MDPS,0.6262458471760798,"of the low-fidelity environment and observe that performance improves as SNR increases. This
326"
SYNTHETIC MDPS,0.627906976744186,"is expected because the low- and high-fidelity environments are better-correlated at higher SNRs.
327"
SYNTHETIC MDPS,0.6295681063122923,"Notice that when the SNR of the low-fidelity environment is -10 dB, there is no benefit from doing
328"
SYNTHETIC MDPS,0.6312292358803987,"multifidelity RL. The reason is that the low- and high-fidelity environments are too weakly correlated
329"
SYNTHETIC MDPS,0.632890365448505,"to benefit from multifidelity estimation. In fact, for this case Es,a,s′[|Phi −Plo|] = 0.275 ± 0.33, and
330"
SYNTHETIC MDPS,0.6345514950166113,"Es,a[|Rhi −Rlo|] = 1.029 ± 0.024, compared with the other extreme case (SNR +3dB) for which
331"
SYNTHETIC MDPS,0.6362126245847176,"Es,a,s′[|Phi −Plo|] = 0.009 ± 0.0002, and Es,a[|Rhi −Rlo|] = 0.230 ± 0.006. This is also evident
332"
SYNTHETIC MDPS,0.6378737541528239,"in Figure 2(c), where we show the mean variance reduction factor Var[ ˆQMFMC]/Var[ ˆQhi] estimated
333"
SYNTHETIC MDPS,0.6395348837209303,"based off the last 1K training episodes. When the low-fidelity environment is less noisy (higher SNR),
334"
SYNTHETIC MDPS,0.6411960132890365,"more variance reduction can be attained.
335"
NAS,0.6428571428571429,"4.2
NAS
336"
NAS,0.6445182724252492,"In NAS, the task is to discover high-performing neural architectures with respect to a given training
337"
NAS,0.6461794019933554,"dataset over a predefined search space. While many earlier works attempted to design RL-based NAS
338"
NAS,0.6478405315614618,"algorithms, [3, 40, 13], it has since become clear that the sample complexity of RL is too high to be
339"
NAS,0.6495016611295681,"competitive with state-of-the-art NAS methods [4, 37]. In this experiment we study how multifidelity
340"
NAS,0.6511627906976745,"RL can improve learning in NAS over standard RL, which could serve to catalyze future work in this
341"
NAS,0.6528239202657807,"direction to make RL more competitive in NAS.
342"
NAS,0.654485049833887,"For this experiment we use the tabular dataset of NAS-Bench-201 [10] to construct multifidelity RL
343"
NAS,0.6561461794019934,"environments as detailed in Appendix C.3. In summary, the RL agent sequentially configures the
344"
NAS,0.6578073089700996,"nodes of an architecture (inducing an MDP), after which the architecture is trained on the training
345"
NAS,0.659468438538206,"dataset for L epochs, and the validation accuracy on a held-out validation data set is provided to the
346"
NAS,0.6611295681063123,"agent as a reward. By maximizing the total rewards, high-performing architectures can be discovered.
347"
NAS,0.6627906976744186,"NAS-Bench-201 reports the validation accuracy curves for all the architectures in the search space
348"
NAS,0.6644518272425249,"0
2000
4000
6000
8000 10000
Training Episode 0.3 0.4 0.5 0.6 0.7"
NAS,0.6661129568106312,Test Rewards MCRL
NAS,0.6677740863787376,"MFMCRL (#τlo = 1/((shi), a))"
NAS,0.6694352159468439,"MFMCRL (#τlo = 5/((shi), a))"
NAS,0.6710963455149501,"MFMCRL (#τlo = 10/((shi), a))"
NAS,0.6727574750830565,(a) Σlo has an SNR of -3dB
NAS,0.6744186046511628,"0
2000
4000
6000
8000 10000
Training Episode 0.3 0.4 0.5 0.6 0.7"
NAS,0.6760797342192691,Test Rewards
NAS,0.6777408637873754,"MCRL
MFMCRL (SNR -10dB)
MFMCRL (SNR -6dB)
MFMCRL (SNR -3dB)
MFMCRL (SNR +3dB)"
NAS,0.6794019933554817,"(b) #τ lo = 10/(T (shi), a))"
NAS,0.6810631229235881,"-10dB
-6dB
-3dB
+2dB
SNR of low-fidelity environment 0.0 0.2 0.4 0.6"
NAS,0.6827242524916943,Var[ ̂QMFMC]/Var[ ̂Qhi]
NAS,0.6843853820598007,(c) Variance reduction factor
NAS,0.686046511627907,"Figure 2: Mean and standard deviation of test episode rewards for the proposed MFMCRL during
training: (a) test episode rewards improve with increasing number of low-fidelity samples (#τ lo); (b)
test episode rewards improve with less noisy low-fidelity environments; (c) variance reduction factor
improves when low- and high-fidelity environments are more correlated. These results are based on a
random MDP with |S| = 200, |A| = 8."
NAS,0.6877076411960132,"0
2000
4000
6000
8000 10000
Training Episode 36 37 38 39 40"
NAS,0.6893687707641196,Test Rewards
NAS,0.6910299003322259,"MCRL
MFMCRL-Case (i)
MFMCRL-Case (ii)"
NAS,0.6926910299003323,"Figure 3: Mean and standard deviation of test
episode rewards for the proposed MFMCRL dur-
ing training on multifidelity NAS environments.
See text for description of the two multifidelity
scenarios (i) and (ii).
In both cases, #τ lo =
5/(T (shi), a))."
NAS,0.6943521594684385,"as a function of the number of training epochs
349"
NAS,0.6960132890365448,"and for three image data sets. We construct two
350"
NAS,0.6976744186046512,"multifidelity scenarios as follows. In both sce-
351"
NAS,0.6993355481727574,"narios, the validation accuracy of an architecture
352"
NAS,0.7009966777408638,"at the end of training (i.e. at L = 200 epochs) is
353"
NAS,0.7026578073089701,"used as a high-fidelity reward in the high-fidelity
354"
NAS,0.7043189368770764,"environment. For the low-fidelity environment,
355"
NAS,0.7059800664451827,"we have two cases: (i) low-fidelity environment
356"
NAS,0.707641196013289,"is identical to the high-fidelity environment ex-
357"
NAS,0.7093023255813954,"cept for the reward function, which is now the
358"
NAS,0.7109634551495017,"validation accuracy at the L = 10th training
359"
NAS,0.7126245847176079,"epoch, and (ii) low-fidelity environment is de-
360"
NAS,0.7142857142857143,"fined for a smaller search space and the reward
361"
NAS,0.7159468438538206,"function is the validation accuracy of an archi-
362"
NAS,0.717607973421927,"tecture at the L = 10th training epoch. Note
363"
NAS,0.7192691029900332,"that in case (ii) the state space and dynamics
364"
NAS,0.7209302325581395,"differ between the low- and high-fidelity envi-
365"
NAS,0.7225913621262459,"ronments. For both cases, we train both our proposed MFMCRL and the MCRL exactly as we did in
366"
NAS,0.7242524916943521,"Section 4.1, and we report the mean and standard deviation of test episode rewards in Figure 3. We
367"
NAS,0.7259136212624585,"can observe that our multifidelity RL framework does indeed improve over standard RL and that
368"
NAS,0.7275747508305648,"performance gains are higher when the low- and high-fidelity environments are more similar, case (i).
369"
CONCLUSION,0.729235880398671,"5
Conclusion
370"
CONCLUSION,0.7308970099667774,"In this paper we have studied the RL problem in the presence of a low- and a high-fidelity environment
371"
CONCLUSION,0.7325581395348837,"for a given control task, with the aim of improving the agent’s performance in the high-fidelity
372"
CONCLUSION,0.7342192691029901,"environment with multifidelity data. We have proposed a multifidelity estimator based on the method
373"
CONCLUSION,0.7358803986710963,"of control variates, which uses low-fidelity data to reduce the variance in the estimation of the
374"
CONCLUSION,0.7375415282392026,"state-action value function. The impacts of variance reduction on policy improvement and policy
375"
CONCLUSION,0.739202657807309,"evaluation are theoretically analyzed, and a multifidelity Monte Carlo RL algorithm (MFMCRL) is
376"
CONCLUSION,0.7408637873754153,"devised. We show that for a finite budget of high-fidelity data, the MFMCRL agent can well exploit the
377"
CONCLUSION,0.7425249169435216,"cross-correlations between low- and high-fidelity data and yield superior performance. In our future
378"
CONCLUSION,0.7441860465116279,"work, we will study the design of a control-variate-based multifidelity RL framework with function
379"
CONCLUSION,0.7458471760797342,"approximation to solve continuous state-action space RL problems.
380"
BROADER IMPACT,0.7475083056478405,"6
Broader impact
381"
BROADER IMPACT,0.7491694352159468,"Positive impacts: The energy/cost associated with generating low-fidelity data is generally much
382"
BROADER IMPACT,0.7508305647840532,"smaller than that of high-fidelity data. By leveraging low-fidelity data to improve the learning of RL
383"
BROADER IMPACT,0.7524916943521595,"agents, greener agents are realized. Negative impacts: Running multifidelity RL agent training with
384"
BROADER IMPACT,0.7541528239202658,"weakly-correlated low- and high-fidelity environments can be wasteful of resources since the benefits
385"
BROADER IMPACT,0.7558139534883721,"in this case are not significant.
386"
REFERENCES,0.7574750830564784,"References
387"
REFERENCES,0.7591362126245847,"[1] Pieter Abbeel, Morgan Quigley, and Andrew Y Ng. Using inaccurate models in reinforcement
388"
REFERENCES,0.760797342192691,"learning. In Proceedings of the 23rd international conference on Machine Learning, pages 1–8,
389"
REFERENCES,0.7624584717607974,"2006.
390"
REFERENCES,0.7641196013289037,"[2] Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
391"
REFERENCES,0.7657807308970099,"[3] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network
392"
REFERENCES,0.7674418604651163,"architectures using reinforcement learning. arXiv preprint arXiv:1611.02167, 2016.
393"
REFERENCES,0.7691029900332226,"[4] Prasanna Balaprakash, Romain Egele, Misha Salim, Stefan Wild, Venkatram Vishwanath,
394"
REFERENCES,0.770764119601329,"Fangfang Xia, Tom Brettin, and Rick Stevens. Scalable reinforcement-learning-based neural
395"
REFERENCES,0.7724252491694352,"architecture search for cancer deep learning research. In Proceedings of the International
396"
REFERENCES,0.7740863787375415,"Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–33,
397"
REFERENCES,0.7757475083056479,"2019.
398"
REFERENCES,0.7774086378737541,"[5] Richard Bellman. A Markovian decision process. Journal of Mathematics and Mechanics,
399"
REFERENCES,0.7790697674418605,"6(5):679–684, 1957.
400"
REFERENCES,0.7807308970099668,"[6] S.N. Bernstein. On a modification of Chebyshev’s inequality and of the error formula of Laplace.
401"
REFERENCES,0.782392026578073,"Ann. Sci. Inst. Sav. Ukraine, Sect. Math. 1, 4(5), 1924.
402"
REFERENCES,0.7840531561461794,"[7] Dimitri P Bertsekas et al. Dynamic programming and optimal control: Vol. 1. Athena Scientific
403"
REFERENCES,0.7857142857142857,"Belmont, 2000.
404"
REFERENCES,0.7873754152823921,"[8] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff,
405"
REFERENCES,0.7890365448504983,"and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real
406"
REFERENCES,0.7906976744186046,"world experience. In 2019 International Conference on Robotics and Automation (ICRA), pages
407"
REFERENCES,0.792358803986711,"8973–8979. IEEE, 2019.
408"
REFERENCES,0.7940199335548173,"[9] Mark Cutler, Thomas J Walsh, and Jonathan P How. Real-world reinforcement learning via
409"
REFERENCES,0.7956810631229236,"multifidelity simulators. IEEE Transactions on Robotics, 31(3):655–671, 2015.
410"
REFERENCES,0.7973421926910299,"[10] Xuanyi Dong and Yi Yang. NAS-Bench-201: Extending the scope of reproducible neural
411"
REFERENCES,0.7990033222591362,"architecture search. arXiv preprint arXiv:2001.00326, 2020.
412"
REFERENCES,0.8006644518272426,"[11] M Giselle Fernández-Godino, Chanyoung Park, Nam-Ho Kim, and Raphael T Haftka. Review
413"
REFERENCES,0.8023255813953488,"of multi-fidelity models. arXiv preprint arXiv:1609.07196, 2016.
414"
REFERENCES,0.8039867109634552,"[12] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
415"
REFERENCES,0.8056478405315615,"policy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy
416"
REFERENCES,0.8073089700996677,"and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine
417"
REFERENCES,0.8089700996677741,"Learning, volume 80 of Proceedings of Machine Learning Research, pages 1861–1870. PMLR,
418"
REFERENCES,0.8106312292358804,"10–15 Jul 2018.
419"
REFERENCES,0.8122923588039868,"[13] Yesmina Jaafra, Jean Luc Laurent, Aline Deruyver, and Mohamed Saber Naceur. A review
420"
REFERENCES,0.813953488372093,"of meta-reinforcement learning for deep neural networks architecture search. arXiv preprint
421"
REFERENCES,0.8156146179401993,"arXiv:1812.07995, 2018.
422"
REFERENCES,0.8172757475083057,"[14] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
423"
REFERENCES,0.8189368770764119,"based policy optimization. Advances in Neural Information Processing Systems, 32:12519–
424"
REFERENCES,0.8205980066445183,"12530, 2019.
425"
REFERENCES,0.8222591362126246,"[15] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning.
426"
REFERENCES,0.8239202657807309,"In In Proc. 19th International Conference on Machine Learning. Citeseer, 2002.
427"
REFERENCES,0.8255813953488372,"[16] Lodewijk Kallenberg. Markov decision processes. Lecture Notes. University of Leiden, 2011.
428"
REFERENCES,0.8272425249169435,"[17] Michael Kearns, Yishay Mansour, and Andrew Y Ng. A sparse sampling algorithm for near-
429"
REFERENCES,0.8289036544850499,"optimal planning in large Markov decision processes. Machine Learning, 49(2):193–208,
430"
REFERENCES,0.8305647840531561,"2002.
431"
REFERENCES,0.8322259136212624,"[18] Sami Khairy, Ruslan Shaydulin, Lukasz Cincio, Yuri Alexeev, and Prasanna Balaprakash.
432"
REFERENCES,0.8338870431893688,"Learning to optimize variational quantum circuits to solve combinatorial problems. In AAAI,
433"
REFERENCES,0.8355481727574751,"pages 2367–2375, 2020.
434"
REFERENCES,0.8372093023255814,"[19] Phaedon-Stelios Koutsourelakis. Accurate uncertainty quantification using inaccurate computa-
435"
REFERENCES,0.8388704318936877,"tional models. SIAM Journal on Scientific Computing, 31(5):3274–3300, 2009.
436"
REFERENCES,0.840531561461794,"[20] Christiane Lemieux. Control variates. Wiley StatsRef: Statistics Reference Online, pages 1–8,
437"
REFERENCES,0.8421926910299004,"2014.
438"
REFERENCES,0.8438538205980066,"[21] Shibo Li, Wei Xing, Robert Kirby, and Shandian Zhe. Multi-fidelity Bayesian optimization via
439"
REFERENCES,0.845514950166113,"deep neural networks. Advances in Neural Information Processing Systems, 33, 2020.
440"
REFERENCES,0.8471760797342193,"[22] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
441"
REFERENCES,0.8488372093023255,"David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
442"
REFERENCES,0.8504983388704319,"preprint arXiv:1509.02971, 2015.
443"
REFERENCES,0.8521594684385382,"[23] Timothy A Mann and Yoonsuck Choe. Directed exploration in reinforcement learning with
444"
REFERENCES,0.8538205980066446,"transferred knowledge. In European Workshop on Reinforcement Learning, pages 59–76.
445"
REFERENCES,0.8554817275747508,"PMLR, 2013.
446"
REFERENCES,0.8571428571428571,"[24] Xuhui Meng and George Em Karniadakis. A composite neural network that learns from multi-
447"
REFERENCES,0.8588039867109635,"fidelity data: Application to function approximation and inverse PDE problems. Journal of
448"
REFERENCES,0.8604651162790697,"Computational Physics, 401:109020, 2020.
449"
REFERENCES,0.8621262458471761,"[25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
450"
REFERENCES,0.8637873754152824,"Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
451"
REFERENCES,0.8654485049833887,"Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.
452"
REFERENCES,0.867109634551495,"[26] Benjamin P Moster, Thorsten Naab, Magnus Lindström, and Joseph A O’Leary. GalaxyNet:
453"
REFERENCES,0.8687707641196013,"connecting galaxies and dark matter haloes with deep neural networks and reinforcement
454"
REFERENCES,0.8704318936877077,"learning in large volumes. Monthly Notices of the Royal Astronomical Society, 507(2):2115–
455"
REFERENCES,0.872093023255814,"2136, 2021.
456"
REFERENCES,0.8737541528239202,"[27] Benjamin Peherstorfer, Karen Willcox, and Max Gunzburger. Optimal model management for
457"
REFERENCES,0.8754152823920266,"multifidelity Monte Carlo estimation. SIAM Journal on Scientific Computing, 38(5):A3163–
458"
REFERENCES,0.8770764119601329,"A3194, 2016.
459"
REFERENCES,0.8787375415282392,"[28] Benjamin Peherstorfer, Karen Willcox, and Max Gunzburger. Survey of multifidelity methods
460"
REFERENCES,0.8803986710963455,"in uncertainty propagation, inference, and optimization. Siam Review, 60(3):550–591, 2018.
461"
REFERENCES,0.8820598006644518,"[29] Paris Perdikaris, Maziar Raissi, Andreas Damianou, Neil D Lawrence, and George Em
462"
REFERENCES,0.8837209302325582,"Karniadakis. Nonlinear information fusion algorithms for data-efficient multi-fidelity mod-
463"
REFERENCES,0.8853820598006644,"elling. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences,
464"
REFERENCES,0.8870431893687708,"473(2198):20160751, 2017.
465"
REFERENCES,0.8887043189368771,"[30] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Si-
466"
REFERENCES,0.8903654485049833,"mon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering
467"
REFERENCES,0.8920265780730897,"Atari, Go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609,
468"
REFERENCES,0.893687707641196,"2020.
469"
REFERENCES,0.8953488372093024,"[31] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
470"
REFERENCES,0.8970099667774086,"policy optimization. In International conference on Machine Learning, pages 1889–1897.
471"
REFERENCES,0.8986710963455149,"PMLR, 2015.
472"
REFERENCES,0.9003322259136213,"[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
473"
REFERENCES,0.9019933554817275,"policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
474"
REFERENCES,0.9036544850498339,"[33] Varun Suryan, Nahush Gondhalekar, and Pratap Tokekar. Multifidelity reinforcement learning
475"
REFERENCES,0.9053156146179402,"with Gaussian processes: model-based and model-free algorithms. IEEE Robotics & Automation
476"
REFERENCES,0.9069767441860465,"Magazine, 27(2):117–128, 2020.
477"
REFERENCES,0.9086378737541528,"[34] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press,
478"
REFERENCES,0.9102990033222591,"2018.
479"
REFERENCES,0.9119601328903655,"[35] Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A
480"
REFERENCES,0.9136212624584718,"survey. Journal of Machine Learning Research, 10(7), 2009.
481"
REFERENCES,0.915282392026578,"[36] Matthew E Taylor, Peter Stone, and Yaxin Liu. Transfer learning via inter-task mappings for
482"
REFERENCES,0.9169435215946844,"temporal difference learning. Journal of Machine Learning Research, 8(9), 2007.
483"
REFERENCES,0.9186046511627907,"[37] Colin White, Willie Neiswanger, and Yash Savani. Bananas: Bayesian optimization with neural
484"
REFERENCES,0.920265780730897,"architectures for neural architecture search. arXiv preprint arXiv:1910.11858, 1(2):4, 2019.
485"
REFERENCES,0.9219269102990033,"[38] Christopher K Williams and Carl Edward Rasmussen. Gaussian processes for machine learning,
486"
REFERENCES,0.9235880398671097,"volume 2. MIT Press, Cambridge, MA, 2006.
487"
REFERENCES,0.925249169435216,"[39] Zhuangdi Zhu, Kaixiang Lin, and Jiayu Zhou. Transfer learning in deep reinforcement learning:
488"
REFERENCES,0.9269102990033222,"A survey. arXiv preprint arXiv:2009.07888, 2020.
489"
REFERENCES,0.9285714285714286,"[40] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv
490"
REFERENCES,0.9302325581395349,"preprint arXiv:1611.01578, 2016.
491"
REFERENCES,0.9318936877076412,"Checklist
492"
REFERENCES,0.9335548172757475,"1. For all authors...
493"
REFERENCES,0.9352159468438538,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
494"
REFERENCES,0.9368770764119602,"contributions and scope? [Yes] See Figure 1.
495"
REFERENCES,0.9385382059800664,"(b) Did you describe the limitations of your work? [Yes] Refer to Section 4.1
496"
REFERENCES,0.9401993355481728,"(c) Did you discuss any potential negative societal impacts of your work? [Yes] Refer to
497"
REFERENCES,0.9418604651162791,"Section 6.
498"
REFERENCES,0.9435215946843853,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
499"
REFERENCES,0.9451827242524917,"them? [Yes]
500"
REFERENCES,0.946843853820598,"2. If you are including theoretical results...
501"
REFERENCES,0.9485049833887044,"(a) Did you state the full set of assumptions of all theoretical results? [Yes] Refer to the
502"
REFERENCES,0.9501661129568106,"theorem statements in Section 3.3.
503"
REFERENCES,0.9518272425249169,"(b) Did you include complete proofs of all theoretical results? [Yes] Refer to Appendix B.
504"
REFERENCES,0.9534883720930233,"3. If you ran experiments...
505"
REFERENCES,0.9551495016611296,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
506"
REFERENCES,0.9568106312292359,"mental results (either in the supplemental material or as a URL)? [Yes] Our codes are
507"
REFERENCES,0.9584717607973422,"included in the supplemental materials and will be shared online after a decision is
508"
REFERENCES,0.9601328903654485,"made on the manuscript.
509"
REFERENCES,0.9617940199335548,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
510"
REFERENCES,0.9634551495016611,"were chosen)? [Yes] Refer to Section 4 and Appendix C.
511"
REFERENCES,0.9651162790697675,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
512"
REFERENCES,0.9667774086378738,"ments multiple times)? [Yes] See Figures 2 and 3.
513"
REFERENCES,0.96843853820598,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
514"
REFERENCES,0.9700996677740864,"of GPUs, internal cluster, or cloud provider)? [Yes] Refer to Appendix C.1.
515"
REFERENCES,0.9717607973421927,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
516"
REFERENCES,0.973421926910299,"(a) If your work uses existing assets, did you cite the creators? [Yes] Refer to Section 4.2.
517"
REFERENCES,0.9750830564784053,"(b) Did you mention the license of the assets? [No] The dataset used in this work is
518"
REFERENCES,0.9767441860465116,"publicly available under the MIT License.
519"
REFERENCES,0.978405315614618,"(c) Did you include any new assets either in the supplemental material or as a URL? [No]
520"
REFERENCES,0.9800664451827242,"Synthetic data used in Section 4.1 can be regenerated by using the codes we provided.
521"
REFERENCES,0.9817275747508306,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
522"
REFERENCES,0.9833887043189369,"using/curating? [N/A]
523"
REFERENCES,0.9850498338870431,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
524"
REFERENCES,0.9867109634551495,"information or offensive content? [N/A]
525"
REFERENCES,0.9883720930232558,"5. If you used crowdsourcing or conducted research with human subjects...
526"
REFERENCES,0.9900332225913622,"(a) Did you include the full text of instructions given to participants and screenshots, if
527"
REFERENCES,0.9916943521594684,"applicable? [N/A]
528"
REFERENCES,0.9933554817275747,"(b) Did you describe any potential participant risks, with links to Institutional Review
529"
REFERENCES,0.9950166112956811,"Board (IRB) approvals, if applicable? [N/A]
530"
REFERENCES,0.9966777408637874,"(c) Did you include the estimated hourly wage paid to participants and the total amount
531"
REFERENCES,0.9983388704318937,"spent on participant compensation? [N/A]
532"
