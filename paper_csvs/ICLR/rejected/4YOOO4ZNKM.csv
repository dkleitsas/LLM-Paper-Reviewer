Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.006172839506172839,"The sequential recommendation aims at predicting the next items in user behav-
iors, which can be solved by characterizing item relationships in sequences. Due
to the data sparsity and noise issues in sequences, a new self-supervised learning
(SSL) paradigm is proposed to improve the performance, which employs con-
trastive learning between positive and negative views of sequences. However,
existing methods all construct views by adopting augmentation from data per-
spectives, while we argue that 1) optimal data augmentation methods are hard to
devise, 2) data augmentation methods destroy sequential correlations, and 3) data
augmentation fails to incorporate comprehensive self-supervised signals. There-
fore, we investigate the possibility of model augmentation to construct view pairs.
We propose three levels of model augmentation methods: neuron masking, layer
dropping, and encoder complementing. This work opens up a novel direction in
constructing views for contrastive SSL. Experiments verify the efÔ¨Åcacy of model
augmentation for the SSL in the sequential recommendation."
INTRODUCTION,0.012345679012345678,"1
INTRODUCTION"
INTRODUCTION,0.018518518518518517,"The sequential recommendation (Fan et al., 2021; Liu et al., 2021c; Chen et al., 2018; Tang &
Wang, 2018; Zheng et al., 2019) aims at predicting future items in sequences, where the crucial part
is to characterize item relationships in sequences. Recent developments in sequence modeling (Fan
et al., 2021; Liu et al., 2021c) verify the superiority of Transform (Vaswani et al., 2017), i.e. the self-
attention mechanism, in revealing item correlations in sequences. A Transformer (Kang & McAuley,
2018) is able to infer the sequence embedding at speciÔ¨Åed positions by weighted aggregation of item
embeddings, where the weights are learned via self-attention. Existing works (Fan et al., 2021; Wu
et al., 2020) further improve Transformer by incorporating additional complex signals."
INTRODUCTION,0.024691358024691357,"However, the data sparsity issue (Liu et al., 2021c) and noise in sequences undermine the perfor-
mance of a model in sequential recommendation. The former hinders performance due to insuf-
Ô¨Åcient training since the complex structure of a sequential model requires a dense corpus to be
adequately trained. The latter also impedes the recommendation ability of a model because noisy
item sequences are unable to reveal actual item correlations. To overcome both, a new contrastive
self-supervised learning (SSL) paradigm (Liu et al., 2021b; Xie et al., 2020; Zhou et al., 2020)
is proposed recently. This paradigm enhances the capacity of encoders by leveraging additional
self-supervised signals. SpeciÔ¨Åcally, the SSL paradigm constructs positive view pairs as two data
augmentations from the same sequences (Xie et al., 2020), while negative pairs are augmentations
from distinct sequences. Incorporating augmentations during training increases the amount of train-
ing data, thus alleviating the sparsity issue. And the contrastive loss (Chen et al., 2020) improves
the robustness of the model, which endows a model with the ability to against noise."
INTRODUCTION,0.030864197530864196,"Though being effective in enhancing sequential modeling, the data augmentation methods adopted
in the existing SSL paradigm suffer from the following weaknesses:"
INTRODUCTION,0.037037037037037035,"‚Ä¢ Optimal data augmentation methods are hard to devise. Current sequence augmentation meth-
ods adopts random sequence perturbations (Liu et al., 2021b; Xie et al., 2020), which includes
crop, mask, reorder, substitute and insert operations. Though a random combination of those aug-
menting operations improves the performance, it is rather time-consuming to search the optimal"
INTRODUCTION,0.043209876543209874,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.04938271604938271,"augmentation methods from a large number of potential combinations for different datasets (Liu
et al., 2021b)."
INTRODUCTION,0.05555555555555555,"‚Ä¢ Data augmentation methods destroy sequential correlations, leading to less conÔ¨Ådent positive
pairs. The existing SSL paradigm requires injecting perturbations into the augmented views of
sequences for contrastive learning. However, because the view construction process is not opti-
mized to characterize sequential correlations, two views of one sequence may reveal distinct item
relationships, which should not be recognized as positive pairs."
INTRODUCTION,0.06172839506172839,"‚Ä¢ Data augmentation fails to incorporate comprehensive self-supervised signals. Current data aug-
mentation methods are designed based on heuristics, which already requires additional prior
knowledge. Moreover, since the view construction process is not optimized with the encoder,
data augmentation may only reveal partial self-supervised signals from data perspectives. Hence,
we should consider other types of views besides data augmentation."
INTRODUCTION,0.06790123456790123,"Therefore, we investigate the possibility of model augmentation to construct view pairs for con-
trastive learning, which functions as a complement to the data augmentation methods. We hypothe-
sis that injecting perturbations into the encoder should enhance the self-supervised learning ability to
existing paradigms. The reasons are threefold: Firstly, model augmentation is jointly trained with the
optimization process, thus endows the end-to-end training fashion. As such, it is easy to discover the
optimal view pairs for contrastive learning. Secondly, model augmentation constructs views without
manipulation to the original data, which leads to high conÔ¨Ådence of positive pairs. Last but not
least, injecting perturbation into the encoder has distinct characteristics to data augmentation, which
should be an important complement in constructing view pairs for existing self-supervised learning
scheme (Liu et al., 2021b; Zhou et al., 2020)."
INTRODUCTION,0.07407407407407407,"This work studies the model augmentation for a self-supervised sequential recommendation from
three levels: 1) neuron masking (dropout), which adopts the dropout layer to randomly mask par-
tial neurons in a layer. By operating the dropout twice to one sequence, we can perturb the output
of the embedding from this layer, which thus constructs two views from model augmentation per-
spective (Gao et al., 2021). 2) layer dropping. Compared with neuron masks, we randomly drop
a complete layer in the encoder to inject more perturbations. By randomly dropping layers in an
encoder twice, we construct two distinct views. Intuitively, layer-drop augmentation enforces the
contrast between deep features and shallows features of the encoder. 3) encoder complementing,
which leverages other encoders to generate sequence embeddings. Encoder complementing aug-
mentation is able to fuse distinct sequential correlations revealed by different types of encoders.
For example, RNN-based sequence encoder (Hidasi et al., 2015) can better characterize direct item
transition relationships, while Transformer-based sequence encoder models position-wise sequen-
tial correlations. Though only investigating SSL for a sequential recommendation, we remark that
model augmentation methods can also be applied in other SSL scenarios. The contributions are as
follows:"
INTRODUCTION,0.08024691358024691,"‚Ä¢ We propose a new contrastive SSL paradigm for sequential recommendation by constructing views
from model augmentation, which is named as SRMA."
INTRODUCTION,0.08641975308641975,‚Ä¢ We introduce three levels of model augmentation methods for constructing view pairs.
INTRODUCTION,0.09259259259259259,"‚Ä¢ We discuss the effectiveness and conduct a comprehensive study of model augmentations for the
sequential recommendation."
INTRODUCTION,0.09876543209876543,‚Ä¢ We investigate the efÔ¨Åcacy of different variants of model augmentation.
RELATED WORK,0.10493827160493827,"2
RELATED WORK"
SEQUENTIAL RECOMMENDATION,0.1111111111111111,"2.1
SEQUENTIAL RECOMMENDATION"
SEQUENTIAL RECOMMENDATION,0.11728395061728394,"Sequential recommendation predicts future items in user sequences by encoding sequences while
modeling item transition correlations (Rendle et al., 2010; Hidasi et al., 2015). Previously, Recurrent
Neural Network (RNN) have been adapted to sequential recommendation (Hidasi et al., 2015; Wu
et al., 2017), ostensibly modeling sequence-level item transitions. Hierarchical RNNs (Quadrana
et al., 2017) incorporate personalization information. Moreover, both long-term and short-term item"
SEQUENTIAL RECOMMENDATION,0.12345679012345678,Under review as a conference paper at ICLR 2022
SEQUENTIAL RECOMMENDATION,0.12962962962962962,"transition correlations are modelled in LSTM (Wu et al., 2017) . Recently, the success of self-
attention models (Vaswani et al., 2017; Devlin et al., 2018) promotes the prosperity of Transformer-
based sequential recommendation models. SASRec (Kang & McAuley, 2018) is a pioneering work
adapting Transformer to characterize complex item transition correlations. BERT4Rec (Sun et al.,
2019) adopts the bidirectional Transformer layer to encode sequence. ASReP (Liu et al., 2021c)
reversely pre-training a Transformer to augment short sequences and Ô¨Åne-tune it to predict the next-
item in sequences. TGSRec (Fan et al., 2021) models temporal collaborative signals in sequences to
recognize item relationships."
SELF-SUPERVISED LEARNING,0.13580246913580246,"2.2
SELF-SUPERVISED LEARNING"
SELF-SUPERVISED LEARNING,0.1419753086419753,"Self-supervised learning (SSL) is proposed recently to describe ‚Äúthe machine predicts any parts of
its input for any observed part‚Äù(Bengio et al., 2021), which stays within the narrow scope of unsu-
pervised learning. To achieve the self-prediction, endeavors from various domains have developed
different SSL schemes from either generative or contrastive perspectives (Liu et al., 2021a). For
generative SSL, the masked language model is adopted in BERT (Devlin et al., 2018) to generate
masked words in sentences. GPT-GNN (Hu et al., 2020) also generates masked edges to realize SSL.
Other generative SSL paradigms in computer vision (Oord et al., 2016) are proposed. Compared
with generative SSL, contrastive SSL schemes have demonstrated more promising performance.
SimCLR (Chen et al., 2020) proposes simple contrastive learning between augmented views for im-
ages, which is rather effective in achieving SSL. GCC (Qiu et al., 2020) and GraphCL (You et al.,
2020) adopts contrastive learning between views from corrupted graph structures. CL4SRec (Xie
et al., 2020) and CoSeRec (Liu et al., 2021b) devise the sequence augmentation methods for SSL
on sequential recommendation. This paper also investigates the contrastive SSL for a sequential
recommendation. Instead of adopting the data augmentation for constructing views to contrast, we
propose the model augmentation to generate contrastive views."
PRELIMINARY,0.14814814814814814,"3
PRELIMINARY"
PROBLEM FORMULATION,0.15432098765432098,"3.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.16049382716049382,"We denote user and item sets as U and V respectively. Each user u ‚ààU is associated with a sequence
of items in chronological order su = [v1, . . . , vt, . . . , v|su|], where vt ‚ààV denotes the item that u
has interacted with at time t and |su| is the total number of items. Sequential recommendation is
formulated as follows:
arg max
vi‚ààV
P(v|su|+1 = vi |su ),
(1)"
PROBLEM FORMULATION,0.16666666666666666,"where v|su|+1 denotes the next item in sequence. Intuitively, we calculate the probability of all
candidate items and recommend items with high probability scores."
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.1728395061728395,"3.2
SEQUENTIAL RECOMMENDATION FRAMEWORK"
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.17901234567901234,"The core of a generic sequential recommendation framework is a sequence encoder SeqEnc(¬∑),
which transforms item sequences to embeddings for scoring. We formulate the encoding step as:"
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.18518518518518517,"hu = SeqEnc(su),
(2)"
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.19135802469135801,"where hu denotes the sequence embedding of su. To be speciÔ¨Åc, if we adopt a Transformer (Kang
& McAuley, 2018; Vaswani et al., 2017) as the encoder, hu is a bag of embeddings, where at
each position t, ht
u, represents a predicted next-item. We adopt the log-likelihood loss function to
optimize the encoder for next-item prediction as follows:"
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.19753086419753085,"Lrec(u, t) = ‚àílog(œÉ(ht
u ¬∑ evt+1)) ‚àí
X"
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.2037037037037037,"vjÃ∏‚ààsu
log(1 ‚àíœÉ(ht
u ¬∑ evj)),
(3)"
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.20987654320987653,"where Lrec(u, t) denotes the loss score for the prediction at position t in sequence su, œÉ is the non-
linear activation function, evt+1 denotes the embedding for item vt+1, and vj is the sampled negative
item for su. The embeddings of items are retrieved from the embedding layer in SeqEnc(¬∑), which
is jointly optimized with other layers."
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.21604938271604937,Under review as a conference paper at ICLR 2022
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.2222222222222222,Encoder
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.22839506172839505,Augmented Seq. ùëÜ1
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.2345679012345679,"ùë£1
ùë£2
ùë£ùëõ
‚Ä¶"
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.24074074074074073,Original Sequence ùëÜ
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.24691358024691357,Embedding
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.25308641975308643,"Concat
Concat"
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.25925925925925924,Contrast
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.2654320987654321,"Model Augmentation
Model Augmentation"
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.2716049382716049,Encoder
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.2777777777777778,Augmented Seq. ùëÜ2
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.2839506172839506,Embedding FFN
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.29012345679012347,Add & Norm
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.2962962962962963,Input Embedding
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.30246913580246915,Output
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.30864197530864196,Neuron Mask
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.3148148148148148,Sequence
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.32098765432098764,Encoder
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.3271604938271605,FFN - 1 1
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.3333333333333333,FFN - 2 ‚Ä¶
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.3395061728395062,FFN - K
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.345679012345679,Output
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.35185185185185186,Sequence
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.35802469135802467,Encoder
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.36419753086419754,Pre-trained
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.37037037037037035,Encoder
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.3765432098765432,Add & Norm FFN
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.38271604938271603,Output
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.3888888888888889,"(a) Contrastive Self-supervised Learning
(b) Neuron Masking
(c) Layer Dropping
(d) Encoder Complementing ùõæ"
SEQUENTIAL RECOMMENDATION FRAMEWORK,0.3950617283950617,"Figure 1: (a) The contrastive SSL framework with model augmentation. We apply the model aug-
mentation to the encoder, which constructs two views for contrastive learning. (b) the neuron mask-
ing augmentation. We demonstrate the neuron masking for the Feed-Forward network. (c) the
layer dropping augmentation. We add K FFN layers after the encoder and randomly drop M lay-
ers (dash blocks) during each batch of training. And (d) the encoder complementing augmentation.
We pre-train another encoder for generating the embedding of sequences. The embedding from the
pre-trained encoder is combined with the model encoder for contrastive learning."
CONTRASTIVE SELF-SUPERVISED LEARNING PARADIGM,0.4012345679012346,"3.3
CONTRASTIVE SELF-SUPERVISED LEARNING PARADIGM"
CONTRASTIVE SELF-SUPERVISED LEARNING PARADIGM,0.4074074074074074,"Other than the next-item prediction, we can leverage other pretext tasks (Sun et al., 2019; Liu et al.,
2021a;b) over the sequence to optimize the encoder, which harnesses the self-supervised signals
within the sequence. This paper investigate the widely adopted contrastive SSL scheme (Liu et al.,
2021b; Xie et al., 2020). This scheme constructs positive and negative view pairs from sequences,
and employs the contrastive loss (Oord et al., 2018) to optimize the encoder. We formulate the SSL
step as follows:"
CONTRASTIVE SELF-SUPERVISED LEARNING PARADIGM,0.41358024691358025,"Lssl(Àúh2u‚àí1, Àúh2u) = ‚àílog
exp(sim(Àúh2u‚àí1, Àúh2u))
P2N
m=1 1mÃ∏=2u‚àí1 exp(sim(Àúh2u‚àí1, Àúhm))
,
(4)"
CONTRASTIVE SELF-SUPERVISED LEARNING PARADIGM,0.41975308641975306,"where Àúh2u and Àúh2u‚àí1 denotes two views constructed for sequence su. 1 is an indication function.
sim(¬∑, ¬∑) is the similarity function, e.g. dot-product. Since each sequence has two view, we have
2N samples in a batch with N sequences for training. The nominator indicates the agreement
maximization between a positive pair, while the denominator is interpreted as push away those
negative pairs. Existing works apply data augmentation for sequences to construct views, e.g. Xie
et al. (2020) propose crop, mask, and reorder a sequence and (Liu et al., 2021b) devises insert and
substitute sequence augmentations. For sequential recommendation, since both SSL and the next-
item prediction charaterize the item relationships in sequences, we add them together to optimize
the encoder. Therefore, the Ô¨Ånal loss L = Lrec + ŒªLssl. Compared with them, we adopt both the
data augmentation and model augmentation to generate views for contrast. We demonstrate the
contrastive SSL step in Figure 1(a)."
MODEL AUGMENTATION,0.42592592592592593,"4
MODEL AUGMENTATION"
MODEL AUGMENTATION,0.43209876543209874,"In this section, we introduce the model augmentation to construct views for sequences. We dis-
cuss three type of model augmentation methods, which are neuron mask, layer drop and encoder
complement. We illustrate these augmentation methods in Figure 1(b), 1(c) and 1(d), respectively."
NEURON MASKING,0.4382716049382716,"4.1
NEURON MASKING"
NEURON MASKING,0.4444444444444444,"This work adopts the Transformer as the sequence encoder, which passes the hidden embeddings to
the next layer through a feed-forward network (FFN). During training, we randomly mask partial
neurons in each FFN layer, which involves a masking probability p. The large value of p leads
to intensive embedding perturbations. As such, we generate a pair of views from one sequence
from model perspectives. Besides, during each batch of training, the masked neurons are randomly"
NEURON MASKING,0.4506172839506173,Under review as a conference paper at ICLR 2022
NEURON MASKING,0.4567901234567901,"selected, which results in comprehensive contrastive learning on model augmentation. Note that,
though we can utilize different probability values for distinct FFN layers, we enforce their neuron
masking probability to be the same for simplicity. The neuron masking augmentation on FFN is
shown in Figure 1(b). Additionally, we remark that the neuron mask can be applied to any neural
layers in a model to inject more perturbations."
LAYER DROPPING,0.46296296296296297,"4.2
LAYER DROPPING"
LAYER DROPPING,0.4691358024691358,"Dropping partial layers of a model decreases the depth and reduces complexity. Previous research ar-
gues that most recommender systems require only shallow embeddings for users and items (Dacrema
et al., 2019). Therefore, it is reasonable to randomly drop a fraction of layers during training, which
functions as a way of regularization. Additionally, existing works (Liu et al., 2020; He et al., 2016)
claim that embeddings at shallow layers and deep layers are both important to reÔ¨Çect the compre-
hensive information of the data. Dropping layers enable contrastive learning between shallow em-
beddings and deep embeddings, thus being an enhancement of existing works that only contrasting
between deep features."
LAYER DROPPING,0.47530864197530864,"On the other hand, dropping layers, especially those necessary layers in a model, may destroy orig-
inal sequential correlations. Thus, views generating by dropping layers may not be a positive pair.
To this end, instead of manipulating the original encoder, we stack K FFN layers after the encoder
and randomly drop M of them during each batch of training, where M < K. We illustrate the layer
dropping as in Figure 1(c), where we append K additional FFN layers after the encoder and use
dash blocks to denote the dropped layers."
ENCODER COMPLEMENTING,0.48148148148148145,"4.3
ENCODER COMPLEMENTING"
ENCODER COMPLEMENTING,0.4876543209876543,"During self-supervised learning, we employ one encoder to generate embeddings of two views of
one sequence. Though this encoder can be effective in revealing complex sequential correlations,
contrasting on one single encoder may result in embedding collapse problems for self-supervised
learning (Hua et al., 2021). Moreover, one single encoder is only able to reÔ¨Çect the item relationships
from a unitary perspective. For example, the Transformer encoder adopts the attentive aggregation
of item embeddings to infer sequence embedding, while an RNN structure (Hidasi et al., 2015) is
more suitable in encoding direct item transitions. Therefore, contrasting between views from distinct
encoders enables the model to learn comprehensive sequential relationships of items."
ENCODER COMPLEMENTING,0.49382716049382713,"However, embeddings from two views of a sequence with distinct encoders lead to a non-Siamese
paradigm for self-supervised learning, which is hard to train and suffers the embedding collapse
problem (Koch et al., 2015; Chen & He, 2021). Additionally, if two distinct encoders reveal sig-
niÔ¨Åcantly diverse sequential correlations, the embeddings are far away from each other, thus being
bad views for contrastive learning (Tian et al., 2020). Moreover, though we can optimize two en-
coders during a training phase, it is still problematic to combine them for the inference of sequence
embeddings to conduct recommendations."
ENCODER COMPLEMENTING,0.5,"As a result, instead of contrastive learning with distinct encoders, we harness another pre-trained
encoder as an encoder complementing model augmentation for the original encoder. To be more
speciÔ¨Åc, we Ô¨Årst pre-train another encoder with the next-item prediction target. Then, in the self-
supervised training stage, we utilize this pre-trained encoder to generate another embedding for a
view. After that, we add the view embeddings from a model encoder and the pre-trained encoder.
We illustrate the encoder complementing augmentation in Figure 1(d). Note that we only apply this
model augmentation in one branch of the SSL paradigm. And the embedding from the pre-trained
encoder is re-scaled by a hyper-parameter Œ≥ before adding to the embedding from the framework‚Äôs
encoder. The smaller value of Œ≥ implies injecting fewer perturbations from a distinct encoder. And
the parameters of this pre-trained encoder are Ô¨Åxed during training. Hence, there is no optimization
for this pre-trained encoder and it is no longer required to take account of both encoders during the
inference stage."
ENCODER COMPLEMENTING,0.5061728395061729,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.5123456790123457,"5
EXPERIMENTS"
EXPERIMENTAL SETTINGS,0.5185185185185185,"5.1
EXPERIMENTAL SETTINGS"
EXPERIMENTAL SETTINGS,0.5246913580246914,"Dataset We conduct experiments on three public datasets. Amazon Sports, Amazon Toys and
Games (McAuley et al., 2015) and Yelp1, which are Amazon review data in Sport and Toys cate-
gories, and a dataset for the business recommendation, respectively. We follow common practice
in (Liu et al., 2021c; Xie et al., 2020) to only keep the ‚Äò5-core‚Äô sequences. In total, Sports dataset
has 35,598 users, 18,357 items and 296,337 interactions. Toys dataset contains 19,412 users, 11,924
items, and 167,597interactions. Yelp dataset consists 30,431 users, 20,033 items and 316,354 inter-
actions."
EXPERIMENTAL SETTINGS,0.5308641975308642,"Evaluation Metrics We follow (Wang et al., 2019; Krichene & Rendle, 2020; Liu et al., 2021c) to
evaluate models‚Äô performances based on the whole item set without negative sampling and report
standard Hit Ratio@k (HR@k) and Normalized Discounted Cumulative Gain@k (NDCG@k) on all
datasets, where k ‚àà{5, 10, 20}."
EXPERIMENTAL SETTINGS,0.5370370370370371,"Baselines We include two groups of sequential models as baselines for comprehensive compar-
isons. The Ô¨Årst group baselines are sequential models that use different deep neural architectures to
encode sequences with a supervised objective. These include GRU4Rec (Hidasi et al., 2015) as an
RNN-based method, Caser (Tang & Wang, 2018) as a CNN-based approach, and SASRec (Kang &
McAuley, 2018) as one of the state-of-the-art Transformer based solution. The second group base-
lines additionally leverage SSL objective. BERT4Rec (Sun et al., 2019) employs a Cloze task (Tay-
lor, 1953) as a generative self-supervised learning sigal. S3Rec (Zhou et al., 2020) uses contrastive
SSL with ‚Äòmask‚Äô data augmentation to fuse correlation-ships among item, sub-sequence, and cor-
respondinng attributes into the networks. We remove the components for fusing attributes for fair
comparison. CL4SRec (Xie et al., 2020) maximize the agreements between two sequences augmen-
tation, where the data augmentation are randomly selected from ‚Äôcrop‚Äô, ‚Äòreorder‚Äô, and ‚Äòmask‚Äô data
augmentations. CoSeRec (Liu et al., 2021b) improves the robustness of data augmentation under
contrastive SSL framework by leveraging item-correlations."
EXPERIMENTAL SETTINGS,0.5432098765432098,"Implementation Details The model encoder in SRMA is the basic Transformer-based encoder.
We adopt the widely used SASRec encoder. The neuron masking probability is searched from
{0.0, 0.1, 0.2, . . . , 0.9}. For layer dropping, the K is searched from {1, 2, 3, 4}, and M is searched
accordingly.
As for encoder complementing, we search the re-scale hyper-parameter Œ≥ from
{0.005, 0.01, 0.05, 0.1, 0.5, 1.0} and the pre-trained encoder is selected from a 1-layer Transformer
and a GRU encoder."
OVERALL PERFORMANCE,0.5493827160493827,"5.2
OVERALL PERFORMANCE"
OVERALL PERFORMANCE,0.5555555555555556,"We compare the proposed paradigm SRMA to existing methods with respect to the performance
on the sequential recommendation. Results are demonstrated in Table 1. We can observe that
Transformer-based sequence encoders, such SASRec and BERT4Rec are better than GRU4Rec or
Caser sequence encoders. Because of this, our proposed model SRMA also adopts the Transformer
as sequence encoder. Moreover, the SSL paradigm can signiÔ¨Åcantly improve performance. For
example, the CL4SRec model, which adopts the random data augmentation, improves the perfor-
mance of SASRec on HR and NDCG by 13.2% and 9.8% on average regarding the Sports dataset,
respectively. Also, since SRMA enhances the SSL with both data augmentation and model augmen-
tation, SRMA thus outperforms all other SSL sequential recommendation models. SRMA adopts
the same data augmentation methods as CL4SRec. Nevertheless, SRMA signiÔ¨Åcantly outperforms
CL4SRec. On the sports dataset, we achieve 18.9% and 27.9% relative improvements on HR and
NDCG, respectively. On the Yelp dataset, we achieve 4.5% and 6.4% relative improvements on HR
and NDCG, respectively. And on Toys data, we achieve 8.6% and 13.8% relative improvements on
HR and NDCG, respectively. In addition, SRMA also performs better than CoSeRec which lever-
ages item correlations for data augmentation. Those results all verify the effectiveness of model
augmentation in improving the SSL paradigm."
OVERALL PERFORMANCE,0.5617283950617284,1https://www.yelp.com/dataset
OVERALL PERFORMANCE,0.5679012345679012,Under review as a conference paper at ICLR 2022
OVERALL PERFORMANCE,0.5740740740740741,"Table 1: Performance comparisons of different methods. The best score is in bold in each row, and
the second best is underlined."
OVERALL PERFORMANCE,0.5802469135802469,"Dataset Metric
GRU4Rec Caser SASRec BERT4Rec S3-Rec CL4SRec CoSeRec SRMA"
OVERALL PERFORMANCE,0.5864197530864198,Sports
OVERALL PERFORMANCE,0.5925925925925926,"HR@5
0.0162
0.0154 0.0206
0.0217
0.0121
0.0231
0.0287
0.0299
HR@10
0.0258
0.0261 0.0320
0.0359
0.0205
0.0369
0.0437
0.0447
HR@20
0.0421
0.0399 0.0497
0.0604
0.0344
0.0557
0.0635
0.0649
NDCG@5
0.0103
0.0114 0.0135
0.0143
0.0084
0.0146
0.0196
0.0199
NDCG@10
0.0142
0.0135 0.0172
0.019
0.0111
0.0191
0.0242
0.0246
NDCG@20
0.0186
0.0178 0.0216
0.0251
0.0146
0.0238
0.0292
0.0297 Yelp"
OVERALL PERFORMANCE,0.5987654320987654,"HR@5
0.0152
0.0142 0.0160
0.0196
0.0101
0.0227
0.0241
0.0243
HR@10
0.0248
0.0254 0.0260
0.0339
0.0176
0.0384
0.0395
0.0395
HR@20
0.0371
0.0406 0.0443
0.0564
0.0314
0.0623
0.0649
0.0646
NDCG@5
0.0091
0.008
0.0101
0.0121
0.0068
0.0143
0.0151
0.0154
NDCG@10
0.0124
0.0113 0.0133
0.0167
0.0092
0.0194
0.0205
0.0207
NDCG@20
0.0145
0.0156 0.0179
0.0223
0.0127
0.0254
0.0263
0.0266 Toys"
OVERALL PERFORMANCE,0.6049382716049383,"HR@5
0.0097
0.0166 0.0463
0.0274
0.0143
0.0525
0.0583
0.0598
HR@10
0.0176
0.0270 0.0675
0.0450
0.0094
0.0776
0.0812
0.0834
HR@20
0.0301
0.0420 0.0941
0.0688
0.0235
0.1084
0.1103
0.1132
NDCG@5
0.0059
0.0107 0.0306
0.0174
0.0123
0.0346
0.0399
0.0407
NDCG@10
0.0084
0.0141 0.0374
0.0231
0.0391
0.0428
0.0473
0.0484
NDCG@20
0.0116
0.0179 0.0441
0.0291
0.0162
0.0505
0.0547
0.0559"
COMPARISON BETWEEN MODEL AND DATA AUGMENTATION,0.6111111111111112,"5.3
COMPARISON BETWEEN MODEL AND DATA AUGMENTATION"
COMPARISON BETWEEN MODEL AND DATA AUGMENTATION,0.6172839506172839,"Because SRMA adopts the random sequence augmentation, we mainly focus on comparing with
CL4SRec to justify the impacts of model augmentation and data augmentation. In fact, CL4SRec
also implicitly uses the neuron masking model augmentation, where dropout layers are stacked
within its original sequence encoder. To separate the joint effects of model and data augmentation,
we create its variants ‚ÄòCL4S. p = 0‚Äô, which sets all the dropout ratios to be 0, thus disables the neu-
ron masking augmentation. Also, another variant ‚ÄòCL4S. w/o D‚Äô, which has no data augmentation
are also compared. Additionally, we create two other variants of SRMA as ‚ÄòSRMA w/o M‚Äô and
‚ÄòSRMA w/o D‚Äô by disabling the model augmentation and data augmentation respectively. ‚ÄòSRMA
w/o M‚Äô has additional FFN layers compared with ‚ÄòCL4S. p = 0‚Äô. The recommendation perfor-
mance on the Sports and Toys dataset is presented in Table 2. We have the following observations.
Firstly, we notice a signiÔ¨Åcant performance drop of the variant ‚ÄòCL4S. p = 0‚Äô, which suggests that
the neuron masking augmentation is rather crucial. It beneÔ¨Åts both the regularization of the training
encoder and model augmentation of SSL. Secondly, ‚ÄòSRMA w/o D‚Äô outperforms other baselines on
the Sports dataset and has comparable performance to ‚ÄòCL4S.‚Äô, which indicates the model augmen-
tation is of more impact in the SSL paradigm compared with data augmentation. Thirdly, SRMA
performs the best against all the variants. This result suggests that we should jointly employ the data
augmentation and model augmentation in an SSL paradigm, which contributes to comprehensive
contrastive self-supervised signals."
HYPER-PARAMETER SENSITIVITY,0.6234567901234568,"5.4
HYPER-PARAMETER SENSITIVITY"
HYPER-PARAMETER SENSITIVITY,0.6296296296296297,"In this section, we vary the hyper-parameters in neuron masking and layer dropping to draw a de-
tailed investigation of model augmentation."
HYPER-PARAMETER SENSITIVITY,0.6358024691358025,"Effect of Neuron Masking. Though all neural layers can apply the neuron masking augmenta-
tion, for simplicity, we only endow the FFN layer with the neuron masking augmentation and set
the masking probability as p for all FFN layers in the framework. We Ô¨Åx the settings of layer drop-
ping and the encoder complementing model augmentation and select p from {0.0, 0.1, 0.2, . . . , 0.9},
where 0.0 is equivalent to no neuron masking. Also, we compare SRMA with SASRec to justify the
effectiveness of the SSL paradigm. The performance curves of HR@5 and NDCG@5 on the Sports
and Toys dataset are demonstrated in Figure 2. We can observe that the performance improves Ô¨Årst"
HYPER-PARAMETER SENSITIVITY,0.6419753086419753,Under review as a conference paper at ICLR 2022
HYPER-PARAMETER SENSITIVITY,0.6481481481481481,"Table 2: Performance comparison w.r.t. the variants of CL4SRec (CL4S.) and SRMA. M and D
denote the model augmentation and data augmentation, respectively. p = 0 indicates no neuron
masking. The best score in each column are in bold, where the second-best are underlined. Model"
HYPER-PARAMETER SENSITIVITY,0.654320987654321,"Sports
Toys
HR
NDCG
HR
NDCG
@5
@10
@5
@10
@5
@10
@5
@10
CL4S. w/o D
0.0162
0.0268
0.0108
0.0142
0.0444
0.0619
0.0306
0.0363
CL4S. p = 0
0.0177
0.0292
0.0119
0.0156
0.0451
0.0654
0.0305
0.037
CL4S.
0.0231
0.0369
0.0146
0.0191
0.0525
0.0776
0.0346
0.0428
SRMA w/o D
0.0285
0.0432
0.0187
0.0234
0.0504
0.0724
0.0331
0.0402
SRMA w/o M
0.0165
0.0272
0.0104
0.0138
0.0412
0.0590
0.0279
0.0336
SRMA
0.0299
0.0447
0.0199
0.0246
0.0598
0.0834
0.0407
0.0484"
HYPER-PARAMETER SENSITIVITY,0.6604938271604939,"Sports
Toys
Sports
Toys"
HYPER-PARAMETER SENSITIVITY,0.6666666666666666,"Figure 2: Performance comparison betweeen SASRec and SRMA in HR@5 and NDCG@5 w.r.t
different values of neuron masking probability p on Sports and Toys dataset."
HYPER-PARAMETER SENSITIVITY,0.6728395061728395,"and then drops when increasing p from 0 to 0.9. The rising of the performance score implies that
the neuron masking augmentation is effective in improving the ability of the sequence encoder for a
recommendation. And the dropping of the performance indicates the intensity of model augmenta-
tion should not be overly aggressive, which may lead to less informative contrastive learning. As to
SASRec, we recognize a higher score of SASRec when p is large, which indicates the optimal model
augmentation should be a slightly perturbation rather than a intensive distortion. Moreover, SRMA
consistently outperforms SASRec when 0.1 < p < 0.6. Since the only difference is that SASRec
has no SSL module, we can thus concludes that the performance gains result from the contrastive
SSL step by using the neuron masking."
HYPER-PARAMETER SENSITIVITY,0.6790123456790124,"Effect of Layer Dropping. The layer dropping model augmentation is controlled by two hyper-
parameters, the number of additional FFN layers and the number of layers to drop during training,
which is denoted as K and M, respectively. Since we can only drop those additional layers, we
have M < K. We select K from {1, 2, 3, 4} while M are searched accordingly. Due to space
limitation, we only report the NDCG@5 on the Sports and Toys dataset in Figure 3. We observe
that K = 2, M = 1 achieves the best performance on both datasets, which implies the efÔ¨Åcacy of
layer dropping. Additionally, we also Ô¨Ånd that the performance on K = 4 is consistently worse than
K = 2 on both datasets, which suggests that adding too many layers increases the complexity of the
model, which is thus unable to enhance the SSL paradigm."
ANALYSES ON ENCODER COMPLEMENTING,0.6851851851851852,"5.5
ANALYSES ON ENCODER COMPLEMENTING"
ANALYSES ON ENCODER COMPLEMENTING,0.691358024691358,"In this section, we investigate the effects of encoder complementing augmentation for constructing
views. Recall that we combine the embedding from the model‚Äôs encoder and a distinct pre-trained
encoder. For this complementary encoder, we select from a Transformer-based and a GRU-based
encoder. Since the model‚Äôs encoder is a 2-layer Transformer, this pre-trained encoder is a 1-layer
Transformer to maintain diversity. We Ô¨Årst pre-train the complementary encoder based on the next-
item prediction task. As such, we empower the pre-trained encoder to characterize the sequential
correlations of items. The comparison is conducted on both Sports and Toys datasets, which are
shown in Table 3. The observations are as follows: Firstly, on the Sports dataset, pre-training a GRU
encoder as a complement performs the best against the other two, which indicates that injecting dis-"
ANALYSES ON ENCODER COMPLEMENTING,0.6975308641975309,Under review as a conference paper at ICLR 2022
ANALYSES ON ENCODER COMPLEMENTING,0.7037037037037037,"Toys
Sports"
ANALYSES ON ENCODER COMPLEMENTING,0.7098765432098766,"Figure 3: The NDCG@5 performance w.r.t. different K and M for layer dropping augmentation on
Sports and Toys dataset."
ANALYSES ON ENCODER COMPLEMENTING,0.7160493827160493,"Table 3: Performance comparison among SRMA without encoder complementing (w/o Enc.), with
Transformer-based (-Trans) and with GRU-based (-GRU) complementary pre-trained encoder. The
best score in each column is in bold."
ANALYSES ON ENCODER COMPLEMENTING,0.7222222222222222,Encoders
ANALYSES ON ENCODER COMPLEMENTING,0.7283950617283951,"Sports
Toys
HR
NDCG
HR
NDCG
@5
@10
@5
@10
@5
@10
@5
@10
w/o Enc.
0.0269
0.0401
0.0181
0.0224
0.0567
0.0806
0.0389
0.0466
-Trans
0.0268
0.0408
0.0181
0.0226
0.0588
0.0811
0.0402
0.0474
-GRU
0.0281
0.0411
0.0186
0.0228
0.0577
0.0811
0.0395
0.047"
ANALYSES ON ENCODER COMPLEMENTING,0.7345679012345679,"tinct encoders for contrastive learning can enhance the SSL signals. Secondly, on the Toys dataset,
adopting a 1-layer pre-trained Transformer as the complementary encoder yields the best scores on
all metrics. Besides the effectiveness of encoder complementing, this result also suggests that the
complementary encoder may not be overly different from the model‚Äôs encoder on some datasets,
which otherwise cannot enhance the comprehensive contrastive learning between views. Lastly,
both Transformer-based and GRU-based pre-trained complementary encoders consistently outper-
form SRMA without encoder complementing, which directly indicates the necessity of encoder
complementing as a way of model augmentation."
CONCLUSION,0.7407407407407407,"6
CONCLUSION"
CONCLUSION,0.7469135802469136,"This work proposes a novel contrastive self-supervised learning paradigm, which simultaneously
employs model augmentation and data augmentation to construct views for contrasting. We pro-
pose three-level model augmentation methods for this paradigm, which are neuron masking, layer
dropping, and encoder complementing. We adopt this paradigm to the sequential recommendation
problem and propose a new model SRMA. This model adopts both the random data augmentation
of sequences and the corresponding three-level model augmentation for the sequence encoder. We
conduct comprehensive experiments to verify the effectiveness of SRMA. The overall performance
comparison justiÔ¨Åes the advantage of contrastive SSL with model augmentation. Additionally, de-
tailed investigation regarding the impacts of model augmentation and data augmentation in improv-
ing the performance are discussed. Moreover, ablation studies with respect to three-level model
augmentation methods are implemented, which also demonstrate the superiority of the proposed
model. Overall, this work opens up a new direction in constructing views from model augmenta-
tion. We believe model augmentation can enhance existing contrastive SSL paradigms with only
data augmentation."
CONCLUSION,0.7530864197530864,Under review as a conference paper at ICLR 2022
REFERENCES,0.7592592592592593,REFERENCES
REFERENCES,0.7654320987654321,"Yoshua Bengio, Yann Lecun, and Geoffrey Hinton. Deep learning for ai. 64(7), 2021. ISSN 0001-
0782."
REFERENCES,0.7716049382716049,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597‚Äì1607. PMLR, 2020."
REFERENCES,0.7777777777777778,"Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750‚Äì15758, 2021."
REFERENCES,0.7839506172839507,"Xu Chen, Hongteng Xu, Yongfeng Zhang, Jiaxi Tang, Yixin Cao, Zheng Qin, and Hongyuan Zha.
Sequential recommendation with user memory networks. In WSDM, pp. 108‚Äì116, 2018."
REFERENCES,0.7901234567901234,"Maurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. Are we really making much
progress? a worrying analysis of recent neural recommendation approaches. In Proceedings of
the 13th ACM Conference on Recommender Systems, pp. 101‚Äì109, 2019."
REFERENCES,0.7962962962962963,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.8024691358024691,"Ziwei Fan, Zhiwei Liu, Jiawei Zhang, Yun Xiong, Lei Zheng, and Philip S. Yu. Continuous-time
sequential recommendation with temporal graph collaborative transformer. In Proceedings of the
30th ACM International Conference on Information and Knowledge Management. ACM, 2021."
REFERENCES,0.808641975308642,"Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence
embeddings. arXiv preprint arXiv:2104.08821, 2021."
REFERENCES,0.8148148148148148,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770‚Äì778, 2016."
REFERENCES,0.8209876543209876,"Bal¬¥azs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based rec-
ommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939, 2015."
REFERENCES,0.8271604938271605,"Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. Gpt-gnn: Generative
pre-training of graph neural networks. In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, pp. 1857‚Äì1867, 2020."
REFERENCES,0.8333333333333334,"Tianyu Hua, Wenxiao Wang, Zihui Xue, Yue Wang, Sucheng Ren, and Hang Zhao. On feature
decorrelation in self-supervised learning. arXiv preprint arXiv:2105.00470, 2021."
REFERENCES,0.8395061728395061,"Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In ICDM, pp.
197‚Äì206. IEEE, 2018."
REFERENCES,0.845679012345679,"Gregory Koch, Richard Zemel, Ruslan Salakhutdinov, et al. Siamese neural networks for one-shot
image recognition. In ICML deep learning workshop, volume 2. Lille, 2015."
REFERENCES,0.8518518518518519,"Walid Krichene and Steffen Rendle. On sampled metrics for item recommendation. In SIGKDD,
pp. 1748‚Äì1757, 2020."
REFERENCES,0.8580246913580247,"Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In Proceedings
of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
338‚Äì348, 2020."
REFERENCES,0.8641975308641975,"Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-
supervised learning: Generative or contrastive. IEEE Transactions on Knowledge and Data En-
gineering, 2021a."
REFERENCES,0.8703703703703703,"Zhiwei Liu, Yongjun Chen, Jia Li, Philip S Yu, Julian McAuley, and Caiming Xiong.
Con-
trastive self-supervised sequential recommendation with robust augmentation.
arXiv preprint
arXiv:2108.06479, 2021b."
REFERENCES,0.8765432098765432,Under review as a conference paper at ICLR 2022
REFERENCES,0.8827160493827161,"Zhiwei Liu, Ziwei Fan, Yu Wang, and Philip S. Yu. Augmenting sequential recommendation with
pseudo-prior items via reversely pre-training transformer. ACM, 2021c."
REFERENCES,0.8888888888888888,"Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based rec-
ommendations on styles and substitutes. In SIGIR, pp. 43‚Äì52, 2015."
REFERENCES,0.8950617283950617,"Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Ko-
ray Kavukcuoglu.
Conditional image generation with pixelcnn decoders.
arXiv preprint
arXiv:1606.05328, 2016."
REFERENCES,0.9012345679012346,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018."
REFERENCES,0.9074074074074074,"Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang,
and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In Pro-
ceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, pp. 1150‚Äì1160, 2020."
REFERENCES,0.9135802469135802,"Massimo Quadrana, Alexandros Karatzoglou, Bal¬¥azs Hidasi, and Paolo Cremonesi. Personalizing
session-based recommendations with hierarchical recurrent neural networks. In RecSys, pp. 130‚Äì
137, 2017."
REFERENCES,0.9197530864197531,"Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme.
Factorizing personalized
markov chains for next-basket recommendation. In WWW, pp. 811‚Äì820, 2010."
REFERENCES,0.9259259259259259,"Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequen-
tial recommendation with bidirectional encoder representations from transformer. In CIKM, pp.
1441‚Äì1450, 2019."
REFERENCES,0.9320987654320988,"Jiaxi Tang and Ke Wang. Personalized top-n sequential recommendation via convolutional sequence
embedding. In WSDM, pp. 565‚Äì573, 2018."
REFERENCES,0.9382716049382716,"Wilson L Taylor. ‚Äúcloze procedure‚Äù: A new tool for measuring readability. Journalism quarterly,
30(4):415‚Äì433, 1953."
REFERENCES,0.9444444444444444,"Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
makes for good views for contrastive learning? arXiv preprint arXiv:2005.10243, 2020."
REFERENCES,0.9506172839506173,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998‚Äì6008, 2017."
REFERENCES,0.9567901234567902,"Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. Neural graph collaborative
Ô¨Åltering.
In Proceedings of the 42nd international ACM SIGIR conference on Research and
development in Information Retrieval, pp. 165‚Äì174, 2019."
REFERENCES,0.9629629629629629,"Chao-Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J Smola, and How Jing. Recurrent recom-
mender networks. In WSDM, pp. 495‚Äì503, 2017."
REFERENCES,0.9691358024691358,"Liwei Wu, Shuqing Li, Cho-Jui Hsieh, and James Sharpnack. Sse-pt: Sequential recommendation
via personalized transformer. In RecSys, pp. 328‚Äì337. ACM, 2020."
REFERENCES,0.9753086419753086,"Xu Xie, Fei Sun, Zhaoyang Liu, Jinyang Gao, Bolin Ding, and Bin Cui. Contrastive pre-training for
sequential recommendation. arXiv preprint arXiv:2010.14395, 2020."
REFERENCES,0.9814814814814815,"Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph
contrastive learning with augmentations. Advances in Neural Information Processing Systems,
33:5812‚Äì5823, 2020."
REFERENCES,0.9876543209876543,"Lei Zheng, Ziwei Fan, Chun-Ta Lu, Jiawei Zhang, and Philip S Yu. Gated spectral units: Modeling
co-evolving patterns for sequential recommendation. In SIGIR, pp. 1077‚Äì1080, 2019."
REFERENCES,0.9938271604938271,"Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang,
and Ji-Rong Wen. S3-rec: Self-supervised learning for sequential recommendation with mutual
information maximization. In Proceedings of the 29th ACM International Conference on Infor-
mation & Knowledge Management, pp. 1893‚Äì1902, 2020."
