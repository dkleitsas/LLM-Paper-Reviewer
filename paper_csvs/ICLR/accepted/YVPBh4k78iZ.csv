Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0018726591760299626,"Recent works have revealed that inﬁnitely-wide feed-forward or recurrent neu-
ral networks of any architecture correspond to Gaussian processes referred to as
Neural Network Gaussian Processes (NNGPs). While these works have extended
the class of neural networks converging to Gaussian processes signiﬁcantly, how-
ever, there has been little focus on broadening the class of stochastic processes
that such neural networks converge to. In this work, inspired by the scale mixture
of Gaussian random variables, we propose the scale mixture of NNGPs for which
we introduce a prior distribution on the scale of the last-layer parameters. We
show that simply introducing a scale prior on the last-layer parameters can turn
inﬁnitely-wide neural networks of any architecture into a richer class of stochas-
tic processes. With certain scale priors, we obtain heavy-tailed stochastic pro-
cesses, and in the case of inverse gamma priors, we recover Student’s t processes.
We further analyze the distributions of the neural networks initialized with our
prior setting and trained with gradient descents and obtain similar results as for
NNGPs. We present a practical posterior-inference algorithm for the scale mixture
of NNGPs and empirically demonstrate its usefulness on regression and classiﬁ-
cation tasks. In particular, we show that in both tasks, the heavy-tailed stochastic
processes obtained from our framework are robust to out-of-distribution data."
INTRODUCTION,0.003745318352059925,"1
INTRODUCTION"
INTRODUCTION,0.0056179775280898875,"There has been growing interest in the literature on the equivalence between wide deep neural
networks and Gaussian Processes (GPs). Neal (1996) showed that a shallow but inﬁnitely-wide
Bayesian Neural Network (BNN) with random weights and biases corresponds to a GP. This result
was extended to fully-connected deep neural networks of any depth (Lee et al., 2018; Matthews et al.,
2018), which are shown to converge to GPs as the width grows. Similar results were later obtained
for deep Convolutional Neural Networks (CNNs) (Novak et al., 2018; Garriga-Alonso et al., 2019)
and attention networks (Hron et al., 2020). In fact, Yang (2019) showed that wide feed-forward or
recurrent neural networks of any architecture converge to GPs and presented a generic method for
computing kernels for such GPs. Under this correspondence, the posterior inference of an inﬁnitely-
wide BNN boils down to the posterior inference of the corresponding GP called NNGP for which a
closed-form posterior can be computed exactly."
INTRODUCTION,0.00749063670411985,"Our goal is to advance this line of research by going beyond GPs. We present a simple yet ﬂexible
recipe for constructing inﬁnitely-wide BNNs that correspond to a wide range of stochastic processes.
Our construction includes heavy-tailed stochastic processes such as Student’s t processes which
have been demonstrated to be more robust than GPs under certain scenarios (Shah et al., 2014)."
INTRODUCTION,0.009363295880149813,"Our construction is inspired by a popular class of distributions called scale mixtures of Gaus-
sians (Andrews & Mallows, 1974); such a distribution is obtained by putting a prior on the scale
or variance parameter of a Gaussian distribution. We extend this scale mixing to NNGPs, where we
introduce a prior distribution on the scale of the parameters for the last layer (which is often re-
ferred to as readout layer) in a wide neural network. We show that simply introducing a scale prior"
INTRODUCTION,0.011235955056179775,Published as a conference paper at ICLR 2022
INTRODUCTION,0.013108614232209739,"on the last layer can turn inﬁnitely-wide BNNs of any architecture into a richer class of stochastic
processes, which we name as scale mixtures of NNGPs. These scale mixtures include a broad class
of stochastic processes, such as heavy-tailed processes which are shown to be robust to outliers.
In particular, when the prior on the scale is inverse gamma, the scale mixture of NNGPs becomes
Stduent’s t process (Shah et al., 2014)."
INTRODUCTION,0.0149812734082397,"We demonstrate that, despite increasing ﬂexibility, mixing NNGPs with a prior on the scale parameter
does not increase the difﬁculty of posterior inference much or at all in some cases. When we mix the
scale parameter with inverse gamma (so that the mixture becomes a t process), we can compute the
kernel efﬁciently and infer the exact posterior, as in the case of NNGPs. For generic scale priors and
regression tasks with them, we present an efﬁcient approximate posterior-inference algorithm based
on importance sampling, which saves computation time by reusing the shared covariance kernels.
For classiﬁcation tasks with categorical likelihood (for which an exact posterior is not available even
for the original NNGP), we present an efﬁcient stochastic variational inference algorithm."
INTRODUCTION,0.016853932584269662,"We further analyze the distributions of the neural networks initialized with our prior setting and
trained with gradient descents and obtain results similar to the ones for NNGPs. For NNGP, it has
been shown (Matthews et al., 2017; Lee et al., 2019) that when a wide neural network is initialized
with the NNGP speciﬁcation and then trained only for the last layer (with all the other layers ﬁxed),
its fully trained version becomes a random function drawn from the NNGP posterior. Similarly,
we analyze the distribution of a wide neural network of any architecture initialized with our prior
speciﬁcation and trained only for the last layer, and show that it becomes a sample from a scale
mixture of NNGPs with some scaling distribution. Interestingly, the scaling distribution is a prior,
not a posterior. For the fully-connected wide neural networks, we extend the analysis to the case
where all the layers are trained with gradient descent, and show that the limiting distribution is again
a scale mixture of GPs with a prior scaling distribution and each GPs using a kernel called Neural
Tangent Kernel (NTK). In the case of an inverse gamma prior, the limiting distribution becomes
Student’s t process, which can be computed analytically."
INTRODUCTION,0.018726591760299626,"We empirically show the usefulness of our construction on various real-world regression and classi-
ﬁcation tasks. We demonstrate that, despite the increased ﬂexibility, the scale mixture of NNGPs is
readily applicable to most of the problems where NNGPs are used, without increasing the difﬁculty
of inference. Moreover, the heavy-tailed processes derived from our construction are shown to be
more robust than NNGPs for out-of-distribution or corrupted data while maintaining similar perfor-
mance for the normal data. Our empirical analysis suggests that our construction is not merely a
theoretical extension of the existing framework, but also provides a practical alternative to NNGPs."
RELATED WORKS,0.020599250936329586,"1.1
RELATED WORKS"
RELATED WORKS,0.02247191011235955,"NNGP and NTK
Our construction heavily depends on the tensor-program framework (Yang, 2019)
which showed that a wide BNN of any feed-forward or recurrent architecture converges in distribu-
tion to an NNGP, as its width increases. Especially, we make use of the so-called master theorem
and its consequence (reviewed in Section 1.1) to derive our results. Building on the seminal work
on NTK (Jacot et al., 2018), Lee et al. (2019) analyzed the dynamics of fully-connected neural net-
works trained with gradient descent and showed that fully-trained inﬁnitely-wide networks are GPs.
In particular, they conﬁrmed the result in Matthews et al. (2017) that when a fully-connected neu-
ral network is initialized from a speciﬁc prior (often referred to as the NTK parameterization) and
trained only for the last layer under gradient descent and squared loss, its fully-trained version be-
comes a posterior sample of the corresponding NNGP. Lee et al. (2019) further analyzed the case
where all the layers are trained with gradient descent and showed that the network also converges to
a GP with speciﬁc parameters computed with the so called NTK kernel. We extend these results to
our scale mixture of NNGPs in Section 3."
RELATED WORKS,0.024344569288389514,"Heavy-tailed stochastic processes from inﬁnitely-wide BNNs
The attempts to extend the results
on NNGPs to heavy-tailed processes have been made in the past, although not common. The rep-
resentative case is the work by Favaro et al. (2020), which showed that under an alternative prior
speciﬁcation, a wide fully-connected neural network converges to stable processes, as the widths in-
crease. This result was later extended to deep CNNs in (Bracale et al., 2021). What distinguishes our
approach from these works is the simplicity of our construction; it simply puts a prior distribution
on the scale of the last-layer parameters of a network and makes the scale a random variable, while"
RELATED WORKS,0.026217228464419477,Published as a conference paper at ICLR 2022
RELATED WORKS,0.028089887640449437,"the constructions in those works replace priors for entire network parameters from Gaussian to other
distributions. This simplicity has multiple beneﬁts. First, most of the nice properties of NNGPs,
such as easy-to-compute posteriors and the correspondence to gradient descent training, continue
to hold in our approach as a version for mixture distributions, while it is at least difﬁcult to ﬁnd
similar adjustments of these results in those works. Second, our approach is applicable to arbitrary
architectures, while those works considered only fully-connected networks and CNNs."
PRELIMINARIES,0.0299625468164794,"2
PRELIMINARIES"
PRELIMINARIES,0.031835205992509365,"We start with a quick review on a few key concepts used in our results. For M ∈N, let [M] be the
set {1, . . . , M}. Also, write R+ for the set {x ∈R | x > 0}."
TENSOR PROGRAMS AND NEURAL NETWORK GAUSSIAN PROCESSES,0.033707865168539325,"2.1
TENSOR PROGRAMS AND NEURAL NETWORK GAUSSIAN PROCESSES"
TENSOR PROGRAMS AND NEURAL NETWORK GAUSSIAN PROCESSES,0.035580524344569285,"Tensor programs (Yang, 2019) are a particular kind of straight-line programs that express compu-
tations of neural networks on ﬁxed inputs. In a sense, they are similar to computation graphs used
by autodiff packages, such as TensorFlow and PyTorch, but differ from computation graphs in two
important aspects. First, a single tensor program can express the computations of inﬁnitely-many
neural networks, which share the same architecture but have different widths. This capability comes
from the parameter n of the tensor program, which determines the dimensions of vectors and ma-
trices used in the program. The parameter corresponds to the width of the hidden layers of a neural
network, so that changing n makes the same tensor program model multiple neural networks of
different widths. Second, tensor programs describe stochastic computations, where stochasticity
comes from the random initialization of network weights and biases. These two points mean that
we can use a single tensor program to model the sequence of BNNs of the same architecture but with
increasing widths, and understand the limit of the sequence by analyzing the program. The syntax
of and other details about tensor programs are given in Appendix A."
TENSOR PROGRAMS AND NEURAL NETWORK GAUSSIAN PROCESSES,0.03745318352059925,"We use tensor programs because of the so called master theorem, which provides a method for
computing the inﬁnite-width limits of BNN sequences expressed by tensor programs. For the precise
formulation of the theorem and the method, again see Appendix A."
TENSOR PROGRAMS AND NEURAL NETWORK GAUSSIAN PROCESSES,0.03932584269662921,"To explain the master theorem more precisely, assume that we are given a family of BNNs {fn}n∈N
that are indexed by their widths n and have the following form:"
TENSOR PROGRAMS AND NEURAL NETWORK GAUSSIAN PROCESSES,0.04119850187265917,"fn(−; vn, Ψn) : RI →R,
fn(x; vn, Ψn) =
1
√n X"
TENSOR PROGRAMS AND NEURAL NETWORK GAUSSIAN PROCESSES,0.04307116104868914,"α∈[n]
vn,α · φ(gn(x; Ψn))α,"
TENSOR PROGRAMS AND NEURAL NETWORK GAUSSIAN PROCESSES,0.0449438202247191,"where vn ∈Rn and Ψn ∈Rn×P (for some ﬁxed P) are the parameters of the n-th network, the
former being the ones of the readout layer and the latter all the other parameters, I is the dimension
of the inputs, φ is a non-linear activation function of the network, and gn(x; Ψn) is the output of
a penultimate linear layer, which feeds directly to the readout layer after being transformed by the
activation function. In order for the computations of these BNNs on some inputs to be represented
by a tensor program, the components of the networks have to satisfy at least the following two
conditions. First, the entries of vn and Ψn are initialized with samples drawn independently from
(possibly different) zero-mean Gaussian distributions. For the entries of vn, the distributions are the
same and have the variance σ2
v for some σv > 0. For the entries of Ψn, the distributions may be
different, but if we restrict them to those in each column j of Ψn, they become the same and have
the variance σj/n for some σj > 0. Second, φ(z) is controlled, i.e., it is bounded by a function
exp(C|z|2−ϵ + c) for some C, ϵ, c > 0. This property ensures that φ(z) for any Gaussian variable z
has a ﬁnite expectation."
TENSOR PROGRAMS AND NEURAL NETWORK GAUSSIAN PROCESSES,0.04681647940074907,"Let x1, . . . , xM ∈RI be M inputs. When the computations of the BNNs on these inputs are
represented by a single tensor program, the master theorem holds. It says that there is a general
method for computing µ ∈RM and Σ ∈RM×M inductively on the syntax of the program such that
µ and Σ characterize the limit of (gn(x1; Ψn), . . . , gn(xM; Ψn))n∈N in the following sense.
Theorem 2.1 (Master Theorem). Let h : RM →R be a controlled function. Then, as n tends to ∞,"
N,0.04868913857677903,"1
n n
X"
N,0.05056179775280899,"α=1
h

gn(x1; Ψn)α, . . . , gn(xM; Ψn)α
 a.s.
−−→EZ∼N(µ,Σ) [h(Z)] ,
(1)"
N,0.052434456928838954,Published as a conference paper at ICLR 2022
N,0.054307116104868915,"where
a.s.
−−→refers to almost sure convergence."
N,0.056179775280898875,"The next corollary describes an important consequence of the theorem.
Corollary 2.2. As n tends to ∞, the joint distribution of (fn(xm; vn, Ψn))m∈[M] converges weakly
to the multivariate Gaussian distribution with zero mean and the following covariance matrix K ∈
RM×M:"
N,0.05805243445692884,"K(i,j) = σ2
v · EZ∼N(µ,Σ) [φ(Zi)φ(Zj)]
(2)"
N,0.0599250936329588,where Zi and Zj are the i-th and j-th components of Z.
N,0.06179775280898876,"The above corollary and the fact that K depend only on the shared architecture of the BNNs, not on
the inputs x1:M, imply that the BNNs converge to a GP whose mean is the constant zero function and
whose kernel is"
N,0.06367041198501873,"κ(x, x′) = σ2
v · EZ∼N(µ,Σ) [φ(Z1)φ(Z2)]
(3)"
N,0.06554307116104868,"where (µ, Σ) is constructed as described above for the case that M = 2 and (x1, x2) = (x, x′).
This GP is called Neural Network Gaussian Process (NNGP) in the literature."
N,0.06741573033707865,"2.2
STUDENT’S t PROCESSES"
N,0.06928838951310862,"Student’s t processes feature prominently in our results to be presented shortly, because they are
marginals of the scale mixtures of NNGPs when the mixing is done by inverse gamma. These pro-
cesses are deﬁned in terms of multivariate Student’s t distributions deﬁned as follows.
Deﬁnition 2.1. A distribution on Rd is a multivariate (Student’s) t distribution with degree of free-
dom ν ∈R+, location µ ∈Rd and positive-deﬁnite scale matrix Σ ∈Rd×d if it has the following
density:"
N,0.07116104868913857,"p(x) =
G((ν + d)/2)"
N,0.07303370786516854,"(νπ)
d
2 · G(ν/2) · |Σ|
1
2"
N,0.0749063670411985,"
1 + 1"
N,0.07677902621722846,"ν (x −µ)⊤Σ−1(x −µ)
−ν+d 2"
N,0.07865168539325842,"where G(·) is the gamma function. To express a random variable drawn from this distribution, we
write x ∼MVTd(ν, µ, Σ)."
N,0.08052434456928839,"When ν > 1, the mean of the distribution MVTd(ν, µ, Σ) exists, and it is µ. When ν > 2, the
covariance of distribution also exists, and it is
ν
ν−2Σ. As for the Gaussian distribution, the multi-
variate t distribution is also closed under marginalization and conditioning. More importantly, it can
be obatined as a scale mixture of Gaussians; if σ2 ∼InvGamma(a, b) and x|σ2 ∼N(µ, σ2Σ),
then the marginal distribution of x is MVT(2a, µ, b"
N,0.08239700374531835,"aΣ). For the deﬁnition of the inverse gamma
distribution, see Deﬁnition B.1."
N,0.08426966292134831,"Student’s t process is a real-valued random function such that the outputs of the function on a ﬁnite
number of inputs are distributed by a multivariate t distribution (Shah et al., 2014). Consider a set
X (for inputs) with a symmetric positive-deﬁnite function κ : X × X →R.
Deﬁnition 2.2. A random function f : X →R is Student’s t process with degree of freedom
ν ∈R+, location function M : X →R, and scale function κ, if for all d and inputs x1, . . . , xd ∈X,
the random vector (f(x1), . . . , f(xd)) has the multivariate t distribution MVTd(ν, µ, Σ), where
µ = (M(x1), . . . , M(xd)) and Σ is the d × d matrix deﬁned by Σ(i,j) = k(xi, xj)."
RESULTS,0.08614232209737828,"3
RESULTS"
RESULTS,0.08801498127340825,"We will present our results for the regression setting. Throughout this section, we consider only
those BNNs whose computations over ﬁxed inputs are representable by tensor programs. Assume
that we are given a training set Dtr = {(xk, yk) ∈RI × R | 1 ≤k ≤K} and a test set Dte =
{(xK+ℓ, yK+ℓ) | 1 ≤ℓ≤L}."
RESULTS,0.0898876404494382,"Our idea is to treat the variance σ2
v for the parameters of the readout layer in a BNN as a random
variable, not a deterministic value, so that the BNN represents a mixture of random functions (where
the randomness comes from that of σ2
v and the random initialization of the network parameters). To"
RESULTS,0.09176029962546817,Published as a conference paper at ICLR 2022
RESULTS,0.09363295880149813,"state this more precisely, consider the computations of the BNNs {fn(−; vn, Ψn)}n∈N on the inputs
x1:K+L in the training and test sets such that n is the width of the hidden layers of fn and these
computations are representable by a tensor program. Our idea is to change the distribution for the
initial value of vn from a Gaussian to a scale mixture of Gaussians, so that at initialization, the BNNs
on x1:K+L become the following random variables: for each n ∈N,
σ2
v ∼H,
vn,α|σ2
v ∼N(0, σ2
v) for α ∈[n],
Ψn,(α,j) ∼N(0, σ2
j /n) for α ∈[n], j ∈[P],"
RESULTS,0.09550561797752809,"fn(xi; vn, Ψn) =
1
√n X"
RESULTS,0.09737827715355805,"α∈[n]
vn,α · φ(gn(xi; Ψn))α for i ∈[K + L],"
RESULTS,0.09925093632958802,"where H is a distribution on positive real numbers, such as the inverse-gamma distribution, P is the
number of columns of Ψn, and σ2
j and gn are, respectively, the variance speciﬁc to the j-th column
of Ψn and the output of the penultimate linear layer, as explained in our review on tensor programs."
RESULTS,0.10112359550561797,"Our ﬁrst result says that as the width of the BNN grows to ∞, the BNN converges in distribution to a
mixture of NNGPs, and, in particular, to Student’s t process if the distribution H is inverse gamma.
Theorem 3.1 (Convergence). As n tends to ∞, the random variable (fn(xi; vn, Ψn))i∈[K+L] con-
verges in distribution to the following random variable (f∞(xi))i∈[K+L]:"
RESULTS,0.10299625468164794,"σ2
v ∼H,
(f∞(xi))i∈[K+L]|σ2
v ∼N(0, σ2
v · K),"
RESULTS,0.10486891385767791,"where K(i,j) = EZ∼N(µ,Σ)[φ(Zi)φ(Zj)] for i, j ∈[K + L]. Furthermore, if H is the inverse-
gamma distribution with shape a and scale b, the marginal distribution of (f∞(xi))i∈[K+L] is the
following multivariate t distribution:
(f∞(xi))i∈[K+L] ∼MVTK+L(2a, 0, (b/a) · K),"
RESULTS,0.10674157303370786,"where MVTd(ν′, µ′, K′) denotes the d-dimensional Student’s t distribution with degree of freedom
ν′, location µ′, and scale matrix K′."
RESULTS,0.10861423220973783,"This theorem holds mainly because of the master theorem for tensor programs (Theorem 2.1). In
fact, the way that its proof uses the master theorem is essentially the same as the one of the NNGP
convergence proof (i.e., the proof of Corollary 2.2), except for one thing: before applying the master
theorem, the proof of Theorem 3.1 conditions on σ2
v and removes its randomness. The detailed proof
can be found in the appendix."
RESULTS,0.1104868913857678,"As in the case of Corollary 2.2, although the theorem is stated for the ﬁnite dimensional case with
the inputs x1:K+L, it implies the convergence of the BNNs with random σ2
v to a scale mixture of GPs
or to Student’s t process. Note that σ2
v · K in the theorem is precisely the covariance matrix K of
the NNGP kernel on x1:K+L in Corollary 2.2. We thus call the limiting stochastic process as scale
mixture of NNGPs."
RESULTS,0.11235955056179775,"Our next results characterize a fully-trained BNN at the inﬁnite-width limit under two different train-
ing schemes. They are the versions of the standard results in the literature, generalized from GPs to
scale mixtures of GPs. Assume the BNNs under the inputs x1:K+L that we have been using so far.
Theorem 3.2 (Convergence under Readout-Layer Training). Assume that we train only the readout
layers of the BNNs using the training set Dtr under mean squared loss and inﬁnitesimal step size.
Formally, this means the network parameters are evolved under the following differential equations:"
RESULTS,0.11423220973782772,"(v(0)
n , Ψ(0)
n ) = (vn, Ψn),
dv(t)
n
dt
= K
X i=1"
RESULTS,0.11610486891385768,"
yi −fn(xi; v(t)
n , Ψ(t)
n )
2φ(gn(xi; Ψ(t)
n ))
K√n
,
dΨ(t)
n
dt
= 0."
RESULTS,0.11797752808988764,"Then, the time and width limit of the random variables (fn(xK+i; v(t)
n , Ψ(t)
n ))i∈[L], denoted
(f ∞
∞(xK+i))i∈[L], is distributed by the following mixture of Gaussians:"
RESULTS,0.1198501872659176,"σ2
v ∼H,
(f ∞
∞(xK+i))i∈[L]|σ2
v ∼N
"
RESULTS,0.12172284644194757,"Kte,trK
−1
tr,trYtr

, σ2
v
"
RESULTS,0.12359550561797752,"Kte,te −Kte,trK
−1
tr,trKtr,te

,"
RESULTS,0.1254681647940075,"where Ytr consists of the y values in Dtr (i.e., Ytr = (y1, . . . , yK)) and the K’s with different sub-
scripts are the restrictions of K in Theorem 3.1 with training or test inputs as directed by those sub-
scripts. Furthermore, if H is the inverse-gamma distribution with shape a and scale b, the marginal
distribution of (f ∞
∞(xK+i))i∈[L] is the following multivariate t distribution:"
RESULTS,0.12734082397003746,"(f ∞
∞(xK+i))i∈[L] ∼MVTL"
RESULTS,0.12921348314606743,"
2a,
"
RESULTS,0.13108614232209737,"Kte,trK
−1
tr,trYtr

, b a "
RESULTS,0.13295880149812733,"Kte,te −Kte,trK
−1
tr,trKtr,te

."
RESULTS,0.1348314606741573,Published as a conference paper at ICLR 2022
RESULTS,0.13670411985018727,"One important point about this theorem is that the distribution of (f ∞
∞(xK+i))i∈[L] is not the pos-
terior of (f∞(xK+i))i∈[L] in Theorem 3.1 under the condition on Ytr. The former uses the prior on
σ2
v for mixing the distribution of (f∞(xK+i))i∈[L] | (σ2
v, Ytr), while the latter uses the posterior on
σ2
v for the same purpose."
RESULTS,0.13857677902621723,"Our last result assumes a change in the initialization rule for our models. This is to enable the use
of the results in Lee et al. (2019) about the training of inﬁnitely-wide neural networks. Concretely,
we use the so called NTK parametrization for Ψn. This means that the BNNs on x1:K+L are now
intialized to be the following random variables: for each n ∈N,"
RESULTS,0.1404494382022472,"σ2
v ∼H,
vn,α|σ2
v ∼N(0, σ2
v),
Ψn,(α,j) ∼N(0, σ2
j )
for α ∈[n], j ∈[P],"
RESULTS,0.14232209737827714,"fn(xi; vn, 1
√nΨn) =
1
√n X"
RESULTS,0.1441947565543071,"α∈[n]
vn,α · φ(gn(xi; 1
√nΨn))α for i ∈[K + L]."
RESULTS,0.14606741573033707,"When we adopt this NTK parameterization and the BNNs are multi-layer perceptrons (more gener-
ally, they share an architecture for which the limit theory of neural tangent kernels has been devel-
oped), we can analyze their distributions after the training of all parameters. Our analysis uses the
following famous result for such BNNs: as n tends to inﬁnity, for all i, j ∈[K + L],
D
∇(vn,Ψn)fn(xi; vn, 1
√nΨn), ∇(vn,Ψn)fn(xj; vn, 1
√nΨn)
E a.s.
−−→Θ(i,j)
(4)"
RESULTS,0.14794007490636704,"for some Θ(i,j) determined by the architecture of the BNNs. Let Θ be the matrix (Θ(i,j))i,j∈[K+L].
The next theorem describes the outcome of our analysis for the fully-trained inﬁnite-width BNN.
Theorem 3.3 (Convergence under General Training). Assume that the BNNs are multi-layer percep-
trons and we train all of their parameters using the training set Dtr under mean squared loss and
inﬁnitesimal step size. Formally, this means the network parameters are evolved under the following
differential equations:"
RESULTS,0.149812734082397,"v(0)
n
= vn,
dv(t)
n
dt
=
2
K√n K
X i=1"
RESULTS,0.15168539325842698,"
yi −fn(xi; v(t)
n , 1
√nΨ(t)
n )

φ(gn(xi; 1
√nΨ(t)
n )),"
RESULTS,0.15355805243445692,"Ψ(0)
n
= Ψn,
dΨ(t)
n
dt
= 2 K K
X i=1"
RESULTS,0.15543071161048688,"
yi −fn(xi; v(t)
n , 1
√nΨ(t)
n )

∇Ψfn(xi; v(t)
n , 1
√nΨ(t)
n )."
RESULTS,0.15730337078651685,"Let ¯Θ be Θ with σ2
v in it set to 1 (so that Θ = σ2
v ¯Θ). Then, the time and width limit of the random
variables (fn(xK+i; v(t)
n ,
1
√nΨ(t)
n ))i∈[L] has the following distribution:"
RESULTS,0.15917602996254682,"σ2
v ∼H,
(f ∞
∞(xK+i))i∈[L]|σ2
v ∼N(µ′, σ2
vΘ′) where"
RESULTS,0.16104868913857678,"µ′ = ¯Θte,tr ¯Θ−1
tr,trYtr,"
RESULTS,0.16292134831460675,"Θ′ = Kte,te +

¯Θte,tr ¯Θ−1
tr,trKtr,tr ¯Θ−1
tr,tr ¯Θtr,te

−

¯Θte,tr ¯Θ−1
tr,trKtr,te + Kte,tr ¯Θ−1
tr,tr ¯Θtr,te

."
RESULTS,0.1647940074906367,"Here the K’s with subscripts are the ones in Theorem 3.1 and the ¯Θ’s with subscripts are similar
restrictions of ¯Θ with training or test inputs. Furthermore, if H is the inverse gamma with shape
a and scale b, the exact marginal distribution of (f ∞
∞(xK+i))i∈[L] is the following multivariate t
distribution:"
RESULTS,0.16666666666666666,"(f ∞
∞(xK+i))i∈[L] ∼MVTL(2a, µ′, (b/a) · Θ′).
(5)"
RESULTS,0.16853932584269662,"Remark (Unifying High-level Principles). All of our results and their proofs are derived from two
high-level principles. First, once we condition on the variance σ2
v of the readout layer’s parameters,
the networks in our setup ﬁt into the standard setting for NNGPs and NTKs, so that they satisfy the
existing results in the setting. This conditioning trick means that most of the results in the standard
setting can be carried over to our setup in a scale-mixture form, as we explained in this section.
Second, if the prior on σ2
v is inverse gamma, the marginalization of σ2
v in those results can be
calculated analytically and have a form of Student’s t distribution or process."
RESULTS,0.1704119850187266,Published as a conference paper at ICLR 2022
POSTERIOR INFERENCE,0.17228464419475656,"3.1
POSTERIOR INFERENCE"
POSTERIOR INFERENCE,0.17415730337078653,"Gaussian likelihood
As for the NNGPs, when the scale is mixed by the inverse gamma prior, we
can exactly compute the posterior predictives of the scale mixture of NNGPs for Gaussian likeli-
hood since it corresponds to the Student’s t process having closed-form characterizations for the
posteriors. For generic prior settings for the scales, we no longer have such closed-form formu-
las for predictive posteriors, and have to rely on approximate inference methods. We describe one
such method, namely, self-normalizing importance sampling with prior as proposal, together with a
technique for optimizing the method speciﬁcally for scale mixtures of NNGPs."
POSTERIOR INFERENCE,0.1760299625468165,"Consider a scale mixture of NNGPs with a prior H on the variance σ2
v. Assume that we want to
estimate the expectation of h(y) for some h : R →R where the random variable y is drawn from the
predictive posterior of the mixture at some input x ∈RI under the condition on Dtr. The importance
sampling with prior as proposal computes the estimator (PN
i=1 wih(yi))/ PN
j=1 wj where βi and
yi for i = 1, . . . , N are independent samples from the prior H and the posterior of the Gaussian
distribution, respectively, and the wi’s are the importance weights of the βi’s:"
POSTERIOR INFERENCE,0.17790262172284643,"βi ∼H,
wi = N(Ytr; 0, βiKtr,tr),
yi ∼N(Kx,trK
−1
tr,trYtr, βi(Kx,x −Kx,trK
−1
tr,trKtr,x))."
POSTERIOR INFERENCE,0.1797752808988764,The K is the covariance matrix computed as in Theorem 3.1 except that x is now a test input.
POSTERIOR INFERENCE,0.18164794007490637,"A naive implementation of this importance sampler is slow unnecessarily due to the inefﬁcient cal-
culation of the likelihoods N(Ytr; 0, βiKtr,tr) of the sampled βi’s. To see this, note that the log
likelihood of βi can be decomposed as follows:"
POSTERIOR INFERENCE,0.18352059925093633,"log N(Ytr; 0, βiKtr,tr) = −K"
POSTERIOR INFERENCE,0.1853932584269663,2 log(2π) −1
LOG DET,0.18726591760299627,"2 log det
 "
LOG DET,0.1891385767790262,"Ktr,tr

−K"
LOG DET,0.19101123595505617,2 log(βi) −1
LOG DET,0.19288389513108614,"2βi
Y ⊤
tr K
−1
tr,trYtr."
LOG DET,0.1947565543071161,The terms K
LOG DET,0.19662921348314608,"2 log(2π), 1"
LOG DET,0.19850187265917604,"2 log det(Ktr,tr), and Y ⊤
tr K
−1
tr,trYtr are shared across all the samples {βi}N
i=1,
so need not be computed everytime we draw βi. To avoid these duplicated calculations, we compute
these shared terms beforehand, so that the log-likleihood for each βi can be computed in O(1) time."
LOG DET,0.20037453183520598,"Generic likelihoods
For generic likelihoods other than Gaussian, even the vanilla NNGPs do not
admit closed-form posterior predictives. In such cases, we can employ the Sparse Variational Gaus-
sian Process (SVGP) (Titsias, 2009) for approximate inference. We present a similar algorithm for
the inverse-gamma mixing case which leads to Student’s t process. See Appendices C and D for the
detailed description."
EXPERIMENTS,0.20224719101123595,"4
EXPERIMENTS"
EXPERIMENTS,0.20411985018726592,"We empirically evaluated the scale mixtures of NNGPs on various synthetic and real-world tasks.
We tested the scale mixture of NNGPs with inverse-gamma prior corresponding to Student’s t pro-
cesses, and with another heavy-tailed prior called Burr Type XII distribution (Appendix H). Our
implementation used Neural Tangents library (Novak et al., 2020) and JAX (Bradbury et al., 2018)."
EMPIRICAL VALIDATION OF THE THEORIES,0.20599250936329588,"4.1
EMPIRICAL VALIDATION OF THE THEORIES"
EMPIRICAL VALIDATION OF THE THEORIES,0.20786516853932585,"To empirically validate our theories, we set up a fully connected network having three layers with
512 hidden units and erf activation function. Except for the last layer, the weights of the network
were initialized with N(0, 8/n) and the biases were initialized with N(0, 0.052/n). For the last
layer, we sampled σ2
v ∼InvGamma(a, b) with varying (a, b) values and sampled weights from
N(0, σ2
v). To check Theorem 3.1, we initialize 1,000 models and computed the distribution of out-
puts evaluated at zero. For Theorem 3.2, using y = sin(x) as the target function, we ﬁrst initialized
the parameters, trained only the last layer for multiple σv values, and averaged the results to get
function value at zero. Similarly, we trained all the layers to check Theorem 3.3. As shown in
Fig. 1, the theoretical limit well-matched the empirically obtained distributions for all settings we
tested. See Appendix G for the details and more results."
EMPIRICAL VALIDATION OF THE THEORIES,0.20973782771535582,"Published as a conference paper at ICLR 2022 2
0
2 1.0 0.5 0.0 0.5 1.0"
EMPIRICAL VALIDATION OF THE THEORIES,0.21161048689138576,y = sin x
EMPIRICAL VALIDATION OF THE THEORIES,0.21348314606741572,"Gaussian 2
0
2 1.0 0.5 0.0 0.5 1.0"
EMPIRICAL VALIDATION OF THE THEORIES,0.2153558052434457,y = sin x
EMPIRICAL VALIDATION OF THE THEORIES,0.21722846441947566,Student's t
EMPIRICAL VALIDATION OF THE THEORIES,0.21910112359550563,"5.0
2.5
0.0
2.5
5.0
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40"
EMPIRICAL VALIDATION OF THE THEORIES,0.2209737827715356,Probability
EMPIRICAL VALIDATION OF THE THEORIES,0.22284644194756553,Initial
EMPIRICAL VALIDATION OF THE THEORIES,0.2247191011235955,"Predicted
Sampled"
EMPIRICAL VALIDATION OF THE THEORIES,0.22659176029962547,"4
2
0
2
4
0.0 0.1 0.2 0.3 0.4 0.5 0.6"
EMPIRICAL VALIDATION OF THE THEORIES,0.22846441947565543,Probability
EMPIRICAL VALIDATION OF THE THEORIES,0.2303370786516854,Last layer training
EMPIRICAL VALIDATION OF THE THEORIES,0.23220973782771537,"Predicted
Sampled"
EMPIRICAL VALIDATION OF THE THEORIES,0.2340823970037453,"4
2
0
2
4
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
EMPIRICAL VALIDATION OF THE THEORIES,0.23595505617977527,Probability
EMPIRICAL VALIDATION OF THE THEORIES,0.23782771535580524,Full layer training
EMPIRICAL VALIDATION OF THE THEORIES,0.2397003745318352,"Predicted
Sampled"
EMPIRICAL VALIDATION OF THE THEORIES,0.24157303370786518,"Figure 1: (1st-2nd column) Posterior predictives obtained from NNGP (left) and scale mixture of
NNGPs with inverse gamma prior (right). (3rd-5th column) correspondence between wide ﬁnite
model vs theoretical limit."
EMPIRICAL VALIDATION OF THE THEORIES,0.24344569288389514,"Table 1: NLL values on UCI dataset. (m, d) denotes number of data points and features, respec-
tively. We take results from Adlam et al. (2020) except our model."
EMPIRICAL VALIDATION OF THE THEORIES,0.24531835205992508,"Dataset
(m, d)
PBP-MV
Dropout
Ensembles
RBF
NNGP
Ours"
EMPIRICAL VALIDATION OF THE THEORIES,0.24719101123595505,"Boston Housing
(506, 13)
2.54 ± 0.08
2.40 ± 0.04
2.41 ± 0.25
2.63 ± 0.09
2.65 ± 0.13
2.72 ± 0.05
Concrete Strength
(1030, 8)
3.04 ± 0.03
2.93 ± 0.02
3.06 ± 0.18
3.52 ± 0.11
3.19 ± 0.05
3.13 ± 0.04
Energy Efﬁciency
(768, 8)
1.01 ± 0.01
1.21 ± 0.01
1.38 ± 0.22
0.78 ± 0.06
1.01 ± 0.04
0.67 ± 0.04
Kin8nm
(8192, 8)
-1.28 ± 0.01
−1.14 ± 0.01
−1.20 ± 0.02
−1.11 ± 0.01
−1.15 ± 0.01
−1.18 ± 0.01
Naval Propulsion
(11934, 16)
−4.85 ± 0.06
−4.45 ± 0.00
−5.63 ± 0.05
-10.07 ± 0.01
−10.01 ± 0.01
−8.04 ± 0.04
Power Plant
(9568, 4)
2.78 ± 0.01
2.80 ± 0.01
2.79 ± 0.04
2.94 ± 0.01
2.77 ± 0.02
2.66 ± 0.01
Wine Quality Red
(1588, 11)
0.97 ± 0.01
0.93 ± 0.01
0.94 ± 0.12
−0.78 ± 0.07
-0.98 ± 0.06
−0.77 ± 0.07
Yacht Hydrodynamics
(308, 6)
1.64 ± 0.02
1.25 ± 0.01
1.18 ± 0.21
0.49 ± 0.06
1.07 ± 0.27
0.17 ± 0.25"
REGRESSION WITH GAUSSIAN LIKELIHOODS,0.24906367041198502,"4.2
REGRESSION WITH GAUSSIAN LIKELIHOODS"
REGRESSION WITH GAUSSIAN LIKELIHOODS,0.250936329588015,"We tested the scale mixtures of NNGPs and other models on various real-world regression tasks. All
the models are trained with the squared loss function. We expect the scale mixtures of NNGPs to be
better calibrated than NNGP due to their heavy-tail behaviors. To see this, we ﬁtted the scale mixture
of NNGPs with inverse gamma prior on eight datasets collected from UCI repositories 1 and measured
the negative log-likelihood values on the test set. Table 1 summarizes the result. The results other
than ours are borrowed from Adlam et al. (2020). In summary, ours generally performed similarly
to NNGP except for some datasets for which ours signiﬁcantly outperformed NNGP. Considering the
fact that Student’s t processes include GPs as limiting cases, this result suggests that one can use the
scale mixtures as an alternative to NNGPs even for the datasets not necessarily including heavy-tailed
noises. See Appendix G for detailed settings for the training."
CLASSIFICATION WITH GAUSSIAN LIKELIHOOD,0.25280898876404495,"4.3
CLASSIFICATION WITH GAUSSIAN LIKELIHOOD"
CLASSIFICATION WITH GAUSSIAN LIKELIHOOD,0.2546816479400749,"Following the literature, we apply the NNGPs and the scale mixtures of NNGPs to classiﬁcation
problems with Gaussian likelihoods (squared loss). We summarize the results in Appendix F. As a
brief summary, ours with heavy-tailed priors (inverse gamma, Burr Type XII) outperformed NNGP
in terms of uncertainty calibration for various datasets including corrupted datasets."
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.2565543071161049,"4.4
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.25842696629213485,"We compared the NNGP and the scale mixture of NNGPs with inverse-gamma priors on the image
classiﬁcation task. Following the standard setting in image classiﬁcation with deep neural networks,
we computed the posterior predictive distributions of the BNNs under categorical likelihood. Since
both NNGPs and the scale mixtures of NNGPs do not admit closed-form posteriors, we employed
SVGP as an approximate inference method for NNGP (Appendix C), and extended the SVGP for the
scale mixture of NNGPs (Appendix D). We call the latter as Sparse Variational Student’s t Process
(SVTP) since it approximates the posteriors whose limit corresponds to Student’s t process. We com-
pared SVGP and SVTP on multiple image classiﬁcation benchmarks, including MNIST, CIFAR10,
and SVHN. We also evaluated the predictive performance on Out-Of-Distribution (OOD) data for
which we intentionally removed three training classes to save as OOD classes to be tested. We used
four-layer CNN as a base model to compute NNGP kernels. See Appendix G for details. Table 2
summarizes the results, which show that SVTP and SVGP perform similarly for in-distribution data,"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.2602996254681648,1https://archive.ics.uci.edu/ml/datasets.php
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.26217228464419473,Published as a conference paper at ICLR 2022
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.2640449438202247,"Table 2: Classiﬁcation accuracy and NLL of SVGP and SVTP for image datasets and their variants.
NLL values are multiplied by 102."
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.26591760299625467,"SVGP
SVTP (Ours)"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.26779026217228463,"Dataset
NLL (×102)
Accuracy (%)
NLL (×102)
Accuracy (%)"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.2696629213483146,"MNIST
8.96 ± 0.12
97.73 ± 0.03
8.90 ± 0.04
97.78 ± 0.04
+ Shot
24.22 ± 0.08
94.63 ± 0.05
24.28 ± 0.10
94.63 ± 0.07
+ Impulse
56.52 ± 0.88
90.29 ± 0.65
57.91 ± 0.57
89.36 ± 0.58
+ Spatter
16.79 ± 0.19
95.95 ± 0.04
16.66 ± 0.05
95.99 ± 0.04
+ Glass Blur
100.65 ± 2.35
62.63 ± 0.65
97.19 ± 2.23
63.52 ± 0.74
w. OOD
216.58 ± 1.83
67.70 ± 0.03
206.86 ± 1.85
67.67 ± 0.03"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.27153558052434457,"KMNIST
53.93 ± 0.13
83.92 ± 0.09
53.95 ± 0.30
83.96 ± 0.08
w. OOD
268.41 ± 2.40
60.76 ± 0.06
257.16 ± 1.86
60.46 ± 0.05"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.27340823970037453,"Fashion MNIST
34.30 ± 0.10
87.84 ± 0.13
34.25 ± 0.13
87.90 ± 0.05
w. OOD
252.61 ± 3.51
62.29 ± 0.04
241.69 ± 2.45
62.24 ± 0.06"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.2752808988764045,"SVGP
SVTP (Ours)"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.27715355805243447,"Dataset
NLL (×102)
Accuracy (%)
NLL (×102)
Accuracy (%)"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.27902621722846443,"CIFAR10
131.96 ± 0.35
54.09 ± 0.12
132.13 ± 0.24
54.10 ± 0.13
+ Shot 5
143.11 ± 0.29
49.43 ± 0.14
142.30 ± 0.18
49.73 ± 0.05
+ Impulse 5
164.90 ± 0.11
41.66 ± 0.19
160.79 ± 0.75
43.08 ± 0.34
+ Spatter 5
141.11 ± 0.30
50.34 ± 0.17
141.14 ± 0.15
50.31 ± 0.15
+ Fog 5
213.50 ± 0.19
25.47 ± 0.21
209.31 ± 0.48
26.03 ± 0.16
+ Snow 5
166.44 ± 0.51
41.47 ± 0.27
166.41 ± 0.28
41.47 ± 0.11
w. OOD
341.59 ± 1.75
41.83 ± 0.11
333.70 ± 1.83
41.19 ± 0.17"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.2808988764044944,"EMNIST
56.49 ± 1.24
84.25 ± 0.22
54.92 ± 0.84
84.55 ± 0.32
w. OOD
183.25 ± 1.40
72.58 ± 0.28
177.65 ± 0.43
72.38 ± 0.16"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.28277153558052437,"SVHN
100.88 ± 0.17
71.67 ± 0.11
101.04 ± 0.13
71.71 ± 0.06
w. OOD
379.75 ± 9.24
46.86 ± 0.19
360.40 ± 3.68
46.43 ± 0.08"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.2846441947565543,"Table 3: Classiﬁcation accuracy and NLL of ensemble models for image datasets and their variants.
We used 8 models of 4-layer CNN for our base ensemble model. NLL values are multiplied by 102."
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.28651685393258425,"Gaussian
Inverse Gamma Prior (Ours)"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.2883895131086142,"Dataset
NLL (×102)
Accuracy (%)
NLL (×102)
Accuracy (%)"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.2902621722846442,"MNIST
0.33 ± 0.01
98.94 ± 0.04
0.32 ± 0.01
98.98 ± 0.04
+ Shot
1.42 ± 0.14
95.73 ± 0.48
1.42 ± 0.17
95.73 ± 0.46
+ Impulse
5.91 ± 0.68
83.10 ± 1.32
6.27 ± 0.78
82.80 ± 1.31
+ Spatter
0.76 ± 0.02
97.58 ± 0.12
0.74 ± 0.02
97.63 ± 0.03
+ Glass Blur
3.25 ± 0.31
88.50 ± 1.27
2.83 ± 0.20
90.22 ± 0.80
w. OOD
82.71 ± 2.65
68.51 ± 0.01
74.31 ± 2.93
68.47 ± 0.03
w. Imbalance
1.50 ± 0.21
95.85 ± 0.44
1.50 ± 0.23
95.82 ± 0.49
w. Noisy Label
7.70 ± 0.06
97.62 ± 0.04
7.63 ± 0.06
97.56 ± 0.03"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.29213483146067415,"CIFAR10
8.68 ± 0.06
70.77 ± 0.31
8.74 ± 0.08
70.22 ± 0.30
+ Shot 5
16.50 ± 0.27
51.48 ± 0.34
15.28 ± 0.38
53.28 ± 0.37
+ Impulse 5
32.39 ± 1.11
33.51 ± 1.24
29.20 ± 1.49
35.66 ± 1.56
+ Spatter 5
12.83 ± 0.23
58.60 ± 0.59
12.36 ± 0.17
59.36 ± 0.26
+ Fog 5
15.40 ± 0.07
45.23 ± 0.07
15.28 ± 0.06
45.73 ± 0.22
+ Snow 5
13.18 ± 0.25
57.08 ± 0.48
12.76 ± 0.08
57.66 ± 0.37
w. OOD
81.57 ± 0.01
50.66 ± 0.13
80.62 ± 0.86
50.46 ± 0.23
w. Imbalance
19.22 ± 0.12
38.73 ± 1.35
19.05 ± 0.18
39.74 ± 0.37
w. Noisy Label
15.88 ± 0.03
54.90 ± 0.34
15.90 ± 0.05
54.81 ± 0.38"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.2940074906367041,"Gaussian
Inverse Gamma Prior (Ours)"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.2958801498127341,"Dataset
NLL (×102)
Accuracy (%)
NLL (×102)
Accuracy (%)"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.29775280898876405,"KMNIST
2.74 ± 0.04
93.24 ± 0.20
2.75 ± 0.02
93.34 ± 0.16
w. OOD
78.64 ± 2.14
65.64 ± 0.07
70.06 ± 3.14
65.61 ± 0.13
w. Imbalance
9.47 ± 0.27
77.04 ± 0.96
9.12 ± 0.57
77.23 ± 1.67
w. Noisy Label
10.78 ± 0.03
84.42 ± 0.10
10.67 ± 0.03
84.30 ± 0.12"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.299625468164794,"Fashion MNIST
2.36 ± 0.04
91.92 ± 0.11
2.37 ± 0.02
91.82 ± 0.11
w. OOD
62.17 ± 0.50
64.47 ± 0.09
58.57 ± 0.97
64.48 ± 0.07
w. Imbalance
5.54 ± 0.03
83.09 ± 0.12
5.63 ± 0.08
83.00 ± 0.11
w. Noisy Label
9.30 ± 0.08
87.05 ± 0.07
9.20 ± 0.06
87.27 ± 0.19"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.301498127340824,"EMNIST
0.76 ± 0.01
91.25 ± 0.13
0.75 ± 0.01
91.30 ± 0.05
w. OOD
8.37 ± 0.15
76.59 ± 0.09
8.25 ± 0.17
76.72 ± 0.08
w. Noisy Label
3.23 ± 0.02
86.64 ± 0.17
3.15 ± 0.01
86.25 ± 0.20"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.30337078651685395,"SVHN
4.68 ± 0.05
87.84 ± 0.17
4.70 ± 0.04
87.82 ± 0.13
w. OOD
105.17 ± 1.75
56.92 ± 0.03
101.87 ± 1.92
56.92 ± 0.03
w. Imbalance
14.20 ± 0.17
63.64 ± 0.32
14.88 ± 0.19
61.97 ± 0.92
w. Noisy Label
12.22 ± 0.11
80.24 ± 0.20
12.36 ± 0.07
79.84 ± 0.24"
CLASSIFICATION WITH CATEGORICAL LIKELIHOOD,0.3052434456928839,"but SVTP signiﬁcantly outperforms SVGP for OOD data in terms of uncertainty calibration (measured
by negative log-likelihoods)."
CLASSIFICATION BY FINITE MODEL ENSEMBLE,0.30711610486891383,"4.5
CLASSIFICATION BY FINITE MODEL ENSEMBLE"
CLASSIFICATION BY FINITE MODEL ENSEMBLE,0.3089887640449438,"Although we proved Theorem 3.3 only for fully-connected networks trained with squared losses,
Lee et al. (2019) empirically observed that there still seem to be a similar correspondence for neural
networks trained with cross-entropy loss. To this end, we tested the ensembles of wide ﬁnite CNNs
with 4 layers trained by gradient descent over the cross-entropy loss. Each model in our test is ini-
tialized by the NTK parameterization with random scales drawn from inverse gamma prior on the last
layer (ours) or without such random scales (NNGP). For each prior setting, we constructed ensem-
bles of 8 models, and compared the performance on MNIST, MNIST-like variants, CIFAR10, and
SVHN. We also compared the models under the presence of corruption, OOD data, label imbalance,
and label noises. See Appendix G for detailed description for the experimental setting. As for the
previous experiments, Table 3 demonstrates that ours with inverse gamma prior largely outperforms
the baseline, especially for the OOD or corrupted data."
CONCLUSION,0.31086142322097376,"5
CONCLUSION"
CONCLUSION,0.31273408239700373,"In this paper, we proposed a simple extension of NNGPs by introducing a scale prior on the last-
layer weight parameters. The resulting method, entitled as the scale mixture of NNGPs, deﬁnes a
broad class of stochastic processes, especially the heavy-tailed ones such as Student’s t processes.
Based on the result in (Yang, 2019), we have shown that an inﬁnitely-wide BNN of any architecture
constructed in a speciﬁc way corresponds to the scale mixture of NNGPs. Also, we have extended
the existing convergence results of inﬁnitely-wide BNNs trained with gradient descent to GPs so
that the results hold for our construction. Our empirical evaluation validates our theory and shows
promising results for multiple real-world regression and classiﬁcation tasks. Especially, it shows
that the heavy-tailed processes from our construction are robust to the out-of-distribution data."
CONCLUSION,0.3146067415730337,Published as a conference paper at ICLR 2022
CONCLUSION,0.31647940074906367,ACKNOWLEDGEMENTS AND DISCLOSURE OF FUNDING
CONCLUSION,0.31835205992509363,"We would like to thank Jaehoon Lee for helping us to understand his results on NNGP and NTK.
This work was partly supported by Institute of Information & communications Technology Planning
& Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artiﬁcial
Intelligence Graduate School Program(KAIST), No. 2021-0-02068, Artiﬁcial Intelligence Inno-
vation Hub), National Research Foundation of Korea (NRF) funded by the Ministry of Education
(NRF2021R1F1A1061655, NRF-2021M3E5D9025030). HY was supported by the Engineering
Research Center Program through the National Research Foundation of Korea (NRF) funded by the
Korean Government MSIT (NRF-2018R1A5A1059921) and also by the Institute for Basic Science
(IBS-R029-C1)."
REFERENCES,0.3202247191011236,REFERENCES
REFERENCES,0.32209737827715357,"Ben Adlam, Jaehoon Lee, Lechao Xiao, Jeffrey Pennington, and Jasper Snoek. Exploring the un-
certainty properties of neural networks’ implicit priors in the inﬁnite-width limit. arXiv preprint
arXiv:2010.07355, 2020."
REFERENCES,0.32397003745318353,"D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. Journal of the Royal
Statistical Society: Series B (Methodological), 36, 1974."
REFERENCES,0.3258426966292135,"D. Bracale, S. Favaro, S. Fortini, and S. Peluchetti. Inﬁnite-channel deep stable convolutional neural
networks. arXiv:2021.03739, 2021."
REFERENCES,0.32771535580524347,"James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:
//github.com/google/jax."
REFERENCES,0.3295880149812734,"S. Favaro, S. Fortini, and S. Peluchetti. Stable behaviour of inﬁnitely wide deep neural networks. In
Proceedings of The 23rd International Conference on Artiﬁcial Intelligence and Statistics (AIS-
TATS 2020), 2020."
REFERENCES,0.33146067415730335,"Adrià Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional net-
works as shallow Gaussian processes. In International Conference on Learning Representations
(ICLR), 2019."
REFERENCES,0.3333333333333333,"Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Inﬁnite attention: NNGP
and NTK for deep attention networks. In Proceedings of The 37th International Conference on
Machine Learning (ICML 2020), pp. 4376–4386. PMLR, 2020."
REFERENCES,0.3352059925093633,"A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: convergence and generalization in
neural networks. In Advances in Neural Information Processing Systems 31 (NeurIPS 2018),
2018."
REFERENCES,0.33707865168539325,"J. Lee, L. Xiao, S. S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington. Wide
neural networks of any depth evolve as linear models under gradient descent. In Advances in
Neural Information Processing Systems 32 (NeurIPS 2019), 2019."
REFERENCES,0.3389513108614232,"Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as Gaussian processes. In International Conference on
Learning Representations (ICLR), 2018."
REFERENCES,0.3408239700374532,"A. G. de G. Matthews, J. Hron, R. E. Turner, and Z. Ghahramani. Sample-then-optimize posterior
sampling for Bayesian linear models. NeurIPS Workshop on Advances in Approximate Bayesian
Inference, 2017."
REFERENCES,0.34269662921348315,"Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahra-
mani. Gaussian process behaviour in wide deep neural networks. In International Conference on
Learning Representations (ICLR), 2018."
REFERENCES,0.3445692883895131,"R. M. Neal. Priors for inﬁnite networks. In Bayesian Learning for Neural Networks, pp. 29–53.
Springer, 1996."
REFERENCES,0.3464419475655431,Published as a conference paper at ICLR 2022
REFERENCES,0.34831460674157305,"Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abo-
laﬁa, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with
many channels are Gaussian processes. In International Conference on Learning Representations
(ICLR), 2018."
REFERENCES,0.350187265917603,"Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy inﬁnite neural networks in python.
In International Conference on Learning Representations (ICLR), 2020.
URL https://
github.com/google/neural-tangents."
REFERENCES,0.352059925093633,"A. Shah, A. Wilson, and Z. Ghahramani. Student-t processes as alternatives to Gaussian processes.
In Proceedings of The 17th International Conference on Artiﬁcial Intelligence and Statistics (AIS-
TATS 2014), 2014."
REFERENCES,0.3539325842696629,"Joram Soch and Carsten Allefeld. Kullback-leibler divergence for the normal-gamma distribution.
arXiv preprint arXiv:1611.01437, 2016."
REFERENCES,0.35580524344569286,"M. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Proceedings
of The 12th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2009),
2009."
REFERENCES,0.35767790262172283,"Greg Yang. Tensor programs I: Wide feedforward or recurrent neural networks of Any architecture
are Gaussian processes. arXiv preprint arXiv:1910.12478, 2019."
REFERENCES,0.3595505617977528,Published as a conference paper at ICLR 2022
REFERENCES,0.36142322097378277,"A
DETAILS ON TENSOR PROGRAMS"
REFERENCES,0.36329588014981273,"In this section, we will review tensor programs from (Yang, 2019), which are sequences of assign-
ment statements followed by a return statement where each variable is updated at most once."
REFERENCES,0.3651685393258427,"Tensor programs use the three types of variables: G-variables, H-variables, and A-variables. G-
and H-variables are vector-type variables, and A-variables are matrix-type variables. Each tensor
program is parameterized by n ∈N, which determines the dimension of all vectors (i.e., G- and
H-variables) and matrices (i.e., A-variables) in the program to n and n × n, respectively. Due to
the random initialization of some of these variables, the contents of these variables are random in
general. G-variables denote Gaussian-distributed random variables, and H-variables store the results
of applying (usually nonlinear) functions on G-variables. A-variables store random matrices whose
entries are set with iid samples from a Gaussian distribution. All H-variables in a tensor program are
set their values by some assignments, but some G-variables and all H-variables may not be assigned
to in the program. Such G-variables are called input G-variables."
REFERENCES,0.36704119850187267,"Assignments in tensor programs have one of the three kinds: MatMul, LinComb, and Nonlin. The
MatMul assignments have the form y = Wx where y is a G-variable, W is an A-variable, and x is
a H-variable. The LinComb assignments compute linear combinations, and they have the form y =
Pl
i=1 wixi where y, x1, . . . , xl are G-variables, and w1, . . . , wl ∈R are constants. The ﬁnal Nonlin
assignments apply (usually nonlinear) scala functions on the values of G-variables coordinatewise.
That is, they have the form y = φ(x1, . . . , xl), where y is an H-variable, x1, . . . , xl are G-variables,
φ : Rl →R is a possibly nonlinear function, and φ(x1, . . . , xl) means the lifted application of φ to
vector arguments."
REFERENCES,0.36891385767790263,Every tensor program ends with a return statement of the form:
REFERENCES,0.3707865168539326,"return(v1⊤y1/√n, . . . , vl⊤yl/√n),
(6)"
REFERENCES,0.37265917602996257,"where v1, . . . , vl are G-variables not appearing anywhere in the program, and y1, . . . , yl are H-
variables.
Assumption A.1. Every A-variable W in a tensor program is initialized with iid samples from
the Gaussian distribution with mean zero and covariance σ2
W /n for some σW > 0 (i.e., Wij ∼
N(0, σ2
W /n) for i, j ∈[n]), and these samples are independent with those used for other A-variables
and input G-variables. For every α ∈[n], the components of all input G-variables x1, . . . , xm
(which store vectors in Rn) are initialized with independent samples from N(µin, Σin) for some
mean µin and (possibly singular) covariance Σin over all input G-variables. Note that the mean
and the covariance do not depend on the component index α ∈[n]. Finally, the input G-variables
vi used in the return statement of the program are independent with all the other random variables
in the program, and their components are set by iid samples from N(0, σ2
v).
Deﬁnition A.1. Given a tensor program satisfying Assumption A.1, we can compute µ and Σ for
not only input G-variables but all the G-variables in the program. Here the dimension of the vector
µ is the number N of all G-variables. The computed µ and Σ specify the multivariate Gaussian
distribution over the α-th components of G-variables that arises at the inﬁnite-width limit (Theo-
rem A.2). The computation is deﬁned inductively as follows. Let g1, . . . , gN be all the G-variables
in the program, ordered as they appear in the program. For any G-variable gk, µ(gk) is deﬁned by:"
REFERENCES,0.37453183520599254,"µ(gk) = 
 "
REFERENCES,0.37640449438202245,"µin(gk)
if gk is an input G-variable
Pm
i=1 wiµ(xi)
if gk = Pm
i=1 aixi"
OTHERWISE,0.3782771535580524,"0
otherwise
."
OTHERWISE,0.3801498127340824,"Here x1, . . . , xm are some G-variables. For any pair of G-variables gk, gl, Σ(gk, gl) is deﬁned by:"
OTHERWISE,0.38202247191011235,"Σ(gk, gl) ="
OTHERWISE,0.3838951310861423,"





"
OTHERWISE,0.3857677902621723,"




"
OTHERWISE,0.38764044943820225,"Σin(gk, gl)
if gk, gl are input G-variables
Pm
i=1 wiΣ(xi, gl)
if gk = Pm
i=1 wixi
Pm
i=1 wiΣ(gk, xi)
if gl = Pm
i=1 wixi"
OTHERWISE,0.3895131086142322,"σ2
W EZ[φ(Z)φ′(Z)]
if gk = Wy, gl = Wy′ for the same W, with Z,φ,φ′ below
0
otherwise ."
OTHERWISE,0.3913857677902622,"Here x1, . . . , xm are some G-variables, and H-variables y and y′ in the second last case are intro-
duced by the Nonlin assignments y = φ(gi1, . . . , gip) and y′ = φ′(gi1, . . . , gip), respectively, for"
OTHERWISE,0.39325842696629215,Published as a conference paper at ICLR 2022
OTHERWISE,0.3951310861423221,"some i1, . . . , ip ∈[max(k −1, l −1)]. We have adjusted the inputs of φ and φ′ to ensure that
they take the same set of G-variables. The random variable Z in the second last case is a random
Gaussian vector drawn from N(µ′, Σ′) where µ′ and Σ′ are the mean vector and covariance matrix
for gi1, . . . , gip by the previous step of this inductive construction."
OTHERWISE,0.3970037453183521,"Deﬁnition A.2. Given h : Rn →R and for any x ∈Rn if |h(x)| is upper bounded by a function
exp(C∥x∥2−ϵ + c) with C, c, ϵ > 0, then this function is controlled."
OTHERWISE,0.398876404494382,"Theorem A.2 (Master Theorem). Consider a tensor program satisfying Assumption A.1 and using
only controlled φ’s. Let g1, . . . , gN be all of the G-variables that appear in this program, and
h : RN →R be a controlled function. Then, as n tends to ∞,"
N,0.40074906367041196,"1
n n
X"
N,0.40262172284644193,"α=1
h(g1
α, . . . , gN
α )
a.s.
−−→EZ∼N(µ,Σ)[h(Z1, . . . , ZN)]."
N,0.4044943820224719,"Here
a.s.
−−→refers to almost sure convergence, and Z = (Z1, . . . , ZN) ∈RN. Also, µ and Σ are
from Deﬁnition A.1."
N,0.40636704119850187,"Corollary A.3. Consider a tensor program satisfying Assumption A.1 and using only controlled
φ’s. Let g1, . . . , gN be all of the G-variables that appear in this program. Assume that the program
returns a vector (v⊤y1/√n, . . . , v⊤yk/√n) for some H-variables yi’s such that each yi is updated
by yi = φi(g1, . . . , gN) for a controlled φi. Then, as n approaches to ∞, the joint distribution of
this vector converges weakly to the multivariate Gaussian distribution N(0, K) with the following
covariance matrix:"
N,0.40823970037453183,"K(i,j) = σ2
v · EZ∼N(µ,Σ)[φi(Z)φj(Z)] for all i, j ∈[k],"
N,0.4101123595505618,where µ and Σ are from Deﬁnition A.1.
N,0.41198501872659177,"B
PROOFS"
N,0.41385767790262173,"In this section, we provide the proofs of our three main results. We ﬁrst describe a lemma related to
a basic property of Student’s t distribution:"
N,0.4157303370786517,"Lemma B.1. The class of multivariate t distributions is closed under marginalization and condi-
tioning. That is, when"
N,0.41760299625468167,"x =

x1
x2"
N,0.41947565543071164,"
∈Rd1+d2 ∼MVTd1+d2"
N,0.42134831460674155,"
ν,

µ1
µ2"
N,0.4232209737827715,"
,

Σ11
Σ12
Σ21
Σ22"
N,0.4250936329588015,"
,
(7)"
N,0.42696629213483145,we have that
N,0.4288389513108614,"x1 ∼MVTd1(ν, µ1, Σ11)
and
x2|x1 ∼MVTd2"
N,0.4307116104868914,"
ν + d1, µ′
2, ν + c"
N,0.43258426966292135,"ν + d1
Σ′
22 
(8) where"
N,0.4344569288389513,"µ′
2 = µ2 + Σ21Σ−1
11 (x1 −µ1),
c = (x1 −µ1)⊤Σ−1
11 (x1 −µ1),
Σ′
22 = Σ22 −Σ21Σ−1
11 Σ12."
N,0.4363295880149813,"Deﬁnition B.1. A distribution on R+ is a inverse gamma distribution with shape parameter a ∈R+
and scale parameter b ∈R+ if it has the following density:"
N,0.43820224719101125,"p(x) =
ba G(a)  1 x"
N,0.4400749063670412,"a+1
e−b/x"
N,0.4419475655430712,"where G(·) is the gamma function. To express a random variable drawn from this distribution, we
write x ∼InvGamma(a, b)."
N,0.4438202247191011,"Lemma B.2. Let Σ ∈Rd×d be a postive deﬁnite matrix. Consider µ ∈Rn and a, b ∈R+. If"
N,0.44569288389513106,"σ2 ∼InvGamma(a, b),
x|σ2 ∼N(µ, σ2Σ),"
N,0.44756554307116103,"then the marginal distribution of x is MVTd(2a, µ, b aΣ)."
N,0.449438202247191,Now we proceed to the proofs of main theorems.
N,0.45131086142322097,Published as a conference paper at ICLR 2022
N,0.45318352059925093,"Theorem 3.1 (Convergence). As n tends to ∞, the random variable (fn(xi; vn, Ψn))i∈[K+L] con-
verges in distribution to the following random variable (f∞(xi))i∈[K+L]:"
N,0.4550561797752809,"σ2
v ∼H,
(f∞(xi))i∈[K+L]|σ2
v ∼N(0, σ2
v · K),"
N,0.45692883895131087,"where K(i,j) = EZ∼N(µ,Σ)[φ(Zi)φ(Zj)] for i, j ∈[K + L]. Furthermore, if H is the inverse-
gamma distribution with shape a and scale b, the marginal distribution of (f∞(xi))i∈[K+L] is the
following multivariate t distribution:"
N,0.45880149812734083,"(f∞(xi))i∈[K+L] ∼MVTK+L(2a, 0, (b/a) · K),"
N,0.4606741573033708,"where MVTd(ν′, µ′, K′) denotes the d-dimensional Student’s t distribution with degree of freedom
ν′, location µ′, and scale matrix K′."
N,0.46254681647940077,"Proof. Let oi[n] = fn(xi; vn, Ψn) for i ∈[K + L] and n.
Pick an arbitrary bounded con-
tinuous function h : RK+L →R.
For each n, deﬁne Gn to be the σ-ﬁeld generated by
gn(x1; Ψn), . . . , gn(xK+L; Ψn)."
N,0.46441947565543074,"We claim that as n tends to ∞,"
N,0.46629213483146065,"E

h
 
o1[n], . . . , oK+L[n]

→Eσ2v∼H
h
EY 1:K+L∼N(0,σ2vK)

h
 
Y 1, . . . , Y K+Li
."
N,0.4681647940074906,The theorem follows immediately from this claim. The rest of the proof is about showing the claim.
N,0.4700374531835206,Note that
N,0.47191011235955055,"E

h
 
o1[n], . . . , oK+L[n]

= E

E

h
 1
√nv⊤
n φ1:K+L
 G, σ2
v 
,"
N,0.4737827715355805,"where φ1:K+L = [φ (gn (x1; Ψn)) , . . . , φ (gn (xK+L; Ψn))] ∈Rn×(K+L). Conditioned on G and
σ2
v, the random variable
1
√nv⊤
n φ1:K+L in the nested expectation from above is distributed by a
multivariate Gaussian. The mean of this Gaussian distribution is:"
N,0.4756554307116105,"E
 1
√nv⊤
n φ1:K+L
 G, σ2
v"
N,0.47752808988764045,"
= E

v⊤
n
 G, σ2
v

×
1
√nφ1:K+L"
N,0.4794007490636704,= 0 ∈RK+L.
N,0.4812734082397004,The covariance is:
N,0.48314606741573035,"K[n](i,j) = E
 1
√nv⊤
n φ (gn (xi; Ψn))
  1
√nv⊤
n φ (gn (xj; Ψn))
 G, σ2
v  = 1 n n
X α=1 n
X"
N,0.4850187265917603,"β=1
E

vn,αvn,β| G, σ2
v

× (φ (gn (xi; Ψn)))α (φ (gn (xj; Ψn)))β = 1 nσ2
v n
X"
N,0.4868913857677903,"α=1
(φ (gn (xi; Ψn)))α (φ (gn (xj; Ψn)))α ."
N,0.4887640449438202,"Using what we have observed so far, we compute the limit in our claimed convergence:"
N,0.49063670411985016,"lim
n→∞E

h
 
o1[n], . . . , oK+L[n]

= lim
n→∞E
h
Eu1:K+L∼N(0,K[n])

h
 
u1:K+Li"
N,0.49250936329588013,"= E
h
lim
n→∞Eu1:K+L∼N(0,K[n])

h(u1:K+L)
i
."
N,0.4943820224719101,"The last equality follows from the dominated convergence theorem and the fact that h is bounded. To
go further in our computation, we observe that K[n] converges almost surely to σ2
vK by Theorem 2.1
and so"
N,0.49625468164794007,"Eu1:K+L∼N(0,K[n])[h(u1:K+L)] =
Z
h(u1:K+L)N(u1:K+L; o, K[n])du1:K+L"
N,0.49812734082397003,"a.s.
−−→
Z
h(u1:K+L)N(u1:K+L; 0, σ2
vK)du1:K+L"
N,0.5,"= EY 1:K+L∼N(0,σ2vK)

h
 
Y 1:K+L
."
N,0.50187265917603,Published as a conference paper at ICLR 2022
N,0.5037453183520599,"Here we use N not just for the multivariate Gaussian distribution but also for its density, and derive
the almost sure convergence from the dominated convergence theorem. Using this observation, we
complete our computation:"
N,0.5056179775280899,"E
h
lim
n→∞Eu1:K+L∼N(0,K[n])

h(u1:K+L)
i
= E
h
EY 1:K+L∼N(0,σ2vK)

h
 
Y 1:K+Li"
N,0.5074906367041199,"= Eσ2v∼H
h
EY 1:K+L∼N(0,σ2vK)

h
 
Y 1:K+Li
."
N,0.5093632958801498,"Theorem 3.2 (Convergence under Readout-Layer Training). Assume that we train only the readout
layers of the BNNs using the training set Dtr under mean squared loss and inﬁnitesimal step size.
Formally, this means the network parameters are evolved under the following differential equations:"
N,0.5112359550561798,"(v(0)
n , Ψ(0)
n ) = (vn, Ψn),
dv(t)
n
dt
= K
X i=1"
N,0.5131086142322098,"
yi −fn(xi; v(t)
n , Ψ(t)
n )
2φ(gn(xi; Ψ(t)
n ))
K√n
,
dΨ(t)
n
dt
= 0."
N,0.5149812734082397,"Then, the time and width limit of the random variables (fn(xK+i; v(t)
n , Ψ(t)
n ))i∈[L], denoted
(f ∞
∞(xK+i))i∈[L], is distributed by the following mixture of Gaussians:"
N,0.5168539325842697,"σ2
v ∼H,
(f ∞
∞(xK+i))i∈[L]|σ2
v ∼N
"
N,0.5187265917602997,"Kte,trK
−1
tr,trYtr

, σ2
v
"
N,0.5205992509363296,"Kte,te −Kte,trK
−1
tr,trKtr,te

,"
N,0.5224719101123596,"where Ytr consists of the y values in Dtr (i.e., Ytr = (y1, . . . , yK)) and the K’s with different sub-
scripts are the restrictions of K in Theorem 3.1 with training or test inputs as directed by those sub-
scripts. Furthermore, if H is the inverse-gamma distribution with shape a and scale b, the marginal
distribution of (f ∞
∞(xK+i))i∈[L] is the following multivariate t distribution:"
N,0.5243445692883895,"(f ∞
∞(xK+i))i∈[L] ∼MVTL"
N,0.5262172284644194,"
2a,
"
N,0.5280898876404494,"Kte,trK
−1
tr,trYtr

, b a "
N,0.5299625468164794,"Kte,te −Kte,trK
−1
tr,trKtr,te

."
N,0.5318352059925093,"Our proof of this theorem reuses the general structure of the proof of the corresponding theorem
for NNGPs in (Lee et al., 2019), but it adjusts the structure slightly so as to use the master theorem
for tensor programs and cover general architectures expressed by those programs. Another new
important part of the proof is to condition on the random scale σ2
v, so that the existing results such
as the master theorem can be used in our case."
N,0.5337078651685393,"Proof. Note that Ψ(t)
n = Ψ(0)
n
for all t because dΨ(t)
n /dt = 0. Let"
N,0.5355805243445693,"¯φi[n] =
1
√nφ

gn

xi; Ψ(t)
n

=
1
√nφ

gn

xi; Ψ(0)
n

for i ∈[K + L],"
N,0.5374531835205992,"¯Φtr[n] =
¯φ1[n], . . . , ¯φK[n]
⊤∈RK×n,
¯Φte[n] =
¯φK+1[n], . . . , ¯φK+L[n]
⊤∈RL×n."
N,0.5393258426966292,"Using this notation, we rewrite the differential equation for v(t)
n as follows:"
N,0.5411985018726592,"d¯v(t)
n
dt
= −2"
N,0.5430711610486891,"K
¯Φtr[n]⊤
¯Φtr[n] ¯v(t)
n −Ytr

."
N,0.5449438202247191,"This ordinary differential equation has a closed-form solution, which we write below:"
N,0.5468164794007491,"¯v(t)
n = ¯v(0)
n
+ ¯Φtr[n]⊤  ¯Φtr[n] ¯Φtr[n]⊤−1 
exp

−2"
N,0.548689138576779,"K t ¯Φtr[n] ¯Φtr[n]⊤

−I
 
¯Φtr[n] ¯v(0)
n
−Ytr

."
N,0.550561797752809,"We multiply both sides of the equation with the ¯Φte[n] matrix, and get"
N,0.552434456928839,"¯Φte[n] ¯v(t)
n = ¯Φte[n] ¯v(0)
n
+ K[n]te,trK[n]−1
tr,tr"
N,0.5543071161048689,"
exp

−2"
N,0.5561797752808989,"K tK[n]tr,tr"
N,0.5580524344569289,"
−I
 
¯Φtr[n] ¯v(0)
n
−Ytr
"
N,0.5599250936329588,"where K[n]te,tr = ¯Φte[n] ¯Φtr[n]⊤and K[n]tr,tr = ¯Φtr[n] ¯Φtr[n]⊤. Then, we send t to ∞, and obtain"
N,0.5617977528089888,"¯Φte[n] ¯v(∞)
n
= ¯Φte[n] ¯v(0)
n
−K[n]te,trK[n]−1
tr,tr

¯Φtr[n] ¯v(0)
n
−Ytr
"
N,0.5636704119850188,"= K[n]te,trK[n]−1
tr,trYtr + ¯Φte[n] ¯v(0)
n
−K[n]te,trK[n]−1
tr,tr

¯Φtr[n] ¯v(0)
n

."
N,0.5655430711610487,Published as a conference paper at ICLR 2022 But
N,0.5674157303370787,"(fn(xK+i; v(∞)
n
, Ψ(∞)
n
))i∈[L] = ¯Φte[n] ¯v(∞)
n
."
N,0.5692883895131086,"Thus, we can complete the proof of the theorem if we show that for every bounded continuous
function h : RL →R,"
N,0.5711610486891385,"lim
n→∞E "" h "
N,0.5730337078651685,"K[n]te,trK[n]−1
tr,trYtr + ¯Φte[n] ¯v(0)
n
−K[n]te,trK[n]−1
tr,tr

¯Φtr[n] ¯v(0)
n
 !#"
N,0.5749063670411985,"= E
h
h

(f ∞
∞(xK+i))i∈[L]
i
."
N,0.5767790262172284,"Pick a bounded continuous function h : RL →R. For each n, let Gn be the σ-ﬁeld generated by
gn(x1; Ψn), . . . , gn(xK+L; Ψn). Also, deﬁne K[n]tr,te = ¯Φtr[n] ¯Φte[n]⊤. We show the sufﬁcient
condition mentioned above:"
N,0.5786516853932584,"lim
n→∞E "" h "
N,0.5805243445692884,"K[n]te,trK[n]−1
tr,trYtr + ¯Φte[n] ¯v(0)
n
−K[n]te,trK[n]−1
tr,tr

¯Φtr[n] ¯v(0)
n
 !#"
N,0.5823970037453183,"= lim
n→∞E "" E "" h "
N,0.5842696629213483,"K[n]te,trK[n]−1
tr,trYtr + ¯Φte[n] ¯v(0)
n
−K[n]te,trK[n]−1
tr,tr

¯Φtr[n] ¯v(0)
n
 ! Gn, σ2
v ##"
N,0.5861423220973783,"= lim
n→∞E
h
EZ∼N(K[n]te,trK[n]−1
tr,trYtr, σ2v(K[n]te,te−K[n]te,trK[n]−1
tr,trK[n]tr,te)) [h(Z)]
i"
N,0.5880149812734082,"= E
h
lim
n→∞EZ∼N(K[n]te,trK[n]−1
tr,trYtr, σ2v(K[n]te,te−K[n]te,trK[n]−1
tr,trK[n]tr,te)) [h(Z)]
i"
N,0.5898876404494382,"= E
h
EZ∼N(Kte,trK
−1
tr,trYtr, σ2v(Kte,te−Kte,trK
−1
tr,trKtr,te)) [h(Z)]
i"
N,0.5917602996254682,"= E
h
h

(f ∞
∞(xK+i))i∈[L]
i
."
N,0.5936329588014981,"The second equality uses the fact that v(0)
n
consists of iid samples from N(0, σ2
v), and it is a conse-
quence of straightforward calculation. The third equality uses the dominated convergence theorem
and the fact that h is bounded. The fourth equality uses the master theorem for tensor programs
(Theorem 2.1), the boundedness of h, and the dominated convergence theorem. The last equality
follows from the deﬁnition of (f ∞
∞(xK+i))i∈[L]."
N,0.5955056179775281,"If σ2
v ∼InvGamma(a, b), we get the following closed-form marginal distribution by Lemma B.2:"
N,0.5973782771535581,"(f ∞
∞(xK+i))i∈[L] ∼MVTL"
N,0.599250936329588,"
2a, Kte,trK
−1
tr,trYtr, b"
N,0.601123595505618,"a(Kte,te −Kte,trK
−1
tr,trKtr,te)

."
N,0.602996254681648,"Our proof of Theorem 3.3 uses the corresponding theorem for the standard NTK setup in Lee et al.
(2019). We restate this existing theorem in our setup using our notation:"
N,0.6048689138576779,"Theorem B.3. Assume that the BNNs are multi-layer perceptrons, and we train these models us-
ing the training set Dtr under the mean squared loss and inﬁnitesimal step size. Regard σ2
v as a
deterministic value. Then as n →∞and t →∞, (fn(xK+i; v(t)
n ,
1
√nΨ(t)
n ))i∈[L] converges in
distribution to the following random variable:"
N,0.6067415730337079,"(f ∞
∞(xK+i))i∈[L] ∼N(µ∞
te , Σ∞
te ) where"
N,0.6086142322097379,"µ∞
te = Θte,trΘ−1
tr,trYtr,"
N,0.6104868913857678,"Σ∞
te = Kte,te + Θte,trΘ−1
tr,trKtr,trΘ−1
tr,trΘtr,te −(Θte,trΘ−1
tr,trKtr,te + Kte,trΘ−1
tr,trΘtr,te)."
N,0.6123595505617978,"Theorem 3.3 (Convergence under General Training). Assume that the BNNs are multi-layer percep-
trons and we train all of their parameters using the training set Dtr under mean squared loss and"
N,0.6142322097378277,Published as a conference paper at ICLR 2022
N,0.6161048689138576,"inﬁnitesimal step size. Formally, this means the network parameters are evolved under the following
differential equations:"
N,0.6179775280898876,"v(0)
n
= vn,
dv(t)
n
dt
=
2
K√n K
X i=1"
N,0.6198501872659176,"
yi −fn(xi; v(t)
n , 1
√nΨ(t)
n )

φ(gn(xi; 1
√nΨ(t)
n )),"
N,0.6217228464419475,"Ψ(0)
n
= Ψn,
dΨ(t)
n
dt
= 2 K K
X i=1"
N,0.6235955056179775,"
yi −fn(xi; v(t)
n , 1
√nΨ(t)
n )

∇Ψfn(xi; v(t)
n , 1
√nΨ(t)
n )."
N,0.6254681647940075,"Let ¯Θ be Θ with σ2
v in it set to 1 (so that Θ = σ2
v ¯Θ). Then, the time and width limit of the random
variables (fn(xK+i; v(t)
n ,
1
√nΨ(t)
n ))i∈[L] has the following distribution:"
N,0.6273408239700374,"σ2
v ∼H,
(f ∞
∞(xK+i))i∈[L]|σ2
v ∼N(µ′, σ2
vΘ′) where"
N,0.6292134831460674,"µ′ = ¯Θte,tr ¯Θ−1
tr,trYtr,"
N,0.6310861423220974,"Θ′ = Kte,te +

¯Θte,tr ¯Θ−1
tr,trKtr,tr ¯Θ−1
tr,tr ¯Θtr,te

−

¯Θte,tr ¯Θ−1
tr,trKtr,te + Kte,tr ¯Θ−1
tr,tr ¯Θtr,te

."
N,0.6329588014981273,"Here the K’s with subscripts are the ones in Theorem 3.1 and the ¯Θ’s with subscripts are similar
restrictions of ¯Θ with training or test inputs. Furthermore, if H is the inverse gamma with shape
a and scale b, the exact marginal distribution of (f ∞
∞(xK+i))i∈[L] is the following multivariate t
distribution:"
N,0.6348314606741573,"(f ∞
∞(xK+i))i∈[L] ∼MVTL(2a, µ′, (b/a) · Θ′).
(5)"
N,0.6367041198501873,"Proof. Assume that we condition on σ2
v. Then, by Theorem B.3, as we send n to ∞and then t to
∞, the conditioned random variable (fn(xK+i; v(t)
n ,
1
√nΨ(t)
n ))i∈[L] | σ2
v converges in distribution
to the following random variable:"
N,0.6385767790262172,"(f ∞
∞(xK+i))i∈[L] | σ2
v ∼N(µ∞
te , Σ∞
te ), where"
N,0.6404494382022472,"µ∞
te = ¯Θte,tr ¯Θ−1
tr,trYte,"
N,0.6423220973782772,"Σ∞
te = σ2
v
"
N,0.6441947565543071,"Kte,te + ¯Θte,tr ¯Θ−1
tr,trKtr,tr ¯Θ−1
tr,tr ¯Θtr,te −(¯Θte,tr ¯Θ−1
tr,trKtr,te + Kte,tr ¯Θ−1
tr,tr ¯Θtr,te)

."
N,0.6460674157303371,"Thus, by Lemma B.4, if we remove the conditioning on σ2
v (i.e., we marginalize over σ2
v), we get the
convergence of the unconditioned (fn(xK+i; v(t)
n ,
1
√nΨ(t)
n ))i∈[L] to the following random variable:"
N,0.6479400749063671,"σ2
v ∼H,
(f ∞
∞(xK+i))i∈[L] ∼N(µ′, σ2
vΘ′) where"
N,0.649812734082397,"µ′ = ¯Θte,tr ¯Θ−1
tr,trYtr,"
N,0.651685393258427,"Θ′ = Kte,te +

¯Θte,tr ¯Θ−1
tr,trKtr,tr ¯Θ−1
tr,tr ¯Θtr,te

−

¯Θte,tr ¯Θ−1
tr,trKtr,te + Kte,tr ¯Θ−1
tr,tr ¯Θtr,te

."
N,0.653558052434457,"In particular, when H = InvGamma(a, b), the exact marginal distribution of (f ∞
∞(xK+i))i∈[L] has
the following form by Lemma B.2:"
N,0.6554307116104869,"MVT

2α, µ′, b"
N,0.6573033707865169,"aΘ′

."
N,0.6591760299625468,"Lemma B.4. Let (Xn)n∈N be a sequence of p-dimensional random vectors, X be a p-dimensional
random variable, and Y be a random variable. If Xn | Y converges in distribution to X | Y almost
surely (with respect to the randomness of Y ) as n tends to ∞, then Xn converges in distribution to
X."
N,0.6610486891385767,Published as a conference paper at ICLR 2022
N,0.6629213483146067,Proof. Let h : Rp →R be a bounded continuous function. We have to show that
N,0.6647940074906367,"lim
n→∞E[h(Xn)] = E[h(X)]."
N,0.6666666666666666,The following calculation shows this equality:
N,0.6685393258426966,"lim
n→∞E[h(Xn)] = lim
n→∞E[ E[h(Xn) | Y ]] = E
h
lim
n→∞E[h(Xn) | Y ]
i
= E[ E[h(X) | Y ]]"
N,0.6704119850187266,= E[h(X)].
N,0.6722846441947565,"The ﬁrst and last equalities follow from the standard tower law for conditional expectation, the
second equality uses the dominated convergence theorem and the boundness of h, and the third
equality uses the fact that Xn | Y converges in distribution to X|Y almost surely."
N,0.6741573033707865,"C
STOCHASTIC VARIATIONAL GAUSSIAN PROCESS"
N,0.6760299625468165,"Let (X, y) be training points and (X∗, y∗) be test points. And let Z be inducing points. We want to
make q(f, fZ) which well approximates p(f, fZ|y) by maximizing ELBO. Then we will construct
q(f, fZ) = p(f|fZ)q(fZ) where p(f|fZ) = N(f|KXZK−1
ZZfZ, KXX −KXZK−1
ZZKZX) and
q(fZ) = N(fZ|µ, Σ). Then"
N,0.6779026217228464,"log p(y) ≥Ef,fZ∼q(f,fZ)[log p(y|f, fZ)] −KL(q(fZ)||p(fZ))."
N,0.6797752808988764,"Here we can calculate likelihood term as
Z
log p(y|f)N(f; Aµ, AΣA⊤+ D)df"
N,0.6816479400749064,"where A = KXZK−1
ZZ, B = KXX −KXZK−1
ZZKZX.
Also we can calculate KL term as
1
2"
N,0.6835205992509363,"
log(det KZZ"
N,0.6853932584269663,"det Σ
) −nZ + Tr{K−1
ZZΣ} + µ⊤K−1
ZZµ

."
N,0.6872659176029963,"We ﬁnd optimal µ, Σ and inducing points Z by using stochastic gradient descent. And with these
optimal values, we can calculate predictive distribution p(f∗|y)."
N,0.6891385767790262,"p(f∗|y) =
Z Z
p(f∗, f, fZ|y)dfdfZ"
N,0.6910112359550562,"=
Z Z
p(f∗|f, fZ)q(f, fZ)dfdfZ"
N,0.6928838951310862,"=
Z Z
p(f∗|f, fZ)p(f|fZ)q(fZ)dfdfZ"
N,0.6947565543071161,"=
Z
p(f∗|fZ)q(fZ)dfZ."
N,0.6966292134831461,"This equation can be write as p(f∗|y) = N(f∗; K∗ZK−1
ZZµ, K∗ZK−1
ZZΣ(K∗ZK−1
ZZ)⊤+ K∗∗−
K∗ZK−1
ZZK⊤
∗Z).
Now if we use reparametrization trick at fZ, we can write fZ = Lu, where L is the lower triangular
matrix from the cholesky decomposition of the matrix KZZ. Then"
N,0.6985018726591761,"p(fZ) = N(fZ; 0, LL⊤) = N(fZ; 0, KZZ)"
N,0.700374531835206,"q(fZ) = N(fZ; Lµu, LΣuL⊤)."
N,0.702247191011236,"In this case ELBO changes into
Z
log(p(y|f))N(f; ALµu, ALΣuL⊤A⊤+ B)df −KL(N(u; µu, Σu)||N(u; 0, I))."
N,0.704119850187266,"With softmax likelihood, we can use MC approximation which is"
N,0.7059925093632958,"ELBO ≃1 T
N B T
X t=1 B
X"
N,0.7078651685393258,"i=1
log p(yi|f (t)
i
) −KL(N(u; µu, Σu)||N(u; 0, I))"
N,0.7097378277153558,Published as a conference paper at ICLR 2022
N,0.7116104868913857,where B is minibatch and T is sample number of f which sampled from
N,0.7134831460674157,"N(f; ABLµu, ABLΣuL⊤A⊤
B + BB)."
N,0.7153558052434457,"Here AB = KXBZK−1
ZZ and BB = KXBXB −KXBZK−1
ZZKZXB. Finally we can calculate predic-
tive distribution with this reparametrization trick."
N,0.7172284644194756,"p(f∗|y) =
Z
p(f∗|fZ)q(fZ)dfZ"
N,0.7191011235955056,"=
Z
N(f∗; K∗ZK−1
ZZfZ, K∗∗−K∗ZK−1
∗Z KZ∗)N(fZ; Lµu, LΣuL⊤)dfZ"
N,0.7209737827715356,"=N(f∗; K∗ZK−1
ZZLµu, K∗ZK−1
ZZLΣuL⊤(K∗ZK−1
ZZ)⊤+ K∗∗−K∗ZK−1
ZZK⊤
∗Z)"
N,0.7228464419475655,Thus the ﬁnal predictive distribution p(y∗|x∗) is
N,0.7247191011235955,"log p(y∗|x∗) = log M
Y"
N,0.7265917602996255,"j
p(yj
∗|xj
∗) = M
X"
N,0.7284644194756554,"j
log p(yj
∗|xj
∗) = M
X"
N,0.7303370786516854,"j
log
Z
p(yj
∗|f∗)p(f∗|y)df∗ ≃ M
X"
N,0.7322097378277154,"j
log 1 N N
X"
N,0.7340823970037453,"i
p(yj
∗|f i
∗)"
N,0.7359550561797753,"where f i
∗s are sampled from N(f∗; K∗ZK−1
ZZLµu, K∗ZK−1
ZZLΣuL⊤(K∗ZK−1
ZZ)⊤+ K∗∗−
K∗ZK−1
ZZK⊤
∗Z)."
N,0.7378277153558053,"D
STOCHASTIC VARIATIONAL STUDENT t PROCESS"
N,0.7397003745318352,"We want to make q(f, fZ, σ2) which well approximates p(f, fZ, σ2|y) by maximizing ELBO. First
p(f, fZ, σ2) = p(f|fZ, σ2)p(fZ|σ2)p(σ2) where p(f|fZ, σ2) = N(f|KXZK−1
ZZfZ, σ2(KXX −
KXZK−1
ZZKZX)), p(fZ|σ2) = N(fZ|0, σ2KZZ) and p(σ2) = Γ−1(σ2|α, β). And q(f, fZ, σ2) =
p(f|fZ, σ2)q(fZ|σ2)q(σ2) where q(fZ|σ2) = N(fZ|µ, σ2Σ) and q(σ2) = Γ−1(σ2|a, b). Then
ELBO can be computed as"
N,0.7415730337078652,"log p(y) ≥Ef,fZ,σ2∼q(f,fZ,σ2)[log p(y|f, fZ, σ2)] −KL(q(f, fZ, σ2)||p(f, fZ, σ2))."
N,0.7434456928838952,Here KL divergence can be reformulated as
N,0.7453183520599251,"KL(q(f, fZ, σ2)||p(f, fZ, σ2)) =
Z Z
q(fZ, σ2) log q(fZ, σ2)"
N,0.7471910112359551,"p(fZ, σ2)dfZdσ2"
N,0.7490636704119851,"= KL(q(fZ, σ2)||p(fZ, σ2))."
N,0.7509363295880149,Now let’s compute likelihood part ﬁrst.
N,0.7528089887640449,"Ef,fZ,σ2∼q(f,fZ,σ2)[log p(y|f, fZ, σ2)] =
Z
q(f) log p(y|f)df"
N,0.7546816479400749,"where q(f) =
R R
p(f|fZ, σ2)q(fZ|σ2)q(σ2)dfZdσ2. And this is"
N,0.7565543071161048,"q(f) = MVT(f|2a, KXZK−1
ZZµ, b"
N,0.7584269662921348,"a(KXZK−1
ZZΣK−1
ZZKZX + KXX −KXZK−1
ZZKZX))."
N,0.7602996254681648,Published as a conference paper at ICLR 2022
N,0.7621722846441947,"By slightly modifying the KL divergence between normal-gamma distributions in Soch & Allefeld
(2016), we get"
N,0.7640449438202247,"KL(q(fZ, σ2)||p(fZ, σ2)) =
Z Z
q(fZ, σ2) log q(fZ, σ2)"
N,0.7659176029962547,"p(fZ, σ2)dfZdσ2 = 1"
A,0.7677902621722846,"2
a
b µ⊤K−1
ZZµ + 1"
A,0.7696629213483146,"2Tr(K−1
ZZΣ) + 1"
A,0.7715355805243446,2 log |KZZ|
A,0.7734082397003745,"|Σ|
−nZ"
A,0.7752808988764045,2 + α log b β
A,0.7771535580524345,−log Γ(a)
A,0.7790262172284644,Γ(α) + (a −α)ψ(a) + (β −b)a b
A,0.7808988764044944,"where ψ(·) is digamma function. Now if we use reparametrization trick at fZ, we can write fZ =
Lu, where L is the lower triangular matrix from the cholesky decomposition of the matrix KZZ.
Then"
A,0.7827715355805244,"p(fZ|σ2) = N(fZ; 0, σ2LL⊤) = N(fZ; 0, σ2KZZ)"
A,0.7846441947565543,"q(fZ|σ2) = N(fZ; Lµu, σ2LΣuL⊤)."
A,0.7865168539325843,"In this case ELBO changes into
Z
log(p(y|f))MVT(f; 2a, ALµu, b"
A,0.7883895131086143,a(ALΣuL⊤A⊤+ B))df
A,0.7902621722846442,"−KL(N(u; µu, σ2Σu)Γ−1(σ2; a, b)||N(u; 0, σ2I)Γ−1(σ2, α, β))."
A,0.7921348314606742,"With softmax likelihood, we can use MC approximation which is"
A,0.7940074906367042,"ELBO ≃1 T
N B T
X t=1 B
X"
A,0.795880149812734,"i=1
log p(yi|f (t)
i
)"
A,0.797752808988764,"−KL(N(u; µu, σ2Σu)Γ−1(σ2; a, b)||N(u; 0, σ2I)Γ−1(σ2, α, β))"
A,0.799625468164794,where B is minibatch and T is sample number of f which sampled from
A,0.8014981273408239,"MVT(f; 2a, ABLµu, b"
A,0.8033707865168539,"a(ABLΣuL⊤A⊤
B + BB))."
A,0.8052434456928839,"Here AB = KXBZK−1
ZZ and BB = KXBXB −KXBZK−1
ZZKZXB. Finally we can calculate predic-
tive distribution with this reparametrization trick."
A,0.8071161048689138,"p(f∗|y) =
Z Z
p(f∗|fZ, σ2)q(fZ|σ2)q(σ2)dfZdσ2"
A,0.8089887640449438,"=
Z Z
N(f∗; K∗ZK−1
ZZfZ, σ2(K∗∗−K∗ZK−1
∗Z KZ∗))N(fZ; Lµu, σ2(LΣuL⊤))Γ−1(a, b)dfZdσ2"
A,0.8108614232209738,"=MVT(f∗; 2a, K∗ZK−1
ZZLµu, b"
A,0.8127340823970037,"a(K∗ZK−1
ZZLΣuL⊤(K∗ZK−1
ZZ)⊤+ K∗∗−K∗ZK−1
ZZK⊤
∗Z))."
A,0.8146067415730337,Thus the ﬁnal predictive distribution p(y∗|x∗) is
A,0.8164794007490637,"log p(y∗|x∗) = log M
Y"
A,0.8183520599250936,"j
p(yj
∗|xj
∗) = M
X"
A,0.8202247191011236,"j
log p(yj
∗|xj
∗) = M
X"
A,0.8220973782771536,"j
log
Z
p(yj
∗|f∗)p(f∗|y)df∗ ≃ M
X"
A,0.8239700374531835,"j
log 1 N N
X"
A,0.8258426966292135,"i
p(yj
∗|f i
∗)"
A,0.8277153558052435,"where f i
∗s are sampled from MVT(f∗; 2a, K∗ZK−1
ZZLµu, b"
A,0.8295880149812734,"a(K∗ZK−1
ZZLΣuL⊤(K∗ZK−1
ZZ)⊤+
K∗∗−K∗ZK−1
ZZK⊤
∗Z))."
A,0.8314606741573034,Published as a conference paper at ICLR 2022
A,0.8333333333333334,"E
INFERENCE ALGORITHMS"
A,0.8352059925093633,"E.1
INFERENCE FOR REGRESSION"
A,0.8370786516853933,"In order to calculate the predictive posterior distribution for an intractable marginal distribution, we
can use self-normalized importance sampling. Consider a scale mixture of NNGPs with a prior H on
the variance σ2
v. Assume that we would like to estimate the expectation of h(y) for some function
h : R →R where the random variable y is drawn from the predictive posterior of the mixture at
some input x ∈RI under the condition on Dtr. Then, the expectation of h(y) is"
A,0.8389513108614233,"E[h(y)] =
Z
h(y)p(y|Dtr)dy"
A,0.8408239700374532,"=
Z Z
h(y)p(y|Dtr, σ2
v)p(σ2
v|Dtr)dσ2
vdy"
A,0.8426966292134831,"=
Z Z
h(y)p(y|Dtr, σ2
v)p(σ2
v, Ytr|Xtr)
p(Ytr|Xtr)
dσ2
vdy = 1 Z"
A,0.8445692883895131,"Z Z
h(y)p(y|Dtr, σ2
v)p(σ2
v, Ytr|Xtr)"
A,0.846441947565543,"p(σ2v)
p(σ2
v)dσ2
vdy"
A,0.848314606741573,"where Z =
R
p(σ2
v, Ytr|Xtr)dσ2
v. We can approximate Z as follows:"
A,0.850187265917603,"Z =
Z p(σ2
v, Ytr|Xtr)"
A,0.8520599250936329,"p(σ2v)
p(σ2
v)dσ2
v ≃1 N N
X i=1"
A,0.8539325842696629,"p(βi, Ytr|Xtr) p(βi) = 1 N N
X"
A,0.8558052434456929,"i=1
p(Ytr|Xtr, βi)"
A,0.8576779026217228,"where βis are sampled independently from H. Using this approximation, we can also approximate
expectation of h(y) as follows:"
A,0.8595505617977528,E[h(y)] = 1 Z
A,0.8614232209737828,"Z Z
h(y)p(y|Dtr, σ2
v)p(σ2
v, Ytr|Xtr)"
A,0.8632958801498127,"p(σ2v)
p(σ2
v)dσ2
vdy ≃ N
X i=1"
A,0.8651685393258427,"wi
PN
j=1 wj
h(yi)"
A,0.8670411985018727,"where the yi’s are sampled from the posterior of the Gaussian distribution and the wi’s are the
corresponding importance weights for all i ∈[N]:"
A,0.8689138576779026,"βi ∼H,
wi = N(Ytr; 0, βiKtr,tr),
yi ∼N(Kx,trK
−1
tr,trYtr, βi(Kx,x −Kx,trK
−1
tr,trKtr,x))."
A,0.8707865168539326,"The K is the covariance matrix computed with test input as in Theorem 3.1. To speed up calculation,
we removed duplicated calculations in our importance weights and also during the sampling of the
yi’s. First, for importance weights, we observe that the log likelihood of βi has the form:"
A,0.8726591760299626,"log N(Ytr; 0, βiKtr,tr) = −K"
A,0.8745318352059925,2 log(2π) −1
A,0.8764044943820225,"2 log det(Ktr,tr) −K"
A,0.8782771535580525,2 log(βi) −1
A,0.8801498127340824,"2βi
Y ⊤
tr K
−1
tr,trYtr,"
A,0.8820224719101124,and the three terms K
A,0.8838951310861424,"2 log(2π), 1"
A,0.8857677902621723,"2 log det(Ktr,tr), and Y ⊤
tr K
−1
tr,trYtr here are independent with βi. Thus,
using this observation, we compute these three terms beforehand only once, and calculate the log
likelihood of each sample βi in O(1) time. Next, for sampling the yi’s, we ﬁrst draw N samples
¯yis from N(0, Kx,x −Kx,trK
−1
tr,trKtr,x), then multiply each of these samples with √βi, and ﬁnally"
A,0.8876404494382022,"add Kx,trK
−1
tr,trYtr to each of the results. Since Kx,trK
−1
tr,trYtr + √βi¯yi ∼N(Kx,trK
−1
tr,trYtr, βi(Kx,x −"
A,0.8895131086142322,"Kx,trK
−1
tr,trKtr,x)) for all i ∈[N], we can use these ﬁnal outcomes of these computations as the yi’s."
A,0.8913857677902621,Published as a conference paper at ICLR 2022
A,0.8932584269662921,Table 4: Experimental results of Classiﬁcation with Gaussian Likelihood
A,0.8951310861423221,"Dataset
Accuracy
NNGP
Inverse Gamma
Burr Type XII"
A,0.897003745318352,"MNIST
96.8 ± 0.2
9.29 ± 0.002
4.89 ± 0.001
4.96 ± 0.001
+ Shot Noise
94.9 ± 0.4
9.32 ± 0.001
4.88 ± 0.001
4.96 ± 0.001
+ Impulse Noise
84.4 ± 2.3
9.63 ± 0.000
5.17 ± 0.001
5.25 ± 0.001
+ Spatter
95.4 ± 0.4
9.27 ± 0.001
4.44 ± 0.002
4.49 ± 0.007
+ Glass Blur
90.5 ± 0.7
9.12 ± 0.001
4.20 ± 0.005
4.00 ± 0.017
+ Zigzag
84.9 ± 1.5
9.51 ± 0.001
4.62 ± 0.006
4.49 ± 0.021"
A,0.898876404494382,"EMNIST
70.4 ± 0.9
9.40 ± 0.001
4.15 ± 0.004
4.45 ± 0.013
Fashion MNIST
61.3 ± 5.1
9.34 ± 0.002
4.96 ± 0.002
4.97 ± 0.002
KMNIST
81.1 ± 1.3
9.48 ± 0.001
4.60 ± 0.002
4.34 ± 0.013"
A,0.900749063670412,"SVHN
42.5 ± 1.5
6.01 ± 0.062
4.16 ± 0.013
4.15 ± 0.009"
A,0.9026217228464419,"E.2
TIME COMPLEXITY ANALYSIS"
A,0.9044943820224719,"For time complexity, as we mentioned in Appendix E, our posterior-predictive algorithm based on
importance sampling does not induce signiﬁcant overhead thanks to the reuse of the shared terms
for calculation. More speciﬁcally, our algorithm with K sample variances spends O(K + N 3)
time, instead of O(KN 3), for computing a posterior predictive, where N is the number of training
points. Compare this with the usual time complexity O(N 3) of the standard algorithm for Gaussian
processes. When it comes to SVGP and SVTP, one update step of both SVGP and SVTP takes
O(BM 2 + M 3) time, where B is the number of the batch size of the input dataset and M is the
number of the inducing points."
A,0.9063670411985019,"F
CLASSIFICATION WITH GAUSSIAN LIKELIHOOD"
A,0.9082397003745318,"We compare the NNGPs and the scale mixture of NNGPs with Inverse Gamma prior and Burr Type
XII prior for the classiﬁcation tasks. Following the convention (Lee et al., 2018; Novak et al., 2018;
Garriga-Alonso et al., 2019), we treat the classiﬁcation problem as a regression problem where
the regression targets are one-hot vectors of class labels and computed the posteriors induced from
squared-loss. We searched hyperparameters refer to Adlam et al. (2020) and we choose both c and
k from [0.5, 1., 2., 3., 4.]. We do not particularly expect the scale-mixtures of NNGPs to outperform
NNGP in terms of predictive accuracy, but we expect them to excel in terms of calibration due to
its ﬂexibility in describing the variances of class probabilities. To better highlight this aspect, for
MNIST data, we trained the models on clean training data but tested on corrupted data to see how
the models would react under such distributional shifts. The results are summarized in Table 4. Due
to the limitation of the resources for computing full data kernel matrix, we use 5000 samples as train
set, and 1000 samples as test set. For all datasets, the scale-mixture of NNGPs outperform NNGP in
terms of NLL."
A,0.9101123595505618,"G
EXPERIMENTAL DETAILS"
A,0.9119850187265918,"In Fig. 1, we used the inverse gamma prior with hyperparameter setting α = 2 and β = 2 for
the experiments validating convergence of initial distribution (Theorem 3.1) and last layer training
(Theorem 3.2). For the full layer training (Theorem 3.3), we used α = 1 and β = 1."
A,0.9138576779026217,"For the regression experiments, we divide each dataset into train/validation/test sets with the ratio
0.8/0.1/0.1. We performed gradient descent in order to update parameters of our models except
number of layers which is discrete value. We referred Adlam et al. (2020) for initializing parameters
of NNGPs. We choose the best hyperparameters based on validation NLL values and measured NLL
values on the test set with permuted train sets."
A,0.9157303370786517,"For the classiﬁcation experiments, we divide each dataset into train/test sets as provided by Ten-
sorFlow Datasets2, and further divide train sets into train/validation sets with the ratio 0.9/0.1. We
choose the best hyperparameters based on the validation NLL values, and measure NLL and accu-
racy values on the test set with different initialization seeds. To measure the uncertainty calibration"
A,0.9176029962546817,2https://www.tensorflow.org/datasets
A,0.9194756554307116,Published as a conference paper at ICLR 2022
A,0.9213483146067416,"Table 5: RMSE values on UCI dataset. (m, d) denotes number of data points and features, respec-
tively. We take results from Adlam et al. (2020) except our model."
A,0.9232209737827716,"Dataset
(m, d)
PBP-MV
Dropout
Ensembles
RBF
NNGP
Ours"
A,0.9250936329588015,"Boston Housing
(506, 13)
3.11 ± 0.15
2.90 ± 0.18
3.28 ± 1.00
3.24 ± 0.21
3.07 ± 0.24
3.30 ± 0.03
Concrete Strength
(1030, 8)
5.08 ± 0.14
4.82 ± 0.16
6.03 ± 0.58
5.63 ± 0.24
5.25 ± 0.20
5.08 ± 0.14
Energy Efﬁciency
(768, 8)
0.45 ± 0.01
0.54 ± 0.06
2.09 ± 0.29
0.50 ± 0.01
0.57 ± 0.02
0.44 ± 0.03
Kin8nm
(8192, 8)
0.07 ± 0.00
0.08 ± 0.00
0.09 ± 0.00
0.07 ± 0.00
0.07 ± 0.00
0.07 ± 0.00
Naval Propulsion
(11934, 16)
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
0.00 ± 0.00
Power Plant
(9568, 4)
3.91 ± 0.04
4.01 ± 0.04
4.11 ± 0.17
3.82 ± 0.04
3.61 ± 0.04
3.53 ± 0.04
Wine Quality Red
(1588, 11)
0.64 ± 0.01
0.62 ± 0.01
0.64 ± 0.04
0.64 ± 0.01
0.57 ± 0.01
0.59 ± 0.01
Yacht Hydrodynamics
(308, 6)
0.81 ± 0.06
0.67 ± 0.05
1.58 ± 0.48
0.60 ± 0.07
0.41 ± 0.04
0.35 ± 0.04"
A,0.9269662921348315,Table 6: Additional regression results for Burr Type XII prior distribution.
A,0.9288389513108615,"Dataset
(m, d)
NNGP
Inverse Gamma prior
Burr Type XII prior"
A,0.9307116104868914,"Boston Housing
(506, 13)
2.65 ± 0.13
2.72 ± 0.05
2.77 ± 0.02
Concrete Strength
(1030, 8)
3.19 ± 0.05
3.13 ± 0.04
3.29 ± 0.09
Energy Efﬁciency
(768, 8)
1.01 ± 0.04
0.67 ± 0.04
0.70 ± 0.04
Kin8nm
(8192, 8)
−1.15 ± 0.01
−1.18 ± 0.01
-1.23 ± 0.01
Naval Propulsion
(11934, 16)
-10.01 ± 0.01
−8.04 ± 0.04
−4.38 ± 0.05
Power Plant
(9568, 4)
2.77 ± 0.02
2.66 ± 0.01
2.78 ± 0.01
Wine Quality Red
(1588, 11)
-0.98 ± 0.06
−0.77 ± 0.07
−0.16 ± 0.05
Yacht Hydrodynamics
(308, 6)
1.07 ± 0.27
0.17 ± 0.25
0.60 ± 0.15"
A,0.9325842696629213,"of the model, we used the corrupted variants of the dataset and made three new dataset from the base
datasets. For the corrupted variants, we trained the model on the original version of the datasets, and
tested on the provided corrupted verions. For the out-of-distribution variants, we removed 3 classses
on the train set. For imbalance variants, we limited the samples per class with exponential ratio on
the train set. For noisy label variants, we selected 50% of labels and assigned uniformly selected
new labels from the train set. Except the corrupted variants, we tested the model on the original test
set."
A,0.9344569288389513,"For the classﬁcation by categorical likelihood experiments, we use the CNN model with two layers
to compute the kernel. Due to the limitations of computing resources, we can only use 400 inducing
points for the variational inference which leads slight degradation on the performance. For the
classiﬁcation by ﬁnite model ensemble experiments, each CNN layer has 128 channels."
A,0.9363295880149812,"H
ADDITIONAL EXPERIMENTS"
A,0.9382022471910112,"The experiment code is available at GitHub3. We used a server with Intel Xeon Silver 4214R CPU
and 128GB of RAM to evaluate the classiﬁcation with Gaussian likelihood experiment, and used
NVIDIA GeForce RTX 2080Ti GPU to conduct other experiments."
A,0.9400749063670412,"Impact of the prior hyperparameters
With same experimental setting with Section 4.1, in order
to inspect the impact of the prior hyperparameters, we sampled 1000 models for initial, last layer
training and full layer training. In Fig. 2, here we can empirically see that consistently with the
theory, if α is smaller, the output distribution is more heavy-tailed."
A,0.9419475655430711,"Full-layer training correspondence for ResNet
Even though we only proved the general training
result (Theorem 3.3) only for the fully-connected neural networks, we experimentally found that
ResNet also shows a behavior predicted by our theorem. The empirical validation with for ResNet
is shown in Fig. 3. We set α = 4 and β = 4 for the inverse gamma prior in this experiment."
A,0.9438202247191011,"Additional results of regression with Gaussian likelihood
In addition to Table 1, we also mea-
sured the root mean square error values on the test set. Table 5 summarizes the results. The results
other than ours are borrowed from Adlam et al. (2020) and we use same settings as described in
Section 4.2."
A,0.9456928838951311,3https://github.com/Anonymous0109/Scale-Mixtures-of-NNGP
A,0.947565543071161,Published as a conference paper at ICLR 2022
A,0.949438202247191,"10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.5
10.0
0.0 0.1 0.2 0.3 0.4"
A,0.951310861423221,Probability
A,0.9531835205992509,Initial
A,0.9550561797752809,"= 1/2, 
= 1/2"
A,0.9569288389513109,"= 2, 
= 2"
A,0.9588014981273408,"10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.5
10.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6"
A,0.9606741573033708,Probability
A,0.9625468164794008,Last layer training
A,0.9644194756554307,"= 1, 
= 1"
A,0.9662921348314607,"= 4, 
= 4"
A,0.9681647940074907,"10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.5
10.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6"
A,0.9700374531835206,Probability
A,0.9719101123595506,Full layer training
A,0.9737827715355806,"= 1/2, 
= 1/2"
A,0.9756554307116105,"= 4, 
= 4"
A,0.9775280898876404,"Figure 2: Impact of the prior hyperparameters for fully connected neural network initial, last layer
training and full layer training."
A,0.9794007490636704,"10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.5
10.0
Output range 0.0 0.1 0.2 0.3 0.4"
A,0.9812734082397003,Probability
A,0.9831460674157303,Full layer training
A,0.9850187265917603,"Predicted
Sampled"
A,0.9868913857677902,"10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.5
10.0
Output range 0.00 0.01 0.02 0.03 0.04 0.05"
A,0.9887640449438202,Full layer training
A,0.9906367041198502,"=4,  =4
=1/2, =1/2"
A,0.9925093632958801,"Figure 3: (left) Correspondence between wide ﬁnite ResNet model vs theoretical limit. (right)
impact of the prior hyperparameters."
A,0.9943820224719101,"Additional regression experiment
As an additional regression experiments, we test the models
which use Burr Type XII distribution as its last layer variance’s prior. We use Appendix E in order
to calculate the predictive posterior distribution. Our results are in Table 6."
A,0.9962546816479401,"I
ADDITIONAL INFORMATION"
A,0.99812734082397,"We refer to the source of each dataset through footnotes with URL links. We don’t use any data
which obtained from people and don’t use any data which contains personally identiﬁable informa-
tion or offensive content."
