Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0055248618784530384,"The Maximum Entropy (Max-Ent) framework has been effectively employed in a
variety of Reinforcement Learning (RL) tasks. In this paper, we first propose a novel
Max-Ent framework for policy evaluation in a distributional RL setting, named
Distributional Maximum Entropy Policy Evaluation (D-Max-Ent PE). We derive a
generalization-error bound that depends on the complexity of the representation
employed, showing that this framework can explicitly take into account the features
used to represent the state space while evaluating a policy. Then, we exploit these
favorable properties to drive the representation learning of the state space in a
Structural Risk Minimization fashion. We employ state-aggregation functions as
feature functions and we specialize the D-Max-Ent approach into an algorithm,
named D-Max-Ent Progressive Factorization, which constructs a progressively
finer-grained representation of the state space by balancing the trade-off between
preserving information (bias) and reducing the effective number of states, i.e., the
complexity of the representation space (variance). Finally, we report the results
of some illustrative numerical simulations, showing that the proposed algorithm
matches the expected theoretical behavior and highlighting the relationship between
aggregations and sample regimes."
INTRODUCTION,0.011049723756906077,"1
Introduction"
INTRODUCTION,0.016574585635359115,"In Distributional Reinforcement Learning (D-RL) [Bellemare et al., 2023], an agent aims to estimate
the entire distribution of the returns achievable by acting according to a specific policy. This is
in contrast to and more complex than classic Reinforcement Learning (RL) [Szepesvári, 2010,
Sutton and Barto, 2018], where the objective is to predict the expected return only. In recent years,
several algorithms for D-RL have been proposed, both in evaluation and control settings. The push
towards distributional approaches was particularly driven by additional flavors they can bring into
the discourse, such as risk-averse considerations, robust control, and many regularization techniques
[Chow et al., 2015, Brown et al., 2020, Keramati et al., 2020]. Most of them varied in how the
distribution of the returns is modeled. The choice of the model was shown to have a cascading effect
on how such a distribution can be learned, how efficiently and with what guarantees, and how it can
be used for the control problem. Similarly to this tradition, this paper investigates the potential of
looking into the entire distribution of returns to address the representation learning of the state-action
spaces, that is to find a good feature representation of the decision-making space so as to make the"
INTRODUCTION,0.022099447513812154,"overall learning problem easier, tenderly by reducing the dimensionality of such spaces. In particular,
it points to answer the following research question:"
INTRODUCTION,0.027624309392265192,Q1: What tools can return distributions provide for distributional RL?
INTRODUCTION,0.03314917127071823,"In Section 3, we answer this methodological question, showing that it is possible to reformulate
Policy Evaluation in a distributional setting so that its performance index is explicitly intertwined
with the representation of the (state or action) spaces. More specifically, this work tackles Policy
Evaluation in a distributional setting as a particular case of a distribution estimation problem and
then applies the Maximum Entropy (Max-Ent) formulation of distribution estimation [Wainwright
and Jordan, 2007]. In this way, it is possible to derive a novel framework for PE, which we name
Distributional Max-Ent Policy Evaluation (D-Max-Ent PE), which inherits the many positives the
Max-Ent framework offers. In particular, it allows the inclusion of constraints that the distribution
needs to satisfy, usually called structural constraints, namely feature-based constraints acting over
the support of the distribution. Such constraints then appear in the generalization-error bound for the
Max-Ent problem, with a term related to the complexity of the family of features used. Unfortunately,
traditional derivations of this bound introduce quantities that are bounded but unknown. We develop
the analysis further to derive a more practical bound, containing quantities that are either estimated or
known. In this way, the generalization-error bound shows a usual bias-variance trade-off that can
be explicitly optimized by changing the feature functions adopted, while making the best use of the
available samples. Thus, PE in a distributional setting is directly linked to representation learning."
INTRODUCTION,0.03867403314917127,"Now, the RL literature proved that reducing the state space size while preserving the important features
of the original state space is beneficial, namely with state-aggregation feature functions [Singh et al.,
1994, Van Roy, 2006, Dong et al., 2020]. This is particularly true when high dimensionality can
make learning slower and more unstable, as in classic RL in general, or when the learning process
is almost unfeasible in small-samples regimes, as for D-RL, where learning the entire distribution
of returns requires a large number of samples. Thus, motivated by these considerations, while
D-Max-Ent Policy Evaluation allows for the use of any type of structural constraint, this work focuses
on state-aggregation feature functions, and we exploit the first and more methodological result to
answer a second more algorithmic question:"
INTRODUCTION,0.04419889502762431,"Q2: How are representation learning and policy evaluation intertwined? Do
distributional methods offer a new way to highlight and exploit this connection?"
INTRODUCTION,0.049723756906077346,"To answer this question, in Section 4, we show that the generalization-error bound changes monotoni-
cally when the state-aggregation constraints are changed in a specific way, namely a finer-grained
representation of the space. This is indeed what happens in the Structural Risk Minimization theory
(SRM) [Vapnik, 1991]. Similarly, we develop a novel algorithm called D-Max-Ent Progressive
Factorization, which exploits the proposed evaluation method to learn a representation of the state
space soundly, i.e., trying to reduce a proxy of the generalization error bound in an SRM fashion,
and allowing us to answer positively to the second research question as well. Finally, in Section 5
we verify through an illustrative numerical simulation whether the proposed algorithm matches the
behaviors suggested by the theoretical analysis."
PRELIMINARIES,0.055248618784530384,"2
Preliminaries"
MARKOV DECISION PROCESSES,0.06077348066298342,"2.1
Markov Decision Processes"
MARKOV DECISION PROCESSES,0.06629834254143646,"A discrete–time finite Markov decision processes (MDP) [Puterman, 1994] is a tuple M :=
(S, A, PS, PR, µ, γ), where S is a finite state space (|S|= S), A is a finite action space (|A|= A),
PS : S ×A →∆(S) is the transition kernel, PR : S ×A →∆(R) is the reward distribution function,
µ ∈∆(S) is the initial-state distribution and γ ∈[0, 1) is discount factor.1 A policy π : S →∆(A)
defines the behavior of an agent interacting with an environment, which goes as follows: starting
from an initial state S0 ∼µ, an agent interacts with the environment through the policy π, generating
a trajectory H = (St, At, Rt)∞
t=0, which is a sequence of states, actions and rewards whose joint
distribution is determined by the transition kernel, reward distribution, and the policy itself, i.e.,
At ∼π(·|St), Rt ∼PR(·|St, At), and St+1 ∼PS(·|St, At)."
MARKOV DECISION PROCESSES,0.0718232044198895,1∆(X) denotes the simplex of a space X.
VALUE FUNCTIONS AND DISTRIBUTIONS OF RETURNS,0.07734806629834254,"2.2
Value Functions and Distributions of Returns"
VALUE FUNCTIONS AND DISTRIBUTIONS OF RETURNS,0.08287292817679558,"Given an MDP M with discount factor γ, the Discounted Return is the sum of rewards received from
the initial state onwards, discounted according to their time of occurrence:"
VALUE FUNCTIONS AND DISTRIBUTIONS OF RETURNS,0.08839779005524862,"Gπ(s) = ∞
X"
VALUE FUNCTIONS AND DISTRIBUTIONS OF RETURNS,0.09392265193370165,"t=0
γtRt|S0 = s.
(1)"
VALUE FUNCTIONS AND DISTRIBUTIONS OF RETURNS,0.09944751381215469,The Value Function of a given policy π is the expectation of this quantity under the policy itself:
VALUE FUNCTIONS AND DISTRIBUTIONS OF RETURNS,0.10497237569060773,"V π(s) = E[G(s)] = E "" ∞
X"
VALUE FUNCTIONS AND DISTRIBUTIONS OF RETURNS,0.11049723756906077,"t=0
γtRt|S0 = s # .
(2)"
VALUE FUNCTIONS AND DISTRIBUTIONS OF RETURNS,0.11602209944751381,"The Return Distribution Function ηπ of a given policy π is a collection of distributions, one for each
state s ∈S, where each element is the distribution of the random variable Gπ(s):"
VALUE FUNCTIONS AND DISTRIBUTIONS OF RETURNS,0.12154696132596685,"ηπ(s) = Dπ
(s) "" ∞
X"
VALUE FUNCTIONS AND DISTRIBUTIONS OF RETURNS,0.1270718232044199,"t=0
γtRt|S0 = s # ,
(3)"
VALUE FUNCTIONS AND DISTRIBUTIONS OF RETURNS,0.13259668508287292,"where Dπ
(s) extracts the probability distribution of a random variable under the joint distribution of
the trajectory."
VALUE FUNCTIONS AND DISTRIBUTIONS OF RETURNS,0.13812154696132597,"The Distributional Policy Evaluation Problem then consists of estimating the return distribution
function of Eq. (3) for a fixed policy π."
MAXIMUM ENTROPY ESTIMATION,0.143646408839779,"2.3
Maximum Entropy Estimation"
MAXIMUM ENTROPY ESTIMATION,0.14917127071823205,"Maximum Entropy (Max-Ent) methods [Dudík and Schapire, 2006, Wainwright and Jordan, 2007,
Sutter et al., 2017] are density estimation methods that select the distribution that maximizes the
uncertainty, i.e., the one with maximum entropy, where the entropy of a distribution p ∈∆(X) is
defined as H(p) := −EX∼p[log p(X)].2 Additionally, they assume that the learner has access to
a feature mapping F from X to RM. In the most general case, we may have M = +∞. We will
denote by Φ the class of real-valued functions containing the component feature functions fj ∈F
with j ∈[M]. A distribution p is consistent with the true underlying distribution p0 if"
MAXIMUM ENTROPY ESTIMATION,0.15469613259668508,"EX∼p[fj(X)] = µj,
∀j ∈[M],
(4)"
MAXIMUM ENTROPY ESTIMATION,0.16022099447513813,"where
µj := EX∼p0[fj(X)]
(5)
In this case, we say that p satisfies (in expectation) the structural constraints imposed by the features
in F. In practice, p0 is not available and Max-Ent methods enforce empirical consistency over N
independent and i.i.d. observations D = {x1, . . . , xN} ∼p0 with support in X by replacing the
definition in Eq. (5) with"
MAXIMUM ENTROPY ESTIMATION,0.16574585635359115,"ˆµj(D) := 1 N N
X"
MAXIMUM ENTROPY ESTIMATION,0.1712707182320442,"i=1
fj(xi),
∀j ∈[M].
(6)"
MAXIMUM ENTROPY ESTIMATION,0.17679558011049723,"The distribution p is said to be consistent with the data D if it matches the empirical expectations.
The empirical Max-Ent problem consists then of the following optimization problem"
MAXIMUM ENTROPY ESTIMATION,0.18232044198895028,"max
p∈∆(X)
H(p)"
MAXIMUM ENTROPY ESTIMATION,0.1878453038674033,"s.t.
EX∼p[fj(X)] = ˆµj,
∀j ∈[M],
(7)"
MAXIMUM ENTROPY ESTIMATION,0.19337016574585636,"with the optimization problem in expectation differing just in the constraints (i.e., replacing constraint
from Eq. (6) with the ones from Eq. (5)). It is well known that the optimal solution to the empirical"
MAXIMUM ENTROPY ESTIMATION,0.19889502762430938,"2With little abuse of notation, we will use the same symbol for the probability distribution and its p.d.f.,
which we assume to exist w.r.t. a reference measure."
MAXIMUM ENTROPY ESTIMATION,0.20441988950276244,"Max-Ent problem in Eq. (7) is a distribution pλ ∈∆(X) belonging to the class of exponential
distributions parametrized by the parameters λ, namely:"
MAXIMUM ENTROPY ESTIMATION,0.20994475138121546,pλ(x) = Φλ exp  X
MAXIMUM ENTROPY ESTIMATION,0.2154696132596685,"j∈[M]
λjfj(x) "
MAXIMUM ENTROPY ESTIMATION,0.22099447513812154,",
(8)"
MAXIMUM ENTROPY ESTIMATION,0.2265193370165746,"where Φλ :=
R"
MAXIMUM ENTROPY ESTIMATION,0.23204419889502761,"X exp
P"
MAXIMUM ENTROPY ESTIMATION,0.23756906077348067,"j∈[M] λjfj(x′)

dx′ is a normalization constant, which ensures that"
MAXIMUM ENTROPY ESTIMATION,0.2430939226519337,"p ∈∆(X), and its log-transformation takes the name of log-partition function A(λ) :=
log
R"
MAXIMUM ENTROPY ESTIMATION,0.24861878453038674,X exp(P
MAXIMUM ENTROPY ESTIMATION,0.2541436464088398,j∈[M] λjfj(x))dx. The log-partition function defines the set of well-behaved dis-
MAXIMUM ENTROPY ESTIMATION,0.2596685082872928,"tributions Ω= {λ ∈RM : A(λ) < +∞}. At optimality, the parameters are defined as ˆλ and
correspond to the optimal Lagrangian multipliers of the dual of the empirical Max-Ent problem in
Eq. (7). Now on, we will use ˆp to identify pˆλ for simplicity."
MAXIMUM ENTROPY ESTIMATION,0.26519337016574585,"3
Distributional Policy Evaluation: A Max-Ent Approach"
MAXIMUM ENTROPY ESTIMATION,0.27071823204419887,Algorithm 1 Distributional Max-Ent Policy Evaluation
MAXIMUM ENTROPY ESTIMATION,0.27624309392265195,"Require: (HN, F)
▷N trajectory samples, set of
features functions"
MAXIMUM ENTROPY ESTIMATION,0.281767955801105,"ˆη = argmax
η
H(η)"
MAXIMUM ENTROPY ESTIMATION,0.287292817679558,"s.t. EX∼η[fj(X)] = ˆµj(HN)
∀j ∈[M]
η ∈∆(X)
return ˆη"
MAXIMUM ENTROPY ESTIMATION,0.292817679558011,"This section aims at answering the first
research question Q1. The proposed ap-
proach turns distributional PE into a pure
density estimation problem in a Max-Ent
framework, called Distributional Max-Ent
Policy Evaluation, as described in Algo-
rithm 1. For this translation, the algorithm
uses the distribution of returns η as p, N-
trajectory samples HN = {H}N
n=0 as data,
and a fixed set of features functions F be-
longing to a function class Φ. Note that to
do this, we need to slightly change the notation concerning the D-RL framework: η will not be a
|S|-vector of distributions with support over R, but rather a joint distribution over the whole support
X = S × R. Turning PE into a Max-Ent problem has many upsides. First of all, the Max-Ent
principle allows to deal with any kind of support X, unifying continuous and discrete cases under
the same framework; secondly, it does not require specifying a family of probability distributions to
choose from; moreover, it implicitly manages the uncertainty by seeking a distribution as agnostic
as possible, i.e., as close to the uniform distribution as possible. Finally, Max-Ent allows to include
of structural constraints over the return distribution under many different flavors, both as in the
standard value-function approximation methods [Van Roy, 2006] and as in more recent works based
on statistical functionals acting over the return portion R of the support [Bellemare et al., 2023]. One
of the possible limitations might be the requirement to have access to a batch of i.i.d. samples, but
this is not necessarily restrictive: the result can be generalized for a single β-mixing sample path by
exploiting blocking techniques [Yu, 1994, Nachum et al., 2019]."
GENERALIZATION ERROR BOUND,0.2983425414364641,"3.1
Generalization Error Bound"
GENERALIZATION ERROR BOUND,0.30386740331491713,"As previously said, the inner properties of Max-Ent allow for translating the results from density
estimation methods to the distributional PE setting, and in particular, generalization-error bounds
defined as KL-divergences.3 Unfortunately, the generalization error bounds of traditional Max-Ent
theory contain a conservative term that compares the solutions of the expectation and empirical
Max-Ent problems, ¯η, ˆη respectively, by taking the maximum between the 1-norm of the respective
multipliers, namely maxλ∈{¯λ,ˆλ} ||λ||1. This quantity is bounded yet unknown, making the result
unpractical. In the following, we extend the previous results with a more practical bound containing
||ˆλ||1 instead of the maximum, requiring some additional assumptions about the expressiveness of
the feature functions. This result is of independent interest and allows us to directly use the bound
from an algorithmic perspective."
GENERALIZATION ERROR BOUND,0.30939226519337015,"Theorem 1 (Generalization Error Bound of D-Max-Ent PE). Assume that the set of features F
belong to the function class Φ, which it is such that supx∈X,f∈F ||f(x)||∞= F < +∞and that the"
GENERALIZATION ERROR BOUND,0.3149171270718232,"3The KL-divergence between two distributions p, q is defined as KL(p||q) = Ex∼p[log(p(x)/q(x))]"
GENERALIZATION ERROR BOUND,0.32044198895027626,"minimum singular value σmin of the empirical covariance matrix of the features
ˆ
Cov(F) is strictly
positive, namely σmin( ˆ
Cov(F)) > 0. Then, given a sample batch {x1, . . . , xN} ∈X N of N i.i.d.
points drawn from the true distribution ηπ, for any δ ∈(0, 1), it holds with probability at least 1 −δ
that the solution to the sampled Max-Ent problem ˆη satisfies the following:"
GENERALIZATION ERROR BOUND,0.3259668508287293,"KL(ηπ||ˆη) ≾−H(ηπ) + ˜L(ˆη) + B(ˆλ, F, N, δ)
(9)"
GENERALIZATION ERROR BOUND,0.3314917127071823,"˜L(ˆη) = −1 N N
X"
GENERALIZATION ERROR BOUND,0.3370165745856354,"i=0
log ˆη(xi)
(10)"
GENERALIZATION ERROR BOUND,0.3425414364640884,"B(ˆλ, F, N, δ) = 10||ˆλ||1 "
GENERALIZATION ERROR BOUND,0.34806629834254144,RN(Φ) + F r
GENERALIZATION ERROR BOUND,0.35359116022099446,log 1/δ
N,0.35911602209944754,2N !
N,0.36464088397790057,",
(11)"
N,0.3701657458563536,"where ≾stands for the fact that the bound comprises additional terms that decrease at a higher rate in
sample complexity and were therefore neglected. H(ηπ) and ˜L(ˆη), the empirical log-likelihood of
the solution, form a bias term. The remaining term B(ˆλ, F, N, δ) is a variance term depending on
the multipliers characterizing the solution ˆλ, the number of samples, the confidence level δ, and the
feature class complexity as the empirical Rademacher complexity of the class RN(Φ) [Mohri et al.,
2018]."
PROOF SKETCH,0.3756906077348066,"3.2
Proof Sketch"
PROOF SKETCH,0.3812154696132597,"Here we report the main steps of the proof of Th. 1. The interested reader can find the complete proof
in Appendix A. First, define the set containing the solutions to the expected and sampled Max-Ent
problems with S := {¯η, ˆη}, the related set for the multipliers ΩS := {¯λ, ˆλ}, and a quantity that will
be central now on h(x1, · · · , xN) := maxη∈S |Eηπ[log η] −1"
PROOF SKETCH,0.3867403314917127,"N
PN
i log η(xi)|. Then, the building
blocks of the error term KL(ηπ||ˆη), namely KL(¯η||ˆη) and KL(ηπ||¯η) are bounded by:"
PROOF SKETCH,0.39226519337016574,KL(¯η||ˆη) ≤2h(·)
PROOF SKETCH,0.39779005524861877,KL(ηπ||¯η) ≤−H(ηπ) + ˜L(ˆη) + 3h(·).
PROOF SKETCH,0.40331491712707185,It is possible to show that:
PROOF SKETCH,0.4088397790055249,"h(·) ≤2 sup
λ∈ΩS
||λ||1 "
PROOF SKETCH,0.4143646408839779,RN(Φ) + F r
PROOF SKETCH,0.4198895027624309,log 1/δ
N,0.425414364640884,2N !
N,0.430939226519337,"sup
λ∈ΩS
||λ||1 ≤||ˆλ||1 + s"
M,0.43646408839779005,6M
M,0.4419889502762431,"σmin( ˆ
Cov(F))
h(·)."
M,0.44751381215469616,"The first inequality is obtained with standard methods as in van der Vaart and Wellner [1996], Dudley
[1999], Koltchinskii and Panchenko [2002], Wang et al. [2013]. The second one is obtained by
exploiting the intrinsic properties of the Max-Ent solution and by noting that it is possible to link
h(·) with the Bregman divergence of the log-partition function DA(¯λ, ˆλ). One can see that the use of
the second inequality introduces an additional assumption about the expressiveness of the feature
functions, requiring the minimum singular value of the sampled covariance matrix σmin( ˆ
Cov(F)) to
be strictly positive. As a final step, setting x =
p"
M,0.4530386740331492,"h(x1, · · · , xN) and combining the two previous
inequalities yields a quadratic inequality:







"
M,0.4585635359116022,"





"
M,0.46408839779005523,x2 −bx −c ≤0
M,0.4696132596685083,"b = 2
q"
M,0.47513812154696133,"6M
σmin( ˆ
Cov(F)) """
M,0.48066298342541436,"RN(Φ) + F
q"
M,0.4861878453038674,log 1/δ
N,0.49171270718232046,2N #
N,0.4972375690607735,"c = 2||ˆλ||1 """
N,0.5027624309392266,"RN(Φ) + F
q"
N,0.5082872928176796,log 1/δ
N,0.5138121546961326,"2N #
,"
N,0.5193370165745856,which is well-defined and solves for
N,0.5248618784530387,"h(x1, · · · , xN) ≾||ˆλ||1 "
N,0.5303867403314917,RN(Φ) + F r
N,0.5359116022099447,log 1/δ
N,0.5414364640883977,"2N ! ,"
N,0.5469613259668509,"by neglecting higher-order terms. The statement of the theorem is then just a matter of combining all
these results."
DISTRIBUTIONAL REPRESENTATION LEARNING WITH STATE AGGREGATION,0.5524861878453039,"4
Distributional Representation Learning with State Aggregation"
DISTRIBUTIONAL REPRESENTATION LEARNING WITH STATE AGGREGATION,0.5580110497237569,"This section addresses the second research question Q2, namely how to use the bound in Th. 1 from
an algorithmic perspective to automatically refine the features used to represent the state space in
a principled way while performing D-Max-Ent PE. In particular, the focus is on a specific instance
of feature functions for return distributions, namely state aggregation. More specifically, the state
aggregation feature functions F = {fj}j∈[M] split the state space into M disjoint subsets, one for
each function, i.e., S = ∪j∈[M]Sj and Sj ∩Sj′ = ∅, j, j′ ∈[M], j ̸= j′, and gives back the
associated return g ∈R, namely:"
DISTRIBUTIONAL REPRESENTATION LEARNING WITH STATE AGGREGATION,0.56353591160221,"fj : S × R →R
fj(s, g) = g1[s∈Sj].
(12)"
DISTRIBUTIONAL REPRESENTATION LEARNING WITH STATE AGGREGATION,0.569060773480663,"These features are bounded by the maximum return Gmax, while the empirical Rademacher complexity
over N samples of returns {(si, gi)}i∈[N] can be directly computed as in Clayton [2014]:"
DISTRIBUTIONAL REPRESENTATION LEARNING WITH STATE AGGREGATION,0.574585635359116,"RN(Φ) = Gmax
X j∈[M] q"
DISTRIBUTIONAL REPRESENTATION LEARNING WITH STATE AGGREGATION,0.580110497237569,"ˆP(Sj),
(13)"
DISTRIBUTIONAL REPRESENTATION LEARNING WITH STATE AGGREGATION,0.585635359116022,"where ˆP(Sj) = Nj/N and Nj = |{(gi, si) : si ∈Sj, i ∈[N]}|.
The decomposition of the
Rademacher term into single terms leads to rewriting B(ˆλ, F, N, δ) as in the following lemma.
Lemma 1. For Distributional Max-Ent Evaluation with a state-aggregation feature class, the variance
term B(ˆλ, F, N, δ) is given by;"
DISTRIBUTIONAL REPRESENTATION LEARNING WITH STATE AGGREGATION,0.5911602209944752,"B(ˆλ, F, N, δ) = 10||ˆλ||1Gmax  X j∈[M] q"
DISTRIBUTIONAL REPRESENTATION LEARNING WITH STATE AGGREGATION,0.5966850828729282,ˆP(Sj) + r
DISTRIBUTIONAL REPRESENTATION LEARNING WITH STATE AGGREGATION,0.6022099447513812,log 1/δ
N,0.6077348066298343,2N 
N,0.6132596685082873,".
(14)"
N,0.6187845303867403,"4.1
Representation Refinement: Progressive Factorization"
N,0.6243093922651933,"State aggregation features are of interest due to the possibility of progressively refining the repre-
sentation by increasing the factorization level, that is, by splitting a subset Sj into further disjoint
subsets. This refinement is called progressive factorization and is defined as follows.
Definition 1 (Progressive Factorization). For two sets of state aggregation feature func-
tions, F, Fj, we say that Fj is a progressive factorization of F, i.e., F
⊂Fj, if F
=
{f1, . . . , fj−1, fj+1, . . . , fM} ∪{fj}, Fj = {f1, . . . , fj−1, fj+1, . . . , fM} ∪{f k
j }k∈[K] and the
additional functions {f k
j }k∈[K] are such that the corresponding subsets satisfy"
N,0.6298342541436464,"Sj =
["
N,0.6353591160220995,"k∈[K]
Sk
j ,
Sk
j ∩Sk′
j = ∅, k, k′ ∈[K], k ̸= k′,"
N,0.6408839779005525,"where only non-degenerate class factorizations will be considered, meaning that the new subsets Sk
j
are non-empty."
N,0.6464088397790055,"It is relevant for our interests that, in the case of progressive factorizations F ⊂F′, the respective
Max-Ent solutions enjoy the following monotonicity property"
N,0.6519337016574586,"Lemma 2 (Monotonicity). The multipliers of the Max-Ent solutions ˆλ, ˆλ′ using F ⊂F′ are such
that"
N,0.6574585635359116,"||ˆλ||1 ≤||ˆλ′||1.
(15)"
N,0.6629834254143646,"This result is fully derived in Appendix C, and it ensures a monotonically increasing of all terms
contained in the variance term of Eq. (11) since the complexity term is monotonically increasing by
definition. On the other hand, the bias represented by Eq. (10) is guaranteed to decrease monotonically
at finer levels of factorizations."
D-MAX-ENT PROGRESSIVE FACTORIZATION ALGORITHM,0.6685082872928176,"4.2
D-Max-Ent Progressive Factorization Algorithm"
D-MAX-ENT PROGRESSIVE FACTORIZATION ALGORITHM,0.6740331491712708,"Algorithm 2 Distributional Max-Ent Progressive
Factorization
Require: (HN, F0, δ, β, K)
▷N-trajectory
samples, initial feature set, confidence level,
boosting factor, factorization factor
1: Done ←False, i∗←0
2: while not Done do
3:
F ←Fi∗, M ←|F|
4:
ˆη ←D-Max-Ent PE(HN, F)
5:
J (ˆη) ←βL(ˆη) + B(ˆλ, F, N, δ)
6:
{Fj}j∈[M] ←Progressive Factor(F, K)
7:
for j ∈[M] do
8:
ˆηj ←D-Max-Ent PE(HN, Fj)
9:
J (ˆηj) ←βL(ˆηj) + B(ˆλj, Fj, N, δ)
10:
if J (ˆηj) < J (ˆη) then
11:
i∗←j
12:
end if
13:
end for
14:
if Fi∗== F then
15:
Done ←True
16:
end if
17: end while
18: return ˆηi∗"
D-MAX-ENT PROGRESSIVE FACTORIZATION ALGORITHM,0.6795580110497238,"In summary, D-Max-Ent PE shows a general-
ization error bound whose quantities are either
known or estimated and change monotonically
between progressive factorizations. On these
results, we build an algorithm called D-Max-Ent
Progressive Factorization, shown in Algorithm
2, which iteratively constructs a sequence of fea-
ture sets F0 ⊂F1 ⊂. . . with progressive fac-
torization while performing PE. The behavior of
the algorithm is similar to what is done in Struc-
tural Risk-Minimization (SRM) [Vapnik, 1991],
and it involves optimizing for a trade-off: the
bias term (i.e., empirical risk) decreases by tak-
ing into account more complex features classes,
while the variance term (i.e., the confidence in-
terval) increases. The whole algorithm is then
based on the progressive search for the new set
of feature functions which reduces a proxy of
the generalization error bound of D-Max-Ent
PE:"
D-MAX-ENT PROGRESSIVE FACTORIZATION ALGORITHM,0.6850828729281768,"J (ˆη) = βL(ˆη) + B(ˆλ, F, N, δ),
(16)"
D-MAX-ENT PROGRESSIVE FACTORIZATION ALGORITHM,0.6906077348066298,"and the procedure will continue until there are
no further improvements in the trade-off. Due
to the nature of the proxy function, the role of
β > 0 is to regulate the tendency to factorize. Higher values of β will increase the magnitude of the
decreasing term, causing a boost in the tendency to factorize. On the other hand, lower values will
further decrease the importance of this term, resulting in a lower tendency to factorization."
D-MAX-ENT PROGRESSIVE FACTORIZATION ALGORITHM,0.6961325966850829,"Finally, the Progressive Factor function takes as input the list of feature functions and a factor K and
returns a list of progressively factored set of feature functions. More specifically, each element in
{Fj}j∈[M] corresponds to a progressive factorization of the feature fj, factoring the related subset
Sj into K disjoint subsets as in Definition 1. The new K subsets {Sj
k}k∈[K] are constructed in the
worst-case scenario: the complexity term in Eq. (13) is maximized with partitions of a set leading to
a uniform distribution of samples in each new partitioned subset, and since it is not possible to know
in advance which samples will be contained in which new subset, one way is then to proceed with a
uniform factorization. We decided to maintain the most agnostic approach over the set of possible
features, but prior knowledge could be used to narrow down the partitions to consider."
ILLUSTRATIVE NUMERICAL SIMULATIONS,0.7016574585635359,"5
Illustrative Numerical Simulations"
ILLUSTRATIVE NUMERICAL SIMULATIONS,0.7071823204419889,"This section reports the results of some illustrative numerical simulations that make use of Algorithm
2."
ILLUSTRATIVE NUMERICAL SIMULATIONS,0.712707182320442,"Simulations Objectives The objective of the simulations is to illustrate two essential features of
the proposed method that were only suggested by the theoretical results. First of all, to analyze the
outcome of performing policy evaluations with aggregated states at different sample regimes, by
comparing the output of the proposed algorithm with some relevant baseline distributions. Secondly,
the aim is to study the role of the boosting parameter β and the sampling regime N, being the main
hyper-parameters of Algorithm 2, in the tendency to factor the representation at utterly different
sample regimes."
ILLUSTRATIVE NUMERICAL SIMULATIONS,0.7182320441988951,"MDP Instance Design The effectiveness of the proposed approach is expected to be particularly
evident in MDPs admitting a factored representation of the return distribution, namely the ones in
which many states are nearly equivalent under the evaluation of a policy. This factorizability property
is not uncommon in general since it is present in any environment with symmetries and Block-MDPs"
ILLUSTRATIVE NUMERICAL SIMULATIONS,0.7237569060773481,"Figure 1: Bound Trend for different β (N = 50)
Figure 2: KL Trend for different β (N = 50)"
ILLUSTRATIVE NUMERICAL SIMULATIONS,0.7292817679558011,"Figure 3: Bound Trend for different β (N =
1000)"
ILLUSTRATIVE NUMERICAL SIMULATIONS,0.7348066298342542,Figure 4: KL Trend for different β (N = 1000)
ILLUSTRATIVE NUMERICAL SIMULATIONS,0.7403314917127072,"[Du et al., 2019] as well. The MDP instance is then designed to be a Block-MDP indeed since
it allows for better evaluate the simulation objectives: one would expect that operating on MDPs
admitting a factored representation would allow for lower values of β to be effective enough, while a
higher level of boosting would force over-factorizations that are unnecessary, leading to no further
improvement or even degradation of the results. The simulations are run on a rectangular GridWorld,
with a height of 4 and length of 8, with traps on the whole second line and goals all over the top. A
visualization of the setting can be seen in Appendix D. The policy is selected as a uniform distribution
over the set of actions A = (up, left, right)."
ILLUSTRATIVE NUMERICAL SIMULATIONS,0.7458563535911602,"Performance Indexes The proposed MDP instance presents many upsides in terms of the inter-
pretability of the output as well. First of all, it allows us to directly compute the true underlying
return distribution with Monte-Carlo estimation. Secondly, it permits to compare of the output
distribution of the algorithm with the result of performing plain Distributional Max-Ent Policy
Evaluation (Algorithm 1) with two baseline representations: an oracle factorization that aggregates
together states known to be equivalent under the policy, and in particular all the upper and lower
states respectively; a full factorization that employs |S|-singletons of states as representations, i.e.,
the most fine-grained representation possible. The comparison is made via two relevant quantities,
the KL divergence with respect to the true distribution (the bounded quantity), and the total bound
Btot = ˜L(ˆη) + B(ˆλ, F, N, δ) (the bounding quantity). Finally, the value of the partition splitting K
is set to 2, to reduce the exponential search space of all possible uniform partitions, the discount
factor γ is set to 0.98 and the confidence δ to 0.1, the results are averaged over 10 rounds with the
respective standard deviation."
ILLUSTRATIVE NUMERICAL SIMULATIONS,0.7513812154696132,"Results Discussion The results of the simulations are reported from Fig. 1 to Fig. 4, with the
quantity related to the oracle parametrization being in orange, while the ones related to the full
parametrization being in blue. It is possible to notice that these two distributions have almost the
same KL divergences with respect to the true return distribution (Fig. 2, 4), yet they highly differ"
ILLUSTRATIVE NUMERICAL SIMULATIONS,0.7569060773480663,"in the bound Btot (Fig. 1, 3) mostly due to the variance term, which is way higher in the case of
full factorization. This suggests that the bound is indeed able to distinguish between the two. The
plotting of the outputs of Algorithm 2 stops at the optimal number of factorization steps found for
different values of β, namely at Fi⋆. The plots should be read as follows: while the bound term
Btot is expected to increase at each factorization step, the KL divergences with respect to the true
return distribution should decrease as much as possible. In all cases, it is evident that the value of β
pushes towards a higher number of factorization steps, going from performing no factorization at all
using low values (β = 3), to performing up to 4 factorization steps even in this simple scenario with
higher values (β = 450), both at low and high sample regimes (N ∈{50, 1000}). Furthermore, at
higher sample regimes, it is possible to see how the higher quality of the estimation counteracts the
action of β, and increasing it generally induces still fewer factorizations compared to the low sample
regimes with same values of β, as in Fig. 3, 4. Finally, it is apparent that minimizing for Eq. (16)
successfully decreases the KL divergence. Nonetheless, its values stop decreasing significantly after
the first factorization, which splits the state space over the two rows and further factorizations might
lead to performance degradation as well."
DISCUSSION,0.7624309392265194,"6
Discussion"
DISCUSSION,0.7679558011049724,"In this section, we briefly discuss the literature related to this work and provide some concluding
remarks about the results and future research paths."
RELATED WORKS,0.7734806629834254,"6.1
Related Works"
RELATED WORKS,0.7790055248618785,"Our work relates to multiple fields. We now highlight the most relevant connections, while an
exhaustive overview is beyond the scope of this paper."
RELATED WORKS,0.7845303867403315,"Distributional Reinforcement Learning D-RL has recently received much attention, both for
the richness of flavors it admits [Rowland et al., 2019, 2021], and the surprising empirical effec-
tiveness [Bellemare et al., 2020]. Our work tries to answer different research questions compared
to traditional policy evaluation in D-RL and applies completely different techniques to derive the
quantities of interest. Firstly, Bellman Operators and consequently contraction arguments cannot
be applied since the estimation process is Monte-Carlo based. In this way, the sequential nature of
the problem is not exploited, but we show that density estimation techniques do offer interesting
properties nonetheless. Additionally, the employed indexes differ from traditional D-RL results.
For example, the most recent bound for Q-TD [Rowland et al., 2023], shows a sub-linear term but
the bound is made over the maximum Wasserstein distance, which cannot be directly related to
the KL-divergence without further assumptions. Finally, distributional considerations have been
employed in the field of function approximations as well. However, to the best of our knowledge, no
other D-RL works explicitly address representation learning."
RELATED WORKS,0.7900552486187845,"Maximum Entropy and Feature Selection Max-Ent methods have a long and extensive literature
in the density estimation field, which mostly focused on the general and algorithmic aspects of the
method [Barron and Sheu, 1991, Dudík et al., 2004, Sutter et al., 2017]. Among the others, Cortes
et al. [2015] proposed a Max-Ent regularized formulation for performing feature selection. Their
method allocates different weights to different features to achieve an even better trade-off based on
a combination. Due to this, their work differs from ours in the nature of the search space, which
is not built progressively but is defined a priori. Additionally, their generalization bound is of the
same nature as standard Max-Ent bounds and contains a supλ∈Ω||λ||1 term, which is bounded yet
unknown. Finally, Mavridis et al. [2021] perform progressive state-aggregation through Max-Ent
methods, but they try to optimize a different objective function based on state-dissimilarity."
CONCLUSIONS AND FUTURE WORKS,0.7955801104972375,"6.2
Conclusions and Future Works"
CONCLUSIONS AND FUTURE WORKS,0.8011049723756906,"In our work, we presented in a D-RL framework a new policy evaluation approach based on Maximum
Entropy density estimation, called Distributional Max-Ent Policy Evaluation, which benefits from
the learning guarantees of Max-Ent and the generality of the setting, being able to enforce even
complex feature families. We extended previous results and derived a practical formulation of the
generalization error bound, which contains only estimated and known quantities of the problem. We
then instantiated a particular class of features, namely state aggregation, and we proposed an algorithm"
CONCLUSIONS AND FUTURE WORKS,0.8066298342541437,"called Distributional Max-Ent Progressive Factorization to adaptively find a feature representation
that optimizes for a proxy of the generalization error bound in a Structural Risk Minimization fashion.
In this way, we showed that performing PE can indeed drive the learning of a reduced-dimension
representation in the distributional setting. We then provided illustrative simulations showing the
empirical behaviors of these approaches, while clarifying the links between some hyperparameters
and the sample regime. Much of our analysis and theoretical guarantees straightforwardly extend to
other feature classes, and an open question is to investigate other instances of features and settings
that can benefit from the proposed framework. Future works will focus on the interaction between
Temporal Difference (TD) distributional methods and representation learning in the proposed setting
and on the existence of MDP instances that enjoy some relevant properties in the bias/variance
trade-off along successive factorizations, leading to high performance or better error bounds."
CONCLUSIONS AND FUTURE WORKS,0.8121546961325967,Acknowledgments
CONCLUSIONS AND FUTURE WORKS,0.8176795580110497,This paper is supported by PNRR-PE-AI FAIR project funded by the NextGeneration EU program.
REFERENCES,0.8232044198895028,References
REFERENCES,0.8287292817679558,"Andrew R. Barron and Chyong-Hwa Sheu. Approximation of Density Functions by Sequences of
Exponential Families. The Annals of Statistics, 19(3), September 1991. ISSN 0090-5364. doi:
10.1214/aos/1176348252."
REFERENCES,0.8342541436464088,"Marc G. Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C. Machado,
Subhodeep Moitra, Sameera S. Ponda, and Ziyu Wang. Autonomous navigation of stratospheric
balloons using reinforcement learning. Nature, 588(7836):77–82, December 2020. ISSN 0028-
0836, 1476-4687. doi: 10.1038/s41586-020-2939-8."
REFERENCES,0.8397790055248618,"Marc G. Bellemare, Will Dabney, and Mark Rowland. Distributional Reinforcement Learning. MIT
Press, 2023."
REFERENCES,0.8453038674033149,"Daniel Brown, Scott Niekum, and Marek Petrik. Bayesian robust optimization for imitation learning.
Advances in Neural Information Processing Systems, 33:2479–2491, 2020."
REFERENCES,0.850828729281768,"Yinlam Chow, M. Ghavamzadeh, Lucas Janson, and M. Pavone. Risk-constrained reinforcement
learning with percentile risk criteria. Journal Of Machine Learning Research, 2015."
REFERENCES,0.856353591160221,"Scott Clayton. Lecture notes (topic 10) of eecs 598: Statistical learning theory, 2014."
REFERENCES,0.861878453038674,"Corinna Cortes, Vitaly Kuznetsov, M. Mohri, and Umar Syed. Structural Maxent Models. July 2015."
REFERENCES,0.8674033149171271,"Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. Provably Efficient Reinforcement Learning
with Aggregated States, February 2020. arXiv:1912.06366 [cs, math, stat]."
REFERENCES,0.8729281767955801,"Simon S Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudík, and John Langford.
Provably efficient RL with Rich Observations via Latent State Decoding. 2019."
REFERENCES,0.8784530386740331,"Miroslav Dudík and Robert E. Schapire. Maximum Entropy Distribution Estimation with Generalized
Regularization. In Learning Theory, volume 4005, pages 123–138. Springer Berlin Heidelberg,
Berlin, Heidelberg, 2006. ISBN 978-3-540-35294-5 978-3-540-35296-9. doi: 10.1007/11776420_
12. Series Title: Lecture Notes in Computer Science."
REFERENCES,0.8839779005524862,"Miroslav Dudík, Steven J. Phillips, and Robert E. Schapire. Performance Guarantees for Regularized
Maximum Entropy Density Estimation. In Learning Theory, volume 3120, pages 472–486. Springer
Berlin Heidelberg, Berlin, Heidelberg, 2004. ISBN 978-3-540-22282-8 978-3-540-27819-1. doi:
10.1007/978-3-540-27819-1_33. Series Title: Lecture Notes in Computer Science."
REFERENCES,0.8895027624309392,"R. M. Dudley.
Uniform Central Limit Theorems.
Cambridge University Press, 1 edition,
July 1999. ISBN 978-0-521-46102-3 978-0-521-05221-4 978-0-511-66562-2. doi: 10.1017/
CBO9780511665622."
REFERENCES,0.8950276243093923,"Ramtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill. Being Optimistic to Be
Conservative: Quickly Learning a CVaR Policy. Proceedings of the AAAI Conference on Artificial
Intelligence, 34(04):4436–4443, April 2020. doi: 10.1609/aaai.v34i04.5870."
REFERENCES,0.9005524861878453,"V. Koltchinskii and D. Panchenko. Empirical Margin Distributions and Bounding the Generalization
Error of Combined Classifiers. The Annals of Statistics, 30(1):1 – 50, 2002. doi: 10.1214/aos/
1015362183."
REFERENCES,0.9060773480662984,"Christos N. Mavridis, Nilesh Suriyarachchi, and John S. Baras. Maximum-Entropy Progressive State
Aggregation for Reinforcement Learning. In 2021 60th IEEE Conference on Decision and Control
(CDC), pages 5144–5149, Austin, TX, USA, December 2021. IEEE. ISBN 978-1-66543-659-5.
doi: 10.1109/CDC45484.2021.9682927."
REFERENCES,0.9116022099447514,"Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
Adaptive computation and machine learning. The MIT Press, Cambridge, Massachusetts, second
edition edition, 2018. ISBN 978-0-262-03940-6."
REFERENCES,0.9171270718232044,"Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-Agnostic Estimation
of Discounted Stationary Distribution Corrections, November 2019. URL http://arxiv.org/
abs/1906.04733. arXiv:1906.04733 [cs, stat]."
REFERENCES,0.9226519337016574,"Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. USA,
1st edition, 1994. ISBN 0471619779."
REFERENCES,0.9281767955801105,"Mark Rowland, Robert Dadashi, Saurabh Kumar, Rémi Munos, Marc G. Bellemare, and Will Dabney.
Statistics and Samples in Distributional Reinforcement Learning. 2019. doi: 10.48550/ARXIV.
1902.08102."
REFERENCES,0.9337016574585635,"Mark Rowland, Shayegan Omidshafiei, Daniel Hennes, Will Dabney, Andrew Jaegle, Paul Muller,
Julien Pérolat, and Karl Tuyls. Temporal Difference and Return Optimism in Cooperative Multi-
Agent Reinforcement Learning. page 9, 2021."
REFERENCES,0.9392265193370166,"Mark Rowland, Rémi Munos, Mohammad Gheshlaghi Azar, Yunhao Tang, Georg Ostrovski, Anna
Harutyunyan, Karl Tuyls, Marc G. Bellemare, and Will Dabney. An Analysis of Quantile Temporal-
Difference Learning, January 2023. arXiv:2301.04462 [cs, stat]."
REFERENCES,0.9447513812154696,"Satinder Singh, Tommi Jaakkola, and Michael Jordan. Reinforcement Learning with Soft State
Aggregation. In Advances in Neural Information Processing Systems, volume 7. MIT Press, 1994."
REFERENCES,0.9502762430939227,"Tobias Sutter, David Sutter, Peyman Mohajerin Esfahani, and John Lygeros. Generalized maximum
entropy estimation. 2017. doi: 10.48550/ARXIV.1708.07311."
REFERENCES,0.9558011049723757,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.9613259668508287,"Csaba Szepesvári. Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelli-
gence and Machine Learning, 4(1):1–103, January 2010. ISSN 1939-4608, 1939-4616."
REFERENCES,0.9668508287292817,"Aad W. van der Vaart and Jon A. Wellner. Weak Convergence and Empirical Processes. Springer
Series in Statistics. Springer New York, New York, NY, 1996. ISBN 978-1-4757-2547-6 978-1-
4757-2545-2. doi: 10.1007/978-1-4757-2545-2."
REFERENCES,0.9723756906077348,"Benjamin Van Roy. Performance Loss Bounds for Approximate Value Iteration with State Ag-
gregation. Mathematics of Operations Research, 31(2):234–244, May 2006. ISSN 0364-765X,
1526-5471. doi: 10.1287/moor.1060.0188."
REFERENCES,0.9779005524861878,"V. Vapnik. Principles of Risk Minimization for Learning Theory. In Advances in Neural Information
Processing Systems, volume 4. Morgan-Kaufmann, 1991."
REFERENCES,0.9834254143646409,"Martin J. Wainwright and Michael I. Jordan. Graphical Models, Exponential Families, and Variational
Inference. Foundations and Trends in Machine Learning, 1:1–305, 2007. ISSN 1935-8237, 1935-
8245. doi: 10.1561/2200000001."
REFERENCES,0.988950276243094,"Shaojun Wang, Russell Greiner, and Shaomin Wang. Consistency and Generalization Bounds for
Maximum Entropy Density Estimation. Entropy, 15(12):5439–5463, December 2013. ISSN
1099-4300. doi: 10.3390/e15125439."
REFERENCES,0.994475138121547,"Bin Yu. Rates of Convergence for Empirical Processes of Stationary Mixing Sequences. The Annals of
Probability, 22(1):94–116, 1994. ISSN 0091-1798. Publisher: Institute of Mathematical Statistics."
