Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0019880715705765406,"We propose to identify directions invariant to a given classiﬁer so that these direc-
tions can be controlled in tasks such as style transfer. While orthogonal decom-
position is directly identiﬁable when the given classiﬁer is linear, we formally de-
ﬁne a notion of orthogonality in the non-linear case. We also provide a surpris-
ingly simple method for constructing the orthogonal classiﬁer (a classiﬁer utiliz-
ing directions other than those of the given classiﬁer). Empirically, we present
three use cases where controlling orthogonal variation is important: style transfer,
domain adaptation, and fairness. The orthogonal classiﬁer enables desired style
transfer when domains vary in multiple aspects, improves domain adaptation with
label shifts and mitigates the unfairness as a predictor. The code is available at
https://github.com/Newbeeer/orthogonal_classifier."
INTRODUCTION,0.003976143141153081,"1
INTRODUCTION"
INTRODUCTION,0.005964214711729622,"Many machine learning applications require explicit control of directions that are orthogonal to a
predeﬁned one. For example, to ensure fairness, we can learn a classiﬁer that is orthogonal to sensitive
attributes such as gender or race (Zemel et al., 2013; Madras et al., 2018). Similar, if we transfer images
from one style to another, content other than style should remain untouched. Therefore images before
and after transfer should align in directions orthogonal to style. Common to these problems is the task
of ﬁnding an orthogonal classiﬁer. Given any principal classiﬁer operating on the basis of principal
variables, our goal is to ﬁnd a classiﬁer, termed orthogonal classiﬁer, that predicts the label on the basis
of orthogonal variables, deﬁned formally later."
INTRODUCTION,0.007952286282306162,"The notion of orthogonality is clear in the linear case. Consider a joint distribution PXY over X ∈Rd and
binary label Y . Suppose the label distribution is Bernoulli, i.e., PY = B(Y ;0.5) and class-conditional
distributions are Gaussian, PX∣Y =y = N(X;µy,σ2
yI), where the means and variances depend on the
label. If the principal classiﬁer is linear, w1 = Pr(Y = 1∣θ⊺
1x), any classiﬁer w2, in the set W2 = {Pr(Y =
1∣θ⊺
2x) ∣θ⊺
1θ2 = 0}, is considered orthogonal to w1. Thus the two classiﬁers w1,w2, with orthogonal
decision boundaries (Fig. 1) focus on distinct but complementary attributes for predicting the same label."
INTRODUCTION,0.009940357852882704,"Finding the orthogonal classiﬁer is no longer straightforward in the non-linear case. To rigorously deﬁne
what we mean by the orthogonal classiﬁer, we ﬁrst introduce the notion of mutually orthogonal random
variables that correspond to (conditinally) independent latent variables mapped to observations through a
diffeomorphism (or bijection if discrete). Each r.v. is predictive of the label but represents complementary
information. Indeed, we show that the orthogonal random variable maximizes the conditional mutual
information with the label given the principal counterpart, subject to an independence constraint that
ensures complementarity."
INTRODUCTION,0.011928429423459244,"Our search for the orthogonal classiﬁer can be framed as follows: given a principal classiﬁer w1 using
some unknown principal r.v. for prediction, how do we ﬁnd its orthogonal classiﬁer w2 relying solely on
orthogonal random variables? The solution to this problem, which we call classiﬁer orthogonalization,
turns out to be surprisingly simple. In addition to the principal classiﬁer, we assume access to a full
classiﬁer wx that predicts the same label based on all the available information, implicitly relying on
both principal and orthogonal latent variables. The full classiﬁer can be trained normally, absent of
constraints1. We can then effectively “subtract” the contribution of w1 from the full classiﬁer to obtain
the orthogonal classiﬁer w2 which we denote as w2 = wx ∖w1. The advantage of this construction is that"
INTRODUCTION,0.013916500994035786,"1The full classiﬁer may fail to depend on all the features, e.g., due to simplicity bias (Shah et al., 2020)."
INTRODUCTION,0.015904572564612324,Published as a conference paper at ICLR 2022
INTRODUCTION,0.017892644135188866,"2
1
0
1
2
3
4
5
x1 2 1 0 1 2 3 4 5 x2"
INTRODUCTION,0.019880715705765408,p(x|Y = 0)
INTRODUCTION,0.02186878727634195,p(x|Y = 1)
INTRODUCTION,0.02385685884691849,"2
1
0
1
2
3
4
5
x1 2 1 0 1 2 3 4 5 x2"
INTRODUCTION,0.02584493041749503,"2
1
0
1
2
3
4
5
x1 2 1 0 1 2 3 4 5 x2"
INTRODUCTION,0.027833001988071572,"2
1
0
1
2
3
4
5
x1 2 1 0 1 2 3 4 5 x2"
INTRODUCTION,0.02982107355864811,"(a) Distribution pX∣Y
(b) wx(x)
(c) w1(x)
(d) w2(x)
Figure 1: An illustrative example of orthogonal classiﬁer in a linear case. (a) is the data distributions in
two classes; (b,c,d) are the probabilities of the data from class 1 predicted by the full/principal/orthogonal
classiﬁers. Red and blue colors mean a probability close to 1 or 0. The white color indicates regions with
a probability close to 0.5, which are classiﬁers’ decision boundaries. Clearly, w1 and w2 have orthogonal
decision boundaries."
INTRODUCTION,0.03180914512922465,"we do not need to explicitly identify the underlying orthogonal variables. It sufﬁces to operate only on
the level of classiﬁer predictions."
INTRODUCTION,0.033797216699801194,"We provide several use cases for the orthogonal classiﬁer, either as a predictor or as a discriminator. As a
predictor, the orthogonal classiﬁer predictions are invariant to the principal sensitive r.v., thus ensuring
fairness. As a discriminator, the orthogonal classiﬁer enforces a partial alignment of distributions,
allowing changes in the principal direction. We demonstrate the value of such discriminators in 1)
controlled style transfer where the source and target domains differ in multiple aspects, but we only wish
to align domain A’s style to domain B, leaving other aspects intact; 2) domain adaptation with label
shift where we align feature distributions between the source and target domains, allowing shifts in label
proportions. Our results show that the simple method is on par with the state-of-the-art methods in each
task."
NOTATIONS AND DEFINITION,0.03578528827037773,"2
NOTATIONS AND DEFINITION"
NOTATIONS AND DEFINITION,0.03777335984095427,"Symbols. We use the uppercase to denote random variable (e.g., data X, label Y ), the lowercase to
denote the corresponding samples and the calligraphic letter to denote the sample spaces of r.v., e.g., data
sample space X. We focus on the setting where label space Y is discrete, i.e., Y = {1,⋯,C}, and denote
the C −1 dimensional probability simplex as ∆C. A classiﬁer w ∶X →∆C is a mapping from sample
space to the simplex. Its y-th dimension w(x)y denotes the predicted probability of label y for sample x."
NOTATIONS AND DEFINITION,0.039761431411530816,"Distributions.
For random variables A,B, we use the notation pA, pA∣B, pAB to denote the
marginal/conditional/joint distribution, i.e., pA(a) = p(A = a), pA∣B(a∣b) = p(A = a∣B = b),pAB(a,b) =
p(A = a,B = b). Sometimes, for simplicity, we may ignore the subscript if there is no ambiguity, e.g.,
p(a∣b) is an abbreviation for pA∣B(a∣b)."
NOTATIONS AND DEFINITION,0.041749502982107355,"We begin by deﬁning the notion of an orthogonal random variable. We consider continuous X,Z1,Z2
and assume their supports are manifolds diffeomorphic to the Euclidean space. The probability density
functions (PDF) are in C1. Given a joint distribution pXY , we deﬁne the orthogonal random variable as
follows:"
NOTATIONS AND DEFINITION,0.0437375745526839,"Deﬁnition 1 (Orthogonal random variables). We say Z1 and Z2 are orthogonal random variables w.r.t
pXY if they satisfy the following properties:"
NOTATIONS AND DEFINITION,0.04572564612326044,"(i) There exists a diffeomorphism f ∶Z1 × Z2 →X such that f(Z1,Z2) = X."
NOTATIONS AND DEFINITION,0.04771371769383698,"(ii) Z1 and Z2 are statistically independent given Y , i.e., Z1 ⊥⊥Z2∣Y ."
NOTATIONS AND DEFINITION,0.04970178926441352,"The orthogonality relation is symmetric by deﬁnition. Note that the orthogonal pair perfectly reconstructs
the observations via the diffeomorphism f; as random variables they are also sampled independently
from class conditional distributions p(Z1∣Y ) and p(Z2∣Y ). For example, we can regard foreground
objects and background scenes in natural images as being mutually orthogonal random variables."
NOTATIONS AND DEFINITION,0.05168986083499006,"Remark.
The deﬁnition of orthogonality can be similarly developed for discrete variables and discrete-
continuous mixtures. For discrete variables, for example, we can replace the requirement of diffeomor-
phism with bijection."
NOTATIONS AND DEFINITION,0.0536779324055666,Published as a conference paper at ICLR 2022
NOTATIONS AND DEFINITION,0.055666003976143144,"Since the diffeomorphism f is invertible, we can use z1 ∶X →Z1 and z2 ∶X →Z2 to denote the two
parts of the inverse mapping so that Z1 = z1(X) and Z2 = z2(X). Note that, for a given joint distribution
pXY , the decomposition into orthogonal random variables is not unique. There are multiple pairs of
random variables that represent valid mutually orthogonal latents of the data. We can further justify our
deﬁnition of orthogonality from an information theoretic perspective by showing that the choice of z2
attains the maximum of the following constrained optimization problem.
Proposition 1. Suppose the orthogonal r.v. of z1(X) w.r.t pXY exists and is denoted as z2(X). Then
z(X) = z2(X) is a maximizer of I(z(X);Y ∣z1(X)) subject to I(z(X);z1(X)∣Y ) = 0."
NOTATIONS AND DEFINITION,0.05765407554671968,"We defer the proof to Appendix B.1. Proposition 1 shows that the orthogonal random variable maxi-
mizes the additional information about the label we can obtain from X while remaining conditionally
independent of the principal random variable. This ensures complementary in predicting the label."
CONSTRUCTING THE ORTHOGONAL CLASSIFIER,0.05964214711729622,"3
CONSTRUCTING THE ORTHOGONAL CLASSIFIER"
CONSTRUCTING THE ORTHOGONAL CLASSIFIER,0.061630218687872766,"Let Z1 = z1(X) and Z2 = z2(X) be mutually orthogonal random variables w.r.t pXY . We call Z1 the
principal variable and Z2 the orthogonal variable. In this section, we describe how we can construct the
Bayes optimal classiﬁer operating on features Z2 from the Bayes optimal classiﬁer relying on Z1. We
formally refer to the classiﬁers of interests as: (1) principal classiﬁer w1(x)y = p(Y = y∣Z1 = z1(x));
(2) orthogonal classiﬁer w2(x)y = p(Y = y∣Z2 = z2(x)); (3) full classiﬁer wx(x)y = p(Y = y∣X = x)."
CLASSIFIER ORTHOGONALIZATION,0.0636182902584493,"3.1
CLASSIFIER ORTHOGONALIZATION"
CLASSIFIER ORTHOGONALIZATION,0.06560636182902585,"Our key idea relies on the bijection between the density ratio and the Bayes optimal classiﬁer (Sugiyama
et al., 2012). Speciﬁcally, the ratio of densities pX∣Y (x∣i) and pX∣Y (x∣j), assigned to an arbitrary point"
CLASSIFIER ORTHOGONALIZATION,0.06759443339960239,"x, can be represented by the Bayes optimal classiﬁer w(x)i = Pr(Y = i∣x) as
pX∣Y (x∣i)
pX∣Y (x∣j) = pY (j)w(x)i"
CLASSIFIER ORTHOGONALIZATION,0.06958250497017893,"pY (i)w(x)j .
Similar, the principal classiﬁer w1 gives us associated density ratios of class-conditional distributions
over Z1. For any i,j ∈Y, we have
pZ1∣Y (z1(x)∣i)
pZ1∣Y (z1(x)∣j) = pY (j)w1(x)i"
CLASSIFIER ORTHOGONALIZATION,0.07157057654075547,"pY (i)w1(x)j . These can be combined to obtain
density ratios of class-conditional distribution pZ2∣Y and subsequently calculate the orthogonal classiﬁer
w2. We additionally rely on the fact that the diffeomorphism f permits us to change variables between
x and z1,z2: pX∣Y (x∣i) = pZ1∣Y (z1∣i) ∗pZ2∣Y (z2∣i) ∗volJf(z1,z2), where volJf is volume of the
Jacobian (Berger et al., 1987) of the diffeomorphism mapping. Taken together,"
CLASSIFIER ORTHOGONALIZATION,0.073558648111332,"pZ2∣Y (z2∣i)
pZ2∣Y (z2∣j) = pZ1∣Y (z1∣i) ∗pZ2∣Y (z2∣i) ∗volJf(z1,z2)"
CLASSIFIER ORTHOGONALIZATION,0.07554671968190854,"pZ1∣Y (z1∣j) ∗pZ2∣Y (z2∣j) ∗volJf(z1,z2) / pZ1∣Y (z1∣i)"
CLASSIFIER ORTHOGONALIZATION,0.0775347912524851,pZ1∣Y (z1∣j) = wx(x)i
CLASSIFIER ORTHOGONALIZATION,0.07952286282306163,"wx(x)j
/ w1(x)i"
CLASSIFIER ORTHOGONALIZATION,0.08151093439363817,"w1(x)j
(1)"
CLASSIFIER ORTHOGONALIZATION,0.08349900596421471,"Note that since the diffeomorphism f is shared with all classes, the Jacobian is the same for all label-
conditioned distributions on Z1,Z2. Hence the Jacobian volume terms cancel each other in the above
equation. We can then ﬁnally work backwards from the density ratios of pZ2∣Y to the orthogonal classiﬁer:"
CLASSIFIER ORTHOGONALIZATION,0.08548707753479125,Pr(Y = i∣Z2 = z2(x)) = Pr(Y = i)wx(x)i
CLASSIFIER ORTHOGONALIZATION,0.0874751491053678,"w1(x)i
/ ∑
j
(Pr(Y = j)wx(x)j"
CLASSIFIER ORTHOGONALIZATION,0.08946322067594434,"w1(x)j
)
(2)"
CLASSIFIER ORTHOGONALIZATION,0.09145129224652088,"We call this procedure classiﬁer orthogonalization since it adjusts the full classiﬁer wx to be orthogonal
to the principal classiﬁer w1. The validity of this procedure requires overlapping supports of the class-
conditional distributions, which ensures the classiﬁer outputs wx(x)i,w1(x)i to remain non-zero for all
x ∈X,i ∈Y."
CLASSIFIER ORTHOGONALIZATION,0.09343936381709742,"Empirically, we usually have access to a dataset D = {(xt,yt)}n
t=1 with n iid samples from the joint
distribution pXY . To obtain the orthogonal classiﬁer, we need to ﬁrst train the full classiﬁer ˆwx based
on the dataset D. We can then follow the classiﬁer orthogonalization to get an empirical orthogonal
classiﬁer, denoted as w2 = ˆwx ∖w1. We use symbol ∖to emphasize that the orthogonal classiﬁer uses
complementary information relative to z1. Algorithm 1 summarizes the construction of the orthogonal
classiﬁer."
CLASSIFIER ORTHOGONALIZATION,0.09542743538767395,"Generalization bound. Since wx is trained on a ﬁnite dataset, we consider the generalization bound of
the constructed orthogonal classiﬁer. We denote the population risk as R(w) = −EpXY (x,y) [log w(x)y]
and the empirical risk as ˆR(w) = −1"
CLASSIFIER ORTHOGONALIZATION,0.09741550695825049,"∣D∣∑(xi,yi)∈D log w(xi)yi. For a function family W whose elements"
CLASSIFIER ORTHOGONALIZATION,0.09940357852882704,"map X to the simplex ∆C, we deﬁne ˆwx = infwx∈W ˆR(wx),w∗
x = infwx∈W R(wx). We further denote
the Rademacher complexity of function family W with sample size ∣D∣as R∣D∣(W)."
CLASSIFIER ORTHOGONALIZATION,0.10139165009940358,Published as a conference paper at ICLR 2022
CLASSIFIER ORTHOGONALIZATION,0.10337972166998012,Algorithm 1 Classiﬁer Orthogonalization
CLASSIFIER ORTHOGONALIZATION,0.10536779324055666,"Input: principal classiﬁer w1, dataset D.
Train an empirical full classiﬁer ˆwx on D by empirical risk minimization.
Construct an orthogonal classiﬁer ˆwx ∖w1 via classiﬁer orthogonalization (Eq. (2)).
return the empirical orthogonal classiﬁer ˆw2 = ˆwx ∖w1"
CLASSIFIER ORTHOGONALIZATION,0.1073558648111332,"Theorem 1. Assume py is uniform distribution, ∀wx ∈W takes values in (m,1 −m)C with m ∈(0, 1"
CLASSIFIER ORTHOGONALIZATION,0.10934393638170974,"2),
and 1/pX∣Y (x∣y) ∈(0,γ) ⊂(0,+∞) holds for ∀x ∈X,y ∈Y. Then for any δ ∈(0,1) with probability
at least 1 −δ, we have:"
CLASSIFIER ORTHOGONALIZATION,0.11133200795228629,"∣R( ˆwx ∖w1) −R(w∗
x ∖w1)∣≤(1 + γ)
⎛
⎜
⎝
2R∣D∣(W) + 2log 1 m"
CLASSIFIER ORTHOGONALIZATION,0.11332007952286283,"¿
Á
Á
À2log 1 δ
∣D∣ ⎞
⎟
⎠"
CLASSIFIER ORTHOGONALIZATION,0.11530815109343936,"Theorem 1 shows that the population risk of the empirical orthogonal classiﬁer in Algorithm 1 would
be close to the optimal risk if the maximum value of the reciprocal of class-conditioned distributions
1/pX∣Y (x∣y) and the Rademacher term are small. Typically, the Rademacher complexity term satisﬁes
R∣D∣(W) = O(∣D∣−1"
CLASSIFIER ORTHOGONALIZATION,0.1172962226640159,"2 ) (Bartlett & Mendelson, 2001; Gao & Zhou, 2016). We note that the empirical full
classiﬁer may fail in speciﬁc ways that are harmful for our purposes. For example, the classiﬁer may
not rely on all the key features due to simplicity bias as demonstrated by (Shah et al., 2020). There are
several ways that this effect can be mitigated, including Ross et al. (2020); Teney et al. (2021)."
CLASSIFIER ORTHOGONALIZATION,0.11928429423459244,"3.2
ALTERNATIVE METHOD: IMPORTANCE SAMPLING"
CLASSIFIER ORTHOGONALIZATION,0.12127236580516898,"An alternative way to get the orthogonal classiﬁer is via importance sampling. For each sample point
(x,y), we construct its importance φ(x,y) ∶=
pZ1(z1(x))
pZ1∣Y (z1(x)∣y). Via Bayes’ rule, the importance φ(x,y) can"
CLASSIFIER ORTHOGONALIZATION,0.12326043737574553,"be calculated from the principal classiﬁer by φ(x,y) =
pY (y)
w1(x)y . We deﬁne the importance sampling (IS)
objective as LIS(w) ∶= Ex,y∼pXY [φ(x,y)log w(x)y]. It can be shown that the orthogonal classiﬁer w2
maximizes the IS objective, i.e., w2 = arg maxLIS. However, the importance sampling method has an
Achilles’ heel: variance. A few bad samples with large weights can drastically deviate the estimation,
which is problematic for mini-batch learning in practice. We provide an analysis of the variance of
the IS objective. It shows the variance increases with the divergences between Z1’s marginal and its
label-conditioned marginals, {Df(pZ1∣∣pZ1∣Y =y)}C
y=1 even when the learned classiﬁer w is approaching
the optimal classiﬁer, i.e., w ≈w2. Let ̂
Ln
IS(w) ∶= 1"
CLASSIFIER ORTHOGONALIZATION,0.12524850894632206,"n ∑n
t=1 φ(xt,yt)log w(xt)yt be the empirical IS
objective estimated with n iid samples. Clearly, Var(̂
Ln
IS(w)) = 1"
CLASSIFIER ORTHOGONALIZATION,0.1272365805168986,"nVar(̂
L1
IS(w)). While Var(̂
L1
IS(w))
at w = w2 is the following,"
CLASSIFIER ORTHOGONALIZATION,0.12922465208747516,"Var(̂
L1
IS(w2)) = EY [(Df(pZ1∣∣pZ1∣Y =y) + 1)EZ2∣Y =y log2 pY ∣Z2(y∣z2)] −L(w2)2"
CLASSIFIER ORTHOGONALIZATION,0.1312127236580517,"where Df is the Pearson χ2-divergence. The expression indicates that the variance grows linearly with
the expected divergence. In contrast, the divergences have little effect when training the empirical full
classiﬁer in classiﬁer orthogonalization. We provide further experimental results in section 4.1.2 to
demonstrate that classiﬁer orthogonalization has better stability than the IS objective with increasing
divergences and learning rates."
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.13320079522862824,"4
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.13518886679920478,"In style transfer, we have two domains A,B with distributions PXY ,QXY . We use binary label Y to
indicate the domain of data X (0 for domain A, 1 for domain B). Assume Z1 and Z2 are mutually
orthogonal variables w.r.t both PXY and QXY . The goal is to conduct controlled style transfer between
the two domains. By controlled style transfer, we mean transferring domain A’s data to domain B only
via making changes on partial latent (Z1) while not touching the other latent (Z2). Mathematically,
we aim to transfer the latent distribution from PZ1PZ2 to QZ1PZ2. This task cannot be achieved by
traditional style transfer algorithms such as CycleGAN (Zhu et al., 2017), since they directly transfer data
distributions from PX to QX, or equivalently from the perspective of latent distributions, from PZ1PZ2
to QZ1QZ2. Below we show that the orthogonal classiﬁer wx ∖w1 enables such controlled style transfer."
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.13717693836978131,"Our strategy is to plug the orthogonal classiﬁer into the objective of CycleGAN. The original Cycle-
GAN objective deﬁnes a min-max game between two generators/discriminator pairs GAB/DAB and"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.13916500994035785,Published as a conference paper at ICLR 2022
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.1411530815109344,"1.2
1.4
1.6
1.8
log
y[Df(pZ1||pZ1|Y = y) + 1] 0.35 0.40 0.45 0.50 0.55"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.14314115308151093,Test loss
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.14512922465208747,"IS
wx\ w1
Oracle (a)"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.147117296222664,"3.0
2.5
2.0
1.5
1.0
Learning rate (log-scale) 0.35 0.40 0.45 0.50 0.55"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.14910536779324055,Test loss
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.15109343936381708,"IS
wx\ w1
Oracle (b)"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.15308151093439365,"0.0
0.2
0.4
0.6
0.8
1.0
Predicted probability 0 1000 2000 3000"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.1550695825049702,# of data
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.15705765407554673,P(Y = 1| B) (wx\ w1)
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.15904572564612326,P(Y = 1| G) (wx\ w1)
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.1610337972166998,P(Y = 1| B) (IS)
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.16302186878727634,P(Y = 1| G) (IS) (c)
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.16500994035785288,"Figure 2: (a,b): Test loss versus log expected divergence and learning rate. (c): The histogram of
predicted probability of different methods. The two red lines are the ground-truth probabilities for
P(Y = 1∣Brown background) and P(Y = 1∣Green background)."
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.16699801192842942,"GBA/DBA. GAB transforms the images from domain A to domain B. We use ̃P to denote the generative
distribution, i.e., GAB(X) ∼̃P for X ∼P. Similarly, ̃Q denotes the generative distribution of generator
GBA. The minimax game of CycleGAN is given by"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.16898608349900596,"min
GAB,GBA
max
DAB,DBALAB
GAN(GAB,DAB) + LBA
GAN(GBA,DBA) + Lcyc(GAB,GBA)
(3)"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.1709741550695825,"where GAN losses LAB
GAN,LBA
GAN minimize the divergence between the generative distributions and the
targets, and the cycle consistency loss Lcyc is used to regularize the generators (Zhu et al., 2017).
Concretely, the GAN loss LAB
GAN is deﬁned as follows,"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.17296222664015903,"LAB
GAN(GAB,DAB) ∶= Ex∼Q[log DAB(x)] + Ex∼̃
P [log(1 −DAB(x))]"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.1749502982107356,"Now we tilt the GAN losses in CycleGAN objective to guide the generators to perform controlled transfer.
Specially, we replace LAB
GAN in Eq. (3) with the orthogonal GAN loss LAB
OGAN when updating the generator
GAB:"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.17693836978131214,"LAB
OGAN(GAB,DAB) ∶= Ex∼̃
P [log(1 −φ(DAB(x),r(x)))]"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.17892644135188868,"where φ(DAB(x),r(x)) =
DAB(x)r(x)
(1−DAB(x))+DAB(x)r(x) and r(x) = wx∖w1(x)0"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.18091451292246521,"wx∖w1(x)1 . Consider an extreme case
where we allow the model to change all latents, i.e., z1(x) = x. As a result, LOGAN degenerates to
the original LGAN since wx ∖w1 ≡1"
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.18290258449304175,"2 and φ(DAB(x),r(x)) ≡DAB(x). The other orthogonal GAN
loss LBA
OGAN can be similarly derived. For a given generator GAB, the discriminator’s optimum is
achieved at D∗
AB(x) = Q(x)/( ̃P(x) + Q(x)). Assuming the discriminator is always trained to be
optimal, the generator GAB can be viewed as optimizing the following virtual objective: LAB
OGAN(GAB) ∶="
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.1848906560636183,"LAB
OGAN(GAB,D∗
AB) = Ex∼̃
P log
̃
P (x)
̃
P (x)+Q(x)r(x). The optimal generator in the new objective satisﬁes the
following property."
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.18687872763419483,"Proposition 2. The global minimum of LAB
OGAN(GAB) is achieved if and only if ̃PZ1,Z2(z1,z2) =
QZ1(z1)PZ2(z2)."
ORTHOGONAL CLASSIFIER FOR CONTROLLED STYLE TRANSFER,0.18886679920477137,"We defer the proof to Appendix B.4. The proposition states that the new objective LAB
OGAN converts the
original global minimum QZ1QZ2 in LAB
GAN to the desired one QZ1PZ2. The symmetric statement holds
for LBA
OGAN(GBA)."
EXPERIMENTS,0.1908548707753479,"4.1
EXPERIMENTS"
SETUP,0.19284294234592445,"4.1.1
SETUP"
SETUP,0.19483101391650098,"Datasets. (a) CMNIST: We construct C(olors)MNIST dataset based on MNIST digits (LeCun & Cortes,
2005). CMNIST has two domains and 60000 images with size (3,28,28). The majority of the images
in each domain will have a digit color and a background color correspond to the domain: domain A
↔“red digit/green background” and domain B ↔“blue digit/brown background”. We deﬁne two bias
degrees λd, λb to control the probability of the images having domain-associated colors, e.g., for an
image in domain A, with λd probability, its digit is set to be red; otherwise, its digit color is randomly
red or blue. The background colors are determined similarly with parameter λb. In our experiments,"
SETUP,0.19681908548707752,Published as a conference paper at ICLR 2022
SETUP,0.1988071570576541,Table 1: CMNIST
SETUP,0.20079522862823063,"Z1 acc.
Z2 acc."
SETUP,0.20278330019880716,"Vanilla
90.2
14.3
wx ∖w1 (MLDG)
94.9
91.1
wx ∖w1 (TRM)
89.2
96.2
wx ∖w1 (Fish)
93.9
98.0"
SETUP,0.2047713717693837,"IS (Oracle)
90.0
100
wx ∖w1 (Oracle)
94.9
100"
SETUP,0.20675944333996024,Table 2: CelebA-GH
SETUP,0.20874751491053678,"Z1 acc.
Z2 acc.
FID ↓"
SETUP,0.21073558648111332,"Vanilla
88.4
16.8
38.5
wx ∖w1 (MLDG)
90.9
53.0
46.2
wx ∖w1 (TRM)
92.2
56.5
39.8
wx ∖w1 (Fish)
88.8
42.9
43.4"
SETUP,0.21272365805168986,"IS (Oracle)
89.1
40.6
44.3
wx ∖w1 (Oracle)
93.7
57.4
39.7"
SETUP,0.2147117296222664,"we set λd = 0.9 and λb = 0.8. Our goal is to transfer the digit color (Z1) while leaving the background
color (Z2) invariant. (b) CelebA-GH: We construct the CelebA-G(ender)H(air) dataset based on the
gender and hair color attributes in CelebA (Liu et al., 2015). CelebA-GH consists of 110919 images
resized to (3,128,128). In CelebA-GH, domain A is non-blond-haired males, and the domain B is
blond-haired females. Our goal is to transfer the facial characteristic of gender (Z1) while keeping the
hair color (Z2) intact."
SETUP,0.21669980119284293,"Models. We compare variants of orthogonal classiﬁers to the vanilla CycleGAN (Vanilla) and the
importance sampling objective (IS). We consider two ways of obtaining the w1 classiﬁer: (a) Oracle:
The principal classiﬁer w1 is trained on an oracle dataset where Z1 is the only discriminative direction,
e.g., setting bias degrees λd = 0.9 and λb = 0 in CMNIST such that only digit colors vary across domains.
(b) Domain generalization: Domain generalization algorithms aim to learn the prediction mechanism that
is invariant across environments and thus generalizable to unseen environments (Blanchard et al., 2011;
Arjovsky et al., 2019). The variability of environments is considered as nuisance variation. We construct
environments such that only Y ∣Z1 is invariant. We defer details of the environments to Appendix E.1.
We use three domain generalization algorithms, Fish (Shi et al., 2021), TRM (Xu & Jaakkola, 2021) and
MLDG (Li et al., 2018), to obtain w1. We indicate different approaches by the parentheses after wx ∖w1
and IS, e.g., wx ∖w1 (Oracle) is the orthogonal classiﬁer with w1 trained on Oracle datasets."
SETUP,0.21868787276341947,"Metric. We use three metrics for quantitative evaluation: 1) Z1 accuracy: the success rate of transferring
an image’s latent z1 from domain A to domain B; 2) Z2 accuracy: percentage of transferred images
whose latents z2 are unchanged; 3) FID scores: a standard metric of image quality (Heusel et al., 2017).
We only report FID scores on the CelebA-GH dataset since it is not common to compute FID on MNIST
images. Z1,Z2 accuracies are measured by two oracle classiﬁers that output an image’s latents z1 and
z2 (Appendix E.1.3)."
SETUP,0.220675944333996,"For more details of datasets, models and training procedure, please refer to Appendix E."
RESULTS,0.22266401590457258,"4.1.2
RESULTS"
RESULTS,0.22465208747514911,"Comparison to IS. In section 3.2, we demonstrate that IS suffers from high variance when the di-
vergences of the marginal and the label-conditioned latent distributions are large. We provide further
empirical evidence that our classiﬁer orthogonalization method is more robust than IS. Fig. 2(a) shows
that the test loss of IS increases dramatically with the divergences. Fig. 2(b) displays that IS’s test loss
grows rapidly when enlarging learning rate, which corroborates the observation that gradient variances
are more detrimental to the model’s generalization with larger learning rates (Wang et al., 2013). In
contrast, the test loss of wx ∖w1 remains stable with varying divergences and learning rates. In Fig. 2(c),
we visualize the histograms of predicted probabilities of wx ∖w1 (light color) and IS (dark color).
We observe that the predicted probabilities of wx ∖w1 better concentrates around the ground-truth
probabilities (red lines)."
RESULTS,0.22664015904572565,"Main result. As shown in Table 1 and 2, adding an orthogonal classiﬁer to the vanilla CycleGAN
signiﬁcantly improves its z2 accuracy (from 14 to 90+ in CMNIST, from 16 to 40+ in CelebA-GH)
while incurring a slight loss of the image quality. We observe that the oracle version of orthogonal classi-
ﬁer (wx ∖w1 (Oracle)) achieves best z2 accuracy on both datasets. In addition, class orthogonalization is
compatible with domain generalization algorithms when the prediction mechanism Z1∣Y is invariant
across collected environments."
RESULTS,0.2286282306163022,"In Fig. 3, we visualize the transferred samples from domain A. We observe that vanilla CycleGAN
models change the background colors/hair colors along with digit colors/facial characteristics in CMNIST
and CelebA. In contrast, orthogonal classiﬁers better preserve the orthogonal aspects Z2 in the input
images. We also provide visualizations for domain B in Appendix C."
RESULTS,0.23061630218687873,Published as a conference paper at ICLR 2022
RESULTS,0.23260437375745527,"Input
Vanilla
Oracle MLDG"
RESULTS,0.2345924453280318,(a) CMNIST: Domain A
RESULTS,0.23658051689860835,"Input
Vanilla
Oracle MLDG"
RESULTS,0.23856858846918488,(b) CelebA-GH: Domain A
RESULTS,0.24055666003976142,"Figure 3: Style transfer samples on the domain A of CMNIST and CelebA-GH. We visualize the
inputs (top row) and the corresponding transferred samples of different methods.
5
ORTHOGONAL CLASSIFIER FOR DOMAIN ADAPTATION"
RESULTS,0.24254473161033796,"In unsupervised domain adaptation, we have two domains, source and target with data distribution ps,
pt. Each sample comes with three variables, data X, task label Y and domain label U. We assume U is
uniformly distributed in {0 (source), 1 (target)}. The task label is only accessible in the source domain.
Consider learning a model h = g ○f that is the composite of the encoder f ∶X →Z and the predictor
g ∶Z →Y. A common practice of domain adaptation is to match the two domain’s feature distributions
ps(f(X)),pt(f(X)) via domain adversarial training (Ganin & Lempitsky, 2015; Long et al., 2018; Shu
et al., 2018). The encoder f is optimized to extract useful feature for the predictor while simultaneously
bridging the domain gap via the following domain adversarial objective:"
RESULTS,0.24453280318091453,"min
f
max
D L(f,D) ∶= Ex∼ps[log D(f(x))] + Ex∼pt[log(1 −D(f(x)))]
(4)"
RESULTS,0.24652087475149106,"where discriminator D ∶Z →[0,1] distinguishes features from two domains. The equilibrium of the
objective is achieved when the encoder perfectly aligns the two domain, i.e., ps(f(x)) = pt(f(x)). We
highlight that such equilibrium is not desirable when the target domain has shifted label distribution (Az-
izzadenesheli et al., 2019; des Combes et al., 2020). Instead, when the label shift appears, it is more
preferred to align the conditioned feature distribution,i.e., ps(f(x)∣y) = pt(f(x)∣y),∀y ∈Y."
RESULTS,0.2485089463220676,"Now we show that our classiﬁer orthogonalization technique can be applied to the discriminator for
making it “orthogonal to” the task label. Speciﬁcally, consider the original discriminator as full classiﬁer
wx, i.e., for a given f, wx(x)0 = arg maxD L(f,D) =
ps(f(x))
ps(f(x))+pt(f(x)). We then construct the principle
classiﬁer w1 that discriminates the domain purely via the task label Y , i.e., w1(x)0 = Pr(U = 0∣Y =
y(x)). Note that here we assume the task label Y can be determined by the data X, i.e., Y = y(X). We
focus on the case that Y is discrete so w1 could be directly computed via frequency count. We propose
to train the encoder f with the orthogonalized discriminator wx ∖w1,"
RESULTS,0.2504970178926441,"min fL(f,(wx ∖w1)(⋅)0) = Ex∼ps[log(wx ∖w1)(x)0] + Ex∼pt[log(1 −(wx ∖w1)(x)0)]
(5)"
RESULTS,0.2524850894632207,"Proposition 3. Suppose there exists random variable Z2 = z2(X) orthogonal to y(X) = Y w.r.t
p(f(X),U). Then f achieves global optimum if and only if it aligns all label-conditioned feature
distributions, i.e., ps(f(x)∣y) = pt(f(x)∣y),∀y ∈Y."
RESULTS,0.2544731610337972,"Note that in practice, we have no access to the target domain label prior pt(Y ) and the label y(x) for
target domain data x ∼pt. Thus we use the pseudo label ˆy as a surrogate to construct the principle
classiﬁer, where ˆy(x) = arg maxy h(x)y is generated by our model h."
EXPERIMENTS,0.25646123260437376,"5.1
EXPERIMENTS"
EXPERIMENTS,0.2584493041749503,"Models. We take a well-known domain adaptation algorithm VADA (Shu et al., 2018) as our baseline.
VADA is based on domain adversarial training and combined with virtual adversarial training and entropy
regularization. We show that utilizing orthogonal classiﬁer can improve its robustness to label shift. We
compare it with two improvements: (1) VADA+wx ∖w1 which is our method that plugs in the orthogonal
classiﬁer as a discriminator in VADA; (2) VADA+IW: We tailor the SOTA method for label shifted
domain adaptation, importance-weighted domain adaptation (des Combes et al., 2020), and apply it to
VADA. We also incorporate two recent domain adaptation algorithms for images— CyCADA (Hoffman
et al., 2018) and ADDA (Tzeng et al., 2017)—into comparisons."
EXPERIMENTS,0.26043737574552683,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.2624254473161034,"Setup. We focus on visual domain adaptation and directly borrow the datasets, architectures and domain
setups from Shu et al. (2018). To add label shift between domains, we control the label distribution in
the two domains. For source domain, we sub-sample 70% data points from the ﬁrst half of the classes
and 30% from the second half. We reverse the sub-sampling ratios on the target domain. The label
distribution remains the same across the target domain train and test set. Please see Appendix E.2 for
more details."
EXPERIMENTS,0.2644135188866799,"Results. Table 3 reports the test accuracy on seven domain adaptation tasks. We observe that VADA+wx∖
w1 improves over VADA across all tasks and outperforms VADA+IW on ﬁve out of seven tasks. We ﬁnd
VADA+IW performs worse than VADA in two tasks, MNIST→SVHN and MNIST→MNIST-M. Our
hypothesis is that the domain gap is large between these datasets, hindering the estimation of importance
weight. Hence, VADA-IW is unable to adapt the label shift appropriately. In addition, the results show
that VADA+wx ∖w1 outperforms ADDA on six out of seven tasks."
EXPERIMENTS,0.2664015904572565,Table 3: Test accuracy on visual domain adaptation benchmarks
EXPERIMENTS,0.268389662027833,"Source
MNIST
SVHN
MNIST
DIGITS
SIGNS
CIFAR
STL
Target
MNIST-M
MNIST
SVHN
SVHN
GTSRB
STL
CIFAR"
EXPERIMENTS,0.27037773359840955,"Source-Only
51.8
75.7
34.5
85.0
74.7
68.7
47.7
ADDA
89.7
78.2
38.4
86.0
90.6
66.8
50.4
CyCADA
-
82.8
39.6
-
-
-
-"
EXPERIMENTS,0.27236580516898606,"VADA
77.8
79.0
35.7
90.3
93.6
72.4
53.1
VADA + IW
71.2↓
87.1↑
34.5 ↓
90.7 ↑
95.4↑
74.0↑
53.8↑
VADA + wx∖w1
79.1↑
88.0↑
40.5↑
90.6↑
95.2↑
74.5 ↑
54.1↑"
ORTHOGONAL CLASSIFIER FOR FAIRNESS,0.27435387673956263,"6
ORTHOGONAL CLASSIFIER FOR FAIRNESS"
ORTHOGONAL CLASSIFIER FOR FAIRNESS,0.27634194831013914,"We are given a dataset D = {(xt,yt,ut)}n
t=1 that is sampled iid from the distribution pXY U, which
contains the observations x ∈X, the sensitive attributes u ∈U and the labels y ∈Y. Our goal is to learn a
classiﬁer that is accurate w.r.t y and fair w.r.t u. We frame the fairness problem as ﬁnding the orthogonal
classiﬁer of an “totally unfair” classiﬁer w1 that only uses the sensitive attributes u for prediction. We
can directly get the unfair classiﬁer w1 from the dataset statistics, i.e., w1(x)y = p(Y = y∣U = u(x)).
Below we show that the orthogonal classiﬁer of unfair classiﬁer meets equalized odds, one metric for
fairness evaluation.
Proposition 4. If the orthogonal random variable of U = u(X) w.r.t pXY exists, then the orthogonal
classiﬁer wx ∖w1 satisﬁes the criterion of equalized odds."
ORTHOGONAL CLASSIFIER FOR FAIRNESS,0.2783300198807157,"We emphasize that, unlike existing algorithms for learning fairness classiﬁer, our method does not require
additional training. We obtain a fair classiﬁer via orthogonalizing an existing model wx which is simply
the vanilla model trained by ERM on the dataset D."
EXPERIMENTS,0.2803180914512923,"6.1
EXPERIMENTS"
EXPERIMENTS,0.2823061630218688,"Setup. We experiment on the UCI Adult dataset, which has gender as the sensitive attribute and the UCI
German credit dataset, which has age as the sensitive attribute (Zemel et al., 2013; Madras et al., 2018;
Song et al., 2019). We compare the orthogonal classiﬁer wx ∖w1 to three baselines: LAFTR (Madras
et al., 2018), which proposes adversarial objective functions that upper bounds the unfairness metrics;
L-MIFR (Song et al., 2019), which uses mutual information objectives to control the expressiveness-
fairness trade-off; ReBias (Bahng et al., 2020), which minimizes the Hilbert-Schmidt Independence
Criterion between the model representations and biased representations; as well as the Vanilla (wx)
trained by ERM. We employ two fairness metrics – demographic parity distance (∆DP) and equalized
odds distance (∆EO) – deﬁned in Madras et al. (2018). We denote the penalty coefﬁcient of the adversarial
or de-biasing objective as γ, whose values govern a trade-off between prediction accuracy and fairness, in
all baselines. We borrow experiment conﬁgurations, such as CNN architecture, from Song et al. (2019).
Please refer to Appendix E.3 for more details."
EXPERIMENTS,0.28429423459244535,"Results. Tables 4, 5 report the test accuracy, ∆DP and ∆EO on Adults and German datasets. We observe
that the orthogonal classiﬁer decreases the unfairness degree of the Vanilla model and has competitive
performances to existing baselines. Especially in the German dataset, compared to LAFTR, our method"
EXPERIMENTS,0.28628230616302186,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.2882703777335984,"has the same ∆EO but better ∆DP and test accuracy. It is surprising that our method outperforms LAFTR,
even though it is not specially designed for fairness. Further, our method has beneﬁt of being training-free,
which allows it to be applied to improve any off-the-shelf classiﬁer’s fairness without additional training."
EXPERIMENTS,0.29025844930417494,Table 4: Accuracy v.s. Fairness (Adults)
EXPERIMENTS,0.2922465208747515,"Acc.↑
∆DP ↓
∆EO ↓"
EXPERIMENTS,0.294234592445328,"Vanilla
84.5
0.19
0.20"
EXPERIMENTS,0.2962226640159046,"LAFTR (γ = 0.1)
84.2
0.14
0.09
LAFTR (γ = 0.5)
84.0
0.12
0.07
L-MIFR (γ = 0.05)
81.6
0.04
0.15
L-MIFR (γ = 0.1)
82.0
0.06
0.16
ReBias (γ = 100)
84.3
0.15
0.11
ReBias (γ = 50)
84.4
0.17
0.16"
EXPERIMENTS,0.2982107355864811,"wx ∖w1
81.6
0.12
0.12"
EXPERIMENTS,0.30019880715705766,Table 5: Accuracy v.s. Fairness (German)
EXPERIMENTS,0.30218687872763417,"Acc.↑
∆DP ↓
∆EO ↓"
EXPERIMENTS,0.30417495029821073,"Vanilla
76.0
0.19
0.33"
EXPERIMENTS,0.3061630218687873,"LAFTR (γ = 0.1)
73.0
0.11
0.17
LAFTR (γ = 0.5)
72.7
0.11
0.19
L-MIFR (γ = 0.1)
75.8
0.10
0.21
L-MIFR (γ = 0.05)
75.6
0.08
0.18
ReBias (γ = 100)
73.0
0.07
0.17
ReBias (γ = 50)
75.0
0.10
0.20"
EXPERIMENTS,0.3081510934393638,"wx ∖w1
75.4
0.09
0.18"
RELATED WORKS,0.3101391650099404,"7
RELATED WORKS"
RELATED WORKS,0.3121272365805169,"Disentangled representation learning Similar to orthogonal random variables, disentangled represen-
tations are also based on speciﬁc notions of feature independence. For example, Higgins et al. (2018)
deﬁnes disentangled representations via equivalence and independence of group transformations, Kim &
Mnih (2018) relates disentanglement to distribution factorization, and Shu et al. (2020) characterizes
disentanglement through generator consistency and a notion of restrictiveness. Our work differs in two
key respects. First, most deﬁnitions of disentanglement rely primarily on the bijection between latents
and inputs (Shu et al., 2020) absent labels. In contrast, our orthogonal features must be conditionally
independent given labels. Further, in our work orthogonal features remain implicit and they are used
discriminatively in predictors. Several approaches aim to learn disentangled representations in an un-
supervised manner (Chen et al., 2016; Higgins et al., 2017; Chen et al., 2018). However, Locatello
et al. (2019) argues that unsupervised disentangled representation learning is impossible without a proper
inductive bias."
RELATED WORKS,0.31411530815109345,"Model de-biasing A line of works focuses on preventing model replying on the dataset biases. Bahng
et al. (2020) learns the de-biased representation by imposing HSIC penalty with biased representation,
and Nam et al. (2020) trains an unbiased model by up-weighting the high loss samples in the biased
model. Li et al. (2021) de-bias training data through data augmentation. However, these works lack
theoretical deﬁnition for the de-biased model in general cases and often require explicit dataset biases."
RELATED WORKS,0.31610337972166996,"Density ratio estimation using a classiﬁer Using a binary classiﬁer for estimating the density ra-
tio (Sugiyama et al., 2012) enjoys widespread attention in machine learning. The density ratio estimated
by classiﬁer has been applied to Monte Carlo inference (Grover et al., 2019; Azadi et al., 2019), class
imbalance (Byrd & Lipton, 2019) and domain adaptation (Azizzadenesheli et al., 2019)."
RELATED WORKS,0.31809145129224653,"Learning a set of diverse classiﬁers Another line of work related to ours is learning a collection of
diverse classiﬁers through imposing penalties that relate to input gradients. Diversity here means that the
classiﬁers rely on different sets of features. To encourage such diversity, Ross et al. (2018; 2020) propose
a notion of local independence, which uses cosine similarity between input gradients of classiﬁers as
the regularizer, while in Teney et al. (2021) the regularizer pertains to dot products. Ross et al. (2017)
sequentially trains multiple classiﬁers to obtain qualitatively different decision boundaries. We defer
more discussions of the advantages of classiﬁer orthogonalization over existing methods to Appendix F."
CONCLUSION,0.32007952286282304,"8
CONCLUSION"
CONCLUSION,0.3220675944333996,"We consider ﬁnding a discriminative direction that is orthogonal to a given principal classiﬁer. The
solution in the linear case is straightforward but does not generalize to the non-linear case. We deﬁne
and investigate orthogonal random variables, and propose a simple but effective algorithm (classiﬁer
orthogonalization) to construct the orthogonal classiﬁer with both theoretical and empirical support.
Empirically, we demonstrate that the orthogonal classiﬁer enables controlled style transfer, improves
existing alignment methods for domain adaptation, and has a lower degree of unfairness."
CONCLUSION,0.3240556660039761,Published as a conference paper at ICLR 2022
CONCLUSION,0.3260437375745527,ACKNOWLEDGEMENTS
CONCLUSION,0.32803180914512925,"The work was partially supported by an MIT-DSTA Singapore project and by an MIT-IBM Grand
Challenge grant. YX was partially supported by the HDTV Grand Alliance Fellowship. We would like
to thank the anonymous reviewers for their valuable feedback."
REFERENCES,0.33001988071570576,REFERENCES
REFERENCES,0.3320079522862823,"Mart´ın Arjovsky, L. Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. ArXiv,
abs/1907.02893, 2019."
REFERENCES,0.33399602385685884,"Samaneh Azadi, Catherine Olsson, Trevor Darrell, Ian J. Goodfellow, and Augustus Odena. Discriminator
rejection sampling. ArXiv, abs/1810.06758, 2019."
REFERENCES,0.3359840954274354,"K. Azizzadenesheli, Anqi Liu, Fanny Yang, and Anima Anandkumar. Regularized learning for domain
adaptation under label shifts. ArXiv, abs/1903.09734, 2019."
REFERENCES,0.3379721669980119,"Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased
representations with biased representations. In ICML, 2020."
REFERENCES,0.3399602385685885,"Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res., 3:463–482, 2001."
REFERENCES,0.341948310139165,"M. Berger, Bernard Gostiaux, and S. Levy. Differential geometry: Manifolds, curves, and surfaces. 1987."
REFERENCES,0.34393638170974156,"G. Blanchard, Gyemin Lee, and C. Scott. Generalizing from several related classiﬁcation tasks to a new
unlabeled sample. In NIPS, 2011."
REFERENCES,0.34592445328031807,"Jonathon Byrd and Zachary Chase Lipton. What is the effect of importance weighting in deep learning?
In ICML, 2019."
REFERENCES,0.34791252485089463,"Tian Qi Chen, Xuechen Li, Roger B. Grosse, and David Kristjanson Duvenaud. Isolating sources of
disentanglement in variational autoencoders. In NeurIPS, 2018."
REFERENCES,0.3499005964214712,"Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and P. Abbeel. Infogan: Inter-
pretable representation learning by information maximizing generative adversarial nets. In NIPS,
2016."
REFERENCES,0.3518886679920477,"R´emi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoffrey J. Gordon. Domain adaptation with
conditional distribution matching and generalized label shift. ArXiv, abs/2003.04475, 2020."
REFERENCES,0.3538767395626243,"Yaroslav Ganin and V. Lempitsky.
Unsupervised domain adaptation by backpropagation.
ArXiv,
abs/1409.7495, 2015."
REFERENCES,0.3558648111332008,"Wei Gao and Zhi-Hua Zhou. Dropout rademacher complexity of deep neural networks. Science China
Information Sciences, 59(7):072104, 2016."
REFERENCES,0.35785288270377735,"Aditya Grover, Jiaming Song, Alekh Agarwal, Kenneth Tran, Ashish Kapoor, E. Horvitz, and S. Er-
mon. Bias correction of learned generative models using likelihood-free importance weighting. In
DGS@ICLR, 2019."
REFERENCES,0.35984095427435386,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and S. Hochreiter. Gans trained
by a two time-scale update rule converge to a local nash equilibrium. In NIPS, 2017."
REFERENCES,0.36182902584493043,"Irina Higgins, Lo¨ıc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained
variational framework. In ICLR, 2017."
REFERENCES,0.36381709741550694,"Irina Higgins, David Amos, David Pfau, S´ebastien Racani`ere, Lo¨ıc Matthey, Danilo Jimenez Rezende,
and Alexander Lerchner. Towards a deﬁnition of disentangled representations. ArXiv, abs/1812.02230,
2018."
REFERENCES,0.3658051689860835,"Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A. Efros, and
Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018."
REFERENCES,0.36779324055666,Published as a conference paper at ICLR 2022
REFERENCES,0.3697813121272366,"Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In ICML, 2018."
REFERENCES,0.3717693836978131,Y. LeCun and Corinna Cortes. The mnist database of handwritten digits. 2005.
REFERENCES,0.37375745526838966,"Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning
for domain generalization. ArXiv, abs/1710.03463, 2018."
REFERENCES,0.3757455268389662,"Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Loddon Yuille, and Cihang
Xie. Shape-texture debiased neural network training. ArXiv, abs/2010.05981, 2021."
REFERENCES,0.37773359840954274,"Z. Liu, Ping Luo, Xiaogang Wang, and X. Tang. Deep learning face attributes in the wild. 2015 IEEE
International Conference on Computer Vision (ICCV), pp. 3730–3738, 2015."
REFERENCES,0.3797216699801193,"Francesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard Sch¨olkopf, and Olivier Bachem.
Challenging common assumptions in the unsupervised learning of disentangled representations. ArXiv,
abs/1811.12359, 2019."
REFERENCES,0.3817097415506958,"Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Conditional adversarial domain
adaptation. In NeurIPS, 2018."
REFERENCES,0.3836978131212724,"David Madras, Elliot Creager, T. Pitassi, and R. Zemel. Learning adversarially fair and transferable
representations. In ICML, 2018."
REFERENCES,0.3856858846918489,"Jun Hyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure:
Training debiased classiﬁer from biased classiﬁer. ArXiv, abs/2007.02561, 2020."
REFERENCES,0.38767395626242546,"Andrew Slavin Ross, Michael C. Hughes, and Finale Doshi-Velez. Right for the right reasons: Training
differentiable models by constraining their explanations. In IJCAI, 2017."
REFERENCES,0.38966202783300197,"Andrew Slavin Ross, Weiwei Pan, and Finale Doshi-Velez. Learning qualitatively diverse and inter-
pretable rules for classiﬁcation. ArXiv, abs/1806.08716, 2018."
REFERENCES,0.39165009940357853,"Andrew Slavin Ross, Weiwei Pan, Leo Anthony Celi, and Finale Doshi-Velez. Ensembles of locally
independent prediction models. In AAAI, 2020."
REFERENCES,0.39363817097415504,"Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls
of simplicity bias in neural networks. ArXiv, abs/2006.07710, 2020."
REFERENCES,0.3956262425447316,"Yuge Shi, Jeffrey S. Seely, Philip H. S. Torr, N. Siddharth, Awni Y. Hannun, Nicolas Usunier, and Gabriel
Synnaeve. Gradient matching for domain generalization. ArXiv, abs/2104.09937, 2021."
REFERENCES,0.3976143141153082,"Rui Shu, H. Bui, H. Narui, and S. Ermon. A dirt-t approach to unsupervised domain adaptation. ArXiv,
abs/1802.08735, 2018."
REFERENCES,0.3996023856858847,"Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised disentan-
glement with guarantees. ArXiv, abs/1910.09772, 2020."
REFERENCES,0.40159045725646125,"Jiaming Song, Pratyusha Kalluri, Aditya Grover, Shengjia Zhao, and S. Ermon. Learning controllable
fair representations. In AISTATS, 2019."
REFERENCES,0.40357852882703776,"Masashi Sugiyama, Taiji Suzuki, and T. Kanamori. Density ratio estimation in machine learning. 2012."
REFERENCES,0.40556660039761433,"Damien Teney, Ehsan Abbasnejad, Simon Lucey, and Anton van den Hengel. Evading the simplicity
bias: Training a diverse set of models discovers solutions with superior ood generalization. ArXiv,
abs/2105.05612, 2021."
REFERENCES,0.40755467196819084,"Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2962–
2971, 2017."
REFERENCES,0.4095427435387674,"Chong Wang, X. Chen, Alex Smola, and E. Xing. Variance reduction for stochastic gradient optimization.
In NIPS, 2013."
REFERENCES,0.4115308151093439,"Yilun Xu and T. Jaakkola. Learning representations that support robust transfer of predictors. ArXiv,
abs/2110.09940, 2021."
REFERENCES,0.4135188866799205,Published as a conference paper at ICLR 2022
REFERENCES,0.415506958250497,"R. Zemel, Ledell Yu Wu, Kevin Swersky, T. Pitassi, and C. Dwork. Learning fair representations. In
ICML, 2013."
REFERENCES,0.41749502982107356,"Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. 2017 IEEE International Conference on Computer Vision
(ICCV), pp. 2242–2251, 2017."
REFERENCES,0.4194831013916501,Published as a conference paper at ICLR 2022
REFERENCES,0.42147117296222664,"A
MORE PROPERTIES OF ORTHOGONAL RANDOM VARIABLE"
REFERENCES,0.4234592445328032,"In this section we demonstrate the properties of orthogonal random variables. We also discuss the
existence and uniqueness of orthogonal classiﬁer when given the principal classiﬁer. Below we list some
useful properties of orthogonal random variables.
Proposition 5. Let Z1 and Z2 be any mutually orthogonal w.r.t PXY , then we have"
REFERENCES,0.4254473161033797,"(i) Invariance: For any diffeomorphism g, g(Z1) and g(Z2) are also mutually orthogonal."
REFERENCES,0.4274353876739563,"(ii) Distribution-free: Z1,Z2 are mutually orthogonal w.r.t PX′Y if and only if there exists a diffeo-
morphism mapping between X and X′."
REFERENCES,0.4294234592445328,"(iii) Existence: Suppose the label-conditioned distributions py ∈C1,supp(py) = X,∀y ∈Y. For z1 ∈C1
and z1(X) is diffeomorphism to Euclidean space, then ∀i ∈Y, there exists a diffeomorphism map-
ping (z1(X),z2,i(X)) such that z1(X) ⊥⊥z2,i(X)∣Y = i. Further if there exists diffeomorphism
mappings between z2,is, then the orthogonal r.v. of z1(X) exists."
REFERENCES,0.43141153081510936,"In the following theorem, we justify the validity of the classiﬁer orthogonalization when z1(x) satisﬁes
some regularity conditions.
Theorem 2. Suppose w1(x)i = p(Y = i∣z1(x)). If the label-condition distributions pys, and z1 satisfy
the conditions in Proposition 5.(iii), then wx ∖w1 is the unique orthogonal classiﬁer of w1."
REFERENCES,0.43339960238568587,"Note that our construction of wx ∖w1 does not require the exact expression of z1(X), its orthogonal r.v
or data generation process. The theorem states that the orthogonal classiﬁer constructed by classiﬁer
orthogonalization is the Bayesian classiﬁer for any orthogonal random variables of z1(X)."
REFERENCES,0.43538767395626243,"B
PROOFS"
REFERENCES,0.43737574552683894,"B.1
PROOF FOR PROPOSITION 1"
REFERENCES,0.4393638170974155,"Proposition 1. Suppose the orthogonal r.v. of z1(X) w.r.t pXY exists and is denoted as z2(X). Then
z(X) = z2(X) is a maximizer of I(z(X);Y ∣z1(X)) subject to I(z(X);z1(X)∣Y ) = 0."
REFERENCES,0.441351888667992,"Proof. For any function z, by chain rule we have I(z1(X),z2(X);Y )
=
I(z1(X);Y ) +
I(z2(X);Y ∣z1(X)) and I(z1(X),z(X);Y ) = I(z1(X);Y ) + I(z(X);Y ∣z1(X)). Further, the deﬁni-
tion of the orthogonal r.v. ensures that there exists a differmorphism f between X and z1(X),z2(X),
which implies I(z1(X),z(f(z1(X),z2(X)));Y ) = I(z1(X),z(X);Y )."
REFERENCES,0.4433399602385686,"Hence by data processing inequality we have:
I(z1(X),z2(X);Y ) ≥I(z1(X),z(f(z1(X),z2(X)));Y ) = I(z1(X),z(X);Y )
⇔I(z1(X);Y ) + I(z2(X);Y ∣z1(X)) ≥I(z1(X);Y ) + I(z(X);Y ∣z1(X))
⇔I(z2(X);Y ∣z1(X)) ≥I(z(X);Y ∣z1(X))"
REFERENCES,0.44532803180914515,"The above inequality shows that z = z2 maximize the mutual information I(z(X);Y ∣z1(X)). Further,
by deﬁnition of the orthogonal r.v., z = z2 is independent of z1 conditioned on Y , i.e. z(X) ⊥⊥z1(X)∣Y
which is equivalent to I(z(X);z1(X)∣Y ) = 0."
REFERENCES,0.44731610337972166,"B.2
PROOF FOR THEOREM 1"
REFERENCES,0.44930417495029823,"Theorem 1. Assume py is uniform distribution, ∀wx ∈W takes values in (m,1 −m)C with m ∈(0, 1"
REFERENCES,0.45129224652087474,"2),
and 1/pX∣Y (x∣y) ∈(0,γ) ⊂(0,+∞) holds for ∀x ∈X,y ∈Y. Then for any δ ∈(0,1) with probability
at least 1 −δ, we have:"
REFERENCES,0.4532803180914513,"∣R( ˆwx ∖w1) −R(w∗
x ∖w1)∣≤(1 + γ)
⎛
⎜
⎝
2R∣D∣(W) + 2log 1 m"
REFERENCES,0.4552683896620278,"¿
Á
Á
À2log 1 δ
∣D∣ ⎞
⎟
⎠"
REFERENCES,0.4572564612326044,Proof. Denote the empirical risk of w as
REFERENCES,0.4592445328031809,ˆR(w) = ˆR(w) = −1
REFERENCES,0.46123260437375746,"∣D∣
∑
(xi,yi)∈D
log w(xi)yi"
REFERENCES,0.46322067594433397,Published as a conference paper at ICLR 2022
REFERENCES,0.46520874751491054,and the population risk as
REFERENCES,0.4671968190854871,"R(w) = −Ep(x,y) [log w(x)y]"
REFERENCES,0.4691848906560636,"Step 1: bounding excess risk ∣R( ˆwx) −R(w∗
x)∣."
REFERENCES,0.4711729622266402,"By the classical result in theorem 8 in Bartlett & Mendelson (2001), we know that for any w ∈W,
δ ∈(0,1), since −log w(x)y ∈(0,log 1"
REFERENCES,0.4731610337972167,"m), with probability at least 1 −δ,"
REFERENCES,0.47514910536779326,∣ˆR(w) −R(w)∣= ∣1 n
REFERENCES,0.47713717693836977,"n
∑
i=1
log(w(xi))yi −Ep[log(w(x)y)]∣≤2Rn(W) + 2log 1 m √"
REFERENCES,0.47912524850894633,"2log 1 δ
n"
REFERENCES,0.48111332007952284,"Since
ˆwx
=
infwx∈W ˆR(wx),w∗
x
=
infwx∈W R(wx), and ∣ˆR( ˆwx) −R( ˆwx)∣
≤
2Rn(W) +"
REFERENCES,0.4831013916500994,2log 1 m √
REFERENCES,0.4850894632206759,2 log 2
REFERENCES,0.4870775347912525,"δ
n
,∣ˆR(w∗
x) −R(w∗
x)∣≤2Rn(W) + 2log 1 m √"
REFERENCES,0.48906560636182905,2 log 1
REFERENCES,0.49105367793240556,"δ
n
, we have"
REFERENCES,0.49304174950298213,"∣R( ˆwx) −R(w∗
x)∣≤2Rn(W) + 2log 1 m √"
REFERENCES,0.49502982107355864,2log 1
REFERENCES,0.4970178926441352,"δ
n
(6)"
REFERENCES,0.4990059642147117,"Step 2: bounding ∣R( ˆwx ∖w1) −R(w∗
x ∖w1)∣by ∣R( ˆwx) −R(w∗
x)∣."
REFERENCES,0.5009940357852882,"R(w ∖w1) = −Ep(x,y)"
REFERENCES,0.5029821073558648,⎡⎢⎢⎢⎢⎢⎣ log
REFERENCES,0.5049701789264414,"w(x)y
w1(x)y"
REFERENCES,0.5069582504970179,"∑y′
w(x)y′
w1(x)y′"
REFERENCES,0.5089463220675944,⎤⎥⎥⎥⎥⎥⎦
REFERENCES,0.510934393638171,"= R(w) + Ep(x,y)"
REFERENCES,0.5129224652087475,"⎡⎢⎢⎢⎢⎣
log w1(x)y∑
y′
w(x)y′
w1(x)y′"
REFERENCES,0.5149105367793241,⎤⎥⎥⎥⎥⎦
REFERENCES,0.5168986083499006,Then we have:
REFERENCES,0.5188866799204771,"∣R( ˆwx ∖w1) −R(w∗
x ∖w1)∣"
REFERENCES,0.5208747514910537,"≤∣R( ˆwx) −R(w∗
x)∣+
RRRRRRRRRRRR
Ep(x,y)"
REFERENCES,0.5228628230616302,"⎡⎢⎢⎢⎢⎣
log w1(x)y∑
y′
ˆwx(x)y′
w1(x)y′"
REFERENCES,0.5248508946322068,"⎤⎥⎥⎥⎥⎦
−Ep(x,y)"
REFERENCES,0.5268389662027833,"⎡⎢⎢⎢⎢⎣
log w1(x)y∑
y′
w∗
x(x)y′
w1(x)y′"
REFERENCES,0.5288270377733598,⎤⎥⎥⎥⎥⎦
REFERENCES,0.5308151093439364,RRRRRRRRRRRR
REFERENCES,0.532803180914513,"= ∣R( ˆwx) −R(w∗
x)∣+"
REFERENCES,0.5347912524850894,RRRRRRRRRRRRRR
REFERENCES,0.536779324055666,"Ep(x,y)"
REFERENCES,0.5387673956262425,⎡⎢⎢⎢⎢⎢⎣
REFERENCES,0.5407554671968191,"log
∑y′
ˆ
wx(x)y′
w1(x)y′"
REFERENCES,0.5427435387673957,"∑y′
w∗x(x)y′
w1(x)y′"
REFERENCES,0.5447316103379721,⎤⎥⎥⎥⎥⎥⎦
REFERENCES,0.5467196819085487,RRRRRRRRRRRRRR
REFERENCES,0.5487077534791253,"≤∣R( ˆwx) −R(w∗
x)∣+ max
⎛
⎜
⎝"
REFERENCES,0.5506958250497018,RRRRRRRRRRRRRR
REFERENCES,0.5526838966202783,"Ep(x,y)"
REFERENCES,0.5546719681908548,⎡⎢⎢⎢⎢⎢⎣
REFERENCES,0.5566600397614314,"log max
y′"
REFERENCES,0.558648111332008,"ˆ
wx(x)y′
w1(x)y′"
REFERENCES,0.5606361829025845,"w∗x(x)y′
w1(x)y′"
REFERENCES,0.562624254473161,⎤⎥⎥⎥⎥⎥⎦
REFERENCES,0.5646123260437376,"RRRRRRRRRRRRRR ,"
REFERENCES,0.5666003976143141,RRRRRRRRRRRRRR
REFERENCES,0.5685884691848907,"Ep(x,y)"
REFERENCES,0.5705765407554672,⎡⎢⎢⎢⎢⎢⎣
REFERENCES,0.5725646123260437,"log min
y′"
REFERENCES,0.5745526838966203,"ˆ
wx(x)y′
w1(x)y′"
REFERENCES,0.5765407554671969,"w∗x(x)y′
w1(x)y′"
REFERENCES,0.5785288270377733,⎤⎥⎥⎥⎥⎥⎦
REFERENCES,0.5805168986083499,"RRRRRRRRRRRRRR ⎞
⎟
⎠"
REFERENCES,0.5825049701789264,"We deﬁne the ratio r(x,y) = ˆ
wx(x)y
w∗x(x)y . We deﬁne y(x) ∶= arg miny r(x,y), ¯y(x) ∶= arg maxy r(x,y)."
REFERENCES,0.584493041749503,"We have r(x,y(x)) ≤∑y′ ˆ
wx(x)y′/w1(x)y′
∑y′ w∗x(x)y′/w1(x)y′ ≤r(x, ¯y(x)). Let assume
1
p(y∣x) ≤γ for all x,y. For any"
REFERENCES,0.5864811133200796,"function y′ ∶X →Y, we have the following bound, where we have importance weight φ(x,y) ∶=
1
p(y∣x)
if y = y′(x) otherwise 0,"
REFERENCES,0.588469184890656,"∣Ex log r(x,y′(x))∣= ∣Ex,yφ(x,y)log r(x,y)∣≤γ∣Ex,y log r(x,y)∣= γ∣R( ˆwx) −R(w∗
x))∣"
REFERENCES,0.5904572564612326,"As a result, we have"
REFERENCES,0.5924453280318092,∣Elog ∑y′ ˆwx(x)y′/w1(x)y′
REFERENCES,0.5944333996023857,"∑y′ w∗x(x)y′/w1(x)y′ ∣≤max(∣Elog r(x,y(x))∣,∣Elog r(x, ¯y(x))∣) ≤γ∣R( ˆwx) −R(w∗
x)∣ Thus"
REFERENCES,0.5964214711729622,"∣R( ˆwx ∖w1) −R(w∗
x ∖w1)∣≤∣R( ˆwx) −R(w∗
x)∣"
REFERENCES,0.5984095427435387,"+ max
⎛
⎜
⎝"
REFERENCES,0.6003976143141153,RRRRRRRRRRRRRR
REFERENCES,0.6023856858846919,"Ep(x,y)"
REFERENCES,0.6043737574552683,⎡⎢⎢⎢⎢⎢⎣
REFERENCES,0.6063618290258449,"log max
y′"
REFERENCES,0.6083499005964215,"ˆ
wx(x)y′
w1(x)y′"
REFERENCES,0.610337972166998,"w∗x(x)y′
w1(x)y′"
REFERENCES,0.6123260437375746,⎤⎥⎥⎥⎥⎥⎦
REFERENCES,0.614314115308151,"RRRRRRRRRRRRRR ,"
REFERENCES,0.6163021868787276,RRRRRRRRRRRRRR
REFERENCES,0.6182902584493042,"Ep(x,y)"
REFERENCES,0.6202783300198808,⎡⎢⎢⎢⎢⎢⎣
REFERENCES,0.6222664015904572,"log min
y′"
REFERENCES,0.6242544731610338,"ˆ
wx(x)y′
w1(x)y′"
REFERENCES,0.6262425447316103,"w∗x(x)y′
w1(x)y′"
REFERENCES,0.6282306163021869,⎤⎥⎥⎥⎥⎥⎦
REFERENCES,0.6302186878727635,"RRRRRRRRRRRRRR ⎞
⎟
⎠"
REFERENCES,0.6322067594433399,"≤(1 + γ)∣R( ˆwx) −R(w∗
x)∣
(7)"
REFERENCES,0.6341948310139165,Published as a conference paper at ICLR 2022
REFERENCES,0.6361829025844931,"Combining Eq. (6) and Eq. (7), we know that with probability 1 −δ, we have:"
REFERENCES,0.6381709741550696,"∣R( ˆwx ∖w1) −R(w∗
x ∖w1)∣≤(1 + γ)
⎛
⎜
⎝
2Rn(W) + 2log 1 m √"
REFERENCES,0.6401590457256461,"2log 1 δ
n ⎞
⎟
⎠"
REFERENCES,0.6421471172962226,"B.3
PROOF FOR PROPOSITION 5 AND THEOREM 2"
REFERENCES,0.6441351888667992,"Proposition 5. Let Z1 and Z2 be any mutually orthogonal w.r.t PXY , then we have"
REFERENCES,0.6461232604373758,"(i) Invariance: For any diffeomorphism g, g(Z1) and g(Z2) are also mutually orthogonal."
REFERENCES,0.6481113320079522,"(ii) Distribution-free: Z1,Z2 are mutually orthogonal w.r.t PX′Y if and only if there exists a diffeo-
morphism mapping between X and X′."
REFERENCES,0.6500994035785288,"(iii) Existence: Suppose the label-conditioned distributions py ∈C1,supp(py) = X,∀y ∈Y. For z1 ∈C1
and z1(X) is diffeomorphism to Euclidean space, then ∀i ∈Y, there exists a diffeomorphism map-
ping (z1(X),z2,i(X)) such that z1(X) ⊥⊥z2,i(X)∣Y = i. Further if there exists diffeomorphism
mappings between z2,is, then the orthogonal r.v. of z1(X) exists."
REFERENCES,0.6520874751491054,"Proof. (i) Since Z1 and Z2 are independent, the independence also holds for g(Z1) and g(Z2). In
addition, we construct the diffeomorphism between (g(Z1),g(Z2)) and X as ˆf(g(Z1),g(Z2)) =
f(g−1(g(Z1)),g−1(g(Z2))) = X. Apparently f(g−1(Z1),g−1(Z2)) is a diffeomorphism."
REFERENCES,0.6540755467196819,"(ii) We denote the diffeomorphism between X′ and X as ˆf. Then the diffeomorphism mapping between
(Z1,Z2) and X is ˆf ○f. Thus Z1,Z2 are mutually orthogonal w.r.t X′."
REFERENCES,0.6560636182902585,(iii) We will use a constructive proof.
REFERENCES,0.658051689860835,"Step 1: Denote dim(X) = d,dim(z1(X)) = k, then we can prove that the manifold[z1(X),Rd−k] is
diffeomorphism of X. We denote the diffeomorphism as f, and denote f(x) = [z1(x),t(x)] where
z1 ∶X →z1(X),t ∶X →Rd−k."
REFERENCES,0.6600397614314115,"Step 2: For X∣Y = y, let Fj(t(x)j ∣t(x)1,⋯,t(x)j−1,z1(x),1) ∶R →[0,1],j ∈{1,...,d −k} be
the CDF corresponding to the distribution of t(x)j ∣t(X)1 = t(x)1,⋯,t(X)j−1 = t(x)j−1,z1(X) =
z1(x),Y = y."
REFERENCES,0.6620278330019881,"h(t(x);z1(x))1 = F1(t(x)1 ∣z1(x),y)
h(t(x);z1(x))i = Fi(t(x)i ∣t(x)1,⋯,t(x)i−1,z1(x),1),i ∈{2,...,d}"
REFERENCES,0.6640159045725647,"The conditions p ∈C1,p(x) > 0,∀x ∈X ensure that the PDF of f(X) is also in C1 and takes values
in (0,∞). Thus the above CDFs are all diffeomorphism mapping. By the inverse CDF theorem, for
any z1(x), h(t(X);z1(X) = z1(x)) is a uniform distribution in (0,1)d−k. Hence the random variable
deﬁned by the mapping h, i.e., z2,y(X) = h(t(X);z1(X)), is independent of z1(X) given Y = 1."
REFERENCES,0.6660039761431411,"In addition, by the construction of z2,y(x) we know that there exists a diffeomorphism ˆf between
(z1(X),t(X)) and (z1(X),z2,y(X)) such that ˆf(z1(X),t(X)) = (z1(X),z2,y(X)). Then f −1 ○
ˆf −1(z1(X),z2,y(X)) = X is also a diffeomorphsim mapping."
REFERENCES,0.6679920477137177,"Step 3: From the condition we know that for every y ∈Y, there exists a diffeomorphism function my such
that my○z2,y(x) = z2,1(x). Apparently ∀y, my○z2,y(X) ⊥⊥z2,1(X)∣Y = y by z2,y(X) ⊥⊥z1(X)∣Y = y.
Further, (z1,z2,1) is a diffeomorphism. Hence z2,1(X) is the orthogonal r.v. of z1(X) w.r.t PXY ."
REFERENCES,0.6699801192842942,"Theorem 2. Suppose w1(x)i = p(Y = i∣z1(x)). If the label-condition distributions pys, and z1 satisfy
the conditions in Proposition 5.(iii), then wx ∖w1 is the unique orthogonal classiﬁer of w1."
REFERENCES,0.6719681908548708,"Proof. By the existence property in Proposition 5, we know that for X, there exists a orthogonal r.v. of
z1(X) and denote it as z2(X1). By the proposition we know that there exists a common diffeomorphism
f mapping (z1(X∣y),z2(X∣y)) to X∣y, ∀y ∈Y."
REFERENCES,0.6739562624254473,Published as a conference paper at ICLR 2022
REFERENCES,0.6759443339960238,"Further, by change of variables and the deﬁnition of orthogonal r.v., we have
pi,2(z2)
pj,2(z2)
="
REFERENCES,0.6779324055666004,"pi,1(z1)pi,2(z2)volJf (z1,z2)
pj,1(z1)pj,2(z2)volJf (z1,z2)/ pi,1(z1)"
REFERENCES,0.679920477137177,"pj,1(z1) =
pi(x)
pj(x)/ pi,1(z1)"
REFERENCES,0.6819085487077535,"pj,1(z1) =
wx(x)i
wx(x)j / w1(x)i"
REFERENCES,0.68389662027833,"w1(x)j . Thus via the bijection of clas-
siﬁer and density ratios we know that wx ∖w1(x)i = p(Y = i∣z2(x))."
REFERENCES,0.6858846918489065,"B.4
PROOF FOR PROPOSITION 2"
REFERENCES,0.6878727634194831,"Proposition 2. The global minimum of LAB
OGAN(GAB) is achieved if and only if ̃PZ1,Z2(z1,z2) =
QZ1(z1)PZ2(z2)."
REFERENCES,0.6898608349900597,"Proof. By deﬁnition, r(x) =
w2(x)
1−w2(x) = P (z2(x))"
REFERENCES,0.6918489065606361,"Q(z2(x)). Thus we can reformulate the criterion LAB
OGAN as the
following:"
REFERENCES,0.6938369781312127,"LAB
OGAN(GAB) = Ex∼̃
P log
̃P(x)
̃P(x) + Q(x)r(x)"
REFERENCES,0.6958250497017893,"= Ez1,z2∼̃
P log
̃P(z1,z2)
̃P(z1,z2) + Q(z1)Q(z2) P (z2) Q(z2)"
REFERENCES,0.6978131212723658,"≥−log Ez1,z2∼̃
P [
̃P(z1,z2) + Q(z1)P(z2)"
REFERENCES,0.6998011928429424,"̃P(z1,z2)
] = −log 2"
REFERENCES,0.7017892644135189,"where we get the lower bound by Jensen’s inequality. The equality holds when ̃P(z1,z2) = Q(z1)P(z2)."
REFERENCES,0.7037773359840954,"B.5
PROOF FOR PROPOSITION 3"
REFERENCES,0.705765407554672,"Proposition 3. Suppose there exists random variable Z2 = z2(X) orthogonal to y(X) = Y w.r.t
p(f(X),U). Then f achieves global optimum if and only if it aligns all label-conditioned feature
distributions, i.e., ps(f(x)∣y) = pt(f(x)∣y),∀y ∈Y."
REFERENCES,0.7077534791252486,"Proof. By deﬁnition, optimal classiﬁer wx(x)0 =
ps(f(x))
ps(f(x))+pt(f(x)), principle classiﬁer w1(x)0 ="
REFERENCES,0.709741550695825,"ps(y(x))
ps(y(x))+pt(y(x)). By the deﬁnition of orthogonal r.v., (Y,U) and f(X) have a bijection between
them. It suggests, conditioned on Y , the supports of f(X)∣Y = y is non-overlapped for different y in
both domains ps and pt. It means if ps(f(x)∣y) > 0 then ∀y′ ≠y, ps(f(x)∣y′) = 0."
REFERENCES,0.7117296222664016,Thus the orthogonal classiﬁer wx ∖w1 satisﬁes:
REFERENCES,0.7137176938369781,(wx ∖w1)(x)0 = ps(f(x))
REFERENCES,0.7157057654075547,ps(y(x)) /(ps(f(x))
REFERENCES,0.7176938369781312,ps(y(x)) + pt(f(x))
REFERENCES,0.7196819085487077,pt(y(x)) )
REFERENCES,0.7216699801192843,= ∑y′ ps(f(x)∣y′)ps(y′)
REFERENCES,0.7236580516898609,"ps(y(x))
/ (∑y′ ps(f(x)∣y′)ps(y′)"
REFERENCES,0.7256461232604374,"ps(y(x))
+ ∑y′ pt(f(x)∣y′)pt(y′)"
REFERENCES,0.7276341948310139,"pt(y(x))
)"
REFERENCES,0.7296222664015904,= ps(f(x)∣y(x))ps(y(x))
REFERENCES,0.731610337972167,"ps(y(x))
/ (ps(f(x)∣y(x))ps(y(x))"
REFERENCES,0.7335984095427436,"ps(y(x))
+ pt(f(x)∣y(x))pt(y(x))"
REFERENCES,0.73558648111332,"pt(y(x))
)"
REFERENCES,0.7375745526838966,"=
ps(f(x)∣y(x))
ps(f(x)∣y(x)) + pt(f(x)∣y(x))
Thus the objective in Eq. (5) can be reformulated as,
L(f,(wx ∖w1)(⋅)0)"
REFERENCES,0.7395626242544732,"=Ex∼ps[log
ps(f(x)∣y(x))
ps(f(x)∣y(x)) + pt(f(x)∣y(x))] + Ex∼pt[log
pt(f(x)∣y(x))
ps(f(x)∣y(x)) + pt(f(x)∣y(x))]"
REFERENCES,0.7415506958250497,=Ey∼psEx∼ps(x∣y)[−log ps(f(x)∣y) + pt(f(x)∣y)
REFERENCES,0.7435387673956262,"ps(f(x)∣y)
] + Ey∼ptEx∼pt(x∣y)[−log ps(f(x)∣y) + pt(f(x)∣y)"
REFERENCES,0.7455268389662028,"pt(f(x)∣y)
]"
REFERENCES,0.7475149105367793,≥−Ey∼ps [log Ex∼ps(x∣y)[ps(f(x)∣y) + pt(f(x)∣y)
REFERENCES,0.7495029821073559,"ps(f(x)∣y)
]] −Ey∼pt [log Ex∼pt(x∣y)[ps(f(x)∣y) + pt(f(x)∣y)"
REFERENCES,0.7514910536779325,"pt(f(x)∣y)
]]"
REFERENCES,0.7534791252485089,= −2log 2.
REFERENCES,0.7554671968190855,Published as a conference paper at ICLR 2022
REFERENCES,0.757455268389662,The lower-bound holds due to Jensen’s inequality. The equality is achieved if and only if ps(f(x)∣y)
REFERENCES,0.7594433399602386,"pt(f(x)∣y) is
invariant to x, for all y, i.e., ∀y ∈Y,ps(f(x)∣y) = pt(f(x)∣y)."
REFERENCES,0.7614314115308151,"B.6
PROOF FOR PROPOSITION 4"
REFERENCES,0.7634194831013916,"Proposition 4. If the orthogonal random variable of U = u(X) w.r.t pXY exists, then the orthogonal
classiﬁer wx ∖w1 satisﬁes the criterion of equalized odds."
REFERENCES,0.7654075546719682,"Proof. We denote the orthogonal random variables as z2(X), and wx ∖w1(x) = Pr[y∣z2(x)]. Since
z2(X) ⊥⊥U∣Y , we know that wx ∖w1(X) ⊥⊥U∣Y . Hence the prediction of the classiﬁer wx ∖w1 is
conditional independent of sensitive attribute U on the ground-truth label Y , which meets the equalized
odds metric."
REFERENCES,0.7673956262425448,"C
EXTRA SAMPLES"
REFERENCES,0.7693836978131213,"Input
Vanilla
Oracle MLDG"
REFERENCES,0.7713717693836978,(a) CMNIST: Domain A
REFERENCES,0.7733598409542743,"Input
Vanilla
Oracle MLDG"
REFERENCES,0.7753479125248509,(b) CMNIST: Domain B
REFERENCES,0.7773359840954275,"Input
Vanilla
Oracle MLDG"
REFERENCES,0.7793240556660039,(c) CelebA-GH: Domain A
REFERENCES,0.7813121272365805,"Input
Vanilla
Oracle MLDG"
REFERENCES,0.7833001988071571,(d) CelebA-GH: Domain B
REFERENCES,0.7852882703777336,Figure 4: CMNIST and CelebA-GH
REFERENCES,0.7872763419483101,"D
EXTRA EXPERIMENTS"
REFERENCES,0.7892644135188867,"D.1
STYLE TRANSFER"
REFERENCES,0.7912524850894632,Table 6: CelebA
REFERENCES,0.7932405566600398,"Z1 acc.
Z2 acc.
FID ↓"
REFERENCES,0.7952286282306164,"Vanilla
75.3
87.0
36.2
wx ∖w1-MLDG
81.8
93.5
35.2
wx ∖w1-TRM
76.3
92.5
33.5
wx ∖w1-Fish
74.2
93.3
33.1"
REFERENCES,0.7972166998011928,"IS-Oracle
73.3
88.1
36.5
wx ∖w1-Oracle
84.0
95.2
38.5"
REFERENCES,0.7992047713717694,We show transferred samples for both domains in Fig. 4 on CMNIST and CelebA-GH.
REFERENCES,0.8011928429423459,Published as a conference paper at ICLR 2022
REFERENCES,0.8031809145129225,"D.2
STYLE TRANSFER ON CELEBA"
REFERENCES,0.805168986083499,"Unlike the split in CelebA-GH dataset, domain A is males, and domain B is females. The two domains
together form the full CelebA (Liu et al., 2015) with 202599 images. Note that there exists large
imbalance of hair colors within the male group, with only around 2% males having blond hair. Table 6
reports the Z1/Z2 accuracies and FID scores on the full CelebA datasets. We ﬁnd that our method
improves the style transfer on the original CelebA dataset, especially the Z2 accuracy. It shows that our
method can help style transfer in real-world datasets with multiple varying factors."
REFERENCES,0.8071570576540755,"E
EXTRA EXPERIMENT DETAILS"
REFERENCES,0.8091451292246521,"E.1
STYLE TRANSFER"
REFERENCES,0.8111332007952287,"E.1.1
DATASET DETAILS"
REFERENCES,0.8131212723658051,"We randomly sampled two digits colors and two background colors for CMNIST. For CelebA-GH, we
extract two subsets—male with non-blond hair and female with blond hair—by the attributes provided in
CelebA dataset. For all datasets, we use 0.8/0.2 proportions to split the train/test set."
REFERENCES,0.8151093439363817,"For CelebA dataset, following the implementation of Zhu et al. (2017) (https://github.com/
junyanz/CycleGAN), we use 128-layer UNet as the generator and the discriminator in PatchGAN.
For CMNIST dataset, we use a 6-layer CNN as the generator and a 5-layer CNN as the discriminator.
We adopt Adam with learning rate 2e-4 as the optimizer and batch size 128/32 for CMNIST/CelebA."
REFERENCES,0.8170974155069582,"E.1.2
DETAILS OF MODELS"
REFERENCES,0.8190854870775348,"We craft datasets for training the oracle w1(x) = Pr(Y ∣z1(x)). For CMNIST, oracle dataset has bias
degrees 0.9/0. For CelebA-GH, oracle dataset has {male, female} as classes and the hair colors under
each class are balanced."
REFERENCES,0.8210735586481114,"For each dataset, we construct two environments to train the domain generalization models. For CMNIST,
the two environments are datasets with bias degrees 0.9/0.4 and 0.9/0.8. For CelebA-GH, the two
environments consist of different groups of equal size: one is {non-blond-haired males, blond-haired
females} and the other is {non-blond-haired males, blond-haired females, non-blond-haired females,
blond-haired males}. Note that the facial characteristic of gender (Z1) is the invariant prediction
mechanism across environments, instead of the hair color."
REFERENCES,0.8230616302186878,"E.1.3
EVALUATION FOR Z1/Z2 ACCURACY"
REFERENCES,0.8250497017892644,"To evaluate the accuracies on Z1,
Z2 latents,
we train two oracle classiﬁers w∗
1(x)
=
Pr(Y ∣z1(x)),w∗
2(x) = Pr(Y ∣z2(x)) on two designed datasets. Speciﬁcally, w1/w2 are trained on
datasets D1/D2 with Z1/Z2 as the only discriminative direction. For CMNIST, D1 is the dataset with
bias degrees 0.9/0 and D2 is the dataset with bias degrees 0/0.8. For CelebA-GH, D1 is the dataset
with {male, female} as classes and the hair colors under each class are balanced. D2 is the dataset with
{blond hair, non-blond hair} as classes and the males/females under each class are balanced."
REFERENCES,0.827037773359841,The Z1 accuracy on dataset D for transfer model G is:
REFERENCES,0.8290258449304175,Z1 accuracy = 1
REFERENCES,0.831013916500994,"∣D∣∑
x∈D
I(arg max
y
w∗
1(x) /= arg max
y
w∗
1(G(x)))"
REFERENCES,0.8330019880715706,And Z2 accuracy is:
REFERENCES,0.8349900596421471,Z1 accuracy = 1
REFERENCES,0.8369781312127237,"∣D∣∑
x∈D
I(arg max
y
w∗
2(x) = arg max
y
w∗
2(G(x)))"
REFERENCES,0.8389662027833003,"where I is the indicator function. Z1 accuracy measures the success rate of transferring the Z1 and Z2
accuracy measures the success rate of keeping Z2 aspects unchanged."
REFERENCES,0.8409542743538767,"E.2
DOMAIN ADAPTATION"
REFERENCES,0.8429423459244533,"The full names for the seven datasets are MNIST, MNIST-M, SVHN, GTSRB, SYN-DIGITS (DIGITS),
SYN-SIGNS (SIGNS), CIFAR-10 (CIFAR) and STL-10 (STL). We sub-sample each dataset by the
designated imbalance ratio to construct the pairs."
REFERENCES,0.8449304174950298,Published as a conference paper at ICLR 2022
REFERENCES,0.8469184890656064,"We use the 18-layer neural network in Shu et al. (2018) with pre-process via instance-normalization.
Following Shu et al. (2018), smaller CNN is used for digits (MNIST, MNIST-M, SVHN, DIGITS) and
trafﬁc signs (SIGNS, GTSRB) and larger CNN is used for CIFAR-10 and STL-10. We use the Adam
optimizer and hyper-parameters in Appendix B in Shu et al. (2018) except for MNIST →MNIST-M,
where we ﬁnd that setting λs = 1,λd = 1e −1 largely improves the VADA performance over label
shifts. Furthermore, we set λd = 1e −2, i.e., the default value suggested by Shu et al. (2018), to enable
adversarial feature alignment."
REFERENCES,0.8489065606361829,"E.3
FAIRNESS"
REFERENCES,0.8508946322067594,"The two datasets are: the UCI Adult dataset2 which has gender as the sensitive attribute; the UCI German
credit dataset3 which has age as the sensitive attribute. We borrow the encoder, decoder and the ﬁnal fc
layer from Song et al. (2019). For fair comparison, we use the same encoder+fc layer as the function
family W of our classiﬁer since our method do not need decoder for reconstruction. We use Adam with
learning rate 1e-3 as the optimizer, and a batch size of 64."
REFERENCES,0.852882703777336,"The original ReBias method trains a set of biased models to learn the de-biased representations (Bahng
et al., 2020), such as designing CNNs with small receptive ﬁelds to capture the texture bias. However, in
the fairness experiments, the bias is explicitly given, i.e., the sensitive attribute. For fair comparison, we
set the HSIC between the sensitive attribute and the representations as the de-bias regularizer."
REFERENCES,0.8548707753479126,"F
LEARNING A DIVERSE SET OF CLASSIFIERS"
REFERENCES,0.856858846918489,"In this section we discuss the difference and advantages of the classiﬁer orthogonalization, compared
to methods that learn a diverse set of classiﬁers. We also provide a counter-example to show that the
orthogonal input gradient does not lead to a pair of orthogonal classiﬁers. We show that orthogonal
classiﬁers might not in optimal solution set of minimizing input gradient penalty."
REFERENCES,0.8588469184890656,"The goals of learning diverse classiﬁers include interpretability (Ross et al., 2017), overcoming simplicity
bias (Teney et al., 2021), improving ensemble performance (Ross et al., 2020), and recovering confound-
ing decision rules (Ross et al., 2018). There is a direct trade-off between diversity and accuracy and
this is controlled by the regularization parameter. In contrast, in our work, given the principal classiﬁer,
our method directly constructs a classiﬁer that uses only the orthogonal variables for prediction. There
is no trade-off to set in this sense. We also focus on novel applications of controlling the orthogonal
directions, such as style transfer, domain adaptation and fairness. Further, our notion of orthogonality is
deﬁned via latent orthogonal variables that relate to inputs via a diffeomorphism (Deﬁnition 1) as opposed
to directly in the input space. Although learning diverse classiﬁers through input gradient penalties
is efﬁcient, it does not guarantee orthogonality in the sense we deﬁne it. We provide an illustrative
counter-example in Appendix F, where the input space is not disentangled nor has orthogonal input
gradients but corresponds to a pair of orthogonal classiﬁers in our sense. We also prove that, given the
principal classiﬁer, minimizing the loss by introducing an input gradient penalty would not necessarily
lead to an orthogonal classiﬁer."
REFERENCES,0.8608349900596421,"We note that one clear limitation of our classiﬁer orthogonalization procedure is that we need access to
the full classiﬁer. Learning this full classiﬁer can be impacted by simplicity bias (Shah et al., 2020) that
could prevent it from relying on all the relevant features. We can address this limitation by leveraging
previous efforts (Ross et al., 2020; Teney et al., 2021) to mitigate simplicity bias when training the full
classiﬁer."
REFERENCES,0.8628230616302187,"F.1
RELATIONSHIP TO ORTHOGONAL INPUT GRADIENTS"
REFERENCES,0.8648111332007953,"The orthogonality constraints on input gradients demonstrate good performance in learning a set of
diverse classiﬁers (Ross et al., 2017; 2018; 2020; Teney et al., 2021). They typically use the dot product
of input gradients between pairs of classiﬁers as the regularizer. We highlight that when facing the latent
orthogonal random variables, the orthogonal gradient constraints on input space no longer guarantees to
learn the orthogonal classiﬁer. We demonstrate it on a simple non-linear example below."
REFERENCES,0.8667992047713717,"Consider a binary classiﬁcation problem with the following data generating process. Label Y is uniformly
distributions in {−1,1}. Conditioned on the label, we have two independent k-dimensional latents"
REFERENCES,0.8687872763419483,"2https://archive.ics.uci.edu/ml/datasets/adult
3https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29"
REFERENCES,0.8707753479125249,Published as a conference paper at ICLR 2022
REFERENCES,0.8727634194831014,"Z1,Z2 such that Z1∣Y = y ∼N(yµ1,Ik), Z2∣Y = y ∼N(yµ2,Ik). µ1 ∈Rk,µ2 ∈Rm are the
means of the latent variables, and k > m. The data X is generated from latents via the following"
REFERENCES,0.8747514910536779,"diffeomorphism X = ⎛
⎝
g(Z1 −(
Z2
0(k−m))) Z2"
REFERENCES,0.8767395626242545,"⎞
⎠where g is an arbitrary non-linear diffeomorphism on Rk."
REFERENCES,0.878727634194831,"We can see that Z1,Z2 are mutually orthogonal w.r.t the distribution pXY . Now consider the Bayes
optimal classiﬁers w1,w2 using variable Z1,Z2. We have w1(x) = pY ∣Z1(1∣z1) = σ(2µT
1 z1) and
w2(x) = pY ∣Z2(1∣z2) = σ(2µT
2 z2) , where σ is the sigmoid function. Apparently, they are a pair of
orthogonal classiﬁers. Besides, we denote the classiﬁer based on the ﬁrst m dimensions in z1 for
prediction as w3, i.e., w3(x) = pY ∣(Z1)∶m(1∣(z1)∶m) = σ(2(µ1)T
∶m(z1)∶m)."
REFERENCES,0.8807157057654076,"Given the principal classiﬁer w1, we consider using the loss function in Ross et al. (2018; 2020) to learn
the orthogonal classiﬁer of w1, by adding a input gradient penalty term to the standard classiﬁcation loss:"
REFERENCES,0.882703777335984,"Lw1(w) = Lc(w) + λE[cos2(∇xw1(x),∇xw(x))]"
REFERENCES,0.8846918489065606,"Lc is the expected cross-entropy loss, cos is the cosine-similarity and λ is the hyper-parameter for the
gradient penalty term (Ross et al., 2018; 2020). Below we show that (1) the orthogonal classiﬁer w2
does not necessarily satisfy E[cos2(∇xw1(x),∇xw2(x))] = 0. (2) the orthogonal classiﬁer w2 is not
necessarily the minimizer of Lw1(w).
Proposition 6. For the above (Z1,Z2), their corresponding Bayes classiﬁers w1,w2 satisfy
E[∇xw1(x)T ∇xw2(x)] = 0 if and only if (µ1)T
∶mµ2 = 0."
REFERENCES,0.8866799204771372,"Proof. Let X1 = g(Z1 −(
Z2
0(k−m))), X2 = Z2 are the two components of X. Note that z1 = g−1(x1) +"
REFERENCES,0.8886679920477137,"(
x2
0(k−m)). By chain rule, the input gradient of w1 is"
REFERENCES,0.8906560636182903,∇xw1(x) = dz1
REFERENCES,0.8926441351888668,dx ∇z1 Pr(Y = 1∣Z1 = z1(x))
REFERENCES,0.8946322067594433,"= (∇x1g−1(x1)
Im×k
)
(k+m)×k"
REFERENCES,0.8966202783300199,"e−2µT
1 z1(x)"
REFERENCES,0.8986083499005965,"(1 + e−2µT
1 z1(x))2 ⋅2µ1"
REFERENCES,0.9005964214711729,"where Im×k is the submatrix of Ik×k. Similarly we get ∇xw2(x) = (
0
Im×m)
(k+m)×m"
REFERENCES,0.9025844930417495,"e−2µT
2 z2(x)"
REFERENCES,0.904572564612326,"(1+e−2µT
2 z2(x))2 ⋅2µ2."
REFERENCES,0.9065606361829026,"Together, the dot product between input gradients is"
REFERENCES,0.9085487077534792,"∇xw1(x)T ∇xw2(x) =
2e−2µT
1 z1(x)µT
1
(1 + e−2µT
1 z1(x))2 (∇x1g−1(x1)
Im×k
) T"
REFERENCES,0.9105367793240556,"(k+m)×k
(
0
Im×m)
(k+m)×m"
REFERENCES,0.9125248508946322,"2e−2µT
2 z2(x)µ2
(1 + e−2µT
2 z2(x))2"
REFERENCES,0.9145129224652088,"=
4e−2µT
1 z1(x)−2µT
2 z2(x)(µ1)T
∶mµ2
(1 + e−2µT
1 z1(x))2(1 + e−2µT
2 z2(x))2"
REFERENCES,0.9165009940357853,"Since
4e−2µT
1 z1(x)−2µT
2 z2(x)"
REFERENCES,0.9184890656063618,"(1+e−2µT
1 z1(x))2(1+e−2µT
2 z2(x))2 > 0, E[∇xw1(x)T ∇xw2(x)] if and only if (µ1)T
∶mµ2 = 0."
REFERENCES,0.9204771371769384,"Proposition 6 ﬁrst shows that the expected input gradient product of the two Bayes classiﬁers of underlying
latents is non-zero, unless the linear decision boundaries on z1,z2 are orthogonal. Hence, given the
principal classiﬁer w1, the expected dot product of input gradients between w1 and its orthogonal
classiﬁer w2 does not have to be zero.
Proposition 7. The orthogonal classifer w2 does not minimize Lw1(w) if (µ1)∶m = µ2 and
µT
1 ∇x1g−1(x1)T ∇x1g−1(x1)k×m(µ1)∶m < 0. Particularly, we show that Lw1(w3) < Lw1(w2) in this
case."
REFERENCES,0.9224652087475149,Proof. Let w3(x) = pY ∣(Z1)∶m(1∣(z1)∶m). The input gradient of w3(x) is
REFERENCES,0.9244532803180915,∇xw3(x) = (∇xw1(x))∶m = dz1
REFERENCES,0.9264413518886679,dx ∇(z1)∶m Pr(Y = 1∣(Z1)∶m = (z1(x))∶m)
REFERENCES,0.9284294234592445,"= ((∇x1g−1(x1))k×m
Im×m
)
(k+m)×m"
REFERENCES,0.9304174950298211,"e−2(µ1)T
∶mz1(x)∶m"
REFERENCES,0.9324055666003976,"(1 + e−2(µ1)T
∶mz1(x)∶m)2 ⋅2(µ1)∶m"
REFERENCES,0.9343936381709742,Published as a conference paper at ICLR 2022
REFERENCES,0.9363817097415507,"When (µ1)∶m = µ2, the dot-product between ∇xw3(x),∇xw1(x) is"
REFERENCES,0.9383697813121272,"∇xw1(x)T ∇xw3(x) =
2e−2µT
1 z1(x)µT
1
(1 + e−2µT
1 z1(x))2 (∇x1g−1(x1)
Im×k
) T"
REFERENCES,0.9403578528827038,(k+m)×k
REFERENCES,0.9423459244532804,"((∇x1g−1(x1))k×m
Im×m
)
(k+m)×m"
REFERENCES,0.9443339960238568,"e−2(µ1)T
∶mz1(x)∶m"
REFERENCES,0.9463220675944334,"(1 + e−2(µ1)T
∶mz1(x)∶m)2 ⋅2(µ1)∶m"
REFERENCES,0.94831013916501,"=
4e−2µT
1 z1(x)−2(µ1)T
∶mz1(x)∶m"
REFERENCES,0.9502982107355865,"(1 + e−2µT
1 z1(x))2(1 + e−2(µ1)T
∶mz1(x)∶m)2 (∥µ2 ∥2
2 +µT
1 ∇x1g−1(x1)T ∇x1g−1(x1)k×m(µ1)∶m)"
REFERENCES,0.952286282306163,"When (µ1)∶m
=
µ2, we know that z1(x),z2(x) are equally predictive of the label and
thus Lc(w3) = Lc(w2).
Note that if µT
1 ∇x1g−1(x1)T ∇x1g−1(x1)k×m(µ1)∶m < 0, we have
E[cos2(∇xw1(x),∇xw2(x))] > E[cos2(∇xw1(x),∇xw3(x))]:"
REFERENCES,0.9542743538767395,"E[cos2(∇xw1(x),∇xw2(x))] = E"
REFERENCES,0.9562624254473161,⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣
REFERENCES,0.9582504970178927,"⎛
⎜⎜⎜⎜
⎝"
REFERENCES,0.9602385685884692,"∥µ2 ∥2
2"
REFERENCES,0.9622266401590457,"∥(∇x1g−1(x1)
I
)
(k+m)×k
µ1 ∥2∥(0
I)
(k+m)×m
µ2 ∥2"
REFERENCES,0.9642147117296223,"⎞
⎟⎟⎟⎟
⎠"
REFERENCES,0.9662027833001988,2⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦ > E
REFERENCES,0.9681908548707754,⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣
REFERENCES,0.9701789264413518,"⎛
⎜⎜⎜⎜
⎝"
REFERENCES,0.9721669980119284,"(∥µ2 ∥2
2 +µT
1 ∇x1g−1(x1)T (∇x1g−1(x1))k×mµ2)"
REFERENCES,0.974155069582505,"∥(∇x1g−1(x1)
I
)
(k+m)×k
µ1 ∥2∥(∇x1g−1(x1)
Im×k
)
(k+m)×k
µ2 ∥2"
REFERENCES,0.9761431411530815,"⎞
⎟⎟⎟⎟
⎠"
REFERENCES,0.9781312127236581,"2⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦
= E[cos2(∇xw1(x),∇xw3(x))]"
REFERENCES,0.9801192842942346,"The strict inequality holds by µT
1 ∇x1g−1(x1)T ∇x1g−1(x1)k×m(µ1)∶m < 0"
REFERENCES,0.9821073558648111,"(∇x1g−1(x1)
Im×k
)
(k+m)×k
µ2 ∥2 =∥(∇x1g−1(x1)
0
)
(k+m)×k
µ2 + ( 0
Im×k)
(k+m)×k
µ2 ∥2"
REFERENCES,0.9840954274353877,"= (∥(∇x1g−1(x1)
0
)
(k+m)×k
µ2 ∥2
2 + ∥( 0
Im×k)
(k+m)×k
µ2 ∥2
2) 1
2"
REFERENCES,0.9860834990059643,"≥∥( 0
Im×k)
(k+m)×k
µ2 ∥2"
REFERENCES,0.9880715705765407,"To verify that µT
1 ∇x1g−1(x1)T ∇x1g−1(x1)k×m(µ1)∶m < 0 does exist, pick k = 3,m = 2, g−1(x) =
⎛
⎝"
REFERENCES,0.9900596421471173,"2
−3
3
1
−1
5
1
−1
1"
REFERENCES,0.9920477137176938,"⎞
⎠x, and µ1 = ⎛
⎝ 1
1
1"
REFERENCES,0.9940357852882704,"⎞
⎠. We have µT
1 ∇x1g−1(x1)T ∇x1g−1(x1)k×m(µ1)∶m = −2 in this case."
REFERENCES,0.9960238568588469,"Together, we have Lw1(w3) < Lw1(w2)."
REFERENCES,0.9980119284294234,"Proposition 7 shows that the orthogonal classiﬁer w2 is not the minimizer of the loss with input gradient
penalty. The results above also hold for un-normalized version of input gradient penalty in Teney et al.
(2021) by similar analysis."
