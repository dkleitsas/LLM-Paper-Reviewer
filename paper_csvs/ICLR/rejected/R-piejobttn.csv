Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.006329113924050633,"Latent representations help unravel complex phenomena. While continuous la-
tent variables can be efﬁciently inferred, ﬁtting mixed discrete-continuous models
remains challenging despite recent progress, especially when the discrete factor
dimensionality is large. A pressing application for such mixture representations is
the analysis of single-cell omic datasets to understand neuronal diversity and its
molecular underpinnings. Here, we propose an unsupervised variational framework
using multiple interacting networks called cpl-mixVAE that signiﬁcantly outper-
forms state-of-the-art in high-dimensional discrete settings. cpl-mixVAE introduces
a consensus constraint on discrete factors of variability across the networks, which
regularizes the mixture representations at the time of training. We justify the
use of this framework with theoretical results and validate it with experiments on
benchmark datasets. We demonstrate that our approach discovers interpretable
discrete and continuous variables describing neuronal identity in two single-cell
RNA sequencing datasets, each proﬁling over a hundred cortical neuron types."
INTRODUCTION,0.012658227848101266,"1
INTRODUCTION"
INTRODUCTION,0.0189873417721519,"Fitting mixed discrete-continuous models arises in many contexts. While continuous latent variables
can be efﬁciently inferred with variational and adversarial formulations, inference of continuous and
discrete factors in generalized mixture models remains challenging despite recent progress (Jang
et al., 2016; Chen et al., 2016; Dupont, 2018; Jeong & Song, 2019). A pressing domain of application
for such models is quantifying factors of biological variability in single-cell omic studies. The
high-throughput and high-dimensional datasets produced by these studies document a previously
unappreciated diversity of gene expression. In neuroscience, this poses the identiﬁcation of cell types
and cell states as a key research area to understand how neuronal circuits function (Bargmann et al.,
2014), where the notions of cell types and states can be considered as biological interpretations of
discrete and continuous variability. While marker gene based studies suggest the existence of more
than 100 neuronal cell types in just a single brain region, there is no agreement on such categorization
and interpretation of the remaining continuous variability (Seung & Sümbül, 2014; Zeng & Sanes,
2017; Tasic et al., 2018). Moreover, existing unsupervised, joint continuous-discrete learning methods
are tailored for problems with relatively few and equally-abundant discrete components, and accurate
inference remains out of reach for these applications."
INTRODUCTION,0.02531645569620253,"Deep generative models have previously been applied to single-cell datasets, where the focus is on
the cluster identity and it is typically inferred by post-hoc analysis of a continuous factor (Lopez
et al., 2018). Deep Gaussian mixture models (Dilokthanakul et al., 2016; Johnson et al., 2016; Jiang
et al., 2017) also focus on the identiﬁcation of categories and do not take interpretability of the
remaining continuous variability into account. To address the need for joint inference of interpretable
discrete and continuous factors, various adversarial and variational methods have been proposed.
While existing adversarial generative models, e.g. InfoGAN (Chen et al., 2016), are susceptible to
stability issues (Higgins et al., 2017; Kim & Mnih, 2018), variational autoencoders (VAEs) (Kingma
& Welling, 2013) emerge as efﬁcient and more stable alternatives (Tschannen et al., 2018; Zhang et al.,
2018; Dupont, 2018; Jeong & Song, 2019). VAE-based approaches approximate the mixture model
by assuming a family of distributions qφ and select the member closest to the true model p. Popular
choices in VAE implementations include (1) using KL divergence to compute discrepancy between
qφ and p, and (2) using a multivariate Gaussian mixture distribution with uniformly distributed
discrete and isotropic Gaussian distributed continuous priors. However, such choices may lead to"
INTRODUCTION,0.03164556962025317,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0379746835443038,"underestimating the posterior variance (Minka et al., 2005; Blei et al., 2017). Solutions to resolve this
issue are mainly applicable in low-dimensional spaces or for continuous factors alone (Deasy et al.,
2020; Kingma et al., 2016; Ranganath et al., 2016; Quiroz et al., 2018)."
INTRODUCTION,0.04430379746835443,"Inspired by collective decision making, we introduce a variational framework using multiple autoen-
coding arms to jointly infer interpretable ﬁnite discrete (categorical) and continuous factors in the
presence of high-dimensional discrete space. Coupled-autoencoders have been previously studied in
the context of multi-modal recordings, where each arm learns only a continuous latent representation
for one of the data modalities (Feng et al., 2014; Gala et al., 2019; Lee & Pavlovic, 2020). Here, we
develop a novel pairwise-coupled autoencoder framework for a single data modality. The proposed
framework imposes a consensus constraint on the categorical posterior at the time of training and
allows dependencies between continuous and categorical factors. We deﬁne the consensus constraint
based on the Aitchison geometry in the probability simplex, which avoids the mode collapse problem.
We show that the coupled multi-arm architecture enhances accuracy, robustness, and interpretability
of the inferred factors without requiring any priors on the relative abundances of categories. Finally,
on datasets proﬁling different cortical regions in the mammalian brain, we show that our method
can be used to discover neuronal types as discrete categories and type-speciﬁc genes regulating the
continuous within-type variability, such as metabolic state or disease state."
INTRODUCTION,0.05063291139240506,"Related work. There is an extensive body of research on clustering in mixture models (Dilokthanakul
et al., 2016; Jiang et al., 2017; Tian et al., 2017; Guo et al., 2016; Locatello et al., 2018b). The idea of
improving the clustering performance through seeking a consensus and co-training and ensembling
across multiple observations has been explored in both unsupervised (Monti et al., 2003; Kumar &
Daumé, 2011) and semi-supervised contexts (Blum & Mitchell, 1998). However, these methods do
not consider the underlying continuous variabilities across observations. Moreover, unlike ensemble
methods, which pool the results of different trained workers, autoencoding arms seek a consensus at
the time of learning in our framework."
INTRODUCTION,0.056962025316455694,"The proposed framework does not need any supervision since the individual arms provide a form
of prior or weak supervision for each other. In this regard, our paper is related to a body of
work that attempts to improve representation learning by using semi-supervised or group-based
settings (Bouchacourt et al., 2017; Hosoya, 2019; Nemeth, 2020). Bouchacourt et al. (2017) demon-
strated a multi-level variational autoencoder (MLVAE) as a semi-supervised VAE by revealing that
observations within groups share the same type. Hosoya (2019) and Nemeth (2020) attempted to
improve MLVAE by imposing a weaker condition to the grouped data. In recent studies (Shu et al.,
2019; Locatello et al., 2020), a weakly supervised variational setting has been proposed for disen-
tangled representation learning by providing pairs of observations that share at least one underlying
factor. These studies rely on learning latent variables in continuous spaces, and have been applied
only to image datasets with low-dimensional latent representations."
INTRODUCTION,0.06329113924050633,"Recent advances in structured variational methods, such as imposing a prior (Ranganath et al., 2016)
or spatio-temporal dependencies (Quiroz et al., 2018) on the latent distribution parameters, allow
for scaling to larger dimensions. However, these solutions are not directly applicable to the discrete
space, which will be addressed in our A-arm VAE framework.
2
SINGLE MIXTURE VAE FRAMEWORK"
INTRODUCTION,0.06962025316455696,"For an observation x ∈RD, a VAE learns a generative model pθ (x|z) and a variational distribution
qφ (z|x), where z ∈RM is a latent variable with a parameterized distribution p(z) and M ≪
D (Kingma & Welling, 2013). Disentangling different sources of variability into different dimensions
of z enables an interpretable selection of latent factors (Higgins et al., 2017; Locatello et al., 2018a).
However, the interplay between continuous and discrete variabilities present in many real-world
datasets is often overlooked by existing methods. This problem can be addressed within the VAE
framework in an unsupervised fashion by introducing a categorical latent variable c denoting the
class label, alongside the continuous latent variable s. We refer to the continuous variable s as the
state or style variable interchangeably. Assuming s and c are independent random variables, the
evidence lower bound (ELBO) (Blei et al., 2017) for a single mixture VAE with the distributions
parameterized by θ and φ is given by,"
INTRODUCTION,0.0759493670886076,"L(φ, θ)
=
Eqφ(s,c|x) [log pθ(x|s, c)] −DKL (qφ(s|x)∥p(s)) −DKL (qφ(c|x)∥p(c)) .
(1)"
INTRODUCTION,0.08227848101265822,"Maximizing ELBO in Eq. 1 imposes characteristics on q(s|x) and q(c|x) that can result in underesti-
mation of posterior probabilities such as the mode collapse problem, where the network ignores a"
INTRODUCTION,0.08860759493670886,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0949367088607595,"Figure 1: (a) Multi-arm autoencoder framework pro-
posed as the cpl-mixVAE model. Individual arms
receive non-identical noisy copies of given samples
x, i.e. {xa, xb, . . . }, where they all belong to the
same category, to learn mixture representations, i.e.
{q(ca, sa), q(cb, sb), . . . }. VAE arms cooperate to
learn the categorical assignment, p(c). (b) Graphical
model of each autoencoder to learn type dependence
of the state variable."
INTRODUCTION,0.10126582278481013,"subset of latent variables (Minka et al., 2005; Blei et al., 2017). Recently, VAE-based solutions were
proposed by imposing a uniform structure on p(c): akin to β-VAE (Higgins et al., 2017; Burgess et al.,
2018), JointVAE (Dupont, 2018) modiﬁes the ELBO by assigning a pair of controlled information
capacities for each variational factor, i.e. Cs ∈R|s| and Cc ∈R|c|. The main drawback of JointVAE is
that its performance is tied to heuristic tuning of |s|×|c| capacities over training iterations so that it is
vulnerable to mode collapse in high-dimensional settings. Another recent VAE-based mixture model
solution, CascadeVAE (Jeong & Song, 2019), maximizes the ELBO through a semi-gradient-based
algorithm by iterating over two separate optimizations for the continuous and categorical variables.
While the separation of the optimization steps avoids the mode collapse problem, this separation is
valid only when the categorical variable is uniformly distributed. Therefore, its performance strongly
depends on the clusters having similar abundances in the dataset. Thus, earlier solutions fall short of
learning interpretable mixture representations with high-dimensional discrete variables in real-world
applications."
INTRODUCTION,0.10759493670886076,"In addition to the issues discussed above, the performance and interpretability of those approaches
are further limited by the common assumption that the continuous variable representing the style of
the data is independent of the categorical variable. In practice, style often depends on the class label.
For instance, even for the well-studied MNIST dataset, the histograms of common digit styles, e.g.
“width”, markedly vary for different digits (Supplementary Section I). Moreover, further analysis of
the identiﬁed continuous factor in the earlier approaches reveals that the independence assumption
among q(s|x) and q(c|x) can be signiﬁcantly violated (see Supplementary Sections H and I)."
COUPLED MIXTURE VAE FRAMEWORK,0.11392405063291139,"3
COUPLED MIXTURE VAE FRAMEWORK"
COUPLED MIXTURE VAE FRAMEWORK,0.12025316455696203,"The key intuition behind multi-arm networks is cooperation to improve posterior estimation. While
the context is different, the popular phrase “wisdom of the crowd” (Surowiecki, 2005) can nevertheless
be revealing: when a crowd (multiple arms) needs to make a decision, multiple estimates can increase
the expected probability of a correct choice."
A-ARM VAE FRAMEWORK,0.12658227848101267,"3.1
A-ARM VAE FRAMEWORK
We deﬁne the A-arm VAE as an A-tuple of independent and architecturally identical autoencoding
arms, where the a-th arm parameterizes a mixture model distribution (Fig. 1a). In this framework,
individual arms receive a collection of non-identical copies, {xa, xb, . . .} of the given sample, x,
belonging to the same category. While each arm has its own mixture representation with potentially
non-identical parameters, all arms cooperate to learn q(ca|xa), where ca = cb = · · · , via a cost
function at the time of training. Accordingly, a crowd of VAEs with A arms can be formulated as a
collection of constrained variational objectives as follows."
A-ARM VAE FRAMEWORK,0.13291139240506328,"max
Ls1|c1(φ1, θ1) + · · · + LsA|cA(φA, θA)
s.t. c1 = · · · = cA
(2)"
A-ARM VAE FRAMEWORK,0.13924050632911392,"where Lsa|ca(φa, θa) is the variational loss for arm a,"
A-ARM VAE FRAMEWORK,0.14556962025316456,"Lsa|ca(φa, θa) = Eq(sa,ca|xa) [log p(xa|sa, ca)] −Eq(ca|xa) [DKL (q(sa|ca, xa)∥p(sa|ca))]"
A-ARM VAE FRAMEWORK,0.1518987341772152,"−Eq(sa|ca,xa) [DKL (q(ca|xa)∥p(ca))] .
(3)"
A-ARM VAE FRAMEWORK,0.15822784810126583,"In Eq. 3, the variational loss for each arm is deﬁned according to the graphical model in Fig. 1b, which
is built upon the traditional ELBO in Eq. 1 by conditioning the continuous state on the categorical
variable (derivation in Supplementary Section B). Therefore, learning an interpretable decomposition
of the data relies on accurate assignment (inference) of the categorical latent factor. Propositions 1
and 2 below show that the shared categorical assignment inferred from q(c|x1, · · · , xA), under the"
A-ARM VAE FRAMEWORK,0.16455696202531644,Under review as a conference paper at ICLR 2022
A-ARM VAE FRAMEWORK,0.17088607594936708,"c = c1 = · · · = cA constraint of the multi-arm framework improves the accuracy of the categorical
assignment on expectation.
Proposition 1. Consider the problem of mixture representation learning in a multi-arm VAE frame-
work. For independent samples from category m, i.e. xi ∼p(x|m),
Eq(x|m) [log q(c = m|{xi}1:A)]
>
Eq(x|m) [log q(c = m|{xi}1:B)]
s.t. c = c1 = · · · = cA
s.t. c = c1 = · · · = cB
(4)
if q(m|xi) < 1 and A > B ≥1 denote the number of arms. (Proof in Supplementary Section A)"
A-ARM VAE FRAMEWORK,0.17721518987341772,"Thus, having more arms increases the expected log posterior for the true categorical latent variable
unless it is already at its maximum.
Proposition 2. In the A-arm VAE framework, there exists an A that guarantees a true categorical
assignment on expectation. That is,
m = arg max
c
Eq(x|m) [log q(c|{xi}1:A)] , s.t. c = c1 = · · · = cA .
(5)"
A-ARM VAE FRAMEWORK,0.18354430379746836,"(Proof in Supplementary Section A)
Accordingly, the consensus constraint is sufﬁcient to enhance inference for mixture representations in
the A-arm VAE framework. Our theoretical results show that the required number of arms satisfying
Eq. 5 is a function of the categorical distribution and the likelihood (Eq. 15, Supplementary Section
A). In the particular case of uniformly distributed categories, one pair of coupled arms is enough to
satisfy Eq. 5 (see Corollary 1, Supplementary Section A)."
A-ARM VAE FRAMEWORK,0.189873417721519,"We emphasize that the proposed framework does not require any weak supervision as in (Bouchacourt
et al., 2017). Instead, it relies on representations that are invariant under non-identical copies of
observations. Moreover, unlike (Bouchacourt et al., 2017; Shu et al., 2019; Locatello et al., 2020),
the multi-arm framework is not restricted to the continuous space."
A-ARM VAE FRAMEWORK,0.1962025316455696,"Arms observe non-identical copies of samples. In the A-arm VAE framework, arms receive
non-identical observations that share the discrete variational factor. To achieve this in a fully
unsupervised setting, we use type-preserving data augmentation that generates independent and
identically distributed copies of data while preserving its categorical identity. For image datasets,
conventional transformations such as rotation, scaling, or translation can serve as type-preserving
augmentations. However, for non-image datasets, e.g. single-cell data, we seek a generative model
that learns transformations representing within-class variability in an unsupervised manner. To this
end, inspired by DAGAN (Antoniou et al., 2017) and VAE-GAN (Larsen et al., 2016), we develop a
generative model to provide collections of observations for our multi-arm framework (Supplementary
Section F). The proposed generative model learns to generate augmented samples in the vicinity of
given samples in the latent space, without knowing their types (Eq. 70). In Supplementary Section
A, Remark 2, we further discuss an under-exploration scenario in data augmentation, in which the
augmented samples are not independently distributed and are concentrated around the given sample."
A-ARM VAE FRAMEWORK,0.20253164556962025,"3.2
CPL-MIXVAE: PAIRWISE COUPLING IN A-ARM VAE
In the A-arm VAE framework, the mixture representation is obtained through the optimization in
Eq. 2. Not only is it challenging to solve the maximization in Eq. 2 due to the equality constraint, but
the objective remains a function of p(c) which is unknown, and typically non-uniform. To overcome
this, we use an equivalent formulation for Eq. 2 by applying the pairwise coupling paradigm as
follows (details of derivation in Supplementary Section C): max A
X"
A-ARM VAE FRAMEWORK,0.2088607594936709,"a=1
(A −1)

Eq(sa,ca|xa) [log p(xa|sa, ca)] −Eq(ca|xa) [DKL (q(sa|ca, xa)∥p(sa|ca))]

−
X"
A-ARM VAE FRAMEWORK,0.21518987341772153,"a<b
Eq(sa|ca,xa)Eq(sb|cb,xb) [DKL (q(ca|xa)q(cb|xb)∥p(ca, cb))]"
A-ARM VAE FRAMEWORK,0.22151898734177214,"s.t. ca = cb ∀a, b ∈[1, A], a < b
(6)
We relax the optimization in Eq. 6 into an unconstrained problem by marginalizing the joint distribu-
tion over a mismatch measure between categorical variables (see Supplementary Section D): max A
X"
A-ARM VAE FRAMEWORK,0.22784810126582278,"a=1
(A −1)

Eq(sa,ca|xa) [log p(xa|sa, ca)] −Eq(ca|xa) [DKL (q(sa|ca, xa)∥p(sa|ca))]

+
X"
A-ARM VAE FRAMEWORK,0.23417721518987342,"a<b
H(ca|xa) + H(cb|xb) −λEq(ca,cb|xa,xb)

d2(ca, cb)
 (7)"
A-ARM VAE FRAMEWORK,0.24050632911392406,Under review as a conference paper at ICLR 2022
A-ARM VAE FRAMEWORK,0.2468354430379747,"In Eq. 7, in addition to entropy-based conﬁdence penalties known as mode collapse regulariz-
ers (Pereyra et al., 2017), the distance measure d(ca, cb) encourages a consensus on the categorical
assignment controlled by λ ≥0, the coupling hyperparameter."
A-ARM VAE FRAMEWORK,0.25316455696202533,"We refer to the model in Eq. 7 as cpl-mixVAE (Fig. 1a). In cpl-mixVAE, VAE arms try to achieve
identical categorical assignments while independently learning their own style variables. In experi-
ments, we set λ = 1 universally. While the bottleneck architecture already encourages interpretable
continuous variables, this formulation can be easily extended to include an additional hyperparameter
to promote disentanglement of continuous variables as in β-VAE (Higgins et al., 2017). Additional
analyses to assess the sensitivity of the cpl-mixVAE’s performance to its coupling factor can be found
in the Supplementary Section G."
A-ARM VAE FRAMEWORK,0.25949367088607594,It may be instructive to cast Eq. 7 in an equivalent constrained optimization form.
A-ARM VAE FRAMEWORK,0.26582278481012656,"Remark 1. The A-arm VAE framework is a collection of constrained variational models as follows: max A
X"
A-ARM VAE FRAMEWORK,0.2721518987341772,"a=1
Eq(sa,ca|xa) [log p(xa|sa, ca)] −Eq(ca|xa) [DKL (q(sa|ca, xa)∥p(sa|ca))] + H(ca|xa)"
A-ARM VAE FRAMEWORK,0.27848101265822783,"s.t. Eq(ca|xa)

d2(ca, cb)

< ϵ
(8)"
A-ARM VAE FRAMEWORK,0.2848101265822785,"where ϵ denotes the strength of the consensus constraint. Here, cb indicates the assigned category by
any one of the arms, b ∈{1, . . . , A}, imposing structure on the discrete variable to approximate its
prior distribution."
A-ARM VAE FRAMEWORK,0.2911392405063291,"Distance between categorical variables. d(ca, cb) denotes the distance between a pair of |c|-
dimensional un-ordered categorical variables, which are associated with probability vectors with
non-negative entries and sum-to-one constraint that form a K-dimensional simplex, where K = |c|.
In the real space, a typical choice to compute the distance between two vectors is using Euclidean
geometry. However, this geometry is not suitable for probability vectors. Here, we utilize Aitchison
geometry (Aitchison, 1982; Egozcue et al., 2003), which deﬁnes a vector space on the simplex.
Accordingly, the distance in the simplex, i.e. dSK(ca, cb) is deﬁned as dSK(ca, cb) = ∥clr(ca) −
clr(cb)∥2, ∀ca, cb ∈SK, where clr(·) denotes the isometric centered-log-ratio transformation in
the simplex. This categorical distance satisﬁes the conditions of a mathematical metric according to
Aitchison geometry."
SEEKING CONSENSUS IN THE SIMPLEX,0.2974683544303797,"3.3
SEEKING CONSENSUS IN THE SIMPLEX
An instance of the mode collapse problem (Lucas et al., 2019) manifests itself in the minimization
of dSK(ca, cb) (Eq. 7): its trivial local optima encourages the network to abuse the discrete latent
factor by ignoring many of the available categories. In the extreme case, the representations can
collapse onto a single category; ca = cb = c0. In this scenario, the continuous variable is com-
pelled to act as a primary latent factor, while the model fails to deliver an interpretable mixture
representation despite achieving an overall low loss value. To avoid such undesirable local equilibria
while training, we add perturbations to the categorical representation of each arm. If posterior
probabilities in the simplex have small dispersion, the perturbed distance calculation overstates
the discrepancies. Thus, instead of minimizing d2
SK(ca, cb), we minimize a perturbed distance"
SEEKING CONSENSUS IN THE SIMPLEX,0.3037974683544304,"d2
σ(ca, cb) = P"
SEEKING CONSENSUS IN THE SIMPLEX,0.310126582278481,"k
 
σ−1
ak log cak −σ−1
bk log cbk
2, which corresponds to the distance between addi-
tively perturbed ca and cb vectors in Aitchison geometry. Here, σ2
ak and σ2
bk indicate the mini-batch
variances of the k-th category, for arms a and b. We next show that the perturbed distance dσ(·) is
bounded by dSK(·) and non-negative values ρu, ρl:"
SEEKING CONSENSUS IN THE SIMPLEX,0.31645569620253167,"Proposition 3. Suppose ca, cb ∈SK, where SK is a simplex of K > 0 parts. If dSK (ca, cb) denotes
the distance in Aitchison geometry and d2
σ(ca, cb) = P"
SEEKING CONSENSUS IN THE SIMPLEX,0.3227848101265823,"k
 
σ−1
ak log cak −σ−1
bk log cbk
2 denotes a
perturbed distance, then"
SEEKING CONSENSUS IN THE SIMPLEX,0.3291139240506329,"d2
SK (ca, cb) −ρl ≤d2
σ (ca, cb) ≤d2
SK (ca, cb) + ρu"
SEEKING CONSENSUS IN THE SIMPLEX,0.33544303797468356,"where ρu, ρl ≥0, ρu = K
 
τ 2
σu + τ 2
c

+ 2∆στc, ρl = ∆2
σ
K −Kτ 2
σl, τc = max
k {log cak −log cbk},"
SEEKING CONSENSUS IN THE SIMPLEX,0.34177215189873417,"τσu = max
k {gk}, τσl = max
k {−gk}, ∆σ =
X"
SEEKING CONSENSUS IN THE SIMPLEX,0.34810126582278483,"k
gk, and gk = (σ−1
ak −1) log cak −(σ−1
bk −"
SEEKING CONSENSUS IN THE SIMPLEX,0.35443037974683544,1) log cbk. (Proof in Supplementary Section E)
SEEKING CONSENSUS IN THE SIMPLEX,0.36075949367088606,Under review as a conference paper at ICLR 2022
SEEKING CONSENSUS IN THE SIMPLEX,0.3670886075949367,"Thus, when ca and cb are similar and their spread is not small, dσ(ca, cb) closely approximates
dSK(ca, cb). Otherwise, it diverges from dSK(·) to avoid mode collapse."
EXPERIMENTS,0.37341772151898733,"4
EXPERIMENTS
We used four datasets: dSprites, MNIST, and two single-cell RNA sequencing (scRNA-seq) datasets;
Smart-seq ALM-VISp (Tasic et al., 2018) and 10X MOp (Yao et al., 2021). Although dSprites and
MNIST datasets do not require high-dimensional settings for mixture representation, to facilitate com-
parisons of cpl-mixVAE with earlier methods, ﬁrst we report the results for these benchmark datasets.
We trained three unsupervised VAE-based methods for mixture modeling: JointVAE (Dupont, 2018),
CascadeVAE (Jeong & Song, 2019), and ours (cpl-mixVAE). For MNIST, we additionally trained
the popular InfoGAN (Chen et al., 2016) as the most comparable GAN-based model. To show the
interpretability of the mixture representations, (i) for the discrete latent factor, we report the accu-
racy (ACC) of categorical assignments and the DKL(q(c)∥p(c))), (ii) for the continuous variable,
we perform latent traversal analysis by ﬁxing the discrete factor and changing the continuous variable
according to p(s|c, x). We calculated the accuracy by using minimum weight matching (Kuhn,
1955) to match the categorical variables obtained by cpl-mixVAE with the available cluster labels for
each dataset. Additionally, we report the computational efﬁciency (number of iterations per second)
to compare the training complexity of the multi-arm framework against earlier methods (Table 1).
All reported numbers for cpl-mixVAE models are average accuracies calculated across arms. In
VAE-based models, to sample from q(ca|xa), we use the Gumbel-softmax distribution (Jang et al.,
2016; Maddison et al., 2014). In cpl-mixVAE, each arm received an augmented copy of the original
input generated by the deep generative augmenter (Supplementary Section F) during training. Details
of the network architectures and training settings can be found in Supplementary Section L."
BENCHMARK DATASETS,0.379746835443038,"4.1
BENCHMARK DATASETS"
BENCHMARK DATASETS,0.3860759493670886,"dSprites. dSprites is procedurally generated from discrete (3 shapes) and continuous (6 style factors:
scale, rotation, and position) latent factors. Based on the uniform distribution of classes, we used a
2-arm cpl-mixVAE with |c| = 3 and |s| = 6 to learn interpretable representations. Results in Table 1
show that our method outperforms the other methods in terms of categorical assignment accuracy.
In addition to demonstrating the traversal results (Fig. 2, bottom row), we report disentanglement
scores (DS in Table 1). Even though the continuous factors do not depend on the discrete factors
in this synthetic dataset, we did not change the architecture and expected the network to infer this
independence. For a fair comparison, we used the same disentanglement metric implemented for
CascadeVAE (Jeong & Song, 2019)."
BENCHMARK DATASETS,0.3924050632911392,"MNIST. Similarly, due to the uniform distribution of digit labels in MNIST, we again used a 2-arm
cpl-mixVAE model. Following the convention (Dupont, 2018; Jeong & Song, 2019; Bouchacourt
Table 1: Training results for all datasets. cpl-mixVAE uses 2 arms. |c| and |s| denote the cardinality of latent
discrete and continuous spaces. Iter, ACC and DS denote number of iterations, the accuracy of the categorical as-
signment, and the disentanglement score, respectively. Computation denotes the training speed (iteration/second)
on a GeForce RTX 2080 Ti GPU. The computation of cpl-mixVAE includes the entire execution time for training
one pair of coupled networks, plus data augmentation."
BENCHMARK DATASETS,0.3987341772151899,"Dataset
Iter
|c|
|s|
Method
ACC (%) ↑
Computation ↑
(mean ± s.d.)
(iteration/sec)"
BENCHMARK DATASETS,0.4050632911392405,"dSprites
300K
3
6"
BENCHMARK DATASETS,0.41139240506329117,"JointVAE
44.79 ± 03.9
52.6
74.5 ± 5.2 DS"
BENCHMARK DATASETS,0.4177215189873418,"CascadeVAE
78.84 ± 15.7
15.4
90.5 ± 5.3
cpl-mixVAE
96.30 ± 09.2
20.6
89.9 ± 4.1"
BENCHMARK DATASETS,0.4240506329113924,"MNIST
120K"
BENCHMARK DATASETS,0.43037974683544306,"10
2
InfoGAN
77.87 ± 21.7
12.2
2.15 ± 4.2"
BENCHMARK DATASETS,0.43670886075949367,"DKL(q(c)∥p(c)) (× 100) 10
10"
BENCHMARK DATASETS,0.4430379746835443,"JointVAE
68.99 ± 11.8
74.1
3.12 ± 1.0
JointVAE†
68.21 ± 09.6
54.4
3.65 ± 2.7
CascadeVAE
74.83 ± 06.9
23.8
1.99 ± 1.1
CascadeVAE†
72.98 ± 13.4
17.8
2.44 ± 1.4
cpl-mixVAE
84.56 ± 06.5
17.5
1.48 ± 1.4
Smart-seq
40K
115
2"
BENCHMARK DATASETS,0.44936708860759494,"JointVAE
12.53 ± 02.9
28.6
459 ± 12
ALM-VISp
CascadeVAE
02.69 ± 00.1
03.4
54.8 ± 0.5
cpl-mixVAE
38.78 ± 01.3
10.1
43.9 ± 3.5"
X MOP,0.45569620253164556,"10X MOp
220K
140
2"
X MOP,0.4620253164556962,"JointVAE
02.32 ± 00.1
14.8
542 ± 17
CascadeVAE
02.85 ± 00.5
00.6
150 ± 3.5
cpl-mixVAE
23.64 ± 00.9
08.2
135 ± 5.2"
X MOP,0.46835443037974683,Under review as a conference paper at ICLR 2022
X MOP,0.47468354430379744,"Figure 2: Interpretable continuous latent traversals of 1-st arm of the cpl-mixVAE framework with two
autoencoders, for MNIST (top) and dSprites (bottom). The discrete variable c is constant for all reconstructions
in the same row."
X MOP,0.4810126582278481,"et al., 2017), each arm of cpl-mixVAE uses a 10-dimensional categorical variable representing
digits (type), and a 10-dimensional continuous random variable representing the writing style (state).
Table 1 displays the accuracy of the categorical assignment and the discrepancy between q(c) and
p(c) for InfoGAN, two 1-arm VAE methods (JointVAE and CascadeVAE), and cpl-mixVAE with
2 arms. Additionally, to isolate the impact of data augmentation in training, we trained JointVAE†
and CascadeVAE† where the models were trained with the same augmented copies of the original
MNIST dataset as cpl-mixVAE. The results in Table 1 suggest that data augmentation by itself
does not enhance the performance. Fig. 2 (top row) illustrates the continuous latent traversals for
four dimensions of the state variable inferred by cpl-mixVAE, where each row corresponds to a
different dimension of the categorical variable, and the state variable monotonically changes across
columns. Both results in Table 1 and Fig. 2 show that cpl-mixVAE achieved an interpretable mixture
representation with the highest categorical assignment accuracy."
X MOP,0.4873417721518987,"Summary. cpl-mixVAE improves the discrete density approximation and infers better mixture
representations. It outperforms earlier methods, without using extraneous optimization or heuristic
channel capacities. Beyond performance and robustness, its computational cost is also comparable to
that of the baselines."
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.4936708860759494,"4.2
SINGLE-CELL RNA SEQUENCING DATA
In this dataset the observations are individual cells and each observation consists of expressions of
thousands of genes. Here, we used two scRNA-seq datasets: (i) Smart-seq ALM-VISp (Tasic et al.,
2018) and (ii) 10X MOp (Yao et al., 2021). The Smart-seq dataset includes transcriptomic proﬁles
of more than 10, 000 genes for ∼22, 000 cells from the mouse anterior lateral motor cortex (ALM)
and the primary visual cortex (VIPs). The 10X MOp dataset proﬁles ∼123000 cells in the mouse
primary motor cortex (MOp) with the droplet-based 10X Genomics Chromium platform. 10X-based
data often display more gene dropouts, especially for genes with lower expression levels. scRNA-seq
datasets are signiﬁcantly more complex than typical benchmark datasets due to (i) large number of
cell types (discrete variable), and (ii) class imbalance; in the 10X MOp dataset, for instance, the most-
and the least-abundant cell types include 17, 000 and 20 samples, respectively. Moreover, whether
the observed diversity corresponds to discrete variability or a continuum is an ongoing debate in
neuroscience (Scala et al., 2020). While using genes that are differentially expressed in subsets of
cells, known as marker genes (MGs) (Trapnell, 2015) is a common approach to deﬁne cell types,
the identiﬁed genes rarely obey the idealized MG deﬁnition in practice. Here, we focus on neuronal
cells and use a subset of 5, 000 highest variance genes. The original MG-based studies for each
dataset suggested 115 (Smart-seq data) (Tasic et al., 2018) and 140 (10X-based data) (Yao et al.,
2021) discrete neuronal types."
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5,"Neuron type identiﬁcation. Based on the suggested taxonomies in (Tasic et al., 2018; Yao et al.,
2021), for the Smart-seq ALM-VISp data, we used 115- and 2-dimensional discrete and continuous
variables, and for the 10X MOp data, we used 140- and 2-dimensional discrete and continuous latent
variables. We compared the suggested cell types in (Tasic et al., 2018; Yao et al., 2021) with the
discrete representations that are inferred from VAE models. Table 1 and Fig. 3(a-b) demonstrate the"
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5063291139240507,Under review as a conference paper at ICLR 2022
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5126582278481012,"Categories
Categories
Categories"
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5189873417721519,"Categories
Categories
Categories"
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5253164556962026,"CascadeVAE
cpl-mixVAE"
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5316455696202531,Cell Types
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5379746835443038,"JointVAE
(a.1)
(a.2)
(a.3) (b.3)"
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5443037974683544,Cell Types
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5506329113924051,"(b.2)
(b.1) (c)"
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5569620253164557,"Figure 3: Categorical assignments for the scRNA-seq datasets. The top row shows confusion matrices of
JointVAE (a.1), CascadeVAE (a.2), and cpl-mixVAE (a.3) trained by |c| = 115, |s| = 2, for the Smart-seq ALM-
VISp dataset. The dendrogram on the y-axis shows MG-based hierarchical classiﬁcation with 115 cell types,
suggested by Tasic et al. (2018). The bottom row shows confusion matrices of JointVAE (b.1), CascadeVAE (b.2),
and cpl-mixVAE (b.3) trained by |c| = 140, |s| = 2, for the 10X MOp dataset. Cell types on the y-axis are
sorted based on a hierarchical classiﬁcation suggested by Yao et al. (2021). (c) Improvement of the categorical
representation (ACC) of cpl-mixVAE by adding more arms to the multi-arm framework. A-arm’s performance
for A ≥2 is compared with the baseline 1-arm, JointVAE, across 3 randomly initialized runs."
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5632911392405063,"performance of a 2-arm cpl-mixVAE model against JointVAE and CascadeVAE. In Fig. 3a.1 and 3b.1,
we observe that for both datasets, JointVAE succeeds in identifying (sub)classes of neurons, e.g.
excitatory class, or Sst subclass, but not neuronal types at leaf nodes. On the other hand, CascadeVAE
learns an almost uniform distribution over all types despite a sizeable difference between the relative
abundances of neuronal types (Fig. 3a.2 and 3b.2). Our results in Fig. 3(a-b) clearly show that cpl-
mixVAE outperforms JointVAE and CascadeVAE in identifying meaningful known cell types. The
confusion matrices in Fig. 3a.3 and 3b.3 demonstrate that even the inaccurate categorical assignments
of cpl-mixVAE are still close to the matrix diagonals, suggesting a small cophenetic distance. That is,
those cells are still assigned to nearby cell types in the dendrogram."
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.569620253164557,"Using A > 2. Unlike the discussed benchmark datasets, the neuronal types are not uniformly
distributed. Accordingly, we also investigated the accuracy improvement for categorical assignment
when more than two arms are used. Fig. 3c illustrates the accuracy improvement with respect to a
single autoencoder model, i.e. JointVAE, in agreement with our theoretical ﬁndings."
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5759493670886076,"Identifying genes regulating cell activity. To examine the role of the continuous latent variable, we
applied a similar traversal analysis to that used for the benchmark datasets. For a given cell sample
and its discrete type, we changed each dimension of the continuous variable using the conditional
distribution, and inspected gene expression changes caused by continuous variable alterations. Fig. 4
shows the results of the continuous traversal study for JointVAE and cpl-mixVAE, for two excitatory
neurons belonging to the “L5 NP” (cell type (I)) and “L6 CT” (cell type (II)) sub-classes in ALM
and MOp regions. Note that here, JointVAE is equivalent to a 1-arm VAE, with the exception of the
type dependence of the state variable. Since CascadeVAE did not learn meaningful clustering of
cells, even at the subclass level, we did not consider it for the continuous factor analysis. In each
sub-ﬁgure, the latent traversal is color-mapped to normalized reconstructed expression values, where
the y-axis corresponds to one dimension of the continuous variable, and the x-axis corresponds to
three gene subsets, namely (i) MGs for the two excitatory types, (ii) immediate early genes (IEGs),
and (iii) housekeeping gene (HKG) subgroups (Hrvatin et al., 2018; Tarasenko et al., 2017). For
cpl-mixVAE (Fig. 4b), the normalized expression of the reported MGs as indicators for excitatory
cell types (discrete factors) is unaffected by changes of identiﬁed continuous variables. In contrast,
for JointVAE (Fig. 4a), we observed that the normalized expression of some MGs (5 out of 10)
are changed due to the continuous factor traversal. Additionally, we found that the expression
changes inferred by cpl-mixVAE for IEGs and HKGs are essentially monotonically linked to the
continuous variable, conﬁrming that the expression of IEGs and HKGs depends strongly on the cell
activity variations under different metabolic and environmental conditions. Conversely, JointVAE"
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5822784810126582,Under review as a conference paper at ICLR 2022 (I) (II)
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5886075949367089,"(a) JointVAE - Smart-seq ALM-VISp
(b) cpl-mixVAE - Smart-seq ALM-VISp
(c) cpl-mixVAE - 10x MOp"
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.5949367088607594,"Figure 4: Continuous latent traversal analysis for two excitatory cell types: (I) “L5 NP” and (II) “L6 CT”,
in different brain regions. For each type, the traversal is color-mapped to a normalized reconstructed gene
expression value (colorbar) as a function of the state variable for 3 gene subsets: marker genes (MG), immediate
early genes (IEG), and housekeeping genes (HKG)."
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.6012658227848101,"fails to reveal such activity-regulated monotonicity for IEGs and HKGs. Furthermore, our results
for cpl-mixVAE reveal that the expression of activity-regulated genes depends on the cell type, i.e.
IEGs and HKGs respond differently to activation depending on their cell types (compare rows I
and II in Fig. 4b). However, in Fig. 4a, since the baseline JointVAE does not take into account the
dependency of discrete and continuous factors, it fails to reveal the dependence of activity-regulated
expression to the cell type, and therefore produces identical expressions for both types (I) and (II).
These ﬁndings are consistent over multiple randomly initialized runs (Supplementary Section K.1).
See Supplementary Section K.2 for more results on other cell types and gene subsets."
"SINGLE-CELL RNA SEQUENCING DATA
IN THIS DATASET THE OBSERVATIONS ARE INDIVIDUAL CELLS AND EACH OBSERVATION CONSISTS OF EXPRESSIONS OF",0.6075949367088608,"Summary. The cpl-mixVAE model successfully identiﬁed the majority of known excitatory and
inhibitory neurons in multiple cortical regions. Our ﬁndings suggest that cpl-mixVAE, by acknowl-
edging the dependencies of continuous and categorical factors, captures relevant and interpretable
continuous variability that can provide insight when deciphering the molecular mechanisms shaping
the landscape of biological states, e.g. due to metabolism or disease."
ABLATION STUDIES,0.6139240506329114,"4.3
ABLATION STUDIES"
ABLATION STUDIES,0.620253164556962,"To elucidate the success of the A-arm VAE framework in mixture modeling, we investigate the
categorical assignment performance under different training settings. Since CascadeVAE does not
learn the categorical factors by variational inference, here we mainly study JointVAE (as a 1-arm
VAE) and cpl-mixVAE (as a 2-arm VAE). In Section 4.1, we show that data augmentation by itself
does not enhance the categorical assignment (JointVAE†). To understand whether architectural
differences put JointVAE at a disadvantage, we trained JointVAE‡ (Table S1), which uses the same
architecture as the one used in cpl-mixVAE. JointVAE‡ uses the same learning procedure as JointVAE,
but its convolutional layers are replaced by fully-connected layers (see Supplementary Section J
and L for details). The result for JointVAE‡ suggests that the superiority of cpl-mixVAE is not
due to the network architecture either. We also examined the performance changes of the proposed
2-arm cpl-mixVAE under three different settings: (i) cpl-mixVAE∗, where coupled networks are not
independent and network parameters are shared; (ii) cpl-mixVAEa, where only afﬁne transformations
are used for data augmentation; and (iii) cpl-mixVAE(s̸ | c), where the state variable is independent
of the discrete variable (Table S1). Our results show that the proposed cpl-mixVAE obtained the best
categorical assignments across all training settings. We also examined the accuracy of categorical
assignments for the cpl-mixVAE model, under different dimensions of discrete latent variable, for
both MNIST and scRNA-seq datasets (see Supplementary Section J). We experimentally observe that
while JointVAE suffers from sensitivity to empirical choices of |c|, cpl-mixVAE is more robust in
encoding the discrete variability, without suffering from mode collapse (Fig. S9 and Fig. S10)."
CONCLUSION,0.6265822784810127,"5
CONCLUSION"
CONCLUSION,0.6329113924050633,"We have proposed cpl-mixVAE as a multi-arm framework to apply the power of collective decision
making in unsupervised joint representation learning of discrete and continuous factors, scalable to
the high-dimensional discrete space. This framework utilizes multiple pairwise-coupled autoencoding
arms with a shared categorical variable, while independently learning the continuous variables. Our
experimental results for all datasets support the theoretical ﬁndings, and show that cpl-mixVAE
outperforms comparable models. Importantly, for challenging scRNA-seq dataset, we showed that
the proposed framework identiﬁes biologically interpretable cell types and differentiate between
type-dependent and activity-regulated genes."
CONCLUSION,0.6392405063291139,Under review as a conference paper at ICLR 2022
REFERENCES,0.6455696202531646,REFERENCES
REFERENCES,0.6518987341772152,"John Aitchison. The statistical analysis of compositional data. Journal of the Royal Statistical Society:
Series B (Methodological), 44(2):139–160, 1982."
REFERENCES,0.6582278481012658,"Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial
networks. arXiv preprint arXiv:1711.04340, 2017."
REFERENCES,0.6645569620253164,"Cornelia Bargmann, William Newsome, A Anderson, E Brown, K Deisseroth, J Donoghue,
P MacLeish, E Marder, R Normann, J Sanes, et al. Brain 2025: a scientiﬁc vision. Brain
Research through Advancing Innovative Neurotechnologies (BRAIN) Working Group Report to the
Advisory Committee to the Director, NIH, 2014."
REFERENCES,0.6708860759493671,"David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.
Journal of the American statistical Association, 112(518):859–877, 2017."
REFERENCES,0.6772151898734177,"Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proceed-
ings of the eleventh annual conference on Computational learning theory, pp. 92–100, 1998."
REFERENCES,0.6835443037974683,"Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin.
Multi-level variational autoen-
coder: Learning disentangled representations from grouped observations.
arXiv preprint
arXiv:1705.08841, 2017."
REFERENCES,0.689873417721519,"Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in β-vae. arXiv preprint arXiv:1804.03599,
2018."
REFERENCES,0.6962025316455697,"Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172–2180, 2016."
REFERENCES,0.7025316455696202,"Jacob Deasy, Nikola Simidjievski, and Pietro Liò. Constraining variational inference with geometric
jensen-shannon divergence. arXiv preprint arXiv:2006.10599, 2020."
REFERENCES,0.7088607594936709,"Nat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni, Kai
Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture varia-
tional autoencoders. arXiv preprint arXiv:1611.02648, 2016."
REFERENCES,0.7151898734177216,"Emilien Dupont. Learning disentangled joint continuous and discrete representations. In Advances in
Neural Information Processing Systems, pp. 710–720, 2018."
REFERENCES,0.7215189873417721,"Juan José Egozcue, Vera Pawlowsky-Glahn, Glòria Mateu-Figueras, and Carles Barcelo-Vidal.
Isometric logratio transformations for compositional data analysis. Mathematical Geology, 35(3):
279–300, 2003."
REFERENCES,0.7278481012658228,"Fangxiang Feng, Xiaojie Wang, and Ruifan Li. Cross-modal retrieval with correspondence au-
toencoder. In Proceedings of the 22nd ACM international conference on Multimedia, pp. 7–16,
2014."
REFERENCES,0.7341772151898734,"Rohan Gala, Nathan Gouwens, Zizhen Yao, Agata Budzillo, Osnat Penn, Bosiljka Tasic, Gabe
Murphy, Hongkui Zeng, and Uygar Sümbül. A coupled autoencoder approach for multi-modal
analysis of cell types. In Advances in Neural Information Processing Systems, pp. 9263–9272,
2019."
REFERENCES,0.740506329113924,"Fangjian Guo, Xiangyu Wang, Kai Fan, Tamara Broderick, and David B Dunson. Boosting variational
inference. arXiv preprint arXiv:1611.05559, 2016."
REFERENCES,0.7468354430379747,"Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. Iclr, 2(5):6, 2017."
REFERENCES,0.7531645569620253,"Haruo Hosoya. Group-based learning of disentangled representations with generalizability for novel
contents. In IJCAI, pp. 2506–2513, 2019."
REFERENCES,0.759493670886076,Under review as a conference paper at ICLR 2022
REFERENCES,0.7658227848101266,"Sinisa Hrvatin, Daniel R Hochbaum, M Aurel Nagy, Marcelo Cicconet, Keiramarie Robertson, Lucas
Cheadle, Rapolas Zilionis, Alex Ratner, Rebeca Borges-Monroy, Allon M Klein, et al. Single-
cell analysis of experience-dependent transcriptomic states in the mouse visual cortex. Nature
neuroscience, 21(1):120–129, 2018."
REFERENCES,0.7721518987341772,"Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016."
REFERENCES,0.7784810126582279,"Yeonwoo Jeong and Hyun Oh Song. Learning discrete and continuous factors of data via alternating
disentanglement. arXiv preprint arXiv:1905.09432, 2019."
REFERENCES,0.7848101265822784,"Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep
embedding: an unsupervised and generative approach to clustering. In Proceedings of the 26th
International Joint Conference on Artiﬁcial Intelligence, pp. 1965–1972, 2017."
REFERENCES,0.7911392405063291,"Matthew J Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta.
Composing graphical models with neural networks for structured representations and fast inference.
In Advances in neural information processing systems, pp. 2946–2954, 2016."
REFERENCES,0.7974683544303798,"Hyunjik Kim and Andriy Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983,
2018."
REFERENCES,0.8037974683544303,"Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
arXiv:1312.6114, 2013."
REFERENCES,0.810126582278481,"Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improving variational inference with inverse autoregressive ﬂow. arXiv preprint arXiv:1606.04934,
2016."
REFERENCES,0.8164556962025317,"Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics
quarterly, 2(1-2):83–97, 1955."
REFERENCES,0.8227848101265823,"Abhishek Kumar and Hal Daumé. A co-training approach for multi-view spectral clustering. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 393–400,
2011."
REFERENCES,0.8291139240506329,"Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoen-
coding beyond pixels using a learned similarity metric. In International conference on machine
learning, pp. 1558–1566. PMLR, 2016."
REFERENCES,0.8354430379746836,"Mihee Lee and Vladimir Pavlovic. Private-shared disentangled multimodal vae for learning of hybrid
latent representations. arXiv preprint arXiv:2012.13024, 2020."
REFERENCES,0.8417721518987342,"Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf,
and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentan-
gled representations. arXiv preprint arXiv:1811.12359, 2018a."
REFERENCES,0.8481012658227848,"Francesco Locatello, Gideon Dresdner, Rajiv Khanna, Isabel Valera, and Gunnar Rätsch. Boosting
black box variational inference. In Advances in Neural Information Processing Systems, pp.
3401–3411, 2018b."
REFERENCES,0.8544303797468354,"Francesco Locatello, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, and
Michael Tschannen. Weakly-supervised disentanglement without compromises. arXiv preprint
arXiv:2002.02886, 2020."
REFERENCES,0.8607594936708861,"Romain Lopez, Jeffrey Regier, Michael B Cole, Michael I Jordan, and Nir Yosef. Deep generative
modeling for single-cell transcriptomics. Nature methods, 15(12):1053–1058, 2018."
REFERENCES,0.8670886075949367,"James Lucas, George Tucker, Roger B Grosse, and Mohammad Norouzi. Don’t blame the elbo!
a linear vae perspective on posterior collapse. In Advances in Neural Information Processing
Systems, pp. 9403–9413, 2019."
REFERENCES,0.8734177215189873,"Chris J Maddison, Daniel Tarlow, and Tom Minka. A* sampling. In Advances in Neural Information
Processing Systems, pp. 3086–3094, 2014."
REFERENCES,0.879746835443038,Under review as a conference paper at ICLR 2022
REFERENCES,0.8860759493670886,"Tom Minka et al. Divergence measures and message passing. Technical report, Citeseer, 2005."
REFERENCES,0.8924050632911392,"Stefano Monti, Pablo Tamayo, Jill Mesirov, and Todd Golub. Consensus clustering: a resampling-
based method for class discovery and visualization of gene expression microarray data. Machine
learning, 52(1-2):91–118, 2003."
REFERENCES,0.8987341772151899,"Jozsef Nemeth. Adversarial disentanglement with grouped observations. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, volume 34, pp. 10243–10250, 2020."
REFERENCES,0.9050632911392406,"Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing conﬁdent output distributions. arXiv preprint arXiv:1701.06548,
2017."
REFERENCES,0.9113924050632911,"Matias Quiroz, David J Nott, and Robert Kohn. Gaussian variational approximation for high-
dimensional state space models. arXiv preprint arXiv:1801.07873, 2018."
REFERENCES,0.9177215189873418,"Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In International
Conference on Machine Learning, pp. 324–333. PMLR, 2016."
REFERENCES,0.9240506329113924,"Federico Scala, Dmitry Kobak, Matteo Bernabucci, Yves Bernaerts, Cathryn René Cadwell, Je-
sus Ramon Castro, Leonard Hartmanis, Xiaolong Jiang, Sophie Laturnus, Elanine Miranda, et al.
Phenotypic variation of transcriptomic cell types in mouse motor cortex. Nature, pp. 1–7, 2020."
REFERENCES,0.930379746835443,"H Sebastian Seung and Uygar Sümbül. Neuronal cell types and connectivity: lessons from the retina.
Neuron, 83(6):1262–1272, 2014."
REFERENCES,0.9367088607594937,"Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised
disentanglement with guarantees. arXiv preprint arXiv:1910.09772, 2019."
REFERENCES,0.9430379746835443,"James Surowiecki. The wisdom of crowds. Anchor, 2005."
REFERENCES,0.9493670886075949,"Tatyana N Tarasenko, Susan E Pacheco, Mary Kay Koenig, Julio Gomez-Rodriguez, Senta M Kapnick,
Francisca Diaz, Patricia M Zerfas, Emanuele Barca, Jessica Sudderth, Ralph J DeBerardinis, et al.
Cytochrome c oxidase activity is a metabolic checkpoint that regulates cell fate decisions during t
cell activation and differentiation. Cell metabolism, 25(6):1254–1268, 2017."
REFERENCES,0.9556962025316456,"Bosiljka Tasic, Zizhen Yao, Lucas T Graybuck, Kimberly A Smith, Thuc Nghi Nguyen, Darren
Bertagnolli, Jeff Goldy, Emma Garren, Michael N Economo, Sarada Viswanathan, et al. Shared
and distinct transcriptomic cell types across neocortical areas. Nature, 563(7729):72–78, 2018."
REFERENCES,0.9620253164556962,"Kai Tian, Shuigeng Zhou, and Jihong Guan. Deepcluster: A general clustering framework based on
deep learning. In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases, pp. 809–825. Springer, 2017."
REFERENCES,0.9683544303797469,"Cole Trapnell. Deﬁning cell types and states with single-cell genomics. Genome research, 25(10):
1491–1498, 2015."
REFERENCES,0.9746835443037974,"Michael Tschannen, Olivier Bachem, and Mario Lucic. Recent advances in autoencoder-based
representation learning. arXiv preprint arXiv:1812.05069, 2018."
REFERENCES,0.9810126582278481,"Zizhen Yao, Cindy TJ van Velthoven, Thuc Nghi Nguyen, Jeff Goldy, Adriana E Sedeno-Cortes,
Fahimeh Baftizadeh, Darren Bertagnolli, Tamara Casper, Megan Chiang, Kirsten Crichton, et al. A
taxonomy of transcriptomic cell types across the isocortex and hippocampal formation. Cell, 2021."
REFERENCES,0.9873417721518988,"Hongkui Zeng and Joshua R Sanes. Neuronal cell-type classiﬁcation: challenges, opportunities and
the path forward. Nature Reviews Neuroscience, 18(9):530, 2017."
REFERENCES,0.9936708860759493,"Cheng Zhang, Judith Bütepage, Hedvig Kjellström, and Stephan Mandt. Advances in variational
inference. IEEE transactions on pattern analysis and machine intelligence, 41(8):2008–2026,
2018."
