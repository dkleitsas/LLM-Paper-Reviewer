Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001984126984126984,"High annotation cost has driven extensive research in active learning and self-
1"
ABSTRACT,0.003968253968253968,"supervised learning. Recent research has shown that in the context of supervised
2"
ABSTRACT,0.005952380952380952,"learning, when we have different numbers of labels, we need to apply different
3"
ABSTRACT,0.007936507936507936,"active learning strategies to ensure that it outperforms the random baseline. This
4"
ABSTRACT,0.00992063492063492,"number of annotations that change the suitable active learning strategy is called the
5"
ABSTRACT,0.011904761904761904,"phase transition point. We found, however, when combining active learning with
6"
ABSTRACT,0.013888888888888888,"self-supervised models to achieve improved performance, the phase transition point
7"
ABSTRACT,0.015873015873015872,"occurs earlier. It becomes challenging to determine which strategy should be used
8"
ABSTRACT,0.017857142857142856,"for previously unseen datasets. We argue that existing active learning algorithms are
9"
ABSTRACT,0.01984126984126984,"heavily influenced by the phase transition because the empirical risk over the entire
10"
ABSTRACT,0.021825396825396824,"active learning pool estimated by these algorithms is inaccurate and influenced by
11"
ABSTRACT,0.023809523809523808,"the number of labeled samples. To address this issue, we propose a novel active
12"
ABSTRACT,0.025793650793650792,"learning strategy, neural tangent kernel clustering-pseudo-labels (NTKCPL). It
13"
ABSTRACT,0.027777777777777776,"estimates empirical risk based on pseudo-labels and the model prediction with
14"
ABSTRACT,0.02976190476190476,"NTK approximation. We analyze the factors affecting this approximation error and
15"
ABSTRACT,0.031746031746031744,"design a pseudo-label clustering generation method to reduce the approximation
16"
ABSTRACT,0.03373015873015873,"error. Finally, our method was validated on five datasets, empirically demonstrating
17"
ABSTRACT,0.03571428571428571,"that it outperforms the baseline methods in most cases and is valid over a wider
18"
ABSTRACT,0.037698412698412696,"range of training budgets.
19"
INTRODUCTION,0.03968253968253968,"1
Introduction
20"
INTRODUCTION,0.041666666666666664,"The boom in deep learning models in recent years stems in part from the massive amounts of
21"
INTRODUCTION,0.04365079365079365,"data [11, 17, 23]. However, the demand for large amounts of data, especially labeled data, in
22"
INTRODUCTION,0.04563492063492063,"turn, constrains the application of deep learning models, since large amounts of labels imply high
23"
INTRODUCTION,0.047619047619047616,"annotation costs [41, 1, 45]. Active learning is a path to alleviate the cost of labeling by selecting
24"
INTRODUCTION,0.0496031746031746,"informative subsets of samples to annotate.
25"
INTRODUCTION,0.051587301587301584,"However, the benefits of active learning have been increasingly questioned in recent years [25, 28].
26"
INTRODUCTION,0.05357142857142857,"One of the main concerns is that training a model initialized by self-supervised learning with randomly
27"
INTRODUCTION,0.05555555555555555,"selected labeled samples often yields results far beyond those obtained by existing active learning
28"
INTRODUCTION,0.057539682539682536,"with supervised training (randomly initialized or initialized by the last round of the active learning
29"
INTRODUCTION,0.05952380952380952,"model) [6, 8, 7, 14, 9]. Because the latter only uses labeled data to train the network, while the
30"
INTRODUCTION,0.061507936507936505,"former uses a large amount of unlabeled data to train the backbone of the network. Since most
31"
INTRODUCTION,0.06349206349206349,"existing active learning algorithms are designed in the context of supervised training, they must be
32"
INTRODUCTION,0.06547619047619048,"validated with a large number of labels compared to the number of labels required in training from a
33"
INTRODUCTION,0.06746031746031746,"self-supervised model. This means that the effectiveness of these active learning algorithms is not
34"
INTRODUCTION,0.06944444444444445,"guaranteed in the case of having access to relatively few annotations, as is the case when combining
35"
INTRODUCTION,0.07142857142857142,"with a self-supervised model. Several studies [15, 42, 4] have shown that many existing active
36"
INTRODUCTION,0.07341269841269842,"learning strategies fail to outperform the random baseline when combining them with self-supervised
37"
INTRODUCTION,0.07539682539682539,"learning. In this paper, we focus on designing an active learning strategy that works well in the
38"
INTRODUCTION,0.07738095238095238,"training method with a self-supervised model.
39"
INTRODUCTION,0.07936507936507936,"The “phase transition” phenomenon [15] is known to occur in active learning with supervised training.
40"
INTRODUCTION,0.08134920634920635,"It refers to the fact that an active learning strategy that outperforms a random baseline when the total
41"
INTRODUCTION,0.08333333333333333,"number of labels is small will be inferior to a random baseline when the total number of labels is
42"
INTRODUCTION,0.08531746031746032,"large (called the low-budget strategy) and vice versa (called the high-budget strategy). We note that
43"
INTRODUCTION,0.0873015873015873,"when combining active learning with the self-supervised model, the cut-off point between low-budget
44"
INTRODUCTION,0.08928571428571429,"and high-budget strategy occurs much earlier. For example, in the CIFAR-100 [21], the cut-off point
45"
INTRODUCTION,0.09126984126984126,"is about 10,000 labeled samples when training in the supervised learning way [16]. But, the cut-off
46"
INTRODUCTION,0.09325396825396826,"point shifts forward to about 1,500 labeled samples when training from a self-supervised model. The
47"
INTRODUCTION,0.09523809523809523,"forward-moving cut-off point means that even if the annotation budget is low (only one order of
48"
INTRODUCTION,0.09722222222222222,"magnitude above the number of classes in the dataset), it is likely to hit that cut-off point. Thus, for a
49"
INTRODUCTION,0.0992063492063492,"previously unseen dataset, it is difficult to simply determine whether a low-budget or high-budget
50"
INTRODUCTION,0.10119047619047619,"strategy should be chosen since the difficulty varies from dataset to dataset. In this paper we use this
51"
INTRODUCTION,0.10317460317460317,"problem to motivate the design of an active learning strategy with a wider effective budget range.
52"
INTRODUCTION,0.10515873015873016,"Since existing low-budget strategies are designed based on the idea of feature space coverage [24, 15,
53"
INTRODUCTION,0.10714285714285714,"42], we first analyze the problems of determining coverage based on sample feature distances in sec. 2.
54"
INTRODUCTION,0.10912698412698413,"After that, we propose that the true coverage where the empirical risk is zero, can be estimated based
55"
INTRODUCTION,0.1111111111111111,"on pseudo-labels and predictions of the model trained on the candidate set. Based on this, we propose
56"
INTRODUCTION,0.1130952380952381,"our active learning strategy, Neural Tangent Kernel Clustering-Pseudo-Labels (NTKCPL), which
57"
INTRODUCTION,0.11507936507936507,"uses the NTK [18, 27] and CPL to approximate empirical risk on active learning pool in sec. 3.2. And
58"
INTRODUCTION,0.11706349206349206,"we analyze which factor affects approximation error in sec. 3.3. Based on this analysis, we design
59"
INTRODUCTION,0.11904761904761904,"a CPL generation method in sec. 3.4. Extensive experimental results demonstrate that our method
60"
INTRODUCTION,0.12103174603174603,"outperforms state-of-the-art approaches in most cases and has a wider effective budget range. As part
61"
INTRODUCTION,0.12301587301587301,"of the results (sec. 4) we also show our method is effective for self-supervised features of different
62"
INTRODUCTION,0.125,"quality.
63"
INTRODUCTION,0.12698412698412698,"Our contribution is summarized as follows: (1) We propose a novel active learning strategy, NTKCPL,
64"
INTRODUCTION,0.12896825396825398,"by estimating empirical risk on the whole active learning pool based on pseudo-labels. (2) We analyze
65"
INTRODUCTION,0.13095238095238096,"the approximation error of the empirical risk in the active learning pool when NTK and CPL are used
66"
INTRODUCTION,0.13293650793650794,"to approximate networks and true labels. (3) Our method outperforms both low- and high-budget
67"
INTRODUCTION,0.1349206349206349,"active learning strategies within a range of annotation quantities one order of magnitude larger than
68"
INTRODUCTION,0.13690476190476192,"traditional low-budget active learning experiments. This means that our approach can be used more
69"
INTRODUCTION,0.1388888888888889,"confidently for active learning on top of self-supervised models than existing low-budget strategies.
70"
RELATED WORK,0.14087301587301587,"1.1
Related Work
71"
RELATED WORK,0.14285714285714285,"Most active learning strategies are designed and validated in the high-budget scenario where network
72"
RELATED WORK,0.14484126984126985,"weights are randomly initialized or initialized from the weights of the previous active learning
73"
RELATED WORK,0.14682539682539683,"round. Active learning methods mainly include uncertainty-based sampling [22, 13, 19], feature
74"
RELATED WORK,0.1488095238095238,"space coverage [32, 24, 42, 33, 5, 40], the combination of uncertainty and diversity [41, 3], learning-
75"
RELATED WORK,0.15079365079365079,"based methods [43], and so on [34, 35]. Moreover, some recent studies explore “look ahead”
76"
RELATED WORK,0.1527777777777778,"strategies [26, 38], where samples are selected based on the model trained on candidate training sets.
77"
RELATED WORK,0.15476190476190477,"However, with the development of self-supervised training, the training approach for low-budget
78"
RELATED WORK,0.15674603174603174,"scenarios has shifted to training based on a self-supervised pre-trained model [24]. This change in
79"
RELATED WORK,0.15873015873015872,"the training method implies a shift in the total number of samples that need to be selected by active
80"
RELATED WORK,0.16071428571428573,"learning. When training based on a self-supervised model, often only 0.4-6% of the total data needs
81"
RELATED WORK,0.1626984126984127,"to be labeled to achieve similar results to training with 20-40% labeled data on a randomly initialized
82"
RELATED WORK,0.16468253968253968,"network [4]. Recent studies have shown that there exists a phase transition phenomenon in active
83"
RELATED WORK,0.16666666666666666,"learning strategies, whereby opposite strategies should be adopted in high-budget and low-budget
84"
RELATED WORK,0.16865079365079366,"scenarios [15], causing many active learning strategies designed for high-budget scenarios unsuitable
85"
RELATED WORK,0.17063492063492064,"for training based on a self-supervised model. As a result, recent studies have explored active learning
86"
RELATED WORK,0.17261904761904762,"strategies specifically designed for low-budget scenarios [15, 42, 31, 20]. However, we find that these
87"
RELATED WORK,0.1746031746031746,"strategies are effective only when the number of labeled data samples is extremely small, and as we
88"
RELATED WORK,0.1765873015873016,"increase the labeled data to one order of magnitude above the number of classes of the dataset, their
89"
RELATED WORK,0.17857142857142858,"performance falls below that of the random baseline.
90"
RELATED WORK,0.18055555555555555,"2
Insight: Distance is not an accurate indicator of empirical risk
91"
RELATED WORK,0.18253968253968253,"The goal of the active learning is to find a labeled subset, DC = (xi, yi)NC
i=1, such that the model
92"
RELATED WORK,0.18452380952380953,"trained on that subset, fDC, has the minimized empirical risk in the entire active learning pool,
93"
RELATED WORK,0.1865079365079365,"D = (xi, yi)N
i=1 as shown in eq. 1.
94"
RELATED WORK,0.1884920634920635,"argminDC
1
N X"
RELATED WORK,0.19047619047619047,"i∈D
Loss(fDC(xi), yi)
(1)"
RELATED WORK,0.19246031746031747,"Unfortunately, during active learning, we do not have the labels of the entire active learning pool, so
95"
RELATED WORK,0.19444444444444445,"we cannot compute this loss directly. To address this problem, current methods [32, 24, 33] covert
96"
RELATED WORK,0.19642857142857142,"empirical risk minimization into feature space coverage based on Lipschitz continuity. Although
97"
RELATED WORK,0.1984126984126984,"Lipschitz continuity guarantees that the difference between the model’s predictions is less than the
98"
RELATED WORK,0.2003968253968254,"product of the Lipschitz constant and the difference between inputs, it does not guarantee that their
99"
RELATED WORK,0.20238095238095238,"predictions fall into the same class. In practice, we cannot determine the true coverage because we
100"
RELATED WORK,0.20436507936507936,"do not know the distance threshold beyond which the model would change its predicted class for
101"
RELATED WORK,0.20634920634920634,"unlabeled samples.
102"
RELATED WORK,0.20833333333333334,"Therefore, the current solution is to minimize the coverage radius assuming full coverage [32] or to
103"
RELATED WORK,0.21031746031746032,"maximize coverage based on high purity coverage [42], where purity refers to the probability that the
104"
RELATED WORK,0.2123015873015873,"sample has the same label within a given distance. Assuming full coverage leads to an overestimated
105"
RELATED WORK,0.21428571428571427,"coverage as shown in fig. 1a, i.e., some covered samples still have a large empirical risk, while high-
106"
RELATED WORK,0.21626984126984128,"purity coverage causes underestimated coverage as shown in fig. 1b. The overestimated coverage
107"
RELATED WORK,0.21825396825396826,"may cause the active learning algorithm to miss samples in areas that are not truly covered, while
108"
RELATED WORK,0.22023809523809523,"underestimated coverage makes active learning algorithms likely to select redundant samples. These
109"
RELATED WORK,0.2222222222222222,"affect the performance of active learning.
110"
RELATED WORK,0.22420634920634921,"(a) Coreset
(b) Probcover
(c) NTKCPL
(d) Neural Network"
RELATED WORK,0.2261904761904762,"Figure 1: Coverage estimation based on sample feature distance vs. NTKCPL. Here different colors
represent different categories, the black star denotes labeled samples and the blue circle represents the
samples considered covered based on the feature distance approach. Coreset assumes full coverage
and Probcover assumes high purity coverage. The coverage estimated by our method, NTKCPL, and
true coverage based on predictions of the neural network is represented by black dots. The coverage
estimated by NTKCPL is more consistent with the true coverage of the neural network than those
estimated based on feature distances."
RELATED WORK,0.22817460317460317,"Additionally, estimating the empirical risk based on distance implies the assumption that model
111"
RELATED WORK,0.23015873015873015,"predictions are only relevant to the nearest labeled sample, which is often not the case in reality. To
112"
RELATED WORK,0.23214285714285715,"estimate the true coverage, we propose a new strategy, NTKCPL. It estimates the empirical risk based
113"
RELATED WORK,0.23412698412698413,"on the predictions of the model trained on the candidate set and pseudo-labels.
114"
RELATED WORK,0.2361111111111111,"3
Method: NTKCPL
115"
RELATED WORK,0.23809523809523808,"In sec. 3.1, we briefly review the Neural Tangent Kernel (NTK) [18] that enables active learning
116"
RELATED WORK,0.2400793650793651,"strategies based on the outputs of a model trained on a candidate set feasible. Then, we propose our
117"
RELATED WORK,0.24206349206349206,"active learning strategy, NTKCPL, in sec. 3.2 and analyze the approximation error of NTKCPL in
118"
RELATED WORK,0.24404761904761904,"sec. 3.3. Finally, based on the analysis, we introduce the method of generating cluster pseudo-label in
119"
RELATED WORK,0.24603174603174602,"sec. 3.4.
120"
PRELIMINARIES,0.24801587301587302,"3.1
Preliminaries
121"
PRELIMINARIES,0.25,"Neural Tangent Kernel (NTK) is a powerful tool to analyze the training dynamics of neural network.
122"
PRELIMINARIES,0.251984126984127,"Jacot et al. [18] show that the neural network is equivalent to the kernel regression with Neural
123"
PRELIMINARIES,0.25396825396825395,"Tangent Kernel when network is sufficiently wide and its weights are initialized properly [2]. The
124"
PRELIMINARIES,0.25595238095238093,"NTK, K, is shown in eq. 2, where the f denotes a neural network with parameters θ and X denotes
125"
PRELIMINARIES,0.25793650793650796,"train samples. When training with MSE loss, the neural network has a closed-form solution for the
126"
PRELIMINARIES,0.25992063492063494,"prediction of test sample x at iteration t as eq. 3, where Y denotes labels of trainset and f0 denotes
127"
PRELIMINARIES,0.2619047619047619,"the output of network with initialized weights.
128"
PRELIMINARIES,0.2638888888888889,"K(X, X) = ∇θf(X)∇θf(X)T
(2)"
PRELIMINARIES,0.26587301587301587,"ft(x) = f0(x) + K(x, X) K(X, X)−1(I −e−tK(X,X))(Y −f0(X)),
(3)"
PRELIMINARIES,0.26785714285714285,"Additionally, for active learning scenarios, Mohamad [26, 27] proposes the computation time of
129"
PRELIMINARIES,0.2698412698412698,"using NTK can be further reduced by considering the block structure of the matrix, which means
130"
PRELIMINARIES,0.2718253968253968,"that look ahead type active learning strategies can be implemented in a reasonable amount of time.
131"
PRELIMINARIES,0.27380952380952384,"For example, as shown in [26], if we want to use the look ahead active learning strategy, each active
132"
PRELIMINARIES,0.2757936507936508,"learning cycle takes 3 hours to train the entire network of 15 epochs on the MNIST dataset, while it
133"
PRELIMINARIES,0.2777777777777778,"takes only 3 minutes to use NTK with a block structure.
134"
FRAMEWORK,0.27976190476190477,"3.2
Framework
135"
FRAMEWORK,0.28174603174603174,"We propose a look ahead strategy, NTKCPL, to approximate the empirical risk on the whole active
136"
FRAMEWORK,0.2837301587301587,"learning pool directly. There are two challenges: (1) estimate empirical risk without labels and (2)
137"
FRAMEWORK,0.2857142857142857,"estimate predictions of models trained with candidate sets efficiently and accurately.
138"
FRAMEWORK,0.2876984126984127,"For the first challenge, clusters on self-supervised features provide good pseudo-labels. Because most
139"
FRAMEWORK,0.2896825396825397,"samples in the same cluster have the same label [39]. And when the number of clusters is increased,
140"
FRAMEWORK,0.2916666666666667,"it can improve the purity of clusters, where purity refers to the probability that the sample has the
141"
FRAMEWORK,0.29365079365079366,"same label within the same cluster. We call these clusters clustering-pseudo-labels (CPL), ycpl.
142"
FRAMEWORK,0.29563492063492064,"For the second challenge, as introduced in sec. 3.1, NTK approximates the network well for random
143"
FRAMEWORK,0.2976190476190476,"initialization and the computation time is acceptable. However, in our scenario, training on top of the
144"
FRAMEWORK,0.2996031746031746,"self-supervised model, NTK does not approximate predictions of the whole network well. The main
145"
FRAMEWORK,0.30158730158730157,"reason is that weights of the neural network are initialized by self-supervised learning rather than
146"
FRAMEWORK,0.30357142857142855,"NTK initialization, i.e., drawn i.i.d. from a standard Gaussian [18]. In addition, the self-supervised
147"
FRAMEWORK,0.3055555555555556,"initialization provides the neural network with a powerful feature representation capability that is not
148"
FRAMEWORK,0.30753968253968256,"available in NTK. This leads to inconsistency between NTK predictions and network outputs. So, in
149"
FRAMEWORK,0.30952380952380953,"our method, the NTK is used to approximate the classifier instead of the whole network. And the
150"
FRAMEWORK,0.3115079365079365,"inputs of NTK are self-supervised features. Accordingly, we choose a training method following [24]
151"
FRAMEWORK,0.3134920634920635,"that freezes the encoder initialized by self-supervised learning and trains only the MLP as a classifier.
152"
FRAMEWORK,0.31547619047619047,"That training method achieves better or equal performance than fine-tuning the whole network in
153"
FRAMEWORK,0.31746031746031744,"the low-budget case while its prediction is more consistent with the results of NTK. We denotes the
154"
FRAMEWORK,0.3194444444444444,"predictions of NTK with trainset DC as ˆfDC. Now, the active learning goal in eq. 1 is approximated
155"
FRAMEWORK,0.32142857142857145,"as eq. 4.
156"
FRAMEWORK,0.32341269841269843,"argminDC
1
N X"
FRAMEWORK,0.3253968253968254,"i∈D
Loss( ˆfDC(xi), ycpl,i)
(4)"
FRAMEWORK,0.3273809523809524,"The algorithm is shown in Alg. 1. For computational simplicity and without loss of generality, we
157"
FRAMEWORK,0.32936507936507936,"use 0-1 loss to calculate empirical risk in eq. 4. In each round of active learning, after computation of
158"
FRAMEWORK,0.33134920634920634,"NTK based on eq. 2 and generation of CPL based on the method introduced in sec. 3.4, the sample
159"
FRAMEWORK,0.3333333333333333,"that minimizes the empirical risk on the whole active learning pool after adding labeled set is selected.
160"
NTKCPL APPROXIMATE ERROR,0.3353174603174603,"3.3
NTKCPL Approximate Error
161"
NTKCPL APPROXIMATE ERROR,0.3373015873015873,"In this section, we analyze what affects the accuracy of NTKCPL estimates of empirical risk on
162"
NTKCPL APPROXIMATE ERROR,0.3392857142857143,"the whole active learning pool. The difference between the true empirical risk and the estimated
163"
NTKCPL APPROXIMATE ERROR,0.3412698412698413,Algorithm 1 NTKCPL
NTKCPL APPROXIMATE ERROR,0.34325396825396826,"1: Input: self-supervised feature fself, active learning feature fal labeled set L, unlabeled set U, budget b,
initial budget b0, maximum cluster number Cmax, model prediction Ypre,t−1 at the last active learning
round
2: Output: labeled set L, model prediction Ypre,t at this round
3: if L is ∅then
4:
Ycpl ←K-means(fself, b0)
5: else
6:
Nclu = min{bi/2, Cmax}
7:
Ycpl ←CPL generation(fal, Ypre,t−1, b0, Nclu, L) based on Alg. 2
8: end if
9: Initialize classifier, MLP, compute f0 and ker based on eq. 2
10: for itr = 1 to b do
11:
Emp_risk = []
12:
for (xi, ycpl,i) in U do
13:
Compute YNT K = ˆf(ker, f0, L ∪(xi, ycpl,i), U) based on eq. 3
14:
Emp_risk + = [0-1Loss(YNT K, Ycpl)]
15:
end for
16:
i′ = argminEmp_risk
17:
L = L ∪(xi′, ycpl,i′), U = U\xi′
18: end for
19: Query label yi′
1,...,b of xi′
1,...,b
20: L = L ∪(xi′
1,...,b, yi′
1,...,b), U = U\xi′
1,...,b
21: Train classifier ft on L
22: model prediction Ypre,t = ft(U)"
NTKCPL APPROXIMATE ERROR,0.34523809523809523,"empirical risk using NTK and CPL is shown in eq. 5. The approximation error can be divided into
164"
NTKCPL APPROXIMATE ERROR,0.3472222222222222,"two terms, the first one is the difference between NTK and neural network prediction, errorNT K,
165"
NTKCPL APPROXIMATE ERROR,0.3492063492063492,"and the second one is the difference caused by CPL during NTK estimation, errorCP L. For the
166"
NTKCPL APPROXIMATE ERROR,0.35119047619047616,"errorNT K, as we mentioned in sec. 3.2, NTK is used to approximate the classifier only to obtain
167"
NTKCPL APPROXIMATE ERROR,0.3531746031746032,"better consistency. To analyze errorCP L, we start with the relationship between the predictions of
168"
NTKCPL APPROXIMATE ERROR,0.3551587301587302,"NTK trained with the ground truth, ˆfy(xi), and CPL, ˆfcpl(xi).
169"
N,0.35714285714285715,"1
N X i∈D"
N,0.35912698412698413,"Loss(f(xi), yi) −Loss( ˆf(xi), ycpl,i) ≤
1
N X"
N,0.3611111111111111,"i∈D
(
Loss(f(xi), yi) −Loss( ˆf(xi), yi)
 +
Loss( ˆf(xi), yi) −Loss( ˆf(xi), ycpl,i)
)
(5)"
N,0.3630952380952381,"Definition
Denotes the jth output of ˆfcpl as ˆf j
cpl. Label mapping function g converts NTK’s
170"
N,0.36507936507936506,"predictions about CPL classes, ˆfcpl(xi), into predictions about true classes, ˆfymap(xi), based on
171"
N,0.36706349206349204,"dominant labels within corresponding CPL classes as shown in eq. 6, where Ddom is a set of index k,
172"
N,0.36904761904761907,"where j is the dominant true label classes within CPL class, ycpl,k.
173"
N,0.37103174603174605,"ˆf j
ymap(xi) =
X"
N,0.373015873015873,k∈Ddom
N,0.375,"ˆf k
cpl(xi)
(6)"
N,0.376984126984127,"Proposition
If the true labels of labeled samples are the dominant labels in their corresponding
174"
N,0.37896825396825395,"CPL clusters, ˆfy(xi) = g( ˆfcpl(xi)). We defer the proof to appendix 1.
175"
N,0.38095238095238093,"errorCP L = Pnff + Pfnf
(7)"
N,0.38293650793650796,"As mentioned in sec. 3.2, we use 0-1 loss to calculate empirical risk. We can expand errorCP L as
176"
N,0.38492063492063494,"eq. 7, where we denote the probability that the NTK prediction agrees with the y but not with ycpl as
177"
N,0.3869047619047619,"Pfnf, and the probability that the NTK prediction does not agree with y but agrees with ycpl as Pnff.
178"
N,0.3888888888888889,"According to the proposition, we argue argmax ˆfy(xi) is most likely equal to g(argmax ˆfcpl(xi)).
179"
N,0.39087301587301587,"Pfnf refers to the case where different CPL classes correspond to the same true label class, i.e.,
180"
N,0.39285714285714285,"over-clustering. Pnff means that the true label of a sample is different from the dominant true label
181"
N,0.3948412698412698,"within its CPL class, i.e., the CPL class includes samples from different true label classes, which is
182"
N,0.3968253968253968,"called impurity. Detailed explanations and empirical evidence can be found in appendix 1.
183"
CLUSTER PSEUDO-LABELS,0.39880952380952384,"3.4
Cluster Pseudo-Labels
184"
CLUSTER PSEUDO-LABELS,0.4007936507936508,"As shown by eq. 7, the effect of CPL on the approximation error comes from the purity of the clusters
185"
CLUSTER PSEUDO-LABELS,0.4027777777777778,"and over-clustering. To improve clustering purity, we take two approaches: (1) clustering on the
186"
CLUSTER PSEUDO-LABELS,0.40476190476190477,"active learning feature, i.e., the output of the penultimate layer of the classifier, and (2) increasing
187"
CLUSTER PSEUDO-LABELS,0.40674603174603174,"the number of clusters. However, increasing the number of clusters may cause the labeled samples
188"
CLUSTER PSEUDO-LABELS,0.4087301587301587,"not to cover all classes of the CPL (under-coverage) and also increase the over-clustering error. For
189"
CLUSTER PSEUDO-LABELS,0.4107142857142857,"example, a group of samples with the same true label is clustered into K different classes. Even
190"
CLUSTER PSEUDO-LABELS,0.4126984126984127,"though NTK incorrectly predicts some samples as other CPL classes, their true empirical risk is zero.
191"
CLUSTER PSEUDO-LABELS,0.4146825396825397,Algorithm 2 CPL generation
CLUSTER PSEUDO-LABELS,0.4166666666666667,"Input: active learning feature fal, model predictions
Ypre, initial cluster number C0, cluster number Cmax,
labeled set L
Output: CPL Ycpl
Clu1,...,C0 ←Constrained K-means(fal, L, C0)
for itr = 1 to (Cmax −C0) do"
CLUSTER PSEUDO-LABELS,0.41865079365079366,"i′ = argmaxi number of Confusing samples(Clui,
Ypre)
fal,i′ ←fal of samples within Clui′
Clui′,C0+1 ←K-means(fal,i′, 2 )
C0 ←C0 + 1
end for
Ycpl ←Clu1,...,Cmax"
CLUSTER PSEUDO-LABELS,0.42063492063492064,"To improve the under-coverage, we set the num-
192"
CLUSTER PSEUDO-LABELS,0.4226190476190476,"ber of clusters to half of the total number of la-
193"
CLUSTER PSEUDO-LABELS,0.4246031746031746,"bels, i.e., each cluster includes two labeled sam-
194"
CLUSTER PSEUDO-LABELS,0.42658730158730157,"ples on average. To improve the over-clustering,
195"
CLUSTER PSEUDO-LABELS,0.42857142857142855,"we manually set the maximum number of clus-
196"
CLUSTER PSEUDO-LABELS,0.4305555555555556,"ters and design a clustering-splitting approach
197"
CLUSTER PSEUDO-LABELS,0.43253968253968256,"instead of directly increasing the number of clus-
198"
CLUSTER PSEUDO-LABELS,0.43452380952380953,"ters. It splits the low-purity clusters and keeps
199"
CLUSTER PSEUDO-LABELS,0.4365079365079365,"the high-purity ones to reduce the extra over-
200"
CLUSTER PSEUDO-LABELS,0.4384920634920635,"clustering errors within samples located in the
201"
CLUSTER PSEUDO-LABELS,0.44047619047619047,"high-purity clusters. Specifically, we use the pre-
202"
CLUSTER PSEUDO-LABELS,0.44246031746031744,"diction of the neural network in each round of
203"
CLUSTER PSEUDO-LABELS,0.4444444444444444,"active learning to estimate the number of confus-
204"
CLUSTER PSEUDO-LABELS,0.44642857142857145,"ing samples within each cluster, i.e., the number
205"
CLUSTER PSEUDO-LABELS,0.44841269841269843,"of samples from classes that are different from
206"
CLUSTER PSEUDO-LABELS,0.4503968253968254,"the dominant class. The clusters that contain the
207"
CLUSTER PSEUDO-LABELS,0.4523809523809524,"largest number of confusing samples are split sequentially until a predefined number of clusters
208"
CLUSTER PSEUDO-LABELS,0.45436507936507936,"is reached. The cluster splitting algorithm is shown in Alg. 2, where we adopt the constrained
209"
CLUSTER PSEUDO-LABELS,0.45634920634920634,"K-Means [37] to improve the clusters from labeled sample constraints.
210"
EXPERIMENT RESULTS,0.4583333333333333,"4
Experiment Results
211"
EXPERIMENT RESULTS,0.4603174603174603,"Our approach is validated on five datasets with various qualities of self-supervised features. Datasets
212"
EXPERIMENT RESULTS,0.4623015873015873,"with good self-supervised features, such as CIFAR-10 [21], CIFAR-100 [21], and ImageNet-100
213"
EXPERIMENT RESULTS,0.4642857142857143,"(a subset of ImageNet [11], following splitting in [36]), are included. SVHN [29] with poor self-
214"
EXPERIMENT RESULTS,0.4662698412698413,"supervised features is also included. Additionally, we consider practical scenarios where the total
215"
EXPERIMENT RESULTS,0.46825396825396826,"number of samples in the trainset is insufficient to support effective self-supervised training, such as
216"
EXPERIMENT RESULTS,0.47023809523809523,"Oxford-IIIT Pet dataset [30]. In this case, we evaluated the effectiveness of our method based on the
217"
EXPERIMENT RESULTS,0.4722222222222222,"model pre-trained on ImageNet [11].
218"
EXPERIMENT RESULTS,0.4742063492063492,"Baseline
We compare our proposed method with representative active learning strategies: (1)
219"
EXPERIMENT RESULTS,0.47619047619047616,"Random, (2) Entropy (uncertainty sampling, maximum entropy of output) [22], (3) Coreset (diversity
220"
EXPERIMENT RESULTS,0.4781746031746032,"active learning strategy, greedy solution of minimum coverage radius) [32], (4) BADGE (combination
221"
EXPERIMENT RESULTS,0.4801587301587302,"of uncertainty and diversity, kmeans++ sampling on grad embedding) [3], where the scalable ver-
222"
EXPERIMENT RESULTS,0.48214285714285715,"sion [10, 12], badge partition, is used in ImageNet-100, CIFAR-100 and Oxford-IIIT Pet because the
223"
EXPERIMENT RESULTS,0.48412698412698413,"huge dimension of grad embedding (5) Typiclust (designed for low-budget case) [15], (6) Lookahead
224"
EXPERIMENT RESULTS,0.4861111111111111,"(maximum output change based on NTK) [26].
225"
EXPERIMENT RESULTS,0.4880952380952381,"Implementation
Our method focuses on the low-budget regime, we followed the training method
226"
EXPERIMENT RESULTS,0.49007936507936506,"in [24], freezing weights of backbone initialized with self-supervised learning and then training a
227"
EXPERIMENT RESULTS,0.49206349206349204,"MLP as the classifier. The hyperparameters for training are set following [15] and can be found in
228"
EXPERIMENT RESULTS,0.49404761904761907,"appendix 3. For the self-supervised model, we adopt simsiam [9] for CIFAR-10, CIFAR-100 and
229"
EXPERIMENT RESULTS,0.49603174603174605,"SVHN and BYOL [14] for ImageNet-100 and Oxford-IIIT Pet. Resnet-18 [17] is used in CIFAR-10
230"
EXPERIMENT RESULTS,0.498015873015873,"Table 1: Comparison of accuracy of different active learning strategies on CIFAR-10. All results are
averages over 5 runs. The best results are shown in red and the second-best results are shown in blue."
EXPERIMENT RESULTS,0.5,# Labels
EXPERIMENT RESULTS,0.501984126984127,Random
EXPERIMENT RESULTS,0.503968253968254,Entropy
EXPERIMENT RESULTS,0.5059523809523809,Coreset(self) BADGE
EXPERIMENT RESULTS,0.5079365079365079,TypiClust
EXPERIMENT RESULTS,0.5099206349206349,LookAhead
EXPERIMENT RESULTS,0.5119047619047619,NTKCPL(self)
EXPERIMENT RESULTS,0.5138888888888888,NTKCPL(al)
EXPERIMENT RESULTS,0.5158730158730159,"20
41.80±3.82
38.58±2.86
20.08±2.75
39.85±3.91
46.38±1.61
40.93±4.04
54.31±3.74
52.67±3.70
40
57.52±3.34
51.10±4.21
36.67±6.29
54.99±3.43
66.18±2.45
58.55±2.71
68.60±2.50
63.55±2.89
60
65.88±3.07
64.46±3.42
46.39±7.41
65.23±1.40
72.93±1.77
66.96±2.90
75.09±1.69
72.22±2.11
80
69.35±3.31
70.49±3.05
58.96±6.15
70.76±1.86
76.98±1.04
72.71±1.94
78.51±1.61
75.32±0.92
100
74.11±1.16
74.34±1.92
62.64±5.07
75.40±0.99
78.24±1.28
75.97±2.04
80.30±1.17
78.45±1.19
200
80.90±0.90
79.86±1.77
76.93±3.56
82.20±1.14
83.16±0.61
81.89±1.31
83.77±1.04
81.87±1.02
300
82.80±0.93
81.43±2.23
82.64±1.42
84.53±0.46
84.16±0.25
83.29±0.89
85.00±0.54
83.78±1.05
400
84.04±0.49
83.37±1.31
84.56±1.15
84.75±0.40
85.13±0.27
84.59±0.59
85.64±0.38
84.73±0.85
500
84.97±0.78
84.24±0.89
85.23±0.59
85.57±0.51
85.37±0.15
85.31±0.12
85.72±0.22
85.48±0.65
1000
86.26±0.38
84.94±0.48
86.75±0.36
86.06±0.31
86.07±0.14
85.69±0.47
86.83±0.33
87.15±0.57
1500
86.95±0.27
85.85±0.39
87.03±0.13
87.05±0.36
86.37±0.11
86.82±0.23
87.18±0.41
87.58±0.29
2000
87.30±0.37
86.92±0.15
87.34±0.27
87.31±0.47
86.55±0.21
87.16±0.19
87.34±0.41
87.87±0.39"
EXPERIMENT RESULTS,0.5178571428571429,"and SVHN, WRN28-8 [44] is used in CIFAR-100 and Resnet-50 [17] is used in ImageNet-100 and
231"
EXPERIMENT RESULTS,0.5198412698412699,"Oxford-IIIT Pet.
232"
EXPERIMENT RESULTS,0.5218253968253969,"The number of clusters in our method is set according to three rules, in the initial selection, it is
233"
EXPERIMENT RESULTS,0.5238095238095238,"set to the number of query samples, after that it is set to half of the query samples until the number
234"
EXPERIMENT RESULTS,0.5257936507936508,"of clusters reaches the maximum number of clusters. For CIFAR-10, CIFAR-100, ImageNet-100,
235"
EXPERIMENT RESULTS,0.5277777777777778,"SVHN, and Oxford-IIIT Pet, the maximum number of clusters is 100, 500, 300, 100, and 150,
236"
EXPERIMENT RESULTS,0.5297619047619048,"respectively. We followed [26] to sample a subset of the unlabeled set as the candidate set to select
237"
EXPERIMENT RESULTS,0.5317460317460317,"samples and estimate coverage. The candidate set includes 10,000 samples.
238"
EXPERIMENT RESULTS,0.5337301587301587,"For the query step, most of the experiments (those on CIFAR-100, SVHN and Oxford-IIIT Pet)
239"
EXPERIMENT RESULTS,0.5357142857142857,"following the active learning literature by drawing a fixed number of samples from the unlabeled
240"
EXPERIMENT RESULTS,0.5376984126984127,"dataset to the oracle. Specifically, 500 for CIFAR-100, 20 for SVHN, and 40 for Oxford-IIIT Pet.
241"
EXPERIMENT RESULTS,0.5396825396825397,"We empirically found that fixed active learning query steps lead to much faster growth of classifier
242"
EXPERIMENT RESULTS,0.5416666666666666,"accuracy in the early stages of active learning (the amount of labels is about 10 times than the number
243"
EXPERIMENT RESULTS,0.5436507936507936,"of class) than in the later stages, so it is difficult to clearly observe the differences between different
244"
EXPERIMENT RESULTS,0.5456349206349206,"active learning strategies. For this reason, we empirically set varying query steps in our experiments
245"
EXPERIMENT RESULTS,0.5476190476190477,"with CIFAR-10 and ImageNet-100. Smaller query steps were used in the early stage of active learning
246"
EXPERIMENT RESULTS,0.5496031746031746,"and switched to larger query steps in the later stage. Specifically, for CIFAR-10, 20 samples are
247"
EXPERIMENT RESULTS,0.5515873015873016,"queried before 100 labels are available, 100 samples are queried before 500 labels and 500 labels
248"
EXPERIMENT RESULTS,0.5535714285714286,"are queried before 2000 labels. For ImageNet-100, 200 samples are selected before 1000 labels are
249"
EXPERIMENT RESULTS,0.5555555555555556,"available and 500 samples are queried before 2000 labels.
250"
MAIN RESULTS,0.5575396825396826,"4.1
Main Results
251"
MAIN RESULTS,0.5595238095238095,"All experiments were run 5 times and the avg. and std. are reported. Considering that the experiments
252"
MAIN RESULTS,0.5615079365079365,"are conducted for the scenario with a low annotation budget, it is not practical to construct a validation
253"
MAIN RESULTS,0.5634920634920635,"set to select the best checkpoints (the benefits of constructing a validation set are much less than
254"
MAIN RESULTS,0.5654761904761905,"using these labeled samples as training samples). Therefore, we report the final checkpoint accuracy,
255"
MAIN RESULTS,0.5674603174603174,"not the accuracy of the checkpoints determined by the validation set. The results are shown in fig. 2
256"
MAIN RESULTS,0.5694444444444444,"and table 1. The detailed results are in appendix 5.
257"
MAIN RESULTS,0.5714285714285714,"NTKCPL outperforms SOTA.
As shown in table 1, fig. 2. In most cases, our proposed method
258"
MAIN RESULTS,0.5734126984126984,"outperforms the baseline methods. For the few cases with only a small number of labels, our method
259"
MAIN RESULTS,0.5753968253968254,"shows comparable performance with the low-budget strategy, TypiClust, such as in CIFAR-100 with
260"
MAIN RESULTS,0.5773809523809523,"500 and 1000 labeled samples, and Oxford-IIIT Pet with 80 and 100 labeled samples.
261"
MAIN RESULTS,0.5793650793650794,"NTKCPL still shows good performance when the self-supervised features do not correspond
262"
MAIN RESULTS,0.5813492063492064,"well to the label classes.
Since the loss of self-supervised training is different from that of image
263"
MAIN RESULTS,0.5833333333333334,"classification, self-supervised features do not always correspond well with label classes. In SVHN
264"
MAIN RESULTS,0.5853174603174603,"dataset, self-supervised features of different classes are mixed together because the images include
265"
MAIN RESULTS,0.5873015873015873,"some irrelevant digits on both sides of the digit of interest [29]. Our method is similar to other
266"
MAIN RESULTS,0.5892857142857143,"1000
2000
3000
4000
Label Amount 45 50 55 60"
MAIN RESULTS,0.5912698412698413,Accuracy / %
MAIN RESULTS,0.5932539682539683,"Random
Coreset(al)
Coreset(self)
TypiClust
Entropy
BADGE
NTKCPL(al)"
MAIN RESULTS,0.5952380952380952,(a) CIFAR-100
MAIN RESULTS,0.5972222222222222,"500
1000
1500
2000
Label Amount 50 60 70 80"
MAIN RESULTS,0.5992063492063492,Accuracy / %
MAIN RESULTS,0.6011904761904762,"Random
Coreset(self)
TypiClust
Entropy
LookAhead
BADGE
NTKCPL(self)
NTKCPL(al)"
MAIN RESULTS,0.6031746031746031,(b) ImageNet-100
MAIN RESULTS,0.6051587301587301,"50
100
150
200
Label Amount 40 60 80"
MAIN RESULTS,0.6071428571428571,Accuracy / %
MAIN RESULTS,0.6091269841269841,"Random
Coreset(self)
TypiClust
Entropy
LookAhead
BADGE
NTKCPL(self)
NTKCPL(al)"
MAIN RESULTS,0.6111111111111112,(c) SVHN
MAIN RESULTS,0.6130952380952381,"100
200
300
400
Label Amount 40 50 60 70 80"
MAIN RESULTS,0.6150793650793651,Accuracy / %
MAIN RESULTS,0.6170634920634921,"Random
Coreset(self)
Coreset(al)
TypiClust
Entropy
BADGE
NTKCPL(self)
NTKCPL(al)"
MAIN RESULTS,0.6190476190476191,(d) Oxford-IIIT Pet
MAIN RESULTS,0.621031746031746,Figure 2: Performance of different active learning strategies. The shaded area represents std.
MAIN RESULTS,0.623015873015873,"baseline strategies at the beginning of active learning, but it shows better results than baselines after
267"
MAIN RESULTS,0.625,"several active learning rounds as shown in fig. 2c.
268"
MAIN RESULTS,0.626984126984127,"Another common scenario is the lack of sufficient samples to support effective self-supervised training.
269"
MAIN RESULTS,0.628968253968254,"To evaluate in this context, we choose the Oxford-IIIT Pet dataset with the self-supervised model
270"
MAIN RESULTS,0.6309523809523809,"trained on ImageNet. The result is shown in fig. 2d. Our method has similar accuracy in the first three
271"
MAIN RESULTS,0.6329365079365079,"rounds as the TypiClust and outperforms all baseline methods afterward.
272"
MAIN RESULTS,0.6349206349206349,"Table 2: Comparison of the effective bud-
get ratio of different active learning strate-
gies."
MAIN RESULTS,0.6369047619047619,Effective Budget Ratio
MAIN RESULTS,0.6388888888888888,"TypiClust
40.8%
BADGE
42.0%
NTKCPL(al)
92.7%"
MAIN RESULTS,0.6408730158730159,"NTKCPL has a wider effective budget range than
273"
MAIN RESULTS,0.6428571428571429,"SOTA.
Active learning based on self-supervised mod-
274"
MAIN RESULTS,0.6448412698412699,"els exhibits an intensified phase transition phenomenon.
275"
MAIN RESULTS,0.6468253968253969,"We plot the active learning gain of our method and base-
276"
MAIN RESULTS,0.6488095238095238,"lines on different datasets in fig. 3. The average accuracy
277"
MAIN RESULTS,0.6507936507936508,"of our method, NTKCPL(al), outperforms the random
278"
MAIN RESULTS,0.6527777777777778,"baseline at all quantities of labels. In contrast, both the
279"
MAIN RESULTS,0.6547619047619048,"typical high-budget strategy, BADGE, and low-budget
280"
MAIN RESULTS,0.6567460317460317,"strategy, TypiClust, appear to be worse than the random
281"
MAIN RESULTS,0.6587301587301587,"baseline over a range of annotation quantities. We show
282"
MAIN RESULTS,0.6607142857142857,"the effective budget range of our method, NTKCPL, as well as the typical high-budget strategy,
283"
MAIN RESULTS,0.6626984126984127,"BADGE, and the typical low-budget strategy, TypiClust, across all experiments in table 2. The effec-
284"
MAIN RESULTS,0.6646825396825397,"tive budget ratio refers to the proportion of the effective annotation quantity to the total annotation
285"
MAIN RESULTS,0.6666666666666666,"quantity, where the effective annotation quantity refers to the number of annotations at which active
286"
MAIN RESULTS,0.6686507936507936,"learning accuracy exceeds the random baseline (avg. + std.).
287"
ABLATION STUDY,0.6706349206349206,"4.2
Ablation Study
288"
ABLATION STUDY,0.6726190476190477,"In this section, we evaluate the coverage estimation of our method and the effect of the maximum
289"
ABLATION STUDY,0.6746031746031746,"cluster number on NTKCPL. Also, we compare the effect of generating CPL on self-supervised
290"
ABLATION STUDY,0.6765873015873016,"features as well as on the active learning feature on the performance of NTKCPL.
291"
ABLATION STUDY,0.6785714285714286,"1000
2000
3000
4000
Label Amount −4 −2 0 2 4 6"
ABLATION STUDY,0.6805555555555556,Active learning Gain / %
ABLATION STUDY,0.6825396825396826,(a) CIFAR-100
ABLATION STUDY,0.6845238095238095,"500
1000
1500
2000
Label Amount −10 0 10"
ABLATION STUDY,0.6865079365079365,Active learning Gain / %
ABLATION STUDY,0.6884920634920635,(b) ImageNet-100
ABLATION STUDY,0.6904761904761905,"50
100
150
200
Label Amount −10 0 10"
ABLATION STUDY,0.6924603174603174,Active learning Gain / %
ABLATION STUDY,0.6944444444444444,(c) SVHN
ABLATION STUDY,0.6964285714285714,"100
200
300
400
Label Amount −10 −5 0 5 10 15"
ABLATION STUDY,0.6984126984126984,Active learning Gain / %
ABLATION STUDY,0.7003968253968254,(d) Oxford-IIIT Pet
ABLATION STUDY,0.7023809523809523,Figure 3: Active learning gain of different active learning strategies.
ABLATION STUDY,0.7043650793650794,"1000
2000
3000
4000
Label Amount 50 55 60 65 70"
ABLATION STUDY,0.7063492063492064,Coverage / %
ABLATION STUDY,0.7083333333333334,"NTK with ture label
NTK with CPL
NN with true label"
ABLATION STUDY,0.7103174603174603,"Figure 4:
Coverage estimation on
CIFAR-100."
ABLATION STUDY,0.7123015873015873,"500
1000
1500
2000
Label Amount 84 85 86 87 88"
ABLATION STUDY,0.7142857142857143,Accuracy / %
ABLATION STUDY,0.7162698412698413,"max # cluster 10
max # cluster 50
max # cluster 100
max # cluster 200"
ABLATION STUDY,0.7182539682539683,"Figure 5: Effect of the maximum number of clus-
ters on active learning performance on CIFAR-10."
ABLATION STUDY,0.7202380952380952,"Coverage Estimation
We conducted experiments on CIFAR-100, where the coverage indicates the
292"
ABLATION STUDY,0.7222222222222222,"proportion of samples that are correctly predicted. The estimated coverage of NTK with true label
293"
ABLATION STUDY,0.7242063492063492,"and with CPL is shown in fig. 4. Our method approximates the true coverage well for most cases.
294"
ABLATION STUDY,0.7261904761904762,"Effect of the Maximum Number of CPL
The ablation experiments are conducted on CIFAR-10.
295"
ABLATION STUDY,0.7281746031746031,"We plot the accuracy when the number of annotations selected by active learning is greater than
296"
ABLATION STUDY,0.7301587301587301,"400 as shown in fig. 5. In this range, the number of classes of CPL is fixed at 10, 50, 100, and
297"
ABLATION STUDY,0.7321428571428571,"200, respectively. The experimental results support our analysis in sec. 3.4 that too many or too few
298"
ABLATION STUDY,0.7341269841269841,"clusters will increase the approximation error, which affects the performance of active learning.
299"
ABLATION STUDY,0.7361111111111112,"Effect of self-supervised feature-based and active learning feature-based clustering-pseudo-
300"
ABLATION STUDY,0.7380952380952381,"labels on NTKCPL.
We denote NTKCPL based on active learning features as NTKCPL(al) and
301"
ABLATION STUDY,0.7400793650793651,"NTKCPL based on self-supervised learning feature as NTKCPL(self). The results are shown in
302"
ABLATION STUDY,0.7420634920634921,"table 1 and fig. 2. From these experiments, we found that clustering on active learning features yields
303"
ABLATION STUDY,0.7440476190476191,"better results except for the case where the number of annotations is very small. Also, NTKCPL(self)
304"
ABLATION STUDY,0.746031746031746,"is better than NTKCPL(al) in a wide range of annotation quantities (no more than 500), when
305"
ABLATION STUDY,0.748015873015873,"self-supervised features are good such as experiment in the CIFAR-10.
306"
CONCLUSION,0.75,"5
Conclusion
307"
CONCLUSION,0.751984126984127,"We study the active learning problem when training on top of a self-supervised model. In this case,
308"
CONCLUSION,0.753968253968254,"an intensified phase transition is observed and it influences the application of active learning. We
309"
CONCLUSION,0.7559523809523809,"propose NTKCPL that approximates empirical risk on the whole pool more directly. We also analyze
310"
CONCLUSION,0.7579365079365079,"the approximation error and design a CPL generation method based on the analysis to reduce the
311"
CONCLUSION,0.7599206349206349,"approximation error. Our method outperforms SOTA in most cases and has a wider effective budget
312"
CONCLUSION,0.7619047619047619,"range. The comprehensive experiments show that our method can work well on self-supervised
313"
CONCLUSION,0.7638888888888888,"features with different qualities.
314"
CONCLUSION,0.7658730158730159,"Our approach is limited to the fixed training approach, i.e., training the classifier on top of a frozen
315"
CONCLUSION,0.7678571428571429,"self-supervised training encoder, which is restricted to the low-budget scenario because the fine-
316"
CONCLUSION,0.7698412698412699,"tuning training approach provides higher accuracy in the high-budget case. Therefore, (1) how to
317"
CONCLUSION,0.7718253968253969,"accurately approximate the fine-tuning model initialized with self-supervised weights using NTK and
318"
CONCLUSION,0.7738095238095238,"(2) whether the samples selected by our current method have good transferability for the fine-tuning
319"
CONCLUSION,0.7757936507936508,"would be interesting future directions.
320"
REFERENCES,0.7777777777777778,"References
321"
REFERENCES,0.7797619047619048,"[1] Inigo Alonso, Matan Yuval, Gal Eyal, Tali Treibitz, and Ana C Murillo. Coralseg: Learning coral
322"
REFERENCES,0.7817460317460317,"segmentation from sparse annotations. Journal of Field Robotics, 36(8):1456–1477, 2019.
323"
REFERENCES,0.7837301587301587,"[2] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact
324"
REFERENCES,0.7857142857142857,"computation with an infinitely wide neural net. Advances in neural information processing systems, 32,
325"
REFERENCES,0.7876984126984127,"2019.
326"
REFERENCES,0.7896825396825397,"[3] Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch
327"
REFERENCES,0.7916666666666666,"active learning by diverse, uncertain gradient lower bounds. In International Conference on Learning
328"
REFERENCES,0.7936507936507936,"Representations, 2020.
329"
REFERENCES,0.7956349206349206,"[4] Javad Zolfaghari Bengar, Joost van de Weijer, Bartlomiej Twardowski, and Bogdan Raducanu. Reducing
330"
REFERENCES,0.7976190476190477,"label effort: Self-supervised meets active learning.
In Proceedings of the IEEE/CVF International
331"
REFERENCES,0.7996031746031746,"Conference on Computer Vision, pages 1631–1639, 2021.
332"
REFERENCES,0.8015873015873016,"[5] Zalán Borsos, Marco Tagliasacchi, and Andreas Krause. Semi-supervised batch active learning via bilevel
333"
REFERENCES,0.8035714285714286,"optimization. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal
334"
REFERENCES,0.8055555555555556,"Processing (ICASSP), pages 3495–3499. IEEE, 2021.
335"
REFERENCES,0.8075396825396826,"[6] Yao-Chun Chan, Mingchen Li, and Samet Oymak. On the marginal benefit of active learning: Does
336"
REFERENCES,0.8095238095238095,"self-supervision eat its cake? In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech
337"
REFERENCES,0.8115079365079365,"and Signal Processing (ICASSP), pages 3455–3459. IEEE, 2021.
338"
REFERENCES,0.8134920634920635,"[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
339"
REFERENCES,0.8154761904761905,"contrastive learning of visual representations. In International conference on machine learning, pages
340"
REFERENCES,0.8174603174603174,"1597–1607. PMLR, 2020.
341"
REFERENCES,0.8194444444444444,"[8] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-
342"
REFERENCES,0.8214285714285714,"supervised models are strong semi-supervised learners. Advances in neural information processing systems,
343"
REFERENCES,0.8234126984126984,"33:22243–22255, 2020.
344"
REFERENCES,0.8253968253968254,"[9] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the
345"
REFERENCES,0.8273809523809523,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15750–15758, 2021.
346"
REFERENCES,0.8293650793650794,"[10] Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin Ros-
347"
REFERENCES,0.8313492063492064,"tamizadeh, and Sanjiv Kumar. Batch active learning at scale. Advances in Neural Information Processing
348"
REFERENCES,0.8333333333333334,"Systems, 34:11933–11944, 2021.
349"
REFERENCES,0.8353174603174603,"[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
350"
REFERENCES,0.8373015873015873,"image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255.
351"
REFERENCES,0.8392857142857143,"Ieee, 2009.
352"
REFERENCES,0.8412698412698413,"[12] Zeyad Ali Sami Emam, Hong-Min Chu, Ping-Yeh Chiang, Wojciech Czaja, Richard Leapman, Micah
353"
REFERENCES,0.8432539682539683,"Goldblum, and Tom Goldstein. Active learning at the imagenet scale. arXiv preprint arXiv:2111.12880,
354"
REFERENCES,0.8452380952380952,"2021.
355"
REFERENCES,0.8472222222222222,"[13] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In
356"
REFERENCES,0.8492063492063492,"International Conference on Machine Learning, pages 1183–1192. PMLR, 2017.
357"
REFERENCES,0.8511904761904762,"[14] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,
358"
REFERENCES,0.8531746031746031,"Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own
359"
REFERENCES,0.8551587301587301,"latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems,
360"
REFERENCES,0.8571428571428571,"33:21271–21284, 2020.
361"
REFERENCES,0.8591269841269841,"[15] Guy Hacohen, Avihu Dekel, and Daphna Weinshall. Active learning on a budget: Opposite strategies suit
362"
REFERENCES,0.8611111111111112,"high and low budgets. In International Conference on Machine Learning, pages 8175–8195. PMLR, 2022.
363"
REFERENCES,0.8630952380952381,"[16] Guy Hacohen and Daphna Weinshall. Misal: Active learning for every budget. 2023.
364"
REFERENCES,0.8650793650793651,"[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
365"
REFERENCES,0.8670634920634921,"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
366"
REFERENCES,0.8690476190476191,"[18] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization
367"
REFERENCES,0.871031746031746,"in neural networks. Advances in neural information processing systems, 31, 2018.
368"
REFERENCES,0.873015873015873,"[19] Andreas Kirsch, Joost Van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition
369"
REFERENCES,0.875,"for deep bayesian active learning. Advances in neural information processing systems, 32, 2019.
370"
REFERENCES,0.876984126984127,"[20] Seo Taek Kong, Soomin Jeon, Dongbin Na, Jaewon Lee, Hong-Seok Lee, and Kyu-Hwan Jung. A neural
371"
REFERENCES,0.878968253968254,"pre-conditioning active learning algorithm to reduce label complexity. Advances in Neural Information
372"
REFERENCES,0.8809523809523809,"Processing Systems, 35:32842–32853, 2022.
373"
REFERENCES,0.8829365079365079,"[21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
374"
REFERENCES,0.8849206349206349,"[22] David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine
375"
REFERENCES,0.8869047619047619,"learning proceedings 1994, pages 148–156. Elsevier, 1994.
376"
REFERENCES,0.8888888888888888,"[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
377"
REFERENCES,0.8908730158730159,"transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF
378"
REFERENCES,0.8928571428571429,"International Conference on Computer Vision, pages 10012–10022, 2021.
379"
REFERENCES,0.8948412698412699,"[24] Rafid Mahmood, Sanja Fidler, and Marc T Law. Low-budget active learning via wasserstein distance: An
380"
REFERENCES,0.8968253968253969,"integer programming approach. In International Conference on Learning Representations, 2022.
381"
REFERENCES,0.8988095238095238,"[25] Sudhanshu Mittal, Maxim Tatarchenko, Özgün Çiçek, and Thomas Brox. Parting with illusions about deep
382"
REFERENCES,0.9007936507936508,"active learning. arXiv preprint arXiv:1912.05361, 2019.
383"
REFERENCES,0.9027777777777778,"[26] Mohamad Amin Mohamadi, Wonho Bae, and Danica J Sutherland. Making look-ahead active learning
384"
REFERENCES,0.9047619047619048,"strategies feasible with neural tangent kernels. In Advances in Neural Information Processing Systems,
385"
REFERENCES,0.9067460317460317,"2022.
386"
REFERENCES,0.9087301587301587,"[27] Mohamad Amin Mohamadi and Danica J Sutherland. A fast, well-founded approximation to the empirical
387"
REFERENCES,0.9107142857142857,"neural tangent kernel. arXiv preprint arXiv:2206.12543, 2022.
388"
REFERENCES,0.9126984126984127,"[28] Prateek Munjal, Nasir Hayat, Munawar Hayat, Jamshid Sourati, and Shadab Khan. Towards robust and
389"
REFERENCES,0.9146825396825397,"reproducible active learning using neural networks. In Proceedings of the IEEE/CVF Conference on
390"
REFERENCES,0.9166666666666666,"Computer Vision and Pattern Recognition, pages 223–232, 2022.
391"
REFERENCES,0.9186507936507936,"[29] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits
392"
REFERENCES,0.9206349206349206,"in natural images with unsupervised feature learning. 2011.
393"
REFERENCES,0.9226190476190477,"[30] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE
394"
REFERENCES,0.9246031746031746,"conference on computer vision and pattern recognition, pages 3498–3505. IEEE, 2012.
395"
REFERENCES,0.9265873015873016,"[31] Kossar Pourahmadi, Parsa Nooralinejad, and Hamed Pirsiavash. A simple baseline for low-budget active
396"
REFERENCES,0.9285714285714286,"learning. UMBC Student Collection, 2022.
397"
REFERENCES,0.9305555555555556,"[32] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach.
398"
REFERENCES,0.9325396825396826,"In International Conference on Learning Representations, 2018.
399"
REFERENCES,0.9345238095238095,"[33] Changjian Shui, Fan Zhou, Christian Gagné, and Boyu Wang. Deep active learning: Unified and principled
400"
REFERENCES,0.9365079365079365,"method for query and training. In International Conference on Artificial Intelligence and Statistics, pages
401"
REFERENCES,0.9384920634920635,"1308–1318. PMLR, 2020.
402"
REFERENCES,0.9404761904761905,"[34] Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational adversarial active learning. In Proceedings
403"
REFERENCES,0.9424603174603174,"of the IEEE/CVF International Conference on Computer Vision, pages 5972–5981, 2019.
404"
REFERENCES,0.9444444444444444,"[35] Toan Tran, Thanh-Toan Do, Ian Reid, and Gustavo Carneiro. Bayesian generative active deep learning. In
405"
REFERENCES,0.9464285714285714,"International Conference on Machine Learning, pages 6295–6304. PMLR, 2019.
406"
REFERENCES,0.9484126984126984,"[36] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool.
407"
REFERENCES,0.9503968253968254,"Scan: Learning to classify images without labels. In Computer Vision–ECCV 2020: 16th European
408"
REFERENCES,0.9523809523809523,"Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part X, pages 268–285. Springer, 2020.
409"
REFERENCES,0.9543650793650794,"[37] Kiri Wagstaff, Claire Cardie, Seth Rogers, Stefan Schrödl, et al. Constrained k-means clustering with
410"
REFERENCES,0.9563492063492064,"background knowledge. In Icml, volume 1, pages 577–584, 2001.
411"
REFERENCES,0.9583333333333334,"[38] Haonan Wang, Wei Huang, Ziwei Wu, Hanghang Tong, Andrew J Margenot, and Jingrui He. Deep
412"
REFERENCES,0.9603174603174603,"active learning by leveraging training dynamics. Advances in Neural Information Processing Systems,
413"
REFERENCES,0.9623015873015873,"35:25171–25184, 2022.
414"
REFERENCES,0.9642857142857143,"[39] Ziting Wen, Oscar Pizarro, and Stefan Williams. Active self-semi-supervised learning for few labeled
415"
REFERENCES,0.9662698412698413,"samples fast training. arXiv e-prints, pages arXiv–2203, 2022.
416"
REFERENCES,0.9682539682539683,"[40] Yichen Xie, Han Lu, Junchi Yan, Xiaokang Yang, Masayoshi Tomizuka, and Wei Zhan. Active finetuning:
417"
REFERENCES,0.9702380952380952,"Exploiting annotation budget in the pretraining-finetuning paradigm. arXiv preprint arXiv:2303.14382,
418"
REFERENCES,0.9722222222222222,"2023.
419"
REFERENCES,0.9742063492063492,"[41] Lin Yang, Yizhe Zhang, Jianxu Chen, Siyuan Zhang, and Danny Z Chen. Suggestive annotation: A deep
420"
REFERENCES,0.9761904761904762,"active learning framework for biomedical image segmentation. In International conference on medical
421"
REFERENCES,0.9781746031746031,"image computing and computer-assisted intervention, pages 399–407. Springer, 2017.
422"
REFERENCES,0.9801587301587301,"[42] Ofer Yehuda, Avihu Dekel, Guy Hacohen, and Daphna Weinshall. Active learning through a covering lens.
423"
REFERENCES,0.9821428571428571,"In Advances in Neural Information Processing Systems, 2022.
424"
REFERENCES,0.9841269841269841,"[43] Donggeun Yoo and In So Kweon. Learning loss for active learning. In Proceedings of the IEEE/CVF
425"
REFERENCES,0.9861111111111112,"conference on computer vision and pattern recognition, pages 93–102, 2019.
426"
REFERENCES,0.9880952380952381,"[44] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference
427"
REFERENCES,0.9900793650793651,"2016. British Machine Vision Association, 2016.
428"
REFERENCES,0.9920634920634921,"[45] Wenqiao Zhang, Lei Zhu, James Hallinan, Shengyu Zhang, Andrew Makmur, Qingpeng Cai, and Beng Chin
429"
REFERENCES,0.9940476190476191,"Ooi. Boostmis: Boosting medical image semi-supervised learning with adaptive pseudo labeling and
430"
REFERENCES,0.996031746031746,"informative active annotation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
431"
REFERENCES,0.998015873015873,"Pattern Recognition, pages 20666–20676, 2022.
432"
