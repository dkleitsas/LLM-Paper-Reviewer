Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0017421602787456446,"We study COMP-AMS, a distributed optimization framework based on gradient
averaging and adaptive AMSGrad algorithm. Gradient compression with error
feedback is applied to reduce the communication cost in the gradient transmission
process. Our convergence analysis of COMP-AMS shows that such compressed
gradient averaging strategy yields same convergence rate as standard AMSGrad,
and also exhibits the linear speedup effect w.r.t.
the number of local work-
ers. Compared with recently proposed protocols on distributed adaptive methods,
COMP-AMS is simple and convenient. Numerical experiments are conducted
to justify the theoretical ﬁndings, and demonstrate that the proposed method can
achieve same test accuracy as the full-gradient AMSGrad with substantial com-
munication savings. With its simplicity and efﬁciency, COMP-AMS can serve as
a useful distributed training framework for adaptive gradient methods."
INTRODUCTION,0.003484320557491289,"1
INTRODUCTION"
INTRODUCTION,0.005226480836236934,"Deep neural network has achieved the state-of-the-art learning performance on numerous AI ap-
plications, e.g., computer vision and natural language processing (Graves et al., 2013; Goodfellow
et al., 2014; He et al., 2016; Young et al., 2018; Zhang et al., 2018), reinforcement learning (Mnih
et al., 2013; Levine et al., 2016; Silver et al., 2017), recommendation systems (Covington et al.,
2016), computational advertising (Zhao et al., 2019; Xu et al., 2021; Zhao et al., 2022), etc. With
the increasing size of data and growing complexity of deep neural networks, standard single-machine
training procedures encounter at least two major challenges:"
INTRODUCTION,0.006968641114982578,"• Due to the limited computing power of a single-machine, processing the massive number of
data samples takes a long time—training is too slow. Many real-world applications cannot
afford spending days or even weeks on training."
INTRODUCTION,0.008710801393728223,"• In many scenarios, data are stored on multiple servers, possibly at different locations, due
to the storage constraints (massive user behavior data, Internet images, etc.) or privacy
reasons (Chang et al., 2018). Hence, transmitting data among servers might be costly."
INTRODUCTION,0.010452961672473868,"Distributed learning framework has been commonly used to tackle the above two issues. Consider
the distributed optimization task where n workers jointly solve the following optimization problem"
INTRODUCTION,0.012195121951219513,"min
θ
f(θ) := min
θ
1
n n
X"
INTRODUCTION,0.013937282229965157,"i=1
fi(θ) = 1 n n
X"
INTRODUCTION,0.0156794425087108,"i=1
Ex∼Xi[Fi(θ; x)],
(1)"
INTRODUCTION,0.017421602787456445,"where the non-convex function fi represents the average loss over the local data samples for worker
i ∈[n], and θ ∈Rd the global model parameter. Xi is the data distribution on each local node. In
the classical centralized distributed setting, in each iteration the central server uniformly randomly
assigns the data to n local workers (Xi’s are the same), at which the gradients of the model are
computed in parallel. Then the central server aggregates the local gradients, updates the global
model (e.g., by stochastic gradient descent (SGD)), and transmits back the updated model to the"
INTRODUCTION,0.01916376306620209,Published as a conference paper at ICLR 2022
INTRODUCTION,0.020905923344947737,"local nodes for subsequent gradient computation. The scenario where Xi’s are different gives rise to
the recently proposed Federated Learning (FL) (McMahan et al., 2017) framework, which will not
be the major focus of this work. As we can see, distributed training naturally solves aforementioned
issues: 1) We use n computing nodes to train the model, so the time per training epoch can be
largely reduced; 2) There is no need to transmit the local data to central server. Besides, distributed
training also provides stronger error tolerance since the training process could continue even one
local machine breaks down. As a result of these advantages, there has been a surge of study and
applications on distributed systems (Nedic & Ozdaglar, 2009; Boyd et al., 2011; Duchi et al., 2012;
Goyal et al., 2017; Hong et al., 2017; Koloskova et al., 2019; Lu et al., 2019)."
INTRODUCTION,0.02264808362369338,"Gradient compression. Among many optimization strategies, SGD is still the most popular proto-
type in distributed training for its simplicity and effectiveness (Chilimbi et al., 2014; Agarwal et al.,
2018; Mikami et al., 2018). Yet, when the deep learning model is very large, the communication be-
tween local nodes and central server could be expensive, and the burdensome gradient transmission
would slow down the whole training system. Thus, reducing the communication cost in distributed
SGD has become an active topic, and an important ingredient of large-scale distributed systems
(e.g., Seide et al. (2014)). Solutions based on quantization, sparsiﬁcation and other compression
techniques of the local gradients have been proposed, e.g., Aji & Heaﬁeld (2017); Alistarh et al.
(2017); Sa et al. (2017); Wen et al. (2017); Bernstein et al. (2018); Stich et al. (2018); Wangni et al.
(2018); Ivkin et al. (2019); Yang et al. (2019); Haddadpour et al. (2020). However, it has been ob-
served both theoretically and empirically (Stich et al., 2018; Ajalloeian & Stich, 2020), that directly
updating with the compressed gradients usually brings non-negligible performance downgrade in
terms of convergence speed and accuracy. To tackle this problem, studies (e.g., Stich et al. (2018);
Karimireddy et al. (2019)) show that the technique of error feedback can to a large extent remedy
the issue of such gradient compression, achieving the same convergence rate as full-gradient SGD."
INTRODUCTION,0.024390243902439025,"Adaptive optimization. In recent years, adaptive optimization algorithms (e.g., AdaGrad (Duchi
et al., 2010), Adam (Kingma & Ba, 2015) and AMSGrad (Reddi et al., 2018)) have become popular
because of their superior empirical performance. These methods use different implicit learning rates
for different coordinates that keep changing adaptively throughout the training process, based on
the learning trajectory. In many cases, adaptive methods have been shown to converge faster than
SGD, sometimes with better generalization as well. Nevertheless, the body of literature that extends
adaptive methods to distributed training is still fairly limited. In particular, even the simple gradient
averaging approach, though appearing standard, has not been analyzed for adaptive optimization
algorithms. Given that distributed SGD with compressed gradient averaging can match the perfor-
mance of standard SGD, one natural question is: is it also true for adaptive methods? In this work,
we ﬁll this gap formally, by analyzing COMP-AMS, a distributed adaptive optimization framework
using the gradient averaging protocol, with communication-efﬁcient gradient compression. Our
method has been implemented in the PaddlePaddle platform (www.paddlepaddle.org.cn)."
INTRODUCTION,0.02613240418118467,"Our contributions. We study a simple algorithm design leveraging the adaptivity of AMSGrad and
the computational virtue of local gradient compression:"
INTRODUCTION,0.027874564459930314,"• We propose COMP-AMS, a synchronous distributed adaptive optimization framework
based on global averaging with gradient compression, which is efﬁcient in both communi-
cation and memory as no local moment estimation is needed. We consider the BlockSign
and Top-k compressors, coupled with the error-feedback technique to compensate for the
bias implied by the compression step for fast convergence."
INTRODUCTION,0.029616724738675958,"• We provide the convergence analysis of distributed COMP-AMS (with n workers) in
smooth non-convex optimization. In the special case of n = 1 (single machine), similar
to SGD, gradient compression with error feedback in adaptive method achieves the same
convergence rate O( 1
√"
INTRODUCTION,0.0313588850174216,"T ) as the standard full-gradient counterpart. Also, we show that with
a properly chosen learning rate, COMP-AMS achieves O(
1
√"
INTRODUCTION,0.033101045296167246,"nT ) convergence, implying a
linear speedup in terms of the number of local workers to attain a stationary point."
INTRODUCTION,0.03484320557491289,"• Experiments are conducted on various training tasks on image classiﬁcation and sentiment
analysis to validate our theoretical ﬁndings on the linear speedup effect. Our results show
that COMP-AMS has comparable performance with other distributed adaptive methods,
and approaches the accuracy of full-precision AMSGrad with a substantially reduced com-
munication cost. Thus, it can serve as a convenient distributed training strategy in practice."
INTRODUCTION,0.036585365853658534,Published as a conference paper at ICLR 2022
RELATED WORK,0.03832752613240418,"2
RELATED WORK"
DISTRIBUTED SGD WITH COMPRESSED GRADIENTS,0.04006968641114982,"2.1
DISTRIBUTED SGD WITH COMPRESSED GRADIENTS"
DISTRIBUTED SGD WITH COMPRESSED GRADIENTS,0.041811846689895474,"Quantization. To reduce the expensive communication in large-scale distributed SGD training
systems, extensive works have considered various compression techniques applied to the gradient
transaction procedure. The ﬁrst strategy is quantization. Dettmers (2016) condenses 32-bit ﬂoat-
ing numbers into 8-bits when representing the gradients. Seide et al. (2014); Bernstein et al. (2018;
2019); Karimireddy et al. (2019) use the extreme 1-bit information (sign) of the gradients, com-
bined with tricks like momentum, majority vote and memory. Other quantization-based methods
include QSGD (Alistarh et al., 2017; Zhang et al., 2017; Wu et al., 2018) and LPC-SVRG (Yu et al.,
2019b), leveraging unbiased stochastic quantization. Quantization has been successfully applied
to industrial-level applications, e.g., Xu et al. (2021). The saving in communication of quantization
methods is moderate: for example, 8-bit quantization reduces the cost to 25% (compared with 32-bit
full-precision). Even in the extreme 1-bit case, the largest compression ratio is around 1/32 ≈3.1%."
DISTRIBUTED SGD WITH COMPRESSED GRADIENTS,0.04355400696864112,"Sparsiﬁcation. Gradient sparsiﬁcation is another popular solution which may provide higher com-
pression rate. Instead of commuting the full gradient, each local worker only passes a few coordi-
nates to the central server and zeros out the others. Thus, we can more freely choose higher compres-
sion ratio (e.g., 1%, 0.1%), still achieving impressive performance in many applications (Lin et al.,
2018). Stochastic sparsiﬁcation methods, including uniform and magnitude based sampling (Wangni
et al., 2018), select coordinates based on some sampling probability, yielding unbiased gradient
compressors with proper scaling. Deterministic methods are simpler, e.g., Random-k, Top-k (Stich
et al., 2018; Shi et al., 2019) (selecting k elements with largest magnitude), Deep Gradient Com-
pression (Lin et al., 2018), but usually lead to biased gradient estimation. More applications and
analysis of compressed distributed SGD can be found in Alistarh et al. (2018); Jiang & Agrawal
(2018); Jiang et al. (2018); Shen et al. (2018); Basu et al. (2019), among others."
DISTRIBUTED SGD WITH COMPRESSED GRADIENTS,0.04529616724738676,"Error Feedback (EF). Biased gradient estimation, which is a consequence of many aforementioned
methods (e.g., signSGD, Top-k), undermines the model training, both theoretically and empirically,
with slower convergence and worse generalization (Ajalloeian & Stich, 2020; Beznosikov et al.,
2020). The technique of error feedback is able to “correct for the bias” and ﬁx the convergence
issues. In this procedure, the difference between the true stochastic gradient and the compressed
one is accumulated locally, which is then added back to the local gradients in later iterations. Stich
et al. (2018); Karimireddy et al. (2019) prove the O( 1"
DISTRIBUTED SGD WITH COMPRESSED GRADIENTS,0.047038327526132406,"T ) and O( 1
√"
DISTRIBUTED SGD WITH COMPRESSED GRADIENTS,0.04878048780487805,"T ) convergence rate of EF-SGD in
strongly convex and non-convex setting respectively, matching the rates of vanilla SGD (Nemirovski
et al., 2009; Ghadimi & Lan, 2013). More recent works on the convergence rate of SGD with error
feedback include Stich & Karimireddy (2019); Zheng et al. (2019); Richtárik et al. (2021), etc."
ADAPTIVE OPTIMIZATION,0.050522648083623695,"2.2
ADAPTIVE OPTIMIZATION"
ADAPTIVE OPTIMIZATION,0.05226480836236934,"Algorithm 1 AMSGRAD (Reddi et al., 2018)"
ADAPTIVE OPTIMIZATION,0.05400696864111498,"1: Input: parameters β1, β2, ϵ, learning rate ηt
2: Initialize: θ1 ∈Rd, m0 = v0 = 0 ∈Rd"
ADAPTIVE OPTIMIZATION,0.05574912891986063,"3: for t = 1, . . . , T do
4:
Compute stochastic gradient gt at θt
5:
mt = β1mt−1 + (1 −β1)gt
6:
vt = β2vt−1 + (1 −β2)g2
t
7:
ˆvt = max(ˆvt−1, vt)
8:
θt+1 = θt −ηt
mt
√ˆvt+ϵ
9: end for"
ADAPTIVE OPTIMIZATION,0.05749128919860627,"In each SGD update, all the coordinates share
the same learning rate, which is either constant
or decreasing through the iterations.
Adap-
tive optimization methods cast different learn-
ing rates on each dimension. For instance, Ada-
Grad, developed in Duchi et al. (2010), divides"
ADAPTIVE OPTIMIZATION,0.059233449477351915,"the gradient elementwise by
qPT
t=1 g2
t ∈Rd,
where gt ∈Rd is the gradient vector at time
t and d is the model dimensionality. Thus, it
intrinsically assigns different learning rates to
different coordinates throughout the training—
elements with smaller previous gradient magni-
tudes tend to move a larger step via larger learn-
ing rate. Other adaptive methods include AdaDelta (Zeiler, 2012) and Adam (Kingma & Ba, 2015),
which introduce momentum and moving average of second moment estimation into AdaGrad hence
leading to better performances. AMSGrad (Reddi et al., 2018) (Algorithm 1, which is the prototype
in our paper), ﬁxes the potential convergence issue of Adam. Wang et al. (2021) and Zhou et al."
ADAPTIVE OPTIMIZATION,0.06097560975609756,Published as a conference paper at ICLR 2022
ADAPTIVE OPTIMIZATION,0.0627177700348432,"(2020) improve the convergence and generalization of AMSGrad through optimistic acceleration
and differential privacy."
ADAPTIVE OPTIMIZATION,0.06445993031358885,"Adaptive optimization methods have been widely used in training deep learning models in language,
computer vision and advertising applications, e.g., Choi et al. (2019); You et al. (2020); Zhang et al.
(2021); Zhao et al. (2022). In distributed setting, Nazari et al. (2019); Chen et al. (2021b) study
decentralized adaptive methods, but communication efﬁciency was not considered. Mostly relevant
to our work, Chen et al. (2021a) proposes a distributed training algorithm based on Adam, which
requires every local node to store a local estimation of the moments of the gradient. Thus, one has
to keep extra two more tensors of the model size on each local worker, which may be less feasible
in terms of memory particularly with large models. More recently, Tang et al. (2021) proposes an
Adam pre-conditioned momentum SGD method. Chen et al. (2020); Karimireddy et al. (2020);
Reddi et al. (2021) proposed local/global adaptive FL methods, which can be further accelerated via
layer-wise adaptivity (Karimi et al., 2021)."
COMMUNICATION-EFFICIENT ADAPTIVE OPTIMIZATION,0.06620209059233449,"3
COMMUNICATION-EFFICIENT ADAPTIVE OPTIMIZATION"
GRADIENT COMPRESSORS,0.06794425087108014,"3.1
GRADIENT COMPRESSORS"
GRADIENT COMPRESSORS,0.06968641114982578,"In this paper, we mainly consider deterministic q-deviate compressors deﬁned as below."
GRADIENT COMPRESSORS,0.07142857142857142,"Assumption 1. The gradient compressor C : Rd 7→Rd is q-deviate: for ∀x ∈Rd, ∃0 ≤q < 1
such that ∥C(x) −x∥≤q ∥x∥."
GRADIENT COMPRESSORS,0.07317073170731707,"Larger q indicates heavier compression, while smaller q implies better approximation of the true
gradient. q = 0 implies C(x) = x, i.e., no compression. In the following, we give two popular and
efﬁcient q-deviate compressors that will be adopted in this paper."
GRADIENT COMPRESSORS,0.07491289198606271,"Deﬁnition 1 (Top-k). For x ∈Rd, denote S as the size-k set of i ∈[d] with largest k magnitude
|xi|. The Top-k compressor is deﬁned as C(x)i = xi, if i ∈S; C(x)i = 0 otherwise."
GRADIENT COMPRESSORS,0.07665505226480836,"Deﬁnition 2 (Block-Sign). For x ∈Rd, deﬁne M blocks indexed by Bi, i = 1, ..., M, with di :=
|Bi|. The Block-Sign compressor is deﬁned as C(x) = [sign(xB1)
∥xB1∥1"
GRADIENT COMPRESSORS,0.078397212543554,"d1
, ..., sign(xBM )
∥xBM ∥1"
GRADIENT COMPRESSORS,0.08013937282229965,"dM
],
where xBi is the sub-vector of x at indices Bi."
GRADIENT COMPRESSORS,0.08188153310104529,"Remark 1. It is well-known (Stich et al., 2018) that for Top-k, q2 = 1 −k"
GRADIENT COMPRESSORS,0.08362369337979095,"d. For Block-Sign,
by Cauchy-Schwartz inequality we have q2 = 1 −mini∈[M]
1
di where M and di are deﬁned in
Deﬁnition 2 (Zheng et al., 2019)."
GRADIENT COMPRESSORS,0.08536585365853659,"The intuition behind Top-k is that, it has been observed empirically that when training many deep
models, most gradients are typically very small, and gradients with large magnitude contain most
information. The Block-Sign compressor is a simple extension of the 1-bit SIGN compressor (Seide
et al., 2014; Bernstein et al., 2018), adapted to different gradient magnitude in different blocks,
which, for neural nets, are usually set as the distinct network layers. The scaling factor in Deﬁnition 2
is to preserve the (possibly very different) gradient magnitude in each layer. In principle, Top-k
would perform the best when the gradient is effectively sparse, while Block-Sign compressor is
favorable by nature when most gradients have similar magnitude within each layer."
GRADIENT COMPRESSORS,0.08710801393728224,"3.2
COMP-AMS: DISTRIBUTED ADAPTIVE TRAINING BY GRADIENT AGGREGATION"
GRADIENT COMPRESSORS,0.08885017421602788,"We present in Algorithm 2 the proposed communication-efﬁcient distributed adaptive method in this
paper, COMP-AMS. This framework can be regarded as an analogue to the standard synchronous
distributed SGD: in each iteration, each local worker transmits to the central server the compressed
stochastic gradient computed using local data. Then the central server takes the average of local
gradients, and performs an AMSGrad update. In Algorithm 2, lines 7-8 depict the error feedback
operation at local nodes. et,i is the accumulated error from gradient compression on the i-th worker
up to time t −1. This residual is added back to gt,i to get the “corrected” gradient. In Section 4
and Section 5, we will show that error feedback, similar to the case of SGD, also brings good
convergence behavior under gradient compression in distributed AMSGrad."
GRADIENT COMPRESSORS,0.09059233449477352,Published as a conference paper at ICLR 2022
GRADIENT COMPRESSORS,0.09233449477351917,Algorithm 2 Distributed COMP-AMS with error feedback (EF)
GRADIENT COMPRESSORS,0.09407665505226481,"1: Input: parameters β1, β2, ϵ, learning rate ηt
2: Initialize: central server parameter θ1 ∈Rd ⊆Rd; e1,i = 0 the error accumulator for each
worker; m0 = 0, v0 = 0, ˆv0 = 0
3: for t = 1, . . . , T do
4:
parallel for worker i ∈[n] do:
5:
Receive model parameter θt from central server
6:
Compute stochastic gradient gt,i at θt
7:
Compute the compressed gradient ˜gt,i = C(gt,i + et,i)
8:
Update the error et+1,i = et,i + gt,i −˜gt,i
9:
Send ˜gt,i back to central server
10:
end parallel
11:
Central server do:
12:
¯gt = 1"
GRADIENT COMPRESSORS,0.09581881533101046,"n
Pn
i=1 ˜gt,i
13:
mt = β1mt−1 + (1 −β1)¯gt
14:
vt = β2vt−1 + (1 −β2)¯g2
t
15:
ˆvt = max(vt, ˆvt−1)
16:
Update the global model θt+1 = θt −ηt
mt
√ˆvt+ϵ
17: end for"
GRADIENT COMPRESSORS,0.0975609756097561,"Comparison with related methods. Next, we discuss the differences between COMP-AMS and
two recently proposed methods also trying to solve compressed distributed adaptive optimization."
GRADIENT COMPRESSORS,0.09930313588850175,"• Comparison with Chen et al. (2021a). Chen et al. (2021a) develops a quantized variant
of Adam (Kingma & Ba, 2015), called “QAdam”. In this method, each worker keeps a local
copy of the moment estimates, commonly noted m and v, and compresses and transmits
the ratio m"
GRADIENT COMPRESSORS,0.10104529616724739,"v as a whole to the server. Their method is hence very much like the compressed
distributed SGD, with the exception that the ratio m"
GRADIENT COMPRESSORS,0.10278745644599303,"v plays the role of the gradient vector g
communication-wise. Thus, two local moment estimators are additionally required, which
have same size as the deep learning model. In our COMP-AMS, the moment estimates
m and v are kept and updated only at the central server, thus not introducing any extra
variable (tensor) on local nodes during training (except for the error accumulator). Hence,
COMP-AMS is not only effective in communication reduction, but also efﬁcient in terms
of memory (space), which is feasible when training large-scale learners like BERT and
CTR prediction models, e.g., Devlin et al. (2019); Xu et al. (2021), to lower the hardware
consumption in practice. Additionally, the convergence rate in Chen et al. (2021a) does not
improve linearly with n, while we prove the linear speedup effect of COMP-AMS.
• Comparison with Tang et al. (2021) The recent work (Tang et al., 2021) proposes
“1BitAdam”. They ﬁrst run some warm-up training steps using standard Adam, and then
store the second moment moving average v. Then, distributed Adam training starts with
v frozen. Thus, 1BitAdam is actually more like a distributed momentum SGD with some
pre-conditioned coordinate-wise learning rates. The number of warm-up steps also needs
to be carefully tuned, otherwise bad pre-conditioning may hurt the learning performance.
Our COMP-AMS is simpler, as no pre-training is needed. Also, 1BitAdam requires extra
tensors for m locally, while COMP-AMS does not need additional local memory."
CONVERGENCE ANALYSIS,0.10452961672473868,"4
CONVERGENCE ANALYSIS"
CONVERGENCE ANALYSIS,0.10627177700348432,"For the convergence analysis of COMP-AMS we will make following additional assumptions.
Assumption 2. (Smoothness) For ∀i ∈[n], fi is L-smooth: ∥∇fi(θ) −∇fi(ϑ)∥≤L ∥θ −ϑ∥.
Assumption 3. (Unbiased and bounded stochastic gradient) For ∀t > 0, ∀i ∈[n], the stochastic
gradient is unbiased and uniformly bounded: E[gt,i] = ∇fi(θt) and ∥gt,i∥≤G.
Assumption 4. (Bounded variance) For ∀t > 0, ∀i ∈[n]: (i) the local variance of the stochas-
tic gradient is bounded: E[∥gt,i −∇fi(θt)∥2] < σ2; (ii) the global variance is bounded by
1
n
Pn
i=1 ∥∇fi(θt) −∇f(θt)∥2 ≤σ2
g."
CONVERGENCE ANALYSIS,0.10801393728222997,Published as a conference paper at ICLR 2022
CONVERGENCE ANALYSIS,0.10975609756097561,"In Assumption 3, the uniform bound on the stochastic gradient is common in the convergence anal-
ysis of adaptive methods, e.g., Reddi et al. (2018); Zhou et al. (2018); Chen et al. (2019). The global
variance bound σ2
g in Assumption 4 characterizes the difference among local objective functions,
which, is mainly caused by different local data distribution Xi in (1). In classical distributed setting
where all the workers can access the same dataset and local data are assigned randomly, σ2
g ≡0.
While typical federated learning (FL) setting with σ2
g > 0 is not the focus of this present paper,
we consider the global variance in our analysis to shed some light on the impact of non-i.i.d. data
distribution in the federated setting for broader interest and future investigation."
CONVERGENCE ANALYSIS,0.11149825783972125,"We derive the following general convergence rate of COMP-AMS in the distributed setting. The
proof is deferred to Appendix B."
CONVERGENCE ANALYSIS,0.1132404181184669,"Theorem 1. Denote C0 =
q"
CONVERGENCE ANALYSIS,0.11498257839721254,4(1+q2)3
CONVERGENCE ANALYSIS,0.11672473867595819,"(1−q2)2 G2 + ϵ, C1 =
β1
1−β1 +
2q
1−q2 , θ∗= arg min f(θ) deﬁned as
(1). Under Assumptions 1 to 4, with ηt = η ≤
ϵ
3C0√"
CONVERGENCE ANALYSIS,0.11846689895470383,"2L max{2L,C1}, Algorithm 2 satisﬁes"
T,0.12020905923344948,"1
T T
X"
T,0.12195121951219512,"t=1
E[∥∇f(θt)∥2] ≤2C0
E[f(θ1) −f(θ∗)]"
T,0.12369337979094076,"Tη
+ ηLσ2"
T,0.1254355400696864,"nϵ
+ 3η2LC0C2
1σ2 nϵ2"
T,0.12717770034843207,"+ 12η2q2LC0σ2
g
(1 −q2)2ϵ2
+ (1 + C1)G2d"
T,0.1289198606271777,"T√ϵ
+ η(1 + 2C1)C1LG2d Tϵ 
."
T,0.13066202090592335,"The LHS of Theorem 1 is the expected squared norm of the gradient from a uniformly chosen iterate
t ∈[T], which is a common convergence measure in non-convex optimization. From Theorem 1, we
see that the more compression we apply to the gradient vectors (i.e., larger q), the larger the gradient
magnitude is, i.e., the slower the algorithm converges. This is intuitive as heavier compression loses
more gradient information which would slower down the learner to ﬁnd a good solution."
T,0.13240418118466898,"Note that, COMP-AMS with n = 1 naturally reduces to the single-machine (sequential) AMSGrad
(Algorithm 1) with compressed gradients instead of full-precision ones. Karimireddy et al. (2019)
speciﬁcally analyzed this case for SGD, showing that compressed single-machine SGD with error
feedback has the same convergence rate as vanilla SGD using full gradients. In alignment with the
conclusion in Karimireddy et al. (2019), for adaptive AMSGrad, we have a similar result."
T,0.13414634146341464,"Corollary 1. When n = 1, under Assumption 1 to Assumption 4, setting the stepsize as η =
min{
ϵ
3C0√"
T,0.13588850174216027,"2L max{2L,C1},
1
√"
T,0.13763066202090593,"T }, Algorithm 2 satisﬁes"
T,0.13937282229965156,"1
T T
X"
T,0.14111498257839722,"t=1
E[∥∇f(θt)∥2] ≤O( 1
√"
T,0.14285714285714285,"T
+ σ2 √ T
+ d T )."
T,0.1445993031358885,"Corollary 1 states that with error feedback, single machine AMSGrad with biased compressed gra-
dients can also match the convergence rate O( 1
√ T + d"
T,0.14634146341463414,"T ) of standard AMSGrad (Zhou et al., 2018) in
non-convex optimization. It also achieves the same rate O( 1
√"
T,0.1480836236933798,"T ) of vanilla SGD (Karimireddy et al.,
2019) when T is sufﬁciently large. In other words, error feedback also ﬁxes the convergence issue
of using compressed gradients in AMSGrad."
T,0.14982578397212543,"Linear Speedup. In Theorem 1, the convergence rate is derived by assuming a constant learning
rate. By carefully choosing a decreasing learning rate dependent on the number of workers, we have
the following simpliﬁed statement."
T,0.15156794425087108,"Corollary 2. Under the same setting as Theorem 1, set η = min{
ϵ
3C0√"
T,0.15331010452961671,"2L max{2L,C1},
√n
√"
T,0.15505226480836237,T }. The
T,0.156794425087108,COMP-AMS iterates admit
T,0.15853658536585366,"1
T T
X"
T,0.1602787456445993,"t=1
E[∥∇f(θt)∥2] ≤O(
1
√"
T,0.16202090592334495,"nT
+
σ2
√"
T,0.16376306620209058,"nT
+ n(σ2 + σ2
g)
T
).
(2)"
T,0.16550522648083624,"In Corollary 2, we see that the global variance σ2
g appears in the O( 1"
T,0.1672473867595819,"T ) term, which says that it
asymptotically has no impact on the convergence. This matches the result of momentum SGD (Yu"
T,0.16898954703832753,Published as a conference paper at ICLR 2022
T,0.17073170731707318,"et al., 2019a). When T ≥O(n3) is sufﬁciently large, the third term in (2) vanishes, and the con-
vergence rate becomes O(
1
√"
T,0.17247386759581881,"nT ). Therefore, to reach an O(δ) stationary point, one worker (n = 1)
needs T = O( 1"
T,0.17421602787456447,"δ2 ) iterations, while distributed training with n workers requires only T = O( 1"
T,0.1759581881533101,"nδ2 )
iterations, which is n times faster than single machine training. That is, COMP-AMS has a linear
speedup in terms of the number of the local workers. Such acceleration effect has also been reported
for compressed SGD (Jiang & Agrawal, 2018; Zheng et al., 2019) and momentum SGD (Yu et al.,
2019a) with error feedback."
EXPERIMENTS,0.17770034843205576,"5
EXPERIMENTS"
EXPERIMENTS,0.1794425087108014,"In this section, we provide numerical results on several common datasets. Our main objective is to
validate the theoretical results, and demonstrate that the proposed COMP-AMS can approach the
learning performance of full-precision AMSGrad with signiﬁcantly reduced communication costs."
EXPERIMENTS,0.18118466898954705,"5.1
DATASETS, MODELS AND METHODS"
EXPERIMENTS,0.18292682926829268,"Our experiments are conducted on various image and text datasets. The MNIST (LeCun et al.,
1998) contains 60000 training samples of 28 × 18 gray-scale hand-written digits from 10 classes,
and 10000 test samples. We train MNIST with a Convolutional Neural Network (CNN), which has
two convolutional layers followed by two fully connected layers with ReLu activation. Dropout is
applied after the max-pooled convolutional layer with rate 0.5. The CIFAR-10 dataset (Krizhevsky
& Hinton, 2009) consists of 50000 32 × 32 RGB natural images from 10 classes for training and
10000 images for testing, which is trained by LeNet-5 (LeCun et al., 1998). Moreover, we also
implement ResNet-18 (He et al., 2016) on this dataset. The IMDB movie review (Maas et al., 2011)
is a popular binary classiﬁcation dataset for sentiment analysis. Each movie review is tokenized by
top-2000 most frequently appeared words and transformed into integer vectors, which is of maximal
length 500. We train a Long-Short Term Memory (LSTM) network with a 32-dimensional embed-
ding layer and 64 LSTM cells, followed by two fully connected layers before output. Cross-entropy
loss is used for all the tasks. Following the classical distributed training setting, in each training
iteration, data samples are uniformly randomly assigned to the workers."
EXPERIMENTS,0.18466898954703834,"We compare COMP-AMS with full-precision distributed AMSGrad, QAdam (Chen et al., 2021a)
and 1BitAdam (Tang et al., 2021). For COMP-AMS, Top-k picks top 1% gradient coordinates
(i.e., sparsity 0.01). QAdam and 1BitAdam both use 1-bit quantization to achieve high compression.
For MNIST and CIFAR-10, the local batch size on each worker is set to be 32. For IMDB, the local
batch size is 16. The hyper-parameters in COMP-AMS are set as default β1 = 0.9, β2 = 0.999 and
ϵ = 10−8, which are also used for QAdam and 1BitAdam. For 1BitAdam, the epochs for warm-up
training is set to be 1/20 of the total epochs. For all methods, we tune the initial learning rate over
a ﬁne grid (see Appendix A) and report the best results averaged over three independent runs. Our
experiments are performed on a GPU cluster with NVIDIA Tesla V100 cards."
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.18641114982578397,"5.2
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.18815331010452963,"The training loss and test accuracy on MNIST + CNN, CIFAR-10 + LeNet and IMDB + LSTM
are reported in Figure 1. We provide more results on larger ResNet-18 model in Appendix A. On
CIFAR-10, we deploy a popular decreasing learning rate schedule, where the step size η is divided
by 10 at the 40-th and 80-th epoch, respectively. We observe:"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.18989547038327526,"• On MNIST, all the methods can approach the training loss and test accuracy of full-
precision AMSGrad. The 1BitAdam seems slightly better, but the gap is very small. On
CIFAR-10, COMP-AMS with Block-Sign performs the best and matches AMSGrad in
terms of test accuracy.
• On IMDB, COMP-AMS with Top-k has both the fastest convergence and best generaliza-
tion compared with other compressed methods. This is because the IMDB text data is more
sparse (with many padded zeros), where Top-k is expected to work better than sign. The
1BitAdam converges slowly. We believe one possible reason is that 1BitAdam is quite sen-
sitive to the quality of the warm-up training. For sparse text data, the estimation of second
moment v is more unstable, making the strategy of freezing v by warm-up less effective."
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.1916376306620209,Published as a conference paper at ICLR 2022
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.19337979094076654,"0
5
10
15
20
Epochs 0 0.1"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.1951219512195122,Train Loss MNIST
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.19686411149825783,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
1BitAdam"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.1986062717770035,"0
25
50
75
100
Epochs 0.5 1 1.5"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.20034843205574912,Train Loss CIFAR
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.20209059233449478,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
1BitAdam"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.2038327526132404,"0
5
10
15
20
Epochs 0 0.2 0.4 0.6 0.8"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.20557491289198607,Train Loss IMDB
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.2073170731707317,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
1BitAdam"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.20905923344947736,"0
5
10
15
20
Epochs 96 97 98 99"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.21080139372822299,Test Accuracy MNIST
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.21254355400696864,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
1BitAdam"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.21428571428571427,"0
25
50
75
100
Epochs 50 60 70 80"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.21602787456445993,Test Accuracy CIFAR
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.21777003484320556,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
1BitAdam"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.21951219512195122,"0
5
10
15
20
Epochs 60 70 80 90"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.22125435540069685,Test Accuracy IMDB
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.2229965156794425,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
1BitAdam"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.22473867595818817,"Figure 1: Training loss and test accuracy vs. epochs, on MNIST + CNN, CIFAR-10 + LeNet and
IMDB + LSTM with n = 16 local workers."
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.2264808362369338,"Communication Efﬁciency. In Figure 2, we plot the training loss and test accuracy against the
number of bits transmitted to the central server during the distributed training process, where we
assume that the full-precision gradient is represented using 32 bits per ﬂoating number. As we
can see, COMP-AMS-Top-0.01 achieves around 100x communication reduction, to attain similar
accuracy as the full-precision distributed AMSGrad. The saving of Block-Sign is around 30x, but
it gives slightly higher accuracy than Top-0.01 on MNIST and CIFAR-10. In all cases, COMP-
AMS can substantially reduce the communication cost compared with full-precision distributed
AMSGrad, without losing accuracy."
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.22822299651567945,"20
25
30
log(No. bits) 0.05 0.1 0.15"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.22996515679442509,Train Loss MNIST
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.23170731707317074,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
1BitAdam"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.23344947735191637,"25
30
35
40
log(No. bits) 0.5 1 1.5 2"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.23519163763066203,Train Loss CIFAR
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.23693379790940766,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
1BitAdam"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.23867595818815332,"20
25
30
log(No. bits) 0 0.2 0.4 0.6 0.8"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.24041811846689895,Train Loss IMDB
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.2421602787456446,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
1BitAdam"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.24390243902439024,"20
25
30
log(No. bits) 97 98 99"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.2456445993031359,Test Accuracy MNIST
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.24738675958188153,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
1BitAdam"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.24912891986062718,"25
30
35
40
log(No. bits) 40 50 60 70 80"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.2508710801393728,Test Accuracy CIFAR
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.25261324041811845,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
1BitAdam"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.25435540069686413,"20
25
30
log(No. bits) 50 60 70 80 90"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.25609756097560976,Test Accuracy IMDB
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.2578397212543554,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
1BitAdam"
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.259581881533101,"Figure 2: Train loss and Test accuracy vs. No. bits transmitted, on MNIST + CNN, CIFAR-10 +
LeNet and IMDB + LSTM with n = 16 local workers."
GENERAL EVALUATION AND COMMUNICATION EFFICIENCY,0.2613240418118467,Published as a conference paper at ICLR 2022
LINEAR SPEEDUP OF COMP-AMS,0.26306620209059234,"5.3
LINEAR SPEEDUP OF COMP-AMS"
LINEAR SPEEDUP OF COMP-AMS,0.26480836236933797,"0
5000
10000
Iterations 0.04 0.06 0.08 0.1 0.12"
LINEAR SPEEDUP OF COMP-AMS,0.2665505226480836,Train Loss MNIST
LINEAR SPEEDUP OF COMP-AMS,0.2682926829268293,"n = 2
n = 4
n = 8
n = 16"
LINEAR SPEEDUP OF COMP-AMS,0.2700348432055749,"0
5000
10000
15000
Iterations 0.6 0.8 1 1.2 1.4"
LINEAR SPEEDUP OF COMP-AMS,0.27177700348432055,Train Loss CIFAR
LINEAR SPEEDUP OF COMP-AMS,0.2735191637630662,"n = 2
n = 4
n = 8
n = 16"
LINEAR SPEEDUP OF COMP-AMS,0.27526132404181186,"Figure 3: The linear speedup of COMP-AMS with varying n. Left: MNIST with Block-Sign
compressor on CNN. Right: CIFAR-10 with Top-k-0.01 compression on LeNet."
LINEAR SPEEDUP OF COMP-AMS,0.2770034843205575,"Corollary 2 reveals the linear speedup of COMP-AMS in distributed training. In Figure 3, we present
the training loss on MNIST and CIFAR-10 against the number of iterations, with varying number
of workers n. We use COMP-AMS with Block-Sign on MNIST, and Top-k-0.01 on CIFAR. As
suggested by the theory, we use 5×10−4√n as the learning rate. From Figure 3, we see the number
of iterations to achieve a certain loss exhibits a strong linear relationship with n—it (approximately)
decreases by half whenever we double n, which justiﬁes the linear speedup of COMP-AMS."
DISCUSSION,0.2787456445993031,"5.4
DISCUSSION"
DISCUSSION,0.2804878048780488,"We provide a brief summary of our empirical observations. The proposed COMP-AMS is able to
match the learning performance of full-gradient AMSGrad in all the presented experiments. In par-
ticular, for data/model involving some sparsity structure, COMP-AMS with the Top-k compressor
could be more effective. Also, our results reveal that 1BitAdam might be quite sensitive to the
pre-conditioning quality, while COMP-AMS can be more easily tuned and implemented in practice."
DISCUSSION,0.28222996515679444,"We would like to emphasize that, the primary goal of the experiments is to show that COMP-AMS
is able to match the performance of full-precision AMSGrad, but not to argue that COMP-AMS
is always better than the other algorithms. Since different methods use different underlying opti-
mization algorithms (e.g., AMSGrad, Adam, momentum SGD), comparing COMP-AMS with other
distributed training methods would be largely determined by the comparison among these optimiza-
tion protocols, which is typically data and task dependent. Our results say that: whenever one wants
to use AMSGrad to train a deep neural network, she/he can simply employ the distributed COMP-
AMS scheme to gain a linear speedup in training time with learning performance as good as the
full-precision training, taking little communication cost and memory consumption."
CONCLUSION,0.28397212543554007,"6
CONCLUSION"
CONCLUSION,0.2857142857142857,"In this paper, we study the simple, convenient, yet unexplored gradient averaging strategy for dis-
tributed adaptive optimization called COMP-AMS. Top-k and Block-Sign compressor are incorpo-
rated for communication efﬁciency, whose biases are compensated by the error feedback strategy.
We develop the convergence rate of COMP-AMS, and show that same as the case of SGD, for AMS-
Grad, compressed gradient averaging with error feedback matches the convergence of full-gradient
AMSGrad, and linear speedup can be obtained in the distributed training. Numerical experiments are
conducted to justify the theoretical ﬁndings, and demonstrate that COMP-AMS provides comparable
performance with other distributed adaptive methods, and achieves similar accuracy as full-precision
AMSGrad with signiﬁcantly reduced communication overhead. Given the simple architecture and
hardware (memory) efﬁciency, we expect COMP-AMS shall be able to serve as a useful and conve-
nient distributed adaptive optimization framework in practice."
CONCLUSION,0.2874564459930314,Published as a conference paper at ICLR 2022
REFERENCES,0.289198606271777,REFERENCES
REFERENCES,0.29094076655052264,"Naman Agarwal, Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and Brendan McMahan.
cpSGD: Communication-efﬁcient and differentially-private distributed SGD. In Advances in Neu-
ral Information Processing Systems (NeurIPS), pp. 7575–7586, Montréal, Canada, 2018."
REFERENCES,0.2926829268292683,"Ahmad Ajalloeian and Sebastian U Stich. Analysis of SGD with biased gradient estimators. arXiv
preprint arXiv:2008.00051, 2020."
REFERENCES,0.29442508710801396,"Alham Fikri Aji and Kenneth Heaﬁeld. Sparse communication for distributed gradient descent.
In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 440–445, Copenhagen, Denmark, 2017."
REFERENCES,0.2961672473867596,"Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
QSGD:
communication-efﬁcient SGD via gradient quantization and encoding. In Advances in Neural
Information Processing Systems (NIPS), pp. 1709–1720, Long Beach, CA, 2017."
REFERENCES,0.2979094076655052,"Dan Alistarh, Torsten Hoeﬂer, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and Cédric
Renggli. The convergence of sparsiﬁed gradient methods. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 5977–5987, Montréal, Canada, 2018."
REFERENCES,0.29965156794425085,"Debraj Basu, Deepesh Data, Can Karakus, and Suhas N. Diggavi. Qsparse-local-sgd: Distributed
SGD with quantization, sparsiﬁcation and local computations. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 14668–14679, Vancouver, Canada, 2019."
REFERENCES,0.30139372822299654,"Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
SIGNSGD: compressed optimisation for non-convex problems. In Proceedings of the 35th Inter-
national Conference on Machine Learning (ICML), pp. 559–568, Stockholmsmässan, Stockholm,
Sweden, 2018."
REFERENCES,0.30313588850174217,"Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signSGD with
majority vote is communication efﬁcient and fault tolerant. In Proceedings of the 7th International
Conference on Learning Representations (ICLR), New Orleans, LA, 2019."
REFERENCES,0.3048780487804878,"Aleksandr Beznosikov, Samuel Horváth, Peter Richtárik, and Mher Safaryan. On biased compres-
sion for distributed learning. arXiv preprint arXiv:2002.12410, 2020."
REFERENCES,0.30662020905923343,"Stephen P. Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed opti-
mization and statistical learning via the alternating direction method of multipliers. Found. Trends
Mach. Learn., 3(1):1–122, 2011."
REFERENCES,0.3083623693379791,"Ken Chang, Niranjan Balachandar, Carson K. Lam, Darvin Yi, James M. Brown, Andrew Beers,
Bruce R. Rosen, Daniel L. Rubin, and Jayashree Kalpathy-Cramer. Distributed deep learning
networks among institutions for medical imaging. J. Am. Medical Informatics Assoc., 25(8):
945–954, 2018."
REFERENCES,0.31010452961672474,"Congliang Chen, Li Shen, Hao-Zhi Huang, and Wei Liu. Quantized adam with error feedback. ACM
Trans. Intell. Syst. Technol., 12(5):56:1–56:26, 2021a."
REFERENCES,0.3118466898954704,"Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of A class of adam-type
algorithms for non-convex optimization. In Proceedings of the 7th International Conference on
Learning Representations (ICLR), New Orleans, LA, 2019."
REFERENCES,0.313588850174216,"Xiangyi Chen, Xiaoyun Li, and Ping Li. Toward communication efﬁcient adaptive gradient method.
In Proceedings of the ACM-IMS Foundations of Data Science Conference (FODS), pp. 119–128,
Seattle, WA, 2020."
REFERENCES,0.3153310104529617,"Xiangyi Chen, Belhal Karimi, Weijie Zhao, and Ping Li.
On the convergence of decentralized
adaptive gradient methods. arXiv preprint arXiv:2109.03194, 2021b."
REFERENCES,0.3170731707317073,"Trishul M. Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. Project adam:
Building an efﬁcient and scalable deep learning training system.
In Proceedings of the 11th
USENIX Symposium on Operating Systems Design and Implementation (OSDI), pp. 571–582,
Broomﬁeld, CO, 2014."
REFERENCES,0.31881533101045295,Published as a conference paper at ICLR 2022
REFERENCES,0.3205574912891986,"Dami Choi, Christopher J Shallue, Zachary Nado, Jaehoon Lee, Chris J Maddison, and
George E Dahl.
On empirical comparisons of optimizers for deep learning.
arXiv preprint
arXiv:1910.05446, 2019."
REFERENCES,0.32229965156794427,"Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations.
In Proceedings of the 10th ACM Conference on Recommender Systems, pp. 191–198, Boston,
MA, 2016."
REFERENCES,0.3240418118466899,"Tim Dettmers. 8-bit approximations for parallelism in deep learning. In Proceedings of the 4th
International Conference on Learning Representations (ICLR), San Juan, Puerto Rico, 2016."
REFERENCES,0.32578397212543553,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies (NAACL-HLT), pp. 4171–4186, Minneapolis, MN, 2019."
REFERENCES,0.32752613240418116,"John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. In Proceedings of the 23rd Conference on Learning Theory (COLT),
pp. 257–269, Haifa, Israel, 2010."
REFERENCES,0.32926829268292684,"John C. Duchi, Alekh Agarwal, and Martin J. Wainwright. Dual averaging for distributed optimiza-
tion: Convergence analysis and network scaling. IEEE Trans. Autom. Control., 57(3):592–606,
2012."
REFERENCES,0.3310104529616725,"Saeed Ghadimi and Guanghui Lan.
Stochastic ﬁrst- and zeroth-order methods for nonconvex
stochastic programming. SIAM J. Optim., 23(4):2341–2368, 2013."
REFERENCES,0.3327526132404181,"Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems (NIPS), pp. 2672–2680, Montreal, Canada, 2014."
REFERENCES,0.3344947735191638,"Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017."
REFERENCES,0.3362369337979094,"Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep
recurrent neural networks. In Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 6645–6649, Vancouver, Canada, 2013."
REFERENCES,0.33797909407665505,"Farzin Haddadpour, Belhal Karimi, Ping Li, and Xiaoyun Li. Fedsketch: Communication-efﬁcient
and private federated learning via sketching. arXiv preprint arXiv:2008.04975, 2020."
REFERENCES,0.3397212543554007,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 770–778, Las Vegas, NV, 2016."
REFERENCES,0.34146341463414637,"Mingyi Hong, Davood Hajinezhad, and Ming-Min Zhao. Prox-pda: The proximal primal-dual algo-
rithm for fast distributed nonconvex optimization and learning over networks. In Proceedings of
the 34th International Conference on Machine Learning (ICML), pp. 1529–1538, Sydney, Aus-
tralia, 2017."
REFERENCES,0.343205574912892,"Nikita Ivkin, Daniel Rothchild, Enayat Ullah, Vladimir Braverman, Ion Stoica, and Raman Arora.
Communication-efﬁcient distributed SGD with sketching. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 13144–13154, Vancouver, Canada, 2019."
REFERENCES,0.34494773519163763,"Jiawei Jiang, Fangcheng Fu, Tong Yang, and Bin Cui. Sketchml: Accelerating distributed machine
learning with data sketches. In Proceedings of the 2018 ACM International Conference on Man-
agement of Data (SIGMOD), pp. 1269–1284, Houston, TX, 2018."
REFERENCES,0.34668989547038326,"Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with sparse
and quantized communication. In Advances in Neural Information Processing Systems (NeurIPS),
pp. 2530–2541, Montréal, Canada, 2018."
REFERENCES,0.34843205574912894,Published as a conference paper at ICLR 2022
REFERENCES,0.3501742160278746,"Belhal Karimi, Xiaoyun Li, and Ping Li. Fed-LAMB: Layerwise and dimensionwise locally adaptive
optimization algorithm. arXiv preprint arXiv:2110.00532, 2021."
REFERENCES,0.3519163763066202,"Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U. Stich, and Martin Jaggi. Error feedback
ﬁxes signsgd and other gradient compression schemes. In Proceedings of the 36th International
Conference on Machine Learning (ICML), pp. 3252–3261, Long Beach, CA, 2019."
REFERENCES,0.35365853658536583,"Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebas-
tian U Stich, and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms
in federated learning. arXiv preprint arXiv:2008.03606, 2020."
REFERENCES,0.3554006968641115,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings
of the 3rd International Conference on Learning Representations (ICLR), San Diego, CA, 2015."
REFERENCES,0.35714285714285715,"Anastasia Koloskova, Sebastian U. Stich, and Martin Jaggi. Decentralized stochastic optimization
and gossip algorithms with compressed communication. In Proceedings of the 36th International
Conference on Machine Learning (ICML), pp. 3478–3487, Long Beach, CA, 2019."
REFERENCES,0.3588850174216028,"A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s
thesis, Department of Computer Science, University of Toronto, 2009."
REFERENCES,0.3606271777003484,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.3623693379790941,"Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. J. Mach. Learn. Res., 17:39:1–39:40, 2016."
REFERENCES,0.3641114982578397,"Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing
the communication bandwidth for distributed training. In Proceedings of the 6th International
Conference on Learning Representations (ICLR), Vancouver, Canada, 2018."
REFERENCES,0.36585365853658536,"Songtao Lu, Xinwei Zhang, Haoran Sun, and Mingyi Hong. GNSD: a gradient-tracking based
nonconvex stochastic algorithm for decentralized optimization. In Proceedings of the 2019 IEEE
Data Science Workshop (DSW), pp. 315–321, 2019."
REFERENCES,0.367595818815331,"Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),
pp. 142–150, Portland, OR, 2011."
REFERENCES,0.3693379790940767,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In Proceedings of
the 20th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pp. 1273–
1282, Fort Lauderdale, FL, 2017."
REFERENCES,0.3710801393728223,"Hiroaki Mikami, Hisahiro Suganuma, Yoshiki Tanaka, and Yuichi Kageyama. Massively distributed
SGD: Imagenet/resnet-50 training in a ﬂash. arXiv preprint arXiv:1811.05233, 2018."
REFERENCES,0.37282229965156793,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller.
Playing atari with deep reinforcement learning.
arXiv preprint
arXiv:1312.5602, 2013."
REFERENCES,0.37456445993031356,"Parvin Nazari, Davoud Ataee Tarzanagh, and George Michailidis.
Dadam: A consensus-based
distributed adaptive gradient method for online optimization. arXiv preprint arXiv:1901.09109,
2019."
REFERENCES,0.37630662020905925,"Angelia Nedic and Asuman E. Ozdaglar. Distributed subgradient methods for multi-agent optimiza-
tion. IEEE Trans. Autom. Control., 54(1):48–61, 2009."
REFERENCES,0.3780487804878049,"Arkadi Nemirovski, Anatoli B. Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM J. Optim., 19(4):1574–1609, 2009."
REFERENCES,0.3797909407665505,"Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
Proceedings of the 6th International Conference on Learning Representations (ICLR), Vancouver,
Canada, 2018."
REFERENCES,0.38153310104529614,Published as a conference paper at ICLR 2022
REFERENCES,0.3832752613240418,"Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcný,
Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In Proceedings
of the 9th International Conference on Learning Representations (ICLR), Virtual Event, 2021."
REFERENCES,0.38501742160278746,"Peter Richtárik, Igor Sokolov, and Ilyas Fatkhullin. EF21: A new, simpler, theoretically better,
and practically faster error feedback. In Advances in Neural Information Processing Systems
(NeurIPS), virtual, 2021."
REFERENCES,0.3867595818815331,"Christopher De Sa, Matthew Feldman, Christopher Ré, and Kunle Olukotun. Understanding and op-
timizing asynchronous low-precision stochastic gradient descent. In Proceedings of the 44th An-
nual International Symposium on Computer Architecture (ISCA), pp. 561–574, Toronto, Canada,
2017."
REFERENCES,0.3885017421602787,"Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and
its application to data-parallel distributed training of speech dnns. In Proceedings of the 15th
Annual Conference of the International Speech Communication Association (ISCA), pp. 1058–
1062, Singapore, 2014."
REFERENCES,0.3902439024390244,"Zebang Shen, Aryan Mokhtari, Tengfei Zhou, Peilin Zhao, and Hui Qian. Towards more efﬁcient
stochastic decentralized learning: Faster convergence and sparse communication. In Proceed-
ings of the 35th International Conference on Machine Learning (ICML), pp. 4631–4640, Stock-
holmsmässan, Stockholm, Sweden, 2018."
REFERENCES,0.39198606271777003,"Shaohuai Shi, Kaiyong Zhao, Qiang Wang, Zhenheng Tang, and Xiaowen Chu. A convergence anal-
ysis of distributed SGD with communication-efﬁcient gradient sparsiﬁcation. In Proceedings of
the 28th International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 3411–3417, Macao,
China, 2019."
REFERENCES,0.39372822299651566,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap,
Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Master-
ing the game of go without human knowledge. Nat., 550(7676):354–359, 2017."
REFERENCES,0.39547038327526135,"Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for
sgd with delayed gradients and compressed communication. arXiv preprint arXiv:1909.05350,
2019."
REFERENCES,0.397212543554007,"Sebastian U. Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsiﬁed SGD with memory.
In Advances in Neural Information Processing Systems (NeurIPS), pp. 4447–4458, Montréal,
Canada, 2018."
REFERENCES,0.3989547038327526,"Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru
Lian, Ji Liu, Ce Zhang, and Yuxiong He. 1-bit adam: Communication efﬁcient large-scale training
with adam’s convergence speed. In Proceedings of the 38th International Conference on Machine
Learning (ICML), pp. 10118–10129, Virtual Event, 2021."
REFERENCES,0.40069686411149824,"Jun-Kun Wang, Xiaoyun Li, Belhal Karimi, and Ping Li. An optimistic acceleration of amsgrad
for nonconvex optimization. In Proceedings of Asian Conference on Machine Learning (ACML),
volume 157, pp. 422–437, Virtual Event, 2021."
REFERENCES,0.4024390243902439,"Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsiﬁcation for communication-
efﬁcient distributed optimization.
In Advances in Neural Information Processing Systems
(NeurIPS), pp. 1299–1309, Montréal, Canada, 2018."
REFERENCES,0.40418118466898956,"Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad:
Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural
Information Processing Systems (NIPS), pp. 1509–1519, Long Beach, CA, 2017."
REFERENCES,0.4059233449477352,"Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang. Error compensated quantized SGD
and its applications to large-scale distributed optimization. In Proceedings of the 35th Interna-
tional Conference on Machine Learning (ICML), pp. 5321–5329, Stockholmsmässan, Stockholm,
Sweden, 2018."
REFERENCES,0.4076655052264808,Published as a conference paper at ICLR 2022
REFERENCES,0.4094076655052265,"Zhiqiang Xu, Dong Li, Weijie Zhao, Xing Shen, Tianbo Huang, Xiaoyun Li, and Ping Li. Agile
and accurate CTR prediction model training for massive-scale online advertising systems. In
Proceedings of the International Conference on Management of Data (SIGMOD), pp. 2404–2409,
Virtual Event, China, 2021."
REFERENCES,0.41114982578397213,"Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew Gordon Wilson, and Christo-
pher De Sa. SWALP : Stochastic weight averaging in low precision training. In Proceedings of
the 36th International Conference on Machine Learning (ICML), pp. 7015–7024, Long Beach,
CA, 2019."
REFERENCES,0.41289198606271776,"Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
Large batch optimization for deep
learning: Training BERT in 76 minutes. In Proceedings of the 8th International Conference
on Learning Representations (ICLR), Addis Ababa, Ethiopia, 2020."
REFERENCES,0.4146341463414634,"Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. Recent trends in deep learn-
ing based natural language processing. IEEE Comput. Intell. Mag., 13(3):55–75, 2018."
REFERENCES,0.4163763066202091,"Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efﬁcient mo-
mentum SGD for distributed non-convex optimization. In Proceedings of the 36th International
Conference on Machine Learning (ICML), pp. 7184–7193, Long Beach, CA, 2019a."
REFERENCES,0.4181184668989547,"Yue Yu, Jiaxiang Wu, and Junzhou Huang. Exploring fast and communication-efﬁcient algorithms
in large-scale distributed networks. In Proceedings of the 22nd International Conference on Arti-
ﬁcial Intelligence and Statistics (AISTATS), pp. 674–683, Naha, Okinawa, Japan, 2019b."
REFERENCES,0.41986062717770034,"Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012."
REFERENCES,0.42160278745644597,"Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. ZipML: Training linear
models with end-to-end low precision, and a little bit of deep learning. In Proceedings of the
34th International Conference on Machine Learning (ICML), pp. 4035–4043, Sydney, Australia,
2017."
REFERENCES,0.42334494773519166,"Lei Zhang, Shuai Wang, and Bing Liu. Deep learning for sentiment analysis: A survey. Wiley
Interdiscip. Rev. Data Min. Knowl. Discov., 8(4), 2018."
REFERENCES,0.4250871080139373,"Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, and Yoav Artzi. Revisiting few-
sample BERT ﬁne-tuning. In Proceedings of the 9th International Conference on Learning Rep-
resentations (ICLR), Virtual Event, 2021."
REFERENCES,0.4268292682926829,"Weijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian, Ronglai Jia, and Ping Li. AIBox: CTR pre-
diction model training on a single node. In Proceedings of the 28th ACM International Conference
on Information and Knowledge Management (CIKM), pp. 319–328, Beijing, China, 2019."
REFERENCES,0.42857142857142855,"Weijie Zhao, Xuewu Jiao, Mingqing Hu, Xiaoyun Li, Xiangyu Zhang, and Ping Li. Communication-
efﬁcient terabyte-scale model training framework for online advertising.
arXiv preprint
arXiv:2201.05500, 2022."
REFERENCES,0.43031358885017423,"Shuai Zheng, Ziyue Huang, and James T. Kwok. Communication-efﬁcient distributed blockwise
momentum SGD with error-feedback. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 11446–11456, Vancouver, Canada, 2019."
REFERENCES,0.43205574912891986,"Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu.
On
the convergence of adaptive gradient methods for nonconvex optimization.
arXiv preprint
arXiv:1808.05671, 2018."
REFERENCES,0.4337979094076655,"Yingxue Zhou, Belhal Karimi, Jinxing Yu, Zhiqiang Xu, and Ping Li. Towards better generalization
of adaptive gradient methods. In Advances in Neural Information Processing Systems (NeurIPS),
virtual, 2020."
REFERENCES,0.4355400696864111,Published as a conference paper at ICLR 2022
REFERENCES,0.4372822299651568,"A
TUNING DETAILS AND MORE RESULTS ON RESNET-18"
REFERENCES,0.43902439024390244,"The search grids of the learning rate of each method can be found in Table 1. Empirically, Dist-
AMS, COMP-AMS and 1BitAdam has similar optimal learning rate, while QAdam usually needs
larger step size to reach its best performance."
REFERENCES,0.44076655052264807,Table 1: Search grids for learning rate tuning.
REFERENCES,0.4425087108013937,"Learning rate range
Dist-AMS
[0.00001, 0.00003, 0.00005, 0.0001, 0.0003, 0.0005, 0.001, 0.003, 0.005, 0.01]
Comp-AMS
[0.00001, 0.00003, 0.00005, 0.0001, 0.0003, 0.0005, 0.001, 0.003, 0.005, 0.01]
QAdam
[0.0001, 0.0003, 0.0005, 0.001, 0.003, 0.005, 0.01, 0.03, 0.05, 0.1, 0.3, 0.5]
1BitAdam
[0.00001, 0.00003, 0.00005, 0.0001, 0.0003, 0.0005, 0.001, 0.003, 0.005, 0.01]"
REFERENCES,0.4442508710801394,"We provide more experimental results on CIFAR-10 dataset, trained with ResNet-18 (He et al.,
2016). For reference, we also present the result of distributed SGD. As we can see from Figure 4,
again COMP-AMS can achieve similar accuracy as AMSGrad, and the Top-k compressor gives the
best accuracy, with substantial communication reduction. Note that distributed SGD converges faster
than adaptive methods, but the generalization error is slightly worse. This experiment again conﬁrms
that COMP-AMS can serve as a simple and convenient distributed adaptive training framework with
fast convergence, reduced communication and little performance drop."
REFERENCES,0.445993031358885,"0
25
50
75
100
Epochs 0 0.5 1 1.5"
REFERENCES,0.44773519163763065,Train Loss CIFAR
REFERENCES,0.44947735191637633,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
Dist-SGD"
REFERENCES,0.45121951219512196,"0
25
50
75
100
Epochs 40 60 80 100"
REFERENCES,0.4529616724738676,Test Accuracy CIFAR
REFERENCES,0.4547038327526132,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
Dist-SGD"
REFERENCES,0.4564459930313589,"30
35
40
45
log(No. bits) 20 40 60 80 100"
REFERENCES,0.45818815331010454,Test Accuracy CIFAR
REFERENCES,0.45993031358885017,"Dist-AMS
COMP-AMS-TopK
COMP-AMS-BkSign
QAdam
Dist-SGD"
REFERENCES,0.4616724738675958,"Figure 4: Training loss and test accuracy of different distributed training methods on CIFAR-10 with
ResNet-18 (He et al., 2016)."
REFERENCES,0.4634146341463415,Published as a conference paper at ICLR 2022
REFERENCES,0.4651567944250871,"B
PROOF OF CONVERGENCE RESULTS"
REFERENCES,0.46689895470383275,"In this section, we provide the proof of our main result."
REFERENCES,0.4686411149825784,"B.1
PROOF OF THEOREM 1"
REFERENCES,0.47038327526132406,"Theorem. Denote C0 =
q"
REFERENCES,0.4721254355400697,4(1+q2)3
REFERENCES,0.4738675958188153,"(1−q2)2 G2 + ϵ, C1 =
β1
1−β1 +
2q
1−q2 . Under Assumption 1 to Assump-
tion 4, with ηt = η ≤
ϵ
3C0√"
REFERENCES,0.47560975609756095,"2L max{2L,C2}, for any T > 0, COMP-AMS satisﬁes"
T,0.47735191637630664,"1
T T
X"
T,0.47909407665505227,"t=1
E[∥∇f(θt)∥2] ≤2C0
E[f(θ1) −f(θ∗)]"
T,0.4808362369337979,"Tη
+ ηLσ2"
T,0.48257839721254353,"nϵ
+ 3η2LC0C2
1σ2 nϵ2"
T,0.4843205574912892,"+ 12η2q2LC0σ2
g
(1 −q2)2ϵ2
+ (1 + C1)G2d"
T,0.48606271777003485,"T√ϵ
+ η(1 + 2C1)C1LG2d Tϵ 
."
T,0.4878048780487805,"Proof. We ﬁrst clarify some notations. At time t, let the full-precision gradient of the i-th worker
be gt,i, the error accumulator be et,i, and the compressed gradient be ˜gt,i = C(gt,i + et,i). Slightly
different from the notations in the algorithm, we denote ¯gt = 1"
T,0.4895470383275261,"n
Pn
i=1 gt,i, ˜gt = 1"
T,0.4912891986062718,"n
Pn
i=1 ˜gt,i and
¯et = 1"
T,0.4930313588850174,"n
Pn
i=1 et,i. The second moment computed by the compressed gradients is denoted as vt ="
T,0.49477351916376305,"β2vt−1 + (1 −β2)˜g
2
t, and ˆvt = max{ˆvt−1, vt}. Also, the ﬁrst order moving average sequence"
T,0.4965156794425087,"mt = β1mt−1 + (1 −β1)˜gt
and
m′
t = β1m′
t−1 + (1 −β1)¯gt,"
T,0.49825783972125437,"where m′
t represents the ﬁrst moment moving average sequence using the uncompressed stochastic
gradients. By construction we have m′
t = (1 −β1) Pt
τ=1 βt−τ
1
¯gτ."
T,0.5,"Our proof will use the following auxiliary sequences,"
T,0.5017421602787456,"Et+1 := (1 −β1) t+1
X"
T,0.5034843205574913,"τ=1
βt+1−τ
1
¯eτ,"
T,0.5052264808362369,"θ′
t+1 := θt+1 −η
Et+1
√ˆvt + ϵ."
T,0.5069686411149826,"Then, we can write the evolution of θ′
t as"
T,0.5087108013937283,"θ′
t+1 = θt+1 −η
Et+1
√ˆvt + ϵ"
T,0.5104529616724739,"= θt −η (1 −β1) Pt
τ=1 βt−τ
1
˜gτ + (1 −β1) Pt+1
τ=1 βt+1−τ
1
¯eτ
√ˆvt + ϵ"
T,0.5121951219512195,"= θt −η (1 −β1) Pt
τ=1 βt−τ
1
(˜gτ + ¯eτ+1) + (1 −β)βt
1¯e1
√ˆvt + ϵ"
T,0.5139372822299652,"= θt −η (1 −β1) Pt
τ=1 βt−τ
1
¯eτ
√ˆvt + ϵ
−η
m′
t
√ˆvt + ϵ"
T,0.5156794425087108,"= θt −η
Et
p"
T,0.5174216027874564,"ˆvt−1 + ϵ
−η
m′
t
√ˆvt + ϵ + η(
1
p"
T,0.519163763066202,"ˆvt−1 + ϵ
−
1
√ˆvt + ϵ)Et"
T,0.5209059233449478,"(a)
= θ′
t −η
m′
t
√ˆvt + ϵ + η(
1
p"
T,0.5226480836236934,"ˆvt−1 + ϵ
−
1
√ˆvt + ϵ)Et"
T,0.524390243902439,":= θ′
t −η
m′
t
√ˆvt + ϵ + ηDtEt,"
T,0.5261324041811847,"where (a) uses the fact that for every i ∈[n], ˜gt,i + et+1,i = gt,i + et,i, and et,1 = 0 at initialization.
Further deﬁne the virtual iterates:"
T,0.5278745644599303,"xt+1 := θ′
t+1 −η
β1
1 −β1"
T,0.5296167247386759,"m′
t
√ˆvt + ϵ,"
T,0.5313588850174216,Published as a conference paper at ICLR 2022
T,0.5331010452961672,which follows the recurrence:
T,0.5348432055749129,"xt+1 = θ′
t+1 −η
β1
1 −β1"
T,0.5365853658536586,"m′
t
√ˆvt + ϵ"
T,0.5383275261324042,"= θ′
t −η
m′
t
√ˆvt + ϵ −η
β1
1 −β1"
T,0.5400696864111498,"m′
t
√ˆvt + ϵ + ηDtEt"
T,0.5418118466898955,"= θ′
t −η
β1m′
t−1 + (1 −β1)¯gt +
β2
1
1−β1 m′
t−1 + β1¯gt
√ˆvt + ϵ
+ ηDtEt"
T,0.5435540069686411,"= θ′
t −η
β1
1 −β1"
T,0.5452961672473867,"m′
t−1
√ˆvt + ϵ −η
¯gt
√ˆvt + ϵ + ηDtEt"
T,0.5470383275261324,"= xt −η
¯gt
√ˆvt + ϵ + η
β1
1 −β1
Dtm′
t−1 + ηDtEt."
T,0.5487804878048781,"When summing over t = 1, ..., T, the difference sequence Dt satisﬁes the bounds of Lemma 5."
T,0.5505226480836237,"By the smoothness Assumption 2, we have"
T,0.5522648083623694,"f(xt+1) ≤f(xt) + ⟨∇f(xt), xt+1 −xt⟩+ L"
T,0.554006968641115,2 ∥xt+1 −xt∥2.
T,0.5557491289198606,"Taking expectation w.r.t. the randomness at time t, we obtain
E[f(xt+1)] −f(xt)"
T,0.5574912891986062,"≤−ηE[⟨∇f(xt),
¯gt
√ˆvt + ϵ⟩] + ηE[⟨∇f(xt),
β1
1 −β1
Dtm′
t−1 + DtEt⟩] + η2L"
T,0.5592334494773519,"2 E[∥
¯gt
√ˆvt + ϵ −
β1
1 −β1
Dtm′
t−1 −DtEt∥2]"
T,0.5609756097560976,"= −ηE[⟨∇f(θt),
¯gt
√ˆvt + ϵ⟩]
|
{z
}
I"
T,0.5627177700348432,"+ ηE[⟨∇f(xt),
β1
1 −β1
Dtm′
t−1 + DtEt⟩]
|
{z
}
II + η2L"
T,0.5644599303135889,"2 E[∥
¯gt
√ˆvt + ϵ −
β1
1 −β1
Dtm′
t−1 −DtEt∥2]
|
{z
}
III"
T,0.5662020905923345,"+ ηE[⟨∇f(θt) −∇f(xt),
¯gt
√ˆvt + ϵ⟩]
|
{z
}
IV , (3)"
T,0.5679442508710801,"In the following, we bound the terms separately."
T,0.5696864111498258,Bounding term I. We have
T,0.5714285714285714,"I = −ηE[⟨∇f(θt),
¯gt
p"
T,0.573170731707317,"ˆvt−1 + ϵ
] −ηE[⟨∇f(θt), (
1
√ˆvt + ϵ −
1
p"
T,0.5749128919860628,"ˆvt−1 + ϵ
)¯gt⟩]"
T,0.5766550522648084,"≤−ηE[⟨∇f(θt),
∇f(θt)
p"
T,0.578397212543554,"ˆvt−1 + ϵ
] + ηG2E[∥Dt∥]."
T,0.5801393728222997,"≤−
η
q"
T,0.5818815331010453,4(1+q2)3
T,0.5836236933797909,"(1−q2)2 G2 + ϵ
E[∥∇f(θt)∥2] + ηG2E[∥Dt∥1],
(4)"
T,0.5853658536585366,"where we use Assumption 3, Lemma 4 and the fact that l2 norm is no larger than l1 norm."
T,0.5871080139372822,"Bounding term II. By the deﬁnition of Et, we know that ∥Et∥≤(1 −β1) Pt
τ=1 βt−τ
1
∥¯et∥≤
2q
1−q2 G. Then we have"
T,0.5888501742160279,"II ≤η(E[⟨∇f(θt),
β1
1 −β1
Dtm′
t−1 + DtEt⟩] + E[⟨∇f(xt) −∇f(θt),
β1
1 −β1
Dtm′
t−1 + DtEt⟩])"
T,0.5905923344947736,"≤ηE[∥∇f(θt)∥∥
β1
1 −β1
Dtm′
t−1 + DtEt∥] + η2 LE[∥"
T,0.5923344947735192,"β1
1−β1 m′
t−1 + Et
p"
T,0.5940766550522648,"ˆvt−1 + ϵ
∥∥
β1
1 −β1
Dtm′
t−1 + DtEt∥]"
T,0.5958188153310104,"≤ηC1G2E[∥Dt∥1] + η2C2
1LG2
√ϵ
E[∥Dt∥1],
(5)"
T,0.5975609756097561,Published as a conference paper at ICLR 2022
T,0.5993031358885017,"where C1 :=
β1
1−β1 +
2q
1−q2 . The second inequality is because of smoothness of f(θ), and the last
inequality is due to Lemma 2, Assumption 3 and the property of norms."
T,0.6010452961672473,Bounding term III. This term can be bounded as follows:
T,0.6027874564459931,"III ≤η2LE[∥
¯gt
√ˆvt + ϵ∥2] + η2LE[∥
β1
1 −β1
Dtm′
t−1 −DtEt∥2]] ≤η2L"
T,0.6045296167247387,"ϵ E[∥1 n n
X"
T,0.6062717770034843,"i=1
gt,i −∇f(θt) + ∇f(θt)∥2] + η2LE[∥Dt(
β1
1 −β1
m′
t−1 −Et)∥2]"
T,0.60801393728223,"(a)
≤η2L"
T,0.6097560975609756,ϵ E[∥∇f(θt)∥2] + η2Lσ2
T,0.6114982578397212,"nϵ
+ η2C2
1LG2E[∥Dt∥2],
(6)"
T,0.6132404181184669,"where (a) follows from ∇f(θt) =
1
n
Pn
i=1 ∇fi(θt) and Assumption 4 that gt,i is unbiased of
∇fi(θt) and has bounded variance σ2."
T,0.6149825783972126,Bounding term IV. We have
T,0.6167247386759582,"IV = ηE[⟨∇f(θt) −∇f(xt),
¯gt
p"
T,0.6184668989547039,"ˆvt−1 + ϵ
⟩] + ηE[⟨∇f(θt) −∇f(xt), (
1
√ˆvt + ϵ −
1
p"
T,0.6202090592334495,"ˆvt−1 + ϵ
)¯gt⟩]"
T,0.6219512195121951,"≤ηE[⟨∇f(θt) −∇f(xt),
∇f(θt)
p"
T,0.6236933797909407,"ˆvt−1 + ϵ
⟩] + η2LE[∥"
T,0.6254355400696864,"β1
1−β1 m′
t−1 + Et
p"
T,0.627177700348432,"ˆvt−1 + ϵ
∥∥Dtgt∥]"
T,0.6289198606271778,"(a)
≤ηρ"
T,0.6306620209059234,2ϵ E[∥∇f(θt)∥2] + η
T,0.632404181184669,2ρE[∥∇f(θt) −∇f(xt)∥2] + η2C1LG2
T,0.6341463414634146,"√ϵ
E[∥Dt∥]"
T,0.6358885017421603,"(b)
≤ηρ"
T,0.6376306620209059,2ϵ E[∥∇f(θt)∥2] + η3L
T,0.6393728222996515,2ρ E[∥
T,0.6411149825783972,"β1
1−β1 m′
t−1 + Et
p"
T,0.6428571428571429,"ˆvt−1 + ϵ
∥2] + η2C1LG2"
T,0.6445993031358885,"√ϵ
E[∥Dt∥1],
(7)"
T,0.6463414634146342,where (a) is due to Young’s inequality and (b) is based on Assumption 2.
T,0.6480836236933798,"Regarding the second term in (7), by Lemma 3 and Lemma 1, summing over t = 1, ..., T we have T
X t=1 η3L"
T,0.6498257839721254,2ρ E[∥
T,0.6515679442508711,"β1
1−β1 m′
t−1 + Et
p"
T,0.6533101045296167,"ˆvt−1 + ϵ
∥2] ≤ T
X t=1 η3L"
T,0.6550522648083623,"2ρϵ E[∥
β1
1 −β1
m′
t−1 + Et∥2] ≤ T
X t=1 η3L ρϵ"
T,0.6567944250871081,"h
β2
1
(1 −β1)2 E[∥m′
t∥2] + E[∥Et∥2]
i"
T,0.6585365853658537,"≤
Tη3β2
1Lσ2"
T,0.6602787456445993,"nρ(1 −β1)2ϵ +
η3β2
1L
ρ(1 −β1)2ϵ T
X"
T,0.662020905923345,"t=1
E[∥∇f(θt)∥2]"
T,0.6637630662020906,"+
4Tη3q2L
ρ(1 −q2)2ϵ(σ2 + σ2
g) +
4η3q2L
ρ(1 −q2)2ϵ T
X"
T,0.6655052264808362,"t=1
E[∥∇f(θt)∥2]"
T,0.6672473867595818,= Tη3LC2σ2
T,0.6689895470383276,"nρϵ
+ 4Tη3q2Lσ2
g
ρ(1 −q2)2ϵ + η3LC2 ρϵ T
X"
T,0.6707317073170732,"t=1
E[∥∇f(θt)∥2],
(8)"
T,0.6724738675958188,"with C2 :=
β2
1
(1−β1)2 +
4q2"
T,0.6742160278745645,"(1−q2)2 . Now integrating (4), (5), (6), (7) and (8) into (3), taking the tele-
scoping summation over t = 1, ..., T, we obtain
E[f(xT +1) −f(x1)] ≤(−η"
T,0.6759581881533101,"C0
+ η2L"
T,0.6777003484320557,"ϵ
+ ηρ"
T,0.6794425087108014,"2ϵ + η3LC2 ρϵ
) T
X"
T,0.681184668989547,"t=1
E[∥∇f(θt)∥2] + Tη2Lσ2"
T,0.6829268292682927,"nϵ
+ Tη3LC2σ2"
T,0.6846689895470384,"nρϵ
+ 4Tη3q2Lσ2
g
ρ(1 −q2)2ϵ"
T,0.686411149825784,"+ (η(1 + C1)G2 + η2(1 + C1)C1LG2 √ϵ
) T
X"
T,0.6881533101045296,"t=1
E[∥Dt∥1] + η2C2
1LG2
T
X"
T,0.6898954703832753,"t=1
E[∥Dt∥2."
T,0.6916376306620209,Published as a conference paper at ICLR 2022
T,0.6933797909407665,"with C0 :=
q"
T,0.6951219512195121,4(1+q2)3
T,0.6968641114982579,"(1−q2)2 G2 + ϵ. Setting η ≤
ϵ
3C0√"
T,0.6986062717770035,"2L max{2L,C2} and choosing ρ =
ϵ
3C0 , we further"
T,0.7003484320557491,arrive at
T,0.7020905923344948,"E[f(xT +1) −f(x1)] ≤−η 2C0 T
X"
T,0.7038327526132404,"t=1
E[∥∇f(θt)∥2] + Tη2Lσ2"
T,0.705574912891986,"nϵ
+ 3Tη3LC0C2σ2"
T,0.7073170731707317,"nϵ2
+ 12Tη3q2LC0σ2
g
(1 −q2)2ϵ2"
T,0.7090592334494773,"+ η(1 + C1)G2d
√ϵ
+ η2(1 + 2C1)C1LG2d ϵ
."
T,0.710801393728223,"where the inequality follows from Lemma 5. Re-arranging terms, we get that"
T,0.7125435540069687,"1
T T
X"
T,0.7142857142857143,"t=1
E[∥∇f(θt)∥2] ≤2C0
E[f(x1) −f(xT +1)]"
T,0.7160278745644599,"Tη
+ ηLσ2"
T,0.7177700348432056,"nϵ
+ 3η2LC0C2σ2 nϵ2"
T,0.7195121951219512,"+ 12η2q2LC0σ2
g
(1 −q2)2ϵ2
+ (1 + C1)G2d"
T,0.7212543554006968,"T√ϵ
+ η(1 + 2C1)C1LG2d Tϵ "
T,0.7229965156794426,"≤2C0
E[f(θ1) −f(θ∗)]"
T,0.7247386759581882,"Tη
+ ηLσ2"
T,0.7264808362369338,"nϵ
+ 3η2LC0C2
1σ2 nϵ2"
T,0.7282229965156795,"+ 12η2q2LC0σ2
g
(1 −q2)2ϵ2
+ (1 + C1)G2d"
T,0.7299651567944251,"T√ϵ
+ η(1 + 2C1)C1LG2d Tϵ 
,"
T,0.7317073170731707,"where C0 =
q"
T,0.7334494773519163,4(1+q2)3
T,0.735191637630662,"(1−q2)2 G2 + ϵ, C1 =
β1
1−β1 +
2q
1−q2 . The last inequality is because x1 = θ1,"
T,0.7369337979094077,"θ∗:= arg minθ f(θ) and the fact that C2 ≤C2
1. This completes the proof."
T,0.7386759581881533,"B.2
INTERMEDIATE LEMMATA"
T,0.740418118466899,The lemmas used in the proof of Theorem 1 are given as below.
T,0.7421602787456446,Lemma 1. Under Assumption 1 to Assumption 4 we have:
T,0.7439024390243902,"∥m′
t∥≤G,
∀t, T
X"
T,0.7456445993031359,"t=1
E∥m′
t∥2 ≤Tσ2 n
+ T
X"
T,0.7473867595818815,"t=1
E[∥∇f(θt)∥2]."
T,0.7491289198606271,"Proof. For the ﬁrst part, it is easy to see that by Assumption 3,"
T,0.7508710801393729,"∥m′
t∥= (1 −β1)∥ t
X"
T,0.7526132404181185,"τ=1
βt−τ
1
¯gt∥≤G."
T,0.7543554006968641,"For the second claim, the expected squared norm of average stochastic gradient can be bounded by"
T,0.7560975609756098,"E[∥¯g2
t ∥] = E[∥1 n n
X"
T,0.7578397212543554,"i=1
gt,i −∇f(θt) + ∇f(θt)∥2]"
T,0.759581881533101,"= E[∥1 n n
X"
T,0.7613240418118467,"i=1
(gt,i −∇fi(θt))∥2] + E[∥∇f(θt)∥2] ≤σ2"
T,0.7630662020905923,"n + E[∥∇f(θt)∥2],"
T,0.764808362369338,Published as a conference paper at ICLR 2022
T,0.7665505226480837,"where we use Assumption 4 that gt,i is unbiased with bounded variance. Let ¯gt,j denote the j-th
coordinate of ¯gt. By the updating rule of COMP-AMS, we have"
T,0.7682926829268293,"E[∥m′
t∥2] = E[∥(1 −β1) t
X"
T,0.7700348432055749,"τ=1
βt−τ
1
¯gτ∥2]"
T,0.7717770034843205,"≤(1 −β1)2
d
X"
T,0.7735191637630662,"j=1
E[( t
X"
T,0.7752613240418118,"τ=1
βt−τ
1
¯gτ,j)2]"
T,0.7770034843205574,"(a)
≤(1 −β1)2
d
X"
T,0.7787456445993032,"j=1
E[( t
X"
T,0.7804878048780488,"τ=1
βt−τ
1
)( t
X"
T,0.7822299651567944,"τ=1
βt−τ
1
¯g2
τ,j)]"
T,0.7839721254355401,"≤(1 −β1) t
X"
T,0.7857142857142857,"τ=1
βt−τ
1
E[∥¯gτ∥2] ≤σ2"
T,0.7874564459930313,"n + (1 −β1) t
X"
T,0.789198606271777,"τ=1
βt−τ
1
E[∥∇f(θt)∥2],"
T,0.7909407665505227,"where (a) is due to Cauchy-Schwartz inequality. Summing over t = 1, ..., T, we obtain T
X"
T,0.7926829268292683,"t=1
E∥m′
t∥2 ≤Tσ2 n
+ T
X"
T,0.794425087108014,"t=1
E[∥∇f(θt)∥2]."
T,0.7961672473867596,This completes the proof.
T,0.7979094076655052,"Lemma 2. Under Assumption 4, we have for ∀t and each local worker ∀i ∈[n],"
T,0.7996515679442509,"∥et,i∥2 ≤
4q2"
T,0.8013937282229965,"(1 −q2)2 G2,"
T,0.8031358885017421,"E[∥et+1,i∥2] ≤
4q2"
T,0.8048780487804879,"(1 −q2)2 σ2 +
2q2 1 −q2 t
X"
T,0.8066202090592335,"τ=1
(1 + q2"
T,0.8083623693379791,"2
)t−τE[∥∇fi(θτ)∥2]."
T,0.8101045296167247,Proof. We start by using Assumption 1 and Young’s inequality to get
T,0.8118466898954704,"∥et+1,i∥2 = ∥gt,i + et,i −C(gt,i + et,i)∥2"
T,0.813588850174216,"≤q2∥gt,i + et,i∥2"
T,0.8153310104529616,"≤q2(1 + ρ)∥et,i∥2 + q2(1 + 1"
T,0.8170731707317073,"ρ)∥gt,i∥2"
T,0.818815331010453,≤1 + q2
T,0.8205574912891986,"2
∥et,i∥2 +
2q2"
T,0.8222996515679443,"1 −q2 ∥gt,i∥2,
(9)"
T,0.8240418118466899,"where (9) is derived by choosing ρ =
1−q2"
T,0.8257839721254355,"2q2
and the fact that q < 1. Now by recursion and the
initialization e1,i = 0, we have"
T,0.8275261324041812,"E[∥et+1,i∥2] ≤
2q2 1 −q2 t
X"
T,0.8292682926829268,"τ=1
(1 + q2"
T,0.8310104529616724,"2
)t−τE[∥gτ,i∥2] ≤
4q2"
T,0.8327526132404182,"(1 −q2)2 σ2 +
2q2 1 −q2 t
X"
T,0.8344947735191638,"τ=1
(1 + q2"
T,0.8362369337979094,"2
)t−τE[∥∇fi(θτ)∥2],"
T,0.837979094076655,"which proves the second argument. Meanwhile, the absolute bound ∥et,i∥2 ≤
4q2"
T,0.8397212543554007,"(1−q2)2 G2 follows
directly from (9)."
T,0.8414634146341463,"Lemma 3. For the moving average error sequence Et, it holds that T
X"
T,0.8432055749128919,"t=1
E[∥Et∥2] ≤
4Tq2"
T,0.8449477351916377,"(1 −q2)2 (σ2 + σ2
g) +
4q2"
T,0.8466898954703833,"(1 −q2)2 T
X"
T,0.8484320557491289,"t=1
E[∥∇f(θt)∥2]."
T,0.8501742160278746,Published as a conference paper at ICLR 2022
T,0.8519163763066202,"Proof. Denote Kt,i := Pt
τ=1( 1+q2"
T,0.8536585365853658,"2
)t−τE[∥∇fi(θτ)∥2]. Using the same technique as in the proof
of Lemma 1, denoting ¯et,j as the j-th coordinate of ¯et, it follows that"
T,0.8554006968641115,"E[∥Et∥2] = E[∥(1 −β1) t
X"
T,0.8571428571428571,"τ=1
βt−τ
1
¯eτ∥2]"
T,0.8588850174216028,"≤(1 −β1)2
d
X"
T,0.8606271777003485,"j=1
E[( t
X"
T,0.8623693379790941,"τ=1
βt−τ
1
¯eτ,j)2]"
T,0.8641114982578397,"(a)
≤(1 −β1)2
d
X"
T,0.8658536585365854,"j=1
E[( t
X"
T,0.867595818815331,"τ=1
βt−τ
1
)( t
X"
T,0.8693379790940766,"τ=1
βt−τ
1
¯e2
τ,j)]"
T,0.8710801393728222,"≤(1 −β1) t
X"
T,0.872822299651568,"τ=1
βt−τ
1
E[∥¯eτ∥2]"
T,0.8745644599303136,"≤(1 −β1) t
X"
T,0.8763066202090593,"τ=1
βt−τ
1
E[ 1 n n
X"
T,0.8780487804878049,"i=1
∥eτ,i∥2]"
T,0.8797909407665505,"(b)
≤
4q2"
T,0.8815331010452961,(1 −q2)2 σ2 + 2q2(1 −β1)
T,0.8832752613240418,"(1 −q2) t
X"
T,0.8850174216027874,"τ=1
βt−τ
1
( 1 n n
X"
T,0.8867595818815331,"i=1
Kτ,i),"
T,0.8885017421602788,"where (a) is due to Cauchy-Schwartz and (b) is a result of Lemma 2. Summing over t = 1, ..., T
and using the technique of geometric series summation leads to T
X"
T,0.8902439024390244,"t=1
E[∥Et∥2] ≤
4Tq2"
T,0.89198606271777,(1 −q2)2 σ2 + 2q2(1 −β1)
T,0.8937282229965157,"(1 −q2) T
X t=1 t
X"
T,0.8954703832752613,"τ=1
βt−τ
1
( 1 n n
X"
T,0.8972125435540069,"i=1
Kτ,i)"
T,0.8989547038327527,"≤
4Tq2"
T,0.9006968641114983,"(1 −q2)2 σ2 +
2q2"
T,0.9024390243902439,"(1 −q2) T
X t=1 t
X"
T,0.9041811846689896,"τ=1
(1 + q2"
T,0.9059233449477352,"2
)t−τE[ 1 n n
X"
T,0.9076655052264808,"i=1
∥∇fi(θτ)∥2]"
T,0.9094076655052264,"≤
4Tq2"
T,0.9111498257839721,"(1 −q2)2 σ2 +
4q2"
T,0.9128919860627178,"(1 −q2)2 T
X"
T,0.9146341463414634,"t=1
E[ 1 n n
X"
T,0.9163763066202091,"i=1
∥∇fi(θt)∥2]"
T,0.9181184668989547,"(a)
≤
4Tq2"
T,0.9198606271777003,"(1 −q2)2 σ2 +
4q2"
T,0.921602787456446,"(1 −q2)2 T
X"
T,0.9233449477351916,"t=1
E[∥1 n n
X"
T,0.9250871080139372,"i=1
∇fi(θt)∥2 + 1 n n
X"
T,0.926829268292683,"i=1
∥∇fi(θt) −∇f(θt)∥2]"
T,0.9285714285714286,"≤
4Tq2"
T,0.9303135888501742,"(1 −q2)2 (σ2 + σ2
g) +
4q2"
T,0.9320557491289199,"(1 −q2)2 T
X"
T,0.9337979094076655,"t=1
E[∥∇f(θt)∥2],"
T,0.9355400696864111,"where (a) is derived by the variance decomposition and the last inequality holds due to Assumption 4.
The desired result is obtained."
T,0.9372822299651568,"Lemma 4. It holds that ∀t ∈[T], ∀i ∈[d], ˆvt,i ≤4(1+q2)3"
T,0.9390243902439024,(1−q2)2 G2.
T,0.9407665505226481,"Proof. For any t, by Lemma 2 and Assumption 3 we have
∥˜gt∥2 = ∥C(gt + et)∥2"
T,0.9425087108013938,≤∥C(gt + et) −(gt + et) + (gt + et)∥2
T,0.9442508710801394,≤2(q2 + 1)∥gt + et∥2
T,0.945993031358885,"≤4(q2 + 1)(G2 +
4q2"
T,0.9477351916376306,(1 −q2)2 G2)
T,0.9494773519163763,= 4(1 + q2)3
T,0.9512195121951219,(1 −q2)2 G2.
T,0.9529616724738676,"It’s then easy to show by the updating rule of ˆvt, there exists a j ∈[t] such that ˆvt = vj. Then"
T,0.9547038327526133,"ˆvt,i = (1 −β2) j
X"
T,0.9564459930313589,"τ=1
βj−τ
2
˜g2
τ,i ≤4(1 + q2)3"
T,0.9581881533101045,"(1 −q2)2 G2,"
T,0.9599303135888502,Published as a conference paper at ICLR 2022
T,0.9616724738675958,which concludes the claim.
T,0.9634146341463414,"Lemma 5. Let Dt :=
1
√"
T,0.9651567944250871,"ˆvt−1+ϵ −
1
√ˆvt+ϵ be deﬁned as above. Then, T
X"
T,0.9668989547038328,"t=1
∥Dt∥1 ≤d
√ϵ, T
X"
T,0.9686411149825784,"t=1
∥Dt∥2 ≤d ϵ ."
T,0.9703832752613241,"Proof. By the updating rule of COMP-AMS, ˆvt−1 ≤ˆvt for ∀t. Therefore, by the initialization
ˆv0 = 0, we have T
X"
T,0.9721254355400697,"t=1
∥Dt∥1 = T
X t=1 d
X"
T,0.9738675958188153,"i=1
(
1
p"
T,0.975609756097561,"ˆvt−1,i + ϵ −
1
p"
T,0.9773519163763066,"ˆvt,i + ϵ) = d
X"
T,0.9790940766550522,"i=1
(
1
p"
T,0.980836236933798,"ˆv0,i + ϵ −
1
p"
T,0.9825783972125436,"ˆvT,i + ϵ)"
T,0.9843205574912892,"≤d
√ϵ."
T,0.9860627177700348,"For the sum of squared l2 norm, note the fact that for a ≥b > 0, it holds that"
T,0.9878048780487805,"(a −b)2 ≤(a −b)(a + b) = a2 −b2. Thus, T
X"
T,0.9895470383275261,"t=1
∥Dt∥2 = T
X t=1 d
X"
T,0.9912891986062717,"i=1
(
1
p"
T,0.9930313588850174,"ˆvt−1,i + ϵ −
1
p"
T,0.9947735191637631,"ˆvt,i + ϵ)2 ≤ T
X t=1 d
X"
T,0.9965156794425087,"i=1
(
1
ˆvt−1,i + ϵ −
1
ˆvt,i + ϵ) ≤d ϵ ,"
T,0.9982578397212544,which gives the desired result.
