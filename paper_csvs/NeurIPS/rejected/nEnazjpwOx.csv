Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0005924170616113745,"Efficient exploration in contextual bandits is crucial due to their large action space,
1"
ABSTRACT,0.001184834123222749,"where uninformed exploration can lead to computational and statistical inefficien-
2"
ABSTRACT,0.0017772511848341231,"cies. However, the rewards of actions are often correlated, which can be leveraged
3"
ABSTRACT,0.002369668246445498,"for more efficient exploration. In this work, we use pre-trained diffusion model pri-
4"
ABSTRACT,0.002962085308056872,"ors to capture these correlations and develop diffusion Thompson sampling (dTS).
5"
ABSTRACT,0.0035545023696682463,"We establish both theoretical and algorithmic foundations for dTS. Specifically,
6"
ABSTRACT,0.004146919431279621,"we derive efficient posterior approximations (required by dTS) under a diffusion
7"
ABSTRACT,0.004739336492890996,"model prior, which are of independent interest beyond bandits and reinforcement
8"
ABSTRACT,0.00533175355450237,"learning. We analyze dTS in linear instances and provide a Bayes regret bound
9"
ABSTRACT,0.005924170616113744,"highlighting the benefits of using diffusion models as priors. Our experiments
10"
ABSTRACT,0.006516587677725118,"validate our theory and demonstrate dTS’s favorable performance.
11"
INTRODUCTION,0.0071090047393364926,"1
Introduction
12"
INTRODUCTION,0.007701421800947867,"A contextual bandit is a popular and practical framework for online learning under uncertainty [Li
13"
INTRODUCTION,0.008293838862559242,"et al., 2010]. In each round, an agent observes a context, takes an action, and receives a reward based
14"
INTRODUCTION,0.008886255924170616,"on the context and action. The goal is to maximize the expected cumulative reward over n rounds,
15"
INTRODUCTION,0.009478672985781991,"striking a balance between exploiting actions with high estimated rewards from available data and
16"
INTRODUCTION,0.010071090047393365,"exploring other actions to improve current estimates. This trade-off is often addressed using either
17"
INTRODUCTION,0.01066350710900474,"upper confidence bound (UCB) [Auer et al., 2002] or Thompson sampling (TS) [Scott, 2010].
18"
INTRODUCTION,0.011255924170616114,"The action space in contextual bandits is often large, resulting in less-than-optimal performance
19"
INTRODUCTION,0.011848341232227487,"with standard exploration strategies. Luckily, actions usually exhibit correlations, making efficient
20"
INTRODUCTION,0.012440758293838863,"exploration possible as one action may inform the agent about other actions. In particular, Thompson
21"
INTRODUCTION,0.013033175355450236,"sampling offers remarkable flexibility, allowing its integration with informative priors [Hong et al.,
22"
INTRODUCTION,0.013625592417061612,"2022b] that capture these correlations. Inspired by the achievements of diffusion models [Sohl-
23"
INTRODUCTION,0.014218009478672985,"Dickstein et al., 2015, Ho et al., 2020], which effectively approximate complex distributions [Dhariwal
24"
INTRODUCTION,0.01481042654028436,"and Nichol, 2021, Rombach et al., 2022], this work captures action correlations by employing
25"
INTRODUCTION,0.015402843601895734,"diffusion models as priors in contextual Thompson sampling.
26"
INTRODUCTION,0.015995260663507108,"We illustrate the idea using video streaming. The objective is to optimize watch time for a user j
27"
INTRODUCTION,0.016587677725118485,"by selecting a video i from a catalog of K videos. Users j and videos i are associated with context
28"
INTRODUCTION,0.017180094786729858,"vectors xj and unknown video parameters θi, respectively. User j’s expected watch time for video i
29"
INTRODUCTION,0.017772511848341232,"is linear as x⊤
j θi. Then, a natural strategy is to independently learn video parameters θi using LinTS
30"
INTRODUCTION,0.018364928909952605,"or LinUCB [Agrawal and Goyal, 2013a, Abbasi-Yadkori et al., 2011], but this proves statistically
31"
INTRODUCTION,0.018957345971563982,"inefficient for larger K. Fortunately, the reward when recommending a movie can provide informative
32"
INTRODUCTION,0.019549763033175356,"insights into other movies. To capture this, we leverage offline estimates of video parameters denoted
33"
INTRODUCTION,0.02014218009478673,"by ˆθi and build a diffusion model on them. This diffusion model approximates the video parameter
34"
INTRODUCTION,0.020734597156398103,"distribution, capturing their dependencies. This model enriches contextual Thompson sampling as a
35"
INTRODUCTION,0.02132701421800948,"prior, effectively capturing complex video dependencies while ensuring computational efficiency.
36"
INTRODUCTION,0.021919431279620854,"We introduce a framework for contextual bandits with diffusion model priors, upon which we develop
37"
INTRODUCTION,0.022511848341232227,"diffusion Thompson sampling (dTS) that is both computationally and statistically efficient. dTS
38"
INTRODUCTION,0.0231042654028436,"requires fast updates of the posterior and fast sampling from the posterior, both of which are achieved
39"
INTRODUCTION,0.023696682464454975,"through our novel efficient posterior approximations. These approximations become exact when
40"
INTRODUCTION,0.02428909952606635,"both the diffusion model and likelihood are linear. We establish a bound on dTS’s Bayes regret for
41"
INTRODUCTION,0.024881516587677725,"this specific case, highlighting the advantages of using diffusion models as priors. Our empirical
42"
INTRODUCTION,0.0254739336492891,"evaluations validate our theory and demonstrate dTS’s strong performance across various settings.
43"
INTRODUCTION,0.026066350710900472,"Diffusion models were applied in offline decision-making [Ajay et al., 2022, Janner et al., 2022, Wang
44"
INTRODUCTION,0.02665876777251185,"et al., 2022], but their use in online learning was only recently explored by Hsieh et al. [2023], who
45"
INTRODUCTION,0.027251184834123223,"focused on multi-armed bandits without theoretical guarantees. Our work extends Hsieh et al. [2023]
46"
INTRODUCTION,0.027843601895734597,"in two ways. First, we apply the concept to the broader contextual bandit, which is more practical and
47"
INTRODUCTION,0.02843601895734597,"realistic. Second, we demonstrate that with diffusion models parametrized by linear score functions
48"
INTRODUCTION,0.029028436018957347,"and linear rewards, we can derive exact closed-form posteriors without approximations. These exact
49"
INTRODUCTION,0.02962085308056872,"posteriors are valuable as they enable theoretical analysis (unlike Hsieh et al. [2023], who did not
50"
INTRODUCTION,0.030213270142180094,"provide theoretical guarantees) and motivate efficient approximations for non-linear score functions
51"
INTRODUCTION,0.030805687203791468,"in contextual bandits, addressing gaps in Hsieh et al. [2023]’s focus on multi-armed bandits.
52"
INTRODUCTION,0.03139810426540284,"A key contribution, beyond applying diffusion models in contextual bandits, is the efficient com-
53"
INTRODUCTION,0.031990521327014215,"putation and sampling of the posterior distribution of a d-dimensional parameter θ | Ht, with Ht
54"
INTRODUCTION,0.032582938388625596,"representing the data, when using a diffusion model prior on θ. This is relevant not only to bandits
55"
INTRODUCTION,0.03317535545023697,"and reinforcement learning but also to a broader range of applications [Chung et al., 2022]. To
56"
INTRODUCTION,0.03376777251184834,"motivate our approximations, we start with exact closed-form solutions for cases where both the
57"
INTRODUCTION,0.034360189573459717,"score functions of the diffusion model and the likelihood are linear. These solutions form the basis for
58"
INTRODUCTION,0.03495260663507109,"our approximations for non-linear score functions, demonstrating both strong empirical performance
59"
INTRODUCTION,0.035545023696682464,"and computational efficiency. Our approach avoids the computational burden of heavy approximate
60"
INTRODUCTION,0.03613744075829384,"sampling algorithms required for each latent parameter. For a detailed comparison with existing
61"
INTRODUCTION,0.03672985781990521,"studies, see Appendix A, where we discuss diffusion models in decision-making, structured bandits,
62"
INTRODUCTION,0.037322274881516584,"approximate posteriors, and more.
63"
SETTING,0.037914691943127965,"2
Setting
64"
SETTING,0.03850710900473934,"The agent interacts with a contextual bandit over n rounds. In round t ∈[n], the agent observes a
65"
SETTING,0.03909952606635071,"context Xt ∈X, where X ⊆Rd is a context space, it takes an action At ∈[K], and then receives a
66"
SETTING,0.039691943127962086,"stochastic reward Yt ∈R that depends on both the context Xt and the taken action At. Each action
67"
SETTING,0.04028436018957346,"i ∈[K] is associated with an unknown action parameter θ∗,i ∈Rd, so that the reward received in
68"
SETTING,0.04087677725118483,"round t is Yt ∼P(· | Xt; θ∗,At), where P(· | x; θ∗,i) is the reward distribution of action i in context
69"
SETTING,0.041469194312796206,"x. Throughout the paper, we assume that the reward distribution is parametrized as a generalized
70"
SETTING,0.04206161137440758,"linear model (GLM) [McCullagh and Nelder, 1989]. That is, for any x ∈X, P(· | x; θ∗,i) is an
71"
SETTING,0.04265402843601896,"exponential-family distribution with mean g(x⊤θ∗,i), where g is the mean function. For example, we
72"
SETTING,0.043246445497630334,"recover linear bandits when P(· | x; θ∗,i) = N(·; x⊤θ∗,i, σ2) where σ > 0 is the observation noise.
73"
SETTING,0.04383886255924171,"Similarly, we recover logistic bandits [Filippi et al., 2010] if we let g(u) = (1 + exp(−u))−1 and
74"
SETTING,0.04443127962085308,"P(· | x; θ∗,i) = Ber(g(x⊤θ∗,i)), where Ber(p) be the Bernoulli distribution with mean p.
75"
SETTING,0.045023696682464455,"We consider the Bayesian bandit setting [Russo and Van Roy, 2014, Hong et al., 2022b], where the
76"
SETTING,0.04561611374407583,"action parameters θ∗,i are assumed to be sampled from a known prior distribution. We proceed to
77"
SETTING,0.0462085308056872,"define this prior distribution using a diffusion model. The correlations between the action parameters
78"
SETTING,0.046800947867298576,"θ∗,i are captured through a diffusion model, where they share a set of L consecutive unknown latent
79"
SETTING,0.04739336492890995,"parameters ψ∗,ℓ∈Rd for ℓ∈[L]. Precisely, the action parameter θ∗,i depends on the L-th latent
80"
SETTING,0.04798578199052133,"parameter ψ∗,L as θ∗,i | ψ∗,1 ∼N(f1(ψ∗,1), Σ1), where the score function f1 : Rd →Rd is known.
81"
SETTING,0.0485781990521327,"Also, the ℓ−1-th latent parameter ψ∗,ℓ−1 depends on the ℓ-th latent parameter ψ∗,ℓas ψ∗,ℓ−1 | ψ∗,ℓ∼
82"
SETTING,0.04917061611374408,"N(fℓ(ψ∗,ℓ), Σℓ), where the score function fℓ: Rd →Rd is known. Finally, the L-th latent parameter
83"
SETTING,0.04976303317535545,"ψ∗,L is sampled as ψ∗,L ∼N(0, ΣL+1). We summarize this model in (1) and its graph in Fig. 1.
84"
SETTING,0.050355450236966824,": taken action
in round"
SETTING,0.0509478672985782,Figure 1: Graphical model of (1). 85
SETTING,0.05154028436018957,"ψ∗,L ∼N(0, ΣL+1) ,
(1)
ψ∗,ℓ−1 | ψ∗,ℓ∼N(fℓ(ψ∗,ℓ), Σℓ) ,
∀ℓ∈[L]/{1} ,
θ∗,i | ψ∗,1 ∼N(f1(ψ∗,1), Σ1) ,
∀i ∈[K] ,
Yt | Xt, θ∗,At ∼P(· | Xt; θ∗,At) ,
∀t ∈[n] ."
SETTING,0.052132701421800945,"The model in (1) represents a Bayesian bandit, where the agent interacts with a bandit instance
86"
SETTING,0.052725118483412325,"defined by θ∗,i over n rounds (4-th line in (1)). These action parameters θ∗,i are drawn from the
87"
SETTING,0.0533175355450237,"generative process in the first 3 lines of (1). In practice, (1) can be built by pre-training a diffusion
88"
SETTING,0.05390995260663507,"model on offline estimates of the action parameters θ∗,i [Hsieh et al., 2023].
89"
SETTING,0.054502369668246446,"A natural goal for the agent in this Bayesian framework is to minimize its Bayes regret [Russo and Van
90"
SETTING,0.05509478672985782,"Roy, 2014] that measures the expected performance across multiple bandit instances θ∗= (θ∗,i)i∈[K],
91"
SETTING,0.05568720379146919,"BR(n) = E
h
n
X"
SETTING,0.05627962085308057,"t=1
r(Xt, At,∗; θ∗) −r(Xt, At; θ∗)
i
,
(2)"
SETTING,0.05687203791469194,"where
the
expectation
in
(2)
is
taken
over
all
random
variables
in
(1).
Here
92"
SETTING,0.057464454976303314,"r(x, i; θ∗) = EY ∼P (·|x;θ∗,i) [Y ] is the expected reward of action i in context x and At,∗=
93"
SETTING,0.058056872037914695,"arg maxi∈[K] r(Xt, i; θ∗) is the optimal action in round t. The Bayes regret is known to capture the
94"
SETTING,0.05864928909952607,"benefits of using informative priors, and hence it is suitable for our problem.
95"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.05924170616113744,"3
Diffusion contextual Thompson sampling
96"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.059834123222748815,"We design Thompson sampling that samples the latent and action parameters hierarchically [Lindley
97"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06042654028436019,"and Smith, 1972]. Precisely, let Ht = (Xk, Ak, Yk)k∈[t−1] be the history of all interactions up to
98"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06101895734597156,"round t and let Ht,i = (Xk, Ak, Yk){k∈[t−1];Ak=i} be the history of interactions with action i up to
99"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.061611374407582936,"round t. To motivate our algorithm, we decompose the posterior P (θ∗,i = θ | Ht) recursively as
100"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06220379146919431,"P (θ∗,i = θ | Ht) =
Z"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06279620853080568,"ψ1:L
Qt,L(ψL) L
Y"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06338862559241706,"ℓ=2
Qt,ℓ−1(ψℓ−1 | ψℓ)Pt,i(θ | ψ1) dψ1:L ,
where
(3)"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06398104265402843,"Qt,L(ψL) = P (ψ∗,L = ψL | Ht) is the latent-posterior density of ψ∗,L | Ht. Moreover, for any
101"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06457345971563981,"ℓ∈[2 : L], Qt,ℓ−1(ψℓ−1 | ψℓ) = P (ψ∗,ℓ−1 = ψℓ−1 | Ht, ψ∗,ℓ= ψℓ) is the conditional latent-
102"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06516587677725119,"posterior density of ψ∗,ℓ−1 | Ht, ψ∗,ℓ= ψℓ. Finally, for any action i ∈[K], Pt,i(θ | ψ1) =
103"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06575829383886256,"P (θ∗,i = θ | Ht,i, ψ∗,1 = ψ1) is the conditional action-posterior density of θ∗,i | Ht,i, ψ∗,1 = ψ1.
104"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06635071090047394,"The decomposition in (3) inspires hierarchical sampling. In round t, we initially sample the L-th
105"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.0669431279620853,"latent parameter as ψt,L ∼Qt,L(·). Then, for ℓ∈[L]/{1}, we sample the ℓ−1-th latent parameter
106"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06753554502369669,"given that ψ∗,ℓ= ψt,ℓ, as ψt,ℓ−1 ∼Qt,ℓ−1(· | ψt,ℓ). Lastly, given that ψ∗,1 = ψt,1, each action
107"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06812796208530805,"parameter is sampled individually as θt,i ∼Pt,i(θ | ψt,1). This is possible because action parameters
108"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06872037914691943,"θ∗,i are conditionally independent given ψ∗,1. This leads to Algorithm 1, named diffusion Thompson
109"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.0693127962085308,"Sampling (dTS). dTS requires sampling from the K + L posteriors Pt,i and Qt,ℓ. Thus we start by
110"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.06990521327014218,"providing an efficient recursive scheme to express these posteriors using known quantities. We note
111"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07049763033175356,"that these expressions do not necessarily lead to closed-form posteriors and approximation might be
112"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07109004739336493,"needed. First, the conditional action-posterior Pt,i(· | ψ1) can be written as
113"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07168246445497631,"Pt,i(θ | ψ1) ∝
Y"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07227488151658767,"k∈St,i
P(Yk | Xk; θ)N(θ; f1(ψ1), Σ1) ,
(4)"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07286729857819906,"where St,i = {ℓ∈[t −1], Aℓ= i} are the rounds where the agent takes action i up to round t.
114"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07345971563981042,"Moreover, let Lℓ(ψℓ) = P (Ht | ψ∗,ℓ= ψℓ) be the likelihood of observations up to round t given that
115"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.0740521327014218,"ψ∗,ℓ= ψℓ. Then, for any ℓ∈[L]/{1}, the ℓ−1-th conditional latent-posterior Qt,ℓ−1(· | ψℓ) is
116"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07464454976303317,"Qt,ℓ−1(ψℓ−1 | ψℓ) ∝Lℓ−1(ψℓ−1)N(ψℓ−1, fℓ(ψℓ), Σℓ) ,
(5)
and Qt,L(ψL) ∝LL(ψL)N(ψL, 0, ΣL+1). All the terms above are known, except the likelihoods
117"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07523696682464455,"Lℓ(ψℓ) for ℓ∈[L]. These are computed recursively as follows. First, the basis of the recursion is
118"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07582938388625593,"L1(ψ1) = K
Y i=1 Z θi Y"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.0764218009478673,"k∈St,i
P(Yk | Xk; θi)N(θi; f1(ψ1), Σ1) dθi.
(6)"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07701421800947868,"Then for ℓ∈[L]/{1}, the recursive step is Lℓ(ψℓ) =
R"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07760663507109004,"ψℓ−1 Lℓ−1(ψℓ−1)N(ψℓ−1; fℓ(ψℓ), Σℓ) dψℓ−1.
119"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07819905213270142,"All posterior expressions above use known quantities (fℓ, Σℓ, P(y | x; θ)). However, these expres-
120"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07879146919431279,"sions typically need to be approximated, except when the score functions fℓare linear and the reward
121"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07938388625592417,"distribution P(· | x; θ) is linear-Gaussian, where closed-form solutions can be obtained with careful
122"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.07997630331753554,"derivations. These approximations are not trivial, and prior studies often rely on computationally
123"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.08056872037914692,"intensive approximate sampling algorithms. In the following sections, we explain how we derive our
124"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.0811611374407583,"efficient approximations which are motivated by the closed-form solutions of linear instances.
125"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.08175355450236967,"Algorithm 1 dTS: diffusion Thompson Sampling
Input: Prior: fℓ, ℓ∈[L], Σℓ, ℓ∈[L + 1], and P.
for t = 1, . . . , n do"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.08234597156398105,"Sample ψt,L ∼Qt,L (requires fast approximate posterior update and sampling)
for ℓ= L, . . . , 2 do"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.08293838862559241,"Sample ψt,ℓ−1 ∼Qt,ℓ−1(· | ψt,ℓ) (requires fast approximate posterior update and sampling)
for i = 1, . . . , K do"
DIFFUSION CONTEXTUAL THOMPSON SAMPLING,0.0835308056872038,"Sample θt,i ∼Pt,i(· | ψt,1) (requires fast approximate posterior update and sampling)
Take action At = argmaxi∈[K]r(Xt, i; θt), where θt = (θt,i)i∈[K]
Receive reward Yt ∼P(· | Xt; θ∗,At) and update posteriors Qt+1,ℓand Pt+1,i."
LINEAR DIFFUSION MODEL,0.08412322274881516,"3.1
Linear diffusion model
126"
LINEAR DIFFUSION MODEL,0.08471563981042654,"Assume the score functions fℓare linear such as fℓ(ψ∗,ℓ) = Wℓψ∗,ℓfor ℓ∈[L], where Wℓ∈Rd×d
127"
LINEAR DIFFUSION MODEL,0.08530805687203792,"are known mixing matrices. Then, (1) becomes a linear Gaussian system (LGS) [Bishop, 2006] in
128"
LINEAR DIFFUSION MODEL,0.08590047393364929,"this case. This model is important, both in theory and practice. For theory, it leads to closed-form
129"
LINEAR DIFFUSION MODEL,0.08649289099526067,"posteriors when the reward distribution is linear-Gaussian as P(· | x; θ∗,i) = N(·; x⊤θ∗,i, σ2). This
130"
LINEAR DIFFUSION MODEL,0.08708530805687204,"allows bounding the Bayes regret of dTS. For practice, the posterior expressions are used to motivate
131"
LINEAR DIFFUSION MODEL,0.08767772511848342,"efficient approximations for the general case in (1) as we show in Section 3.2.
132"
LINEAR DIFFUSION MODEL,0.08827014218009478,"The reward distribution is parameterized as a generalized linear model (GLM) [McCullagh and
133"
LINEAR DIFFUSION MODEL,0.08886255924170616,"Nelder, 1989], allowing for non-linear rewards. Thus, we need posterior approximation despite
134"
LINEAR DIFFUSION MODEL,0.08945497630331753,"linearity in score functions. Since this non-linearity arises solely from the reward distribution, we
135"
LINEAR DIFFUSION MODEL,0.09004739336492891,"approximate it by a Gaussian and propagate this approximation to the latent parameters. This results
136"
LINEAR DIFFUSION MODEL,0.09063981042654029,"in efficient posterior approximations that are exact when the reward function is Gaussian (a special
137"
LINEAR DIFFUSION MODEL,0.09123222748815166,"case of the GLM model). Specifically, the reward distribution P(· | x; θ) is an exponential family
138"
LINEAR DIFFUSION MODEL,0.09182464454976304,"distribution with a mean function denoted by g. Then, we approximate the corresponding likelihood
139"
LINEAR DIFFUSION MODEL,0.0924170616113744,"as P (Ht,i | θ∗,i = θ) ≈N
 
θ; ˆBt,i, ˆG−1
t,i

, where ˆBt,i and ˆGt,i are the maximum likelihood estimate
140"
LINEAR DIFFUSION MODEL,0.09300947867298578,"(MLE) and the Hessian of the negative log-likelihood, respectively, and they are defined as
141"
LINEAR DIFFUSION MODEL,0.09360189573459715,"ˆBt,i = arg maxθ∈Rd log P (Ht,i | θ∗,i = θ) ,
ˆGt,i = P"
LINEAR DIFFUSION MODEL,0.09419431279620853,"k∈St,i ˙g
 
X⊤
k ˆBt,i

XkX⊤
k .
(7)"
LINEAR DIFFUSION MODEL,0.0947867298578199,"where St,i = {ℓ∈[t −1] : Aℓ= i} represents the rounds where the agent takes action i up to
142"
LINEAR DIFFUSION MODEL,0.09537914691943128,"round t. This simple approximation makes all posteriors Gaussian. Specifically, the conditional
143"
LINEAR DIFFUSION MODEL,0.09597156398104266,"action-posterior is Gaussian and is given by Pt,i(· | ψ1) = N(·; ˆµt,i, ˆΣt,i), where ˆµt,i and ˆΣt,i are
144"
LINEAR DIFFUSION MODEL,0.09656398104265403,"computed using ˆBt,i and ˆGt,i in (7). Moreover, for ℓ∈[L−1], the ℓ-th conditional latent-posterior is
145"
LINEAR DIFFUSION MODEL,0.0971563981042654,"also Gaussian, Qt,ℓ(· | ψℓ+1) = N(·; ¯µt,ℓ, ¯Σt,ℓ), where ¯µt,ℓand ¯Σt,ℓare computed recursively. The
146"
LINEAR DIFFUSION MODEL,0.09774881516587677,"recursion starts with ¯µt,1 and ¯Σt,1, which are calculated using ˆBt,i and ˆGt,i in (7). Full expressions are
147"
LINEAR DIFFUSION MODEL,0.09834123222748815,"provided in Appendix B.1. The only approximation made is P (Ht,i | θ∗,i = θ) ≈N
 
θ; ˆBt,i, ˆG−1
t,i

,
148"
LINEAR DIFFUSION MODEL,0.09893364928909952,"and we propagated it to latent posteriors. Thus, these posterior approximations become exact when
149"
LINEAR DIFFUSION MODEL,0.0995260663507109,"the reward distribution follows a linear-Gaussian model, P(· | x; θ∗,a) = N(·; x⊤θ∗,a, σ2).
150"
NON-LINEAR DIFFUSION MODEL,0.10011848341232228,"3.2
Non-linear diffusion model
151"
NON-LINEAR DIFFUSION MODEL,0.10071090047393365,"After deriving the posteriors for linear score functions, we return to the general model in (1).
152"
NON-LINEAR DIFFUSION MODEL,0.10130331753554503,"Approximation is needed since both the score functions and rewards can be non-linear. To avoid
153"
NON-LINEAR DIFFUSION MODEL,0.1018957345971564,"computational challenges, we use a simple and intuitive approximation, where all posteriors Pt,i
154"
NON-LINEAR DIFFUSION MODEL,0.10248815165876778,"and Qt,ℓare approximated by Gaussians that are computed recursively. First, the conditional action-
155"
NON-LINEAR DIFFUSION MODEL,0.10308056872037914,"posterior is approximated by a Gaussian distribution as Pt,i(· | ψ1) = N(·; ˆµt,i, ˆΣt,i), where
156"
NON-LINEAR DIFFUSION MODEL,0.10367298578199052,"ˆΣ−1
t,i = Σ−1
1
+ ˆGt,i
ˆµt,i = ˆΣt,i
 
Σ−1
1 f1(ψ1) + ˆGt,i ˆBt,i

.
(8)"
NON-LINEAR DIFFUSION MODEL,0.10426540284360189,"In the absence of samples, Gt,i = 0d×d. Thus, the approximate action posterior in (8) matches
157"
NON-LINEAR DIFFUSION MODEL,0.10485781990521327,"precisely the term N(f1(ψ1), Σ1) in the diffusion prior (1). Moreover, as more data is accumulated,
158"
NON-LINEAR DIFFUSION MODEL,0.10545023696682465,"Gt,i increases, and the influence of the prior diminishes as ˆGt,i ˆBt,i will dominate the prior term
159"
NON-LINEAR DIFFUSION MODEL,0.10604265402843602,"Σ−1
1 f1(ψ1). Similarly, for ℓ∈[L]/{1}, the ℓ−1-th conditional latent-posterior is approximated by
160"
NON-LINEAR DIFFUSION MODEL,0.1066350710900474,"a Gaussian distribution as Qt,ℓ−1(· | ψℓ) = N(¯µt,ℓ−1, ¯Σt,ℓ−1), where
161"
NON-LINEAR DIFFUSION MODEL,0.10722748815165876,"¯Σ−1
t,ℓ−1 = Σ−1
ℓ
+ ¯Gt,ℓ−1 ,
¯µt,ℓ−1 = ¯Σt,ℓ−1
 
Σ−1
ℓfℓ(ψℓ) + ¯Bt,ℓ−1

,
(9)"
NON-LINEAR DIFFUSION MODEL,0.10781990521327015,"and the L-th latent-posterior is Qt,L(·) = N(¯µt,L, ¯Σt,L),
162"
NON-LINEAR DIFFUSION MODEL,0.10841232227488151,"¯Σ−1
t,L = Σ−1
L+1 + ¯Gt,L ,
¯µt,L = ¯Σt,L ¯Bt,L .
(10)"
NON-LINEAR DIFFUSION MODEL,0.10900473933649289,"Here, ¯Gt,ℓand ¯Bt,ℓfor ℓ∈[L] are computed recursively. The basis of the recursion are
163"
NON-LINEAR DIFFUSION MODEL,0.10959715639810426,"¯Gt,1 = PK
i=1
 
Σ−1
1
−Σ−1
1 ˆΣt,iΣ−1
1

,
¯Bt,1 = Σ−1
1
PK
i=1 ˆΣt,i ˆGt,i ˆBt,i .
(11)"
NON-LINEAR DIFFUSION MODEL,0.11018957345971564,"Then, the recursive step for ℓ∈[L]/{1} is,
164"
NON-LINEAR DIFFUSION MODEL,0.11078199052132702,"¯Gt,ℓ= Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ
,
¯Bt,ℓ= Σ−1
ℓ
¯Σt,ℓ−1 ¯Bt,ℓ−1 .
(12)"
NON-LINEAR DIFFUSION MODEL,0.11137440758293839,"Similarly, in the absence of samples, Qt,ℓ−1 in (9) precisely matches the term N(fℓ(ψ1), Σℓ) in the
165"
NON-LINEAR DIFFUSION MODEL,0.11196682464454977,"diffusion prior (1). As more data is accumulated, the influence of this prior diminishes. Therefore,
166"
NON-LINEAR DIFFUSION MODEL,0.11255924170616113,"this approximation retains a key attribute of exact posteriors: they match the prior when there is no
167"
NON-LINEAR DIFFUSION MODEL,0.11315165876777251,"data, and the prior’s effect diminishes as data accumulates.
168"
ANALYSIS,0.11374407582938388,"4
Analysis
169"
ANALYSIS,0.11433649289099526,"We analyze dTS under the linear diffusion model in Section 3.1 with linear rewards P(· | x; θ∗,a) =
170"
ANALYSIS,0.11492890995260663,"N(·; x⊤θ∗,a, σ2). This assumption leads to a structure with L layers of linear Gaussian relationships,
171"
ANALYSIS,0.11552132701421801,"allowing for theory inspired by linear bandits [Agrawal and Goyal, 2013a, Abbasi-Yadkori et al.,
172"
ANALYSIS,0.11611374407582939,"2011]. However, proofs are not the same, and technical challenges remain (explained in Appendix D).
173"
ANALYSIS,0.11670616113744076,"Although our result holds for milder assumptions, we make some simplifications for clarity and
174"
ANALYSIS,0.11729857819905214,"interpretability. We assume that (A1) Contexts satisfy ∥Xt∥2
2 = 1 for any t ∈[n]. (A2) Mixing
175"
ANALYSIS,0.1178909952606635,"matrices and covariances satisfy λ1(W⊤
ℓWℓ) = 1 for any ℓ∈[L] and Σℓ= σ2
ℓId for any ℓ∈[L + 1].
176"
ANALYSIS,0.11848341232227488,"Note that (A1) can be relaxed to any contexts Xt with bounded norms ∥Xt∥2. Also, (A2) can be
177"
ANALYSIS,0.11907582938388625,"relaxed to positive definite covariances Σℓand arbitrary mixing matrices Wℓ. In this section, we
178"
ANALYSIS,0.11966824644549763,"write ˜O for the big-O notation up to polylogarithmic factors. We start by stating our bound for dTS.
179"
ANALYSIS,0.12026066350710901,"Theorem 4.1. Let σ2
MAX = maxℓ∈[L+1] 1 + σ2
ℓ
σ2 . For any δ ∈(0, 1), the Bayes regret of dTS under
180"
ANALYSIS,0.12085308056872038,"Section 3.1 with linear rewards, (A1) and (A2) is bounded as
181"
ANALYSIS,0.12144549763033176,"BR(n) ≤
r"
N,0.12203791469194313,"2n
 
RACT(n) + PL
ℓ=1 RLAT
ℓ

log(1/δ)

+ cnδ , with c > 0 is constant and,
(13)"
N,0.1226303317535545,"RACT(n) = c0dK log
 
1 + nσ2
1
d

, c0 =
σ2
1
log(1+σ2
1) ,
RLAT
ℓ
= cℓd log
 
1 +
σ2
ℓ+1
σ2
ℓ

, cℓ=
σ2
ℓ+1σ2ℓ
MAX
log(1+σ2
ℓ+1),"
N,0.12322274881516587,"(13) holds for any δ ∈(0, 1). In particular, the term cnδ is constant when δ = 1/n. Then, the
182"
N,0.12381516587677725,"bound is ˜O(√n), and this dependence on the horizon n aligns with prior Bayes regret bounds. The
183"
N,0.12440758293838862,"bound comprises L + 1 main terms, RACT(n) and RLAT
ℓ
for ℓ∈[L]. First, RACT(n) relates to action
184"
N,0.125,"parameters learning, conforming to a standard form [Lu and Van Roy, 2019]. Similarly, RLAT
ℓ
is
185"
N,0.12559241706161137,"associated with learning the ℓ-th latent parameter. Roughly speaking, our bound captures that our
186"
N,0.12618483412322276,"problem can be seen as L + 1 sequential linear bandit instances stacked upon each other.
187"
N,0.12677725118483413,"Technical contributions. dTS uses hierarchical sampling. Thus the marginal posterior distribution of
188"
N,0.1273696682464455,"θ∗,i | Ht is not explicitly defined. The first contribution is deriving θ∗,i | Ht using the total covariance
189"
N,0.12796208530805686,"decomposition combined with an induction proof, as our posteriors in Section 3.1 were derived
190"
N,0.12855450236966826,"recursively. Unlike standard analyses where the posterior distribution of θ∗,i | Ht is predetermined
191"
N,0.12914691943127962,"due to the absence of latent parameters, our method necessitates this recursive total covariance
192"
N,0.129739336492891,"decomposition. Moreover, in standard proofs, we need to quantify the increase in posterior precision
193"
N,0.13033175355450238,"for the action taken At in each round t ∈[n]. However, in dTS, our analysis extends beyond this.
194"
N,0.13092417061611375,"We not only quantify the posterior information gain for the taken action but also for every latent
195"
N,0.13151658767772512,"parameter, since they are also learned. To elaborate, we use the recursive formulas in Section 3.1 that
196"
N,0.13210900473933648,"connect the posterior covariance of each latent parameter ψ∗,ℓwith the covariance of the posterior
197"
N,0.13270142180094788,"action parameters θ∗,i. This allows us to propagate the information gain associated with the action
198"
N,0.13329383886255924,"taken in round At to all latent parameters ψ∗,ℓ, for ℓ∈[L] by induction. Finally, we carefully bound
199"
N,0.1338862559241706,"the resulting terms so that the constants reflect the parameters of the linear diffusion model. More
200"
N,0.13447867298578198,"technical details are provided in Appendix D.
201"
N,0.13507109004739337,"To include more structure, we propose the sparsity assumption (A3) Wℓ= ( ¯Wℓ, 0d,d−dℓ), where
202"
N,0.13566350710900474,"¯Wℓ∈Rd×dℓfor any ℓ∈[L]. Note that (A3) is not an assumption when dℓ= d for any ℓ∈[L].
203"
N,0.1362559241706161,"Notably, (A3) incorporates a plausible structural characteristic that a diffusion model could capture.
204"
N,0.1368483412322275,"Proposition 4.2 (Sparsity). Let σ2
MAX = maxℓ∈[L+1] 1 + σ2
ℓ
σ2 . For any δ ∈(0, 1), the Bayes regret of
205"
N,0.13744075829383887,"dTS under Section 3.1 with linear rewards, (A1), (A2) and (A3) is bounded as
206"
N,0.13803317535545023,"BR(n) ≤
r"
N,0.1386255924170616,"2n
 
RACT(n) + PL
ℓ=1 ˜RLAT
ℓ

log(1/δ)

+ cnδ , with c > 0 is constant,
(14)"
N,0.139218009478673,"RACT(n) = c0dK log
 
1 + nσ2
1
d

, c0 =
σ2
1
log(1+σ2
1) ,
˜RLAT
ℓ
= cℓdℓlog
 
1 +
σ2
ℓ+1
σ2
ℓ

, cℓ=
σ2
ℓ+1σ2ℓ
MAX
log(1+σ2
ℓ+1)."
N,0.13981042654028436,"From Proposition 4.2, our bounds scales as BR(n) = ˜O
q"
N,0.14040284360189573,"n(dKσ2
1 + PL
ℓ=1 dℓσ2
ℓ+1σ2ℓ
MAX)

. The
207"
N,0.14099526066350712,"Bayes regret bound has a clear interpretation: if the true environment parameters are drawn from
208"
N,0.1415876777251185,"the prior, then the expected regret of an algorithm stays below that bound. Consequently, a less
209"
N,0.14218009478672985,"informative prior (such as high variance) leads to a more challenging problem and thus a higher
210"
N,0.14277251184834122,"bound. Then, smaller values of K, L, d or dℓtranslate to fewer parameters to learn, leading to lower
211"
N,0.14336492890995262,"regret. The regret also decreases when the initial variances σ2
ℓdecrease. These dependencies are
212"
N,0.14395734597156398,"common in Bayesian analysis, and empirical results match them. The reader might question the
213"
N,0.14454976303317535,"dependence of our bound on both L and K. We will address this next.
214"
N,0.14514218009478674,"Why the bound increases with K? This arises due to our conditional learning of θ∗,i given
215"
N,0.1457345971563981,"ψ∗,1. Rather than assuming deterministic linearity, θ∗,i = W1ψ∗,1, we account for stochasticity by
216"
N,0.14632701421800948,"modeling θ∗,i ∼N(W1ψ∗,1, σ2
1Id). This makes dTS robust to misspecification scenarios where θ∗,i
217"
N,0.14691943127962084,"is not perfectly linear with respect to ψ∗,1, at the cost of additional learning of θ∗,i | ψ∗,1. If we were
218"
N,0.14751184834123224,"to assume deterministic linearity (σ1 = 0), our regret bound would scale with L only.
219"
N,0.1481042654028436,"Why the bound increases with L? This is because increasing the number of layers L adds more
220"
N,0.14869668246445497,"initial uncertainty due to the additional covariance introduced by the extra layers. However, this does
221"
N,0.14928909952606634,"not imply that we should always use L = 1 (the minimum possible L). While a higher L complicates
222"
N,0.14988151658767773,"online learning and increases regret bound, it also enables the capture of a more complex prior
223"
N,0.1504739336492891,"distribution through offline pre-training of the diffusion model. Thus, a trade-off exists in practice.
224"
N,0.15106635071090047,"A smaller L results in faster computation and easier learning for dTS, but the learned prior might
225"
N,0.15165876777251186,"deviate from reality, potentially violating the ""true prior assumption"" used to derive the regret bound.
226"
N,0.15225118483412323,"On the other hand, a larger L allows for better modeling of complex action distributions, producing a
227"
N,0.1528436018957346,"prior that more accurately reflects reality and strengthens the validity of the bound.
228"
DISCUSSION,0.15343601895734596,"4.1
Discussion
229"
DISCUSSION,0.15402843601895735,"Computational benefits. Action correlations prompt an intuitive approach: marginalize all latent
230"
DISCUSSION,0.15462085308056872,"parameters and maintain a joint posterior of (θ∗,i)i∈[K] | Ht. Unfortunately, this is computationally
231"
DISCUSSION,0.1552132701421801,"inefficient for large action spaces. To illustrate, suppose that all posteriors are multivariate Gaussians
232"
DISCUSSION,0.15580568720379148,"(Section 3.1). Then maintaining the joint posterior (θ∗,i)i∈[K] | Ht necessitates converting and
233"
DISCUSSION,0.15639810426540285,"storing its dK × dK-dimensional covariance matrix. Then the time and space complexities are
234"
DISCUSSION,0.15699052132701422,"O(K3d3) and O(K2d2). In contrast, the time and space complexities of dTS are O
  
L + K

d3
235"
DISCUSSION,0.15758293838862558,"and O
  
L + K

d2
. This is because dTS requires converting and storing L + K covariance matrices,
236"
DISCUSSION,0.15817535545023698,"each being d × d-dimensional. The improvement is huge when K ≫L, which is common in
237"
DISCUSSION,0.15876777251184834,"practice. Certainly, a more straightforward way to enhance computational efficiency is to discard
238"
DISCUSSION,0.1593601895734597,"latent parameters and maintain K individual posteriors, each relating to an action parameter θ∗,i ∈Rd
239"
DISCUSSION,0.15995260663507108,"(LinTS). This improves time and space complexity to O
 
Kd3
and O
 
Kd2
, respectively. However,
240"
DISCUSSION,0.16054502369668247,"LinTS maintains independent posteriors and fails to capture the correlations among actions; it only
241"
DISCUSSION,0.16113744075829384,"models θ∗,i | Ht,i rather than θ∗,i | Ht as done by dTS. Consequently, LinTS incurs higher regret
242"
DISCUSSION,0.1617298578199052,"due to the information loss caused by unused interactions of similar actions. Our regret bound and
243"
DISCUSSION,0.1623222748815166,"empirical results reflect this aspect.
244"
DISCUSSION,0.16291469194312796,"Statistical benefits. We do not provide a matching lower bound. The only Bayesian lower bound
245"
DISCUSSION,0.16350710900473933,"that we know of is Ω(log2(n)) for a much simpler K-armed bandit [Lai, 1987, Theorem 3]. All
246"
DISCUSSION,0.1640995260663507,"seminal works on Bayesian bandits do not match it and providing such lower bounds on Bayes regret
247"
DISCUSSION,0.1646919431279621,"is still relatively unexplored (even in standard settings) compared to the frequentist one. Therefore,
248"
DISCUSSION,0.16528436018957346,"we argue that our bound reflects the overall structure of the problem by comparing dTS to algorithms
249"
DISCUSSION,0.16587677725118483,"that only partially use the structure or do not use it at all as follows.
250"
DISCUSSION,0.16646919431279622,"The linear diffusion model in Section 3.1 can be transformed into a Bayesian linear model (LinTS)
251"
DISCUSSION,0.1670616113744076,"by marginalizing out the latent parameters; in which case the prior on action parameters becomes
252"
DISCUSSION,0.16765402843601895,"θ∗,i ∼N(0, Σ), with the θ∗,i being not necessarily independent, and Σ is the marginal initial
253"
DISCUSSION,0.16824644549763032,"covariance of action parameters and it writes Σ = σ2
1Id + PL
ℓ=1 σ2
ℓ+1BℓB⊤
ℓwith Bℓ= Qℓ
k=1 Wk.
254"
DISCUSSION,0.16883886255924171,"Then, it is tempting to directly apply LinTS to solve our problem. This approach will induce
255"
DISCUSSION,0.16943127962085308,"higher regret because the additional uncertainty of the latent parameters is accounted for in Σ
256"
DISCUSSION,0.17002369668246445,"despite integrating them. This causes the marginal action uncertainty Σ to be much higher than the
257"
DISCUSSION,0.17061611374407584,"conditional action uncertainty σ2
1Id in (3.1), since we have Σ = σ2
1Id + PL
ℓ=1 σ2
ℓ+1BℓB⊤
ℓ≽σ2
1Id.
258"
DISCUSSION,0.1712085308056872,"This discrepancy leads to higher regret, especially when K is large. This is due to LinTS needing to
259"
DISCUSSION,0.17180094786729858,"learn K independent d-dimensional parameters, each with a considerably higher initial covariance Σ.
260"
DISCUSSION,0.17239336492890994,"This is also reflected by our regret bound. To simply comparisons, suppose that σ ≥maxℓ∈[L+1] σℓ
261"
DISCUSSION,0.17298578199052134,"so that σ2
MAX ≤2. Then the regret bounds of dTS (where we bound σ2ℓ
MAX by 2ℓ) and LinTS read
262"
DISCUSSION,0.1735781990521327,"dTS : ˜O
 q"
DISCUSSION,0.17417061611374407,"n(dKσ2
1 + PL
ℓ=1 dℓσ2
ℓ+12ℓ)

,
LinTS : ˜O
 q"
DISCUSSION,0.17476303317535544,"ndK(σ2
1 + PL
ℓ=1 σ2
ℓ+1)

."
DISCUSSION,0.17535545023696683,"Then regret improvements are captured by the variances σℓand the sparsity dimensions dℓ, and we
263"
DISCUSSION,0.1759478672985782,"proceed to illustrate this through the following scenarios.
264"
DISCUSSION,0.17654028436018956,"(I) Decreasing variances. Assume that σℓ= 2ℓfor any ℓ∈[L + 1]. Then, the regrets become
265"
DISCUSSION,0.17713270142180096,"dTS : ˜O
 q"
DISCUSSION,0.17772511848341233,"n(dK + PL
ℓ=1 dℓ4ℓ))

,
LinTS : ˜O
 p"
DISCUSSION,0.1783175355450237,"ndK2L)
"
DISCUSSION,0.17890995260663506,"Now to see the order of gain, assume the problem is high-dimensional (d ≫1), and set L = log2(d)
266"
DISCUSSION,0.17950236966824645,and dℓ= ⌊d
DISCUSSION,0.18009478672985782,"2ℓ⌋. Then the regret of dTS becomes ˜O
 p"
DISCUSSION,0.1806872037914692,"nd(K + L))

, and hence the multiplicative
267"
DISCUSSION,0.18127962085308058,"factor 2L in LinTS is removed and replaced with a smaller additive factor L.
268"
DISCUSSION,0.18187203791469195,"(II) Constant variances. Assume that σℓ= 1 for any ℓ∈[L + 1]. Then, the regrets become
269"
DISCUSSION,0.18246445497630331,"dTS : ˜O
 q"
DISCUSSION,0.18305687203791468,"n(dK + PL
ℓ=1 dℓ2ℓ))

,
LinTS : ˜O
 p"
DISCUSSION,0.18364928909952608,"ndKL)
"
DISCUSSION,0.18424170616113744,"Similarly, let L = log2(d), and dℓ= ⌊d"
DISCUSSION,0.1848341232227488,"2ℓ⌋. Then dTS’s regret is ˜O
 p"
DISCUSSION,0.1854265402843602,"nd(K + L)

. Thus the
270"
DISCUSSION,0.18601895734597157,"multiplicative factor L in LinTS is removed and replaced with the additive factor L. By comparing
271"
DISCUSSION,0.18661137440758294,"this to (I), the gain with decreasing variances is greater than with constant ones. In general, diffusion
272"
DISCUSSION,0.1872037914691943,"models use decreasing variances [Ho et al., 2020] and hence we expect great gains in practice.
273"
DISCUSSION,0.1877962085308057,"All observed improvements in this section could become even more pronounced when employing
274"
DISCUSSION,0.18838862559241706,"non-linear diffusion models. In our current analysis, we used linear diffusion models, and yet we can
275"
DISCUSSION,0.18898104265402843,"already discern substantial differences. Moreover, under non-linear diffusion (1), the latent parameters
276"
DISCUSSION,0.1895734597156398,"cannot be analytically marginalized, making LinTS with exact marginalization inapplicable. Finally,
277"
DISCUSSION,0.1901658767772512,"Appendix D.7 provide an additional comparison and connection to hierarchies with two levels.
278"
DISCUSSION,0.19075829383886256,"Large action space aspect. dTS’s regret bound scales with Kσ2
1 instead of K P"
DISCUSSION,0.19135071090047392,"ℓσ2
ℓ, particularly
279"
DISCUSSION,0.19194312796208532,"beneficial when σ1 is small, as often seen in diffusion models. Our regret bound and experiments
280"
DISCUSSION,0.19253554502369669,"show that dTS outperforms LinTS more distinctly when the action space becomes larger. Prior
281"
DISCUSSION,0.19312796208530805,"studies [Foster et al., 2020, Xu and Zeevi, 2020, Zhu et al., 2022] proposed bandit algorithms that
282"
DISCUSSION,0.19372037914691942,"do not scale with K. However, our setting differs significantly from theirs, explaining our inherent
283"
DISCUSSION,0.1943127962085308,"dependency on K when σ1 > 0. Precisely, they assume a reward function of r(x, i; θ∗) = ϕ(x, i)⊤θ∗,
284"
DISCUSSION,0.19490521327014218,"with a shared θ∗∈Rd and a known mapping ϕ. In contrast, we consider r(x, i; θ∗) = x⊤θ∗,i, with
285"
DISCUSSION,0.19549763033175355,"θ∗= (θ∗,i)i∈[K] ∈RdK, requiring the learning of K separate d-dimensional action parameters.
286"
DISCUSSION,0.19609004739336494,"In their setting, with the availability of ϕ, the regret of dTS would similarly be independent of
287"
DISCUSSION,0.1966824644549763,"K. However, obtaining such a mapping ϕ can be challenging as it needs to encapsulate complex
288"
DISCUSSION,0.19727488151658767,"context-action dependencies. Notably, our setting reflects a common practical scenario, such as in
289"
DISCUSSION,0.19786729857819904,"recommendation systems where each product is often represented by its unique embedding.
290"
EXPERIMENTS,0.19845971563981044,"5
Experiments
291"
EXPERIMENTS,0.1990521327014218,"We evaluate dTS using synthetic data, to validate our theory and test dTS in large action spaces. We
292"
EXPERIMENTS,0.19964454976303317,"omit semi-synthetic data [Riquelme et al., 2018] as they often result in small action spaces. This
293"
EXPERIMENTS,0.20023696682464456,"0
1000 2000 3000 4000 5000
0
1
2
3
4
5
6
7
8
9"
EXPERIMENTS,0.20082938388625593,Regret
EXPERIMENTS,0.2014218009478673,"1e3
Linear diffusion, linear reward"
EXPERIMENTS,0.20201421800947866,"K=100, L=2, d=5"
EXPERIMENTS,0.20260663507109006,"dTS-LL
HierTS
LinTS
LinUCB"
EXPERIMENTS,0.20319905213270142,"0
1000 2000 3000 4000 5000
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8 1e3"
EXPERIMENTS,0.2037914691943128,"Linear diffusion, nonlinear reward"
EXPERIMENTS,0.20438388625592416,"K=100, L=2, d=5"
EXPERIMENTS,0.20497630331753555,"dTS-LN
dTS-LL
GLM-TS
UCB-GLM"
EXPERIMENTS,0.20556872037914692,"0
1000 2000 3000 4000 5000
0.0 0.2 0.4 0.6 0.8"
EXPERIMENTS,0.20616113744075829,1.0 1e4
EXPERIMENTS,0.20675355450236968,"Nonlinear diffusion, linear reward"
EXPERIMENTS,0.20734597156398105,"K=100, L=2, d=5"
EXPERIMENTS,0.2079383886255924,"dTS-NL
LinTS
LinUCB"
EXPERIMENTS,0.20853080568720378,"0
1000 2000 3000 4000 5000
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6 1e3
Nonlinear diffusion, nonlinear reward"
EXPERIMENTS,0.20912322274881517,"K=100, L=2, d=5"
EXPERIMENTS,0.20971563981042654,"dTS-NN
dTS-NL
GLM-TS
UCB-GLM"
EXPERIMENTS,0.2103080568720379,"0
1000 2000 3000 4000 5000"
EXPERIMENTS,0.2109004739336493,Round t 2 [n]
EXPERIMENTS,0.21149289099526067,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0"
EXPERIMENTS,0.21208530805687204,Regret
EXPERIMENTS,0.2126777251184834,"1e5
    K=10000, L=4, d=20"
EXPERIMENTS,0.2132701421800948,"dTS-LL
HierTS
LinTS
LinUCB"
EXPERIMENTS,0.21386255924170616,"0
1000 2000 3000 4000 5000"
EXPERIMENTS,0.21445497630331753,Round t 2 [n] 0.0 0.5 1.0 1.5 2.0 2.5
EXPERIMENTS,0.21504739336492892,"3.0 1e3
    K=10000, L=4, d=20"
EXPERIMENTS,0.2156398104265403,"dTS-LN
dTS-LL
GLM-TS
UCB-GLM"
EXPERIMENTS,0.21623222748815166,"0
1000 2000 3000 4000 5000"
EXPERIMENTS,0.21682464454976302,Round t 2 [n]
EXPERIMENTS,0.21741706161137442,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0 1e7
    K=10000, L=4, d=20"
EXPERIMENTS,0.21800947867298578,"dTS-NL
LinTS
LinUCB"
EXPERIMENTS,0.21860189573459715,"0
1000 2000 3000 4000 5000"
EXPERIMENTS,0.21919431279620852,Round t 2 [n]
EXPERIMENTS,0.2197867298578199,"0
1
2
3
4
5
6
7
8 1e2
    K=10000, L=4, d=20"
EXPERIMENTS,0.22037914691943128,"dTS-NN
dTS-NL
GLM-TS
UCB-GLM"
EXPERIMENTS,0.22097156398104265,"Figure 2: Regret of dTS with varying diffusion and reward models and varying parameters d, K, L."
EXPERIMENTS,0.22156398104265404,"choice is further justified by the fact that Hsieh et al. [2023] has already demonstrated the advantages
294"
EXPERIMENTS,0.2221563981042654,"of diffusion models in multi-armed bandits using such data, without theoretical guarantees.
295"
SETTINGS AND BASELINES,0.22274881516587677,"5.1
Settings and baselines
296"
SETTINGS AND BASELINES,0.22334123222748814,"We run 50 random simulations and plot the average regret with its standard error. We consider both
297"
SETTINGS AND BASELINES,0.22393364928909953,"linear and non-linear rewards. The distribution of linear rewards is P(· | x; θa) = N(x⊤θa, σ2) with
298"
SETTINGS AND BASELINES,0.2245260663507109,"σ = 1. The non-linear rewards are binary and generated from P(· | x; θa) = Ber(g(x⊤θa))), where
299"
SETTINGS AND BASELINES,0.22511848341232227,"g is the sigmoid function. The covariances are Σℓ= Id, and the context Xt is uniformly drawn from
300"
SETTINGS AND BASELINES,0.22571090047393366,"[−1, 1]d. We vary d ∈{5, 20}, L ∈{2, 4} and K ∈{102, 104}. We set the horizon n = 5000.
301"
SETTINGS AND BASELINES,0.22630331753554503,"Linear diffusion. We consider the linear diffusion model in (3.1) where score functions are linear as
302"
SETTINGS AND BASELINES,0.2268957345971564,"fℓ(ψ) = Wℓψ where Wℓare uniformly drawn from [−1, 1]d×d. To introduce sparsity, we zero out
303"
SETTINGS AND BASELINES,0.22748815165876776,"the last dℓcolumns of Wℓ, resulting in Wℓ= ( ¯Wℓ, 0d,d−dℓ), where (d1, d2) = (5, 2) when d = 5
304"
SETTINGS AND BASELINES,0.22808056872037916,"and L = 2 and (d1, d2, d3, d4) = (20, 10, 5, 2) when d = 20 and L = 4.
305"
SETTINGS AND BASELINES,0.22867298578199052,"Non-linear diffusion. We consider the general diffusion model in (1) with score functions fℓdefined
306"
SETTINGS AND BASELINES,0.2292654028436019,"by two-layer neural networks with random weights in [−1, 1], ReLU activation, and a hidden layer
307"
SETTINGS AND BASELINES,0.22985781990521326,"dimension of h = 20 when d = 5 and h = 60 when d = 20.
308"
SETTINGS AND BASELINES,0.23045023696682465,"Baselines. When rewards are linear, we use LinUCB [Abbasi-Yadkori et al., 2011], LinTS [Agrawal
309"
SETTINGS AND BASELINES,0.23104265402843602,"and Goyal, 2013a], and HierTS [Hong et al., 2022b] that marginalizes out all latent parameters
310"
SETTINGS AND BASELINES,0.23163507109004738,"except ψ∗,L. This corresponds to HierTS-1 in Appendix D.7. When rewards are non-linear, we
311"
SETTINGS AND BASELINES,0.23222748815165878,"include UCB-GLM [Li et al., 2017], and GLM-TS [Chapelle and Li, 2012]. GLM-UCB [Filippi et al.,
312"
SETTINGS AND BASELINES,0.23281990521327015,"2010] induced high regret while HierTS was designed for linear rewards only and thus both are not
313"
SETTINGS AND BASELINES,0.2334123222748815,"included. We name dTS for each setting as dTS-dr, where the suffix d indicates the type of diffusion;
314"
SETTINGS AND BASELINES,0.23400473933649288,"L for linear and N for non-linear. The suffix r indicates the type of rewards; L for linear and N for
315"
SETTINGS AND BASELINES,0.23459715639810427,"non-linear. For instance, dTS-LL signifies dTS in linear diffusion (Section 3.1) with linear rewards.
316"
RESULTS AND INTERPRETATIONS,0.23518957345971564,"5.2
Results and interpretations
317"
RESULTS AND INTERPRETATIONS,0.235781990521327,"Results are shown in Fig. 2 and we make the following observations:
318"
RESULTS AND INTERPRETATIONS,0.2363744075829384,"1) dTS has better performance. dTS outperforms the baselines. First, when both the diffusion and
319"
RESULTS AND INTERPRETATIONS,0.23696682464454977,"rewards are linear, dTS-LL consistently outperforms all baselines that disregard the latent structure
320"
RESULTS AND INTERPRETATIONS,0.23755924170616113,"(LinTS and LinUCB) or incorporate it only partially (HierTS). Second, when the diffusion is linear
321"
RESULTS AND INTERPRETATIONS,0.2381516587677725,"and rewards are non-linear, dTS-LN surpasses all baselines. Third, when the diffusion is non-linear
322"
RESULTS AND INTERPRETATIONS,0.2387440758293839,"and rewards are linear, dTS-NL demonstrates significant performance gains compared to both LinTS
323"
RESULTS AND INTERPRETATIONS,0.23933649289099526,"and LinUCB. With non-linear diffusion and rewards, dTS-NN surpasses both GLM-TS and UCB-GLM.
324"
RESULTS AND INTERPRETATIONS,0.23992890995260663,"2) Latent diffusion structure may be more important than the reward distribution. When
325"
RESULTS AND INTERPRETATIONS,0.24052132701421802,"rewards are non-linear (second and fourth columns in Fig. 2), we included variants of dTS that use
326"
RESULTS AND INTERPRETATIONS,0.2411137440758294,"the correct diffusion prior but the wrong reward distribution, employing linear-Gaussian instead of
327"
RESULTS AND INTERPRETATIONS,0.24170616113744076,"logistic-Bernoulli (dTS-LL in the second column and dTS-NL in the fourth column). In both cases,
328"
RESULTS AND INTERPRETATIONS,0.24229857819905212,"despite the misspecification of the reward distribution, these variants outperform models that use the
329"
RESULTS AND INTERPRETATIONS,0.24289099526066352,"correct reward distribution but neglect the latent diffusion structure, such as GLM-TS and UCB-GLM.
330"
RESULTS AND INTERPRETATIONS,0.24348341232227488,"This underscores the significance of accounting for the latent structure, which can sometimes be more
331"
RESULTS AND INTERPRETATIONS,0.24407582938388625,"crucial than having an accurate reward distribution. Also, the performance gap between dTS-NL
332"
RESULTS AND INTERPRETATIONS,0.24466824644549762,"(non-linear diffusion) and GLM-TS and UCB-GLM is even more pronounced compared to the gap
333"
RESULTS AND INTERPRETATIONS,0.245260663507109,"between dTS-LL (linear diffusion) and these baselines, possibly due to the increased complexity of
334"
RESULTS AND INTERPRETATIONS,0.24585308056872038,"the latent structure, in the non-linear diffusion, overshadowing the impact of the reward model itself.
335"
RESULTS AND INTERPRETATIONS,0.24644549763033174,"0
1000
2000
3000
4000
5000"
RESULTS AND INTERPRETATIONS,0.24703791469194314,Round t 2 [n] 0 500 1000 1500 2000 2500
RESULTS AND INTERPRETATIONS,0.2476303317535545,Regret
RESULTS AND INTERPRETATIONS,0.24822274881516587,Effect of prior misspecification
RESULTS AND INTERPRETATIONS,0.24881516587677724,"LindTS (v=0.5)
LindTS (v=1)
LindTS (v=1.5)
LindTS
HierTS"
RESULTS AND INTERPRETATIONS,0.24940758293838863,Figure 3: Prior misspecification effect.
RESULTS AND INTERPRETATIONS,0.25,"3) Prior misspecification (Fig. 3). We consider a scenario
336"
RESULTS AND INTERPRETATIONS,0.2505924170616114,"where the prior used by dTS does not match the true prior.
337"
RESULTS AND INTERPRETATIONS,0.25118483412322273,"To simulate this, we use our setting with linear diffusion
338"
RESULTS AND INTERPRETATIONS,0.2517772511848341,"and rewards above, but the true parameters Wℓand Σℓare
339"
RESULTS AND INTERPRETATIONS,0.2523696682464455,"replaced by misspecified parameters Wℓ+ ϵ1 and Σℓ+ ϵ2.
340"
RESULTS AND INTERPRETATIONS,0.25296208530805686,"Here, ϵ1 and ϵ2 are sampled uniformly from [v, v+0.5]d×d,
341"
RESULTS AND INTERPRETATIONS,0.25355450236966826,"with v controlling the level of misspecification. The higher
342"
RESULTS AND INTERPRETATIONS,0.2541469194312796,"the value of v, the greater the misspecification. We vary
343"
RESULTS AND INTERPRETATIONS,0.254739336492891,"v ∈{0.5, 1, 1.5} and analyze its impact on dTS’s perfor-
344"
RESULTS AND INTERPRETATIONS,0.2553317535545024,"mance. For comparison, we include the well-specified
345"
RESULTS AND INTERPRETATIONS,0.2559241706161137,"dTS-LL and the most competitive baseline, HierTS. Re-
346"
RESULTS AND INTERPRETATIONS,0.2565165876777251,"sults are shown in Fig. 3. As expected, dTS’s performance
347"
RESULTS AND INTERPRETATIONS,0.2571090047393365,"decreases with increasing misspecification. However, even
348"
RESULTS AND INTERPRETATIONS,0.25770142180094785,"with misspecification, dTS outperforms the most competitive baseline, except when v = 1.5, where
349"
RESULTS AND INTERPRETATIONS,0.25829383886255924,"their performances are comparable. Note that the entries of the true parameters Wℓand Σℓare smaller
350"
RESULTS AND INTERPRETATIONS,0.25888625592417064,"than 1, so values of v ∈{0.5, 1, 1.5} can lead to significant parameter misspecification. Yet, the
351"
RESULTS AND INTERPRETATIONS,0.259478672985782,"performance of dTS with misspecified prior parameters remains favorable, suggesting that even an
352"
RESULTS AND INTERPRETATIONS,0.26007109004739337,"imperfect pre-trained diffusion model can be beneficial when used as prior.
353"
RESULTS AND INTERPRETATIONS,0.26066350710900477,"0.0
0.5
1.0
1.5
2.0
2.5
3.0"
RESULTS AND INTERPRETATIONS,0.2612559241706161,Increasing values of either K; d or L 0 500 1000 1500 2000 2500 3000 3500
RESULTS AND INTERPRETATIONS,0.2618483412322275,Regret
RESULTS AND INTERPRETATIONS,0.26244075829383884,Effect of K; d; L on the regret of LindTS
RESULTS AND INTERPRETATIONS,0.26303317535545023,Increasing K
RESULTS AND INTERPRETATIONS,0.2636255924170616,Increasing d
RESULTS AND INTERPRETATIONS,0.26421800947867297,Increasing L
RESULTS AND INTERPRETATIONS,0.26481042654028436,Figure 4: dTS-LL’s regret scaling.
RESULTS AND INTERPRETATIONS,0.26540284360189575,"4) Regret scaling with K, d and L matches our theory
354"
RESULTS AND INTERPRETATIONS,0.2659952606635071,"(Fig. 4). We verify the impact of the number of actions
355"
RESULTS AND INTERPRETATIONS,0.2665876777251185,"K, the context dimension d, and the diffusion depth L
356"
RESULTS AND INTERPRETATIONS,0.2671800947867299,"on the regret of dTS. We maintain the same experimental
357"
RESULTS AND INTERPRETATIONS,0.2677725118483412,"setup with linear diffusion and rewards, for which we have
358"
RESULTS AND INTERPRETATIONS,0.2683649289099526,"derived a Bayes regret upper bound. In Fig. 4, we plot
359"
RESULTS AND INTERPRETATIONS,0.26895734597156395,"the regret of dTS-LL across varying values of these pa-
360"
RESULTS AND INTERPRETATIONS,0.26954976303317535,"rameters: K ∈{10, 100, 500, 1000}, d ∈{5, 10, 15, 20},
361"
RESULTS AND INTERPRETATIONS,0.27014218009478674,"and L ∈{2, 4, 5, 6}. As anticipated and aligned with our
362"
RESULTS AND INTERPRETATIONS,0.2707345971563981,"theory, the empirical regret increases as the values of K, d,
363"
RESULTS AND INTERPRETATIONS,0.2713270142180095,"or L grow. This trend arises because larger values of K, d,
364"
RESULTS AND INTERPRETATIONS,0.27191943127962087,"or L result in problem instances that are more challenging
365"
RESULTS AND INTERPRETATIONS,0.2725118483412322,"to learn, consequently leading to higher regret.
366"
RESULTS AND INTERPRETATIONS,0.2731042654028436,"10
100
500
5000
50000"
RESULTS AND INTERPRETATIONS,0.273696682464455,Number of actions K
RESULTS AND INTERPRETATIONS,0.27428909952606634,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8"
RESULTS AND INTERPRETATIONS,0.27488151658767773,Regret in round n
RESULTS AND INTERPRETATIONS,0.2754739336492891,"1e4
Regret as a function of K"
RESULTS AND INTERPRETATIONS,0.27606635071090047,"dTS-LL
LinTS"
RESULTS AND INTERPRETATIONS,0.27665876777251186,"Figure 5: Regret of dTS-LL and LinTS
with varying K."
RESULTS AND INTERPRETATIONS,0.2772511848341232,"5) Performance gap between dTS and LinTS widens
367"
RESULTS AND INTERPRETATIONS,0.2778436018957346,"as K increases (Fig. 5). To showcase dTS’s improved
368"
RESULTS AND INTERPRETATIONS,0.278436018957346,"scalability to larger action spaces, we examine its perfor-
369"
RESULTS AND INTERPRETATIONS,0.2790284360189573,"mance across a range of K values, from 10 to 50, 000,
370"
RESULTS AND INTERPRETATIONS,0.2796208530805687,"in our setting with linear diffusion and rewards. Fig. 5
371"
RESULTS AND INTERPRETATIONS,0.2802132701421801,"reports the final cumulative regret for varying values of K
372"
RESULTS AND INTERPRETATIONS,0.28080568720379145,"for both dTS-LL and LinTS, observing that the gap in the
373"
RESULTS AND INTERPRETATIONS,0.28139810426540285,"performance becomes larger as K increases.
374"
CONCLUSION,0.28199052132701424,"6
Conclusion
375"
CONCLUSION,0.2825829383886256,"Grappling with large action spaces in contextual bandits is challenging. Recognizing this, we focused
376"
CONCLUSION,0.283175355450237,"on structured problems where action parameters are sampled from a diffusion model; upon which we
377"
CONCLUSION,0.2837677725118483,"built diffusion Thompson sampling (dTS). We developed both theoretical and algorithmic foundations
378"
CONCLUSION,0.2843601895734597,"for dTS in numerous practical settings. We identified several directions for future work. Exploring
379"
CONCLUSION,0.2849526066350711,"other approximations for non-linear diffusion models, both empirically and theoretically. From a
380"
CONCLUSION,0.28554502369668244,"theoretical perspective, future research could explore the advantages of non-linear diffusion models
381"
CONCLUSION,0.28613744075829384,"by deriving their Bayes regret bounds, akin to our analysis in Section 4. Empirically, investigating
382"
CONCLUSION,0.28672985781990523,"our and other approximations in complex tasks would be interesting. Additionally, exploring the
383"
CONCLUSION,0.28732227488151657,"extension of this work to offline (or off-policy) learning in contextual bandits [Swaminathan and
384"
CONCLUSION,0.28791469194312796,"Joachims, 2015, Aouali et al., 2023a] represents a promising avenue for future research.
385"
REFERENCES,0.28850710900473936,"References
386"
REFERENCES,0.2890995260663507,"Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
387"
REFERENCES,0.2896919431279621,"bandits. In Advances in Neural Information Processing Systems 24, pages 2312–2320, 2011.
388"
REFERENCES,0.2902843601895735,"Marc Abeille and Alessandro Lazaric. Linear Thompson sampling revisited. In Proceedings of the
389"
REFERENCES,0.2908767772511848,"20th International Conference on Artificial Intelligence and Statistics, 2017.
390"
REFERENCES,0.2914691943127962,"Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In
391"
REFERENCES,0.29206161137440756,"Proceedings of the 30th International Conference on Machine Learning, pages 127–135, 2013a.
392"
REFERENCES,0.29265402843601895,"Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In
393"
REFERENCES,0.29324644549763035,"Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics,
394"
REFERENCES,0.2938388625592417,"pages 99–107, 2013b.
395"
REFERENCES,0.2944312796208531,"Shipra Agrawal and Navin Goyal. Near-optimal regret bounds for thompson sampling. Journal of
396"
REFERENCES,0.2950236966824645,"the ACM (JACM), 64(5):1–24, 2017.
397"
REFERENCES,0.2956161137440758,"Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is con-
398"
REFERENCES,0.2962085308056872,"ditional generative modeling all you need for decision-making? arXiv preprint arXiv:2211.15657,
399"
REFERENCES,0.2968009478672986,"2022.
400"
REFERENCES,0.29739336492890994,"Imad Aouali, Victor-Emmanuel Brunel, David Rohde, and Anna Korba. Exponential smoothing for
401"
REFERENCES,0.29798578199052134,"off-policy learning. In International Conference on Machine Learning, pages 984–1017. PMLR,
402"
REFERENCES,0.2985781990521327,"2023a.
403"
REFERENCES,0.29917061611374407,"Imad Aouali, Branislav Kveton, and Sumeet Katariya. Mixed-effect thompson sampling. In Interna-
404"
REFERENCES,0.29976303317535546,"tional Conference on Artificial Intelligence and Statistics, pages 2087–2115. PMLR, 2023b.
405"
REFERENCES,0.3003554502369668,"Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
406"
REFERENCES,0.3009478672985782,"problem. Machine Learning, 47:235–256, 2002.
407"
REFERENCES,0.3015402843601896,"Mohammad Gheshlaghi Azar, Alessandro Lazaric, and Emma Brunskill. Sequential transfer in
408"
REFERENCES,0.30213270142180093,"multi-armed bandit with finite set of models. In Advances in Neural Information Processing
409"
REFERENCES,0.3027251184834123,"Systems 26, pages 2220–2228, 2013.
410"
REFERENCES,0.3033175355450237,"Hamsa Bastani, David Simchi-Levi, and Ruihao Zhu. Meta dynamic pricing: Transfer learning across
411"
REFERENCES,0.30390995260663506,"experiments. CoRR, abs/1902.10918, 2019. URL https://arxiv.org/abs/1902.10918.
412"
REFERENCES,0.30450236966824645,"Soumya Basu, Branislav Kveton, Manzil Zaheer, and Csaba Szepesvari. No regrets for learning the
413"
REFERENCES,0.30509478672985785,"prior in bandits. In Advances in Neural Information Processing Systems 34, 2021.
414"
REFERENCES,0.3056872037914692,"Christopher M Bishop. Pattern Recognition and Machine Learning, volume 4 of Information science
415"
REFERENCES,0.3062796208530806,"and statistics. Springer, 2006.
416"
REFERENCES,0.3068720379146919,"Leonardo Cella, Alessandro Lazaric, and Massimiliano Pontil. Meta-learning with stochastic linear
417"
REFERENCES,0.3074644549763033,"bandits. In Proceedings of the 37th International Conference on Machine Learning, 2020.
418"
REFERENCES,0.3080568720379147,"Leonardo Cella, Karim Lounici, and Massimiliano Pontil. Multi-task representation learning with
419"
REFERENCES,0.30864928909952605,"stochastic linear bandits. arXiv preprint arXiv:2202.10066, 2022.
420"
REFERENCES,0.30924170616113744,"Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In Advances in
421"
REFERENCES,0.30983412322274884,"Neural Information Processing Systems 24, pages 2249–2257, 2012.
422"
REFERENCES,0.3104265402843602,"Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion
423"
REFERENCES,0.31101895734597157,"posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022.
424"
REFERENCES,0.31161137440758296,"Aniket Anand Deshmukh, Urun Dogan, and Clayton Scott. Multi-task learning for contextual bandits.
425"
REFERENCES,0.3122037914691943,"In Advances in Neural Information Processing Systems 30, pages 4848–4856, 2017.
426"
REFERENCES,0.3127962085308057,"Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances
427"
REFERENCES,0.31338862559241704,"in neural information processing systems, 34:8780–8794, 2021.
428"
REFERENCES,0.31398104265402843,"Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric bandits: The
429"
REFERENCES,0.3145734597156398,"generalized linear case. In Advances in Neural Information Processing Systems 23, pages 586–594,
430"
REFERENCES,0.31516587677725116,"2010.
431"
REFERENCES,0.31575829383886256,"Dylan J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert. Adapting to misspecification in
432"
REFERENCES,0.31635071090047395,"contextual bandits. Advances in Neural Information Processing Systems, 33:11478–11489, 2020.
433"
REFERENCES,0.3169431279620853,"Claudio Gentile, Shuai Li, and Giovanni Zappella. Online clustering of bandits. In Proceedings of
434"
REFERENCES,0.3175355450236967,"the 31st International Conference on Machine Learning, pages 757–765, 2014.
435"
REFERENCES,0.3181279620853081,"Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson sampling for complex online
436"
REFERENCES,0.3187203791469194,"problems. In Proceedings of the 31st International Conference on Machine Learning, pages
437"
REFERENCES,0.3193127962085308,"100–108, 2014.
438"
REFERENCES,0.31990521327014215,"Samarth Gupta, Shreyas Chaudhari, Subhojyoti Mukherjee, Gauri Joshi, and Osman Yagan. A
439"
REFERENCES,0.32049763033175355,"unified approach to translate classical bandit algorithms to the structured bandit setting. CoRR,
440"
REFERENCES,0.32109004739336494,"abs/1810.08164, 2018. URL https://arxiv.org/abs/1810.08164.
441"
REFERENCES,0.3216824644549763,"Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
442"
REFERENCES,0.3222748815165877,"neural information processing systems, 33:6840–6851, 2020.
443"
REFERENCES,0.32286729857819907,"Joey Hong, Branislav Kveton, Manzil Zaheer, Yinlam Chow, Amr Ahmed, and Craig Boutilier. Latent
444"
REFERENCES,0.3234597156398104,"bandits revisited. In Advances in Neural Information Processing Systems 33, 2020.
445"
REFERENCES,0.3240521327014218,"Joey Hong, Branislav Kveton, Sumeet Katariya, Manzil Zaheer, and Mohammad Ghavamzadeh.
446"
REFERENCES,0.3246445497630332,"Deep hierarchy in bandits. In International Conference on Machine Learning, pages 8833–8851.
447"
REFERENCES,0.32523696682464454,"PMLR, 2022a.
448"
REFERENCES,0.32582938388625593,"Joey Hong, Branislav Kveton, Manzil Zaheer, and Mohammad Ghavamzadeh. Hierarchical Bayesian
449"
REFERENCES,0.3264218009478673,"bandits. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics,
450"
REFERENCES,0.32701421800947866,"2022b.
451"
REFERENCES,0.32760663507109006,"Yu-Guan Hsieh, Shiva Prasad Kasiviswanathan, Branislav Kveton, and Patrick Blöbaum. Thompson
452"
REFERENCES,0.3281990521327014,"sampling with diffusion generative prior. arXiv preprint arXiv:2301.05182, 2023.
453"
REFERENCES,0.3287914691943128,"Jiachen Hu, Xiaoyu Chen, Chi Jin, Lihong Li, and Liwei Wang. Near-optimal representation learning
454"
REFERENCES,0.3293838862559242,"for linear bandits and linear rl. In International Conference on Machine Learning, pages 4349–4358.
455"
REFERENCES,0.3299763033175355,"PMLR, 2021.
456"
REFERENCES,0.3305687203791469,"Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for
457"
REFERENCES,0.3311611374407583,"flexible behavior synthesis. arXiv preprint arXiv:2205.09991, 2022.
458"
REFERENCES,0.33175355450236965,"Emilie Kaufmann, Nathaniel Korda, and Rémi Munos. Thompson sampling: An asymptotically
459"
REFERENCES,0.33234597156398105,"optimal finite-time analysis. In International conference on algorithmic learning theory, pages
460"
REFERENCES,0.33293838862559244,"199–213. Springer, 2012.
461"
REFERENCES,0.3335308056872038,"Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT
462"
REFERENCES,0.3341232227488152,"Press, Cambridge, MA, 2009.
463"
REFERENCES,0.3347156398104265,"Nathaniel Korda, Emilie Kaufmann, and Remi Munos. Thompson sampling for 1-dimensional
464"
REFERENCES,0.3353080568720379,"exponential family bandits. Advances in neural information processing systems, 26, 2013.
465"
REFERENCES,0.3359004739336493,"John K Kruschke. Bayesian data analysis. Wiley Interdisciplinary Reviews: Cognitive Science, 1(5):
466"
REFERENCES,0.33649289099526064,"658–676, 2010.
467"
REFERENCES,0.33708530805687204,"Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and
468"
REFERENCES,0.33767772511848343,"Craig Boutilier. Randomized exploration in generalized linear bandits. In International Conference
469"
REFERENCES,0.33827014218009477,"on Artificial Intelligence and Statistics, pages 2066–2076. PMLR, 2020.
470"
REFERENCES,0.33886255924170616,"Branislav Kveton, Mikhail Konobeev, Manzil Zaheer, Chih-Wei Hsu, Martin Mladenov, Craig
471"
REFERENCES,0.33945497630331756,"Boutilier, and Csaba Szepesvari. Meta-Thompson sampling. In Proceedings of the 38th Interna-
472"
REFERENCES,0.3400473933649289,"tional Conference on Machine Learning, 2021.
473"
REFERENCES,0.3406398104265403,"Tze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of
474"
REFERENCES,0.3412322274881517,"Statistics, 15(3):1091–1114, 1987.
475"
REFERENCES,0.341824644549763,"Tor Lattimore and Remi Munos. Bounded regret for finite-armed structured bandits. In Advances in
476"
REFERENCES,0.3424170616113744,"Neural Information Processing Systems 27, pages 550–558, 2014.
477"
REFERENCES,0.34300947867298576,"Lihong Li, Wei Chu, John Langford, and Robert Schapire. A contextual-bandit approach to personal-
478"
REFERENCES,0.34360189573459715,"ized news article recommendation. In Proceedings of the 19th International Conference on World
479"
REFERENCES,0.34419431279620855,"Wide Web, 2010.
480"
REFERENCES,0.3447867298578199,"Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual
481"
REFERENCES,0.3453791469194313,"bandits. In Proceedings of the 34th International Conference on Machine Learning, pages 2071–
482"
REFERENCES,0.3459715639810427,"2080, 2017.
483"
REFERENCES,0.346563981042654,"Dennis Lindley and Adrian Smith. Bayes estimates for the linear model. Journal of the Royal
484"
REFERENCES,0.3471563981042654,"Statistical Society: Series B (Methodological), 34(1):1–18, 1972.
485"
REFERENCES,0.3477488151658768,"Xiuyuan Lu and Benjamin Van Roy. Information-theoretic confidence bounds for reinforcement
486"
REFERENCES,0.34834123222748814,"learning. In Advances in Neural Information Processing Systems 32, 2019.
487"
REFERENCES,0.34893364928909953,"Odalric-Ambrym Maillard and Shie Mannor. Latent bandits. In Proceedings of the 31st International
488"
REFERENCES,0.3495260663507109,"Conference on Machine Learning, pages 136–144, 2014.
489"
REFERENCES,0.35011848341232227,"P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman & Hall, 1989.
490"
REFERENCES,0.35071090047393366,"Amit Peleg, Naama Pearl, and Ron Meirr. Metalearning linear bandits by prior update. In Proceedings
491"
REFERENCES,0.351303317535545,"of the 25th International Conference on Artificial Intelligence and Statistics, 2022.
492"
REFERENCES,0.3518957345971564,"Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical
493"
REFERENCES,0.3524881516587678,"comparison of bayesian deep networks for thompson sampling. arXiv preprint arXiv:1802.09127,
494"
REFERENCES,0.35308056872037913,"2018.
495"
REFERENCES,0.3536729857819905,"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
496"
REFERENCES,0.3542654028436019,"resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-
497"
REFERENCES,0.35485781990521326,"ence on computer vision and pattern recognition, pages 10684–10695, 2022.
498"
REFERENCES,0.35545023696682465,"Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of
499"
REFERENCES,0.35604265402843605,"Operations Research, 39(4):1221–1243, 2014.
500"
REFERENCES,0.3566350710900474,"Steven Scott. A modern bayesian look at the multi-armed bandit. Applied Stochastic Models in
501"
REFERENCES,0.3572274881516588,"Business and Industry, 26:639 – 658, 2010.
502"
REFERENCES,0.3578199052132701,"Max Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel Hsu, Thodoris Lykouris, Miro
503"
REFERENCES,0.3584123222748815,"Dudik, and Robert Schapire. Bayesian decision-making under misspecified priors with applications
504"
REFERENCES,0.3590047393364929,"to meta-learning. In Advances in Neural Information Processing Systems 34, 2021.
505"
REFERENCES,0.35959715639810425,"Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
506"
REFERENCES,0.36018957345971564,"learning using nonequilibrium thermodynamics. In International conference on machine learning,
507"
REFERENCES,0.36078199052132703,"pages 2256–2265. PMLR, 2015.
508"
REFERENCES,0.3613744075829384,"Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged
509"
REFERENCES,0.36196682464454977,"bandit feedback. In International Conference on Machine Learning, pages 814–823. PMLR, 2015.
510"
REFERENCES,0.36255924170616116,"Runzhe Wan, Lin Ge, and Rui Song. Metadata-based multi-task bandits with Bayesian hierarchical
511"
REFERENCES,0.3631516587677725,"models. In Advances in Neural Information Processing Systems 34, 2021.
512"
REFERENCES,0.3637440758293839,"Runzhe Wan, Lin Ge, and Rui Song. Towards scalable and robust structured bandits: A meta-learning
513"
REFERENCES,0.36433649289099523,"framework. CoRR, abs/2202.13227, 2022. URL https://arxiv.org/abs/2202.13227.
514"
REFERENCES,0.36492890995260663,"Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy
515"
REFERENCES,0.365521327014218,"class for offline reinforcement learning. arXiv preprint arXiv:2208.06193, 2022.
516"
REFERENCES,0.36611374407582936,"Neil Weiss. A Course in Probability. Addison-Wesley, 2005.
517"
REFERENCES,0.36670616113744076,"Yunbei Xu and Assaf Zeevi. Upper counterfactual confidence bounds: a new optimism principle for
518"
REFERENCES,0.36729857819905215,"contextual bandits. arXiv preprint arXiv:2007.07876, 2020.
519"
REFERENCES,0.3678909952606635,"Jiaqi Yang, Wei Hu, Jason D Lee, and Simon S Du. Impact of representation learning in linear bandits.
520"
REFERENCES,0.3684834123222749,"arXiv preprint arXiv:2010.06531, 2020.
521"
REFERENCES,0.3690758293838863,"Tong Yu, Branislav Kveton, Zheng Wen, Ruiyi Zhang, and Ole Mengshoel. Graphical models meet
522"
REFERENCES,0.3696682464454976,"bandits: A variational Thompson sampling approach. In Proceedings of the 37th International
523"
REFERENCES,0.370260663507109,"Conference on Machine Learning, 2020.
524"
REFERENCES,0.3708530805687204,"Yinglun Zhu, Dylan J Foster, John Langford, and Paul Mineiro. Contextual bandits with large action
525"
REFERENCES,0.37144549763033174,"spaces: Made practical. In International Conference on Machine Learning, pages 27428–27453.
526"
REFERENCES,0.37203791469194314,"PMLR, 2022.
527"
REFERENCES,0.3726303317535545,"Supplementary materials
528"
REFERENCES,0.3732227488151659,"Notation. For any positive integer n, we define [n] = {1, 2, ..., n}. Let v1, . . . , vn ∈Rd be n vectors,
529"
REFERENCES,0.37381516587677727,"(vi)i∈[n] ∈Rnd is the nd-dimensional vector obtained by concatenating v1, . . . , vn. For any matrix
530"
REFERENCES,0.3744075829383886,"A ∈Rd×d, λ1(A) and λd(A) denote the maximum and minimum eigenvalues of A, respectively.
531"
REFERENCES,0.375,"Finally, we write ˜O for the big-O notation up to polylogarithmic factors.
532"
REFERENCES,0.3755924170616114,"A
Extended related work
533"
REFERENCES,0.37618483412322273,"Thompson sampling (TS) operates within the Bayesian framework and it involves specifying a
534"
REFERENCES,0.3767772511848341,"prior/likelihood model. In each round, the agent samples unknown model parameters from the
535"
REFERENCES,0.3773696682464455,"current posterior distribution. The chosen action is the one that maximizes the resulting reward. TS
536"
REFERENCES,0.37796208530805686,"is naturally randomized, particularly simple to implement, and has highly competitive empirical
537"
REFERENCES,0.37855450236966826,"performance in both simulated and real-world problems [Russo and Van Roy, 2014, Chapelle and Li,
538"
REFERENCES,0.3791469194312796,"2012]. Regret guarantees for the TS heuristic remained open for decades even for simple models.
539"
REFERENCES,0.379739336492891,"Recently, however, significant progress has been made. For standard multi-armed bandits, TS is
540"
REFERENCES,0.3803317535545024,"optimal in the Beta-Bernoulli model [Kaufmann et al., 2012, Agrawal and Goyal, 2013b], Gaussian-
541"
REFERENCES,0.3809241706161137,"Gaussian model [Agrawal and Goyal, 2013b], and in the exponential family using Jeffrey’s prior
542"
REFERENCES,0.3815165876777251,"[Korda et al., 2013]. For linear bandits, TS is nearly-optimal [Russo and Van Roy, 2014, Agrawal and
543"
REFERENCES,0.3821090047393365,"Goyal, 2017, Abeille and Lazaric, 2017]. In this work, we build TS upon complex diffusion priors
544"
REFERENCES,0.38270142180094785,"and analyze the resulting Bayes regret [Russo and Van Roy, 2014] in the linear contextual bandit
545"
REFERENCES,0.38329383886255924,"setting.
546"
REFERENCES,0.38388625592417064,"Decision-making with diffusion models gained attention recently, especially in offline learning
547"
REFERENCES,0.384478672985782,"[Ajay et al., 2022, Janner et al., 2022, Wang et al., 2022]. However, their application in online
548"
REFERENCES,0.38507109004739337,"learning was only examined by Hsieh et al. [2023], which focused on meta-learning in multi-armed
549"
REFERENCES,0.38566350710900477,"bandits without theoretical guarantees. In this work, we expand the scope of Hsieh et al. [2023] to
550"
REFERENCES,0.3862559241706161,"encompass the broader contextual bandit framework. In particular, we provide theoretical analysis for
551"
REFERENCES,0.3868483412322275,"linear instances, effectively capturing the advantages of using diffusion models as priors in contextual
552"
REFERENCES,0.38744075829383884,"Thompson sampling. These linear cases are particularly captivating due to closed-form posteriors,
553"
REFERENCES,0.38803317535545023,"enabling both theoretical analysis and computational efficiency; an important practical consideration.
554"
REFERENCES,0.3886255924170616,"Hierarchical Bayesian bandits [Bastani et al., 2019, Kveton et al., 2021, Basu et al., 2021, Sim-
555"
REFERENCES,0.38921800947867297,"chowitz et al., 2021, Wan et al., 2021, Hong et al., 2022b, Peleg et al., 2022, Wan et al., 2022, Aouali
556"
REFERENCES,0.38981042654028436,"et al., 2023b] applied TS to simple graphical models, wherein action parameters are generally sampled
557"
REFERENCES,0.39040284360189575,"from a Gaussian distribution centered at a single latent parameter. These works mostly span meta-
558"
REFERENCES,0.3909952606635071,"and multi-task learning for multi-armed bandits, except in cases such as Aouali et al. [2023b], Hong
559"
REFERENCES,0.3915876777251185,"et al. [2022a] that consider the contextual bandit setting. Precisely, Aouali et al. [2023b] assume that
560"
REFERENCES,0.3921800947867299,"action parameters are sampled from a Gaussian distribution centered at a linear mixture of multiple
561"
REFERENCES,0.3927725118483412,"latent parameters. On the other hand, Hong et al. [2022a] applied TS to a graphical model represented
562"
REFERENCES,0.3933649289099526,"by a tree. Our work can be seen as an extension of all these works to much more complex graphical
563"
REFERENCES,0.39395734597156395,"models, for which both theoretical and algorithmic foundations are developed. Note that the settings
564"
REFERENCES,0.39454976303317535,"in most of these works can be recovered with specific choices of the diffusion depth L and functions
565"
REFERENCES,0.39514218009478674,"fℓ. This attests to the modeling power of dTS.
566"
REFERENCES,0.3957345971563981,"Approximate Thompson sampling is a major problem in the Bayesian inference literature. This is
567"
REFERENCES,0.3963270142180095,"because most posterior distributions are intractable, and thus practitioners must resort to sophisti-
568"
REFERENCES,0.39691943127962087,"cated computational techniques such as Markov chain Monte Carlo [Kruschke, 2010]. Prior works
569"
REFERENCES,0.3975118483412322,"[Riquelme et al., 2018, Chapelle and Li, 2012, Kveton et al., 2020] highlight the favorable empirical
570"
REFERENCES,0.3981042654028436,"performance of approximate Thompson sampling. Particularly, [Kveton et al., 2020] provide the-
571"
REFERENCES,0.398696682464455,"oretical guarantees for Thompson sampling when using the Laplace approximation in generalized
572"
REFERENCES,0.39928909952606634,"linear bandits (GLB). In our context, we incorporate approximate sampling when the reward exhibits
573"
REFERENCES,0.39988151658767773,"non-linearity. While our approximation does not come with formal guarantees, it enjoys strong
574"
REFERENCES,0.4004739336492891,"practical performance. An in-depth analysis of this approximation is left as a direction for future
575"
REFERENCES,0.40106635071090047,"works. Similarly, approximating the posterior distribution when the diffusion model is non-linear as
576"
REFERENCES,0.40165876777251186,"well as analyzing it is an interesting direction of future works.
577"
REFERENCES,0.4022511848341232,"Bandits with underlying structure also align with our work, where we assume a structured relation-
578"
REFERENCES,0.4028436018957346,"ship among actions, captured by a diffusion model. In latent bandits [Maillard and Mannor, 2014,
579"
REFERENCES,0.403436018957346,"Hong et al., 2020], a single latent variable indexes multiple candidate models. Within structured
580"
REFERENCES,0.4040284360189573,"finite-armed bandits [Lattimore and Munos, 2014, Gupta et al., 2018], each action is linked to a known
581"
REFERENCES,0.4046208530805687,"mean function parameterized by a common latent parameter. This latent parameter is learned. TS
582"
REFERENCES,0.4052132701421801,"was also applied to complex structures [Yu et al., 2020, Gopalan et al., 2014]. However, simultaneous
583"
REFERENCES,0.40580568720379145,"computational and statistical efficiencies aren’t guaranteed. Meta- and multi-task learning with
584"
REFERENCES,0.40639810426540285,"upper confidence bound (UCB) approaches have a long history in bandits [Azar et al., 2013, Gentile
585"
REFERENCES,0.40699052132701424,"et al., 2014, Deshmukh et al., 2017, Cella et al., 2020]. These, however, often adopt a frequentist
586"
REFERENCES,0.4075829383886256,"perspective, analyze a stronger form of regret, and sometimes result in conservative algorithms.
587"
REFERENCES,0.408175355450237,"In contrast, our approach is Bayesian, with analysis centered on Bayes regret. Remarkably, our
588"
REFERENCES,0.4087677725118483,"algorithm, dTS, performs well as analyzed without necessitating additional tuning. Finally, Low-rank
589"
REFERENCES,0.4093601895734597,"bandits [Hu et al., 2021, Cella et al., 2022, Yang et al., 2020] also relate to our linear diffusion model
590"
REFERENCES,0.4099526066350711,"when L = 1. Broadly, there exist two key distinctions between these prior works and the special
591"
REFERENCES,0.41054502369668244,"case of our model (linear diffusion model with L = 1). First, they assume θ∗,i = W1ψ∗,1, whereas
592"
REFERENCES,0.41113744075829384,"we incorporate additional uncertainty in the covariance Σ1 to account for possible misspecification
593"
REFERENCES,0.41172985781990523,"as θ∗,i = N(W1ψ∗,1, Σ1). Consequently, these algorithms might suffer linear regret due to model
594"
REFERENCES,0.41232227488151657,"misalignment. Second, we assume that the mixing matrix W1 is available and pre-learned offline,
595"
REFERENCES,0.41291469194312796,"whereas they learn it online. While this is more general, it leads to computationally expensive
596"
REFERENCES,0.41350710900473936,"methods that are difficult to employ in a real-world online setting.
597"
REFERENCES,0.4140995260663507,"Large action spaces. Roughly speaking, the regret bound of dTS scales with Kσ2
1 rather than
598 K P"
REFERENCES,0.4146919431279621,"ℓσ2
ℓ. This is particularly beneficial when σ1 is small, a common scenario in diffusion models
599"
REFERENCES,0.4152843601895735,"with decreasing variances. A notable case is when σ1 = 0, where the regret becomes independent of
600"
REFERENCES,0.4158767772511848,"K. Also, our analysis (Section 4.1) indicates that the gap in performance between dTS and LinTS
601"
REFERENCES,0.4164691943127962,"becomes more pronounced when the number of action increases, highlighting dTS’s suitability for
602"
REFERENCES,0.41706161137440756,"large action spaces. Note that some prior works [Foster et al., 2020, Xu and Zeevi, 2020, Zhu et al.,
603"
REFERENCES,0.41765402843601895,"2022] proposed bandit algorithms that do not scale with K. However, our setting differs significantly
604"
REFERENCES,0.41824644549763035,"from theirs, explaining our inherent dependency on K when σ1 > 0. Precisely, they assume a
605"
REFERENCES,0.4188388625592417,"reward function of r(x, i) = ϕ(x, i)⊤θ∗, with a shared θ∗∈Rd across actions and a known mapping
606"
REFERENCES,0.4194312796208531,"ϕ. In contrast, we consider r(x, i) = x⊤θ∗,i, requiring the learning of K separate d-dimensional
607"
REFERENCES,0.4200236966824645,"action parameters. In their setting, with the availability of ϕ, the regret of dTS would similarly be
608"
REFERENCES,0.4206161137440758,"independent of K. However, obtaining such a mapping ϕ can be challenging as it needs to encapsulate
609"
REFERENCES,0.4212085308056872,"complex context-action dependencies. Notably, our setting reflects a common practical scenario,
610"
REFERENCES,0.4218009478672986,"such as in recommendation systems where each product is often represented by its embedding. In
611"
REFERENCES,0.42239336492890994,"summary, the dependency on K is more related to our setting than the method itself, and dTS would
612"
REFERENCES,0.42298578199052134,"scale with d only in their setting. Note that dTS is both computationally and statistically efficient
613"
REFERENCES,0.4235781990521327,"(Section 4.1). This becomes particularly notable in large action spaces. Our empirical results in
614"
REFERENCES,0.42417061611374407,"Fig. 2, notably with K = 104, demonstrate that dTS significantly outperforms the baselines. More
615"
REFERENCES,0.42476303317535546,"importantly, the performance gap between dTS and these baselines is larger when the number of
616"
REFERENCES,0.4253554502369668,"actions (K) increases, highlighting the improved scalability of dTS to large action spaces.
617"
REFERENCES,0.4259478672985782,"B
Posterior derivations for linear diffusion models
618"
REFERENCES,0.4265402843601896,"Here, we assume the score functions fℓare linear such as fℓ(ψ∗,ℓ) = Wℓψ∗,ℓfor ℓ∈[L], where
619"
REFERENCES,0.42713270142180093,"Wℓ∈Rd×d are known mixing matrices. Then, (1) becomes a linear Gaussian system (LGS) [Bishop,
620"
REFERENCES,0.4277251184834123,"2006] and can be summarized as follows
621"
REFERENCES,0.4283175355450237,"ψ∗,L ∼N(0, ΣL+1) ,
(15)
ψ∗,ℓ−1 | ψ∗,ℓ∼N(Wℓψ∗,ℓ, Σℓ) ,
∀ℓ∈[L]/{1} ,
θ∗,i | ψ∗,1 ∼N(W1ψ∗,1, Σ1) ,
∀i ∈[K] ,
Yt | Xt, θ∗,At ∼P(· | Xt; θ∗,At) ,
∀t ∈[n] ."
REFERENCES,0.42890995260663506,"In this section, we derive the K +L posteriors Pt,i and Qt,ℓ, for which we provide the full expressions
622"
REFERENCES,0.42950236966824645,"in Appendix B.1. In our proofs, p(x) ∝f(x) means that the probability density p satisfies p(x) =
623 f(x)"
REFERENCES,0.43009478672985785,"Z
for any x ∈Rd, where Z is a normalization constant. In particular, we extensively use that if
624"
REFERENCES,0.4306872037914692,p(x) ∝exp[−1
REFERENCES,0.4312796208530806,"2x⊤Λx + x⊤m], where Λ is positive definite. Then p is the multivariate Gaussian
625"
REFERENCES,0.4318720379146919,"density with covariance Σ = Λ−1 and mean µ = Σm. These are standard notations and techniques
626"
REFERENCES,0.4324644549763033,"to manipulate Gaussian distributions [Koller and Friedman, 2009, Chapter 7].
627"
REFERENCES,0.4330568720379147,"B.1
Posterior expressions for linear diffusion models
628"
REFERENCES,0.43364928909952605,"Recall that we posit that the reward distribution is parameterized as a generalized linear model (GLM)
629"
REFERENCES,0.43424170616113744,"[McCullagh and Nelder, 1989], allowing for non-linear rewards. As a result, despite linearity in
630"
REFERENCES,0.43483412322274884,"score functions, the non-linearity in rewards makes it challenging to obtain closed-form posteriors.
631"
REFERENCES,0.4354265402843602,"However, since this non-linearity arises solely from the reward distribution, we approximate it using
632"
REFERENCES,0.43601895734597157,"a Gaussian distribution. This leads to efficient posterior approximations that are exact in cases where
633"
REFERENCES,0.43661137440758296,"the reward function is indeed Gaussian (a special case of the GLM model). Precisely, the reward
634"
REFERENCES,0.4372037914691943,"distribution P(· | x; θ) is an exponential-family distribution. Therefore, the log-likelihoods write
635"
REFERENCES,0.4377962085308057,"log P (Ht,i | θ∗,i = θ) = P"
REFERENCES,0.43838862559241704,"k∈St,i YkX⊤
k θ −A(X⊤
k θ) + C(Yk), where C is a real function, and A
636"
REFERENCES,0.43898104265402843,"is a twice continuously differentiable function whose derivative is the mean function, ˙A = g. Now
637"
REFERENCES,0.4395734597156398,"we let ˆBt,i and ˆGt,i be the maximum likelihood estimate (MLE) and the Hessian of the negative
638"
REFERENCES,0.44016587677725116,"log-likelihood, respectively, defined as
639"
REFERENCES,0.44075829383886256,"ˆBt,i = arg max
θ∈Rd
log P (Ht,i | θ∗,i = θ) ,
ˆGt,i =
X"
REFERENCES,0.44135071090047395,"k∈St,i
˙g
 
X⊤
k ˆBt,i

XkX⊤
k .
(16)"
REFERENCES,0.4419431279620853,"where St,i = {ℓ∈[t −1] : Aℓ= i} are the rounds where the agent takes action i up to round t.
640"
REFERENCES,0.4425355450236967,"Then we approximation the respective likelihood as P (Ht,i | θ∗,i = θ) ≈N
 
θ; ˆBt,i, ˆG−1
t,i

. This
641"
REFERENCES,0.4431279620853081,"approximation makes all posteriors Gaussian. First, the conditional action-posterior reads Pt,i(· |
642"
REFERENCES,0.4437203791469194,"ψ1) = N(·; ˆµt,i, ˆΣt,i),
643"
REFERENCES,0.4443127962085308,"ˆΣ−1
t,i = Σ−1
1
+ ˆGt,i
ˆµt,i = ˆΣt,i
 
Σ−1
1 W1ψ1 + ˆGt,i ˆBt,i

.
(17)"
REFERENCES,0.44490521327014215,"For ℓ∈[L]/{1}, the ℓ−1-th conditional latent-posterior is Qt,ℓ−1(· | ψℓ) = N(¯µt,ℓ−1, ¯Σt,ℓ−1),
644"
REFERENCES,0.44549763033175355,"¯Σ−1
t,ℓ−1 = Σ−1
ℓ
+ ¯Gt,ℓ−1 ,
¯µt,ℓ−1 = ¯Σt,ℓ−1
 
Σ−1
ℓWℓψℓ+ ¯Bt,ℓ−1

,
(18)"
REFERENCES,0.44609004739336494,"and the L-th latent-posterior is Qt,L(·) = N(¯µt,L, ¯Σt,L),
645"
REFERENCES,0.4466824644549763,"¯Σ−1
t,L = Σ−1
L+1 + ¯Gt,L ,
¯µt,L = ¯Σt,L ¯Bt,L .
(19)"
REFERENCES,0.4472748815165877,"Finally, ¯Gt,ℓand ¯Bt,ℓfor ℓ∈[L] are computed recursively. The basis of the recursion are
646"
REFERENCES,0.44786729857819907,"¯Gt,1 = W⊤
1 K
X i=1"
REFERENCES,0.4484597156398104," 
Σ−1
1
−Σ−1
1 ˆΣt,iΣ−1
1

W1 ,
¯Bt,1 = W⊤
1 Σ−1
1 K
X"
REFERENCES,0.4490521327014218,"i=1
ˆΣt,i ˆGt,i ˆBt,i .
(20)"
REFERENCES,0.4496445497630332,"Then, the recursive step for ℓ∈[L]/{1} is,
647"
REFERENCES,0.45023696682464454,"¯Gt,ℓ= W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ

Wℓ,
¯Bt,ℓ= W⊤
ℓΣ−1
ℓ
¯Σt,ℓ−1 ¯Bt,ℓ−1 .
(21)"
REFERENCES,0.45082938388625593,"This concludes the derivation of our posterior approximation. Note that these approximations are exact
648"
REFERENCES,0.4514218009478673,"when the reward distribution follows a linear-Gaussian model, P(· | x; θ∗,a) = N(·; x⊤θ∗,a, σ2).
649"
REFERENCES,0.45201421800947866,"B.2
Derivation of Action-Posteriors for Linear Diffusion Models
650"
REFERENCES,0.45260663507109006,"To simplify derivations, we consider the case where the reward distribution is indeed linear-
651"
REFERENCES,0.4531990521327014,"Gaussian as P(· | Xt; θ∗,At) = N
 
X⊤
t θ∗,At, σ2
, but the same derivations can be applied when
652"
REFERENCES,0.4537914691943128,"the rewards are non-linear. In this case, the likelihood approximation in (16) becomes exact as
653"
REFERENCES,0.4543838862559242,"we have that P (Ht,i | θ∗,i = θ) ∝N
 
θ; ˆBt,i, ˆG−1
t,i

, where ˆBt,i is the corresponding MLE and
654"
REFERENCES,0.4549763033175355,"ˆGt,i = σ−2 P"
REFERENCES,0.4555687203791469,"k∈St,i XkX⊤
k in this case. Our derivations rely on the fact that the MLE ˆBt,i in this
655"
REFERENCES,0.4561611374407583,"linear-Gaussian case satisfies: ˆGt,i ˆBt,i = v P"
REFERENCES,0.45675355450236965,"k∈St,i XkY ⊤
k .
656"
REFERENCES,0.45734597156398105,"Proposition B.1. Consider the following model, which corresponds to the last two layers in Eq. (15)
657"
REFERENCES,0.45793838862559244,"θ∗,i | ψ∗,1 ∼N (W1ψ∗,1, Σ1) ,"
REFERENCES,0.4585308056872038,"Yt | Xt, θ∗,At ∼N
 
X⊤
t θ∗,At, σ2
,
∀t ∈[n] ."
REFERENCES,0.4591232227488152,"Then we have that for any t ∈[n] and i ∈[K], Pt,i(θ | ψ1) = P (θ∗,i = θ | ψ∗,1 = ψ1, Ht,i) =
658"
REFERENCES,0.4597156398104265,"N(θ; ˆµt,i, ˆΣt,i), where
659"
REFERENCES,0.4603080568720379,"ˆΣ−1
t,i = ˆGt,i + Σ−1
1
,
ˆµt,i = ˆΣt,i

ˆGt,i ˆBt,i + Σ−1
1 W1ψ1

."
REFERENCES,0.4609004739336493,"Proof. Let v = σ−2 ,
Λ1 = Σ−1
1
. Then the action-posterior decomposes as
660"
REFERENCES,0.46149289099526064,"Pt,i(θ | ψ1) = P (θ∗,i = θ | ψ∗,1 = ψ1, Ht,i) ,
∝P (Ht,i | ψ∗,1 = ψ1, θ∗,i = θ) P (θ∗,i = θ | ψ∗,1 = ψ1) ,
(Bayes rule)
= P (Ht,i | θ∗,i = θ) P (θ∗,i = θ | ψ∗,1 = ψ1) , (given θ∗,i, Ht,i is independent of ψ∗,1) =
Y"
REFERENCES,0.46208530805687204,"k∈St,i
N(Yk; X⊤
k θ, σ2)N(θ; W1ψ1, Σ1) ,"
REFERENCES,0.46267772511848343,"= exp
h
−1 2 
v
X"
REFERENCES,0.46327014218009477,"k∈St,i
(Y 2
k −2YkX⊤
k θ + (X⊤
k θ)2) + θ⊤Λ1θ −2θ⊤Λ1W1ψ1"
REFERENCES,0.46386255924170616,"+
 
W1ψ1
⊤Λ1
 
W1ψ1
i
,"
REFERENCES,0.46445497630331756,"∝exp
h
−1 2"
REFERENCES,0.4650473933649289,"
θ⊤(v
X"
REFERENCES,0.4656398104265403,"k∈St,i
XkX⊤
k + Λ1)θ −2θ⊤
v
X"
REFERENCES,0.4662322274881517,"k∈St,i
XkYk + Λ1W1ψ1
i
,"
REFERENCES,0.466824644549763,"∝N
 
θ; ˆµt,i, ˆΛ−1
t,i

,"
REFERENCES,0.4674170616113744,"with ˆΛt,i = v P"
REFERENCES,0.46800947867298576,"k∈St,i XkX⊤
k + Λ1 , ˆΛt,iˆµt,i = v P"
REFERENCES,0.46860189573459715,"k∈St,i XkYk + Λ1W1ψ1. Using that, in this
661"
REFERENCES,0.46919431279620855,"linear-Gaussian case, ˆGt,i = v P"
REFERENCES,0.4697867298578199,"k∈St,i XkX⊤
k and ˆGt,i ˆBt,i = v P"
REFERENCES,0.4703791469194313,"k∈St,i XkYk concludes the
662"
REFERENCES,0.4709715639810427,"proof.
663"
REFERENCES,0.471563981042654,"The same proof applies when the reward distribution is not linear-Gaussian, with the approximation
664"
REFERENCES,0.4721563981042654,"P (Ht,i | θ∗,i = θ) ≈N
 
θ; ˆBt,i, ˆG−1
t,i

. Using this approximation in the derivations above leads to
665"
REFERENCES,0.4727488151658768,"the same results.
666"
REFERENCES,0.47334123222748814,"B.3
Derivation of recursive latent-posteriors for linear diffusion models
667"
REFERENCES,0.47393364928909953,"Again, to simplify derivations, we consider the case where the reward distribution is indeed linear-
668"
REFERENCES,0.4745260663507109,"Gaussian as P(· | Xt; θ∗,At) = N
 
X⊤
t θ∗,At, σ2
, but the same derivations can be applied when the
669"
REFERENCES,0.47511848341232227,"rewards are non-linear.
670"
REFERENCES,0.47571090047393366,"Proposition B.2. For any ℓ∈[L]/{1}, the ℓ−1-th conditional latent-posterior reads Qt,ℓ−1(· |
671"
REFERENCES,0.476303317535545,"ψℓ) = N(¯µt,ℓ−1, ¯Σt,ℓ−1), with
672"
REFERENCES,0.4768957345971564,"¯Σ−1
t,ℓ−1 = Σ−1
ℓ
+ ¯Gt,ℓ−1 ,
¯µt,ℓ−1 = ¯Σt,ℓ−1
 
Σ−1
ℓWℓψℓ+ ¯Bt,ℓ−1

,
(22)"
REFERENCES,0.4774881516587678,"and the L-th latent-posterior reads Qt,L(·) = N(¯µt,L, ¯Σt,L), with
673"
REFERENCES,0.47808056872037913,"¯Σ−1
t,L = Σ−1
L+1 + ¯Gt,L ,
¯µt,L = ¯Σt,L ¯Bt,L .
(23)"
REFERENCES,0.4786729857819905,"Proof. Let ℓ∈[L]/{1}. Then, Bayes rule yields that
674"
REFERENCES,0.4792654028436019,"Qt,ℓ−1(ψℓ−1 | ψℓ) ∝P (Ht | ψ∗,ℓ−1 = ψℓ−1) N(ψℓ−1, Wℓψℓ, Σℓ) ,"
REFERENCES,0.47985781990521326,"But from Lemma B.3, we know that
675"
REFERENCES,0.48045023696682465,"P (Ht | ψ∗,ℓ−1 = ψℓ−1) ∝exp
h
−1"
REFERENCES,0.48104265402843605,"2ψ⊤
ℓ−1 ¯Gt,ℓ−1ψℓ−1 + ψ⊤
ℓ−1 ¯Bt,ℓ−1
i
."
REFERENCES,0.4816350710900474,"Therefore,
676"
REFERENCES,0.4822274881516588,"Qt,ℓ−1(ψℓ−1 | ψℓ) ∝exp
h
−1"
REFERENCES,0.4828199052132701,"2ψ⊤
ℓ−1 ¯Gt,ℓ−1ψℓ−1 + ψ⊤
ℓ−1 ¯Bt,ℓ−1
i
N(ψℓ−1, Wℓψℓ, Σℓ) ,"
REFERENCES,0.4834123222748815,"∝exp
h
−1"
REFERENCES,0.4840047393364929,"2ψ⊤
ℓ−1 ¯Gt,ℓ−1ψℓ−1 + ψ⊤
ℓ−1 ¯Bt,ℓ−1 −1"
REFERENCES,0.48459715639810425,"2(ψℓ−1 −Wℓψℓ)⊤Σ−1
ℓ(ψℓ−1 −Wℓψℓ))
i
,"
REFERENCES,0.48518957345971564,"(i)
∝exp
h
−1"
REFERENCES,0.48578199052132703,"2ψ⊤
ℓ−1( ¯Gt,ℓ−1 + Σ−1
ℓ)ψℓ−1 + ψ⊤
ℓ−1( ¯Bt,ℓ−1 + Σ−1
ℓWℓψℓ)
i
,"
REFERENCES,0.4863744075829384,"(ii)
∝N(ψℓ−1; ¯µt,ℓ−1, ¯Σt,ℓ−1) ,"
REFERENCES,0.48696682464454977,"with ¯Σ−1
t,ℓ−1 = Σ−1
ℓ
+ ¯Gt,ℓ−1 and ¯µt,ℓ−1 = ¯Σt,ℓ−1
 
Σ−1
ℓWℓψℓ+ ¯Bt,ℓ−1

. In (i), we omit terms that
677"
REFERENCES,0.48755924170616116,"are constant in ψℓ−1. In (ii), we complete the square. This concludes the proof for ℓ∈[L]/{1}. For
678"
REFERENCES,0.4881516587677725,"Qt,L, we use Bayes rule to get
679"
REFERENCES,0.4887440758293839,"Qt,L(ψL) ∝P (Ht | ψ∗,L = ψL) N(ψL, 0, ΣL+1) ."
REFERENCES,0.48933649289099523,"Then from Lemma B.3, we know that
680"
REFERENCES,0.48992890995260663,"P (Ht | ψ∗,L = ψL) ∝exp
h
−1"
REFERENCES,0.490521327014218,"2ψ⊤
L ¯Gt,LψL + ψ⊤
L ¯Bt,L
i
,"
REFERENCES,0.49111374407582936,"We then use the same derivations above to compute the product exp
h
−1"
REFERENCES,0.49170616113744076,"2ψ⊤
L ¯Gt,LψL + ψ⊤
L ¯Bt,L
i
×
681"
REFERENCES,0.49229857819905215,"N(ψL, 0, ΣL+1), which concludes the proof.
682"
REFERENCES,0.4928909952606635,"Lemma B.3. The following holds for any t ∈[n] and ℓ∈[L],
683"
REFERENCES,0.4934834123222749,"P (Ht | ψ∗,ℓ= ψℓ) ∝exp
h
−1"
REFERENCES,0.4940758293838863,"2ψ⊤
ℓ¯Gt,ℓψℓ+ ψ⊤
ℓ¯Bt,ℓ
i
,"
REFERENCES,0.4946682464454976,"where ¯Gt,ℓand ¯Bt,ℓare defined by recursion in Section 3.1.
684"
REFERENCES,0.495260663507109,"Proof. We prove this result by induction. To reduce clutter, we let v = σ−2, and Λ1 = Σ−1
1 . We
685"
REFERENCES,0.4958530805687204,"start with the base case of the induction when ℓ= 1.
686"
REFERENCES,0.49644549763033174,"(I) Base case. Here we want to show that P (Ht | ψ∗,1 = ψ1) ∝exp
h
−1"
REFERENCES,0.49703791469194314,"2ψ⊤
1 ¯Gt,1ψ1 + ψ⊤
1 ¯Bt,1
i
,
687"
REFERENCES,0.4976303317535545,"where ¯Gt,1 and ¯Bt,1 are given in Eq. (20). First, we have that
688"
REFERENCES,0.4982227488151659,"P (Ht | ψ∗,1 = ψ1)
(i)
=
Y"
REFERENCES,0.49881516587677727,"i∈[K]
P (Ht,i | ψ∗,1 = ψ1) =
Y i∈[K] Z"
REFERENCES,0.4994075829383886,"θ
P (Ht,i, θ∗,i = θ | ψ∗,1 = ψ1) dθ , =
Y i∈[K] Z"
REFERENCES,0.5,"θ
P (Ht,i | θ∗,i = θ) N (θ; W1ψ1, Σ1) dθ , =
Y i∈[K] Z θ  Y"
REFERENCES,0.5005924170616114,"k∈St,i
N(Yk; X⊤
k θ, σ2)

N (θ; W1ψ1, Σ1) dθ"
REFERENCES,0.5011848341232228,"|
{z
}
hi(ψ1) , =
Y"
REFERENCES,0.5017772511848341,"i∈[K]
hi(ψ1) ,
(24)"
REFERENCES,0.5023696682464455,"where (i) follows from the fact that θ∗,i for i ∈[K] are conditionally independent given
689"
REFERENCES,0.5029620853080569,"ψ∗,1 = ψ1 and that given θ∗,i, Ht,i is independent of ψ∗,1.
Now we compute hi(ψ1) =
690
R"
REFERENCES,0.5035545023696683,"θ
Q
k∈St,i N(Yk; X⊤
k θ, σ2)

N (θ; W1ψ1, Σ1) dθ as
691"
REFERENCES,0.5041469194312796,"hi(ψ1) =
Z θ  Y"
REFERENCES,0.504739336492891,"k∈St,i
N(Yk; X⊤
k θ, σ2)

N(θ; W1ψ1, Σ1) dθ , ∝
Z"
REFERENCES,0.5053317535545023,"θ
exp
h
−1"
"V
X",0.5059241706161137,"2v
X"
"V
X",0.5065165876777251,"k∈St,i
(Yk −X⊤
k θ)2 −1"
"V
X",0.5071090047393365,"2(θ −W1ψ1)⊤Λ1(θ −W1ψ1)
i
dθ , =
Z"
"V
X",0.5077014218009479,"θ
exp
h
−1 2 
v
X"
"V
X",0.5082938388625592,"k∈St,i
(Y 2
k −2Ykθ⊤Xk + (θ⊤Xk)2) + θ⊤Λ1θ −2θ⊤Λ1W1ψ1"
"V
X",0.5088862559241706,"+ (W1ψ1)⊤Λ1(W1ψ1)
i
dθ , ∝
Z"
"V
X",0.509478672985782,"θ
exp
h
−1 2"
"V
X",0.5100710900473934,"
θ⊤
v
X"
"V
X",0.5106635071090048,"k∈St,i
XkX⊤
k + Λ1

θ −2θ⊤
v
X"
"V
X",0.5112559241706162,"k∈St,i
YkXk"
"V
X",0.5118483412322274,"+ Λ1W1ψ1

+ (W1ψ1)⊤Λ1(W1ψ1)
i
dθ ."
"V
X",0.5124407582938388,"But we know that ˆGt,i = v P
k∈St,i XkX⊤
k , and ˆGt,i ˆBt,i = v P
k∈St,i YkXk (because we assumed
692"
"V
X",0.5130331753554502,"linear-Gaussian likelihood). To further simplify expressions, we also let
693"
"V
X",0.5136255924170616,"V =
  ˆGt,i + Λ1
−1 ,
U = V −1 ,
β = V
  ˆGt,i ˆBt,i + Λ1W1ψ1

."
"V
X",0.514218009478673,"We have that UV = V U = Id , and thus
694"
"V
X",0.5148104265402843,"hi(ψ1) ∝
Z"
"V
X",0.5154028436018957,"θ
exp

−1 2"
"V
X",0.5159952606635071,"
θ⊤Uθ −2θ⊤UV

ˆGt,i ˆBt,i + Λ1W1ψ1

+ (W1ψ1)⊤Λ1(W1ψ1)

dθ , =
Z"
"V
X",0.5165876777251185,"θ
exp

−1"
"V
X",0.5171800947867299,"2
 
θ⊤Uθ −2θ⊤Uβ + (W1ψ1)⊤Λ1(W1ψ1)

dθ , =
Z"
"V
X",0.5177725118483413,"θ
exp

−1"
"V
X",0.5183649289099526,"2
 
(θ −β)⊤U(θ −β) −β⊤Uβ + (W1ψ1)⊤Λ1(W1ψ1)

dθ ,"
"V
X",0.518957345971564,"∝exp

−1"
"V
X",0.5195497630331753,"2
 
−β⊤Uβ + (W1ψ1)⊤Λ1(W1ψ1)

,"
"V
X",0.5201421800947867,"= exp

−1 2"
"V
X",0.5207345971563981,"
−

ˆGt,i ˆBt,i + Λ1W1ψ1
⊤
V

ˆGt,i ˆBt,i + Λ1W1ψ1

+ (W1ψ1)⊤Λ1(W1ψ1)

,"
"V
X",0.5213270142180095,"∝exp

−1 2"
"V
X",0.5219194312796208,"
ψ⊤
1 W⊤
1 (Λ1 −Λ1V Λ1) W1ψ1 −2ψ⊤
1

W⊤
1 Λ1V ˆGt,i ˆBt,i

,"
"V
X",0.5225118483412322,"= exp

−1"
"V
X",0.5231042654028436,"2ψ⊤
1 Ωiψ1 + ψ⊤
1 mi 
,"
"V
X",0.523696682464455,"where
695"
"V
X",0.5242890995260664,"Ωi = W⊤
1 (Λ1 −Λ1V Λ1) W1 = W⊤
1

Λ1 −Λ1( ˆGt,i + Λ1)−1Λ1

W1 ,"
"V
X",0.5248815165876777,"mi = W⊤
1 Λ1V ˆGt,i ˆBt,i = W⊤
1 Λ1( ˆGt,i + Λ1)−1 ˆGt,i ˆBt,i .
(25)"
"V
X",0.5254739336492891,"But notice that V = ( ˆGt,i + Λ1)−1 = ˆΣt,i and thus
696"
"V
X",0.5260663507109005,"Ωi = W⊤
1
 
Λ1 −Λ1 ˆΣt,iΛ1

W1 ,
mi = W⊤
1 Λ1 ˆΣt,i ˆGt,i ˆBt,i .
(26)"
"V
X",0.5266587677725119,"Finally, we plug this result in Eq. (24) to get
697"
"V
X",0.5272511848341233,"P (Ht | ψ∗,1 = ψ1) =
Y"
"V
X",0.5278436018957346,"i∈[K]
hi(ψ1) ∝
Y"
"V
X",0.5284360189573459,"i∈[K]
exp

−1"
"V
X",0.5290284360189573,"2ψ⊤
1 Ωiψ1 + ψ⊤
1 mi 
, = exp  −1"
"V
X",0.5296208530805687,"2ψ⊤
1
X"
"V
X",0.5302132701421801,"i∈[K]
Ωiψ1 + ψ⊤
1
X"
"V
X",0.5308056872037915,"i∈[K]
mi  ,"
"V
X",0.5313981042654028,"= exp

−1"
"V
X",0.5319905213270142,"2ψ⊤
1 ¯Gt,1ψ1 + ψ⊤
1 ¯Bt,1 
,"
"V
X",0.5325829383886256,"where
698"
"V
X",0.533175355450237,"¯Gt,1 = K
X"
"V
X",0.5337677725118484,"i=1
Ωi = K
X"
"V
X",0.5343601895734598,"i=1
W⊤
1
 
Λ1 −Λ1 ˆΣt,iΛ1

W1 = W⊤
1 K
X i=1"
"V
X",0.534952606635071," 
Σ−1
1
−Σ−1
1 ˆΣt,iΣ−1
1

W1 ,"
"V
X",0.5355450236966824,"¯Bt,1 = K
X"
"V
X",0.5361374407582938,"i=1
mi = K
X"
"V
X",0.5367298578199052,"i=1
ˆΣt,i ˆGt,i ˆBt,i = W⊤
1 Σ−1
1 K
X"
"V
X",0.5373222748815166,"i=1
ˆΣt,i ˆGt,i ˆBt,i ."
"V
X",0.5379146919431279,"This concludes the proof of the base case.
699"
"V
X",0.5385071090047393,"(II) Induction step. Let ℓ∈[L]/{1}. Suppose that
700"
"V
X",0.5390995260663507,"P (Ht | ψ∗,ℓ−1 = ψℓ−1) ∝exp

−1"
"V
X",0.5396919431279621,"2ψ⊤
ℓ−1 ¯Gt,ℓ−1ψℓ−1 + ψ⊤
ℓ−1 ¯Bt,ℓ−1"
"V
X",0.5402843601895735,"
.
(27)"
"V
X",0.5408767772511849,"Then we want to show that
701"
"V
X",0.5414691943127962,"P (Ht | ψ∗,ℓ= ψℓ) ∝exp

−1"
"V
X",0.5420616113744076,"2ψ⊤
ℓ¯Gt,ℓψℓ+ ψ⊤
ℓ¯Bt,ℓ 
,"
"V
X",0.542654028436019,"where
702"
"V
X",0.5432464454976303,"¯Gt,ℓ= W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ

Wℓ= W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ(Σ−1
ℓ
+ ¯Gt,ℓ−1)−1Σ−1
ℓ

Wℓ,
¯Bt,ℓ= W⊤
ℓΣ−1
ℓ
¯Σt,ℓ−1 ¯Bt,ℓ−1 = W⊤
ℓΣ−1
ℓ(Σ−1
ℓ
+ ¯Gt,ℓ−1)−1 ¯Bt,ℓ−1 ."
"V
X",0.5438388625592417,"To achieve this, we start by expressing P (Ht | ψ∗,ℓ= ψℓ) in terms of P (Ht | ψ∗,ℓ−1 = ψℓ−1) as
703"
"V
X",0.544431279620853,"P (Ht | ψ∗,ℓ= ψℓ) =
Z"
"V
X",0.5450236966824644,"ψℓ−1
P (Ht, ψ∗,ℓ−1 = ψℓ−1 | ψ∗,ℓ= ψℓ) dψℓ−1 , =
Z"
"V
X",0.5456161137440758,"ψℓ−1
P (Ht | ψ∗,ℓ−1 = ψℓ−1, ψ∗,ℓ= ψℓ) N(ψℓ−1; Wℓψℓ, Σℓ) dψℓ−1 , =
Z"
"V
X",0.5462085308056872,"ψℓ−1
P (Ht | ψ∗,ℓ−1 = ψℓ−1) N(ψℓ−1; Wℓψℓ, Σℓ) dψℓ−1 , ∝
Z"
"V
X",0.5468009478672986,"ψℓ−1
exp
h
−1"
"V
X",0.54739336492891,"2ψ⊤
ℓ−1 ¯Gt,ℓ−1ψℓ−1 + ψ⊤
ℓ−1 ¯Bt,ℓ−1
i
N(ψℓ−1; Wℓψℓ, Σℓ) dψℓ−1 , ∝
Z"
"V
X",0.5479857819905213,"ψℓ−1
exp
h
−1"
"V
X",0.5485781990521327,"2ψ⊤
ℓ−1 ¯Gt,ℓ−1ψℓ−1 + ψ⊤
ℓ−1 ¯Bt,ℓ−1"
"V
X",0.5491706161137441,"+ (ψℓ−1 −Wℓψℓ)⊤Λℓ(ψℓ−1 −Wℓψℓ)
i
dψℓ−1 ."
"V
X",0.5497630331753555,"Now let S = ¯Gt,ℓ−1 + Λℓand V = ¯Bt,ℓ−1 + ΛℓWℓψℓ. Then we have that,
704"
"V
X",0.5503554502369669,"P (Ht | ψ∗,ℓ= ψℓ) ∝
Z"
"V
X",0.5509478672985783,"ψℓ−1
exp
h
−1"
"V
X",0.5515402843601895,"2ψ⊤
ℓ−1 ¯Gt,ℓ−1ψℓ−1 + ψ⊤
ℓ−1 ¯Bt,ℓ−1"
"V
X",0.5521327014218009,"+ (ψℓ−1 −Wℓψℓ)⊤Λℓ(ψℓ−1 −Wℓψℓ)
i
dψℓ−1 , ∝
Z"
"V
X",0.5527251184834123,"ψℓ−1
exp
h
−1 2"
"V
X",0.5533175355450237,"
ψ⊤
ℓ−1Sψℓ−1 −2ψ⊤
ℓ−1
  ¯Bt,ℓ−1 + ΛℓWℓψℓ

+ ψ⊤
ℓW⊤
ℓΛℓWℓψℓ
i
dψℓ−1 , =
Z"
"V
X",0.5539099526066351,"ψℓ−1
exp
h
−1 2"
"V
X",0.5545023696682464,"
ψ⊤
ℓ−1S(ψℓ−1 −2S−1V ) + ψ⊤
ℓW⊤
ℓΛℓWℓψℓ
i
dψℓ−1 , =
Z"
"V
X",0.5550947867298578,"ψℓ−1
exp
h
−1 2"
"V
X",0.5556872037914692,"
(ψℓ−1 −S−1V )⊤S(ψℓ−1 −S−1V )"
"V
X",0.5562796208530806,"+ ψ⊤
ℓW⊤
ℓΛℓWℓψℓ−V ⊤S−1V
i
dψℓ−1."
"V
X",0.556872037914692,"In the second step, we omit constants in ψℓand ψℓ−1. Thus
705"
"V
X",0.5574644549763034,"P (Ht | ψ∗,ℓ= ψℓ) ∝
Z"
"V
X",0.5580568720379147,"ψℓ−1
exp

−1"
"V
X",0.558649289099526,"2
 
(ψℓ−1 −S−1V )⊤S(ψℓ−1 −S−1V ) + ψ⊤
ℓW⊤
ℓΛℓWℓψℓ−V ⊤S−1V

dψℓ−1,"
"V
X",0.5592417061611374,"∝exp

−1"
"V
X",0.5598341232227488,"2
 
ψ⊤
ℓW⊤
ℓΛℓWℓψℓ−V ⊤S−1V

."
"V
X",0.5604265402843602,"It follows that
706"
"V
X",0.5610189573459715,"P (Ht | ψ∗,ℓ= ψℓ)"
"V
X",0.5616113744075829,"∝exp

−1"
"V
X",0.5622037914691943,"2
 
ψ⊤
ℓW⊤
ℓΛℓWℓψℓ−V ⊤S−1V

,"
"V
X",0.5627962085308057,"= exp

−1 2"
"V
X",0.5633886255924171,"
ψ⊤
ℓW⊤
ℓΛℓWℓψℓ−
  ¯Bt,ℓ−1 + ΛℓWℓψℓ
⊤S−1   ¯Bt,ℓ−1 + ΛℓWℓψℓ
"
"V
X",0.5639810426540285,"∝exp

−1"
"V
X",0.5645734597156398,"2
 
ψ⊤
ℓ
 
W⊤
ℓΛℓWℓ−W⊤
ℓΛℓS−1ΛℓWℓ

ψℓ−2ψ⊤
ℓW⊤
ℓΛℓS−1 ¯Bt,ℓ−1

,"
"V
X",0.5651658767772512,"= exp

−1"
"V
X",0.5657582938388626,"2ψ⊤
ℓ¯Gt,ℓψℓ+ ψ⊤
ℓ¯Bt,ℓ 
."
"V
X",0.566350710900474,"In the last step, we omit constants in ψℓand we set
707"
"V
X",0.5669431279620853,"¯Gt,ℓ= W⊤
ℓ
 
Λℓ−ΛℓS−1Λℓ

Wℓ= W⊤
ℓ
 
Λℓ−Λℓ(Λℓ+ ¯Gt,ℓ−1)−1Σ−1
ℓΛℓ

Wℓ,
¯Bt,ℓ= W⊤
ℓΛℓS−1 ¯Bt,ℓ−1 = W⊤
ℓΛℓ(Λℓ+ ¯Gt,ℓ−1)−1 ¯Bt,ℓ−1 ."
"V
X",0.5675355450236966,"This completes the proof.
708"
"V
X",0.568127962085308,"Similarly, this same proof applies when the reward distribution is not linear-Gaussian, with the
709"
"V
X",0.5687203791469194,"approximation P (Ht,i | θ∗,i = θ) ≈N
 
θ; ˆBt,i, ˆG−1
t,i

. Using this approximation in the derivations
710"
"V
X",0.5693127962085308,"above leads to the same results.
711"
"V
X",0.5699052132701422,"C
Posterior derivations for non-linear diffusion models
712"
"V
X",0.5704976303317536,"After deriving the posteriors for linear score functions fℓ, we now get back to the general case in (1),
713"
"V
X",0.5710900473933649,"where the score functions are potentially non-linear. Approximation is needed since both the score
714"
"V
X",0.5716824644549763,"functions and rewards can be non-linear. To avoid any computational challenges, we use a simple
715"
"V
X",0.5722748815165877,"and intuitive approximation, where all posteriors Pt,i and Qt,ℓare approximated by the Gaussian
716"
"V
X",0.5728672985781991,"distributions in Appendix B.1, with few changes. First, the terms Wℓψℓin (18) are replaced by fℓ(ψℓ).
717"
"V
X",0.5734597156398105,"This accounts for the fact that the prior mean is now fℓ(ψℓ) rather than Wℓψℓ, and this is the main
718"
"V
X",0.5740521327014217,"difference between the linear diffusion model in (15) and the general, potentially non-linear, diffusion
719"
"V
X",0.5746445497630331,"model in (1). Second, the matrix multiplications that involve the matrices Wℓin (20) and (21) are
720"
"V
X",0.5752369668246445,"simply removed. Despite being simple, this approximation is efficient and avoids the computational
721"
"V
X",0.5758293838862559,"burden of heavy approximate sampling algorithms required for each latent parameter. This is why
722"
"V
X",0.5764218009478673,"deriving the exact posterior for linear score functions was key beyond enabling theoretical analyses.
723"
"V
X",0.5770142180094787,"Moreover, this approximation retains some key attributes of exact posteriors. Specifically, in the
724"
"V
X",0.57760663507109,"absence of data, it recovers precisely the prior in (1), and as more data is accumulated, the influence
725"
"V
X",0.5781990521327014,"of the prior diminishes.
726"
"V
X",0.5787914691943128,"D
Regret proof and additional discussions
727"
"V
X",0.5793838862559242,"D.1
Sketch of the proof
728"
"V
X",0.5799763033175356,"We start with the following standard lemma upon which we build our analysis [Aouali et al., 2023b].
729"
"V
X",0.580568720379147,"Lemma D.1. Assume that P (θ∗,i = θ | Ht) = N(θ; ˇµt,i, ˇΣt,i) for any i ∈[K], then for any δ ∈
730"
"V
X",0.5811611374407583,"(0, 1),
731"
"V
X",0.5817535545023697,"BR(n) ≤
p"
"V
X",0.582345971563981,"2n log(1/δ)
r"
"V
X",0.5829383886255924,"E
hPn
t=1 ∥Xt∥2
ˇΣt,At"
"V
X",0.5835308056872038,"i
+ cnδ ,
where c > 0 is a constant .
(28)"
"V
X",0.5841232227488151,"Applying Lemma D.1 requires proving that the marginal action-posteriors P (θ∗,i = θ | Ht) in Eq. (3)
732"
"V
X",0.5847156398104265,"are Gaussian and computing their covariances, while we only know the conditional action-posteriors
733"
"V
X",0.5853080568720379,"Pt,i and latent-posteriors Qt,ℓ. This is achieved by leveraging the preservation properties of the
734"
"V
X",0.5859004739336493,"family of Gaussian distributions [Koller and Friedman, 2009] and the total covariance decomposition
735"
"V
X",0.5864928909952607,"[Weiss, 2005] which leads to the next lemma.
736"
"V
X",0.5870853080568721,"Lemma D.2. Let t ∈[n] and i ∈[K], then the marginal covariance matrix ˇΣt,i reads
737"
"V
X",0.5876777251184834,"ˇΣt,i = ˆΣt,i + P
ℓ∈[L] Pi,ℓ¯Σt,ℓP⊤
i,ℓ,
where Pi,ℓ= ˆΣt,iΣ−1
1 W1
Qℓ−1
k=1 ¯Σt,kΣ−1
k+1Wk+1.
(29)"
"V
X",0.5882701421800948,"The marginal covariance matrix ˇΣt,i in Eq. (29) decomposes into L + 1 terms. The first term
738"
"V
X",0.5888625592417062,"corresponds to the posterior uncertainty of θ∗,i | ψ∗,1. The remaining L terms capture the posterior
739"
"V
X",0.5894549763033176,"uncertainties of ψ∗,L and ψ∗,ℓ−1 | ψ∗,ℓfor ℓ∈[L]/{1}. These are then used to quantify the posterior
740"
"V
X",0.590047393364929,"information gain of latent parameters after one round as follows.
741"
"V
X",0.5906398104265402,"Lemma D.3 (Posterior information gain). Let t ∈[n] and ℓ∈[L], then
742"
"V
X",0.5912322274881516,"¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓ⪰σ−2σ−2ℓ
MAXP⊤
At,ℓXtX⊤
t PAt,ℓ,
where σ2
MAX = maxℓ∈[L+1] 1 + σ2
ℓ
σ2 .
(30)"
"V
X",0.591824644549763,"Finally, Lemma D.2 is used to decompose ∥Xt∥2
ˇΣt,At in Eq. (28) into L + 1 terms. Each term is
743"
"V
X",0.5924170616113744,"bounded thanks to Lemma D.3. This results in the Bayes regret bound in Theorem 4.1.
744"
"V
X",0.5930094786729858,"D.2
Technical contributions
745"
"V
X",0.5936018957345972,"Our main technical contributions are the following.
746"
"V
X",0.5941943127962085,"Lemma D.2. In dTS, sampling is done hierarchically, meaning the marginal posterior distribution of
747"
"V
X",0.5947867298578199,"θ∗,i|Ht is not explicitly defined. Instead, we use the conditional posterior distribution of θ∗,i|Ht, ψ∗,1.
748"
"V
X",0.5953791469194313,"The first contribution was deriving θ∗,i|Ht using the total covariance decomposition combined with
749"
"V
X",0.5959715639810427,"an induction proof, as our posteriors in Section 3.1 were derived recursively. Unlike in Bayes
750"
"V
X",0.5965639810426541,"regret analysis for standard Thompson sampling, where the posterior distribution of θ∗,i|Ht is
751"
"V
X",0.5971563981042654,"predetermined due to the absence of latent parameters, our method necessitates this recursive total
752"
"V
X",0.5977488151658767,"covariance decomposition, marking a first difference from the standard Bayesian proofs of Thompson
753"
"V
X",0.5983412322274881,"sampling. Note that HierTS, which is developed for multi-task linear bandits, also employs total
754"
"V
X",0.5989336492890995,"covariance decomposition, but it does so under the assumption of a single latent parameter; on which
755"
"V
X",0.5995260663507109,"action parameters are centered. Our extension significantly differs as it is tailored for contextual
756"
"V
X",0.6001184834123223,"bandits with multiple, successive levels of latent parameters, moving away from HierTS’s assumption
757"
"V
X",0.6007109004739336,"of a 1-level structure. Roughly speaking, HierTS when applied to contextual would consider a single-
758"
"V
X",0.601303317535545,"level hierarchy, where θ∗,i|ψ∗,1 ∼N(ψ∗,1, Σ1) with L = 1. In contrast, our model proposes a
759"
"V
X",0.6018957345971564,"multi-level hierarchy, where the first level is θ∗,i|ψ∗,1 ∼N(W1ψ∗,1, Σ1). This also introduces a new
760"
"V
X",0.6024881516587678,"aspect to our approach – the use of a linear function W1ψ∗,1, as opposed to HierTS’s assumption
761"
"V
X",0.6030805687203792,"where action parameters are centered directly on the latent parameter. Thus, while HierTS also
762"
"V
X",0.6036729857819905,"uses the total covariance decomposition, our generalize it to multi-level hierarchies under L linear
763"
"V
X",0.6042654028436019,"functions Wℓψ∗,ℓ, instead of a single-level hierarchy under a single identity function ψ∗,1.
764"
"V
X",0.6048578199052133,"Lemma D.3. In Bayes regret proofs for standard Thompson sampling, we often quantify the posterior
765"
"V
X",0.6054502369668247,"information gain. This is achieved by monitoring the increase in posterior precision for the action
766"
"V
X",0.606042654028436,"taken At in each round t ∈[n]. However, in dTS, our analysis extends beyond this. We not only
767"
"V
X",0.6066350710900474,"quantify the posterior information gain for the taken action but also for every latent parameter, since
768"
"V
X",0.6072274881516587,"they are also learned. This lemma addresses this aspect. To elaborate, we use the recursive formulas
769"
"V
X",0.6078199052132701,"in Section 3.1 that connect the posterior covariance of each latent parameter ψ∗,ℓwith the covariance
770"
"V
X",0.6084123222748815,"of the posterior action parameters θ∗,i. This allows us to propagate the information gain associated
771"
"V
X",0.6090047393364929,"with the action taken in round At to all latent parameters ψ∗,ℓ, for ℓ∈[L] by induction. This is a
772"
"V
X",0.6095971563981043,"novel contribution, as it is not a feature of Bayes regret analyses in standard Thompson sampling.
773"
"V
X",0.6101895734597157,"Proposition 4.2. Building upon the insights of Theorem 4.1, we introduce the sparsity assumption
774"
"V
X",0.610781990521327,"(A3). Under this assumption, we demonstrate that the Bayes regret outlined in Theorem 4.1 can be
775"
"V
X",0.6113744075829384,"significantly refined. Specifically, the regret becomes contingent on dimensions dℓ≤d, as opposed
776"
"V
X",0.6119668246445498,"to relying on the entire dimension d. This sparsity assumption is both a novel and a key technical
777"
"V
X",0.6125592417061612,"contribution to our work. Its underlying principle is straightforward: the Bayes regret is influenced
778"
"V
X",0.6131516587677726,"by the quantity of parameters that require learning. With the sparsity assumption, this number is
779"
"V
X",0.6137440758293838,"reduced to less than d for each latent parameter. To substantiate this claim, we revisit the proof of
780"
"V
X",0.6143364928909952,"Theorem 4.1 and modify a crucial equality. This adjustment results in a more precise representation by
781"
"V
X",0.6149289099526066,"partitioning the covariance matrix of each latent parameter ψ∗,ℓinto blocks. These blocks comprise
782"
"V
X",0.615521327014218,"a dℓ× dℓsegment corresponding to the learnable dℓparameters of ψ∗,ℓ, and another block of size
783"
"V
X",0.6161137440758294,"(d −dℓ) × (d −dℓ) that does not necessitate learning. This decomposition allows us to conclude that
784"
"V
X",0.6167061611374408,"the final regret is solely dependent on dℓ, marking a significant refinement from the original theorem.
785"
"V
X",0.6172985781990521,"D.3
Proof of lemma D.2
786"
"V
X",0.6178909952606635,"In this proof, we heavily rely on the total covariance decomposition [Weiss, 2005]. Also, refer to
787"
"V
X",0.6184834123222749,"[Hong et al., 2022b, Section 5.2] for a brief introduction to this decomposition. Now, from Eq. (17),
788"
"V
X",0.6190758293838863,"we have that
789"
"V
X",0.6196682464454977,"cov [θ∗,i | Ht, ψ∗,1] = ˆΣt,i =

ˆGt,i + Σ−1
1
−1
,"
"V
X",0.620260663507109,"E [θ∗,i | Ht, ψ∗,1] = ˆµt,i = ˆΣt,i

ˆGt,i ˆBt,i + Σ−1
1 W1ψ∗,1

."
"V
X",0.6208530805687204,"First, given Ht, cov [θ∗,i | Ht, ψ∗,1] =

ˆGt,i + Σ−1
1
−1
is constant. Thus
790"
"V
X",0.6214454976303317,"E [cov [θ∗,i | Ht, ψ∗,1] | Ht] = cov [θ∗,i | Ht, ψ∗,1] =

ˆGt,i + Σ−1
1
−1
= ˆΣt,i ."
"V
X",0.6220379146919431,"In addition, given Ht, ˆΣt,i, ˆGt,i and ˆBt,i are constant. Thus
791"
"V
X",0.6226303317535545,"cov [E [θ∗,i | Ht, ψ∗,1] | Ht] = cov
h
ˆΣt,i

ˆGt,i ˆBt,i + Σ−1
1 W1ψ∗,1
  Ht
i
,"
"V
X",0.6232227488151659,"= cov
h
ˆΣt,iΣ−1
1 W1ψ∗,1
 Ht
i
,"
"V
X",0.6238151658767772,"= ˆΣt,iΣ−1
1 W1cov [ψ∗,1 | Ht] W⊤
1 Σ−1
1 ˆΣt,i ,"
"V
X",0.6244075829383886,"= ˆΣt,iΣ−1
1 W1 ¯¯Σt,1W⊤
1 Σ−1
1 ˆΣt,i ,"
"V
X",0.625,"where ¯¯Σt,1 = cov [ψ∗,1 | Ht] is the marginal posterior covariance of ψ∗,1. Finally, the total covariance
792"
"V
X",0.6255924170616114,"decomposition [Weiss, 2005, Hong et al., 2022b] yields that
793"
"V
X",0.6261848341232228,"ˇΣt,i = cov [θ∗,i | Ht] = E [cov [θ∗,i | Ht, ψ∗,1] | Ht] + cov [E [θ∗,i | Ht, ψ∗,1] | Ht] ,"
"V
X",0.6267772511848341,"= ˆΣt,i + ˆΣt,iΣ−1
1 W1 ¯¯Σt,1W⊤
1 Σ−1
1 ˆΣt,i ,
(31)"
"V
X",0.6273696682464455,"However, ¯¯Σt,1 = cov [ψ∗,1 | Ht] is different from ¯Σt,1 = cov [ψ∗,1 | Ht, ψ∗,2] that we already derived
794"
"V
X",0.6279620853080569,"in Eq. (18). Thus we do not know the expression of ¯¯Σt,1. But we can use the same total covariance
795"
"V
X",0.6285545023696683,"decomposition trick to find it. Precisely, let ¯¯Σt,ℓ= cov [ψ∗,ℓ| Ht] for any ℓ∈[L]. Then we have that
796"
"V
X",0.6291469194312796,"¯Σt,1 = cov [ψ∗,1 | Ht, ψ∗,2] =
 
Σ−1
2
+ ¯Gt,1
−1 ,"
"V
X",0.629739336492891,"¯µt,1 = E [ψ∗,1 | Ht, ψ∗,2] = ¯Σt,1

Σ−1
2 W2ψ∗,2 + ¯Bt,1

."
"V
X",0.6303317535545023,"First, given Ht, cov [ψ∗,1 | Ht, ψ∗,2] =
 
Σ−1
2
+ ¯Gt,1
−1 is constant. Thus
797"
"V
X",0.6309241706161137,"E [cov [ψ∗,1 | Ht, ψ∗,2] | Ht] = cov [ψ∗,1 | Ht, ψ∗,2] = ¯Σt,1 ."
"V
X",0.6315165876777251,"In addition, given Ht, ¯Σt,1, ˜Σt,1 and ¯Bt,1 are constant. Thus
798"
"V
X",0.6321090047393365,"cov [E [ψ∗,1 | Ht, ψ∗,2] | Ht] = cov
h
¯Σt,1

Σ−1
2 W2ψ∗,2 + ¯Bt,1
  Ht
i
,"
"V
X",0.6327014218009479,"= cov
¯Σt,1Σ−1
2 W2ψ∗,2
 Ht

,"
"V
X",0.6332938388625592,"= ¯Σt,1Σ−1
2 W2cov [ψ∗,2 | Ht] W⊤
2 Σ−1
2 ¯Σt,1 ,"
"V
X",0.6338862559241706,"= ¯Σt,1Σ−1
2 W2 ¯¯Σt,2W⊤
2 Σ−1
2 ¯Σt,1 ."
"V
X",0.634478672985782,"Finally, total covariance decomposition [Weiss, 2005, Hong et al., 2022b] leads to
799"
"V
X",0.6350710900473934,"¯¯Σt,1 = cov [ψ∗,1 | Ht] = E [cov [ψ∗,1 | Ht, ψ∗,2] | Ht] + cov [E [ψ∗,1 | Ht, ψ∗,2] | Ht] ,"
"V
X",0.6356635071090048,"= ¯Σt,1 + ¯Σt,1Σ−1
2 W2 ¯¯Σt,2W⊤
2 Σ−1
2 ¯Σt,1 ."
"V
X",0.6362559241706162,"Now using the techniques, this can be generalized using the same technique as above to
800"
"V
X",0.6368483412322274,"¯¯Σt,ℓ= ¯Σt,ℓ+ ¯Σt,ℓΣ−1
ℓ+1Wℓ+1 ¯¯Σt,ℓ+1W⊤
ℓ+1Σ−1
ℓ+1 ¯Σt,ℓ,
∀ℓ∈[L −1] ."
"V
X",0.6374407582938388,"Then, by induction, we get that
801"
"V
X",0.6380331753554502,"¯¯Σt,1 =
X"
"V
X",0.6386255924170616,"ℓ∈[L]
¯Pℓ¯Σt,ℓ¯P⊤
ℓ,
∀ℓ∈[L −1] ,"
"V
X",0.639218009478673,"where we use that by definition ¯¯Σt,L = cov [ψ∗,L | Ht] = ¯Σt,L and set ¯P1 = Id and ¯Pℓ=
802
Qℓ−1
k=1 ¯Σt,kΣ−1
k+1Wk+1 for any ℓ∈[L]/{1}. Plugging this in Eq. (31) leads to
803"
"V
X",0.6398104265402843,"ˇΣt,i = ˆΣt,i +
X"
"V
X",0.6404028436018957,"ℓ∈[L]
ˆΣt,iΣ−1
1 W1¯Pℓ¯Σt,ℓ¯P⊤
ℓW⊤
1 Σ−1
1 ˆΣt,i ,"
"V
X",0.6409952606635071,"= ˆΣt,i +
X"
"V
X",0.6415876777251185,"ℓ∈[L]
ˆΣt,iΣ−1
1 W1¯Pℓ¯Σt,ℓ(ˆΣt,iΣ−1
1 W1)⊤,"
"V
X",0.6421800947867299,"= ˆΣt,i +
X"
"V
X",0.6427725118483413,"ℓ∈[L]
Pi,ℓ¯Σt,ℓP⊤
i,ℓ,"
"V
X",0.6433649289099526,"where Pi,ℓ= ˆΣt,iΣ−1
1 W1¯Pℓ= ˆΣt,iΣ−1
1 W1
Qℓ−1
k=1 ¯Σt,kΣ−1
k+1Wk+1.
804"
"V
X",0.643957345971564,"D.4
Proof of lemma D.3
805"
"V
X",0.6445497630331753,"We prove this result by induction. We start with the base case when ℓ= 1.
806"
"V
X",0.6451421800947867,(I) Base case. Let u = σ−1 ˆΣ
"V
X",0.6457345971563981,"1
2
t,AtXt From the expression of ¯Σt,1 in Eq. (18), we have that
807"
"V
X",0.6463270142180095,"¯Σ−1
t+1,1 −¯Σ−1
t,1 = W⊤
1

Σ−1
1
−Σ−1
1 (ˆΣ−1
t,At + σ−2XtX⊤
t )−1Σ−1
1
−(Σ−1
1
−Σ−1
1 ˆΣt,AtΣ−1
1 )

W1 ,"
"V
X",0.6469194312796208,"= W⊤
1

Σ−1
1 (ˆΣt,At −(ˆΣ−1
t,At + σ−2XtX⊤
t )−1)Σ−1
1

W1 ,"
"V
X",0.6475118483412322,"= W⊤
1

Σ−1
1 ˆΣ"
"V
X",0.6481042654028436,"1
2
t,At(Id −(Id + σ−2 ˆΣ"
"V
X",0.648696682464455,"1
2
t,AtXtX⊤
t ˆΣ"
"V
X",0.6492890995260664,"1
2
t,At)−1)ˆΣ"
"V
X",0.6498815165876777,"1
2
t,AtΣ−1
1

W1 ,"
"V
X",0.6504739336492891,"= W⊤
1

Σ−1
1 ˆΣ"
"V
X",0.6510663507109005,"1
2
t,At(Id −(Id + uu⊤)−1)ˆΣ"
"V
X",0.6516587677725119,"1
2
t,AtΣ−1
1

W1 ,"
"V
X",0.6522511848341233,"(i)
= W⊤
1"
"V
X",0.6528436018957346,"
Σ−1
1 ˆΣ"
"V
X",0.6534360189573459,"1
2
t,At
uu⊤"
"V
X",0.6540284360189573,"1 + u⊤u
ˆΣ"
"V
X",0.6546208530805687,"1
2
t,AtΣ−1
1"
"V
X",0.6552132701421801,"
W1 ,"
"V
X",0.6558056872037915,"(ii)
= σ−2W⊤
1 Σ−1
1 ˆΣt,At
XtX⊤
t
1 + u⊤u
ˆΣt,AtΣ−1
1 W1 .
(32)"
"V
X",0.6563981042654028,"In (i) we use the Sherman-Morrison formula. Note that (ii) says that ¯Σ−1
t+1,1 −¯Σ−1
t,1 is one-rank
808"
"V
X",0.6569905213270142,"which we will also need in induction step. Now, we have that ∥Xt∥2 = 1. Therefore,
809"
"V
X",0.6575829383886256,"1 + u⊤u = 1 + σ−2X⊤
t ˆΣt,AtXt ≤1 + σ−2λ1(Σ1)∥Xt∥2 = 1 + σ−2σ2
1 ≤σ2
MAX ,"
"V
X",0.658175355450237,"where we use that by definition of σ2
MAX in Lemma D.3, we have that σ2
MAX ≥1 + σ−2σ2
1. Therefore,
810"
"V
X",0.6587677725118484,"by taking the inverse, we get that
1
1+u⊤u ≥σ−2
MAX. Combining this with Eq. (32) leads to
811"
"V
X",0.6593601895734598,"¯Σ−1
t+1,1 −¯Σ−1
t,1 ⪰σ−2σ−2
MAXW⊤
1 Σ−1
1 ˆΣt,AtXtX⊤
t ˆΣt,AtΣ−1
1 W1"
"V
X",0.659952606635071,"Noticing that PAt,1 = ˆΣt,AtΣ−1
1 W1 concludes the proof of the base case when ℓ= 1.
812"
"V
X",0.6605450236966824,"(II) Induction step. Let ℓ∈[L]/{1} and suppose that ¯Σ−1
t+1,ℓ−1 −¯Σ−1
t,ℓ−1 is one-rank and that it
813"
"V
X",0.6611374407582938,"holds for ℓ−1 that
814"
"V
X",0.6617298578199052,"¯Σ−1
t+1,ℓ−1 −¯Σ−1
t,ℓ−1 ⪰σ−2σ−2(ℓ−1)
MAX
P⊤
At,ℓ−1XtX⊤
t PAt,ℓ−1 ,
where σ−2
MAX = max
ℓ∈[L] 1 + σ−2σ2
ℓ."
"V
X",0.6623222748815166,"Then, we want to show that ¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓis also one-rank and that it holds that
815"
"V
X",0.6629146919431279,"¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓ⪰σ−2σ−2ℓ
MAXP⊤
At,ℓXtX⊤
t PAt,ℓ,
where σ−2
MAX = max
ℓ∈[L] 1 + σ−2σ2
ℓ."
"V
X",0.6635071090047393,"This is achieved as follows. First, we notice that by the induction hypothesis, we have that ˜Σ−1
t+1,ℓ−1 −
816"
"V
X",0.6640995260663507,"¯Gt,ℓ−1 = ¯Σ−1
t+1,ℓ−1 −¯Σ−1
t,ℓ−1 is one-rank. In addition, the matrix is positive semi-definite. Thus we
817"
"V
X",0.6646919431279621,"can write it as ˜Σ−1
t+1,ℓ−1 −¯Gt,ℓ−1 = uu⊤where u ∈Rd. Then, similarly to the base case, we have
818"
"V
X",0.6652843601895735,"¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓ= ˜Σ−1
t+1,ℓ−˜Σ−1
t,ℓ,"
"V
X",0.6658767772511849,"= W⊤
ℓ
 
Σℓ+ ˜Σt+1,ℓ−1
−1Wℓ−W⊤
ℓ
 
Σℓ+ ˜Σt,ℓ−1
−1Wℓ,"
"V
X",0.6664691943127962,"= W⊤
ℓ
h 
Σℓ+ ˜Σt+1,ℓ−1
−1 −
 
Σℓ+ ˜Σt,ℓ−1
−1i
Wℓ,"
"V
X",0.6670616113744076,"= W⊤
ℓΣ−1
ℓ
h 
Σ−1
ℓ
+ ¯Gt,ℓ−1
−1 −
 
Σ−1
ℓ
+ ˜Σ−1
t+1,ℓ−1
−1i
Σ−1
ℓWℓ,"
"V
X",0.667654028436019,"= W⊤
ℓΣ−1
ℓ
h 
Σ−1
ℓ
+ ¯Gt,ℓ−1
−1 −
 
Σ−1
ℓ
+ ¯Gt,ℓ−1 + ˜Σ−1
t+1,ℓ−1 −¯Gt,ℓ−1
−1i
Σ−1
ℓWℓ,"
"V
X",0.6682464454976303,"= W⊤
ℓΣ−1
ℓ
h 
Σ−1
ℓ
+ ¯Gt,ℓ−1
−1 −
 
Σ−1
ℓ
+ ¯Gt,ℓ−1 + uu⊤−1i
Σ−1
ℓWℓ,"
"V
X",0.6688388625592417,"= W⊤
ℓΣ−1
ℓ
h
¯Σt,ℓ−1 −
 ¯Σ−1
t,ℓ−1 + uu⊤−1i
Σ−1
ℓWℓ,"
"V
X",0.669431279620853,"= W⊤
ℓΣ−1
ℓ
h
¯Σt,ℓ−1
uu⊤"
"V
X",0.6700236966824644,"1 + u⊤¯Σt,ℓ−1u
¯Σt,ℓ−1
i
Σ−1
ℓWℓ,"
"V
X",0.6706161137440758,"= W⊤
ℓΣ−1
ℓ
¯Σt,ℓ−1
uu⊤"
"V
X",0.6712085308056872,"1 + u⊤¯Σt,ℓ−1u
¯Σt,ℓ−1Σ−1
ℓWℓ"
"V
X",0.6718009478672986,"However, we it follows from the induction hypothesis that uu⊤= ˜Σ−1
t+1,ℓ−1 −¯Gt,ℓ−1 = ¯Σ−1
t+1,ℓ−1 −
819"
"V
X",0.67239336492891,"¯Σ−1
t,ℓ−1 ⪰σ−2σ−2(ℓ−1)
MAX
P⊤
At,ℓ−1XtX⊤
t PAt,ℓ−1. Therefore,
820"
"V
X",0.6729857819905213,"¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓ= W⊤
ℓΣ−1
ℓ
¯Σt,ℓ−1
uu⊤"
"V
X",0.6735781990521327,"1 + u⊤¯Σt,ℓ−1u
¯Σt,ℓ−1Σ−1
ℓWℓ,"
"V
X",0.6741706161137441,"⪰W⊤
ℓΣ−1
ℓ
¯Σt,ℓ−1
σ−2σ−2(ℓ−1)
MAX
P⊤
At,ℓ−1XtX⊤
t PAt,ℓ−1
1 + u⊤¯Σt,ℓ−1u
¯Σt,ℓ−1Σ−1
ℓWℓ,"
"V
X",0.6747630331753555,"=
σ−2σ−2(ℓ−1)
MAX
1 + u⊤¯Σt,ℓ−1uW⊤
ℓΣ−1
ℓ
¯Σt,ℓ−1P⊤
At,ℓ−1XtX⊤
t PAt,ℓ−1 ¯Σt,ℓ−1Σ−1
ℓWℓ,"
"V
X",0.6753554502369669,"=
σ−2σ−2(ℓ−1)
MAX
1 + u⊤¯Σt,ℓ−1uP⊤
At,ℓXtX⊤
t PAt,ℓ."
"V
X",0.6759478672985783,"Finally, we use that 1 + u⊤¯Σt,ℓ−1u ≤1 + ∥u∥2λ1(¯Σt,ℓ−1) ≤1 + σ−2σ2
ℓ. Here we use that
821"
"V
X",0.6765402843601895,"∥u∥2 ≤σ−2, which can also be proven by induction, and that λ1(¯Σt,ℓ−1) ≤σ2
ℓ, which follows from
822"
"V
X",0.6771327014218009,"the expression of ¯Σt,ℓ−1 in Section 3.1. Therefore, we have that
823"
"V
X",0.6777251184834123,"¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓ⪰
σ−2σ−2(ℓ−1)
MAX
1 + u⊤¯Σt,ℓ−1uP⊤
At,ℓXtX⊤
t PAt,ℓ,"
"V
X",0.6783175355450237,"⪰σ−2σ−2(ℓ−1)
MAX
1 + σ−2σ2
ℓ
P⊤
At,ℓXtX⊤
t PAt,ℓ,"
"V
X",0.6789099526066351,"⪰σ−2σ−2ℓ
MAXP⊤
At,ℓXtX⊤
t PAt,ℓ,"
"V
X",0.6795023696682464,"where the last inequality follows from the definition of σ2
MAX = maxℓ∈[L] 1 + σ−2σ2
ℓ. This concludes
824"
"V
X",0.6800947867298578,"the proof.
825"
"V
X",0.6806872037914692,"D.5
Proof of theorem 4.1
826"
"V
X",0.6812796208530806,"We start with the following standard result which we borrow from [Hong et al., 2022a, Aouali et al.,
827"
"V
X",0.681872037914692,"2023b],
828"
"V
X",0.6824644549763034,"BR(n) ≤
p"
"V
X",0.6830568720379147,2n log(1/δ)
"V
X",0.683649289099526,"v
u
u
tE "" n
X"
"V
X",0.6842417061611374,"t=1
∥Xt∥2
ˇΣt,At #"
"V
X",0.6848341232227488,"+ cnδ ,
where c > 0 is a constant .
(33)"
"V
X",0.6854265402843602,"Then we use Lemma D.2 and express the marginal covariance ˇΣt,At as
829"
"V
X",0.6860189573459715,"ˇΣt,i = ˆΣt,i +
X"
"V
X",0.6866113744075829,"ℓ∈[L]
Pi,ℓ¯Σt,ℓP⊤
i,ℓ,
where Pi,ℓ= ˆΣt,iΣ−1
1 W1 ℓ−1
Y"
"V
X",0.6872037914691943,"k=1
¯Σt,kΣ−1
k+1Wk+1.
(34)"
"V
X",0.6877962085308057,"Therefore, we can decompose ∥Xt∥2
ˇΣt,At as
830"
"V
X",0.6883886255924171,"∥Xt∥2
ˇΣt,At = σ2 X⊤
t ˇΣt,AtXt σ2"
"V
X",0.6889810426540285,"(i)
= σ2 
σ−2X⊤
t ˆΣt,AtXt + σ−2 X"
"V
X",0.6895734597156398,"ℓ∈[L]
X⊤
t PAt,ℓ¯Σt,ℓP⊤
At,ℓXt

,"
"V
X",0.6901658767772512,"(ii)
≤c0 log(1 + σ−2X⊤
t ˆΣt,AtXt) +
X"
"V
X",0.6907582938388626,"ℓ∈[L]
cℓlog(1 + σ−2X⊤
t PAt,ℓ¯Σt,ℓP⊤
At,ℓXt) ,
(35)"
"V
X",0.691350710900474,"where (i) follows from Eq. (34), and we use the following inequality in (ii)
831"
"V
X",0.6919431279620853,"x =
x
log(1 + x) log(1 + x) ≤

max
x∈[0,u]
x
log(1 + x)"
"V
X",0.6925355450236966,"
log(1 + x) =
u
log(1 + u) log(1 + x) ,"
"V
X",0.693127962085308,"which holds for any x ∈[0, u], where constants c0 and cℓare derived as
832"
"V
X",0.6937203791469194,"c0 =
σ2
1
log(1 + σ2
1
σ2 )
,
cℓ=
σ2
ℓ+1"
"V
X",0.6943127962085308,"log(1 +
σ2
ℓ+1
σ2 )
, with the convention that σL+1 = 1 ."
"V
X",0.6949052132701422,"The derivation of c0 uses that
833"
"V
X",0.6954976303317536,"X⊤
t ˆΣt,AtXt ≤λ1(ˆΣt,At)∥Xt∥2 ≤λ−1
d (Σ−1
1
+ Gt,At) ≤λ−1
d (Σ−1
1 ) = λ1(Σ1) = σ2
1 ."
"V
X",0.6960900473933649,"The derivation of cℓfollows from
834"
"V
X",0.6966824644549763,"X⊤
t PAt,ℓ¯Σt,ℓP⊤
At,ℓXt ≤λ1(PAt,ℓP⊤
At,ℓ)λ1(¯Σt,ℓ)∥Xt∥2 ≤σ2
ℓ+1 ."
"V
X",0.6972748815165877,"Therefore, from Eq. (35) and Eq. (33), we get that
835"
"V
X",0.6978672985781991,"BR(n) ≤
p"
"V
X",0.6984597156398105,"2n log(1/δ)

E
h
c0 n
X"
"V
X",0.6990521327014217,"t=1
log(1 + σ−2X⊤
t ˆΣt,AtXt) +
X"
"V
X",0.6996445497630331,"ℓ∈[L]
cℓ n
X"
"V
X",0.7002369668246445,"t=1
log(1 + σ−2X⊤
t PAt,ℓ¯Σt,ℓP⊤
At,ℓXt)
i 1"
"V
X",0.7008293838862559,"2 + cnδ
(36)"
"V
X",0.7014218009478673,"Now we focus on bounding the logarithmic terms in Eq. (36).
836"
"V
X",0.7020142180094787,"(I) First term in Eq. (36) We first rewrite this term as
837"
"V
X",0.70260663507109,"log(1 + σ−2X⊤
t ˆΣt,AtXt)
(i)
= log det(Id + σ−2 ˆΣ"
"V
X",0.7031990521327014,"1
2
t,AtXtX⊤
t ˆΣ"
"V
X",0.7037914691943128,"1
2
t,At) ,"
"V
X",0.7043838862559242,"= log det(ˆΣ−1
t,At + σ−2XtX⊤
t ) −log det(ˆΣ−1
t,At) = log det(ˆΣ−1
t+1,At) −log det(ˆΣ−1
t,At) ,"
"V
X",0.7049763033175356,"where (i) follows from the Weinstein–Aronszajn identity. Then we sum over all rounds t ∈[n], and
838"
"V
X",0.705568720379147,"get a telescoping
839 n
X"
"V
X",0.7061611374407583,"t=1
log det(Id + σ−2 ˆΣ"
"V
X",0.7067535545023697,"1
2
t,AtXtX⊤
t ˆΣ"
"V
X",0.707345971563981,"1
2
t,At) = n
X"
"V
X",0.7079383886255924,"t=1
log det(ˆΣ−1
t+1,At) −log det(ˆΣ−1
t,At) , = n
X t=1 K
X"
"V
X",0.7085308056872038,"i=1
log det(ˆΣ−1
t+1,i) −log det(ˆΣ−1
t,i ) = K
X i=1 n
X"
"V
X",0.7091232227488151,"t=1
log det(ˆΣ−1
t+1,i) −log det(ˆΣ−1
t,i ) , = K
X"
"V
X",0.7097156398104265,"i=1
log det(ˆΣ−1
n+1,i) −log det(ˆΣ−1
1,i )
(i)
= K
X"
"V
X",0.7103080568720379,"i=1
log det(Σ"
"V
X",0.7109004739336493,"1
2
1 ˆΣ−1
n+1,iΣ"
"V
X",0.7114928909952607,"1
2
1 ) ,"
"V
X",0.7120853080568721,"where (i) follows from the fact that ˆΣ1,i = Σ1. Now we use the inequality of arithmetic and
840"
"V
X",0.7126777251184834,"geometric means and get
841 n
X"
"V
X",0.7132701421800948,"t=1
log det(Id + σ−2 ˆΣ"
"V
X",0.7138625592417062,"1
2
t,AtXtX⊤
t ˆΣ"
"V
X",0.7144549763033176,"1
2
t,At) = K
X"
"V
X",0.715047393364929,"i=1
log det(Σ"
"V
X",0.7156398104265402,"1
2
1 ˆΣ−1
n+1,iΣ"
"V
X",0.7162322274881516,"1
2
1 ) , ≤ K
X"
"V
X",0.716824644549763,"i=1
d log
1"
"V
X",0.7174170616113744,d Tr(Σ
"V
X",0.7180094786729858,"1
2
1 ˆΣ−1
n+1,iΣ"
"V
X",0.7186018957345972,"1
2
1 )

,
(37) ≤ K
X"
"V
X",0.7191943127962085,"i=1
d log

1 + n"
"V
X",0.7197867298578199,"d
σ2
1
σ2"
"V
X",0.7203791469194313,"
= Kd log

1 + n"
"V
X",0.7209715639810427,"d
σ2
1
σ2 
."
"V
X",0.7215639810426541,"(II) Remaining terms in Eq. (36) Let ℓ∈[L]. Then we have that
842"
"V
X",0.7221563981042654,"log(1 + σ−2X⊤
t PAt,ℓ¯Σt,ℓP⊤
At,ℓXt) = σ2ℓ
MAXσ−2ℓ
MAX log(1 + σ−2X⊤
t PAt,ℓ¯Σt,ℓP⊤
At,ℓXt) ,"
"V
X",0.7227488151658767,"≤σ2ℓ
MAX log(1 + σ−2σ−2ℓ
MAXX⊤
t PAt,ℓ¯Σt,ℓP⊤
At,ℓXt) ,"
"V
X",0.7233412322274881,"(i)
= σ2ℓ
MAX log det(Id + σ−2σ−2ℓ
MAX ¯Σ"
"V
X",0.7239336492890995,"1
2
t,ℓP⊤
At,ℓXtX⊤
t PAt,ℓ¯Σ"
"V
X",0.7245260663507109,"1
2
t,ℓ) ,"
"V
X",0.7251184834123223,"= σ2ℓ
MAX

log det(¯Σ−1
t,ℓ+ σ−2σ−2ℓ
MAXP⊤
At,ℓXtX⊤
t PAt,ℓ) −log det(¯Σ−1
t,ℓ)

,"
"V
X",0.7257109004739336,"where we use the Weinstein–Aronszajn identity in (i). Now we know from Lemma D.3 that the
843"
"V
X",0.726303317535545,"following inequality holds σ−2σ−2ℓ
MAXP⊤
At,ℓXtX⊤
t PAt,ℓ⪯¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓ. As a result, we get that
844"
"V
X",0.7268957345971564,"¯Σ−1
t,ℓ+ σ−2σ−2ℓ
MAXP⊤
At,ℓXtX⊤
t PAt,ℓ⪯¯Σ−1
t+1,ℓ. Thus,
845"
"V
X",0.7274881516587678,"log(1 + σ−2X⊤
t PAt,ℓ¯Σt,ℓP⊤
At,ℓXt) ≤σ2ℓ
MAX

log det(¯Σ−1
t+1,ℓ) −log det(¯Σ−1
t,ℓ)

,"
"V
X",0.7280805687203792,"Then we sum over all rounds t ∈[n], and get a telescoping
846 n
X"
"V
X",0.7286729857819905,"t=1
log(1 + σ−2X⊤
t PAt,ℓ¯Σt,ℓP⊤
At,ℓXt) ≤σ2ℓ
MAX n
X"
"V
X",0.7292654028436019,"t=1
log det(¯Σ−1
t+1,ℓ) −log det(¯Σ−1
t,ℓ) ,"
"V
X",0.7298578199052133,"= σ2ℓ
MAX

log det(¯Σ−1
n+1,ℓ) −log det(¯Σ−1
1,ℓ)

,"
"V
X",0.7304502369668247,"(i)
= σ2ℓ
MAX

log det(¯Σ−1
n+1,ℓ) −log det(Σ−1
ℓ+1)

,"
"V
X",0.731042654028436,"= σ2ℓ
MAX

log det(Σ"
"V
X",0.7316350710900474,"1
2
ℓ+1 ¯Σ−1
n+1,ℓΣ"
"V
X",0.7322274881516587,"1
2
ℓ+1)

,"
"V
X",0.7328199052132701,"where we use that ¯Σ1,ℓ= Σℓ+1 in (i). Finally, we use the inequality of arithmetic and geometric
847"
"V
X",0.7334123222748815,"means and get that
848 n
X"
"V
X",0.7340047393364929,"t=1
log(1 + σ−2X⊤
t PAt,ℓ¯Σt,ℓP⊤
At,ℓXt) ≤σ2ℓ
MAX

log det(Σ"
"V
X",0.7345971563981043,"1
2
ℓ+1 ¯Σ−1
n+1,ℓΣ"
"V
X",0.7351895734597157,"1
2
ℓ+1)

,"
"V
X",0.735781990521327,"≤dσ2ℓ
MAX log
1"
"V
X",0.7363744075829384,d Tr(Σ
"V
X",0.7369668246445498,"1
2
ℓ+1 ¯Σ−1
n+1,ℓΣ"
"V
X",0.7375592417061612,"1
2
ℓ+1)

,
(38)"
"V
X",0.7381516587677726,"≤dσ2ℓ
MAX log

1 + σ2
ℓ+1
σ2
ℓ 
,"
"V
X",0.7387440758293838,"The last inequality follows from the expression of ¯Σ−1
n+1,ℓin Eq. (18) that leads to
849 Σ"
"V
X",0.7393364928909952,"1
2
ℓ+1 ¯Σ−1
n+1,ℓΣ"
"V
X",0.7399289099526066,"1
2
ℓ+1 = Id + Σ"
"V
X",0.740521327014218,"1
2
ℓ+1 ¯Gt,ℓΣ"
"V
X",0.7411137440758294,"1
2
ℓ+1 ,"
"V
X",0.7417061611374408,= Id + Σ
"V
X",0.7422985781990521,"1
2
ℓ+1W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ

WℓΣ"
"V
X",0.7428909952606635,"1
2
ℓ+1 ,
(39)"
"V
X",0.7434834123222749,"since ¯Gt,ℓ= W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ

Wℓ. This allows us to bound 1"
"V
X",0.7440758293838863,d Tr(Σ
"V
X",0.7446682464454977,"1
2
ℓ+1 ¯Σ−1
n+1,ℓΣ"
"V
X",0.745260663507109,"1
2
ℓ+1) as
850"
"V
X",0.7458530805687204,"1
d Tr(Σ"
"V
X",0.7464454976303317,"1
2
ℓ+1 ¯Σ−1
n+1,ℓΣ"
"V
X",0.7470379146919431,"1
2
ℓ+1) = 1"
"V
X",0.7476303317535545,d Tr(Id + Σ
"V
X",0.7482227488151659,"1
2
ℓ+1W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ

WℓΣ"
"V
X",0.7488151658767772,"1
2
ℓ+1) , = 1"
"V
X",0.7494075829383886,d(d + Tr(Σ
"V
X",0.75,"1
2
ℓ+1W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ

WℓΣ"
"V
X",0.7505924170616114,"1
2
ℓ+1) ,"
"V
X",0.7511848341232228,"≤1 + 1 d d
X"
"V
X",0.7517772511848341,"k=1
λ1(Σ"
"V
X",0.7523696682464455,"1
2
ℓ+1W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ

WℓΣ"
"V
X",0.7529620853080569,"1
2
ℓ+1 ,"
"V
X",0.7535545023696683,"≤1 + 1 d d
X"
"V
X",0.7541469194312796,"k=1
λ1(Σℓ+1)λ1(W⊤
ℓWℓ)λ1
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ

,"
"V
X",0.754739336492891,"≤1 + 1 d d
X"
"V
X",0.7553317535545023,"k=1
λ1(Σℓ+1)λ1(W⊤
ℓWℓ)λ1
 
Σ−1
ℓ

,"
"V
X",0.7559241706161137,"≤1 + 1 d d
X k=1"
"V
X",0.7565165876777251,"σ2
ℓ+1
σ2
ℓ
= 1 + σ2
ℓ+1
σ2
ℓ
,
(40)"
"V
X",0.7571090047393365,"where we use the assumption that λ1(W⊤
ℓWℓ) = 1 (A2) and that λ1(Σℓ+1) = σ2
ℓ+1 and λ1(Σ−1
ℓ) =
851"
"V
X",0.7577014218009479,"1/σ2
ℓ. This is because Σℓ= σ2
ℓId for any ℓ∈[L + 1]. Finally, plugging Eqs. (37) and (38) in Eq. (36)
852"
"V
X",0.7582938388625592,"concludes the proof.
853"
"V
X",0.7588862559241706,"D.6
Proof of proposition 4.2
854"
"V
X",0.759478672985782,"We use exactly the same proof in Appendix D.5, with one change to account for the sparsity
855"
"V
X",0.7600710900473934,"assumption (A3). The change corresponds to Eq. (38). First, recall that Eq. (38) writes
856 n
X"
"V
X",0.7606635071090048,"t=1
log(1 + σ−2X⊤
t PAt,ℓ¯Σt,ℓP⊤
At,ℓXt) ≤σ2ℓ
MAX

log det(Σ"
"V
X",0.7612559241706162,"1
2
ℓ+1 ¯Σ−1
n+1,ℓΣ"
"V
X",0.7618483412322274,"1
2
ℓ+1)

,"
"V
X",0.7624407582938388,"where
857 Σ"
"V
X",0.7630331753554502,"1
2
ℓ+1 ¯Σ−1
n+1,ℓΣ"
"V
X",0.7636255924170616,"1
2
ℓ+1 = Id + Σ"
"V
X",0.764218009478673,"1
2
ℓ+1W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ

WℓΣ"
"V
X",0.7648104265402843,"1
2
ℓ+1 ,"
"V
X",0.7654028436018957,"= Id + σ2
ℓ+1W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ

Wℓ,
(41)"
"V
X",0.7659952606635071,"where the second equality follows from the assumption that Σℓ+1 = σ2
ℓ+1Id. But notice that in
858"
"V
X",0.7665876777251185,"our assumption, (A3), we assume that Wℓ= ( ¯Wℓ, 0d,d−dℓ), where ¯Wℓ∈Rd×dℓfor any ℓ∈[L].
859"
"V
X",0.7671800947867299,"Therefore, we have that for any d × d matrix B ∈Rdd×d, the following holds, W⊤
ℓBWℓ=
860
 ¯W⊤
ℓB ¯Wℓ
0dℓ,d−dℓ
0d−dℓ,dℓ
0d−dℓ,d−dℓ"
"V
X",0.7677725118483413,"
. In particular, we have that
861"
"V
X",0.7683649289099526,"W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ

Wℓ=
 ¯W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ
 ¯Wℓ
0dℓ,d−dℓ
0d−dℓ,dℓ
0d−dℓ,d−dℓ"
"V
X",0.768957345971564,"
.
(42)"
"V
X",0.7695497630331753,"Therefore, plugging this in Eq. (41) yields that
862 Σ"
"V
X",0.7701421800947867,"1
2
ℓ+1 ¯Σ−1
n+1,ℓΣ"
"V
X",0.7707345971563981,"1
2
ℓ+1 =

Idℓ+ σ2
ℓ+1 ¯W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ
 ¯Wℓ
0dℓ,d−dℓ
0d−dℓ,dℓ
Id−dℓ"
"V
X",0.7713270142180095,"
.
(43)"
"V
X",0.7719194312796208,"As a result, det(Σ"
"V
X",0.7725118483412322,"1
2
ℓ+1 ¯Σ−1
n+1,ℓΣ"
"V
X",0.7731042654028436,"1
2
ℓ+1) = det(Idℓ+σ2
ℓ+1 ¯W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ
 ¯Wℓ). This allows
863"
"V
X",0.773696682464455,"us to move the problem from a d-dimensional one to a dℓ-dimensional one. Then we use the inequality
864"
"V
X",0.7742890995260664,"of arithmetic and geometric means and get that
865 n
X"
"V
X",0.7748815165876777,"t=1
log(1 + σ−2X⊤
t PAt,ℓ¯Σt,ℓP⊤
At,ℓXt) ≤σ2ℓ
MAX

log det(Σ"
"V
X",0.7754739336492891,"1
2
ℓ+1 ¯Σ−1
n+1,ℓΣ"
"V
X",0.7760663507109005,"1
2
ℓ+1)

,"
"V
X",0.7766587677725119,"= σ2ℓ
MAX log det(Idℓ+ σ2
ℓ+1 ¯W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ
 ¯Wℓ) ,"
"V
X",0.7772511848341233,"≤dℓσ2ℓ
MAX log
 1"
"V
X",0.7778436018957346,"dℓ
Tr(Idℓ+ σ2
ℓ+1 ¯W⊤
ℓ
 
Σ−1
ℓ
−Σ−1
ℓ
¯Σt,ℓ−1Σ−1
ℓ
 ¯Wℓ)

,"
"V
X",0.7784360189573459,"≤dℓσ2ℓ
MAX log

1 + σ2
ℓ+1
σ2
ℓ"
"V
X",0.7790284360189573,"
.
(44)"
"V
X",0.7796208530805687,"To get the last inequality, we use derivations similar to the ones we used in Eq. (40). Finally, the
866"
"V
X",0.7802132701421801,"desired result in obtained by replacing Eq. (38) by Eq. (44) in the previous proof in Appendix D.5.
867"
"V
X",0.7808056872037915,"D.7
Additional discussion: link to two-level hierarchies
868"
"V
X",0.7813981042654028,"The linear diffusion (15) can be marginalized into a 2-level hierarchy using two different strategies.
869"
"V
X",0.7819905213270142,"The first one yields,
870"
"V
X",0.7825829383886256,"ψ∗,L ∼N(0, σ2
L+1BLB⊤
L) ,
(45)
θ∗,i | ψ∗,L ∼N(ψ∗,L, Ω1) ,
∀i ∈[K] ,"
"V
X",0.783175355450237,"with Ω1 = σ2
1Id + PL−1
ℓ=1 σ2
ℓ+1BℓB⊤
ℓand Bℓ= Qℓ
k=1 Wk. The second strategy yields,
871"
"V
X",0.7837677725118484,"ψ∗,1 ∼N(0, Ω2) ,
(46)"
"V
X",0.7843601895734598,"θ∗,i | ψ∗,1 ∼N(ψ∗,1, σ2
1Id) ,
∀i ∈[K] ,"
"V
X",0.784952606635071,"where Ω2 = PL
ℓ=1 σ2
ℓ+1BℓB⊤
ℓ. Recently, HierTS [Hong et al., 2022b] was developed for such
872"
"V
X",0.7855450236966824,"two-level graphical models, and we call HierTS under (45) by HierTS-1 and HierTS under (46)
873"
"V
X",0.7861374407582938,"by HierTS-2. Then, we start by highlighting the differences between these two variants of HierTS.
874"
"V
X",0.7867298578199052,"First, their regret bounds scale as
875"
"V
X",0.7873222748815166,"HierTS-1 : ˜O
 q"
"V
X",0.7879146919431279,"nd(K PL
ℓ=1 σ2
ℓ+ Lσ2
L+1

,
HierTS-2 : ˜O
 q"
"V
X",0.7885071090047393,"nd(Kσ2
1 + PL
ℓ=1 σ2
ℓ+1)

."
"V
X",0.7890995260663507,"When K ≈L, the regret bounds of HierTS-1 and HierTS-2 are similar. However, when K > L,
876"
"V
X",0.7896919431279621,"HierTS-2 outperforms HierTS-1. This is because HierTS-2 puts more uncertainty on a single
877"
"V
X",0.7902843601895735,"d-dimensional latent parameter ψ∗,1, rather than K individual d-dimensional action parameters
878"
"V
X",0.7908767772511849,"θ∗,i. More importantly, HierTS-1 implicitly assumes that action parameters θ∗,i are conditionally
879"
"V
X",0.7914691943127962,"independent given ψ∗,L, which is not true. Consequently, HierTS-2 outperforms HierTS-1. Note
880"
"V
X",0.7920616113744076,"that, under the linear diffusion model (15), dTS and HierTS-2 have roughly similar regret bounds.
881"
"V
X",0.792654028436019,"Specifically, their regret bounds dependency on K is identical, where both methods involve mul-
882"
"V
X",0.7932464454976303,"tiplying K by σ2
1, and both enjoy improved performance compared to HierTS-1. That said, note
883"
"V
X",0.7938388625592417,"that Theorem 4.1 and Proposition 4.2 provide an understanding of how dTS’s regret scales under
884"
"V
X",0.794431279620853,"linear score functions fℓ, and do not say that using dTS is better than using HierTS when the score
885"
"V
X",0.7950236966824644,"functions fℓare linear since the latter can be obtained by a proper marginalization of latent parameters
886"
"V
X",0.7956161137440758,"(i.e., HierTS-2 instead of HierTS-1). While such a comparison is not the goal of this work, we still
887"
"V
X",0.7962085308056872,"provide it for completeness next.
888"
"V
X",0.7968009478672986,"When the mixing matrices Wℓare dense (i.e., assumption (A3) is not applicable), dTS and HierTS-2
889"
"V
X",0.79739336492891,"have comparable regret bounds and computational efficiency. However, under the sparsity assumption
890"
"V
X",0.7979857819905213,"(A3) and with mixing matrices that allow for conditional independence of ψ∗,1 coordinates given
891"
"V
X",0.7985781990521327,"ψ∗,2, dTS enjoys a computational advantage over HierTS-2. This advantage explains why works
892"
"V
X",0.7991706161137441,"focusing on multi-level hierarchies typically benchmark their algorithms against two-level structures
893"
"V
X",0.7997630331753555,"akin to HierTS-1, rather than the more competitive HierTS-2. This is also consistent with prior
894"
"V
X",0.8003554502369669,"works in Bayesian bandits using multi-level hierarchies, such as Tree-based priors [Hong et al.,
895"
"V
X",0.8009478672985783,"2022a], which compared their method to HierTS-1. In line with this, we also compared dTS with
896"
"V
X",0.8015402843601895,"HierTS-1 in our experiments. But this is only given for completeness as this is not the aim of
897"
"V
X",0.8021327014218009,"Theorem 4.1 and Proposition 4.2. More importantly, HierTS is inapplicable in the general case in (1)
898"
"V
X",0.8027251184834123,"with non-linear score functions since the latent parameters cannot be analytically marginalized.
899"
"V
X",0.8033175355450237,"E
Broader impact
900"
"V
X",0.8039099526066351,"This work contributes to the development and analysis of practical algorithms for online learning to
901"
"V
X",0.8045023696682464,"act under uncertainty. While our generic setting and algorithms have broad potential applications,
902"
"V
X",0.8050947867298578,"the specific downstream social impacts are inherently dependent on the chosen application domain.
903"
"V
X",0.8056872037914692,"Nevertheless, we acknowledge the crucial need to consider potential biases that may be present in
904"
"V
X",0.8062796208530806,"pre-trained diffusion models, given that our method relies on them.
905"
"V
X",0.806872037914692,"F
Limitations
906"
"V
X",0.8074644549763034,"Our work investigated contextual bandits, laying the groundwork for future exploration into reinforce-
907"
"V
X",0.8080568720379147,"ment learning. This exploration can be done from both practical (empirical) and theoretical angles.
908"
"V
X",0.808649289099526,"While our method, which approximates rewards using a Gaussian distribution, worked well for linear
909"
"V
X",0.8092417061611374,"rewards and those following a generalized linear model, its effectiveness in real-world, complex
910"
"V
X",0.8098341232227488,"scenarios needs further testing. Another interesting direction for future research is pre-training the
911"
"V
X",0.8104265402843602,"diffusion model prior. Hsieh et al. [2023] proposed a method for this in multi-armed bandits, but its
912"
"V
X",0.8110189573459715,"application to contextual bandits remains unexplored.
913"
"V
X",0.8116113744075829,"G
Amount of computation required
914"
"V
X",0.8122037914691943,"Our experiments were conducted on internal machines with 30 CPUs and thus they required a moder-
915"
"V
X",0.8127962085308057,"ate amount of computation. These experiments are also reproducible with minimal computational
916"
"V
X",0.8133886255924171,"resources.
917"
"V
X",0.8139810426540285,"NeurIPS Paper Checklist
918"
CLAIMS,0.8145734597156398,"1. Claims
919"
CLAIMS,0.8151658767772512,"Question: Do the main claims made in the abstract and introduction accurately reflect the
920"
CLAIMS,0.8157582938388626,"paper’s contributions and scope?
921"
CLAIMS,0.816350710900474,"Answer: [Yes]
922"
CLAIMS,0.8169431279620853,"Justification: All claims are supported by the theory in Section 4 (with proofs provided in
923"
CLAIMS,0.8175355450236966,"the appendix) and experiments in Section 5.
924"
CLAIMS,0.818127962085308,"Guidelines:
925"
CLAIMS,0.8187203791469194,"• The answer NA means that the abstract and introduction do not include the claims
926"
CLAIMS,0.8193127962085308,"made in the paper.
927"
CLAIMS,0.8199052132701422,"• The abstract and/or introduction should clearly state the claims made, including the
928"
CLAIMS,0.8204976303317536,"contributions made in the paper and important assumptions and limitations. A No or
929"
CLAIMS,0.8210900473933649,"NA answer to this question will not be perceived well by the reviewers.
930"
CLAIMS,0.8216824644549763,"• The claims made should match theoretical and experimental results, and reflect how
931"
CLAIMS,0.8222748815165877,"much the results can be expected to generalize to other settings.
932"
CLAIMS,0.8228672985781991,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
933"
CLAIMS,0.8234597156398105,"are not attained by the paper.
934"
LIMITATIONS,0.8240521327014217,"2. Limitations
935"
LIMITATIONS,0.8246445497630331,"Question: Does the paper discuss the limitations of the work performed by the authors?
936"
LIMITATIONS,0.8252369668246445,"Answer: [Yes]
937"
LIMITATIONS,0.8258293838862559,"Justification: Limitations were discussed in Section 6 and Appendix F.
938"
LIMITATIONS,0.8264218009478673,"Guidelines:
939"
LIMITATIONS,0.8270142180094787,"• The answer NA means that the paper has no limitation while the answer No means that
940"
LIMITATIONS,0.82760663507109,"the paper has limitations, but those are not discussed in the paper.
941"
LIMITATIONS,0.8281990521327014,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
942"
LIMITATIONS,0.8287914691943128,"• The paper should point out any strong assumptions and how robust the results are to
943"
LIMITATIONS,0.8293838862559242,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
944"
LIMITATIONS,0.8299763033175356,"model well-specification, asymptotic approximations only holding locally). The authors
945"
LIMITATIONS,0.830568720379147,"should reflect on how these assumptions might be violated in practice and what the
946"
LIMITATIONS,0.8311611374407583,"implications would be.
947"
LIMITATIONS,0.8317535545023697,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
948"
LIMITATIONS,0.832345971563981,"only tested on a few datasets or with a few runs. In general, empirical results often
949"
LIMITATIONS,0.8329383886255924,"depend on implicit assumptions, which should be articulated.
950"
LIMITATIONS,0.8335308056872038,"• The authors should reflect on the factors that influence the performance of the approach.
951"
LIMITATIONS,0.8341232227488151,"For example, a facial recognition algorithm may perform poorly when image resolution
952"
LIMITATIONS,0.8347156398104265,"is low or images are taken in low lighting. Or a speech-to-text system might not be
953"
LIMITATIONS,0.8353080568720379,"used reliably to provide closed captions for online lectures because it fails to handle
954"
LIMITATIONS,0.8359004739336493,"technical jargon.
955"
LIMITATIONS,0.8364928909952607,"• The authors should discuss the computational efficiency of the proposed algorithms
956"
LIMITATIONS,0.8370853080568721,"and how they scale with dataset size.
957"
LIMITATIONS,0.8376777251184834,"• If applicable, the authors should discuss possible limitations of their approach to
958"
LIMITATIONS,0.8382701421800948,"address problems of privacy and fairness.
959"
LIMITATIONS,0.8388625592417062,"• While the authors might fear that complete honesty about limitations might be used by
960"
LIMITATIONS,0.8394549763033176,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
961"
LIMITATIONS,0.840047393364929,"limitations that aren’t acknowledged in the paper. The authors should use their best
962"
LIMITATIONS,0.8406398104265402,"judgment and recognize that individual actions in favor of transparency play an impor-
963"
LIMITATIONS,0.8412322274881516,"tant role in developing norms that preserve the integrity of the community. Reviewers
964"
LIMITATIONS,0.841824644549763,"will be specifically instructed to not penalize honesty concerning limitations.
965"
THEORY ASSUMPTIONS AND PROOFS,0.8424170616113744,"3. Theory Assumptions and Proofs
966"
THEORY ASSUMPTIONS AND PROOFS,0.8430094786729858,"Question: For each theoretical result, does the paper provide the full set of assumptions and
967"
THEORY ASSUMPTIONS AND PROOFS,0.8436018957345972,"a complete (and correct) proof?
968"
THEORY ASSUMPTIONS AND PROOFS,0.8441943127962085,"Answer: [Yes]
969"
THEORY ASSUMPTIONS AND PROOFS,0.8447867298578199,"Justification: Assumptions are mentioned in the main text. Complete proofs are provided in
970"
THEORY ASSUMPTIONS AND PROOFS,0.8453791469194313,"the appendix.
971"
THEORY ASSUMPTIONS AND PROOFS,0.8459715639810427,"Guidelines:
972"
THEORY ASSUMPTIONS AND PROOFS,0.8465639810426541,"• The answer NA means that the paper does not include theoretical results.
973"
THEORY ASSUMPTIONS AND PROOFS,0.8471563981042654,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
974"
THEORY ASSUMPTIONS AND PROOFS,0.8477488151658767,"referenced.
975"
THEORY ASSUMPTIONS AND PROOFS,0.8483412322274881,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
976"
THEORY ASSUMPTIONS AND PROOFS,0.8489336492890995,"• The proofs can either appear in the main paper or the supplemental material, but if
977"
THEORY ASSUMPTIONS AND PROOFS,0.8495260663507109,"they appear in the supplemental material, the authors are encouraged to provide a short
978"
THEORY ASSUMPTIONS AND PROOFS,0.8501184834123223,"proof sketch to provide intuition.
979"
THEORY ASSUMPTIONS AND PROOFS,0.8507109004739336,"• Inversely, any informal proof provided in the core of the paper should be complemented
980"
THEORY ASSUMPTIONS AND PROOFS,0.851303317535545,"by formal proofs provided in appendix or supplemental material.
981"
THEORY ASSUMPTIONS AND PROOFS,0.8518957345971564,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
982"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8524881516587678,"4. Experimental Result Reproducibility
983"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8530805687203792,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
984"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8536729857819905,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
985"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8542654028436019,"of the paper (regardless of whether the code and data are provided or not)?
986"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8548578199052133,"Answer: [Yes]
987"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8554502369668247,"Justification: Information needed to reproduce the main experimental results of the paper is
988"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.856042654028436,"described in Section 5.
989"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8566350710900474,"Guidelines:
990"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8572274881516587,"• The answer NA means that the paper does not include experiments.
991"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8578199052132701,"• If the paper includes experiments, a No answer to this question will not be perceived
992"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8584123222748815,"well by the reviewers: Making the paper reproducible is important, regardless of
993"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8590047393364929,"whether the code and data are provided or not.
994"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8595971563981043,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
995"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8601895734597157,"to make their results reproducible or verifiable.
996"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.860781990521327,"• Depending on the contribution, reproducibility can be accomplished in various ways.
997"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8613744075829384,"For example, if the contribution is a novel architecture, describing the architecture fully
998"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8619668246445498,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
999"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8625592417061612,"be necessary to either make it possible for others to replicate the model with the same
1000"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8631516587677726,"dataset, or provide access to the model. In general. releasing code and data is often
1001"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8637440758293838,"one good way to accomplish this, but reproducibility can also be provided via detailed
1002"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8643364928909952,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
1003"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8649289099526066,"of a large language model), releasing of a model checkpoint, or other means that are
1004"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.865521327014218,"appropriate to the research performed.
1005"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8661137440758294,"• While NeurIPS does not require releasing code, the conference does require all submis-
1006"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8667061611374408,"sions to provide some reasonable avenue for reproducibility, which may depend on the
1007"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8672985781990521,"nature of the contribution. For example
1008"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8678909952606635,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
1009"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8684834123222749,"to reproduce that algorithm.
1010"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8690758293838863,"(b) If the contribution is primarily a new model architecture, the paper should describe
1011"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8696682464454977,"the architecture clearly and fully.
1012"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.870260663507109,"(c) If the contribution is a new model (e.g., a large language model), then there should
1013"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8708530805687204,"either be a way to access this model for reproducing the results or a way to reproduce
1014"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8714454976303317,"the model (e.g., with an open-source dataset or instructions for how to construct
1015"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8720379146919431,"the dataset).
1016"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8726303317535545,"(d) We recognize that reproducibility may be tricky in some cases, in which case
1017"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8732227488151659,"authors are welcome to describe the particular way they provide for reproducibility.
1018"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8738151658767772,"In the case of closed-source models, it may be that access to the model is limited in
1019"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8744075829383886,"some way (e.g., to registered users), but it should be possible for other researchers
1020"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.875,"to have some path to reproducing or verifying the results.
1021"
OPEN ACCESS TO DATA AND CODE,0.8755924170616114,"5. Open access to data and code
1022"
OPEN ACCESS TO DATA AND CODE,0.8761848341232228,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
1023"
OPEN ACCESS TO DATA AND CODE,0.8767772511848341,"tions to faithfully reproduce the main experimental results, as described in supplemental
1024"
OPEN ACCESS TO DATA AND CODE,0.8773696682464455,"material?
1025"
OPEN ACCESS TO DATA AND CODE,0.8779620853080569,"Answer: [Yes]
1026"
OPEN ACCESS TO DATA AND CODE,0.8785545023696683,"Justification: The code for the main experiments is shared in the supplementary material.
1027"
OPEN ACCESS TO DATA AND CODE,0.8791469194312796,"Guidelines:
1028"
OPEN ACCESS TO DATA AND CODE,0.879739336492891,"• The answer NA means that paper does not include experiments requiring code.
1029"
OPEN ACCESS TO DATA AND CODE,0.8803317535545023,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
1030"
OPEN ACCESS TO DATA AND CODE,0.8809241706161137,"public/guides/CodeSubmissionPolicy) for more details.
1031"
OPEN ACCESS TO DATA AND CODE,0.8815165876777251,"• While we encourage the release of code and data, we understand that this might not be
1032"
OPEN ACCESS TO DATA AND CODE,0.8821090047393365,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
1033"
OPEN ACCESS TO DATA AND CODE,0.8827014218009479,"including code, unless this is central to the contribution (e.g., for a new open-source
1034"
OPEN ACCESS TO DATA AND CODE,0.8832938388625592,"benchmark).
1035"
OPEN ACCESS TO DATA AND CODE,0.8838862559241706,"• The instructions should contain the exact command and environment needed to run to
1036"
OPEN ACCESS TO DATA AND CODE,0.884478672985782,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
1037"
OPEN ACCESS TO DATA AND CODE,0.8850710900473934,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
1038"
OPEN ACCESS TO DATA AND CODE,0.8856635071090048,"• The authors should provide instructions on data access and preparation, including how
1039"
OPEN ACCESS TO DATA AND CODE,0.8862559241706162,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
1040"
OPEN ACCESS TO DATA AND CODE,0.8868483412322274,"• The authors should provide scripts to reproduce all experimental results for the new
1041"
OPEN ACCESS TO DATA AND CODE,0.8874407582938388,"proposed method and baselines. If only a subset of experiments are reproducible, they
1042"
OPEN ACCESS TO DATA AND CODE,0.8880331753554502,"should state which ones are omitted from the script and why.
1043"
OPEN ACCESS TO DATA AND CODE,0.8886255924170616,"• At submission time, to preserve anonymity, the authors should release anonymized
1044"
OPEN ACCESS TO DATA AND CODE,0.889218009478673,"versions (if applicable).
1045"
OPEN ACCESS TO DATA AND CODE,0.8898104265402843,"• Providing as much information as possible in supplemental material (appended to the
1046"
OPEN ACCESS TO DATA AND CODE,0.8904028436018957,"paper) is recommended, but including URLs to data and code is permitted.
1047"
OPEN ACCESS TO DATA AND CODE,0.8909952606635071,"6. Experimental Setting/Details
1048"
OPEN ACCESS TO DATA AND CODE,0.8915876777251185,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
1049"
OPEN ACCESS TO DATA AND CODE,0.8921800947867299,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
1050"
OPEN ACCESS TO DATA AND CODE,0.8927725118483413,"results?
1051"
OPEN ACCESS TO DATA AND CODE,0.8933649289099526,"Answer: [Yes]
1052"
OPEN ACCESS TO DATA AND CODE,0.893957345971564,"Justification: All experimental details are described in Section 5.
1053"
OPEN ACCESS TO DATA AND CODE,0.8945497630331753,"Guidelines:
1054"
OPEN ACCESS TO DATA AND CODE,0.8951421800947867,"• The answer NA means that the paper does not include experiments.
1055"
OPEN ACCESS TO DATA AND CODE,0.8957345971563981,"• The experimental setting should be presented in the core of the paper to a level of detail
1056"
OPEN ACCESS TO DATA AND CODE,0.8963270142180095,"that is necessary to appreciate the results and make sense of them.
1057"
OPEN ACCESS TO DATA AND CODE,0.8969194312796208,"• The full details can be provided either with the code, in appendix, or as supplemental
1058"
OPEN ACCESS TO DATA AND CODE,0.8975118483412322,"material.
1059"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8981042654028436,"7. Experiment Statistical Significance
1060"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.898696682464455,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
1061"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8992890995260664,"information about the statistical significance of the experiments?
1062"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8998815165876777,"Answer: [Yes]
1063"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9004739336492891,"Justification: Standard error bars are included in the figures.
1064"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9010663507109005,"Guidelines:
1065"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9016587677725119,"• The answer NA means that the paper does not include experiments.
1066"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9022511848341233,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
1067"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9028436018957346,"dence intervals, or statistical significance tests, at least for the experiments that support
1068"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9034360189573459,"the main claims of the paper.
1069"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9040284360189573,"• The factors of variability that the error bars are capturing should be clearly stated (for
1070"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9046208530805687,"example, train/test split, initialization, random drawing of some parameter, or overall
1071"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9052132701421801,"run with given experimental conditions).
1072"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9058056872037915,"• The method for calculating the error bars should be explained (closed form formula,
1073"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9063981042654028,"call to a library function, bootstrap, etc.)
1074"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9069905213270142,"• The assumptions made should be given (e.g., Normally distributed errors).
1075"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9075829383886256,"• It should be clear whether the error bar is the standard deviation or the standard error
1076"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.908175355450237,"of the mean.
1077"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9087677725118484,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
1078"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9093601895734598,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
1079"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.909952606635071,"of Normality of errors is not verified.
1080"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9105450236966824,"• For asymmetric distributions, the authors should be careful not to show in tables or
1081"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9111374407582938,"figures symmetric error bars that would yield results that are out of range (e.g. negative
1082"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9117298578199052,"error rates).
1083"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9123222748815166,"• If error bars are reported in tables or plots, The authors should explain in the text how
1084"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9129146919431279,"they were calculated and reference the corresponding figures or tables in the text.
1085"
EXPERIMENTS COMPUTE RESOURCES,0.9135071090047393,"8. Experiments Compute Resources
1086"
EXPERIMENTS COMPUTE RESOURCES,0.9140995260663507,"Question: For each experiment, does the paper provide sufficient information on the com-
1087"
EXPERIMENTS COMPUTE RESOURCES,0.9146919431279621,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
1088"
EXPERIMENTS COMPUTE RESOURCES,0.9152843601895735,"the experiments?
1089"
EXPERIMENTS COMPUTE RESOURCES,0.9158767772511849,"Answer: [Yes]
1090"
EXPERIMENTS COMPUTE RESOURCES,0.9164691943127962,"Justification: As mentioned in Appendix G, our experiments were conducted on internal
1091"
EXPERIMENTS COMPUTE RESOURCES,0.9170616113744076,"machines with 30 CPUs and thus they required a moderate amount of computation. These
1092"
EXPERIMENTS COMPUTE RESOURCES,0.917654028436019,"experiments are also reproducible with minimal computational resources.
1093"
EXPERIMENTS COMPUTE RESOURCES,0.9182464454976303,"Guidelines:
1094"
EXPERIMENTS COMPUTE RESOURCES,0.9188388625592417,"• The answer NA means that the paper does not include experiments.
1095"
EXPERIMENTS COMPUTE RESOURCES,0.919431279620853,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
1096"
EXPERIMENTS COMPUTE RESOURCES,0.9200236966824644,"or cloud provider, including relevant memory and storage.
1097"
EXPERIMENTS COMPUTE RESOURCES,0.9206161137440758,"• The paper should provide the amount of compute required for each of the individual
1098"
EXPERIMENTS COMPUTE RESOURCES,0.9212085308056872,"experimental runs as well as estimate the total compute.
1099"
EXPERIMENTS COMPUTE RESOURCES,0.9218009478672986,"• The paper should disclose whether the full research project required more compute
1100"
EXPERIMENTS COMPUTE RESOURCES,0.92239336492891,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
1101"
EXPERIMENTS COMPUTE RESOURCES,0.9229857819905213,"didn’t make it into the paper).
1102"
CODE OF ETHICS,0.9235781990521327,"9. Code Of Ethics
1103"
CODE OF ETHICS,0.9241706161137441,"Question: Does the research conducted in the paper conform, in every respect, with the
1104"
CODE OF ETHICS,0.9247630331753555,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1105"
CODE OF ETHICS,0.9253554502369669,"Answer: [Yes]
1106"
CODE OF ETHICS,0.9259478672985783,"Justification: This work contributes to the development and theoretical analysis of online
1107"
CODE OF ETHICS,0.9265402843601895,"learning to act under uncertainty and it adheres to the Neurips Code Of Ethics.
1108"
CODE OF ETHICS,0.9271327014218009,"Guidelines:
1109"
CODE OF ETHICS,0.9277251184834123,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1110"
CODE OF ETHICS,0.9283175355450237,"• If the authors answer No, they should explain the special circumstances that require a
1111"
CODE OF ETHICS,0.9289099526066351,"deviation from the Code of Ethics.
1112"
CODE OF ETHICS,0.9295023696682464,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
1113"
CODE OF ETHICS,0.9300947867298578,"eration due to laws or regulations in their jurisdiction).
1114"
BROADER IMPACTS,0.9306872037914692,"10. Broader Impacts
1115"
BROADER IMPACTS,0.9312796208530806,"Question: Does the paper discuss both potential positive societal impacts and negative
1116"
BROADER IMPACTS,0.931872037914692,"societal impacts of the work performed?
1117"
BROADER IMPACTS,0.9324644549763034,"Answer: [Yes]
1118"
BROADER IMPACTS,0.9330568720379147,"Justification: Broader Impacts are discussed in Appendix E.
1119"
BROADER IMPACTS,0.933649289099526,"Guidelines:
1120"
BROADER IMPACTS,0.9342417061611374,"• The answer NA means that there is no societal impact of the work performed.
1121"
BROADER IMPACTS,0.9348341232227488,"• If the authors answer NA or No, they should explain why their work has no societal
1122"
BROADER IMPACTS,0.9354265402843602,"impact or why the paper does not address societal impact.
1123"
BROADER IMPACTS,0.9360189573459715,"• Examples of negative societal impacts include potential malicious or unintended uses
1124"
BROADER IMPACTS,0.9366113744075829,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1125"
BROADER IMPACTS,0.9372037914691943,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1126"
BROADER IMPACTS,0.9377962085308057,"groups), privacy considerations, and security considerations.
1127"
BROADER IMPACTS,0.9383886255924171,"• The conference expects that many papers will be foundational research and not tied
1128"
BROADER IMPACTS,0.9389810426540285,"to particular applications, let alone deployments. However, if there is a direct path to
1129"
BROADER IMPACTS,0.9395734597156398,"any negative applications, the authors should point it out. For example, it is legitimate
1130"
BROADER IMPACTS,0.9401658767772512,"to point out that an improvement in the quality of generative models could be used to
1131"
BROADER IMPACTS,0.9407582938388626,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1132"
BROADER IMPACTS,0.941350710900474,"that a generic algorithm for optimizing neural networks could enable people to train
1133"
BROADER IMPACTS,0.9419431279620853,"models that generate Deepfakes faster.
1134"
BROADER IMPACTS,0.9425355450236966,"• The authors should consider possible harms that could arise when the technology is
1135"
BROADER IMPACTS,0.943127962085308,"being used as intended and functioning correctly, harms that could arise when the
1136"
BROADER IMPACTS,0.9437203791469194,"technology is being used as intended but gives incorrect results, and harms following
1137"
BROADER IMPACTS,0.9443127962085308,"from (intentional or unintentional) misuse of the technology.
1138"
BROADER IMPACTS,0.9449052132701422,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1139"
BROADER IMPACTS,0.9454976303317536,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1140"
BROADER IMPACTS,0.9460900473933649,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1141"
BROADER IMPACTS,0.9466824644549763,"feedback over time, improving the efficiency and accessibility of ML).
1142"
SAFEGUARDS,0.9472748815165877,"11. Safeguards
1143"
SAFEGUARDS,0.9478672985781991,"Question: Does the paper describe safeguards that have been put in place for responsible
1144"
SAFEGUARDS,0.9484597156398105,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1145"
SAFEGUARDS,0.9490521327014217,"image generators, or scraped datasets)?
1146"
SAFEGUARDS,0.9496445497630331,"Answer: [NA]
1147"
SAFEGUARDS,0.9502369668246445,"Justification: Our paper is mainly theoretical and the used data is simulated. Thus, we
1148"
SAFEGUARDS,0.9508293838862559,"believe that our work poses no such risks.
1149"
SAFEGUARDS,0.9514218009478673,"Guidelines:
1150"
SAFEGUARDS,0.9520142180094787,"• The answer NA means that the paper poses no such risks.
1151"
SAFEGUARDS,0.95260663507109,"• Released models that have a high risk for misuse or dual-use should be released with
1152"
SAFEGUARDS,0.9531990521327014,"necessary safeguards to allow for controlled use of the model, for example by requiring
1153"
SAFEGUARDS,0.9537914691943128,"that users adhere to usage guidelines or restrictions to access the model or implementing
1154"
SAFEGUARDS,0.9543838862559242,"safety filters.
1155"
SAFEGUARDS,0.9549763033175356,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1156"
SAFEGUARDS,0.955568720379147,"should describe how they avoided releasing unsafe images.
1157"
SAFEGUARDS,0.9561611374407583,"• We recognize that providing effective safeguards is challenging, and many papers do
1158"
SAFEGUARDS,0.9567535545023697,"not require this, but we encourage authors to take this into account and make a best
1159"
SAFEGUARDS,0.957345971563981,"faith effort.
1160"
LICENSES FOR EXISTING ASSETS,0.9579383886255924,"12. Licenses for existing assets
1161"
LICENSES FOR EXISTING ASSETS,0.9585308056872038,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1162"
LICENSES FOR EXISTING ASSETS,0.9591232227488151,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1163"
LICENSES FOR EXISTING ASSETS,0.9597156398104265,"properly respected?
1164"
LICENSES FOR EXISTING ASSETS,0.9603080568720379,"Answer: [Yes]
1165"
LICENSES FOR EXISTING ASSETS,0.9609004739336493,"Justification: To the best of our knowledge, all relevant and used papers were cited.
1166"
LICENSES FOR EXISTING ASSETS,0.9614928909952607,"Guidelines:
1167"
LICENSES FOR EXISTING ASSETS,0.9620853080568721,"• The answer NA means that the paper does not use existing assets.
1168"
LICENSES FOR EXISTING ASSETS,0.9626777251184834,"• The authors should cite the original paper that produced the code package or dataset.
1169"
LICENSES FOR EXISTING ASSETS,0.9632701421800948,"• The authors should state which version of the asset is used and, if possible, include a
1170"
LICENSES FOR EXISTING ASSETS,0.9638625592417062,"URL.
1171"
LICENSES FOR EXISTING ASSETS,0.9644549763033176,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1172"
LICENSES FOR EXISTING ASSETS,0.965047393364929,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1173"
LICENSES FOR EXISTING ASSETS,0.9656398104265402,"service of that source should be provided.
1174"
LICENSES FOR EXISTING ASSETS,0.9662322274881516,"• If assets are released, the license, copyright information, and terms of use in the
1175"
LICENSES FOR EXISTING ASSETS,0.966824644549763,"package should be provided. For popular datasets, paperswithcode.com/datasets
1176"
LICENSES FOR EXISTING ASSETS,0.9674170616113744,"has curated licenses for some datasets. Their licensing guide can help determine the
1177"
LICENSES FOR EXISTING ASSETS,0.9680094786729858,"license of a dataset.
1178"
LICENSES FOR EXISTING ASSETS,0.9686018957345972,"• For existing datasets that are re-packaged, both the original license and the license of
1179"
LICENSES FOR EXISTING ASSETS,0.9691943127962085,"the derived asset (if it has changed) should be provided.
1180"
LICENSES FOR EXISTING ASSETS,0.9697867298578199,"• If this information is not available online, the authors are encouraged to reach out to
1181"
LICENSES FOR EXISTING ASSETS,0.9703791469194313,"the asset’s creators.
1182"
NEW ASSETS,0.9709715639810427,"13. New Assets
1183"
NEW ASSETS,0.9715639810426541,"Question: Are new assets introduced in the paper well documented and is the documentation
1184"
NEW ASSETS,0.9721563981042654,"provided alongside the assets?
1185"
NEW ASSETS,0.9727488151658767,"Answer: [Yes]
1186"
NEW ASSETS,0.9733412322274881,"Justification: We include our code as supplementary material, with all details needed for
1187"
NEW ASSETS,0.9739336492890995,"reproducibility given in Section 5.
1188"
NEW ASSETS,0.9745260663507109,"Guidelines:
1189"
NEW ASSETS,0.9751184834123223,"• The answer NA means that the paper does not release new assets.
1190"
NEW ASSETS,0.9757109004739336,"• Researchers should communicate the details of the dataset/code/model as part of their
1191"
NEW ASSETS,0.976303317535545,"submissions via structured templates. This includes details about training, license,
1192"
NEW ASSETS,0.9768957345971564,"limitations, etc.
1193"
NEW ASSETS,0.9774881516587678,"• The paper should discuss whether and how consent was obtained from people whose
1194"
NEW ASSETS,0.9780805687203792,"asset is used.
1195"
NEW ASSETS,0.9786729857819905,"• At submission time, remember to anonymize your assets (if applicable). You can either
1196"
NEW ASSETS,0.9792654028436019,"create an anonymized URL or include an anonymized zip file.
1197"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798578199052133,"14. Crowdsourcing and Research with Human Subjects
1198"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804502369668247,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1199"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981042654028436,"include the full text of instructions given to participants and screenshots, if applicable, as
1200"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9816350710900474,"well as details about compensation (if any)?
1201"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9822274881516587,"Answer: [NA]
1202"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828199052132701,"Justification: This work does not involve crowdsourcing nor research with human subjects.
1203"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9834123222748815,"Guidelines:
1204"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840047393364929,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1205"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845971563981043,"human subjects.
1206"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851895734597157,"• Including this information in the supplemental material is fine, but if the main contribu-
1207"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985781990521327,"tion of the paper involves human subjects, then as much detail as possible should be
1208"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9863744075829384,"included in the main paper.
1209"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9869668246445498,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1210"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875592417061612,"or other labor should be paid at least the minimum wage in the country of the data
1211"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881516587677726,"collector.
1212"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9887440758293838,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1213"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893364928909952,"Subjects
1214"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9899289099526066,"Question: Does the paper describe potential risks incurred by study participants, whether
1215"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.990521327014218,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1216"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9911137440758294,"approvals (or an equivalent approval/review based on the requirements of your country or
1217"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9917061611374408,"institution) were obtained?
1218"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922985781990521,"Answer: [NA]
1219"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9928909952606635,"Justification: This work does not involve crowdsourcing nor research with human subjects.
1220"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934834123222749,"Guidelines:
1221"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940758293838863,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1222"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946682464454977,"human subjects.
1223"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995260663507109,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1224"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958530805687204,"may be required for any human subjects research. If you obtained IRB approval, you
1225"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964454976303317,"should clearly state this in the paper.
1226"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970379146919431,"• We recognize that the procedures for this may vary significantly between institutions
1227"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976303317535545,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1228"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9982227488151659,"guidelines for their institution.
1229"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988151658767772,"• For initial submissions, do not include any information that would break anonymity (if
1230"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9994075829383886,"applicable), such as the institution conducting the review.
1231"
