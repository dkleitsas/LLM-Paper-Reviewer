Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003134796238244514,"A recent emerging trend in the literature on learning in games has been concerned
with providing accelerated learning dynamics for correlated and coarse correlated
equilibria in normal-form games. Much less is known about the signiﬁcantly
more challenging setting of extensive-form games, which can capture sequential
and simultaneous moves, as well as imperfect information. In this paper, we de-
velop faster no-regret learning dynamics for extensive-form correlated equilibrium
(EFCE) in multiplayer general-sum imperfect-information extensive-form games.
When all agents play T repetitions of the game according to the accelerated dy-
namics, the correlated distribution of play is an O(T −3/4)-approximate EFCE.
This signiﬁcantly improves over the best prior rate of O(T −1/2). One of our
conceptual contributions is to connect predictive (that is, optimistic) regret mini-
mization with the framework of Φ-regret. One of our main technical contributions
is to characterize the stability of certain ﬁxed point strategies through a reﬁned
perturbation analysis of a structured Markov chain, which may be of independent
interest. Finally, experiments on standard benchmarks corroborate our ﬁndings."
INTRODUCTION,0.006269592476489028,"1
INTRODUCTION"
INTRODUCTION,0.009404388714733543,"Game-theoretic solution concepts describe how agents should rationally act in games. Over the
last two decades there has been tremendous progress in imperfect-information game solving and
algorithms based on game-theoretic solution concepts have become the state of the art. Prominent
milestones of this were an optimal strategy for Rhode Island hold’em poker (Gilpin & Sandholm,
2007), a near-optimal strategy for limit Texas hold’em (Bowling et al., 2015), and a superhuman
strategy for no-limit Texas hold’em (Brown & Sandholm, 2017). In particular, these advances rely
on algorithms that approximate Nash equilibria (NE) of two-player zero-sum extensive-form games
(EFGs). EFGs are a broad class of games that capture sequential and simultaneous interaction,
and imperfect information. For two-player zero-sum EFGs, it is by now well-understood how to
compute a Nash equilibrium at scale: in theory this can be achieved using accelerated uncoupled no-
regret learning dynamics, for example by having each player use an optimistic regret minimizer and
leveraging suitable distance-generating functions (Hoda et al., 2010; Kroer et al., 2020; Farina et al.,
2021c) for the EFG decision space. Such a setup converges to an equilibrium at a rate of O(T −1).
In practice, modern variants of the counterfactual regret minimization (CFR) framework typically
lead to better performance, although the worst-case convergence rate is O(T −1/2) (Zinkevich et al.,
2007). CFR is also an uncoupled no-regret learning dynamic."
INTRODUCTION,0.012539184952978056,"However, many real-world applications are not two-player zero-sum games, but instead have
general-sum utilities and often more than two players. In such settings, Nash equilibrium suffers
from several drawbacks when used as a prescriptive tool. First, there can be multiple equilibria,
and an equilibrium strategy may perform very poorly when played against the “wrong” equilibrium
strategies of the other player(s). Thus, the players effectively would need to communicate in order
to ﬁnd an equilibrium, or hope to converge to it via some sort of learning dynamics. Second, ﬁnd-
ing a Nash equilibrium is computationally hard both in theory (Daskalakis et al., 2006; Etessami &
Yannakakis, 2007) and in practice (Berg & Sandholm, 2017). This effectively squashes any hope of
developing efﬁcient learning dynamics that converge to general-sum Nash equilibria."
INTRODUCTION,0.01567398119122257,"A competing notion of rationality proposed by Aumann (1974) is that of correlated equilibrium
(CE), typically modeled via a trusted mediator who privately recommends actions to the players."
INTRODUCTION,0.018808777429467086,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0219435736677116,"Unlike NE, it is known that the latter can be computed in polynomial time and, perhaps even more
importantly, it can be attained through uncoupled learning dynamics, where the players only need
to reason about their own observed utilities. This overcomes the often unreasonable presumption
that players have knowledge about the other players’ utilities. At the same time, uncoupled learning
algorithms have proven to be a remarkably scalable approach for computing equilibria in large-scale
games, as described above. The basic CE notion is deﬁned for normal-form games, and there it has
long been known that uncoupled no-regret learning dynamics can converge to CE or the coarse
correlated equilibrium (CCE) variant at a rate of O(T −1/2) (Hart & Mas-Colell, 2000; Celli et al.,
2019). More recently, it was shown that accelerated uncoupled no-regret learning dynamics can
compute CCE and CE at a rate of O(T −3/4) (Syrgkanis et al., 2015; Chen & Peng, 2020)."
INTRODUCTION,0.025078369905956112,"In the context of EFGs, the idea of correlation is much more intricate, and there are several no-
tions of correlated equilibrium, based on when the mediator gives recommendations and how the
mediator reacts to players who disregard the advice. One of the most compelling notions for EFGs
is the extensive-form correlated equilibrium (henceforth EFCE) (von Stengel & Forges, 2008) for
extensive-form games with perfect recall. Because of the sequential nature, the presence of private
information in the game, and the gradual revelation of recommendations, the constraints associated
with EFCE are signiﬁcantly more complex than for normal-form games. For these reasons, the
question of whether uncoupled learning dynamics can converge to an EFCE was only very recently
resolved by Celli et al. (2020). Moreover, in a follow-up work they also established an explicit
rate of convergence of O(T −1/2) (Farina et al., 2021a). Our paper is concerned with the following
fundamental question: Can one develop faster uncoupled no-regret learning dynamics for EFCE?"
INTRODUCTION,0.02821316614420063,"Contributions.
Our primary contribution is to answer this question in the positive:"
INTRODUCTION,0.03134796238244514,"Theorem 1.1. On any ﬁnite perfect-recall general-sum multiplayer extensive-form game, the un-
coupled no-regret learning dynamics described in this paper lead to a correlated distribution of
play that is an O(T −3/4)-approximate EFCE, where the O(·) notation suppresses game-speciﬁc
parameters polynomial in the size of the game."
INTRODUCTION,0.034482758620689655,"We achieve this result using the framework of predictive (also known as optimistic) regret mini-
mization (Chiang et al., 2012; Rakhlin & Sridharan, 2013b). One of our conceptual contributions
is to connect this line of work with the framework of Φ-regret minimization of Greenwald & Jafari
(2003); Gordon et al. (2008), by providing a general template for stable-predictive Φ-regret min-
imization. The importance of Φ-regret is that it leads to substantially more powerful notions of
hindsight rationality, beyond the usual external regret (Gordon et al., 2008), including the powerful
notion of swap regret (Blum & Mansour, 2007). Moreover, one of the primary insights behind the re-
sult of Farina et al. (2021a) is to cast convergence to an EFCE as a Φ-regret minimization problem.
Given these prior connections, we believe that our stable-predictive Φ template is of independent
interest, and could lead to further applications in the future."
INTRODUCTION,0.03761755485893417,"Theorem 1.1 extends and strengthens several prior papers in the literature, including the seminal
work of Syrgkanis et al. (2015) that provides accelerated dynamics for coarse correlated equilib-
rium in normal-form games, as well as the more recent result of Chen & Peng (2020) which showed
O(T −3/4) convergence to a correlated equilibrium in normal-form games. For the more challeng-
ing class of extensive-form games, accelerated rates were previously known only for ﬁnding a Nash
equilibrium in the special case of two-player zero-sum games, where an O(T −3/4) rate was achieved
via a stable-predictive CFR setup (Farina et al., 2019a) and an O(T −1) rate was achieved via opti-
mistic regret minimizers coupled with good distance-generating functions (Farina et al., 2019c)."
INTRODUCTION,0.04075235109717868,"From a technical standpoint, in order to apply our generic template for accelerated Φ-regret mini-
mization, we establish two separate ingredients. First, we develop a stable-predictive external regret
minimizer for the set of transformations Φ associated with EFCE. This differs from the construction
by Farina et al. (2021a) in that we have to additionally guarantee and preserve the stability—and sub-
sequently the predictivity—throughout the construction. The second component consists of sharply
characterizing the stability of ﬁxed points of trigger deviation functions. This turns out to be par-
ticularly challenging, and direct extensions of prior techniques appear to only give a bound that is
exponential in the size of the game. In this context, one of our key technical contributions is to pro-
vide a reﬁned perturbation analysis for a Markov chain consisting of a rank-one stochastic matrix,
employing tools that have not been used before in this line of work, and substantially extending the
techniques of Chen & Peng (2020). This leads to a rate of convergence that depends polynomially"
INTRODUCTION,0.0438871473354232,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.047021943573667714,"on the description of the game, which is crucial for the applicability of the accelerated dynamics.
Finally, we support our theoretical ﬁndings with experiments on several general-sum benchmarks."
INTRODUCTION,0.050156739811912224,"Further Related Work.
The line of work on accelerated no-regret learning for Nash equilibrium
was pioneered by Daskalakis et al. (2015), showing that one can bypass the adversarial ⌦(T −1/2)
barrier for the incurred average regret if both players in a zero-sum game employ an uncoupled vari-
ant of the excessive gap technique (Nesterov, 2005), leading to a near-optimal rate of O(log T/T).
Subsequently, Rakhlin & Sridharan (2013a) showed that the optimal rate of O(1/T) can be obtained
with a remarkably simple variant of Online Mirror Descent which incorporates a prediction term in
the update step. While these results only hold for zero-sum games, Syrgkanis et al. (2015) showed
that O(T −3/4) rate can be obtained for multiplayer general-sum normal-form games. In a recent
result, Chen & Peng (2020) strengthened the regret bounds of Syrgkanis et al. (2015) from external
to swap regret using the celebrated construction of Blum & Mansour (2007). We also acknowledge
a recent result of Daskalakis et al. (2021) which establishes a near-optimal rate of convergence of"
INTRODUCTION,0.05329153605015674,"eO(1/T) to a coarse correlated equilibrium when all players employ the Optimistic Multiplicative
Weights Update (OMWU) algorithm in a normal-form game. Extending their result to extensive-
form games presents considerable technical challenges since their analysis crucially hinges on the
closed-form softmax-type structure of OMWU on the simplex."
INTRODUCTION,0.05642633228840126,"Correlated equilibrium in extensive-form games is much less understood than Nash equilibrium.A
feasible EFCE can also be computed efﬁciently through a variant of the Ellipsoid algorithm (Pa-
padimitriou & Roughgarden, 2008; Jiang & Leyton-Brown, 2015), and an alternative sampling-
based approach was given by Dudík & Gordon (2009). However, those approaches perform poorly
in large-scale problems, and do not allow the players to arrive at EFCE via distributed learning.
Celli et al. (2019) devised variants of the CFR algorithm that provably convergence to normal-form
coarse correlated equilibria, a solution concept much less appealing than EFCE in extensive-form
games Gordon et al. (2008). Finally, Morrill et al. (2021a;b) characterize hindsight rationality no-
tions and associate a set of solution concepts with suitable O(T −1/2) no-regret learning dynamics."
PRELIMINARIES,0.05956112852664577,"2
PRELIMINARIES"
PRELIMINARIES,0.06269592476489028,"Extensive-form Games. An extensive-form game is abstracted on a directed and rooted game tree
T . The set of nodes of T is denoted with H; non-terminal nodes are referred as decision nodes,
and are associated with a player who acts by selecting an action from a set of possible actions A(h),
where h 2 H represents the decision node. By convention, the set of players [n] [ {c} includes a
ﬁctitious agent c who “selects” actions according to ﬁxed probability distributions dictated by the
nature of the game (e.g., the roll of a dice); this intends to model external stochastic phenomena
occurring during the game. For a player i 2 [n] [ {c}, we let H(i) ✓H be the subset of decision
nodes wherein a player i makes a decision. The set of leaves Z ✓H, or equivalently the terminal
nodes, correspond to different outcomes; once the game transitions to a terminal node z 2 Z, payoffs
are assigned to each player based on a set of normalized utility functions {u(i) : Z ! [−1, 1]}i2[n].
It will also be convenient to represent with p(c)(z) the product of probabilities of “chance” moves
encountered in the path from the root until the terminal node z 2 Z."
PRELIMINARIES,0.06583072100313479,"Imperfect Information.
To model imperfect information, the set of decision nodes H(i) of player
i are partitioned into a collection of sets J (i), which are called information sets. Each information
set j 2 J (i) groups nodes which cannot be distinguished by i. Thus, for any nodes h, h0 2 j we
have A(h) = A(h0). As usual, we assume that the game satisﬁes perfect recall: players never
forget information once acquired. We will also deﬁne a partial order ≺on J (i), so that j ≺j0, for
j, j0 2 J (i), if there exist nodes h 2 j and h0 2 j0 such that the path from the root to h0 passes
through h. If j ≺j0, we will say that j is an ancestor of j0, or equivalently, j is a descendant of j0."
PRELIMINARIES,0.06896551724137931,"Sequence-form Strategies.
For a player i 2 [n], an information set j 2 J (i), and an action
a 2 A(j), we will denote with σ = (j, a) the sequence of i’s actions encountered on the path from
the root of the game until (and included) action a. For notational convenience, we will use the special
symbol ? to denote the empty sequence. Then, i’s set of sequences is deﬁned as ⌃(i) := {(j, a) : j 2
J (i), a 2 A(j)} [ {?}; we will also use the notation ⌃(i)"
PRELIMINARIES,0.07210031347962383,"⇤
:= ⌃(i) \ {?}. For a given information
set j 2 J (i) we will use σ(i)(j) 2 ⌃(i) to represent the parent sequence; i.e. the last sequence"
PRELIMINARIES,0.07523510971786834,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.07836990595611286,"encountered by player i before reaching any node in the information set j, assuming that it exists.
Otherwise, we let σ(i)(j) = ?, and we say that j is the root information set of player i. A strategy
for a player speciﬁes a probability distribution for every possible information set encountered in the
game tree. For perfect-recall EFGs, strategies can be equivalently represented in sequence-form:
Deﬁnition 2.1 (Sequence-form Polytope). The sequence-form strategy polytope for player i 2 [n]
is deﬁned as the following (convex) polytope:"
PRELIMINARIES,0.08150470219435736,Q(i) := ⇢
PRELIMINARIES,0.08463949843260188,q 2 R|⌃(i)|
PRELIMINARIES,0.0877742946708464,"≥0
: q[?] = 1,
q[σ(i)(j)] = X"
PRELIMINARIES,0.09090909090909091,a2A(j)
PRELIMINARIES,0.09404388714733543,"q[(j, a)],
8j 2 J (i) $ .
(1)"
PRELIMINARIES,0.09717868338557993,"Analogously, one can deﬁne the sequence-form strategy polytope for the subtree of the partially
ordered set (J (i), ≺) rooted at j 2 J ((i), which will be denoted as Q(i)"
PRELIMINARIES,0.10031347962382445,"j . Moreover, the set of"
PRELIMINARIES,0.10344827586206896,"deterministic sequence-form strategies for player i 2 [n] is the set ⇧(i) = Q(i) \ {0, 1}|⌃(i)|, and
similarly for ⇧(i)"
PRELIMINARIES,0.10658307210031348,"j . The joint set of deterministic sequence-form strategies of the players will be
represented with ⇧:= ⇥i2[n] ⇧(i). As such, an element ⇡2 ⇧is an n-tuple (⇡(1), . . . , ⇡(n))
specifying a deterministic sequence-form strategy for every player i 2 [n]. Finally, the utility of
player i 2 [n] under a proﬁle ⇡2 ⇧can be expressed as"
PRELIMINARIES,0.109717868338558,u(i)(⇡) := X z2Z
PRELIMINARIES,0.11285266457680251,"p(c)(z)u(i)(z)1{⇡(k)[σ(k)(z)] = 1, 8k 2 [n]}.
(2)"
PRELIMINARIES,0.11598746081504702,We summarized in Table 1 the EFG notation that we will be using most often throughout the paper.
PRELIMINARIES,0.11912225705329153,"An Illustrative Example. To clarify some of the concepts we have introduced, we illustrate a simple
two-player EFG in Figure 1. Black nodes belong to player 1, white round nodes to player 2, square
nodes are terminal nodes (aka leaves), and the crossed node is a chance node. Player 2 has two
information sets, J (2) := {C, D}, each containing two nodes. This captures the lack of knowledge
regarding the action played by player 1. In contrast, the outcome of the chance move is observed by
both players. At the information set C, player 2 has two possible actions, A(C) := {5, 6}. Thus,
one possible sequence for player 2 is the pair σ = (C, 5) 2 ⌃(2). 1 5 2 5
6
6 3 7 4 7
8
8 A
B C
D"
PRELIMINARIES,0.12225705329153605,Figure 1: Example of a two-player EFG.
PRELIMINARIES,0.12539184952978055,Description
PRELIMINARIES,0.12852664576802508,"J (i)
Information sets of player i
A(j)
Actions at information set j
⌃(i)
Set of sequences of player i
Q(i)"
PRELIMINARIES,0.13166144200626959,"j
Sequence-form strategies rooted at j 2 J (i)"
PRELIMINARIES,0.13479623824451412,"D(i)
Maximum depth of any j 2 J (i)"
PRELIMINARIES,0.13793103448275862,Table 1: Summary of the basic notation.
PRELIMINARIES,0.14106583072100312,"Regret, Φ-Regret and Optimistic Regret Minimization. Consider a convex and compact set X ✓
Rd representing the space of strategies of some agent. In the online decision making framework,
a regret minimizer R can be thought of as a black-box device which interacts with the external
environment via the following two basic subroutines:"
PRELIMINARIES,0.14420062695924765,"• R. NEXTSTRATEGY(): The regret minimizer returns the strategy xt 2 X at time t;
• R. OBSERVEUTILITY(`t): The regret minimizer receives as feedback a linear utility function"
PRELIMINARIES,0.14733542319749215,"`t : X 3 x 7! h`t, xi, and may alter its internal state accordingly."
PRELIMINARIES,0.15047021943573669,"The decision making is online in the sense that the regret minimizer can adapt to previously received
information, but no information about future utilities is available. The error of a regret minimizer is
typically measured in terms of external regret, deﬁned, for a time horizon T, as follows:"
PRELIMINARIES,0.1536050156739812,"RT := max x⇤2X T
X t=1"
PRELIMINARIES,0.15673981191222572,"hx⇤, `ti − T
X t=1"
PRELIMINARIES,0.15987460815047022,"hxt, `ti,
(3)"
PRELIMINARIES,0.16300940438871472,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.16614420062695925,"That is, the performance of the online algorithm is compared with the best ﬁxed strategy in hindsight."
PRELIMINARIES,0.16927899686520376,"Φ-Regret.
A conceptual generalization of the concept of external regret is the so-called Φ-regret.
Speciﬁcally, in this framework the performance of the learning algorithm is measured based on a set
of transformations Φ : X ! X, leading to the notion of cumulative Φ-regret:"
PRELIMINARIES,0.1724137931034483,"RT := max φ⇤2Φ T
X t=1"
PRELIMINARIES,0.1755485893416928,"hφ⇤(xt), `ti − T
X t=1"
PRELIMINARIES,0.1786833855799373,"hxt, `ti.
(4)"
PRELIMINARIES,0.18181818181818182,"When the set of transformations Φ coincides with the set of constant functions, one recovers the
notion of external regret given in Equation (3). However, Φ-regret is substantially more expressive
and yields a more appealing notion of hindsight rationality (Gordon et al., 2008), incorporating the
notion of swap regret (Blum & Mansour, 2007)."
PRELIMINARIES,0.18495297805642633,"We will employ the following deﬁnition, which is a slight modiﬁcation of the RVU property intro-
duced by (Syrgkanis et al., 2015, Deﬁnition 3).
Deﬁnition 2.2 (Stable-predictivity). Let R be a regert minimizer and let k · k be a norm. R is said
to be -stable with respect to k · k if for all t ≥2, the strategies output by R satisfy"
PRELIMINARIES,0.18808777429467086,"kxt −xt−1k ,
(5)"
PRELIMINARIES,0.19122257053291536,"Moreover, it is said to be (↵, β)-predictive with respect to k · k if for all t ≥1 its regret RT satisﬁes"
PRELIMINARIES,0.19435736677115986,"RT ↵(T) + β T
X t=2"
PRELIMINARIES,0.1974921630094044,k`t −`t−1k2
PRELIMINARIES,0.2006269592476489,"⇤,
(6)"
PRELIMINARIES,0.20376175548589343,"no matter the sequence of utility vectors `1, . . . , `T , where k · k⇤is the dual norm of k · k."
PRELIMINARIES,0.20689655172413793,"Optimistic Follow the Regularized Leader.
Let d be a 1-strongly convex function with respect to
some norm k · k, and ⌘> 0 the learning rate. OFTRL’s update rule takes the following form:"
PRELIMINARIES,0.21003134796238246,xt := arg max x2X (*
PRELIMINARIES,0.21316614420062696,"x, 2`t−1 + t−2
X ⌧=1 `⌧ + −d(x) ⌘ )"
PRELIMINARIES,0.21630094043887146,",
(OFTRL)"
PRELIMINARIES,0.219435736677116,"where x1 := arg minx2X d(x). Syrgkanis et al. (2015) established the following property:
Lemma 2.3. (OFTRL) is 2⌘-stable and (⌦d/⌘, ⌘)-predictive with respect to any norm k·k for which
d is 1-strongly convex, where ⌦d is the range of d on X, that is, ⌦d := maxx,x02X {d(x) −d(x0)}."
PRELIMINARIES,0.2225705329153605,"In this paper, we consider the entropic regularizer with respect to the simplex d(x)
:=
Pd"
PRELIMINARIES,0.22570532915360503,"i=1 xi log xi, which is 1-strongly convex with respect to the `1 norm. The pair of dual norms
in the predictivity bound will therefore be (k · k1, k · k1). We call this OFTRL setup Optimistic
Multiplicative Weights Updates (OMWU)."
PRELIMINARIES,0.22884012539184953,"Extensive-Form Correlated Equilibrium.
We will work with the deﬁnition of EFCE due to
Farina et al. (2019e), which is equivalent to that of von Stengel & Forges (2008). First, let us
introduce the concept of a trigger deviation function."
PRELIMINARIES,0.23197492163009403,"Deﬁnition 2.4. Consider some player i 2 [n], a sequence ˆσ = (j, a) 2 ⌃(i)"
PRELIMINARIES,0.23510971786833856,"⇤, and joint sequence-
form strategies ⇡2 ⇧(i)"
PRELIMINARIES,0.23824451410658307,j . A trigger deviation function with respect to a trigger sequence ˆσ and
PRELIMINARIES,0.2413793103448276,continuation strategy ˆ⇡is any linear function f : R|⌃(i)| ! R|⌃(i)| with the following properties.
PRELIMINARIES,0.2445141065830721,"• Any strategy ⇡2 ⇧(i) which does not prescribe the sequence ˆσ remains invariant. That is,"
PRELIMINARIES,0.2476489028213166,"f(⇡) = ⇡for any ⇡2 ⇧(i) such that ⇡[ˆσ] = 0;
• Otherwise, the prescribed sequence ˆσ = (j, a) is modiﬁed so that the behavior at j, as well as"
PRELIMINARIES,0.2507836990595611,all its descendants is replaced by the behavior speciﬁed by the continuation strategy:
PRELIMINARIES,0.25391849529780564,f(⇡)[σ] =
PRELIMINARIES,0.25705329153605017,"⇢⇡[σ]
if σ 6⌫j;
ˆ⇡[σ]
if σ ⌫j,
(7)"
PRELIMINARIES,0.2601880877742947,for all σ 2 ⌃(i) and ⇡2 ⇧(i) such that ⇡[ˆσ] = 1.
PRELIMINARIES,0.26332288401253917,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.2664576802507837,We will let  (i) := {φ(i)
PRELIMINARIES,0.26959247648902823,"ˆσ!ˆ⇡: ˆσ = (j, a) 2 ⌃(i)"
PRELIMINARIES,0.2727272727272727,"⇤, ˆ⇡2 ⇧(i)"
PRELIMINARIES,0.27586206896551724,"j } be the set of all possible linear mappings
deﬁning trigger deviation functions for player i. We are ready to introduce the concept of EFCE."
PRELIMINARIES,0.27899686520376177,"Deﬁnition 2.5 (EFCE). For ✏≥0, a probability distribution µ 2 ∆|⇧| is an ✏-approximate EFCE
if for every player i 2 [n] and every trigger deviation function φ(i)"
PRELIMINARIES,0.28213166144200624,"ˆσ!ˆ⇡2  (i), it holds that E⇡⇠µ h"
PRELIMINARIES,0.2852664576802508,u(i) ⇣ φ(i)
PRELIMINARIES,0.2884012539184953,"ˆσ!ˆ⇡(⇡(i)), ⇡(−i)⌘"
PRELIMINARIES,0.29153605015673983,−u(i)(⇡) i
PRELIMINARIES,0.2946708463949843,"✏,
(8)"
PRELIMINARIES,0.29780564263322884,"where ⇡= (⇡1, . . . , ⇡n) 2 ⇧. A probability distribution µ 2 ∆|⇧| is an EFCE if it is a 0-EFCE."
PRELIMINARIES,0.30094043887147337,"Theorem 2.6 (Farina et al. (2021a)). For every player i 2 [n], let ⇡(i),1, . . . , ⇡(i),T 2 ⇧(i) be
a sequence of deterministic sequence-form strategies whose cumulative  (i)-regret is R(i),T with
respect to the sequence of linear utility functions"
PRELIMINARIES,0.30407523510971785,"`(i),t : ⇧(i) 3 ⇡(i) 7! u(i) ⇣"
PRELIMINARIES,0.3072100313479624,"⇡(i), ⇡(−i),t⌘ .
(9)"
PRELIMINARIES,0.3103448275862069,"Then, the empirical frequency of play µ 2 ∆|⇧| is an ✏-EFCE, where ✏:= 1"
PRELIMINARIES,0.31347962382445144,"T maxi2[n] R(i),T ."
PRELIMINARIES,0.3166144200626959,"3
ACCELERATING Φ-REGRET MINIMIZATION VIA OPTIMISM"
PRELIMINARIES,0.31974921630094044,"In this section we develop a general template for accelerated Φ-regret minimization for general sets,
and then we instantiate the template for dynamics for EFCE. Our approach combines a framework of
Gordon et al. (2008) with the framework of stable-predictive (aka. optimistic) regret minimization.
As in Gordon et al. (2008), in our template we combine 1) a regret minimizer that outputs a linear
transformation φt 2 Φ at every time t, and 2) a ﬁxed-point oracle for each φt 2 Φ. However, in our
framework, we further require that 2) is stable (in the sense of Deﬁnition 2.2). To achieve this, we
will focus on regret minimizers that have the following property:
Deﬁnition 3.1. Consider a set of functions Φ such that φ(X) ✓X for all φ 2 Φ, and a no-regret
algorithm RΦ for the set of transformations Φ which returns a sequence φt 2 Φ. We say that RΦ is
ﬁxed point G-stable, for G ≥0, if the following conditions hold:"
PRELIMINARIES,0.322884012539185,"• Every φt admits a ﬁxed point. That is, there exists xt 2 X such that φt(xt) = xt.
• For any xt such that xt = φt(xt), there exists xt+1 with xt+1 = φt+1(xt+1) such that"
PRELIMINARIES,0.32601880877742945,kxt+1 −xtk G.
PRELIMINARIES,0.329153605015674,We will show how to construct an accelerated Φ-regret minimizer starting from the following:
PRELIMINARIES,0.3322884012539185,"1. RΦ: A -stable (↵, β)-predictive ﬁxed point G-stable regret minimizer for Φ;
2. STABLEFPORACLE(φ; ex, G, ✏): A stable ﬁxed point oracle which returns a point x 2 X such"
PRELIMINARIES,0.335423197492163,"that (i) kφ(x)−xk ✏, and (ii) kx−exk G (the existence of such a ﬁxed point is guaranteed
by the ﬁxed point G-stability assumption for the regret minimizer)."
PRELIMINARIES,0.3385579937304075,"Given these two components, our next theorem builds a stable-predictive Φ-regret minimizer.
Theorem 3.2 (Accelerated Φ-Regret Minimization). Consider a -stable (↵, β)-predictive regret
minimizer RΦ for a set of linear transformations Φ, with respect to the `1 norm k · k1. More-
over, assume that RΦ is ﬁxed point G-stable with respect to Φ. Then, if we have access to a
STABLEFPORACLE, we can construct a G-stable algorithm with Φ-regret RT bounded as"
PRELIMINARIES,0.34169278996865204,RT ↵(T) + 2βD2
PRELIMINARIES,0.3448275862068966,"`2T + 2β T
X t=2"
PRELIMINARIES,0.34796238244514105,k`t −`t−1k2
PRELIMINARIES,0.3510971786833856,"1 + D` T
X t=1"
PRELIMINARIES,0.3542319749216301,"✏t,
(10)"
PRELIMINARIES,0.3573667711598746,"where ✏t is the error of STABLEFPORACLE at time t, and D` is an upper bound on the `1 norm of
`t’s. It is also assumed that kxk1 1 for all x 2 X."
PRELIMINARIES,0.3605015673981191,"The proof is similar to that of Gordon et al. (2008), and is included in Appendix B."
PRELIMINARIES,0.36363636363636365,"3.1
CONSTRUCTING A STABLE-PREDICTIVE REGRET MINIMIZER FOR  (i)"
PRELIMINARIES,0.3667711598746082,"Here we develop a regret minimizer for the set co  (i), the convex hull of the set of trigger deviation
functions. Given that co  (i) ◆ (i), this will immediately imply a regret minimizer for the set"
PRELIMINARIES,0.36990595611285265,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.3730407523510972,"CFR
(OMWU)"
PRELIMINARIES,0.3761755485893417,"CFR
(OMWU) R(i) ∆ R(i)"
PRELIMINARIES,0.3793103448275862,"1
(Proposition 3.3) R(i)"
PRELIMINARIES,0.3824451410658307,"m
(Proposition 3.3)"
PRELIMINARIES,0.38557993730407525,"`(i), t
L(i), t
φ(i),t
x(i),t R(i)"
PRELIMINARIES,0.3887147335423197,"(Theorem 3.4)
 (i)-Regret Minimizer for Q(i)"
PRELIMINARIES,0.39184952978056425,"x(i),t φ(i) 1!qt 1 φ(i) m!qtm Fixed point"
PRELIMINARIES,0.3949843260188088,Figure 2: An overview of the overall construction. For notational convenience we have let ⌃(i)
PRELIMINARIES,0.3981191222570533,"⇤
:=
{1, 2, . . . , m}. The symbol ⌦in the ﬁgure denotes a multilinear transformation of the inputs. We
also note that blue corresponds to the iterates, while red corresponds to the utilities."
PRELIMINARIES,0.4012539184952978,"(i). An overview of the algorithm is given in Figure 2. Farina et al. (2021a) observed that the
set co  (i) can be evaluated in two stages. First, for a ﬁxed sequence ˆσ = (j, a) 2 ⌃(i)"
PRELIMINARIES,0.4043887147335423,"⇤
we deﬁne"
PRELIMINARIES,0.40752351097178685,the set  (i)
PRELIMINARIES,0.4106583072100313,"ˆσ
:= co n"
PRELIMINARIES,0.41379310344827586,φˆσ!ˆ⇡: ˆ⇡2 ⇧(i) j o
PRELIMINARIES,0.4169278996865204,"; then, we take the convex hull of all  (i)"
PRELIMINARIES,0.4200626959247649,"ˆσ , that is, co  (i) ="
PRELIMINARIES,0.4231974921630094,co{ (i)
PRELIMINARIES,0.4263322884012539,ˆσ : ˆσ 2 ⌃(i)
PRELIMINARIES,0.42946708463949845,"⇤}. Correspondingly, we ﬁrst develop a stable-predictive regret minimizer for the
set  (i)"
PRELIMINARIES,0.43260188087774293,"ˆσ , for any ˆσ 2 ⌃(i)"
PRELIMINARIES,0.43573667711598746,"⇤, and these individual regret minimizers are then combined using a regret
circuit to conclude the construction in Theorem 3.4. All the omitted poofs and pseudocode for this
section are included in Appendix B.1."
PRELIMINARIES,0.438871473354232,Stable-Predictive Regret Minimizer for the set  (i)
PRELIMINARIES,0.44200626959247646,ˆσ . Consider a sequence ˆσ 2 ⌃(i)
PRELIMINARIES,0.445141065830721,⇤. Farina et al.
PRELIMINARIES,0.4482758620689655,(2021a) observed that the set of transformations  (i)
PRELIMINARIES,0.45141065830721006,"ˆσ
:= co n"
PRELIMINARIES,0.45454545454545453,φˆσ!ˆ⇡: ˆ⇡2 ⇧(i) j o
PRELIMINARIES,0.45768025078369906,is the image of Q(i)
PRELIMINARIES,0.4608150470219436,"j
under the afﬁne mapping h(i)"
PRELIMINARIES,0.46394984326018807,ˆσ : q 7! φ(i)
PRELIMINARIES,0.4670846394984326,"ˆσ!q. Hence, it is well-known that a regret minimizer for (i)"
PRELIMINARIES,0.4702194357366771,ˆσ can be constructed starting from a regret minimizer for Q(i)
PRELIMINARIES,0.47335423197492166,"j . We now show that the same can
be said if one restricts to stable-predictive regret minimizers. In particular, we have the following."
PRELIMINARIES,0.47648902821316613,"Proposition 3.3. Consider a player i 2 [n] and any trigger sequence ˆσ = (j, a) 2 ⌃(i)"
PRELIMINARIES,0.47962382445141066,"⇤. There
exists an algorithm which constructs a deterministic regret minimizer R(i)"
PRELIMINARIES,0.4827586206896552,"ˆσ with access to a K-stable
(AT , B)-predictive deterministic regret minimizer R(i)"
PRELIMINARIES,0.48589341692789967,Q for the set Q(i)
PRELIMINARIES,0.4890282131661442,"j , such that R(i)"
PRELIMINARIES,0.49216300940438873,"ˆσ is K-stable
and (AT , B)-predictive."
PRELIMINARIES,0.4952978056426332,In Appendix A we describe a stable-predictive variant of CFR for the set Q(i)
PRELIMINARIES,0.49843260188087773,"j , for each j 2 J (i),
following the construction of Farina et al. (2019a)."
PRELIMINARIES,0.5015673981191222,"Stable-Predictive Regret Minimizer for co  (i).
The next step consists of combining the regret
minimizers  (i)"
PRELIMINARIES,0.5047021943573667,"ˆσ , for all ˆσ 2 ⌃(i)"
PRELIMINARIES,0.5078369905956113,"⇤, to a composite regret minimizer for the set co  (i). To this end,
we employ regret circuits (Farina et al., 2019d), leading to the main result of this section:"
PRELIMINARIES,0.5109717868338558,"Theorem 3.4. Consider a -stable (↵, β)-predictive regret minimizer R(i)"
PRELIMINARIES,0.5141065830721003,"∆for the the simplex
∆|⌃(i)"
PRELIMINARIES,0.5172413793103449,"⇤|, and K-stable (A, B)-predictive regret minimizers R(i)"
PRELIMINARIES,0.5203761755485894,"ˆσ
for each ˆσ 2 ⌃(i)"
PRELIMINARIES,0.5235109717868338,"⇤, all with re-
spect to the pair of norms (k · k1, k · k1). Then, there exists an algorithm which constructs a regret
minimizer R(i)"
PRELIMINARIES,0.5266457680250783,for the set co  (i) such that (i) R(i)
PRELIMINARIES,0.5297805642633229,"is O(K + |⌃(i)|)-stable, and (ii) under any
sequence of linear utility functions L1, . . . , LT the regret incurred can be bounded as RT"
PRELIMINARIES,0.5329153605015674,O(↵(T) + A(T) + βD2
PRELIMINARIES,0.5360501567398119,"LK2T) + O(B + β|⌃(i)|2) T
X t=2"
PRELIMINARIES,0.5391849529780565,kLt −Lt−1k2
PRELIMINARIES,0.542319749216301,"1,
(11)"
PRELIMINARIES,0.5454545454545454,where kLtk1 DL.
PRELIMINARIES,0.54858934169279,Under review as a conference paper at ICLR 2022
STABILITY OF THE FIXED POINTS,0.5517241379310345,"3.2
STABILITY OF THE FIXED POINTS"
STABILITY OF THE FIXED POINTS,0.554858934169279,"In this subsection we complete the construction of the  (i)-regret minimizer by establishing a stable
ﬁxed point oracle for any φ 2 co  (i). All of the proofs of this section are included in Appendix B.2."
STABILITY OF THE FIXED POINTS,0.5579937304075235,"Multiplicative Stability.
A sequence {zt}, with zt 2 Rd"
STABILITY OF THE FIXED POINTS,0.5611285266457681,"≥0, is said to be -multiplicative-stable
if (1 −)zt−1 i
zt"
STABILITY OF THE FIXED POINTS,0.5642633228840125,i (1 + )zt−1
STABILITY OF THE FIXED POINTS,0.567398119122257,"i
, for any i 2 [d], and for all t ≥2. Importantly, this notion of
multiplicative stability is guaranteed by OMWU (see Lemma B.2). Thus, if D(i) is the depth of i’s
actions and D(i)"
STABILITY OF THE FIXED POINTS,0.5705329153605015,"x is an upper bound on the `1 norm in the treeplex, we can show the following:"
STABILITY OF THE FIXED POINTS,0.5736677115987461,Lemma 3.5. When each regret minimizer R(i)
STABILITY OF THE FIXED POINTS,0.5768025078369906,"ˆσ
is constructed using predictive CFR instantiated
with OMWU with learning rate ⌘(Theorem A.4) such that for all ˆσ 2 ⌃(i)"
STABILITY OF THE FIXED POINTS,0.5799373040752351,"⇤, the output sequence is
O(⌘(D(i))2D(i)"
STABILITY OF THE FIXED POINTS,0.5830721003134797,"x D`)-multiplicatively-stable. Moreover, if the regret minimizer R(i)"
STABILITY OF THE FIXED POINTS,0.5862068965517241,"∆is realized using
OMWU with learning rate ⌘, it will output an O(⌘|⌃(i)|D`)-multiplicatively-stable sequence."
STABILITY OF THE FIXED POINTS,0.5893416927899686,"This characterization will be crucial for establishing the stability of the ﬁxed points. In particular,
following the approach of Farina et al. (2021a), let us introduce the following deﬁnitions:
Deﬁnition 3.6. Consider a player i 2 [n] and let J ✓J (i) be a subset of i’s information sets. We
say than J is a trunk of J (i) if, for every j 2 J, all predecessors of j are also in J."
STABILITY OF THE FIXED POINTS,0.5924764890282131,"Deﬁnition 3.7. Consider a player i 2 [n], a trunk J ✓J (i), and φ 2 co  (i). A vector x 2 R|⌃(i)|"
STABILITY OF THE FIXED POINTS,0.5956112852664577,"≥0
is a J-partial ﬁxed point of φ if the following conditions hold:"
STABILITY OF THE FIXED POINTS,0.5987460815047022,• x[?] = 1 and x[σ(i)(j)] = P
STABILITY OF THE FIXED POINTS,0.6018808777429467,"a2A(j) x[(j, a)], for all j 2 J;
• φ(x)[?] = x[?] = 1, and φ(x)[(j, a)] = x[(j, a)], for all j 2 J, and a 2 A(j)."
STABILITY OF THE FIXED POINTS,0.6050156739811913,"An important property is that a J-partial ﬁxed point can be efﬁciently “promoted” to a J [ {j⇤}-
partial ﬁxed point by computing the stationary distribution of a certain Markov chain. However, a
signiﬁcant concern is whether this ﬁxed point operation can potentially cause a substantial degrada-
tion in terms of stability. One of our key results is that the associated Markov chain has a particular
structure, which enables us to substantially improve the stability bound and thereby obtain a poly-
nomial degradation in stability. More precisely, this boils down to the following technical lemma.
Lemma 3.8. Let M and M0 be transition matrices of m-state Markov chains such that M =
v1> + C and M0 = v01> + C0, where C, C0, v, v0 have strictly positive entries. Moreover, let ⇡
and ⇡0 be the (unique) stationary distributions of M and M0 respectively. Then, if (i) the entries
of the matrices C and C0 are -multiplicatively-close, (ii) the entries of the vectors v and v0 are
γ-multiplicatively-close, and (iii) the sum of the entries of v and v0 are -multiplicatively-close,
then ⇡and ⇡0 are (γ + O(m))-multiplicatively-close, for a sufﬁciently small = O(1/m)."
STABILITY OF THE FIXED POINTS,0.6081504702194357,"Using a slightly more general result (Corollary B.10), we manage to obtain the following:"
STABILITY OF THE FIXED POINTS,0.6112852664576802,"Proposition 3.9. Consider a player i 2 [n], and let φ = P"
STABILITY OF THE FIXED POINTS,0.6144200626959248,ˆσ2⌃(i)
STABILITY OF THE FIXED POINTS,0.6175548589341693,⇤λ[ˆσ]φ(i)
STABILITY OF THE FIXED POINTS,0.6206896551724138,ˆσ!qˆσ be a transformation
STABILITY OF THE FIXED POINTS,0.6238244514106583,in co  (i) such that the sequence of λt’s and qt
STABILITY OF THE FIXED POINTS,0.6269592476489029,"ˆσ’s is -multiplicatively-stable, for all ˆσ 2 ⌃(i)"
STABILITY OF THE FIXED POINTS,0.6300940438871473,"⇤. If xt
is a γ-multiplicatively-stable J-partial ﬁxed point sequence, there is an algorithm which computes
a (J [ {j⇤})-partial ﬁxed point (xt)0 of φ such that the sequence of (x0)t’s is (γ + O(|A(j⇤)|))-
multiplicatively-stable, for any sufﬁciently small = O(1/|A(j⇤)|)."
STABILITY OF THE FIXED POINTS,0.6332288401253918,"Thus, using our technical lemma, we manage to bypass the substantial overhead of the term
γ|A(j⇤)|, which would follow using techniques similar to Chen & Peng (2020). This turns out to be
crucial for obtaining a polynomial dependence on the size of the game. Finally, we can inductively
employ this proposition to show the overall stability of the ﬁxed points:"
STABILITY OF THE FIXED POINTS,0.6363636363636364,"Theorem 3.10. Consider a player i 2 [n], and let φ = P"
STABILITY OF THE FIXED POINTS,0.6394984326018809,ˆσ2⌃(i)
STABILITY OF THE FIXED POINTS,0.6426332288401254,⇤λ[ˆσ]φ(i)
STABILITY OF THE FIXED POINTS,0.64576802507837,ˆσ!qˆσ be a transformation in
STABILITY OF THE FIXED POINTS,0.6489028213166145,co  (i) such that the sequence of λt’s and qt
STABILITY OF THE FIXED POINTS,0.6520376175548589,"ˆσ’s is -multiplicatively-stable, for all ˆσ 2 ⌃(i)"
STABILITY OF THE FIXED POINTS,0.6551724137931034,"⇤. Then,
there exists an algorithm which computes a ﬁxed point qt 2 Q(i) of φ such that the sequence of qt’s
is O(|A(i)|D(i))-multiplicatively-stable, where |A(i)| := maxj2J (i) |A(j)|, and for a sufﬁciently
small = O(1/(|A(i)|D(i)))."
STABILITY OF THE FIXED POINTS,0.658307210031348,"Finally, if we use the stability values derived in Lemma 3.5, we arrive at the following conclusion:"
STABILITY OF THE FIXED POINTS,0.6614420062695925,Under review as a conference paper at ICLR 2022
STABILITY OF THE FIXED POINTS,0.664576802507837,Corollary 3.11. For = O((D(i)
STABILITY OF THE FIXED POINTS,0.6677115987460815,"x (D(i))2 + |⌃(i)|)|A(i)|D(i)D`), the sequence of ﬁxed points will
be (⌘)-multiplicatively-stable, for any sufﬁciently small ⌘= O(1/)."
STABILITY OF THE FIXED POINTS,0.670846394984326,"Putting Everything Together. Finally, having established these ingredients, we can use the template
of Theorem 3.2 to obtain Theorem 1.1, as we formally show in Appendix B.3."
EXPERIMENTS,0.6739811912225705,"4
EXPERIMENTS"
EXPERIMENTS,0.677115987460815,"In this section we experimentally investigate the performance of our stable-predictive algorithm
compared to two other popular approaches based on a CFR-style decomposition of regrets into lo-
cal regret-minimization problems: the existing algorithm by Farina et al. (2021a) instantiated with
(i) regret matching+ (RM+) (Tammelin, 2014) for each simplex (in place of regret matching), and
(ii) using the vanilla MWU algorithm for each simplex. In accordance to the theoretical predic-
tions, the stepsize for OMWU is set as ⌘t = ⌧· t−1/4 (cf. Corollary B.13), and for MWU it is set
as ⌘t = ⌧· t−1/2, where the parameter ⌧is chosen by picking the best-performing value among
{0.01, 0.1, 1, 10, 100}. In particular, we evaluate their performance based on the following popular
benchmark games: (i) a three-player variant of Kuhn poker (Kuhn, 1950); (ii) a two-player bargain-
ing game known as Sheriff (Farina et al., 2019e)—a benchmark game introduced speciﬁcally for the
study of correlated equilibria; and (iii) a three-player version of Liar’s dice (Lisý et al., 2015). A
detailed description of each of the three game instances is available in Appendix D."
EXPERIMENTS,0.6802507836990596,"0
5000
10000
Iteration 10−3"
EXPERIMENTS,0.6833855799373041,EFCE gap
EXPERIMENTS,0.6865203761755486,Kuhn poker
EXPERIMENTS,0.6896551724137931,"OMWU (⌧= 10)
MWU (⌧= 10)
RM+"
EXPERIMENTS,0.6927899686520376,"0
5000
10000
Iteration 10−2 10−1"
EXPERIMENTS,0.6959247648902821,Sheriff
EXPERIMENTS,0.6990595611285266,"OMWU (⌧= 1)
MWU (⌧= 1)
RM+"
EXPERIMENTS,0.7021943573667712,"0
500
1000
Iteration 10−2 10−1"
EXPERIMENTS,0.7053291536050157,Liar’s dice
EXPERIMENTS,0.7084639498432602,"OMWU (⌧= 100)
MWU (⌧= 100)
RM+"
EXPERIMENTS,0.7115987460815048,"Figure 3: The performance of MWU, OMWU, and RM+ on three general-sum EFGs."
EXPERIMENTS,0.7147335423197492,"Figure 3 shows the performance of each of the three learning dynamics for computing EFCE. On
the x-axis we plot the number of iterations performed by each algorithm, and on the y-axis we plot
the EFCE gap, deﬁned as the maximum advantage that any player can gain by defecting optimally
from the mediator’s recommendations. It should be noted that one iteration costs the same for every
algorithm, up to constant factors. We see that on every game, OMWU performs better than or on
par with RM+ and MWU. On Sheriff, OMWU performs signiﬁcantly better than both RM+ and
MWU, by about an order of magnitude. One caveat to these results is that we did not use two tricks
that help CFR+ in two-player zero-sum EFG solving: alternation and linear averaging. These tricks
are known to retain convergence guarantees in that context (Tammelin et al., 2015; Farina et al.,
2019b; Burch et al., 2019), but it is unclear if they still guarantee convergence in the EFCE setting."
CONCLUSIONS,0.7178683385579937,"5
CONCLUSIONS"
CONCLUSIONS,0.7210031347962382,"We described uncoupled no-regret learning dynamics so that if all agents play T repetitions of the
game according to the dynamics, the correlated distribution of play is an O(T −3/4)-approximate
EFCE. This substantially improves over the prior best rate of O(T −1/2). One of our conceptual
contributions is to connect the line of work on optimistic regret minimization with the framework
of Φ-regret. One of our main technical contributions is to characterize the stability of the ﬁxed
points associated with trigger deviation functions through a reﬁned perturbation analysis of a certain
structured Markov chain, which may be of independent interest. Finally, experiments conducted on
standard benchmarks corroborated our theoretical ﬁndings."
CONCLUSIONS,0.7241379310344828,Under review as a conference paper at ICLR 2022
REFERENCES,0.7272727272727273,REFERENCES
REFERENCES,0.7304075235109718,Robert Aumann. Subjectivity and correlation in randomized strategies. Journal of Mathematical
REFERENCES,0.7335423197492164,"Economics, 1:67–96, 1974."
REFERENCES,0.7366771159874608,Kimmo Berg and Tuomas Sandholm. Exclusion method for ﬁnding nash equilibrium in multiplayer
REFERENCES,0.7398119122257053,"games. In AAAI Conference on Artiﬁcial Intelligence (AAAI), 2017."
REFERENCES,0.7429467084639498,"Avrim Blum and Yishay Mansour. From external to internal regret. J. Mach. Learn. Res., 8:1307–"
REFERENCES,0.7460815047021944,"1324, 2007."
REFERENCES,0.7492163009404389,"Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up limit hold’em"
REFERENCES,0.7523510971786834,"poker is solved. Science, 347(6218), January 2015."
REFERENCES,0.7554858934169278,Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats
REFERENCES,0.7586206896551724,"top professionals. Science, pp. eaao1733, Dec. 2017."
REFERENCES,0.7617554858934169,"Neil Burch, Matej Moravcik, and Martin Schmid. Revisiting CFR+ and alternating updates. Journal"
REFERENCES,0.7648902821316614,"of Artiﬁcial Intelligence Research, 64:429–443, 2019."
REFERENCES,0.768025078369906,"Andrea Celli, Alberto Marchesi, Tommaso Bianchi, and Nicola Gatti.
Learning to correlate in
multi-player general-sum sequential games. In Proceedings of the Annual Conference on Neural
Information Processing Systems (NeurIPS), volume 32, 2019."
REFERENCES,0.7711598746081505,"Andrea Celli, Alberto Marchesi, Gabriele Farina, and Nicola Gatti. No-regret learning dynamics for"
REFERENCES,0.774294670846395,"extensive-form correlated equilibrium. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, 2020."
REFERENCES,0.7774294670846394,Xi Chen and Binghui Peng. Hedging in games: Faster convergence of external and swap regrets.
REFERENCES,0.780564263322884,"In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS),
2020."
REFERENCES,0.7836990595611285,"Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and"
REFERENCES,0.786833855799373,"Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning Theory,
pp. 6–1, 2012."
REFERENCES,0.7899686520376176,"Constantinos Daskalakis, Paul Goldberg, and Christos Papadimitriou. The complexity of computing"
REFERENCES,0.7931034482758621,"a Nash equilibrium. In Proceedings of the Annual Symposium on Theory of Computing (STOC),
2006."
REFERENCES,0.7962382445141066,"Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms"
REFERENCES,0.799373040752351,"for zero-sum games. Games and Economic Behavior, 92:327–348, 2015."
REFERENCES,0.8025078369905956,"Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning"
REFERENCES,0.8056426332288401,"in general games. CoRR, abs/2108.06924, 2021."
REFERENCES,0.8087774294670846,Miroslav Dudík and Geoffrey J. Gordon. A sampling-based approach to computing equilibria in suc-
REFERENCES,0.8119122257053292,"cinct extensive-form games. In Jeff A. Bilmes and Andrew Y. Ng (eds.), UAI 2009, Proceedings
of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, Montreal, QC, Canada,
June 18-21, 2009, pp. 151–160. AUAI Press, 2009."
REFERENCES,0.8150470219435737,Kousha Etessami and Mihalis Yannakakis. On the complexity of Nash equilibria and other ﬁxed
REFERENCES,0.8181818181818182,"points (extended abstract). In Proceedings of the Annual Symposium on Foundations of Computer
Science (FOCS), pp. 113–123, 2007."
REFERENCES,0.8213166144200627,"Gabriele Farina, Christian Kroer, Noam Brown, and Tuomas Sandholm. Stable-predictive optimistic"
REFERENCES,0.8244514106583072,"counterfactual regret minimization. In International Conference on Machine Learning (ICML),
2019a."
REFERENCES,0.8275862068965517,"Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Online convex optimization for sequential"
REFERENCES,0.8307210031347962,"decision processes and extensive-form games. In AAAI Conference on Artiﬁcial Intelligence,
2019b."
REFERENCES,0.8338557993730408,Under review as a conference paper at ICLR 2022
REFERENCES,0.8369905956112853,"Gabriele Farina, Christian Kroer, and Tuomas Sandholm.
Optimistic regret minimization for
extensive-form games via dilated distance-generating functions. In Advances in Neural Infor-
mation Processing Systems, NeurIPS 2019,, pp. 5222–5232, 2019c."
REFERENCES,0.8401253918495298,"Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Regret circuits: Composability of regret"
REFERENCES,0.8432601880877743,"minimizers. In International Conference on Machine Learning, pp. 1863–1872, 2019d."
REFERENCES,0.8463949843260188,"Gabriele Farina, Chun Kai Ling, Fei Fang, and Tuomas Sandholm. Correlation in extensive-form"
REFERENCES,0.8495297805642633,"games: Saddle-point formulation and benchmarks. In Conference on Neural Information Pro-
cessing Systems (NeurIPS), 2019e."
REFERENCES,0.8526645768025078,"Gabriele Farina, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Simple uncoupled no-regret"
REFERENCES,0.8557993730407524,"learning dynamics for extensive-form correlated equilibrium, 2021a."
REFERENCES,0.8589341692789969,"Gabriele Farina, Andrea Celli, and Tuomas Sandholm. Efﬁcient decentralized learning dynamics"
REFERENCES,0.8620689655172413,"for extensive-form coarse correlated equilibrium: No expensive computation of stationary distri-
butions required. ArXiv preprint, 2021b."
REFERENCES,0.8652037617554859,"Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Better regularization for sequential deci-"
REFERENCES,0.8683385579937304,"sion spaces: Fast convergence rates for Nash, correlated, and team equilibria. In ACM Conference
on Economics and Computation, 2021c."
REFERENCES,0.8714733542319749,Andrew Gilpin and Tuomas Sandholm. Lossless abstraction of imperfect information games. Jour-
REFERENCES,0.8746081504702194,"nal of the ACM, 54(5), 2007."
REFERENCES,0.877742946708464,"Geoffrey J Gordon, Amy Greenwald, and Casey Marks. No-regret learning in convex games. In"
REFERENCES,0.8808777429467085,"Proceedings of the 25th international conference on Machine learning, pp. 360–367. ACM, 2008."
REFERENCES,0.8840125391849529,"Amy Greenwald and Amir Jafari.
A general class of no-regret learning algorithms and game-
theoretic equilibria. In Conference on Learning Theory (COLT), Washington, D.C., 2003."
REFERENCES,0.8871473354231975,Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.
REFERENCES,0.890282131661442,"Econometrica, 68:1127–1150, 2000."
REFERENCES,0.8934169278996865,"Samid Hoda, Andrew Gilpin, Javier Peña, and Tuomas Sandholm. Smoothing techniques for com-"
REFERENCES,0.896551724137931,"puting Nash equilibria of sequential games. Mathematics of Operations Research, 35(2), 2010."
REFERENCES,0.8996865203761756,Albert Xin Jiang and Kevin Leyton-Brown. Polynomial-time computation of exact correlated equi-
REFERENCES,0.9028213166144201,"librium in compact games. Games Econ. Behav., 91:347–359, 2015."
REFERENCES,0.9059561128526645,"Christian Kroer, Kevin Waugh, Fatma Kılınç-Karzan, and Tuomas Sandholm. Faster algorithms for"
REFERENCES,0.9090909090909091,"extensive-form game solving via improved smoothing functions. Mathematical Programming,
2020."
REFERENCES,0.9122257053291536,"Alex Kruckman, Amy Greenwald, and John R. Wicks. An elementary proof of the Markov chain"
REFERENCES,0.9153605015673981,"tree theorem. Technical Report 10-04, Brown University, 2010."
REFERENCES,0.9184952978056427,"H. W. Kuhn. A simpliﬁed two-person poker. In H. W. Kuhn and A. W. Tucker (eds.), Contributions"
REFERENCES,0.9216300940438872,"to the Theory of Games, volume 1 of Annals of Mathematics Studies, 24, pp. 97–103. Princeton
University Press, Princeton, New Jersey, 1950."
REFERENCES,0.9247648902821317,"Viliam Lisý, Marc Lanctot, and Michael Bowling. Online Monte Carlo counterfactual regret min-"
REFERENCES,0.9278996865203761,"imization for search in imperfect information games. In Autonomous Agents and Multi-Agent
Systems, pp. 27–36, 2015."
REFERENCES,0.9310344827586207,"Dustin Morrill, Ryan D’Orazio, Marc Lanctot, James R. Wright, Michael Bowling, and Amy R."
REFERENCES,0.9341692789968652,"Greenwald.
Efﬁcient deviation types and learning for hindsight rationality in extensive-form
games. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference
on Machine Learning, ICML 2021, volume 139 of Proceedings of Machine Learning Research,
pp. 7818–7828. PMLR, 2021a."
REFERENCES,0.9373040752351097,Under review as a conference paper at ICLR 2022
REFERENCES,0.9404388714733543,"Dustin Morrill, Ryan D’Orazio, Reca Sarfati, Marc Lanctot, James R. Wright, Amy R. Greenwald,"
REFERENCES,0.9435736677115988,"and Michael Bowling. Hindsight and sequential rationality of correlated play. In Thirty-Fifth
AAAI Conference on Artiﬁcial Intelligence, AAAI 2021, pp. 5584–5594. AAAI Press, 2021b."
REFERENCES,0.9467084639498433,Yurii Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM Journal of
REFERENCES,0.9498432601880877,"Optimization, 16(1), 2005."
REFERENCES,0.9529780564263323,Christos H. Papadimitriou and Tim Roughgarden. Computing correlated equilibria in multi-player
REFERENCES,0.9561128526645768,"games. J. ACM, 55(3):14:1–14:29, 2008."
REFERENCES,0.9592476489028213,Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Confer-
REFERENCES,0.9623824451410659,"ence on Learning Theory, pp. 993–1019, 2013a."
REFERENCES,0.9655172413793104,"Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable se-"
REFERENCES,0.9686520376175548,"quences. In Advances in Neural Information Processing Systems, pp. 3066–3074, 2013b."
REFERENCES,0.9717868338557993,"Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of reg-"
REFERENCES,0.9749216300940439,"ularized learning in games. In Advances in Neural Information Processing Systems, pp. 2989–
2997, 2015."
REFERENCES,0.9780564263322884,"Oskari Tammelin. Solving large imperfect information games using CFR+. arXiv preprint, 2014."
REFERENCES,0.9811912225705329,"Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up limit"
REFERENCES,0.9843260188087775,"Texas hold’em. In Proceedings of the 24th International Joint Conference on Artiﬁcial Intelli-
gence (IJCAI), 2015."
REFERENCES,0.987460815047022,Bernhard von Stengel and Françoise Forges. Extensive-form correlated equilibrium: Deﬁnition and
REFERENCES,0.9905956112852664,"computational complexity. Mathematics of Operations Research, 33(4):1002–1022, 2008."
REFERENCES,0.9937304075235109,"Martin Zinkevich, Michael Bowling, Michael Johanson, and Carmelo Piccione. Regret minimization"
REFERENCES,0.9968652037617555,"in games with incomplete information.
In Proceedings of the Annual Conference on Neural
Information Processing Systems (NIPS), 2007."
