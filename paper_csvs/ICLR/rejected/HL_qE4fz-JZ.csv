Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003215434083601286,"Gaussian Processes (GPs) are Bayesian models that provide uncertainty estimates
associated to the predictions made. They are also very ﬂexible due to their non-
parametric nature. Nevertheless, GPs suffer from poor scalability as the number
of training instances N increases since their cost is cubic in N. To overcome
this problem, sparse GP approximations are often used, where a set of M ≪N
inducing points is introduced during training. The location of the inducing points is
learned by considering them as parameters of an approximate posterior distribution
q. Sparse GPs, combined with variational inference for inferring q, reduce the
cost of GPs per iteration to O(M 3). Critically, the inducing points determine
the ﬂexibility of the model and they are often located in regions of the input
space where the latent function changes. A limitation is, however, that for some
learning tasks a large number of inducing points may be required to obtain a good
prediction performance. To address this limitation, we propose here to amortize
the computation of the inducing points locations, as well as the parameters of
the variational posterior approximation q. For this, we use a neural network that
receives the observed data as an input and outputs the inducing points locations
and the parameters of q. We evaluate our method in several experiments, showing
that it performs similar or better than other state-of-the-art sparse variational GP
approaches. However, with our method the number of inducing points is reduced
drastically due to their dependency on the input data. This makes our method scale
to larger datasets and have faster training and prediction times."
INTRODUCTION,0.006430868167202572,"1
INTRODUCTION"
INTRODUCTION,0.00964630225080386,"Gaussian Processes (GPs) are non-parametric models that can be used to address regression and clas-
siﬁcation machine learning problems (Rasmussen & Williams, 2006). GPs become more expressive
as the number of training instances N grows and, since they are Bayesian models, they provide a pre-
dictive distribution that estimates the uncertainty associated to the predictions made. This uncertainty
estimation or ability to know what is not known is critical in many practical applications (Gal, 2016).
Nevertheless, GPs suffer from poor scalability as their training cost is O(N 3) per iteration due to the
need of computing the inverse of a N × N covariance matrix. Another limitation is that approximate
inference is needed with non-Gaussian likelihoods (Rasmussen & Williams, 2006)."
INTRODUCTION,0.012861736334405145,"Sparse approximations can improve the cost of GPs (Rasmussen & Williams, 2006). The most
popular ones introduce a set of M ≪N inducing points (Snelson & Ghahramani, 2006; Titsias,
2009). The inducing points and their associated posterior values completely specify the posterior
process at test points. In Snelson & Ghahramani (2006), the computational gain is obtained by
assuming independence among the process values at the training points given the inducing points
and their values. This can also be seen as using an approximate GP prior (Qui˜nonero-Candela &
Rasmussen, 2005). By contrast, in Titsias (2009) the computational gain is obtained by combining
variational inference (VI) with a posterior approximation q that has a ﬁxed part and a tunable part. In
both methods the cost is reduced to O(NM 2) per iteration and the inducing points, considered as
model’s hyper-parameters, are learned by maximizing an estimate of the marginal likelihood."
INTRODUCTION,0.01607717041800643,"Importantly, the VI approach of Titsias (2009) maximizes a lower bound on the log-marginal
likelihood as an indirect way of minimizing the KL-divergence between an approximate posterior
distribution for the process values at the inducing points and the corresponding exact posterior.
The advantage is that the objective is expressed as a sum over the training instances, allowing for"
INTRODUCTION,0.01929260450160772,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.022508038585209004,"mini-batch training and stochastic optimization techniques to be applied on the objective (Hensman
et al., 2015b). This reduces the cost to O(M 3) per iteration, making GPs scalable to large datasets."
INTRODUCTION,0.02572347266881029,"In sparse approximations one often observes in practice that after the optimization process the
inducing points are located in regions of the input space in which the latent function changes (Snelson
& Ghahramani, 2006; Titsias, 2009; Hensman et al., 2015a; Bauer et al., 2016). Therefore, the
expressive power of the model depends on the number of inducing points M and their correct location
on the input space. Some problems may require a large number of inducing points, in the order of
thousands, to get good prediction results (Hensman et al., 2015b; Shi et al., 2020; Tran et al., 2020).
This makes training inducing point based sparse GPs difﬁcult in those problems."
INTRODUCTION,0.028938906752411574,"There have been some attempts to improve the training cost of sparse approximations, including using
different sets of inducing points for the computation of the posterior mean and variance (Cheng &
Boots, 2017). Other approaches use an orthogonal decomposition of the GP that allows to introduce
an extra set of inducing points with less cost (Shi et al., 2020). Finally, other methods consider a
large set of inducing points, but restrict the computations for a particular data point to the nearest
neighbors to that point from the set of inducing points (Tran et al., 2020)."
INTRODUCTION,0.03215434083601286,"In this work we are inspired by Tran et al. (2020) and propose a novel method to improve the
training cost of sparse GPs. Our method also tries to produce a set of inducing points (and associated
variational approximation q) that are speciﬁc of each input data point. For that, we note that some
works in the literature have observed that one can learn the mappings from inputs to proposal
distributions instead of directly optimizing their parameters (Kingma & Welling, 2014; Shu et al.,
2018). This approach, known as amortized variational inference, is a key contribution of variational
auto-encoders (VAE) (Kingma & Welling, 2014), and has also been explored in the context of GP to
solve other types of problems such as multi-class classiﬁcation with input noise (Villacampa-Calvo
et al., 2021). Amortized inference has also been empirically shown to lead to useful regularization
properties that improve the generalization performance (Shu et al., 2018)."
INTRODUCTION,0.03536977491961415,"Speciﬁcally, here we combine sparse GPs with a neural network architecture that computes, for each
potential data point, the associated inducing points to be used for prediction. We also employ a
neural network to carry out amortized VI to compute the parameters of the variational distribution
q approximating the posterior of the process values for the outputted inducing points. While the
number of parameters that need optimization may increase with the use of a neural network, this
approach allows for a big reduction in the total number of inducing points without losing expressive
power. In particular, it enables different sets of inducing points associated to each input location.
The inducing points are simply given by a mapping from the inputs provided by a neural network.
We show on several experiments that the proposed method is able to perform similar or better than
standard sparse GPs and competitive methods for improving the cost of sparse GPs (Tran et al., 2020;
Shi et al., 2020). However, the training and prediction times of our method are much better."
GAUSSIAN PROCESSES,0.03858520900321544,"2
GAUSSIAN PROCESSES"
GAUSSIAN PROCESSES,0.04180064308681672,"A Gaussian Process (GP) is a stochastic process for which any ﬁnite set of variables has a Gaussian
distribution (Rasmussen & Williams, 2006). In a learning task, we use a GP as a prior over a
latent function. Then, Bayes’ rule is used to get a posterior for that function given the observed data.
Consider a dataset D = {(xi, yi)}N
i=1, where each scalar yi is assumed to be obtained as yi = f(xi)+
ϵi, with f(·) a latent function and ϵi Gaussian noise with variance σ2, i.e., ϵi ∼N(0, σ2). We specify
a prior distribution for f in the form of a GP, which is described by a mean function m(x) (often set
to zero) and covariance function k(x, x′) such that k(x, x′) = E[(f(x) −m(x))(f(x′) −m(x′))].
Covariance functions typically have some parameters θ. Given D, the predictive distribution for f at
a new test point x⋆is Gaussian with mean and variance given by"
GAUSSIAN PROCESSES,0.04501607717041801,"µ(x⋆) = k(x⋆)T(K + σ2I)−1y ,
σ2(x⋆) = k(x⋆, x⋆) −k(x⋆)T(K + σ2I)−1k(x⋆) ,
(1)"
GAUSSIAN PROCESSES,0.04823151125401929,"where µ(x⋆) and σ2(x⋆) are the prediction mean and variance, respectively. k(x⋆) is a vector with
the covariances between f(x⋆) and each f(xi). Similarly, K has the covariances between f(xi) and
f(xj) for i, j = 1, . . . , N. Finally, I stands for the identity matrix. Popular covariances functions
k(·, ·) are the squared exponential and the Mat´ern (Rasmussen & Williams, 2006). Their parameters,
θ, and σ2 can be found by maximizing p(y) (Rasmussen & Williams, 2006). The computational"
GAUSSIAN PROCESSES,0.05144694533762058,Under review as a conference paper at ICLR 2022
GAUSSIAN PROCESSES,0.05466237942122187,"complexity of this approach is O(N 3) since it needs the inversion of K, a N × N matrix. This makes
GPs unsuitable for large data sets."
SPARSE VARIATIONAL GAUSSIAN PROCESSES,0.05787781350482315,"2.1
SPARSE VARIATIONAL GAUSSIAN PROCESSES"
SPARSE VARIATIONAL GAUSSIAN PROCESSES,0.06109324758842444,"Sparse approximations improve the cost of GPs. The most popular methods introduce, in the same
input space as the original data, a new set of M ≪N points , called the inducing points, denoted
by Z = (z1, . . . , zM)T (Snelson & Ghahramani, 2006; Titsias, 2009). Let the corresponding latent
function values be u = (f(z1), . . . , f(zM))T. The inducing points are not restricted to be part of the
observed data and their location can be learned during training. A GP prior is placed on u. Namely,
p(u) ∼N(0, KZ), where KZ is a matrix with the covariances associated to each pair of points from
Z. The idea is that the posterior for f can be approximated in terms of the posterior for u."
SPARSE VARIATIONAL GAUSSIAN PROCESSES,0.06430868167202572,"In this work we focus on a widely used variational inference (VI) approach to approximate the
posterior for f (Titsias, 2009). Let f = (f(x1), . . . , f(xN))T. In VI, the goal is to ﬁnd an approximate
posterior for f and u, q(f, u), that resembles as much as possible the true posterior p(f, u|y).
Critically, q is constrained to be q(f, u) = p(f|u)q(u), with p(f|u) ﬁxed and q(u) a tunable multi-
variate Gaussian. To ﬁnd q(u) a lower bound of the marginal likelihood is maximized. The evidence
lower bound (or ELBO) is obtained via Jensen’s inequality, leading to (after some simpliﬁcations):"
SPARSE VARIATIONAL GAUSSIAN PROCESSES,0.06752411575562701,"L = PN
i=1 Eq(f)[log p(yi|fi)] −KL[q(u)|p(u)] ,
(2)"
SPARSE VARIATIONAL GAUSSIAN PROCESSES,0.0707395498392283,"where p(yi|fi) is the model’s likelihood for the i-th point and KL[·|·] is the Kullback-Leibler di-
vergence between probability distributions. In Titsias (2009), they optimize q(u) in closed-form.
The resulting expression is then maximized to estimate Z, θ and σ2. This leads to a complexity of
O(NM 2). However, if the variational posterior q(u) is optimized alongside with Z, θ and σ2, as pro-
posed in Hensman et al. (2013), the ELBO can be expressed as a sum over training instances, which
allows for mini-batch training and stochastic optimization techniques. Using stochastic variational
inference (SVI) reduces the training cost to O(M 3) per iteration (Hensman et al., 2013). Importantly,
the ﬁrst term in (2) is an expectation that has closed-form solution in the case of Gaussian likelihoods.
It needs to be approximated for other cases, e.g., binary classiﬁcation, either by quadrature or MCMC
methods (Hensman et al., 2015b). The second term is the KL-divergence between the variational
posterior and the prior, which can be computed analytically since they are both Gaussians."
SPARSE VARIATIONAL GAUSSIAN PROCESSES,0.07395498392282958,"The expressive power of the sparse GP critically depends on the number of inducing points M and
on their correct placement in the input space via optimizing (2) (Titsias, 2009; Hensman et al., 2015a;
Bauer et al., 2016). In some problems several thousands of inducing points may be required to get
good results (Hensman et al., 2015b; Shi et al., 2020; Tran et al., 2020). This makes difﬁcult and
expensive using sparse GPs in those problems. In the next section we describe how to alleviate this."
INPUT DEPENDENT SPARSE GPS,0.07717041800643087,"3
INPUT DEPENDENT SPARSE GPS"
INPUT DEPENDENT SPARSE GPS,0.08038585209003216,"We develop a new formulation of sparse GPs which for every given input computes the corresponding
inducing points to be used for prediction, and also the associated parameters of the approximate
distribution q. To achieve this, we consider a meta-point ˜x that is used to determine the inducing
points Z and the corresponding u. Namely, now u depends on ˜x, i.e., u ∼p(u|˜x). In particular, we
set p(u|˜x) = N(0, KZ(˜x)) where the inducing points Z depend non-linearly, e.g., via a deep neural
network, on ˜x. The joint distribution of u and ˜x is then given by p(u, ˜x) = p(u|˜x)p(˜x) for some
prior distribution p(˜x). Following (Tran et al., 2020), we can consider an implicit distribution p(˜x).
That is, its analytical form is unknown, but we can draw samples from it. Later on, we specify p(˜x)."
INPUT DEPENDENT SPARSE GPS,0.08360128617363344,"Note that the marginalized prior p(u) is no longer Gaussian. However, we can show that this
formulation does not impact on the prior over f. For an arbitrary selected meta-point ˜x we have that"
INPUT DEPENDENT SPARSE GPS,0.08681672025723473,"p(f, u|˜x) =N

0,

K
KX,Z(˜x)
KZ(˜x),X
KZ(˜x)"
INPUT DEPENDENT SPARSE GPS,0.09003215434083602,"
,
(3)"
INPUT DEPENDENT SPARSE GPS,0.0932475884244373,"where KX,Z(˜x) are the cross-covariances between f and u. Therefore, if u is marginalized out in
(3), the prior for f is the standard GP prior and does not depend on ˜x. Hence, p(f|˜x) = p(f). Thus,
p(f, u) =
R
p(f, u|˜x)p(˜x)d˜x is a mixture of Gaussian densities, where the marginal over f is the
same for every component of the mixture. In the standard sparse GP, the inducing points also have an"
INPUT DEPENDENT SPARSE GPS,0.09646302250803858,Under review as a conference paper at ICLR 2022
INPUT DEPENDENT SPARSE GPS,0.09967845659163987,"impact on the variational approximation q via the ﬁxed conditional distribution p(f|u) (Titsias, 2009).
Therefore, we also incorporate the input dependence on ˜x in q. This is done in the next section."
LOWER BOUND ON THE LOG-MARGINAL LIKELIHOOD,0.10289389067524116,"3.1
LOWER BOUND ON THE LOG-MARGINAL LIKELIHOOD"
LOWER BOUND ON THE LOG-MARGINAL LIKELIHOOD,0.10610932475884244,"We follow Tran et al. (2020) to derive a lower bound on the log-marginal likelihood of the ex-
tended model described above. Consider a posterior approximation of the form q(f, u, ˜x) =
p(f|u)q(u|˜x)p(˜x), where only q(u|˜x) can be adjusted and the other factors are ﬁxed. Using this
posterior’s factorization and Jensen’s inequality we obtain the lower bound after some simpliﬁcations:"
LOWER BOUND ON THE LOG-MARGINAL LIKELIHOOD,0.10932475884244373,"L = PN
i=1
R
p(˜x)

p(fi|u)q(u|˜x) log p(yi|fi)dfdu −1"
LOWER BOUND ON THE LOG-MARGINAL LIKELIHOOD,0.11254019292604502,"N KL[q(u|˜x)|p(u|˜x)]

d˜x .
(4)
Now, assuming that p(˜x) is an implicit distribution, we can draw samples from it and approximate
the expectation w.r.t p(˜x). Thus, for a meta-point sample ˜xs from p(˜x), (4) is approximated as"
LOWER BOUND ON THE LOG-MARGINAL LIKELIHOOD,0.1157556270096463,"L ≈PN
i=1

Ep(fi|u)q(u|˜xs)[log p(yi|fi)] −1"
LOWER BOUND ON THE LOG-MARGINAL LIKELIHOOD,0.1189710610932476,"N KL[q(u|˜xs)|p(u|˜xs)]

.
(5)"
LOWER BOUND ON THE LOG-MARGINAL LIKELIHOOD,0.12218649517684887,"We can evaluate (5) and its gradients to maximize the original objective in (4) using stochastic
optimization techniques. This is valid for any implicit distribution p(˜x). Consider now that we use
mini-batch-based training for optimization, and we set ˜xs = xi. In this case, the value of ˜x remains
random, as it depends on the points (xi, yi) that are selected in the random mini-batch. This results
in a method that computes different inducing points for each input location. In practice, we use the
same sample to approximate the expectation w.r.t. p(˜x) and the sum across the data in (5). This could
introduce a bias in the objective. However, such a reusing of the samples is done in Tran et al. (2020)
with good empirical results. Moreover, our experiments in Section 5 also validate this approximation."
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.12540192926045016,"3.2
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.12861736334405144,"Maximizing the lower bound ﬁnds the optimal approximate distribution q.
A problem, how-
ever, is that we have a potential large number of parameters to ﬁx, corresponding to each
q(u|xi).
In particular, if we set q(u|xi) to be Gaussian, we will have to infer different X nd X nd"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.13183279742765272,"h(
)
P
1"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.13504823151125403,"h(1)
P −1"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.1382636655948553,"h(1)
2"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.1414790996784566,"h(1)
1"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.14469453376205788,"h(2)
P"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.14790996784565916,"h(2)
P −1"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.15112540192926044,"h(2)
2"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.15434083601286175,"h(2)
1"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.15755627009646303,"h(L )
P"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.1607717041800643,"h(L )
P −1"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.1639871382636656,"h(L )
2"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.16720257234726688,"h(L )
1"
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.17041800643086816,"Figure 1: The network’s inputs is ˜x. The outputs
are the inducing points Z, the mean vector m and
the Cholesky factor, L, of q(u|xi)."
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.17363344051446947,"means and covariance matrices for each differ-
ent xi. This is expected to be memory inefﬁcient
and to make optimization more difﬁcult. To re-
duce the number of parameters of our method
we use amortized variational inference and spec-
ify a function that can generate these parame-
ters for each xi (Shu et al., 2018). More pre-
cisely, we set the mean and covariance matrix
of q(u|xi) to be m(xi) and S(xi), for some
non-linear functions."
AMORTIZED VARIATIONAL INFERENCE AND DEEP NEURAL NETWORKS,0.17684887459807075,"Deep neural networks (DNN) are ﬂexible mod-
els that can describe complicated functions. In
these models, the inputs go through several lay-
ers of non-linear transformations. We use these
models to compute the non-linearities that gen-
erate from xi the inducing points, Z(xi), and
the means and covariances of q(u|xi), i.e., m(xi) and S(xi). The architecture employed is displayed
in Figure 1. At the output of the DNN we obtain Z, a mean vector m and the Cholesky factor of the
covariance matrix S = LLT. The maximization of the lower bound in (4) when using DNNs for the
non-linearities is shown in Algorithm 1. The required expectations are computed in closed-form, in
regression. In binary classiﬁcation, we use 1-dimensional quadrature, as in Hensman et al. (2015a)."
PREDICTIONS AND TRAINING COST,0.18006430868167203,"3.3
PREDICTIONS AND TRAINING COST"
PREDICTIONS AND TRAINING COST,0.1832797427652733,"At test the instances are not randomly chosen. In that case, we simply set p(˜x) to be a deterministic
distribution placed on the candidate point x⋆. The DNN is used to obtain the associated information.
Namely, Z, and the parameters of q(u|x⋆), m and S. The predictive distribution for f(x⋆) is:"
PREDICTIONS AND TRAINING COST,0.1864951768488746,"f(x⋆) ∼N
 
Kx⋆,ZK−1
Z m, k(x⋆, x⋆) + Kx⋆,ZK−1
Z (S −KZ) K−1
Z KT
x⋆,Z

.
(6)"
PREDICTIONS AND TRAINING COST,0.18971061093247588,Under review as a conference paper at ICLR 2022
PREDICTIONS AND TRAINING COST,0.19292604501607716,"Algorithm 1 Training input dependent sparse GPs
Input: D, M, neural network NNet with L hidden layers and P hidden units
Output: Optimal parameters of the model"
PREDICTIONS AND TRAINING COST,0.19614147909967847,"initialize neural network’s weights and kernel’s parameters θ
while stopping criteria is False do"
PREDICTIONS AND TRAINING COST,0.19935691318327975,"Loglk = 0, KLdiv = 0
gather mini-batch Mb of size n from D
for (xi, yi) in Mb do"
PREDICTIONS AND TRAINING COST,0.20257234726688103,"(Zxi, mxi, Lxi) = NNet(xi); Loglk += Eq(fi,u)[log p(yi|fi)] ;KLdiv += KL[q(u|xi)|p(u|xi)]
ELBO ←N"
PREDICTIONS AND TRAINING COST,0.2057877813504823,n × Loglk −1
PREDICTIONS AND TRAINING COST,0.2090032154340836,"n× KLdiv
Update parameters of the model using the gradient of ELBO"
PREDICTIONS AND TRAINING COST,0.21221864951768488,"Given this distribution for f(x⋆), the probability distribution for y⋆can be computed in closed form in
the case of regression problems and with 1-dimensional quadrature in the case of binary classiﬁcation.
Note that (6) is only suitable for predictions at individual test points, as in Tran et al. (2020). This can
be a limitation in applications needing covariances. As a solution, one could consider for prediction
the union of all input dependent inducing points for test points. This would be inconsistent with the
proposed training method. Nonetheless, such approach can also be modiﬁed to consider the union of
input dependent inducing points for the training points within a mini-batch, as in (Tran et al., 2020)."
PREDICTIONS AND TRAINING COST,0.21543408360128619,"The cost of our method is smaller than the cost of a standard sparse GP if a smaller number of
inducing points M is used. The cost of a DNN with L layers, P hidden units, di dimension of the
input data, and output dimension do is O(ndiP + nP 2L + nPdo + n(L + 1)). The cost of the
sparse GPs is O(nM 3), with n the mini-batch size. Therefore, the cost of our method per iteration is
O(ndiP + nP 2L + nPdo + n(L + 1) + nM 3). Since in our method the inducing points are input
dependent, we expect to obtain good prediction results even for M values that are fairly small."
RELATED WORK,0.21864951768488747,"4
RELATED WORK"
RELATED WORK,0.22186495176848875,"Early works on sparse GPs simply chose a subset of the training data for inference based on an
information criterion (Csat´o & Opper, 2002; Lawrence et al., 2003; Seeger et al., 2003; Henao &
Winther, 2012). This approach is limited in practice and more advanced methods in which the
inducing points need not be equal to the training points are believed to be superior. In the literature
there are several works analyzing and studying sparse GP approximations based on inducing points.
Some of these works include Qui˜nonero-Candela & Rasmussen (2005); Snelson & Ghahramani
(2006); Naish-Guzman & Holden (2007); Titsias (2009); Bauer et al. (2016); Hern´andez-Lobato &
Hern´andez-Lobato (2016). We focus here on a variational approach (Titsias, 2009) which allows for
stochastic optimization and mini-batch training (Hensman et al., 2013; 2015a). This enables learning
in very large datasets with a cost of O(M 3) per iteration, with M the number of inducing points."
RELATED WORK,0.22508038585209003,"In some problems, however, several thousands of inducing points may be needed to get good
prediction results (Hensman et al., 2015b; Shi et al., 2020; Tran et al., 2020). There is hence a need
to improve the cost of sparse GPs, without losing expressive power. One work addressing this task is
that of Cheng & Boots (2017). In that work it is proposed to decouple the process of inferring the
posterior mean and variance, allowing to consider a different number of inducing points for each
one. Importantly, the computation of the mean has a linear complexity, which allows to have more
expressive posterior means at a lower cost. A disadvantage is that such an approach suffers from
optimization difﬁculties. An alternative decoupled parameterization adopts an orthogonal basis in
the mean Salimbeni et al. (2018a). Such a method can be considered as a speciﬁc case of Shi et al.
(2020). There, the authors introduce a new interpretation of sparse variational approximations for GP
using inducing points. For this, the GP is decomposed as a sum of two independent processes. This
leads to tighter lower bounds on the marginal likelihood and new inference algorithms considering
two different sets of inducing points. This enables using more inducing points at a linear cost."
RELATED WORK,0.2282958199356913,"Our work is closer to Tran et al. (2020). There, a mechanism is also described to consider input
dependent inducing points in the context of sparse GP. However, the difference is signiﬁcant. In
particular, in Tran et al. (2020) a very large set of inducing points M is considered initially. Then, for
each input point, a subset of these inducing points is considered. This subset is obtained by ﬁnding"
RELATED WORK,0.2315112540192926,Under review as a conference paper at ICLR 2022
RELATED WORK,0.2347266881028939,"the K ≪M nearest inducing points to the current data instance xi. This approach signiﬁcantly
reduces the cost of the standard sparse GP described in Titsias (2009). However, it suffers from the
difﬁculty of having to ﬁnd the K nearest neighbors for each point in a mini-batch, which is very
expensive. Therefore, the ﬁnal cost is higher than what would be thought initially. Our method is
expected to be better because of the extra ﬂexibility by the non-linear relation between xi and Z
given by the DNN. Furthermore, the DNN is expected to make a better use of GPU acceleration."
RELATED WORK,0.2379421221864952,"Another method to improve the training cost of GP is described in Wilson & Nickisch (2015); Evans
& Nair (2018); Gardner et al. (2018). It consists in placing the inducing points on a grid. This allows
to perform fast computation exploiting the inducing points structure. One can easily consider values
for M that are even larger than N. However, to get such beneﬁts the inducing points need to be ﬁxed
due to the structure constraints. This may be detrimental in high-dimensional problems."
RELATED WORK,0.24115755627009647,"Instead of using inducing points, there are some works that scale GPs by approximating the posterior
GP process using an inference network (Shi et al., 2019; Sun et al., 2019). An inference network
receives some random noise and outputs function values for each input. Particular examples include
among others Bayesian DNNs. Inference networks are expected to lead to ﬂexible stochastic processes.
However, it is difﬁcult to enforce that the approximate posterior process looks similar to the prior
GP in regions where there is no data. For this, approximate inference is carried out on a ﬁnite
subset of points chosen at random from the input space. This is expected to lead to poor results in
high-dimensional spaces. Moreover, another problem of using an inference network is that tuning the
prior GP hyper-parameters is challenging and has often to be done in a separate step."
RELATED WORK,0.24437299035369775,"Amortized variational inference (Shu et al., 2018) has also been explored in the context of GPs in
Villacampa-Calvo et al. (2021). There, input noise is considered in a multi-class learning problem
and the performance of the ﬁnal model is improved by amortizing the variational parameters of the
posterior approximation for the noiseless inputs, using a DNN that receives both xi and yi."
RELATED WORK,0.24758842443729903,"Other sparse GPs in the literature do not fully rely on inducing points, e.g., (Tresp, 2000; Snelson,
2007; Gramacy & Apley, 2015). These techniques, however, cannot use, in general, stochastic
optimization and do not scale to very large problems. Finally, sparse GPs, and our method, can beneﬁt
from natural-gradients (Salimbeni et al., 2018b). They could result in an orthogonal improvement."
EXPERIMENTS,0.2508038585209003,"5
EXPERIMENTS"
EXPERIMENTS,0.2540192926045016,"We evaluate the performance of the proposed method, to which we refer to as Input Dependent Sparse
GP (IDSGP). We consider both regression and binary classiﬁcation with a probit likelihood. In this
later case, we approximate the expectation in the lower bound using 1-dimensional quadrature, as in
Hensman et al. (2015a). The code of the proposed method in Tensorﬂow 2.0 (Abadi et al., 2015) is
provided in the supplementary material. In the experiments we compare results with the standard
variational sparse GP (Titsias, 2009). We refer to such a method as VSGP. We also compare results
with two of the methods described in Section 4. Namely, the sparse within sparse GP (SWSGP)
described in Tran et al. (2020), and the sparse GP based on an orthogonal decomposition that allows
to consider two different sets of inducing points (Shi et al., 2020). We refer to this last method as
SOLVE. All methods use a Mat´ern 3/2 covariance function (Rasmussen & Williams, 2006). The
DNN architecture employed in IDSGP is described in detail in Appendix B."
TOY PROBLEMS,0.2572347266881029,"5.1
TOY PROBLEMS"
TOY PROBLEMS,0.2604501607717042,"We show the posterior mean and standard deviation of each method on a 1-dimensional regression
problem (Snelson & Ghahramani, 2006). We compare results with a full GP. Figure 2 shows the
results obtained, including the learned locations of the inducing points. In the case of IDSGP we
show the locations of the inducing points for the point represented with a star at x = 3.9. The number
of inducing points, for each method, are indicated in the ﬁgure’s caption. We consider a small number
of inducing points to study the beneﬁts of having input dependent inducing points. IDSGP uses
smaller number of inducing points than the other methods. The ﬁgure shows that, in regions with
observed data, IDSGP’s predictions look closer to those of the full GP. Appendix D.1 has results
for an increasing number of inducing points M. They show that as M increases IDSGP becomes
more similar to the full GP. Figure 3 shows the decision boundary of each method on the banana
classiﬁcation dataset (Hensman et al., 2013). IDSGP produces the most accurate boundaries."
TOY PROBLEMS,0.26366559485530544,Under review as a conference paper at ICLR 2022
TOY PROBLEMS,0.26688102893890675,"IDSGP
VSGP"
TOY PROBLEMS,0.27009646302250806,"SWSGP
SOLVE"
TOY PROBLEMS,0.2733118971061093,"Figure 2: Toy data set with N = 200 points. Initial and ﬁnal locations for the inducing points are
shown on the top and bottom of each ﬁgure. In IDSGP, the inducing points correspond to the point
drawn with star. The posterior mean and standard deviation of full GP are shown with blue and brown
dashed lines, respectively. VSGP method with M = 4. IDSGP with M = 2 and a neural network
with 2 layers with 50 units. SWSGP with M = 4 and 2 neighbors. SOLVE with M1 = M2 = 2."
TOY PROBLEMS,0.2765273311897106,"VSGP
IDSGP"
TOY PROBLEMS,0.2797427652733119,"SOLVE
SWSGP"
TOY PROBLEMS,0.2829581993569132,"Figure 3: Banana classiﬁcation data set with N = 5300 points. The ﬁnal location of inducing points
are shown inside the ﬁgures. For IDSGP, we show the location of inducing points related to the green
colored point. VSGP with M = 4. IDSGP with M = 2 and a neural network with 2 hidden layers
each contains 50 hidden nodes. SWSGP with M = 4 and 2 neighbors. SOLVE with M1 = M2 = 2."
EXPERIMENTS ON UCI DATASETS,0.2861736334405145,"5.2
EXPERIMENTS ON UCI DATASETS"
EXPERIMENTS ON UCI DATASETS,0.28938906752411575,"We consider several regression and binary classiﬁcation datasets extracted from the UCI repository
(Dua & Graff, 2017). The number of inducing points of IDSGP is set to M = 15. In SOLVE we
use M1 = 1024 and M2 = 1024 inducing points. In VSGP we set M = 1024. In SWSGP we set
M = 1024 and K = 50 neighbors. All the methods are trained using ADAM (Kingma & Ba, 2015)"
EXPERIMENTS ON UCI DATASETS,0.29260450160771706,Under review as a conference paper at ICLR 2022
EXPERIMENTS ON UCI DATASETS,0.2958199356913183,"with a mini-batch size of 100 and a learning rate of 0.01. In the classiﬁcation setting we use the same
setup, but the number of inducing points of IDSGP is set to M = 3. All methods are trained on a
Tesla P100 GPU with 16GB of memory. On each dataset we use 80% of the data for training and the
rest for testing. We report results across 5 splits of the data since the datasets are already quite big."
EXPERIMENTS ON UCI DATASETS,0.2990353697749196,"The average negative test log-likelihood of each method on each dataset is displayed in Table 1, for
the regression datasets, and in Table 2, for the classiﬁcation datasets, respectively. The average rank
of each method is also displayed at the last row of each table. The RMSE and prediction accuracy
results are similar to those displayed here. They can be found in Appendix D.3 and D.4. Each table
also shows the number of instances N and dimensions d of each dataset. We observe that in the
regression datasets, the proposed method, IDSGP, obtains best results in 6 out of the 8 datasets.
IDSGP also obtains the best average rank (closer to always performing best on each train / test data
split). This is remarkable given that IDSGP a much smaller number of inducing points (e.g., M = 15
for IDSGP vs. M = 1024 for VSGP). In classiﬁcation, however, all the methods seem to perform
similar to each other and the differences between them are smaller. Again IDSGP uses here a smaller
number of M = 3 inducing points. Increasing M in IDSGP does not improve the results."
EXPERIMENTS ON UCI DATASETS,0.3022508038585209,"Table 1: Avg. neg. test log-likelihood values for the UCI regression datasets. The numbers in
parentheses are standard errors. Best mean values are highlighted in bold face."
EXPERIMENTS ON UCI DATASETS,0.3054662379421222,"N
d
VSGP
SOLVE
SWSGP
IDSGP
Kin40k
32,000
8
-0.047 (0.003) -0.415 (0.006) -0.110 (0.007) -1.461 (0.019)
Protein
36,584
9
2.848 (0.002)
2.818 (0.003)
2.835 (0.002)
2.775 (0.007)
KeggDirected
42,730
19
-1.955 (0.013) -1.756 (0.073) -2.256 (0.012) -2.410 (0.012)
KEGGU
51,686
26
-2.344 (0.012) -2.531 (0.015) -2.396 (0.006) -2.908 (0.042)
3dRoad
347,899
3
3.691 (0.006)
3.726 (0.010)
3.879 (0.026)
3.399 (0.009)
Song
412,276
90
3.613 (0.003)
3.608 (0.002)
3.618 (0.004)
3.637 (0.002)
Buzz
466,600
77
6.272 (0.012)
6.297 (0.009)
6.137 (0.008)
6.317 (0.055)
HouseElectric
1,639,424 6
-1.737 (0.006) -1.743 (0.005) -1.711 (0.010) -1.774 (0.004)
Avg. Ranks
3.125 (0.125)
2.475 (0.156)
2.850 (0.150)
1.550 (0.172)
# Inducing points
1024
1024 / 1024
(K=50) / 1024
15"
EXPERIMENTS ON UCI DATASETS,0.3086816720257235,"Table 2: Avg. test neg. log-likelihood values for the UCI classiﬁcation datasets. The numbers in
parentheses are standard errors. Best mean values are highlighted in bold face."
EXPERIMENTS ON UCI DATASETS,0.31189710610932475,"N
d
VSGP
SOLVE
SWSGP
IDSGP
MagicGamma
15,216
10
0.308 (0.004) 0.314 (0.005) 0.371 (0.005) 0.311 (0.002)
DefaultOrCredit
24,000
30
0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000)
NOMAO
27,572
174
0.113 (0.004) 0.103 (0.004) 0.134 (0.004) 0.121 (0.004)
BankMarketing
36,169
51
0.206 (0.001) 0.199 (0.001) 0.304 (0.021) 0.209 (0.002)
Miniboone
104,051 50
0.151 (0.001) 0.142 (0.001) 0.180 (0.007) 0.153 (0.001)
Skin
196,046 3
0.005 (0.000) 0.005 (0.000) 0.006 (0.001) 0.003 (0.000)
Crop
260,667 174
0.003 (0.000) 0.003 (0.000) 0.002 (0.000) 0.003 (0.000)
HTSensor
743,193 11
0.003 (0.001) 0.001 (0.000) 0.030 (0.009) 0.005 (0.001)
Avg. Ranks
2.425 (0.143) 1.775 (0.158) 3.175 (0.182) 2.625 (0.155)
# Inducing points
1024
1024 / 1024
(K=50) / 1024
3"
EXPERIMENTS ON UCI DATASETS,0.31511254019292606,"In these experiments we also measure the average training time per epoch, for each method. The
results corresponding to the UCI regression datasets are displayed in Table 3. The results for the UCI
classiﬁcation datasets are found in Appendix D.4. They look very similar to ones reported here. We
observe that the fastest method in terms of training time is the proposed approach. Namely, IDSGP.
Nevertheless, the speed-up obtained is impaired by the overhead of having to compute the output
of the DNN and update its parameters. IDSGP also results in fastest prediction times than VSGP,
SOLVE or SWSGP. See Appendix D.3 and D.4 for further results showing average prediction times."
EXPERIMENTS ON UCI DATASETS,0.3183279742765273,"Table 3: Average training time per epoch across the 5 splits for the UCI regression datasets. The
numbers in parentheses are standard errors. Best mean values are highlighted."
EXPERIMENTS ON UCI DATASETS,0.3215434083601286,"Kin40k
Protein
KeggDirected
KEGGU
3dRoad
Song
Buzz
HouseElectric
VSGP
591.7 (0.58)
737.2 (1.16)
932.7 (2.56) 1128.1 (3.78) 7880.9 (66.79)
9777.7 (42.84) 9901.0 (146.07) 32784.2 (190.18)
SOLVE
1739.3 (0.45) 2015.9 (0.66) 2357.3 (1.70) 2909.1 (1.19) 19567.1 (10.34) 23196.6 (98.35) 25769.5 (20.12) 92214.9 (452.18)
SWSGP
875.7 (0.68)
1023.5 (0.35) 1220.6 (1.89) 1458.0 (5.57) 10203.4 (12.03) 12241.7 (62.01) 13371.5 (12.34) 46163.3 (427.23)
IDSGP
190.3 (0.75)
371.5 (1.25)
533.0 (1.73)
693.7 (5.77) 4070.1 (201.09) 4296.5 (25.03)
3640.4 (33.36)
16352.2 (90.15)"
EXPERIMENTS ON UCI DATASETS,0.3247588424437299,Under review as a conference paper at ICLR 2022
LARGE SCALE DATASETS,0.3279742765273312,"5.3
LARGE SCALE DATASETS"
LARGE SCALE DATASETS,0.3311897106109325,"A last set of experiments considers two very large datasets. The ﬁrst dataset is the Airlines Delay
binary classiﬁcation dataset, as described in Hern´andez-Lobato & Hern´andez-Lobato (2016), with
N = 2, 127, 068 data instances and d = 8 attributes. The second dataset is the Yellow taxi dataset, as
described in Salimbeni & Deisenroth (2017), with N = 1 billion data-points and d = 9 attributes. In
each dataset we use a test set of 10, 000 instances chosen at random. The number of inducing points
is set to be equal to M = 50 in IDSGP. In the other methods, we use the same number of inducing
points as in the previous section. The mini-batch size is set to 100. Training is also performed on the
same GPU as in the previous section. The ADAM learning rate is set to 0.001."
LARGE SCALE DATASETS,0.33440514469453375,"The average negative test log-likelihood of each method is displayed in Figure 4, for each dataset.
We report performance in terms of the training time, in a log10 scale. The results corresponding to
the RMSE are very similar to the ones displayed here. They can be found in Appendix D.5. We
observe that the proposed method IDSGP performs best on each dataset. In particular, it obtains a
better performance in a smaller computational time. We believe this is a consequence of using a
smaller number of inducing points, and also because of the extra ﬂexibility that the DNN provides for
specifying the locations of the inducing points. 6.75 7.00 7.25 7.50 7.75"
LARGE SCALE DATASETS,0.33762057877813506,"0
1
2
3
4
5
Training time in log10 scale"
LARGE SCALE DATASETS,0.3408360128617363,Negative Test Log−Likelihood IDSGP SOLVE SWSGP VSGP
LARGE SCALE DATASETS,0.3440514469453376,Yellow Taxi 0.60 0.65
LARGE SCALE DATASETS,0.34726688102893893,"0
1
2
3
4
5
Training time in log10 scale"
LARGE SCALE DATASETS,0.3504823151125402,Negative Test Log−Likelihood IDSGP SOLVE SWSGP VSGP
LARGE SCALE DATASETS,0.3536977491961415,Airplane Delays
LARGE SCALE DATASETS,0.35691318327974275,"Figure 4: Negative log-likelihood on the test set for each method as a function of the training time in
seconds, in log10 scale, for the Yellow taxi and the Airline delays datasets. Best seen in color."
CONCLUSIONS,0.36012861736334406,"6
CONCLUSIONS"
CONCLUSIONS,0.3633440514469453,"Gaussian processes (GPs) are ﬂexible models for regression and classiﬁcation. However, they have
a cost of O(N 3) per iteration with N the number of training points. Sparse approximations based
on M ≪N inducing points reduce such a cost to O(M 3). A problem, however, is that in some
situations a large number of inducing points have to be used in practice, since they determine the
ﬂexibility of the resulting approximation. There is hence a need to reduce their training cost."
CONCLUSIONS,0.3665594855305466,"We have proposed here input dependent sparse GP (IDSGP), a method that can improve the training
time and the ﬂexibility of sparse GP approximations. IDSGP uses a deep neural network (DNN) to
output speciﬁc inducing points for each point at which the predictive distribution of the GP needs to be
computed. The DNN also outputs the parameters of the corresponding variational approximation on
the inducing values associated to the inducing points. IDSGP can be obtained under a formulation that
considers an implicit distribution for the input instance to the DNN. Importantly, such a formulation
is shown to keep intact the GP prior on the latent function values associated to the training points."
CONCLUSIONS,0.36977491961414793,"The extra ﬂexibility provided by the DNN allows to signiﬁcantly reduce the number M of induc-
ing points used in IDSGP. Such a model provides similar or better results than other sparse GP
approximations from the literature at a smaller training cost. IDSGP has been evaluated on several
regression and binary classiﬁcation problems from the UCI repository. The results obtained show
that it improves the quality of the predictive distribution and reduces the training cost of sparse GP
approximations. Better results are most of the times obtained in regression problems. In classiﬁcation
problems, however, the performances obtained are similar to those of the state-of-the-art, although
the training and prediction times are always shorter. The scalability of IDSGP is also illustrated on
massive datasets for regression and binary classiﬁcation of up to 1 billion points. There, IDSGP also
obtains better results than alternative sparse GP approximations at a smaller training cost."
CONCLUSIONS,0.3729903536977492,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.3762057877813505,ETHICS STATEMENT
ETHICS STATEMENT,0.37942122186495175,"The authors acknowledge to have read and commit to adhering to the ICLR Code of Ethics 1. We do
not see any direct potential negative societal impact, because this paper focuses on the development
of a new methodology. We believe these would be indirect through the particular application in which
the proposed method is used. As one of the main advantages of GPs is that they provide uncertainty
estimates associated with the predictions made, we think the potential harm of these models in society
could arise in applications when this uncertainty estimation is critical. For example, an AI system in
which the decisions made can have an inﬂuence on people’s life, such as autonomous vehicles or
automatic medical diagnosis tools."
REPRODUCIBILITY STATEMENT,0.38263665594855306,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.3858520900321543,"Most of the details needed to reproduce this paper’s results are described in Section 5. In Appendix B
we give further details about the neural network architecture and initialization. We provide the code
used to run the experiments in the supplementary material, with implementation of our method and
the rest of methods of the experimental comparison. Regarding the pre-processing of the data, you
can ﬁnd all the information needed to reproduce this step in Appendix A."
REFERENCES,0.3890675241157556,REFERENCES
REFERENCES,0.39228295819935693,"M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz,
L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas,
O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-
scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/.
Software available from tensorﬂow.org."
REFERENCES,0.3954983922829582,"M. Bauer, M. van der Wilk, and C. E. Rasmussen. Understanding probabilistic sparse Gaussian
process approximations. In Advances in Neural Information Processing Systems 29, pp. 1533–1541.
2016."
REFERENCES,0.3987138263665595,"C.-A. Cheng and B. Boots. Variational inference for Gaussian process models with linear complexity.
In Advances in Neural Information Processing Systems, pp. 5184–5194, 2017."
REFERENCES,0.40192926045016075,"L. Csat´o and M. Opper. Sparse on-line Gaussian processes. Neural Computation, 14:641–668, 2002."
REFERENCES,0.40514469453376206,"D. Dua and C. Graff. UCI machine learning repository, 2017. URL http://archive.ics.
uci.edu/ml."
REFERENCES,0.40836012861736337,"T. Evans and P. Nair. Scalable Gaussian processes with grid-structured eigenfunctions (GP-GRIEF).
In International Conference on Machine Learning, pp. 1417–1426, 2018."
REFERENCES,0.4115755627009646,"Y. Gal. Uncertainty in deep learning. PhD thesis, University of Cambridge, 2016."
REFERENCES,0.41479099678456594,"J. Gardner, G. Pleiss, R. Wu, K. Weinberger, and A. G. Wilson. Product kernel interpolation for
scalable Gaussian processes. In International Conference on Artiﬁcial Intelligence and Statistics,
pp. 1407–1416, 2018."
REFERENCES,0.4180064308681672,"Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and
statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010."
REFERENCES,0.4212218649517685,"R. B. Gramacy and D. W. Apley. Local Gaussian process approximation for large computer experi-
ments. Journal of Computational and Graphical Statistics, 24:561–578, 2015."
REFERENCES,0.42443729903536975,"R. Henao and O. Winther. Predictive active set selection methods for Gaussian processes. Neurocom-
puting, 80:10–18, 2012."
REFERENCES,0.42765273311897106,1https://iclr.cc/public/CodeOfEthics
REFERENCES,0.43086816720257237,Under review as a conference paper at ICLR 2022
REFERENCES,0.4340836012861736,"J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for big data. In Uncertainty in
Artiﬁcial Intelligence, pp. 282–290, 2013."
REFERENCES,0.43729903536977494,"J. Hensman, A. Matthews, and Z. Ghahramani. Scalable variational Gaussian process classiﬁcation.
In International Conference on Artiﬁcial Intelligence and Statistics, pp. 351–360, 2015a."
REFERENCES,0.4405144694533762,"J. Hensman, A. G. Matthews, M. Filippone, and Z. Ghahramani. MCMC for variationally sparse
Gaussian processes. In Advances in Neural Information Processing Systems 28, pp. 1648–1656.
2015b."
REFERENCES,0.4437299035369775,"D. Hern´andez-Lobato and J. M. Hern´andez-Lobato. Scalable Gaussian process classiﬁcation via
expectation propagation. In Artiﬁcial Intelligence and Statistics, pp. 168–176, 2016."
REFERENCES,0.44694533762057875,"D. P. Kingma and J. Ba. ADAM: a method for stochastic optimization. In Inrernational Conference
on Learning Representations, pp. 1–15, 2015."
REFERENCES,0.45016077170418006,"D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on
Learning Representations, 2014."
REFERENCES,0.4533762057877814,"N. Lawrence, M. Seeger, and R. Herbrich. Fast sparse Gaussian process methods: The informative
vector machine. In Neural Information Processing Systems, pp. 609–616, 2003."
REFERENCES,0.4565916398713826,"A. Naish-Guzman and S. Holden. The generalized FITC approximation. Advances in neural
information processing systems, 20:1057–1064, 2007."
REFERENCES,0.45980707395498394,"F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825–2830, 2011."
REFERENCES,0.4630225080385852,"J. Qui˜nonero-Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian process
regression. Journal of Machine Learning Research, 6:1939–1959, 2005."
REFERENCES,0.4662379421221865,"C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning (Adaptive
Computation and Machine Learning). The MIT Press, 2006."
REFERENCES,0.4694533762057878,"H. Salimbeni and M. Deisenroth. Doubly stochastic variational inference for deep Gaussian processes.
In Advances in Neural Information Processing Systems, pp. 4588–4599, 2017."
REFERENCES,0.47266881028938906,"H. Salimbeni, C.-A. Cheng, B. Boots, and M. Deisenroth. Orthogonally decoupled variational
Gaussian processes. In Neural Information Processing Systems, pp. 8725–8734, 2018a."
REFERENCES,0.4758842443729904,"H. Salimbeni, S. Eleftheriadis, and J. Hensman. Natural gradients in practice: Non-conjugate
variational inference in Gaussian process models. In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 689–697, 2018b."
REFERENCES,0.4790996784565916,"M. W. Seeger, C. K. I. Williams, and N. D. Lawrence. Fast forward selection to speed up sparse
Gaussian process regression. In Proceedings of the Ninth International Workshop on Artiﬁcial
Intelligence and Statistics, pp. 254–261, 2003."
REFERENCES,0.48231511254019294,"J. Shi, M. Titsias, and A. Mnih. Sparse orthogonal variational inference for Gaussian processes. In
International Conference on Artiﬁcial Intelligence and Statistics, pp. 1932–1942, 2020."
REFERENCES,0.4855305466237942,"Jiaxin Shi, Mohammad Emtiyaz Khan, and Jun Zhu. Scalable training of inference networks for
gaussian-process models. In International Conference on Machine Learning, pp. 5758–5768.
PMLR, 2019."
REFERENCES,0.4887459807073955,"R. Shu, H. H Bui, S. Zhao, M. J. Kochenderfer, and S. Ermon. Amortized inference regularization.
In Advances in Neural Information Processing Systems, pp. 4393–4402, 2018."
REFERENCES,0.4919614147909968,"E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in
Neural Information Processing Systems, pp. 1257–1264, 2006."
REFERENCES,0.49517684887459806,"Z. Snelson, E.and Ghahramani. Local and global sparse Gaussian process approximations. In
International Conference on Artiﬁcial Intelligence and Statistics, pp. 524–531, 2007."
REFERENCES,0.4983922829581994,Under review as a conference paper at ICLR 2022
REFERENCES,0.5016077170418006,"Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse. Functional variational bayesian
neural networks. In International Conference on Learning Representations, 2019."
REFERENCES,0.5048231511254019,"M.K. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In International
Conference on Artiﬁcial Intelligence and Statistics, pp. 567–574, 2009."
REFERENCES,0.5080385852090032,"G.-L. Tran, D. Milios, P. Michiardi, and M. Filippone. Sparse within sparse Gaussian processes using
neighbor information, 2020. 2011.05041 arXiv stat.ML."
REFERENCES,0.5112540192926045,"V. Tresp. A Bayesian committee machine. Neural Computation, 12:2719–2741, 2000."
REFERENCES,0.5144694533762058,"C. Villacampa-Calvo, B. Zald´ıvar, E. C. Garrido-Merch´an, and D. Hern´andez-Lobato. Multi-class
gaussian process classiﬁcation with noisy inputs. Journal of Machine Learning Research, 22:1–52,
2021."
REFERENCES,0.5176848874598071,"A. G. Wilson and H. Nickisch. Kernel interpolation for scalable structured Gaussian processes
(kiss-gp). In International Conference on International Conference on Machine Learning, pp.
1775–1784, 2015."
REFERENCES,0.5209003215434084,"A
DATASETS PRE-PROCESSING"
REFERENCES,0.5241157556270096,"All the datasets are publicly available. The UCI repository datasets can be downloaded from
the repository (Dua & Graff, 2017). Yellow taxi dataset was preprocessed following Salimbeni
& Deisenroth (2017) and downloaded from https://www1.nyc.gov/site/tlc/about/
tlc-trip-record-data.page, where we have used data records from year 2015. Similarly,
the Airlines Delay dataset was preprocessed following Hern´andez-Lobato & Hern´andez-Lobato (2016)
and was downloaded from https://community.amstat.org/jointscsg-section/
dataexpo/dataexpo2009, keeping only the records from January 2008 to April 2008. All
datasets have been standardized using scikit-learn’s built-int StandardScaler class (Pedregosa et al.,
2011), which removes the mean and scales to unit variance."
REFERENCES,0.5273311897106109,"B
NEURAL NETWORK ARCHITECTURE"
REFERENCES,0.5305466237942122,"About the choice of architecture for the DNN we have tried to keep it small in order to take more
advantage of the computational gain of the amortized scheme. In particular, we used a 2 hidden-layer
with 50 hidden units network for the toy problems in Section 5.1, a 1 layer with 50 hidden units
network for the UCI datasets in Section 5.2 and a 2 layer with 25 hidden units for the large scale
datasets in Section 5.3. We used ReLu activation functions. Keeping the network small reduces the
number of parameters to optimize making the optimization process easier. In all problems we are
using fully-connected layers with batch normalization and no skip layers. Regarding the initialization
of the weights, all were initialized using the Glorot initialization (Glorot & Bengio, 2010). In our
experiments we did not exhaustively explore the DNN architecture. This choice of architecture and
initialization was based on some preliminary tests done before running the experiments. This does
not mean that this is the best possible conﬁguration. We did not optimize the architecture of the
neural network. In practical applications, we suggest to run some preliminary tests in order to choose
a conﬁguration that performs well. The main suggestion, however, is to keep the network small as the
input dependence will make the model expressive enough to still get very good results."
REFERENCES,0.5337620578778135,"C
CHOOSING THE NUMBER OF INDUCING POINTS"
REFERENCES,0.5369774919614148,"We have observed that our proposed method IDSGP performs well in general with a fairly small
number of inducing points, much smaller than the number of inducing points used in the other
methods. Namely, SOLVE, VSGP and SWSGP. This is probably related to the extra ﬂexibility of
having input-dependent inducing points in IDSGP. In very large datasets we recommend using around
M = 50 inducing points. In medium-size regression datasets M = 15 inducing points seem enough.
In medium-size binary classiﬁcation datasets, however, a smaller number of inducing points is enough
M = 3. We believe the reason is that binary classiﬁcation problems require less complicated latent
functions. We did not optimize the number of inducing points. In practical applications, we suggest
to run some preliminary tests in order to choose a number of inducing points that performs well."
REFERENCES,0.5401929260450161,Under review as a conference paper at ICLR 2022
REFERENCES,0.5434083601286174,"C.1
KL-DIVERGENCE MINIMIZATION"
REFERENCES,0.5466237942122186,"In this section we show that maximizing (4) effectively minimizes the KL-divergence between
q(˜x, f, u) and p(˜x, f, u|y). In particular,"
REFERENCES,0.5498392282958199,"KL(q(˜x, f, u)|p(˜x, f, u|y)) = −
Z
q(˜x, f, u) log p(y, ˜x, f, u)"
REFERENCES,0.5530546623794212,"q(˜x, f, u) dxdfdu + const."
REFERENCES,0.5562700964630225,"= −
Z
q(˜x, f, u) log p(y|f)p(f|u)p(u|˜x)p(˜x)"
REFERENCES,0.5594855305466238,"p(f|u)q(u|˜x)p(˜x)
dxdfdu + const."
REFERENCES,0.5627009646302251,"= −
Z
q(˜x, f, u) log p(y|f)p(u|˜x)"
REFERENCES,0.5659163987138264,"q(u|˜x)
dxdfdu + const."
REFERENCES,0.5691318327974276,"= −
Z
q(f, ˜x, u) log p(y|f)dfdxdu"
REFERENCES,0.572347266881029,"+
Z
q(u, ˜x) log p(u|˜x)"
REFERENCES,0.5755627009646302,q(u|˜x)dxdu + const.
REFERENCES,0.5787781350482315,= −Eq[log p(y|f)] + Ep(˜x)[KL(q(u|˜x)|p(u|˜x))]
REFERENCES,0.5819935691318328,"= −L + const. ,
(7)"
REFERENCES,0.5852090032154341,"where we have used that the posterior is equal to the joint p(˜x, f, u, y) divided by a normalization
constant, i.e., the marginal likelihood. Moreover, L is simply the lower bound deﬁned in (4). There-
fore, maximizing L effectively leads to the minimization of the KL-divergence between q(˜x, f, u)
and p(˜x, f, u|y)."
REFERENCES,0.5884244372990354,"D
EXTRA EXPERIMENTAL RESULTS"
REFERENCES,0.5916398713826366,"In this section, we include some extra results that do not ﬁt in the main manuscript. Namely, the
RMSE in the test set results and prediction times for the UCI regression datasets, and the accuracy in
the test set, training and prediction times for the UCI classiﬁcation datasets. In both cases, the setup
is the same as described in Section 5 and the results are similar that the ones obtained in terms of
the negative test log likelihood and training times in that section. Finally, we include similar plots to
those in Section 5.3 but in terms of the test RMSE for the Yellow Taxi dataset and in terms of the test
classiﬁcation error for the Airline Delays dataset."
REFERENCES,0.594855305466238,"D.1
TOY REGRESSION DATASETS"
REFERENCES,0.5980707395498392,"Our method looks more and more similar to the full GP as number of inducing points M increases.
However, with a small number of inducing points, it gives similar results to those of the full GP and
similar to the results obtained when more inducing points are considered, which does not happen in
the other methods. This is probably due to the extra ﬂexibility of the neural network. The ﬁgures
below (Figures 6 to 8) show the results of each method on the toy regression problem as we increase
the number of inducing points M. For M = 128 IDSGP gives almost the same results as VSGP."
REFERENCES,0.6012861736334405,Under review as a conference paper at ICLR 2022
REFERENCES,0.6045016077170418,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.6077170418006431,IDSGP (number of inducing points for each point = 2)
REFERENCES,0.6109324758842444,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points for X
point X"
REFERENCES,0.6141479099678456,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.617363344051447,IDSGP (number of inducing points for each point = 4)
REFERENCES,0.6205787781350482,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points for X
point X"
REFERENCES,0.6237942122186495,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.6270096463022508,IDSGP (number of inducing points for each point = 8)
REFERENCES,0.6302250803858521,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points for X
point X"
REFERENCES,0.6334405144694534,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.6366559485530546,IDSGP (number of inducing points for each point = 16)
REFERENCES,0.639871382636656,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points for X
point X"
REFERENCES,0.6430868167202572,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.6463022508038585,IDSGP (number of inducing points for each point = 32)
REFERENCES,0.6495176848874598,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points for X
point X"
REFERENCES,0.6527331189710611,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.6559485530546624,IDSGP (number of inducing points for each point = 64)
REFERENCES,0.6591639871382636,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points for X
point X"
REFERENCES,0.662379421221865,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.6655948553054662,IDSGP (number of inducing points for each point = 128)
REFERENCES,0.6688102893890675,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points for X
point X"
REFERENCES,0.6720257234726688,"Figure 5:
Toy regression example by varying number of inducing points
Mx
=
{2, 4, 8, 16, 32, 64, 128} with location of initial and ﬁnal inducing points for an arbitrary selected
point x from training sets. The mean and standard deviation of full GP prediction are shown with
dashed blue and brown lines, respectively. The blue lines and the dashed red lines are the mean and
standard deviation of IDSGP."
REFERENCES,0.6752411575562701,Under review as a conference paper at ICLR 2022
REFERENCES,0.6784565916398714,"2
0
2
4
6
8
10 3 2 1 0 1 2"
REFERENCES,0.6816720257234726,"SOLVE (inducing points U = 2, inducing points V = 2)"
REFERENCES,0.684887459807074,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points U
Inducing points V"
REFERENCES,0.6881028938906752,"2
0
2
4
6
8
10 3 2 1 0 1 2"
REFERENCES,0.6913183279742765,"SOLVE (inducing points U = 4, inducing points V = 4)"
REFERENCES,0.6945337620578779,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points U
Inducing points V"
REFERENCES,0.6977491961414791,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.7009646302250804,"SOLVE (inducing points U = 8, inducing points V = 8)"
REFERENCES,0.7041800643086816,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points U
Inducing points V"
REFERENCES,0.707395498392283,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.7106109324758842,"SOLVE (inducing points U = 16, inducing points V = 16)"
REFERENCES,0.7138263665594855,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points U
Inducing points V"
REFERENCES,0.7170418006430869,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.7202572347266881,"SOLVE (inducing points U = 32, inducing points V = 32)"
REFERENCES,0.7234726688102894,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points U
Inducing points V"
REFERENCES,0.7266881028938906,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.729903536977492,"SOLVE (inducing points U = 64, inducing points V = 64)"
REFERENCES,0.7331189710610932,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points U
Inducing points V"
REFERENCES,0.7363344051446945,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.7395498392282959,"SOLVE (inducing points U = 128, inducing points V = 128)"
REFERENCES,0.7427652733118971,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points U
Inducing points V"
REFERENCES,0.7459807073954984,"Figure 6:
Toy regression example by varying number of inducing points Mu, Mv
=
{2, 4, 8, 16, 32, 64, 128} with location of initial and ﬁnal inducing points. The mean and standard
deviation of full GP prediction are shown with dashed blue and brown lines, respectively. The blue
lines and the dashed red lines are the mean and standard deviation of SOLVE."
REFERENCES,0.7491961414790996,Under review as a conference paper at ICLR 2022
REFERENCES,0.752411575562701,"2
0
2
4
6
8
10 3 2 1 0 1 2"
REFERENCES,0.7556270096463023,VSGP (number of inducing points = 2)
REFERENCES,0.7588424437299035,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.7620578778135049,"2
0
2
4
6
8
10
3 2 1 0 1 2"
REFERENCES,0.7652733118971061,VSGP (number of inducing points = 4)
REFERENCES,0.7684887459807074,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.7717041800643086,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.77491961414791,VSGP (number of inducing points = 8)
REFERENCES,0.7781350482315113,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.7813504823151125,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.7845659163987139,VSGP (number of inducing points = 16)
REFERENCES,0.7877813504823151,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.7909967845659164,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.7942122186495176,VSGP (number of inducing points = 32)
REFERENCES,0.797427652733119,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.8006430868167203,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.8038585209003215,VSGP (number of inducing points = 64)
REFERENCES,0.8070739549839229,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.8102893890675241,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.8135048231511254,VSGP (number of inducing points = 128)
REFERENCES,0.8167202572347267,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.819935691318328,"Figure
7:
Toy
regression
example
by
varying
number
of
inducing
points
M
=
{2, 4, 8, 16, 32, 64, 128} with location of initial and ﬁnal inducing points. The mean and standard
deviation of full GP prediction are shown with dashed blue and brown lines, respectively. The blue
lines and the dashed red lines are the mean and standard deviation of VSGP."
REFERENCES,0.8231511254019293,Under review as a conference paper at ICLR 2022
REFERENCES,0.8263665594855305,"2
0
2
4
6
8
10 3 2 1 0 1 2"
REFERENCES,0.8295819935691319,"SWSGP (number of closest inducing points = 2, total number of inducing points = 128)"
REFERENCES,0.8327974276527331,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.8360128617363344,"2
0
2
4
6
8
10
3 2 1 0 1 2"
REFERENCES,0.8392282958199357,"SWSGP (number of closest inducing points = 4, total number of inducing points = 128)"
REFERENCES,0.842443729903537,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.8456591639871383,"2
0
2
4
6
8
10 3 2 1 0 1 2 3"
REFERENCES,0.8488745980707395,"SWSGP (number of closest inducing points = 8, total number of inducing points = 128)"
REFERENCES,0.8520900321543409,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.8553054662379421,"2
0
2
4
6
8
10
3 2 1 0 1 2"
REFERENCES,0.8585209003215434,"SWSGP (number of closest inducing points = 16, total number of inducing points = 128)"
REFERENCES,0.8617363344051447,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.864951768488746,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.8681672025723473,"SWSGP (number of closest inducing points = 32, total number of inducing points = 128)"
REFERENCES,0.8713826366559485,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.8745980707395499,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.8778135048231511,"SWSGP (number of closest inducing points = 64, total number of inducing points = 128)"
REFERENCES,0.8810289389067524,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.8842443729903537,"2
0
2
4
6
8
10 2 1 0 1 2"
REFERENCES,0.887459807073955,"SWSGP (number of closest inducing points = 128, total number of inducing points = 128)"
REFERENCES,0.8906752411575563,"mean prediction
mean GP prediction
Standard deviation
Standard deviation GP
Training data
Initial inducing points
Inducing points"
REFERENCES,0.8938906752411575,"Figure 8: Toy regression example by varying number of the neighbor inducing points Mc =
{2, 4, 8, 16, 32, 64, 128} and total number of inducing points M = 128, with location of initial and
ﬁnal inducing points. The mean and standard deviation of full GP prediction are shown with dashed
blue and brown lines, respectively. The blue lines and the dashed red lines are the mean and standard
deviation of SWSGP."
REFERENCES,0.8971061093247589,"D.2
EXTRA RESULTS FOR THE TOY REGRESSION EXPERIMENT"
REFERENCES,0.9003215434083601,"Here we run the 1D toy regression experiment of Section 5.1 using the closed-form solution approach
of Titsias (2009) for ﬁnding q. More precisely, this method is exactly the same as the SVGP method
we compare results with, but where the approximate distribution q is not optimized at all. The reason
for this is that it is possible to ﬁnd a closed-form solution for q. However and importantly, the
resulting method does not allow for mini-batch training. Since SVGP* does not allow for stochastic
optimization, the batch size is set equal to the number of training points (N = 200). Figure 9 shows
the ﬁt obtained for an increasing number of inducing points M. The results are very similar to the
ones of SVGP in Figure 7."
REFERENCES,0.9035369774919614,Under review as a conference paper at ICLR 2022
REFERENCES,0.9067524115755627,"Figure
9:
Toy
regression
example
by
varying
number
of
inducing
points
M
=
{2, 4, 8, 16, 32, 64, 128} with location of initial and ﬁnal inducing points. The mean and standard
deviation of full GP prediction are shown with dashed blue and brown lines, respectively. The blue
lines and the dashed red lines are the mean and standard deviation of VSGP*."
REFERENCES,0.909967845659164,"D.3
UCI REGRESSION DATASETS"
REFERENCES,0.9131832797427653,"Table 4: Test Root Mean Squared Error (RMSE) values for the UCI regression datasets. The numbers
in parentheses are standard errors. Best mean values are highlighted."
REFERENCES,0.9163987138263665,"N
d
VSGP
SOLVE
SWSGP
IDSGP
Kin40k
32,000
8
0.198 (0.002)
0.157 (0.001)
0.215 (0.002)
0.050 (0.002)
Protein
36,584
9
4.161 (0.011)
4.062 (0.011)
4.133 (0.008)
3.756 (0.019)
KeggDirected
42,730
19
0.032 (0.001)
0.079 (0.032)
0.024 (0.000)
0.022 (0.001)
KEGGU
51,686
26
0.024 (0.000)
0.020 (0.000)
0.022 (0.000)
0.014 (0.000)
3dRoad
347,899
3
9.641 (0.063)
10.020 (0.095)
11.726 (0.327)
7.250 (0.069)
Song
412,276
90
8.966 (0.022)
8.925 (0.020)
9.013 (0.029)
9.068 (0.011)
Buzz
466,600
77
175.076 (15.021) 173.352 (14.957) 160.744 (13.467) 166.784 (18.040)
HouseElectric
1,639,424 6
0.035 (0.000)
0.034 (0.000)
0.036 (0.001)
0.032 (0.000)
Avg. Ranks
3.075 (0.126)
2.400 (0.138)
3.025 (0.170)
1.500 (0.151)
# Inducing points
1024
1024 / 1024
(K=50) / 1024
15"
REFERENCES,0.9196141479099679,Under review as a conference paper at ICLR 2022
REFERENCES,0.9228295819935691,"Table 5: Average prediction time per epoch across the 5 splits for the UCI regression datasets. The
numbers in parentheses are standard errors. Best mean values are highlighted."
REFERENCES,0.9260450160771704,"Kin40k Protein KeggDirected KEGGU
3dRoad
Song
Buzz
HouseElectric
VSGP
0.9(0.00) 1.1(0.0)
1.4(0.01)
1.7(0.01) 11.6(0.07) 14.5(0.08) 18.0(0.08)
48.3(0.12)
SOLVE 2.4(0.00) 2.8(0.0)
3.2(0.00)
4.0(0.00) 27.0(0.04) 31.8(0.19) 37.0(0.03) 127.7(0.43)
SWSGP 1.3(0.00) 1.5(0.0)
1.8(0.01)
2.1(0.01) 14.8(0.03) 17.6(0.02) 21.3(0.10)
66.2(0.88)
IDSGP 0.3(0.00) 0.5(0.0)
0.7(0.00)
1.0(0.02) 5.7(0.26) 5.6(0.05) 8.1(0.08)
22.1(0.14)"
REFERENCES,0.9292604501607717,"D.4
UCI CLASSIFICATION DATASETS"
REFERENCES,0.932475884244373,"Table 6: Test Accuracy values for the UCI classiﬁcation datasets. The numbers in parentheses are
standard errors. Best mean values are highlighted."
REFERENCES,0.9356913183279743,"N
d
VSGP
SOLVE
SWSGP
IDSGP
MagicGamma
15,216
10
0.876 (0.001) 0.877 (0.002) 0.867 (0.002) 0.877 (0.002)
DefaultOrCredit
24,000
30
1.000 (0.000) 1.000 (0.000) 1.000 (0.000) 1.000 (0.000)
NOMAO
27,572
174
0.956 (0.002) 0.960 (0.001) 0.961 (0.001) 0.955 (0.001)
BankMarketing
36,169
51
0.906 (0.001) 0.907 (0.001) 0.897 (0.001) 0.905 (0.001)
Miniboone
104,051 50
0.941 (0.001) 0.945 (0.001) 0.938 (0.000) 0.937 (0.001)
Skin
196,046 3
0.999 (0.000) 0.999 (0.000) 0.999 (0.000) 0.999 (0.000)
Crop
260,667 174
0.999 (0.000) 0.999 (0.000) 0.999 (0.000) 0.999 (0.000)
HTSensor
743,193 11
0.999 (0.000) 1.000 (0.000) 0.989 (0.003) 0.999 (0.000)
Avg. Ranks
2.475 (0.127) 1.975 (0.152) 2.900 (0.187) 2.650 (0.168)
# Inducing points
1024
1024 / 1024
(K=50) / 1024
3"
REFERENCES,0.9389067524115756,"Table 7: Average training time per epoch across the 5 splits for the UCI classiﬁcation datasets. The
numbers in parentheses are standard errors. Best mean values are highlighted."
REFERENCES,0.9421221864951769,"Magic
DefaultOrCredit
NOMAO
BankMarket Miniboone
Skin
Crop
HTSensor
VSGP
3105(459)
4759(516)
4445(549)
6231(862)
18447(1279) 37835(7065)
49962(9292) 115463(17511)
SOLVE
5154(1061)
7554(1039)
6718(1028) 8949(1635) 37022(7902) 64606(13314) 88819(18864) 168709(21194)
SWSGP
1547(145)
2354(182)
2728(188)
3682(351)
10040(347)
20283(2796)
21770(3038)
67687(5880)
IDSGP
1143(100)
1293(90)
2026(94)
2987(354)
7654(134)
15700(1918)
21378(2561)
53895(5652)"
REFERENCES,0.9453376205787781,"Table 8: Average prediction time per epoch across the 5 splits for the UCI classiﬁcation datasets. The
numbers in parentheses are standard errors. Best mean values are highlighted."
REFERENCES,0.9485530546623794,"MagicGamma DefaultOrCredit NOMAO BankMarketing Miniboone
Skin
Crop
HTSensor
VSGP
3.6(0.56)
4.1(0.59)
4.4(0.74)
9.5(4.39)
17.9(1.84) 49.7(11.15) 58.7(12.82) 139.7(41.67)
SOLVE
4.0(0.85)
5.4(0.73)
3.4(0.76)
4.9(1.24)
49.2(17.83) 48.8(16.73) 86.8(36.26) 93.5(15.73)
SWSGP
3.0(0.43)
3.9(0.38)
4.3(0.51)
5.4(0.72)
16.4(1.82)
36.0(8.00)
33.9(6.25)
88.4(9.16)
IDSGP
2.5(0.24)
2.5(0.21)
3.5(0.39)
4.8(0.54)
13.9(0.75)
26.1(4.92)
37.5(4.96)
83.4(8.23)"
REFERENCES,0.9517684887459807,"D.5
LARGE SCALE DATASETS 200 300 400 500"
REFERENCES,0.954983922829582,"0
1
2
3
4
5
Training time in log10 scale"
REFERENCES,0.9581993569131833,Test RMSE IDSGP SOLVE SWSGP VSGP
REFERENCES,0.9614147909967846,Yellow Taxi 0.30 0.35 0.40 0.45
REFERENCES,0.9646302250803859,"0
1
2
3
4
5
Training time in log10 scale"
REFERENCES,0.9678456591639871,Test Error IDSGP SOLVE SWSGP VSGP
REFERENCES,0.9710610932475884,Airplane Delays
REFERENCES,0.9742765273311897,"Figure 10: (left) Test RMSE for each method as a function of the training time in seconds, in log10
scale, for the Yellow taxi dataset. (right) Prediction error on the test set for each method as a function
of the training time in seconds, in log10 scale, for the Airlines Delays dataset. Best seen in color"
REFERENCES,0.977491961414791,Under review as a conference paper at ICLR 2022
REFERENCES,0.9807073954983923,"D.6
NEURAL NETWORK TRAINED VIA MAXIMUM LIKELIHOOD"
REFERENCES,0.9839228295819936,"In this subsection we show extra experiment results on the UCI datasets when using a neural network
trained via maximum likelihood. The architecture of the neural network is the same as the one of the
network used in the proposed method IDSGP. Training is done using ADAM. The learning rate used
is 0.001. The mini-batch size is the same for the GP-based methods. In regression, we use the neural
network to predict the mean and variance of the Gaussian predictive distribution. In classiﬁcation, we
use a sigmoid activation function. The average test negative log-likelihood obtained in each problem
is shown in Table 9 and Table 10. The results are high-lighted in bold-face when the NN performs
worse than any other GP based method. The tables show that, in the case of regression problems,
most of the times the neural network performs worse than the GP based methods. In the case of
classiﬁcation problems, the performance of the neural network is worse in those problems in which
the error is higher according to Table 6. By contrast, in those problems in which the accuracy is
almost equal to 100%, there are no differences or it performs slightly better."
REFERENCES,0.9871382636655949,"Table 9: Avg. neg. test log-likelihood values for the UCI regression datasets for the neural network.
The numbers in parentheses are standard errors."
REFERENCES,0.9903536977491961,"Kin40k
Protein
KeggDirected
KEGGU
3dRoad
Song
Buzz
HouseElectric
0.099(0.03) 2.794(0.02) -2.407(0.14) -5.124(0.18) 3.661(0.02) 3.363(0.01) 27.840(33.44) -2.110(0.03)"
REFERENCES,0.9935691318327974,"Table 10: Avg. neg. test log-likelihood values in the UCI classiﬁcation datasets for the neural network.
The numbers in parentheses are standard errors."
REFERENCES,0.9967845659163987,"MagicGamma DefaultOrCredit NOMAO BankMarketing Miniboone
Skin
Crop
HTSensor
0.315(0.02)
0.000(0.00)
0.119(0.01)
0.232(0.00)
0.152(0.00) 0.002(0.00) 0.002(0.00) 0.000(0.00)"
