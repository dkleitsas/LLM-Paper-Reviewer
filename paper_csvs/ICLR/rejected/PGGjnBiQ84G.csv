Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0051813471502590676,"In this paper, we present a novel approach to learn texture mapping for a 3D sur-
face and apply it to document image unwarping. We propose an efÔ¨Åcient method
to learn surface parameterization by learning a continuous bijective mapping be-
tween 3D surface positions and 2D texture-space coordinates. Our surface param-
eterization network can be conveniently plugged into a differentiable rendering
pipeline and trained using multi-view images and rendering loss. Recent work on
differentiable rendering techniques for implicit surfaces has shown high-quality
3D scene reconstruction and view synthesis results. However, these methods typi-
cally learn the appearance color as a function of the surface points and lack explicit
surface parameterization. Thus they do not allow texture map extraction or texture
editing. By introducing explicit surface parameterization and learning with a re-
cent differentiable renderer for implicit surfaces, we demonstrate state-of-the-art
document-unwarping via texture extraction. We show that our approach can re-
construct high-frequency textures for arbitrary document shapes in both synthetic
and real scenarios. We also demonstrate the usefulness of our system by applying
it to document texture editing."
INTRODUCTION,0.010362694300518135,"1
INTRODUCTION"
INTRODUCTION,0.015544041450777202,"Reconstructing 3D shapes from images is a core problem in computer vision and graphics research.
With the progress in differentiable rendering (Sitzmann et al., 2019b; Kato et al., 2018; Niemeyer
et al., 2020; Li et al., 2018; Liu et al., 2019b), recent learning-based 3D reconstruction approaches
have achieved impressive results using 2D supervision from single image (Chen & Zhang, 2019;
Groueix et al., 2018; Choy et al., 2016; Mescheder et al., 2019; Wang et al., 2018) or multi-view
images (Tang & Tan, 2018; Yariv et al., 2020). These methods achieve high quality 3D reconstruc-
tion using differentiable rendering with various 3D representations such as 3D mesh (Wang et al.,
2018), volumetric representation (Mildenhall et al., 2020), or implicit functions (Mescheder et al.,
2019). In recent neural rendering methods such as NeRF (Mildenhall et al., 2020) and IDR (Yariv
et al., 2020), continuous representations such as volume or implicit functions achieve signiÔ¨Åcantly
better reconstruction results than meshes or voxels because they do not discretize the 3D surface a
priori. However, these continuous representations usually do not encode explicit surface parame-
terization, allowing 3D shape re-texturing, editing the existing texture in the 2D texture space, or
recovering 2D texture from 3D surfaces. One of the most direct applications of 2D texture recov-
ery in a geometrically constrained manner, is document unwarping, i.e., inference of a document‚Äôs
Ô¨Çatbed-scanned version from a casual photo of a potentially creased document. Whereas 2D tex-
ture recovery could be equally valuable for other domains such as garments, or faces, the existing
datasets are not directly applicable to our method."
INTRODUCTION,0.02072538860103627,"Our novel texture mapping approach learns surface parameterization for document unwarping by
learning continuous bijective functions between 3D surface positions and 2D texture-space coor-
dinates. We use a signed distance function (SDF) (Chan & Zhu, 2005) to represent geometry and
model the appearance as a function of the 2D texture coordinates. By utilizing implicit differentiable
rendering (IDR), (Yariv et al., 2020) we can reconstruct 3D shape and learn the corresponding UV
parameterization of the surface simultaneously using a per-pixel rendering loss and appropriate ge-
ometric regularizations."
INTRODUCTION,0.025906735751295335,"We utilize two fully connected multi-layer perceptrons (MLPs) to learn a bijective mapping be-
tween 3D shapes and 2D texture space. More speciÔ¨Åcally, the forward MLP maps the 3D surface"
INTRODUCTION,0.031088082901554404,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03626943005181347,"Input
Unwarped
Edited
Texture edited images"
INTRODUCTION,0.04145077720207254,"Figure 1: Proposed forward-backward network can be utilized in unwarping or editing the surface
texture: The Ô¨Çattened texture can be edited and warped back to produce a texture edited image."
INTRODUCTION,0.046632124352331605,"coordinates to 2D texture coordinates and the backward MLP maps the 2D texture coordinates to
corresponding 3D surface coordinates. Following IDR (Yariv et al., 2020), we obtain the 3D surface
coordinates by sphere-tracing along the ray, cast through each pixel. Our appearance rendering is
formulated as a function of the 3D and the texture coordinates. Therefore, the forward and backward
MLPs can be trained with a 2D pixel-wise loss between the rendered image and the given ground
truth image. To the best of our knowledge, this is the Ô¨Årst neural rendering method that can learn
effective UV parameterization for implicit surfaces."
INTRODUCTION,0.05181347150259067,"As a corollary, our method is also the Ô¨Årst method which utilizes implicit surface based neural
rendering for document unwarping. It is a challenging task due to the presence of geometric and
photometric distortions in the document. For this particular problem we introduce a prior for shape-
speciÔ¨Åc texture mapping to initialize the forward MLP (3D to 2D mapping). This prior is learned
from a large dataset of UV mapped document meshes, assuming that document texture space maps
to a 2D equiangular quadrilateral. This assumption regularizes the forward MLP to output a high-
quality texture space that avoids degenerate solutions (see Fig. 3). Moreover, we introduce a con-
formality constraint in the backward MLP, which is consistent with how paper folds happen in the
physical world, i.e., without any stretch or tear. This constraint also ensures that the backward
function is bijective and smooth (Petrini et al., 2018)."
INTRODUCTION,0.05699481865284974,"The main contributions of our paper are the following: First, we propose an efÔ¨Åcient way to learn
texture parameterization for implicit neural representations using a differentiable rendering frame-
work. Without 3D supervision, it only requires multi-view images as ground-truth and a texture
mapping prior. Second, we show that our method can be effectively used for document unwarping
tasks by learning a prior for explicit texture mapping on the document shape. We show that this
prior can be learned from a dataset of texture-mapped meshes. Third, we show that our method is
effective for document image unwarping and texture editing (see Fig. 1). We achieve a 52% relative
improvement over the publicly available state-of-the-art1 (Das et al., 2019) in terms of mean local
distortion across 500 views from ten synthetic scenes. Additionally, we achieve a ‚àº25% improve-
ment in optical character recognition (OCR) in terms of character and word error rate."
PREVIOUS WORK,0.06217616580310881,"2
PREVIOUS WORK"
PREVIOUS WORK,0.06735751295336788,"Neural Rendering. Neural rendering generates images and videos by integrating conventional com-
puter graphics rendering pipelines into deep neural networks (Tewari et al., 2020). It enables explicit
or implicit control of scene properties, including illumination, geometry, texture, etc. Neural render-
ing can synthesize semantic photos (Park et al., 2019b; Bau et al., 2019), novel views (Hedman et al.,
2018; Sitzmann et al., 2019a), relighting (Xu et al., 2018; Meka et al., 2019), facial/body reenact-
ment (Chan et al., 2019; Wei et al., 2019), estimate scene properties etc. Kato (Kato et al., 2018)
proposed a differentiable neural renderer using an approximate gradient for rasterization. Liu (Liu
et al., 2019a) proposed SoftRas, which extended differentiable rasterization. Li (Li et al., 2018)
further demonstrated the feasibility of integrating ray-tracing in deep neural networks. More re-
cently, implicit surface or volume rendering has become mainstream in neural rendering approaches
such as IDR (Yariv et al., 2020) and NeRF (Mildenhall et al., 2020). These approaches are based"
PREVIOUS WORK,0.07253886010362694,"1A more recent approach, CREASE (Markovitz et al., 2020), data and models are not publicly available."
PREVIOUS WORK,0.07772020725388601,Under review as a conference paper at ICLR 2022
PREVIOUS WORK,0.08290155440414508,"on multi-view surface reconstruction to associate the scene geometry to the appearance in different
views. NeRF is extended to lot of variants including PixelNeRF (Yu et al., 2020), MVSNeRF (Chen
et al., 2021), dynamic NeRF (Li et al., 2020; Pumarola et al., 2020), GRAF (Schwarz et al., 2020)
and so on."
PREVIOUS WORK,0.08808290155440414,"Texture Mapping. Texture mapping is an essential step in the computer graphics rendering pipeline.
It deÔ¨Ånes a correspondence between a vertex on the 3D mesh and a pixel in the 2D texture image.
To Ô¨Ånd such a mapping, FlexiStickers (Tzur & Tal, 2009) required users to specify a sparse set of
correspondences. Bi (Bi et al., 2017) proposed a patch-based texture mapping method using the
3D shape and images from multiple views. Morreale (Morreale et al., 2021) used networks to rep-
resent 3D surfaces/shapes. Besides the above general texture mapping methods, some approaches
focus on a speciÔ¨Åc object categories such as faces (Deng et al., 2018; Chen et al., 2019) and human
bodies (Mir et al., 2020; Zhao et al., 2020). Recently, AtlasNet (Groueix et al., 2018) represented
a 3D mesh as a collection of parametric surfaces; thus, texture mapping is trivial to obtain from a
2D parametric surface. A similar idea was adopted by Bednarik (Bednarik et al., 2020) where they
introduced geometric constraints when learning the decomposition. More recently NeuTex (Xiang
et al., 2021) aims to recover the texture of a subject using NeRF (Mildenhall et al., 2020). How-
ever, NeuTex uses a spherical UV domain without any geometric constraints. Therefore, it is not
suitable for document unwarping. Moreover, since NeRF (Mildenhall et al., 2020) doesn‚Äôt learn
an explicit geometry, NeuTex requires a coarse point-cloud to initialize the backward MLP. With
an SDF based (Yariv et al., 2020) approach, our approach does not require such an initialization
routine. We jointly learn the texture mapping and the geometry from scratch."
PREVIOUS WORK,0.09326424870466321,"Document Unwarping. Document unwarping is a special application of texture mapping: the
3D object is usually a rectangular piecewise-developable surface, and the texture is well-structured,
containing straight text lines, (usually) rectangular text blocks and Ô¨Ågures, etc. Previous work usually
adopted a two-step methodology: 1) 3D surface estimation and 2) deformed surface Ô¨Çattening. The
3D surface of a deformed document can be estimated from shading (Wada et al., 1997), multi-view
images (Ulges et al., 2004), text lines (Tian & Narasimhan, 2011), local character orientations (Meng
et al., 2018), document boundaries (Koo et al., 2009), and learning-based strategies (Pumarola et al.,
2018). Flattening the obtained 3D surface always involves an expensive optimization process under
certain geometry constraints such as conformality (You et al., 2017) or isometries (Bartoli et al.,
2015). Flattening could be easier if the obtained 3D shape had a low dimensional parameterization
like Generalized Cylindrical Surface (GCS) (Kil et al., 2017). Some studies (Das et al., 2017; Liang
et al., 2008; Meng et al., 2015) proposed to unwarp each patch on the surface individually and then
stitch the unwarped patches together. In recent years, data-driven methods (Ma et al., 2018; Das
et al., 2019; Li et al., 2019; Markovitz et al., 2020; Das et al., 2021) have addressed document
unwarping by leveraging large-scale synthetic datasets. These datasets contain deformed document
images and their corresponding ground-truth UV coordinates. Methods trained on synthetic images
often suffer from generalization performance due to the domain gap between synthetic and real
data. In this paper, we utilize neural rendering techniques to learn a surface parameterization of a
deformed document. We simultaneously estimate both 3D shapes and UV coordinates with a cycle
consistency loss and geometric constraints. By leveraging the information from multi-view images,
the proposed method demonstrates better document unwarping performance compared to a previous
state-of-the-art, Das et al. (2019)."
METHOD,0.09844559585492228,"3
METHOD"
METHOD,0.10362694300518134,"A schematic diagram of the proposed approach is shown in Fig. 2. We utilize a recent differen-
tiable rendering method, IDR (Yariv et al., 2020) for surface reconstruction and jointly learn the
texture mapping of the learned implicit surface using two MLPs. In Sec. 3.1 we Ô¨Årst describe some
preliminaries about surface parameterization and IDR."
PRELIMINARIES,0.10880829015544041,"3.1
PRELIMINARIES"
PRELIMINARIES,0.11398963730569948,"Surface Parameterization. The problem of surface parameterization focuses on Ô¨Ånding a bijective
mapping F between a surface Z ‚ààR3 and a polygonal domain ‚Ñ¶‚ààRn. For a parametric or discrete
surface representation, we can explicitly compute this mapping (Tzur & Tal, 2009) using constrained
optimization. In contrast, implicit surfaces are represented as continuous functions and cannot be
readily parameterized. In this paper, we propose to learn such bijective mapping between a learned"
PRELIMINARIES,0.11917098445595854,"Under review as a conference paper at ICLR 2022 ùêπ!"" ùêπ# ùëß
ùë¢ ùë£ ùë¶ ùë•"
PRELIMINARIES,0.12435233160621761,"ÃÇùëß$
ùë°$
ùúè ùëù ùê∂$"
PRELIMINARIES,0.12953367875647667,"ùêø%
ùêø!"" ùêø& ùëç' IDR"
PRELIMINARIES,0.13471502590673576,"Figure 2: Proposed surface parameterization learning using the forward (Fuv) and backward MLP
(Fz): Given camera pose œÑ, and a pixel p we jointly learn the geometry represented by a SDF ZŒ∏, the
Fuv, and the Fz. ÀÜzp is the ray-surface intersection point in 3D domain and tp is the corresponding
texture coordinate in UV domain. The yellow arrows denote the input and output of the IDR (Yariv
et al., 2020), and Cp is the predicted RGB color. Triangles denote the losses deÔ¨Åned in Eq. 12."
PRELIMINARIES,0.13989637305699482,"implicit surface and a 2D planar domain ‚Ñ¶‚ààR2 using our proposed forward and backward MLPs.
‚Ñ¶is the texture space or UV space, parameterized using 2D UV coordinates t = (u, v). We can use
any continuous parameterization function as the UV space. Since this work particularly focuses on
document unwarping, we choose the UV space to be a regular 2D grid."
PRELIMINARIES,0.14507772020725387,"Implicit Differentiable Rendering. Implicit Differentiable Rendering (Yariv et al., 2020) recon-
structs the geometry of an object from multi-view images as the zero level set, ZŒ∏ of an MLP S,
ZŒ∏ = {z ‚ààR3 | S(z; Œ∏) = 0}
(1)
where Œ∏ are the learnable parameters. To render the surface ZŒ∏, IDR uses another MLP to model
the radiance (RGB color) as a function of the surface point (zp), corresponding surface normal (np),
view direction (vp) and a global geometry feature vector (gp):"
PRELIMINARIES,0.15025906735751296,"Cp = A(zp, np, vp, gp)
(2)
Here, Cp denote the predicted color at pixel p and A denotes the appearance MLP. The surface point
is obtained by a sphere-tracing method (Hart, 1996) along the ray rp(œÑ) through pixel p. œÑ ‚ààRk
denotes camera parameters of the scene. Additionally, IDR also presents a differentiable way to
obtain a ray and geometry intersection point (ÀÜzp) as a function of the camera ray. Although,the IDR
can disentangle geometry and appearance, it only allows to re-texture a new geometry with a learned
appearance MLP, A. Editing a texture or extracting a surface texture map is not possible in a vanilla
IDR framework since no explicit texture mapping is learned."
LEARNING SURFACE PARAMETERIZATION,0.15544041450777202,"3.2
LEARNING SURFACE PARAMETERIZATION
To learn a meaningful parameterization of the implicit surface ZŒ∏, we represent the radiance at pixel
p as a function of the UV space. To this end, we modify the IDR model (Eq. 2):
Cp = Auv(tp, zp, np, vp, gp)
(3)
The texture parameterized appearance MLP is modeled as a function of the texture coordinate tp
at surface point zp, corresponding to a pixel p. We can jointly train the surface MLP (S) and tex-
ture parameterized appearance MLP (Auv) using a pixel wise rendering loss between the predicted
radiance (Cp) and ground-truth radiance (Cgt
p ) at pixel p."
LEARNING SURFACE PARAMETERIZATION,0.16062176165803108,"Forward and backward texture parameterization. We represent the mapping between the 3D
surface and 2D texture space using the forward function Fuv:
z ‚Üít.
(4)
The Fuv is modeled as an MLP. It is trained by mapping a ray-surface intersection point ÀÜzp to its
corresponding texture coordinate tp corresponding to a pixel p. Now to establish the bijective map-
ping (discussed in Sec. 3.1) between the surface and texture space we utilize a backward function
Fz:
t ‚Üíz.
(5)
Fz is an MLP that learns an inverse mapping between the texture and the 3D space. It is trained by
mapping a texture coordinate tp to its corresponding ray-surface intersection point ÀÜzp."
LEARNING SURFACE PARAMETERIZATION,0.16580310880829016,"Shape speciÔ¨Åc prior for Fuv. Jointly training the forward, backward and rendering network leads"
LEARNING SURFACE PARAMETERIZATION,0.17098445595854922,Under review as a conference paper at ICLR 2022
LEARNING SURFACE PARAMETERIZATION,0.17616580310880828,"With UV prior
Without UV prior"
LEARNING SURFACE PARAMETERIZATION,0.18134715025906736,"Figure 3: Without a prior the for-
ward network, Fuv leads to degen-
erate cases: multiple 3D points, ÀÜzp
are mapped to the same texture co-
ordinate tp."
LEARNING SURFACE PARAMETERIZATION,0.18652849740932642,"to the wrong UV mapping with local minima (see Fig. 3)
where multiple ÀÜzp map to a single texture coordinate. To avoid
such degenerate cases, we initialize Fuv with a texture map-
ping prior, learned from a large dataset of UV mapped meshes.
This learned prior ( ÀÜFuv) makes the learned texture mapping
suitable for document unwarping. We assume the document
shape to be a deformed quadrilateral and the corresponding
UV space to be a regular grid (‚àà[0.0, 1.0]). The top leftmost
and the bottom rightmost 3D coordinate of the shape maps to
(u, v) = (0, 0) and (u, v) = (1, 1) respectively. To learn ÀÜFuv
we utilize a collection of UV mapped document meshes from
the Doc3D (Das et al., 2019) dataset and train an MLP with
the same parameters as Fuv. For each scene, we use ÀÜFuv to
initialize the weights of Fuv and train jointly with S and Auv."
LEARNING SURFACE PARAMETERIZATION,0.19170984455958548,"Deformation constraints for Fz.
Conformal map (Haker et al., 2000) allows a 3D
domain to be mapped
to a texture
domain
with
low
distortion
satisfying
the
bijec-
tive property between domains.
We use a conformality constraint for Fz
to ensure
the deformation properties mentioned above.
We deÔ¨Åne the conformality constraint in
terms of the metric tensor,
J‚ä§J of the Fz,
where J is the jacobian of Fz (Eq. 6):"
LEARNING SURFACE PARAMETERIZATION,0.19689119170984457,"J =
Œ¥Fz"
LEARNING SURFACE PARAMETERIZATION,0.20207253886010362,"Œ¥u
Œ¥Fz Œ¥v"
LEARNING SURFACE PARAMETERIZATION,0.20725388601036268,"
= [Du Dv]
J‚ä§J =
D‚ä§
u Du
D‚ä§
u Dv
D‚ä§
u Dv
D‚ä§
v Dv"
LEARNING SURFACE PARAMETERIZATION,0.21243523316062177,"
=
E
F
F
G 
(6)"
LEARNING SURFACE PARAMETERIZATION,0.21761658031088082,"The conformality constraint is deÔ¨Åned as J‚ä§J = Œ≤I. Here Œ≤ is a unknown local scaling function
and I is the identity matrix. For developable surfaces which can be physically Ô¨Çattened without any
stretch e.g. papers, Œ≤ doesn‚Äôt vary across the parameterization space. Therefore, we consider a Ô¨Åxed
global scale ([Œ≤g
u, Œ≤g
v]) for the conformality constraint."
LEARNING SURFACE PARAMETERIZATION,0.22279792746113988,"Unwarping by sampling Fz. To unwarp an input image, we determine the pixel at p = (x, y) in
the input image should be projected to (u, v) in the unwarped image. Here the unwarped image
refers to the texture space. The coordinates (u, v) and p are associated by Fz and œÑ: For a (u, v)
coordinate, its corresponding point in 3D is obtained by ÀÜz‚Ä≤
p = Fz(u, v). Given the camera parameter
œÑ, ÀÜz‚Ä≤
p is projected to p in the input image. Thus for each pixel in the unwarped image, we can Ô¨Ånd
its corresponding pixel in the input image which is all we need for unwarping."
LOSS FUNCTIONS,0.22797927461139897,"3.3
LOSS FUNCTIONS"
LOSS FUNCTIONS,0.23316062176165803,"We use the rendering losses on the predicted color, Cp, and predicted document mask Mp at pixel
p to train the geometry S. Here Mp ‚àà{0, 1} refers to whether the pixel p is occupied (Mp = 1)
by the shape or not (Mp = 0). We assume masks are provided as input. Additionally, we employ
appropriate regularization losses to jointly train S, Auv, Fuv and Fz."
LOSS FUNCTIONS,0.23834196891191708,"Loss for S. Following IDR (Yariv et al., 2020), for each p we apply a sphere tracing (Hart, 1996)
algorithm to Ô¨Ånd the intersection point of the ray rp(œÑ) and the surface ZŒ∏. Given the ground-truth
RGB color Cgt
p and the predicted RGB color Cp, the RGB loss is deÔ¨Åned as:"
LOSS FUNCTIONS,0.24352331606217617,"Lrgb =
1
|P| X p‚ààPin"
LOSS FUNCTIONS,0.24870466321243523,"Cgt
p ‚àíCp

1
(7)"
LOSS FUNCTIONS,0.2538860103626943,"Where P is the set of pixels in the minibatch. The pixels Pin ‚äÇP for which ray surface intersection
has been found and Mp = 1. The mask loss is deÔ¨Åned as:"
LOSS FUNCTIONS,0.25906735751295334,"Lmask =
1
Œ±|P| X"
LOSS FUNCTIONS,0.26424870466321243,"p‚ààPout
CE(M gt
p , Mp)
(8)"
LOSS FUNCTIONS,0.2694300518134715,"Here Pout = P \Pin, alpha is a tunable parameter and CE(.) is the cross-entropy loss. The value of
Mp = Mp,Œ±(Œ∏, œÑ) is a differentiable function of the learned ZŒ∏ (Yariv et al., 2020). Additionally, to
force ZŒ∏ to be a approximate signed distance function we use Eikonal Regularization (Gropp et al.,"
LOSS FUNCTIONS,0.27461139896373055,Under review as a conference paper at ICLR 2022
LOSS FUNCTIONS,0.27979274611398963,"2020):
Lek = Ez(‚à•‚àázS(z; Œ∏)‚à•‚àí1)2
(9)
where z denotes uniformly sampled points within a bounding box of the 3D domain."
LOSS FUNCTIONS,0.2849740932642487,"Loss for Fuv. Although we initialize Fuv with learned prior parameters, we constrain the predicted
2D texture coordinates during training in order to avoid non-uniform mapping of the 3D and the UV
domain which can squeeze or stretch the warped texture (example in supplementary). We employ
a Chamfer distance between the tp and uniformly sampled 2D points T ‚àà[0, 1] to ensure Fuv
approximately outputs U ‚àº[0, 1]. This regularization term is deÔ¨Åned as:
Luv = CDp‚ààPin(T , tp)
(10)
here CD(.) denotes the Chamfer distance and tp the predicted texture coordinates corresponding to
ray-surface intersection points ÀÜzp."
LOSS FUNCTIONS,0.29015544041450775,"Loss for Fz. ÀÜz‚Ä≤
p is the output of Fz. Fz is trained with weighted regression loss between ÀÜzp and ÀÜz‚Ä≤
p:"
LOSS FUNCTIONS,0.29533678756476683,"Lz =
1
|Pin| X"
LOSS FUNCTIONS,0.3005181347150259,"p‚ààPin
wp(ÀÜzp ‚àíÀÜz‚Ä≤
p)2
(11)"
LOSS FUNCTIONS,0.30569948186528495,"wp is a pre-calculated per-pixel weight based on the document mask (M) which assigns higher value
to the pixels at the boundary of the document. (More weight calculation details in Supplementary)."
LOSS FUNCTIONS,0.31088082901554404,"Additionally, to constrain Fz to be a Ô¨Åxed scale conformal mapping (Bednarik et al., 2020). We
employ three constraints on the elements of the metric tensor E, F and G deÔ¨Åned in Eq.
6."
LOSS FUNCTIONS,0.3160621761658031,"LE =
1
|Pin| X"
LOSS FUNCTIONS,0.32124352331606215,"p‚ààPin
(Ep ‚àíÀúE)2
LG =
1
|Pin| X"
LOSS FUNCTIONS,0.32642487046632124,"p‚ààPin
(Gp ‚àíÀúG)2
LF =
1
|Pin| X"
LOSS FUNCTIONS,0.3316062176165803,"p‚ààPin
(Fp)2"
LOSS FUNCTIONS,0.33678756476683935,Here ÀúE and ÀúG is the mean of E and G.
LOSS FUNCTIONS,0.34196891191709844,"Our combined loss function is deÔ¨Åned as:
L = (Lrgb + Œ≥1Lmask + Œ≥2Lek)
|
{z
}
LS"
LOSS FUNCTIONS,0.3471502590673575,"+œÅLuv + (Œ¥1Lz + Œ¥2LE + Œ¥3LG + Œ¥4LF )
|
{z
}
LT (12)"
LOSS FUNCTIONS,0.35233160621761656,"Here Œ≥, œÅ and Œ¥ denote the hyperparameters associated with the losses."
TRAINING DETAILS,0.35751295336787564,"3.4
TRAINING DETAILS
The surface MLP S(z, Œ∏) consists of 8 layers with a hidden layer dimension of 128, with a skip
connection to the middle layer (Park et al., 2019a). Following IDR (Yariv et al., 2020), S is ini-
tialized to produce an approximate SDF of a unit sphere. The rendering network Auv has 4 layers
with hidden layer dimension of 512 and uses a sine activation function (Sitzmann et al., 2020) at
each layer. Fuv and Fz share identical architecture with 8 layers with 512 dimensional hidden units
and sine activation (Sitzmann et al., 2020). Following NeRF (Mildenhall et al., 2020), we use a k
dimensional Fourier mapping (œák : R ‚ÜíR2k) to learn high frequency details in the shape, RGB
and the UV space. For S, Auv we follow the setting of (Yariv et al., 2020), and set k = 6 and k = 4
respectively. For Fuv and Fz we empirically set number of Fourier bands k = 10. We start with an
initial learning rate of 1e√ó‚àí5 and train for 150K iterations by halving the learning rate after every
50K iterations. Initially, Œ± is set to 50 and doubled during the training after every 50K iterations. We
set Œ≥1 = 100.0, Œ≥2 = 0.1 and œÅ = 0.001. Œ¥1 is set to 0.001 for the initial 30K iterations. Afterward,
Œ¥1 is multiplied by a factor 2 at every 10K iterations for a maximum of 7 times. Œ¥2, Œ¥3 and Œ¥4, are set
to zero for the initial 100K iterations. Only Lz is sufÔ¨Åcient to achieve a good texture to 3D mapping
during the shape optimization phase. Afterwards we set Œ¥2 = Œ¥3 = 0.001 and Œ¥4 = 0.01. The metric
tensor calculation is implemented using auto-differentiation."
EXPERIMENTAL RESULTS,0.3626943005181347,"4
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.36787564766839376,"First, we quantitatively compare the proposed method with state-of-the-art document unwarping
method DewarpNet (Das et al., 2019). Our quantitative and qualitative experiments are performed
on 10 synthetic scenes and 10 real scenes. Second, we apply our method to texture editing. Last, we
conduct ablation studies to demonstrate the effectiveness of our proposed loss functions."
EXPERIMENTAL RESULTS,0.37305699481865284,Under review as a conference paper at ICLR 2022
EVALUATION DATASET AND METRICS,0.37823834196891193,"4.1
EVALUATION DATASET AND METRICS"
EVALUATION DATASET AND METRICS,0.38341968911917096,"Our synthetic evaluation data consists of 10 scenes rendered using Blender following a rendering
pipeline similar to Doc3D. Each scene consists of 50 random views sampled from a 45o solid an-
gle in the upper hemisphere. The real-world evaluation data consists of 3 scenes from the dataset
of (You et al., 2017), and 9 scenes captured by us. Each scene consists of 5-20 images per scene.
We manually annotate the masks for each scene. To obtain camera poses for the real-world data,
we utilize the COLMAP (Sch¬®onberger & Frahm, 2016) multi-view reconstruction pipeline. Both
synthetic and real data include the document scan, as the unwarping ground-truth."
EVALUATION DATASET AND METRICS,0.38860103626943004,"We use image-based evaluation metrics for quantitative evaluation, including Local Distortion (LD)
and Multi-Scale Structural Similarity (MS-SSIM). These are standard metrics used for document
unwarping evaluation (Das et al., 2019; Ma et al., 2018). LD is based on dense SIFT Ô¨Çow (Liu et al.,
2011) between the unwarped and scanned images. Image similarity metric, MS-SSIM (Wang et al.,
2003) is based on local image statistics (mean and variance) of the unwarped and scanned (ground-
truth) images calculated over multiple Gaussian pyramid scales. We use the same settings as (Das
et al., 2019; Ma et al., 2018) for fair comparison."
DOCUMENT UNWARPING,0.39378238341968913,"4.2
DOCUMENT UNWARPING"
DOCUMENT UNWARPING,0.39896373056994816,"Scene
DewarpNet
(all views)
DewarpNet
(best view)
Proposed
(all views)"
DOCUMENT UNWARPING,0.40414507772020725,"MSSIM ‚Üë
LD ‚Üì
MSSIM ‚Üë
LD ‚Üì
MSSIM ‚Üë
LD ‚Üì"
DOCUMENT UNWARPING,0.40932642487046633,"Synth 1
0.42
9.54
0.68
3.29
0.74
2.59
Synth 2
0.75
5.68
0.83
2.59
0.76
4.40
Synth 3
0.73
7.80
0.85
2.94
0.78
5.44
Synth 4
0.59
6.88
0.63
2.53
0.64
2.85
Synth 5
0.48
7.11
0.64
3.13
0.61
4.55
Synth 6
0.50
6.34
0.62
2.53
0.47
3.92
Synth 7
0.52
7.99
0.76
2.64
0.74
2.55
Synth 8
0.56
10.05
0.70
3.44
0.64
5.31
Synth 9
0.49
7.48
0.73
1.87
0.78
1.56
Synth 10
0.52
8.07
0.78
2.78
0.73
3.13"
DOCUMENT UNWARPING,0.41450777202072536,"Syn. Mean
0.56
7.69
0.70
2.82
0.69
3.63"
DOCUMENT UNWARPING,0.41968911917098445,"Real 1
0.26
9.77
0.39
5.78
0.37
5.68
Real 12
0.24
12.94
0.24
10.98
0.35
8.38
Real 6
0.44
9.15
0.48
7.78
0.37
16.80"
DOCUMENT UNWARPING,0.42487046632124353,"Real Mean
0.31
10.62
0.37
8.18
0.36
10.28"
DOCUMENT UNWARPING,0.43005181347150256,"Table 1: Comparison with (Das et al., 2019) on synthetic
scenes: all views refers to the mean error metric on all scene
images, best view refers to the lowest possible error from an
image in a scene."
DOCUMENT UNWARPING,0.43523316062176165,"The
primary
application
of
our
learned forward and backward MLP
is document unwarping. The quan-
titative comparison with the state-of-
the-art model (Das et al., 2019) is
shown in Table 1 for the synthetic and
real scenes. In terms of average per-
formance of all the views (all views
col. in Table 1) we improve the LD
by ‚àº52% compared to (Das et al.,
2019). Since we use multi-view im-
ages for training, our results are more
consistent across all the views com-
pared to DewarpNet, which is also
a key reason for the signiÔ¨Åcant im-
provement. We conjecture that (Das
et al., 2019) as a single image un-
warping method should perform well
on simpler deformations and frontal
view images. However, it is not al-
ways the case. In qualitative compar-
isons in Fig. 4, DewarpNet often gen-
erates artifacts even for reasonably frontal views and simple deformations. Comparatively, our re-
sults are qualitatively superior."
DOCUMENT UNWARPING,0.44041450777202074,"We also report in a stricter evaluation scenario (best view column of Table 1) where we compare
our results with the best possible numerical results achieved by DewarpNet from a single view in
a scene. We perform better than DewarpNet in 91.2% of all views, however when the best view
can be selected our method do slightly worse in 7 scenes. This ‚Äòstricter‚Äô setting shows quantitatively
competitive results compared to DewarpNet with a oracle (practically challenging) view selector.The
choice of the best unwarped result is often subjective. For a more comprehensive comparison,
we qualitatively compare the best results of DewarpNet with our results across 6 scenes in Fig. 5.
These 6 scenes are chosen among the 7 scenes for which DewarpNet achieves a better quantitative
result than the proposed approach for at least one view. In Fig. 5 our results are clearly better
than the DewarpNet in all cases, with straighter lines and better rectiÔ¨Åed structure. The evaluation
scores do not accurately reÔ¨Çect the improvement due to the sensitivity of LD and MSSIM to subtle
perceptually unimportant global transformations, such as translation of the image by few pixels.
However, such transformations do not affect the visual quality or readability of the unwarped results.
More discussion and qualitative comparison is available in supplementary material."
DOCUMENT UNWARPING,0.44559585492227977,Under review as a conference paper at ICLR 2022
DOCUMENT UNWARPING,0.45077720207253885,"(a)
(b)
(c)
(d)
(e)"
DOCUMENT UNWARPING,0.45595854922279794,"(a)
(b)
(c)
(d)
(e)"
DOCUMENT UNWARPING,0.46113989637305697,"(a)
(b)
(c)
(a)
(b)
(c)"
DOCUMENT UNWARPING,0.46632124352331605,"Figure 4: Qualitative comparison with DewarpNet (Das et al., 2019): (a) Input image, (b) Dewarp-
Net unwarping, (c) proposed unwarping, (d) GT scanned image, (e) enlarged regions: DewarpNet
(top), and proposed (bottom). We use reasonable frontal view of the document for a fair comparison."
DOCUMENT UNWARPING,0.47150259067357514,"DewarpNet
Proposed"
DOCUMENT UNWARPING,0.47668393782383417,"ED ‚Üì
CER (std) ‚Üì
WER (std) ‚Üì
ED ‚Üì
CER (std) ‚Üì
WER (std) ‚Üì"
DOCUMENT UNWARPING,0.48186528497409326,"Mean
798.30
0.2827 (0.12)
0.4646 (0.17)
600.78
0.2122 (0.10)
0.3568 (0.11)"
DOCUMENT UNWARPING,0.48704663212435234,"Table 2: Comparison of OCR error metrics: We improve the OCR performance of Das et al. (2019)
by ‚àº25% in terms of Edit Distance (ED), Character Error Rate (CER), and Word Error Rate (WER)."
DOCUMENT UNWARPING,0.49222797927461137,"The quantitative comparison for real scenes are reported in Table 1 (bottom). We achieve better
results in terms of mean and best evaluation score than DewarpNet in 2 out of 3 scenes. We notice
that the evaluation results are a little worse for the real scenes than synthetic scenes due to the
fewer available views (5-10 compared to 50). Moreover, there are cases like Real 6, which do not
have sufÔ¨Åcient texture. Such data are a failure case of IDR since there is insufÔ¨Åcient information to
reconstruct the 3D shape. As a result of the poor 3D shape, our texture parameterization network
produces an inferior unwarping result (More details are available in Supplementary). We also report
qualitative comparisons with You et al. (2017) and Das et al. (2019) on additional real documents in
supplementary."
DOCUMENT UNWARPING,0.49740932642487046,"OCR Evaluation. We also evaluated the OCR performance on 5 real scenes across 77 images in
Table 2. We use Edit Distance (ED) (Miller et al., 2009), Character Error Rate (CER) and Word
Error Rate (WER) as our evaluation metrics. ED is deÔ¨Åned as the total number of substitutions (s),
insertions (i) and deletions (d) required to obtain the reference text, given the recognized text. The
reference text is obtained by running the OCR algorithm on the scanned ground-truth image of each
document. CER is deÔ¨Åned as: (s + i + d)/N where N is the number of characters in the reference
text. We use Tesseract 4.1.1 based LSTM OCR engine for this experiment. Our unwarped results
reduce the ED, CER and WER by ‚àº25%. This improvement proves our unwarped results are more
suitable for downstream applications tasks like OCR."
DOCUMENT UNWARPING,0.5025906735751295,Under review as a conference paper at ICLR 2022
DOCUMENT UNWARPING,0.5077720207253886,"(a)
(b)
(c)
(d)
(e)
(f)"
DOCUMENT UNWARPING,0.5129533678756477,"Figure 5: Comparison of DewarpNet (a,c,e) with the proposed unwarped result (b,d,f) for the view
that yields the best LD with DewarpNet. Proposed results are clearly better, however this improve-
ment is not captured by LD. Follow the blue dashed boxes for discrimitative regions."
DOCUMENT UNWARPING,0.5181347150259067,"Texture Editing. In addition to document unwarping, our proposed forward and backward MLP
can also be used for high quality texture editing. We show two texture editing examples in Fig. 1.
We use the backward MLP to unwarp the texture from the input image, then we edit the texture and
warp it back to image space using the learned forward MLP. (More details in Supplementary)."
DOCUMENT UNWARPING,0.5233160621761658,"Ablation Study. We ablate how loss terms Lz, LE, LF , and LG affect the unwarping results. We
train Fuv and Fz with different combinations of these loss terms and report the mean MSSIM and
LD in Table 3 (appendix). Qualitative results for one scene are shown in Fig. 6 (appendix)."
DOCUMENT UNWARPING,0.5284974093264249,"5
TRAINING TIME, GENERALIZABILITY AND FUTURE WORK"
DOCUMENT UNWARPING,0.533678756476684,"Our proposed method for a scene can be trained in approximately 18 hours for 448 √ó 448 resolution
images using a single Titan Xp GPU. The current training time per scene is very high compared to
DewarpNet‚Äôs inference time which makes it unsuitable for real time applications. However, this is a
fast growing Ô¨Åeld and there are multiple other works that are focusing on improving the speed and
generalization abilities (Garbin et al., 2021; Bergman et al., 2021) of neural rendering. Therefore,
obtaining a faster training scheme is considered as a future work."
DOCUMENT UNWARPING,0.538860103626943,"Our method can be applied to fabrics, which are very similar to papers and lead to practical appli-
cations of texture editing. However, none of the current 3D garment/fabric datasets (Patel et al.,
2020) can be easily adapted to train the ÀÜFuv prior. For more complex UV spaces (e.g., texture atlas),
learning the prior may require decomposing the shape to multiple simple UV maps. The proper way
to do this is beyond the scope of this paper, however we believe it‚Äôs an exciting future work. As
importantly, in this paper, we have introduced a number of domain speciÔ¨Åc strong constraints that
suit the rectangular paper shape. These constraints improve empirical results. More general objects
will require different constraints e.g., spherical UV domain, local scaling of the conformal map etc."
CONCLUSIONS,0.5440414507772021,"6
CONCLUSIONS"
CONCLUSIONS,0.5492227979274611,"We have introduced an end-to-end trainable architecture that can simultaneously learn texture pa-
rameterized 3D shapes from multi-view images. This is the Ô¨Årst work to learn surface parameteri-
zation of an implicit neural representation to the best of our knowledge. We have demonstrated the
applicability of our approach on multiple synthetic and real scenes for the task of document unwarp-
ing and document texture editing. We want to extend this method to learn surface parameterization
for more complex shapes such as faces or general 3D objects in future work."
CONCLUSIONS,0.5544041450777202,Under review as a conference paper at ICLR 2022
REFERENCES,0.5595854922279793,REFERENCES
REFERENCES,0.5647668393782384,"Adrien Bartoli, Yan Gerard, Francois Chadebecq, Toby Collins, and Daniel Pizarro. Shape-from-
template. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(10):2099‚Äì2118,
2015."
REFERENCES,0.5699481865284974,"David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, and Antonio
Torralba. Semantic photo manipulation with a generative image prior. ACM Transactions on
Graphics (TOG), 38(4), 2019."
REFERENCES,0.5751295336787565,"Jan Bednarik, Shaifali Parashar, Erhan Gundogdu, Mathieu Salzmann, and Pascal Fua. Shape re-
construction by learning differentiable surface representations. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, 2020."
REFERENCES,0.5803108808290155,"Alexander W. Bergman, Petr Kellnhofer, and Gordon Wetzstein. Fast training of neural lumigraph
representations using meta learning, 2021."
REFERENCES,0.5854922279792746,"Sai Bi, Nima Khademi Kalantari, and Ravi Ramamoorthi. Patch-based optimization for image-based
texture mapping. ACM Transactions on Graphics (TOG), 36(4):106‚Äì1, 2017."
REFERENCES,0.5906735751295337,"Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In Pro-
ceedings of the International Conference on Computer Vision, 2019."
REFERENCES,0.5958549222797928,"Tony Chan and Wei Zhu. Level set based shape prior segmentation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. IEEE, 2005."
REFERENCES,0.6010362694300518,"Anpei Chen, Zhang Chen, Guli Zhang, Kenny Mitchell, and Jingyi Yu. Photo-realistic facial details
synthesis from single image. In Proceedings of the International Conference on Computer Vision,
2019."
REFERENCES,0.6062176165803109,"Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao
Su. MVSNeRF: Fast generalizable radiance Ô¨Åeld reconstruction from multi-view stereo. arXiv
preprint arXiv:2103.15595, 2021."
REFERENCES,0.6113989637305699,"Zhiqin Chen and Hao Zhang. Learning implicit Ô¨Åelds for generative shape modeling. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5939‚Äì5948, 2019."
REFERENCES,0.616580310880829,"Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A
uniÔ¨Åed approach for single and multi-view 3d object reconstruction. In European conference on
computer vision, pp. 628‚Äì644. Springer, 2016."
REFERENCES,0.6217616580310881,"Sagnik Das, Gaurav Mishra, Akshay Sudharshana, and Roy Shilkrot. The Common Fold: Utilizing
the Four-Fold to Dewarp Printed Documents from a Single Image. In Proceedings of the 2017
ACM Symposium on Document Engineering, DocEng ‚Äô17, pp. 125‚Äì128, 2017. ISBN 978-1-4503-
4689-4. doi: 10.1145/3103010.3121030."
REFERENCES,0.6269430051813472,"Sagnik Das, Ke Ma, Zhixin Shu, Dimitris Samaras, and Roy Shilkrot. DewarpNet: Single-image
document unwarping with stacked 3D and 2D regression networks. In Proceedings of the Inter-
national Conference on Computer Vision, 2019."
REFERENCES,0.6321243523316062,"Sagnik Das, Kunwar Yashraj Singh, Jon Wu, Erhan Bas, Vijay Mahadevan, Rahul Bhotika, and
Dimitris Samaras. End-to-end piece-wise unwarping of document images. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp. 4268‚Äì4277, 2021."
REFERENCES,0.6373056994818653,"Jiankang Deng, Shiyang Cheng, Niannan Xue, Yuxiang Zhou, and Stefanos Zafeiriou. Uv-gan:
Adversarial facial uv map completion for pose-invariant face recognition. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2018."
REFERENCES,0.6424870466321243,"Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf:
High-Ô¨Ådelity neural rendering at 200fps, 2021."
REFERENCES,0.6476683937823834,"Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regu-
larization for learning shapes. arXiv preprint arXiv:2002.10099, 2020."
REFERENCES,0.6528497409326425,Under review as a conference paper at ICLR 2022
REFERENCES,0.6580310880829016,"Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, and Mathieu Aubry.
A
papier-mÀÜach¬¥e approach to learning 3d surface generation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2018."
REFERENCES,0.6632124352331606,"Steven Haker, Sigurd Angenent, Allen Tannenbaum, Ron Kikinis, Guillermo Sapiro, and Michael
Halle. Conformal surface parameterization for texture mapping. IEEE Transactions on Visualiza-
tion and Computer Graphics, 6(2):181‚Äì189, 2000."
REFERENCES,0.6683937823834197,"John C Hart. Sphere tracing: A geometric method for the antialiased ray tracing of implicit surfaces.
The Visual Computer, 12(10):527‚Äì545, 1996."
REFERENCES,0.6735751295336787,"Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Bros-
tow. Deep blending for free-viewpoint image-based rendering. ACM Transactions on Graphics
(TOG), 37(6):1‚Äì15, 2018."
REFERENCES,0.6787564766839378,"Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3d mesh renderer. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, 2018."
REFERENCES,0.6839378238341969,"Taeho Kil, Wonkyo Seo, Hyung Il Koo, and Nam Ik Cho. Robust Document Image Dewarping
Method Using Text-Lines and Line Segments. In Proceedings of the International Conference on
Document Analysis and Recognition, pp. 865‚Äì870. IEEE, 2017."
REFERENCES,0.689119170984456,"Hyung Il Koo, Jinho Kim, and Nam Ik Cho. Composition of a dewarped and enhanced document
image from two view images. IEEE Transactions on Image Processing, 18(7):1551‚Äì1562, 2009."
REFERENCES,0.694300518134715,"Tzu-Mao Li, Miika Aittala, Fr¬¥edo Durand, and Jaakko Lehtinen. Differentiable monte carlo ray
tracing through edge sampling. ACM Transactions on Graphics (TOG), 37(6):1‚Äì11, 2018."
REFERENCES,0.6994818652849741,"Xiaoyu Li, Bo Zhang, Jing Liao, and Pedro V. Sander. Document RectiÔ¨Åcation and Illumination
Correction using a Patch-based CNN. ACM Transactions on Graphics (TOG), 2019."
REFERENCES,0.7046632124352331,"Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene Ô¨Çow Ô¨Åelds for space-time
view synthesis of dynamic scenes. arXiv preprint arXiv:2011.13084, 2020."
REFERENCES,0.7098445595854922,"Jian Liang, Daniel DeMenthon, and David Doermann. Geometric rectiÔ¨Åcation of camera-captured
document images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(4):591‚Äì
605, 2008."
REFERENCES,0.7150259067357513,"Ce Liu, Jenny Yuen, and Antonio Torralba. Sift Ô¨Çow: Dense correspondence across scenes and its
applications. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(5):978‚Äì994,
2011."
REFERENCES,0.7202072538860104,"Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft rasterizer: A differentiable renderer for
image-based 3d reasoning. In Proceedings of the International Conference on Computer Vision,
2019a."
REFERENCES,0.7253886010362695,"Shichen Liu, Shunsuke Saito, Weikai Chen, and Hao Li. Learning to infer implicit surfaces without
3d supervision. arXiv preprint arXiv:1911.00767, 2019b."
REFERENCES,0.7305699481865285,"Ke Ma, Zhixin Shu, Xue Bai, Jue Wang, and Dimitris Samaras. DocUNet: Document Image Un-
warping via A Stacked U-Net. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2018."
REFERENCES,0.7357512953367875,"Amir Markovitz, Inbal Lavi, Or Perel, Shai Mazor, and Roee Litman. Can you read me now?
Content aware rectiÔ¨Åcation using angle supervision. In Proceedings of the European Conference
on Computer Vision. Springer, 2020."
REFERENCES,0.7409326424870466,"Abhimitra Meka, Christian Haene, Rohit Pandey, Michael Zollh¬®ofer, Sean Fanello, Graham Fyffe,
Adarsh Kowdle, Xueming Yu, Jay Busch, Jason Dourgarian, et al. Deep reÔ¨Çectance Ô¨Åelds: High-
quality facial reÔ¨Çectance Ô¨Åeld inference from color gradient illumination. ACM Transactions on
Graphics (TOG), 38(4):1‚Äì12, 2019."
REFERENCES,0.7461139896373057,"Gaofeng Meng, Zuming Huang, Yonghong Song, Shiming Xiang, and Chunhong Pan. Extraction
of virtual baselines from distorted document images using curvilinear projection. In Proceedings
of the International Conference on Computer Vision, 2015."
REFERENCES,0.7512953367875648,Under review as a conference paper at ICLR 2022
REFERENCES,0.7564766839378239,"Gaofeng Meng, Yuanqi Su, Ying Wu, Shiming Xiang, and Chunhong Pan. Exploiting Vector Fields
for Geometric RectiÔ¨Åcation of Distorted Document Images.
In Proceedings of the European
Conference on Computer Vision, 2018."
REFERENCES,0.7616580310880829,"Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Oc-
cupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 4460‚Äì4470, 2019."
REFERENCES,0.7668393782383419,"Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and
Ren Ng. NeRF: Representing scenes as neural radiance Ô¨Åelds for view synthesis. In Proceedings
of the European Conference on Computer Vision, 2020."
REFERENCES,0.772020725388601,"Frederic P. Miller, Agnes F. Vandome, and John McBrewster. Levenshtein Distance: Information
Theory, Computer Science, String (Computer Science), String Metric, Damerau?Levenshtein Dis-
tance, Spell Checker, Hamming Distance. Alpha Press, 2009. ISBN 6130216904."
REFERENCES,0.7772020725388601,"Aymen Mir, Thiemo Alldieck, and Gerard Pons-Moll. Learning to transfer texture from clothing
images to 3d humans. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2020."
REFERENCES,0.7823834196891192,"Luca Morreale, Noam Aigerman, Vladimir Kim, and Niloy J. Mitra. Neural surface maps. 2021."
REFERENCES,0.7875647668393783,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumet-
ric rendering: Learning implicit 3d representations without 3d supervision. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3504‚Äì3515, 2020."
REFERENCES,0.7927461139896373,"Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.
Deepsdf: Learning continuous signed distance functions for shape representation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 165‚Äì174,
2019a."
REFERENCES,0.7979274611398963,"Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with
spatially-adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2019b."
REFERENCES,0.8031088082901554,"Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-Moll. Tailornet: Predicting clothing in 3d
as a function of human pose, shape and garment style. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR). IEEE, jun 2020."
REFERENCES,0.8082901554404145,"Michela Petrini, Gianfranco Pradisi, and Alberto Zaffaroni. Guide To Mathematical Methods For
Physicists, A: Advanced Topics And Applications. World ScientiÔ¨Åc, 2018."
REFERENCES,0.8134715025906736,"Albert Pumarola, Antonio Agudo, Lorenzo Porzi, Alberto Sanfeliu, Vincent Lepetit, and Francesc
Moreno-Noguer. Geometry-Aware Network for Non-Rigid Shape Prediction from a Single View.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018."
REFERENCES,0.8186528497409327,"Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural
radiance Ô¨Åelds for dynamic scenes. arXiv preprint arXiv:2011.13961, 2020."
REFERENCES,0.8238341968911918,"Johannes Lutz Sch¬®onberger and Jan-Michael Frahm. Structure-from-motion revisited. In Confer-
ence on Computer Vision and Pattern Recognition (CVPR), 2016."
REFERENCES,0.8290155440414507,"Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance Ô¨Åelds
for 3d-aware image synthesis. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.8341968911917098,"Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie√üner, Gordon Wetzstein, and Michael
Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2019a."
REFERENCES,0.8393782383419689,"Vincent Sitzmann, Michael Zollh¬®ofer, and Gordon Wetzstein.
Scene representation networks:
Continuous 3d-structure-aware neural scene representations. arXiv preprint arXiv:1906.01618,
2019b."
REFERENCES,0.844559585492228,Under review as a conference paper at ICLR 2022
REFERENCES,0.8497409326424871,"Vincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wet-
zstein. Implicit neural representations with periodic activation functions. In arXiv, 2020."
REFERENCES,0.8549222797927462,"Chengzhou Tang and Ping Tan.
Ba-net: Dense bundle adjustment network.
arXiv preprint
arXiv:1806.04807, 2018."
REFERENCES,0.8601036269430051,"Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli,
Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nie√üner, et al. State of the art on
neural rendering. In Computer Graphics Forum, volume 39, pp. 701‚Äì727. Wiley Online Library,
2020."
REFERENCES,0.8652849740932642,"Yuandong Tian and Srinivasa G Narasimhan. RectiÔ¨Åcation and 3D reconstruction of curved docu-
ment images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2011."
REFERENCES,0.8704663212435233,"Yochay Tzur and Ayellet Tal. FlexiStickers: Photogrammetric texture mapping using casual images.
In Proceedings of the ACM SIGGRAPH Conference on Computer Graphics, 2009."
REFERENCES,0.8756476683937824,"Adrian Ulges, Christoph H. Lampert, and Thomas Breuel. Document Capture Using Stereo Vision.
In Proceedings of the 2004 ACM Symposium on Document Engineering, DocEng ‚Äô04, pp. 198‚Äì
200, 2004. ISBN 1-58113-938-1. doi: 10.1145/1030397.1030434."
REFERENCES,0.8808290155440415,"Toshikazu Wada, Hiroyuki Ukida, and Takashi Matsuyama. Shape from shading with interreÔ¨Çec-
tions under a proximal light source: Distortion-free copying of an unfolded book. International
Journal of Computer Vision, 24(2):125‚Äì135, 1997."
REFERENCES,0.8860103626943006,"Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh:
Generating 3d mesh models from single rgb images. In Proceedings of the European Conference
on Computer Vision (ECCV), pp. 52‚Äì67, 2018."
REFERENCES,0.8911917098445595,"Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality
assessment. In The Thirty-Seventh Asilomar Conference on Signals, Systems and Computers,
2003."
REFERENCES,0.8963730569948186,"Shih-En Wei, Jason Saragih, Tomas Simon, Adam W Harley, Stephen Lombardi, Michal Perdoch,
Alexander Hypes, Dawei Wang, Hernan Badino, and Yaser Sheikh. Vr facial animation via mul-
tiview image translation. ACM Transactions on Graphics (TOG), 38(4):1‚Äì16, 2019."
REFERENCES,0.9015544041450777,"Fanbo Xiang, Zexiang Xu, MiloÀás HaÀásan, Yannick Hold-Geoffroy, Kalyan Sunkavalli, and Hao
Su.
NeuTex:
Neural texture mapping for volumetric neural rendering.
arXiv preprint
arXiv:2103.00762, 2021."
REFERENCES,0.9067357512953368,"Zexiang Xu, Kalyan Sunkavalli, Sunil Hadap, and Ravi Ramamoorthi. Deep image-based relighting
from optimal sparse samples. ACM Transactions on Graphics (TOG), 37(4):1‚Äì13, 2018."
REFERENCES,0.9119170984455959,"Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lip-
man. Multiview neural surface reconstruction by disentangling geometry and appearance. Ad-
vances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.917098445595855,"Shaodi You, Yasuyuki Matsushita, Sudipta Sinha, Yusuke Bou, and Katsushi Ikeuchi. Multiview
RectiÔ¨Åcation of Folded Documents. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2017."
REFERENCES,0.9222797927461139,"Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance Ô¨Åelds
from one or few images. arXiv preprint arXiv:2012.02190, 2020."
REFERENCES,0.927461139896373,"Fang Zhao, Shengcai Liao, Kaihao Zhang, and Ling Shao. Human parsing based texture transfer
from single image to 3D human via cross-view consistency. In Advances in Neural Information
Processing Systems, 2020."
REFERENCES,0.9326424870466321,Under review as a conference paper at ICLR 2022
REFERENCES,0.9378238341968912,"Mean
b
b+w
b+c
b+w+c"
REFERENCES,0.9430051813471503,"LD ‚Üì
11.2
10.50
6.24
3.63
MSSIM ‚Üë
0.4632
0.4622
0.5556
0.6888"
REFERENCES,0.9481865284974094,"Table 3: Weighted Lz, and conformality effects: b is for the model trained without conformality
constraints and with wp = 1; w is for weighted Lz and c is for the use of conformality constraints."
ETHICS STATEMENT,0.9533678756476683,"7
ETHICS STATEMENT"
ETHICS STATEMENT,0.9585492227979274,"Texture editing application of our proposed approach can have both positive and negative societal
impact. On the positive side, real document images can be gracefully redacted to protect sensitive
information. On the contrary, it can be potentially used for editing real documents and change the
content to commit fraud and spread misinformation."
REPRODUCIBILITY STATEMENT,0.9637305699481865,"8
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.9689119170984456,"We believe our results are reproducible by following the training details in Section 3.4 of main
submission and Section 2, 3, 4, 5 from the supplementary material."
REPRODUCIBILITY STATEMENT,0.9740932642487047,"A
APPENDIX"
REPRODUCIBILITY STATEMENT,0.9792746113989638,"A.1
ABLATION STUDY"
REPRODUCIBILITY STATEMENT,0.9844559585492227,"We ablate how loss terms Lz, LE, LF , and LG affect the unwarping results. We train Fuv and Fz
with different combinations of these loss terms and report the mean MSSIM and LD in Table 3.
Qualitative results for one scene are shown in Fig. 6 (appendix). In the basic version (listed as b), no
conformality constraints (LE, LF and LG) are used and wp = 1 in Lz. The b+w version introduces
a weighting function that assigns a higher value to wp if a pixel is closer to the document boundary.
Introducing this loss improves the boundary; notice the white margin at the top and bottom in the
second column of Fig. 6. Introducing the conformality constraints alleviates the unusual stretch in
the texture and improves smoothness (Fig. 6, col. 3). Using both improves the boundary and the
texture smoothness (Fig. 6, col. 4) and achieves the best result."
REPRODUCIBILITY STATEMENT,0.9896373056994818,"b (11.01)
b+w (9.83)
b+w+c (2.71)
b+c (6.59)
GT"
REPRODUCIBILITY STATEMENT,0.9948186528497409,"Figure 6: Illustration of weighted Lz, and conformality effects: b is for the model trained without
conformality constraints and with wp = 1; w is for weighted Lz and c is for the use of conformality
constraints. GT is the ground-truth scan. The number in parenthesis denote the respective LD values."
