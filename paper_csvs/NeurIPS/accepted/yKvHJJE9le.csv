Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001876172607879925,"Ensuring safety is a key aspect in sequential decision making problems, such
as robotics or process control. The complexity of the underlying systems often
makes finding the optimal decision challenging, especially when the safety-critical
system is time-varying. Overcoming the problem of optimizing an unknown
time-varying reward subject to unknown time-varying safety constraints, we pro-
pose TVSAFEOPT, a new algorithm built on Bayesian optimization with a spatio-
temporal kernel. The algorithm is capable of safely tracking a time-varying safe
region without the need for explicit change detection. Optimality guarantees are
also provided for the algorithm when the optimization problem becomes stationary.
We show that TVSAFEOPT compares favorably against SAFEOPT on synthetic
data, both regarding safety and optimality. Evaluation on a realistic case study
with gas compressors confirms that TVSAFEOPT ensures safety when solving
time-varying optimization problems with unknown reward and safety functions."
INTRODUCTION,0.00375234521575985,"1
Introduction"
INTRODUCTION,0.005628517823639775,"We seek to interactively optimize an unknown time-varying reward function f : X √ó T ‚ÜíR, where
X is a finite set of decisions, and T := {0, 1, 2, . . . , T}, T ‚ààN+ denotes the discretized time
domain. We assume that the optimization problem is safety-critical, that is, there are constraints that
evaluated decisions must satisfy with high probability. Similar to the reward, the constraints are also
unknown and potentially time-varying, encoded through ci : X √ó T ‚ÜíR, i ‚ààIc := {1, 2, . . . , m},
where m ‚ààN+ denotes the number of safety constraints. The optimization problem at time t is"
INTRODUCTION,0.0075046904315197,"max
x‚ààX f(x, t)"
INTRODUCTION,0.009380863039399626,"subject to ci(x, t) ‚â•0, i ‚ààIc
(1)"
INTRODUCTION,0.01125703564727955,"Both the reward function and the safety constraints are assumed to be unknown but can be evaluated.
This is a plausible setting, for example, for UAV that need to perform rescue missions in dangerous
and poorly lit environments."
INTRODUCTION,0.013133208255159476,"‚àí2
‚àí1
0
1
2
x ‚àí2 ‚àí1 0 1 2 y (a)"
INTRODUCTION,0.0150093808630394,safe set (t=30)
INTRODUCTION,0.016885553470919325,"‚àí2
‚àí1
0
1
2
x ‚àí2 ‚àí1 0 1 2 y (b)"
INTRODUCTION,0.01876172607879925,safe set (t=100)
INTRODUCTION,0.020637898686679174,"‚àí2
‚àí1
0
1
2
x ‚àí2 ‚àí1 0 1 2 y (c)"
INTRODUCTION,0.0225140712945591,safe set (t=170)
INTRODUCTION,0.024390243902439025,"initial safe seeds
safe points
maximizers
current maximizer
true maximizers
current true maximizers"
INTRODUCTION,0.02626641651031895,"‚àí2
‚àí1
0
1
2
x ‚àí2 ‚àí1 0 1 2 y (d)"
INTRODUCTION,0.028142589118198873,"‚àí2
‚àí1
0
1
2
x ‚àí2 ‚àí1 0 1 2 y (e)"
INTRODUCTION,0.0300187617260788,"‚àí2
‚àí1
0
1
2
x ‚àí2 ‚àí1 0 1 2 y (f)"
INTRODUCTION,0.03189493433395872,"initial safe seeds
safe points
maximizers
current maximizer
true maximizers
current true maximizers"
INTRODUCTION,0.03377110694183865,"‚àí2
‚àí1
0
1
2
x ‚àí2 ‚àí1 0 1 2 y (g)"
INTRODUCTION,0.03564727954971857,"‚àí2
‚àí1
0
1
2
x ‚àí2 ‚àí1 0 1 2 y (h)"
INTRODUCTION,0.0375234521575985,"‚àí2
‚àí1
0
1
2
x ‚àí2 ‚àí1 0 1 2 y (i)"
INTRODUCTION,0.039399624765478425,"initial safe seeds
safe points
maximizers
current maximizer
true maximizers
current true maximizers"
INTRODUCTION,0.04127579737335835,"Figure 1: Comparison of safe sets computed by TVSAFEOPT (top row), ETSAFEOPT (middle row),
and SAFEOPT (bottom row) at t = 30, t = 100, and t = 170. Because TVSAFEOPT takes the
possible changes in time into consideration, the safe sets computed by TVSAFEOPT are contained in
the ground truth safe regions while those computed by ETSAFEOPT and SAFEOPT have multiple
violations. The reason for the violations in ETSAFEOPT is that the algorithm is unable to detect small
changes in the constraints, confirming that the performance of ETSAFEOPT depends on the event
detection algorithm."
RELATED WORK,0.043151969981238276,"1.1
Related Work"
RELATED WORK,0.0450281425891182,"Bayesian Optimization (BO) is a well-established approach for interactively optimizing unknown
reward functions. Various BO based approaches have been proposed to solve a wide range of
problems in robotics [1, 2], combinatorial optimization [3], sensor networks [4], and automatic
machine learning [5, 6]. However, Safe Bayesian Optimization in the time-varying setting is still
under-explored."
RELATED WORK,0.04690431519699812,"Safe Bayesian Optimization To address safety requirements in safety-critical applications, Safe
Bayesian Optimization (SBO) [7] has been proposed to avoid unsafe decisions with high probability
by interactively optimizing a reward function under safety constraints. SAFEOPT [7], one of
the first SBO algorithms, expands an initial safe set iteratively based on new evaluations and an"
RELATED WORK,0.04878048780487805,Table 1: Overview of safe learning methods based on BO for time-varying problems.
RELATED WORK,0.05065666041275797,"Handling Changes
Safety Guarantee
Optimality Guarantee
Safe Seed
in Time
A-GOOSE [21, 17]
Spatio-temporal kernel
‚úì
‚úó
For all t
C-SAFEOPT [8]
Spatio-temporal kernel
‚úì
‚úó
For all t
ETSAFEOPT [22]
Event detection
‚úó
‚úó
For all t
TVSAFEOPT (ours)
Spatio-temporal kernel
‚úì
‚úì
For initial t"
RELATED WORK,0.0525328330206379,"updated Gaussian Process (GP) model of safety functions. It calculates two subsets, maximizers
and expanders, from the current safe set and selects the most uncertain decision within their union
to balance maximizing the reward function and expanding the safe set. Subsequent algorithms extend
SAFEOPT to handle multiple constraints [8], decouple safe set expansion from optimization [9], and
expand the safe set in a goal-oriented manner [10]. These methods also explore disconnected safe
regions [11, 12] and enhance information-theoretic efficiency [13, 14]. They have been applied to
controller tuning for a ball-screw drive [15] and quadrupeds [16], and adaptive control on a rotational
motion system [17]. However, SBO typically does not take into account changes with time."
RELATED WORK,0.054409005628517824,"Contextual Bayesian Optimization Contextual Bayesian Optimization (CBO) has been introduced
to address the influence of external environmental factors on reward and safety functions. Krause
and Ong [18] extend the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm [19] by in-
corporating contextual variables into unconstrained BO, demonstrating sub-linear regret analogous to
GP-UCB. An advancement of this framework is proposed in [20], with the Safe Contextual GP-UCB
optimizing the contextual upper confidence bound within a safe set to manage room temperature
via a PID controller. Berkenkamp et al. [8] present a contextual adaptation of SAFEOPT, discussing
its safety and optimality guarantees by framing contextual SBO as distinct SBO sub-problems.
Additionally, K√∂nig et al. [21] extends GOOSE [10] to the contextual domain for model-free adaptive
control scenarios. Similarly to SBO, CBO does not explicitly consider time-varying problems."
RELATED WORK,0.05628517823639775,"Time-Varying Bayesian Optimization Time-Varying Bayesian Optimization (TVBO) addresses
problems where the objective is time-dependent, modeled with a temporal kernel [23]. Methods in
this setting include periodical resetting [23], change detection [24, 25], sliding-window approaches
using recent data [26], and discounting via exponentially decaying past observations [27]. However,
these techniques have been developed for unconstrained BO and are unsuitable for safety-critical
applications."
RELATED WORK,0.058161350844277676,"Time-Varying Safe Bayesian Optimization In the safety-critical time-varying setting, contextual
lower confidence bounds can be optimized within the safe set [20], but it does not guarantee
optimality theoretically. An event triggering mechanism is introduced to SBO to restart exploration
from a backup policy [22], but it may not trigger reliably during changes, posing a safety risk.
Extensions to SBO with contextual variables provide theoretical safety and optimality analyses
[8, 16], treating contextual SBO as separate sub-problems for each contextual value, and assuming
an initial safe set for each. However, ensuring optimality requires each contextual value to appear
frequently, which is impractical in time-varying scenarios."
METHODOLOGY AND CONTRIBUTIONS,0.0600375234521576,"1.2
Methodology and Contributions"
METHODOLOGY AND CONTRIBUTIONS,0.06191369606003752,"Methodology We propose the TVSAFEOPT algorithm to optimize an unknown time-varying reward
subject to unknown time-varying safety constraints. The algorithm focuses on Time-Varying Safe
Bayesian Optimization (TVSBO). TVSAFEOPT utilizes a spatio-temporal kernel and time Lipschitz
constants as prior knowledge about how the problem depends on time. The temporal part of the kernel
encodes the continuity of the functions with time while the Lipschitz constants explicitly provide upper
bounds on how fast the functions may change. Instead of considering safe sets at previous iteration as
safe at the current iteration, which might lead to unsafe decisions, TVSAFEOPT robustly subtracts the
safety margin when updating the safe sets (Figure 1). In this way, the algorithm is capable of adapting
in real time and guarantees safety even when exploring the safe region of non-stationary problems."
METHODOLOGY AND CONTRIBUTIONS,0.06378986866791744,"Contributions Our contributions are threefold: a) We propose the TVSAFEOPT algorithm based
on Gaussian processes with spatio-temporal kernels; b) We provide formal safety guarantees for
TVSAFEOPT in the most general time-varying setting and optimality guarantees for TVSAFEOPT for"
METHODOLOGY AND CONTRIBUTIONS,0.06566604127579738,"locally stationary optimization problems; c) We show TVSAFEOPT performs well in the most general
time-varying setting both on synthetic data and on a realistic case study on gas compressors. In Table 1,
we compare Adaptive GOOSE, Contextual SAFEOPT, ETSAFEOPT, and TVSAFEOPT in terms of
how they handle changes in time, safety guarantees, optimality guarantees, and required safe seeds."
EXPECTED SOCIETAL IMPACT,0.0675422138836773,"1.2.1
Expected societal impact"
EXPECTED SOCIETAL IMPACT,0.06941838649155722,"The TVSAFEOPT algorithm proposed in this paper extends the state of the art in Time-Varying Safe
Bayesian Optimization by enabling solving optimization problems with time-varying reward and
constraints without pre-defining the time changes that can be compensated.Thus, the algorithm can
be used at the design stage of operating strategies for safety-critical systems, such as medical dosage
design [28] and controller design in robotics [17], or during online operation of chemical plants [29]
or autonomous racing [30]."
TVSAFEOPT ALGORITHM,0.07129455909943715,"2
TVSAFEOPT Algorithm"
TVSAFEOPT ALGORITHM,0.07317073170731707,"The TVSAFEOPT algorithm builds upon SAFEOPT [7], to handle time-varying reward function and
safety functions. The key new feature of TVSAFEOPT is its capability of safely transferring the current
safe set to the next time step. TVSAFEOPT achieves this with the help of the spatio-temporal kernel
as well as the sequence of time Lipschitz constants. The approach is summarized in Algorithm 1."
ASSUMPTIONS,0.075046904315197,"2.1
Assumptions"
ASSUMPTIONS,0.07692307692307693,"Following [8], we incorporate the reward and safety functions into an auxiliary function h : X √ó T √ó
I ‚ÜíR, where I := {0} ‚à™Ic,"
ASSUMPTIONS,0.07879924953095685,"h(x, t, i) :=
f(x, t)
, if i = 0
ci(x, t)
, if i ‚ààIc
(2)"
ASSUMPTIONS,0.08067542213883677,"We model the auxiliary function using a prior Gaussian Process (GP) with zero mean and spatio-
temporal kernel Œ∫ : (X √óT √óI)√ó(X √óT √óI) ‚ÜíR, [31]. We require h to be Lipschitz continuous
with respect to both x and t, and to have bounded norm in the Reproducing Kernel Hilbert Space
(RKHS) [32] associated with the kernel Œ∫ as formalized in the following."
ASSUMPTIONS,0.0825515947467167,"Assumption
2.1.
The
spatio-temporal
kernel
is
positive
definite,
and
satisfies
Œ∫ ((x, t, i), (x, t, i))
‚â§
1, for all x ‚ààX, t ‚ààT , i ‚ààI. The function h(x, t, i) has bounded
norm in the RKHS associated with kernel Œ∫. The function h(x, t, i) is Lx-Lipschitz continuous
with respect to x in the domain X with respect to some metric d : X √ó X ‚ÜíR‚â•0 for all t ‚ààN,
i ‚ààI. There exists a sequence {L(t)}t‚ààN,t<T , such that, for all x ‚ààX, i ‚ààI, t ‚ààN, t < T,
|h(x, t + 1, i) ‚àíh(x, t, i)| ‚â§L(t)."
ASSUMPTIONS,0.08442776735459662,"At each algorithm iteration k, we make a decision xk, which we then apply to the system and get
noisy measurements yi
k of the reward function and safety functions during the iteration. We use the
index k to refer to the algorithm iteration. Even though k and t might differ in principle, in practice
we run one algorithm iteration k for each time step t."
ASSUMPTIONS,0.08630393996247655,"Assumption 2.2. Observations yi
k = h(xk, t, i) + Œµi
k, ‚àÄi ‚ààI, t ‚ààN are perturbed by i.i.d. zero
mean and œÉ-sub-Gaussian noise."
ASSUMPTIONS,0.08818011257035648,"Based on the measurements, we compute the posterior GP and make the decision for the next time
step. To start the exploration, an initial set of safe decisions is assumed to be available to the algorithm.
To ensure that the safe set remains non-empty after the first iteration, it is necessary that the initial
safety function values at every decision within the initial safe set are positive."
ASSUMPTIONS,0.0900562851782364,"Assumption 2.3. An initial set S0 ‚äÜX of safe decisions is known and for all decisions x ‚ààS0, we
have ci(x, 0) > 0, ‚àÄi ‚ààIc."
ASSUMPTIONS,0.09193245778611632,"Similar assumptions have also been made for the standard SAFEOPT algorithm [7] and are necessary
to ensure feasibility of the exploration steps and to be able to identify new safe decision."
SAFETY UPDATES,0.09380863039399624,"2.2
Safety Updates"
SAFETY UPDATES,0.09568480300187618,"To ensure safety, based on Assumption 2.1 and 2.2, we extend the definition of the confidence
intervals from [7] so that, with high probability, they contain f and ci using the posterior GP estimate
given the data sampled so far. The confidence intervals for h(x, t, i) given training samples until
iteration k ‚â•1 are defined for all x ‚ààX and for all i ‚ààI as"
SAFETY UPDATES,0.0975609756097561,"Qk(x, i) :=
h
¬µk‚àí1(x, i) ¬±
p"
SAFETY UPDATES,0.09943714821763602,"Œ≤kœÉk‚àí1(x, i)
i
,
(3)"
SAFETY UPDATES,0.10131332082551595,"where Œ≤k is a scalar that determines the desired confidence interval, ¬µk‚àí1(x, i) and œÉk‚àí1(x, i) are
the posterior mean and standard deviation of h(x, t, i) inferred with Dk, training samples till iteration
k [31]. The probability of the true function value h lying within this interval depends on the choice
of Œ≤k [8]. We provide more details for this choice in Section 2.4."
SAFETY UPDATES,0.10318949343339587,"We now construct a tighter confidence interval for h(x, t, i) by using the sequence {QœÑ(x, i)}œÑ‚â§k
instead of Qk(x, i) alone. To this end, we recursively define for all x ‚ààX and for all i ‚ààI the
intersection"
SAFETY UPDATES,0.1050656660412758,"Ck(x, i) := (Ck‚àí1(x, i) ‚äï[‚àíL(t ‚àí1), L(t ‚àí1)]) ‚à©Qk(x, i),
(4)"
SAFETY UPDATES,0.10694183864915573,"where ‚äïdenotes the Minkowski sum, C0(x, i) is [L(0), ‚àû) for all x ‚ààS0, i ‚ààIc and R otherwise.
We use the lower bound lk(x, i) := min Ck(x, i) and the upper bound uk(x, i) := max Ck(x, i), to
define the width of Ck(x, i)
wk(x, i) := uk(x, i) ‚àílk(x, i)
(5)"
SAFETY UPDATES,0.10881801125703565,further used to update the safe set as well as pick the next decision to explore.
SAFETY UPDATES,0.11069418386491557,"Based on the updated posterior and Lipschitz constants, we can update the safe set Sk with the lower
bounds lk and the previous safe set Sk‚àí1 as"
SAFETY UPDATES,0.1125703564727955,"Sk = ‚à©i‚ààIc ‚à™x‚ààSk‚àí1 {x‚Ä≤ ‚ààX | lk(x, i) ‚àíLxd(x, x‚Ä≤) ‚àíL(t) ‚â•0}.
(6)"
SAFETY UPDATES,0.11444652908067542,"The set Sk contains decisions that with high probability fulfill the safety constraints given the GP
confidence intervals and the Lipschitz constants. In contrast to SAFEOPT, the safe set of TVSAFEOPT
is allowed to shrink to adapt to the potential change of the safe region given the time-varying setting.
However, the safe set might even become empty after the update. This is either because the safe
region indeed becomes empty or because the updated safe set conservatively excludes all decisions
with a lower bound of some safety function below L to guarantee safety. In all these cases, if the
updated safe set is empty, we terminate the algorithm."
SAFE EXPLORATION AND EXPLOITATION,0.11632270168855535,"2.3
Safe Exploration and Exploitation"
SAFE EXPLORATION AND EXPLOITATION,0.11819887429643527,"With the safe set updated, the next challenge is to trade off between exploitation and expansion of the
safe region. As in the standard SAFEOPT, the potential maximizers are those decisions, for which the
upper confidence bound of the reward function is higher than the largest lower confidence bound"
SAFE EXPLORATION AND EXPLOITATION,0.1200750469043152,"Mk =

x ‚ààSk | uk(x, 0) ‚â•max
x‚Ä≤‚ààSk lk(x‚Ä≤, 0)

.
(7)"
SAFE EXPLORATION AND EXPLOITATION,0.12195121951219512,"To identify the potential expanders, Gk, containing all decisions that could potentially expand the safe
set, we first quantify the potential enlargement of the current safe set after sampling a new decision x.
To do so, we define the function"
SAFE EXPLORATION AND EXPLOITATION,0.12382739212007504,"ek(x) := |{x‚Ä≤ ‚ààX\Sk | ‚àÉi ‚ààIc : uk(x, i) ‚àíLxd(x, x‚Ä≤) ‚àíL(t) ‚â•0}|,
(8)"
SAFE EXPLORATION AND EXPLOITATION,0.12570356472795496,"where | ¬∑ | refers to the cardinality of a set, and then update"
SAFE EXPLORATION AND EXPLOITATION,0.1275797373358349,"Gk = {x ‚ààSk | ek(x) > 0} .
(9)"
SAFE EXPLORATION AND EXPLOITATION,0.1294559099437148,"At iteration k, TVSAFEOPT selects a decision xk within the union of potential maximizers (7) and
expanders (9)"
SAFE EXPLORATION AND EXPLOITATION,0.13133208255159476,"xk =
arg max
x‚ààGk‚à™Mk,i‚ààI
wk(x, i),
(10)"
SAFE EXPLORATION AND EXPLOITATION,0.13320825515947468,Algorithm 1 TVSAFEOPT
SAFE EXPLORATION AND EXPLOITATION,0.1350844277673546,"1: Input: Sample set X
GP priors for f, ci
Lipschitz constants Lx and {L(t)}t‚ààN,t<T
Safe set seed S0
2: C0(x, i) ‚Üê[L(0), ‚àû), for all x ‚ààS0, i ‚ààIc
3: C0(x, i) ‚ÜêR, for all x ‚ààX\S0, i ‚ààIc
4: C0(x, 0) ‚ÜêR
5: Query a point x0 ‚ààS0, yi
0 ‚Üêh(x0, 0, i) + Œµi
0, i ‚ààI
6: D0 = {(x0, y0)}
7: for k = 1, 2, ¬∑ ¬∑ ¬∑ , T do
8:
Calculate Qk(x, i) as in (3), ‚àÄx ‚ààX, ‚àÄi ‚ààI
9:
Ck(x, i) ‚Üê(Ck‚àí1(x, i) ‚äï[‚àíL(t ‚àí1), L(t ‚àí1)]) ‚à©Qk(x, i)
10:
Sk ‚Üê‚à©i‚ààIc ‚à™x‚ààSt‚àí1 {x‚Ä≤ ‚ààX | lk(x, i) ‚àíLxd(x, x‚Ä≤) ‚àíL(t) ‚â•0}
11:
if Sk = ‚àÖthen
12:
break
13:
end if
14:
Mk ‚Üê{x ‚ààSk | uk(x, 0) ‚â•maxx‚Ä≤‚ààSk lk(x‚Ä≤, 0)}
15:
Gk ‚Üê{x ‚ààSk | ek(x) > 0} with ek(x) from (8)
16:
xk ‚Üêarg maxx‚ààGk‚à™Mk,i‚ààI wk(x, i)
17:
yi
k ‚Üêh(xk, t, i) + Œµi
k, i ‚ààI
18:
Dk = Dk‚àí1 ‚à™{(xk, yk)}
19: end for"
SAFE EXPLORATION AND EXPLOITATION,0.13696060037523453,"with wk from (5). The objective of the greedy selection process in (10) is to take the most uncertain
decision among the expanders Gk and the maximizers Mk. The decision xk is then applied to the
system and after making observations of the reward and safety functions, yk := (y0
k, y1
k, . . . , ym
k ),
we add (xk, yk) to the training samples."
SAFE EXPLORATION AND EXPLOITATION,0.13883677298311445,"At any iteration, we can obtain an estimate for the current best decisions from"
SAFE EXPLORATION AND EXPLOITATION,0.14071294559099437,"ÀÜxk = arg max
x‚ààSk
lk(x, 0),
(11)"
SAFE EXPLORATION AND EXPLOITATION,0.1425891181988743,which returns the maximizer of the lower bound of the reward function within the current safe set.
SAFETY GUARANTEE,0.14446529080675422,"2.4
Safety Guarantee"
SAFETY GUARANTEE,0.14634146341463414,"To provide safety guarantees, we need the confidence intervals in (3) to contain the safety functions
with high probability for all iterations. Note that the parameter Œ≤k in (3) tunes the tightness of the
confidence interval. The following lemma guides us to make a proper choice for Œ≤k: This choice
depends on the information capacity Œ≥h
k associated with the kernel Œ∫, namely is the maximal mutual
information [33] we can obtain from the GP model of h through k noisy measurements ÀÜhXk at data
points Xk := {(xœÑ ‚ààX, œÑ, iœÑ ‚ààI)}œÑ<k"
SAFETY GUARANTEE,0.14821763602251406,"Œ≥h
k := max
Xk I(ÀÜhXk; h).
(12)"
SAFETY GUARANTEE,0.150093808630394,"Lemma 2.4. Assume that h(x, t, i) has RKHS norm associated with Œ∫ bounded by B > 0 and that
measurements are perturbed by œÉ-sub-Gaussian noise. Let the variable Œ≥h
k be defined as in (12)."
SAFETY GUARANTEE,0.15196998123827393,"For any Œ¥ ‚àà(0, 1), let ‚àöŒ≤k = B + œÉ
r"
SAFETY GUARANTEE,0.15384615384615385,"2

Œ≥h
k¬∑|I| + 1 + ln(1/Œ¥)

, then the following holds for all"
SAFETY GUARANTEE,0.15572232645403378,"decisions x ‚ààX, function indices i ‚ààI, and iterations k ‚â•1 jointly with probability at least 1 ‚àíŒ¥:"
SAFETY GUARANTEE,0.1575984990619137,"|h(x, t, i) ‚àí¬µk‚àí1(x, i)| ‚â§
p"
SAFETY GUARANTEE,0.15947467166979362,"Œ≤kœÉk‚àí1(x, i)."
SAFETY GUARANTEE,0.16135084427767354,"Proof. This lemma is a straightforward consequence of Lemma 1 of [16], a contextual extension of
Lemma 4.1 of [8]. We can prove it by selecting time as the context and picking {t}t‚â•1,t‚ààN as the
context sequence."
SAFETY GUARANTEE,0.16322701688555347,"Lemma 2.4 indicates that, by selecting Œ≤k properly, the confidence intervals Qk will w.h.p. contain
the reward function and the safety functions. Due to this, they can be leveraged to provide theoretical
guarantees for safety and optimality."
SAFETY GUARANTEE,0.1651031894934334,"The following theorem provides a sufficient condition for safety of TVSAFEOPT.
Theorem 2.5. Let Assumptions 2.1 - 2.3 hold, and let Œ≥h
k be defined as in (12). For any Œ¥ ‚àà(0, 1),"
SAFETY GUARANTEE,0.1669793621013133,"let ‚àöŒ≤k = B + œÉ
r"
SAFETY GUARANTEE,0.16885553470919323,"2

Œ≥h
k¬∑|I| + 1 + ln(1/Œ¥)

, then TVSAFEOPT guarantees that with probability at"
SAFETY GUARANTEE,0.17073170731707318,"least 1 ‚àíŒ¥, for all i ‚ààIc and for all t ‚â•0, and x ‚ààSk it holds ci(x, t) ‚â•0."
SAFETY GUARANTEE,0.1726078799249531,"The proof builds on Lemma 2.4 to show first that for all t ‚â•0, for all i ‚ààI and for all x ‚ààX, then
h(x, t, i) ‚ààCk(x, i) with high probability. Then using the recursive definition of the safe set from
(6), we obtain w.h.p. ci(x, t) ‚â•lk(x‚Ä≤, i) ‚àíLxd(x, x‚Ä≤) ‚àíL(t) ‚â•0, which concludes the proof. For
details we refer the reader to Appendix B."
NEAR-OPTIMALITY GUARANTEE,0.17448405253283303,"2.5
Near-Optimality Guarantee"
NEAR-OPTIMALITY GUARANTEE,0.17636022514071295,"In many safety critical real world applications, such as nuclear power plant operations, medical
devices calibration, automated emergency response systems, the reward function is stationary most of
the time. The problems are stationary until some changes happen and become stationary again when
the systems reach new equilibria [34]. However, ensuring optimality is non-trivial even when the
problem becomes stationary. Suppose the auxiliary function (2) becomes stationary in a time interval
[œï, œï], namely suppose there exist œï > œï ‚â•1 such that ‚àÄt1, t2 ‚àà[œï, œï], f(x, t1) = f(x, t2) =: ¬Øf(x)
and c(x, t1) = c(x, t2) =: ¬Øc(x), so that the optimization problem (1) becomes"
NEAR-OPTIMALITY GUARANTEE,0.17823639774859287,"max
x‚ààX
¬Øf(x)"
NEAR-OPTIMALITY GUARANTEE,0.1801125703564728,"subject to ¬Øci(x) ‚â•0, i ‚ààIc.
(13)"
NEAR-OPTIMALITY GUARANTEE,0.18198874296435272,We first define the largest safe set expanded from a set S subject to a measurement error a within
NEAR-OPTIMALITY GUARANTEE,0.18386491557223264,‚Ä¢ a single time step:
NEAR-OPTIMALITY GUARANTEE,0.18574108818011256,"Ra(S) := S ‚à™{x ‚ààX | ‚àÄi ‚ààIc, ‚àÉx‚Ä≤
i ‚ààS, s.t. ¬Øci (x‚Ä≤
i) ‚àíLxd(x, x‚Ä≤
i) ‚àía ‚â•0}"
NEAR-OPTIMALITY GUARANTEE,0.18761726078799248,"‚Ä¢ n time steps: Rn
a(S) := Ra (Ra . . . Ra (Ra
|
{z
}
n times"
NEAR-OPTIMALITY GUARANTEE,0.1894934333958724,(S)) . . . )
NEAR-OPTIMALITY GUARANTEE,0.19136960600375236,"‚Ä¢ arbitrary time steps: ¬ØRa(S) := limn‚Üí‚àûRn
a(S)"
NEAR-OPTIMALITY GUARANTEE,0.19324577861163228,"We also define ¬ØLt as an upper bound of the sum of all time Lipschitz constants, that is,
T ‚àí1
P"
NEAR-OPTIMALITY GUARANTEE,0.1951219512195122,"œÑ=0
L(œÑ) ‚â§¬ØLt."
NEAR-OPTIMALITY GUARANTEE,0.19699812382739212,"We find it reasonable that a tight upper bound ¬ØLt can be provided when the underlying system slowly
switches to the new stationarity condition."
NEAR-OPTIMALITY GUARANTEE,0.19887429643527205,"Given these definitions, we are now in the position to provide optimality guarantees for TVSAFEOPT.
In particular, we aim at comparing the found reward value ¬Øf(xk) with the optimal reward value
within the largest safe set obtained in ideal conditions with no measurement error, ¬ØR0(S0). We also
aim at providing TVSAFEOPT with an upper bound on the iterations needed to find a near-optimal
solution. The following theorem states the optimality guarantee of TVSAFEOPT.
Theorem 2.6. Let Assumptions 2.1 - 2.3 hold, let Œ≥h
k be defined as in (12) and, for any Œ¥ ‚àà(0, 1), let
‚àöŒ≤k = B + œÉ
r"
NEAR-OPTIMALITY GUARANTEE,0.20075046904315197,"2

Œ≥h
k¬∑|I| + 1 + ln(1/Œ¥)

. Define ÀÜxk as in (11), and, for any œµ > 0, let k‚àó(œµ, Œ¥) be"
NEAR-OPTIMALITY GUARANTEE,0.2026266416510319,the smallest positive integer satisfying k‚àó
NEAR-OPTIMALITY GUARANTEE,0.2045028142589118,"Œ≤k‚àóŒ≥h
k‚àó
‚â•b1
  ¬ØR0 (S0)
 + 1
 œµ2
,"
NEAR-OPTIMALITY GUARANTEE,0.20637898686679174,"where b1 = 8/ log
 
1 + œÉ‚àí2
. Then, the TVSAFEOPT algorithm, applied to (13), guarantees that,
with probability at least 1 ‚àíŒ¥, there exists k ‚â§k‚àósuch that
¬Øf (ÀÜxk) ‚â•
max
x‚àà¬Ø
Rœµ+ ¬Ø
Lt(S0)
¬Øf(x) ‚àíœµ."
NEAR-OPTIMALITY GUARANTEE,0.20825515947467166,"0
20
40
60
80
days t 0 1 2 3 4 5 6 7 8 1e4 (a)"
NEAR-OPTIMALITY GUARANTEE,0.2101313320825516,card. of safe set
NEAR-OPTIMALITY GUARANTEE,0.21200750469043153,"TVSafeOPT
SafeOPT
Approx. Opt."
NEAR-OPTIMALITY GUARANTEE,0.21388367729831145,"0
20
40
60
80
days t 0.0 0.1 0.2 0.3 0.4"
NEAR-OPTIMALITY GUARANTEE,0.21575984990619138,"0.5
(b)"
NEAR-OPTIMALITY GUARANTEE,0.2176360225140713,num. of unsafe decisions / card. of safe set
NEAR-OPTIMALITY GUARANTEE,0.21951219512195122,"TVSafeOPT
SafeOPT
Approx. Opt."
NEAR-OPTIMALITY GUARANTEE,0.22138836772983114,"0
20
40
60
80
days t 0.0 0.2 0.4 0.6 0.8 1.0 (c)"
NEAR-OPTIMALITY GUARANTEE,0.22326454033771106,num. of safe decisions / card. of ground truth safe region
NEAR-OPTIMALITY GUARANTEE,0.225140712945591,"TVSafeOPT
SafeOPT
Approx. Opt."
NEAR-OPTIMALITY GUARANTEE,0.2270168855534709,"Figure 2: Comparison between TVSAFEOPT, SAFEOPT, and approximate optimization on the
gas compressor case study, showing average of 10 repetitions with different initial sets. (a): The
cardinality of the safe sets, (b): The ratio between the number of unsafe decisions in the safe sets
and the cardinality of the safe sets, (c): The ratio between the number of safe decisions in the safe
sets and the cardinality of the ground truth safe regions. TVSAFEOPT robustly shrinks its safe sets
based on its observations and thus maintains much less violations in its safe sets than SAFEOPT and
approximate optimization, at the cost of covering less of the ground truth safe region."
NEAR-OPTIMALITY GUARANTEE,0.22889305816135083,"The proof consists in showing a decaying upper bound of uncertainty wk(x, i) ‚â§œµ and exploiting
local stationarity of (13) to provide bounds on the expansion of the safe set Sk. Details can be found
in Appendix C."
EXPERIMENTS,0.23076923076923078,"3
Experiments"
SYNTHETIC EXAMPLE,0.2326454033771107,"3.1
Synthetic Example"
SYNTHETIC EXAMPLE,0.23452157598499063,We first illustrate TVSAFEOPT on a synthetic two-dimensional time-varying optimization problem
SYNTHETIC EXAMPLE,0.23639774859287055,"max
x,y ‚àíex2 ‚àílog(1 + y2) + 0.01t"
SYNTHETIC EXAMPLE,0.23827392120075047,"s.t.

x + 0.5 ‚àí0.5

1 ‚àícos 2œÄ"
T,0.2401500938086304,"50 t

cos œÄ 6"
T,0.24202626641651032,"2
+

y ‚àí0.3 ‚àí0.5

1 ‚àícos 2œÄ"
T,0.24390243902439024,"50 t

sin œÄ 6"
T,0.24577861163227016,"2
‚â§1."
T,0.24765478424015008,"Figure 1 compares the safe sets computed by TVSAFEOPT, ETSAFEOPT, and SAFEOPT at t = 30,
t = 100 and t = 170. All algorithms start from the same singleton initial safe set S0 = {(‚àí0.5, 0.0)}.
Implementation details are described in Appendix A. Figure 1 illustrates that the safe sets computed
by TVSAFEOPT are contained in the ground truth safe regions while those computed by SAFEOPT
and ETSAFEOPT have multiple violations. Due to the dependence on time of the example (Figure 3),
the initial safe set becomes unsafe at t = 30, and t = 170. Taking the possible changes in time
into consideration, TVSAFEOPT correctly identifies the possible unsafety of the initial safe set. In
contrast, SAFEOPT always consider the initial safe set to be safe. Meanwhile, ETSAFEOPT correctly
identifies the lack of safety of the initial safe set at t = 30, but fails at t = 170. This is because
the event trigger is naturally insensitive to continuous changes. This toy example indicates that,
in contrast to SAFEOPT and ETSAFEOPT, TVSAFEOPT safely adapts to the time changes of the
optimization problem."
T,0.24953095684803,"In this example, TVSAFEOPT and ETSAFEOPT overall find better reward function values than
SAFEOPT, see Figure 3. The reward function value found by TVSAFEOPT is close to the optimal
values when the reward function changes slowly, which supports Theorem 2.6."
T,0.25140712945590993,"Quantitative metrics are listed in Table 2. Taking SAFEOPT as a baseline, TVSAFEOPT and
ETSAFEOPT achieves less violations, lower cumulative regret at the cost of covering less part
of the safe region. Furthermore, TVSAFEOPT has little violations, and achieves larger coverage ratio
than ETSAFEOPT. Meanwhile, its cumulative regret is just slightly higher than that of ETSAFEOPT."
T,0.25328330206378985,"Table 2: Synthetic example: comparison of TVSAFEOPT and ETSAFEOPT with respect to SAFEOPT,
showing the average and the standard deviation results from five runs with different initial safe sets
(chosen randomly from the feasible space)."
T,0.2551594746716698,"ETSAFEOPT
TVSAFEOPT
Violations
-84.4% ¬± 1.7 %
-99.99% ¬± 0.01%
Coverage Ratio
-30.9% ¬± 2.9 %
-21.0% ¬± 1.3%
Cumulative Regret
-73.6% ¬± 14.7%
-66.9% ¬± 14.4%"
T,0.2570356472795497,"Table 3: Compressors case study: comparison of TVSAFEOPT and SAFEOPT with respect to
Approximate Optimization, showing the average and the standard deviation results from 10 runs
with different initial safe sets (chosen randomly from [x0 ‚àí0.5/
‚àö"
T,0.2589118198874296,"3d, x0 + 0.5/
‚àö"
T,0.2607879924953096,"3d] where x0
is the initial safe seed and d is the distance to the boundary of the feasible region). ETSAFEOPT
is not included due to its high dependency on event detection methods, which are unavailable for
compressor degradation."
T,0.2626641651031895,"SAFEOPT
TVSAFEOPT
Violations
-89.2% ¬± 4.2 %
-96.8% ¬± 1.0%
Coverage Ratio
-35.7% ¬± 2.6 %
-61.0% ¬± 1.3%
Cumulative Regret
+95.8% ¬± 32.3%
+178.3% ¬± 29.2%"
GAS COMPRESSOR CASE STUDY,0.26454033771106944,"3.2
Gas Compressor Case Study"
PROBLEM SETUP,0.26641651031894936,"3.2.1
Problem Setup"
PROBLEM SETUP,0.2682926829268293,"We show the performance of the proposed algorithm in a compressor station with three identical
compressors operating in parallel at the time-varying compressor head Ht with time-varying power
consumption at time t (adapted from [35], details in Appendix A.3)"
PROBLEM SETUP,0.2701688555347092,"min
mi N
X i=1"
PROBLEM SETUP,0.27204502814258913,"1
1 ‚àídit"
PROBLEM SETUP,0.27392120075046905,"
Œ±1 + Œ±2 Àúmi + Œ±3 ÀúHt + Œ±4 Àúm2
i + Œ±5 Àúmi ÀúHt + Œ±6 ÀúH2
t

(14) s.t. N
X"
PROBLEM SETUP,0.275797373358349,"i=1
mi ‚â•Mt
(15)"
PROBLEM SETUP,0.2776735459662289,"mi ‚â•Œ≤1 ¬ØH2
t + Œ≤2 ¬ØHt + Œ≤3 , ‚àÄi = 1, . . . , N
(16)"
PROBLEM SETUP,0.2795497185741088,"mi ‚â•Œ≥1 ¬Ø¬ØH2
t + Œ≥2 ¬Ø¬ØHt + Œ≥3 , ‚àÄi = 1, . . . , N"
PROBLEM SETUP,0.28142589118198874,"mi ‚â§Œ¥1 ÀúÀúHt + Œ¥2 , ‚àÄi = 1, . . . , N"
PROBLEM SETUP,0.28330206378986866,"mi ‚â§œÉ1 Àú¬ØH2
t + œÉ2 Àú¬ØHt + œÉ3 , ‚àÄi = 1, . . . , N,
(17)"
PROBLEM SETUP,0.2851782363977486,"where the objective (14) corresponds to the power to run the station with N compressors, here N = 3,
affected by individual degradation dit, i = 1, l . . . , N. The station must also satisfy time-varying
demand Mt in (15). In practice, it is common to linearly approximate (16)-(17) with respect to the
compressor head Ht (dashed lines in Figure 4) [36]."
RESULTS,0.2870544090056285,"3.2.2
Results"
RESULTS,0.28893058161350843,"We compare the performance of TVSAFEOPT, SAFEOPT, and approximate optimization.
ETSAFEOPT is not applicable to this case study because the magnitude of changes in the demand Mt,
compressor head Ht, and degradation dit keeps triggering the event trigger and thus the maintained
safe set becomes empty very quickly. Implementation details are described in Appendix A. Figure 2
compares the number of unsafe decisions in the safe sets calculated by TVSAFEOPT, SAFEOPT, and
approximate optimization. We see that, by considering the uncertainty with respect to the decision
variables, SAFEOPT maintains fewer unsafe decisions in its safe sets than the approximate optimiza-
tion. However, SAFEOPT tends to expand its safe sets regardless of external changes. TVSAFEOPT
further improves this based on SAFEOPT by taking into consideration the time-varying safety func-
tions. TVSAFEOPT robustly shrinks its safe sets based on its observations and thus maintains much"
RESULTS,0.29080675422138835,"Figure 3: Comparison of reward functions from different methods with different initial safe sets,
averaged over 5 runs for the synthetic example (left) and 10 runs for the compressor case study (right,
indicating power in MW obtained from maximization of (14)), with error bars, with respect to the
optimal values (black). In the synthetic example, TVSAFEOPT finds better reward function values
than SAFEOPT, and similar to these of ETSAFEOPT. In the compressor case study, TVSAFEOPT
finds lower reward function values than SAFEOPT, but guarantees fewer violations (Table 2 and 3)
than either SAFEOPT or Approximate Optimization."
RESULTS,0.2926829268292683,"less violations in its safe sets than SAFEOPT (70.4%) and approximate optimization (96.8%). It
achieves this at the cost of covering less of the ground truth safe region than SAFEOPT (39.3%) and
Approximate Optimization (61.0%)."
RESULTS,0.2945590994371482,"The right-hand side of Figure 3 shows that TVSAFEOPT preserves safety at the expense of optimality.
In the compressor case study, TVSAFEOPT overall finds lower reward function values than SAFEOPT
and approximate optimization, which is consistent with the fact that it covers a lower fraction of the
ground truth safe regions and the reward function changes significantly between iterations. Because
of its strong focus on safety, TVSAFEOPT deviates more from the ground truth. The cumulative regret
of TVSAFEOPT is above the one of SAFEOPT by 42.1%, and the one of approximate optimization
by 178.3%. This illustrates the trade-off between safety and optimality in the presence of strong
uncertainties due to the varying reward and safety constraints. Quantitative metrics using Approximate
Optimization as the baseline are listed in Table 3."
LIMITATIONS AND CONCLUSION,0.2964352720450281,"4
Limitations and Conclusion"
LIMITATIONS AND CONCLUSION,0.29831144465290804,"Limitations The compressor case study demonstrated that TVSAFEOPT ensures safety at the expense
of optimality if the stationarity assumption is not satisfied. The assumption about the local stationarity
of the optimization problem (1) is thus the main limitation. Even though TVSAFEOPT demonstrates
good empirical performance with respect to safety even when the problem is non-stationary,
theoretical guarantee for its near-optimality in the non-stationary case warrant further investigation."
LIMITATIONS AND CONCLUSION,0.300187617260788,"The need for obtaining the Lipschitz constants with respect to both x and t in order to compute the
safe set Sk in (6) may prove limiting in real applications. To overcome this limitation, we propose
practical modifications in Appendix A.1."
LIMITATIONS AND CONCLUSION,0.30206378986866794,"Conclusions We propose TVSAFEOPT algorithm, which extends SAFEOPT to handle time-varying
optimization problems. In conclusion, TVSAFEOPT outperforms SAFEOPT in terms of adaptation to
changes in time and maintains fewer unsafe decisions in its safe sets for time-varying problems. This
is at the cost of covering less of the ground truth safe regions and may lead to poorer performance
in terms of optimality."
LIMITATIONS AND CONCLUSION,0.30393996247654786,"We prove the safety guarantee for TVSAFEOPT in the general time-varying setting and prove its
near-optimality guarantee for the case in which the optimization problem becomes stationary. The
two theoretical results together guarantee that TVSAFEOPT is capable of safely transferring safety
of the decisions into the future and, based on the transferred safe sets, it will find the near-optimal
decision when the reward function stops changing. We show that TVSAFEOPT performs well in
practice for the most general settings where both the reward function and the safety constraint are
time-varying, both on synthetic data and for real case study on a gas compressor."
LIMITATIONS AND CONCLUSION,0.3058161350844278,Acknowledgments and Disclosure of Funding
LIMITATIONS AND CONCLUSION,0.3076923076923077,"Research supported by NCCR Automation, National Centre of Competence in Research, funded by
the Swiss National Science Foundation (grant no. 180545). Marta Zagorowska also acknowledges
funding from the Marie Curie Horizon Postdoctoral Fellowship project RELIC (grant no 101063948)
for writing, revisions, and the compressor case study. Alisa Rupenyan acknowledges also support
from the Johann Jakob Rieter foundation."
REFERENCES,0.30956848030018763,References
REFERENCES,0.31144465290806755,"[1] Daniel Lizotte, Tang Wao, Michael Bowling, and Dale Schuurmans. Automatic gait optimization
with Gaussian process regression. International Joint Conference on Artificial Intelligence,
pages 944‚Äì949, 2007."
REFERENCES,0.3133208255159475,"[2] Ruben Martinez-Cantin, Nando de Freitas, Arnaud Douchet, and Jos√© A. Castellanos. Active
policy learning for robot planning and exploration under uncertainty. In Proceedings Robotics:
Science and Systems, pages 321‚Äì328, 2007."
REFERENCES,0.3151969981238274,"[3] Ziyu Wang, Masrour Zoghi, Frank Hutter, David Matheson, and Nando de Freitas. Bayesian
optimization in high dimension via random embeddings. International Joint Conference on
Artificial Intelligence, pages 1778‚Äì1784, 2013."
REFERENCES,0.3170731707317073,"[4] Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias Seeger. Gaussian process
optimization in the bandit setting: No regret and experimental design. International Joint
Conference on Machine Learning, pages 1015‚Äì1022, 2010."
REFERENCES,0.31894934333958724,"[5] Matthew Hoffman, Bobak Shahriari, and Nando de Freitas. On correlation and budget con-
straints in model-based bandit optimization with application to automatic machine learning.
International Conference on Artificial Intelligence and Statistics, pages 365‚Äì374, 2014."
REFERENCES,0.32082551594746717,"[6] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian optimization of machine
learning algorithms. In Proceedings of Advances in Neural Information Processing Systems,
pages 2951‚Äì2959, 2012."
REFERENCES,0.3227016885553471,"[7] Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause. Safe exploration for optimization
with Gaussian processes. In International conference on machine learning, pages 997‚Äì1005.
PMLR, 2015."
REFERENCES,0.324577861163227,"[8] Felix Berkenkamp, Andreas Krause, and Angela P Schoellig. Bayesian optimization with safety
constraints: safe and automatic parameter tuning in robotics. Machine Learning, pages 1‚Äì35,
2021."
REFERENCES,0.32645403377110693,"[9] Yanan Sui, Vincent Zhuang, Joel Burdick, and Yisong Yue. Stagewise safe Bayesian opti-
mization with Gaussian processes. In International conference on machine learning, pages
4781‚Äì4789. PMLR, 2018."
REFERENCES,0.32833020637898686,"[10] Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe exploration for interactive
machine learning. Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.3302063789868668,"[11] Dominik Baumann, Alonso Marco, Matteo Turchetta, and Sebastian Trimpe. GoSafe: Globally
optimal safe robot learning. In 2021 IEEE International Conference on Robotics and Automation
(ICRA), pages 4452‚Äì4458. IEEE, 2021."
REFERENCES,0.3320825515947467,"[12] Bhavya Sukhija, Matteo Turchetta, David Lindner, Andreas Krause, Sebastian Trimpe, and
Dominik Baumann. GoSafeOpt: Scalable safe exploration for global optimization of dynamical
systems. Artificial Intelligence, page 103922, 2023."
REFERENCES,0.3339587242026266,"[13] Alessandro G. Bottero, Carlos E. Luis, Julia Vinogradska, Felix Berkenkamp, and Jan Peters.
Information-theoretic safe Bayesian optimization, 2024."
REFERENCES,0.33583489681050654,"[14] Jonas H√ºbotter, Bhavya Sukhija, Lenart Treven, Yarden As, and Andreas Krause. Information-
based transductive active learning. arXiv preprint arXiv:2402.15898, 2024."
REFERENCES,0.33771106941838647,"[15] Marta Zagorowska, Efe C. Balta, Varsha Behrunani, Alisa Rupenyan, and John Lygeros.
Efficient sample selection for safe learning. IFAC-PapersOnLine, 56(2):10107‚Äì10112, 2023.
22nd IFAC World Congress."
REFERENCES,0.3395872420262664,"[16] Daniel Widmer, Dongho Kang, Bhavya Sukhija, Jonas H√ºbotter, Andreas Krause, and Stelian
Coros. Tuning legged locomotion controllers via safe Bayesian optimization. In 7th Annual
Conference on Robot Learning (CoRL), 6-9 November, Atlanta, GA, 2023. URL https:
//proceedings.mlr.press/v229/widmer23a.html."
REFERENCES,0.34146341463414637,"[17] Christopher K√∂nig, Miks Ozols, Anastasia Makarova, Efe C. Balta, Andreas Krause, and Alisa
Rupenyan. Safe risk-averse Bayesian optimization for controller tuning. IEEE Robotics and
Automation Letters, 8(12):8208‚Äì8215, 2023."
REFERENCES,0.3433395872420263,"[18] Andreas Krause and Cheng Ong. Contextual Gaussian process bandit optimization. Advances
in neural information processing systems, 24, 2011."
REFERENCES,0.3452157598499062,"[19] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process
optimization in the bandit setting: no regret and experimental design. In Proceedings of the
27th International Conference on International Conference on Machine Learning, ICML‚Äô10,
page 1015‚Äì1022, Madison, WI, USA, 2010. Omnipress."
REFERENCES,0.34709193245778613,"[20] Marcello Fiducioso, Sebastian Curi, Benedikt Schumacher, Markus Gwerder, and Andreas
Krause. Safe contextual bayesian optimization for sustainable room temperature PID control
tuning. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelli-
gence, IJCAI-19, pages 5850‚Äì5856. International Joint Conferences on Artificial Intelligence
Organization, 7 2019."
REFERENCES,0.34896810506566606,"[21] Christopher K√∂nig, Matteo Turchetta, John Lygeros, Alisa Rupenyan, and Andreas Krause. Safe
and efficient model-free adaptive control via Bayesian optimization. In 2021 IEEE International
Conference on Robotics and Automation (ICRA), pages 9782‚Äì9788. IEEE, 2021."
REFERENCES,0.350844277673546,"[22] Antonia Holzapfel, Paul Brunzema, and Sebastian Trimpe. Event-triggered safe Bayesian
optimization on quadcopters. In Alessandro Abate, Mark Cannon, Kostas Margellos, and
Antonis Papachristodoulou, editors, Proceedings of the 6th Annual Learning for Dynamics &
Control Conference (L4DC), 15-17 July, Oxford, UK, volume 242 of Proceedings of Machine
Learning Research, pages 1033‚Äì1045. PMLR, 15‚Äì17 Jul 2024."
REFERENCES,0.3527204502814259,"[23] Ilija Bogunovic, Jonathan Scarlett, and Volkan Cevher. Time-varying Gaussian process bandit
optimization. In Artificial Intelligence and Statistics, pages 314‚Äì323. PMLR, 2016."
REFERENCES,0.3545966228893058,"[24] Paul Brunzema, Alexander von Rohr, Friedrich Solowjow, and Sebastian Trimpe. Event-
triggered time-varying Bayesian optimization. arXiv preprint arXiv:2208.10790, 2022."
REFERENCES,0.35647279549718575,"[25] Kihyuk Hong, Yuhang Li, and Ambuj Tewari. An optimization-based algorithm for non-
stationary kernel bandits without prior knowledge. In International Conference on Artificial
Intelligence and Statistics, pages 3048‚Äì3085. PMLR, 2023."
REFERENCES,0.35834896810506567,"[26] Xingyu Zhou and Ness Shroff. No-regret algorithms for time-varying Bayesian optimization.
In 2021 55th Annual Conference on Information Sciences and Systems (CISS), pages 1‚Äì6. IEEE,
2021."
REFERENCES,0.3602251407129456,"[27] Yuntian Deng, Xingyu Zhou, Baekjin Kim, Ambuj Tewari, Abhishek Gupta, and Ness Shroff.
Weighted Gaussian process bandits for non-stationary environments. In International Confer-
ence on Artificial Intelligence and Statistics, pages 6909‚Äì6932. PMLR, 2022."
REFERENCES,0.3621013133208255,"[28] Dinesh Krishnamoorthy and Francis J Doyle. Safe Bayesian optimization using interior-point
methods‚Äîapplied to personalized insulin dose guidance. IEEE Control Systems Letters, 6:
2834‚Äì2839, 2022."
REFERENCES,0.36397748592870544,"[29] Dinesh Krishnamoorthy and Francis J Doyle III. Model-free real-time optimization of process
systems using safe Bayesian optimization. AIChE Journal, 69(4):e17993, 2023."
REFERENCES,0.36585365853658536,"[30] Lukas Hewing, Kim P Wabersich, Marcel Menner, and Melanie N Zeilinger. Learning-based
model predictive control: Toward safe learning in control. Annual Review of Control, Robotics,
and Autonomous Systems, 3:269‚Äì296, 2020."
REFERENCES,0.3677298311444653,"[31] Carl Edward Rasmussen and Chris Williams. Gaussian Processes for Machine Learning.
Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA, USA, January
2006."
REFERENCES,0.3696060037523452,"[32] Bernhard Sch√∂lkopf and Alexander J Smola. Learning with kernels: support vector machines,
regularization, optimization, and beyond. The MIT Press, 2002."
REFERENCES,0.3714821763602251,"[33] Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999."
REFERENCES,0.37335834896810505,"[34] Michael Vogt and Holger Dette. Detecting gradual changes in locally stationary processes. The
Annals of Statistics, 43(2):713‚Äì740, 2015."
REFERENCES,0.37523452157598497,"[35] Marta Zagorowska and Nina F. Thornhill. Influence of compressor degradation on optimal
load-sharing. Computers and Chemical Engineering, 143(5):107104, 2020."
REFERENCES,0.3771106941838649,"[36] Andrea Cortinovis, Joachim Ferreau, Daniel Lewandowski, and Mehmet Mercang√∂z. Experi-
mental evaluation of MPC-based anti-surge and process control for electric driven centrifugal
gas compressors. Journal of Process Control, 34:13‚Äì25, 2015."
REFERENCES,0.3789868667917448,"[37] Felix Berkenkamp, Angela P Schoellig, and Andreas Krause. Safe controller optimization for
quadrotors with Gaussian processes. In 2016 IEEE International Conference on Robotics and
Automation (ICRA), 16-21 May, Stockholm, Sweden, pages 491‚Äì496. IEEE, 2016."
REFERENCES,0.3808630393996248,"[38] Rainer Kurz, Matt Lubomirsky, and Klaus Brun. Gas compressor station economic optimization.
International Journal of Rotating Machinery, 2012:Article ID 715017, 9 pages, 2012."
REFERENCES,0.3827392120075047,"[39] Predrag Milosavljevic, Alejandro G. Marchetti, Andrea Cortinovis, Timm Faulwasser, Mehmet
Mercang√∂z, and Dominique Bonvin. Real-time optimization of load sharing for gas compressors
in the presence of uncertainty. Applied Energy, 272:114883, 2020."
REFERENCES,0.38461538461538464,"[40] Marta Zagorowska. Degradation modelling in process control applications. PhD thesis, Imperial
College London, 2020. available at https://spiral.imperial.ac.uk/handle/10044/1/
105173, online 22 May 2024."
REFERENCES,0.38649155722326456,"[41] Do Won Kang and Tong Seop Kim. Model-based performance diagnostics of heavy-duty gas
turbines using compressor map adaptation. Applied Energy, 212:1345‚Äì1359, 2018."
REFERENCES,0.3883677298311445,"[42] Andrea Cortinovis, Mehmet Mercang√∂z, Matteo Zovadelli, Diego Pareschi, Antonio De Marco,
and Sergio Bittanti. Online performance tracking and load sharing optimization for parallel
operation of gas compressors. Computers & Chemical Engineering, 88:145‚Äì156, 5 2016."
REFERENCES,0.3902439024390244,"[43] Ayman Al Zawaideh, Khalifa Al Hosani, Igor Boiko, and Mohammad Luai Hammadih. Mini-
mum energy adaptive load sharing of parallel operated compressors. IEEE Open Journal of
Industry Applications, 3:178‚Äì191, 2022."
REFERENCES,0.3921200750469043,"[44] Vibeke St√¶rkebye N√∏rsteb√∏. Optimum Operation of Gas Export Systems. PhD thesis, Norwegian
University of Science and Technology, 2008."
REFERENCES,0.39399624765478425,"[45] Rainer Kurz and Klaus Brun. Degradation of gas turbine performance in natural gas service.
Journal of Natural Gas Science and Engineering, 1(3):95‚Äì102, 2009."
REFERENCES,0.39587242026266417,"[46] Yiuguang Li and Pannawat Nilkitsaranont. Gas turbine performance prognostic for condition-
based maintenance. Applied Energy, 86(10):2152‚Äì2161, October 2009. ISSN 0306-2619."
REFERENCES,0.3977485928705441,"[47] Matteo Cicciotti. Adaptive Monitoring of Health-state and Performance of Industrial Centrifugal
Compressors. PhD thesis, Imperial College London, 2015."
REFERENCES,0.399624765478424,"[48] Marta Zagorowska, Frederik Schulze Sp√ºntrup, Arne-Marius Ditlefsen, Lars Imsland, Erling
Lunde, and Nina F. Thornhill. Adaptive detection and prediction of performance degradation in
off-shore turbomachinery. Applied Energy, 268:p. 114934, 2020."
REFERENCES,0.40150093808630394,"[49] Yu-Zhi Chen, Xu-Dong Zhao, Heng-Chao Xiang, and Elias Tsoutsanis. A sequential model-
based approach for gas turbine performance diagnostics. Energy, 220:119657, 2021-04. ISSN
0360-5442."
REFERENCES,0.40337711069418386,"[50] Wenjie Xu, Yuning Jiang, Bratislav Svetozarevic, and Colin Jones. Constrained efficient
global optimization of expensive black-box functions. In Andreas Krause, Emma Brunskill,
Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings
of the 40th International Conference on Machine Learning, volume 202 of Proceedings of
Machine Learning Research, pages 38485‚Äì38498. PMLR, 23‚Äì29 Jul 2023."
REFERENCES,0.4052532833020638,"[51] Wenjie Xu, Yuning Jiang, Bratislav Svetozarevic, and Colin N Jones. Primal-dual contextual
bayesian optimization for control system online optimization with time-average constraints. In
2023 62nd IEEE Conference on Decision and Control (CDC), pages 4112‚Äì4117. IEEE, 2023."
REFERENCES,0.4071294559099437,"A
Experiment Details"
REFERENCES,0.4090056285178236,"Experiments are conducted on an Intel i7-11370H CPU using Python 3.8.5. The implementation
utilizes the following libraries: GPy 1.12.0, NumPy 1.22.0, and Matplotlib 3.5.0."
REFERENCES,0.41088180112570355,"A.1
Practical Modifications"
REFERENCES,0.41275797373358347,"In practice, Lipschitz constants are difficult to estimate. Thus, here we provide a Lipschitz-constant-
free version of TVSAFEOPT algorithm by modifying (6) and (8)."
REFERENCES,0.4146341463414634,"The safe set is updated as all decisions with non-negative lower confidence bounds for the safety
functions at the current iteration k, that is,"
REFERENCES,0.4165103189493433,"Sk = {x ‚ààX | ‚àÄi ‚ààIc, lk(x, i) ‚â•0} .
(18)"
REFERENCES,0.41838649155722324,"Furthermore, the expanders are intuitively defined as decisions within the current safe set such that,
by evaluating any of the decisions, at least one decision outside the current safe set will be considered
as safe, that is, Gk = {x ‚ààSk | ek(x) > 0}, where ek(x) denotes the number of decisions outside
Sk that will be considered safe when evaluating x. Instead of using Lipschitz constants, we define
ek(x) using lower bound of auxiliary GP similar to the method by Berkenkamp et al. [37]"
REFERENCES,0.4202626641651032,"ek(x) =

x‚Ä≤ ‚ààD\Sk | ‚àÉi ‚ààIc : lk,(x,uk(x,i)) (x‚Ä≤, k + 1, i) ‚â•0
	 ,"
REFERENCES,0.42213883677298314,"where lk,(x,uk(x,i)) (x‚Ä≤, k + 1, i) denotes the lower bound of the function values at x and t = k + 1
if x is evaluated at the k-th iteration and the upper bound is observed."
REFERENCES,0.42401500938086306,"A.2
Synthetic Example"
REFERENCES,0.425891181988743,"The search space is X = [‚àí2, 2]2, uniformly quantized into 100 √ó 100 points. Both algorithms start
with the singleton initial safe set {(‚àí0.5, 0.0)}. The measurements are perturbed by i.i.d. Gaussian
noise N(0, 0.012)."
REFERENCES,0.4277673545966229,"The reward function is formulated as: f(x, t) = ‚àíex2 ‚àílog(1 + y2) + 0.01t;"
REFERENCES,0.42964352720450283,"The safety function is formulated as: c1(x, t) = 1 ‚àí

x + 0.5 ‚àí0.5
 
1 ‚àícos 2œÄ"
T,0.43151969981238275,"50 t

cos œÄ"
T,0.4333958724202627,"6
2 ‚àí

y ‚àí0.3 ‚àí0.5
 
1 ‚àícos 2œÄ"
T,0.4352720450281426,"50 t

sin œÄ 6
2."
T,0.4371482176360225,"The hyperparameters of GPs in the synthetic case study are modelled as follows,"
T,0.43902439024390244,"‚Ä¢ TVSAFEOPT: The reward function and the safety function are modeled by independent
GPs with zero mean and spatio-temporal kernel Œ∫((x, t), (x‚Ä≤, t‚Ä≤)) = exp

‚àí‚à•x‚àíx‚Ä≤‚à•2
2
2œÉ2
1 
¬∑"
T,0.44090056285178236,"exp

‚àí( t‚àít‚Ä≤)2 2œÉ2
2"
T,0.4427767354596623,"
, where œÉ1 ‚â°1.0, œÉ2 = 25.0 for f, and œÉ2 = 15.0 for c1."
T,0.4446529080675422,"‚Ä¢ SAFEOPT: The reward function and the safety function are modeled by independent GPs
with zero mean and 2d Gaussian kernel Œ∫(x, x‚Ä≤) = exp

‚àí‚à•x‚àíx‚Ä≤‚à•2
2
2œÉ2
3"
T,0.44652908067542213,"
, where œÉ3 ‚â°1.0."
T,0.44840525328330205,"‚Ä¢ ETSAFEOPT: Hyperparameters for GPs are the same as SAFEOPT. Besides, we choose the
sentivity of the event trigger Œ¥ as 0.01."
T,0.450281425891182,"A.3
Compressor Case Study"
T,0.4521575984990619,"Centrifugal compressors are often used in gas transport networks to deliver the required amount of
gas by boosting the pressure in the pipelines. Organised as compressor stations with N units, the
compressors are often operated to minimise their power consumption P while satisfying the demand
Mt and operating constraints, capturing how compressor head Ht depends on the mass flow through
the compressor [38, 39]:"
T,0.4540337711069418,‚Ä¢ Àúmi = mi‚àí157.4
T,0.45590994371482174,"34.37
, ÀúHt = Ht‚àí1.016e5"
T,0.45778611632270166,"3.210e4
, Œ±1 = 1.979e7, Œ±2 = 5.274e6, Œ±3 = 5.375e6, Œ±4 =
6.055e5, Œ±5 = 5.718e5, Œ±6 = 3.319e5"
T,0.4596622889305816,‚Ä¢ ¬ØHt = Ht‚àí1.235e5
T,0.46153846153846156,"3.764e4
, Œ≤1 = ‚àí1.953, Œ≤2 = 16.86, Œ≤3 = 118.1"
T,0.4634146341463415,‚Ä¢ ¬Ø¬ØHt = Ht‚àí6.152e4
T,0.4652908067542214,"7002
, Œ≥1 = ‚àí1.516, Œ≥2 = ‚àí11.12, Œ≥3 = 116.9"
T,0.46716697936210133,‚Ä¢ ÀúÀúHt = Ht‚àí8.706e4
T,0.46904315196998125,"5.289e4
, Œ¥1 = 73.21, Œ¥2 = 183.7"
T,0.4709193245778612,‚Ä¢ Àú¬ØHt = Ht‚àí1.572e5
T,0.4727954971857411,"2.044e4
, œÉ1 = ‚àí7.260, œÉ2 = ‚àí29.65, œÉ3 = 204.4"
T,0.474671669793621,"The compressor case study has been adapted from [35]. The data for the demand, compressor head,
and degradation for the three compressors were obtained from [40] (Creative Commons Attribution
NonCommercial Licence)."
T,0.47654784240150094,"Individual characteristics of compressors in (16)-(17) are called compressor maps (Figure 4). The
operating area for a compressor is defined by minimal and maximal speed of the compressor and
its mechanical properties. The operating area can be obtained from compressors maps delivered by
the manufacturer of the compressor, or estimated during the operation [41]. However, estimation
would require collecting datapoints close to the boundary of the operating area, which may be
unavailable due to safety consideration [42, 43]. Using safe learning has the potential to improve
the operation of the station because it enables safe exploration of the unknown operating area of
individual compressors."
T,0.47842401500938087,"Max. speed app.
Choke app.
Min. speed app.
Surge app."
T,0.4803001876172608,"Surge
Minimum speed"
T,0.4821763602251407,"Choke
Maximum speed"
T,0.48405253283302063,"Figure 4: Ground truth (solid) and linear approximation (dashed) of the operating area from com-
pressor maps, adapted from [44, 35]. For a given compressor head at time t (dotted horizontal line
for Ht = 120000 J kg‚àí1), the mass flow mit through the i-th compressor is required to be between
minimum speed (red) and surge (blue) lines, and maximum speed (violet) and choke (yellow) lines"
T,0.48592870544090055,"Furthermore, varying operating conditions and demand often lead to compressor degradation dit
(Figure 5), over time increasing power consumption (14) of the entire compressor station [45].
Capturing the time-varying aspect of compressor degradation is a subject of research (e.g. [46‚Äì48])
but limited availability of measured degradation data presents a challenge [49]."
T,0.4878048780487805,"For convenience, the optimization variables are scaled by a factor K = 200, that is, x =
(m1, m2, m3)/K.
The search space is X
= [50.0/K, 250.0/K]3, uniformly quantized into
60 √ó 60 √ó 60 points. Both algorithms start with the singleton initial safe set {(M0, M0.M0)/3K}.
The measurements are perturbed by i.i.d. Gaussian noise N(0, 0.012)."
T,0.4896810506566604,The reward function is formulated as:
T,0.4915572232645403,"f(x, t) = ‚àí"
X,0.49343339587242024,"3
X i=1"
X,0.49530956848030017,"1
(1 ‚àídit) ¬∑ 107"
X,0.4971857410881801,"
Œ±1 + Œ±2 Àúmi + Œ±3 ÀúHt + Œ±4 Àúm2
i + Œ±5 Àúmi ÀúHt + Œ±6 ÀúH2
t
"
X,0.49906191369606,"The safety functions are formulated as ci(x, t) ‚â•0, i = 1, . . . , 7, with:"
X,0.50093808630394,"‚Ä¢ c1(x, t) = x1 ‚àíLt"
X,0.5028142589118199,"0
20
40
60
80
days t 0 1 2 3 4 5 6 7 8"
X,0.5046904315196998,kg ‚ãÖ s‚àí1 1e2 (a)
X,0.5065666041275797,Demand
X,0.5084427767354597,"0
20
40
60
80
days t 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
X,0.5103189493433395,J ‚ãÖ kg‚àí1 1e5 (a)
X,0.5121951219512195,Compressor Head
X,0.5140712945590994,"0
20
40
60
80
days t 0.00 0.01 0.02 0.03 0.04 0.05"
X,0.5159474671669794,"0.06
(c)"
X,0.5178236397748592,Degradation
X,0.5196998123827392,"Compressor 1
Compressor 2
Compressor 3"
X,0.5215759849906192,"Figure 5: Visualization of demand (a), compressor head (b), and degradation for the compressors (c)
changing with time."
X,0.5234521575984991,"‚Ä¢ c2(x, t) = Ut ‚àíx1
‚Ä¢ c3(x, t) = x2 ‚àíLt
‚Ä¢ c4(x, t) = Ut ‚àíx2
‚Ä¢ c5(x, t) = x3 ‚àíLt
‚Ä¢ c6(x, t) = Ut ‚àíx3
‚Ä¢ c7(x, t) = x1 + x2 + x3 ‚àí0.67Mt/K,"
X,0.525328330206379,"where Lt = max{Œ≤1 ¬ØH2
t + Œ≤2 ¬ØHt + Œ≤3, Œ≥1 ¬Ø¬ØH2
t + Œ≥2 ¬Ø¬ØHt + Œ≥3}/K, Ut = min{Œ¥1 ÀúÀúHt + Œ¥2, œÉ1 Àú¬ØH2
t +
œÉ2 Àú¬ØHt + œÉ3}/K."
X,0.5272045028142589,"The hyperparameters of GPs in the compressor case study are modelled as follows,"
X,0.5290806754221389,"‚Ä¢ TVSAFEOPT: The reward function and the safety functions are modeled by independent
GPs with zero mean and spatio-temporal kernel Œ∫((x, t), (x‚Ä≤, t‚Ä≤)) = exp

‚àí‚à•x‚àíx‚Ä≤‚à•2
2
2œÉ2
1 
¬∑"
X,0.5309568480300187,"exp

‚àí( t‚àít‚Ä≤)2 2œÉ2
2"
X,0.5328330206378987,"
, where œÉ1 ‚â°1.0, œÉ2 = 80.0 for f and c1 - c6, and œÉ2 = 70.0 for c7."
X,0.5347091932457786,"‚Ä¢ SAFEOPT: The reward function and the safety functions are modeled by independent GPs
with zero mean and 3d Gaussian kernel Œ∫(x, x‚Ä≤) = exp

‚àí‚à•x‚àíx‚Ä≤‚à•2
2
2œÉ2
3"
X,0.5365853658536586,"
, where œÉ3 ‚â°1.0."
X,0.5384615384615384,"As for approximate optimization, the r.h.s. of (16) - (17) are linearly approximated as follows:"
X,0.5403377110694184,"‚Ä¢ Surge line: Œ≤1 ¬ØH2
t + Œ≤2 ¬ØHt + Œ≤3 ‚âà4.481e ‚àí4 ¬∑ Ht + 59.76"
X,0.5422138836772983,"‚Ä¢ Min. speed line: Œ≥1 ¬Ø¬ØH2
t + Œ≥2 ¬Ø¬ØHt + Œ≥3 ‚âà‚àí1.333e ‚àí3 ¬∑ Ht + 193.3"
X,0.5440900562851783,‚Ä¢ Choke line: Œ¥1 ÀúÀúHt + Œ¥2 ‚âà1.611e ‚àí3 ¬∑ Ht + 46.77
X,0.5459662288930581,"‚Ä¢ Max. speed line: œÉ1 Àú¬ØH2
t + œÉ2 Àú¬ØHt + œÉ3 ‚âà‚àí1.667e ‚àí3 ¬∑ Ht + 461.7"
X,0.5478424015009381,"B
Proof of Safety Guarantee"
X,0.549718574108818,"Note all following lemmas hold for any Œ¥ ‚àà(0, 1), and S0, such that ‚àÖ‚ääS0 ‚äÜX."
X,0.551594746716698,"First, we want to show that the intersected confidence interval Ck in (4) w.h.p. contains the reward
function and safety functions h(x, t, i) as in (2)."
X,0.5534709193245778,"Lemma B.1. Let ‚àöŒ≤k = B + œÉ
r"
X,0.5553470919324578,"2

Œ≥h
k¬∑|I| + 1 + ln(1/Œ¥)

, with Œ≥h
k defined as in (12) and Ck(x, i)"
X,0.5572232645403377,"defined as in (4), then the following holds with probability at least 1 ‚àíŒ¥ :"
X,0.5590994371482176,"h(x, t, i) ‚ààCk(x, i)
‚àÄt ‚â•0, ‚àÄi ‚ààI, ‚àÄx ‚ààX,"
X,0.5609756097560976,"Proof by induction.
If t = 0, by Assumption 2.3 and the definition of Ck in (4), then h(x, 0, i) ‚ààC0(x, i), for all i ‚ààI
and for all x ‚ààX."
X,0.5628517823639775,"Suppose, for any t = œÑ ‚â•0, that h(x, œÑ, i) ‚ààCœÑ(x, i), then for t = œÑ + 1, from the Lipschitz
continuity of h, |h(x, œÑ + 1, i) ‚àíh(x, œÑ, i)| ‚â§L(œÑ), it holds that h(x, œÑ + 1, i) ‚ààCœÑ(x, i) ‚äï
[‚àíL(œÑ), L(œÑ)]."
X,0.5647279549718575,"Moreover, by Lemma 2.4 and (3) we have that h(x, œÑ + 1, i) ‚ààQœÑ+1(x, i)."
X,0.5666041275797373,"Thus, h(x, œÑ + 1, i) ‚àà(CœÑ(x, i) ‚äï[‚àíL(œÑ), L(œÑ)]) ‚à©QœÑ+1(x, i) = CœÑ+1(x, i), ‚àÄi ‚ààI, ‚àÄx ‚ààX."
X,0.5684803001876173,"Therefore, for all t ‚â•0, for all i ‚ààI and for all x ‚ààX we have that h(x, t, i) ‚ààCk(x, i), and this
concludes the proof."
X,0.5703564727954972,"We are now ready to prove Theorem 2.5 that provides a sufficient condition for TVSAFEOPT to
ensure safety embedded in the constraints ci(x, t) ‚â•0, for all i ‚ààIc."
X,0.5722326454033771,"Proof of Theorem 2.5.
If t = 0, by definition of S0, one has ci(x, t) = ci(x, 0) ‚â•L(0) ‚â•0, ‚àÄi ‚ààIc, ‚àÄx ‚ààS0."
X,0.574108818011257,"For any t ‚â•1, ‚àÄx ‚ààSt, by recursive definition of Sk in (6), ‚àÄi ‚ààIc, there exists x‚Ä≤ ‚ààSt‚àí1,
s.t. lk(x‚Ä≤, i) ‚àíLxd(x, x‚Ä≤) ‚àíL(t) ‚â•0. Then, ‚àÄi ‚ààIc"
X,0.575984990619137,"ci(x, t)"
X,0.5778611632270169,"‚â•ci(x‚Ä≤, t) ‚àíLxd(x, x‚Ä≤)
by Lipschitz continuity with x"
X,0.5797373358348968,"‚â•lk(x‚Ä≤, i) ‚àíLxd(x, x‚Ä≤)
by Lemma B.1"
X,0.5816135084427767,"‚â•lk(x‚Ä≤, i) ‚àíLxd(x, x‚Ä≤) ‚àíL(t)
‚â•0"
X,0.5834896810506567,and this concludes the proof.
X,0.5853658536585366,"C
Proof of Near-Optimality Guarantee"
X,0.5872420262664165,"The proof of near optimality consists in two parts: i) bounding the uncertainty and ii) bounding the
expansion of the safe set."
X,0.5891181988742964,"C.1
Bounding the Uncertainty"
X,0.5909943714821764,"We first derive a decaying upper bound of uncertainty for TVSAFEOPT. In this way we can ensure
the uncertainty of the reward function and safety functions to drop below a desired threshold.
Lemma C.1. Define b1 := 8/ log
 
1 + œÉ‚àí2
‚ààR, and Œ≥h
k as in (12). For any k > k0 ‚â•1, there
exists k‚Ä≤ ‚àà(k0, k], such that the following holds for all i ‚ààI:"
X,0.5928705440900562,"wk‚Ä≤(xk‚Ä≤, i) ‚â§ s"
X,0.5947467166979362,"b1Œ≤kŒ≥h
k
k ‚àík0
,"
X,0.5966228893058161,"Proof.
Let iœÑ := arg max
i‚ààI
wœÑ(xœÑ, i), where xœÑ = arg max
x‚ààGœÑ ‚à™MœÑ
max
i‚ààI wœÑ(x, i). For all i ‚ààI, k0 < k, there"
X,0.5984990619136961,"exists k‚Ä≤ ‚àà(k0, k]:"
X,0.600375234521576,"wk‚Ä≤(xk‚Ä≤, i)"
X,0.6022514071294559,"‚â§
1
k ‚àík0 k
X"
X,0.6041275797373359,"œÑ=k0+1
wœÑ(xœÑ, iœÑ)"
X,0.6060037523452158,"(a)
‚â§
2
k ‚àík0 k
X"
X,0.6078799249530957,œÑ=k0+1 p
X,0.6097560975609756,"Œ≤œÑœÉœÑ‚àí1(xœÑ, iœÑ) ‚â§2‚àöŒ≤k k ‚àík0 k
X"
X,0.6116322701688556,"œÑ=k0+1
œÉœÑ‚àí1(xœÑ, iœÑ) (b)
‚â§"
X,0.6135084427767354,"v
u
u
t 4Œ≤k k ‚àík0 k
X"
X,0.6153846153846154,"œÑ=k0+1
œÉ2
œÑ‚àí1(xœÑ, iœÑ) (c)
‚â§"
X,0.6172607879924953,"v
u
u
t b1Œ≤k k ‚àík0 1
2 k
X"
X,0.6191369606003753,"œÑ=k0+1
log(1 + œÉ‚àí2œÉ2
œÑ‚àí1(xœÑ, iœÑ)) (d)
‚â§"
X,0.6210131332082551,"v
u
u
t b1Œ≤k k ‚àík0 1
2 k
X"
X,0.6228893058161351,"œÑ=k0+1
log(1 + œÉ‚àí2œÉ‚Ä≤2
œÑ‚àí1(xœÑ, iœÑ)) (e)
= s"
X,0.624765478424015,"b1Œ≤kI(ÀÜhXk; h) k ‚àík0 (f)
‚â§ s"
X,0.626641651031895,"b1Œ≤kŒ≥h
k
k ‚àík0
(a): Definition of wk in (5),"
X,0.6285178236397748,"(b): From the fact that the quadratic mean upper bounds the arithmetic mean,"
X,0.6303939962476548,"(c): œÉ2
œÑ‚àí1(xœÑ, iœÑ) ‚â§k ((xœÑ, œÑ, iœÑ), (xœÑ, œÑ, iœÑ)) ‚â§1 by Assumption 2.1,
and the fact that a ‚â§
b1"
X,0.6322701688555347,"8 log(1 + œÉ‚àí2a), ‚àÄa ‚àà[0, 1],"
X,0.6341463414634146,"(d): œÉ‚Ä≤
œÑ‚àí1(x, i) denotes the posterior standard deviation of h(x, œÑ, i) inferred by observations
at XœÑ := {(xj, j, ij)}j<œÑ. Since {(xj, j, ij)}j=<œÑ ‚ää{(xj, j)}j<œÑ √ó I, then œÉœÑ‚àí1(xœÑ, iœÑ) ‚â§
œÉ‚Ä≤
œÑ‚àí1(xœÑ, iœÑ),"
X,0.6360225140712945,"(e): From [19, Lemma 5.3],"
X,0.6378986866791745,"(f): Definition of Œ≥h
k (12)."
X,0.6397748592870544,"Corollary C.2. Given b1 := 8/ log
 
1 + œÉ‚àí2
‚ààR, take Tk as the smallest positive integer satisfying
Tk
Œ≤k+Tk Œ≥h
k+Tk
‚â•b1"
X,0.6416510318949343,"œµ2 . Then, there exists k‚Ä≤ ‚àà(k, k + Tk], such that for any x ‚ààGk‚Ä≤ ‚à™Mk‚Ä≤, and for all"
X,0.6435272045028143,"i ‚ààI it holds that
wk‚Ä≤(x, i) ‚â§œµ."
X,0.6454033771106942,"C.2
Bounding the Expansion of the Safe Set"
X,0.6472795497185742,"All following lemmas hold for any Œ¥ ‚àà(0, 1), œµ > 0 and S0, such that ‚àÖ‚ääS0 ‚äÜX."
X,0.649155722326454,"To facilitate the theoretical analysis, we define ‚àÄx ‚ààX, ‚àÄi ‚ààI:"
X,0.651031894934334,"(
Àúlk(x, i) := max{Àúlk‚àí1(x, i), ¬µk‚àí1(x, i) ‚àíŒ≤1/2
k
œÉk‚àí1(x, i)},
k ‚â•1
Àúl0(x, i) := l0(x, i)
(19)"
X,0.6529080675422139,"Remember that, from (4), we can derive ‚àÄx ‚ààX, ‚àÄi ‚ààI:"
X,0.6547842401500938,"lk(x, i) = max{lk‚àí1(x, i) ‚àíL(t ‚àí1), ¬µk‚àí1(x, i) ‚àíŒ≤1/2
k
œÉk‚àí1(x, i)}
(20)"
X,0.6566604127579737,"Therefore, Àúlk can be viewed as updating lk with L(t) ‚â°0. With a slight abuse of notation, we omit
arguments x and i when not ambiguous."
X,0.6585365853658537,"Lemma C.3. The following holds for any k ‚â•1, ‚àÄx ‚ààX, ‚àÄi ‚ààI:"
X,0.6604127579737336,"(i) lk(x, i) ‚â•lk‚àí1(x, i) ‚àíL(t ‚àí1)"
X,0.6622889305816135,"(ii) Àúlk(x, i) ‚â•Àúlk‚àí1(x, i)"
X,0.6641651031894934,"(iii) lk(x, i) ‚â§Àúlk(x, i)"
X,0.6660412757973734,"(iv) Àúlk(x, i) ‚àí¬ØLt ‚â§lk(x, i)"
X,0.6679174484052532,Proof.
X,0.6697936210131332,(i) Direct consequence of (20).
X,0.6716697936210131,(ii) Direct consequence of (19).
X,0.6735459662288931,"(iii) We proceed by induction. Suppose lœÑ ‚â§ÀúlœÑ, then lœÑ ‚àíL(œÑ) ‚â§ÀúlœÑ, thus according to (20),
lœÑ+1 = max{lœÑ ‚àíL(œÑ), ¬µœÑ ‚àíŒ≤1/2
œÑ+1œÉœÑ} ‚â§max{ÀúlœÑ, ¬µœÑ ‚àíŒ≤1/2
œÑ+1œÉœÑ} = ÀúlœÑ+1, from which it
follows lk(x, i) ‚â§Àúlk(x, i)."
X,0.6754221388367729,"(iv) We proceed by induction. Suppose lœÑ ‚â•ÀúlœÑ ‚àí
œÑ‚àí1
P"
X,0.6772983114446529,"k=0
L(k)."
X,0.6791744840525328,"If ¬µœÑ ‚àíŒ≤1/2
œÑ+1œÉœÑ > ÀúlœÑ, then lœÑ+1
(20)
= ¬µœÑ ‚àíŒ≤1/2
œÑ+1œÉœÑ
(19)
= ÀúlœÑ+1 ‚â•ÀúlœÑ+1 ‚àí
œÑP"
X,0.6810506566604128,"k=0
L(k)."
X,0.6829268292682927,"If ¬µœÑ ‚àíŒ≤1/2
œÑ+1œÉœÑ < lœÑ ‚àíL(œÑ), then lœÑ+1
(20)
= lœÑ ‚àíL(œÑ) ‚â•ÀúlœÑ ‚àí
œÑ‚àí1
P"
X,0.6848030018761726,"k=0
L(k) ‚àíL(œÑ) ="
X,0.6866791744840526,"ÀúlœÑ+1 ‚àí
œÑP"
X,0.6885553470919324,"k=0
L(k)."
X,0.6904315196998124,"Otherwise, lœÑ+1
(20)
= ¬µœÑ ‚àíŒ≤1/2
œÑ+1œÉœÑ ‚â•lœÑ ‚àíL(œÑ) ‚â•ÀúlœÑ ‚àí
œÑ‚àí1
P"
X,0.6923076923076923,"k=0
L(k)‚àíL(œÑ) = ÀúlœÑ+1 ‚àí
œÑP"
X,0.6941838649155723,"k=0
L(k)."
X,0.6960600375234521,"To summarize, lk ‚â•Àúlk ‚àí
t‚àí1
P"
X,0.6979362101313321,"k=0
L(k) ‚â•Àúlk ‚àí¬ØLt"
X,0.699812382739212,"Lemma C.3 allows us to define auxiliary safe sets based on Àúlk such that they are contained in Sk.
Furthermore, due to the monotonicity of Àúlk, we can prove the auxiliary safe sets never shrink, which
will play a fundamental role in studying their convergence property and provide near-optimality
guarantee of TVSAFEOPT."
X,0.701688555347092,"Based on (19), we further define:"
X,0.7035647279549718,"Sk := {x ‚ààX | ‚àÄi ‚ààIc, ‚àÉx‚Ä≤
i ‚ààSt‚àí1, s.t. Àúlk(x‚Ä≤
i, i) ‚àíLxd(x, x‚Ä≤
i) ‚â•0}"
X,0.7054409005628518,"Sk := {x ‚ààX | ‚àÄi ‚ààIc, ‚àÉx‚Ä≤
i ‚ààSt‚àí1, s.t. Àúlk(x‚Ä≤
i, i) ‚àíLxd(x, x‚Ä≤
i) ‚àí¬ØLt ‚â•0}"
X,0.7073170731707317,"S0 = S0 = S0
Remember Sk = {x ‚ààX | ‚àÄi ‚ààIc, ‚àÉx‚Ä≤
i ‚ààSt‚àí1, s.t. lk(x‚Ä≤
i, i) ‚àíLxd(x, x‚Ä≤
i) ‚àíL(t) ‚â•0}. Thus,
Sk = Sk = Sk if and only if L(t) ‚â°0."
X,0.7091932457786116,"The following lemma proves that Sk never shrinks, and that Sk and Sk are a subset and a superset
for Sk, respectively.
Lemma C.4. The following holds for any t ‚â•1:"
X,0.7110694183864915,(i) St‚àí1 ‚äÜSk
X,0.7129455909943715,(ii) Sk ‚äÜSk ‚äÜSk
X,0.7148217636022514,Proof.
X,0.7166979362101313,"(i) We refer the reader to [8, Lemma 7.1]."
X,0.7185741088180112,(ii) We proceed by induction. Suppose SœÑ ‚äÜSœÑ ‚äÜSœÑ.
X,0.7204502814258912,"For all x ‚ààSœÑ+1, and for all i ‚ààIc, there exists x‚Ä≤
i ‚ààSœÑ ‚äÜSœÑ, s.t. ÀúlœÑ(x‚Ä≤
i, i)‚àíLxd(x, x‚Ä≤
i) ‚â•
lœÑ(x‚Ä≤
i, i) ‚àíLxd(x, x‚Ä≤
i) ‚àíL(œÑ) ‚â•0, hence x ‚ààSœÑ+1 as well. Therefore, SœÑ ‚äÜSœÑ ‚àÄœÑ."
X,0.7223264540337712,"For all x ‚ààSœÑ+1, and for all i ‚ààIc, there exists x‚Ä≤
i ‚ààSœÑ ‚äÜSœÑ, s.t. lœÑ(x‚Ä≤
i, i)‚àíLxd(x, x‚Ä≤
i)‚àí"
X,0.724202626641651,"L(œÑ) ‚â•ÀúlœÑ(x‚Ä≤
i, i) ‚àí
œÑ‚àí1
P"
X,0.726078799249531,"k=0
L(k) ‚àíLxd(x, x‚Ä≤
i) ‚àíL(œÑ) = ÀúlœÑ(x‚Ä≤
i, i) ‚àíLxd(x, x‚Ä≤
i) ‚àí
œÑP"
X,0.7279549718574109,"k=0
L(k) ‚â•"
X,0.7298311444652908,"ÀúlœÑ(x‚Ä≤
i, i) ‚àíLxd(x, x‚Ä≤
i) ‚àí¬ØLt ‚â•0, thus x ‚ààSœÑ+1. Therefore, SœÑ ‚äÜSœÑ, ‚àÄœÑ. From which we
conclude SœÑ ‚äÜSœÑ ‚äÜSœÑ, ‚àÄœÑ."
X,0.7317073170731707,"Note: Where needed in the following lemmas, we assume b1 and Tk are defined as in Lemma C.1
and Corollary C.2
Lemma C.5 (Lemma 7.4 in [8]). For any k ‚â•1, a > 0, if ¬ØRa (S0) \Sk Ã∏= ‚àÖ, then Ra (Sk) \Sk Ã∏= ‚àÖ."
X,0.7335834896810507,"The following lemma provides a sufficient condition for the expansion of the auxiliary safe set Sk.
Lemma C.6. For any t ‚â•1, if ¬ØR¬ØLt+œµ(S0)\Sk Ã∏= ‚àÖ, then, with probability at least 1 ‚àíŒ¥, it holds
that Sk+Tk ‚äãSk."
X,0.7354596622889306,"Proof.
Similar to the proof of [8, Lemma 7.5]."
X,0.7373358348968105,"By Lemma C.5, we get that, R¬ØLt+œµ (Sk) \Sk Ã∏= ‚àÖ. Equivalently, ‚àÉx ‚ààR¬ØLt+œµ (Sk) \Sk which
implies that, for all i ‚ààIc,"
X,0.7392120075046904,"‚àÉzi ‚ààSk : ¬Øci(zi) ‚àíLxd(zi, x) ‚àí¬ØLt ‚àíœµ ‚â•0"
X,0.7410881801125704,"Now assume, to the contrary, that Sk+Tk = Sk. Thus, ‚àÄk‚Ä≤ ‚àà(k, k + Tk], x ‚ààD\Sk‚Ä≤, and ‚àÄi ‚ààIc,
zi ‚ààSk‚Ä≤."
X,0.7429643527204502,"uk‚Ä≤(zi, i) ‚àíLxd(zi, x) ‚àíL(k‚Ä≤)"
X,0.7448405253283302,"‚â•¬Øci(zi) ‚àíLxd(zi, x) ‚àíL(k‚Ä≤)
by Lemma B.1"
X,0.7467166979362101,"‚â•¬Øci(zi) ‚àíLxd(zi, x) ‚àí¬ØLt ‚àíœµ
‚â•0"
X,0.7485928705440901,"Therefore, by definition (8), ek‚Ä≤(zi) > 0, which implies zi ‚ààGk‚Ä≤, ‚àÄk‚Ä≤ ‚àà(k, k + Tk], ‚àÄi ‚ààIc."
X,0.7504690431519699,"Therefore, we know that there exists k‚Ä≤ ‚àà(k, k + Tk], for all i ‚ààIc, wk‚Ä≤(zi, i) ‚â§œµ. (Corollary C.2)
Hence, for all i ‚ààIc,
Àúlk‚Ä≤(zi, i) ‚àíLxd(zi, x)
‚â•¬Øci(zi) ‚àíwk‚Ä≤(zi, i) ‚àíLxd(zi, x)
by Lemma B.1
‚â•¬Øci(zi) ‚àíœµ ‚àíLxd(zi, x)"
X,0.7523452157598499,"‚â•¬ØLt
This means x ‚ààSk‚Ä≤ = Sk, which leads to a contradiction."
X,0.7542213883677298,"The following lemma gives a superset for the auxiliary safe set Sk.
Lemma C.7. Sk ‚äÜ¬ØR¬ØLt(S0) with probability at least 1 ‚àíŒ¥."
X,0.7560975609756098,"Proof by induction.
S0 = S0 ‚äÜ¬ØR¬ØLt(S0)"
X,0.7579737335834896,Suppose SœÑ ‚äÜ¬ØR¬ØLt(S0).
X,0.7598499061913696,"For all x ‚ààSœÑ+1 and for all i ‚ààIc there exists x‚Ä≤
i ‚ààSœÑ, s.t. ¬Øci (x‚Ä≤
i) ‚àíLxd(x, x‚Ä≤
i) ‚àí¬ØLt"
X,0.7617260787992496,"(a)
‚â•
Àúlk(x‚Ä≤
i, i) ‚àíLxd(x, x‚Ä≤
i) ‚àí¬ØLt ‚â•0."
X,0.7636022514071295,(a): Lemma B.1.
X,0.7654784240150094,"Thus, SœÑ+1 ‚äÜR¬ØLt(SœÑ) ‚äÜ¬ØR¬ØLt(S0)"
X,0.7673545966228893,"Lemma C.8 (Lemma 7.8 in [8]). Let k‚àóbe the smallest integer, such that k‚àó‚â•
 ¬ØR¬ØLt (S0)
 Tk‚àó.
Then, there exists k0 ‚â§k‚àó, such that Sk0+Tk0 = Sk0."
X,0.7692307692307693,"Lemma C.8 together with Lemma C.6, and Lemma C.7 entail convergence of Sk within k‚àótime
steps, which ultimately leads us to the near-optimality of TVSAFEOPT when the problem becomes
stationary.
Lemma C.9. For any k ‚â•1, if Sk+Tk = Sk, then, with probability at least 1 ‚àíŒ¥, there exists
k‚Ä≤ ‚àà(k, k + Tk] such that
¬Øf (ÀÜxk‚Ä≤) ‚â•
max
x‚àà¬Ø
R ¬Ø
Lt+œµ(S0)
¬Øf(x) ‚àíœµ."
X,0.7711069418386491,Proof.
X,0.7729831144465291,"Let x‚àó
k‚Ä≤ := arg max
x‚ààSk‚Ä≤
¬Øf(x). Note that x‚àó
k‚Ä≤ ‚ààMk‚Ä≤, since"
X,0.774859287054409,"uk‚Ä≤(x‚àó
k‚Ä≤, 0)
(a)
‚â•¬Øf(x‚àó
k‚Ä≤)"
X,0.776735459662289,‚â•¬Øf(ÀÜxk‚Ä≤)
X,0.7786116322701688,"(b)
‚â•lk‚Ä≤(ÀÜxk‚Ä≤, 0)"
X,0.7804878048780488,"(c)
‚â•max
x‚ààSk‚Ä≤ lk‚Ä≤(x, 0)"
X,0.7823639774859287,"(a) and (b): Lemma B.1,"
X,0.7842401500938087,(c): Definition of ÀÜxk (11).
X,0.7861163227016885,"We will first show that ‚àÉk‚Ä≤ ‚àà(k, k + Tk], s.t. ¬Øf(ÀÜxk‚Ä≤) ‚â•¬Øf(x‚àó
k‚Ä≤) ‚àíœµ. Assume, to the contrary, that"
X,0.7879924953095685,"‚àÄk‚Ä≤ ‚àà(k, k + Tk], ¬Øf(ÀÜxk‚Ä≤) < ¬Øf(x‚àó
k‚Ä≤) ‚àíœµ
Then, we have, ‚àÉk‚Ä≤ ‚àà(k, k + Tk]"
X,0.7898686679174484,"lt‚Ä≤(x‚àó
k‚Ä≤, 0)"
X,0.7917448405253283,"(d)
‚â§lk‚Ä≤(ÀÜxk‚Ä≤, 0)"
X,0.7936210131332082,"(e)
‚â§¬Øf(ÀÜxk‚Ä≤)"
X,0.7954971857410882,"< ¬Øf(x‚àó
k‚Ä≤) ‚àíœµ"
X,0.797373358348968,"(f)
‚â§lk‚Ä≤(x‚àó
k‚Ä≤, 0),
which is a contradiction."
X,0.799249530956848,"(d): Definition of ÀÜxk (11),"
X,0.801125703564728,"(e): Lemma B.1,"
X,0.8030018761726079,"(f): Corollary C.2, and x‚àó
k‚Ä≤ ‚ààMk‚Ä≤"
X,0.8048780487804879,"Finally, ¬ØR¬ØLt+œµ (S0) ‚äÜSk‚Ä≤ ‚äÜSk‚Ä≤, by Lemma C.6 and Lemma C.4 (ii). Therefore, ‚àÉk‚Ä≤ ‚àà(k, k + Tk]
such that
max
x‚àà¬Ø
R ¬Ø
Lt+œµ(S0)
¬Øf(x) ‚àíœµ ‚â§max
x‚ààSk‚Ä≤
¬Øf(x) ‚àíœµ"
X,0.8067542213883677,"= ¬Øf(x‚àó
k‚Ä≤) ‚àíœµ"
X,0.8086303939962477,‚â§¬Øf(ÀÜxk‚Ä≤)
X,0.8105065666041276,"C.3
Near-Optimality Proof"
X,0.8123827392120075,"Proof of Theorem 2.6. Theorem 2.6 is a direct consequence of Corollary C.2, Lemma C.8, and
Lemma C.9."
X,0.8142589118198874,"D
Practical Considerations"
X,0.8161350844277674,"D.1
Trade-off between Safety and Optimality"
X,0.8180112570356473,"In this work, we focus on safety critical systems where satisfying the safety constraints has highest
priority over finding the optima. Through pessimistically considering change with time in the decision-
making process, TVSAFEOPT emphasizes safety in non-stationary conditions at the inevitable expense
of optimality. In practice, such sacrifice on optimality can be alleviated by tighter bound of rate of
change."
X,0.8198874296435272,"Besides, in the case where safety can be to some extent comprised, which is beyond the focus of this
work, constrained BO and its time-varying extension would be a more suitable method to apply. We
refer the readers to [50, 51] for further information."
X,0.8217636022514071,"D.2
Scalability"
X,0.8236397748592871,"Explicit considering time can be viewed roughly as adding dimension by 1, Therefore, TVSAFEOPT
achieves time adaptation without adding much computational cost. With the increase in dimensionality
of the problem, safety constraints might arise across multiple dimensions, from multiple directions at
the price of optimality. As our approach is suitable for safety critical conditions, the focus is put on
maintaining safety under change, therefore safety considerations ‚Äúdictate‚Äù the optima. Additionally,
in practice, the safety functions are modeled independently with a GP, and thus the computational
cost scales linearly with the number of constraints."
X,0.8255159474671669,NeurIPS Paper Checklist
CLAIMS,0.8273921200750469,1. Claims
CLAIMS,0.8292682926829268,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?"
CLAIMS,0.8311444652908068,Answer: [Yes]
CLAIMS,0.8330206378986866,"Justification: The paper set out to solve the problem of optimizing an unknown time-varying
reward subject to unknown time-varying safety constraints, as stated in the Abstract and
the Introduction (Section 1). In a partial fulfilment of this goal, the paper develops a safe
learning algorithm and provides safety guarantees for a general time-varying case, and
near-optimality guarantees for when the time-varying problem becomes stationary."
CLAIMS,0.8348968105065666,Guidelines:
CLAIMS,0.8367729831144465,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.8386491557223265,2. Limitations
LIMITATIONS,0.8405253283302064,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8424015009380863,Answer: [Yes]
LIMITATIONS,0.8442776735459663,"Justification: The paper contains a separate section on limitations, see Section 4"
LIMITATIONS,0.8461538461538461,Guidelines:
LIMITATIONS,0.8480300187617261,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.849906191369606,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.851782363977486,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.8536585365853658,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.8555347091932458,"Justification: The paper provides the necessary assumptions in Section 2.1, the definitions in
Section 2, and proofs in Appendices B and C."
THEORY ASSUMPTIONS AND PROOFS,0.8574108818011257,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8592870544090057,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8611632270168855,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8630393996247655,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8649155722326454,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8667917448405253,"Justification: The paper provides the necessary information in Section 3 and the background
details are in Appendix A. In particular, the modifications necessary for practical imple-
mentation of the proposed algorithm, for example regarding the Lipschitz constants, are
provided in Appendix A.1 and the hyperparameters for the examples are in Appendix A.2
and Appendix A.3."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8686679174484052,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8705440900562852,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset)."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8724202626641651,"(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.874296435272045,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8761726078799249,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.8780487804878049,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8799249530956847,"Justification: The paper provides the details necessary to reproduce the simulational results
in Appendix A. The code accompanying the paper is currently under review and will appear
shortly at https://www.research-collection.ethz.ch/."
OPEN ACCESS TO DATA AND CODE,0.8818011257035647,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8836772983114447,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.8855534709193246,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.8874296435272045,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8893058161350844,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8911819887429644,"Justification: The paper provides the necessary information in Section 3 and the background
details are in Appendix A. In particular, the modifications necessary for practical imple-
mentation of the proposed algorithm, for example regarding the Lipschitz constants, are
provided in Appendix A.1 and the hyperparameters for the examples are in Appendix A.2
and Appendix A.3."
OPEN ACCESS TO DATA AND CODE,0.8930581613508443,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8949343339587242,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8968105065666041,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8986866791744841,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.900562851782364,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9024390243902439,"Justification: The main contribution of the paper is in theoretical guarantees for the proposed
algorithm. The only source of uncertainty in the experiments is in assumed measurement
noise, with the standard deviation 0.01, making error bars negligible, as stated in respective
experimental sections."
EXPERIMENTS COMPUTE RESOURCES,0.9043151969981238,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9061913696060038,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9080675422138836,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9099437148217636,"Justification: The paper provides the necessary information in Section 3 and the background
details are in Appendix A."
EXPERIMENTS COMPUTE RESOURCES,0.9118198874296435,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9136960600375235,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper)."
CODE OF ETHICS,0.9155722326454033,9. Code Of Ethics
CODE OF ETHICS,0.9174484052532833,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9193245778611632,Answer: [Yes]
CODE OF ETHICS,0.9212007504690432,"Justification: The paper uses neither external datasets nor human subjects, and no data
have been collected for the paper. The algorithm proposed in the paper is an optimization
algorithm and thus does not introduce additional biases or privacy and dual-use concerns.
The necessary licences for fair use have been provided in Appendix A.3."
CODE OF ETHICS,0.9230769230769231,Guidelines:
CODE OF ETHICS,0.924953095684803,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.926829268292683,10. Broader Impacts
BROADER IMPACTS,0.9287054409005628,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9305816135084428,Answer: [Yes]
BROADER IMPACTS,0.9324577861163227,"Justification: The paper states the need for safe optimization algorithm with potential
applications in the Introduction (Section 1, in particular Section 1.2.1).
The algorithm proposed in the paper is an optimization algorithm and thus does not introduce
additional biases or privacy and dual-use concerns. The algorithm does not use external
datasets, and as such does not require privacy or security considerations. Neither does the
algorithm generate data that can be used for disinformation or discrimination."
BROADER IMPACTS,0.9343339587242027,Guidelines:
BROADER IMPACTS,0.9362101313320825,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact."
BROADER IMPACTS,0.9380863039399625,"‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9399624765478424,11. Safeguards
SAFEGUARDS,0.9418386491557224,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9437148217636022,Answer: [Yes]
SAFEGUARDS,0.9455909943714822,"Justification: The paper does not contain data of high risk for misuse. The necessary licences
for fair use have been provided in Appendix A.3."
SAFEGUARDS,0.9474671669793621,Guidelines:
SAFEGUARDS,0.949343339587242,"‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9512195121951219,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9530956848030019,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9549718574108818,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9568480300187617,"Justification: All previously existing results used in the paper have been cited in References,
including the necessary licensing information for the data for the compressor case study in
Appendix A.3."
LICENSES FOR EXISTING ASSETS,0.9587242026266416,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9606003752345216,"‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided."
LICENSES FOR EXISTING ASSETS,0.9624765478424016,"‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators."
NEW ASSETS,0.9643527204502814,13. New Assets
NEW ASSETS,0.9662288930581614,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9681050656660413,Answer: [NA]
NEW ASSETS,0.9699812382739212,Justification: The paper does not introduce additional assets.
NEW ASSETS,0.9718574108818011,Guidelines:
NEW ASSETS,0.9737335834896811,"‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975609756097561,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9774859287054409,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793621013133208,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9812382739212008,Justification: The paper uses neither crowdsourcing nor human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831144465290806,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9849906191369606,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868667917448405,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9887429643527205,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9906191369606003,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924953095684803,Justification: The paper does not use human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9943714821763602,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9962476547842402,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.99812382739212,"‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
