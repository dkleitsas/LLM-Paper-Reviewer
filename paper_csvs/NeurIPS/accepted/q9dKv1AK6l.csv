Section,Section Appearance Order,Paragraph
GOOGLE DEEPMIND,0.0,"1Google DeepMind
2Google Research
3Georgia Institute of Technology
4University of Alberta
5Simon Fraser University
6Indian Institute of Science"
GOOGLE DEEPMIND,0.001034126163391934,"{jcmei,bodai,alekhagarwal,szepi,schuurmans}@google.com
vaswani.sharan@gmail.com
anantraj@iisc.ac.in"
ABSTRACT,0.002068252326783868,Abstract
ABSTRACT,0.0031023784901758012,"We provide a new understanding of the stochastic gradient bandit algorithm by
showing that it converges to a globally optimal policy almost surely using any con-
stant learning rate. This result demonstrates that the stochastic gradient algorithm
continues to balance exploration and exploitation appropriately even in scenarios
where standard smoothness and noise control assumptions break down. The proofs
are based on novel findings about action sampling rates and the relationship be-
tween cumulative progress and noise, and extend the current understanding of how
simple stochastic gradient methods behave in bandit settings."
INTRODUCTION,0.004136504653567736,"1
Introduction"
INTRODUCTION,0.005170630816959669,"The stochastic gradient method has been ubiquitous in the field of machine learning for decades [4].
When applied to reinforcement learning (RL), a representative instantiation of stochastic gradient
is the well known policy gradient [32] (or REINFORCE [34]) algorithm, where in each iteration
an online sample is gathered using the current policy, from which a gradient estimate is obtained
to conduct parameter updates. In the simplest setting of a stochastic bandit problem [16], where
decisions matter only for one step, the REINFORCE policy gradient method becomes equivalent to
the stochastic gradient bandit algorithm [31, Section 2.8]. Compared to other statistical methods,
such as the upper confidence bound algorithm (UCB, [14, 3]), and Thompson sampling (TS, [33, 2]),
the stochastic gradient bandit algorithm is conceptually simpler and more computationally efficient,
as it does not calculate exploration bonuses nor posterior distributions. Moreover, the stochastic
gradient method is highly scalable and naturally applicable to large scale neural networks [29, 30]."
INTRODUCTION,0.0062047569803516025,"However, unlike UCB or TS, the stochastic gradient bandit algorithm does not have an equivalently
well established and comprehensive theoretical footing. Given its pervasive success and widespread
application in RL [30] and fine-tuning for large language models [26, 27]), it remains an important
question to understand the success of stochastic gradient based algorithms in bandit-like settings, not
only to bridge the gap between theory and practice, but also to identify more effective and robust
variants. In this paper, we make a significant contribution to the theoretical understanding of the
stochastic gradient bandit algorithm, bringing its justification closer to that of other less scalable but
theoretically well established methods. In particular, we establish the surprising result that:"
INTRODUCTION,0.007238883143743537,"For any constant learning rate η > 0, the stochastic gradient bandit algorithm is guaranteed to
converge to the globally optimal policy almost surely."
INTRODUCTION,0.008273009307135471,"Since learning rate is the only tuning parameter in the stochastic gradient bandit algorithm, this result
offers a remarkable robustness for the method, that it converges to a near optimal policy, irrespective
of the value of this hyperparameter! Analysis of this algorithm is challenging because it requires
techniques for simultaneously handling non-convex optimization, stochastic approximation, and the
exploration-exploitation trade-off. Prior theoretical work on the stochastic gradient algorithm has
primarily focused on non-convex optimization and stochastic approximation, but understanding the
simultaneous effect on exploration has been largely lacking."
INTRODUCTION,0.009307135470527405,"Recently, significant progress has been made in establishing global convergence results for policy
gradient (PG) methods. For example, it has been shown that using exact gradients, Softmax PG
converges to a globally optimal policy asymptotically as the number of iterations t goes to infinity
[1]. Subsequent work has demonstrated that the asymptotic rate of convergence is O(1/t)[23], albeit
with problem and initialization dependent constants [22, 17]. The rate and constant dependence in
the true gradient setting have been improved via several techniques, including entropy regularization
[23], normalization [21], and using natural gradient (mirror descent) [1, 6, 15]."
INTRODUCTION,0.010341261633919338,"Unfortunately, in the online stochastic setting, where the policy gradient has to be estimated using
the current policy to collect samples, these accelerated methods all obtain worse asymptotic results
than the standard Softmax PG [20], failing to converge to a global optimum without careful design
choices [19]. Yet in the same setting, standard Softmax PG has been shown to succeed in its simplest
form, provided only that a sufficiently small learning constant rate η ∈Θ(1) is used [24]."
INTRODUCTION,0.011375387797311272,"For stochastic gradient based methods, decaying or sufficiently small learning rates are used by almost
all current approaches, motivated by classical convergence analyses from stochastic optimization
[28, 12, 39, 38, 9, 37, 36, 24, 8]. Stationary point convergence is guaranteed for learning rates
sufficiently small with respect to the smoothness of the objective function, while also decaying to
zero at a precise rate if noise in the gradient estimator persists. In addition to appropriate learning rate
control, many other techniques have been developed to control the effects of gradient noise, including
regularization [38, 9], variance reduction [37], and carefully considering growth conditions [36, 24]."
INTRODUCTION,0.012409513960703205,"The technical challenges we face in the current study can be understood in the following aspects:
(1) Using an arbitrarily large constant learning rate for online stochastic gradient optimization
immediately renders the smoothness and noise control techniques mentioned above inapplicable.
(2) With any constant learning rate η > 0, the question of whether oscillation or convergence will
ultimately occur needs to be addressed before even considering whether any convergence is to a
global optimum. This additional level of complexity arises because the optimization objective is not
necessarily improved monotonically in expectation. Finally, (3) The gradient bandit algorithm does
not use any exploration bonus, which means that new techniques are required to demonstrate that it
adequately balances the exploration-exploitation trade-off."
INTRODUCTION,0.01344364012409514,"In this paper, we resolve the above difficulties by uncovering intriguing exploration properties of
stochastic gradient when using any constant learning rate. In particular, we establish the following."
INTRODUCTION,0.014477766287487074,"(i) In the stochastic online setting, with probability 1, the stochastic gradient bandit algorithm will
not keep sampling any single action forever, implying that it will exhibit a minimal form of
exploration without any further modification. This asymptotic event (as t →∞) happens with
probability 1 and holds for any constant learning rate η > 0.
(ii) This result can then be leveraged to show that, as a consequence, given any constant learning rate,
the stochastic gradient bandit algorithms will converge to the globally optimal policy as t →∞,
with probability 1. That is, the probability of sub-optimal actions decays to 0, even though some
of them are taken infinitely often asymptotically."
SETTING AND BACKGROUND,0.015511892450879007,"2
Setting and Background"
SETTING AND BACKGROUND,0.016546018614270942,"We consider the stochastic multi-armed bandit problem [16], specified by K actions and a true mean
reward vector r ∈RK, where for each action a ∈[K] := {1, 2, . . . , K},"
SETTING AND BACKGROUND,0.017580144777662874,"r(a) =
Z Rmax"
SETTING AND BACKGROUND,0.01861427094105481,"−Rmax
x · Pa(x)µ(dx),
(1)"
SETTING AND BACKGROUND,0.01964839710444674,"where Rmax > 0 is the reward range, µ is a finite measure over [−Rmax, Rmax], and Pa(x) ≥0 is
the probability density function with respect to µ. We use Ra to denote the reward distribution for"
SETTING AND BACKGROUND,0.020682523267838676,Algorithm 1 Gradient bandit algorithm (without baselines)
SETTING AND BACKGROUND,0.02171664943123061,"Input: initial parameters θ1 ∈RK, learning rate η > 0.
Output: policies πθt = softmax(θt).
while t ≥1 do"
SETTING AND BACKGROUND,0.022750775594622543,"Sample an action at ∼πθt(·) and observe reward Rt(at) ∼Pat.
for all a ∈[K] do"
SETTING AND BACKGROUND,0.023784901758014478,if a = at then
SETTING AND BACKGROUND,0.02481902792140641,"θt+1(a) ←θt(a) + η · (1 −πθt(a)) · Rt(at).
else"
SETTING AND BACKGROUND,0.025853154084798345,"θt+1(a) ←θt(a) −η · πθt(a) · Rt(at).
end if
end for
end while"
SETTING AND BACKGROUND,0.02688728024819028,"action a defined by the density Pa and base measure µ. The goal is to find a policy πθ ∈[0, 1]K to
achieve high expected reward,"
SETTING AND BACKGROUND,0.027921406411582212,"max
θ∈RK π⊤
θ r,
(2)"
SETTING AND BACKGROUND,0.028955532574974147,where πθ is parameterized by θ ∈RK.
SETTING AND BACKGROUND,0.02998965873836608,"The gradient bandit algorithm. A natural idea to optimize Eq. (2) is to use stochastic gradient
ascent, which is shown in Algorithm 1 and known as the gradient bandit algorithm [31, Section 2.8].
In Algorithm 1, in each iteration t ≥1, the probability of pulling arm a ∈[K] is given as"
SETTING AND BACKGROUND,0.031023784901758014,"πθt(a) = [softmax(θt)](a) :=
exp{θt(a)}
P"
SETTING AND BACKGROUND,0.03205791106514995,"a′∈[K] exp{θt(a′)},
for all a ∈[K],
(3)"
SETTING AND BACKGROUND,0.033092037228541885,"where θt ∈RK is the parameter vector to be updated. The following proposition shows that
Algorithm 1 is an instance of stochastic gradient ascent with an unbiased gradient estimator [31, 24]."
SETTING AND BACKGROUND,0.03412616339193381,"Proposition 1 (Proposition 2.3 of [24]). Algorithm 1 is equivalent to the following update,"
SETTING AND BACKGROUND,0.03516028955532575,"θt+1 ←θt + η · d π⊤
θtˆrt
dθt
= θt + η ·
 
diag(πθt) −πθtπ⊤
θt

ˆrt,
(4)"
SETTING AND BACKGROUND,0.03619441571871768,"where Et
h d π⊤
θt ˆrt
dθt"
SETTING AND BACKGROUND,0.03722854188210962,"i
=
d π⊤
θtr
dθt , and Et[·] is defined with respect to randomness from on-policy sampling"
SETTING AND BACKGROUND,0.038262668045501554,"at ∼πθt(·) and reward sampling Rt(at) ∼Pat. The Jacobian of θ 7→πθ := softmax(θ) is
  d πθ"
SETTING AND BACKGROUND,0.03929679420889348,"dθ
⊤= diag(πθ) −πθπ⊤
θ
∈RK×K, and ˆrt(a) :=
I{at=a}"
SETTING AND BACKGROUND,0.04033092037228542,"πθt(a) · Rt(a) for all a ∈[K] is the
importance sampling (IS) estimator, and we set Rt(a) = 0 for all a ̸= at."
SETTING AND BACKGROUND,0.04136504653567735,"Known results on the convergence of the gradient bandit algorithm. Since Eq. (2) corresponds to
a smooth non-concave maximization problem over θ ∈RK [23], using Algorithm 1 with decaying
learning rates is sufficient to guarantee convergence to a stationary point [28, 25, 12, 39]. However,
this is insufficient to ensure the globally optimal solution of Eq. (2) is reached, since there exist
multiple stationary points. More recently, guarantees of convergence to a globally optimal policy have
been developed for PG methods in the true gradient setting [1, 23], where the algorithm has access
to exact mean rewards. These results were later extended to achieve global convergence guarantees
(almost surely) in the stochastic setting [38, 9, 37, 36, 24, 18]. However, these extended results
have required decaying or sufficiently small learning rates, motivated by exploiting smoothness and
combating the inherent noise in stochastic gradients."
SETTING AND BACKGROUND,0.04239917269906929,"Despite these previous assumptions, there exists empirical and theoretical evidence that using a
large learning rate in the stochastic gradient bandit algorithm is a viable option. For example, it
has been observed that softmax policies learn even with extremely large learning rates such as 214
[11]. For logistic regression on linearly separable data, the objective has an exponential tail and the
minimizer is unbounded, yet it has been shown that gradient descent with iteration dependent learning
rate η ∈Θ(t) achieves accelerated O(1/t2) convergence [35]. Though the objective in Eq. (2) has"
SETTING AND BACKGROUND,0.04343329886246122,"similar properties, unlike logistic regression, the problem we are considering is non-concave, so the
same techniques cannot be directly applied. The most related results are from [24], which proved
that with a small problem specific constant learning rate, Algorithm 1 achieves convergence to a
globally optimal policy almost surely. However, as mentioned, the learning rate choices in [24] rely
on assumptions of (non-uniform) smoothness and noise growth conditions (their Lemmas 4.2, 4.3,
and 4.6), which cannot be directly applied here for a large learning rate."
SETTING AND BACKGROUND,0.04446742502585315,"Consequently, the use of large learning rates appear to render existing results and techniques inapplica-
ble. Furthermore, with a large constant learning rate, it is unclear whether Algorithm 1 will converge
to any stationary point, or the iterates will keep oscillating. If the algorithm does converge, it is also
not clear what effect large step-sizes have on exploration, and whether the algorithm will converge to
the optimal arm in such cases. Resolving these questions requires new results that characterize the
behavior of Algorithm 1, since the classical optimization and stochastic approximation convergence
theories are no longer applicable, as explained."
ASYMPTOTIC GLOBAL CONVERGENCE OF GRADIENT BANDIT ALGORITHM,0.045501551189245086,"3
Asymptotic Global Convergence of Gradient Bandit Algorithm"
ASYMPTOTIC GLOBAL CONVERGENCE OF GRADIENT BANDIT ALGORITHM,0.04653567735263702,"We have seen that solving the non-concave maximization problem Eq. (2) using Algorithm 1 with any
constant (potentially large) learning rate requires ideas beyond classical optimization theory. Here,
we take a different perspective to investigate how Algorithm 1 samples actions. For analysis, we
make the following assumption about the reward distribution."
ASYMPTOTIC GLOBAL CONVERGENCE OF GRADIENT BANDIT ALGORITHM,0.047569803516028956,"Assumption 1 (True mean reward has no ties). For all i, j ∈[K], if i ̸= j, then r(i) ̸= r(j)."
ASYMPTOTIC GLOBAL CONVERGENCE OF GRADIENT BANDIT ALGORITHM,0.04860392967942089,"Remark 1. Removing Assumption 1 remains an open question for future work, while we believe that
Algorithm 1 works without Assumption 1. One piece of evidence to support this conjecture is that
even in the exact gradient setting, the set of initializations where Softmax PG approaches non-strict
one-hot policies has zero measure."
FAILURE MODE OF AGGRESSIVE UPDATES,0.04963805584281282,"3.1
Failure Mode of Aggressive Updates"
FAILURE MODE OF AGGRESSIVE UPDATES,0.050672182006204755,"It has been observed that several accelerated PG methods in the true gradient setting, including natural
PG [13, 1] and normalized PG [21], obtain worse results than standard softmax PG if combined with
online sampling at ∼πθt(·) using constant learning rates [20]. The failure mode in these cases is
that the update is too aggressive and commits to a sub-optimal arm without sufficiently exploring
all arms. This results in a non-trivial probability of sampling one action forever, i.e., there exists
a potentially sub-optimal action a ∈[K], such that with some constant probability, at = a for all
t ≥1. Such an outcome implies that πθt(a) →1 as t →∞[20, Theorem 3]. Since a ∈[K] could
be a sub-optimal action with r(a) < r(a∗) = maxa∈[K] r(a), this results in a lack of exploration,
and consequently, methods such as natural PG and normalized PG are not guaranteed to converge to
the optimal action a∗:= arg maxa∈[K] r(a) with probability 1."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.05170630816959669,"3.2
Stochastic Gradient Automatically Avoids Lack of Exploration"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.052740434332988625,"Our first key finding is that Algorithm 1 does not keep sampling one action forever, no matter how
large the constant learning rate is. This property avoids the problem of a lack of exploration, in the
sense that Algorithm 1 will at least explore more than one action infinitely often. At first glance, this
might not seem like a strong property, since the algorithm might somehow explore only sub-optimal
actions forever. However, we will argue below that this property coupled with additional arguments
is sufficient to guarantee convergence to the globally optimal policy."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.05377456049638056,"Let us now formally prove the above property. By Algorithm 1, for all a ∈[K], for all t ≥1,"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.05480868665977249,"θt+1(a) ←θt(a) +
 η · (1 −πθt(a)) · Rt(a),
if at = a,
−η · πθt(a) · Rt(at),
otherwise."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.055842812823164424,"(5)
(6)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.05687693898655636,"We define Nt(a) as the number of times action a ∈[K] is sampled up to iteration t ≥1, i.e.,"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.057911065149948295,"Nt(a) := t
X"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.05894519131334023,"s=1
I {as = a},
(7)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.05997931747673216,"and its asymptotic limit N∞(a) := limt→∞Nt(a), which could possibly be infinity. For all a ∈[K],
we have either N∞(a) = ∞or N∞(a) < ∞, meaning that a ∈[K] is sampled infinitely often or
only finitely many times asymptotically. First, we prove the following Lemma 1, which shows that if
an action a ∈[K] is sampled only finitely many times as t →∞, then the parameter corresponding
to action a is also finite, i.e., supt≥1 |θt(a)| < ∞."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.06101344364012409,"Lemma 1. Using Algorithm 1 with any constant η ∈Θ(1), if N∞(a) < ∞for an action a ∈[K],
then we have, almost surely,"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.06204756980351603,"sup
t≥1
θt(a) < ∞, and inf
t≥1 θt(a) > −∞.
(8)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.06308169596690796,Lemma 1 will be used multiple times in the subsequent convergence arguments.
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.0641158221302999,"Proof sketch.
Since we assume action a ∈[K] is sampled finitely many times, the update given
in the case depicted by Eq. (5) happens finitely many times. Each update is bounded since the
sampled reward is in [−Rmax, Rmax] by Eq. (1), and the learning rate is a constant, i.e., η ∈Θ(1).
In Algorithm 1, θt(a) is still updated even when at ̸= a, with the corresponding update given
by the case depicted by Eq. (6). Therefore, whether θt(a) is bounded depends on the cumulative
probability Pt
s=1 πθs(a) being summable as t →∞. According to the extended Borel-Cantelli
lemma (Lemma 3), we have, almost surely,
n X"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.06514994829369183,"t≥1
πθt(a) = ∞
o
= {N∞(a) = ∞} ,
(9)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.06618407445708377,which implies (by taking complements) that P
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.0672182006204757,"t≥1 πθt(a) < ∞if and only if N∞(a) < ∞. There-
fore, if a ∈[K] is sampled finitely often, θt(a) will be updated in a bounded manner (using Eqs. (5)
and (6)) as t →∞, hence establishing Lemma 1. Detailed proofs for this lemma, as well as for all
other results in this paper can be found in the appendix."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.06825232678386763,"Given Lemma 1, we can then establish the above-mentioned finding about the exploration effect
of Algorithm 1 in Lemma 2.
Lemma 2 (Avoiding a lack of exploration). Using Algorithm 1 with any η ∈Θ(1), there exists at
least a pair of distinct actions i, j ∈[K] and i ̸= j, such that, almost surely,"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.06928645294725956,"N∞(i) = ∞, and N∞(j) = ∞.
(10)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.0703205791106515,"Proof sketch.
The argument for the existence of one such action is straightforward, since by the
pigeonhole principle, if there are finitely many actions, i.e., K < ∞, there must be at least one action
i ∈[K] that is sampled infinitely often as t →∞."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.07135470527404343,"The argument for the existence of a second such action is by contradiction. Suppose that all the
other actions j ∈[K] with j ̸= i are sampled only finitely many times as t →∞. According
to Lemma 1, their corresponding parameters must remain finite, i.e., supt≥1 |θt(j)| < ∞for all
j ∈[K] with j ̸= i. Now consider θt(i). By assumption, the second update case for this parameter,
Eq. (6), happens only finitely often, since Eq. (6) can only occur when at ̸= i. Therefore, the key
question is whether the cumulative probability Pt
s=1 (1 −πθs(i)) involved in the first case of the
update, Eq. (5), is summable as t →∞. Note that Pt
s=1 (1 −πθs(i)) = Pt
s=1
P"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.07238883143743537,"j̸=i πθs(j), which
is indeed summable as t →∞, by the assumption and Eq. (9). This implies that action i, which is
sampled infinitely often, achieves a parameter magnitude, supt≥1 |θt(i)| < ∞, that remains bounded
as t →∞. Using the softmax parameterization Eq. (3) in the above argument, we conclude that for
all a ∈[K], inft≥1 πθt(a) > 0, i.e., every action’s probability remains bounded away from zero, and
hence is not summable. Using Eq. (9), this implies that every action is sampled infinitely often, which
contradicts the assumption that only action i is sampled infinitely often as t →∞."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.0734229576008273,"Discussion.
Lemma 2 implies that Algorithm 1 is not an aggressive method in the sense of [20],
no matter how large the learning rate is, as long as it is constant, i.e., η ∈Θ(1). According to [20,
Theorem 7], even if we fix the sampling in Algorithm 1 to a sub-optimal action a ∈[K] forever, i.e.,
at = a for all t ≥1, its probability will not approach 1 faster than O(1/t), i.e., 1 −πθt(a) ∈Ω(1/t).
This means that there must be at least one another action a′ ∈[K] with a′ ̸= a, such that a′ will also
be sampled infinitely often. A more intuitive explanation is that the (1 −πθt(a)) term in Eq. (5) will"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.07445708376421924,"be near 0, which slows the speed of committing to a deterministic policy on a whenever πθt(a) is
close to 1, which encourages exploration. Such natural exploratory behavior arises in Algorithm 1
because of the softmax Jacobian diag(πθ) −πθπ⊤
θ in the update shown in Proposition 1, which
determines the growth order of θt(a) for all a ∈[K] as t →∞, making the effect of a constant
learning rate η ∈Θ(1) asymptotically inconsequential."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.07549120992761117,"3.3
Warm up: Global Asymptotic Convergence when K = 2"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.07652533609100311,"We now consider the simplest case, where we have only two possible actions. According to Lemma 2,
each of the two actions must be sampled infinitely often as t →∞. We now illustrate the second
key result, that for both actions a ∈[K], the random sequence {θt(a)}t≥1 follows the direction of
the expected gradient for sufficiently large t ≥1 almost surely. The proof uses a technique that has
been previously used in [19, 24] for small learning rates, but here we observe that the same technique
continues to work for Algorithm 1 no matter how large the learning rate is, as long as η ∈Θ(1).
Theorem 1. Let K = 2 and r(1) > r(2). Using Algorithm 1 with any η ∈Θ(1), we have, almost
surely, πθt(a∗) →1 as t →∞, where a∗:= arg maxa∈[K] r(a) (equal to Action 1 in this case)."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.07755946225439504,"Proof sketch.
According to Lemma 2, N∞(1) = N∞(2) = ∞. Denote the the reward gap as
∆:= r(a∗) −maxa̸=a∗r(a) > 0, which becomes ∆= r(1) −r(2) for two actions. Since the
stochastic gradient is unbiased (Proposition 1), we have, for all t ≥1 (detailed calculations omitted),"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.07859358841778696,"Et[θt+1(a∗)] = θt(a∗) + η · πθt(a∗) ·
 
r(a∗) −π⊤
θtr

(11)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.0796277145811789,"= θt(a∗) + η · πθt(a∗) · ∆· (1 −πθt(a∗)) > θt(a∗).
(12)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.08066184074457083,"A similar calculation shows that,"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.08169596690796277,"Et[θt+1(2)] = θt(2) −η · πθt(2) · ∆· (1 −πθt(2)) < θt(2),
(13)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.0827300930713547,"which means that θt(a∗) is monotonically increasing in expectation and θt(2) is monotonically
decreasing in expectation. In other words, {θt(a∗)}t≥1 is a sub-martingale, while {θt(2)}t≥1
is a super-martingale. However, since θt ∈RK is unbounded, Doob’s martingale convergence
results cannot be directly applied, so we pursue a different argument. Following [19, 24], given an
action a ∈[K], we define Pt(a) := Et[θt+1(a)] −θt(a) as the “progress”, and define Wt(a) :=
θt(a) −Et−1[θt(a)] as the “noise”, where θt(a) = Wt(a) + Pt−1(a) + θt−1(a). By recursion we
can determine that,"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.08376421923474664,"θt(a) = E[θ1(a)] + t
X"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.08479834539813857,"s=1
Ws(a) + t−1
X"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.08583247156153051,"s=1
Ps(a),
(14)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.08686659772492245,"i.e., θt(a) is the result of “cumulative progress” and “cumulative noise”. According to [24, Theorem
C.3], the cumulative noise term can be bounded by using martingale concentration, where the order of
the corresponding confidence interval is smaller than the order of the cumulative progress. Therefore,
the summation will always be determined by the cumulative progress as t →∞. According
to the calculations in Eqs. (12) and (13), we have Pt(a∗) > 0 and Pt(2) < 0, both of which
are not summable. As a result, θt(a∗) →∞and θt(2) →−∞as t →∞, which implies that
πθt(a∗)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.08790072388831438,"πθt(2) = exp{θt(a∗) −θt(a)} →∞, hence πθt(a∗) →1 as t →∞."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.0889348500517063,"3.4
Global Asymptotic Convergence for all K ≥2"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.08996897621509824,"The illustrative two-action case shows that if π⊤
θtr ∈(r(2), r(a∗)) and if both actions are sampled
infinitely often, then we have, almost surely θt(a∗) →∞and θt(2) →−∞as t →∞. However, the
question at the beginning of Section 3.2 remains: when K > 2, if the two actions sampled infinitely
often in Lemma 2 are both sub-optimal, will that result in a similar failure mode to the one described
in Section 3.1? The answer is no, which follows from our third key finding, which is based on another
contradiction-based argument that establishes almost sure convergence to a globally optimal policy in
the general K > 2 case."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.09100310237849017,"Theorem 2. Given K ≥2, using Algorithm 1 with any η ∈Θ(1), we have, almost surely, πθt(a∗) →
1 as t →∞, where a∗= arg maxa∈[K] r(a) is the optimal action."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.09203722854188211,"Proof sketch.
We consider two cases: N∞(a∗) < ∞and N∞(a∗) = ∞, corresponding to
whether the optimal action is sampled finitely or infinitely often as t →∞. We argue that the
first case (N∞(a∗) < ∞) is impossible, while for the second case (N∞(a∗) = ∞) we prove that
θt(a∗) −θt(a) →∞for all a ∈[K] with r(a) < r(a∗), which implies πθt(a∗) →1 as t →∞."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.09307135470527404,"First case. Suppose that N∞(a∗) < ∞. We argue that this is impossible via contradiction. Given the
assumption and Lemma 2 we know there must be at least two other sub-optimal actions i1, i2 ∈[K],
i1 ̸= i2, such that N∞(i1) = N∞(i2) = ∞. In particular, let i1 = arg mina∈[K],N∞(a)=∞r(a) and
i2 = arg maxa∈[K],N∞(a)=∞r(a), hence r(i1) < r(i2) < r(a∗). By Lemma 6 (see Appendix) we
will also have r(i1) < π⊤
θtr < r(i2) for sufficiently large t ≥1 , which implies for action i1,"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.09410548086866598,"Et[θt+1(i1)] = θt(i1) + η · πθt(i1) ·
 
r(i1) −π⊤
θtr

< θt(i1),
(15)
for sufficiently large t ≥1, which further implies that supt≥1 θt(i1) < ∞. Meanwhile, for the
optimal action a∗, the assumption and Lemma 1 imply that inft≥1 θt(a∗) > −∞. Combining these
two observations gives,"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.09513960703205791,"sup
t≥1"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.09617373319544985,"πθt(i1)
πθt(a∗) = sup
t≥1
exp{θt(i1) −θt(a∗)} < ∞.
(16)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.09720785935884178,"On the other hand, since N∞(i1) = ∞and N∞(a∗) < ∞(by assumption), we then have
supt≥1 exp{θt(i1) −θt(a∗)} = ∞by Lemma 5 (see Appendix), which contradicts Eq. (16)."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.09824198552223372,"Second case. Suppose that N∞(a∗) = ∞. We will argue that πθt(a∗) →1 as t →∞almost surely.
First, according to Lemma 2, there exists at least one sub-optimal action i1 ∈[K], i1 ̸= a∗, such
that N∞(i1) = ∞. Let i1 = arg mina∈[K],N∞(a)=∞r(a). By Lemma 6 and the definition of a∗,
we have r(i1) < π⊤
θtr < r(a∗) for all sufficiently large t ≥1. Since N∞(i1) = ∞, using similar
calculations to Eqs. (13) and (14) in Theorem 1, we have, θt(i1) →−∞as t →∞. We also have,
inft≥1 θt(a∗) > −∞as t →∞. Hence, πθt(a∗)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.09927611168562564,πθt(i1) = exp{θt(a∗) −θt(i1)} →∞as t →∞.
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.10031023784901758,"Define A∞:= {a ∈[K] | N∞(a) = ∞} as the set of actions that are sampled infinitely often, and
note that |A∞| ≥2 by Lemma 2. Sort the action indices in A∞according to their expected reward
values in descending order, i.e.,
r(a∗) > r(i|A∞|−1) > r(i|A∞|−2) > · · · > r(i2) > r(i1).
(17)
Assumption 1 is used here to prevent two arms from having the same reward and thus guarantee the
inequalities are strict in Eq. (17). Next, using similar calculations as in Lemma 6, we have,"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.10134436401240951,"π⊤
θtr −r(i2) > πθt(a∗) ·

r(a∗) −r(i2) −
X"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.10237849017580145,a−∈A−(i2)
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.10341261633919338,πθt(a−)
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.10444674250258532,"πθt(a∗) · (r(i2) −r(a−))

,
(18)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.10548086866597725,"where A−(i2) := {a−∈[K] : r(a−) < r(i2)} is the set of actions that have lower mean reward than
i2 ∈[K], and note that i1 ∈A−(i2). Using the above definitions, we can conclude that i1 is the only
arm in A−(i2) that has been sampled infinitely often. According to Lemma 5, for all a−∈A−(i2)
with a−̸= i1, we have,
πθt(a∗)
πθt(a−) →∞as t →∞, since N∞(a∗) = ∞(by assumption) and
N∞(a−) < ∞(by Eq. (17)). Therefore, for all sufficiently large t, the probability ratio in Eq. (18)
πθt(a∗)
πθt(a−) →∞for all a−∈A−(i2), which implies that, for all sufficiently large t ≥1,"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.10651499482936919,"π⊤
θtr −r(i2) > 0.5 · πθt(a∗) ·
 
r(a∗) −r(i2)

> 0.
(19)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.10754912099276112,"We have thus shown that π⊤
θtr > r(i2). Recall that we had previously proved that π⊤
θtr > r(i1).
Hence, we will apply this argument recursively: after this point, i2 ∈[K] will become the new
“i1 ∈[K]”, and a similar inequality to Eq. (13) will then hold for i2 ∈[K] from similar calculations
to Eqs. (13) and (14) in Theorem 1, establishing θt(i2) →−∞as t →∞. This will imply that for
all sufficiently large t ≥1,
π⊤
θtr −r(i3) > 0.5 · πθt(a∗) ·
 
r(a∗) −r(i3)

> 0.
(20)
Continuing the recursive argument, we can conclude for all actions a ∈A∞with a ̸= a∗that
πθt(a∗)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.10858324715615306,"πθt(a) →∞as t →∞. Meanwhile, for all actions a ̸∈A∞, Lemma 5 also shows that πθt(a∗)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.10961737331954498,"πθt(a) →∞
as t →∞. Combining these two results yields the conclusion that for all sub-optimal actions a ∈[K]
with r(a) < r(a∗) we have πθt(a∗)"
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.11065149948293691,"πθt(a) →∞as t →∞, which implies πθt(a∗) →1 as t →∞. Thus,
we have established almost sure convergence to the globally optimal policy."
STOCHASTIC GRADIENT AUTOMATICALLY AVOIDS LACK OF EXPLORATION,0.11168562564632885,"Discussion.
Lemma 2 is important to prove that the optimal arm will be sampled infinitely often.
In particular, Lemma 2 guarantees |A∞| ≥2 and the existence of i1 in Eq. (15), which can then be
used to construct the contradiction in Eq. (16). Without Lemma 2, |A∞| might be equal to 1 and
the failure mode in Section 3.1 can occur, resulting in Algorithm 1 not sampling the optimal action
infinitely often as t →∞."
ASYMPTOTIC RATE OF CONVERGENCE,0.11271975180972078,"3.5
Asymptotic Rate of Convergence"
ASYMPTOTIC RATE OF CONVERGENCE,0.11375387797311272,"According to Theorem 2, almost surely, πθt(a∗) →1 as t →∞. Therefore, after a large enough time
τ < ∞, we have πθt(a∗) ≥1/2, which implies that the “progress” term in Eq. (14) can be lower
bounded. With this, an asymptotic rate of convergence can be proved as follows.
Theorem 3. For a large enough τ > 0, for all T > τ, the average sub-optimality decreases at an
O

ln(T )"
ASYMPTOTIC RATE OF CONVERGENCE,0.11478800413650465,"T

rate. Formally, if a∗is the optimal arm, then, for a constant c,
PT
s=τ r(a∗) −⟨πs, r⟩"
ASYMPTOTIC RATE OF CONVERGENCE,0.11582213029989659,"T
≤c ln(T)"
ASYMPTOTIC RATE OF CONVERGENCE,0.11685625646328852,T −τ .
SIMULATION STUDY,0.11789038262668046,"4
Simulation Study"
SIMULATION STUDY,0.1189245087900724,"To validate and enhance the theoretical findings, we ran experiments with a four action bandit
environment (K = 4) with a true mean reward vector of r = (0.2, 0.05, −0.1, −0.4)⊤∈R4. The
reward distribution Pa for arm 1 ≤a ≤4 is Gaussian, centered at r(a) and with a standard deviation
of 0.1. The environment is chosen to illustrate various phenomenon, which we discuss after presenting
the results. The algorithm is Algorithm 1 with θ1 = 0 ∈RK."
SIMULATION STUDY,0.11995863495346432,"For comparison, assuming that the random rewards belong to the [−1, 1] interval, the only result
for the stochastic gradient bandit algorithm [24, Lemma 4.6] that allowed a constant learning rate
required that the learning rate be less than ηc =
∆2"
SIMULATION STUDY,0.12099276111685625,"40·K3/2·R3max =
9
128000 ≈0.00007, where we used
∆= 0.15, K = 4, and Rmax = 1. While technically, the result does not apply to our case where
the reward distributions have unbounded support, the probability of the reward landing outside of
[−1, 1] is in the order of 10−9. Choosing Rmax to be larger, this probability falls extremely quickly,
which suggests that the above threshold is generous. For the experiments we use the learning rates
η ∈{1, 10, 100, 1000}, that are several orders of magnitudes larger than ηc. For each learning rate,
we plot the outcome of 10 runs, corresponding to different random seeds. Each run lasts 106(≈e14)
iterations. The log-suboptimality gaps for the 4 × 10 cases are shown on Figures 1a-1d, where they
are plotted against the logarithm of time. Additional results for K = 2 arms are shown in Appendix D.
Note that in this example small sub-optimality implies that the optimal arm is chosen with high
probability. In what follows, we discuss the results in the plots."
SIMULATION STUDY,0.12202688728024819,"Asymptotic convergence.
For the smaller learning rates of η = 1 and 10, all 10 seeds rapidly and
steadily converge, reaching a sub-optimality of e−14 or less. For η = 100 and 1000, most of the runs
reach even small error even faster, but some runs are “stuck” even after 106 steps. Note that this
does not contradict the theoretical result; nor do we suspect numerical issues. As seen for the case of
η = 100, even after a long phase with little to no progress, a run can “recover” (see the grey curve).
In fact, it is reasonable to expect that the price of increasing the learning rate is larger variance; as
seen in these plots (subplots (a) and (b) are also attesting to this). Differences between learning rates
are further discussed below."
SIMULATION STUDY,0.12306101344364012,"Non-monotone objective value.
Using a very small learning rate guarantees monotonic improve-
ment (in expectation) in the policy’s expected reward [24]. Conversely, a large learning rate results in
non-monotonic evolution of the expected rewards {π⊤
θtr}t≥1, even in the final stages of convergence,
as can be seen clearly for the learning rates of η = 1 and 10 in Figures 1a and 1b. For larger η, the
non-monotone behavior happens over longer periods and is less visible in the plots. This is because
using large learning rates causes the policy to rapidly increase the parameters for some action, after
which the gradient becomes small, limiting further progress."
SIMULATION STUDY,0.12409513960703206,"Rate of convergence.
Figures 1a and 1b, where the log-log plot has a slope of nearly −1, give
some evidence that an O(1/t) asymptotic rate is achieved. In general, such a rate cannot be improved"
SIMULATION STUDY,0.125129265770424,"0
2
4
6
8
10
12
14
log(t) 14 12 10 8 6 4 2"
SIMULATION STUDY,0.12616339193381593,Log sub-optimality gap
SIMULATION STUDY,0.12719751809720786,(a) η = 1.
SIMULATION STUDY,0.1282316442605998,"0
2
4
6
8
10
12
14
log(t) 20.0 17.5 15.0 12.5 10.0 7.5 5.0 2.5"
SIMULATION STUDY,0.12926577042399173,Log sub-optimality gap
SIMULATION STUDY,0.13029989658738367,(b) η = 10.
SIMULATION STUDY,0.1313340227507756,"0
2
4
6
8
10
12
14
log(t) 30 25 20 15 10 5 0"
SIMULATION STUDY,0.13236814891416754,Log sub-optimality gap
SIMULATION STUDY,0.13340227507755947,(c) η = 100.
SIMULATION STUDY,0.1344364012409514,"0
2
4
6
8
10
12
14
log(t) 300 250 200 150 100 50 0"
SIMULATION STUDY,0.13547052740434332,Log sub-optimality gap
SIMULATION STUDY,0.13650465356773525,(d) η = 1000.
SIMULATION STUDY,0.1375387797311272,"Figure 1: Log sub-optimality gap, log (r(a∗) −π⊤
θtr), plotted against the logarithm of time, log t, in
a 4-action problem with various learning rates, η. Each subplot shows a run with a specific learning
rate. The curves in a subplot correspond to 10 different random seeds. Theory predicts that essentially
all seeds will lead to a curve converging to zero (−∞in these plots). For a discussion of the results,
see the text.,"
SIMULATION STUDY,0.13857290589451912,"in terms of t [14]. Theorem 3 gives a weaker version of convergence rate over averaged iterates (not
last iterate), which is slightly worse than O(1/t). More work is needed to verify if the asymptotic
convergence rate in Theorem 3 is improvable or not."
SIMULATION STUDY,0.13960703205791106,"Different learning rates.
Two observations can be made from Figure 1 regarding the effect of
using different η values: First, during the final stage of convergence when r(a∗) −π⊤
θtr ≈0, using
larger η results in faster convergence on average. As η increases, the order of log (r(a∗) −π⊤
θtr)
also changes from e−14 (η = 1), to e−20 (η = 100), and e−200 (η = 1000). We conjecture that the
asymptotic rate of convergence has an O(1/η) dependence. Second, using larger learning rates can
take a longer time to enter the final stage of convergence. When η = 1 or 10, all curves quickly enter
the final stage of r(a∗) −π⊤
θtr ≈0. However, for larger η values, 1/10 runs (η = 100) and 3/10 runs
(η = 1000) result in r(a∗) −π⊤
θtr values far from 0 even after 106 iterations. These runs take orders
of magnitude more iterations to eventually achieve r(a∗) −π⊤
θtr ≈0. These situations correspond to
the policy πθt getting stuck near sub-optimal corners of the simplex, meaning that πθt(i) ≈1 for a
sub-optimal action i ∈[K] with r(i) < r(a∗). In such cases, even Softmax PG with the true gradient
can remain stuck on a sub-optimal plateau for an extremely long time [23]. However, the reason why
larger learning rates lead to longer plateaus in the stochastic setting remains unclear."
SIMULATION STUDY,0.140641158221303,"Trade-offs and multi-stage chracterizations of convergence.
Given the above observations, there
appears to exist a trade-off for η: larger η values result in faster convergence during the final stage
where r(a∗)−π⊤
θtr ≈0, but at the the cost of taking far longer to enter this final stage of convergence.
Since asymptotic convergence results are insufficient for explaining these subtleties in a satisfactory
manner, a more refined analysis that considers the different stages of convergence is required."
CONCLUSIONS AND FUTURE DIRECTIONS,0.14167528438469493,"5
Conclusions and Future Directions"
CONCLUSIONS AND FUTURE DIRECTIONS,0.14270941054808686,"This work refines our understanding of stochastic gradient bandit algorithms by proving that it
converges to a globally optimal policy almost surely with any constant learning rate. Our new proof
strategy based on the asymptotics of sample counts opens new directions for better characterizing
exploration effects of stochastic gradient methods, while also suggesting interesting new questions.
Characterizing the multiple stages of convergence remains another interesting future direction. One
interesting possibility is that there might exist an optimal time-dependent scheme for increasing the
learning rate (such as η ∈O(log t)) to accelerate convergence, rather than use a constant η ∈O(1).
This is corroborated by our experiments: As seen in Figure 1, small learning rates perform better
during the early stages of optimization, while larger learning rates achieve faster convergence during
the final stage. Other directions include extending our bandit results to the more general RL setting
[34], as well as extending our results for the softmax tabular parameterization to handle function
approximation [1]."
CONCLUSIONS AND FUTURE DIRECTIONS,0.1437435367114788,"Limitations: While this work establishes a surprising asymptotic convergence result for any constant
learning rate, it does not shed light on the effect of different learning rates on the convergence.
Moreover, our analysis is limited to multi-armed bandits, and does not immediately extend to the
general RL setting. These aspects are the main limitations of this paper."
CONCLUSIONS AND FUTURE DIRECTIONS,0.14477766287487073,"Broader impact: This is primarily theoretical work on a fundamental algorithm that is used broadly
in RL applications. We expect these results to improve the research community’s understanding of
the basic stochastic gradient bandit method."
CONCLUSIONS AND FUTURE DIRECTIONS,0.14581178903826267,Acknowledgments and Disclosure of Funding
CONCLUSIONS AND FUTURE DIRECTIONS,0.1468459152016546,"Jincheng Mei would like to thank Ramki Gummadi for providing feedback on a draft of this
manuscript. Csaba Szepesvári and Dale Schuurmans gratefully acknowledge funding from the
Canada CIFAR AI Chairs Program, Amii and NSERC. Sharan Vaswani acknowledges the support
from the NSERC Discovery Grant (RGPIN-2022-04816)."
REFERENCES,0.14788004136504654,References
REFERENCES,0.14891416752843847,"[1] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient
methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research,
22(98):1–76, 2021."
REFERENCES,0.1499482936918304,"[2] Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In
Conference on learning theory, pages 39–1. JMLR Workshop and Conference Proceedings, 2012."
REFERENCES,0.15098241985522234,"[3] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine learning, 47(2):235–256, 2002."
REFERENCES,0.15201654601861428,"[4] Léon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP-
STAT’2010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010
Keynote, Invited and Contributed Papers, pages 177–186. Springer, 2010."
REFERENCES,0.15305067218200621,"[5] Leo Breiman. Probability. SIAM, 1992."
REFERENCES,0.15408479834539815,"[6] Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of natural
policy gradient methods with entropy regularization. Operations Research, 70(4):2563–2578, 2022."
REFERENCES,0.15511892450879008,"[7] Nicolò Cesa-bianchi and Claudio Gentile. Improved risk tail bounds for on-line algorithms. In Y. Weiss,
B. Schölkopf, and J. Platt, editors, Advances in Neural Information Processing Systems, volume 18. MIT
Press, 2005."
REFERENCES,0.15615305067218202,"[8] Denis Denisov and Neil Walton. Regret analysis of a markov policy gradient algorithm for multi-arm
bandits. arXiv preprint arXiv:2007.10229, 2020."
REFERENCES,0.15718717683557393,"[9] Yuhao Ding, Junzi Zhang, and Javad Lavaei. Beyond exact gradients: Convergence of stochastic soft-max
policy gradient methods with entropy regularization. arXiv preprint arXiv:2110.10117, 2021."
REFERENCES,0.15822130299896586,"[10] David A. Freedman. On Tail Probabilities for Martingales. The Annals of Probability, 3(1):100 – 118,
1975."
REFERENCES,0.1592554291623578,"[11] Shivam Garg, Samuele Tosatto, Yangchen Pan, Martha White, and A Rupam Mahmood. An alternate
policy gradient estimator for softmax policies. arXiv preprint arXiv:2112.11622, 2021."
REFERENCES,0.16028955532574973,"[12] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013."
REFERENCES,0.16132368148914167,"[13] Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems, pages
1531–1538, 2002."
REFERENCES,0.1623578076525336,"[14] Tze Leung Lai, Herbert Robbins, et al. Asymptotically efficient adaptive allocation rules. Advances in
applied mathematics, 6(1):4–22, 1985."
REFERENCES,0.16339193381592554,"[15] Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling
complexity, and generalized problem classes. Mathematical programming, 198(1):1059–1106, 2023."
REFERENCES,0.16442605997931747,"[16] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.1654601861427094,"[17] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Softmax policy gradient methods can take
exponential time to converge. In Conference on Learning Theory, pages 3107–3110. PMLR, 2021."
REFERENCES,0.16649431230610134,"[18] Michael Lu, Matin Aghaei, Anant Raj, and Sharan Vaswani. Towards principled, practical policy gradient
for bandits and tabular mdps. arXiv preprint arXiv:2405.13136, 2024."
REFERENCES,0.16752843846949328,"[19] Jincheng Mei, Wesley Chung, Valentin Thomas, Bo Dai, Csaba Szepesvari, and Dale Schuurmans. The
role of baselines in policy gradient optimization. Advances in Neural Information Processing Systems,
35:17818–17830, 2022."
REFERENCES,0.16856256463288521,"[20] Jincheng Mei, Bo Dai, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. Understanding the effect of
stochasticity in policy optimization. Advances in Neural Information Processing Systems, 34:19339–19351,
2021."
REFERENCES,0.16959669079627715,"[21] Jincheng Mei, Yue Gao, Bo Dai, Csaba Szepesvari, and Dale Schuurmans. Leveraging non-uniformity in
first-order non-convex optimization. In International Conference on Machine Learning, pages 7555–7564.
PMLR, 2021."
REFERENCES,0.17063081695966908,"[22] Jincheng Mei, Chenjun Xiao, Bo Dai, Lihong Li, Csaba Szepesvári, and Dale Schuurmans. Escaping the
gravitational pull of softmax. Advances in Neural Information Processing Systems, 33:21130–21140, 2020."
REFERENCES,0.17166494312306102,"[23] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of
softmax policy gradient methods. In International Conference on Machine Learning, pages 6820–6829.
PMLR, 2020."
REFERENCES,0.17269906928645296,"[24] Jincheng Mei, Zixin Zhong, Bo Dai, Alekh Agarwal, Csaba Szepesvari, and Dale Schuurmans. Stochastic
gradient succeeds for bandits. arXiv preprint arXiv:2402.17235, 2024."
REFERENCES,0.1737331954498449,"[25] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic ap-
proximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574–1609,
2009."
REFERENCES,0.17476732161323683,"[26] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. arXiv preprint arXiv:2203.02155, 2022."
REFERENCES,0.17580144777662876,"[27] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.
Direct preference optimization: Your language model is secretly a reward model. Advances in Neural
Information Processing Systems, 36, 2024."
REFERENCES,0.1768355739400207,"[28] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pages 400–407, 1951."
REFERENCES,0.1778697001034126,"[29] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy
optimization. In International conference on machine learning, pages 1889–1897, 2015."
REFERENCES,0.17890382626680454,"[30] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.17993795243019647,"[31] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT Press, 2018."
REFERENCES,0.1809720785935884,"[32] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for
reinforcement learning with function approximation. Advances in neural information processing systems,
12, 1999."
REFERENCES,0.18200620475698034,"[33] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3-4):285–294, 1933."
REFERENCES,0.18304033092037228,"[34] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8:229–256, 1992."
REFERENCES,0.18407445708376421,"[35] Jingfeng Wu, Peter L Bartlett, Matus Telgarsky, and Bin Yu. Large stepsize gradient descent for logistic
loss: Non-monotonicity of the loss improves optimization efficiency. arXiv preprint arXiv:2402.15926,
2024."
REFERENCES,0.18510858324715615,"[36] Rui Yuan, Robert M Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla
policy gradient. In International Conference on Artificial Intelligence and Statistics, pages 3332–3380.
PMLR, 2022."
REFERENCES,0.18614270941054809,"[37] Junyu Zhang, Chengzhuo Ni, Csaba Szepesvari, Mengdi Wang, et al. On the convergence and sample
efficiency of variance-reduced policy gradient method. Advances in Neural Information Processing Systems,
34:2228–2240, 2021."
REFERENCES,0.18717683557394002,"[38] Junzi Zhang, Jongho Kim, Brendan O’Donoghue, and Stephen Boyd. Sample efficient reinforcement
learning with reinforce. arXiv preprint arXiv:2010.11364, 2020."
REFERENCES,0.18821096173733196,"[39] Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Basar. Global convergence of policy gradient methods
to (almost) locally optimal policies. SIAM Journal on Control and Optimization, 58(6):3586–3612, 2020."
REFERENCES,0.1892450879007239,"A
Asymptotic Convergence"
REFERENCES,0.19027921406411583,"Lemma 1. Using Algorithm 1 with any constant η ∈Θ(1), if N∞(a) < ∞for an action a ∈[K],
then we have, almost surely,"
REFERENCES,0.19131334022750776,"sup
t≥1
θt(a) < ∞, and inf
t≥1 θt(a) > −∞.
(21)"
REFERENCES,0.1923474663908997,"Proof. Suppose N∞(a) < ∞for an action a ∈[K]. According to Algorithm 1,"
REFERENCES,0.19338159255429163,"θt+1(a) ←θt(a) +
η · (1 −πθt(a)) · Rt(a),
if at = a ,
−η · πθt(a) · Rt(at),
otherwise ,
(22)"
REFERENCES,0.19441571871768357,and let
REFERENCES,0.1954498448810755,"It(a) :=
1,
if at = a ,
0,
otherwise .
(23)"
REFERENCES,0.19648397104446744,"According to Eq. (22), we have, for all t ≥1,"
REFERENCES,0.19751809720785937,"θt(a) −θ1(a) = t−1
X"
REFERENCES,0.19855222337125128,"s=1
Is(a) · η · (1 −πθs(a)) · Rs(a) + t−1
X"
REFERENCES,0.19958634953464321,"s=1
(1 −Is(a)) · (−η) · πθs(a) · Rs(as). (24)"
REFERENCES,0.20062047569803515,"Using triangle inequality, we have,"
REFERENCES,0.20165460186142709,"|θt(a) −θ1(a)| ≤ t−1
X s=1"
REFERENCES,0.20268872802481902,"Is(a) · η · (1 −πθs(a)) · Rs(a)
 + t−1
X s=1"
REFERENCES,0.20372285418821096,(1 −Is(a)) · (−η) · πθs(a) · Rs(as) (25)
REFERENCES,0.2047569803516029,"≤η · Rmax · t−1
X"
REFERENCES,0.20579110651499483,"s=1
Is(a) + η · Rmax · t−1
X"
REFERENCES,0.20682523267838676,"s=1
πθs(a)
(26)"
REFERENCES,0.2078593588417787,"= η · Rmax ·

Nt−1(a) + t−1
X"
REFERENCES,0.20889348500517063,"s=1
πθs(a)

.
(27)"
REFERENCES,0.20992761116856257,"By assumption, we have,"
REFERENCES,0.2109617373319545,"N∞(a) := lim
t→∞Nt(a) < ∞.
(28)"
REFERENCES,0.21199586349534644,"According to the extended Borel-Cantelli Lemma 3, we have, almost surely, ∞
X"
REFERENCES,0.21302998965873837,"t=1
πθt(a) := lim
t→∞ t
X"
REFERENCES,0.2140641158221303,"s=1
πθs(a) < ∞.
(29)"
REFERENCES,0.21509824198552224,"Combining Eqs. (25), (28) and (29), we have, almost surely,"
REFERENCES,0.21613236814891418,"sup
t≥1
|θt(a) −θ1(a)| < ∞,
(30)"
REFERENCES,0.2171664943123061,"which implies that, almost surely,"
REFERENCES,0.21820062047569805,"sup
t≥1
|θt(a)| ≤sup
t≥1
|θt(a) −θ1(a)| + |θ1(a)| < ∞."
REFERENCES,0.21923474663908996,"Lemma 2 (Avoiding lack of exploration). Using Algorithm 1 with any η ∈Θ(1), there exists at least
a pair of distinct actions i, j ∈[K] and i ̸= j, such that, almost surely,"
REFERENCES,0.2202688728024819,"N∞(i) = ∞, and N∞(j) = ∞.
(31)"
REFERENCES,0.22130299896587383,"Proof. First, we have, for all t ≥1, t = t
X s=1 X"
REFERENCES,0.22233712512926576,"a∈[K]
Is(a)
 X"
REFERENCES,0.2233712512926577,"a∈[K]
It(a) = 1 for all t ≥1

(32) =
X a∈[K] t
X"
REFERENCES,0.22440537745604963,"s=1
Is(a)
(33) =
X"
REFERENCES,0.22543950361944157,"a∈[K]
Nt(a).
(34)"
REFERENCES,0.2264736297828335,"By pigeonhole principle, there exists at least one action i ∈[K], such that, almost surely,"
REFERENCES,0.22750775594622544,"N∞(i) := lim
t→∞Nt(i) = ∞.
(35)"
REFERENCES,0.22854188210961737,"We argue the existence of another action by contradiction. Suppose for all the other actions j ∈[K]
and j ̸= i, we have N∞(j) < ∞. According to the extended Borel-Cantelli Lemma 3, we have,
almost surely, ∞
X"
REFERENCES,0.2295760082730093,"t=1
πθt(j) := lim
t→∞ t
X"
REFERENCES,0.23061013443640124,"s=1
πθs(j) < ∞.
(36)"
REFERENCES,0.23164426059979318,"According to the update Eq. (22), we have, for all t ≥1,"
REFERENCES,0.2326783867631851,"θt(i) −θ1(i) = t−1
X"
REFERENCES,0.23371251292657705,"s=1
Is(i) · η · (1 −πθs(i)) · Rs(i) + t−1
X"
REFERENCES,0.23474663908996898,"s=1
(1 −Is(i)) · (−η) · πθs(i) · Rs(as). (37)"
REFERENCES,0.23578076525336092,"By triangle inequality, we have,"
REFERENCES,0.23681489141675285,"|θt(i) −θ1(i)| ≤ t−1
X s=1"
REFERENCES,0.2378490175801448,"Is(i) · η · (1 −πθs(i)) · Rs(i)
 + t−1
X s=1"
REFERENCES,0.23888314374353672,(1 −Is(i)) · (−η) · πθs(i) · Rs(as) (38)
REFERENCES,0.23991726990692863,"≤η · Rmax · t−1
X"
REFERENCES,0.24095139607032057,"s=1
(1 −πθs(i)) + η · Rmax · t−1
X"
REFERENCES,0.2419855222337125,"s=1
(1 −Is(i))
(39)"
REFERENCES,0.24301964839710444,"= η · Rmax ·
 t−1
X s=1 X"
REFERENCES,0.24405377456049637,"j̸=i
πθs(j) + t−1
X s=1 X"
REFERENCES,0.2450879007238883,"j̸=i
Is(j)

(40)"
REFERENCES,0.24612202688728024,"= η · Rmax ·
 X j̸=i t−1
X"
REFERENCES,0.24715615305067218,"s=1
πθs(j) +
X"
REFERENCES,0.2481902792140641,"j̸=i
Nt−1(j)

.
(41)"
REFERENCES,0.24922440537745605,"Combing Eqs. (36) and (38) and the assumption of N∞(j) < ∞for all j ̸= i, we have, almost surely,"
REFERENCES,0.250258531540848,"sup
t≥1
|θt(i)| ≤sup
t≥1
|θt(i) −θ1(i)| + |θ1(i)| < ∞.
(42)"
REFERENCES,0.2512926577042399,"Since N∞(j) < ∞for all j ̸= i by assumption, and according to Lemma 1, we have, almost surely,"
REFERENCES,0.25232678386763185,"sup
t≥1
|θt(j)| < ∞.
(43)"
REFERENCES,0.2533609100310238,"Combining Eqs. (42) and (43), we have, for all action a ∈[K],"
REFERENCES,0.2543950361944157,"sup
t≥1
|θt(a)| < ∞,
(44)"
REFERENCES,0.25542916235780766,"which implies that, there exists c > 0 and c ∈O(1), such that, for all a ∈[K],"
REFERENCES,0.2564632885211996,"inf
t≥1 πθt(a) = inf
t≥1
exp{θt(a)}
P"
REFERENCES,0.25749741468459153,"a′∈[K] exp{θt(a′)} ≥c > 0.
(45)"
REFERENCES,0.25853154084798347,"Therefore, for all action a ∈[K], ∞
X"
REFERENCES,0.2595656670113754,"t=1
πθt(a) := lim
t→∞ t
X"
REFERENCES,0.26059979317476734,"s=1
πθs(a)
(46)"
REFERENCES,0.26163391933815927,"≥lim
t→∞ t
X"
REFERENCES,0.2626680455015512,"s=1
c
(47)"
REFERENCES,0.26370217166494314,"= lim
t→∞t · c
(48)"
REFERENCES,0.2647362978283351,"= ∞.
(49)"
REFERENCES,0.265770423991727,"According to the extended Borel-Cantelli Lemma 3, we have, almost surely, for all action a ∈[K],"
REFERENCES,0.26680455015511895,"N∞(a) = ∞,
(50)"
REFERENCES,0.2678386763185109,"which is a contradiction with the assumption of N∞(j) < ∞for all the other actions j ∈[K] with
j ̸= i. Therefore, there exists another action j ∈[K] with j ̸= i, such that N∞(j) = ∞."
REFERENCES,0.2688728024819028,"Theorem 1. Let K = 2 and r(1) > r(2). Using Algorithm 1 with any η ∈Θ(1), we have, almost
surely, πθt(a∗) →1 as t →∞, where a∗:= arg maxa∈[K] r(a) (equal to Action 1 in this case)."
REFERENCES,0.26990692864529475,"Proof. The proof uses the same notations as of [24, Theorem 5.1]."
REFERENCES,0.27094105480868663,"According to Lemma 2, we have, N∞(1) = ∞and N∞(2) = ∞, i.e., both of the two actions are
sampled for infinitely many times as t →∞."
REFERENCES,0.27197518097207857,"Let Ft be the σ-algebra generated by a1, R1(a1), · · · , at−1, Rt−1(at−1):"
REFERENCES,0.2730093071354705,"Ft = σ({a1, R1(a1), · · · , at−1, Rt−1(at−1)}) .
(51)"
REFERENCES,0.27404343329886244,"Note that θt and It (defined by Eq. (23)) are Ft-measurable for all t ≥1. Let Et[·] denote the
conditional expectation with respect to Ft: Et[X] = E[X|Ft]. Define the following notations,"
REFERENCES,0.2750775594622544,"Wt(a) := θt(a) −Et−1[θt(a)],
(“noise”)
(52)
Pt(a) := Et[θt+1(a)] −θt(a).
(“progress”)
(53)"
REFERENCES,0.2761116856256463,"For each action a ∈[K], for t ≥2, we have the following decomposition,"
REFERENCES,0.27714581178903824,"θt(a) = Wt(a) + Pt−1(a) + θt−1(a).
(54)"
REFERENCES,0.2781799379524302,"By recursion we can determine that,"
REFERENCES,0.2792140641158221,"θt(a) = E[θ1(a)] + t
X"
REFERENCES,0.28024819027921405,"s=1
Ws(a) + t−1
X"
REFERENCES,0.281282316442606,"s=1
Ps(a),
(55)"
REFERENCES,0.2823164426059979,"while we also have,"
REFERENCES,0.28335056876938985,"θ1(a) = θ1(a) −E[θ1(a)]
|
{z
}
W1(a)"
REFERENCES,0.2843846949327818,"+E[θ1(a)],
(56)"
REFERENCES,0.2854188210961737,"and E[θ1(a)] accounts for potential randomness in initializing θ1 ∈RK. According to Proposition 1,
we have,"
REFERENCES,0.28645294725956566,"Pt(a) = Et[θt+1(a)] −θt(a) = η · πθt(a) ·
 
r(a) −π⊤
θtr

,
(57)"
REFERENCES,0.2874870734229576,"which implies that, for all t ≥1,"
REFERENCES,0.28852119958634953,"Pt(a∗) > 0 > Pt(2).
(58)"
REFERENCES,0.28955532574974147,"Next, we show that Pt
s=1 Ps(a∗) →∞as t →∞by contradiction. Suppose that, ∞
X"
REFERENCES,0.2905894519131334,"t=1
Pt(a∗) < ∞.
(59)"
REFERENCES,0.29162357807652534,"For the optimal action a∗= 1, t
X"
REFERENCES,0.29265770423991727,"s=1
Ps(a∗) = t
X"
REFERENCES,0.2936918304033092,"s=1
η · πθs(a∗) · (r(a∗) −π⊤
θsr)
(60)"
REFERENCES,0.29472595656670114,"= η · ∆· t
X"
REFERENCES,0.2957600827300931,"s=1
πθs(a∗) · (1 −πθs(a∗)),
(61)"
REFERENCES,0.296794208893485,"where ∆:= r(a∗) −maxa̸=a∗r(a) = r(1) −r(2) > 0 is the reward gap. Denote that, for all t ≥1,"
REFERENCES,0.29782833505687695,"Vt(a∗) := 5 18 · t−1
X"
REFERENCES,0.2988624612202689,"s=1
πθs(a∗) · (1 −πθs(a∗)).
(62)"
REFERENCES,0.2998965873836608,"According to Lemma 8 (using Eqs. (58) to (60) and (62)), we have, almost surely,"
REFERENCES,0.30093071354705275,"sup
t≥1
|θt(a∗)| < ∞.
(63)"
REFERENCES,0.3019648397104447,"For the sub-optimal action (Action 2), we have, t
X"
REFERENCES,0.3029989658738366,"s=1
Ps(2) = t
X"
REFERENCES,0.30403309203722856,"s=1
η · πθs(2) · (r(2) −π⊤
θsr)
(64)"
REFERENCES,0.3050672182006205,"= −η · ∆· t
X"
REFERENCES,0.30610134436401243,"s=1
πθs(2) · (1 −πθs(2))
(65) = − t
X"
REFERENCES,0.30713547052740436,"s=1
Ps(a∗),
(66)"
REFERENCES,0.3081695966907963,"and also denote, for all t ≥1,"
REFERENCES,0.30920372285418823,"Vt(2) := 5 18 · t−1
X"
REFERENCES,0.31023784901758017,"s=1
πθs(2) · (1 −πθs(2)) = Vt(a∗).
(67)"
REFERENCES,0.3112719751809721,"According to Lemma 8 (using Eqs. (58), (59), (64) and (67)), we have, almost surely,"
REFERENCES,0.31230610134436404,"sup
t≥1"
REFERENCES,0.3133402275077559,"θt(2)
 < ∞.
(68)"
REFERENCES,0.31437435367114785,"Combining Eqs. (63) and (68), there exists c > 0 and c ∈O(1), such that, for all a ∈{a∗, 2},"
REFERENCES,0.3154084798345398,"inf
t≥1 πθt(a) = inf
t≥1
exp{θt(a)}
P"
REFERENCES,0.3164426059979317,"a′∈[K] exp{θt(a′)} ≥c > 0,
(69)"
REFERENCES,0.31747673216132366,"which implies that, ∞
X"
REFERENCES,0.3185108583247156,"t=1
πθt(a∗) · (1 −πθt(a∗)) := lim
t→∞ t
X"
REFERENCES,0.31954498448810753,"s=1
πθs(a∗) · (1 −πθs(a∗))
(70)"
REFERENCES,0.32057911065149947,"≥lim
t→∞ t
X"
REFERENCES,0.3216132368148914,"s=1
c · c
(71)"
REFERENCES,0.32264736297828334,"= lim
t→∞t · c2
(72)"
REFERENCES,0.32368148914167527,"= ∞,
(73)"
REFERENCES,0.3247156153050672,"which is a contradiction with the assumption of Eq. (59). Therefore, we have, t
X"
REFERENCES,0.32574974146845914,"s=1
Ps(a∗) →∞, as t →∞.
(74)"
REFERENCES,0.3267838676318511,"According to Lemma 9 (using Eqs. (58), (60), (62) and (74)), we have, almost surely,"
REFERENCES,0.327817993795243,"θt(a∗) →∞, as t →∞.
(75)"
REFERENCES,0.32885211995863495,"Similarly, according to Lemma 10 (using Eqs. (58), (64), (67) and (74)), we have, almost surely,"
REFERENCES,0.3298862461220269,"lim
t→∞θt(2) = −∞.
(76)"
REFERENCES,0.3309203722854188,"Note that,"
REFERENCES,0.33195449844881075,"πθt(a∗) =
πθt(a∗)
πθt(2) + πθt(a∗)
(77)"
REFERENCES,0.3329886246122027,"=
1
exp{θt(2) −θt(a∗)} + 1.
(78)"
REFERENCES,0.3340227507755946,"Combining Eqs. (75) to (77), we have, almost surely,"
REFERENCES,0.33505687693898656,"πθt(a∗) →1, as t →∞."
REFERENCES,0.3360910031023785,"Theorem 2. Under Assumption 1, given K ≥2, using Algorithm 1 with any η ∈Θ(1), we have,
almost surely, πθt(a∗) →1 as t →∞, where a∗= arg maxa∈[K] r(a) is the optimal action."
REFERENCES,0.33712512926577043,"Proof. Similar to the proof of Theorem 1, we define Ft to be the σ-algebra generated by a1, R1(a1),
· · · , at−1, Rt−1(at−1) i.e."
REFERENCES,0.33815925542916236,"Ft = σ({a1, R1(a1), · · · , at−1, Rt−1(at−1)}) .
(79)"
REFERENCES,0.3391933815925543,"Note that θt and It (defined by Eq. (23)) are Ft-measurable for all t ≥1. Let Et[·] denote the
conditional expectation with respect to Ft: Et[X] = E[X|Ft]. Define the following notations,"
REFERENCES,0.34022750775594623,"Wt(a) := θt(a) −Et−1[θt(a)],
(“noise”)
(80)
Pt(a) := Et[θt+1(a)] −θt(a).
(“progress”)
(81)"
REFERENCES,0.34126163391933817,"For each action a ∈[K], for t ≥2, we have the following decomposition,"
REFERENCES,0.3422957600827301,"θt(a) = Wt(a) + Pt−1(a) + θt−1(a).
(82)"
REFERENCES,0.34332988624612204,"By recursion we can determine that,"
REFERENCES,0.344364012409514,"θt(a) = E[θ1(a)] + t
X"
REFERENCES,0.3453981385729059,"s=1
Ws(a) + t−1
X"
REFERENCES,0.34643226473629785,"s=1
Ps(a),
(83)"
REFERENCES,0.3474663908996898,"while we also have,"
REFERENCES,0.3485005170630817,"θ1(a) = θ1(a) −E[θ1(a)]
|
{z
}
W1(a)"
REFERENCES,0.34953464322647365,"+E[θ1(a)],
(84)"
REFERENCES,0.3505687693898656,"and E[θ1(a)] accounts for potential randomness in initializing θ1 ∈RK. For an action a ∈[K],"
REFERENCES,0.3516028955532575,"A+(a) :=

a+ ∈[K] : r(a+) > r(a)
	
,
(85)"
REFERENCES,0.35263702171664946,"A−(a) :=

a−∈[K] : r(a−) < r(a)
	
.
(86)"
REFERENCES,0.3536711478800414,"Define A∞as the set of actions which are sampled for infinitely many times as t →∞, i.e.,"
REFERENCES,0.35470527404343327,"A∞:= {a ∈[K] | N∞(a) = ∞} .
(87)"
REFERENCES,0.3557394002068252,First part. We show that N∞(a∗) = ∞by contradiction.
REFERENCES,0.35677352637021714,"Suppose a∗̸∈A∞i.e. N∞(a∗) < ∞. According to Lemma 2, we have, |A∞| ≥2. Since a∗̸∈A∞
by assumption, there must be at least two other sub-optimal actions i1, i2 ∈[K], i1 ̸= i2, such that,"
REFERENCES,0.3578076525336091,"N∞(i1) = N∞(i2) = ∞.
(88)"
REFERENCES,0.358841778697001,"In particular, define"
REFERENCES,0.35987590486039295,"i1 := arg min
a∈[K],
N∞(a)=∞"
REFERENCES,0.3609100310237849,"r(a),
(89)"
REFERENCES,0.3619441571871768,"i2 := arg max
a∈[K],
N∞(a)=∞"
REFERENCES,0.36297828335056875,"r(a).
(90)"
REFERENCES,0.3640124095139607,"Using the update for arm i1, we know that, for all s ≥1,"
REFERENCES,0.3650465356773526,"Ps(i1) = η · πθs(i1) · (r(i1) −π⊤
θsr)
(91)"
REFERENCES,0.36608066184074456,"By definition and because of Assumption 1, we have that r(i1) < r(i2) < r(a∗). Furthermore,
according to Lemma 6, we have, for sufficiently large τ ≥1,"
REFERENCES,0.3671147880041365,"r(i1) < π⊤
θtr < r(i2),
(92)
which implies that after some large enough τ ≥1, t
X"
REFERENCES,0.36814891416752843,"s=τ
Ps(i1) = t
X"
REFERENCES,0.36918304033092036,"s=τ
η · πθs(i1) · (r(i1) −π⊤
θsr)
(93)"
REFERENCES,0.3702171664943123,"< 0.
(by Eq. (92))
(94)"
REFERENCES,0.37125129265770423,"r(i1) −π⊤
θsr =
X"
REFERENCES,0.37228541882109617,"a̸=i1
πθs(a) [r(i1) −r(a)] = −
X"
REFERENCES,0.3733195449844881,"a+∈A+(i1)
πθs(a+) · (r(a+) −r(i1)) +
X"
REFERENCES,0.37435367114788004,"a−∈A−(i1)
πθs(a−) · (r(i1) −r(a−)). (95)"
REFERENCES,0.375387797311272,"From Eq. (89), we have,
A∞⊆A+(i1) ∪{i1},
(96)"
REFERENCES,0.3764219234746639,"which implies that, for all a−∈A−(i1), we have,"
REFERENCES,0.37745604963805585,"N∞(a−) < ∞.
(97)"
REFERENCES,0.3784901758014478,"Note that, by definition, i2 ∈A+(i1), and
N∞(i2) = ∞.
(98)
In order to bound Eq. (95) using the above relations, note that,
X"
REFERENCES,0.3795243019648397,a−∈A−(i1)
REFERENCES,0.38055842812823165,"πθs(a−)
P"
REFERENCES,0.3815925542916236,"a+∈A+(i1) πθs(a+) · (r(a+) −r(i1)) · (r(i1) −r(a−))
(99) <
X"
REFERENCES,0.3826266804550155,a−∈A−(i1)
REFERENCES,0.38366080661840746,"πθs(a−)
πθs(i2) · (r(i2) −r(i1)) · (r(i1) −r(a−))"
REFERENCES,0.3846949327817994,(Fewer terms in the denominator)
REFERENCES,0.3857290589451913,"By Lemma 5, for large enough τ ≥1, for all a−, πθs(a−)"
REFERENCES,0.38676318510858326,"πθs(i2) ≤
1
|A−(i1)| · r(i2)−r(i1)"
REFERENCES,0.3877973112719752,"r(i1)−r(a−). Hence, ≤
X"
REFERENCES,0.38883143743536713,a−∈A−(i1)
REFERENCES,0.38986556359875907,"1
2 ·
1
|A−(i1)| · r(i2) −r(i1)"
REFERENCES,0.390899689762151,r(i1) −r(a−) · r(i1) −r(a−)
REFERENCES,0.39193381592554294,"r(i2) −r(i1)
(100) = 1"
REFERENCES,0.3929679420889349,"2
(101) =⇒
X"
REFERENCES,0.3940020682523268,"a−∈A−(i1)
πθs(a−) · (r(i1) −r(a−)) ≤1 2 X"
REFERENCES,0.39503619441571874,"a+∈A+(i1)
πθs(a+) · (r(a+) −r(i1)).
(102)"
REFERENCES,0.3960703205791106,"Combining Eqs. (94), (95) and (102), we have, t
X"
REFERENCES,0.39710444674250256,"s=τ
Ps(i1) = t
X"
REFERENCES,0.3981385729058945,"s=τ
η · πθs(i1) · (r(i1) −π⊤
θsr)
(103) ≤−η 2 · t
X"
REFERENCES,0.39917269906928643,"s=τ
πθs(i1)
X"
REFERENCES,0.40020682523267836,"a+∈A+(i1)
πθs(a+) · (r(a+) −r(i1))
(104)"
REFERENCES,0.4012409513960703,"≤−η · ∆ 2
· t
X"
REFERENCES,0.40227507755946224,"s=τ
πθs(i1)
X"
REFERENCES,0.40330920372285417,"a+∈A+(i1)
πθs(a+) ,
(105) where"
REFERENCES,0.4043433298862461,"∆:=
min
i,j∈[K], i̸=j |r(i) −r(j)| > 0.
(by Assumption 1)
(106)"
REFERENCES,0.40537745604963804,"Bounding the variance for arm i1, we have that for all large enough τ ≥1,"
REFERENCES,0.40641158221303,"Vt(i1) := 5 18 · t−1
X"
REFERENCES,0.4074457083764219,"s=τ
πθs(i1) · (1 −πθs(i1))
(107) = 5 18 · t−1
X"
REFERENCES,0.40847983453981385,"s=τ
πθs(i1) ·  
X"
REFERENCES,0.4095139607032058,"a−∈A−(i1)
πθs(a−) +
X"
REFERENCES,0.4105480868665977,"a+∈A+(i1)
πθs(a+) "
REFERENCES,0.41158221302998965,"
(108)"
REFERENCES,0.4126163391933816,"Using similar calculations as for the progress term, we have, X"
REFERENCES,0.4136504653567735,a−∈A−(i1)
REFERENCES,0.41468459152016546,"πθs(a−)
P"
REFERENCES,0.4157187176835574,"a+∈A+(i1) πθs(a+) <
X"
REFERENCES,0.41675284384694933,a−∈A−(i1)
REFERENCES,0.41778697001034126,πθs(a−)
REFERENCES,0.4188210961737332,"πθs(i2)
(Fewer terms in the denominator)"
REFERENCES,0.41985522233712513,"By Lemma 5, for large enough τ ≥1, for all a−, πθs(a−)"
REFERENCES,0.42088934850051707,πθs(i2) ≤13
REFERENCES,0.421923474663909,"5
1
|A−(i1)|. Hence, ≤
X"
REFERENCES,0.42295760082730094,a−∈A−(i1) 13
REFERENCES,0.4239917269906929,"5 ·
1
|A−(i1)|
(109) = 13"
REFERENCES,0.4250258531540848,"5
(110) =⇒
X"
REFERENCES,0.42605997931747674,"a−∈A−(i1)
πθs(a−) ≤13 5 X"
REFERENCES,0.4270941054808687,"a+∈A+(i1)
πθs(a+)
(111)"
REFERENCES,0.4281282316442606,"Combining Eqs. (108) and (111), we have that for large enough τ ≥1,"
REFERENCES,0.42916235780765255,"Vt(i1) = 5 18 · t−1
X"
REFERENCES,0.4301964839710445,"s=τ
πθs(i1) · (1 −πθs(i1))
(112) ≤ t−1
X"
REFERENCES,0.4312306101344364,"s=τ
πθs(i1)
X"
REFERENCES,0.43226473629782836,"a+∈A+(i1)
πθs(a+).
(113)"
REFERENCES,0.4332988624612203,"According to Lemma 12 (using Eqs. (94), (105) and (113)), we have, almost surely,"
REFERENCES,0.4343329886246122,"sup
t≥1
θt(i1) < ∞.
(114)"
REFERENCES,0.43536711478800416,"Since N∞(a∗) < ∞by assumption, and according to Lemma 1, we have, almost surely,"
REFERENCES,0.4364012409513961,"inf
t≥1 θt(a∗) > −∞.
(115)"
REFERENCES,0.43743536711478803,"Combining Eqs. (114) and (115), we have,"
REFERENCES,0.4384694932781799,"sup
t≥1"
REFERENCES,0.43950361944157185,"πθt(i1)
πθt(a∗) = sup
t≥1
exp{θt(i1) −θt(a∗)} < ∞.
(116)"
REFERENCES,0.4405377456049638,"On the other hand, by Lemma 5, we have,"
REFERENCES,0.4415718717683557,"sup
t≥1"
REFERENCES,0.44260599793174765,"πθt(i1)
πθt(a∗) = ∞
(117)"
REFERENCES,0.4436401240951396,"which contradicts Eq. (116). Hence, the assumption that N∞(a∗) < ∞cannot hold. This completes
the proof by contradiction, and implies that N∞(a∗) = ∞."
REFERENCES,0.4446742502585315,"Second part. With a∗∈A∞i.e. N∞(a∗) = ∞, we now argue that πθt(a∗) →1 as t →∞almost
surely."
REFERENCES,0.44570837642192346,"According to Lemma 2, we have, |A∞| ≥2. Since a∗∈A∞by assumption, there must be at least
one sub-optimal action i1 ∈[K] with r(i1) < r(a∗), such that,"
REFERENCES,0.4467425025853154,"N∞(i1) = ∞.
(118)"
REFERENCES,0.44777662874870733,"In particular, define"
REFERENCES,0.44881075491209926,"i1 := arg min
a∈[K],
N∞(a)=∞"
REFERENCES,0.4498448810754912,"r(a).
(119)"
REFERENCES,0.45087900723888313,"According to Lemma 6, we have, for all sufficiently large τ ≥1,"
REFERENCES,0.45191313340227507,"r(i1) < π⊤
θtr < r(a∗).
(120)"
REFERENCES,0.452947259565667,"Using the same arguments as in the first part (except that i2 in Eq. (98) is replaced with a∗), both
Eqs. (105) and (113) hold."
REFERENCES,0.45398138572905894,"Next, we argue that Pt
s=τ πθs(i1) P"
REFERENCES,0.4550155118924509,a+∈A+(i1) πθs(a+) →∞as t →∞by contradiction.
REFERENCES,0.4560496380558428,"Suppose ∞
X"
REFERENCES,0.45708376421923474,"t=τ
πθt(i1)
X"
REFERENCES,0.4581178903826267,"a+∈A+(i1)
πθt(a+) < ∞.
(121)"
REFERENCES,0.4591520165460186,"According to Lemma 8 (using Eqs. (94), (105), (113) and (121)), we have,"
REFERENCES,0.46018614270941055,"sup
t≥1
|θt(i1)| < ∞.
(122)"
REFERENCES,0.4612202688728025,"Calculating the progress and variance for arm a∗, for t ≥1,"
REFERENCES,0.4622543950361944,"Pt(a∗) = η · πθt(a∗) · (r(a∗) −π⊤
θtr)
(123)"
REFERENCES,0.46328852119958636,"≥η · ∆· πθt(a∗) ·
 
1 −πθt(a∗)

.
 
∆:= r(a∗) −max
a̸=a∗r(a)

(124)"
REFERENCES,0.4643226473629783,"≥0.
(125)"
REFERENCES,0.4653567735263702,"Denote that, for all t ≥1,"
REFERENCES,0.46639089968976216,"Vt(a∗) := 5 18 · t−1
X"
REFERENCES,0.4674250258531541,"s=1
πθs(a∗) · (1 −πθs(a∗)).
(126)"
REFERENCES,0.46845915201654603,"According to Lemma 11 (using Eqs. (123) and (126)), we have,"
REFERENCES,0.46949327817993797,"inf
t≥1 θt(a∗) > −∞.
(127)"
REFERENCES,0.4705274043433299,"Combining Eqs. (122) and (127), we have,"
REFERENCES,0.47156153050672184,"sup
t≥1"
REFERENCES,0.4725956566701138,"πθt(i1)
πθt(a∗) = sup
t≥1
exp{θt(i1) −θt(a∗)} < ∞,
(128)"
REFERENCES,0.4736297828335057,"For all t ≥1, |θt(i1)| < ∞and since there is at least one arm (a∗) s.t. inft≥1 θt(a) > −∞, there
exists ϵ > 0 and ϵ ∈O(1), such that,"
REFERENCES,0.47466390899689764,"sup
t≥1
πθt(i1) < 1 −2 ϵ.
(129)"
REFERENCES,0.4756980351602896,"According to Eq. (118), we know that N∞(i1) = ∞and for all a−∈A−(i1), N∞(a−) < ∞."
REFERENCES,0.4767321613236815,"Using Lemma 5, we have, for all large enough t ≥1, πθt(a−)"
REFERENCES,0.47776628748707345,"πθt(i1) <
ϵ
|A−(i1)|."
REFERENCES,0.4788004136504654,"πθt(i1) +
X"
REFERENCES,0.47983453981385726,"a+∈A+(i1)
πθt(a+) = 1 −πθt(i1)
X"
REFERENCES,0.4808686659772492,a−∈A−(i1)
REFERENCES,0.48190279214064113,πθt(a−)
REFERENCES,0.48293691830403307,"πθt(i1) > 1 −πθt(i1) ϵ
(130)"
REFERENCES,0.483971044467425,"=⇒πθt(i1) +
X"
REFERENCES,0.48500517063081694,"a+∈A+(i1)
πθt(a+) ≥1 −ϵ.
(131)"
REFERENCES,0.4860392967942089,"Combining Eqs. (129) and (131), we have, for all large enough t ≥1,
X"
REFERENCES,0.4870734229576008,"a+∈A+(i1)
πθt(a+) ≥ϵ,
(132)"
REFERENCES,0.48810754912099275,"which implies that, ∞
X"
REFERENCES,0.4891416752843847,"s=τ
πθs(i1)
X"
REFERENCES,0.4901758014477766,"a+∈A+(i1)
πθs(a+) ≥ϵ · ∞
X"
REFERENCES,0.49120992761116855,"s=τ
πθs(i1)
(133)"
REFERENCES,0.4922440537745605,"= ∞,
(since N∞(i1) = ∞and by Lemma 3)
(134)"
REFERENCES,0.4932781799379524,"which contradicts the assumption of Eq. (121). This completes the proof by contradiction, and
therefore, we have, t
X"
REFERENCES,0.49431230610134436,"s=τ
πθs(i1)
X"
REFERENCES,0.4953464322647363,"a+∈A+(i1)
πθs(a+) →∞, as t →∞.
(135)"
REFERENCES,0.4963805584281282,"According to Lemma 10 (using Eqs. (105), (113) and (135)), we have, almost surely,"
REFERENCES,0.49741468459152016,"θt(i1) →−∞, as t →∞.
(136)"
REFERENCES,0.4984488107549121,"Combining Eqs. (127) and (136), we have, almost surely,"
REFERENCES,0.49948293691830403,πθt(a∗)
REFERENCES,0.500517063081696,"πθt(i1) = exp{θt(a∗) −θt(i1)} →∞, as t →∞.
(137)"
REFERENCES,0.5015511892450879,"Hence, we have proved that if N∞(i1) = ∞and r(i1) < π⊤
θtr < r(a∗) for sufficiently large τ ≥1,"
REFERENCES,0.5025853154084798,"then, πθt(a∗)"
REFERENCES,0.5036194415718718,"πθt(i1) →∞, as t →∞."
REFERENCES,0.5046535677352637,"In order to use this argument recursively, consider sorting the action indices in A∞according to their
descending expected reward values,"
REFERENCES,0.5056876938986556,"r(a∗) > r(i|A∞|−1) > r(i|A∞|−2) > · · · > r(i2) > r(i1).
(138)"
REFERENCES,0.5067218200620476,"We know that,"
REFERENCES,0.5077559462254395,"π⊤
θtr −r(i2) =
X"
REFERENCES,0.5087900723888314,"a̸=i2
πθt(a) · (r(a) −r(i2))
(139) =
X"
REFERENCES,0.5098241985522234,"a−∈A−(i2)
πθt(a−) · (r(a−) −r(i2)) +
X"
REFERENCES,0.5108583247156153,"a+∈A+(i2)
πθt(a+) · (r(a+) −r(i2)) (140)"
REFERENCES,0.5118924508790073,"> πθt(a∗) · (r(a∗) −r(i2)) −
X"
REFERENCES,0.5129265770423992,"a−∈A−(i2)
πθt(a−) · (r(i2) −r(a−))
(141)"
REFERENCES,0.5139607032057911,"= πθt(a∗) ·

r(a∗) −r(i2) −
X"
REFERENCES,0.5149948293691831,a−∈A−(i2)
REFERENCES,0.516028955532575,πθt(a−)
REFERENCES,0.5170630816959669,"πθt(a∗) · (r(i2) −r(a−))

,
(142)"
REFERENCES,0.5180972078593589,"According to Lemma 5, for all a−∈A−(i2) with a−̸= i1, we have,"
REFERENCES,0.5191313340227508,"πθt(a∗)
πθt(a−) →∞, as t →∞.
(143)"
REFERENCES,0.5201654601861427,"Combining Eqs. (137) and (143), we have, for all a−∈A−(i2),"
REFERENCES,0.5211995863495347,"πθt(a∗)
πθt(a−) →∞, as t →∞.
(144)"
REFERENCES,0.5222337125129266,"Hence, for all sufficiently large τ ≥1, for all a−∈A−(i2),"
REFERENCES,0.5232678386763185,πθt(a−)
REFERENCES,0.5243019648397105,"πθt(a∗) ≤
1
2 |A−(i2)|
r(a∗) −r(i2)
r(i2) −r(a−).
(145)"
REFERENCES,0.5253360910031024,"Combining Eqs. (142) and (145), we have, for all sufficiently large t ≥1,"
REFERENCES,0.5263702171664943,"π⊤
θtr −r(i2) > πθt(a∗) · r(a∗) −r(i2)"
REFERENCES,0.5274043433298863,"2
> 0.
(146)"
REFERENCES,0.5284384694932782,"Hence we have, for all sufficiently large τ ≥1,"
REFERENCES,0.5294725956566702,"r(i2) < π⊤
θtr < r(a∗)
(147)"
REFERENCES,0.5305067218200621,"Comparing Eqs. (120) and (147), we can use a similar argument for i2 and conclude that, for
sufficiently large τ ≥1, then, πθt(a∗)"
REFERENCES,0.531540847983454,"πθt(i2) →∞, as t →∞. This further implies that"
REFERENCES,0.532574974146846,"r(i3) < π⊤
θtr < r(a∗).
(148)"
REFERENCES,0.5336091003102379,"Continuing this recursive argument, we have, for all actions a ∈A∞with a ̸= a∗,"
REFERENCES,0.5346432264736298,πθt(a∗)
REFERENCES,0.5356773526370218,"πθt(a) →∞, as t →∞.
(149)"
REFERENCES,0.5367114788004137,"Meanwhile, according to Lemma 5, we have, for all actions a ̸∈A∞,"
REFERENCES,0.5377456049638056,πθt(a∗)
REFERENCES,0.5387797311271976,"πθt(a) →∞, as t →∞.
(150)"
REFERENCES,0.5398138572905895,"Combining Eqs. (149) and (150), we have, for all sub-optimal actions a ∈[K] with r(a) < r(a∗),"
REFERENCES,0.5408479834539814,πθt(a∗)
REFERENCES,0.5418821096173733,"πθt(a) →∞, as t →∞.
(151)"
REFERENCES,0.5429162357807652,"Finally, note that,"
REFERENCES,0.5439503619441571,"πθt(a∗) =
πθt(a∗)
P"
REFERENCES,0.5449844881075491,"a∈[K]: r(a)<r(a∗) πθt(a) + πθt(a∗)
(152) =
1
P"
REFERENCES,0.546018614270941,"a∈[K]: r(a)<r(a∗)
πθt(a)
πθt(a∗) + 1
.
(153)"
REFERENCES,0.5470527404343329,"Combining Eqs. (151) and (153), we have, almost surely,"
REFERENCES,0.5480868665977249,"πθt(a∗) →1, as t →∞."
REFERENCES,0.5491209927611168,"B
Miscellaneous Extra Supporting Results"
REFERENCES,0.5501551189245087,"Lemma 3 (Extended Borel-Cantelli Lemma, Corollary 5.29 of [5]). Let (Fn)n≥1 be a filtration,
An ∈Fn. Then, almost surely,"
REFERENCES,0.5511892450879007,"{ω : ω ∈An infinitely often } = ( ω : ∞
X"
REFERENCES,0.5522233712512926,"n=1
P(An|Fn) = ∞ )"
REFERENCES,0.5532574974146846,".
(154)"
REFERENCES,0.5542916235780765,"Lemma 4 (Freedman’s inequality [10, 7], Theorem C.3 of [24]). Let X1, X2, . . . be a sequence of
random variables, such that for all t ≥1, |Xt| ≤1/2. Define Sn :=  n
X"
REFERENCES,0.5553257497414684,"t=1
E[Xt|X1, . . . , Xt−1] −Xt"
REFERENCES,0.5563598759048604,"and
Vn := n
X"
REFERENCES,0.5573940020682523,"t=1
Var[Xt|X1, . . . , Xt−1].
(155)"
REFERENCES,0.5584281282316442,"Then, for all δ > 0, Pr "
REFERENCES,0.5594622543950362,∃n : Sn ≥6
REFERENCES,0.5604963805584281,"s
Vn + 4 3"
REFERENCES,0.56153050672182,"
log
Vn + 1 δ"
REFERENCES,0.562564632885212,"
+ 2 log
1 δ 
+ 4"
REFERENCES,0.5635987590486039,3 log 3 !
REFERENCES,0.5646328852119958,"≤δ.
(156)"
REFERENCES,0.5656670113753878,"Lemma 5. Using Algorithm 1, for any two different actions i, j ∈[K] with i ̸= j, if N∞(i) = ∞
and N∞(j) < ∞, then we have, almost surely,"
REFERENCES,0.5667011375387797,"sup
t≥1"
REFERENCES,0.5677352637021716,"πθt(i)
πθt(j) = ∞.
(157)"
REFERENCES,0.5687693898655636,Proof. We prove the result by contradiction. Suppose
REFERENCES,0.5698035160289555,"c := sup
t≥1"
REFERENCES,0.5708376421923474,"πθt(i)
πθt(j) < ∞.
(158)"
REFERENCES,0.5718717683557394,"According to the extended Borel-Cantelli Lemma 3, we have, for all a ∈[K], almost surely,
n X"
REFERENCES,0.5729058945191313,"t≥1
πθt(j) = ∞
o
= {N∞(j) = ∞} .
(159)"
REFERENCES,0.5739400206825233,"Hence, taking complements, we have,
n X"
REFERENCES,0.5749741468459152,"t≥1
πθt(j) < ∞
o
= {N∞(j) < ∞}
(160)"
REFERENCES,0.5760082730093071,"also holds almost surely, which implies that, ∞
X"
REFERENCES,0.5770423991726991,"t=1
πθt(i) = ∞, and
(161) ∞
X"
REFERENCES,0.578076525336091,"t=1
πθt(j) < ∞.
(162)"
REFERENCES,0.5791106514994829,"Therefore, we have, ∞
X"
REFERENCES,0.5801447776628749,"t=1
πθt(i) = ∞
X"
REFERENCES,0.5811789038262668,"t=1
πθt(j) · πθt(i)"
REFERENCES,0.5822130299896587,"πθt(j)
(163) ≤c · ∞
X"
REFERENCES,0.5832471561530507,"t=1
πθt(j)
(164)"
REFERENCES,0.5842812823164426,"< ∞,
(165)"
REFERENCES,0.5853154084798345,which is a contradiction with Eq. (161).
REFERENCES,0.5863495346432265,"Lemma 6. Using Algorithm 1, we have, almost surely, for all large enough t ≥1,"
REFERENCES,0.5873836608066184,"r(i1) < π⊤
θtr < r(i2),
(166)"
REFERENCES,0.5884177869700103,"where i1, i2 ∈[K] and i1 ̸= i2 are the action indices defined as,"
REFERENCES,0.5894519131334023,"i1 := arg min
a∈[K],
N∞(a)=∞"
REFERENCES,0.5904860392967942,"r(a),
(167)"
REFERENCES,0.5915201654601862,"i2 := arg max
a∈[K],
N∞(a)=∞"
REFERENCES,0.5925542916235781,"r(a).
(168)"
REFERENCES,0.59358841778697,"Proof. Define A∞as the set of actions which are sampled for infinitely many times as t →∞, i.e.,"
REFERENCES,0.594622543950362,"A∞:= {a ∈[K] | N∞(a) = ∞} .
(169)"
REFERENCES,0.5956566701137539,"According to Lemma 2, we have |A∞| ≥2, which implies that i1 ̸= i2."
REFERENCES,0.5966907962771458,"Given any sub-optimal action i ∈[K] with r(i) < r(a∗), we partition the remaining actions into two
parts using r(i)."
REFERENCES,0.5977249224405378,"A+(i) :=

a+ ∈[K] : r(a+) > r(i)
	
,
(170)"
REFERENCES,0.5987590486039297,"A−(i) :=

a−∈[K] : r(a−) < r(i)
	
.
(171)"
REFERENCES,0.5997931747673216,"By definition, we have,"
REFERENCES,0.6008273009307136,"A∞⊆A+(i1) ∪{i1}, and
(172)"
REFERENCES,0.6018614270941055,"A∞⊆A−(i2) ∪{i2}.
(173)"
REFERENCES,0.6028955532574974,"First part. r(i1) < π⊤
θtr."
REFERENCES,0.6039296794208894,"If r(i1) = mina∈[K] r(a), i.e., i1 is the “worst action”, then r(i1) < π⊤
θtr holds trivially. Otherwise,
suppose r(i1) ̸= mina∈[K] r(a). We have,"
REFERENCES,0.6049638055842813,"π⊤
θtr −r(i1) =
X"
REFERENCES,0.6059979317476732,"a+∈A+(i1)
πθt(a+) · (r(a+) −r(i1)) −
X"
REFERENCES,0.6070320579110652,"a−∈A−(i1)
πθt(a−) · (r(i1) −r(a−)). (174)"
REFERENCES,0.6080661840744571,"Consider the non-empty set A∞∩A+(i1). Pick an action j1 ∈A∞∩A+(i1), and ignore all the
other actions a+ ∈A+(i1) with a+ ̸= j1 in the above equation. We have,"
REFERENCES,0.609100310237849,"π⊤
θtr −r(i1) > πθt(j1) · (r(j1) −r(i1)) −
X"
REFERENCES,0.610134436401241,"a−∈A−(i1)
πθt(a−) · (r(i1) −r(a−))
(175)"
REFERENCES,0.6111685625646329,"= πθt(j1) ·

r(j1) −r(i1) −
X"
REFERENCES,0.6122026887280249,a−∈A−(i1)
REFERENCES,0.6132368148914168,πθt(a−)
REFERENCES,0.6142709410548087,"πθt(j1) · (r(i1) −r(a−))

,
(176)"
REFERENCES,0.6153050672182007,"where the first inequality is because of r(a+) −r(i1) > 0 for all a+ ∈A+(i1) by Eq. (170). Note
that N∞(j1) = ∞and N∞(a−) < ∞. According to Lemma 5, we have, for all large enough t ≥1, X"
REFERENCES,0.6163391933815926,a−∈A−(i1)
REFERENCES,0.6173733195449845,πθt(a−)
REFERENCES,0.6184074457083765,"πθt(j1) · (r(i1) −r(a−)) =
X"
REFERENCES,0.6194415718717684,"a−∈A−(i1)
(r(i1) −r(a−))
. πθt(j1)"
REFERENCES,0.6204756980351603,"πθt(a−)
(177) <
X"
REFERENCES,0.6215098241985523,"a−∈A−(i1)
(r(i1) −r(a−)) ·
1
A−(i1)
 · r(j1) −r(i1)"
REFERENCES,0.6225439503619442,r(i1) −r(a−) · 1
REFERENCES,0.6235780765253361,"2
(178)"
REFERENCES,0.6246122026887281,= r(j1) −r(i1)
REFERENCES,0.6256463288521199,"2
.
(179)"
REFERENCES,0.6266804550155118,"Combining Eqs. (175) and (177), we have,"
REFERENCES,0.6277145811789038,"π⊤
θtr −r(i1) > πθt(j1) · r(j1) −r(i1)"
REFERENCES,0.6287487073422957,"2
> 0,
(180)"
REFERENCES,0.6297828335056876,where the last inequality is because j1 ∈A+(i1).
REFERENCES,0.6308169596690796,"Second part. π⊤
θtr < r(i2)."
REFERENCES,0.6318510858324715,"The arguments are similar to the first part. If r(i2) = r(a∗), i.e., i2 is the optimal action, then
π⊤
θtr < r(i2) holds trivially. Otherwise, suppose r(i2) ̸= r(a∗). We have,"
REFERENCES,0.6328852119958635,"r(i2) −π⊤
θtr =
X"
REFERENCES,0.6339193381592554,"a−∈A−(i2)
πθt(a−) · (r(i2) −r(a−)) −
X"
REFERENCES,0.6349534643226473,"a+∈A+(i2)
πθt(a+) · (r(a+) −r(i2)). (181)"
REFERENCES,0.6359875904860393,"Consider the non-empty set A∞∩A−(i2). Pick an action j2 ∈A∞∩A−(i2), and ignore all the
other actions a−∈A−(i2) with a−̸= j2 in the above equation. We have,"
REFERENCES,0.6370217166494312,"r(i2) −π⊤
θtr ≥πθt(j2) · (r(i2) −r(j2)) −
X"
REFERENCES,0.6380558428128231,"a+∈A+(i2)
πθt(a+) · (r(a+) −r(i2))
(182)"
REFERENCES,0.6390899689762151,"= πθt(j2) ·

r(i2) −r(j2) −
X"
REFERENCES,0.640124095139607,a+∈A+(i2)
REFERENCES,0.6411582213029989,πθt(a+)
REFERENCES,0.6421923474663909,"πθt(j2) · (r(a+) −r(i2))

,
(183)"
REFERENCES,0.6432264736297828,"where the first inequality is because of r(i2) −r(a−) > 0 for all a−∈A−(i2) by Eq. (170). Note
that N∞(j2) = ∞and N∞(a+) < ∞. According to Lemma 5, we have, for all large enough t ≥1, X"
REFERENCES,0.6442605997931747,a+∈A+(i2)
REFERENCES,0.6452947259565667,πθt(a+)
REFERENCES,0.6463288521199586,"πθt(j2) · (r(a+) −r(i2)) =
X"
REFERENCES,0.6473629782833505,"a+∈A+(i2)
(r(a+) −r(i2))
. πθt(j2)"
REFERENCES,0.6483971044467425,"πθt(a+)
(184) <
X"
REFERENCES,0.6494312306101344,"a+∈A+(i2)
(r(a+) −r(i2)) ·
1
A+(i2)
 · r(i2) −r(j2)"
REFERENCES,0.6504653567735263,r(a+) −r(i2) · 1
REFERENCES,0.6514994829369183,"2
(185)"
REFERENCES,0.6525336091003102,= r(i2) −r(j2)
REFERENCES,0.6535677352637022,"2
.
(186)"
REFERENCES,0.6546018614270941,"Combining Eqs. (182) and (184), we have,"
REFERENCES,0.655635987590486,"r(i2) −π⊤
θtr ≥πθt(j2) · r(i2) −r(j2)"
REFERENCES,0.656670113753878,"2
> 0,
(187)"
REFERENCES,0.6577042399172699,where the last inequality is because j2 ∈A−(i2).
REFERENCES,0.6587383660806618,"For the following lemmas, we will use the notation defined in the proofs for Theorems 1 and 2.
Lemma 7 (Concentration of noise). Given an action a ∈[K]. We have, with probability at least
1 −δ, ∀t :  t
X"
REFERENCES,0.6597724922440538,"s=1
Ws+1(a)"
REFERENCES,0.6608066184074457,≤36 η Rmax s
REFERENCES,0.6618407445708376,"(Vt(a) + 4/3) log
Vt(a) + 1 δ"
REFERENCES,0.6628748707342296,"
+ 12 η Rmax log(1/δ) + 8 η Rmax log 3, (188) where"
REFERENCES,0.6639089968976215,"Vt(a) := 5 18 · t
X"
REFERENCES,0.6649431230610134,"s=1
πθs(a) · (1 −πθs(a)),
(189) and"
REFERENCES,0.6659772492244054,"Wt(a) := θt(a) −Et−1[θt(a)]
(190)"
REFERENCES,0.6670113753877973,is originally defined by Eq. (52).
REFERENCES,0.6680455015511892,"Proof. First, note that,"
REFERENCES,0.6690796277145812,"Et[Wt+1(a)] = 0, for all t ≥0.
(191)"
REFERENCES,0.6701137538779731,"Using the update Eq. (22), we have,"
REFERENCES,0.671147880041365,"Wt+1(a) = θt+1(a) −Et[θt+1(a)]
(192)"
REFERENCES,0.672182006204757,"= θt(a) + η · (It(a) −πθt(a)) · Rt(at) −
 
θt(a) + η · πθt(a) ·
 
r(a) −π⊤
θtr

(193)"
REFERENCES,0.6732161323681489,"= η · (It(a) −πθt(a)) · Rt(at) −η · πθt(a) ·
 
r(a) −π⊤
θtr

.
(194)"
REFERENCES,0.6742502585315409,"According to Eq. (1), we have,"
REFERENCES,0.6752843846949328,"|Wt+1(a)| ≤3 η · Rmax.
(195)"
REFERENCES,0.6763185108583247,"The conditional variance of noise is,"
REFERENCES,0.6773526370217167,"Var[Wt+1(a)|Ft] := Et[(Wt+1(a))2]
(196)"
REFERENCES,0.6783867631851086,"≤2 η2 · Et[(It(a) −πθt(a))2 · Rt(at)2] + 2 η2 · πθt(a)2 ·
 
r(a) −π⊤
θtr
2 ,
(197)"
REFERENCES,0.6794208893485005,"where the inequality is by (a + b)2 ≤2a2 + 2b2. Next, we have,"
REFERENCES,0.6804550155118925,"Et[(It(a) −πθt(a))2 · Rt(at)2] = πt(a) · (1 −πt(a))2 · r(a)2 +
X"
REFERENCES,0.6814891416752844,"a′̸=a
πθt(a′) · πt(a)2 · r(a′)2 (198)"
REFERENCES,0.6825232678386763,"≤R2
max ·

πt(a) · (1 −πt(a))2 + (1 −πt(a)) · πt(a)2
(199)"
REFERENCES,0.6835573940020683,"= R2
max · πθt(a) · (1 −πθt(a)),
(200) and,"
REFERENCES,0.6845915201654602,"r(a) −π⊤
θtr
 =

X"
REFERENCES,0.6856256463288521,"a′̸=a
πθt(a′) · (r(a) −r(a′))

(201) ≤
X"
REFERENCES,0.6866597724922441,"a′̸=a
πθt(a′) · |r(a) −r(a′)|
(202)"
REFERENCES,0.687693898655636,"≤2 Rmax ·
X"
REFERENCES,0.688728024819028,"a′̸=a
πθt(a′)
(203)"
REFERENCES,0.6897621509824199,"= 2 Rmax · (1 −πθt(a)) .
(204)"
REFERENCES,0.6907962771458118,"Combining Eqs. (196), (198) and (201), we have,"
REFERENCES,0.6918304033092038,"Var[Wt+1(a)|Ft] ≤2 η2 · R2
max · πθt(a) · (1 −πθt(a)) + 8 η2 · R2
max · πθt(a)2 · (1 −πθt(a))2 (205)"
REFERENCES,0.6928645294725957,"≤10 η2 · R2
max · πθt(a) · (1 −πθt(a)).
(206)"
REFERENCES,0.6938986556359876,Let Xt+1(a) := Wt+1(a)
REFERENCES,0.6949327817993796,"6 η·Rmax . Then we have,"
REFERENCES,0.6959669079627715,"|Xt+1(a)| ≤1/2, and
(207)"
REFERENCES,0.6970010341261634,Var[Xt+1(a)|Ft] ≤5
REFERENCES,0.6980351602895554,"18 · πθt(a) · (1 −πθt(a)).
(208)"
REFERENCES,0.6990692864529473,"According to Lemma 4, there exists an event E1 such that Pr(E1) ≥1 −δ, and when E1 holds, ∀t :  t
X"
REFERENCES,0.7001034126163392,"s=1
Xs+1(a) ≤6 s"
REFERENCES,0.7011375387797312,"(Vt(a) + 4/3) log
Vt(a) + 1 δ"
REFERENCES,0.7021716649431231,"
+ 2 log(1/δ) + 4"
REFERENCES,0.703205791106515,"3 log 3,
(209)"
REFERENCES,0.704239917269907,"which implies that, ∀t :  t
X"
REFERENCES,0.7052740434332989,"s=1
Ws+1(a)"
REFERENCES,0.7063081695966908,≤36 η Rmax s
REFERENCES,0.7073422957600828,"(Vt(a) + 4/3) log
Vt(a) + 1 δ"
REFERENCES,0.7083764219234746,"
+ 12 η Rmax log(1/δ) + 8 η Rmax log 3, (210)"
REFERENCES,0.7094105480868665,"where Vt(a) :=
5
18 · Pt
s=1 πθs(a) · (1 −πθs(a))."
REFERENCES,0.7104446742502585,"Lemma 8 (Bounded progress). For any action a ∈[K] with N∞(a) = ∞, if there exists c > 0 and
τ < ∞, such that, for all t ≥τ, t
X s=1"
REFERENCES,0.7114788004136504,"Ps(a)
 ≥c · Vt(a),
(211)"
REFERENCES,0.7125129265770423,"where Vt(a) is defined in Eq. (189), and if also ∞
X t=1"
REFERENCES,0.7135470527404343,"Pt(a)
 < ∞,
(212)"
REFERENCES,0.7145811789038262,"then we have, almost surely,"
REFERENCES,0.7156153050672182,"sup
t≥1
|θt(a)| < ∞.
(213)"
REFERENCES,0.7166494312306101,"Proof. According to Eq. (55) and triangle inequality, we have,"
REFERENCES,0.717683557394002,"θt(a)
 ≤
E[θ1(a)]
 + t
X"
REFERENCES,0.718717683557394,"s=1
Ws(a)
 + t−1
X"
REFERENCES,0.7197518097207859,"s=1
Ps(a)
.
(214)"
REFERENCES,0.7207859358841778,"According to Lemma 7, there exists an event E1, such that Pr(E1) ≥1 −δ, and when E1 holds, we
have, for all t ≥τ + 1, t
X"
REFERENCES,0.7218200620475698,"s=1
Ws(a)
 ≤36 η Rmax s"
REFERENCES,0.7228541882109617,"(Vt−1(a) + 4/3) · log
Vt−1(a) + 1 δ"
REFERENCES,0.7238883143743536,"
+ 12 η Rmax log(1/δ) + 8 η Rmax log 3. (215)"
REFERENCES,0.7249224405377456,"According to Eq. (212), as t →∞, t−1
X"
REFERENCES,0.7259565667011375,"s=1
Ps(a)
 ≤ t−1
X s=1"
REFERENCES,0.7269906928645294,"Ps(a)
 < ∞.
(216)"
REFERENCES,0.7280248190279214,"According to Eqs. (211), (215) and (216), we have, t
X"
REFERENCES,0.7290589451913133,"s=1
Ws(a)
 < ∞.
(217)"
REFERENCES,0.7300930713547052,"Combining Eqs. (214), (216) and (217), we have, as t →∞,"
REFERENCES,0.7311271975180972,"|θt(a)| < ∞.
(218)"
REFERENCES,0.7321613236814891,"Take any ω ∈E := {N∞(a) = ∞}. Because P(E \ (E ∩E1)) ≤P(Ω\ E1) ≤δ →0 as δ →0, we
have that P-almost surely for all ω ∈E there exists δ > 0 such that ω ∈E ∩E1 while Eq. (215) also
holds for this δ. Take such a δ. We have, almost surely,"
REFERENCES,0.733195449844881,"sup
t≥1
|θt(a)| < ∞."
REFERENCES,0.734229576008273,"Lemma 9 (Unbounded positive progress). For any action a ∈[K] with N∞(a) = ∞, if there exists
c > 0 and τ < ∞, such that, for all t ≥τ,"
REFERENCES,0.7352637021716649,"Pt(a) > 0, and
(219) t
X"
REFERENCES,0.7362978283350569,"s=τ
Ps(a) ≥c · Vt(a),
(220)"
REFERENCES,0.7373319544984488,"where Vt(a) is defined in Eq. (189), and if also, ∞
X"
REFERENCES,0.7383660806618407,"t=τ
Pt(a) = ∞,
(221)"
REFERENCES,0.7394002068252327,"then we have, almost surely,"
REFERENCES,0.7404343329886246,"θt(a) →∞, as t →∞.
(222)"
REFERENCES,0.7414684591520165,"Proof. According to Eq. (55),"
REFERENCES,0.7425025853154085,"θt(a) = E[θ1(a)] + t
X"
REFERENCES,0.7435367114788004,"s=1
Ws(a) + t−1
X"
REFERENCES,0.7445708376421923,"s=1
Ps(a).
(223)"
REFERENCES,0.7456049638055843,"According to Lemma 7, there exists an event E1, such that Pr(E1) ≥1 −δ, and when E1 holds, we
have, for all t ≥τ + 1, t
X"
REFERENCES,0.7466390899689762,"s=1
Ws(a) ≥−36 η Rmax s"
REFERENCES,0.7476732161323681,"(Vt−1(a) + 4/3) · log
Vt−1(a) + 1 δ "
REFERENCES,0.7487073422957601,"|
{z
}
♡"
REFERENCES,0.749741468459152,−12 η Rmax log(1/δ) −8 η Rmax log 3.
REFERENCES,0.750775594622544,"(224)
By Eq. (221), as t →∞, t−1
X"
REFERENCES,0.7518097207859359,"s=1
Ps(a) →∞,
(225)"
REFERENCES,0.7528438469493278,"and the speed of Pt−1
s=1 Ps(a) →∞is strictly faster than ♡→∞, according to Eq. (220). This
implies that θt(a) →∞, as a result of “cumulative progress” dominates “cumulative noise”."
REFERENCES,0.7538779731127198,"Take any ω ∈E := {N∞(a) = ∞}. Because P(E \ (E ∩E1)) ≤P(Ω\ E1) ≤δ →0 as δ →0, we
have that P-almost surely for all ω ∈E there exists δ > 0 such that ω ∈E ∩E1 while Eq. (224) also
holds for this δ. Take such a δ. We have, almost surely,
θt(a) →∞, as t →∞.
Lemma 10 (Unbounded negative progress). For any action a ∈[K] with N∞(a) = ∞, if there
exists c > 0 and τ < ∞, such that, for all t ≥τ,
Pt(a) < 0, and
(226) − t
X"
REFERENCES,0.7549120992761117,"s=τ
Ps(a) ≥c · Vt(a),
(227)"
REFERENCES,0.7559462254395036,"where Vt(a) is defined in Eq. (189), and if also, ∞
X"
REFERENCES,0.7569803516028956,"t=τ
Pt(a) = −∞,
(228)"
REFERENCES,0.7580144777662875,"then we have, almost surely,
θt(a) →−∞, as t →∞.
(229)"
REFERENCES,0.7590486039296794,Proof. The proof follows almost the same arguments for Lemma 9.
REFERENCES,0.7600827300930714,"Lemma 11 (Positive progress). For any action a ∈[K] with N∞(a) = ∞, if there exists c > 0 and
τ < ∞, such that, for all t ≥τ,
Pt(a) > 0, and
(230) t
X"
REFERENCES,0.7611168562564633,"s=τ
Ps(a) ≥c · Vt(a),
(231)"
REFERENCES,0.7621509824198552,"where Vt(a) is defined in Eq. (189), then we have, almost surely,
inf
t≥1 θt(a) > −∞.
(232)"
REFERENCES,0.7631851085832472,"Proof. First case: if P∞
t=1 Pt(a) < ∞, then according to Lemma 8, we have, almost surely,
sup
t≥1
|θt(a)| < ∞,
(233)"
REFERENCES,0.7642192347466391,which implies Eq. (232).
REFERENCES,0.765253360910031,"Second case: if P∞
t=1 Pt(a) = ∞, then according to Lemma 9, we have, almost surely,
θt(a) →∞, as t →∞,
(234)
which also implies Eq. (232)."
REFERENCES,0.766287487073423,"Lemma 12 (Negative progress). For any action a ∈[K] with N∞(a) = ∞, if there exists c > 0 and
τ < ∞, such that, for all t ≥τ,"
REFERENCES,0.7673216132368149,"Pt(a) < 0, and
(235) − t
X"
REFERENCES,0.7683557394002068,"s=τ
Ps(a) ≥c · Vt(a),
(236)"
REFERENCES,0.7693898655635988,"where Vt(a) is defined in Eq. (189), then we have, almost surely,"
REFERENCES,0.7704239917269907,"sup
t≥1
θt(a) < ∞.
(237)"
REFERENCES,0.7714581178903827,"Proof. First case: if −P∞
t=1 Pt(a) < ∞, then according to Lemma 8, we have, almost surely,"
REFERENCES,0.7724922440537746,"sup
t≥1
|θt(a)| < ∞,
(238)"
REFERENCES,0.7735263702171665,which implies Eq. (237).
REFERENCES,0.7745604963805585,"Second case: if −P∞
t=1 Pt(a) = ∞, then according to Lemma 10, we have, almost surely,"
REFERENCES,0.7755946225439504,"θt(a) →−∞, as t →∞,
(239)"
REFERENCES,0.7766287487073423,which also implies Eq. (237).
REFERENCES,0.7776628748707343,"C
Rate of Convergence"
REFERENCES,0.7786970010341262,"Theorem 3. For a large enough τ > 0, for all T > τ, the average sub-optimality decreases at an
O

ln(T )"
REFERENCES,0.7797311271975181,"T

rate. Formally, if a∗is the optimal arm, then, for a constant c"
REFERENCES,0.7807652533609101,"PT
s=τ r(a∗) −⟨πs, r⟩"
REFERENCES,0.781799379524302,"T
≤c ln(T) T −τ"
REFERENCES,0.7828335056876939,"Proof. The progress for the optimal action is,"
REFERENCES,0.7838676318510859,"Pt(a∗) = η · πθt(a∗) · (r(a∗) −π⊤
θtr)
(240)"
REFERENCES,0.7849017580144778,"≥η · ∆· πθt(a∗) ·
 
1 −πθt(a∗)

.
 
∆:= r(a∗) −max
a̸=a∗r(a)

(241)"
REFERENCES,0.7859358841778697,"≥0.
(242)"
REFERENCES,0.7869700103412617,"Since limt→∞πθt(a∗) = 1, we have for all large enough t ≥1,"
REFERENCES,0.7880041365046536,"πθt(a∗) ≥1/2.
(243)"
REFERENCES,0.7890382626680456,"Since N∞(a∗) = ∞(Theorem 2 ) and |A∞| ≥2 (Lemma 2), we have, ∞
X"
REFERENCES,0.7900723888314375,"t=1
(1 −πt(a∗)) ≥ ∞
X"
REFERENCES,0.7911065149948294,"t=1
πt(i1) = ∞,
(244)"
REFERENCES,0.7921406411582212,"where N∞(i1) = ∞and r(i1) < r(a∗). Therefore, we have, ∞
X"
REFERENCES,0.7931747673216132,"t=1
Pt(a∗) = ∞.
(245)"
REFERENCES,0.7942088934850051,"The variance of noise is,"
REFERENCES,0.795243019648397,"Vt(a∗) := 5 18 · t−1
X"
REFERENCES,0.796277145811789,"s=1
πθs(a∗) · (1 −πθs(a∗)),
(246)"
REFERENCES,0.7973112719751809,"which will be dominated by sum of Pt(a∗) since Vt(a∗) appears under square root (Lemma 7).
Therefore, for all large enough t ≥τ,"
REFERENCES,0.7983453981385729,"θt(a∗) ≥C · t
X"
REFERENCES,0.7993795243019648,"s=τ
(1 −πs(a∗)).
(247)"
REFERENCES,0.8004136504653567,"On the other hand, we argued recursively (in the proofs for Theorem 2) that for all sub-optimal action
a ∈[K] with r(a) < r(a∗),"
REFERENCES,0.8014477766287487,"sup
t≥1
θt(a) < ∞.
(248)"
REFERENCES,0.8024819027921406,"Therefore, we have, for all large enough t ≥τ,"
REFERENCES,0.8035160289555325,"θt(a∗) −θt(a) ≥C · t
X"
REFERENCES,0.8045501551189245,"s=τ
(1 −πs(a∗)),
(249)"
REFERENCES,0.8055842812823164,"which implies that, X"
REFERENCES,0.8066184074457083,"a̸=a∗
exp{θt(a) −θt(a∗)} ≤(K −1) · exp

−C · t
X"
REFERENCES,0.8076525336091003,"s=τ
(1 −πs(a∗))

.
(250)"
REFERENCES,0.8086866597724922,"Therefore, we have,"
REFERENCES,0.8097207859358841,1 −πt(a∗) ≤1 −πt(a∗)
REFERENCES,0.8107549120992761,"πt(a∗)
=
X a̸=a∗"
REFERENCES,0.811789038262668,"πt(a)
πt(a∗) ≤(K −1) · exp

−C · t−1
X"
REFERENCES,0.81282316442606,"s=τ
(1 −πs(a∗))

.
(251)"
REFERENCES,0.8138572905894519,"Using Lemma 15 with xn = Pt−1
s=τ (1 −πs(a∗)) > 0, xn+1 = Pt
s=τ (1 −πs(a∗)) > 0, c = C > 0
and B = K −1 ≥1 gives us that for all t > τ, t
X"
REFERENCES,0.8148914167528438,"s=τ
(1 −πs(a∗)) ≤1"
REFERENCES,0.8159255429162358,C ln(Ct + eCM) + π2
REFERENCES,0.8169596690796277,"12C ,
(252)"
REFERENCES,0.8179937952430196,"where M = max{K −1, 1"
REFERENCES,0.8190279214064116,"C ln((K −1) C)), (1 −πτ(a∗))} = K −1."
REFERENCES,0.8200620475698035,"Finally, we use Eq. (252) to bound the average sub-optimality. For any s ≥τ and T > τ,"
REFERENCES,0.8210961737331954,"r(a∗) −⟨πs, r⟩=
X"
REFERENCES,0.8221302998965874,"a̸=a∗
πs [r(a∗) −r(a)] ≤2 Rmax (1 −πs(a∗))."
REFERENCES,0.8231644260599793,"(Since r(a) ∈[−Rmax, Rmax])"
REFERENCES,0.8241985522233712,"Summing from s = τ to T,"
REFERENCES,0.8252326783867632,"=⇒
PT
s=τ r(a∗) −⟨πs, r⟩"
REFERENCES,0.8262668045501551,"T
≤
2 Rmax
h
1
C ln(C T + eCM) +
π2
12C
i"
REFERENCES,0.827300930713547,"T −τ
."
REFERENCES,0.828335056876939,"Lemma 13. Let {yn} be the solution to the difference equation yn+1 = yn + B e−cyn with B ≥1,
c > 0 and y0 ≥max(B, 1"
REFERENCES,0.8293691830403309,"c ln(B c)). Let {xn} be a nonnegative valued sequence such that x0 ≤y0
and xn+1 ≤xn + B e−cxn for all n ≥0. Then, xn ≤yn for all n ≥0."
REFERENCES,0.8304033092037229,"Proof. Define the function f(y) = max{x + B e−cx : 0 ≤x ≤y}. Clearly, f is an increasing
function of its argument."
REFERENCES,0.8314374353671148,"Claim: For y ≥max(B, 1"
REFERENCES,0.8324715615305067,"c ln(B c)), f(y) = y + B e−cy. To prove this claim, for x ∈R, define"
REFERENCES,0.8335056876938987,"g(x) := x + B e−cx .
(253)"
REFERENCES,0.8345398138572906,Function g is increasing on ( 1
REFERENCES,0.8355739400206825,"c ln(B c), ∞) and decreasing on (−∞, 1"
REFERENCES,0.8366080661840745,c ln(B c)).
REFERENCES,0.8376421923474664,"If c < 1/B, 1"
REFERENCES,0.8386763185108583,"c ln(Bc) < 0, hence g is increasing on (0, ∞). Hence, f(y) = g(y), proving the claim."
REFERENCES,0.8397104446742503,"If c ≥1/B, then 1"
REFERENCES,0.8407445708376422,"c ln(B c) ≥0. Hence, g is decreasing on (0, 1/c ln(B c)) and then increasing
on (1/c ln(B c)). Since y ≥1"
REFERENCES,0.8417786970010341,"c ln(B c), f(y) = max(g(0), g(y)). Since we also have y ≥B >
1
c ln(B c), g(y) ≥g(B) > B = g(0) and thus f(y) = g(y), finishing the proof of the claim."
REFERENCES,0.8428128231644261,"The difference equation for yn is yn+1 = yn + B e−cyn. Since y0 ≥max(B, 1"
REFERENCES,0.843846949327818,"c ln(B c)) and yn is
increasing, we have yn ≥max(B, 1"
REFERENCES,0.8448810754912099,"c ln(B c)) for all n. Therefore, we can apply the equality from
the previous claim to get
yn+1 = yn + B e−cyn = f(yn) .
The sequence (xn) satisfies the inequality xn+1 ≤xn + B e−cxn ≤f(xn)."
REFERENCES,0.8459152016546019,"Now, let us prove xn ≤yn using induction. The base case is x0 ≤y0 (given). Assume xk ≤yk
for some k ≥0. Since f is increasing, xk ≤yk implies f(xk) ≤f(yk). Using the properties
xk+1 ≤f(xk) and f(yk) = yk+1, we get xk+1 ≤f(xk) ≤f(yk) = yk+1. By the principle of
mathematical induction, xn ≤yn for all n ≥0."
REFERENCES,0.8469493278179938,"Lemma 14. Let c > 0 and B ≥1, and let {yn}∞
n=0 be a sequence defined by the recurrence relation"
REFERENCES,0.8479834539813857,yn+1 = yn + B e−cyn
REFERENCES,0.8490175801447777,"s.t. y0 ≥max(B, 1"
REFERENCES,0.8500517063081696,"c ln(B c)). Then, yn ≤1"
REFERENCES,0.8510858324715616,c ln(cn + ecy0) + π2 12c.
REFERENCES,0.8521199586349535,Proof. Define the function:
REFERENCES,0.8531540847983454,y(t) := 1
REFERENCES,0.8541882109617374,"c ln(ct + ecy0) ,
t ≥0 ,"
REFERENCES,0.8552223371251293,"and note that it is an increasing function, Moreover, ˙y(t)(= d"
REFERENCES,0.8562564632885212,"dty(t)) = e−cy(t) for any t ≥0. Hence,"
REFERENCES,0.8572905894519132,"y(t) = y0 +
Z t"
REFERENCES,0.8583247156153051,"0
e−cy(s) ds ,
t ≥0"
REFERENCES,0.859358841778697,"Define the function:
g(x) := x + B e−cx ,"
REFERENCES,0.860392967942089,and note that g is increasing when y ≥1
REFERENCES,0.8614270941054809,"c ln(B c). Moreover, y(0) = y0 and yn+1 = g(yn)."
REFERENCES,0.8624612202688728,Since y(0) = y0 ≥1
REFERENCES,0.8634953464322648,"c ln(B c)) and both {yn} and y(t) are increasing, yn ≥1"
REFERENCES,0.8645294725956567,"c ln(B c) for all n and ,
y(t) ≥1"
REFERENCES,0.8655635987590486,c ln(B c) for all t ≥0.
REFERENCES,0.8665977249224406,"We first prove that
y(n) ≤yn ,
n = 0, 1, . . . .
(254)
We prove Eq. (254) by induction. The claim holds for n = 0 by construction. Now assume that
y(n) ≤yn holds for some n ≥0. Let us show that that y(n + 1) ≤yn+1 also holds. For this note
that"
REFERENCES,0.8676318510858325,"y(n + 1) = y(n) +
Z n+1"
REFERENCES,0.8686659772492245,"n
e−cy(s)ds"
REFERENCES,0.8697001034126164,"≤y(n) +
Z n+1"
REFERENCES,0.8707342295760083,"n
e−cy(n)ds
(t 7→y(t) is increasing)"
REFERENCES,0.8717683557394003,= y(n) + e−cy(n)
REFERENCES,0.8728024819027922,"≤y(n) + B e−cy(n)
(since B ≥1)
= g(y(n))
(definition of g)
≤g(yn)
(induction hypothesis, g is increasing for y ≥1"
REFERENCES,0.8738366080661841,"c ln(B c) and yn, y(n) ≥1"
REFERENCES,0.8748707342295761,c ln(B c))
REFERENCES,0.8759048603929679,"= yn+1 .
(By definition of yn+1)
By the principle of mathematical induction, y(n) ≤yn for all n ≥0."
REFERENCES,0.8769389865563598,"Define ∆n := yn −y(n). From our previous inequality, we know that ∆n ≥0. We now show that
{∆n}n is bounded, from which the desired statement follows immediately. To show that {∆n}n
is bounded we will show that ∆n+1 −∆n is summable. To show this, we start by obtaining an
expression for ∆n+1 −∆n. Let A = ecy0. Let n ≥0. Direct calculation gives"
REFERENCES,0.8779731127197518,∆n+1 −∆n = e−cyn −1
REFERENCES,0.8790072388831437,"c ln

1 +
c
cn + A 
."
REFERENCES,0.8800413650465356,"Using that for all x > 0, ln(1 + x) ≥
x
1+ x"
REFERENCES,0.8810754912099276,"2 , we get"
REFERENCES,0.8821096173733195,"∆n+1 −∆n ≤e−cyn −
1
cn + A + c 2"
REFERENCES,0.8831437435367114,"≤e−cy(n) −
1
cn + A + c"
REFERENCES,0.8841778697001034,"2
(from Eq. (254))"
REFERENCES,0.8852119958634953,"≤
1
cn + A −
1
cn + A + c"
REFERENCES,0.8862461220268872,"2
(definition of y(n)) = 1"
C,0.8872802481902792,"2c
1
(n + A/c) (n + A/c + 1 2) ≤1"
C,0.8883143743536711,"2c
1
(n + 1)2 .
(Since y0 ≥1"
C,0.889348500517063,"c ln(B c)), A/c = B ≥1)"
C,0.890382626680455,"For a fixed m > 0, summing up the above inequality from n = 0 to m −1,"
C,0.8914167528438469,∆m −∆0 ≤1
C,0.8924508790072389,"2c m−1
X n=0"
C,0.8934850051706308,"1
(n + 1)2 ≤π2"
C,0.8945191313340227,"12c
(Since P∞
i=1
1
i2 = π2 6 )"
C,0.8955532574974147,=⇒∆m ≤π2
C,0.8965873836608066,"12c.
(Since ∆0 = 0)"
C,0.8976215098241985,"where π = 3.14159 . . . . Hence, it follows that for any n ≥0,"
C,0.8986556359875905,yn = y(n) + ∆n ≤y(n) + π2
C,0.8996897621509824,12c = 1
C,0.9007238883143743,c ln(cn + ecy0) + π2 12c.
C,0.9017580144777663,"Lemma 15. Let {xn} be a nonnegative valued sequence such that xn+1 ≤xn + B e−cxn for all
n ≥0 with B ≥1, c > 0. Then, for all n ≥0, xn ≤1"
C,0.9027921406411582,"c ln(cn + ecM) + π2 12c ,"
C,0.9038262668045501,"where M = max{B, 1"
C,0.9048603929679421,"c ln(B c)), x0}."
C,0.905894519131334,"Proof. Let {yn} be the solution to the difference equation yn+1 = yn + B e−cyn where y0 =
max{B, 1"
C,0.9069286452947259,"c ln(B c)), x0}. Since y0 ≥x0 and y0 ≥max(B, 1"
C,0.9079627714581179,"c ln(B c)), we can use Lemma 13 to
conclude that for all n ≥0,
xn ≤yn ."
C,0.9089968976215098,"Furthermore, using Lemma 14 we can conclude that, yn ≤1"
C,0.9100310237849017,c ln(cn + ecy0) + π2 12c.
C,0.9110651499482937,Combining the above inequalities completes the proof.
C,0.9120992761116856,"D
Additional simulation results"
C,0.9131334022750776,"0.0
0.2
0.4
0.6
0.8
1.0
t
1e6 10
7 10
6 10
5 10
4 10
3 10
2 10
1 100"
C,0.9141675284384695,Probability of optimal action
C,0.9152016546018614,"(a) πθt(a∗), η = 100."
C,0.9162357807652534,"0.0
0.2
0.4
0.6
0.8
1.0
t
1e6 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200"
C,0.9172699069286453,Sub-optimality gap
C,0.9183040330920372,"(b) r(a∗) −π⊤
θtr, η = 100."
C,0.9193381592554292,"0
2
4
6
8
10
12
14
log(t) 18 16 14 12 10 8 6 4 2"
C,0.9203722854188211,Log sub-optimality gap
C,0.921406411582213,"(c) log (r(a∗) −π⊤
θtr), η = 10."
C,0.922440537745605,"Figure 2: Visualization in a two-action stochastic bandit problem. Here the rewards are defined as
(−0.05, −0.25). Other details are same as for Figure 1. Figures 2a and 2a are based on a single run,
while Figure 2c averages across 10 runs. Note that log (r(a∗) −π⊤
θtr) ≈10−33 at the final stages on
Figure 2b."
C,0.9234746639089969,NeurIPS Paper Checklist
CLAIMS,0.9245087900723888,1. Claims
CLAIMS,0.9255429162357808,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In our view, the abstract and introduction summarize the main results, as well
as the technical novelty in obtaining these results.
Guidelines:"
CLAIMS,0.9265770423991727,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.9276111685625646,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See Section 5.
Guidelines:"
CLAIMS,0.9286452947259566,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.9296794208893485,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]"
CLAIMS,0.9307135470527405,"Justification: We describe the setting and assumptions clearly. The main results are backed
by proof sketches in the main text and detailed proofs in the appendix."
CLAIMS,0.9317476732161324,Guidelines:
CLAIMS,0.9327817993795243,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9338159255429163,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9348500517063082,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9358841778697001,"Answer: [Yes]
Justification: Our experiments consist of simple simulations, for which all the details are
included in Section 4. The algorithm studied here is remarkably simple and well-known, so
the experiments should be straightforward to reproduce with these details, without any code."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9369183040330921,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.937952430196484,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9389865563598759,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9400206825232679,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: Given that we are doing simulations with a very classical algorithm, for which
we give all simulation details and hyperparameters, we do not believe that code is necessary
to reproduce the results. Our data is simulated, and hence easily reproduced, given the
distribution which we explicitly describe.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9410548086866598,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
OPEN ACCESS TO DATA AND CODE,0.9420889348500517,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Please see the details in Section 4.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9431230610134437,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
OPEN ACCESS TO DATA AND CODE,0.9441571871768356,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: There are no comparisons for which significance needs to be demonstrated,
but we provide evidence for the convergence by repeating 10 independent runs, which are
presented in the plots.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9451913133402275,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
OPEN ACCESS TO DATA AND CODE,0.9462254395036195,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9472595656670114,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9482936918304034,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9493278179937953,Answer: [No]
EXPERIMENTS COMPUTE RESOURCES,0.9503619441571872,"Justification: Our experiments are simply run on a single laptop or Python notebook, without
requiring any particular infrastructure."
EXPERIMENTS COMPUTE RESOURCES,0.9513960703205792,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9524301964839711,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.953464322647363,9. Code Of Ethics
CODE OF ETHICS,0.954498448810755,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9555325749741469,Answer: [Yes]
CODE OF ETHICS,0.9565667011375388,Justification: The research does not involve human subjects or introduces any new datasets.
CODE OF ETHICS,0.9576008273009308,Guidelines:
CODE OF ETHICS,0.9586349534643226,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9596690796277145,10. Broader Impacts
BROADER IMPACTS,0.9607032057911065,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9617373319544984,Answer: [Yes]
BROADER IMPACTS,0.9627714581178903,Justification: See Section 5.
BROADER IMPACTS,0.9638055842812823,Guidelines:
BROADER IMPACTS,0.9648397104446742,• The answer NA means that there is no societal impact of the work performed.
BROADER IMPACTS,0.9658738366080661,"• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9669079627714581,11. Safeguards
SAFEGUARDS,0.96794208893485,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9689762150982419,Answer: [NA]
SAFEGUARDS,0.9700103412616339,Justification: No data or models are released.
SAFEGUARDS,0.9710444674250258,Guidelines:
SAFEGUARDS,0.9720785935884177,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9731127197518097,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9741468459152016,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9751809720785936,Answer: [NA]
LICENSES FOR EXISTING ASSETS,0.9762150982419855,Justification: No existing assets are used.
LICENSES FOR EXISTING ASSETS,0.9772492244053774,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9782833505687694,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided."
LICENSES FOR EXISTING ASSETS,0.9793174767321613,"• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9803516028955532,13. New Assets
NEW ASSETS,0.9813857290589452,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9824198552223371,Answer: [NA]
NEW ASSETS,0.983453981385729,Justification: The paper does not release new assets.
NEW ASSETS,0.984488107549121,Guidelines:
NEW ASSETS,0.9855222337125129,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865563598759048,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875904860392968,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9886246122026887,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896587383660806,Justification: No human subjects or crowdsourcing is involved in this work.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9906928645294726,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9917269906928645,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9927611168562565,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937952430196484,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948293691830403,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958634953464323,Justification: No human subjects or crowdsourcing is involved in this work.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968976215098242,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979317476732161,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989658738366081,"• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
