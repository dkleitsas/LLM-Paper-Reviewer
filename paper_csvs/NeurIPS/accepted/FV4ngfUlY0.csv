Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0034482758620689655,"Experimentation has been critical and increasingly popular across various domains,
such as clinical trials and online platforms, due to its widely recognized benefits.
One of the primary objectives of classical experiments is to estimate the average
treatment effect (ATE) to inform future decision-making. However, in healthcare
and many other settings, treatment effects may be non-stationary, meaning that they
can change over time, rendering the traditional experimental design inadequate and
the classical static ATE uninformative. In this work, we address the problem of non-
stationary experimental design under linear trends by considering two objectives:
estimating the dynamic treatment effect and minimizing welfare loss within the
experiment. We propose an efficient design that can be customized for optimal
estimation error rate, optimal regret rate, or the Pareto optimal trade-off between the
two objectives. We establish information-theoretical lower bounds that highlight
the inherent challenge in estimating dynamic treatment effects and minimizing
welfare loss, and also statistically reveal the fundamental trade-off between them."
INTRODUCTION,0.006896551724137931,"1
Introduction"
INTRODUCTION,0.010344827586206896,"Experimental design plays a critical role in conducting research across various fields including clinical
trials (see, e.g., [75], [14], [69]) and online platforms (see, e.g., [52], [1], [44], [21]). It enables the
evaluation of the effectiveness of treatments, interventions, or policies in a rigorous and systematic
manner, protecting the safety and well-being of participants and the wider population. Typically,
one of the primary objectives of conducting experiments is to estimate the average treatment effect
(ATE), which represents the difference in outcomes between different treatments or controls. The
outcomes of each treatment are assumed to be stationary by default (see, e.g., [48]), ensuring that the
information collected and conclusions drawn from in-experiment periods are informative for future
decision-making. However, in some situations, ATE may be non-stationary, meaning that it evolves
over time. Here, we present one illustrative example from clinical trials to highlight the importance
of considering non-stationarity in ATE estimation."
INTRODUCTION,0.013793103448275862,"Malaria has been one of the world‚Äôs deadliest diseases, with approximately 247 million cases and 618
thousand deaths reported in 2021 according to the World Health Organization (WHO). Antimalarial
drug resistance in Africa has been highlighted by the world malaria report (see, [86]) as one the
global key events of malaria control and elimination efforts in 2021-2022. P. falciparum, the deadliest
species of Plasmodium that causes malaria in humans, has shown resistance to first-line treatments
such as artemether-lumefantrine (AL), artesunate-amodiaquine (AS-AQ), artesunate-pyronaridine
(AS-PY), and dihydroartemisinin-piperaquine (DHA-PPQ) in Africa recently. Alarmingly, at least
four recent studies have reported treatment failure rates greater than 10% after treatment with AL"
INTRODUCTION,0.017241379310344827,"in Burkina Faso (see, [35]) and Uganda (see, [91]). Two study sites in Burkina Faso also reported
treatment failure rates greater than 10% after treatment with DHA-PPQ. These alarming findings
underscore the importance of understanding and estimating how the treatment effects evolve over
time, i.e., how rapidly drug resistance escalates. Failure to account for non-stationarity in treatment
effect may lead to suboptimal treatment plans, harming social welfare. Time"
INTRODUCTION,0.020689655172413793,Expected reward
INTRODUCTION,0.02413793103448276,"Treatment 
Control"
INTRODUCTION,0.027586206896551724,"In-Experiment Periods
After-Experimental Periods
ùëá"
INTRODUCTION,0.03103448275862069,ATE estimator without considering the non-stationarity
INTRODUCTION,0.034482758620689655,Figure 1: An illustrating example of the non-stationarity.
INTRODUCTION,0.03793103448275862,"Given the ubiquity of non-stationarity,
ignoring such a structure can lead to mis-
leading information and poor decision-
making. Figure 1 provides an illustrative
example where non-stationarity can be
well captured by linear models. If we
use traditional randomized control trials
and classical estimators based on sam-
ple averages without considering non-
stationarity, we may conclude that the
treatment is always better than the con-
trol. However, the linear non-stationarity
implies that we need to pay more atten-
tion since the treatment outcomes de-
crease much faster than the control. This
means that the treatment does not always
outperform the control during the after-experimental periods. In the context of the drug resistance
example, this phenomenon can be interpreted as follows: a new treatment may have a significant
positive effect during the experiment, but drug resistance develops much faster than the control.
Therefore, we cannot always rely on the treatment after the experiment. Understanding how treatment
effects evolve over time is thus crucial for making informed decisions."
INTRODUCTION,0.041379310344827586,"In this paper, we focus on the challenge of designing experiments in non-stationary environments.
To tackle this problem, we first need to answer the question of how to formulate non-stationarity. If
there is no underlying structure to the non-stationarity, such as when outcomes are determined by a
malicious adversary, then it is not justifiable to conduct experiments because there is little information
that we can infer for future decision-making. In this work, we limit our attention to scenarios where
potential outcomes follow structured trends. Specifically, we consider situations where outcomes
are linear with time, which provides a suitable starting point for research on experimental design
under non-stationarity. Such structures also help us to determine meaningful metrics to use instead of
the traditional static ATE, since it is no longer informative enough in non-stationary environments.
While other ways to formulate non-stationarity can be found in the literature (e.g., [15], [16], [24],
[10], [36]), we carefully argue in Section 1.2 that these traditional formulations may not suffice for
our purposes."
INTRODUCTION,0.04482758620689655,"In addition to metrics related to the treatment effect, non-stationarity can also increase attention to
the welfare loss issue during experiments. Specifically, every time a suboptimal treatment is assigned,
there is a welfare loss as the unit could have been treated better with the optimal treatment. In a
stationary environment, this welfare loss remains constant over time. However, non-stationarity can
lead to an increasing welfare loss over time. Thus, it might not be ideal to ignore the welfare loss
during the experiment, particularly when the in-experiment periods are not negligible compared
to the after-experiment periods. In this work, we also aim to address how to control welfare loss
during experiments and explore the relationship between minimizing welfare loss and maximizing the
statistical power of estimating the dynamic treatment effect."
MAIN RESULTS AND CONTRIBUTION,0.04827586206896552,"1.1
Main results and contribution"
MAIN RESULTS AND CONTRIBUTION,0.05172413793103448,"The primary contribution of this work is the presentation of a framework that enables experimenters
and researchers to design reliable, efficient, and flexible experiments in structured non-stationary
environments. To the best of our knowledge, this is the first study of the dynamic treatment effect in
the experimental design literature, extending the classical static ATE. We believe that this work could
serve as a starting point for further research on designing experiments in non-stationary environments."
MAIN RESULTS AND CONTRIBUTION,0.05517241379310345,"We examine what is arguably the most foundational scenario, where the expected potential outcomes
of each treatment are represented by linear functions of time t. In this context, the dynamic treatment"
MAIN RESULTS AND CONTRIBUTION,0.05862068965517241,"effect can be described by a linear function as well. We introduce a flexible experimental design
framework that draws upon the well-established principle of Optimism in the Face of Uncertainty
from the online learning literature [29][2]. Our design can be readily customized to suit various
notions of optimality. Specifically, in this work, we systematically explore optimality from three
distinct perspectives: optimal estimation error rate, optimal regret rate, and Pareto optimality."
MAIN RESULTS AND CONTRIBUTION,0.06206896551724138,"The optimal estimation error rate design maximizes statistical power for inferring dynamic
treatment effects. We establish a lower bound of
1
‚àö"
MAIN RESULTS AND CONTRIBUTION,0.06551724137931035,"T for the error rate of any estimator, where
T is the total experimental horizon length. Our design achieves this lower bound by selecting
appropriate input parameters. The optimal regret rate design minimizes in-experiment welfare loss
(regret) in the worst case scenario. Traditional regret analysis techniques are not applicable due to
unbounded outcomes. Our design achieves an
‚àö"
MAIN RESULTS AND CONTRIBUTION,0.06896551724137931,"T upper bound for regret rate by selecting input
parameters, matching the worst-case lower bound. Linear non-stationarity in our problem may result
in a regret rate of T 2 without careful design, unlike the stationary setting where regret is bounded by
T. The Pareto optimal design strikes an optimal and flexible balance between the two objectives
of maximizing statistical power and minimizing regret. We reveal the inherent and fundamental
trade-off between these objectives statistically. Specifically, among all designs that can ensure a
regret upper bound of T Œ≤, the smallest error any estimator can achieve is T ‚àíŒ≤"
MAIN RESULTS AND CONTRIBUTION,0.07241379310344828,"4 in the worst case.
Conversely, among all designs that can guarantee an error bound of T ‚àíŒ≤"
MAIN RESULTS AND CONTRIBUTION,0.07586206896551724,"4 , a regret of order T Œ≤ is
unavoidable. Our design achieves different levels of trade-off by flexibly selecting input parameters."
LITERATURE REVIEW,0.07931034482758621,"1.2
Literature Review"
LITERATURE REVIEW,0.08275862068965517,"Inference under non-stationarity. Considerable literature focuses on inference and decision-making
under non-stationarity. One area is the inference for non-stationary Poisson processes [4][63][53]
[96][39]. [39] is relevant as they establish statistical inference models for non-stationary arrival
models with linear trends. [88] shows that ignoring non-stationarity can lead to inefficient or invalid
A/B tests. They propose a new framework for statistical analysis of non-stationary A/B tests."
LITERATURE REVIEW,0.08620689655172414,"Decision-making under non-stationarity. This work relates to adaptive decision-making in non-
stationary environments. Existing literature has two main streams. The first stream focuses on
budget-based drifting environments, where changes are dynamically and adversarially made within a
variation budget [15][16][17][47][84][61][24][80]. The second stream considers piecewise station-
ary/switching environments, with adaptive assignment policies designed for various bandit settings
[10][36][58][22][47][61][89][18]. However, these formulations lack the knowledge transferability
to after-experimental periods, which is essential for our purpose. Recent studies address bandit
experiments with non-stationary contexts [70], provide information-theoretic analysis [65], pro-
pose improved algorithms [59], and define non-stationary bandits more rigorously [60]. Moreover,
various bandit models consider specific non-stationary reward structures, such as recharging ban-
dits [51][90][66], restless bandits [85][81][94], blocking bandits [13][12][20][8], rotting bandits
[56][71][72], and rising bandits [64][28]. In operations, assortment decisions under non-stationarity
are studied [34], as well as pricing with evolving demands [50][42][99]. [89] study the recommenda-
tion problem. Recent work extends bandit analysis to reinforcement learning [62][97][25]. Some
works focus on optimizing with full feedback [26][16][43]."
LITERATURE REVIEW,0.0896551724137931,"Adaptive Experimental Design. Multi-armed bandit is an efficient paradigm for adaptive experi-
ments [77][54][9]. [78] and [3] apply diffusion-asymptotic analysis to study randomized experiments,
including bandit problems. [48] consider experimental design for choosing the best arm with arriving
units in waves. [30] integrate adaptively weighted doubly robust estimator into Thompson sampling.
[32] incorporate synthetic control into MAB for interference scenarios. [73] also investigate the
trade-off between regret and causal inference using Pareto optimality. Non-stationarity that we
consider here greatly impacts Pareto optimality structure and reveals stochastic bandit as a special
case. [41] design a two-stage experiment for ATE estimation. [6] model adaptive assignment as
a Markov decision process, highlighting suboptimality of RCTs. [49] propose sequential testing
framework with finite and infinite sample analysis. [87] consider an optimization problem in adaptive
experiments. [38] transform experimental design with temporal interference into a Markov decision
problem. [19] optimize subject allocation for ATE estimation precision. [95] address adaptive
experimental design for sequential unit assignment with heterogeneous covariates."
LITERATURE REVIEW,0.09310344827586207,"Post-experiment Inference. There is a substantial literature on post-experiment inference from
logged adaptively collected data where adaptive experiment data is a typical example. One of"
LITERATURE REVIEW,0.09655172413793103,"the central tasks along this line is the evaluation of a new policy given historic data (see, e.g.,
[31][76][57][83][46][7][92][98]). [23] provide a procedure to adopt bootstrap on the data collected
by a bandit to debias the sample means. [40] construct confidence intervals for policy evaluation,
including ATE, in adaptive experiments. Works along this line usually impose some ‚Äúoverlapping‚Äù
assumptions on the data collection process. Since we are jointly considering the experimentation and
the inference, such an overlapping assumption may harm the welfare gained within the experiment."
LITERATURE REVIEW,0.1,"Finally, we remark that the full version of this paper (containing additional theoretical results,
extensions, and missing proofs) is available at https://ssrn.com/abstract=4514568."
EXPERIMENTAL DESIGN WITH LINEAR TRENDS,0.10344827586206896,"2
Experimental Design with Linear Trends"
FORMULATION,0.10689655172413794,"2.1
Formulation
We establish the formulation of our non-stationary experiments with a finite set of arms A. Without
loss of generality, we focus on A = {1, 2} representing a treatment and a control. The total length of
the experiment is denoted as T. At each time t ‚àà[T]1, the environment generates potential outcomes
rt(a) for each arm a ‚ààA. The chosen arm at determines the observed outcome rt = rt(at). Our
study focuses on non-stationary experiments with linear trends, specifically considering the follows,"
FORMULATION,0.1103448275862069,"rt(a) = Œ∏a,0 + Œ∏a,1t + Œµa,t := Œ∏‚ä§
a œï(t) + Œµa,t,
(1)"
FORMULATION,0.11379310344827587,"where œï(t) = (1, t)‚ä§, Œ∏a ‚ààR2 is fixed throughout the experimental horizon T but unknown to the
decision maker. For a specific arm a, {Œµa,t}t‚â•1 are independent and identically distributed (i.i.d.)
mean-zero œÉ2
a-sub-Gaussian random variables (r.v.‚Äôs), i.e., E[eŒªŒµa,t] ‚â§exp(Œª2œÉ2
a/2) for any Œª ‚ààR.
For simplicity, we also assume for œÉa ‚â§œÉ0 < ‚àûfor all a. Note that while Œ∏a remains fixed, the
distribution of the reward of arm a, rt(a), evolves over time, thereby satisfying the definition of
non-stationarity as outlined in [60]. Another mild assumption we make is the boundedness of Œ∏a, i.e.,
‚à•Œ∏a‚à•‚â§S for all a ‚ààA, which is commonly used (see, e.g., [27], [2], and [55]). A non-stationary
experiment instance with linear trends can be denoted by ŒΩ = (P1, P2), where Pi is the distribution of
the rewards of arm i. The optimal arm at time t is the one with the maximum expected reward, which
is denoted by a‚àó
t := arg maxa‚ààA Œ∏a,0 + Œ∏a,1t, and thus is also unknown and may even change over
time. The dynamic treatment effect can be fully captured by the column vector Œ∏ŒΩ := Œ∏1 ‚àíŒ∏2 ‚ààR2.
Denote all non-stationary experiment instances with linear trends to constitute a feasible set E1."
FORMULATION,0.11724137931034483,"At every time t, the decision maker observes the history Ht‚àí1 = (a1, r1, ¬∑ ¬∑ ¬∑ , at‚àí1, rt‚àí1). An admis-
sible design œÄ = {œÄt}t‚â•1 maps the history Ht‚àí1 to an action at. We use the traditional accumulative
regret to measure the efficiency of online learning, defined as the difference between the expected re-
ward under the clairvoyant optimal policy and the policy œÄ, i.e., RœÄ
ŒΩ(T) = PT
t=1 Œ∏a‚àó
t œï(t)‚ä§‚àíŒ∏atœï(t)‚ä§.
Note that RœÄ
ŒΩ(T) is still a random variable because {at}t‚â•1 are still random. Similarly, an admissible
estimator ÀÜŒ∏t of the treatment effect at time t maps Ht to an estimate of Œ∏ŒΩ. The quality of the
estimator can be measured by the norm of the difference between ÀÜŒ∏t and Œ∏ŒΩ, for example, the l1-norm
‚à•ÀÜŒ∏t ‚àíŒ∏ŒΩ‚à•1. The design of experiment includes designing œÄ (i.e., how to make an adaptive assignment
decision) and ÀÜŒ∏T (i.e., how to conduct causal inference)."
FORMULATION,0.1206896551724138,"Remark 1: Linear trends, admittedly, is a strong assumption, but they have proven to be highly
useful for understanding and predicting certain phenomena across various fields. [82] observe a
linear trend in the prevalence of obesity and overweight among US adults, based on national survey
data collected between 1970s and 2004. This linear trend served as a benchmark for predicting the
prevalence from 2004 to 2050, influencing numerous epidemiology studies. [37] conduct a study on
monitoring and predicting the California sea otter population using linear trends observed in aerial
surveys, showcasing the usefulness of linear trends in ecological studies. [68] reveal a linear trend
in the integrated water vapor content based on a 10-year ground-based GPS data, highlighting the
application of linear trends in atmospheric research"
FORMULATION,0.12413793103448276,"Remark 2: The traditional multi-armed bandit problems are primarily focused on designing œÄ to
have minimal regrets (see, e.g., [55]). However, in our design, we have an equally important task of
designing ÀÜŒ∏T . Furthermore, we outline the unique challenges and opportunities we face compared to
the stochastic bandits and contextual bandits studied in the current literature."
FORMULATION,0.12758620689655173,"1Throughout this paper, we define [n] := {1, ¬∑ ¬∑ ¬∑ , n}, a ‚à®b := max{a, b} for a, b ‚ààR and ‚à•u‚à•V :=
‚àö"
FORMULATION,0.1310344827586207,u‚ä§V u for the positive definite matrix V .
FORMULATION,0.13448275862068965,"Our problem encompasses the well-studied stationary stochastic multi-armed bandit problem (e.g.,
[54], [9]) by setting Œ∏a,1 = 0 in Eq. (1) for all a ‚ààA. The non-stationary structure introduces
new challenges, such as the possibility of the optimal action varying over time, requiring additional
efforts to control the regret. Second, while (1, t)‚ä§in Eq. (1) can be seen as a special kind of context
connecting our problem to the contextual bandit problem (e.g., [27]), the first difference lies in that
the context (1, t)‚ä§is fixed and known at each time period. However, a challenge arises as we can
no longer assume the context (1, t)‚ä§to be bounded, as commonly done in the literature. Some
works assume the l2-norm to be no larger than a universal constant, say 1 (see, e.g., [27] and [24]).
A straightforward solution to address this issue may involve relaxing the bound of the context to
‚àö"
FORMULATION,0.13793103448275862,"T 2 + 1 if the experimental horizon T is known in advance. However, such a relaxation might be
too loose, particularly when analyzing regret upper bounds. A more thorough discussion on this
relaxation is deferred to the next subsection when we analyze the regret of our new design."
DESIGN OF EXPERIMENTS WITH LINEAR TRENDS,0.1413793103448276,"2.2
Design of Experiments with Linear Trends"
DESIGN OF EXPERIMENTS WITH LINEAR TRENDS,0.14482758620689656,"We present our design called Linear Exploration and OFU (L-EOFU) design in Algorithm 1. The
term OFU refers to the well-known principle of Optimism in the Face of Uncertainty, which arises
from and has been widely adopted in the multi-armed bandit literature (see, e.g., [29], [2] and their
follow-ups). For brevity, we use the notation L-EOFU(Œ±) to emphasize the crucial input parameter Œ±,
whose role will become evident in the subsequent discussion. Our design operates in a two-phase
manner: the exploration phase and the OFU phase. We now elaborate on each phase in detail."
DESIGN OF EXPERIMENTS WITH LINEAR TRENDS,0.1482758620689655,"The exploration phase is the first steps of our design, which lasts for approximately T Œ± periods.
The parameter Œ± plays a crucial role in controlling the length of this phase. During this phase, our
L-EOFU algorithm deterministically alternates between playing arms 1 and 2. In Algorithm 1, we
choose to play arm 1 when the time index t is odd and arm 2 when t is even. This deterministic
approach preserves the independence between the collected data. By the end of this phase, the
decision-maker obtains approximately T Œ±"
DESIGN OF EXPERIMENTS WITH LINEAR TRENDS,0.15172413793103448,"2 independent samples for each arm, although these samples
may not be identically distributed. L-EOFU will utilize the data collected during this initial phase
to produce an estimator for the treatment effect, denoted as ÀÜŒ∏L-EOFU(Œ±)
T
in line 18 of the algorithm.
Specifically, we estimate Œ∏1 and Œ∏2 separately by regressing the rewards against the time t using the
data collected up to time T Œ±, i.e., ÀÜŒ∏L-EOFU(Œ±)
1,T
= (P"
DESIGN OF EXPERIMENTS WITH LINEAR TRENDS,0.15517241379310345,t‚àà[[T Œ±]] œï(t)œï(t)‚ä§)‚àí1 P
DESIGN OF EXPERIMENTS WITH LINEAR TRENDS,0.15862068965517243,"t‚àà[[T Œ±]] rtœï(t), where
[[T Œ±]] denotes the set of all the odd numbers that are no larger than T Œ±. The final estimate of Œ∏ŒΩ is
ÀÜŒ∏L-EOFU(Œ±)
T
= ÀÜŒ∏L-EOFU(Œ±)
1,T
‚àíÀÜŒ∏L-EOFU(Œ±)
2,T
. While the estimator we use may be sample-inefficient as we
do not utilize the T ‚àíT Œ± samples in the second phase, we will show in the next subsection that the
data collected in the first stage is sufficient to achieve a rate-optimal estimator of the treatment effect.
Intuitively, if one arm consistently outperforms the other, most of the data in the second phase comes
from the optimal arm, with fewer observations from the suboptimal arm. Thus, observations from the
suboptimal arm are mostly obtained during the first stage. Moreover, the bottleneck for estimating
the treatment effect is very likely determined by the arm with the fewest observations, which is
typically the suboptimal arm. Although we discard much data from the optimal arm when conducting
inference, the bottleneck is still the suboptimal arm whose most information will be collected through
the first phase. Additionally, we choose not to use all the data since adaptively collected data in the
second stage can harm the independence of the data, leading to undesirable statistical properties such
as bias. This issue has been pointed out in a large body of recent work [67][93][23]. Formally, we
have the following Theorem 2 characterizing the properties of the estimator we choose."
DESIGN OF EXPERIMENTS WITH LINEAR TRENDS,0.16206896551724137,"Theorem 1 Assuming T Œ± ‚â•4, for any ŒΩ ‚ààE1, there exists a constant c1 > 0 such that for any œµ > 0,"
DESIGN OF EXPERIMENTS WITH LINEAR TRENDS,0.16551724137931034,"with probability at least 1 ‚àíœµ,
ÀÜŒ∏L-EOFU(Œ±)
T
‚àíŒ∏ŒΩ

1 ‚â§c1T ‚àíŒ±"
Q,0.16896551724137931,"2
q log 8"
Q,0.1724137931034483,"œµ , where c1 = 128"
Q,0.17586206896551723,3 (œÉ1 ‚à®œÉ2).
Q,0.1793103448275862,"Moreover, we have maxŒΩ‚ààE1 E
hÀÜŒ∏L-EOFU(Œ±)
T
‚àíŒ∏ŒΩ

1"
Q,0.18275862068965518,"i
= O(T ‚àíŒ± 2 )."
Q,0.18620689655172415,"Our assumption that T Œ± ‚â•4 aligns with our intuition that a minimum of two samples per arm is
required to identify a linear structure. Theorem 1 demonstrates that the bound T ‚àíŒ±"
DECREASES AS,0.1896551724137931,"2 decreases as
Œ± increases, indicating that a longer exploration period leads to better estimation of the dynamic
treatment effect. When Œ± is set to 1, we conduct pure exploration throughout the entire experimental
horizon, similar to the well-known switchback experiments [21]. In this case, the estimation error
can be controlled to the order of O(1/
‚àö"
DECREASES AS,0.19310344827586207,"T), which aligns with traditional statistical results of linear
regression [79]. Moreover, our analysis can be extended to any lp-norm, with only slight variations"
DECREASES AS,0.19655172413793104,Algorithm 1: Linear Exploration and OFU (L-EOFU)
DECREASES AS,0.2,"1 Input: Controlling parameter Œ±, instance parameters: œÉa for all a ‚àà{1, 2}, T, S, Œ¥"
DECREASES AS,0.20344827586206896,"2 Initialization: Œª = 1, D1,0 = D2,0 = D1,T = D1,T = ‚àÖ, œï(¬∑) = (1, ¬∑)‚ä§"
DECREASES AS,0.20689655172413793,"3 for t = 1, 2, ¬∑ ¬∑ ¬∑ , ‚åàT Œ±‚åâdo // Phase 1:
Exploration"
DECREASES AS,0.2103448275862069,"4
Select At = 1 if t is odd; otherwise select At = 2;"
DECREASES AS,0.21379310344827587,"5
Observe reward rt;"
DECREASES AS,0.21724137931034482,"6
DAt,0 ‚ÜêDAt,0 ‚à™{(œï(t), rt)} and DAt,T ‚ÜêDAt,T ‚à™{(œï(t), rt)};"
END FOR,0.2206896551724138,7 end for
END FOR,0.22413793103448276,"8 for t = ‚åàT Œ±‚åâ+ 1, ‚åàT Œ±‚åâ+ 2, ¬∑ ¬∑ ¬∑ , T do // Phase 2:
OFU"
END FOR,0.22758620689655173,"9
Va,t = P"
END FOR,0.23103448275862068,"(œï(i),ri)‚ààDa,T œï(i)œï(i)‚ä§+ ŒªI for a ‚àà{1, 2};"
END FOR,0.23448275862068965,"10
Œ≥a,t = œÉa
q"
END FOR,0.23793103448275862,"2 log(
2 det(Va,t)1/2 det(ŒªI)‚àí1/2"
END FOR,0.2413793103448276,"Œ¥
) + Œª1/2S for a ‚àà{1, 2};"
END FOR,0.24482758620689654,"11
ÀúŒ∏a,t = V ‚àí1
a,t
P"
END FOR,0.2482758620689655,"(œï(i),ri)‚ààDa,T riœï(i) for a ‚àà{1, 2};"
END FOR,0.2517241379310345,"12
Àú
Rt(a) = ÀúŒ∏‚ä§
a,tœï(t) + Œ≥a,t‚à•œï(t)‚à•V ‚àí1
a,t for a ‚àà{1, 2};"
END FOR,0.25517241379310346,"13
Select At = arg maxa‚àà{1,2} Àú
Rt(a);"
END FOR,0.25862068965517243,"14
Observe reward rt;"
END FOR,0.2620689655172414,"15
DAt,T ‚ÜêDAt,T ‚à™{(œï(t), rt)};"
END FOR,0.2655172413793103,16 end for
END FOR,0.2689655172413793,"17 ÀÜŒ∏L-EOFU(Œ±)
a,T
= (P"
END FOR,0.27241379310344827,"(œï(i),ri)‚ààDa,0 œï(i)œï(i)‚ä§)‚àí1 P"
END FOR,0.27586206896551724,"(œï(i),ri)‚ààDa,0 riœï(i) for a ‚àà{1, 2};"
END FOR,0.2793103448275862,"18 Output: ÀÜŒ∏L-EOFU(Œ±)
T
= ÀÜŒ∏L-EOFU(Œ±)
1,T
‚àíÀÜŒ∏L-EOFU(Œ±)
2,T
;"
END FOR,0.2827586206896552,"in the constant c1. Additionally, note that the high probability bound can be used for constructing
confidence intervals and conduct hypothesis testing."
END FOR,0.28620689655172415,"ùë°!
ùë°""
ùë°#
ùë°$
ùë°%
ùë°& Time"
END FOR,0.2896551724137931,Expected reward
END FOR,0.29310344827586204,"Arm 1
Arm 2"
END FOR,0.296551724137931,Estimated reward and confidence interval
END FOR,0.3,Figure 2: An illustrative example on OFU.
END FOR,0.30344827586206896,"The second phase of our L-EOFU follows the OFU prin-
ciple. Specifically, at each time period t, L-EOFU calcu-
lates an optimistic estimate of the rewards for each arm,
denoted as Àú
Rt(a) in line 12. This estimate is referred
to as optimistic because, with probability at least 1 ‚àíŒ¥,
for all t ‚àà[T] and a ‚àà{1, 2}, ÀúRt(a) = ÀúŒ∏‚ä§
a,tœï(t) +
Œ≥a,t‚à•œï(t)‚à•V ‚àí1
a,t ‚â•Œ∏‚ä§
a œï(t) = E[Rt(a)], where Œ≥a,t ‚â§ œÉa(
p"
END FOR,0.30689655172413793,"12 log(T) ‚àí2 log(4.5ŒªŒ¥) + Œª1/2S) and by de-
fault, Œª is set to be 1. The proof of this claim follows
the standard techniques outlined in Theorem 2 of [2].
Based on the optimistic estimation, L-EOFU selects the
arm with the highest optimistic estimated value. The
OFU principle has been shown to strike a delicate bal-
ance between exploration and exploitation in the bandit literature [2][55]. As mentioned earlier,
one of the challenges in our problem is the possibility of the optimal arm changing over time. The
OFU principle has an additional advantage in that it can automatically and implicitly detect when
the optimal arm switches from one to another. To illustrate this, Figure 2 presents an example of
how OFU behaves when the optimal arm switches. Initially (before t3), arm 1 outperforms arm
2. According to L-EOFU, we continue to play arm 1, resulting in a relatively fast shrinkage of the
confidence interval for the estimated values of arm 1. Conversely, for arm 2, the confidence interval
even expands as œï(t) increases in magnitude. As time approaches t3, the difference between arm 1
and arm 2 gradually diminishes. At time period t3, the optimistic estimation of arm 2 surpasses that
of arm 1, prompting L-EOFU to start playing arm 2. This addresses the challenge mentioned earlier.
The following theorem presents the regret upper bound."
END FOR,0.3103448275862069,"Theorem 2 For any ŒΩ ‚ààE1 and 0 < Œ± ‚â§1, with probability at least 1 ‚àíŒ¥,"
END FOR,0.3137931034482759,"RL-EOFU(Œ±)
ŒΩ
(T) ‚â§4ST 2Œ±+4(œÉ1‚à®œÉ2)(
p"
END FOR,0.31724137931034485,12 log(T) ‚àí2 log(4.5ŒªŒ¥)+Œª1/2S) s 41+ 2
END FOR,0.32068965517241377,"Œ±
ln 2 ln(2T)T, (2)"
END FOR,0.32413793103448274,"where Œª is initialized as 1. Furthermore, Œ¥ = 1/T leads to E[RL-EOFU(Œ±)
ŒΩ
(T)] = e
O(T 2Œ± ‚à®
‚àö T)."
END FOR,0.3275862068965517,"Equation (2) can be decomposed into two terms: a T 2Œ± term and a
p"
END FOR,0.3310344827586207,"T ln(T) term. The first term
arises from the exploration stage, as PT Œ±"
END FOR,0.33448275862068966,t=1 t = O(T 2Œ±) intuitively. The second term is a result of the
END FOR,0.33793103448275863,OFU phase. Only when Œ± ‚â•1
END FOR,0.3413793103448276,"4 does the first term dominate the second term. In fact, there is little
motivation for a decision maker to choose Œ± < 1"
END FOR,0.3448275862068966,"4, as we will discuss in detail in the next subsection.
For pure theoretical interest, the regret behavior in the region around Œ± = 0 could be studied by
choosing Œ± = 1/ln ln(T), given that there is an Œ± in the denominator. For those familiar with the
bandit literature, a similar O(
p"
END FOR,0.3482758620689655,"T ln(T)) bound can be obtained in the stochastic contextual bandit
setting [2]. As we mentioned before, one possible approach to reduce our problem to the one in [2] is
by treating œï(t) as a special kind of context. However, a key difference is that the context in [2] is
assumed to be bounded by a constant, whereas in our setting, the bound should be
‚àö"
END FOR,0.35172413793103446,"1 + T 2 if we
know T in advance. Consequently, to apply all the techniques from [2], Œª in Equation (2) should
be chosen as Œò(T) instead of 1. However, such a choice of Œª would yield a regret bound of O(T),
which is significantly weaker than the O(
‚àö"
END FOR,0.35517241379310344,"T) regret we derived. Furthermore, in the stochastic
contextual bandit setting, where everything is assumed to be bounded, the regret can always be upper
bounded by O(T) for any policy, and thus only sublinear regrets are meaningful. However, here,
with the linear trend, the largest possible regret for a policy without careful design can be on the order
of O(T 2). Thus, the O(T 2Œ±) regret bound is still informative even if 1/2 < Œ± < 1."
END FOR,0.3586206896551724,"Another important point to emphasize is that although the regret can be decomposed into two terms,
it does not mean that we can analyze each phase independently of the other. The T Œ± samples we
collect from the first stage play a crucial role in our analysis for the second phase. This is evidenced
by the fact that Œ± not only appears in the first term, but also takes effect in the second term by
a non-trivial way. Specifically, the regret of the second phase can be upper bounded by the sum
of the length of the confidence interval at each time, i.e., the regret at time t can be bounded by
2Œ≥ ‚à•œï(t)‚à•V ‚àí1
1,t IAt=1 +2Œ≥ ‚à•œï(t)‚à•V ‚àí1
2,t IAt=2, where Va,t is defined to be Pt
i=1 œï(i)œï(i)‚ä§IAi=a +ŒªI.
The T Œ± samples collected in the first stage are efficient in controlling the minimum eigenvalue of
Va,t, and thus be useful in controlling the magnitude of the regret."
END FOR,0.3620689655172414,"In addition, we would like to compare our two-stage design with the traditional exploration-then-
exploitation design [55]. The similarity between the two lies in the fact that the first exploration phase
provides valuable information for future decision-making, as we discussed earlier. However, the key
difference is that exploration-then-exploitation typically only exploits the information gathered during
the first exploration phase and does not continue to learn new information during the second phase.
In contrast, our L-EOFU algorithm continues to and has to learn throughout the entire learning period,
since the environment may rapidly evolve over time. Moreover, the exploration phase in L-EOFU
also plays an important role in conducting causal inference. Therefore, when designing the length
of the first phase, we must consider not only the regret but also the quality of causal inference. One
reason why we move away from the exploration-then-exploitation method is that it may not always
guarantee the optimal regret rate. Another important motivation for conducting pure exploration first
is that the cost of exploration may increase linearly with time."
OPTIMALITY OF THE DESIGN,0.36551724137931035,"2.3
Optimality of The Design"
OPTIMALITY OF THE DESIGN,0.3689655172413793,"In this subsection, we will examine the optimality of our L-EOFU design from three different perspec-
tives. Firstly, we will consider two extremes where the decision maker is solely concerned about
either the estimation error of the dynamic treatment effect or the regret. Subsequently, when jointly
considering the two objectives, we will introduce the concept of Pareto optimality, which describes
a scenario where neither the statistical power of estimating the treatment effect nor the regret can
be improved without compromising the other. First, we focus on the best achievable quality of
the inference of Œ∏ŒΩ, regardless of the regret incurred during the experiment. The following lemma
describes the intrinsic difficulty of conducting inference for the dynamic treatment effect."
OPTIMALITY OF THE DESIGN,0.3724137931034483,"Lemma 1 For any constant c2 > 0, inf ÀÜŒ∏ supŒΩ‚ààE1 PŒΩ
ÀÜŒ∏ ‚àíŒ∏ŒΩ

1 > 2
‚àö"
OPTIMALITY OF THE DESIGN,0.3758620689655172,"2c2œÉ0
‚àö T 
‚â•1"
OPTIMALITY OF THE DESIGN,0.3793103448275862,"2 ‚àíc2. Moreover,"
OPTIMALITY OF THE DESIGN,0.38275862068965516,"inf ÀÜŒ∏ supŒΩ‚ààE1 EŒΩ
hÀÜŒ∏ ‚àíŒ∏ŒΩ

1 i
‚â•
‚àö"
OPTIMALITY OF THE DESIGN,0.38620689655172413,"2œÉ0
8
‚àö T ."
OPTIMALITY OF THE DESIGN,0.3896551724137931,"This lemma establishes an information-theoretic lower bound on the best achievable quality of
inference for Œ∏ŒΩ. It indicates that, for any estimator ÀÜŒ∏, there will always exist a challenging instance
that results in the estimator being at least Œò(1/
‚àö"
OPTIMALITY OF THE DESIGN,0.3931034482758621,"T) away from the true value. Our proof idea is
to reduce the two-dimensional estimation problem to a one-dimensional problem. Specifically, we
consider a subset of all the problem instances E0 := {ŒΩ ‚ààE1 : Œ∏a,1 = 0, ‚àÄa ‚àà{1, 2}}, which
includes all the stationary instances (i.e., without time trends), and this provides a natural lower"
OPTIMALITY OF THE DESIGN,0.39655172413793105,"bound that inf ÀÜŒ∏ supŒΩ‚ààE1 PŒΩ
ÀÜŒ∏ ‚àíŒ∏ŒΩ

1 > 2
‚àö"
OPTIMALITY OF THE DESIGN,0.4,"2c2œÉ0
‚àö T"
OPTIMALITY OF THE DESIGN,0.40344827586206894,"
‚â•inf ÀÜŒ∏ supŒΩ‚ààE0 PŒΩ
ÀÜŒ∏ ‚àíŒ∏ŒΩ

1 > 2
‚àö"
OPTIMALITY OF THE DESIGN,0.4068965517241379,"2c2œÉ0
‚àö T"
OPTIMALITY OF THE DESIGN,0.4103448275862069,"
.
For the instance class E0, the problem is reduced to estimating only the intercept terms. By choosing
Œ± = 1, Theorem 1 indicates ‚à•ÀÜŒ∏L-EOFU(1)
T
‚àíŒ∏ŒΩ‚à•1 can be upper bounded by O(1/
‚àö"
OPTIMALITY OF THE DESIGN,0.41379310344827586,"T), which matches
with the lower bound we derive in Lemma 1 in terms of the dependence of the order of T. Therefore,
we can state the following theorem."
OPTIMALITY OF THE DESIGN,0.41724137931034483,"Theorem 3 (Optimal Error Rate Design) ÀÜŒ∏L-EOFU(1)
T
achieves the optimal 1/
‚àö"
OPTIMALITY OF THE DESIGN,0.4206896551724138,"T error rate, which"
OPTIMALITY OF THE DESIGN,0.4241379310344828,"can not be improved up to a constant factor. Moreover, inf ÀÜŒ∏ supŒΩ‚ààE1 EŒΩ
hÀÜŒ∏ ‚àíŒ∏ŒΩ

1"
OPTIMALITY OF THE DESIGN,0.42758620689655175,"i
= Œò(1/
‚àö T)."
OPTIMALITY OF THE DESIGN,0.43103448275862066,"Theorem 3 further establishes that the existence of ÀÜŒ∏L-EOFU(1)
T
confirms the tightness of the 1/
‚àö"
OPTIMALITY OF THE DESIGN,0.43448275862068964,"T
lower bound derived in Lemma 1. As mentioned earlier, setting Œ± = 1 corresponds to the simple
switchback experimental design. Theorem 3 implies that even in the presence of linear trends,
switchback experiments can still maintain strong statistical power. However, there is no free lunch.
Choosing Œ± = 1 results in a regret upper bound of O(T 2), which can be highly undesirable if
minimizing regret is also one of the objectives. Now, we turn to discuss about the other extreme that
the regret minimization is the only objective. We present the minimax regret lower bound as follows."
OPTIMALITY OF THE DESIGN,0.4379310344827586,"Lemma 2 (Regret Lower Bound) infœÄ supŒΩ‚ààE1 E [RœÄ
ŒΩ(T)] ‚â•œÉ0
‚àö"
OPTIMALITY OF THE DESIGN,0.4413793103448276,"T
32e ."
OPTIMALITY OF THE DESIGN,0.44482758620689655,"This lemma describes that any admissible policy œÄ will face some hard instance such that the expected
regret could be no less than ‚Ñ¶(
‚àö"
OPTIMALITY OF THE DESIGN,0.4482758620689655,"T). Similar as before, we can bridge our problem with the tradition
stochastic bandit problem by noticing that E0 ‚äÇE1. It is widely recognized that the stochastic bandit
problem has a well-established minimax regret lower bound of ‚Ñ¶(
‚àö"
OPTIMALITY OF THE DESIGN,0.4517241379310345,"T) [55]. Intuitively, given that
the traditional stochastic bandit problem is a special case within our framework, this lower bound
naturally extends to our setting. Formally, by Theorem 2, we have the following theorem, which
states that the
‚àö"
OPTIMALITY OF THE DESIGN,0.45517241379310347,T lower bound is indeed possible to achieve and is optimal.
OPTIMALITY OF THE DESIGN,0.4586206896551724,"Theorem 4 (Optimal Regret Rate Design) For any 0 < Œ± ‚â§
1
4, L-EOFU(Œ±) can guarantee a
regret of e
O(
‚àö"
OPTIMALITY OF THE DESIGN,0.46206896551724136,"T), which can not be further improved up to logarithmic factors. Furthermore,
infœÄ supŒΩ‚ààE1 E [RœÄ
ŒΩ(T)] = eŒò(
‚àö T)."
OPTIMALITY OF THE DESIGN,0.46551724137931033,"Theorem 4 reveals that any Œ± ‚àà(0, 1"
OPTIMALITY OF THE DESIGN,0.4689655172413793,"4] yields a homogeneous regret rate dependence on T. However,
the estimation error, as shown in Theorem 1, decreases with increasing Œ±. Consequently, increasing Œ±
from nearly 0 to 1"
OPTIMALITY OF THE DESIGN,0.4724137931034483,"4 does not significantly increase the regret rate, which remains at e
O(
‚àö"
OPTIMALITY OF THE DESIGN,0.47586206896551725,"T). However,
it significantly reduces the estimation error bound. Therefore, there is limited motivation for decision
makers to select Œ± < 1"
OPTIMALITY OF THE DESIGN,0.4793103448275862,"4. Nevertheless, there is no free lunch. Even if we choose Œ± = 1"
OPTIMALITY OF THE DESIGN,0.4827586206896552,"4, the estimation
error can only be bounded by O(T ‚àí1"
OPTIMALITY OF THE DESIGN,0.4862068965517241,"8 ), which is much larger than the optimal bound of
1
‚àö"
OPTIMALITY OF THE DESIGN,0.4896551724137931,"T . While
the optimal regret rate in this case aligns with that of the traditional stochastic bandit, the optimal
design for linear trends faces additional challenges. Intuitively, a regret rate optimal design for linear
trends needs to improve from a naive design‚Äôs T 2 regret to the
‚àö"
OPTIMALITY OF THE DESIGN,0.49310344827586206,"T order. In contrast, the regret rate
optimal design for traditional multi-armed bandits only needs to improve from T regret to
‚àö T."
OPTIMALITY OF THE DESIGN,0.496551724137931,"From the two cases discussed above, we have uncovered a fundamental trade-off between regret and
the statistical power. The design that achieves the optimal regret rate suffers from a large estimation
error, while the design that achieves the optimal error rate incurs a large regret. To address this
trade-off, we now employ the concept of Pareto optimality from multi-objective optimization theory.
This concept allows us to characterize the circumstances where neither regret nor estimation error
of the treatment effect can be improved without worsening the other. In other words, we aim to
statistically determine the type of optimality that can be achieved between these two extremes."
OPTIMALITY OF THE DESIGN,0.5,"Formally, we first define the policy class Œ†Œ≤ as Œ†Œ≤ := {œÄ : ‚àÉc0 > 0, ‚àÄŒΩ ‚ààE1 and T, E[RœÄ
ŒΩ(T)] ‚â§
c0T Œ≤}, where c0 in this context can depend on universal constants or logarithmic terms of T, but it
cannot include any polynomial terms of T. The set Œ†Œ≤ encompasses all policies that can achieve a
regret of e
O(T Œ≤). It is evident that L-EOFU(Œ±) belongs to Œ† 1"
OPTIMALITY OF THE DESIGN,0.503448275862069,2 ‚à®(2Œ±). Our aim is to investigate the best
OPTIMALITY OF THE DESIGN,0.506896551724138,"achievable estimation error when the decision maker incurs a regret of at most e
O(T Œ≤). The following
lemma reveals the inherent difficulty of conducting inference under limited regret budget."
OPTIMALITY OF THE DESIGN,0.5103448275862069,"Lemma 3 For any œÄ ‚ààŒ†Œ≤, there exists a constant cœÄ only dependent of numerical constants and"
OPTIMALITY OF THE DESIGN,0.5137931034482759,"logarithm terms of T, such that for any Œæ > 0, inf ÀÜŒ∏ maxŒΩ‚ààE1 PœÄ
ŒΩ
ÀÜŒ∏ ‚àíŒ∏ŒΩ

1 ‚â•cœÄœÉ0ŒæT ‚àíŒ≤"
OPTIMALITY OF THE DESIGN,0.5172413793103449,"4

‚â•1 2 ‚àíŒæ."
OPTIMALITY OF THE DESIGN,0.5206896551724138,"Moreover, inf ÀÜŒ∏ maxŒΩ‚ààE1 EœÄ
ŒΩ
hÀÜŒ∏ ‚àíŒ∏ŒΩ

1"
OPTIMALITY OF THE DESIGN,0.5241379310344828,"i
‚â•cœÄœÉ0"
OPTIMALITY OF THE DESIGN,0.5275862068965518,16 T ‚àíŒ≤ 4 .
OPTIMALITY OF THE DESIGN,0.5310344827586206,"Lemma 3 demonstrates that when employing an assignment policy œÄ with a regret of e
O(T Œ≤), any
estimator, regardless of its sophistication, will encounter challenging instances where the estimation
error is at least e‚Ñ¶(T ‚àíŒ≤"
OPTIMALITY OF THE DESIGN,0.5344827586206896,"4 ). This finding distinguishes our study, which incorporates linear trends, from
the work of [74] that investigates multi-armed bandit experimental design without linear trends. In
the absence of linear trends, [74] reveals that under an assignment policy œÄ with a regret of e
O(T Œ≤),
the estimation error has a minimax lower bound of e‚Ñ¶(T ‚àíŒ≤"
OPTIMALITY OF THE DESIGN,0.5379310344827586,"2 ). The key distinction arises from the fact
that linear trends can significantly increase the cost of exploration, resulting in higher regret during
the exploration phase. Consequently, with the same regret budget, the information collected in our
case may be substantially less than in the absence of linear trends. The proof follows this intuition by
constructing two similar instances with the same optimal arm but slightly different suboptimal arms.
The only way to obtain useful information to distinguish the two is by playing the suboptimal arm.
Given a regret budget of T Œ≤, at most approximately T
Œ≤
2 samples can be obtained from the suboptimal
arm, leading to a lower bound of T ‚àíŒ≤ 4 ."
OPTIMALITY OF THE DESIGN,0.5413793103448276,Theorem 5 (Pareto Optimal Design) If Œ± ‚â•1
OPTIMALITY OF THE DESIGN,0.5448275862068965,"4, L-EOFU(Œ±) ‚ààŒ†2Œ±, and thus, the T ‚àíŒ±"
ERROR RATE,0.5482758620689655,"2 error rate
in Theorem 1 can not be further improved in terms of the dependence on T. Furthermore, for a fixed
Œ≤ ‚â•1"
ERROR RATE,0.5517241379310345,"2, inf ÀÜŒ∏T ,œÄ‚ààŒ†Œ≤ maxŒΩ‚ààE1 EœÄ
ŒΩ
hÀÜŒ∏T ‚àíŒ∏ŒΩ

1"
ERROR RATE,0.5551724137931034,"i
= eŒò

T ‚àíŒ≤ 4

."
ERROR RATE,0.5586206896551724,The theorem establishes that the regret bound of T ‚àíŒ±
ERROR RATE,0.5620689655172414,"2 achieved by L-EOFU(Œ±) cannot be improved by
a constant factor, given that the decision-making policy belongs to Œ†2Œ±. This implies that L-EOFU(Œ±)
is Pareto optimal for any Œ± ‚â•1"
ERROR RATE,0.5655172413793104,"4. The reasons are as follows: on the one hand, if one desires a
better guarantee on the estimation error than T ‚àíŒ±"
ERROR RATE,0.5689655172413793,"2 , the only option is to accept a higher regret rate
by adopting a larger decision-making policy class Œ†2Œ±+œµ for some œµ > 0. On the other hand, if a
regret of e
O(T 2Œ±‚àíœµ) is aimed for, the estimation error will inevitably increase to the order of T ‚àí2Œ±+œµ.
In other words, improving either the statistical power of causal inference or the efficiency of online
decision-making necessitates a degradation in the other. Intuitively, Theorem 5 captures a statistical
Pareto frontier for the trade-off between conducting causal inference and minimizing regret, which
can be expressed as Estimation Error = Œò(Regret‚àí1"
ERROR RATE,0.5724137931034483,"4 ). In Theorem 3, we reveal that Œ± = 1 leads to
the optimal design in terms of error rate. By combining Theorem 5, we further conclude that the
large T 2 regret incurred by Œ± = 1 is necessary for any estimator achieving the optimal error rate
of
1
‚àö"
ERROR RATE,0.5758620689655173,"T . Moreover, the T ‚àí1"
ERROR RATE,0.5793103448275863,8 error rate achieved by Œ± = 1
ERROR RATE,0.5827586206896552,"4, although relatively large, is the best that
any regret-optimal policy can achieve. Figure 3 illustrates the Pareto frontier under linear trends,
alongside the Pareto frontier for standard multi-armed bandit experimental design studied in [74]."
ERROR RATE,0.5862068965517241,Regret
ERROR RATE,0.5896551724137931,Estimation Error
ERROR RATE,0.593103448275862,"ùëá!/#
ùëá
ùëá# ùëá$!/# ùëá$!/% ùëá$!/&"
ERROR RATE,0.596551724137931,"Worst-case 
Infeasible"
ERROR RATE,0.6,Standard multi-armed bandit experiment design
ERROR RATE,0.603448275862069,Non-stationary experimental design with linear trends (this work)
ERROR RATE,0.6068965517241379,"Error ‚âÉRegret!""/$"
ERROR RATE,0.6103448275862069,"Error ‚âÉRegret!""/%"
ERROR RATE,0.6137931034482759,L‚àíEOFU 1/4
ERROR RATE,0.6172413793103448,Theorem 4
ERROR RATE,0.6206896551724138,L‚àíEOFU 1
ERROR RATE,0.6241379310344828,Theorem 3
ERROR RATE,0.6275862068965518,Figure 3: The Pareto Frontier.
ERROR RATE,0.6310344827586207,"From Figure 3, we can observe the sig-
nificant impact of the non-stationary
structure on optimal experimental de-
sign. First, in standard bandit exper-
iments, the maximum possible regret
is of the order T since all parameters
are assumed to be bounded. However,
with the presence of linear trends, tra-
ditional designs such as switchback ex-
periments are likely to incur regret of
the order T 2. Furthermore, under the
same regret budget, the best achievable
estimation error bounds differ substan-
tially. For instance, given assignment
policies with regret of the order T, the
optimal design for standard bandit experiments can attain an error bound of T ‚àí1/2, whereas in the
presence of linear trends, the best achievable bound is T ‚àí1/4. Intuitively, this is because linear trends
make gathering information from the suboptimal arm increasingly costly in terms of regret. Similarly,"
ERROR RATE,0.6344827586206897,"when linear trends exist, the decision maker will always have to pay additional regret to obtain an
estimator with the same level of precision. Additionally, there is a worst-case infeasible region as
indicated by Lemma 2 that no policy can achieve a regret smaller than
‚àö"
ERROR RATE,0.6379310344827587,T order in the worst case.
DISCUSSION,0.6413793103448275,"3
Discussion"
DISCUSSION,0.6448275862068965,"In this section, we‚Äôll discuss the limitations and future directions of our work. Firstly, the assumption
of prior knowledge about non-stationarity following a linear form is strong. While it provides
a starting point for understanding experiment design under structured non-stationarity, practical
approaches are needed. One option is to use historical data for prior information, if there exists
some. Alternatively, we can use data collected during the first phase of our L-OFUE algorithms to
verify trends. Another question is model misspecification. What if the true underlying model is
non-parametric, but the decision maker prefers using interpretable linear functions? Understanding
how our design performs in such scenarios will provide valuable insights into its practical applicability.
Furthermore, our framework and results are formulated based on a specific notion of non-stationarity.
Exploring whether our framework can be adapted to different formulations of non-stationarity is
another area of interest for future research. Another important direction for our future research is the
exploration of instance-dependent regret bounds under linear and polynomial trends. In this paper,
we focus on studying the worst-case optimal regret. However, in the field of classical stochastic
bandits, there has been increasing attention on instance-dependent regret bounds (e.g., [11] [33][5]).
It would be valuable to extend our analysis to incorporate instance-dependent parameters. Defining
suitable instance-dependent parameters is not a trivial task, as the classical parameters such as arm
gaps may not be directly applicable in our setting, where the parameters themselves change over time.
Another more practical question that people may be of interest is how to choose Œ±. One possible
approach is to adaptively end the first phase of the experiment. Ideally, continuous monitoring during
Phase 1 would allow for the experiment to halt as soon as certain criteria are met, eliminating the
need to manually select Œ±. However, reliable and efficient continuous monitoring of the switchback
experiment poses significant challenges. Even in basic A/B testing without a linear trend, designing
continuous monitoring requires substantial effort and delicate power analysis, as demonstrated in
[45]."
CONCLUSION,0.6482758620689655,"4
Conclusion"
CONCLUSION,0.6517241379310345,"In this work, we present a comprehensive investigation into the problem of non-stationary experi-
mental design with linear trends. Our focus is on two objectives: accurate estimation of dynamic
treatment effects and minimizing welfare loss within the experiment. For scenarios involving linear
trends, we propose an efficient design called L-EOFU. This design can be customized to achieve
optimal estimation error rates, optimal regret rates, or strike a Pareto optimal trade-off between the
two objectives. To establish the efficacy of our approach, we provide information-theoretical lower
bounds that highlight the inherent challenges in estimating dynamic treatment effects and minimizing
welfare loss. Additionally, we unveil the fundamental trade-off between the two objectives."
REFERENCES,0.6551724137931034,References
REFERENCES,0.6586206896551724,"[1] Alberto Abadie and Jinglong Zhao. Synthetic controls for experimental design. arXiv preprint
arXiv:2108.02196, 2021."
REFERENCES,0.6620689655172414,"[2] Yasin Abbasi-Yadkori, D√°vid P√°l, and Csaba Szepesv√°ri. Improved algorithms for linear
stochastic bandits. In Advances in Neural Information Processing Systems, pages 2312‚Äì2320,
2011."
REFERENCES,0.6655172413793103,"[3] Karun Adusumilli.
Risk and optimal policies in bandit experiments.
arXiv preprint
arXiv:2112.06363, 2021."
REFERENCES,0.6689655172413793,"[4] Volkan E Akman and Adrian E Raftery. Asymptotic inference for a change-point poisson
process. the Annals of Statistics, pages 1583‚Äì1590, 1986."
REFERENCES,0.6724137931034483,"[5] Raman Arora, Teodor Vanislavov Marinov, and Mehryar Mohri. Corralling stochastic bandit
algorithms. In International Conference on Artificial Intelligence and Statistics, pages 2116‚Äì
2124. PMLR, 2021."
REFERENCES,0.6758620689655173,"[6] Onur Atan, William R Zame, and Mihaela Schaar. Sequential patient recruitment and allocation
for adaptive clinical trials. In The 22nd International Conference on Artificial Intelligence and
Statistics, pages 1891‚Äì1900. PMLR, 2019."
REFERENCES,0.6793103448275862,"[7] Susan Athey and Stefan Wager. Policy learning with observational data.
Econometrica,
89(1):133‚Äì161, 2021."
REFERENCES,0.6827586206896552,"[8] Alexia Atsidakou, Orestis Papadigenopoulos, Soumya Basu, Constantine Caramanis, and Sanjay
Shakkottai. Combinatorial blocking bandits with stochastic delays. In International Conference
on Machine Learning, pages 404‚Äì413. PMLR, 2021."
REFERENCES,0.6862068965517242,"[9] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed
bandit problem. Machine learning, 47(2-3):235‚Äì256, 2002."
REFERENCES,0.6896551724137931,"[10] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic
multiarmed bandit problem. SIAM journal on computing, 32(1):48‚Äì77, 2002."
REFERENCES,0.6931034482758621,"[11] Akshay Balsubramani, Zohar Karnin, Robert E Schapire, and Masrour Zoghi.
Instance-
dependent regret bounds for dueling bandits. In Conference on Learning Theory, pages 336‚Äì360.
PMLR, 2016."
REFERENCES,0.696551724137931,"[12] Soumya Basu, Orestis Papadigenopoulos, Constantine Caramanis, and Sanjay Shakkottai.
Contextual blocking bandits. In International Conference on Artificial Intelligence and Statistics,
pages 271‚Äì279. PMLR, 2021."
REFERENCES,0.7,"[13] Soumya Basu, Rajat Sen, Sujay Sanghavi, and Sanjay Shakkottai. Blocking bandits. Advances
in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.7034482758620689,"[14] Donald A Berry. Bayesian clinical trials. Nature reviews Drug discovery, 5(1):27‚Äì36, 2006."
REFERENCES,0.7068965517241379,"[15] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with
non-stationary rewards. In Advances in neural information processing systems, pages 199‚Äì207,
2014."
REFERENCES,0.7103448275862069,"[16] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Opera-
tions research, 63(5):1227‚Äì1244, 2015."
REFERENCES,0.7137931034482758,"[17] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Optimal exploration‚Äìexploitation in a multi-armed
bandit problem with non-stationary rewards. Stochastic Systems, 9(4):319‚Äì337, 2019."
REFERENCES,0.7172413793103448,"[18] Lilian Besson, Emilie Kaufmann, Odalric-Ambrym Maillard, and Julien Seznec. Efficient
change-point detection for tackling piecewise-stationary bandits. The Journal of Machine
Learning Research, 23(1):3337‚Äì3376, 2022."
REFERENCES,0.7206896551724138,"[19] Nikhil Bhat, Vivek F Farias, Ciamac C Moallemi, and Deeksha Sinha. Near-optimal ab testing.
Management Science, 66(10):4477‚Äì4495, 2020."
REFERENCES,0.7241379310344828,"[20] Nicholas Bishop, Hau Chan, Debmalya Mandal, and Long Tran-Thanh. Adversarial blocking
bandits. Advances in Neural Information Processing Systems, 33:8139‚Äì8149, 2020."
REFERENCES,0.7275862068965517,"[21] Iavor Bojinov, David Simchi-Levi, and Jinglong Zhao. Design and analysis of switchback
experiments. Management Science, 2022."
REFERENCES,0.7310344827586207,"[22] Yang Cao, Zheng Wen, Branislav Kveton, and Yao Xie. Nearly optimal adaptive procedure
with change detection for piecewise-stationary bandit. In The 22nd International Conference on
Artificial Intelligence and Statistics, pages 418‚Äì427. PMLR, 2019."
REFERENCES,0.7344827586206897,"[23] Ningyuan Chen, Xuefeng Gao, and Yi Xiong. Debiasing samples from online learning using
bootstrap. In International Conference on Artificial Intelligence and Statistics, pages 8514‚Äì8533.
PMLR, 2022."
REFERENCES,0.7379310344827587,"[24] Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Hedging the drift: Learning to
optimize under nonstationarity. Management Science, 68(3):1696‚Äì1713, 2022."
REFERENCES,0.7413793103448276,"[25] Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Nonstationary reinforcement learning:
The blessing of (more) optimism. Management Science, 2023."
REFERENCES,0.7448275862068966,"[26] Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin,
and Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning
Theory, pages 6‚Äì1. JMLR Workshop and Conference Proceedings, 2012."
REFERENCES,0.7482758620689656,"[27] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff
functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence
and Statistics, pages 208‚Äì214. JMLR Workshop and Conference Proceedings, 2011."
REFERENCES,0.7517241379310344,"[28] Giulia Clerici, Pierre Laforgue, and Nicolo Cesa-Bianchi. Linear bandits with memory: from
rotting to rising. arXiv preprint arXiv:2302.08345, 2023."
REFERENCES,0.7551724137931034,"[29] Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under
bandit feedback. In Proceedings of the 21st Conference on Learning Theory, 2008."
REFERENCES,0.7586206896551724,"[30] Maria Dimakopoulou, Zhimei Ren, and Zhengyuan Zhou. Online multi-armed bandits with
adaptive inference. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.7620689655172413,"[31] Miroslav Dud√≠k, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation
and optimization. Statistical Science, 29(4):485‚Äì511, 2014."
REFERENCES,0.7655172413793103,"[32] Vivek Farias, Ciamac Moallemi, Tianyi Peng, and Andrew Zheng. Synthetically controlled
bandits. arXiv preprint arXiv:2202.07079, 2022."
REFERENCES,0.7689655172413793,"[33] Dylan J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu. Instance-dependent
complexity of contextual bandits and reinforcement learning: A disagreement-based perspective.
arXiv preprint arXiv:2010.03104, 2020."
REFERENCES,0.7724137931034483,"[34] Ayoub Foussoul, Vineet Goyal, and Varun Gupta. Mnl-bandit in non-stationary environments.
arXiv preprint arXiv:2303.02504, 2023."
REFERENCES,0.7758620689655172,"[35] Adama Gansan√©, Leah F Moriarty, Didier M√©nard, Isidore Yerbanga, Esperance Ouedraogo,
Paul Sondo, Rene Kinda, Casimir Tarama, Edwige Soulama, Madou Tapsoba, et al. Anti-
malarial efficacy and resistance monitoring of artemether-lumefantrine and dihydroartemisinin-
piperaquine shows inadequate efficacy in children in burkina faso, 2017‚Äì2018. Malaria Journal,
20:1‚Äì12, 2021."
REFERENCES,0.7793103448275862,"[36] Aur√©lien Garivier and Eric Moulines. On upper-confidence bound policies for switching
bandit problems. In International Conference on Algorithmic Learning Theory, pages 174‚Äì188.
Springer, 2011."
REFERENCES,0.7827586206896552,"[37] TIM Gerrodette. A power analysis for detecting trends. Ecology, 68(5):1364‚Äì1372, 1987."
REFERENCES,0.7862068965517242,"[38] Peter W Glynn, Ramesh Johari, and Mohammad Rasouli. Adaptive experimental design with
temporal interference: A maximum likelihood approach. Advances in Neural Information
Processing Systems, 33:15054‚Äì15064, 2020."
REFERENCES,0.7896551724137931,"[39] Peter W Glynn and Zeyu Zheng. Estimation and inference for non-stationary arrival models
with a linear trend. In 2019 Winter Simulation Conference (WSC), pages 3764‚Äì3773. IEEE,
2019."
REFERENCES,0.7931034482758621,"[40] Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence
intervals for policy evaluation in adaptive experiments. Proceedings of the National Academy
of Sciences, 118(15), 2021."
REFERENCES,0.7965517241379311,"[41] Jinyong Hahn, Keisuke Hirano, and Dean Karlan. Adaptive experimental design using the
propensity score. Journal of Business & Economic Statistics, 29(1):96‚Äì108, 2011."
REFERENCES,0.8,"[42] Peter L Jackson, John A Muckstadt, and Yuexing Li. Multiperiod stock allocation via robust
optimization. Management Science, 65(2):794‚Äì818, 2019."
REFERENCES,0.803448275862069,"[43] Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan.
Online
optimization: Competing with dynamic comparators. In Artificial Intelligence and Statistics,
pages 398‚Äì406. PMLR, 2015."
REFERENCES,0.8068965517241379,"[44] Ramesh Johari, Hannah Li, Inessa Liskovich, and Gabriel Y Weintraub. Experimental design in
two-sided platforms: An analysis of bias. Management Science, 2022."
REFERENCES,0.8103448275862069,"[45] Ramesh Johari, Leo Pekelis, and David J Walsh. Always valid inference: Bringing sequential
analysis to a/b testing. arXiv preprint arXiv:1512.04922, 2015."
REFERENCES,0.8137931034482758,"[46] Nathan Kallus and Angela Zhou. Confounding-robust policy improvement. Advances in neural
information processing systems, 31, 2018."
REFERENCES,0.8172413793103448,"[47] Zohar S Karnin and Oren Anava. Multi-armed bandits: Competing with optimal sequences.
Advances in Neural Information Processing Systems, 29, 2016."
REFERENCES,0.8206896551724138,"[48] Maximilian Kasy and Anja Sautmann. Adaptive treatment assignment in experiments for policy
choice. Econometrica, 89(1):113‚Äì132, 2021."
REFERENCES,0.8241379310344827,"[49] Masahiro Kato, Takuya Ishihara, Junya Honda, Yusuke Narita, et al. Efficient adaptive ex-
perimental design for average treatment effect estimation. arXiv preprint arXiv:2002.05308,
2020."
REFERENCES,0.8275862068965517,"[50] N Bora Keskin and Assaf Zeevi. Chasing demand: Learning and earning in a changing
environment. Mathematics of Operations Research, 42(2):277‚Äì307, 2017."
REFERENCES,0.8310344827586207,"[51] Robert Kleinberg and Nicole Immorlica. Recharging bandits. In 2018 IEEE 59th Annual
Symposium on Foundations of Computer Science (FOCS), pages 309‚Äì319. IEEE, 2018."
REFERENCES,0.8344827586206897,"[52] Ron Kohavi, Randal M Henne, and Dan Sommerfield. Practical guide to controlled experiments
on the web: listen to your customers not to the hippo. In Proceedings of the 13th ACM SIGKDD
international conference on Knowledge discovery and data mining, pages 959‚Äì967, 2007."
REFERENCES,0.8379310344827586,"[53] Michael E Kuhl, Halim Damerdji, and James R Wilson. Estimating and simulating poisson
processes with trends or asymmetric cyclic effects. In Proceedings of the 29th conference on
Winter simulation, pages 287‚Äì295, 1997."
REFERENCES,0.8413793103448276,"[54] Tze Leung Lai, Herbert Robbins, et al. Asymptotically efficient adaptive allocation rules.
Advances in applied mathematics, 6(1):4‚Äì22, 1985."
REFERENCES,0.8448275862068966,"[55] Tor Lattimore and Csaba Szepesv√°ri. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.8482758620689655,"[56] Nir Levine, Koby Crammer, and Shie Mannor. Rotting bandits. Advances in neural information
processing systems, 30, 2017."
REFERENCES,0.8517241379310345,"[57] Lihong Li, R√©mi Munos, and Csaba Szepesv√°ri. Toward minimax off-policy value estimation.
In Artificial Intelligence and Statistics, pages 608‚Äì616. PMLR, 2015."
REFERENCES,0.8551724137931035,"[58] Fang Liu, Joohyun Lee, and Ness Shroff. A change-detection based framework for piecewise-
stationary multi-armed bandit problem. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018."
REFERENCES,0.8586206896551725,"[59] Yueyang Liu, Benjamin Van Roy, and Kuang Xu. Nonstationary bandit learning via predictive
sampling. arXiv preprint arXiv:2205.01970, 2022."
REFERENCES,0.8620689655172413,"[60] Yueyang Liu, Benjamin Van Roy, and Kuang Xu. A definition of non-stationary bandits. arXiv
preprint arXiv:2302.12202, 2023."
REFERENCES,0.8655172413793103,"[61] Haipeng Luo, Chen-Yu Wei, Alekh Agarwal, and John Langford. Efficient contextual bandits in
non-stationary worlds. In Conference On Learning Theory, pages 1739‚Äì1776. PMLR, 2018."
REFERENCES,0.8689655172413793,"[62] Weichao Mao, Kaiqing Zhang, Ruihao Zhu, David Simchi-Levi, and Tamer Ba¬∏sar. Model-free
non-stationary rl: Near-optimal regret and applications in multi-agent rl and inventory control.
arXiv preprint arXiv:2010.03161, 2020."
REFERENCES,0.8724137931034482,"[63] William A Massey, Geraldine A Parker, and Ward Whitt. Estimating the parameters of a
nonhomogeneous poisson process with linear rate. Telecommunication systems, 5:361‚Äì388,
1996."
REFERENCES,0.8758620689655172,"[64] Alberto Maria Metelli, Francesco Trovo, Matteo Pirola, and Marcello Restelli. Stochastic rising
bandits. In International Conference on Machine Learning, pages 15421‚Äì15457. PMLR, 2022."
REFERENCES,0.8793103448275862,"[65] Seungki Min and Daniel Russo. An information-theoretic analysis of nonstationary bandit
learning. arXiv preprint arXiv:2302.04452, 2023."
REFERENCES,0.8827586206896552,"[66] Yonatan Mintz, Anil Aswani, Philip Kaminsky, Elena Flowers, and Yoshimi Fukuoka. Nonsta-
tionary bandits with habituation and recovery dynamics. Operations Research, 68(5):1493‚Äì1516,
2020."
REFERENCES,0.8862068965517241,"[67] Xinkun Nie, Xiaoying Tian, Jonathan Taylor, and James Zou. Why adaptively collected data
have negative bias and how to correct for it. In International Conference on Artificial Intelligence
and Statistics, pages 1261‚Äì1269. PMLR, 2018."
REFERENCES,0.8896551724137931,"[68] Tobias Nilsson and Gunnar Elgered. Long-term trends in the atmospheric water vapor con-
tent estimated from ground-based gps data. Journal of Geophysical Research: Atmospheres,
113(D19), 2008."
REFERENCES,0.8931034482758621,"[69] Steven Piantadosi. Clinical trials: a methodologic perspective. John Wiley & Sons, 2017."
REFERENCES,0.896551724137931,"[70] Chao Qin and Daniel Russo. Adaptivity and confounding in multi-armed bandit experiments.
arXiv preprint arXiv:2202.09036, 2022."
REFERENCES,0.9,"[71] Julien Seznec, Andrea Locatelli, Alexandra Carpentier, Alessandro Lazaric, and Michal Valko.
Rotting bandits are no harder than stochastic ones. In The 22nd International Conference on
Artificial Intelligence and Statistics, pages 2564‚Äì2572. PMLR, 2019."
REFERENCES,0.903448275862069,"[72] Julien Seznec, Pierre Menard, Alessandro Lazaric, and Michal Valko. A single algorithm for
both restless and rested rotting bandits. In International Conference on Artificial Intelligence
and Statistics, pages 3784‚Äì3794. PMLR, 2020."
REFERENCES,0.906896551724138,"[73] David Simchi-Levi and Chonghuan Wang. Multi-armed bandit experimental design: Online
decision-making and adaptive inference. Available at SSRN 4224969, 2022."
REFERENCES,0.9103448275862069,"[74] David Simchi-Levi and Chonghuan Wang. Multi-armed bandit experimental design: Online
decision-making and adaptive inference. In Proceedings of The 26th International Conference
on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research, pages
3086‚Äì3097. PMLR, 25‚Äì27 Apr 2023."
REFERENCES,0.9137931034482759,"[75] Richard Simon. Adaptive treatment assignment methods and clinical trials. Biometrics, pages
743‚Äì749, 1977."
REFERENCES,0.9172413793103448,"[76] Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback
through counterfactual risk minimization.
The Journal of Machine Learning Research,
16(1):1731‚Äì1755, 2015."
REFERENCES,0.9206896551724137,"[77] William R Thompson. On the likelihood that one unknown probability exceeds another in view
of the evidence of two samples. Biometrika, 25(3/4):285‚Äì294, 1933."
REFERENCES,0.9241379310344827,"[78] Stefan Wager and Kuang Xu. Diffusion asymptotics for sequential experiments. arXiv preprint
arXiv:2101.09855, 2021."
REFERENCES,0.9275862068965517,"[79] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48.
Cambridge University Press, 2019."
REFERENCES,0.9310344827586207,"[80] Jing Wang, Peng Zhao, and Zhi-Hua Zhou. Revisiting weighted strategy for non-stationary
parametric bandits. arXiv preprint arXiv:2303.02691, 2023."
REFERENCES,0.9344827586206896,"[81] Siwei Wang, Longbo Huang, and John Lui. Restless-ucb, an efficient and low-complexity
algorithm for online restless bandits. Advances in Neural Information Processing Systems,
33:11878‚Äì11889, 2020."
REFERENCES,0.9379310344827586,"[82] Youfa Wang, May A Beydoun, Lan Liang, Benjamin Caballero, and Shiriki K Kumanyika.
Will all americans become overweight or obese? estimating the progression and cost of the us
obesity epidemic. Obesity, 16(10):2323‚Äì2330, 2008."
REFERENCES,0.9413793103448276,"[83] Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudƒ±k. Optimal and adaptive off-policy evalua-
tion in contextual bandits. In International Conference on Machine Learning, pages 3589‚Äì3597.
PMLR, 2017."
REFERENCES,0.9448275862068966,"[84] Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Tracking the best expert in non-stationary stochastic
environments. Advances in neural information processing systems, 29, 2016."
REFERENCES,0.9482758620689655,"[85] Peter Whittle. Restless bandits: Activity allocation in a changing world. Journal of applied
probability, 25(A):287‚Äì298, 1988."
REFERENCES,0.9517241379310345,"[86] WHO. World malaria report 2022. World Health Organization, 2022."
REFERENCES,0.9551724137931035,"[87] Yuhang Wu, Zeyu Zheng, Guangyu Zhang, Zuohua Zhang, and Chu Wang. Adaptive a/b tests
and simultaneous treatment parameter optimization. arXiv preprint arXiv:2210.06737, 2022."
REFERENCES,0.9586206896551724,"[88] Yuhang Wu, Zeyu Zheng, Guangyu Zhang, Zuohua Zhang, and Chu Wang. Non-stationary a/b
tests. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining, KDD ‚Äô22, page 2079‚Äì2089, New York, NY, USA, 2022. Association for Computing
Machinery."
REFERENCES,0.9620689655172414,"[89] Xiao Xu, Fang Dong, Yanghua Li, Shaojian He, and Xin Li. Contextual-bandit based personal-
ized recommendation with time-varying user interests. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 34, pages 6518‚Äì6525, 2020."
REFERENCES,0.9655172413793104,"[90] Kevin P Yancey and Burr Settles. A sleeping, recovering bandit algorithm for optimizing
recurring notifications. In Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pages 3008‚Äì3016, 2020."
REFERENCES,0.9689655172413794,"[91] Adoke Yeka, Erika Wallender, Ronald Mulebeke, Afizi Kibuuka, Ruth Kigozi, Agaba Bosco,
Paul Kyambadde, Jimmy Opigo, Simeon Kalyesubula, Joseph Senzoga, et al. Comparative
efficacy of artemether-lumefantrine and dihydroartemisinin-piperaquine for the treatment of
uncomplicated malaria in ugandan children. The Journal of infectious diseases, 219(7):1112‚Äì
1120, 2019."
REFERENCES,0.9724137931034482,"[92] Ruohan Zhan, Vitor Hadad, David A Hirshberg, and Susan Athey. Off-policy evaluation via
adaptive weighting with data from contextual bandits. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining, pages 2125‚Äì2135, 2021."
REFERENCES,0.9758620689655172,"[93] Kelly Zhang, Lucas Janson, and Susan Murphy. Inference for batched bandits. Advances in
neural information processing systems, 33:9818‚Äì9829, 2020."
REFERENCES,0.9793103448275862,"[94] Xiangyu Zhang and Peter I Frazier. Restless bandits with many arms: Beating the central limit
theorem. arXiv preprint arXiv:2107.11911, 2021."
REFERENCES,0.9827586206896551,"[95] Jinglong Zhao and Zijie Zhou. Pigeonhole design: Balancing sequential experiments from an
online matching perspective, 2022."
REFERENCES,0.9862068965517241,"[96] Zeyu Zheng and Peter W Glynn. Fitting continuous piecewise linear poisson intensities via
maximum likelihood and least squares. In 2017 Winter Simulation Conference (WSC), pages
1740‚Äì1749. IEEE, 2017."
REFERENCES,0.9896551724137931,"[97] Xiang Zhou, Yi Xiong, Ningyuan Chen, and Xuefeng Gao. Regime switching bandits. Advances
in Neural Information Processing Systems, 34:4542‚Äì4554, 2021."
REFERENCES,0.993103448275862,"[98] Zhengyuan Zhou, Susan Athey, and Stefan Wager. Offline multi-action policy learning: Gener-
alization and optimization. Operations Research, 2022."
REFERENCES,0.996551724137931,"[99] Feng Zhu and Zeyu Zheng. When demands evolve larger and noisier: Learning and earning in a
growing environment. In International Conference on Machine Learning, pages 11629‚Äì11638.
PMLR, 2020."
