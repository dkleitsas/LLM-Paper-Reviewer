Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0031446540880503146,"Supervised learning usually requires a large amount of labelled data. However,
attaining ground-truth labels is costly for many tasks. Alternatively, weakly su-
pervised methods learn with cheap weak signals that only approximately label
some data. Many existing weakly supervised learning methods learn a determin-
istic function that estimates labels given the input data and weak signals. In this
paper, we develop label learning ﬂows (LLF), a general framework for weakly
supervised learning problems. Our method is a generative model based on nor-
malizing ﬂows. The main idea of LLF is to optimize the conditional likelihoods
of all possible labelings of the data within a constrained space deﬁned by weak
signals. We develop a training method for LLF that trains the conditional ﬂow
inversely and avoids estimating the labels. Once a model is trained, we can make
predictions with a sampling algorithm. We apply LLF to three weakly supervised
learning problems. Experiment results show that our method outperforms many
state-of-the-art alternatives."
INTRODUCTION,0.006289308176100629,"1
INTRODUCTION"
INTRODUCTION,0.009433962264150943,"Machine learning has achieved great success in many supervised learning tasks. However, in prac-
tice, data labeling is usually human intensive and costly. To address this problem, practitioners are
turning to weakly supervised learning (Zhou, 2018), which trains machine learning models with
only noisy labels that are generated by human speciﬁed rules or pretrained models for related tasks.
Recent research shows that these models trained with weak supervisions can also perform well."
INTRODUCTION,0.012578616352201259,"Many existing weakly supervised learning methods (Ratner et al., 2016; 2017; Bach et al., 2019;
Arachie & Huang, 2021b) learn a deterministic function that estimates the unknown labels y given
input data x and weak signals Q. Since the observed information is incomplete, the predictions
based on it can be varied. However, these methods ignore this uncertainty between x and y. In this
paper, we develop label learning ﬂows (LLF), a general framework for weakly supervised learning
problems. LLF is a ﬂow-based generative model (Dinh et al., 2014; Rezende & Mohamed, 2015;
Dinh et al., 2016; Kingma & Dhariwal, 2018). The main idea behind LLF is that we deﬁne the rela-
tionship between x and y with a probability distribution p(y|x), which is modeled by a conditional
ﬂow. In training, we use the weak signals Q to deﬁne a constrained space for y and then optimize
the likelihood of all possible y that are within this constrained space. Therefore, this model captures
all possible relationships between the input x and output y. Learning LLF can be deﬁned as a con-
strained optimization problem. We develop a learning method for LLF that trains the conditional
ﬂow inversely and avoids estimating the labels. For prediction, we use sample-based method (Lu &
Huang, 2020) to estimate the labels."
INTRODUCTION,0.015723270440251572,"We apply LLF to three weakly supervised learning problems:
weakly supervised classiﬁca-
tion (Arachie & Huang, 2021b; Mazzetto et al., 2021b), weakly supervised regression, and unpaired
point cloud completion (Chen et al., 2019; Wu et al., 2020). These three problems have very different
label types and weak signals. Our method outperforms all other state-of-the-art methods on weakly
supervised classiﬁcation and regression, and it can perform comparably to other recent methods on
unpaired point cloud completion. These experiments show that LLF is versatile and powerful."
BACKGROUND,0.018867924528301886,"2
BACKGROUND"
BACKGROUND,0.0220125786163522,"In this section, we introduce weakly supervised learning and conditional normalizing ﬂows."
BACKGROUND,0.025157232704402517,Under review as a conference paper at ICLR 2022
BACKGROUND,0.02830188679245283,"Weakly Supervised Learning. Given a dataset D = {x1, ..., xN}, and weak signals Q, weakly
supervised learning ﬁnds a model that can predict the unknown label yi for each input data xi.
Some methods, e.g., ALL (Arachie & Huang, 2021b), deﬁne a set of constrained functions based
on Q and x. These functions form a space of possible y. The methods then look for one possible
y within this constrained space. In this work, we follow this idea and use constrained functions to
restrict the predicted y."
BACKGROUND,0.031446540880503145,"Conditional Normalizing Flows. A normalizing ﬂow (Rezende & Mohamed, 2015) is a series of
invertible functions f = f1 ◦f2 ◦· · · ◦fK that transform the probability density of output variables
y to the density of latent variables z. In conditional ﬂows (Trippe & Turner, 2018), a ﬂow layer
function fi is also parameterized by the input variables x, i.e., fi = fx,φi, where φi is the parameters
of fi. With the change-of-variable formula, the log conditional distribution log p(y|x) can be exactly
and tractably computed as"
BACKGROUND,0.03459119496855346,"log p(y|x) = log pZ(fx,φ(y)) + K
X"
BACKGROUND,0.03773584905660377,"i=1
log
det
∂fx,φi ∂ri−1"
BACKGROUND,0.040880503144654086," ,
(1)"
BACKGROUND,0.0440251572327044,"where pZ(z) is a tractable base distribution, e.g. Gaussian distribution. The
∂fx,φi"
BACKGROUND,0.04716981132075472,"∂ri−1 is the Jacobian
matrix of fx,φi. The ri = fx,φi(ri−1), r0 = y, and rK = z."
BACKGROUND,0.050314465408805034,"To use normalizing ﬂows, we need to develop ﬂow layers that are invertible and have tractable
Jacobian determinant. In this paper, we use afﬁne coupling layer (Dinh et al., 2014; 2016) to form
normalizing ﬂows. It splits the input to two parts, and force the ﬁrst part only relate to the second
part, so that the Jacobian is a triangular matrix. For conditional ﬂows, we can deﬁne conditional
afﬁne coupling layer as"
BACKGROUND,0.05345911949685535,"ya, yb
=
split(y),
zb
=
s(x, ya) ⊙yb + b(x, ya),
z
=
concat(ya, zb),"
BACKGROUND,0.05660377358490566,"where s and b are two neural networks. The split() and the concat() functions split and concatenate
the variables."
PROPOSED METHOD,0.059748427672955975,"3
PROPOSED METHOD"
PROPOSED METHOD,0.06289308176100629,"In this section, we introduce the label learning ﬂows (LLF) framework for weakly supervised learn-
ing. Given Q and x, we deﬁne a set of constraints to restrict the predicted label y. These constraints
can be inequalities, formatting like f(x, y, Q) ≤b, or equations, formatting like f(x, y, Q) = b.
For simplicity, we represent this set of constraints as C(x, y, Q). Let Ωbe the constrained space
of all possible y deﬁned by C(x, y, Q). Previous methods only look for one possible y within Ω.
In contrast, LLF optimizes the conditional log-likelihood of all possible y within Ω, resulting in the
following objective
max
φ
Epdata(x)Ey∼U(Ω) [log p(y|x, φ)] ,
(2)"
PROPOSED METHOD,0.0660377358490566,"where U(Ω) is a uniform distribution within Ω, and p(y|x) is a continuous density model of y."
PROPOSED METHOD,0.06918238993710692,"Let ˆy be the true label. We assume that each data point xi only has one unique label ˆyi, so that
pdata(x, ˆy) = pdata(x). Let q(ˆy|x) be a certain model of ˆy. Traditional supervised learning learns a
q(ˆy|x) that maximizes the cross entropy Epdata(x,ˆy) [log q(ˆy|x, φ)]. Following theorem indicates that
maximizing log p(y|x) can be interpreted as maximizing a lower bound of log q(ˆy|x)."
PROPOSED METHOD,0.07232704402515723,"Theorem 1 Let Ω∗⊆Ωis a tight enough space satisfying that ˆy ∈Ω∗and for two different ˆyi
and ˆyj, the Ω∗
i and Ω∗
j are non-overlapped. The volume of Ω∗is bounded such that
1
|Ω∗| ≤M.
The relationship between p(y|x) and q(ˆy|x) can be deﬁned as: q(ˆy|x) =
R"
PROPOSED METHOD,0.07547169811320754,"y∈Ω∗p(y|x)dy. Then
maximizing log p(y|x) can be interpreted as maximizing the lower bound of log q(ˆy|x). That is,"
PROPOSED METHOD,0.07861635220125786,"Epdata(x)Ey∼U(Ω∗) [log p(y|x, φ)] ≤MEpdata(x,ˆy) [log q(ˆy|x, φ)]
(3)"
PROPOSED METHOD,0.08176100628930817,"The complete proof is in appendix. Theorem 1 indicates that, when Ωis well-deﬁned, learning LLF
is analogous to dequantization (Theis et al., 2015; Ho et al., 2019). That is, the method optimizes the"
PROPOSED METHOD,0.08490566037735849,Under review as a conference paper at ICLR 2022
PROPOSED METHOD,0.0880503144654088,"likelihood of dequantized true labels. In practice, the real constrained space, i.e., Ω, may be loose
and does not fulﬁll the conditions in Theorem 1, which will result in inevitable errors that come from
the weakly supervised setting. Moreover, for some regression problems, the ideal Ω∗only contains
a single point: the ground truth label, i.e., Ω∗= {ˆy}."
LEARNING AND PREDICTION,0.09119496855345911,"3.1
LEARNING AND PREDICTION"
LEARNING AND PREDICTION,0.09433962264150944,"Since y is unobserved, directly optimizing the conditional likelihood, i.e., Equation 2, is impossible.
Using the invertibility of normalizing ﬂows, we can rewrite log p(y|x) as"
LEARNING AND PREDICTION,0.09748427672955975,"log p(y|x)
=
log pZ(fx,φ(y)) + K
X"
LEARNING AND PREDICTION,0.10062893081761007,"i=1
log
det
∂fx,φi ∂ri−1 "
LEARNING AND PREDICTION,0.10377358490566038,"=
log pZ(z) − K
X"
LEARNING AND PREDICTION,0.1069182389937107,"i=1
log
det
∂gx,φi ∂ri"
LEARNING AND PREDICTION,0.11006289308176101," ,
(4)"
LEARNING AND PREDICTION,0.11320754716981132,"where gx,φi = f −1
x,φi is the inverse ﬂow."
LEARNING AND PREDICTION,0.11635220125786164,"With the inverse ﬂow, Equation 2 can be interpreted as a constrained optimization problem"
LEARNING AND PREDICTION,0.11949685534591195,"max
φ
Epdata(x)EpZ(z) """
LEARNING AND PREDICTION,0.12264150943396226,"log pZ(z) − K
X"
LEARNING AND PREDICTION,0.12578616352201258,"i=1
log
det
∂gx,φi ∂ri  # ,
(5)"
LEARNING AND PREDICTION,0.1289308176100629,"s.t. C(x, gx,φ(z), Q)."
LEARNING AND PREDICTION,0.1320754716981132,"Note that in Equation 5, the original constraint for z is gx,φ(z) ∈Ω, and this constraint can be
replaced with C(x, gx,φ(z), Q). For efﬁcient training, this constrained optimization problem can
be approximated with the penalty method, resulting in the objective"
LEARNING AND PREDICTION,0.13522012578616352,"max
φ
Epdata(x)EpZ(z) """
LEARNING AND PREDICTION,0.13836477987421383,"log pZ(z) − K
X"
LEARNING AND PREDICTION,0.14150943396226415,"i=1
log
det
∂gx,φi ∂ri"
LEARNING AND PREDICTION,0.14465408805031446," + λCr(x, gx,φ(z), Q) # ,
(6)"
LEARNING AND PREDICTION,0.14779874213836477,"where λ is the penalty coefﬁcient, and Cr() means we reformulate the constraints to be penalty
losses. For example, an inequality constraint will be redeﬁned as a hinge loss."
LEARNING AND PREDICTION,0.1509433962264151,"In practice, the expectation with respect to pZ(z) can be approximated with Monte Carlo esti-
mate with L samples.
Since we only need to obtain stochastic gradients, we follow previous
works (Kingma & Welling, 2013) and set L = 1."
LEARNING AND PREDICTION,0.1540880503144654,"Given a trained model and a data point xi, prediction requires outputting a label yi for xi. We
follow (Lu & Huang, 2020) and use a sample average, i.e., yi = PL
j=1 gxi,φ(zj) as the prediction.
In our experiments, we found that L = 10 is enough for generating high-quality labels."
CASE STUDY,0.15723270440251572,"4
CASE STUDY"
CASE STUDY,0.16037735849056603,"In this section, we illustrate three scenarios of using LLF to address weakly supervised learning
problems."
WEAKLY SUPERVISED CLASSIFICATION,0.16352201257861634,"4.1
WEAKLY SUPERVISED CLASSIFICATION"
WEAKLY SUPERVISED CLASSIFICATION,0.16666666666666666,"We follow previous works (Arachie & Huang, 2021b; Mazzetto et al., 2021b) and consider binary
classiﬁcation. For each example, the label y is a two-dimensional vector within a one-simplex. That
is, the y ∈Y = {y ∈[0, 1]2 : P"
WEAKLY SUPERVISED CLASSIFICATION,0.16981132075471697,"i y[i] = 1}, where y[i] is the ith dimension of y. Each ground
truth label ˆy ∈{0, 1}2 is a two-dimensional one-hot vector. We have M weak labelers, which
will generate M weak signals for each data point xi, i.e., qi = [qi,1, ..., qi,M]. Each weak signal
qi,m ∈Q = {q ∈[0, 1]2 : P"
WEAKLY SUPERVISED CLASSIFICATION,0.17295597484276728,"i q[i] = 1} is a soft labeling of the data. In practice, if a weak labeler
m fails to label a data point xi, the qi,m can be null, i.e., qi,m = ∅(Arachie & Huang, 2021a).
Following Arachie & Huang (2021b), we assume we have access to expected error rate bounds of"
WEAKLY SUPERVISED CLASSIFICATION,0.1761006289308176,Under review as a conference paper at ICLR 2022
WEAKLY SUPERVISED CLASSIFICATION,0.1792452830188679,"these weak signals b = [b1, .., bM]. Therefore, the weak signals imply constraints N
X"
WEAKLY SUPERVISED CLASSIFICATION,0.18238993710691823,"i=1,qi,m̸=∅
(1 −yi) ⊙qi,m + yi ⊙(1 −qi,m) ≤Nmbm
∀m ∈{1, ..., M},
(7)"
WEAKLY SUPERVISED CLASSIFICATION,0.18553459119496854,where Nm is the number of data points that are labeled by weak labeler m.
WEAKLY SUPERVISED CLASSIFICATION,0.18867924528301888,"This problem can be solved with LLF, i.e., Equation 5, by deﬁning C(x, gx,φ(z, Q) to be a com-
bination of weak signal constraints, i.e., Equation 7, and simplex constraints, i.e., y ∈Y. The
objective function of LLF for weakly supervised classiﬁcation is"
WEAKLY SUPERVISED CLASSIFICATION,0.1918238993710692,"max
φ
log pZ(z) − K
X"
WEAKLY SUPERVISED CLASSIFICATION,0.1949685534591195,"i=1
log
det
∂gx,φi ∂ri "
WEAKLY SUPERVISED CLASSIFICATION,0.19811320754716982,"+λ1 [gx,φ(z)]2
+ + λ2 [1 −gx,φ(z)]2
+ + λ3 X"
WEAKLY SUPERVISED CLASSIFICATION,0.20125786163522014,"i
gx,φ(z)[i] −1 !2 +λ4 M
X m=1   N
X"
WEAKLY SUPERVISED CLASSIFICATION,0.20440251572327045,"i=0
qi,m̸=∅"
WEAKLY SUPERVISED CLASSIFICATION,0.20754716981132076,"(1 −gx,φ(z)i) ⊙qi,m + gx,φ(z)i ⊙(1 −qi,m) −Nmbm   2 + , (8)"
WEAKLY SUPERVISED CLASSIFICATION,0.21069182389937108,"where the second row describes the simplex constraints, and the last row is the weak signal con-
straints. The [.]+ is a hinge function that returns its input if positive and zero otherwise. We omit
the expectation terms for simplicity."
WEAKLY SUPERVISED REGRESSION,0.2138364779874214,"4.2
WEAKLY SUPERVISED REGRESSION"
WEAKLY SUPERVISED REGRESSION,0.2169811320754717,"For weakly supervised regression, we predict one-dimensional continuous labels y ∈[0, 1] given
input dataset D = {x1, ..., xN} and weak signals Q. We deﬁne the weak signals as follows. For the
mth feature of input data, we have access to a threshold ϵm, which splits D to two parts, i.e., D1, D2,
such that for each xi ∈D1, the xi,m ≥ϵm, and for each xj ∈D2, the xj,m < ϵm. We also have
access to estimated values of labels for subsets D1 and D2, i.e., bm,1 and bm,2. This design of weak
signals tries to mimic that in practical scenarios, human experts can design rule-based methods for
predicting labels for given data. For example, in disease prediction, a medical doctor can predict
the disease rates for patients based on their ages. For a group of people whose age is greater than a
threshold, an experienced physician would know an estimate of their average disease rate. Assuming
that we have M rule-based weak signals, the constraints can be mathematically deﬁned as follows:"
WEAKLY SUPERVISED REGRESSION,0.22012578616352202,"1
|Dm,1| X"
WEAKLY SUPERVISED REGRESSION,0.22327044025157233,"i∈Dm,1
yi = bm,1,
1
|Dm,2| X"
WEAKLY SUPERVISED REGRESSION,0.22641509433962265,"j∈Dm,2
yj = bm,2,
m ∈1, ..., M.
(9)"
WEAKLY SUPERVISED REGRESSION,0.22955974842767296,"Plugging in Equation 9 to Equation 6, we have"
WEAKLY SUPERVISED REGRESSION,0.23270440251572327,"max
φ
log pZ(z) − K
X"
WEAKLY SUPERVISED REGRESSION,0.2358490566037736,"i=1
log
det
∂gx,φi ∂ri"
WEAKLY SUPERVISED REGRESSION,0.2389937106918239," + λ1 [gx,φ(z)]2
+ + λ2 [1 −gx,φ(z)]2
+ +λ3 M
X m=1 "
WEAKLY SUPERVISED REGRESSION,0.24213836477987422,"
1
|Dm,1| X"
WEAKLY SUPERVISED REGRESSION,0.24528301886792453,"i∈Dm,1
gx,φ(z)i −bm,1   2 + "
WEAKLY SUPERVISED REGRESSION,0.24842767295597484,"
1
|Dm,2| X"
WEAKLY SUPERVISED REGRESSION,0.25157232704402516,"j∈Dm,2
gx,φ(z)j −bm,2   2 (10)"
UNPAIRED POINT CLOUD COMPLETION,0.25471698113207547,"4.3
UNPAIRED POINT CLOUD COMPLETION"
UNPAIRED POINT CLOUD COMPLETION,0.2578616352201258,"Unpaired point cloud completion (Chen et al., 2019; Wu et al., 2020) is a practical problem in 3D
scanning. Given a set of partial point clouds Xp = {x(p)
1 , ..., x(p)
N }, and a set of complete point
clouds Xc = {x(c)
1 , ..., x(c)
N }, we want to restore each x(p)
i
∈Xp. Each point cloud is a set of points,"
UNPAIRED POINT CLOUD COMPLETION,0.2610062893081761,Under review as a conference paper at ICLR 2022
UNPAIRED POINT CLOUD COMPLETION,0.2641509433962264,"i.e., xi = {xi,1, ..., xi,T }, where each xi,t ∈R3 is a 3D point, and the counts T represent the
number of points in a point cloud."
UNPAIRED POINT CLOUD COMPLETION,0.2672955974842767,"Note that the point clouds in Xp and Xc are unpaired, so directly modeling the relationship between
x(c) and x(p) is impossible. This problem can be interpreted as a weakly supervised learning prob-
lem, in which the weak signals Q are derived from the referred complete point clouds Xc. We predict
complete point clouds y ∈Y for partial point clouds in Xp. The conditional distribution p(y|xp) is
an exchangable distribution. We follow previous works (Yang et al., 2019; Klokov et al., 2020) and
use De Finetti’s representation theorem and variational inference to compute its lower bound as the
objective."
UNPAIRED POINT CLOUD COMPLETION,0.27044025157232704,"log p(y|xp)
≥
Eq(u|xp)"
UNPAIRED POINT CLOUD COMPLETION,0.27358490566037735,""" Tc
X"
UNPAIRED POINT CLOUD COMPLETION,0.27672955974842767,"i=1
log p(yi|u, xp) #"
UNPAIRED POINT CLOUD COMPLETION,0.279874213836478,"−KL(q(u|xp)||p(u)),
(11)"
UNPAIRED POINT CLOUD COMPLETION,0.2830188679245283,"where q(u|xp) is a variational distribution of latent variable u. In practice, it can be represented by
an encoder, and uses the reparameterization trick (Kingma & Welling, 2013) to sample u. The p(u)
is a standard Gaussian prior. The p(yi|u, xp) is deﬁned by a conditional ﬂow. We follow Chen et al.
(2019); Wu et al. (2020) and use the adversarial loss and Hausdorff distance loss as constraints. The
ﬁnal objective function is"
UNPAIRED POINT CLOUD COMPLETION,0.2861635220125786,"max
φ
Eq(u|xp)"
UNPAIRED POINT CLOUD COMPLETION,0.2893081761006289,""" Tc
X"
UNPAIRED POINT CLOUD COMPLETION,0.29245283018867924,"t=1
log pZ(zt) − K
X"
UNPAIRED POINT CLOUD COMPLETION,0.29559748427672955,"i=1
log
det
∂gu,xp,φi ∂rt,i  #"
UNPAIRED POINT CLOUD COMPLETION,0.29874213836477986,−KL(q(u|xp)||p(u))
UNPAIRED POINT CLOUD COMPLETION,0.3018867924528302,"+Eq(u|xp)

λ1(D(gu,xp,φ(z)) −1)2 + λ2dH(gu,xp,φ(z), xp)

,
(12)"
UNPAIRED POINT CLOUD COMPLETION,0.3050314465408805,"where D() represents the discriminator of a least square GAN (Mao et al., 2017), and dH() repre-
sents the Haudorsff distance, which measures the distance between a generated complete point cloud
its corresponding input partial point cloud. For clarity, we use zt and rt to represent variables of the
tth point in a point cloud, and gu,xp,φ(z) to represent a generated point cloud. Detailed derivations
of Equation 12 are in appendix."
UNPAIRED POINT CLOUD COMPLETION,0.3081761006289308,"Training with Equation 12 is different from the previous settings, because we also need to train the
discriminator of the GAN. The objective for D() is"
UNPAIRED POINT CLOUD COMPLETION,0.3113207547169811,"min
D Epdata(xc)

(D(xc) −1)2
+ Epdata(xp),pZ(z),q(u|xp)

D(gxp,u,φ(z))2
.
(13)"
UNPAIRED POINT CLOUD COMPLETION,0.31446540880503143,"The training process is similar to traditional GAN training. The inverse ﬂow gu,xp,φ can be roughly
seen as the generator. In training, we train the ﬂow to optimize Equation 12 and the discriminator to
optimize Equation 13, alternatively."
RELATED WORK,0.31761006289308175,"5
RELATED WORK"
RELATED WORK,0.32075471698113206,"In this section, we introduce the research that most related to our work."
RELATED WORK,0.3238993710691824,"Weakly Supervised Learning. For weakly supervised classiﬁcation, we use the same strategy as
adversarial label learning (ALL) (Arachie & Huang, 2021b) to deﬁne constraint functions based on
weak signals. ALL then uses a min-max optimization to learn the model parameters and estimate y
alternatively. In contrast to ALL, LLF is a generative model, so it can learn the model parameters and
output y simultaneously, and it does not need a min-max optimization. Moreover, LLF optimizes the
likelihoods of all possible ys within Ω, while ALL only estimates one possible y. Other methods
also constrain the label space of the predicted labels using weak supervision (Arachie & Huang,
2021a;b; Mazzetto et al., 2021a;b). These methods are deterministic and developed for classiﬁcation
tasks. However, LLF can be used for regression and uses sampling during inference."
RELATED WORK,0.3270440251572327,"Non-constraint based weak supervision methods typically assume a joint distribution for the weak
signals and the true labels of the data. These methods use a latent variable model to estimate the
labels of the data while accounting for the dependency among the weak signals (Ratner et al., 2016;
2019; Fu et al., 2020). Like these methods, we assume a family of distributions for the label space
of the data. This space is deﬁned by the constraints of the weak supervision and the data. Unlike
these methods, we use a ﬂow network rather than a graphical model to solve for the label of the"
RELATED WORK,0.330188679245283,Under review as a conference paper at ICLR 2022
RELATED WORK,0.3333333333333333,"data. Additionally, we do not solve for the dependence amongst the weak signals thereby avoiding
the need for making extra assumptions."
RELATED WORK,0.33647798742138363,"Normalizing Flows. Normalizing ﬂows (Dinh et al., 2014; Rezende & Mohamed, 2015; Dinh et al.,
2016; Kingma & Dhariwal, 2018) have gained recent attention because of their advantages of ex-
act latent variable inference and log-likelihood evaluation. Speciﬁcally, conditional normalizing
ﬂows have been widely applied to many supervised learning problems (Trippe & Turner, 2018;
Lu & Huang, 2020; Lugmayr et al., 2020; Pumarola et al., 2020) and semi-supervised classiﬁca-
tion (Atanov et al., 2019; Izmailov et al., 2020). However, normalizing ﬂows have not previously
been applied to weakly supervised learning problems."
RELATED WORK,0.33962264150943394,"Our inverse training method for LLF is similar to that of injective ﬂows (Kumar et al., 2020). Injec-
tive ﬂows are used to model unconditional datasets. They use an encoder network to map the input
data x to latent code z, and then they use an inverse ﬂow to map z back to x, resulting in an autoen-
coder architecture. Different from injective ﬂow, LLF directly samples z from a prior distribution
and uses a conditional ﬂow to map z back to y conditioned on x. We use constraint functions to
restrict y to be valid, so that does not need an encoder network."
RELATED WORK,0.34276729559748426,"Point Cloud Modeling. Recently, Yang et al. (2019) and Tran et al. (2019) combine normaliz-
ing ﬂows with variational autoencoders (Kingma & Welling, 2013) and developed continuous and
discrete normalizing ﬂows for point clouds. The basic idea of point normalizing ﬂows is to use a
conditional ﬂow to model each point in a point cloud. The conditional ﬂow is conditioned on a la-
tent variable generated by an encoder. To guarantee exchangeability, the encoder uses a PointNet (Qi
et al., 2017) to extract features from input point clouds."
RELATED WORK,0.34591194968553457,"The unpaired point cloud completion problem is ﬁrst deﬁned by Chen et al. (2019). They develop
pcl2pcl—a GAN (Goodfellow et al., 2014) based model—to solve it. Their method is two-staged.
In the ﬁrst stage, it trains autoencoders to map partial and complete point clouds to their latent
space. In the second stage, a GAN is used to transform the latent features of partial point clouds to
latent features of complete point clouds. In their follow-up paper (Wu et al., 2020), they develop
a variant of pcl2pcl, called multi-modal pcl2pcl (mm-pcl2pcl), which incorporates random noise to
the generative process, so that can capture the uncertainty in reasoning."
RELATED WORK,0.3490566037735849,"In contrast to pcl2pcl, LLF can be trained end-to-end. When applying LLF to this problem, LLF has
a similar framework to VAE-GAN (Larsen et al., 2016). The main differences are that LLF models
a conditional distribution of points, and its encoder is a point normalizing ﬂow."
EMPIRICAL STUDY,0.3522012578616352,"6
EMPIRICAL STUDY"
EMPIRICAL STUDY,0.3553459119496855,"In this section, we evaluate LLF on the three weakly supervised learning problems."
EMPIRICAL STUDY,0.3584905660377358,"Model architecture. For weakly supervised classiﬁcation and unpaired point cloud completion, the
labels y are multi-dimensional variables. We follow Klokov et al. (2020) and use ﬂows with only
conditional coupling layers. We use the same method as Klokov et al. (2020) to deﬁne the condi-
tional afﬁne layer. Each ﬂow model contains 8 ﬂow steps. For unpaired point cloud completion,
each ﬂow step has 3 coupling layers. For weakly supervised classiﬁcation, each ﬂow step has 2
coupling layers. For weakly supervised regression, since y is a scalar, we use simple conditional
afﬁne transformation as ﬂow layer, which is deﬁned as:y = s(x) ∗z + b(x), where s and b are two
neural networks that take x as input and output parameters for y. The ﬂow for this problem contains
8 conditional afﬁne transformations."
EMPIRICAL STUDY,0.36163522012578614,"For the unpaired point cloud completion task, we need to also use an encoder network, i.e., q(u|xp)
and a discriminator D(). We follow Klokov et al. (2020); Wu et al. (2020) and use PointNet (Qi
et al., 2017) in these two networks to extract features for point clouds."
EMPIRICAL STUDY,0.36477987421383645,"Experiment setup. In weakly supervised classiﬁcation and regression experiments, we assume that
the ground truth labels are inaccessible, so tuning hyper-parameters for models are impossible. We
use default settings for all hyper-parameters of LLF, e.g., λs and learning rates. We ﬁx λ = 10
and use Adam (Kingma & Ba, 2014) with default settings, i.e., η = 0.001, β1 = 0.9 and β2 =
0.999. We use an exponential learning rate scheduler with a decreasing rate of 0.996 to guarantee
convergence. We track the decrease of loss and when the decrease is small enough, the training
stops. Following previous works (Arachie & Huang, 2021b;a), we use full gradient optimization"
EMPIRICAL STUDY,0.36792452830188677,Under review as a conference paper at ICLR 2022
EMPIRICAL STUDY,0.3710691823899371,"to train the models. For fair comparison, we run each experiment 5 times with different random
seeds {0, 10, 100, 123, 1234}. We split each dataset to training, simulation, and test sets. We follow
Arachie & Huang (2021b;a) and create weak signals with randomly chosen features, and estimate
error bounds and thresholds on simulation set."
EMPIRICAL STUDY,0.3742138364779874,"For experiments with unpaired point cloud completion, the datasets contain validation sets, so we
tune the hyper-parameters using these. We use Adam with an initial learning rate η = 0.0001 and
default βs. The best coefﬁcients for the constraints in Equation 12 are λ1 = 10, λ2 = 100. We use
stochastic optimization to train the models, and the batch size is 32. Each model is trained for at
most 2000 epochs."
WEAKLY SUPERVISED CLASSIFICATION,0.37735849056603776,"6.1
WEAKLY SUPERVISED CLASSIFICATION"
WEAKLY SUPERVISED CLASSIFICATION,0.3805031446540881,"Datasets. We follow Arachie & Huang (2021a;b) and conduct experiments on 12 datasets. Specif-
ically, the Breast Cancer, OBS Network, Cardiotocography, Clave Direction, Credit Card, Statlog
Satellite, Phishing Websites, Wine Quality are tabular datasets from UCI repository (Dua & Graff,
2017). The Fashion-MNIST (Xiao et al., 2017) is an image set with 10 classes of clothing types. We
choose 3 pairs of classes, i.e., dresses/sneakers (DvK), sandals/ankle boots (SvA), and coats/bags
(CvB), to conduct binary classiﬁcation. We follow Arachie & Huang (2021b) and create 3 synthetic
weak signals for each dataset. The IMDB (Maas et al., 2011), SST (Socher et al., 2013) and YELP
are real text datasets. We follow Arachie & Huang (2021a) and use keyword-based weak super-
vision. Each dataset has more than 10 weak signals. For the experiments on tabular datasets, we
set the maximum epochs to 2000. Since the experiments on real text datasets are larger, we set the
maximum epochs to 500."
WEAKLY SUPERVISED CLASSIFICATION,0.3836477987421384,"Baselines. We compare our method with state-of-the-art methods for weakly supervised classiﬁ-
cation. For the experiments on tabular datasets and image sets, we use ALL (Arachie & Huang,
2021b), generalized expectation (GE) (Druck et al., 2008; Mann & McCallum, 2010) and averaging
of weak signals (AVG). For experiments on text datasets, we use CLL (Arachie & Huang, 2021a),
Snorkel MeTaL (Ratner et al., 2019), Data Programming (DP) (Ratner et al., 2016), regularized min-
imax conditional entropy for crowdsourcing (MMCE) (Zhou et al., 2015), and majority-vote. We
also show supervised learning (SL) results for reference."
WEAKLY SUPERVISED CLASSIFICATION,0.3867924528301887,"Results. We report the mean and standard deviation of accuracy on test sets in Table 1 and Table 2.
LLF outperforms all baseline methods on almost all datasets. On some datasets, LLF can perform
as well as supervised learning methods. These results prove that LLF is powerful and effective.
In our experiments, we also found that the performance of LLF will also be impacted by different
initialization of weights. This is why LLF has relatively larger variance on some datasets."
WEAKLY SUPERVISED CLASSIFICATION,0.389937106918239,"Table 1: Test set accuracy on tabular and image datasets. We report the mean accuracy of 5 experi-
ments, and the subscripts are standard deviation. LLF outperforms other baselines on 10 datasets."
WEAKLY SUPERVISED CLASSIFICATION,0.39308176100628933,"LLF
ALL
GE
AVG
SL"
WEAKLY SUPERVISED CLASSIFICATION,0.39622641509433965,"Fashion MNIST (DvK)
1.0000.000
0.9950.000
0.9790.000
0.8350.000
1.0000.000
Fashion MNIST (SvA)
0.9440.001
0.9080.000
0.5010.000
0.7910.000
0.9720.000
Fashion MNIST (CvB)
0.9160.038
0.8050.000
0.5010.000
0.7400.000
0.9880.000
Breast Cancer
0.9680.008
0.9370.019
0.9330.016
0.9110.023
0.9730.007
OBS Network
0.6840.006
0.6910.011
0.6760.010
0.7090.024
0.7040.032
Cardiotocography
0.9310.010
0.7950.011
0.6630.061
0.9020.047
0.9410.008
Clave Direction
0.8580.017
0.7500.013
0.7560.028
0.7070.003
0.9630.001
Credit Card
0.6800.022
0.6780.021
0.4920.088
0.6020.010
0.7170.031
Statlog Satellite
0.9970.002
0.9590.008
0.9870.012
0.9150.011
0.9990.001
Phishing Websites
0.9060.003
0.8960.005
0.8700.009
0.8480.002
0.9290.001
Wine Quality
0.6470.017
0.6230.000
0.4450.014
0.5550.000
0.6850.000"
WEAKLY SUPERVISED CLASSIFICATION,0.39937106918238996,"Ablation Study. We can directly train the model using only the constraints as the objective function.
In our experiments, we found that training LLF without likelihood (LLF-w/o-nll) will still work.
However, the model performs worse than training with likelihood. We believe that this is because
the likelihood helps accumulate more probability mass to the constrained space Ω, so the model will
more likely generate y samples within Ω, and the predictions are more accurate."
WEAKLY SUPERVISED CLASSIFICATION,0.4025157232704403,Under review as a conference paper at ICLR 2022
WEAKLY SUPERVISED CLASSIFICATION,0.4056603773584906,Table 2: Test set accuracy on real text datasets. LLF outperforms all other baselines.
WEAKLY SUPERVISED CLASSIFICATION,0.4088050314465409,"LLF
CLL
MMCE
DP
MV
MeTaL
SL"
WEAKLY SUPERVISED CLASSIFICATION,0.4119496855345912,"SST
0.7660.002
0.7290.001
0.727
0.7200.001
0.7200.001
0.7280.001
0.7920.001
IMDB
0.8040.000
0.7400.005
0.551
0.6230.007
0.7240.004
0.7420.004
0.8200.003
YELP
0.8610.000
0.8400.001
0.680
0.7600.005
0.7980.007
0.7800.002
0.8790.001 0.6 0.7 0.8 0.9"
WEAKLY SUPERVISED CLASSIFICATION,0.41509433962264153,"0
500
1000
1500
2000
Epoch Acc"
WEAKLY SUPERVISED CLASSIFICATION,0.41823899371069184,Accuracy of MNIST −3 −2 −1 0 1
WEAKLY SUPERVISED CLASSIFICATION,0.42138364779874216,"0
500
1000
1500
2000
Epoch NLL"
WEAKLY SUPERVISED CLASSIFICATION,0.42452830188679247,NLL of MNIST 0.000 0.025 0.050 0.075 0.100
WEAKLY SUPERVISED CLASSIFICATION,0.4276729559748428,"0
500
1000
1500
2000
Epoch"
WEAKLY SUPERVISED CLASSIFICATION,0.4308176100628931,Violation
WEAKLY SUPERVISED CLASSIFICATION,0.4339622641509434,Violation of MNIST 0.5 0.6 0.7 0.8
WEAKLY SUPERVISED CLASSIFICATION,0.4371069182389937,"0
500
1000
1500
2000
Epoch Acc"
WEAKLY SUPERVISED CLASSIFICATION,0.44025157232704404,Accuracy of Clave −3 −2 −1 0 1
WEAKLY SUPERVISED CLASSIFICATION,0.44339622641509435,"0
500
1000
1500
2000
Epoch NLL"
WEAKLY SUPERVISED CLASSIFICATION,0.44654088050314467,NLL of Clave 0.000 0.025 0.050 0.075
WEAKLY SUPERVISED CLASSIFICATION,0.449685534591195,"0
500
1000
1500
2000
Epoch"
WEAKLY SUPERVISED CLASSIFICATION,0.4528301886792453,Violation
WEAKLY SUPERVISED CLASSIFICATION,0.4559748427672956,Violation of Clave
WEAKLY SUPERVISED CLASSIFICATION,0.4591194968553459,"method
LLF (test)
LLF (train)
LLF−w/o−nll (test)
LLF−w/o−nll (train)"
WEAKLY SUPERVISED CLASSIFICATION,0.46226415094339623,"Figure 1: Evolution of accuracy, likelihood and violation of weak signal constraints. Training with
likelihood makes LLF accumulate more probability mass to the constrained space, so that the gen-
erated y are more likely to be within Ω, and the predictions are more accurate."
WEAKLY SUPERVISED REGRESSION,0.46540880503144655,"6.2
WEAKLY SUPERVISED REGRESSION"
WEAKLY SUPERVISED REGRESSION,0.46855345911949686,"Datasets. We use 3 tabular datasets from the UCI repository Dua & Graff (2017): Air Quality,
Temperature Forecast, and Bike Sharing dataset. For each dataset, we randomly choose 5 features
to develop the rule based weak signals. We split each dataset to training, simulation, and test sets.
The simulation set is then used to compute the threshold ϵs, and the estimated label values bs. Since
we do not have human experts to estimate these values, we use the mean value of a feature as its
threshold, i.e., ϵm =
1
|Dvalid|
P"
WEAKLY SUPERVISED REGRESSION,0.4716981132075472,"i∈Dvalid xi[m]. We then compute the estimated label values bm,1 and
bm,2 based on labels in the valid set. Note that the labels in the simulation set are only used for
generating weak signals, simulating human expertise. In training, we still assume that we do not
have access to labels. The original label is within an interval [ly, uy]. We normalize the original
label to within [0, 1] by computing y = (y −ly)/(uy −ly). In prediction, we recover the predicted
label to original value by computing y = y(uy −ly) + ly."
WEAKLY SUPERVISED REGRESSION,0.4748427672955975,"Baselines. To the best of our knowledge, there are no methods speciﬁcally designed for weakly
supervised regression of this form. We use average of weak signals (AVG) and LFF-w/o-nll as
baselines. We also report the supervised learning results for reference."
WEAKLY SUPERVISED REGRESSION,0.4779874213836478,"Results. We use root square mean error (RSME) as metric. The results of test set are in Table 3. In
general, LLF can predict reasonable labels. Its results are much better than AVG or any of the weak
signals alone. Similar to the classiﬁcation results, training LLF without using likelihood will reduce
its performance."
WEAKLY SUPERVISED REGRESSION,0.4811320754716981,"Table 3: Test set RMSE of different methods. The numbers in brackets indicate the label’s range.
LLF outperforms other baselines on all datasets."
WEAKLY SUPERVISED REGRESSION,0.48427672955974843,"LLF
LLF-w/o-nll
AVG
SL"
WEAKLY SUPERVISED REGRESSION,0.48742138364779874,"Air Quality (0.1847 ∼2.231)
0.2110.009
0.2660.004
0.3730.005
0.1230.002
Temperature Forecast (17.4 ∼38.9)
2.5520.050
2.6560.055
2.8270.027
1.4650.031
Bike Sharing (1 ∼999)
157.3480.541
162.6971.585
171.3381.300
141.9201.280"
WEAKLY SUPERVISED REGRESSION,0.49056603773584906,Under review as a conference paper at ICLR 2022
UNPAIRED POINT CLOUD COMPLETION,0.4937106918238994,"6.3
UNPAIRED POINT CLOUD COMPLETION"
UNPAIRED POINT CLOUD COMPLETION,0.4968553459119497,"Datasets. We use the Partnet (Mo et al., 2019) dataset in our experiments. We follow Wu et al.
(2020) and conduct experiments on the 3 largest classes of PartNet: Table, Chair, and Lamp. We
treat each class as a dataset, which is split to training, validation, and test sets based on ofﬁcial splits
of PartNet. For each point cloud, we remove points of randomly selected parts to create a partial
point cloud. We follow Chen et al. (2019); Wu et al. (2020) and let the partial point clouds have
1024 points, and the complete point clouds have 2048 points. We let the latent variable u of the
VAE to be a 128-dimensional vector."
UNPAIRED POINT CLOUD COMPLETION,0.5,"Metrics. We follow Wu et al. (2020) and use minimal matching distance (MMD) (Achlioptas et al.,
2018), total mutual difference (TMD), and unidirectional Hausdorff distance (UHD) as metrics.
MMD measures the quality of generated. A lower MMD is better. TMD measures the diversity of
samples. A higher TMD is better. UHD measures the ﬁdelity of samples. A lower UHD is better."
UNPAIRED POINT CLOUD COMPLETION,0.5031446540880503,"Baselines. We compare our method with pcl2pcl (Chen et al., 2019), mm-pcl2pcl (Wu et al., 2020),
and LLF-w/o-nll. We use two variants of mm-pcl2pcl. Another variant is called mm-pcl2pcl-im,
which is different from the original model in that it jointly trains the encoder of modeling multi-
modality and the GAN."
UNPAIRED POINT CLOUD COMPLETION,0.5062893081761006,Table 4: Evaluation results on three classes of PartNet. LLF performs comparable to baselines.
UNPAIRED POINT CLOUD COMPLETION,0.5094339622641509,"PartNet
Chair
Lamp
Table"
UNPAIRED POINT CLOUD COMPLETION,0.5125786163522013,"MMD↓
TMD↑
UHD↓
MMD↓
TMD↑
UHD↓
MMD↓
TMD↑
UHD↓"
UNPAIRED POINT CLOUD COMPLETION,0.5157232704402516,"LLF
1.72
0.63
5.74
2.11
0.57
4.71
1.57
0.55
5.42
LLF-w/o-nll
1.79
0.47
5.49
2.21
0.41
4.61
1.57
0.43
5.13
pcl2pcl
1.90
0.00
4.88
2.50
0.00
4.64
1.90
0.00
4.78
mm-pcl2pcl
1.52
2.75
6.89
1.97
3.31
5.72
1.46
3.30
5.56
mm-pcl2pcl-im
1.90
1.01
6.65
2.55
0.56
5.40
1.54
0.51
5.38"
UNPAIRED POINT CLOUD COMPLETION,0.5188679245283019,"Results. We list the test set results in Table 4. In general, pcl2pcl has the best ﬁdelity, i.e., lowest
UHD. This is because pcl2pcl is a discriminative model, and it will only predict one certain sample
for each input. This is also why pcl2pcl has the worse diversity as measured by TMD. Mm-pcl2pcl
has the best diversity. However, we found in our experiments that some samples generated by mm-
pcl2pcl are invalid, i.e., they are totally different from the input partial point clouds. Therefore,
mm-pcl2pcl has the worse ﬁdelity. LLF scores between pcl2pcl and mm-pcl2pcl. It has better UHD
than mm-pcl2pcl, and better TMD and MMD than pcl2pcl. The LLF-w/0-nll has a slightly better
UHD than LLF. We believe this is because, without using the likelihood, LLF-w/o-nll is trained
directly by optimizing the Hausdorff distance. However, the sample diversity and quality, i.e., TMD
and MMD, are worse than LLF. As argued by Yang et al. (2019), the current metrics for evaluating
point cloud samples all have ﬂaws, so these scores cannot be treated as hard metrics for evaluating
model performance. We therefore visualize some samples in appendix, which show that LLF can
generate samples that comparable to mm-pcl2pcl."
CONCLUSION,0.5220125786163522,"7
CONCLUSION"
CONCLUSION,0.5251572327044025,"In this paper, we propose label learning ﬂows, which represent a general framework for weakly
supervised learning. LLF uses a conditional ﬂow to deﬁne the conditional distribution p(y|x), so
that can model the uncertainty between input x and all possible y. Learning LLF is a constrained
optimization problem that optimizes the likelihood of all possible y within the constrained space
deﬁned by weak signals. We develop a speciﬁc training method to train LLF inversely, avoiding
the need of estimating y. We apply LLF to three weakly supervised learning problems, and the
results show that our method outperforms many state-of-the-art baselines on the weakly supervised
classiﬁcation and regression problems, and performs comparably to other new methods for unpaired
point cloud completion. These results indicate that LLF is a powerful and effective tool for weakly
supervised learning problems."
CONCLUSION,0.5283018867924528,Under review as a conference paper at ICLR 2022
REFERENCES,0.5314465408805031,REFERENCES
REFERENCES,0.5345911949685535,"Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representa-
tions and generative models for 3d point clouds. In International conference on machine learning,
pp. 40–49. PMLR, 2018."
REFERENCES,0.5377358490566038,"Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. In Inter-
national Conference in Uncertainty in Artiﬁcial Intelligence, 2021a."
REFERENCES,0.5408805031446541,"Chidubem Arachie and Bert Huang. A general framework for adversarial label learning. Journal of
Machine Learning Research, 22(118):1–33, 2021b."
REFERENCES,0.5440251572327044,"Andrei Atanov, Alexandra Volokhova, Arsenii Ashukha, Ivan Sosnovik, and Dmitry Vetrov. Semi-
conditional normalizing ﬂows for semi-supervised learning. arXiv preprint arXiv:1905.00505,
2019."
REFERENCES,0.5471698113207547,"Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik
Sen, Alex Ratner, Braden Hancock, Houman Alborzi, et al. Snorkel drybell: A case study in de-
ploying weak supervision at industrial scale. In Proceedings of the 2019 International Conference
on Management of Data, pp. 362–375, 2019."
REFERENCES,0.550314465408805,"Xuelin Chen, Baoquan Chen, and Niloy J Mitra. Unpaired point cloud completion on real scans
using adversarial training. arXiv preprint arXiv:1904.00069, 2019."
REFERENCES,0.5534591194968553,"Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014."
REFERENCES,0.5566037735849056,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. arXiv
preprint arXiv:1605.08803, 2016."
REFERENCES,0.559748427672956,"Gregory Druck, Gideon Mann, and Andrew McCallum. Learning from labeled features using gen-
eralized expectation criteria. In Proceedings of the 31st annual international ACM SIGIR confer-
ence on Research and development in information retrieval, pp. 595–602, 2008."
REFERENCES,0.5628930817610063,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.5660377358490566,"Daniel Fu, Mayee Chen, Frederic Sala, Sarah Hooper, Kayvon Fatahalian, and Christopher R´e. Fast
and three-rious: Speeding up weak supervision with triplet methods. In International Conference
on Machine Learning, pp. 3280–3291. PMLR, 2020."
REFERENCES,0.5691823899371069,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672–2680, 2014."
REFERENCES,0.5723270440251572,"Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving ﬂow-
based generative models with variational dequantization and architecture design. arXiv preprint
arXiv:1902.00275, 2019."
REFERENCES,0.5754716981132075,"Pavel Izmailov, Polina Kirichenko, Marc Finzi, and Andrew Gordon Wilson. Semi-supervised learn-
ing with normalizing ﬂows. In International Conference on Machine Learning, pp. 4615–4630.
PMLR, 2020."
REFERENCES,0.5786163522012578,"Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.5817610062893082,"Diederik P Kingma and Max Welling.
Auto-encoding variational Bayes.
arXiv preprint
arXiv:1312.6114, 2013."
REFERENCES,0.5849056603773585,"Durk P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems, pp. 10215–10224, 2018."
REFERENCES,0.5880503144654088,"Roman Klokov, Edmond Boyer, and Jakob Verbeek. Discrete point ﬂow networks for efﬁcient point
cloud generation. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
August 23–28, 2020, Proceedings, Part XXIII 16, pp. 694–710. Springer, 2020."
REFERENCES,0.5911949685534591,Under review as a conference paper at ICLR 2022
REFERENCES,0.5943396226415094,"Abhishek Kumar, Ben Poole, and Kevin Murphy. Regularized autoencoders via relaxed injective
probability ﬂow. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 4292–
4301. PMLR, 2020."
REFERENCES,0.5974842767295597,"Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoen-
coding beyond pixels using a learned similarity metric. In International conference on machine
learning, pp. 1558–1566. PMLR, 2016."
REFERENCES,0.60062893081761,"You Lu and Bert Huang. Structured output learning with conditional generative ﬂows. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 5005–5012, 2020."
REFERENCES,0.6037735849056604,"Andreas Lugmayr, Martin Danelljan, Luc Van Gool, and Radu Timofte. Srﬂow: Learning the super-
resolution space with normalizing ﬂow. In European Conference on Computer Vision, pp. 715–
732. Springer, 2020."
REFERENCES,0.6069182389937107,"Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the
association for computational linguistics: Human language technologies, pp. 142–150, 2011."
REFERENCES,0.610062893081761,"Gideon S Mann and Andrew McCallum. Generalized expectation criteria for semi-supervised learn-
ing with weakly labeled data. Journal of machine learning research, 11(2), 2010."
REFERENCES,0.6132075471698113,"Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.
Least squares generative adversarial networks. In Proceedings of the IEEE international confer-
ence on computer vision, pp. 2794–2802, 2017."
REFERENCES,0.6163522012578616,"A. Mazzetto, C. Cousins, D. Sam, S. H. Bach, and E. Upfal. Adversarial multiclass learning under
weak supervision with performance guarantees. In International Conference on Machine Learn-
ing (ICML), 2021a."
REFERENCES,0.6194968553459119,"Alessio Mazzetto, Dylan Sam, Andrew Park, Eli Upfal, and Stephen Bach. Semi-supervised ag-
gregation of dependent weak supervision sources with performance guarantees. In International
Conference on Artiﬁcial Intelligence and Statistics, pp. 3196–3204. PMLR, 2021b."
REFERENCES,0.6226415094339622,"Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, and Hao
Su. Partnet: A large-scale benchmark for ﬁne-grained and hierarchical part-level 3d object under-
standing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pp. 909–918, 2019."
REFERENCES,0.6257861635220126,"Albert Pumarola, Stefan Popov, Francesc Moreno-Noguer, and Vittorio Ferrari. C-ﬂow: Condi-
tional generative ﬂow models for images and 3d point clouds. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 7949–7958, 2020."
REFERENCES,0.6289308176100629,"Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets
for 3d classiﬁcation and segmentation. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 652–660, 2017."
REFERENCES,0.6320754716981132,"Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher R´e.
Snorkel: Rapid training data creation with weak supervision. In Proceedings of the VLDB En-
dowment. International Conference on Very Large Data Bases, volume 11, pp. 269. NIH Public
Access, 2017."
REFERENCES,0.6352201257861635,"Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic Sala, Shreyash Pandey, and Christo-
pher R´e. Training complex models with multi-task weak supervision. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, volume 33, pp. 4763–4771, 2019."
REFERENCES,0.6383647798742138,"Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R´e. Data
programming: Creating large training sets, quickly. Advances in neural information processing
systems, 29:3567–3575, 2016."
REFERENCES,0.6415094339622641,"Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. arXiv
preprint arXiv:1505.05770, 2015."
REFERENCES,0.6446540880503144,Under review as a conference paper at ICLR 2022
REFERENCES,0.6477987421383647,"Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-
cessing, pp. 1631–1642, 2013."
REFERENCES,0.6509433962264151,"Lucas Theis, A¨aron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844, 2015."
REFERENCES,0.6540880503144654,"Dustin Tran, Keyon Vafa, Kumar Krishna Agrawal, Laurent Dinh, and Ben Poole. Discrete ﬂows:
Invertible generative models of discrete data. arXiv preprint arXiv:1905.10347, 2019."
REFERENCES,0.6572327044025157,"Brian L Trippe and Richard E Turner. Conditional density estimation with Bayesian normalising
ﬂows. arXiv preprint arXiv:1802.04908, 2018."
REFERENCES,0.660377358490566,"Rundi Wu, Xuelin Chen, Yixin Zhuang, and Baoquan Chen. Multimodal shape completion via
conditional generative adversarial networks. arXiv preprint arXiv:2003.07717, 2020."
REFERENCES,0.6635220125786163,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017."
REFERENCES,0.6666666666666666,"Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan.
Pointﬂow: 3d point cloud generation with continuous normalizing ﬂows. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp. 4541–4550, 2019."
REFERENCES,0.6698113207547169,"Dengyong Zhou, Qiang Liu, John C Platt, Christopher Meek, and Nihar B Shah.
Regularized
minimax conditional entropy for crowdsourcing. arXiv preprint arXiv:1503.07240, 2015."
REFERENCES,0.6729559748427673,"Zhi-Hua Zhou. A brief introduction to weakly supervised learning. National science review, 5(1):
44–53, 2018."
REFERENCES,0.6761006289308176,"A
PROOF OF THEOREM 1"
REFERENCES,0.6792452830188679,"The proof of Theorem 1 is similar to the proof of dequantization (Theis et al., 2015; Ho et al., 2019).
The complete proof is as follows."
REFERENCES,0.6823899371069182,Proof.
REFERENCES,0.6855345911949685,"Epdata(x)Ey∼U(Ω∗) [log p(y|x, φ)]
=
X"
REFERENCES,0.6886792452830188,"x
pdata(x)
Z"
REFERENCES,0.6918238993710691,"y∈Ω∗
1
|Ω∗| log p(y|x)dy ≤
M
X"
REFERENCES,0.6949685534591195,"x
pdata(x) log
Z"
REFERENCES,0.6981132075471698,"y∈Ω∗p(y|x)dy =
M
X"
REFERENCES,0.7012578616352201,"x,ˆy
pdata(x, ˆy) log q(ˆy|x)"
REFERENCES,0.7044025157232704,"=
MEpdata(x,ˆy) [log q(ˆy|x)]
(14)"
REFERENCES,0.7075471698113207,"In the second row, we use the properties that
1
|Ω∗| ≤M, and the Jensen’s inequality. In the third row,
we use the assumption that pdata(x) = pdata(x, ˆy), and the relationship between p(y|x) and q(ˆy|x). □"
REFERENCES,0.710691823899371,"B
LABEL LEARNING FLOW FOR UNPAIRED POINT CLOUD COMPLETION"
REFERENCES,0.7138364779874213,"In this section, we provide complete derivations of LLF for unpaired point cloud completion. The
conditional likelihood log p(y|xp) is an exchangeable distribution. We use De Finetti’s representa-
tion theorem and variational inference to derive a tractable lower bound for it."
REFERENCES,0.7169811320754716,Under review as a conference paper at ICLR 2022
REFERENCES,0.720125786163522,"log p(y|xp)
=
Z
p(y, u|xp)du"
REFERENCES,0.7232704402515723,"=
Z
p(y|u, xp)p(u)du"
REFERENCES,0.7264150943396226,"≥
Eq(u|xp) [log p(y|u, xp)] −KL(q(u|xp)||p(u))"
REFERENCES,0.7295597484276729,"≥
Eq(u|xp)"
REFERENCES,0.7327044025157232,""" Tc
X"
REFERENCES,0.7358490566037735,"i=1
log p(yi|u, xp) #"
REFERENCES,0.7389937106918238,"−KL(q(u|xp)||p(u)),
(15)"
REFERENCES,0.7421383647798742,"where in the third row, we use Jensen’s inequality to compute the lower bound, and in the last row,
we use De Finetti’s theorem to factorize p(y|u, xp) to the distributions of points."
REFERENCES,0.7452830188679245,"The least square GAN discriminator and the Hausdorff distance for generated complete point clouds
can be treated as two equality constraints"
REFERENCES,0.7484276729559748,"D(y)
=
1
dH(y, xp)
=
0."
REFERENCES,0.7515723270440252,"Note that the dH() is non-negative. Convert these two constraints to penalty functions, we have"
REFERENCES,0.7547169811320755,"max
φ
Eq(u|xp)"
REFERENCES,0.7578616352201258,""" Tc
X"
REFERENCES,0.7610062893081762,"t=1
log pZ(zt) − K
X"
REFERENCES,0.7641509433962265,"i=1
log
det
∂gu,xp,φi ∂rt,i  #"
REFERENCES,0.7672955974842768,−KL(q(u|xp)||p(u))
REFERENCES,0.7704402515723271,"+Eq(u|xp)

λ1(D(gu,xp,φ(z)) −1)2 + λ2dHL(gu,xp,φ(z), xp)

.
(16)"
REFERENCES,0.7735849056603774,"C
EXPERIMENT DETAILS"
REFERENCES,0.7767295597484277,"In this section, we provide more details on our experiments to help readers reproduce our results."
REFERENCES,0.779874213836478,"C.1
MODEL ARCHITECTURE"
REFERENCES,0.7830188679245284,"For experiments of weakly supervised classiﬁcation, and unpaired point cloud completion, we use
normalizing ﬂows with only conditional afﬁne coupling layers Klokov et al. (2020). Each layer is
deﬁned as"
REFERENCES,0.7861635220125787,"ya, yb = split(y)
s = ms(wy(ya) ⊙wx(x) + wb(x))
b = mb(cy(ya) ⊙cx(x) + cb(x))
zb = s ⊙yb + b
z = concat(ya, zb),
(17)"
REFERENCES,0.789308176100629,"where m, w, c are all small neural networks."
REFERENCES,0.7924528301886793,"For LLF, we only need the inverse ﬂow, i.e., gx,φ, for training and prediction, so in our experiments,
we actually deﬁne gx,φ as the forward transformation, and let y = s ⊙z + b. We do this because
multiplication and addition are more stable than division and subtraction."
REFERENCES,0.7955974842767296,"C.1.1
WEAKLY SUPERVISED CLASSIFICATION"
REFERENCES,0.7987421383647799,"In this problem, we use a ﬂow with 8 ﬂow steps, and each step has 2 conditional afﬁne coupling
layers. These two layers will transform different dimensions. Each w and c are small MLPs with
two linear layers. Each m has one linear layer. The hidden dimension of linear layers is ﬁxed to 64."
REFERENCES,0.8018867924528302,"C.1.2
WEAKLY SUPERVISED REGRESSION"
REFERENCES,0.8050314465408805,"In this problem, since the label y is a scalar, we use conditional afﬁne transformation introduced in
Section 6, as a ﬂow layer. A ﬂow has 8 ﬂow layers. The s and b in a ﬂow layer are three layer
MLPs. The hidden dimension of linear layers is 64."
REFERENCES,0.8081761006289309,Under review as a conference paper at ICLR 2022
REFERENCES,0.8113207547169812,"C.1.3
UNPAIRED POINT CLOUD COMPLETION"
REFERENCES,0.8144654088050315,"The model architecture used LLF used for this problem is illustrated in Figure 2. We use the same
architecture as DPF (Klokov et al., 2020) for point ﬂow. Speciﬁcally, the ﬂow has 8 ﬂow steps,
and each step has 3 conditional afﬁne coupling layers, i.e., Equation 17. Slightly different from the
original DPF, the conditioning networks wx, cx, wb, and cb will take the latent variable u and the
features of partial point cloud xp as input. The ws and cs are MLPs with two linear layers, whose
hidden dimension is 64. The ms are one layer MLPs."
REFERENCES,0.8176100628930818,"We use a PointNet (Qi et al., 2017) to extract features from partial point cloud xp. Following Klokov
et al. (2020), the hidden dimensions of this PointNet is set as 64−128−256−512. Given the features
of xp, the encoder E then uses the reparameterization trick (Kingma & Welling, 2013) to generate
latent variable u. The encoder has two linear layers, whose hidden dimension is 512."
REFERENCES,0.8207547169811321,"The GAN discriminator uses another PointNet to extract features from (generated) complete point
clouds. We follow Wu et al. (2020) and set the hidden dimensions of this PointNet as 64−128−128−
256−128. The discriminator D is a three layer MLP, whose hidden dimensions are 128−256−512."
REFERENCES,0.8238993710691824,PointNet
REFERENCES,0.8270440251572327,"E
Point"
REFERENCES,0.8301886792452831,"Flow
PointNet D xp u z y"
REFERENCES,0.8333333333333334,"xc
0 or 1"
REFERENCES,0.8364779874213837,"Figure 2: Model architecture of LLF for unpaired point cloud completion. The E represents the
encoder, and the D represents the GAN discriminator."
REFERENCES,0.839622641509434,"C.2
MORE EXPERIMENT DETAILS"
REFERENCES,0.8427672955974843,"C.2.1
WEAKLY SUPERVISED CLASSIFICATION"
REFERENCES,0.8459119496855346,"We use the same way as Arachie & Huang (2021b;a) to split each dataset to training, simulation,
and test sets. We use the data and labels in simulation sets to create weak signals, and estimated
bounds. We train models on training sets and test model on test sets. We assume that the models do
not have access to any labels. The labels in simulation sets are only used to generated weak signals
and estimate bounds."
REFERENCES,0.8490566037735849,"For experiments on Fashion-MNIST and the tabular datasets, we follow Arachie & Huang (2021b)
and choose 3 features to create weak signals. We train a logistic regression with each feature on the
simulation set, and use the label probabilities predicted by this logistic regression as weak signals.
We compute the error of this trained logistic regression on simulation set as estimated error bound."
REFERENCES,0.8522012578616353,"For experiments on real text datasets, we use the same keyword-based method as Arachie & Huang
(2021a) to create weak supervision. Speciﬁcally, we choose key words that can weakly indicate
positive and negative sentiments. Documents containing possitive words will be labeled as positive,
and vice versa."
REFERENCES,0.8553459119496856,"We list some main features of these datasets in Table 5. We refer to their original papers for more
details. For those datasets without ofﬁcial splits, we randomly split them with a ratio of 4 : 3 : 3."
REFERENCES,0.8584905660377359,Under review as a conference paper at ICLR 2022
REFERENCES,0.8616352201257862,"Table 5: Summary of datasets used in weakly supervised classiﬁcation experiments. The “—”
indicates this dataset does not have a ofﬁcial split"
REFERENCES,0.8647798742138365,"Dataset
Size
Train Size
Test Size
No. features
No. weak signals"
REFERENCES,0.8679245283018868,"Fashion MNIST (DvK)
14, 000
12, 000
2, 000
784
3
Fashion MNIST (SvA)
14, 000
12, 000
2, 000
784
3
Fashion MNIST (CvB)
14, 000
12, 000
2, 000
784
3
Breast Cancer
569
—
—
30
3
OBS Network
795
—
—
21
3
Cardiotocography
963
—
—
21
3
Clave Direction
8, 606
—
—
16
3
Credit Card
1, 000
—
—
24
3
Statlog Satellite
3, 041
—
—
36
3
Phishing Websites
11, 055
—
—
30
3
Wine Quality
4, 974
—
—
11
3
IMDB
49, 574
29, 182
20, 392
300
10
SST
5, 819
3, 998
1, 821
300
14
YELP
55, 370
45, 370
10, 000
300
14"
REFERENCES,0.8710691823899371,"C.2.2
WEAKLY SUPERVISED REGRESSION"
REFERENCES,0.8742138364779874,"We use three datasets from the UCI repository. For each dataset, we randomly split it to training,
simulation, and test sets with a ratio of 4 : 3 : 3. We use the simulation set to create weak signals and
estimated label values. We choose 5 features to create weak signals for each dataset. The detailed
introduction of these datasets are as follows. Table 6 summarize the statistical results of them."
REFERENCES,0.8773584905660378,"Air Quality. In this task, we predict the absolute humidity in air, based on other air quality features
such as hourly averaged temperature, hourly averaged NO2 concentration etc. The raw dataset has
9, 358 instances. We remove those instances with Nan values, resulting in a dataset with 8, 991 in-
stances. We use hourly averaged concentration CO, hourly averaged Benzene concentration, hourly
averaged NOx concentration, tungsten oxide hourly averaged sensor response, and relative humidity
as features for creating weak signals."
REFERENCES,0.8805031446540881,"Temperature Forecast. In this task, we predict the next day maximum air temperature based on
current day information. The raw dataset has 7, 750 instances, and we remove those instances with
Nan values, resulting in 7, 588 instances. We use present max temperature, forecasting next day
wind speed, forecasting next day cloud cover, forecasting next day precipitation, solar radiation as
features for creating weak signals."
REFERENCES,0.8836477987421384,"Bike Sharing. In this task, we predict the count of total rental bikes given weather and date in-
formation. The raw dataset has 17, 389 instances, and we remove those instances with Nan values,
resulting in 17, 379 instances. We use season, hour, if is working day, normalized feeling tempera-
ture, and wind speed as features for creating weak signals."
REFERENCES,0.8867924528301887,Table 6: Summary of datasets used in weakly supervised regression experiments
REFERENCES,0.889937106918239,"Dataset
Size
No. features
No. weak signals"
REFERENCES,0.8930817610062893,"Air Quality
8, 991
12
5
Temperature Forecast
7, 588
24
5
Bike Sharing
17, 379
12
5"
REFERENCES,0.8962264150943396,"C.2.3
UNPAIRED POINT CLOUD COMPLETION"
REFERENCES,0.89937106918239,"Datasets. We use the same way as Wu et al. (2020) to process PartNet Mo et al. (2019). PartNet
provides point-wise semantic labels for point clouds. The original point clouds are used as complete
point clouds. To generate partial point clouds, we randomly removed parts from complete point"
REFERENCES,0.9025157232704403,Under review as a conference paper at ICLR 2022
REFERENCES,0.9056603773584906,"clouds, based on the semantic labels. We use Chair, Table, and Lamp categories. The summary of
these three subsets are in Table 7."
REFERENCES,0.9088050314465409,Table 7: Summary of datasets used unpaird point cloud completion experiments
REFERENCES,0.9119496855345912,"Dataset
Train Size
Valid Size
No. Test Size"
REFERENCES,0.9150943396226415,"Chair
4, 489
617
1, 217
Table
5, 707
843
1, 668
Lamp
1, 545
234
416"
REFERENCES,0.9182389937106918,"Metrics. Let Xc be the set of referred complete point clouds, and Xp be the set of input par-
tial point clouds. For each partial point cloud x(p)
i
, we generate M complete point cloud samples
y(1)
i
, ..., y(M)
i
. All these samples form a new set of complete point clouds Y. In our experiments,
we follow Wu et al. (2020) and set M = 10."
REFERENCES,0.9213836477987422,The MMD Achlioptas et al. (2018) is deﬁned as
REFERENCES,0.9245283018867925,"MMD =
1
|Xc| X"
REFERENCES,0.9276729559748428,"xi∈Xc
dC(xi, NN(xi)),
(18)"
REFERENCES,0.9308176100628931,"where NN(x) is the nearest neighbor of x in Y. The dC represents Chamfer distance. MMD
computes the distance between the set of generated samples and the set of target complete shapes,
so it measures the quality of generated."
REFERENCES,0.9339622641509434,The TMD is deﬁned as
REFERENCES,0.9371069182389937,"TMD =
1
|Xp|"
REFERENCES,0.940251572327044,"|Xp|
X i=1 "
REFERENCES,0.9433962264150944,"
2
M −1 M
X j=1 M
X"
REFERENCES,0.9465408805031447,"k=j+1
dC(y(j)
i , y(k)
i
) "
REFERENCES,0.949685534591195,".
(19)"
REFERENCES,0.9528301886792453,"TMD measures the difference of generated samples given an input partial point cloud, so it measures
the diversity of samples."
REFERENCES,0.9559748427672956,The UHD is deﬁned as
REFERENCES,0.9591194968553459,"UHD =
1
|Xp|"
REFERENCES,0.9622641509433962,"|Xp|
X i=1  1 M M
X"
REFERENCES,0.9654088050314465,"j=1
dH(xi, y(j)
i ) "
REFERENCES,0.9685534591194969,",
(20)"
REFERENCES,0.9716981132075472,"where dH represents the unidirectional Hausdorff distance. UHD measures the similarity between
generated samples and input partial point clouds, so it measures the ﬁdelity of samples."
REFERENCES,0.9748427672955975,"Samples. We compare LLF to mm-pc2pc in Figure 3, and more samples of LLF in Figure 4,
Figure 5, and Figure 6. LLF can generate samples that are as good as mm-pc2pc. Mm-pc2pc has a
higher diversity in samples, but some generated samples may be unreasonable."
REFERENCES,0.9779874213836478,Under review as a conference paper at ICLR 2022
REFERENCES,0.9811320754716981,mm-pc2pc           LLF            Partial
REFERENCES,0.9842767295597484,"Figure 3: Random sample point clouds generated LLF and mm-pc2pc. The point clouds generated
by LLF are as realistic as mm-pc2pc. Mm-pc2pc has a higher diversity in samples. However,
sometimes it may generate unreasonable or invalid shapes."
REFERENCES,0.9874213836477987,"Figure 4: Random chair samples generated by LLF. The ﬁrst row is partial point clouds, and the
second row is generated complete point clouds."
REFERENCES,0.9905660377358491,Figure 5: Random lamp samples generated by LLF.
REFERENCES,0.9937106918238994,Under review as a conference paper at ICLR 2022
REFERENCES,0.9968553459119497,Figure 6: Random table samples generated by LLF.
