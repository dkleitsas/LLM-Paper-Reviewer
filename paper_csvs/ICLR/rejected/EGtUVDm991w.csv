Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0026666666666666666,"Despite the recent success in many applications, the high computational require-
ments of vision transformers limit their use in resource-constrained settings.
While many existing methods improve the quadratic complexity of attention, in
most vision transformers, self-attention is not the major computation bottleneck,
e.g., more than 80% of the computation is spent on fully-connected layers. To
improve the computational complexity of all layers, we propose a novel token
downsampling method, called Token Pooling, efÔ¨Åciently exploiting redundancies
in the images and intermediate token representations. We show that, under mild
assumptions, softmax-attention acts as a high-dimensional low-pass (smoothing)
Ô¨Ålter. Thus, its output contains redundancy that can be pruned to achieve a better
trade-off between the computational cost and accuracy. Our new technique accu-
rately approximates a set of tokens by minimizing the reconstruction error caused
by downsampling. We solve this optimization problem via cost-efÔ¨Åcient cluster-
ing. We rigorously analyze and compare to prior downsampling methods. Our
experiments show that Token Pooling signiÔ¨Åcantly improves the cost-accuracy
trade-off over the state-of-the-art downsampling. Token Pooling is simple and ef-
fective and can beneÔ¨Åt many architectures with global attention. Applied to DeiT,
NEW
it achieves the same ImageNet top-1 accuracy using 42% fewer computations."
INTRODUCTION,0.005333333333333333,"1
INTRODUCTION"
INTRODUCTION,0.008,"Vision transformers (Dosovitskiy et al., 2020; Touvron et al., 2021; Liu et al., 2021; Heo et al.,
2021; Zheng et al., 2021) have demonstrated state-of-the-art results in many vision applications,
from image classiÔ¨Åcation to segmentation. However, the high computational cost limits their use
in resource-restricted, real-time, or low-powered applications. While most prior work in Natural
Language Processing (NLP) improve the time-complexity of attention (Tay et al., 2020b; Ilharco
et al., 2020), in vision transformers the main computation bottleneck is the fully-connected layers,
as we show in ¬ß3.1. The computational complexity of these layers is determined by the number of
tokens and their feature dimensionality. While reducing the dimensionality improves computational
cost, it sacriÔ¨Åces model capacity and often signiÔ¨Åcantly deteriorates the accuracy of the model. On
the other hand, since images often contain mostly smooth surfaces with sparsely located edges and
corners, they contain similar (and thus redundant) features. Moreover, we show that, under mild
assumptions, softmax-attention is equivalent to low-pass Ô¨Åltering of tokens and thereby produces
tokens with similar features, as empirically observed by Goyal et al. (2020) and Rao et al. (2021).
This redundancy in representations suggests that we can reduce the number of tokens, i.e., down-
sampling, without a signiÔ¨Åcant impact to the accuracy, achieving a better cost-accuracy trade-off
than reducing feature dimensionality alone."
INTRODUCTION,0.010666666666666666,"Downsampling is widely used in Convolutional Neural Network (CNN) architectures to improve
computational efÔ¨Åciency, among other purposes. Given a grid of pixels or features, downsampling
gradually reduces the grid dimensions via combining neighboring vertices on the grid. The prevail-
ing max/average pooling and sub-sampling are examples of (spatially uniform) grid-downsampling
that only uses locations on the grid to decide which vertices to combine. Such methods do not efÔ¨Å-
ciently address non-uniformly distributed redundancy in images and features (Recasens et al., 2018;
Marin et al., 2019). Unlike CNNs that require grid preservation, transformers allow a wider range
of nonuniform data-aware downsampling layers, where a better operator can be designed."
INTRODUCTION,0.013333333333333334,"We propose Token Pooling, a novel nonuniform data-aware downsampling operator for transformers
efÔ¨Åciently exploiting redundancy in features. See the illustration and performance metric in Fig-"
INTRODUCTION,0.016,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.018666666666666668,"(a) Proposed Token Pooling
(b) Accuracy vs. computation"
INTRODUCTION,0.021333333333333333,"(c) Token Pooling via cluster analysis of token representations.
Figure 1: (a) We propose Token Pooling, a novel token downsampling method, for visual trans-
formers. (b) The proposed method achieves a state-of-the-art trade-off between accuracy and com-
putation. (c) Token Pooling uses cluster analysis to aggregate information from individual tokens
automatically. We show the input images and the token clusters at the 6-th layer of DeiT-S."
INTRODUCTION,0.024,"ures 1a & 1b. Motivated by nonuniform sampling and image compression (Marvasti, 2012; Unat
et al., 2009; Belfor et al., 1994; Rabbani, 2002), we formulate token downsampling as an opti-
mization problem that minimizes the reconstruction error caused by downsampling. We show that
clustering algorithms, K-Means and K-Medoids, efÔ¨Åciently solve this problem, see illustration in
Figure 1c. To the best of our knowledge, we are the Ô¨Årst to use this formulation and simple cluster-
ing analysis for token donwsampling in transformers. We compare with various prior downsampling
techniques, including grid-downsampling (Pan et al., 2021; Liu et al., 2021; Wang et al., 2021) and
token pruning (Goyal et al., 2020; Rao et al., 2021). The proposed Token Pooling outperforms exist-
ing methods and provides the best trade-off between computational cost and classiÔ¨Åcation accuracy."
INTRODUCTION,0.02666666666666667,Contributions. The paper makes the following contributions:
INTRODUCTION,0.029333333333333333,"‚Ä¢ We conduct an extensive study of prior downsampling techniques for visual transformers by com-
paring their computation-accuracy trade-offs."
INTRODUCTION,0.032,"‚Ä¢ We analyze the computational cost of vision-transformer components and the limitations of the
prior score-based downsampling methods. We also show that attention layers behave like low-pass
Ô¨Åltering and thus produce redundant tokens."
INTRODUCTION,0.034666666666666665,"‚Ä¢ Motivated by the redundancy in images and features, we propose a novel token downsampling
technique, Token Pooling, for transformers with global attention via error minimization and
NEW
achieve a signiÔ¨Åcant improvement in the computation-accuracy trade-off."
RELATED WORK,0.037333333333333336,"2
RELATED WORK"
RELATED WORK,0.04,"In this section, we introduce vision transformers, and review existing methods that improve the
efÔ¨Åciency of transformers including existing token-downsampling methods."
RELATED WORK,0.042666666666666665,Under review as a conference paper at ICLR 2022
VISION TRANSFORMERS,0.04533333333333334,"2.1
VISION TRANSFORMERS"
VISION TRANSFORMERS,0.048,"Vision transformers (Dosovitskiy et al., 2020; Touvron et al., 2021; Heo et al., 2021; Liu et al., 2021;
Pan et al., 2021) utilize the transformer architecture that is originally designed for NLP by Vaswani
et al. (2017) and further popularized by Radford et al. (2018) and Devlin et al. (2019). In a high
level, a vision transformer is a composition of L transformer blocks that take a set of input tokens and
return another set of output tokens. In vision, input tokens are features representing individual non-
overlapping image patches. To perform classiÔ¨Åcation, a classiÔ¨Åcation token is inserted to estimate
the probabilities of individual classes. To achieve the state-of-the-art, ViT (Dosovitskiy et al., 2020)
used pretraining on JFT-300M, a proprietary dataset much larger than standard ImageNet1k (Deng
et al., 2009). Recently, DeiT (Touvron et al., 2021) achieved state-of-the-art results with advanced
training on ImageNet1k only."
VISION TRANSFORMERS,0.050666666666666665,"Let the set of tokens at depth l be Fl = {f l
0, . . . , f l
N} where f l
i ‚ààRM is the fea-
ture values of the i-th token. A typical transformer block œÜ at depth l processes Fl
by a Multi-head Self-Attention (MSA) layer and a point-wise Multi-Layer Perceptron
(MLP). Let matrix F ‚ààRN√óM be a row-wise concatenation of tokens Fl. Then1,"
VISION TRANSFORMERS,0.05333333333333334,"œÜ(F ) = MLP(MSA(F ))
such that
(1)"
VISION TRANSFORMERS,0.056,"MSA(F ) = [O1, O2, . . . , OH]W O
(2)"
VISION TRANSFORMERS,0.058666666666666666,"where H is the number of heads, matrix W O ‚ààRM√óM is a learnable parameter of the block, [, ] is
column-wise concatenation and Oh ‚ààRN√ód is the output of h-th attention head for d = M/H:"
VISION TRANSFORMERS,0.06133333333333333,"Oh = AhVh
such that
Ah = softmax(QhK‚ä§
h
‚àö"
VISION TRANSFORMERS,0.064,"d) ‚ààRN√óN.
(3)"
VISION TRANSFORMERS,0.06666666666666667,"Keys Kh, queries Qh and values Vh are linear projections of the input tokens (QKV projections):"
VISION TRANSFORMERS,0.06933333333333333,"Qh = F W Q
h ,
Kh = F W K
h ,
Vh = F W V
h
(4)"
VISION TRANSFORMERS,0.072,"where W Q
h ‚ààRM√ód, W K
h ‚ààRM√ód, W V
h ‚ààRM√ód are learnable linear transformations. Note, the
number of tokens is not affected by the transformer blocks, i.e., |Fl+1| = |Fl|."
EFFICIENT TRANSFORMERS,0.07466666666666667,"2.2
EFFICIENT TRANSFORMERS"
EFFICIENT TRANSFORMERS,0.07733333333333334,"Similar to many machine learning models, the efÔ¨Åciency of transformers can be improved via meta-
parameter search (Howard et al., 2017; Tan & Le, 2019), automated neural architecture search
(Elsken et al., 2019; Tan et al., 2019; Wu et al., 2019), manipulating the input size and resolu-
tion of feature maps (Paszke et al., 2016; Howard et al., 2017; Zhao et al., 2018), pruning (LeCun
et al., 1990), quantization (Jacob et al., 2018), and sparsiÔ¨Åcation (Gale et al., 2019), etc. For exam-
ple, Dosovitskiy et al. (2020) and Touvron et al. (2021) obtain a family of ViT and DeiT models,
respectively, by varying the input resolution, the number of heads H, and the feature dimensionality
M. Each of the models operates with a different computational requirement and accuracy. In the
following, we review techniques developed for transformers."
EFFICIENT SELF-ATTENTION,0.08,"2.2.1
EFFICIENT SELF-ATTENTION"
EFFICIENT SELF-ATTENTION,0.08266666666666667,"The softmax-attention layer (3) has a quadratic time complexity w.r.t. the number of tokens, i.e.,
O(N 2). In many NLP applications where every token represents a word or a character, N can be
large, making attention a computation bottleneck (Dai et al., 2019; Rae et al., 2019). While many
works improve the time complexity of attention layers, as we will see in ¬ß3.1, they are not the
bottleneck in most current vision transformers."
EFFICIENT SELF-ATTENTION,0.08533333333333333,"The time complexity of an attention layer can be reduced by restricting the attention Ô¨Åeld-of-view
and thus imposing sparsity on Ah in (3). This can be achieved using the spatial relationship between
tokens in the image/text domain (Parmar et al., 2018; Ramachandran et al., 2019; Qiu et al., 2020;
Beltagy et al., 2020; Child et al., 2019; Zaheer et al., 2020) or based on token values using locality-
sensitive hashing, sorting, compression, etc. (Kitaev et al., 2020; Vyas et al., 2020; Tay et al., 2020a;
Liu et al., 2018; Wang et al., 2020; Tay et al., 2021). Prior works have also proposed attention
mechanisms with lower time complexity, e.g., O(N) or O(N log N) (Katharopoulos et al., 2020;"
EFFICIENT SELF-ATTENTION,0.088,Under review as a conference paper at ICLR 2022
EFFICIENT SELF-ATTENTION,0.09066666666666667,"Peng et al., 2021; Choromanski et al., 2021; Tay et al., 2021). Roy et al. (2021) cluster queries and
NEW
keys to sparsify attention matrices to speed-up the attention, but they do not downsample the tokens."
EFFICIENT SELF-ATTENTION,0.09333333333333334,"Note that the goal of these methods is to reduce the time complexity of the attention layer‚Äîthe
number of tokens remains the same across the transformer blocks. In contrast, our method reduces
the number of tokens after attention has been computed. Thereby, we can utilize these methods to
further improve the overall efÔ¨Åciency of transformers."
EFFICIENT SELF-ATTENTION,0.096,"Recently, Wu et al. (2020) proposed a new attention-based layer that learns a small number of query
vectors to extract information from the input feature map. Similarly, Wu et al. (2021) replace self-
attention with a new recurrent layer that outputs a smaller number of tokens. In comparison, our
method directly minimizes the token reconstruction error due to token downsampling. Also, our
layer has no learnable parameters and can be easily incorporated into existing vision transformers."
DOWNSAMPLING METHODS FOR TRANSFORMERS,0.09866666666666667,"2.2.2
DOWNSAMPLING METHODS FOR TRANSFORMERS"
DOWNSAMPLING METHODS FOR TRANSFORMERS,0.10133333333333333,"Grid-downsampling.
The input tokens of the Ô¨Årst vision transformer block are computed from
image patches. Therefore, even though transformers treat tokens as a set, we can still associate a
grid to the tokens using their initial locations on the image. The regular grid structure allows typical
downsampling methods, such as max/mean pooling, uniform sub-sampling. For example, Liu et al.
(2021), Heo et al. (2021), and Wang et al. (2021) use convolutions with stride to downsample the
feature maps formed by the tokens."
DOWNSAMPLING METHODS FOR TRANSFORMERS,0.104,"Score-based token downsampling.
In the area of NLP, Goyal et al. (2020) introduce the idea
of dropping tokens. Their approach, called PoWER-BERT, is based on a measure of signiÔ¨Åcance
score, which is deÔ¨Åned as the total attention given to a token from all other tokens. SpeciÔ¨Åcally, the
signiÔ¨Åcance scores of all tokens in the l-th transformer block, sl ‚ààRN, is computed by sl = H
X"
DOWNSAMPLING METHODS FOR TRANSFORMERS,0.10666666666666667,"h=1
Al
h
‚ä§1,
(5)"
DOWNSAMPLING METHODS FOR TRANSFORMERS,0.10933333333333334,"where Al
h is the attention weights of head h deÔ¨Åned in (3). They only pass Kl tokens with the
highest scores in sl to the next transformer block. The pruning is performed on all blocks."
DOWNSAMPLING METHODS FOR TRANSFORMERS,0.112,"PoWER-BERT is trained using a three-stage process. First, given a base architecture, they pretrain a
model without pruning. In the second stage, a soft-selection layer is inserted after each transformer
block, and the model is Ô¨Ånetuned for a small number of epochs. Once learned, the number of tokens
to keep, Kl, for each layer is computed from the soft-selection layers. Last, the model is Ô¨Ånetuned
again with the tokens pruned using the Kl from the second stage. See Goyal et al. (2020) for details."
DOWNSAMPLING METHODS FOR TRANSFORMERS,0.11466666666666667,"Recently, Rao et al. (2021) proposed Dynamic-ViT that also uses scores to prune tokens. Unlike
PoWER-BERT, which computes signiÔ¨Åcance scores from attention weights, Rao et al. use a dedi-
cated sub-network with learned parameters. The method requires knowledge distillation, Gumbel-
Softmax, and straight-through estimators on top of the DeiT training."
DOWNSAMPLING METHODS FOR TRANSFORMERS,0.11733333333333333,"We will analyze the limitations of score-based methods in ¬ß3.2. Note that while downsampling
NEW
tokens can complicate some downstream tasks, e.g., semantic segmentation and pose estimation,
many techniques Wu et al. (2020); Marin et al. (2019) have been proposed to deal with the problem."
ANALYSIS,0.12,"3
ANALYSIS"
ANALYSIS,0.12266666666666666,"This section addresses three questions. First, it identiÔ¨Åes the computational bottleneck of vision
transformers. Second, it discusses the limitations of score-based downsampling. Third, it analyzes
how the softmax-attention affects the redundancy in tokens."
COMPUTATION ANALYSIS OF VISION TRANSFORMERS,0.12533333333333332,"3.1
COMPUTATION ANALYSIS OF VISION TRANSFORMERS"
COMPUTATION ANALYSIS OF VISION TRANSFORMERS,0.128,"We analyze the time complexity and computational costs (measured in Ô¨Çops) of commonly used
vision transformers, namely ViT and DeiT. We breakdown the computation into four categories:"
COMPUTATION ANALYSIS OF VISION TRANSFORMERS,0.13066666666666665,"1For compactness, we omit layer norm and skip-connections, see Dosovitskiy et al. (2020) for details."
COMPUTATION ANALYSIS OF VISION TRANSFORMERS,0.13333333333333333,Under review as a conference paper at ICLR 2022
COMPUTATION ANALYSIS OF VISION TRANSFORMERS,0.136,"Layer
Complexity
Computation (109 Flops)
ViT-B/384
(N = 577)
ViT-B
(N = 197)
DeiT-S
(N = 197)
DeiT-Ti
(N = 197)
softmax-attention
O(LN 2M)
6.18
0.72
0.36
0.18
QKV projections
O(LNM 2)
12.25
4.18
1.05
0.26
O projection
O(LNM 2)
4.08
1.39
0.35
0.09
Multi-linear Perceptron
O(LNM 2)
32.67
11.15
2.79
0.70
Total
O(LNM(M + N))
55.5
17.6
4.6
1.3"
COMPUTATION ANALYSIS OF VISION TRANSFORMERS,0.13866666666666666,"Table 1: Time complexity and computation breakdown of ViT (Dosovitskiy et al., 2020) and DeiT
(Touvron et al., 2021). L is the number of transformer blocks, N is the number of input tokens
(patches), and M is the feature dimensionality. All models take input images of size 224 √ó 224
except ViT-B/384, which uses 384√ó384. The softmax-attention layers constitute a fraction (15% or
less) of the total compute, whereas fully-connected layers (MLP and projections) spend over 80%."
COMPUTATION ANALYSIS OF VISION TRANSFORMERS,0.14133333333333334,"(a) Score-based downsampling
(b) Proposed downsampling"
COMPUTATION ANALYSIS OF VISION TRANSFORMERS,0.144,"Figure 2: Score-based downsampling methods (Goyal et al., 2020; Rao et al., 2021) vs. our method.
In the Ô¨Ågure, the x-axis represents the token values (in one dimension), and the y-axis represents
their scores. Suppose four tokens are to be selected. (a) Score-based methods select tokens with
higher scores. Since the scoring function is continuous, all tokens in the left lobe will be selected,
resulting in redundancy and information loss in the right lobe. (b) The proposed method Ô¨Årst forms
four clusters to approximate the set of tokens, then selects the cluster centers. Thus, the output
tokens are a more accurate representation of the original token set than the score-based methods."
COMPUTATION ANALYSIS OF VISION TRANSFORMERS,0.14666666666666667,"softmax-attention (3), QKV projections (4), O projection (2) and MLP (1). As shown in Table 1, in
all these vision transformers, the main computational bottleneck is the fully-connected layers that
spend over 80% of the total computation. In comparison, softmax-attention only takes less than
15%. Note that we explicitly break down the multi-head attention into the softmax-attention, QKV
and O projections, as they have different time complexity, see Table 1. This decomposition reveals
that the QKV and O projections spend most of the computations of the multi-head self-attention."
LIMITATIONS OF SCORE-BASED TOKEN DOWNSAMPLING,0.14933333333333335,"3.2
LIMITATIONS OF SCORE-BASED TOKEN DOWNSAMPLING"
LIMITATIONS OF SCORE-BASED TOKEN DOWNSAMPLING,0.152,"Existing score-based token downsampling methods like PoWER-BERT and Dynamic-ViT utilize
scoring functions to determine the tokens to keep (or prune). They keep tokens with the top-K
scores and discard the rest. Since these scoring functions are continuous with limited Lipschitz
constants, tokens that are close in the feature space will be assigned similar scores. Therefore,
these similar tokens will likely either be all kept or discarded, as illustrated in Figure 2a. As our
experiments show, this redundancy (in the kept tokens) and severe information loss (in the pruned
tokens) deteriorate the computation-accuracy trade-off of the score-based downsampling methods."
LIMITATIONS OF SCORE-BASED TOKEN DOWNSAMPLING,0.15466666666666667,Under review as a conference paper at ICLR 2022
ATTENTION AS A LOW-PASS FILTER,0.15733333333333333,"3.3
ATTENTION AS A LOW-PASS FILTER"
ATTENTION AS A LOW-PASS FILTER,0.16,"Given a query vector q, a set of key vectors K = {k1, . . . , kN}, the corresponding value vectors
V = {v1, . . . , vN} and a scalar Œ± > 0, softmax-attention computes the output via"
ATTENTION AS A LOW-PASS FILTER,0.16266666666666665,"o(q) =
1
z(q) N
X"
ATTENTION AS A LOW-PASS FILTER,0.16533333333333333,"i=1
exp(Œ± q ¬∑ ki) vi
where
z(q) = N
X"
ATTENTION AS A LOW-PASS FILTER,0.168,"i=1
exp(Œ± q ¬∑ ki).
(6)"
ATTENTION AS A LOW-PASS FILTER,0.17066666666666666,"Note that we write o(q) to indicate that the output vector o is a function of the query q. If the query
vector and all key vectors are normalized to have a Ô¨Åxed ‚Ñì2 norm, we can rewrite (6) as"
ATTENTION AS A LOW-PASS FILTER,0.17333333333333334,"o(q) =
1
z‚Ä≤(q) N
X"
ATTENTION AS A LOW-PASS FILTER,0.176,"i=1
exp

‚àíŒ±"
ATTENTION AS A LOW-PASS FILTER,0.17866666666666667,"2 ‚à•q ‚àíki‚à•2
vi =
1
z‚Ä≤(q)"
ATTENTION AS A LOW-PASS FILTER,0.18133333333333335,"Z
exp

‚àíŒ±"
ATTENTION AS A LOW-PASS FILTER,0.184,"2 ‚à•q ‚àík‚à•2 N
X"
ATTENTION AS A LOW-PASS FILTER,0.18666666666666668,"i=1
Œ¥(k ‚àíki)vi ! dk"
ATTENTION AS A LOW-PASS FILTER,0.18933333333333333,"=
1
z‚Ä≤(q) G

q; 1 Œ±"
ATTENTION AS A LOW-PASS FILTER,0.192,"
‚àóS(q; K, V)
(7)"
ATTENTION AS A LOW-PASS FILTER,0.19466666666666665,"where ‚àórepresents high-dimensional convolution, z‚Ä≤(q) = P"
ATTENTION AS A LOW-PASS FILTER,0.19733333333333333,"i exp
 
‚àíŒ±"
ATTENTION AS A LOW-PASS FILTER,0.2,"2 ‚à•q ‚àíki‚à•2
= G
 
q; 1"
ATTENTION AS A LOW-PASS FILTER,0.20266666666666666,"Œ±

‚àó
S(q; K, 1) is the normalization scalar function, G
 
q; œÉ2
= exp
 
‚àí‚à•q‚à•2
2œÉ2
is an isometric Gaus-
sian kernel, and S(q; K, V) = PN
i=1 Œ¥(q ‚àíki) vi is a high-dimensional sparse signal, which is
composed of N delta functions located at ki with value vi. According to (7), given query vectors
q1, . . . , qN, there are two conceptual steps to compute softmax-attention:"
ATTENTION AS A LOW-PASS FILTER,0.20533333333333334,"1. Ô¨Ålter S(q; K, V) with a Gaussian kernel to get o(q), and"
ATTENTION AS A LOW-PASS FILTER,0.208,"2. sample o(q) at coordinates q1, . . . , qN to get the output vectors o1, . . . , oN."
ATTENTION AS A LOW-PASS FILTER,0.21066666666666667,"Since Gaussian Ô¨Åltering is low-pass, o(q) is a smooth signal. Therefore, the output tokens of the
attention layer, i.e., discrete samples of o(q), contain similar feature values. So, based on Nyquist-
Shannon sampling theorem (Shannon, 1949), there exists redundant information in the output, which
is used by our methods to reduce computation without losing much of the important information."
ATTENTION AS A LOW-PASS FILTER,0.21333333333333335,"Note that our analysis is based on the normalized query and key vectors, which can be achieved
by inserting a normalizing layer before the softmax-attention layer without signiÔ¨Åcantly affecting
the performance of a transformer, as shown by Kitaev et al. (2020). It has also been empirically
observed by Goyal et al. (2020) and Rao et al. (2021) that even without the normalization, trans-
formers produce tokens with similar values. To demonstrate this, in all our experiments, we use
standard multi-head attention without normalizing keys and queries. We conduct an ablation study
with normalized keys and queries in Appendix F."
TOKEN POOLING,0.216,"4
TOKEN POOLING"
TOKEN POOLING,0.21866666666666668,"Pruning tokens inevitably loses information. In this section, we formulate a new token downsam-
pling principle enabling strategical tokens selection that preserves the most information. Based on
this principle, we formulate and discuss several Token Pooling algorithms."
TOKEN POOLING,0.22133333333333333,"Given a set of output tokens F = {f1, . . . , fN} of a transformer block, our goal is to Ô¨Ånd a smaller
set of tokens ÀÜF = { ÀÜf1, . . . , ÀÜfK} that after upsampling minimizes the reconstruction error of F.
SpeciÔ¨Åcally, the reconstruction of u(fi; ÀÜF) of token fi is computed via interpolating the tokens
in ÀÜF. The reconstruction error is then deÔ¨Åned as"
TOKEN POOLING,0.224,"‚Ñì(F, ÀÜF) =
X"
TOKEN POOLING,0.22666666666666666,"fi‚ààF
‚à•fi ‚àíu(fi; ÀÜF)‚à•2.
(8)"
TOKEN POOLING,0.22933333333333333,"To simplify our formulation and reduce computation, we use nearest-neighbor interpolation as u.
As a consequence, the reconstruction error (8) becomes"
TOKEN POOLING,0.232,"‚Ñì(F, ÀÜF) =
X"
TOKEN POOLING,0.23466666666666666,"fi‚ààF
min
ÀÜ
fj‚ààÀÜ
F
‚à•fi ‚àíÀÜfj‚à•2,
(9)"
TOKEN POOLING,0.23733333333333334,"which is the asymmetric Chamfer divergence between F and ÀÜF (Barrow et al., 1977; Mechrez et al.,
2018). The loss (9) can be minimized by the K-Means algorithm, i.e., clustering the tokens in F
into K clusters."
TOKEN POOLING,0.24,Under review as a conference paper at ICLR 2022
TOKEN POOLING,0.24266666666666667,"Algorithm 1: Token Pooling
input : tokens F = {f1, . . . , fN}; target set size K; (optional) weights W = {w1, . . . , wN}
output: downsampled set ÀÜF = { ÀÜf1, . . . , ÀÜfK}"
TOKEN POOLING,0.24533333333333332,1 if k ‚â•N then return F;
TOKEN POOLING,0.248,2 initialize cluster centers ÀÜF to be the K tokens from F with the highest weights ;
WHILE NOT CONVERGED AND MAX NUMBER OF ITERATIONS IS NOT REACHED DO,0.25066666666666665,3 while not converged and max number of iterations is not reached do
WHILE NOT CONVERGED AND MAX NUMBER OF ITERATIONS IS NOT REACHED DO,0.25333333333333335,"4
for i ‚àà{1, ..., N} do update cluster assignment z(i) ‚Üêarg minK
j=1 ‚à•fi ‚àíÀÜfj‚à•;"
WHILE NOT CONVERGED AND MAX NUMBER OF ITERATIONS IS NOT REACHED DO,0.256,"5
for j ‚àà{1, ..., K} do update cluster center ÀÜfj according to the chosen clustering
algorithm, that is either (11), (12), (15) or (18) ;"
RETURN WEIGHTED MEANS OF TOKENS IN EACH CLUSTER,0.25866666666666666,6 return weighted means of tokens in each cluster
RETURN WEIGHTED MEANS OF TOKENS IN EACH CLUSTER,0.2613333333333333,"The proposed Token Pooling layer is deÔ¨Åned in Algorithm 1. It downsamples input tokens via
clustering the tokens and returns the cluster centers (the average of the tokens in a cluster). As
we have shown above, this operation directly minimizes the reconstruction error (9) caused by the
downsampling. Intuitively, clustering the tokens provides a more accurate and diverse representation
of the original set of tokens, compared to the top-K selection used by score-based downsampling
methods, as shown in Figure 2b. Note that Token Pooling is robust to the initialization of cluster
centers, as shown in Appendix B. Below, we provide details of the clustering algorithms.
K-Means.
We use the K-Means algorithm to minimize (9) via the following iterations:"
RETURN WEIGHTED MEANS OF TOKENS IN EACH CLUSTER,0.264,"a(i)
‚Üê
arg min
j
‚à•fi ‚àíÀÜfj‚à•
‚àÄi ‚àà{1, 2, . . . , N}
(10) ÀÜfj
‚Üê N
X"
RETURN WEIGHTED MEANS OF TOKENS IN EACH CLUSTER,0.26666666666666666,"i=1
[a(i) = j]fi , N
X"
RETURN WEIGHTED MEANS OF TOKENS IN EACH CLUSTER,0.2693333333333333,"i=1
[a(i) = j]
‚àÄj ‚àà{1, 2, . . . , K}
(11)"
RETURN WEIGHTED MEANS OF TOKENS IN EACH CLUSTER,0.272,"where [ ] is the Iverson bracket and a is the cluster assignment function. The overall algorithm com-
plexity is O(TKNM) where T is the number of iterations. The vast majority of the computation is
spent on the repetitive evaluation of the distances between tokens and centroids in step (10).
K-Medoids.
We can use the more efÔ¨Åcient K-Medoids algorithm by replacing step (11) with:"
RETURN WEIGHTED MEANS OF TOKENS IN EACH CLUSTER,0.27466666666666667,"n(j) ‚Üêarg min
i:a(i)=j X"
RETURN WEIGHTED MEANS OF TOKENS IN EACH CLUSTER,0.2773333333333333,"i‚Ä≤:a(i‚Ä≤)=j
‚à•fi ‚àífi‚Ä≤‚à•2
and
ÀÜfj ‚Üêfn(j).
(12)"
RETURN WEIGHTED MEANS OF TOKENS IN EACH CLUSTER,0.28,These steps minimize objective (9) under the medoid constraint: ÀÜF ‚äÜF.
RETURN WEIGHTED MEANS OF TOKENS IN EACH CLUSTER,0.2826666666666667,"The advantage of the K-Medoids algorithm is its time complexity O(TKN + N 2M), which is
substantially lower in practice as we only compute the distances between tokens once. In our ex-
perience, it requires less than 5 iterations to converge. Apart from the distance matrix computation,
the cost of the K-Medoids algorithm is negligible when compared with the cost of the other layers.
Weighted clustering.
Reconstruction error (9) treats every token equally; however, each token
contributes differently to the Ô¨Ånal output of an attention layer. Thus, we also consider a weighted
reconstruction error: ‚Ñì(F, ÀÜF; w) = P"
RETURN WEIGHTED MEANS OF TOKENS IN EACH CLUSTER,0.2853333333333333,"fi‚ààF min ÀÜ
fj‚ààÀÜ
F wi‚à•fi ‚àíÀÜfj‚à•2 where w = [w1, . . . , wN] are
the positive weights corresponding to the individual tokens in F. Appendix A details the clustering
algorithms for the weighted case. A good choice of the weights is the signiÔ¨Åcance scores (5), i.e.,
wi = si. The signiÔ¨Åcance score identiÔ¨Åes the tokens that inÔ¨Çuence the current transformer block
most and thus should be approximated more precisely."
RETURN WEIGHTED MEANS OF TOKENS IN EACH CLUSTER,0.288,"Note, when applying Token Pooling, it is important to keep special tokens like the classiÔ¨Åcation
NEW
token. If spatial locations on an image are need, e.g., in the case of semantic segmentation or pose
estimation, techniques like (Wu et al., 2020) can be utilized to project tokens back to the image grid."
EXPERIMENTS,0.2906666666666667,"5
EXPERIMENTS"
EXPERIMENTS,0.29333333333333333,"Our implementation is based on DeiT (Touvron et al., 2021) where we insert downsampling lay-
ers after each transformer block. To evaluate the pure effect of downsampling, we keep all meta-
parameters of DeiT, including the feature dimensionality, network depth, learning rate schedule, etc."
EXPERIMENTS,0.296,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2986666666666667,"1
2
3
4
Gflops 70 72 74 76 78 80"
EXPERIMENTS,0.30133333333333334,accuracy (%)
EXPERIMENTS,0.304,"Ours (WK-Medoids)
PoWER-BERT
Importance Selection
Random Selection
Convolution"
EXPERIMENTS,0.30666666666666664,"(a) Models based on DeiT-S
(b) Comparison with current SOTA"
EXPERIMENTS,0.30933333333333335,"Figure 3: Main results. (a) shows the accuracy when we apply different downsampling methods
to DeiT-S. More is in Appendix G. (b) shows a comparison between the proposed method with the
state-of-the-art downsampling methods. The results of our method and PoWER-BERT are acquired
by varying K and the base architecture among DeiT-Ti, DeiT-e252, DeiT-e318, and DeiT-S."
EXPERIMENTS,0.312,"We do not use knowledge distillation. We use ImageNet1k (Russakovsky et al., 2015). Our cost and
performance metrics are Ô¨Çops and top-1 accuracy. Appendix H reports throughput of our models.
NEW"
EXPERIMENTS,0.31466666666666665,"Methods.
We evaluate the following token downsampling methods:"
EXPERIMENTS,0.31733333333333336,"1. Convolution downsampling. We implement uniform grid-downsampling via convolution with
stride 2, i.e., the tokens corresponding to the adjacent image patches are concatenated and mapped
to RM. This design is used in Liu et al. (2021); Heo et al. (2021). See details in Appendix D."
EXPERIMENTS,0.32,"2. PoWER-BERT. We implement PoWER-BERT on DeiT, following Goyal et al. (2020)."
EXPERIMENTS,0.32266666666666666,"3. Random selection, a simple baseline randomly selecting K tokens without replacement."
IMPORTANCE SELECTION CHOOSES K TOKENS BY DRAWING FROM A CATEGORICAL DISTRIBUTION WITHOUT RE-,0.3253333333333333,"4. Importance selection chooses K tokens by drawing from a categorical distribution without re-
placement with probabilities proportional to the signiÔ¨Åcance score (5) of each token."
IMPORTANCE SELECTION CHOOSES K TOKENS BY DRAWING FROM A CATEGORICAL DISTRIBUTION WITHOUT RE-,0.328,"5. Token Pooling use K-Means or K-Medoids algorithms, or their weighted versions, WK-Means or
WK-Medoids, respectively. The weights are the signiÔ¨Åcance scores (5)."
IMPORTANCE SELECTION CHOOSES K TOKENS BY DRAWING FROM A CATEGORICAL DISTRIBUTION WITHOUT RE-,0.33066666666666666,"Selection of K = [K1, . . . , KL].
To fairly compare our Token Pooling with PoWER-BERT and
other baselines, all methods (except convolution downsampling) use the same number of retained
tokens for downsampling layers. Appendix C details the selection of K."
IMPORTANCE SELECTION CHOOSES K TOKENS BY DRAWING FROM A CATEGORICAL DISTRIBUTION WITHOUT RE-,0.3333333333333333,"Training protocol.
(1) A base DeiT model (e.g., DeiT-S) is trained using the original training
(Touvron et al., 2021). (2) We then Ô¨Ånetune the base DeiT model using the second stage of PoWER-
BERT‚Äôs training and acquire K. (3) We further Ô¨Ånetune the downsampling methods using K.
(4) We also Ô¨Ånetune the base DeiT model. Our protocol ensures a fair comparison such that all of
the models are trained with the same number of iterations, the same learning rate schedule, etc."
MAIN RESULTS,0.336,"5.1
MAIN RESULTS"
MAIN RESULTS,0.33866666666666667,"First, we apply different downsampling methods on DeiT-S. As shown in Figure 3a, random se-
lection achieves a similar trade-off as lowering feature dimensionality M. While convolution with
stride is better than adjusting M at the low-compute regime, it fails in high-compute regimes. Im-
portance selection improves upon random selection but is still outperformed by PoWER-BERT. Our
Token Pooling (with weighted K-Medoids) achieves the best trade-off in all regimes."
MAIN RESULTS,0.3413333333333333,"Next, we apply Token Pooling to DeiT models with different M (DeiT-Ti, DeiT-e252, DeiT-e318,
and DeiT-S). Figure 4 shows trade-off curves for each DeiT model. Token Pooling enables each
of the models to achieve a better computation-accuracy trade-off than simply varying the feature
dimensionality M. For each computational budget, we Ô¨Ånd the combination of M and K that gives
the highest accuracy. The best balance is achieved by applying Token Pooling and selecting the best"
MAIN RESULTS,0.344,Under review as a conference paper at ICLR 2022
MAIN RESULTS,0.3466666666666667,"1
2
3
4
Gflops 72 74 76 78 80"
MAIN RESULTS,0.34933333333333333,accuracy (%)
MAIN RESULTS,0.352,DeiT-e252
MAIN RESULTS,0.3546666666666667,"DeiT-e318
DeiT-S"
MAIN RESULTS,0.35733333333333334,"Ours (on DeiT-S)
Ours (on DeiT-e318)
Ours (on DeiT-e252)"
MAIN RESULTS,0.36,"Figure 4: The Ô¨Ågure shows the results when we
apply Token Pooling to various DeiT architec-
tures. Token Pooling consistently improves the
computation-accuracy trade-off for all evaluated
architectures.
By utilizing both Token Pooling
and architecture search, we can further improve
the accuracy at a given Ô¨Çops budget. For exam-
ple, at 1 GÔ¨Çop, we should use Token Pooling on
DeiT-e252 instead of DeiT-S."
MAIN RESULTS,0.3626666666666667,"1
1.5
2
2.5
3
3.5
4
Gflops 74 76 78 80"
MAIN RESULTS,0.36533333333333334,accuracy (%)
MAIN RESULTS,0.368,"Ours (WK-Medoids)
PoWER-BERT
Importance Selection
Random Selection"
MAIN RESULTS,0.37066666666666664,(a) Ours vs. methods using signiÔ¨Åcance score
MAIN RESULTS,0.37333333333333335,"1
2
3
4
Gflops 74 76 78 80"
MAIN RESULTS,0.376,accuracy (%)
MAIN RESULTS,0.37866666666666665,"(Ours) WK-Medoids
(Ours) K-Medoids
(Ours) WK-Means
(Ours) K-Means
PoWER-BERT"
MAIN RESULTS,0.38133333333333336,(b) Variants of Token Pooling
MAIN RESULTS,0.384,"Figure 5: Ablation studies of (a) downsampling methods using signiÔ¨Åcance score, and (b) proposed
Token Pooling using different clustering algorithms. The base model is DeiT-S for all methods."
MAIN RESULTS,0.38666666666666666,"M. We do the same for PoWER-BERT. Figure 3b shows the results of the proposed Token Pooling
and state-of-the-art methods. We cite the results of Rao et al. (2021), d‚ÄôAscoli et al. (2021), and Pan
et al. (2021). Our Token Pooling achieves the best accuracy across all evaluated compute regimes."
MAIN RESULTS,0.3893333333333333,"Finally, we compare the best computation-accuracy trade-off achieved by Token Pooling (with WK-
Medoids and varying M) with standard DeiT models in Figure 1b. As can be seen, utilizing Token
Pooling, we signiÔ¨Åcantly improve the computation costs of DeiT-Ti by 42% and improve the top-1
accuracy by 3.3 points at the same Ô¨Çops. Similar beneÔ¨Åts can be seen on DeiT-e252 and DeiT-e318."
ABLATION STUDIES,0.392,"5.2
ABLATION STUDIES"
ABLATION STUDIES,0.39466666666666667,"Figure 5a compares methods utilizing signiÔ¨Åcance scores. As can be seen, using importance selec-
tion improves upon the simple random selection. By minimizing the reconstruction error (9), our
method achieves better cost-accuracy trade-off. Figure 5b evaluates Token Pooling with different
clustering algorithms. Weighted versions outperform regular versions of K-Means and K-Medoids.
Due to the higher time complexity, K-Means is outperformed by K-Medoids (the curves are shifted
toward the right). See the metrics and Ô¨Çops used by clustering in table format in Appendix G. All
Token Pooling variants outperform the baseline, demonstrating the effectiveness of our method."
ABLATION STUDIES,0.3973333333333333,"More ablation studies are in the appendices: clustering initialization (Appendix B), convolution
downsampling (D), Token Pooling without Ô¨Ånetuning (E), results with normalized keys and queries
(F), and detailed information (K, Ô¨Çops, accuracy, clustering cost) of our models (G)."
CONCLUSIONS,0.4,"6
CONCLUSIONS"
CONCLUSIONS,0.4026666666666667,"This paper provides two insights of vision transformers: Ô¨Årst, their computational bottleneck is
the fully-connected layers, and second, attention layers generate redundant representations due to
the connection to Gaussian Ô¨Åltering. Token Pooling, our novel nonuniform data-aware downsam-
pling operator, utilizes these insights and signiÔ¨Åcantly improves the computation-accuracy balance
of DeiT, compared to existing downsampling techniques. We believe that Token Pooling is a timely
development and can be used with other techniques (e.g., architecture search, quantization) to spur
the design of efÔ¨Åcient vision transformers."
CONCLUSIONS,0.4053333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.408,REFERENCES
REFERENCES,0.4106666666666667,"Harry G Barrow, Jay M Tenenbaum, Robert C Bolles, and Helen C Wolf. Parametric correspon-
dence and chamfer matching: Two new techniques for image matching. Technical report, SRI
International Menlo Park CA ArtiÔ¨Åcial Intelligence Center, 1977."
REFERENCES,0.41333333333333333,"Ricardo AF Belfor, Marc PA Hesp, Reginald L Lagendijk, and Jan Biemond. Spatially adaptive
subsampling of image sequences. Transactions on Image Processing, 3:492‚Äì500, 1994."
REFERENCES,0.416,"Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv:2004.05150, 2020."
REFERENCES,0.4186666666666667,"Yizong Cheng. Mean shift, mode seeking, and clustering. IEEE Transactions on Pattern Analysis
and Machine Intelligence (PAMI), 17(8):790‚Äì799, 1995."
REFERENCES,0.42133333333333334,"Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509, 2019."
REFERENCES,0.424,"Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention
with performers. International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.4266666666666667,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-XL: Attentive language models beyond a Ô¨Åxed-length context. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pp. 2978‚Äì2988, 2019."
REFERENCES,0.42933333333333334,"St¬¥ephane d‚ÄôAscoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun.
ConViT: Improving vision transformers with soft convolutional inductive biases. International
Conference on Machine Learning (ICML), 2021."
REFERENCES,0.432,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 248‚Äì255, 2009."
REFERENCES,0.43466666666666665,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171‚Äì4186, 2019."
REFERENCES,0.43733333333333335,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An im-
age is worth 16x16 words: Transformers for image recognition at scale. In International Confer-
ence on Learning Representations (ICLR), 2020."
REFERENCES,0.44,"Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The
Journal of Machine Learning Research, 20(1):1997‚Äì2017, 2019."
REFERENCES,0.44266666666666665,"Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv
preprint arXiv:1902.09574, 2019."
REFERENCES,0.44533333333333336,"Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan Chakaravarthy, Yogish Sab-
harwal, and Ashish Verma. PoWER-BERT: Accelerating BERT inference via progressive word-
vector elimination. In International Conference on Machine Learning (ICML), pp. 3690‚Äì3699,
2020."
REFERENCES,0.448,"Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon
Oh. Rethinking spatial dimensions of vision transformers. In IEEE International Conference on
Computer Vision (ICCV), 2021."
REFERENCES,0.45066666666666666,"Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. MobileNets: EfÔ¨Åcient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017."
REFERENCES,0.4533333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.456,"Gabriel Ilharco, Cesar Ilharco, Iulia Turc, Tim Dettmers, Felipe Ferreira, and Kenton Lee. High
performance natural language processing. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: Tutorial Abstracts, pp. 24‚Äì27, 2020."
REFERENCES,0.45866666666666667,"Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,
Hartwig Adam, and Dmitry Kalenichenko.
Quantization and training of neural networks for
efÔ¨Åcient integer-arithmetic-only inference. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 2704‚Äì2713, 2018."
REFERENCES,0.4613333333333333,"Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc¬∏ois Fleuret. Transformers are
RNNs: Fast autoregressive transformers with linear attention. In International Conference on
Machine Learning (ICML), pp. 5156‚Äì5165, 2020."
REFERENCES,0.464,"Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
Reformer: The efÔ¨Åcient transformer.
In
International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.4666666666666667,"Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 598‚Äì605, 1990."
REFERENCES,0.4693333333333333,"Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam
Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on
Learning Representations (ICLR), 2018."
REFERENCES,0.472,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
Swin transformer: Hierarchical vision transformer using shifted windows, October 2021."
REFERENCES,0.4746666666666667,"Dmitrii Marin, Zijian He, Peter Vajda, Priyam Chatterjee, Sam Tsai, Fei Yang, and Yuri Boykov.
EfÔ¨Åcient segmentation: Learning downsampling near semantic boundaries. In IEEE International
Conference on Computer Vision (ICCV), pp. 2131‚Äì2141, 2019."
REFERENCES,0.47733333333333333,"Farokh Marvasti. Nonuniform Sampling: Theory and Practice. Springer Science & Business Media,
2012."
REFERENCES,0.48,"Roey Mechrez, Itamar Talmi, Firas Shama, and Lihi Zelnik-Manor. Maintaining natural image
statistics with the contextual loss. In Asian Conference on Computer Vision (ACCV), pp. 427‚Äì
443, 2018."
REFERENCES,0.4826666666666667,"Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jianfei Cai. Scalable visual transformers with
hierarchical pooling. IEEE International Conference on Computer Vision (ICCV), 2021."
REFERENCES,0.48533333333333334,"Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In International Conference on Machine Learning (ICML), pp.
4055‚Äì4064. PMLR, 2018."
REFERENCES,0.488,"Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello. ENet: A deep neu-
ral network architecture for real-time semantic segmentation. arXiv preprint arXiv:1606.02147,
2016."
REFERENCES,0.49066666666666664,"Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong.
Random feature attention.
In International Conference on Learning Representations (ICLR),
2021."
REFERENCES,0.49333333333333335,"Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-
attention for long document understanding. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: Findings, pp. 2555‚Äì2565, 2020."
REFERENCES,0.496,"Majid Rabbani. JPEG2000: Image compression fundamentals, standards and practice. Journal of
Electronic Imaging, 11(2):286, 2002."
REFERENCES,0.49866666666666665,"Alec
Radford,
Karthik
Narasimhan,
Tim
Salimans,
and
Ilya
Sutskever.
Improv-
ing
language
understanding
by
generative
pre-training.
2018.
URL
https:
//s3-us-west-2.amazonaws.com/openai-assets/research-covers/
language-unsupervised/language_understanding_paper.pdf."
REFERENCES,0.5013333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.504,"Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.
Compressive transformers for long-range sequence modelling. In International Conference on
Learning Representations (ICLR), 2019."
REFERENCES,0.5066666666666667,"Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.
Stand-alone self-attention in vision models. 32, 2019."
REFERENCES,0.5093333333333333,"Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. DynamicViT:
EfÔ¨Åcient vision transformers with dynamic token sparsiÔ¨Åcation. ArXiv, abs/2106.02034, 2021."
REFERENCES,0.512,"Adria Recasens, Petr Kellnhofer, Simon Stent, Wojciech Matusik, and Antonio Torralba. Learn-
ing to zoom: a saliency-based sampling layer for neural networks. In European Conference on
Computer Vision (ECCV), pp. 51‚Äì66, 2018."
REFERENCES,0.5146666666666667,"Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. EfÔ¨Åcient content-based sparse
attention with routing transformers. Transactions of the Association for Computational Linguis-
tics, 9:53‚Äì68, 2021."
REFERENCES,0.5173333333333333,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual
recognition challenge. International Journal of Computer Vision (IJCV), 115(3):211‚Äì252, 2015."
REFERENCES,0.52,"Claude E Shannon. Communication in the presence of noise. Proceedings of the IRE, 37(1):10‚Äì21,
jan 1949. doi: 10.1109/jrproc.1949.232969. URL https://doi.org/10.1109/jrproc.
1949.232969."
REFERENCES,0.5226666666666666,"Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on
Pattern Analysis and Machine Intelligence (PAMI), 22(8):888‚Äì905, 2000."
REFERENCES,0.5253333333333333,"Mingxing Tan and Quoc Le. EfÔ¨ÅcientNet: Rethinking model scaling for convolutional neural net-
works. In International Conference on Machine Learning (ICML), pp. 6105‚Äì6114, 2019."
REFERENCES,0.528,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
Quoc V Le. MnasNet: Platform-aware neural architecture search for mobile. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 2820‚Äì2828, 2019."
REFERENCES,0.5306666666666666,"Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In
International Conference on Machine Learning (ICML), pp. 9438‚Äì9447, 2020a."
REFERENCES,0.5333333333333333,"Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. EfÔ¨Åcient transformers: A survey. arXiv
preprint arXiv:2009.06732, 2020b."
REFERENCES,0.536,"Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer:
Rethinking self-attention in transformer models. International Conference on Machine Learning
(ICML), 2021."
REFERENCES,0.5386666666666666,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Herv¬¥e J¬¥egou. Training data-efÔ¨Åcient image transformers & distillation through attention. pp.
10347‚Äì10357, 2021."
REFERENCES,0.5413333333333333,"Didem Unat, Theodore Hromadka, and Scott B Baden. An adaptive sub-sampling method for in-
memory compression of scientiÔ¨Åc data. In Data Compression Conference, pp. 262‚Äì271. IEEE,
2009."
REFERENCES,0.544,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Informa-
tion Processing Systems (NeurIPS), 30:5998‚Äì6008, 2017."
REFERENCES,0.5466666666666666,"Andrea Vedaldi and Stefano Soatto. Quick shift and kernel methods for mode seeking. In European
Conference on Computer Vision (ECCV), pp. 705‚Äì718, 2008."
REFERENCES,0.5493333333333333,"Apoorv Vyas, Angelos Katharopoulos, and Franc¬∏ois Fleuret. Fast transformers with clustered atten-
tion. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020."
REFERENCES,0.552,Under review as a conference paper at ICLR 2022
REFERENCES,0.5546666666666666,"Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768, 2020."
REFERENCES,0.5573333333333333,"Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,
and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction with-
out convolutions. In IEEE International Conference on Computer Vision (ICCV), pp. 568‚Äì578,
October 2021."
REFERENCES,0.56,"Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian,
Peter Vajda, Yangqing Jia, and Kurt Keutzer. FBNet: Hardware-aware efÔ¨Åcient ConvNet design
via differentiable neural architecture search. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 10734‚Äì10742, 2019."
REFERENCES,0.5626666666666666,"Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi
Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based
image representation and processing for computer vision. arXiv preprint arXiv:2006.03677, 2020."
REFERENCES,0.5653333333333334,"Lemeng Wu, Xingchao Liu, and Qiang Liu. Centroid transformers: Learning to abstract with atten-
tion. arXiv preprint arXiv:2102.08606, 2021."
REFERENCES,0.568,"Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer
sequences. Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.5706666666666667,"Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. ICNet for real-time
semantic segmentation on high-resolution images. In European Conference on Computer Vision
(ECCV), pp. 405‚Äì420, 2018."
REFERENCES,0.5733333333333334,"Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei
Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from
a sequence-to-sequence perspective with transformers. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 6881‚Äì6890, 2021."
REFERENCES,0.576,Under review as a conference paper at ICLR 2022
REFERENCES,0.5786666666666667,"A
WEIGHTED CLUSTERING ALGORITHMS"
REFERENCES,0.5813333333333334,"Weighted K-Means
minimizes the following objective w.r.t. ÀÜF = { ÀÜf1, . . . , ÀÜfK} ‚äÇRM:"
REFERENCES,0.584,"‚Ñì(F, ÀÜF) =
X"
REFERENCES,0.5866666666666667,"fi‚ààF
min
ÀÜ
fj‚ààÀÜ
F
wi‚à•fi ‚àíÀÜfj‚à•2
(13)"
REFERENCES,0.5893333333333334,The extension of the K-Means algorithm to the weighted case iterates the following steps:
REFERENCES,0.592,"a(i)
‚Üê
arg min
j
‚à•fi ‚àíÀÜfj‚à•
‚àÄi ‚àà{1, 2, . . . , N},
(14)"
REFERENCES,0.5946666666666667,"ÀÜfj
‚Üê
PN
i=1[a(i) = j]wifi
PN
i=1[a(i) = j]wi
‚àÄj ‚àà{1, 2, . . . , K}.,
(15)"
REFERENCES,0.5973333333333334,"Weighted K-Medoids
optimizes objective (13) under the medoid constraint ÀÜF ‚äÇF:"
REFERENCES,0.6,"a(i)
‚Üê
arg min
j
‚à•fi ‚àíÀÜfj‚à•
‚àÄi ‚àà{1, 2, . . . , N},
(16)"
REFERENCES,0.6026666666666667,"n(j)
‚Üê
arg min
i: z(i)=j X"
REFERENCES,0.6053333333333333,"i‚Ä≤: a(i‚Ä≤)=j
‚à•fi ‚àífi‚Ä≤‚à•2
‚àÄj ‚àà{1, 2, . . . , K},
(17)"
REFERENCES,0.608,"ÀÜfj
‚Üê
fn(j).
(18)"
REFERENCES,0.6106666666666667,"B
CLUSTERING INITIALIZATION ABLATIONS"
REFERENCES,0.6133333333333333,"We examine the effect of the cluster center initialization. We compare our default initialization,
which uses the tokens with top-K signiÔ¨Åcance scores as initial cluster centers, with random ini-
tialization, which randomly selects tokens as initial cluster centers. As shown in Figure 6, Token
Pooling is robust to the initialization methods."
REFERENCES,0.616,"1
2
3
4
72 74 76 78 80 82 GÔ¨Çops"
REFERENCES,0.6186666666666667,accuracy (%)
REFERENCES,0.6213333333333333,"K-Means
K-Means, rand cluster init
K-Medoids
K-Medoids, rand cluster init"
REFERENCES,0.624,"Figure 6: Default initialization vs. random initialization. Our Token Pooling is robust to initialization
of clustering algorithms. The default initialization is top-K w.r.t. signiÔ¨Åcance score, see Algorithm 1."
REFERENCES,0.6266666666666667,"C
TRAINING DETAILS & HYPERPARAMETERS"
REFERENCES,0.6293333333333333,"A Token Pooling layer has one parameter‚Äîthe number of the output tokens K (or equivalently the
NEW
downsampling ratio)‚Äîwhich is the same as downsampling layers like max or average pooling. For
max or average pooling layers, the sizes of the kernels and strides play the same role as K. We use
the method proposed by Goyal et al. (2020) to automatically select Ks of all Token Pooling layers
(see below for details). All other hyperparameters are inherited from DeiT (Touvron et al., 2021)."
REFERENCES,0.632,Under review as a conference paper at ICLR 2022
REFERENCES,0.6346666666666667,"To fairly compare PoWER-BERT and the baseline methods with the proposed Token Pooling, all
methods (except convolution downsampling) use the same target number of tokens for downsam-
pling layers after each transformer block. SpeciÔ¨Åcally, we run the second stage of the PoWER-BERT
training (Goyal et al., 2020) for 30 epochs with various values of the token-selection weight parame-
ter Œª producing a family of models. The single parameter Œª controls the overall efÔ¨Åciency of a model,
and the numbers of retained tokens of all Token Pooling layers are selected automatically based on
the choice of Œª. Each of the resulted models has a different number of retained tokens at each of
its L transformer blocks: K = (K1, . . . , KL). Appendix G lists all combinations of automatically
determined K. We then Ô¨Ånetune these models using the third (last) stage of PoWER-BERT. Note
that we apply the same process for PoWER-BERT, random selection, importance selection, and our
Token Pooling, and we use the same K when comparing these methods."
REFERENCES,0.6373333333333333,"We Ô¨Ånd that the DeiT models provided by Touvron et al. (2021) are under-Ô¨Åt, and their accuracy
improves with additional training, see Figure 7. After the standard DeiT training, we restart the
training. This ensures that downsampling models and DeiT with a similar number of training steps."
REFERENCES,0.64,"1
2
3
4
5
72 74 76 78 80 82 GÔ¨Çops"
REFERENCES,0.6426666666666667,accuracy (%)
REFERENCES,0.6453333333333333,"DeiT (Ti ‚ÜíS)
DeiT-Distil (Ti and S)
DeiT-2x (Ti ‚ÜíS)"
REFERENCES,0.648,"Figure 7: This Ô¨Ågure shows the results of the pretrained DeiT models provided by Touvron et al.
(2021) (DeiT) and the DeiT models trained with our protocol (DeiT-2x). Our training protocal uses
the same hyper-parameters provided by Touvron et al. (2021), but after the model is trained, we
Ô¨Ånetune the model using the same hyper-parameters (i.e., restart the learning rate schedule). We
also show DeiT-Distil results (cited from (Touvron et al., 2021)), which use knowledge distillation."
REFERENCES,0.6506666666666666,"D
CONVOLUTION DOWNSAMPLING"
REFERENCES,0.6533333333333333,"As mentioned in the main paper, we enumerate combinations of layers to insert the convolution
downsampling layer. We use 2x2 convolution with stride 2 like in Liu et al. (2021). To keep the
feature dimensionality of DeiT the same (and evaluate the pure effect of the downsampling layers),
the output feature dimensionality is the same as the input. With 196 tokens in DeiT-S model, we can
include no more than 3 convolution downsampling layers as each layer reduces the number of token
by a factor of 4. When using 3 layers at depths l1 < l2 < l3, we restrict l2 ‚àíl1 = l3 ‚àíl2. With these
constraints, we enumerate all possible downsampling conÔ¨Ågurations. Each of the combinations
produces a model with a different computation-accuracy trade-off, and we report the Pareto front,
i.e., the best accuracy these models achieve at a given Ô¨Çop. See Figure 8."
REFERENCES,0.656,"Note that the convolution downsampling layer is equivalent to the patch-merging layer used by Liu
NEW
et al. (2021), Heo et al. (2021), and Wang et al. (2021) (except that we keep the output feature
dimension the same to compare fairly with DeiT). It is also a generalized version of the commonly
used average pooling layer. Average pooling can be implemented by convolving the feature map
with a constant kernel of size n√ón containing
1
n2 and subsample (stride) every n pixels. By learning
its kernel, the convolution downsampling layer is able to Ô¨Ånd a kernel suitable for the downsampling
and the current task than average pooling."
REFERENCES,0.6586666666666666,Under review as a conference paper at ICLR 2022
REFERENCES,0.6613333333333333,"1
1.5
2
2.5
3
3.5
4
4.5
Gflops 70 72 74 76 78 80"
REFERENCES,0.664,accuracy (%)
REFERENCES,0.6666666666666666,Ours (WK-Medoids)
REFERENCES,0.6693333333333333,"Convolution (best)
Convolution"
REFERENCES,0.672,Figure 8: Results of convolution downsampling
REFERENCES,0.6746666666666666,"E
RESULTS WITHOUT FINETUNING"
REFERENCES,0.6773333333333333,"Since Token Pooling minimizes the reconstruction error due to token downsampling, in this section,
we evaluate the performance when we insert Token Pooling layers into a pre-trained model without
Ô¨Ånetuning. Figure 9 shows the results when we directly insert Token Pooling layers (using the
same downsampling ratios in Figure 5b) into a pretrained DeiT-S. As can be seen, minimizing the
reconstruction error, Token Pooling preserves information that enables the model to retain accuracy
during token downsampling."
REFERENCES,0.68,"In Figure 9, we also show the results when we replace tokens to their cluster centers (WK-Means,
carry, no Ô¨Ånetuning and WK-Medoids, carry, no Ô¨Ånetuning). SpeciÔ¨Åcally, in addition to outputting
K cluster centers, we count the number of tokens assigned to a cluster center and carry the count
when we compute softmax-attention in the next transformer blocks. This operation preserves the
attention weights, and since the models are not trained after inserting Token Pooling layers, it pre-
serves the most accuracy. In practice, when the models are trained with Token Pooling layers, the
carry operation is not important."
REFERENCES,0.6826666666666666,"2
2.5
3
3.5
4
4.5
20 40 60 80"
REFERENCES,0.6853333333333333,"¬∑109, Flops"
REFERENCES,0.688,accuracy (%)
REFERENCES,0.6906666666666667,"DeiT-S
K-Means, no Ô¨Ånetuning
WK-Means, no Ô¨Ånetuning
WK-Means, carry, no Ô¨Ånetuning
K-Medoids, no Ô¨Ånetuning
WK-Medoids, no Ô¨Ånetuning
WK-Medoids, carry, no Ô¨Ånetuning"
REFERENCES,0.6933333333333334,"Figure 9: The Ô¨Ågure shows the performance results when we insert Token Pooling layers into a
pretrained DeiT-S without Ô¨Ånetuning the model (i.e., skip step 3 of the training protocol in Section 5)."
REFERENCES,0.696,Under review as a conference paper at ICLR 2022
REFERENCES,0.6986666666666667,"1
2
3
4
5
Gflops 66 68 70 72 74 76 78 80 82"
REFERENCES,0.7013333333333334,accuracy (%)
REFERENCES,0.704,"PoWER-BERT, normalized
Ours (WK-Medoids), normalized"
REFERENCES,0.7066666666666667,(a) Œ± = 1
REFERENCES,0.7093333333333334,"1
2
3
4
5
Gflops 66 68 70 72 74 76 78 80 82"
REFERENCES,0.712,accuracy (%)
REFERENCES,0.7146666666666667,"Ours (WK-Medoids)
Ours (WK-Medoids), normalized
PoWER-BERT
PoWER-BERT, normalized"
REFERENCES,0.7173333333333334,"(b) Learned Œ± for each layer
Figure 10: Results of models using normalized key and query vectors with (a) Œ± = 1 and (b) learned
Œ± in (6). The base model architecture is DeiT-S."
REFERENCES,0.72,"F
NORMALIZED KEY AND QUERY VECTORS"
REFERENCES,0.7226666666666667,"Our analysis of softmax-attention in ¬ß3.3 assumes the query and the key vectors are normalized to
have a constant ‚Ñì2 norm. It has been observed by Kitaev et al. (2020) that normalizing key and query
vectors does not change the performance of a transformer. Thus, for all experiments, we train models
without the norm normalization. In this section, we verify this observation by training various DeiT,
PoWER-BERT, and Token Pooling with normalized keys and queries. We found that the scalar Œ± in
the softmax attention layer (6) can affect the performance of a transformer with normalized keys and
queries. As can be seen in Figure 10a, setting the scalar Œ± = 1 slightly deteriorates the performance
of the model. Instead of using a Ô¨Åxed Œ±, we let the model learn the Œ± for each layer. As can be seen
in Figure 10b, learning Œ± enables the resulting models to achieve similar cost-accuracy trade-off as
the standard (unnormalized) models. With or without the normalization and the learnable Œ±, the
proposed Token Pooling signiÔ¨Åcantly improves the cost-accuracy trade-off of DeiT and outperforms
PoWER-BERT."
REFERENCES,0.7253333333333334,"G
CLUSTERING ALGORITHM ABLATIONS"
REFERENCES,0.728,"Tables 2‚Äì5 detail the results of PoWER-BERT and Token Pooling on the DeiT architectures that
we tested (DeiT-S, DeiT-e318, and DeiT-e252). Figure 11 shows the ablation results with different
backbone models. Table 2 details the results of the best cost-accuracy trade-off achieved by the
proposed Token Pooling (using K-Medoids and WK-Medoids) and PoWER-BERT via varying token
sparsity and feature dimensionality of DeiT."
REFERENCES,0.7306666666666667,"Apart from the standard K-Means and K-Medoids, other clustering approaches could be used. Many
methods are not suitable due to efÔ¨Åciency constraints. For example, normalized cut (Shi & Malik,
2000) uses expensive spectral methods. One prerequisite of using K-Means and K-Medoids is the
number of clusters, K. While selecting K for each of the layers may be tedious and difÔ¨Åcult, one
can choose K via computational budgets and heuristics. In this work, we use the automatic search
procedure proposed by Goyal et al. (2020) to determine K, see Appendix C."
REFERENCES,0.7333333333333333,"Since both K-Means and K-Medoids require specifying the number of clusters K in advance, one
may consider using methods automatically determining K. Nevertheless, such methods typically
have other parameters, which are less interpretable than K. For example, mean-shift (Cheng, 1995)
or quick-shift (Vedaldi & Soatto, 2008) require specifying the kernel size. From our experience,
determining these parameters is challenging. Also, since the number of clusters (and hence the
number of output tokens) is determined on the Ô¨Çy during inference, the computational requirement
can Ô¨Çuctuate, making deployment of these models difÔ¨Åcult."
REFERENCES,0.736,Under review as a conference paper at ICLR 2022
REFERENCES,0.7386666666666667,"0.5
1
1.5
2
Gflops 64 66 68 70 72 74 76 78"
REFERENCES,0.7413333333333333,accuracy (%)
REFERENCES,0.744,"Ours (WK-Medoids)
Ours (K-Medoids)
PoWER-BERT"
REFERENCES,0.7466666666666667,(a) DeiT-e252 as the base architecture
REFERENCES,0.7493333333333333,"1
1.5
2
2.5
3
Gflops 70 72 74 76 78 80"
REFERENCES,0.752,accuracy (%)
REFERENCES,0.7546666666666667,"Ours (WK-Medoids)
Ours (K-Medoids)
PoWER-BERT"
REFERENCES,0.7573333333333333,(b) DeiT-e318 as the base architecture
REFERENCES,0.76,"Figure 11: This Ô¨Ågure compares the cost-accuracy curves of Token Pooling with PoWER-BERT
using (a) DeiT-e252 and (b) DeiT-e318 as the base architectures."
REFERENCES,0.7626666666666667,"DeiT (Ti ‚ÜíS)
PoWER-BERT
Token Pooling (K-Medoids)
Token Pooling (WK-Medoids)
GÔ¨Çops
Accuracy (%)
GÔ¨Çops
Accuracy (%)
GÔ¨Çops
Accuracy (%)
GÔ¨Çops
Accuracy (%)"
REFERENCES,0.7653333333333333,"-
-
0.46
65.0
0.48
68.5
0.48
69.4
-
-
0.55
68.0
0.57
70.9
0.57
71.7
-
-
0.70
71.7
0.73
73.6
0.73
74.0
-
-
0.89
72.0
0.93
74.6
0.93
75.0
-
-
0.96
74.8
1.00
75.9
1.00
76.4
-
-
1.05
75.4
1.11
76.4
1.11
76.6
1.25
73.9
1.19
76.6
1.25
77.0
1.25
77.2
-
-
1.38
77.1
1.46
77.4
1.46
77.6
-
-
1.46
77.4
1.52
78.4
1.52
78.9
-
-
1.62
78.4
1.68
78.9
1.68
79.2
2.07
77.7
1.81
78.8
1.88
79.3
1.88
79.4
-
-
2.11
79.3
2.13
79.9
2.13
80.1
-
-
2.27
79.4
2.35
80.1
2.35
80.4
-
-
2.52
80.0
2.61
80.5
2.61
80.6
3.21
80.0
2.93
80.6
3.04
80.7
3.04
80.7
4.60
81.2
4.28
81.2
4.44
81.4
4.44
81.2"
REFERENCES,0.768,"Table 2: Best cost-accuracy trade-off achieved by PoWER-BERT and the proposed Token Pooling
via varying sparsity level and feature dimensionality."
REFERENCES,0.7706666666666667,Under review as a conference paper at ICLR 2022
REFERENCES,0.7733333333333333,"clustering method
Weighting
ImageNet Accuracy
GFlops
Sparsity level 0: K = [196, 196, 195, 194, 189, 180, 173, 173, 173, 173, 173, 173]"
REFERENCES,0.776,"PoWER-BERT
N/A
81.2
4.3"
REFERENCES,0.7786666666666666,"K-Means
81.3
4.7 (+0.4)
‚úì
81.1
4.7 (+0.4)"
REFERENCES,0.7813333333333333,"K-Medoids
81.4
4.4 (+0.1)
‚úì
81.2
4.4 (+0.1)
Sparsity level 1: K = [196, 195, 193, 188, 169, 140, 121, 110, 73, 38, 7, 0]"
REFERENCES,0.784,"PoWER-BERT
N/A
80.6
2.9"
REFERENCES,0.7866666666666666,"K-Means
80.7
3.3 (+0.4)
‚úì
80.8
3.3 (+0.4)"
REFERENCES,0.7893333333333333,"K-Medoids
80.7
3.0 (+0.1)
‚úì
80.7
3.0 (+0.1)
Sparsity level 2: K = [196, 195, 190, 177, 141, 108, 84, 69, 35, 18, 3, 0]"
REFERENCES,0.792,"PoWER-BERT
N/A
80.0
2.5"
REFERENCES,0.7946666666666666,"K-Means
80.5
2.8 (+0.3)
‚úì
80.5
2.8 (+0.3)"
REFERENCES,0.7973333333333333,"K-Medoids
80.5
2.6 (+0.1)
‚úì
80.6
2.6 (+0.1)
Sparsity level 3: K = [196, 194, 187, 163, 118, 85, 58, 47, 20, 12, 2, 0]"
REFERENCES,0.8,"PoWER-BERT
N/A
79.4
2.3"
REFERENCES,0.8026666666666666,"K-Means
80.1
2.5 (+0.2)
‚úì
80.4
2.5 (+0.2)"
REFERENCES,0.8053333333333333,"K-Medoids
80.1
2.3 (+0.08)
‚úì
80.4
2.3 (+0.08)
Sparsity level 4: K = [196, 193, 179, 142, 97, 64, 46, 34, 13, 9, 1, 0]"
REFERENCES,0.808,"PoWER-BERT
N/A
78.8
2.1"
REFERENCES,0.8106666666666666,"K-Means
79.8
2.3 (+0.2)
‚úì
80.0
2.3 (+0.2)"
REFERENCES,0.8133333333333334,"K-Medoids
79.9
2.1 (+0.07)
‚úì
80.1
2.1 (+0.07)
Sparsity level 5: K = [194, 183, 142, 89, 41, 20, 10, 7, 0, 0, 0, 0]"
REFERENCES,0.816,"PoWER-BERT
N/A
76.2
1.5"
REFERENCES,0.8186666666666667,"K-Means
77.6
1.7 (+0.2)
‚úì
78.1
1.7 (+0.2)"
REFERENCES,0.8213333333333334,"K-Medoids
77.8
1.6 (+0.05)
‚úì
78.1
1.6 (+0.05)
Sparsity level 6: K = [186, 162, 102, 56, 13, 4, 2, 2, 0, 0, 0, 0]"
REFERENCES,0.824,"PoWER-BERT
N/A
73.3
1.2"
REFERENCES,0.8266666666666667,"K-Means
75.0
1.4 (+0.2)
‚úì
75.6
1.4 (+0.2)"
REFERENCES,0.8293333333333334,"K-Medoids
75.4
1.3 (+0.04)
‚úì
75.7
1.3 (+0.04)
Sparsity level 7: K = [162, 129, 66, 33, 4, 1, 1, 0, 0, 0, 0, 0]"
REFERENCES,0.832,"PoWER-BERT
N/A
69.6
1.0"
REFERENCES,0.8346666666666667,"K-Means
72.4
1.1 (+0.1)
‚úì
73.0
1.1 (+0.1)"
REFERENCES,0.8373333333333334,"K-Medoids
72.3
1.0 (+0.03)
‚úì
73.0
1.0 (+0.03)"
REFERENCES,0.84,"Table 3: Results of applying Token Pooling and PoWER-BERT on DeiT-S model. The models are
grouped by K described in Appendix C. The integer list denotes the maximal number of tokens re-
tained after each transformer block. These numbers do not take into account the classiÔ¨Åcation token,
which is always retained. Thus, ‚Äú0‚Äù means that only the classiÔ¨Åcation token remains. Additional
Ô¨Çops (denoted by the parentheses) are due to clustering."
REFERENCES,0.8426666666666667,Under review as a conference paper at ICLR 2022
REFERENCES,0.8453333333333334,"clustering method
Weighting
ImageNet Accuracy
GFlops"
REFERENCES,0.848,"Sparsity level 0: K = [196, 196, 196, 194, 192, 184, 181, 181, 170, 170, 170, 170]"
REFERENCES,0.8506666666666667,"PoWER-BERT
N/A
80.1
3.0"
REFERENCES,0.8533333333333334,"K-Medoids
80.1
3.1 (+0.1)
‚úì
80.0
3.1 (+0.1)"
REFERENCES,0.856,"Sparsity level 1: K = [196, 195, 195, 190, 171, 147, 132, 122, 66, 43, 15, 0]"
REFERENCES,0.8586666666666667,"PoWER-BERT
N/A
79.3
2.1"
REFERENCES,0.8613333333333333,"K-Medoids
79.6
2.2 (+0.1)
‚úì
79.7
2.2 (+0.1)"
REFERENCES,0.864,"Sparsity level 2: K = [196, 195, 193, 180, 143, 109, 92, 77, 40, 21, 4, 0]"
REFERENCES,0.8666666666666667,"PoWER-BERT
N/A
78.8
1.8"
REFERENCES,0.8693333333333333,"K-Medoids
79.3
1.9 (+0.09)
‚úì
79.4
1.9 (+0.09)"
REFERENCES,0.872,"Sparsity level 3: K = [196, 195, 190, 165, 119, 84, 65, 50, 26, 14, 3, 0]"
REFERENCES,0.8746666666666667,"PoWER-BERT
N/A
78.3
1.6"
REFERENCES,0.8773333333333333,"K-Medoids
78.9
1.7 (+0.07)
‚úì
79.2
1.7 (+0.07)"
REFERENCES,0.88,"Sparsity level 4: K = [196, 194, 184, 149, 97, 65, 47, 33, 12, 9, 2, 0]"
REFERENCES,0.8826666666666667,"PoWER-BERT
N/A
77.4
1.5"
REFERENCES,0.8853333333333333,"K-Medoids
78.4
1.5 (+0.06)
‚úì
78.9
1.5 (+0.06)"
REFERENCES,0.888,"Sparsity level 5: K = [196, 186, 153, 93, 40, 15, 12, 9, 0, 0, 0, 0]"
REFERENCES,0.8906666666666667,"PoWER-BERT
N/A
74.5
1.1"
REFERENCES,0.8933333333333333,"K-Medoids
76.2
1.1 (+0.05)
‚úì
76.8
1.1 (+0.05)"
REFERENCES,0.896,"Sparsity level 6: K = [193, 173, 109, 52, 16, 4, 4, 4, 0, 0, 0, 0]"
REFERENCES,0.8986666666666666,"PoWER-BERT
N/A
72.0
0.89"
REFERENCES,0.9013333333333333,"K-Medoids
74.6
0.93 (+0.04)
‚úì
75.0
0.93 (+0.04)"
REFERENCES,0.904,"Sparsity level 7: K = [183, 145, 80, 33, 5, 1, 1, 2, 0, 0, 0, 0]"
REFERENCES,0.9066666666666666,"PoWER-BERT
N/A
69.4
0.75"
REFERENCES,0.9093333333333333,"K-Medoids
72.1
0.78 (+0.03)
‚úì
72.6
0.78 (+0.03)"
REFERENCES,0.912,"Table 4: Results of applying Token Pooling and PoWER-BERT on DeiT-e318 model. The mod-
els are grouped by K described in Appendix C. The integer list denotes the maximal number of
tokens retained after each transformer block. These numbers do not take into account the classiÔ¨Å-
cation token, which is always retained. Thus, ‚Äú0‚Äù means that only the classiÔ¨Åcation token remains.
Additional Ô¨Çops (denoted by the parentheses) are due to clustering."
REFERENCES,0.9146666666666666,Under review as a conference paper at ICLR 2022
REFERENCES,0.9173333333333333,"clustering method
Weighting
ImageNet Accuracy
GFlops"
REFERENCES,0.92,"Sparsity level 0: K = [196, 196, 196, 195, 193, 189, 177, 177, 177, 177, 177, 177]"
REFERENCES,0.9226666666666666,"PoWER-BERT
N/A
78.0
2.0"
REFERENCES,0.9253333333333333,"K-Medoids
77.7
2.1 (+0.1)
‚úì
77.9
2.1 (+0.1)"
REFERENCES,0.928,"Sparsity level 1: K = [196, 196, 195, 189, 176, 161, 123, 122, 76, 48, 17, 0]"
REFERENCES,0.9306666666666666,"PoWER-BERT
N/A
77.1
1.4"
REFERENCES,0.9333333333333333,"K-Medoids
77.4
1.5 (+0.07)
‚úì
77.6
1.5 (+0.07)"
REFERENCES,0.936,"Sparsity level 2: K = [196, 196, 193, 179, 154, 123, 88, 79, 39, 22, 5, 0],"
REFERENCES,0.9386666666666666,"PoWER-BERT
N/A
76.6
1.2"
REFERENCES,0.9413333333333334,"K-Medoids
77.0
1.3 (+0.06)
‚úì
77.2
1.3 (+0.06)"
REFERENCES,0.944,"Sparsity level 3: K = [196, 195, 188, 167, 131, 96, 63, 52, 13, 11, 2, 0],"
REFERENCES,0.9466666666666667,"PoWER-BERT
N/A
75.4
1.1"
REFERENCES,0.9493333333333334,"K-Medoids
76.4
1.1 (+0.05)
‚úì
76.6
1.1 (+0.05)"
REFERENCES,0.952,"Sparsity level 4: K = [196, 194, 181, 147, 111, 75, 48, 36, 5, 7, 2, 0]"
REFERENCES,0.9546666666666667,"PoWER-BERT
N/A
74.8
0.96"
REFERENCES,0.9573333333333334,"K-Medoids
75.9
1.0 (+0.04)
‚úì
76.4
1.0 (+0.04)"
REFERENCES,0.96,"Sparsity level 5: K = [196, 187, 129, 88, 54, 20, 10, 10, 0, 1, 0, 0]"
REFERENCES,0.9626666666666667,"PoWER-BERT
N/A
71.7
0.70"
REFERENCES,0.9653333333333334,"K-Medoids
73.6
0.73 (+0.03)
‚úì
74.0
0.73 (+0.03)"
REFERENCES,0.968,"Sparsity level 6: K = [194, 163, 83, 44, 22, 6, 1, 3, 0, 0, 0, 0]"
REFERENCES,0.9706666666666667,"PoWER-BERT
N/A
68.0
0.55"
REFERENCES,0.9733333333333334,"K-Medoids
70.9
0.57 (+0.02)
‚úì
71.7
0.57 (+0.02)"
REFERENCES,0.976,"Sparsity level 7: K = [188, 122, 58, 28, 12, 2, 0, 2, 0, 0, 0, 0]"
REFERENCES,0.9786666666666667,"PoWER-BERT
N/A
65.0
0.46"
REFERENCES,0.9813333333333333,"K-Medoids
68.5
0.48 (+0.02)
‚úì
69.4
0.48 (+0.02)"
REFERENCES,0.984,"Table 5: Results of applying Token Pooling and PoWER-BERT on DeiT-e252 model. The mod-
els are grouped by K described in Appendix C. The integer list denotes the maximal number of
tokens retained after each transformer block. These numbers do not take into account the classiÔ¨Å-
cation token, which is always retained. Thus, ‚Äú0‚Äù means that only the classiÔ¨Åcation token remains.
Additional Ô¨Çops (denoted by the parentheses) are due to clustering."
REFERENCES,0.9866666666666667,Under review as a conference paper at ICLR 2022
REFERENCES,0.9893333333333333,"H
THROUGHPUT
NEW"
REFERENCES,0.992,"Throughput reÔ¨Çects the actual speed during inference time, and it is often measured by the number
of processed images per second (fps). As a result, the value of throughput highly depends on the
speciÔ¨Åc hardware, implementation quality, and the workload and the state (e.g., temperature) of the
machines. Since it is usually difÔ¨Åcult to control all these factors to have a fair comparison with other
papers, in the main paper, we choose to report the theoretical computational cost (i.e., Ô¨Çops) that is
known to be highly correlated with the energy consumption on device."
REFERENCES,0.9946666666666667,"In Figure 12, we provide the throughput of our models, served as a reference on how our models
actually perform given our implementation and the type of GPU used. Note that our PyTorch imple-
mentation of the clustering algorithms can be signiÔ¨Åcantly improved, for example, via implementing
as a CUDA kernel. To determine the throughput, we run the model inference several times with dif-
ferent batch sizes. We report the average throughput of 30 different runs using the best batch size.
As can be seen, with the same accuracy, using Token Pooling increases the throughput of DeiT-e318
by 25% (1640 vs. 1310 fps), of DeiT-e252 by 22% (2190 vs. 1791 fps), and of DeiT-Ti by 14%
(3220 vs. 2834 fps)."
REFERENCES,0.9973333333333333,"Figure 12: Throughput of our PyTorch implementation. Note that our implementation has not been
optimized for the throughput. The throughput is measured in frames (images) per second (fps).
These numbers are measured on a single Nvidia V100 GPU. As can be seen, despite our non-
optimized code, Token Pooling signiÔ¨Åcantly improves both the Ô¨Çops (see Figure 1b) and throughput
of DeiT models."
