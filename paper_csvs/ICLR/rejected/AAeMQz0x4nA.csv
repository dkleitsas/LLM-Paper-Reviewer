Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0032258064516129032,"Multi-agent joint Q-learning based on Centralized Training with Decentralized
Execution (CTDE) has become an effective technique for multi-agent coopera-
tion. During centralized training, these methods are essentially addressing the
multi-agent credit assignment problem. However, most of the existing methods
implicitly learn the credit assignment just by ensuring that the joint Q-value satis-
Ô¨Åes the Bellman optimality equation. In contrast, we formulate an explicit credit
assignment problem where each agent gives its suggestion about how to weight
individual Q-values to explicitly maximize the joint Q-value, besides guarantee-
ing the Bellman optimality of the joint Q-value. In this way, we can conduct credit
assignment among multiple agents and along the time horizon. Theoretically, we
give a gradient ascent solution for this problem. Empirically, we instantiate the
core idea with deep neural networks and propose Explicit Credit Assignment joint
Q-learning (ECAQ) to facilitate multi-agent cooperation in complex problems.
Extensive experiments justify that ECAQ achieves interpretable credit assignment
and superior performance compared to several advanced baselines."
INTRODUCTION,0.0064516129032258064,"1
INTRODUCTION"
INTRODUCTION,0.00967741935483871,"Many real-world problems such as robot swarm control can be naturally modeled as cooperative
multi-agent systems where each agent can only observe parts of the systems‚Äô state and all agents
share the same global reward. Recently, the IGM-based multi-agent joint Q-learning has become
an effective technique to solve such problems, where IGM (i.e., Individual-Global-Max) means the
consistency between individual and joint greedy action selections."
INTRODUCTION,0.012903225806451613,"During training, most IGM-based methods (Rashid et al., 2018; Sunehag et al., 2018; Wang et al.,
2020b; Yang et al., 2020a;b) are essentially addressing the multi-agent credit assignment problem
as pointed out by Yang et al. (2020a); Zhou et al. (2020). SpeciÔ¨Åcally, they try to learn an assign-
ment function f parameterized by w to align the joint Q-value Qtotal shared by all agents with
the individual Q-values Qi belonging to agent i, i.e., Qtotal = f(Q1, ..., QN; w). Typically, these
methods mainly apply temporal difference learning (TD-learning) to extract the parameter w, which
represents a speciÔ¨Åc credit assignment. However, TD-learning does not explicitly optimize the credit
assignment among multiple agents at a given timestep 1, and the existing methods are often called
the implicit multi-agent credit assignment as mentioned by Zhou et al. (2020); Wang et al. (2020a);
Naderializadeh et al. (2020); Li et al. (2021b;a). Besides, the extracted parameter w is usually lack
of interpretability in terms of multi-agent credit assignment. Finally, the performance may be poor
due to unsuitable credit assignment (Zhou et al., 2020; Yang et al., 2020a)."
INTRODUCTION,0.016129032258064516,"In contrast, conducting explicit multi-agent credit assignment could distribute the global reward to
each agent based on its contribution to the agent group, thus it may substantially facilitate policy
optimization and promote learning performance as pointed out by many previous methods (Proper
& Tumer, 2012; Tumer & Agogino, 2007; Wang et al., 2020c). More importantly, it can Ô¨Ågure
out which agent is critical according to the assigned credits, so as to achieve better interpretability,
which makes up for the defect that deep neural networks are unexplainable. Inspired by these, we
investigate explicit multi-agent credit assignment for the IGM-based joint Q-learning methods."
INTRODUCTION,0.01935483870967742,"1In general, TD-learning is considered to explicitly assign the credit along the time horizon, namely, dis-
tributing the future credit (i.e., the delayed reward) to previous timesteps."
INTRODUCTION,0.02258064516129032,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.025806451612903226,"Our contributions are three-fold. First, we propose a criterion to measure an assignment function so
that we can Ô¨Ånd better assignments by explicitly optimizing this criterion. SpeciÔ¨Åcally, a good as-
signment function should be helpful for agent cooperation to maximize the reward, so we deÔ¨Åne the
criterion as the maximization of Qtotal (i.e., the expected long-term cumulative reward) 2. Second,
we introduce an exact solution to optimize the deÔ¨Åned criterion. Theoretically, our solution can Ô¨Ånd
the optimal Qtotal with mild conditions. Empirically, we approximate the core idea with deep neural
networks and propose Explicit Credit Assignment joint Q-learning (ECAQ) to facilitate multi-agent
cooperation in complex scenarios. Third, we evaluate ECAQ on several challenging tasks. The
results demonstrate that ECAQ achieves interpretable credit assignment and superior performance
compared to advanced baselines."
BACKGROUND,0.02903225806451613,"2
BACKGROUND"
BACKGROUND,0.03225806451612903,"DEC-POMDP.
We consider a fully cooperative multi-agent setting that can be formulated as
DEC-POMDP (Bernstein et al., 2002). It is formally deÔ¨Åned as a tuple ‚ü®N, S, A, T, R, O, Z, Œ≥‚ü©,
where N is the number of agents; S is the set of state; A = A1 √ó ... √ó AN represents the set of
joint action, and Ai is the set of local action that agent i can take; T(s‚Ä≤|s, a) : S √ó A √ó S ‚Üí[0, 1]
represents the state transition function; R : S √ó A ‚ÜíR is the reward function; O = [O1, ..., ON] is
the set of joint observation controlled by the observation function Z : S √ó A ‚ÜíO; and Œ≥ ‚àà[0, 1]
is the discount factor."
BACKGROUND,0.035483870967741936,"In a given state s, each agent i generates an action ai based on its observation oi. The joint action
a = ‚ü®ai, a‚àíi‚ü©results in a new state s‚Ä≤ (i.e., the state of the next timestep) and a global reward r,
where a‚àíi is the joint action of teammates of agent i. The agent aims at learning a policy œÄi(ai|oi)
that can maximize Eoi‚àºZi,ai‚àºœÄi[G] where G is the discount return deÔ¨Åned as G = PH
t=0 Œ≥trt and
H is the time horizon. In partially observable scenarios, the action is typically generated based on
the entire observation-action history œÑi, i.e., œÄi(ai|œÑi), rather than the current observation oi."
BACKGROUND,0.03870967741935484,"Multi-agent Joint Q-learning.
The multi-agent joint Q-learning is a notable approach to solve
DEC-POMDP problems. The idea is to coordinate all agents by the joint Q-value Qjoint(œÑ, a)
where œÑ = ‚ü®œÑi, œÑ‚àíi‚ü©is the joint history of all agents, then the best joint action can be derived
by a‚àó= argmaxa Qjoint(œÑ, a). In practice, the true but unknown joint Q-value Qjoint(œÑ, a)
is approximated by Qtotal(œÑ, a), which in turn is implemented using a deep neural network
Qtotal(œÑ, a; w) parameterized by w. Typically, Qtotal(œÑ, a; w) is optimized by minimizing the
following TD-loss with temporal difference learning (TD-learning):
L(w)
=
E(œÑ,a,r,œÑ ‚Ä≤)‚àºD[(r + Œ≥ max
a‚Ä≤ Qtotal(œÑ ‚Ä≤, a‚Ä≤; w‚àí) ‚àíQtotal(œÑ, a; w))2]
(1)"
BACKGROUND,0.041935483870967745,"where D is the replay buffer containing recent experience tuples (œÑ, a, r, œÑ ‚Ä≤), and Qtotal(œÑ, a; w‚àí)
is the target network whose parameter w‚àíis periodically updated by copying w."
BACKGROUND,0.04516129032258064,"However, vanilla joint Q-learning has some disadvantages. First, the scalability is poor for large-
scale agents because it needs to search the whole joint action space to Ô¨Ånd the optimal one.
Second, the agent cannot interact with the environment based on its own information œÑi, since
the optimal action also relies on the teammates‚Äô information œÑ‚àíi, namely, a‚àó
i
i
‚Üê‚àí‚àí‚àí‚àí
readout
a‚àó="
BACKGROUND,0.04838709677419355,"argmaxa Q‚àó
total(‚ü®œÑi, œÑ‚àíi‚ü©, a; w)."
BACKGROUND,0.05161290322580645,"To remedy these disadvantages, the Centralized Training with Decentralized Execution (CTDE)
paradigm is applied (Lowe et al., 2017; Foerster et al., 2018; Sunehag et al., 2018; Rashid et al.,
2018). During centralized training, agents are granted access to other agents‚Äô information œÑ‚àíi (and
possibly the global state s if available) to estimate the joint Q-value Qtotal(œÑ, a; w) in a stationary
way, while during decentralized execution, the agent makes decision independently based on indi-
vidual Q-value ai = argmaxai Qi(œÑi, ai; Œ∏i) where Œ∏i is the policy parameter of agent i. In order
to achieve effective value-based CTDE, it is critical to ensure the consistency between individual
and joint greedy action selections, which induces the Individual-Global-Max (IGM) principle (Son
et al., 2019):
‚ü®argmaxa1 Q1(œÑ1, a1) , ... , argmaxaN QN(œÑN, aN) ‚ü©= argmaxa Qjoint(œÑ, a)
(2)"
BACKGROUND,0.054838709677419356,"2We notice that several previous methods (Zhou et al., 2020; Wang et al., 2020e) also take the maximization
of Qtotal as a target or measurement for good credit assignment. The differences are discussed in Section 3."
BACKGROUND,0.05806451612903226,Under review as a conference paper at ICLR 2022
BACKGROUND,0.06129032258064516,"The IGM principle is very effective to train large-scale agents because it has a linear (rather than
exponential) search space for the optimal joint action (compared to the vanilla joint Q-learning).
Recently, QTRAN (Son et al., 2019) proposes a sufÔ¨Åcient and necessary condition for IGM:"
BACKGROUND,0.06451612903225806,"Œ£N
i=1Œ±iQi(œÑi, ai) ‚àíQjoint(œÑ, a) + Vjoint(œÑ) =

0
a = [argmaxai Qi(œÑi, ai)]N
i=1
‚â•0
otherwise
(3)"
BACKGROUND,0.06774193548387097,"where Œ±i > 0, and Vjoint(œÑ) = maxa Qjoint(œÑ, a)‚àíŒ£N
i=1Œ±i maxai Qi(œÑi, ai) could be interpreted
as a baseline function to correct for the discrepancy between the optimal joint Q-value and the
weighted summation of the optimal individual Q-values. Note that Equation (2) and (3) are deÔ¨Åned
under a speciÔ¨Åc state (equally, under a speciÔ¨Åc joint history œÑ) because it is hard to satisfy IGM for
all states. Nevertheless, if we could always Ô¨Ånd a corresponding Œ±i satisfying Equation (3) for each
possible œÑ, we say the task itself is factorizable (Son et al., 2019)."
RELATED WORK,0.07096774193548387,"3
RELATED WORK"
RELATED WORK,0.07419354838709677,"IGM-based Credit Assignment.
During centralized training, the IGM-based joint Q-learning
methods are essentially addressing the multi-agent credit assignment problem (Yang et al., 2020a;
Zhou et al., 2020), namely, aligning the joint Q-value Qtotal with individual Q-values Qi:
Qtotal(œÑ, a; w) = f([Qi(œÑi, ai; Œ∏i)]N
i=1; w). The major difference lies in the detailed implemen-
tation of the credit assignment function f. For example, VDN (Sunehag et al., 2018) proposes a
simple additivity assignment function Qtotal(œÑ, a) = Œ£N
i=1Qi(œÑi, ai; Œ∏i), and it works pretty well.
QMIX (Rashid et al., 2018) applies a nonlinear assignment function to increase representation ex-
pressiveness, but with the constraint of monotonic improvement ‚àÄi, ‚àÇQtotal(œÑ,a;w)"
RELATED WORK,0.07741935483870968,"‚àÇQi(œÑi,ai;Œ∏i) ‚â•0 to satisfy the
IGM. Recent methods such as WQMIX (Rashid et al., 2020), QTRAN (Son et al., 2019), QPLEX
(Wang et al., 2020b), Qatten (Yang et al., 2020b) and QPD (Yang et al., 2020a) propose more so-
phisticated assignment functions to enhance the representation expressiveness, e.g., the multi-head
attention function (Yang et al., 2020b) and the duplex dueling function (Wang et al., 2020b)."
RELATED WORK,0.08064516129032258,"Policy-based Credit Assignment.
A well-known method is COMA (Foerster et al., 2018), which
conducts credit assignment by counterfactual baseline. We argue that maximizing Qtotal is one of
the effective ways to learn a good credit assignment function. For example, LICA (Zhou et al., 2020)
and DOP (Wang et al., 2020e) take this as the target or measurement of good credit assignment. The
key differences between our ECAQ and these methods are two-fold: Ô¨Årst, ECAQ is a Q-learning
method, while LICA and DOP are actor-critic methods; second, ECAQ explicitly optimizes a credit
assignment criterion, while LICA and DOP learn the decomposed credit assignment implicitly."
RELATED WORK,0.08387096774193549,"Other Methods.
There are other types of multi-agent credit assignment methods (Proper &
Tumer, 2012; Tumer & Agogino, 2007; Wang et al., 2020c; Zhang et al., 2020; Zhou et al., 2021)
and multi-agent cooperation methods (Nguyen et al., 2018; Zhang et al., 2020; Mahajan et al., 2019;
Wang et al., 2020d). However, they are beyond the scope of this paper. Due to space limitation, we
provide a brief review for these methods in the Appendix."
OUR METHOD,0.08709677419354839,"4
OUR METHOD"
OUR METHOD,0.09032258064516129,"Explicit credit assignment is important for achieving better performance and interpretability, but
most IGM-based methods only implicitly learn the credit assignment function, resulting in non-
interpretable assignment and possibly poor performance (Zhou et al., 2020; Yang et al., 2020a). In
this section, we investigate explicit credit assignment for the IGM-based joint Q-learning. SpeciÔ¨Å-
cally, Section 4.1 deÔ¨Ånes the considered problem; Section 4.2 proposes an exact solution to assign
credit among multiple agents at a given timestep/state; Section 4.3 approximates the exact solution
to handle complex problems; Section 4.4 combines TD-learning with our solution, so the integrated
approach can conduct credit assignment among multiple agents and between different timesteps."
PROBLEM FORMULATION,0.0935483870967742,"4.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.0967741935483871,"In this paper, the true but unknown joint Q-value Qjoint(œÑ, a) is approximated by Qtotal(œÑ, a),
which in turn is implemented using a deep neural network Qtotal(œÑ, a; w) parameterized by w. Us-"
PROBLEM FORMULATION,0.1,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.1032258064516129,"ing these notations, the considered multi-agent credit assignment function is formulated as follows:"
PROBLEM FORMULATION,0.1064516129032258,"Qtotal(œÑ, a) = Œ£N
i=1Œ±i(œÑi)Qi(œÑi, ai; Œ∏i) + b(œÑ) ‚â•Qjoint(œÑ, a)
(4)"
PROBLEM FORMULATION,0.10967741935483871,"where Œ±i(œÑi) > 0 is the weight of Qi, and b(œÑ) := Vjoint(œÑ). We choose this formulation due to two
important reasons. First, it is a sufÔ¨Åcient and necessary condition for IGM under some conditions
as demonstrated by Equation (3), so it has good Ô¨Åtting ability theoretically (Son et al., 2019; Wang
et al., 2020b). Second, it allows us to intuitively interpret the weight Œ±i(œÑi) as the importance of Qi,
which can be analyzed to understand the concrete credit assignment (please see the experiments)."
PROBLEM FORMULATION,0.11290322580645161,"There are inÔ¨Ånite possible assignment solutions satisfying Equation (4). In order to Ô¨Ånd the best one,
we can deÔ¨Åne a criterion to justify how good an assignment function is, then explicitly optimize such
a criterion. We call this kind of methods the explicit multi-agent credit assignment (MACA)."
PROBLEM FORMULATION,0.11612903225806452,"Recall that the ultimate goal of MACA is to boost learning performance, which is measured by the
maximization of Qtotal (i.e., the expected long-term system-level rewards). Thus, we propose an
explicit multi-agent credit assignment criterion as follows:"
PROBLEM FORMULATION,0.11935483870967742,"{Œ±‚àó
i (œÑi), Œ∏‚àó
i }
=
argmax
{Œ±i(œÑi),Œ∏i}
Qtotal(œÑ, a)
(5)"
PROBLEM FORMULATION,0.12258064516129032,"s.t.
Qtotal(œÑ, a)
=
Œ£N
i=1Œ±i(œÑi)Qi(œÑi, ai; Œ∏i) + b(œÑ) and Œ±i(œÑi) > 0 and Œ£N
i=1Œ±i(œÑi) = 1"
PROBLEM FORMULATION,0.12580645161290321,"The additional constraint Œ£N
i=1Œ±i(œÑi) = 1 makes sure that Œ±i(œÑi) is bounded, so we cannot maximize
Qtotal by simply using an inÔ¨Ånite Œ±i(œÑi)."
AN EXACT SOLUTION FOR MACA,0.12903225806451613,"4.2
AN EXACT SOLUTION FOR MACA"
AN EXACT SOLUTION FOR MACA,0.13225806451612904,"In this section, we introduce an exact solution of Equation (5) under the single-state/timestep setting.
This will guide us to design the approximated solution for multi-state problems in Section 4.3. In
the following, we omit unnecessary notations (i.e., œÑ and [œÑi]N
i=1 due to single state) for simplicity."
AN EXACT SOLUTION FOR MACA,0.13548387096774195,"SpeciÔ¨Åcally, Equation (5) is a two-objective (i.e., Œ±‚àó= [Œ±‚àó
i ]N
i=1 and Œ∏‚àó= [Œ∏‚àó
i ]N
i=1) optimization
problem. The Generalized Expectation Maximization (GEM) (Fessler & Hero, 1994) is a popular
technique to solve such problems. We adopt this idea and propose a two-stage optimization method.
At the initial weighting stage, each agent i proposes an initial weighting vector Œ±i = [Œ±1
i , ..., Œ±N
i ]
that assigns Œ±l
i weights (i.e., credits) to Ql to show its preference about how much contribution
agent l has made to the agent team. Here, Œ±i = [Œ±l
i]N
l=1 is agent i‚Äôs estimation of the optimal
Œ±‚àó= [Œ±‚àó
i ]N
i=1. At the optimization stage, each agent i iteratively optimizes its weighting vector Œ±i
and its policy parameter Œ∏i as follows:"
AN EXACT SOLUTION FOR MACA,0.13870967741935483,"Œ±l
i ‚ÜêŒ±l
i + Œ≤1 1"
AN EXACT SOLUTION FOR MACA,0.14193548387096774,"N Œ£N
j=1(Œ±l
j ‚àíŒ±l
i)
(6)"
AN EXACT SOLUTION FOR MACA,0.14516129032258066,"Œ∏i ‚ÜêŒ£N
j=1Œ±j
iŒ∏j + Œ≤2‚àáQiQtotal(a)‚àáŒ∏iQi(ai; Œ∏i)
(7)"
AN EXACT SOLUTION FOR MACA,0.14838709677419354,"where Œ≤1 and Œ≤2 are the learning rates. Equation (6) guarantees that all agents eventually reach
consistent weighting vectors based on the difference of them. Equation (7) makes sure that Qtotal
can be maximized by gradient ascent given the weighting vectors. Interlacing the above updates
could be seen as a kind of GEM algorithms, and the advantages are two-fold (Blondin & Hale,
2020a;b). First, this ‚Äúdecentralized negotiation‚Äù will be more robust to the weighting disagreement
among agents compared to a centralized weighting mechanism. Second, it can guarantee that the
convergent values [Œ±l
i
‚àó, Œ∏‚àó
i ]N
i=1 are optimal under mild assumptions as shown by Proposition 1."
AN EXACT SOLUTION FOR MACA,0.15161290322580645,"Proposition 1. Under assumption that the individual Q-value functions [Qi]N
i=1 were continuously
differentiable and convex, interlacing Equation (6) and (7) enough times will result in convergent
[Œ±l
i
‚àó, Œ∏‚àó
i ]N
i=1 where Œ±l
i
‚àó= Œ±l
j
‚àó= Œ±‚àó
l , and the joint Q-value Qtotal(a) = Œ£N
i=1Œ±‚àó
i Qi(ai; Œ∏‚àó
i ) will
converge to the optimal value Q‚àó
total."
AN EXACT SOLUTION FOR MACA,0.15483870967741936,"Proof. First, Olfati-Saber et al. (2007) have proved that updating Equation (6) enough times will
make all agents reach the same convergent weighting vector, i.e., Œ±l
i = Œ±l
j. Second, assuming all
[Qi]N
i=1 were continuously differentiable and convex (this is a common assumption, and it is true if
the Q-values are linear in ‚Äúfeatures‚Äù as pointed out by Silver et al. (2014)), it is easy to prove that
Qtotal will also be convex given a speciÔ¨Åc weighting vector; therefore, gradient ascent in Equation"
AN EXACT SOLUTION FOR MACA,0.15806451612903225,Under review as a conference paper at ICLR 2022
AN EXACT SOLUTION FOR MACA,0.16129032258064516,"ùëÑùëñ(ùúèùëñ, ùëéùëñ)
ùú∂ùëñ(ùúèùëñ)=[ùõºùëñ"
AN EXACT SOLUTION FOR MACA,0.16451612903225807,"1, ‚Ä¶ , ùõºùëñ ùëÅ] ùëú1"
AN EXACT SOLUTION FOR MACA,0.16774193548387098,"Agent 
Network N
‚Ä¶"
AN EXACT SOLUTION FOR MACA,0.17096774193548386,Consistency
AN EXACT SOLUTION FOR MACA,0.17419354838709677,Network
AN EXACT SOLUTION FOR MACA,0.1774193548387097,"Agent 
Network 1 ùëÑùëÅ ‚Ä¶
‚Ä¶ ùë†
|¬∑| |¬∑|"
AN EXACT SOLUTION FOR MACA,0.18064516129032257,"[ùëÑùëñ(ùùâ, ùëéùëñ)]ùëñ=1"
AN EXACT SOLUTION FOR MACA,0.18387096774193548,"ùëÅ, ùëè(ùùâ)"
AN EXACT SOLUTION FOR MACA,0.1870967741935484,"[ùëÑùëñ(ùùâ, ùëéùëñ)]ùëñ=1"
AN EXACT SOLUTION FOR MACA,0.19032258064516128,"ùëÅ, ùëè(ùùâ)
[ùõºùëñ"
AN EXACT SOLUTION FOR MACA,0.1935483870967742,‚àó(ùúèùëñ)]ùëñ=1 ùëÅ
AN EXACT SOLUTION FOR MACA,0.1967741935483871,"ùëÑùë°ùëúùë°ùëéùëô(ùùâ, ùíÇ) MLP +
GRU +
MLP ùëúùëñ MLP +
MLP"
AN EXACT SOLUTION FOR MACA,0.2,ùú∂ùëñ(ùúèùëñ)=[ùõºùëñ
AN EXACT SOLUTION FOR MACA,0.2032258064516129,"1, ‚Ä¶ , ùõºùëñ ùëÅ] ùõºùëô ‚àó= 1 ùëÅ ùëñ=1 ùëÅ ùõºùëñ ùëô [ùõºùëñ"
AN EXACT SOLUTION FOR MACA,0.2064516129032258,‚àó(ùúèùëñ)]ùëñ=1 ùëÅ
AN EXACT SOLUTION FOR MACA,0.20967741935483872,"[ùëÑùëñ(ùúèùëñ, ùëéùëñ)]ùëñ=1 ùëÅ"
AN EXACT SOLUTION FOR MACA,0.2129032258064516,"ùëúùëñ
ùëúùëñ
VAE MLP GRU"
AN EXACT SOLUTION FOR MACA,0.2161290322580645,"MLP
MLP ‚Ñéùëñ
‚Ñéùëñ ‚Ä≤"
AN EXACT SOLUTION FOR MACA,0.21935483870967742,"(a)
(b)
(c) ùëúùëÅ ùë†"
AN EXACT SOLUTION FOR MACA,0.22258064516129034,"ECA-loss
TD-loss"
AN EXACT SOLUTION FOR MACA,0.22580645161290322,"ùú∂ùëó(ùúèùëó)
KL-loss ùëúùëó"
AN EXACT SOLUTION FOR MACA,0.22903225806451613,L2-loss relu elu ùëä1
AN EXACT SOLUTION FOR MACA,0.23225806451612904,"ùëä2
relu relu"
AN EXACT SOLUTION FOR MACA,0.23548387096774193,Q-value head           Weighting head
AN EXACT SOLUTION FOR MACA,0.23870967741935484,"ùú∂ùëÅ
ùëÑ1
ùú∂1"
AN EXACT SOLUTION FOR MACA,0.24193548387096775,Transformation
AN EXACT SOLUTION FOR MACA,0.24516129032258063,Network
AN EXACT SOLUTION FOR MACA,0.24838709677419354,"Figure 1: (a) The structures of Agent Network and Transformation Network. (b) The overall ECAQ
architecture. (c) The Consistency Network structure."
AN EXACT SOLUTION FOR MACA,0.25161290322580643,"(7) can always Ô¨Ånd the maximum point Q‚àó
total with proper learning rate (since the Ô¨Årst term does
not affect the gradient direction in expectation). Finally, the convergence theory of Expectation
Maximization (Wu, 1983; Xu & Jordan, 1996) tells us that interlacing Equation (6) and (7) will not
affect the Ô¨Ånal convergence properties, so we will eventually Ô¨Ånd the corresponding [Œ±‚àó
i , Œ∏‚àó
i ]N
i=1."
APPROXIMATING THE EXACT SOLUTION,0.25483870967741934,"4.3
APPROXIMATING THE EXACT SOLUTION"
APPROXIMATING THE EXACT SOLUTION,0.25806451612903225,"The real-world problems often consist of multiple states, but the above solution can hardly learn a
set of [Œ±‚àó
i , Œ∏‚àó
i ]N
i=1 that is optimal for all states because the optimal [Œ±‚àó
i , Œ∏‚àó
i ]N
i=1 is state-dependent. In
general, a policy with strong Ô¨Åtting ability can alleviate this problem, so we approximate the exact
solution using deep neural networks (DNN) as the function approximator."
APPROXIMATING THE EXACT SOLUTION,0.26129032258064516,"Overall Design. Our DNN-based method is called ECAQ, which consists of three sub-networks as
shown in Figure 1. The Agent Network Ô¨Årst applies a GRU to encode the local observation oi into
the history œÑi. Then, it uses the Q-value head and the weighting head to generate the individual Q-
value Qi(œÑi, ai; Œ∏i) and the weighting vector Œ±i(œÑi; Œ∏i) = [Œ±1
i , ..., Œ±N
i ], respectively. The weighting
head adopts a Softmax activation function to guarantee Œ£N
l=1Œ±l
i = 1."
APPROXIMATING THE EXACT SOLUTION,0.2645161290322581,"The Consistency Network is proposed to optimize the weighting vector. It takes as input the original
weighting vectors from all agents, and optimizes them to the same converged value [Œ±‚àó
i (œÑi; Œ∏i)]N
i=1
(i.e., mimicking the effect of Equation (6)). The details will be introduced in the following section."
APPROXIMATING THE EXACT SOLUTION,0.267741935483871,"The Transformation Network is proposed to increase ECAQ‚Äôs representation expressiveness. As pre-
vious methods, it uses the joint history œÑ (or the state s if available) to generate a two-layer hypernet-
work (Ha et al., 2017), then transforms individual Q-value [Qi(œÑi, ai; Œ∏i)]N
i=1 to [Qi(œÑ, ai; Œ∏i, w)]N
i=1
using this hypernetwork. Besides, it also generates a baseline function b(œÑ; w)."
APPROXIMATING THE EXACT SOLUTION,0.2709677419354839,"Finally, we form the joint Q-value as Qtotal(œÑ, a; w) = Œ£N
i=1Œ±‚àó
i (œÑi; Œ∏i)Qi(œÑ, ai; Œ∏i, w) + b(œÑ; w).
Afterwards, we can optimize the policy parameter to the converged value [Œ∏‚àó
i ]N
i=1 (i.e., mimicking
the effect of Equation (7)). The details will be introduced in the following section."
APPROXIMATING THE EXACT SOLUTION,0.27419354838709675,"Optimizing the Weighting Vector. As mentioned before, the Consistency Network is proposed to
optimize the weighting vectors [Œ±i]N
i=1 to the same converged value. The main difÔ¨Åculty is how
to apply DNN to achieve this goal. Here, we adopt the variational inference technique (Mao et al.,
2020b) with the following key idea: assuming that the optimal weighting vector is Œ±‚àó= [Œ±‚àó
i ]N
i=1, but
it is unknown; each agent i would like to infer Œ±‚àóbased on oi by p(Œ±‚àó|oi); if all agents‚Äô weighting
vectors [Œ±i]N
i=1 could really converge to the same Œ±‚àó, we would achieve our goal."
APPROXIMATING THE EXACT SOLUTION,0.27741935483870966,"In practice, directly computing p(Œ±‚àó|oi) =
p(oi|Œ±‚àó)p(Œ±‚àó)
R
p(oi|Œ±‚àó)p(Œ±‚àó)dŒ±‚àóis quite difÔ¨Åcult, so we approximate
p(Œ±‚àó|oi) using another tractable distribution q(Œ±‚àó|oi) by minimizing the KL-divergence between"
APPROXIMATING THE EXACT SOLUTION,0.2806451612903226,Under review as a conference paper at ICLR 2022
APPROXIMATING THE EXACT SOLUTION,0.2838709677419355,"them, namely, min KL(q(Œ±‚àó|oi)||p(Œ±‚àó|oi)), which equals to:"
APPROXIMATING THE EXACT SOLUTION,0.2870967741935484,"max Eq(Œ±‚àó|oi) log p(oi|Œ±‚àó) ‚àíKL(q(Œ±‚àó|oi)||p(Œ±‚àó))
(8)"
APPROXIMATING THE EXACT SOLUTION,0.2903225806451613,"Equation (8) can be modeled by a variational autoencoder (VAE), which is the main part of the
Consistency Network. The encoder of this VAE learns a mapping q(Œ±i|oi; Œ∏i) from oi to Œ±i, and the
decoder learns a mapping p(boi|Œ±i; Œ∏i) from Œ±i back to boi. The loss function to train this VAE is:"
APPROXIMATING THE EXACT SOLUTION,0.29354838709677417,"Lvae
i
(Œ∏i) = L2(oi, boi; Œ∏i) + KL(q(Œ±i|oi; Œ∏i)||p(Œ±‚àó))
(9)"
APPROXIMATING THE EXACT SOLUTION,0.2967741935483871,"where the Ô¨Årst term represents the reconstruction error of observations/states, and minimizing this
error makes sure that the weighting vector Œ±i(œÑi; Œ∏i) is state-dependent so as to better handle real-
world problems consisting of multiple states; the second term ensures that the learned distribution
q(Œ±i|oi; Œ∏i) is similar to the true prior distribution p(Œ±‚àó). However, the true prior p(Œ±‚àó) in Equation
(9) is unknown. One could assume that p(Œ±‚àó) follows a unit Gaussian distribution as previous
methods, but it cannot be true for all states. In practice, we Ô¨Ånd that other agents‚Äô weighting vector
q(Œ±j|oj; Œ∏j) is a good surrogate for p(Œ±‚àó), namely, we approximate Equation (9) by:"
APPROXIMATING THE EXACT SOLUTION,0.3,"Lvae
i
(Œ∏i) ‚âàL2(oi, boi; Œ∏i) + 1"
APPROXIMATING THE EXACT SOLUTION,0.3032258064516129,"N Œ£N
j=1KL(q(Œ±i|oi; Œ∏i)||q(Œ±j|oj; Œ∏j))
(10)"
APPROXIMATING THE EXACT SOLUTION,0.3064516129032258,"Provably, Proposition 2 shows that Equation (10) has the same effect as Equation (6) under single-
state setting. Intuitively, this approximation punishes any pair of agents ‚ü®i, j‚ü©with inconsistent
weighting vectors, namely, with a large KL(q(Œ±i|oi; Œ∏i)||q(Œ±j|oj; Œ∏j)). Therefore, it is helpful for
converging to the same weighting vector. In practice, the above optimization cannot be iterated
inÔ¨Ånitely, so we make sure that the weighting vectors are eventually consistent by averaging them
Œ±‚àó
l ‚âà1"
APPROXIMATING THE EXACT SOLUTION,0.3096774193548387,"N Œ£N
i=1Œ±l
i.
Proposition 2. Under single-state setting, Equation (10) has the same effect as Equation (6)."
APPROXIMATING THE EXACT SOLUTION,0.31290322580645163,"Proof. For single-state, some common assumptions hold: 1) there is no need to recover observation,
so the Ô¨Årst term of Equation (10) is removed; 2) q(Œ±i) and q(Œ±j) are Gaussians with equal stan-"
APPROXIMATING THE EXACT SOLUTION,0.3161290322580645,"dard deviation; therefore, minimizing
1
N Œ£N
j=1KL(qi||qj) =
1
N Œ£N
j=1 log œÉj"
APPROXIMATING THE EXACT SOLUTION,0.3193548387096774,"œÉi + œÉ2
i +(¬µi‚àí¬µj)2"
APPROXIMATING THE EXACT SOLUTION,0.3225806451612903,"2œÉ2
j
‚àí1 2 ="
APPROXIMATING THE EXACT SOLUTION,0.3258064516129032,"1
N Œ£N
j=1(¬µi(Œ±i)‚àí¬µj(Œ±j))2 has the same effect as minimizing 1"
APPROXIMATING THE EXACT SOLUTION,0.32903225806451614,"N Œ£N
j=1(Œ±j ‚àíŒ±i) in Equation (6)."
APPROXIMATING THE EXACT SOLUTION,0.33225806451612905,"Optimizing the Policy Parameter. In practice, we share policy parameters among agents as the
previous methods (e.g., VDN, QMIX and QTRAN). Besides accelerating convergence, the special
advantage is that the Ô¨Årst term of Equation (7) can be simpliÔ¨Åed as Œ£N
j=1Œ±j
iŒ∏j = Œ∏i, therefore
Equation (7) can be rewritten as:"
APPROXIMATING THE EXACT SOLUTION,0.33548387096774196,"Œ∏i ‚ÜêŒ∏i + Œ≤2‚àáQiQtotal(œÑ, a; w)‚àáŒ∏iQi(œÑi, ai; Œ∏i)
(11)"
APPROXIMATING THE EXACT SOLUTION,0.3387096774193548,"The loss function of Equation (11) is Leca
i
(Œ∏i) = ‚àíQtotal. We call it Explicit Credit Assignment
loss (ECA-loss) because it explicitly maximizes Qtotal (i.e., the criterion of good credit assignment)."
PUTTING IT ALL TOGETHER,0.3419354838709677,"4.4
PUTTING IT ALL TOGETHER"
PUTTING IT ALL TOGETHER,0.34516129032258064,"The exact solution proposed in Section 4.2 is summarized by Equation (6) and (7). In Section 4.3,
ECAQ adopts the VAE-loss (i.e., Equation (10)) and ECA-loss (i.e., Equation (11)) to approxi-
mate Equation (6) and (7), respectively. Nevertheless, VAE-loss and ECA-loss are mainly used to
optimize the credit assignment among multiple agents at a given timestep/state. For the sequen-
tial decision-making problems consisting of multiple timesteps/states, it is also critical to do credit
assignment along the time horizon (i.e., assigning the delayed reward to previous timesteps). There-
fore, ECAQ also minimizes the following TD-loss:"
PUTTING IT ALL TOGETHER,0.34838709677419355,"Ltd(w) = E(œÑ,a,r,œÑ ‚Ä≤)‚àºD[(r + Œ≥ max
a‚Ä≤ Qtotal(œÑ ‚Ä≤, a‚Ä≤; w‚àí) ‚àíQtotal(œÑ, a; w))2]
(12)"
PUTTING IT ALL TOGETHER,0.35161290322580646,"It ensures the Bellman optimality of Qtotal, namely, Qtotal can approximate the true but unknown
Qjoint very closely. Putting it all together, the total loss to train ECAQ is:"
PUTTING IT ALL TOGETHER,0.3548387096774194,"L(w, Œ∏i) = Ltd(w) + Œ∑(Œ£N
i=1Lvae
i
(Œ∏i) + Œ£N
i=1Leca
i
(Œ∏i))
(13)"
PUTTING IT ALL TOGETHER,0.3580645161290323,"where Œ∑ is the hyperparameter to balance the credit assignment among multiple agents and between
different timesteps. The detailed training algorithm is provided in the Appendix."
PUTTING IT ALL TOGETHER,0.36129032258064514,Under review as a conference paper at ICLR 2022
PUTTING IT ALL TOGETHER,0.36451612903225805,"0
20
40
60
80
100
episode 0 20 40 60 80 100"
PUTTING IT ALL TOGETHER,0.36774193548387096,test win rate (%)
PUTTING IT ALL TOGETHER,0.3709677419354839,"ECAQ (ours)
QTRAN
COMA
QMIX
VDN
IDQN
QPLEX"
PUTTING IT ALL TOGETHER,0.3741935483870968,(a) The easy 2s3z map.
PUTTING IT ALL TOGETHER,0.3774193548387097,"0
20
40
60
80
100
episode 0 20 40 60 80 100"
PUTTING IT ALL TOGETHER,0.38064516129032255,test win rate (%)
PUTTING IT ALL TOGETHER,0.38387096774193546,"ECAQ (ours)
QTRAN
COMA
QMIX
VDN
IDQN
QPLEX"
PUTTING IT ALL TOGETHER,0.3870967741935484,(b) The easy 1c3s5z map.
PUTTING IT ALL TOGETHER,0.3903225806451613,"0
20
40
60
80
100
episode 0 20 40 60 80"
PUTTING IT ALL TOGETHER,0.3935483870967742,test win rate (%)
PUTTING IT ALL TOGETHER,0.3967741935483871,"ECAQ (ours)
QTRAN
COMA
QMIX
VDN
IDQN
QPLEX"
PUTTING IT ALL TOGETHER,0.4,(c) The hard 2c vs 64zg map.
PUTTING IT ALL TOGETHER,0.4032258064516129,"0
20
40
60
80
100
episode 0 10 20 30 40 50 60 70 80"
PUTTING IT ALL TOGETHER,0.4064516129032258,test win rate (%)
PUTTING IT ALL TOGETHER,0.4096774193548387,"ECAQ (ours)
QTRAN
COMA
QMIX
VDN
IDQN
QPLEX"
PUTTING IT ALL TOGETHER,0.4129032258064516,(d) The hard 5m vs 6m map.
PUTTING IT ALL TOGETHER,0.4161290322580645,"0
50
100
150
200
episode 0 5 10 15 20"
PUTTING IT ALL TOGETHER,0.41935483870967744,test win rate (%)
PUTTING IT ALL TOGETHER,0.42258064516129035,"ECAQ (ours)
QTRAN
COMA
QMIX
VDN
IDQN
QPLEX"
PUTTING IT ALL TOGETHER,0.4258064516129032,(e) Super hard 3s5z vs 3s6z.
PUTTING IT ALL TOGETHER,0.4290322580645161,"0
25
50
75
100
125
150
175
200
episode 0 20 40 60 80"
PUTTING IT ALL TOGETHER,0.432258064516129,test win rate (%)
PUTTING IT ALL TOGETHER,0.43548387096774194,"ECAQ (ours)
QTRAN
COMA
QMIX
VDN
IDQN
QPLEX"
PUTTING IT ALL TOGETHER,0.43870967741935485,(f) The super hard MMM2 map.
PUTTING IT ALL TOGETHER,0.44193548387096776,"Figure 2: The test win rate on different StarCraft II maps. ECAQ achieves the best performance on
four maps (i.e., the easy 1c3s5z, the hard 2c vs 64zg, and the super hard 3s5z vs 3s6z and MMM2),
and performs as good as QPLEX on two maps (i.e., the easy 2s3z and the hard 5m vs 6m)."
EXPERIMENT,0.44516129032258067,"5
EXPERIMENT"
EXPERIMENT,0.4483870967741935,"Environment. To guarantee a fair comparison, the decentralized StarCraft II micromanagement
problem (Samvelyan et al., 2019) is used since it is usually considered as the ofÔ¨Åcial testbed for the
IGM-based methods. Besides, previous works show that StarCraft II is suitable for credit assignment
study (Zhou et al., 2020; Yang et al., 2020a; Wang et al., 2020e; Foerster et al., 2018). SpeciÔ¨Åcally,
six maps with different conÔ¨Ågurations (e.g., easy, hard, and super hard settings; homogeneous and
heterogeneous agents) are used to guarantee that ECAQ does not overÔ¨Åt to one speciÔ¨Åc scenario."
EXPERIMENT,0.45161290322580644,"We also evaluate ECAQ on the cooperative navigation problem (Lowe et al., 2017; Mordatch &
Abbeel, 2018), which is a simple yet popular multi-agent environment. SpeciÔ¨Åcally, there are N
agents and N landmarks on a 10-by-10 2D plane. The agents are controlled by our methods, and
they try to cover all landmarks. The observation is the relative positions and velocities of other
agents and landmarks. The action is the velocity of agents. The reward is the negative distance of
any agent to each landmark. We test three scenarios where N = 4, 6 and 10, respectively."
EXPERIMENT,0.45483870967741935,"Baseline. Since we study the explicit credit assignment for the IGM-based joint Q-learning, here we
mainly compare with these IGM-based methods. SpeciÔ¨Åcally, we consider the most relevant VDN
(Sunehag et al., 2018), QMIX (Rashid et al., 2018) and QTRAN (Son et al., 2019). We also adopt
COMA (Foerster et al., 2018) and IDQN (Tampuu et al., 2017). COMA applies a counterfactual
baseline to do credit assignment; in contrast, there is no credit assignment in IDQN. The advanced
methods like QPLEX (Wang et al., 2020b), WQMIX (Rashid et al., 2020), LICA (Zhou et al., 2020),
DOP (Wang et al., 2020e) and Qatten (Yang et al., 2020b) are also reported."
EXPERIMENT,0.45806451612903226,"Implementation. Since most baselines are ofÔ¨Åcially provided in the PyMARL framework 3, we
also use this framework to implement ECAQ and to conduct the experiments. We do not modify any
of the default conÔ¨Ågurations/hyperparameters of PyMARL to guarantee a fair comparison. For the
special hyperparameter Œ∑ of ECAQ, we decrease it from 1.0 to 0.05 gradually as training goes on."
MAIN RESULT,0.4612903225806452,"5.1
MAIN RESULT"
MAIN RESULT,0.4645161290322581,"Result of StarCraft II. The average test win rates of Ô¨Åve independent runs are shown in Figure 2. As
can be observed, ECAQ achieves the best performance on four maps (i.e., the easy 1c3s5z, the hard
2c vs 64zg, and the super hard 3s5z vs 3s6z and MMM2), and performs as good as QPLEX on two
maps (i.e., the easy 2s3z and the hard 5m vs 6m). Notably, the performance of COMA is unstable:
it works well in some scenarios but it is even worse than IDQN in other scenarios. These results
demonstrate that a good credit assignment is very necessary for consistent multi-agent cooperation.
In order to compare ECAQ with advanced methods, we show the results on hard and super hard maps"
MAIN RESULT,0.46774193548387094,3https://github.com/oxwhirl/pymarl.
MAIN RESULT,0.47096774193548385,Under review as a conference paper at ICLR 2022
MAIN RESULT,0.47419354838709676,Table 1: The test win rate on the hard and super hard StarCraft II maps.
MAIN RESULT,0.4774193548387097,"Map
Qatten
QPLEX
WQMIX
LICA
DOP
ECAQ (ours)
hard 2c vs 64zg
66
55
67
68
84
85
hard 5m vs 6m
72
70
60
60
63
73
super hard 3s5z vs 3s6z
17
12
6
0
0
15
super hard MMM2
79
72
23
84
50
80
average test win rate
58.5
52.25
39.0
53.0
49.25
63.25"
MAIN RESULT,0.4806451612903226,"in Table 1. As can be seen, ECAQ‚Äôs performance is better than or as good as many advanced methods
(e.g., Qatten, QPLEX and WQMIX) in speciÔ¨Åc maps, while its average performance is the best,
although ECAQ does not involve advanced DNN architectures like duplex dueling or multi-head
attention critic. This is because ECAQ can directly learn a good credit assignment that maximizes
the long-term rewards, which is highly positive for the test win rate. For example, in the super hard
3s5z vs 3s6z scenario, ECAQ assigns a high credit to ally‚Äôs Zealots because the learned policy is
that ally‚Äôs Zealots hold enemy‚Äôs Zealots and attack enemy‚Äôs Stalkers at the same time."
MAIN RESULT,0.4838709677419355,"Result of Cooperative Navigation. The average rewards
of ten independent runs are shown in Figure 3. It can
be observed that VDN outperforms QMIX in cooperative
navigation, which is in contrast to the results of StarCraft
II where QMIX is better than VDN. This highlights the re-
lationship among method performance, method complex-
ity and task complexity: complex methods do not always
get better performance in simple tasks. Nevertheless, as
can be seen, ECAQ obtains more rewards than other base-
lines in most scenarios. It seems that the complexity of
the evaluated tasks does not inÔ¨Çuence ECAQ too much. -160 -140 -120 -100 -80 -60 -40 -20 0"
MAIN RESULT,0.4870967741935484,"4_vs_4
6_vs_6
10_vs_10"
MAIN RESULT,0.49032258064516127,average rewards
MAIN RESULT,0.4935483870967742,"IDQN
VDN
QMIX
ECAQ (ours)"
MAIN RESULT,0.4967741935483871,"Figure 3: The average rewards in coop-
erative navigation. Lower bar is better."
FURTHER ANALYSIS,0.5,"5.2
FURTHER ANALYSIS"
FURTHER ANALYSIS,0.5032258064516129,"Ablation Study. The average test win rate is shown in Figure 4, where TD, ECA and VAE represent
the three loss functions deÔ¨Åned before (namely, TD+ECA+VAE stands for ECAQ). Surprisingly, it
can be observed that neither TD+ECA nor TD+VAE performs well, and they are somehow worse
than simply applying a single TD-loss. The reason may be that 1) TD+ECA cannot reach consistent
weighting vectors due to the lack of VAE-loss, so the optimization of ECA-loss may be incorrect; 2)
TD+VAE does not optimize the credit assignment at all due to the lack of ECA-loss. Therefore, both
TD+ECA and TD+VAE deteriorate the solution. These results assert a conclusion that both VAE-
loss and ECA-loss are necessary for ECAQ‚Äôs good performance. We also Ô¨Ånd the same conclusion
in the matrix games, and the details are shown in the Appendix."
FURTHER ANALYSIS,0.5064516129032258,"0
20
40
60
80
100
timestpe (x 10,000) 0 20 40 60 80"
FURTHER ANALYSIS,0.5096774193548387,test win rate (%)
FURTHER ANALYSIS,0.5129032258064516,"TD + ECA + VAE
TD + VAE
TD + ECA
TD"
FURTHER ANALYSIS,0.5161290322580645,(a) The hard 2c vs 64zg.
FURTHER ANALYSIS,0.5193548387096775,"0
25
50
75
100
125
150
175
200
timestpe (x 10,000) 0 5 10 15 20"
FURTHER ANALYSIS,0.5225806451612903,test win rate (%)
FURTHER ANALYSIS,0.5258064516129032,"TD + ECA + VAE
TD + VAE
TD + ECA
TD"
FURTHER ANALYSIS,0.5290322580645161,(b) The super hard 3s5z vs 3s6z.
FURTHER ANALYSIS,0.532258064516129,"0
25
50
75
100
125
150
175
200
timestpe (x 10,000) 0 20 40 60 80"
FURTHER ANALYSIS,0.535483870967742,test win rate (%)
FURTHER ANALYSIS,0.5387096774193548,"TD + ECA + VAE
TD + VAE
TD + ECA
TD"
FURTHER ANALYSIS,0.5419354838709678,(c) The super hard MMM2.
FURTHER ANALYSIS,0.5451612903225806,Figure 4: The ablation results for (super) hard scenarios. Other scenarios are shown in the Appendix.
FURTHER ANALYSIS,0.5483870967741935,"Credit Assignment Study. To make a clear analysis, we should make the environment as simple
as possible, so we adopt the matrix games in this study. Firstly, we use the asymmetric monotonic
game G as shown in Table 2(a). There are two agents in G and each agent i has three actions aiA,
aiB, and aiC. The asymmetricity means that different agents have different impact on the payoff.
SpeciÔ¨Åcally, the column agent (i.e., agent 2) has larger impact compared to the row agent (i.e., agent
1) in game G, since the payoff changes by two units in column while by one unit in row. We train
ECAQ through a full exploration conducted over 20,000 steps as QTRAN (Son et al., 2019). We"
FURTHER ANALYSIS,0.5516129032258065,Under review as a conference paper at ICLR 2022
FURTHER ANALYSIS,0.5548387096774193,Table 2: The asymmetric monotonic game and the converged results on the game.
FURTHER ANALYSIS,0.5580645161290323,(a) Payoff of the matrix game G.
FURTHER ANALYSIS,0.5612903225806452,"a1
a2
a2A
a2B
a2C
a1A
7
5
3
a1B
6
4
2
a1C
5
3
1"
FURTHER ANALYSIS,0.5645161290322581,"(b) Q1, Q2 and Qtotal learned by ECAQ under setting #1."
FURTHER ANALYSIS,0.567741935483871,"Q1
Q2
4.828(a2A)
2.107(a2B)
-0.611(a2C)
6.639(a1A)
7.004
5.005
3.007
2.854(a1B)
6.000
4.001
2.003
-0.927(a1C)
4.997
2.998
1.000"
FURTHER ANALYSIS,0.5709677419354838,"(c) The converged values. ECAQ has learned the optimal policy (i.e., a1A and a2A)."
FURTHER ANALYSIS,0.5741935483870968,"Setting
Œ±‚àó
1
Œ±‚àó
2
Q‚àó
1
Q‚àó
2
Q‚àó
total
#1: G
0.26523
0.73477
6.639(a1A)
4.828(a2A)
7.004
#2: GT
0.72562
0.27438
4.755(a1A)
6.435(a2A)
6.982
#3: G ‚àó10
5.83e-08
1.00e+00
55.900(a1A)
33.236(a2A)
63.738"
FURTHER ANALYSIS,0.5774193548387097,Table 3: The symmetric non-monotonic game and the converged results on the game.
FURTHER ANALYSIS,0.5806451612903226,(a) Payoff of the matrix game.
FURTHER ANALYSIS,0.5838709677419355,"a1
a2
a2A
a2B
a2C
a1A
8
-12
-12
a1B
-12
0
0
a1C
-12
0
0"
FURTHER ANALYSIS,0.5870967741935483,(b) The converged values in different settings.
FURTHER ANALYSIS,0.5903225806451613,"Setting
Œ±‚àó
1
Œ±‚àó
2
Q‚àó
total
#1: full exploration
0.55601
0.44399
-3.568
#2: high probability for a1A
1.35e-07
9.99e-01
0.666
#3: high probability for a2A
9.99e-01
9.09e-08
0.151"
FURTHER ANALYSIS,0.5935483870967742,"check how different payoff settings will affect the credit assignment. As shown in Table 2(c), ECAQ
assigns a large weight to agent 2 in game G (i.e., Œ±‚àó
2 ‚âà0.735 in setting #1) because agent 2 has
larger impact on the payoff. In contrast, when we transpose the payoff of G, denoted by GT , the
weight of agent 2 will be small (i.e., Œ±‚àó
2 ‚âà0.274 in setting #2) since agent 2 is less inÔ¨Çuential on the
payoff in GT . We further increase the payoff of G by ten times, denoted by G ‚àó10, and the weight
of agent 2 changes to a very large value (i.e., Œ±‚àó
2 ‚âà1.0 in setting #3). This is because the payoff
changes by twenty units (rather than the original two units) in column, and the absolute impact of
agent 2 becomes much larger. These analyses have demonstrated the good credit assignment ability
of ECAQ. Consequently, ECAQ can easily Ô¨Ånd the optimal decentralized policy (i.e., a1A and a2A)
in all games as observed from Table 2(c). Finally, comparing Table 2(b) with 2(a), it can be seen
that ECAQ Ô¨Åts the payoff matrix very accurately, which asserts the good Ô¨Åtting ability of ECAQ."
FURTHER ANALYSIS,0.5967741935483871,"Secondly, we use the symmetric non-monotonic game as shown in Table 3(a). This game is proposed
by QTRAN (Son et al., 2019). We check how different exploration settings will affect the credit
assignment. As shown in Table 3(b), the full exploration (i.e., setting #1 as QTRAN) will result in
almost random credit assignment values (i.e., Œ±‚àó
i ‚âà0.5). This is expected because the payoff matrix
is symmetric and the exploration is full, so there is no difference between the two agents. In contrast,
when we make one agent more stable (e.g., raising the probability of a1A in setting #2), ECAQ will
assign a large weight to focus on the other agent (e.g., Œ±‚àó
2 ‚âà1.0 in setting #2). The reason is that the
other agent is more random, and it has greater impact on the obtained reward. The results of setting
#3 are similar to these of setting #2, and both settings can Ô¨Ånd the optimal policy as shown in the
Appendix. Overall, these analyses have demonstrated the good credit assignment ability of ECAQ."
CONCLUSION,0.6,"6
CONCLUSION"
CONCLUSION,0.603225806451613,"Multi-agent credit assignment has long been a fundamental issue for multi-agent cooperation. This
paper presented the explicit multi-agent credit assignment for the IGM-based joint Q-learning, which
not only ensures the Bellman optimality of Qtotal to do credit assignment along the time horizon, but
also optimizes a criterion to do credit assignment among different agents explicitly. We instantiate
this idea with deep neural networks and propose ECAQ to facilitate multi-agent cooperation in more
realistic scenarios. Extensive experiments justify the superior performance of ECAQ. Furthermore,
the detailed analyses show that ECAQ has really learned interpretable credit assignment values. To
our best knowledge, the explicit credit assignment is complementary yet novel to the existing IGM-
based studies. We believe that it is basic for building effective learning-based multi-agent systems."
CONCLUSION,0.6064516129032258,Under review as a conference paper at ICLR 2022
REFERENCES,0.6096774193548387,REFERENCES
REFERENCES,0.6129032258064516,"Adrian Agogino and Kagan Turner. Multi-agent reward analysis for learning in noisy domains. In
Proceedings of the fourth international joint conference on Autonomous agents and multiagent
systems, pp. 81‚Äì88, 2005."
REFERENCES,0.6161290322580645,"Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of
decentralized control of markov decision processes. Mathematics of operations research, 27(4):
819‚Äì840, 2002."
REFERENCES,0.6193548387096774,"Maude J Blondin and Matthew Hale. An algorithm for multi-objective multi-agent optimization. In
2020 American Control Conference (ACC), pp. 1489‚Äì1494, 2020a. doi: 10.23919/ACC45564.
2020.9148017."
REFERENCES,0.6225806451612903,"Maude J Blondin and MT Hale. A decentralized multi-objective optimization algorithm. arXiv
preprint arXiv:2010.04781, 2020b."
REFERENCES,0.6258064516129033,"J.A. Fessler and A.O. Hero. Space-alternating generalized expectation-maximization algorithm.
IEEE Transactions on Signal Processing, 42(10):2664‚Äì2677, 1994. doi: 10.1109/78.324732."
REFERENCES,0.6290322580645161,"Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial
Intelligence, pp. 7219‚Äì7226, 2018."
REFERENCES,0.632258064516129,"John J Grefenstette. Lamarckian learning in multi-agent environments. Technical report, NAVY
CENTER FOR APPLIED RESEARCH IN ARTIFICIAL INTELLIGENCE WASHINGTON DC,
1995."
REFERENCES,0.635483870967742,"David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. In Proceedings of the International
Conference on Learning Representations, 2017."
REFERENCES,0.6387096774193548,"Kenneth E. Kinnear (ed.). Advances in Genetic Programming. MIT Press, Cambridge, MA, USA,
1994. ISBN 0262111888."
REFERENCES,0.6419354838709678,"Jiahui Li, Kun Kuang, Baoxiang Wang, Furui Liu, Long Chen, Fei Wu, and Jun Xiao. Shapley
counterfactual credits for multi-agent reinforcement learning. In Feida Zhu, Beng Chin Ooi, and
Chunyan Miao (eds.), KDD ‚Äô21: The 27th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pp. 934‚Äì942. ACM, 2021a. doi:
10.1145/3447548.3467420. URL https://doi.org/10.1145/3447548.3467420."
REFERENCES,0.6451612903225806,"Wenhao Li, Xiangfeng Wang, Bo Jin, Dijun Luo, and Hongyuan Zha. Structured cooperative re-
inforcement learning with time-varying composite action space. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2021b."
REFERENCES,0.6483870967741936,"Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-
agent actor-critic for mixed cooperative-competitive environments. In Advances in neural infor-
mation processing systems, pp. 6379‚Äì6390, 2017."
REFERENCES,0.6516129032258065,"Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson.
MAVEN: multi-
agent variational exploration.
In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence d‚ÄôAlch¬¥e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu-
ral Information Processing Systems 32:
Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
7611‚Äì7622, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
f816dc0acface7498e10496222e9db10-Abstract.html."
REFERENCES,0.6548387096774193,"Patrick Mannion, Sam Devlin, Jim Duggan, and Enda Howley. Multi-agent credit assignment in
stochastic resource management games. The Knowledge Engineering Review, 32, 2017."
REFERENCES,0.6580645161290323,"Hangyu Mao, Zhibo Gong, and Zhen Xiao. Reward design in cooperative multi-agent reinforcement
learning for packet routing. arXiv preprint arXiv:2003.03433, 2020a."
REFERENCES,0.6612903225806451,"Hangyu Mao, Wulong Liu, Jianye Hao, Jun Luo, Dong Li, Zhengchao Zhang, Jun Wang, and Zhen
Xiao. Neighborhood cognition consistent multi-agent reinforcement learning. In Proceedings of
the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 34, pp. 7219‚Äì7226, 2020b."
REFERENCES,0.6645161290322581,Under review as a conference paper at ICLR 2022
REFERENCES,0.667741935483871,"Maja J Mataric. Learning to behave socially. In Third international conference on simulation of
adaptive behavior, volume 617, pp. 453‚Äì462. Citeseer, 1994."
REFERENCES,0.6709677419354839,"Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent
populations. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 32, 2018."
REFERENCES,0.6741935483870968,"Navid Naderializadeh, Fan H Hung, Sean Soleyman, and Deepak Khosla. Graph convolutional value
decomposition in multi-agent reinforcement learning. arXiv preprint arXiv:2010.04740, 2020."
REFERENCES,0.6774193548387096,"Duc Thien Nguyen, Akshat Kumar, and Hoong Chuin Lau. Credit assignment for collective multi-
agent rl with global rewards. In Advances in Neural Information Processing Systems, pp. 8102‚Äì
8113, 2018."
REFERENCES,0.6806451612903226,"Reza Olfati-Saber, J. Alex Fax, and Richard M. Murray. Consensus and cooperation in networked
multi-agent systems. Proceedings of the IEEE, 95(1):215‚Äì233, 2007. doi: 10.1109/JPROC.2006.
887293."
REFERENCES,0.6838709677419355,"Scott Proper and Kagan Tumer.
Modeling difference rewards for multiagent learning.
In Pro-
ceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems -
Volume 3, AAMAS ‚Äô12, pp. 1397‚Äì1398, Richland, SC, 2012. International Foundation for Au-
tonomous Agents and Multiagent Systems. ISBN 0981738133."
REFERENCES,0.6870967741935484,"Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent rein-
forcement learning. In International Conference on Machine Learning, pp. 4292‚Äì4301, 2018."
REFERENCES,0.6903225806451613,"Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding
monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in
Neural Information Processing Systems, 33, 2020."
REFERENCES,0.6935483870967742,"Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas
Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon White-
son. The starcraft multi-agent challenge. In Proceedings of the 18th International Conference on
Autonomous Agents and MultiAgent Systems, pp. 2186‚Äì2188, 2019."
REFERENCES,0.6967741935483871,"Lloyd S. Shapley.
A value for n-person games.
In The Shapley Value, pp. 31‚Äì40. Cambridge
University Press, oct 1988. doi: 10.1017/cbo9780511528446.003. URL https://doi.org/
10.1017%2Fcbo9780511528446.003."
REFERENCES,0.7,"David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International Conference on Machine Learning, pp.
387‚Äì395, 2014."
REFERENCES,0.7032258064516129,"Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning, pp. 5887‚Äì5896, 2019."
REFERENCES,0.7064516129032258,"Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th
International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085‚Äì2087. Inter-
national Foundation for Autonomous Agents and Multiagent Systems, 2018."
REFERENCES,0.7096774193548387,"Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan
Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning.
PloS one, 12(4):e0172395, 2017."
REFERENCES,0.7129032258064516,"Kagan Tumer and Adrian Agogino. Distributed agent-based air trafÔ¨Åc Ô¨Çow management. In Pro-
ceedings of the 6th international joint conference on Autonomous agents and multiagent systems,
pp. 1‚Äì8, 2007."
REFERENCES,0.7161290322580646,"Jianhao Wang, Zhizhou Ren, Beining Han, Jianing Ye, and Chongjie Zhang.
Towards un-
derstanding linear value decomposition in cooperative multi-agent q-learning.
arXiv preprint
arXiv:2006.00587, 2020a."
REFERENCES,0.7193548387096774,Under review as a conference paper at ICLR 2022
REFERENCES,0.7225806451612903,"Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. arXiv preprint arXiv:2008.01062, 2020b."
REFERENCES,0.7258064516129032,"Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yunjie Gu. Shapley q-value: A local reward
approach to solve global reward games. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial
Intelligence, volume 34, pp. 7285‚Äì7292, 2020c."
REFERENCES,0.7290322580645161,"Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. ROMA: Multi-agent reinforcement
learning with emergent roles. In Hal Daum¬¥e III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 9876‚Äì9886. PMLR, 13‚Äì18 Jul 2020d.
URL http://proceedings.mlr.
press/v119/wang20f.html."
REFERENCES,0.7322580645161291,"Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Off-policy
multi-agent decomposed policy gradients. In International Conference on Learning Representa-
tions, 2020e."
REFERENCES,0.7354838709677419,"C. F. Jeff Wu. On the Convergence Properties of the EM Algorithm. The Annals of Statistics, 11(1):
95 ‚Äì 103, 1983. doi: 10.1214/aos/1176346060. URL https://doi.org/10.1214/aos/
1176346060."
REFERENCES,0.7387096774193549,"Lei Xu and Michael I. Jordan. On convergence properties of the em algorithm for gaussian mixtures.
Neural Computation, 8(1):129‚Äì151, 1996. doi: 10.1162/neco.1996.8.1.129."
REFERENCES,0.7419354838709677,"Yaodong Yang, Jianye Hao, Guangyong Chen, Hongyao Tang, Yingfeng Chen, Yujing Hu, Changjie
Fan, and Zhongyu Wei. Q-value path decomposition for deep multiagent reinforcement learning.
arXiv preprint arXiv:2002.03950, 2020a."
REFERENCES,0.7451612903225806,"Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao
Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv
preprint arXiv:2002.03939, 2020b."
REFERENCES,0.7483870967741936,"Tianjun Zhang, Huazhe Xu, Xiaolong Wang, Yi Wu, Kurt Keutzer, Joseph E Gonzalez, and Yuan-
dong Tian.
Multi-agent collaboration via reward attribution decomposition.
arXiv preprint
arXiv:2010.08531, 2020."
REFERENCES,0.7516129032258064,"Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning implicit credit as-
signment for cooperative multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems, 2020."
REFERENCES,0.7548387096774194,"Tianze Zhou, Fubiao Zhang, Kun Shao, Kai Li, Wenhan Huang, Jun Luo, Weixun Wang, Yaodong
Yang, Hangyu Mao, Bin Wang, et al.
Cooperative multi-agent transfer learning with level-
adaptive credit assignment. arXiv preprint arXiv:2106.00517, 2021."
REFERENCES,0.7580645161290323,Under review as a conference paper at ICLR 2022
REFERENCES,0.7612903225806451,"A
THE TRAINING ALGORITHM"
REFERENCES,0.7645161290322581,"ECAQ adopts the Centralized Training with Decentralized Execution (CTDE) paradigm. The train-
ing algorithm for ECAQ is shown in Algorithm 1."
REFERENCES,0.7677419354838709,"Algorithm 1: Training Algorithm for ECAQ
Input: Randomly initialized Œ∏i and w for the policy networks (i.e., individual Q-value functions
Qi(œÑi, ai; Œ∏i)) and the mixing critic (i.e., the joint Q-value function Qtotal(œÑ, a; w)).
Output: Converged individual Q-value Q‚àó
i (œÑi, ai; Œ∏‚àó
i ) for future decentralized execution."
REFERENCES,0.7709677419354839,"1 Qtotal(œÑ, a; w‚àí) ‚ÜêQtotal(œÑ, a; w);"
WHILE NOT TERMINATED DO,0.7741935483870968,2 while not terminated do
WHILE NOT TERMINATED DO,0.7774193548387097,"3
The agents interact with the environment based on Qi(œÑi, ai; Œ∏i), and put the experience
tuples (s, œÑ, a, r, œÑ ‚Ä≤) into replay buffer D;"
WHILE NOT TERMINATED DO,0.7806451612903226,"4
Sample batch size of tuples (s, œÑ, a, r, œÑ ‚Ä≤) from replay buffer D;"
WHILE NOT TERMINATED DO,0.7838709677419354,"5
for each tuple (s, œÑ, a, r, œÑ ‚Ä≤) do"
WHILE NOT TERMINATED DO,0.7870967741935484,"6
Each agent i extracts its observation oi;"
WHILE NOT TERMINATED DO,0.7903225806451613,"7
The Agent Network generates individual Q-values Qi(œÑi, ai; Œ∏i) and weighting vector
Œ±i(œÑi; Œ∏i) = [Œ±1
i , ..., Œ±N
i ] based on oi, and further generates the decoded observation
ÀÜoi based on Œ±i(œÑi);"
"CALCULATE THE VAE-LOSS AS
LVAE
I",0.7935483870967742,"8
Calculate the VAE-loss as
Lvae
i
(Œ∏i) ‚âàL2(oi, boi; Œ∏i) + 1"
"CALCULATE THE VAE-LOSS AS
LVAE
I",0.7967741935483871,"N Œ£N
j=1KL(q(Œ±i|oi; Œ∏i)||q(Œ±j|oj; Œ∏j));"
"CALCULATE THE VAE-LOSS AS
LVAE
I",0.8,"9
The Consistency Network takes as input [Œ±i]N
i=1 and outputs the converged
[Œ±‚àó
i (œÑi; Œ∏i)]N
i=1;"
"CALCULATE THE VAE-LOSS AS
LVAE
I",0.8032258064516129,"10
The Transformation Network takes as input [Qi(œÑi, ai; Œ∏i)]N
i=1 and state s, then
generates [Qi(œÑ, ai; Œ∏i, w)]N
i=1 and b(œÑ; w);"
CALCULATE THE JOINT Q-VALUE AS,0.8064516129032258,"11
Calculate the joint Q-value as
Qtotal(œÑ, a; w) = Œ£N
i=1Œ±‚àó
i (œÑi; Œ∏i)Qi(œÑ, ai; Œ∏i, w) + b(œÑ; w);"
CALCULATE THE JOINT Q-VALUE AS,0.8096774193548387,"12
Calculate the ECA-loss as Leca(w) = ‚àíQtotal(œÑ, a; w);"
CALCULATE THE TD-LOSS AS,0.8129032258064516,"13
Calculate the TD-loss as
Ltd(w) = (r + Œ≥ maxa‚Ä≤ Qtotal(œÑ ‚Ä≤, a‚Ä≤; w‚àí) ‚àíQtotal(œÑ, a; w))2;"
CALCULATE THE TD-LOSS AS,0.8161290322580645,"14
Calculate the total loss as L(w, Œ∏i) = Ltd(w) + uŒ£N
i=1Lvae
i
(Œ∏i) + vŒ£N
i=1Leca
i
(Œ∏i);"
CALCULATE THE TD-LOSS AS,0.8193548387096774,"15
Train the network parameters [Œ∏i]N
i=1 and w by back-propagation based on the total loss;"
END,0.8225806451612904,"16
end"
END,0.8258064516129032,"17
# In practice, the above for iteration is processed in a mini-batch manner;"
IF AT TARGET UPDATE INTERVAL THEN,0.8290322580645161,"18
if at target update interval then"
IF AT TARGET UPDATE INTERVAL THEN,0.832258064516129,"19
Update the target mixing critic by Qtotal(œÑ, a; w‚àí) ‚ÜêQtotal(œÑ, a; w);"
END,0.8354838709677419,"20
end"
END,0.8387096774193549,21 end
END,0.8419354838709677,"B
CREDIT ASSIGNMENT ANALYSIS"
END,0.8451612903225807,"B.1
CREDIT ASSIGNMENT ANALYSIS FOR COOPERATIVE NAVIGATION"
END,0.8483870967741935,"We analyze the training behaviors of the weighting value (i.e., credit assignment value) Œ±i (i =
1, 2, 3) using the 3 vs 3 cooperative navigation. We Ô¨Årst analyze the dynamics of Œ±1 =
1
N Œ£N
i=1Œ±1
i
and agent i‚Äôs estimation Œ±1
i (i = 1, 2, 3), which are shown by the four solid lines in Figure 5(a).
As shown by the black, green and blue lines, although the values are very different at the beginning
of training, different Œ±1
i (i = 1, 2, 3) tend to be consistent as training goes on. We then analyze
whether the converged Œ±i (i = 1, 2, 3) is meaningful. Figure 5(b) shows the learned policies by
the red arrows, and the credit assignment values (which can be estimated from Figure 5(a)) are as
follows : Œ±1 ‚âà0.47 since agent A1 is far from the landmark and its policy has the largest inÔ¨Çuence
on the reward; Œ±2 ‚âà0.35 and Œ±3 ‚âà0.18 since agent A2 and A3 are near the landmark and their
policies have little inÔ¨Çuence on the reward. The results are consistent with these shown in the main
paper: the most inÔ¨Çuential agent usually corresponds to the largest assignment value. These analyses
indicate that the assignment values Œ±i (i = 1, 2, 3) are meaningful to identify critical agents."
END,0.8516129032258064,Under review as a conference paper at ICLR 2022
END,0.8548387096774194,"0
2000
4000
6000
8000
10000
12000
14000
training step 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
END,0.8580645161290322,"alpha_1^1
alpha_2^1
alpha_3^1
alpha_1
alpha_2
alpha_3"
END,0.8612903225806452,"(a) The training dynamic of Œ±i and agent i‚Äôs estimation Œ±1
i of Œ±1. A1 A2 A3"
END,0.864516129032258,(b) Œ±i is meaningful for MACA. Ai represents agent i.
END,0.867741935483871,Figure 5: The credit assignment analysis using the 3 vs 3 cooperative navigation task.
END,0.8709677419354839,Table 4: The payoff matrix of the one-step game and the converged results on the game.
END,0.8741935483870967,(a) Payoff of the matrix game.
END,0.8774193548387097,"a1
a2
a2A
a2B
a2C
a1A
8
-12
-12
a1B
-12
0
0
a1C
-12
0
0"
END,0.8806451612903226,"(b) Q1, Q2 and Qtotal learned by ECAQ under setting #3."
END,0.8838709677419355,"Q1
Q2
-0.014(a2A)
-2.497(a2B)
-3.062(a2C)
1.674(a1A)
0.1509
0.1505
0.1505
-5.407(a1B)
-6.9395
-6.9395
-6.9300
-6.487(a1C)
-8.0108
-8.0108
-8.0109"
END,0.8870967741935484,(c) The converged values for different settings. Note that both #2 and #3 can Ô¨Ånd the optimal policy.
END,0.8903225806451613,"Setting
Œ±‚àó
1
Œ±‚àó
2
Q‚àó
1
Q‚àó
2
Q‚àó
total
#1: fully exploration
0.56
0.44
0.0403(a1C)
0.6296(a2C)
-3.568
#2: high probability for a1A
1.35e-07
9.99e-01
0.4702(a1A)
1.7616(a2A)
0.666
#3: high probability for a2A
9.99e-01
9.09e-08
1.6739(a1A)
-0.0140(a2A)
0.151
#4: training only by TD-loss
9.99e-01
2.58e-06
0.4889(a1A)
-0.8592(a2B)
-0.695"
END,0.8935483870967742,Under review as a conference paper at ICLR 2022
END,0.896774193548387,"B.2
CREDIT ASSIGNMENT ANALYSIS FOR MATRIX GAME"
END,0.9,"We also adopt a matrix game (Son et al., 2019) to check the credit assignment ability of ECAQ.
As shown in Table 4(a), there are two agents and each agent i has three actions aiA, aiB and aiC.
We check how different settings will affect the credit assignment. As shown in Table 4(c), the
fully exploration (i.e., setting #1 as QTRAN (Son et al., 2019)) will result in almost random credit
assignment value (i.e., Œ±‚àó
i ‚âà0.5). This is expected because the payoff matrix is symmetric and the
exploration is full, so there is no difference between the two agents. In contrast, when we make one
agent more stable (e.g., raising the probability of a1A in setting #2), ECAQ will assign large weights
to focus on the other agent (e.g., Œ±‚àó
2 ‚âà1.0 in setting #2). The reason is that the other agent is more
random, and it has greater impact on the obtained reward. The results of setting #3 are similar to
these of setting #2, and both settings can Ô¨Ånd the optimal decentralized policy (i.e., a1A and a2A) as
observed. Comparing the results shown in Table 4(b) with these shown in (Son et al., 2019; Wang
et al., 2020b), it can be seen that ECAQ Ô¨Åts the optimal payoff more accurately than VDN, QMIX
and Qatten. Overall, these analyses have demonstrated the good credit assignment ability and Ô¨Åtting
ability of ECAQ."
END,0.9032258064516129,"C
MORE ABLATION STUDY"
END,0.9064516129032258,"We conduct ablation study on the easy maps (e.g., 2s3z), Ô¨Ånding that there is no signiÔ¨Åcant perfor-
mance difference for different ablation models as shown by Figure 6(a). The reason is that the maps
are too easy, and all methods can get good results. Therefore, we focus on doing ablation study on
the hard and super hard maps. As shown in the main paper, neither TD+ECA nor TD+VAE performs
well, and they are somehow worse than simply applying a single TD-loss in the hard 2c vs 64zg,
super hard 3s5z vs 3s6z and super hard MMM2 maps. However, for the hard 5m vs 6m scenarios,
TD+ECA, TD+VAE and TD have almost similar performance as shown by Figure 6(b), but they are
worse than TD+ECA+VAC (i.e., ECAQ). In the future, we will do more ablation study on different
maps and draw more detailed conclusions when the computing resources are available."
END,0.9096774193548387,"0
20
40
60
80
100
timestpe (x 10,000) 0 20 40 60 80 100"
END,0.9129032258064517,test win rate (%)
END,0.9161290322580645,"TD + ECA + VAE
TD + VAE
TD + ECA
TD"
END,0.9193548387096774,(a) The easy 2s3z map.
END,0.9225806451612903,"0
25
50
75
100
125
150
175
200
timestpe (x 10,000) 0 20 40 60 80"
END,0.9258064516129032,test win rate (%)
END,0.9290322580645162,"TD + ECA + VAE
TD + VAE
TD + ECA
TD"
END,0.932258064516129,(b) The hard 5m vs 6m map.
END,0.9354838709677419,Figure 6: The ablation results on StarCraft II.
END,0.9387096774193548,"We also conduct ablation study on the matrix game shown in Table 4(a). The payoff matrix is
symmetric, so there will be no difference between the two agents under full exploration. Therefore,
we raise the probability of a2A (i.e., setting #3 in Table 4(c)), and ECAQ can Ô¨Ånd the optimal
decentralized policy (i.e., a1A and a2A) as observed. In contrast, if we train ECAQ only by TD-loss
with the same exploration (i.e., setting #4), agent 2 can just Ô¨Ånd a non-optimal policy a2B, which
asserts the necessity of both VAE-loss and ECA-loss."
END,0.9419354838709677,"D
RELATED WORK"
END,0.9451612903225807,"D.1
MULTI-AGENT CREDIT ASSIGNMENT APPROACH"
END,0.9483870967741935,"In this section, we provide a brief review about the multi-agent credit assignment approaches that
are closely related to the proposed ECAQ."
END,0.9516129032258065,Under review as a conference paper at ICLR 2022
END,0.9548387096774194,"Explicit Credit Assignment.
In general, the explicit methods attribute agent contributions that
are at least locally optimal (Kinnear, 1994). A notable approach is to assess an action by calculating
difference reward against a certain reward baseline (Tumer & Agogino, 2007; Agogino & Turner,
2005; Proper & Tumer, 2012). The key idea is that the true contribution of agent i can be approxi-
mated by the difference between rewards induced by a and [ac, a‚àíi], where ac is a counterfactual
action (i.e., not the true action agent i has taken). For example, Agogino & Turner (2005) and
Tumer & Agogino (2007) adopt speciÔ¨Åc ac to calculate the difference reward or CLEAN reward
to do credit assignment. COMA (Foerster et al., 2018) and SQDDPG (Wang et al., 2020c) extends
this idea from reward difference to Q-value difference, and calculates the counterfactual baseline or
Shapley Q-value to do credit assignment. SpeciÔ¨Åcally, COMA (Foerster et al., 2018) uses a cen-
tralized critic to estimate the counterfactual advantage of an action. SQDDPG (Wang et al., 2020c)
applies the Shapley-value framework (Shapley, 1988) to do credit assignment based on an agent‚Äôs
marginal contribution as it is sequentially added to possible agent groups. However, the explicit
mentioned here is different from the explicit mentioned by ECAQ."
END,0.9580645161290322,"The IGM-based Approach. The representative methods are VDN (Sunehag et al., 2018), QMIX
(Rashid et al., 2018), QTRAN (Son et al., 2019), Qatten (Yang et al., 2020b), QPLEX (Wang et al.,
2020b), etc. We have reviewed these methods in the main paper. They are often called the implicit
credit assignment methods as noted by Zhou et al. (2020). In contrast, ECAQ applies explicit credit
assignment training signal to optimize the IGM-based approaches."
END,0.9612903225806452,"The Attribution Approach. The key idea is that split the Ô¨Ånal reward into two kinds of rewards:
self-reward and attributed reward; and the attributed reward is assumed to be redistributed by other
agents based on the agent‚Äôs contribution. Methods following this idea are (Nguyen et al., 2018;
Zhang et al., 2020; Mao et al., 2020a), and they can also be seen as explicit credit assignment."
END,0.964516129032258,"Other Approach. There are many other multi-agent credit assignment methods that can be hardly
classiÔ¨Åed clearly, for example, the implicit credit assignment (Zhou et al., 2020), the social reward
credit assignment (Mataric, 1994) and others (Mannion et al., 2017; Grefenstette, 1995)."
END,0.967741935483871,"D.2
MULTI-AGENT COOPERATION APPROACH"
END,0.9709677419354839,"Please note that we aim at giving a complementary thought for the multi-agent credit assignment
problem rather than beating all methods (with hyperparameter tuning), so we implement ECAQ
based on the basic QMIX instead of the advanced approaches like Qatten (Yang et al., 2020b) and
QPLEX (Wang et al., 2020b). Therefore, we do not intend to give a detailed review for all recent
deep MARL methods. Nevertheless, there are many topics to facilitate multi-agent cooperations,
for example, the role-based methods (Mahajan et al., 2019), the coordinative exploration methods
(Wang et al., 2020d), the graph-based methods (Blondin & Hale, 2020a;b; Mao et al., 2020b) and so
on. However, these methods are beyond the scope of this paper."
END,0.9741935483870968,"E
IMPLEMENTATION DETAILS OF ECAQ"
END,0.9774193548387097,"As mentioned in the main paper, we use the up-to-date PyMARL framework 4 to conduct the ex-
periments. We do not modify any of the default conÔ¨Ågurations of PyMARL to guarantee a fair
comparison. We do not tune the hyperparameters of ECAQ too much: the weights of VAE-loss and
ECA-loss are directly set equal, and they decrease from 1.0 to 0.05 gradually as training goes on; all
of the other hyperparameters (e.g., exploration and activation functions) keep the same as these of
PyMARL. The code is available on the submission system. Therefore, the main experimental results
can be easily reproduced. In addition, we train our methods on the 64-core CPU machine with a
total memory of 128G. The training time is 4 hours to 10 hours for different StarCraft II maps. This
is not too computing heavy for MARL applications."
END,0.9806451612903225,"4https://github.com/oxwhirl/pymarl. The code licensed under the Apache License v2.0, so we can use it
freely for research purpose."
END,0.9838709677419355,Under review as a conference paper at ICLR 2022
END,0.9870967741935484,"F
LIMITATIONS OF ECAQ"
END,0.9903225806451613,"ECAQ is implemented based on deep neural networks, so it faces the ‚Äúblack box problem‚Äù where the
behaviors of individual agents may not be interpretable from the perspective of human. Fortunately,
ECAQ adopts an explicit manner to do multi-agent credit assignment, and the learned weighting
vectors are interpretable to some extend."
END,0.9935483870967742,"ECAQ focuses on the fully cooperative setting, so the goal is set as the maximization of the long-
term shared global reward. As a result, the multi-agent credit assignment (MACA) may raise ethical
issues when the optimal joint actions require sacriÔ¨Åcing certain agents. Certainly, we can deÔ¨Åne the
‚Äúfair-criterion‚Äù for MACA, then optimize this criterion to guarantee fairness between agents, and
this is our future work."
END,0.9967741935483871,"ECAQ is an IGM-based method. The main limitation is that it cannot Ô¨Åt the non-monotonic pay-
off perfectly, as shown in Table 4(b). The reason is that both the Transformation Network and the
weighting vector Œ±i are nonnegative. However, as mentioned before, we aim at giving a comple-
mentary thought for the multi-agent credit assignment problem rather than beating all methods, so
we implement ECAQ by an easy-to-explain joint Q-value function. If we implemented ECAQ based
on more advanced approaches like Qatten, QTRAN++ or QPLEX, it will be much easier to Ô¨Åt the
non-monotonic payoff."
