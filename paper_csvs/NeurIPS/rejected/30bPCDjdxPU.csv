Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00186219739292365,"We show that an arbitrary lower bound of the maximum achievable value can be
1"
ABSTRACT,0.0037243947858473,"used to improve the Bellman value target during value learning. In the tabular case,
2"
ABSTRACT,0.00558659217877095,"value learning using the lower bounded Bellman operator converges to the same
3"
ABSTRACT,0.0074487895716946,"optimal value as using the original Bellman operator, at a potentially faster speed.
4"
ABSTRACT,0.00931098696461825,"In practice, discounted episodic return in episodic tasks and n-step bootstrapped
5"
ABSTRACT,0.0111731843575419,"return in continuing tasks can serve as lower bounds to improve the value target.
6"
ABSTRACT,0.01303538175046555,"We experiment on Atari games, FetchEnv tasks and a challenging physically
7"
ABSTRACT,0.0148975791433892,"simulated car push and reach task. We see large gains in sample efficiency as
8"
ABSTRACT,0.01675977653631285,"well as converged performance over common baselines such as TD3, SAC and
9"
ABSTRACT,0.0186219739292365,"Hindsight Experience Replay (HER) in most tasks, and observe a reliable and
10"
ABSTRACT,0.020484171322160148,"competitive performance against the stronger n-step methods such as td-lambda,
11"
ABSTRACT,0.0223463687150838,"Retrace and optimality tightening. Prior works have already successfully applied a
12"
ABSTRACT,0.024208566108007448,"special case of lower bounding (using episodic return), but are limited to a small
13"
ABSTRACT,0.0260707635009311,"number of episodic tasks. To the best of our knowledge, we are the first to propose
14"
ABSTRACT,0.027932960893854747,"the general method of value target lower bounding (with possibly bootstrapped
15"
ABSTRACT,0.0297951582867784,"return), to demonstrate its optimality in theory, and effectiveness in a wide range
16"
ABSTRACT,0.03165735567970205,"of tasks over many strong baselines.
17"
INTRODUCTION,0.0335195530726257,"1
Introduction
18"
INTRODUCTION,0.035381750465549346,"The value function is a key concept in dynamic programming approaches to Reinforcement Learning
19"
INTRODUCTION,0.037243947858473,"(RL) (Bellman, 1957). It estimates the sum of all future rewards (usually time-discounted) of a given
20"
INTRODUCTION,0.03910614525139665,"state. In temporal difference (TD) learning, the value function is adjusted toward its Bellman target
21"
INTRODUCTION,0.040968342644320296,"which adds the reward of the current step with the (discounted) value of the next state (Sutton &
22"
INTRODUCTION,0.04283054003724395,"Barto, 2018). This forms the basis of many state of the art RL algorithms such as DQN (Mnih et al.,
23"
INTRODUCTION,0.0446927374301676,"2013), DDPG (Lillicrap et al., 2016), TD3 (Fujimoto et al., 2018), and SAC (Haarnoja et al., 2018).
24"
INTRODUCTION,0.04655493482309125,"The value of the next state is typically estimated using a “bootstrapped value” based on the value
25"
INTRODUCTION,0.048417132216014895,"function itself, which is being actively learned during training. The bootstrapped values can be
26"
INTRODUCTION,0.05027932960893855,"random and far from the optimal value, especially at the initial stage of training, or with sparse reward
27"
INTRODUCTION,0.0521415270018622,"tasks where rewards can only be achieved through a long sequence of actions. Consequently, the
28"
INTRODUCTION,0.054003724394785846,"Bellman value targets as well as the learned values are usually far away from the optimal value (the
29"
INTRODUCTION,0.055865921787709494,"value of the optimal policy).
30"
INTRODUCTION,0.05772811918063315,"Naturally, this leads to the following idea: If we can make the value target closer to the optimal value,
31"
INTRODUCTION,0.0595903165735568,"we may speedup TD learning. For example, we know that the optimal value is just the expected
32"
INTRODUCTION,0.061452513966480445,"discounted return of the optimal policy, which always upper bounds the expected return of any policy.
33"
INTRODUCTION,0.0633147113594041,"For episodic RL tasks, we could use the observed discounted return up to episode end from the
34"
INTRODUCTION,0.06517690875232775,"training trajectories to lower bound the value target. This makes the new value target closer to the
35"
INTRODUCTION,0.0670391061452514,"optimal value, when the empirical return is higher than the Bellman target.
36"
INTRODUCTION,0.06890130353817504,Algorithm 1 Value iteration with value target lower bounding
INTRODUCTION,0.07076350093109869,"Input: Finite MDP p(s′, r|s, a), convergence threshold θ, a lower bound f(s) of the maximum
achievable value ¯Gv(s)
Output: State value v(s)
v(s) ←0
repeat"
INTRODUCTION,0.07262569832402235,"∆←0
vp(s) ←v(s)
for each state s do"
INTRODUCTION,0.074487895716946,"ˆv(s) ←maxa
P"
INTRODUCTION,0.07635009310986965,"s′,r p(s′, r|s, a)[r + γvp(s′)]
ˆvf(s) ←max(f(s), ˆv(s))
v(s) ←ˆvf(s)
∆←max(∆, |v(s) −vp(s)|)
end for
until ∆< θ"
INTRODUCTION,0.0782122905027933,"The case for continuing or non-episodic tasks is less clear though. When a continuing task can
37"
INTRODUCTION,0.08007448789571694,"return negative rewards, any safe lower bound of the optimal value can be too low to be useful. One
38"
INTRODUCTION,0.08193668528864059,"could take the risk and use n-step bootstrapped return as a lower bound, which is unsafe because
39"
INTRODUCTION,0.08379888268156424,"bootstrapped return can overestimate and be greater than the optimal value. Can we still use them as
40"
INTRODUCTION,0.0856610800744879,"lower bounds to improve TD value targets?
41"
THEORETICAL RESULTS FOR THE TABULAR CASE,0.08752327746741155,"2
Theoretical Results for the Tabular Case
42"
THEORETICAL RESULTS FOR THE TABULAR CASE,0.0893854748603352,"Our results show that for the tabular case, arbitrary functions below a certain bootstrap bound can be
43"
THEORETICAL RESULTS FOR THE TABULAR CASE,0.09124767225325885,"used to lower bound the value target to still converge to the same optimal value.
44"
BACKGROUND,0.0931098696461825,"2.1
Background
45"
BACKGROUND,0.09497206703910614,"In finite MDPs with a limited number of states and actions, a table can keep track of the value of
46"
BACKGROUND,0.09683426443202979,"each state. Using dynamic programming algorithms such as value iteration, values are guaranteed to
47"
BACKGROUND,0.09869646182495345,"converge to the optimum through Bellman updates (Chapter 4.4 (Sutton & Barto, 2018)).
48"
BACKGROUND,0.1005586592178771,"The core of the value iteration algorithm (Algorithm 1) is the Bellman update of the value function,
49"
BACKGROUND,0.10242085661080075,"B(v), where v(s′) is the bootstrapped value:
50"
BACKGROUND,0.1042830540037244,"B(v)(s) := max
a X"
BACKGROUND,0.10614525139664804,"s′,r
p(s′, r|s, a)[r + γv(s′)]
(1)"
BACKGROUND,0.10800744878957169,"It is well known that the Bellman operator, B, is a contraction mapping over value functions (Denardo,
51"
BACKGROUND,0.10986964618249534,"1967). That is, for any two value functions v1 and v2, ||B(v1) −B(v2)||∞≤γ||v1 −v2||∞for the
52"
BACKGROUND,0.11173184357541899,"discount factor γ ∈[0, 1) and ||x||∞:= maxi |xi| (the L∞norm). This guarantees that any value
53"
BACKGROUND,0.11359404096834265,"function under the algorithm converges to the optimal value B∞(v) = v∗.1
54"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1154562383612663,"2.2
Convergence of value target lower bounding
55"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.11731843575418995,"Definition 2.1. The expected n-step bootstrapped return for a given policy π and value function v(s)
56"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1191806331471136,"is defined as the expected bootstrapped return of taking n steps according to policy π:
57"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.12104283054003724,"Gπ,v
n (s0) := Eπ{r1 + ... + γn−1rn + γnv(sn)}
(2)"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.12290502793296089,"Here, the step rewards ri and the resulting n-th step state sn are random variables, with the expectation
58"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.12476722532588454,"Eπ taken over all possible n-step trajectories under the policy π and the given MDP.
59"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1266294227188082,"1For the gist of the proof, see for example page 8 of https://people.eecs.berkeley.edu/~pabbeel/
cs287-fa09/lecture-notes/lecture5-2pp.pdf"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.12849162011173185,"Definition 2.2. Given the current learned value function v(s), policy class Π, the maximum achievable
60"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1303538175046555,"value of a state s is defined as:
61"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.13221601489757914,"¯Gv(s) :=
max
π∈Π,n∈[1,+∞) Gπ,v
n (s)
(3)"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1340782122905028,"This is a more relaxed definition of maximum because for each state s, a different policy π(s) and a
62"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.13594040968342644,"different number of steps n(s) can be used to achieve the maximum ¯Gv(s). And the theorem below
63"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1378026070763501,"says any function not exceeding the maximum achievable value can be used to lower bound the value
64"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.13966480446927373,"target, and still achieve the optimal value in convergence.
65"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.14152700186219738,"Theorem 2.3. Under the same assumptions for Bellman value contraction, for any function f that
66"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.14338919925512103,"lower bounds the maximum achievable value, i.e. ∀s, f(s) ≤¯Gv(s), if we define the lower bounded
67"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1452513966480447,"Bellman operator as Bf(v) := max(B(v), f), then B∞
f (v) = B∞(v).
68"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.14711359404096835,"Note, the value v(s) and the bootstrapped value can be inaccurate, and even above the optimal value.
69"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.148975791433892,"As a consequence, when n is finite, the maximum achievable value ¯Gv(s) (and f) can be above the
70"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.15083798882681565,"maximum expected return (i.e. the optimal value). On the other hand, when n is sufficiently large,
71"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1527001862197393,"the effect of the bootstrap value v(sn) diminishes (see Equation 2), and the maximum achievable
72"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.15456238361266295,"value becomes the maximum expected return (i.e. the optimal value). Therefore, ∀s, ¯Gv(s) is no
73"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1564245810055866,"smaller than the optimal value B∞(v)(s). As a special case of the theorem, as long as f is below the
74"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.15828677839851024,"optimal value, value target lower bounding converges correctly:
75"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1601489757914339,"Corollary 2.4. If function f lower bounds the optimal value, i.e. ∀s, f(s) ≤B∞(v)(s), then
76"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.16201117318435754,"B∞
f (v) = B∞(v).
77"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.16387337057728119,"A few things to note about the proof of Theorem 2.3 (included in Appendix 1.1).
78"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.16573556797020483,"First, this only proves convergence, not contraction under the original ||v1 −v2||∞metric. In the
79"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.16759776536312848,"case of the Bellman operator, contraction shows that ∀v1, v2 value functions, ||B(v1) −B(v2)||∞≤
80"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.16945996275605213,"γ||v1 −v2||∞. Here, for value target lower bounding, what’s proved is convergence to v∗at a rate
81"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1713221601489758,"of γ, not contraction. There can be counter examples where the distance between v1 and v2 under
82"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.17318435754189945,"one application of Bf can increase in the original L∞metric space, even though v1 and v2 are both
83"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1750465549348231,"getting closer to v∗at a rate of γ. One difficulty caused by convergence instead of contraction is that
84"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.17690875232774675,"the stopping criterion in Algorithm 1 (∆< θ) no longer works, due to the inaccessible v∗during
85"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1787709497206704,"learning. In practice, this may not be a serious concern, as people often train algorithms for a fixed
86"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.18063314711359404,"number of iterations or time steps.
87"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1824953445065177,"Second, based on the proof, the new algorithm is at least as fast as the original. When the lower
88"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.18435754189944134,"bound actually improves the value target, i.e. f(s) > B(v1)(s), there is a chance for the convergence
89"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.186219739292365,"to be faster. Convergence is strictly faster when the lower bound f has an impact on the L∞distance
90"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.18808193668528864,"between the current value and the optimal value, i.e. it increases the value target for the states where
91"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.18994413407821228,"the differences between the current value and the optimal value are the largest.
92"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.19180633147113593,"Third, the lower bound function doesn’t have to be static during training. As long as there is a single
93"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.19366852886405958,"f during each training update, convergence is preserved.
94"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.19553072625698323,"The following sections detail how to compute lower bounds of the maximum achievable value
95"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.1973929236499069,"(Section 3), how to integrate the lower bounds into state of the art RL algorithms (Section 4), and
96"
CONVERGENCE OF VALUE TARGET LOWER BOUNDING,0.19925512104283055,"provide an illustration of how this method may benefit value learning in practice (Section 4.3).
97"
EXAMPLE LOWER BOUND FUNCTIONS,0.2011173184357542,"3
Example Lower Bound Functions
98"
EXAMPLE LOWER BOUND FUNCTIONS,0.20297951582867785,"We show a few cases where lower bound functions can be readily obtained from the training
99"
EXAMPLE LOWER BOUND FUNCTIONS,0.2048417132216015,"experience. Future work may investigate alternatives.
100"
EPISODIC TASKS,0.20670391061452514,"3.1
Episodic tasks
101"
EPISODIC TASKS,0.2085661080074488,"In episodic tasks, discounted return is accumulated up to the last step of an episode. In this case, we
102"
EPISODIC TASKS,0.21042830540037244,"can wait until an episode ends, and compute future discounted returns of all time steps up to the end
103"
EPISODIC TASKS,0.2122905027932961,"of the episode. This episodic return is a lower bound of the optimal value when the environment is
104"
EPISODIC TASKS,0.21415270018621974,"deterministic, because the reward sequence can be repeated using the same sequence of actions2. To
105"
EPISODIC TASKS,0.21601489757914338,"make training efficient, we can compute and store such discounted returns into the replay buffer for
106"
EPISODIC TASKS,0.21787709497206703,"each time step, and simply read them out during training, which adds very little computation to the
107"
EPISODIC TASKS,0.21973929236499068,"baseline one-step TD computation.
108"
EPISODIC TASKS,0.22160148975791433,"f(s0) =
X"
EPISODIC TASKS,0.22346368715083798,"i=0,..,∞
γir(si, ai)
(4)"
EPISODIC TASKS,0.22532588454376165,"We call this variant “lb-DR”, short for lower bounding with discounted return.
109"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2271880819366853,"3.1.1
Episodic with hindsight relabeled goals
110"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.22905027932960895,"In goal conditioned tasks, one helpful technique is hindsight goal relabeling (Andrychowicz et al.,
111"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2309124767225326,"2017). It takes a future state that is d time steps away from the current state as the hindsight / relabeled
112"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.23277467411545624,"goal for the current state. When the goal is reached, a reward of 0 is given, otherwise a -1 reward is
113"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2346368715083799,"given for each time step.
114"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.23649906890130354,"In this case, we know it took d steps to reach the hindsight goal, so the discounted future return is:
115"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2383612662942272,"f(s0) =
X"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.24022346368715083,"i=0,..,d−1
−1γi"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.24208566108007448,"= −1(1 −γd)/(1 −γ)
(5)"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.24394785847299813,"This calculation can be done on the fly as hindsight relabeling happens, requiring no extra space and
116"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.24581005586592178,"very little computation.
117"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.24767225325884543,"We call this variant “lb-GD”, short for lower bounding with goal distance based return.
118"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.24953445065176907,"Additionally, we can also apply lb-DR and lb-GD together, with discounted episodic return (lb-DR)
119"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.25139664804469275,"on the original experience and goal distance based return (lb-GD) on the hindsight experience, giving
120"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2532588454376164,"the “lb-DR+GD” variant, which was used in Fujita et al. (2020).
121"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.25512104283054005,"3.2
In general (including non-episodic tasks)
122"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2569832402234637,"If the task is continuing, without an episode end3, discounted return needs to be accumulated all the
123"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.25884543761638734,"way to infinity. When rewards are always non-negative, one can still use the accumulated discounted
124"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.260707635009311,"reward of the future n-steps to lower bound the value. But accumulated n-step discounted reward is no
125"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.26256983240223464,"longer a lower bound when rewards can be negative, in which case, the more general lower bounding
126"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2644320297951583,"with bootstrapped value can be used: given a trajectory of training experience τ :=< s0, ..., sn >:
127"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.26629422718808193,"Gv
n(τ) := r1 + γr2 + ... + γn−1rn + γnv(sn)
(6)"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2681564245810056,"Assuming the rewards and the state sn can be repeated with the same action sequence, Gv
n(τ) lower
128"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.27001862197392923,"bounds the maximum achievable value ¯Gv(s0) (Equation 3).
129"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2718808193668529,"Two variations are possible: Given a trajectory of length n,
130"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2737430167597765,"1. compute v(si) for all i ∈[1, n] and take the maximum of all Gv
i (τ) to obtain a tighter lower
131"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2756052141527002,"bound. We call this variant “lb-b-nstep”:
132"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2774674115456238,"f(s0) = max
i∈[1,n] Gv
i (τ)
(7)"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.27932960893854747,"2. only evaluate v on the last (nth) step and use the nth-step bootstrapped return as the lower
133"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2811918063314711,"bound, which involves less compute but results in a looser bound. (When n is large enough,
134"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.28305400372439476,"this becomes the lb-DR variant.) We call this variant “lb-b-nstep-only”.
135"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.2849162011173184,"f(s0) = Gv
n(τ)
(8)"
EPISODIC WITH HINDSIGHT RELABELED GOALS,0.28677839851024206,"2Note that the behavior policy can be stochastic, as long as the policy class contains the optimal policy, value
learning will converge to the optimal.
3Chapter 3.3 of Sutton & Barto (2018) has more details on episodic vs continuing tasks."
INTEGRATION INTO RL ALGORITHMS,0.2886405959031657,"4
Integration into RL algorithms
136"
BACKGROUND,0.2905027932960894,"4.1
Background
137"
BACKGROUND,0.29236499068901306,"The value target lower bounds can be readily plugged into RL algorithms that regresses value to a
138"
BACKGROUND,0.2942271880819367,"target, e.g. DQN, DDPG or SAC.
139"
BACKGROUND,0.29608938547486036,"In these algorithms, the action value q(s, a) is learned through a squared loss with the target value y.
140"
BACKGROUND,0.297951582867784,"In one step TD return, for a batch B of experience {s, a →r, s′}, the loss is:
141"
BACKGROUND,0.29981378026070765,"Lq :=
X"
BACKGROUND,0.3016759776536313,"(s,a,r,s′)∈B
|q(s, a) −y|2
(9)"
BACKGROUND,0.30353817504655495,"In one step TD return, y is the one step TD return ˆq(s, a, r, s′):
142"
BACKGROUND,0.3054003724394786,"ˆq(s, a, r, s′) := r(s, a) + γq′(s′, µ′(s′))
(10)"
BACKGROUND,0.30726256983240224,"Here, q′ and µ′ are the bootstrap value and policy functions, typically following the value and policy
143"
BACKGROUND,0.3091247672253259,"functions in a delayed schedule during training. (They are also called “target value” and “target
144"
BACKGROUND,0.31098696461824954,"policy”, and are very different from the “value target” y in this paper.)
145"
VALUE TARGET LOWER BOUNDING,0.3128491620111732,"4.2
Value target lower bounding
146"
VALUE TARGET LOWER BOUNDING,0.31471135940409684,"With lower bounding, we replace the value target y with the lower bounded target:
147"
VALUE TARGET LOWER BOUNDING,0.3165735567970205,"y ←max(f, ˆq(s, a, r, s′)) = max(f, r + γq′(s′, µ′(s′)))
(11)"
VALUE TARGET LOWER BOUNDING,0.31843575418994413,"This way of lower bounding the value target is the same as was done by Fujita et al. (2020) (confirmed
148"
VALUE TARGET LOWER BOUNDING,0.3202979515828678,"via personal communication), but is subtly and importantly different from lower bounding the q
149"
VALUE TARGET LOWER BOUNDING,0.3221601489757914,"value directly (Oh et al., 2018; Tang, 2020): q(s, a) ←max(f, q(s, a)), which stays overestimated if
150"
VALUE TARGET LOWER BOUNDING,0.3240223463687151,"q(s, a) initially overestimates.
151"
VALUE TARGET LOWER BOUNDING,0.3258845437616387,"To the best of our knowledge, value target lower bounding with bootstrapped values is a novel
152"
VALUE TARGET LOWER BOUNDING,0.32774674115456237,"contribution of this work.
153"
AN ILLUSTRATIVE EXAMPLE,0.329608938547486,"4.3
An Illustrative Example
154"
AN ILLUSTRATIVE EXAMPLE,0.33147113594040967,"Figure 1 includes a fairly general example showing how value target lower bounding would improve
155"
AN ILLUSTRATIVE EXAMPLE,0.3333333333333333,"value learning. Suppose we enhance an off policy algorithm such as DDPG with value target lower
156"
AN ILLUSTRATIVE EXAMPLE,0.33519553072625696,"bounding (lb-DR), when there is no training experience hitting the target state, no meaningful training
157"
AN ILLUSTRATIVE EXAMPLE,0.3370577281191806,"happens for the baseline or lb-DR. However, when there is one trajectory hitting the target state, all
158"
AN ILLUSTRATIVE EXAMPLE,0.33891992551210426,"states along the trajectory will soon be propagated with meaningful return, and nearby states will also
159"
AN ILLUSTRATIVE EXAMPLE,0.3407821229050279,"enjoy faster learning. As the state space becomes larger and the time horizon longer, a successful
160"
AN ILLUSTRATIVE EXAMPLE,0.3426443202979516,"trajectory will speed up learning quite a bit.
161"
EXPERIMENTS,0.34450651769087526,"5
Experiments
162"
EXPERIMENTS,0.3463687150837989,"The goal is to demonstrate the sample efficiency of lower bounding the value target over baseline such
163"
EXPERIMENTS,0.34823091247672255,"as DDPG, TD3, SAC and HER. Because the lower bounded value target can now look potentially
164"
EXPERIMENTS,0.3500931098696462,"many steps into the future, we suspect it to be best suited for long horizon, sparse reward tasks.
165"
EXPERIMENTS,0.35195530726256985,"Hence, we choose to experiment on a sampled subset of Atari games, the goal conditioned FetchEnv
166"
EXPERIMENTS,0.3538175046554935,"tasks and the harder goal conditioned Pioneer Push and Reach tasks. See details of the experiment
167"
EXPERIMENTS,0.35567970204841715,"setup in Appendix 1.2.
168"
BASELINES,0.3575418994413408,"5.1
Baselines
169"
BASELINES,0.35940409683426444,"Baselines include DDPG (Lillicrap et al., 2016), TD3 (Fujimoto et al., 2018), SAC (Haarnoja et al.,
170"
BASELINES,0.3612662942271881,"2018) and HER (Andrychowicz et al., 2017; Plappert et al., 2018). Implementations are based on
171 S
T"
BASELINES,0.36312849162011174,"Baseline: 0 S
T"
BASELINES,0.3649906890130354,"Baseline: 1 S
T"
BASELINES,0.36685288640595903,"Baseline: 2 S
T"
BASELINES,0.3687150837988827,"Baseline: 3 S
T"
BASELINES,0.37057728119180633,"Lowerbound 0 T
S"
BASELINES,0.37243947858473,"Lowerbound 1 T
S"
BASELINES,0.3743016759776536,"Lowerbound 2 T
S"
BASELINES,0.3761638733705773,Lowerbound 3
BASELINES,0.3780260707635009,"Figure 1: Illustration of value target lower bounding speeding up value learning as training progresses
from stages 0 to 3. The task is to navigate in the state space from start state S to end state T, with
sparse reward 1 at T and 0 elsewhere. The curve from S to T denotes a training experience that reaches
the target. The shaded areas denote roughly states whose value has been significantly improved
during training up to that stage."
BASELINES,0.37988826815642457,"open sourced repositories, and baseline performance is verified against published results under similar
172"
BASELINES,0.3817504655493482,"settings. The Appendix 1.6 and 1.5 include results on more baselines such as DDQN (van Hasselt
173"
BASELINES,0.38361266294227186,"et al., 2015), td-labmda (Sutton & Barto, 2018) and Retrace (Munos et al., 2016).
174"
HYPERPARAMETERS,0.3854748603351955,"5.2
Hyperparameters
175"
HYPERPARAMETERS,0.38733705772811916,"Value target lower bounding is applied on top of these baselines without any additional hyperparameter
176"
HYPERPARAMETERS,0.3891992551210428,"(Section 4). The only hyperparameters come from the baselines. These hyperparameters follow
177"
HYPERPARAMETERS,0.39106145251396646,"published work as much as possible. When baseline hyperparameters need to be tuned for an
178"
HYPERPARAMETERS,0.3929236499068901,"environment, e.g. Atari games or Pioneer tasks, we search for the best performance in total episode
179"
HYPERPARAMETERS,0.3947858472998138,"reward averaged across all tasks for that environment on one set of random seeds, then the optimal
180"
HYPERPARAMETERS,0.39664804469273746,"hyperparameters are fixed and evaluated on a separate set of random seeds never seen during
181"
HYPERPARAMETERS,0.3985102420856611,"development. Value target lower bounding simply uses the the parameter values optimal for the
182"
HYPERPARAMETERS,0.40037243947858475,"baselines. Details are in Appendix 1.3.
183"
RESULTS,0.4022346368715084,"5.3
Results
184"
RESULTS,0.40409683426443205,"We report results on both episodic and continuing/non-episodic tasks. We report evaluation per-
185"
RESULTS,0.4059590316573557,"formance averaged across several runs of the algorithms (five for the less stable Atari games and
186"
RESULTS,0.40782122905027934,"three for the others). Each run uses a random seed never seen during development. Due to space
187"
RESULTS,0.409683426443203,"constraints, the main paper only reports performance aggregated across all tasks for each environment.
188"
RESULTS,0.41154562383612664,"During each run, we take one task and one random seed, run baseline and treatment algorithms, and
189"
RESULTS,0.4134078212290503,"record whether treatment agent evaluates strictly above the baseline agent as training progresses. We
190"
RESULTS,0.41527001862197394,"average across all the runs of the same environment, and plot the fraction of times where treatment is
191"
RESULTS,0.4171322160148976,"above baseline and the standard deviation of that fraction in Figure 2. Appendix 1.4 contains per task
192"
RESULTS,0.41899441340782123,"evaluation curves.
193"
RESULTS,0.4208566108007449,"Overall, value target lower bounding is a simple, effective, efficient, carefree (no hyperparameter)
194"
RESULTS,0.4227188081936685,"and theoretically justified approach. Although the example lower bounds are limited to deterministic
195"
RESULTS,0.4245810055865922,"environments, the theory is generally applicable to stochastic environments. A similar prior work
196"
RESULTS,0.4264432029795158,"to compare would be Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), which is
197"
RESULTS,0.42830540037243947,"simple, effective, efficient, and also limited to deterministic environments (Blier & Ollivier, 2021).
198"
RESULTS,0.4301675977653631,"However, unlike our work, HER relies on the task being goal conditioned with full knowledge of the
199"
RESULTS,0.43202979515828677,"reward function, has one hyperparameter to tune (the proportion of hindsight experience), and is not
200"
RESULTS,0.4338919925512104,"justified in theory for stochastic environments. Our work shows further significant gains on top of
201"
RESULTS,0.43575418994413406,"HER on hard continuous control tasks.
202"
RESULTS,0.4376163873370577,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
Environment Steps
1e7 0.0 0.2 0.4 0.6 0.8 1.0"
RESULTS,0.43947858472998136,Fraction of random task trials
RESULTS,0.441340782122905,Fraction lb­DR (ours) above sac
RESULTS,0.44320297951582865,(a) Atari (lb-DR)
RESULTS,0.4450651769087523,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
Environment Steps
1e7 0.0 0.2 0.4 0.6 0.8 1.0"
RESULTS,0.44692737430167595,Fraction of random task trials
RESULTS,0.44878957169459965,Fraction lb­b­3step (ours) above sac
RESULTS,0.4506517690875233,(b) Atari (bootstrapped)
RESULTS,0.45251396648044695,"0.0
0.5
1.0
1.5
2.0
Environment Steps
1e6 0.0 0.2 0.4 0.6 0.8 1.0"
RESULTS,0.4543761638733706,Fraction of random task trials
RESULTS,0.45623836126629425,Fraction lb­DR (ours) above ddpg
RESULTS,0.4581005586592179,(c) FetchEnv
RESULTS,0.45996275605214154,"0
1
2
3
4
5
Environment Steps
1e6 0.0 0.2 0.4 0.6 0.8 1.0"
RESULTS,0.4618249534450652,Fraction of random task trials
RESULTS,0.46368715083798884,Fraction lb­DR+GD (ours) above her
RESULTS,0.4655493482309125,(d) Pioneer
RESULTS,0.46741154562383613,"Figure 2: Aggregated evaluation performance: The fraction of times where treatment performs strictly
above baseline, plotted along the number of time steps used for training. The solid curve is the sample
mean of the fraction across all runs, and the shaded area is +/- one standard deviation. We use, for
Atari (lb-DR), 85 runs – 17 games each with 5 seeds, for Atari (bootstrapped), 20 runs – 4 games 5
seeds, for FetchEnv, 9 runs – 3 tasks 3 seeds, and for Pioneer, 6 runs – 2 tasks 3 seeds."
RESULTS,0.4692737430167598,"0
1
2
3
4
5
Training Iterations
1e4 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200"
RESULTS,0.47113594040968343,Average fraction of experience with higher value target
RESULTS,0.4729981378026071,lb­DR (ours) above Bellman target
RESULTS,0.4748603351955307,(a) Atari (lb-DR)
RESULTS,0.4767225325884544,"0
1
2
3
4
5
Training Iterations
1e4 0.0 0.2 0.4 0.6 0.8 1.0"
RESULTS,0.478584729981378,Average fraction of experience with higher value target
RESULTS,0.48044692737430167,lb­b­3step (ours) above Bellman target
RESULTS,0.4823091247672253,(b) Atari (bootstrapped)
RESULTS,0.48417132216014896,"0.0
0.2
0.4
0.6
0.8
1.0
Training Iterations
1e3 0.00 0.01 0.02 0.03 0.04 0.05"
RESULTS,0.4860335195530726,Average fraction of experience with higher value target
RESULTS,0.48789571694599626,lb­DR (ours) above Bellman target
RESULTS,0.4897579143389199,(c) FetchEnv
RESULTS,0.49162011173184356,"0.0
0.5
1.0
1.5
Training Iterations
1e3 0.00 0.02 0.04 0.06 0.08 0.10"
RESULTS,0.4934823091247672,Average fraction of experience with higher value target
RESULTS,0.49534450651769085,lb­DR+GD (ours) above Bellman target
RESULTS,0.4972067039106145,(d) Pioneer
RESULTS,0.49906890130353815,"Figure 3: Average fraction of training experience where lower bounding improves Bellman value
target, plotted along the number of iterations of training. The solid curve is the average of the fraction
across all runs – number of tasks times number of seeds, and the shaded area is +/- one standard
deviation. Other setups are the same as Figure 2."
RESULTS,0.5009310986964618,"5.3.1
Lower bounding vs baselines SAC/DDPG/HER
203"
RESULTS,0.5027932960893855,"Figure 2 compares the lower bounding treatment with SAC, DDPG or HER baseline on 17 sampled
204"
RESULTS,0.5046554934823091,"Atari games, the FetchEnv tasks and the Pioneer tasks. For all the environments, value target lower
205"
RESULTS,0.5065176908752328,"bounding is not only more sample efficient, but also enjoys a higher converged performance. After
206"
RESULTS,0.5083798882681564,"training starts, it quickly gains ground and outperforms the baseline for 70% to 100% of the runs. It
207"
RESULTS,0.5102420856610801,"keeps that advantage even at the end of the training, outperforming baseline in converged performance.
208"
RESULTS,0.5121042830540037,"These plots show how frequently treatment is above baseline, but is insensitive to the magnitude of
209"
RESULTS,0.5139664804469274,"change.
210"
RESULTS,0.515828677839851,"Appendix 1.4 shows the magnitude of change with total episode return plotted for each task, often
211"
RESULTS,0.5176908752327747,"with large gains in sample efficiency and sometimes much higher converged performance. Among all
212"
RESULTS,0.5195530726256983,"the 22 tasks, only one task (Atari Breakout) shows lb-DR underperforming the baseline.
213"
RESULTS,0.521415270018622,"Investigations show that the loss on Atari Breakout is likely due to the mismatch between training
214"
RESULTS,0.5232774674115456,"objective and evaluation metric. During training, raw rewards are clipped to [-1, 1] and step discounted
215"
RESULTS,0.5251396648044693,"at γ = 0.99 to compute value, while in evaluation, total reward is the unclipped and undiscounted
216"
RESULTS,0.527001862197393,"cumulative sum of episode rewards. The discount and clipping together severely penalizes large
217"
RESULTS,0.5288640595903166,"rewards earned later in the episode, which is what’s happening for Breakout, because hitting a top
218"
RESULTS,0.5307262569832403,"layer block produces a reward of 7 while hitting a bottom layer block produces 1. When we use
219"
RESULTS,0.5325884543761639,"non-clipped rewards or a higher γ in training, the lower bounding method performs much better in
220"
RESULTS,0.5344506517690876,"total reward. Note, this train-test discrepancy as well as an additional training bias (Thomas, 2014) is
221"
RESULTS,0.5363128491620112,"likely present in all of the prior works using policy gradient methods on Atari games.
222"
VALUE TARGET IMPROVEMENT,0.5381750465549349,"5.3.2
Value target improvement
223"
VALUE TARGET IMPROVEMENT,0.5400372439478585,"The lb-DR method is mostly effective, but is it really due to improvements to the value targets?
224"
VALUE TARGET IMPROVEMENT,0.5418994413407822,"Figure 3 looks at the fraction of training experience where lower bounding actually improves the
225"
VALUE TARGET IMPROVEMENT,0.5437616387337058,"Bellman value target over the course of training. Overall, improved value target roughly coincides
226"
VALUE TARGET IMPROVEMENT,0.5456238361266295,"with performance gains. Appendix 1.4 shows the plots per each task.
227"
VALUE TARGET IMPROVEMENT,0.547486033519553,"The Appendix also has more results, comparing with baselines such as n-step methods, DDQN and
228"
VALUE TARGET IMPROVEMENT,0.5493482309124768,"optimality tightening, and more analyses such as ablations and robustness to hyperparameter choices.
229"
RELATED WORK,0.5512104283054003,"6
Related Work
230"
RELATED WORK,0.553072625698324,"Prior works (Fujita et al., 2020; Hoppe & Toussaint, 2020; He et al., 2017; Oh et al., 2018; Tang,
231"
RELATED WORK,0.5549348230912476,"2020) employed several different ways of computing future returns and using that as a lower bound
232"
RELATED WORK,0.5567970204841713,"to improve value learning. It is quite easy to introduce biases and inefficiencies into the process and
233"
RELATED WORK,0.5586592178770949,"end up with a suboptimal or inefficient algorithm. Our work is the first to propose the general form of
234"
RELATED WORK,0.5605214152700186,"value target lower bounding (possibly with bootstrapping), to show its convergence to the optimal
235"
RELATED WORK,0.5623836126629422,"value in the tabular case, and to demonstrate its effectiveness in illustrative examples and extensive
236"
RELATED WORK,0.5642458100558659,"experiments on a wide range of tasks.
237"
RELATED WORK,0.5661080074487895,"Fujita et al. (2020)’s method is similar to a special case (the lb-DR+GD variant) of the general
238"
RELATED WORK,0.5679702048417132,"method. They used it as a part of a large system and showed that it improved sample efficiency for a
239"
RELATED WORK,0.5698324022346368,"robotic grasping task. Hoppe & Toussaint (2020) also bounded the value target. But instead of using
240"
RELATED WORK,0.5716945996275605,"empirical return, they used a simplified MDP with a subset of actions. Although without theoretical
241"
RELATED WORK,0.5735567970204841,"proof and only experimented on a limited set of robotic manipulation tasks, both works show that
242"
RELATED WORK,0.5754189944134078,"value target lower bounding increased sample efficiency. This work, in addition to the theory and the
243"
RELATED WORK,0.5772811918063314,"more general method, shows that lower bounding improves both sample efficiency and converged
244"
RELATED WORK,0.5791433891992551,"performance in a wide range of tasks.
245"
RELATED WORK,0.5810055865921788,"He et al. (2017) used empirical return with bootstrap to improve value learning. They formulated value
246"
RELATED WORK,0.5828677839851024,"learning as a constrained optimization problem with the empirical bootstrapped value being the lower
247"
RELATED WORK,0.5847299813780261,"(and upper) constraints of the value function. In their experiments, the Lagrangian multiplier was
248"
RELATED WORK,0.5865921787709497,"fixed, which would likely lead to suboptimal solutions. Our lb-b-nstep method also uses bootstrapped
249"
RELATED WORK,0.5884543761638734,"value. But we lower bound the value target directly, which is simpler, more efficient, and likely more
250"
RELATED WORK,0.590316573556797,"optimal. Our work points out that for episodic tasks, even more efficient and effective methods like
251"
RELATED WORK,0.5921787709497207,"lb-DR exist. Appendix 1.5 offers more discussion and results related to this.
252"
RELATED WORK,0.5940409683426443,"Our work is subtly but importantly different from the prior works on lower bound Q learning or Self
253"
RELATED WORK,0.595903165735568,"Imitation Learning (SIL) (Oh et al., 2018; Tang, 2020). SIL uses empirical return R to lower bound
254"
RELATED WORK,0.5977653631284916,"the value function itself (instead of the value target). This is achieved by adding an off policy value
255"
RELATED WORK,0.5996275605214153,"loss during on-policy (AC or PPO) training (Lsil
value = 1"
RELATED WORK,0.6014897579143389,"2|v(s) −max(v(s), R)|2). When the value
256"
RELATED WORK,0.6033519553072626,"function overestimates, the SIL value loss becomes zero, and keeps overestimating. Mixing the SIL
257"
RELATED WORK,0.6052141527001862,"loss with the loss from the baseline algorithms probably helped to correct the overestimation, but no
258"
RELATED WORK,0.6070763500931099,"theoretical guarantee was given. In evaluation, SIL was often compared to on-policy Actor Critic or
259"
RELATED WORK,0.6089385474860335,"PPO baselines, so it was not clear how much of the gain was due to lower bounding and how much
260"
RELATED WORK,0.6108007448789572,"due to off-policy value learning. In this work, we bound the Bellman value target (Equation 11), so
261"
RELATED WORK,0.6126629422718808,"overestimates are automatically corrected via Bellman updates, and convergence is guaranteed in the
262"
RELATED WORK,0.6145251396648045,"tabular case. We also use off-policy algorithms as baselines for a cleaner comparison.
263"
RELATED WORK,0.6163873370577281,"N-step return methods such as td-lambda (Sutton & Barto, 2018) and Retrace (Munos et al., 2016)
264"
RELATED WORK,0.6182495344506518,"also look a few steps ahead, but to obtain more accurate value of the behavior policy. Traditionally,
265"
RELATED WORK,0.6201117318435754,"this requires careful off-policy correction, and the value can still be far from the optimal value due
266"
RELATED WORK,0.6219739292364991,"to the often suboptimal behavior. This work shows that value target lower bounding efficiently and
267"
RELATED WORK,0.6238361266294227,"effectively looks ahead much further without the need for off-policy correction, due to aiming at the
268"
RELATED WORK,0.6256983240223464,"optimal value. Appendix 1.5 has more detailed observations and discussions.
269"
RELATED WORK,0.62756052141527,"Planning methods can look into the future to achieve higher value targets and better control. Examples
270"
RELATED WORK,0.6294227188081937,"include Monte Carlo Tree Search (MCTS) (Schrittwieser et al., 2019; Ye et al., 2021) and Model
271"
RELATED WORK,0.6312849162011173,"Predictive Control (MPC) or receding horizon planning with raw actions (Chua et al., 2018; Hafner
272"
RELATED WORK,0.633147113594041,"et al., 2019; Zhang et al., 2022), options (Silver & Ciosek, 2012), or subgoals (Nasiriany et al.,
273"
RELATED WORK,0.6350093109869647,"2019; Nair & Finn, 2020; Chane-Sane et al., 2021). Planning methods use either a dynamics model
274"
RELATED WORK,0.6368715083798883,"together with the learned value or just the learned value (in the case of goal conditioned tasks)
275"
RELATED WORK,0.638733705772812,"(Nasiriany et al., 2019) to improve policy or value estimates. Planning typically happens during roll
276"
RELATED WORK,0.6405959031657356,"out (Nasiriany et al., 2019), but can also be used to improve the value target, as in Reanalyze of
277"
RELATED WORK,0.6424581005586593,"MuZero (Schrittwieser et al., 2019; Ye et al., 2021). During value improvement, if planning takes
278"
RELATED WORK,0.6443202979515829,"the maximum over a set of possible future values (e.g. from different trajectories as in the case
279"
RELATED WORK,0.6461824953445066,"of MPC), and if this set includes the one step Bellman value target, then the planner is essentially
280"
RELATED WORK,0.6480446927374302,"using alternative trajectories and their values to lower bound the Bellman value target. In this sense,
281"
RELATED WORK,0.6499068901303539,"the theory developed here can potentially justify and improve Reanalyze. In general, planning is
282"
RELATED WORK,0.6517690875232774,"orthogonal to value target lower bounding, and typically requires additional components and a lot
283"
RELATED WORK,0.6536312849162011,"more compute than the basic TD learning does. Therefore, we leave it to future work to explore the
284"
RELATED WORK,0.6554934823091247,"synergy between the two.
285"
RELATED WORK,0.6573556797020484,"Interestingly, it is common practice to lower and upper bound the returns to the possible region,
286"
RELATED WORK,0.659217877094972,"e.g. Andrychowicz et al. (2017) bounds value between [−
1
1−γ , 0]. Similar to lower bounding with
287"
RELATED WORK,0.6610800744878957,"episodic return (Section 3.1), such strict bounds of the actual value can be thought of as admissible
288"
RELATED WORK,0.6629422718808193,"heuristics (bounds) used during search of the optimal solution (Russell & Norvig, 2020). What’s new
289"
RELATED WORK,0.664804469273743,"in this work is that lower bounding with bootstrapped values (which can overestimate the value) still
290"
RELATED WORK,0.6666666666666666,"converges to the optimal value.
291"
RELATED WORK,0.6685288640595903,"Kumar et al. (2020) (DisCor) also recognized that bootstrapped value targets can be inaccurate. This
292"
RELATED WORK,0.6703910614525139,"bias impacts learning adversely under function approximation. DisCor uses distribution correction to
293"
RELATED WORK,0.6722532588454376,"sample experience with accurate bootstrap targets more frequently, while value target lower bounding
294"
RELATED WORK,0.6741154562383612,"aims to directly reduce the bias.
295"
RELATED WORK,0.6759776536312849,"While in theory using empirical return to lower bound the value target is only correct for deterministic
296"
RELATED WORK,0.6778398510242085,"environments, in practice, it seems as long as the environment is not heavily impacted by random fluc-
297"
RELATED WORK,0.6797020484171322,"tuations, they still perform well. In fact, with function approximation, the agents cannot distinguish
298"
RELATED WORK,0.6815642458100558,"between two slightly different states, making the problem partially observable (Sutton & Barto, 2018)
299"
RELATED WORK,0.6834264432029795,"and appear slightly random. Prior methods such as SIL (Oh et al., 2018), Optimality Tightening (He
300"
RELATED WORK,0.6852886405959032,"et al., 2017), and even Hindsight relabeling (Andrychowicz et al., 2017) and MuZero (Schrittwieser
301"
RELATED WORK,0.6871508379888268,"et al., 2019) require the environment to be deterministic. Despite this theoretical limitation, the lower
302"
RELATED WORK,0.6890130353817505,"bounding methods and the prior methods can still be very useful, outperforming baselines often by
303"
RELATED WORK,0.6908752327746741,"large margins and when deploying to the real world (Fujita et al., 2020).
304"
CONCLUSIONS,0.6927374301675978,"7
Conclusions
305"
CONCLUSIONS,0.6945996275605214,"We propose a general form of lower bounding the value target using possibly bootstrapped return. In
306"
CONCLUSIONS,0.6964618249534451,"theory, value target lower bounding converges to the same optimal solution as the original Bellman
307"
CONCLUSIONS,0.6983240223463687,"operator. In practice, several ways of finding value lower bounds are examined.
308"
CONCLUSIONS,0.7001862197392924,"For episodic tasks, discounted episodic return is an efficient and effective method involving very
309"
CONCLUSIONS,0.702048417132216,"little extra computation. Precomputing the episodic return and storing it into the replay buffer
310"
CONCLUSIONS,0.7039106145251397,"allows efficient lower bound computation. It achieves much higher sample efficiency and converged
311"
CONCLUSIONS,0.7057728119180633,"performance than one-step baselines such as SAC, DDPG or TD3 in most tasks, and is competitive
312"
CONCLUSIONS,0.707635009310987,"among n-step baselines. Simple goal distance based return uses even less compute and achieves large
313"
CONCLUSIONS,0.7094972067039106,"gains in certain long horizon tasks over Hindsight relabeling (HER).
314"
CONCLUSIONS,0.7113594040968343,"For non-episodic tasks or in general, lower bounding with n-step bootstrapped return outperforms
315"
CONCLUSIONS,0.7132216014897579,"one-step baselines and is a strong competitor to the n-step methods such as (truncated) td-lambda and
316"
CONCLUSIONS,0.7150837988826816,"Retrace.
317"
FUTURE WORK,0.7169459962756052,"7.1
Future Work
318"
FUTURE WORK,0.7188081936685289,"There are probably better ways of finding value lower bounds that improve training even more. One
319"
FUTURE WORK,0.7206703910614525,"direction may be to use planning (e.g. Monte Carlo Tree Search, the Cross Entropy Method or using
320"
FUTURE WORK,0.7225325884543762,"subgoals) to achieve tighter lower bounds given a model of the task.
321"
FUTURE WORK,0.7243947858472998,"Estimating value lower bound for stochastic tasks may be possible, e.g. by learning a reward
322"
FUTURE WORK,0.7262569832402235,"function and a dynamics model and using imagined rollouts to obtain bootstrapped returns without
323"
FUTURE WORK,0.7281191806331471,"overestimation.
324"
FUTURE WORK,0.7299813780260708,"Other ways of bounding the value target, e.g. upper bounding (He et al., 2017), may be worth
325"
FUTURE WORK,0.7318435754189944,"investigating as well, e.g. to reduce overestimation in regions of poor reward.
326"
REFERENCES,0.7337057728119181,"References
327"
REFERENCES,0.7355679702048417,"Andrychowicz, M., Crow, D., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B.,
328"
REFERENCES,0.7374301675977654,"Tobin, J., Abbeel, P., and Zaremba, W.
Hindsight experience replay.
In Guyon, I., von
329"
REFERENCES,0.7392923649906891,"Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Gar-
330"
REFERENCES,0.7411545623836127,"nett, R. (eds.), Advances in Neural Information Processing Systems 30: Annual Conference
331"
REFERENCES,0.7430167597765364,"on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
332"
REFERENCES,0.74487895716946,"USA, pp. 5048–5058, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
333"
REFERENCES,0.7467411545623837,"453fadbd8a1a3af50a9df4df899537b5-Abstract.html.
334"
REFERENCES,0.7486033519553073,"Bellman, R. Dynamic Programming. Princeton Univ. Press, Princeton, NJ, USA, 1957. ISBN
335"
REFERENCES,0.750465549348231,"0-691-07951-X.
336"
REFERENCES,0.7523277467411545,"Blier, L. and Ollivier, Y.
Unbiased methods for multi-goal reinforcement learning.
CoRR,
337"
REFERENCES,0.7541899441340782,"abs/2106.08863, 2021. URL https://arxiv.org/abs/2106.08863.
338"
REFERENCES,0.7560521415270018,"Chane-Sane, E., Schmid, C., and Laptev, I. Goal-conditioned reinforcement learning with imagined
339"
REFERENCES,0.7579143389199255,"subgoals. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on
340"
REFERENCES,0.7597765363128491,"Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 1430–1440.
341"
REFERENCES,0.7616387337057728,"PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/chane-sane21a.
342"
REFERENCES,0.7635009310986964,"html.
343"
REFERENCES,0.7653631284916201,"Chua, K., Calandra, R., McAllister, R., and Levine, S. Deep reinforcement learning in a handful of
344"
REFERENCES,0.7672253258845437,"trials using probabilistic dynamics models. In Proceedings of the 32nd International Conference
345"
REFERENCES,0.7690875232774674,"on Neural Information Processing Systems, NIPS’18, pp. 4759–4770, Red Hook, NY, USA, 2018.
346"
REFERENCES,0.770949720670391,"Curran Associates Inc.
347"
REFERENCES,0.7728119180633147,"Denardo, E. V. Contraction mappings in the theory underlying dynamic programming. SIAM Review,
348"
REFERENCES,0.7746741154562383,"9(2):165–177, 1967. ISSN 00361445. URL http://www.jstor.org/stable/2027440.
349"
REFERENCES,0.776536312849162,"Fujimoto, S., van Hoof, H., and Meger, D. Addressing function approximation error in actor-critic
350"
REFERENCES,0.7783985102420856,"methods. CoRR, 2018. URL http://arxiv.org/abs/1802.09477.
351"
REFERENCES,0.7802607076350093,"Fujita, Y., Uenishi, K., Ummadisingu, A., Nagarajan, P., Masuda, S., and Castro, M. Distributed rein-
352"
REFERENCES,0.7821229050279329,"forcement learning of targeted grasping with active vision for mobile manipulators. In IEEE/RSJ
353"
REFERENCES,0.7839851024208566,"International Conference on Intelligent Robots and Systems (IROS), pp. 9712–9719, Oct 2020.
354"
REFERENCES,0.7858472998137802,"Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy
355"
REFERENCES,0.7877094972067039,"deep reinforcement learning with a stochastic actor. In Dy, J. and Krause, A. (eds.), Proceedings of
356"
REFERENCES,0.7895716945996276,"the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
357"
REFERENCES,0.7914338919925512,"Learning Research, pp. 1861–1870. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.
358"
REFERENCES,0.7932960893854749,"press/v80/haarnoja18b.html.
359"
REFERENCES,0.7951582867783985,"Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent
360"
REFERENCES,0.7970204841713222,"dynamics for planning from pixels. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of
361"
REFERENCES,0.7988826815642458,"the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
362"
REFERENCES,0.8007448789571695,"Learning Research, pp. 2555–2565. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.
363"
REFERENCES,0.8026070763500931,"press/v97/hafner19a.html.
364"
REFERENCES,0.8044692737430168,"He, F. S., Liu, Y., Schwing, A. G., and Peng, J. Learning to play in a day: Faster deep reinforcement
365"
REFERENCES,0.8063314711359404,"learning by optimality tightening. In 5th International Conference on Learning Representations,
366"
REFERENCES,0.8081936685288641,"ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net,
367"
REFERENCES,0.8100558659217877,"2017. URL https://openreview.net/forum?id=rJ8Je4clg.
368"
REFERENCES,0.8119180633147114,"Hoppe, S. and Toussaint, M. Qgraph-bounded q-learning: Stabilizing model-free off-policy deep
369"
REFERENCES,0.813780260707635,"reinforcement learning. CoRR, abs/2007.07582, 2020. URL https://arxiv.org/abs/2007.
370"
REFERENCES,0.8156424581005587,"07582.
371"
REFERENCES,0.8175046554934823,"Kumar, A., Gupta, A., and Levine, S. Discor: Corrective feedback in reinforcement learning via
372"
REFERENCES,0.819366852886406,"distribution correction. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin,
373"
REFERENCES,0.8212290502793296,"H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18560–18572.
374"
REFERENCES,0.8230912476722533,"Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
375"
REFERENCES,0.8249534450651769,"d7f426ccbc6db7e235c57958c21c5dfa-Paper.pdf.
376"
REFERENCES,0.8268156424581006,"Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.
377"
REFERENCES,0.8286778398510242,"Continuous control with deep reinforcement learning. In Bengio, Y. and LeCun, Y. (eds.), 4th
378"
REFERENCES,0.8305400372439479,"International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May
379"
REFERENCES,0.8324022346368715,"2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1509.02971.
380"
REFERENCES,0.8342644320297952,"Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller,
381"
REFERENCES,0.8361266294227188,"M. A. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013. URL
382"
REFERENCES,0.8379888268156425,"http://arxiv.org/abs/1312.5602.
383"
REFERENCES,0.839851024208566,"Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. Safe and efficient off-policy
384"
REFERENCES,0.8417132216014898,"reinforcement learning.
In Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., and
385"
REFERENCES,0.8435754189944135,"Garnett, R. (eds.), Advances in Neural Information Processing Systems 29: Annual Con-
386"
REFERENCES,0.845437616387337,"ference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona,
387"
REFERENCES,0.8472998137802608,"Spain, pp. 1046–1054, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/
388"
REFERENCES,0.8491620111731844,"c3992e9a68c5ae12bd18488bc579b30d-Abstract.html.
389"
REFERENCES,0.851024208566108,"Nair, S. and Finn, C. Hierarchical foresight: Self-supervised learning of long-horizon tasks via
390"
REFERENCES,0.8528864059590316,"visual subgoal generation. In International Conference on Learning Representations, 2020. URL
391"
REFERENCES,0.8547486033519553,"https://openreview.net/forum?id=H1gzR2VKDH.
392"
REFERENCES,0.8566108007448789,"Nasiriany, S., Pong, V., Lin, S., and Levine, S. Planning with goal-conditioned policies. Advances in
393"
REFERENCES,0.8584729981378026,"Neural Information Processing Systems, 2019.
394"
REFERENCES,0.8603351955307262,"Oh, J., Guo, Y., Singh, S., and Lee, H. Self-imitation learning. CoRR, abs/1806.05635, 2018. URL
395"
REFERENCES,0.8621973929236499,"http://arxiv.org/abs/1806.05635.
396"
REFERENCES,0.8640595903165735,"Plappert, M., Andrychowicz, M., Ray, A., McGrew, B., Baker, B., Powell, G., Schneider, J., Tobin,
397"
REFERENCES,0.8659217877094972,"J., Chociej, M., Welinder, P., Kumar, V., and Zaremba, W. Multi-goal reinforcement learning:
398"
REFERENCES,0.8677839851024208,"Challenging robotics environments and request for research. CoRR, abs/1802.09464, 2018. URL
399"
REFERENCES,0.8696461824953445,"http://arxiv.org/abs/1802.09464.
400"
REFERENCES,0.8715083798882681,"Russell, S. J. and Norvig, P. Artificial Intelligence: A Modern Approach (4th Edition). Pearson, 2020.
401"
REFERENCES,0.8733705772811918,"ISBN 9780134610993. URL http://aima.cs.berkeley.edu/.
402"
REFERENCES,0.8752327746741154,"Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart,
403"
REFERENCES,0.8770949720670391,"E., Hassabis, D., Graepel, T., Lillicrap, T. P., and Silver, D. Mastering atari, go, chess and shogi by
404"
REFERENCES,0.8789571694599627,"planning with a learned model. CoRR, abs/1911.08265, 2019. URL http://arxiv.org/abs/
405"
REFERENCES,0.8808193668528864,"1911.08265.
406"
REFERENCES,0.88268156424581,"Silver, D. and Ciosek, K. Compositional Planning Using Optimal Option Models. In Proceedings of
407"
REFERENCES,0.8845437616387337,"the 29th International Conference on Machine Learning, pp. 165. icml.cc / Omnipress, 2012.
408"
REFERENCES,0.8864059590316573,"Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. A Bradford Book,
409"
REFERENCES,0.888268156424581,"Cambridge, MA, USA, 2018. ISBN 0262039249.
410"
REFERENCES,0.8901303538175046,"Tang, Y. Self-imitation learning via generalized lower bound q-learning. CoRR, abs/2006.07442,
411"
REFERENCES,0.8919925512104283,"2020. URL https://arxiv.org/abs/2006.07442.
412"
REFERENCES,0.8938547486033519,"Thomas, P. Bias in natural actor-critic algorithms. In Xing, E. P. and Jebara, T. (eds.), Proceedings of
413"
REFERENCES,0.8957169459962756,"the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine
414"
REFERENCES,0.8975791433891993,"Learning Research, pp. 441–448, Bejing, China, 22–24 Jun 2014. PMLR. URL https://
415"
REFERENCES,0.8994413407821229,"proceedings.mlr.press/v32/thomas14.html.
416"
REFERENCES,0.9013035381750466,"van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. CoRR,
417"
REFERENCES,0.9031657355679702,"abs/1509.06461, 2015. URL http://arxiv.org/abs/1509.06461.
418"
REFERENCES,0.9050279329608939,"Ye, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y. Mastering atari games with limited data. CoRR,
419"
REFERENCES,0.9068901303538175,"abs/2111.00210, 2021. URL https://arxiv.org/abs/2111.00210.
420"
REFERENCES,0.9087523277467412,"Zhang, H., Xu, W., and Yu, H. Generative planning for temporally coordinated exploration in
421"
REFERENCES,0.9106145251396648,"reinforcement learning. In International Conference on Learning Representations, 2022. URL
422"
REFERENCES,0.9124767225325885,"https://openreview.net/forum?id=YZHES8wIdE.
423"
REFERENCES,0.9143389199255121,"Checklist
424"
REFERENCES,0.9162011173184358,"1. For all authors...
425"
REFERENCES,0.9180633147113594,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
426"
REFERENCES,0.9199255121042831,"contributions and scope? [Yes]
427"
REFERENCES,0.9217877094972067,"(b) Did you describe the limitations of your work? [Yes] Regarding the implementations
428"
REFERENCES,0.9236499068901304,"being theoretically limited to deterministic MDPs, see Section 3.1 and the last paragraph
429"
REFERENCES,0.925512104283054,"of Related Works (Section 6) for details, and Appendix 1.7 for an example. The
430"
REFERENCES,0.9273743016759777,"theorems, however, do not assume a deterministic MDP, and follow standard Bellman
431"
REFERENCES,0.9292364990689013,"value contraction assumptions. Experiments use function approximation on continuous
432"
REFERENCES,0.931098696461825,"observations or action spaces, which already violate the theoretical assumptions, but
433"
REFERENCES,0.9329608938547486,"the lower bounding methods still show large gains over the baselines.
434"
REFERENCES,0.9348230912476723,"(c) Did you discuss any potential negative societal impacts of your work? [Yes] Appendix
435"
REFERENCES,0.9366852886405959,"1.9
436"
REFERENCES,0.9385474860335196,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
437"
REFERENCES,0.9404096834264432,"them? [Yes]
438"
REFERENCES,0.9422718808193669,"2. If you are including theoretical results...
439"
REFERENCES,0.9441340782122905,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
440"
REFERENCES,0.9459962756052142,"(b) Did you include complete proofs of all theoretical results? [Yes] Appendix 1.1
441"
REFERENCES,0.9478584729981379,"3. If you ran experiments...
442"
REFERENCES,0.9497206703910615,"(a) Did you include the code, data, and instructions needed to reproduce the main ex-
443"
REFERENCES,0.9515828677839852,"perimental results (either in the supplemental material or as a URL)? [No] Omitted
444"
REFERENCES,0.9534450651769087,"due to anonymity. Code is already in a github repository, and will be released upon
445"
REFERENCES,0.9553072625698324,"publication.
446"
REFERENCES,0.957169459962756,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
447"
REFERENCES,0.9590316573556797,"were chosen)? [Yes] Appendix 1.2 and 1.3.
448"
REFERENCES,0.9608938547486033,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
449"
REFERENCES,0.962756052141527,"ments multiple times)? [Yes] Done for all plots
450"
REFERENCES,0.9646182495344506,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
451"
REFERENCES,0.9664804469273743,"of GPUs, internal cluster, or cloud provider)? [No] Treatments only cost a bit more
452"
REFERENCES,0.9683426443202979,"compute than the baselines.
453"
REFERENCES,0.9702048417132216,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
454"
REFERENCES,0.9720670391061452,"(a) If your work uses existing assets, did you cite the creators? [Yes] Except for the open
455"
REFERENCES,0.9739292364990689,"source code repository, to keep anonymity.
456"
REFERENCES,0.9757914338919925,"(b) Did you mention the license of the assets? [No]
457"
REFERENCES,0.9776536312849162,"(c) Did you include any new assets either in the supplemental material or as a URL? [No]
458"
REFERENCES,0.9795158286778398,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
459"
REFERENCES,0.9813780260707635,"using/curating? [N/A]
460"
REFERENCES,0.9832402234636871,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
461"
REFERENCES,0.9851024208566108,"information or offensive content? [N/A]
462"
REFERENCES,0.9869646182495344,"5. If you used crowdsourcing or conducted research with human subjects...
463"
REFERENCES,0.9888268156424581,"(a) Did you include the full text of instructions given to participants and screenshots, if
464"
REFERENCES,0.9906890130353817,"applicable? [N/A]
465"
REFERENCES,0.9925512104283054,"(b) Did you describe any potential participant risks, with links to Institutional Review
466"
REFERENCES,0.994413407821229,"Board (IRB) approvals, if applicable? [N/A]
467"
REFERENCES,0.9962756052141527,"(c) Did you include the estimated hourly wage paid to participants and the total amount
468"
REFERENCES,0.9981378026070763,"spent on participant compensation? [N/A]
469"
