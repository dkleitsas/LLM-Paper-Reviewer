Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00205761316872428,"Human decisions are often suboptimal due to limited cognitive resources and time
1"
ABSTRACT,0.00411522633744856,"constraints. Prior work has shown that errors in human decision-making can in part
2"
ABSTRACT,0.006172839506172839,"be avoided by leveraging artificial intelligence to automatically discover efficient
3"
ABSTRACT,0.00823045267489712,"decision strategies and teach them to people. So far, this line of research has
4"
ABSTRACT,0.0102880658436214,"been limited to simplified decision problems that are not directly related to the
5"
ABSTRACT,0.012345679012345678,"problems people face in the real world. Current methods are mainly limited by
6"
ABSTRACT,0.01440329218106996,"the computational difficulties of deriving efficient decision strategies for complex
7"
ABSTRACT,0.01646090534979424,"real-world problems through metareasoning. To bridge this gap, we model a real-
8"
ABSTRACT,0.018518518518518517,"world decision problem in which people have to choose which project to pursue,
9"
ABSTRACT,0.0205761316872428,"and develop a metareasoning method that enables us to discover and teach efficient
10"
ABSTRACT,0.02263374485596708,"decision strategies in this setting. Our main contributions are: formulating the
11"
ABSTRACT,0.024691358024691357,"metareasoning problem of deciding how to select a project, developing a metar-
12"
ABSTRACT,0.026748971193415638,"easoning method that can automatically discover near-optimal project selection
13"
ABSTRACT,0.02880658436213992,"strategies, and developing an intelligent tutor that teaches people the discovered
14"
ABSTRACT,0.030864197530864196,"strategies. We test our strategy discovery method on a computational benchmark
15"
ABSTRACT,0.03292181069958848,"and experimentally evaluate its utility for improving human decision-making. In
16"
ABSTRACT,0.03497942386831276,"the benchmark, we demonstrate that our method outperforms PO-UCT while also
17"
ABSTRACT,0.037037037037037035,"being more computationally efficient. In the experiment, we taught the discovered
18"
ABSTRACT,0.03909465020576132,"planning strategies to people using an intelligent tutor. People who were trained by
19"
ABSTRACT,0.0411522633744856,"our tutor showed a significant improvement in their decision strategies compared
20"
ABSTRACT,0.043209876543209874,"to people who tried to discover good decision strategies on their own or practiced
21"
ABSTRACT,0.04526748971193416,"with an equivalent tutor that did not reveal the optimal strategy. Project selec-
22"
ABSTRACT,0.047325102880658436,"tion is a very consequential high-stakes decision regularly faced by organizations,
23"
ABSTRACT,0.04938271604938271,"companies, and individuals. Our results indicate that our method can successfully
24"
ABSTRACT,0.051440329218107,"improve human decision-making in naturalistic settings similar to the project selec-
25"
ABSTRACT,0.053497942386831275,"tion decisions people face in the real-world. This is a first step towards applying
26"
ABSTRACT,0.05555555555555555,"strategy discovery methods to improve people’s decisions in the real-world.
27"
INTRODUCTION,0.05761316872427984,"1
Introduction
28"
INTRODUCTION,0.059670781893004114,"Corporations and individuals commonly have to select a project to pursue out of multiple alternatives.
29"
INTRODUCTION,0.06172839506172839,"These project selection problems are usually high-stakes decisions that can be highly impactful for the
30"
INTRODUCTION,0.06378600823045268,"future of an organization. For example, an organization looking for a sustainable investment project
31"
INTRODUCTION,0.06584362139917696,"[17] could benefit both financially and by improving its public image by selecting an impactful and
32"
INTRODUCTION,0.06790123456790123,"profitable project, or incur major losses by selecting an unsuitable project.
33"
INTRODUCTION,0.06995884773662552,"Previous research on project selection recommends that candidate projects should be evaluated by
34"
INTRODUCTION,0.0720164609053498,"multiple experts [9, 17, 21], and many structured approaches to integrate the experts’ opinions exist
35"
INTRODUCTION,0.07407407407407407,"[11]. However, structured project selection techniques are not well utilized in the real-world [28, 21],
36"
INTRODUCTION,0.07613168724279835,"and decision-makers often rely on their intuition and much simpler techniques like brainstorming [18].
37"
INTRODUCTION,0.07818930041152264,"This is concerning because the intuitive decisions of groups and individuals are highly susceptible
38"
INTRODUCTION,0.08024691358024691,"to biases and unsystematic error [16]. However, people’s errors in decision-making can partly be
39"
INTRODUCTION,0.0823045267489712,"prevented by teaching them better decision strategies. This approach is known as boosting [15].
40"
INTRODUCTION,0.08436213991769548,"To identify appropriate decision strategies, we can score candidate strategies by their resource-
41"
INTRODUCTION,0.08641975308641975,"rationality, that is the degree to which they make rational use of people’s limited time and bounded
42"
INTRODUCTION,0.08847736625514403,"cognitive resources [19]. In the resource-rational framework, the decision operations people can
43"
INTRODUCTION,0.09053497942386832,"perform to arrive at a decision are modeled explicitly and assigned a cost. The overall efficiency of
44"
INTRODUCTION,0.09259259259259259,"a decision strategy h in an environment e can then be computed by subtracting the expected costs
45"
INTRODUCTION,0.09465020576131687,"λ of the N used decision operations from the expected utility Rtotal of the resulting decision (see
46"
INTRODUCTION,0.09670781893004116,"Equation 1) [10]. This measure is called resource-rationality score (RR-score) [10]. People are
47"
INTRODUCTION,0.09876543209876543,"usually not fully resource-rational, and identifying decision strategies would enable people to perform
48"
INTRODUCTION,0.10082304526748971,"as well as possible is an important open problem [6, 10, 14, 22].
49"
INTRODUCTION,0.102880658436214,"RR(h, e) = E[Rtotal|h, e] −λE[N|h, e]
(1)"
INTRODUCTION,0.10493827160493827,"Recent work has demonstrated that the theory of resource rationality makes it possible to leverage AI
50"
INTRODUCTION,0.10699588477366255,"to automatically discover and teach decision strategies that enable real people to make their decisions
51"
INTRODUCTION,0.10905349794238683,"as well as possible [6, 10, 1, 32, 22]. This approach has been dubbed AI-powered boosting. The
52"
INTRODUCTION,0.1111111111111111,"first step of AI-powered boosting is to compute resource-rational decision strategies. Automatic
53"
INTRODUCTION,0.11316872427983539,"strategy discovery methods [6, 10, 14, 32, 22] can discover efficient decision strategies by solving
54"
INTRODUCTION,0.11522633744855967,"the metareasoning problem of deciding which decision operations to perform. While recent work has
55"
INTRODUCTION,0.11728395061728394,"extended automatic strategy discovery methods to larger [10] and partially observable environments
56"
INTRODUCTION,0.11934156378600823,"[14], so far, they have not been applied to real-world decisions such as project selection.
57"
INTRODUCTION,0.12139917695473251,"In this article, we extend AI-powered boosting to improve how people select projects. Project
58"
INTRODUCTION,0.12345679012345678,"selection is challenging because many crucial attributes of the candidate projects, such as their
59"
INTRODUCTION,0.12551440329218108,"expected profitability, cannot be observed directly. Instead, they have to be inferred from observations
60"
INTRODUCTION,0.12757201646090535,"that are not fully reliable. We, therefore, formalize project selection strategies as policies for solving
61"
INTRODUCTION,0.12962962962962962,"a particular class of partially observable Markov Decision Processes (POMDPs). This formulation
62"
INTRODUCTION,0.13168724279835392,"allows us to develop the first algorithm for discovering resource-rational strategies for human project
63"
INTRODUCTION,0.1337448559670782,"selection.To achieve this, we model a realistic project selection task as a metareasoning problem. The
64"
INTRODUCTION,0.13580246913580246,"metareasoning consists in deciding which information one should request from which advisors when
65"
INTRODUCTION,0.13786008230452676,"information is highly limited, uncertain, and costly. We develop an efficient algorithm for solving this
66"
INTRODUCTION,0.13991769547325103,"problem and apply it to derive the optimal decision strategy for a project selection problem a financial
67"
INTRODUCTION,0.1419753086419753,"institution faced in the real world [17]. Finally, we develop an intelligent tutor [6] that teaches the
68"
INTRODUCTION,0.1440329218106996,"decision strategy discovered by our method to people. We evaluated our approach by letting our
69"
INTRODUCTION,0.14609053497942387,"intelligent tutor teach the automatically discovered project selection strategy to about 100 people,
70"
INTRODUCTION,0.14814814814814814,"and then evaluated the quality of their decisions in realistic project selection problems against two
71"
INTRODUCTION,0.15020576131687244,"control groups. Our results indicate that our approach can successfully improve human decisions in
72"
INTRODUCTION,0.1522633744855967,"real-world problems where people are reluctant to let machines decide for them.
73"
BACKGROUND,0.15432098765432098,"2
Background
74"
BACKGROUND,0.15637860082304528,"Project selection
In the project selection problem, a decision-maker aims to select the best-fitting
75"
BACKGROUND,0.15843621399176955,"project out of several candidates [27]. Apart from a project’s profitability, the evaluation usually also
76"
BACKGROUND,0.16049382716049382,"considers other factors, such as the alignment with organizational goals [7]. This problem can be
77"
BACKGROUND,0.16255144032921812,"formalized as multi-criteria decision-making (MCDM) [11, 23]. Projects can be evaluated using a
78"
BACKGROUND,0.1646090534979424,"scoring technique, which evaluates relevant criteria and then combines them to a weighted sum [27].
79"
BACKGROUND,0.16666666666666666,"Common approaches to solving the project selection problem include techniques such as the analytic
80"
BACKGROUND,0.16872427983539096,"hierarchy process, the analytic network process, real options analysis, and TOPSIS (see [11] for a
81"
BACKGROUND,0.17078189300411523,"review). These methods are commonly combined with fuzzy sets to account for uncertainty [17].
82"
BACKGROUND,0.1728395061728395,"However, these methods are rarely used in real-world problems because implementing them would
83"
BACKGROUND,0.1748971193415638,"require gathering and integrating a lot of information through a time-consuming process, which is
84"
BACKGROUND,0.17695473251028807,"often incompatible with the organizational decision process [28, 21]. Instead, organizations often
85"
BACKGROUND,0.17901234567901234,"rely on simpler, less structured methods like brainstorming [18]. In addition, the detailed information
86"
BACKGROUND,0.18106995884773663,"required by these methods can be costly to acquire in real-world settings.
87"
BACKGROUND,0.1831275720164609,"Judge-advisor systems
In a Judge Advisor System (JAS) [2], typically, a single decision-maker has
88"
BACKGROUND,0.18518518518518517,"to make a decision, and multiple advisors support the decision-maker by offering advice. Variations of
89"
BACKGROUND,0.18724279835390947,"the task can include costly advice [33, 12] , or advisors with varying reliability [24]. This is a common
90"
BACKGROUND,0.18930041152263374,"situation when CEOs decide which project their company should launch next. Unfortunately, decision-
91"
BACKGROUND,0.19135802469135801,"makers are known to be highly susceptible to systematic errors, such as weighing one’s own opinion
92"
BACKGROUND,0.1934156378600823,"too strongly, overconfidence, egocentric advice discounting, and weighting the recommendations of
93"
BACKGROUND,0.19547325102880658,"advisors by their confidence rather than their competence [2, 25, 33].
94"
BACKGROUND,0.19753086419753085,"Strategy discovery methods
Discovering resource-rational planning strategies can be achieved by
95"
BACKGROUND,0.19958847736625515,"solving a meta-level Markov decision process [13, 4, 5], which models the metareasoning process as
96"
BACKGROUND,0.20164609053497942,"a Markov Decision Process (MDP), which state represents the current belief about the environment
97"
BACKGROUND,0.2037037037037037,"and actions represent decision operations. Performing a decision operation results in a negative
98"
BACKGROUND,0.205761316872428,"cost and results in an update to the belief state. A special termination action represents exiting the
99"
BACKGROUND,0.20781893004115226,"metareasoning process and making a real-world decision, guided by the current beliefs [13]. Multiple
100"
BACKGROUND,0.20987654320987653,"methods for solving meta-level MDPs exist (e.g. [26, 4, 13, 10]). We refer to these algorithms
101"
BACKGROUND,0.21193415637860083,"as strategy discovery methods [4, 6, 32, 10, 14, 22]. They learn or compute policies for selecting
102"
BACKGROUND,0.2139917695473251,"sequences of cognitive operations (i.e., computations) people can perform to reach good decisions.
103"
BACKGROUND,0.21604938271604937,"MGPO [14] is currently the only strategy discovery algorithm that can efficiently approximate
104"
BACKGROUND,0.21810699588477367,"resource-rational strategies for decision-making in partially observable environments. MGPO chooses
105"
BACKGROUND,0.22016460905349794,"decision operations by approximating their value of computation [26] in a myopic manner: it always
106"
BACKGROUND,0.2222222222222222,"selects the computation whose execution would yield the highest expected gain in reward if a decision
107"
BACKGROUND,0.2242798353909465,"had to be made immediately afterward, without any additional planning.
108"
BACKGROUND,0.22633744855967078,"Cognitive tutors
Past work has developed cognitive tutors that teach automatically discovered
109"
BACKGROUND,0.22839506172839505,"planning strategies to people [6, 32, 22, 10]. Training experiments indicated that training with
110"
BACKGROUND,0.23045267489711935,"these cognitive tutors could significantly boost the quality of people’s planning and decision-making
111"
BACKGROUND,0.23251028806584362,"[6, 32, 10, 14, 22]. These cognitive tutors teach efficient decision strategies in an automated manner,
112"
BACKGROUND,0.2345679012345679,"usually by computing the value of available decision operations using strategy discovery methods,
113"
BACKGROUND,0.2366255144032922,"and providing the learner feedback on the quality of the computations they select. Initially limited to
114"
BACKGROUND,0.23868312757201646,"small planning tasks due to the computational complexity of solving meta-level MDPs [20, 6], recent
115"
BACKGROUND,0.24074074074074073,"work has extended existing methods to larger [10] and partially observable [14] settings. However,
116"
BACKGROUND,0.24279835390946503,"none of these methods have been applied to naturalistic problems so far.
117"
BACKGROUND,0.2448559670781893,"A crucial obstacle is that these methods are limited to settings where all decision-relevant information
118"
BACKGROUND,0.24691358024691357,"comes from the same source. By contrast, in the real world, people have to choose between and
119"
BACKGROUND,0.24897119341563786,"integrate multiple different sources of information. In doing so, they have to take into account that
120"
BACKGROUND,0.25102880658436216,"some information sources are more reliable than others. Additionally, current strategy discovery
121"
BACKGROUND,0.25308641975308643,"methods are limited to artificial settings where each piece of information is an estimate of a potential
122"
BACKGROUND,0.2551440329218107,"future reward. By contrast, in the real world, most information is only indirectly related to future
123"
BACKGROUND,0.257201646090535,"rewards, and different pieces of information have different units (e.g., temperature vs. travel time).
124"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.25925925925925924,"3
Formalizing optimal decision strategies for human project selection as the
125"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.2613168724279835,"solution to a meta-level MDP
126"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.26337448559670784,"In this section, we introduce explain our general resource-rational model of project selection, which
127"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.2654320987654321,"we expect to be widely applicable to concrete, real-world project selection problems.
128"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.2674897119341564,"Our model of project selection consists of two decision problems, an object-level decision-problem
129"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.26954732510288065,"and a meta-level MDP [4, 13]. The two decision problems separate the actions the decision-maker
130"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.2716049382716049,"has to choose between (object level), such as executing one project versus another, from decision
131"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.2736625514403292,"operations that represent thinking about which of those object-level actions to perform (meta-level),
132"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.2757201646090535,"such as gathering information about the projects’ attributes. This allows us to solve both problems
133"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.2777777777777778,"separately. The object-level decision problem is a MCDM problem, where a set of NP potential
134"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.27983539094650206,"projects P = {p1, ...pNP } are evaluated using NC relevant criteria C = [c1, ...cNC] weighted by
135"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.28189300411522633,"fixed predetermined weights W = [w1, ...wNC]. Actions in the object-level problem represent
136"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.2839506172839506,"selecting the corresponding project (A = {a1, ..., aNP }). The reward of a selecting a project is
137"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.28600823045267487,computed by summing the weighted criteria scores of the selected project: rO(ai) = P
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.2880658436213992,"c wccc,i [9].
138"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.29012345679012347,"While the object-level decision problem is our model of the project selection task, the meta-level
139"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.29218106995884774,"MDP is our formalization of the problem of discovering resource-rational project selection strategies.
140"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.294238683127572,"It models the task of gathering information about deciding which project to select. States in the meta-
141"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.2962962962962963,"level MDP are belief states that represent the current information about each project’s attributes. We
142"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.29835390946502055,"model belief states using a multivariate Normal distribution to quantify the estimated value and uncer-
143"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3004115226337449,"tainty about the NP projects’ scores on the NC criteria: b = [(µ1,1, σ1,1) , · · · , (µNP ,NC, σNP ,NC)].
144"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.30246913580246915,"The actions (decision operations) of the meta-level MDP gather information about the different
145"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3045267489711934,"attributes of projects by asking one of the NE experts for their estimate of how one of the projects
146"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3065843621399177,"scores on one of the criteria. Experts provide discrete estimates from minobs to maxobs, and ex-
147"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.30864197530864196,"pert estimates can differ in their reliability and their cost. Specifically, the available actions are
148"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.31069958847736623,"AM = {a1,1,1, · · · , aNP ,NC,NE, ⊥}, where the meta-level action ai,j,k represents asking the expert
149"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.31275720164609055,"ek for their estimate of criterion cj of project pi. After receiving information obs from an expert,
150"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3148148148148148,"the current belief state bpicj = N(µ, σ) is updated by applying the update equation for a Gaussian
151"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3168724279835391,"likelihood function with standard deviation σe (i.e. the expert’s reliability) and a conjugate Gaussian
152"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.31893004115226337,"prior (i.e., the current belief), that is ˆµ ←

wci·µ
(wci·σ)2 +
wci·obs
(wci·σe)2

·
 
(wci · σ)2 + (wci · σe)2
and
153"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.32098765432098764,"ˆσ ←
r"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3230452674897119,"1
1
(σ·s)2 +
1
(σe·s)2 .
154"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.32510288065843623,"The reward of these meta-level actions is the negative cost rM(ai,j,k) = −λek of asking the expert ek
155"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3271604938271605,"for help. Finally, the meta-level action ⊥is the termination action, which corresponds to terminating
156"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3292181069958848,"the decision-making process and selecting a project. The reward of the termination action is the
157"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.33127572016460904,"expected reward of selecting the best project according to the current belief-state. An optional
158"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3333333333333333,"budget parameter Na specifies the maximum number of available meta-level actions, after which the
159"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.33539094650205764,"termination action is performed automatically.
160"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3374485596707819,"Meta-level MDPs are notoriously difficult to solve due to their extremely large state space [10, 13]. In
161"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3395061728395062,"the project selection task, the meta-level MDP has (maxobs −minobs + 2)NP ·NC·NE possible belief
162"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.34156378600823045,"states and NP · NC · NE + 1 possible meta-level actions. Our meta-level MDP introduces multiple
163"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3436213991769547,"new intricacies that current metareasoning methods like MGPO [14] aren’t equipped to handle,
164"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.345679012345679,"making strategy discovery in this setting especially difficult. Compared to previously formulated
165"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3477366255144033,"meta-level MDPs [4, 13, 10, 14, 22], our meta-level description of project selection differs in the
166"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3497942386831276,"following ways: (1) the maximum amount of meta-level actions is constrained with a budget, (2) the
167"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.35185185185185186,"project selection task features multiple consultants who differ in both the quality of their advice and
168"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.35390946502057613,"the cost of their services, (3) consultants in the project selection task offer discrete estimates of each
169"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.3559670781893004,"criterion , requiring that (4) criteria ratings are scaled to allow weighting the criteria according to
170"
FORMALIZING OPTIMAL DECISION STRATEGIES FOR HUMAN PROJECT SELECTION AS THE,0.35802469135802467,"their importance.
171"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.360082304526749,"4
A new metareasoning algorithm for discovering optimal decision strategies
172"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.36213991769547327,"for human project selection
173"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.36419753086419754,"Previous metareasoning methods are unable to handle some of the intricacies of the project selection
174"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.3662551440329218,"problem. Therefore, we developed a new strategy discovery method based on MGPO [14], which
175"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.3683127572016461,"overcomes the limitations that prevent MGPO from being applicable to project selection. To reflect
176"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.37037037037037035,"the commonalities and innovations, we call our new strategy discovery method MGPS (meta-greedy
177"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.3724279835390947,"policy for project selection). Similar to MGPO, our method approximates the value of computation
178"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.37448559670781895,"(VOC) [26] by myopically estimating the immediate improvement in decision quality. Improving
179"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.3765432098765432,"upon MGPO, MGPS calculates the myopic approximation to the VOC in a way that accounts for
180"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.3786008230452675,"discrete criteria ratings, criteria scaling, and multiple sources of information with different costs and
181"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.38065843621399176,"reliabilities.
182"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.38271604938271603,"MGPS calculates a myopic approximation to the VOC of asking an expert about a specific criterion
183"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.38477366255144035,"of a single project according to Algorithm 1. To account for discrete advisor outputs, Algorithm 1
184"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.3868312757201646,"iterates over the discrete set of possible ratings the expert might give and estimates the probability
185"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.3888888888888889,"pobs of each rating (obs) and the belief update that would result from it ˆµobs . The probability of each
186"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.39094650205761317,"rating is computed using the cumulative distribution function (Φ) of the belief state for the selected
187"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.39300411522633744,"project’s criterion score (see Line 7). Here, the standard deviation σe of the likelihood function
188"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.3950617283950617,"encodes the expert’s reliability, the prior (N(µ, σ)) is the current belief about the project’s score on
189"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.39711934156378603,"the evaluated criterion, and the weights wci convert the criteria into a common currency. The belief
190"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.3991769547325103,"update that would result from the observation (ˆµobs) is computed by applying the belief state update
191"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4012345679012346,"Algorithm 1 MGPS VOC calculation for an action api,ci,ei"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.40329218106995884,"1: function MYOPIC_VOC(pi, ci, ei, b)
2:
rp ←E[rO(pi)]
3:
ralt ←maxpj∈P−{pi} E[rO(pj)]
4:
µ, σ ←bpici
5:
for obs from minobs to maxobs do
6:
if minobs < obs < maxobs then"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4053497942386831,"7:
pobs ←Φ

wci·(obs+0.5−µ)
√"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4074074074074074,(wci·σ)2+(wci·σe)2
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4094650205761317,"
−Φ

wci·(obs−0.5−µ)
√"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.411522633744856,(wci·σ)2+(wci·σe)2 
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.41358024691358025,"8:
else if obs == minobs then"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4156378600823045,"9:
pobs ←Φ

wci·(obs+0.5−µ)
√"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4176954732510288,(wci·σ)2+(wci·σe)2 
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.41975308641975306,"10:
else"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4218106995884774,"11:
pobs ←1 −Φ

wci·(obs−0.5−µ)
√"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.42386831275720166,(wci·σ)2+(wci·σe)2 
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.42592592592592593,"12:
end if
13:
ˆµobs ←

wci·µ
(wci·σ)2 +
wci·obs
(wci·σe)2

·
 
(wci · σ)2 + (wci · σe)2"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4279835390946502,"14:
end for
15:
if rp > ralt then
16:
voc ←Pmaxobs
obs=minobs pobs(rpalt + µ −rp −ˆµobs) · 1(rp −µ + ˆµobs < ralt)
17:
else
18:
voc ←Pmaxobs
obs=minobs pobs(rp + ˆµobs −µ −rpalt) · 1(rp −µ + ˆµobs > ralt)
19:
end if
20:
return (1 −wλ)voc - wλλei
21: end function"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.43004115226337447,"(see Line 13 and Equation 3). For the highest and lowest possible ratings, we instead calculate pobs
192"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.43209876543209874,"using the open interval (see Lines 9 and 11). The updated expected value of the belief state according
193"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.43415637860082307,"to an observation obs is then calculated using Bayesian inference to integrate the new observation
194"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.43621399176954734,"into the belief state (see Line 13).
195"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4382716049382716,"The VOC calculation depends on the posterior belief states that would result from the different
196"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4403292181069959,"possible observations (ˆµobs), weighted by their probabilities. If the evaluated project currently has the
197"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.44238683127572015,"highest expected reward (see Line 15), the VOC calculation expresses the probability of observing a
198"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4444444444444444,"value low enough that the second-best project becomes the most promising option (see Line 16). In
199"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.44650205761316875,"the case where the evaluated project did not have the highest expected termination reward, the VOC
200"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.448559670781893,"calculation expresses the probability of observing a value high enough to make the evaluated project
201"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4506172839506173,"the most promising option (see Line 18). Finally, the cost of the requested expert λei is weighted
202"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.45267489711934156,"using a free cost weight parameter wλ and subtracted from the VOC estimate (see Line 20).
203"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4547325102880658,"The full meta-greedy policy consists of calculating the VOC for all possible meta-level actions and
204"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4567901234567901,"iteratively selecting the meta-level action with the highest VOC. If no action has a positive VOC, the
205"
A NEW METAREASONING ALGORITHM FOR DISCOVERING OPTIMAL DECISION STRATEGIES,0.4588477366255144,"termination action ⊥is chosen.
206"
IMPROVING HUMAN PROJECT SELECTION,0.4609053497942387,"5
Improving human project selection
207"
IMPROVING HUMAN PROJECT SELECTION,0.46296296296296297,"Having developed a general metareasoning method for discovering resource-rational strategies for
208"
IMPROVING HUMAN PROJECT SELECTION,0.46502057613168724,"human project selection, we now extend it to an intelligent cognitive tutor for teaching people how to
209"
IMPROVING HUMAN PROJECT SELECTION,0.4670781893004115,"select better projects. Our goal is to provide a proof of concept for a general AI-powered boosting
210"
IMPROVING HUMAN PROJECT SELECTION,0.4691358024691358,"approach that can be used to improve human decision-making across a wide range of project selection
211"
IMPROVING HUMAN PROJECT SELECTION,0.4711934156378601,"problems. We first introduce a general approach for teaching people the project selection strategies
212"
IMPROVING HUMAN PROJECT SELECTION,0.4732510288065844,"discovered by MGPS, and then apply it to a real-world project selection problem.
213"
IMPROVING HUMAN PROJECT SELECTION,0.47530864197530864,"5.1
MGPS Tutor: An intelligent tutor for teaching people how to select better projects
214"
IMPROVING HUMAN PROJECT SELECTION,0.4773662551440329,"Our intelligent tutor for project selection (MGPS Tutor) trains people to select the near-optimal
215"
IMPROVING HUMAN PROJECT SELECTION,0.4794238683127572,"decision operations identified by MGPS. To achieve this, it lets people practice on a series of project
216"
IMPROVING HUMAN PROJECT SELECTION,0.48148148148148145,"Figure 1: Example of the MGPS tutor offering a choice between requesting information from three
different experts (highlighted in orange) in the simplified training task of deciding between two
project alternatives. Refer to the supplemental material for an explanation of the experiment interface."
IMPROVING HUMAN PROJECT SELECTION,0.4835390946502058,"selection problems and gives them feedback. MPGS Tutor leverages MPGS in two ways: i) to
217"
IMPROVING HUMAN PROJECT SELECTION,0.48559670781893005,"pedagogically construct the set of queries the learner is asked to choose from, and ii) to give the
218"
IMPROVING HUMAN PROJECT SELECTION,0.4876543209876543,"learner feedback on their chosen query.
219"
IMPROVING HUMAN PROJECT SELECTION,0.4897119341563786,"Building on the choice tutor by [14], our tutor repeatedly asks the learner to choose from a pedagog-
220"
IMPROVING HUMAN PROJECT SELECTION,0.49176954732510286,"ically chosen set of decision operations (see Figure 1) that always includes the query that MGPS
221"
IMPROVING HUMAN PROJECT SELECTION,0.49382716049382713,"would have performed. Moreover, it leverages MGPS’s VOC calculation (Algorithm 1) to score the
222"
IMPROVING HUMAN PROJECT SELECTION,0.49588477366255146,"chosen query, and then provides binary feedback on its quality. If learners select a suboptimal expert,
223"
IMPROVING HUMAN PROJECT SELECTION,0.49794238683127573,"project, or criterion, they receive feedback indicating the correct choice and have to wait for a short
224"
IMPROVING HUMAN PROJECT SELECTION,0.5,"time. The unpleasantness of having to wait serves as a penalty [6]. Otherwise, they receive positive
225"
IMPROVING HUMAN PROJECT SELECTION,0.5020576131687243,"reinforcement and the next choice is displayed. To receive positive reinforcement, the learner must
226"
IMPROVING HUMAN PROJECT SELECTION,0.5041152263374485,"select a query whose VOC is sufficiently close, as determined by a tolerance parameter t, to the VOC
227"
IMPROVING HUMAN PROJECT SELECTION,0.5061728395061729,"of the optimal action. We set the tolerance to t = 0.001.
228"
IMPROVING HUMAN PROJECT SELECTION,0.5082304526748971,"Our tutor teaches the strategy discovered by MGPS using a novel sophisticated training schedule,
229"
IMPROVING HUMAN PROJECT SELECTION,0.5102880658436214,"which fosters learning by incrementally increasing the complexity of the training task. This learning
230"
IMPROVING HUMAN PROJECT SELECTION,0.5123456790123457,"methodology is also known as shaping [31], and has been successfully applied to teach decision
231"
IMPROVING HUMAN PROJECT SELECTION,0.51440329218107,"strategies to humans [14]. Our training schedule varies the numbers of projects, how many different
232"
IMPROVING HUMAN PROJECT SELECTION,0.5164609053497943,"expert assessments learners have to choose between, and the specific types of expert assessments
233"
IMPROVING HUMAN PROJECT SELECTION,0.5185185185185185,"offered as choices. In total, our tutor teaches the discovered project selection strategy using ten
234"
IMPROVING HUMAN PROJECT SELECTION,0.5205761316872428,"training trials. The first seven training trials use a smaller version of the project selection task with
235"
IMPROVING HUMAN PROJECT SELECTION,0.522633744855967,"only two projects, while the last three trials use the full environment with five projects. The number
236"
IMPROVING HUMAN PROJECT SELECTION,0.5246913580246914,"of choices gradually increases throughout training from 1 in the first training trial to 9 in the last three
237"
IMPROVING HUMAN PROJECT SELECTION,0.5267489711934157,"training trials. The tutor varies the types of choices across trials. After an initial trial with only a
238"
IMPROVING HUMAN PROJECT SELECTION,0.5288065843621399,"single choice, the tutor offers choices that focus on different criteria within the same project for two
239"
IMPROVING HUMAN PROJECT SELECTION,0.5308641975308642,"trials. Then, the tutor offers choices that focus on different experts within the same project for two
240"
IMPROVING HUMAN PROJECT SELECTION,0.5329218106995884,"trials. The remaining trials combine both types of highlights while sometimes also featuring queries
241"
IMPROVING HUMAN PROJECT SELECTION,0.5349794238683128,"about different projects and also increasing the overall number of choices.
242"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5370370370370371,"5.2
Evaluating the effectiveness of MGPS Tutor in a training experiment
243"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5390946502057613,"To evaluate if AI-powered boosting can improve human project selection, we tested the MPGS tutor
244"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5411522633744856,"in a training experiment. We tested if people trained by MPGS tutor learn more resource-rational
245"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5432098765432098,"project selection strategies. To make our assessment task as naturalistic as possible, we modelled it
246"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5452674897119342,"on a real project selection problem that was faced by an Iranian financial institution [17]. We will
247"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5473251028806584,"first describe how we modeled this real-world problem, and then the training experiment.
248"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5493827160493827,"Table 1: Results of the human training experiment. Per condition, the normalized mean resource-
rationality score and the mean click agreement are reported. For both measures, we also report the
95% confidence interval under the Gaussian assumption (±1.96 standard errors)."
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.551440329218107,"Condition
RR-score
Click Agreement"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5534979423868313,"MGPS Tutor
0.3256 ± 0.0609
0.4271 ± 0.0201
No tutor
−0.0227 ± 0.0622
0.2521 ± 0.0171
Dummy tutor
0.0225 ± 0.0612
0.2664 ± 0.0159"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5555555555555556,"A project-selection problem from the real world
Khalili-Damghani and Sadi-Nezhad [17] worked
249"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5576131687242798,"on the real-world problem of helping a financial institution select between five potential projects with
250"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5596707818930041,"an eye to sustainability. Each project was evaluated by six advisors, who assigned scores from one to
251"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5617283950617284,"five on six different criteria. For our model of the task, we use the same number of experts, criteria,
252"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5637860082304527,"and projects, and the same criteria weights as the financial institution. The remaining parameters
253"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.565843621399177,"of the meta-level MDP were estimated as follows. We initialized the beliefs about the project’s
254"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5679012345679012,"attributes by calculating the mean and the standard deviation of all expert ratings for each criterion
255"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5699588477366255,"according to [17]. We estimated the reliability of each expert by calculating the standard deviation
256"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5720164609053497,"from the average distance of their ratings from the average rating of all other experts, weighted by the
257"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5740740740740741,"number of occurrences of each guess. We estimated the cost parameter λ of the meta-level MDP by
258"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5761316872427984,"λ =
cost
stakes · r(⊥) to align the meta-level MDP’s cost-reward ratio to its real-world equivalent. Using
259"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5781893004115226,"the expected termination reward of the environment r(⊥) = 3.4 and rough estimates for the stakes
260"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5802469135802469,"stakes = $10000000 and expert costs cost = $5000, this led to λ = 0.002. While [17] assumed all
261"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5823045267489712,"expert ratings are available for free, this is rarely the case. To make our test case more representative,
262"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5843621399176955,"we assumed that advisor evaluations are available on-request for a consulting fee. To capture that
263"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5864197530864198,"real-world decisions often have deadlines that limit how much information can be gathered, we set
264"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.588477366255144,"the maximum number of sequentially requested expert consultations to 5.
265"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5905349794238683,"Methods of the experiment
We recruited 301 participants for an online training experiment
266"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5925925925925926,"on Prolific. The average participant age was 29 years, and 148 participants identified as female.
267"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5946502057613169,"Participants were paid £3.50 for completing the experiment, plus an average bonus of £0.50. The
268"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5967078189300411,"median duration of the experiment was 22 minutes, resulting in a median pay of £10.9 per hour. Our
269"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.5987654320987654,"experiment was preregistered on AsPredicted and approved by the ethics commission of [removed]
270"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6008230452674898,"under IRB protocol [removed].
271"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.602880658436214,"Each participant was randomly assigned to one of three conditions: (1) the No tutor condition, in
272"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6049382716049383,"which participants did not receive any feedback and were free to discover efficient strategies on
273"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6069958847736625,"their own; (2) the MGPS tutor condition, in which participants practiced with our cognitive tutor
274"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6090534979423868,"that provided feedback on the resource-rationality score MGPS assigns to the selected planning
275"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6111111111111112,"actions; and (3) the Dummy tutor condition, an additional control condition in which participants
276"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6131687242798354,"practiced with a dummy tutor comparable to the MGPS tutor, albeit with randomized feedback on
277"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6152263374485597,"which planning actions are correct. All participants practiced their planning strategy in 10 training
278"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6172839506172839,"trials and were then evaluated across 10 test trials.
279"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6193415637860082,"We evaluated the participants’ performance using two measures: their RR-score and click agreement.
280"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6213991769547325,"RR-score’s are normalized by subtracting the average reward of a random baseline algorithm and
281"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6234567901234568,"dividing by the participant scores’ standard deviation. The random baseline algorithm is defined as the
282"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6255144032921811,"policy that chooses meta-level actions at random until the maximum number of decision operations is
283"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6275720164609053,"reached. Click agreement measures, how well participants learned to follow the near-optimal strategy
284"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6296296296296297,"discovered by our method. Specifically, we computed for each participant, which proportion of
285"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6316872427983539,"their information requests matched the action taken by the approximately resource-rational strategy
286"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6337448559670782,"discovered by MGPS.
287"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6358024691358025,"Experiment results
Table 1 shows the results of the experiment. To determine whether the condition
288"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6378600823045267,"of participants had a significant effect on the RR-score and click agreement, we used an ANOVA
289"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6399176954732511,"analysis with Box approximation [3]. The ANOVA revealed a significant effect of condition on
290"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6419753086419753,"both RR-score (F(1.99, 293.57) = 10.48, p < .0001) and click agreement (F(1.99, 291.48) = 15.5,
291"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6440329218106996,"p < .0001). We further compared the performance of participants in the MGPS tutor condition to
292"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6460905349794238,"participants in the two control conditions with post hoc ANOVA-type statistics and used Cohen’s d
293"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6481481481481481,"Table 2: Results of the performance evaluation. For each algorithm, we report the averge normalized
resource-rationality score (RR-Scores) and the runtime per decision problem. For both measures, we
also report the 95% confidence interval under the Gaussian assumption (±1.96 standard errors)."
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6502057613168725,"Algorithm
RR-score
Runtime (s)"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6522633744855967,"MGPS
0.9942 ± 0.0234
0.9079 ± 0.0052
PO-UCT (10 steps)
−0.4344 ± 0.0106
0.0175 ± 0.0004
PO-UCT (100 steps)
0.7309 ± 0.0302
0.1972 ± 0.0008
PO-UCT (1000 steps)
0.8681 ± 0.0256
2.3567 ± 0.0028
PO-UCT (5000 steps)
0.9054 ± 0.0232
10.8913 ± 0.0173"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.654320987654321,"[8] to evaluate the size of the effects. The post hoc tests revealed that participants in the MGPS tutor
294"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6563786008230452,"condition achieved a significantly higher RR-score than participants in the No tutor (F(1) = 17.88,
295"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6584362139917695,"p < .0001, d = .35) and Dummy tutor (F(1) = 13.4, p = .0002, d = .31) conditions. Additionally,
296"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6604938271604939,"participants in the MGPS tutor reached a higher click agreement with our pre-computed near optimal
297"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6625514403292181,"strategy than participants in the No tutor (F(1) = 25.08, p < .0001, d = .58) and Dummy tutor
298"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6646090534979424,"(F(1) = 19.3, p < .0001, d = .56) conditions.
299"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6666666666666666,"When evaluated on the same test trials and normalizing against the baseline reward and the standard
300"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.668724279835391,"deviation of the experiment results, MGPS achieves a mean reward of 1.17, demonstrating that
301"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6707818930041153,"MGPS discovers more resource-rational strategies than participants across all conditions. Although
302"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6728395061728395,"participants in the MGPS tutor condition performed significantly the better than participants in the
303"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6748971193415638,"other conditions, they did not learn to follow the strategy taught by the tutor exactly. Participants
304"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.676954732510288,"in the other conditions only discovered strategies with a similar RR-score to the random baseline
305"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6790123456790124,"strategy, with participants in the No tutor condition performing even worse than the random baseline
306"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6810699588477366,"strategy, and participants in the Dummy tutor condition outperforming the random baseline only by a
307"
EVALUATING THE EFFECTIVENESS OF MGPS TUTOR IN A TRAINING EXPERIMENT,0.6831275720164609,"small margin.
308"
PERFORMANCE EVALUATION,0.6851851851851852,"6
Performance evaluation
309"
PERFORMANCE EVALUATION,0.6872427983539094,"The results reported in the previous section show that MPGS can discover project selection strategies
310"
PERFORMANCE EVALUATION,0.6893004115226338,"that are more effective than the strategies people discover on their own. But how does its performance
311"
PERFORMANCE EVALUATION,0.691358024691358,"compare to other strategy discovery algorithms? To answer this question, we evaluated MGPS on
312"
PERFORMANCE EVALUATION,0.6934156378600823,"a computational benchmark. We chose PO-UCT [29] for comparisons because it is an established
313"
PERFORMANCE EVALUATION,0.6954732510288066,"baseline for metareasoning algorithms in partially observable environments [14] and the more
314"
PERFORMANCE EVALUATION,0.6975308641975309,"specialized MGPO algorithm is not applicable to project selection. PO-UCT utilizes Monte Carlo tree
315"
PERFORMANCE EVALUATION,0.6995884773662552,"search to simulate the effects of different actions, resulting in more accurate results with increased
316"
PERFORMANCE EVALUATION,0.7016460905349794,"computation time, making it a useful baseline for MGPS’s computational efficiency and performance.
317"
PERFORMANCE EVALUATION,0.7037037037037037,"Method
We evaluated the effectiveness of our method in the project selection task by comparing it
318"
PERFORMANCE EVALUATION,0.7057613168724279,"against PO-UCT [29] with different numbers of steps. All methods were evaluated across the same
319"
PERFORMANCE EVALUATION,0.7078189300411523,"5000 randomly generated instances of the project selection environment.
320"
PERFORMANCE EVALUATION,0.7098765432098766,"Our main performance measure was the resource-rationality score (RR-Score defined in Equation 1).
321"
PERFORMANCE EVALUATION,0.7119341563786008,"To highlight the achieved improvements over a baseline algorithm that performs random meta-level
322"
PERFORMANCE EVALUATION,0.7139917695473251,"actions, we normalized the reported RR-scores. Specifically, we applied a z-score transformation,
323"
PERFORMANCE EVALUATION,0.7160493827160493,"subtracting the average reward of the random baseline algorithm (see Section 5.2) from the RR-
324"
PERFORMANCE EVALUATION,0.7181069958847737,"Scores and dividing by the evaluated algorithm’s RR-Scores’ standard deviation. We analyze the
325"
PERFORMANCE EVALUATION,0.720164609053498,"differences in RR-Scores with an ANOVA and evaluate the size of statistical effects with Cohen’s d
326"
PERFORMANCE EVALUATION,0.7222222222222222,"[8]. Additionally, we compare the computational efficiency of the different methods, which is crucial
327"
PERFORMANCE EVALUATION,0.7242798353909465,"for being able to provide real-time feedback in our cognitive tutor.
328"
PERFORMANCE EVALUATION,0.7263374485596708,"Results
As shown in Table 2, MGPS outperformed all tested versions of PO-UCT and the random
329"
PERFORMANCE EVALUATION,0.7283950617283951,"baseline strategy. 1 An ANOVA revealed significant differences in the RR-scores of the strategies
330"
PERFORMANCE EVALUATION,0.7304526748971193,"discovered by the different methods (F(4, 24995) = 2447, p < .0001). Hukey-HSD post-hoc
331"
PERFORMANCE EVALUATION,0.7325102880658436,"1As the RR-scores are normalized by subtracting the mean RR-score of the random baseline, the random
baseline strategy itself has a normalized RR-score of 0."
PERFORMANCE EVALUATION,0.7345679012345679,"comparisons indicated that the strategies discovered by MGPS are significantly more resource-
332"
PERFORMANCE EVALUATION,0.7366255144032922,"rational than the strategies discovered by PO-UCT with 10 steps (p < .0001, d = 2.18), 100 steps
333"
PERFORMANCE EVALUATION,0.7386831275720165,"(p < .0001, d = .27), 1000 steps (p < .0001, d = .14), or 5000 steps (p < .0001, d = .11).
334"
PERFORMANCE EVALUATION,0.7407407407407407,"While MGPS achieves significantly higher RR-scores than all PO-UCT variants, the size of the effect
335"
PERFORMANCE EVALUATION,0.742798353909465,"decreases from a very large effect to a small effect when increasing PO-UCT’s computational budget
336"
PERFORMANCE EVALUATION,0.7448559670781894,"sufficiently. We therefore expect that PO-UCT with a much more than 5000 steps would ultimately
337"
PERFORMANCE EVALUATION,0.7469135802469136,"achieve comparable RR-scores to MGPO, albeit at a much higher computational cost. Moreover,
338"
PERFORMANCE EVALUATION,0.7489711934156379,"MGPS was substantially faster than PO-UCT with a computational budget of 1000 steps or more,
339"
PERFORMANCE EVALUATION,0.7510288065843621,"which is when PO-UCT’s performance starts to approach that of MGPS. With a computational budget
340"
PERFORMANCE EVALUATION,0.7530864197530864,"of 100 steps or fewer, PO-UCT is faster than MGPS. However, such a small computational budget is
341"
PERFORMANCE EVALUATION,0.7551440329218106,"not enough for PO-UCT to discover strategies with a RR-score anywhere near that of the strategy
342"
PERFORMANCE EVALUATION,0.757201646090535,"discovered by MGPS. Critically, the high amount of computation required for PO-UCT to achieve an
343"
PERFORMANCE EVALUATION,0.7592592592592593,"approximately similar level of resource-rationality would render PO-UCT unusable for a cognitive
344"
PERFORMANCE EVALUATION,0.7613168724279835,"tutor that computes feedback in real time.
345"
CONCLUSION,0.7633744855967078,"7
Conclusion
346"
CONCLUSION,0.7654320987654321,"People’s decision-making is prone to systematic errors [16], and although people are happy to delegate
347"
CONCLUSION,0.7674897119341564,"some decisions, most CEOs are unlikely to let AI decide which projects their company should work
348"
CONCLUSION,0.7695473251028807,"on. Moreover, people are reluctant to use the more accurate technical decision procedures because
349"
CONCLUSION,0.7716049382716049,"they tend to be more tedious [28, 21, 18]. Motivated by people’s insistence on making their own
350"
CONCLUSION,0.7736625514403292,"decisions with simple heuristics, we leveraged AI to discover and teach decision strategies that
351"
CONCLUSION,0.7757201646090535,"perform substantially better than people’s intuitive strategies but are nevertheless simple enough that
352"
CONCLUSION,0.7777777777777778,"people use them. To this end, we introduced a metareasoning method for leveraging AI to discover
353"
CONCLUSION,0.779835390946502,"optimal decision strategies for human project selection. Modeling project selection through the lens
354"
CONCLUSION,0.7818930041152263,"of resource rationality allowed is to formulate a mathematically precise criterion for the quality of
355"
CONCLUSION,0.7839506172839507,"decision strategies for human project selection. We further develop an efficient automatic strategy
356"
CONCLUSION,0.7860082304526749,"discovery algorithm automatically discovers efficient strategies for human project selection. Our
357"
CONCLUSION,0.7880658436213992,"algorithm discovered decision strategies that are much more resource-rational than the strategies
358"
CONCLUSION,0.7901234567901234,"humans discovered on their own and the strategies discovered by a general-purpose algorithm for
359"
CONCLUSION,0.7921810699588477,"solving POMDPs (PO-UCT). Using the efficient decision strategies discovered by our algorithm,
360"
CONCLUSION,0.7942386831275721,"we create a cognitive tutor that uses a shaping schedule and metacognitive feedback to teach the
361"
CONCLUSION,0.7962962962962963,"strategies to humans. In the training experiment, our cognitive tutor fostered significant improvements
362"
CONCLUSION,0.7983539094650206,"in participants’ resource rationality.
363"
CONCLUSION,0.8004115226337448,"A main limitation of our method is that it is unknown how precisely the environment parameters
364"
CONCLUSION,0.8024691358024691,"need to be estimated to construct the metareasoning task, which can prove especially problematic
365"
CONCLUSION,0.8045267489711934,"when there isn’t much data on past decisions. Future work could investigate and potentially address
366"
CONCLUSION,0.8065843621399177,"it by extending MGPS with a Bayesian inference approach to estimate the environment structure.
367"
CONCLUSION,0.808641975308642,"Encouraged by the promising results from successfully teaching humans in our naturalistic model
368"
CONCLUSION,0.8106995884773662,"of project selection, we are excited about future work assessing the real-world impact of improving
369"
CONCLUSION,0.8127572016460906,"people’s decision-making by evaluating their decisions directly in the real world. Additionally,
370"
CONCLUSION,0.8148148148148148,"we are also excited about potential future work that combines MGPS with AI-Interpret [32] to
371"
CONCLUSION,0.8168724279835391,"automatically derive human-legible recommendations for how to make project selection decisions.
372"
CONCLUSION,0.8189300411522634,"Lastly, although MGPS performed very well on our benchmarks, MGPS’s myopic approximation
373"
CONCLUSION,0.8209876543209876,"could fail in more complicated scenarios with interdependent criteria. Such challenges could be
374"
CONCLUSION,0.823045267489712,"addressed by solving meta-level MDPs with methods from deep reinforcement learning, for example
375"
CONCLUSION,0.8251028806584362,"by utilizing AlphaZero [30].
376"
CONCLUSION,0.8271604938271605,"Our results indicate that it is possible to use resource-rational analysis combined with automatic
377"
CONCLUSION,0.8292181069958847,"strategy discovery to improve human decision-making in a realistic project selection problem. As
378"
CONCLUSION,0.831275720164609,"selecting projects is a common problem faced by both organizations and individuals, improving their
379"
CONCLUSION,0.8333333333333334,"decision strategies in this setting would have a direct positive impact. For example, a project-selection
380"
CONCLUSION,0.8353909465020576,"tutor could be integrated into MBA programs to teach future decision-makers efficient decision
381"
CONCLUSION,0.8374485596707819,"strategies as part of their education. We are optimistic that our general methodology is also applicable
382"
CONCLUSION,0.8395061728395061,"to other real-world problems, offering a promising pathway to teach people efficient strategies for
383"
CONCLUSION,0.8415637860082305,"making better decisions in other areas as well. Besides project selection problems, we believe our
384"
CONCLUSION,0.8436213991769548,"approach could be used to improve real-world decision-making in areas such as career choice, grant
385"
CONCLUSION,0.845679012345679,"making, and public policy.
386"
REFERENCES,0.8477366255144033,"References
387"
REFERENCES,0.8497942386831275,"[1] F. Becker, J. Skirzy´nski, B. van Opheusden, and F. Lieder. Boosting human decision-making with
388"
REFERENCES,0.8518518518518519,"ai-generated decision aids. Computational Brain & Behavior, pages 1–24, 2022.
389"
REFERENCES,0.8539094650205762,"[2] S. Bonaccio and R. S. Dalal. Advice taking and decision-making: An integrative literature review, and
390"
REFERENCES,0.8559670781893004,"implications for the organizational sciences. Organizational behavior and human decision processes, 101
391"
REFERENCES,0.8580246913580247,"(2):127–151, 2006.
392"
REFERENCES,0.8600823045267489,"[3] G. E. Box et al. Some theorems on quadratic forms applied in the study of analysis of variance problems, i.
393"
REFERENCES,0.8621399176954733,"effect of inequality of variance in the one-way classification. The annals of mathematical statistics, 25(2):
394"
REFERENCES,0.8641975308641975,"290–302, 1954.
395"
REFERENCES,0.8662551440329218,"[4] F. Callaway, S. Gul, P. M. Krueger, T. L. Griffiths, and F. Lieder. Learning to select computations.
396"
REFERENCES,0.8683127572016461,"Uncertainty in Artificial Intelligence, 2018.
397"
REFERENCES,0.8703703703703703,"[5] F. Callaway, F. Lieder, P. Das, S. Gul, P. M. Krueger, and T. Griffiths. A resource-rational analysis of
398"
REFERENCES,0.8724279835390947,"human planning. In CogSci, 2018.
399"
REFERENCES,0.8744855967078189,"[6] F. Callaway, Y. R. Jain, B. van Opheusden, P. Das, G. Iwama, S. Gul, P. M. Krueger, F. Becker, T. L. Griffiths,
400"
REFERENCES,0.8765432098765432,"and F. Lieder. Leveraging artificial intelligence to improve people’s planning strategies. Proceedings of the
401"
REFERENCES,0.8786008230452675,"National Academy of Sciences of the United States of America, Mar. 2022. doi: 10.1073/pnas.2117432119.
402"
REFERENCES,0.8806584362139918,"URL https://www.pnas.org/doi/10.1073/pnas.2117432119.
403"
REFERENCES,0.8827160493827161,"[7] A. F. Carazo, I. Contreras, T. Gómez, and F. Pérez. A project portfolio selection problem in a group
404"
REFERENCES,0.8847736625514403,"decision-making context. Journal of Industrial & Management Optimization, 8(1):243, 2012. Publisher:
405"
REFERENCES,0.8868312757201646,"American Institute of Mathematical Sciences.
406"
REFERENCES,0.8888888888888888,"[8] J. Cohen. Statistical power analysis for the behavioral sciences. Academic press, 2013.
407"
REFERENCES,0.8909465020576132,"[9] S. Coldrick, C. Lawson, P. Ivey, and C. Lockwood. A decision framework for r&d project selection. In
408"
REFERENCES,0.8930041152263375,"IEEE International Engineering Management Conference, volume 1, page 413–418. IEEE, 2002.
409"
REFERENCES,0.8950617283950617,"[10] S. Consul, L. Heindrich, J. Stojcheski, and F. Lieder. Improving human decision-making by discovering
410"
REFERENCES,0.897119341563786,"efficient strategies for hierarchical planning. Computational Brain & Behavior, 5(2):185–216, 2022.
411"
REFERENCES,0.8991769547325102,"[11] D. G. B. de Souza, E. A. dos Santos, N. Y. Soma, and C. E. S. da Silva. MCDM-Based R&D Project
412"
REFERENCES,0.9012345679012346,"Selection: A Systematic Literature Review. Sustainability, 13(21):11626, 2021. Publisher: MDPI.
413"
REFERENCES,0.9032921810699589,"[12] F. Gino. Do we listen to advice just because we paid for it? the impact of advice cost on its use.
414"
REFERENCES,0.9053497942386831,"Organizational Behavior and Human Decision Processes, 107:234–245, 2008.
415"
REFERENCES,0.9074074074074074,"[13] N. Hay, S. Russell, D. Tolpin, and S. E. Shimony. Selecting computations: Theory and applications. arXiv
416"
REFERENCES,0.9094650205761317,"preprint arXiv:1408.2048, 2014.
417"
REFERENCES,0.911522633744856,"[14] L. Heindrich, S. Consul, and F. Lieder. Leveraging ai to improve human planning in large partially
418"
REFERENCES,0.9135802469135802,"observable environments. arXiv preprint arXiv:2302.02785, 2023.
419"
REFERENCES,0.9156378600823045,"[15] R. Hertwig and T. Grüne-Yanoff.
Nudging and boosting: Steering or empowering good decisions.
420"
REFERENCES,0.9176954732510288,"Perspectives on Psychological Science, 12(6):973–986, 2017.
421"
REFERENCES,0.9197530864197531,"[16] D. Kahneman, S. P. Slovic, P. Slovic, and A. Tversky. Judgment under uncertainty: Heuristics and biases.
422"
REFERENCES,0.9218106995884774,"Cambridge university press, 1982.
423"
REFERENCES,0.9238683127572016,"[17] K. Khalili-Damghani and S. Sadi-Nezhad. A hybrid fuzzy multiple criteria group decision making approach
424"
REFERENCES,0.9259259259259259,"for sustainable project selection. Applied Soft Computing, 13(1):339–352, 2013.
425"
REFERENCES,0.9279835390946503,"[18] B. Kornfeld and S. Kara. Selection of Lean and Six Sigma projects in industry. International Journal of
426"
REFERENCES,0.9300411522633745,"Lean Six Sigma, 2013. Publisher: Emerald Group Publishing Limited.
427"
REFERENCES,0.9320987654320988,"[19] F. Lieder and T. L. Griffiths. Resource-rational analysis: understanding human cognition as the optimal
428"
REFERENCES,0.934156378600823,"use of limited computational resources. Behavioral and Brain Sciences, 43, 2020.
429"
REFERENCES,0.9362139917695473,"[20] F. Lieder, F. Callaway, Y. Jain, P. Krueger, P. Das, S. Gul, and T. Griffiths. A cognitive tutor for helping
430"
REFERENCES,0.9382716049382716,"people overcome present bias. In RLDM 2019, 2019.
431"
REFERENCES,0.9403292181069959,"[21] F. Liu, W.-d. Zhu, Y.-w. Chen, D.-l. Xu, and J.-b. Yang. Evaluation, ranking and selection of R&D projects
432"
REFERENCES,0.9423868312757202,"by multiple experts: an evidential reasoning rule based approach. Scientometrics, 111(3):1501–1519, 2017.
433"
REFERENCES,0.9444444444444444,"Publisher: Springer.
434"
REFERENCES,0.9465020576131687,"[22] A. Mehta, Y. R. Jain, A. Kemtur, J. Stojcheski, S. Consul, M. Toši´c, and F. Lieder. Leveraging machine
435"
REFERENCES,0.948559670781893,"learning to automatically derive robust decision strategies from imperfect knowledge of the real world.
436"
REFERENCES,0.9506172839506173,"Computational Brain & Behavior, 5(3):343–377, 2022.
437"
REFERENCES,0.9526748971193416,"[23] V. Mohagheghi, S. M. Mousavi, J. Antucheviˇcien˙e, and M. Mojtahedi. Project portfolio selection problems:
438"
REFERENCES,0.9547325102880658,"a review of models, uncertainty approaches, solution techniques, and case studies. Technological and
439"
REFERENCES,0.9567901234567902,"Economic Development of Economy, 25(6):1380–1412, 2019.
440"
REFERENCES,0.9588477366255144,"[24] K. Olsen, A. Roepstorff, and D. Bang. Knowing whom to learn from: individual differences in metacogni-
441"
REFERENCES,0.9609053497942387,"tion and weighting of social information. PsyArXiv, 2019.
442"
REFERENCES,0.9629629629629629,"[25] D. Ronayne, D. Sgroi, et al. Ignoring good advice. University of Warwick, Centre for Competitive
443"
REFERENCES,0.9650205761316872,"Advantage in the Global ..., 2019.
444"
REFERENCES,0.9670781893004116,"[26] S. Russell and E. Wefald. Principles of metareasoning. Artificial intelligence, 49(1–3):361–395, 1991.
445"
REFERENCES,0.9691358024691358,"[27] S. Sadi-Nezhad. A state-of-art survey on project selection using mcdm techniques. Journal of Project
446"
REFERENCES,0.9711934156378601,"Management, 2(1):1–10, 2017.
447"
REFERENCES,0.9732510288065843,"[28] R. L. Schmidt and J. R. Freeland. Recent progress in modeling R&D project-selection processes. IEEE
448"
REFERENCES,0.9753086419753086,"Transactions on Engineering Management, 39(2):189–201, 1992. Publisher: IEEE.
449"
REFERENCES,0.977366255144033,"[29] D. Silver and J. Veness. Monte-carlo planning in large pomdps. Advances in neural information processing
450"
REFERENCES,0.9794238683127572,"systems, 23, 2010.
451"
REFERENCES,0.9814814814814815,"[30] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran,
452"
REFERENCES,0.9835390946502057,"T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a
453"
REFERENCES,0.98559670781893,"general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017. URL http://arxiv.org/abs/
454"
REFERENCES,0.9876543209876543,"1712.01815.
455"
REFERENCES,0.9897119341563786,"[31] B. Skinner. Shaping and maintaining operant behavior, pages 91–106. Free Press New York, 1953.
456"
REFERENCES,0.9917695473251029,"[32] J. Skirzy´nski, F. Becker, and F. Lieder. Automatic discovery of interpretable planning strategies. Machine
457"
REFERENCES,0.9938271604938271,"Learning, 110:2641–2683, 2021.
458"
REFERENCES,0.9958847736625515,"[33] I. Yaniv and E. Kleinberger. Advice taking in decision making: Egocentric discounting and reputation
459"
REFERENCES,0.9979423868312757,"formation. Organizational behavior and human decision processes, 83(2):260–281, 2000.
460"
