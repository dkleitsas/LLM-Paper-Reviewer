Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000651890482398957,"For a given additive cost function R (regularizer), a neuron is said to be in balance
1"
ABSTRACT,0.001303780964797914,"if the total cost of its input weights is equal to the total cost of its output weights.
2"
ABSTRACT,0.001955671447196871,"The basic example is provided by feedforward layered networks of ReLU units
3"
ABSTRACT,0.002607561929595828,"trained with L2 regularizers, which exhibit balance after proper training. We
4"
ABSTRACT,0.003259452411994785,"develop a general theory that extends this phenomenon in three broad directions
5"
ABSTRACT,0.003911342894393742,"in terms of: (1) activation functions; (2) regularizers, including all Lp (p > 0)
6"
ABSTRACT,0.0045632333767926985,"regularizers; and (3) architectures (non-layered, recurrent, convolutional, mixed
7"
ABSTRACT,0.005215123859191656,"activations). Gradient descent on the error function alone does not converge in
8"
ABSTRACT,0.005867014341590613,"general to a balanced state where every neuron is in balance, even when starting
9"
ABSTRACT,0.00651890482398957,"from a balanced state. However, gradient descent on the regularized error function
10"
ABSTRACT,0.007170795306388526,"must converge to a balanced state, and thus network balance can be used to assess
11"
ABSTRACT,0.007822685788787484,"learning progress. The theory is based on two local neuronal operations: scaling
12"
ABSTRACT,0.00847457627118644,"which is commutative, and balancing which is not commutative. Finally, and most
13"
ABSTRACT,0.009126466753585397,"importantly, given any initial set of weights, when local balancing operations are
14"
ABSTRACT,0.009778357235984355,"applied to each neuron in a stochastic manner, global order always emerges through
15"
ABSTRACT,0.010430247718383311,"the convergence of the stochastic algorithm to the same unique set of balanced
16"
ABSTRACT,0.01108213820078227,"weights. The reason for this convergence is the existence of an underlying strictly
17"
ABSTRACT,0.011734028683181226,"convex optimization problem where the relevant variables are constrained to a
18"
ABSTRACT,0.012385919165580182,"linear, only architecture-dependent, manifold. The theory is corroborated through
19"
ABSTRACT,0.01303780964797914,"simulations carried out on benchmark data sets. Balancing operations are entirely
20"
ABSTRACT,0.013689700130378096,"local and thus physically plausible in biological and neuromorphic networks.
21"
INTRODUCTION,0.014341590612777053,"1
Introduction
22"
INTRODUCTION,0.01499348109517601,"When large neural networks are trained on complex tasks, they produce large arrays of synaptic
23"
INTRODUCTION,0.01564537157757497,"weights that have no clear structure and are difficult to interpret. Thus finding any kind of structure in
24"
INTRODUCTION,0.016297262059973925,"the weights of large neural networks is of great interest. Here we study a particular kind of structure
25"
INTRODUCTION,0.01694915254237288,"we call neural synaptic balance and the conditions under which it emerges. Neural synaptic balance
26"
INTRODUCTION,0.017601043024771838,"is different from the biological notion of balance between excitation and inhibition [Froemke, 2015,
27"
INTRODUCTION,0.018252933507170794,"Field et al., 2020, Howes and Shatalina, 2022, Kim and Lee, 2022, Shirani and Choi, 2023]. We
28"
INTRODUCTION,0.018904823989569754,"use this term to refer to any systematic relationship between the input and output synaptic weights
29"
INTRODUCTION,0.01955671447196871,"of individual neurons or layers of neurons. Here we consider the case where the cost of the input
30"
INTRODUCTION,0.020208604954367666,"weights is equal to the cost of the output weights, where the cost is defined by some regularizer. One
31"
INTRODUCTION,0.020860495436766623,"of the most basic examples of such a relationship is when the sum of the squares of the input weights
32"
INTRODUCTION,0.02151238591916558,"of a neuron is equal to the sum of the squares of its output weights.
33"
INTRODUCTION,0.02216427640156454,"Basic Example: The basic example where this happens is with a neuron with a ReLU activation
34"
INTRODUCTION,0.022816166883963495,"function inside a network trained to minimize an error function with L2 regularization. If we multiply
35"
INTRODUCTION,0.02346805736636245,"the incoming weights of the neuron by some λ > 0 (including the bias) and divide the outgoing
36"
INTRODUCTION,0.024119947848761408,"weights of the neuron by the same λ, it is easy to see that this scaling operation does not affect in any
37"
INTRODUCTION,0.024771838331160364,"way the contribution of the neuron to the rest of the network. Thus, the component of the overall
38"
INTRODUCTION,0.025423728813559324,"error function that depends only on the input-output function of the network is unchanged. However,
39"
INTRODUCTION,0.02607561929595828,"the value of the L2 regularizer changes with λ and we can ask what is the value of λ that minimizes
40"
INTRODUCTION,0.026727509778357236,"the corresponding contribution given by:
41 X"
INTRODUCTION,0.027379400260756193,"i∈IN
(λwi)2 +
X"
INTRODUCTION,0.02803129074315515,"i∈OUT
(wi/λ)2 = λ2A + 1"
INTRODUCTION,0.028683181225554105,"λ2 B
(1.1)"
INTRODUCTION,0.029335071707953065,"where IN and OUT denote the set of incoming and outgoing weights respectively, A = P"
INTRODUCTION,0.02998696219035202,"i∈IN w2
i ,
42"
INTRODUCTION,0.030638852672750978,and B = P
INTRODUCTION,0.03129074315514994,"i∈OUT w2
i . The product of the two terms on the right-hand side of Equation 1.1 is equal to
43"
INTRODUCTION,0.031942633637548894,"AB and does not depend on λ. Thus, the minimum is achieved when these two terms are equal, which
44"
INTRODUCTION,0.03259452411994785,"yields: (λ∗)4 = B/A for the optimal λ∗. The corresponding new set of weights, vi = λ∗wi for the
45"
INTRODUCTION,0.033246414602346806,"input weights and vi = wi/λ∗for the outgoing weights, must be balanced: P"
INTRODUCTION,0.03389830508474576,"i∈IN v2
i = P"
INTRODUCTION,0.03455019556714472,"i∈OUT v2
i .
46"
INTRODUCTION,0.035202086049543675,"This is because its optimal scaling factor can only be λ∗= 1. Thus, we can define two operations
47"
INTRODUCTION,0.03585397653194263,"that can be applied to the incoming and outgoing weights of a neuron: scaling and balancing. It
48"
INTRODUCTION,0.03650586701434159,"is easy to check that scaling operations applied to any two neurons commute, whereas balancing
49"
INTRODUCTION,0.037157757496740544,"operations do not commute if the two neurons are directly connected (Appendix). If a network of
50"
INTRODUCTION,0.03780964797913951,"ReLU neurons is properly trained using a standard error function with an L2 regularizer, at the end of
51"
INTRODUCTION,0.038461538461538464,"training one observes a remarkable phenomenon: for each ReLU neuron, the norm of the incoming
52"
INTRODUCTION,0.03911342894393742,"synaptic weights is approximately equal to the norm of the outgoing synaptic weights, i.e. every
53"
INTRODUCTION,0.039765319426336376,"neuron is balanced.
54"
INTRODUCTION,0.04041720990873533,"There have been isolated previous studies of this kind of synaptic balance [Du et al., 2018, Stock
55"
INTRODUCTION,0.04106910039113429,"et al., 2022] under special conditions. For instance, in Du et al. [2018], it is shown that if a deep
56"
INTRODUCTION,0.041720990873533245,"network is initialized in a balanced state with respect to the sum of squares metric, and if training
57"
INTRODUCTION,0.0423728813559322,"progresses with an infinitesimal learning rate, then balance is preserved throughout training. Here,
58"
INTRODUCTION,0.04302477183833116,"we take a different approach aimed at uncovering the generality of neuronal balance phenomena,
59"
INTRODUCTION,0.043676662320730114,"the learning conditions under which they occur, as well as new local balancing algorithms and their
60"
INTRODUCTION,0.04432855280312908,"convergence properties. We study neural synaptic balance in its generality in terms of activation
61"
INTRODUCTION,0.044980443285528034,"functions, regularizers, network architectures, and training stages. In particular, we systematically
62"
INTRODUCTION,0.04563233376792699,"answer questions such as: Why does balance occur? Does it occur only with ReLU neurons? Does it
63"
INTRODUCTION,0.046284224250325946,"occur only with L2 regularizers? Does it occur only in fully connected feedforward architectures?
64"
INTRODUCTION,0.0469361147327249,"Does it occur only at the end of training? And what happens if we balance neurons at random in a
65"
INTRODUCTION,0.04758800521512386,"large network?
66"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.048239895697522815,"2
Generalization of the Activation Functions
67"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.04889178617992177,"What enables scaling ReLU neurons without changing their input-output function is the homogeneous
68"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.04954367666232073,"property of ReLU activation function. An activation function f is said to be homogeneous if for every
69"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.050195567144719684,"λ > 0, f(λx) = λf(x). To fully characterize the class of homogeneous activation functions, we first
70"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.05084745762711865,"define a new class of activation functions, corresponding to bilinear units (BiLU), consisting of two
71"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.051499348109517604,"half-lines meeting at the origin.
72"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.05215123859191656,"Definition 2.1. (BiLU) A neuronal activation function f : R →R is bilinear (BiLU) if and only if
73"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.052803129074315516,"f(x) = ax when x < 0, and f(x) = bx when x ≥0, for some fixed parameters a and b in R.
74"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.05345501955671447,"BiLU units include linear units (a = b), ReLU units (a = 0, b = 1), leaky ReLU (a = ϵ; b = 1) units,
75"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.05410691003911343,"and symmetric linear units (a = −b), all of which can also be viewed as special cases of piece-wise
76"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.054758800521512385,"linear units [Tavakoli et al., 2021], with a single hinge. One advantage of ReLU and more generally
77"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.05541069100391134,"BiLU neurons, which is very important during backpropagation learning, is that their derivative is
78"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.0560625814863103,"very simple and can only take one of two values (a or b). We have the following equivalence.
79"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.056714471968709254,"Proposition 2.2. A neuronal activation function f : R →R is homogeneous if and only if it is a
80"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.05736636245110821,"BiLU activation function.
81"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.058018252933507174,"Proof. Every function in BiLU is clearly homogeneous. Conversely, any homogeneous function f
82"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.05867014341590613,"must satisfy: (1) f(0x) = 0f(x) = f(0) = 0; (2)f(x) = f(1x) = f(1)x for any positive x; and (3)
83"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.059322033898305086,"f(x) = f(−u) = f(−1)u = −f(−1)x for any negative x. Thus f is in BiLU with a = −f(−1)
84"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.05997392438070404,"and b = f(1).
85"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.060625814863103,"In the Appendix, we provide a simple proof that networks of BiLU neurons, even with a single hidden
86"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.061277705345501955,"layer, have universal approximation properties.
87"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.06192959582790091,"While in the rest of this work we use BiLU neurons, it is possible to generalize the notions of scaling
88"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.06258148631029987,"and balancing even further. To see this, suppose that there is a neuron with an activation function
89"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.06323337679269883,"f : R →R, and functions g : (a, b) →R and h : (a, b) →R, such that: f(g(λ)x) = h(λ)f(x),
90"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.06388526727509779,"for any λ ∈(a, b). Then if we multiply the incoming weights by g(λ) and divide the outgoing
91"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.06453715775749674,"weights by h(λ) ̸= 0 (generalized scaling), we see again that the influence of the neuron on the
92"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.0651890482398957,"rest of the network is unchanged. And thus, again, we can try to find the value of λ that minimizes
93"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.06584093872229466,"the regularization cost (generalized balancing). Here we provide an example of such an activation
94"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.06649282920469361,"function, with g(λ) = λ and h(λ) = λc. Additional details are given in the Appendix.
95"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.06714471968709257,"Proposition 2.3. The set of activation functions f satisfying f(λx) = λcf(x) for any x ∈R and
96"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.06779661016949153,"any λ > 0 consist of the functions of the form:
97"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.06844850065189048,"f(x) =
Cxc
if
x ≥0
Dxc
if
x < 0.
(2.1)"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.06910039113428944,"where c ∈R, C = f(1) ∈R, and D = f(−1) ∈R. We call these bi-power units (BiPU). If, in
98"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.0697522816166884,"addition, we want f to be continuous at 0, we must have either c > 0, or c = 0 with C = D.
99"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.07040417209908735,"Note that in the general case where c > 0, C and D do not need to be equal. In particular, one of
100"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.0710560625814863,"them can be equal to zero, and the other one can be different from zero giving rise to rectified power
101"
GENERALIZATION OF THE ACTIVATION FUNCTIONS,0.07170795306388526,"units.
102"
GENERALIZATION OF THE REGULARIZERS,0.07235984354628422,"3
Generalization of the Regularizers
103"
GENERALIZATION OF THE REGULARIZERS,0.07301173402868318,"As we have seen, given a BiLU neuron, scaling its input and output weights by λ and 1/λ respectively
104"
GENERALIZATION OF THE REGULARIZERS,0.07366362451108213,"does not alter its contribution to the rest of the network and thus we can adjust λ to reduce or even
105"
GENERALIZATION OF THE REGULARIZERS,0.07431551499348109,"minimize the contribution of the corresponding weights to the regularizer. It is reasonable to assume
106"
GENERALIZATION OF THE REGULARIZERS,0.07496740547588006,that the regularizer has the general additive form: R(W) = P
GENERALIZATION OF THE REGULARIZERS,0.07561929595827901,"w gw(w) where W denotes all the
107"
GENERALIZATION OF THE REGULARIZERS,0.07627118644067797,"weights in the network. Without much loss of generality, we can assume that the gw are continuous,
108"
GENERALIZATION OF THE REGULARIZERS,0.07692307692307693,"and lower-bounded by 0. To ensure the existence and uniqueness of a minimum during the balancing
109"
GENERALIZATION OF THE REGULARIZERS,0.07757496740547588,"of any neuron, We will assume that each function gw depends only on the magnitude |w| of the
110"
GENERALIZATION OF THE REGULARIZERS,0.07822685788787484,"corresponding weight, and that gw monotonically increases from 0 to +∞. Clearly, L2, L1 and
111"
GENERALIZATION OF THE REGULARIZERS,0.0788787483702738,"more generally all Lp regularizers are special cases where, for p > 0, Lp regularization is defined
112"
GENERALIZATION OF THE REGULARIZERS,0.07953063885267275,by: R(W) = P
GENERALIZATION OF THE REGULARIZERS,0.08018252933507171,"w |w|p. Differentiability conditions can be added to be able to derive closed form
113"
GENERALIZATION OF THE REGULARIZERS,0.08083441981747067,"solutions for the balance (optimal scaling). This is satisfied by all forms of Lp regularization, for
114"
GENERALIZATION OF THE REGULARIZERS,0.08148631029986962,"p > 0. We have the following theorem.
115"
GENERALIZATION OF THE REGULARIZERS,0.08213820078226858,"Theorem 3.1. (Balance and Regularizer Minimization) Assume an additive regularizer with the
116"
GENERALIZATION OF THE REGULARIZERS,0.08279009126466753,"properties described above, where in addition we assume that the functions gw are continuously
117"
GENERALIZATION OF THE REGULARIZERS,0.08344198174706649,"differentiable, except perhaps at the origin. Then, for any neuron, there exists one optimal value λ∗
118"
GENERALIZATION OF THE REGULARIZERS,0.08409387222946545,"that minimizes R(W). This value must be a solution of the consistency equation:
119 λ2
X"
GENERALIZATION OF THE REGULARIZERS,0.0847457627118644,"w∈IN(i)
wg′
w(λw) =
X"
GENERALIZATION OF THE REGULARIZERS,0.08539765319426336,"w∈OUT (i)
wg′
w(w/λ)
(3.1)"
GENERALIZATION OF THE REGULARIZERS,0.08604954367666232,"Once the weights are rebalanced accordingly, the new weights must satisfy the generalized balance
120"
GENERALIZATION OF THE REGULARIZERS,0.08670143415906127,"equation:
121 X"
GENERALIZATION OF THE REGULARIZERS,0.08735332464146023,"w∈IN(i)
wg′(w) =
X"
GENERALIZATION OF THE REGULARIZERS,0.0880052151238592,"w∈OUT (i)
wg′(w)
(3.2)"
GENERALIZATION OF THE REGULARIZERS,0.08865710560625815,"In particular, if gw(w) = |w|p for all the incoming and outgoing weights of neuron i, then the optimal
122"
GENERALIZATION OF THE REGULARIZERS,0.08930899608865711,"value λ∗is unique and equal to:
123"
GENERALIZATION OF THE REGULARIZERS,0.08996088657105607,"λ∗=
P"
GENERALIZATION OF THE REGULARIZERS,0.09061277705345502,"w∈OUT (i) |w|p
P"
GENERALIZATION OF THE REGULARIZERS,0.09126466753585398,w∈IN(i) |w|p
GENERALIZATION OF THE REGULARIZERS,0.09191655801825294,"1/2p
=
||OUT(i)||p"
GENERALIZATION OF THE REGULARIZERS,0.09256844850065189,||IN(i)||p
GENERALIZATION OF THE REGULARIZERS,0.09322033898305085,"1/2
(3.3)"
GENERALIZATION OF THE REGULARIZERS,0.0938722294654498,"After balancing, the decrease ∆R ≥0 in the value of the Lp regularizer R = P"
GENERALIZATION OF THE REGULARIZERS,0.09452411994784876,"w |w|p is given by:
124"
GENERALIZATION OF THE REGULARIZERS,0.09517601043024772,"∆R =
 
X"
GENERALIZATION OF THE REGULARIZERS,0.09582790091264667,"w∈IN(i)
|w|p1/2 −
 
X"
GENERALIZATION OF THE REGULARIZERS,0.09647979139504563,"w∈OUT (i)
|w|p1/2
2
(3.4)"
GENERALIZATION OF THE REGULARIZERS,0.09713168187744459,"After balancing neuron i, its new weights satisfy the generalized Lp balance equation:
125 X"
GENERALIZATION OF THE REGULARIZERS,0.09778357235984354,"w∈IN(i)
|w|p =
X"
GENERALIZATION OF THE REGULARIZERS,0.0984354628422425,"w∈OUT (i)
|w|p
(3.5)"
GENERALIZATION OF THE REGULARIZERS,0.09908735332464146,"Proof. The results are obtained by setting the derivative of the regularizer with respect to the scaling
126"
GENERALIZATION OF THE REGULARIZERS,0.09973924380704041,"factor λ to 0. Note that the theorem applies to regularizers combining different Lp’s (e.g. of the form
127"
GENERALIZATION OF THE REGULARIZERS,0.10039113428943937,"$alphaL2 + βL1). The details are given in the Appendix.
128"
GENERALIZATION OF THE ARCHITECTURES,0.10104302477183832,"4
Generalization of the Architectures
129"
GENERALIZATION OF THE ARCHITECTURES,0.1016949152542373,"It is straightforward to check that the scaling and balancing operations can be extended in the
130"
GENERALIZATION OF THE ARCHITECTURES,0.10234680573663625,"following cases (see Appendix for additional details):
131"
GENERALIZATION OF THE ARCHITECTURES,0.10299869621903521,"1. Mixed networks containing both BiLU and non-BiLU units. One can just restrict those
132"
GENERALIZATION OF THE ARCHITECTURES,0.10365058670143416,"operations to the BiLU neurons.
133"
GENERALIZATION OF THE ARCHITECTURES,0.10430247718383312,"2. Recurrent networks containing BiLU neurons, not just feedforward networks.
134"
GENERALIZATION OF THE ARCHITECTURES,0.10495436766623208,"3. Networks that are not layered, or not fully connected.
135"
GENERALIZATION OF THE ARCHITECTURES,0.10560625814863103,"4. In addition, scaling and balancing operations can be applied layer-wise to an entire layer of
136"
GENERALIZATION OF THE ARCHITECTURES,0.10625814863102999,"BiLU neurons in a tied manner, by using the same scaling factor λ with a single optimal
137"
GENERALIZATION OF THE ARCHITECTURES,0.10691003911342895,"value λ∗for all the neurons in the layer. In particular, this allows the application of scaling
138"
GENERALIZATION OF THE ARCHITECTURES,0.1075619295958279,"and balancing to convolutional layers of BiLU neurons.
139"
BALANCING ALGORITHMS,0.10821382007822686,"5
Balancing Algorithms
140"
BALANCING ALGORITHMS,0.10886571056062581,"Gradient Descent: When a network of BiLU neurons is trained by gradient descent to minimize
141"
BALANCING ALGORITHMS,0.10951760104302477,"an error function E(W), such as the negative log-likelihood of the data, there is no reason for the
142"
BALANCING ALGORITHMS,0.11016949152542373,"final weights to be balanced. However, when a network is properly trained to minimize a regularized
143"
BALANCING ALGORITHMS,0.11082138200782268,"error function E = E(W) + R(W), the final weights ought to be balanced. The reason is that if a
144"
BALANCING ALGORITHMS,0.11147327249022164,"neuron is not in a balanced state at the end of training, then we can further reduce its contribution to
145"
BALANCING ALGORITHMS,0.1121251629726206,"R smoothly by balancing it. This implies that the gradient of E(W) is not equal to zero at the end of
146"
BALANCING ALGORITHMS,0.11277705345501955,"training, and thus training has not properly converged. The converse is that the degree of balance can
147"
BALANCING ALGORITHMS,0.11342894393741851,"be used as a proxy for assessing whether learning has converged or not.
148"
BALANCING ALGORITHMS,0.11408083441981746,"Stochastic Balancing: More interestingly, we now investigate what happens if we fix the weights W
149"
BALANCING ALGORITHMS,0.11473272490221642,"of a network and iteratively balance its BiLU neurons.
150"
BALANCING ALGORITHMS,0.11538461538461539,"Theorem 5.1. (Convergence of Stochastic Balancing) Consider a network of BiLU neurons with
151"
BALANCING ALGORITHMS,0.11603650586701435,"an error function E(W) = E(W) + R(W) where R is any Lp (p > 0) regularizer. Let W denote
152"
BALANCING ALGORITHMS,0.1166883963494133,"the initial weights. When the neuronal stochastic balancing algorithm is applied throughout the
153"
BALANCING ALGORITHMS,0.11734028683181226,"network so that every neuron is visited from time to time, then E(W) remains unchanged but R(W)
154"
BALANCING ALGORITHMS,0.11799217731421122,"must converge to some finite value that is less or equal to the initial value, strictly less if the initial
155"
BALANCING ALGORITHMS,0.11864406779661017,"weights are not balanced. In addition, for every neuron i, λ∗
i (t) →1 and the weights themselves must
156"
BALANCING ALGORITHMS,0.11929595827900913,"converge to a limit W ∗which is globally balanced, with E(W) = E(W ∗) and R(W) ≥R(W ∗),
157"
BALANCING ALGORITHMS,0.11994784876140809,"and with equality if only if W is already balanced. Finally, W ∗is unique as it corresponds to the
158"
BALANCING ALGORITHMS,0.12059973924380704,"solution of a strictly convex optimization problem with special linear constraints that depend only on
159"
BALANCING ALGORITHMS,0.121251629726206,"the network architecture (and not on W). Stochastic balancing projects to stochastic trajectories in
160"
BALANCING ALGORITHMS,0.12190352020860495,"the linear manifold that run from the origin to the unique optimal configuration.
161"
BALANCING ALGORITHMS,0.12255541069100391,"Proof. Each individual balancing operation leaves E(W) unchanged because the BiLU neurons are
162"
BALANCING ALGORITHMS,0.12320730117340287,"homogeneous. Furthermore, each balancing operation reduces the regularization error R(W), or
163"
BALANCING ALGORITHMS,0.12385919165580182,"leaves it unchanged. Since the regularizer is lower-bounded by zero, the value of the regularizer must
164"
BALANCING ALGORITHMS,0.12451108213820078,"approach a limit as the stochastic updates are being applied. However, this alone does not imply
165"
BALANCING ALGORITHMS,0.12516297262059975,"Figure 1: Two hidden units (1 and 7) connected by two different directed paths 1-2-3-4-7 and 1-5-6-7 in a
BiLU network. Each unit i has a scaling factor Λi, and each directed edge from unit j to unit i has a scaling
factor Mij = Λi/Λj. The products of the Mij’s along each path is equal to: Λ2"
BALANCING ALGORITHMS,0.1258148631029987,"Λ1
Λ3
Λ2
Λ4
Λ3
Λ7
Λ4 = Λ5"
BALANCING ALGORITHMS,0.12646675358539766,"Λ1
Λ6
Λ5
Λ7
Λ6 = Λ7"
BALANCING ALGORITHMS,0.1271186440677966,"Λ1 .
Therefore the variables Lij = log Mij must satisfy the linear equation: L21 + L32 + L43 + L74 = L51 +
L65 + L76 =log Λ7 −log Λ1."
BALANCING ALGORITHMS,0.12777053455019557,"that the weights are converging and whether the limit is unique or not. To address these issues, for
166"
BALANCING ALGORITHMS,0.12842242503259452,"simplicity, we use a continuous time notation. After a certain time t each neuron has been balanced a
167"
BALANCING ALGORITHMS,0.1290743155149935,"certain number of times. While the balancing operations are not commutative as balancing operations,
168"
BALANCING ALGORITHMS,0.12972620599739243,"they are commutative as scaling operations. Thus we can reorder the scaling operations and group
169"
BALANCING ALGORITHMS,0.1303780964797914,"them neuron by neuron so that, for instance, neuron i has been scaled by the sequence of scaling
170"
BALANCING ALGORITHMS,0.13102998696219034,"operations of the form:
171"
BALANCING ALGORITHMS,0.1316818774445893,"Sλ∗
1(i)Sλ∗
2(i) . . . Sλ∗nit(i) = SΛi(t)(i)
(5.1)"
BALANCING ALGORITHMS,0.13233376792698825,"where nit corresponds to the count of the last update of neuron i prior to time t, and:
172"
BALANCING ALGORITHMS,0.13298565840938723,"Λi(t) =
Y"
BALANCING ALGORITHMS,0.13363754889178617,"1≤n≤nit
λ∗
n(i)
(5.2)"
BALANCING ALGORITHMS,0.13428943937418514,"For the input and output units, we can consider that their balancing coefficients λ∗are always equal
173"
BALANCING ALGORITHMS,0.13494132985658408,"to 1 (at all times) and therefore Λi(t) = 1 for any visible unit i. At time t the weight connecting unit
174"
BALANCING ALGORITHMS,0.13559322033898305,"j to unit i is given by: wij(t) = wij(0)Λi(t)/Λj(t), where wij(0) corresponds to the initial value.
175"
BALANCING ALGORITHMS,0.13624511082138202,"In the Appendix, we show upfront that for all BiLU units i, Λi(t) converges to some limit Λi > 0,
176"
BALANCING ALGORITHMS,0.13689700130378096,"and thus the weights converge too. Here, we first suppose that the coefficients Λi(t) converge to
177"
BALANCING ALGORITHMS,0.13754889178617993,"some limit Λi, and recover the convergence at the end from understanding the overall proof. As a
178"
BALANCING ALGORITHMS,0.13820078226857888,"result, for any Lp regularizer, the coefficients Λi corresponding to a globally balanced state must be
179"
BALANCING ALGORITHMS,0.13885267275097785,"solutions of the following optimization problem:
180"
BALANCING ALGORITHMS,0.1395045632333768,"min
Λ R(Λ) =
X"
BALANCING ALGORITHMS,0.14015645371577576,"ij
| Λi"
BALANCING ALGORITHMS,0.1408083441981747,"Λj
wij|p
(5.3)"
BALANCING ALGORITHMS,0.14146023468057367,"under the simple constraints: Λi > 0 for all the BiLU hidden units, and Λi = 1 for all the visible (input
181"
BALANCING ALGORITHMS,0.1421121251629726,"and output) units. In this form, the problem is not convex. Introducing new variables Mj = 1/Λj
182"
BALANCING ALGORITHMS,0.14276401564537158,"is not sufficient to render the problem convex. Using variables Mij = Λi/Λj is better, but still
183"
BALANCING ALGORITHMS,0.14341590612777053,"problematic for 0 < p ≤1. However, let us instead introduce the new variables Lij = log(Λi/Λj).
184"
BALANCING ALGORITHMS,0.1440677966101695,"These are well defined since we know that Λi/Λj > 0. The objective now becomes:
185"
BALANCING ALGORITHMS,0.14471968709256844,"min R(L) =
X"
BALANCING ALGORITHMS,0.1453715775749674,"ij
|eLijwij|p =
X"
BALANCING ALGORITHMS,0.14602346805736635,"ij
epLij|wij|p
(5.4)"
BALANCING ALGORITHMS,0.14667535853976532,"This objective is strictly convex in the variables Lij, as a sum of strictly convex functions (exponen-
186"
BALANCING ALGORITHMS,0.14732724902216426,"tials). However, to show that it is a convex optimization problem we need to study the constraints
187"
BALANCING ALGORITHMS,0.14797913950456323,"on the variables Lij. In particular, from the set of Λi’s it is easy to construct a unique set of Lij.
188"
BALANCING ALGORITHMS,0.14863102998696218,"However what about the converse?
189"
BALANCING ALGORITHMS,0.14928292046936115,"Definition 5.2. A set of real numbers Lij, one per connection of a given neural architecture, is
190"
BALANCING ALGORITHMS,0.14993481095176012,"self-consistent if and only if there is a unique corresponding set of numbers Λi > 0 (one per unit)
191"
BALANCING ALGORITHMS,0.15058670143415906,"such that: Λi = 1 for all visible units and Lij = log Λi/Λj for every directed connection from a unit
192"
BALANCING ALGORITHMS,0.15123859191655803,"j to a unit i.
193 C
B A"
BALANCING ALGORITHMS,0.15189048239895697,"Figure 2: The problem of minimizing the strictly con-
vex regularizer R(Lij) = P"
BALANCING ALGORITHMS,0.15254237288135594,"ij epLij|wij|p (p > 0), over
the linear (hence convex) manifold of self-consistent con-
figurations defined by the linear constraints of the form
P"
BALANCING ALGORITHMS,0.15319426336375488,"π Lij = 0, where π runs over input-output paths. The
regularizer function depends on the weights. The linear
manifold depends only on the architecture, i.e., the graph
of connections. This is a strictly convex optimization prob-
lem with a unique solution associated with the point A. At
A the corresponding weights must be balanced, or else a
self-consistent configuration of lower cost could be found
by balancing any non-balanced neuron. Finally, any other
self-consistent configuration B cannot correspond to a bal-
anced state of the network, since there must exist balancing
moves that further reduce the regularizer cost (see main
text). Stochastic balancing produces random paths from the
origin, where Lij= log Mij = 0, to the unique optimum
point A."
BALANCING ALGORITHMS,0.15384615384615385,"Remark 5.3. This definition depends on the graph of connections, but not on the original values of
194"
BALANCING ALGORITHMS,0.1544980443285528,"the synaptic weights. Every balanced state is associated with a self-consistent set of Lij, but not
195"
BALANCING ALGORITHMS,0.15514993481095177,"every self-consistent set of Lij is associated with a balanced state.
196"
BALANCING ALGORITHMS,0.1558018252933507,"Proposition 5.4. A set Lij associated with a neural architecture is self-consistent if and only if
197
P"
BALANCING ALGORITHMS,0.15645371577574968,"π Lij = 0 where π is any directed path connecting an input unit to an output unit or any directed
198"
BALANCING ALGORITHMS,0.15710560625814862,"cycle (for recurrent networks).
199"
BALANCING ALGORITHMS,0.1577574967405476,"Proof. If we look at any directed path π from unit i to unit j, it is easy to see that we must have:
200 X"
BALANCING ALGORITHMS,0.15840938722294653,"π
Lkl = log Λi −log Λj
(5.5)"
BALANCING ALGORITHMS,0.1590612777053455,"This is illustrated in Figure 1. Thus along any directed path that connects any input unit to any output
201"
BALANCING ALGORITHMS,0.15971316818774445,"unit, we must have P"
BALANCING ALGORITHMS,0.16036505867014342,"π Lij = 0. In addition, for recurrent neural networks, if π is a directed cycle
202"
BALANCING ALGORITHMS,0.16101694915254236,we must also have: P
BALANCING ALGORITHMS,0.16166883963494133,"π Lij = 0. Thus in short we only need to add linear constraints of the form:
203
P"
BALANCING ALGORITHMS,0.1623207301173403,"π Lij = 0. Any unit is situated on a path from an input unit to an output unit. Along that path, it is
204"
BALANCING ALGORITHMS,0.16297262059973924,"easy to assign a value Λi to each unit by simple propagation starting from the input unit which has a
205"
BALANCING ALGORITHMS,0.1636245110821382,"multiplier equal to 1. When the propagation terminates in the output unit, it terminates consistently
206"
BALANCING ALGORITHMS,0.16427640156453716,"because the output unit has a multiplier equal to 1 and, by assumption, the sum of the multipliers
207"
BALANCING ALGORITHMS,0.16492829204693613,"along the path must be zero. So we can derive scaling values Λi from the variables Lij. Finally, it is
208"
BALANCING ALGORITHMS,0.16558018252933507,"easy to show that there are no clashes, i.e. that it is not possible for two different propagation paths to
209"
BALANCING ALGORITHMS,0.16623207301173404,"assign different multiplier values to the same unit i (see Appendix).
210"
BALANCING ALGORITHMS,0.16688396349413298,"Remark 5.5. Thus the constraints associated with being a self-consistent configuration of Lij’ s are
211"
BALANCING ALGORITHMS,0.16753585397653195,"all linear. This linear manifold of constraints depends only on the architecture, i.e., the graph of
212"
BALANCING ALGORITHMS,0.1681877444589309,"connections. The strictly convex function R(Lij) depends on the actual weights W. Different sets of
213"
BALANCING ALGORITHMS,0.16883963494132986,"weights W produce different convex functions over the same linear manifold.
214"
BALANCING ALGORITHMS,0.1694915254237288,"Remark 5.6. One could coalesce all the input units and all output units into a single unit, in which
215"
BALANCING ALGORITHMS,0.17014341590612778,"case a path from an input unit to and output unit becomes also a directed cycle. In this representation,
216"
BALANCING ALGORITHMS,0.17079530638852672,"the constraints are that the sum of the Lij must be zero along any directed cycle. In general, it is not
217"
BALANCING ALGORITHMS,0.1714471968709257,"necessary to write a constraint for every path from input units to output units. It is sufficient to select
218"
BALANCING ALGORITHMS,0.17209908735332463,"a representative set of paths such that every unit appears in at least one path.
219"
BALANCING ALGORITHMS,0.1727509778357236,"We can now complete the proof of Theorem 5.1. Given a neural network of BiLUs with a set
220"
BALANCING ALGORITHMS,0.17340286831812254,"of weights W, we can consider the problem of minimizing the regularizer R(Lij) over the self-
221"
BALANCING ALGORITHMS,0.17405475880052151,"admissible configuration Lij. For any p > 0, the Lp regularizer is strictly convex and the space of
222"
BALANCING ALGORITHMS,0.17470664928292046,"self-admissible configurations is linear and hence convex. Thus this is a strictly convex optimization
223 A B C D F E"
BALANCING ALGORITHMS,0.17535853976531943,"Figure 3: SGD applied to E alone,
in general, does not converge to
a balanced state, but SGD ap-
plied to E + R converges to a bal-
anced state. (A-C) Simulations use
a deep fully connected autoencoder
trained on the MNIST dataset. (D-
F) Simulations use a deep locally
connected network trained on the
CFAR10 dataset. (A,D) Regulariza-
tion leads to neural balance. (B,E)
The training loss decreases and con-
verges during training (these panels
are not meant for assessing the qual-
ity of learning when using a regu-
larizer). (C,F) Using weight reg-
ularization decreases the norm of
weights. (A-F) Shaded areas corre-
spond to one s.t.d around the mean
(in some cases the s.t.d. is small and
the shaded area is not visible)."
BALANCING ALGORITHMS,0.1760104302477184,"problem that has a unique solution (Figure 2). Note that the minimization is carried over self-
224"
BALANCING ALGORITHMS,0.17666232073011734,"consistent configurations, which in general are not associated with balanced states. However, the
225"
BALANCING ALGORITHMS,0.1773142112125163,"configuration of the weights associated with the optimum set of Lij (point A in Figure 2) must be
226"
BALANCING ALGORITHMS,0.17796610169491525,"balanced. To see this, imagine that one of the BiLU units–unit i in the network is not balanced. Then
227"
BALANCING ALGORITHMS,0.17861799217731422,"we can balance it using a multiplier λ∗
i and replace Λi by Λ′
i = Λiλ∗. It is easy to check that the new
228"
BALANCING ALGORITHMS,0.17926988265971316,"configuration including Λ′
i is self-consistent. Thus, by balancing unit i, we are able to reach a new
229"
BALANCING ALGORITHMS,0.17992177314211213,"self-consistent configuration with a lower value of R which contradicts the fact that we are at the
230"
BALANCING ALGORITHMS,0.18057366362451108,"global minimum of the strictly convex optimization problem.
231"
BALANCING ALGORITHMS,0.18122555410691005,"We know that the stochastic balancing algorithm always converges to a balanced state. We need to
232"
BALANCING ALGORITHMS,0.181877444589309,"show that it cannot converge to any other balanced state, and in fact that the global optimum is the
233"
BALANCING ALGORITHMS,0.18252933507170796,"only balanced state. By contradiction, suppose it converges to a different balanced state associated
234"
BALANCING ALGORITHMS,0.1831812255541069,"with the coordinates (LB
ij) (point B in Figure 2). Because of the self-consistency, this point is also
235"
BALANCING ALGORITHMS,0.18383311603650587,"associated with a unique set of (ΛB
i ) coordinates. The cost function is continuous and differentiable
236"
BALANCING ALGORITHMS,0.18448500651890481,"in both the Lij’s and the Λi’s coordinates. If we look at the negative gradient of the regularizer, it
237"
BALANCING ALGORITHMS,0.18513689700130379,"is non-zero and therefore it must have at least one non-zero component ∂R/∂Λi along one of the
238"
BALANCING ALGORITHMS,0.18578878748370273,"Λi coordinates. This implies that by scaling the corresponding unit i in the network, the regularizer
239"
BALANCING ALGORITHMS,0.1864406779661017,"can be further reduced, and by balancing unit i the balancing algorithm will reach a new point (C in
240"
BALANCING ALGORITHMS,0.18709256844850064,"Figure 2) with lower regularizer cost. This contradicts the assumption that B was associated with a
241"
BALANCING ALGORITHMS,0.1877444589308996,"balanced stated. Thus, given an initial set of weights W, the stochastic balancing algorithm must
242"
BALANCING ALGORITHMS,0.18839634941329855,"always converge to the same and unique optimal balanced state W ∗associated with the self-consistent
243"
BALANCING ALGORITHMS,0.18904823989569752,"point A. A particular stochastic schedule corresponds to a random path within the linear manifold
244"
BALANCING ALGORITHMS,0.1897001303780965,"from the origin (at time zero, all the multipliers are equal to 1, and therefore Mij = 1 and Lij = 0
245"
BALANCING ALGORITHMS,0.19035202086049544,"for any i and any j) to the unique optimum point A.
246 247"
BALANCING ALGORITHMS,0.1910039113428944,"Remark 5.7. From the proof, it is clear that the same result holds also for any deterministic balancing
248"
BALANCING ALGORITHMS,0.19165580182529335,"schedule, as well as for tied and non-tied subset balancing, e.g., for layer-wise balancing and tied
249"
BALANCING ALGORITHMS,0.19230769230769232,"layer-wise balancing. In the Appendix, we provide an analytical solution for the case of tied layer-wise
250"
BALANCING ALGORITHMS,0.19295958279009126,"balancing in a layered feed-forward network.
251"
BALANCING ALGORITHMS,0.19361147327249023,"Remark 5.8. From the proof, it is also clear that the same convergence to the unique global optimum
252"
BALANCING ALGORITHMS,0.19426336375488917,"is observed if each neuron, when stochastically visited, is favorably scaled rather than balanced, i.e.,
253"
BALANCING ALGORITHMS,0.19491525423728814,"it is scaled with a factor that reduces R but not necessarily minimizes R. Stochastic balancing can
254"
BALANCING ALGORITHMS,0.19556714471968709,"also be viewed as a form of EM algorithm where the E and M steps can be taken fully or partially.
255 A B D C E F"
BALANCING ALGORITHMS,0.19621903520208606,"Figure 4: Even if the starting state
is balanced, SGD does not pre-
serve the balance unless the learn-
ing rate is infinitely small. (A-C)
Simulations use a deep fully con-
nected autoencoder trained on the
MNIST dataset. (D-F) Simulations
use a deep locally connected net-
work trained on the CFAR10 dataset.
(A-F) The initial weights are bal-
anced using the stochastic balanc-
ing algorithm. Then the network is
trained by SGD. (A,D) When the
learning rate (lr) is relatively large,
without regularization, the initial
balance of the network is rapidly dis-
rupted. (B,E) The training loss de-
creases and converges during train-
ing (these panels are not meant for
assessing the quality of learning
when using a regularizer). (C,F) Us-
ing weight regularization decreases
the norm of the weights.
(A-F)
Shaded areas correspond to one s.t.d
around the mean (in some cases the
s.t.d. is small and the shaded area is
not visible)."
SIMULATIONS,0.196870925684485,"6
Simulations
256"
SIMULATIONS,0.19752281616688397,"To further corroborate the results, we ran multiple experiments. Here we report the results from two
257"
SIMULATIONS,0.1981747066492829,"series of experiments. The first one is conducted using a six-layer, fully connected, autoencoder
258"
SIMULATIONS,0.19882659713168188,"trained on MNIST [Deng, 2012] for a reconstruction task with ReLU activation functions in all layers
259"
SIMULATIONS,0.19947848761408082,"and the sum of squares errors loss function. The number of neurons in consecutive layers, from
260"
SIMULATIONS,0.2001303780964798,"input to output, is 784, 200, 100, 50, 100, 200, 784. Stochastic gradient descent (SGD) learning by
261"
SIMULATIONS,0.20078226857887874,"backpropagation is used for learning with a batch size of 200.
262"
SIMULATIONS,0.2014341590612777,"The second one is conducted using three locally connected layers followed by three fully connected
263"
SIMULATIONS,0.20208604954367665,"layers trained on CFAR10 [Krizhevsky and Hinton, 2009] for a classification task with leaky ReLU
264"
SIMULATIONS,0.20273794002607562,"activation functions in the hidden layers, a softmax output layer, and the cross entropy loss function.
265"
SIMULATIONS,0.2033898305084746,"The number of neurons in consecutive layers, from input to output, is 3072, 5000, 2592, 1296, 300,
266"
SIMULATIONS,0.20404172099087353,"100, 10. Stochastic gradient descent (SGD) learning by backpropagation is used for learning with a
267"
SIMULATIONS,0.2046936114732725,"batch size of 5.
268"
SIMULATIONS,0.20534550195567144,"In all the simulation figures (Figures 3, 4, and 5) the left column presents results obtained from the
269"
SIMULATIONS,0.20599739243807041,"first experiment, while the right column presents results obtained from the second experiment. While
270"
SIMULATIONS,0.20664928292046936,"we used both L1 and L2 regularizers in the experiments, in the figures we report the results obtained
271"
SIMULATIONS,0.20730117340286833,"with the L2 regularizer, which is the most widely used regularizer. In Figures 3 and 4, training is
272"
SIMULATIONS,0.20795306388526727,"done using batch gradient descent on the MNIST and CIFAR data. The balance deficit for a single
273"
SIMULATIONS,0.20860495436766624,"neuron i is defined as:
 P"
SIMULATIONS,0.20925684485006518,w∈IN(i) w2 −P
SIMULATIONS,0.20990873533246415,"w∈OUT (i) w22, and the overall balance deficit is defined
274"
SIMULATIONS,0.2105606258148631,"as the sum of these single-neuron balance deficits across all the hidden neurons in the network. The
275"
SIMULATIONS,0.21121251629726207,"overall deficit is zero if and only if each neuron is in balance. In all the figures, ||W||F denotes the
276"
SIMULATIONS,0.211864406779661,"Frobenius norm of the weights.
277"
SIMULATIONS,0.21251629726205998,"Figure 3 shows that learning by gradient descent with a L2 regularizer results in a balanced state.
278"
SIMULATIONS,0.21316818774445892,"Figure 4 shows that even when the network is initialized in a balanced state, without the regularizer
279"
SIMULATIONS,0.2138200782268579,"the network can become unbalanced if the fixed learning rate is not very small. Figure 5 shows that
280"
SIMULATIONS,0.21447196870925683,"the local stochastic balancing algorithm, by which neurons are randomly balanced in an asynchronous
281"
SIMULATIONS,0.2151238591916558,"fashion, always converges to the same (unique) global balanced state.
282 A
C B
D"
SIMULATIONS,0.21577574967405475,"Figure 5: Stochastic balancing converges to
a unique global balanced state (A-B) Sim-
ulations use a deep fully connected autoen-
coder trained on the MNIST dataset. (C-D)
Simulations use a deep locally connected net-
work trained on the CFAR10 dataset. (A,C)
The weights of the network are initialized
randomly and saved. The stochastic balanc-
ing algorithm is applied and the resulting
balanced weights are denoted by Wbalanced.
The stochastic balancing algorithm is ap-
plied 1,000 different times.
In all repeti-
tions, the weights converge to the same value
Wbalanced. (B,D) Stochastic balancing de-
creases the norm of the weights.
(A-D)
Shaded areas correspond to one standard de-
viation around the mean."
CONCLUSION,0.21642764015645372,"7
Conclusion
283"
CONCLUSION,0.21707953063885269,"While the theory of neural synaptic balance is a mathematical theory that stands on its own, it is
284"
CONCLUSION,0.21773142112125163,"worth considering some of its possible consequences and applications, at the theoretical, algorithmic,
285"
CONCLUSION,0.2183833116036506,"biological, and neuromorphic hardware levels. At the theory level, for instance, it suggests extending
286"
CONCLUSION,0.21903520208604954,"theorems obtained with ReLU neurons to BiLU neurons, using balance ideas to study learning in
287"
CONCLUSION,0.2196870925684485,"linear regularized networks, and using the manifolds of equivalent weights to study issues of over-
288"
CONCLUSION,0.22033898305084745,"parameterization (e.g. the data needs only to specify the balanced state, not the entire equivalence
289"
CONCLUSION,0.22099087353324642,"class). At the algorithmic level, balancing algorithms could be used for instance to balance networks
290"
CONCLUSION,0.22164276401564537,"at any stage of learning, including at the beginning, and as an alternative way to regularize networks.
291"
CONCLUSION,0.22229465449804434,"Finally, because scaling and balancing are local operations, they are potentially of interest in physical,
292"
CONCLUSION,0.22294654498044328,"as opposed to digitally-simulated, neural networks. In particular, it would be interesting to know if
293"
CONCLUSION,0.22359843546284225,"some notion of balance applies to biological neurons. Unfortunately, current recording technologies
294"
CONCLUSION,0.2242503259452412,"do not allow the measurement of all incoming and outgoing synapses of a neuron. Perhaps some
295"
CONCLUSION,0.22490221642764016,"approximation could be obtained statistically and at the population level, or perhaps approximate
296"
CONCLUSION,0.2255541069100391,"measurements could be carried in very simple networks (e.g. C. elegans)or using neurons in culture.
297"
CONCLUSION,0.22620599739243807,"Finally, in neuromorphic hardware, the balance could be relevant for training spiking neural networks
298"
CONCLUSION,0.22685788787483702,"with low energy consumption [Sorbaro et al., 2020, Rueckauer et al., 2017]). In particular, ReLU
299"
CONCLUSION,0.227509778357236,"scaling can influence the number of spikes generated in each layer and the average energy consumption
300"
CONCLUSION,0.22816166883963493,"at each layer. Similarly, in memristor networks [Ivanov et al., 2022, Liang and Wong, 2000] ), L2
301"
CONCLUSION,0.2288135593220339,"minimization is directly connected to power consumption. Moreover, the issue of the limited
302"
CONCLUSION,0.22946544980443284,"conductivity range of memristors is mentioned in Ivanov et al. [2022] and in Ji et al. [2016] Therefore,
303"
CONCLUSION,0.2301173402868318,"a local algorithm to reduce the norm of the weights could help mitigate this issue as well.
304"
CONCLUSION,0.23076923076923078,"The theory of neural synaptic balance explains some basic findings regarding L2 balance in feedfor-
305"
CONCLUSION,0.23142112125162972,"ward networks of ReLU neurons and extends them in several directions. The first direction is the
306"
CONCLUSION,0.2320730117340287,"extension to BiLU and other activation functions (BiPU). The second direction is the extension to
307"
CONCLUSION,0.23272490221642764,"more general regularizers, including all Lp (p > 0) regularizers. The third direction is the extension to
308"
CONCLUSION,0.2333767926988266,"non-layered architectures, recurrent architectures, convolutional architectures, as well as architectures
309"
CONCLUSION,0.23402868318122555,"with mixed activation functions. The theory is based on two local neuronal operations: scaling
310"
CONCLUSION,0.23468057366362452,"which is commutative, and balancing which is not commutative. Finally, and most importantly, given
311"
CONCLUSION,0.23533246414602346,"any initial set of weights, when local balancing operations are applied in a stochastic or determin-
312"
CONCLUSION,0.23598435462842243,"istic manner, global order always emerges through the convergence of the balancing algorithm to
313"
CONCLUSION,0.23663624511082137,"the same unique set of balanced weights. The reason for this convergence is the existence of an
314"
CONCLUSION,0.23728813559322035,"underlying convex optimization problem where the relevant variables are constrained to a linear,
315"
CONCLUSION,0.2379400260756193,"only architecture-dependent, manifold. Scaling and balancing operations are local and thus may
316"
CONCLUSION,0.23859191655801826,"have applications in physical, non-digitally simulated, neural networks where the emergence of
317"
CONCLUSION,0.2392438070404172,"global order from local operations may lead to better operating characteristics and lower energy
318"
CONCLUSION,0.23989569752281617,"consumption.
319"
REFERENCES,0.2405475880052151,"References
320"
REFERENCES,0.24119947848761408,"P. Baldi. Deep Learning in Science. Cambridge University Press, Cambridge, UK, 2021.
321"
REFERENCES,0.24185136897001303,"Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
322"
REFERENCES,0.242503259452412,"Processing Magazine, 29(6):141–142, 2012.
323"
REFERENCES,0.24315514993481094,"Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
324"
REFERENCES,0.2438070404172099,"models: Layers are automatically balanced. Advances in Neural Information Processing Systems,
325"
REFERENCES,0.24445893089960888,"31, 2018.
326"
REFERENCES,0.24511082138200782,"Rachel E Field, James A D’amour, Robin Tremblay, Christoph Miehl, Bernardo Rudy, Julijana
327"
REFERENCES,0.2457627118644068,"Gjorgjieva, and Robert C Froemke. Heterosynaptic plasticity determines the set point for cortical
328"
REFERENCES,0.24641460234680573,"excitatory-inhibitory balance. Neuron, 106(5):842–854, 2020.
329"
REFERENCES,0.2470664928292047,"Robert C Froemke. Plasticity of cortical excitatory-inhibitory balance. Annual review of neuroscience,
330"
REFERENCES,0.24771838331160365,"38:195–219, 2015.
331"
REFERENCES,0.24837027379400262,"Oliver D Howes and Ekaterina Shatalina. Integrating the neurodevelopmental and dopamine hypothe-
332"
REFERENCES,0.24902216427640156,"ses of schizophrenia and the role of cortical excitation-inhibition balance. Biological psychiatry,
333"
REFERENCES,0.24967405475880053,"2022.
334"
REFERENCES,0.2503259452411995,"Dmitry Ivanov, Aleksandr Chezhegov, Mikhail Kiselev, Andrey Grunin, and Denis Larionov. Neuro-
335"
REFERENCES,0.25097783572359844,"morphic artificial intelligence systems. Frontiers in Neuroscience, 16:1513, 2022.
336"
REFERENCES,0.2516297262059974,"Yu Ji, YouHui Zhang, ShuangChen Li, Ping Chi, CiHang Jiang, Peng Qu, Yuan Xie, and WenGuang
337"
REFERENCES,0.2522816166883963,"Chen. Neutrams: Neural network transformation and co-design under neuromorphic hardware
338"
REFERENCES,0.2529335071707953,"constraints. In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture
339"
REFERENCES,0.25358539765319427,"(MICRO), pages 1–13. IEEE, 2016.
340"
REFERENCES,0.2542372881355932,"Dongshin Kim and Jang-Sik Lee. Neurotransmitter-induced excitatory and inhibitory functions in
341"
REFERENCES,0.25488917861799215,"artificial synapses. Advanced Functional Materials, 32(21):2200497, 2022.
342"
REFERENCES,0.25554106910039115,"Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
343"
REFERENCES,0.2561929595827901,"Faming Liang and Wing Hung Wong. Evolutionary monte carlo: Applications to cp model sampling
344"
REFERENCES,0.25684485006518903,"and change point problem. STATISTICA SINICA, 10:317–342, 2000.
345"
REFERENCES,0.25749674054758803,"Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Data-dependent path
346"
REFERENCES,0.258148631029987,"normalization in neural networks. arXiv preprint arXiv:1511.06747, 2015.
347"
REFERENCES,0.2588005215123859,"Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conver-
348"
REFERENCES,0.25945241199478486,"sion of continuous-valued deep networks to efficient event-driven networks for image classification.
349"
REFERENCES,0.26010430247718386,"Frontiers in neuroscience, 11:294078, 2017.
350"
REFERENCES,0.2607561929595828,"Farshad Shirani and Hannah Choi. On the physiological and structural contributors to the dynamic
351"
REFERENCES,0.26140808344198174,"balance of excitation and inhibition in local cortical networks. bioRxiv, pages 2023–01, 2023.
352"
REFERENCES,0.2620599739243807,"Martino Sorbaro, Qian Liu, Massimo Bortone, and Sadique Sheik. Optimizing the energy consump-
353"
REFERENCES,0.2627118644067797,"tion of spiking neural networks for neuromorphic applications. Frontiers in neuroscience, 14:662,
354"
REFERENCES,0.2633637548891786,"2020.
355"
REFERENCES,0.26401564537157757,"Christopher H Stock, Sarah E Harvey, Samuel A Ocko, and Surya Ganguli. Synaptic balancing: A
356"
REFERENCES,0.2646675358539765,"biologically plausible local learning rule that provably increases neural network noise robustness
357"
REFERENCES,0.2653194263363755,"without sacrificing task performance. PLOS Computational Biology, 18(9):e1010418, 2022.
358"
REFERENCES,0.26597131681877445,"A. Tavakoli, F. Agostinelli, and P. Baldi. SPLASH: Learnable activation functions for improving
359"
REFERENCES,0.2666232073011734,"accuracy and adversarial robustness. Neural Networks, 140:1–12, 2021. Also: arXiv:2006.08947.
360"
REFERENCES,0.26727509778357234,"Appendix
361"
REFERENCES,0.26792698826597133,"A
Homogeneous and BiLU Activation Functions
362"
REFERENCES,0.2685788787483703,"In this section, we generalize the basic example of the introduction from the standpoint of the
363"
REFERENCES,0.2692307692307692,"activation functions. In particular, we consider homogeneous activation functions (defined below).
364"
REFERENCES,0.26988265971316816,"The importance of homogeneity has been previously identified in somewhat different contexts
365"
REFERENCES,0.27053455019556716,"Neyshabur et al. [2015]. Intuitively, homogeneity is a form of linearity with respect to weight scaling
366"
REFERENCES,0.2711864406779661,"and thus it is useful to motivate the concept of homogeneous activation functions by looking at other
367"
REFERENCES,0.27183833116036504,"notions of linearity for activation functions. This will also be useful for Section E where even more
368"
REFERENCES,0.27249022164276404,"general classes of activation functions are considered.
369"
REFERENCES,0.273142112125163,"A.1
Additive Activation Functions
370"
REFERENCES,0.2737940026075619,"Definition A.1. A neuronal activation function f : R →R is additively linear if and only if
371"
REFERENCES,0.27444589308996087,"f(x + y) = f(x) = (f(y) for any real numbers x and y.
372"
REFERENCES,0.27509778357235987,"Proposition A.2. The class of additively linear activation functions is exactly equal to the class of
373"
REFERENCES,0.2757496740547588,"linear activation functions, i.e., activation functions of the form f(x) = ax.
374"
REFERENCES,0.27640156453715775,"Proof. Obviously linear activation functions are additively linear. Conversely, if f is additively linear,
375"
REFERENCES,0.2770534550195567,"the following three properties are true:
376"
REFERENCES,0.2777053455019557,"(1) One must have: f(nx) = nf(x) and f(x/n) = f(x)/n for any x ∈R and any n ∈N. As a
377"
REFERENCES,0.27835723598435463,"result, f(n/m) = nf(1)/m for any integers n and m (m ̸= 0).
378"
REFERENCES,0.2790091264667536,"(2) Furthermore, f(0 + 0) = f(0) + f(0) which implies: f(0) = 0.
379"
REFERENCES,0.2796610169491525,"(3) And thus f(x −x) = f(x) + f(−x) = 0, which in turn implies that f(−x) = −f(x).
380"
REFERENCES,0.2803129074315515,"From these properties, it is easy to see that f must be continuous, with f(x) = xf(1), and thus f
381"
REFERENCES,0.28096479791395046,"must be linear.
382"
REFERENCES,0.2816166883963494,"A.2
Multiplicative Activation Functions
383"
REFERENCES,0.28226857887874834,"Definition A.3. A neuronal activation function f : R →R is multiplicative if and only if f(xy) =
384"
REFERENCES,0.28292046936114734,"f(x)(f(y) for any real numbers x and y.
385"
REFERENCES,0.2835723598435463,"Proposition A.4. The class of continuous multiplicative activation functions is exactly equal to the
386"
REFERENCES,0.2842242503259452,"class of functions comprising the functions: f(x) = 0 for every x, f(x) = 1 for every x, and all the
387"
REFERENCES,0.2848761408083442,"even and odd functions satisfying f(x) = xc for x ≥0, where c is any constant in R.
388"
REFERENCES,0.28552803129074317,"Proof. It is easy to check the functions described in the proposition are multiplicative. Conversely,
389"
REFERENCES,0.2861799217731421,"assume f is multiplicative. For both x = 0 and x = 1, we must have f(x) = f(xx) = f(x)f(x) and
390"
REFERENCES,0.28683181225554105,"thus f(0) is either 0 or 1, and similarly for f(1). If f(1) = 0, then for any x we must have f(x) = 0
391"
REFERENCES,0.28748370273794005,"because: f(x) = f(1x) = f(1)f(x) = 0. Likewise, if f(0) = 1, then for any x we must have
392"
REFERENCES,0.288135593220339,"f(x) = 1 because: 1 = f(0) = f(0x) = f(0)f(x) = f(x). Thus, in the rest of the proof, we can
393"
REFERENCES,0.28878748370273793,"assume that f(0) = 0 and f(1) = 1. By induction, it is easy to see that for any x ≥0 we must have:
394"
REFERENCES,0.2894393741851369,"f(xn) = f(x)n and f(x1/n) = (f(x))1/n for any integer (positive or negative). As a result, for any
395"
REFERENCES,0.2900912646675359,"x ∈R and any integers n and m we must have: f(xn/m) = f(x)n/m. By continuity this implies
396"
REFERENCES,0.2907431551499348,"that for any x ≥0 and any r ∈R, we must have: f(xr) = f(x)r. Now there is some constant c such
397"
REFERENCES,0.29139504563233376,"that: f(e) = ec. And thus, for any x > 0, f(x) = f(elog x) = [f(e)]log x = ec log x = xc. To address
398"
REFERENCES,0.2920469361147327,"negative values of x, note that we must have f[(−1)(−1 = f(1) = 1f(−1)2. Thus, f(−1) is either
399"
REFERENCES,0.2926988265971317,"equal to 1 or to -1. Since for any x > 0 we have f(−x) = f(−1)f(x), we see that if f(−1) = 1
400"
REFERENCES,0.29335071707953064,"the function must be even (f(−x) = f(x) = xc), and if f(−1) = −1 the function must be odd
401"
REFERENCES,0.2940026075619296,"(f(−x) = −f(x)).
402"
REFERENCES,0.29465449804432853,"We will return to multiplicative activation function in a later section.
403"
REFERENCES,0.2953063885267275,"A.3
Linearly Scalable Activation Functions
404"
REFERENCES,0.29595827900912647,"Definition A.5. A neuronal activation function f : R →R is linearly scalable if and only if
405"
REFERENCES,0.2966101694915254,"f(λx) = λf(x) for every λ ∈R.
406"
REFERENCES,0.29726205997392435,"Proposition A.6. The class of linearly scalable activation functions is exactly equal to the class of
407"
REFERENCES,0.29791395045632335,"linear activation functions, i.e., activation functions of the form f(x) = ax.
408"
REFERENCES,0.2985658409387223,"Proof. Obviously, linear activation functions are linearly scalable. For the converse, if f is linearly
409"
REFERENCES,0.29921773142112124,"multiplicative we must have f(λx) = λf(x) = xf(λ) for any x and any λ. By taking λ = 1, we get
410"
REFERENCES,0.29986962190352023,"f(x) = f(1)x and thus f is linear.
411"
REFERENCES,0.3005215123859192,"Thus the concepts of linearly additive or linearly scalable activation function are of limited interest
412"
REFERENCES,0.3011734028683181,"since both of them are equivalent to the concept of linear activation function. A more interesting
413"
REFERENCES,0.30182529335071706,"class is obtained if we consider linearly scalable activation functions, where the scaling factor λ is
414"
REFERENCES,0.30247718383311606,"constrained to be positive (λ > 0), also called homogeneous functions.
415"
REFERENCES,0.303129074315515,"A.4
Homogeneous Activation Functions
416"
REFERENCES,0.30378096479791394,"Definition A.7. (Homogeneous) A neuronal activation function f : R →R is homogeneous if and
417"
REFERENCES,0.3044328552803129,"only if: f(λx) = λf(x) for every λ ∈R with λ > 0.
418"
REFERENCES,0.3050847457627119,"Remark A.8. Note that if f is homogeneous, f(λ0) = λf(0) = f(0) for any λ > 0 and thus
419"
REFERENCES,0.3057366362451108,"f(0) = 0. Thus it makes no difference in the definition of homogeneous if we set λ ≥0 instead of
420"
REFERENCES,0.30638852672750977,"λ > 0).
421"
REFERENCES,0.3070404172099087,"Remark A.9. Clearly, linear activation functions are homogeneous. However, there exists also
422"
REFERENCES,0.3076923076923077,"homogeneous functions that are non-linear, such as ReLU or leaky ReLU activation functions.
423"
REFERENCES,0.30834419817470665,"We now provide a full characterization of the class of homogeneous activation functions.
424"
REFERENCES,0.3089960886571056,"A.5
BiLU Activation Functions
425"
REFERENCES,0.30964797913950454,"We first define a new class of activation functions, corresponding to bilinear units (BiLU), consisting
426"
REFERENCES,0.31029986962190353,"of two half-lines meeting at the origin. This class contains all the linear functions, as well as the
427"
REFERENCES,0.3109517601043025,"ReLU and leaky ReLU functions, and many other functions.
428"
REFERENCES,0.3116036505867014,"Definition A.10. (BiLU) A neuronal activation function f : R →R is bilinear (BiLU) if and only if
429"
REFERENCES,0.3122555410691004,"f(x) = ax when x < 0, and f(x) = bx when x ≥0, for some fixed parameters a and b in R.
430"
REFERENCES,0.31290743155149936,"These include linear units (a = b), ReLU units (a = 0, b = 1), leaky ReLU (a = ϵ; b = 1) units,
431"
REFERENCES,0.3135593220338983,"and symmetric linear units (a = −b), all of which can also be viewed as special cases of piece-wise
432"
REFERENCES,0.31421121251629724,"linear units Tavakoli et al. [2021], with a single hinge. One advantage of ReLU and more generally
433"
REFERENCES,0.31486310299869624,"BiLU neurons, which is very important during backpropagation learning, is that their derivative is
434"
REFERENCES,0.3155149934810952,"very simple and can only take one of two values (a or b).
435"
REFERENCES,0.3161668839634941,"Proposition A.11. A neuronal activation function f : R →R is homogeneous if and only if it is a
436"
REFERENCES,0.31681877444589307,"BiLU activation function.
437"
REFERENCES,0.31747066492829207,"Proof. Every function in BiLU is clearly homogeneous. Conversely, any homogeneous function f
438"
REFERENCES,0.318122555410691,"must satisfy: (1) f(0x) = 0f(x) = f(0) = 0; (2)f(x) = f(1x) = f(1)x for any positive x; and (3)
439"
REFERENCES,0.31877444589308995,"f(x) = f(−u) = f(−1)u = −f(−1)x for any negative x. Thus f is in BiLU with a = −f(−1)
440"
REFERENCES,0.3194263363754889,"and b = f(1).
441"
REFERENCES,0.3200782268578879,"In Appendix A, we provide a simple proof that networks of BiLU neurons, even with a single
442"
REFERENCES,0.32073011734028684,"hidden layer, have universal approximation properties. In the next two sections, we introduce two
443"
REFERENCES,0.3213820078226858,"fundamental neuronal operations, scaling and balancing, that can be applied to the incoming and
444"
REFERENCES,0.3220338983050847,"outgoing synaptic weights of neurons with BiLU activation functions.
445"
REFERENCES,0.3226857887874837,"B
Scaling
446"
REFERENCES,0.32333767926988266,"Definition B.1. (Scaling) For any BiLU neuron i in network and any λ > 0, we let Sλ(i) denote the
447"
REFERENCES,0.3239895697522816,"synaptic scaling operation by which the incoming connection weights of neuron i are multiplied by λ
448"
REFERENCES,0.3246414602346806,"and the outgoing connection weights of neuron i are divided by λ.
449"
REFERENCES,0.32529335071707954,"Note that because of the homogeneous property, the scaling operation does not change how neuron i
450"
REFERENCES,0.3259452411994785,"affects the rest of the network. In particular, the input-output function of the overall network remains
451"
REFERENCES,0.32659713168187743,"unchanged after scaling neuron i bt any λ > 0. Note also that scaling always preserves the sign of
452"
REFERENCES,0.3272490221642764,"the synaptic weights to which it is applied, and the scaling operation can never convert a non-zero
453"
REFERENCES,0.32790091264667537,"synaptic weight into a zero synaptic weight, or vice versa.
454"
REFERENCES,0.3285528031290743,"As usual, the bias is treated here as an additional synaptic weight emanating from a unit clamped to
455"
REFERENCES,0.32920469361147325,"the value one. Thus scaling is applied to the bias.
456"
REFERENCES,0.32985658409387225,"Proposition B.2. (Commutativity of Scaling) Scaling operations applied to any pair of BiLU neurons
457"
REFERENCES,0.3305084745762712,"i and j in a neural network commute: Sλ(i)Sµ(j) = Sµ(j)Sλ(i), in the sense that the resulting
458"
REFERENCES,0.33116036505867014,"network weights are the same, regardless of the order in which the scaling operations are applied.
459"
REFERENCES,0.3318122555410691,"Furthermore, for any BiLU neuron i: Sλ(i)Sµ(i) = Sµ(i)Sλ(i) = Sλµ(i).
460"
REFERENCES,0.3324641460234681,"This is obvious. As a result, any set I of BiLU neurons in a network can be scaled simultaneously or
461"
REFERENCES,0.333116036505867,"in any sequential order while leading to the same final configuration of synaptic weights. If we denote
462"
REFERENCES,0.33376792698826596,"by 1, 2, . . . , n the neurons in I, we can for instance write: Q"
REFERENCES,0.3344198174706649,i∈I Sλi(i) = Q
REFERENCES,0.3350717079530639,"σ(i)∈I Sλσ(i)(σ(i)) for
463"
REFERENCES,0.33572359843546284,"any permutation σ of the neurons. Likewise, we can collapse operations applied to the same neuron.
464"
REFERENCES,0.3363754889178618,"For instance, we can write: S5(1)S2(2)S3(1)S4(2) = S15(1)S8(2) = S8(2)S15(1)
465"
REFERENCES,0.33702737940026073,"Definition B.3. (Coordinated Scaling) For any set I of BiLU neurons in a network and any λ > 0,
466"
REFERENCES,0.3376792698826597,"we let Sλ(I) denote the synaptic scaling operation by which all the neurons in I are scaled by the
467"
REFERENCES,0.33833116036505867,"same λ.
468"
REFERENCES,0.3389830508474576,"C
Balancing
469"
REFERENCES,0.3396349413298566,"Definition C.1. (Balancing) Given a BiLU neuron in a network, the balancing operation B(i) is
470"
REFERENCES,0.34028683181225555,"a particular scaling operation B(i) = Sλ∗(i), where the scaling factor λ∗is chosen to optimize a
471"
REFERENCES,0.3409387222946545,"particular cost function, or regularizer, associated with the incoming and outgoing weights of neuron
472"
REFERENCES,0.34159061277705344,"i.
473"
REFERENCES,0.34224250325945244,"For now, we can imagine that this cost function is the usual L2 (least squares) regularizer, but in
474"
REFERENCES,0.3428943937418514,"the next section, we will consider more general classes of regularizers and study the corresponding
475"
REFERENCES,0.3435462842242503,"optimization process. For the L2 regularizer, as shown in the next section, this optimization process
476"
REFERENCES,0.34419817470664926,"results in a unique value of λ∗such that sum of the squares of the incoming weights is equal to
477"
REFERENCES,0.34485006518904826,"the sum of the squares of the outgoing weights, hence the term “balance”. Note that obviously
478"
REFERENCES,0.3455019556714472,"B(B(i)) = B(i) and that, as a special case of scaling operation, the balancing operation does not
479"
REFERENCES,0.34615384615384615,"change how neuron i contributes to the rest of the network, and thus it leaves the overall input-output
480"
REFERENCES,0.3468057366362451,"function of the network unchanged.
481"
REFERENCES,0.3474576271186441,"Unlike scaling operations, balancing operations in general do not commute as balancing operations
482"
REFERENCES,0.34810951760104303,"(they still commute as scaling operations). Thus, in general, B(i)B(j) ̸= B(j)B(i). This is because
483"
REFERENCES,0.34876140808344197,"if neuron i is connected to neuron j, balancing i will change the connection between i and j, and, in
484"
REFERENCES,0.3494132985658409,"turn, this will change the value of the optimal scaling constant for neuron j and vice versa. However,
485"
REFERENCES,0.3500651890482399,"if there are no non-zero connections between neuron i and neuron j then the balancing operations
486"
REFERENCES,0.35071707953063885,"commute since each balancing operation will modify a different, non-overlapping, set of weights.
487"
REFERENCES,0.3513689700130378,"Definition C.2. (Disjoint neurons) Two neurons i and j in a neural network are said to be disjoint if
488"
REFERENCES,0.3520208604954368,"there are no non-zero connections between i and j.
489"
REFERENCES,0.35267275097783574,"Thus in this case B(i)B(j) = Sλ∗(i)Sµ∗(j) = Sµ∗(j)Sλ∗(i) = B(j)B(i). This can be extended to
490"
REFERENCES,0.3533246414602347,"disjoint sets of neurons.
491"
REFERENCES,0.3539765319426336,"Definition C.3. (Disjoint Set of Neurons) A set I of neurons is said to be disjoint if for any pair i and
492"
REFERENCES,0.3546284224250326,"j of neurons in I there are no non-zero connections between i and j.
493"
REFERENCES,0.35528031290743156,"For example, in a layered feedforward network, all the neurons in a layer form a disjoint set, as long
494"
REFERENCES,0.3559322033898305,"as there are no intra-layer connections or, more precisely, no non-zero intra-layer connections. All
495"
REFERENCES,0.35658409387222945,"the neurons in a disjoint set can be balanced in any order resulting in the same final set of synaptic
496"
REFERENCES,0.35723598435462844,"weights. Thus we have:
497"
REFERENCES,0.3578878748370274,"Proposition C.4. If we index by 1, 2, . . . , n the neurons in a disjoint set I of BiLU neurons in a
498"
REFERENCES,0.35853976531942633,"network, we have: Q"
REFERENCES,0.35919165580182527,i∈I B(i) = Q
REFERENCES,0.35984354628422427,"i∈I Sλ∗
i (i) = Q"
REFERENCES,0.3604954367666232,"σ(i)∈I Sλ∗
σ(i)(σ(i)) = Q"
REFERENCES,0.36114732724902215,"σ(i)∈I B(σ(i)) for any
499"
REFERENCES,0.3617992177314211,"permutation σ of the neurons.
500"
REFERENCES,0.3624511082138201,"Finally, we can define the coordinated balancing of any set I of BiLU neurons (disjoint or not
501"
REFERENCES,0.36310299869621904,"disjoint).
502"
REFERENCES,0.363754889178618,"Definition C.5. (Coordinated Balancing) Given any set I of BiLU neurons (disjoint or not disjoint)
503"
REFERENCES,0.3644067796610169,"in a network, the coordinated balancing of these neurons, written as Bλ∗(I), corresponds to the
504"
REFERENCES,0.3650586701434159,"coordinated scaling all the neurons in I by the same factor λ∗, Where λ∗minimizes the cost functions
505"
REFERENCES,0.36571056062581486,"of all the weights, incoming and outgoing, associated with all the neurons in I.
506"
REFERENCES,0.3663624511082138,"Remark C.6. While balancing corresponds to a full optimization of the scaling operation, it is also
507"
REFERENCES,0.3670143415906128,"possible to carry a partial optimization of the scaling operation by choosing a scaling factor that
508"
REFERENCES,0.36766623207301175,"reduces the corresponding contribution to the regularizer without minimizing it.
509"
REFERENCES,0.3683181225554107,"D
General Framework and Single Neuron Balance
510"
REFERENCES,0.36897001303780963,"In this section, we generalize the kinds of regularizer to which the notion of neuronal synaptic balance
511"
REFERENCES,0.36962190352020863,"can be applied, beyond the usual L2 regularizer and derive the corresponding balance equations.
512"
REFERENCES,0.37027379400260757,"Thus we consider a network (feedforward or recurrent) where the hidden units are BiLU units.
513"
REFERENCES,0.3709256844850065,"The visible units can be partitioned into input units and output units. For any hidden unit i, if we
514"
REFERENCES,0.37157757496740546,"multiply all its incoming weights IN(i) by some λ > 0 and all its outgoing weights OUT(i) by
515"
REFERENCES,0.37222946544980445,"1/λ the overall function computed by the network remains unchanged due to the BiLU homogeneity
516"
REFERENCES,0.3728813559322034,"property. In particular, if there is an error function that depends uniquely on the input-output function
517"
REFERENCES,0.37353324641460234,"being computed, this error remains unchanged by the introduction of the multiplier λ. However, if
518"
REFERENCES,0.3741851368970013,"there is also a regularizer R for the weights, its value is affected by λ and one can ask what is the
519"
REFERENCES,0.3748370273794003,"optimal value of λ with respect to the regularizer, and what are the properties of the resulting weights.
520"
REFERENCES,0.3754889178617992,"This approach can be applied to any regularizer. For most practical purposes, we can assume that
521"
REFERENCES,0.37614080834419816,"the regularizer is continuous in the weights (hence in λ) and lower-bounded. Without any loss of
522"
REFERENCES,0.3767926988265971,"generality, we can assume that it is lower-bounded by zero. If we want the minimum value to be
523"
REFERENCES,0.3774445893089961,"achieved by some λ > 0, we need to add some mild condition that prevents the minimal value from
524"
REFERENCES,0.37809647979139505,"being approached as λ →0), or as λ →+∞. For instance, it is enough if there is an interval [a, b]
525"
REFERENCES,0.378748370273794,"with 0 < a < b where R achieves a minimal value Rmin and R ≥Rmin in the intervals (0, a] and
526"
REFERENCES,0.379400260756193,"[b, +∞). Additional (mild) conditions must be imposed if one wants the optimal value of λ to be
527"
REFERENCES,0.38005215123859193,"unique, or computable in closed form (see Theorems below). Finally, we want to be able to apply the
528"
REFERENCES,0.38070404172099087,"balancing approach
529"
REFERENCES,0.3813559322033898,"Thus, we consider overall regularized error functions, where the regularizer is very general, as long
530"
REFERENCES,0.3820078226857888,"as it has an additive form with respect to the individual weights:
531"
REFERENCES,0.38265971316818775,"E(W) = E(W) + R(W)
with
R(W) =
X"
REFERENCES,0.3833116036505867,"w
gw(w)
(D.1)"
REFERENCES,0.38396349413298564,"where W denotes all the weights in the network and E(W) is typically the negative log-likelihood
532"
REFERENCES,0.38461538461538464,"(LMS error in regression tasks, or cross-entropy error in classification tasks). We assume that the gw
533"
REFERENCES,0.3852672750977836,"are continuous, and lower-bounded by 0. To ensure the existence and uniqueness of minimum during
534"
REFERENCES,0.3859191655801825,"the balancing of any neuron, We will assume that each function gw depends only on the magnitude
535"
REFERENCES,0.38657105606258146,"|w| of the corresponding weight, and that gw is monotonically increasing from 0 to +∞(gw(0) = 0
536"
REFERENCES,0.38722294654498046,"and limx→+∞gw(x) = +∞). Clearly, L2, L1 and more generally all Lp regularizers are special
537"
REFERENCES,0.3878748370273794,"cases where, for p > 0, Lp regularization is defined by: R(W) = P"
REFERENCES,0.38852672750977835,"w |w|p.
538"
REFERENCES,0.3891786179921773,"When indicated, we may require also that the functions gw be continuously differentiable, except
539"
REFERENCES,0.3898305084745763,"perhaps at the origin in order to be able to differentiate the regularizer with respect to the λ’s and
540"
REFERENCES,0.39048239895697523,"derive closed form conditions for the corresponding optima. This is satisfied by all forms of Lp
541"
REFERENCES,0.39113428943937417,"regularization, for p > 0.
542"
REFERENCES,0.3917861799217731,"Remark D.1. Often one introduces scalar multiplicative hyperparameters to balance the effect of E
543"
REFERENCES,0.3924380704041721,"and R, for instance in the form: E = E + βR. These cases are included in the framework above:
544"
REFERENCES,0.39308996088657105,"multipliers like β can easily be absorbed into the functions gw above.
545"
REFERENCES,0.39374185136897,"Theorem D.2. (General Balance Equation). Consider a neural network with BiLU activation
546"
REFERENCES,0.394393741851369,"functions in all the hidden units and overall error function of the form:
547"
REFERENCES,0.39504563233376794,"E = E(W) + R(W)
with
R(W) =
X"
REFERENCES,0.3956975228161669,"w
gw(w)
(D.2)"
REFERENCES,0.3963494132985658,"where each function gw(w) is continuous, depends on the magnitude |w| alone, and grows monotoni-
548"
REFERENCES,0.3970013037809648,"cally from gw(0) = 0 to gw(+∞) = +∞. For any setting of the weights W and any hidden unit i in
549"
REFERENCES,0.39765319426336376,"the network and any λ > 0 we can multiply the incoming weights of i by λ and the outgoing weights
550"
REFERENCES,0.3983050847457627,"of i by 1/λ without changing the overall error E. Furthermore, there exists a unique value λ∗where
551"
REFERENCES,0.39895697522816165,"the corresponding weights v (v = λ∗w for incoming weights, v = w/λ∗for the outgoing weights)
552"
REFERENCES,0.39960886571056065,"achieve the balance equation:
553 X"
REFERENCES,0.4002607561929596,"v∈IN(i)
gw(v) =
X"
REFERENCES,0.40091264667535853,"w∈OUT (i)
gw(v)
(D.3)"
REFERENCES,0.4015645371577575,"Proof. Under the assumptions of the theorem, E is unchanged under the rescaling of the incoming and
554"
REFERENCES,0.40221642764015647,"outgoing weights of unit i due to the homogeneity property of BiLUs. Without any loss of generality,
555"
REFERENCES,0.4028683181225554,let us assume that at the beginning: P
REFERENCES,0.40352020860495436,w∈IN(i) gw(w) < P
REFERENCES,0.4041720990873533,"w∈OUT (i) gw(w). As we increase λ from
556"
REFERENCES,0.4048239895697523,"1 to +∞, by the assumptions on the functions gw, the term P"
REFERENCES,0.40547588005215124,"w∈IN(i) gw(λw) increases continuously
557"
REFERENCES,0.4061277705345502,"from its initial value to +∞, whereas the term P
w∈OUT (i) gw)w/λ) decreases continuously from
558"
REFERENCES,0.4067796610169492,"its initial value to 0. Thus, there is a unique value λ∗where the balance is realized. If at the beginning
559
P"
REFERENCES,0.4074315514993481,w∈IN(i) gw(w) > P
REFERENCES,0.40808344198174706,"w∈OUT (i) gw(w), then the same argument is applied by decreasing λ from 1
560"
REFERENCES,0.408735332464146,"to 0.
561"
REFERENCES,0.409387222946545,"Remark D.3. For simplicity, here and in other sections, we state the results in terms of a network of
562"
REFERENCES,0.41003911342894395,"BiLU units. However, the same principles can be applied to networks where only a subset of neurons
563"
REFERENCES,0.4106910039113429,"are in the BiLU class, simply by applying scaling and balancing operations to only those neurons.
564"
REFERENCES,0.41134289439374183,"Furthermore, not all BiLU neurons need to have the same BiLU activation function. For instance, the
565"
REFERENCES,0.41199478487614083,"results still hold for a mixed network containing both ReLU and linear units.
566"
REFERENCES,0.41264667535853977,"Remark D.4. In the setting of Theorem D.2, the balance equations do not necessarily minimize the
567"
REFERENCES,0.4132985658409387,"corresponding regularization term. This is addressed in the next theorem.
568"
REFERENCES,0.41395045632333766,"Remark D.5. Finally, zero weights (w = 0) can be ignored entirely as they play no role in scaling or
569"
REFERENCES,0.41460234680573665,"balancing. Furthermore, if all the incoming or outgoing weights of a hidden unit were to be zero, it
570"
REFERENCES,0.4152542372881356,"could be removed entirely from the network
571"
REFERENCES,0.41590612777053454,"Theorem D.6. (Balance and Regularizer Minimization) We now consider the same setting as in
572"
REFERENCES,0.4165580182529335,"Theorem D.2, but in addition, we assume that the functions gw are continuously differentiable, except
573"
REFERENCES,0.4172099087353325,"perhaps at the origin. Then, for any neuron, there exists at least one optimal value λ∗that minimizes
574"
REFERENCES,0.4178617992177314,"R(W). This value must be a solution of the consistency equation:
575 λ2
X"
REFERENCES,0.41851368970013036,"w∈IN(i)
wg′
w(λw) =
X"
REFERENCES,0.41916558018252936,"w∈OUT (i)
wg′
w(w/λ)
(D.4)"
REFERENCES,0.4198174706649283,"Once the weights are rebalanced accordingly, the new weights must satisfy the generalized balance
576"
REFERENCES,0.42046936114732725,"equation:
577 X"
REFERENCES,0.4211212516297262,"w∈IN(i)
wg′(w) =
X"
REFERENCES,0.4217731421121252,"w∈OUT (i)
wg′(w)
(D.5)"
REFERENCES,0.42242503259452413,"In particular, if gw(w) = |w|p for all the incoming and outgoing weights of neuron i, then the optimal
578"
REFERENCES,0.4230769230769231,"value λ∗is unique and equal to:
579"
REFERENCES,0.423728813559322,"λ∗=
P"
REFERENCES,0.424380704041721,"w∈OUT (i) |w|p
P"
REFERENCES,0.42503259452411996,w∈IN(i) |w|p
REFERENCES,0.4256844850065189,"1/2p
=
||OUT(i)||p"
REFERENCES,0.42633637548891784,||IN(i)||p
REFERENCES,0.42698826597131684,"1/2
(D.6)"
REFERENCES,0.4276401564537158,The decrease ∆R ≥0 in the value of the Lp regularizer R = P
REFERENCES,0.4282920469361147,"w |w|p is given by:
580"
REFERENCES,0.42894393741851367,"∆R =
 
X"
REFERENCES,0.42959582790091266,"w∈IN(i)
|w|p1/2 −
 
X"
REFERENCES,0.4302477183833116,"w∈OUT (i)
|w|p1/2
2
(D.7)"
REFERENCES,0.43089960886571055,"After balancing neuron i, its new weights satisfy the generalized Lp balance equation:
581 X"
REFERENCES,0.4315514993481095,"w∈IN(i)
|w|p =
X"
REFERENCES,0.4322033898305085,"w∈OUT (i)
|w|p
(D.8)"
REFERENCES,0.43285528031290743,"Proof. Due to the additivity of the regularizer, the only component of the regularizer that depends on
582"
REFERENCES,0.4335071707953064,"λ has the form:
583"
REFERENCES,0.43415906127770537,"R(λ) =
X"
REFERENCES,0.4348109517601043,"w∈IN(i)
gw(λw) +
X"
REFERENCES,0.43546284224250326,"w∈OUT (i)
gw(w/λ)
(D.9)"
REFERENCES,0.4361147327249022,"Because of the properties of the functions gw, Rλ is continuously differentiable and strictly bounded
584"
REFERENCES,0.4367666232073012,"below by 0. So it must have a minimum, as a function of λ where its derivative is zero. Its derivative
585"
REFERENCES,0.43741851368970014,"with respect to λ has the form:
586"
REFERENCES,0.4380704041720991,"R′(λ) =
X"
REFERENCES,0.438722294654498,"w∈IN(i)
wg′
w(λw) +
X"
REFERENCES,0.439374185136897,"w∈OUT (i)
(−w/λ2)g′
w(w/λ)
(D.10)"
REFERENCES,0.44002607561929596,"Setting the derivative to zero, gives:
587 λ2
X"
REFERENCES,0.4406779661016949,"w∈IN(i)
wg′
w(λw) =
X"
REFERENCES,0.44132985658409385,"w∈OUT (i)
wg′
w(w/λ)
(D.11)"
REFERENCES,0.44198174706649285,"Assuming that the left-hand side is non-zero, which is generally the case, the optimal value for λ
588"
REFERENCES,0.4426336375488918,"must satisfy:
589"
REFERENCES,0.44328552803129073,"λ =
P"
REFERENCES,0.4439374185136897,"w∈OUT (i) wg′
w(w/λ)
P"
REFERENCES,0.4445893089960887,w∈IN(i) wg′w(λw)
REFERENCES,0.4452411994784876,"1/2
(D.12)"
REFERENCES,0.44589308996088656,"If the regularizing function is the same for all the incoming and outgoing weights (gw = g), then the
590"
REFERENCES,0.44654498044328556,"optimal value λ must satisfy:
591"
REFERENCES,0.4471968709256845,"λ =
P"
REFERENCES,0.44784876140808344,"w∈OUT (i) wg′(w/λ)
P
w∈IN(i) wg′(λw)"
REFERENCES,0.4485006518904824,"1/2
(D.13)"
REFERENCES,0.4491525423728814,"In particular, if g(w) = |w|p then g(w) is differentiable except possibly at 0 and g′(w) =
592"
REFERENCES,0.4498044328552803,"s(w)p|w|p−1, where s(w) denotes the sign of the weight w. Substituting in Equation D.13, the
593"
REFERENCES,0.45045632333767927,"optimal rescaling λ must satisfy:
594"
REFERENCES,0.4511082138200782,"λ∗=
P"
REFERENCES,0.4517601043024772,"w∈OUT (i) ws(w)|w|p−1
P"
REFERENCES,0.45241199478487615,w∈IN(i) w|ws(w)|p−1
REFERENCES,0.4530638852672751,"1/2p
=
P"
REFERENCES,0.45371577574967403,"w∈OUT (i) |w|p
P"
REFERENCES,0.45436766623207303,w∈IN(i) |w|p
REFERENCES,0.455019556714472,"1/2p
=
||OUT(i)||p"
REFERENCES,0.4556714471968709,||IN(i)||p 1/2
REFERENCES,0.45632333767926986,"(D.14)
At the optimum, no further balancing is possible, and thus λ∗= 1. Equation D.11 yields immediately
595"
REFERENCES,0.45697522816166886,"the generalized balance equation to be satisfied at the optimum:
596 X"
REFERENCES,0.4576271186440678,"w∈IN(i)
wg′(w) =
X"
REFERENCES,0.45827900912646674,"w∈OUT (i)
wg′(w)
(D.15)"
REFERENCES,0.4589308996088657,"In the case of LP regularization, it is easy to check by applying Equation D.15, or by direct calculation
597"
REFERENCES,0.4595827900912647,"that:
598 X"
REFERENCES,0.4602346805736636,"w∈IN(i)
|λ∗w|p =
X"
REFERENCES,0.46088657105606257,"w∈OUT (i)
|w/λ∗|p
(D.16)"
REFERENCES,0.46153846153846156,"which is the generalized balance equation. Thus after balancing neuron, the weights of neuron i
599"
REFERENCES,0.4621903520208605,"satisfy the Lp balance (Equation D.8). The change in the value of the regularizer is given by:
600"
REFERENCES,0.46284224250325945,"∆R =
X"
REFERENCES,0.4634941329856584,"w∈IN(i)
|w|p +
X"
REFERENCES,0.4641460234680574,"w∈OUT (i)
|w|p −
X"
REFERENCES,0.46479791395045633,"w∈IN(i)
|λ∗w|p −
X"
REFERENCES,0.4654498044328553,"w∈OUT (i)
|w/λ∗|p
(D.17)"
REFERENCES,0.4661016949152542,"By substituting λ∗by its explicit value given by Equation D.14 and collecting terms gives Equation
601"
REFERENCES,0.4667535853976532,"D.7.
602"
REFERENCES,0.46740547588005216,"Remark D.7. The monotonicity of the functions gw is not needed to prove the first part of Theorem
603"
REFERENCES,0.4680573663624511,"D.6. It is only needed to prove the uniqueness of λ∗in the Lp cases.
604"
REFERENCES,0.46870925684485004,"Remark D.8. Note that the same approach applies to the case where there are multiple additive
605"
REFERENCES,0.46936114732724904,"regularizers. For instance with both L2 and L1 regularization, in this case the function f has the form:
606"
REFERENCES,0.470013037809648,"gw(w) = αw2 + β|w|. Generalized balance still applies. It also applies to the case where different
607"
REFERENCES,0.4706649282920469,"regularizers are applied in different disconnected portions of the network.
608"
REFERENCES,0.47131681877444587,"Remark D.9. The balancing of a single BiLU neuron has little to do with the number of connections.
609"
REFERENCES,0.47196870925684486,"It applies equally to fully connected neurons, or to sparsely connected neurons.
610"
REFERENCES,0.4726205997392438,"E
Scaling and Balancing Beyond BiLU Activation Functions
611"
REFERENCES,0.47327249022164275,"So far we have generalized ReLU activation functions to BiLU activation functions in the context of
612"
REFERENCES,0.47392438070404175,"scaling and balancing operations with positive scaling factors. While in the following sections we
613"
REFERENCES,0.4745762711864407,"will continue to work with BiLU activation functions, in this section we show that the scaling and
614"
REFERENCES,0.47522816166883963,"balancing operations can be extended even further to other activation functions. The section can be
615"
REFERENCES,0.4758800521512386,"skipped if one prefers to progress towards the main results on stochastic balancing.
616"
REFERENCES,0.4765319426336376,"Given a neuron with activation function f(x), during scaling instead of multiplying and dividing by
617"
REFERENCES,0.4771838331160365,"λ > 0, we could multiply the incoming weights by a function g(λ) and divide the outgoing weights
618"
REFERENCES,0.47783572359843546,"by a function h(λ), as long as the activation function f satisfies:
619"
REFERENCES,0.4784876140808344,"f(g(λ)x) = h(λ)f(x)
(E.1)"
REFERENCES,0.4791395045632334,"for every x ∈R to ensure that the contribution of the neuron to the rest of the network remains
620"
REFERENCES,0.47979139504563234,"unchanged. Note that if the activation function f satisfies Equation E.1, so does the activation
621"
REFERENCES,0.4804432855280313,"function −f. In Equation E.1, λ does not have to be positive–we will simply assume that λ belongs
622"
REFERENCES,0.4810951760104302,"to some open (potentially infinite) interval (a, b). Furthermore, the functions g and h cannot be zero
623"
REFERENCES,0.4817470664928292,"for λ ∈(a, b) since they are used for scaling. It is reasonable to assume that the functions g and h are
624"
REFERENCES,0.48239895697522817,"continuous, and thus they must have a constant sign as λ varies over (a, b).
625"
REFERENCES,0.4830508474576271,"Now, taking x = 0 gives f(0) = h(λ)f(0) for every λ ∈(a, b), and thus either f(0) = 0 or h(λ) = 1
626"
REFERENCES,0.48370273794002605,"for every λ ∈(a, b). The latter is not interesting and thus we can assume that the activation function
627"
REFERENCES,0.48435462842242505,"f satisfies f(0) = 0. Taking x = 1 gives f(g(λ)) = h(λ)f(1) for every λ in (a, b). For simplicity,
628"
REFERENCES,0.485006518904824,"let us assume that f(x) = 1. Then, we have: f(g(λ)) = h(λ) for every λ. Substituting in Equation
629"
REFERENCES,0.48565840938722293,"E.1 yields:
630"
REFERENCES,0.4863102998696219,"f(g(λ)x) = f(g(λ))f(x)
(E.2)"
REFERENCES,0.4869621903520209,"for every x ∈R and every λ ∈(a, b). This relation is essentially the same as the relation that defines
631"
REFERENCES,0.4876140808344198,"multiplicative activation functions over the corresponding domain (see Proposition A.4), and thus
632"
REFERENCES,0.48826597131681876,"we can identify a key family of solutions using power functions. Note that we can define a new
633"
REFERENCES,0.48891786179921776,"parameter µ = g(λ), where µ ranges also over some positive or negative interval I over which:
634"
REFERENCES,0.4895697522816167,"f(µx) = f(µ)f(x).
635"
REFERENCES,0.49022164276401564,"E.1
Bi-Power Units (BiPU)
636"
REFERENCES,0.4908735332464146,"Let us assume that λ > 0, g(λ) = λ and h(λ) = λc for some c ∈R. Then the activation function
637"
REFERENCES,0.4915254237288136,"must satisfy the equation:
638"
REFERENCES,0.4921773142112125,"f(λx) = λcf(x)
(E.3)"
REFERENCES,0.49282920469361147,"for any x ∈R and any λ > 0. Note that if f(x) = xc we get a multiplicative activation function.
639"
REFERENCES,0.4934810951760104,"More generally, these functions are characterized by the following proposition.
640"
REFERENCES,0.4941329856584094,"Proposition E.1. The set of activation functions f satisfying f(λx) = λcf(x) for any x ∈R and
641"
REFERENCES,0.49478487614080835,"any λ > 0 consist of the functions of the form:
642"
REFERENCES,0.4954367666232073,"f(x) =
Cxc
if
x ≥0
Dxc
if
x < 0.
(E.4)"
REFERENCES,0.49608865710560623,"where c ∈R, C = f(1) ∈R, and D = f(−1) ∈R. We call these bi-power units (BiPU). If, in
643"
REFERENCES,0.49674054758800523,"addition, we want f to be continuous at 0, we must have either c > 0, or c = 0 with C = D.
644"
REFERENCES,0.4973924380704042,"Given the general shape, these activations functions can be called BiPU (Bi-Power-Units). Note that
645"
REFERENCES,0.4980443285528031,"in the general case where c > 0, C and D do not need to be equal. In particular, one of them can
646"
REFERENCES,0.49869621903520206,"be equal to zero, and the other one can be different from zero giving rise to “rectified power units”
647"
REFERENCES,0.49934810951760106,"(Figure 6).
648"
REFERENCES,0.5,"Linear
Leaky ReLU
BiPU (D=0,C=1,c=2)
BiPU (D=1,C=1,c=2)"
REFERENCES,0.500651890482399,Figure 6
REFERENCES,0.5013037809647979,"Proof. By taking x = 1, we get f(λ) = f(1)λc for any λ > 0. Let f(1) = C. Then we see
649"
REFERENCES,0.5019556714471969,"that for any x > 0 we must have: f(x) = Cxc. In addition, for every λ > 0 we must have:
650"
REFERENCES,0.5026075619295959,"f(λ0) = f(0) = λcf(0). So if c = 0, then f(x) = C = f(1) for x ≥0. If c ̸= 0, then f(0) = 0. In
651"
REFERENCES,0.5032594524119948,"this case, if we want the activation function to be continuous, then we see that we must have c ≥0. So
652"
REFERENCES,0.5039113428943938,"in summary for x > 0 we must have f(x) = f(1)xc = Cxc. For the function to be right continuous
653"
REFERENCES,0.5045632333767927,"at 0, we must have either f(0) = f(1) = C with c = 0 or f(0) = 0 with c > 0. We can now look
654"
REFERENCES,0.5052151238591917,"at negative values of x. By the same reasoning, we have f(λ(−1)) = f(−λ) = λcf(−1) for any
655"
REFERENCES,0.5058670143415906,"λ > 0. Thus for any x < 0 we must have: f(x) = f(−1)|x|c = D|x|c where D = f(−1). Thus, if
656"
REFERENCES,0.5065189048239895,"f is continuous, there are two possibilities. If c = 0, then we must have C = f(1) = D(f −1)−and
657"
REFERENCES,0.5071707953063885,"thus f(x) = C everywhere. If c ̸= 0, then continuity requires that c > 0. In this case f(x) = Cxc
658"
REFERENCES,0.5078226857887875,"for x ≥0 with C = f(1), and f(x) = Dxc for x < 0 with f(−1) = D. In all cases, it is easy to
659"
REFERENCES,0.5084745762711864,"check directly that the resulting functions satisfy the functional equation given by Equation E.3.
660"
REFERENCES,0.5091264667535854,"E.2
Scaling BiPU Neurons
661"
REFERENCES,0.5097783572359843,"A BiPU neuron can be scaled by multiplying its incoming weight by λ > 0 and dividing its outgoing
662"
REFERENCES,0.5104302477183833,"weights by 1/λc. This will not change the role of the corresponding unit in the network, and thus it
663"
REFERENCES,0.5110821382007823,"will not change the input-output function of the network.
664"
REFERENCES,0.5117340286831812,"E.3
Balancing BiPU Neurons
665"
REFERENCES,0.5123859191655802,"As in the case of BiLU neurons, we balance a multiplicative neuron by asking what is the optimal
666"
REFERENCES,0.5130378096479792,"scaling factor λ that optimizes a particular regularizer. For simplicity, here we assume that the
667"
REFERENCES,0.5136897001303781,"regularizer is in the Lp class. Then we are interested in the value of λ > 0 that minimizes the
668"
REFERENCES,0.5143415906127771,"function:
669 λp X"
REFERENCES,0.5149934810951761,"w∈IN
|w|p + 1 λpc
X"
REFERENCES,0.515645371577575,"w∈OUT
|w|p
(E.5)"
REFERENCES,0.516297262059974,"A simple calculation shows that the optimal value of λ is given by:
670"
REFERENCES,0.5169491525423728,"λ∗=
c P
OUT |w|p
P"
REFERENCES,0.5176010430247718,IN |w|p
REFERENCES,0.5182529335071708,"1/p(c+1)
(E.6)"
REFERENCES,0.5189048239895697,"Thus after balancing the weights, the neuron must satisfy the balance equation:
671 c
X"
REFERENCES,0.5195567144719687,"OUT
|w|p =
X"
REFERENCES,0.5202086049543677,"IN
|w|p
(E.7)"
REFERENCES,0.5208604954367666,"in the new weights w.
672"
REFERENCES,0.5215123859191656,"So far, we have focused on balancing individual neurons. In the next two sections, we look at
673"
REFERENCES,0.5221642764015645,"balancing across all the units of a network. We first look at what happens to network balance when a
674"
REFERENCES,0.5228161668839635,"network is trained by gradient descent and then at what happens to network balance when individual
675"
REFERENCES,0.5234680573663625,"neurons are balanced iteratively in a regular or stochastic manner.
676"
REFERENCES,0.5241199478487614,"F
Network Balance: Gradient Descent
677"
REFERENCES,0.5247718383311604,"A natural question is whether gradient descent (or stochastic gradient descent) applied to a network of
678"
REFERENCES,0.5254237288135594,"BiLU neurons, with or without a regularizer, converges to a balanced state of the network, where all
679"
REFERENCES,0.5260756192959583,"the BiLU neurons are balanced. So we first consider the case where there is no regularizer (E = E).
680"
REFERENCES,0.5267275097783573,"The results in Du et al. [2018] may suggest that gradient descent may converge to a balanced state. In
681"
REFERENCES,0.5273794002607562,"particular, they write that for any neuron i:
682"
REFERENCES,0.5280312907431551,"d
dt
 
X"
REFERENCES,0.5286831812255541,"w∈IN(i)
w2 −
X"
REFERENCES,0.529335071707953,"w∈OUT (i)
w2
= 0
(F.1)"
REFERENCES,0.529986962190352,"Thus the gradient flow exactly preserves the difference between the L2 cost of the incoming and
683"
REFERENCES,0.530638852672751,"outgoing weights or, in other words, the derivative of the L2 balance deficit is zero. Thus if one were
684"
REFERENCES,0.5312907431551499,"to start from a balanced state and use an infinitesimally small learning rate one ought to stay in a
685"
REFERENCES,0.5319426336375489,"balanced state at all times.
686"
REFERENCES,0.5325945241199479,"However, it must be noted that this result was derived for the L2 metric only, and thus would not
687"
REFERENCES,0.5332464146023468,"cover other Lp forms of balance. Furthermore, it requires an infinitesimally small learning rate. In
688"
REFERENCES,0.5338983050847458,"practice, when any standard learning rate is applied, we find that gradient descent does not converge
689"
REFERENCES,0.5345501955671447,"to a balanced state (Figure 1). However, things are different when a regularizer term is included in
690"
REFERENCES,0.5352020860495437,"the error functions as described in the following theorem.
691"
REFERENCES,0.5358539765319427,"Theorem F.1. Gradient descent in a network of BiLU units with error function E = E + R where R
692"
REFERENCES,0.5365058670143416,"has the properties described in Theorem D.6 (including all Lp) must converge to a balanced state,
693"
REFERENCES,0.5371577574967406,"where every BiLU neuron is balanced.
694"
REFERENCES,0.5378096479791395,"Proof. By contradiction, suppose that gradient descent converges to a state that is unbalanced and
695"
REFERENCES,0.5384615384615384,"where the gradient with respect to all the weights is zero. Then there is at least one unbalanced neuron
696"
REFERENCES,0.5391134289439374,"in the network. We can then multiply the incoming weights of such a neuron by λ and the outgoing
697"
REFERENCES,0.5397653194263363,"weights by 1/λ as in the previous section without changing the value of E. Since the neuron is not in
698"
REFERENCES,0.5404172099087353,"balance, we can move λ infinitesimally so as to reduce R, and hence E. But this contradicts the fact
699"
REFERENCES,0.5410691003911343,"that the gradient is zero.
700"
REFERENCES,0.5417209908735332,"Remark F.2. In practice, in the case of stochastic gradient descent applied to E + R, at the end of
701"
REFERENCES,0.5423728813559322,"learning the algorithm may hover around a balanced state. If the state reached by the stochastic
702"
REFERENCES,0.5430247718383312,"gradient descent procedure is not approximately balanced, then learning ought to continue. In other
703"
REFERENCES,0.5436766623207301,"words, the degree of balance could be used to monitor whether learning has converged or not. Balance
704"
REFERENCES,0.5443285528031291,"is a necessary, but not sufficient, condition for being at the optimum.
705"
REFERENCES,0.5449804432855281,"Remark F.3. If early stopping is being used to control overfitting, there is no reason for the stopping
706"
REFERENCES,0.545632333767927,"state to be balanced. However, the balancing algorithms described in the next section could be used
707"
REFERENCES,0.546284224250326,"to balance this state.
708"
REFERENCES,0.5469361147327249,"G
Network Balance: Stochastic or Deterministic Balancing Algorithms
709"
REFERENCES,0.5475880052151239,"In this section, we look at balancing algorithms where, starting from an initial weight configuration
710"
REFERENCES,0.5482398956975228,"W, the BiLU neurons of a network are balanced iteratively according to some deterministic or
711"
REFERENCES,0.5488917861799217,"stochastic schedule that periodically visits all the neurons. We can also include algorithms where
712"
REFERENCES,0.5495436766623207,"neurons are partitioned into groups (e.g. neuronal layers) and neurons in each group are balanced
713"
REFERENCES,0.5501955671447197,"together.
714"
REFERENCES,0.5508474576271186,"G.1
Basic Stochastic Balancing
715"
REFERENCES,0.5514993481095176,"The most interesting algorithm is when the BiLU neurons of a network are iteratively balanced
716"
REFERENCES,0.5521512385919165,"in a purely stochastic manner. This algorithm is particularly attractive from the standpoint of
717"
REFERENCES,0.5528031290743155,"physically implemented neural networks because the balancing algorithm is local and the updates
718"
REFERENCES,0.5534550195567145,"occur randomly without the need for any kind of central coordination. As we shall see in the following
719"
REFERENCES,0.5541069100391134,"section, the random local operations remarkably lead to a unique form of global order. The proof
720"
REFERENCES,0.5547588005215124,"for the stochastic case extends immediately to the deterministic case, where the BiLU neurons are
721"
REFERENCES,0.5554106910039114,"updated in a deterministic fashion, for instance by repeatedly cycling through them according to
722"
REFERENCES,0.5560625814863103,"some fixed order.
723"
REFERENCES,0.5567144719687093,"G.2
Subset Balancing (Independent or Tied)
724"
REFERENCES,0.5573663624511083,"It is also possible to partition the BiLU neurons into non-overlapping subsets of neurons, and then
725"
REFERENCES,0.5580182529335072,"balance each subset, especially when the neurons in each subset are disjoint of each other. In this
726"
REFERENCES,0.5586701434159062,"case, one can balance all the neurons in a given subset, and repeat this subset-balancing operation
727"
REFERENCES,0.559322033898305,"subset-by-subset, again in a deterministic or stochastic manner. Because the BiLU neurons in each
728"
REFERENCES,0.559973924380704,"subset are disjoint, it does not matter whether the neurons in a given subset are updated synchronously
729"
REFERENCES,0.560625814863103,"or sequentially (and in which order). Since the neurons are balanced independently of each other,
730"
REFERENCES,0.5612777053455019,"this can be called independent subset balancing. For example, in a layered feedforward network with
731"
REFERENCES,0.5619295958279009,"no lateral connections, each layer corresponds to a subset of disjoint neurons. The incoming and
732"
REFERENCES,0.5625814863102999,"outgoing connections of each neuron are distinct from the incoming and outgoing connections of
733"
REFERENCES,0.5632333767926988,"any other neuron in the layer, and thus the balancing operation of any neuron in the layer does not
734"
REFERENCES,0.5638852672750978,"interfere with the balancing operation of any other neuron in the same layer. So this corresponds to
735"
REFERENCES,0.5645371577574967,"independent layer balancing,
736"
REFERENCES,0.5651890482398957,"As a side note, balancing a layer h, may disrupt the balance of layer h + 1. However, balancing
737"
REFERENCES,0.5658409387222947,"layers h and h + 2 (or any other layer further apart) can be done without interference of the balancing
738"
REFERENCES,0.5664928292046936,"processes. This suggests also an alternating balancing scheme, where one alternatively balances all
739"
REFERENCES,0.5671447196870926,"the odd-numbered layers, and all the evenly-numbered layers.
740"
REFERENCES,0.5677966101694916,"Yet another variation is when the neurons in a disjoint subset are tied to each other in the sense that
741"
REFERENCES,0.5684485006518905,"they must all share the same scaling factor λ. In this case, balancing the subset requires finding the
742"
REFERENCES,0.5691003911342895,"optimal λ for the entire subset, as opposed to finding the optimal λ for each neuron in the subset.
743"
REFERENCES,0.5697522816166884,"Since the neurons are balanced in a coordinated or tied fashion, this can be called coordinated or tied
744"
REFERENCES,0.5704041720990873,"subset balancing. For example, tied layer balancing must use the same λ for all the neurons in a given
745"
REFERENCES,0.5710560625814863,"layer. It is easy to see that this approach leads to layer synaptic balance which has the form (for an
746"
REFERENCES,0.5717079530638852,"Lp regularizer):
747 X i X"
REFERENCES,0.5723598435462842,"w∈IN(i)
|w|p =
X i X"
REFERENCES,0.5730117340286832,"w∈OUT (i)
|w|p
(G.1)"
REFERENCES,0.5736636245110821,"where i runs over all the neurons in the layer. This does not necessarily imply that each neuron
748"
REFERENCES,0.5743155149934811,"in the layer is individually balanced. Thus neuronal balance for every neuron in a layer implies
749"
REFERENCES,0.5749674054758801,"layer balance, but the converse is not true. Independent layer balancing will lead to layer balance.
750"
REFERENCES,0.575619295958279,"Coordinated layer balancing will lead to layer balance, but not necessarily to neuronal balance of
751"
REFERENCES,0.576271186440678,"each neuron in the layer. Layer-wise balancing, independent or tied, can be applied to all the layers
752"
REFERENCES,0.5769230769230769,"and in a deterministic (e.g. sequential) or stochastic manner. Again the proof given in the next section
753"
REFERENCES,0.5775749674054759,"for the basic stochastic algorithm can easily be applied to these cases (see also Appendix B).
754"
REFERENCES,0.5782268578878749,"G.3
Remarks about Weight Sharing and Convolutional Neural Networks
755"
REFERENCES,0.5788787483702738,"Suppose that two connections share the same weight so that we must have: wij = wkl at all times.
756"
REFERENCES,0.5795306388526728,"In general, when the balancing algorithm is applied to neuron i or j, the weight wij will change
757"
REFERENCES,0.5801825293350718,"and the same change must be applied to wkl. The latter may disrupt the balance of neuron k or l.
758"
REFERENCES,0.5808344198174706,"Furthermore, this may not lead to a decrease in the overall value of the regularizer R.
759"
REFERENCES,0.5814863102998696,"The case of convolutional networks is somewhat special, since all the incoming weights of the
760"
REFERENCES,0.5821382007822686,"neurons sharing the same convolutional kernel are shared. However, in general, the outgoing weights
761"
REFERENCES,0.5827900912646675,"are not shared. Furthermore, certain operations like max-pooling are not homogeneous. So if one
762"
REFERENCES,0.5834419817470665,"trains a CNN with E alone, or even with E + R, one should not expect any kind of balance to emerge
763"
REFERENCES,0.5840938722294654,"in the convolution units. However, all the other BiLU units in the network should become balanced
764"
REFERENCES,0.5847457627118644,"by the same argument used for gradient descent above. The balancing algorithm applied to individual
765"
REFERENCES,0.5853976531942634,"neurons, or the independent layer balancing algorithm, will not balance individual neurons sharing
766"
REFERENCES,0.5860495436766623,"the same convolution kernel. The only balancing algorithm that could lead to some convolution layer
767"
REFERENCES,0.5867014341590613,"balance, but not to individual neuronal balance, is the coordinated layer balancing, where the same λ
768"
REFERENCES,0.5873533246414603,"is used for all the neurons in the same convolution layer, provided that their activation functions are
769"
REFERENCES,0.5880052151238592,"BiLU functions.
770"
REFERENCES,0.5886571056062582,"We can now study the convergence properties of balancing algorithms.
771"
REFERENCES,0.5893089960886571,"H
Convergence of Balancing Algorithms
772"
REFERENCES,0.589960886571056,"We now consider the basic stochastic balancing algorithm, where BiLU neurons are iteratively and
773"
REFERENCES,0.590612777053455,"stochastically balanced. It is essential to note that balancing a neuron j may break the balance of
774"
REFERENCES,0.5912646675358539,"another neuron i to which j is connected. Thus convergence of iterated balancing is not obvious.
775"
REFERENCES,0.5919165580182529,"There are three key questions to be addressed for the basic stochastic algorithm, as well as all the
776"
REFERENCES,0.5925684485006519,"other balancing variations. First, does the value of the regularizer converge to a finite value? Second,
777"
REFERENCES,0.5932203389830508,"do the weights themselves converge to fixed finite values representing a balanced state for the entire
778"
REFERENCES,0.5938722294654498,"network? And third, if the weights converge, do they always converge to the same values, irrespective
779"
REFERENCES,0.5945241199478487,"of the order in which the units are being balanced? In other words, given an initial state W for the
780"
REFERENCES,0.5951760104302477,"network, is there a unique corresponding balanced state, with the same input-output functionalities?
781"
REFERENCES,0.5958279009126467,"H.1
Notation and Key Questions
782"
REFERENCES,0.5964797913950456,"For simplicity, we use a continuous time notation. After a certain time t each neuron has been
783"
REFERENCES,0.5971316818774446,"balanced a certain number of times. While the balancing operations are not commutative as balancing
784"
REFERENCES,0.5977835723598436,"operations, they are commutative as scaling operations. Thus we can reorder the scaling operations
785"
REFERENCES,0.5984354628422425,"and group them neuron by neuron so that, for instance, neuron i has been scaled by the sequence of
786"
REFERENCES,0.5990873533246415,"scaling operations:
787"
REFERENCES,0.5997392438070405,"Sλ∗
1(i)Sλ∗
2(i) . . . Sλ∗nit(i) = SΛi(t)(i)
(H.1)"
REFERENCES,0.6003911342894394,"where nit corresponds to the count of the last update of neuron i prior to time t, and:
788"
REFERENCES,0.6010430247718384,"Λi(t) =
Y"
REFERENCES,0.6016949152542372,"1≤n≤nit
λ∗
n(i)
(H.2)"
REFERENCES,0.6023468057366362,"For the input and output units, we can consider that their balancing coefficients λ∗are always equal
789"
REFERENCES,0.6029986962190352,"to 1 (at all times) and therefore Λi(t) = 1 for any visible unit i.
790"
REFERENCES,0.6036505867014341,"Thus, we first want to know if R converges. Second, we want to know if the weights converge. This
791"
REFERENCES,0.6043024771838331,"question can be split into two sub-questions: (1) Do the balancing factors λ∗
n(i) converge to a limit as
792"
REFERENCES,0.6049543676662321,"time goes to infinity? Even if the λ∗
n(i)’s converge to a limit, this does not imply that the weights of
793"
REFERENCES,0.605606258148631,"the network converge to a limit. After a time t, the weight wij(t) between neuron j and neuron i has
794"
REFERENCES,0.60625814863103,"the value wijΛi(t)/Λj(t), where wij = wij(0) is the value of the weight at the start of the stochastic
795"
REFERENCES,0.6069100391134289,"balancing algorithm. Thus: (2) Do the quantities Λi(t) converge to finite values, different from 0?
796"
REFERENCES,0.6075619295958279,"And third, if the weights converge to finite values different from 0, are these values unique or not, i.e.
797"
REFERENCES,0.6082138200782269,"do they depend on the details of the stochastic updates or not? These questions are answered by the
798"
REFERENCES,0.6088657105606258,"following main theorem..
799"
REFERENCES,0.6095176010430248,"H.2
Convergence of the Basic Stochastic Balancing Algorithm to a Unique Optimum
800"
REFERENCES,0.6101694915254238,"Theorem H.1. (Convergence of Stochastic Balancing) Consider a network of BiLU neurons with an
801"
REFERENCES,0.6108213820078227,"error function E(W) = E(W)+R(W) where R satisfies the conditions of Theorem D.2 including all
802"
REFERENCES,0.6114732724902217,"Lp (p > 0). Let W denote the initial weights. When the neuronal stochastic balancing algorithm is
803"
REFERENCES,0.6121251629726207,"applied throughout the network so that every neuron is visited from time to time, then E(W) remains
804"
REFERENCES,0.6127770534550195,"unchanged but R(W) must converge to some finite value that is less or equal to the initial value,
805"
REFERENCES,0.6134289439374185,"strictly less if the initial weights are not balanced. In addition, for every neuron i, λ∗
i (t) →1 and
806"
REFERENCES,0.6140808344198174,"Λi(t) →Λi as t →∞, where Λi is finite and Λi > 0 for every i. As a result, the weights themselves
807"
REFERENCES,0.6147327249022164,"must converge to a limit W ′ which is globally balanced, with E(W) = E(W ′) and R(W) ≥R(W ′),
808"
REFERENCES,0.6153846153846154,"and with equality if only if W is already balanced. Finally, W ′ is unique as it corresponds to the
809"
REFERENCES,0.6160365058670143,"solution of a strictly convex optimization problem in the variables Lij = log(Λi/Λj) with linear
810"
REFERENCES,0.6166883963494133,"constraints of the form P
π Lij = 0 along any path π joining an input unit to an output unit and along
811"
REFERENCES,0.6173402868318123,"any directed cycle (for recurrent networks). Stochastic balancing projects to stochastic trajectories in
812"
REFERENCES,0.6179921773142112,"the linear manifold that run from the origin to the unique optimal configuration.
813"
REFERENCES,0.6186440677966102,"Proof. Each individual balancing operation leaves E(W) unchanged because the BiLU neurons are
814"
REFERENCES,0.6192959582790091,"homogeneous. Furthermore, each balancing operation reduces the regularization error R(W), or
815"
REFERENCES,0.6199478487614081,"leaves it unchanged. Since the regularizer is lower-bounded by zero, the value of the regularizer must
816"
REFERENCES,0.6205997392438071,"approach a limit as the stochastic updates are being applied.
817"
REFERENCES,0.621251629726206,"For the second question, when neuron i is balanced at some step, we know that the regularizer R
818"
REFERENCES,0.621903520208605,"decreases by:
819"
REFERENCES,0.622555410691004,"∆R =
 
X"
REFERENCES,0.6232073011734028,"w∈IN(i)
|w|p1/2 −
 
X"
REFERENCES,0.6238591916558018,"w∈OUT (i)
|w|p1/2
2
(H.3)"
REFERENCES,0.6245110821382008,"If the convergence were to occur in a finite number of steps, then the coefficients λ∗
i (t) must become
820"
REFERENCES,0.6251629726205997,"equal and constant to 1 and the result is obvious. So we can focus on the case where the convergence
821"
REFERENCES,0.6258148631029987,"does not occur in a finite number of steps (indeed this is the main scenario, as we shall see at the end
822"
REFERENCES,0.6264667535853976,"of the proof). Since ∆R →0, we must have:
823 X"
REFERENCES,0.6271186440677966,"w∈IN(i)
|w|p →
X"
REFERENCES,0.6277705345501956,"w∈OUT (i)
|w|p
(H.4)"
REFERENCES,0.6284224250325945,"But from the expression for λ∗(Equation D.14), this implies that for every i, λ∗
n(i) →1 as time
824"
REFERENCES,0.6290743155149935,"increases (n →∞). This alone is not sufficient to prove that Λi(t) converges for every i as t →∞.
825"
REFERENCES,0.6297262059973925,"However, it is easy to see that Λi(t) cannot contain a sub-sequence that approaches 0 or ∞(Figure 7).
826"
REFERENCES,0.6303780964797914,"Furthermore, not only ∆R converges to 0, but the series P ∆R is convergent. This shows that, for
827"
REFERENCES,0.6310299869621904,"every i, ∆i(t) must converge to a finite, non-zero value ∆i. Therefore all the weights must converge
828"
REFERENCES,0.6316818774445893,"to fixed values given by wij(0)Λi/Λj.
829"
REFERENCES,0.6323337679269883,"Finally, we prove that given an initial set of weights W, the final balanced state is unique and
830"
REFERENCES,0.6329856584093873,"independent of the order of the balancing operations. The coefficients Λi corresponding to a globally
831"
REFERENCES,0.6336375488917861,"balanced state must be solutions of the following optimization problem:
832"
REFERENCES,0.6342894393741851,"min
Λ R(Λ) =
X"
REFERENCES,0.6349413298565841,"ij
| Λi"
REFERENCES,0.635593220338983,"Λj
wij|p
(H.5)"
REFERENCES,0.636245110821382,"under the simple constraints: Λi > 0 for all the BiLU hidden units, and Λi = 1 for all the visible (input
833"
REFERENCES,0.636897001303781,"and output) units. In this form, the problem is not convex. Introducing new variables Mj = 1/Λj
834"
REFERENCES,0.6375488917861799,"is not sufficient to render the problem convex. Using variables Mij = Λi/Λj is better, but still
835"
REFERENCES,0.6382007822685789,"problematic for 0 < p ≤1. However, let us instead introduce the new variables Lij = log(Λi/Λj).
836"
REFERENCES,0.6388526727509778,"These are well defined since we know that Λi/Λj > 0. The objective now becomes:
837"
REFERENCES,0.6395045632333768,"min R(L) =
X"
REFERENCES,0.6401564537157758,"ij
|eLijwij|p =
X"
REFERENCES,0.6408083441981747,"ij
epLij|wij|p
(H.6)"
REFERENCES,0.6414602346805737,"This objective is strictly convex in the variables Lij, as a sum of strictly convex functions (exponen-
838"
REFERENCES,0.6421121251629727,"tials). However, to show that it is a convex optimization problem we need to study the constraints on
839"
REFERENCES,0.6427640156453716,"ȿ1(t)=1
ȿ2(t)
ȿ3(t)
ȿ4(t)
ȿ5(t)=1
ȿ2(t)/ȿ1(t)
ȿ3(t)/ȿ2(t)
ȿ4(t)/ȿ3(t)
ȿ5(t)/ȿ4(t)"
REFERENCES,0.6434159061277706,"Input Unit
Output Unit"
REFERENCES,0.6440677966101694,"Figure 7: A path with three hidden BiLU units connecting one input unit to one output unit. During the
application of the stochastic balancing algorithm, at time t each unit i has a cumulative scaling factor Λi(t),
and each directed edge from unit j to unit i has a scaling factor Mij(t) = Λi(t)/Λj(t). The λi(t) must
remain within a finite closed interval away from 0 and infinity. To see this, imagine for instance that there
is a subsequence of Λ3(t) that approaches 0. Then there must be a corresponding subsequence of Λ4(t) that
approaches 0, or else the contribution of the weight w43Λ4(t)/Λ3(t) to the regularizer would go to infinity. But
then, as we reach the output layer, the contribution of the last weight w54Λ5(t)/Λ4(t) to the regularizer goes to
infinity because Λ5(t) is fixed to 1 and cannot compensate for the small values of Λ4(t). And similarly, if there
is a subsequence of Λ3(t) going to infinity, we obtain a contradiction by propagating its effect towards the input
layer."
REFERENCES,0.6447196870925684,"Λ1
Λ2
Λ3
Λ4
Λ5
Λ2/Λ1
Λ3/Λ2
Λ4/Λ3
Λ5/Λ4"
REFERENCES,0.6453715775749674,"Input Unit
Output Unit"
REFERENCES,0.6460234680573663,"Figure 8: A path with five units. After the stochastic balancing algorithm has converged, each unit i has a scaling
factor Λi, and each directed edge from unit j to unit i has a scaling factor Mij = Λi/Λj. The products of the
Mij’s along the path is given by: Λ2"
REFERENCES,0.6466753585397653,"Λ1
Λ3
Λ2
Λ4
Λ3
Λ5
Λ4 = Λ5"
REFERENCES,0.6473272490221643,"Λ1 . Accordingly, if we sum the variables Lij = log Mij
along the directed path, we get L21 +L32 +L43 +L54 = log Λ5 −log Λ1. In particular, if unit 1 is an input unit
and unit 5 is an output unit, we must have Λ1 = Λ5 = 1 and thus: L21 +L32 +L43 +L54 = 0. Likewise, in the
case of a directed cycle where unit 1 and unit 5 are the same, we must have: L21 + L32 + L43 + L54 + L15 = 0."
REFERENCES,0.6479791395045632,"the variables Lij. From the set of Λi’s it is easy to construct a unique set of Lij. However what about
840"
REFERENCES,0.6486310299869622,"the converse?
841"
REFERENCES,0.6492829204693612,"Definition H.2. A set of real numbers Lij, one per connection of a given neural architecture, is
842"
REFERENCES,0.6499348109517601,"self-consistent if and only if there is a unique corresponding set of numbers Λi > 0 (one per unit)
843"
REFERENCES,0.6505867014341591,"such that: Λi = 1 for all visible units and Lij = log Λi/Λj for every directed connection from a unit
844"
REFERENCES,0.651238591916558,"j to a unit i.
845"
REFERENCES,0.651890482398957,"Remark H.3. This definition depends on the graph of connections, but not on the original values of
846"
REFERENCES,0.652542372881356,"the synaptic weights. Every balanced state is associated with a self-consistent set of Lij, but not
847"
REFERENCES,0.6531942633637549,"every self-consistent set of Lij is associated with a balanced state.
848"
REFERENCES,0.6538461538461539,"Proposition H.4. A set Lij associated with a neural architecture is self-consistent if and only if
849
P"
REFERENCES,0.6544980443285529,"π Lij = 0 where π is any directed path connecting an input unit to an output unit or any directed
850"
REFERENCES,0.6551499348109517,"cycle (for recurrent networks).
851"
REFERENCES,0.6558018252933507,"Remark H.5. Thus the constraints associated with being a self-consistent configuration of Lij’ s
852"
REFERENCES,0.6564537157757496,"are all linear. This linear manifold of constraints depends only on the architecture, i.e., the graph of
853"
REFERENCES,0.6571056062581486,"connections. The strictly convex function R(Lij) depends on the actual weights W. Different sets of
854"
REFERENCES,0.6577574967405476,"weights W produce different convex functions over the same linear manifold.
855 ɲ
ɴ"
REFERENCES,0.6584093872229465,"ɶ
ɷ
ȿ’i ȿi"
REFERENCES,0.6590612777053455,"Figure 9: Consider two paths α + β and γ + δ from the input layer to the output layer going through the
same unit i. Let us assume that the first path assigns a multiplier Λi to unit i and the second path assigns a
multiplier Λ′
i to the same unit. By assumption we must have: P"
REFERENCES,0.6597131681877445,α Lij + P
REFERENCES,0.6603650586701434,"β Lij = 0 for the first path, and
P"
REFERENCES,0.6610169491525424,γ Lij + P
REFERENCES,0.6616688396349413,"δ Lij = 0. But α + δ and γ + β are also paths from the input layer to the output layer and
therefore: P"
REFERENCES,0.6623207301173403,α Lij +P
REFERENCES,0.6629726205997393,δ Lij = 0 and P
REFERENCES,0.6636245110821382,γ Lij +P
REFERENCES,0.6642764015645372,"β Lij = 0. As a result, P"
REFERENCES,0.6649282920469362,α Lij = log Λi = P
REFERENCES,0.665580182529335,"γ Lij = Λ′
i.
Therefore the assignment of the multiplier Λi must be consistent across different paths going through unit i."
REFERENCES,0.666232073011734,"Remark H.6. Note that one could coalesce all the input units and all output units into a single unit,
856"
REFERENCES,0.666883963494133,"in which case a path from an input unit to and output unit becomes also a directed cycle. In this
857"
REFERENCES,0.6675358539765319,"representation, the constraints are that the sum of the Lij must be zero along any directed cycle. In
858"
REFERENCES,0.6681877444589309,"general, it is not necessary to write a constraint for every path from input units to output units. It is
859"
REFERENCES,0.6688396349413298,"sufficient to select a representative set of paths such that every unit appears in at least one path.
860"
REFERENCES,0.6694915254237288,"Proof. If we look at any directed path π from unit i to unit j, it is easy to see that we must have:
861 X"
REFERENCES,0.6701434159061278,"π
Lkl = log Λi −log Λj
(H.7)"
REFERENCES,0.6707953063885267,"This is illustrated in Figures 8 and 1. Thus along any directed path that connects any input unit to any
862"
REFERENCES,0.6714471968709257,"output unit, we must have P"
REFERENCES,0.6720990873533247,"π Lij = 0. In addition, for recurrent neural networks, if π is a directed
863"
REFERENCES,0.6727509778357236,cycle we must also have: P
REFERENCES,0.6734028683181226,"π Lij = 0. Thus in short we only need to add linear constraints of the
864"
REFERENCES,0.6740547588005215,form: P
REFERENCES,0.6747066492829205,"π Lij = 0. Any unit is situated on a path from an input unit to an output unit. Along that
865"
REFERENCES,0.6753585397653195,"path, it is easy to assign a value Λi to each unit by simple propagation starting from the input unit
866"
REFERENCES,0.6760104302477183,"which has a multiplier equal to 1. When the propagation terminates in the output unit, it terminates
867"
REFERENCES,0.6766623207301173,"consistently because the output unit has a multiplier equal to 1 and, by assumption, the sum of the
868"
REFERENCES,0.6773142112125163,"multipliers along the path must be zero. So we can derive scaling values Λi from the variables
869"
REFERENCES,0.6779661016949152,"Lij. Finally, we need to show that there are no clashes, i.e. that it is not possible for two different
870"
REFERENCES,0.6786179921773142,"propagation paths to assign different multiplier values to the same unit i. The reason for this is
871"
REFERENCES,0.6792698826597132,"illustrated in Figure 9.
872"
REFERENCES,0.6799217731421121,"We can now complete the proof Theorem H.1. Given a neural network of BiLUs with a set of weights
873"
REFERENCES,0.6805736636245111,"W, we can consider the problem of minimizing the regularizer R(Lij over the self-admissible
874"
REFERENCES,0.68122555410691,"configuration Lij. For any P > 0, the Lp regularizer is strictly convex and the space of self-
875"
REFERENCES,0.681877444589309,"admissible configurations is linear and hence convex. Thus this is a strictly convex optimization
876"
REFERENCES,0.682529335071708,"problem that has a unique solution (Figure 2). Note that the minimization is carried over self-
877"
REFERENCES,0.6831812255541069,"consistent configurations, which in general are not associated with balanced states. However, the
878"
REFERENCES,0.6838331160365059,"configuration of the weights associated with the optimum set of Lij (point A in Figure 2) must be
879"
REFERENCES,0.6844850065189049,"balanced. To see this, imagine that one of the BiLU units–unit i in the network is not balanced. Then
880"
REFERENCES,0.6851368970013038,"we can balance it using a multiplier λ∗
i and replace Λi by Λ′
i = Λiλ∗. It is easy to check that the new
881"
REFERENCES,0.6857887874837028,"configuration including Λ′
i is self-consistent. Thus, by balancing unit i, we are able to reach a new
882"
REFERENCES,0.6864406779661016,"self-consistent configuration with a lower value of R which contradicts the fact that we are at the
883"
REFERENCES,0.6870925684485006,"global minimum of the strictly convex optimization problem.
884"
REFERENCES,0.6877444589308996,"We know that the stochastic balancing algorithm always converges to a balanced state. We need to
885"
REFERENCES,0.6883963494132985,"show that it cannot converge to any other balanced state, and in fact that the global optimum is the
886"
REFERENCES,0.6890482398956975,"only balanced state. By contradiction, suppose it converges to a different balanced state associated
887"
REFERENCES,0.6897001303780965,"with the coordinates (LB
ij) (point B in Figure 2). Because of the self-consistency, this point is also
888"
REFERENCES,0.6903520208604954,"associated with a unique set of (ΛB
i ) coordinates. The cost function is continuous and differentiable
889"
REFERENCES,0.6910039113428944,"in both the Lij’s and the Λi’s coordinates. If we look at the negative gradient of the regularizer, it
890"
REFERENCES,0.6916558018252934,"is non-zero and therefore it must have at least one non-zero component ∂R/∂Λi along one of the
891"
REFERENCES,0.6923076923076923,"Λi coordinates. This implies that by scaling the corresponding unit i in the network, the regularizer
892"
REFERENCES,0.6929595827900913,"can be further reduced, and by balancing unit i the balancing algorithm will reach a new point (C in
893"
REFERENCES,0.6936114732724902,"Figure 2) with lower regularizer cost. This contradicts the assumption that B was associated with a
894"
REFERENCES,0.6942633637548892,"balanced stated. Thus, given an initial set of weights W, the stochastic balancing algorithm must
895"
REFERENCES,0.6949152542372882,"always converge to the same and unique optimal balanced state W ∗associated with the self-consistent
896"
REFERENCES,0.6955671447196871,"point A. A particular stochastic schedule corresponds to a random path within the linear manifold
897"
REFERENCES,0.6962190352020861,"from the origin (at time zero all the multipliers are equal to 1, and therefore Mij = 1 and Lij = 0)
898"
REFERENCES,0.696870925684485,"for any i and any j to the unique optimum point A.
899"
REFERENCES,0.6975228161668839,"Remark H.7. It should be clear from the proof that the same result holds also for any deterministic
900"
REFERENCES,0.6981747066492829,"balancing schedule, as well as for tied and non-tied subset balancing, e.g., for layer-wise balancing
901"
REFERENCES,0.6988265971316818,"and tied layer-wise balancing. In the Appendix, we provide an analytical solution for the case of tied
902"
REFERENCES,0.6994784876140808,"layer-wise balancing in a layered feed-forward network.
903"
REFERENCES,0.7001303780964798,"Remark H.8. It should be clear from the proof that the same convergence to the unique global
904"
REFERENCES,0.7007822685788787,"optimum is observed if each neuron, when stochastically visited, is favorably scaled rather than
905"
REFERENCES,0.7014341590612777,"balanced, i.e., it is scaled with a factor that reduces R but not necessarily minimizes R. Stochastic
906"
REFERENCES,0.7020860495436767,"balancing can also be viewed as a form of EM algorithm where the E and M steps can be taken fully
907"
REFERENCES,0.7027379400260756,"or partially.
908"
REFERENCES,0.7033898305084746,"I
Universal Approximation Properties of BiLU Neurons
909"
REFERENCES,0.7040417209908736,"Here we show that any continuous real-valued function defined over a compact set of the Euclidean
910"
REFERENCES,0.7046936114732725,"space can be approximated to any degree of precision by a network of BiLU neurons with a single
911"
REFERENCES,0.7053455019556715,"hidden layer. As in the case of the similar proof given in Baldi [2021] using linear threshold gates in
912"
REFERENCES,0.7059973924380704,"the hidden layer, it is enough to prove the theorem for a continuous function f: 0, 1 →R.
913"
REFERENCES,0.7066492829204694,"Theorem I.1. (Universal Approximation Properties of BiLU Neurons) Let f be any continuous
914"
REFERENCES,0.7073011734028684,"function from [0, 1] to R and ϵ > 0. Let gλ be the ReLU activation function with slope λ ∈Rs. Then
915"
REFERENCES,0.7079530638852672,"there exists a feedforward network with a single hidden layer of neurons with ReLU activations of the
916"
REFERENCES,0.7086049543676662,"form gλ and a single output linear neuron, i.e., with BiLU activation equal to the identity function,
917"
REFERENCES,0.7092568448500652,"capable of approximating f everywhere within ϵ (sup norm).
918"
REFERENCES,0.7099087353324641,"Proof. To be clear, gλ(x) = 0 for x < 0 and gλ(x) = λx for 0 ≤x. Since f is continuous over a
919"
REFERENCES,0.7105606258148631,"compact set, it is uniformly continuous. Thus there exists α > 0 such that for any x1 and x2 in the
920"
REFERENCES,0.711212516297262,"[0, 1] interval:
921"
REFERENCES,0.711864406779661,"|x2 −x1| < α =⇒|f(x2) −f(x1)| < ϵ
(I.1)"
REFERENCES,0.71251629726206,"Let N be an integer such that 1 < Nα, and let us slice the interval [0, 1] into N consecutive slices
922"
REFERENCES,0.7131681877444589,"of width h = 1/N, so that within each slice the function f cannot jump by more than ϵ. Let us
923"
REFERENCES,0.7138200782268579,"connect the input unit to all the hidden units with a weight equal to 1. Let us have N hidden units
924"
REFERENCES,0.7144719687092569,"numbered 1, . . . , N with biases equal to 0, 1/N, 2/N, ...., N1/N respectively and activation function
925"
REFERENCES,0.7151238591916558,"of the form gλk. It is essential that different units be allowed to have different slopes λk. The input
926"
REFERENCES,0.7157757496740548,"unit is connected to all the hidden units and all the weights on these connections are equal to 1. Thus
927"
REFERENCES,0.7164276401564538,"when x is in the k-th slice, (k −1)/N ≤x < k/N, all the units from k + 1 to N have an output
928"
REFERENCES,0.7170795306388527,"equal to 0, and all the units from 1 to k have an output determined by the corresponding slopes. All
929"
REFERENCES,0.7177314211212517,"the hidden units are connected to the output unit with weights β1, . . . , βN, and β0 is the bias of the
930"
REFERENCES,0.7183833116036505,"output unit. We want the output unit to be linear. In order for the ϵ approximation to be satisfied,
931"
REFERENCES,0.7190352020860495,"it is sufficient if in the (k −1)/N ≤x < k/N interval, the output is equal to the line joining the
932"
REFERENCES,0.7196870925684485,"point f((k −1)/N) to the point f(k/N). In other words, if x ∈[(k −1)/N, k/N), then we want
933"
REFERENCES,0.7203389830508474,"the output of the network to be:
934 β0 + k
X"
REFERENCES,0.7209908735332464,"i=1
βiλi(x −(i −1)h) = f(k −1"
REFERENCES,0.7216427640156454,"N
) + f( k"
REFERENCES,0.7222946544980443,N ) −f( k−1
REFERENCES,0.7229465449804433,"N )
h
(x −(k −1)h)
(I.2)"
REFERENCES,0.7235984354628422,"By equating the y-intercept and slope of the lines on the left-hand side and the righ- hand side of
935"
REFERENCES,0.7242503259452412,"Equation I.2, we can solve for the weights β’s and the slopes λ’s.
936"
REFERENCES,0.7249022164276402,"As in the case of the similar proof using linear threshold functions in the hidden layer (see Baldi
937"
REFERENCES,0.7255541069100391,"[2021],) this proof can easily be adapted to continuous functions defined over a compact set of Rn,
938"
REFERENCES,0.7262059973924381,"even with a finite number of finite discontinuities, and into Rm.
939"
REFERENCES,0.7268578878748371,"J
Analytical Solution for the Unique Global Balanced State
940"
REFERENCES,0.727509778357236,"Here we directly prove the convergence of stochastic balancing to a unique final balanced state, and
941"
REFERENCES,0.728161668839635,"derive the equations for the balanced state, in the special case of tied layer balancing (as opposed to
942"
REFERENCES,0.7288135593220338,"single neuron balancing). The Proof and the resulting equations are also valid for stochastic balancing
943"
REFERENCES,0.7294654498044328,"(one neuron at a time) in a layered architecture comprising a single neuron per layer. Let us call tied
944"
REFERENCES,0.7301173402868318,"layer scaling the operation by which all the incoming weights to a given layer of BiLU neurons are
945"
REFERENCES,0.7307692307692307,"multiplied by λ > 0 and all the outgoing weights of the layer are multiplied by 1/λ, again leaving the
946"
REFERENCES,0.7314211212516297,"training error unchanged. Let us call layer balancing the particular scaling operation corresponding
947"
REFERENCES,0.7320730117340287,"to the value of λ that minimizes the contribution of the layer to the L2 (or any other Lp) regularizer
948"
REFERENCES,0.7327249022164276,"value. This optimal value of λ∗results in layer-wise balance equations: the sum of the squares of all
949"
REFERENCES,0.7333767926988266,"the incoming weights of the layer must be equal to the sum of the squares of all the outgoing weights
950"
REFERENCES,0.7340286831812256,"of the layer in the L2 case, and similarly in all LP cases.
951"
REFERENCES,0.7346805736636245,"Theorem J.1. Assume that tied layer balancing is applied iteratively and stochastically to the layers
952"
REFERENCES,0.7353324641460235,"of a layered feedforward network of BiLU neurons. As long as all the layers are visited periodically,
953"
REFERENCES,0.7359843546284224,"this procedure will always converge to the same unique set of weights, which will satisfy the layer-
954"
REFERENCES,0.7366362451108214,"balance equations at all layers, irrespective of the details of the schedule. Furthermore, the balance
955"
REFERENCES,0.7372881355932204,"state can be solved analytically.
956"
REFERENCES,0.7379400260756193,"Proof. Every time a layer balancing operation is applied, the training error remains the same, and the
957"
REFERENCES,0.7385919165580183,"L2 (or any other Lp) regularization error decreases or stays the same. Since the regularization error
958"
REFERENCES,0.7392438070404173,"is always positive, it must converge to a certain value. Using the same arguments as in the proof of
959"
REFERENCES,0.7398956975228161,"Theorem H.1, the weights must also converge to a stable configuration, and since the configuration
960"
REFERENCES,0.7405475880052151,"is stable all its layers must satisfy the layer-wise balance equation. The key remaining question is
961"
REFERENCES,0.741199478487614,"why is this configuration unique and can we solve it analytically? Let A1, A2, . . . AN denote the
962"
REFERENCES,0.741851368970013,"matrices of connections between the layers of the network. Let Λ1, Λ2, . . . , ΛN−1 be N −1 strictly
963"
REFERENCES,0.742503259452412,"positive multipliers, representing the limits of the products of the corresponding λ∗
i associated with
964"
REFERENCES,0.7431551499348109,"each balancing step at layer i, as in the proof of Theorem H.1. In this notation, layer 0 is the input
965"
REFERENCES,0.7438070404172099,"layer and layer N is the output layer (with Λ0 = 1 and ΛN = 1).
966"
REFERENCES,0.7444589308996089,"After converging, each matrix Ai becomes the matrix Λi/Λi−1Ai = MiAi for i = 1 . . . N, with
967"
REFERENCES,0.7451108213820078,"Mi = λi/Λi−1. The multipliers Mi must minimize the regularizer while satisfying M1 . . . MN = 1
968"
REFERENCES,0.7457627118644068,"to ensure that the training error remains unchanged. In other words, to find the values of the Mi’s we
969"
REFERENCES,0.7464146023468058,"must minimize the Lagrangian:
970"
REFERENCES,0.7470664928292047,"L(M1, . . . , MN) = N
X"
REFERENCES,0.7477183833116037,"i=1
||MiAi||2 + µ(1 − N
Y"
REFERENCES,0.7483702737940026,"i=1
Mi)
(J.1)"
REFERENCES,0.7490221642764016,"written for the L2 case in terms of the Frobenius norm, but the analysis is similar in the general Lp
971"
REFERENCES,0.7496740547588006,"case. From this, we get the critical equations:
972"
REFERENCES,0.7503259452411994,"∂L
∂Mi
= 2Mi||Ai||2 −µM1 . . . Mi−1Mi+1 . . . MN = 0
for i = 1, . . . , N
and N
Y"
REFERENCES,0.7509778357235984,"i=1
Mi = 1"
REFERENCES,0.7516297262059974,"(J.2)
As a resut, for every i:
973"
REFERENCES,0.7522816166883963,2Mi||Ai||2 −µ
REFERENCES,0.7529335071707953,"Mi
= 0
or
µ = 2M 2
i ||Ai||2
(J.3)"
REFERENCES,0.7535853976531942,"Thus each Mi > 0 can be expressed in a unique way as a function of the Lagrangian multiplier µ as:
974"
REFERENCES,0.7542372881355932,"Mi = (µ/2||Ai||2)1/2. By writing again that the product of the Miis equal to 1, we finally get:
975"
REFERENCES,0.7548891786179922,"µN = 2N
N
Y"
REFERENCES,0.7555410691003911,"i=1
||Ai||2
or
µ = 2 N
Y"
REFERENCES,0.7561929595827901,"i=1
||Ai||2/N
(J.4)"
REFERENCES,0.7568448500651891,"Thus we can solve for Mi:
976"
REFERENCES,0.757496740547588,"Mi =
µ
2||Ai||2 =
QN
i=1 ||Ai||2/N"
REFERENCES,0.758148631029987,"||Ai||2
for i = 1, . . . , N
(J.5)"
REFERENCES,0.758800521512386,"Thus, in short, we obtain a unique closed-form expression for each Mi. From there, we infer the
977"
REFERENCES,0.7594524119947849,"unique and final state of the weights, where A∗
i = MiAi = ΛiAl/Λl−1. Note that each Mi depends
978"
REFERENCES,0.7601043024771839,"on all the other Mj’s, again showcasing how the local balancing algorithm leads to a unique global
979"
REFERENCES,0.7607561929595827,"solution.
980"
REFERENCES,0.7614080834419817,"K
Computer Resources
981"
REFERENCES,0.7620599739243807,"The simulations we have described do not require major computing resources. They were all
982"
REFERENCES,0.7627118644067796,"performed using Google Colab and the NVIDIA TESLA T4 GPU that it provides.
983"
REFERENCES,0.7633637548891786,"L
Code Availability
984"
REFERENCES,0.7640156453715776,"The code for reproducing the simulation results is available under the Apache 2.0 license at:
985"
REFERENCES,0.7646675358539765,"https://anonymous.4open.science/r/a-theory-of-neural-synaptic-balance-00C1
986"
REFERENCES,0.7653194263363755,"References
987"
REFERENCES,0.7659713168187744,"P. Baldi. Deep Learning in Science. Cambridge University Press, Cambridge, UK, 2021.
988"
REFERENCES,0.7666232073011734,"Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
989"
REFERENCES,0.7672750977835724,"Processing Magazine, 29(6):141–142, 2012.
990"
REFERENCES,0.7679269882659713,"Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
991"
REFERENCES,0.7685788787483703,"models: Layers are automatically balanced. Advances in Neural Information Processing Systems,
992"
REFERENCES,0.7692307692307693,"31, 2018.
993"
REFERENCES,0.7698826597131682,"Rachel E Field, James A D’amour, Robin Tremblay, Christoph Miehl, Bernardo Rudy, Julijana
994"
REFERENCES,0.7705345501955672,"Gjorgjieva, and Robert C Froemke. Heterosynaptic plasticity determines the set point for cortical
995"
REFERENCES,0.7711864406779662,"excitatory-inhibitory balance. Neuron, 106(5):842–854, 2020.
996"
REFERENCES,0.771838331160365,"Robert C Froemke. Plasticity of cortical excitatory-inhibitory balance. Annual review of neuroscience,
997"
REFERENCES,0.772490221642764,"38:195–219, 2015.
998"
REFERENCES,0.7731421121251629,"Oliver D Howes and Ekaterina Shatalina. Integrating the neurodevelopmental and dopamine hypothe-
999"
REFERENCES,0.7737940026075619,"ses of schizophrenia and the role of cortical excitation-inhibition balance. Biological psychiatry,
1000"
REFERENCES,0.7744458930899609,"2022.
1001"
REFERENCES,0.7750977835723598,"Dmitry Ivanov, Aleksandr Chezhegov, Mikhail Kiselev, Andrey Grunin, and Denis Larionov. Neuro-
1002"
REFERENCES,0.7757496740547588,"morphic artificial intelligence systems. Frontiers in Neuroscience, 16:1513, 2022.
1003"
REFERENCES,0.7764015645371578,"Yu Ji, YouHui Zhang, ShuangChen Li, Ping Chi, CiHang Jiang, Peng Qu, Yuan Xie, and WenGuang
1004"
REFERENCES,0.7770534550195567,"Chen. Neutrams: Neural network transformation and co-design under neuromorphic hardware
1005"
REFERENCES,0.7777053455019557,"constraints. In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture
1006"
REFERENCES,0.7783572359843546,"(MICRO), pages 1–13. IEEE, 2016.
1007"
REFERENCES,0.7790091264667536,"Dongshin Kim and Jang-Sik Lee. Neurotransmitter-induced excitatory and inhibitory functions in
1008"
REFERENCES,0.7796610169491526,"artificial synapses. Advanced Functional Materials, 32(21):2200497, 2022.
1009"
REFERENCES,0.7803129074315515,"Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
1010"
REFERENCES,0.7809647979139505,"Faming Liang and Wing Hung Wong. Evolutionary monte carlo: Applications to cp model sampling
1011"
REFERENCES,0.7816166883963495,"and change point problem. STATISTICA SINICA, 10:317–342, 2000.
1012"
REFERENCES,0.7822685788787483,"Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Data-dependent path
1013"
REFERENCES,0.7829204693611473,"normalization in neural networks. arXiv preprint arXiv:1511.06747, 2015.
1014"
REFERENCES,0.7835723598435462,"Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conver-
1015"
REFERENCES,0.7842242503259452,"sion of continuous-valued deep networks to efficient event-driven networks for image classification.
1016"
REFERENCES,0.7848761408083442,"Frontiers in neuroscience, 11:294078, 2017.
1017"
REFERENCES,0.7855280312907431,"Farshad Shirani and Hannah Choi. On the physiological and structural contributors to the dynamic
1018"
REFERENCES,0.7861799217731421,"balance of excitation and inhibition in local cortical networks. bioRxiv, pages 2023–01, 2023.
1019"
REFERENCES,0.7868318122555411,"Martino Sorbaro, Qian Liu, Massimo Bortone, and Sadique Sheik. Optimizing the energy consump-
1020"
REFERENCES,0.78748370273794,"tion of spiking neural networks for neuromorphic applications. Frontiers in neuroscience, 14:662,
1021"
REFERENCES,0.788135593220339,"2020.
1022"
REFERENCES,0.788787483702738,"Christopher H Stock, Sarah E Harvey, Samuel A Ocko, and Surya Ganguli. Synaptic balancing: A
1023"
REFERENCES,0.7894393741851369,"biologically plausible local learning rule that provably increases neural network noise robustness
1024"
REFERENCES,0.7900912646675359,"without sacrificing task performance. PLOS Computational Biology, 18(9):e1010418, 2022.
1025"
REFERENCES,0.7907431551499348,"A. Tavakoli, F. Agostinelli, and P. Baldi. SPLASH: Learnable activation functions for improving
1026"
REFERENCES,0.7913950456323338,"accuracy and adversarial robustness. Neural Networks, 140:1–12, 2021. Also: arXiv:2006.08947.
1027"
REFERENCES,0.7920469361147328,"NeurIPS Paper Checklist
1028"
CLAIMS,0.7926988265971316,"1. Claims
1029"
CLAIMS,0.7933507170795306,"Question: Do the main claims made in the abstract and introduction accurately reflect the
1030"
CLAIMS,0.7940026075619296,"paper’s contributions and scope?
1031"
CLAIMS,0.7946544980443285,"Answer: [Yes]
1032"
CLAIMS,0.7953063885267275,"Justification: We have included all the main points of the paper in the abstract.
1033"
CLAIMS,0.7959582790091264,"Guidelines:
1034"
CLAIMS,0.7966101694915254,"• The answer NA means that the abstract and introduction do not include the claims
1035"
CLAIMS,0.7972620599739244,"made in the paper.
1036"
CLAIMS,0.7979139504563233,"• The abstract and/or introduction should clearly state the claims made, including the
1037"
CLAIMS,0.7985658409387223,"contributions made in the paper and important assumptions and limitations. A No or
1038"
CLAIMS,0.7992177314211213,"NA answer to this question will not be perceived well by the reviewers.
1039"
CLAIMS,0.7998696219035202,"• The claims made should match theoretical and experimental results, and reflect how
1040"
CLAIMS,0.8005215123859192,"much the results can be expected to generalize to other settings.
1041"
CLAIMS,0.8011734028683182,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
1042"
CLAIMS,0.8018252933507171,"are not attained by the paper.
1043"
LIMITATIONS,0.8024771838331161,"2. Limitations
1044"
LIMITATIONS,0.803129074315515,"Question: Does the paper discuss the limitations of the work performed by the authors?
1045"
LIMITATIONS,0.8037809647979139,"Answer: [Yes]
1046"
LIMITATIONS,0.8044328552803129,"Justification: The majority of our results are theorems backed up by mathematical proofs.
1047"
LIMITATIONS,0.8050847457627118,"We discuss at lenght that balancing improves the value of the regularizer only (it leaves the
1048"
LIMITATIONS,0.8057366362451108,"valuue of the data-dependent component of the error unchanged). We also mention that
1049"
LIMITATIONS,0.8063885267275098,"while it would be interesting to study any kind of balance in biological neural networks,
1050"
LIMITATIONS,0.8070404172099087,"current technnological limirations do not allow recording all the incoming and outgoing
1051"
LIMITATIONS,0.8076923076923077,"synaptic strengths of a neuron.
1052"
LIMITATIONS,0.8083441981747066,"Guidelines:
1053"
LIMITATIONS,0.8089960886571056,"• The answer NA means that the paper has no limitation while the answer No means that
1054"
LIMITATIONS,0.8096479791395046,"the paper has limitations, but those are not discussed in the paper.
1055"
LIMITATIONS,0.8102998696219035,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
1056"
LIMITATIONS,0.8109517601043025,"• The paper should point out any strong assumptions and how robust the results are to
1057"
LIMITATIONS,0.8116036505867015,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
1058"
LIMITATIONS,0.8122555410691004,"model well-specification, asymptotic approximations only holding locally). The authors
1059"
LIMITATIONS,0.8129074315514994,"should reflect on how these assumptions might be violated in practice and what the
1060"
LIMITATIONS,0.8135593220338984,"implications would be.
1061"
LIMITATIONS,0.8142112125162972,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
1062"
LIMITATIONS,0.8148631029986962,"only tested on a few datasets or with a few runs. In general, empirical results often
1063"
LIMITATIONS,0.8155149934810951,"depend on implicit assumptions, which should be articulated.
1064"
LIMITATIONS,0.8161668839634941,"• The authors should reflect on the factors that influence the performance of the approach.
1065"
LIMITATIONS,0.8168187744458931,"For example, a facial recognition algorithm may perform poorly when image resolution
1066"
LIMITATIONS,0.817470664928292,"is low or images are taken in low lighting. Or a speech-to-text system might not be
1067"
LIMITATIONS,0.818122555410691,"used reliably to provide closed captions for online lectures because it fails to handle
1068"
LIMITATIONS,0.81877444589309,"technical jargon.
1069"
LIMITATIONS,0.8194263363754889,"• The authors should discuss the computational efficiency of the proposed algorithms
1070"
LIMITATIONS,0.8200782268578879,"and how they scale with dataset size.
1071"
LIMITATIONS,0.8207301173402868,"• If applicable, the authors should discuss possible limitations of their approach to
1072"
LIMITATIONS,0.8213820078226858,"address problems of privacy and fairness.
1073"
LIMITATIONS,0.8220338983050848,"• While the authors might fear that complete honesty about limitations might be used by
1074"
LIMITATIONS,0.8226857887874837,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
1075"
LIMITATIONS,0.8233376792698827,"limitations that aren’t acknowledged in the paper. The authors should use their best
1076"
LIMITATIONS,0.8239895697522817,"judgment and recognize that individual actions in favor of transparency play an impor-
1077"
LIMITATIONS,0.8246414602346805,"tant role in developing norms that preserve the integrity of the community. Reviewers
1078"
LIMITATIONS,0.8252933507170795,"will be specifically instructed to not penalize honesty concerning limitations.
1079"
THEORY ASSUMPTIONS AND PROOFS,0.8259452411994785,"3. Theory Assumptions and Proofs
1080"
THEORY ASSUMPTIONS AND PROOFS,0.8265971316818774,"Question: For each theoretical result, does the paper provide the full set of assumptions and
1081"
THEORY ASSUMPTIONS AND PROOFS,0.8272490221642764,"a complete (and correct) proof?
1082"
THEORY ASSUMPTIONS AND PROOFS,0.8279009126466753,"Answer: [Yes]
1083"
THEORY ASSUMPTIONS AND PROOFS,0.8285528031290743,"Justification: All the theorems and propositions have clear assumptions and all the proofs
1084"
THEORY ASSUMPTIONS AND PROOFS,0.8292046936114733,"are complete and have been checked carefully multiple times. Details of some of the proofs
1085"
THEORY ASSUMPTIONS AND PROOFS,0.8298565840938722,"are provided in the Appendix.
1086"
THEORY ASSUMPTIONS AND PROOFS,0.8305084745762712,"Guidelines:
1087"
THEORY ASSUMPTIONS AND PROOFS,0.8311603650586702,"• The answer NA means that the paper does not include theoretical results.
1088"
THEORY ASSUMPTIONS AND PROOFS,0.8318122555410691,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
1089"
THEORY ASSUMPTIONS AND PROOFS,0.8324641460234681,"referenced.
1090"
THEORY ASSUMPTIONS AND PROOFS,0.833116036505867,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
1091"
THEORY ASSUMPTIONS AND PROOFS,0.833767926988266,"• The proofs can either appear in the main paper or the supplemental material, but if
1092"
THEORY ASSUMPTIONS AND PROOFS,0.834419817470665,"they appear in the supplemental material, the authors are encouraged to provide a short
1093"
THEORY ASSUMPTIONS AND PROOFS,0.8350717079530638,"proof sketch to provide intuition.
1094"
THEORY ASSUMPTIONS AND PROOFS,0.8357235984354628,"• Inversely, any informal proof provided in the core of the paper should be complemented
1095"
THEORY ASSUMPTIONS AND PROOFS,0.8363754889178618,"by formal proofs provided in appendix or supplemental material.
1096"
THEORY ASSUMPTIONS AND PROOFS,0.8370273794002607,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
1097"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8376792698826597,"4. Experimental Result Reproducibility
1098"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8383311603650587,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
1099"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8389830508474576,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
1100"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8396349413298566,"of the paper (regardless of whether the code and data are provided or not)?
1101"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8402868318122555,"Answer: [Yes]
1102"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8409387222946545,"Justification: We have provided all the explanations necessary for reproducing the exper-
1103"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8415906127770535,"imental results in the technical appendix and also provided the code for reproducing our
1104"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8422425032594524,"experimental results.
1105"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8428943937418514,"Guidelines:
1106"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8435462842242504,"• The answer NA means that the paper does not include experiments.
1107"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8441981747066493,"• If the paper includes experiments, a No answer to this question will not be perceived
1108"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8448500651890483,"well by the reviewers: Making the paper reproducible is important, regardless of
1109"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8455019556714471,"whether the code and data are provided or not.
1110"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8461538461538461,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
1111"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8468057366362451,"to make their results reproducible or verifiable.
1112"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.847457627118644,"• Depending on the contribution, reproducibility can be accomplished in various ways.
1113"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.848109517601043,"For example, if the contribution is a novel architecture, describing the architecture fully
1114"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.848761408083442,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
1115"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8494132985658409,"be necessary to either make it possible for others to replicate the model with the same
1116"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8500651890482399,"dataset, or provide access to the model. In general. releasing code and data is often
1117"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8507170795306388,"one good way to accomplish this, but reproducibility can also be provided via detailed
1118"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8513689700130378,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
1119"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8520208604954368,"of a large language model), releasing of a model checkpoint, or other means that are
1120"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8526727509778357,"appropriate to the research performed.
1121"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8533246414602347,"• While NeurIPS does not require releasing code, the conference does require all submis-
1122"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8539765319426337,"sions to provide some reasonable avenue for reproducibility, which may depend on the
1123"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8546284224250326,"nature of the contribution. For example
1124"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8552803129074316,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
1125"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8559322033898306,"to reproduce that algorithm.
1126"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8565840938722294,"(b) If the contribution is primarily a new model architecture, the paper should describe
1127"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8572359843546284,"the architecture clearly and fully.
1128"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8578878748370273,"(c) If the contribution is a new model (e.g., a large language model), then there should
1129"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8585397653194263,"either be a way to access this model for reproducing the results or a way to reproduce
1130"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8591916558018253,"the model (e.g., with an open-source dataset or instructions for how to construct
1131"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8598435462842242,"the dataset).
1132"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8604954367666232,"(d) We recognize that reproducibility may be tricky in some cases, in which case
1133"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8611473272490222,"authors are welcome to describe the particular way they provide for reproducibility.
1134"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8617992177314211,"In the case of closed-source models, it may be that access to the model is limited in
1135"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8624511082138201,"some way (e.g., to registered users), but it should be possible for other researchers
1136"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.863102998696219,"to have some path to reproducing or verifying the results.
1137"
OPEN ACCESS TO DATA AND CODE,0.863754889178618,"5. Open access to data and code
1138"
OPEN ACCESS TO DATA AND CODE,0.864406779661017,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
1139"
OPEN ACCESS TO DATA AND CODE,0.8650586701434159,"tions to faithfully reproduce the main experimental results, as described in supplemental
1140"
OPEN ACCESS TO DATA AND CODE,0.8657105606258149,"material?
1141"
OPEN ACCESS TO DATA AND CODE,0.8663624511082139,"Answer: [Yes]
1142"
OPEN ACCESS TO DATA AND CODE,0.8670143415906127,"Justification: We have provided an anonymous link to our code which is available in the
1143"
OPEN ACCESS TO DATA AND CODE,0.8676662320730117,"appendix and also uploaded our code as supplementary material.
1144"
OPEN ACCESS TO DATA AND CODE,0.8683181225554107,"Guidelines:
1145"
OPEN ACCESS TO DATA AND CODE,0.8689700130378096,"• The answer NA means that paper does not include experiments requiring code.
1146"
OPEN ACCESS TO DATA AND CODE,0.8696219035202086,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
1147"
OPEN ACCESS TO DATA AND CODE,0.8702737940026075,"public/guides/CodeSubmissionPolicy) for more details.
1148"
OPEN ACCESS TO DATA AND CODE,0.8709256844850065,"• While we encourage the release of code and data, we understand that this might not be
1149"
OPEN ACCESS TO DATA AND CODE,0.8715775749674055,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
1150"
OPEN ACCESS TO DATA AND CODE,0.8722294654498044,"including code, unless this is central to the contribution (e.g., for a new open-source
1151"
OPEN ACCESS TO DATA AND CODE,0.8728813559322034,"benchmark).
1152"
OPEN ACCESS TO DATA AND CODE,0.8735332464146024,"• The instructions should contain the exact command and environment needed to run to
1153"
OPEN ACCESS TO DATA AND CODE,0.8741851368970013,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
1154"
OPEN ACCESS TO DATA AND CODE,0.8748370273794003,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
1155"
OPEN ACCESS TO DATA AND CODE,0.8754889178617992,"• The authors should provide instructions on data access and preparation, including how
1156"
OPEN ACCESS TO DATA AND CODE,0.8761408083441982,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
1157"
OPEN ACCESS TO DATA AND CODE,0.8767926988265972,"• The authors should provide scripts to reproduce all experimental results for the new
1158"
OPEN ACCESS TO DATA AND CODE,0.877444589308996,"proposed method and baselines. If only a subset of experiments are reproducible, they
1159"
OPEN ACCESS TO DATA AND CODE,0.878096479791395,"should state which ones are omitted from the script and why.
1160"
OPEN ACCESS TO DATA AND CODE,0.878748370273794,"• At submission time, to preserve anonymity, the authors should release anonymized
1161"
OPEN ACCESS TO DATA AND CODE,0.8794002607561929,"versions (if applicable).
1162"
OPEN ACCESS TO DATA AND CODE,0.8800521512385919,"• Providing as much information as possible in supplemental material (appended to the
1163"
OPEN ACCESS TO DATA AND CODE,0.8807040417209909,"paper) is recommended, but including URLs to data and code is permitted.
1164"
OPEN ACCESS TO DATA AND CODE,0.8813559322033898,"6. Experimental Setting/Details
1165"
OPEN ACCESS TO DATA AND CODE,0.8820078226857888,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
1166"
OPEN ACCESS TO DATA AND CODE,0.8826597131681877,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
1167"
OPEN ACCESS TO DATA AND CODE,0.8833116036505867,"results?
1168"
OPEN ACCESS TO DATA AND CODE,0.8839634941329857,"Answer: [Yes]
1169"
OPEN ACCESS TO DATA AND CODE,0.8846153846153846,"Justification: We have provided the required details in the appendix.
1170"
OPEN ACCESS TO DATA AND CODE,0.8852672750977836,"Guidelines:
1171"
OPEN ACCESS TO DATA AND CODE,0.8859191655801826,"• The answer NA means that the paper does not include experiments.
1172"
OPEN ACCESS TO DATA AND CODE,0.8865710560625815,"• The experimental setting should be presented in the core of the paper to a level of detail
1173"
OPEN ACCESS TO DATA AND CODE,0.8872229465449805,"that is necessary to appreciate the results and make sense of them.
1174"
OPEN ACCESS TO DATA AND CODE,0.8878748370273793,"• The full details can be provided either with the code, in appendix, or as supplemental
1175"
OPEN ACCESS TO DATA AND CODE,0.8885267275097783,"material.
1176"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8891786179921773,"7. Experiment Statistical Significance
1177"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8898305084745762,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
1178"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8904823989569752,"information about the statistical significance of the experiments?
1179"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8911342894393742,"Answer: [Yes]
1180"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8917861799217731,"Justification: Error bars are included in all images.
1181"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8924380704041721,"Guidelines:
1182"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8930899608865711,"• The answer NA means that the paper does not include experiments.
1183"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.89374185136897,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
1184"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.894393741851369,"dence intervals, or statistical significance tests, at least for the experiments that support
1185"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8950456323337679,"the main claims of the paper.
1186"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8956975228161669,"• The factors of variability that the error bars are capturing should be clearly stated (for
1187"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8963494132985659,"example, train/test split, initialization, random drawing of some parameter, or overall
1188"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8970013037809648,"run with given experimental conditions).
1189"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8976531942633638,"• The method for calculating the error bars should be explained (closed form formula,
1190"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8983050847457628,"call to a library function, bootstrap, etc.)
1191"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8989569752281616,"• The assumptions made should be given (e.g., Normally distributed errors).
1192"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8996088657105606,"• It should be clear whether the error bar is the standard deviation or the standard error
1193"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9002607561929595,"of the mean.
1194"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9009126466753585,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
1195"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9015645371577575,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
1196"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9022164276401564,"of Normality of errors is not verified.
1197"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9028683181225554,"• For asymmetric distributions, the authors should be careful not to show in tables or
1198"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9035202086049544,"figures symmetric error bars that would yield results that are out of range (e.g. negative
1199"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9041720990873533,"error rates).
1200"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9048239895697523,"• If error bars are reported in tables or plots, The authors should explain in the text how
1201"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9054758800521513,"they were calculated and reference the corresponding figures or tables in the text.
1202"
EXPERIMENTS COMPUTE RESOURCES,0.9061277705345502,"8. Experiments Compute Resources
1203"
EXPERIMENTS COMPUTE RESOURCES,0.9067796610169492,"Question: For each experiment, does the paper provide sufficient information on the com-
1204"
EXPERIMENTS COMPUTE RESOURCES,0.9074315514993481,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
1205"
EXPERIMENTS COMPUTE RESOURCES,0.9080834419817471,"the experiments?
1206"
EXPERIMENTS COMPUTE RESOURCES,0.9087353324641461,"Answer: [Yes]
1207"
EXPERIMENTS COMPUTE RESOURCES,0.909387222946545,"Justification: We have provided this information in the computer resources section in the
1208"
EXPERIMENTS COMPUTE RESOURCES,0.910039113428944,"appendix.
1209"
EXPERIMENTS COMPUTE RESOURCES,0.9106910039113429,"Guidelines:
1210"
EXPERIMENTS COMPUTE RESOURCES,0.9113428943937418,"• The answer NA means that the paper does not include experiments.
1211"
EXPERIMENTS COMPUTE RESOURCES,0.9119947848761408,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
1212"
EXPERIMENTS COMPUTE RESOURCES,0.9126466753585397,"or cloud provider, including relevant memory and storage.
1213"
EXPERIMENTS COMPUTE RESOURCES,0.9132985658409387,"• The paper should provide the amount of compute required for each of the individual
1214"
EXPERIMENTS COMPUTE RESOURCES,0.9139504563233377,"experimental runs as well as estimate the total compute.
1215"
EXPERIMENTS COMPUTE RESOURCES,0.9146023468057366,"• The paper should disclose whether the full research project required more compute
1216"
EXPERIMENTS COMPUTE RESOURCES,0.9152542372881356,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
1217"
EXPERIMENTS COMPUTE RESOURCES,0.9159061277705346,"didn’t make it into the paper).
1218"
CODE OF ETHICS,0.9165580182529335,"9. Code Of Ethics
1219"
CODE OF ETHICS,0.9172099087353325,"Question: Does the research conducted in the paper conform, in every respect, with the
1220"
CODE OF ETHICS,0.9178617992177314,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1221"
CODE OF ETHICS,0.9185136897001304,"Answer: [Yes]
1222"
CODE OF ETHICS,0.9191655801825294,"Justification: The research conducted in our paper conforms, in every respect, with the
1223"
CODE OF ETHICS,0.9198174706649282,"NeurIPS Code of Ethics.
1224"
CODE OF ETHICS,0.9204693611473272,"Guidelines:
1225"
CODE OF ETHICS,0.9211212516297262,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1226"
CODE OF ETHICS,0.9217731421121251,"• If the authors answer No, they should explain the special circumstances that require a
1227"
CODE OF ETHICS,0.9224250325945241,"deviation from the Code of Ethics.
1228"
CODE OF ETHICS,0.9230769230769231,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
1229"
CODE OF ETHICS,0.923728813559322,"eration due to laws or regulations in their jurisdiction).
1230"
BROADER IMPACTS,0.924380704041721,"10. Broader Impacts
1231"
BROADER IMPACTS,0.9250325945241199,"Question: Does the paper discuss both potential positive societal impacts and negative
1232"
BROADER IMPACTS,0.9256844850065189,"societal impacts of the work performed?
1233"
BROADER IMPACTS,0.9263363754889179,"Answer: [NA]
1234"
BROADER IMPACTS,0.9269882659713168,"Justification: Our paper has no conceivable direct societal impact.
1235"
BROADER IMPACTS,0.9276401564537158,"Guidelines:
1236"
BROADER IMPACTS,0.9282920469361148,"• The answer NA means that there is no societal impact of the work performed.
1237"
BROADER IMPACTS,0.9289439374185137,"• If the authors answer NA or No, they should explain why their work has no societal
1238"
BROADER IMPACTS,0.9295958279009127,"impact or why the paper does not address societal impact.
1239"
BROADER IMPACTS,0.9302477183833116,"• Examples of negative societal impacts include potential malicious or unintended uses
1240"
BROADER IMPACTS,0.9308996088657105,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1241"
BROADER IMPACTS,0.9315514993481095,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1242"
BROADER IMPACTS,0.9322033898305084,"groups), privacy considerations, and security considerations.
1243"
BROADER IMPACTS,0.9328552803129074,"• The conference expects that many papers will be foundational research and not tied
1244"
BROADER IMPACTS,0.9335071707953064,"to particular applications, let alone deployments. However, if there is a direct path to
1245"
BROADER IMPACTS,0.9341590612777053,"any negative applications, the authors should point it out. For example, it is legitimate
1246"
BROADER IMPACTS,0.9348109517601043,"to point out that an improvement in the quality of generative models could be used to
1247"
BROADER IMPACTS,0.9354628422425033,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1248"
BROADER IMPACTS,0.9361147327249022,"that a generic algorithm for optimizing neural networks could enable people to train
1249"
BROADER IMPACTS,0.9367666232073012,"models that generate Deepfakes faster.
1250"
BROADER IMPACTS,0.9374185136897001,"• The authors should consider possible harms that could arise when the technology is
1251"
BROADER IMPACTS,0.9380704041720991,"being used as intended and functioning correctly, harms that could arise when the
1252"
BROADER IMPACTS,0.9387222946544981,"technology is being used as intended but gives incorrect results, and harms following
1253"
BROADER IMPACTS,0.939374185136897,"from (intentional or unintentional) misuse of the technology.
1254"
BROADER IMPACTS,0.940026075619296,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1255"
BROADER IMPACTS,0.940677966101695,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1256"
BROADER IMPACTS,0.9413298565840938,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1257"
BROADER IMPACTS,0.9419817470664928,"feedback over time, improving the efficiency and accessibility of ML).
1258"
SAFEGUARDS,0.9426336375488917,"11. Safeguards
1259"
SAFEGUARDS,0.9432855280312907,"Question: Does the paper describe safeguards that have been put in place for responsible
1260"
SAFEGUARDS,0.9439374185136897,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1261"
SAFEGUARDS,0.9445893089960886,"image generators, or scraped datasets)?
1262"
SAFEGUARDS,0.9452411994784876,"Answer: [NA]
1263"
SAFEGUARDS,0.9458930899608866,"Justification: Our paper poses no such risks.
1264"
SAFEGUARDS,0.9465449804432855,"Guidelines:
1265"
SAFEGUARDS,0.9471968709256845,"• The answer NA means that the paper poses no such risks.
1266"
SAFEGUARDS,0.9478487614080835,"• Released models that have a high risk for misuse or dual-use should be released with
1267"
SAFEGUARDS,0.9485006518904824,"necessary safeguards to allow for controlled use of the model, for example by requiring
1268"
SAFEGUARDS,0.9491525423728814,"that users adhere to usage guidelines or restrictions to access the model or implementing
1269"
SAFEGUARDS,0.9498044328552803,"safety filters.
1270"
SAFEGUARDS,0.9504563233376793,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1271"
SAFEGUARDS,0.9511082138200783,"should describe how they avoided releasing unsafe images.
1272"
SAFEGUARDS,0.9517601043024772,"• We recognize that providing effective safeguards is challenging, and many papers do
1273"
SAFEGUARDS,0.9524119947848761,"not require this, but we encourage authors to take this into account and make a best
1274"
SAFEGUARDS,0.9530638852672751,"faith effort.
1275"
LICENSES FOR EXISTING ASSETS,0.953715775749674,"12. Licenses for existing assets
1276"
LICENSES FOR EXISTING ASSETS,0.954367666232073,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1277"
LICENSES FOR EXISTING ASSETS,0.9550195567144719,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1278"
LICENSES FOR EXISTING ASSETS,0.9556714471968709,"properly respected?
1279"
LICENSES FOR EXISTING ASSETS,0.9563233376792699,"Answer: [Yes]
1280"
LICENSES FOR EXISTING ASSETS,0.9569752281616688,"Justification: The only assets that we have use are the MNIST and CIFAR-10 datasets and
1281"
LICENSES FOR EXISTING ASSETS,0.9576271186440678,"we have cited these datasets in the paper properly.
1282"
LICENSES FOR EXISTING ASSETS,0.9582790091264668,"Guidelines:
1283"
LICENSES FOR EXISTING ASSETS,0.9589308996088657,"• The answer NA means that the paper does not use existing assets.
1284"
LICENSES FOR EXISTING ASSETS,0.9595827900912647,"• The authors should cite the original paper that produced the code package or dataset.
1285"
LICENSES FOR EXISTING ASSETS,0.9602346805736637,"• The authors should state which version of the asset is used and, if possible, include a
1286"
LICENSES FOR EXISTING ASSETS,0.9608865710560626,"URL.
1287"
LICENSES FOR EXISTING ASSETS,0.9615384615384616,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1288"
LICENSES FOR EXISTING ASSETS,0.9621903520208605,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1289"
LICENSES FOR EXISTING ASSETS,0.9628422425032594,"service of that source should be provided.
1290"
LICENSES FOR EXISTING ASSETS,0.9634941329856584,"• If assets are released, the license, copyright information, and terms of use in the
1291"
LICENSES FOR EXISTING ASSETS,0.9641460234680573,"package should be provided. For popular datasets, paperswithcode.com/datasets
1292"
LICENSES FOR EXISTING ASSETS,0.9647979139504563,"has curated licenses for some datasets. Their licensing guide can help determine the
1293"
LICENSES FOR EXISTING ASSETS,0.9654498044328553,"license of a dataset.
1294"
LICENSES FOR EXISTING ASSETS,0.9661016949152542,"• For existing datasets that are re-packaged, both the original license and the license of
1295"
LICENSES FOR EXISTING ASSETS,0.9667535853976532,"the derived asset (if it has changed) should be provided.
1296"
LICENSES FOR EXISTING ASSETS,0.9674054758800521,"• If this information is not available online, the authors are encouraged to reach out to
1297"
LICENSES FOR EXISTING ASSETS,0.9680573663624511,"the asset’s creators.
1298"
NEW ASSETS,0.9687092568448501,"13. New Assets
1299"
NEW ASSETS,0.969361147327249,"Question: Are new assets introduced in the paper well documented and is the documentation
1300"
NEW ASSETS,0.970013037809648,"provided alongside the assets?
1301"
NEW ASSETS,0.970664928292047,"Answer: [NA]
1302"
NEW ASSETS,0.9713168187744459,"Justification: Our paper does not introduce new assets.
1303"
NEW ASSETS,0.9719687092568449,"Guidelines:
1304"
NEW ASSETS,0.9726205997392438,"• The answer NA means that the paper does not release new assets.
1305"
NEW ASSETS,0.9732724902216427,"• Researchers should communicate the details of the dataset/code/model as part of their
1306"
NEW ASSETS,0.9739243807040417,"submissions via structured templates. This includes details about training, license,
1307"
NEW ASSETS,0.9745762711864406,"limitations, etc.
1308"
NEW ASSETS,0.9752281616688396,"• The paper should discuss whether and how consent was obtained from people whose
1309"
NEW ASSETS,0.9758800521512386,"asset is used.
1310"
NEW ASSETS,0.9765319426336375,"• At submission time, remember to anonymize your assets (if applicable). You can either
1311"
NEW ASSETS,0.9771838331160365,"create an anonymized URL or include an anonymized zip file.
1312"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9778357235984355,"14. Crowdsourcing and Research with Human Subjects
1313"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9784876140808344,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1314"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791395045632334,"include the full text of instructions given to participants and screenshots, if applicable, as
1315"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9797913950456323,"well as details about compensation (if any)?
1316"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804432855280313,"Answer: [NA]
1317"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9810951760104303,"Justification: Our research does not involve human subjects or crowdsourcing.
1318"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817470664928292,"Guidelines:
1319"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823989569752282,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1320"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830508474576272,"human subjects.
1321"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983702737940026,"• Including this information in the supplemental material is fine, but if the main contribu-
1322"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984354628422425,"tion of the paper involves human subjects, then as much detail as possible should be
1323"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850065189048239,"included in the main paper.
1324"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856584093872229,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1325"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9863102998696219,"or other labor should be paid at least the minimum wage in the country of the data
1326"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9869621903520208,"collector.
1327"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876140808344198,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1328"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9882659713168188,"Subjects
1329"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9889178617992177,"Question: Does the paper describe potential risks incurred by study participants, whether
1330"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895697522816167,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1331"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902216427640157,"approvals (or an equivalent approval/review based on the requirements of your country or
1332"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908735332464146,"institution) were obtained?
1333"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9915254237288136,"Answer: [NA]
1334"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9921773142112125,"Justification: Our research does not involve any human subjects.
1335"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9928292046936115,"Guidelines:
1336"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934810951760105,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1337"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941329856584094,"human subjects.
1338"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947848761408083,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1339"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9954367666232073,"may be required for any human subjects research. If you obtained IRB approval, you
1340"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9960886571056062,"should clearly state this in the paper.
1341"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967405475880052,"• We recognize that the procedures for this may vary significantly between institutions
1342"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9973924380704041,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1343"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980443285528031,"guidelines for their institution.
1344"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9986962190352021,"• For initial submissions, do not include any information that would break anonymity (if
1345"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.999348109517601,"applicable), such as the institution conducting the review.
1346"
