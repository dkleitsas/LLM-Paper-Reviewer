Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0011723329425556857,"In Federated Learning, a common approach for aggregating local models across
clients is periodic averaging of the full model parameters. It is, however, known
that different layers of neural networks can have a different degree of model dis-
crepancy across the clients. The conventional full aggregation scheme does not
consider such a difference and synchronizes the whole model parameters at once,
resulting in inefÔ¨Åcient network bandwidth consumption. Aggregating the parame-
ters that are similar across the clients does not make meaningful training progress
while increasing the communication cost. We propose FedLAMA, a layer-wise
model aggregation scheme for scalable Federated Learning. FedLAMA adap-
tively adjusts the aggregation interval in a layer-wise manner, jointly considering
the model discrepancy and the communication cost. The layer-wise aggregation
method enables to Ô¨Ånely control the aggregation interval to relax the aggregation
frequency without a signiÔ¨Åcant impact on the model accuracy. Our empirical study
shows that FedLAMA reduces the communication cost by up to 60% for IID data
and 70% for non-IID data while achieving a comparable accuracy to FedAvg."
INTRODUCTION,0.0023446658851113715,"1
INTRODUCTION"
INTRODUCTION,0.0035169988276670576,"In Federated Learning, periodic full model aggregation is the most common approach for aggregat-
ing local models across clients. Many Federated Learning algorithms, such as FedAvg (McMahan
et al. (2017)), FedProx (Li et al. (2018)), FedNova (Wang et al. (2020)), and SCAFFOLD (Karim-
ireddy et al. (2020)), assume the underlying periodic full aggregation scheme. However, it has been
observed that the magnitude of gradients can be signiÔ¨Åcantly different across the layers of neural net-
works (You et al. (2019)). That is, all the layers can have a different degree of model discrepancy.
The periodic full aggregation scheme does not consider such a difference and synchronizes the en-
tire model parameters at once. Aggregating the parameters that are similar across all the clients does
not make meaningful training progress while increasing the communication cost. Considering the
limited network bandwidth in usual Federated Learning environments, such an inefÔ¨Åcient network
bandwidth consumption can signiÔ¨Åcantly harm the scalability of Federated Learning applications."
INTRODUCTION,0.004689331770222743,"Many researchers have put much effort into addressing the expensive communication issue. Adap-
tive model aggregation methods adjust the aggregation interval to reduce the total communication
cost (Wang & Joshi (2018); Haddadpour et al. (2019)). Gradient (model) compression (Alistarh
et al. (2018); Albasyoni et al. (2020)), sparsiÔ¨Åcation (Wangni et al. (2017); Wang et al. (2018)),
low-rank approximation (Vogels et al. (2020); Wang et al. (2021)), and quantization (Alistarh et al.
(2017); Wen et al. (2017); Reisizadeh et al. (2020)) techniques directly reduce the local data size.
Employing heterogeneous model architectures across clients is also a communication-efÔ¨Åcient ap-
proach (Diao et al. (2020)). While all these works effectively tackle the expensive communication
issue from different angles, they commonly assume the underlying periodic full model aggregation."
INTRODUCTION,0.005861664712778429,"To break such a convention of periodic full model aggregation, we propose FedLAMA, a novel layer-
wise adaptive model aggregation scheme for scalable and accurate Federated Learning. FedLAMA
Ô¨Årst prioritizes all the layers based on their contributions to the total model discrepancy. We present
a metric for estimating the layer-wise degree of model discrepancy at run-time. The aggregation
intervals are adjusted based on the layer-wise model discrepancy such that the layers with a smaller
degree of model discrepancy is assigned with a longer aggregation interval than the other layers.
The above steps are repeatedly performed once the entire model is synchronized once."
INTRODUCTION,0.007033997655334115,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.008206330597889801,"Our focus is on how to relax the model aggregation frequency at each layer, jointly considering the
communication efÔ¨Åciency and the impact on the convergence properties of federated optimization.
By adjusting the aggregation interval based on the layer-wise model discrepancy, the local mod-
els can be effectively synchronized while reducing the number of communications at each layer.
The model accuracy is marginally affected since the intervals are increased only at the layers that
have the smallest contribution to the total model discrepancy. Our empirical study demonstrates
that FedLAMA automatically Ô¨Ånds the interval settings that make a practical trade-off between the
communication cost and the model accuracy. We also provide a theoretical convergence analysis of
FedLAMA for smooth and non-convex problems under non-IID data settings."
INTRODUCTION,0.009378663540445486,"We evaluate the performance of FedLAMA across three representative image classiÔ¨Åcation bench-
mark datasets: CIFAR-10 (Krizhevsky et al. (2009)), CIFAR-100, and Federated Extended MNIST
(Cohen et al. (2017)). Our experimental results deliver novel insights on how to aggregate the local
models efÔ¨Åciently consuming the network bandwidth. Given a Ô¨Åxed number of training iterations, as
the aggregation interval increases, FedLAMA reduces the communication cost by up to 60% under
IID data settings and 70% under non-IID data settings, while having only a marginal accuracy drop."
RELATED WORKS,0.010550996483001172,"2
RELATED WORKS"
RELATED WORKS,0.011723329425556858,"Compression Methods ‚Äì The communication-efÔ¨Åcient global model update methods can be catego-
rized into two groups: structured update and sketched update (KoneÀácn`y et al. (2016)). The structured
update indicates the methods that enforce a pre-deÔ¨Åned Ô¨Åxed structure of the local updates, such as
low-rank approximation and random mask methods. The sketched update indicates the methods
that post-process the local updates via compression, sparsiÔ¨Åcation, or quantization. Both directions
are well studied and have shown successful results (Alistarh et al. (2018); Albasyoni et al. (2020);
Wangni et al. (2017); Wang et al. (2018); Vogels et al. (2020); Wang et al. (2021); Alistarh et al.
(2017); Wen et al. (2017); Reisizadeh et al. (2020)). The common principle behind these methods is
that the local updates can be replaced with a different data representation with a smaller size."
RELATED WORKS,0.012895662368112544,"These compression methods can be independently applied to our layer-wise aggregation scheme
such that the each layer‚Äôs local update is compressed before being aggregated. Since our focus is on
adjusting the aggregation frequency rather than changing the data representation, we do not directly
compare the performance between these two approaches. We leave harmonizing the layer-wise
aggregation scheme and a variety of compression methods as a promising future work."
RELATED WORKS,0.01406799531066823,"Similarity Scores ‚Äì Canonical Correlation Analysis (CCA) methods are proposed to estimate the
representational similarity across different models (Raghu et al. (2017); Morcos et al. (2018)). Cen-
tered Kernel Alignment (CKA) is an improved extension of CCA (Kornblith et al. (2019)). While
these methods effectively quantify the degree of similarity, they commonly require expensive com-
putations. For example, SVCCA performs singular vector decomposition of the model and CKA
computes Hilbert-Schmidt Independence Criterion multiple times (Gretton et al. (2005)). In addi-
tion, the representational similarity does not deliver any information regarding the gradient differ-
ence that is strongly related to the convergence property. We will propose a practical metric for
estimating the layer-wise model discrepancy, which is cheap enough to be used at run-time."
RELATED WORKS,0.015240328253223915,"Layer-wise Model Freezing ‚Äì Layer freezing (dropping) is the representative layer-wise technique
for neural network training (Brock et al. (2017); Kumar et al. (2019); Zhang & He (2020); Goutam
et al. (2020)). All these methods commonly stop updating the parameters of the layers in a bottom-
up direction. These empirical techniques are supported by the analysis presented in (Raghu et al.
(2017)). Since the layers converge from the input-side sequentially, the layer-wise freezing can re-
duce the training time without strongly affecting the accuracy. These previous works clearly demon-
strate the advantages of processing individual layers separately."
BACKGROUND,0.016412661195779603,"3
BACKGROUND"
BACKGROUND,0.017584994138335287,Federated Optimization ‚Äì We consider federated optimization problems as follows.
BACKGROUND,0.01875732708089097,"min
x‚ààRd """
BACKGROUND,0.01992966002344666,"F(x) := m
X"
BACKGROUND,0.021101992966002344,"i=1
piFi(x) # ,
(1)"
BACKGROUND,0.022274325908558032,Under review as a conference paper at ICLR 2022
BACKGROUND,0.023446658851113716,"Algorithm 1: FedLAMA: Federated Layer-wise Adaptive Model Aggregation.
Input: œÑ ‚Ä≤: base aggregation interval, œÜ: interval increasing factor, pi, i ‚àà{1, ¬∑ ¬∑ ¬∑ , m}."
BACKGROUND,0.0246189917936694,"1 œÑl ‚ÜêœÑ ‚Ä≤, ‚àÄl ‚àà{1, ¬∑ ¬∑ ¬∑ , L};"
BACKGROUND,0.02579132473622509,2 for k = 1 to K do
BACKGROUND,0.026963657678780773,"3
SGD step: xi
k = xi
k‚àí1 ‚àíŒ∑‚àáf(wi
k‚àí1, Œæk);"
BACKGROUND,0.02813599062133646,"4
for l = 1 to L do"
BACKGROUND,0.029308323563892145,"5
if k mod œÑl is 0 then"
BACKGROUND,0.03048065650644783,"6
Synchronize layer l: u(l,k) ‚ÜêPm
i=1 pixi
(l,k);"
BACKGROUND,0.031652989449003514,"7
dl ‚ÜêPm
i=1

pi‚à•u(l,k) ‚àíxi
(l,k)‚à•2
/(œÑl(dim(u(l,k))) ;"
BACKGROUND,0.032825322391559206,"8
if k mod œÜœÑ ‚Ä≤ is 0 then"
BACKGROUND,0.03399765533411489,"9
Adjust aggregation interval at all L layers (Algorithm 2).;"
BACKGROUND,0.035169988276670575,10 Output: uK;
BACKGROUND,0.03634232121922626,"where pi = ni/n is the ratio of local data to the total dataset, and Fi(x) =
1
ni
P"
BACKGROUND,0.03751465416178194,"Œæ‚ààD fi(x, Œæ) is the
local objective function of client i. n is the global dataset size and ni is the local dataset size."
BACKGROUND,0.038686987104337635,"FedAvg is a basic algorithm that solves the above minimization problem. As the degree of data het-
erogeneity increases, FedAvg converges more slowly. Several variants of FedAvg, such as FedProx,
FedNova, and SCAFFOLD, tackle the data heterogeneity issue. All these algorithms commonly
aggregate the local solutions using the periodic full aggregation scheme."
BACKGROUND,0.03985932004689332,"Model Discrepancy ‚Äì All local SGD-based algorithms allow the clients to independently train their
local models within each communication round. The variance of stochastic gradients and heteroge-
neous data distribution can lead the local models to different directions on parameter space during
the local update steps. We formally deÔ¨Åne such a discrepancy among the models as follows. m
X"
BACKGROUND,0.041031652989449004,"i=1
pi‚à•u ‚àíxi‚à•2,"
BACKGROUND,0.04220398593200469,"where m is the number of clients, u is the synchronized model, and xi is client i‚Äôs local model.
This quantity bounds the difference between the local gradients and the global gradients under a
smoothness assumption on objective functions."
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.04337631887456037,"4
LAYER-WISE ADAPTIVE MODEL AGGREGATION"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.044548651817116064,"Layer Prioritization ‚Äì In theoretical analysis, it is common to assume the smoothness of objective
functions such that the difference between local gradients and global gradients is bounded by a
scaled difference of the corresponding sets of parameters. Motivated by this convention, we deÔ¨Åne
‚Äòlayer-wise unit model discrepancy‚Äô, a useful metric for prioritizing the layers as follows."
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.04572098475967175,"dl =
Pm
i=1 pi‚à•ul ‚àíxi
l‚à•2"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.04689331770222743,"œÑl(dim(ul))
,
l ‚àà{1, ¬∑ ¬∑ ¬∑ , L}
(2)"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.04806565064478312,"where L is the number of layers, l is the layer index, u is the global parameters, xi is the client i‚Äôs
local parameters, œÑ is the aggregation interval, and dim(¬∑) is the number of parameters."
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.0492379835873388,"This metric quantiÔ¨Åes how much each parameter contributes to the model discrepancy at each iter-
ation. The communication cost is proportional to the number of parameters. Thus, Pm
i=1 pi‚à•ul ‚àí
xi
l‚à•2/dim(ul) shows how much model discrepancy can be eliminated by synchronizing the layer
at a unit communication cost. This metric allows prioritizing the layers such that the layers with a
smaller dl value has a lower priority than the others."
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05041031652989449,"Adaptive Model Aggregation Algorithm ‚Äì We propose FedLAMA, a layer-wise adaptive model
aggregation scheme. Algorithm 1 shows FedLAMA algorithm. There are two input parameters: œÑ ‚Ä≤
is the base aggregation interval and œÜ is the interval increase factor. First, the parameters at layer
l are synchronized across the clients after every œÑl iterations (line 6). Then, the proposed metric"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05158264947245018,Under review as a conference paper at ICLR 2022
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05275498241500586,"Algorithm 2: Layer-wise Adaptive Interval Adjustment.
Input: d: the observed model discrepancy at all L layers, œÑ ‚Ä≤: the base aggregation interval, œÜ:
the interval increasing factor."
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.053927315357561546,1 Sorted model discrepancy: ÀÜd ‚Üêsort (d);
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05509964830011723,2 Sorted index of the layers: ÀÜi ‚Üêargsort (d);
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05627198124267292,"3 Total model size: Œª ‚ÜêPL
l=1 dim(ul);"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05744431418522861,"4 Total model discrepancy: Œ¥ ‚ÜêPL
l=1 dl ‚àódim(ul);"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05861664712778429,5 for l = 1 to L do
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.059788980070339975,"6
Œ¥l ‚Üê(Pl
i=1 ÀÜdi ‚àódim(ui))/Œ¥;"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.06096131301289566,"7
Œªl ‚Üê(Pl
i=1 dim(ui))/Œª;"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.06213364595545135,"8
Find the layer index: i ‚ÜêÀÜil ;"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.06330597889800703,"9
if Œ¥l < Œªl then"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.06447831184056271,"10
œÑi ‚ÜêœÜœÑ ‚Ä≤;"
ELSE,0.06565064478311841,"11
else"
ELSE,0.0668229777256741,"12
œÑi ‚ÜêœÑ ‚Ä≤;"
ELSE,0.06799531066822978,13 Output: œÑ: the adjusted aggregation intervals at all L layers.;
ELSE,0.06916764361078546,"dl is calculated using the synchronized parameters ul (line 7). At the end of every œÜœÑ ‚Ä≤ iterations,
FedLAMA adjusts the model aggregation interval at all the L layers. (line 9)."
ELSE,0.07033997655334115,"Algorithm 2 Ô¨Ånds the layers that can be less frequently aggregated making a minimal impact on the
total model discrepancy. First, the layer-wise degree of model discrepancy is estimated as follows."
ELSE,0.07151230949589683,"Œ¥l =
Pl
i=1 ÀÜdi ‚àódim(ui)
PL
i=1 ÀÜdi ‚àódim(ui)
,
(3)"
ELSE,0.07268464243845252,"where ÀÜdi is the ith smallest element in the sorted list of the proposed metric d. Given l layers with
the smallest dl values, Œ¥l quantiÔ¨Åes their contribution to the total model discrepancy. Second, the
communication cost impact is estimated as follows."
ELSE,0.0738569753810082,"Œªl =
Pl
i=1 dim(ui)
PL
i=1 dim(ui)
(4)"
ELSE,0.07502930832356389,"Œªl is the ratio of the parameters at the l layers with the smallest dl values. Thus, 1 ‚àíŒªl estimates
the number of parameters that will be more frequently synchronized than the others. As l increases,
Œ¥l increases while 1 ‚àíŒªl decreases monotonically. Algorithm 2 loops over the L layers Ô¨Ånding the
l value that makes Œ¥l and 1 ‚àíŒªl similar. In this way, it Ô¨Ånds the aggregation interval setting that
slightly sacriÔ¨Åces the model discrepancy while remarkably reducing the communication cost."
ELSE,0.07620164126611957,"Figure 1 shows the Œ¥l and 1 ‚àíŒªl curves collected from a) CIFAR-10 (ResNet20) training and b)
CIFAR-100 (Wide-ResNet28-10) training. The x-axis is the number of layers to increase the aggre-
gation interval and the y-axis is the Œ¥l and 1 ‚àíŒªl values. The cross point of the two curves is much
lower than 0.5 on y-axis in both charts. For instance, in Figure 1.a), the two curves are crossed when
x value is 9, and the corresponding y value is near 0.2. That is, when the aggregation interval is
increased at those 9 layers, 20% of the total model discrepancy will increase by a factor of œÜ while
80% of the total communication cost will decrease by the same factor. Note that the cross points are
below 0.5 since the Œ¥l and 1 ‚àíŒªl are calculated using the dl values sorted in an increasing order."
ELSE,0.07737397420867527,"It is worth noting that FedLAMA can be easily extended to improve the convergence rate at the cost
of having minor extra communications. In this work, we do not consider Ô¨Ånding such interval set-
tings because it can increase the latency cost, which is not desired in Federated Learning. However,
in the environments where the latency cost can be ignored, such as high-performance computing
platforms, FedLAMA can accelerate the convergence by adjusting the intervals based on the cross
point of 1 ‚àíŒ¥l and Œªl calculated using the list of dl values sorted in a decreasing order."
ELSE,0.07854630715123095,"Impact of Aggregation Interval Increasing Factor œÜ ‚Äì In Federated Learning, the communication
latency cost is usually not negligible, and the total number of communications strongly affects the"
ELSE,0.07971864009378664,Under review as a conference paper at ICLR 2022
ELSE,0.08089097303634232,"a) CIFAR-10 (ResNet20) 
b) CIFAR-100 (WRN28-10)"
ELSE,0.08206330597889801,"0
5
10
15
20
25
0.0 0.2 0.4 0.6 0.8 1.0"
ELSE,0.08323563892145369,normalized magnitude
ELSE,0.08440797186400938,# of layers with increased interval
ELSE,0.08558030480656506,"delta
 lambda"
ELSE,0.08675263774912075,"0
5
10
15
20
0.0 0.2 0.4 0.6 0.8 1.0"
ELSE,0.08792497069167643,normalized magnitude
ELSE,0.08909730363423213,# of layers with increased interval
ELSE,0.09026963657678781,"delta
 lambda"
ELSE,0.0914419695193435,"Figure 1: The comparison between the model discrepancy increase factor Œ¥l and the communication
cost decrease factor 1 ‚àíŒªl for a) CIFAR-10 and b) CIFAR-100 training."
ELSE,0.09261430246189918,"scalability. When increasing the aggregation interval, Algorithm 2 multiplies a pre-deÔ¨Åned small
constant œÜ to the Ô¨Åxed base interval œÑ ‚Ä≤ (line 10). This approach ensures that the communication
latency cost is not increased while the network bandwidth consumption is reduced by a factor of œÜ."
ELSE,0.09378663540445487,"FedAvg can be considered as a special case of FedLAMA where œÜ is set to 1. When œÜ > 1, Fed-
LAMA less frequently synchronize a subset of layers, and it results in reducing their communication
costs. When increasing the aggregation interval, FedLAMA multiplies œÜ to the base interval œÑ ‚Ä≤. So,
it is guaranteed that the whole model parameters are fully synchronized after œÜœÑ ‚Ä≤ iterations. Because
of the layers with the base aggregation interval œÑ ‚Ä≤, the total model discrepancy of FedLAMA after
œÜœÑ ‚Ä≤ iterations is always smaller than that of FedAvg with an aggregation interval of œÜœÑ ‚Ä≤."
CONVERGENCE ANALYSIS,0.09495896834701055,"5
CONVERGENCE ANALYSIS"
PRELIMINARIES,0.09613130128956623,"5.1
PRELIMINARIES"
PRELIMINARIES,0.09730363423212192,"Notations ‚Äì All vectors in this paper are column vectors. x ‚ààRd denotes the parameters of one
local model and m is the number of clients. The stochastic gradient computed from a single training
data point Œæ is denoted by g(x, Œæ). For convenience, we use g(x) instead. The full batch gradient is
denoted by ‚àáF(x). We use ‚à•¬∑‚à•and ‚à•¬∑‚à•op to denote l2 norm and matrix operator norm, respectively."
PRELIMINARIES,0.0984759671746776,Assumptions ‚Äì We analyze the convergence rate of FedLAMA under the following assumptions.
PRELIMINARIES,0.09964830011723329,"1. (Smoothness). Each local objective function is L-smooth, that is, ‚à•‚àáFi(x) ‚àí‚àáFi(y)‚à•‚â§
L‚à•x ‚àíy‚à•, ‚àÄi ‚àà{1, ¬∑ ¬∑ ¬∑ , m}.
2. (Unbiased Gradient). The stochastic gradient at each client is an unbiased estimator of the
local full-batch gradient: EŒæ[gi(x, Œæ)] = ‚àáFi(x).
3. (Bounded Variance).
The stochastic gradient at each client has bounded variance:
EŒæ[‚à•gi(x, Œæ) ‚àí‚àáFi(x)‚à•2 ‚â§œÉ2], ‚àÄi ‚àà{1, ¬∑ ¬∑ ¬∑ , m}, œÉ2 ‚â•0.
4. (Bounded Dissimilarity). For any sets of weights {pi ‚â•0}m
i=1, Pm
i=1 pi = 1, there exist
constants Œ≤2 ‚â•1 and Œ∫2 ‚â•0 such that Pm
i=1 pi‚à•‚àáFi(x)‚à•2 ‚â§Œ≤2‚à•Pm
i=1 pi‚àáFi(x)‚à•2+Œ∫2.
If local objective functions are identical to each other, Œ≤2 = 1 and Œ∫2 = 0."
ANALYSIS,0.10082063305978899,"5.2
ANALYSIS"
ANALYSIS,0.10199296600234467,"We begin with showing two key lemmas. All the proofs can be found in Appendix.
Lemma 5.1. (Framework) Under Assumption 1 ‚àº3, if the learning rate satisÔ¨Åes Œ∑ ‚â§
1
2L, FedLAMA
ensures"
K,0.10316529894490035,"1
K K
X"
K,0.10433763188745604,"k=1
E
h
‚à•‚àáF(uk)‚à•2i
‚â§
2
Œ∑K E [F(u1) ‚àíF(u‚àó)] + 2Œ∑LœÉ2
m
X"
K,0.10550996483001172,"i=1
(pi)2 + L2 K K
X k=1 m
X"
K,0.10668229777256741,"i=1
pi E
huk ‚àíxi
k
2i
. (5)"
K,0.10785463071512309,Under review as a conference paper at ICLR 2022
K,0.10902696365767878,"Lemma 5.2. (Model Discrepancy) Under Assumption 1 ‚àº4, if the learning rate satisÔ¨Åes Œ∑ <
1
2(œÑmax‚àí1)L, FedLAMA ensures"
K,0.11019929660023446,"1
K K
X k=1 m
X"
K,0.11137162954279015,"i=1
pi E
huk ‚àíxi
k
2i
‚â§2Œ∑2(œÑmax ‚àí1)œÉ2"
K,0.11254396248534584,"1 ‚àíA
+
AŒ∫2"
K,0.11371629542790153,"L2(1 ‚àíA) +
AŒ≤2"
K,0.11488862837045721,"KL2(1 ‚àíA) K
X"
K,0.1160609613130129,"k=1
E
h
‚à•‚àáF(uk)‚à•2i
, (6)"
K,0.11723329425556858,where A = 4Œ∑2(œÑmax ‚àí1)2L2 and œÑmax is the largest averaging interval across all the layers.
K,0.11840562719812427,"Based on Lemma 5.1 and 5.2, we analyze the convergence rate of FedLAMA as follows."
K,0.11957796014067995,"Theorem 5.3. Suppose all m local models are initialized to the same point u1.
Under As-
sumption 1
‚àº
4, if FedLAMA runs for K iterations and the learning rate satisÔ¨Åes Œ∑
‚â§"
K,0.12075029308323564,"min

1
2(œÑmax‚àí1)L,
1
L‚àö"
K,0.12192262602579132,2œÑmax(œÑmax‚àí1)(2Œ≤2+1)
K,0.123094958968347,"
, FedLAMA ensures E ""
1
K K
X"
K,0.1242672919109027,"i=1
‚à•‚àáF(uk)‚à•2
#"
K,0.1254396248534584,"‚â§
4
Œ∑K (E [F(u1) ‚àíF(u‚àó)]) + 4Œ∑ m
X"
K,0.12661195779601406,"i=1
p2
i LœÉ2
(7)"
K,0.12778429073856976,"+ 3Œ∑2(œÑmax ‚àí1)L2œÉ2 + 6Œ∑2œÑmax(œÑmax ‚àí1)L2Œ∫2,"
K,0.12895662368112543,where u‚àóindicates a local minimum and œÑmax is the largest averaging interval across all the layers.
K,0.13012895662368112,"Remark 1. (Linear Speedup) With a sufÔ¨Åciently small diminishing learning rate and a large number
of training iterations, FedLAMA achieves linear speedup. If the learning rate is Œ∑ =
‚àöm
‚àö"
K,0.13130128956623682,"K and
pi = 1"
K,0.1324736225087925,"m, ‚àÄi ‚àà{1, ¬∑ ¬∑ ¬∑ , m}, we have E ""
1
K K
X"
K,0.1336459554513482,"i=1
‚à•‚àáF(uk)‚à•2
#"
K,0.13481828839390386,"‚â§O

1
‚àö mK"
K,0.13599062133645956,"
+ O
m K 
(8)"
K,0.13716295427901523,"If K > m3, the Ô¨Årst term on the right-hand side becomes dominant and it achieves linear speedup."
K,0.13833528722157093,"Remark 2. (Impact of Interval Increase Factor œÜ) The worst-case model discrepancy depends on the
largest averaging interval across all the layers, œÑmax = œÜœÑ ‚Ä≤. The larger the interval increase factor œÜ,
the larger the model discrepancy terms in (7). In the meantime, as œÜ increases, the communication
frequency at the selected layers is proportionally reduced. So, œÜ should be appropriately tuned to
effectively reduce the communication cost while not much increasing the model discrepancy."
EXPERIMENTS,0.1395076201641266,"6
EXPERIMENTS"
EXPERIMENTS,0.1406799531066823,"Experimental Settings ‚Äì We evaluate FedLAMA using three representative benchmark datasets:
CIFAR-10 (ResNet20 (He et al. (2016))), CIFAR-100 (WideResNet28-10 (Zagoruyko & Komodakis
(2016))), and Federated Extended MNIST (CNN (Caldas et al. (2018))). We use TensorFlow 2.4.3
for local training and MPI for model aggregation. All our experiments are conducted on 4 compute
nodes each of which has 2 NVIDIA v100 GPUs."
EXPERIMENTS,0.141852286049238,"Due to the limited compute resources, we simulate Federated Learning such that each process se-
quentially trains multiple models and then the models are aggregated across all the processes at
once. While it provides the same classiÔ¨Åcation results as the actual Federated Learning, the train-
ing time is serialized within each process. Thus, instead of wall-clock time, we consider the total
communication cost calculated as follows. C = L
X"
EXPERIMENTS,0.14302461899179367,"l=1
Cl = L
X"
EXPERIMENTS,0.14419695193434937,"l=1
dim(ul) ‚àóŒ∫l,
(9)"
EXPERIMENTS,0.14536928487690504,where Œ∫l is the total number of communications at layer l during the training.
EXPERIMENTS,0.14654161781946073,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.1477139507620164,"Table 1:
(IID data) CIFAR-10 classiÔ¨Åcation results. The number of workers is 128 and the local
batch size is 32 in all the experiments. The epoch budget is 300."
EXPERIMENTS,0.1488862837045721,"LR
Base aggregation interval: œÑ ‚Ä≤
Interval increase factor: œÜ
Validation acc.
Comm. cost
0.8
6
1 (FedAvg)
88.37 ¬± 0.02%
100%
0.8
12
1 (FedAvg)
84.74 ¬± 0.05%
50%
0.4
6
2 (FedLAMA)
88.41 ¬±0.01%
62.33%
0.6
24
1 (FedAvg)
80.34 ¬± 0.3%
25%
0.6
6
4 (FedLAMA)
86.21 ¬±0.1%
42.17%"
EXPERIMENTS,0.15005861664712777,"Table 2: (IID data) CIFAR-100 classiÔ¨Åcation results. The number of workers is 128 and the local
batch size is 32 in all the experiments. The epoch budget is 250."
EXPERIMENTS,0.15123094958968347,"LR
Base aggregation interval: œÑ ‚Ä≤
Interval increase factor: œÜ
Validation acc.
Comm. cost
0.6
6
1 (FedAvg)
76.50 ¬± 0.02%
100%
0.6
12
1 (FedAvg)
66.97 ¬± 0.9%
50%
0.5
6
2 (FedLAMA)
76.02 ¬±0.01%
66.01%
0.6
24
1 (FedAvg)
45.01 ¬± 1.1%
25%
0.5
6
4 (FedLAMA)
76.17 ¬±0.02%
39.91%"
EXPERIMENTS,0.15240328253223914,"Hyper-Parameter Settings ‚Äì We use 128 clients in our experiments. The local batch size is set to 32
and the learning rate is tuned based on a grid search. For CIFAR-10 and CIFAR-100, we artiÔ¨Åcially
generate heterogeneous data distributions using Dirichlet‚Äôs distribution. When using Non-IID data,
we also consider partial device participation such that randomly chosen 25% of the clients participate
in training at every œÜœÑ ‚Ä≤ iterations. We report the average accuracy across at least three separate runs."
CLASSIFICATION PERFORMANCE ANALYSIS,0.15357561547479484,"6.1
CLASSIFICATION PERFORMANCE ANALYSIS"
CLASSIFICATION PERFORMANCE ANALYSIS,0.15474794841735054,"To evaluate the proposed model aggregation scheme, we keep all the other factors the same, such as
optimizer, the number of clients, the degree of heterogeneity, and compare the performance across
different model aggregation schemes. We compare the performance across three different model
aggregation settings as follows."
CLASSIFICATION PERFORMANCE ANALYSIS,0.1559202813599062,‚Ä¢ Periodic full aggregation with an interval of œÑ ‚Ä≤
CLASSIFICATION PERFORMANCE ANALYSIS,0.1570926143024619,‚Ä¢ Periodic full aggregation with an interval of œÜœÑ ‚Ä≤
CLASSIFICATION PERFORMANCE ANALYSIS,0.15826494724501758,‚Ä¢ Layer-wise adaptive aggregation with intervals of œÑ ‚Ä≤ and œÜ
CLASSIFICATION PERFORMANCE ANALYSIS,0.15943728018757328,"The Ô¨Årst setting provides the baseline communication cost, and we compare it to the other settings‚Äô
communication costs. The third setting is FedLAMA with the base aggregation interval œÑ ‚Ä≤ and the
interval increase factor œÜ. Due to the limited space, we present a part of experimental results that
deliver the key insights. More results can be found in Appendix."
CLASSIFICATION PERFORMANCE ANALYSIS,0.16060961313012895,"Experimental Results with IID Data ‚Äì We Ô¨Årst present CIFAR-10 and CIFAR-100 classiÔ¨Åcation
results under IID data settings. Table 1 and 2 show the CIFAR-10 and CIFAR-100 results, respec-
tively. Note that the learning rate is individually tuned for each setting using a grid search, and
we report the best settings. In both tables, the Ô¨Årst row shows the performance of FedAvg with
a short interval œÑ ‚Ä≤ = 6. As the interval increases, FedAvg signiÔ¨Åcantly loses the accuracy while
the communication cost is proportionally reduced. FedLAMA achieves a comparable accuracy to
FedAvg with œÑ ‚Ä≤ = 6 while its communication cost is similar to that of FedAvg with œÜœÑ ‚Ä≤. These
results demonstrate that Algorithm 2 effectively Ô¨Ånds the layer-wise interval settings that maximize
the communication cost reduction while minimizing the model discrepancy increase."
CLASSIFICATION PERFORMANCE ANALYSIS,0.16178194607268465,"Experimental Results with Non-IID Data ‚Äì We now evaluate the performance of FedLAMA using
non-IID data. FEMNIST is inherently heterogeneous such that it contains the hand-written digit
pictures collected from 3, 550 different writers. We use random 10% of the writers‚Äô training samples
in our experiments. Table 3 shows the FEMNIST classiÔ¨Åcation results. The base interval œÑ ‚Ä≤ is set
to 10. FedAvg (œÜ = 1) signiÔ¨Åcantly loses the accuracy as the aggregation interval increases. For
example, when the interval increases from 10 to 40, the accuracy is dropped by 2.1% ‚àº2.7%.
In contrast, FedLAMA maintains the accuracy when œÜ increases, while the communication cost
is remarkably reduced. This result demonstrates that FedLAMA effectively Ô¨Ånds the best interval
setting that reduces the communication cost while maintaining the accuracy."
CLASSIFICATION PERFORMANCE ANALYSIS,0.16295427901524032,Under review as a conference paper at ICLR 2022
CLASSIFICATION PERFORMANCE ANALYSIS,0.16412661195779601,"Table 3:
(Non-IID data) FEMNIST classiÔ¨Åcation results. The number of workers is 128 and the
local batch size is 32 in all the experiments. The number of training iterations is 2, 000."
CLASSIFICATION PERFORMANCE ANALYSIS,0.1652989449003517,"LR
Base aggregation interval: œÑ ‚Ä≤
Interval increase factor: œÜ
active ratio
Validation acc.
Comm. cost 0.04"
CLASSIFICATION PERFORMANCE ANALYSIS,0.16647127784290738,"10
1 (FedAvg) 25%"
CLASSIFICATION PERFORMANCE ANALYSIS,0.16764361078546308,"86.04 ¬± 0.01%
100%
20
1 (FedAvg)
85.38 ¬± 0.02%
50%
10
2 (FedLAMA)
86.01 ¬±0.01%
52.83%
40
1 (FedAvg)
83.97 ¬± 0.02%
25%
10
4 (FedLAMA)
85.61 ¬±0.02%
29.97% 0.04"
CLASSIFICATION PERFORMANCE ANALYSIS,0.16881594372801875,"10
1 (FedAvg) 50%"
CLASSIFICATION PERFORMANCE ANALYSIS,0.16998827667057445,"86.59 ¬± 0.01%
100%
20
1 (FedAvg)
85.50 ¬± 0.02%
50%
10
2 (FedLAMA)
86.07 ¬±0.02%
53.32%
40
1 (FedAvg)
83.92 ¬± 0.02%
25%
10
4 (FedLAMA)
85.77 ¬±0.02%
29.98% 0.04"
CLASSIFICATION PERFORMANCE ANALYSIS,0.17116060961313012,"10
1 (FedAvg) 100%"
CLASSIFICATION PERFORMANCE ANALYSIS,0.17233294255568582,"85.74 ¬± 0.03%
100%
20
1 (FedAvg)
85.08 ¬± 0.01%
50%
10
2 (FedLAMA)
85.40 ¬±0.02%
51.86%
40
1 (FedAvg)
83.62 ¬± 0.02%
25%
10
4 (FedLAMA)
84.67 ¬±0.02%
29.98%"
CLASSIFICATION PERFORMANCE ANALYSIS,0.1735052754982415,"Table 4:
(Non-IID data) CIFAR-10 classiÔ¨Åcation results. The number of workers is 128 and the
local batch size is 32 in all the experiments. The number of training iterations is 6, 000."
CLASSIFICATION PERFORMANCE ANALYSIS,0.1746776084407972,"LR
Base aggregation interval: œÑ ‚Ä≤
Interval increase factor: œÜ
active ratio
Dirichlet‚Äôs coeff.
Validation acc.
Comm. cost 0.4"
CLASSIFICATION PERFORMANCE ANALYSIS,0.17584994138335286,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.17702227432590856,"25%
0.1"
CLASSIFICATION PERFORMANCE ANALYSIS,0.17819460726846426,"84.02 ¬± 0.1%
100%
24
1 (FedAvg)
76.27 ¬± 0.08%
25%
6
4 (FedLAMA)
83.06 ¬±0.1%
39.52% 0.8"
CLASSIFICATION PERFORMANCE ANALYSIS,0.17936694021101993,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.18053927315357562,"25%
0.5"
CLASSIFICATION PERFORMANCE ANALYSIS,0.1817116060961313,"87.59 ¬± 0.2%
100%
24
1 (FedAvg)
83.36 ¬± 0.4%
25%
6
4 (FedLAMA)
86.57 ¬±0.02%
42.40% 0.8"
CLASSIFICATION PERFORMANCE ANALYSIS,0.182883939038687,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.18405627198124266,"100%
0.1"
CLASSIFICATION PERFORMANCE ANALYSIS,0.18522860492379836,"89.52 ¬± 0.05%
100%
24
1 (FedAvg)
84.82 ¬± 0.06%
25%
6
4 (FedLAMA)
87.47 ¬±0.1%
42.49% 0.8"
CLASSIFICATION PERFORMANCE ANALYSIS,0.18640093786635403,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.18757327080890973,"100%
0.5"
CLASSIFICATION PERFORMANCE ANALYSIS,0.18874560375146543,"90.53 ¬± 0.08%
100%
24
1 (FedAvg)
85.68 ¬± 0.1%
25%
6
4 (FedLAMA)
87.45 ¬±0.05%
42.73%"
CLASSIFICATION PERFORMANCE ANALYSIS,0.1899179366940211,"Table 5: (Non-IID data) CIFAR-100 classiÔ¨Åcation results. The number of workers is 128 and the
local batch size is 32 in all the experiments. The number of training iterations is 6, 000."
CLASSIFICATION PERFORMANCE ANALYSIS,0.1910902696365768,"LR
Base aggregation interval: œÑ ‚Ä≤
Interval increase factor: œÜ
active ratio
Dirichlet‚Äôs coeff.
Validation acc.
Comm. cost 0.4"
CLASSIFICATION PERFORMANCE ANALYSIS,0.19226260257913247,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.19343493552168817,"25%
0.1"
CLASSIFICATION PERFORMANCE ANALYSIS,0.19460726846424384,"79.15 ¬± 0.02%
100%
12
1 (FedAvg)
76.16 ¬± 0.05%
50%
6
2 (FedLAMA)
78.63 ¬±0.03%
63.14% 0.4"
CLASSIFICATION PERFORMANCE ANALYSIS,0.19577960140679954,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.1969519343493552,"25%
0.5"
CLASSIFICATION PERFORMANCE ANALYSIS,0.1981242672919109,"78.81 ¬± 0.1%
100%
12
1 (FedAvg)
76.11 ¬± 0.05%
50%
6
2 (FedLAMA)
77.86 ¬±0.04%
63.20% 0.4"
CLASSIFICATION PERFORMANCE ANALYSIS,0.19929660023446658,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.20046893317702227,"100%
0.1"
CLASSIFICATION PERFORMANCE ANALYSIS,0.20164126611957797,"79.77 ¬± 0.04%
100%
12
1 (FedAvg)
77.71 ¬± 0.08%
50%
6
2 (FedLAMA)
79.07 ¬±0.1%
60.48% 0.4"
CLASSIFICATION PERFORMANCE ANALYSIS,0.20281359906213364,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.20398593200468934,"100%
0.5"
CLASSIFICATION PERFORMANCE ANALYSIS,0.205158264947245,"80.19 ¬± 0.05%
100%
12
1 (FedAvg)
77.40 ¬± 0.06%
50%
6
2 (FedLAMA)
78.88 ¬±0.05%
61.73%"
CLASSIFICATION PERFORMANCE ANALYSIS,0.2063305978898007,"Table 4 and 5 show the non-IID CIFAR-10 and CIFAR-100 experimental results. We use Dirichlet‚Äôs
distribution to generate heterogeneous data across all the clients. The detailed settings regarding
Dirichlet‚Äôs distribution can be found in Appendix. The base aggregation interval œÑ ‚Ä≤ is set to 6. The
interval increase factor œÜ is set to 2 for FedLAMA. Likely to the IID data experiments, we observe
that the periodic full averaging signiÔ¨Åcantly loses the accuracy as the model aggregation interval
increases, while it has a proportionally reduced communication cost. For both datasets, FedLAMA
achieves a comparable accuracy to the periodic full averaging with the interval of œÑ ‚Ä≤ while having
the communication cost that is close to the periodic full averaging with the increased interval of
œÜœÑ ‚Ä≤. Especially, FedLAMA works effectively even when the Dirichlet‚Äôs coefÔ¨Åcient is set to 0.1.
The coefÔ¨Åcient of 0.1 represents an extremely high degree of data heterogeneity in terms of not
only the number of samples per client but also the balance of the classes assigned to each client.
These results imply that FedLAMA is a practical algorithm for Federated Learning applications
with highly heterogeneous data distributions."
CLASSIFICATION PERFORMANCE ANALYSIS,0.20750293083235638,Under review as a conference paper at ICLR 2022
CLASSIFICATION PERFORMANCE ANALYSIS,0.20867526377491208,"1
3
5
7
9
11 13 15 17 19 21 23
0 500 1000 1500"
CLASSIFICATION PERFORMANCE ANALYSIS,0.20984759671746775,"1
2
3
4
0 50 100 150 200"
CLASSIFICATION PERFORMANCE ANALYSIS,0.21101992966002345,"1
3
5
7
9 11 13 15 17 19 21 23 25 27 29
0 500 1000"
CLASSIFICATION PERFORMANCE ANALYSIS,0.21219226260257915,"a) CIFAR-10 (ResNet20)
b) CIFAR-100 (WRN28-10)
c) FEMNIST (CNN)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.21336459554513482,"layer ID
layer ID
layer ID"
CLASSIFICATION PERFORMANCE ANALYSIS,0.21453692848769051,# of communications
CLASSIFICATION PERFORMANCE ANALYSIS,0.21570926143024619,"FedAvg
FedLAMA"
CLASSIFICATION PERFORMANCE ANALYSIS,0.21688159437280188,"Figure 2: The number of communications at the individual layers. The communications are counted
during the whole training (non-IID data)."
CLASSIFICATION PERFORMANCE ANALYSIS,0.21805392731535755,"1
3
5
7
9 11 13 15 17 19 21 23 25 27 29
0.00E+000"
CLASSIFICATION PERFORMANCE ANALYSIS,0.21922626025791325,1.00E+009
CLASSIFICATION PERFORMANCE ANALYSIS,0.22039859320046892,2.00E+009
CLASSIFICATION PERFORMANCE ANALYSIS,0.22157092614302462,3.00E+009
CLASSIFICATION PERFORMANCE ANALYSIS,0.2227432590855803,4.00E+009
CLASSIFICATION PERFORMANCE ANALYSIS,0.223915592028136,"1
3
5
7
9
11 13 15 17 19 21 23
0.00E+000"
CLASSIFICATION PERFORMANCE ANALYSIS,0.2250879249706917,2.00E+007
CLASSIFICATION PERFORMANCE ANALYSIS,0.22626025791324736,4.00E+007
CLASSIFICATION PERFORMANCE ANALYSIS,0.22743259085580306,6.00E+007
CLASSIFICATION PERFORMANCE ANALYSIS,0.22860492379835873,"1
2
3
4
0.00E+000"
CLASSIFICATION PERFORMANCE ANALYSIS,0.22977725674091443,5.00E+008
CLASSIFICATION PERFORMANCE ANALYSIS,0.2309495896834701,1.00E+009
CLASSIFICATION PERFORMANCE ANALYSIS,0.2321219226260258,1.50E+009
CLASSIFICATION PERFORMANCE ANALYSIS,0.23329425556858147,"a) CIFAR-10 (ResNet20)
b) CIFAR-100 (WRN28-10)
c) FEMNIST (CNN)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.23446658851113716,"layer ID
layer ID
layer ID"
CLASSIFICATION PERFORMANCE ANALYSIS,0.23563892145369286,total data size
CLASSIFICATION PERFORMANCE ANALYSIS,0.23681125439624853,"FedAvg
FedLAMA"
CLASSIFICATION PERFORMANCE ANALYSIS,0.23798358733880423,"Figure 3:
The total data size (communication cost) that correspond to Figure 2. The data size
comparison clearly shows where the performance gain of FedLAMA comes from."
COMMUNICATION EFFICIENCY ANALYSIS,0.2391559202813599,"6.2
COMMUNICATION EFFICIENCY ANALYSIS"
COMMUNICATION EFFICIENCY ANALYSIS,0.2403282532239156,"We analyze the total number of communications and the accumulated data size to evaluate the com-
munication efÔ¨Åciency of FedLAMA. Figure 2 shows the total number of communications at the
individual layers. The œÑ ‚Ä≤ is set to 6 and œÜ is 2 for FedLAMA. The key insight is that FedLAMA
increases the aggregation interval mostly at the output-side large layers. This means the dl value
shown in Equation (2) at the these layers are smaller than the others. Since these large layers take
up most of the total model parameters, the communication cost is remarkably reduced when their
aggregation intervals are increased. Figure 3 shows the layer-wise local data size shown in Equation
9. FedLAMA shows the signiÔ¨Åcantly smaller total data size than FedAvg. The extra computational
cost of FedLAMA is almost negligible since it calculates dl after each communication round only.
Therefore, given the virtually same computational cost, FedLAMA aggregates the local models at a
cheaper communication cost, and thus it improves the scalablity of Federated Learning."
COMMUNICATION EFFICIENCY ANALYSIS,0.24150058616647127,"We found that the amount of the reduced communication cost was not strongly affected by the
degree of data heterogeneity. As shown in Table 4 and 5, the reduced communication cost is similar
across different Dirichlet‚Äôs coefÔ¨Åcients and device participation ratios. That is, FedLAMA can be
considered as an effective model aggregation scheme regardless of the degree of data heterogeneity."
CONCLUSION,0.24267291910902697,"7
CONCLUSION"
CONCLUSION,0.24384525205158264,"We proposed a layer-wise model aggregation scheme that adaptively adjusts the model aggregation
interval at run-time. Breaking the convention of aggregating the whole model parameters at once,
this novel model aggregation scheme introduces a Ô¨Çexible communication strategy for scalable Fed-
erated Learning. Furthermore, we provide a solid convergence guarantee of FedLAMA under the
assumptions on the non-convex objective functions and the non-IID data distribution. Our empirical
study also demonstrates the efÔ¨Åcacy of FedLAMA for scalable and accurate Federated Learning."
CONCLUSION,0.24501758499413834,Under review as a conference paper at ICLR 2022
CONCLUSION,0.246189917936694,"Harmonizing FedLAMA with other advanced optimizers, gradient compression, and low-rank ap-
proximation methods is a promising future work."
CONCLUSION,0.2473622508792497,Under review as a conference paper at ICLR 2022
CODE OF ETHICS,0.2485345838218054,"8
CODE OF ETHICS"
CODE OF ETHICS,0.24970691676436108,"Our work does not deliver potentially harmful insights or conÔ¨Çicts of interests. We also do not Ô¨Ånd
any potential inappropriate application or privacy/security issues. The datasets we used in our study
are all public benchmark datasets, and our source code will be opened once the paper is accepted."
REPRODUCIBILITY STATEMENT,0.2508792497069168,"9
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.25205158264947247,"The software versions, implementation details, hyper-parameter settings can be found in the Ô¨Årst
two paragraphs of Section 6. The entire source code used in our experiments will be published as
an open source once the paper is accepted. We believe one can exactly reproduce our experimental
results following the provided descriptions."
REFERENCES,0.2532239155920281,REFERENCES
REFERENCES,0.2543962485345838,"Alyazeed Albasyoni, Mher Safaryan, Laurent Condat, and Peter Richt¬¥arik. Optimal gradient com-
pression for distributed and federated learning. arXiv preprint arXiv:2010.03246, 2020."
REFERENCES,0.2555685814771395,"Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
Qsgd:
Communication-efÔ¨Åcient sgd via gradient quantization and encoding. Advances in Neural In-
formation Processing Systems, 30:1709‚Äì1720, 2017."
REFERENCES,0.2567409144196952,"Dan Alistarh, Torsten HoeÔ¨Çer, Mikael Johansson, Sarit Khirirat, Nikola Konstantinov, and C¬¥edric
Renggli. The convergence of sparsiÔ¨Åed gradient methods. arXiv preprint arXiv:1809.10505,
2018."
REFERENCES,0.25791324736225085,"Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Freezeout: Accelerate training
by progressively freezing layers. arXiv preprint arXiv:1706.04983, 2017."
REFERENCES,0.25908558030480655,"Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub KoneÀácn`y, H Brendan McMa-
han, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv
preprint arXiv:1812.01097, 2018."
REFERENCES,0.26025791324736225,"Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
2921‚Äì2926. IEEE, 2017."
REFERENCES,0.26143024618991795,"Enmao Diao, Jie Ding, and Vahid Tarokh. HeteroÔ¨Ç: Computation and communication efÔ¨Åcient
federated learning for heterogeneous clients. arXiv preprint arXiv:2010.01264, 2020."
REFERENCES,0.26260257913247365,"Kelam Goutam, S Balasubramanian, Darshan Gera, and R Raghunatha Sarma. Layerout: Freezing
layers in deep neural networks. SN Computer Science, 1(5):1‚Äì9, 2020."
REFERENCES,0.2637749120750293,"Priya Goyal, Piotr Doll¬¥ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017."
REFERENCES,0.264947245017585,"Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch¬®olkopf. Measuring statistical de-
pendence with hilbert-schmidt norms. In International conference on algorithmic learning theory,
pp. 63‚Äì77. Springer, 2005."
REFERENCES,0.2661195779601407,"Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck R Cadambe. Lo-
cal sgd with periodic averaging: Tighter analysis and adaptive synchronization. arXiv preprint
arXiv:1910.13598, 2019."
REFERENCES,0.2672919109026964,"Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Pra-
neeth Vepakomma, Abhishek Singh, Hang Qiu, et al. Fedml: A research library and benchmark
for federated machine learning. arXiv preprint arXiv:2007.13518, 2020."
REFERENCES,0.268464243845252,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770‚Äì778, 2016."
REFERENCES,0.2696365767878077,Under review as a conference paper at ICLR 2022
REFERENCES,0.2708089097303634,"Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132‚Äì5143. PMLR, 2020."
REFERENCES,0.2719812426729191,"Jakub KoneÀácn`y, H Brendan McMahan, Felix X Yu, Peter Richt¬¥arik, Ananda Theertha Suresh, and
Dave Bacon.
Federated learning: Strategies for improving communication efÔ¨Åciency.
arXiv
preprint arXiv:1610.05492, 2016."
REFERENCES,0.2731535756154748,"Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited. In International Conference on Machine Learning, pp. 3519‚Äì
3529. PMLR, 2019."
REFERENCES,0.27432590855803046,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.27549824150058616,"Adarsh Kumar, Arjun Balasubramanian, Shivaram Venkataraman, and Aditya Akella. Accelerat-
ing deep learning inference via freezing. In 11th {USENIX} Workshop on Hot Topics in Cloud
Computing (HotCloud 19), 2019."
REFERENCES,0.27667057444314186,"Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018."
REFERENCES,0.27784290738569756,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efÔ¨Åcient learning of deep networks from decentralized data. In ArtiÔ¨Åcial intelli-
gence and statistics, pp. 1273‚Äì1282. PMLR, 2017."
REFERENCES,0.2790152403282532,"Ari S Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural
networks with canonical correlation. arXiv preprint arXiv:1806.05759, 2018."
REFERENCES,0.2801875732708089,"Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector
canonical correlation analysis for deep learning dynamics and interpretability. arXiv preprint
arXiv:1706.05806, 2017."
REFERENCES,0.2813599062133646,"Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani.
Fedpaq: A communication-efÔ¨Åcient federated learning method with periodic averaging and quan-
tization. In International Conference on ArtiÔ¨Åcial Intelligence and Statistics, pp. 2021‚Äì2031.
PMLR, 2020."
REFERENCES,0.2825322391559203,"Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. Practical low-rank communication com-
pression in decentralized deep learning. In NeurIPS, 2020."
REFERENCES,0.283704572098476,"Hongyi Wang, Scott Sievert, Zachary Charles, Shengchao Liu, Stephen Wright, and Dimitris Pa-
pailiopoulos. Atomo: Communication-efÔ¨Åcient learning via atomic sparsiÔ¨Åcation. arXiv preprint
arXiv:1806.04090, 2018."
REFERENCES,0.28487690504103164,"Hongyi Wang, Saurabh Agarwal, and Dimitris Papailiopoulos. PufferÔ¨Åsh: Communication-efÔ¨Åcient
models at no extra cost. arXiv preprint arXiv:2103.03936, 2021."
REFERENCES,0.28604923798358733,"Jianyu Wang and Gauri Joshi. Adaptive communication strategies to achieve the best error-runtime
trade-off in local-update sgd. arXiv preprint arXiv:1810.08313, 2018."
REFERENCES,0.28722157092614303,"Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective in-
consistency problem in heterogeneous federated optimization. arXiv preprint arXiv:2007.07481,
2020."
REFERENCES,0.28839390386869873,"Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsiÔ¨Åcation for communication-
efÔ¨Åcient distributed optimization. arXiv preprint arXiv:1710.09854, 2017."
REFERENCES,0.2895662368112544,"Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
Tern-
grad: Ternary gradients to reduce communication in distributed deep learning. arXiv preprint
arXiv:1705.07878, 2017."
REFERENCES,0.29073856975381007,"Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
Large batch optimization for deep
learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019."
REFERENCES,0.29191090269636577,Under review as a conference paper at ICLR 2022
REFERENCES,0.29308323563892147,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
arXiv:1605.07146, 2016."
REFERENCES,0.29425556858147717,"Minjia Zhang and Yuxiong He. Accelerating training of transformer-based language models with
progressive layer dropping. arXiv preprint arXiv:2010.13369, 2020."
REFERENCES,0.2954279015240328,Under review as a conference paper at ICLR 2022
REFERENCES,0.2966002344665885,"A
APPENDIX"
REFERENCES,0.2977725674091442,"A.1
CONVERGENCE ANALYSIS"
REFERENCES,0.2989449003516999,"Herein, we provide the proofs of the lemmas and theorem shown in Section 5."
REFERENCES,0.30011723329425555,"A.1.1
PRELIMINARIES"
REFERENCES,0.30128956623681125,"FedLAMA periodically chooses a few layers that will be less frequently synchronized. We call these
layers Least Critical Layers (LCL) for short."
REFERENCES,0.30246189917936694,"Notations ‚Äì All vectors in this paper are column vectors. x ‚ààRd denotes the parameters of one
local model and m is the number of workers. The stochastic gradient computed from a single
training data point Œæ is denoted by g(x, Œæ). For convenience, we use g(x) instead. The full batch
gradient is denoted by ‚àáF(x). We use ‚à•¬∑ ‚à•and ‚à•¬∑ ‚à•op to denote l2 norm and matrix operator norm,
respectively."
REFERENCES,0.30363423212192264,"Objective Function ‚Äì In this paper, we consider federated optimization problems as follows."
REFERENCES,0.3048065650644783,"min
x‚ààRd """
REFERENCES,0.305978898007034,"F(x) := m
X"
REFERENCES,0.3071512309495897,"i=1
piFi(x) #"
REFERENCES,0.3083235638921454,",
(10)"
REFERENCES,0.3094958968347011,"where pi = ni/n is the ratio of local data to the total dataset, and Fi(x) =
1
ni
P"
REFERENCES,0.3106682297772567,"Œæ‚ààD fi(x, Œæ) is the
local objective function of client i. n is the global dataset size and ni is the local dataset size. Note
that, by deÔ¨Ånition, Pm
i=1 pi = 1."
REFERENCES,0.3118405627198124,"Averaging Matrix ‚Äì We deÔ¨Åne a time-varying averaging matrix Wk ‚ààRdm√ódm as follows. Wk = Ô£±
Ô£≤ Ô£≥"
REFERENCES,0.3130128956623681,"P,
if k mod œÑmin is 0
J,
if k mod œÑmax is 0
I,
otherwise
(11)"
REFERENCES,0.3141852286049238,"I is an identity matrix, P is also a time-varying averaging matrix, and J is a full averaging matrix.
First, P1
i is a d √ó d diagonal matrix that has 1 for the diagonal elements that correspond to the LCL
parameters and pi for all the other diagonal elements. Likewise, P0
i is another d√ód diagonal matrix
that has 0 for the diagonal elements that correspond to the LCL parameters and pi for all the other
diagonal elements. Then, P is deÔ¨Åned as follows."
REFERENCES,0.31535756154747946,"P =
P1, for m diagonal blocks
P0, for all the other blocks
(12)"
REFERENCES,0.31652989449003516,"The ith block column of P consists of P1
i and P0
i following the above deÔ¨Ånition."
REFERENCES,0.31770222743259086,"Here we present an example of P where m = 2 and d = 2. In this example, p0 = 1/3 and p1 = 2/3.
Saying the LCL is the second parameter, P is deÔ¨Åned as follows."
REFERENCES,0.31887456037514655,"P1
0 =
 1"
REFERENCES,0.32004689331770225,"3
0
0
1"
REFERENCES,0.3212192262602579,"
, P0
0 =
 1"
REFERENCES,0.3223915592028136,"3
0
0
0"
REFERENCES,0.3235638921453693,"
, P1
1 =
 2"
REFERENCES,0.324736225087925,"3
0
0
1"
REFERENCES,0.32590855803048063,"
, P0
1 =
 2"
REFERENCES,0.32708089097303633,"3
0
0
0"
REFERENCES,0.32825322391559203,"
(13)"
REFERENCES,0.32942555685814773,"P =

P1
0
P0
1
P0
0
P1
1 
= Ô£Æ Ô£ØÔ£∞"
REFERENCES,0.3305978898007034,"1
3
0
2
3
0
0
1
0
0
1
3
0
2
3
0
0
0
0
1 Ô£π"
REFERENCES,0.33177022274325907,"Ô£∫Ô£ª.
(14)"
REFERENCES,0.33294255568581477,"The full-averaging matrix J is deÔ¨Åned as follows. First, Ji is a d √ó d diagonal matrix that has pi for
the diagonal elements. Then, J consists of m √ó m blocks of Ji such that each column block is m of
Ji blocks. Here we present an example of J where m = 2 and d = 2 as follows."
REFERENCES,0.33411488862837047,"J0 =
 1"
REFERENCES,0.33528722157092616,"3
0
0
1
3"
REFERENCES,0.3364595545134818,"
, J1 =
 2"
REFERENCES,0.3376318874560375,"3
0
0
2
3"
REFERENCES,0.3388042203985932,"
(15)"
REFERENCES,0.3399765533411489,Under review as a conference paper at ICLR 2022
REFERENCES,0.34114888628370454,"J =

J0
J1
J0
J1 
= Ô£Æ Ô£ØÔ£ØÔ£∞"
REFERENCES,0.34232121922626024,"1
3
0
2
3
0
0
1
3
0
2
3
1
3
0
2
3
0
0
1
3
0
2
3 Ô£π"
REFERENCES,0.34349355216881594,"Ô£∫Ô£∫Ô£ª.
(16)"
REFERENCES,0.34466588511137164,The averaging matrix P and J have the following properties:
REFERENCES,0.34583821805392734,"1. P1dm = 1dm, J1dm = 1dm."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.347010550996483,"2. The product of any two averaging matrices consists only of diagonal block matrices because
all the blocks in P and J are diagonal."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3481828839390387,3. PJ = JP = J regardless of which layers are chosen as the LCL.
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3493552168815944,4. PP = P regardless of which layers are chosen as the LCL.
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3505275498241501,"Vectorization ‚Äì We deÔ¨Åne a vectorized form of m local model parameters xk ‚ààRdm, its stochastic
gradients gk ‚ààRdm, and the full gradients fk ‚ààRdm as follows"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3516998827667057,"xk = vec

x1
k, x2
k, ¬∑ ¬∑ ¬∑ , xm
k"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3528722157092614,"gk = vec

g1(x1
k), g2(x2
k), ¬∑ ¬∑ ¬∑ , gm(xm
k )"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3540445486518171,"fk = vec

‚àáF1(x1
k), ‚àáF2(x2
k), ¬∑ ¬∑ ¬∑ , ‚àáFm(xm
k )
	
. (17)"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3552168815943728,"The full model aggregation can be written using the vectorized form of local models xk and the
averaging matrix J as follows. Jxk = Ô£Æ Ô£ØÔ£ØÔ£∞"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3563892145369285,"1
3
0
2
3
0
0
1
3
0
2
3
1
3
0
2
3
0
0
1
3
0
2
3 Ô£π Ô£∫Ô£∫Ô£ª Ô£Æ Ô£ØÔ£ØÔ£ØÔ£∞"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.35756154747948415,"x(1,1)
k
x(1,2)
k
x(2,1)
k
x(2,2)
k Ô£π Ô£∫Ô£∫Ô£∫Ô£ª= Ô£Æ Ô£ØÔ£ØÔ£ØÔ£∞"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.35873388042203985,"(x(1,1)
k
+ 2x(2,1)
k
)/3
(x(1,2)
k
+ 2x(2,2)
k
)/3
(x(1,1)
k
+ 2x(2,1)
k
)/3
(x(1,2)
k
+ 2x(2,2)
k
)/3 Ô£π"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.35990621336459555,"Ô£∫Ô£∫Ô£∫Ô£ª
(18)"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.36107854630715125,"where x(i,j)
k
is the jth model parameter of local model i at iteration k."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3622508792497069,"We also deÔ¨Åne the following additional vectorized forms of the weighted model parameters and
gradients for convenience."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3634232121922626,"ÀÜxk = vec
‚àöp1x1
k, ‚àöp2x2
k, ¬∑ ¬∑ ¬∑ , ‚àöpmxm
k"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3645955451348183,"ÀÜgk = vec
‚àöp1g1(x1
k), ‚àöp2g2(x2
k), ¬∑ ¬∑ ¬∑ , ‚àöpmgm(xm
k )"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.365767878077374,"ÀÜfk = vec
‚àöp1‚àáF1(x1
k), ‚àöp2‚àáF2(x2
k), ¬∑ ¬∑ ¬∑ , ‚àöpm‚àáFm(xm
k )
	
(19)"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3669402110199297,Assumptions ‚Äì We analyze the convergence rate of FedLAMA under the following assumptions.
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.36811254396248533,"1. (Smoothness). Each local objective function is L-smooth, that is, ‚à•‚àáFi(x) ‚àí‚àáFi(y)‚à•‚â§
L‚à•x ‚àíy‚à•, ‚àÄi ‚àà{1, ¬∑ ¬∑ ¬∑ , m}."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.369284876905041,"2. (Unbiased Gradient). The stochastic gradient at each client is an unbiased estimator of the
local full-batch gradient: EŒæ[gi(x, Œæ)] = ‚àáFi(x)."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3704572098475967,"3. (Bounded Variance).
The stochastic gradient at each client has bounded variance:
EŒæ[‚à•gi(x, Œæ) ‚àí‚àáFi(x)‚à•2 ‚â§œÉ2], ‚àÄi ‚àà{1, ¬∑ ¬∑ ¬∑ , m}, œÉ2 ‚â•0."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3716295427901524,"4. (Bounded Dissimilarity). For any sets of weights {pi ‚â•0}m
i=1, Pm
i=1 pi = 1, there exist
constants Œ≤2 ‚â•1 and Œ∫2 ‚â•0 such that Pm
i=1 pi‚à•‚àáFi(x)‚à•2 ‚â§Œ≤2‚à•Pm
i=1 pi‚àáFi(x)‚à•2+Œ∫2.
If local objective functions are identical to each other, Œ≤2 = 1 and Œ∫2 = 0."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.37280187573270807,Under review as a conference paper at ICLR 2022
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.37397420867526376,"A.1.2
PROOFS"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.37514654161781946,"Theorem 5.1.
Suppose all m local models are initialized to the same point u1.
Under As-
sumption 1
‚àº
4, if FedLAMA runs for K iterations and the learning rate satisÔ¨Åes Œ∑
‚â§"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.37631887456037516,"min

1
2(œÑmax‚àí1)L,
1
L‚àö"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.37749120750293086,2œÑmax(œÑmax‚àí1)(2Œ≤2+1)
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3786635404454865,"
, FedLAMA ensures E ""
1
K K
X"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3798358733880422,"i=1
‚à•‚àáF(uk)‚à•2
#"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3810082063305979,"‚â§
4
Œ∑K (E [F(u1) ‚àíF(u‚àó)]) + 4Œ∑ m
X"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3821805392731536,"i=1
p2
i LœÉ2"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.38335287221570924,"+ 3Œ∑2(œÑmax ‚àí1)L2œÉ2 + 6Œ∑2œÑmax(œÑmax ‚àí1)L2Œ∫2,
(20)"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.38452520515826494,where u‚àóindicates a local minimum.
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.38569753810082064,"Proof. Based on Lemma 5.1 and 5.2, we have"
K,0.38686987104337633,"1
K K
X"
K,0.388042203985932,"k=1
E
h
‚à•‚àáF(uk)‚à•2i
‚â§
2
Œ∑K (E [F(u1) ‚àíF(u‚àó)]) + 2Œ∑ m
X"
K,0.3892145369284877,"i=1
p2
i LœÉ2"
K,0.3903868698710434,"+ L2
 
Œ∑2(œÑmax ‚àí1)œÉ2
j
1 ‚àíA
+
AŒ≤2"
K,0.39155920281359907,"KL2(1 ‚àíA) K
X"
K,0.39273153575615477,"k=1
E
h
‚à•‚àáF(uk)‚à•2i
+
AŒ∫2"
K,0.3939038686987104,L2(1 ‚àíA) ! .
K,0.3950762016412661,"After re-writing the left-hand side and a minor rearrangement, we have"
K,0.3962485345838218,"1
K K
X"
K,0.3974208675263775,"k=1
E
h
‚à•‚àáF(uk)‚à•2i
‚â§
2
Œ∑K (E [F(u1) ‚àíF(u‚àó)]) + 2Œ∑ m
X"
K,0.39859320046893315,"i=1
p2
i LœÉ2 + 1 K K
X k=1 AŒ≤2"
K,0.39976553341148885,"1 ‚àíA E
h
‚à•‚àáF(uk)‚à•2i"
K,0.40093786635404455,"+ L2
Œ∑2(œÑmax ‚àí1)œÉ2"
K,0.40211019929660025,"1 ‚àíA
+
AŒ∫2"
K,0.40328253223915594,"L2(1 ‚àíA) 
."
K,0.4044548651817116,"By moving the third term on the right-hand side to the left-hand side, we have"
K,0.4056271981242673,"1
K K
X k=1"
K,0.406799531066823,"
1 ‚àíAŒ≤2 1 ‚àíA "
K,0.4079718640093787,"E
h
‚à•‚àájF(uk)‚à•2i
‚â§
2
Œ∑K (E [F(u1) ‚àíF(u‚àó)]) + 2Œ∑ m
X"
K,0.4091441969519343,"i=1
p2
i LœÉ2"
K,0.41031652989449,"+ L2
Œ∑2(œÑmax ‚àí1)œÉ2"
K,0.4114888628370457,"1 ‚àíA
+
AŒ∫2"
K,0.4126611957796014,L2(1 ‚àíA)
K,0.4138335287221571,"
.
(21)"
K,0.41500586166471276,"If A ‚â§
1
2Œ≤2+1, then AŒ≤2"
K,0.41617819460726846,1‚àíA ‚â§1
K,0.41735052754982416,"2. Therefore, (21) can be simpliÔ¨Åed as follows."
K,0.41852286049237986,"1
K K
X"
K,0.4196951934349355,"k=1
E
h
‚à•‚àáF(uk)‚à•2i
‚â§
4
Œ∑K (E [F(u1) ‚àíF(u‚àó)]) + 4Œ∑ m
X"
K,0.4208675263774912,"i=1
p2
i LœÉ2
(22)"
K,0.4220398593200469,"+ 2L2
Œ∑2(œÑmax ‚àí1)œÉ2 1 ‚àíA"
K,0.4232121922626026,"
+ 2 AŒ∫2 1 ‚àíA."
K,0.4243845252051583,"The learning rate condition A ‚â§
1
2Œ≤2+1 also ensures that
1
1‚àíA ‚â§1 +
1
2Œ≤2 . Based on Assumption 4,
1
2Œ≤2 ‚â§2"
K,0.42555685814771393,"3, and thus
1
1‚àíA ‚â§2"
K,0.42672919109026963,"3. Therefore, we have"
K,0.42790152403282533,"1
K K
X"
K,0.42907385697538103,"k=1
E
h
‚à•‚àáF(uk)‚à•2i
‚â§
4
Œ∑K (E [F(u1) ‚àíF(u‚àó)]) + 4Œ∑ m
X"
K,0.43024618991793667,"i=1
p2
i LœÉ2"
K,0.43141852286049237,+ 3Œ∑2(œÑmax ‚àí1)L2œÉ2 + 6Œ∑2œÑmax(œÑmax ‚àí1)L2Œ∫2.
K,0.43259085580304807,We complete the proof.
K,0.43376318874560377,Under review as a conference paper at ICLR 2022
K,0.4349355216881594,"Learning Rate Constraints ‚Äì In Theorem 5.3, we have two learning rate constraints, one from (22)
and the other from (51) as follows."
K,0.4361078546307151,"A <
1
2Œ≤2 + 1
from (22)"
K,0.4372801875732708,"A < 1
from (51)
After a minor rearrangement, we have a uniÔ¨Åed learning rate constraint as follows."
K,0.4384525205158265,Œ∑ ‚â§min
K,0.4396248534583822,"(
1
2(œÑmax ‚àí1)L,
1 L
p"
K,0.44079718640093785,2œÑmax(œÑmax ‚àí1)(2Œ≤2 + 1) )
K,0.44196951934349354,"Lemma 5.1. (Framework) Under Assumption 1 ‚àº3, if the learning rate satisÔ¨Åes Œ∑ ‚â§
1
2L, FedLAMA
ensures
1
K K
X"
K,0.44314185228604924,"k=1
E
h
‚à•‚àáF(uk)‚à•2i
‚â§
2
Œ∑K E [F(u1) ‚àíF(u‚àó)] + 2Œ∑LœÉ2
m
X"
K,0.44431418522860494,"i=1
(pi)2 + L2 K K
X k=1 m
X"
K,0.4454865181711606,"i=1
pi E
huk ‚àíxi
k
2i
. (23)"
K,0.4466588511137163,"Proof. Based on Assumption 1, we have"
K,0.447831184056272,"E [F(uk+1) ‚àíF(uk)] ‚â§‚àíŒ∑E """
K,0.4490035169988277,"‚ü®‚àáF(uk), m
X"
K,0.4501758499413834,"i=1
pigi(xi
k)‚ü© #"
K,0.451348182883939,"|
{z
}
T1 + Œ∑2L"
E,0.4525205158264947,"2 E Ô£Æ Ô£∞  m
X"
E,0.4536928487690504,"i=1
pigi(xi
k)  2Ô£π Ô£ª"
E,0.4548651817116061,"|
{z
}
T2 (24)"
E,0.45603751465416176,"First, T1 can be rewritten as follows."
E,0.45720984759671746,"T1 = E """
E,0.45838218053927315,"‚ü®‚àáF(uk), m
X"
E,0.45955451348182885,"i=1
pi
 
gi(xi
k) ‚àí‚àáFi(xi
k)

‚ü© # + E """
E,0.46072684642438455,"‚ü®‚àáF(uk), m
X"
E,0.4618991793669402,"i=1
pi‚àáFi(xi
k)‚ü© # = E """
E,0.4630715123094959,"‚ü®‚àáF(uk), m
X"
E,0.4642438452520516,"i=1
pi‚àáFi(xi
k)‚ü© # = 1"
E,0.4654161781946073,2 ‚à•‚àáF(uk)‚à•2 + 1
E,0.46658851113716293,"2 E Ô£Æ Ô£∞  m
X"
E,0.46776084407971863,"i=1
pi‚àáFi(xi
k)  2Ô£π Ô£ª‚àí1"
E,0.46893317702227433,2 E Ô£Æ Ô£∞
E,0.47010550996483,"‚àáF(uk) ‚àí m
X"
E,0.4712778429073857,"i=1
pi‚àáFi(xi
k)  2Ô£π Ô£ª,"
E,0.47245017584994137,"(25)
where the last equality holds based on a basic equality: 2a‚ä§b = ‚à•a‚à•2 + ‚à•b‚à•2 ‚àí‚à•a ‚àíb‚à•2 ."
E,0.47362250879249707,"Then, T2 can be bounded as follows."
E,0.47479484173505276,"T2 = E Ô£Æ Ô£∞  m
X"
E,0.47596717467760846,"i=1
pi
 
gi(xi
k) ‚àíE

gi(xi
k)

+ m
X"
E,0.4771395076201641,"i=1
pi E

gi(xi
k)
 2Ô£π Ô£ª = E Ô£Æ Ô£∞  m
X"
E,0.4783118405627198,"i=1
pi
 
gi(xi
k) ‚àí‚àáFi(xi
k)

+ m
X"
E,0.4794841735052755,"i=1
pi‚àáFi(xi
k)  2Ô£π Ô£ª ‚â§2 E Ô£Æ Ô£∞  m
X"
E,0.4806565064478312,"i=1
pi
 
gi(xi
k) ‚àí‚àáFi(xi
k)
 2Ô£π"
E,0.48182883939038684,"Ô£ª+ 2 E Ô£Æ Ô£∞  m
X"
E,0.48300117233294254,"i=1
pi‚àáFi(xi
k)  2Ô£π Ô£ª = 2 m
X"
E,0.48417350527549824,"i=1
p2
i E
hgi(xi
k) ‚àí‚àáFi(xi
k)
2i
+ 2 E Ô£Æ Ô£∞  m
X"
E,0.48534583821805394,"i=1
pi‚àáFi(xi
k)  2Ô£π Ô£ª"
E,0.48651817116060964,"‚â§2œÉ2
m
X"
E,0.4876905041031653,"i=1
p2
i + 2 E Ô£Æ Ô£∞  m
X"
E,0.488862837045721,"i=1
pi‚àáFi(xi
k)  2Ô£π"
E,0.4900351699882767,"Ô£ª,
(26)"
E,0.4912075029308324,Under review as a conference paper at ICLR 2022
E,0.492379835873388,"where the last equality holds because gi(xi
k) ‚àí‚àáFi(xi
k) has 0 mean and is independent across i,
and the last inequality follows Assumption 3."
E,0.4935521688159437,"By plugging in (25) and (26) into (24), we have the following."
E,0.4947245017584994,E [F(uk+1) ‚àíF(uk)] ‚â§‚àíŒ∑
E,0.4958968347010551,2 ‚à•‚àáF(uk)‚à•2 ‚àíŒ∑
E,0.4970691676436108,"2 E Ô£Æ Ô£∞  m
X"
E,0.49824150058616645,"i=1
pi‚àáFi(xi
k)  2Ô£π Ô£ª + Œ∑"
E,0.49941383352872215,2 E Ô£Æ Ô£∞
E,0.5005861664712778,"‚àáF(uk) ‚àí m
X"
E,0.5017584994138335,"i=1
pi‚àáFi(xi
k)  2Ô£π"
E,0.5029308323563892,"Ô£ª+ Œ∑2LœÉ2
m
X"
E,0.5041031652989449,"i=1
p2
i"
E,0.5052754982415005,"+ Œ∑2L E Ô£Æ Ô£∞  m
X"
E,0.5064478311840562,"i=1
pi‚àáFi(xi
k)  2Ô£π Ô£ª = ‚àíŒ∑"
E,0.5076201641266119,2 ‚à•‚àáF(uk)‚à•2 ‚àíŒ∑
E,0.5087924970691676,"2(1 ‚àí2Œ∑L) E Ô£Æ Ô£∞  m
X"
E,0.5099648300117233,"i=1
pi‚àáFi(xi
k)  2Ô£π Ô£ª + Œ∑"
E,0.511137162954279,2 E Ô£Æ Ô£∞
E,0.5123094958968347,"‚àáF(uk) ‚àí m
X"
E,0.5134818288393904,"i=1
pi‚àáFi(xi
k)  2Ô£π"
E,0.5146541617819461,"Ô£ª+ Œ∑2LœÉ2
m
X"
E,0.5158264947245017,"i=1
p2
i"
E,0.5169988276670574,"If Œ∑ ‚â§
1
2L, it follows"
E,0.5181711606096131,"E [F(uk+1) ‚àíF(uk)] Œ∑
‚â§‚àí1"
E,0.5193434935521688,"2 ‚à•‚àáF(uk)‚à•2 + Œ∑LœÉ2
m
X"
E,0.5205158264947245,"i=1
p2
i + 1"
E,0.5216881594372802,2 E Ô£Æ Ô£∞
E,0.5228604923798359,"‚àáF(uk) ‚àí m
X"
E,0.5240328253223916,"i=1
pi‚àáFi(xi
k)  2Ô£π Ô£ª ‚â§‚àí1"
E,0.5252051582649473,"2 ‚à•‚àáF(uk)‚à•2 + Œ∑LœÉ2
m
X"
E,0.5263774912075029,"i=1
p2
i
(27) + 1 2 m
X"
E,0.5275498241500586,"i=1
pi E
h‚àáFi(uk) ‚àí‚àáFi(xi
k)
2i ‚â§‚àí1"
E,0.5287221570926143,"2 ‚à•‚àáF(uk)‚à•2 + Œ∑LœÉ2
m
X"
E,0.52989449003517,"i=1
p2
i + L2 2 m
X"
E,0.5310668229777257,"i=1
pi E
huk ‚àíxi
k
2i
,"
E,0.5322391559202814,where (27) holds based on the convexity of ‚Ñì2 norm and Jensen‚Äôs inequality.
E,0.5334114888628371,"By taking expectation and averaging across K iterations, we have."
K,0.5345838218053928,"1
K K
X k=1"
K,0.5357561547479485,"E [F(uk+1) ‚àíF(uk)] Œ∑
‚â§‚àí1"
K,0.536928487690504,"2K K
X"
K,0.5381008206330598,"k=1
‚à•‚àáF(uk)‚à•2 + Œ∑LœÉ2
m
X"
K,0.5392731535756154,"i=1
p2
i + L2"
K,0.5404454865181711,"2K K
X k‚àí1 m
X"
K,0.5416178194607268,"i=1
pi E
huk ‚àíxi
k
2i
."
K,0.5427901524032825,Under review as a conference paper at ICLR 2022
K,0.5439624853458382,"After a minor rearrangement, we have a telescoping sum as follows."
K,0.5451348182883939,"1
K K
X"
K,0.5463071512309496,"k=1
E
h
‚à•‚àáF(uk)‚à•2i
‚â§
2
Œ∑K E [F(u1) ‚àíF(uk+1)] + 2Œ∑LœÉ2
m
X"
K,0.5474794841735052,"i=1
p2
i + L2 K K
X k=1 m
X"
K,0.5486518171160609,"i=1
pi E
huk ‚àíxi
k
2i"
K,0.5498241500586166,"‚â§
2
Œ∑K E [F(u1) ‚àíF(u‚àó)] + 2Œ∑LœÉ2
m
X"
K,0.5509964830011723,"i=1
p2
i + L2 K K
X k=1 m
X"
K,0.552168815943728,"i=1
pi E
huk ‚àíxi
k
2i
,"
K,0.5533411488862837,"where u‚àóindicates the local minimum. Here, we complete the proof."
K,0.5545134818288394,"Lemma 5.2. (Model Discrepancy) Under Assumption 1 ‚àº4, if the learning rate satisÔ¨Åes Œ∑ <
1
2(œÑmax‚àí1)L, FedLAMA ensures"
K,0.5556858147713951,"1
K K
X k=1 m
X"
K,0.5568581477139508,"i=1
pi E
huk ‚àíxi
k
2i
‚â§2Œ∑2(œÑmax ‚àí1)œÉ2"
K,0.5580304806565064,"1 ‚àíA
+
AŒ∫2"
K,0.5592028135990621,"L2(1 ‚àíA) +
AŒ≤2"
K,0.5603751465416178,"KL2(1 ‚àíA) K
X"
K,0.5615474794841735,"k=1
E
h
‚à•‚àáF(uk)‚à•2i
, (28)"
K,0.5627198124267292,where A = 4Œ∑2(œÑmax ‚àí1)2L2 and œÑmax is the largest averaging interval across all the layers.
K,0.5638921453692849,"Proof. We begin with rewriting the weighted average of the squared distance using the vectorized
form of the local models as follows. m
X"
K,0.5650644783118406,"i=1
pi
uk ‚àíxi
k
2 = m
X i=1"
K,0.5662368112543963,"‚àöpi
 
uk ‚àíxi
k
2"
K,0.567409144196952,"= ‚à•JÀÜxk ‚àíÀÜxk‚à•2
(29)"
K,0.5685814771395076,"= ‚à•(J ‚àíI)ÀÜxk‚à•2 ,"
K,0.5697538100820633,where (29) holds by the commutative property of multiplication.
K,0.570926143024619,"Then, according to the parameter update rule, we have"
K,0.5720984759671747,"(J ‚àíI)ÀÜxk = (J ‚àíI)Wk‚àí1(ÀÜxk‚àí1 ‚àíŒ∑ÀÜgk‚àí1)
= (J ‚àíI)Wk‚àí1ÀÜxk‚àí1 ‚àí(J ‚àíWk‚àí1)Œ∑ÀÜgk‚àí1,
(30)"
K,0.5732708089097304,"where (30) holds because JW = J based on the averaging matrix property 3, and IW = W."
K,0.5744431418522861,"Then, expanding the expression of xk‚àí1, we have"
K,0.5756154747948418,"(J ‚àíI)ÀÜxk = (J ‚àíI)Wk‚àí1(Wk‚àí2(ÀÜxk‚àí2 ‚àíŒ∑ÀÜgk‚àí2)) ‚àí(J ‚àíWk‚àí1)Œ∑ÀÜgk‚àí1
= (J ‚àíI)Wk‚àí1Wk‚àí2ÀÜxk‚àí2 ‚àí(J ‚àíWk‚àí1Wk‚àí2)Œ∑ÀÜgk‚àí2 ‚àí(J ‚àíWk‚àí1)Œ∑ÀÜgk‚àí1."
K,0.5767878077373975,"Repeating the same procedure for ÀÜxk‚àí2, ÀÜxk‚àí3, ¬∑ ¬∑ ¬∑ , ÀÜx2, we have"
K,0.5779601406799532,"(J ‚àíI)ÀÜxk = (J ‚àíI) k‚àí1
Y"
K,0.5791324736225087,"s=1
WsÀÜx1 ‚àíŒ∑ k‚àí1
X"
K,0.5803048065650644,"s=1
(J ‚àí k‚àí1
Y"
K,0.5814771395076201,"l=s
Wl)ÀÜgs = ‚àíŒ∑ k‚àí1
X"
K,0.5826494724501758,"s=1
(J ‚àí k‚àí1
Y"
K,0.5838218053927315,"l=s
Wl)ÀÜgs,
(31)"
K,0.5849941383352872,"where (31) holds because xi
1 is the same across all the workers and thus (J ‚àíI)ÀÜx1 = 0."
K,0.5861664712778429,Under review as a conference paper at ICLR 2022
K,0.5873388042203986,"Based on (31), we have"
K,0.5885111371629543,"1
K K
X k=1 m
X"
K,0.5896834701055099,"i=1
pi E
huk ‚àíxi
k
2i = 1 K K
X k=1"
K,0.5908558030480656,"
E
h
‚à•(J ‚àíI)ÀÜxk‚à•2i = 1 K K
X k=1 Ô£´ Ô£≠Œ∑2 E Ô£Æ Ô£∞  k‚àí1
X"
K,0.5920281359906213,"s=1
(J ‚àí k‚àí1
Y"
K,0.593200468933177,"l=s
Wl)ÀÜgs  2Ô£π Ô£ª Ô£∂ Ô£∏ = 1 K K
X k=1 Ô£´ Ô£≠Œ∑2 E Ô£Æ Ô£∞  k‚àí1
X"
K,0.5943728018757327,"s=1
(J ‚àí k‚àí1
Y"
K,0.5955451348182884,"l=s
Wl)(ÀÜgs ‚àíÀÜfs) + k‚àí1
X"
K,0.5967174677608441,"s=1
(J ‚àí k‚àí1
Y"
K,0.5978898007033998,"l=s
Wl)ÀÜfs  2Ô£π Ô£ª Ô£∂ Ô£∏ ‚â§2Œ∑2 K Ô£´"
K,0.5990621336459554,"Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠ K
X k=1
E Ô£Æ Ô£∞  k‚àí1
X"
K,0.6002344665885111,"s=1
(J ‚àí k‚àí1
Y"
K,0.6014067995310668,"l=s
Wl)(ÀÜgs ‚àíÀÜfs)  2Ô£π Ô£ª"
K,0.6025791324736225,"|
{z
}
T3 + K
X k=1
E Ô£Æ Ô£∞  k‚àí1
X"
K,0.6037514654161782,"s=1
(J ‚àí k‚àí1
Y"
K,0.6049237983587339,"l=s
Wl)ÀÜfs  2Ô£π Ô£ª"
K,0.6060961313012896,"|
{z
}
T4 Ô£∂"
K,0.6072684642438453,"Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏ (32)"
K,0.608440797186401,"where (32) holds based on the convexity of ‚Ñì2 norm and Jensen‚Äôs inequality. Now, we focus on
bounding T3 and T4, separately."
K,0.6096131301289566,"Bounding T3 K
X k=1
E Ô£Æ Ô£∞  k‚àí1
X"
K,0.6107854630715123,"s=1
(J ‚àí k‚àí1
Y"
K,0.611957796014068,"l=s
Wl)(ÀÜgs ‚àíÀÜfs)  2Ô£π Ô£ª = K
X k=1 k‚àí1
X s=1
E Ô£Æ Ô£∞ (J ‚àí k‚àí1
Y"
K,0.6131301289566237,"l=s
Wl)(ÀÜgs ‚àíÀÜfs)  2Ô£π"
K,0.6143024618991794,"Ô£ª
(33) ‚â§ K
X k=1 k‚àí1
X s=1
E Ô£Æ"
K,0.6154747948417351,"Ô£∞
(ÀÜgs ‚àíÀÜfs)

2
(J ‚àí k‚àí1
Y"
K,0.6166471277842908,"l=s
Wl)  2 op Ô£π"
K,0.6178194607268465,"Ô£ª,
(34)"
K,0.6189917936694022,"where (33) holds because ÀÜgs ‚àíÀÜfs has 0 mean and independent across s, and (34) holds based on
Lemma A.1."
K,0.6201641266119577,Under review as a conference paper at ICLR 2022
K,0.6213364595545134,"Without loss of generality, we replace k with aœÑmax + b, where a is the communication round index
and b is the iteration index within each communication round. Then, we have"
K,0.6225087924970691,"K/œÑmax‚àí1
X a=0"
K,0.6236811254396248,"œÑmax
X b=1"
K,0.6248534583821805,"aœÑmax+b‚àí1
X s=1
E Ô£Æ"
K,0.6260257913247362,"Ô£∞
(ÀÜgs ‚àíÀÜfs)

2
(J ‚àí k‚àí1
Y"
K,0.6271981242672919,"l=s
Wl)  2 op Ô£π Ô£ª ="
K,0.6283704572098476,"K/œÑmax‚àí1
X a=0"
K,0.6295427901524033,"œÑmax
X b=1 aœÑ
X s=1
E Ô£Æ"
K,0.6307151230949589,"Ô£∞
(ÀÜgs ‚àíÀÜfs)

2
(J ‚àí"
K,0.6318874560375146,"aœÑmax+b‚àí1
Y"
K,0.6330597889800703,"l=s
Wl)  2 op Ô£π Ô£ª +"
K,0.634232121922626,"K/œÑmax‚àí1
X a=0"
K,0.6354044548651817,"œÑmax
X b=1"
K,0.6365767878077374,"aœÑmax+b‚àí1
X"
K,0.6377491207502931,"s=aœÑmax+1
E Ô£Æ"
K,0.6389214536928488,"Ô£∞
(ÀÜgs ‚àíÀÜfs)

2
(J ‚àí"
K,0.6400937866354045,"aœÑmax+b‚àí1
Y"
K,0.6412661195779601,"l=s
Wl)  2 op Ô£π Ô£ª ="
K,0.6424384525205158,"K/œÑmax‚àí1
X a=0"
K,0.6436107854630715,"œÑmax
X b=1"
K,0.6447831184056272,"aœÑmax+b‚àí1
X"
K,0.6459554513481829,"s=aœÑmax+1
E Ô£Æ"
K,0.6471277842907386,"Ô£∞
(ÀÜgs ‚àíÀÜfs)

2
(J ‚àí"
K,0.6483001172332943,"aœÑmax+b‚àí1
Y"
K,0.64947245017585,"l=s
Wl)  2 op Ô£π"
K,0.6506447831184057,"Ô£ª
(35) ="
K,0.6518171160609613,"K/œÑmax‚àí1
X a=0"
K,0.652989449003517,"œÑmax
X b=1"
K,0.6541617819460727,"aœÑ+b‚àí1
X"
K,0.6553341148886284,"s=aœÑmax+1
E"
K,0.6565064478311841,"(ÀÜgs ‚àíÀÜfs)

2
(36) ="
K,0.6576787807737398,"K/œÑmax‚àí1
X a=0"
K,0.6588511137162955,"œÑmax
X b=1"
K,0.6600234466588512,"aœÑmax+b‚àí1
X"
K,0.6611957796014069,"s=aœÑmax+1 m
X"
K,0.6623681125439624,"i=1
pi E
h(gi(xi
s) ‚àí‚àáFi(xi
s))
2i ‚â§"
K,0.6635404454865181,"K/œÑmax‚àí1
X a=0"
K,0.6647127784290738,"œÑmax
X b=1"
K,0.6658851113716295,"aœÑmax+b‚àí1
X"
K,0.6670574443141852,"s=aœÑmax+1 m
X"
K,0.6682297772567409,"i=1
piœÉ2
(37) ="
K,0.6694021101992966,"K/œÑmax‚àí1
X a=0"
K,0.6705744431418523,"œÑmax
X"
K,0.671746776084408,"b=1
(b ‚àí1)œÉ2 ="
K,0.6729191090269636,"K/œÑmax‚àí1
X a=0"
K,0.6740914419695193,"œÑmax(œÑmax ‚àí1) 2
œÉ2"
K,0.675263774912075,‚â§K (œÑmax ‚àí1)
K,0.6764361078546307,"2
œÉ2.
(38)"
K,0.6776084407971864,"Remember FedLAMA synchronizes the whole parameters at least once after every œÑmax iterations.
Thus, (35) holds because QaœÑmax+b‚àí1
l=s
Wl is J when s ‚â§aœÑmax, and thus J ‚àíQaœÑmax+b‚àí1
l=s
Wl
becomes 0. (36) holds based on Lemma A.2. (37) holds based on Assumption 3."
K,0.6787807737397421,Bounding T4
K,0.6799531066822978,Under review as a conference paper at ICLR 2022
K,0.6811254396248535,"K‚àíœÑmax
X k=1
E Ô£Æ Ô£∞  k‚àí1
X"
K,0.6822977725674091,"s=1
(J ‚àí k‚àí1
Y"
K,0.6834701055099648,"l=s
Wl)ÀÜfs  2Ô£π Ô£ª ="
K,0.6846424384525205,"K/œÑmax‚àí1
X a=0"
K,0.6858147713950762,"œÑmax
X b=1
E Ô£Æ Ô£∞ "
K,0.6869871043376319,"aœÑ+b‚àí1
X"
K,0.6881594372801876,"s=1
(J ‚àí"
K,0.6893317702227433,"aœÑmax+b‚àí1
Y"
K,0.690504103165299,"l=s
Wl)ÀÜfs  2Ô£π Ô£ª ="
K,0.6916764361078547,"K/œÑmax‚àí1
X a=0"
K,0.6928487690504103,"œÑmax
X b=1
E Ô£Æ Ô£∞ "
K,0.694021101992966,"aœÑmax+b‚àí1
X"
K,0.6951934349355217,"s=aœÑmax+1
(J ‚àí"
K,0.6963657678780774,"aœÑmax+b‚àí1
Y"
K,0.6975381008206331,"l=s
Pl)ÀÜfs  2Ô£π"
K,0.6987104337631888,"Ô£ª
(39) ‚â§"
K,0.6998827667057445,"K/œÑmax‚àí1
X a=0"
K,0.7010550996483002,"œÑmax
X b=1 Ô£´"
K,0.7022274325908558,Ô£≠(b ‚àí1)
K,0.7033997655334114,"aœÑmax+b‚àí1
X"
K,0.7045720984759671,"s=aœÑmax+1
E Ô£Æ Ô£∞ (J ‚àí"
K,0.7057444314185228,"aœÑmax+b‚àí1
Y"
K,0.7069167643610785,"l=s
Pl)ÀÜfs  2Ô£π Ô£ª Ô£∂"
K,0.7080890973036342,"Ô£∏
(40) ‚â§"
K,0.7092614302461899,"K/œÑmax‚àí1
X a=0"
K,0.7104337631887456,"œÑmax
X b=1 Ô£´"
K,0.7116060961313013,Ô£≠(b ‚àí1)
K,0.712778429073857,"aœÑmax+b‚àí1
X"
K,0.7139507620164126,"s=aœÑmax+1
E Ô£Æ"
K,0.7151230949589683,"Ô£∞
ÀÜfs

2
(J ‚àí"
K,0.716295427901524,"aœÑmax+b‚àí1
Y"
K,0.7174677608440797,"l=s
Pl)  2 op Ô£π Ô£ª Ô£∂"
K,0.7186400937866354,"Ô£∏
(41) ‚â§"
K,0.7198124267291911,"K/œÑmax‚àí1
X a=0"
K,0.7209847596717468,"œÑmax
X b=1 "
K,0.7221570926143025,(b ‚àí1)
K,0.7233294255568582,"aœÑmax+b‚àí1
X"
K,0.7245017584994138,"s=aœÑmax+1
E"
K,0.7256740914419695,"ÀÜfs

2! (42)"
K,0.7268464243845252,‚â§œÑmax(œÑmax ‚àí1) 2
K,0.7280187573270809,"K/œÑmax‚àí1
X a=0"
K,0.7291910902696366,"aœÑmax+œÑmax‚àí1
X"
K,0.7303634232121923,"s=aœÑmax+1
E"
K,0.731535756154748,"ÀÜfs

2!"
K,0.7327080890973037,"‚â§œÑmax(œÑmax ‚àí1) 2 K
X k=1
E"
K,0.7338804220398594,"ÀÜfk

2"
K,0.735052754982415,"= œÑmax(œÑmax ‚àí1) 2 K
X k=1 m
X"
K,0.7362250879249707,"i=1
pi E
h‚àáFi(xi
k)
2i
,
(43)"
K,0.7373974208675264,"where (39) holds because J ‚àíQaœÑmax+b‚àí1
l=s
Pl becomes 0 when s ‚â§aœÑ. (40) holds based on the
convexity of ‚Ñì2 norm and Jensen‚Äôs inequality. (41) holds based on Lemma A.1. (42) holds based on
Lemma A.2."
K,0.738569753810082,Final Result
K,0.7397420867526378,"By plugging in (38) and (43) into (32), we have"
K,0.7409144196951934,"1
K K
X k=1 m
X"
K,0.7420867526377491,"i=1
pi E
huk ‚àíxi
k
2i ‚â§2Œ∑2 K "
K,0.7432590855803048,K (œÑmax ‚àí1)
K,0.7444314185228605,"2
œÉ2 + œÑmax(œÑmax ‚àí1) 2 K
X k=1 m
X"
K,0.7456037514654161,"i=1
pi E
h‚àáFi(xi
k)
2i!!"
K,0.7467760844079718,"= Œ∑2(œÑmax ‚àí1)œÉ2 + Œ∑2œÑmax(œÑmax ‚àí1) K K
X k=1 m
X"
K,0.7479484173505275,"i=1
pi E
h‚àáFi(xi
k)
2i! (44)"
K,0.7491207502930832,The local gradient term on the right-hand side in (44) can be rewritten using the following inequality.
K,0.7502930832356389,"E
h‚àáFi(xi
k)
2i
= E
h‚àáFi(xi
k) ‚àí‚àáFi(uk) + ‚àáFi(uk)
2i"
K,0.7514654161781946,"‚â§2 E
h‚àáFi(xi
k) ‚àí‚àáFi(uk)
2i
+ 2 E
h
‚à•‚àáFi(uk)‚à•2i
(45)"
K,0.7526377491207503,"‚â§2L2 E
huk ‚àíxi
k
2i
+ 2 E
h
‚à•‚àáFi(uk)‚à•2i
,
(46)"
K,0.753810082063306,Under review as a conference paper at ICLR 2022
K,0.7549824150058617,where (45) holds based on the convexity of ‚Ñì2 norm and Jensen‚Äôs inequality.
K,0.7561547479484173,"Plugging in (46) into (44), we have"
K,0.757327080890973,"1
K K
X k=1 m
X"
K,0.7584994138335287,"i=1
pi E
huk ‚àíxi
k
2i"
K,0.7596717467760844,"‚â§Œ∑2(œÑmax ‚àí1)œÉ2 + 2Œ∑2œÑmax(œÑmax ‚àí1)L2 K K
X k=1 m
X"
K,0.7608440797186401,"i=1
pi E
huk ‚àíxi
k
2i"
K,0.7620164126611958,"+ 2Œ∑2œÑmax(œÑmax ‚àí1) K K
X k=1 m
X"
K,0.7631887456037515,"i=1
pi E
h
‚à•‚àáFi(uk)‚à•2i
(47)"
K,0.7643610785463072,"After a minor rearranging, we have"
K,0.7655334114888629,"1
K K
X k=1 m
X"
K,0.7667057444314185,"i=1
pi E
huk ‚àíxi
k
2i
‚â§
Œ∑2(œÑmax ‚àí1)œÉ2"
K,0.7678780773739742,1 ‚àí2Œ∑2œÑmax(œÑmax ‚àí1)L2
K,0.7690504103165299,"+
2Œ∑2œÑmax(œÑmax ‚àí1)
K(1 ‚àí2Œ∑2œÑmax(œÑmax ‚àí1)L2) K
X k=1 m
X"
K,0.7702227432590856,"i=1
pi E
h
‚à•‚àáFi(uk)‚à•2i (48)"
K,0.7713950762016413,Let us deÔ¨Åne A = 2Œ∑2œÑmax(œÑmax ‚àí1)L2. Then (48) is simpliÔ¨Åed as follows.
K,0.772567409144197,"1
K K
X k=1 m
X"
K,0.7737397420867527,"i=1
pi E
huk ‚àíxi
k
2i"
K,0.7749120750293084,‚â§Œ∑2(œÑmax ‚àí1)œÉ2
K,0.776084407971864,"1 ‚àíA
+
A
KL2(1 ‚àíA) K
X k=1 m
X"
K,0.7772567409144197,"i=1
pi E
h
‚à•‚àáFi(uk)‚à•2i"
K,0.7784290738569754,"Based on Assumption 4, we have"
K,0.779601406799531,"1
K K
X k=1 m
X"
K,0.7807737397420867,"i=1
pi E
huk ‚àíxi
k
2i"
K,0.7819460726846424,‚â§Œ∑2(œÑmax ‚àí1)œÉ2
K,0.7831184056271981,"1 ‚àíA
+
AŒ≤2"
K,0.7842907385697538,"KL2(1 ‚àíA) K
X k=1
E Ô£Æ Ô£∞  m
X"
K,0.7854630715123095,"i=1
pi‚àáFi(uk)  2Ô£π"
K,0.7866354044548651,"Ô£ª+
AŒ∫2"
K,0.7878077373974208,"L2(1 ‚àíA)
(49)"
K,0.7889800703399765,= Œ∑2(œÑmax ‚àí1)œÉ2
K,0.7901524032825322,"1 ‚àíA
+
AŒ≤2"
K,0.7913247362250879,"KL2(1 ‚àíA) K
X"
K,0.7924970691676436,"k=1
E
h
‚à•‚àáF(uk)‚à•2i
+
AŒ∫2"
K,0.7936694021101993,"L2(1 ‚àíA),
(50)"
K,0.794841735052755,where (50) holds based on the deÔ¨Ånition of the objective function (10).
K,0.7960140679953107,"Note that (49) is true only when 1 ‚àíA > 0. Thus, after a minor rearrangement, we have a learning
rate constraint as follows."
K,0.7971864009378663,"Œ∑ <
1
2(œÑmax ‚àí1)L
(51)"
K,0.798358733880422,"Here, we complete the proof."
K,0.7995310668229777,"A.1.3
PROOF OF OTHER LEMMAS"
K,0.8007033997655334,"Lemma A.1. Consider a real matrix A ‚ààRmdj√ómdj and a real vector b ‚ààRmdj. If b Ã∏= 0mdj,
we have
‚à•Ab‚à•‚â§‚à•A‚à•op‚à•b‚à•
(52)"
K,0.8018757327080891,Under review as a conference paper at ICLR 2022
K,0.8030480656506448,Proof.
K,0.8042203985932005,‚à•Ab‚à•2 = ‚à•Ab‚à•2
K,0.8053927315357562,‚à•b‚à•2 ‚à•b‚à•2
K,0.8065650644783119,"‚â§‚à•A‚à•2
op‚à•b‚à•2
(53)"
K,0.8077373974208675,where (53) holds based on the deÔ¨Ånition of operator norm.
K,0.8089097303634232,"Lemma A.2. Suppose an md √ó md averaging matrix P and the full-averaging matrix J, then"
K,0.8100820633059789,"‚à•J ‚àíP‚à•2
op = 1.
(54)"
K,0.8112543962485346,regardless of which layers are chosen as the LCL.
K,0.8124267291910903,"Proof. First, by the deÔ¨Ånition of averaging matrix P, all the columns that do not correspond to the
LCL are zeroed out in J ‚àíP. Then, based on the averaging matrix property 1 and 2, the remaining
columns in P has 1 at all different rows. By the deÔ¨Ånition of J, all the non-zero elements in ith
column are the same pi, i ‚àà{1, ¬∑ ¬∑ ¬∑ , m}. Consequently, the remaining columns in J ‚àíP are
always orthogonal regardless of which layers are chosen as the LCL, and thus the eigenvalues of
J ‚àíP are either 1 or ‚àí1. Finally, by the deÔ¨Ånition of the matrix operator norm, ‚à•J ‚àíP‚à•2
op =
max{|Œª(J ‚àíP)|} = 1, where Œª(¬∑) indicates the eigenvalues of the input matrix."
K,0.813599062133646,Under review as a conference paper at ICLR 2022
K,0.8147713950762017,"0
100
200
300
0.2 0.4 0.6 0.8 1.0"
K,0.8159437280187574,Validation accuracy (%) Epoch
K,0.8171160609613131,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8182883939038686,"0
100
200
300
0.2 0.4 0.6 0.8 1.0"
K,0.8194607268464243,Validation accuracy (%) Epoch
K,0.82063305978898,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.8218053927315357,"0
100
200
300 1.0 1.5 2.0 2.5"
K,0.8229777256740914,Training loss (softmax) Epoch
K,0.8241500586166471,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.8253223915592028,"0
100
200
300 1.0 1.5 2.0 2.5"
K,0.8264947245017585,Training loss (softmax) Epoch
K,0.8276670574443142,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8288393903868698,"0
2000
4000
6000
0.2 0.4 0.6 0.8 1.0"
K,0.8300117233294255,Validation accuracy (%)
K,0.8311840562719812,Iteration
K,0.8323563892145369,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8335287221570926,"0
2000
4000
6000
0.2 0.4 0.6 0.8 1.0"
K,0.8347010550996483,Validation accuracy (%)
K,0.835873388042204,Iteration
K,0.8370457209847597,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.8382180539273154,"0
2000
4000
6000 1.0 1.5 2.0 2.5"
K,0.839390386869871,Training loss (softmax)
K,0.8405627198124267,Iteration
K,0.8417350527549824,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.8429073856975381,"0
2000
4000
6000 1.0 1.5 2.0 2.5"
K,0.8440797186400938,Training loss (softmax)
K,0.8452520515826495,Iteration
K,0.8464243845252052,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8475967174677609,a) CIFAR-10 IID setting learning curves
K,0.8487690504103166,b) CIFAR-10 non-IID setting learning curves
K,0.8499413833528722,"Figure 4: The learning curves of CIFAR-10 (ResNet20) training (128 clients). a): The curves for
IID data distribution. b): The curves for non-IID data distribution (Œ± = 0.1). FedAvg (x) indicates
FedAvg with the interval of x. FedLAMA (x, y) indicates FedLAMA with the base interval of x
and the interval increase factor of y. As the aggregation interval increases, FedAvg rapidly loses the
convergence speed, and it results in achieving a lower validation accuracy within the Ô¨Åxed iteration
budget. In contrast, FedLAMA effectively increases the aggregation interval while maintaining the
convergence speed."
K,0.8511137162954279,"0
100
200 1 2 3 4"
K,0.8522860492379836,Training loss (softmax) Epoch
K,0.8534583821805393,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.854630715123095,"0
100
200 1 2 3 4"
K,0.8558030480656507,Training loss (softmax) Epoch
K,0.8569753810082064,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8581477139507621,"0
100
200 0.2 0.4 0.6 0.8"
K,0.8593200468933178,Validation accuracy (%) Epoch
K,0.8604923798358733,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.861664712778429,"0
100
200 0.2 0.4 0.6 0.8"
K,0.8628370457209847,Validation accuracy (%) Epoch
K,0.8640093786635404,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8651817116060961,"0
2000
4000
6000
0.2 0.4 0.6 0.8"
K,0.8663540445486518,Validation accuracy (%)
K,0.8675263774912075,Iteration
K,0.8686987104337632,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8698710433763188,"0
2000
4000
6000
0.2 0.4 0.6 0.8"
K,0.8710433763188745,Validation accuracy (%)
K,0.8722157092614302,Iteration
K,0.8733880422039859,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.8745603751465416,"0
2000
4000
6000 1 2 3 4"
K,0.8757327080890973,Training loss (softmax)
K,0.876905041031653,Iteration
K,0.8780773739742087,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.8792497069167644,"0
2000
4000
6000 1 2 3 4"
K,0.88042203985932,Training loss (softmax)
K,0.8815943728018757,Iteration
K,0.8827667057444314,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8839390386869871,a) CIFAR-100 IID setting learning curves
K,0.8851113716295428,b) CIFAR-100 non-IID setting learning curves
K,0.8862837045720985,"Figure 5:
The learning curves of CIFAR-100 (WideResNet28-10) training (128 clients). a): The
curves for IID data distribution. b): The curves for non-IID data distribution (Œ± = 0.1). FedAvg
(x) indicates FedAvg with the interval of x. FedLAMA (x, y) indicates FedLAMA with the base
interval of x and the interval increase factor of y. While FedAvg signiÔ¨Åcantly loses the convergence
speed as the aggregation interval increases, FedLAMA has a marginl impact on it which results in a
higher validation accuracy."
K,0.8874560375146542,"A.2
ADDITIONAL EXPERIMENTAL RESULTS"
K,0.8886283704572099,"In this section, we provide extra experimental results with extensive hyper-parameter settings. We
commonly use 128 clients and a local batch size of 32 in all the experiments. The gradual learning
rate warmup (Goyal et al. (2017)) is also applied to the Ô¨Årst 10 epochs in all the experiments. Overall,
the learning curve charts and the validation accuracy tables deliver the key insight that FedLAMA
achieves a comparable convergence speed to the periodic full aggregation with the base interval"
K,0.8898007033997656,Under review as a conference paper at ICLR 2022
K,0.8909730363423212,"0
500
1000
1500
2000
0.2 0.4 0.6 0.8 1.0"
K,0.8921453692848769,Validation accuracy (%)
K,0.8933177022274326,Iteration
K,0.8944900351699883,"FedAvg (10)
 FedLAMA (10, 2)
 FedLAMA (10, 4)"
K,0.895662368112544,"0
500
1000
1500
2000 0.2 0.4 0.6 0.8 1.0"
K,0.8968347010550997,Validation accuracy (%)
K,0.8980070339976554,Iteration
K,0.8991793669402111,"FedAvg (10)
 FedAvg (20)
 FedAvg (40)"
K,0.9003516998827668,"0
500
1000
1500
2000 1 2 3 4"
K,0.9015240328253223,Training loss (softmax)
K,0.902696365767878,Iteration
K,0.9038686987104337,"FedAvg (10)
 FedAvg (20)
 FedAvg (40)"
K,0.9050410316529894,"0
500
1000
1500
2000 1 2 3 4"
K,0.9062133645955451,Training loss (softmax)
K,0.9073856975381008,Iteration
K,0.9085580304806565,"FedAvg (10)
 FedLAMA (10, 2)
 FedLAMA (10, 4)"
K,0.9097303634232122,"Figure 6:
The learning curves of FEMNIST (CNN) training. FedAvg (x) indicates FedAvg with
the interval of x. FedLAMA (x, y) indicates FedLAMA with the base interval of x and the interval
increase factor of y. FedLAMA curves are not strongly affected by the increased aggregation interval
while FedAvg signiÔ¨Åcantly loses the convergence speed as well as the validation accuracy."
K,0.9109026963657679,"(œÑ‚Äô) while having the communication cost that is similar to the periodic full aggregation with the
increased interval (œÜœÑ ‚Ä≤)."
K,0.9120750293083235,"ArtiÔ¨Åcial Data Heterogeneity ‚Äì For CIFAR-10 and CIFAR-100, we artiÔ¨Åcially generate the het-
erogeneous data distribution using Dirichlet‚Äôs distribution. The concentration coefÔ¨Åcient Œ± is set to
0.1, 0.5, and 1.0 to evaluate the performance of FedLAMA across a variety of degree of data hetero-
geneity. Note that the small concentration coefÔ¨Åcient represents the highly heterogeneous numbers
of local samples across clients as well as the balance of the samples across the labels. We used the
data distribution source code provided by FedML (He et al. (2020))."
K,0.9132473622508792,"CIFAR-10 ‚Äì Figure 4 shows the full learning curves for IID and non-IID CIFAR-10 datasets. The
hyper-parameter settings correspond to Table 4 and 1. First, as the aggregation interval increases
from 6 to 24, FedAvg suffers from the slower convergence, and it results in achieving a lower val-
idation accuracy, regardless of the data distribution. In contrast, FedLAMA learning curves are
marginally affected by the increased aggregation interval. Table 6 and 7 show the CIFAR-10 classi-
Ô¨Åcation performance of FedLAMA across different œÜ settings. As expected, the accuracy is reduced
as œÜ increases. The IID and non-IID data settings show the common trend. Depending on the system
network bandwidth, œÜ can be tuned to be an appropriate value. When œÜ = 2, the accuracy is almost
the same as or even slightly higher than FedAvg accuracy. If the network bandwidth is limited, one
can increase œÜ and slightly increase the epoch budget to achieve a good accuracy. Table 8 shows the
CIFAR-10 accuracy across different œÑ ‚Ä≤ settings. We see that the accuracy is signiÔ¨Åcantly dropped as
œÑ ‚Ä≤ increases."
K,0.9144196951934349,"CIFAR-100 ‚Äì Figure 5 shows the learning curves for IID and non-IID CIFAR-100 datasets. Likely
to CIFAR-10 results, FedAvg learning curves are strongly affected as the aggregation interval in-
creases from 6 to 24 while FedLAMA learning curves are not strongly affected. Table 9 and 10
show the CIFAR-100 classiÔ¨Åcation performance of FedLAMA across different œÜ settings. Fed-
LAMA achieves a comparable accuracy to FedAvg with a short aggregation interval, even when
the degree of data heterogeneity is extreamly high (25% device sampling and Direchlet‚Äôs coefÔ¨Åcient
of 0.1). Table 11 shows the FedAvg accuracy with different œÑ ‚Ä≤ settings. Under the strongly het-
erogeneous data distributions, FedAvg with a large aggregation interval (œÑ ‚â•12) do not achieve a
reasonable accuracy."
K,0.9155920281359906,"FEMNIST ‚Äì Figure 6 shows the learning curves of CNN training. Likely to the previous two
datasets, the periodic full aggregation suffers from the slower convergence as the aggregation in-
terval increases. FedLAMA learning curves are not much affected by the increased aggregation
interval, and it results in achieving a higher validation accuracy after the same number of iterations.
Table 12 shows the FEMNIST classiÔ¨Åcation performance of FedLAMA across different œÜ settings.
FedLAMA achieves a similar accuracy to the baseline (FedAvg with œÑ ‚Ä≤ = 10) even when using a
large interval increase factor œÜ ‚â•4. These results demonstrate the effectiveness of the proposed
layer-wise adaptive model aggregation method on the problems with heterogeneous data distribu-
tions."
K,0.9167643610785463,Under review as a conference paper at ICLR 2022
K,0.917936694021102,Table 6: (IID data) CIFAR-10 classiÔ¨Åcation results of FedLAMA with different œÜ settings.
K,0.9191090269636577,"# of clients
Local batch size
LR
Averaging interval: œÑ ‚Ä≤
Interval increase factor: œÜ
Validation acc."
K,0.9202813599062134,"128
32 0.8 6"
K,0.9214536928487691,"1 (FedAvg)
88.37 ¬± 0.1% 0.5"
K,0.9226260257913247,"2
88.41 ¬± 0.04%
4
86.33 ¬± 0.2%
8
85.08 ¬± 0.04%"
K,0.9237983587338804,Table 7: (Non-IID data) CIFAR-10 classiÔ¨Åcation results of FedLAMA with different œÜ settings.
K,0.9249706916764361,"# of clients
Local batch size
LR
œÑ ‚Ä≤
Active ratio
Dirichlet coeff.
œÜ
Validation acc."
K,0.9261430246189918,"128
32 0.8 6"
K,0.9273153575615475,"100%
1"
K,0.9284876905041032,"1 (FedAvg)
90.79 ¬± 0.1%
2
89.01 ¬± 0.04%
4
87.84 ¬± 0.01%"
K,0.9296600234466589,"100%
0.5"
K,0.9308323563892146,"1 (FedAvg)
90.53 ¬± 0.18%
2
89.21 ¬± 0.2%
4
86.68 ¬± 0.12%"
K,0.9320046893317703,"100%
0.1"
K,0.9331770222743259,"1 (FedAvg)
89.52 ¬± 0.11%
2
89.00 ¬± 0.1%
4
84.82 ¬± 0.08% 50%
1"
K,0.9343493552168816,"1 (FedAvg)
90.34 ¬± 0.12%
2
89.56 ¬± 0.13%
4
87.48 ¬± 0.21%"
K,0.9355216881594373,"50%
0.5"
K,0.936694021101993,"1 (FedAvg)
89.86 ¬± 0.13%
2
88.44 ¬± 0.15%
4
87.29 ¬± 0.18%"
K,0.9378663540445487,"50%
0.1"
K,0.9390386869871044,"1 (FedAvg)
87.83 ¬± 0.2%
2
87.40 ¬± 0.17%
4
85.92 ¬± 0.21% 0.6 25%
1"
K,0.94021101992966,"1 (FedAvg)
88.97 ¬± 0.03%
2
87.89 ¬± 0.2%
4
86.61 ¬± 0.1%"
K,0.9413833528722158,"25%
0.5"
K,0.9425556858147714,"1 (FedAvg)
87.59 ¬± 0.05%
2
87.12 ¬± 0.08%
4
86.57 ¬± 0.02%"
K,0.943728018757327,"0.3
25%
0.1"
K,0.9449003516998827,"1 (FedAvg)
84.02 ¬± 0.04%
2
83.55 ¬± 0.02%
4
83.06 ¬± 0.03%"
K,0.9460726846424384,Table 8: (Non-IID data) CIFAR-10 classiÔ¨Åcation results of FedAvg with different œÑ ‚Ä≤ settings.
K,0.9472450175849941,"# of clients
Local batch size
LR
œÑ ‚Ä≤
Active ratio
Dirichlet coeff.
œÜ
Validation acc."
K,0.9484173505275498,"128
32
0.8 6"
K,0.9495896834701055,"100%
0.1"
K,0.9507620164126612,"1 (FedAvg)
89.52 ¬± 0.11%
12
1 (FedAvg)
87.29 ¬± 0.05%
24
1 (FedAvg)
84.82 ¬± 0.1%"
K,0.9519343493552169,"128
32
0.3 6"
K,0.9531066822977726,"25%
0.1"
K,0.9542790152403282,"1 (FedAvg)
84.02 ¬± 0.1%
12
1 (FedAvg)
82.48 ¬± 0.2%
24
1 (FedAvg)
76.72 ¬± 0.1%"
K,0.9554513481828839,Table 9: (IID data) CIFAR-100 classiÔ¨Åcation results of FedLAMA with different œÜ settings.
K,0.9566236811254396,"# of clients
Local batch size
LR
Averaging interval: œÑ ‚Ä≤
Interval increase factor: œÜ
Validation acc."
K,0.9577960140679953,"128
32
0.6
6"
K,0.958968347010551,"1 (FedAvg)
76.50 ¬± 0.02%
2
75.99 ¬± 0.03%
4
76.17 ¬± 0.2%
8
76.15 ¬± 0.2%"
K,0.9601406799531067,Under review as a conference paper at ICLR 2022
K,0.9613130128956624,Table 10: (Non-IID data) CIFAR-100 classiÔ¨Åcation results of FedLAMA with different œÜ settings.
K,0.9624853458382181,"# of clients
Local batch size
LR
œÑ ‚Ä≤
Active ratio
Dirichlet coeff.
œÜ
Validation acc."
K,0.9636576787807737,"128
32 0.4 6"
K,0.9648300117233294,"100%
1"
K,0.9660023446658851,"1 (FedAvg)
80.34 ¬± 0.01%
2
78.92 ¬± 0.01%
4
77.16 ¬± 0.05%"
K,0.9671746776084408,"100%
0.5"
K,0.9683470105509965,"1 (FedAvg)
80.19 ¬± 0.02%
2
78.88 ¬± 0.1%
4
78.03 ¬± 0.08%"
K,0.9695193434935522,"0.2
100%
0.1"
K,0.9706916764361079,"1 (FedAvg)
79.78 ¬± 0.02%
2
79.07 ¬± 0.02%
4
79.32 ¬± 0.01% 0.4 50%
1"
K,0.9718640093786636,"1 (FedAvg)
79.94 ¬± 0.1%
2
78.98 ¬± 0.01%
4
77.50 ¬± 0.02%"
K,0.9730363423212193,"50%
0.5"
K,0.9742086752637749,"1 (FedAvg)
79.95 ¬± 0.05%
2
78.37 ¬± 0.05%
4
76.93 ¬± 0.1%"
K,0.9753810082063306,"0.2
50%
0.1"
K,0.9765533411488863,"1 (FedAvg)
79.62 ¬± 0.06%
2
78.76 ¬± 0.02%
4
77.44 ¬± 0.02%"
K,0.977725674091442,"0.4
25%
1"
K,0.9788980070339977,"1 (FedAvg)
78.78 ¬± 0.02%
2
78.10 ¬± 0.02%
0.2
4
76.84 ¬± 0.03% 0.4"
K,0.9800703399765534,"25%
0.5"
K,0.981242672919109,"1 (FedAvg)
78.81 ¬± 0.01%
2
77.86 ¬± 0.04%
4
77.01 ¬± 0.1%"
K,0.9824150058616647,"25%
0.1"
K,0.9835873388042204,"1 (FedAvg)
79.06 ¬± 0.03%
2
78.63 ¬± 0.02%
0.2
4
77.17 ¬± 0.01%"
K,0.984759671746776,Table 11: (Non-IID data) CIFAR-100 classiÔ¨Åcation results of FedAvg with different œÑ ‚Ä≤ settings.
K,0.9859320046893317,"# of clients
Local batch size
LR
œÑ ‚Ä≤
Active ratio
Dirichlet coeff.
œÜ
Validation acc."
K,0.9871043376318874,"128
32
0.4 6"
K,0.9882766705744431,"100%
0.1"
K,0.9894490035169988,"1 (FedAvg)
79.78 ¬± 0.02%
12
1 (FedAvg)
77.71 ¬± 0.1%
24
1 (FedAvg)
69.63 ¬± 0.1%"
K,0.9906213364595545,"128
32
0.4 6"
K,0.9917936694021102,"25%
0.1"
K,0.9929660023446659,"1 (FedAvg)
79.06 ¬± 0.03%
12
1 (FedAvg)
76.16 ¬± 0.05%
24
1 (FedAvg)
67.43 ¬± 0.1%"
K,0.9941383352872216,"Table 12: FEMNIST classiÔ¨Åcation results of FedLAMA with different œÜ settings.
# of clients
Local batch size
LR
Averaging interval: œÑ ‚Ä≤
Active ratio
Interval increase factor: œÜ
Validation acc."
K,0.9953106682297772,"128
32
0.04
12 100%"
K,0.9964830011723329,"1 (FedAvg)
85.74 ¬± 0.21%
2
85.40 ¬± 0.13%
4
84.67 ¬± 0.1%
8
84.15 ¬± 0.18% 50%"
K,0.9976553341148886,"1 (FedAvg)
86.59 ¬± 0.2%
2
86.07 ¬± 0.1%
4
85.77 ¬± 0.15%
8
85.31 ¬± 0.03% 25%"
K,0.9988276670574443,"1 (FedAvg)
86.04 ¬± 0.2%
2
86.01 ¬± 0.1%
4
85.62 ¬± 0.08%
8
85.23 ¬± 0.1%"
