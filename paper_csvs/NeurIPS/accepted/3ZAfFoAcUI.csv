Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0031545741324921135,"Given the increasing scale of model sizes, efﬁcient training strategies like gradual
stacking [Gong et al., 2019, Reddi et al., 2023] have garnered interest. Stacking
enables efﬁcient training by gradually growing the depth of a model in stages
and using layers from a smaller model in an earlier stage to initialize the next
stage. Although efﬁcient for training, the model biases induced by such growing
approaches are largely unexplored. In this work, we examine this fundamental
aspect of gradual stacking, going beyond its efﬁciency beneﬁts. We propose a
variant of gradual stacking called MIDAS that can speed up language model train-
ing by up to 40%. Furthermore we discover an intriguing phenomenon: MIDAS
is not only training-efﬁcient but surprisingly also has an inductive bias towards
improving downstream tasks, especially tasks that require reasoning abilities like
reading comprehension and math problems, despite having similar or slightly worse
perplexity compared to baseline training. To further analyze this inductive bias, we
construct reasoning primitives – simple synthetic tasks that are building blocks for
reasoning – and ﬁnd that a model pretrained with stacking is signiﬁcantly better
than standard pretraining on these primitives, with and without ﬁne-tuning. This
provides stronger and more robust evidence for this inductive bias towards reason-
ing. These ﬁndings of training efﬁciency and inductive bias towards reasoning are
veriﬁed at 1B, 2B and 8B parameter language models. Finally, we conjecture the
underlying reason for this inductive bias by exploring the connection of stacking to
looped models and provide strong supporting empirical analysis."
INTRODUCTION,0.006309148264984227,"1
Introduction"
INTRODUCTION,0.00946372239747634,"With the advent of very large deep learning models, efﬁcient training to reduce the compute and
time requirements is becoming increasingly important. Along with efﬁcient optimization procedures,
there has been a surge in interest to design efﬁcient training strategies. One practical approach is
to use smaller models to initialize larger models. Usually, this results in much faster convergence
compared to vanilla training [Chen et al., 2022, 2016, Gong et al., 2019, Reddi et al., 2023, Wang
et al., 2023, Li et al., 2023, Kim et al., 2023, Yao et al., 2024, Wang et al., 2024]. Stacking and
growing based approaches have particularly gained traction recently. For instance, gradual stacking
[Reddi et al., 2023] is a prominent approach where in each stage the last few layers of the model"
INTRODUCTION,0.012618296529968454,∗Corresponding author
INTRODUCTION,0.015772870662460567,"Figure 1: (a) Pictorial depiction of gradual stacking and MIDAS. (b) Accuracy improvements (in
%) for model trained with MIDAS over baseline for various task groups, despite having the same
perplexity. For both 1B, 2B and 8B models, we see that improvements are mostly positive, and are
much larger for tasks that require a lot of reasoning."
INTRODUCTION,0.01892744479495268,"are stacked onto itself to initialize the model’s next stage, until the desired depth is reached. This
has been shown to signiﬁcantly speed up BERT pretraining and also has some theoretical justiﬁcation
for the efﬁciency aspect. While these methods can speed up training, such changes can also induce
speciﬁc biases into the model. However, the effect of stacking-based approaches on generalization
remains a fundamental open question and is largely unexplored."
INTRODUCTION,0.022082018927444796,"Modern deep learning models, when trained carefully, have been shown to exhibit interesting inductive
biases, and their success is partially attributed to them. Such biases can arise either from model
architecture, optimization techniques, or training strategies, and these biases come in various forms
including simplicity bias, ﬂatness of learned function, and sparsity. The implicit bias of optimizers,
in particular, has been subject to extensive research. For instance, the implicit bias of ﬁrst-order
methods like stochastic gradient descent has been studied extensively in overparametrized settings
[Gunasekar et al., 2018, Liu et al., 2023]. Similarly, the inductive biases of architecture components
like self-attention and convolution have also been studied [Edelman et al., 2022, Wang and Wu,
2023]. More recently, there has also been interest in constructs like looped models [Lan et al., 2020,
Dehghani et al., 2018] that share weights across layers. They have been shown to be powerful enough
to emulate programmable computers [Giannou et al., 2023] and have the inductive bias to simulate
iterative solutions [Yang et al., 2023], thereby yielding models with algorithmic abilities. However,
in this vein, very little is known about the implicit biases of newer training strategies (e.g., greedy
layerwise training or gradual stacking) that are gaining popularity."
INTRODUCTION,0.025236593059936908,"In this work, we investigate the inductive bias of stacking-based approaches beyond training
efﬁciency. We uncover an intriguing phenomenon — pretraining with a variant of stacking is not
only efﬁcient, but also has a desirable inductive bias towards improving downstream benchmarks.
First, through comprehensive empirical analysis, we discover a novel variant of gradual stacking
called MIDAS (MIDdle grAdual Stacking) which copies the middle block of layers of a small
network to initialize a larger network (see Figure 1). We demonstrate that MIDAS is more efﬁcient
in training compared to standard training and the previous leading stagewise training approach.
However, remarkably, it also yields signiﬁcantly better performance on many downstream reasoning
tasks. For instance, we see in Figure 1 that MIDAS has signiﬁcantly better performance on math
word problems and reasoning primitives. This performance boost should come as a surprise, since
MIDAS uses exactly the same data and fewer training FLOPS compared to standard training. In fact,
the pretraining perplexity of MIDAS on a validation set matches that of standard baseline training.
This strongly suggests that there is some inductive bias for MIDAS at play."
INTRODUCTION,0.028391167192429023,"In this paper, we formalize and provide strong evidence for such an ""inductive bias"" – MIDAS
achieves better downstream evaluations despite performing similarly in terms of pretraining validation
perplexity. Thus, the improved quality of MIDAS is not because of better generalization in the
pretraining objective, but rather due to its ability to extract more skills and abilities from the pretraining
process. This kind of inductive bias phenomenon was ﬁrst formalized in Saunshi et al. [2022] for
contrastive learning and later in Liu et al. [2023] for language modeling on synthetic data. However,
this is the ﬁrst evidence of a strong inductive bias for a training procedure in real language model"
INTRODUCTION,0.031545741324921134,"training. While our real-world benchmarks already provide strong evidence, in order to better
isolate the contributing factors, we construct simple synthetic tasks that are building blocks for
reasoning, called reasoning primitives. We ﬁnd that a model pretrained with MIDAS has much better
performance on the reasoning primitives than a model obtained through standard pretraining, as is
evident in Figure 1. In light of the above discussion, we state the main contributions of our paper."
INTRODUCTION,0.03470031545741325,"• We propose a novel variant of gradual stacking, called MIDAS, that achieves better training
efﬁciency than gradual stacking.
• Our investigation of the inductive bias in gradual stacking approaches, particularly with
MIDAS, reveals a surprising beneﬁt: beyond enabling efﬁcient training, it also enhances
performance on downstream tasks. This improvement is especially notable in tasks that rely
on context and reasoning abilities.
• We provide strong evidence of the aforementioned phenomenon on several datasets that
have previously been used to demonstrate reasoning capabilities.
• We construct simple synthetic tasks that are building blocks for reasoning and demonstrate
that MIDAS performs signiﬁcantly better than baseline training on these tasks. These
datasets may be of independent interest to the LLM reasoning community.
• Finally, we conjecture the reason behind improved reasoning capabilities of MIDAS by
presenting connections between gradual stacking and looped models and provide strong
empirical evidence to support it."
PROBLEM SETUP,0.03785488958990536,"2
Problem Setup"
PROBLEM SETUP,0.04100946372239748,"In this section, we ﬁrst present the problem setup and background material needed for this paper.
Before we discuss the problem setting, we set up the following notation for the rest of the paper."
PROBLEM SETUP,0.04416403785488959,"Notation. For a deep network f, we use fi and #(f) to denote the ith layer and the number of layers
of the network, respectively. With slight abuse of notation, we use fi,b (where i, b ∈Z+) to denote
the layers between (i −1) · b to i · b of a deep network f. In other words, fi,b denotes the ith block of
b layers in a deep network f. a1:k is used to denote a sequence of k scalars {a1, . . . , ak}."
PROBLEM SETUP,0.0473186119873817,"Our goal is to learn a function f : X →Y which minimizes the loss E(x,y)∼D ℓ(f(x), y), for some
loss function ℓ: Y ×Y →R+ ∪{0} and data distribution D on X ×Y. We are interested in functions
of the form f = fL ◦fL−1 ◦· · · ◦f1 where ◦and L represent function composition and depth of the
network, respectively. We use FL to denote the function class consisting of functions of this form.
Given samples from the distribution D, we typically use an iterative stochastic optimizer (e.g., SGD)
to learn a function that minimizes the loss. We note that the optimization procedure is inconsequential
to the arguments in the paper. For standard training, each iteration is of the form:"
PROBLEM SETUP,0.050473186119873815,"f t = f t−1 + A(f t−1, Bt, ηt),
(Standard Training)"
PROBLEM SETUP,0.05362776025236593,"where Bt is a mini-batch from distribution D and A(f t−1, Bt, ηt) represents the iterative optimizer
update at f t−1 on Bt and learning rate ηt. The computation cost and memory requirement for training
typically increases linearly with the depth, making even simple algorithms, like SGD, slow for very
large models. Throughout this paper, we use T to denote the total number of training iterations."
K-STAGE TRAINING,0.056782334384858045,"2.1
k-stage training"
K-STAGE TRAINING,0.05993690851735016,"Since we primarily focus on stagewise training approaches, it is useful to formally deﬁne a stagewise
training procedure. In contrast to standard training, k-stage training involves dividing the training
process into k stages, and at each stage, using the the model from the previous stage to initialize the
model in the current stage. For simplicity, we assume L is divisible by k. The following are the key
ingredients:"
K-STAGE TRAINING,0.06309148264984227,"1. Function class across stages. At stage i, we use function class Fd(i) where d(i) denotes the
depth of the network at that stage. When d(i) ≪L, training is more efﬁcient.
2. Training schedules across stages. As training is divided into k stages, we use T1, · · · , Tk steps
across stages such that Pk
i=1 Ti = T."
K-STAGE TRAINING,0.06624605678233439,"(a) ALBert layer similarity
(b) GRADSTACK block similarity
(c) MIDAS block similarity"
K-STAGE TRAINING,0.0694006309148265,"Figure 2: (a) For an ALBert model trained with weight sharing across all layers, we measure the
functional similarity between layers by looking at the top 1% activated neurons in each MLP layer
and measure the intersection-over-union (IoU) metric for each pair of layers. Despite all layers
having the same parameters, a natural functional similarity structure emerges around the middle.
(b) For a UL2 model trained with GRADSTACK, we measure the cosine similarity between every pair
of layer blocks for the ﬁrst feedforward layer weights. (c) The same similarity measured for MIDAS.
The cosine similarities for stacking based models suggests a strong connection to looped models,
and MIDAS has a closer similarity structure to ALBert style looped models than GRADSTACK."
K-STAGE TRAINING,0.07255520504731862,"3. Stage initialization. This is the key component of stagewise training. Given a network f ∈
Fd(i−1) trained in the (i −1)th stage, let Mi(f) denote the network initialization for the next
stage where Mi : Fd(i−1) →Fd(i) is a growth operator."
K-STAGE TRAINING,0.07570977917981073,"Almost all the recent stagewise training procedures are different instantiations of this framework, using
different training schedules and stage initializations. We will revisit some prominent instantiations of
the framework in the next section."
PROGRESSIVE & GRADUAL STACKING,0.07886435331230283,"2.2
Progressive & Gradual Stacking"
PROGRESSIVE & GRADUAL STACKING,0.08201892744479496,"Progressive and gradual stacking are two special instantiations of the aforementioned framework. We
provide a brief description of these approaches since they are important for our discussion."
PROGRESSIVE & GRADUAL STACKING,0.08517350157728706,"Progressive Stacking [Gong et al., 2019]. This is a simple instance of k-stage training setup where
model in the previous stage is stacked onto itself to initialize the model in the next stage. In particular,
(1) depth d(i) = 2i−1d(1) grows exponentially, (2) schedule Ti is typically T/k or proportional to
d(i), and (3) the growth function Mi(f) = f ◦f."
PROGRESSIVE & GRADUAL STACKING,0.08832807570977919,"Gradual Stacking [Reddi et al., 2023]. In contrast to progressive stacking, gradual stacking linearly
increases the model depth by k in each stage. It only stacks the last L/k layers of model from the
previous stage to initialize the model in the next stage, as follows."
PROGRESSIVE & GRADUAL STACKING,0.0914826498422713,1. The depth d(i) = L·i
PROGRESSIVE & GRADUAL STACKING,0.0946372239747634,"k grows linearly with the stage.
2. Ti is typically either T/k or allocated proportional or exponential to depth.
3. Mi(fd(i−1) ◦· · · ◦f1) = fd(i−1) · · · ◦fd(i−1)−(L/k)+1 ◦fd(i−1) · · · f1. This corresponds
to stacking the last L/k layers onto the network to initialize the next stage model."
PROGRESSIVE & GRADUAL STACKING,0.09779179810725552,"In the next section, we study a novel variant of gradual stacking that enables faster training and
exhibits an interesting inductive bias, which we examine carefully."
PROGRESSIVE & GRADUAL STACKING,0.10094637223974763,"3
Algorithm: MIDAS"
PROGRESSIVE & GRADUAL STACKING,0.10410094637223975,"We present the MIDAS algorithm in this section. We ﬁrst discuss the motivation behind this variant
of gradual stacking and then formally deﬁne the algorithm.
3.1
Motivation"
PROGRESSIVE & GRADUAL STACKING,0.10725552050473186,"The motivation for MIDAS touches upon two crucial aspects: (a) the role of different layers in a
deep network and (b) a connection to looped models. Before delving into more technical details, it is
important to illustrate these points. We present the case for MIDAS based on three observations."
PROGRESSIVE & GRADUAL STACKING,0.11041009463722397,"Observation 1: gradual stacking breaks the natural role of layers. Recall that gradual stacking
initializes a larger model by duplicating and stacking the last block of b from the smaller model.
Thus in the newly initialized model, the second-last block of b layers will be the same as the last
b layers of the smaller model (see Figure 1). Intuitively, this is undesirable since the last few layers
have been shown to play a different role compared to other layers for Transformer models [Belrose
et al., 2023]. We further validate this in Figure 6. Thus, duplicating the last few layers can break
the natural role of layers at the initialization, making it a suboptimal choice. However, it is plausible
that the similarity structure across layers is broken after continued training and the initialization
is inconsequential. The next observation shows that this is not true, and establishes a connection
to looped models – networks with shared parameters between layers."
PROGRESSIVE & GRADUAL STACKING,0.11356466876971609,"Observation 2: gradual stacking leads to models resembling looped models. To check the
effect of the initialization, we measure the cosine similarity between weights of layers for a model
pretrained with gradual stacking. In Figure 2b, we observe that indeed the layers continue to have
very high cosine similarity at the end of training, thus establishing a connection between stacking and
looped models like ALBert [Lan et al., 2020] and Universal Transformers [Dehghani et al., 2018].
Unsurprisingly, the similarity structure for gradual stacking is lopsided towards the end of the model,
which raises the question: Is this similarity structure natural for looped models?"
PROGRESSIVE & GRADUAL STACKING,0.1167192429022082,"Observation 3: looped models exhibit similarity in the middle. In order to study this, we train a
prototypical looped model, ALBert, where all layers share the same parameters. Surprisingly, despite
parameters being shared, a natural similarity structure emerges between layers: yet again the ﬁrst
and last layers tend to be functionally dissimilar to other layers, whereas the functional similarity
between layers is the highest in the middle (see Figure 2a)."
PROGRESSIVE & GRADUAL STACKING,0.11987381703470032,"The above observations provides a strong motivation for stacking in the middle rather than at the end,
thus inspiring our MIDAS algorithm."
MIDAS ALGORITHM,0.12302839116719243,"3.2
MIDAS algorithm"
MIDAS ALGORITHM,0.12618296529968454,First we deﬁne the following mapping operator that is useful for stage initialization in MIDAS.
MIDAS ALGORITHM,0.12933753943217666,"M(f, b) = fn,b ◦· · · ◦f⌈n/2⌉,b ◦f⌈n/2⌉,b
|
{z
}
Replication"
MIDAS ALGORITHM,0.13249211356466878,"◦· · · ◦f1,b,
(1)"
MIDAS ALGORITHM,0.13564668769716087,"where n = #(f)/b is the number of blocks of b layers in deep network f. Note that operator M(f, b)
expands the size of the network by size b. Based on this operator, MIDAS can again be described
as a simple instantiation of the k-stage training framework, as seen below. For completeness, the
pseudocode for MIDAS in listed in Algorithm 1."
MIDAS ALGORITHM,0.138801261829653,Algorithm 1 MIDAS
MIDAS ALGORITHM,0.14195583596214512,"Require: Schedule T1:k, η1:T , optimizer update
A (see Section 2), data distribution D.
Initialize f 1,0 ∈FL/k.
for s = 1 →k do"
MIDAS ALGORITHM,0.14511041009463724,for t = 1 →Ts do
MIDAS ALGORITHM,0.14826498422712933,"Sample batch Bt from D.
f s,t = f s,t−1 + A(f s,t−1, Bt, ηt)
end for
Initializer for next stage:"
MIDAS ALGORITHM,0.15141955835962145,"f s+1,0 = M(f s,Ts, L/k)"
MIDAS ALGORITHM,0.15457413249211358,"(see Equation 1)
end for
return f k,T"
MIDAS ALGORITHM,0.15772870662460567,"Figure 3: Histogram of accuracy improvements
for models trained with MIDAS over baseline.
The data points are MIDAS 1B models listed
in Table 1. The ﬁgure shows that MIDAS-based
models have much higher improvement in the
contextual version of TyDiQA compared to the
non-contextual version."
MIDAS ALGORITHM,0.1608832807570978,1. The depth d(i) = L·i
MIDAS ALGORITHM,0.1640378548895899,"k grows linearly with the stage, similar to gradual stacking."
MIDAS ALGORITHM,0.167192429022082,"2. Ti is typically either proportional to i (linear proportional) or i2 (square proportional) or
exp(i) (exponential). We will revisit this during our empirical analysis.
3. We use growth operator M in equation 1 for initializing the next stage, which corresponds
to replicating the middle L/k layers to initialize the next stage model."
MIDAS ALGORITHM,0.17034700315457413,"3.3
Experiments: UL2 Pretraining"
MIDAS ALGORITHM,0.17350157728706625,"In this section, we evaluate MIDAS for standard language model pretraining. We train a 24L decoder-
only model with 1.5B parameters using the UL2 objective [Tay et al., 2022] on a mixture of C4,
Wikipedia, Arxiv and Github. The observations also hold for GPT-style autoregressive language
modeling. To enable fair comparison, we cached the pretraining dataset and so all methods are
trained for the same number 500B tokens in the same order, using the same batch size (refer to
Appendix A.1 for more details on the training setup). We pretrain models with three methods:
(a) standard training (Baseline), (b) gradual stacking (GRADSTACK) and (c) our proposed method
MIDAS. The goal is to compare them with respect to validation loss and downstream performance
on several diverse benchmarks. Motivated by the proportional schedules from prior work, we try the
following generalized proportional schedules for gradual stacking and MIDAS."
MIDAS ALGORITHM,0.17665615141955837,"Deﬁnition 3.1 (PROP-α schedule). For a total training budget of T steps, the schedule PROP-α
spends time Ti in each stage such that Ti ∝iα for all stages i ∈[k]. Thus Ti =
iα
Pk
j=1 jα T"
MIDAS ALGORITHM,0.17981072555205047,"PROP-1 schedule has been found to work very well for BERT pretraining [Reddi et al., 2023]. Since
UL2 pretraining is a harder task, we also explore less aggressive schedules like PROP-2 and PROP-3
that spend more time on larger models."
MIDAS ALGORITHM,0.1829652996845426,"Efﬁciency and perplexity ﬁndings. We summarize the main results in Table 1, for various stacking
methods and schedules. Firstly, we note that for all schedules, MIDAS has signiﬁcantly better
validation log perplexity than GRADSTACK at the same speedup level. This suggests that stacking
in the middle is a lot more effective for optimization than stacking at the end of the model. With
the PROP-2 schedule, MIDAS is 24% faster and nearly matches the baseline’s log perplexity.
Additionally, we observe that the ﬁndings are robust to the choice of block size for stacking."
MIDAS ALGORITHM,0.1861198738170347,"Downstream benchmark evaluations. While perplexity can serve as a decent proxy for model
quality, there is growing evidence that it is not the best measure [Liang et al., 2023]. Downstream
benchmark evaluations serve as a more holistic measure for quality and are out-of-distribution
evaluations of skills. To this effect, we evaluate MIDAS on many standard benchmarks and these
are grouped into task categories in Table 1 (refer to Appendix A.2 for more detailed evaluations on
individual tasks). The accuracy for task category is an average over representative tasks from that
group. For instance, for closed book QA task, we consider an average accuracy on TriviaQA, TydiQA
(no context), NaturalQuestions and WebQuestions."
MIDAS ALGORITHM,0.1892744479495268,"Surprisingly, we ﬁnd that downstream improvements for MIDAS are signiﬁcantly larger than the
improvements in perplexity. In particular, MIDAS with PROP-2 schedule has very similar perplexity
to baseline at 24% speedup, but the average downstream performance for MIDAS (26.8%) is much
better than baseline (24.0%). In fact, even MIDAS with PROP-1 schedule which has worse log
perplexity is much better on downstream evaluations. Similar trends of better downstream evals
holds for the 2B parameter model. The improvements are particularly large for open book QA and
math word problems, both of which are tasks that require reasoning abilities whereas memorization
tasks like closed book QA do not improve. We conjecture that these downstream improvements
are due to an inductive bias induced by stacking and we dive deeper into this in the next section."
INDUCTIVE BIAS OF STACKING,0.19242902208201892,"4
Inductive bias of stacking"
INDUCTIVE BIAS OF STACKING,0.19558359621451105,"Results in Table 1 demonstrate that MIDAS not only yields training speedups, but also improves
downstream evaluations when trained on the same number of tokens as standard training. This
suggests that stacking can extract more skills out of the same data. Here, we take a closer look at
these improvements in downstream evaluations through the lens of an inductive bias of stacking."
INDUCTIVE BIAS OF STACKING,0.19873817034700317,"Table 1: Downstream evaluations for UL2 pretrained models with 1B, 2B and 8B parameters.
Comparisons include standard training (Baseline), gradual stacking (GRADSTACK) from [Reddi
et al., 2023] and our proposed method MIDAS. The downstream evaluations are averaged over
tasks within 3 task groups. See Appendix A for precise tasks included in each task group. For
each cateory and model size, we highlight the top model is bolded and the second best model
is underlined. Firstly, MIDAS is much better than GRADSTACK, thus justifying stacking in the
middle. Secondly, MIDAS can match the log perplexity of baseline training while being roughly 24%
faster. Furthermore, even the schedule with 40% speedup has much better downstream evaluations
compared to baseline, even though it has worse log perplexity. The improvements are particularly
large for task groups that require reasoning (open book QA, math word problems)."
INDUCTIVE BIAS OF STACKING,0.20189274447949526,"Loss (↓)
Closed
Open
Math Word
All Tasks
d(i)/i
Schedule
Speedup
(validation)
Book QA (↑)
Book QA (↑)
Problems (↑)
Average (↑)"
INDUCTIVE BIAS OF STACKING,0.20504731861198738,"(block size)
(4 tasks)
(5 tasks)
(6 tasks)
(15 tasks)"
"B PARAMETERS
BASELINE",0.2082018927444795,"1B Parameters
Baseline
24
1x
1.996
13.2
33.3
23.5
24.0
GRADSTACK
4
PROP-1
1.39x
2.045
10.3
31.4
23.5
22.6
MIDAS
4
PROP-1
1.39x
2.028
11.6
34.5
30.3
26.7
MIDAS
3
PROP-1
1.41x
2.032
10.6
36.1
27.0
25.6
GRADSTACK
4
PROP-2
1.24x
2.024
11.0
31.6
17.3
20.4
MIDAS
4
PROP-2
1.24x
2.009
11.7
36.3
29.0
26.8
MIDAS
3
PROP-2
1.26x
2.012
11.9
37.3
29.8
27.5
MIDAS
4
PROP-3
1.16x
1.999
12.5
34.8
33.3
28.3
2B Parameters
Baseline
48
1x
1.926
15.2
39.1
27.1
28.0
MIDAS
8
PROP-1
1.39x
1.947
14.0
38.9
32.0
29.5
GRADSTACK
8
PROP-2
1.24x
1.945
14.2
37.0
24.5
25.9
MIDAS
8
PROP-2
1.24x
1.929
15.7
40.2
38.2
32.9
8B Parameters
Baseline
72
1x
1.841
21.1
39.6
34.9
32.8
MIDAS
9
PROP-2
1.26x
1.844
21.8
40.0
43.1
36.4"
DOWNSTREAM PERFORMANCE VS LOG PERPLEXITY,0.2113564668769716,"4.1
Downstream performance vs log perplexity"
DOWNSTREAM PERFORMANCE VS LOG PERPLEXITY,0.21451104100946372,"A reasonable expectation from pretraining is that improvements in the pretraining objective would
correlate with improvements in model quality and downstream performance. This notion of transfer
has even been theoretically formalized for language modeling in Saunshi et al. [2020], Arora and
Goyal [2023]. Thus, based on this, a natural explanation for the downstream improvements of
stacking would be that it generalizes better on the pretraining objective. However, as we see in
Table 1, downstream performance of MIDAS is better despite having similar or worse validation
perplexity – hence this is not simply the case of better generalization to unseen pretraining data. It is
natural to ask: If not perplexity, what explains this downstream phenomenon?"
DOWNSTREAM PERFORMANCE VS LOG PERPLEXITY,0.21766561514195584,"Since pretraining objective is just a proxy objective for model quality, it is plausible that different
training strategies and model architectures can extract different levels of skills from it. This is because
there are multiple ways of doing well on the pretraining tasks, and some training strategies can be
biased to pick one solution over another one. This behavior has been formalized as the inductive
bias in pretraining by recent work [Saunshi et al., 2022, Liu et al., 2023] – at the same level of
validation pretraining loss, different optimization algorithms could have vastly different downstream
performance. We hypothesize that a similar phenomenon is at play when it comes to stacking."
DOWNSTREAM PERFORMANCE VS LOG PERPLEXITY,0.22082018927444794,"Isoplots. Inspired by this phenomenon of different downstream performance at the same perplexity,
we visualize the inductive bias of a method by plotting downstream accuracy vs log perplexity isoplots
as training proceeds. We use the UL2 1B models that are pretrained with standard (baseline) training
and with MIDAS using the PROP-2 schedule (refer to Section 3.3 for more details). In Figure 4,
we visualize the downstream vs log perplexity plots for different task groups – closed-book QA,
open-book QA and math word problems. We observe a very interesting trend – MIDAS and baseline
training can have different isoplot behaviors and the divergence is different for different tasks."
DOWNSTREAM PERFORMANCE VS LOG PERPLEXITY,0.22397476340694006,"Figure 4: Downstream evalulation vs validation log perplexity isoplots as training proceeds for
baseline and MIDAS 1B models trained on the same data (stacking is 24% faster here). On the y-axis
we track the performance on various task groups – closed book QA, open book QA, math word
problems and our reasoning primitives from Section 5. On the x-axis the log perplexity is presented
in the reverse order, thus downstream performance for both methods improves as log perplexity gets
lower. For closed book QA (memorization) tasks MIDAS has very similar trends to baseline. For
open book QA tasks and math word problems, MIDAS has much better downstream performance
at an equivalent log perplexity. This showcases the inductive bias of MIDAS towards better overall
quality and better reasoning abilities."
REASONING VS MEMORIZATION FOR QA,0.22712933753943218,"4.2
Reasoning vs memorization for QA"
REASONING VS MEMORIZATION FOR QA,0.2302839116719243,"For a clearer display of the inductive bias, we measure the improvements due to MIDAS on closed
book vs open book QA tasks. It is reasonable to assume that closed book QA tasks require strong
memorization abilities whereas open book QA tasks require some reasoning abilities to infer answers
from the context that is provided. On average, we see much larger improvements on open book QA
tasks compared to closed book QA tasks, as already evident in Figure 1 and Table 1."
REASONING VS MEMORIZATION FOR QA,0.2334384858044164,"MIDAS is signiﬁcantly better on Open book QA. To make a direct comparison, we consider
TydiQA-GoldP and TydiQA-NoContext tasks – the datasets are identical and the only difference is
whether or not additional context is provided (the answer for the contextual version is guaranteed to
be inferred from the given context). In Figure 3, we see that the improvements by various MIDAS
based models on the contextual version of TydiQA are much higher than those on the non-contextual
version. This provides direct evidence of the bias of MIDAS towards improving tasks that require
reasoning. Furthermore, we ﬁnd that the memorization performance of stacking improves as the
schedule spends more time on the larger model."
REASONING IN MATH TASKS,0.23659305993690852,"4.3
Reasoning in math tasks"
REASONING IN MATH TASKS,0.23974763406940064,"To test reasoning abilities, we evaluate the language models on various math word problem datasets
like SVAMP [Patel et al., 2021], ASDiv [Miao et al., 2020], AQuA dataset for algebraic word
problems, the MAWPS benchmark [Koncel-Kedziorski et al., 2016]. We report 5-shot evaluation for
the pretrained model on these tasks. Following Wei et al. [2022], we use an external calculator to
do the arithmetic and evaluate the models on their ability to compute the correct expression for the
answer. This is because small models have bad arithmetic accuracy. The choice of using calculator or
not does not signiﬁcantly affect the trends of the results. For stacking, we use MIDAS PROP-2 model
because it achieves nearly the same perplexity as the baseline model (while being 24% faster), thus,
leading to a fair comparison based on the previous notion of inductive bias."
REASONING IN MATH TASKS,0.24290220820189273,"MIDAS is signiﬁcantly better on Math/Reasoning tasks. Detailed results can be found in Table 5.
For most math tasks, we observe that the MIDAS-based pretrained model is signiﬁcantly better than
the baseline model, especially for the MAWPs benchmark. This provides further evidence of better
math and reasoning capabilities of MIDAS."
REASONING IN MATH TASKS,0.24605678233438485,"GSM8K ﬁne-tuning.
We also evaluate the 2B and 8B models on harder math problems from the
GSM8k dataset [Cobbe et al., 2021] through few-shot prompting and ﬁne-tuning. Full results are
presented in Table 2. For MIDAS we use the PROP-2 model that has very similar perplexity as the"
REASONING IN MATH TASKS,0.24921135646687698,"Table 2: Evaluation on math tasks, including math word problems from Table 1 and a harder task
GSM8k. For GSM8k we report accuracy with 8-shot prompts and with ﬁnetuning. We also report
accuracy on all tasks after using an external calculator to ﬁx arithmetic errors; this corresponds to
w/ calc. Overall the use of calculator improves the accuracy for all models on all tasks. The beneﬁt
of MIDAS over baseline is even higher with calculator."
REASONING IN MATH TASKS,0.25236593059936907,"Model
Pretraining
Math WPs (5-shot)
GSM8k (8-shot)
GSM8k (Finetune)
Loss (↓)
W/o calc.
W calc.
W/o calc.
W calc.
W/o calc.
W calc.
2B Parameters
Baseline
1.926
15.4
27.1
3.0
3.6
5.3
8.5
MIDAS
1.929
22.5
38.3
3.0
4.1
10.4
14.5
8B Parameters
Baseline
1.841
27.3
34.9
4.5
6.6
12.3
15.8
MIDAS
1.844
32.9
43.1
5.5
7.4
15.2
18.7"
REASONING IN MATH TASKS,0.2555205047318612,"Copying
(5-shot)"
REASONING IN MATH TASKS,0.2586750788643533,"Depth 0
(5-shot)"
REASONING IN MATH TASKS,0.2618296529968454,"Depth 1
(5-shot)"
REASONING IN MATH TASKS,0.26498422712933756,Depth 1 (FT)
REASONING IN MATH TASKS,0.26813880126182965,"Depth 2
(5-shot)"
REASONING IN MATH TASKS,0.27129337539432175,Depth 2 (FT)
REASONING IN MATH TASKS,0.2744479495268139,PSM-calc
REASONING IN MATH TASKS,0.277602523659306,"(5-shot)
Task 0 20 40 60 80 100"
REASONING IN MATH TASKS,0.2807570977917981,Test accuracy (%) 24.3 86.0 28.3 68.5 19.5 45.0 69.5 14.9 49.7 21.6 43.8
REASONING IN MATH TASKS,0.28391167192429023,"19.0
23.9 62.0"
REASONING IN MATH TASKS,0.2870662460567823,Performance comparison on reasoning primitives
REASONING IN MATH TASKS,0.2902208201892745,"MIDAS
Baseline"
REASONING IN MATH TASKS,0.29337539432176657,"Figure 5: Accuracy improvements for model trained with MIDAS over baseline for representative
reasoning primitives, despite having the same perplexity. We see clear improvements for MIDAS
on almost all the primitives, both with 5-shot evaluation and after ﬁne-tuning (FT) for the depth
1 and 2 primitive."
REASONING IN MATH TASKS,0.29652996845425866,"baseline model. We ﬁnd that MIDAS has much higher accuracy after ﬁne-tuning, thus suggesting
that the beneﬁts of the inductive bias continue after ﬁne-tuning and are not just restricted to few-shot
evaluations. In particular, on the test set, the accuracy metric increased from 5.3% (for the baseline
model) to 10.4% (for MIDAS) for the 2B model (these numbers were produced by computing the
average score over three runs with different random seeds). Similarly the GSM8k accuracy of the 8B
model improves from 12.3% to 15.2%. This suggests that MIDAS not only improves the performance
on harder math tasks, but also that the gains remain or improve after ﬁne-tuning."
REASONING IN MATH TASKS,0.2996845425867508,"Effect of calculator.
For LLMs with less than 20B parameters, Wei et al. [2022] found that models
often solve math problems correctly but make arithmetic errors, leading to low accuracy. Wei et al.
[2022] remedied this by computing all arithmetic expressions using a Python program as an external
calculator. In Table 2 we ﬁnd that this improves the accuracy for our models too. Interestingly, the
gap between MIDAS and baseline gets even larger with calculator use in almost all comparisons. We
believe this is because arithmetic abilities are closer to memorization for smaller models [Razeghi
et al., 2022] and calculator use makes the problem closer to reasoning, since now the model only has
to infer the right expression. We believe this interplay between reasoning and memorization for math
problems deserves further investigation."
CONNECTION TO LOOPED MODELS,0.3028391167192429,"4.4
Connection to looped models"
CONNECTION TO LOOPED MODELS,0.305993690851735,"Given the nature of the growth operator in each stage, we hypothesize that stacking based models are
close to looped models. The layer duplication that happens at every stage ensures that blocks of layers
start from a common initialization. We measure the similarity between different blocks of layers by
measuring cosine similarities between the parameter vectors (see Figure 2). Since looped models
have been conjectured to solve algorithmic problems [Giannou et al., 2023] by ﬁnding iterative
solutions [Yang et al., 2023], we conjecture that the better reasoning abilities of MIDAS are due to
this connection to looped models We believe exploring this further is a very fruitful direction."
DEEP DIVE INTO REASONING IMPROVEMENTS,0.30914826498422715,"5
Deep dive into reasoning improvements"
DEEP DIVE INTO REASONING IMPROVEMENTS,0.31230283911671924,"To further investigate the nature of this inductive bias, we construct various simple synthetic tasks
to help tease apart the model’s capabilities. We conjecture that these simple tasks capture core
basic capabilities needed for contextual reasoning, and we therefore call these tasks “contextual
reasoning primitives”. They are: induction copying, variable assignment, and pre-school math
(PSM), discussed further below. Overall, across various few-shot evaluations and ﬁne-tuning, we
see signiﬁcant performance gaps between MIDAS and baseline training, suggesting that we have
successfully isolated some of the basic capabilities at which MIDAS excels relative to baseline
training. We refer the reader to Appendix B for more results and the exact input format."
DEEP DIVE INTO REASONING IMPROVEMENTS,0.31545741324921134,"Primitive 1: Induction copying. The “induction copying” primitive presents a sequence of words,
followed by a subsequence selected randomly from within this original sequence, and asks the model
to output the next word in the sequence. A simpliﬁed example is: “pum nyj gdq ocu rzk jbw
mlz eny kyx uni rzk jbw mlz eny kyx”, and the expected output is “uni”. This primitive is
inspired by the “induction head” mechanism introduced in Olsson et al. [2022], which is posited to be
the basic mechanism for in-context learning more generally. In Figure 5, task “Copying”, we present
results for 3-letter words of random letters, separated by spaces, with a sequence length of 10 and a
subsequence length of 5."
DEEP DIVE INTO REASONING IMPROVEMENTS,0.3186119873817035,"Primitive 2: Variable assignment. The “variable assignment” primitive tests the model’s ability to
associate a value with a variable name and apply this ability compositionally, which we test by varying
the “depth” of the task. We conjecture that this ability is a core function in contextual reasoning,
particularly in math. An example of the depth-0 variant is “u=1; t=0; v=13; y=4; f=22; y=”,
and the expected output is 4. An example of the depth-2 variant is “y=7; f=0; z=3; b=9; x=8;
q=y; l=f; m=z; h=x; a=b; n=h; j=m; t=a; i=l; g=q; n=”, and the expected output is 8.
Refer to Appendix B for more details."
DEEP DIVE INTO REASONING IMPROVEMENTS,0.3217665615141956,"Primitive 3: Pre-school math (PSM). This tests the model’s ability to solve a very simple “pre-
school math” problem by correctly associating multiple values and variables simultaneously and
applying this association to a particular task. An example is “z=6; b=5; i=-z+b; i=”, and the
expected answer (with chain-of-thought) is “-6+5=-1”."
DEEP DIVE INTO REASONING IMPROVEMENTS,0.3249211356466877,"5-shot evaluation results. Figure 5 presents the results for representative tasks, with more results in
Appendix B. Overall, we see that MIDAS outperforms baseline training across all tasks. In particular,
we see that MIDAS is signiﬁcantly stronger than the baseline at Depth 0, Copying, PSM-calc, and
Depth 1, in decreasing order of magnitude of the performance gap. Depth-2 is much harder and is at
random guessing (20%) for both models."
DEEP DIVE INTO REASONING IMPROVEMENTS,0.3280757097791798,"Fine-tuning results. Due to the difﬁculty of the variable assignment task at Depths 1 and 2, we
investigate ﬁne-tuning on these tasks as well. We ﬁne-tune on a mixture of 32 depth-1 examples and
32 depth-2 examples (i.e., only 64 examples total), using full-batch gradient descent. Figure 5 reports
the validation accuracy on Depth 1 and Depth 2 after ﬁne-tuning on this mixture (tasks “Depth 1 (FT)”
and “Depth 2 (FT)”). Overall, we see that ﬁne-tuning with just 64 examples signiﬁcantly improves
performance, resulting in MIDAS outperforming the baseline by a gap of over 20% validation
accuracy at both depths. See Appendix B for further ﬁne-tuning and evaluation details."
CONCLUSIONS AND FUTURE WORK,0.3312302839116719,"6
Conclusions and future work"
CONCLUSIONS AND FUTURE WORK,0.334384858044164,"In this work we propose a novel stacking method that outperforms previous stacking methods and
speeds up language model pretraining by 25-40%. In the process, we uncover a very intriguing
inductive bias of stacking – its ability to improve downstream reasoning tasks. Through extensive
empirical analysis, the paper makes a strong case for the presence and signiﬁcance of this inductive
bias. We believe this deserves further attention and exploration since understanding this inductive
bias could unlock new approaches to improving model quality, reasoning in particular. The reasoning
primitives start to provide more insights by isolating the reasoning improvements and we hope that
the dataset is useful for future research on improving reasoning. Finally, understanding the dichotomy
between memorization and reasoning, and how this affects the performance on various tasks, is an
interesting direction to pursue."
CONCLUSIONS AND FUTURE WORK,0.33753943217665616,"Acknowledgments.
We thank Srinadh Bhojanapalli and Vaishnavh Nagarajan for discussions on
the role of layers and memory vs contextual tasks, respectively, in the early stages of the project. We
also thank Satyen Kale for valuable feedback throughout the project."
REFERENCES,0.34069400630914826,References
REFERENCES,0.3438485804416404,"Sanjeev Arora and Anirudh Goyal. A theory for emergence of complex skills in language models.
arXiv preprint arXiv:2307.15936, 2023."
REFERENCES,0.3470031545741325,"Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella
Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens.
arXiv preprint arXiv:2303.08112, 2023."
REFERENCES,0.3501577287066246,"Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao
Chen, Zhiyuan Liu, and Qun Liu. bert2BERT: Towards reusable pretrained language models. In
Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, 2022."
REFERENCES,0.35331230283911674,"Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge
transfer. International Conference on Learning Representations, 2016."
REFERENCES,0.35646687697160884,"Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training veriﬁers to solve
math word problems. arXiv preprint arXiv:2110.14168, 2021."
REFERENCES,0.35962145110410093,"Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal
transformers. In International Conference on Learning Representations, 2018."
REFERENCES,0.3627760252365931,"Benjamin L. Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable
creation in self-attention mechanisms. In International Conference on Machine Learning, 2022."
REFERENCES,0.3659305993690852,"Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris
Papailiopoulos. Looped transformers as programmable computers. In International Conference on
Machine Learning, 2023."
REFERENCES,0.36908517350157727,"Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efﬁcient training of bert
by progressively stacking. In International conference on machine learning, pages 2337–2346.
PMLR, 2019."
REFERENCES,0.3722397476340694,"Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In Proceedings of the 35th International Conference on Machine
Learning, Proceedings of Machine Learning Research. PMLR, 10–15 Jul 2018."
REFERENCES,0.3753943217665615,"Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo
Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et al. Solar 10.7 b: Scaling large language models with
simple yet effective depth up-scaling. arXiv preprint arXiv:2312.15166, 2023."
REFERENCES,0.3785488958990536,"Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. Mawps:
A math word problem repository. In Proceedings of the 2016 conference of the north american
chapter of the association for computational linguistics: human language technologies, 2016."
REFERENCES,0.38170347003154576,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
Albert: A lite bert for self-supervised learning of language representations. In International
Conference on Learning Representations, 2020."
REFERENCES,0.38485804416403785,"Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du,
Bowen Qin, et al. Flm-101b: An open llm and how to train it with $100 k budget. arXiv preprint
arXiv:2309.03852, 2023."
REFERENCES,0.38801261829652994,"Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language
models. Transactions on Machine Learning Research, 2023."
REFERENCES,0.3911671924290221,"Hong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. Same pre-training loss, better downstream:
Implicit bias matters for language models. In International Conference on Machine Learning.
PMLR, 2023."
REFERENCES,0.3943217665615142,"Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing
english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, pages 975–984, 2020."
REFERENCES,0.39747634069400634,"Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,
Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli,
Zac Hatﬁeld-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane
Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish,
and Chris Olah. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022."
REFERENCES,0.40063091482649843,"Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple
math word problems? In Proceedings of the 2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies. Association for
Computational Linguistics, 2021."
REFERENCES,0.4037854889589905,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. Journal of machine learning research, 2020."
REFERENCES,0.4069400630914827,"Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining
term frequencies on few-shot numerical reasoning. In Yoav Goldberg, Zornitsa Kozareva, and
Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022.
Association for Computational Linguistics, 2022."
REFERENCES,0.41009463722397477,"Sashank Reddi, Sobhan Miryooseﬁ, Stefani Karp, Shankar Krishnan, Satyen Kale, Seungyeon Kim,
and Sanjiv Kumar. Efﬁcient training of language models using few-shot learning. In Proceedings
of the 40th International Conference on Machine Learning, 2023."
REFERENCES,0.41324921135646686,"Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. A mathematical exploration of why language
models help solve downstream tasks. In International Conference on Learning Representations,
2020."
REFERENCES,0.416403785488959,"Nikunj Saunshi, Jordan Ash, Surbhi Goel, Dipendra Misra, Cyril Zhang, Sanjeev Arora, Sham
Kakade, and Akshay Krishnamurthy. Understanding contrastive learning requires incorporating
inductive biases. In Proceedings of the 39th International Conference on Machine Learning, 2022."
REFERENCES,0.4195583596214511,"Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.
In International Conference on Machine Learning, pages 4596–4604. PMLR, 2018."
REFERENCES,0.4227129337539432,"Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won
Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. Ul2: Unifying language learning paradigms.
In The Eleventh International Conference on Learning Representations, 2022."
REFERENCES,0.42586750788643535,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and ﬁne-tuned chat models. arXiv preprint arXiv:2307.09288, 2023."
REFERENCES,0.42902208201892744,"Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky,
Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained
models for efﬁcient transformer training. arXiv preprint arXiv:2303.00980, 2023."
REFERENCES,0.43217665615141954,"Yite Wang, Jiahao Su, Hanlin Lu, Cong Xie, Tianyi Liu, Jianbo Yuan, Haibin Lin, Ruoyu Sun, and
Hongxia Yang. LEMON: Lossless model expansion. In The Twelfth International Conference on
Learning Representations, 2024."
REFERENCES,0.4353312302839117,"Zihao Wang and Lei Wu. Theoretical analysis of the inductive biases in deep convolutional networks.
In Thirty-seventh Conference on Neural Information Processing Systems, 2023."
REFERENCES,0.4384858044164038,"Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
neural information processing systems, 2022."
REFERENCES,0.4416403785488959,"Liu Yang, Kangwook Lee, Robert Nowak, and Dimitris Papailiopoulos. Looped transformers are
better at learning learning algorithms. arXiv preprint arXiv:2311.12424, 2023."
REFERENCES,0.444794952681388,"Yiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. Masked structural growth for 2x faster language
model pre-training. In The Twelfth International Conference on Learning Representations, 2024."
REFERENCES,0.4479495268138801,"A
Experimental Details"
REFERENCES,0.45110410094637227,"A.1
Pretraining details"
REFERENCES,0.45425867507886436,"Model architecture.
We use a decoder-only model and train it using the UL2 objective [Tay et al.,
2022] with 60% causal LM, 20% preﬁx LM and 20% span corruption. The 1B model uses 24 layers,
model dimension of 2048, hidden dimension of 5120 and 32 attention heads. The 2B model is very
similar to the 1B model, except it uses 48 layers instead of 24. The 8B model uses 72 layers, model
dimension of 2048, hidden dimension of 16384 and 16 attention heads."
REFERENCES,0.45741324921135645,"Dataset.
We use a mixture of C4 (57%) [Raffel et al., 2020], Wikipedia (17%), Github (17%),
Arxiv (9%); the proportions are motivated by the dataset used for Llama pretraining [Touvron et al.,
2023]. All models are trained for 512B tokens that are precached so that all model see exactly the
same data in the same order. This corresponds to 0.86 epochs of C4, 9 epochs of Wikipedia, 0.58
epochs of Arxiv and 0.44 epochs of Github."
REFERENCES,0.4605678233438486,"Training details.
For the 1B and 2B models, we use a cosine learning schedule with a peak learning
rate of 0.01 that decays to 0.001 in the end, and use a batch size of 512. For the 8B model we use a
peak learning rate of 0.001 and decay it to 0.0001, and use a batch size of 1024. Peak learning rate
was tuned to be optimal for baseline training. All experiments use the AdaFactor optimizer [Shazeer
and Stern, 2018] and sequence length of 1280."
REFERENCES,0.4637223974763407,"A.2
Additional downstream evaluations"
REFERENCES,0.4668769716088328,In this section we share further experimental details related to the results summarized in the Table 1.
REFERENCES,0.47003154574132494,"Trivia QA
TyDi QA
Natural
Web
d(i)/i
Schedule
Speedup
(w/o Context)
Questions
Questions"
"B PARAMETERS
BASELINE",0.47318611987381703,"1B Parameters
Baseline
24
1x
28.1
12.0
4.5
8.1
GRADSTACK
4
PROP-1
1.39x
22.4
10.1
3.0
5.8
MIDAS
4
PROP-1
1.39x
25.0
11.7
3.7
5.9
MIDAS
3
PROP-1
1.41x
22.9
9.6
3.5
6.5
GRADSTACK
4
PROP-2
1.24x
22.9
11.4
4.0
5.9
MIDAS
4
PROP-2
1.24x
26.4
10.4
3.7
6.4
MIDAS
3
PROP-2
1.26x
25.5
10.9
3.8
7.4
MIDAS
4
PROP-3
1.16x
26.9
12.0
4.5
6.8
2B Parameters
Baseline
48
1x
33.6
12.8
5.9
8.7
MIDAS
8
PROP-1
1.39x
31.1
11.7
5.6
7.8
GRADSTACK
8
PROP-2
1.24x
32.0
12.5
5.8
6.7
MIDAS
8
PROP-2
1.24x
34.6
13.0
6.3
8.9
8B Parameters
Baseline
72
1x
47.0
15.2
9.6
12.9
MIDAS
9
PROP-2
1.26x
47.9
17.0
9.2
13.1
Table 3: Closed Book QA"
"B PARAMETERS
BASELINE",0.47634069400630913,"TyDi QA
SquadV2
DROP
QuAC
CoQA
d(i)/i
Schedule
Speedup
(w/ Context)"
"B PARAMETERS
BASELINE",0.4794952681388013,"1B Parameters
Baseline
24
1x
31.4
41.1
22.9
18.8
52.6
GRADSTACK
4
PROP-1
1.39x
34.3
36.9
21.5
17.5
46.8
MIDAS
4
PROP-1
1.39x
36.1
39.1
24.3
18.7
54.4
MIDAS
3
PROP-1
1.41x
37.0
44.9
25.0
18.4
55.1
GRADSTACK
4
PROP-2
1.24x
30.0
41.0
22.1
17.2
47.8
MIDAS
4
PROP-2
1.24x
35.5
46.6
24.4
19.7
55.4
MIDAS
3
PROP-2
1.26x
38.2
46.3
24.8
19.9
57.3
MIDAS
4
PROP-3
1.16x
33.6
40.2
24.7
19.5
55.9
2B Parameters
Baseline
48
1x
42.5
49.6
25.1
20.6
57.8
MIDAS
8
PROP-1
1.39x
37.7
48.9
26.1
20.1
61.8
GRADSTACK
8
PROP-2
1.24x
38.0
47.9
23.6
19.0
56.7
MIDAS
8
PROP-2
1.24x
41.8
48.0
27.9
20.7
62.6
8B Parameters
Baseline
72
1x
39.1
51.8
25.9
19.6
61.6
MIDAS
9
PROP-2
1.26x
38.9
48.9
27.0
20.5
64.8
Table 4: Open Book QA"
"B PARAMETERS
BASELINE",0.48264984227129337,"ASDiv
MAWPS
MAWPS
MAWPS
MAWPS
SVAMP
d(i)/i
Schedule
Speedup
Add/Sub
Multi-Arith
Single-Eq
Single-Op"
"B PARAMETERS
BASELINE",0.48580441640378547,"1B Parameters
Baseline
24
1x
21.7
39.0
1.7
30.5
34.2
13.9
GRADSTACK
4
PROP-1
1.39x
19.1
38.8
2.0
31.1
35.2
15.1
MIDAS
4
PROP-1
1.39x
27.7
45.1
2.8
40.2
49.1
16.9
MIDAS
3
PROP-1
1.41x
25.8
45.1
2.5
33.1
40.7
14.8
GRADSTACK
4
PROP-2
1.24x
15.2
29.1
1.0
24.6
26.3
7.6
MIDAS
4
PROP-2
1.24x
26.3
51.9
3.3
39.4
40.0
13.0
MIDAS
3
PROP-2
1.26x
28.6
39.0
3.0
41.1
50.4
16.8
MIDAS
4
PROP-3
1.16x
28.9
55.7
1.5
41.1
50.9
21.8
2B Parameters
Baseline
48
1x
27.9
41.5
3.2
37.4
36.5
16.4
MIDAS
8
PROP-1
1.39x
29.0
56.2
1.0
41.9
45.9
18.1
GRADSTACK
8
PROP-2
1.24x
22.7
43.0
3.2
30.5
33.1
14.3
MIDAS
8
PROP-2
1.24x
34.7
58.2
7.3
50.0
57.5
21.8
8B Parameters
Baseline
72
1x
35.0
44.6
3.7
46.0
57.1
22.9
MIDAS
9
PROP-2
1.26x
39.3
60.8
5.2
54.9
66.0
32.2
Table 5: Math World Problems"
"B PARAMETERS
BASELINE",0.4889589905362776,"B
Details for contextual reasoning primitives"
"B PARAMETERS
BASELINE",0.4921135646687697,"In this section, we provide further details corresponding to Section 5."
"B PARAMETERS
BASELINE",0.4952681388012618,"All evaluations in Section 5 were performed on the 1B-parameter models. For MIDAS, we use the
variant with block size 4 and the PROP-2 schedule."
"B PARAMETERS
BASELINE",0.49842271293375395,"B.1
Exact input format"
"B PARAMETERS
BASELINE",0.501577287066246,"Expanding on Section 5, here we provide the format of the inputs and target outputs. The only caveat
is that, for simplicity of presentation, we present the inputs in 0-shot form here vs. their 5-shot form.
In 5-shot form, which is how we conduct the 5-shot evaluations, each example is separated by two
consecutive newline characters."
"B PARAMETERS
BASELINE",0.5047318611987381,"Figure 6: Measure of linearity for different layers in pretrained BERT-Base and BERT-Large models.
For each layer i, we ﬁt a linear map Ai between inputs Yi and the output of the Transformer block
(without the residual connection), Yi+1 −Yi. We then measure the r2 score and cosine similarity
for the learned linear ﬁt. The ﬁrst and last few layers demonstrate a much higher level of linearity
compared to the rest of the layers."
"B PARAMETERS
BASELINE",0.5078864353312302,"For each dataset below, the inputs are separated from the targets by the “|” character (this is not a
token in the input), and the targets are colored in red."
"B PARAMETERS
BASELINE",0.5110410094637224,"Figure 5 uses the following evaluation datasets, in the following order:"
"B PARAMETERS
BASELINE",0.5141955835962145,1. Copying (random-letter words)
"B PARAMETERS
BASELINE",0.5173501577287066,2. Variable assignment depth 0 (code)
"B PARAMETERS
BASELINE",0.5205047318611987,3. Variable assignment depth 1 (code)
"B PARAMETERS
BASELINE",0.5236593059936908,4. Variable assignment depth 1 (code)
"B PARAMETERS
BASELINE",0.526813880126183,5. Variable assignment depth 2 (code)
"B PARAMETERS
BASELINE",0.5299684542586751,6. Variable assignment depth 2 (code)
"B PARAMETERS
BASELINE",0.5331230283911672,7. Pre-school math (PSM)
"B PARAMETERS
BASELINE",0.5362776025236593,Copying (random-letter words):
"B PARAMETERS
BASELINE",0.5394321766561514,Fill in blank:
"B PARAMETERS
BASELINE",0.5425867507886435,"pum nyj gdq ocu rzk jbw mlz eny kyx uni rzk jbw mlz eny kyx ___.
->|uni"
"B PARAMETERS
BASELINE",0.5457413249211357,Copying (real words):
"B PARAMETERS
BASELINE",0.5488958990536278,Fill in blank:
"B PARAMETERS
BASELINE",0.5520504731861199,"eat fit ban sea vet zit pea cat van tea sea vet zit pea cat ___.
->|van"
"B PARAMETERS
BASELINE",0.555205047318612,Variable assignment depth 0 (basic):
"B PARAMETERS
BASELINE",0.5583596214511041,Fill in blank:
"B PARAMETERS
BASELINE",0.5615141955835962,"o=14
s=4
u=8
m=10
q=12
m=___.
->|10"
"B PARAMETERS
BASELINE",0.5646687697160884,Variable assignment depth 1 (basic):
"B PARAMETERS
BASELINE",0.5678233438485805,Fill in blank:
"B PARAMETERS
BASELINE",0.5709779179810726,"g=21
b=24
v=3
s=23
h=20
k=b
a=s
n=v
f=g
d=h
a=___.
->|23"
"B PARAMETERS
BASELINE",0.5741324921135647,Variable assignment depth 2 (basic):
"B PARAMETERS
BASELINE",0.5772870662460567,Fill in blank:
"B PARAMETERS
BASELINE",0.580441640378549,"w=24
l=12
d=16
e=5
j=9
g=j
y=e
r=l
k=d
h=w
v=g
i=r
c=h
t=k
p=y
c=___.
->|24"
"B PARAMETERS
BASELINE",0.583596214511041,Variable assignment depth 0 (math):
"B PARAMETERS
BASELINE",0.5867507886435331,"The following is a set of simple mathematical equations.
n=22
r=16
w=13
v=6
k=10
What is the numerical value of n?
Answer:|22"
"B PARAMETERS
BASELINE",0.5899053627760252,Variable assignment depth 1 (math):
"B PARAMETERS
BASELINE",0.5930599369085173,"The following is a set of simple mathematical equations.
h=20
w=9
c=22
j=11
v=5
g=c
k=w
a=j
s=h
o=v
What is the numerical value of s?
Answer:|20"
"B PARAMETERS
BASELINE",0.5962145110410094,Variable assignment depth 2 (math):
"B PARAMETERS
BASELINE",0.5993690851735016,"The following is a set of simple mathematical equations.
g=9
v=24
k=15
p=6
c=10
t=p
s=g
a=c
y=v
n=k
l=s
w=n
j=t
m=y
i=a
What is the numerical value of j?
Answer:|6"
"B PARAMETERS
BASELINE",0.6025236593059937,Variable assignment depth 0 (code):
"B PARAMETERS
BASELINE",0.6056782334384858,"The following is a very short Python program.
Use the program to resolve
the value of the variable in the question."
"B PARAMETERS
BASELINE",0.6088328075709779,"Program:
q=12
k=17
l=1
y=3
a=6"
"B PARAMETERS
BASELINE",0.61198738170347,"Question:
What is the value of k?"
"B PARAMETERS
BASELINE",0.6151419558359621,"Answer:
|17"
"B PARAMETERS
BASELINE",0.6182965299684543,Variable assignment depth 1 (code):
"B PARAMETERS
BASELINE",0.6214511041009464,"The following is a very short Python program.
Use the program to resolve
the value of the variable in the question."
"B PARAMETERS
BASELINE",0.6246056782334385,"Program:
k=11
f=21
e=10
l=7
c=13
y=f
o=c
r=e
u=k
n=l"
"B PARAMETERS
BASELINE",0.6277602523659306,"Question:
What is the value of o?"
"B PARAMETERS
BASELINE",0.6309148264984227,"Answer:
|13"
"B PARAMETERS
BASELINE",0.6340694006309149,Variable assignment depth 2 (code):
"B PARAMETERS
BASELINE",0.637223974763407,"The following is a very short Python program.
Use the program to resolve
the value of the variable in the question."
"B PARAMETERS
BASELINE",0.6403785488958991,"Program:
t=13
j=14
v=4
s=17
y=21
q=j
l=s"
"B PARAMETERS
BASELINE",0.6435331230283912,"e=y
h=t
x=v
b=x
f=e
n=q
a=h
i=l"
"B PARAMETERS
BASELINE",0.6466876971608833,"Question:
What is the value of i?"
"B PARAMETERS
BASELINE",0.6498422712933754,"Answer:
|17"
"B PARAMETERS
BASELINE",0.6529968454258676,Pre-school math (PSM):
"B PARAMETERS
BASELINE",0.6561514195583596,Fill in blank:
"B PARAMETERS
BASELINE",0.6593059936908517,"k=1
j=8
l=-k+j
l=___.
->|-1+8=7"
"B PARAMETERS
BASELINE",0.6624605678233438,Arithmetic:
"B PARAMETERS
BASELINE",0.6656151419558359,-3+2=-1
"B PARAMETERS
BASELINE",0.668769716088328,-6+1=-5
"B PARAMETERS
BASELINE",0.6719242902208202,+9-7=2
"B PARAMETERS
BASELINE",0.6750788643533123,-6-4=-10
"B PARAMETERS
BASELINE",0.6782334384858044,-6-1=-7
"B PARAMETERS
BASELINE",0.6813880126182965,+1+9=|10
"B PARAMETERS
BASELINE",0.6845425867507886,"B.2
Fine-tuning details"
"B PARAMETERS
BASELINE",0.6876971608832808,"For ﬁne-tuning, we use the “code” variant of the variable assignment task, depths 1 and 2, in 0-shot
form (i.e., no in-context examples). Due to the randomness of the data generation process and
the rather small size of each dataset (64 examples), we randomly generate 3 different 64-example
ﬁne-tuning datasets (consisting of 32 depth-1 examples and 32 depth-2 examples), ﬁne tune on each,
and report our results as an average across the 3 runs. Table 7 reports the standard deviations as well."
"B PARAMETERS
BASELINE",0.6908517350157729,"Regarding hyperparameters, we continue to use AdaFactor [Shazeer and Stern, 2018] with the same
hyperparameters as in the pretraining phase, with the exception of learning rate and batch size. We use"
"B PARAMETERS
BASELINE",0.694006309148265,"a constant learning rate of 0.001, which was chosen to match the ﬁnal learning rate of the pretraining
phase. We use full-batch training with our 64-example datasets. We then evaluate performance
separately on depth 1 and depth 2."
"B PARAMETERS
BASELINE",0.6971608832807571,"For every step i ∈{200, . . . , 300}, chosen to be signiﬁcantly after training has converged to 100%
accuracy (we do not observe overﬁtting in this range as training continues), we evaluate performance
on a 1000-example holdout set. For smoothing purposes, we average over steps 200 through 300 and
report the ﬁnal averaged performance."
"B PARAMETERS
BASELINE",0.7003154574132492,"B.3
Full 5-shot and ﬁne-tuning results"
"B PARAMETERS
BASELINE",0.7034700315457413,"5-shot.
Table 6 includes 5-shot evaluation results for all contextual reasoning primitives. Rows 1, 9,
10, 11, and 14 are the rows which appear in Figure 5."
"B PARAMETERS
BASELINE",0.7066246056782335,"When performance is better than random guessing, MIDAS consistently outperforms the baseline in
rows 1-11."
"B PARAMETERS
BASELINE",0.7097791798107256,"For pre-school math (rows 12-14), the value we report in Figure 5 is “with calculator”. This is because
the pre-school math task actually combines two capabilities: reasoning and arithmetic. Arithmetic
can be thought of as a memorization task. We evaluate arithmetic for MIDAS and baseline training,
and we see that arithmetic is quite poor for both models (7.8% and 9.6%, respectively, in Table 6).
However, by evaluating PSM with chain-of-thought and only assessing the accuracy of the reasoning
chain itself, i.e., “-6+5” vs. “-1”, we can successfully disentangle reasoning and memorization in our
evaluation. This is equivalent to having access to a calculator, so we call it “PSM with calculator” or
“PSM-calc” in Figure 5."
"B PARAMETERS
BASELINE",0.7129337539432177,"Task
MIDAS (%)
Baseline (%)
Random guessing(%)
Copying (random-letter words)
24.3
14.9
10
Copying (real words)
17.8
10.3
10
Variable assignment depth 0 (basic)
35.6
32.1
20
Variable assignment depth 1 (basic)
20.6
21.9
20
Variable assignment depth 2 (basic)
18.9
17.7
20
Variable assignment depth 0 (math)
92.8
50.1
20
Variable assignment depth 1 (math)
26.5
19.2
20
Variable assignment depth 2 (math)
20.4
18.8
20
Variable assignment depth 0 (code)
86.0
49.7
20
Variable assignment depth 1 (code)
28.3
21.6
20
Variable assignment depth 2 (code)
19.5
19
20
Pre-school math (PSM), no calculator
7.8
9.6
n/a
Arithmetic-only accuracy
9.7
10.3
n/a
Pre-school math (PSM), with calculator
69.5
62
n/a"
"B PARAMETERS
BASELINE",0.7160883280757098,"Table 6: 5-shot results for all variants of the contextual reasoning primitives. This is an expanded set
compared to Figure 5."
"B PARAMETERS
BASELINE",0.7192429022082019,"Fine tuning.
Table 7 presents the ﬁne-tuning results from Figure 5 along with corresponding
standard deviations (across the 3 trials)."
"B PARAMETERS
BASELINE",0.722397476340694,"Task
MIDAS (%)
Baseline (%)
Random guessing(%)
Variable assignment depth 1 (code)
68.54 ± 7.69
43.75 ± 5.54
20
Variable assignment depth 2 (code)
44.97 ± 7.26
23.88 ± 1.56
20"
"B PARAMETERS
BASELINE",0.7255520504731862,"Table 7: Fine-tuning results corresponding to Figure 5’s 2 ﬁne-tuning tasks. Additionally, this table
reports the standard deviation across the 3 runs with ± std dev."
"B PARAMETERS
BASELINE",0.7287066246056783,NeurIPS Paper Checklist
CLAIMS,0.7318611987381703,1. Claims
CLAIMS,0.7350157728706624,"Question: Do the main claims made in the abstract and introduction accurately reﬂect the
paper’s contributions and scope?"
CLAIMS,0.7381703470031545,Answer: [Yes]
CLAIMS,0.7413249211356467,"Justiﬁcation: The Abstract and Introduction provide a good summary of the paper’s contri-
butions. They express measured excitement about the results without overpromising."
CLAIMS,0.7444794952681388,Guidelines:
CLAIMS,0.7476340694006309,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reﬂect how
much the results can be expected to generalize to other settings.
• It is ﬁne to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.750788643533123,2. Limitations
LIMITATIONS,0.7539432176656151,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.7570977917981072,Answer: [Yes]
LIMITATIONS,0.7602523659305994,"Justiﬁcation: It has been discussed throughout the paper. In particular, MIDAS provides
limited improvement on memorization-based tasks which has been discussed in Section 3.3
and Section 4."
LIMITATIONS,0.7634069400630915,Guidelines:
LIMITATIONS,0.7665615141955836,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-speciﬁcation, asymptotic approximations only holding locally). The authors
should reﬂect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reﬂect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reﬂect on the factors that inﬂuence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efﬁciency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be speciﬁcally instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.7697160883280757,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.7728706624605678,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]
Justiﬁcation: No theoretical result is provided in this paper.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.7760252365930599,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.7791798107255521,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justiﬁcation: Section 3 and Appendix A are sufﬁcient for reproducing the main results
discussed in this paper.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.7823343848580442,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or veriﬁable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.7854889589905363,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.7886435331230284,"Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.7917981072555205,Answer: [No]
OPEN ACCESS TO DATA AND CODE,0.7949526813880127,"Justiﬁcation: We do not provide access to the data and code, but the data and models come
from prior works; where differences between our work and prior work appear, we highlight
them (Section 3 and Section A)."
OPEN ACCESS TO DATA AND CODE,0.7981072555205048,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8012618296529969,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.804416403785489,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.807570977917981,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8107255520504731,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8138801261829653,Justiﬁcation: The experimental details have been provided in Appendix A.
OPEN ACCESS TO DATA AND CODE,0.8170347003154574,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8201892744479495,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
OPEN ACCESS TO DATA AND CODE,0.8233438485804416,7. Experiment Statistical Signiﬁcance
OPEN ACCESS TO DATA AND CODE,0.8264984227129337,"Question: Does the paper report error bars suitably and correctly deﬁned or other appropriate
information about the statistical signiﬁcance of the experiments?"
OPEN ACCESS TO DATA AND CODE,0.8296529968454258,Answer: [No]
OPEN ACCESS TO DATA AND CODE,0.832807570977918,"Justiﬁcation: Due to the computational demands of exploring a reasonably large set of
experiments, it was not feasible to perform multiple runs per setting."
OPEN ACCESS TO DATA AND CODE,0.8359621451104101,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8391167192429022,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, conﬁ-
dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
the main claims of the paper."
OPEN ACCESS TO DATA AND CODE,0.8422712933753943,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not veriﬁed.
• For asymmetric distributions, the authors should be careful not to show in tables or
ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding ﬁgures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.8454258675078864,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.8485804416403786,"Question: For each experiment, does the paper provide sufﬁcient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.8517350157728707,Answer: [No]
EXPERIMENTS COMPUTE RESOURCES,0.8548895899053628,"Justiﬁcation: We do not report these details in the submission but can include them in a ﬁnal
version."
EXPERIMENTS COMPUTE RESOURCES,0.8580441640378549,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.861198738170347,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.8643533123028391,9. Code Of Ethics
CODE OF ETHICS,0.8675078864353313,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.8706624605678234,Answer: [Yes]
CODE OF ETHICS,0.8738170347003155,"Justiﬁcation: We have reviewed the NeurIPS Code of Ethics. To the best of our knowledge,
this work conforms."
CODE OF ETHICS,0.8769716088328076,Guidelines:
CODE OF ETHICS,0.8801261829652997,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.8832807570977917,10. Broader Impacts
BROADER IMPACTS,0.886435331230284,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.889589905362776,"Answer: [No]
Justiﬁcation: This a foundational/analytical paper not tied to any speciﬁc application or
deployment."
BROADER IMPACTS,0.8927444794952681,Guidelines:
BROADER IMPACTS,0.8958990536277602,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact speciﬁc
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efﬁciency and accessibility of ML)."
SAFEGUARDS,0.8990536277602523,11. Safeguards
SAFEGUARDS,0.9022082018927445,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9053627760252366,Answer: [NA]
SAFEGUARDS,0.9085173501577287,Justiﬁcation: No data or models have been released with this paper.
SAFEGUARDS,0.9116719242902208,Guidelines:
SAFEGUARDS,0.9148264984227129,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety ﬁlters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.917981072555205,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9211356466876972,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9242902208201893,Answer: [No]
LICENSES FOR EXISTING ASSETS,0.9274447949526814,"Justiﬁcation: The creators of data and models are cited, but we did not include licenses for
all assets."
LICENSES FOR EXISTING ASSETS,0.9305993690851735,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9337539432176656,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset."
LICENSES FOR EXISTING ASSETS,0.9369085173501577,"• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9400630914826499,13. New Assets
NEW ASSETS,0.943217665615142,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9463722397476341,Answer: [NA]
NEW ASSETS,0.9495268138801262,Justiﬁcation: The paper does not release new assets.
NEW ASSETS,0.9526813880126183,Guidelines:
NEW ASSETS,0.9558359621451105,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip ﬁle."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9589905362776026,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9621451104100947,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9652996845425867,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9684542586750788,Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9716088328075709,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9747634069400631,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is ﬁne, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9779179810725552,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9810725552050473,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842271293375394,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873817034700315,Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9905362776025236,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936908517350158,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968454258675079,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary signiﬁcantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
