Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003937007874015748,"Convolutional Neural Networks (CNNs) have reigned for a decade as the de facto
approach to automated medical image diagnosis, pushing the state-of-the-art in
classiﬁcation, detection and segmentation tasks. Recently, vision transformers
(ViTs) have appeared as a competitive alternative to CNNs, yielding impressive
levels of performance in the natural image domain, while possessing several in-
teresting properties that could prove beneﬁcial for medical imaging tasks. In this
work, we explore whether it is feasible to switch to transformer-based models for
medical image classiﬁcation as well, or if we should keep working with CNNs –
can we trivially replace CNNs with transformers? We consider this question in a
series of experiments on several standard medical image benchmark datasets and
tasks. Our ﬁndings show that, while CNNs perform better if trained from scratch,
off-the-shelf vision transformers can perform on par with CNNs when pretrained
on ImageNet, both in a supervised and self-supervised setting."
INTRODUCTION,0.007874015748031496,"1
INTRODUCTION"
INTRODUCTION,0.011811023622047244,"Breakthroughs in medical image analysis over the past decade have been largely fuelled by convo-
lutional neural networks (CNNs). CNN architectures have served as the workhorse for numerous
medical image analysis tasks including breast cancer detection (Wu et al., 2019), ultrasound diag-
nosis (Christiansen et al., 2021), and diabetic retinopathy (De Fauw et al., 2018). Whether applied
directly as a plug-and-play solution or used as the backbone for a bespoke model, CNNs such as
ResNet (He et al., 2016) are the dominant model for medical image analysis. Recently, however,
vision transformers have gained increased popularity for natural image recognition tasks, possibly
signalling a transition from convolution-based feature extractors to attention-based models (ViTs).
This raises the question: can ViTs also replace CNNs in the medical imaging domain?"
INTRODUCTION,0.015748031496062992,"In the natural image domain, transformers have been shown to outperform CNNs on standard vision
tasks such as IMAGENET classiﬁcation (Dosovitskiy et al., 2020), as well as in object detection (Car-
ion et al., 2020) and semantic segmentation (Ranftl et al., 2021). Interestingly, transformers succeed
despite their lack of inductive biases tied to the convolution operation. This may be explained, in
part, by the attention mechanism central to transformers which offers several key advantages over
convolutions: it explicitly models and more efﬁciently captures long-range relationships (Raghu
et al., 2021), and it has the capacity for adaptive modeling via dynamically computed self-attention
weights that capture relationships between tokens. In addition, it provides a type of built-in saliency,
giving insight as to what the model focused on (Caron et al., 2021)."
INTRODUCTION,0.01968503937007874,"Over the years, we have gained a solid understanding of how CNNs perform on medical images, and
which techniques beneﬁt them. For example, it is well-known that performance drops dramatically
when training data is scarce (Cho et al., 2015). This problem is particularly acute in the medical
imaging domain, where datasets are smaller and often accompanied by less reliable labels due to
the inherent ambiguity of medical diagnosis. For CNNs, the standard solution is to employ transfer
learning (Morid et al., 2020): typically, a model is pretrained on a larger dataset such as IMAGENET
(Deng et al., 2009) and then ﬁne-tuned for speciﬁc tasks using smaller, specialized datasets. For
medical imaging, CNNs pre-trained on IMAGENET typically outperform those trained from scratch,
both in terms of ﬁnal performance and reduced training time despite the differences between the two
domains (Morid et al., 2020; Raghu et al., 2019; Tajbakhsh et al., 2016). However, recent studies"
INTRODUCTION,0.023622047244094488,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.027559055118110236,"show that, for CNNs, transfer from IMAGENET to medical tasks is not as useful as previously
thought (Raghu et al., 2019; Neyshabur et al., 2020)."
INTRODUCTION,0.031496062992125984,"Supervised transfer learning requires many annotations. This can be problematic for medical tasks
where annotations are costly and require specialized experts. Self-supervised approaches, on the
other hand, can learn powerful representations by leveraging the intrinsic structure present in the
image, rather than explicit labels (He et al., 2020; Chen et al., 2020). Although self-supervision
performs best when large amounts of unlabelled data are available, it has been shown to be effective
with CNNs for certain medical image analysis tasks (Azizi et al., 2021; Sowrirajan et al., 2021)."
INTRODUCTION,0.03543307086614173,"In contrast to CNNs, we know relatively little about how vision transformers perform at medical
image classiﬁcation. Are vanilla ViTs competitive with CNNs? Despite their success in the natural
image domain, there are questions that cast doubt as to whether that success will translate to medical
tasks. Evidence suggests that ViTs require very large datasets to outperform CNNs – in Dosovitskiy
et al. (2020), the beneﬁts of ViTs only became evident when Google’s private 300 million image
dataset, JFT-300M, was used for pretraining. Reliance on data of this scale may be a barrier to the
application of transformers for medical tasks. As outlined above, CNNs rely on techniques such
as transfer learning and self-supervision to perform well on medical datasets. Are these techniques
as effective for transformers as they are for CNNs? Finally, the variety and type of patterns and
textures in medical images differ signiﬁcantly from the natural domain. Studies on CNNs indicate
that the beneﬁts from transfer diminish with the distance from the source domain (Azizpour et al.,
2016; Raghu et al., 2019; Neyshabur et al., 2020) Can ViTs cope with this difference?"
INTRODUCTION,0.03937007874015748,"In this work we explore whether it is feasible for ViTs to replace CNNs for medical image classiﬁca-
tion, and if there is an advantage of doing so. Given the enormous computational cost of considering
the countless variations in CNN and ViT architectures, we limit our study to prototypical CNN and
ViT models over a representative set of well-known publicly available medical image benchmark
datasets. Through these experiments we show that:"
INTRODUCTION,0.04330708661417323,"• ViTs pretrained on IMAGENET perform comparably to CNNs for classiﬁcation tasks on
medical images. While CNNs outperform ViTs when trained from scratch, ViTs receive a
larger boost in performance from pretraining on IMAGENET."
INTRODUCTION,0.047244094488188976,"• Despite their reliance on large datasets, self-supervised ViTs perform on par or better than
CNNs, if IMAGENET pretraining is utilized, albeit by a small margin."
INTRODUCTION,0.051181102362204724,"• Features of off-the-shelf IMAGENET pre-trained ViTs are versatile. k-NN tests show self-
supervised ViT representations outperform CNNs, indicating their potential for other tasks.
We also show they can serve as drop-in replacements for segmentation."
INTRODUCTION,0.05511811023622047,"These ﬁndings, along with additional ablation studies, suggest that ViTs can be used in medical
image analysis, while at the same time potentially gaining from other properties of ViTs such as
built-in explainability. The source code to reproduce our work is included in the supplementary
material, and we will release it at the time of publication."
RELATED WORK,0.05905511811023622,"2
RELATED WORK"
RELATED WORK,0.06299212598425197,"Transformers in vision problems.
Following the success of transformers (Vaswani et al., 2017a)
in natural language processing (NLP), attention-based models captured the interest of the vision
community and inspired numerous improvements to CNNs (Hu et al., 2018; Zhang et al., 2020). The
ﬁrst work to show that vanilla transformers from NLP can be applied with minimal changes to large-
scale computer vision tasks and compete with standard CNNs was from Dosovitskiy et al. (2020),
which introduced the term vision transformer, or ViT1. Like their NLP counterparts, the original
vision transformers required an enormous corpus of training data to perform well. To overcome this
problem, Touvron et al. (2021) introduced DEITs, which proposed a distillation token in addition to
the CLS token that allows transformers to learn more efﬁciently on smaller datasets. As the number
of applications of vision transformers and architectural variations have exploded (Khan et al. (2021)"
RELATED WORK,0.06692913385826772,"1While Dosovitskiy’s use of the term ViT referred to a direct adaptation of the original transformer model
from NLP (Vaswani et al., 2017b), confusingly ViT has since been adopted as a term to refer to any transformer-
based architecture for a vision task. We also use ViT in the latter sense, for brevity."
RELATED WORK,0.07086614173228346,Under review as a conference paper at ICLR 2022
RELATED WORK,0.07480314960629922,"provides a good review), DeiT and the original ViT (Dosovitskiy et al., 2020) have become the
standard benchmark architectures for vision transformers."
RELATED WORK,0.07874015748031496,"Vision transformers in medical imaging.
Given the very recent appearance of vision transform-
ers, their application in medical image analysis has been limited. Segmentation has seen the most
applications of ViTs. Chen et al. (2021a), Chang et al. (2021) and Hatamizadeh et al. (2021) in-
dependently suggested different methods for replacing CNN encoders with transformers in U-Nets
(Ronneberger et al., 2015), resulting in improved performance for several medical segmentation
tasks. Vanilla transformers were not used in any of these works, however. In each, modiﬁcations
to the standard vision transformer from Dosovitskiy et al. (2020) are proposed, either to adapt to
the U-Net framework (Chen et al., 2021a) or to add components from CNNs (Chang et al., 2021).
Other transformer-based adaptations of CNN based-architectures suggested for medical segmenta-
tion tasks include Zhang et al. (2021), who combined pyramid networks with transformers, and
Lopez Pinaya et al. (2021) who used transformers in variational auto encoders (VAEs) for segmen-
tation and anomaly detection in brain MR imaging."
RELATED WORK,0.08267716535433071,"Only a handful of studies have tackled tasks other than segmentation, such as 3-D image registration
(Chen et al., 2021b) and detection (Duong et al., 2021). The only work so far, to our knowledge,
applying transformers to medical image classiﬁcation is a CNN/transformer hybrid model proposed
by Dai et al. (2021) applied to a small (344 images) private MRI dataset. Notably, none of these
works consider pure, off-the-shelf vision transformers – all propose custom architectures combining
transformer/attention modules with components from convolutional feature extractors."
RELATED WORK,0.08661417322834646,"Improved initialization with transfer- and self-supervised learning.
Medical imaging datasets
are typically orders of magnitude smaller than natural image datasets due to cost, privacy concerns,
and the rarity of certain diseases. A common strategy to learn good representations for smaller
datasets is transfer learning. For medical imaging, it is well-known that CNNs usually beneﬁt from
transfer learning with IMAGENET (Morid et al., 2020) despite the distinct differences between the
domains. However, the question of whether ViTs beneﬁt similarly from transfer learning to medical
domains has yet to be explored."
RELATED WORK,0.09055118110236221,"While transfer learning is one option to initialize models with good features, another more recent
approach is self-supervised pre-training. Recent advances in self-supervised learning have dramat-
ically improved performance of label-free learning. State-of-the-art methods such as DINO (Caron
et al., 2021) and BYOL (Grill et al., 2020) have reached performance on par with supervised learning
on IMAGENET and other standard benchmarks. While these top-performing methods have not yet
been proven for medical imaging, there has been some work using earlier self-supervision schemes
on medical data. Azizi et al. (2021) adopted SimCLR (Chen et al., 2020), a self-supervised con-
trastive learning method, to pretrain CNNs. This yielded state-of-the-art results for predictions on
chest X-rays and skin lesions. Similarly, Sowrirajan et al. (2021) employed MoCo (He et al., 2020)
pre-training on a target chest X-ray dataset, demonstrating again the power of this approach. While
these works exhibit promising improvements thanks to self-supervision, they have all employed
CNN-based encoders. It has yet to be shown how self-supervised learning combined with ViTs
performs in medical imaging, and how this combination compares to its CNN counterparts."
METHODS,0.09448818897637795,"3
METHODS"
METHODS,0.0984251968503937,"The main question we investigate is whether prototypical vision transformers can be used as a drop-
in alternative to CNNs for medical diagnostic tasks. More concretely, we consider how each model
type performs on various tasks in different domains, with different types of initialization, and across
a range of model capacities. To that end, we conducted a series of experiments to compare rep-
resentative ViTs and CNNs for various medical image analysis tasks under the same conditions.
Hyperparameters such as weight decay and augmentations were optimized for CNNs and used for
both CNNs and ViTs – with the exception of the initial learning rate, which was determined individ-
ually for each model type using a grid search."
METHODS,0.10236220472440945,"To keep our study tractable, we selected representative CNN and ViT model types as there are too
many architecture variations to consider each and every one. For CNNs, an obvious choice is the
RESNET family (He et al., 2016), as it is the most common and highly cited CNN backbone, and"
METHODS,0.1062992125984252,Under review as a conference paper at ICLR 2022
METHODS,0.11023622047244094,"recent works have shown that RESNETs are competitive with more recent CNNs when modern
training methods are applied (Bello et al., 2021). The choice for a representative vision transformer
is less clear because the ﬁeld is still developing. We considered several options including the original
ViT (Dosovitskiy et al., 2020), SWIN transformers (Liu et al., 2021), CoaT (Xu et al., 2021), and
focal transformers (Yang et al., 2021), but we selected the DEIT family (Touvron et al., 2021) for
the following reasons: (1) it is one of the earliest and most established vision transformers, with
the most citations aside from the original ViT, (2) it is similar in spirit to a pure transformer, (3) it
was the ﬁrst to show that vision transformers can compete on mid-sized datasets with short training
times, (4) it retains the interpretability properties of the original transformer."
METHODS,0.1141732283464567,"As mentioned above, CNNs rely on initialization strategies to improve performance for smaller
datasets. Accordingly, we consider three common initialization strategies: randomly initialized
weights, transfer learning from IMAGENET, and self-supervised pretraining on the target dataset.
Using these models and initialization strategies, we consider the following datasets and tasks:"
METHODS,0.11811023622047244,"Medical image classiﬁcation.
Five standard medical image classiﬁcation datasets were chosen to
be representative of a diverse set of target domains. These cover different imaging modalities, color
distributions, dataset sizes, and tasks, with expert labels."
METHODS,0.1220472440944882,"• APTOS 2019 – In this dataset, the task is classiﬁcation of diabetic retinopathy images into 5
categories of disease severity (Kaggle, 2019). APTOS 2019 contains 3,662 high-resolution
retinal images.
• CBIS-DDSM – A mammography dataset containing 10,239 images. The task is to detect
the presence of masses in the mammograms (Sawyer-Lee et al., 2016; Lee et al., 2017;
Clark et al., 2013).
• ISIC 2019 – Here, the task is to classify 25,333 dermoscopic images among nine different
diagnostic categories of skin lesions (Tschandl et al., 2018; Codella et al., 2018; Combalia
et al., 2019).
• CheXpert – This dataset contains 224,316 chest X-rays with labels over 14 categories of
diagnostic observations (Irvin et al., 2019).
• PatchCamelyon – Sourced from the Camelyon16 segmentation challenge (Bejnordi et al.,
2017), this dataset contains 327,680 patches of H&E stained WSIs of sentinel lymph node
sections. The task is to classify each patch as cancerous or normal (Veeling et al., 2018)."
METHODS,0.12598425196850394,"Medical image segmentation.
Although there has been some recent work applying vision trans-
formers to medical image segmentation, all works thus far have proposed hybrid architectures. Here,
we evaluate the ability of vanilla ViTs to directly replace the encoder of a standard segmentation
framework designed for CNNs. In particular, we use DEEPLAB3 (Chen et al., 2017) featuring a
RESNET50 encoder. We merely replace the encoder of the segmentation model with DEIT-S and
feed the outputs of the CLS token to the decoder of DEEPLAB3. As DEEPLAB3 was designed and
optimized for RESNETs, CNNs have a clear advantage in this setup. Nevertheless, these experiments
can shed some light into the quality and versatility of ViT representations for segmentation. Note
that, unlike the classiﬁcation experiments, all models were initialized with IMAGENET pretrained
weights. We consider the following medical image segmentation datasets:"
METHODS,0.12992125984251968,"• ISIC 2018 – This dataset consists of 2,694 dermoscopic images with their corresponding
pixel-wise binary annotations. The task is to segment skin lesions (Codella et al., 2019;
Tschandl et al., 2018).
• CSAW-S – This dataset contains 338 images of mammographic screenings. The task is
pixel-wise segmentation of tumor masses (Matsoukas et al., 2020)."
METHODS,0.13385826771653545,"Experimental setup.
We selected the RESNET family (He et al., 2016) as representative CNNs,
and DEIT2 (Touvron et al., 2021) as representative ViTs.
Speciﬁcally, we test RESNET-18,
RESNET-50, and RESNET-152 against DEIT-T, DEIT-S, and DEIT-B. These models were cho-
sen because they are easily comparable in the number of parameters, memory requirements, and
compute. They are also some of simplest, most commonly used, and versatile architectures of their
respective type. Note that ViTs have an additional parameter, patch size, that directly inﬂuences the
memory and computational requirement. We use the default 16 × 16 patches to keep the number of
parameters comparable to the CNN models."
METHODS,0.1377952755905512,"2Note that we refer to the DEIT architecture pretrained without the distillation token (Touvron et al., 2021)."
METHODS,0.14173228346456693,Under review as a conference paper at ICLR 2022
METHODS,0.14566929133858267,"Initialization
Model
APTOS2019, κ ↑
DDSM, ROC-AUC ↑
ISIC2019, Recall ↑
CheXpert, ROC-AUC ↑
Camelyon, ROC-AUC ↑
n = 3,662
n = 10,239
n = 25,333
n = 224,316
n = 327,680"
METHODS,0.14960629921259844,"Random
ResNet50
0.849 ± 0.022
0.916 ± 0.005
0.660 ± 0.016
0.796 ± 0.000
0.943 ± 0.008
DeiT-S
0.687 ± 0.017
0.906 ± 0.005
0.579 ± 0.013
0.762 ± 0.002
0.921 ± 0.002"
METHODS,0.15354330708661418,"ImageNet (supervised)
ResNet50
0.893 ± 0.004
0.954 ± 0.005
0.810 ± 0.008
0.801 ± 0.001
0.960 ± 0.004
DeiT-S
0.896 ± 0.005
0.949 ± 0.006
0.844 ± 0.021
0.794 ± 0.000
0.964 ± 0.008"
METHODS,0.15748031496062992,"ImageNet (supervised) +
self-supervised with DINO
ResNet50
0.894 ± 0.008
0.955 ± 0.002
0.833 ± 0.007
0.801 ± 0.000
0.962 ± 0.004
DeiT-S
0.896 ± 0.010
0.956 ± 0.002
0.853 ± 0.009
0.801 ± 0.001
0.976 ± 0.002"
METHODS,0.16141732283464566,"Table 1: Comparison of vanilla CNNs vs. ViTs with different initialization strategies on medical
image classiﬁcation tasks."
METHODS,0.16535433070866143,"Using these models, we compare three commonly used initialization strategies:"
METHODS,0.16929133858267717,"1. Randomly initialized weights (Kaiming initialization from He et al. (2015)),"
METHODS,0.1732283464566929,"2. Transfer learning using supervised IMAGENET pretrained weights,"
METHODS,0.17716535433070865,"3. Self-supervised pretraining on the target dataset, following IMAGENET initialization."
METHODS,0.18110236220472442,"After initialization using the strategies above, the models are ﬁne-tuned on the target data using the
procedure described below. Early experiments indicated that initialization using self-supervision on
the target dataset consistently outperforms self-supervision on IMAGENET. We also determined that
DINO performs better than other self-supervision strategies such as BYOL (Grill et al., 2020)."
METHODS,0.18503937007874016,"Training procedure.
For supervised training, we use the ADAM optimizer (Kingma & Ba, 2014)3.
We performed independent grid searches to ﬁnd suitable learning rates, and found that 10−4 works
best for both pretrained CNNs and ViTs, while 10−3 is best for random initialization. We used
these as base learning rates for the optimizer along with default 1,000 warm-up iterations. When
the validation metrics saturated, the learning rate was dropped by a factor of 10 until it reached its
ﬁnal value of 10−6. For classiﬁcation tasks, images were resized to 256 × 256 with the following
augmentations applied: normalization; color jitter which consists of brightness, contrast, saturation,
hue; horizontal ﬂip; vertical ﬂip; and random resized crops. For segmentation, all images were
resized to 512 × 512 and we use the same augmentation methods as for classiﬁcation except for
CSAW-S, where elastic transforms are also employed. For each run, we select the checkpoint with
highest validation performance. For self-supervision, pretraining starts with IMAGENET initializa-
tion, then applies DINO (Caron et al., 2021) on the target data following the default settings – except
for three small changes determined to work better on medical data: (1) the base learning rate was set
to 10−4, (2) the initial weight decay is set at 10−5 and increased to 10−4 using a cosine schedule,
and (3) we used an EMA of 0.99. The same settings were used for both CNNs and ViTs; both were
pre-trained for 300 epochs using a batch size of 256, followed by ﬁne-tuning as described above."
METHODS,0.1889763779527559,"The classiﬁcation datasets were divided into train/test/validation splits (80/10/10), with the exception
of APTOS2019, which was divided 70/15/15 due to its small size. For the segmentation datasets,
we split the training set into train/validation with a 90/10 ratio. For ISIC2018 and CSAW-S, we
used the provided validation and test sets for model evaluation."
METHODS,0.19291338582677164,"Evaluation.
Unless otherwise speciﬁed, each experimental condition was repeated ﬁve times. We
report the median and standard deviation of the appropriate metric for each dataset: Cohen Kappa,
Recall, ROC-AUC, and IoU. For classiﬁcation, we measure performance of the:"
METHODS,0.1968503937007874,"• initialized model representations, through a k-NN evaluation protocol (Caron et al., 2021)"
METHODS,0.20078740157480315,• ﬁnal model after ﬁne-tuning on the target dataset.
METHODS,0.2047244094488189,"The k-NN evaluation protocol is a technique to measure the usefulness of learned representations
for some target task; it is typically applied before ﬁne-tuning. For CNNs, it works by freezing the
pretrained encoder, passing test examples through the network, and performing a k-NN classiﬁcation
using the cosine similarity between embeddings of the test images and the k nearest training images
(k = 200 in our case). For ViTs, the principle is the same except the output of the CLS token is used
instead of the CNN’s penultimate layer."
METHODS,0.20866141732283464,"3DEIT trained with random initialization used AdamW (Loshchilov & Hutter, 2017) instead of Adam."
METHODS,0.2125984251968504,Under review as a conference paper at ICLR 2022
METHODS,0.21653543307086615,"Random
ImageNet
Initialization Method"
METHODS,0.2204724409448819,"DINO
0.0 0.2 0.4 0.6 0.8"
METHODS,0.22440944881889763,"1.0
APTOS2019,"
METHODS,0.2283464566929134,"Random
ImageNet
Initialization Method"
METHODS,0.23228346456692914,"DINO
0.4 0.5 0.6 0.7 0.8 0.9"
METHODS,0.23622047244094488,"1.0
DDSM, ROC-AUC"
METHODS,0.24015748031496062,"Random
ImageNet
Initialization Method"
METHODS,0.2440944881889764,"DINO
0.0 0.2 0.4 0.6 0.8"
METHODS,0.24803149606299213,"1.0
ISIC2019, Recall"
METHODS,0.25196850393700787,"Random
ImageNet
Initialization Method"
METHODS,0.2559055118110236,"DINO
0.4 0.5 0.6 0.7 0.8 0.9"
METHODS,0.25984251968503935,"1.0 CheXpert, ROC-AUC"
METHODS,0.2637795275590551,"Random
ImageNet
Initialization Method"
METHODS,0.2677165354330709,"DINO
0.4 0.5 0.6 0.7 0.8 0.9"
METHODS,0.27165354330708663,"1.0 Camelyon, ROC-AUC"
METHODS,0.2755905511811024,"ReNet50, fine-tuned
DeiT-S, fine-tuned
ReNet50, k-NN
DeiT-S, k-NN"
METHODS,0.2795275590551181,"Figure 1: Performance comparison of RESNET50 and DEIT-S, two commonly used CNN-based
and ViT-based architectures. The comparison covers several standard medical image classiﬁcation
datasets and different types of initialization including random init, IMAGENET pretraining, and
self-supervision using DINO (Caron et al., 2021). Performance is measured after ﬁne-tuning on
the dataset, as well as using k-NN evaluation without ﬁne-tuning. We report the median over 5
repetitions, error bars represent standard deviation. Numeric values appear in Table 3 (Appendix A)"
EXPERIMENTAL RESULTS,0.28346456692913385,"4
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.2874015748031496,"Are randomly initialized vision transformers useful?
Our ﬁrst experiment compares the perfor-
mance of ViTs against CNNs when initialized with random weights (He et al., 2015). The results in
Table 1 and Figure 1 indicate that in this setting, CNNs outperform ViTs across the board. This is in
line with previous observations in the natural image domain, where ViTs trained on smaller datasets
are outperformed by similarly-sized CNNs, a trend that was attributed to the vision transformer’s
lack of inductive bias (Dosovitskiy et al., 2020). Note that the performance gap appears to shrink as
the number of training examples n increases. However, since most medical imaging datasets are of
modest size, the usefulness of randomly initialized ViTs appears to be limited."
EXPERIMENTAL RESULTS,0.29133858267716534,"Does pretraining transformers on IMAGENET work in the medical image domain?
To deal
with the smaller size of medical imaging datasets, CNNs are often initialized with IMAGENET pre-
trained weights. This typically results in better performance than random initialization (Morid et al.,
2020), while recent works have questioned the usefulness of this approach for CNNs (Raghu et al.,
2019; Neyshabur et al., 2020). We investigate if ViTs beneﬁt from IMAGENET pre-training in the
medical domain and to which extent. To test this, we initialize all models with supervised IMA-
GENET-pretrained weights and then ﬁne-tune on the target data using the procedure described in
Section 3. The results in Table 1 and Figure 1 show that both CNNs and ViTs beneﬁt from IMA-
GENET initialization. However, ViTs appear to beneﬁt more from transfer learning, as they make up
for the gap observed using random initialization, performing on par with their CNN counterparts."
EXPERIMENTAL RESULTS,0.2952755905511811,"Do transformers beneﬁt from self-supervision in the medical image domain?
Recent self-
supervised learning schemes such as DINO and BYOL achieve performance near that of supervised
learning. When used for pretraining in combination with supervised ﬁne-tuning, they can achieve
a new state-of-the-art (Caron et al., 2021; Grill et al., 2020). While this phenomenon has been
demonstrated for CNNs and ViTs on big data in the natural image domain, it is not clear whether
self-supervised pretraining of ViTs helps for medical imaging tasks. To test if ViTs beneﬁt from self-
supervision, we adopt the learning scheme of DINO, which can be readily applied to both CNNs
and ViTs (Caron et al., 2021). The results reported in Table 1 and Figure 1 show that both ViTs and
CNNs perform better with self-supervised pretraining. ViTs appear to perform on par, or better than
CNNs in this setting, albeit by a small margin."
EXPERIMENTAL RESULTS,0.2992125984251969,"Are pretrained ViT features useful for medical imaging, even without ﬁne-tuning?
When
data is particularly scarce, features from pre-trained networks can sometimes prove useful, even
without ﬁne-tuning on the target domain. The low dimensional embeddings from a good feature
extractor can be used for tasks such as clustering or few-shot classiﬁcation. We investigate whether
different types of pretraining in ViTs yields useful representations. We measure this by applying
the k-NN evaluation protocol described in Section 3. The embeddings from the penultimate layer
of the CNN and the CLS token of the ViT are used to perform k-NN classiﬁcation of test images,
assigning labels based on the most similar examples from the training set. We test whether out-
of-domain IMAGENET pretraining or in-domain self-supervised DINO pretraining (Caron et al.,
2021) yields useful representations when no supervised ﬁne-tuning has been done. The results"
EXPERIMENTAL RESULTS,0.3031496062992126,Under review as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.30708661417322836,"Model
ISIC2018, IoU ↑
CSAW-S, IoU ↑"
EXPERIMENTAL RESULTS,0.3110236220472441,"DEEPLAB3-RESNET50
0.802 ± 0.012
0.320 ± 0.008
DEEPLAB3-DEIT-S
0.845 ± 0.014
0.322 ± 0.028"
EXPERIMENTAL RESULTS,0.31496062992125984,Table 2: Medical image segmentation with DEEPLAB3 comparing CNN vs. ViT encoders.
EXPERIMENTAL RESULTS,0.3188976377952756,"ISIC2018
CSAW-S"
EXPERIMENTAL RESULTS,0.3228346456692913,"Original
Ground truth
Predictions
Original
Ground truth
Predictions"
EXPERIMENTAL RESULTS,0.32677165354330706,"Figure 2:
Medical image segmentation results comparing DEEPLAB3-RESNET50 (blue),
DEEPLAB3-DEIT-S (red). Ground truth mask appears in yellow. Note that the ViT segmentations
tend to do a better job of segmenting distant regions."
EXPERIMENTAL RESULTS,0.33070866141732286,"appear in Figure 1 and Table 3 of the Appendix A, where we compare embeddings for DEIT-S and
RESNET50. Pretraining with IMAGENET yields surprisingly useful features for both CNNs and
ViTs, considering that the embeddings were learned on out-of-domain data. We observe even more
impressive results with in-domain self-supervised DINO pretraining. ViTs appear to beneﬁt more
from self-supervision than CNNs – in fact, ViTs trained without any labels using DINO perform
similar or better than the same model trained from scratch with full supervision. The results indicate
that representations from pretrained vision transformers, both in-domain and out-of domain, can
serve as a solid starting point for transfer learning or k-shot methods."
EXPERIMENTAL RESULTS,0.3346456692913386,"Do ViTs learn meaningful representations for other tasks like segmentation?
Segmentation is
an important medical image analysis task. While previous works have shown that custom attention-
based segmentation models can outperform CNNs (see Section 2), here we ask: can we achieve
comparable performance to CNNs by merely replacing the encoder of a CNN-based segmentation
model with a vision transformer? This would demonstrate versatility of ViT representations for
other tasks. We consider DEEPLAB3 (Chen et al., 2017), a mainstream segmentation model with
a RESNET50 encoder. We simply replace the RESNET50 with DEIT-S, and train the model as
prescribed in Section 3. Despite the model being designed for RESNET50 and DEIT-S having
fewer parameters connected to the DEEPLAB3 head, we observe that ViTs perform on par or better
than CNNs in our segmentation tasks. The results in Table 2 and Figure 2 clearly indicate that
vision transformers are able to produce high quality embeddings for segmentation, even in this
disadvantaged setting."
EXPERIMENTAL RESULTS,0.33858267716535434,"How does capacity of ViT models affect medical image classiﬁcation?
To understand the practi-
cal implications of ViT model size for medical image classiﬁcation, we conducted an ablation study
comparing different model capacities of CNNs and ViTs. All models were initialized with IMA-
GENET pretrained weights, and we followed the training settings outlined in Section 3. The results
are presented in Figure 3 and Table 4 in Appendix B. Both CNNs and ViTs seem to beneﬁt similarly
from increased model capacity, within the margin of error. For most of the datasets, switching from
a small ViT to a bigger model results in minor improvements, with the exception of ISIC 2019
where model size appears to be an important factor for optimal performance and APTOS2019
where DeiT-T performs better than its larger variants. Perhaps the tiny size of APTOS2019 results
in diminishing returns for large models."
EXPERIMENTAL RESULTS,0.3425196850393701,"Other ablation studies.
We also considered the effect of token size on ViT performance. Table 5
in Appendix C shows that reducing patch size results in a slight boost in performance, but at the cost
of increasing the memory footprint. Finally, we investigate how ViTs attend to different regions of
medical images. In Figure 5 of Appendix D we compute the attention distance, the average distance
in the image across which information is integrated in ViTs. We ﬁnd that, in early layers, some heads
attend to local regions and some to the entire image. Deeper into the network, attention becomes
more global – though more quickly for some datasets than for others."
EXPERIMENTAL RESULTS,0.3464566929133858,Under review as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.35039370078740156,"5 12
25
# parameters (M) 60
86 0.87 0.88 0.89 0.9 0.91 0.92 0.93"
EXPERIMENTAL RESULTS,0.3543307086614173,DeiT-T
EXPERIMENTAL RESULTS,0.35826771653543305,ResNet18
EXPERIMENTAL RESULTS,0.36220472440944884,DeiT-S
EXPERIMENTAL RESULTS,0.3661417322834646,ResNet50
EXPERIMENTAL RESULTS,0.3700787401574803,ResNet152
EXPERIMENTAL RESULTS,0.37401574803149606,DeiT-B
EXPERIMENTAL RESULTS,0.3779527559055118,"APTOS2019,"
EXPERIMENTAL RESULTS,0.38188976377952755,"5 12
25
# parameters (M) 60
86 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99"
EXPERIMENTAL RESULTS,0.3858267716535433,DeiT-T
EXPERIMENTAL RESULTS,0.38976377952755903,ResNet18
EXPERIMENTAL RESULTS,0.3937007874015748,DeiT-S
EXPERIMENTAL RESULTS,0.39763779527559057,"ResNet50
ResNet152"
EXPERIMENTAL RESULTS,0.4015748031496063,DeiT-B
EXPERIMENTAL RESULTS,0.40551181102362205,"DDSM, ROC-AUC"
EXPERIMENTAL RESULTS,0.4094488188976378,"5 12
25
# parameters (M) 60
86 0.74 0.76 0.78 0.8 0.82 0.84 0.86 0.88"
EXPERIMENTAL RESULTS,0.41338582677165353,DeiT-T
EXPERIMENTAL RESULTS,0.41732283464566927,ResNet18
EXPERIMENTAL RESULTS,0.421259842519685,DeiT-S
EXPERIMENTAL RESULTS,0.4251968503937008,ResNet50
EXPERIMENTAL RESULTS,0.42913385826771655,ResNet152
EXPERIMENTAL RESULTS,0.4330708661417323,DeiT-B
EXPERIMENTAL RESULTS,0.43700787401574803,"ISIC2019, Recall"
EXPERIMENTAL RESULTS,0.4409448818897638,"5 12
25
# parameters (M) 60
86 0.77 0.78 0.79 0.8 0.81 0.82"
EXPERIMENTAL RESULTS,0.4448818897637795,DeiT-T
EXPERIMENTAL RESULTS,0.44881889763779526,ResNet18
EXPERIMENTAL RESULTS,0.452755905511811,DeiT-S
EXPERIMENTAL RESULTS,0.4566929133858268,ResNet50
EXPERIMENTAL RESULTS,0.46062992125984253,ResNet152
EXPERIMENTAL RESULTS,0.4645669291338583,DeiT-B
EXPERIMENTAL RESULTS,0.468503937007874,"CheXpert, ROC-AUC"
EXPERIMENTAL RESULTS,0.47244094488188976,"5 12
25
# parameters (M)"
EXPERIMENTAL RESULTS,0.4763779527559055,"60
86
0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99"
EXPERIMENTAL RESULTS,0.48031496062992124,DeiT-T
EXPERIMENTAL RESULTS,0.484251968503937,ResNet18
EXPERIMENTAL RESULTS,0.4881889763779528,DeiT-S
EXPERIMENTAL RESULTS,0.4921259842519685,ResNet50
EXPERIMENTAL RESULTS,0.49606299212598426,ResNet152
EXPERIMENTAL RESULTS,0.5,DeiT-B
EXPERIMENTAL RESULTS,0.5039370078740157,"Camelyon, ROC-AUC"
EXPERIMENTAL RESULTS,0.5078740157480315,"ResNet
DeiT"
EXPERIMENTAL RESULTS,0.5118110236220472,"Figure 3: Impact of model capacity on performance for the RESNET and DEIT families on standard
medical image classiﬁcation datasets. Both model types seem to perform better with increasing
capacity, roughly scaling similarly. Numeric results appear in Table 4 of Appendix B."
DISCUSSION,0.515748031496063,"5
DISCUSSION"
DISCUSSION,0.5196850393700787,"Our investigation of prototypical CNNs and ViTs indicate that CNNs can readily be replaced with
vision transformers in medical imaging tasks without sacriﬁcing performance. The caveat being
that employing some form of transfer learning is necessary. Our experiments corroborate previous
ﬁndings in the natural image domain and provide new insights, which we discuss below."
DISCUSSION,0.5236220472440944,"Discussion and implications of ﬁndings. Unsurprisingly, we found that CNNs outperform ViTs
when trained from scratch on medical image datasets, which corroborates previous ﬁndings in the
natural image domain (Dosovitskiy et al., 2020). This trend appears consistently and ﬁts with the
“insufﬁcient data to overcome the lack of inductive bias” argument. Thus, the usefulness of ran-
domly initialized ViTs appears to be limited in the medical imaging domain."
DISCUSSION,0.5275590551181102,"When initialized with supervised IMAGENET pretrained weights, the gap between CNN and ViT
performance disappears on medical tasks. The beneﬁts of supervised IMAGENET pretraining of
CNNs is well-known, but it was unexpected that ViTs would beneﬁt so strongly. To the best of our
knowledge, we are the ﬁrst to conﬁrm that supervised IMAGENET pretraining is so effective for ViTs
in the medical domain. This suggests that further improvements could be gained via transfer learning
from other domains more closely related to the task, as is the case for CNNs (Azizpour et al., 2016).
The beneﬁts of IMAGENET pretraining was not only observed for the ﬁne-tuned classiﬁcation tasks,
but also self-supervised learning, k-nn classiﬁcation and segmentation."
DISCUSSION,0.531496062992126,"The best overall performance on medical images is obtained using ViTs with in-domain self-
supervision, where small improvements over CNNs and other initialization methods were recorded.
Our k-NN evaluation showed that ViT features learned this way are even strong enough to outper-
form supervised learning with random initialization. Interestingly, we did not observe as strong of
an advantage for self-supervised ViTs over IMAGENET pretraining as was previously reported in
the natural image domain, e.g. in Caron et al. (2021). We suspect this is due to the limited size of
medical datasets, suggesting a tantalizing opportunity to apply self-supervision on larger and easier
to obtain unlabeled medical image datasets, where greater beneﬁts may appear."
DISCUSSION,0.5354330708661418,"Our other ablation studies showed that ViT performance scales with capacity in a similar manner to
CNNs; that ViT performance can be improved by reducing the patch size; and that ViTs attend to
information across the whole image even in the earliest layers. Although individually unsurprising,
it is worthwhile to conﬁrm these ﬁndings for medical images. It should be noted however, that the
memory demands of DeiTs increase quadratically with the image and patch size which might limit
their application in large medical images. However, newer ViT architectures (e.g. Liu et al. (2021)
and Yang et al. (2021)) mitigate this issue – while demonstrating increased predictive performance."
DISCUSSION,0.5393700787401575,"Interesting properties of ViTs. Our results indicate that switching from CNNs to ViTs can be done
without compromising performance, but are there any other advantages to switching to ViTs? There
are a number of important differences between CNNs and ViTs. We brieﬂy discuss some of these
differences and why they may be interesting for medical image analysis."
DISCUSSION,0.5433070866141733,"Lack of inductive bias – ViTs do away with convolutional layers. Our experiments indicate their
implicit locality bias is only necessary when training from scratch on medical datasets. For larger
datasets, evidence suggests removing this bias improves performance (Dosovitskiy et al., 2020)."
DISCUSSION,0.547244094488189,"Global + local features – ViTs, unlike CNNs, can combine information from distant and local
regions of the image, even in early layers (see Appendix D). This information can be propagated"
DISCUSSION,0.5511811023622047,Under review as a conference paper at ICLR 2022
DISCUSSION,0.5551181102362205,"ISIC 2019
APTOS 2019
PatchCamelyon
CheXpert
CBIS-DDSM"
DISCUSSION,0.5590551181102362,"Figure 4: Comparing saliency for RESNET50 (2nd row) and DEIT-S (3rd row) on medical classi-
ﬁcation. Each column contains the original, a Grad-CAM visualization visualisation for ResNet50
(Selvaraju et al., 2017) and the top-50% attention map of the CLS token of DEIT-S."
DISCUSSION,0.562992125984252,"efﬁciently to later layers due to better utilization of residual connections that are unpolluted by
pooling layers. This may prove beneﬁcial for medical modalities which rely on local features. It
may be necessary to use transfer learning to beneﬁt from local features, as they require huge amounts
of data to learn (Raghu et al., 2021)."
DISCUSSION,0.5669291338582677,"Interpretability – transformers’ self-attention mechanism provides, for free, new insight into how
the model makes decisions. CNNs do not naturally lend themselves well to visualizing saliency.
Popular CNN explainability methods such as class activation maps (Zhou et al., 2016) and Grad-
CAM (Selvaraju et al., 2017) provide coarse visualizations because of pooled layers. Transformer
tokens give a ﬁner picture of attention, and the self-attention maps explicitly model interactions
between every region in the image. In Figure 4, we show examples from each dataset along with
Grad-CAM visualizations of RESNET-50 and the top-50% self-attention of 16 × 16 DEIT-S CLS
token heads. The ViTs provide a clear, localized picture of attention, e.g. attention at the boundary
of the skin lesion in ISIC, on hemorrhages and exudates in APTOS, the membrane of the lymphatic
nodule in PatchCamelyon, the opacity in the Chest X-ray, and a dense region in CBIS-DDSM."
CONCLUSION,0.5708661417322834,"6
CONCLUSION"
CONCLUSION,0.5748031496062992,"To answer the question posed in the title: vanilla transformers can reliably replace CNNs on medical
image classiﬁcation with little effort. More precisely, ViTs reach the same level of performance as
CNNs in an array of medical classiﬁcation and segmentation tasks, but they require transfer learning
to do so. But since IMAGENET pretraining is the standard approach for CNNs, this does not incur
any additional cost in practice. The best overall performance on medical imaging tasks is achieved
using in-domain self-supervised pretraining, where ViTs show a small advantage over CNNs. As the
data size grows, this advantage is expected to grow as well, as observed by Caron et al. (2021) and
our own experiments. Additionally, ViTs have a number of properties that make them attractive: they
scale similarly to CNNs (or better), their lack of inductive bias, global attention and skip connections
may improve performance, and their self-attention mechanism provides a clearer picture of saliency.
From a practitioner’s point of view, these beneﬁts are compelling enough to explore the use of
ViTs in the medical domain. Finally, modern CNNs have been studied extensively for 15 years,
while the ﬁrst pure vision transformer appeared one year ago – the potential for ViTs to improve is
considerable."
CONCLUSION,0.5787401574803149,Under review as a conference paper at ICLR 2022
REFERENCES,0.5826771653543307,REFERENCES
REFERENCES,0.5866141732283464,"Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary Beaver, Jan Freyberg, Jonathan Deaton,
Aaron Loh, Alan Karthikesalingam, Simon Kornblith, Ting Chen, et al.
Big self-supervised
models advance medical image classiﬁcation. arXiv preprint arXiv:2101.05224, 2021."
REFERENCES,0.5905511811023622,"Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki, and Stefan Carlsson. Fac-
tors of transferability for a generic convnet representation. IEEE transactions on pattern analysis
and machine intelligence, 38(9):1790–1802, 2016."
REFERENCES,0.594488188976378,"Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes Van Diest, Bram Van Ginneken, Nico
Karssemeijer, Geert Litjens, Jeroen AWM Van Der Laak, Meyke Hermsen, Quirine F Manson,
Maschenka Balkenhol, et al. Diagnostic assessment of deep learning algorithms for detection of
lymph node metastases in women with breast cancer. Jama, 318(22):2199–2210, 2017."
REFERENCES,0.5984251968503937,"Irwan Bello, William Fedus, Xianzhi Du, Ekin D Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon
Shlens, and Barret Zoph. Revisiting resnets: Improved training and scaling strategies. arXiv
preprint arXiv:2103.07579, 2021."
REFERENCES,0.6023622047244095,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko.
End-to-end object detection with transformers.
In European Conference
on Computer Vision, pp. 213–229. Springer, 2020."
REFERENCES,0.6062992125984252,"Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin.
Emerging properties in self-supervised vision transformers.
arXiv preprint
arXiv:2104.14294, 2021."
REFERENCES,0.610236220472441,"Yao Chang, Hu Menghan, Zhai Guangtao, and Zhang Xiao-Ping. Transclaw u-net: Claw u-net with
transformers for medical image segmentation. arXiv preprint arXiv:2107.05188, 2021."
REFERENCES,0.6141732283464567,"Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille,
and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation.
arXiv preprint arXiv:2102.04306, 2021a."
REFERENCES,0.6181102362204725,"Junyu Chen, Yufan He, Eric C Frey, Ye Li, and Yong Du. Vit-v-net: Vision transformer for unsu-
pervised volumetric medical image registration. arXiv preprint arXiv:2104.06468, 2021b."
REFERENCES,0.6220472440944882,"Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous
convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017."
REFERENCES,0.6259842519685039,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597–1607. PMLR, 2020."
REFERENCES,0.6299212598425197,"Junghwan Cho, Kyewook Lee, Ellie Shin, Garry Choy, and Synho Do. How much data is needed to
train a medical image deep learning system to achieve necessary high accuracy? arXiv preprint
arXiv:1511.06348, 2015."
REFERENCES,0.6338582677165354,"F Christiansen, EL Epstein, E Smedberg, M ˚Akerlund, Kevin Smith, and E Epstein. Ultrasound im-
age analysis using deep neural networks for discriminating between benign and malignant ovarian
tumors: comparison with expert subjective assessment. Ultrasound in Obstetrics & Gynecology,
57(1):155–163, 2021."
REFERENCES,0.6377952755905512,"Kenneth Clark, Bruce Vendt, Kirk Smith, John Freymann, Justin Kirby, Paul Koppel, Stephen
Moore, Stanley Phillips, David Mafﬁtt, Michael Pringle, et al. The cancer imaging archive (tcia):
maintaining and operating a public information repository. Journal of digital imaging, 26(6):
1045–1057, 2013."
REFERENCES,0.6417322834645669,"Noel Codella, Veronica Rotemberg, Philipp Tschandl, M Emre Celebi, Stephen Dusza, David Gut-
man, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al. Skin lesion
analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging
collaboration (isic). arXiv preprint arXiv:1902.03368, 2019."
REFERENCES,0.6456692913385826,Under review as a conference paper at ICLR 2022
REFERENCES,0.6496062992125984,"Noel CF Codella, David Gutman, M Emre Celebi, Brian Helba, Michael A Marchetti, Stephen W
Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al. Skin lesion anal-
ysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical
imaging (isbi), hosted by the international skin imaging collaboration (isic). In 2018 IEEE 15th
international symposium on biomedical imaging (ISBI 2018), pp. 168–172. IEEE, 2018."
REFERENCES,0.6535433070866141,"Marc Combalia, Noel CF Codella, Veronica Rotemberg, Brian Helba, Veronica Vilaplana, Ofer
Reiter, Cristina Carrera, Alicia Barreiro, Allan C Halpern, Susana Puig, et al. Bcn20000: Dermo-
scopic lesions in the wild. arXiv preprint arXiv:1908.02288, 2019."
REFERENCES,0.65748031496063,"Yin Dai, Yifan Gao, and Fayu Liu. Transmed: Transformers advance multi-modal medical image
classiﬁcation. Diagnostics, 11(8):1384, 2021."
REFERENCES,0.6614173228346457,"Jeffrey De Fauw, Joseph R Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Toma-
sev, Sam Blackwell, Harry Askham, Xavier Glorot, Brendan O’Donoghue, Daniel Visentin, et al.
Clinically applicable deep learning for diagnosis and referral in retinal disease. Nature medicine,
24(9):1342–1350, 2018."
REFERENCES,0.6653543307086615,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009."
REFERENCES,0.6692913385826772,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An im-
age is worth 16x16 words: Transformers for image recognition at scale. In International Confer-
ence on Learning Representations, 2020."
REFERENCES,0.6732283464566929,"Linh T Duong, Nhi H Le, Toan B Tran, Vuong M Ngo, and Phuong T Nguyen.
Detection of
tuberculosis from chest x-ray images: Boosting the performance with vision transformer and
transfer learning. Expert Systems with Applications, pp. 115519, 2021."
REFERENCES,0.6771653543307087,"Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint
arXiv:2006.07733, 2020."
REFERENCES,0.6811023622047244,"Ali Hatamizadeh, Dong Yang, Holger Roth, and Daguang Xu. Unetr: Transformers for 3d medical
image segmentation. arXiv preprint arXiv:2103.10504, 2021."
REFERENCES,0.6850393700787402,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.6889763779527559,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016."
REFERENCES,0.6929133858267716,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2020."
REFERENCES,0.6968503937007874,"Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 7132–7141, 2018."
REFERENCES,0.7007874015748031,"Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik
Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al.
Chexpert: A large chest
radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI
conference on artiﬁcial intelligence, volume 33, pp. 590–597, 2019."
REFERENCES,0.7047244094488189,"Kaggle.
Aptos 2019 blindness detection, 2019.
URL https://www.kaggle.com/c/
aptos2019-blindness-detection/data."
REFERENCES,0.7086614173228346,"Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and
Mubarak Shah. Transformers in vision: A survey. arXiv preprint arXiv:2101.01169, 2021."
REFERENCES,0.7125984251968503,Under review as a conference paper at ICLR 2022
REFERENCES,0.7165354330708661,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.7204724409448819,"Rebecca Sawyer Lee, Francisco Gimenez, Assaf Hoogi, Kanae Kawai Miyake, Mia Gorovoy, and
Daniel L Rubin.
A curated mammography data set for use in computer-aided detection and
diagnosis research. Scientiﬁc data, 4:170177, 2017."
REFERENCES,0.7244094488188977,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint
arXiv:2103.14030, 2021."
REFERENCES,0.7283464566929134,"Walter Hugo Lopez Pinaya, Petru-Daniel Tudosiu, Robert Gray, Geraint Rees, Parashkev Nachev,
Sebastien Ourselin, and M Jorge Cardoso. Unsupervised brain anomaly detection and segmenta-
tion with transformers. arXiv e-prints, pp. arXiv–2102, 2021."
REFERENCES,0.7322834645669292,"Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
arXiv preprint
arXiv:1711.05101, 2017."
REFERENCES,0.7362204724409449,"Christos Matsoukas, Albert Bou Hernandez, Yue Liu, Karin Dembrower, Gisele Miranda, Emir
Konuk, Johan Fredin Haslum, Athanasios Zouzos, Peter Lindholm, Fredrik Strand, et al. Adding
seemingly uninformative labels helps in low data regimes. In International Conference on Ma-
chine Learning, pp. 6775–6784. PMLR, 2020."
REFERENCES,0.7401574803149606,"Mohammad Amin Morid, Alireza Borjali, and Guilherme Del Fiol. A scoping review of transfer
learning research on medical image analysis using imagenet. Computers in biology and medicine,
pp. 104115, 2020."
REFERENCES,0.7440944881889764,"Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learn-
ing? arXiv preprint arXiv:2008.11687, 2020."
REFERENCES,0.7480314960629921,"Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Understanding
transfer learning for medical imaging. In Advances in Neural Information Processing Systems,
pp. 3342–3352, 2019."
REFERENCES,0.7519685039370079,"Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy.
Do vision transformers see like convolutional neural networks? arXiv preprint arXiv:2108.08810,
2021."
REFERENCES,0.7559055118110236,"Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.
arXiv preprint arXiv:2103.13413, 2021."
REFERENCES,0.7598425196850394,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234–241. Springer, 2015."
REFERENCES,0.7637795275590551,"Rebecca Sawyer-Lee, Francisco Gimenez, Assaf Hoogi, and Daniel Rubin. Curated breast imag-
ing subset of ddsm, 2016.
URL https://wiki.cancerimagingarchive.net/x/
lZNXAQ."
REFERENCES,0.7677165354330708,"Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618–626,
2017."
REFERENCES,0.7716535433070866,"Hari Sowrirajan, Jingbo Yang, Andrew Y. Ng, and Pranav Rajpurkar. Moco pretraining improves
representation and transferability of chest x-ray models. In Mattias Heinrich, Qi Dou, Marleen
de Bruijne, Jan Lellmann, Alexander Schl¨afer, and Floris Ernst (eds.), Proceedings of the Fourth
Conference on Medical Imaging with Deep Learning, volume 143 of Proceedings of Machine
Learning Research, pp. 728–744. PMLR, 07–09 Jul 2021. URL https://proceedings.
mlr.press/v143/sowrirajan21a.html."
REFERENCES,0.7755905511811023,Under review as a conference paper at ICLR 2022
REFERENCES,0.7795275590551181,"Nima Tajbakhsh, Jae Y Shin, Suryakanth R Gurudu, R Todd Hurst, Christopher B Kendall,
Michael B Gotway, and Jianming Liang. Convolutional neural networks for medical image anal-
ysis: Full training or ﬁne tuning?
IEEE transactions on medical imaging, 35(5):1299–1312,
2016."
REFERENCES,0.7834645669291339,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Herv´e J´egou.
Training data-efﬁcient image transformers & distillation through attention.
In
International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021."
REFERENCES,0.7874015748031497,"Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of
multi-source dermatoscopic images of common pigmented skin lesions. Scientiﬁc data, 5(1):1–9,
2018."
REFERENCES,0.7913385826771654,"Ashish Vaswani,
Noam Shazeer,
Niki Parmar,
Jakob Uszkoreit,
Llion Jones,
Aidan N
Gomez, Ł ukasz Kaiser, and Illia Polosukhin.
Attention is all you need.
In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017a. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf."
REFERENCES,0.7952755905511811,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017b."
REFERENCES,0.7992125984251969,"Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv-
ariant cnns for digital pathology. In International Conference on Medical image computing and
computer-assisted intervention, pp. 210–218. Springer, 2018."
REFERENCES,0.8031496062992126,"Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, Stanislaw Jastrzebski,
Thibault Fevry, Joe Katsnelson, Eric Kim, et al. Deep neural networks improve radiologists’
performance in breast cancer screening. IEEE transactions on medical imaging, 39(4):1184–
1194, 2019."
REFERENCES,0.8070866141732284,"Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transform-
ers. arXiv preprint arXiv:2104.06399, 2021."
REFERENCES,0.8110236220472441,"Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng
Gao.
Focal self-attention for local-global interactions in vision transformers.
arXiv preprint
arXiv:2107.00641, 2021."
REFERENCES,0.8149606299212598,"Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong
He, Jonas Mueller, R Manmatha, et al.
Resnest: Split-attention networks.
arXiv preprint
arXiv:2004.08955, 2020."
REFERENCES,0.8188976377952756,"Zhuangzhuang Zhang, Baozhou Sun, and Weixiong Zhang. Pyramid medical transformer for medi-
cal image segmentation. arXiv preprint arXiv:2104.14702, 2021."
REFERENCES,0.8228346456692913,"Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
features for discriminative localization.
In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2921–2929, 2016."
REFERENCES,0.8267716535433071,Under review as a conference paper at ICLR 2022
REFERENCES,0.8307086614173228,APPENDIX
REFERENCES,0.8346456692913385,"The Appendix contains several details and additional experimental results that did not ﬁt within the
page limit, organized by the order in which they are referenced in the main article."
REFERENCES,0.8385826771653543,"A
k-NN EVALUATION OF DEIT-S AND RESNET50"
REFERENCES,0.84251968503937,"The k-NN evaluation protocol is a technique to measure the usefulness of learned representations
for some target task; it is typically applied before ﬁne-tuning. For CNNs, it works by freezing the
pretrained encoder, passing test examples through the network, and performing a k-NN classiﬁcation
using the cosine similarity between embeddings of the test images and the k nearest training images
(k = 200 in our case). For ViTs, the principle is the same except the output of the CLS token is used
instead of the CNN’s penultimate layer."
REFERENCES,0.8464566929133859,"We applied the k-NN evaluation protocol to test whether out-of-domain IMAGENET pretraining or
in-domain self-supervised DINO pretraining yields useful representations when no supervised ﬁne-
tuning has been done. The results appear in Table 3. Pretraining with IMAGENET yields surprisingly
useful features for both CNNs and ViTs, considering that the embeddings were learned on out-
of-domain data. We observe even more impressive results with in-domain self-supervised DINO
pretraining. ViTs appear to beneﬁt more from self-supervision than CNNs – in fact, ViTs trained
with DINO perform similar or better than the same model trained from scratch with full supervision!
The results indicate that representations from pretrained vision transformers, both in-domain and
out-of domain, can serve as a solid starting point for transfer learning or k-shot methods."
REFERENCES,0.8503937007874016,"Initialization
Model
APTOS2019, κ ↑
DDSM, ROC-AUC ↑
ISIC2019, Recall ↑
CheXpert, ROC-AUC ↑
Camelyon, ROC-AUC ↑"
REFERENCES,0.8543307086614174,"Random
ResNet50
0.000 ± 0.000
0.531 ± 0.063
0.125 ± 0.000
0.490 ± 0.029
0.396 ± 0.079
DeiT-S
0.018 ± 0.086
0.596 ± 0.103
0.129 ± 0.026
0.501 ± 0.013
0.443 ± 0.073"
REFERENCES,0.8582677165354331,"ImageNet (supervised)
ResNet50
0.676 ± 0.000
0.874 ± 0.000
0.248 ± 0.000
0.682 ± 0.000
0.896 ± 0.000
DeiT-S
0.687 ± 0.000
0.856 ± 0.000
0.242 ± 0.000
0.675 ± 0.000
0.892 ± 0.000"
REFERENCES,0.8622047244094488,"ImageNet (supervised) +
self-supervised with DINO
ResNet50
0.786 ± 0.000
0.897 ± 0.000
0.338 ± 0.000
0.697 ± 0.000
0.907 ± 0.000
DeiT-S
0.781 ± 0.000
0.898 ± 0.000
0.637 ± 0.000
0.739 ± 0.000
0.953 ± 0.000"
REFERENCES,0.8661417322834646,"Table 3: The k-NN evaluation of CNNs and ViTs with different initialization methods including
random initialization, IMAGENET pretraining, and self-supervision using DINO (Caron et al., 2021)
on the target medical dataset. For each task, we use the metrics that are commonly used in the liter-
ature, and for random initialization we report the median (± standard deviation) over 5 repetitions.
The numbers in this table correspond to Figure 1."
REFERENCES,0.8700787401574803,"B
DEIT AND RESNET WITH DIFFERENT CAPACITIES"
REFERENCES,0.8740157480314961,"To understand the practical implications of ViT model size for medical image classiﬁcation, we
compare different model capacities of CNNs and ViTs."
REFERENCES,0.8779527559055118,"Speciﬁcally, we test RESNET-18, RESNET-50, and RESNET-152 against DEIT-T, DEIT-S, and
DEIT-B. These models were chosen because they are easily comparable in the number of param-
eters, memory requirements, and compute. They are also some of simplest, most commonly used,
and versatile architectures of their respective type. All models were initialized with IMAGENET
pretrained weights, and trained following the settings outlined in Section 3. The results appear in
Table 4."
REFERENCES,0.8818897637795275,"Both CNNs and ViTs seem to beneﬁt similarly from increased model capacity, within the margin
of error. For most of the datasets, switching from a small ViT to a bigger model results in minor
improvements, with the exception of ISIC 2019 where model size appears to be an important factor
for optimal performance."
REFERENCES,0.8858267716535433,Under review as a conference paper at ICLR 2022
REFERENCES,0.889763779527559,"Model
APTOS2019, κ ↑
DDSM, ROC-AUC ↑
ISIC2019, Recall ↑
CheXpert, ROC-AUC ↑
Camelyon, ROC-AUC ↑"
REFERENCES,0.8937007874015748,"ResNet18
0.893 ± 0.003
0.950 ± 0.003
0.785 ± 0.015
0.793 ± 0.001
0.959 ± 0.012
ResNet50
0.893 ± 0.004
0.954 ± 0.005
0.810 ± 0.008
0.801 ± 0.001
0.960 ± 0.004
ResNet152
0.900 ± 0.004
0.960 ± 0.003
0.798 ± 0.012
0.801 ± 0.001
0.965 ± 0.003"
REFERENCES,0.8976377952755905,"DeiT-T
0.901 ± 0.005
0.946 ± 0.006
0.810 ± 0.013
0.789 ± 0.001
0.961 ± 0.003
DeiT-S
0.896 ± 0.005
0.949 ± 0.006
0.844 ± 0.021
0.794 ± 0.000
0.964 ± 0.008
DeiT-B
0.897 ± 0.004
0.953 ± 0.004
0.840 ± 0.013
0.795 ± 0.001
0.969 ± 0.002"
REFERENCES,0.9015748031496063,"Table 4: Effect of model capacity on the performance after ﬁne-tuning, with three different RESNET
variants (RESNET18, RESNET50 and RESNET152) and DEIT variants (DEIT-T, DEIT-S and
DEIT-B ) using IMAGENET pretraining. For each task, we report the median (± standard devi-
ation) over 5 repetitions using the metrics that are commonly used in the literature. We can see
that both CNNs and ViTs mostly beneﬁt from increased model capacity. Further, model types with
similar capacity perform approximately on par in most of the datasets, indicating that ViTs scale
similarly to CNNs. The numbers in this table correspond to Figure 3."
REFERENCES,0.905511811023622,"C
EFFECT OF PATCH SIZE"
REFERENCES,0.9094488188976378,"ViTs have an additional parameter, patch size, that directly inﬂuences the memory and computational
requirement. We compare the default 16 × 16 patches, used in this work, with 8 × 8 patches, as it
has been noted in previous works that reducing the patch size leads to increased performance (Caron
et al., 2021)."
REFERENCES,0.9133858267716536,"The results appear in Table 5. We observe that smaller patch sizes result in marginally better perfor-
mance, though to a lesser degree than has been observed in the natural image domain."
REFERENCES,0.9173228346456693,"Patch size
APTOS2019, κ ↑
DDSM, ROC-AUC ↑
ISIC2019, Recall ↑
CheXpert, ROC-AUC ↑
Camelyon, ROC-AUC ↑"
REFERENCES,0.9212598425196851,"16 × 16
0.896 ± 0.005
0.949 ± 0.006
0.844 ± 0.021
0.794 ± 0.000
0.964 ± 0.008
8 × 8
0.898 ± 0.006
0.947 ± 0.007
0.847 ± 0.015
0.800 ± 0.001
0.964 ± 0.002"
REFERENCES,0.9251968503937008,"Table 5: Comparison of performance of DEIT-S using 8 × 8 and 16 × 16 pixel input patches on
standard medical image classiﬁcation datasets. Performance is measured after ﬁne-tuning on the
dataset. For each task we report the median (± standard deviation) over 5 repetitions using the
metrics that are commonly used in the literature. Although, the model using 8×8 patch sizes median
performance is slightly better in the majority of datasets, we see no signiﬁcant improvements, aside
from CheXpert."
REFERENCES,0.9291338582677166,"D
MEAN ATTENDED DISTANCE OF VITS"
REFERENCES,0.9330708661417323,"We compute the attention distance, the average distance in the image across which information is
integrated in ViTs. As reported previously (e.g. Dosovitskiy et al. (2020)), in early layers, some
heads attend to local regions and some to the entire image. Deeper into the network, attention
becomes more global – though more quickly for some datasets than for others."
REFERENCES,0.937007874015748,"0
2
4
6
8
10
Network depth (layer) 20 40 60 80 100 120"
REFERENCES,0.9409448818897638,Mean attention distance (pixels)
REFERENCES,0.9448818897637795,APTOS2019
REFERENCES,0.9488188976377953,"0
2
4
6
8
10
Network depth (layer) 20 40 60 80 100 120"
REFERENCES,0.952755905511811,Mean attention distance (pixels) DDSM
REFERENCES,0.9566929133858267,"0
2
4
6
8
10
Network depth (layer) 40 60 80 100 120"
REFERENCES,0.9606299212598425,Mean attention distance (pixels)
REFERENCES,0.9645669291338582,ISIC2019
REFERENCES,0.968503937007874,"0
2
4
6
8
10
Network depth (layer) 20 40 60 80 100 120"
REFERENCES,0.9724409448818898,Mean attention distance (pixels)
REFERENCES,0.9763779527559056,CheXpert
REFERENCES,0.9803149606299213,"0
2
4
6
8
10
Network depth (layer) 20 40 60 80 100 120"
REFERENCES,0.984251968503937,Mean attention distance (pixels)
REFERENCES,0.9881889763779528,Camelyon
REFERENCES,0.9921259842519685,"Head 1
Head 2
Head 3
Head 4
Head 5
Head 6"
REFERENCES,0.9960629921259843,"Figure 5: Mean attention distance with respect to the attention head and the network depth. Each
point is calculated as the average over 512 test samples as the mean of the element-wise multiplica-
tion of each query token’s attention and its distance from the other tokens"
