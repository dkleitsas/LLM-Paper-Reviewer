Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0013192612137203166,"Neighbor sampling is a commonly used technique for training Graph Neural
Networks (GNNs) on large graphs. Previous work has shown that sampling-
based GNN training can be considered as Stochastic Compositional Optimization
(SCO) problems and can be better solved by SCO algorithms. However, we Ô¨Ånd
that SCO algorithms are impractical for training GNNs on large graphs because
they need to store the moving averages of the aggregated features of all nodes
in the graph. The moving averages can easily exceed the GPU memory limit
and even the CPU memory limit. In this work, we propose a variant of SCO
algorithms with sparse moving averages for GNN training. By storing the moving
averages in the most recent iterations, our algorithm only requires a Ô¨Åxed size
buffer, regardless of the graph size. We show that our algorithm preserves the
convergence rate of the original SCO algorithm when the buffer size satisÔ¨Åes
certain conditions. Our experiments validate our theoretical results and show that
our algorithm outperforms the traditional Adam SGD for GNN training with a
small memory overhead."
INTRODUCTION,0.002638522427440633,"1
INTRODUCTION"
INTRODUCTION,0.00395778364116095,"Graph Neural Networks (GNNs) have become the state-of-the-art models for machine learning
tasks on graph-structured data. By recursively aggregating the features of neighboring nodes,
GNNs learn an embedding of the nodes and use the embedding for downstream tasks such as node
classiÔ¨Åcation (Kipf & Welling, 2017; Duran & Niepert, 2017) or link prediction (Zhang & Chen,
2017; 2018)."
INTRODUCTION,0.005277044854881266,"Due to the recursive neighbor aggregation, training GNNs on large graphs is computationally
challenging. To alleviate the computation burden, various neighbor sampling methods have been
proposed (Hamilton et al., 2017; Ying et al., 2018; Chen et al., 2018; Zou et al., 2019; Li et al.,
2018; Chiang et al., 2019; Zeng et al., 2020). The idea is to compute an unbiased estimation of the
aggregation result in each layer based on a sampled subset of neighbors. These sampling techniques
enable GNN training on large graphs. However, due to the composition of the aggregation functions
in multiple layers, the stochastic gradient obtained with sampled neighbor aggregation is not an
unbiased estimation of the true gradient, which undermines the convergence property of SGD-based
training algorithms."
INTRODUCTION,0.006596306068601583,"Previous work has shown that sampling-based GNN training is actually a Stochastic Compositional
Optimization (SCO) problem (Cong et al., 2020; 2021). Cong et al. (2021) show that SCO algorithms
can achieve faster convergence than the commonly used Adam SGD for GNN training on small
graphs. Despite their good convergence property, SCO algorithms are not widely adopted for GNN
training due to two reasons. First, although SCO algorithms achieve smaller training losses, the
obtained GNN models usually have poor generalization ‚Äì the validation and test accuracy are lower
than the models trained by Adam SGD. Second, SCO algorithms need to maintain the moving
averages of aggregation results of all nodes in the graph. For large graphs, the moving averages may
exceed the memory capacity of the GPU. While it is possible to store the moving averages in CPU
memory, copying the data from CPU to GPU in each iteration is expensive, which may negate the
beneÔ¨Åts of the faster convergence of SCO algorithms."
INTRODUCTION,0.0079155672823219,"To address the above issues, we propose a Sparse Stochastic Compositional (SpSC) gradient method
in this work. Our main idea is to store the moving averages for nodes sampled in the most recent"
INTRODUCTION,0.009234828496042216,"Under review as a conference paper at ICLR 2022 ùëå!""#"
INTRODUCTION,0.010554089709762533,"(%)
ùëå! (%) ""ùëç! (%)"
INTRODUCTION,0.011873350923482849,"+ ùõΩ!√ó
= (1 ‚àíùõΩ!)√ó"
INTRODUCTION,0.013192612137203167,Sampled Nodes
INTRODUCTION,0.014511873350923483,Unsampled Nodes
INTRODUCTION,0.0158311345646438,(a) SCGD
INTRODUCTION,0.017150395778364115,chunk-(k mod t)
INTRODUCTION,0.018469656992084433,"= (1 ‚àíùõΩ!)√ó
+ ùõΩ!√ó )ùëç! (#)"
INTRODUCTION,0.01978891820580475,chunk-0
INTRODUCTION,0.021108179419525065,chunk-1
INTRODUCTION,0.022427440633245383,"chunk-(t-1) ‚Ä¶
‚Ä¶ ‚Ä¶
‚Ä¶ ‚Ä¶
‚Ä¶"
INTRODUCTION,0.023746701846965697,"Sampled nodes found in buffer
Sampled nodes not found in buffer
Unsampled Nodes
Invalidated nodes"
INTRODUCTION,0.025065963060686015,(b) SpSC
INTRODUCTION,0.026385224274406333,"Figure 1: Updating the moving average of eZ(l). SCGD needs to store the moving averages for all
nodes in the graph. In SpSC, we only stores the data for nodes sampled in the past t iterations."
INTRODUCTION,0.027704485488126648,"iterations instead of all nodes. As only a small number of nodes are stored, our algorithm has
small memory consumption even for large graphs. We provide a convergence analysis on SpSC
and show that, when the number of stored iterations satisÔ¨Åes certain constraints, SpSC can preserve
the asymptotic convergence rate of the original SCO algorithm. In practice, the sparse moving
averaging slightly slows down the convergence of SCO algorithm, but it surprisingly overcomes
its poor generalization problem and achieves higher accuracy for sampling-based GNN training.
Compared with Adam SGD, our algorithm incurs a small overhead for updating the moving averages
in each iteration, but the overhead can be easily justiÔ¨Åed by the faster convergence of our algorithm."
INTRODUCTION,0.029023746701846966,"Our experiments with two GNN models on different input graphs validate our theoretical results and
show that our algorithm achieves higher accuracy than Adam SGD with the same or less amount of
training time."
BACKGROUND AND MOTIVATION,0.030343007915567283,"2
BACKGROUND AND MOTIVATION"
BACKGROUND AND MOTIVATION,0.0316622691292876,"To facilitate our discussion, we Ô¨Årst give background on sampling-based GNN training and its relation
to stochastic compositional optimization."
GNN COMPUTATION,0.032981530343007916,"2.1
GNN COMPUTATION"
GNN COMPUTATION,0.03430079155672823,"The computation at each layer of a GNN is conducted in two steps: aggregate and update. For
each node v in the graph, the aggregate function gathers data from its neighboring nodes and
returns the aggregation result as"
GNN COMPUTATION,0.03562005277044855,"zv = Aggv(hne[v], xv, xne[v]).
(1)"
GNN COMPUTATION,0.036939313984168866,"Here, hne[v] is the intermediate features of v‚Äôs neighbors from the previous layer, xv is the input
feature of v, and xne[v] is the input features of v‚Äôs neighbors. The update function uses the
aggregated value to produce the intermediate features of v as"
GNN COMPUTATION,0.03825857519788918,"hv = Updv(zv, xv).
(2)"
GNN COMPUTATION,0.0395778364116095,"By stacking the intermediate features and the input features of all nodes, the computation at layer l
can be written as"
GNN COMPUTATION,0.040897097625329816,"Z(l) = Agg(H(l‚àí1), X),
H(l) = Upd(Z(l), X, W (l)).
(3)"
GNN COMPUTATION,0.04221635883905013,"Here, H(l‚àí1) = [h(l‚àí1)
1
, . . . , h(l)
N ] denotes the intermediate features of all nodes at layer l‚àí1, Z(l) ‚àà
RN√ódl is the aggregated features of all nodes at layer l, X = [x1, . . . , xN] is the input features of all
nodes, and W (l) is the learnable weights. As an example, Graph Convolutional Network (GCN) (Kipf
& Welling, 2017) has Agg(H(l‚àí1), X) = PH(l‚àí1) where P is the normalized Laplacian matrix
of the graph, and Upd(Z(l), X, W (l)) = œÉ(Z(l)W (l)) where œÉ is a non-linear activation function.
Many other GNNs can be expressed in this form with different deÔ¨Ånitions of Agg and Upd (Zhou
et al., 2018)."
GNN COMPUTATION,0.04353562005277045,Under review as a conference paper at ICLR 2022
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.044854881266490766,"2.2
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION"
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.04617414248021108,"When the graph is large, the neigbhbor aggregation operation Agg incurs a large overhead, making
the training of GNNs computationally challenging. Therefore, prior work has proposed to replace
the Agg function with a sampled neighbor aggregation operation g
Agg. By sampling the neighboring
nodes, an unbiased estimate of Z(l) is computed at each layer, i.e.,"
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.047493403693931395,"eZ(l) = g
Agg(H(l‚àí1), X)
(4)"
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.048812664907651716,with E[ eZ(l)] = Z(l). If we deÔ¨Åne the computation at layer l of the original GNN as a function
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.05013192612137203,"f (l)(Z(l‚àí1), W (l‚àí1), ..., W (T )) = [Z(l), W (l), ..., W (T )]
(5)"
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.051451187335092345,"=[Agg(Upd(Z(l‚àí1), X, W (l‚àí1))), W (l), ..., W (T )]],"
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.052770448548812667,the computation with sampled neighbor aggregation can be written as a stochastic function
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.05408970976253298,"f (l)
Œæl ( eZ(l‚àí1), W (l‚àí1), ..., W (T )) = [ eZ(l), W (l), ..., W (T )]
(6)"
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.055408970976253295,"=[ g
Agg(Upd( eZ(l‚àí1), X, W (l‚àí1))), W (l), ..., W (T )]]"
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.05672823218997362,"where Œæl represents the sampled neighbors at layer l. Since eZ(l) is an unbiased estimate of Z(l), we
have E[f (l)
Œæl ] = f (l), and the computation of a T-layer GNN can be written as"
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.05804749340369393,"F(Œ∏) = EŒæT +1
h
f (T +1)
ŒæT +1"
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.059366754617414245,"
EŒæT
h
f (T )
ŒæT"
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.06068601583113457,"
...EŒæ1[f (1)(Œ∏)]...
ii
(7)"
SAMPLING-BASED GNN TRAINING AS STOCHASTIC COMPOSITIONAL OPTIMIZATION,0.06200527704485488,"where Œ∏ = [X, W (1), ..., W (T )], f (T +1) is the loss function, and f (T +1)
ŒæT +1
corresponds to the estimated
loss with mini-batch sampling. Note that we put all the learnable weights in Œ∏ to formulate the
computation as a stochastic compositional function. Our goal is to minimize F(Œ∏), which is exactly a
multi-level SCO problem."
LARGE MEMORY CONSUMPTION ISSUE WITH A NAIVE IMPLEMENTATION,0.0633245382585752,"2.3
LARGE MEMORY CONSUMPTION ISSUE WITH A NAIVE IMPLEMENTATION"
LARGE MEMORY CONSUMPTION ISSUE WITH A NAIVE IMPLEMENTATION,0.06464379947229551,"SCO has been well studied in the past few years, and many algorithms with guaranteed convergence
have been proposed (Zhang & Xiao, 2019; Yang et al., 2019; Chen et al., 2020; Yang et al., 2019;
Balasubramanian et al., 2020; Chen et al., 2020; Lian et al., 2017; Wang et al., 2017b; Ghadimi
et al., 2020). It seems straightforward to adopt these SCO algorithms to achieve faster training of
GNNs. However, these algorithms have large memory consumption when applied to GNN training
and cannot run on GPUs for large graphs."
LARGE MEMORY CONSUMPTION ISSUE WITH A NAIVE IMPLEMENTATION,0.06596306068601583,"To see the problem, let us consider the implementation of the SCGD algorithm (Yang et al., 2019) for
GNN training. Formally, the algorithm is written as"
LARGE MEMORY CONSUMPTION ISSUE WITH A NAIVE IMPLEMENTATION,0.06728232189973615,"y(1)
k+1 = (1 ‚àíŒ≤k)y(1)
k
+ Œ≤kf (1)
Œæ1,k(Œ∏k),
(8)"
LARGE MEMORY CONSUMPTION ISSUE WITH A NAIVE IMPLEMENTATION,0.06860158311345646,"y(l)
k+1 = (1 ‚àíŒ≤k)y(l)
k + Œ≤kf (l)
Œæl,k(y(l‚àí1)
k+1 ),
2 ‚â§l ‚â§T,
(9)"
LARGE MEMORY CONSUMPTION ISSUE WITH A NAIVE IMPLEMENTATION,0.06992084432717678,"Œ∏k+1 = Œ∏k ‚àíŒ±k‚àáf (1)
Œæ1,k(Œ∏k)‚àáf (2)
Œæ2,k(y(1)
k ) . . . ‚àáf (T +1)
ŒæT +1,k(y(T )
k
).
(10)"
LARGE MEMORY CONSUMPTION ISSUE WITH A NAIVE IMPLEMENTATION,0.0712401055408971,"The key idea is to store an auxiliary variable y(i) to maintain the moving average of each composite
function. Since f (l)
Œæl returns the exact values of W (l), ..., W (T ), we only need to maintain a moving"
LARGE MEMORY CONSUMPTION ISSUE WITH A NAIVE IMPLEMENTATION,0.07255936675461741,"average of eZ(l) for each layer. The computation is shown in Figure 1a. The moving average of the
aggregated features is stored in Y (l) with each row for one node. In each iteration, some nodes (rows)
are sampled, and the estimated aggregation results eZ(l) are merged into Y (l) based on Formula (8)
and (9). For the nodes that are not sampled, we simply multiply the corresponding rows of ¬ØZ(l) by
(1 ‚àíŒ≤k). Since the number of rows in Y (l) is the number of nodes in the graph, Y (l) takes a lot
of memory when the graph is large. For example, for training a 3-layer GCN on a graph with two
million nodes, suppose the hidden state dimension dl = 512 and a Ô¨Çoating point has 4 bytes, Y takes
3 √ó 2M √ó 512 √ó 4 = 12GB of memory. All of the existing SCO algorithms need to maintain this
moving average, which impedes their application to large-scale GNN training."
LARGE MEMORY CONSUMPTION ISSUE WITH A NAIVE IMPLEMENTATION,0.07387862796833773,Under review as a conference paper at ICLR 2022
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.07519788918205805,"3
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT"
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.07651715039577836,"To reduce the memory consumption of SCO algorithms for GNN training, we propose a Sparse
Stochastic Compositional (SpSC) gradient method. Instead of storing the moving averages of all
nodes in the graph, we only store the moving averages of nodes that are sampled in the most recent
iterations."
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.07783641160949868,"As shown in Figure 1b, we maintain a Ô¨Åxed size buffer of the moving averages. The buffer is divided
into t chunks with each chunk for the eZ(l) of one iteration. The size of each chunk is ml ¬∑ dl where
ml is the maximum number of the nodes that can be sampled at layer l and dl is the hidden state
dimension. Initially, the buffer is empty. In every iteration, we Ô¨Årst check if the sampled nodes are in
the buffer. For the nodes that are found in the buffer, we collect the corresponding rows of the buffer
and add them to eZ(l)
k
based on Formula (8) and (9). For the nodes that are not found in the buffer, we
multiply the corresponding rows of eZ(l)
k
by Œ≤k. The updated eZ(l)
k
is then written to chunk-(k mod t).
All the other chunks are multiplied by (1 ‚àíŒ≤k). Since the sampled nodes found in the buffer are
updated to chunk-(k mod t), the original values in chunk-0 and chunk-1 are invalidated, as shown
by the shadowed rows in Figure 1b. As the buffer size is a constant (T ¬∑ t ¬∑ ml ¬∑ dl) regardless of the
graph size, our algorithm can be employed to train GNN on very large graphs."
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.079155672823219,"Our algorithm overwrites chunk-(k mod t) in iteration k. The information of the overwritten nodes is
lost. The update of the moving averages can be written as"
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.08047493403693931,"y(l)
k+1 = (1 ‚àíŒ≤k)y(l)
k + Œ≤kf (l)
Œæl,k(y(l‚àí1)
k+1 ) ‚àí k‚àí1
Y"
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.08179419525065963,"j=k‚àít+1
(1 ‚àíŒ≤j)u(l)
k
(11)"
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.08311345646437995,"where
u(l)
k = P(Œæl,k‚àít/(Œæl,k‚àít+1 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œæl,k))y(l)
k‚àít+1.
(12)"
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.08443271767810026,"P(Œæl,k‚àít/(Œæl,k‚àít+1 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œæl,k)) is a matrix with binary values representing the overwritten nodes
in the current iteration k, i.e., the nodes that are sampled in iteration k ‚àít and are not sampled
in the following t iterations. Since we only store the moving averages of nodes that are sampled
in the most recent t iterations, the information of the overwritten nodes is lost. These nodes are
multiplied by (1 ‚àíŒ≤j) in every iteration after iteration k ‚àít. The values of the overwritten rows are
Qk‚àí1
j=k‚àít+1(1 ‚àíŒ≤j)u(l)
k . Our algorithm simply replaces Formula (8) and (9) in the SCGD algorithm
with Formula (11)."
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.08575197889182058,"To study the convergence property of SpSC, we make the following assumptions that are commonly
used in the analysis of SCO algorithms (Yang et al., 2019; Balasubramanian et al., 2020)."
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.0870712401055409,"Assumption 1. The composite functions f (l) are Ll-smooth. That is, for any y and y‚Ä≤, we have
‚à•‚àáf (l)
Œæl (y) ‚àí‚àáf (l)
Œæl (y‚Ä≤)‚à•‚â§Ll‚à•y ‚àíy‚Ä≤‚à•."
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.08839050131926121,"Assumption 2. The stochastic gradients of the composite functions f (l) are bounded in expectation,
i.e., E[‚à•‚àáf (l)
Œæl (y)‚à•2] ‚â§C2
l ."
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.08970976253298153,"Assumption 3. The estimated aggregation results obtained by sampled neighbor aggregation
is unbiased, i.e., E[f (l)
Œæl (y)] = f (l)(y), and the stochastic gradient of f (l) is unbiased, i.e.,"
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.09102902374670185,"E[‚àáf (l)
Œæl (y)] = ‚àáf (l)(y)."
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.09234828496042216,"Following the single-timescale analysis of the algorithm (Balasubramanian et al., 2020), we use large
batches for estimating the composite functions and assume that the estimation variance is small."
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.09366754617414248,"Assumption 4. The estimated aggregation results have small bounded variance, i.e., E[‚à•f (l)
Œæl,k(y) ‚àí
f (l)(y)‚à•] ‚â§Œ≤kV 2."
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.09498680738786279,"This is a reasonable assumption for GNN training on GPUs as we always sample a batch of nodes for
neighbor aggregation to achieve better utilization of the GPU parallelism."
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.09630606860158311,"In additional to the conventional assumptions, we make an assumption on the moving averages."
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.09762532981530343,"Assumption 5. The moving average of the aggregated features are bounded, i.e., E[‚à•y(l)‚à•2] ‚â§D2."
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.09894459102902374,Under review as a conference paper at ICLR 2022
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.10026385224274406,The convergence rate of our algorithm is summarized in the following theorem.
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.10158311345646438,"Theorem 1. Under Assumptions 1-5, if we set Œ≤
‚â§
1 ‚àí2(‚àí1/(2T ‚àí1)) and Œ±
=
Œ≤ ¬∑"
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.10290237467018469,"min

1
2C1T q"
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.10422163588390501,2(1‚àíŒ≤)2T ‚àí1‚àí1
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.10554089709762533,"Cmax
,
2
PT
l=1 A2
l +16CmaxC2
1T"
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.10686015831134564,"
, the model parameters {Œ∏k} of our training"
SPARSE STOCHASTIC COMPOSITIONAL GRADIENT DESCENT,0.10817941952506596,algorithm with (11) for updating sparse moving averages satisfy
K,0.10949868073878628,"1
K K‚àí1
X"
K,0.11081794195250659,"k=0
E [‚à•‚àáF(Œ∏k)‚à•]2 ‚â§O(Œ≤) + 4CmaxT 2
(1 + Cmax)(1 ‚àíŒ≤)2(t‚àí1)"
K,0.11213720316622691,"Œ±Œ≤2
+ (1 ‚àíŒ≤)2t+1 TŒ±Œ≤2 
D2 (13)"
K,0.11345646437994723,"where Cmax = max(C2
2C2
3 ¬∑ ¬∑ ¬∑ C2
l , C2
3C2
4 ¬∑ ¬∑ ¬∑ C2
l , ¬∑ ¬∑ ¬∑ , C2
l , 1) for l = 2 . . . T, t is the number of
buffer chunks used in our algorithm."
K,0.11477572559366754,"The last term on the RHS of (13) reveals how the convergence of the SCGD algorithm is affected by
the sparse moving averages. The larger t we use (i.e., the more chunks we have in the buffer), the
smaller (1 ‚àíŒ≤)2(t‚àí1) and (1 ‚àíŒ≤)2t+1we have, and the faster convergence we achieve. In theory, if
we can make (1 ‚àíŒ≤)2(t‚àí1) = O(Œ≤4), the algorithm will achieve O(
p"
K,0.11609498680738786,"1/K) convergence rate. As
Œ≤ ‚Üí0, we need larger t to maintain the convergence rate, and eventually, we will need to store the
moving average of all nodes in the graph."
K,0.11741424802110818,"Applying Sparse Moving Averages to SCSC. Our sparse moving average can also be applied to
other SCO algorithms. For example, the Stochastically Corrected Stochastic Compositional gradient
method (SCSC) (Chen et al., 2020) has a correction term in the update of the moving averages.
Because of the correction term, SCSC needs a relaxed assumption on the estimation error of the
composite functions. More speciÔ¨Åcally, if we change (11) to"
K,0.11873350923482849,"y(l)
k+1 = (1 ‚àíŒ≤k)y(l)
k + f (l)
Œæl,k(y(l‚àí1)
k+1 ) ‚àí(1 ‚àíŒ≤k)f (l)
Œæl,k(y(l‚àí1)
k
) ‚àí k‚àí1
Y"
K,0.12005277044854881,"j=k‚àít+1
(1 ‚àíŒ≤j)u(l)
k
(14)"
K,0.12137203166226913,"and replace (8) and (9) with (14), we can relax Assumption 4 as"
K,0.12269129287598944,"Assumption 6. The estimated aggregation results have bounded variance, i.e., E[‚à•f (l)
Œæl,k(y) ‚àí
f (l)(y)‚à•] ‚â§V 2."
K,0.12401055408970976,The convergence rate of SCSC with sparse moving averages is summarized as follows.
K,0.12532981530343007,"Theorem 2. Under Assumptions 1-3 and 5-6, if we choose Œ±k = Œ± =
cŒ±
‚àö"
K,0.1266490765171504,"K and Œ≤k = Œ≤ =
cŒ≤
‚àö"
K,0.1279683377308707,"K , the
model parameters {Œ∏k} of SCSC with (14) for updating sparse moving averages satisfy"
K,0.12928759894459102,"1
K K‚àí1
X"
K,0.13060686015831136,"k=0
E[‚à•‚àáF(Œ∏k)‚à•2] ‚â§O (Œ≤) + 2(1 + Œ≤) (1 ‚àíŒ≤)2(t‚àí1) Œ±Œ≤ T
X"
K,0.13192612137203166,"l=1
D2
l"
K,0.13324538258575197,"+ 6 (1 ‚àíŒ≤)2(t‚àí1) Œ±
Cu T
X"
K,0.1345646437994723,"l=1
D2
l
(15)"
K,0.1358839050131926,"where Cu = max
 
4C2
l + Œ≥l

for l = 1 . . . T."
K,0.13720316622691292,"The result suggests that, if we can set t such that (1 ‚àíŒ≤)2(t‚àí1) = O(Œ≤3), the algorithm will achieve
O(
p"
K,0.13852242744063326,1/K) convergence rate.
IMPLEMENTATION DETAILS,0.13984168865435356,"4
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.14116094986807387,"Algorithm 1 describes an implementation of Formula (11) in our algorithm. For each layer, we
allocate a buffer (buf) of size tml √ó dl for the moving averages. The buffered nodes are maintained
in a list node list ‚ààRtml where node list[i] is the index of the node stored at buf[i]. If buf[i] is
empty, node list[i] is set to -1. In every iteration k, we Ô¨Årst get the location of chunk-(k mod t).
Then, we look up each of the sampled nodes in the buffer (line 3). The LookUp function computes"
IMPLEMENTATION DETAILS,0.1424802110817942,Under review as a conference paper at ICLR 2022
IMPLEMENTATION DETAILS,0.1437994722955145,Algorithm 1: Updating sparse moving average of aggregated features at layer l in iteration k
IMPLEMENTATION DETAILS,0.14511873350923482,"Input: Sampled nodes S, Buffered nodes node list ‚ààRtml, buf (l) ‚ààRtml√ódl, eZ(l)
k
‚ààR|S|√ódl
// Get the location of chunk-(k mod t)"
IMPLEMENTATION DETAILS,0.14643799472295516,1 start = (k mod t) ‚àóml;
IMPLEMENTATION DETAILS,0.14775725593667546,2 end = chunk start + |S|;
IMPLEMENTATION DETAILS,0.14907651715039577,// Look up the sampled nodes in the buffer
IMPLEMENTATION DETAILS,0.1503957783641161,"3 idx in buf, idx in z = LookUp(S, node list);"
IMPLEMENTATION DETAILS,0.1517150395778364,// Update the moving average for the sampled nodes
IMPLEMENTATION DETAILS,0.15303430079155672,"4 eZ(l)
k
= Œ≤k ‚àóeZ(l)
k ;
eZ(l)
k [idx in z] = (1 ‚àíŒ≤k) ‚àóbuf (l)[idx in buf] + eZ(l)
k [idx in z];
// Update the moving average for all buffered nodes"
IMPLEMENTATION DETAILS,0.15435356200527706,"5 buf (l) = (1 ‚àíŒ≤k) ‚àóbuf (l); buf (l)[start : end] = eZ(l)
k ;
// Invalidate the old buffer for the sampled nodes"
IMPLEMENTATION DETAILS,0.15567282321899736,6 node list[idx in buf] = ‚àí1;
IMPLEMENTATION DETAILS,0.15699208443271767,// Add the sampled nodes to node list
IMPLEMENTATION DETAILS,0.158311345646438,7 node list[start : end] = S;
IMPLEMENTATION DETAILS,0.15963060686015831,"the intersection of S and node list and returns the indices of the overlapping nodes in the two arrays.
If a sampled node is not found in the buffer, we multiply the corresponding row of eZ(l)
k
by Œ≤k. If a
sampled node is in the buffer, we read in its current moving average and update the corresponding row
of eZ(l)
k
(line 4). For buffered nodes that are not sampled, we simply multiply their moving averages
by (1 ‚àíŒ≤k) (line 5). For buffered nodes that are sampled, we invalidate their original buffer by setting
node list[idx in buf] to ‚àí1 (line 6). Last, we add the sampled nodes to the node list."
IMPLEMENTATION DETAILS,0.16094986807387862,"Most of the operations in Algorithm 1 are simple vector operations, and they incur little overhead. The
performance bottleneck is the LookUp function. With a naive implementation, it has O(tml log |S|)
time complexity, assuming S is sorted. In our implementation, we use an auxiliary array node loc ‚àà
RN to store the locations of all nodes in the buffer and accelerate the LookUp function. SpeciÔ¨Åcally, if
node-i is in the buffer, we store its location in buffer in node loc[i]; otherwise, node loc[i] is set to -1.
With the auxiliary array, the idx in z can be obtained by comparing node loc[S] with zero, and the
idx in buf is simply node loc[S][idx in z]. Before updating the node list at line 7 of Algorithm 1,
we remove the overwritten nodes from node loc by setting node loc[node list[start : end]] to -1.
Finally, we store the locations of the newly sampled nodes to node loc by setting node loc[S] to
[start, start + 1, . . . , end]. It is easy to see that all these operations have O(|S|) time complexity."
EVALUATION,0.16226912928759896,"5
EVALUATION
5.1
EXPERIMENTAL SETUP"
EVALUATION,0.16358839050131926,"We conduct our experiments on a workstation with an Nvidia RTX 3090 GPU, an Intel Xeon Gold
6226R CPU, and 512GB RAM. Our code is implemented with PyTorch 1.8.0 and PyTorch Geometric
1.7.0."
EVALUATION,0.16490765171503957,"We evaluate our algorithm on Ô¨Åve graphs as listed in Table 1. The reddit and yelp graph are
adopted from GraphSAINT (Zeng et al., 2020), and the arxiv, proteins, products are from
the Open Graph Benchmark (Hu et al., 2020)."
EVALUATION,0.1662269129287599,"We apply our algorithm to two GNN models: GCN (Kipf & Welling, 2017) and GraphSAGE (Hamil-
ton et al., 2017). Both models have three convolutional layers. We use Formula (11) for updating
the moving average instead of Formula (14). This is because Formula (14) requires two forward
passes which incurs extra overheads. The algorithm is run for 50 epochs. We set Œ≤ to 0.2 initially,
and decrease it to 0.1 at epoch 20, and further decrease it to 0.05 at epoch 40. The number of buffered
chunks (t) is set to 8. We adopt the layer-wise sampling method in Zou et al. (2019) for neighbor
sampling. The batch size is set to 4096, and the number of sampled neighbors in each layer is set to
8192."
TRAINING RESULTS,0.16754617414248021,"5.2
TRAINING RESULTS"
TRAINING RESULTS,0.16886543535620052,"Validation Accuracy. Figure 2a shows the validation accuracy of different algorithms for training
a GCN on arxiv. We can see that full neighbor aggregation (Adam Full) achieves the highest"
TRAINING RESULTS,0.17018469656992086,Under review as a conference paper at ICLR 2022
TRAINING RESULTS,0.17150395778364116,Table 1: Graph datasets (‚Äôm‚Äô stands for multi-label classiÔ¨Åcation).
TRAINING RESULTS,0.17282321899736147,"reddit
yelp
arxiv
proteins
products
#nodes
233K
717K
169K
132K
2.4M
#edges
11.6M
7.0M
1.2M
79M
123M
#classes
41
100 (m)
40
112 (m)
47"
TRAINING RESULTS,0.1741424802110818,"0
10
20
30
40
50
Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
TRAINING RESULTS,0.17546174142480211,Validation Accuracy
TRAINING RESULTS,0.17678100263852242,(a) arxiv GCN
TRAINING RESULTS,0.17810026385224276,"0
10
20
30
40
50
Epoch 0.5 0.6 0.7 0.8 0.9"
TRAINING RESULTS,0.17941952506596306,Validation Accuracy
TRAINING RESULTS,0.18073878627968337,(b) reddit GraphSAGE
TRAINING RESULTS,0.1820580474934037,"0
10
20
30
40
50
Epoch 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80"
TRAINING RESULTS,0.18337730870712401,Validation Accuracy
TRAINING RESULTS,0.18469656992084432,(c) proteins GraphSAGE
TRAINING RESULTS,0.18601583113456466,"0
10
20
30
40
50
Epoch 0.82 0.84 0.86 0.88 0.90"
TRAINING RESULTS,0.18733509234828497,Validation Accuracy
TRAINING RESULTS,0.18865435356200527,(d) products GCN
TRAINING RESULTS,0.18997361477572558,"0
10
20
30
40
50
Epoch 0.3 0.4 0.5 0.6"
TRAINING RESULTS,0.19129287598944592,Validation Accuracy
TRAINING RESULTS,0.19261213720316622,(e) yelp GraphSAGE
TRAINING RESULTS,0.19393139841688653,Sparse_SCO
TRAINING RESULTS,0.19525065963060687,Adam_Full SCO
TRAINING RESULTS,0.19656992084432717,Adam_Sample
TRAINING RESULTS,0.19788918205804748,Figure 2: Validation accuracy over epochs.
TRAINING RESULTS,0.19920844327176782,"accuracy. This is reasonable because full neighbor aggregation returns unbiased estimates of gra-
dients. If neighbor sampling is used, the accuracy clearly drops with the same training algorithm
(Adam Sample). Our algorithm (Sparse SCO) is able to improve the convergence of sampling-based
GNN training and achieves almost the same accuracy as full neighbor aggregation. The results on
reddit and yelp are similar. On products graph, Adam Full runs out of memory, so we only
show the results of sampling-based training in Figure 2d. Interestingly, we Ô¨Ånd that our algorithm
with neighbor sampling achieves even higher accuracy than Adam Full on proteins graph, as
shown in Figure 2c. The original SCO algorithm (which stores the moving averages for all nodes
in the graph) also has lower accuracy than Sparse SCO. This is probably due to the overÔ¨Åtting of
models by Adam Full and SCO."
TRAINING RESULTS,0.20052770448548812,"Training Loss.
Figure 3 shows the training loss of different algorithms on different graphs.
For reddit and yelp, we are able to run full neighbor aggregation on our GPU. As expected,
Adam Full achieves the smallest training loss. Adam Sample, however, has the slowest convergence.
SCO achieves training loss close to Adam Full. We run our algorithm with different t‚Äôs. The larger t
we use, the smaller training loss we obtain. The results are consistent with our theoretical analysis and
also suggesting that the poor accuracy of the original SCO algorithm is probably due to overÔ¨Åtting.
For products graph, we are not able to run full neighbor aggregation. The results show a clearly
faster convergence of our algorithm than Adam SGD for sampling-based training."
TRAINING RESULTS,0.20184696569920843,"Test Accuracy. Table 2 lists the test accuracy of the models trained by different algorithms. For
reddit and yelp, we follow the GraphSAINT paper (Zeng et al., 2020) and report the F1-micro
score. For proteins, we follow the OGB (Hu et al., 2020) and report the ROC-AUC. We can see
that our training algorithm achieves the highest test accuracy for both GCN and GraphSAGE on
almost all the graphs. We do not include the results for GCN on yelp graph because its accuracy
is apparently lower than GraphSAGE with all training algorithms, probably due to the limited
expressiveness of the GCN model. While it is hard to draw a direct comparison with GraphSAINT
because the sampling methods and the model architectures are different, our test accuracy on reddit
and yelp matches the best accuracy reported by GraphSAINT (Zeng et al., 2020). It is worth noting
that our algorithm achieves higher accuracy than Adam SGD with both full neighbor aggregation and
neighbor sampling on proteins and products graph. The numbers are higher than the accuracy"
TRAINING RESULTS,0.20316622691292877,Under review as a conference paper at ICLR 2022
TRAINING RESULTS,0.20448548812664907,"0
10
20
30
40
50
Epoch 0.0 0.5 1.0 1.5 2.0"
TRAINING RESULTS,0.20580474934036938,Training Loss
TRAINING RESULTS,0.20712401055408972,"SCO
Adam_Sample
Adam_Full
Sparse_SCO (t=8)
Sparse_SCO (t=16)
44
46
48 0.05 0.10"
TRAINING RESULTS,0.20844327176781002,(a) reddit GraphSAGE
TRAINING RESULTS,0.20976253298153033,"0
10
20
30
40
50
Epoch 0.14 0.16 0.18 0.20 0.22 0.24 0.26"
TRAINING RESULTS,0.21108179419525067,Training Loss
TRAINING RESULTS,0.21240105540897097,"SCO
Adam_Sample
Adam_Full
Sparse_SCO (t=8)
Sparse_SCO (t=16)
Sparse_SCO (t=32)
44
46
48
0.140 0.145 0.150"
TRAINING RESULTS,0.21372031662269128,(b) yelp GraphSAGE
TRAINING RESULTS,0.21503957783641162,"0
10
20
30
40
50
Epoch 0.4 0.6 0.8 1.0 1.2"
TRAINING RESULTS,0.21635883905013192,Training Loss
TRAINING RESULTS,0.21767810026385223,"SCO
Adam_Sample
Sparse_SCO (t=8)
Sparse_SCO (t=16)
Sparse_SCO (t=32)
44
46
48 0.30 0.35"
TRAINING RESULTS,0.21899736147757257,(c) products GCN
TRAINING RESULTS,0.22031662269129287,Figure 3: Training loss over epochs.
TRAINING RESULTS,0.22163588390501318,"Table 2: Test accuracy of models trained by different algorithms on different graphs (‚Äô-‚Äô means not
available due to out of memory)."
TRAINING RESULTS,0.22295514511873352,(a) GCN
TRAINING RESULTS,0.22427440633245382,"reddit
arxiv
proteins
products
Adam Full
0.961
0.712
0.749
-
Adam Sample
0.957
0.663
0.726
0.790
SCO
0.945
0.698
0.744
0.773
Sparse SCO
0.961
0.711
0.770
0.802"
TRAINING RESULTS,0.22559366754617413,(b) GraphSAGE
TRAINING RESULTS,0.22691292875989447,"reddit
yelp
arxiv
proteins
products
Adam Full
0.963
0.632
0.714
0.758
-
Adam Sample
0.956
0.631
0.685
0.718
0.787
SCO
0.913
0.628
0.644
0.717
-
Sparse SCO
0.966
0.651
0.713
0.779
0.801"
TRAINING RESULTS,0.22823218997361477,"of the same models reported in OGB Leaderboards (ogb, 2021). The results suggest that there is an
improvement space for the accuracy of GNN models by using better training algorithms."
TRAINING RESULTS,0.22955145118733508,"Execution Time. Table 3 lists the time per epoch of different training algorithms on different graphs.
Although full neighbor aggregation achieves good convergence in many cases, it incurs a much large
computation overhead than sampled neighbor aggregation. The execution time of Adam Full is 7x to
55x longer than that of Adam Sample. Compared with Adam Sample, the SCO algorithm incurs a
small overhead for updating the moving average of aggregation results. Our algorithm runs slightly
slower than SCO because the LookUp operation in Algorithm 1 incurs an extra overhead. However,
the execution time is still much smaller than full neighbor aggregation."
TRAINING RESULTS,0.23087071240105542,"Memory Consumption.
Figure 4 shows the memory consumption of different algorithms. We
collect the numbers by calling the max memory allocated function in PyTorch at the end of the
Ô¨Årst epoch. For arxiv, reddit, proteins, and yelp, full neighbor aggregation takes much
larger memory space than sampled aggregation. SCO and Sparse SCO require additional memory for
storing the moving averages. Because we only store the moving average of nodes sampled in recent
iterations, Sparse SCO uses less memory than SCO. On products graph, SCO requires extremely
large memory due to the massive number of nodes, while our Sparse SCO can run with only 2GB of
GPU memory."
RELATED WORK,0.23218997361477572,"6
RELATED WORK"
RELATED WORK,0.23350923482849603,"To overcome the scalability limitation of GNN training, various neighbor sampling methods have
been proposed, including node-wise sampling (Hamilton et al., 2017; Ying et al., 2018), layer-wise
sampling (Chen et al., 2018; Huang et al., 2018; Zou et al., 2019), and subgraph sampling (Zeng
et al., 2020; Chiang et al., 2019). These sampling-based training methods achieve good accuracy in
practice (particularly on small graphs), but they lack theoretical justiÔ¨Åcation."
RELATED WORK,0.23482849604221637,Under review as a conference paper at ICLR 2022
RELATED WORK,0.23614775725593667,"Table 3: Time per epoch of different training algorithms in seconds (‚Äô-‚Äô means not available due to
out of memory)."
RELATED WORK,0.23746701846965698,(a) GCN
RELATED WORK,0.23878627968337732,"reddit
arxiv
proteins
products
Adam Full
28.4
1.97
20.65
-
Adam Sample
0.53
0.28
0.55
1.01
SCO
0.72
0.37
0.67
2.31
Sparse SCO
0.82
0.51
0.81
1.24"
RELATED WORK,0.24010554089709762,(b) GraphSAGE
RELATED WORK,0.24142480211081793,"reddit
yelp
arxiv
proteins
products
Adam Full
45.29
23.07
4.10
46.16
-
Adam Sample
0.82
2.38
0.50
0.93
1.47
SCO
1.09
3.61
0.61
1.03
-
Sparse SCO
1.26
2.93
0.85
1.26
1.87"
RELATED WORK,0.24274406332453827,"0
0.5
1
1.5
2
2.5
3"
RELATED WORK,0.24406332453825857,"Adam (Full)
Adam (Sample)
SCO
Sparse SCO"
RELATED WORK,0.24538258575197888,GPU Memory Cost (GB)
RELATED WORK,0.24670184696569922,"(a) arxiv
GCN"
RELATED WORK,0.24802110817941952,"0
2
4
6
8
10
12
14
16"
RELATED WORK,0.24934036939313983,"Adam (Full)
Adam (Sample)
SCO
Sparse SCO"
RELATED WORK,0.25065963060686014,"(b) reddit
GraphSAGE"
RELATED WORK,0.2519788918205805,"0
5
10
15
20
25"
RELATED WORK,0.2532981530343008,"Adam (Full)
Adam (Sample)
SCO
Sparse SCO"
RELATED WORK,0.2546174142480211,"(c) proteins
GraphSAGE"
RELATED WORK,0.2559366754617414,"0
2
4
6
8
10
12"
RELATED WORK,0.25725593667546176,Adam (Sample) SCO
RELATED WORK,0.25857519788918204,Sparse SCO
RELATED WORK,0.2598944591029024,"(d) products
GCN"
RELATED WORK,0.2612137203166227,"0
1
2
3
4
5"
RELATED WORK,0.262532981530343,"Adam (Full)
Adam (Sample)
SCO
Sparse SCO"
RELATED WORK,0.2638522427440633,"(e) yelp
GraphSAGE"
RELATED WORK,0.26517150395778366,Figure 4: GPU memory consumption of different algorithms.
RELATED WORK,0.26649076517150394,"Some recent works (Cong et al., 2020; 2021) point out that sampling-based GNN training is actually
multi-level Stochastic Compositional Optimization (SCO). However, they either use this connection
to justify their sampling techniques and fall back to Adam SGD for training (Cong et al., 2020), or
they directly adopt an SCO algorithm without considering the large memory consumption issue (Cong
et al., 2021)."
RELATED WORK,0.2678100263852243,"The research on SCO traces back to (Ermoliev, 1976) where a two-timescale stochastic approximation
scheme was proposed for two-level problems. Various SCO algorithms and convergence analyses
have been proposed ever since (Wang et al., 2017a;b; Lian et al., 2017; Yang et al., 2019; Zhang &
Xiao, 2019; Chen et al., 2020; Balasubramanian et al., 2020; Ghadimi et al., 2020; Hu et al., 2019;
Lin et al., 2018). Despite a substantial volume of work on SCO in recent years, none of the existing
work has considered the large memory consumption issue and the data movement overhead of the
algorithms. Our work is the Ô¨Årst to establish a convergence analysis for SCO algorithms with sparse
moving averages."
CONCLUSION,0.2691292875989446,"7
CONCLUSION"
CONCLUSION,0.2704485488126649,"In this work, we propose a new variant of SCO algorithm for training graph neural networks on
large graphs. Our main idea is to maintain a sparse moving average of the aggregation results in
each convolutional layer. We study the convergence property of our algorithm and show that the
algorithm can achieve O(
p"
CONCLUSION,0.2717678100263852,"1/K) convergence rate when a sufÔ¨Åcient amount of moving averages
are maintained. Our experiments with two GNN models on different graphs validate our theoretical
results and show a clear advantage of our algorithm against Adam SGD for GNN training."
CONCLUSION,0.27308707124010556,Under review as a conference paper at ICLR 2022
REFERENCES,0.27440633245382584,REFERENCES
REFERENCES,0.2757255936675462,"Ogb leaderboards for node classiÔ¨Åcation, 2021. URL https://ogb.stanford.edu/docs/
leader_nodeprop/."
REFERENCES,0.2770448548812665,"Krishnakumar Balasubramanian, Saeed Ghadimi, and Anthony Nguyen. Stochastic multi-level
composition optimization algorithms with level-independent convergence rates. arXiv preprint
arXiv:2008.10526, 2020."
REFERENCES,0.2783641160949868,"Jie Chen, Tengfei Ma, and Cao Xiao. FastGCN: Fast learning with graph convolutional networks via
importance sampling. In International Conference on Learning Representations, 2018."
REFERENCES,0.2796833773087071,"Tianyi Chen, Yuejiao Sun, and Wotao Yin. Solving stochastic compositional optimization is nearly
as easy as solving stochastic optimization. arXiv preprint arXiv:2008.10847, 2020."
REFERENCES,0.28100263852242746,"Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efÔ¨Åcient algorithm for training deep and large graph convolutional networks. In Proceedings of
the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
257‚Äì266, 2019."
REFERENCES,0.28232189973614774,"Weilin Cong, Rana Forsati, Mahmut Kandemir, and Mehrdad Mahdavi. Minimal variance sampling
with provable guarantees for fast training of graph neural networks. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1393‚Äì1403,
2020."
REFERENCES,0.2836411609498681,"Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On the importance of sampling in learning
graph convolutional networks. arXiv preprint arXiv:2103.02696, 2021."
REFERENCES,0.2849604221635884,"Alberto Garcia Duran and Mathias Niepert. Learning graph representations with embedding propaga-
tion. In Advances in neural information processing systems, pp. 5119‚Äì5130, 2017."
REFERENCES,0.2862796833773087,"Yu. Ermoliev. Methods of stochastic programming. Monographs in Optimization and OR, 1976. In
Russian."
REFERENCES,0.287598944591029,"Saeed Ghadimi, Andrzej Ruszczynski, and Mengdi Wang. A single timescale stochastic approxima-
tion method for nested stochastic optimization. SIAM Journal on Optimization, 30(1):960‚Äì979,
2020."
REFERENCES,0.28891820580474936,"Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in neural information processing systems, pp. 1024‚Äì1034, 2017."
REFERENCES,0.29023746701846964,"Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv
preprint arXiv:2005.00687, 2020."
REFERENCES,0.29155672823219,"Wenqing Hu, Chris Junchi Li, Xiangru Lian, Ji Liu, and Huizhuo Yuan. EfÔ¨Åcient smooth non-convex
stochastic compositional optimization via stochastic recursive gradient descent. In Advances in
Neural Information Processing Systems, pp. 6929‚Äì6937, 2019."
REFERENCES,0.2928759894459103,"Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph
representation learning. In Advances in neural information processing systems, pp. 4558‚Äì4567,
2018."
REFERENCES,0.2941952506596306,"Thomas N. Kipf and Max Welling. Semi-supervised classiÔ¨Åcation with graph convolutional networks.
In International Conference on Learning Representations (ICLR), 2017."
REFERENCES,0.2955145118733509,"Ruoyu Li, Sheng Wang, Feiyun Zhu, and Junzhou Huang. Adaptive graph convolutional neural
networks. In AAAI Conference on ArtiÔ¨Åcial Intelligence, 2018."
REFERENCES,0.29683377308707126,"Xiangru Lian, Mengdi Wang, and Ji Liu. Finite-sum composition optimization via variance reduced
gradient descent. In ArtiÔ¨Åcial Intelligence and Statistics, pp. 1159‚Äì1167, 2017."
REFERENCES,0.29815303430079154,"Tianyi Lin, Chenyou Fan, and Mengdi Wang. Improved oracle complexity of variance reduced
methods for nonsmooth convex stochastic composition optimization. arXiv, pp. arXiv‚Äì1802, 2018."
REFERENCES,0.2994722955145119,Under review as a conference paper at ICLR 2022
REFERENCES,0.3007915567282322,"Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms
for minimizing compositions of expected-value functions. Mathematical Programming, 161(1-2):
419‚Äì449, 2017a."
REFERENCES,0.3021108179419525,"Mengdi Wang, Ji Liu, and Ethan X Fang. Accelerating stochastic composition optimization. The
Journal of Machine Learning Research, 18(1):3721‚Äì3743, 2017b."
REFERENCES,0.3034300791556728,"Shuoguang Yang, Mengdi Wang, and Ethan X Fang. Multilevel stochastic gradient methods for
nested composition optimization. SIAM Journal on Optimization, 29(1):616‚Äì659, 2019."
REFERENCES,0.30474934036939316,"Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. In Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
974‚Äì983, 2018."
REFERENCES,0.30606860158311344,"Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-
saint: Graph sampling based inductive learning method. In International Conference on Learning
Representations, 2020."
REFERENCES,0.3073878627968338,"Junyu Zhang and Lin Xiao. Multi-level composite stochastic optimization via nested variance
reduction. arXiv preprint arXiv:1908.11468, 2019."
REFERENCES,0.3087071240105541,"Muhan Zhang and Yixin Chen. Weisfeiler-lehman neural machine for link prediction. In Proceedings
of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pp. 575‚Äì583, 2017."
REFERENCES,0.3100263852242744,"Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Advances in
Neural Information Processing Systems, pp. 5165‚Äì5175, 2018."
REFERENCES,0.3113456464379947,"Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li,
and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv preprint
arXiv:1812.08434, 2018."
REFERENCES,0.31266490765171506,"Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent
importance sampling for training deep and large graph convolutional networks. In Advances in
Neural Information Processing Systems, pp. 11249‚Äì11259, 2019."
REFERENCES,0.31398416886543534,Under review as a conference paper at ICLR 2022
APPENDIX,0.3153034300791557,"8
APPENDIX"
APPENDIX,0.316622691292876,"Link
to
our
source
code:
https://anonymous.4open.science/r/iclr2022_
artifact-0FE6/"
APPENDIX,0.3179419525065963,"8.1
PROOF TO THEOREM 1"
APPENDIX,0.31926121372031663,"Based on the discussion in Section 3, our proposed algorithm can be written as"
APPENDIX,0.32058047493403696,"y(1)
k+1 = (1 ‚àíŒ≤k)y(1)
k
+ Œ≤kf (1)
Œæ1,k(Œ∏k) ‚àí k‚àí1
Y"
APPENDIX,0.32189973614775724,"j=k‚àít+1
(1 ‚àíŒ≤j)u(1)
k ,
(16)"
APPENDIX,0.3232189973614776,"y(l)
k+1 = (1 ‚àíŒ≤k)y(l)
k + Œ≤kf (l)
Œæl,k(y(l‚àí1)
k+1 ) ‚àí k‚àí1
Y"
APPENDIX,0.3245382585751979,"j=k‚àít+1
(1 ‚àíŒ≤j)u(l)
k ,
2 ‚â§l ‚â§T,
(17)"
APPENDIX,0.3258575197889182,"Œ∏k+1 = Œ∏k ‚àíŒ±k‚àák,
(18)"
APPENDIX,0.32717678100263853,"with
u(l)
k = P(Œæl,k‚àít/(Œæl,k‚àít+1 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œæl,k))y(l)
k+t‚àí1
(19)
and
‚àák = ‚àáf (1)
Œæ1,k(Œ∏k) ¬∑ ¬∑ ¬∑ ‚àáf (T )
ŒæT,k(y(T ‚àí1)
k+1
)‚àáf (T +1)
ŒæT +1,k(y(T )
k+1).
(20)"
SUPPORTING LEMMA,0.32849604221635886,"8.1.1
SUPPORTING LEMMA"
SUPPORTING LEMMA,0.32981530343007914,"Lemma 3. The change of y(l)
k in every iteration is bounded, i.e.,"
SUPPORTING LEMMA,0.3311345646437995,"E
y(1)
k+1 ‚àíy(1)
k

2
‚â§
Œ≤2
k
1 ‚àíŒ≤k
E
f (1) (Œ∏k) ‚àíy(1)
k+1

2
+ 1 Œ≤k k‚àí1
Y"
SUPPORTING LEMMA,0.3324538258575198,"j=k‚àít+1
(1 ‚àíŒ≤j)2 E
u(1)
k

2
+ Œ≤3
kV 2"
SUPPORTING LEMMA,0.3337730870712401,"(l = 1)
(21)
and"
SUPPORTING LEMMA,0.33509234828496043,"E
y(l)
k+1 ‚àíy(l)
k

2
‚â§
Œ≤2
k
1 ‚àíŒ≤k
E
f (l) 
y(l‚àí1)
k+1

‚àíy(l)
k+1

2
+ 1 Œ≤k k‚àí1
Y"
SUPPORTING LEMMA,0.33641160949868076,"j=k‚àít+1
(1 ‚àíŒ≤j)2 E
u(l)
k

2
+ Œ≤3
kV 2"
SUPPORTING LEMMA,0.33773087071240104,"(l ‚â§2)
(22)"
SUPPORTING LEMMA,0.3390501319261214,"Proof. According to (17), we have

y(l)
k+1 ‚àíy(l)
k

=Œ≤k

f (l)
Œæl,k"
SUPPORTING LEMMA,0.3403693931398417,"
y(l‚àí1)
k+1

‚àíy(l)
k

‚àí k‚àí1
Y"
SUPPORTING LEMMA,0.341688654353562,"j=k‚àít+1
(1 ‚àíŒ≤j) u(l)
k"
SUPPORTING LEMMA,0.34300791556728233,"=Œ≤k

f (l) 
y(l‚àí1)
k+1

‚àíy(l)
k

+ Œ≤k

f (l)
Œæl,k(y(l‚àí1)
k+1 ) ‚àíf (l)(y(l‚àí1)
k+1 )

‚àí k‚àí1
Y"
SUPPORTING LEMMA,0.34432717678100266,"j=k‚àít+1
(1 ‚àíŒ≤j) u(l)
k"
SUPPORTING LEMMA,0.34564643799472294,"(23)
Taking the square of both sides and using Assumption 4, we have"
SUPPORTING LEMMA,0.3469656992084433,"E
y(l)
k+1 ‚àíy(l)
k

2
= E Ô£Æ Ô£ØÔ£∞"
SUPPORTING LEMMA,0.3482849604221636,"Œ≤k

f (l) 
y(l‚àí1)
k+1

‚àíy(l)
k

‚àí k‚àí1
Y"
SUPPORTING LEMMA,0.3496042216358839,"j=k‚àít+1
(1 ‚àíŒ≤j) u(l)
k  2Ô£π"
SUPPORTING LEMMA,0.35092348284960423,"Ô£∫Ô£ª+ Œ≤3
kV 2"
SUPPORTING LEMMA,0.35224274406332456,"‚â§
Œ≤2
k
1 ‚àíŒ≤k
E
f (l) 
y(l‚àí1)
k+1

‚àíy(l)
k+1

2
+ 1 Œ≤k k‚àí1
Y"
SUPPORTING LEMMA,0.35356200527704484,"j=k‚àít+1
(1 ‚àíŒ≤j)2 E
u(l)
k

2
+ Œ≤3
kV 2 (24)"
SUPPORTING LEMMA,0.3548812664907652,Under review as a conference paper at ICLR 2022
SUPPORTING LEMMA,0.3562005277044855,"Lemma 4. The difference between the stochastic gradient and the true gradient is bounded, i.e.,"
SUPPORTING LEMMA,0.3575197889182058,‚à•E [‚àák|Fk] ‚àí‚àáF (Œ∏k)‚à•‚â§
SUPPORTING LEMMA,0.35883905013192613,"T +1
X l=2 l‚àí1
X"
SUPPORTING LEMMA,0.36015831134564646,"m=1
Am,lE
hy(m)
k+1 ‚àíf (m) 
y(m‚àí1)
k+1
 |Fk
i := T
X"
SUPPORTING LEMMA,0.36147757255936674,"l=1
AlE
hy(l)
k+1 ‚àíf (l) 
y(l‚àí1)
k+1
 |Fk
i
(25)"
SUPPORTING LEMMA,0.3627968337730871,"where Al := PT
m=l+1 Al,m."
SUPPORTING LEMMA,0.3641160949868074,"This lemma is commonly used in the analysis of SCO algorithms Chen et al. (2020); Yang et al.
(2019). Proof can be found in Chen et al. (2020), page 20-23."
SUPPORTING LEMMA,0.3654353562005277,"Lemma 5. The difference between the composite function and its moving average satisÔ¨Åes K‚àí1
X k=0 T
X"
SUPPORTING LEMMA,0.36675461741424803,"l=1
E
y(l)
k+1 ‚àíf (l) 
y(l‚àí1)
k+1

2"
SUPPORTING LEMMA,0.36807387862796836,"‚â§4CmaxC2
1TŒ±2 Œ≤2 K‚àí1
X"
SUPPORTING LEMMA,0.36939313984168864,"k=0
E
h
‚à•‚àáF(Œ∏k)‚à•2i"
SUPPORTING LEMMA,0.370712401055409,"+ 2CmaxT 2K
(1 ‚àíŒ≤)2t‚àí1"
SUPPORTING LEMMA,0.3720316622691293,"Œ≤3
+ Cmax(1 ‚àíŒ≤)2(t‚àí1)"
SUPPORTING LEMMA,0.3733509234828496,"Œ≤3
+ (1 ‚àíŒ≤)2t+1 TŒ≤3 
D2"
SUPPORTING LEMMA,0.37467018469656993,"+ 2CmaxT 2K

Œ≤Cmax + Œ≤2(1 ‚àíŒ≤)(1 + T) T"
SUPPORTING LEMMA,0.3759894459102902,"
V 2,
(26)"
SUPPORTING LEMMA,0.37730870712401055,"where Cmax = max(C2
2C2
3 ¬∑ ¬∑ ¬∑ C2
l , C2
3C2
4 ¬∑ ¬∑ ¬∑ C2
l , ¬∑ ¬∑ ¬∑ , C2
l , 1) for l = 2 . . . T."
SUPPORTING LEMMA,0.3786279683377309,"Proof. According to (17), we have"
SUPPORTING LEMMA,0.37994722955145116,"y(l)
k+1 ‚àíf (l) 
y(l‚àí1)
k+1
"
SUPPORTING LEMMA,0.3812664907651715,"= (1 ‚àíŒ≤k)

y(l)
k ‚àíf (l) 
y(l‚àí1)
k+1

+ Œ≤k

f (l)
Œæl,k"
SUPPORTING LEMMA,0.38258575197889183,"
y(l‚àí1)
k+1

‚àíf (l) 
y(l‚àí1)
k+1

‚àí k‚àí1
Y"
SUPPORTING LEMMA,0.3839050131926121,"j=k‚àít+1
(1 ‚àíŒ≤j) u(l)
k"
SUPPORTING LEMMA,0.38522427440633245,"= (1 ‚àíŒ≤k)

y(l)
k ‚àíf (l) 
y(l‚àí1)
k

+ (1 ‚àíŒ≤k)

f (l) 
y(l‚àí1)
k

‚àíf (l) 
y(l‚àí1)
k+1
"
SUPPORTING LEMMA,0.3865435356200528,"+ Œ≤k

f (l)
Œæl,k"
SUPPORTING LEMMA,0.38786279683377306,"
y(l‚àí1)
k+1

‚àíf (l) 
y(l‚àí1)
k+1

‚àí k‚àí1
Y"
SUPPORTING LEMMA,0.3891820580474934,"j=k‚àít+1
(1 ‚àíŒ≤j) u(l)
k"
SUPPORTING LEMMA,0.39050131926121373,"= (1 ‚àíŒ≤k)

y(l)
k ‚àíf (l) 
y(l‚àí1)
k

+ f (l) 
y(l‚àí1)
k

‚àíf (l) 
y(l‚àí1)
k+1
"
SUPPORTING LEMMA,0.391820580474934,"|
{z
}"
SUPPORTING LEMMA,0.39313984168865435,":=Y (l)
k"
SUPPORTING LEMMA,0.3944591029023747,"+ Œ≤k

f (l)
Œæl,k"
SUPPORTING LEMMA,0.39577836411609496,"
y(l‚àí1)
k+1

‚àíf (l) 
y(l‚àí1)
k

‚àí k‚àí1
Y"
SUPPORTING LEMMA,0.3970976253298153,"j=k‚àít+1
(1 ‚àíŒ≤j) u(l)
k . (27)"
SUPPORTING LEMMA,0.39841688654353563,Under review as a conference paper at ICLR 2022
SUPPORTING LEMMA,0.3997361477572559,"Taking the square of both sides of (27) and taking the expectation conditioned on Fk,n :=
n
Fk, y(1)
k+1, ¬∑ ¬∑ ¬∑ , y(l‚àí1)
k+1
o
, we have:"
SUPPORTING LEMMA,0.40105540897097625,"E
y(l)
k+1 ‚àíf (l) 
y(l‚àí1)
k+1

2 =E Ô£Æ Ô£ØÔ£∞"
SUPPORTING LEMMA,0.4023746701846966,"(1 ‚àíŒ≤k)

y(l)
k ‚àíf (l) 
y(l‚àí1)
k

+ Y (l)
k
‚àí k‚àí1
Y"
SUPPORTING LEMMA,0.40369393139841686,"j=k‚àít+1
(1 ‚àíŒ≤j) u(l)
k  2Ô£π Ô£∫Ô£ª"
SUPPORTING LEMMA,0.4050131926121372,"+ Œ≤2
kE


f (l)
Œæl,k"
SUPPORTING LEMMA,0.40633245382585753,"
y(l‚àí1)
k+1

‚àíf (l) 
y(l‚àí1)
k

2"
SUPPORTING LEMMA,0.4076517150395778,"(Assumption4)
‚â§
(1 ‚àíŒ≤k) E
y(l)
k ‚àíf (l) 
y(l‚àí1)
k

2
+ 1"
SUPPORTING LEMMA,0.40897097625329815,"Œ≤k
¬∑
1
1 ‚àíŒ≤k
E
Y (l)
k

2 + 1 Œ≤2
k k
Y"
SUPPORTING LEMMA,0.4102902374670185,"j=k‚àít+1
(1 ‚àíŒ≤j)2 E
u(l)
k

2
+ Œ≤3
kV 2. (28)"
SUPPORTING LEMMA,0.41160949868073876,Let us deÔ¨Åne
SUPPORTING LEMMA,0.4129287598944591,"S(l)
k+1 = E
y(l)
k+1 ‚àíf (l) 
y(l‚àí1)
k+1

2
,
(29)"
SUPPORTING LEMMA,0.41424802110817943,Formula (28) can be rewritten as
SUPPORTING LEMMA,0.4155672823218997,"S(l)
k+1 ‚â§(1 ‚àíŒ≤k)S(l)
k + 1"
SUPPORTING LEMMA,0.41688654353562005,"Œ≤k
¬∑
1
1 ‚àíŒ≤k
E
Y (l)
k

2 + 1 Œ≤2
k k
Y"
SUPPORTING LEMMA,0.4182058047493404,"j=k‚àít+1
(1 ‚àíŒ≤j)2 E
u(l)
k

2
+ Œ≤3
kV 2.
(30)"
SUPPORTING LEMMA,0.41952506596306066,Because
SUPPORTING LEMMA,0.420844327176781,"E
Y (l)
k

2
= E
f (l)(y(l‚àí1)
k+1 ) ‚àíf (l)(y(l‚àí1)
k
)

2"
SUPPORTING LEMMA,0.42216358839050133,"(Assumption2)
‚â§
C2
l E
y(l‚àí1)
k+1 ‚àíy(l‚àí1)
k

2
,
(31)"
SUPPORTING LEMMA,0.4234828496042216,"when l = 1,"
SUPPORTING LEMMA,0.42480211081794195,"E
Y (1)
k

2
‚â§C2
1E
h
‚à•Œ∏k+1 ‚àíŒ∏k‚à•2i"
SUPPORTING LEMMA,0.4261213720316623,"‚â§C2
1E
h
‚à•Œ±k‚àák + Œ±k‚àáF(Œ∏k) ‚àíŒ±k‚àáF(Œ∏k)‚à•2i"
SUPPORTING LEMMA,0.42744063324538256,"‚â§C2
1 ¬∑ Œ±2
k ¬∑ (2E
h
‚à•‚àáF(Œ∏k)‚à•2i
+ 2T T
X"
SUPPORTING LEMMA,0.4287598944591029,"l=1
A2
l S(l)
k+1). (32)"
SUPPORTING LEMMA,0.43007915567282323,"plugging (32) into (30), we have"
SUPPORTING LEMMA,0.4313984168865435,"S(1)
k+1 ‚â§(1 ‚àíŒ≤k)S(1)
k
+ 1 Œ≤2
k k‚àí1
Y"
SUPPORTING LEMMA,0.43271767810026385,"j=k‚àít+1
(1 ‚àíŒ≤j)2E
u(1)
k

2
+ Œ≤3
kV 2 + 1"
SUPPORTING LEMMA,0.4340369393139842,"Œ≤k
¬∑
1
1 ‚àíŒ≤k
¬∑ E
Y (1)
k

2"
SUPPORTING LEMMA,0.43535620052770446,"‚â§(1 ‚àíŒ≤k)S(1)
k
+ 1 Œ≤2
k k‚àí1
Y"
SUPPORTING LEMMA,0.4366754617414248,"j=k‚àít+1
(1 ‚àíŒ≤j)2E
u(1)
k

2
+ Œ≤3
kV 2 + 2"
SUPPORTING LEMMA,0.43799472295514513,"Œ≤k
¬∑
1
1 ‚àíŒ≤k
¬∑ C2
1Œ±2
kE ‚à•‚àáF(Œ∏k)‚à•2 + 2"
SUPPORTING LEMMA,0.4393139841688654,"Œ≤k
¬∑
1
1 ‚àíŒ≤k
¬∑ C2
1Œ±2
kT T
X"
SUPPORTING LEMMA,0.44063324538258575,"l=1
A2
l S(l)
k+1. (33)"
SUPPORTING LEMMA,0.4419525065963061,Under review as a conference paper at ICLR 2022
SUPPORTING LEMMA,0.44327176781002636,"When k = 0, ||u1
0||2 = 0, and"
SUPPORTING LEMMA,0.4445910290237467,"S(1)
1
‚â§(1 ‚àíŒ≤0)S(1)
0
+ Œ≤0 (Œ≤2
0V 2 + 2"
SUPPORTING LEMMA,0.44591029023746703,"Œ≤2
0
¬∑
1
1 ‚àíŒ≤0
C2
1Œ±2
0 ‚à•‚àáF(Œ∏0)‚à•2 + 2"
SUPPORTING LEMMA,0.4472295514511873,"Œ≤2
0
¬∑
1
1 ‚àíŒ≤0
C2
1Œ±2
0T T
X"
SUPPORTING LEMMA,0.44854881266490765,"l=1
S(l)
1 )"
SUPPORTING LEMMA,0.449868073878628,"|
{z
}"
SUPPORTING LEMMA,0.45118733509234826,":=‚àÜ(1)
0"
SUPPORTING LEMMA,0.4525065963060686,"= (1 ‚àíŒ≤0)S(1)
0
+ Œ≤0‚àÜ(1)
0 .
(34)"
SUPPORTING LEMMA,0.45382585751978893,"Similarly, we have"
SUPPORTING LEMMA,0.4551451187335092,"S(1)
2
‚â§(1 ‚àíŒ≤1)S(1)
1
+ Œ≤1‚àÜ(1)
1
..."
SUPPORTING LEMMA,0.45646437994722955,"S(1)
k+1 ‚â§(1 ‚àíŒ≤k)S(1)
k
+ Œ≤k‚àÜ(1)
k . (35)"
SUPPORTING LEMMA,0.4577836411609499,"Here, for i > t,"
SUPPORTING LEMMA,0.45910290237467016,"‚àÜ(1)
i
= 1 Œ≤3
i i‚àí1
Y"
SUPPORTING LEMMA,0.4604221635883905,"j=i‚àít‚àí1
(1 ‚àíŒ≤j)2E
u(1)
i

2
+ Œ≤2
i V 2 + 2"
SUPPORTING LEMMA,0.46174142480211083,"Œ≤2
i
¬∑
1
1 ‚àíŒ≤i
C2
1Œ±2
i E
h
‚à•‚àáF(Œ∏i)‚à•2i + 2"
SUPPORTING LEMMA,0.4630606860158311,"Œ≤2
i
¬∑
1
1 ‚àíŒ≤i
C2
1Œ±2
i T T
X"
SUPPORTING LEMMA,0.46437994722955145,"l=1
S(l)
i+1; (36)"
SUPPORTING LEMMA,0.4656992084432718,"for 0 < i ‚â§t,"
SUPPORTING LEMMA,0.46701846965699206,"‚àÜ(1)
i
=Œ≤2
i V 2 + 2"
SUPPORTING LEMMA,0.4683377308707124,"Œ≤2
i
¬∑
1
1 ‚àíŒ≤i
C2
1Œ±2
i E
h
‚à•‚àáF(Œ∏i)‚à•2i + 2"
SUPPORTING LEMMA,0.46965699208443273,"Œ≤2
i
¬∑
1
1 ‚àíŒ≤i
C2
1Œ±2
i T T
X"
SUPPORTING LEMMA,0.470976253298153,"l=1
S(l)
i+1.
(37)"
SUPPORTING LEMMA,0.47229551451187335,"Because S(1)
0
= 0, we can recursively write:"
SUPPORTING LEMMA,0.4736147757255937,"S(1)
1
‚â§Œ≤0‚àÜ(1)
0"
SUPPORTING LEMMA,0.47493403693931396,"S(1)
2
‚â§(1 ‚àíŒ≤1)Œ≤0‚àÜ(1)
0
+ Œ≤1‚àÜ(1)
1"
SUPPORTING LEMMA,0.4762532981530343,"S(1)
3
‚â§(1 ‚àíŒ≤2)(1 ‚àíŒ≤1)Œ≤0‚àÜ(1)
0
+ (1 ‚àíŒ≤2)Œ≤1‚àÜ(1)
1
+ Œ≤2‚àÜ(1)
2
..."
SUPPORTING LEMMA,0.47757255936675463,"S(1)
k+1 ‚â§(1 ‚àíŒ≤k) ¬∑ ¬∑ ¬∑ (1 ‚àíŒ≤1)Œ≤0‚àÜ(1)
0
+ (1 ‚àíŒ≤k) ¬∑ ¬∑ ¬∑ (1 ‚àíŒ≤2)Œ≤1‚àÜ(1)
1
+ ¬∑ ¬∑ ¬∑"
SUPPORTING LEMMA,0.4788918205804749,"+ (1 ‚àíŒ≤k)Œ≤k‚àí1‚àÜ(1)
k‚àí1 + Œ≤k‚àÜ(1)
k = k
X"
SUPPORTING LEMMA,0.48021108179419525,"i=1
wi,k‚àÜ(1)
i , (38)"
SUPPORTING LEMMA,0.4815303430079156,"where wi,k = (1 ‚àíŒ≤k) ¬∑ ¬∑ ¬∑ (1 ‚àíŒ≤i+1)Œ≤i."
SUPPORTING LEMMA,0.48284960422163586,"For l > 1, (30) can be rewritten as"
SUPPORTING LEMMA,0.4841688654353562,"S(l)
k+1 ‚â§(1 ‚àíŒ≤k) S(l)
k +
C2
l
Œ≤k (1 ‚àíŒ≤k)E
y(l‚àí1)
k+1 ‚àíy(l‚àí1)
k

2
+ 1 Œ≤2
k k‚àí1
Y"
SUPPORTING LEMMA,0.48548812664907653,"j=k‚àít+1
(1 ‚àíŒ≤j)2 E
u(l)
k

2
+ Œ≤3
kV 2"
SUPPORTING LEMMA,0.4868073878627968,"(Lemma 3)
‚â§
(1 ‚àíŒ≤k) S(l)
k +
Œ≤kC2
l
(1 ‚àíŒ≤k)2 S(l‚àí1)
k+1 + Œ≤kZ(l)
k , (39)"
SUPPORTING LEMMA,0.48812664907651715,Under review as a conference paper at ICLR 2022 where
SUPPORTING LEMMA,0.4894459102902375,"Z(l)
k
= 1 Œ≤3
k k‚àí1
Y"
SUPPORTING LEMMA,0.49076517150395776,"j=k‚àít+1
(1 ‚àíŒ≤j)2 E
u(l)
k

2
+
C2
l
Œ≤3
k (1 ‚àíŒ≤k) k‚àí1
Y"
SUPPORTING LEMMA,0.4920844327176781,"j=k‚àít+1
(1 ‚àíŒ≤j)2 E
u(l‚àí1)
k

2
+ Œ≤2
kV 2 + Œ≤kC2
l
1 ‚àíŒ≤k
V 2. (40)"
SUPPORTING LEMMA,0.49340369393139843,It follows that
SUPPORTING LEMMA,0.4947229551451187,"S(2)
k+1 ‚â§(1 ‚àíŒ≤k)S(2)
k
+
Œ≤kC2
2
(1 ‚àíŒ≤k)2 S(1)
k+1 + Œ≤kZ(2)
k"
SUPPORTING LEMMA,0.49604221635883905,"Accordingto(38)
‚â§
(1 ‚àíŒ≤k)S(2)
k
+
Œ≤kC2
2
(1 ‚àíŒ≤k)2 k
X"
SUPPORTING LEMMA,0.4973614775725594,"i=0
wi,k‚àÜ(1)
i
+ Œ≤kZ(2)
k"
SUPPORTING LEMMA,0.49868073878627966,"=(1 ‚àíŒ≤k)S(2)
k
+ Œ≤k"
SUPPORTING LEMMA,0.5,"C2
2
(1 ‚àíŒ≤k)2 k
X"
SUPPORTING LEMMA,0.5013192612137203,"i=0
wi,k‚àÜ(1)
i
+ Z(2)
k"
SUPPORTING LEMMA,0.5026385224274407,"!
(41)"
SUPPORTING LEMMA,0.503957783641161,"Following the same steps as (38), by recursively plugging S(2)
k
into S(2)
k+1, we can obtain"
SUPPORTING LEMMA,0.5052770448548812,"S(2)
k+1 ‚â§ k
X"
SUPPORTING LEMMA,0.5065963060686016,"j=0
wj,k"
SUPPORTING LEMMA,0.5079155672823219,"C2
2
(1 ‚àíŒ≤j)2 j
X"
SUPPORTING LEMMA,0.5092348284960422,"i=0
wi,k‚àÜ(1)
i
+ Z(2)
j ! = k
X j=0"
SUPPORTING LEMMA,0.5105540897097626,"C2
2wj,k
(1 ‚àíŒ≤j)2 j
X"
SUPPORTING LEMMA,0.5118733509234829,"i=0
wi,k‚àÜ(1)
i
+ k
X"
SUPPORTING LEMMA,0.5131926121372031,"j=0
wj,kZ(2)
j = k
X i=0 Ô£´ Ô£≠
k
X j=i"
SUPPORTING LEMMA,0.5145118733509235,"C2
2wj,k
(1 ‚àíŒ≤j)2 Ô£∂"
SUPPORTING LEMMA,0.5158311345646438,"Ô£∏wi,k‚àÜ(1)
i
+ k
X"
SUPPORTING LEMMA,0.5171503957783641,"i=0
wi,kZ(2)
i (42)"
SUPPORTING LEMMA,0.5184696569920845,"According to (38), it is easy to Ô¨Ånd Pk
j=i wj,k < 1. Thus, we have k
X j=i"
SUPPORTING LEMMA,0.5197889182058048,"C2
2wj,k
(1 ‚àíŒ≤j)2 ‚â§ k
X j=i"
SUPPORTING LEMMA,0.521108179419525,"C2
2wj,k
(1 ‚àíŒ≤i)2 ‚â§
C2
2
(1 ‚àíŒ≤i)2 k
X"
SUPPORTING LEMMA,0.5224274406332454,"j=i
wj,k ‚â§
C2
2
(1 ‚àíŒ≤i)2 .
(43)"
SUPPORTING LEMMA,0.5237467018469657,"Combining (43) and (42), we have"
SUPPORTING LEMMA,0.525065963060686,"S(2)
k+1 ‚â§ k
X i=0"
SUPPORTING LEMMA,0.5263852242744064,"
C2
2
(1 ‚àíŒ≤i)2 wi,k‚àÜ(1)
i
+ wi,kZ(2)
i  ‚â§ k
X"
SUPPORTING LEMMA,0.5277044854881267,"i=0
wi,k"
SUPPORTING LEMMA,0.5290237467018469,"
C2
2
(1 ‚àíŒ≤i)2 ‚àÜ(1)
i
+ Z(2)
i  = k
X"
SUPPORTING LEMMA,0.5303430079155673,"i=0
wi,k‚àÜ(2)
i (44)"
SUPPORTING LEMMA,0.5316622691292876,"Similarly, for any l > 1, we have"
SUPPORTING LEMMA,0.5329815303430079,"S(l)
k+1 ‚â§ k
X"
SUPPORTING LEMMA,0.5343007915567283,"i=0
wi,k‚àÜ(l)
i
(45)"
SUPPORTING LEMMA,0.5356200527704486,Under review as a conference paper at ICLR 2022 where
SUPPORTING LEMMA,0.5369393139841688,"‚àÜ(2)
i
=
C2
2
(1 ‚àíŒ≤i)2 ‚àÜ(1)
i
+ Z(2)
i"
SUPPORTING LEMMA,0.5382585751978892,"‚àÜ(3)
i
=
C2
3
(1 ‚àíŒ≤i)2 ‚àÜ(2)
i
+ Z(3)
i"
SUPPORTING LEMMA,0.5395778364116095,"=
C2
2C2
3
(1 ‚àíŒ≤i)4 ‚àÜ(1)
i
+
C2
3
(1 ‚àíŒ≤i)2 Z(2)
i
+ Z(3)
i"
SUPPORTING LEMMA,0.5408970976253298,"‚àÜ(l)
i
= C2
2C2
3 ¬∑ ¬∑ ¬∑ C2
l
(1 ‚àíŒ≤i)2(l‚àí1) ‚àÜ(1)
i
+
C2
3 ¬∑ ¬∑ ¬∑ C2
l
(1 ‚àíŒ≤i)2(l‚àí2) Z(2)
i
+
C2
4 ¬∑ ¬∑ ¬∑ C2
l
(1 ‚àíŒ≤i)2(l‚àí3) Z(3)
i
+ ¬∑ ¬∑ ¬∑ + Z(l)
i (46)"
SUPPORTING LEMMA,0.5422163588390502,"Let Cmax = max(C2
2C2
3 ¬∑ ¬∑ ¬∑ C2
l , C2
3C2
4 ¬∑ ¬∑ ¬∑ C2
l , ¬∑ ¬∑ ¬∑ , C2
l , 1), ‚àÄl ‚àà[1, T] and Zmax
i
deÔ¨Åned as follows:"
SUPPORTING LEMMA,0.5435356200527705,"Zmax
i
= 1 Œ≤3
i i‚àí1
Y"
SUPPORTING LEMMA,0.5448548812664907,"j=i‚àít+1
(1 ‚àíŒ≤j)2 D2 +
Cmax
Œ≤3
i (1 ‚àíŒ≤i) i‚àí1
Y"
SUPPORTING LEMMA,0.5461741424802111,"j=i‚àít+1
(1 ‚àíŒ≤j)2 D2 + Œ≤2
i V 2 + Œ≤iCmax"
SUPPORTING LEMMA,0.5474934036939314,"1 ‚àíŒ≤i
V 2 (47)"
SUPPORTING LEMMA,0.5488126649076517,"Plugging (46) into (45), we can obtain"
SUPPORTING LEMMA,0.5501319261213721,"S(l)
k+1 ‚â§ k
X i=0"
SUPPORTING LEMMA,0.5514511873350924,"Cmax
(1 ‚àíŒ≤i)2(T ‚àí1) wi,k‚àÜ(1)
i
+ k
X"
SUPPORTING LEMMA,0.5527704485488126,"i=0
wi,k l
X j=2"
SUPPORTING LEMMA,0.554089709762533,"Cmax
(1 ‚àíŒ≤i)2(T ‚àí1) Z(l)
i .
(48)"
SUPPORTING LEMMA,0.5554089709762533,"Summing (48) from l = 1 to T, setting Œ≤k = Œ≤ and Œ±k = Œ±, then substituting ‚àÜ(1)
k
we have: K‚àí1
X k=0 T
X"
SUPPORTING LEMMA,0.5567282321899736,"l=1
S(l)
k+1 ‚â§ K‚àí1
X k=0 k
X i=0 T
X l=1"
SUPPORTING LEMMA,0.558047493403694,"Cmax
(1 ‚àíŒ≤)2(T ‚àí1) wi,k‚àÜ(1)
i
+ K‚àí1
X k=0 k
X"
SUPPORTING LEMMA,0.5593667546174143,"i=0
wi,k T
X l=1 l
X j=2"
SUPPORTING LEMMA,0.5606860158311345,"Cmax
(1 ‚àíŒ≤)2(T ‚àí1) Z(l)
i ‚â§ K‚àí1
X k=0 K‚àí1
X i=k"
SUPPORTING LEMMA,0.5620052770448549,"CmaxT
(1 ‚àíŒ≤)2(T ‚àí1) wi,K‚àí1‚àÜ(1)
k
+ K‚àí1
X k=0 K‚àí1
X i=k"
SUPPORTING LEMMA,0.5633245382585752,CmaxT 2
SUPPORTING LEMMA,0.5646437994722955,"(1 ‚àíŒ≤)2(T ‚àí1) wi,K‚àí1Zmax
i ‚â§ K‚àí1
X k=0"
SUPPORTING LEMMA,0.5659630606860159,"CmaxT
(1 ‚àíŒ≤)2(T ‚àí1) ‚àÜ(1)
k
+ K‚àí1
X k=0 K‚àí1
X i=k"
SUPPORTING LEMMA,0.5672823218997362,CmaxT 2
SUPPORTING LEMMA,0.5686015831134564,"(1 ‚àíŒ≤)2(T ‚àí1) wi,K‚àí1Zmax
i"
SUPPORTING LEMMA,0.5699208443271768,"‚â§
CmaxT
(1 ‚àíŒ≤)2(T ‚àí1) K‚àí1
X k=0 "
SUPPORTING LEMMA,0.5712401055408971,"Œ≤2V 2 +
2C2
1Œ±2"
SUPPORTING LEMMA,0.5725593667546174,"Œ≤2(1 ‚àíŒ≤)E
h
‚à•‚àáF(Œ∏k)‚à•2i
+ 2C2
1Œ±2T
Œ≤2(1 ‚àíŒ≤) T
X"
SUPPORTING LEMMA,0.5738786279683378,"l=1
S(l)
k+1 !"
SUPPORTING LEMMA,0.575197889182058,"+
CmaxTD2K
Œ≤3(1 ‚àíŒ≤)2(T ‚àí1‚àít) + CmaxT 2KZmax"
SUPPORTING LEMMA,0.5765171503957783,(1 ‚àíŒ≤)2(T ‚àí1) . (49)
SUPPORTING LEMMA,0.5778364116094987,"It follows that K‚àí1
X k=0 T
X"
SUPPORTING LEMMA,0.579155672823219,"l=1
S(l)
k+1 ‚â§
CmaxTŒ≤2(1 ‚àíŒ≤)
Œ≤2(1 ‚àíŒ≤)2T ‚àí1 ‚àí2C2
1CmaxŒ±2T 2 K‚àí1
X k=0"
SUPPORTING LEMMA,0.5804749340369393,"
Œ≤2V 2 +
2C2
1Œ±2"
SUPPORTING LEMMA,0.5817941952506597,"Œ≤2(1 ‚àíŒ≤)E
h
‚à•‚àáF(Œ∏k)‚à•2i"
SUPPORTING LEMMA,0.58311345646438,"+
CmaxTD2K(1 ‚àíŒ≤)2t+1"
SUPPORTING LEMMA,0.5844327176781002,"Œ≤3(1 ‚àíŒ≤)2T ‚àí1 ‚àí2C2
1CmaxT 2Œ±2Œ≤ +
CmaxT 2Œ≤2(1 ‚àíŒ≤)KZmax"
SUPPORTING LEMMA,0.5857519788918206,"Œ≤2(1 ‚àíŒ≤)2T ‚àí1 ‚àí2C2
1CmaxŒ±2T 2
(50)"
SUPPORTING LEMMA,0.5870712401055409,Under review as a conference paper at ICLR 2022
SUPPORTING LEMMA,0.5883905013192612,"If we set Œ≤ ‚â§1 ‚àí
2T ‚àí1q"
SUPPORTING LEMMA,0.5897097625329816,"1
2 and Œ± ‚â§
Œ≤
2C1T q"
SUPPORTING LEMMA,0.5910290237467019,2(1‚àíŒ≤)2T ‚àí1‚àí1
SUPPORTING LEMMA,0.5923482849604221,"Cmax
, we can ensure Œ≤2(1 ‚àíŒ≤)2T ‚àí1 ‚àí"
SUPPORTING LEMMA,0.5936675461741425,"2C2
1CmaxŒ±2T 2 ‚â•Œ≤2/2, and (50) can be simpliÔ¨Åed as K‚àí1
X k=0 T
X"
SUPPORTING LEMMA,0.5949868073878628,"l=1
S(l)
k+1 ‚â§2CmaxTKŒ≤2(1 ‚àíŒ≤)2V 2 + 4CmaxC2
1TŒ±2 Œ≤2 K‚àí1
X"
SUPPORTING LEMMA,0.5963060686015831,"k=0
E
h
‚à•‚àáF(Œ∏k)‚à•2i"
SUPPORTING LEMMA,0.5976253298153035,+ 2CmaxTK(1 ‚àíŒ≤)2t+1
SUPPORTING LEMMA,0.5989445910290238,"Œ≤3
D2 + 2CmaxT 2K(1 ‚àíŒ≤)Zmax"
SUPPORTING LEMMA,0.600263852242744,"‚â§2CmaxTKŒ≤2(1 ‚àíŒ≤)2V 2 + 4CmaxC2
1TŒ±2 Œ≤2 K‚àí1
X"
SUPPORTING LEMMA,0.6015831134564644,"k=0
E
h
‚à•‚àáF(Œ∏k)‚à•2i"
SUPPORTING LEMMA,0.6029023746701847,+ 2CmaxTK(1 ‚àíŒ≤)2t+1
SUPPORTING LEMMA,0.604221635883905,"Œ≤3
D2 + 2CmaxT 2K(1 ‚àíŒ≤)

Œ≤2 + Œ≤Cmax 1 ‚àíŒ≤ 
V 2"
SUPPORTING LEMMA,0.6055408970976254,+ 2CmaxT 2K(1 ‚àíŒ≤)2t‚àí1
SUPPORTING LEMMA,0.6068601583113457,"Œ≤3
(1 + Cmax"
SUPPORTING LEMMA,0.6081794195250659,1 ‚àíŒ≤ )D2
SUPPORTING LEMMA,0.6094986807387863,"‚â§4CmaxC2
1TŒ±2 Œ≤2 K‚àí1
X"
SUPPORTING LEMMA,0.6108179419525066,"k=0
E
h
‚à•‚àáF(Œ∏k)‚à•2i"
SUPPORTING LEMMA,0.6121372031662269,"+ 2CmaxT 2K
(1 ‚àíŒ≤)2t‚àí1"
SUPPORTING LEMMA,0.6134564643799473,"Œ≤3
+ Cmax(1 ‚àíŒ≤)2(t‚àí1)"
SUPPORTING LEMMA,0.6147757255936676,"Œ≤3
+ (1 ‚àíŒ≤)2t+1 TŒ≤3 
D2"
SUPPORTING LEMMA,0.6160949868073878,"+ 2CmaxT 2K

Œ≤Cmax + Œ≤2(1 ‚àíŒ≤)(1 + T) T 
V 2 (51)"
SUPPORTING LEMMA,0.6174142480211082,"8.1.2
REMAINING STEPS TOWARDS THEOREM 1."
SUPPORTING LEMMA,0.6187335092348285,"Using the smoothness of F(Œ∏k), we have"
SUPPORTING LEMMA,0.6200527704485488,"F(Œ∏k+1) ‚â§F(Œ∏k) + ‚ü®‚àáF(Œ∏k), Œ∏k+1 ‚àíŒ∏k‚ü©+ L"
SUPPORTING LEMMA,0.6213720316622692,2 ‚à•Œ∏k+1 ‚àíŒ∏k‚à•2
SUPPORTING LEMMA,0.6226912928759895,"= F(Œ∏k) ‚àíŒ±k ‚ü®‚àáF(Œ∏k), gk+1‚ü©+ L"
SUPPORTING LEMMA,0.6240105540897097,"2 Œ±2
k ‚à•gk+1‚à•2"
SUPPORTING LEMMA,0.6253298153034301,"= F(Œ∏k) ‚àíŒ±kE
h
‚à•‚àáF(Œ∏k)‚à•2i
+ LŒ±2
k
2
‚à•gk+1‚à•2 + Œ±k ‚ü®‚àáF(Œ∏k), ‚àáF(Œ∏k) ‚àígk+1‚ü©"
SUPPORTING LEMMA,0.6266490765171504,"= F(Œ∏k) ‚àíŒ±kE
h
‚à•‚àáF(Œ∏k)‚à•2i
+ LŒ±2
k
2
‚à•gk+1‚à•2 + Œ±k ‚ü®‚àáF(Œ∏k), ‚àáF(Œ∏k) ‚àí‚àák‚ü©+ Œ±k ‚ü®‚àáF(Œ∏k), ‚àák ‚àígk+1‚ü©. (52)"
SUPPORTING LEMMA,0.6279683377308707,"Conditioning on Fk and taking expectation of both sides w.r.t. Œæk, we have"
SUPPORTING LEMMA,0.6292875989445911,E [F (Œ∏k+1) |Fk]
SUPPORTING LEMMA,0.6306068601583114,"‚â§F(Œ∏k) ‚àíŒ±kE
h
‚à•‚àáF(Œ∏k)‚à•2i
+ L"
"E
H",0.6319261213720316,"2 E
h
‚à•Œ∏k+1 ‚àíŒ∏k‚à•2 | Fk
i
+ Œ±k ‚ü®‚àáF(Œ∏k), E [‚àáF(Œ∏k) ‚àí‚àák|Fk]‚ü©"
"E
H",0.633245382585752,"‚â§F(Œ∏k) ‚àíŒ±kE
h
‚à•‚àáF(Œ∏k)‚à•2i
+ L"
"E
H",0.6345646437994723,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T Œ±2
k + Œ±kE [‚à•‚àáF(Œ∏k)‚à•] ‚à•E [‚àák|Fk] ‚àí‚àáF(Œ∏k)‚à•"
"E
H",0.6358839050131926,"(Lemma 4)
‚â§
F(Œ∏k) ‚àíŒ±kE
h
‚à•‚àáF(Œ∏k)‚à•2i
+ L"
"E
H",0.637203166226913,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T Œ±2
k + Œ±k T
X"
"E
H",0.6385224274406333,"l=1
(Al ‚à•‚àáF(Œ∏k)‚à•E
hy(l)
k+1 ‚àíf (l)(y(l‚àí1)
k+1 )|Fk

i"
"E
H",0.6398416886543535,"‚â§F(Œ∏k) ‚àíŒ±k  1 ‚àíŒ±k 4Œ≤k T
X"
"E
H",0.6411609498680739,"l=1
A2
l !"
"E
H",0.6424802110817942,"E
h
‚à•‚àáF(Œ∏k)‚à•2i
+ Œ≤k T
X"
"E
H",0.6437994722955145,"l=1
E
y(l)
k+1 ‚àíf (l)(y(l‚àí1)
k+1 )

2
|Fk 
+ L"
"E
H",0.6451187335092349,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T Œ±2
k. (53)"
"E
H",0.6464379947229552,Under review as a conference paper at ICLR 2022
"E
H",0.6477572559366754,"Summing from k = 0 to K ‚àí1, we have K‚àí1
X"
"E
H",0.6490765171503958,"k=0
E [F(Œ∏k+1|Fk)] ‚â§ K‚àí1
X"
"E
H",0.6503957783641161,"k=0
F(Œ∏k) ‚àí K‚àí1
X"
"E
H",0.6517150395778364,"k=0
Œ±k(1 ‚àíŒ±k 4Œ≤k T
X"
"E
H",0.6530343007915568,"l=1
A2
l )E [‚à•‚àáF(Œ∏k)‚à•]2 + K‚àí1
X"
"E
H",0.6543535620052771,"k=0
Œ≤k T
X"
"E
H",0.6556728232189973,"l=1
S(l)
k+1 + K‚àí1
X k=0 L"
"E
H",0.6569920844327177,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T Œ±2
k. (54)"
"E
H",0.658311345646438,"Substitute (50) into the above equation, we have K‚àí1
X"
"E
H",0.6596306068601583,"k=0
E [F(Œ∏k+1|Fk)] ‚â§ K‚àí1
X"
"E
H",0.6609498680738787,"k=0
F(Œ∏k) ‚àíŒ±(1 ‚àíŒ± 4Œ≤ T
X"
"E
H",0.662269129287599,"l=1
A2
l ) K‚àí1
X"
"E
H",0.6635883905013192,"k=0
E [‚à•‚àáF(Œ∏k)‚à•]2"
"E
H",0.6649076517150396,"+ 4CmaxC2
1TŒ±2 Œ≤ K‚àí1
X"
"E
H",0.6662269129287599,"k=0
E
h
‚à•‚àáF(Œ∏k)‚à•2i
+ LK"
"E
H",0.6675461741424802,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T Œ±2"
"E
H",0.6688654353562006,"+ 2CmaxT 2K
(1 ‚àíŒ≤)2t‚àí1"
"E
H",0.6701846965699209,"Œ≤2
+ Cmax(1 ‚àíŒ≤)2(t‚àí1)"
"E
H",0.6715039577836411,"Œ≤2
+ (1 ‚àíŒ≤)2t+1 TŒ≤2 
D2"
"E
H",0.6728232189973615,"+ 2CmaxT 2K

Œ≤2Cmax + Œ≤3(1 ‚àíŒ≤)(1 + T) T"
"E
H",0.6741424802110818,"
V 2. (55)"
"E
H",0.6754617414248021,"If we set Œ± ‚â§
2Œ≤
PT
l=1 A2
l +16CmaxC2
1T , we can ensure 1 ‚àíŒ±"
"E
H",0.6767810026385225,"4Œ≤
PT
l=1 A2
l ‚àí4CmaxC2
1T Œ±
Œ≤
‚â•1"
"E
H",0.6781002638522428,"2, and thus, K‚àí1
X"
"E
H",0.679419525065963,"k=0
E [F(Œ∏k+1|Fk)] ‚â§ K‚àí1
X"
"E
H",0.6807387862796834,"k=0
F(Œ∏k) + Œ± 2 K‚àí1
X"
"E
H",0.6820580474934037,"k=0
E [‚à•‚àáF(Œ∏k)‚à•]2 + LK"
"E
H",0.683377308707124,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T Œ±2"
"E
H",0.6846965699208444,"+ 2CmaxT 2K
(1 ‚àíŒ≤)2t‚àí1"
"E
H",0.6860158311345647,"Œ≤2
+ Cmax(1 ‚àíŒ≤)2(t‚àí1)"
"E
H",0.6873350923482849,"Œ≤2
+ (1 ‚àíŒ≤)2t+1 TŒ≤2 
D2"
"E
H",0.6886543535620053,"+ 2CmaxT 2K

Œ≤2Cmax + Œ≤3(1 ‚àíŒ≤)(1 + T) T"
"E
H",0.6899736147757256,"
V 2. (56)"
"E
H",0.6912928759894459,"Combining with the condition for Œ± and Œ≤ in (50), if we set"
"E
H",0.6926121372031663,Œ≤ ‚â§1 ‚àí2(‚àí1/(2T ‚àí1))
"E
H",0.6939313984168866,Œ± = Œ≤ ¬∑ min Ô£´
"E
H",0.6952506596306068,"Ô£≠
1
2C1T s"
"E
H",0.6965699208443272,2(1 ‚àíŒ≤)2T ‚àí1 ‚àí1
"E
H",0.6978891820580475,"Cmax
,
2
PT
l=1 A2
l + 16CmaxC2
1T Ô£∂"
"E
H",0.6992084432717678,"Ô£∏,
(57)"
"E
H",0.7005277044854882,we have
K,0.7018469656992085,"1
K K‚àí1
X"
K,0.7031662269129287,"k=0
E [‚à•‚àáF(Œ∏k)‚à•]2 ‚â§2(F(Œ∏0) ‚àíF(Œ∏‚àó))"
K,0.7044854881266491,"KŒ±
+ LC2
1 ¬∑ ¬∑ ¬∑ C2
T Œ±"
K,0.7058047493403694,"+ 4CmaxT 2
(1 ‚àíŒ≤)2t‚àí1"
K,0.7071240105540897,"Œ±Œ≤2
+ Cmax(1 ‚àíŒ≤)2(t‚àí1)"
K,0.7084432717678101,"Œ±Œ≤2
+ (1 ‚àíŒ≤)2t+1 TŒ±Œ≤2 
D2"
K,0.7097625329815304,"+ 4CmaxT 2
Œ≤2Cmax"
K,0.7110817941952506,"Œ±
+ Œ≤3(1 ‚àíŒ≤)(1 + T) Œ±T 
V 2"
K,0.712401055408971,"‚â§O(Œ≤) + 4CmaxT 2
1 + Cmax)(1 ‚àíŒ≤)2(t‚àí1)"
K,0.7137203166226913,"Œ±Œ≤2
+ (1 ‚àíŒ≤)2t+1 TŒ±Œ≤2 
D2 (58)"
K,0.7150395778364116,"This completes the proof of Theorem 1.If we can set t such that (1‚àíŒ≤)2(t‚àí1) = O(Œ≤4), the algorithm
achieves O(1/
‚àö"
K,0.716358839050132,K) convergence rate.
K,0.7176781002638523,Under review as a conference paper at ICLR 2022
K,0.7189973614775725,"8.2
PROOF TO THEOREM 2"
K,0.7203166226912929,The SCSC algorithm with our sparse moving average can be written as
K,0.7216358839050132,"y(1)
k+1 = (1 ‚àíŒ≤k)y(1)
k
+ f (1)
Œæ1,k(Œ∏k) ‚àí(1 ‚àíŒ≤k)f (1)
Œæ1,k(Œ∏k‚àí1) ‚àí k‚àí1
Y"
K,0.7229551451187335,"j=k‚àít
(1 ‚àíŒ≤j)u(1)
k ,
(59)"
K,0.7242744063324539,"y(l)
k+1 = (1 ‚àíŒ≤k)y(l)
k + f (l)
Œæl,k(y(l‚àí1)
k+1 ) ‚àí(1 ‚àíŒ≤k)f (l)
Œæl,k(y(l‚àí1)
k
) ‚àí k‚àí1
Y"
K,0.7255936675461742,"j=k‚àít
(1 ‚àíŒ≤j)u(l)
k , 2 ‚â§l ‚â§T, (60)"
K,0.7269129287598944,"Œ∏k+1 = Œ∏k ‚àíŒ±k‚àák,
(61)"
K,0.7282321899736148,"with
u(l)
k = P(Œæ1,k‚àít/(Œæ1,k‚àít+1 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œæ1,k))y(l)
k+t‚àí1
(62)"
K,0.7295514511873351,"and
‚àák = ‚àáf (1)
Œæ1,k(Œ∏k) ¬∑ ¬∑ ¬∑ ‚àáf (T )
ŒæN,k(y(T ‚àí1)
k+1
)‚àáf (T +1)
ŒæT +1,k(y(T )
k+1).
(63)"
SUPPORTING LEMMA,0.7308707124010554,"8.2.1
SUPPORTING LEMMA"
SUPPORTING LEMMA,0.7321899736147758,"Lemma 6. The change of y(l)
k in every iteration is bounded, i.e.,"
SUPPORTING LEMMA,0.7335092348284961,"E
y(l)
k+1 ‚àíy(l)
k

2
‚â§3

Œ≤k
1 ‚àíŒ≤k"
SUPPORTING LEMMA,0.7348284960422163,"2
E
y(l)
k+1 ‚àíf (l) 
y(l‚àí1)
k+1

2"
SUPPORTING LEMMA,0.7361477572559367,"+

Œ≤k
1 ‚àíŒ≤k"
SUPPORTING LEMMA,0.737467018469657,"2
V 2 + 3C2
l‚àí1E
y(l‚àí1)
k+1 ‚àíy(l‚àí1)
k

2 + 3 k‚àí1
Y"
SUPPORTING LEMMA,0.7387862796833773,"j=k‚àít
(1 ‚àíŒ≤j)2E
u(l)
k

2 (64)"
SUPPORTING LEMMA,0.7401055408970977,"Proof. According to (60), we have"
SUPPORTING LEMMA,0.741424802110818,"(1 ‚àíŒ≤k)

y(l)
k+1 ‚àíy(l)
k

=Œ≤k

f (l)
Œæl,k"
SUPPORTING LEMMA,0.7427440633245382,"
y(l‚àí1)
k+1

‚àíy(l)
k+1
"
SUPPORTING LEMMA,0.7440633245382586,"+ (1 ‚àíŒ≤k)

f (l)
Œæl,k"
SUPPORTING LEMMA,0.7453825857519789,"
y(l‚àí1)
k+1

‚àíf (l)
Œæl,k"
SUPPORTING LEMMA,0.7467018469656992,"
y(l‚àí1)
k

‚àí k‚àí1
Y"
SUPPORTING LEMMA,0.7480211081794196,"j=k‚àít
(1 ‚àíŒ≤j)u(l)
k"
SUPPORTING LEMMA,0.7493403693931399,"=Œ≤k

f (l) 
y(l‚àí1)
k+1

‚àíy(l)
k+1

+ Œ≤k

f (l)
Œæl,k"
SUPPORTING LEMMA,0.7506596306068601,"
y(l‚àí1)
k+1

‚àíf (l) 
y(l‚àí1)
k+1
"
SUPPORTING LEMMA,0.7519788918205804,"+ (1 ‚àíŒ≤k)

f (l)
Œæl,k"
SUPPORTING LEMMA,0.7532981530343008,"
y(l‚àí1)
k+1

‚àíf (l)
Œæl,k"
SUPPORTING LEMMA,0.7546174142480211,"
y(l‚àí1)
k

‚àí k‚àí1
Y"
SUPPORTING LEMMA,0.7559366754617414,"j=k‚àít
(1 ‚àíŒ≤j)u(l)
k (65)"
SUPPORTING LEMMA,0.7572559366754618,Taking the square of both sides of (65) gives us (64).
SUPPORTING LEMMA,0.758575197889182,"Lemma 7. The difference between the composite function and its moving average is bounded, i.e.,"
SUPPORTING LEMMA,0.7598944591029023,"E
y(l)
k+1 ‚àíf (l) 
y(l‚àí1)
k+1

2
‚â§(1 ‚àíŒ≤k) E
y(l)
k ‚àíf (l) 
y(l‚àí1)
k

2"
SUPPORTING LEMMA,0.7612137203166227,"+ 4 (1 ‚àíŒ≤k)2 C2
l E
y(l‚àí1)
k
‚àíy(l‚àí1)
k+1

2
+ 2Œ≤2
kV 2 + 1 Œ≤k k‚àí1
Y"
SUPPORTING LEMMA,0.762532981530343,"j=k‚àít
(1 ‚àíŒ≤j)2 E
h
‚à•u(l)
k ‚à•2i (66)"
SUPPORTING LEMMA,0.7638522427440633,Under review as a conference paper at ICLR 2022
SUPPORTING LEMMA,0.7651715039577837,Proof.
SUPPORTING LEMMA,0.7664907651715039,"y(l)
k+1 ‚àíf (l) 
y(l‚àí1)
k+1
"
SUPPORTING LEMMA,0.7678100263852242,"= (1 ‚àíŒ≤k)

y(l)
k ‚àíf (l) 
y(l‚àí1)
k

+ (1 ‚àíŒ≤k)

f (l) 
y(l‚àí1)
k

‚àíf (l) 
y(l‚àí1)
k+1
"
SUPPORTING LEMMA,0.7691292875989446,"|
{z
}
:=T1"
SUPPORTING LEMMA,0.7704485488126649,"+ Œ≤k

f (l)
Œæl,k"
SUPPORTING LEMMA,0.7717678100263852,"
y(l‚àí1)
k+1

‚àíf (l) 
y(l‚àí1)
k+1
"
SUPPORTING LEMMA,0.7730870712401056,"|
{z
}
:=T2"
SUPPORTING LEMMA,0.7744063324538258,"+ (1 ‚àíŒ≤k)

f (l)
Œæl,k"
SUPPORTING LEMMA,0.7757255936675461,"
y(l‚àí1)
k+1

‚àíf (l)
Œæl,k"
SUPPORTING LEMMA,0.7770448548812665,"
y(l‚àí1)
k
"
SUPPORTING LEMMA,0.7783641160949868,"|
{z
}
:=T3 + k‚àí1
Y"
SUPPORTING LEMMA,0.7796833773087071,"j=k‚àít
(1 ‚àíŒ≤j) u(l)
k (67)"
SUPPORTING LEMMA,0.7810026385224275,"Conditioned on Fk, taking expectation over Œæl,k, we have:"
SUPPORTING LEMMA,0.7823218997361477,E [(1 ‚àíŒ≤k) T1 + Œ≤kT2 + (1 ‚àíŒ≤k) T3]
SUPPORTING LEMMA,0.783641160949868,"= E
h
(1 ‚àíŒ≤k) f (l) 
y(l‚àí1)
k

‚àíf (l) 
y(l‚àí1)
k+1

+ f (l)
Œæl,k"
SUPPORTING LEMMA,0.7849604221635884,"
y(l)
k+1

‚àí(1 ‚àíŒ≤k) f (l)
Œæl,k"
SUPPORTING LEMMA,0.7862796833773087,"
y(l)
k
i = 0"
SUPPORTING LEMMA,0.787598944591029,"Taking the square of both sides of (67) and taking the expectation conditioned on Fk,n :=
n
Fk, y(1)
k+1, ¬∑ ¬∑ ¬∑ , y(l‚àí1)
k+1
o
, we have:"
SUPPORTING LEMMA,0.7889182058047494,"E
y(l)
k+1 ‚àíf (l) 
y(l‚àí1)
k+1

2 = E Ô£Æ Ô£ØÔ£∞"
SUPPORTING LEMMA,0.7902374670184696,"(1 ‚àíŒ≤k)

y(l)
k ‚àíf (l) 
y(l‚àí1)
k

‚àí k‚àí1
Y"
SUPPORTING LEMMA,0.7915567282321899,"j=k‚àít
(1 ‚àíŒ≤j) u(l)
k  2Ô£π Ô£∫Ô£ª"
SUPPORTING LEMMA,0.7928759894459103,"+ E
h
‚à•(1 ‚àíŒ≤k) T1 + Œ≤kT2 + (1 ‚àíŒ≤k) T3‚à•2i"
SUPPORTING LEMMA,0.7941952506596306,"‚â§(1 ‚àíŒ≤k) E
y(l)
k ‚àíf (l) 
y(l‚àí1)
k

2
+ 1 Œ≤k k‚àí1
Y"
SUPPORTING LEMMA,0.7955145118733509,"j=k‚àít
(1 ‚àíŒ≤j)2 E
h
‚à•u(l)
k ‚à•2i"
SUPPORTING LEMMA,0.7968337730870713,"+ 2E
h
‚à•(1 ‚àíŒ≤k) T1 + Œ≤kT2‚à•2 |Fk,n
i
+ 2 (1 ‚àíŒ≤k)2 E
h
‚à•T3‚à•2 |Fk,n
i"
SUPPORTING LEMMA,0.7981530343007915,"‚â§(1 ‚àíŒ≤k) E
y(l)
k ‚àíf (l) 
y(l‚àí1)
k

2
+ 2 (1 ‚àíŒ≤k)2 E
h
‚à•T1‚à•2 |Fk,n
i
+ 2Œ≤2
kE
h
‚à•T2‚à•2 |Fk,n
i"
SUPPORTING LEMMA,0.7994722955145118,"+ 2 (1 ‚àíŒ≤k)2 E
h
‚à•T3‚à•2 |Fk,n
i
+ 1 Œ≤k k‚àí1
Y"
SUPPORTING LEMMA,0.8007915567282322,"j=k‚àít
(1 ‚àíŒ≤j)2 E
h
‚à•u(l)
k ‚à•2i"
SUPPORTING LEMMA,0.8021108179419525,"‚â§(1 ‚àíŒ≤k) E
y(l)
k ‚àíf (l) 
y(l‚àí1)
k

2
+ 2 (1 ‚àíŒ≤k)2 E
f (l) 
y(l‚àí1)
k

‚àíf (l) 
y(l‚àí1)
k+1

2"
SUPPORTING LEMMA,0.8034300791556728,"+ 2 (1 ‚àíŒ≤k)2 E
f (l)
Œæl,k"
SUPPORTING LEMMA,0.8047493403693932,"
y(l‚àí1)
k

‚àíf (l)
Œæl,k"
SUPPORTING LEMMA,0.8060686015831134,"
y(l‚àí1)
k+1

2
+ 2Œ≤2
kV 2 + 1 Œ≤k k‚àí1
Y"
SUPPORTING LEMMA,0.8073878627968337,"j=k‚àít
(1 ‚àíŒ≤j)2 E
h
‚à•u(l)
k ‚à•2i (68)"
SUPPORTING LEMMA,0.8087071240105541,Under review as a conference paper at ICLR 2022
SUPPORTING LEMMA,0.8100263852242744,"‚â§(1 ‚àíŒ≤k) E
y(l)
k ‚àíf (l) 
y(l‚àí1)
k

2"
SUPPORTING LEMMA,0.8113456464379947,"+ 4 (1 ‚àíŒ≤k)2 C2
l E
y(l‚àí1)
k
‚àíy(l‚àí1)
k+1

2
+ 2Œ≤2
kV 2 + 1 Œ≤k k‚àí1
Y"
SUPPORTING LEMMA,0.8126649076517151,"j=k‚àít
(1 ‚àíŒ≤j)2 E
h
‚à•u(l)
k ‚à•2i (69)"
SUPPORTING LEMMA,0.8139841688654353,"8.2.2
REMAINING STEPS TOWARDS THEOREM 2."
SUPPORTING LEMMA,0.8153034300791556,"Using the smoothness of F(Œ∏k), we have"
SUPPORTING LEMMA,0.816622691292876,"F(Œ∏k+1) ‚â§F(Œ∏k) + ‚ü®‚àáF(Œ∏k), Œ∏k+1 ‚àíŒ∏k‚ü©+ L"
SUPPORTING LEMMA,0.8179419525065963,2 ‚à•Œ∏k+1 ‚àíŒ∏k‚à•2
SUPPORTING LEMMA,0.8192612137203166,"= F(Œ∏k) ‚àíŒ±k ‚ü®‚àáF(Œ∏k), ‚àák‚ü©+ L"
SUPPORTING LEMMA,0.820580474934037,2 ‚à•Œ∏k+1 ‚àíŒ∏k‚à•2
SUPPORTING LEMMA,0.8218997361477572,= F(Œ∏k) ‚àíŒ±kE[‚à•‚àáF(Œ∏k)‚à•2] + L
SUPPORTING LEMMA,0.8232189973614775,"2 ‚à•Œ∏k+1 ‚àíŒ∏k‚à•2 + Œ±k ‚ü®‚àáF(Œ∏k), ‚àáF(Œ∏k) ‚àí‚àák‚ü©. (70)"
SUPPORTING LEMMA,0.8245382585751979,"Conditioning on Fk and taking expectation of both sides w.r.t. Œæk, we have"
SUPPORTING LEMMA,0.8258575197889182,E [F (Œ∏k+1) |Fk]
SUPPORTING LEMMA,0.8271767810026385,"‚â§F(Œ∏k) ‚àíŒ±kE
h
‚à•‚àáF(Œ∏k)‚à•2i
+ L"
"E
H",0.8284960422163589,"2 E
h
‚à•Œ∏k+1 ‚àíŒ∏k‚à•2 | Fk
i
+ Œ±k ‚ü®‚àáF(Œ∏k), E [‚àáF(Œ∏k) ‚àí‚àák|Fk]‚ü©"
"E
H",0.8298153034300791,"‚â§F(Œ∏k) ‚àíŒ±kE
h
‚à•‚àáF(Œ∏k)‚à•2i
+ L"
"E
H",0.8311345646437994,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T +1Œ±2
k + Œ±kE [‚à•‚àáF(Œ∏k)‚à•] ‚à•E [‚àák|Fk] ‚àí‚àáF(Œ∏k)‚à•"
"E
H",0.8324538258575198,"(Lemma 4)
‚â§
F(Œ∏k) ‚àíŒ±kE
h
‚à•‚àáF(Œ∏k)‚à•2i
+ L"
"E
H",0.8337730870712401,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T +1Œ±2
k + Œ±k T
X"
"E
H",0.8350923482849604,"l=1
AlE [‚à•‚àáF(Œ∏k)‚à•] E
hy(T )
k+1 ‚àíf (T )(y(T ‚àí1)
k+1
)
 |Fk
i"
"E
H",0.8364116094986808,"‚â§F(Œ∏k) ‚àíŒ±k  1 ‚àíŒ±k 2Œ≤k T
X"
"E
H",0.837730870712401,"l=1
A2
l !"
"E
H",0.8390501319261213,"E
h
‚à•‚àáF(Œ∏k)‚à•2i
+ Œ≤k 2 T
X"
"E
H",0.8403693931398417,"l=1
E
y(T )
k+1 ‚àíf (T )(y(T ‚àí1)
k+1
)

2
|Fk  + L"
"E
H",0.841688654353562,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T +1Œ±2
k. (71)"
"E
H",0.8430079155672823,The subsequent analysis builds on the following Lyapunov function:
"E
H",0.8443271767810027,"Vk := F(Œ∏k) ‚àíF(Œ∏‚àó) + T
X"
"E
H",0.8456464379947229,"l=1
E
y(l)
k ‚àíf (l)(y(l‚àí1)
k
)

2
(72)"
"E
H",0.8469656992084432,where Œ∏‚àóis the optimal solution of the problem.
"E
H",0.8482849604221636,"Conditioning on Fk and taking expectation of Vk+1 w.r.t. Œæk, we have"
"E
H",0.8496042216358839,Under review as a conference paper at ICLR 2022
"E
H",0.8509234828496042,"E [Vk+1|Fk] ‚â§Vk ‚àíŒ±k  1 ‚àíŒ±k 2Œ≤k T
X"
"E
H",0.8522427440633246,"l=1
A2
l !"
"E
H",0.8535620052770448,"E
h
‚à•‚àáF (Œ∏k)‚à•2i
+ L"
"E
H",0.8548812664907651,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T +1Œ±2
k"
"E
H",0.8562005277044855,"+ (1 + Œ≤k) T
X"
"E
H",0.8575197889182058,"l=1
E
y(l)
k+1 ‚àíf (l) 
y(l‚àí1)
k+1

2
|Fk 
‚àí T
X"
"E
H",0.8588390501319261,"l=1
E
y(l)
k ‚àíf (l) 
y(l‚àí1)
k

2
|Fk  ‚àíŒ≤k 2 T
X"
"E
H",0.8601583113456465,"l=1
E
y(l)
k+1 ‚àíf (l) 
y(l‚àí1)
k+1

2
|Fk "
"E
H",0.8614775725593667,"(Lemma 8)
‚â§
Vk ‚àíŒ±k  1 ‚àíŒ±k 2Œ≤k T
X"
"E
H",0.862796833773087,"l=1
A2
l !"
"E
H",0.8641160949868074,"E
h
‚à•‚àáF (Œ∏k)‚à•2i
+ L"
"E
H",0.8654353562005277,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T +1Œ±2
k"
"E
H",0.866754617414248,"+ ((1 + Œ≤k) (1 ‚àíŒ≤k) ‚àí1) T
X"
"E
H",0.8680738786279684,"l=1
E
y(l)
k ‚àíf (l)(y(l‚àí1)
k
)

2
|Fk "
"E
H",0.8693931398416886,"+ 4 (1 + Œ≤k) (1 ‚àíŒ≤k)2C2
1E
h
‚à•Œ∏k ‚àíŒ∏k‚àí1‚à•2 |Fk
i + T
X l=2"
"E
H",0.8707124010554089,"
4 (1 + Œ≤k) (1 ‚àíŒ≤k)2C2
l + Œ≥l

E
hy(l‚àí1)
k+1 ‚àíy(l‚àí1)
k
 |Fk
i"
"E
H",0.8720316622691293,"+ 2(1 + Œ≤k)Œ≤2
kNV 2 + 1 + Œ≤k Œ≤k k‚àí1
Y"
"E
H",0.8733509234828496,"j=k‚àít
(1 ‚àíŒ≤j)2
T
X"
"E
H",0.8746701846965699,"l=1
E
h
‚à•u(l)
k ‚à•2i ‚àí T
X"
"E
H",0.8759894459102903,"l=2
Œ≥lE
hy(l‚àí1)
k+1 ‚àíy(l‚àí1)
k
 |Fk
i
‚àíŒ≤k 2 T
X"
"E
H",0.8773087071240105,"l=1
E
y(l)
k+1 ‚àíf (l)(y(l‚àí1)
k+1 )

2"
"E
H",0.8786279683377308,"‚â§Vk ‚àíŒ±k  1 ‚àíŒ±k 2Œ≤k T
X"
"E
H",0.8799472295514512,"l=1
A2
l !"
"E
H",0.8812664907651715,‚à•‚àáF (Œ∏k)‚à•2 + L
"E
H",0.8825857519788918,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T +1Œ±2
k + 2(1 + Œ≤k)Œ≤2
kNV 2"
"E
H",0.8839050131926122,"+ 4C2
1E
h
‚à•Œ∏k ‚àíŒ∏k‚àí1‚à•2 |Fk
i + T
X l=2"
"E
H",0.8852242744063324," 
4C2
l + Œ≥l

E
y(l‚àí1)
k+1 ‚àíy(l‚àí1)
k

2
|Fk  ‚àí T
X"
"E
H",0.8865435356200527,"l=2
Œ≥lE
hy(l‚àí1)
k+1 ‚àíy(l‚àí1)
k
 |Fk
i
‚àíŒ≤k 2 T
X"
"E
H",0.8878627968337731,"l=1
E
y(l)
k+1 ‚àíf (l)(y(l‚àí1)
k+1 )

2"
"E
H",0.8891820580474934,"+ 1 + Œ≤k Œ≤k k‚àí1
Y"
"E
H",0.8905013192612137,"j=k‚àít
(1 ‚àíŒ≤j)2
T
X"
"E
H",0.8918205804749341,"l=1
E
h
‚à•u(l)
k ‚à•2i"
"E
H",0.8931398416886543,Under review as a conference paper at ICLR 2022
"E
H",0.8944591029023746,"(Lemma 7)
‚â§
Vk ‚àíŒ±k  1 ‚àíŒ±k 2Œ≤k T
X"
"E
H",0.895778364116095,"l=1
A2
l !"
"E
H",0.8970976253298153,"E
h
‚à•‚àáF (Œ∏k)‚à•2i
+ L"
"E
H",0.8984168865435356,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T +1Œ±2
k"
"E
H",0.899736147757256,"+ 4C2
1E
h
‚à•Œ∏k ‚àíŒ∏k‚àí1‚à•2 |Fk
i"
"E
H",0.9010554089709762,"+3

Œ≤k
1 ‚àíŒ≤k"
"E
H",0.9023746701846965,"2
T
X l=2"
"E
H",0.9036939313984169," 
4C2
l + Œ≥l

E
y(l‚àí1)
k+1 ‚àíf (l) 
y(l‚àí2)
k+1

2"
"E
H",0.9050131926121372,"|
{z
}
:=Ik1 ‚àíŒ≤k 2 T
X"
"E
H",0.9063324538258575,"l=1
E
y(l)
k+1 ‚àíf (l) 
y(l‚àí1)
k+1

2"
"E
H",0.9076517150395779,"|
{z
}
:=Ik2 +3 T
X l=2"
"E
H",0.9089709762532981," 
4C2
l + Œ≥l

C2
l‚àí1E
y(l‚àí2)
k+1 ‚àíy(l‚àí2)
k

2
|Fk "
"E
H",0.9102902374670184,"|
{z
}
:=Ik3 ‚àí T
X"
"E
H",0.9116094986807388,"l=2
Œ≥lE
hy(l‚àí1)
k+1 ‚àíy(l‚àí1)
k
 |Fk
i"
"E
H",0.9129287598944591,"|
{z
}
:=Ik4 + "
"E
H",0.9142480211081794,"2 (1 + Œ≤k) Œ≤2
kN +

Œ≤k
1 ‚àíŒ≤k"
"E
H",0.9155672823218998,"2
T
X l=2"
"E
H",0.91688654353562," 
4C2
l + Œ≥l

! V 2"
"E
H",0.9182058047493403,"+ 1 + Œ≤k Œ≤k k‚àí1
Y"
"E
H",0.9195250659630607,"j=k‚àít
(1 ‚àíŒ≤j)2
T
X"
"E
H",0.920844327176781,"l=1
E
h
‚à•u(l)
k ‚à•2i
+ 3 k‚àí1
Y"
"E
H",0.9221635883905013,"j=k‚àít
(1 ‚àíŒ≤j)2
T
X l=2"
"E
H",0.9234828496042217," 
4C2
l + Œ≥l

E
u(l‚àí1)
k

2 (73)"
"E
H",0.924802110817942,If we deÔ¨Åne Œ≥l such that:
"E
H",0.9261213720316622,"3

Œ≤k
1 ‚àíŒ≤k"
"E
H",0.9274406332453826,"2
T
X l=2"
"E
H",0.9287598944591029," 
4C2
l + Œ≥l

< Œ≤k"
"E
H",0.9300791556728232,"2
(74) and"
"E
H",0.9313984168865436,"3
 
4C2
l + Œ≥l

C2
l‚àí1 < Œ≥l‚àí1,
(75)"
"E
H",0.9327176781002638,"then
Ik1 + Ik2 < 0
(76)"
"E
H",0.9340369393139841,"Ik3 + Ik4 < 0.
(77)"
"E
H",0.9353562005277045,It follows that
"E
H",0.9366754617414248,"E [Vk+1|Fk] ‚â§Vk ‚àíŒ±k  1 ‚àíŒ±k 2Œ≤k T
X"
"E
H",0.9379947229551451,"l=1
A2
l !"
"E
H",0.9393139841688655,"E
h
‚à•‚àáF (Œ∏k)‚à•2i
+ L"
"E
H",0.9406332453825857,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T +1Œ±2
k"
"E
H",0.941952506596306,"+ (3(4C2
2 + Œ≥2) + 4)C2
1E
h
‚à•Œ∏k ‚àíŒ∏k‚àí1‚à•2 |Fk
i + "
"E
H",0.9432717678100264,"2 (1 + Œ≤k) Œ≤2
kN +

Œ≤k
1 ‚àíŒ≤k"
"E
H",0.9445910290237467,"2
T
X l=2"
"E
H",0.945910290237467," 
4C2
l + Œ≥l

! V 2"
"E
H",0.9472295514511874,"+ 1 + Œ≤k Œ≤k k‚àí1
Y"
"E
H",0.9485488126649076,"j=k‚àít
(1 ‚àíŒ≤j)2
T
X"
"E
H",0.9498680738786279,"l=1
E
h
‚à•u(l)
k ‚à•2i
+ 3 k‚àí1
Y"
"E
H",0.9511873350923483,"j=k‚àít
(1 ‚àíŒ≤j)2
T
X l=2"
"E
H",0.9525065963060686," 
4C2
l + Œ≥l

E
u(l‚àí1)
k

2 (78)"
"E
H",0.9538258575197889,Under review as a conference paper at ICLR 2022
"E
H",0.9551451187335093,"Setting Œ≤k = Œ±k
PT ‚àí1
l=1 A2
l , we have"
"E
H",0.9564643799472295,E [Vk+1|Fk] ‚â§Vk ‚àíŒ±k
"E
H",0.9577836411609498,2 E[‚à•‚àáF (Œ∏k)‚à•2] + L
"E
H",0.9591029023746702,"2 C2
1 ¬∑ ¬∑ ¬∑ C2
T +1Œ±2
k + (3(4C2
2 + Œ≥2) + 4)C2
1E
h
‚à•Œ∏k ‚àíŒ∏k‚àí1‚à•2 |Fk
i + "
"E
H",0.9604221635883905,"2 (1 + Œ≤k) Œ≤2
kN +

Œ≤k
1 ‚àíŒ≤k"
"E
H",0.9617414248021108,"2
T
X l=2"
"E
H",0.9630606860158312," 
4C2
l + Œ≥l

! V 2"
"E
H",0.9643799472295514,"+ 1 + Œ≤k Œ≤k k‚àí1
Y"
"E
H",0.9656992084432717,"j=k‚àít
(1 ‚àíŒ≤j)2
T
X"
"E
H",0.9670184696569921,"l=1
E
u(l)
k

2
+ 3 k‚àí1
Y"
"E
H",0.9683377308707124,"j=k‚àít
(1 ‚àíŒ≤j)2
T
X l=2"
"E
H",0.9696569920844327," 
4C2
l + Œ≥l

E
u(l‚àí1)
k

2 (79)"
"E
H",0.9709762532981531,"Setting Œ±k = Œ± =
cŒ±
‚àö"
"E
H",0.9722955145118733,"K and Œ≤k = Œ≤ =
cŒ≤
‚àö"
"E
H",0.9736147757255936,"K and summing up both sides of (79) from k = 0 to K ‚àí1,
we have"
K,0.974934036939314,"1
K K‚àí1
X"
K,0.9762532981530343,"k=0
‚à•‚àáF (Œ∏k)‚à•2 ‚â§2V0"
K,0.9775725593667546,"KŒ± + LC2
1 ¬∑ ¬∑ ¬∑ C2
T +1Œ± + 2(3(4C2
2 + Œ≥2) + 4)C4
1C2
2 ¬∑ ¬∑ ¬∑ C2
T +1Œ± + 2 Œ± "
K,0.978891820580475,"2 (1 + Œ≤) Œ≤2N +

Œ≤
1 ‚àíŒ≤"
K,0.9802110817941952,"2
T
X l=2"
K,0.9815303430079155," 
4C2
l + Œ≥l

! V 2"
K,0.9828496042216359,+ 2(1 + Œ≤) (1 ‚àíŒ≤)2(t‚àí1)
K,0.9841688654353562,"Œ±Œ≤
1
K K‚àí1
X k=0 T
X"
K,0.9854881266490765,"l=1
E
u(l)
k

2"
K,0.9868073878627969,"+ 6 (1 ‚àíŒ≤)2(t‚àí1) Œ±
1
K K‚àí1
X k=0 T
X l=2"
K,0.9881266490765171," 
4C2
l + Œ≥l

E
u(l‚àí1)
k

2"
K,0.9894459102902374,"(Assumption 5)
‚â§
O
 1
‚àö K "
K,0.9907651715039578,"+ 2(1 + Œ≤) (1 ‚àíŒ≤)2(t‚àí1) Œ±Œ≤ T
X"
K,0.9920844327176781,"l=1
D2
l"
K,0.9934036939313984,"+ 6 (1 ‚àíŒ≤)2(t‚àí1) Œ±
Cu T
X"
K,0.9947229551451188,"l=1
D2
l (80)"
K,0.996042216358839,"where Cu = max
 
4C2
l + Œ≥l

for l = 1 . . . T."
K,0.9973614775725593,"This completes the proof of Theorem 2. If we can set t such that (1 ‚àíŒ≤)2(t‚àí1) = O(Œ≤3), the
algorithm achieves O(1/
‚àö"
K,0.9986807387862797,K) convergence rate.
