Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.005952380952380952,"To improve the robustness of deep classifiers against adversarial perturbations,
many approaches have been proposed, such as designing new architectures with
better robustness properties (e.g., Lipschitz-capped networks), or modifying the
training process itself (e.g., min-max optimization, constrained learning, or regu-
larization). These approaches, however, might not be effective at increasing the
margin in the input (feature) space. In this paper, we propose a differentiable regu-
larizer that is a lower bound on the distance of the data points to the classification
boundary. The proposed regularizer requires knowledge of the model’s Lipschitz
constant along certain directions. To this end, we develop a scalable method for
calculating guaranteed differentiable upper bounds on the Lipschitz constant of
neural networks accurately and efficiently. The relative accuracy of the bounds
prevents excessive regularization and allows for more direct manipulation of the
decision boundary. Furthermore, our Lipschitz bounding algorithm exploits the
monotonicity and Lipschitz continuity of the activation layers, and the resulting
bounds can be used to design new layers with controllable bounds on their Lips-
chitz constant. Experiments on the MNIST, CIFAR-10, and Tiny-ImageNet data
sets verify that our proposed algorithm obtains competitively improved results
compared to the state-of-the-art."
INTRODUCTION,0.011904761904761904,"1
Introduction"
INTRODUCTION,0.017857142857142856,"Motivated by the vulnerability of deep neural networks to adversarial attacks [1], i.e., impercep-
tible perturbations that can drastically change a model’s prediction, researchers and practitioners
have proposed various approaches to enhance the robustness of deep neural networks, including
adversarial training [2–4], regularization [5, 6], constrained learning [7], randomized smoothing [8–
10], relaxation-based defenses [11, 12], and model ensembles [13, 14]. These approaches modify
the model architecture, the optimization procedure (loss function and algorithm), the inference
mechanism, or the dataset itself [15] to enhance accuracy on both natural and adversarial examples."
INTRODUCTION,0.023809523809523808,"However, most of these methods typically do not directly operate on input margins and rather target
the output margin or its surrogates. For example, it is an established property that deep models trained
with the cross-entropy loss, which is a surrogate for maximizing the output margin (see Appendix
A.2), are prone to adversarial attacks [1]. From this perspective, it is critical to design regularized
loss functions that can explicitly and effectively target the margin in the input space [16–18]."
INTRODUCTION,0.02976190476190476,∗Equal contribution.
INTRODUCTION,0.03571428571428571,"Our Contribution
(1) Using first principles, we design a novel regularized loss function for
training adversarially robust deep classifiers. We design a differentiable regularizer, using the
Lipschitz constants of the logit differences, that is a lower bound on the input margin. We empirically
show that this regularizer promotes larger margins in the input space. (2) We develop a scalable
method for calculating guaranteed analytic and differentiable upper bounds on the Lipschitz constant
of deep networks accurately and efficiently. Our method, called LipLT, hinges on the idea of Loop
Transformation on the nonlinearities, which allows us to exploit the monotonicity and Lipschitz
continuity of activations functions effectively. We prove that the resulting upper bound is better than
the product of the Lipschitz constants of all layers (the so-called naive bound), and in practice, it
is significantly better. Furthermore, our Lipschitz bounding algorithm can be used to design new
layers with controllable bound on their Lipschitz constant. (3) We integrate our Lipschitz estimation
algorithm within the proposed training loss to develop a robust training algorithm that eliminates the
need for any inner-loop optimization subroutine. We utilize the recurring structure of the calculations
that enable parallelized implementation on GPUs. When integrated into training, the relative accuracy
of the bounds prevents excessive regularization and allows for more direct manipulation of the
decision boundary."
INTRODUCTION,0.041666666666666664,"Experiments on the MNIST, CIFAR-10, and Tiny-ImageNet data sets verify that our proposed
algorithm obtains competitively improved results compared to the state-of-the-art. Code available on
https://github.com/o4lc/CRM-LipLT."
RELATED WORK,0.047619047619047616,"1.1
Related Work"
RELATED WORK,0.05357142857142857,"In the interest of space, we only review the most relevant literature and defer the comprehensive
version to the supplementary materials."
RELATED WORK,0.05952380952380952,"Enhancing Robustness via Optimization
The most well-known approach in adversarial de-
fenses is perhaps adversarial training (AT) [4] and its certified variants [11, 19–23], which involves
minimizing a worst-case loss (or its approximation) using uniformly-bounded perturbations to the
training data. Since these approaches might hurt accuracy on non-adversarial examples, several
regularization-based methods have been proposed to trade off adversarial robustness against natural
accuracy [5, 6, 24, 25]. Intuitively, these approaches aim to control the variations of the model
close to the decision boundary or around the training data points. In general, the main challenge
is to formulate computationally efficient differentiable regularizers that can directly increase the
margin of the classifier. Of notable recent work in this direction is [18], in which the authors design a
regularizer that directly increases the margin in the input space and prioritizes more vulnerable points
by leveraging the dynamics of the decision boundary during training. Our loss function also takes
advantage of this approach, but rather than using an iterative algorithm to compute the margin, we
use Lipschitz continuity arguments to find closed-form differentiable lower bounds on the margin."
RELATED WORK,0.06547619047619048,"Robustness and Lipschitz Regularity
To obtain efficient and scalable certificates of robustness
one can bound or control the global Lipschitz constant of the model. To train robust networks, the
global Lipschitz bound is typically computed as the product of the spectral norm of linear layers (the
naive bound), which provides computationally efficient but overly conservative certificates [5, 19,
26]. If these bounds are localized (e.g., in a neighborhood of each training data point), conservatism
can be mitigated at the expense of losing computational efficiency [20, 27]. However, it is not clear
if this approach provides any advantages over other local bounding schemes [28, 29] that can be
more accurate with comparable complexity. Hence, for training purposes, it is highly desirable to
have global but less conservative differentiable Lipschitz bounds. Of notable work in this direction is
the semidefinite programming (SDP) approach of [30] (as well as its local version [31]) to provide
accurate numerical bounds, which has also been leveraged in training low-Lipschitz networks [32].
However, these SDP-based approaches are restricted to small-scale models. In comparison, LipLT is
less accurate than LipSDP but can scale to significantly larger models and larger input dimensions.
Compared to the naive method, LipLT is significantly more accurate but has a comparable practical
complexity thanks to a specialized GPU implementation that exploits the recursive nature of the
algorithm."
RELATED WORK,0.07142857142857142,"Lipschitz Layers
Another approach to improving robustness is to design new layers with control-
lable Lipschitz constant. Various methods have been proposed to design the so-called 1-Lipschitz
networks [7, 33–37]. Inspired by the LipSDP framework of [30], the recent work [38] designs
1-Lipschitz layers that generalize many of the aforementioned 1-Lipschitz designs. However, their
proposed approach is currently not applicable to multi-layer networks. In [39], the authors propose
a reparameterization that directly satisfies the SDP constraint of LipSDP, but the proposed method
cannot handle skip connections. [21] introduces ℓ∞distance neurons that are 1-Lipschitz and provide
inherent robustness against ℓ∞perturbations."
PRELIMINARIES AND NOTATION,0.07738095238095238,"1.2
Preliminaries and Notation"
PRELIMINARIES AND NOTATION,0.08333333333333333,"The log-sum-exp function is defined as LSE(x) = log(Pn
i=1 exp(xi)). For any t > 0, the per-
spective [40] of the log-sum-exp function satisfies the inequalities maxi(xi) < t−1LSE(tx) ≤
maxi(xi) + t−1 log(n). We denote the cross-entropy loss by CE(q, p) = −PK
k=1 pk log(qk),
where p, q ∈RK are on the probability simplex. For an integer y ∈{1, · · · , K}, we define uy ∈RK"
PRELIMINARIES AND NOTATION,0.08928571428571429,"as the y-th standard unit vector. For a vector a ∈RK, we have ∥a∥p = (PK
i=1 |ai|p)1/p. Additionally,
we have ∥a⊤∥p = ∥a∥q, where 1/p + 1/q = 1. When p = 1, it is understood that q = ∞. For a
matrix A ∈Rm×n, ∥A∥denotes a matrix norm of A. In this paper we focus on ∥A∥2 = σmax(A),
i.e., the spectral norm of the matrix A. We, however, note that except for the implementation notes
regarding the power iteration, the rest of the Lipschitz estimation algorithm is valid in any other
matrix norm as well. We denote the indicator function of the event E as 1E, where 1E outputs 1 if
the event E is realized and 0 otherwise."
PRELIMINARIES AND NOTATION,0.09523809523809523,"2
Certified Radius Maximization (CRM)"
PRELIMINARIES AND NOTATION,0.10119047619047619,"Consider a K-class deep classifier C(x; θ)
=
arg max1≤i≤K fi(x; θ), where f(x; θ)
=
softmax(z(x; θ)) is the vector of class probabilities, and z(x; θ) is a deep network. If a data point x
is classified correctly as y ∈{1, · · · K}, then the logit margin γ(z(x; θ), y), the difference between
the largest and second-largest logit, is γ(z(x; θ), y) = zy(x; θ) −maxj̸=y zj(x; θ)."
PRELIMINARIES AND NOTATION,0.10714285714285714,"Several existing loss functions for training classifiers are differentiable surrogates for maximizing the
logit margin, including the cross entropy, and its combination with label smoothing (see Appendix
A.2 for more details). However, maximizing the logit margin, or its surrogates, does not necessarily
increase the margin in the input space. This can happen, for example, when the model has rapid
variations close to the decision boundary. Thus, we need a regularizer targeted for maximizing the
input margin."
PRELIMINARIES AND NOTATION,0.1130952380952381,"Regularization Based on Certified Radius
For a correctly-classified data x with label y, the
distance of x to the decision boundary can be calculated by solving the optimization problem [18],"
PRELIMINARIES AND NOTATION,0.11904761904761904,"R(x, y; θ) = inf
ˆx ∥x −ˆx∥subject to γ(z(ˆx; θ), y) = 0,
(1)"
PRELIMINARIES AND NOTATION,0.125,"where the constraint enforces ˆx to be on the decision boundary. 2 To ensure a large margin in the
input space, we must, in principle, maximize the radius R(x, y; θ) of all correctly classified data
points. To achieve this goal, we can add a regularizer that penalizes small input margins,"
PRELIMINARIES AND NOTATION,0.13095238095238096,"min
θ
E(x,y)∼D[−γ(z(x; θ), y)] + λE(x,y)∼D[1{γ(z(x;θ),y)>0}g(R(x, y; θ))],
(2)"
PRELIMINARIES AND NOTATION,0.13690476190476192,"where λ > 0 is the regularization constant and g: R →R is a decreasing function to promote larger
certified radii. See appendix A.2 for more discussion on the properties of g."
PRELIMINARIES AND NOTATION,0.14285714285714285,"To optimize (2), we must compute and differentiate through R(x, y; θ). For ℓ∞norm and ReLU
activations, we can reformulate (1) as a mixed-integer linear program by using a binary representation"
PRELIMINARIES AND NOTATION,0.1488095238095238,"2We note that we can replace the equality constraint with the inequality constraint γ(z(ˆx; θ), y) ≤0 and get
the same result."
PRELIMINARIES AND NOTATION,0.15476190476190477,"of the activation functions. However, it is not affordable to solve this optimization problem during
training. Hence, we must resort to computing lower bounds on R(x, y; θ) instead."
PRELIMINARIES AND NOTATION,0.16071428571428573,"By replacing the logit margin with its soft lower bound in the constraint of (1), we obtain a lower
bound on R(x, y; θ),"
PRELIMINARIES AND NOTATION,0.16666666666666666,"R(x, y; θ) ≥min
ˆx
∥x −ˆx∥"
PRELIMINARIES AND NOTATION,0.17261904761904762,"s.t.
zy(x; θ) −1"
PRELIMINARIES AND NOTATION,0.17857142857142858,"t log
X"
PRELIMINARIES AND NOTATION,0.18452380952380953,"j̸=y
etzj(x;θ) ≤0.
(3)"
PRELIMINARIES AND NOTATION,0.19047619047619047,"To solve this non-convex problem, the authors of [18] adapt the successive linearization algorithm of
[41], which in turn does not provide a provable lower bound. To avoid this iterative algorithm and to
provide sound lower bounds, we propose to use Lipschitz continuity arguments."
PRELIMINARIES AND NOTATION,0.19642857142857142,"Lipschitz-Based Surrogates for Certified Radius
Suppose we add a norm-bounded perturbation
δ to a data point x that is classified correctly as y. For the label of x + δ to not change, we must
enforce"
PRELIMINARIES AND NOTATION,0.20238095238095238,"∆zyi(x + δ) := zy(x + δ; θ) −zi(x + δ; θ) > 0
∀i ̸= y."
PRELIMINARIES AND NOTATION,0.20833333333333334,"Suppose the logit difference x 7→∆zyi(x) is Lipschitz continuous with constant Lyi (in p norm),
implying |∆zyi(x + δ; θ) −∆zyi(x)| ≤Lyi∥δ∥p, ∀x, δ. Then we can write"
PRELIMINARIES AND NOTATION,0.21428571428571427,"∆zyi(x + δ; θ) ≥∆zyi(x; θ) −Lyi∥δ∥p
∀i ̸= y."
PRELIMINARIES AND NOTATION,0.22023809523809523,"The right-hand side remaining positive for all i ̸= y is a sufficient condition for the correct classifica-
tion of x + δ as y. This sufficient condition yields a lower bound on R(x, y; θ) as follows,"
PRELIMINARIES AND NOTATION,0.2261904761904762,"R(x, y; θ) := min
i̸=y
zy(x; θ) −zi(x; θ)"
PRELIMINARIES AND NOTATION,0.23214285714285715,"Lyi
≤R(x, y; θ).
(4)"
PRELIMINARIES AND NOTATION,0.23809523809523808,"This lower bound is the pointwise minimum of logit differences normalized by their Lipschitz
constants. This lower bound is not differentiable everywhere, but similar to the logit margin, we
propose a smooth underapproximation of the min operator using scaled LSE:"
PRELIMINARIES AND NOTATION,0.24404761904761904,"Rsoft
t
(x, y; θ) := −1"
PRELIMINARIES AND NOTATION,0.25,"t log(
X"
PRELIMINARIES AND NOTATION,0.25595238095238093,"i̸=y
exp(−tzy(x; θ) −zi(x; θ)"
PRELIMINARIES AND NOTATION,0.2619047619047619,"Lyi
)).
(5)"
PRELIMINARIES AND NOTATION,0.26785714285714285,"In addition to differentiability, another advantage of using this soft lower bound is that as opposed to
R(x, y; θ), which involves only two logits, the soft lower bound Rsoft
t
(x, y; θ) includes all the logits,
and hence, makes more effective use of information."
PRELIMINARIES AND NOTATION,0.27380952380952384,"Proposition 1 We have the following relationship between Rsoft
t
(x, y; θ) defined in (5) and
R(x, y; θ) defined in (4)."
PRELIMINARIES AND NOTATION,0.27976190476190477,"Rsoft
t
(x, y; θ) ≤R(x, y; θ) ≤Rsoft
t
(x, y; θ) + log(K −1) t
."
PRELIMINARIES AND NOTATION,0.2857142857142857,"In summary, we use the following loss function to train our classifier,"
PRELIMINARIES AND NOTATION,0.2916666666666667,"min
θ
E(x,y)∼D[−γ(z(x; θ), y)] + λE(x,y)∼D[1{γ(z(x;θ),y)>0}g(Rsoft
t
(x, y; θ))].
(6)"
PRELIMINARIES AND NOTATION,0.2976190476190476,"The first and the second terms are differentiable surrogates to the negative logit margin, and the input
margin, respectively. For example, as we show in Appendix A.2 the negative cross-entropy loss is a dif-
ferentiable lower bound on the logit margin, i.e., we can choose γ(z(x; θ), y) = −CE(z(x; θ), uy

."
PRELIMINARIES AND NOTATION,0.30357142857142855,"It now remains to estimate the Lyi’s (the Lipschitz constants of x 7→∆zyi(x)) that are needed to
compute the second term in the loss function. While any guaranteed upper bound on Lyi would
suffice to preserve the lower bound property in (4), a more accurate upper bound prevents excessive
regularization and allows for more direct manipulation of the decision boundary. To achieve this goal,
we propose a new method which we will discuss next."
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.30952380952380953,"3
Scalable Estimation of Lipschitz Constants via Loop Transfor-
mation (LipLT)"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.31547619047619047,"In this section, we propose a general-purpose algorithm for computing a differentiable upper bound on
the Lipschitz constant of deep neural networks, which is also of independent interest. For simplicity in
the exposition, we first consider single hidden layer neural networks of the form h(x) = W 1ϕ(W 0x),
where W 1, W 0 have compatible dimensions, and the bias terms are ignored without loss of generality.
The activation layers ϕ are of the form ϕ(z) = (φ(z1), · · · , φ(zn1))
z ∈Rn1, where φ: R →R
is the activation function, which we assume to be monotone and Lipschitz continuous, implying
α ≤(φ(x) −φ(x′))/(x −x′) ≤β
∀x ̸= x′ for some 0 ≤α ≤β < ∞[30, 42]. In [30], the
authors propose an SDP for computing an upper bound on the global Lipschitz constant of multi-layer
neural networks when ℓ2 norm is considered in both input and output domains. This result for the
single hidden layer case is formally stated in the following theorem.
Theorem 1 ([30]) Consider a single-layer neural network described by h(x) = W 1ϕ(W 0x). Sup-
pose ϕ(x): Rn1 →Rn1 = [φ(x1) · · · φ(xn1)], where φ is slope-restricted over R with parameters
0 ≤α ≤β < ∞. Suppose there exist ρ > 0 and diagonal T ∈Sn1
+ such that the matrix inequality"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.32142857142857145,"M(ρ, T) :="
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.3273809523809524,"""
−2αβW 0⊤TW 0 −ρIn0
(α + β)W 0⊤T
(α + β)TW 0
−2T + W 1⊤W 1 #"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.3333333333333333,"⪯0,
(7)"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.3392857142857143,"holds. Then ∥h(x) −h(y)∥2 ≤√ρ∥x −y∥2 for all x, y ∈Rn0."
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.34523809523809523,"The key advantage of this SDP formulation is that we can exploit several properties of the structure,
namely monotonicity (α ≥0) and Lipschitz continuity (β < ∞) of the activation functions, as well
as using the same activation function in the activation layer. However, solving this SDP and enforcing
them during training can be challenging even for small-scale neural networks. The recent work [43]
has exploited the chordal structure of the resulting SDP imposed by the sequential structure of the
network to solve the SDP for larger instances. However, these approaches are still unable to scale to
larger problems and are not suitable for training purposes."
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.35119047619047616,"To guide the search for analytic solutions to the linear matrix inequality (LMI) of Theorem 1, the
authors in [38] consider the following residual structure,
h(x) = Hx + Gϕ(Wx).
(8)
Then it can be shown that the LMI condition (7) generalizes to condition (9) (see Appendix A.3 for
details)."
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.35714285714285715,"M(ρ, T) :=

−2αβW ⊤TW + H⊤H −ρIn0
(α + β)W ⊤T + H⊤G
(α + β)TW + G⊤H
−2T + G⊤G"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.3630952380952381,"
⪯0,
(9)"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.36904761904761907,"By choosing ρ = 1, H = I and G = −(α + β)W ⊤T, then the LMI (9) simplifies to (α +
β)2TWW ⊤T ⪯2T (all blocks in the LMI except for the lower diagonal block become zero). When
we restrict T to be positive definite, then the latter condition is equivalent to (α+β)2WW ⊤⪯2T −1,
which can be satisfied analytically using various choices of T [38]. In summary, the function
h(x) = x−(α+β)W ⊤Tϕ(Wx) is guaranteed to be 1-Lipschitz as long as (α+β)2WW ⊤⪯2T −1."
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.375,"A potential issue with the above parameterization (and 1-Lipschitz networks in general) is that, since
the true Lipschitz constant of the layer can be less than one, the multi-layer concatenation can become
overly contractive. One way to resolve this limitation is to modify the parameterization as follows.
Proposition 2 Suppose WW ⊤⪯
2ρ
(α+β)2 T −1 for some ρ > 0 and some diagonal positive definite
T. Then the following function is √ρ-Lipschitz."
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.38095238095238093,"h(x) = √ρx −α + β
√ρ W ⊤Tϕ(Wx).
(10)"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.3869047619047619,"Now if we make a cascade connection of L ≥2 layers of the form (10) (each having its own ρ),
a naive bound on the Lipschitz constant of the corresponding deep network would be QL
i=1 ρ1/2
i
.
However, this upper bound can still be very crude. We now propose an alternative approach to
compute an analytic upper bound on the Lipschitz constant of (8). For the multi-layer case, we then
show that our method can capture the coupling between different layers to improve the naive bound.
For the sake of space, we defer all the proofs to the supplementary materials. 𝑥 + - +
+"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.39285714285714285,"Ψ
Loop Transformation"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.39880952380952384,"+mS6ehvpsro9PF/81Edo="">AB7nicbVDLSgMxFL3xWeur6tJNsAh1U2bE17LgxmUF+4B2KJk04ZmMiHJiGXoR7hxoYhbv8edf2PazkJbD1w4nHMv94TKsGN9bxvtLK6tr6xWdgqbu/s7u2XDg6bJk1ZQ2aiES3Q2KY4JI1LeCtZVmJA4Fa4Wj26nfemTa8EQ+2LFiQUwGkecEukVlcZXnk65XKXtWbAS8TPydlyFHvlb6/YSmMZOWCmJMx/eUDTKiLaeCTYrd1DBF6IgMWMdRSWJmgmx27gSfOqWPo0S7khbP1N8TGYmNGceh64yJHZpFbyr+53VSG90EGZcqtUzS+aIoFdgmePo7nPNqBVjRwjV3N2K6ZBoQq1LqOhC8BdfXibN86p/Vb28vyjXvDyOAhzDCVTAh2uowR3UoQEURvAMr/CGFHpB7+hj3rqC8pkj+AP0+QPHTI8p</latexit> (x) x
x"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.40476190476190477,"(x1, '(x1))"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.4107142857142857,"(x2, '(x2))"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.4166666666666667,"B+HicbZDLSsNAFIZP6q3WS6Mu3QwWoQUpSfG2LhxWcFeoA1hMp20QycXZiZiDX0SNy4UceujuPNtnLRZaOsPAx/OYdz5vdizqSyrG+jsLa+sblV3C7t7O7tl82Dw46MEkFom0Q8Ej0PS8pZSNuKU57saA48DjtepObrN59oEKyKLxX05g6AR6FzGcEK25Zrn6DbOBrFkGdRqrlmx6tZcaBXsHCqQq+WaX4NhRJKAhopwLGXftmLlpFgoRjidlQaJpDEmEzyifY0hDqh0vnhM3SqnSHyI6FfqNDc/T2R4kDKaeDpzgCrsVyuZeZ/tX6i/GsnZWGcKBqSxSI/4UhFKEsBDZmgRPGpBkwE07ciMsYCE6WzKukQ7OUvr0KnUbcv6xd35WmlcdRhGM4gSrYcAVNuIUWtIFAs/wCm/Gk/FivBsfi9aCkc8cwR8Znz+q/JHB</latexit>(x2,  (x2))"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.4226190476190476,"(x1,  (x1))"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.42857142857142855,"x
G
Φ
W"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.43452380952380953,"i1bnil3xTebOuKgqHmJ8aU="">AB6HicbVDLSgNBEOz1GeMr6tHLYBA8hV3xdQx4yTEB84BkCbOT3mTM7OwyMyuEJV/gxYMiXv0kb/6Nk2QPmljQUFR1090VJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJsFrHqBFSj4BKbhuBnUQhjQKB7WB8P/PbT6g0j+WDmSToR3QoecgZNVZq1Pqlsltx5yCrxMtJGXLU+6Wv3iBmaYTSMEG17npuYvyMKsOZwGmxl2pMKBvTIXYtlTRC7WfzQ6fk3CoDEsbKljRkrv6eyGik9SQKbGdEzUgvezPxP6+bmvDOz7hMUoOSLRaFqSAmJrOvyYArZEZMLKFMcXsrYSOqKDM2m6INwVt+eZW0LiveTeW6cVWunkcBTiFM7gAD26hCjWoQxMYIDzDK7w5j86L8+58LFrXnHzmBP7A+fwBm0+MxA=</latexit>H"
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.44047619047619047,B6HicbVDLSgNBEOz1GeMr6tHLYBA8hV3xdQx4yTEB84BkCbOT3mTM7OwyMyuEJV/gxYMiXv0kb/6Nk2QPmljQUFR1090VJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJsFrHqBFSj4BKbhuBnUQhjQKB7WB8P/PbT6g0j+WDmSToR3QoecgZNVZq1Pqlsltx5yCrxMtJGXLU+6Wv3iBmaYTSMEG17npuYvyMKsOZwGmxl2pMKBvTIXYtlTRC7WfzQ6fk3CoDEsbKljRkrv6eyGik9SQKbGdEzUgvezPxP6+bmvDOz7hMUoOSLRaFqSAmJrOvyYArZEZMLKFMcXsrYSOqKDM2m6INwVt+eZW0LiveTeW6cVWunkcBTiFM7gAD26hCjWoQxMYIDzDK7w5j86L8+58LFrXnHzmBP7A+fwBm0+MxA=</latexit>H G
SCALABLE ESTIMATION OF LIPSCHITZ CONSTANTS VIA LOOP TRANSFOR-,0.44642857142857145,"B6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PAi8cEzAOSJcxOepMxs7PLzKwQlnyBFw+KePWTvPk3TpI9aGJBQ1HVTXdXkAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHsHM0nQj+hQ8pAzaqzUaPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYJq3fXcxPgZVYzgdNSL9WYUDamQ+xaKmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGtn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUWZsNiUbgrf8ipXVS96+pV47JSc/M4inACp3AOHtxADe6hDk1gPAMr/DmPDovzrvzsWgtOPnMfyB8/kDsguM0w=</latexit>W
Φ ↵+ β"
I,0.4523809523809524,"2
I ↵+ β"
GW,0.4583333333333333,"2
GW"
GW,0.4642857142857143,"yWoF8zHZWYI="">ACKXicbVDLSgMxFM3UV62vUZdugkVoF5aZ4mtZcOyglWhU8qd9I4NzTxMsUy9Hfc+CtuFBR164+Y1gGfBwIn5xLco+fCK6047xahZnZufmF4mJpaXldc1e3zhXcSoZtlgsYnpg0LBI2xprgVeJhIh9AVe+IPjiX8xRKl4HJ3pUYKdEK4iHnAG2khdu+GBSPpAPYHX1AsksMwbgkz6vHLTrVd3vy5udZwZadewcR73UPXLjs1Zwr6l7g5KZMcza796PViloYaSZAqbrJLqTgdScCRyXvFRhAmwAV9g2NIQVSebjqmO0bp0SCW5kSaTtXvExmESo1C3yRD0H3125uI/3ntVAdHnYxHSaoxYp8PBamgOqaT2miPS2RajAwBJrn5K2V9MHVpU27JlOD+XvkvOa/X3IPa/uleueHkdRTJFtkmFeKSQ9IgJ6RJWoSRW3JPnsizdWc9WC/W2e0YOUzm+QHrPcPZOmCw=</latexit>↵'(x2) −'(x1)"
GW,0.47023809523809523,x2 −x1
GW,0.47619047619047616,"β
−β −↵"
GW,0.48214285714285715,"2
 (x2) − (x1)"
GW,0.4880952380952381,x2 −x1 β −↵ 2
GW,0.49404761904761907,"Figure 1: Loop transformation on a residual layer of the form h(x) = Hx + Gϕ(Wx). Here we use
φ(x) = tanh x for the illustration of the loop transformation."
GW,0.5,"The power of loop transformation
Starting from (8), using the fact that ϕ is β-Lipschitz, a global
Lipschitz constant of h can be computed as"
GW,0.5059523809523809,∥h(x)−h(x′)∥≤∥H(x −x′)∥+∥G(ϕ(Wx) −ϕ(Wx′))∥
GW,0.5119047619047619,"≤(∥H∥+ β∥G∥∥W∥)
|
{z
}
Lnaive"
GW,0.5178571428571429,"∥x −x′∥.
(11)"
GW,0.5238095238095238,"This upper bound is pessimistic as it does not exploit the monotonicity (α ≥0) of activation functions.
In other words, this bound would not change as long as α ≥−β. To inform the bound with
monotonicity, we perform a loop transformation [44, 45] to bring the non-linearities to the symmetric
sector [−(β −α)/2, (β −α)/2], resulting in the representation"
GW,0.5297619047619048,h(x) = (H + α + β
GW,0.5357142857142857,"2
GW)x + Gψ(Wx),
(12)"
GW,0.5416666666666666,where ψ(x) := ϕ(x) −α+β
GW,0.5476190476190477,"2 x is the loop transformed nonlinearity, which is no longer monotone, and
is β−α"
GW,0.5535714285714286,2 -Lipschitz– see Figure 1. We can now compute a Lipschitz constant as
GW,0.5595238095238095,∥h(x)−h(x′)∥=∥(H + α + β
GW,0.5654761904761905,"2
GW)(x −x′)+G(Ψ(Wx)−Ψ(Wx′))∥"
GW,0.5714285714285714,≤∥(H + α + β
GW,0.5773809523809523,"2
GW)(x −x′)∥+∥G(Ψ(Wx) −Ψ(Wx′))∥"
GW,0.5833333333333334,≤(∥H + α + β
GW,0.5892857142857143,"2
GW∥+ β −α"
GW,0.5952380952380952,"2
∥G∥∥W∥)
|
{z
}
LLT"
GW,0.6011904761904762,∥x −x′∥. (13)
GW,0.6071428571428571,"As we show below, this loop transformation does improve the naive bound. In Appendix A.3 we also
prove the optimality of the transformation."
GW,0.6130952380952381,"Proposition 3 Consider a single-layer residual structure given by h(x) = Hx + Gϕ(Wx). Suppose
ϕ: Rn1 →Rn1, ϕ(x) = [φ(x1) · · · φ(xn1)], where φ is slope restricted in [α, β]. Then h is Lipschitz
continuous with constant ∥H + α+β"
GW,0.6190476190476191,2 GW∥+ β−α
GW,0.625,2 ∥G∥∥W∥≤Lnaive.
GW,0.6309523809523809,"Bound refinement.
For the case of the ℓ2 norm, we can further improve the derived bound in (13).
Specifically, we can bound the Lipschitz constant of the second term of h in (12) analytically using
LipSDP, which we summarize in the following theorem. 3.
Theorem 2 Suppose there exists a diagonal T that satisfies G⊤G ⪯T. Then a Lipschitz constant of
(8) in ℓ2 norm is given by"
GW,0.6369047619047619,LLT-SDP(T) = ∥H + α + β
GW,0.6428571428571429,"2
GW∥2 + β −α"
GW,0.6488095238095238,"2
∥W ⊤TW∥"
GW,0.6547619047619048,"1
2
2 ."
GW,0.6607142857142857,"By optimizing over the choice of diagonal T satisfying G⊤G ⪯T, we can find the best Lipschitz
bound achievable by this method. In this paper, however, we are more interested in analytical choices
of T. Using the same techniques outlined in [38], we have the following choices:"
GW,0.6666666666666666,"1. Spectral Normalization (SN): We can satisfy G⊤G ⪯T simply by choosing T = TSN := ∥G∥2
2I.
In this case, LLT-SDP(TSN) = ∥H + α+β"
GW,0.6726190476190477,2 GW∥2 + β−α
GW,0.6785714285714286,2 ∥W∥2∥G∥2 = LLT.
GW,0.6845238095238095,"2. AOL [36]: Choose Tii = Pn
j=1 |G⊤G|ij. As a result, T −G⊤G is real-symmetric and diagonally
dominant with positive diagonal elements, and thus, is positive semidefinite.
3. SLL [38]: Choose Tii = Pn
j=1 |G⊤G|ij
qj
qi , where qi > 0. [38] uses the Gershgorin circle
theorem to prove that this choice of T satisfies G⊤G ⪯T."
MULTI-LAYER NEURAL NETWORKS,0.6904761904761905,"3.1
Multi-layer Neural Networks"
MULTI-LAYER NEURAL NETWORKS,0.6964285714285714,"We now extend the proposed technique to the multi-layer case. Consider the following structure,
yk = Wkxk
k = 0, · · · , L
xk+1 = Hkxk + Gkϕ(yk)
k = 0, · · · , L −1
(14)"
MULTI-LAYER NEURAL NETWORKS,0.7023809523809523,"with WL = I for consistency. Then yL = xL is the output of the neural network. To compute
a Lipschitz constant, we first apply loop transformation to each activation layer, resulting in the
following representation."
MULTI-LAYER NEURAL NETWORKS,0.7083333333333334,Lemma 1 Consider the sequences in (14). Define ˆHk := Hk + α+β
MULTI-LAYER NEURAL NETWORKS,0.7142857142857143,"2 GkWk for k = 0, · · · , L −1.4
Then"
MULTI-LAYER NEURAL NETWORKS,0.7202380952380952,"yk+1 = Wk+1 ˆHk · · · ˆH0x0 + k
X"
MULTI-LAYER NEURAL NETWORKS,0.7261904761904762,"j=0
Wk+1 ˆHk · · · ˆHj+1Gjψ(yj)
k = 0, · · · , L −1.
(15)"
MULTI-LAYER NEURAL NETWORKS,0.7321428571428571,"We are now ready to state our result for the multi-layer case.
Theorem 3 Let mk, k ≥1 be defined recursively as"
MULTI-LAYER NEURAL NETWORKS,0.7380952380952381,"mk+1 = ∥Wk+1 ˆHk · · · ˆH0∥+ β −α 2 k
X"
MULTI-LAYER NEURAL NETWORKS,0.7440476190476191,"j=0
∥Wk+1 ˆHk · · · ˆHj+1Gj∥mj
(16)"
MULTI-LAYER NEURAL NETWORKS,0.75,"with m0 = ∥W0∥. Then mk is a Lipschitz constant of x0 7→yk, k = 0, · · · , L −1. In particular, mL
is a Lipschitz constant of the neural network x0 7→yL = xL."
MULTI-LAYER NEURAL NETWORKS,0.7559523809523809,"Similar to the single-layer case, we can refine the bounds for the ℓ2 norm. For the sake of space, we
defer this to the Appendix, and we use the bounds in (16) in our experiments."
MULTI-LAYER NEURAL NETWORKS,0.7619047619047619,"Complexity
For an L-layer network, the recursion in (16) requires O(L2) number of matrix norm
calculations. For the naive bound, this number is exactly L. Despite this increase in complexity, we
utilize the recurring structure of the calculations that enable parallelized implementation on GPUs.
This utilization results in a reduction of the time complexity to O(L), the same as the naive Lipschitz
estimation algorithm. We provide a more detailed analysis of the computational and time complexity
and the GPU implementation in Appendix A.4."
MULTI-LAYER NEURAL NETWORKS,0.7678571428571429,"3The Theorem of [30] assumes that α ≥0 (monotonicity), but it can be shown that the theorem holds as long
as −∞< α, which is the case for the loop-transformed nonlinearity.
4Note that we let ˆHk · · · ˆHj+1

j=k = I."
EXPERIMENTAL RESULTS,0.7738095238095238,"4
Experimental Results"
EXPERIMENTAL RESULTS,0.7797619047619048,"In this section, we evaluate our proposed method for training deep classifiers on the MNIST [46],
CIFAR-10 [47] and Tiny-Imagement [48] datasets. We compare our results with the closely related
state-of-the-art methods. We further analyze the efficiency of our improved Lipschitz bounding
algorithm and leverage it to compute the certified robust radii for the test dataset."
EXPERIMENTAL RESULTS,0.7857142857142857,"4.1
ℓ2-Robustness"
EXPERIMENTAL RESULTS,0.7916666666666666,"Experimental setup
We train using our novel loss function to certify robustness (6), using cross-
entropy as the differentiable surrogate, against ℓ2 perturbations of size 1.58 on MNIST and 36/255
on CIFAR-10 and Tiny-Imagenet 5. We train convolutional neural networks of the form mCnF
with ReLU activation functions, as in [20], where m and n denote the number of convolutional and
fully connected layers, respectively. The details of the architectures, training process, and most
hyperparameters are deferred to the supplementary materials."
EXPERIMENTAL RESULTS,0.7976190476190477,"Baselines
For baselines, we consider recent state-of-the-art methods that have provided results on
convolutional neural networks of similar size. We consider: (1) GloRo [5] which uses naive Lipschitz
estimation with a smoothness regularizer inspired by TRADES; (2) Local-lip-B/G (+ MaxMin) [20]
which uses local Lipschitz constant calculation along with cut-off ReLU and MaxMin activation
functions; (3) LipConvnet-5/10/15 [34] that uses 1-Lipschitz convolutional networks with GNP
Householder activations; (4) SLL-small (SDP-based Lipschitz Layers) [38] which is a much larger
27 layer 1-Lipschitz network; (5) AOL-Small/Medium [36] which presents much larger 1-Lipschitz
networks trained to have almost orthogonal layers."
EXPERIMENTAL RESULTS,0.8035714285714286,"Evaluation
We evaluate the accuracy of the trained networks under 3 criteria; standard accuracy,
adversarial accuracy, and certified accuracy. For adversarial accuracy we perform PGD attacks using
[4] (hyperparameters provided in supplementary material). For certified accuracy, we calculate the
certified radii using (4) and verify if they are larger than the priori-assigned perturbation budget. For
the methods of recent literature, we report their best numbers as per the manuscript."
EXPERIMENTAL RESULTS,0.8095238095238095,"Results
The results of the experiments are presented in Table 1. On MNIST, we outperform the
state-of-the-art by a margin of 7.5% on verified accuracy whilst maintaining the same standard
accuracy. On CIFAR-10 we surpass all of the state-of-the-art in verified accuracy. Furthermore, on
methods using networks of similar sizes, i.e., 6C2F or LipConvnet-5, we surpass by a margin of
around 4%. The networks of the recent literature AOL [36] and SLL [38] are achieving slightly worse
certified accuracy even with much larger networks. On Tiny-Imagenet, our current best results are on
par with the state-of-the-art."
LIPSCHITZ ESTIMATION,0.8154761904761905,"4.1.1
Lipschitz Estimation"
LIPSCHITZ ESTIMATION,0.8214285714285714,"A key part of our method is the new improved Lipschitz estimation algorithm (LipLT) and the
effective use of pairwise Lipschitz constants. Unlike previous works that estimate the pairwise
Lipschitz between class i and j by the upper bound
√"
LIPSCHITZ ESTIMATION,0.8273809523809523,"2Lz [20], where Lz is the Lipschitz constant
of the whole network, or as Li + Lj [5], where Li is the Lipschitz constant of the i-th class, we
approximate this value directly as Lij by considering the map zi(x; θ) −zj(x; θ). Table 2 shows the
average statistic for Lipschitz constants estimated using different methods. Our improved Lipschitz
calculation algorithm provides near an order-of-magnitude improvement over the naive method.
Furthermore, Table 2 portrays the superiority of using pairwise Lipschitz constants instead of other
proxies. Figure 2a illustrates the difference between using LipLT versus the naive method to calculate
the certified radius. In this experiment, the naive method barely certifies one percent of the data
points at the perturbation level of 1.58. However, using LipLT, we can certify a significant portion of"
LIPSCHITZ ESTIMATION,0.8333333333333334,5We selected these values for the perturbation budget for consistency with prior work.
LIPSCHITZ ESTIMATION,0.8392857142857143,"Table 1: Comparison with recent certified training algorithms. Best certified training accuracies are
highlighted in bold.
∗Due to a size mismatch that occurs in power iteration, we had to modify the architecture slightly by changing the padding of some of the convolutional layers. The
number of neurons are the same as that of the original architecture in [20]. More details in the supplementary material"
LIPSCHITZ ESTIMATION,0.8452380952380952,"Method
Model
Clean (%)
PGD (%)
Certified(%)"
LIPSCHITZ ESTIMATION,0.8511904761904762,MNIST (ϵ = 1.58)
LIPSCHITZ ESTIMATION,0.8571428571428571,"Standard
4C3F
99.0
45.4
0.0
GloRo
4C3F
92.9
68.9
50.1
Local-Lip
4C3F
96.3
78.2
55.8
CRM (ours)
4C3F
96.27
88.04
63.37"
LIPSCHITZ ESTIMATION,0.8630952380952381,CIFAR-10 (ϵ = 36/255)
LIPSCHITZ ESTIMATION,0.8690476190476191,"Standard
6C2F
87.5
32.5
0.0
GloRo
6C2F
77.0
69.2
58.4
Local-Lip-G
6C2F
76.4
69.2
51.3
Local-Lip-B
6C2F
70.7
64.8
54.3
Local-Lip-B + MaxMin
6C2F
77.4
70.4
60.7
LipConvnet
5-CR
75.31
-
60.37
LipConvnet
10-CR
76.23
-
62.57
LipConvnet
15-CR
76.39
-
62.96
SLL
Small
71.2
-
62.6
AOL
Small
69.8
-
62.0
AOL
Medium
71.1
-
63.8
CRM (ours)
6C2F
74.82
72.31
64.16"
LIPSCHITZ ESTIMATION,0.875,Tiny-Imagenet (ϵ = 36/255)
LIPSCHITZ ESTIMATION,0.8809523809523809,"Standard
8C2F
35.9
19.4
0.0
Local-Lip-G
8C2F
37.4
34.2
13.2
Local-Lip-B
8C2F
30.8
28.4
20.7
GloRo
8C2F
35.5
32.3
22.4
Local-Lip-B + MaxMin
8C2F
36.9
33.3
23.4
SLL
Small
26.6
-
19.5
CRM (ours)
8C2F∗
23.97
23.04
17.98"
LIPSCHITZ ESTIMATION,0.8869047619047619,"(a)
(b)
(c)"
LIPSCHITZ ESTIMATION,0.8928571428571429,"Figure 2: (a) Distribution of certified radii calculated using direct pairwise Lipschitz constants for
the MNIST test dataset for the network trained using CRM. (b-c) Comparison of the distribution
of the certified radii for a model trained using CRM (top) versus a standard trained model (bottom)
for MNIST (b) and CIFAR-10 (c). For any given ϵ, the probability curves denote the empirical
probability of a data point from that data set having a certified radius of at least ϵ."
LIPSCHITZ ESTIMATION,0.8988095238095238,"the data points.
We further conduct an analysis of the certified radii of the data points for regularized versus unregular-
ized networks for MNIST and CIFAR-10. Figures 2b and 2c illustrate the distribution of the certified
radii. These radii were computed using the direct pairwise Lipschitz bounding. When comparing
the results of the CRM-trained models (top figures) with the standard models (bottom figures), it
becomes evident that our loss function enhances the certified radius."
LIPSCHITZ ESTIMATION,0.9047619047619048,"Table 2: Comparison of average pairwise Lipschitz constant calculated using the naive and improved
method on networks trained using CRM. We use the same architectures as Table 1. To calculate the
averages, we form all unique pairs and calculate the Lipschitz constant according to the relevant
formulation and then take the average."
LIPSCHITZ ESTIMATION,0.9107142857142857,"MNIST
CIFAR10
Tiny-Imagenet"
LIPSCHITZ ESTIMATION,0.9166666666666666,"Lij
Li + Lj
√"
"L
LIJ",0.9226190476190477,"2L
Lij
Li + Lj
√"
"L
LIJ",0.9285714285714286,"2L
Lij
Li + Lj
√"
L,0.9345238095238095,2L
L,0.9404761904761905,"Naive
266.08
1285.18
2759.63
93.52
131.98
139.75
313.62
485.62
2057.00
Improved
64.10
212.68
419.83
11.26
18.30
23.43
9.46
14.77
60.37"
LIMITATIONS,0.9464285714285714,"5
Limitations"
LIMITATIONS,0.9523809523809523,"Although our proposed formulation has an elegant mathematical structure, there exist some limitations:
(1) We are still using global Lipschitz constants to bound the margin, which can be conservative. We
will investigate how we can localize our calculations without a significant increase in computation.
(2) Computation of pairwise Lipschitz bounds for very deep architectures and or a large number of
classes can become computationally intensive for training purposes (see Appendix A.4 for further
discussions). (3) Several hyper-parameters require manual tuning. It is highly desirable to explore
adaptive approaches to automatically select these hyper-parameters."
CONCLUSION,0.9583333333333334,"6
Conclusion"
CONCLUSION,0.9642857142857143,"Adversarial defense methods are numerous and their approaches are different, but they all attempt
to explicitly or implicitly increase the margin of the classifier, which is a measure of adversarial
robustness (or vulnerability). From this perspective, it is highly desirable to develop adversarial
defenses that can manipulate the decision boundary and increase the margin effectively and efficiently.
We attempt to maximize the input margin by penalizing the Lipschitz constant of the neural network
along vulnerable directions. Additionally, we develop a new method for calculating guaranteed
analytic and differentiable upper bounds on the Lipschitz constant of the deep network. LipLT is
provably better than the naive Lipschitz constant. We have also provided a parallelized implementation
of LipLT using the recurring structure of the calculations, which is fast and scalable. Our proposed
method achieves competitive results in terms of verified accuracy on the MNIST and CIFAR-10
datasets."
REFERENCES,0.9702380952380952,References
REFERENCES,0.9761904761904762,"[1]
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus. “Intriguing properties of neural networks”. In: arXiv preprint
arXiv:1312.6199 (2013).
[2]
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. “Explaining and harnessing
adversarial examples”. In: arXiv preprint arXiv:1412.6572 (2014).
[3]
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. “Adversarial machine learning at scale”.
In: arXiv preprint arXiv:1611.01236 (2016).
[4]
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. “Towards deep learning models resistant to adversarial attacks”. In: arXiv preprint
arXiv:1706.06083 (2017).
[5]
Klas Leino, Zifan Wang, and Matt Fredrikson. “Globally-robust neural networks”. In: Interna-
tional Conference on Machine Learning. PMLR. 2021, pp. 6212–6222.
[6]
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael
Jordan. “Theoretically principled trade-off between robustness and accuracy”. In: International
conference on machine learning. PMLR. 2019, pp. 7472–7482.
[7]
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. “Par-
seval networks: Improving robustness to adversarial examples”. In: International Conference
on Machine Learning. PMLR. 2017, pp. 854–863.
[8]
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. “Certified adversarial robustness via random-
ized smoothing”. In: international conference on machine learning. PMLR. 2019, pp. 1310–
1320.
[9]
Aounon Kumar, Alexander Levine, Soheil Feizi, and Tom Goldstein. “Certifying confidence
via randomized smoothing”. In: Advances in Neural Information Processing Systems 33 (2020),
pp. 5165–5177.
[10]
Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck,
and Greg Yang. “Provably robust deep learning via adversarially trained smoothed classifiers”.
In: Advances in Neural Information Processing Systems 32 (2019).
[11]
Eric Wong and Zico Kolter. “Provable defenses against adversarial examples via the convex
outer adversarial polytope”. In: International Conference on Machine Learning. PMLR. 2018,
pp. 5286–5295.
[12]
Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan
Uesato, Rudy R Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy S Liang, et al.
“Enabling certification of verification-agnostic networks via memory-efficient semidefinite
programming”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 5318–
5331.
[13]
Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. “Defense
against adversarial attacks using high-level representation guided denoiser”. In: Proceedings
of the IEEE conference on computer vision and pattern recognition. 2018, pp. 1778–1787.
[14]
Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and
Patrick McDaniel. “Ensemble adversarial training: Attacks and defenses”. In: arXiv preprint
arXiv:1705.07204 (2017).
[15]
Nikolaos Tsilivis, Jingtong Su, and Julia Kempe. “Can we achieve robustness from data alone?”
In: arXiv preprint arXiv:2207.11727 (2022).
[16]
Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. “Mma training:
Direct input space margin maximization through adversarial training”. In: arXiv preprint
arXiv:1812.02637 (2018).
[17]
Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan
Kankanhalli. “Geometry-aware instance-reweighted adversarial training”. In: arXiv preprint
arXiv:2010.01736 (2020).
[18]
Yuancheng Xu, Yanchao Sun, Micah Goldblum, Tom Goldstein, and Furong Huang. “Exploring
and Exploiting Decision Boundary Dynamics for Adversarial Robustness”. In: arXiv preprint
arXiv:2302.03015 (2023).
[19]
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. “Lipschitz-margin training: Scalable
certification of perturbation invariance for deep neural networks”. In: Advances in neural
information processing systems 31 (2018)."
REFERENCES,0.9821428571428571,"[20]
Yujia Huang, Huan Zhang, Yuanyuan Shi, J Zico Kolter, and Anima Anandkumar. “Training
certifiably robust neural networks with efficient local lipschitz bounds”. In: Advances in Neural
Information Processing Systems 34 (2021), pp. 22745–22757.
[21]
Bohang Zhang, Tianle Cai, Zhou Lu, Di He, and Liwei Wang. “Towards certifying l-infinity
robustness using neural networks with l-inf-dist neurons”. In: International Conference on
Machine Learning. PMLR. 2021, pp. 12368–12379.
[22]
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. “On the effectiveness
of interval bound propagation for training verifiably robust models”. In: arXiv preprint
arXiv:1810.12715 (2018).
[23]
Sungyoon Lee, Jaewook Lee, and Saerom Park. “Lipschitz-certifiable training with a tight outer
bound”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 16891–16902.
[24]
Judy Hoffman, Daniel A Roberts, and Sho Yaida. “Robust learning with jacobian regulariza-
tion”. In: arXiv preprint arXiv:1908.02729 (2019).
[25]
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. “Regularisation of neural
networks by enforcing lipschitz continuity”. In: Machine Learning 110 (2021), pp. 393–416.
[26]
Christina Baek, Yiding Jiang, Aditi Raghunathan, and J Zico Kolter. “Agreement-on-the-line:
Predicting the performance of neural networks under distribution shift”. In: Advances in Neural
Information Processing Systems 35 (2022), pp. 19274–19289.
[27]
Zhouxing Shi, Yihan Wang, Huan Zhang, J Zico Kolter, and Cho-Jui Hsieh. “Efficiently
computing local Lipschitz constants of neural networks via bound propagation”. In: Advances
in Neural Information Processing Systems 35 (2022), pp. 2350–2364.
[28]
Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. “An abstract domain for
certifying neural networks”. In: Proceedings of the ACM on Programming Languages 3.POPL
(2019), pp. 1–30.
[29]
Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter.
“Beta-crown: Efficient bound propagation with per-neuron split constraints for neural network
robustness verification”. In: Advances in Neural Information Processing Systems 34 (2021),
pp. 29909–29921.
[30]
Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas.
“Efficient and accurate estimation of lipschitz constants for deep neural networks”. In: Advances
in Neural Information Processing Systems 32 (2019).
[31]
Navid Hashemi, Justin Ruths, and Mahyar Fazlyab. “Certifying incremental quadratic con-
straints for neural networks via convex optimization”. In: Learning for Dynamics and Control.
PMLR. 2021, pp. 842–853.
[32]
Patricia Pauli, Anne Koch, Julian Berberich, Paul Kohler, and Frank Allgöwer. “Training
robust neural networks using Lipschitz bounds”. In: IEEE Control Systems Letters 6 (2021),
pp. 121–126.
[33]
Sahil Singla and Soheil Feizi. “Skew orthogonal convolutions”. In: International Conference
on Machine Learning. PMLR. 2021, pp. 9756–9766.
[34]
Sahil Singla, Surbhi Singla, and Soheil Feizi. “Improved deterministic l2 robustness on CIFAR-
10 and CIFAR-100”. In: arXiv preprint arXiv:2108.04062 (2021).
[35]
Cem Anil, James Lucas, and Roger Grosse. “Sorting out Lipschitz function approximation”.
In: International Conference on Machine Learning. PMLR. 2019, pp. 291–301.
[36]
Bernd Prach and Christoph H Lampert. “Almost-orthogonal layers for efficient general-purpose
Lipschitz networks”. In: Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv,
Israel, October 23–27, 2022, Proceedings, Part XXI. Springer. 2022, pp. 350–365.
[37]
Asher Trockman and J Zico Kolter. “Orthogonalizing convolutional layers with the cayley
transform”. In: arXiv preprint arXiv:2104.07167 (2021).
[38]
Alexandre Araujo, Aaron Havens, Blaise Delattre, Alexandre Allauzen, and Bin Hu. “A unified
algebraic perspective on lipschitz neural networks”. In: arXiv preprint arXiv:2303.03169
(2023).
[39]
Ruigang Wang and Ian Manchester. “Direct parameterization of lipschitz-bounded deep
networks”. In: International Conference on Machine Learning. PMLR. 2023, pp. 36093–
36110."
REFERENCES,0.9880952380952381,"[40]
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press,
2004.
[41]
Francesco Croce and Matthias Hein. “Minimally distorted adversarial examples with a fast
adaptive boundary attack”. In: International Conference on Machine Learning. PMLR. 2020,
pp. 2196–2205.
[42]
Mahyar Fazlyab, Manfred Morari, and George J Pappas. “Safety verification and robustness
analysis of neural networks via quadratic constraints and semidefinite programming”. In: IEEE
Transactions on Automatic Control 67.1 (2020), pp. 1–15.
[43]
Anton Xue, Lars Lindemann, Alexander Robey, Hamed Hassani, George J Pappas, and Rajeev
Alur. “Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks”. In:
2022 IEEE 61st Conference on Decision and Control (CDC). IEEE. 2022, pp. 3389–3396.
[44]
Stephen Boyd, Laurent El Ghaoui, Eric Feron, and Venkataramanan Balakrishnan. Linear
matrix inequalities in system and control theory. Vol. 15. Siam, 1994.
[45]
Charles A Desoer and Mathukumalli Vidyasagar. Feedback systems: input-output properties.
SIAM, 2009.
[46]
Yann LeCun. “The MNIST database of handwritten digits”. In: http://yann. lecun.
com/exdb/mnist/ (1998).
[47]
Alex Krizhevsky, Geoffrey Hinton, et al. “Learning multiple layers of features from tiny
images”. In: (2009).
[48]
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. “Imagenet: A large-
scale hierarchical image database”. In: 2009 IEEE conference on computer vision and pattern
recognition. Ieee. 2009, pp. 248–255.
[49]
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning,
Inderjit S Dhillon, and Luca Daniel. “Towards Fast Computation of Certified Robustness for
ReLU Networks”. In: arXiv preprint arXiv:1804.09699 (2018).
[50]
Trevor Avant and Kristi A Morgansen. “Analytical bounds on the local Lipschitz constants of
affine-ReLU functions”. In: arXiv preprint arXiv:2008.06141 (2020).
[51]
Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh,
and Luca Daniel. “Evaluating the robustness of neural networks: An extreme value theory
approach”. In: arXiv preprint arXiv:1801.10578 (2018).
[52]
Aladin Virmaux and Kevin Scaman. “Lipschitz regularity of deep neural networks: analysis
and efficient estimation”. In: Advances in Neural Information Processing Systems. 2018,
pp. 3835–3844.
[53]
Fabian Latorre, Paul Rolland, and Volkan Cevher. “Lipschitz constant estimation of Neural
Networks via sparse polynomial optimization”. In: International Conference on Learning
Representations. 2020.
[54]
Tong Chen, Jean B Lasserre, Victor Magron, and Edouard Pauwels. “Semialgebraic optimiza-
tion for lipschitz constants of relu networks”. In: Advances in Neural Information Processing
Systems 33 (2020), pp. 19189–19200.
[55]
Matt Jordan and Alexandros G Dimakis. “Exactly computing the local lipschitz constant of relu
networks”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 7344–7353.
[56]
Patrick L Combettes and Jean-Christophe Pesquet. “Lipschitz certificates for layered network
structures driven by averaged activation operators”. In: SIAM Journal on Mathematics of Data
Science 2.2 (2020), pp. 529–557.
[57]
Gal Chechik, Geremy Heitz, Gal Elidan, Pieter Abbeel, and Daphne Koller. “Max-margin
Classification of Data with Absent Features.” In: Journal of Machine Learning Research 9.1
(2008).
[58]
Johan AK Suykens and Joos Vandewalle. “Least squares support vector machine classifiers”.
In: Neural processing letters 9 (1999), pp. 293–300.
[59]
Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio.
“Large margin deep networks for classification”. In: Advances in neural information processing
systems 31 (2018).
[60]
Ziang Yan, Yiwen Guo, and Changshui Zhang. “Deep defense: Training dnns with improved
adversarial robustness”. In: Advances in Neural Information Processing Systems 31 (2018).
[61]
Yiwen Guo and Changshui Zhang. “Recent advances in large margin learning”. In: IEEE
Transactions on Pattern Analysis and Machine Intelligence (2021)."
REFERENCES,0.9940476190476191,"[62]
Feng Liu, Bo Han, Tongliang Liu, Chen Gong, Gang Niu, Mingyuan Zhou, Masashi Sugiyama,
et al. “Probabilistic margins for instance reweighting in adversarial training”. In: Advances in
Neural Information Processing Systems 34 (2021), pp. 23258–23269.
[63]
Matthew Mirman, Timon Gehr, and Martin Vechev. “Differentiable abstract interpretation for
provably robust neural networks”. In: International Conference on Machine Learning. 2018,
pp. 3575–3583.
[64]
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. “Efficient neural
network robustness certification with general activation functions”. In: Advances in neural
information processing systems. 2018, pp. 4939–4948.
[65]
Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. “Semidefinite relaxations for cer-
tifying robustness to adversarial examples”. In: Advances in Neural Information Processing
Systems. 2018, pp. 10900–10910.
[66]
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. “Certified defenses against adversarial
examples”. In: arXiv preprint arXiv:1801.09344 (2018).
[67]
Yuhao Mao, Mark Niklas Müller, Marc Fischer, and Martin Vechev. “TAPS: Connecting
Certified and Adversarial Training”. In: arXiv preprint arXiv:2305.04574 (2023).
[68]
Diederik P Kingma and Jimmy Ba. “Adam: A method for stochastic optimization”. In: arXiv
preprint arXiv:1412.6980 (2014)."
