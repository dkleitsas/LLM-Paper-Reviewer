Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00117096018735363,"We consider the problem of multi-task reasoning (MTR), where an agent can
solve multiple tasks via (ﬁrst-order) logic reasoning. This capability is essential
for human-like intelligence due to its strong generalizability and simplicity for
handling multiple tasks. However, a major challenge in developing effective
MTR is the intrinsic conﬂict between reasoning capability and efﬁciency. An
MTR-capable agent must master a large set of “skills” to tackle diverse tasks,
but executing a particular task at the inference stage requires only a small subset
of immediately relevant skills. How can we maintain broad reasoning capability
but efﬁcient speciﬁc-task performance? To address this problem, we propose a
Planner-Reasoner framework capable of state-of-the-art MTR capability and high
efﬁciency. The Reasoner models shareable (ﬁrst-order) logic deduction rules, from
which the Planner selects a subset to compose into efﬁcient reasoning paths. The
entire model is trained in an end-to-end manner using deep reinforcement learning,
and experimental studies over a variety of domains validate its effectiveness. 1"
INTRODUCTION,0.00234192037470726,"1
INTRODUCTION"
INTRODUCTION,0.00351288056206089,"Multi-task learning (MTL) (Zhang & Yang, 2021; Zhou et al., 2011) demonstrates superior sample
complexity and generalizability compared with the conventional “one model per task” style to solve
multiple tasks. Recent research has additionally leveraged the great success of deep learning (LeCun
et al., 2015) to empower learning deep multi-task models (Zhang & Yang, 2021; Crawshaw, 2020).
Deep MTL models either learn a common multi-task feature representation by sharing several bottom
layers of deep neural networks (Zhang et al., 2014; Liu et al., 2015; Zhang et al., 2015a; Mrksic
et al., 2015; Li et al., 2015), or learn task-invariant and task-speciﬁc neural modules (Shinohara,
2016; Liu et al., 2017) via generative adversarial networks (Goodfellow et al., 2014). Although
MTL is successful in many applications, a major challenge is the often impractically large MTL
models. Although still smaller than piling up all models across different tasks, existing MTL
models are signiﬁcantly larger than a single model for tackling a speciﬁc task. This results from the
intrinsic conﬂict underlying all MTL algorithms: balancing across-task generalization capability to
perform different tasks with single-task efﬁciency in executing a speciﬁc task. On one hand, good
generalization ability requires an MTL agent to be equipped with a large set of skills that can be
combined to solve many different tasks. On the other hand, solving one particular task does not
require all these skills. Instead, the agent needs to compose only a (small) subset of these skills into
an efﬁcient solution for a speciﬁc task. This conﬂict often hobbles existing MTL approaches."
INTRODUCTION,0.00468384074941452,"This paper focuses on multi-task reasoning (MTR), a subarea of MTL that uses logic reasoning to
solve multiple tasks. MTR is ubiquitous in human reasoning, where humans construct different
reasoning paths for multiple tasks from the same set of reasoning skills. Conventional deep learning,
although capable of strong expressive power, falls short in reasoning capabilities (Bengio, 2019).
Considerable research has been devoted to endowing deep learning with logic reasoning abilities, the
results of which include Deep Neural Reasoning (Jaeger, 2016), Neural Logic Reasoning (Besold
et al., 2017; Bader et al., 2004; Bader & Hitzler, 2005), Neural Logic Machines (Dong et al., 2019),
and other approaches (Besold et al., 2017; Bader et al., 2004; Bader & Hitzler, 2005). However,
these approaches consider only single-task reasoning rather than a multi-task setting, and applying"
INTRODUCTION,0.00585480093676815,1The code will be released after acceptance.
INTRODUCTION,0.00702576112412178,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.00819672131147541,Figure 1: An example from the AdjacentToRed task and its formulation as a logical reasoning problem.
INTRODUCTION,0.00936768149882904,"existing MTL approaches to learning these neural reasoning models leads to the same conﬂict between
across-task generalization capability and single-task efﬁciency."
INTRODUCTION,0.01053864168618267,"To strike a balance between reasoning capability and efﬁciency in MTR, we develop a Planner-
Reasoner architecture Inside a Multi-task reasoning Agent (PRIMA) (Section 2), wherein the
reasoner deﬁnes a set of neural logic operators for modeling reusable reasoning meta-rules (“skills”)
across tasks (Section 2.2). When deﬁning the logic operators, we focus on ﬁrst-order logic because
of its simplicity and wide applicability to many reasoning problems, such as automated theorem
proving (Fitting, 2012; Gallier, 2015) and knowledge-based systems (Van Harmelen et al., 2008). A
separate planner module activates only a small subset of the meta-rules necessary for a given task
and composes them into a deduction process (Section 2.3). Thus, our planner-reasoner architecture
features the dual capabilities of composing and pruning a logic deduction process, achieving a
graceful capability-efﬁciency trade-off in MTR (Section 2.4). The model architecture is trained in an
end-to-end manner using deep reinforcement learning (Section 3), and experimental results on several
benchmarks demonstrate that this framework leads to a more principled predicate space search and
reduces reasoning complexity (Section 4). We discuss related works in Section 5, and conclude our
paper in Section 6."
PLANNER-REASONER FOR MULTI-TASK REASONING,0.0117096018735363,"2
PLANNER-REASONER FOR MULTI-TASK REASONING"
PLANNER-REASONER FOR MULTI-TASK REASONING,0.01288056206088993,"This section proposes the Planner-Reasoner framework for MTR. To that end, we ﬁrst formally
state the logic reasoning problem in Section 2.1, and then in Sections 2.2 and 2.3, we describe
the Planner-Reasoner Inside a Single Reasoning Agent (PRISA) framework. This is a planner-
reasoner architecture, which is a neural-logic architecture for traversing the ﬁrst-order predicate space.
Typically, logical reasoning can be reduced to learning to search for a reasoning path with logical
operators and then derive a logical consequence from premises. Therefore, a reasoning problem
can be addressed in two steps: (i) constructing the elementary logical operators and (ii) selecting
a reasoning path that chains these logical operators together. This key observation motivates the
Reasoner (Section 2.2) and Planner (Section 2.3) modules in our framework. In Section 2.4, the
PRISA framework is extended to the MTR setting, which results in the Planner-Reasoner Inside a
Multi-task Reasoning Agent (PRIMA) framework."
PROBLEM FORMULATION,0.01405152224824356,"2.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.01522248243559719,"Logic reasoning
We begin with a brief introduction of the logic reasoning problem. Speciﬁcally,
we consider a special variant of the First-Order Logic (FOL) system, which only consists of individual
variables, constants and up to r-ary predicate variables. That is, we do not consider functions that
map individual variables/constants into terms. An r-ary predicate p(x1, . . . , xr) can be considered as
a relation between r constants, which takes the value of True or False. An atom p(x1, · · · , xr) is
an r-ary predicate with its arguments x1, · · · , xr being either variables or constants. A well-deﬁned
formula in our FOL system is a logical expression that is composed from atoms, logical connectives
(e.g., negation ¬, conjunction ^, disjunction _, implication  ), and possibly existential 9 and
universal 8 quantiﬁers according to certain formation rules (see Andrews (2002) for the details). In
particular, the quantiﬁers 9 and 8 are only allowed to be applied to individual variables in FOL. In
Fig. 1, we give an example from the AdjacentToRed task (Graves et al., 2016) and show how
it could be formulated as a logical reasoning problem. Speciﬁcally, we are given a random graph
along with the properties (i.e., the color) of the nodes and the relations (i.e., connectivity) between
nodes. In our context, each node i in the graph is a constant and an individual variable x takes"
PROBLEM FORMULATION,0.01639344262295082,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.01756440281030445,"values in the set of constants {1, . . . , 5}. The properties of nodes and the relations between nodes
are modeled as the unary predicate IsRed(x) (5 ⇥1 vector) and the binary predicate HasEdge(x, y)
(5 ⇥5 matrix), respectively. The objective of logical reasoning is to deduce the value of the unary
predicate AdjacentToRed(x) (i.e., whether a node x has a neighbor of red color) from the base
predicates IsRed(x) and HasEdge(x, y) (see Fig. 1 for an example of the deduction process)."
PROBLEM FORMULATION,0.01873536299765808,"Multi-Task Reasoning
Next, we introduce the deﬁnition of MTR. With a slight abuse of notations,
let {p(x1, . . . , xr) : r 2 [1, n]} be the set of input predicates sampled from any of the k different
reasoning tasks, where x1, . . . , xr are the individual variables and n is the maximum arity. A
multi-task reasoning model takes p(x1, . . . , xr) as its input and seeks to predict the corresponding
ground-truth output predicates q(x1, . . . , xr). The aim is to learn multiple reasoning tasks jointly in
a single model so that the reasoning skills in a task can be leveraged by other tasks to improve the
general performance of all tasks at hand."
PROBLEM FORMULATION,0.01990632318501171,"2.2
REASONER: TRANSFORMING LOGIC RULES INTO NEURAL OPERATORS"
PROBLEM FORMULATION,0.02107728337236534,"The Reasoner module conducts logical deduction using a set of neural operators constructed from
ﬁrst-order logic rules (more speciﬁcally, a set of “learnable” Horn clauses). Its architecture is inspired
by NLM (Dong et al., 2019) (details about the difference can be found in Appendix D). Three logic
rules are considered as essential meta-rules: BooleanLogic, Expansion, and Reduction."
PROBLEM FORMULATION,0.02224824355971897,"BooleanLogic :
expression(x1, x2, · · · , xr) ! ˆp(x1, x2, · · · , xr),"
PROBLEM FORMULATION,0.0234192037470726,"where expression is composed of a combination of Boolean operations (AND, OR, and NOT)
and ˆp is the output predicate. For a given r-ary predicate and a given permutation  2 Sn, we
deﬁne p (x1, · · · , xr) = p(x (1), · · · , x (r)) where Sn is the set of all possible permutations as
the arguments to an input predicate. The corresponding neural implementation of BooleanLogic is
σ (MLP (p (x1, · · · , xr)) ; ✓), where σ is the sigmoid activation function, MLP refers to a multi-
layer perceptron, a Permute(·) neural operator transforms input predicates to p (x1, · · · , xr), and
✓is the learnable parameter within the model. This is similar to the implicit Horn clause with the
universal quantiﬁer(8), e.g., p1(x) ^ p2(x) ! ˆp(x) implicitly denoting 8x p1(x) ^ p2(x) ! ˆp(x).
The class of neural operators can be viewed as “learnable” Horn clauses."
PROBLEM FORMULATION,0.02459016393442623,"Expansion, and Reduction are two types of meta-rules for quantiﬁcation that bridge predicates of
different arities with logic quantiﬁers (8 and 9). Expansion introduces a new and distinct variable
xr+1 for a set of r-ary predicates with the universal quantiﬁer(8). For this reason, Expansion creates
a new predicate q from p."
PROBLEM FORMULATION,0.02576112412177986,"Expansion :
p(x1, x2, · · · , xr) ! 8xr+1, q(x1, x2, · · · , xr, xr+1),"
PROBLEM FORMULATION,0.026932084309133488,where xr+1 /2 {xi}r
PROBLEM FORMULATION,0.02810304449648712,"i=1. The corresponding neural implementation of Expansion, denoted by
Expand(·), expands the r-ary predicates into the (r + 1)-ary predicates by repeating the r-ary
predicates and stacking them in a new dimension. Conversely, Reduction removes the variable xr+1
in a set of (r + 1)-ary predicates via the quantiﬁers of 8 or 9."
PROBLEM FORMULATION,0.02927400468384075,"Reduction :
8xr+1 p(x1, x2, · · · , xr, xr+1) ! q(x1, x2, · · · , xr), or
9xr+1 p(x1, x2, · · · , xr, xr+1) ! q(x1, x2, · · · , xr)."
PROBLEM FORMULATION,0.03044496487119438,"The corresponding neural implementation of Reduction, denoted by Reduce(·), reducing the (r+1)-
ary predicates into the r-ary predicates by taking the minimum (resp. maximum) along the dimension
of xr+1 due to the universal quantiﬁer 8 (resp. existential quantiﬁer 9)."
PROBLEM FORMULATION,0.03161592505854801,"2.3
PLANNER: ACTIVATING AND FORWARD-CHAINING LEARNABLE HORN CLAUSES"
PROBLEM FORMULATION,0.03278688524590164,"The Planner is our key module to address the capability-efﬁciency tradeoff in the MTR problem; it
is responsible for activating the neural operators in the Reasoner and chaining them into reasoning
paths. Existing learning-to-reason approaches, which are often based on inductive logic programming
(ILP) (Cropper et al., 2020; Cropper & Dumancic, 2020; Muggleton & De Raedt, 1994) and the
correspondingly neural-ILP methods (Dong et al., 2019; Shi et al., 2020). Conventional ILP methods
suffer from several drawbacks, such as heavy reliance on human-crafted templates and sensitivity
to noise. On the other hand, neural-ILP methods (Dong et al., 2019; Shi et al., 2020), leveraging"
PROBLEM FORMULATION,0.03395784543325527,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.0351288056206089,"Figure 2: Left (A): The Planner-Reasoner Architecture with an example solution. Right (B): The overall
(end-to-end) training process of the model by deep reinforcement learning. “Perm” denotes Permute operator."
PROBLEM FORMULATION,0.03629976580796253,"the strength of deep learning and ILP, such as the Neural Logic Machine (NLM) (Dong et al.,
2019), lack explicitness in the learning process of searching for reasoning paths. Let us take the
learning process of NLM for example, which follows an intuitive two-step procedure. It ﬁrst fully
connects all the neural blocks and then searches all possible connections (corresponding to all possible
predicate candidates) exhaustively to identify the desired reasoning path (corresponding to the desired
predicate)."
PROBLEM FORMULATION,0.03747072599531616,"By using our proposed Planner module, we can strike a better capability-efﬁciency tradeoff. Rather
than conducting an exhaustive search over all possible predicate candidates as in NLM, the Planner
prunes all the unnecessary operators and identiﬁes an essential reasoning path with low complexity
for a given problem. Consider the following example. As shown in Fig. 1 and Fig. 2A (the
highlighted orange-colored blocks), at each reasoning step, the Planner takes the input predicates
and determines which neural operators should be activated. The decision is represented as a binary
vector — [I0 . . . I6] (termed operator footprint) — that corresponds to the neural operators of
Expand, Reduce and DirectInput2 at different arity groups. By chaining these sequential decisions,
a sequence of operator footprints is formulated as a reasoning path, as the highlighted orange-colored
path in Fig. 2A. Generally, the neural operators deﬁned in the Reasoner can also be viewed as
(learnable) Horn Clauses (Horn, 1951; Chandra & Harel, 1985), and the Planner forward-chains them
into a reasoning path."
THE PRIMA FRAMEWORK FOR MULTI-TASK REASONING,0.03864168618266979,"2.4
THE PRIMA FRAMEWORK FOR MULTI-TASK REASONING"
THE PRIMA FRAMEWORK FOR MULTI-TASK REASONING,0.03981264637002342,"By far, we have developed the PRISA architecture for the single-task reasoning setting. To extend
PRISA to PRIMA for the multi-task reasoning setting, we add an extra nullary input predicate (O0 in
Fig. 2) to inform the planner of the current task. Based on the value of O0, the planner will learn to
activate the appropriate subset of neural operators, including a subset of shared operators and a few
task-speciﬁc operators, for the current task. By doing so, PRIMA can learn to make the best use of
reusable neural operators across different tasks. In Section 4, we will demonstrate such a capability
of the planner on different tasks."
LEARNING-TO-REASON VIA REINFORCEMENT LEARNING,0.040983606557377046,"3
LEARNING-TO-REASON VIA REINFORCEMENT LEARNING"
LEARNING-TO-REASON VIA REINFORCEMENT LEARNING,0.04215456674473068,"In our problem, the decision variables of the Planner are binary indicators of whether a neural operator
module should be activated. Readers familiar with Markov decision processes (MDPs) (Puterman,
2014) might notice that the reasoning path of our model in Fig. 2A resembles a temporal rollout in
MDP formulation (Sutton & Barto, 2018). Therefore, we frame learning-to-reason as a sequential
decision-making problem and adopt off-the-shelf reinforcement learning (RL) algorithms."
LEARNING-TO-REASON VIA REINFORCEMENT LEARNING,0.04332552693208431,"2DirectInput is an identity mapping of the inputs where its following operators of Permute, MLP and
sigmoid nonlinearity can directly approximate the BooleanLogic."
LEARNING-TO-REASON VIA REINFORCEMENT LEARNING,0.04449648711943794,Under review as a conference paper at ICLR 2022
LEARNING-TO-REASON VIA REINFORCEMENT LEARNING,0.04566744730679157,"Figure 3: The architecture of the Base Planner and its enhanced version. Left: The Base Planner is based on a
one-layer fully-activated Reasoner followed by max-pooling operations to predict an indicator of whether an op
should be activated. Right: The Enhanced Planner uses MCTS to further boost the performance."
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.0468384074941452,"3.1
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.04800936768149883,"An MDP is deﬁned as the tuple (S, A, P a"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.04918032786885246,"ss0, R, γ), where S and A are ﬁnite sets of states and actions,
the transition kernel P a"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.05035128805620609,"ss0 speciﬁes the probability of transition from state s 2 S to state s0 2 S by
taking action a 2 A, R(s, a) : S ⇥A ! R is the reward function, and 0 γ 1 is a discount factor.
A stationary policy ⇡: S ⇥A ! [0, 1] is a probabilistic mapping from states to actions. The primary
objective of an RL algorithm is to identify a near-optimal policy ⇡⇤that satisﬁes"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.05152224824355972,⇡⇤:= arg max ⇡ ⇢
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.05269320843091335,J(⇡) := E
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.053864168618266976,"Tmax−1
X t=0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.05503512880562061,"γtr(st, at ⇠⇡) $% ,"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.05620608899297424,"where Tmax is a positive integer denoting the horizon length — that is, the maximum length of a
rollout. The resemblance between Fig. 2 and an MDP formulation is relatively intuitive as summarized
in Table 1. At the t-th time step, the state st corresponds to the set of predicates, st = [O0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.05737704918032787,"t , O1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.0585480093676815,"t , O2"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.059718969555035126,"t ],
with the superscript denoting the corresponding arity. The action at = [I0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.06088992974238876,"t , I1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.06206088992974239,"t , . . . , IK−1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.06323185011709602,"t
] (e.g.,
a0 = [000011]) is a binary vector that indicates the activated neural operators (i.e., the operator
footprint), where K is the number of operators per layer. The reward rt is deﬁned to be rt := ( −PK−1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.06440281030444965,i=0 Ii
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.06557377049180328,"t,
if t < Tmax
Accuracy,
t = Tmax
.
(1)"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.06674473067915691,"That is, the terminal reward is set to be the reasoning accuracy at the end of the reasoning path (see
Appendix A.2 for its deﬁnition), and the intermediate reward at each step is chosen to be the negated
number of activated operators (which penalizes the cost of performing the current reasoning step).
The transition kernel P a"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.06791569086651054,"ss0 corresponds to the function modeled by one Reasoner layer; each Reasoner
layer will take the state (predicates) st−1 = [O0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.06908665105386416,"t−1, O1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.0702576112412178,"t−1, O2"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.07142857142857142,"t−1, ] and the action (operator footprint)
at = [I0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.07259953161592506,"t , I1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.07377049180327869,"t , . . . , IK−1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.07494145199063232,"t
] as its input and then generate the next state (predicates) st = [O0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.07611241217798595,"t , O1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.07728337236533958,"t , O2"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.07845433255269321,"t , ].
This also implies that the Reasoner layer deﬁnes a deterministic transition kernel, i.e., given st−1 and
at the next state st is determined."
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.07962529274004684,Table 1: The identiﬁcation between the concepts of PRIMA/PRISA and that of RL at the t-th time step.
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.08079625292740047,"RL
State st
Action at
Reward rt
Transition
Kernel P a"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.08196721311475409,"ss0
Policy
Rollout"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.08313817330210772,"PRIMA/PRISA
Predicates of different arities: [O0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.08430913348946135,"t , O1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.08548009367681499,"t , O2 t ]t"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.08665105386416862,Operator footprint: [I0
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.08782201405152225,t . . . IK−1
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.08899297423887588,"t
]
Eq. (1)
One layer of"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.09016393442622951,"Reasoner
Planner
Reasoning path"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.09133489461358314,"3.2
POLICY NETWORK: MODELING OF PLANNER"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.09250585480093677,"The Planner module is embodied in the policy network. As shown in Fig. 3A, the base planner
is a separate module that has the same architecture of 1-layer (fully-activated) Reasoner followed"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.0936768149882904,Under review as a conference paper at ICLR 2022
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.09484777517564402,"by a max-pooling layer. This architecture enables the reduction of input predicates to the speciﬁc
indicators by reﬂecting whether the operations at the corresponding position of one layer of Reasoner
are active or inactive. Further, we can also leverage Monte-Carlo Tree Search (MCTS) (Browne
et al., 2012; Munos, 2014) to boost the performance, which leads to an Enhanced Planner (Fig. 3B).
An MCTS algorithm such as the Upper Conﬁdence Bound for Trees (UCT) method (Kocsis et al.,
2006), is a model-based RL algorithm that plans the best action at each time step (Browne et al.,
2012) by constructing a search tree, with states as nodes and actions as edges. The Enhanced Planner
uses MCTS to exploit partial knowledge of the problem structure (i.e., the deterministic transition
kernel P a"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.09601873536299765,"ss0 deﬁned by the Reasoner layer) and construct a search tree to help identify the best actions
(which ops to activate). Details of the MCTS algorithms used in the Enhanced Planner can be found
in Appendix A.1."
OVERALL LEARNING FRAMEWORK,0.09718969555035128,"3.3
OVERALL LEARNING FRAMEWORK"
OVERALL LEARNING FRAMEWORK,0.09836065573770492,"As illustrated in Fig. 3, we introduce concrete data-driven decision-making methods—that is, RL
approaches (Sutton & Barto, 2018)—to address the learning-to-reason problem. To illustrate this,
we apply the model-free RL method REINFORCE (Williams, 1992) and the model-based method
MuZero (Schrittwieser et al., 2019). Compared with model-free reinforcement learning, model-based
reinforcement learning (MBRL) more effectively handles large search-space problems, such as the
game of Go (Silver et al., 2017b;b;a). MuZero (Schrittwieser et al., 2019), a recently proposed MBRL
approach to integrating planning and learning, has achieved great success with a variety of complex
tasks. Motivated by the success of MuZero, we propose an MBRL approach for neural-symbolic
reasoning. The key insight behind adopting MuZero is that in real applications, we typically have
partial structural knowledge of the transition kernel P a"
OVERALL LEARNING FRAMEWORK,0.09953161592505855,"ss0 and reward function r(s, a). As a result
of the model-based module, testing complexity can be greatly reduced by adopting MCTS, which
leads to a better set of predicates. Of course, MuZero is just one option in the model-based family of
approaches. We leave it as future research to propose and compare other model-based alternatives."
OVERALL LEARNING FRAMEWORK,0.10070257611241218,"The pipeline of the training process is illustrated in Fig.2.B (the right subﬁgure). After loading the
model weights, the reasoning path rollouts are executed by the agent (or model instance), according
to the current policy network. The performed reasoning path rollout is then stored in the replay buffer.
The Planner-Reasoner is trained via rollouts sampled from the replay buffer."
EXPERIMENTAL RESULTS AND ANALYSIS,0.10187353629976581,"4
EXPERIMENTAL RESULTS AND ANALYSIS"
EXPERIMENTAL RESULTS AND ANALYSIS,0.10304449648711944,"In this section, we evaluate the performance of different variants of PRISA (for single-task rea-
soning) and PRIMA (for multi-task reasoning) on eight tasks from the family tree and graph
benchmarks (Graves et al., 2016), including 1-Outdegree, AdjacentToRed, HasFather,
HasSister, 4-Connectivity, IsGrandparent, IsUncle, IsMGUncle.
These
tasks are widely used benchmark domains for inductive logic programming (Krötzsch, 2020; Calautti
et al., 2015). Detailed descriptions about those tasks can be found in Appendix B.1. We evaluate their
testing accuracy and reasoning cost (measured in FLOPs: the number of ﬂoating-point operations
executed (Clark et al., 2020)) on these tasks and compare them to several baselines. Besides, the
case study is conducted on the reasoning path, which indicates the operator sharing among different
tasks. All the results demonstrate the graceful capability-efﬁciency tradeoff of PRIMA in multi-task
reasoning."
EXPERIMENTAL SETUPS,0.10421545667447307,"4.1
EXPERIMENTAL SETUPS"
EXPERIMENTAL SETUPS,0.1053864168618267,"Regarding the data generated in the single-task setting, we use the same methods as in NLM (Dong
et al., 2019). A task will be randomly sampled in the multi-task setting according to a pre-deﬁned
probability distribution on different tasks. Compared to the generated data with that of the single
task, it is augmented by using one-hot encoding for different tasks and wrapping it into nullary
(background) predicates. Also, task-speciﬁc output heads are introduced to adapt to the multi-task
setting. These adaptions apply to NLM-MTR, DLM-MTR, and PRIMA. The reasoning accuracy
is used as the reward for both PRISA-MuZero and PRIMA. In the inference (or testing) stage, the
Reasoner is combined with the learned Base Planner to perform the tasks for PRISA-MuZero and
PRIMA, instead of an enhanced planner (MCTS), to reduce the extra computation. The problem"
EXPERIMENTAL SETUPS,0.10655737704918032,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETUPS,0.10772833723653395,"Table 2: Testing Accuracy and PSS of different variants of PRISA on different tasks. PRISA-MuZero achieves
the best performance on single-task reasoning, which conﬁrms the strength of the MCTS-based Enhanced Planner
and the MuZero learning strategy. “m”: the problem size. “PSS”: Percentage of Successful Seeds."
EXPERIMENTAL SETUPS,0.10889929742388758,testing acc
EXPERIMENTAL SETUPS,0.11007025761124122,Family
EXPERIMENTAL SETUPS,0.11124121779859485,"Tree
HasFather
HasSister
IsGrandparent
IsUncle
IsMGUncle
Graph
AdjacentToRed
4-Connectivity
1-OutDegree"
EXPERIMENTAL SETUPS,0.11241217798594848,Single Task
EXPERIMENTAL SETUPS,0.11358313817330211,"PRISA-
REINFORCE"
EXPERIMENTAL SETUPS,0.11475409836065574,"m=20
62.6
50.7
96.5
97.3
99.8
m=10
47.7
33.5
48.7
m=100
87.8
69.8
2.3
97.7
98.4
m=50
71.6
92.8
97.4
PSS
0
0
0
0
0
PSS
0
0
0"
EXPERIMENTAL SETUPS,0.11592505854800937,PRISA-PPO
EXPERIMENTAL SETUPS,0.117096018735363,"m=20
71.5
64.3
97.5
98.1
99.6
m=10
62.3
57.8
61.6
m=100
93.2
78.7
98.2
97.3
99.1
m=50
85.5
95.2
96.3
PSS
0
0
0
0
0
PSS
0
0
0"
EXPERIMENTAL SETUPS,0.11826697892271663,PRISA-MuZero
EXPERIMENTAL SETUPS,0.11943793911007025,"m=20
100
100
100
100
100
m=10
100
100
100
m=100
100
100
100
100
100
m=50
100
100
100
PSS
100
100
100
100
100
PSS
90
100
100"
EXPERIMENTAL SETUPS,0.12060889929742388,"size for training is always 10 for graph tasks and 20 for family tree tasks across all single-task and
multi-task settings, regardless of the sizes of testing problems."
OVERALL PERFORMANCE,0.12177985948477751,"4.2
OVERALL PERFORMANCE"
OVERALL PERFORMANCE,0.12295081967213115,"Single-task reasoning capability
First, we compare the performance of three different variants of
PRISA (Section 3.3) for single-task reasoning, which learns their planners based on different reinforce-
ment learning algorithms; PRISA-REINFORCE uses REINFORCE (Williams, 1992), PRISA-PPO
uses PPO (Schulman et al., 2017), and PRISA-MuZero uses MuZero (Schrittwieser et al., 2019).
We report the test accuracy and the Percentage of Successful Seeds (PSS) in Table 2 to measure
the model’s reasoning capabilities, where the PSS reaches 100% of success rates (Matthieu et al.,
2021). We note that PRISA-MuZero has the same 100% accuracy as NLM (Dong et al., 2019),
DLM (Matthieu et al., 2021), and @ILP (Evans & Grefenstette, 2018) across different tasks, and
outperforms MemNN (Sukhbaatar et al., 2015) (shown in the single-task part in Table 3). But it also
has a higher successful percentage (PSS) in comparison with other methods. The results show that
PRISA-MuZero achieves the best performance on single-task reasoning, which conﬁrms the strength
of the MCTS-based Enhanced Planner (Section 3.3) and the MuZero learning strategy. Therefore, we
will use the Enhanced Planner and MuZero in PRISA and PRIMA in the rest of our empirical studies."
OVERALL PERFORMANCE,0.12412177985948478,"Multi-task reasoning capability
Next, we evaluate the MTR capabilities of PRIMA. To the best
of our knowledge, there is no existing approach that is designed speciﬁcally for MTR. Therefore, we
adapt NLM and DLM into their multi-task versions, named NLM-MTR and DLM-MTR, respectively.
NLM-MTR and DLM-MTR follow the same input and output modiﬁcation as what we did to
upgrade PRISA to PRIMA (Section 2.4). By this, we can examine the contribution of our proposed
Planner-Reasoner architecture for MTR. As shown in Table 3, PRIMA (with MuZero as the Base
Planner) performs well (perfectly) on different reasoning tasks. On the other hand, DLM-MTR
experiences some performance degradation (on AdjacentToRed). This result conﬁrms that our
Planner-Reasoner architecture is more suitable for MTR. We conjecture that the beneﬁt comes from
using a Planner to explicitly select the necessary neural operators for each task, avoiding potential
conﬂicts between different tasks during the learning process."
OVERALL PERFORMANCE,0.1252927400468384,"Experiments are also conducted to test the performance of PRIMA with different problem sizes.
The problem size in training is 10 for all graph tasks and 20 for all family-tree tasks. In testing, we
evaluate the methods on much larger problem sizes (50 for the graph tasks and 100 for the family tree
tasks), which the methods have not seen before. Therefore, the Planner must dynamically activate a
proper set of neural operators to construct a path to solve the new problem. As reported in Fig. 4 and
Tables 2 and 3, PRIMA can achieve the best accuracy and lower ﬂops when the problem sizes for
training and testing are different."
OVERALL PERFORMANCE,0.12646370023419204,"Reasoning efﬁciency
To measure the reasoning efﬁciency of the proposed methods at the inference
stage, PRIMA is compared with NLM, MemNN, and NLM-MTR in terms of FLOPs. As shown in
Fig. 4, NLM and NLM-MTR demonstrate a similar performance and suffer the highest reasoning
cost when it is tested with large problem sizes, such as 50 for graph tasks and 100 for family tree
tasks. For MemNN, although the FLOPs of it seem low in most cases of testing, its testing accuracy
is bad and cannot achieve accurate predictions (Table 3). In contrast, PRIMA can signiﬁcantly reduce
the reasoning complexity by intelligently selecting ops using a planner. Overall, PRIMA strikes a
satisfactory capability-efﬁciency tradeoff in comparison with all available multi-tasking baselines."
OVERALL PERFORMANCE,0.12763466042154567,Under review as a conference paper at ICLR 2022
OVERALL PERFORMANCE,0.1288056206088993,"Table 3: Testing Accuracy and PSS of PRIMA and other baselines on different reasoning tasks. The results of
@ILP, NLM, and DLM are merged in one row due to space constraints and are presented in the same order if the
results are different. Note that PRIMA’s test efﬁciency is superior to NLM-MTR’s as shown in Fig. 4. “m”: the
problem size. “PSS”: Percentage of Successful Seeds. Numbers in red denote < 100%."
OVERALL PERFORMANCE,0.12997658079625293,testing accuracy
OVERALL PERFORMANCE,0.13114754098360656,Family
OVERALL PERFORMANCE,0.1323185011709602,"Tree
HasFather
HasSister
IsGrandparent
IsUncle
IsMGUncle
Graph
AdjacentToRed
4-Connectivity
1-OutDegree"
OVERALL PERFORMANCE,0.13348946135831383,Single Task MemNN
OVERALL PERFORMANCE,0.13466042154566746,"m=20
99.9
86.3
96.5
96.3
99.7
m=10
95.2
92.3
99.8
m=100
59.8
59.8
97.7
96
98.4
m=50
93.1
81.3
78.6
PSS
0
0
0
0
0
PSS
0
0
0"
OVERALL PERFORMANCE,0.1358313817330211,@ILP/ NLM/ DLM
OVERALL PERFORMANCE,0.13700234192037472,"m=20
100
100
100
100
100
m=10
100
100
100
m=100
100
100
100
100
100
m=50
100
100
100
PSS
100
100
100
100/ 90/ 100
100/ 20/ 70
PSS
100/ 90/ 90
100
100"
OVERALL PERFORMANCE,0.13817330210772832,Multi-Task
OVERALL PERFORMANCE,0.13934426229508196,NLM-MTR
OVERALL PERFORMANCE,0.1405152224824356,"m=20
100
100
100
100
100
m=10
100
100
100
m=100
100
100
100
100
100
m=50
100
100
100
PSS
100
100
100
100
90
PSS
90
100
100"
OVERALL PERFORMANCE,0.14168618266978922,DLM-MTR
OVERALL PERFORMANCE,0.14285714285714285,"m=20
100
100
100
100
100
m=10
96.7
100
100
m=100
100
100
100
100
100
m=50
97.2
100
100
PSS
100
100
100
100
100
PSS
0
100
100 PRIMA"
OVERALL PERFORMANCE,0.14402810304449648,"m=20
100
100
100
100
100
m=10
100
100
100
m=100
100
100
100
100
100
m=50
100
100
100
PSS
100
100
100
100
90
PSS
90
100
100"
OVERALL PERFORMANCE,0.1451990632318501,"Operator/Path sharing in MTR
To take a closer look into how PRIMA achieves such a better
capability-efﬁciency tradeoff (in Fig. 4), we examine the reasoning paths on three different graph
tasks: 1-Outdegree, AdjacentToRed, and 4-Connectivity. Speciﬁcally, we sample
instances from these three tasks and feed them into PRIMA separately to generate their corresponding
reasoning paths. The results are plotted in Fig. 5A, where the gray paths denote the ones shared
across tasks, and the colored ones are task-speciﬁc. It clearly shows that PRIMA learns a large set
of neural operators sharable across tasks. Given each input instance from a particular task, PRIMA
activates a set of shared paths along with a few task-speciﬁc paths to deduce the logical consequences."
OVERALL PERFORMANCE,0.14637002341920374,"Generalizability of the Planner
To demonstrate the generalizability of the Planner module in
PRIMA, we generate input instances of AdjacentToRed with topologies that have not been seen
during training. In Fig. 5B, we show the reasoning paths activated by the Planner for these different
topologies, which demonstrates that different input instances share a large portion of reasoning
paths. This fact is not surprising as solving the same task of AdjacentToRed should rely on
a common set of skills. More interestingly, we notice that even for solving this same task, the
Planner will have to call upon a few instance-dependent sub-paths to handle the subtle inherent
differences (e.g., the graph topology) that exist between different input instances. For this reason,
PRIMA maintains an instance-dependent dynamic architecture, which is in sharp contrast to the
Neural Architecture Search (NAS) approaches. Although NAS may also use RL algorithms to seek
for a smaller architecture (Zoph & Le, 2017), it only searches for a static architecture that will be
applied to all the input instances."
OVERALL PERFORMANCE,0.14754098360655737,"Figure 4: The reasoning costs (in FLOPs) of different models at the inference stage (1 M=1⇥106, 1 G=1⇥109).
Compared to NLM and NLM-MTR, our PRIMA signiﬁcantly reduces the reasoning complexity by intelligently
selecting ops (short for neural operators) using a planner. Although the FLOPs of MemNN seem low in
most cases of testing, its testing accuracy is bad and cannot achieve accurate predictions (see Table 3). “OD”
denotes 1-Outdegree, likewise, “AR”:AdjacentToRed, “4C”:4-Connectivity, “HF”:HasFather,
“HS”:HasSister, “GP”:IsGrandparent, “UN”:IsUncle, “MG”:IsMGUncle."
RELATED WORK,0.148711943793911,"5
RELATED WORK"
RELATED WORK,0.14988290398126464,"Multi-task learning
Multi-task learning (Zhang & Yang, 2021; Zhou et al., 2011; Pan & Yang,
2009) has focused primarily on the supervised learning paradigm, which itself can be divided into
several approaches. The ﬁrst is feature-based MTL, also called multi-task feature learning, (Argyriou"
RELATED WORK,0.15105386416861827,Under review as a conference paper at ICLR 2022
RELATED WORK,0.1522248243559719,"Figure 5: The reasoning paths of PRIMA. The gray arrows denote the “shared path”. The colored arrows in
sub-ﬁgures (A) and (B) denote task-speciﬁc paths and instance-speciﬁc paths, respectively. In (A), besides the
AdjacentToRed task we have introduced earlier, we also consider the 1-Outdegree task, which reasons
about whether the out-degree of a node is exactly equal to 1, and the 4-Connectivity task, which is to
decide whether there are two nodes connected within 4 hops."
RELATED WORK,0.15339578454332553,"et al., 2008), which explores the sharing of features across different tasks using regularization tech-
niques (Shinohara, 2016; Liu et al., 2017). The second approach assumes that tasks are intrinsically
related, such as low-rank learning (Ando & Zhang, 2005; Zhang et al., 2005), learning with task
clustering (Gu et al., 2011; Zhang et al., 2016), and task-relation learning (such as via task similarity,
task correlation, or task covariance) (Goncalves et al., 2016; Zhang, 2013; Ciliberto et al., 2015).
MTL has also been explored under other paradigms such as unsupervised learning, for example, via
multi-task clustering in (Zhang & Zhang, 2013; Zhang et al., 2015b; Gu et al., 2011; Zhang, 2015)."
RELATED WORK,0.15456674473067916,"Neural-symbolic reasoning
Neural-symbolic AI for reasoning and inference has a long his-
tory (Besold et al., 2017; Bader et al., 2004; Garcez et al., 2008), and neural-ILP has developed
primarily in two major directions (Cropper et al., 2020). The ﬁrst direction applies neural operators
such as tensor calculus to simulate logic reasoning (Yang et al., 2017; Dong et al., 2019; Shi et al.,
2020) and (Manhaeve et al., 2018). This approach uses binary tensors over constant domains to
represent the predicates and tensor chain products to simulate logic clauses. The second direction
involves relaxed subset selection (Evans & Grefenstette, 2018; Si et al., 2019) with a predeﬁned set of
task-speciﬁc logic clauses. This approach reduces the task to a subset-selection problem by selecting
a subset of clauses from the predeﬁned set and using neural networks to search for a relaxed solution."
RELATED WORK,0.1557377049180328,"Our work is different from the most recent work, such as (Dong et al., 2019; Shi et al., 2020), in
several ways. Compared with (Dong et al., 2019), the most notable difference is the improvement in
efﬁciency by introducing learning-to-reason via reinforcement learning. A second major difference is
that PRIMA offers more generalizability than NLM by decomposing the logic operators into more
ﬁnely-grained units. We refer readers for more detailed related work to Appendix D."
CONCLUSION,0.15690866510538642,"6
CONCLUSION"
CONCLUSION,0.15807962529274006,"A long-standing challenge in multi-task learning is the intrinsic conﬂict between capability and
efﬁciency. In this paper, we propose a Planner-Reasoner framework for multi-task reasoning. To
maintain broad capability but efﬁcient speciﬁc performance, the Reasoner extracts shareable meta-
rules via ﬁrst-order logic, from which the Planner efﬁciently selects a subset of meta-rules to formulate
the reasoning path. The model’s training follows a complete data-driven end-to-end approach via
deep reinforcement learning, and the performance is validated across a variety of benchmark tasks.
Future work could include extending the framework to high-order logic and investigating scenarios
when meta-rules have a hierarchical structure."
CONCLUSION,0.1592505854800937,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.16042154566744732,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.16159250585480095,"We commit to ensuring that other researchers with reasonable background knowledge in our area can
reproduce our theoretical and empirical results. The algorithm details can be seen in Appendix A.
Speciﬁcally, benchmark details are provided in Appendix B.1, hyper-parameter settings are provided
in Appendix B.2, and computing infrastructure are detailed in Appendix B.3. The experiment details
can be seen in Appendix B."
ETHICS STATEMENT,0.16276346604215455,ETHICS STATEMENT
ETHICS STATEMENT,0.16393442622950818,"This work is about the methodology of achieving a capability-efﬁciency trade-off in multi-task
ﬁrst-order logic reasoning. The potential impact of this work is likely to further extend the framework
to high-order logic and investigate scenarios when meta-rules have a hierarchical structure, which
should be generally beneﬁcial to the multi-task learning research community. We have not considered
speciﬁc applications or practical scenarios as the goal of this work. Hence, it does not have any direct
ethical consequences."
REFERENCES,0.16510538641686182,REFERENCES
REFERENCES,0.16627634660421545,Rie Kubota Ando and Tong Zhang. A framework for learning predictive structures from multiple
REFERENCES,0.16744730679156908,"tasks and unlabeled data. JMLR, 2005."
REFERENCES,0.1686182669789227,"Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy"
REFERENCES,0.16978922716627634,"sketches. In ICML, 2017."
REFERENCES,0.17096018735362997,"Peter B Andrews. An introduction to mathematical logic and type theory, volume 27. Springer"
REFERENCES,0.1721311475409836,"Science & Business Media, 2002."
REFERENCES,0.17330210772833723,"Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature learning."
REFERENCES,0.17447306791569087,"MLJ, 2008."
REFERENCES,0.1756440281030445,Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine
REFERENCES,0.17681498829039813,"Learning Research, 3(Nov):397–422, 2002."
REFERENCES,0.17798594847775176,"Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit"
REFERENCES,0.1791569086651054,"problem. Machine learning, 47(2-3):235–256, 2002."
REFERENCES,0.18032786885245902,Sebastian Bader and Pascal Hitzler. Dimensions of neural-symbolic integration-a structured survey.
REFERENCES,0.18149882903981265,"arXiv preprint cs/0511042, 2005."
REFERENCES,0.18266978922716628,"Sebastian Bader, Pascal Hitzler, and Steffen Hölldobler. The integration of connectionism and"
REFERENCES,0.18384074941451992,"ﬁrst-order knowledge representation and reasoning as a challenge for artiﬁcial intelligence. arXiv
preprint cs/0408069, 2004."
REFERENCES,0.18501170960187355,"Peter W Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, and Koray Kavukcuoglu. Interac-"
REFERENCES,0.18618266978922718,"tion networks for learning about objects, relations and physics. arXiv preprint arXiv:1612.00222,
2016."
REFERENCES,0.1873536299765808,"Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,"
REFERENCES,0.1885245901639344,"Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018."
REFERENCES,0.18969555035128804,Yoshua Bengio. From system 1 deep learning to system 2 deep learning. In Thirty-third Conference
REFERENCES,0.19086651053864168,"on Neural Information Processing Systems, 2019."
REFERENCES,0.1920374707259953,"Tarek R Besold, Artur d’Avila Garcez, Sebastian Bader, Howard Bowman, Pedro Domingos, Pas-"
REFERENCES,0.19320843091334894,"cal Hitzler, Kai-Uwe Kühnberger, Luis C Lamb, Daniel Lowd, Priscila Machado Vieira Lima,
et al. Neural-symbolic learning and reasoning: A survey and interpretation. arXiv preprint
arXiv:1711.03902, 2017."
REFERENCES,0.19437939110070257,Under review as a conference paper at ICLR 2022
REFERENCES,0.1955503512880562,"Timo Bräm, Gino Brunner, Oliver Richter, and Roger Wattenhofer. Attentive multi-task deep"
REFERENCES,0.19672131147540983,"reinforcement learning. In ECML/PKDD, 2019."
REFERENCES,0.19789227166276346,"Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp"
REFERENCES,0.1990632318501171,"Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey
of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in
games, 4(1):1–43, 2012."
REFERENCES,0.20023419203747073,"Salam El Bsat, Haitham Bou-Ammar, and Matthew E. Taylor. Scalable multitask policy gradient"
REFERENCES,0.20140515222482436,"reinforcement learning. In AAAI, 2017."
REFERENCES,0.202576112412178,"Daniele Calandriello, Alessandro Lazaric, and Marcello Restelli. Sparse multi-task reinforcement"
REFERENCES,0.20374707259953162,"learning. In NIPS, 2014."
REFERENCES,0.20491803278688525,"Marco Calautti, Georg Gottlob, and Andreas Pieris. Chase termination for guarded existential rules."
REFERENCES,0.20608899297423888,"In Proceedings of the 34th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database
Systems, pp. 91–103, 2015."
REFERENCES,0.20725995316159251,L. Cao. Non-iid recommender systems: A review and framework of recommendation paradigm
REFERENCES,0.20843091334894615,"shifting. Engineering, 2(2):212–224, 2016."
REFERENCES,0.20960187353629978,"Deepayan Chakrabarti and Christos Faloutsos. Graph mining: Laws, generators, and algorithms."
REFERENCES,0.2107728337236534,"ACM computing surveys (CSUR), 38(1):2–es, 2006."
REFERENCES,0.21194379391100704,Ashok K Chandra and David Harel. Horn clause queries and generalizations. The Journal of Logic
REFERENCES,0.21311475409836064,"Programming, 2(1):1–15, 1985."
REFERENCES,0.21428571428571427,"Jianzhong Chen, Stephen Muggleton, and José Santos. Learning probabilistic logic models from"
REFERENCES,0.2154566744730679,"probabilistic examples. Machine learning, 73(1):55–85, 2008."
REFERENCES,0.21662763466042154,"K. Chen, F. Yang, and X. Chen. Planning with task-oriented knowledge acquisition for a service"
REFERENCES,0.21779859484777517,"robot. In IJCAI, pp. 812–818, 2016."
REFERENCES,0.2189695550351288,"Xinlei Chen, Li-Jia Li, Li Fei-Fei, and Abhinav Gupta. Iterative visual reasoning beyond convolutions."
REFERENCES,0.22014051522248243,"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7239–
7248, 2018."
REFERENCES,0.22131147540983606,"Carlo Ciliberto, Youssef Mroueh, Tomaso A. Poggio, and Lorenzo Rosasco. Convex learning of"
REFERENCES,0.2224824355971897,"multiple tasks and their structure. In ICML, 2015."
REFERENCES,0.22365339578454332,"Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text"
REFERENCES,0.22482435597189696,"encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020."
REFERENCES,0.2259953161592506,"Diane J Cook and Lawrence B Holder. Mining graph data. John Wiley & Sons, 2006."
REFERENCES,0.22716627634660422,Michael Crawshaw. Multi-task learning with deep neural networks: A survey. arXiv preprint
REFERENCES,0.22833723653395785,"arXiv:2009.09796, 2020."
REFERENCES,0.22950819672131148,Andrew Cropper and Sebastijan Dumancic. Inductive logic programming at 30: a new introduction.
REFERENCES,0.2306791569086651,"arXiv preprint arXiv:2008.07912, 2020."
REFERENCES,0.23185011709601874,"Andrew Cropper, Sebastijan Dumancic, and Stephen H Muggleton. Turning 30: New ideas in"
REFERENCES,0.23302107728337237,"inductive logic programming. arXiv preprint arXiv:2002.11002, 2020."
REFERENCES,0.234192037470726,"Aniket Anand Deshmukh, Ürün Dogan, and Clayton Scott. Multi-task learning for contextual bandits."
REFERENCES,0.23536299765807964,"In NIPS, 2017."
REFERENCES,0.23653395784543327,"Carlos Diuk, Andre Cohen, and Michael L Littman. An object-oriented representation for efﬁcient"
REFERENCES,0.23770491803278687,"reinforcement learning. In Proceedings of the 25th international conference on Machine learning,
pp. 240–247, 2008."
REFERENCES,0.2388758782201405,"Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic"
REFERENCES,0.24004683840749413,"machines. arXiv preprint arXiv:1904.11694, 2019."
REFERENCES,0.24121779859484777,Under review as a conference paper at ICLR 2022
REFERENCES,0.2423887587822014,"Sašo Džeroski, Luc De Raedt, and Kurt Driessens. Relational reinforcement learning. Machine"
REFERENCES,0.24355971896955503,"learning, 43(1):7–52, 2001."
REFERENCES,0.24473067915690866,"Lasse Espeholt, Hubert Soyer, Rémi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam"
REFERENCES,0.2459016393442623,"Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA:
scalable distributed deep-RL with importance weighted actor-learner architectures. In ICML, 2018."
REFERENCES,0.24707259953161592,Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. Journal of
REFERENCES,0.24824355971896955,"Artiﬁcial Intelligence Research, 61:1–64, 2018."
REFERENCES,0.24941451990632318,Melvin Fitting. First-order logic and automated theorem proving. Springer Science & Business
REFERENCES,0.2505854800936768,"Media, 2012."
REFERENCES,0.25175644028103045,Jean H Gallier. Logic for computer science: foundations of automatic theorem proving. Courier
REFERENCES,0.2529274004683841,"Dover Publications, 2015."
REFERENCES,0.2540983606557377,"M. C. Ganiz, C. George, and W. M Pottenger. Higher order naive bayes: A novel non-iid approach to"
REFERENCES,0.25526932084309134,"text classiﬁcation. IEEE Transactions on Knowledge and Data Engineering, 23(7):1022–1034,
2010."
REFERENCES,0.25644028103044497,"Artur SD’Avila Garcez, Luis C Lamb, and Dov M Gabbay. Neural-symbolic cognitive reasoning."
REFERENCES,0.2576112412177986,"Springer Science & Business Media, 2008."
REFERENCES,0.25878220140515223,"M. Gebser, B. Kaufmann, and T. Schaub. Conﬂict-driven answer set solving: From theory to practice."
REFERENCES,0.25995316159250587,"Artiﬁcial Intelligence, 187-188:52–89, 2012."
REFERENCES,0.2611241217798595,Michael Gelfond and Vladimir Lifschitz. Action languages. 1998.
REFERENCES,0.26229508196721313,"André R. Goncalves, Fernando J. Von Zuben, and Arindam Banerjee. Multi-task sparse structure"
REFERENCES,0.26346604215456676,"learning with Gaussian copula models. JMLR, 2016."
REFERENCES,0.2646370023419204,"Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil"
REFERENCES,0.265807962529274,"Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014."
REFERENCES,0.26697892271662765,"Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-"
REFERENCES,0.2681498829039813,"Barwi´nska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al.
Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):
471–476, 2016."
REFERENCES,0.2693208430913349,"Quanquan Gu, Zhenhui Li, and Jiawei Han. Learning a kernel for multi-task clustering. In AAAI, 2011."
REFERENCES,0.27049180327868855,"Carlos Guestrin, Daphne Koller, Chris Gearhart, and Neal Kanodia. Generalizing plans to new"
REFERENCES,0.2716627634660422,"environments in relational mdps. In Proceedings of the 18th international joint conference on
Artiﬁcial intelligence, pp. 1003–1010, 2003."
REFERENCES,0.2728337236533958,"Marc Hanheide, Moritz Göbelbecker, Graham S Horn, Andrzej Pronobis, Kristoffer Sjöö, Alper"
REFERENCES,0.27400468384074944,"Aydemir, Patric Jensfelt, Charles Gretton, Richard Dearden, Miroslav Janicek, et al. Robot task
planning and explanation in open and uncertain worlds. Artiﬁcial Intelligence, 2015."
REFERENCES,0.275175644028103,"M. Helmert. The fast downward planning system. Journal of Artiﬁcial Intelligence Research, 26:"
REFERENCES,0.27634660421545665,"191–246, 2006."
REFERENCES,0.2775175644028103,Alfred Horn. On sentences which are true of direct unions of algebras1. The Journal of Symbolic
REFERENCES,0.2786885245901639,"Logic, 16(1):14–21, 1951."
REFERENCES,0.27985948477751754,"Maximilian Igl, Andrew Gambardella, Nantas Nardelli, N. Siddharth, Wendelin Böhmer, and Shimon"
REFERENCES,0.2810304449648712,"Whiteson. Multitask soft option learning. In UAI, 2020."
REFERENCES,0.2822014051522248,"Herbert Jaeger. Deep neural reasoning. Nature, 538(7626):467–468, 2016."
REFERENCES,0.28337236533957844,"Ramtin Keramati, Jay Whang, Patrick Cho, and Emma Brunskill. Strategic object oriented rein-"
REFERENCES,0.28454332552693207,"forcement learning. In Exploration in Reinforcement Learning Workshop at the 35th International
Conference on Machine Learning, 2018."
REFERENCES,0.2857142857142857,Under review as a conference paper at ICLR 2022
REFERENCES,0.28688524590163933,"P. Khandelwal, S. Zhang, J. Sinapov, M. Leonetti, J. Thomason, F. Yang, I. Gori, M. Svetlik, P. Khante,"
REFERENCES,0.28805620608899296,"V. Lifschitz, and P. Stone. Bwibots: A platform for bridging the gap between ai and human–robot
interaction research. The International Journal of Robotics Research, 36(5-7):635–659, 2017."
REFERENCES,0.2892271662763466,"Levente Kocsis, Csaba Szepesvári, and Jan Willemson. Improved monte-carlo search. Univ. Tartu,"
REFERENCES,0.2903981264637002,"Estonia, Tech. Rep, 1, 2006."
REFERENCES,0.29156908665105385,Daphne Koller. Probabilistic relational models. In International Conference on Inductive Logic
REFERENCES,0.2927400468384075,"Programming, pp. 3–13. Springer, 1999."
REFERENCES,0.2939110070257611,"Daphne Koller, Nir Friedman, Sašo Džeroski, Charles Sutton, Andrew McCallum, Avi Pfeffer, Pieter"
REFERENCES,0.29508196721311475,"Abbeel, Ming-Fai Wong, David Heckerman, Chris Meek, et al. Introduction to statistical relational
learning. MIT press, 2007."
REFERENCES,0.2962529274004684,Markus Krötzsch. Computing cores for existential rules with the standard chase and asp. In
REFERENCES,0.297423887587822,"Proceedings of the International Conference on Principles of Knowledge Representation and
Reasoning, volume 17, pp. 603–613, 2020."
REFERENCES,0.29859484777517564,Alessandro Lazaric and Mohammad Ghavamzadeh. Bayesian multi-task reinforcement learning. In
REFERENCES,0.2997658079625293,"ICML, 2010."
REFERENCES,0.3009367681498829,"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015."
REFERENCES,0.30210772833723654,"Hui Li, Xuejun Liao, and Lawrence Carin. Multi-task reinforcement learning in partially observable"
REFERENCES,0.30327868852459017,"stochastic environments. JMLR, 2009."
REFERENCES,0.3044496487119438,"Sijin Li, Zhi-Qiang Liu, and Antoni B. Chan. Heterogeneous multi-task learning for human pose"
REFERENCES,0.30562060889929743,"estimation with deep convolutional neural network. IJCV, 2015."
REFERENCES,0.30679156908665106,V. Lifschitz. What is answer set programming? In Proceedings of the AAAI Conference on Artiﬁcial
REFERENCES,0.3079625292740047,"Intelligence, pp. 1594–1597. MIT Press, 2008."
REFERENCES,0.3091334894613583,"Xingyu Lin, Harjatin Singh Baweja, George Kantor, and David Held. Adaptive auxiliary task"
REFERENCES,0.31030444964871196,"weighting for reinforcement learning. In NeurIPS, 2019."
REFERENCES,0.3114754098360656,"Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Adversarial multi-task learning for text classiﬁcation."
REFERENCES,0.3126463700234192,"In ACL, 2017."
REFERENCES,0.31381733021077285,"Wu Liu, Tao Mei, Yongdong Zhang, Cherry Che, and Jiebo Luo. Multi-task deep visual-semantic"
REFERENCES,0.3149882903981265,"embedding for video thumbnail selection. In CVPR, 2015."
REFERENCES,0.3161592505854801,"Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt."
REFERENCES,0.31733021077283374,"Deepproblog: Neural probabilistic logic programming.
In Advances in Neural Information
Processing Systems, pp. 3749–3759, 2018."
REFERENCES,0.3185011709601874,"Zimmer Matthieu, Feng Xuening, Glanois Claire, Jiang Zhaohui, Zhang Jianyi, Weng Paul, Jianye"
REFERENCES,0.319672131147541,"Hao, Dong Li, and Wulong Liu. Differentiable logic machines. arXiv preprint arXiv:2102.11529,
2021."
REFERENCES,0.32084309133489464,"Drew McDermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso,"
REFERENCES,0.32201405152224827,"Daniel Weld, and David Wilkins. Pddl-the planning domain deﬁnition language. 1998."
REFERENCES,0.3231850117096019,"Nikola Mrksic, Diarmuid Ó Séaghdha, Blaise Thomson, Milica Gasic, Pei-hao Su, David Vandyke,"
REFERENCES,0.32435597189695553,"Tsung-Hsien Wen, and Steve J. Young. Multi-domain dialog state tracking using recurrent neural
networks. In ACL, 2015."
REFERENCES,0.3255269320843091,Stephen Muggleton and Luc De Raedt. Inductive logic programming: Theory and methods. The
REFERENCES,0.32669789227166274,"Journal of Logic Programming, 19:629–679, 1994."
REFERENCES,0.32786885245901637,Remi Munos. From bandits to monte-carlo tree search: The optimistic principle applied to optimiza-
REFERENCES,0.32903981264637,"tion and planning. Foundations and Trends in Machine Learning, 2014."
REFERENCES,0.33021077283372363,Under review as a conference paper at ICLR 2022
REFERENCES,0.33138173302107726,"Shayegan Omidshaﬁei, Jason Pazis, Christopher Amato, Jonathan P. How, and John Vian. Deep"
REFERENCES,0.3325526932084309,"decentralized multi-task multi-agent reinforcement learning under partial observability. In ICML,
2017."
REFERENCES,0.3337236533957845,"Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. arXiv preprint"
REFERENCES,0.33489461358313816,"arXiv:1711.08028, 2017."
REFERENCES,0.3360655737704918,Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
REFERENCES,0.3372365339578454,"and data engineering, 22(10):1345–1359, 2009."
REFERENCES,0.33840749414519905,"Emilio Parisotto, Jimmy Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer"
REFERENCES,0.3395784543325527,"reinforcement learning. In ICLR, 2016."
REFERENCES,0.3407494145199063,Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
REFERENCES,0.34192037470725994,"Wiley & Sons, 2014."
REFERENCES,0.3430913348946136,Christopher D Rosin. Multi-armed bandits with episode context. Annals of Mathematics and Artiﬁcial
REFERENCES,0.3442622950819672,"Intelligence, 61(3):203–230, 2011."
REFERENCES,0.34543325526932084,"Andrei A. Rusu, Sergio Gomez Colmenarejo, Çaglar Gülçehre, Guillaume Desjardins, James Kirk-"
REFERENCES,0.34660421545667447,"patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell.
Policy
distillation. In ICLR, 2016."
REFERENCES,0.3477751756440281,"Adam Santoro, David Raposo, David GT Barrett, Mateusz Malinowski, Razvan Pascanu, Peter"
REFERENCES,0.34894613583138173,"Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. arXiv
preprint arXiv:1706.01427, 2017."
REFERENCES,0.35011709601873536,"Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-"
REFERENCES,0.351288056206089,"ization help optimization? arXiv preprint arXiv:1805.11604, 2018."
REFERENCES,0.3524590163934426,"Andrew M. Saxe, Adam Christopher Earle, and Benjamin Rosman. Hierarchy through composition"
REFERENCES,0.35362997658079626,"with multitask LMDPs. In ICML, 2017."
REFERENCES,0.3548009367681499,"Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon"
REFERENCES,0.3559718969555035,"Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019."
REFERENCES,0.35714285714285715,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy"
REFERENCES,0.3583138173302108,"optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.3594847775175644,Sahil Sharma and Balaraman Ravindran. Online multi-task learning using active sampling. In ICLR
REFERENCES,0.36065573770491804,"Workshop, 2017."
REFERENCES,0.3618266978922717,"Sahil Sharma, Ashutosh Kumar Jha, Parikshit Hegde, and Balaraman Ravindran. Learning to"
REFERENCES,0.3629976580796253,"multi-task by active sampling. In ICLR, 2018."
REFERENCES,0.36416861826697894,"Shaoyun Shi, Hanxiong Chen, Weizhi Ma, Jiaxin Mao, Min Zhang, and Yongfeng Zhang. Neural"
REFERENCES,0.36533957845433257,"logic reasoning. In Proceedings of the 29th ACM International Conference on Information &
Knowledge Management, pp. 1365–1374, 2020."
REFERENCES,0.3665105386416862,Yusuke Shinohara. Adversarial multi-task learning of deep neural networks for robust speech
REFERENCES,0.36768149882903983,"recognition. In Interspeech, 2016."
REFERENCES,0.36885245901639346,"Xujie Si, Mukund Raghothaman, Kihong Heo, and Mayur Naik. Synthesizing datalog programs"
REFERENCES,0.3700234192037471,"using numerical relaxation. arXiv preprint arXiv:1906.00163, 2019."
REFERENCES,0.3711943793911007,"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,"
REFERENCES,0.37236533957845436,"Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi
by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815,
2017a."
REFERENCES,0.373536299765808,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,"
REFERENCES,0.3747072599531616,"Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354–359, 2017b."
REFERENCES,0.3758782201405152,Under review as a conference paper at ICLR 2022
REFERENCES,0.3770491803278688,"Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks."
REFERENCES,0.37822014051522246,"In Advances in Neural Information Processing Systems, pp. 2440–2448, 2015."
REFERENCES,0.3793911007025761,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.3805620608899297,"Prasad Tadepalli, Robert Givan, and Kurt Driessens. Relational reinforcement learning: An overview."
REFERENCES,0.38173302107728335,"In Proceedings of the ICML-2004 workshop on relational reinforcement learning, pp. 1–9, 2004."
REFERENCES,0.382903981264637,"Yee Whye Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell,"
REFERENCES,0.3840749414519906,"Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In NIPS,
2017."
REFERENCES,0.38524590163934425,"Rasul Tutunov, Dongho Kim, and Haitham Bou-Ammar. Distributed multitask reinforcement learning"
REFERENCES,0.3864168618266979,"with quadratic convergence. In NeurIPS, 2018."
REFERENCES,0.3875878220140515,"Frank Van Harmelen, Vladimir Lifschitz, and Bruce Porter. Handbook of knowledge representation."
REFERENCES,0.38875878220140514,"Elsevier, 2008."
REFERENCES,0.38992974238875877,Nelson Vithayathil Varghese and Qusay H Mahmoud. A survey of multi-task deep reinforcement
REFERENCES,0.3911007025761124,"learning. Electronics, 9(9):1363, 2020."
REFERENCES,0.39227166276346603,"Tung-Long Vuong, Do Van Nguyen, Tai-Long Nguyen, Cong-Minh Bui, Hai-Dang Kieu, Viet-Cuong"
REFERENCES,0.39344262295081966,"Ta, Quoc-Long Tran, and Thanh Ha Le. Sharing experience in multitask reinforcement learning.
In IJCAI, 2019."
REFERENCES,0.3946135831381733,Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
REFERENCES,0.3957845433255269,"learning. Machine learning, 8(3-4):229–256, 1992."
REFERENCES,0.39695550351288056,"Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: A"
REFERENCES,0.3981264637002342,"hierarchical Bayesian approach. In ICML, 2007."
REFERENCES,0.3992974238875878,"Victoria Xia, Zi Wang, and Leslie Pack Kaelbling. Learning sparse relational transition models. arXiv"
REFERENCES,0.40046838407494145,"preprint arXiv:1810.11177, 2018."
REFERENCES,0.4016393442622951,"Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge"
REFERENCES,0.4028103044496487,"base reasoning. In Advances in Neural Information Processing Systems, pp. 2319–2328, 2017."
REFERENCES,0.40398126463700235,"Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-"
REFERENCES,0.405152224824356,"symbolic vqa: Disentangling reasoning from vision and language understanding. In NIPS, pp.
1031–1042, 2018."
REFERENCES,0.4063231850117096,"N. Yoshida, T. Nishio, M. Morikura, K. Yamamoto, and R. Yonetani. Hybrid-ﬂfor wireless networks:"
REFERENCES,0.40749414519906324,"Cooperative learning mechanism using non-iid data. In ICC 2020-2020 IEEE International
Conference on Communications (ICC), pp. 1–7. IEEE, 2020."
REFERENCES,0.40866510538641687,"Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl"
REFERENCES,0.4098360655737705,"Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Relational deep reinforcement
learning. arXiv preprint arXiv:1806.01830, 2018."
REFERENCES,0.41100702576112413,"Jian Zhang, Zoubin Ghahramani, and Yiming Yang. Learning multiple related tasks using latent"
REFERENCES,0.41217798594847777,"independent component analysis. In NIPS, 2005."
REFERENCES,0.4133489461358314,"Si Zhang, Hanghang Tong, Jiejun Xu, and Ross Maciejewski. Graph convolutional networks: a"
REFERENCES,0.41451990632318503,"comprehensive review. Computational Social Networks, 6(1):1–23, 2019."
REFERENCES,0.41569086651053866,"Wenlu Zhang, Rongjian Li, Tao Zeng, Qian Sun, Sudhir Kumar, Jieping Ye, and Shuiwang Ji. Deep"
REFERENCES,0.4168618266978923,"model based transfer and multi-task learning for biological image analysis. In KDD, 2015a."
REFERENCES,0.4180327868852459,Xianchao Zhang and Xiaotong Zhang. Smart multi-task Bregman clustering and multi-task kernel
REFERENCES,0.41920374707259955,"clustering. In AAAI, 2013."
REFERENCES,0.4203747072599532,"Xianchao Zhang, Xiaotong Zhang, and Han Liu. Smart multitask Bregman clustering and multitask"
REFERENCES,0.4215456674473068,"kernel clustering. ACM TKDD, 2015b."
REFERENCES,0.42271662763466045,"Xianchao Zhang, Xiaotong Zhang, and Han Liu. Self-adapted multi-task clustering. In IJCAI, 2016."
REFERENCES,0.4238875878220141,Under review as a conference paper at ICLR 2022
REFERENCES,0.42505854800936765,"Xiao-Lei Zhang. Convex discriminative multitask clustering. IEEE TPAMI, 2015."
REFERENCES,0.4262295081967213,"Yu Zhang. Heterogeneous-neighborhood-based multi-task local learning algorithms. In NIPS, 2013."
REFERENCES,0.4274004683840749,Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and
REFERENCES,0.42857142857142855,"Data Engineering, 2021."
REFERENCES,0.4297423887587822,"Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Facial landmark detection by deep"
REFERENCES,0.4309133489461358,"multi-task learning. In ECCV, 2014."
REFERENCES,0.43208430913348944,"Jiayu Zhou, Jianhui Chen, and Jieping Ye. Malsar: Multi-task learning via structural regularization."
REFERENCES,0.4332552693208431,"Arizona State University, 21, 2011."
REFERENCES,0.4344262295081967,"Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,"
REFERENCES,0.43559718969555034,"Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications.
AI Open, 1:57–81, 2020."
REFERENCES,0.43676814988290397,"Zhuangdi Zhu, Kaixiang Lin, and Jiayu Zhou. Transfer learning in deep reinforcement learning: A"
REFERENCES,0.4379391100702576,"survey. arXiv preprint arXiv:2009.07888, 2020."
REFERENCES,0.43911007025761123,"Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. ICLR, 2017."
REFERENCES,0.44028103044496486,Under review as a conference paper at ICLR 2022
REFERENCES,0.4414519906323185,SUPPLEMENTARY MATERIAL
REFERENCES,0.4426229508196721,"A
ALGORITHM DETAILS"
REFERENCES,0.44379391100702575,"A.1
DETAILS OF MCTS: FOUR KEY STEPS"
REFERENCES,0.4449648711943794,"During each epoch, MCTS repeatedly performs four sequential steps: selection, expansion, simulation,
and backpropagation. The selection step traverses the existing search tree until the leaf node (or
termination condition) by choosing actions (edges) as at each node s according to a tree policy. One
widely used tree policy is the UCT (Kocsis et al., 2006) policy, which conducts the node-selection via"
REFERENCES,0.446135831381733,as = arg max
REFERENCES,0.44730679156908665,s02C(s) ⇢
REFERENCES,0.4484777517564403,Vs0 + β r
LOG NS,0.4496487119437939,"2 log Ns Ns0 % ,
(2)"
LOG NS,0.45081967213114754,"where C(s) denotes the set of all child nodes for s, the ﬁrst term Vs0 is an estimate of the long-
term cumulative reward that can be received when starting from the state represented by node s0,
and the second term represents the uncertainty (conﬁdence-interval size) of that estimate. The
conﬁdence interval is calculated based on the upper conﬁdence bound (UCB) (Auer et al., 2002; Auer,
2002) using Ns and Ns0, which denote the number of times that nodes s and s0 have been visited
(respectively). The key idea of UCT policy (2) is to select the best action according to an optimistic
estimation (the upper conﬁdence bound) of the expected return, which balances the exploitation
(ﬁrst term) and exploration (second term) with β controlling the trade-off. The second step (node
expansion) is conducted according to a prior policy by adding a new child node if the selection
process reaches a leaf node of the search tree. Next, the simulation step estimates the value function
(cumulative reward) ˆVs by running the environment simulator with a default (simulation) policy.
Finally, the backpropagation step updates the statistics Vs and Ns from the leaf node sT to the root
node s0 of the selected path by recursively performing the following update (i.e., from t = T −1 to
t = 0):"
LOG NS,0.4519906323185012,"Nst  Nst + 1,
ˆVst  r(st, at) + γ ˆVst+1,
Vst '"
LOG NS,0.4531615925058548,"(Nst −1)Vst + ˆVst ( /Nst,"
LOG NS,0.45433255269320844,"where ˆVsT is the simulation return of sT , and at denotes the action selected following (2) at state st.
Generally, the expansion step and the simulation step are more time-consuming than the other two
steps, as they involve a large number of interactions with the environment."
LOG NS,0.45550351288056207,"In our Enhanced Planner, we adopt a recently proposed variant of MCTS that is based on probabilistic
upper conﬁdence tree (PUCT) (Rosin, 2011). It conducts node-selection according to a so-called
PUCT score with two components PUCT(s, a) = Q(s, a) + U(s, a). Q(s, a) is the mean state-action
value calculated from the averaged game result that takes action a during current simulations, and
U(s, a) is the exploration bonus calculated as"
LOG NS,0.4566744730679157,"U(s, a) = ⇡(a|s) pP"
LOG NS,0.45784543325526933,"b N(s, b)
1 + N(s, a) ✓"
LOG NS,0.45901639344262296,c1 + log ⇣P
LOG NS,0.4601873536299766,"b N(s, b) + c2 + 1 c2 ⌘◆ ,"
LOG NS,0.4613583138173302,"where a, b are possible actions, ⇡(a|s) is the output of the Base Planner, N(s, a) is the visit count
of the (s, a) pair during current simulations, and constants c1, c2 are the exploration-controlling
hyper-parameters."
LOG NS,0.46252927400468385,"An MCTS algorithm is a model-based RL algorithm that plans the best action at each time step
(Browne et al., 2012) by constructing a search tree, with states as nodes and actions as edges.
It uses the MDP model to identify the best action at each time step until the leaf node (or until
other termination conditions are satisﬁed) by choosing actions (edges) as at each node s according
to a tree policy. For implementations of Monte-Carlo Tree Search, we refer readers to https:
//github.com/werner-duvaud/muzero-general for details."
LOG NS,0.4637002341920375,"A.2
DETAILS OF CALCULATING REASONING ACCURACY"
LOG NS,0.4648711943793911,"Since the “target” predicates are available for all objects or pairs of objects, the reasoning accuracy
refers to the accuracy evaluated on all objects (for properties such as AdjacentToRed(x) or
1-OutDegree(x)) or pairs of objects (for relations such as 4-Connectivity(x, y))."
LOG NS,0.46604215456674475,Under review as a conference paper at ICLR 2022
LOG NS,0.4672131147540984,"B
EXPERIMENTAL DETAILS"
LOG NS,0.468384074941452,"B.1
REASONING TASKS INTRODUCTION"
LOG NS,0.46955503512880564,"Graph
For graph tasks, they have the same background knowledge (or background predicate):
HasEdge(x, y), i.e., HasEdge(x, y) is True if there is an undirected edge between node x and node
y. However, AdjacentToRed has an extra background predicate, Red(x). Red(x) is True if the
color of node x is red. Speciﬁcally, these graph tasks seeks to predict the target concepts (or target
predicates) shown as below."
LOG NS,0.4707259953161593,• 1-OutDegree: 1-OutDegree(x) is True if the out-degree of node x is exactly 1.
LOG NS,0.4718969555035129,• AdjacentToRed: AdjacentToRed(x) is True if the node x has an edge with a red node.
LOG NS,0.47306791569086654,"• 4-Connectivity: 4-Connectivity(x, y) is True if there are within 4 hops between"
LOG NS,0.47423887587822017,node x and node y.
LOG NS,0.47540983606557374,"Family Tree
For family tree tasks, they have the same background knowledge (or background pred-
icates): IsFather(x, y), IsMother(x, y), IsSon(x, y) and IsDaughter(x, y). For instance,
IsFather(x, y) is True when y is x’s father. Speciﬁcally, these family tree tasks seek to predict the
target concepts (or target predicates) shown below."
LOG NS,0.4765807962529274,• HasFather: HasFather(x) is True if x has father.
LOG NS,0.477751756440281,• HasSister: HasSister(x) is True if x has at least one sister.
LOG NS,0.47892271662763464,"• IsGrandparent: IsGrandparent(x, y) is True if y is x’s grandparent."
LOG NS,0.48009367681498827,"• IsUncle: IsUncle(x, y) is True if y is x’s uncle."
LOG NS,0.4812646370023419,"• IsMGUncle: IsMGUncle(x, y) is True if y is x’s maternal great uncle."
LOG NS,0.48243559718969553,"B.2
HYPER-PARAMETER SETTINGS"
LOG NS,0.48360655737704916,"For simplicity, we set discount factor γ = 1 in all experiments. The Tmax depends on the depth
of the Reasoner. In single-task setting, we use the same hyper-parameter settings as in the original
papers for MemNN (Sukhbaatar et al., 2015), @ILP (Evans & Grefenstette, 2018), NLM (Dong
et al., 2019), and DLM (Matthieu et al., 2021). In multi-task setting, the probability distribution of
sampling a task for NLM-MTR is [0.09, 0.11, 0.11, 0.09, 0.09, 0.135, 0.135, 0.24] that is correspond-
ing to the task list of [1-Outdegree, AdjacentToRed, 4-Connectivity, HasFather,
HasSister, IsGrandparent, IsUncle, IsMGUncle]. That distribution for DLM-MTR is
[0.1, 0.12, 0.12, 0.1, 0.1, 0.13, 0.13, 0.2]. The details of hyper-parameters in NLM-MTR and DML-
MTR can be found in Table 4. For all MLPs inside NLM-MTR, we keep the same settings as NLM.
Similarly, the “Internal Logic”, initial temperature and scale of the Gumbel distribution, and the
dropout probability in DLM-MTR are also kept the same settings as those in DLM. Besides, the
probability distribution of sampling a task for PRIMA is [0.1, 0.12, 0.12, 0.1, 0.1, 0.13, 0.13, 0.2].
Regarding the hyper-parameters of PRIMA, details can be found in Table 5. We also show the
hyper-parameters of PRISA-REINFORCE, PRISA-PPO, PRISA-MuZero in Table 5."
LOG NS,0.4847775175644028,"We note that the problem size for training is always 10 for graph tasks and 20 for family tree tasks in
single-task and multi-task settings."
LOG NS,0.4859484777517564,"B.3
COMPUTING INFRASTRUCTURE"
LOG NS,0.48711943793911006,"We conducted our experiments on a CPU server where the CPU is “Intel(R) Xeon(R) Silver 4114
CPU” with 40 cores and 64 GB memory in total."
LOG NS,0.4882903981264637,Under review as a conference paper at ICLR 2022
LOG NS,0.4894613583138173,Table 4: Hyper-parameter settings for NLM-MTR and DML-MTR.
LOG NS,0.49063231850117095,"NLM-MTR
DML-MTR"
LOG NS,0.4918032786885246,"learning rate
0.005
0.005
epochs
50
200
epoch size
8000
2000
batch size (train)
4
4
breadth
3
3
depth
4
9"
LOG NS,0.4929742388758782,"Table 5: Hyper-parameter settings for PRISA and PRIMA. “lr-reasoner” refers to the learning rate
for the reasoner, “lr-policy” denotes the learning rate for the policy network, “lr-value” denotes
the learning rate for the value network, “residual” refers to the residual connection in the reasoner,
“NumWarmups” refers to the number of warm-ups before starting the training, “NumRollouts” refers"
LOG NS,0.49414519906323184,"to the number of roll-outs in MCTS, “RwdDecay” refers to the constant exponential decay applied
on the reward, “c1 and c2” refers to the same constants in PUCT formula in Section A.1, “RBsize”
refers to the replay buffer size that is used to count the number of stored trajectories, “BatchSize”
refers to the batch size for training, “TrainingSteps” refers to the number of training steps. For
PRISA-REINFORCE, PRISA-PPO, PRISA-MuZero, the “TrainingSteps” denotes the training steps
for each task, while that of PRIMA represents the training steps for all 8 tasks."
LOG NS,0.4953161592505855,"PRISA-
PRISA-
PRISA-
PRIMA
REINFORCE
PPO
MuZero"
LOG NS,0.4964871194379391,"lr-reasoner
0.005
0.005
0.005
0.004
lr-policy
0.085
0.085
0.075
0.075
lr-value
-
0.15
0.075
0.075
breadth
3
3
3
3
depth
4
4
4
4
residual
False
False
False
False
NumWarmups
-
-
200
200
NumRollouts
-
-
1200
1200
RwdDecay
5
5
5
5
c1
-
-
30
30
c2
-
-
19652
19652
RBsize
16
32
400
400
BatchSize
16
16
16
16
TrainingSteps
7 ⇥104
7 ⇥104
7 ⇥104
39 ⇥104"
LOG NS,0.49765807962529274,"Table 6: Additional results: The performance of PRIMA w.r.t different numbers of intermediate
predicates. “# pred”: the number of intermediate predicates."
LOG NS,0.49882903981264637,testing accuracy
LOG NS,0.5,"Family Tree
HasFather
HasSister
IsGrandparent
IsUncle
IsMGUncle
Graph
AdjacentToRed
4-Connectivity
1-OutDegree"
LOG NS,0.5011709601873536,# pred = 2
LOG NS,0.5023419203747073,"m=20
61.78
54.97
96.46
97.19
99.67
m=10
42.62
89.28
81.88
m=100
87.5
31.2
97.7
96.5
98.4
m=50
11.1
96.6
96.5
PSS
0
0
0
0
0
PSS
0
0
0"
LOG NS,0.5035128805620609,# pred = 4
LOG NS,0.5046838407494145,"m=20
100
100
100
97.2
99.7
m=10
67.3
93.4
100
m=100
100
100
100
96.6
98.4
m=50
91.7
97.8
100
PSS
100
100
100
0
0
PSS
0
0
100"
LOG NS,0.5058548009367682,# pred = 6
LOG NS,0.5070257611241218,"m=20
100
100
100
100
100
m=10
99.7
100
100
m=100
100
100
100
100
100
m=50
100
100
100
PSS
100
100
100
100
90
PSS
0
100
100"
LOG NS,0.5081967213114754,# pred = 8
LOG NS,0.509367681498829,"m=20
100
100
100
100
100
m=10
100
100
100
m=100
100
100
100
100
100
m=50
100
100
100
PSS
100
100
100
100
90
PSS
90
100
100"
LOG NS,0.5105386416861827,Under review as a conference paper at ICLR 2022
LOG NS,0.5117096018735363,"Figure 6: The reasoning costs (in FLOPs) of PRIMA and three variants of PRISA at the inference stage (1
M=1⇥106, 1 G=1⇥109)."
LOG NS,0.5128805620608899,"C
ADDITIONAL EXPERIMENTS"
LOG NS,0.5140515222482436,"C.1
REASONING COSTS OF PRISA"
LOG NS,0.5152224824355972,"To demonstrate the improved reasoning efﬁciency in PRISA at the inference stage, three variants of
PRISA (e.g., PRISA-REINFORCE, PRISA-PPO, and PRISA-MuZero) are compared with PRIMA
in terms of FLOPs. As shown in Fig. 6, three variants of PRISA generally have lower reasoning
complexity than PRIMA. The difference in FLOPs between PRIMA and PRISA-MuZero is primarily
from the difference between multi-tasking setting and single-task setting."
LOG NS,0.5163934426229508,"C.2
PREDICATE DIMENSION ANALYSIS"
LOG NS,0.5175644028103045,"Since the Reasoner only realizes a partial set of Horn clauses of FOL, the number of intermediate
predicates will greatly determine the expressive power of the model. The performance of PRIMA
degrades gracefully when the number of intermediate predicates decreases. This is shown in the
additional experiments of examining the limiting performance of our PRIMA, where we examine
the limiting performance of our PRIMA when the number of intermediate predicates at each layer
decreases. The results in Table 6 show that the performance of PRIMA degrades gracefully when the
number of intermediate predicates decreases; for example, it still performs reasonably well on most
tasks even when the number of predicates becomes 4."
LOG NS,0.5187353629976581,"C.3
ANALYSIS OF MULTI-TASKING CAPABILITIES"
LOG NS,0.5199063231850117,Figure 7: The quantitative evaluation of ops reuse between different Tasks in PRIMA.
LOG NS,0.5210772833723654,"Why/how do different tasks share blocks (of ops) as in Fig. 7?
To quantitatively measure how
different tasks reuses the “skills”, we show in Fig. 7, which is a heat-map showing the probability
of overlapping about Ops reuse across different tasks, where red color denotes a high-level of
Ops overlapping. Ops reuse factor is deﬁned as min(Pa(Ij), Pb(Ij)) where Pa(Ij) refers to the
probability of ops Ij(j 2 {0, . . . , 9}) being active for task a and Pb(Ij) denotes the probability of
Ij being active for task b. From Fig. 7, it can be inferred that some tasks share skills more than others.
For example, in most of the actions, I8 is shared across different tasks."
LOG NS,0.522248243559719,Under review as a conference paper at ICLR 2022
LOG NS,0.5234192037470726,"D
ADDITIONAL RELATED WORK"
LOG NS,0.5245901639344263,This section provides complementary contents to Sec. 5.
LOG NS,0.5257611241217799,"Model-based symbolic planning
Another large body of related work is symbolic planning
(SP) (Van Harmelen et al., 2008; Hanheide et al., 2015; Chen et al., 2016; Khandelwal et al., 2017).
In SP approaches, a planning agent carries prior symbolic knowledge of objects, properties, and how
they are changed by executing actions in the dynamic system, represented in a formal, logic-based
language such as PDDL (McDermott et al., 1998) or an action language (Gelfond & Lifschitz, 1998)
that relates to logic programming under answer set semantics (answer set programming) (Lifschitz,
2008). The agent utilizes a symbolic planner, such as a PDDL planner FASTDOWNWARD (Helmert,
2006) or an answer set solver CLINGO (Gebser et al., 2012) to generate a sequence of actions based
on its symbolic knowledge, executes the actions to achieve its goal. Compared with RL approaches,
an SP agent does not require a large number of trial-and-error to behave reasonably well, yet requires
predeﬁned symbolic knowledge as the model prior. It should be noted that it remains questionable if
PDDL-based methods can be applied directly to our problem, as there is no explicitly predeﬁned prior
knowledge under our problem setting, which is required by PDDL-based approaches. In contrast, we
address the learning-to-reason problem via a complete data-driven deep reinforcement approach."
LOG NS,0.5269320843091335,"Multi-task RL
Multi-task learning can help boost the performance of reinforcement learning, lead-
ing to multi-task reinforcement learning (Vithayathil Varghese & Mahmoud, 2020; Zhu et al., 2020).
Some research (Wilson et al., 2007; Li et al., 2009; Lazaric & Ghavamzadeh, 2010; Calandriello
et al., 2014; Andreas et al., 2017; Deshmukh et al., 2017; Saxe et al., 2017; Bräm et al., 2019; Vuong
et al., 2019; Igl et al., 2020) has adapted the ideas behind multi-task learning to RL. For example, in
(Bräm et al., 2019), a multi-task deep RL model based on attention can group tasks into sub-networks
with state-level granularity. The idea of compression and distillation has been incorporated into
multi-task RL as in (Parisotto et al., 2016; Rusu et al., 2016; Omidshaﬁei et al., 2017; Teh et al.,
2017). For example, in (Parisotto et al., 2016), the proposed actor-mimic method combines deep
reinforcement learning with model-compression techniques to train a policy network that can learn
to act for multiple tasks. Other research in multi-task RL focuses on online and distributed settings
(Bsat et al., 2017; Sharma & Ravindran, 2017; Sharma et al., 2018; Espeholt et al., 2018; Tutunov
et al., 2018; Lin et al., 2019)."
LOG NS,0.5281030444964872,"RL for logic reasoning
Relational RL integrates RL with statistical relational learning and connects
RL with classical AI for knowledge representation and reasoning. Most prominently, Džeroski et al.
(2001) originally proposed relational RL, Tadepalli et al. (2004) surveyed relational RL, Guestrin
et al. (2003) introduced relational MDPs, and Diuk et al. (2008) introduced objected-oriented MDPs
(OO-MDPs). More recently, Battaglia et al. (2018) proposed to incorporate relational inductive
bias, Zambaldi et al. (2018) proposed deep relational RL, Keramati et al. (2018) proposed strategic
object-oriented RL, and some researchers have also adopted deep learning approaches for dealing
with relations and/or reasoning, for example (Battaglia et al., 2016; Chen et al., 2018; Santoro et al.,
2017; Santurkar et al., 2018; Palm et al., 2017; Xia et al., 2018; Yi et al., 2018)."
LOG NS,0.5292740046838408,"Statistical relational reasoning
Statistical relational reasoning (SRL) (Koller et al., 2007) often
provides a better understanding of domains and predictive accuracy, but with more complex learning
and inference processes. Unlike ILP, SRL takes a statistical and probabilistic learning perspective
and extends the probabilistic formalism with relational aspects. Examples of SRL include text
classiﬁcation (Ganiz et al., 2010), recommendation systems (Cao, 2016), and wireless networks
(Yoshida et al., 2020). SRL can be divided into two research branches: probabilistic relational models
(PRMs) (Koller, 1999) and probabilistic logic models (PLMs) (Chen et al., 2008). PRMs start from
probabilistic graphical models and then extend to relational aspects, while PLMs start from ILP and
extend to probabilistic semantics. A related research area is graph relational reasoning, such as using
graph neural networks (Zhou et al., 2020) to conduct reasoning. A discussion of this approach is
beyond the scope of this paper, but we refer interested readers to (Chakrabarti & Faloutsos, 2006;
Cook & Holder, 2006; Zhang et al., 2019) for a comprehensive review."
LOG NS,0.5304449648711944,"Comparisons with NLM in details
The Reasoner shares the same functionality with NLM, in
terms of approximating the meta-rules (BooleanLogic, Expansion, and Reduction). However,"
LOG NS,0.531615925058548,Under review as a conference paper at ICLR 2022
LOG NS,0.5327868852459017,"our Reasoner has major differences from NLM. First, the targets are different. Our Reasoner targets
the trade-off between capability and efﬁciency. Compared with NLM, our Reasoner can greatly
improve the efﬁciency by using a Planner module. Second, our Reasoner has more explicitness in
the reasoning path. Let [Or−1"
LOG NS,0.5339578454332553,"i−1 , Or"
LOG NS,0.5351288056206089,"i−1, Or+1"
LOG NS,0.5362997658079626,"i−1 ] denote the output predicates (in tensor representation)
from the previous layer, and Concate(·) denotes the concatenation operation. NLM performs the
intra-group computation as Or"
LOG NS,0.5374707259953162,i = σ (MLP (Permute (Zr
LOG NS,0.5386416861826698,i ) ; ✓r
LOG NS,0.5398126463700235,"i )) ,"
LOG NS,0.5409836065573771,where Or
LOG NS,0.5421545667447307,"i is the output predicate, Zr"
LOG NS,0.5433255269320844,i = Concat '
LOG NS,0.544496487119438,"Expand ' Or−1 i−1 ( , Or"
LOG NS,0.5456674473067916,"i−1, Reduce ' Or+1 i−1 (("
LOG NS,0.5468384074941453,", σ is the
sigmoid nonlinearity and ✓r"
LOG NS,0.5480093676814989,"i denotes learnable parameters. On the contrary, in the Reasoner module
of our PRISA/PRIMA, the implementation is Or i = σ "
LOG NS,0.5491803278688525,MLP(Permute '
LOG NS,0.550351288056206,Expand(Or−1 i−1 ) ( ; ✓r
LOG NS,0.5515222482435597,i ) + MLP(Permute ' Or
LOG NS,0.5526932084309133,i−1); ✓r i (
LOG NS,0.5538641686182669,+MLP(Permute '
LOG NS,0.5550351288056206,Reduce(Or+1
LOG NS,0.5562060889929742,"i−1 )); ✓r i ( ! ,"
LOG NS,0.5573770491803278,"which allows for the neural implementations of BooleanLogic, Expansion, and Reduction (at
the same arity group) to be executed independently to some extent and reduce the unnecessary
computations."
LOG NS,0.5585480093676815,Under review as a conference paper at ICLR 2022
LOG NS,0.5597189695550351,"PRIMA: PLANNER-REASONER INSIDE A MULTI-TASK
REASONING AGENT"
LOG NS,0.5608899297423887,"Anonymous authors
Paper under double-blind review"
ABSTRACT,0.5620608899297423,ABSTRACT
ABSTRACT,0.563231850117096,"We consider the problem of multi-task reasoning (MTR), where an agent can
solve multiple tasks via (ﬁrst-order) logic reasoning. This capability is essential
for human-like intelligence due to its strong generalizability and simplicity for
handling multiple tasks. However, a major challenge in developing effective
MTR is the intrinsic conﬂict between reasoning capability and efﬁciency. An
MTR-capable agent must master a large set of “skills” to tackle diverse tasks,
but executing a particular task at the inference stage requires only a small subset
of immediately relevant skills. How can we maintain broad reasoning capability
but efﬁcient speciﬁc-task performance? To address this problem, we propose a
Planner-Reasoner framework capable of state-of-the-art MTR capability and high
efﬁciency. The Reasoner models shareable (ﬁrst-order) logic deduction rules, from
which the Planner selects a subset to compose into efﬁcient reasoning paths. The
entire model is trained in an end-to-end manner using deep reinforcement learning,
and experimental studies over a variety of domains validate its effectiveness."
INTRODUCTION,0.5644028103044496,"1
INTRODUCTION"
INTRODUCTION,0.5655737704918032,"Multi-task learning (MTL) (Zhang & Yang, 2021; Zhou et al., 2011) demonstrates superior sample
complexity and generalizability compared with the conventional “one model per task” style to solve
multiple tasks. Recent research has additionally leveraged the great success of deep learning (LeCun
et al., 2015) to empower learning deep multi-task models (Zhang & Yang, 2021; Crawshaw, 2020).
Deep MTL models either learn a common multi-task feature representation by sharing several bottom
layers of deep neural networks (Zhang et al., 2014; Liu et al., 2015; Zhang et al., 2015a; Mrksic
et al., 2015; Li et al., 2015), or learn task-invariant and task-speciﬁc neural modules (Shinohara,
2016; Liu et al., 2017) via generative adversarial networks (Goodfellow et al., 2014). Although
MTL is successful in many applications, a major challenge is the often impractically large MTL
models. Although still smaller than piling up all models across different tasks, existing MTL
models are signiﬁcantly larger than a single model for tackling a speciﬁc task. This results from the
intrinsic conﬂict underlying all MTL algorithms: balancing across-task generalization capability to
perform different tasks with single-task efﬁciency in executing a speciﬁc task. On one hand, good
generalization ability requires an MTL agent to be equipped with a large set of skills that can be
combined to solve many different tasks. On the other hand, solving one particular task does not
require all these skills. Instead, the agent needs to compose only a (small) subset of these skills into
an efﬁcient solution for a speciﬁc task. This conﬂict often hobbles existing MTL approaches."
INTRODUCTION,0.5667447306791569,"This paper focuses on multi-task reasoning (MTR), a subarea of MTL that uses logic reasoning to
solve multiple tasks. MTR is ubiquitous in human reasoning, where humans construct different
reasoning paths for multiple tasks from the same set of reasoning skills. Conventional deep learning,
although capable of strong expressive power, falls short in reasoning capabilities (Bengio, 2019).
Considerable research has been devoted to endowing deep learning with logic reasoning abilities, the
results of which include Deep Neural Reasoning (Jaeger, 2016), Neural Logic Reasoning (Besold
et al., 2017; Bader et al., 2004; Bader & Hitzler, 2005), Neural Logic Machines (Dong et al., 2019),
and other approaches (Besold et al., 2017; Bader et al., 2004; Bader & Hitzler, 2005). However,
these approaches consider only single-task reasoning rather than a multi-task setting, and applying
existing MTL approaches to learning these neural reasoning models leads to the same conﬂict between
across-task generalization capability and single-task efﬁciency."
INTRODUCTION,0.5679156908665105,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.5690866510538641,Figure 1: An example from the AdjacentToRed task and its formulation as a logical reasoning problem.
INTRODUCTION,0.5702576112412178,"To strike a balance between reasoning capability and efﬁciency in MTR, we develop a Planner-
Reasoner architecture Inside a Multi-task reasoning Agent (PRIMA) (Section 3), wherein the
reasoner deﬁnes a set of neural logic operators for modeling reusable reasoning meta-rules (“skills”)
across tasks (Section 3.2). When deﬁning the logic operators, we focus on ﬁrst-order logic because
of its simplicity and wide applicability to many reasoning problems, such as automated theorem
proving (Fitting, 2012; Gallier, 2015) and knowledge-based systems (Van Harmelen et al., 2008). A
separate planner module activates only a small subset of the meta-rules necessary for a given task
and composes them into a deduction process (Section 3.3). Thus, our planner-reasoner architecture
features the dual capabilities of composing and gpruning a logic deduction process, achieving a
graceful capability-efﬁciency trade-off in MTR. The model architecture is trained in an end-to-
end manner using deep reinforcement learning (Section 4), and experimental results on several
benchmarks demonstrate that this framework leads to a state-of-the-art balance between capability
and efﬁcency (Section 5). We discuss related works in Section 6, and conclude our paper in Section 7."
PROBLEM FORMULATION,0.5714285714285714,"2
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.572599531615925,"Logic reasoning
We begin with a brief introduction of the logic reasoning problem. Speciﬁcally,
we consider a special variant of the First-Order Logic (FOL) system, which only consists of individual
variables, constants and up to r-ary predicate variables. That is, we do not consider functions that
map individual variables/constants into terms. An r-ary predicate p(x1, . . . , xr) can be considered as
a relation between r constants, which takes the value of True or False. An atom p(x1, · · · , xr) is
an r-ary predicate with its arguments x1, · · · , xr being either variables or constants. A well-deﬁned
formula in our FOL system is a logical expression that is composed from atoms, logical connectives
(e.g., negation ¬, conjunction ^, disjunction _, implication  ), and possibly existential 9 and
universal 8 quantiﬁers according to certain formation rules (see Andrews (2002) for the details). In
particular, the quantiﬁers 9 and 8 are only allowed to be applied to individual variables in FOL. In
Fig. 1, we give an example from the AdjacentToRed task (Graves et al., 2016) and show how
it could be formulated as a logical reasoning problem. Speciﬁcally, we are given a random graph
along with the properties (i.e., the color) of the nodes and the relations (i.e., connectivity) between
nodes. In our context, each node i in the graph is a constant and an individual variable x takes
values in the set of constants {1, . . . , 5}. The properties of nodes and the relations between nodes
are modeled as the unary predicate IsRed(x) (5 ⇥1 vector) and the binary predicate HasEdge(x, y)
(5 ⇥5 matrix), respectively. The objective of logical reasoning is to deduce the value of the unary
predicate AdjacentToRed(x) (i.e., whether a node x has a neighbor of red color) from the base
predicates IsRed(x) and HasEdge(x, y) (see Fig. 1 for an example of the deduction process).
Multi-task reasoning
Next, we introduce the deﬁnition of MTR. With a slight abuse of notations,
let {p(x1, . . . , xr) : r 2 [1, n]} be the set of input predicates sampled from any of the k different
reasoning tasks, where x1, . . . , xr are the individual variables and n is the maximum arity. A
multi-task reasoning model takes p(x1, . . . , xr) as its input and seeks to predict the corresponding
ground-truth output predicates q(x1, . . . , xr). The aim is to learn multiple reasoning tasks jointly in
a single model so that the reasoning skills in a task can be leveraged by other tasks to improve the
general performance of all tasks at hand."
PROBLEM FORMULATION,0.5737704918032787,"3
PRIMA: PLANNER-REASONER FOR MULTI-TASK REASONING"
PROBLEM FORMULATION,0.5749414519906323,"In this section, we develop our PRIMA framework, which is a novel neural-logic model architecture
for multi-task reasoning. We begin with an introduction to the overall architecture (Section 3.1) along
with its key design insights, and then proceed to discuss its two modules in Sections 3.2–3.3."
PROBLEM FORMULATION,0.5761124121779859,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.5772833723653396,"Figure 2: Left (A): The Planner-Reasoner Architecture with an example solution. Right (B): The overall
(end-to-end) training process of the model by deep reinforcement learning. “Perm” denotes Permute operator."
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5784543325526932,"3.1
THE OVERALL MODEL ARCHITECTURE OF PRIMA"
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5796252927400468,"Logic reasoning typically seeks to chain an appropriate sequence of logic operators to reach desirable
logical consequences from premises. There are two types of chaining to conduct logic reasoning: (i)
forward-chaining and (ii) backward-chaining. The forward-chaining strategy recursively deduces
all the possible conclusions based on all the available facts and deduction rules until it reaches the
answer. In contrast, backward-chaining starts from the desired conclusion (i.e., the goal) and then
works backward recursively to validate conditions through available facts. This paper adopts the
forward-chaining strategy since the goal is unavailable here beforehand. Note that, in the MTR
setting, it generally requires a large set of “skills” (logic operators) to tackle diverse tasks. Therefore,
it is unrealistic to recursively generate all possible conclusions by chaining all the available deduction
rules as in the vanilla forward-chaining strategy. Therefore, pruning the forward-chaining process is
essential to improve the reasoning efﬁciency in the MTR setting."
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5807962529274004,"We note that reasoning via pruned forward-chaining could be implemented by: (i) constructing the
elementary logic operators and (ii) selecting and chaining a subset of these logic operators together.
Accordingly, we develop a novel neural model architecture, named Planner-Reasoner (see Figure
2A), to jointly accomplish these two missions with two interacting neural modules. Speciﬁcally,
a “Reasoner” deﬁnes a set of neural-logic operators (representing learnable Horn clauses (Horn,
1951)). Meanwhile, a “Planner” learns to select and chain a small subset of these logic operators"
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5819672131147541,"together towards the correct solution (e.g., the orange-colored path in Figure 2A). The inputs to the
Planner-Reasoner model are the base predicates {p, p(x), p(x, y), . . .} that describe the premises. In
particular, in the MTR setting, the nullary predicate p characterizes the current task to be solved.
This design allows the Planner to condition its decisions on the input task type. We feed the outputs
(i.e., the conclusion predicates) into multiple output heads, one for each task (just as in standard
multitask learning), to generate predictions. The entire model is trained in a complete data-driven
end-to-end approach via deep reinforcement learning (Figure 2B and Section 4): it trains the Planner
to dynamically prune the forward-chaining process while jointly learning the logic operators in the
Reasoner, leading to a state-of-the-art balance between MTR capability and efﬁciency (Section 5)."
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5831381733021077,"3.2
REASONER: TRANSFORMING LOGIC RULES INTO NEURAL OPERATORS"
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5843091334894613,"The Reasoner module conducts logical deduction using a set of neural operators constructed from
ﬁrst-order logic rules (more speciﬁcally, a set of “learnable” Horn clauses). Its architecture is inspired
by NLM (Dong et al., 2019) (details about the difference can be found in Appendix D). Three logic
rules are considered as essential meta-rules: BooleanLogic, Expansion, and Reduction."
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.585480093676815,"BooleanLogic :
expression(x1, x2, · · · , xr) ! ˆp(x1, x2, · · · , xr),"
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5866510538641686,"where expression is composed of a combination of Boolean operations (AND, OR, and NOT)
and ˆp is the output predicate. For a given r-ary predicate and a given permutation  2 Sn, we
deﬁne p (x1, · · · , xr) = p(x (1), · · · , x (r)) where Sn is the set of all possible permutations as
the arguments to an input predicate. The corresponding neural implementation of BooleanLogic is"
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5878220140515222,Under review as a conference paper at ICLR 2022
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5889929742388759,"σ (MLP (p (x1, · · · , xr)) ; ✓), where σ is the sigmoid activation function, MLP refers to a multi-
layer perceptron, a Permute(·) neural operator transforms input predicates to p (x1, · · · , xr), and
✓is the learnable parameter within the model. This is similar to the implicit Horn clause with the
universal quantiﬁer(8), e.g., p1(x) ^ p2(x) ! ˆp(x) implicitly denoting 8x p1(x) ^ p2(x) ! ˆp(x).
The class of neural operators can be viewed as “learnable” Horn clauses."
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5901639344262295,"Expansion, and Reduction are two types of meta-rules for quantiﬁcation that bridge predicates of
different arities with logic quantiﬁers (8 and 9). Expansion introduces a new and distinct variable
xr+1 for a set of r-ary predicates with the universal quantiﬁer(8). For this reason, Expansion creates
a new predicate q from p."
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5913348946135831,"Expansion :
p(x1, x2, · · · , xr) ! 8xr+1, q(x1, x2, · · · , xr, xr+1),"
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5925058548009368,where xr+1 /2 {xi}r
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5936768149882904,"i=1. The corresponding neural implementation of Expansion, denoted by
Expand(·), expands the r-ary predicates into the (r + 1)-ary predicates by repeating the r-ary
predicates and stacking them in a new dimension. Conversely, Reduction removes the variable xr+1
in a set of (r + 1)-ary predicates via the quantiﬁers of 8 or 9."
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.594847775175644,"Reduction :
8xr+1 p(x1, x2, · · · , xr, xr+1) ! q(x1, x2, · · · , xr), or
9xr+1 p(x1, x2, · · · , xr, xr+1) ! q(x1, x2, · · · , xr)."
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5960187353629977,"The corresponding neural implementation of Reduction, denoted by Reduce(·), reducing the (r+1)-
ary predicates into the r-ary predicates by taking the minimum (resp. maximum) along the dimension
of xr+1 due to the universal quantiﬁer 8 (resp. existential quantiﬁer 9)."
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5971896955503513,"3.3
PLANNER: ACTIVATING AND FORWARD-CHAINING LEARNABLE HORN CLAUSES"
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5983606557377049,"The Planner is our key module to address the capability-efﬁciency tradeoff in the MTR problem; it
is responsible for activating the neural operators in the Reasoner and chaining them into reasoning
paths. Existing learning-to-reason approaches, which are often based on inductive logic programming
(ILP) (Cropper et al., 2020; Cropper & Dumancic, 2020; Muggleton & De Raedt, 1994) and the
correspondingly neural-ILP methods (Dong et al., 2019; Shi et al., 2020). Conventional ILP methods
suffer from several drawbacks, such as heavy reliance on human-crafted templates and sensitivity
to noise. On the other hand, neural-ILP methods (Dong et al., 2019; Shi et al., 2020), leveraging
the strength of deep learning and ILP, such as the Neural Logic Machine (NLM) (Dong et al.,
2019), lack explicitness in the learning process of searching for reasoning paths. Let us take the
learning process of NLM for example, which follows an intuitive two-step procedure. It ﬁrst fully
connects all the neural blocks and then searches all possible connections (corresponding to all possible
predicate candidates) exhaustively to identify the desired reasoning path (corresponding to the desired
predicate)."
THE OVERALL MODEL ARCHITECTURE OF PRIMA,0.5995316159250585,"By using our proposed Planner module, we can strike a better capability-efﬁciency tradeoff. Rather
than conducting an exhaustive search over all possible predicate candidates as in NLM, the Planner
prunes all the unnecessary operators and identiﬁes an essential reasoning path with low complexity
for a given problem. Consider the following example. As shown in Fig. 1 and Fig. 2A (the
highlighted orange-colored blocks), at each reasoning step, the Planner takes the input predicates
and determines which neural operators should be activated. The decision is represented as a binary
vector — [I0 . . . I6] (termed operator footprint) — that corresponds to the neural operators of
Expand, Reduce and DirectInput1 at different arity groups. By chaining these sequential decisions,
a sequence of operator footprints is formulated as a reasoning path, as the highlighted orange-colored
path in Fig. 2A. Generally, the neural operators deﬁned in the Reasoner can also be viewed as
(learnable) Horn Clauses (Horn, 1951; Chandra & Harel, 1985), and the Planner forward-chains them
into a reasoning path."
LEARNING-TO-REASON VIA REINFORCEMENT LEARNING,0.6007025761124122,"4
LEARNING-TO-REASON VIA REINFORCEMENT LEARNING"
LEARNING-TO-REASON VIA REINFORCEMENT LEARNING,0.6018735362997658,"In our problem, the decision variables of the Planner are binary indicators of whether a neural operator
module should be activated. Readers familiar with Markov decision processes (MDPs) (Puterman,"
LEARNING-TO-REASON VIA REINFORCEMENT LEARNING,0.6030444964871194,"1DirectInput is an identity mapping of the inputs where its following operators of Permute, MLP and
sigmoid nonlinearity can directly approximate the BooleanLogic."
LEARNING-TO-REASON VIA REINFORCEMENT LEARNING,0.6042154566744731,Under review as a conference paper at ICLR 2022
LEARNING-TO-REASON VIA REINFORCEMENT LEARNING,0.6053864168618267,"Figure 3: The architecture of the Base Planner and its enhanced version. Left: The Base Planner is based on a
one-layer fully-activated Reasoner followed by max-pooling operations to predict an indicator of whether an op
should be activated. Right: The Enhanced Planner uses MCTS to further boost the performance."
LEARNING-TO-REASON VIA REINFORCEMENT LEARNING,0.6065573770491803,"2014) might notice that the reasoning path of our model in Fig. 2A resembles a temporal rollout in
MDP formulation (Sutton & Barto, 2018). Therefore, we frame learning-to-reason as a sequential
decision-making problem and adopt off-the-shelf reinforcement learning (RL) algorithms."
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.607728337236534,"4.1
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6088992974238876,"An MDP is deﬁned as the tuple (S, A, P a"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6100702576112412,"ss0, R, γ), where S and A are ﬁnite sets of states and actions,
the transition kernel P a"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6112412177985949,"ss0 speciﬁes the probability of transition from state s 2 S to state s0 2 S by
taking action a 2 A, R(s, a) : S ⇥A ! R is the reward function, and 0 γ 1 is a discount factor.
A stationary policy ⇡: S ⇥A ! [0, 1] is a probabilistic mapping from states to actions. The primary
objective of an RL algorithm is to identify a near-optimal policy ⇡⇤that satisﬁes"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6124121779859485,⇡⇤:= arg max ⇡ ⇢
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6135831381733021,J(⇡) := E
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6147540983606558,"Tmax−1
X t=0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6159250585480094,"γtr(st, at ⇠⇡) $% ,"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.617096018735363,"where Tmax is a positive integer denoting the horizon length — that is, the maximum length of a
rollout. The resemblance between Fig. 2 and an MDP formulation is relatively intuitive as summarized
in Table 1. At the t-th time step, the state st corresponds to the set of predicates, st = [O0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6182669789227166,"t , O1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6194379391100703,"t , O2"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6206088992974239,"t ],
with the superscript denoting the corresponding arity. The action at = [I0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6217798594847775,"t , I1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6229508196721312,"t , . . . , IK−1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6241217798594848,"t
] (e.g.,
a0 = [000011]) is a binary vector that indicates the activated neural operators (i.e., the operator
footprint), where K is the number of operators per layer. The reward rt is deﬁned to be rt := ( −PK−1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6252927400468384,i=0 Ii
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6264637002341921,"t,
if t < Tmax
Accuracy,
t = Tmax
.
(1)"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6276346604215457,"That is, the terminal reward is set to be the reasoning accuracy at the end of the reasoning path (see
Appendix A.2 for its deﬁnition), and the intermediate reward at each step is chosen to be the negated
number of activated operators (which penalizes the cost of performing the current reasoning step).
The transition kernel P a"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6288056206088993,"ss0 corresponds to the function modeled by one Reasoner layer; each Reasoner
layer will take the state (predicates) st−1 = [O0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.629976580796253,"t−1, O1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6311475409836066,"t−1, O2"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6323185011709602,"t−1, ] and the action (operator footprint)
at = [I0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6334894613583139,"t , I1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6346604215456675,"t , . . . , IK−1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6358313817330211,"t
] as its input and then generate the next state (predicates) st = [O0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6370023419203747,"t , O1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6381733021077284,"t , O2"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.639344262295082,"t , ].
This also implies that the Reasoner layer deﬁnes a deterministic transition kernel, i.e., given st−1 and
at the next state st is determined."
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6405152224824356,Table 1: The identiﬁcation between the concepts of PRIMA and that of RL at the t-th time step.
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6416861826697893,"RL
State st
Action at
Reward rt
Transition
Kernel P a"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6428571428571429,"ss0
Policy
Rollout"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6440281030444965,"PRIMA
Predicates of different arities: [O0"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6451990632318502,"t , O1"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6463700234192038,"t , O2 t ]t"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6475409836065574,Operator footprint: [I0
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6487119437939111,t . . . IK−1
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6498829039812647,"t
]
Eq. (1)
One layer of"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6510538641686182,"Reasoner
Planner
Reasoning path"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6522248243559718,Under review as a conference paper at ICLR 2022
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6533957845433255,"4.2
POLICY NETWORK: MODELING OF PLANNER"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6545667447306791,"The Planner module is embodied in the policy network. As shown in Fig. 3A, the base planner
is a separate module that has the same architecture of 1-layer (fully-activated) Reasoner followed
by a max-pooling layer. This architecture enables the reduction of input predicates to the speciﬁc
indicators by reﬂecting whether the operations at the corresponding position of one layer of Reasoner
are active or inactive. Further, we can also leverage Monte-Carlo Tree Search (MCTS) (Browne
et al., 2012; Munos, 2014) to boost the performance, which leads to an Enhanced Planner (Fig. 3B).
An MCTS algorithm such as the Upper Conﬁdence Bound for Trees (UCT) method (Kocsis et al.,
2006), is a model-based RL algorithm that plans the best action at each time step (Browne et al.,
2012) by constructing a search tree, with states as nodes and actions as edges. The Enhanced Planner
uses MCTS to exploit partial knowledge of the problem structure (i.e., the deterministic transition
kernel P a"
AN MDP FORMULATION OF THE LEARNING-TO-REASON PROBLEM,0.6557377049180327,"ss0 deﬁned by the Reasoner layer) and construct a search tree to help identify the best actions
(which ops to activate). Details of the MCTS algorithms used in the Enhanced Planner can be found
in Appendix A.1."
OVERALL LEARNING FRAMEWORK,0.6569086651053864,"4.3
OVERALL LEARNING FRAMEWORK"
OVERALL LEARNING FRAMEWORK,0.65807962529274,"As illustrated in Fig. 3, we introduce concrete data-driven decision-making methods—that is, RL
approaches (Sutton & Barto, 2018)—to address the learning-to-reason problem. To illustrate this,
we apply the model-free RL method REINFORCE (Williams, 1992) and the model-based method
MuZero (Schrittwieser et al., 2019). Compared with model-free reinforcement learning, model-based
reinforcement learning (MBRL) more effectively handles large search-space problems, such as the
game of Go (Silver et al., 2017b;b;a). MuZero (Schrittwieser et al., 2019), a recently proposed MBRL
approach to integrating planning and learning, has achieved great success with a variety of complex
tasks. Motivated by the success of MuZero, we propose an MBRL approach for neural-symbolic
reasoning. The key insight behind adopting MuZero is that in real applications, we typically have
partial structural knowledge of the transition kernel P a"
OVERALL LEARNING FRAMEWORK,0.6592505854800936,"ss0 and reward function r(s, a). As a result
of the model-based module, testing complexity can be greatly reduced by adopting MCTS, which
leads to a better set of predicates. Of course, MuZero is just one option in the model-based family of
approaches. We leave it as future research to propose and compare other model-based alternatives."
OVERALL LEARNING FRAMEWORK,0.6604215456674473,"The pipeline of the training process is illustrated in Fig.2.B (the right subﬁgure). After loading the
model weights, the reasoning path rollouts are executed by the agent (or model instance), according
to the current policy network. The performed reasoning path rollout is then stored in the replay buffer.
The Planner-Reasoner is trained via rollouts sampled from the replay buffer."
EXPERIMENTAL RESULTS AND ANALYSIS,0.6615925058548009,"5
EXPERIMENTAL RESULTS AND ANALYSIS"
EXPERIMENTAL RESULTS AND ANALYSIS,0.6627634660421545,"In this section, we evaluate the performance of different variants of PRIMA on eight tasks
from the family tree and graph benchmarks (Graves et al., 2016), including 1-Outdegree,
AdjacentToRed,
HasFather,
HasSister,
4-Connectivity,
IsGrandparent,
IsUncle, IsMGUncle. These tasks are widely used benchmarks for inductive logic program-
ming (Krötzsch, 2020; Calautti et al., 2015). Detailed descriptions about those tasks can be found
in Appendix B.1. We evaluate their testing accuracy and reasoning cost (measured in FLOPs:
the number of ﬂoating-point operations executed (Clark et al., 2020)) on these tasks and compare
them to several baselines. Furthermore, detailed case studies are conducted on the reasoning path,
which indicates the operator sharing among different tasks. All the results demonstrate the graceful
capability-efﬁciency tradeoff of PRIMA in multi-task reasoning."
EXPERIMENTAL SETUPS,0.6639344262295082,"5.1
EXPERIMENTAL SETUPS"
EXPERIMENTAL SETUPS,0.6651053864168618,"In the multi-task setting, we ﬁrst randomly sample a task according to a pre-deﬁned probability
distribution (over different tasks), and then generate the data for the selected task using the same
methods as in NLM (Dong et al., 2019). In addition, we also augment the generated data with one-hot
encoding to indicate the selected task, which will be further converted into nullary (background)
input predicates. Also, task-speciﬁc output heads are introduced to generate outputs for different
tasks. These adaptions apply to NLM-MTR, DLM-MTR, and PRIMA. The reasoning accuracy is"
EXPERIMENTAL SETUPS,0.6662763466042154,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETUPS,0.667447306791569,"Table 2: Testing Accuracy and PSS of different variants of PRISA on different tasks. PRISA-MuZero achieves
the best performance on single-task reasoning, which conﬁrms the strength of the MCTS-based Enhanced Planner
and the MuZero learning strategy. “m”: the problem size. “PSS”: Percentage of Successful Seeds."
EXPERIMENTAL SETUPS,0.6686182669789227,testing acc
EXPERIMENTAL SETUPS,0.6697892271662763,Family
EXPERIMENTAL SETUPS,0.6709601873536299,"Tree
HasFather
HasSister
IsGrandparent
IsUncle
IsMGUncle
Graph
AdjacentToRed
4-Connectivity
1-OutDegree"
EXPERIMENTAL SETUPS,0.6721311475409836,Single Task
EXPERIMENTAL SETUPS,0.6733021077283372,"PRISA-
REINFORCE"
EXPERIMENTAL SETUPS,0.6744730679156908,"m=20
62.6
50.7
96.5
97.3
99.8
m=10
47.7
33.5
48.7
m=100
87.8
69.8
2.3
97.7
98.4
m=50
71.6
92.8
97.4
PSS
0
0
0
0
0
PSS
0
0
0"
EXPERIMENTAL SETUPS,0.6756440281030445,PRISA-PPO
EXPERIMENTAL SETUPS,0.6768149882903981,"m=20
71.5
64.3
97.5
98.1
99.6
m=10
62.3
57.8
61.6
m=100
93.2
78.7
98.2
97.3
99.1
m=50
85.5
95.2
96.3
PSS
0
0
0
0
0
PSS
0
0
0"
EXPERIMENTAL SETUPS,0.6779859484777517,PRISA-MuZero
EXPERIMENTAL SETUPS,0.6791569086651054,"m=20
100
100
100
100
100
m=10
100
100
100
m=100
100
100
100
100
100
m=50
100
100
100
PSS
100
100
100
100
100
PSS
90
100
100"
EXPERIMENTAL SETUPS,0.680327868852459,"used as the reward. In the inference (or testing) stage, the Reasoner is combined with the learned
Base Planner to perform the tasks, instead of an enhanced planner (MCTS), to reduce the extra
computation. The problem size for training is always 10 for graph tasks and 20 for family tree tasks
across all settings, regardless of the sizes of testing problems."
OVERALL PERFORMANCE,0.6814988290398126,"5.2
OVERALL PERFORMANCE"
OVERALL PERFORMANCE,0.6826697892271663,"Single-task reasoning capability
Before we proceed to evaluate the MTR capabilities of PRIMA,
we ﬁrst adapt it to the single-task setting by letting the nullary input predicates to be zero, and then
train a separate model for each individual task. We name this single-task version of PRIMA as
PRISA (i.e., Planner-Reasoner Inside a Single Reasoning Agent), and compare it with existing
(single-task) neural-logic reasoning approaches (e.g., NLM (Dong et al., 2019), DLM (Matthieu et al.,
2021), and MemNN (Sukhbaatar et al., 2015)). First, we compare the performance of three different
variants of PRISA (Section 4.3) for single-task reasoning, which learns their planners based on differ-
ent reinforcement learning algorithms; PRISA-REINFORCE uses REINFORCE (Williams, 1992),
PRISA-PPO uses PPO (Schulman et al., 2017), and PRISA-MuZero uses MuZero (Schrittwieser
et al., 2019). We report the test accuracy and the Percentage of Successful Seeds (PSS) in Table 2 to
measure the model’s reasoning capabilities, where the PSS reaches 100% of success rates (Matthieu
et al., 2021). We note that PRISA-MuZero has the same 100% accuracy as NLM (Dong et al., 2019),
DLM (Matthieu et al., 2021), and @ILP (Evans & Grefenstette, 2018) across different tasks, and
outperforms MemNN (Sukhbaatar et al., 2015) (shown in the single-task part in Table 3). But it also
has a higher successful percentage (PSS) in comparison with other methods. The results show that
PRISA-MuZero achieves the best performance on single-task reasoning, which conﬁrms the strength
of the MCTS-based Enhanced Planner (Section 4.3) and the MuZero learning strategy. Therefore, we
will use the Enhanced Planner and MuZero in PRISA and PRIMA in the rest of our empirical studies."
OVERALL PERFORMANCE,0.6838407494145199,"Multi-task reasoning capability
Next, we evaluate the MTR capabilities of PRIMA. To the best
of our knowledge, there is no existing approach that is designed speciﬁcally for MTR. Therefore, we
adapt NLM and DLM into their multi-task versions, named NLM-MTR and DLM-MTR, respectively.
NLM-MTR and DLM-MTR follow the same input and output modiﬁcation as what we did to
upgrade PRISA to PRIMA (Section 3.1). By this, we can examine the contribution of our proposed
Planner-Reasoner architecture for MTR. As shown in Table 3, PRIMA (with MuZero as the Base
Planner) performs well (perfectly) on different reasoning tasks. On the other hand, DLM-MTR
experiences some performance degradation (on AdjacentToRed). This result conﬁrms that our
Planner-Reasoner architecture is more suitable for MTR. We conjecture that the beneﬁt comes from
using a Planner to explicitly select the necessary neural operators for each task, avoiding potential
conﬂicts between different tasks during the learning process."
OVERALL PERFORMANCE,0.6850117096018735,"Experiments are also conducted to test the performance of PRIMA with different problem sizes.
The problem size in training is 10 for all graph tasks and 20 for all family-tree tasks. In testing, we
evaluate the methods on much larger problem sizes (50 for the graph tasks and 100 for the family tree
tasks), which the methods have not seen before. Therefore, the Planner must dynamically activate a
proper set of neural operators to construct a path to solve the new problem. As reported in Fig. 4 and
Tables 2 and 3, PRIMA can achieve the best accuracy and lower ﬂops when the problem sizes for
training and testing are different."
OVERALL PERFORMANCE,0.6861826697892272,"Reasoning efﬁciency
To measure the reasoning efﬁciency of the proposed methods at the inference
stage, PRIMA is compared with NLM, MemNN, and NLM-MTR in terms of FLOPs. As shown in"
OVERALL PERFORMANCE,0.6873536299765808,Under review as a conference paper at ICLR 2022
OVERALL PERFORMANCE,0.6885245901639344,"Table 3: Testing Accuracy and PSS of PRIMA and other baselines on different reasoning tasks. The results of
@ILP, NLM, and DLM are merged in one row due to space constraints and are presented in the same order if the
results are different. Note that PRIMA’s test efﬁciency is superior to NLM-MTR’s as shown in Fig. 4. “m”: the
problem size. “PSS”: Percentage of Successful Seeds. Numbers in red denote < 100%."
OVERALL PERFORMANCE,0.689695550351288,testing accuracy
OVERALL PERFORMANCE,0.6908665105386417,Family
OVERALL PERFORMANCE,0.6920374707259953,"Tree
HasFather
HasSister
IsGrandparent
IsUncle
IsMGUncle
Graph
AdjacentToRed
4-Connectivity
1-OutDegree"
OVERALL PERFORMANCE,0.6932084309133489,Single Task MemNN
OVERALL PERFORMANCE,0.6943793911007026,"m=20
99.9
86.3
96.5
96.3
99.7
m=10
95.2
92.3
99.8
m=100
59.8
59.8
97.7
96
98.4
m=50
93.1
81.3
78.6
PSS
0
0
0
0
0
PSS
0
0
0"
OVERALL PERFORMANCE,0.6955503512880562,@ILP/ NLM/ DLM
OVERALL PERFORMANCE,0.6967213114754098,"m=20
100
100
100
100
100
m=10
100
100
100
m=100
100
100
100
100
100
m=50
100
100
100
PSS
100
100
100
100/ 90/ 100
100/ 20/ 70
PSS
100/ 90/ 90
100
100"
OVERALL PERFORMANCE,0.6978922716627635,Multi-Task
OVERALL PERFORMANCE,0.6990632318501171,NLM-MTR
OVERALL PERFORMANCE,0.7002341920374707,"m=20
100
100
100
100
100
m=10
100
100
100
m=100
100
100
100
100
100
m=50
100
100
100
PSS
100
100
100
100
90
PSS
90
100
100"
OVERALL PERFORMANCE,0.7014051522248244,DLM-MTR
OVERALL PERFORMANCE,0.702576112412178,"m=20
100
100
100
100
100
m=10
96.7
100
100
m=100
100
100
100
100
100
m=50
97.2
100
100
PSS
100
100
100
100
100
PSS
0
100
100 PRIMA"
OVERALL PERFORMANCE,0.7037470725995316,"m=20
100
100
100
100
100
m=10
100
100
100
m=100
100
100
100
100
100
m=50
100
100
100
PSS
100
100
100
100
90
PSS
90
100
100"
OVERALL PERFORMANCE,0.7049180327868853,"Fig. 4, NLM and NLM-MTR demonstrate a similar performance and suffer the highest reasoning
cost when it is tested with large problem sizes, such as 50 for graph tasks and 100 for family tree
tasks. For MemNN, although the FLOPs of it seem low in most cases of testing, its testing accuracy
is bad and cannot achieve accurate predictions (Table 3). In contrast, PRIMA can signiﬁcantly reduce
the reasoning complexity by intelligently selecting ops using a planner. Overall, PRIMA strikes a
satisfactory capability-efﬁciency tradeoff in comparison with all available multi-tasking baselines."
OVERALL PERFORMANCE,0.7060889929742389,"Operator/Path sharing in MTR
To take a closer look into how PRIMA achieves such a better
capability-efﬁciency tradeoff (in Fig. 4), we examine the reasoning paths on three different graph
tasks: 1-Outdegree, AdjacentToRed, and 4-Connectivity. Speciﬁcally, we sample
instances from these three tasks and feed them into PRIMA separately to generate their corresponding
reasoning paths. The results are plotted in Fig. 5A, where the gray paths denote the ones shared
across tasks, and the colored ones are task-speciﬁc. It clearly shows that PRIMA learns a large set
of neural operators sharable across tasks. Given each input instance from a particular task, PRIMA
activates a set of shared paths along with a few task-speciﬁc paths to deduce the logical consequences."
OVERALL PERFORMANCE,0.7072599531615925,"Generalizability of the Planner
To demonstrate the generalizability of the Planner module in
PRIMA, we generate input instances of AdjacentToRed with topologies that have not been seen
during training. In Fig. 5B, we show the reasoning paths activated by the Planner for these different
topologies, which demonstrates that different input instances share a large portion of reasoning
paths. This fact is not surprising as solving the same task of AdjacentToRed should rely on
a common set of skills. More interestingly, we notice that even for solving this same task, the
Planner will have to call upon a few instance-dependent sub-paths to handle the subtle inherent
differences (e.g., the graph topology) that exist between different input instances. For this reason,
PRIMA maintains an instance-dependent dynamic architecture, which is in sharp contrast to the
Neural Architecture Search (NAS) approaches. Although NAS may also use RL algorithms to seek
for a smaller architecture (Zoph & Le, 2017), it only searches for a static architecture that will be
applied to all the input instances."
OVERALL PERFORMANCE,0.7084309133489461,"Figure 4: The reasoning costs (in FLOPs) of different models at the inference stage (1 M=1⇥106, 1 G=1⇥109).
Compared to NLM and NLM-MTR, our PRIMA signiﬁcantly reduces the reasoning complexity by intelligently
selecting ops (short for neural operators) using a planner. Although the FLOPs of MemNN seem low in
most cases of testing, its testing accuracy is bad and cannot achieve accurate predictions (see Table 3). “OD”
denotes 1-Outdegree, likewise, “AR”:AdjacentToRed, “4C”:4-Connectivity, “HF”:HasFather,
“HS”:HasSister, “GP”:IsGrandparent, “UN”:IsUncle, “MG”:IsMGUncle."
OVERALL PERFORMANCE,0.7096018735362998,Under review as a conference paper at ICLR 2022
OVERALL PERFORMANCE,0.7107728337236534,"Figure 5: The reasoning paths of PRIMA. The gray arrows denote the “shared path”. The colored arrows in
sub-ﬁgures (A) and (B) denote task-speciﬁc paths and instance-speciﬁc paths, respectively. In (A), besides the
AdjacentToRed task we have introduced earlier, we also consider the 1-Outdegree task, which reasons
about whether the out-degree of a node is exactly equal to 1, and the 4-Connectivity task, which is to
decide whether there are two nodes connected within 4 hops."
RELATED WORK,0.711943793911007,"6
RELATED WORK"
RELATED WORK,0.7131147540983607,"Multi-task learning
Multi-task Learning (Zhang & Yang, 2021; Zhou et al., 2011; Pan & Yang,
2009) has focused primarily on the supervised learning paradigm, which itself can be divided into
several approaches. The ﬁrst is feature-based MTL, also called multi-task feature learning, (Argyriou
et al., 2008), which explores the sharing of features across different tasks using regularization tech-
niques (Shinohara, 2016; Liu et al., 2017). The second approach assumes that tasks are intrinsically
related, such as low-rank learning (Ando & Zhang, 2005; Zhang et al., 2005), learning with task
clustering (Gu et al., 2011; Zhang et al., 2016), and task-relation learning (such as via task similarity,
task correlation, or task covariance) (Goncalves et al., 2016; Zhang, 2013; Ciliberto et al., 2015).
MTL has also been explored under other paradigms such as unsupervised learning, for example, via
multi-task clustering in (Zhang & Zhang, 2013; Zhang et al., 2015b; Gu et al., 2011; Zhang, 2015)."
RELATED WORK,0.7142857142857143,"Neural-symbolic reasoning
Neural-symbolic AI for reasoning and inference has a long his-
tory (Besold et al., 2017; Bader et al., 2004; Garcez et al., 2008), and neural-ILP has developed
primarily in two major directions (Cropper et al., 2020). The ﬁrst direction applies neural operators
such as tensor calculus to simulate logic reasoning (Yang et al., 2017; Dong et al., 2019; Shi et al.,
2020) and (Manhaeve et al., 2018). The second direction involves relaxed subset selection (Evans &
Grefenstette, 2018; Si et al., 2019) with a predeﬁned set of task-speciﬁc logic clauses. This approach
reduces the task to a subset-selection problem by selecting a subset of clauses from the predeﬁned set
and using neural networks to search for a relaxed solution."
RELATED WORK,0.7154566744730679,"Our work is different from the most recent work, such as (Dong et al., 2019; Shi et al., 2020), in
several ways. Compared with (Dong et al., 2019), the most notable difference is the improvement in
efﬁciency by introducing learning-to-reason via reinforcement learning. A second major difference is
that PRIMA offers more generalizability than NLM by decomposing the logic operators into more
ﬁnely-grained units. We refer readers for more detailed related work to Appendix D."
CONCLUSION,0.7166276346604216,"7
CONCLUSION"
CONCLUSION,0.7177985948477752,"A long-standing challenge in multi-task reasoning (MTR) is the intrinsic conﬂict between capability
and efﬁciency. Our main contribution is the development of a novel neural-logic architecture termed
PRIMA, which learns to perform efﬁcient multitask reasoning (in ﬁrst-order logic) by dynamically
chaining learnable Horn clauses (represented by the neural logic operators). PRIMA improves
inference efﬁciency by dynamically pruning unnecessary neural logic operators, and achieves a state-
of-the-art balance between MTR capability and inference efﬁciency. The model’s training follows
a complete data-driven end-to-end approach via deep reinforcement learning, and the performance
is validated across a variety of benchmarks. Future work could include extending the framework to
high-order logic and investigating scenarios when meta-rules have a hierarchical structure."
CONCLUSION,0.7189695550351288,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.7201405152224825,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.7213114754098361,"We commit to ensuring that other researchers with reasonable background knowledge in our area can
reproduce our theoretical and empirical results. The algorithm details can be seen in Appendix A.
Speciﬁcally, benchmark details are provided in Appendix B.1, hyper-parameter settings are provided
in Appendix B.2, and computing infrastructure are detailed in Appendix B.3. The experiment details
can be seen in Appendix B."
ETHICS STATEMENT,0.7224824355971897,ETHICS STATEMENT
ETHICS STATEMENT,0.7236533957845434,"This work is about the methodology of achieving a capability-efﬁciency trade-off in multi-task
ﬁrst-order logic reasoning. The potential impact of this work is likely to further extend the framework
to high-order logic and investigate scenarios when meta-rules have a hierarchical structure, which
should be generally beneﬁcial to the multi-task learning research community. We have not considered
speciﬁc applications or practical scenarios as the goal of this work. Hence, it does not have any direct
ethical consequences."
REFERENCES,0.724824355971897,REFERENCES
REFERENCES,0.7259953161592506,Rie Kubota Ando and Tong Zhang. A framework for learning predictive structures from multiple
REFERENCES,0.7271662763466042,"tasks and unlabeled data. JMLR, 2005."
REFERENCES,0.7283372365339579,"Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy"
REFERENCES,0.7295081967213115,"sketches. In ICML, 2017."
REFERENCES,0.7306791569086651,"Peter B Andrews. An introduction to mathematical logic and type theory, volume 27. Springer"
REFERENCES,0.7318501170960188,"Science & Business Media, 2002."
REFERENCES,0.7330210772833724,"Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature learning."
REFERENCES,0.734192037470726,"MLJ, 2008."
REFERENCES,0.7353629976580797,Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine
REFERENCES,0.7365339578454333,"Learning Research, 3(Nov):397–422, 2002."
REFERENCES,0.7377049180327869,"Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit"
REFERENCES,0.7388758782201406,"problem. Machine learning, 47(2-3):235–256, 2002."
REFERENCES,0.7400468384074942,Sebastian Bader and Pascal Hitzler. Dimensions of neural-symbolic integration-a structured survey.
REFERENCES,0.7412177985948478,"arXiv preprint cs/0511042, 2005."
REFERENCES,0.7423887587822015,"Sebastian Bader, Pascal Hitzler, and Steffen Hölldobler. The integration of connectionism and"
REFERENCES,0.7435597189695551,"ﬁrst-order knowledge representation and reasoning as a challenge for artiﬁcial intelligence. arXiv
preprint cs/0408069, 2004."
REFERENCES,0.7447306791569087,"Peter W Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, and Koray Kavukcuoglu. Interac-"
REFERENCES,0.7459016393442623,"tion networks for learning about objects, relations and physics. arXiv preprint arXiv:1612.00222,
2016."
REFERENCES,0.747072599531616,"Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,"
REFERENCES,0.7482435597189696,"Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018."
REFERENCES,0.7494145199063232,Yoshua Bengio. From system 1 deep learning to system 2 deep learning. In Thirty-third Conference
REFERENCES,0.7505854800936768,"on Neural Information Processing Systems, 2019."
REFERENCES,0.7517564402810304,"Tarek R Besold, Artur d’Avila Garcez, Sebastian Bader, Howard Bowman, Pedro Domingos, Pas-"
REFERENCES,0.752927400468384,"cal Hitzler, Kai-Uwe Kühnberger, Luis C Lamb, Daniel Lowd, Priscila Machado Vieira Lima,
et al. Neural-symbolic learning and reasoning: A survey and interpretation. arXiv preprint
arXiv:1711.03902, 2017."
REFERENCES,0.7540983606557377,Under review as a conference paper at ICLR 2022
REFERENCES,0.7552693208430913,"Timo Bräm, Gino Brunner, Oliver Richter, and Roger Wattenhofer. Attentive multi-task deep"
REFERENCES,0.7564402810304449,"reinforcement learning. In ECML/PKDD, 2019."
REFERENCES,0.7576112412177985,"Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp"
REFERENCES,0.7587822014051522,"Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey
of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in
games, 4(1):1–43, 2012."
REFERENCES,0.7599531615925058,"Salam El Bsat, Haitham Bou-Ammar, and Matthew E. Taylor. Scalable multitask policy gradient"
REFERENCES,0.7611241217798594,"reinforcement learning. In AAAI, 2017."
REFERENCES,0.7622950819672131,"Daniele Calandriello, Alessandro Lazaric, and Marcello Restelli. Sparse multi-task reinforcement"
REFERENCES,0.7634660421545667,"learning. In NIPS, 2014."
REFERENCES,0.7646370023419203,"Marco Calautti, Georg Gottlob, and Andreas Pieris. Chase termination for guarded existential rules."
REFERENCES,0.765807962529274,"In Proceedings of the 34th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database
Systems, pp. 91–103, 2015."
REFERENCES,0.7669789227166276,L. Cao. Non-iid recommender systems: A review and framework of recommendation paradigm
REFERENCES,0.7681498829039812,"shifting. Engineering, 2(2):212–224, 2016."
REFERENCES,0.7693208430913349,"Deepayan Chakrabarti and Christos Faloutsos. Graph mining: Laws, generators, and algorithms."
REFERENCES,0.7704918032786885,"ACM computing surveys (CSUR), 38(1):2–es, 2006."
REFERENCES,0.7716627634660421,Ashok K Chandra and David Harel. Horn clause queries and generalizations. The Journal of Logic
REFERENCES,0.7728337236533958,"Programming, 2(1):1–15, 1985."
REFERENCES,0.7740046838407494,"Jianzhong Chen, Stephen Muggleton, and José Santos. Learning probabilistic logic models from"
REFERENCES,0.775175644028103,"probabilistic examples. Machine learning, 73(1):55–85, 2008."
REFERENCES,0.7763466042154566,"K. Chen, F. Yang, and X. Chen. Planning with task-oriented knowledge acquisition for a service"
REFERENCES,0.7775175644028103,"robot. In IJCAI, pp. 812–818, 2016."
REFERENCES,0.7786885245901639,"Xinlei Chen, Li-Jia Li, Li Fei-Fei, and Abhinav Gupta. Iterative visual reasoning beyond convolutions."
REFERENCES,0.7798594847775175,"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7239–
7248, 2018."
REFERENCES,0.7810304449648712,"Carlo Ciliberto, Youssef Mroueh, Tomaso A. Poggio, and Lorenzo Rosasco. Convex learning of"
REFERENCES,0.7822014051522248,"multiple tasks and their structure. In ICML, 2015."
REFERENCES,0.7833723653395784,"Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text"
REFERENCES,0.7845433255269321,"encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020."
REFERENCES,0.7857142857142857,"Diane J Cook and Lawrence B Holder. Mining graph data. John Wiley & Sons, 2006."
REFERENCES,0.7868852459016393,Michael Crawshaw. Multi-task learning with deep neural networks: A survey. arXiv preprint
REFERENCES,0.788056206088993,"arXiv:2009.09796, 2020."
REFERENCES,0.7892271662763466,Andrew Cropper and Sebastijan Dumancic. Inductive logic programming at 30: a new introduction.
REFERENCES,0.7903981264637002,"arXiv preprint arXiv:2008.07912, 2020."
REFERENCES,0.7915690866510539,"Andrew Cropper, Sebastijan Dumancic, and Stephen H Muggleton. Turning 30: New ideas in"
REFERENCES,0.7927400468384075,"inductive logic programming. arXiv preprint arXiv:2002.11002, 2020."
REFERENCES,0.7939110070257611,"Aniket Anand Deshmukh, Ürün Dogan, and Clayton Scott. Multi-task learning for contextual bandits."
REFERENCES,0.7950819672131147,"In NIPS, 2017."
REFERENCES,0.7962529274004684,"Carlos Diuk, Andre Cohen, and Michael L Littman. An object-oriented representation for efﬁcient"
REFERENCES,0.797423887587822,"reinforcement learning. In Proceedings of the 25th international conference on Machine learning,
pp. 240–247, 2008."
REFERENCES,0.7985948477751756,"Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic"
REFERENCES,0.7997658079625293,"machines. arXiv preprint arXiv:1904.11694, 2019."
REFERENCES,0.8009367681498829,Under review as a conference paper at ICLR 2022
REFERENCES,0.8021077283372365,"Sašo Džeroski, Luc De Raedt, and Kurt Driessens. Relational reinforcement learning. Machine"
REFERENCES,0.8032786885245902,"learning, 43(1):7–52, 2001."
REFERENCES,0.8044496487119438,"Lasse Espeholt, Hubert Soyer, Rémi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam"
REFERENCES,0.8056206088992974,"Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA:
scalable distributed deep-RL with importance weighted actor-learner architectures. In ICML, 2018."
REFERENCES,0.8067915690866511,Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. Journal of
REFERENCES,0.8079625292740047,"Artiﬁcial Intelligence Research, 61:1–64, 2018."
REFERENCES,0.8091334894613583,Melvin Fitting. First-order logic and automated theorem proving. Springer Science & Business
REFERENCES,0.810304449648712,"Media, 2012."
REFERENCES,0.8114754098360656,Jean H Gallier. Logic for computer science: foundations of automatic theorem proving. Courier
REFERENCES,0.8126463700234192,"Dover Publications, 2015."
REFERENCES,0.8138173302107728,"M. C. Ganiz, C. George, and W. M Pottenger. Higher order naive bayes: A novel non-iid approach to"
REFERENCES,0.8149882903981265,"text classiﬁcation. IEEE Transactions on Knowledge and Data Engineering, 23(7):1022–1034,
2010."
REFERENCES,0.8161592505854801,"Artur SD’Avila Garcez, Luis C Lamb, and Dov M Gabbay. Neural-symbolic cognitive reasoning."
REFERENCES,0.8173302107728337,"Springer Science & Business Media, 2008."
REFERENCES,0.8185011709601874,"M. Gebser, B. Kaufmann, and T. Schaub. Conﬂict-driven answer set solving: From theory to practice."
REFERENCES,0.819672131147541,"Artiﬁcial Intelligence, 187-188:52–89, 2012."
REFERENCES,0.8208430913348946,Michael Gelfond and Vladimir Lifschitz. Action languages. 1998.
REFERENCES,0.8220140515222483,"André R. Goncalves, Fernando J. Von Zuben, and Arindam Banerjee. Multi-task sparse structure"
REFERENCES,0.8231850117096019,"learning with Gaussian copula models. JMLR, 2016."
REFERENCES,0.8243559718969555,"Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil"
REFERENCES,0.8255269320843092,"Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014."
REFERENCES,0.8266978922716628,"Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-"
REFERENCES,0.8278688524590164,"Barwi´nska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al.
Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):
471–476, 2016."
REFERENCES,0.8290398126463701,"Quanquan Gu, Zhenhui Li, and Jiawei Han. Learning a kernel for multi-task clustering. In AAAI, 2011."
REFERENCES,0.8302107728337237,"Carlos Guestrin, Daphne Koller, Chris Gearhart, and Neal Kanodia. Generalizing plans to new"
REFERENCES,0.8313817330210773,"environments in relational mdps. In Proceedings of the 18th international joint conference on
Artiﬁcial intelligence, pp. 1003–1010, 2003."
REFERENCES,0.832552693208431,"Marc Hanheide, Moritz Göbelbecker, Graham S Horn, Andrzej Pronobis, Kristoffer Sjöö, Alper"
REFERENCES,0.8337236533957846,"Aydemir, Patric Jensfelt, Charles Gretton, Richard Dearden, Miroslav Janicek, et al. Robot task
planning and explanation in open and uncertain worlds. Artiﬁcial Intelligence, 2015."
REFERENCES,0.8348946135831382,"M. Helmert. The fast downward planning system. Journal of Artiﬁcial Intelligence Research, 26:"
REFERENCES,0.8360655737704918,"191–246, 2006."
REFERENCES,0.8372365339578455,Alfred Horn. On sentences which are true of direct unions of algebras1. The Journal of Symbolic
REFERENCES,0.8384074941451991,"Logic, 16(1):14–21, 1951."
REFERENCES,0.8395784543325527,"Maximilian Igl, Andrew Gambardella, Nantas Nardelli, N. Siddharth, Wendelin Böhmer, and Shimon"
REFERENCES,0.8407494145199064,"Whiteson. Multitask soft option learning. In UAI, 2020."
REFERENCES,0.84192037470726,"Herbert Jaeger. Deep neural reasoning. Nature, 538(7626):467–468, 2016."
REFERENCES,0.8430913348946136,"Ramtin Keramati, Jay Whang, Patrick Cho, and Emma Brunskill. Strategic object oriented rein-"
REFERENCES,0.8442622950819673,"forcement learning. In Exploration in Reinforcement Learning Workshop at the 35th International
Conference on Machine Learning, 2018."
REFERENCES,0.8454332552693209,Under review as a conference paper at ICLR 2022
REFERENCES,0.8466042154566745,"P. Khandelwal, S. Zhang, J. Sinapov, M. Leonetti, J. Thomason, F. Yang, I. Gori, M. Svetlik, P. Khante,"
REFERENCES,0.8477751756440282,"V. Lifschitz, and P. Stone. Bwibots: A platform for bridging the gap between ai and human–robot
interaction research. The International Journal of Robotics Research, 36(5-7):635–659, 2017."
REFERENCES,0.8489461358313818,"Levente Kocsis, Csaba Szepesvári, and Jan Willemson. Improved monte-carlo search. Univ. Tartu,"
REFERENCES,0.8501170960187353,"Estonia, Tech. Rep, 1, 2006."
REFERENCES,0.8512880562060889,Daphne Koller. Probabilistic relational models. In International Conference on Inductive Logic
REFERENCES,0.8524590163934426,"Programming, pp. 3–13. Springer, 1999."
REFERENCES,0.8536299765807962,"Daphne Koller, Nir Friedman, Sašo Džeroski, Charles Sutton, Andrew McCallum, Avi Pfeffer, Pieter"
REFERENCES,0.8548009367681498,"Abbeel, Ming-Fai Wong, David Heckerman, Chris Meek, et al. Introduction to statistical relational
learning. MIT press, 2007."
REFERENCES,0.8559718969555035,Markus Krötzsch. Computing cores for existential rules with the standard chase and asp. In
REFERENCES,0.8571428571428571,"Proceedings of the International Conference on Principles of Knowledge Representation and
Reasoning, volume 17, pp. 603–613, 2020."
REFERENCES,0.8583138173302107,Alessandro Lazaric and Mohammad Ghavamzadeh. Bayesian multi-task reinforcement learning. In
REFERENCES,0.8594847775175644,"ICML, 2010."
REFERENCES,0.860655737704918,"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015."
REFERENCES,0.8618266978922716,"Hui Li, Xuejun Liao, and Lawrence Carin. Multi-task reinforcement learning in partially observable"
REFERENCES,0.8629976580796253,"stochastic environments. JMLR, 2009."
REFERENCES,0.8641686182669789,"Sijin Li, Zhi-Qiang Liu, and Antoni B. Chan. Heterogeneous multi-task learning for human pose"
REFERENCES,0.8653395784543325,"estimation with deep convolutional neural network. IJCV, 2015."
REFERENCES,0.8665105386416861,V. Lifschitz. What is answer set programming? In Proceedings of the AAAI Conference on Artiﬁcial
REFERENCES,0.8676814988290398,"Intelligence, pp. 1594–1597. MIT Press, 2008."
REFERENCES,0.8688524590163934,"Xingyu Lin, Harjatin Singh Baweja, George Kantor, and David Held. Adaptive auxiliary task"
REFERENCES,0.870023419203747,"weighting for reinforcement learning. In NeurIPS, 2019."
REFERENCES,0.8711943793911007,"Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Adversarial multi-task learning for text classiﬁcation."
REFERENCES,0.8723653395784543,"In ACL, 2017."
REFERENCES,0.8735362997658079,"Wu Liu, Tao Mei, Yongdong Zhang, Cherry Che, and Jiebo Luo. Multi-task deep visual-semantic"
REFERENCES,0.8747072599531616,"embedding for video thumbnail selection. In CVPR, 2015."
REFERENCES,0.8758782201405152,"Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt."
REFERENCES,0.8770491803278688,"Deepproblog: Neural probabilistic logic programming.
In Advances in Neural Information
Processing Systems, pp. 3749–3759, 2018."
REFERENCES,0.8782201405152225,"Zimmer Matthieu, Feng Xuening, Glanois Claire, Jiang Zhaohui, Zhang Jianyi, Weng Paul, Jianye"
REFERENCES,0.8793911007025761,"Hao, Dong Li, and Wulong Liu. Differentiable logic machines. arXiv preprint arXiv:2102.11529,
2021."
REFERENCES,0.8805620608899297,"Drew McDermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso,"
REFERENCES,0.8817330210772834,"Daniel Weld, and David Wilkins. Pddl-the planning domain deﬁnition language. 1998."
REFERENCES,0.882903981264637,"Nikola Mrksic, Diarmuid Ó Séaghdha, Blaise Thomson, Milica Gasic, Pei-hao Su, David Vandyke,"
REFERENCES,0.8840749414519906,"Tsung-Hsien Wen, and Steve J. Young. Multi-domain dialog state tracking using recurrent neural
networks. In ACL, 2015."
REFERENCES,0.8852459016393442,Stephen Muggleton and Luc De Raedt. Inductive logic programming: Theory and methods. The
REFERENCES,0.8864168618266979,"Journal of Logic Programming, 19:629–679, 1994."
REFERENCES,0.8875878220140515,Remi Munos. From bandits to monte-carlo tree search: The optimistic principle applied to optimiza-
REFERENCES,0.8887587822014051,"tion and planning. Foundations and Trends in Machine Learning, 2014."
REFERENCES,0.8899297423887588,Under review as a conference paper at ICLR 2022
REFERENCES,0.8911007025761124,"Shayegan Omidshaﬁei, Jason Pazis, Christopher Amato, Jonathan P. How, and John Vian. Deep"
REFERENCES,0.892271662763466,"decentralized multi-task multi-agent reinforcement learning under partial observability. In ICML,
2017."
REFERENCES,0.8934426229508197,"Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. arXiv preprint"
REFERENCES,0.8946135831381733,"arXiv:1711.08028, 2017."
REFERENCES,0.8957845433255269,Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
REFERENCES,0.8969555035128806,"and data engineering, 22(10):1345–1359, 2009."
REFERENCES,0.8981264637002342,"Emilio Parisotto, Jimmy Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer"
REFERENCES,0.8992974238875878,"reinforcement learning. In ICLR, 2016."
REFERENCES,0.9004683840749415,Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
REFERENCES,0.9016393442622951,"Wiley & Sons, 2014."
REFERENCES,0.9028103044496487,Christopher D Rosin. Multi-armed bandits with episode context. Annals of Mathematics and Artiﬁcial
REFERENCES,0.9039812646370023,"Intelligence, 61(3):203–230, 2011."
REFERENCES,0.905152224824356,"Andrei A. Rusu, Sergio Gomez Colmenarejo, Çaglar Gülçehre, Guillaume Desjardins, James Kirk-"
REFERENCES,0.9063231850117096,"patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell.
Policy
distillation. In ICLR, 2016."
REFERENCES,0.9074941451990632,"Adam Santoro, David Raposo, David GT Barrett, Mateusz Malinowski, Razvan Pascanu, Peter"
REFERENCES,0.9086651053864169,"Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. arXiv
preprint arXiv:1706.01427, 2017."
REFERENCES,0.9098360655737705,"Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-"
REFERENCES,0.9110070257611241,"ization help optimization? arXiv preprint arXiv:1805.11604, 2018."
REFERENCES,0.9121779859484778,"Andrew M. Saxe, Adam Christopher Earle, and Benjamin Rosman. Hierarchy through composition"
REFERENCES,0.9133489461358314,"with multitask LMDPs. In ICML, 2017."
REFERENCES,0.914519906323185,"Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon"
REFERENCES,0.9156908665105387,"Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019."
REFERENCES,0.9168618266978923,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy"
REFERENCES,0.9180327868852459,"optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.9192037470725996,Sahil Sharma and Balaraman Ravindran. Online multi-task learning using active sampling. In ICLR
REFERENCES,0.9203747072599532,"Workshop, 2017."
REFERENCES,0.9215456674473068,"Sahil Sharma, Ashutosh Kumar Jha, Parikshit Hegde, and Balaraman Ravindran. Learning to"
REFERENCES,0.9227166276346604,"multi-task by active sampling. In ICLR, 2018."
REFERENCES,0.9238875878220141,"Shaoyun Shi, Hanxiong Chen, Weizhi Ma, Jiaxin Mao, Min Zhang, and Yongfeng Zhang. Neural"
REFERENCES,0.9250585480093677,"logic reasoning. In Proceedings of the 29th ACM International Conference on Information &
Knowledge Management, pp. 1365–1374, 2020."
REFERENCES,0.9262295081967213,Yusuke Shinohara. Adversarial multi-task learning of deep neural networks for robust speech
REFERENCES,0.927400468384075,"recognition. In Interspeech, 2016."
REFERENCES,0.9285714285714286,"Xujie Si, Mukund Raghothaman, Kihong Heo, and Mayur Naik. Synthesizing datalog programs"
REFERENCES,0.9297423887587822,"using numerical relaxation. arXiv preprint arXiv:1906.00163, 2019."
REFERENCES,0.9309133489461359,"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,"
REFERENCES,0.9320843091334895,"Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi
by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815,
2017a."
REFERENCES,0.9332552693208431,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,"
REFERENCES,0.9344262295081968,"Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354–359, 2017b."
REFERENCES,0.9355971896955504,Under review as a conference paper at ICLR 2022
REFERENCES,0.936768149882904,"Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks."
REFERENCES,0.9379391100702577,"In Advances in Neural Information Processing Systems, pp. 2440–2448, 2015."
REFERENCES,0.9391100702576113,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.9402810304449649,"Prasad Tadepalli, Robert Givan, and Kurt Driessens. Relational reinforcement learning: An overview."
REFERENCES,0.9414519906323185,"In Proceedings of the ICML-2004 workshop on relational reinforcement learning, pp. 1–9, 2004."
REFERENCES,0.9426229508196722,"Yee Whye Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell,"
REFERENCES,0.9437939110070258,"Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In NIPS,
2017."
REFERENCES,0.9449648711943794,"Rasul Tutunov, Dongho Kim, and Haitham Bou-Ammar. Distributed multitask reinforcement learning"
REFERENCES,0.9461358313817331,"with quadratic convergence. In NeurIPS, 2018."
REFERENCES,0.9473067915690867,"Frank Van Harmelen, Vladimir Lifschitz, and Bruce Porter. Handbook of knowledge representation."
REFERENCES,0.9484777517564403,"Elsevier, 2008."
REFERENCES,0.949648711943794,Nelson Vithayathil Varghese and Qusay H Mahmoud. A survey of multi-task deep reinforcement
REFERENCES,0.9508196721311475,"learning. Electronics, 9(9):1363, 2020."
REFERENCES,0.9519906323185011,"Tung-Long Vuong, Do Van Nguyen, Tai-Long Nguyen, Cong-Minh Bui, Hai-Dang Kieu, Viet-Cuong"
REFERENCES,0.9531615925058547,"Ta, Quoc-Long Tran, and Thanh Ha Le. Sharing experience in multitask reinforcement learning.
In IJCAI, 2019."
REFERENCES,0.9543325526932084,Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
REFERENCES,0.955503512880562,"learning. Machine learning, 8(3-4):229–256, 1992."
REFERENCES,0.9566744730679156,"Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: A"
REFERENCES,0.9578454332552693,"hierarchical Bayesian approach. In ICML, 2007."
REFERENCES,0.9590163934426229,"Victoria Xia, Zi Wang, and Leslie Pack Kaelbling. Learning sparse relational transition models. arXiv"
REFERENCES,0.9601873536299765,"preprint arXiv:1810.11177, 2018."
REFERENCES,0.9613583138173302,"Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge"
REFERENCES,0.9625292740046838,"base reasoning. In Advances in Neural Information Processing Systems, pp. 2319–2328, 2017."
REFERENCES,0.9637002341920374,"Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-"
REFERENCES,0.9648711943793911,"symbolic vqa: Disentangling reasoning from vision and language understanding. In NIPS, pp.
1031–1042, 2018."
REFERENCES,0.9660421545667447,"N. Yoshida, T. Nishio, M. Morikura, K. Yamamoto, and R. Yonetani. Hybrid-ﬂfor wireless networks:"
REFERENCES,0.9672131147540983,"Cooperative learning mechanism using non-iid data. In ICC 2020-2020 IEEE International
Conference on Communications (ICC), pp. 1–7. IEEE, 2020."
REFERENCES,0.968384074941452,"Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl"
REFERENCES,0.9695550351288056,"Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Relational deep reinforcement
learning. arXiv preprint arXiv:1806.01830, 2018."
REFERENCES,0.9707259953161592,"Jian Zhang, Zoubin Ghahramani, and Yiming Yang. Learning multiple related tasks using latent"
REFERENCES,0.9718969555035128,"independent component analysis. In NIPS, 2005."
REFERENCES,0.9730679156908665,"Si Zhang, Hanghang Tong, Jiejun Xu, and Ross Maciejewski. Graph convolutional networks: a"
REFERENCES,0.9742388758782201,"comprehensive review. Computational Social Networks, 6(1):1–23, 2019."
REFERENCES,0.9754098360655737,"Wenlu Zhang, Rongjian Li, Tao Zeng, Qian Sun, Sudhir Kumar, Jieping Ye, and Shuiwang Ji. Deep"
REFERENCES,0.9765807962529274,"model based transfer and multi-task learning for biological image analysis. In KDD, 2015a."
REFERENCES,0.977751756440281,Xianchao Zhang and Xiaotong Zhang. Smart multi-task Bregman clustering and multi-task kernel
REFERENCES,0.9789227166276346,"clustering. In AAAI, 2013."
REFERENCES,0.9800936768149883,"Xianchao Zhang, Xiaotong Zhang, and Han Liu. Smart multitask Bregman clustering and multitask"
REFERENCES,0.9812646370023419,"kernel clustering. ACM TKDD, 2015b."
REFERENCES,0.9824355971896955,"Xianchao Zhang, Xiaotong Zhang, and Han Liu. Self-adapted multi-task clustering. In IJCAI, 2016."
REFERENCES,0.9836065573770492,Under review as a conference paper at ICLR 2022
REFERENCES,0.9847775175644028,"Xiao-Lei Zhang. Convex discriminative multitask clustering. IEEE TPAMI, 2015."
REFERENCES,0.9859484777517564,"Yu Zhang. Heterogeneous-neighborhood-based multi-task local learning algorithms. In NIPS, 2013."
REFERENCES,0.9871194379391101,Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and
REFERENCES,0.9882903981264637,"Data Engineering, 2021."
REFERENCES,0.9894613583138173,"Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Facial landmark detection by deep"
REFERENCES,0.990632318501171,"multi-task learning. In ECCV, 2014."
REFERENCES,0.9918032786885246,"Jiayu Zhou, Jianhui Chen, and Jieping Ye. Malsar: Multi-task learning via structural regularization."
REFERENCES,0.9929742388758782,"Arizona State University, 21, 2011."
REFERENCES,0.9941451990632318,"Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,"
REFERENCES,0.9953161592505855,"Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications.
AI Open, 1:57–81, 2020."
REFERENCES,0.9964871194379391,"Zhuangdi Zhu, Kaixiang Lin, and Jiayu Zhou. Transfer learning in deep reinforcement learning: A"
REFERENCES,0.9976580796252927,"survey. arXiv preprint arXiv:2009.07888, 2020."
REFERENCES,0.9988290398126464,"Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. ICLR, 2017."
