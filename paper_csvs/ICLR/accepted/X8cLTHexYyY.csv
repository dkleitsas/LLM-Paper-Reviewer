Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0016313213703099511,"k-means clustering is a well-studied problem due to its wide applicability. Unfor-
tunately, there exist strong theoretical limits on the performance of any algorithm
for the k-means problem on worst-case inputs. To overcome this barrier, we con-
sider a scenario where “advice” is provided to help perform clustering. Specif-
ically, we consider the k-means problem augmented with a predictor that, given
any point, returns its cluster label in an approximately optimal clustering up to
some, possibly adversarial, error. We present an algorithm whose performance
improves along with the accuracy of the predictor, even though na¨ıvely following
the accurate predictor can still lead to a high clustering cost. Thus if the predictor
is sufﬁciently accurate, we can retrieve a close to optimal clustering with nearly
optimal runtime, breaking known computational barriers for algorithms that do
not have access to such advice. We evaluate our algorithms on real datasets and
show signiﬁcant improvements in the quality of clustering."
INTRODUCTION,0.0032626427406199023,"1
INTRODUCTION"
INTRODUCTION,0.004893964110929853,"Clustering is a fundamental task in data analysis that is typically one of the ﬁrst methods used to
understand the structure of large datasets. The most common formulation of clustering is the k-
means problem where given a set P ⊂Rd of n points, the goal is to ﬁnd a set of centers C ⊂Rd of
k points to minimize the objective cost(P, C) = P"
INTRODUCTION,0.0065252854812398045,"p∈P minc∈C ∥p −c∥2
2. (1)"
INTRODUCTION,0.008156606851549755,"Despite decades of work, there exist strong theoretical limitations about the performance of any al-
gorithm for the k-means problem. Finding the optimal set C is NP-hard even for the case of k = 2
(Dasgupta, 2008) and even ﬁnding an approximate solution with objective value that is within a
factor 1.07 of the optimal solution is NP-hard (Cohen-Addad & S., 2019; Lee et al., 2017). Fur-
thermore, the best-known practical polynomial time algorithms can only provably achieve a large
constant factor approximation to the optimal clustering, e.g., the 50-approximation in Song & Ra-
jasekaran (2010), or use techniques such as linear programming that do not scale, e.g., the 6.357-
approximation in Ahmadian et al. (2020)."
INTRODUCTION,0.009787928221859706,"A natural approach to overcome these computational barriers is to leverage the fact that in many
applications, the input is often not arbitrary and contains auxiliary information that can be used to
construct a good clustering, e.g., in many applications, the input can be similar to past instances.
Thus, it is reasonable to create a (possibly erroneous) predictor by using auxiliary information or
through clusterings of similar datasets, which can inform the proper label of an item in our current
dataset. Indeed, inspired by the developments in machine learning, many recent papers have stud-
ied algorithms augmented with predictions (Mitzenmacher & Vassilvitskii, 2020). Such algorithms
utilize a predictor that, when invoked, provides an (imperfect) prediction for future inputs. The
predictions are then used by the algorithm to improve performance (see references in Section 1.3)."
INTRODUCTION,0.011419249592169658,"Hence, we consider the problem of k-means clustering given additional access to a predictor that
outputs advice for which points should be clustered together, by outputting a label for each point.
The goal is to ﬁnd k centers that minimize objective (1) and assign each point to one of these centers."
INTRODUCTION,0.013050570962479609,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01468189233278956,"The question is then whether one can utilize such predictions to boost the accuracy and runtime of
clustering of new datasets. Our results demonstrate the answer in the afﬁrmative."
INTRODUCTION,0.01631321370309951,"Formal learning-augmented problem deﬁnition.
Given a set P ⊆Rd of n points, the goal is to
ﬁnd a set of k points C (called centers) to minimize objective (1). In the learning-augmented setting,
we assume we have access to a predictor Π that provides information about the label of each point
consistent with a (1+α)-approximately optimal clustering C. We say that a predictor has label error
rate λ ≤α if for each label i ∈[k] := {1, . . . , k}, Π errs on at most a λ ≤α fraction of all points
in cluster i in C, and Π errs on at most a λ ≤α fraction of all points given label i by Π. In other
words, Π has at least (1 −λ) precision and recall for each label."
INTRODUCTION,0.01794453507340946,"Our predictor model subsumes both random and adversarial errors by the predictor. For example if
the cluster sizes are somewhat well-balanced, then a special case of our model is when Π(p) outputs
the correct label of point p ∈P with some probability 1 −λ and otherwise outputs a random label
in [k] with probability λ. The example where the predictor outputs an adversarial label instead of a
random label with probability λ also falls under our model. For more detail, see Theorems 2.1 and
3.4. We also adjust our algorithm to have better performance when the errors are random rather than
adversarial in the supplementary material."
MOTIVATION FOR OUR WORK,0.01957585644371941,"1.1
MOTIVATION FOR OUR WORK"
MOTIVATION FOR OUR WORK,0.021207177814029365,We ﬁrst motivate studying k-means clustering under the learning-augmented algorithms framework.
MOTIVATION FOR OUR WORK,0.022838499184339316,"Overcoming theoretical barriers. As stated above, no polynomial time algorithm can achieve
better than a constant factor approximation to the optimal clustering. In addition, the best provable
approximation guarantees by polynomial time algorithms have a large constant factor (for example
the 50 approximation in Song & Rajasekaran (2010)), or use methods which do not scale (such as the
linear programming based algorithm in Ahmadian et al. (2020) which gives a 6.357-approximation).
Therefore, it is of interest to study whether a natural assumption can overcome these complexity
barriers. In our work, we show that knowing the true labels up to some possibly adversarial noise can
give us arbitrarily good clusterings, depending on the noise level, which breaks these computational
barriers. Furthermore, we present an algorithm that runs in nearly linear time, rather than just
polynomial time. Lastly, we introduce tools from the robust statistics literature to study k-means
clustering rather than the distance-based sampling procedure that is commonly analyzed (this is the
basis of kmeans++). This new toolkit and connection could have further applications in other
learning-augmented clustering problems."
MOTIVATION FOR OUR WORK,0.024469820554649267,"Practical considerations. In practice, good predictors can be learned for datasets with auxiliary
information. For a concrete example, we can take any dataset that has a train/test split and use a clus-
tering on the training dataset to help us cluster the testing portion of the dataset. Therefore, datasets
do not have to be speciﬁcally curated to ﬁt our modelling assumption, which is a requirement in
other modelling formulations that leverage extra information such as the SSAC model discussed in
Section 1.3. A predictor can also be created from the natural class of datasets that vary over time,
such as Census data or spectral clustering for temporal graphs (graphs slowly varying over time).
For this class of datasets, a clustering from an earlier time step can function as a predictor for later
time steps. Lastly, we can simply use the labels given by another clustering algorithm (such as
kmeans++) or heuristic as a predictor. Therefore, predictors are readily and easily available for a
wide class of natural datasets."
MOTIVATION FOR OUR WORK,0.026101141924959218,"Following the predictor alone is insufﬁcient. Given a predictor that outputs noisy labels, it is
conceivable that its output alone can give us a good clustering relative to optimal. However, this
is not the case and na¨ıvely using the label provided by the predictor for each point can result in an
arbitrarily bad solution, even when the predictor errs with low probability. For example, consider a
cluster of n"
POINTS AT THE ORIGIN AND A CLUSTER OF N,0.02773246329526917,2 points at the origin and a cluster of n
POINTS AT THE ORIGIN AND A CLUSTER OF N,0.02936378466557912,"2 points at x = 1. Then for k = 2, choosing centers
at the origin and at x = 1 induces a k-means clustering cost of zero. However, even for a predictor
that errs with probability 1"
POINTS AT THE ORIGIN AND A CLUSTER OF N,0.03099510603588907,"n, some point will be mislabeled with constant probability, which results
in a positive k-means clustering cost, and so does not provide a relative error approximation. Thus,
using the provided labels by the predictor can induce an arbitrarily bad clustering, even as the label
error rate of the predictor tends to zero. This subtlety makes the model rich and interesting, and
requires us to create non-trivial clustering algorithms."
POINTS AT THE ORIGIN AND A CLUSTER OF N,0.03262642740619902,Published as a conference paper at ICLR 2022
POINTS AT THE ORIGIN AND A CLUSTER OF N,0.03425774877650897,"Predictors with adversarial errors. Since the predictor is separate from the clustering algorithm,
interference with the output of the predictor following the clustering algorithm’s query can be a
source of non-random noise. Thus any scenario in which communication is performed over a noisy
channel (for example, if the predictor is hosted at one server and the algorithm is hosted at another
server) is susceptible to such errors. Another source of adversarial failure by the predictor is when
the predictor is trained on a dataset that can be generated by an adversary, such as in the context of
adversarial machine learning. Moreover, our algorithms have better guarantees when the predictor
does not fail adversarially, e.g., see the supplementary material)."
OUR RESULTS,0.03588907014681892,"1.2
OUR RESULTS"
OUR RESULTS,0.037520391517128875,"In this paper we study “learning-augmented” methods for efﬁcient k-means clustering. Our con-
tributions are both theoretical and empirical. On the theoretical side, we introduce an algorithm
that provably solves the k-means problem almost optimally, given access to a predictor that out-
puts a label for each point p ∈P according to a (1 + α)-approximately optimal clustering, up to
some noise. Speciﬁcally, suppose we have access to a predictor Π with label error rate λ upper
bounded by a parameter α. Then, Algorithm 1 outputs a set of centers eC in ˜O(knd) time1, such
that cost(P, eC) ≤(1 + O(α)) · cost(P, Copt), where Copt is an optimal set of centers. We improve
the runtime in Section 3 by introducing Algorithm 3, which has the same error guarantees, but uses
˜O(nd) runtime, which is nearly optimal since one needs at least nd time to read the points for dense
inputs (Theorem 3.4, and Remark A.14)."
OUR RESULTS,0.03915171288743882,"To output labels for all points, Algorithm 3 requires n queries to the predictor. However, if the goal
is to just output centers for each cluster, then we only require eO(k/α) queries. This is essentially
optimal; we show in Theorem 3.5 that any polynomial time algorithm must perform approximately
eΩ(k/α) queries to output a 1+α-approximate solution assuming the Exponential Time Hypothesis,
a well known complexity-theoretic assumption (Impagliazzo & Paturi, 2001). Note that one could
ignore the oracle entirely, but then one is limited by the constant factor hardness for polynomial time
algorithms, which we bypass with a small number of queries."
OUR RESULTS,0.040783034257748776,"Surprisingly, we do not require assumptions that the input is well-separated or approximation-stable
(Braverman et al., 2011; Balcan et al., 2013), which are assumed in other works. Finally in the
supplementary material, we also give a learning-augmented algorithm for the related problem of
k-median clustering, which has less algebraic structure than that of k-means clustering. We also
consider a deletion predictor, which either outputs a correct label or a failure symbol ⊥and give a
(1 + α)-approximation algorithm even when the “deletion rate” is 1 −1/ poly(k)."
OUR RESULTS,0.04241435562805873,"On the empirical side, we evaluate our algorithms on real and synthetic datasets. We experimentally
show that good predictors can be learned for all of our varied datasets, which can aid in clustering.
We also show our methodology is more robust than other heuristics such as random sampling."
RELATED WORK,0.04404567699836868,"1.3
RELATED WORK"
RELATED WORK,0.04567699836867863,"Learning-augmented algorithms.
Our paper adds to the growing body of work on learning-
augmented algorithms. In this framework, additional “advice” from a possibly erroneous predic-
tor is used to improve performance of classical algorithms. For example, a common predictor is a
“heaviness” predictor that outputs how “important” a given input point is. It has been shown that
such predictors can be learned using modern machine learning techniques or other methods on train-
ing datasets and can be successfully applied to similar testing datasets. This methodology has found
applications in improving data structures (Kraska et al., 2018; Mitzenmacher, 2018), streaming al-
gorithms (Hsu et al., 2019; Jiang et al., 2020), online algorithms (Lykouris & Vassilvtiskii, 2018;
Purohit et al., 2018), graph algorithms (Dai et al., 2017), and many other domains (Mousavi et al.,
2015; Wang et al., 2016; Bora et al., 2017; Sablayrolles et al., 2019; Dong et al., 2020; Sanchez et al.,
2020; Eden et al., 2021). See Mitzenmacher & Vassilvitskii (2020) for an overview and applications."
RELATED WORK,0.04730831973898858,"Clustering with additional information. There have been numerous works that study clustering in
a semi-supervised setting where extra information is given. Basu et al. (2004) gave an active learning
framework of clustering with “must-link”/“cannot-link” constraints, where an algorithm is allowed"
RELATED WORK,0.048939641109298535,1The notation eO hides logarithmic factors.
RELATED WORK,0.05057096247960848,Published as a conference paper at ICLR 2022
RELATED WORK,0.052202283849918436,"to interact with a predictor that determines if two points must or cannot belong to the same cluster.
Their objective function is different than that of k-means and they do not give theoretical bounds on
the quality of their solution. Balcan & Blum (2008) and Awasthi et al. (2017) studied an interactive
framework for clustering, where a predictor interactively provides feedback about whether or not
to split a current cluster or merge two clusters. Vikram & Dasgupta (2016) also worked with an
interactive oracle but for the Bayesian hierarchical clustering problem. These works differ from
ours in their assumptions since their predictors must answer different questions about partitions of
the input points. In contrast, Howe (2017) used logistic regression to aid k-means clustering but do
not give any theoretical guarantees."
RELATED WORK,0.053833605220228384,"The framework closest in spirit to ours is the semi-supervised active clustering framework (SSAC)
introduced by Ashtiani et al. (2016) and further studied by Kim & Ghosh (2017); Mazumdar & Saha
(2017); Gamlath et al. (2018); Ailon et al. (2018); Chien et al. (2018); Huleihel et al. (2019). The
goal of this framework is also to produce a (1 + α)-approximate clustering while minimizing the
number of queries to a predictor that instead answers queries of the form “same-cluster(u, v)”, which
returns 1 if points u, v ∈P are in the same cluster in a particular optimal clustering and 0 otherwise.
Our work differs from the SSAC framework in terms of both runtime guarantees, techniques used,
and model assumptions, as detailed below."
RELATED WORK,0.05546492659053834,"We brieﬂy compare to the most relevant works in the SSAC framework, which are Ailon et al.
(2018) and Mazumdar & Saha (2017). First, the runtime of Ailon et al. (2018) is O(ndk9/α4) even
for a perfectly accurate predictor, while the algorithm of Mazumdar & Saha (2017) uses O(nk2)
queries and runtime ˜O(ndk2). By comparison, we use signiﬁcantly fewer queries, with near linear
runtime ˜O(nd) even for an erroneous predictor. Moreover, a predictor of Mazumdar & Saha (2017)
independently fails each query with probability p so that repeating with pairs containing the same
point can determine the correct label of a point whereas our oracle will always repeatedly fail with
the same query, so that repeated queries do not help."
RELATED WORK,0.057096247960848286,"The SSAC framework uses the predictor to perform importance sampling to obtain a sufﬁcient num-
ber of points from each cluster whereas we use techniques from robust mean estimation, dimension-
ality reduction, and approximate nearest neighbor data structures. Moreover, it is unclear how the
SSAC predictor can be implemented in practice to handle adversarial corruptions. One may consider
simulating the SSAC predictor using information from individual points by simply checking if the
labels of the two input points are the same. However, if a particular input is mislabeled, then all
of the pairs containing this input can also be reported incorrectly, which violates their independent
noise assumption. Finally, the noisy predictor algorithm in Ailon et al. (2018) invokes a step of
recovering a hidden clique in a stochastic block model, making it prohibitively costly to implement."
RELATED WORK,0.05872756933115824,"Lastly, in the SSAC framework, datasets need to be speciﬁcally created to ﬁt into their model since
one requires pairwise information. In contrast, our predictor requires information about individual
points, which can be learned from either a training dataset, from past similar datasets, or from
another approximate or heuristic clustering and is able to handle adversarial corruptions. Thus, we
obtain signiﬁcantly faster algorithms while using an arguably more realistic predictor."
RELATED WORK,0.06035889070146819,"Approximation stability. Another approach to overcome the NP-hardness of approximation for
k-means clustering is the assumption that the underlying dataset follows certain distributional prop-
erties. Introduced by Balcan et al. (2013), the notion of (c, α)-approximate stability (Agarwal et al.,
2015; Awasthi et al., 2019; Balcan et al., 2020) requires that every c-approximation is α-close to
the optimal solution in terms of the fraction of incorrectly clustered points. In contrast, we allow
inputs so that an arbitrarily small fraction of incorrectly clustered points can induce arbitrarily bad
approximations, as previously discussed, e.g., in Section 1.1."
LEARNING-AUGMENTED K-MEANS ALGORITHM,0.06199021207177814,"2
LEARNING-AUGMENTED k-MEANS ALGORITHM"
LEARNING-AUGMENTED K-MEANS ALGORITHM,0.0636215334420881,"Preliminaries. We use [n] to denote the set {1, . . . , n}. Given the set of cluster centers C, we
can partition the input points P into k clusters {C1, . . . , Ck} according to the closest center to each
point. If a point is grouped in Ci in the clustering, we refer to its label as i. Note that labels can
be arbitrarily permuted as long as the labeling across the points of each cluster is consistent. It is
well-known that in k-means clustering, the i-th center is given by the coordinate-wise mean of the"
LEARNING-AUGMENTED K-MEANS ALGORITHM,0.06525285481239804,Published as a conference paper at ICLR 2022
LEARNING-AUGMENTED K-MEANS ALGORITHM,0.06688417618270799,Algorithm 1 Learning-augmented k-means clustering
LEARNING-AUGMENTED K-MEANS ALGORITHM,0.06851549755301795,"Input: A point set X with labels given by a predictor
Π with label error rate λ
Output: (1 + O(α))-approximate k-means clustering
of X
1: for i = 1 to i = k do
2:
Let Yi be the set of points with label i.
3:
Run CRDEST for each of the d coordinates of Yi."
LEARNING-AUGMENTED K-MEANS ALGORITHM,0.0701468189233279,"4:
Let
C′
i
be
the
coordinate-wise
outputs
of
CRDEST.
5: end for
6: Return clustering with centers C′
1, . . . , C′
k."
LEARNING-AUGMENTED K-MEANS ALGORITHM,0.07177814029363784,"Algorithm 2 Coordinate-wise estimation
CRDEST
Input: Points x1, . . . , x2m ∈R, corrup-
tion level λ ≤α
1: Randomly partition the points into
two groups X1, X2 of size m.
2: Let I = [a, b] be the shortest interval
containing m(1 −5α) points of X1.
3: Z ←X2 ∩I
4: z ←
1
|Z|
P"
LEARNING-AUGMENTED K-MEANS ALGORITHM,0.0734094616639478,"x∈Z x
5: Return z"
LEARNING-AUGMENTED K-MEANS ALGORITHM,0.07504078303425775,"points in Ci. Given x ∈Rd and a set C ⊂Rd, we deﬁne d(x, C) = minc∈C ∥x −c∥2. Note that
there may be many approximately optimal clusterings but we consider a ﬁxed one for our analysis."
OUR ALGORITHM,0.0766721044045677,"2.1
OUR ALGORITHM"
OUR ALGORITHM,0.07830342577487764,"Our main result is an algorithm for outputting a clustering that achieves a (1 + 20α) approximation
2 to the optimal objective cost when given access to approximations of the correct labeling of the
points in P. We ﬁrst present a suboptimal algorithm in Algorithm 1 for intuition and then optimize
the runtime in Algorithm 3, which is provided in Section 3."
OUR ALGORITHM,0.0799347471451876,"The intuition for Algorithm 1 is as follows. We ﬁrst address the problem of identifying an approxi-
mate center for each cluster. Let Copt
1
, · · · , Copt
k
be an optimal grouping of the points and consider
all the points labeled i by our predictor for some ﬁxed 1 ≤i ≤k. Since our predictor can err, a
large number of points that are not in Copt
i
may also be labeled i. This is especially problematic
when points that are “signiﬁcantly far” from cluster Copt
i
are given the label i, which may increase
the objective function arbitrarily if we simply take the mean of the points labeled i by the predictor."
OUR ALGORITHM,0.08156606851549755,"To ﬁlter out such outliers, we consider a two step view from the robust statistics literature,
e.g., Prasad et al. (2019); these two steps can respectively be interpreted as a “training” phase and
a “testing” phase that removes “bad” outliers. We ﬁrst randomly partition the points that are given
label i into two groups, X1 and X2, of equal size. We then estimate the mean of Copt
i
using a
coordinate-wise approach through Algorithm 2 (CRDEST), decomposing the total cost as the sum
of the costs in each dimension."
OUR ALGORITHM,0.08319738988580751,"For each coordinate, we ﬁnd the smallest interval I that contains a (1 −4α) fraction of the points
in X1. We show that for label error rate λ ≤α, this “training” phase removes any outliers and
thus provides a rough estimation to the location of the “true” points that are labeled i. To remove
dependency issues, we then “test” X2 on I by computing the mean of X2 ∩I. This allows us to get
empirical centers that are a sufﬁciently good approximation to the coordinates of the true center for
each coordinate. We then repeat on the other labels. The key insight is that the error from mean-
estimation can be directly charged to the approximation error due to the special structure of the
k-means problem. Our main theoretical result considers predictors that err on at most a λ-fraction
of all cluster labels. Note that all omitted proofs appear in the supplementary material."
OUR ALGORITHM,0.08482871125611746,"Theorem 2.1. Let α ∈(10 log n/√n, 1/7), Π be a predictor with label error rate λ ≤α, and
γ ≥1 a sufﬁciently large constant. If each cluster in the (1 + α)-approximately optimal k-means
clustering of the predictor has at least γηk/α points, then Algorithm 1 can be used to output a
(1 + 20α)-approximation to the k-means objective with prob. 1 −1/η, using O(kdn log n) runtime."
OUR ALGORITHM,0.0864600326264274,"We improve the running time to O(nd log n + poly(k, log n)) in Theorem 3.4 in Section 3. Our al-
gorithms can also tolerate similar error rates when failures correspond to random labels, adversarial
labels, or a special failure symbol."
OUR ALGORITHM,0.08809135399673736,2Note that we have not attempted to optimize the constant 20.
OUR ALGORITHM,0.08972267536704731,Published as a conference paper at ICLR 2022
OUR ALGORITHM,0.09135399673735727,"Error rate λ vs. accuracy parameter α.
We emphasize that λ is the error rate of the predictor
and α is only some loose upper bound on λ. It is reasonable that some algorithms can provide lossy
guarantees on their outputs, which translates to the desired loose upper bound α on the accuracy
of the predictor. Even if is not known, multiple instances of the algorithm can be run in parallel
with separate exponentially decreasing “guesses” for the value α. We can simply return the best
clustering among these algorithms, which will provide the same theoretical guarantees as if we set
α = 1.01λ , for example. Thus α does not need to be known in advance and it does not need to be
tuned as a hyperparameter."
NEARLY OPTIMAL RUNTIME ALGORITHM,0.0929853181076672,"3
NEARLY OPTIMAL RUNTIME ALGORITHM"
NEARLY OPTIMAL RUNTIME ALGORITHM,0.09461663947797716,"We now describe Algorithm 3, which is an optimized runtime version of Algorithm 1 and whose
guarantees we present in Theorem 3.4. The bottleneck for Algorithm 1 is that after selecting k
empirical centers, it must still assign each of the n points to the closest empirical center. The main
intuition for Algorithm 3 is that although reading all points uses O(nd) time, we do not need to spend
O(dk) time per point to ﬁnd its closest empirical center, if we set up the correct data structures. In
fact, as long as we assign each point to a “relatively good” center, the assigned clustering is still a
“good” approximation to the optimal solution. Thus we proceed in a similar manner as before to
sample a number of input points and ﬁnd the optimal k centers for the sampled points."
NEARLY OPTIMAL RUNTIME ALGORITHM,0.09624796084828711,"We use dimensionality reduction and an approximate nearest neighbor (ANN) data structure to ef-
ﬁciently assign each point to a “sufﬁciently close” center. Namely if a point p ∈P should be
assigned to its closest empirical Ci then p must be assigned to some empirical center Cj such that
∥p −Cj∥2 ≤2∥p −Ci∥2. Hence, points that are not assigned to their optimal centers only incur
a “small” penalty due to the ANN data structure and so the cost of the clustering does not increase
“too much” in expectation. Formally, we need the following deﬁnitions."
NEARLY OPTIMAL RUNTIME ALGORITHM,0.09787928221859707,"Theorem 3.1 (JL transform). Johnson & Lindenstrauss (1984) Let d(·, ·) be the standard Euclidean
norm. There exists a family of linear maps A : Rd →Rk and an absolute constant C > 0 such that
for any x, y ∈Rd, Pr [φ ∈A, d(φ(x), φ(y)) ∈(1 ± α)d(x, y)] ≥1 −e−Cα2k."
NEARLY OPTIMAL RUNTIME ALGORITHM,0.09951060358890701,"Deﬁnition 3.2 (Terminal dimension reduction). Given a set of points called terminals C ⊂Rd, we
call a map f : Rd →Rk a terminal dimension reduction with distortion D if for every terminal
c ∈C and point p ∈Rd, we have d(p, c) ≤d(f(p), f(c)) ≤D · d(p, c)."
NEARLY OPTIMAL RUNTIME ALGORITHM,0.10114192495921696,"Deﬁnition 3.3 (Approximate nearest neighbor search). Given a set P of n points in a metric space
(X, d), a (c, r)-approximate nearest neighbor search (ANN) data structure takes any query point q ∈
X with non-empty {p ∈P : 0 < d(p, q) ≤r} and outputs a point in {p ∈P : 0 < d(p, q) ≤cr}."
NEARLY OPTIMAL RUNTIME ALGORITHM,0.10277324632952692,"To justify the guarantees of Algorithm 3, we need runtime guarantees on creating a suitable dimen-
sionality reduction map and an ANN data structure. These are from Makarychev et al. (2019) and
Indyk & Motwani (1998); Har-Peled et al. (2012); Andoni et al. (2018) respectively, and are stated
in Theorems A.12 and A.13 in the supplementary section. They ensure that each point is mapped to
a “good” center. Thus, we obtain our main result describing the guarantees of Algorithm 3."
NEARLY OPTIMAL RUNTIME ALGORITHM,0.10440456769983687,"Theorem 3.4. Let α ∈(10 log n/√n, 1/7), Π be a predictor with label error rate λ ≤α, and γ ≥1
be a sufﬁciently large constant. If each cluster in the optimal k-means clustering of the predictor
has at least γk log k/α points, then Algorithm 3 outputs a (1 + 20α)-approximation to the k-means
objective with probability at least 3/4, using O(nd log n + poly(k, log n)) total time."
NEARLY OPTIMAL RUNTIME ALGORITHM,0.10603588907014681,"Note that if we wish to only output the k centers rather than labeling all of the input points, then the
query complexity of Algorithm 3 is eO(k/α) (see Step 1 of Algorithm 3) with high probability. We
show in the supplementary material that this is nearly optimal."
NEARLY OPTIMAL RUNTIME ALGORITHM,0.10766721044045677,"Theorem 3.5. For any δ ∈(0, 1], any algorithm that makes O

k1−δ
α log n

queries to the predictor"
NEARLY OPTIMAL RUNTIME ALGORITHM,0.10929853181076672,"with label error rate α cannot output a (1 + Cα)-approximation to the optimal k-means clustering
cost in time 2O(n1−δ) time, assuming the Exponential Time Hypothesis."
NEARLY OPTIMAL RUNTIME ALGORITHM,0.11092985318107668,Published as a conference paper at ICLR 2022
NEARLY OPTIMAL RUNTIME ALGORITHM,0.11256117455138662,Algorithm 3 Fast learning-augmented algorithm for k-means clustering.
NEARLY OPTIMAL RUNTIME ALGORITHM,0.11419249592169657,"Input: A point set X, a predictor Π with label error rate λ ≤α, and a tradeoff parameter ζ
Output: A (1 + α)-approximate k-means clustering of X"
NEARLY OPTIMAL RUNTIME ALGORITHM,0.11582381729200653,1: Form S by sampling each point of X with probability 100 log k
NEARLY OPTIMAL RUNTIME ALGORITHM,0.11745513866231648,"α|Ax|
where Ax is the set of points
with the same label as x according to Π.
2: Let C1, . . . , Ck be the output of Algorithm 1 on S.
3: Let φ2 be a random JL linear map with distortion 5"
NEARLY OPTIMAL RUNTIME ALGORITHM,0.11908646003262642,"4, i.e., dimension O(log n).
4: Let φ1 be a terminal dimension reduction with distortion 5"
NEARLY OPTIMAL RUNTIME ALGORITHM,0.12071778140293637,"4.
5: Let φ := φ1 ◦φ2 be the composition map.
6: Let A be a (2, r)-ANN data structure on the points φ(C1), . . . , φ(Ck).
7: for x ∈X do
8:
Let ℓx be the label of x from Π.
9:
ϱ ←d(x, Cℓx)
10:
Query A to ﬁnd the closest center φ(Cpx) to x with r = ϱ"
NEARLY OPTIMAL RUNTIME ALGORITHM,0.12234910277324633,"2.
11:
if d(x, Cpx) < 2d(x, Cℓx) then
12:
Assign label px to x.
13:
else
14:
Assign label ℓx to x.
15:
end if
16: end for"
EXPERIMENTS,0.12398042414355628,"4
EXPERIMENTS"
EXPERIMENTS,0.12561174551386622,"In this section we evaluate Algorithm 1 empirically on real datasets. We choose to implement
Algorithm 1, as opposed to the runtime optimal Algorithm 3, for simplicity and because the goal
of our experiments is to highlight the error guarantees of our methodology, which both algorithms
share. Further, we will see that Algorithm 1 is already very fast compared to alternatives. Thus,
we implement the simpler of the two algorithms. We primarily ﬁx the number of clusters to be
k = 10 and k = 25 throughout our experiments for all datasets. Note that our predictors can readily
generalize to other values of k but we focus on these two values for clarity. All of our experiments
were done on a CPU with i5 2.7 GHz dual core and 8 GB RAM. Furthermore, all our experimental
results are averaged over 20 independent trials and ± one standard deviation error is shaded when
applicable. We give the full details of our datasets below."
EXPERIMENTS,0.1272430668841762,"1) Oregon: Dataset of 9 graph snapshots sampled across 3 months from an internet router commu-
nication network (Leskovec et al., 2005). We then use the top two eigenvectors of the normalized
Laplacian matrix to give us node embeddings into R2 for each graph which gives us 9 datasets, one
for each graph. Each dataset has roughly n ∼104 points. This is an instance of spectral clustering.
2) PHY: Dataset from KDD cup 2004 (kdd, 2004). We take 104 random samples to form our dataset.
3) CIFAR10: Testing portion of CIFAR-10 (n = 104, dimension 3072) (Krizhevsky, 2009)."
EXPERIMENTS,0.12887438825448613,"Baselines.
We compare against the following algorithms.
Additional experimental results on
Lloyd’s heuristic are given in Section E.3 in the supplementary material."
EXPERIMENTS,0.13050570962479607,"1) kmeans++: We measure the performance of our algorithm in comparison to the kmeans++
seeding algorithm. Since kmeans++ is a randomized algorithm, we take the average clustering
cost after running kmeans++ seeding on 20 independent trials. We then standardize this value
to have cost 1.0 and report all other costs in terms of this normalization. For example, the cost
2.0 means that the clustering cost is twice that of the average kmeans++ clustering cost. We
also use the labels of kmeans++ as the predictor in the input for Algorithm 1 (denoted as “Alg
+ kmeans++”) which serves to highlight the fact that one can use any heuristic or approximate
clustering algorithm as a predictor."
EXPERIMENTS,0.13213703099510604,"2) Random sampling: For this algorithm, we subsample the predictor labels with probability q
ranging from 1% to 50%. We then construct the k-means centers using the labels of the sampled
points and measure the clustering cost using the whole dataset. We use the best value of q in our
range every time to give this baseline as much power as possible. We emphasize that random
sampling cannot have theoretical guarantees since the random sample can be corrupted (similarly"
EXPERIMENTS,0.13376835236541598,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.13539967373572595,"as in the example in Section 1.1). Thus some outlier detection steps (such as our algorithms) are
required."
EXPERIMENTS,0.1370309951060359,"Predictor Description.
We use the following predictors in our experiments."
EXPERIMENTS,0.13866231647634583,"1) Nearest neighbor: We use this predictor for the Oregon dataset. We ﬁnd the best clustering of
the node embeddings in Graph #1. In practice, this means running many steps of Lloyd’s algorithm
until convergence after initial seeding by kmeans++. Our predictor takes as input a point in R2
representing a node embedding of any of the later 8 graphs and outputs the label of the closest node
in Graph #1."
EXPERIMENTS,0.1402936378466558,"2) Noisy predictor. This is the main predictor for PHY. We form this predictor by ﬁrst ﬁnding the
best k-means solution on our datasets. This again means initial seeding by kmeans++ and then
many steps of Lloyd’s algorithm until convergence. We then randomly corrupt the resulting labels
by changing them to a uniformly random label independently with error probability ranging from 0
to 1. We report the cost of clustering using only these noisy labels versus processing these labels
using Algorithm 1."
EXPERIMENTS,0.14192495921696574,"3) Neural network. We use a standard neural network architecture (ResNet18) trained on the train-
ing portion of the CIFAR-10 dataset as the oracle for the testing portion which we use in our ex-
periments. We used a pretrained model obtained from Huy (2020). Note that the neural network
is predicting the class of the input image. However, the class value is highly correlated with the
optimal k-means cluster group."
EXPERIMENTS,0.14355628058727568,"Summary of results.
Our experiments show that our algorithm can leverage predictors to signiﬁ-
cantly improve the cost of k-means clustering and that good predictors can be easily tailored to the
data at hand. The cost of k-means clustering reduces signiﬁcantly after applying our algorithm com-
pared to just using the predictor labels for two of our predictors. Lastly, the quality of the predictor
remains high for the Oregon dataset even though the later graphs have changed and “moved away”
from Graph #1."
EXPERIMENTS,0.14518760195758565,"Selecting α in Algorithm 2.
In practice, the choice of α to use in our algorithm depends on the
given predictor whose properties may be unknown. Since our goal is to minimize the k-means
clustering objective (1), we can simply pick the ‘best’ value. To do so, we iterate over a small range
of possible α from .01 to .15 in Algorithm 2 with a step size of 0.01 and select the clustering that
results in the lowest objective cost. The range is ﬁxed for all of our experiments. (See Paragraph 2.1"
RESULTS,0.1468189233278956,"4.1
RESULTS"
RESULTS,0.14845024469820556,"2
3
4
5
6
7
8
9
Graph # 0.0 0.1 0.2 0.3 0.4 0.5"
RESULTS,0.1500815660685155,Clustering Cost
RESULTS,0.15171288743882544,"Dataset: Oregon Spectral Clustering, k=10"
RESULTS,0.1533442088091354,"Alg + Predictor
Alg + k-means++
Random Sampling
Predictor"
RESULTS,0.15497553017944535,(a) k = 10
RESULTS,0.1566068515497553,"2
3
4
5
6
7
8
9
Graph # 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6"
RESULTS,0.15823817292006526,Clustering Cost
RESULTS,0.1598694942903752,"Dataset: Oregon Spectral Clustering, k=25"
RESULTS,0.16150081566068517,"Alg + Predictor
Alg + k-means++
Random Sampling
Predictor"
RESULTS,0.1631321370309951,(b) k = 25
RESULTS,0.16476345840130505,"0
5
10
15
20
25
Corruption % 0 2 4 6 8"
RESULTS,0.16639477977161501,Clustering Cost
RESULTS,0.16802610114192496,"Dataset: Oregon Spectral Clustering, Graph #5, k=10"
RESULTS,0.16965742251223492,"Alg + Noisy Predictor
Random Sampling
Noisy Predictor
kmeans++"
RESULTS,0.17128874388254486,(c) k = 25
RESULTS,0.1729200652528548,Figure 1: Performance of Algorithm 1 on later graph embeddings using Graph #1 as predictor.
RESULTS,0.17455138662316477,"Oregon.
We ﬁrst compare our algorithm with Graph #1 as the predictor against various base-
lines. This is shown in Figures 1(a) and Figure 1(b). In the k = 10 case, Figure 1(a) shows that
the predictor returns a clustering better than using just the kmeans++ seeding, which is normalized
to have cost 1.0. This is to be expected since the subsequent graphs represent a similar network as
Graph #1, just sampled later in time. However, the clustering improves signiﬁcantly after using
our algorithm on the predictor labels as the average cost drops by 55%. We also see that using our
algorithm after kmeans++ is also sufﬁcient to give signiﬁcant decrease in clustering cost. Lastly,"
RESULTS,0.1761827079934747,Published as a conference paper at ICLR 2022
RESULTS,0.17781402936378465,"0
10
20
30
40
50
Corruption % 0.0 0.5 1.0 1.5 2.0 2.5"
RESULTS,0.17944535073409462,Clustering Cost
RESULTS,0.18107667210440456,"Dataset: PHY, k=10"
RESULTS,0.18270799347471453,"Alg + Noisy Predictor
Alg + k-means++
Random Sampling
Noisy Predictor
kmeans++"
RESULTS,0.18433931484502447,"(a) PHY, k = 10"
RESULTS,0.1859706362153344,"0.0
0.2
0.4
0.6
0.8
1.0
Fraction Queried 0 2 4 6 8 10 12"
RESULTS,0.18760195758564438,Percent Increase in Clustering Cost
RESULTS,0.18923327895595432,"Dataset: CIFAR-10, k=10"
RESULTS,0.19086460032626426,"(b) CIFAR-10, k = 10"
RESULTS,0.19249592169657423,Figure 2: Our algorithm is able to recover a good clustering even for very high levels of noise.
RESULTS,0.19412724306688417,"random sampling also gives comparable results. This can be explained because we are iterating over
a large range of subsampling probabilities for random sampling."
RESULTS,0.19575856443719414,"In the k = 25 case, Figure 1(b) shows that the oracle performance degrades and is worse than the
baseline in 5 of the 8 graphs. However our algorithm again improves the quality of the clustering
over the oracle across all graphs. Using kmeans++ as the predictor in our algorithm also improves
the cost of clustering. The performance of random sampling is also worse. For example in Graph
#3 for k = 25, it performed the worst out of all the tested algorithms."
RESULTS,0.19738988580750408,"Our algorithm also remains competitive with kmeans++ seeding even if the predictor for the Ore-
gon dataset is highly corrupted. We consider a later graph, Graph #5, and corrupt the labels of
the predictor randomly with probability q ranging from 1% to 25% for the k = 10 case in Figure
1(c). While the cost of clustering using just the predictor labels can become increasingly worse, our
algorithm is able to sufﬁciently “clean” the predictions. In addition, the cost of random sampling
also gets worse as the corruptions increase, implying that it is much more sensitive to noise than our
algorithm. The qualitatively similar plot for k = 25 is given in the supplementary section. Note that
in spectral clustering, one may wish to get a mapping to Rd for d > 2. We envision that our results
translate to those settings as well since having higher order spectral information only results in a
stronger predictor. We continue the discussion on the PHY and CIFAR-10 datasets in Section E."
RESULTS,0.19902120717781402,"Comparison to Lloyd’s Heuristic.
In Section E.3, we provide additional results on experiments
using Lloyd’s heuristic. In summary, we give both theoretical and empirical justiﬁcations for why
our algorithms are superior to blindly following a predictor and then running Lloyd’s heuristic."
RESULTS,0.200652528548124,ACKNOWLEDGEMENTS
RESULTS,0.20228384991843393,"Zhili Feng, David P. Woodruf, and Samson Zhou would like to thank partial support from NSF
grant No. CCF- 181584, Ofﬁce of Naval Research (ONR) grant N00014-18-1-256, and a Simons
Investigator Award. Sandeep Silwal was supported in part by a NSF Graduate Research Fellowship
Program."
REFERENCES,0.2039151712887439,REFERENCES
REFERENCES,0.20554649265905384,"Kdd cup. http://osmot.cs.cornell.edu/kddcup/datasets.html, 2004."
REFERENCES,0.20717781402936378,"Manu Agarwal, Ragesh Jaiswal, and Arindam Pal. k-means++ under approximation stability. Theor.
Comput. Sci., 588:37–51, 2015."
REFERENCES,0.20880913539967375,"Sara Ahmadian, Ashkan Norouzi-Fard, Ola Svensson, and Justin Ward. Better guarantees for k-
means and euclidean k-median by primal-dual algorithms. SIAM J. Comput., 49(4), 2020."
REFERENCES,0.21044045676998369,"Nir Ailon, Anup Bhattacharya, Ragesh Jaiswal, and Amit Kumar. Approximate Clustering with
Same-Cluster Queries. In 9th Innovations in Theoretical Computer Science Conference (ITCS),
pp. 40:1–40:21, 2018."
REFERENCES,0.21207177814029363,Published as a conference paper at ICLR 2022
REFERENCES,0.2137030995106036,"Alexandr Andoni, Piotr Indyk, and Ilya P. Razenshteyn. Approximate nearest neighbor search in
high dimensions. CoRR, abs/1806.09823, 2018."
REFERENCES,0.21533442088091354,"Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cam-
bridge University Press, 1999. doi: 10.1017/CBO9780511624216."
REFERENCES,0.2169657422512235,"David Arthur and Sergei Vassilvitskii. How slow is the k-means method? In SCG ’06, 2006."
REFERENCES,0.21859706362153344,"David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In Pro-
ceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA, pp.
1027–1035, 2007."
REFERENCES,0.22022838499184338,"Hassan Ashtiani, Shrinu Kushagra, and Shai Ben-David. Clustering with same-cluster queries. In
Advances in Neural Information Processing Systems 29, pp. 3216–3224, 2016."
REFERENCES,0.22185970636215335,"Pranjal Awasthi, Maria Florina Balcan, and Konstantin Voevodski. Local algorithms for interactive
clustering. Journal of Machine Learning Research, 18(3):1–35, 2017."
REFERENCES,0.2234910277324633,"Pranjal Awasthi, Ainesh Bakshi, Maria-Florina Balcan, Colin White, and David P. Woodruff. Robust
communication-optimal distributed clustering algorithms. In 46th International Colloquium on
Automata, Languages, and Programming, ICALP, pp. 18:1–18:16, 2019."
REFERENCES,0.22512234910277323,"Maria-Florina Balcan and Avrim Blum. Clustering with interactive feedback. In Proceedings of the
19th International Conference on Algorithmic Learning Theory, pp. 316–328, 2008."
REFERENCES,0.2267536704730832,"Maria-Florina Balcan, Avrim Blum, and Anupam Gupta. Clustering under approximation stability.
J. ACM, 60(2):8:1–8:34, 2013."
REFERENCES,0.22838499184339314,"Maria-Florina Balcan, Nika Haghtalab, and Colin White. k-center clustering under perturbation
resilience. ACM Trans. Algorithms, 16(2):22:1–22:39, 2020."
REFERENCES,0.2300163132137031,"Sugato Basu, Arindam Banerjee, and Raymond J. Mooney. Active semi-supervision for pairwise
constrained clustering. In Proceedings of the 2004 SIAM International Conference on Data Min-
ing (SDM-04), April 2004."
REFERENCES,0.23164763458401305,"Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G. Dimakis. Compressed sensing using gener-
ative models. In Proceedings of the 34th International Conference on Machine Learning, ICML,
pp. 537–546, 2017."
REFERENCES,0.233278955954323,"Vladimir Braverman, Adam Meyerson, Rafail Ostrovsky, Alan Roytman, Michael Shindler, and
Brian Tagiku. Streaming k-means on well-clusterable data. In Proceedings of the Twenty-Second
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA, pp. 26–40. SIAM, 2011."
REFERENCES,0.23491027732463296,"I Chien, Chao Pan, and Olgica Milenkovic. Query k-means clustering and the double dixie cup
problem. In Advances in Neural Information Processing Systems 31, pp. 6649–6658. 2018."
REFERENCES,0.2365415986949429,"Miroslav Chleb´ık and Janka Chleb´ıkov´a. Complexity of approximating bounded variants of opti-
mization problems. Theor. Comput. Sci., 354(3):320–338, 2006."
REFERENCES,0.23817292006525284,"Michael B. Cohen, Yin Tat Lee, Gary L. Miller, Jakub Pachocki, and Aaron Sidford. Geometric
median in nearly linear time. In Proceedings of the 48th Annual ACM SIGACT Symposium on
Theory of Computing, STOC, pp. 9–21, 2016."
REFERENCES,0.2398042414355628,"Vincent Cohen-Addad and Karthik C. S. Inapproximability of clustering in lp metrics. 2019 IEEE
60th Annual Symposium on Foundations of Computer Science (FOCS), pp. 519–539, 2019."
REFERENCES,0.24143556280587275,"Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial
optimization algorithms over graphs. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pp. 6351–6361, 2017."
REFERENCES,0.24306688417618272,"Sanjoy Dasgupta.
How fast is k-means?
In Computational Learning Theory and Kernel Ma-
chines, 16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop,
COLT/Kernel, Proceedings, pp. 735, 2003."
REFERENCES,0.24469820554649266,Sanjoy Dasgupta. The hardness of k-means clustering. 2008.
REFERENCES,0.2463295269168026,Published as a conference paper at ICLR 2022
REFERENCES,0.24796084828711257,"Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster
matchings via learned duals. In Advances in Neural Information Processing Systems, 2021. URL
https://arxiv.org/abs/2107.09770."
REFERENCES,0.2495921696574225,"Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Learning space partitions for nearest
neighbor search. In International Conference on Learning Representations, 2020."
REFERENCES,0.25122349102773245,"Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wagner.
Learning-based support estimation in sublinear time. In International Conference on Learning
Representations, 2021."
REFERENCES,0.2528548123980424,"Dan Feldman, Morteza Monemizadeh, and Christian Sohler. A PTAS for k-means clustering based
on weak coresets. In Proceedings of the 23rd ACM Symposium on Computational Geometry, pp.
11–18, 2007."
REFERENCES,0.2544861337683524,"Dimitris Fotakis, Michael Lampis, and Vangelis Th. Paschos.
Sub-exponential approximation
schemes for csps: From dense to almost sparse.
In 33rd Symposium on Theoretical Aspects
of Computer Science, STACS, pp. 37:1–37:14, 2016."
REFERENCES,0.2561174551386623,"Buddhima Gamlath, Sangxia Huang, and Ola Svensson. Semi-Supervised Algorithms for Approx-
imately Optimal and Accurate Clustering. In 45th International Colloquium on Automata, Lan-
guages, and Programming (ICALP), pp. 57:1–57:14, 2018."
REFERENCES,0.25774877650897227,"Sariel Har-Peled and Bardia Sadri. How fast is the k-means method? Algorithmica, 41:185–202,
2005."
REFERENCES,0.25938009787928223,"Sariel Har-Peled, Piotr Indyk, and Rajeev Motwani. Approximate nearest neighbor: Towards re-
moving the curse of dimensionality. Theory Comput., 8(1):321–350, 2012."
REFERENCES,0.26101141924959215,"Johan H˚astad. Some optimal inapproximability results. J. ACM, 48(4):798–859, 2001."
REFERENCES,0.2626427406199021,"J. A. Howe. Improved clustering with augmented k-means. arXiv: Machine Learning, 2017."
REFERENCES,0.2642740619902121,"Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation
algorithms. In International Conference on Learning Representations, 2019."
REFERENCES,0.265905383360522,"Wasim Huleihel, Arya Mazumdar, Muriel Medard, and Soumyabrata Pal. Same-cluster querying
for overlapping clusters. In Advances in Neural Information Processing Systems 32, pp. 10485–
10495. 2019."
REFERENCES,0.26753670473083196,"Phan Huy.
Pytorchcifar10.
https://github.com/huyvnphan/PyTorch_CIFAR10,
2020."
REFERENCES,0.26916802610114193,"Russell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. J. Comput. Syst. Sci., 62
(2):367–375, 2001."
REFERENCES,0.2707993474714519,"Mary Inaba, Naoki Katoh, and Hiroshi Imai. Applications of weighted voronoi diagrams and ran-
domization to variance-based k-clustering (extended abstract). In Proceedings of the Tenth Annual
Symposium on Computational Geometry, pp. 332–339, 1994."
REFERENCES,0.2724306688417618,"Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse
of dimensionality. In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of
Computing, pp. 604–613, 1998."
REFERENCES,0.2740619902120718,"Zachary Izzo, Sandeep Silwal, and Samson Zhou.
Dimensionality reduction for wasserstein
barycenter. In Advances in Neural Information Processing Systems, 2021."
REFERENCES,0.27569331158238175,"Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodruff. Learning-augmented data
stream algorithms. In International Conference on Learning Representations, 2020."
REFERENCES,0.27732463295269166,"William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space,
1984."
REFERENCES,0.27895595432300163,"Taewan Kim and Joydeep Ghosh.
Relaxed oracles for semi-supervised clustering.
CoRR,
abs/1711.07433, 2017."
REFERENCES,0.2805872756933116,Published as a conference paper at ICLR 2022
REFERENCES,0.2822185970636215,"Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned
index structures. In Proceedings of the 2018 International Conference on Management of Data,
pp. 489–504, 2018."
REFERENCES,0.2838499184339315,"Robert Krauthgamer.
Randomized algorithms course notes.
http://www.wisdom.
weizmann.ac.il/˜robi/teaching/2019a-RandomizedAlgorithms/
lecture8.pdf, 2019. Lecture 8."
REFERENCES,0.28548123980424145,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.28711256117455136,"Amit Kumar, Yogish Sabharwal, and Sandeep Sen. A simple linear time (1 + ϵ)-approximation
algorithm for k-means clustering in any dimensions. pp. 454–462, 2004."
REFERENCES,0.28874388254486133,"Silvio Lattanzi and Christian Sohler. A better k-means++ algorithm via local search. In Proceedings
of the 36th International Conference on Machine Learning, ICML, pp. 3662–3671, 2019."
REFERENCES,0.2903752039151713,"Euiwoong Lee, Melanie Schmidt, and John Wright. Improved and simpliﬁed inapproximability for
k-means. Inf. Process. Lett., 120:40–43, 2017."
REFERENCES,0.29200652528548127,"Jure Leskovec, Jon M. Kleinberg, and Christos Faloutsos. Graphs over time: densiﬁcation laws,
shrinking diameters and possible explanations. In Proceedings of the Eleventh ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 177–187, 2005."
REFERENCES,0.2936378466557912,"Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training gaussian mixture
models at scale via coresets. Journal of Machine Learning Research, 18(160):1–25, 2018. URL
http://jmlr.org/papers/v18/15-506.html."
REFERENCES,0.29526916802610115,"Thodoris Lykouris and Sergei Vassilvtiskii. Competitive caching with machine learned advice. vol-
ume 80 of Proceedings of Machine Learning Research, pp. 3296–3305. PMLR, 2018."
REFERENCES,0.2969004893964111,"Konstantin Makarychev, Yury Makarychev, and Ilya P. Razenshteyn.
Performance of johnson-
lindenstrauss transform for k-means and k-medians clustering. In Proceedings of the 51st Annual
ACM SIGACT Symposium on Theory of Computing, STOC, pp. 1027–1038, 2019."
REFERENCES,0.29853181076672103,"Arya Mazumdar and Barna Saha. Clustering with noisy queries. In Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information Processing Systems, pp. 5788–
5799, 2017."
REFERENCES,0.300163132137031,"Michael Mitzenmacher. A model for learned bloom ﬁlters, and optimizing by sandwiching. In
Proceedings of the 32nd International Conference on Neural Information Processing Systems,
pp. 462–471, 2018."
REFERENCES,0.30179445350734097,"Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions, 2020."
REFERENCES,0.3034257748776509,"Ali Mousavi, Ankit B. Patel, and Richard G. Baraniuk. A deep learning approach to structured sig-
nal recovery. In 53rd Annual Allerton Conference on Communication, Control, and Computing,
Allerton, pp. 1336–1343, 2015."
REFERENCES,0.30505709624796085,"Adarsh Prasad, Sivaraman Balakrishnan, and Pradeep Ravikumar. A uniﬁed approach to robust
mean estimation. CoRR, abs/1907.00927, 2019."
REFERENCES,0.3066884176182708,"Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions.
In Advances in Neural Information Processing Systems 31, pp. 9661–9670. 2018."
REFERENCES,0.3083197389885807,"Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herv´e J´egou. Spreading vectors for
similarity search. In International Conference on Learning Representations, 2019."
REFERENCES,0.3099510603588907,"Thomas Sanchez, Baran G¨ozc¨u, Ruud B. van Heeswijk, Armin Eftekhari, Efe Ilicak, Tolga C¸ ukur,
and Volkan Cevher. Scalable learning-based sampling optimization for compressive dynamic
MRI. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP,
pp. 8584–8588, 2020."
REFERENCES,0.31158238172920066,"Mingjun Song and S. Rajasekaran. Fast algorithms for constant approximation k-means clustering.
Trans. Mach. Learn. Data Min., 3:67–79, 2010."
REFERENCES,0.3132137030995106,Published as a conference paper at ICLR 2022
REFERENCES,0.31484502446982054,"Andrea Vattani. k-means requires exponentially many iterations even in the plane. Discrete &
Computational Geometry, 45:596–616, 2011."
REFERENCES,0.3164763458401305,"Sharad Vikram and Sanjoy Dasgupta. Interactive bayesian hierarchical clustering. volume 48 of
Proceedings of Machine Learning Research, pp. 2081–2090. PMLR, 2016."
REFERENCES,0.3181076672104405,"J. Wang, W. Liu, S. Kumar, and S. Chang.
Learning to hash for indexing big data—a survey.
Proceedings of the IEEE, 104(1):34–57, 2016."
REFERENCES,0.3197389885807504,Published as a conference paper at ICLR 2022
REFERENCES,0.32137030995106036,"A
APPENDIX"
REFERENCES,0.32300163132137033,"Theorem A.1 (Chernoff Bounds). Let X1, . . . , Xn be independent random variables taking values
in {0, 1}. Let X = Pn
i=1 Xi denote their sum and let µ = E[X] denote the sum’s expected value.
Then for any δ ∈(0, 1) and t > 0,"
REFERENCES,0.32463295269168024,Pr [X ≤(1 −δ)µ] ≤e−δ2µ 2 .
REFERENCES,0.3262642740619902,"For any δ > 0,"
REFERENCES,0.3278955954323002,Pr [X ≥(1 + δ)µ] ≤e−δ2µ 3 .
REFERENCES,0.3295269168026101,"Furthermore,"
REFERENCES,0.33115823817292006,Pr [|X −µ| ≥t] ≤e−t2 4n .
REFERENCES,0.33278955954323003,"A.1
PROOF OF THEOREM 2.1"
REFERENCES,0.33442088091353994,"We ﬁrst prove Theorem 2.1, which shows that Algorithm 1 provides a (1 + α)-approximation to the
optimal k-means clustering, but uses suboptimal time compared to a faster algorithm we present in
Section 3. All omitted proofs of lemmas appear in Section A.2."
REFERENCES,0.3360522022838499,"We ﬁrst show that for each coordinate, the empirical center for any (1 −α)-fraction of the input
points provides a good approximation to the optimal k-means clustering cost."
REFERENCES,0.3376835236541599,"Lemma A.2. Let P, Q ⊆R be sets of points on the real line such that |P| ≥(1 −α)n and
|Q| ≤αn. Let X = P ∪Q, CP be the mean of P and CX be the mean of X. Then cost(X, CP ) ≤

1 +
α
1−α2

cost(X, CX)."
REFERENCES,0.33931484502446985,"We now show that a conceptual interval I∗⊂R with “small” length contains a signiﬁcant fraction
of the true points. Ultimately, we will show that the interval I computed in the “training” phase
in CRDEST has smaller length than I∗with high probability and yet I also contains a signiﬁcant
fraction of the true points. The main purpose of I∗(and eventually I) is to ﬁlter out extreme outliers
because the “testing” phase only considers points in I ∩X2."
REFERENCES,0.34094616639477976,"Lemma A.3. For a ﬁxed set X ⊆R, let C be the mean of X and σ2 =
1
2|X|
P"
REFERENCES,0.3425774877650897,x∈X(x −C)2 be the
REFERENCES,0.3442088091353997,"variance. Then the interval I∗=
h
C −
σ
√α, C +
σ
√α
i
contains at least a (1 −4α) fraction of the
points in X."
REFERENCES,0.3458401305057096,"Using Lemma A.3, we show that the interval I that is computed in the “training” phase contains a
signiﬁcant fraction of the true points."
REFERENCES,0.3474714518760196,"Lemma A.4. Let m be a sufﬁciently large consatnt. We have that I := [a, b] contains at least a
1 −6α fraction of points of X2 and b −a ≤2σ/√α, with high probability, i.e., 1 −1/ poly(m)."
REFERENCES,0.34910277324632955,"We next show that the optimal clustering on a subset obtained by independently sampling each
input point provides a rough approximation of the optimal clustering. That is, the optimal center is
well-approximated by the empirical center of the sampled points."
REFERENCES,0.35073409461663946,"Lemma A.5. Let S be a set of points obtained by independently sampling each point of X ⊆Rd
with probability p = 1"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3523654159869494,"2. Let C be the optimal center of these points and CS be the empirical center
of these points. Conditioned on |S| ≥1, then E[CS] = x and there exists a constant γ such that for
η ≥1 and |X| > ηγk α ,"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3539967373572594,"E

∥CS −x∥2
2

≤
γ
|X|2 · X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3556280587275693,"x∈X
∥x −x∥2
2 !"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3572593800978793,"Pr [cost(X, CS) > (1 + α) cost(X, C)] < 1/(ηk)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.35889070146818924,"Using Lemma A.2, Lemma A.4, and Lemma A.5, we justify the correctness of the subroutine
CRDEST."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3605220228384992,Published as a conference paper at ICLR 2022
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3621533442088091,"Lemma A.6. Let α ∈(10 log n/√n, 1/7). Let P, Q ⊆R be sets of points on the real line such that
|P| ≥(1 −α)2m and |Q| ≤2αm, and X = P ∪Q. Let C be the center of P. Then CRDEST
on input set X outputs a point C′ such that with probability at least 1 −1/(ηk), cost(P, C′) ≤"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3637846655791191,"(1 + 18α)(1 + α)

1 +
α
(1−α)2

cost(P, C)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.36541598694942906,"Using CRDEST as a subroutine for each coordinate, we now prove Theorem 2.1, justifying the
correctness of Algorithm 1 by generalizing to all coordinates and centers and analyzing the runtime
of Algorithm 1."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.367047308319739,"Proof of Theorem 2.1. Since Π has label error rate λ ≤α, then by deﬁnition of label error rate,
at least a (1 −α) fraction of the points in each cluster are correctly labeled. Note that the k-
means clustering cost can be decomposed into the sum of the costs induced by the centers in each
dimension. Speciﬁcally, for a set C = {C1, . . . , Ck} of optimal centers,"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.36867862969004894,"cost(X, C) :=
X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3703099510603589,"x∈X
d(x, C)2 = k
X i=1 X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3719412724306688,"x∈Si
d(x, Ci)2,"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3735725938009788,"where Si is the set of points in X that are assigned to center Ci. For a particular i ∈[k], we have X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.37520391517128876,"x∈Si
d(x, Ci)2 =
X x∈Si d
X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3768352365415987,"j=1
d(xj, (Ci)j)2,"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.37846655791190864,"where xj and (Ci)j are the j-th coordinate of x and Ci, respectively."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3800978792822186,"By Lemma A.6, the cost induced by CRDEST for each dimension in each center C′
i is a (1 + α)-
approximation of the total clustering cost for the optimal center Ci in that dimension with probability
1 −1/(ηk). That is,
X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3817292006525285,"x∈Si
d(xj, (C′
i)j)2 ≤(1 + 18α)(1 + α)(1 + α/(1 −α)2)
X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3833605220228385,"x∈Si
d(xj, (Ci)j)2"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.38499184339314846,"for each j ∈[d]. Thus, taking a sum over all dimensions j ∈[d] and union bounding over all centers
i ∈[k], we have that the total cost induced by Algorithm 1 is a (1 + 20α)-approximation to the
optimal k-means clustering cost with probability at least 1 −1/η."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3866231647634584,"To analyze the time complexity of Algorithm 1, ﬁrst consider the subroutine CRDEST. It takes
O(kdn) time to ﬁrst split each of the points in each cluster and dimension into two disjoint groups.
Finding the smallest interval that contains a certain number of points can be done by ﬁrst sorting
the points and then iterating from the smallest point to the largest point and taking the smallest
interval that contains enough points. This requires O(n log n) time for each dimension and each
center, which results in O(kdn log n) total time. Once each of the intervals is found, computing the
approximate center then takes O(kdn) total time. Hence, the total running time of Algorithm 1 is
O(kdn log n)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.38825448613376834,"A.2
PROOF OF AUXILIARY LEMMAS"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3898858075040783,"Lemma A.2. Let P, Q ⊆R be sets of points on the real line such that |P| ≥(1 −α)n and
|Q| ≤αn. Let X = P ∪Q, CP be the mean of P and CX be the mean of X. Then cost(X, CP ) ≤

1 +
α
1−α2

cost(X, CX)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3915171288743883,"Proof. Suppose without loss of generality, that CX = 0 and CP ≤0, so that CQ ≥0, where CQ is
the mean of Q. Then it is well-known, e.g., see Inaba et al. (1994), that"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3931484502446982,"cost(X, CP ) = cost(X, CX) + |X| · |CP −CX|2."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.39477977161500816,"Hence, it sufﬁces to show that |X| · |CP −CX|2 ≤
α
(1−α)2 cost(X, CX)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.3964110929853181,Published as a conference paper at ICLR 2022
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.39804241435562804,"Since CX = 0 we have |P|·CP = −|Q|·CQ, with |P| ≥(1−α)n and |Q| ≤αn. Let |P| = (1−ϱ)n
and |Q| = ϱn for some ϱ ≤α. Thus, CQ = −1−ϱ"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.399673735725938,"ϱ
· CP . By convexity, we thus have that"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.401305057096248,"cost(Q, CX) ≥|Q| · (1 −ϱ)2"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.4029363784665579,"ϱ2
· |CP |2"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.40456769983686786,= n(1 −ϱ)2
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.4061990212071778,"ϱ
· |CP |2"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.4078303425774878,≥n(1 −α)2
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.4094616639477977,"α
· |CP |2."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.4110929853181077,"Therefore, we have"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.41272430668841764,"|CP −CX|2 = |CP |2 ≤
α
n(1 −α)2 cost(Q, CX) ≤
α
n(1 −α)2 cost(X, CX)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.41435562805872755,"Thus,
|X| · |CP −CX|2 ≤
α
(1 −α)2 cost(X, CX),"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.4159869494290375,as desired.
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.4176182707993475,"Lemma A.3. For a ﬁxed set X ⊆R, let C be the mean of X and σ2 =
1
2|X|
P"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.4192495921696574,x∈X(x −C)2 be the
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.42088091353996737,"variance. Then the interval I∗=
h
C −
σ
√α, C +
σ
√α
i
contains at least a (1 −4α) fraction of the
points in X."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.42251223491027734,"Proof. Note that any point x ∈X \ I∗satisﬁes |x −C|2 > σ2/(4α). Thus, if more than a 4α
fraction of the points of X are outside of I∗, then the total variance is larger than σ2, which is a
contradiction."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.42414355628058725,"For ease of presentation, we analyze λ = 1"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4257748776508972,"2 and we note that the analysis extends easily to general
λ. We now prove the technical lemma that we will use in the proof of Lemma A.8."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4274061990212072,"Lemma A.7. We have
m
X j=1"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4290375203915171," m
j
"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.43066884176182707,"j · 2m = Θ
 1 m 
."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.43230016313213704,"Proof. Let m be sufﬁciently large. A Chernoff bound implies that for a sufﬁciently large constant
C,
X"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.433931484502447,|j−m/2|≥C√m
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4355628058727569," m
j
"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4371941272430669,"2m ≤
1
m2 ."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.43882544861337686,"Furthermore,
X j≥C′m"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.44045676998368677," m
j
"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.44208809135399674,"j · 2m = O
 1 m 
·
X j≥1"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4437194127243067," m
j
"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4453507340946166,"2m = O
 1 m "
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4469820554649266,so the upper bound on the desired relation holds. A similar analysis provides a lower bound.
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.44861337683523655,"Lemma A.4. Let m be a sufﬁciently large consatnt. We have that I := [a, b] contains at least a
1 −6α fraction of points of X2 and b −a ≤2σ/√α, with high probability, i.e., 1 −1/ poly(m)."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.45024469820554647,"Proof. By Lemma A.3, I∗contains at least 2m(1 −4α) of the points in X. Hence, by applying
an additive Chernoff bound for t = O(√m log m) and for sufﬁciently large m, we have that the
number of points in I∗∩X1 is at least m(1 −5α) with high probability. Since I is the interval
of minimal length with at least m(1 −5α) points, then the length of I is at most the length of I∗.
Moreover, again applying Chernoff bounds, we have that the number of points in I ∩X2 is at least
m(1 −6α)."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.45187601957585644,"More formally, suppose we have a set of 2m points that we randomly partition into two sets X1
and X2. Consider any ﬁxed interval J that has at least 2cm total points for c ≥1 −5α (note there"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4535073409461664,Published as a conference paper at ICLR 2022
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4551386623164764,"are at most O(m2) intervals in total since our points are in one dimension). Let J1 and J2 denote
the number of points in J that are in X1 and X2 respectively. By a Chernoff bound, we have that
both J1 and J2 are at least mc(1 −α) with high probability. In particular, |J1 −J2| ≤αmc with
high probability. Thus by using a union bound, all intervals with at least cm total points satisfy
the property that the number of points partitioned to X1 and the number of points partitioned to
X2 differ by at most αmc with high probability. Conditioning on this event, I must also contain
m(1 −6α) points in X2 since it contains at least m(1 −5α) points in X1, as desired."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4567699836867863,"Lemma A.8. Let S be a set of points obtained by independently sampling each point of X ⊆Rd
with probability 1"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.45840130505709625,"2, and let CS be the centroid of S. Let x be the centroid of X. Conditioned on
|S| ≥1, we have E[CS] = x, and there exists a constant γ such that"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4600326264274062,"E

∥CS −x∥2
2

≤
γ
|X|2 · X"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.46166394779771613,"x∈X
∥x −x∥2
2 ! ."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4632952691680261,"Proof. We ﬁrst prove that E[CS] = x. Note that by the law of iterated expectations,"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.46492659053833607,E[CS] = E|S|E[CS | |S| ].
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.466557911908646,"Let xi1, . . . , xi|S| be a random permutation of the elements in S, so that for each 1 ≤j ≤|S|, we
have E[xij] = x. Now conditioning on the size of S, we can write"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.46818923327895595,"CS = xi1 + · · · + xi|S| |S|
."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4698205546492659,"Therefore,"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.47145187601957583,E[CS | |S| ] = x · |S|
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4730831973898858,"|S|
= x"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.47471451876019577,and it follows that E[CS] = x.
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4763458401305057,To prove that
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.47797716150081565,"E

∥CS −x∥2
≤
γ
|X|2 · X"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4796084828711256,"x∈X
∥x −x∥2
! ,"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4812398042414356,"we again condition on |S|. Suppose that |S| = j. Then,"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4828711256117455,CS −x = (xi1 −x) + · · · + (xij −x) j
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.48450244698205547,"Now let yit = xit −x for all 1 ≤t ≤j. Therefore,"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.48613376835236544,"E
|S|=j"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.48776508972267535,"
∥CS −x∥2
= 1"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4893964110929853,"j2 · E

∥yi1 + · · · + yij∥2 = 1"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4910277324632953,j · E[∥yi1∥2] + j −1
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.4926590538336052,"j
· E[yT
i1yi2]."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.49429037520391517,"Note that xi1 is uniform over elements in X, so it follows that"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.49592169657422513,"E[∥yi1∥2] =
1
|X| X"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.49755301794453505,"x∈X
∥x −x∥2."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.499184339314845,"Now if j ≥2, we have that"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5008156606851549,"E[yT
i1yi2] =
P"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5024469820554649,"a<b yT
a yb
 |X|
2

= ∥P"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5040783034257749,i yi∥2 −P
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5057096247960848,i ∥yi∥2
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5073409461663948,"|X|(|X| −1)
≤0"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5089722675367048,since P
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5106035889070146,"i yi = 0 by deﬁnition. Hence,"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5122349102773246,"E
|S|≥2"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5138662316476346,"
∥CS −x∥2
≤
1
j · |X| X"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5154975530179445,"x∈X
∥x −x∥2."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5171288743882545,Published as a conference paper at ICLR 2022
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5187601957585645,"Now the probability that |S| = j for j ≥2 is precisely
 |X|
j

/2|X|, so we have"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5203915171288744,"Pr [|S| ≥2] · E
|S|≥2"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5220228384991843,"
∥CS −x∥2"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5236541598694943,"≤
1
|X| · X"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5252854812398042,"x∈X
∥x −x∥2
! · |X|
X j=1"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5269168026101142," |X|
j
"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5285481239804242,j · 2|X| .
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5301794453507341,"From Lemma A.7, we have that
|X|
X j=1"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.531810766721044," |X|
j
"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.533442088091354,"j · 2|X| ≤
c
|X|"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5350734094616639,for some constant c so it follows that
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5367047308319739,"E∥CS −x∥2 ≤
c′"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5383360522022839,|X|2 · X
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5399673735725938,"x∈X
∥x −x∥2
!"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5415986949429038,for some constant c′.
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5432300163132137,"For j = 1, note that"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5448613376835236,"E
|S|=j=1"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5464926590538336,"
∥CS −x∥2
=
1
|X| X"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5481239804241436,"x∈X
∥x −x∥2."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5497553017944535,"Moreover, we have Pr [|S| = 1] =
|X|
2|X| and Pr [|S| = 0] =
1
2|X| . Thus from the law of total
expectation, we have"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5513866231647635,"E

∥CS −x∥2
= Pr [|S| < 2] ·
E
|S|<2"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5530179445350734,"
∥CS −x∥2"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5546492659053833,"+ Pr [|S| ≥2] ·
E
|S|≥2"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5562805872756933,"
∥CS −x∥2 ≤|X|"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5579119086460033,"2|X| ·
1
|X| X x∈X"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5595432300163132," 
∥x −x∥2 +
c′"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5611745513866232,|X|2 · X
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5628058727569332,"x∈X
∥x −x∥2
!"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.564437194127243,"≤
γ
|X|2 · X"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.566068515497553,"x∈X
∥x −x∥2
!"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.567699836867863,"for some constant γ, as desired."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5693311582381729,"Lemma A.9. Let S be a set of points obtained by independently sampling each point of X ⊆Rd
with probability p = 1"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5709624796084829,"2. Let C be the optimal center of X and CS be the empirical center of S. Let
γ ≥1 be the constant from Lemma A.8. Then for η ≥1 and |X| > ηγk α ,"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5725938009787929,"Pr [cost(X, CS) > (1 + α) cost(X, C)] < 1/(ηk)."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5742251223491027,"Proof. By Lemma A.8 and Markov’s inequality, we have Pr """
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5758564437194127,"∥CS −C∥2
2 ≥ηγk"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5774877650897227,"|X|2
X"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5791190864600326,"x∈X
x2
# ≤1 ηk ."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5807504078303426,"We have
X"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5823817292006526,"x∈X
∥x −CS∥2
2 =
X"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5840130505709625,"x∈X
∥x −C∥2
2 + |X| · ∥C −CS∥2
2,"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5856443719412724,so that by Lemma A.8 X
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5872756933115824,"x∈X
∥x −CS∥2
2 ≤

1 + ηγk |X|  X"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5889070146818923,"x∈X
∥x −C∥2
2"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5905383360522023,"=

1 + ηγk |X|"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5921696574225123,"
cost(X, C),"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5938009787928222,Published as a conference paper at ICLR 2022
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5954323001631321,"with probability at least 1 −
1
ηk. Hence for |X| ≥ηγk"
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.5970636215334421,"α , the approximate centroid of each cluster
induces a (1 + α)-approximation to the cost of the corresponding cluster."
AND WE NOTE THAT THE ANALYSIS EXTENDS EASILY TO GENERAL,0.598694942903752,"Lemma A.5. Let S be a set of points obtained by independently sampling each point of X ⊆Rd
with probability p = 1"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.600326264274062,"2. Let C be the optimal center of these points and CS be the empirical center
of these points. Conditioned on |S| ≥1, then E[CS] = x and there exists a constant γ such that for
η ≥1 and |X| > ηγk α ,"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.601957585644372,"E

∥CS −x∥2
2

≤
γ
|X|2 · X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6035889070146819,"x∈X
∥x −x∥2
2 !"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6052202283849919,"Pr [cost(X, CS) > (1 + α) cost(X, C)] < 1/(ηk)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6068515497553018,Proof. Lemma A.5 follows immediately from Lemma A.8 and Lemma A.9.
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6084828711256117,"Lemma A.6. Let α ∈(10 log n/√n, 1/7). Let P, Q ⊆R be sets of points on the real line such that
|P| ≥(1 −α)2m and |Q| ≤2αm, and X = P ∪Q. Let C be the center of P. Then CRDEST
on input set X outputs a point C′ such that with probability at least 1 −1/(ηk), cost(P, C′) ≤"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6101141924959217,"(1 + 18α)(1 + α)

1 +
α
(1−α)2

cost(P, C)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6117455138662317,"Proof. Let α ∈(10 log n/√n, 1/7). Then from Lemma A.4, we have that I ∩X contains at least
(1 −6α)m points of P ∩X2 and at most 2αm points of Q in an interval of length 2σ/√α, where"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6133768352365416,"σ2 =
1
2|P| X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6150081566068516,"p∈p
(p −C)2 =
1
2|P| · cost(P, C)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6166394779771615,"From Lemma A.2, we have that"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6182707993474714,"cost(P, C0) ≤

1 +
α
(1 −α)2"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6199021207177814,"
cost(P, C1),"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6215334420880914,where C0 is the center of I ∩P ∩X2 and C1 is the center of P ∩X2.
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6231647634584013,"For sufﬁciently large m and from Lemma A.9, we have that
cost(P, C1) ≤(1 + α) cost(P, C),
with probability at least 1 −1/(ηk).
Thus, it remains to show that cost(P, C′) ≤(1 +
O(α)) cost(P, C0)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6247960848287113,"Since C0 is the center of I ∩P ∩X2 and C′ is the center of I ∩X2, then we have"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6264274061990212,"|I ∩P ∩X2|C0 +
X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6280587275693311,"q∈I∩Q∩X2
q = |I ∩X2|C′."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6296900489396411,"Since I has length 2σ/√α, then q ∈
h
C0 −2σ
√α, C0 + 2σ
√α
i
. Because |I ∩P ∩X2| ≥(1 −6α)m"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6313213703099511,"and |Q| = 2αm, then for sufﬁciently small α, we have that
|C′ −C0| ≤6√ασ.
Note that we have cost(P, C′) = cost(P, C0) + |P| · |C0 −C′|2, so that
cost(P, C′) ≤cost(P, C0) + |P| · 36ασ2.
Finally, σ2 =
1
2|P | · cost(P, C) and cost(P, C) ≤cost(P, C0) due to the optimality of C. This
implies
cost(P, C′) ≤cost(P, C0) + |P| · 36ασ2"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.632952691680261,"≤cost(P, C0) + |P| · 36α ·
1
2|P| · cost(P, C)"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.634584013050571,"≤cost(P, C0) + 18α cost(P, C0)
= (1 + 18α) cost(P, C0),
as desired. Thus putting things together, we have"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.636215334420881,"cost(P, C′) ≤(1 + 18α)(1 + α)

1 +
α
(1 −α)2"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6378466557911908,"
cost(P, C)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6394779771615008,Published as a conference paper at ICLR 2022
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6411092985318108,"A.3
PROOF OF THEOREM 3.4"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6427406199021207,"We now give the proofs for optimal query complexity and runtime. We ﬁrst require the following
analogue to Lemma A.5:"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6443719412724307,Lemma A.10. Let S be a set of points obtained by independently sampling each point of X ⊆Rd
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6460032626427407,"with probability p = min

1, 100 log k"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6476345840130505,"α|S|

. Let C be the optimal center of these points and CS be the"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6492659053833605,"empirical center of these points. Conditioned on |S| ≥1, then E[CS] = x and for |X| > γk α ,"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6508972267536705,"E

∥CS −x∥2
2

≤
γ
p|X|2 · X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6525285481239804,"x∈X
∥x −x∥2
2 !"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6541598694942904,for some constant γ.
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6557911908646004,"Lemma A.11. For α ∈(10 log n/√n, 1/7), let Π be a predictor with error rate λ ≤α/2. If each
cluster has at least γk log k/α points, then Algorithm 3 outputs a (1 + 20α)-approximation to the
k-means objective value with probability at least 3/4."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6574225122349103,"Proof. Since S samples each of points independently with probability proportional to cluster sizes
given by Π, for a ﬁxed i ∈[k] at least 90 log k"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6590538336052202,"α
points with label i are sampled, with probability at
least 1 −1"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6606851549755302,"k4 from Chernoff bounds. Let γ1, . . . , γk be the empirical means corresponding to each of
the sampled points with labels 1, . . . , k, respectively, and let Γ0 = {γ1, . . . , γk}. Let C1, . . . , Ck be
centers of a (1 + α)-approximate optimal solution C with corresponding clusters X1, . . . , Xk. By
Lemma A.10, we have that"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6623164763458401,"E

∥Ci −γi∥2
2

≤
γ
p|Xi|2 · X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6639477977161501,"x∈Xi
∥x −Ci∥2
2 ! ,"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6655791190864601,"where p = min

1, 100 log k"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.66721044045677,"α|S|

. By Markov’s inequality, we have that X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6688417618270799,"i∈[k]
∥Ci −γi∥2
2 ≤100
X i∈[k]"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6704730831973899,"γ
p|Xi|2 · X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6721044045676998,"x∈Xi
∥x −Ci∥2
2 !"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6737357259380098,"with probability at least 0.99. Similar to the proof of Lemma A.9, we use the identity
X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6753670473083198,"x∈Xi
∥x −γi∥2
2 =
X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6769983686786297,"x∈Xi
∥x −Ci∥2
2 + |Xi| · ∥Ci −γi∥2
2."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6786296900489397,"Hence, we have that
cost(X, Γ0) ≤(1 + α) · cost(X, C),"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6802610114192496,with probability at least 0.99.
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6818923327895595,"Suppose Π has error rate λ ≤α and each error chooses a label uniformly at random from the k
possible labels. Then by deﬁnition of error rate, at most α/2 fraction of the points are erroneously
labeled for each cluster. Each cluster in the optimal k-means clustering of the predictor Π has at least
n/(ζk) points, so that at least a (1 −α) fraction of the points in each cluster are correctly labeled.
Thus, by the same argument as in the proof of Lemma A.6, we have that Algorithm 1 outputs a set
of centers C1, . . . , Ck such that for Γ = {C1, . . . , Ck}, we have"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6835236541598695,"cost(X, Γ) ≤(1 + 18α)

1 −
α
(1 −α)2"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6851549755301795,"
· cost(X, Γ0),"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6867862969004894,with sufﬁciently large probability.
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6884176182707994,"Let E be the event that cost(X, Γ) ≤(1 + α)(1 + 18α)

1 −
α
(1−α)2

· cost(X, C), so that Pr [E] ≥"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6900489396411092,"1 −1/ poly(k). Conditioned on E, let X1 be the subset of X that is assigned the correct label by
Π, and let X2 be the subset of X assigned the incorrect label. For each point x ∈X1 assigned the"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6916802610114192,Published as a conference paper at ICLR 2022
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6933115823817292,"correct label ℓx by Π, the closest center to x in Γ is Cℓx, so Algorithm 3 will always label x with ℓx.
Thus,"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6949429037520392,"cost(X1, Γ) ≤cost(X, Γ) ≤(1 + α)(1 + 18α)

1 −
α
(1 −α)2"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6965742251223491,"
· cost(X, C),"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6982055464926591,"conditioned on E. On the other hand, if x ∈X2 is assigned an incorrect label ℓx by Π, then the
(2, r)-approximate nearest neighbor data assigns the label px to x, where φ(Cpx) is the closest center
to φ(x) in the projected space. Recall that φ is the composition map φ1◦φ2, where φ1 has a terminal
dimension reduction with distortion 5/4, and φ2 is a random JL linear map with distortion 5/4. Thus
the distance between x and Cpx is a 2-approximation between x and its closest center Ci. Hence,
by assigning all points x to their respective centers Cpx, we have d(x, Cpx) ≤2 cost(x, Γ). Since
each point x ∈X is assigned the incorrect label with probability λ ≤α/2, the expected cost of the
labels assigned to X2 is α cost(X, Γ). By Markov’s inequality, the cost of the labels assigned to X2
is at most
10α cost(X, Γ) < 10α(1 + α) cost(X, C),
with probability at least 1 −1"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.6998368678629691,"5, conditioned on E."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7014681892332789,"Therefore by a union bound, the total cost is at most (1+20α)·cost(X, C), with probability at least
3/4."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7030995106035889,"We need the following theorems on the quality of the data structures utilized in Algorithm 3.
Theorem A.12. Makarychev et al. (2019) For every set C ⊂Rd of size k, a parameter 0 < α < 1"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7047308319738989,"2
and the standard Euclidean norm d(·, ·), there exists a terminal dimension reduction f : C →Rd′"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7063621533442088,"with distortion (1 + α), where d′ = O

log k"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7079934747145188,"α2

. The dimension reduction can be computed in
polynomial time.
Theorem A.13. Indyk & Motwani (1998); Har-Peled et al. (2012); Andoni et al. (2018) For α > 0,
there exists a (1+α, r)-ANN data structure over R equipped with the standard Euclidean norm that"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7096247960848288,"achieves query time O

d · log n"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7112561174551386,"α2

and space S := O
  1"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7128874388254486,α2 log 1
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7145187601957586,"α + d(n + q)

, where q := log n"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7161500815660685,α2 . The
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7177814029363785,runtime of building the data structure is O(S + ndq).
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7194127243066885,"We now prove Theorem 3.4.
Theorem 3.4. Let α ∈(10 log n/√n, 1/7), Π be a predictor with label error rate λ ≤α, and γ ≥1
be a sufﬁciently large constant. If each cluster in the optimal k-means clustering of the predictor
has at least γk log k/α points, then Algorithm 3 outputs a (1 + 20α)-approximation to the k-means
objective with probability at least 3/4, using O(nd log n + poly(k, log n)) total time."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7210440456769984,"Proof. The approximation guarantee of the algorithm follows from Lemma A.11. To analyze the
running time, we ﬁrst note that we apply a JL matrix with dimension O(log n) to each of the n points
in Rd, which uses O(nd log n) time. As a result of the JL embedding, each of the n points has dimen-
sion O(log n). Thus, by Theorem A.12, constructing the terminal embedding uses poly(k, log n)
time. As a result of the terminal embedding, each of the k possible centers has dimension O(log k).
Hence, by Theorem A.13, constructing the (2, r)-ANN data structure for the k possible centers uses
O(k log2 k) time. Subsequently, each query to the data structure uses O(log2 k) time. Therefore,
the overall runtime is O(nd log n + poly(k, log n))."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7226753670473083,"A.4
REMARK ON TRULY-POLYNOMIAL TIME ALGORITHMS VS. PTAS/PRAS."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7243066884176182,"Remark A.14. We emphasize that the runtime of our algorithm in Theorem 2.1 is truly polynomial
in all input parameters n, d, k and 1/α (and even near-linear in the input size nd). Although there
exist polynomial-time randomized approximation schemes for k-means clustering, e.g., Inaba et al.
(1994); Feldman et al. (2007); Kumar et al. (2004), their runtimes all have exponential dependency
on k and 1/α, i.e., 2poly(k,1/α). However, this does not sufﬁce for many applications, since k and
1/α should be treated as input parameters rather than constants. For example, it is undesirable to
pay an exponential amount of time to linearly improve the accuracy α of the algorithm. Similarly,
if the number of desired clusters k = O(log2 n), then the runtime would be exponential. Thus we
believe the exponential improvement of Theorem 2.1 over existing PRAS in terms of k and 1/α is
signiﬁcant."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7259380097879282,Published as a conference paper at ICLR 2022
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7275693311582382,"A.5
REMARK ON POSSIBLE INSTANTIATIONS OF PREDICTOR"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7292006525285482,"Remark A.15.
We can instantiate Theorem 2.1 with various versions of the predictor. Assume
each cluster in the (1 + α)-approximately optimal k-means clustering of the predictor has size at
least n/(ζk) for some tradeoff parameter ζ ∈[1, (√n)/(8k log n)]. Then the clustering quality and
runtime guarantees of Theorem 2.1 hold if the predictor Π is such that"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7308319738988581,"1. Π outputs the right label for each point independently with probability 1 −λ and otherwise
outputs a random label for λ ≤O(α/ζ),
2. Π outputs the right label for each point independently with probability 1 −λ and otherwise
outputs an adversarial label for λ ≤O(α/(kζ))."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.732463295269168,"In addition, if the predictor Π outputs a failure symbol when it fails, then for constant ζ > 0, there
exists an algorithm (see supplementary material) that outputs a (1+α)-approximation to the k-means
objective with probability at least 2/3, even when Π has failure rate λ = 1 −1/ poly(k). Note that
this remark (but not Theorem 2.1) assumes that each of the k clusters in the (1 + α)-approximately
optimal clustering has at least
n
ζk points. This is a natural assumption that the clusters are “roughly
balanced” which often holds in practice, e.g., for Zipﬁan distributions."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.734094616639478,"B
DELETION PREDICTOR"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7357259380097879,"In this section, we present a fast and simple algorithm for k-means clustering, given access to a label
predictor Π with deletion rate λ. That is, for each point, the predictor Π either outputs a label for the
point consistent with an optimal k-means clustering algorithm with probability λ, or outputs nothing
at all (or a failure symbol ⊥) with probability 1 −λ. Since the deletion predictor fails explicitly, we
can actually achieve a (1 + α)-approximation even when λ = 1 −
1
poly(k)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7373572593800979,"Our algorithm ﬁrst queries all points in the input X. Although the predictor does not output the label
for each point, for each cluster Ci with a sufﬁciently large number of points, with high probability,
the predictor assigns at least λ"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7389885807504079,"2 |Ci| points of Ci to the correct label. We show that if |Ci| = Ω
  k"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7406199021207178,"α

,
then with high probability, the empirical center is a good estimator for the true center. That is, the k-
means objective using the centroid of the points labeled i is a (1+α)-approximation to the k-means
objective using the true center of Ci. We give the full details in Algorithm 4."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7422512234910277,"To show that the empirical center is a good estimator for the true center, recall that a common
approach for mean estimation is to sample roughly an O
  1"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7438825448613376,"α2

number of points uniformly at random
with replacement. The argument follows from observing that each sample is an unbiased estimator
of the true mean, and repeating O
  1"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7455138662316476,"α2

times sufﬁciently upper bounds the variance."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7471451876019576,"Observe that the predictor can be viewed as sampling the points from each cluster without replace-
ment. Thus, for sufﬁciently large cluster sizes, we actually have a huge number of samples, which
intuitively should sufﬁciently upper bound the variance. Moreover, the empirical mean is again an
unbiased estimator of the true mean. Thus, although the above analysis does not quite hold due to
dependencies between the number of samples and the resulting averaging term, we show that the
above intuition does hold."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7487765089722676,Algorithm 4 Linear time k-means algorithm with access to a label predictor Π with deletion rate λ.
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7504078303425775,"Input: A point set x ∈X with labels given by a label predictor Π with deletion rate λ.
Output: A (1 + α)-approximate k-means clustering of X."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7520391517128875,"1: for each label i ∈[k] do
2:
Let Si be the set of points labeled i.
3:
ci ←
1
|Si| · P"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7536704730831973,"x∈Si x
4: end for
5: for all points x ∈X do
6:
if x is unlabeled then
7:
ℓx ←arg min d(x, ci)
8:
Assign label ℓx to x.
9:
end if
10: end for"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7553017944535073,Published as a conference paper at ICLR 2022
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7569331158238173,"We ﬁrst show that independently sampling points uniformly at random from a sufﬁciently large point
set guarantees a (1 + α)-approximation to the objective cost. Inaba et al. (1994); Ailon et al. (2018)
proved a similar statement for sampling with replacement."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7585644371941273,"It remains to justify the correctness of Algorithm 4 by arguing that with high probability, the overall
k-means cost is preserved up to a (1+α)-factor by the empirical means. We also analyze the running
time of Algorithm 4."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7601957585644372,Theorem B.1. If each cluster in the optimal k-means clustering of the predictor Π has at least 3k
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7618270799347472,"α
points, then Algorithm 4 outputs a (1 + α)-approximation to the k-means objective with probability
at least 2"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.763458401305057,"3, using O(kdn) total time."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.765089722675367,"Proof. We ﬁrst justify the correctness of Algorithm 4. Suppose each cluster in the optimal k-means
clustering of the predictor Π has at least 3k"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.766721044045677,"α points. Let C = {c1, . . . , ck} be the optimal centers
selected by Π and let CS = {c′
1, . . . , c′
k} be the empirical centers chosen by Algorithm 4. For each
i ∈[k], let Ci be the points of X that are assigned to Ci by the predictor Π. By Lemma A.9 with
η = 3, the approximate centroid of a cluster induces a (1 + α)-approximation to the cost of the
corresponding cluster so that"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.768352365415987,"cost(Ci, c′
i) ≤(1 + α) cost(Ci, ci),"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7699836867862969,"with probability at least 1 −
1
3k. Taking a union bound over all k clusters, we have that
X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7716150081566069,"i∈[k]
cost(Ci, c′
i) ≤
X"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7732463295269169,"i∈[k]
(1 + α) cost(Ci, ci),"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7748776508972267,with probability at least 2
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7765089722675367,"3. Equivalently, cost(X, C) ≤(1 + α) cost(X, CS)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7781402936378466,"To analyze the running time of Algorithm 4, observe that the estimated centroids for all labels can
be computed in O(dn) time. Subsequently, assigning each unlabeled point to the closest estimated
centroid uses O(kd) time for each unlabeled point. Thus, the total running time is O(kdn)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7797716150081566,"C
k-MEDIAN CLUSTERING"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7814029363784666,"We ﬁrst recall that a well-known result states that the geometric median that results from uniformly
sampling a number of points from the input is a “good” approximation to the actual geometric
median for the 1-median problem."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7830342577487766,"Theorem C.1. Krauthgamer (2019) Given a set P of n points in Rd, the geometric median of a
sample of O
  d"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7846655791190864,α2 log d
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7862969004893964,"α

points of P provides a (1 + α)-approximation to the 1-median clustering
problem with probability at least 1 −1/ poly(d)."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7879282218597063,"Note that we can ﬁrst apply Theorem A.12 to project all points to a space with dimension
O
  1"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7895595432300163,α2 log k
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7911908646003263,"α

before applying Theorem C.1. Instead of computing the geometric median, we re-
call the following procedure that produces a (1 + α)-approximation to the geometric median."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7928221859706363,"Theorem C.2. Cohen et al. (2016) There exists an algorithm that outputs a (1 + α)-approximation
to the geometric median in O
 
nd log3 n"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7944535073409462,"α

time."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.7960848287112561,We give our algorithm in full in Algorithm 5.
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.797716150081566,"Theorem C.3. For α ∈(0, 1), let Π be a predictor with error rate λ = O

α4"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.799347471451876,k log k
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.800978792822186,α log log k α
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.802610114192496,"
. If each"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8042414355628059,"cluster in the optimal k-median clustering of the predictor has at least n/(ζk) points, then there
exists an algorithm that outputs a (1 + α)-approximation to the k-median objective with probability
at least 1 −1/ poly(k), using O(nd log3 n + poly(k, log n)) total time."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8058727569331158,"Proof. Observe that Algorithm 5 samples O
  1"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8075040783034257,α4 log2 k
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8091353996737357,"α

points for each of the clusters labeled i,"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8107667210440457,"with i ∈[k]. Thus Algorithm 5 samples O
  k"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8123980424143556,α4 log2 k
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8140293637846656,"α

points in total. For λ = O

α4"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8156606851549756,k log k
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8172920065252854,α log log k α 
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8189233278955954,"with a sufﬁciently small constant, the expected number of incorrectly labeled points sampled by
Algorithm 5 is less than 1"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8205546492659054,"32. Thus, by Markov’s inequality, the probability that no incorrectly labeled"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8221859706362153,Published as a conference paper at ICLR 2022
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8238172920065253,Algorithm 5 Learning-Augmented k-median Clustering
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8254486133768353,"Input: A point set x ∈X with labels given by a predictor Π with error rate λ.
Output: A (1 + α)-approximate k-median clustering of X."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8270799347471451,"1: Use a terminal embedding to project all points into a space with dimension O
  1"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8287112561174551,α2 log k
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8303425774877651,"α

.
2: for i = 1 to i = k do
3:
Let ℓi be the most common remaining label.
4:
Sample O
  1"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.831973898858075,α4 log2 k
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.833605220228385,"α

points with label ℓi.
5:
Let C′
i be a
 
1 + α"
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.835236541598695,"4

-approximation to the geometric median of the sampled points.
6: end for
7: Return C′
1, . . . , C′
k."
LET C BE THE OPTIMAL CENTER OF THESE POINTS AND CS BE THE EMPIRICAL CENTER,0.8368678629690048,points are sampled by Algorithm 5 is at least 3
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8384991843393148,"4. Conditioned on the event that no incorrectly labeled
points are sampled by Algorithm 5, then by Theorem C.1, the empirical geometric median for each
cluster induces a
 
1 + α"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8401305057096248,"4

-approximation to the optimal geometric median in the projected space.
Hence the set of k empirical geometric medians induces a
 
1 + α"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8417618270799347,"4

-approximation to the optimal
k-median clustering cost in the projected space. Since the projected space is the result of a terminal
embedding, the set of k empirical geometric medians for the sampled points in the projected space
induces a k-median clustering cost that is a
 
1 + α"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8433931484502447,"4

-approximation to the k-median clustering cost
induced by the set of k empirical geometric medians for the sampled points in the original space.
Taking the set of k empirical geometric medians for the sampled points in the original space induces
a
 
1 + α"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8450244698205547,"4
2-approximation to the k-median clustering cost. We take a
 
1 + α"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8466557911908646,"4

-approximation to
each of the geometric medians. Thus for sufﬁciently small α, Algorithm 5 outputs a (1 + α)-
approximation to the k-median clustering problem."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8482871125611745,"To embed the points into the space of dimension O
  1"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8499184339314845,α2 log k
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8515497553017944,"α

, Algorithm 5 spends O(nd log n)
total time. By Theorem C.2, it takes O(nd log3 n) total time to compute the approximate geometric
medians."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8531810766721044,"D
LOWER BOUNDS"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8548123980424144,"MAX-E3-LIN-2 is the optimization problem of maximizing the number of equations satisﬁed by a
system of linear equations of Z2 with exactly 3 distinct variables in each equation. EK-MAX-E3-
LIN-2 is the problem of MAX-E3-LIN-2 when each variable appears in exactly k equations. Fotakis
et al. (2016) showed that assuming the exponential time hypothesis (ETH) (Impagliazzo & Paturi,
2001), there exists an absolute constant C1 such that MAX k-SAT (and thus MAX k-CSP) instances
with fewer than O(nk−1) clauses cannot be approximated within a factor of C1 in time 2O(n1−δ)
for any δ > 0. As a consequence, the reduction by H˚astad (2001) shows that there exist absolute
constants C2, C3 such that EK-MAX-E3-LIN-2 with k ≥C2 cannot be approximated within a
factor of C3 in time 2O(n1−δ) for any δ > 0. Hence, the reduction by Chleb´ık & Chleb´ıkov´a (2006)
shows that there exists a constant C4 such that approximating the minimum vertex cover of 4-regular
graphs within a factor of C4 cannot be done in time 2O(n1−δ) for any δ > 0. Thus the reduction by
Lee et al. (2017) shows that there exists a constant C5 such that approximating k-means within a
factor of C5 cannot be done in time 2O(n1−δ) for any δ > 0, assuming ETH. Namely, the reduction of
Lee et al. (2017) shows that an algorithm that provides a C5-approximation to the optimal k-means
clustering can be used to compute a C4-approximation to the minimum vertex cover."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8564437194127243,"Theorem D.1. If ETH is true, then there does not exist an algorithm A that takes a set S of n1−δ"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8580750407830342,"log n
vertices and ﬁnds a C4-approximation to the minimum vertex cover that contains S on a 4-regular
graph G, using 2O(n1−δ) time for some constant δ ∈(0, 1]."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8597063621533442,Proof. Suppose by way of contradiction that there exists an algorithm A that takes a set S of n1−δ
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8613376835236541,"log n
vertices and ﬁnds a C4-approximation to the minimum vertex cover that contains S on a 4-regular
graph G, using 2O(n1−δ) time for some constant δ ∈(0, 1]. We claim that we can use A to cre-
ate an overall algorithm that violates ETH. Indeed, suppose we guess each subset of n1−δ"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8629690048939641,log n ver-
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8646003262642741,Published as a conference paper at ICLR 2022
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.866231647634584,"tices and which vertices of the subset are in the cover. There are
 
n
n1−δ/ log n

· 2n1−δ/ log n ≤"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.867862969004894,"(enδ log n)n1−δ/ log n · 2n/ log n such combinations of vertices. For each guess, we then run the
purported algorithm A that uses 2O(n1−δ) time. Thus we can identify a C4-approximation to the
minimum vertex cover in time
(enδ log n)n1−δ/ log n · 2n/ log n · 2O(n1−δ) = 2O(n1−δ) · 2O(n1−δ) = 2O(n1−δ),
which would contradict ETH."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8694942903752039,"Finally, we show the query complexity of Algorithm 3 is nearly optimal. Lee et al. (2017) con-
structed an instance of k-means that cannot be approximated within a factor of 1.0013 in polynomial
time. The reduction of Lee et al. (2017) creates 4n points in R3n that must be clustered by O(n)
centers and an algorithm that provides a C5-approximation to the optimal k-means clustering can
be used to compute a C4-approximation to the minimum vertex cover. Thus, there exists a constant
C5 such that approximating k-means within a factor of C5 cannot be done in time 2O(n1−δ) for any
δ > 0, assuming ETH."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8711256117455138,"Theorem 3.5. For any δ ∈(0, 1], any algorithm that makes O

k1−δ
α log n

queries to the predictor"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8727569331158238,"with label error rate α cannot output a (1 + Cα)-approximation to the optimal k-means clustering
cost in time 2O(n1−δ) time, assuming the Exponential Time Hypothesis."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8743882544861338,"Proof. Let α be a ﬁxed constant such that α < C5. Given an instance I of a k-means clustering con-
structed from the reduction of Lee et al. (2017), the optimal clustering cost is Ω(n) and k1 = Ω(n).
We embed this instance into a k-means clustering by adding an additional Ω(n) points arbitrarily
far from I, so that the additional points contribute Ω(n/α) cost upon partitioning into k2 = Ω(n)
clusters. We set k = k1 + k2."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8760195758564437,"In summary, the resulting instance has O(n) points and k = Ω(n). The optimal solution has cost"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8776508972267537,"O(n/α) cost so that I contributes an Ω(α) fraction of the cost. By querying O

k1−δ
α log n

points with"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8792822185970636,"sufﬁciently small constant, at most O

k1−δ
α log n

of the cluster centers in I will be revealed by the
construction of I. Each center corresponds to a selected vertex in the corresponding vertex cover in
the reduction from minimum vertex cover on 4-regular graphs. Hence, in the corresponding vertex
cover instance, at most O

k1−δ
α log n

vertices are revealed. Thus by Theorem D.1, any algorithm"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8809135399673735,"running in 2O(n1−δ) time cannot determine a C5-approximation to the optimal k-means clustering
cost on I, as it would correspond to a C4-approximation to the optimal vertex cover, assuming ETH.
Since I induces an Ω(α) fraction of the total clustering cost, it follows that any algorithm that makes"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8825448613376835,"O

k1−δ
α log n

queries cannot output a (1+Cα)-approximation to the optimal k-means clustering cost"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8841761827079935,"in time 2O(n1−δ) time, assuming ETH."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8858075040783034,"E
ADDITIONAL EXPERIMENTAL RESULTS"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8874388254486134,"E.1
OMITTED DISCUSSION"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8890701468189234,"In this section we continue the discussion from the experimental section of the main body. In Figure
3(a), we show the qualitatively similar version of Figure 2(c) of the main body for the case of
k = 25. In Figure 3(b), we display the qualitatively similar version of Figure 3(a) of the main body
for the k = 25 case of dataset PHY."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8907014681892332,"PHY.
We use the noisy predictor for this dataset. We see in Figure 2(a) that as the corruption
percentage rises, the clustering given by just the predictor labels can have increasingly large cost.
Nevertheless, even if the clustering cost of the corrupted labels is rising, the cost decreases signiﬁ-
cantly after applying Algorithm 1 by roughly a factor of 3x. Indeed, we see that our algorithm can
beat the kmeans++ seeding baseline for q as high as 50%. Just as in Figure 1(c), random sampling
is sensitive to noise. Lastly, we also remain competitive with the purple line which uses the labels
output by kmeans++ as the predictor in our algorithm (no corruptions added). The qualitatively
similar plot for k = 25 is given in the supplementary material."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8923327895595432,Published as a conference paper at ICLR 2022
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8939641109298532,"CIFAR-10.
The cost of clustering on CIFAR-10 using only the predictor, the predictor with our
algorithm, random sampling, and kmeans++ as the predictor for our algorithm were 0.733, 0.697,
0.721, and 0.640, respectively, where 1.0 represents the cost of kmeans++. The neural network
was very accurate (∼93%) in predicting the class of the input points which is highly correlated
with the optimal k-means clusters. Nevertheless, our algorithm improved upon this highly precise
predictor."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8955954323001631,"Note that using kmeans++ as the predictor for our algorithm resulted in a clustering that was 13%
better than the one given by the neural network predictor. This highlights the fact that an approxi-
mate clustering combined with our algorithm can be competitive against a highly precise predictor,
such as a neural network, even though creating the highly accurate predictor can be expensive. In-
deed, obtaining a neural network predictor requires prior training data and also the time to train the
network. On the other hand, using a heuristic clustering as a predictor is extremely ﬂexible and
can be applied to any dataset even if no prior training dataset is available (50, 000 test images were
required to train the neural network predictor but kmeans++ as a predictor requires no test images),
in addition to considerable savings in computation."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8972267536704731,"For example, the time taken to train the particular neural network we used was approximately 18
minutes using the optimized PyTorch library (see training details under the “Details Report” section
in Huy (2020)). In general, the time can be much longer for more complicated datasets. On the
other hand, our unoptimized algorithm implementation which used the labels of a sample run of
kmeans++ was still able to achieve a better clustering than the neural network predictor with α =
0.01 in Algorithm 2 in 4.4 seconds. In conclusion, we can achieve a better clustering by combining
a much weaker predictor with our algorithm with the additional beneﬁt of using a more ﬂexible and
computationally inexpensive methodology."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.8988580750407831,"We also conducted an experiment where we only use a small fraction of the predictor labels in our
algorithm. We select a p fraction of the images, query their labels, and run our algorithm on only
these points. We then report the cost of clustering on the entire dataset as p ranges from 1% to 100%.
Figure 2(b) shows the percentage increase in clustering cost relative to querying the whole dataset is
quite low for moderate values of p but increasingly worse as p becomes smaller. This suggests that
the quality of our algorithm does not suffer drastically by querying a smaller fraction of the dataset."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9004893964110929,"0
5
10
15
20
25
Corruption % 0 20 40 60 80"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9021207177814029,Clustering Cost
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9037520391517129,"Dataset: Oregon Spectral Clustering, Graph #5, k=25"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9053833605220228,"Alg + Noisy Predictor
Random Sampling
kmeans++
Predictor"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9070146818923328,"(a) Oregon Graph #5, k = 25"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9086460032626428,"0
10
20
30
40
50
Corruption % 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9102773246329527,Clustering Cost
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9119086460032626,"Dataset: PHY, k=25"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9135399673735726,"Alg + Noisy Predictor
Alg + k-means++
Random Sampling
Noisy Predictor
kmeans++"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9151712887438825,"(b) PHY, k = 25"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9168026101141925,Figure 3: Our algorithm is able to recover a good clustering even for very high levels of noise.
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9184339314845025,"E.2
SYNTHETIC DATASET"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9200652528548124,"We use a dataset of 10010 points in Rd for d = 103 created using the kmeans++ lower bound
construction presented in Arthur & Vassilvitskii (2007). The dataset consists of 10 well separated
clusters in Rd: let ei denote the basis vectors. Our dataset is {1000ei} ∪{1000ei + ej} for all
1 ≤i ≤10, 1 ≤j ≤1000."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9216965742251223,"From the description of the dataset, we can explicitly calculate the optimal clustering and its cost.
Our predictor for this dataset was to take the optimal clustering and randomly change each label"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9233278955954323,Published as a conference paper at ICLR 2022
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9249592169657422,"with probability 1/2 to another uniformly random label. Empirically, kmeans++ seeding returned
a clustering that had cost at least 1.9x the optimal clustering. Furthermore, using just the predictor
labels na¨ıvely resulted in a clustering with cost up to ﬁve orders of magnitude larger than the op-
timal clustering. In contrast, our algorithm was able to precisely recover the true clustering after
processing the predictor outputs. In addition, applying our algorithm using the labels of a sample
run of kmeans++ was also able to precisely recover the true clustering."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9265905383360522,"E.3
COMPARISON TO LLOYD’S HEURISTIC"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9282218597063622,"We give both theoretical and empiricial justiﬁcations for why our algorithms could be superior to
blindly following a predictor and then running Lloyd’s heuristic."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9298531810766721,"(a)
(b)
(c)"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9314845024469821,Figure 4: Additional experimental results for comparison to Lloyd’s heuristic.
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.933115823817292,"Empirical Comparison.
We ﬁrst compared Lloyd’s algorithm on kmeans++ seeding to our al-
gorithm with the predictor on the PHY dataset. The predictor is a noisy predictor that has corruption
level 50% as described in Section 4 so that outputting the clustering from the predictor alone has
cost 1.9x the average kmeans++ cost. Hence, it is clear that the predictor is much worse than
kmeans++, yet our algorithm using the predictor (horizontal line in Figure 4(a)) is much better
than kmeans++ and Lloyd’s algorithm (orange line in Figure 4(a))."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9347471451876019,"Next, we took the noisy predictor for the PHY dataset with corruption level 50% and repeatedly
applied Lloyd’s algorithm. We observe that even with ∼5 Lloyd’s iterations (Figure 4(b)), the
clustering cost does not seem to improve upon kmeans++, much less the clustering output by
simply applying our algorithm to the noisy predictions (Figure 4(a))."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9363784665579119,"Finally, we compared Lloyd’s algorithm on kmeans++ seeding to Lloyd’s algorithm on the seed-
ing output by our algorithm (with kmeans++ as the predictor) on the CIFAR-10 dataset, similar to
the experiments you suggested by Lattanzi & Sohler (2019). Lloyd’s algorithm on the seeding out-
put by our algorithm exhibits superior performance than Lloyd’s algorithm on kmeans++ (Figure
4(c)), which is consistent with our previous experiments showing that our algorithm improves upon
kmeans++. This further strengthens our claim that our algorithm and methodology with provable
worst-case guarantees can be applied in conjunction with heuristics such as Lloyd’s that do not have
provable worst-case guarantees. Moreover, Figure 4(c) indicates that our approach may be more
advantageous than just running kmeans++ with heuristics. Note that a na¨ıve implementation of
Lloyd’s algorithm is O(ndk) time while our algorithm can be implemented in nearly linear time."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9380097879282219,"We emphasize that all of our above experiments use a noisy predictor with corruption level 50% as
input to our algorithm. Our experiments exhibit even better behavior when a clustering produced by
kmeans++ is used for a predictor as input to our algorithm. Combined with our other experiments
in Section 4, this gives empirical evidence that there exist many scenarios in which running our
algorithm with an erroneous predictor is advantageous."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9396411092985318,"Theoretical Comparison.
We now provide an example that demonstrates why blindly following
the predictor and then running Lloyd’s heuristic would run into issues. We emphasize that it is well-
known e.g., see Dasgupta (2003); Har-Peled & Sadri (2005); Arthur & Vassilvitskii (2006); Vattani
(2011), that Lloyd’s algorithm can take a large number of steps to converge. In particular, Vattani
(2011) shows that an exponential number of Lloyd iterations can be required for the algorithm to
converge to the optimal solution. Nevertheless, we offer the following concrete answer:"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9412724306688418,Published as a conference paper at ICLR 2022
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9429037520391517,"We describe a simple set of points that guarantees Lloyd’s algorithm will fail. This is based on the
example given in Har-Peled & Sadri (2005) and is also conceptually similar to the example given
in 1.1. Consider 4n points on the real line x1, . . . , x2n, y1, . . . , y2n so that y1 ≤. . . ≤y2n ≤
A < B ≤x1 ≤. . . ≤x2n where B −A is large. Suppose k = 2 in which case the optimal
clustering groups all the yi points together and all the xi points together as two separate clusters.
Suppose the predictor initially gives label “1” to points y1, . . . , yn and gives label “2” to points
yn+1, . . . , x1, . . . , x2n, so that the predictor has corruption level λ = 1/2. Then our algorithm that
uses this predictor will get a constant-factor approximation in only one iteration. However, since
B −A can be arbitrarily large without affecting the optimal clustering cost, blindly listening to
the predictor will give a worse clustering. Furthermore, Theorem 2.1 in Har-Peled & Sadri (2005)
implies that Lloyd’s will take Θ(n) iterations to converge if initialized using this predictor. Note that
even a single (na¨ıve) iteration of Lloyd’s algorithm already uses O(ndk) time while our algorithm
only uses O(nd) + poly(k, 1/α) time. Note that there are also more complex examples in higher-
dimensional spaces in the literature which provably have even worse convergence rates for Lloyd’s
method."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9445350734094616,"E.4
CONCLUSION"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9461663947797716,"Although 1.07-approximation for k-means clustering in polynomial time is NP-hard and a clustering
consistent with the labels of any predictor with nonzero error can be arbitrarily bad, we give a
(1 + α)-approximation algorithm that uses the labels output by the predictor as “advice” and runs
in nearly linear time. We use a linear number of queries to the predictor, which can be improved to
nearly optimal under natural assumptions about the cluster sizes. Our results are well-supported by
empirical evaluations and are an important step in demonstrating the power of learning-augmented
algorithms beyond the limitations of classical algorithms for clustering-based applications."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9477977161500816,"F
LEARNABILITY RESULTS"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9494290375203915,"In this section, we present formal learning bounds for learning a good predictor for the learning-
augmented k-means problem. Namely, we show that a good predictor can be learned efﬁciently if
the problem instances are drawn from a particular distribution. Our result is inspired by derived
from data-driven algorithm design and utilizes the PAC learning framework. Formally, our setting
is the following."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9510603588907015,"Suppose there exists an underlying distribution D that generates independent k-means clustering
instances, representing the case where similar instances of the k-means clustering problem are being
solved. Note that this setting also mirrors some of our experiments in Section 4, speciﬁcally in the
case of our spectral clustering datasets which are derived from snapshots of a dynamic graph across
time."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9526916802610114,"Our goal is to efﬁciently learn a good predictor f among some family of functions F. The input
to each predictor f is a clustering instance C and the output is a feature vector. We assume that
the each input instance C is encoded as a vector in Rnd, that is, we think of nd as the size of the
description of the input. We also assume that the output of f is in n dimensions, which represents
the prediction of the oracle. For example in the case of k-means clustering, the output can be an
integer in [k] for each point in C."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9543230016313213,"To select the “best” f, we need to formally deﬁne what we mean by best. In most learning settings,
a loss function is used to measure the quality of a solution. Indeed, suppose we have a loss function
L : f ×C →R which represents how well a predictor f performs on some input C. For example, f
can represent the cluster labels of a points in C and L outputs the cost of the clustering. Alternatively,
L can represent an algorithm which uses the hints given by f and performs a clustering algorithm
such as some number of steps of Lloyd’s heuristic."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9559543230016313,Our goal is to learn the best function f ∈F that minimizes the following objective:
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9575856443719413,"EC∼D[L(f, C)].
(2)"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9592169657422512,"We deﬁne f ∗to be an optimal function f ∈F, so that f ∗= arg min EC∼D[L(f, C)]. We also
assume that for each clustering instance C and each f ∈F, f(C) and L(f, C) can be computed in
time T(n, d) that should be interpreted as a (small) polynomial in n and d."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9608482871125612,Published as a conference paper at ICLR 2022
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9624796084828712,Our main result is the following.
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.964110929853181,"Theorem F.1. There exists an algorithm that uses poly(T(n, d), 1/ε) samples and returns a ˆf that
satisﬁes
EC∼D[L( ˆf, C)] ≤EC∼D[L(f ∗, C)] + ε
with probability at least 9/10."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.965742251223491,"The above theorem is a PAC-style bound that shows only a small number of samples are needed in
order to have a good probability of learning an approximately-optimal function ˆf. The algorithm to
compute ˆf is the following: we simply minimize the empirical loss after an appropriate number of
samples are drawn, i.e., we perform empirical risk minimization. This result is proven by Theorem
F.3. Before introducing it, we need to deﬁne the concept of pseudo-dimension for a function class,
which is the more familiar VC dimension, generalized to real functions.
Deﬁnition F.2 (Pseudo-Dimension, e.g., Deﬁnition 9 in Lucic et al. (2018)). Let X be a ground set
and F be a set of functions from X to the interval [0, 1]. Fix a set S = {x1, · · · , xn} ⊂X, a set of
reals numbers R = {r1, · · · , rn} with ri ∈[0, 1] and a function f ∈F. The set Sf = {xi ∈S |
f(xi) ≥ri} is called the induced subset of S formed by f and R. The set S with associated values
R is shattered by F if |{Sf | f ∈F}| = 2n. The pseudo-dimension of F is the cardinality of the
largest shattered subset of X (or ∞)."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.967373572593801,"The following theorem relates the performance of empirical risk minimization and the number of
samples needed, to pseudo-dimension. We specialize the theorem statement to our situation at hand.
For notational simplicity, we deﬁne G be the class of functions in F composed with L:"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9690048939641109,G := {L ◦f : f ∈F}.
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9706362153344209,"Furthermore, by normalizing, we can assume that the range of L is equal to [0, 1].
Theorem F.3 (Anthony & Bartlett (1999)). Let D be a distribution over problem instances C and
G be a class of functions g : C →[0, 1] with pseudo-dimension dG. Consider t i.i.d. samples
C1, C2, . . . , Ct from D. There is a universal constant c0, such that for any ε > 0, if t ≥c0 · dG/ε2,
then we have

1 t t
X"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9722675367047309,"i=1
g (Ci) −EC∼D g(C) ≤ε"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9738988580750407,for all g ∈G with probability at least 9/10.
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9755301794453507,"The following corollary follows from the triangle inequality.
Corollary F.4. Consider a set of t independent samples C1, . . . , Ct from D and let ˆg be a function
in G that minimizes 1"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9771615008156607,"t
Pt
i=1 g(Ci). If the number of samples t is chosen as in Theorem F.3, then"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9787928221859706,EC∼D[ˆg(C)] ≤EC∼D[g∗(C)] + 2ε
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9804241435562806,holds with probability at least 9/10.
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9820554649265906,"Therefore, the main challenge at hand is to bound the pseudo-dimension of our given function class
G. To do so, we ﬁrst relate the pseudo-dimension to the VC dimension of a related class of threshold
functions. This relationship has been fruitful in obtaining learning bounds in a variety of works such
as Lucic et al. (2018); Izzo et al. (2021).
Lemma F.5 (Pseudo-dimension to VC dimension, Lemma 10 in Lucic et al. (2018)). For any g ∈
G, let Bg be the indicator function of the region on or below the graph of g, i.e., Bg(x, y) =
sgn(g(x) −y). The pseudo-dimension of G is equivalent to the VC-dimension of the subgraph class
BG = {Bg | g ∈G}."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9836867862969005,"Finally, the following theorem relates the VC dimension of a given function class to its computa-
tional complexity, i.e., the time complexity of computing a function in the class.
Lemma F.6 (Theorem 8.14 in Anthony & Bartlett (1999)). Let h : Ra × Rb →{0, 1}, determining
the class
H = {x →h(θ, x) : θ ∈Ra}.
Suppose that any h can be computed by an algorithm that takes as input the pair (θ, x) ∈Ra × Rb
and returns h(θ, x) after no more than t of the following operations:"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9853181076672104,Published as a conference paper at ICLR 2022
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9869494290375204,"• arithmetic operations +, −, ×, and / on real numbers,"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9885807504078303,"• jumps conditioned on >, ≥, <, ≤, =, and = comparisons of real numbers, and"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9902120717781403,"• output 0, 1,"
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9918433931484503,then the VC dimension of H is O(a2t2 + t2a log a).
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9934747145187602,"Combining the previous results allows us prove Theorem F.1. At a high level, we are instantiating
Lemma F.6 with the complexity of computing any function in the function class G."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9951060358890701,"Proof of Theorem F.1. First by Theorem F.3 and Corollary F.4, it sufﬁces to bound the pseudo-
dimension of the class G = L ◦F. Then from Lemmas F.5, the pseudo-dimension of G is the VC
dimension of threshold functions deﬁned by G. Finally from Lemma F.6, the VC dimension of the
appropriate class of threshold functions is polynomial in the complexity of computing a member
of the function class. In other words, Lemma F.6 tells us that the VC dimension of BG deﬁned in
Lemma F.5 is polynomial in the number of arithmetic operations needed to compute the threshold
function associated to some g ∈G. By our deﬁnition, this quantity is polynomial in T(n, d). Hence,
the pseudo-dimension of G is also polynomial in T(n, d) and our desired result follows."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.9967373572593801,"Note that we can consider initializing Theorem F.1 with speciﬁc predictions. If each function in the
family of oracles we are interested can be computed efﬁciently, which is the case of the predictors
we employ in our experiments, then Theorem F.1 assures us that we only require polynomially many
samples to be able to learn a nearly optimal oracle."
CONDITIONED ON THE EVENT THAT NO INCORRECTLY LABELED,0.99836867862969,"Note that our result is in similar in spirit to the recent paper Dinitz et al. (2021). They derive
sample complexity learning bounds for a different algorithmic problem of computing matchings in a
graph. Since they specialize their analysis to a speciﬁc function class and loss function, their bounds
are possibly tighter rather than the possibly loose polynomial bounds we have stated. However,
our analysis above is more general as it allows for a variety of predictors, as we employ in our
experiments, and loss functions to measure the quality of the predictions."
