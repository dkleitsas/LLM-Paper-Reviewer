Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0012437810945273632,"Quantization emerges as one of the most promising compression technologies for
1"
ABSTRACT,0.0024875621890547263,"deploying efficient large models in recent years. However, existing quantization
2"
ABSTRACT,0.0037313432835820895,"schemes suffer from significant accuracy degradation at very low bits, or require
3"
ABSTRACT,0.004975124378109453,"some additional computational overhead when deployed, making it difficult to be
4"
ABSTRACT,0.006218905472636816,"applied to large-scale applications in industry. In this paper, we propose decoupleQ,
5"
ABSTRACT,0.007462686567164179,"achieving a substantial increase in model accuracy, especially at very low bits.
6"
ABSTRACT,0.008706467661691543,"decoupleQ abandons the traditional heuristic quantization paradigm and decouples
7"
ABSTRACT,0.009950248756218905,"the model parameters into integer and floating-point parts, then transforming the
8"
ABSTRACT,0.011194029850746268,"quantization problem into a mathematical constrained optimization problem, which
9"
ABSTRACT,0.012437810945273632,"is then solved alternatively by off-the-shelf solution methods. decoupleQ gets rid
10"
ABSTRACT,0.013681592039800995,"of any tricks for dealing with outliers, sensitive channels, etc., and focuses only
11"
ABSTRACT,0.014925373134328358,"on the basic optimization objective to achieve high model accuracy on extreme
12"
ABSTRACT,0.01616915422885572,"low bit quantization. Quantization via decoupleQ is linear and uniform, making
13"
ABSTRACT,0.017412935323383085,"it hardware-friendlier than non-uniform counterpart, and enabling the idea to be
14"
ABSTRACT,0.018656716417910446,"migrated to high-bit quantization to enhance its robustness.
15"
ABSTRACT,0.01990049751243781,"decoupleQ has achieved comparable accuracy as fp16/bf16 for 2-bit quantization of
16"
ABSTRACT,0.021144278606965175,"large speech models in our company. The code (including the W2 CUDA kernels)
17"
ABSTRACT,0.022388059701492536,"is attached and will be made public.
18"
INTRODUCTION,0.0236318407960199,"1
Introduction
19"
INTRODUCTION,0.024875621890547265,"Serving large models (1; 2; 37; 38) in industry is budget-consuming because of the huge computa-
20"
INTRODUCTION,0.026119402985074626,"tional, IO and storage cost. Model compression (10; 11; 16) has therefore become a necessity to
21"
INTRODUCTION,0.02736318407960199,"alleviate this pain. Among which, Post-Training Quantization (PTQ) (9; 26) has gained more and
22"
INTRODUCTION,0.028606965174129355,"more popularity among researchers and engineers because it does not require heavy GPU-hours
23"
INTRODUCTION,0.029850746268656716,"training with labeled datasets.
24"
INTRODUCTION,0.03109452736318408,"However, previous quantization schemes remain confined within the traditional heuristic quantization
25"
INTRODUCTION,0.03233830845771144,"paradigm, e.g., how to deal with outliers (33; 35), how to deal with sensitive channels (6), how
26"
INTRODUCTION,0.033582089552238806,"to determine the clipping range (29), and so on. These methods have achieved some success, but
27"
INTRODUCTION,0.03482587064676617,"the quantization at extreme low bit often suffers from significant accuracy degradation, thus failing
28"
INTRODUCTION,0.036069651741293535,"to meet the launching requirements of industrial practice. There are also some other options to
29"
INTRODUCTION,0.03731343283582089,"mitigate the accuracy loss. QuIP (4) pushes the accuracy limits of 2-bit quantization and can achieve
30"
INTRODUCTION,0.03855721393034826,"performance close to fp16/bf16. However, compared to traditional quantization schemes, its inference
31"
INTRODUCTION,0.03980099502487562,"imposes an additional burden due to the need to multiply two random orthogonal matrices to de-
32"
INTRODUCTION,0.041044776119402986,"quant the weights. N2UQ (20) fit the real-value distribution with non-uniform grids then quantize
33"
INTRODUCTION,0.04228855721393035,"them into equidistant output levels. But it need to train to get the input thresholds. SpQR (7)
34"
INTRODUCTION,0.043532338308457715,"and SqueezeLLM (14) use mixed-precision quantization or non-uniform scheme to safeguard the
35"
INTRODUCTION,0.04477611940298507,"important channels, but they need customized hardware support.
36"
INTRODUCTION,0.046019900497512436,"In order to alleviate the above pains in industry, we proposed decoupleQ, which completely abandons
37"
INTRODUCTION,0.0472636815920398,"the traditional heuristic quantization paradigm and instead decouples the model parameters into
38"
INTRODUCTION,0.048507462686567165,"integer and floating point parts, then transforming the quantization problem into a mathematical
39"
INTRODUCTION,0.04975124378109453,"constrained optimization problem, which is then solved alternatively by off-the-shelf solution methods.
40"
INTRODUCTION,0.05099502487562189,"The integer part contains the main weights of the model, and the floating-point part contains scales
41"
INTRODUCTION,0.05223880597014925,"and zero points induced via quantization. decoulpeQ starts from an abstract objective function and
42"
INTRODUCTION,0.053482587064676616,"thus does not need any tricks to deal with the minutiae of traditional quantization paradigm, such as
43"
INTRODUCTION,0.05472636815920398,"outlier, salient weights (19), and so on. Quantization via decoupleQ is linear and uniform, making it
44"
INTRODUCTION,0.055970149253731345,"hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit
45"
INTRODUCTION,0.05721393034825871,"quantization to enhance its robustness.
46"
INTRODUCTION,0.05845771144278607,"decoupleQ contains two stages: 1. layer-wise minimization, defined in Eq. 1, is used to optimize
47"
INTRODUCTION,0.05970149253731343,"the integer part and the floating-point part; 2. block-wise minimization, defined in Eq. 2, is used to
48"
INTRODUCTION,0.060945273631840796,"further optimize the floating-point part while freezing the integer part1.
49"
INTRODUCTION,0.06218905472636816,"Layer-wise minimization is to minimize the ℓ2 loss of the outputs between pre- and post-quantization
50"
INTRODUCTION,0.06343283582089553,"for a linear layer:
51"
INTRODUCTION,0.06467661691542288,"min
f
W
∥Xf
W −XW0∥2
2
(1)"
INTRODUCTION,0.06592039800995025,"where X ∈Rbatch×din is the input of this layer, W0 ∈Rdin×dout is the pre-trained full precision
52"
INTRODUCTION,0.06716417910447761,"weight, din and dout are the input and output dimensions respectively. The objective is to find a
53"
INTRODUCTION,0.06840796019900497,"matrix f
W with quantized-then-dequantized elements to minimize Eq. 1.
54"
INTRODUCTION,0.06965174129353234,"Some works (4; 8; 9; 13; 25) started from Eq. 1 and achieved some success, but they still haven’t
55"
INTRODUCTION,0.0708955223880597,"thought outside the box of traditional quantization. GPTQ series (8; 9) fake-quantize the first element
56"
INTRODUCTION,0.07213930348258707,"of W0 and then update the the remaining elements so as to keep Eq. 1 minimized. This process is
57"
INTRODUCTION,0.07338308457711443,"then continued element by element until all elements are fake-quantized. However, on the one hand,
58"
INTRODUCTION,0.07462686567164178,"they do not give any indication of how scale and zero point should be calculated, and on the other
59"
INTRODUCTION,0.07587064676616916,"hand, the optimization problem formulated for updating the remaining elements is unconstrained
60"
INTRODUCTION,0.07711442786069651,"(explained in detail later). decoupleQ models Eq. 1 as a constrained optimization problem, as shown
61"
INTRODUCTION,0.07835820895522388,"in Eq. 6. It no longer needs to pay attention to some of the minutiae unique to quantization, such as
62"
INTRODUCTION,0.07960199004975124,"outliers, clipping threshold, etc., but abstracts the essence of the problem from a higher level.
63"
INTRODUCTION,0.08084577114427861,"In the second stage, block-wise minimization is used to further improve the model accuracy:
64"
INTRODUCTION,0.08208955223880597,"min ∥
^
Block(X) −Block(X)∥2
2
(2)"
INTRODUCTION,0.08333333333333333,"where
^
Block(·) is a common transformer block (32) with quantized weights. In this stage, we freeze
65"
INTRODUCTION,0.0845771144278607,"the integer part of the weights, and train the scales, zero points and norm layers.
66"
INTRODUCTION,0.08582089552238806,"decoupleQ implements 2-bit uniform quantization and achieves state-of-the-art accuracy in Llama-
67"
INTRODUCTION,0.08706467661691543,"1/2 (30; 31). Like traditional uniform quantization, decoupleQ does not incur additional inference
68"
INTRODUCTION,0.08830845771144279,"burden and only requires a linear transformation to convert the quantized weights into floating point
69"
INTRODUCTION,0.08955223880597014,"ones.
70"
INTRODUCTION,0.09079601990049752,"Our main highlights are summarized as follows:
71"
INTRODUCTION,0.09203980099502487,"• New insight: We abandoned the traditional quantization paradigm, and no longer need
72"
INTRODUCTION,0.09328358208955224,"to focus on some of the minutiae unique to quantization, but abstracts the essence of the
73"
INTRODUCTION,0.0945273631840796,"problem from a higher level and transforms it into a constrained optimization problem.
74"
INTRODUCTION,0.09577114427860696,"• Extreme low-bit: decoupleQ achieves 2-bit uniform quantization with performance match-
75"
INTRODUCTION,0.09701492537313433,"ing fp16/bf16 for industrial applications in the ASR model in our company, and we will also
76"
INTRODUCTION,0.09825870646766169,"release the W2A16 CUDA kernel as one of our core contribution.
77"
INTRODUCTION,0.09950248756218906,"• Extensibility: As a bonus, if labeled datasets are available, the idea of decoupleQ can be
78"
INTRODUCTION,0.10074626865671642,"easily extended to supervised fune-tuning (sft) to further improve model accuracy, or the
79"
INTRODUCTION,0.10199004975124377,"adaptation to the downstream sub-tasks.
80"
INTRODUCTION,0.10323383084577115,"1We define the term “layer"" as a linear transformation, “block"" as a common transformer block containing
the multi-head attention, feed forward, and some layer norm."
RELATED WORKS,0.1044776119402985,"2
Related Works
81"
RELATED WORKS,0.10572139303482588,"Quantization can be roughly divided into Quantization Aware Training (QAT) (21; 33) and Post-
82"
RELATED WORKS,0.10696517412935323,"Training Quantization (PTQ) (4; 35). In this paper, we focus on weight-only quantization in PTQ,
83"
RELATED WORKS,0.10820895522388059,"and we will only summarize a few works that are closely related to our work.
84"
RELATED WORKS,0.10945273631840796,"PTQ is commonly used for LLM quantization because it does not require a lot of GPU hours of
85"
RELATED WORKS,0.11069651741293532,"training with labeled datasets. AdaRound (25) and BRECQ (18) start from the rounding operation
86"
RELATED WORKS,0.11194029850746269,"and explore whether to round up or down is better. SqQR (7) and OWQ (17) use mixed-precision
87"
RELATED WORKS,0.11318407960199005,"quantization strategy to protect sensitive parameters, while AWQ (19) opts for scaling up the weights
88"
RELATED WORKS,0.11442786069651742,"of sensitive channels to reduce the loss of quantization of sensitive channels. OmniQuant (29) use
89"
RELATED WORKS,0.11567164179104478,"gradient decent to optimize for the weight clipping threshold and the rescale factors. In decoupleQ, we
90"
RELATED WORKS,0.11691542288557213,"abandon patchwork solutions and transform the quantization into a principled traditional optimization
91"
RELATED WORKS,0.1181592039800995,"problem by decoupling the model parameters into integer and floating-point parts.
92"
RELATED WORKS,0.11940298507462686,"GPTQ (9) is an influential work, and it quantizes the current weights and then updates the remaining
93"
RELATED WORKS,0.12064676616915423,"weights to minimize the ℓ2 loss of the output of the layer between pre- and post-quantization. As we
94"
RELATED WORKS,0.12189054726368159,"will see later, this update actually approximates much, and GPTQ does not optimize for the scale and
95"
RELATED WORKS,0.12313432835820895,"zero point reduced by quantization.
96"
RELATED WORKS,0.12437810945273632,"QALora (36) also decouples model parameters at a certain level and uses labeled datasets to fine-tune
97"
RELATED WORKS,0.1256218905472637,"the zero points. decoupleQ takes this idea a step further, optimizing the scales, zero points and norm
98"
RELATED WORKS,0.12686567164179105,"layers with supervised fine-tuning, while freezing the integer weights.
99"
METHODS,0.1281094527363184,"3
Methods
100"
PRELIMINARIES,0.12935323383084577,"3.1
Preliminaries
101"
PRELIMINARIES,0.13059701492537312,"For a linear layer with input dimension din and output dimension dout, quantization maps the weights
102"
PRELIMINARIES,0.1318407960199005,"with high-precision into discrete level, and the previous scheme can be described as follows:
103"
PRELIMINARIES,0.13308457711442787,"c
W = clip(⌊W0 −z"
PRELIMINARIES,0.13432835820895522,"s
⌉, α, β)
(3)
f
W = c
W ∗s + z
(4)
104"
PRELIMINARIES,0.13557213930348258,"where W0 ∈Rdin×dout is the pre-trained full precision weights, s and z are the scale and zero point
105"
PRELIMINARIES,0.13681592039800994,"(what we call floating-point part above), ⌊·⌉is the round-to-nearest function, c
W ∈Rdin×dout is the
106"
PRELIMINARIES,0.13805970149253732,"quantized integer-point matrix (what we call integer part above), f
W is the de-quantized floating-point
107"
PRELIMINARIES,0.13930348258706468,"matrix, α and β are the lower and upper bounds of the range of integer representations, respectively.
108"
PRELIMINARIES,0.14054726368159204,"For example, in 2-bit weight only linear quantization scheme, the value of each entry of c
W is
109"
PRELIMINARIES,0.1417910447761194,"limited to one of {−2, −1, 0, 1}, and α = −2, β = 1 in this case. To get the values of f
W, previous
110"
PRELIMINARIES,0.14303482587064675,"methods (8; 9) show that layer-wise ℓ2 loss between the outputs pre- and post-quantization is well
111"
PRELIMINARIES,0.14427860696517414,"related to the model accuracy, i.e., to optimize the following objective function,
112"
PRELIMINARIES,0.1455223880597015,"arg minf
W ∥Xf
W −XW0∥2
2 = tr{(f
W −W0)T H(f
W −W0)}
(5)"
PRELIMINARIES,0.14676616915422885,"where X ∈Rbatch×din is the input of this linear layer, generated by a small set of calibration dataset,
113"
PRELIMINARIES,0.1480099502487562,"and H = XT X.
114"
PRELIMINARIES,0.14925373134328357,"In the very low-bit quantization regime, the model accuracy can be further improved via finer-grained
115"
PRELIMINARIES,0.15049751243781095,"grouping. This would impose additional overhead on inference. For example, when groupsize = 64,
116"
PRELIMINARIES,0.1517412935323383,"it imposes an average overhead of 0.5 bit per element (FP16/BF16 for scale s and zero point z). The
117"
PRELIMINARIES,0.15298507462686567,"extra overhead is acceptable compared to the model accuracy gain.
118"
DECOUPLEQ,0.15422885572139303,"3.2
decoupleQ
119"
DECOUPLEQ,0.15547263681592038,"When a model is quantized, only the integer part c
W and the floating-point part (s, z) in Eq. 4 are
120"
DECOUPLEQ,0.15671641791044777,"delivered to the downstream inference engine, and the inference process does not need to know how
121"
DECOUPLEQ,0.15796019900497513,"c
W and (s, z) are obtained at all. That is, if we can find the values of c
W and (s, z) to minimize Eq. 5
122"
DECOUPLEQ,0.15920398009950248,"by other methods, then we don’t need to use Eq. 3. So, we can decouple the model parameters into
123"
DECOUPLEQ,0.16044776119402984,"integer part c
W and floating point part (s, z), which are then optimized alternatively via off-the-shelf
124"
DECOUPLEQ,0.16169154228855723,"solution methods. decoupleQ views the process of solving for c
W and (s, z) in Eq. 4 as a constrained
125"
DECOUPLEQ,0.16293532338308458,"optimization problem independent of the previous quantization paradigm! We only need to regard
126"
DECOUPLEQ,0.16417910447761194,"Eq. 4 as an ordinary affine transformation, in which the value of s can be 0 or even negative.
127"
DECOUPLEQ,0.1654228855721393,"In per-channel quantization, each column of the weight matrix is optimized independently of each
128"
DECOUPLEQ,0.16666666666666666,"other. For simplicity of notation, we only focus on one column in c
W later and re-define the notations.
129"
DECOUPLEQ,0.16791044776119404,"Based on Eq. 5, the optimization problem of decoupleQ in the first stage, layer-wise minimization,
130"
DECOUPLEQ,0.1691542288557214,"can then be formulated as:
131"
DECOUPLEQ,0.17039800995024876,"min
w;s,zg(w; s, z)"
DECOUPLEQ,0.17164179104477612,"s.t. ∀i = 1, 2, ..., din
wi −β ≤0
−wi + α ≤0
wi ∈Z (6)"
DECOUPLEQ,0.17288557213930347,"where the objective function is:
132"
DECOUPLEQ,0.17412935323383086,"g(w; s, z) = 1"
DECOUPLEQ,0.17537313432835822,"2(w ∗s + z −b)T H(w ∗s + z −b)
(7)"
DECOUPLEQ,0.17661691542288557,"w ∈Rdin is one column of c
W, b ∈Rdin is the corresponding column of W0, s ∈Rng is the
133"
DECOUPLEQ,0.17786069651741293,"scale and z ∈Rng is the zero point, ng is the number of groups when grouping-quantization. The
134"
DECOUPLEQ,0.1791044776119403,"operations w.r.t (s, z), i.e., ∗s and +z, need to be broadcasted to each group. In this paradigm,
135"
DECOUPLEQ,0.18034825870646767,"we have completely abandoned the traditional framework of quantization and instead transformed
136"
DECOUPLEQ,0.18159203980099503,"quantization into a constrained optimization problem 6, which is then solved to achieve the purpose
137"
DECOUPLEQ,0.1828358208955224,"of quantization. (s, z) in problem 6 have lost the traditional meaning of scale and zero point, and are
138"
DECOUPLEQ,0.18407960199004975,"just two optimization variables.
139"
DECOUPLEQ,0.1853233830845771,"Transforming the traditional quantization problem into problem 6 is the soul of decoupleQ! Problem 6
140"
DECOUPLEQ,0.1865671641791045,"is a quadratic programming problem with an additional non-convex constraints wi ∈Z. Quadratic
141"
DECOUPLEQ,0.18781094527363185,"programming has been studied for many years and there are now many well-established solution (24;
142"
DECOUPLEQ,0.1890547263681592,"34). We provide one solution in the next subsection, which may not be efficient or optimal.
143"
DECOUPLEQ,0.19029850746268656,"The core idea of decoupleQ is to decouple the model weights into the integer part w and the
144"
DECOUPLEQ,0.19154228855721392,"floating-point part (s, z), with the integer part occupying most of the model’s expressive power. The
145"
DECOUPLEQ,0.1927860696517413,"extensibility of the idea of decoupleQ is that we can freeze the integer part of the entire model, and
146"
DECOUPLEQ,0.19402985074626866,"use labeled data to train the (s, z) as well as other floating point parameters. The advantage of this is
147"
DECOUPLEQ,0.19527363184079602,"that on the one hand, it can further improve the accuracy of the model, on the other hand, it can fit
148"
DECOUPLEQ,0.19651741293532338,"specific downstream sub-tasks while maintaining the generalization ability of the model.
149"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.19776119402985073,"3.3
Optimization via Alternative Iteration
150"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.19900497512437812,"The problem 6 is not easy to solve because of the non-convex constraint wi ∈Z. After obtaining a
151"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.20024875621890548,"good initialization (explained in detail later), we solve for w and (s, z) alternately and iteratively. In
152"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.20149253731343283,"each round of alternation, the objective function 7 w.r.t (s, z) is an unconstrained quadratic function,
153"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.2027363184079602,"thus (s, z) can be readily determined analytically: by differentiating the objective function and
154"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.20398009950248755,"equating the derivative to zero, followed by solving the resultant linear system of equations. While
155"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.20522388059701493,"for w, the problem become problem 8:
156"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.2064676616915423,"min
w g(w; s, z)"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.20771144278606965,"s.t. ∀i = 1, 2, ..., din
wi −β ≤0
−wi + α ≤0
wi ∈Z (8)"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.208955223880597,"min
wi;i>jg(w; s, z)"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.21019900497512436,"s.t. ∀i = j + 1, ..., din
wi −β ≤0
−wi + α ≤0
wi ∈Z"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.21144278606965175,"(9)
157"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.2126865671641791,"For problem 8, one solution is to round-and-clip one element of w to be integer in [α, β] and then
158"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.21393034825870647,"update the remaining. And then this process is then performed sequentially for all elements. After the
159"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.21517412935323382,"j-th element has been rounded-and-clipped, the objective for the updating then becomes problem 9.
160"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.21641791044776118,"problem 9 is also intractable, and we can make two levels of approximation:
161"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.21766169154228857,"min
wi;i>jg(w; s, z)"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.21890547263681592,"s.t. ∀i = j + 1, ..., din
wi −β ≤0
−wi + α ≤0"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.22014925373134328,"(10)
min
wi;i>jg(w; s, z)
(11)
162"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.22139303482587064,"In the first-level approximation 10, only the non-convex constraint wi ∈Z is discarded, while in the
163"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.22263681592039802,"second-level approximation 11, both the non-convex constraint wi ∈Z and the convex constraint
164"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.22388059701492538,"wi ∈[α, β] are discarded. Intuitively, problem 11 is much simpler to solve than problem 10, but
165"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.22512437810945274,"solving problem 10 will lead to a better convergence of the primary objective( 6) than solving
166"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.2263681592039801,"problem 11. GPTQ (9) provides an efficient analytical solution for problem 11, which we will
167"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.22761194029850745,"directly utilize in our experiments. ( GPTQ updates the remaining elements by considering only the
168"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.22885572139303484,"second-level approximation 11 and ignoring the constrain wi ∈[α, β] in the first ( 10), which is what
169"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.2300995024875622,"we mentioned in the introduction, that the update of GPTQ is unconstrained.) As for problem 10,
170"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.23134328358208955,"there are many mature solutions in the field of convex optimization, such as active-set method,
171"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.2325870646766169,"projected gradient descent (PGD), projected coordinate descent and so on (3). We choose PGD
172"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.23383084577114427,"because its parallelization is much better than the other two methods. In the experimental part, we
173"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.23507462686567165,"will compare the final accuracy of the model via between solving the first level (10) and the second
174"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.236318407960199,"level 11 approximation on small models, while on large models (e.g. lager than 7 billion parameters),
175"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.23756218905472637,"we have to choose the second level 11 approximation because the intolerable runtime of solving the
176"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.23880597014925373,"first (10). The algorithm is shown in Alg. 1 and Alg. 2.
177"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.24004975124378108,"Algorithm 1: Alternative Iteration to
solve problem 6.
Input: predefined iteration number N.
Result: w∗, s∗, z∗"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.24129353233830847,"1 Initialize t = 1, w0, s0, z0;"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.24253731343283583,2 while t ≤N do
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.24378109452736318,"3
Freeze (st−1, zt−1), and optimize
g(w; st−1, zt−1) to obtain an
approximate solution wt via
solving 8 via 2;"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.24502487562189054,"4
Freeze wt, and solve the
unconstraint quadratic equation
g(wt; s, z) to obtain an analytic
solution for (st, zt);"
OPTIMIZATION VIA ALTERNATIVE ITERATION,0.2462686567164179,"5
t = t + 1"
END,0.24751243781094528,6 end
END,0.24875621890547264,7 w∗= wN; s∗= sN; z∗= zN
END,0.25,"Algorithm 2: Approximate solution of 8
Input: predefined iteration number K, M, and
the froozen (s, z).
Result: w∗"
END,0.2512437810945274,1 if Approximaton (10) is used then
END,0.2524875621890547,"2
Ignoring the constraint wi ∈Z in Eq. 8, and
train Eq. 8 with M iterations via PGD;"
END,0.2537313432835821,3 Initialize j = 1;
END,0.25497512437810943,4 for j = 1 →din do
END,0.2562189054726368,"5
round and clip the j-th element of w, then
keep the first j elements frozen, and
update the remainings via PGD to
optimize 10 with K iterations or until
converged, or via the method in GPTQ to
optimize 11."
END,0.2574626865671642,6 end
END,0.25870646766169153,7 w∗= w 178
END,0.2599502487562189,"3.4
Initialization of w and (s, z)
179"
END,0.26119402985074625,"min
p
1
2(w ∗s + z −b)T H(w ∗s + z −b) s.t."
END,0.26243781094527363,w = clip(⌊b −z
END,0.263681592039801,"s
⌉, α, β)"
END,0.26492537313432835,s = p ∗(bmax −bbmin)
END,0.26616915422885573,"β −α
z = p ∗bmin −s ∗α (12)"
END,0.26741293532338306,"Since the values of w are discrete, a good ini-
180"
END,0.26865671641791045,"tialization is very important in order to obtain a
181"
END,0.26990049751243783,"more accurate solution to the original problem 6
182"
END,0.27114427860696516,"with a faster convergence. Intuitively, the func-
183"
END,0.27238805970149255,"tion g(w; s, z) contains the term w ∗s, which
184"
END,0.2736318407960199,"means that the scales of the initial values of w
185"
END,0.27487562189054726,"and s have to be reasonably distributed. For ex-
186"
END,0.27611940298507465,"ample, in the extreme case when the initial value
187"
END,0.277363184079602,"of (s, z) have a very large scale, the first itera-
188"
END,0.27860696517412936,"tion will make most of the entries of w strictly
189"
END,0.2798507462686567,"0, which will make the iteration crash. We start
190"
END,0.2810945273631841,"by initializing (s, z). We can use grid search to
191"
END,0.28233830845771146,"solve the Eq. 12 for the initial value of (s, z). In
192"
END,0.2835820895522388,"Eq. 12, p is a single number, may be different
193"
END,0.2848258706467662,"for different columns of W0, bmin and bmax are the minimum and maximum value of b respectively.
194"
END,0.2860696517412935,"This step is the same as the previous post-training quantization (19) process. Once the grid search is
195"
END,0.2873134328358209,"finished, we no longer need to concern ourselves with the (s, z) inside the ⌊·⌉function. The point of
196"
END,0.2885572139303483,"this step is simply to find an initial value for (s, z) for the optimization problem 6.
197"
END,0.2898009950248756,"When solving problem 8 via the first-level approximation ( 10), before entering the for-loop in Alg. 2,
198"
END,0.291044776119403,"we ignore the constraint wi ∈Z in problem 8 and optimize it via projected gradient decent with M
199"
END,0.2922885572139303,"iterations. The purpose of this is to allow the first-level approximation to converge in a small number
200"
END,0.2935323383084577,"of iterations, i.e., a small K.
201"
BLOCK-WISE MINIMIZATION,0.2947761194029851,"3.5
Block-wise minimization
202"
BLOCK-WISE MINIMIZATION,0.2960199004975124,"After solving problem 6, we obtain a solution for the layer-wise minimization stage and a reasonable
203"
BLOCK-WISE MINIMIZATION,0.2972636815920398,"model accuracy. But minimizing the ℓ2 loss at the layer level does not necessarily lead to the
204"
BLOCK-WISE MINIMIZATION,0.29850746268656714,"minimizing the ℓ2 loss at the block level. We found that the model accuracy can be further improved
205"
BLOCK-WISE MINIMIZATION,0.2997512437810945,"via optimization 2. BRECQ (18) also shows that block-reconstruction results in a better model
206"
BLOCK-WISE MINIMIZATION,0.3009950248756219,"accuracy than layer-reconstruction. In this stage, we freeze the integer part c
W in the whole block and
207"
BLOCK-WISE MINIMIZATION,0.30223880597014924,"fine-tuning (s, z) and the parameters in norm layer with J epochs.
208"
EXPERIMENTS,0.3034825870646766,"4
Experiments
209"
EXPERIMENTS,0.30472636815920395,"In this section, we describe in detail the experimental results of our method in comparison with other
210"
EXPERIMENTS,0.30597014925373134,"methods. Unless otherwise stated, all the experiments are conducted on a single A100-SXM-80GB,
211"
EXPERIMENTS,0.3072139303482587,"and the default experimental setting is as follows:
212"
EXPERIMENTS,0.30845771144278605,"ResNet: 10240 images in the training dateloader are used as calibration data, with the standard
213"
EXPERIMENTS,0.30970149253731344,"augmentation in Pytorch official code (27), and the pretrained full precision checkpoints are from
214"
EXPERIMENTS,0.31094527363184077,"Torchvision (22). N = 4, M = 50 (N and M is defined in refalg1 and refalg2). All the convolution
215"
EXPERIMENTS,0.31218905472636815,"layers and fully-connected layers are quantized into W2 without groups.
216"
EXPERIMENTS,0.31343283582089554,"Llama-1/2: 128 2048-token segments from C4 (28) are used as calibration data. We choose C4
217"
EXPERIMENTS,0.31467661691542287,"as calibration dataset instead of WikiText2 (23) to be consistent with GPTQ. If the block-wise
218"
EXPERIMENTS,0.31592039800995025,"minimization is used, we use Adam optimizer (15) to finetune the (s, z) and the parameters in norm
219"
EXPERIMENTS,0.31716417910447764,"layer with J = 4 epochs. The learning rate is 1e-5, weight decay is 1e-6.
220"
PRIVATE EXPERIMENTS,0.31840796019900497,"4.1
Private Experiments
221"
PRIVATE EXPERIMENTS,0.31965174129353235,"Figure 1: The latency (in 1e-6 seconds) of the
four GEMMs in transformer block on L4 GPU,
(The three GEMMs for query, key and value are
concatenated into GEMM 1), with hidden_dim =
5120, batch_size = 4."
PRIVATE EXPERIMENTS,0.3208955223880597,"We applied decoupleQ to our company’s two
222"
PRIVATE EXPERIMENTS,0.32213930348258707,"Automatic Speech Recognition models(ASR)
223"
PRIVATE EXPERIMENTS,0.32338308457711445,"(corresponding to task A and task B). Each of
224"
PRIVATE EXPERIMENTS,0.3246268656716418,"the models contain an encoder and an LLM de-
225"
PRIVATE EXPERIMENTS,0.32587064676616917,"coder. The input of the models is a speech se-
226"
PRIVATE EXPERIMENTS,0.3271144278606965,"quence and some prompt, and the output is the
227"
PRIVATE EXPERIMENTS,0.3283582089552239,"corresponding text. We quantize the LLM de-
228"
PRIVATE EXPERIMENTS,0.32960199004975127,"coder to W2A16g64. The decoders of the two
229"
PRIVATE EXPERIMENTS,0.3308457711442786,"models contain 40 transformer blocks with 13
230"
PRIVATE EXPERIMENTS,0.332089552238806,"billion parameters and 32 transformer blocks
231"
PRIVATE EXPERIMENTS,0.3333333333333333,"with 7 billion parameters, respectively. Word Er-
232"
PRIVATE EXPERIMENTS,0.3345771144278607,"ror Rate (WER) is used as metric to measure the
233"
PRIVATE EXPERIMENTS,0.3358208955223881,"accuracy of the models (less is better). In this
234"
PRIVATE EXPERIMENTS,0.3370646766169154,"experiments, we use about 8 millions of speech
235"
PRIVATE EXPERIMENTS,0.3383084577114428,"tokens as calibration dataset, and train 3 epoch
236"
PRIVATE EXPERIMENTS,0.33955223880597013,"in each block-wise minimization process. When
237"
PRIVATE EXPERIMENTS,0.3407960199004975,"an input batch contains sequences of varying
238"
PRIVATE EXPERIMENTS,0.3420398009950249,"lengths, we use a mask to make sure that the
239"
PRIVATE EXPERIMENTS,0.34328358208955223,"padding part is not involved in the computation of H and the loss of Eq. 2. In task B, once the whole
240"
PRIVATE EXPERIMENTS,0.3445273631840796,"model is quantized, we also fine-tune all the (s, z) and layer norm in the LLM with labeled dataset,
241"
PRIVATE EXPERIMENTS,0.34577114427860695,"while freezing all the integer part c
W, with 8 A100-SXM-80GB GPUs. The accuracy is shown in
242"
PRIVATE EXPERIMENTS,0.34701492537313433,"Tab. 1, and the CUDA kernel latency is shown in Fig. 1. The W2A16 CUDA kernel is attached and
243"
PRIVATE EXPERIMENTS,0.3482587064676617,"will be merged into the NVIDIA repo as one of our core contribution.
244"
PRIVATE EXPERIMENTS,0.34950248756218905,"Table 1: The results of our two ASR models. The models are quantized into W2A16g64. runtime for
the quantization process is measured in hours. There are two sub-domains in task B, and we report
the WER of both."
PRIVATE EXPERIMENTS,0.35074626865671643,"Task A
Task B
BF16
decoupleQ
BF16
decoupleQ
decoupleQ+sft
WER
6.68
6.70
(5.86, 11.43)
(5.87, 11.56)
(5.77, 11.43)
runtime
-
25
-
32
32+5"
PRIVATE EXPERIMENTS,0.35199004975124376,"Table 2: Comparison of decoupleQ with other methods. In decoupleQ, we only use the first stage,
layer-wise minimization. All the models are quantized into W2A16 without groups. In decoupleQ+sft,
we train the (s, z) and norm layers for one epoch, using the regular labeled dataset containing 1.2
million images."
PRIVATE EXPERIMENTS,0.35323383084577115,"method
res18-69.76%
res50-76.13%
2bit
3bit
4bit
2bit
3bit
4bit
GPTQ
-
67.88
69.37
-
74.87
75.71
OBQ
64.04
68.69
69.56
70.71
75.24
75.72
BRECQ
64.70
68.47
69.37
72.41
75.32
75.88
decoupleQ
64.15
68.65
69.58
71.34
75.24
76.00
decoupleQ+sft
65.45
68.94
69.71
72.65
75.61
75.97"
PUBLIC COMPARISON,0.35447761194029853,"4.2
Public Comparison
245"
PUBLIC COMPARISON,0.35572139303482586,"As a first comparison, we compare decoupleQ with other methods on ImageNet (5) with ResNet (12),
246"
PUBLIC COMPARISON,0.35696517412935325,"which are standard benchmarks and are efficient to implement. Most importantly, its Top-1 is a strong
247"
PUBLIC COMPARISON,0.3582089552238806,"indicator of model accuracy. Tab. 2 shows the results of decoupleQ and others. The results other than
248"
PUBLIC COMPARISON,0.35945273631840796,"decoupleQ are copied from GPTQ (9) and OBQ (8).
249"
PUBLIC COMPARISON,0.36069651741293535,"Tab. 3 shows the results on Llama. In this experiment, we have to choose the second level approxima-
250"
PUBLIC COMPARISON,0.3619402985074627,"tion(11) because the intolerable runtime of solving the first(10). For a fair comparison, the calibration
251"
PUBLIC COMPARISON,0.36318407960199006,"dataset contains 128 samples, although a larger calibration dataset will result in stronger results.
252"
PUBLIC COMPARISON,0.3644278606965174,"we can see that decoupleQ outperforms others almost in all settings, although we use a weaker
253"
PUBLIC COMPARISON,0.3656716417910448,"approximation(11) to save time. As for the hype-parameters, we choose {N = 4, J = 4}.
254"
ABLATION STUDIES,0.36691542288557216,"4.3
Ablation studies
255"
THE TWO APPROXIMATIONS,0.3681592039800995,"4.3.1
the two approximations
256"
THE TWO APPROXIMATIONS,0.3694029850746269,"The soul of decoupleQ is problem 6, but when solving problem 6, we have to take some approxima-
257"
THE TWO APPROXIMATIONS,0.3706467661691542,"tions(10 or 11). Obviously, solving approximation 10 will be much more time consuming than solving
258"
THE TWO APPROXIMATIONS,0.3718905472636816,"approximation 11. But if solving approximation 10 yields better results, the time cost may be worth
259"
THE TWO APPROXIMATIONS,0.373134328358209,"it. We first evaluate these two approximations from the perspective of model accuracy. In practice,
260"
THE TWO APPROXIMATIONS,0.3743781094527363,"we don’t have to wait for approximation 10 to fully converge when we solve it via projected gradient
261"
THE TWO APPROXIMATIONS,0.3756218905472637,"decent, and only need to iterate some steps to get a sub-optimal solution. In Alg. 2, the for-loop takes
262"
THE TWO APPROXIMATIONS,0.376865671641791,"up the majority of the runtime. So, we first study the influence of the number of iterations K (defined
263"
THE TWO APPROXIMATIONS,0.3781094527363184,"in the for-loop) on the final accuracy of the model. Fig. 2 shows the Top-1 accuracy of ResNet-18 on
264"
THE TWO APPROXIMATIONS,0.3793532338308458,"ImageNet w.r.t the number of iterations K. First of all, in the blue line, we use only the layer-wise
265"
THE TWO APPROXIMATIONS,0.3805970149253731,"minimization of decooupleQ to quantize the model. After the quantization is finished, in the red line,
266"
THE TWO APPROXIMATIONS,0.3818407960199005,"we use the labeled dataset with the common 1.2 millions images to fine-tune all the (s, z) and norm
267"
THE TWO APPROXIMATIONS,0.38308457711442784,"layers for one epoch, with the integer part being frozen. In this step, we use SGD optimizer with
268"
THE TWO APPROXIMATIONS,0.3843283582089552,"learning rate 1e-6, weight decaying rate 1e-4 to train for only one epoch. Fig. 2 clearly indicates the
269"
THE TWO APPROXIMATIONS,0.3855721393034826,"following conclusions: 1. As the number of iterations K increases, the model accuracy increases
270"
THE TWO APPROXIMATIONS,0.38681592039800994,"almost monotonically; 2. When K > 4, model accuracy via the first approximation(10) is better than
271"
THE TWO APPROXIMATIONS,0.3880597014925373,"via the second(11). This is to be expected, since the second approximation(11) drops the constraint
272"
THE TWO APPROXIMATIONS,0.38930348258706465,"α ≤wi ≤β, leading to a looser approximation; 3. By the supervised fine-tuning (sft), the model
273"
THE TWO APPROXIMATIONS,0.39054726368159204,"accuracy is further improved. The same experimental phenomenon also occurs on the ResNet-50
274"
THE TWO APPROXIMATIONS,0.3917910447761194,"model, which we do not show here.
275"
THE TWO APPROXIMATIONS,0.39303482587064675,"Table 3: The results of PPL of wikitext-2 on Llama-1/2. We also report the runtime (measured in
hours) for the W2 quantization via decoupleQ in the gray background row. The results other than
decoupleQ are copied from OmniQuant (29). All the results of decoupleQ use the approximation 11."
THE TWO APPROXIMATIONS,0.39427860696517414,"Llama
1-7B
1-13B
1-30B
1-65B
2-7B
2-13B
2-70B
FP16
5.68
5.09
4.10
3.53
5.47
4.88
3.31
GPTQ
2.1e3
5.5e3
499.75
55.91
7.7e3
2.1e3
77.95
OmniQuant
15.47
13.21
8.71
7.58
37.37
17.21
7.81
decoupleQ
9.49
7.86
6.37
5.59
9.74
13.03
5.23
W2A16"
THE TWO APPROXIMATIONS,0.39552238805970147,"runtime
2.5
4.8
12.7
27.6
2.5
4.5
33.4
GPTQ
44.01
15.60
10.92
9.51
36.77
28.14
-
OmniQuant
8.90
7.34
6.59
5.65
9.62
7.56
6.11
decoupleQ
8.65
7.25
6.04
5.19
8.79
7.44
4.96
W2A16g128"
THE TWO APPROXIMATIONS,0.39676616915422885,"runtime
3.7
7.7
24.3
55.0
3.7
7.9
70.6
GPTQ
22.10
10.06
8.54
8.31
20.85
22.44
-
OmniQuant
8.90
7.34
6.59
5.65
9.62
7.56
6.11
decoupleQ
8.18
6.96
5.81
5.07
8.41
6.98
5.34
W2A16g64"
THE TWO APPROXIMATIONS,0.39800995024875624,"runtime
4.3
8.9
27.9
64.5
4.4
9.0
98.2
GPTQ
8.06
6.76
5.84
5.06
8.37
6.44
4.82
AWQ
11.88
7.45
10.07
5.21
24.00
10.45
-
OmniQuant
6.49
5.68
4.74
4.04
6.58
5.58
3.92
W3A16"
THE TWO APPROXIMATIONS,0.39925373134328357,"decoupleQ
6.38
5.60
4.67
6.05
6.22
5.72
3.84
GPTQ
6.13
5.40
4.48
3.83
5.83
5.13
3.58
AWQ
6.08
5.34
4.39
3.76
6.15
5.12
-
OmniQuant
5.86
5.21
4.25
3.71
5.74
5.02
3.47
W4A16"
THE TWO APPROXIMATIONS,0.40049751243781095,"decoupleQ
5.85
5.21
4.24
3.67
5.70
5.06
3.45"
THE TWO APPROXIMATIONS,0.4017412935323383,"Figure 2: The Top-1 accuracy of ResNet-18
on ImageNet. Solid and dashed lines are for
approximation 10 and 11 respectively."
THE TWO APPROXIMATIONS,0.40298507462686567,"Figure 3: The PPL of Llama-7B on Wiki-
Text2. Solid and dashed lines are for approxi-
mation 10 and 11 respectively."
THE TWO APPROXIMATIONS,0.40422885572139305,"In the experiment shown in 3, we randomly select 512 2048-token segments from C4 (28). We chose
276"
THE TWO APPROXIMATIONS,0.4054726368159204,"512 segments here instead of the common 128 in order to reduce the effect of overfitting and thus
277"
THE TWO APPROXIMATIONS,0.40671641791044777,"compare the two approximations more objectively. In this experiment, we take N = 2, and quantize
278"
THE TWO APPROXIMATIONS,0.4079601990049751,"Llama-7B into W2A16 without groups, and only the layer-wise minimization is used to exclude the
279"
THE TWO APPROXIMATIONS,0.4092039800995025,"interference of other factors. The PPL decrease almost monotonically as the number of iterations K
280"
THE TWO APPROXIMATIONS,0.41044776119402987,"increases. It shows that, when K > 1, solving approximation 10 yields better model accuracy than
281"
THE TWO APPROXIMATIONS,0.4116915422885572,"approximation 11.
282"
THE TWO APPROXIMATIONS,0.4129353233830846,"However, when block-wise minimization is introduced in addition to the experiment in 3, the situation
283"
THE TWO APPROXIMATIONS,0.4141791044776119,"becomes a little more elusive. The results are shown in 4. The model’s best PPL is where K = 1,
284"
THE TWO APPROXIMATIONS,0.4154228855721393,"and then fluctuates within a range as K continues to increase. But all PPLs are inferior to when
285"
THE TWO APPROXIMATIONS,0.4166666666666667,"the second-level approximation (11) is used. We also plot the loss, defined in 2, of the first block
286"
THE TWO APPROXIMATIONS,0.417910447761194,"between pre-and post quantization on the right vertical axis. As K increases, the loss decreases
287"
THE TWO APPROXIMATIONS,0.4191542288557214,"strictly monotonically, and when K > 2, the loss falls below the case when the approximation 11 is
288"
THE TWO APPROXIMATIONS,0.42039800995024873,"used. This suggests that the correlation between PPL and loss is perhaps weak, and we will investigate
289"
THE TWO APPROXIMATIONS,0.4216417910447761,"this in the future.
290"
THE TWO APPROXIMATIONS,0.4228855721393035,"Figure 4: The PPL of Llama-7B on WikiText2
and the loss of the first block between pre-and
post-quantization. Solid and dashed lines are
for approximation 10 and 11 respectively."
THE TWO APPROXIMATIONS,0.42412935323383083,"Figure 5: The perplexity of Llama-7B on
WikiText2 and C4 dataset w.r.t the number of
segments as calibration datasets. The model
is quantized into W2A16g64."
THE SIZE OF CALIBRATION DATASET,0.4253731343283582,"4.3.2
the size of calibration dataset
291"
THE SIZE OF CALIBRATION DATASET,0.42661691542288555,"The solution of problem 6 is dependent on H and thus on the the calibration dataset, as does Eq. 2.
292"
THE SIZE OF CALIBRATION DATASET,0.42786069651741293,"Fig. 5 shows the relationship between dataset size and PPL. In this experiment, Llama-7B is quantized
293"
THE SIZE OF CALIBRATION DATASET,0.4291044776119403,"into W2A16g64. We use the second-level approximation (11) to save time, and {N = 4, J = 4}. For
294"
THE SIZE OF CALIBRATION DATASET,0.43034825870646765,"runtime reference, when the number of segments is 128/2048, the experiment took 4.3/19.5 hours.
295"
THE NECESSITY OF BLOCK-WISE MINIMIZATION,0.43159203980099503,"4.3.3
the necessity of block-wise minimization
296"
THE NECESSITY OF BLOCK-WISE MINIMIZATION,0.43283582089552236,"Table 4: The perplexity of Llama on WikiText2 with
and without the block-wise minimization. All the mod-
els are quantized into W2A16."
THE NECESSITY OF BLOCK-WISE MINIMIZATION,0.43407960199004975,"Llama
1-7B
1-13B
1-30B
2-7B
2-13B
w/o
13.66
9.68
7.35
14.66
12.93
w
9.49
7.86
6.37
9.74
13.03"
THE NECESSITY OF BLOCK-WISE MINIMIZATION,0.43532338308457713,"Tab. 4 shows that block-wise minimiza-
297"
THE NECESSITY OF BLOCK-WISE MINIMIZATION,0.43656716417910446,"tion(2) can further improve the model accu-
298"
THE NECESSITY OF BLOCK-WISE MINIMIZATION,0.43781094527363185,"racy. In this experiment, we choose N = 4
299"
THE NECESSITY OF BLOCK-WISE MINIMIZATION,0.43905472636815923,"and the approximation 11 for the layer-wise
300"
THE NECESSITY OF BLOCK-WISE MINIMIZATION,0.44029850746268656,"minimization, and J = 4 if block-wise min-
301"
THE NECESSITY OF BLOCK-WISE MINIMIZATION,0.44154228855721395,"imization is used.
302"
CONCLUSION AND DISCUSSION,0.4427860696517413,"5
Conclusion and Discussion
303"
CONCLUSION AND DISCUSSION,0.44402985074626866,"deocupleQ decouples the model parameters into the integer part and a floating point part, and
304"
CONCLUSION AND DISCUSSION,0.44527363184079605,"then optimizes them alternately. This optimization process contains two stages. In the layer-wise
305"
CONCLUSION AND DISCUSSION,0.4465174129353234,"minimization, we transform the quantization problem into the purely mathematical constrained
306"
CONCLUSION AND DISCUSSION,0.44776119402985076,"optimization problem refdecoupleQ; while in the block-wise minimization, we freeze the integer part
307"
CONCLUSION AND DISCUSSION,0.4490049751243781,"and then finetune the floating point part.
308"
CONCLUSION AND DISCUSSION,0.4502487562189055,"The risks of decoupleQ include the following: 1. How much the minimization of the ℓ2 loss of
309"
CONCLUSION AND DISCUSSION,0.45149253731343286,"the layer’s or block’s output correlates with the accuracy of the model; 2. decoupleQ is prone to
310"
CONCLUSION AND DISCUSSION,0.4527363184079602,"overfitting the calibration dataset; 3. The runtime of the quantization process is longer than others.
311"
CONCLUSION AND DISCUSSION,0.4539800995024876,"For the first risk, we find experimentally that the correlation between Top-1 and the loss is strong in
312"
CONCLUSION AND DISCUSSION,0.4552238805970149,"the Imagenet classification task; however, the correlation between PPL and the loss is slightly weaker
313"
CONCLUSION AND DISCUSSION,0.4564676616915423,"in LLM. This could be mainly because of an inherent bias between the loss and the accuracy of the
314"
CONCLUSION AND DISCUSSION,0.4577114427860697,"model, or because PPL is not a good indicator of the accuracy of LLM, or for other reasons. For
315"
CONCLUSION AND DISCUSSION,0.458955223880597,"the second risk, when H in Eq. 7 is an underdetermined matrix, the risk of overfitting rises sharply.
316"
CONCLUSION AND DISCUSSION,0.4601990049751244,"In this case, the possibility of H being underdetermined can be reduced either by enhancing the
317"
CONCLUSION AND DISCUSSION,0.4614427860696517,"diagonal element values of H or by increasing the amount of calibration data. In our practice, we
318"
CONCLUSION AND DISCUSSION,0.4626865671641791,"found that the accuracy of quantization models can rise monotonically with the increase of the size of
319"
CONCLUSION AND DISCUSSION,0.4639303482587065,"the calibration dataset, especially in W2 quantization, but the runtime of quantization rise as well. In
320"
CONCLUSION AND DISCUSSION,0.4651741293532338,"addition, due to time constraints, we do not provide a wealth of public comparisons. However, we
321"
CONCLUSION AND DISCUSSION,0.4664179104477612,"believe that the novelty of a method may outweigh the number of experiments.
322"
CONCLUSION AND DISCUSSION,0.46766169154228854,"The idea of decoupleQ is helpful for the adaptation of large model to downstream sub-task. We can
323"
CONCLUSION AND DISCUSSION,0.4689054726368159,"quantize a large foundation model via decoupleQ, then freeze the integer part of the model, and
324"
CONCLUSION AND DISCUSSION,0.4701492537313433,"finetune the floating-point part with labeled dataset from downstream sub-task. Tab. 1 and Tab. 2
325"
CONCLUSION AND DISCUSSION,0.47139303482587064,"show that the model accuracy can be further improved by end-to-end supervised learning.
326"
REFERENCES,0.472636815920398,"References
327"
REFERENCES,0.47388059701492535,"[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
328"
REFERENCES,0.47512437810945274,"Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
329"
REFERENCES,0.4763681592039801,"Advances in neural information processing systems, 33:1877–1901, 2020.
330"
REFERENCES,0.47761194029850745,"[2] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
331"
REFERENCES,0.47885572139303484,"Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early
332"
REFERENCES,0.48009950248756217,"experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
333"
REFERENCES,0.48134328358208955,"[3] Sébastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends® in
334"
REFERENCES,0.48258706467661694,"Machine Learning, 8(3-4):231–357, 2015.
335"
REFERENCES,0.48383084577114427,"[4] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large
336"
REFERENCES,0.48507462686567165,"language models with guarantees. arXiv preprint arXiv:2307.13304, 2023.
337"
REFERENCES,0.486318407960199,"[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
338"
REFERENCES,0.48756218905472637,"image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255.
339"
REFERENCES,0.48880597014925375,"Ieee, 2009.
340"
REFERENCES,0.4900497512437811,"[6] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication
341"
REFERENCES,0.49129353233830847,"for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.
342"
REFERENCES,0.4925373134328358,"[7] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos,
343"
REFERENCES,0.4937810945273632,"Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for
344"
REFERENCES,0.49502487562189057,"near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023.
345"
REFERENCES,0.4962686567164179,"[8] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training
346"
REFERENCES,0.4975124378109453,"quantization and pruning. Advances in Neural Information Processing Systems, 35:4475–4488, 2022.
347"
REFERENCES,0.4987562189054726,"[9] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Optq: Accurate quantization for
348"
REFERENCES,0.5,"generative pre-trained transformers. In The Eleventh International Conference on Learning Representations,
349"
REFERENCES,0.5012437810945274,"2022.
350"
REFERENCES,0.5024875621890548,"[10] Yi Guo, Yiqian He, Xiaoyang Li, Haotong Qin, Van Tung Pham, Yang Zhang, and Shouda Liu. Rdimkd:
351"
REFERENCES,0.503731343283582,"Generic distillation paradigm by dimensionality reduction. arXiv preprint arXiv:2312.08700, 2023.
352"
REFERENCES,0.5049751243781094,"[11] Yi Guo, Huan Yuan, Jianchao Tan, Zhangyang Wang, Sen Yang, and Ji Liu. Gdp: Stabilized neural
353"
REFERENCES,0.5062189054726368,"network pruning via gates with differentiable polarization. In Proceedings of the IEEE/CVF International
354"
REFERENCES,0.5074626865671642,"Conference on Computer Vision, pages 5239–5250, 2021.
355"
REFERENCES,0.5087064676616916,"[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
356"
REFERENCES,0.5099502487562189,"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
357"
REFERENCES,0.5111940298507462,"[13] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry.
Accurate post training
358"
REFERENCES,0.5124378109452736,"quantization with small calibration sets. In International Conference on Machine Learning, pages 4466–
359"
REFERENCES,0.513681592039801,"4475. PMLR, 2021.
360"
REFERENCES,0.5149253731343284,"[14] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney,
361"
REFERENCES,0.5161691542288557,"and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.
362"
REFERENCES,0.5174129353233831,"[15] Diederik P Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
arXiv preprint
363"
REFERENCES,0.5186567164179104,"arXiv:1412.6980, 2014.
364"
REFERENCES,0.5199004975124378,"[16] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper.
365"
REFERENCES,0.5211442786069652,"arXiv preprint arXiv:1806.08342, 2018.
366"
REFERENCES,0.5223880597014925,"[17] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. Owq: Lessons learned from
367"
REFERENCES,0.5236318407960199,"activation outliers for weight quantization in large language models. arXiv preprint arXiv:2306.02272,
368"
REFERENCES,0.5248756218905473,"2023.
369"
REFERENCES,0.5261194029850746,"[18] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi
370"
REFERENCES,0.527363184079602,"Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint
371"
REFERENCES,0.5286069651741293,"arXiv:2102.05426, 2021.
372"
REFERENCES,0.5298507462686567,"[19] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware
373"
REFERENCES,0.5310945273631841,"weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.
374"
REFERENCES,0.5323383084577115,"[20] Zechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, and Zhiqiang Shen. Nonuniform-to-uniform
375"
REFERENCES,0.5335820895522388,"quantization: Towards accurate quantization via generalized straight-through estimation. In Proceedings of
376"
REFERENCES,0.5348258706467661,"the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4942–4952, 2022.
377"
REFERENCES,0.5360696517412935,"[21] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,
378"
REFERENCES,0.5373134328358209,"Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large
379"
REFERENCES,0.5385572139303483,"language models. arXiv preprint arXiv:2305.17888, 2023.
380"
REFERENCES,0.5398009950248757,"[22] Sébastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In Proceedings
381"
REFERENCES,0.5410447761194029,"of the 18th ACM international conference on Multimedia, pages 1485–1488, 2010.
382"
REFERENCES,0.5422885572139303,"[23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.
383"
REFERENCES,0.5435323383084577,"arXiv preprint arXiv:1609.07843, 2016.
384"
REFERENCES,0.5447761194029851,"[24] Katta G Murty and Feng-Tien Yu. Linear complementarity, linear and nonlinear programming, volume 3.
385"
REFERENCES,0.5460199004975125,"Heldermann Berlin, 1988.
386"
REFERENCES,0.5472636815920398,"[25] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or
387"
REFERENCES,0.5485074626865671,"down? adaptive rounding for post-training quantization. In International Conference on Machine Learning,
388"
REFERENCES,0.5497512437810945,"pages 7197–7206. PMLR, 2020.
389"
REFERENCES,0.5509950248756219,"[26] Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bronstein, and
390"
REFERENCES,0.5522388059701493,"Avi Mendelson. Loss aware post-training quantization. Machine Learning, 110(11-12):3245–3262, 2021.
391"
REFERENCES,0.5534825870646766,"[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
392"
REFERENCES,0.554726368159204,"Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
393"
REFERENCES,0.5559701492537313,"learning library. Advances in neural information processing systems, 32, 2019.
394"
REFERENCES,0.5572139303482587,"[28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
395"
REFERENCES,0.5584577114427861,"Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
396"
REFERENCES,0.5597014925373134,"Journal of machine learning research, 21(140):1–67, 2020.
397"
REFERENCES,0.5609452736318408,"[29] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng
398"
REFERENCES,0.5621890547263682,"Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language
399"
REFERENCES,0.5634328358208955,"models. arXiv preprint arXiv:2308.13137, 2023.
400"
REFERENCES,0.5646766169154229,"[30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
401"
REFERENCES,0.5659203980099502,"Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
402"
REFERENCES,0.5671641791044776,"language models. arXiv preprint arXiv:2302.13971, 2023.
403"
REFERENCES,0.568407960199005,"[31] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
404"
REFERENCES,0.5696517412935324,"Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and
405"
REFERENCES,0.5708955223880597,"fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
406"
REFERENCES,0.572139303482587,"[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
407"
REFERENCES,0.5733830845771144,"Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,
408"
REFERENCES,0.5746268656716418,"30, 2017.
409"
REFERENCES,0.5758706467661692,"[33] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong
410"
REFERENCES,0.5771144278606966,"Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal
411"
REFERENCES,0.5783582089552238,"shifting and scaling. arXiv preprint arXiv:2304.09145, 2023.
412"
REFERENCES,0.5796019900497512,"[34] Stephen J Wright. Numerical optimization. 2006.
413"
REFERENCES,0.5808457711442786,"[35] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant:
414"
REFERENCES,0.582089552238806,"Accurate and efficient post-training quantization for large language models. In International Conference
415"
REFERENCES,0.5833333333333334,"on Machine Learning, pages 38087–38099. PMLR, 2023.
416"
REFERENCES,0.5845771144278606,"[36] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng
417"
REFERENCES,0.585820895522388,"Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models. arXiv
418"
REFERENCES,0.5870646766169154,"preprint arXiv:2309.14717, 2023.
419"
REFERENCES,0.5883084577114428,"[37] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
420"
REFERENCES,0.5895522388059702,"Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.
421"
REFERENCES,0.5907960199004975,"arXiv preprint arXiv:2205.01068, 2022.
422"
REFERENCES,0.5920398009950248,"[38] Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li,
423"
REFERENCES,0.5932835820895522,"Vera Axelrod, Gary Wang, et al. Google usm: Scaling automatic speech recognition beyond 100 languages.
424"
REFERENCES,0.5945273631840796,"arXiv preprint arXiv:2303.01037, 2023.
425"
REFERENCES,0.595771144278607,"NeurIPS Paper Checklist
426"
CLAIMS,0.5970149253731343,"1. Claims
427"
CLAIMS,0.5982587064676617,"Question: Do the main claims made in the abstract and introduction accurately reflect the
428"
CLAIMS,0.599502487562189,"paper’s contributions and scope?
429"
CLAIMS,0.6007462686567164,"Answer: [Yes]
430"
CLAIMS,0.6019900497512438,"Justification: Our claims and justification include:
431"
CLAIMS,0.6032338308457711,"a. Our results are higher than others in very low bit (2-bit) quantization.( This is
432"
CLAIMS,0.6044776119402985,"justified in Tab. 3.);
433"
CLAIMS,0.6057213930348259,"b. decoupleQ has achieved comparable accuracy as fp16/bf16 for 2-bit quantiza-
434"
CLAIMS,0.6069651741293532,"tion of large speech models in our company. (This is justified in Tab. 1, and the
435"
CLAIMS,0.6082089552238806,"W2 CUDA kernel used in our company are attached.);
436"
CLAIMS,0.6094527363184079,"c. decoupleQ gets rid of any tricks for dealing with outliers, sensitive channels,
437"
CLAIMS,0.6106965174129353,"etc. (This is justified in the Problem 6, we do not use any tricks, such as scaling
438"
CLAIMS,0.6119402985074627,"factor (19; 29), mixed-precision quantization (6), etc., to deal with outliers and
439"
CLAIMS,0.6131840796019901,"sensitive channels.)
440"
CLAIMS,0.6144278606965174,"Guidelines:
441"
CLAIMS,0.6156716417910447,"• The answer NA means that the abstract and introduction do not include the claims
442"
CLAIMS,0.6169154228855721,"made in the paper.
443"
CLAIMS,0.6181592039800995,"• The abstract and/or introduction should clearly state the claims made, including the
444"
CLAIMS,0.6194029850746269,"contributions made in the paper and important assumptions and limitations. A No or
445"
CLAIMS,0.6206467661691543,"NA answer to this question will not be perceived well by the reviewers.
446"
CLAIMS,0.6218905472636815,"• The claims made should match theoretical and experimental results, and reflect how
447"
CLAIMS,0.6231343283582089,"much the results can be expected to generalize to other settings.
448"
CLAIMS,0.6243781094527363,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
449"
CLAIMS,0.6256218905472637,"are not attained by the paper.
450"
LIMITATIONS,0.6268656716417911,"2. Limitations
451"
LIMITATIONS,0.6281094527363185,"Question: Does the paper discuss the limitations of the work performed by the authors?
452"
LIMITATIONS,0.6293532338308457,"Answer: [Yes]
453"
LIMITATIONS,0.6305970149253731,"Justification: The paper has discussed the three limitations of decoupleQ in the last section,
454"
LIMITATIONS,0.6318407960199005,"Conclusion and Discussion, and the risk overall that we did not provide as many public
455"
LIMITATIONS,0.6330845771144279,"comparison experiments as other work due to time constraints.
456"
LIMITATIONS,0.6343283582089553,"Guidelines:
457"
LIMITATIONS,0.6355721393034826,"• The answer NA means that the paper has no limitation while the answer No means that
458"
LIMITATIONS,0.6368159203980099,"the paper has limitations, but those are not discussed in the paper.
459"
LIMITATIONS,0.6380597014925373,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
460"
LIMITATIONS,0.6393034825870647,"• The paper should point out any strong assumptions and how robust the results are to
461"
LIMITATIONS,0.6405472636815921,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
462"
LIMITATIONS,0.6417910447761194,"model well-specification, asymptotic approximations only holding locally). The authors
463"
LIMITATIONS,0.6430348258706468,"should reflect on how these assumptions might be violated in practice and what the
464"
LIMITATIONS,0.6442786069651741,"implications would be.
465"
LIMITATIONS,0.6455223880597015,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
466"
LIMITATIONS,0.6467661691542289,"only tested on a few datasets or with a few runs. In general, empirical results often
467"
LIMITATIONS,0.6480099502487562,"depend on implicit assumptions, which should be articulated.
468"
LIMITATIONS,0.6492537313432836,"• The authors should reflect on the factors that influence the performance of the approach.
469"
LIMITATIONS,0.650497512437811,"For example, a facial recognition algorithm may perform poorly when image resolution
470"
LIMITATIONS,0.6517412935323383,"is low or images are taken in low lighting. Or a speech-to-text system might not be
471"
LIMITATIONS,0.6529850746268657,"used reliably to provide closed captions for online lectures because it fails to handle
472"
LIMITATIONS,0.654228855721393,"technical jargon.
473"
LIMITATIONS,0.6554726368159204,"• The authors should discuss the computational efficiency of the proposed algorithms
474"
LIMITATIONS,0.6567164179104478,"and how they scale with dataset size.
475"
LIMITATIONS,0.6579601990049752,"• If applicable, the authors should discuss possible limitations of their approach to
476"
LIMITATIONS,0.6592039800995025,"address problems of privacy and fairness.
477"
LIMITATIONS,0.6604477611940298,"• While the authors might fear that complete honesty about limitations might be used by
478"
LIMITATIONS,0.6616915422885572,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
479"
LIMITATIONS,0.6629353233830846,"limitations that aren’t acknowledged in the paper. The authors should use their best
480"
LIMITATIONS,0.664179104477612,"judgment and recognize that individual actions in favor of transparency play an impor-
481"
LIMITATIONS,0.6654228855721394,"tant role in developing norms that preserve the integrity of the community. Reviewers
482"
LIMITATIONS,0.6666666666666666,"will be specifically instructed to not penalize honesty concerning limitations.
483"
THEORY ASSUMPTIONS AND PROOFS,0.667910447761194,"3. Theory Assumptions and Proofs
484"
THEORY ASSUMPTIONS AND PROOFS,0.6691542288557214,"Question: For each theoretical result, does the paper provide the full set of assumptions and
485"
THEORY ASSUMPTIONS AND PROOFS,0.6703980099502488,"a complete (and correct) proof?
486"
THEORY ASSUMPTIONS AND PROOFS,0.6716417910447762,"Answer:[NA]
487"
THEORY ASSUMPTIONS AND PROOFS,0.6728855721393034,"Justification: This paper does not include theoretical results.
488"
THEORY ASSUMPTIONS AND PROOFS,0.6741293532338308,"Guidelines:
489"
THEORY ASSUMPTIONS AND PROOFS,0.6753731343283582,"• The answer NA means that the paper does not include theoretical results.
490"
THEORY ASSUMPTIONS AND PROOFS,0.6766169154228856,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
491"
THEORY ASSUMPTIONS AND PROOFS,0.677860696517413,"referenced.
492"
THEORY ASSUMPTIONS AND PROOFS,0.6791044776119403,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
493"
THEORY ASSUMPTIONS AND PROOFS,0.6803482587064676,"• The proofs can either appear in the main paper or the supplemental material, but if
494"
THEORY ASSUMPTIONS AND PROOFS,0.681592039800995,"they appear in the supplemental material, the authors are encouraged to provide a short
495"
THEORY ASSUMPTIONS AND PROOFS,0.6828358208955224,"proof sketch to provide intuition.
496"
THEORY ASSUMPTIONS AND PROOFS,0.6840796019900498,"• Inversely, any informal proof provided in the core of the paper should be complemented
497"
THEORY ASSUMPTIONS AND PROOFS,0.6853233830845771,"by formal proofs provided in appendix or supplemental material.
498"
THEORY ASSUMPTIONS AND PROOFS,0.6865671641791045,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
499"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6878109452736318,"4. Experimental Result Reproducibility
500"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6890547263681592,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
501"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6902985074626866,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
502"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6915422885572139,"of the paper (regardless of whether the code and data are provided or not)?
503"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6927860696517413,"Answer: [Yes]
504"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6940298507462687,"Justification: At the beginning of the section Experiments, we provide details of the experi-
505"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.695273631840796,"mental parameters; specifically for each experiment, we also provide the key experimental
506"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6965174129353234,"parameters.
507"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6977611940298507,"Guidelines:
508"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6990049751243781,"• The answer NA means that the paper does not include experiments.
509"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7002487562189055,"• If the paper includes experiments, a No answer to this question will not be perceived
510"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7014925373134329,"well by the reviewers: Making the paper reproducible is important, regardless of
511"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7027363184079602,"whether the code and data are provided or not.
512"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7039800995024875,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
513"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7052238805970149,"to make their results reproducible or verifiable.
514"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7064676616915423,"• Depending on the contribution, reproducibility can be accomplished in various ways.
515"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7077114427860697,"For example, if the contribution is a novel architecture, describing the architecture fully
516"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7089552238805971,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
517"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7101990049751243,"be necessary to either make it possible for others to replicate the model with the same
518"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7114427860696517,"dataset, or provide access to the model. In general. releasing code and data is often
519"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7126865671641791,"one good way to accomplish this, but reproducibility can also be provided via detailed
520"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7139303482587065,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
521"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7151741293532339,"of a large language model), releasing of a model checkpoint, or other means that are
522"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7164179104477612,"appropriate to the research performed.
523"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7176616915422885,"• While NeurIPS does not require releasing code, the conference does require all submis-
524"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7189054726368159,"sions to provide some reasonable avenue for reproducibility, which may depend on the
525"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7201492537313433,"nature of the contribution. For example
526"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7213930348258707,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
527"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.722636815920398,"to reproduce that algorithm.
528"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7238805970149254,"(b) If the contribution is primarily a new model architecture, the paper should describe
529"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7251243781094527,"the architecture clearly and fully.
530"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7263681592039801,"(c) If the contribution is a new model (e.g., a large language model), then there should
531"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7276119402985075,"either be a way to access this model for reproducing the results or a way to reproduce
532"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7288557213930348,"the model (e.g., with an open-source dataset or instructions for how to construct
533"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7300995024875622,"the dataset).
534"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7313432835820896,"(d) We recognize that reproducibility may be tricky in some cases, in which case
535"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7325870646766169,"authors are welcome to describe the particular way they provide for reproducibility.
536"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7338308457711443,"In the case of closed-source models, it may be that access to the model is limited in
537"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7350746268656716,"some way (e.g., to registered users), but it should be possible for other researchers
538"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.736318407960199,"to have some path to reproducing or verifying the results.
539"
OPEN ACCESS TO DATA AND CODE,0.7375621890547264,"5. Open access to data and code
540"
OPEN ACCESS TO DATA AND CODE,0.7388059701492538,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
541"
OPEN ACCESS TO DATA AND CODE,0.7400497512437811,"tions to faithfully reproduce the main experimental results, as described in supplemental
542"
OPEN ACCESS TO DATA AND CODE,0.7412935323383084,"material?
543"
OPEN ACCESS TO DATA AND CODE,0.7425373134328358,"Answer: [Yes]
544"
OPEN ACCESS TO DATA AND CODE,0.7437810945273632,"Justification: The code (including W2 CUDA kernels) is attached in supplementary material,
545"
OPEN ACCESS TO DATA AND CODE,0.7450248756218906,"and can reproduce the results in the public experiments.
546"
OPEN ACCESS TO DATA AND CODE,0.746268656716418,"Guidelines:
547"
OPEN ACCESS TO DATA AND CODE,0.7475124378109452,"• The answer NA means that paper does not include experiments requiring code.
548"
OPEN ACCESS TO DATA AND CODE,0.7487562189054726,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
549"
OPEN ACCESS TO DATA AND CODE,0.75,"public/guides/CodeSubmissionPolicy) for more details.
550"
OPEN ACCESS TO DATA AND CODE,0.7512437810945274,"• While we encourage the release of code and data, we understand that this might not be
551"
OPEN ACCESS TO DATA AND CODE,0.7524875621890548,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
552"
OPEN ACCESS TO DATA AND CODE,0.753731343283582,"including code, unless this is central to the contribution (e.g., for a new open-source
553"
OPEN ACCESS TO DATA AND CODE,0.7549751243781094,"benchmark).
554"
OPEN ACCESS TO DATA AND CODE,0.7562189054726368,"• The instructions should contain the exact command and environment needed to run to
555"
OPEN ACCESS TO DATA AND CODE,0.7574626865671642,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
556"
OPEN ACCESS TO DATA AND CODE,0.7587064676616916,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
557"
OPEN ACCESS TO DATA AND CODE,0.7599502487562189,"• The authors should provide instructions on data access and preparation, including how
558"
OPEN ACCESS TO DATA AND CODE,0.7611940298507462,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
559"
OPEN ACCESS TO DATA AND CODE,0.7624378109452736,"• The authors should provide scripts to reproduce all experimental results for the new
560"
OPEN ACCESS TO DATA AND CODE,0.763681592039801,"proposed method and baselines. If only a subset of experiments are reproducible, they
561"
OPEN ACCESS TO DATA AND CODE,0.7649253731343284,"should state which ones are omitted from the script and why.
562"
OPEN ACCESS TO DATA AND CODE,0.7661691542288557,"• At submission time, to preserve anonymity, the authors should release anonymized
563"
OPEN ACCESS TO DATA AND CODE,0.7674129353233831,"versions (if applicable).
564"
OPEN ACCESS TO DATA AND CODE,0.7686567164179104,"• Providing as much information as possible in supplemental material (appended to the
565"
OPEN ACCESS TO DATA AND CODE,0.7699004975124378,"paper) is recommended, but including URLs to data and code is permitted.
566"
OPEN ACCESS TO DATA AND CODE,0.7711442786069652,"6. Experimental Setting/Details
567"
OPEN ACCESS TO DATA AND CODE,0.7723880597014925,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
568"
OPEN ACCESS TO DATA AND CODE,0.7736318407960199,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
569"
OPEN ACCESS TO DATA AND CODE,0.7748756218905473,"results?
570"
OPEN ACCESS TO DATA AND CODE,0.7761194029850746,"Answer: [Yes]
571"
OPEN ACCESS TO DATA AND CODE,0.777363184079602,"Justification: At the beginning of the section Experiments, we provide details of the experi-
572"
OPEN ACCESS TO DATA AND CODE,0.7786069651741293,"mental parameters; specifically for each experiment, we also provide the key experimental
573"
OPEN ACCESS TO DATA AND CODE,0.7798507462686567,"parameters. The code is attached in supplementary material and will be made public.
574"
OPEN ACCESS TO DATA AND CODE,0.7810945273631841,"Guidelines:
575"
OPEN ACCESS TO DATA AND CODE,0.7823383084577115,"• The answer NA means that the paper does not include experiments.
576"
OPEN ACCESS TO DATA AND CODE,0.7835820895522388,"• The experimental setting should be presented in the core of the paper to a level of detail
577"
OPEN ACCESS TO DATA AND CODE,0.7848258706467661,"that is necessary to appreciate the results and make sense of them.
578"
OPEN ACCESS TO DATA AND CODE,0.7860696517412935,"• The full details can be provided either with the code, in appendix, or as supplemental
579"
OPEN ACCESS TO DATA AND CODE,0.7873134328358209,"material.
580"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7885572139303483,"7. Experiment Statistical Significance
581"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7898009950248757,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
582"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7910447761194029,"information about the statistical significance of the experiments?
583"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7922885572139303,"Answer: [No]
584"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7935323383084577,"Justification: The cost of the experiment is high.
585"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7947761194029851,"Guidelines:
586"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7960199004975125,"• The answer NA means that the paper does not include experiments.
587"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7972636815920398,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
588"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7985074626865671,"dence intervals, or statistical significance tests, at least for the experiments that support
589"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7997512437810945,"the main claims of the paper.
590"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8009950248756219,"• The factors of variability that the error bars are capturing should be clearly stated (for
591"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8022388059701493,"example, train/test split, initialization, random drawing of some parameter, or overall
592"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8034825870646766,"run with given experimental conditions).
593"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.804726368159204,"• The method for calculating the error bars should be explained (closed form formula,
594"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8059701492537313,"call to a library function, bootstrap, etc.)
595"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8072139303482587,"• The assumptions made should be given (e.g., Normally distributed errors).
596"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8084577114427861,"• It should be clear whether the error bar is the standard deviation or the standard error
597"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8097014925373134,"of the mean.
598"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8109452736318408,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
599"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8121890547263682,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
600"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8134328358208955,"of Normality of errors is not verified.
601"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8146766169154229,"• For asymmetric distributions, the authors should be careful not to show in tables or
602"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8159203980099502,"figures symmetric error bars that would yield results that are out of range (e.g. negative
603"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8171641791044776,"error rates).
604"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.818407960199005,"• If error bars are reported in tables or plots, The authors should explain in the text how
605"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8196517412935324,"they were calculated and reference the corresponding figures or tables in the text.
606"
EXPERIMENTS COMPUTE RESOURCES,0.8208955223880597,"8. Experiments Compute Resources
607"
EXPERIMENTS COMPUTE RESOURCES,0.822139303482587,"Question: For each experiment, does the paper provide sufficient information on the com-
608"
EXPERIMENTS COMPUTE RESOURCES,0.8233830845771144,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
609"
EXPERIMENTS COMPUTE RESOURCES,0.8246268656716418,"the experiments?
610"
EXPERIMENTS COMPUTE RESOURCES,0.8258706467661692,"Answer: [Yes]
611"
EXPERIMENTS COMPUTE RESOURCES,0.8271144278606966,"Justification: We have reported that most of the experiments are conducted in one single
612"
EXPERIMENTS COMPUTE RESOURCES,0.8283582089552238,"A100-SXM-80GB, except for the sft process. And we also reported the time of execution.
613"
EXPERIMENTS COMPUTE RESOURCES,0.8296019900497512,"Guidelines:
614"
EXPERIMENTS COMPUTE RESOURCES,0.8308457711442786,"• The answer NA means that the paper does not include experiments.
615"
EXPERIMENTS COMPUTE RESOURCES,0.832089552238806,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
616"
EXPERIMENTS COMPUTE RESOURCES,0.8333333333333334,"or cloud provider, including relevant memory and storage.
617"
EXPERIMENTS COMPUTE RESOURCES,0.8345771144278606,"• The paper should provide the amount of compute required for each of the individual
618"
EXPERIMENTS COMPUTE RESOURCES,0.835820895522388,"experimental runs as well as estimate the total compute.
619"
EXPERIMENTS COMPUTE RESOURCES,0.8370646766169154,"• The paper should disclose whether the full research project required more compute
620"
EXPERIMENTS COMPUTE RESOURCES,0.8383084577114428,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
621"
EXPERIMENTS COMPUTE RESOURCES,0.8395522388059702,"didn’t make it into the paper).
622"
CODE OF ETHICS,0.8407960199004975,"9. Code Of Ethics
623"
CODE OF ETHICS,0.8420398009950248,"Question: Does the research conducted in the paper conform, in every respect, with the
624"
CODE OF ETHICS,0.8432835820895522,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
625"
CODE OF ETHICS,0.8445273631840796,"Answer: [Yes]
626"
CODE OF ETHICS,0.845771144278607,"Justification: The research conducted in the paper conform, in every respect, with the
627"
CODE OF ETHICS,0.8470149253731343,"NeurIPS Code of Ethics
628"
CODE OF ETHICS,0.8482587064676617,"Guidelines:
629"
CODE OF ETHICS,0.849502487562189,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
630"
CODE OF ETHICS,0.8507462686567164,"• If the authors answer No, they should explain the special circumstances that require a
631"
CODE OF ETHICS,0.8519900497512438,"deviation from the Code of Ethics.
632"
CODE OF ETHICS,0.8532338308457711,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
633"
CODE OF ETHICS,0.8544776119402985,"eration due to laws or regulations in their jurisdiction).
634"
BROADER IMPACTS,0.8557213930348259,"10. Broader Impacts
635"
BROADER IMPACTS,0.8569651741293532,"Question: Does the paper discuss both potential positive societal impacts and negative
636"
BROADER IMPACTS,0.8582089552238806,"societal impacts of the work performed?
637"
BROADER IMPACTS,0.8594527363184079,"Answer: [NA]
638"
BROADER IMPACTS,0.8606965174129353,"Justification: This is a work for accelerating the inference of deep models, where the social
639"
BROADER IMPACTS,0.8619402985074627,"impact is determined by the function of the model, not by how the inference is accelerated.
640"
BROADER IMPACTS,0.8631840796019901,"Guidelines:
641"
BROADER IMPACTS,0.8644278606965174,"• The answer NA means that there is no societal impact of the work performed.
642"
BROADER IMPACTS,0.8656716417910447,"• If the authors answer NA or No, they should explain why their work has no societal
643"
BROADER IMPACTS,0.8669154228855721,"impact or why the paper does not address societal impact.
644"
BROADER IMPACTS,0.8681592039800995,"• Examples of negative societal impacts include potential malicious or unintended uses
645"
BROADER IMPACTS,0.8694029850746269,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
646"
BROADER IMPACTS,0.8706467661691543,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
647"
BROADER IMPACTS,0.8718905472636815,"groups), privacy considerations, and security considerations.
648"
BROADER IMPACTS,0.8731343283582089,"• The conference expects that many papers will be foundational research and not tied
649"
BROADER IMPACTS,0.8743781094527363,"to particular applications, let alone deployments. However, if there is a direct path to
650"
BROADER IMPACTS,0.8756218905472637,"any negative applications, the authors should point it out. For example, it is legitimate
651"
BROADER IMPACTS,0.8768656716417911,"to point out that an improvement in the quality of generative models could be used to
652"
BROADER IMPACTS,0.8781094527363185,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
653"
BROADER IMPACTS,0.8793532338308457,"that a generic algorithm for optimizing neural networks could enable people to train
654"
BROADER IMPACTS,0.8805970149253731,"models that generate Deepfakes faster.
655"
BROADER IMPACTS,0.8818407960199005,"• The authors should consider possible harms that could arise when the technology is
656"
BROADER IMPACTS,0.8830845771144279,"being used as intended and functioning correctly, harms that could arise when the
657"
BROADER IMPACTS,0.8843283582089553,"technology is being used as intended but gives incorrect results, and harms following
658"
BROADER IMPACTS,0.8855721393034826,"from (intentional or unintentional) misuse of the technology.
659"
BROADER IMPACTS,0.8868159203980099,"• If there are negative societal impacts, the authors could also discuss possible mitigation
660"
BROADER IMPACTS,0.8880597014925373,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
661"
BROADER IMPACTS,0.8893034825870647,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
662"
BROADER IMPACTS,0.8905472636815921,"feedback over time, improving the efficiency and accessibility of ML).
663"
SAFEGUARDS,0.8917910447761194,"11. Safeguards
664"
SAFEGUARDS,0.8930348258706468,"Question: Does the paper describe safeguards that have been put in place for responsible
665"
SAFEGUARDS,0.8942786069651741,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
666"
SAFEGUARDS,0.8955223880597015,"image generators, or scraped datasets)?
667"
SAFEGUARDS,0.8967661691542289,"Answer: [NA]
668"
SAFEGUARDS,0.8980099502487562,"Justification: This work does not release models or datasets.
669"
SAFEGUARDS,0.8992537313432836,"Guidelines:
670"
SAFEGUARDS,0.900497512437811,"• The answer NA means that the paper poses no such risks.
671"
SAFEGUARDS,0.9017412935323383,"• Released models that have a high risk for misuse or dual-use should be released with
672"
SAFEGUARDS,0.9029850746268657,"necessary safeguards to allow for controlled use of the model, for example by requiring
673"
SAFEGUARDS,0.904228855721393,"that users adhere to usage guidelines or restrictions to access the model or implementing
674"
SAFEGUARDS,0.9054726368159204,"safety filters.
675"
SAFEGUARDS,0.9067164179104478,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
676"
SAFEGUARDS,0.9079601990049752,"should describe how they avoided releasing unsafe images.
677"
SAFEGUARDS,0.9092039800995025,"• We recognize that providing effective safeguards is challenging, and many papers do
678"
SAFEGUARDS,0.9104477611940298,"not require this, but we encourage authors to take this into account and make a best
679"
SAFEGUARDS,0.9116915422885572,"faith effort.
680"
LICENSES FOR EXISTING ASSETS,0.9129353233830846,"12. Licenses for existing assets
681"
LICENSES FOR EXISTING ASSETS,0.914179104477612,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
682"
LICENSES FOR EXISTING ASSETS,0.9154228855721394,"the paper, properly credited and are the license and terms of use explicitly mentioned and
683"
LICENSES FOR EXISTING ASSETS,0.9166666666666666,"properly respected?
684"
LICENSES FOR EXISTING ASSETS,0.917910447761194,"Answer: [Yes]
685"
LICENSES FOR EXISTING ASSETS,0.9191542288557214,"Justification: The paper has cited all related works, and included the relevant license.
686"
LICENSES FOR EXISTING ASSETS,0.9203980099502488,"Guidelines:
687"
LICENSES FOR EXISTING ASSETS,0.9216417910447762,"• The answer NA means that the paper does not use existing assets.
688"
LICENSES FOR EXISTING ASSETS,0.9228855721393034,"• The authors should cite the original paper that produced the code package or dataset.
689"
LICENSES FOR EXISTING ASSETS,0.9241293532338308,"• The authors should state which version of the asset is used and, if possible, include a
690"
LICENSES FOR EXISTING ASSETS,0.9253731343283582,"URL.
691"
LICENSES FOR EXISTING ASSETS,0.9266169154228856,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
692"
LICENSES FOR EXISTING ASSETS,0.927860696517413,"• For scraped data from a particular source (e.g., website), the copyright and terms of
693"
LICENSES FOR EXISTING ASSETS,0.9291044776119403,"service of that source should be provided.
694"
LICENSES FOR EXISTING ASSETS,0.9303482587064676,"• If assets are released, the license, copyright information, and terms of use in the
695"
LICENSES FOR EXISTING ASSETS,0.931592039800995,"package should be provided. For popular datasets, paperswithcode.com/datasets
696"
LICENSES FOR EXISTING ASSETS,0.9328358208955224,"has curated licenses for some datasets. Their licensing guide can help determine the
697"
LICENSES FOR EXISTING ASSETS,0.9340796019900498,"license of a dataset.
698"
LICENSES FOR EXISTING ASSETS,0.9353233830845771,"• For existing datasets that are re-packaged, both the original license and the license of
699"
LICENSES FOR EXISTING ASSETS,0.9365671641791045,"the derived asset (if it has changed) should be provided.
700"
LICENSES FOR EXISTING ASSETS,0.9378109452736318,"• If this information is not available online, the authors are encouraged to reach out to
701"
LICENSES FOR EXISTING ASSETS,0.9390547263681592,"the asset’s creators.
702"
NEW ASSETS,0.9402985074626866,"13. New Assets
703"
NEW ASSETS,0.9415422885572139,"Question: Are new assets introduced in the paper well documented and is the documentation
704"
NEW ASSETS,0.9427860696517413,"provided alongside the assets?
705"
NEW ASSETS,0.9440298507462687,"Answer: [Yes]
706"
NEW ASSETS,0.945273631840796,"Justification: We provide the source code, and a readme and license file are alongside.
707"
NEW ASSETS,0.9465174129353234,"Guidelines:
708"
NEW ASSETS,0.9477611940298507,"• The answer NA means that the paper does not release new assets.
709"
NEW ASSETS,0.9490049751243781,"• Researchers should communicate the details of the dataset/code/model as part of their
710"
NEW ASSETS,0.9502487562189055,"submissions via structured templates. This includes details about training, license,
711"
NEW ASSETS,0.9514925373134329,"limitations, etc.
712"
NEW ASSETS,0.9527363184079602,"• The paper should discuss whether and how consent was obtained from people whose
713"
NEW ASSETS,0.9539800995024875,"asset is used.
714"
NEW ASSETS,0.9552238805970149,"• At submission time, remember to anonymize your assets (if applicable). You can either
715"
NEW ASSETS,0.9564676616915423,"create an anonymized URL or include an anonymized zip file.
716"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9577114427860697,"14. Crowdsourcing and Research with Human Subjects
717"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9589552238805971,"Question: For crowdsourcing experiments and research with human subjects, does the paper
718"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9601990049751243,"include the full text of instructions given to participants and screenshots, if applicable, as
719"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9614427860696517,"well as details about compensation (if any)?
720"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9626865671641791,"Answer: [NA]
721"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9639303482587065,"Justification: the paper does not involve crowdsourcing nor research with human subjects
722"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9651741293532339,"Guidelines:
723"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9664179104477612,"• The answer NA means that the paper does not involve crowdsourcing nor research with
724"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9676616915422885,"human subjects.
725"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9689054726368159,"• Including this information in the supplemental material is fine, but if the main contribu-
726"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9701492537313433,"tion of the paper involves human subjects, then as much detail as possible should be
727"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9713930348258707,"included in the main paper.
728"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972636815920398,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
729"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9738805970149254,"or other labor should be paid at least the minimum wage in the country of the data
730"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9751243781094527,"collector.
731"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9763681592039801,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
732"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776119402985075,"Subjects
733"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9788557213930348,"Question: Does the paper describe potential risks incurred by study participants, whether
734"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9800995024875622,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
735"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813432835820896,"approvals (or an equivalent approval/review based on the requirements of your country or
736"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825870646766169,"institution) were obtained?
737"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9838308457711443,"Answer: [NA]
738"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850746268656716,"Justification: the paper does not involve crowdsourcing nor research with human subjects
739"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986318407960199,"Guidelines:
740"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875621890547264,"• The answer NA means that the paper does not involve crowdsourcing nor research with
741"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888059701492538,"human subjects.
742"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900497512437811,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
743"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912935323383084,"may be required for any human subjects research. If you obtained IRB approval, you
744"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925373134328358,"should clearly state this in the paper.
745"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937810945273632,"• We recognize that the procedures for this may vary significantly between institutions
746"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9950248756218906,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996268656716418,"guidelines for their institution.
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9975124378109452,"• For initial submissions, do not include any information that would break anonymity (if
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9987562189054726,"applicable), such as the institution conducting the review.
750"
