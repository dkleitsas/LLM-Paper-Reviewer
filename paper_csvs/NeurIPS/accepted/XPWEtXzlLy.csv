Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0025380710659898475,"Modern successes of diffusion models in learning complex, high-dimensional data
distributions are attributed, in part, to their capability to construct diffusion pro-
cesses with analytic transition kernels and score functions. The tractability results
in a simulation-free framework with stable regression losses, from which reversed,
generative processes can be learned at scale. However, when data is conÔ¨Åned to a
constrained set as opposed to a standard Euclidean space, these desirable character-
istics appear to be lost based on prior attempts. In this work, we propose Mirror
Diffusion Models (MDM), a new class of diffusion models that generate data on
convex constrained sets without losing any tractability. This is achieved by learning
diffusion processes in a dual space constructed from a mirror map, which, crucially,
is a standard Euclidean space. We derive efÔ¨Åcient computation of mirror maps
for popular constrained sets, such as simplices and ‚Ñì2-balls, showing signiÔ¨Åcantly
improved performance of MDM over existing methods. For safety and privacy
purposes, we also explore constrained sets as a new mechanism to embed invisible
but quantitative information (i.e., watermarks) in generated data, for which MDM
serves as a compelling approach. Our work brings new algorithmic opportunities
for learning tractable diffusion on complex domains."
INTRODUCTION,0.005076142131979695,"1
Introduction"
INTRODUCTION,0.007614213197969543,MDM: learns tractable diffusions in dual spaces
INTRODUCTION,0.01015228426395939,Corresponding nonlinear diffusions in ‚Ñ≥
INTRODUCTION,0.012690355329949238,"Figure 1: Mirror Diffusion Models (MDM) is a new class of diffusion models for convex constrained
manifolds M ‚äÜRd. (left) Instead of learning score-approximate diffusions on M, MDM applies
a mirror map ‚àáœÜ and learns tractable diffusions in its unconstrained dual-space ‚àáœÜ(M) = Rd.
(right) We also present MDM for watermarked generation, where generated contents (e.g., images)
live in a high-dimensional token constrained set M that is certiÔ¨Åable only from the private user."
INTRODUCTION,0.015228426395939087,"Diffusion models [1‚Äì3] have emerged as powerful generative models with their remarkable successes
in synthesizing high-Ô¨Ådelity data such as images [4, 5], audio [6, 7], and 3D geometry [8, 9].
These models work by progressively diffusing data to noise, and learning the score functions (often"
INTRODUCTION,0.017766497461928935,‚Ä†Equal advising.
INTRODUCTION,0.02030456852791878,"Table 1: Comparison of different diffusion models for constrained generation on M ‚äÜRd. Rather
than learning reÔ¨Çected diffusions on M with approximate scores, our Mirror Diffusion constructs
a mirror map ‚àáœÜ and lift the diffusion processes to the unconstrained dual space ‚àáœÜ(M) = Rd,
inheriting favorable features from standard Euclidean-space diffusion models. Constraints are satisÔ¨Åed
by construction via the inverse map ‚àáœÜ‚àó; that is, ‚àáœÜ‚àó(y) ‚ààM for all y ‚àà‚àáœÜ(M)."
INTRODUCTION,0.02284263959390863,"Diffusion models
Domain
Tractable
conditional score
Simulation-free
training
Regression
objective"
INTRODUCTION,0.025380710659898477,"ReÔ¨Çected Diffusion
Fishman et al. [18]
M



Lou and Ermon [25]
M

‚ñ≥2
"
INTRODUCTION,0.027918781725888325,"Mirror Diffusion
This work (ours)
‚àáœÜ(M)


"
INTRODUCTION,0.030456852791878174,"parameterized by neural networks) to reverse the processes [10]; the reversed processes thereby
provide transport maps that generate data from noise. Modern diffusion models [11‚Äì13] often employ
diffusion processes whose transition kernels are analytically available. This characteristic enables
simulation-free training, bypassing the necessity to simulate the underlying diffusion processes
by directly sampling the diffused data. It also leads to tractable conditional score functions and
simple regression objectives, facilitating computational scalability for high-dimensional problems. In
standard Euclidean spaces, tractability can be accomplished by designing the diffusion processes as
linear stochastic differential equations, but doing so for non-Euclidean spaces is nontrivial."
INTRODUCTION,0.03299492385786802,"Recent progress of diffusion models has expanded their application to non-Euclidean spaces, such as
Riemannian manifolds [14, 15], where data live in a curved geometry. The development of generative
models on manifolds has enabled various new applications, such as modeling earth and climate
events [16] and learning densities on meshes [17]. In this work, we focus on generative modeling
on constrained sets (also called constrained manifolds [18]) that are deÔ¨Åned by a set of inequality
constraints. Such constrained sets, denoted by M ‚äÜRd, are also ubiquitous in several scientiÔ¨Åc
Ô¨Åelds such as computational statistics [19], biology [20], and robotics [21, 22]."
INTRODUCTION,0.03553299492385787,"Previous endeavors in the direction of constrained generation have primarily culminated in reÔ¨Çected
diffusions [23, 24], which reÔ¨Çect the direction at the boundary ‚àÇM to ensure that samples remain
inside the constraints. Unfortunately, reÔ¨Çected diffusions do not possess closed-form transition
kernels [18, 25], thus necessitating the approximation of the conditional score functions that can
hinder learning performance. Although simulation-free training is still possible2, it comes with a
computational overhead due to the use of geometric techniques [25]. From a theoretical standpoint,
reÔ¨Çected Langevin dynamics are widely regarded for their extended mixing time [26, 27]. This
raises practical concerns when simulating reÔ¨Çected diffusions, as prior methods often adopted special
treatments such as thresholding and discretization [18, 25]."
INTRODUCTION,0.03807106598984772,"An alternative diffusion for constrained sampling is the Mirror Langevin Dynamics (MLD [28]).
MLD, as a subclass of Riemannian Langevin Dynamics [29, 30], is tailored speciÔ¨Åcally to convex
constrained sampling. SpeciÔ¨Åcally, MLD constructs a strictly convex function œÜ so that its gradient
map, often referred to as the mirror map ‚àáœÜ : M ‚ÜíRd, deÔ¨Ånes a nonlinear mapping from the initial
constrained set to an unconstrained dual space. Despite extensive investigation of MLD [31‚Äì34], its
potential as a foundation for designing diffusion generative models remains to be comprehensively
examined. Thus, there is a need to explore the potential of MLD for designing new, effective, and
tailored diffusion models for constrained generation."
INTRODUCTION,0.04060913705583756,"With this in mind, we propose Mirror Diffusion Models (MDM), a new class of diffusion generative
models for convex constrained sets. Similar to MLD, MDM utilizes mirror maps ‚àáœÜ that transform
data distributions on M to its dual space ‚àáœÜ(M). From there, MDM learns a dual-space diffusion
between the mirrored distribution and Gaussian prior in the dual space. Note that this is in contrast to
MLD, which constructs invariant distributions on the initial constrained set; MDM instead considers
dual-space priors. Since the dual space is also unconstrained Euclidean space ‚àáœÜ(M) = Rd, MDM"
INTRODUCTION,0.04314720812182741,2Lou and Ermon [25] proposed a semi-simulation-free training with the aid of geometric techniques.
INTRODUCTION,0.04568527918781726,"‚àáùúô‚Ñ≥
User-defined tokens"
INTRODUCTION,0.048223350253807105,"‚Ñ≥‚âîùë•‚àà‚Ñù"" ‚à∂ùëê# ‚â§ùëé#"
INTRODUCTION,0.050761421319796954,"$ùë•‚â§ùëè#, ‚àÄùëñ"
INTRODUCTION,0.0532994923857868,Constrained Manifold
INTRODUCTION,0.05583756345177665,External users
INTRODUCTION,0.0583756345177665,Pretrained (primal-space)
INTRODUCTION,0.06091370558375635,Diffusion Models
INTRODUCTION,0.06345177664974619,"Dual-Space 
Diffusion Models ‚Ñ≥"
INTRODUCTION,0.06598984771573604,"Watermark
Dataset
‚àáùúô‚Ñ≥ ‚àó ‚Ñù"""
INTRODUCTION,0.06852791878172589,"Figure 2: MDM for watermarked generation: (left) We Ô¨Årst construct a constrained set M based on a
set of user-deÔ¨Åned tokens private to other users. (right) MDM can be instantiated by either learning
the corresponding dual-space diffusions, or projecting pretrained, i.e., unwatermarked, diffusion
models onto M. In both cases, MDM embeds watermarks that are certiÔ¨Åable only from the user."
INTRODUCTION,0.07106598984771574,"preserves many important features of standard diffusion models, such as simulation-free training and
tractable conditional scores, which yields simple regression objectives; see Table 1. After learning
the dual-space distribution, MDM applies the inverse mapping ‚àáœÜ‚àó, transforming mirrored samples
back to the constrained set. We propose efÔ¨Åcient construction of mirror maps and demonstrate that
MDM outperforms prior reÔ¨Çected diffusion models on common constrained sets, such as ‚Ñì2-balls and
simplices."
INTRODUCTION,0.07360406091370558,"Moreover, we show that MDM also stands as a novel approach for watermarked generation, a
technique that aims to embed undetectable information (i.e., watermarks) into generated contents
to prevent unethical usage and ensure copyright protection. This newly emerging goal nevertheless
attracted signiÔ¨Åcant attention in natural language generation [35‚Äì37], while what we will present is a
general algorithmic strategy demonstrated for image generations."
INTRODUCTION,0.07614213197969544,"To generate watermarked contents that are private to individual users, we begin by constructing a
constrained set M based on user-deÔ¨Åned tokens (see Figure 2). MDM can then be instantiated to
learn the dual-space distribution. Alternatively, MDM can distill pretrained diffusion models [38] by
projecting the generated data onto the constraint set. In either case, MDM generates watermarked
contents that can be certiÔ¨Åed only by users who know the tokens. Our empirical results suggest that
MDM serves as a competitive approach to prior watermarking techniques for diffusion models, such
as those proposed by Zhao et al. [39], by achieving lower (hence better) FID values."
INTRODUCTION,0.07868020304568528,"In summary, we present the following contributions:"
INTRODUCTION,0.08121827411167512,"‚Ä¢ We introduce Mirror Diffusion Models (MDM), a new class of diffusion models that
utilizes mirror maps to enable constrained generation with Euclidean-space diffusion models."
INTRODUCTION,0.08375634517766498,"‚Ä¢ We propose efÔ¨Åcient computation of mirror maps on common constrained sets, including
balls and simplices. Our results demonstrate that MDM consistently outperforms previous
reÔ¨Çected diffusion models."
INTRODUCTION,0.08629441624365482,"‚Ä¢ As a novel application of constrained generation, we explore MDM‚Äôs potential for water-
marking and copyrighting diffusion models where constraints serve as private tokens. We
show that it yields competitive performance compared to prior methods."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.08883248730964467,"2
Preliminary on Euclidean-space Diffusion Models"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.09137055837563451,"Notation
M ‚äÜRd denotes a convex constrained set. We preserve x ‚ààM and y ‚ààRd to better
distinguish variables on constrained sets and standard d-dimensional Euclidean space. I ‚ààRd√ód
denotes the identity matrix. xi denotes the i-th element of a vector x. Similarly, [A]ij denotes the the
element of a matrix A at i-th row and j-th column."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.09390862944162437,"Diffusion process
Given y0 ‚ààRd sampled from some Euclidean-space distribution, conventional
diffusion models deÔ¨Åne the diffusion process as a forward Markov chain with the joint distribution:"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.09644670050761421,"q(y1:T |y0) = T
Y"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.09898477157360407,"t=1
q(yt|yt‚àí1),
q(yt|yt‚àí1) := N(yt;
p"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.10152284263959391,"1 ‚àíŒ≤tyt‚àí1, Œ≤tI),
(1)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.10406091370558376,"which progressively injects Gaussian noise to y0 such that, for a sufÔ¨Åciently large T and a properly
chosen noise schedule Œ≤t ‚àà(0, 1), yT approaches an approximate Gaussian, i.e., q(yT ) ‚âàN(0, I)."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.1065989847715736,"Equation (1) has tractable marginal densities, and two of which that are of particular interest are:"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.10913705583756345,"q(yt|y0) = N(yt; ‚àö¬ØŒ±ty0, (1 ‚àíŒ±t)I),
q(yt‚àí1|yt, y0) = N(yt‚àí1; Àú¬µt(yt, y0), ÀúŒ≤tI),
(2)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.1116751269035533,"Àú¬µt(yt, y0) :=
‚àö¬ØŒ±t‚àí1Œ≤t"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.11421319796954314,"1 ‚àí¬ØŒ±t
y0 +
‚àö1 ‚àíŒ≤t(1 ‚àí¬ØŒ±t‚àí1)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.116751269035533,"1 ‚àí¬ØŒ±t
yt,
ÀúŒ≤t := 1 ‚àí¬ØŒ±t‚àí1"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.11928934010152284,"1 ‚àí¬ØŒ±t
Œ≤t,
¬ØŒ±t := tY"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.1218274111675127,"s=0
(1 ‚àíŒ≤s)."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.12436548223350254,"The tractable marginal q(yt|y0) enables direct sampling of yt without simulating the forward Markov
chain (1), i.e., yt|y0 can be sampled in a simulation-free manner. It also suggests a closed-form score
function ‚àálog q(yt|y0). The marginal q(yt‚àí1|yt, y0) hints the optimal reverse process given y0."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.12690355329949238,"Generative process
The generative process, which aims to reverse the forward diffusion process
(1), is deÔ¨Åned by a backward Markov chain with the joint distribution:"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.12944162436548223,"pŒ∏(y0:T ) = pT (yT ) T
Y"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.1319796954314721,"t=1
pŒ∏(yt‚àí1|yt),
pŒ∏(yt‚àí1|yt) := N(yt‚àí1; ¬µŒ∏(yt, t), ÀúŒ≤tI).
(3)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.13451776649746192,"Here, the mean ¬µŒ∏ is typically parameterized by some DNNs, e.g., U-net [40], with Œ∏. Equations (1)
and (3) imply an evidence lower bound (ELBO) for maximum likelihood training, indicating that the
optimal prediction of ¬µŒ∏(yt, t) should match Àú¬µt(yt, y0) in Equation (2)."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.13705583756345177,"Parameterization and training
There exists many different ways to parameterize ¬µŒ∏(yt, t), and
each of them leads to a distinct training objective. For instance, Ho et al. [2] found that"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.13959390862944163,"¬µŒ∏(yt, t) :=
1
‚àö1 ‚àíŒ≤t"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.14213197969543148,"
yt ‚àí
Œ≤t
‚àö1 ‚àí¬ØŒ±t
œµŒ∏(yt, t)

,
(4)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.1446700507614213,"achieves better empirical results compared to directly predicting y0 or Àú¬µt. This parameterization aims
to predict the ‚Äúnoise‚Äù injected to yt ‚àºq(yt|y0), as the ELBO reduces to a simple regression:
L(Œ∏) := Et,y0,œµ [Œª(t)‚à•œµ ‚àíœµŒ∏(yt, t)‚à•] .
(5)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.14720812182741116,"where yt = ‚àö¬ØŒ±ty0 + ‚àö1 ‚àíŒ±tœµ, œµ ‚àºN(0, I), and t sampled uniformly from {1, ¬∑ ¬∑ ¬∑ , T}. The
weighting Œª(t) is often set to 1. Once œµŒ∏ is properly trained, we can generate samples in Euclidean
space by simulating Equation (3) backward from yT ‚àºN(0, I)."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.14974619289340102,"3
Mirror Diffusion Models (MDM)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.15228426395939088,"In this section, we present a novel class of diffusion models, Mirror Diffusion Models, which are
designed to model probability distributions pdata(x) supported on a convex constrained set M ‚äÜRd
while retraining the computational advantages of Euclidean-space diffusion models."
DUAL-SPACE DIFFUSION,0.1548223350253807,"3.1
Dual-Space Diffusion"
DUAL-SPACE DIFFUSION,0.15736040609137056,"Mirror map
Following the terminology in Li et al. [34], let œÜ : M ‚ÜíR be a twice differentiable3
function that is strictly convex and satisfying that limx‚Üí‚àÇM ‚à•‚àáœÜ(x)‚à•‚Üí‚àûand ‚àáœÜ(M) = Rd. We
call its gradient map ‚àáœÜ : M ‚ÜíRd the mirror map and its image, ‚àáœÜ(M) = Rd, as its dual/mirror
space. Let œÜ‚àó: Rd ‚ÜíR be the dual function of œÜ deÔ¨Åned by
œÜ‚àó(y) = sup
x‚ààM
‚ü®x, y‚ü©‚àíœÜ(x)
(6)"
DUAL-SPACE DIFFUSION,0.1598984771573604,"A notable property of this dual function œÜ‚àó(y) is that its gradient map reverses the mirror map. In
other words, we have ‚àáœÜ‚àó= (‚àáœÜ)‚àí1, implying that"
DUAL-SPACE DIFFUSION,0.16243654822335024,"‚àáœÜ‚àó(‚àáœÜ(x)) = x for all x ‚ààM,
‚àáœÜ(‚àáœÜ‚àó(y)) = y for all y ‚àà‚àáœÜ(M) = Rd."
DUAL-SPACE DIFFUSION,0.1649746192893401,"The functions (‚àáœÜ, ‚àáœÜ‚àó) deÔ¨Åne a nonlinear, yet bijective, mapping between the convex constrained
set M and Euclidean space Rd. As a result, MDM does not require building diffusion models on M,
as done in previous approaches, but rather they learn a standard Euclidean-space diffusion model in
the corresponding dual space ‚àáœÜ(M) = Rd."
DUAL-SPACE DIFFUSION,0.16751269035532995,"3We note that standard mirror maps require twice differentiability to induce a (Riemannian) metric on
the mirror space for constructing MLD. For our MDM, however, twice differentiability is needed only for
Equation (8), and continuous differentiability, i.e., C1, would sufÔ¨Åce for training. 1
0
1 1 0 1 Ball 3
0
3 3 0 3"
DUAL-SPACE DIFFUSION,0.1700507614213198,"Ball 
(
) 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.8 0.6 0.4 0.2 0.0"
DUAL-SPACE DIFFUSION,0.17258883248730963,"0.0
0.2
0.4
0.6
0.8
1.0"
DUAL-SPACE DIFFUSION,0.1751269035532995,Simplex
DUAL-SPACE DIFFUSION,0.17766497461928935,"3
0
3
2 1 4 7"
DUAL-SPACE DIFFUSION,0.1802030456852792,"Simplex 
(
)"
DUAL-SPACE DIFFUSION,0.18274111675126903,"Figure 3: Examples of how mirror maps ‚àáœÜ pushforward constrained distributions to unconstrained
ones, for (left) an ‚Ñì2-ball where M := {x ‚ààR2 : ‚à•x‚à•2
2 < R} and (right) a simplex ‚àÜ3 where
M := {x ‚ààR2 : P2
i=1 xi ‚â§1, xi ‚â•0}. Note that for simplex, x3 = 1 ‚àíx1 ‚àíx2 is a redundant
coordinate; see Section 3.2 for more details."
DUAL-SPACE DIFFUSION,0.18527918781725888,"Essentially, the goal of MDM is to model the dual-space distribution:
Àúpdata(y) := ([‚àáœÜ‚ôØ]pdata)(x),
(7)
where ‚ôØis the push-forward operation and y = ‚àáœÜ(x) is the mirror of data x ‚ààM."
DUAL-SPACE DIFFUSION,0.18781725888324874,"Training and generation
MDM follows the same training procedure of Euclidean-space diffusion
models, except with an additional push-forward given by Equation (7). Similarly, its generation pro-
cedure includes an additional step that maps the dual-space samples y0, generated from Equation (3),
back to the constrained set via ‚àáœÜ‚àó(y0) ‚ààM."
DUAL-SPACE DIFFUSION,0.19035532994923857,"Tractable variational bound
Given a datapoint x ‚ààM, its mirror y0 = ‚àáœÜ(x) ‚ààRd, and the
dual-space Markov chain {y1, ¬∑ ¬∑ ¬∑ , yT }, the ELBO of MDM can be computed by"
DUAL-SPACE DIFFUSION,0.19289340101522842,"LELBO(x) := log | det ‚àá2œÜ‚àó(y0)| + ÀúLELBO(y0),
(8)
where we apply the change of variables theorem [41] w.r.t. the mapping x = ‚àáœÜ‚àó(y0). The
dual-space ELBO ÀúLELBO(y0) shares the same formula with the Euclidean-space ELBO [42, 43],"
DUAL-SPACE DIFFUSION,0.19543147208121828,ÀúLELBO(y0) := DKL(q(yT |y0) || p(yT )) +
DUAL-SPACE DIFFUSION,0.19796954314720813,"T ‚àí1
X"
DUAL-SPACE DIFFUSION,0.20050761421319796,"t=1
DKL(q(yt|yt+1, y0) || pŒ∏(yt|yt+1)) ‚àílog pŒ∏(y0|y1)."
DUAL-SPACE DIFFUSION,0.20304568527918782,"(9)
It should be noted that (8,9) provide a tractable variational bound for constrained generation. This
stands in contrast to concurrent methods relying on reÔ¨Çected diffusions, whose ELBO entails either
intractable [18] or approximate [25] scores that could be restricted when dealing with more complex
constrained sets. While, in practice, MDM is trained using the regression objective in Equation (5),
computing tractable ELBO might be valuable for independent purposes such as likelihood estimation."
DUAL-SPACE DIFFUSION,0.20558375634517767,"3.2
EfÔ¨Åcient Computation of Mirror Maps"
DUAL-SPACE DIFFUSION,0.20812182741116753,"What remains to be answered pertains to the construction of mirror maps for common constraint sets
such as ‚Ñì2-balls, simplices, polytopes, and their products. In particular, we seek efÔ¨Åcient, preferably
closed-form, computation for ‚àáœÜ, ‚àáœÜ‚àóand ‚àá2œÜ‚àó."
DUAL-SPACE DIFFUSION,0.21065989847715735,"‚Ñì2-Ball
Let M := {x ‚ààRd : ‚à•x‚à•2
2 < R} denote the ‚Ñì2-ball of radius R in Rd. We consider the
common log-barrier function:
œÜball(x) := ‚àíŒ≥ log(R ‚àí‚à•x‚à•2
2).
(10)"
DUAL-SPACE DIFFUSION,0.2131979695431472,"where Œ≥ ‚ààR+. The mirror map, its inverse, and the Hessian can be computed analytically by"
DUAL-SPACE DIFFUSION,0.21573604060913706,"‚àáœÜball(x) =
2Œ≥x
R ‚àí‚à•x‚à•2
2
,
‚àáœÜ‚àó
ball(y) =
Ry
p"
DUAL-SPACE DIFFUSION,0.2182741116751269,"R‚à•y‚à•2
2 + Œ≥2 + Œ≥
,"
DUAL-SPACE DIFFUSION,0.22081218274111675,"‚àá2œÜ‚àó
ball(y) =
R R
p"
DUAL-SPACE DIFFUSION,0.2233502538071066,"‚à•y‚à•2
2 + Œ≥2 + Œ≥ Ô£´"
DUAL-SPACE DIFFUSION,0.22588832487309646,"Ô£≠I ‚àí
R

R
p"
DUAL-SPACE DIFFUSION,0.22842639593908629,"‚à•y‚à•2
2 + Œ≥2 + Œ≥
 p"
DUAL-SPACE DIFFUSION,0.23096446700507614,"R‚à•y‚à•2
2 + Œ≥2 yy‚ä§ Ô£∂"
DUAL-SPACE DIFFUSION,0.233502538071066,"Ô£∏.
(11)"
DUAL-SPACE DIFFUSION,0.23604060913705585,"We refer to Appendix A for detailed derivation. Figure 3 visualizes such a mirror map for an ‚Ñì2-ball in
2D. It is worth noting that while the mirror map of log-barriers generally does not admit an analytical
inverse, this is not the case for the ‚Ñì2-ball constraints."
DUAL-SPACE DIFFUSION,0.23857868020304568,"Simplex
Given a (d+1)-dimensional simplex, ‚àÜd+1 := {x ‚ààRd+1 : Pd+1
i=1 xi = 1, xi ‚â•0}, we
follow standard practices [18, 25] by constructing the constrained set M := {x ‚ààRd : Pd
i=1 xi ‚â§
1, xi ‚â•0} and deÔ¨Åne xd+1 := 1 ‚àíPd
i=1 xi.4"
DUAL-SPACE DIFFUSION,0.24111675126903553,"While conventional log-barriers may remain a viable option, a preferred alternative that is widely
embraced in the MLD literature for enforcing simplices [28, 44] is the entropic function [45, 46]:"
DUAL-SPACE DIFFUSION,0.2436548223350254,"œÜsimplex(x) := d
X"
DUAL-SPACE DIFFUSION,0.24619289340101522,"i=1
xi log(xi) +  1 ‚àí d
X"
DUAL-SPACE DIFFUSION,0.24873096446700507,"i=1
xi ! log  1 ‚àí d
X"
DUAL-SPACE DIFFUSION,0.2512690355329949,"i=1
xi !"
DUAL-SPACE DIFFUSION,0.25380710659898476,".
(12)"
DUAL-SPACE DIFFUSION,0.2563451776649746,"The mirror map, its inverse, and the hessian of the dual function can be computed analytically by"
DUAL-SPACE DIFFUSION,0.25888324873096447,"[‚àáœÜsimplex(x)]i = log xi ‚àílog  1 ‚àí d
X"
DUAL-SPACE DIFFUSION,0.2614213197969543,"i=1
xi !"
DUAL-SPACE DIFFUSION,0.2639593908629442,",
[‚àáœÜ‚àó
simplex(y)]i =
eyi"
DUAL-SPACE DIFFUSION,0.26649746192893403,"1 + Pd
i=1 eyi ,"
DUAL-SPACE DIFFUSION,0.26903553299492383,"[‚àá2œÜ‚àó
simplex(y)]ij =
eyi"
DUAL-SPACE DIFFUSION,0.2715736040609137,"1 + Pd
i=1 eyi I(i = j) ‚àí
eyieyj

1 + Pd
i=1 eyi
2 ,
(13)"
DUAL-SPACE DIFFUSION,0.27411167512690354,"where I(¬∑) is an indicator function. Again, we leave the derivation of Equation (13) to Appendix A.
Figure 3 visualizes the entropic mirror map for a 3-dimensional simplex ‚àÜ3."
DUAL-SPACE DIFFUSION,0.2766497461928934,"Polytope
Let the constrained set be M := {x ‚ààRd : ci < a‚ä§
i x < bi, ‚àÄi ‚àà{1, ¬∑ ¬∑ ¬∑ , m}}, where
ci, bi ‚ààR and ai ‚ààRd are linearly independent to each other. We consider the standard log-barrier:"
DUAL-SPACE DIFFUSION,0.27918781725888325,"œÜpolytope(x) := ‚àí m
X"
DUAL-SPACE DIFFUSION,0.2817258883248731,"i=1
(log (‚ü®ai, x‚ü©‚àíci) + log (bi ‚àí‚ü®ai, x‚ü©)) + d
X j=m+1"
DUAL-SPACE DIFFUSION,0.28426395939086296,"1
2‚ü®aj, x‚ü©2,
(14)"
DUAL-SPACE DIFFUSION,0.2868020304568528,"‚àáœÜpolytope(x) = m
X"
DUAL-SPACE DIFFUSION,0.2893401015228426,"i=1
si(‚ü®ai, x‚ü©)ai + d
X"
DUAL-SPACE DIFFUSION,0.2918781725888325,"j=m+1
‚ü®aj, x‚ü©aj,
(15)"
DUAL-SPACE DIFFUSION,0.29441624365482233,"2
0
2
100 0 100"
DUAL-SPACE DIFFUSION,0.2969543147208122,"si( ai, x )"
DUAL-SPACE DIFFUSION,0.29949238578680204,"Log-barrier 2
0
2 2.5 0.0 2.5"
DUAL-SPACE DIFFUSION,0.3020304568527919,"Hyperbolic 
 Tangent in Eq.(18)"
DUAL-SPACE DIFFUSION,0.30456852791878175,"Figure 4: Comparison between si
induced by standard log-barriers vs.
hyperbolic tangents in Eq. (18)."
DUAL-SPACE DIFFUSION,0.30710659898477155,"where the monotonic function si : (ci, bi) ‚ÜíR is given by
si =
‚àí1
‚ü®ai,x‚ü©‚àíci +
‚àí1
‚ü®ai,x‚ü©‚àíbi , and {aj} do not impose any con-
straints and can be chosen arbitrarily as long as they span Rd
with {ai} (see below for more details); hence ‚àáœÜpolytope(M) =
Rd. While its inverse, ‚àáœÜ‚àó
polytope, does not admit closed-form
in general, it does when all d constraints are orthonormal:"
DUAL-SPACE DIFFUSION,0.3096446700507614,"‚àáœÜ‚àó
polytope(y) = m
X"
DUAL-SPACE DIFFUSION,0.31218274111675126,"i=1
s‚àí1
i (‚ü®ai, y‚ü©)ai + d
X"
DUAL-SPACE DIFFUSION,0.3147208121827411,"j=m+1
‚ü®aj, y‚ü©aj. (16)"
DUAL-SPACE DIFFUSION,0.31725888324873097,"Here, s‚àí1
i
: R ‚Üí(ci, bi) is the inverse of si. Essentially, (15,16) manipulate the coefÔ¨Åcients of the
bases from which the polytope set is constructed. These manipulations are nonlinear yet bijective,
deÔ¨Åned uniquely by si and s‚àí1
i , for each orthonormal basis ai or, equivalently, for each constraint."
DUAL-SPACE DIFFUSION,0.3197969543147208,"Naively implementing the mirror map via (15,16) can be problematic due to (i) numerical instability
of si at boundaries (see Figure 4), (ii) the lack of closed-form solution for its inverse s‚àí1
i , and (iii) an
one-time computation of orthonormal bases, which scales as O(d3) for orthonormalization methods
such as Gram-Schmidt, Householder or Givens [e.g., 47]. Below, we devise a modiÔ¨Åcation for
efÔ¨Åcient and numerically stable computation, which will be later adopted for watermarked generation."
DUAL-SPACE DIFFUSION,0.3223350253807107,"Improved computation for Polytope (15,16)
Given the fact that only the Ô¨Årst m orthonormal
bases {a1, ¬∑ ¬∑ ¬∑ , am} matter in the computation of (15,16), as the remaining (d ‚àím) orthonormal
bases merely involve orthonormal projections, we can simplify the computational (15,16) to"
DUAL-SPACE DIFFUSION,0.3248730964467005,"‚àáœÜpoly(x) = x + m
X"
DUAL-SPACE DIFFUSION,0.32741116751269034,"i=1
(si(‚ü®ai, x‚ü©) ‚àí‚ü®ai, x‚ü©) ai, ‚àáœÜ‚àó
poly(y) = y + m
X i=1"
DUAL-SPACE DIFFUSION,0.3299492385786802," 
s‚àí1
i (‚ü®ai, y‚ü©) ‚àí‚ü®ai, y‚ü©

ai, (17)"
DUAL-SPACE DIFFUSION,0.33248730964467005,4The transformation allows x to be bounded in a constrained set M ‚äÜRd rather than a hyperplane in Rd+1.
DUAL-SPACE DIFFUSION,0.3350253807106599,"which, intuitively, add and subtract the coefÔ¨Åcients of the Ô¨Årst m orthonormal bases while leaving
the rest intact. This reduces the complexity of computing orthonormal bases to O(md2) ‚âàO(d2)
and improves the numerical accuracy of inner-product multiplication, which can be essential for
high-dimensional applications when d is large."
DUAL-SPACE DIFFUSION,0.33756345177664976,"To address the instability of si and intractability of s‚àí1
i
for log-barriers, we re-interpret the mirror
maps (15,16) as the changes of coefÔ¨Åcient bases. As this implies that any nonlinear bijective mapping
with a tractable inverse would sufÔ¨Åce, we propose a rescaling of the hyperbolic tangent:"
DUAL-SPACE DIFFUSION,0.3401015228426396,"si(‚ü®ai, x‚ü©) :=
 
tanh‚àí1 ‚ó¶rescalei

(‚ü®ai, x‚ü©) ,
s‚àí1
i (‚ü®ai, y‚ü©) :=
 
rescale‚àí1
i
‚ó¶tanh

(‚ü®ai, y‚ü©) , (18)"
DUAL-SPACE DIFFUSION,0.3426395939086294,"Table 2: Complexity of ‚àáœÜ and
‚àáœÜ‚àófor each constrained set."
DUAL-SPACE DIFFUSION,0.34517766497461927,"‚Ñì2-Ball
Simplex
Polytope"
DUAL-SPACE DIFFUSION,0.3477157360406091,"O(d)
O(d)
O(md)"
DUAL-SPACE DIFFUSION,0.350253807106599,"where rescalei(z) := 2
z‚àíci
(bi‚àíci) ‚àí1 rescales the range from
(ci, bi) to (‚àí1, 1). It is clear from Figure 4 that, compared to
the si induced by the log-barrier, the mapping (18) is numerical
stable at the boundaries and admits tractable inverse."
DUAL-SPACE DIFFUSION,0.35279187817258884,"Complexity
Table 2 summarizes the complexity of the mirror
maps for each constrained set. We note that all computations
are parallelizable, inducing nearly no computational overhead.
Remark 1. Equations (17) and (18) can be generalized to non-orthonormal constraints by adopting
‚ü®Àúai, x‚ü©and ‚ü®Àúai, y‚ü©, where Àúai is the i-th row of the matrix Àú
A := (A‚ä§A)‚àí1A‚ä§and A := [a1, ¬∑ ¬∑ ¬∑ , am].
Indeed, when A is orthonormal, we recover Àúai = ai. We leave more discussions to Appendix B."
RELATED WORK,0.3553299492385787,"4
Related Work"
RELATED WORK,0.35786802030456855,"Constrained sampling
Sampling from a probability distribution on a constrained set has been a
long-standing problem not only due to its extensive applications in, e.g., topic modeling [48, 49] and
Bayesian inference [50, 51], but its unique challenges in non-asymptotic analysis and algorithmic
design [28, 32‚Äì34, 52, 53]. Mirror Langevin is a special instance of endowing Langevin dynamics
with a Riemannian metric [30, 54], and it chooses a speciÔ¨Åc reshaped geometry so that dynamics
have to travel inÔ¨Ånite distance in order to cross the boundary of the constrained set. It is a sampling
generalization of the celebrated Mirror Descent Algorithm [45] for convex constrained optimization,
and the convexity of constrained set leads to the existence of a mirror map. However, Mirror Langevin
is designed to draw samples from an unnormalized density supported on a constrained set. This
stands in contrast to our MDM, which instead tackles problems of constrained generation, where only
samples, not unnormalized density, are available, sharing much similarity to conventional generative
modeling [3, 55]. In addition, Mirror Langevin can be understood to go back and forth between
primal (constrained) and mirror (unconstrained) spaces, while MDM only does one such round trip."
RELATED WORK,0.3604060913705584,"Diffusion Models in non-Euclidean spaces
There has been growing interest in developing diffu-
sion generative models that can operate on domains that are not limited to standard Euclidean spaces,
e.g., discrete domains [56, 57] and equality constraints [58]. Seminar works by De Bortoli et al. [14]
and Huang et al. [15] have introduced diffusion models on Riemannian manifolds, which have shown
promising results in, e.g., biochemistry [59, 60] and rotational groups [61, 62]. Regarding diffusion
models on constrained manifolds, most concurrent works consider similar convex constrained sets
such as simplices and polytopes [18, 25, 63]. Our MDM works on the same classes of constraints and
includes additionally ‚Ñì2-ball, which is prevalent in social science domains such as opinion dynamics
[64, 65]. It is also worth mentioning that our MDM may be conceptually similar to Simplex Diffusion
(SD) [63] which reconstructs samples on simplices similar to the mirror mapping. However, SD is
designed speciÔ¨Åcally for simplices and Dirichlet distributions, while MDM is applicable to a broader
class of convex constrained sets. Additionally, SD adopts Cox-Ingersoll-Ross processes [66], yielding
a framework that, unlike MDM, is not simulation-free."
RELATED WORK,0.3629441624365482,"Latent Diffusion Models (LDMs)
A key component to our MDM is the construction of mirror
map, which allows us to deÔ¨Åne diffusion models in a different space than the original constrained set,
thereby alleviating many computational difÔ¨Åculties. This makes MDM a particular type of LDMs
[5, 67, 68], which generally consider structurally advantageous spaces for learning diffusion models.
While LDMs typically learn such latent spaces either using pretrained model or on the Ô¨Çy using, e.g.,
variational autoencoders, MDM instead derives analytically a latent (dual) space through a mirror
map tailored speciÔ¨Åcally to the constrained set. Similar to LDM [69], the diffusion processes of
MDM on the initial constrained set are implicitly deÔ¨Åned and can be highly nonlinear."
RELATED WORK,0.36548223350253806,"Table 3: Results of ‚Ñì2-ball constrained sets on Ô¨Åve synthetic datasets of various dimension d. We
report Sliced Wasserstein [70] w.r.t. 1000 samples, averaged over three trials, and include constraint
violations for unconstrained diffusion models. Note that ReÔ¨Çected Diffusion [18] and MDM satisfy
constraints by design. Our MDM clearly achieves similar or better performance to standard diffusion
models while fully respecting the constraints. Other metrics, e.g., W1, are reported in Appendix C. sc"
RELATED WORK,0.3680203045685279,"d = 2
d = 2
d = 6
d = 8
d=20"
RELATED WORK,0.37055837563451777,Sliced Wasserstein ‚Üì
RELATED WORK,0.3730964467005076,"DDPM [2]
0.0704 ¬± 0.0095
0.0236 ¬± 0.0048
0.0379 ¬± 0.0015
0.0231 ¬± 0.0020
0.0200 ¬± 0.0034
ReÔ¨Çected [18]
0.0642 ¬± 0.0181
0.0491 ¬± 0.0103
0.0609 ¬± 0.0055
0.0881 ¬± 0.0010
0.0574 ¬± 0.0065
MDM (ours)
0.0544 ¬± 0.0070
0.0214 ¬± 0.0025
0.0467 ¬± 0.0096
0.0292 ¬± 0.0017
0.0159 ¬± 0.0044"
RELATED WORK,0.3756345177664975,Constraint violation (%) ‚Üì
RELATED WORK,0.37817258883248733,"DDPM [2]
0.00 ¬± 0.00
0.00 ¬± 0.00
8.67 ¬± 0.87
13.60 ¬± 0.62
19.33 ¬± 1.29"
RELATED WORK,0.38071065989847713,Table 4: Results of simplices constrained sets on Ô¨Åve synthetic datasets of various dimension d.
RELATED WORK,0.383248730964467,"d = 3
d = 3
d = 7
d = 9
d=20"
RELATED WORK,0.38578680203045684,Sliced Wasserstein ‚Üì
RELATED WORK,0.3883248730964467,"DDPM [2]
0.0089 ¬± 0.0002
0.0110 ¬± 0.0032
0.0047 ¬± 0.0004
0.0053 ¬± 0.0003
0.0031 ¬± 0.0003
ReÔ¨Çected [18]
0.0233 ¬± 0.0019
0.0336 ¬± 0.0009
0.0411 ¬± 0.0039
0.0897 ¬± 0.0112
0.0231 ¬± 0.0011
MDM (ours)
0.0074 ¬± 0.0008
0.0169 ¬± 0.0016
0.0051 ¬± 0.0006
0.0040 ¬± 0.0003
0.0027 ¬± 0.0000"
RELATED WORK,0.39086294416243655,Constraint violation (%) ‚Üì
RELATED WORK,0.3934010152284264,"DDPM [2]
0.73 ¬± 0.12
14.40 ¬± 1.39
11.63 ¬± 0.90
27.53 ¬± 0.57
68.83 ¬± 1.66 x3 x4"
RELATED WORK,0.39593908629441626,Ground Truth x3 x4 MDM x3 x4
RELATED WORK,0.39847715736040606,Reflected[18] x5 x6
RELATED WORK,0.4010152284263959,Ground Truth x5 x6 MDM x5 x6
RELATED WORK,0.4035532994923858,Reflected[18]
RELATED WORK,0.40609137055837563,"Figure 5: Comparison between MDM and ReÔ¨Çected Diffusion
[18] in modeling a Dirichlet distribution on a 7-dimensional
simplex. We visualize the joint densities between x3:4 and x5:6."
RELATED WORK,0.4086294416243655,"Table 5: Runtime and memory
complexity w.r.t. DDPM [2]."
RELATED WORK,0.41116751269035534,Runtime Memory
RELATED WORK,0.4137055837563452,"MDM
108%
100%
ReÔ¨Çected
Diff. [18] >1200%
905%"
EXPERIMENT,0.41624365482233505,"5
Experiment"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.41878172588832485,"5.1
Constrained Generation on Balls and Simplices"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4213197969543147,"Setup
We evaluate the performance of Mirror Diffusion Model (MDM) on common constrained
generation problems, such as ‚Ñì2-ball and simplex constrained sets with dimensions d ranging from
2 to 20. Following Fishman et al. [18], we consider mixtures of Gaussian (Figure 1) and Spiral
(Figure 3) for ball constraints and Dirichlet distributions [48] with various concentrations for simplex
constraints. We compare MDM against standard unconstrained diffusion models, such as DDPM [2],
and their constrained counterparts, such as ReÔ¨Çected Diffusion [18], using the same time-embedded
fully-connected network and 1000 sampling time steps. Evaluation metrics include Sliced Wasserstein
distance [70] and constraint violation. Other details are left to Appendix C."
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.42385786802030456,"MDM surpasses ReÔ¨Çected Diffusion on all tasks.
Tables 3 and 4 summarize our quantitative
results. For all constrained generation tasks, our MDM surpasses ReÔ¨Çected Diffusion by a large mar-
gin, with larger performance gaps as the dimension d increases. Qualitatively, Figure 5 demonstrates
how MDM better captures the underlying distribution than ReÔ¨Çected Diffusion. Additionally, Table 5
reports the relative complexity of both methods compared to simulation-free diffusion models such
as DDPM. While ReÔ¨Çected Diffusion requires extensive computation due to the use of implicit score
matching [71] and assertion of boundary reÔ¨Çection at every propagation step, our MDM constructs
efÔ¨Åcient, parallelizable, mirror maps in unconstrained dual spaces (see Section 3), thereby enjoying
preferable simulation-free complexity and better numerical scalability."
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4263959390862944,"MDM matches DDPM without violating constraints.
It should be highlighted that, while DDPM
remains comparable to MDM in terms of distributional metric, the generated samples often violate
the designated constrained sets. As shown in Tables 3 and 4, these constraint violations worsen as the
dimension increases. In contrast, MDM is by design violation-free; hence marrying the best of both."
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4289340101522843,"Table 6: The 50k-FID for unconditional watermarked generation. Wa-
termark precision denotes the percentage of constraint-satisÔ¨Åed images
that are generated by MDMs, controlled by loosing/tightening the con-
strained sets. As MDM-dual is trained on constraint-projected images,
we also report its FIDs‚àów.r.t. the shifted distributions. The prior wa-
termarked diffusion model [39] reported 5.03 on FFHQ and 4.32 on
AFHQv2."
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.43147208121827413,"FFHQ 64√ó64
AFHQv2 64√ó64
Precision
59.3%
71.8%
93.3%
56.9%
75.0%
92.7%"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.434010152284264,"MDM-proj
2.54
2.59
3.08
2.10
2.12
2.30"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4365482233502538,"MDM-dual
2.96
4.57
15.74
2.23
2.86
6.79
2.65‚àó
2.93‚àó
4.40‚àó
2.21‚àó
2.32‚àó
3.05‚àó FFHQ"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.43908629441624364,"MDM-proj
MDM-dual"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4416243654822335,AFHQv2
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.44416243654822335,"Figure 6: Uncurated water-
marked samples by MDMs.
More samples can be found
in Appendix C."
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4467005076142132,"0.9
0.9"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.44923857868020306,Original
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4517766497461929,"0.9
0.9"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4543147208121827,MDM-proj
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.45685279187817257,"0.9
0.9"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4593908629441624,"MDM-dual
Histogram of ai, x  on constriant range [-0.9,0.9]"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4619289340101523,"Figure 7: Histogram of ‚ü®ai, x‚ü©for orig-
inal and MDM-watermarked images x.
Dashed lines indicate the constrained set."
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.46446700507614214,"101
102  2.1 2.2 2.3 FID"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.467005076142132,"101
102
0.50 0.75"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.46954314720812185,Precision
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4720812182741117,"0.5
1.0
1.5 2.1 2.2 2.3 FID"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4746192893401015,"0.5
1.0
1.5 0.7 1.0"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.47715736040609136,Precision
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4796954314720812,"Number of constraints, m, in log-scale
Polytope constraint range, bi =
ci
Figure 8: Ablation studies on how (top) number of con-
straints m and (bottom) their ranges, i.e., (ci, bi), affect
the FID and precision of MDM-proj on AFHQv2."
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.48223350253807107,"5.2
Watermarked Generation on High-Dimensional Image Datasets"
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.4847715736040609,"Setup
We present MDM as a new approach for watermarked generation, where the watermark
is attached to samples within an orthonormal polytope M := {x ‚ààRd : ci < a‚ä§
i x < bi, ‚àÄi}.
These parameters {ai, bi, ci}m
i=1 serve as the private tokens visible only to individual users (see
Figure 2). While our approach is application-agnostic, we demonstrate MDM mainly on image
datasets, including both unconditional and conditional generation. Given a designated watermark
precision, deÔ¨Åned as the percentage of constraint-satisÔ¨Åed images that are generated by MDMs,
we randomly construct tokens as m orthonormalized Gaussian random vectors and instantiate
MDMs by either projecting unwatermarked diffusion models onto M (MDM-proj), or learning the
corresponding dual-space diffusions (MDM-dual). More precisely, MDM-proj projects samples
generated by pretrained diffusion models to a constraint set whose parameters (i.e., tokens) are
visible only to the private user. In contrast, MDM-dual learns a dual-space diffusion model from the
constrained-projected samples; hence, like other MDMs in Section 5.1, it is constraint-dependent.
Other details are left to Appendix C."
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.4873096446700508,"Unconditional watermark generation on FFHQ and AFHQv2
We Ô¨Årst test out MMD on FFHQ
[72] and AFHQv2 [73] on unconditional 64√ó64 image generation. Following Zhao et al. [39], we
adopt EDM parameterization [38] and report in Table 6 the performance of MDM in generating
watermarked images, quantiÔ¨Åed by the Frechet Inception Distance (FID) [74]. Compared to prior
watermarked approaches, which require additional training of latent spaces [39], MDM stands as a
competitive approach for watermarked generation by directly constructing analytic dual spaces
from the tokens (i.e., constraints) themselves. Intriguingly, despite the fact that MDM-dual learns
a smoother dual-space distribution compared to the truncated one from MDM-proj (see Figure 7),
the latter consistently achieves lower (hence better) FIDs. Since samples generated by MDM-dual
still remain close to the watermarked distribution, as indicated in Table 6 by the low FIDs‚àów.r.t. the
shifted training statistics, we conjecture that the differences may be due to the signiÔ¨Åcant distributional
shift induced by naively projecting the training data onto the constraint set. This is validated, partly,
in the ablation study from Figure 8, where we observe that both the FID and watermark precision
improve as the number of constraints m decreases, and as the constraint ranges (ci, bi) loosen. We
highlight this fundamental trade off between watermark protection and generation quality. Examples
of watermarked images are shown in Figure 6 and Appendix C."
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.48984771573604063,"Conditional generation on ImageNet256
Finally, we consider conditional watermarked gen-
eration on ImageNet 256√ó256. SpeciÔ¨Åcally, we focus on image restoration tasks, where the goal"
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.49238578680203043,"Degraded (JPEG QF=5) Input y
Ground Truth Reference (x
)"
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.4949238578680203,"MDM-dual Watermarked Output (x
)
MDM-proj Watermarked Output (x
)"
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.49746192893401014,"Figure 9: Conditional watermarked generation on ImageNet 256√ó256. SpeciÔ¨Åcally, we consider the
JPEG restoration task, where, given degraded, low-quality inputs y (upper-left), we wish to generate
their corresponding clean images x (upper-right) by learning p(x|y). It is clear that both MDM-dual
and MDM-proj are capable of solving this conditional generation task, generating clean images that
additionally embed invisible watermarks, i.e., x ‚ààM."
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.5,"is to generate clean, watermarked, images conditioned on degraded‚Äîrestored with JPEG in this
case‚Äîinputs. Similar to unconditional generation, we consider a polytope constraint set whose
parameters are chosen such that the watermark yields high precision (> 95%) and low false positive
rate (< 0.001%). SpeciÔ¨Åcally, we set m = 100 and b = ‚àíc = 1.2. We initialize the networks with
pretrained checkpoints from Liu et al. [75]."
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.5025380710659898,"Figure 9 reports the qualitative results. It is clear that both MDM-dual and MDM-proj are capable of
solving this conditional generation task, generating clean images that additionally embed invisible
watermarks, i.e., x ‚ààM. Note that all non-MDM-generated images, despite being indistinguishable,
actually violate the polytope constraint, whereas MDM-generated images always satisfy the con-
straint. Overall, our results suggest that both MDM-dual and MDM-proj scale to high-dimensional
applications and are capable of embedding invisible watermarks in high-resolution images."
CONCLUSION AND LIMITATION,0.5050761421319797,"6
Conclusion and Limitation"
CONCLUSION AND LIMITATION,0.5076142131979695,"We developed Mirror Diffusion Models (MDMs), a dual-space diffusion model that enjoys simulation-
free training for constrained generation. We showed that MDM outperforms prior methods in standard
constrained generation, such as balls and simplices, and offers a competitive alternative in generating
watermarked contents. It should be noted that MDM concerns mainly convex constrained sets, which,
despite their extensive applications, limits the application of MDM to general constraints. It will be
interesting to combine MDM with other dynamic generative models, such as Ô¨Çow-based approaches."
CONCLUSION AND LIMITATION,0.5101522842639594,Broader Impact
CONCLUSION AND LIMITATION,0.5126903553299492,"Mirror Diffusion Models (MDMs) advance the recent development of diffusion models to complex
domains subjected to convex constrained sets. This opens up new possibilities for MDMs to serve as
preferred models for generating samples that live in e.g., simplices and balls. Additionally, MDMs
introduce an innovative application of constrained sets as a watermarking technique. This has the
potential to address concerns related to unethical usage and safeguard the copyright of generative
models. By incorporating constrained sets into the generating process, MDMs offer a means to
prevent unauthorized usage and ensure the integrity of generated content."
CONCLUSION AND LIMITATION,0.5152284263959391,Acknowledgements
CONCLUSION AND LIMITATION,0.5177664974619289,"The authors would like to thank Bogdon Vlahov for sharing the computational resources. GHL,
TC, ET are supported by ARO Award #W911NF2010151 and DoD Basic Research OfÔ¨Åce Award
HQ00342110002. MT is partially supported by NSF DMS-1847802, NSF ECCS-1942523, Cullen-
Peck Scholarship, and GT-Emory AI.Humanity Award."
REFERENCES,0.5203045685279187,References
REFERENCES,0.5228426395939086,"[1] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
vised learning using nonequilibrium thermodynamics. In International Conference on Machine
Learning (ICML), 2015."
REFERENCES,0.5253807106598984,"[2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In
Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.5279187817258884,"[3] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-based generative modeling through stochastic differential equations. In
International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.5304568527918782,"[4] Prafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. In
Advances in Neural Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.5329949238578681,"[5] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-
resolution image synthesis with latent diffusion models. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2022."
REFERENCES,0.5355329949238579,"[6] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. In International Conference on Learning Representations
(ICLR), 2021."
REFERENCES,0.5380710659898477,"[7] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: Generative adversarial networks
for efÔ¨Åcient and high Ô¨Ådelity speech synthesis. In Advances in Neural Information Processing
Systems (NeurIPS), 2020."
REFERENCES,0.5406091370558376,"[8] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3D using
2D diffusion. In International Conference on Learning Representations (ICLR), 2023."
REFERENCES,0.5431472081218274,"[9] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten
Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-3D
content creation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2023."
REFERENCES,0.5456852791878173,"[10] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their
Applications, 12(3):313‚Äì326, 1982."
REFERENCES,0.5482233502538071,"[11] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In
International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.550761421319797,"[12] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
In Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.5532994923857868,"[13] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David
Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH
2022 Conference Proceedings, pages 1‚Äì10, 2022."
REFERENCES,0.5558375634517766,"[14] Valentin De Bortoli, Emile Mathieu, Michael Hutchinson, James Thornton, Yee Whye Teh,
and Arnaud Doucet. Riemannian score-based generative modeling. In Advances in Neural
Information Processing Systems (NeurIPS), 2022."
REFERENCES,0.5583756345177665,"[15] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron C Courville.
Riemannian diffusion models. In Advances in Neural Information Processing Systems (NeurIPS),
2022."
REFERENCES,0.5609137055837563,"[16] Emile Mathieu and Maximilian Nickel. Riemannian continuous normalizing Ô¨Çows. In Advances
in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.5634517766497462,"[17] Ricky T. Q. Chen and Yaron Lipman. Riemannian Ô¨Çow matching on general geometries. arXiv
preprint arXiv:2302.03660, 2023."
REFERENCES,0.565989847715736,"[18] Nic Fishman, Leo Klarner, Valentin De Bortoli, Emile Mathieu, and Michael Hutchinson.
Diffusion models for constrained domains. Transactions on Machine Learning Research
(TMLR), 2023."
REFERENCES,0.5685279187817259,"[19] Ben J Morris. Improved bounds for sampling contingency tables. Random Structures &
Algorithms, 21(2):135‚Äì146, 2002."
REFERENCES,0.5710659898477157,"[20] Nathan E Lewis, Harish Nagarajan, and Bernhard O Palsson. Constraining the metabolic
genotype‚Äìphenotype relationship using a phylogeny of in silico methods. Nature Reviews
Microbiology, 10(4):291‚Äì305, 2012."
REFERENCES,0.5736040609137056,"[21] Li Han and Lee Rudolph. Inverse kinematics for a serial chain with joints under distance
constraints. In Robotics: Science and Systems (RSS), 2006."
REFERENCES,0.5761421319796954,"[22] Tsuneo Yoshikawa. Manipulability of robotic mechanisms. The international journal of
Robotics Research, 4(2):3‚Äì9, 1985."
REFERENCES,0.5786802030456852,"[23] Ruth J Williams. ReÔ¨Çected brownian motion with skew symmetric data in a polyhedral domain.
Probability Theory and Related Fields, 75(4):459‚Äì485, 1987."
REFERENCES,0.5812182741116751,"[24] Andrey Pilipenko. An introduction to stochastic differential equations with reÔ¨Çection, volume 1.
Universit√§tsverlag Potsdam, 2014."
REFERENCES,0.583756345177665,"[25] Aaron Lou and Stefano Ermon. ReÔ¨Çected diffusion models. In International Conference on
Machine Learning (ICML), 2023."
REFERENCES,0.5862944162436549,"[26] Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Finite-time analysis of projected langevin
monte carlo. In Advances in Neural Information Processing Systems (NeurIPS), 2015."
REFERENCES,0.5888324873096447,"[27] Kanji Sato, Akiko Takeda, Reiichiro Kawai, and Taiji Suzuki. Convergence error analysis of
reÔ¨Çected gradient langevin dynamics for globally optimizing non-convex constrained problems.
arXiv preprint arXiv:2203.10215, 2022."
REFERENCES,0.5913705583756346,"[28] Ya-Ping Hsieh, Ali Kavis, Paul Rolland, and Volkan Cevher. Mirrored langevin dynamics. In
Advances in Neural Information Processing Systems (NeurIPS), 2018."
REFERENCES,0.5939086294416244,"[29] Dominique Bakry, Ivan Gentil, and Michel Ledoux. Analysis and geometry of Markov diffusion
operators, volume 103. Springer, 2014."
REFERENCES,0.5964467005076142,"[30] Mark Girolami and Ben Calderhead. Riemann manifold langevin and hamiltonian monte carlo
methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):
123‚Äì214, 2011."
REFERENCES,0.5989847715736041,"[31] Kelvin Shuangjian Zhang, Gabriel Peyr√©, Jalal Fadili, and Marcelo Pereyra. Wasserstein control
of mirror langevin monte carlo. In Conference on Learning Theory (COLT), 2020."
REFERENCES,0.6015228426395939,"[32] Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, Philippe Rigollet, and Austin Stromme.
Exponential ergodicity of mirror-Langevin diffusions. In Advances in Neural Information
Processing Systems (NeurIPS), 2020."
REFERENCES,0.6040609137055838,"[33] Kwangjun Ahn and Sinho Chewi. EfÔ¨Åcient constrained sampling via the mirror-Langevin
algorithm. In Advances in Neural Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.6065989847715736,"[34] Ruilin Li, Molei Tao, Santosh S Vempala, and Andre Wibisono. The mirror langevin algorithm
converges with vanishing bias. In International Conference on Algorithmic Learning Theory
(ALT), 2022."
REFERENCES,0.6091370558375635,"[35] Michael Liebrenz, Roman Schleifer, Anna Buadze, Dinesh Bhugra, and Alexander Smith.
Generating scholarly content with chatgpt: ethical challenges for medical publishing. The
Lancet Digital Health, 5(3):e105‚Äìe106, 2023."
REFERENCES,0.6116751269035533,"[36] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein.
A watermark for large language models. In International Conference on Machine Learning
(ICML), 2023."
REFERENCES,0.6142131979695431,"[37] J√ºrgen Rudolph, Samson Tan, and Shannon Tan. Chatgpt: Bullshit spewer or the end of
traditional assessments in higher education? Journal of Applied Learning and Teaching, 6(1),
2023."
REFERENCES,0.616751269035533,"[38] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of
diffusion-based generative models. In Advances in Neural Information Processing Systems
(NeurIPS), 2022."
REFERENCES,0.6192893401015228,"[39] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, and Min Lin. A recipe
for watermarking diffusion models. arXiv preprint arXiv:2303.10137, 2023."
REFERENCES,0.6218274111675127,"[40] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for
biomedical image segmentation. In International Conference on Medical Image Computing
and Computer-assisted Intervention. Springer, 2015."
REFERENCES,0.6243654822335025,"[41] Peter D Lax. Change of variables in multiple integrals. The American mathematical monthly,
106(6):497‚Äì501, 1999."
REFERENCES,0.6269035532994924,"[42] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic
models. In International Conference on Machine Learning, pages 8162‚Äì8171. PMLR, 2021."
REFERENCES,0.6294416243654822,"[43] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of
score-based diffusion models. In Advances in Neural Information Processing Systems (NeurIPS),
2021."
REFERENCES,0.631979695431472,"[44] S√©bastien Bubeck and Ronen Eldan. The entropic barrier: a simple and optimal universal
self-concordant barrier. In Conference on Learning Theory (COLT), 2015."
REFERENCES,0.6345177664974619,"[45] Arkadij SemenoviÀác Nemirovskij and David Borisovich Yudin. Problem complexity and method
efÔ¨Åciency in optimization. Wiley-Interscience, 1983."
REFERENCES,0.6370558375634517,"[46] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167‚Äì175, 2003."
REFERENCES,0.6395939086294417,"[47] Yousef Saad. Iterative methods for sparse linear systems. SIAM, 2003."
REFERENCES,0.6421319796954315,"[48] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of
Machine Learning Research (JMLR), 2003."
REFERENCES,0.6446700507614214,"[49] Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. On smoothing and
inference for topic models. In Conference on Uncertainty in ArtiÔ¨Åcial Intelligence (UAI), 2009."
REFERENCES,0.6472081218274112,"[50] Lixin Lang, Wen-shiang Chen, Bhavik R Bakshi, Prem K Goel, and Sridhar Ungarala. Bayesian
estimation via sequential monte carlo sampling‚Äîconstrained dynamic systems. Automatica, 43
(9):1615‚Äì1622, 2007."
REFERENCES,0.649746192893401,"[51] Michael A Gelbart, Jasper Snoek, and Ryan P Adams. Bayesian optimization with unknown
constraints. In Conference on Uncertainty in ArtiÔ¨Åcial Intelligence (UAI), 2014."
REFERENCES,0.6522842639593909,"[52] Yann Brenier. D√©composition polaire et r√©arrangement monotone des champs de vecteurs. CR
Acad. Sci. Paris S√©r. I Math., 305:805‚Äì808, 1987."
REFERENCES,0.6548223350253807,"[53] Nicolas Brosse, Alain Durmus, √âric Moulines, and Marcelo Pereyra. Sampling from a log-
concave distribution with compact support with proximal Langevin Monte Carlo. In Conference
on Learning Theory (COLT), 2017."
REFERENCES,0.6573604060913706,"[54] Sam Patterson and Yee Whye Teh. Stochastic gradient Riemannian Langevin dynamics on the
probability simplex. In Advances in Neural Information Processing Systems (NeurIPS), 2013."
REFERENCES,0.6598984771573604,"[55] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow
matching for generative modeling. In International Conference on Learning Representations
(ICLR), 2023."
REFERENCES,0.6624365482233503,"[56] Ricky TQ Chen, Brandon Amos, and Maximilian Nickel. Semi-discrete normalizing Ô¨Çows
through differentiable tessellation. In Advances in Neural Information Processing Systems
(NeurIPS), 2022."
REFERENCES,0.6649746192893401,"[57] Xingchao Liu, Lemeng Wu, Mao Ye, et al. Learning diffusion bridges on constrained domains.
In International Conference on Learning Representations (ICLR), 2022."
REFERENCES,0.6675126903553299,"[58] Mao Ye, Lemeng Wu, and Qiang Liu. First hitting diffusion models for generating manifold,
graph and categorical data. In Advances in Neural Information Processing Systems (NeurIPS),
2022."
REFERENCES,0.6700507614213198,"[59] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional
diffusion for molecular conformer generation. In Advances in Neural Information Processing
Systems (NeurIPS), 2022."
REFERENCES,0.6725888324873096,"[60] Gabriele Corso, Hannes St√§rk, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Diffdock:
Diffusion steps, twists, and turns for molecular docking. In International Conference on
Learning Representations (ICLR), 2023."
REFERENCES,0.6751269035532995,"[61] Adam Leach, Sebastian M Schmon, Matteo T Degiacomi, and Chris G Willcocks. Denoising
diffusion probabilistic models on SO(3) for rotational alignment. In International Conference
on Learning Representations (ICLR), Workshop Track, 2022."
REFERENCES,0.6776649746192893,"[62] Julen Urain, Niklas Funk, Georgia Chalvatzaki, and Jan Peters. SE(3)-DiffusionFields: Learn-
ing cost functions for joint grasp and motion optimization through diffusion. In IEEE Interna-
tional Conference on Robotics and Automation (ICRA), Workshop Track, 2022."
REFERENCES,0.6802030456852792,"[63] Pierre H Richemond, Sander Dieleman, and Arnaud Doucet. Categorical SDEs with simplex
diffusion. arXiv preprint arXiv:2210.14784, 2022."
REFERENCES,0.682741116751269,"[64] Marco Caponigro, Anna Chiara Lai, and Benedetto Piccoli. A nonlinear model of opinion
formation on the sphere. Discrete & Continuous Dynamical Systems-A, 35(9):4241‚Äì4268, 2015."
REFERENCES,0.6852791878172588,"[65] Jason Gaitonde, Jon Kleinberg, and √âva Tardos. Polarization in geometric opinion dynamics.
In ACM Conference on Economics and Computation, pages 499‚Äì519, 2021."
REFERENCES,0.6878172588832487,"[66] John C Cox, Jonathan E Ingersoll Jr, and Stephen A Ross. A theory of the term structure of
interest rates. In Theory of valuation, pages 129‚Äì164. World ScientiÔ¨Åc, 2005."
REFERENCES,0.6903553299492385,"[67] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space.
In Advances in Neural Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.6928934010152284,"[68] Kushagra Pandey, Avideep Mukherjee, Piyush Rai, and Abhishek Kumar. Diffusevae: EfÔ¨Åcient,
controllable and high-Ô¨Ådelity generation from low-dimensional latents. Transactions on Machine
Learning Research (TMLR), 2022."
REFERENCES,0.6954314720812182,"[69] Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee, Wanmo Kang, and Il-Chul Moon.
Maximum likelihood training of implicit nonlinear diffusion models. In Advances in Neural
Information Processing Systems (NeurIPS), 2022."
REFERENCES,0.6979695431472082,"[70] Nicolas Bonneel, Julien Rabin, Gabriel Peyr√©, and Hanspeter PÔ¨Åster. Sliced and radon wasser-
stein barycenters of measures. Journal of Mathematical Imaging and Vision, 51:22‚Äì45, 2015."
REFERENCES,0.700507614213198,"[71] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable
approach to density and score estimation. In Conference on Uncertainty in ArtiÔ¨Åcial Intelligence
(UAI), 2020."
REFERENCES,0.7030456852791879,"[72] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2019."
REFERENCES,0.7055837563451777,"[73] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthe-
sis for multiple domains. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2020."
REFERENCES,0.7081218274111675,"[74] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems (NeurIPS), 2017."
REFERENCES,0.7106598984771574,"[75] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima
Anandkumar. I2SB: Image-to-Image Schr√∂dinger bridge. In International Conference on
Machine Learning (ICML), 2023."
REFERENCES,0.7131979695431472,"[76] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems 32,
pages 8024‚Äì8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf."
REFERENCES,0.7157360406091371,"[77] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.7182741116751269,"[78] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network
function approximation in reinforcement learning. Neural Networks, 107:3‚Äì11, 2018."
REFERENCES,0.7208121827411168,"[79] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference
on computer vision (ECCV), pages 3‚Äì19, 2018."
REFERENCES,0.7233502538071066,"A
Derivation of Mirror Mappings"
REFERENCES,0.7258883248730964,"Here, we provide addition derivation of ‚àáœÜ‚àó. Computation of ‚àáœÜ(x) and ‚àá2œÜ‚àó(y) follow straight-
forwardly by differentiating œÜ(x) and ‚àáœÜ‚àó(y) w.r.t. x and y, respectively."
REFERENCES,0.7284263959390863,"‚Ñì2-Ball
Since the gradient map also reverses the mirror map, we aim to rewrite y =
2Œ≥
R‚àí‚à•x‚à•2
2 x as
x = f(y) = ‚àáœÜ‚àó
ball(y). Solving the second-order polynomial,"
REFERENCES,0.7309644670050761,"‚à•y‚à•2
2 =

2Œ≥
R ‚àí‚à•x‚à•2
2"
REFERENCES,0.733502538071066,"2
‚à•x‚à•2
2,
(19)"
REFERENCES,0.7360406091370558,yields
REFERENCES,0.7385786802030457,"‚à•x‚à•2
2 = R + 2Œ≥"
REFERENCES,0.7411167512690355,"‚à•y‚à•2
2"
REFERENCES,0.7436548223350253,"
Œ≥ ‚àí
q"
REFERENCES,0.7461928934010152,"R‚à•y‚à•2
2 + Œ≥2

.
(20)"
REFERENCES,0.748730964467005,"With that, we can rewrite Equation (11) by"
REFERENCES,0.751269035532995,"x = R ‚àí‚à•x‚à•2
2
2Œ≥
y
(20)
= p"
REFERENCES,0.7538071065989848,"R‚à•y‚à•2
2 + Œ≥2 ‚àíŒ≥
‚à•y‚à•2
2
y =
R
p"
REFERENCES,0.7563451776649747,"R‚à•y‚à•2
2 + Œ≥2 + Œ≥
y."
REFERENCES,0.7588832487309645,"Simplex
Standard calculations in convex analysis [28] shows"
REFERENCES,0.7614213197969543,"œÜ‚àó
simplex(y) = log  1 + d
X"
REFERENCES,0.7639593908629442,"i
eyi
!"
REFERENCES,0.766497461928934,".
(21)"
REFERENCES,0.7690355329949239,"Differentiating Equation (21) w.r.t. y yields ‚àáœÜ‚àó
simplex in Equation (13)."
REFERENCES,0.7715736040609137,"Polytope
Since the gradient map also reverses the mirror map, we aim to inverse y = m
X"
REFERENCES,0.7741116751269036,"i=1
si(‚ü®ai, x‚ü©)ai + d
X"
REFERENCES,0.7766497461928934,"j=m+1
‚ü®aj, x‚ü©aj.
(22)"
REFERENCES,0.7791878172588832,"When all d constraints are orthonormal, taking inner product between y and each a yields"
REFERENCES,0.7817258883248731,"‚ü®ai, y‚ü©= si(‚ü®ai, x‚ü©),
‚ü®aj, y‚ü©= ‚ü®aj, x‚ü©.
(23)"
REFERENCES,0.7842639593908629,"Therefore, we can reconstruct x from y via x = m
X"
REFERENCES,0.7868020304568528,"i=1
‚ü®ai, x‚ü©ai + d
X"
REFERENCES,0.7893401015228426,"j=m+1
‚ü®aj, x‚ü©aj"
REFERENCES,0.7918781725888325,"(23)
= m
X"
REFERENCES,0.7944162436548223,"i=1
s‚àí1
i (‚ü®ai, y‚ü©)ai + d
X"
REFERENCES,0.7969543147208121,"j=m+1
‚ü®aj, y‚ü©aj,"
REFERENCES,0.799492385786802,"which deÔ¨Ånes x = ‚àáœÜ‚àó
polytope(y). For completeness, the Hessian can be presented compactly as"
REFERENCES,0.8020304568527918,"‚àá2œÜ‚àó
polytope(y) = I + AŒ£A‚ä§,
(24)"
REFERENCES,0.8045685279187818,"where I is the identity matrix, A := [a1, ¬∑ ¬∑ ¬∑ , am] is a d-by-m matrix whose column vector ai
corresponds to each constraint, and Œ£ ‚ààRm√óm is a diagonal matrix with leading entries"
REFERENCES,0.8071065989847716,"[Œ£]ii = ‚àÇs‚àí1
i (z)"
REFERENCES,0.8096446700507615,"‚àÇz
|z=‚ü®ai,y‚ü©‚àí1
(18)
= bi ‚àíci"
REFERENCES,0.8121827411167513,"2
 
1 ‚àítanh2(‚ü®ai, y‚ü©)

‚àí1."
REFERENCES,0.8147208121827412,"B
Additional Remarks on Polytope"
REFERENCES,0.817258883248731,"Derivation of Equation (17)
Since the subspaces spanned by {ai} and {aj} are orthogonal to each
other, we can rewrite (15) as"
REFERENCES,0.8197969543147208,"‚àáœÜpolytope(x) = m
X"
REFERENCES,0.8223350253807107,"i=1
si(‚ü®ai, x‚ü©)ai +  x ‚àí m
X"
REFERENCES,0.8248730964467005,"i=1
‚ü®ai, x‚ü©ai ! = x + m
X"
REFERENCES,0.8274111675126904,"i=1
(si(‚ü®ai, x‚ü©) ‚àí‚ü®ai, x‚ü©) ai."
REFERENCES,0.8299492385786802,"‚àáœÜ‚àó
polytope(y) follows similar derivation."
REFERENCES,0.8324873096446701,"Generalization to non-orthonormal constraints
The mirror maps of a polytope, as described in
Equations (15) to (17), can be seen as operations that manipulate the coefÔ¨Åcients associated with
the bases deÔ¨Åned by the constraints. This understanding allows us to extend the computation to
non-orthonormal constraints by identifying the corresponding ‚ÄúcoefÔ¨Åcients‚Äù through a change of
bases, utilizing the reproducing formula: x = d
X"
REFERENCES,0.8350253807106599,"i=1
‚ü®Àúai, x‚ü©ai, where Àúai is the i-th row of (A‚ä§A)‚àí1A‚ä§,"
REFERENCES,0.8375634517766497,"and A := [a1, ¬∑ ¬∑ ¬∑ , am]. Similarly, we have y = Pd
i=1‚ü®Àúai, y‚ü©ai. Applying similar derivation leads to"
REFERENCES,0.8401015228426396,"‚àáœÜpoly(x) = x + m
X"
REFERENCES,0.8426395939086294,"i=1
(si(‚ü®Àúai, x‚ü©) ‚àí‚ü®Àúai, x‚ü©) ai, ‚àáœÜ‚àó
poly(y) = y + m
X i=1"
REFERENCES,0.8451776649746193," 
s‚àí1
i (‚ü®Àúai, y‚ü©) ‚àí‚ü®Àúai, y‚ü©

ai."
REFERENCES,0.8477157360406091,"C
Experiment Details & Additional Results"
REFERENCES,0.850253807106599,Table 7: The concentration parameter Œ± of each Dirichlet distribution in simplices constrained sets.
REFERENCES,0.8527918781725888,"d
3
3
7
9
20"
REFERENCES,0.8553299492385786,"Œ±
[2, 4, 8]
[1, 0.1, 5]
[1, 2, 2, 4, 4, 8, 8]
[1, 0.5, 2, 0.3, 0.6, 4, 8, 8, 2]
[0.2, 0.4, ¬∑ ¬∑ ¬∑ , 4, 4.2]"
REFERENCES,0.8578680203045685,"Table 8: Hyperparameters of the polytope M := {x ‚ààRd : ci < a‚ä§
i x < bi} for
each dataset and watermark precision. Note that we Ô¨Åx b = bi = ‚àíci in practice."
REFERENCES,0.8604060913705583,"FFHQ 64√ó64 (uncon)
AFHQv2 64√ó64 (uncon)
Precision
59.3%
71.8%
93.3%
56.9%
75.0%
92.7%"
REFERENCES,0.8629441624365483,"Number of constraints m
7
20
100
4
20
100
Constraint range b
1.05
1.05
1.05
0.9
0.9
0.9"
REFERENCES,0.8654822335025381,Dataset & constrained sets
REFERENCES,0.868020304568528,"‚Ä¢ ‚Ñì2-balls constrained sets: For d = 2, we consider the Gaussian Mixture Model (with variance
0.05) and the Spiral shown respectively in Figures 1 and 3. For d = {6, 8, 20}, we place d
isotropic Gaussians, each with variance 0.05, at the corner of each dimension, and reject samples
outside the constrained sets."
REFERENCES,0.8705583756345178,"‚Ä¢ Simplices constrained sets: We consider Dirichlet distributions [48], Dir(Œ±), with various con-
centration parameters Œ± detailed in Table 7."
REFERENCES,0.8730964467005076,"‚Ä¢ Hypercube constrained sets: For all dimensions d = {2, 3, 6, 8, 20}, we place d isotropic
Gaussians, each with variance 0.2, at the corner of each dimension, and either reject (d =
{2, 3, 6, 8}) or reÔ¨Çect (d = 20) samples outside the constrained sets."
REFERENCES,0.8756345177664975,"‚Ä¢ Watermarked datasets and polytope constrained sets: We follow the same data preprocessing from
EDM5 [38] and rescale both FFHQ and AFHQv2 to 64√ó64 image resolution. For the polytope
constrained sets M := {x ‚ààRd : ci < a‚ä§
i x < bi, ‚àÄi}, we construct ai from orthonormalized
Gaussian random vectors and detail other hyperparameters in Table 8."
REFERENCES,0.8781725888324873,"5https://github.com/NVlabs/edm, released under Nvidia Source Code License."
REFERENCES,0.8807106598984772,"Implementation
All methods are implemented in PyTorch [76]. We adopt ADM6 and EDM5 [38]
respectively as the MDM‚Äôs diffusion backbones for constrained and watermarked generation. We
implemented ReÔ¨Çected Diffusion [18] by ourselves as their codes have not yet been made available by
the submission time (May 2023), and used the ofÔ¨Åcial implementation7 of ReÔ¨Çected Diffusion [25] in
Table 11. We also implemented Simplex Diffusion [63], but as observed in previous works [25], it
encountered computational instability especially when computing the modiÔ¨Åed Bessel functions."
REFERENCES,0.883248730964467,"Training
For constrained generation, all methods are trained with AdamW [77] and an exponential
moving average with the decay rate of 0.99. As standard practices, we decay the learning rate by the
decay rate 0.99 every 1000 steps. For watermarked generation, we follow the default hyperparameters
from EDM5 [38]. All experiments are conducted on two TITAN RTXs and one RTX 2080."
REFERENCES,0.8857868020304569,"Network
For constrained generation, all networks take (y, t) as inputs and follow"
REFERENCES,0.8883248730964467,"out = out_mod(norm(y_mod( y ) + t_mod(timestep_embedding( t )))),"
REFERENCES,0.8908629441624365,"where timestep_embedding(¬∑) is the standard sinusoidal embedding. t_mod and out_mod consist
of 2 fully-connected layers (Linear) activated by the Sigmoid Linear Unit (SiLU) [78]:"
REFERENCES,0.8934010152284264,t_mod = out_mod = Linear ‚ÜíSiLU ‚ÜíLinear
REFERENCES,0.8959390862944162,"and y_mod consists of 3 residual blocks, i.e., y_mod(y) = y + res_mod(norm(y)), where"
REFERENCES,0.8984771573604061,res_mod = Linear ‚ÜíSiLU ‚ÜíLinear ‚ÜíSiLU ‚ÜíLinear ‚ÜíSiLU ‚ÜíLinear
REFERENCES,0.9010152284263959,"All Linear‚Äôs have 128 hidden dimension. We use group normalization [79] for all norm. For
watermarked generation, we use EDM parameterization5 [38]."
REFERENCES,0.9035532994923858,"Evaluation
We compute the Wasserstein and Sliced Wasserstein distances using the geomloss8
and ot9 packages, respectively. The Maximum Mean Discrepancy (MMD) is based on the popular
package https://github.com/ZongxianLee/MMD_Loss.Pytorch, which is unlicensed. For
watermarked generation, we follow the same evaluation pipeline from EDM5 [38] by Ô¨Årst generating
50,000 watermarked samples and computing the FID w.r.t. the training statistics."
REFERENCES,0.9060913705583756,"False-positive rate
Similar to Kirchenbauer et al. [36], we reject the null hypothesis and detect the
watermark if the sample produces no violation of the polytope constraint, i.e., if x ‚ààM. Hence, the
false-positive samples are those that are actually true null hypothesis (i.e., not generated by MDM) yet
accidentally fall into the constraint set, hence being mistakenly detected as watermarked. SpeciÔ¨Åcally,
the false-positive rates of our MDMs are respectively 0.07% and 0.08% for FFHQ and AFHQv2.
Lastly, we note that the fact that both MDM-proj and MDM-dual generate samples that always satisfy
the constraint readily implies 100% recall and 0% Type II error."
REFERENCES,0.9086294416243654,"C.1
Additional Results"
REFERENCES,0.9111675126903553,"Groundtruth
MDM NLL"
REFERENCES,0.9137055837563451,"Figure 10: Tractable variational
bound by our MDM."
REFERENCES,0.916243654822335,"Tractable variational bound in Equation (8)
Figure 10
demonstrates how MDM faithfully captures the variational bound
to the negative log-likelihood (NLL) of 2-dimensional GMM."
REFERENCES,0.9187817258883249,"More constrained sets, distributional metrics, & baseline
Ta-
bles 9 and 10 expand the analysis in Tables 3 and 4 with additional
distributional metrics such as Wasserstein-1 (W1) and Maximum
Mean Discrepancy (MMD). Additionally, Table 11 reports the
results of hypercube [0, 1]d constrained set, a special instance
of polytopes, and includes additional baseline from Lou and Ermon [25], which approximate the"
REFERENCES,0.9213197969543148,"6https://github.com/openai/guided-diffusion, released under MIT License.
7https://github.com/louaaron/Reflected-Diffusion, latest commit (65d05c6) at submission, un-
licensed.
8https://github.com/jeanfeydy/geomloss, released under MIT License.
9https://pythonot.github.io/gen_modules/ot.sliced.html#ot.sliced.sliced_
wasserstein_distance, released under MIT License."
REFERENCES,0.9238578680203046,"intractable scores in ReÔ¨Çected Diffusion using eigenfunctions tailored speciÔ¨Åcally to hypercubes,
rather than implicit score matching as in Fishman et al. [18]. Consistently, our Ô¨Åndings conclude
that the MDM is the only constrained-based diffusion model that achieves comparable or better
performance to DDPM. These results afÔ¨Årm the effectiveness of MDM in generating high-quality
samples within constrained settings, making it a reliable choice for constrained generative modeling."
REFERENCES,0.9263959390862944,"More watermarked samples
Figures 11 and 12 provide additional qualitative results on the
watermarked samples generated by MDMs."
REFERENCES,0.9289340101522843,"Figure 11: FFHQ 64√ó64 unconditional watermarked samples generated by (left) MDM-proj and
(right) MDM-dual from the same set of random seeds. Despite the fact that some images, such
as the one in the Ô¨Årst row and sixth column, were altered possibly due to the change of dual-space
distribution (see Figure 7), they look realistic and remain close to the data distribution."
REFERENCES,0.9314720812182741,"Figure 12: AFHQv2 64√ó64 unconditional watermarked samples generated by (left) MDM-proj and
(right) MDM-dual from the same set of random seeds. Despite the fact that some images, such
as the one in the Ô¨Åfth row and Ô¨Årst column, were altered possibly due to the change of dual-space
distribution (see Figure 7), they all look realistic and remain close to the data distribution."
REFERENCES,0.934010152284264,"Table 9: Expanded results of ‚Ñì2-ball constrained sets, where we include additional distributional
metrics such as W1 and Maximum Mean Discrepancy (MMD), all computed with 1000 samples and
averaged over three trials. Consistently, our Ô¨Åndings conclude that the MDM is the only constrained-
based diffusion model that achieves comparable or better performance to DDPM."
REFERENCES,0.9365482233502538,"d = 2
d = 2
d = 6
d = 8
d=20"
REFERENCES,0.9390862944162437,W1 ‚Üì(unit: 10‚àí2)
REFERENCES,0.9416243654822335,"DDPM [2]
0.66 ¬± 0.15
0.14 ¬± 0.03
0.52 ¬± 0.09
0.58 ¬± 0.10
3.45 ¬± 0.50
ReÔ¨Çected [18]
0.55 ¬± 0.29
0.46 ¬± 0.17
3.11 ¬± 0.40
10.13 ¬± 0.21
19.42 ¬± 0.13
MDM (ours)
0.46 ¬± 0.07
0.12 ¬± 0.04
0.72 ¬± 0.39
1.05 ¬± 0.26
2.63 ¬± 0.31"
REFERENCES,0.9441624365482234,MMD ‚Üì(unit: 10‚àí2)
REFERENCES,0.9467005076142132,"DDPM [2]
0.67 ¬± 0.23
0.23 ¬± 0.07
0.37 ¬± 0.19
0.75 ¬± 0.24
0.98 ¬± 0.42
ReÔ¨Çected [18]
0.58 ¬± 0.46
5.03 ¬± 1.17
2.34 ¬± 0.14
28.82 ¬± 0.66
14.83 ¬± 0.62
MDM (ours)
0.52 ¬± 0.36
0.27 ¬± 0.19
0.54 ¬± 0.12
0.35 ¬± 0.23
0.50 ¬± 0.17"
REFERENCES,0.949238578680203,Constraint violation (%) ‚Üì
REFERENCES,0.9517766497461929,"DDPM [2]
0.00 ¬± 0.00
0.00 ¬± 0.00
8.67 ¬± 0.87
13.60 ¬± 0.62
19.33 ¬± 1.29"
REFERENCES,0.9543147208121827,Table 10: Expanded results of simplices constrained sets.
REFERENCES,0.9568527918781726,"d = 3
d = 3
d = 7
d = 9
d=20"
REFERENCES,0.9593908629441624,W1 ‚Üì(unit: 10‚àí2)
REFERENCES,0.9619289340101523,"DDPM [2]
0.01 ¬± 0.00
0.02 ¬± 0.01
0.03 ¬± 0.00
0.05 ¬± 0.00
0.11 ¬± 0.00
ReÔ¨Çected [18]
0.06 ¬± 0.01
0.12 ¬± 0.00
0.62 ¬± 0.08
3.57 ¬± 0.05
0.98 ¬± 0.02
MDM (ours)
0.01 ¬± 0.00
0.01 ¬± 0.01
0.03 ¬± 0.00
0.05 ¬± 0.00
0.13 ¬± 0.00"
REFERENCES,0.9644670050761421,MMD ‚Üì(unit: 10‚àí2)
REFERENCES,0.9670050761421319,"DDPM [2]
0.72 ¬± 0.07
0.72 ¬± 0.30
0.74 ¬± 0.10
0.97 ¬± 0.22
1.12 ¬± 0.07
ReÔ¨Çected [18]
3.91 ¬± 0.95
15.12 ¬± 1.36
16.48 ¬± 1.04
131.44 ¬± 2.65
57.90 ¬± 2.07
MDM (ours)
0.44 ¬± 0.16
0.50 ¬± 0.26
0.42 ¬± 0.08
0.55 ¬± 0.13
0.61 ¬± 0.03"
REFERENCES,0.9695431472081218,Constraint violation (%) ‚Üì
REFERENCES,0.9720812182741116,"DDPM [2]
0.73 ¬± 0.12
14.40 ¬± 1.39
11.63 ¬± 0.90
27.53 ¬± 0.57
68.83 ¬± 1.66"
REFERENCES,0.9746192893401016,"Table 11: Results of hypercube [0, 1]d constrained sets."
REFERENCES,0.9771573604060914,"d = 2
d = 3
d = 6
d = 8
d=20"
REFERENCES,0.9796954314720813,Sliced Wasserstein ‚Üì(unit: 10‚àí2)
REFERENCES,0.9822335025380711,"DDPM [2]
2.24 ¬± 1.22
2.17 ¬± 0.65
2.05 ¬± 0.41
2.01 ¬± 0.16
1.54 ¬± 0.01
ReÔ¨Çected [25]
3.75 ¬± 1.20
6.58 ¬± 1.18
2.77 ¬± 0.06
3.50 ¬± 0.69
3.37 ¬± 0.46
ReÔ¨Çected [18]
19.05 ¬± 1.51
17.16 ¬± 0.88
11.90 ¬± 0.43
7.49 ¬± 0.13
4.32 ¬± 0.23
MDM (ours)
3.00 ¬± 0.72
1.92 ¬± 0.81
1.75 ¬± 0.17
1.85 ¬± 0.34
3.35 ¬± 0.64"
REFERENCES,0.9847715736040609,W1 ‚Üì(unit: 10‚àí2)
REFERENCES,0.9873096446700508,"DDPM [2]
0.07 ¬± 0.05
0.22 ¬± 0.07
1.65 ¬± 0.14
3.30 ¬± 0.16
16.74 ¬± 0.12
ReÔ¨Çected [25]
0.20 ¬± 0.12
1.21 ¬± 0.39
2.53 ¬± 0.04
4.82 ¬± 0.42
25.47 ¬± 0.20
ReÔ¨Çected [18]
4.40 ¬± 0.57
6.01 ¬± 0.97
9.34 ¬± 0.56
9.84 ¬± 0.24
25.27 ¬± 0.36
MDM (ours)
0.08 ¬± 0.03
0.20 ¬± 0.07
1.57 ¬± 0.08
3.34 ¬± 0.23
20.59 ¬± 1.19"
REFERENCES,0.9898477157360406,MMD ‚Üì(unit: 10‚àí2)
REFERENCES,0.9923857868020305,"DDPM [2]
0.27 ¬± 0.26
0.32 ¬± 0.14
0.69 ¬± 0.21
0.81 ¬± 0.23
0.73 ¬± 0.07
ReÔ¨Çected [25]
0.92 ¬± 0.53
3.56 ¬± 1.31
1.16 ¬± 0.04
2.09 ¬± 0.70
2.83 ¬± 0.58
ReÔ¨Çected [18]
32.26 ¬± 3.19
26.64 ¬± 5.07
29.83 ¬± 1.42
15.84 ¬± 0.89
7.21 ¬± 0.68
MDM (ours)
0.27 ¬± 0.09
0.29 ¬± 0.17
0.39 ¬± 0.14
0.61 ¬± 0.23
0.62 ¬± 0.05"
REFERENCES,0.9949238578680203,Constraint violation (%) ‚Üì
REFERENCES,0.9974619289340102,"DDPM [2]
9.37 ¬± 0.12
17.57 ¬± 1.27
41.70 ¬± 1.30
59.30 ¬± 1.39
94.47 ¬± 0.64"
