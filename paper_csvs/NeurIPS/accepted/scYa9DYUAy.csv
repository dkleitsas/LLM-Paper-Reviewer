Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002564102564102564,"Vision and text have been fully explored in contemporary video-text foundational
models, while other modalities such as audio and subtitles in videos have not
received sufficient attention. In this paper, we resort to establish connections
between multi-modality video tracks, including Vision, Audio, and Subtitle, and
Text by exploring an automatically generated large-scale omni-modality video
caption dataset called VAST-27M. Specifically, we first collect 27 million open-
domain video clips and separately train a vision and an audio captioner to generate
vision and audio captions. Then, we employ an off-the-shelf Large Language Model
(LLM) to integrate the generated captions, together with subtitles and instructional
prompts into omni-modality captions. Based on the proposed VAST-27M dataset,
we train an omni-modality video-text foundational model named VAST, which
can perceive and process vision, audio, and subtitle modalities from video, and
better support various tasks including vision-text, audio-text, and multi-modal
video-text tasks (retrieval, captioning and QA). Extensive experiments have been
conducted to demonstrate the effectiveness of our proposed VAST-27M corpus
and VAST foundation model. VAST achieves 22 new state-of-the-art results on
various cross-modality benchmarks. Code, model and dataset will be released at
https://github.com/TXH-mercury/VAST."
INTRODUCTION,0.005128205128205128,"1
Introduction"
INTRODUCTION,0.007692307692307693,"A vast number of videos are recorded and uploaded to social media platforms every day, making
the understanding of video content become a critical area of research in artificial intelligence.
Simultaneously, the text serves as the most direct and efficient means of information propagation
in our daily lives. Consequently, video-text cross-modality learning is crucial for AI systems to
interact with humans, assisting them in understanding video content (video captioning), searching
for desired videos (text-to-video retrieval), or answering questions about videos (video QA). With
the advent of unsupervised pre-training techniques [1; 2] and large-scale video-text corpora [3; 4],
video-text pre-training models have made rapid advancements and achieved relatively impressive
performances on the aforementioned tasks. However, we contend that current pre-training models
are far from perfect, as most of them are restricted to establishing connections solely between
the text and visual content of videos, without incorporating other modality tracks such as audio
and subtitles. Specifically, environmental audio can provide additional contexts overlapping with
the visual information to reduce ambiguity and increase the prediction confidence of models, or"
INTRODUCTION,0.010256410256410256,"∗Equal Contribution.
†Corresponding author."
INTRODUCTION,0.01282051282051282,"V-T  Pretraining
Model: Frozen, GIT, CoCa,...
Corpus: WebVid10M,CC14M"
INTRODUCTION,0.015384615384615385,A-T Pretraining
INTRODUCTION,0.017948717948717947,"Model: WavCaps
Corpus: WavCaps
Vision
VC
Audio
AC"
INTRODUCTION,0.020512820512820513,"VS-T Pretraining
Model: UniVL,  CoMVT, ...
Corpus: HowTo100M, HD_VILA_100M..."
INTRODUCTION,0.023076923076923078,Vision
INTRODUCTION,0.02564102564102564,Subtitle
INTRODUCTION,0.028205128205128206,Subtitle
INTRODUCTION,0.03076923076923077,"VA-T Pretraining
Model: AVLNet, MERLOT-Reserve,VALOR..."
INTRODUCTION,0.03333333333333333,"Corpus: HowTo100M, VALOR-1M..."
INTRODUCTION,0.035897435897435895,Vision
INTRODUCTION,0.038461538461538464,Subtitle/AVC Audio
INTRODUCTION,0.041025641025641026,Omni-Modality VAS-T Pretraining
INTRODUCTION,0.04358974358974359,"Model: VAST (Ours)
Corpus: VAST-27M (Ours)"
INTRODUCTION,0.046153846153846156,Vision
INTRODUCTION,0.04871794871794872,Subtitle
INTRODUCTION,0.05128205128205128,"Audio
Omni-Modality Caption"
INTRODUCTION,0.05384615384615385,"(OMC)
Omni-Modality Video (OMV)"
INTRODUCTION,0.05641025641025641,"Figure 1: Illustration of the difference between conventional cross-modality pretraining and the
proposed omni-modality pretraining. Thanks to the proposed omni-modality video caption corpus
VAST-27M, the VAST foundation model can perceive videos from multiple information sources,
including vision, audio, and subtitles, and enhance the connections between omni-modalities videos
(OMV) and omni-modality captions (OMC) through large-scale pretraining. A, V, S, and T represent
audio, vision, subtitle, and text, respectively. AC, VC, and AVC are abbreviations for audio, vision,
and audiovisual captions."
INTRODUCTION,0.05897435897435897,"complement the visual information to supply more multi-modal cues. Human speech or transcriptions
(subtitles) also contain valuable information about conversations, news, or instructional procedures,
which can be jointly modeled to enhance the comprehensive understanding of videos. Given that
both audio and subtitles are informative for video comprehension, it is necessary to integrate them
into a more advanced omni-modality foundational model, as depicted in Figure 1."
INTRODUCTION,0.06153846153846154,"The primary challenge lies in the absence of an appropriate training corpus. To train such a foun-
dational model, we require an omni-modality video caption corpus in which captions exhibit cor-
respondence with vision, audio, and subtitles simultaneously. Unfortunately, as shown in Table 2,
current video-text corpora either utilize raw subtitles as captions [4; 5; 6], contain only vision captions
(alt-texts) [3], or possess audiovisual captions but limited in scale [7]. On the other hand, manual
annotation of an omni-modality corpus is not feasible due to the exorbitant cost associated with the
high demand for descriptions."
INTRODUCTION,0.0641025641025641,"To address this issue, we propose a two-stage automatic pipeline to generate omni-modality captions
for video clips from a large-scale open-domain video corpus (HD_VILA_100M [5]). Specifically,
as depicted in Figure 2, in the first stage, we train separate vision and audio captioners on publicly
available vision and audio caption corpora, which are then utilized to generate high-quality single-
modality objective descriptions. In the second stage, we employ Vicuna-13b [8], an off-the-shelf
Large Language Model (LLM), as a zero-shot omni-modality captioner. We feed the LLM with
the generated single-modality captions, subtitles, and instructional prompts to encourage LLM to
integrate different modality information and summarize them into a single long sentence, forming an
omni-modality caption. Through this process, we create VAST-27M, a dataset that consists of 27
million video clips, each paired with 11 captions (including 5 vision, 5 audio, and 1 omni-modality
caption). Given its comprehensive caption types, VAST-27M can contribute to various research
communities, including vision-text, audio-text, and omni-modality pretraining. Extensive experiments
have demonstrated that VAST-27M surpasses current video-text, audio-text, and audiovisual-text
corpora by a significant margin."
INTRODUCTION,0.06666666666666667,"Based on VAST-27M, we train a Vision-Audio-Subtitle-Text omni-modality foundational model
named VAST. As shown in Figure 2, VAST consists of three single modality encoders while text
encoder can fulfill cross-modality fusion through cross-attention layers. VAST is trained with three
objectives including OM-VCC, OM-VCM, and OM-VCG (detailed in section 4.2) to enhance omni-
modality understanding and generation capabilities. As shown in Table 1, compared to existing
cross-modality pre-training models, VAST is trained on omni-modality video caption corpus and is
capable of perceiving and processing all four modalities and supports a wide range of downstream
tasks, with various modalities (vision-text, audio-text, and multi-modal video-text), and multiple
types (retrieval, captioning, and QA)."
INTRODUCTION,0.06923076923076923,"Table 1: Comparisons between current cross-modality pretraining models and the proposed VAST.
VAST is the first foundation model capable of perceiving four modalities and generalizing to various
types of downstream tasks. V, A, S, and T represent vision, audio, subtitle, and text, respectively."
INTRODUCTION,0.07179487179487179,"Model
Task Type
Task Modality"
INTRODUCTION,0.07435897435897436,"Retrieval
Discriminative
Generative
V-T
A-T
VA-T
VS-T
VAS-T"
INTRODUCTION,0.07692307692307693,"GIT [19]
✓
✓
✓
VALUE [20]
✓
✓
✓
✓
✓
VALOR [7]
✓
✓
✓
✓
✓
✓
VAST
✓
✓
✓
✓
✓
✓
✓
✓"
INTRODUCTION,0.07948717948717948,"Our contributions are summarized as follows: (i) We introduce VAST-27M, the first large-scale
omni-modality video caption dataset automatically generated by trained single modality captioners
and Large Language Model; (ii) We train the first vision-audio-subtitle-text foundation model
VAST which fulfills omni-modality perception and understanding; (iii) Extensive experiments verify
the effectiveness of VAST and it outperforms state-of-the-art in a diverse range of cross-modality
benchmarks."
RELATED WORK,0.08205128205128205,"2
Related Work"
CROSS-MODALITY PRETRAINING CORPUS,0.08461538461538462,"2.1
Cross-Modality Pretraining Corpus"
CROSS-MODALITY PRETRAINING CORPUS,0.08717948717948718,"Video-Text Pretraining Corpus. In the early stages, most video-text models were trained on the
large-scale HowTo100M dataset [4]. This dataset consists of 136 million video clips from 1.22
million instructional YouTube videos, and employs automatically generated subtitles from ASR
transcription tools as corresponding clip captions. Later, following similar schemes for collecting
narrative videos-subtitles corpora, Zellers et al. proposed the YT-Temporal-180M corpus [6], which
contains 180 million clips from 6 million YouTube videos. To overcome the constraint of the
instructional domain, Xue et al. assembled the open-domain, high-resolution HD_VILA_100M
corpus [5], consisting of 100 million clips from 3.3 million YouTube videos. However, while labeling
subtitles is more cost-effective than annotating captions, subtitles often lack direct relevance to
visual content, resulting in weak video-text correlations in model learning. Inspired by the collection
schemes of the Conceptual Captions datasets (CC) [9; 10], Bain et al. compiled the WebVid2.5M
and WebVid10M datasets [3], utilizing alt-texts as video captions, surpassing the aforementioned
datasets that use subtitles as captions. However, alt-texts, although more related to video content
than subtitles, differ in style from standard captions and still contain noise. Additionally, alt-texts
typically describe visual content only, without incorporating other modalities. Taking a step further
beyond vision, Chen et al. created the VALOR-1M dataset [7], which contains 1 million video clips
from AudioSet [11] paired with annotated audiovisual captions. However, subtitle contents are not
reflected in these captions, and the dataset’s scale is limited and challenging to expand due to the
expensive cost of manual annotation. In comparison to the aforementioned training corpora, our
VAST-27M is the first high-quality, large-scale omni-modality video caption dataset whose captions
encompass all modalities of video including vision, audio, and subtitles. In addition, its expansion is
more cost-effective due to the automated generation pipeline."
CROSS-MODALITY PRETRAINING CORPUS,0.08974358974358974,"Audio-Text Pretraining Corpus. In contrast to vision-text pretraining, audio-text pretraining has
progressed relatively slowly due to limitations in training corpora, both in terms of quantity and scale.
Human-labeled datasets such as Clotho [12], AudioCaps [13], MCAS [14], and AudioCaption [15]
all contain fewer than 50,000 audio clips, which fall far short of the requirements for large-scale
pretraining. Wu et al. compiled LAION-Audio-630K [16] by crawling audios and corresponding
alt-texts from multiple sources, with a majority of audios sourced from Freesound [17]. Mei et al.
introduced WavCaps [18], which comprises 403,000 audios and descriptions collected from various
sources. They utilized ChatGPT to filter and rewrite the raw descriptions into caption-like sentences.
However, the quality of the captions is heavily influenced by the quality of the raw descriptions. In
contrast, we generates audio captions through a trained high-quality caption model, and VAST-27M
is nearly two orders of magnitude larger than LAION-Audio-630K or WavCaps."
CROSS-MODALITY PRETRAINING CORPUS,0.09230769230769231,"Table 2: Comparisons between current video-text and audio-text pretraining corpora and the proposed
VAST-27M. VAST-27M is the first large-scale omni-modality video-caption corpus containing various
types of captions generated automatically. S, VC, AC, AVC, and OMC stand for subtitle, vision,
audio, audiovisual, and omni-modality captions, respectively."
CROSS-MODALITY PRETRAINING CORPUS,0.09487179487179487,"Dataset
Domain
#Clips
#Captions
Text Source
S
VC
AC
AVC
OMC"
CROSS-MODALITY PRETRAINING CORPUS,0.09743589743589744,"HowTo100M [4]
Instructional
136M
136M
ASR transcription
✓
YT_Temporal_180M [6]
Instructional
180M
180M
ASR transcription
✓
HD_VILA_100M [5]
Open
103M
103M
ASR transcription
✓
WebVid-2.5M [3]
Open
2.5M
2.5M
Alt-texts
✓
WebVid-10M [3]
Open
10M
10M
Alt-texts
✓
VALOR-1M [7]
Open
1M
1M
Manual
✓
LAION-Audio-630K [16]
Open
634K
634K
Alt-texts
✓
WavCaps [18]
Open
403K
403K
Generated
✓
VAST-27M (Ours)
Open
27M
297M
Generated
✓
✓
✓
✓"
MULTI-MODALITY LEARNING,0.1,"2.2
Multi-Modality Learning"
MULTI-MODALITY LEARNING,0.10256410256410256,"There have been early methods exploring multi-modality tracks for video-text understanding. For
instance, MMT [21] leverages seven experts to enhance text-to-video retrieval, while SMPFF [22]
introduces audio for video captioning. In the context of video-text pretraining, there are works that
jointly model subtitles or audio together with vision and text. Notable examples of incorporating
subtitles include UniVL [23], CoMVT [24], and VALUE [20]. However, the subtitle-text correlations
established in these models are implicit and weak, achieved through masked prediction [23] or
next utterance prediction [24] that uses raw subtitles as prediction targets, rather than abstract text.
This approach introduces inconsistency between the pretraining and fine-tuning stages. In contrast,
VAST fully harnesses the generalization capabilities of Large Language Models to extract the most
crucial information from subtitles into language descriptions. On the other hand, notable examples
of incorporating audio include AVLNet [25], MERLOT Reserve [26], i-Code [27], and VALOR
[7]. However, AVLNet, MERLOT Reserve, and i-Code focus on learning audio-subtitle relations
rather than audio-text, limiting their generalization to tasks such as text-to-audio retrieval and audio
captioning. While VALOR jointly models audio, vision, and text, it primarily targets perceiving
environmental sounds and pays less attention to human speech. In comparison to the aforementioned
methods, VAST is the first omni-modality foundation model capable of perceiving four modalities,
connecting vision, audio, and subtitle to caption."
DATASET,0.10512820512820513,"3
Dataset"
DATASET,0.1076923076923077,"3.1
Data Collection of VAST-27M"
DATASET,0.11025641025641025,"Vision Captioner Training. To establish the general correspondence between objects and visual
concepts, we draw inspiration from BLIP [28] and commence by training a vision caption model
on large-scale image-text corpora, including CC4M, CC12M, and a randomly selected set of 100M
image-text pairs from LAION-400M [29]. Subsequently, we fine-tune the model on a combination of
manually labeled image and video caption datasets, such as MSCOCO [30], VATEX [31], MSRVTT
[32], and MSVD [33]. Through this process, we achieve a high-quality captioner capable of perceiving
both static objects and dynamic actions, thus generating smooth and accurate captions."
DATASET,0.11282051282051282,"Audio Captioner Training. For the audio captioning task, we train a dedicated audio caption model
using a combination of large-scale audio-text corpora, namely VALOR-1M and WavCaps datasets.
Unlike its vision counterpart, we do not employ a second-stage fine-tuning due to the limited scale
of downstream audio caption datasets, which could lead to overfitting to a narrow range of audio
concepts. Additionally, VALOR-1M is a human-annotated corpus that provides a certain level of
assurance regarding the accuracy of the generated captions."
DATASET,0.11538461538461539,"Clip Selection. Due to the consideration about time and money cost of downloading, storage and
computation, we select a 27M video clips from the HD_VILA_100M dataset instead of using all
of them. The selection rules are introduced as follows: 1) Clips whose length are shorter than 5
seconds or longer than 30 seconds are dropped. 2) Clips which have missing modalities of vision,
audio or subtitle are dropped. 3) Clips are evenly chosen from the original 3.3M long-form videos in
HD_VILA_100M."
DATASET,0.11794871794871795,Text encoder
DATASET,0.12051282051282051,OM-VCC
DATASET,0.12307692307692308,"Subtitle
Audio
OMC"
DATASET,0.12564102564102564,"Text encoder
Vision
Encoder"
DATASET,0.1282051282051282,"Audio
Encoder
Text encoder"
DATASET,0.13076923076923078,"Vision
OMC"
DATASET,0.13333333333333333,"OM-VCM 
OM-VCG"
DATASET,0.1358974358974359,"Omni-Modality
Captioner (LLM)"
DATASET,0.13846153846153847,VAST Training Framework
DATASET,0.14102564102564102,VAST-27M   Generation Pipeline
DATASET,0.14358974358974358,"Vision
Audio"
DATASET,0.14615384615384616,Text encoder
DATASET,0.14871794871794872,Subtitle
DATASET,0.15128205128205127,"Vision
Captioner"
DATASET,0.15384615384615385,"Audio
Captioner"
DATASET,0.1564102564102564,"""She's in second place on this left opens up with a little snap, gets back into the lip to finish it off, rides through the
explosive closeout and look into better."""
DATASET,0.15897435897435896,"1. A person surfing in the ocean in front of the waves.
2. A man is surfing in the middle of the ocean and it
starts to wave in the ocean。
3. A surfer on a surfboard rides in a wave, then jumps
up and starts surfing."
DATASET,0.16153846153846155,"1. The ocean waves and men speaking are heard.  
2. Someone is playing a surf - on.
3. Sea waves and people talking can be heard."
DATASET,0.1641025641025641,"""Human: About one video, here are 3 vision captions: {}, 3 audio captions: {}, and one speech subtitle: {}. You need to
understand and encode them into 1 sentence. Vision and audio captions maybe redundant, summarize them before encode
with speech subtitle. Do not simply concatenate them together. The weights of video/audio/speech are equaled.   
Assistant: The sentence is """
DATASET,0.16666666666666666,"A surfer rides a
wave, jumping up
and surfing in the
ocean with waves
and men speaking
in the background,
as they compete
for the top spot,
with a close-up of
the surfer finishing
the ride and
looking ahead."
DATASET,0.16923076923076924,"Vision
Encoder"
DATASET,0.1717948717948718,"Audio
Encoder"
DATASET,0.17435897435897435,"Global representation
Local representation
Concatenation"
DATASET,0.17692307692307693,"fv
fa
fs fomv"
DATASET,0.1794871794871795,"fvg
fag
fsg
fomcg fomvg"
DATASET,0.18205128205128204,"Figure 2: Illustration of the caption generation process of VAST-27M (top) and training framework
of VAST (bottom). The vision and audio captioners generate captions based on the input video clip,
and the Omni-Modality Captioner (Vicuna-13b) integrates them along with the raw subtitle and
instructional prompts, to generate the omni-modality caption. The VAST model consists of three
encoders and is trained under three objectives including OM-VCC, OM-VCM, and OM-VCG."
DATASET,0.18461538461538463,"Caption Generation. For each video clip of VAST-27M, we employ the trained vision and audio
captioners to generate 5 captions each, using a Top-K sampling approach with K=10. Subsequently,
we utilize the off-the-shelf Vicuna-13b [8] model as the omni-modality captioner. Vicuna-13b is an
open-source auto-regressive Large Language Model (LLM) based on the Transformer architecture,
trained through fine-tuning of LLaMA [34] on user-shared conversations collected from ShareGPT.
For each video clip, we randomly select 3 vision captions and 3 audio captions, and feed them, along
with the raw subtitle and designed instructional prompts, into the LLM. As depicted in Figure 2,
the LLM generates omni-modality captions that effectively describe the visual, audio, and subtitle
contents, while adhering to a natural human captioning style. More examples of VAST-27M dataset
can be found in Appendix."
DATASET,0.18717948717948718,"3.2
Statistics of VAST-27M"
DATASET,0.18974358974358974,"VAST-27M consists of a total of 27M video clips sampled from the large-scale HD_VILA_100M
corpus. The dataset covers 15+ categories, including music, gaming, education, entertainment,
animals, and more, as opposed to being limited to instructional domains like HowTo100M. Compared
to other commonly used cross-modality training corpora, as shown in Table 2, VAST-27M contains a
larger number of captions (in terms of both modalities and quantity). The average lengths of vision,
audio, and omni-modality captions in VAST-27M are 12.5, 7.2, and 32.4, respectively."
APPROACH,0.19230769230769232,"4
Approach"
BASIC FRAMEWORK,0.19487179487179487,"4.1
Basic Framework"
BASIC FRAMEWORK,0.19743589743589743,"As shown in Figure 2, VAST employs a fully end-to-end Transformer architecture, comprising a
vision encoder (ViT [35]), an audio encoder (BEATs [36]), and a text encoder (BERT [2]). This
framework can accommodate multi-modal inputs such as images, videos, audios, subtitles, and
captions. The text encoder is responsible for encoding single-modality captions or subtitles, as well
as performing multi-modal encoding/decoding through cross-attention layers. The omni-modality
captions and subtitles are tokenized using the WordPiece tokenizer [37] and then fed into the text
encoder to obtain the output features fomc and fs, respectively. The vision encoder takes raw images"
BASIC FRAMEWORK,0.2,"or sparsely sampled video frames as input and produces the output feature fv. For audio clips, they
are first divided into multiple 10-second-long clips, padded with zeros, converted to 64-dimensional
log Mel filterbank spectrograms using a 25ms Hamming window, and then fed into the audio encoder
to obtain the output feature fa. The global representations ( [CLS] token feature) of these features
are denoted as fomcg, fsg, fvg, and fag, respectively."
PRETRAINING OBJECTIVES,0.20256410256410257,"4.2
Pretraining Objectives"
PRETRAINING OBJECTIVES,0.20512820512820512,"Building upon conventional vision-text pretraining objectives, VAST employs the following three
pretraining objectives:"
PRETRAINING OBJECTIVES,0.2076923076923077,"Omni-Modality Video-Caption Contrastive Loss (OM-VCC). The global omni-modality representa-
tions of video clips, denoted as fomvg, are obtained by concatenating fsg, fvg, and fag. Subsequently,
fomvg and fomcg are projected into the same semantic space using two linear layers and are then nor-
malized. The contrastive loss is employed to regularize the feature distance between omni-modality
video (OMV) and caption (OMC). The contrastive loss is defined as follows, where sim(·) represents
the dot product of fomcg and fomvg, and B and τ denote the batch size and a learnable parameter,
respectively."
PRETRAINING OBJECTIVES,0.21025641025641026,"LOM−VCC = −1 2 B
X"
PRETRAINING OBJECTIVES,0.2128205128205128,"i=1
log
exp(τ · sim(vi, ci))
PB
j=1 exp(τ · sim(vi, cj)))
−1 2 B
X"
PRETRAINING OBJECTIVES,0.2153846153846154,"i=1
log
exp(τ · sim(vi, ci)))
PB
j=1 exp(τ · sim(vj, ci)))"
PRETRAINING OBJECTIVES,0.21794871794871795,"(1)
Omni-Modality Video-Caption Matching Loss (OM-VCM). This loss encourages the model to infer
whether a pair of OMV and OMC is matched or not. Specifically, caption tokens are fed into the
text encoder again, this time with cross-attention layers activated to attend to the condition features
fomv which is obtained by concatenating those unpooled features fs, fv, and fa along the sequential
dimension. Before concatenation, three independent linear layers are applied to adjust their hidden
size to the same value. The output feature of the text encoder is then fed into a two-layer MLP to
make binary predictions. To create informative negative pairs, we adopt a hard negative mining
strategy following [38]. The loss function is formulated as follows, where y = 1 if OMV and OMC
are matched, and 0 otherwise."
PRETRAINING OBJECTIVES,0.2205128205128205,"LOM−VCM = E(vi,ci)∼(V,C) [y log pvcm + (1 −y) log (1 −pvcm)]
(2)"
PRETRAINING OBJECTIVES,0.2230769230769231,"Omni-Modality Video Caption Generation Loss (OM-VCG). This loss employs conditional causal
masked language modeling to enhance the model’s ability to generate omni-modality captions.
Specifically, 60% of the tokens in OMC are masked at the input of the text encoder. Cross-attention
layers are activated and fomv is used as condition features as in OM-VCM. The self-attention layers
in the text encoder utilize a single-directional causal attention mask to prevent information leakage,
and the masked tokens are reconstructed at the output of the text encoder using BERT’s vanilla
prediction layer. The loss is defined as follows, where cm and c<m denote the masked tokens and the
tokens before them, respectively."
PRETRAINING OBJECTIVES,0.22564102564102564,"LOM−VCG = −E(vi,ci)∼(V,C) log P (cm | c<m, v)
(3)"
PRETRAINING OBJECTIVES,0.2282051282051282,"The overall loss LOM is the sum of the three losses, with equal weights assigned for simplicity."
PRETRAINING OBJECTIVES,0.23076923076923078,"LOM = LOM−VCC + LOM−VCM + LOM−VCG
(4)"
MODALITY GROUPING,0.23333333333333334,"4.3
Modality Grouping"
MODALITY GROUPING,0.2358974358974359,"While VAST establishes omni-modality video-caption correspondence during pretraining, it is im-
portant to address the potential absence of modalities in downstream benchmarks and practical
applications, due to that inconsistencies between the modalities used in pretraining and adaptation
can have negative effects. Inspired by the modality grouping strategy proposed by VALOR [7],
we uniformly model the relations of V-T, A-T, VA-T, VS-T, and VAS-T (previously introduced as
LOM). Specifically, vision and audio captions are used in V-T and A-T modeling, respectively,
while omni-modality captions are employed in VA-T, VS-T, and VAS-T modeling. The final loss is
formulated as follows:"
MODALITY GROUPING,0.23846153846153847,"L = LOM + LV−T + LA−T + LVA−T + LVS−T
(5)"
MODALITY GROUPING,0.24102564102564103,"Table 3: Performance comparison between VAST and state-of-the-art methods. VAST has achieved
22 new SOTA results. Recall@1, CIDEr, and Acc are used as evaluation metrics for retrieval,
captioning, and QA tasks, respectively. For captioning tasks, results marked with ‘*’ means that
SCST finetuning [41] is employed. ‘MM benchmark’ means that either audio or subtitle tracks are
available in those benchmarks, and SOTA_Vis and SOTA_MM are the best methods with vision
track used only or employing multi-modal tracks. More detailed benchmark introductions, model
references and comparison results can be found in Appendix."
MODALITY GROUPING,0.24358974358974358,"Vision-Text
benchmark
T-to-V Retrieval
Captioning
QA"
MODALITY GROUPING,0.24615384615384617,"COCO[30]
Flickr[42]
Flickr(ZS)[42]
COCO[30]
TGIF[43]
MSVD[44]
VQAv2[45]"
MODALITY GROUPING,0.24871794871794872,"SOTA
68.3[46]
90.3[47]
89.7[46]
154.9*[48]
78.7[7]
60.2[49]
84.3[50]
VAST
68.0
91.0(+0.7)
90.4(+0.7)
149.0*
79.1(+0.4)
60.2
80.2"
MODALITY GROUPING,0.2512820512820513,"Audio-Text
benchmark
T-to-A Retrieval
Captioning"
MODALITY GROUPING,0.25384615384615383,"Clothov1[12]
Clothov2[12]
AudioCaps[13]
Clothov1[12]
Clothov2[12]
AudioCaps[13]"
MODALITY GROUPING,0.2564102564102564,"SOTA
17.5[7]
21.5[18]
42.2[18]
42.3[7]
48.8[18]
78.7[18]
VAST
25.1(+7.6)
26.9(+5.4)
52.0(+9.8)
50.7(+8.4)
51.9(+3.1)
78.2"
MODALITY GROUPING,0.258974358974359,"MM Video-Text
benchmark
T-to-V Retrieval"
MODALITY GROUPING,0.26153846153846155,"MSRVTT[32]
YouCook2[51]
VALOR-32K[7]
VATEX[31]
DiDeMo[52]
ANET[53]"
MODALITY GROUPING,0.2641025641025641,"SOTA_Vis
58.8[54]
33.7[55]
43.4[56]
71.1[57]
70.4[54]
66.8[54]
SOTA_MM
54.4[7]
31.3[20]
73.2[7]
76.9[7]
57.6[7]
63.4[7]
VAST
63.9(+5.1)
50.4(+16.7)
80.0(+6.8)
83.0(+6.1)
72.0(+1.6)
70.5(+3.7)"
MODALITY GROUPING,0.26666666666666666,"MM Video-Text
benchmark
Captioning
QA"
MODALITY GROUPING,0.2692307692307692,"MSRVTT[32]
YouCook2[51]
VALOR-32K[7]
VATEX[31]
TVC[58]
MSRVTT[44]
MUSIC[59]
ANET[60]"
MODALITY GROUPING,0.2717948717948718,"SOTA_Vis
75.9[19]
131.2[19]
27.3[61]
94.5*[19]
66.1[19]
49.5[49]
-
47.9[54]
SOTA_MM
74.0[7]
190.0[55]
61.5[7]
95.8*[7]
66.0[62]
49.2[7]
78.9[7]
48.6[7]
VAST
78.0(+2.1)
198.8(+8.8)
62.0(+0.5)
99.5*(+3.7)
74.1(+8.0)
50.1(+0.6)
80.7(+1.8)
50.4(+1.8)"
EXPERIMENTS,0.2743589743589744,"5
Experiments"
IMPLEMENTATION DETAILS,0.27692307692307694,"5.1
Implementation Details"
IMPLEMENTATION DETAILS,0.2794871794871795,"VAST is trained using the PyTorch framework on 64 Tesla V100 cards. The vision, audio, and text
encoders are initialized from EVAClip-ViT-G [39], BEATs, and BERT-B, respectively, resulting in a
total parameter size of 1.3B. The training is conducted on a combination corpus consisting of VAST-
27M, VALOR-1M, WavCaps, CC14M, and 110M randomly sampled pairs from LAION-400M, for a
total of 200K training steps. At each training step, one corpus is sampled for training. In addition,
the raw captions of CC14M and LAION are replaced with captions generated by the trained vision
captioner. The initial learning rate is set to 1e-4, and a linear decay schedule is used. The batch
size is set to 1024. During the pertaining stage, one video frame and two 10s long audio clips are
randomly sampled for each video clip. For ablation studies, CLIP-ViT-B [40] is used as the vision
encoder, with its parameters frozen to increase efficiency. Further details such as the mixture ratio of
the pretraining dataset and the downstream finetuning configurations can be found in the Appendix."
IMPLEMENTATION DETAILS,0.28205128205128205,"Regarding the adaptation to downstream tasks, for retrieval tasks, all candidates are ranked using
VCC, and then the Top-50 candidates are reranked using VCM. For captioning tasks, beam search
with a beam size of 3 is employed. For QA tasks, they are formulated as open-ended generative
problems, where questions serve as prefixes, and answers are predicted without any constraints. For
SOTA comparison and ablation studies in the main paper, Recall@1, CIDEr, and Acc are used as
evaluation metrics for retrieval, captioning, and QA tasks, respectively."
COMPARISON TO STATE-OF-THE-ART MODELS,0.2846153846153846,"5.2
Comparison to State-of-the-Art Models"
COMPARISON TO STATE-OF-THE-ART MODELS,0.28717948717948716,"We present a comparison of the VAST foundation model with state-of-the-art models across var-
ious vision-text, audio-text, and multi-modal video-text benchmarks. The corresponding results
are summarized in Table 3. Although the primary focus of VAST lies in enhancing omni-modality
understanding and generation capabilities, it also demonstrates remarkable performance in image-text,"
COMPARISON TO STATE-OF-THE-ART MODELS,0.28974358974358977,Table 4: Comparisons of V-T quality in VAST-27M and open-sourced vision-text corpora.
COMPARISON TO STATE-OF-THE-ART MODELS,0.2923076923076923,"Model
PT Data
MSVD
MSRVTT"
COMPARISON TO STATE-OF-THE-ART MODELS,0.2948717948717949,"RET
CAP
QA
RET
CAP
QA"
COMPARISON TO STATE-OF-THE-ART MODELS,0.29743589743589743,"(a)
-
23.7
101.2
46.8
35.4
58.9
43.6
(b)
WebVid2.5M
43.1
139.2
53.0
42.4
63.7
45.5
(c)
CC14M
46.1
139.2
53.9
45.2
66.4
46.3
(d)
LAION-150M
42.4
139.3
53.2
44.9
66.9
45.4
(e)
VALOR-1M
42.3
136.7
52.6
42.8
65.3
45.5
(f)
VAST-27M (subtitle)
37.3
124.5
40.3
40.3
61.9
45.1
(g)
VAST-27M (omc)
46.8
143.2
54.3
47.5
67.4
46.7
(h)
VAST-27M (vision caption)
47.7
149.6
55.3
49.1
68.9
46.8"
COMPARISON TO STATE-OF-THE-ART MODELS,0.3,Table 5: Comparisons of A-T quality between VAST-27M and open-sourced audio-text corpora.
COMPARISON TO STATE-OF-THE-ART MODELS,0.30256410256410254,"Model
PT Data
Clothov2
AudioCaps"
COMPARISON TO STATE-OF-THE-ART MODELS,0.30512820512820515,"RET
CAP
RET
CAP"
COMPARISON TO STATE-OF-THE-ART MODELS,0.3076923076923077,"(a)
-
17.2
42.1
41.1
70.4
(b)
VALOR-1M
21.0
47.3
47.4
74.8
(c)
WavCaps
22.0
47.2
43.2
73.0
(d)
VAST-27M (audio caption)
23.1
48.9
47.4
76.9"
COMPARISON TO STATE-OF-THE-ART MODELS,0.31025641025641026,"vision-only video-text, and audio-text benchmarks. Specifically, VAST surpasses BEiT-3 [47] and
BLIP-2 [46] on the text-to-image retrieval benchmark of Flickr30K under finetuning and zero-shot
settings, respectively, establishing new state-of-the-art results in TGIF-QA and MSVD-QA bench-
marks. Furthermore, VAST exhibits exceptional performance in audio-text benchmarks, achieving
five new state-of-the-art results with significant improvements, due to the abundant generated captions
in the VAST-27M corpus."
COMPARISON TO STATE-OF-THE-ART MODELS,0.3128205128205128,"In the context of multi-modal video-text benchmarks, they can be classified into three categories:
subtitle-oriented (YouCook2, TVC) benchmarks that require a comprehensive understanding of subti-
tles for reasoning, audio-oriented (VALOR-32K) benchmarks that necessitates careful attention to
audio cues, and even-oriented benchmarks (others) where both audio and subtitles provide supportive
roles to vision. Our results demonstrate that VAST excels across all types of benchmarks, outper-
forming previous both vision-only and multi-modal state-of-the-art models. Notably, we surpass the
GIT2 [19] foundation model, specialized in captioning tasks, by 2.1, 67.6, 5.0, and 8.0 CIDEr points
on MSRVTT, YouCook2, VATEX, and TVC captioning benchmarks, respectively, while utilizing
only 22.5% of its parameters and 3.4% of its training data size. Furthermore, we achieve significant
margins over the vision-audio-text foundation model VALOR [7] on 11 benchmarks. More detailed
comparison results can be found in Appendix."
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.3153846153846154,"5.3
Comparison to Open-Source Cross-Modality Training Corpus"
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.31794871794871793,"In this subsection, we quantitatively compare the quality of the proposed VAST-27M to current
open-source video, audio, and audiovisual pretraining corpora. We achieve this by training models on
these corpora and fine-tuning them on different types of downstream tasks, including video retrieval
(RET), captioning (CAP), and question answering (QA)."
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.32051282051282054,"V-T Quality. We train multiple V-T models on different corpora and fine-tune them on the MSVD
and MSRVTT datasets. It is important to note that only the visual contents in videos are utilized for
both pretraining and fine-tuning, and all models are trained for 50K steps. As shown in Table 4, the
model trained with vision captions in VAST-27M (h) achieves the best results on all six benchmarks,
outperforming the baselines and models trained with other corpora by a significant margin. In
contrast, model (f), which treats subtitles as captions and can be viewed as raw HD_VILA_100M
dataset, performs the worst due to the weak vision-text relations. Model trained on VAST-27M (g)
outperforms other corpora but is still inferior to model (h), as the multi-modal captions introduce
noise when only vision contents are fed into the model."
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.3230769230769231,Table 6: Comparisons of OMV-OMC quality between VAST-27M and VALOR-1M.
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.32564102564102565,"Model
PT Data
MSRVTT
YouCook2
VALOR-32K"
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.3282051282051282,"RET
CAP
QA
RET
CAP
RET
CAP"
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.33076923076923076,"(a)
-
37.6
60.3
44.6
32.6
120.9
37.8
40.8
(b)
VALOR-1M
45.4
67.8
46.4
29.3
120.1
70.8
53.9
(c)
VAST-27M (omc w/o LLM)
44.4
71.4
47.7
36.9
159.0
49.9
48.1
(d)
VAST-27M (omc)
53.0
71.7
47.9
44.0
187.5
58.8
48.5"
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.3333333333333333,"Table 7: Downstream performances of models trained on VAST-27M with different modalities used
during training and finetuning."
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.33589743589743587,"Model
PT
FT
MSRVTT
YouCook2
VALOR-32K"
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.3384615384615385,"RET
CAP
QA
RET
CAP
RET
CAP"
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.34102564102564104,"(a)
-
V
35.4
58.7
43.7
6.2
77.0
29.1
36.6
(b)
-
V+A
37.3(+1.9)
61.1(+2.4)
44.6(+0.9)
7.1(+0.9)
74.1(-2.9)
38.4(+9.3)
40.2(+3.6)
(c)
-
V+S
35.8(+0.4)
59.1(+0.4)
43.6(-0.1)
31.2(+25.0)
119.1(+42.1)
29.6(+0.5)
38.1(+1.5)
(d)
-
V+A+S
37.6(+2.2)
60.3(+1.6)
44.6(+0.9)
32.6(+26.4)
120.9(+43.9)
37.8(+8.7)
40.8(+4.2)"
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.3435897435897436,"(e)
V
V
47.5
67.4
46.7
14.0
93.6
56.5
45.0
(f)
V
V+A+S
49.2(+1.7)
70.6(+3.2)
47.3(+0.6)
30.4(+16.4)
130.1(+36.5)
58.5(+2.0)
44.9(-0.1)
(g)
V+A
V+A
53.8(+6.3)
70.9(+3.5)
47.4(+0.7)
30.3(+16.3)
130.7(+37.1)
62.6(+6.1)
46.3(+1.3)
(h)
V+S
V+S
49.5(+2.0)
68.9(+1.5)
47.4(+0.7)
42.6(+28.6)
186.5(+92.9)
48.2(-8.3)
43.6(-1.4)
(i)
V+A+S
V+A+S
53.0(+5.5)
71.7(+4.3)
47.9(+1.2)
44.0(+30.0)
187.5(+93.9)
58.8(+2.3)
48.5(+3.5)
(j)
V+A& V+S & V+A+S
V+A+S
55.3(+7.8)
71.7(+4.3)
48.2(+1.5)
45.9(+31.9)
188.0(+94.4)
62.2(+5.7)
48.4(+3.4)"
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.34615384615384615,"A-T Quality. We train multiple A-T models on different corpora and fine-tune them on audio-text
benchmarks, including Clothov2 and AudioCaps. We observe that VALOR-1M and WavCaps are
prone to overfitting, so we adjust the training iterations to obtain the best results for models trained
on each corpus instead of training for the same number of iterations. As shown in Table 5, model (d)
surpasses the baseline and models trained on both VALOR-1M and WavCaps on all four benchmarks."
COMPARISON TO OPEN-SOURCE CROSS-MODALITY TRAINING CORPUS,0.3487179487179487,"OMV-OMC Quality. We further investigate the correspondence quality between OMV and OMC in
the VAST-27M dataset and compare it primarily to the VALOR-1M dataset. All models undergo 50K
training steps with LOM as the objective, utilizing all modalities in both pretraining and finetuning.
The results are presented in Table 6. From the table, we observe that omni-modality pretraining on
VAST-27M significantly improves performance across all 7 benchmarks compared to the baseline.
Additionally, VAST-27M outperforms VALOR-1M on 5 benchmarks in the MSRVTT and YouCook2
datasets, but lags behind on the VALOR-32K benchmarks due to the consistent video distribution
and caption style between VALOR-1M and VALOR-32K. As an additional experiment, we train
model (c) on VAST-27M but replace the omni-modality captions generated by LLM with a simple
concatenation of vision, audio captions, and subtitles. Model (d) outperforms model (c) on all
benchmarks, showcasing the necessity and effectiveness of leveraging the powerful capabilities of
LLM to integrate single-modality captions into omni-modality ones."
ABLATION STUDY,0.35128205128205126,"5.4
Ablation Study"
ABLATION STUDY,0.35384615384615387,"We conduct comprehensive ablation studies on pretraining and finetuning models using different
modalities for video representations to demonstrate the strength and necessity of omni-modality
foundation model pretraining. Models are evaluated on the MSRVTT (open-domain), YouCook2
(subtitle-important), and VALOR-32K (audio-important) benchmarks. The results are presented
in Table 7, when no pretraining is applied, models trained with audio and subtitles incorporated
(d) can improve the vision-only baseline on all benchmarks. When omni-modality pretraining is
applied, the improvement becomes more evidently on most benchmarks, which can be reflected by
the comparison between the green values in model (f) and model (d), demonstrating the effectiveness
of proposed method. In addition, model (i) outperforms model (f) on all benchmarks, indicating that,
similar to the V-T capability, the OMV-OMC capability also benefits from large-scale pretraining. It
is worth noting that model (h) and (f) exhibit inferior results on VALOR-32K benchmarks compared
to model (g) and even the vision baseline (e). This degradation can be attributed to the inconsistency
between pretraining and finetuning, where all clips have subtitles in VAST-27M while most videos in
VALOR-32K lack subtitles. When the modality grouping strategy is applied, model (j) demonstrates"
ABLATION STUDY,0.3564102564102564,"good generalization across all types of tasks, as the correlations between text and every modality
group have been explored during pretraining."
ABLATION STUDY,0.358974358974359,"6
Conclusion, Broader Impact and Limitation"
ABLATION STUDY,0.36153846153846153,"In this paper, we introduce the VAST-27M corpus, a large-scale omni-modality video caption dataset,
aimed at advancing research in multi-modality pretraining. Each video clip in the dataset is accom-
panied by automatically generated vision and audio captions, as well as an omni-modality caption
that integrates information from vision, audio, and subtitles using a pre-existing Large Language
Model (LLM). We also train a unified foundation model called VAST, capable of understanding and
connecting the various modalities in videos and captions. Through extensive evaluations, VAST
demonstrates its effectiveness in a wide range of vision-text, audio-text, and multi-modal video-text
tasks, including retrieval, captioning, and question answering, surpassing existing state-of-the-art
methods on public cross-modality benchmarks."
ABLATION STUDY,0.3641025641025641,"The advancements in multi-modal understanding and generation of video content can have a sig-
nificant impact on various domains such as entertainment, education, security, transportation, and
healthcare, among others. The development of omni-modality foundation models, like VAST-27M
and VAST, can contribute to the progress and practical applications in these areas. However, it is
important to acknowledge the limitations of our method. For instance, there is a need for more diverse
and larger-scale omni-modality corpora beyond VAST-27M. Furthermore, although VAST already
supports a wide range of downstream tasks, the integration of LLM is necessary to further enhance
its generalization capabilities. Additionally, due to the reason that the data collection process of
VAST-27M have used LLM, and the training process of video and audio captioners have utilized
some open-sourced cross-modality corpus, so VAST-27M dataset and VAST model may suffer the
same bias of those datasets and models."
ABLATION STUDY,0.36666666666666664,Acknowledgments
ABLATION STUDY,0.36923076923076925,"This work was supported by the National Key Research and Development Program of China (No.
2022ZD0118801), National Natural Science Foundation of China (U21B2043, 62102416, 62206279)."
REFERENCES,0.3717948717948718,References
REFERENCES,0.37435897435897436,"[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell et al., “Language models are few-shot learners,” Advances in neural information processing
systems, vol. 33, pp. 1877–1901, 2020."
REFERENCES,0.3769230769230769,"[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers
for language understanding,” arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.37948717948717947,"[3] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, “Frozen in time: A joint video and image encoder for
end-to-end retrieval,” in Proceedings of the IEEE/CVF International Conference on Computer Vision,
2021, pp. 1728–1738."
REFERENCES,0.382051282051282,"[4] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, “Howto100m: Learning a
text-video embedding by watching hundred million narrated video clips,” in Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2019, pp. 2630–2640."
REFERENCES,0.38461538461538464,"[5] H. Xue, T. Hang, Y. Zeng, Y. Sun, B. Liu, H. Yang, J. Fu, and B. Guo, “Advancing high-resolution
video-language representation with large-scale video transcriptions,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2022, pp. 5036–5045."
REFERENCES,0.3871794871794872,"[6] R. Zellers, X. Lu, J. Hessel, Y. Yu, J. S. Park, J. Cao, A. Farhadi, and Y. Choi, “Merlot: Multimodal
neural script knowledge models,” Advances in Neural Information Processing Systems, vol. 34, pp.
23 634–23 651, 2021."
REFERENCES,0.38974358974358975,"[7] S. Chen, X. He, L. Guo, X. Zhu, W. Wang, J. Tang, and J. Liu, “Valor: Vision-audio-language omni-
perception pretraining model and dataset,” arXiv preprint arXiv:2304.08345, 2023."
REFERENCES,0.3923076923076923,"[8] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez,
I. Stoica, and E. P. Xing, “Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,”
March 2023. [Online]. Available: https://lmsys.org/blog/2023-03-30-vicuna/"
REFERENCES,0.39487179487179486,"[9] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, “Conceptual 12m: Pushing web-scale image-text
pre-training to recognize long-tail visual concepts,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2021, pp. 3558–3568."
REFERENCES,0.3974358974358974,"[10] P. Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual captions: A cleaned, hypernymed, image
alt-text dataset for automatic image captioning,” in Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), 2018, pp. 2556–2565."
REFERENCES,0.4,"[11] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter,
“Audio set: An ontology and human-labeled dataset for audio events,” in 2017 IEEE international
conference on acoustics, speech and signal processing (ICASSP).
IEEE, 2017, pp. 776–780."
REFERENCES,0.4025641025641026,"[12] K. Drossos, S. Lipping, and T. Virtanen, “Clotho: An audio captioning dataset,” in ICASSP 2020-2020
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2020, pp.
736–740."
REFERENCES,0.40512820512820513,"[13] C. D. Kim, B. Kim, H. Lee, and G. Kim, “Audiocaps: Generating captions for audios in the wild,” in
Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019, pp. 119–132."
REFERENCES,0.4076923076923077,"[14] I. Martin Morato and A. Mesaros, “Diversity and bias in audio captioning datasets,” 2021."
REFERENCES,0.41025641025641024,"[15] M. Wu, H. Dinkel, and K. Yu, “Audio caption: Listen and tell,” in ICASSP 2019-2019 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2019, pp. 830–834."
REFERENCES,0.4128205128205128,"[16] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, “Large-scale contrastive language-
audio pretraining with feature fusion and keyword-to-caption augmentation,” in ICASSP 2023-2023 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2023, pp. 1–5."
REFERENCES,0.4153846153846154,"[17] F. Font, G. Roma, and X. Serra, “Freesound technical demo,” in Proceedings of the 21st ACM international
conference on Multimedia, 2013, pp. 411–412."
REFERENCES,0.41794871794871796,"[18] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley, Y. Zou, and W. Wang, “Wavcaps:
A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,”
arXiv preprint arXiv:2303.17395, 2023."
REFERENCES,0.4205128205128205,"[19] J. Wang, Z. Yang, X. Hu, L. Li, K. Lin, Z. Gan, Z. Liu, C. Liu, and L. Wang, “Git: A generative
image-to-text transformer for vision and language,” arXiv preprint arXiv:2205.14100, 2022."
REFERENCES,0.4230769230769231,"[20] L. Li, J. Lei, Z. Gan, L. Yu, Y.-C. Chen, R. Pillai, Y. Cheng, L. Zhou, X. E. Wang, W. Y. Wang
et al., “Value: A multi-task benchmark for video-and-language understanding evaluation,” arXiv preprint
arXiv:2106.04632, 2021."
REFERENCES,0.4256410256410256,"[21] V. Gabeur, C. Sun, K. Alahari, and C. Schmid, “Multi-modal transformer for video retrieval,” in European
Conference on Computer Vision.
Springer, 2020, pp. 214–229."
REFERENCES,0.4282051282051282,"[22] S. Chen, X. Zhu, D. Hao, W. Liu, J. Liu, Z. Zhao, L. Guo, and J. Liu, “Mm21 pre-training for video
understanding challenge: Video captioning with pretraining techniques,” in Proceedings of the 29th ACM
International Conference on Multimedia, 2021, pp. 4853–4857."
REFERENCES,0.4307692307692308,"[23] H. Luo, L. Ji, B. Shi, H. Huang, N. Duan, T. Li, J. Li, T. Bharti, and M. Zhou, “Univl: A unified
video and language pre-training model for multimodal understanding and generation,” arXiv preprint
arXiv:2002.06353, 2020."
REFERENCES,0.43333333333333335,"[24] P. H. Seo, A. Nagrani, and C. Schmid, “Look before you speak: Visually contextualized utterances,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp.
16 877–16 887."
REFERENCES,0.4358974358974359,"[25] A. Rouditchenko, A. Boggust, D. Harwath, B. Chen, D. Joshi, S. Thomas, K. Audhkhasi, H. Kuehne,
R. Panda, R. Feris et al., “Avlnet: Learning audio-visual language representations from instructional
videos,” arXiv preprint arXiv:2006.09199, 2020."
REFERENCES,0.43846153846153846,"[26] R. Zellers, J. Lu, X. Lu, Y. Yu, Y. Zhao, M. Salehi, A. Kusupati, J. Hessel, A. Farhadi, and Y. Choi,
“Merlot reserve: Neural script knowledge through vision and language and sound,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16 375–16 387."
REFERENCES,0.441025641025641,"[27] Z. Yang, Y. Fang, C. Zhu, R. Pryzant, D. Chen, Y. Shi, Y. Xu, Y. Qian, M. Gao, Y.-L. Chen et al., “i-code:
An integrative and composable multimodal learning framework,” arXiv preprint arXiv:2205.01818, 2022."
REFERENCES,0.44358974358974357,"[28] J. Li, D. Li, C. Xiong, and S. Hoi, “Blip: Bootstrapping language-image pre-training for unified vision-
language understanding and generation,” arXiv preprint arXiv:2201.12086, 2022."
REFERENCES,0.4461538461538462,"[29] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and
A. Komatsuzaki, “Laion-400m: Open dataset of clip-filtered 400 million image-text pairs,” arXiv preprint
arXiv:2111.02114, 2021."
REFERENCES,0.44871794871794873,"[30] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft
coco: Common objects in context,” in European conference on computer vision.
Springer, 2014, pp.
740–755."
REFERENCES,0.4512820512820513,"[31] X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang, “Vatex: A large-scale, high-quality
multilingual dataset for video-and-language research,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision, 2019, pp. 4581–4591."
REFERENCES,0.45384615384615384,"[32] J. Xu, T. Mei, T. Yao, and Y. Rui, “Msr-vtt: A large video description dataset for bridging video and
language,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp.
5288–5296."
REFERENCES,0.4564102564102564,"[33] D. Chen and W. B. Dolan, “Collecting highly parallel data for paraphrase evaluation,” in Proceedings of
the 49th annual meeting of the association for computational linguistics: human language technologies,
2011, pp. 190–200."
REFERENCES,0.45897435897435895,"[34] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,
E. Hambro, F. Azhar et al., “Llama: Open and efficient foundation language models,” arXiv preprint
arXiv:2302.13971, 2023."
REFERENCES,0.46153846153846156,"[35] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly et al., “An image is worth 16x16 words: Transformers for image
recognition at scale,” arXiv preprint arXiv:2010.11929, 2020."
REFERENCES,0.4641025641025641,"[36] S. Chen, Y. Wu, C. Wang, S. Liu, D. Tompkins, Z. Chen, and F. Wei, “Beats: Audio pre-training with
acoustic tokenizers,” arXiv preprint arXiv:2212.09058, 2022."
REFERENCES,0.4666666666666667,"[37] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey
et al., “Google’s neural machine translation system: Bridging the gap between human and machine
translation,” arXiv preprint arXiv:1609.08144, 2016."
REFERENCES,0.46923076923076923,"[38] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi, “Align before fuse: Vision and
language representation learning with momentum distillation,” Advances in neural information processing
systems, vol. 34, pp. 9694–9705, 2021."
REFERENCES,0.4717948717948718,"[39] Q. Sun, Y. Fang, L. Wu, X. Wang, and Y. Cao, “Eva-clip: Improved training techniques for clip at scale,”
arXiv preprint arXiv:2303.15389, 2023."
REFERENCES,0.47435897435897434,"[40] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,
J. Clark et al., “Learning transferable visual models from natural language supervision,” in International
Conference on Machine Learning.
PMLR, 2021, pp. 8748–8763."
REFERENCES,0.47692307692307695,"[41] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel, “Self-critical sequence training for image
captioning,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp.
7008–7024."
REFERENCES,0.4794871794871795,"[42] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik, “Flickr30k en-
tities: Collecting region-to-phrase correspondences for richer image-to-sentence models,” in Proceedings
of the IEEE international conference on computer vision, 2015, pp. 2641–2649."
REFERENCES,0.48205128205128206,"[43] Y. Li, Y. Song, L. Cao, J. Tetreault, L. Goldberg, A. Jaimes, and J. Luo, “Tgif: A new dataset and
benchmark on animated gif description,” in Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2016, pp. 4641–4650."
REFERENCES,0.4846153846153846,"[44] D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang, “Video question answering via gradually
refined attention over appearance and motion,” in Proceedings of the 25th ACM international conference
on Multimedia, 2017, pp. 1645–1653."
REFERENCES,0.48717948717948717,"[45] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, “Making the v in vqa matter: Elevating the
role of image understanding in visual question answering,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2017, pp. 6904–6913."
REFERENCES,0.4897435897435897,"[46] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-image pre-training with frozen
image encoders and large language models,” arXiv preprint arXiv:2301.12597, 2023."
REFERENCES,0.49230769230769234,"[47] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal,
S. Som et al., “Image as a foreign language: Beit pretraining for all vision and vision-language tasks,”
arXiv preprint arXiv:2208.10442, 2022."
REFERENCES,0.4948717948717949,"[48] P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang, “Ofa: Unifying
architectures, tasks, and modalities through a simple sequence-to-sequence learning framework,” in
International Conference on Machine Learning.
PMLR, 2022, pp. 23 318–23 340."
REFERENCES,0.49743589743589745,"[49] W. Kuo, A. Piergiovanni, D. Kim, X. Luo, B. Caine, W. Li, A. Ogale, L. Zhou, A. Dai, Z. Chen et al.,
“Mammut: A simple architecture for joint learning for multimodal tasks,” arXiv preprint arXiv:2303.16839,
2023."
REFERENCES,0.5,"[50] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner,
B. Mustafa, L. Beyer et al., “Pali: A jointly-scaled multilingual language-image model,” arXiv preprint
arXiv:2209.06794, 2022."
REFERENCES,0.5025641025641026,"[51] L. Zhou, C. Xu, and J. J. Corso, “Towards automatic learning of procedures from web instructional
videos,” in Thirty-Second AAAI Conference on Artificial Intelligence, 2018."
REFERENCES,0.5051282051282051,"[52] L. Anne Hendricks, O. Wang, E. Shechtman, J. Sivic, T. Darrell, and B. Russell, “Localizing moments in
video with natural language,” in Proceedings of the IEEE international conference on computer vision,
2017, pp. 5803–5812."
REFERENCES,0.5076923076923077,"[53] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. Carlos Niebles, “Dense-captioning events in videos,” in
Proceedings of the IEEE international conference on computer vision, 2017, pp. 706–715."
REFERENCES,0.5102564102564102,"[54] K. Li, Y. Wang, Y. Li, Y. Wang, Y. He, L. Wang, and Y. Qiao, “Unmasked teacher: Towards training-
efficient video foundation models,” arXiv preprint arXiv:2303.16058, 2023."
REFERENCES,0.5128205128205128,"[55] D. Ko, J. Choi, H. K. Choi, K.-W. On, B. Roh, and H. J. Kim, “Meltr: Meta loss transformer for learning
to fine-tune video foundation models,” arXiv preprint arXiv:2303.13009, 2023."
REFERENCES,0.5153846153846153,"[56] H. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li, “Clip4clip: An empirical study of clip for
end to end video clip retrieval and captioning,” Neurocomputing, vol. 508, pp. 293–304, 2022."
REFERENCES,0.517948717948718,"[57] Y. Wang, K. Li, Y. Li, Y. He, B. Huang, Z. Zhao, H. Zhang, J. Xu, Y. Liu, Z. Wang et al., “Intern-
video: General video foundation models via generative and discriminative learning,” arXiv preprint
arXiv:2212.03191, 2022."
REFERENCES,0.5205128205128206,"[58] J. Lei, L. Yu, T. L. Berg, and M. Bansal, “Tvr: A large-scale dataset for video-subtitle moment retrieval,”
in Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XXI 16.
Springer, 2020, pp. 447–463."
REFERENCES,0.5230769230769231,"[59] G. Li, Y. Wei, Y. Tian, C. Xu, J.-R. Wen, and D. Hu, “Learning to answer questions in dynamic
audio-visual scenarios,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2022, pp. 19 108–19 118."
REFERENCES,0.5256410256410257,"[60] Z. Yu, D. Xu, J. Yu, T. Yu, Z. Zhao, Y. Zhuang, and D. Tao, “Activitynet-qa: A dataset for understanding
complex web videos via question answering,” in Proceedings of the AAAI Conference on Artificial
Intelligence, vol. 33, no. 01, 2019, pp. 9127–9134."
REFERENCES,0.5282051282051282,"[61] K. Lin, L. Li, C.-C. Lin, F. Ahmed, Z. Gan, Z. Liu, Y. Lu, and L. Wang, “Swinbert: End-to-end
transformers with sparse attention for video captioning,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2022, pp. 17 949–17 958."
REFERENCES,0.5307692307692308,"[62] M. Tang, Z. Wang, Z. Zeng, F. Rao, and D. Li, “Clip4caption++: Multi-clip for video caption,” arXiv
preprint arXiv:2110.05204, 2021."
REFERENCES,0.5333333333333333,"[63] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back,
P. Natsev et al., “The kinetics human action video dataset,” arXiv preprint arXiv:1705.06950, 2017."
REFERENCES,0.5358974358974359,"[64] S. Chen, Y. Zhao, Q. Jin, and Q. Wu, “Fine-grained video-text retrieval with hierarchical graph reasoning,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp.
10 638–10 647."
REFERENCES,0.5384615384615384,"[65] A. Rohrbach, A. Torabi, M. Rohrbach, N. Tandon, C. Pal, H. Larochelle, A. Courville, and B. Schiele,
“Movie description,” International Journal of Computer Vision, vol. 123, no. 1, pp. 94–120, 2017."
REFERENCES,0.541025641025641,"[66] Y. Jang, Y. Song, Y. Yu, Y. Kim, and G. Kim, “Tgif-qa: Toward spatio-temporal reasoning in visual
question answering,” in Proceedings of the IEEE conference on computer vision and pattern recognition,
2017, pp. 2758–2766."
REFERENCES,0.5435897435897435,"[67] A.-M. Oncescu, A. Koepke, J. F. Henriques, Z. Akata, and S. Albanie, “Audio retrieval with natural
language queries,” arXiv preprint arXiv:2105.02192, 2021."
REFERENCES,0.5461538461538461,"[68] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for generating image descriptions,” in
Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3128–3137."
REFERENCES,0.5487179487179488,"[69] X. Cheng, H. Lin, X. Wu, F. Yang, and D. Shen, “Improving video-text retrieval by multi-stream corpus
alignment and dual softmax loss,” arXiv preprint arXiv:2109.04290, 2021."
REFERENCES,0.5512820512820513,"[70] J. Lei, T. L. Berg, and M. Bansal, “Revealing single frame bias for video-and-language learning,” arXiv
preprint arXiv:2206.03428, 2022."
REFERENCES,0.5538461538461539,"[71] J. Wang, D. Chen, Z. Wu, C. Luo, L. Zhou, Y. Zhao, Y. Xie, C. Liu, Y.-G. Jiang, and L. Yuan, “Omnivl:
One foundation model for image-language and video-language tasks,” arXiv preprint arXiv:2209.07526,
2022."
REFERENCES,0.5564102564102564,"[72] Q. Ye, G. Xu, M. Yan, H. Xu, Q. Qian, J. Zhang, and F. Huang, “Hitea: Hierarchical temporal-aware
video-language pre-training,” arXiv preprint arXiv:2212.14546, 2022."
REFERENCES,0.558974358974359,"[73] F. Cheng, X. Wang, J. Lei, D. Crandall, M. Bansal, and G. Bertasius, “Vindlu: A recipe for effective
video-and-language pretraining,” arXiv preprint arXiv:2212.05051, 2022."
REFERENCES,0.5615384615384615,"[74] L. Li, Z. Gan, K. Lin, C.-C. Lin, Z. Liu, C. Liu, and L. Wang, “Lavender: Unifying video-language
understanding as masked language modeling,” arXiv preprint arXiv:2206.07160, 2022."
REFERENCES,0.5641025641025641,"[75] A. J. Wang, Y. Ge, R. Yan, Y. Ge, X. Lin, G. Cai, J. Wu, Y. Shan, X. Qie, and M. Z. Shou, “All in one:
Exploring unified video-language pre-training,” arXiv preprint arXiv:2203.07303, 2022."
REFERENCES,0.5666666666666667,"[76] Y. Ma, G. Xu, X. Sun, M. Yan, J. Zhang, and R. Ji, “X-clip: End-to-end multi-grained contrastive learning
for video-text retrieval,” in Proceedings of the 30th ACM International Conference on Multimedia, 2022,
pp. 638–647."
REFERENCES,0.5692307692307692,"[77] H. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Bi, Q. Qian, W. Wang et al., “mplug-2: A modularized
multi-modal foundation model across text, image and video,” arXiv preprint arXiv:2302.00402, 2023."
REFERENCES,0.5717948717948718,"[78] H. Xue, Y. Sun, B. Liu, J. Fu, R. Song, H. Li, and J. Luo, “Clip-vip: Adapting pre-trained image-text
model to video-language representation alignment,” arXiv preprint arXiv:2209.06430, 2022."
REFERENCES,0.5743589743589743,"[79] V. Gabeur, A. Nagrani, C. Sun, K. Alahari, and C. Schmid, “Masking modalities for cross-modal video
retrieval,” in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022,
pp. 1766–1775."
REFERENCES,0.5769230769230769,"[80] Y.-B. Lin, J. Lei, M. Bansal, and G. Bertasius, “Eclipse: Efficient long-range video retrieval using sight
and sound,” arXiv preprint arXiv:2204.02874, 2022."
REFERENCES,0.5794871794871795,"[81] M. Patrick, P.-Y. Huang, Y. Asano, F. Metze, A. Hauptmann, J. Henriques, and A. Vedaldi, “Support-set
bottlenecks for video-text representation learning,” arXiv preprint arXiv:2010.02824, 2020."
REFERENCES,0.5820512820512821,"[82] Q. Wang, Y. Zhang, Y. Zheng, P. Pan, and X.-S. Hua, “Disentangled representation learning for text-video
retrieval,” arXiv preprint arXiv:2203.07111, 2022."
REFERENCES,0.5846153846153846,"[83] H. Xu, G. Ghosh, P.-Y. Huang, P. Arora, M. Aminzadeh, C. Feichtenhofer, F. Metze, and L. Zettle-
moyer, “Vlm: Task-agnostic video-language model pre-training for video understanding,” arXiv preprint
arXiv:2105.09996, 2021."
REFERENCES,0.5871794871794872,"[84] D. Li, J. Li, H. Li, J. C. Niebles, and S. C. Hoi, “Align and prompt: Video-and-language pre-training
with entity prompts,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2022, pp. 4953–4963."
REFERENCES,0.5897435897435898,"[85] T.-J. Fu, L. Li, Z. Gan, K. Lin, W. Y. Wang, L. Wang, and Z. Liu, “Violet: End-to-end video-language
transformers with masked visual-token modeling,” arXiv preprint arXiv:2111.12681, 2021."
REFERENCES,0.5923076923076923,"[86] L. Yuan, D. Chen, Y.-L. Chen, N. Codella, X. Dai, J. Gao, H. Hu, X. Huang, B. Li, C. Li et al., “Florence:
A new foundation model for computer vision,” arXiv preprint arXiv:2111.11432, 2021."
REFERENCES,0.5948717948717949,"[87] J. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu, “Less is more: Clipbert for video-and-
language learning via sparse sampling,” in Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2021, pp. 7331–7341."
REFERENCES,0.5974358974358974,"[88] T.-J. Fu, L. Li, Z. Gan, K. Lin, W. Y. Wang, L. Wang, and Z. Liu, “An empirical study of end-to-end
video-language transformers with masked visual modeling,” arXiv preprint arXiv:2209.01540, 2022."
REFERENCES,0.6,"[89] J. Huang, Y. Li, J. Feng, X. Sun, and R. Ji, “Clover: Towards a unified video-language alignment and
fusion model,” arXiv preprint arXiv:2207.07885, 2022."
REFERENCES,0.6025641025641025,"[90] A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid, “Just ask: Learning to answer questions from
millions of narrated videos,” in Proceedings of the IEEE/CVF International Conference on Computer
Vision, 2021, pp. 1686–1697."
REFERENCES,0.6051282051282051,"[91] ——, “Zero-shot video question answering via frozen bidirectional language models,” arXiv preprint
arXiv:2206.08155, 2022."
REFERENCES,0.6076923076923076,"[92] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Milli-
can, M. Reynolds et al., “Flamingo: a visual language model for few-shot learning,” arXiv preprint
arXiv:2204.14198, 2022."
REFERENCES,0.6102564102564103,"[93] S. Yan, T. Zhu, Z. Wang, Y. Cao, M. Zhang, S. Ghosh, Y. Wu, and J. Yu, “Video-text modeling with
zero-shot transfer from contrastive captioners,” arXiv preprint arXiv:2212.04979, 2022."
REFERENCES,0.6128205128205129,"[94] A. Nagrani, P. H. Seo, B. Seybold, A. Hauth, S. Manen, C. Sun, and C. Schmid, “Learning audio-video
modalities from image captions,” arXiv preprint arXiv:2204.00679, 2022."
REFERENCES,0.6153846153846154,"[95] P. Wang, S. Wang, J. Lin, S. Bai, X. Zhou, J. Zhou, X. Wang, and C. Zhou, “One-peace: Exploring one
general representation model toward unlimited modalities,” arXiv preprint arXiv:2305.11172, 2023."
REFERENCES,0.617948717948718,"[96] X. Xu, H. Dinkel, M. Wu, Z. Xie, and K. Yu, “Investigating local and global information for automated
audio captioning with transfer learning,” in ICASSP 2021-2021 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2021, pp. 905–909."
REFERENCES,0.6205128205128205,"[97] Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao, “Simvlm: Simple visual language model
pretraining with weak supervision,” arXiv preprint arXiv:2108.10904, 2021."
REFERENCES,0.6230769230769231,"[98] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig, “Scaling
up visual and vision-language representation learning with noisy text supervision,” in International
Conference on Machine Learning.
PMLR, 2021, pp. 4904–4916."
REFERENCES,0.6256410256410256,"[99] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu, “Coca: Contrastive captioners are
image-text foundation models,” arXiv preprint arXiv:2205.01917, 2022."
REFERENCES,0.6282051282051282,"[100] M. Ding, B. Xiao, N. Codella, P. Luo, J. Wang, and L. Yuan, “Davit: Dual attention vision transformers,”
arXiv preprint arXiv:2204.03645, 2022."
REFERENCES,0.6307692307692307,Appendix
REFERENCES,0.6333333333333333,"A
More about VAST Foundation Model"
REFERENCES,0.6358974358974359,"A.1
Pretraining Settings"
REFERENCES,0.6384615384615384,"Specific pretraining configurations of VAST including training corpora, training steps for each corpus
(i.e., dataset mix ratio), and training objectives on each corpus are presented in Table 8. To enhance
data quality, we use trained vision captioner to generate new captions for CC12M and LAION datasets
and replace original captions with them. It is noted that VAST have been trained for relatively small
steps (205K steps), but have already shown excellent performances on various types of downstream
tasks, and we believe that training by more steps can further increase the model capabilities."
REFERENCES,0.6410256410256411,"Table 8: Model configurations and pretraining settings of VAST. It is noted that 400M Web data used
in CLIP [40] and LAION-400M [29] used in EVAClip [39] are also counted for training samples
statics. LAION-102M and LAION-110M are both random sampled subsets from LAION-400M.
Regarding training objectives, ‘ret’ represents for the combination of VCC and VCM, while ‘cap’
denotes VCG, and differnet modality groups are separeted by ‘%’."
REFERENCES,0.6435897435897436,"Model
Param
Sample
Training Corpus
Batch Size
Steps
Epoch
Objectives"
REFERENCES,0.6461538461538462,"VAST
1.3B
442M"
REFERENCES,0.6487179487179487,"VAST-27M
1024
60000
2.3
ret%vast%vat%vst%vt%at +
cap%vast%vat%vst%vt%at"
REFERENCES,0.6512820512820513,"VALOR-1M
1024
25000
25
ret%vat%vt%at +
cap%vat%vt%at"
REFERENCES,0.6538461538461539,"WavCaps
1024
15000
38
ret%at + cap%at"
REFERENCES,0.6564102564102564,"CC4M
2048
30000
12
ret%vt + cap%vt"
REFERENCES,0.658974358974359,"CC12M
2048
20000
4
ret%vt + cap%vt"
REFERENCES,0.6615384615384615,"LAION-110M
2048
55000
1
ret%vt + cap%vt"
REFERENCES,0.6641025641025641,"A.2
Downstream Datasets Descriptions"
REFERENCES,0.6666666666666666,"We evaluate VAST on multiple popular domnstream datasets, including MSRVTT, VATEX,
YouCook2, VALOR-32K, MSVD, LSMDC, DiDeMo, ActivityNet Caption, TGIF, MUSIC-AVQA,
TVC, Clotho, AudioCaps, MSCOCO, Flickr30K and VQAv2. Specific train/val/test splits of those
benchmarks can be found in Table 9 and specific descriptions of them are as follows."
REFERENCES,0.6692307692307692,"MSRVTT [32] contains 10K video clips and 200K captions. The videos cover a wide range of
topics and scenes, including human activities, sports, natural landscapes, and more. We evaluate
text-to-video retrieval, video captioning and video QA on this dataset. Following methods presented
in Table 11, we use the ‘1K-A split’ for retrieval evaluation. For captioning and QA, we use the
standard split."
REFERENCES,0.6717948717948717,"VATEX [31] contains 41,250 video clips sourced from Kinetics-600 dataset [63] and 825,000
sentence-level descriptions. We evaluate text-to-video retrieval and video captioning on this dataset.
For captioning, we use the official split. For retrieval. we follow the HGR [64] split protocol."
REFERENCES,0.6743589743589744,"YouCook2 [51] consists of 14K video clips from 2K instructional cooking videos from YouTube.
Each video includes multiple actions performed by the chef, along with corresponding textual
descriptions and temporal annotations. We evaluate text-to-video retrieval and video captioning on
this dataset with official splits."
REFERENCES,0.676923076923077,"VALOR-32K [7] is an audiovisual video-language benchmark that contains 32K 10 seconds long
audible video clips sourced from AudioSet [11]. Each video clip is annotated with an audiovisual
caption which simultaneously describes both visual and audio contents in videos. We evaluate
text-to-video retrieval and video captioning on this dataset with official splits."
REFERENCES,0.6794871794871795,"MSVD [33] contains 1,970 videos, each of which is paired with around 40 captions. We evaluate
video QA on this dataset and use the split proposed by Xu et al. [44]."
REFERENCES,0.6820512820512821,"LSMDC [65] consists of 118K clips form 202 movies, each of which is paird with one caption. We
evaluate text-to-video retrieval on this dataset with official split."
REFERENCES,0.6846153846153846,Table 9: Downstream dataset splits.
REFERENCES,0.6871794871794872,"Task Type
Modal Type
Benchmark
#Videos/#Images
#Captions/#QA-pairs"
REFERENCES,0.6897435897435897,"Train
Val
Test
Train
Val
Test"
REFERENCES,0.6923076923076923,Retrieval
REFERENCES,0.6948717948717948,"V-T(Vis)
MSCOCO
113287
5000
5000
566747
25010
25010
Flickr30K
29000
1014
1000
145000
5070
5000"
REFERENCES,0.6974358974358974,"A-T
ClothoV1
2893
1045
-
14465
5225
-
ClothoV2
3839
1045
-
19195
5225
-
AudioCaps
49291
428
816
49291
2140
4080"
REFERENCES,0.7,V-T(MM)
REFERENCES,0.7025641025641025,"MSRVTT
9000
-
1000
180000
-
1000
YouCook2
10337
3492
-
10337
3492
-
VALOR-32K
25000
3500
3500
25000
3500
3500
VATEX
25991
1500
1500
259910
1500
1500
DiDeMo
8394
1065
1003
8394
1065
1003
ANET
10009
-
4917
10009
-
4917
LSMDC
101046
7408
1000
101046
7408
1000"
REFERENCES,0.7051282051282052,Caption
REFERENCES,0.7076923076923077,"V-T(Vis)
MSCOCO
113287
5000
5000
566747
25010
25010
MSVD
1200
100
670
48774
4290
27763"
REFERENCES,0.7102564102564103,"A-T
ClothoV1
2893
1045
-
14465
5225
-
ClothoV2
3839
1045
-
19195
5225
-
AudioCaps
49838
495
975
49438
2475
4875"
REFERENCES,0.7128205128205128,V-T(MM)
REFERENCES,0.7153846153846154,"MSRVTT
6513
497
2990
130260
9940
59800
YouCook2
10337
3492
-
10337
3492
-
VALOR-32K
25000
3500
3500
25000
3500
3500
VATEX
25991
3000
6000
259910
30000
60000
TVC
86603
10841
-
174350
43580
- QA"
REFERENCES,0.717948717948718,"V-T(Vis)
MSVD-QA
1200
250
520
30933
6,415
13157
TGIF-FrameQA
32345
-
7132
39389
-
13691
VQAv2
82783
40504
37K/81K
4437570
2143540
1.1M/4.5M"
REFERENCES,0.7205128205128205,"V-T(MM)
MSRVTT-QA
6513
497
2990
158581
12278
72821
MUSIC-AVQA
9277
3815
6399
32087
4595
9185
ANET-QA
3200
1800
800
32000
18000
8000"
REFERENCES,0.7230769230769231,"DiDeMo [52] contains 10K long-form videos from Flickr and for each video, four short sentences
are annotated in temporal order. We follow methods in Table 11 to concatenate those short sentences
and evaluate ‘paragraph-to-video’ retrieval on this benchmark. The official split is used."
REFERENCES,0.7256410256410256,"ActivityNet Caption [53] contains 20K long-form videos (180s as average length) from YouTube
and 100K captions. We evaluate text-to-video retrieval and video QA on this dataset. For retrieval we
use official split and for video QA, split proposed by Yu et al. [60] is used."
REFERENCES,0.7282051282051282,"TGIF [66] contains three video QA benchmarks including TGIF-Action, TGIF-transition and TGIF-
Frame, and the first two are multiple-choice QA while the last is open-ended QA. We evaluate VAST
on TGIF-frame benchmark with official split."
REFERENCES,0.7307692307692307,"MUSIC-AVQA [59] is a audiovisual video QA benchmark containing more than 45K Q-A pairs
covering 33 different question templates spanning over different modalities and question types. The
offcial split is used."
REFERENCES,0.7333333333333333,"TVC [58] is a multi-channel video captioning dataset containing 108K video moments and 262K
paired captions. Video subtitles can be used as additional input. We evaluate video captioning on this
benchmark with official split."
REFERENCES,0.735897435897436,"Clotho [12] contains 15-30 second audio clips and has two versions. The original (v1) has 4981
audios, while an expanded version (v2) includes 6974 audios, enlarging solely the training set. We
evaluate text-to-audio retrieval and audio captioning on those benchmarks with official split."
REFERENCES,0.7384615384615385,"AudioCaps [13] contains 51K 10-second clips, with one caption in the training set and five in the
validation and test sets. We evaluate text-to-audio retrieval and audio captioning on it. For captioning,
we use the official split, and for retrieval we follow the Sophia et al. [67] split protocol."
REFERENCES,0.7410256410256411,"MSCOCO [30] contains 123K images each of which is paired with 5 annotated captions, We evaluate
text-to-image retrieval and image captioning on this dataset with Karpathy split [68]."
REFERENCES,0.7435897435897436,"Table 10: Downstream task finetuning settings. Lr, Bs, Epo, Obj and Res denote learning rate, batch
size, epoch, training objectives and resolution, respectively. Vf(Tr), Vf(Te), Ac(Tr), Ac(Te) denotes
sampled video frames (Vf) or audio clips (Ac) in training (Tr) and testing (Te), respectively. The
marks in Obj are the same as those in Table 8. Most hyperparameters in the table are not precisely
tuned."
REFERENCES,0.7461538461538462,"Task
Modality
Benchmark
Lr
Bs
Epo
Obj
Vf(Tr)
Vf(Te)
Ac(Tr)
Ac(Te)
Res RET"
REFERENCES,0.7487179487179487,"V-T(Vis)
MSCOCO
1e-5
256
5
ret%vt
-
-
-
-
384
Flickr
1e-5
256
5
ret%vt
-
-
-
-
384"
REFERENCES,0.7512820512820513,"A-T
ClothoV1/V2
2e-5
64
10
ret%at
-
-
3
3
-
AudioCaps
2e-5
64
10
ret%at
-
-
1
1
-"
REFERENCES,0.7538461538461538,V-T(MM)
REFERENCES,0.7564102564102564,"MSRVTT
2e-5
64
3.6
ret%vast
8
16
1
1
224
YouCook2
3e-5
64
30
ret%vast
8
16
1
1
224
VALOR-32K
2e-5
64
10
ret%vat
8
8
1
1
224
VATEX
2e-5
64
2.5
ret%vast
8
16
1
1
224
DiDeMo
2e-5
64
40
ret%vat
8
32
2
2
224
ANET
2e-5
64
20
ret%vat
8
32
2
2
224
LSMDC
2e-5
64
5
ret%vat
8
32
1
1
224 CAP"
REFERENCES,0.7589743589743589,"V-T(Vis)
MSCOCO
1e-5
64
5
cap%vt
-
-
-
-
480
MSCOCO(SCST)
2.5e-6
64
2.5
cap%vt
-
-
-
-
480"
REFERENCES,0.7615384615384615,"A-T
ClothoV1/V2
2e-5
64
10
cap%at
-
-
3
3
-
AudioCaps
2e-5
64
10
cap%at
-
-
1
1
-"
REFERENCES,0.764102564102564,V-T(MM)
REFERENCES,0.7666666666666667,"MSRVTT
2e-5
128
10
cap%vast
8
8
1
1
224
YouCook2
3e-5
64
30
cap%vast
8
16
1
1
224
VALOR-32K
1e-5
64
10
cap%vat
8
12
1
1
224
VATEX
2e-5
64
10
cap%vast
8
20
1
1
224
VATEX(SCST)
7e-6
64
5
cap%vast
8
20
1
1
224
TVC
3e-5
64
40
cap%vst
8
8
-
-
224 QA"
REFERENCES,0.7692307692307693,"V-T(Vis)
MSVD-QA
1e-5
64
10
qa%vt
8
14
-
-
224
TGIF-FrameQA
2e-5
64
10
qa%vt
4
4
-
-
224
VQAv2
2e-5
128
20
qa%vt
-
-
-
-
384"
REFERENCES,0.7717948717948718,"V-T(MM)
MSRVTT-QA
2e-5
64
4.5
qa%vast
8
8
1
1
224
MUSIC-AVQA
2e-5
64
20
qa%vat
8
8
2
2
224
ANET-QA
2e-5
64
10
qa%vat
8
16
2
2
224"
REFERENCES,0.7743589743589744,"Flickr30K [42] contains 31K images each of which is paired with 5 annotated captions, We evaluate
text-to-image retrieval on this dataset with Karpathy split [68]."
REFERENCES,0.7769230769230769,"VQAv2 [45] was used as the basis of the 2017 VQA Challenge2, it contains 1.1M questions with
11.1M answers relating to MSCOCO images. The official split is used."
REFERENCES,0.7794871794871795,"A.3
Finetuning Settings"
REFERENCES,0.782051282051282,Specific finetuning hyperparameters of VAST for different benchmarks are presented in Table 10.
REFERENCES,0.7846153846153846,"A.4
Detailed Comparisons to State-of-the-Art Methods"
REFERENCES,0.7871794871794872,"Text-to-Video Retrieval. We compare VAST to SOTA methods on six multi-modal text-to-video
retrieval benchmarks. As shown in Table 4, VAST improves previous SOTA methods by 5.1, 1.6, 3.7,
6.1 points on MSRVTT, DiDeMo, ActivityNet, VATEX benchmarks, respectively. Besides above
mentioned vision-oriented benchmarks, VAST outperforms VALOR-L [7] by 6.8 points on the audio-
oriented benchmark VALOR-32K, and surpass MELTR [55] by 16.7 points on the subtitle-oriented
benchmark YouCook2, which demonstrate the strong generalization capabilities of VAST towards
different types of downstream datasets. In addition, the zero-shot retrieval performance comparison is
shown in Table 12, VAST achieves 49.3 and 55.5 zero-shot R@1 performance that surpasses previous
SOTA by 7.3 and 6.9 points, respectively."
REFERENCES,0.7897435897435897,"Video QA. We evaluate VAST on five open-ended video QA benchmarks. As shown in Table 13,
VAST have achieved new SOTA performances on all benchmarks, and outperform recent proposed
large-scale foundation models such as GIT [19], MaMMUT [49], Flamingo [92] and CoCa [93]. In
addition, on the audiovisual video QA benchmark MUSIC-AVQA, VAST surpasses VALOR by 1.8
points, demonstrating its better capabilities to answer both visual and audio questions."
REFERENCES,0.7923076923076923,"Table 11: Performance comparison on Text-to-Video Retrieval benchmarks. Recall@1,5,10 are
used as evaluation metrics. For fair comparisons, performances before employing post-processing
such as dual-softmax [69] are reported and compared. All benchmarks are multi-modal benchmarks
(containing audio and subtitle tracks). Methods utilizing audio or subtitle modalities besides vision
for video representation are marked with gray background color."
REFERENCES,0.7948717948717948,"Method
Sample
MSRVTT
DiDeMo
ActivityNet
R@1
R@5
R@10
R@1
R@5
R@10
R@1
R@5
R@10"
REFERENCES,0.7974358974358975,"Singularity [70]
17M
41.5
68.7
77.0
53.9
79.4
86.9
47.1
75.5
85.5
OmniVL [71]
17M
47.8
74.2
83.8
52.4
79.5
85.4
-
-
-
HiTeA [72]
17M
46.8
71.2
81.9
56.5
81.7
89.7
49.7
77.1
86.7
VINDLU-L [73]
25M
48.8
72.4
82.2
59.8
86.6
91.5
55.9
82.3
90.9
LAVENDER [74]
30M
40.7
66.9
77.6
53.4
78.6
85.3
-
-
-
All-in-one [75]
138M
37.9
68.1
77.1
32.7
61.4
73.5
-
-
-
CLIP4Clip [56]
400M
44.5
71.4
81.6
43.4
70.2
80.6
40.5
72.4
-
X-CLIP [76]
400M
49.3
75.8
84.8
47.8
79.3
-
46.2
75.5
-
mPLUG-2 [77]
417M
53.1
77.6
84.7
56.4
79.1
85.2
-
-
-
UMT-L [54]
425M
58.8
81.0
87.1
70.4
90.1
93.5
66.8
89.1
94.9
CLIP-VIP [78]
500M
54.2
77.2
84.8
50.5
78.4
87.1
53.4
81.4
90.0"
REFERENCES,0.8,"MMT [21]
136M
26.6
57.1
69.6
-
-
-
28.7
61.4
-
AVLNet [25]
136M
22.5
50.5
64.1
-
-
-
-
-
-
Gabeur et al. [79]
136M
28.7
59.5
70.3
-
-
-
29.0
61.7
-
ECLIPSE [80]
400M
-
-
-
44.2
-
-
45.3
75.7
86.2
VALOR-L [7]
433.5M
54.4
79.8
87.6
57.6
83.3
88.8
63.4
87.8
94.1
VAST
442M
63.9
84.3
89.6
72.0
89.0
91.4
70.5
90.9
95.5"
REFERENCES,0.8025641025641026,"Method
VATEX
R@1
R@5
R@10"
REFERENCES,0.8051282051282052,"Support-set [81]
44.9
82.1
89.7
CLIP4Clip [56]
55.9
89.2
95.0
DCR [82]
65.7
92.6
96.7"
REFERENCES,0.8076923076923077,"VALOR-L
76.9
96.7
98.6
VAST
83.0
98.2
99.2"
REFERENCES,0.8102564102564103,"Method
VALOR-32K
R@1
R@5
R@10"
REFERENCES,0.8128205128205128,"Frozen [3]
32.9
60.4
71.2
CLIP4Clip [56]
43.4
69.9
79.7"
REFERENCES,0.8153846153846154,"AVLNet [25]
21.6
47.2
59.8
VALOR-L [7]
73.2
91.6
95.4
VAST
80.0
93.7
96.6"
REFERENCES,0.8179487179487179,"Method
YouCook2
R@1
R@5
R@10"
REFERENCES,0.8205128205128205,"UniVL [23]
28.9
57.6
70.0
MELTR [55]
33.7
63.1
74.8
VLM [83]
27.1
56.9
69.4"
REFERENCES,0.823076923076923,"VALUE [20]
31.3
53.0
62.2
VAST
50.4
74.3
80.8"
REFERENCES,0.8256410256410256,"Table 12: Performance comparison on zero-shot Text-to-Video Retrieval benchmarks. Methods
utilizing audio or subtitle modalities besides vision for video representation are marked with gray
background color."
REFERENCES,0.8282051282051283,"Method
Sample
MSRVTT
DiDeMo
R@1
R@5
R@10
R@1
R@5
R@10"
REFERENCES,0.8307692307692308,"Frozen [3]
5M
18.7
39.5
51.6
21.1
46.0
56.2
ALPRO [84]
5M
24.1
44.7
55.4
23.8
47.3
57.9
Singularity [70]
5M
28.4
50.2
59.5
36.9
61.6
69.3
HiTeA [72]
17M
34.4
60.0
69.9
43.2
69.3
79.0
OmniVL [71]
18M
42.0
63.0
73.0
40.6
64.6
74.3
VIOLET [85]
183M
25.9
49.5
59.7
23.5
49.8
59.8
UMT-L [54]
425M
40.7
63.4
71.8
48.6
72.9
79.0
Florence [86]
900M
37.6
63.8
72.6
-
-
-"
REFERENCES,0.8333333333333334,"VAST
443M
49.3
68.3
73.9
55.5
74.3
79.6"
REFERENCES,0.8358974358974359,"Video Captioning. In Table 14, we compare VAST to state-of-the-art methods on five multi-modal
video captioning benchmarks. According to the results, VAST have achieved new state-of-the-art
CIDEr score on all five benchmarks with evident margins. Compared to previous vision-language
modal SOTA method GIT [19] which takes a 5.1B DaViT [100] as vision encoder and conduct
pretraining on 12.9B private image-text corpus, VAST surpass it with only 22.5% parameters and
3.4% training data, demonstrating the high efficiency of our method. Compared to previous multi-
modal video-language SOTA method VALOR [19], VAST can additionally process subtitle-oriented
benchmarks such as YouCook2 and TVC, and achieves better results due to that it jointly models the
relations between text and omni-modalities in videos."
REFERENCES,0.8384615384615385,"Table 13: Performance comparison on Video QA benchmarks. Acc is used as evaluation metric.
MSVD-QA and TGIF-QA are vision-only benchmarks while the others are multi-modal benchmarks.
Methods utilizing audio or subtitle modalities besides vision for video representation are marked with
gray background color."
REFERENCES,0.841025641025641,"Method
Sample
MSRVTT-QA
MSVD-QA
TGIF-QA
ActivityNet-QA
MUSIC-AVQA"
REFERENCES,0.8435897435897436,"ClipBERT [87]
5.4M
37.4
-
60.3
-
ALPRO [84]
5M
42.1
45.9
-
-
-
VIOLETv2 [88]
5M
44.5
54.7
72.8
-
-
Clover [89]
5M
43.9
51.9
71.4
-
-
OmniVL [71]
17M
44.1
51.0
-
HiTeA [72]
17M
45.9
55.3
73.2
46.4
-
SINGULARITY [70]
17M
43.5
-
-
43.1
-
VINDLU-B [73]
17M
43.8
-
-
44.6
-
LAVENDER [74]
30M
45.0
56.6
73.5
-
-
JustAsk [90]
69M
41.5
46.3
-
38.9
-
MERLOT [6]
180M
43.1
-
69.5
41.4
-
All-in-one [75]
228.5M
46.8
48.3
66.3
-
-
FrozenBiLM [91]
410M
47.0
54.8
68.6
43.2
-
mPLUG-2 [77]
417M
48.0
58.1
75.4
-
-
UMT-L [54]
425M
47.1
55.2
-
-
-
InternVideo [57]
646M
47.1
55.5
72.2
-
-
GIT [19]
1.7B
43.2
56.8
72.8
-
-
MaMMUT [49]
2B
49.5
60.2
-
-
-
Flamingo (80B) [92]
2.3B
47.4
-
-
-
-
VideoCoCa (2.1B) [93]
4.8B
46.0
56.9
-
-
-
GIT2 (5.1B) [19]
12.9B
45.6
58.2
74.9
-
-"
REFERENCES,0.8461538461538461,"VALOR-L [7]
433.5M
49.2
60.0
78.7
48.6
78.9
VAST(1.3B)
442M
50.1
60.2
79.1
50.4
80.7"
REFERENCES,0.8487179487179487,"Table 14: Performance comparison on Video Captioning benchmarks. All benchmarks are multi-
modal benchmarks. BLEU@4 (B@4) and CIDEr (C) metrics are used as evaluation metrics. On
VATEX benchmark, we follow most state-of-the-art methods [19; 7; 62] employing SCST finetun-
ing [41] after cross-entropy training, and corresponding results are marked with ‘*’. Methods utilizing
audio or subtitle modalities besides vision for video representation are marked with gray background
color."
REFERENCES,0.8512820512820513,"Method
Sample
MSRVTT
VATEX
YouCook2
TVC
VALOR-32K
B@4
C
B@4
C
B@4
C
B@4
C
B@4
C"
REFERENCES,0.8538461538461538,"SwinBERT [61]
-
41.9
53.8
38.7
73.0
9.0
109.0
14.5
55.4
5.4
27.3
VIOLETv2 [88]
5M
-
58.0
-
-
-
-
-
-
-
-
HiTeA [72]
5M
-
62.5
-
-
-
-
-
-
-
-
LAVENDER [74]
30M
-
60.1
-
-
-
-
-
-
-
-
MaMMUT [49]
2B
-
73.6
-
-
-
-
-
-
-
-
GIT [19]
1.7B
53.8
73.9
41.6*
91.5*
10.3
129.8
16.2
63.0
GIT2(5.1B) [19]
12.9B
54.8
75.9
42.7*
94.5*
9.4
131.2
16.9
66.1
-
-"
REFERENCES,0.8564102564102564,"SMPFF [22]
-
48.4
58.5
39.7
70.5
-
-
-
-
7.5
37.1
VALUE [20]
136M
-
-
-
58.1
12.4
130.3
11.6
50.5
-
-
UniVL [23]
136M
41.8
50.0
-
-
17.4
181.0
-
-
-
-
MELTR [55]
136M
44.2
52.8
-
-
17.9
190.0
-
-
-
-
CLIP4Caption++ [62]
400M
-
-
40.6*
85.7*
-
-
15.0
66.0
-
-
VALOR-L [7]
433.5M
54.4
74.0
45.6*
95.8*
-
-
-
-
9.6
61.5
VAST(1.3B)
442M
56.7
78.0
45.0*
99.5*
18.2
198.8
19.9
74.1
9.9
62.2"
REFERENCES,0.8589743589743589,"Table 15: Performance comparison on Text-to-Audio Retrieval benchmarks. Recall@1,5,10 are used
as evaluation metrics."
REFERENCES,0.8615384615384616,"Method
ClothoV1
ClothoV2
AudioCaps
R@1
R@5
R@10
R@1
R@5
R@10
R@1
R@5
R@10"
REFERENCES,0.8641025641025641,"Oncescu et al. [67]
9.6
-
40.1
-
-
-
25.1
-
73.2
Nagrani et al. [94]
12.6
-
45.4
-
-
-
35.5
-
84.5
LAION [16]
-
-
-
16.1
38.3
51.1
36.1
71.8
83.9
CNN14-BERT [18]
-
-
-
21.5
47.9
61.9
35.1
70.0
82.1
HTSAT-BERT [18]
-
-
19.7
45.7
59.4
42.2
76.5
87.1
VALOR-B [7]
17.5
42.7
55.3
-
-
-
40.1
73.9
83.1
ONE-PIECE [95]
-
-
-
22.4
49.0
62.7
42.5
77.5
88.4
VAST
25.1
51.5
64.0
26.9
53.2
66.1
52.0
76.8
82.9"
REFERENCES,0.8666666666666667,"Table 16: Performance comparison on Audio Captioning benchmarks. BLEU@4 (B@4) and CIDEr
(C) metrics are used as evaluation metrics."
REFERENCES,0.8692307692307693,"Method
ClothoV1
ClothoV2
AudioCaps
B@4
M
R
C
B@4
M
R
C
B@4
M
R
C"
REFERENCES,0.8717948717948718,"Xu et al. [96]
15.9
16.9
36.8
37.7
-
-
-
-
23.1
22.9
46.7
66.0
CNN14-BART [18]
-
-
-
-
18.0
18.5
40.0
48.8
27.2
24.7
49.9
75.6
HTSAT-BART [18]
-
-
-
-
16.8
18.4
38.3
46.2
28.3
25.0
50.7
78.7
VALOR-B [7]
16.2
17.4
38.2
42.3
-
-
-
-
27.0
23.1
49.4
74.1
VAST
18.5
18.9
39.9
50.7
19.0
19.3
40.8
51.9
29.5
24.7
50.9
78.1"
REFERENCES,0.8743589743589744,"Text-to-Audio Retrieval and Audio Captioning. As shown in Table 15, VAST have largely
improved previous SOTA methods on three text-to-audio retrieval benchmarks, by 7.6, 5.4 and 9.8
R@1 points, respectively. and for audio captioning task, VAST achieves new SOTA performances
on Clotho benchmark (both V1 and V2), and comparable performance on AudioCaps benchmark
to WavCaps [18]. It is noted that WavCaps explored four model architectures with different audio
encoder and text encoders targeting at different benchmarks, while VAST takes a unified architecture
without targeted optimizations for specific downstream benchmarks."
REFERENCES,0.8769230769230769,"Image-Text Benchmarks. We evaluate VAST on text-to-image retrieval, image captioning and image
QA benchmarks. The results are presented in Table 17, from which we can find that even though
VAST is designed as a omni-modality video-language understanding and generation model, it also
shows strong capabilities on image-text benchmarks, demonstrating its generalization capabilities
towards tasks of various modality types. Specifically, VAST achieves new SOTA performance on R@1"
REFERENCES,0.8794871794871795,"Table 17: Performance comparison on Image-Text downstream tasks. CIDEr (C) and SPICE (S)
metrics are reported for captioning. On MSCOCO caption benchmark, we follow SOTA methods [19;
7; 48] employing SCST finetuning [41], and corresponding results are marked with ‘*’."
REFERENCES,0.882051282051282,"MSCOCO-Ret
Flickr30K-Ret
MSCOCO-Cap
VQAv2
Method
Sample
R@1
R@5
R@10
R@1
R@5
R@10
C
S
dev
std"
REFERENCES,0.8846153846153846,"ALBEF [38]
14M
60.7
84.3
90.5
85.6
97.5
98.9
-
-
75.84
76.04
OFA [48]
18M
-
-
-
-
-
-
154.9*
26.6*
82.0
82.0
BEiT-3 [47]
21M
67.2
87.7
92.8
90.3
98.7
99.5
147.6
25.4
84.19
84.03
BLIP [28]
129M
65.1
86.3
91.8
87.6
97.7
99.0
136.7
-
78.25
78.32
BLIP-2 [46]
129M
68.3
87.7
92.6
-
-
-
145.8
-
82.19
82.30
mPLUG-2 [77]
417M
65.7
87.1
92.6
88.1
97.6
99 .1
137.7
23.7
81.11
81.13
VALOR-L [7]
433.5M
61.4
84.4
90.9
-
-
-
152.5*
25.7*
78.46
78.62
Florence [86]
900M
63.2
85.7
-
87.9
98.1
-
-
-
80.16
80.36
PaLI [50]
1.6B
-
-
-
-
-
-
149.1
-
84.3
84.3
GIT [19]
1.7B
-
-
-
-
-
-
151.1*
26.3*
78.6
78.8
SimVLM [97]
1.8B
-
-
-
-
-
-
143.3
25.4
80.03
80.34
ALIGN [98]
1.8B
59.9
83.3
89.8
84.9
97.4
98.6
-
-
-
-
Flamingo (80B) [92]
2.3B
-
-
-
-
-
-
138.1
-
82.0
82.1
CoCa(2.1B) [99]
4.8B
-
-
-
-
-
-
143.6
24.7
82.3
82.3
GIT2(5.1B) [19]
12.9B
-
-
-
-
-
-
152.7*
26.4*
81.7
81.9
VAST
442M
68.0
87.7
92.8
91.0
98.5
99.5
149.0*
27.0*
80.23
80.19"
REFERENCES,0.8871794871794871,"score of Flicker30K and R@5, R@10 scores of MSCOCO dataset, which outperforms image-text
pretrained foundation models such as BLIP-2 [46] and BEiT-3 [47]. On COCO caption benchmark,
VAST achieves 27.0 SPICE score which outperforms all previous methods such as OFA [48] and
GIT2 [19]. On image QA benchmark, VAST achieves better performance than GIT [19], which is
also a generative methods and predicts answers in a fully open way without any constraints."
REFERENCES,0.8897435897435897,"B
More about VAST-27M Dataset"
REFERENCES,0.8923076923076924,"B.1
Word cloud distribution"
REFERENCES,0.8948717948717949,Audio Caption
REFERENCES,0.8974358974358975,"Omni-modality Caption
Subtitle"
REFERENCES,0.9,Vision Caption
REFERENCES,0.9025641025641026,"Figure 3: Word cloud map (Top-200) for vision, audio, omni-modality captions and raw subtitles of
VAST-27M."
REFERENCES,0.9051282051282051,"B.2
Prompts for Omni-Modality Caption Generation"
REFERENCES,0.9076923076923077,"1. A person surfing in the ocean in front of the waves.
2. A man is surfing in the middle of the ocean and it
starts to wave in the ocean。
3. A surfer on a surfboard rides in a wave, then jumps up
and starts surfing."
REFERENCES,0.9102564102564102,"""She's in second place on this left opens up with a
little snap, gets back into the lip to finish it off, rides
through the explosive closeout and look into better."""
REFERENCES,0.9128205128205128,"A surfer rides a wave, jumping up and surfing in the
ocean with waves and men speaking in the
background, as they compete for the top spot,
with a close-up of the surfer finishing the ride and
looking ahead."
REFERENCES,0.9153846153846154,"1. The ocean waves and men speaking are heard.  
2. Someone is playing a surf - on.
3. Sea waves and people talking can be heard."
REFERENCES,0.9179487179487179,"""Human: About one video, here are 3 vision captions: {}, 3
audio captions: {}, and one speech subtitle: {}. You need to
understand and encode them into 1 sentence. Vision and
audio captions maybe redundant, summarize them before
encode with speech subtitle. Do not simply concatenate them
together. The weights of video/audio/speech are equaled. 
Assistant: The sentence is """
REFERENCES,0.9205128205128205,"A person surfing in the ocean in front of the waves with
the sound of the ocean waves and men speaking, a man is
surfing in the middle of the ocean with the sound of
someone playing a surfboard, and a surfer on a surfboard
rides in a wave, then jumps up and starts surfing with the
sound of sea waves and people talking ..."
REFERENCES,0.9230769230769231,"""Human: About one video, here are 3 vision captions: {}, 3
audio captions: {}, and one speech subtitle: {}. You need to
understand and encode them into 1 sentence. Vision and
audio captions maybe redundant, summarize them before
encode with speech subtitle. Do not simply concatenate them
together. The weights of video/audio/speech are equaled. 
Assistant: The sentence is """
REFERENCES,0.9256410256410257,"A surfer rides a wave, performing a snap and finishing it
off, before navigating through an explosive closeout and
looking for a better spot."
REFERENCES,0.9282051282051282,"""Human: About one video, here are 3 vision captions: {}, 3
audio captions: {}, and one speech subtitle: {}. You need to
understand and encode them into 1 sentence. Vision and
audio captions maybe redundant, summarize them before
encode with speech subtitle. Do not simply concatenate them
together. The weights of video/audio/speech are equaled. 
Assistant: The sentence is """
REFERENCES,0.9307692307692308,Omni-Modality Captioner
REFERENCES,0.9333333333333333,(Vicuna-13b)
REFERENCES,0.9358974358974359,Omni-Modality Captioner
REFERENCES,0.9384615384615385,(Vicuna-13b)
REFERENCES,0.941025641025641,Omni-Modality Captioner
REFERENCES,0.9435897435897436,(Vicuna-13b)
REFERENCES,0.9461538461538461,Single Modality Captions
REFERENCES,0.9487179487179487,Onmi-Modality Caption Generation with Different Instructional Prompts
REFERENCES,0.9512820512820512,"Figure 4: Ablation study for instructional prompt used for omni-modality video caption generation in
VAST-27M."
REFERENCES,0.9538461538461539,"B.3
More Examples"
REFERENCES,0.9564102564102565,"1. a young boy riding a skateboard at a skate park.
2. a man is skateboarding.
3. a person riding a skateboard on a skate park course."
REFERENCES,0.958974358974359,"This is to use your hands once you swing it above through
your legs. Just grab just grab your nose a little bit and help
get it in there."
REFERENCES,0.9615384615384616,"A young boy rides
a skateboard at a
skate park, while
talking about
skateboarding
moves and
explaining how to
do a trick with
your hands and
grab your nose to
help get it in
there."
REFERENCES,0.9641025641025641,"1. skateboarder is explaining skateboarding moves.
2. skateboarder talking about skateboard.
3. skateboarders are skating."
REFERENCES,0.9666666666666667,"1. cartoon characters are getting ready to go with a suitcase.
2. a group of piggy characters are in their house with a
suitcase.
3. a group of cartoon characters have their suitcase open in a
bedroom."
REFERENCES,0.9692307692307692,"Are you sure we need it all? Yes, its all very important."
REFERENCES,0.9717948717948718,"A group of cartoon
characters,
including piggy
ones, are preparing
to go on a trip, with
one character
checking if they
need to bring
everything and
another character
confirming the
importance of all
the items."
REFERENCES,0.9743589743589743,"1. a cartoon character is speaking.
2. an animated character is talking.
3. someone is making a conversation in a cartoon."
REFERENCES,0.9769230769230769,"1. a car driver is driving down a high hill on the road.
2. a man in sunglasses is talking while driving a car on a road.
3. a man is driving down the road."
REFERENCES,0.9794871794871794,"Milk is readily available in market nowadays. So you can
get that condensed Milk. so here I put the condensed
milk in a big bowl."
REFERENCES,0.982051282051282,"A man in
sunglasses is
talking while
driving a car with
a high speed
engine sound in
the background,
on a road, with
the desire to have
unlimited fuel and
just drive off into
the sunset
forever."
A MAN IS SPEAKING ABOUT DRIVING A CAR WITH A HIGH,0.9846153846153847,"1. a man is speaking about driving a car with a high
speed engine sound in the background.
2. a person is speaking.
3. a car engine is heard."
A MAN IS SPEAKING ABOUT DRIVING A CAR WITH A HIGH,0.9871794871794872,"1. a woman is pouring a liquid into a large glass bowl.
2. a person is putting yellow cheese in a bowl.
3. the person is pouring butter into a bowl."
A MAN IS SPEAKING ABOUT DRIVING A CAR WITH A HIGH,0.9897435897435898,"Milk is readily available in market nowadays. So you can get
that condensed Milk. so here I put the condensed milk in a
big bowl."
A MAN IS SPEAKING ABOUT DRIVING A CAR WITH A HIGH,0.9923076923076923,"A woman is
introducing and
showing her
ingredients,
including
condensed milk,
which is readily
available in the
market, and is
putting it in a big
bowl."
A MAN IS SPEAKING ABOUT DRIVING A CAR WITH A HIGH,0.9948717948717949,"1. a woman is introducing and showing her ingredients.
2. a woman is introducing her favorite foods.
3. a person is cooking."
A MAN IS SPEAKING ABOUT DRIVING A CAR WITH A HIGH,0.9974358974358974,Figure 5: More samples in VAST-27M.
