Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0072992700729927005,"Due to the representation limitation of the joint Q value function, multi-agent reinforcement
learning (MARL) methods with linear or monotonic value decomposition suffer from the
relative overgeneralization. As a result, they can not ensure the optimal coordination,
leading to instability and poor performance. Existing methods address the relative overgen-
eralization by achieving complete expressiveness or learning a bias, which are insufﬁcient
to solve the problem. In this paper, we propose the optimal consistency, a criterion to
evaluate the optimality of coordination. To achieve the optimal consistency, we introduce
the True-Global-Max (TGM) condition for linear and monotonic value decomposition,
where the TGM condition can be ensured when the optimal stable point is the unique stable
point. Therefore, we propose the greedy-based value representation (GVR) to ensure the
optimal stable point via inferior target shaping and eliminate the non-optimal stable points
via superior experience replay. We conduct experiments on various benchmarks, where
GVR signiﬁcantly outperforms state-of-the-art baselines. Experiment results demonstrate
that our method can ensure the optimal consistency under sufﬁcient exploration."
INTRODUCTION,0.014598540145985401,"1
INTRODUCTION"
INTRODUCTION,0.021897810218978103,"By taking advantage of the deep learning technique, cooperative multi-agent reinforcement learning (MARL)
shows great scalability and excellent performance on challenging tasks (Vorotnikov et al., 2018; Wu et al.,
2020) such as StarCraft unit micromanagement (Foerster et al., 2018). As an efﬁcient paradigm of cooperative
MARL, centralized training with decentralized execution (CTDE) (Oliehoek et al., 2008; Foerster et al., 2016;
Lowe et al., 2017) gains growing attention. A simple and effective approach to adopt CTDE in value-based
cooperative MARL is linear value decomposition (LVD) or monotonic value decomposition (MVD). However,
both LVD and MVD suffer from relative overgeneralization (Panait et al., 2006; Wei et al., 2018) due to
the representation limitation of the joint Q value function. As a result, they can not guarantee optimal
coordination."
INTRODUCTION,0.029197080291970802,"Recent works address the problem from two different perspectives. The ﬁrst kind of method aims to solve
the representation limitation directly through value functions with complete expressiveness capacity (e.g.,
QTRAN (Son et al., 2019) and QPLEX (Wang et al., 2020)). However, learning the complete expressiveness
is impractical in complicated MARL tasks because the joint action space increases exponentially with the
number of agents. The other kind of method tries to overcome relative overgeneralization by learning a
bias (e.g., WQMIX (Rashid et al., 2020) and MAVEN (Mahajan et al., 2019)), which lacks theoretical and
quantitative analysis of the problem and is only applicable in speciﬁc tasks. As a result, these methods
are insufﬁcient to guarantee optimal coordination. More discussions about related works are provided in
Appendix A."
INTRODUCTION,0.0364963503649635,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.043795620437956206,"Value decomposition is a popular approach to assign credit for individual agents in fully cooperative MARL
tasks, where the main concern is the optimality of coordination. In a successful case of credit assignment
via value decomposition, individual agents act according to their local policies and achieve the best team’s
performance. To evaluate the value decomposition, we propose the optimal consistency, a criterion con-
cerning the optimality of coordination. The optimal consistency can be decomposed into two conditions:
Individual-Global-Max (IGM) and True-Global-Max (TGM)."
INTRODUCTION,0.051094890510948905,"In this paper, to achieve the optimal consistency efﬁciently, we investigate the requirements of the TGM
condition and go deep into the mechanism of the value representation for LVD and MVD, where the IGM
condition always holds. We ﬁrst derive the expression of the joint Q value function of LVD and MVD, by
which we draw some interesting conclusions. Firstly, LVD and MVD share the same expression of the
joint Q value function. Secondly, the correspondence between the joint greedy action and the maximal Q
true value (i.e., the team’s best performance) depends heavily on the task-speciﬁc reward function for LVD
and MVD. Thirdly, there may be multiple stable points of the joint greedy action. As a result, the joint
policy may converge to different results. More importantly, in some of the stable points, the true Q value
of the joint greedy action is not maximal, which is the root cause of non-optimal coordination and relative
overgeneralization. To ensure the TGM condition for LVD and MVD, the stable point satisfying the TGM
condition (we call it the optimal stable point) is required to be the unique stable point, which is the target
problem to be solved in this paper."
INTRODUCTION,0.058394160583941604,"To solve the target problem, we propose the greedy-based value representation (GVR). According to previous
conclusions, the stable points are task-speciﬁc due to their dependency on the reward function, for which we
propose the inferior target shaping (ITS). ITS dynamically modiﬁes the true Q value of inferior samples (i.e.,
the samples worse than the current greedy) according to current greedy Q value, which is theoretically proved
to stabilize the optimal point under any reward function. Besides, under ITS, the stability of a non-optimal
point depends only on the probability ratio of superior samples (i.e., the samples better than the current
greedy) to the non-optimal sample, where the non-optimal stable points can be eliminated under a large
enough ratio (Eq.7). We prove two simple ways applied by previous works (i.e. applying weight on the
superior samples (Rashid et al., 2020) and improving exploration (Mahajan et al., 2019)) are both inapplicable
to raise the ratio because the probability of superior samples decreases exponentially with the number of
agents. Therefore, we further propose the superior experience replay (SER), which achieves almost constant
probability of superior samples by saving them in a superior buffer. SER is theoretically proved to eliminate
the non-optimal stable points under ITS."
INTRODUCTION,0.06569343065693431,"We have three contributions in this work. (1) This is the ﬁrst work to derive the exact expression of the joint
Q value function for LVD and MVD. (2) We point out the root cause of non-optimal coordination and further
propose the target problem to be solved for LVD and MVD. (3) We propose the GVR method, which is proved
theoretically to ensure the optimal consistency under sufﬁcient exploration, and our method outperforms
state-of-the-art baselines in various benchmarks."
PRELIMINARIES,0.072992700729927,"2
PRELIMINARIES"
DEC-POMDP,0.08029197080291971,"2.1
DEC-POMDP"
DEC-POMDP,0.08759124087591241,"We model a fully cooperative multi-agent task as a decentralized partially observable Markov decision process
(Dec-POMDP) described by a tuple G =< S, U, P, r, Z, O, n, γ > (Guestrin et al., 2001; Oliehoek & Amato,
2016). s ∈S denotes the true state of the environment. At each time step, each agent a ∈A ≡{1, 2, · · · , n}
receives a local observation za ∈Z produced by the observation function O : S × A →Z, and then
chooses an individual action ua ∈U according to a local policy πa(ua|τ a) : T × U →[0, 1], where
τ a ∈T ≡(Z × U)∗denotes the local action-observation history. The joint action of n agents u results in a
shared reward r(s, u) and a transition to the next state s′ ∼P(·|s, u). γ ∈[0, 1) is a discount factor."
DEC-POMDP,0.0948905109489051,Under review as a conference paper at ICLR 2022
DEC-POMDP,0.10218978102189781,"We denote the joint variable of group agents with bold symbols, e.g., the joint action u ∈U ≡U n, the
joint action-observation history τ ∈T ≡T n, and the joint interactive policy (i.e., the policy interacts with
environment to generate trajectories) π(u|τ). The true Q value of π(ut|τt) is denoted by Qπ(st, ut) =
Est+1:∞,ut+1:∞[Rt|st, ut], where Rt = P∞
i=0 γirt+1 is the discounted return. The action-state value function
of agent a and the group of agents are deﬁned as utility function Ua(ua, τ a) and joint Q value function
Q(u, τ) respectively. The true Q value is the target of the joint Q value in training, serving as the unique
external criterion of the team’s performance. The greedy action u∗:= argmaxuQ(u, τ) is deﬁned as the
joint action with the maximal joint Q value . The optimal action uopt := argmaxuQ(s, u) is deﬁned as
the joint action with the best team’s performance. For brevity, we sometimes omit the preﬁx ”joint” for the
joint variables."
OPTIMAL CONSISTENCY AND TGM CONDITION,0.10948905109489052,"2.2
OPTIMAL CONSISTENCY AND TGM CONDITION"
OPTIMAL CONSISTENCY AND TGM CONDITION,0.11678832116788321,"In CTDE paradigm, agents are expected to act individually according to their local policies (i.e., the individual
greedy actions) while achieve the optimal coordination (i.e., the maximal true Q value). Here we deﬁne
the correspondence between the individual greedy actions and the maximal true Q value as the optimal
consistency."
OPTIMAL CONSISTENCY AND TGM CONDITION,0.12408759124087591,"Deﬁnition 1 (Optimal consistency). Given a set of utility functions {U1(u1, τ 1)), · · · , Un(un, τ n)}, and
the true Q value Q(s, u), if the following holds"
OPTIMAL CONSISTENCY AND TGM CONDITION,0.13138686131386862,"{argmax
u1
U1(u1, τ 1), · · · , argmax
un
Un(un, τ n)} = argmax
u
Q(s, u)
(1)"
OPTIMAL CONSISTENCY AND TGM CONDITION,0.1386861313868613,"then we say the set of utility functions {U1(u1, τ 1)), · · · , Un(un, τ n)} satisﬁes the optimal consistency. For
simplicity, we ignore situations with non-unique optimal actions."
OPTIMAL CONSISTENCY AND TGM CONDITION,0.145985401459854,"The optimal consistency can be decomposed into two conditions: Individual-Global-Max (IGM) and
True-Global-Max (TGM). The IGM condition proposed by QTRAN (Son et al., 2019) is deﬁned
on the correspondence between individual greedy actions and the joint greedy actions (formally,
{argmaxu1 U1(u1, τ 1), · · · , argmaxun Un(on, τ n)} = argmaxuQ(u, τ)). To achieve the optimal con-
sistency, the correspondence between the joint greedy action and the maximal true Q value is required, for
which we deﬁne the TGM condition:"
OPTIMAL CONSISTENCY AND TGM CONDITION,0.15328467153284672,"Deﬁnition 2 (TGM). Given a joint value function Q(u, τ), and the true Q value Q(s, u), if the following
holds"
OPTIMAL CONSISTENCY AND TGM CONDITION,0.16058394160583941,"argmax
u
Q(u, τ) = argmax
u
Q(s, u)
(2)"
OPTIMAL CONSISTENCY AND TGM CONDITION,0.1678832116788321,"then we say the joint value function Q(u, τ) satisﬁes the TGM condition. For simplicity, we ignore situations
with non-unique optimal actions."
INVESTIGATION OF THE TGM CONDITION FOR LVD & MVD,0.17518248175182483,"3
INVESTIGATION OF THE TGM CONDITION FOR LVD & MVD"
INVESTIGATION OF THE TGM CONDITION FOR LVD & MVD,0.18248175182481752,"Linear value decomposition (LVD) and monotonic value decomposition (MVD) are simple and naturally meet
the IGM condition. To achieve the optimal consistency, we investigate the requirements of the TGM condition
for LVD and MVD. According to Def.2, the TGM condition is related to the joint Q value function Q(u, τ).
In this section, we ﬁrst derive the expression of Q(u, τ) for LVD and MVD under ϵ−greedy visitation. The
expression indicates there may be non-optimal stable points that violate the TGM condition, which is the root
cause of non-optimal coordination."
INVESTIGATION OF THE TGM CONDITION FOR LVD & MVD,0.1897810218978102,Under review as a conference paper at ICLR 2022
EXPRESSION OF THE JOINT Q VALUE FUNCTION FOR LVD & MVD,0.19708029197080293,"3.1
EXPRESSION OF THE JOINT Q VALUE FUNCTION FOR LVD & MVD"
EXPRESSION OF THE JOINT Q VALUE FUNCTION FOR LVD & MVD,0.20437956204379562,"Firstly, take two-agent linear value decomposition as an example, where the joint Q value function
Q(u1
i , u2
j, τ) is linearly factorized into two utility functions Q(u1
i , u2
j, τ) = U1(u1
i , τ 1) + U2(u2
j, τ 2).
u1
i , u2
j ∈{u1, · · · , um} denote the individual actions of agent 1,2 respectively, where {u1, · · · , um} is
the discrete individual action space. Specially, we denote the individual greedy action of agent 1,2 with
u1
i∗, u2
j∗respectively. For brevity, Q(u1
i , u2
j, τ) and Ua(ua
i , τ a) are represented by Qij and Ua
i (a ∈{1, 2})
respectively. Through the derivation provided in Appendix B.1, Qij can be represented by the true Q values
as"
EXPRESSION OF THE JOINT Q VALUE FUNCTION FOR LVD & MVD,0.2116788321167883,"Qij = ϵ m m
X"
EXPRESSION OF THE JOINT Q VALUE FUNCTION FOR LVD & MVD,0.21897810218978103,"k=1
(Qik + Qkj) + (1 −ϵ)(Qi∗j + Qij∗) −ϵ2 m2 m
X i=1 m
X"
EXPRESSION OF THE JOINT Q VALUE FUNCTION FOR LVD & MVD,0.22627737226277372,"j=1
Qij"
EXPRESSION OF THE JOINT Q VALUE FUNCTION FOR LVD & MVD,0.23357664233576642,"−ϵ(1 −ϵ) m m
X"
EXPRESSION OF THE JOINT Q VALUE FUNCTION FOR LVD & MVD,0.24087591240875914,"k=1
(Qi∗k + Qkj∗) −(1 −ϵ)2Qi∗j∗ (3)"
EXPRESSION OF THE JOINT Q VALUE FUNCTION FOR LVD & MVD,0.24817518248175183,"Veriﬁcation of the expression is provided in Appendix B.2. For monotonic value decomposition, the expression
is identical to Eq.3 (the proof is provided in Appendix C), which indicates the coefﬁcients and bias on utility
functions do not affect the joint Q value function. For situations with more than two agents, by referring to
the derivation in Appendix B.1 and C, the expression of joint Q values can also be obtained."
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.25547445255474455,"3.2
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD"
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.26277372262773724,"According to Eq.3, the joint Q values change with the greedy action {u1
i∗, u2
j∗}. An example of the joint
Q values under 3 different {u1
i∗, u2
j∗} is shown in Tab.1(b,c,d), where ϵ = 0.2."
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.27007299270072993,"8(optimal)
-12
-12
-12
0
0
-12
0
6
(a) The given Qij."
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.2773722627737226,"-19.90
-10.04
-9.64
-10.04
-0.17
0.23
-9.64
0.23
0.63"
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.2846715328467153,"(c) Qij under {u1
i∗, u2
j∗} = {1, 1}."
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.291970802919708,"7.40
-8.33
-7.93
-8.33
-24.06
-23.66
-7.93
-23.66
-23.26"
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.29927007299270075,"(b) Qij under {u1
i∗, u2
j∗} = {0, 0}."
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.30656934306569344,"-24.38
-14.52
-9.32
-14.52
-4.65
0.55
-9.32
0.5
5.75"
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.31386861313868614,"(d) Qij under {u1
i∗, u2
j∗} = {2, 2}."
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.32116788321167883,"Table 1: (a) The given true Q values Qij(u1
i , u2
j ∈{0, 1, 2}). (b,c,d) Calculation results of the joint Q values
Qij with LVD or MVD under different greedy actions {u1
i∗, u2
j∗}, where {u1
i∗, u2
j∗} is marked with a pink
background. We denote the maximal joint Q values with numbers in bold. Due to the representation limitation,
Qij ̸= Qij. Instead, Qij overgeneralizes to Qij."
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.3284671532846715,"Notice that the example in Tab.1(c) is an unstable point, because {u1
i∗, u2
j∗} = {1, 1} ̸= argmax Qij =
{2, 2}. To explain this, assume {u1
i∗, u2
j∗} and Qij of iteration t are {u1
i∗, u2
j∗}t and Qij,t respectively,
where {u1
i∗, u2
j∗}t ({u1
i∗, u2
j∗} in Tab.1(c)) = argmax Qij,t = {1, 1}. After training, {u1
i∗, u2
j∗}t+1 =
argmax Qij,t+1 (Qij in Tab.1(c)) = {2, 2}, which means {u1
i∗, u2
j∗}t changes with iteration. Since
Qij,t+1 is related to {u1
i∗, u2
j∗}t, both {u1
i∗, u2
j∗}t and Qij,t+1 are unstable. As a result, Tab.1(c) ﬁnally
converges to the stable point in Tab.1(d). Here we deﬁne the stable point of LVD and MVD."
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.3357664233576642,"Deﬁnition 3 (Stable point of LVD and MVD). Given the joint Q value function Q(u, τ) which has con-
verged under LVD (or MVD) with the target Q(s, u), where Q(s, u) is the true Q value, and given an"
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.34306569343065696,Under review as a conference paper at ICLR 2022
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.35036496350364965,"interactive policy π(u, τ) with the greedy action u∗(u∗= argmax π(s, u)), if the following holds
argmax
u
Q(u, τ) = u∗
(4)"
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.35766423357664234,"then we say Q(u, τ) and u∗are stable, and this is a stable point of LVD (or MVD)."
REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD,0.36496350364963503,"According to Def.3, the examples in Tab.1(b) (argmax Qij = {u1
i∗, u2
j∗} = {0, 0}) and Tab.1(d)
(argmax Qij = {u1
i∗, u2
j∗} = {2, 2}) are both stable points (i.e., the joint Q values and the greedy ac-
tion may converge to Tab.1(b) or Tab.1(d)). However, the stable points in Tab.1(d) violates the TGM
condition (i.e., argmax Qij ̸= argmax Qij), which is the root cause of non-optimal coordination. Tab.1(b)
and Tab.1(d) are deﬁned as non-optimal stable point and optimal stable point respectively. To ensure the
TGM condition, the optimal stable point is required to be the unique stable point. More discussion about the
stable points is provided in appendix H."
METHOD,0.3722627737226277,"4
METHOD"
METHOD,0.3795620437956204,"In this section, we introduce the greedy-based value representation (GVR). We ﬁrst propose inferior target
shaping (ITS) to ensure the optimal stable point under any reward function. Besides, under ITS, the stability of
a non-optimal point depends only on the probability ratio of superior samples to the non-optimal sample. To
eliminate the non-optimal stable points, we then investigate two simple methods (i.e., improving exploration
and applying a weight on the superior samples) which raise the ratio. However, both methods are proved
inapplicable due to the extremely small probability of superior samples under large joint action space. Finally,
we describe superior experience replay, which achieves an almost constant proportion of superior samples
through a superior buffer."
INFERIOR TARGET SHAPING,0.38686131386861317,"4.1
INFERIOR TARGET SHAPING"
INFERIOR TARGET SHAPING,0.39416058394160586,"According to Eq.3, the joint Q value of any action is related to the true Q values of the whole joint action
space for LVD and MVD, which indicates the stable points depend heavily on the reward function (proofs are
provided in Appendix H). Firstly, we consider to remove the dependency. Because the exact true Q values of
non-optimal samples are uninformative, we only represent the ”non-optimality” of these samples. Here we
propose the ITS target Qits(s, u)"
INFERIOR TARGET SHAPING,0.40145985401459855,"Qits(s, u) =

Q(u∗, τ) −α|Q(u∗, τ)|
Q(s, u) < Q(u∗, τ) and u ̸= u∗
Q(s, u)
others
(5)"
INFERIOR TARGET SHAPING,0.40875912408759124,"where u∗is the joint greedy action and α ∈(0, 1]. A large enough α prevents the confusion between greedy
and inferior samples. The samples in the ﬁrst case are called inferior samples. Besides, the action samples u
which satisfy Q(s, u) > Q(s, u∗) are called superior samples. ITS target only remains the information of
non-optimality for inferior samples, which eases representation. Under ITS, given the greedy action u∗and
any action us(us ̸= u∗), assuming Q(s, u∗) > 0, we have
∆Q(us, τ) = Q(us, τ) −Q(u∗, τ) = n(η1 −η2) [Q(s, u∗) −(1 −α)Q(u∗, τ)] + nη1eQQ(s, u∗) (6)"
INFERIOR TARGET SHAPING,0.41605839416058393,"where eQ = Q(s,us)−Q(s,u∗)"
INFERIOR TARGET SHAPING,0.4233576642335766,"Q(s,u∗)
,η1 = ( ϵ"
INFERIOR TARGET SHAPING,0.4306569343065693,m)n−1 and η2 = (1 −ϵ + ϵ
INFERIOR TARGET SHAPING,0.43795620437956206,"m)n−1. We provide two different versions
of proof for Eq.6 in Appendix D, and the calculation result is veriﬁed in the experimental part (Fig.1). It is
proved in Appendix D.1 that there is always an optimal stable point under ITS."
INFERIOR TARGET SHAPING,0.44525547445255476,"When ∆Q(us, τ) > 0, argmaxuQ(u, τ) ̸= u∗, which suggests current greedy action is unstable. If u∗is a
non-optimal action, to destabilize it, let ∆Q(us, τ) > 0 and assume Q(u∗, τ) ≈Q(s, u∗) (this assumption
is quite accurate, as veriﬁed in Appendix E.2), we have
η1
η2
>
α
α + eQ
(7)"
INFERIOR TARGET SHAPING,0.45255474452554745,Under review as a conference paper at ICLR 2022
INFERIOR TARGET SHAPING,0.45985401459854014,which indicates the non-optimal stable points can be eliminated by raising η1
INFERIOR TARGET SHAPING,0.46715328467153283,"η2 under ITS. A simple way to
raise η1"
INFERIOR TARGET SHAPING,0.4744525547445255,"η2 is improving exploration. Substituting the expression of η1 and η2 into Eq.7, we have ϵ >
m ( eQ"
INFERIOR TARGET SHAPING,0.48175182481751827,"α )
1
n−1 + 1 + m −1
(8)"
INFERIOR TARGET SHAPING,0.48905109489051096,"When us = argmaxuQ(s, u), we obtain the lower bound of ϵ (denoted by ϵ0) from the right side of Eq.8.
However, as the number of agents n and the size of individual action spaces m increases, ϵ0 grows close to 1
(as veriﬁed in Fig.1(b)), which is inapplicable in tasks with long episodes."
INFERIOR TARGET SHAPING,0.49635036496350365,Another way to raise η1
INFERIOR TARGET SHAPING,0.5036496350364964,η2 is increasing the relative weights of the superior samples us. It is proved in
INFERIOR TARGET SHAPING,0.5109489051094891,Appendix E that the non-optimal stable points can be eliminated by applying a weight w(w > α(η2−η1)
INFERIOR TARGET SHAPING,0.5182481751824818,"eQη1
) to
the superior samples under ITS. When us = argmaxuQ(s, u), we obtain the lower bound of w (denoted
by w0). However, the w0 grows exponentially as the number of agents n increases (as veriﬁed in Appendix
E.2, w0 = 659.50 when n = 4), which introduces instability in training."
INFERIOR TARGET SHAPING,0.5255474452554745,Since η1
INFERIOR TARGET SHAPING,0.5328467153284672,"η2 decreases rapidly as the joint action space grows, the essence to raise η1"
INFERIOR TARGET SHAPING,0.5401459854014599,"η2 is increasing the proportion
of superior samples and freeing the raised proportion from the dependency on the joint action space
size, for which we introduce the superior experience replay."
SUPERIOR EXPERIENCE REPLAY,0.5474452554744526,"4.2
SUPERIOR EXPERIENCE REPLAY"
SUPERIOR EXPERIENCE REPLAY,0.5547445255474452,"To distinguish superior samples, we ﬁrst introduce a critic V (s), which approximates the true Q value of the
joint greedy action. The target of the critic is deﬁned as"
SUPERIOR EXPERIENCE REPLAY,0.5620437956204379,"Vgvr(s) =

Q(s, u)
V (s) < Q(s, u) or u = u∗
V (s)
others
(9)"
SUPERIOR EXPERIENCE REPLAY,0.5693430656934306,"Inspired by prioritized experience replay (Schaul et al., 2015; Zhang & Sutton, 2017), we introduce a
superior buffer where the episodic trajectories are stored. The number of superior samples which satisfy
Q(s, u) > V (s) and u ̸= u∗within a trajectory is deﬁned as the priority of the trajectory. In the superior
buffer, trajectories are ranked according to their priorities. The training batch consists of two parts: trajectories
randomly sampled from the replay buffer and the top k trajectories from the superior buffer. At the end of
each episode, the trajectories sampled from the superior buffer will be put back after the update of their
priorities. The working principle of GVR and the algorithm is given in Appendix G."
SUPERIOR EXPERIENCE REPLAY,0.5766423357664233,"With SER, the superior samples are consist of two part: sampled from the superior buffer with a small
probability and sampled from the superior buffer with a constant probability. As a result, the proportion of
superior sample is mainly determined by the relative weight on samples from superior buffer, which is a
constant and irrelevant to the joint action space size. It is proved in Appendix F that SER can eliminate the
non-optimal stable points under ITS."
EXPERIMENTS,0.583941605839416,"5
EXPERIMENTS"
EXPERIMENTS,0.5912408759124088,"In this section, ﬁrstly we verify the attributes of stable points for linear value decomposition (LVD) in matrix
games, where we also evaluate the improvements of GVR. Secondly, to evaluate the stability and scalability
of our method, we test the performance of GVR in predator-prey tasks with extreme reward shaping. Thirdly,
we conduct experiments on challenging tasks of StarCraft multi-agent challenge (SMAC) (Samvelyan et al.,
2019). Finally, we design ablation studies to investigate the improvement of GVR. Our method is compared
with state-of-the-art baselines including QMIX (Rashid et al., 2018), QPLEX (Wang et al., 2020), and
WQMIX (Rashid et al., 2020). All results are evaluated over 5 seeds. Additional experiments are provided in
Appendix I."
EXPERIMENTS,0.5985401459854015,Under review as a conference paper at ICLR 2022
ONE-STEP MATRIX GAME,0.6058394160583942,"5.1
ONE-STEP MATRIX GAME"
ONE-STEP MATRIX GAME,0.6131386861313869,"Matrix game is a simple fully cooperative multi-agent task, where the shared reward is deﬁned by a payoff
matrix. In one-step matrix games, the true Q values are directly accessible from the payoff matrix, which is
convenient for the veriﬁcation of the optimal consistency."
ONE-STEP MATRIX GAME,0.6204379562043796,"The veriﬁcation of stable points for LVD. We conduct experiments on two-agent one-step matrix game to
verify the expression of joint Q values (i.e., Eq.3). We also verify the effect of reward function and exploration
rate ϵ on the stable points. The experimental results and conclusions are provided in Appendix G."
ONE-STEP MATRIX GAME,0.6277372262773723,"Evaluation of GVR. Since the stability under ITS is determined by Eq.6, we ﬁrst verify the expression under
a random inferior true Q values (Q). Two payoff matrices of size 34 (i.e., m=3 and n=4) are generated for
VDN and ITS respectively:"
ONE-STEP MATRIX GAME,0.635036496350365,Q(vdn) =
ONE-STEP MATRIX GAME,0.6423357664233577,"( 6(1 + eQ)
u = {0, 0, 0, 0}
6
u = {2, 2, 2, 2}
6(1 −α)
others
Q(its) ="
ONE-STEP MATRIX GAME,0.6496350364963503,"( 6(1 + eQ)
u = {0, 0, 0, 0}
6
u = {2, 2, 2, 2}
random(−20, 6)
others
(10)"
ONE-STEP MATRIX GAME,0.656934306569343,"where eQ = 0.3, α = 0.1. We measure ∆Qopt = Q(0, 0, 0, 0) −Q(2, 2, 2, 2) for VDN and ITS trained
with corresponding matrices, where the greedy action is ﬁxed to u∗= {2, 2, 2, 2}. As shown in Fig.1(a),
the tested ∆Q(uopt, τ) consists with our calculation result (∆Q(uopt, τ) = −1.951) from Eq.6. Besides,
∆Q(uopt, τ) of ITS with random inferior Q equals to that of VDN with ﬁxed inferior Q, which suggests that
under ITS target, the stable points are irrelevant to the return of inferior samples. As a result, ITS largely
frees the stable points from the interference of the reward function."
ONE-STEP MATRIX GAME,0.6642335766423357,"Figure 1: Evaluation of GVR in 4-agent matrix games. (a) Comparison of ∆Qopt between ITS with random
reward shaping and VDN with ﬁxed reward shaping. The red dash line (∆Q(uopt, τ) = −1.951) denotes the
calculation result (from Eq.6) under ITS target. (b) Median test ratio of the optimal stable points as ϵ grows
(i.e., test probability of the optimal convergence), where y = 1 indicates the optimal stable point is the unique
stable point and y = 0 indicates the optimal point is unstable. The red dash line (ϵ0 = 0.837) denotes the
calculated lower bound of ϵ (from Eq.8) when ITS eliminates non-optimal stable points."
ONE-STEP MATRIX GAME,0.6715328467153284,"We also evaluate the ratio of optimal stable points for VDN, ITS and GVR as ϵ grows, where the payoff
matrices are generated according to Q(its) (Eq.10) over 5 seeds. At each value of ϵ, 100 times of independent
training and test are executed. According to Fig.1(b), for VDN, the optimal point become unstable (i.e.,
y = 0) when ϵ > 0.8. For ITS, the optimal points is always stable (i.e., y > 0 (∀ϵ ∈(0, 1])). Besides, ITS
eliminates most of the non-optimal stable points under large exploration (ϵ > 0.837), which consists with
our calculation result (the red dash line) from Eq.8. For GVR, the optimal stable point is the unique stable
point (i.e., the optimal consistency holds) under a wild range of ϵ. However, GVR is unable to ensure the"
ONE-STEP MATRIX GAME,0.6788321167883211,Under review as a conference paper at ICLR 2022
ONE-STEP MATRIX GAME,0.6861313868613139,Figure 2: GVR vs methods with complete expressiveness capacity in matrix games.
ONE-STEP MATRIX GAME,0.6934306569343066,"optimal consistency under a small ϵ, where the optimal sample is not explored in given training steps (e.g., the
probability of the optimal sample uopt = {0, 0, 0, 0} is 1.98e −5 under the greedy action u∗= {2, 2, 2, 2})."
ONE-STEP MATRIX GAME,0.7007299270072993,"GVR vs methods with complete expressiveness capacity. To compare the efﬁciency between GVR and
methods with complete expressiveness capacity, we evaluate GVR, QPLEX and CWQMIX in matrix games
with different scales of action space. Similar to Q(its) in Eq.10, we generate random matrices of size 32,
63, and 124 over 5 seeds, where the ﬁrst and the last element are set to be 8 and 6 respectively. From Fig.2,
in the last two tasks (U n = 63 and U n = 124), the representation errors of CWQMIX and QPLEX do not
decrease during training, which suggests that they are unable to learn the complete expressiveness within
given steps. We do not measure the representation error of GVR because the representation target is modiﬁed
by ITS. GVR is the only method ensuring the optimal coordination (i.e. median test return = 8)."
ONE-STEP MATRIX GAME,0.708029197080292,"Figure 3: Experiment results on predator-prey. (a) Comparison between GVR and baselines. (b) Performance
of GVR with different punishments."
ONE-STEP MATRIX GAME,0.7153284671532847,Under review as a conference paper at ICLR 2022
PREDATOR-PREY,0.7226277372262774,"5.2
PREDATOR-PREY"
PREDATOR-PREY,0.7299270072992701,"In this subsection, we compare GVR with state-of-the-art baselines in the predator-prey tasks (B¨ohmer et al.,
2020). In our experiments, 8 predators are trained to capture 8 preys, where the preys are controlled by
random policies. The reward is shared by all predators, and a punishment is applied to the reward when only
a single agent capture a prey. Our experiments are carried out under 3 punishment values."
PREDATOR-PREY,0.7372262773722628,"From Fig.3, VDN and QMIX fail in all experiments, where all agents tend to avoid the preys. QPLEX also
fails in spite of its complete expressiveness capacity. WQMIX can only solve the task with a small punishment
of -2, while GVR is able to solve the tasks under all punishments."
STARCRAFT MULTI-AGENT CHALLENGE,0.7445255474452555,"5.3
STARCRAFT MULTI-AGENT CHALLENGE"
STARCRAFT MULTI-AGENT CHALLENGE,0.7518248175182481,"StarCraft multi-agent challenge (SMAC) is a popular benchmark for the evaluation of MARL algorithms. We
compare GVR with baselines in various difﬁcult tasks of SMAC. The game version is 69232. Each algorithm
is trained for 2e6 steps in MMM2, 2c vs 64 zg and 6h vs 8z, with ϵ damping from 1 to 0.05 during the ﬁrst
5e4 steps. Especially, in 6h vs 8z, each algorithm is trained for 5e6 steps, with ϵ damping from 1 to 0.05
during the ﬁrst 1e6 steps."
STARCRAFT MULTI-AGENT CHALLENGE,0.7591240875912408,Figure 4: Median test win rate vs training steps.
STARCRAFT MULTI-AGENT CHALLENGE,0.7664233576642335,"From Fig.4, GVR shows the best performance. Different from predator-prey with abnormal punishments,
the reward function in SMAC is more reasonable, where the linear and monotonic value decomposition can
meet the TGM condition approximately. As a result, the algorithms with full representation expressiveness
capacity (QPLEX, OWQMIX, CWQMIX) do not perform better than QMIX due to the difﬁculty of complete
expressiveness."
CONCLUSION AND FUTURE WORK,0.7737226277372263,"6
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.781021897810219,"This paper discusses the optimal coordination in fully cooperative MARL tasks and proposes a new criterion
(i.e., optimal consistency) to evaluate the optimality of coordination in value decomposition. To achieve the
optimal consistency, we introduce the TGM condition for linear and monotonic value decomposition, where
it is proved the TGM can be ensured if the optimal stable point is the unique stable point. Therefore, we
propose the GVR algorithm. Which ensures the optimal stable points via ITS and eliminates the non-optimal
stable points via SER. In experiments on matrix games, we verify our calculation results, which directly
demonstrates the effect of GVR. Besides, in experiments on predator-prey and SMAC, GVR outperforms
state-of-the-art baselines. However, GVR is unable to ensure the TGM condition in hard exploration tasks,
where the superior samples are difﬁcult to obtain. We are interested in the combination of GVR with efﬁcient
exploration approaches in future work."
CONCLUSION AND FUTURE WORK,0.7883211678832117,Under review as a conference paper at ICLR 2022
REFERENCES,0.7956204379562044,REFERENCES
REFERENCES,0.8029197080291971,"Wendelin B¨ohmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International
Conference on Machine Learning, pp. 980–991. PMLR, 2020."
REFERENCES,0.8102189781021898,"Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counter-
factual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 32, 2018."
REFERENCES,0.8175182481751825,"Jakob N Foerster, Yannis M Assael, Nando De Freitas, and Shimon Whiteson. Learning to communicate with
deep multi-agent reinforcement learning. arXiv preprint arXiv:1605.06676, 2016."
REFERENCES,0.8248175182481752,"Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored mdps. In NIPS, volume 1,
pp. 1523–1530, 2001."
REFERENCES,0.8321167883211679,"Tarun Gupta, Anuj Mahajan, Bei Peng, Wendelin B¨ohmer, and Shimon Whiteson. Uneven: Universal value
exploration for multi-agent reinforcement learning. In International Conference on Machine Learning, pp.
3930–3941. PMLR, 2021a."
REFERENCES,0.8394160583941606,"Tarun Gupta, Anuj Mahajan, Bei Peng, Wendelin B¨ohmer, and Shimon Whiteson. Uneven: Universal value
exploration for multi-agent reinforcement learning. In International Conference on Machine Learning, pp.
3930–3941. PMLR, 2021b."
REFERENCES,0.8467153284671532,"Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for
mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017."
REFERENCES,0.8540145985401459,"Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent variational
exploration. arXiv preprint arXiv:1910.07483, 2019."
REFERENCES,0.8613138686131386,"Frans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs. Springer, 2016."
REFERENCES,0.8686131386861314,"Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value functions for
decentralized pomdps. Journal of Artiﬁcial Intelligence Research, 32:289–353, 2008."
REFERENCES,0.8759124087591241,"Liviu Panait, Sean Luke, and R Paul Wiegand. Biasing coevolutionary search for optimal multiagent behaviors.
IEEE Transactions on Evolutionary Computation, 10(6):629–645, 2006."
REFERENCES,0.8832116788321168,"Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon
Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In
International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018."
REFERENCES,0.8905109489051095,"Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic
value function factorisation for deep multi-agent reinforcement learning. arXiv preprint arXiv:2006.10800,
2020."
REFERENCES,0.8978102189781022,"Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ
Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent
challenge. arXiv preprint arXiv:1902.04043, 2019."
REFERENCES,0.9051094890510949,"Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint
arXiv:1511.05952, 2015."
REFERENCES,0.9124087591240876,"Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In International
Conference on Machine Learning, pp. 5887–5896. PMLR, 2019."
REFERENCES,0.9197080291970803,Under review as a conference paper at ICLR 2022
REFERENCES,0.927007299270073,"Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg,
Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for
cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017."
REFERENCES,0.9343065693430657,"Sergey Vorotnikov, Konstantin Ermishin, Anaid Nazarova, and Arkady Yuschenko. Multi-agent robotic
systems in collaborative robotics. In International Conference on Interactive Collaborative Robotics, pp.
270–279. Springer, 2018."
REFERENCES,0.9416058394160584,"Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling multi-agent
q-learning. arXiv preprint arXiv:2008.01062, 2020."
REFERENCES,0.948905109489051,"Tonghan Wang, Jianhao Wang, Yi Wu, and Chongjie Zhang. Inﬂuence-based multi-agent exploration. arXiv
preprint arXiv:1910.05512, 2019."
REFERENCES,0.9562043795620438,"Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent soft q-learning. In 2018 AAAI Spring
Symposium Series, 2018."
REFERENCES,0.9635036496350365,"Chao Wen, Xinghu Yao, Yuhui Wang, and Xiaoyang Tan. Smix (λ): Enhancing centralized value functions
for cooperative multi-agent reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 34, pp. 7301–7308, 2020."
REFERENCES,0.9708029197080292,"Tong Wu, Pan Zhou, Kai Liu, Yali Yuan, Xiumin Wang, Huawei Huang, and Dapeng Oliver Wu. Multi-agent
deep reinforcement learning for urban trafﬁc light control in vehicular networks. IEEE Transactions on
Vehicular Technology, 69(8):8243–8256, 2020."
REFERENCES,0.9781021897810219,"Yaodong Yang, Jianye Hao, Guangyong Chen, Hongyao Tang, Yingfeng Chen, Yujing Hu, Changjie Fan, and
Zhongyu Wei. Q-value path decomposition for deep multiagent reinforcement learning. In International
Conference on Machine Learning, pp. 10706–10715. PMLR, 2020a."
REFERENCES,0.9854014598540146,"Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang. Qatten:
A general framework for cooperative multiagent reinforcement learning. arXiv preprint arXiv:2002.03939,
2020b."
REFERENCES,0.9927007299270073,"Shangtong Zhang and Richard S Sutton. A deeper look at experience replay. arXiv preprint arXiv:1712.01275,
2017. •"
