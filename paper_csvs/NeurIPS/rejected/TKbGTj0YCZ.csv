Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008741258741258741,"Uncertainty prevails due to the lack of knowledge about data or model, and con-
1"
ABSTRACT,0.0017482517482517483,"formal prediction (CP) predicts multiple potential targets, hoping to cover the true
2"
ABSTRACT,0.0026223776223776225,"target with a high probability. Regarding CP robustness, importance weighting
3"
ABSTRACT,0.0034965034965034965,"can address covariate shifts, but CP under joint distribution shifts remains more
4"
ABSTRACT,0.004370629370629371,"challenging. Prior attempts addressing joint shift via f-divergence ignores the
5"
ABSTRACT,0.005244755244755245,"nuance of calibration and test distributions that are critical for coverage guaran-
6"
ABSTRACT,0.006118881118881119,"tees. More generally, with multiple test distributions shifted from the calibration
7"
ABSTRACT,0.006993006993006993,"distribution, simultaneous coverage guarantees for all test domains requires a new
8"
ABSTRACT,0.007867132867132868,"paradigm. We design Multi-domain Robust Conformal Prediction (mRCP) that first
9"
ABSTRACT,0.008741258741258742,"formulates the coverage difference that importance weighting fails to capture under
10"
ABSTRACT,0.009615384615384616,"any joint shift. To squeeze such coverage difference and guarantee the (1 −α)
11"
ABSTRACT,0.01048951048951049,"coverage in all test domains, we propose Normalized Truncated Wasserstein dis-
12"
ABSTRACT,0.011363636363636364,"tance (NTW) to comprehensively capture the nuance of any test and calibration
13"
ABSTRACT,0.012237762237762238,"conformal score distributions, and design an end-to-end training algorithm incorpo-
14"
ABSTRACT,0.013111888111888112,"rating NTW to provide elasticity for simultaneous coverage guarantee over distinct
15"
ABSTRACT,0.013986013986013986,"test domains. With diverse tasks (seven datasets) and architectures (black-box and
16"
ABSTRACT,0.01486013986013986,"physics-informed models), NTW strongly correlates (Pearson coefficient=0.905)
17"
ABSTRACT,0.015734265734265736,"with coverage differences beyond covariate shifts, while mRCP reduces coverage
18"
ABSTRACT,0.016608391608391608,"gap by 50% on average robustly over multiple distinct test domains.
19"
INTRODUCTION,0.017482517482517484,"1
Introduction
20"
INTRODUCTION,0.018356643356643356,"The growing data volume, enhanced computation capability, and advanced models significantly
21"
INTRODUCTION,0.019230769230769232,"improve machine learning predictive accuracy. Nevertheless, noises, unobservable factors, and the
22"
INTRODUCTION,0.020104895104895104,"lack of knowledge lead to uncertainty that stakeholders should ponder along model predictions
23"
INTRODUCTION,0.02097902097902098,"when making decisions particularly in areas such as fintech [25], autonomous driving [2], traffic
24"
INTRODUCTION,0.021853146853146852,"forecasting [4], and epidemiology [32, 27]. Conformal Prediction (CP) addresses uncertainty by
25"
INTRODUCTION,0.022727272727272728,"predicting a set of possible target(s) rather than a single guess [31]. Specifically, CP computes
26"
INTRODUCTION,0.0236013986013986,"conformal scores (residuals between predicted and true targets for regression tasks) of a trained
27"
INTRODUCTION,0.024475524475524476,"model f on a calibration set, and calculates the 1 −α quantile q of these scores. For any input x, CP
28"
INTRODUCTION,0.025349650349650348,"produces the smallest prediction set C(x) consisting of target values whose conformal scores are less
29"
INTRODUCTION,0.026223776223776224,"than q. Assuming that the test and calibration data are exchangeable (including i.i.d.), the true target
30"
INTRODUCTION,0.027097902097902096,"y is guaranteed to be covered by C(x) with at least 1 −α probability.
31"
INTRODUCTION,0.027972027972027972,"In practice, calibration distribution PXY and test distribution QXY may differ thus PXY ̸= QXY ,
32"
INTRODUCTION,0.028846153846153848,"termed as joint distribution shift and violate the exchangeability assumption. Joint shift can occur
33"
INTRODUCTION,0.02972027972027972,"with either covariate shift (PX ̸= QX) or concept shift (PY |X ̸= QY |X), though what causes a
34"
INTRODUCTION,0.030594405594405596,"joint shift is difficult to infer from the observed data only. With importance weighting, covariate
35"
INTRODUCTION,0.03146853146853147,"shift is shown not to affect the coverage confidence guarantee [29]. To address CP under joint
36"
INTRODUCTION,0.032342657342657344,"Weighted calibration conformal score CDF, ෠𝐹𝑄/𝑃"
INTRODUCTION,0.033216783216783216,"Training Set 1
Training Set M"
INTRODUCTION,0.03409090909090909,Calibration Set
INTRODUCTION,0.03496503496503497,Model 𝑓𝜃
INTRODUCTION,0.03583916083916084,Normalization and Truncation
INTRODUCTION,0.03671328671328671,"Robustness Penalty (NTW) 
mRCP Loss 
Prediction Residual … …"
INTRODUCTION,0.037587412587412584,Optimization
INTRODUCTION,0.038461538461538464,"𝑝(𝑋)
𝑝(𝑌|𝑋) 𝑋 𝑝(𝑋) 𝑌"
INTRODUCTION,0.039335664335664336,𝑝(𝑌|𝑋) … 𝑋 𝑝(𝑋)
INTRODUCTION,0.04020979020979021,𝑝(𝑌|𝑋) 𝑌
INTRODUCTION,0.04108391608391608,Test Domain 1 𝑋 𝑝(𝑋)
INTRODUCTION,0.04195804195804196,𝑝(𝑌|𝑋) 𝑌
INTRODUCTION,0.04283216783216783,"Test Domain M
(a)"
INTRODUCTION,0.043706293706293704,"(b)
(c)
(d) 𝑋 𝑝(𝑋)"
INTRODUCTION,0.044580419580419584,𝑝(𝑌|𝑋) 𝑌
INTRODUCTION,0.045454545454545456,Test Domain 2
INTRODUCTION,0.04632867132867133,"Calibration conformal score CDF, ෠𝐹𝑃"
INTRODUCTION,0.0472027972027972,"Test domain conformal score, ෠𝐹𝑄 𝑣"
INTRODUCTION,0.04807692307692308,"Cumulative Probability 𝑞
𝑞∗"
INTRODUCTION,0.04895104895104895,"𝐷joint
𝐷concept"
INTRODUCTION,0.049825174825174824,𝐷covariate
INTRODUCTION,0.050699300699300696,(1−𝛼)(𝑛+1) 𝑛 ෠𝐹𝑃𝑞 ෠𝐹𝑄𝑞
INTRODUCTION,0.051573426573426576,Weighted Calibration conformal score
INTRODUCTION,0.05244755244755245,Test domain 1 conformal score
INTRODUCTION,0.05332167832167832,Test domain 2 conformal score
INTRODUCTION,0.05419580419580419,𝑞2(highest)
INTRODUCTION,0.05506993006993007,Cumulative Probability
INTRODUCTION,0.055944055944055944,confidence 𝑣
INTRODUCTION,0.056818181818181816,overestimated coverage
INTRODUCTION,0.057692307692307696,"Wasserstein-1 distance between weighted calibration
and test domain 2"
INTRODUCTION,0.05856643356643357,"Wasserstein-1 distance between  weighted calibration 
and test domain 1"
INTRODUCTION,0.05944055944055944,"Total variation
𝑞1 𝑣 𝑝(𝑣)"
INTRODUCTION,0.06031468531468531,Calibration & Test Domain 1 𝑝(𝑣) 𝑣
INTRODUCTION,0.06118881118881119,Calibration & Test Domain 2
INTRODUCTION,0.062062937062937064,෠𝐹𝑄/𝑃𝑞∗ ෠𝐹𝑄𝑞∗
INTRODUCTION,0.06293706293706294,Calibration Domain
INTRODUCTION,0.06381118881118882,"Figure 1: (a) Multiple test domains E = {e1, ..., eM} with joint shifts (Q(e)
XY ̸= PXY ); (b) coverage difference
Djoint = ˆFQ(q) −ˆFP (q) (Eq. (5)) due to Q(e)
XY ̸= PXY is decomposed into the Dcoviriate (Eq. (7)) caused by
covariate shift (Q(e)
X
̸= PX) and the remaining Dconcept (Eq. (8)) due to concept shift (Q(e)
Y |X ̸= PY |X); (c)
Wasserstein-1 (W-) distances (Eq. (11)) between test and weighted calibration conformal score CDFs capture
the expected Dconcept (Eq. (10)). However, f-divergence (e.g., total variation, KL divergence) does not compare
two CDFs pointwisely and fails to capture such an expectation. Test domains 1 and 2 both have identical
total variations to the calibration domain but different W-distances. With multiple test domains, using a single
1 −α quantile [33, 6] lead to the dilemma of CP coverage efficiency and confidence guarantee; (d) Solution:
Normalized Truncated W-distance (Eq. (16)) is robust to outlier scores and different score scales across test
domains, and the mRCP algorithm reduces NTW on all test domains can elastically train a model to guarantee
conformal coverage for Q(e)
Y |X ̸= PY |X∀e ∈E = {e1, ..., eM}."
INTRODUCTION,0.06468531468531469,"shift, f-divergence is adopted in [33, 6] to measure the difference between PXY and QXY or
37"
INTRODUCTION,0.06555944055944056,"the corresponding conformal scores distributions. However, f-divergence ignores where the two
38"
INTRODUCTION,0.06643356643356643,"distributions differ, which quantiles and coverage guarantees depend on (Figure 1, (c)). When test
39"
INTRODUCTION,0.0673076923076923,"data are sampled from multiple distinct test distributions Q(e)
XY , e ∈E = {e1, ..., eM}, it is desired
40"
INTRODUCTION,0.06818181818181818,"to ensure simultaneous 1 −α coverage for all test distributions. Previous work selects the highest
41"
INTRODUCTION,0.06905594405594405,"1 −α quantile from all test distributions and constructs C(x) for x ∈Q(e)
X , ∀e ∈E = {e1, ..., eM},
42"
INTRODUCTION,0.06993006993006994,"producing excessively large set C(x). Selecting other quantiles may lead to smaller coverage on a
43"
INTRODUCTION,0.07080419580419581,"test domain than was expected during calibration, leading to prediction overconfidence. Without a
44"
INTRODUCTION,0.07167832167832168,"new paradigm to guarantee coverage under multiple shifted test distributions, the dilemma between
45"
INTRODUCTION,0.07255244755244755,"CP coverage efficiency and confidence guarantee seems unavoidable.
46"
INTRODUCTION,0.07342657342657342,"We first decompose the coverage difference under any joint distribution shift to a component due to
47"
INTRODUCTION,0.0743006993006993,"covariate shift (PX ̸= Q(e)
X , addressed by importance weighting [29]) and that due to concept shift
48"
INTRODUCTION,0.07517482517482517,"(PY |X ̸= Q(e)
Y |X). We propose Normalized Truncated Wasserstein distance (NTW) to robustly capture
49"
INTRODUCTION,0.07604895104895106,"where the test and importance-weighted calibration conformal score cumulative density function
50"
INTRODUCTION,0.07692307692307693,"(CDF) deviate (Figure 1, (b)). We design Multi-domain Robust Conformal Prediction (mRCP) by
51"
INTRODUCTION,0.0777972027972028,"minimizing all NTW terms over E = {e1, ..., eM} during model training (Figure 1, (d)) to elastically
52"
INTRODUCTION,0.07867132867132867,"guarantee coverage confidence for all test domains. Experiments on regression tasks on seven datasets
53"
INTRODUCTION,0.07954545454545454,"demonstrate that: 1) NTW well-correlates with the coverage difference after importance weighting
54"
INTRODUCTION,0.08041958041958042,"(Pearson coefficient 0.905); 2) mRCP provides conformal predictions that reduce average coverage
55"
INTRODUCTION,0.08129370629370629,"difference by 50% compared to baselines under multiple joint shifts; 3) mRCP is sufficiently general
56"
INTRODUCTION,0.08216783216783216,"to address joint distribution shifts even after incorporating domain knowledge when available.
57"
BACKGROUND AND RELATED WORK,0.08304195804195805,"2
Background and related work
58"
CONFORMAL PREDICTION,0.08391608391608392,"2.1
Conformal prediction
59"
CONFORMAL PREDICTION,0.08479020979020979,"Let x ∈X and y ∈Y denote the input and output random variable, respectively, where X and
60"
CONFORMAL PREDICTION,0.08566433566433566,"Y ⊆R is the input and output space, respectively. On X × Y, the calibration domain is defined by a
61"
CONFORMAL PREDICTION,0.08653846153846154,"joint distribution PXY , and we consider a calibration set Sc = {(x1, y1), . . . , (xn, yn)} are drawn
62"
CONFORMAL PREDICTION,0.08741258741258741,"i.i.d. from PXY . Similarly, a test set St = {(x1, y1), . . . , (xm, ym)} is drawn i.i.d. from test domain,
63"
CONFORMAL PREDICTION,0.08828671328671328,"which is defined by a joint distribution QXY .
64"
CONFORMAL PREDICTION,0.08916083916083917,"With a trained regression model f, the conformal score vi = v(xi, yi) = |f(xi) −yi| is the residual
65"
CONFORMAL PREDICTION,0.09003496503496504,"between the predicted target f(xi) and the true target yi. The set of calibration conformal scores is
66"
CONFORMAL PREDICTION,0.09090909090909091,"denoted as Vc = {v(xi, yi)|(xi, yi) ∈Sc}. Let q be the ⌈(1 −α)(n + 1)⌉/n quantile of Vc:
67"
CONFORMAL PREDICTION,0.09178321678321678,"q = Quantile
⌈(1 −α)(n + 1)⌉ n
, 1 n X"
CONFORMAL PREDICTION,0.09265734265734266,vi∈Vc δvi
CONFORMAL PREDICTION,0.09353146853146853,"
,
(1)"
CONFORMAL PREDICTION,0.0944055944055944,"where δvi represents the point mass at vi (i.e., the distribution placing all mass at the value vi).
68"
CONFORMAL PREDICTION,0.09527972027972027,"Quantile(1 −α, F) := inf{z|Pr(Z < z) ≥1 −α} and F is the CDF of Z. With the quantile q, the
69"
CONFORMAL PREDICTION,0.09615384615384616,"CP prediction set of an input x from St is
70"
CONFORMAL PREDICTION,0.09702797202797203,"C(x) = {ˆy ∈R||f(x) −ˆy| ≤q, (x, y) ∈St} .
(2)"
CONFORMAL PREDICTION,0.0979020979020979,"Most CP methods, such as[22, 23], rely on the assumption of exchangeability, which is relaxed from
71"
CONFORMAL PREDICTION,0.09877622377622378,"the i.i.d. assumption [31]. In our scenario, if the calibration and test samples are drawn from the
72"
CONFORMAL PREDICTION,0.09965034965034965,"identical joint probability distribution (PXY = QXY ), these calibration and test samples are i.i.d.
73"
CONFORMAL PREDICTION,0.10052447552447552,"Under this assumption, the probability that the true target y is included in C(x) is at least 1 −α,
74"
CONFORMAL PREDICTION,0.10139860139860139,"which is called coverage guanrantee, or more formally,
75"
CONFORMAL PREDICTION,0.10227272727272728,"Pr (y ∈C(x)) ≥1 −α.
(3)"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.10314685314685315,"2.2
Conformal prediction under domain shift
76"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.10402097902097902,"Covariate shift (PX ̸= QX) means marginal distributions between the calibration and test domains
77"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.1048951048951049,"are different. CP under covariate shift is addressed using importance weighting [29]. Under a
78"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.10576923076923077,"probabilistic view, [14] defined the covariate shift as a bounded perturbation on any test input and
79"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.10664335664335664,"developed adaptive probabilistically robust CP. The condition of multiple test domains is discussed
80"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.10751748251748251,"in [15], and similar topics include coverages under feature-stratification [7, 11].
81"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.10839160839160839,"Joint distribution shift (PXY ̸= QXY ) indicates at least one of covariate shift (PX ̸= QX) and
82"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.10926573426573427,"concept shift (different conditional distributions, PY |X ̸= QY |X) will occur [17]. This shift is more
83"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.11013986013986014,"general and the importance weighting method cannot address changes in conditional distribution.
84"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.11101398601398602,"With M test domains E = {e1, ..., eM}, each e ∈E is defined by a joint distribution Q(e)
XY and holds
85"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.11188811188811189,"a joint shift with calibration domain PXY (i.e., PXY ̸= Q(e)
XY ). Considering this condition, previous
86"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.11276223776223776,"works, such as [6, 33], presume all test domains fall in a predefined f-divergence range, calculate
87"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.11363636363636363,"confidence-specified quantile of each test domain, and apply the highest quantile to all domains.
88"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.1145104895104895,"This method causes excessively high coverages and thus overlarge prediction sets, which reduces
89"
CONFORMAL PREDICTION UNDER DOMAIN SHIFT,0.11538461538461539,"prediction efficiency because smaller prediction sets can help locate true targets better.
90"
CONFORMAL PREDICTION UNDER JOINT DISTRIBUTION SHIFT,0.11625874125874126,"3
Conformal prediction under joint distribution shift
91"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.11713286713286714,"3.1
Decomposition of coverage difference
92"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.11800699300699301,"We decompose the coverage difference between a calibration domain PXY and a test domain QXY
93"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.11888111888111888,"under joint distribution shift at a user-specified confidence (1 −α).
94"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.11975524475524475,"Similar to Vc, we define the test conformal score set Vt = {v(xi, yi)|(xi, yi) ∈St}. With the
95"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.12062937062937062,"indicator function 1, empirical CDFs of calibration and test conformal scores are
96"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.1215034965034965,ˆFP (v) = 1 n X
DECOMPOSITION OF COVERAGE DIFFERENCE,0.12237762237762238,"vi∈Vc δv1vi<v,
ˆFQ(v) = 1 m X"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.12325174825174826,"vi∈Vt δv1vi<v.
(4)"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.12412587412587413,"With given 1 −α confidence, quantile q is calculated in Eq. (1), and the coverage difference under a
97"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.125,"joint distribution shift can be quantified as
98"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.1258741258741259,"Djoint(q) = ˆFQ(q) −ˆFP (q).
(5)"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.12674825174825174,"[29] employs importance weighting for CP under covariate shift. Specifically, if the ratio of test
99"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.12762237762237763,"to calibration covariate likelihoods, QX/PX, is known, a calibration conformal score vi ∈Vc is
100"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.1284965034965035,"weighted by pi = w(xi)/Pn
j=1 w(xj), where w(xi) = QX(xi)/PX(xi). Therefore, the empirical
101"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.12937062937062938,"CDF of weighted empirical calibration scores is given by
102"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.13024475524475523,"ˆFQ/P (v) =
Xn"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.13111888111888112,"i=1 piδvi1vi<v,"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.131993006993007,"where the subscript Q/P indicates conformal scores of calibration domain P is weighted by confor-
103"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.13286713286713286,"mal scores of test domain Q. The confidence-specified quantile of the weighted calibration conformal
104"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.13374125874125875,"scores is
105"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.1346153846153846,"q∗= Quantile

⌈(1 −α)(n + 1)⌉/n,
Xn"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.1354895104895105,"i=1 piδvi

.
(6)"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.13636363636363635,"As importance weighting ensures the 1 −α coverage as though covariate shift were absent, coverage
106"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.13723776223776224,"difference Dcovariate caused by covariate shift is the gap between the coverages under test conformal
107"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.1381118881118881,"score CDF using quantiles on unweighted and weighted calibration conformal score distributions.
108"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.13898601398601398,"Dcovariate(q, q∗) = ˆFQ(q) −ˆFQ(q∗).
(7)"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.13986013986013987,"Importance weighting can not address CP under joint shift as it fails to capture changes in conditional
109"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.14073426573426573,"probability distribution caused by concept shift, thus we present the coverage difference caused by
110"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.14160839160839161,"concept shift is
111"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.14248251748251747,"Dconcept(q, q∗) = Djoint(q) −Dcovariate(q, q∗) = ˆFQ(q∗) −ˆFP (q),
(8)"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.14335664335664336,"which is remaining coverage difference after applying importance weighting. Here we assume
112"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.14423076923076922,"ˆFP (q) = ˆFQ/P (q∗), so we can rewrite Dconcept by
113"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.1451048951048951,"Dconcept(q∗) = ˆFQ(q∗) −ˆFQ/P (q∗).
(9)"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.145979020979021,"The error bound for the assumption is quite small especially when the calibration set size n is large.
114"
DECOMPOSITION OF COVERAGE DIFFERENCE,0.14685314685314685,"The detailed proof is provided in Appendix B. We denote Dconcept as D for simplification.
115"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.14772727272727273,"3.2
Normalized Truncated Wasserstein distance
116"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1486013986013986,"To develop a metric that is independent of confidence level and can quantify the overall closeness
117"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.14947552447552448,"between weight calibration and test conformal scores, we estimate the expected coverage difference
118"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.15034965034965034,"under concept shift as
119"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.15122377622377622,E[D] = 1 n X vi∈Vc
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1520979020979021,"ˆFQ(vi) −ˆFQ/P (vi)
 ,
(10)"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.15297202797202797,"based on the approximation in Eq. (9), where E indicates the expectation function.
120"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.15384615384615385,"Definition 1 (Wasserstein-1 Distance). If F1 and F2 are two cumulative distribution functions (CDFs),
121"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1547202797202797,"the Wasserstein-1 distance, dW, is quantified by the area between F1 and F2.
122"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1555944055944056,"dW(F1, F2) =
Z"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.15646853146853146,"R
|F1(v) −F2(v)|dx.
(11)"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.15734265734265734,"Applying Wasserstein-1 distance (W-distance) in Eq. (11) to ˆFQ and ˆFQ/P , we get
123"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.15821678321678323,"dW( ˆFQ, ˆFQ/P ) =
Z ∞"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1590909090909091,"0
| ˆFQ(v) −ˆFQ/P (v)|dv.
(12)"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.15996503496503497,"As we define conformal scores as the residuals between predicted and true targets, they are always
124"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.16083916083916083,"positive, so we only need to integral from 0 to ∞in Eq. (12).
125"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.16171328671328672,"We assume Vc is sorted. As both ˆFQ and ˆFQ/P are empirical CDFs, we can approximately represent
126"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.16258741258741258,"dW( ˆFQ, ˆFQ/P ) in a discrete form as
127"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.16346153846153846,"dW( ˆFQ, ˆFQ/P ) ≈
Xn−1 i=1"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.16433566433566432,"ˆFQ(vi) −ˆFQ/P (vi)
 (vi+1 −vi), vi ∈Vc.
(13)"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1652097902097902,"Eq. (13) shows dW( ˆFQ, ˆFQ/P ) can be estimated as a weighted summation of | ˆFQ(vi) −ˆFQ/P (vi)|
128"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1660839160839161,"for vi ∈Vc\{vn} with the corresponding weight vi+1 −vi. Also, Eq. (10) indicates that E[D] can
129"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.16695804195804195,"be regarded as the weighted summation of | ˆFQ(vi) −ˆFQ/P (vi)| for vi ∈Vc with weight 1/n. The
130"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.16783216783216784,"similarity between Eq. (13) and Eq. (10) allows us to apply the W-distance between the test and
131"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1687062937062937,"weighted calibration conformal score to capture expected coverage difference under concept shift.
132"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.16958041958041958,"Care needs to be taken for Eq. (13) to make this metric more robust. At first, we expect the weights
133"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.17045454545454544,"vi+1 −vi to be approximately equal, as weights in Eq. (10) are constants 1/n. However, some outlier
134"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.17132867132867133,"calibration conformal scores have large distances from their neighbors, causing involved weights
135"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.17220279720279721,"much higher than 1/n. These outlier scores are represented as a long tail of ˆFQ/P when it converges
136"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.17307692307692307,"to 1. Therefore, it is necessary to establish a partition threshold to truncate the long tail. We calculate
137"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.17395104895104896,"the partition threshold
138"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.17482517482517482,"vσ = inf
n
vi| ˆFQ/P (vi) ≥1 −σ, vi ∈Vc
o
,
(14)"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1756993006993007,"which is the smallest calibration conformal score whose coverage is greater or equal to a user-defined
139"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.17657342657342656,"value 1 −σ. In contrast to the original dW( ˆFQ, ˆFQ/P ) integrated on the set of real numbers, the
140"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.17744755244755245,"truncated form is integrated from 0 to vσ as
141"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.17832167832167833,"dTW( ˆFQ, ˆFQ/P ) =
Z vσ"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1791958041958042,"0
| ˆFQ(v) −ˆFQ/P (v)|dv.
(15)"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.18006993006993008,"Secondly, as the summation of weights in Eq. (10) is 1, we also need to divide each vi+1 −vi
142"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.18094405594405594,"by vσ −v1. When the calibration set is large enough, it is plausible to assume the existence of a
143"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.18181818181818182,"calibration sample fitting the trained model f very well, causing the smallest calibration conformal
144"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.18269230769230768,"score v1 ≈0. Therefore, this normalized can be formulated as
145"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.18356643356643357,"dNTW( ˆFQ, ˆFQ/P ) = 1 vσ Z vσ"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.18444055944055945,"0
| ˆFQ(v) −ˆFQ/P (v)|dv.
(16)"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1853146853146853,"A lower dNTW indicates more similarity between ˆFQ/P and ˆFQ, thus leading to more robust conformal
146"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1861888111888112,"prediction in the test domain. As a result, NTW enables us to assess the expected coverage difference
147"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.18706293706293706,"due to concept shift in Eq. (10). Experiment results in Section 5 and Appendix E show the necessity
148"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.18793706293706294,"of truncation and normalization. We also prove that the W-distance between the test and weighted
149"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1888111888111888,"calibration conformal score population CDF can establish an upper bound for coverage difference
150"
NORMALIZED TRUNCATED WASSERSTEIN DISTANCE,0.1896853146853147,"under concept shift in Appendix C.
151"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.19055944055944055,"4
Multi-domain robust conformal prediction
152"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.19143356643356643,"If a calibration set Sc, and a test set St are drawn from a domain PXY , the i.i.d. assumption is
153"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.19230769230769232,"satisfied, and the coverage guarantee in Eq. (3) holds for (x, y) ∈St.
154"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.19318181818181818,"The domain PXY can be decomposed into M multiple domains, denoted as E = {e1, ..., eM}.
155"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.19405594405594406,"PXY (x, y) = 1 M X"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.19493006993006992,"e∈E Q(e)
XY (x, y)
(17)"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.1958041958041958,"However, for e ∈E, denote S(e)
t
a test set drawn from Q(e)
XY , then the coverage guarantee may no
156"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.19667832167832167,"longer hold for (x, y) ∈S(e)
t
, because joint distribution shift may occur between PXY and Q(e)
XY . It
157"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.19755244755244755,"indicates CP can be overconfident and underconfident for samples from different Q(e)
XY , resulting in
158"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.19842657342657344,"prediction biases.
159"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.1993006993006993,"Inspired by the works of multi-domain generalization [26, 18, 19, 1], we propose Multi-domain
160"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.20017482517482518,"Robust Conformal Prediction (mRCP) to make the coverage approach confidence in all domains,
161"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.20104895104895104,"using a training set S(e) from the data distribution Q(e)
XY for e ∈E and a calibration set Sc from PXY .
162"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.20192307692307693,"The objective function of mRCP includes two components. First, for the minimization of prediction
163"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.20279720279720279,"residuals, denoting l a loss function, Empirical Risk Minimization (ERM) [30] is incorporated as
164"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.20367132867132867,"LERM(θ) =
X"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.20454545454545456,"e∈E L(e)(θ) =
X"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.20541958041958042,"e∈E E(xi,yi)∼S(e) [l(fθ(xi), yi)] .
(18)"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.2062937062937063,"Secondly, we aim for robust conformal prediction on each domain during testing, seeking a low value
165"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.20716783216783216,"of E[D] in Eq. (10) across test domains, so mRCP needs to address coverage differences due to
166"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.20804195804195805,"covariate and concept shifts simultaneously. To remove coverage differences due to covariate shifts,
167"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.2089160839160839,"it applies importance weighting to each domain e ∈E during training and obtains ˆFQ(e)/P , which is
168"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.2097902097902098,"the calibration conformal score CDF weighted by Q(e)
XY .
169"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.21066433566433568,"Besides, as we have a training set S(e) from domain Q(e)
XY , an empirical CDF of conformal scores in
170"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.21153846153846154,"Q(e)
XY can be computed, denoted as ˆF tr
Q(e). NTW quantifies the expected coverage difference caused
171"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.21241258741258742,"by concept shift between ˆFQ(e)/P and training conformal score CDF ˆF tr
Q(e). Combining these two
172"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.21328671328671328,"components, the objective function of mRCP is
173"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.21416083916083917,"LmRCP(θ) =
X"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.21503496503496503,"e∈E L(e)(θ) + β
X"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.2159090909090909,"e∈E dNTW( ˆF tr
Q(e), ˆFQ(e)/P ),
(19)"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.21678321678321677,where β is a hyperparameter balancing these two parts. mRCP algorithm is shown in Algorithm 1.
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.21765734265734266,Algorithm 1 Multi-domain Robust Conformal Prediction
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.21853146853146854,"Require: M training sets S(e), e ∈E; one calibration set Sc; N training epochs; model fθ; partition value σ;
loss function l; penalty hyperparameter β.
1: for e ∈E do
2:
for (xi, yi) ∈Sc do"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.2194055944055944,"3:
w(xi) =
Q(e)
X (xi)
PX(xi) , p(i,e) =
w(xi)
Pn
j=1 w(xj)
▷Covariate shift between Q(e)
XY and PXY
4:
end for
5: end for
6:
7: for i = 1 to N do
8:
Vc = {v(xi, yi)|(xi, yi) ∈Sc}
▷Calibration score set
9:
for e ∈E do
10:
L(e)(θ) = E(xi,yi)∼S(e) [l(fθ(xi), yi)]
▷ERM loss of domain e"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.2202797202797203,"11:
V (e) =
n
v(xi, yi)|(xi, yi) ∈S(e)o
▷Training score set of domain e"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.22115384615384615,"12:
ˆF tr
Q(e) = P"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.22202797202797203,"vi∈V (e) δvi1vi≤v
▷Training score CDF of domain e"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.2229020979020979,"13:
ˆFQ(e)/P (v) = P"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.22377622377622378,"vi∈Vc p(i,e)δvi1vi≤v
▷Calibration score CDF weighted by Q(e)
XY
14:
vσ = inf
n
ˆFQ(e)/P (vi) ≥1 −σ, vi ∈Vc
o
▷Truncation threshold"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.22465034965034966,"15:
dNTW

ˆF tr
Q(e), ˆFQ(e)/P

=
1
vσ
R vσ
0"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.22552447552447552,"ˆF tr
Q(e)(v) −ˆFQ(e)/P )
 dv
▷NTW calculation
16:
end for
17:
Optimize fθ based on LmRCP(θ) = P"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.2263986013986014,e∈E L(e)(θ) + β P
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.22727272727272727,"e∈E dNTW

ˆF tr
Q(e), ˆFQ(e)/P
"
MULTI-DOMAIN ROBUST CONFORMAL PREDICTION,0.22814685314685315,18: end for 174
EXPERIMENT,0.229020979020979,"5
Experiment
175"
EXPERIMENT,0.2298951048951049,"In this section, we validate NTW in Eq. (16) as a good indicator of expected coverage difference due
176"
EXPERIMENT,0.23076923076923078,"to concept shift and demonstrate the effectiveness of mRCP in obtaining coverage robustness across
177"
EXPERIMENT,0.23164335664335664,"different test domains.
178"
DATASETS AND MODELS,0.23251748251748253,"5.1
Datasets and models
179"
DATASETS AND MODELS,0.23339160839160839,"We conducted experiments across various datasets: (a) the airfoil self-noise dataset [5]; (b) Seattle-
180"
DATASETS AND MODELS,0.23426573426573427,"loop [9], PeMSD4, PeMSD8 [16] for traffic speed prediction; (c) US-Regions, US-States, and
181"
DATASETS AND MODELS,0.23513986013986013,"Japan-Prefectures [10] for epidemic spread forecasting. The airfoil dataset was manually altered to
182"
DATASETS AND MODELS,0.23601398601398602,"create three subsets demonstrating covariate and concept shifts. 24 domains for the traffic datasets
183"
DATASETS AND MODELS,0.2368881118881119,"were designated based on data generation hours, while epidemic dataset instances were categorized
184"
DATASETS AND MODELS,0.23776223776223776,"into four domains reflecting different pandemic stages. A multilayer perceptron (MLP) with a (input
185"
DATASETS AND MODELS,0.23863636363636365,"dimension, 64, 64, 1) architecture was utilized for all datasets. Traffic and epidemic prediction tasks
186"
DATASETS AND MODELS,0.2395104895104895,"were also trained on corresponding physics-informed partial differential equations (PDEs), which are
187"
DATASETS AND MODELS,0.2403846153846154,"the Susceptible-Infected-Recovered (SIR) model and the Reaction-Diffusion (RD) model respectively.
188"
DATASETS AND MODELS,0.24125874125874125,"We refer to Appendix D for detailed experiment setups.
189"
EXPERIMENTS OF NTW,0.24213286713286714,"5.2
Experiments of NTW
190"
EXPERIMENTS OF NTW,0.243006993006993,"For each of the experiment setups, a training set, a validation set, and a test set were sampled from
191"
EXPERIMENTS OF NTW,0.24388111888111888,"each Q(e)
XY for e ∈E. One calibration set was sampled from PXY which is a mixture probability
192"
EXPERIMENTS OF NTW,0.24475524475524477,"distribution of Q(e)
XY for e ∈E, as shown in Eq. (17). To validate NTW is a good indicator of E[D],
193"
EXPERIMENTS OF NTW,0.24562937062937062,"we only need to use ERM in Eq. (18) to train the model fθ, which can be an MLP or a PDE. The loss
194"
EXPERIMENTS OF NTW,0.2465034965034965,"function l is the ℓ1 norm, as same as how we compute conformal scores.
195"
EXPERIMENTS OF NTW,0.24737762237762237,"After training, for e ∈E, we first calculated the NTW between the calibration conformal score CDF
196"
EXPERIMENTS OF NTW,0.24825174825174826,"weighted by Q(e)
X /PX , and validation conformal score CDF of Q(e)
X . Denote the NTW of domain
197"
EXPERIMENTS OF NTW,0.2491258741258741,"e as d(e)
NTW. Then, we estimated the expected coverage difference caused by concept shift on a test
198"
EXPERIMENTS OF NTW,0.25,"domain e, denoted as Eα[D(e)], using the coverage difference expectation between the test and
199"
EXPERIMENTS OF NTW,0.2508741258741259,"weighted calibration conformal score CDFs on a 1 −α confidence set {0.1, ..., 0.9}.
200"
EXPERIMENTS OF NTW,0.2517482517482518,"Eα[D(e)] and d(e)
NTWshould have a positive correlation for e ∈E , proving NTW can capture the
201"
EXPERIMENTS OF NTW,0.2526223776223776,"expected coverage difference caused by concept shift.
202"
EXPERIMENTS OF NTW,0.2534965034965035,"Baselines: We select six baseline metrics to validate the effectiveness of NTW. Total variation
203"
EXPERIMENTS OF NTW,0.2543706293706294,"dTV [13], and Kullback-Leibler (KL) divergence dKL [21] are chosen as two typical f-divergence
204"
EXPERIMENTS OF NTW,0.25524475524475526,"metrics. Expectation difference ∆E [19] is selected since it is a widely applied generalization metric.
205"
EXPERIMENTS OF NTW,0.2561188811188811,"We also measure standard, normalized, and truncated W-distance, denoted as dW, dNW, and dTW
206"
EXPERIMENTS OF NTW,0.256993006993007,"respectively, to demonstrate applying normalization and truncation together is necessary.
207"
EXPERIMENTS OF NTW,0.25786713286713286,"Metric: We apply the Pearson coefficient to quantify the correlations between metrics and the
208"
EXPERIMENTS OF NTW,0.25874125874125875,"coverage difference expectation. It measures the linear correlation between two values by giving a
209"
EXPERIMENTS OF NTW,0.25961538461538464,"value between -1 and 1 inclusive. 1,0, and -1 indicate perfect positive linear, no linear, and negative
210"
EXPERIMENTS OF NTW,0.26048951048951047,"linear correlations, respectively. Therefore, if the Pearson coefficient of a metric is higher, this metric
211"
EXPERIMENTS OF NTW,0.26136363636363635,"can indicate the expected coverage difference better. We provide a detailed definition of the Pearson
212"
EXPERIMENTS OF NTW,0.26223776223776224,coefficient in Appendix E.
EXPERIMENTS OF NTW,0.2631118881118881,Table 1: Pearson coefficients between metrics and coverage difference expectation under concept shift
EXPERIMENTS OF NTW,0.263986013986014,"Dataset
Model
dNTW
dTV
dKL
∆E
dW
dNW
dTW
Airfoil
MLP
1.000
-0.356
-0.545
0.891
0.878
0.951
0.967
Seattle-
loop"
EXPERIMENTS OF NTW,0.26486013986013984,"MLP
0.971
0.461
0.054
0.781
0.759
0.762
0.765
PDE
0.996
0.890
0.058
0.897
0.893
0.909
0.921"
EXPERIMENTS OF NTW,0.26573426573426573,"PeMSD4
MLP
0.992
0.846
-0.390
0.926
0.915
0.964
0.941
PDE
0.986
0.682
-0.068
0.858
0.872
0.928
0.858"
EXPERIMENTS OF NTW,0.2666083916083916,"PeMSD8
MLP
0.905
0.397
-0.089
0.333
0.267
0.371
0.529
PDE
0.827
0.129
-0.114
0.253
0.118
0.141
0.527
US-
States"
EXPERIMENTS OF NTW,0.2674825174825175,"MLP
0.999
0.966
0.965
0.872
0.885
0.912
0.931
PDE
0.999
0.966
0.964
0.817
0.848
0.890
0.899
US-
Regions"
EXPERIMENTS OF NTW,0.26835664335664333,"MLP
0.636
-0.530
-0.338
-0.205
-0.308
-0.352
-0.405
PDE
0.709
0.308
0.350
0.484
0.355
0.322
0.137
Japan-
Prefectures"
EXPERIMENTS OF NTW,0.2692307692307692,"MLP
0.996
0.986
0.988
0.943
0.948
0.954
0.950
PDE
0.997
0.983
0.981
0.907
0.918
0.935
0.924
Average
0.905
0.574
0.325
0.619
0.583
0.607
0.629
Standard Deviation
0.128
0.474
0.562
0.368
0.420
0.437
0.428 213"
EXPERIMENTS OF NTW,0.2701048951048951,"Results: Table 1 illustrates the Pearson coefficients between NTW and the coverage difference
214"
EXPERIMENTS OF NTW,0.270979020979021,"expectation among seven datasets and different models, compared with the other six baseline metrics.
215"
EXPERIMENTS OF NTW,0.2718531468531469,"We highlight that NTW keeps holding the largest Pearson coefficient among all experiment setups,
216"
EXPERIMENTS OF NTW,0.2727272727272727,"which means the proposed metric can keep indicating the coverage difference expectation. Specifically,
217"
EXPERIMENTS OF NTW,0.2736013986013986,"the coefficients of total variation dTV and KL divergence dKL fluctuate along experiments, meaning
218"
EXPERIMENTS OF NTW,0.2744755244755245,"that they can not truly indicate the coverage difference expectation. ∆E can not capture the coverage
219"
EXPERIMENTS OF NTW,0.27534965034965037,"difference expectation either. Lastly, due to the lack of robustness to score scales and outliers,
220"
EXPERIMENTS OF NTW,0.2762237762237762,"standard, normalized, and truncated W-distance, denoted as dW, dNW, and dTW respectively, can
221"
EXPERIMENTS OF NTW,0.2770979020979021,"not indicate the coverage difference expectation as well as dNTW. It also displays the average and
222"
EXPERIMENTS OF NTW,0.27797202797202797,"standard deviation of the Pearson coefficient of the proposed NTW and six baselines. NTW not only
223"
EXPERIMENTS OF NTW,0.27884615384615385,"has the highest average Pearson coefficient but also has the lowest standard deviation, which means
224"
EXPERIMENTS OF NTW,0.27972027972027974,"the correlation between NTW and the coverage difference expectation caused by concept shift is
225"
EXPERIMENTS OF NTW,0.28059440559440557,"very stable. In Figure 3 and Figure 4, we also visually show the correlation between the expected
226"
EXPERIMENTS OF NTW,0.28146853146853146,"coverage difference under concept shift and each metric. We refer to Appendix E for detailed analysis.
227"
EXPERIMENTS OF NTW,0.28234265734265734,"This observation suggests the potential of incorporating NTW in the training process, leading to the
228"
EXPERIMENTS OF NTW,0.28321678321678323,"development of the mRCP approach. By applying the NTW metric, mRCP aims to enhance coverage
229"
EXPERIMENTS OF NTW,0.2840909090909091,"robustness in test domains.
230"
EXPERIMENTS OF MRCP,0.28496503496503495,"5.3
Experiments of mRCP
231"
EXPERIMENTS OF MRCP,0.28583916083916083,"Since we prove NTW can assess expected coverage difference under concept shift effectively, mRCP
232"
EXPERIMENTS OF MRCP,0.2867132867132867,"is designed to minimize it during training. In this case, validation sets are unnecessary, and we only
233"
EXPERIMENTS OF MRCP,0.2875874125874126,"draw training, and test sets from Q(e)
XY . Again, we draw one calibration set from PXY . The model fθ
234"
EXPERIMENTS OF MRCP,0.28846153846153844,"can also be an MLP or PDE based on different experiment setups. The loss function l is the ℓ1 norm.
235"
EXPERIMENTS OF MRCP,0.2893356643356643,"We implement mRCP according to Algorithm 1.
236"
EXPERIMENTS OF MRCP,0.2902097902097902,"Baselines: Two methods of optimization with out-of-distribution data are selected as baselines. DRO
237"
EXPERIMENTS OF MRCP,0.2910839160839161,"in Eq. (20) by [26] follows the minimax principle to reduce the highest L(e) to obtain fair prediction
238"
EXPERIMENTS OF MRCP,0.291958041958042,"among test distributions. On the other hand, V-REx in Eq. (21), introduced by [18], focuses on
239"
EXPERIMENTS OF MRCP,0.2928321678321678,"reducing the variance of L(e) to obtain fairness. As we include importance weighting in mRCP, we
240"
EXPERIMENTS OF MRCP,0.2937062937062937,"do not take it as a baseline, and the effectiveness of importance weighting is discussed in Section 6.
241"
EXPERIMENTS OF MRCP,0.2945804195804196,"LDRO(θ) = max
e∈E L(e).
(20) 242"
EXPERIMENTS OF MRCP,0.29545454545454547,"LV-REx(θ) =
X"
EXPERIMENTS OF MRCP,0.29632867132867136,"e∈E L(e) + β Var(L(e) | e ∈E).
(21)"
EXPERIMENTS OF MRCP,0.2972027972027972,"Metric: Denote E′
e[Eα[D(e)]] the expectation of coverage difference over confidence levels
243"
EXPERIMENTS OF MRCP,0.2980769230769231,"and test domains and E′
e[L(e)] the expectation of prediction residual over test domains. The
244"
EXPERIMENTS OF MRCP,0.29895104895104896,"two expectations become smaller means the algorithm’s performance is better. Both values are
245"
EXPERIMENTS OF MRCP,0.29982517482517484,"normalized by the corresponding results from the same experiment setup trained by ERM. Changing
246"
EXPERIMENTS OF MRCP,0.3006993006993007,"the weight β in Eq. (19) will draw a Pareto front, thus we want the Pareto front closer to the origin.
247"
EXPERIMENTS OF MRCP,0.30157342657342656,"Since V-REX is also controlled by a hyperparameter, we draw Pareto fronts for it as well.
248"
EXPERIMENTS OF MRCP,0.30244755244755245,"Result: Figure 2 displays the Pareto fronts for mRCP, DRO, and V-REx, highlighting the trade-offs
249"
EXPERIMENTS OF MRCP,0.30332167832167833,"between prediction residual and coverage difference expectation across different models and datasets.
250"
EXPERIMENTS OF MRCP,0.3041958041958042,"Figure 2, (a) shows the results for the airfoil self-noise dataset when trained with a Multilayer
251"
EXPERIMENTS OF MRCP,0.30506993006993005,"Perceptron (MLP) model. The mRCP method achieves a more favorable Pareto front compared to
252"
EXPERIMENTS OF MRCP,0.30594405594405594,"V-REx, indicating a better balance between prediction residual and coverage difference expectation.
253"
EXPERIMENTS OF MRCP,0.3068181818181818,"Additionally, mRCP attains a lower normalized coverage difference expectation than DRO at a
254"
EXPERIMENTS OF MRCP,0.3076923076923077,"comparable level of the prediction residual. In Figure 2, (b), we observe the experiment results on
255"
EXPERIMENTS OF MRCP,0.30856643356643354,"the epidemic spread prediction task using three epidemic datasets. With the same MLP architecture,
256"
EXPERIMENTS OF MRCP,0.3094405594405594,"mRCP delivers superior Pareto fronts relative to the baselines. When employing the epidemic PDE,
257"
EXPERIMENTS OF MRCP,0.3103146853146853,"the SIR model only has two trainable parameters, so their data points can not compose Pareto curves
258"
EXPERIMENTS OF MRCP,0.3111888111888112,"due to the model’s limited flexibility. Thus, we show the average of these points. Despite this
259"
EXPERIMENTS OF MRCP,0.3120629370629371,"limitation, mRCP maintains its advantage over the baseline methods. Figure 2, (c) and (d) present
260"
EXPERIMENTS OF MRCP,0.3129370629370629,"results from the traffic prediction task on three different traffic datasets. Here, the Pareto curves
261"
EXPERIMENTS OF MRCP,0.3138111888111888,"for both the MLP and the reaction-diffusion (RD) PDE model are well-defined, because RD model
262"
EXPERIMENTS OF MRCP,0.3146853146853147,"with six parameters, offers greater adaptability, allowing for clearer Pareto fronts. Overall, Figure 2
263"
EXPERIMENTS OF MRCP,0.3155594405594406,"collectively indicates that mRCP consistently achieves lower coverage difference expectations without
264"
EXPERIMENTS OF MRCP,0.31643356643356646,"compromising prediction residual as significantly as DRO and V-REx in different tasks and datasets.
265"
DISCUSSION,0.3173076923076923,"6
Discussion
266"
DISCUSSION,0.3181818181818182,"mRCP can distinguish coverage differences under concept shift and covariate shift. A notable
267"
DISCUSSION,0.31905594405594406,"feature of the mRCP Pareto curves depicted in Figure 2 is their results when β is small, which are not
268"
DISCUSSION,0.31993006993006995,"at E′
e[Eα[D(e)]] = 1, unlike the Pareto curves of V-REx. This is because, during training, mRCP
269"
DISCUSSION,0.3208041958041958,"Figure 2: Pareto fronts of Multi-domain Robust Conformal Prediction(mRCP), compared with DRO and
V-REx: Experimental results of (a) airfoil self-noise example, (b) epidemic spread prediction, and (c) (d) traffic
speed prediction. mRCP always reaches a smaller coverage difference expectation than DRO and V-REx with
less increase in prediction residual. Red boxes in (b) are zoomed-in areas. Shadow areas and error bars indicate
the standard error."
DISCUSSION,0.32167832167832167,"has considered the coverage difference under covariate shift by applying importance weighting to
270"
DISCUSSION,0.32255244755244755,"calibration conformal score CDF. Consequently, as β in Eq. (19) increases, the NTW term is only
271"
DISCUSSION,0.32342657342657344,"trained to mitigate the coverage difference under the concept shift, as shown in Figure 2,(a).
272"
DISCUSSION,0.3243006993006993,"DRO and V-REx are defeated because of improper selection of optimization metrics. Examining
273"
DISCUSSION,0.32517482517482516,"Eq. (20) and Eq. (21), we can see both baselines aim to promote fairness by equalizing the expected
274"
DISCUSSION,0.32604895104895104,"losses across different domains. As the loss function is ℓ1 norm, which is identical to how conformal
275"
DISCUSSION,0.3269230769230769,"scores are calculated, the experiment results of ∆E in the last row of Figure 3 show this metric is
276"
DISCUSSION,0.3277972027972028,"ineffective in capturing the coverage difference due to concept shift.
277"
DISCUSSION,0.32867132867132864,"Nonetheless, mRCP’s limitations arise from the inherent challenges associated with penalty-
278"
DISCUSSION,0.32954545454545453,"based optimization algorithms. Whether it is mRCP or V-REx, penalty-based optimization algo-
279"
DISCUSSION,0.3304195804195804,"rithms necessitate a model with a high capacity for fitting complex patterns. For instance, in Figure 2,
280"
DISCUSSION,0.3312937062937063,"(b), the Pareto curves are not discernible when predictions are derived from an epidemic PDE (SIR
281"
DISCUSSION,0.3321678321678322,"model) with only two adjustable parameters. In contrast, as shown in Figure 2, (d), the traffic PDE
282"
DISCUSSION,0.333041958041958,"(RD model) demonstrates greater flexibility and adaptability with six tunable parameters, exhibiting
283"
DISCUSSION,0.3339160839160839,"distinct Pareto curves.
284"
CONCLUSION,0.3347902097902098,"7
Conclusion
285"
CONCLUSION,0.3356643356643357,"This study begins by decomposing the coverage difference caused by covariate and concept shifts.
286"
CONCLUSION,0.33653846153846156,"We then introduce the Normalized Truncated Wasserstein distance (NTW) as a metric for capturing
287"
CONCLUSION,0.3374125874125874,"coverage difference expectation under concept shift by comparing the test and weighted calibration
288"
CONCLUSION,0.3382867132867133,"conformal score CDFs. This metric can indicate the discrepancy position in calibration and test score
289"
CONCLUSION,0.33916083916083917,"distributions. Normalization and truncation make the metric score scales and outliers. Finally, we
290"
CONCLUSION,0.34003496503496505,"develop an end-to-end algorithm called Multi-domain Robust Conformal Prediction (mRCP) that
291"
CONCLUSION,0.3409090909090909,"incorporates NTW during training, allowing coverage to approach confidence in all test domains.
292"
REFERENCES,0.34178321678321677,"References
293"
REFERENCES,0.34265734265734266,"[1] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk mini-
294"
REFERENCES,0.34353146853146854,"mization. arXiv preprint arXiv:1907.02893, 2019.
295"
REFERENCES,0.34440559440559443,"[2] Mrinal R Bachute and Javed M Subhedar. Autonomous driving architectures: insights of
296"
REFERENCES,0.34527972027972026,"machine learning and deep learning algorithms. Machine Learning with Applications, 6:100164,
297"
REFERENCES,0.34615384615384615,"2021.
298"
REFERENCES,0.34702797202797203,"[3] Leonardo Bellocchi and Nikolas Geroliminis. Unraveling reaction-diffusion-like dynamics in
299"
REFERENCES,0.3479020979020979,"urban congestion propagation: Insights from a large-scale road network. Scientific reports,
300"
REFERENCES,0.3487762237762238,"10(1):4876, 2020.
301"
REFERENCES,0.34965034965034963,"[4] Azzedine Boukerche and Jiahao Wang. Machine learning-based traffic prediction models for
302"
REFERENCES,0.3505244755244755,"intelligent transportation systems. Computer Networks, 181:107530, 2020.
303"
REFERENCES,0.3513986013986014,"[5] Pope D. Brooks, Thomas and Michael Marcolini. Airfoil Self-Noise. UCI Machine Learning
304"
REFERENCES,0.3522727272727273,"Repository, 2014. DOI: https://doi.org/10.24432/C5VW2C.
305"
REFERENCES,0.3531468531468531,"[6] Maxime Cauchois, Suyash Gupta, Alnur Ali, and John C Duchi. Robust validation: Confident
306"
REFERENCES,0.354020979020979,"predictions even when distributions shift. Journal of the American Statistical Association, pages
307"
REFERENCES,0.3548951048951049,"1–66, 2024.
308"
REFERENCES,0.3557692307692308,"[7] Maxime Cauchois, Suyash Gupta, and John C Duchi. Knowing what you know: valid and
309"
REFERENCES,0.35664335664335667,"validated confidence sets in multiclass and multilabel prediction. Journal of machine learning
310"
REFERENCES,0.3575174825174825,"research, 22(81):1–42, 2021.
311"
REFERENCES,0.3583916083916084,"[8] Ian Cooper, Argha Mondal, and Chris G Antonopoulos. A sir model assumption for the spread
312"
REFERENCES,0.35926573426573427,"of covid-19 in different communities. Chaos, Solitons & Fractals, 139:110057, 2020.
313"
REFERENCES,0.36013986013986016,"[9] Zhiyong Cui, Kristian Henrickson, Ruimin Ke, and Yinhai Wang. Traffic graph convolutional
314"
REFERENCES,0.361013986013986,"recurrent neural network: A deep learning framework for network-scale traffic learning and
315"
REFERENCES,0.3618881118881119,"forecasting. IEEE Transactions on Intelligent Transportation Systems, 2019.
316"
REFERENCES,0.36276223776223776,"[10] Songgaojun Deng, Shusen Wang, Huzefa Rangwala, Lijing Wang, and Yue Ning. Cola-gnn:
317"
REFERENCES,0.36363636363636365,"Cross-location attention based graph neural networks for long-term ili prediction. In Proceedings
318"
REFERENCES,0.36451048951048953,"of the 29th ACM international conference on information & knowledge management, pages
319"
REFERENCES,0.36538461538461536,"245–254, 2020.
320"
REFERENCES,0.36625874125874125,"[11] Shai Feldman, Stephen Bates, and Yaniv Romano. Improving conditional coverage via orthog-
321"
REFERENCES,0.36713286713286714,"onal quantile regression. Advances in neural information processing systems, 34:2060–2071,
322"
REFERENCES,0.368006993006993,"2021.
323"
REFERENCES,0.3688811188811189,"[12] Robert E. Gaunt and Siqi Li. Bounding kolmogorov distances through wasserstein and related
324"
REFERENCES,0.36975524475524474,"integral probability metrics, 2022.
325"
REFERENCES,0.3706293706293706,"[13] Amanda Gentzel, Dan Garant, and David Jensen. The case for evaluating causal models
326"
REFERENCES,0.3715034965034965,"using interventional measures and empirical data. Advances in Neural Information Processing
327"
REFERENCES,0.3723776223776224,"Systems, 32, 2019.
328"
REFERENCES,0.3732517482517482,"[14] Subhankar Ghosh, Yuanjie Shi, Taha Belkhouja, Yan Yan, Jana Doppa, and Brian Jones.
329"
REFERENCES,0.3741258741258741,"Probabilistically robust conformal prediction. In Uncertainty in Artificial Intelligence, pages
330"
REFERENCES,0.375,"681–690. PMLR, 2023.
331"
REFERENCES,0.3758741258741259,"[15] Isaac Gibbs, John J Cherian, and Emmanuel J Candès. Conformal prediction with conditional
332"
REFERENCES,0.3767482517482518,"guarantees. arXiv preprint arXiv:2305.12616, 2023.
333"
REFERENCES,0.3776223776223776,"[16] Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. Attention based spatial-
334"
REFERENCES,0.3784965034965035,"temporal graph convolutional networks for traffic flow forecasting. In Proceedings of the AAAI
335"
REFERENCES,0.3793706293706294,"Conference on Artificial Intelligence, volume 33, pages 922–929, 2019.
336"
REFERENCES,0.38024475524475526,"[17] Wouter M Kouw and Marco Loog. An introduction to domain adaptation and transfer learning.
337"
REFERENCES,0.3811188811188811,"arXiv preprint arXiv:1812.11806, 2018.
338"
REFERENCES,0.381993006993007,"[18] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai
339"
REFERENCES,0.38286713286713286,"Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrap-
340"
REFERENCES,0.38374125874125875,"olation (rex). In International Conference on Machine Learning, pages 5815–5826. PMLR,
341"
REFERENCES,0.38461538461538464,"2021.
342"
REFERENCES,0.38548951048951047,"[19] Sara Magliacane, Thijs Van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, and
343"
REFERENCES,0.38636363636363635,"Joris M Mooij. Domain adaptation by using causal inference to predict invariant conditional
344"
REFERENCES,0.38723776223776224,"distributions. Advances in neural information processing systems, 31, 2018.
345"
REFERENCES,0.3881118881118881,"[20] Pascal Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality. The annals of
346"
REFERENCES,0.388986013986014,"Probability, pages 1269–1283, 1990.
347"
REFERENCES,0.38986013986013984,"[21] Harsh Parikh, Carlos Varjao, Louise Xu, and Eric Tchetgen Tchetgen. Validating causal
348"
REFERENCES,0.39073426573426573,"inference methods. In International conference on machine learning, pages 17346–17358.
349"
REFERENCES,0.3916083916083916,"PMLR, 2022.
350"
REFERENCES,0.3924825174825175,"[22] Yaniv Romano, Evan Patterson, and Emmanuel J. Candès. Conformalized quantile regression.
351"
REFERENCES,0.39335664335664333,"In Neural Information Processing Systems, 2019.
352"
REFERENCES,0.3942307692307692,"[23] Yaniv Romano, Matteo Sesia, and Emmanuel J. Candès. Classification with valid and adaptive
353"
REFERENCES,0.3951048951048951,"coverage. arXiv: Methodology, 2020.
354"
REFERENCES,0.395979020979021,"[24] Nathan Ross. Fundamentals of stein’s method. 2011.
355"
REFERENCES,0.3968531468531469,"[25] Hyun-Sun Ryu and Kwang Sun Ko. Sustainable development of fintech: Focused on uncertainty
356"
REFERENCES,0.3977272727272727,"and perceived quality issues. Sustainability, 12(18):7669, 2020.
357"
REFERENCES,0.3986013986013986,"[26] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally
358"
REFERENCES,0.3994755244755245,"robust neural networks for group shifts: On the importance of regularization for worst-case
359"
REFERENCES,0.40034965034965037,"generalization. arXiv preprint arXiv:1911.08731, 2019.
360"
REFERENCES,0.4012237762237762,"[27] Silvia Seoni, Vicnesh Jahmunah, Massimo Salvi, Prabal Datta Barua, Filippo Molinari, and
361"
REFERENCES,0.4020979020979021,"U Rajendra Acharya. Application of uncertainty quantification to artificial intelligence in
362"
REFERENCES,0.40297202797202797,"healthcare: A review of last decade (2013–2023). Computers in Biology and Medicine, page
363"
REFERENCES,0.40384615384615385,"107441, 2023.
364"
REFERENCES,0.40472027972027974,"[28] Yue Sun, Chao Chen, Yuesheng Xu, Sihong Xie, Rick S Blum, and Parv Venkitasubramaniam.
365"
REFERENCES,0.40559440559440557,"Reaction-diffusion graph ordinary differential equation networks: Traffic-law-informed speed
366"
REFERENCES,0.40646853146853146,"prediction under mismatched data. The 12th International Workshop on Urban Computing, held
367"
REFERENCES,0.40734265734265734,"in conjunction with ..., 2023.
368"
REFERENCES,0.40821678321678323,"[29] Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal
369"
REFERENCES,0.4090909090909091,"prediction under covariate shift. Advances in neural information processing systems, 32, 2019.
370"
REFERENCES,0.40996503496503495,"[30] Vladimir Vapnik. Principles of risk minimization for learning theory. Advances in neural
371"
REFERENCES,0.41083916083916083,"information processing systems, 4, 1991.
372"
REFERENCES,0.4117132867132867,"[31] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random
373"
REFERENCES,0.4125874125874126,"world, volume 29. Springer, 2005.
374"
REFERENCES,0.41346153846153844,"[32] Timothy L Wiemken and Robert R Kelley. Machine learning in epidemiology and health
375"
REFERENCES,0.4143356643356643,"outcomes research. Annu Rev Public Health, 41(1):21–36, 2020.
376"
REFERENCES,0.4152097902097902,"[33] Xin Zou and Weiwei Liu. Coverage-guaranteed prediction sets for out-of-distribution data. In
377"
REFERENCES,0.4160839160839161,"Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17263–17270,
378"
REFERENCES,0.416958041958042,"2024.
379"
REFERENCES,0.4178321678321678,"A
Related work
380"
REFERENCES,0.4187062937062937,Table 2: Related works and mRCP
REFERENCES,0.4195804195804196,"Task
Number of
Test Domains
Test Domain Property
Work"
REFERENCES,0.42045454545454547,"Adaptive Conformal Prediction
under Exchangeability
1
Identical to Calibration Domain
[22, 23]"
REFERENCES,0.42132867132867136,"Conformal Prediction
under Covariate Shift
1
Covariate Shift
[29, 14]"
REFERENCES,0.4222027972027972,"Multi-Domain Conformal Prediction
Multiple"
REFERENCES,0.4230769230769231,"Feature-stratified
[7, 11]
Covariate Shift
[15]
Joint Distribution Shift in Certain
F-divergence Range
[33, 6]"
REFERENCES,0.42395104895104896,"Joint Distribution Shift
mRCP"
REFERENCES,0.42482517482517484,"B
Error bound for the assumption of identical coverages
381"
REFERENCES,0.4256993006993007,"According to the computation of q and q∗in Eq. (1) and Eq. (6), respectively, we can define the
382"
REFERENCES,0.42657342657342656,"coverages in unweighted and weighted calibration score distributions as
383"
REFERENCES,0.42744755244755245,"ˆFP (q) = inf
n
ˆFP (vi)| ˆFP (vi) ≥⌈(1 −α)(n + 1)⌉/n, vi ∈Vc
o
, 384"
REFERENCES,0.42832167832167833,"ˆFQ/P (q∗) = inf
n
ˆFQ/P (vi)| ˆFQ/P (vi) ≥⌈(1 −α)(n + 1)⌉/n, vi ∈Vc
o
."
REFERENCES,0.4291958041958042,"Denoting q+ = inf{vi|vi ∈Vc, vi > q} and q∗
+ = inf{vi|vi ∈Vc, vi > q∗}, we can bound ˆFP (q)
385"
REFERENCES,0.43006993006993005,"and ˆFQ/P (q∗) as
386"
REFERENCES,0.43094405594405594,"ˆFP (q) ∈
h
⌈(1 −α)(n + 1)⌉/n, ˆFP (q+)

,
ˆFQ/P (q∗) ∈
h
⌈(1 −α)(n + 1)⌉/n, ˆFQ/P (q∗
+)

."
REFERENCES,0.4318181818181818,"Therefore, the absolute difference between ˆF ∗(q∗) and ˆF(q) is bounded by
387"
REFERENCES,0.4326923076923077,"| ˆFQ/P (q∗) −ˆFP (q)| < max

ˆFQ/P (q∗
+) −⌈(1 −α)(n + 1)⌉/n, ˆFP (q+) −⌈(1 −α)(n + 1)⌉/n

."
REFERENCES,0.43356643356643354,"Especially, when the calibration set size n is large enough (like having thousands of samples),
388"
REFERENCES,0.4344405594405594,"ˆFQ/P and ˆFP will be quite smooth, the upper above will be even negligible, allowing us to assume
389"
REFERENCES,0.4353146853146853,"ˆFQ/P (q∗) = ˆFP (q).
390"
REFERENCES,0.4361888111888112,"C
Upper bound of coverage difference under concept shift
391"
REFERENCES,0.4370629370629371,"In this section, we prove that the W-distance between a test and weighted calibration conformal score
392"
REFERENCES,0.4379370629370629,"population CDF can establish an upper bound for coverage difference under concept shift.
393"
REFERENCES,0.4388111888111888,"As D quantifies the absolute difference between ˆFQ/P and ˆFQ at a calibration conformal score, it
394"
REFERENCES,0.4396853146853147,"can be constrained by an upper bound given by the Kolmogorov distance [12] defined as follows.
395"
REFERENCES,0.4405594405594406,"Definition 2 (Kolmogorov Distance). If F1 and F2 are two cumulative distribution functions (CDFs),
396"
REFERENCES,0.44143356643356646,"the Kolmogorov distance, dK, is defined as the maximum absolute difference between the CDFs.
397"
REFERENCES,0.4423076923076923,"dK(F1, F2) = sup
v∈R
|F1(v) −F2(v)|."
REFERENCES,0.4431818181818182,"As ˆFQ/P and ˆFQ are empirical (not population) CDFs of weighted calibration and test conformal
398"
REFERENCES,0.44405594405594406,"scores, the bounding relationship can be reformulated as
399"
REFERENCES,0.44493006993006995,"dK( ˆFQ, ˆFQ/P ) =
sup
v∈Vc∪Vt
| ˆFQ(v) −ˆFQ/P (v)| ≥sup
v∈Vc
| ˆFQ(v) −ˆFQ/P (v)| = sup
v∈Vc
|D(v)|.
(22)"
REFERENCES,0.4458041958041958,"The upper bound dK( ˆFQ, ˆFQ/P ) depends on the two conformal score sets Vc and Vt, indicating that
400"
REFERENCES,0.44667832167832167,"the inclusion of samples in Sc and St is likely to introduce variability in dK( ˆFQ, ˆFQ/P ). Nevertheless,
401"
REFERENCES,0.44755244755244755,"we aim for an upper bound that is not reliant on specific samples and relies on the calibration and test
402"
REFERENCES,0.44842657342657344,"conformal score population CDFs, FP and FQ.
403"
REFERENCES,0.4493006993006993,"Firstly, we convert the upper limit in Eq. (22) into terms of FP and FQ. Denoting the joint probability
404"
REFERENCES,0.45017482517482516,"density function (PDF) of features and score in the calibration and test domain as pXV and qXV
405"
REFERENCES,0.45104895104895104,"respectively, the corresponding continuous CDFs of conformal scores are illustrated as
406"
REFERENCES,0.4519230769230769,"FP (v) =
Z v 0 Z"
REFERENCES,0.4527972027972028,"X
pXV (u, t)dudt, FQ(v) =
Z v 0 Z"
REFERENCES,0.45367132867132864,"X
qXV (u, t)dudt,
(23)"
REFERENCES,0.45454545454545453,"where X is the space of the feature variable X.
407"
REFERENCES,0.4554195804195804,"PDFs of features in calibration and test domains, denoted as pX and qX respectively, are defined as
408"
REFERENCES,0.4562937062937063,"pX =
Z"
REFERENCES,0.4571678321678322,"R
pXV (u, t)dt, qX =
Z"
REFERENCES,0.458041958041958,"R
qXV (u, t)dt.
(24)"
REFERENCES,0.4589160839160839,"To address the coverage difference due to covariate shift, importance weighting from [29] is rewritten
409"
REFERENCES,0.4597902097902098,as w = qX
REFERENCES,0.4606643356643357,"pX . Also, normalization is unnecessary, because w here is a correction function to transform
410"
REFERENCES,0.46153846153846156,"the marginal distribution of pX into qX. The weighted version of pXV is denoted as p′
XV =
411"
REFERENCES,0.4624125874125874,"wpXV = qXpV |X, which can be applied to derive the weighted continuous CDF of calibration
412"
REFERENCES,0.4632867132867133,"conformal score by
413"
REFERENCES,0.46416083916083917,"FQ/P (v) =
Z v 0 Z"
REFERENCES,0.46503496503496505,"X
p′
XV (u, t)dudt =
Z v 0 Z"
REFERENCES,0.4659090909090909,"X
qX(u)pV |X(u, t)dudt.
(25)"
REFERENCES,0.46678321678321677,"The Kolmogorov distance between FQ/P and FQ is dK(FQ, FQ/P ) = supv∈R |FQ(v) −FQ/P (v)|.
414"
REFERENCES,0.46765734265734266,"Theorem 1 (Triangular Inequality for Kolmogorov Distance). If F1, F2, and F3 are three cumulative
415"
REFERENCES,0.46853146853146854,"distribution functions (CDFs), their Kolmogorov distances follow this inequality:
416"
REFERENCES,0.46940559440559443,"dK(F1, F3) ≤dK(F1, F2) + dK(F2, F3)."
REFERENCES,0.47027972027972026,"Proof. Consider any point x ∈R, then we have |F1(x) −F3(x)| ≤|F1(x) −F2(x)| + |F2(x) −
417"
REFERENCES,0.47115384615384615,"F3(x)|.This inequality holds due to the triangle inequality for absolute values. Now, taking the supre-
418"
REFERENCES,0.47202797202797203,"mum over all x, we have supx∈R |F1(x) −F3(x)| ≤supx∈R (|F1(x) −F2(x)| + |F2(x) −F3(x)|).
419"
REFERENCES,0.4729020979020979,"Note that the right-hand side is not necessarily equal to the sum of the suprema of the individ-
420"
REFERENCES,0.4737762237762238,"ual terms, because the points at which the suprema of |F1(x) −F2(x)| and |F2(x) −F3(x)|
421"
REFERENCES,0.47465034965034963,"are attained may be different. However, we know that for any x, |F1(x) −F2(x)| is at most
422"
REFERENCES,0.4755244755244755,"dK(F1, F2) and |F2(x) −F3(x)| is at most dK(F2, F3). Therefore, supx∈R |F1(x) −F3(x)| ≤
423"
REFERENCES,0.4763986013986014,"dK(F1, F2) + dK(F2, F3). Since the left-hand side is the definition of dK(F1, F3), we can demon-
424"
REFERENCES,0.4772727272727273,"strate that dK(F1, F3) ≤dK(F1, F2) + dK(F2, F3).
425"
REFERENCES,0.4781468531468531,"As Kolmogorov distance satisfies the triangular inequality theorem, as shown and proved in Theo-
426"
REFERENCES,0.479020979020979,"rem 1, the triangular inequality relationship can be expanded to
427"
REFERENCES,0.4798951048951049,"dK( ˆFQ, ˆFQ/P ) ≤dK(FQ/P , ˆFQ/P ) + dK(FQ, FQ/P ) + dK( ˆFQ, FQ).
(26)"
REFERENCES,0.4807692307692308,"Secondly, the Kolmogorov distance between an empirical CDF and its corresponding population CDF
428"
REFERENCES,0.48164335664335667,"can be constrained by Dvoretzky–Kiefer–Wolfowitz (DKW) inequality [20], defined in Definition 3.
429"
REFERENCES,0.4825174825174825,"Definition 3 (Dvoretzky–Kiefer–Wolfowitz (DKW) Inequality). If F is a population cumulative
430"
REFERENCES,0.4833916083916084,"distribution function (CDF), and ˆF is an empirical CDF with n samples of a random variable X,
431"
REFERENCES,0.48426573426573427,"then for any ϵ ≥
q"
REFERENCES,0.48513986013986016,"1
2n ln 2, the following inequality holds.
432"
REFERENCES,0.486013986013986,"Pr(dK( ˆF, F) > ϵ) ≤e−2nϵ2."
REFERENCES,0.4868881118881119,"Based on Definition 3, saying |Vc| = n and |Vt| = m, we can apply DKW inequality to
433"
REFERENCES,0.48776223776223776,"dK( ˆFQ/P , FQ/P ) and dK( ˆFQ, FQ) as follows, for ϵ ≥
q"
REFERENCES,0.48863636363636365,"1
2n ln 2 and ρ ≥
q"
REFERENCES,0.48951048951048953,"1
2m ln 2.
434"
REFERENCES,0.49038461538461536,"Pr(dK( ˆFQ/P , FQ/P ) ≤ϵ) > e−2nϵ2, Pr(dK( ˆFQ, FQ) ≤ρ) > e−2mρ2."
REFERENCES,0.49125874125874125,"If the two events dK( ˆFQ/P , FQ/P ) < ϵ and dK( ˆFQ, FQ) < ρ are independent, the inequality in
435"
REFERENCES,0.49213286713286714,"Eq. (26) can be expanded in Eq. (27), which holds with at least probability e−2(nϵ2+mρ2). By applying
436"
REFERENCES,0.493006993006993,"DKW inequality, we successfully quantify the variability of dK( ˆFQ, ˆFQ/P ) in Eq. (22) as a form of a
437"
REFERENCES,0.4938811188811189,"probable event, and use the population conformal score CDFs to limit the worst-case of coverage
438"
REFERENCES,0.49475524475524474,"difference under concept shift.
439"
REFERENCES,0.4956293706293706,"dK( ˆFQ, ˆFQ/P ) ≤dK(FQ, FQ/P ) + ρ + ϵ.
(27)"
REFERENCES,0.4965034965034965,"Finally, having established in Eq. (13) that the W-distance can serve as an estimator for coverage
440"
REFERENCES,0.4973776223776224,"difference expectation, we explore whether Eq. (27) may similarly be bounded by this metric. The
441"
REFERENCES,0.4982517482517482,"W-distance of the two population conformal score CDFs are explicitly shown as
442"
REFERENCES,0.4991258741258741,"dW(FQ, FQ/P ) =
Z R"
REFERENCES,0.5,"FQ(v) −FQ/P (v)
dv =
Z R  Z v 0 Z"
REFERENCES,0.5008741258741258,"R
qXV (u, t)dudt −
Z v 0 Z"
REFERENCES,0.5017482517482518,"R
p′
XV (u, t)dudt
dv =
Z R  Z v 0 Z"
REFERENCES,0.5026223776223776,"R
qXV (u, t)dudt −
Z v 0 Z"
REFERENCES,0.5034965034965035,"R
qX(u)pV |X(u, t)dudt
dv (28)"
REFERENCES,0.5043706293706294,"According to [24], if the weighted calibration conformal score probability density function (PDF) has
443"
REFERENCES,0.5052447552447552,"Lebesgue density bounded by C, which means p′
V does not exceed C, then for any test conformal
444"
REFERENCES,0.5061188811188811,"score PDF qV , dK(FQ, FQ/P ) can be bounded as
445"
REFERENCES,0.506993006993007,"dK(FQ, FQ/P ) ≤
q"
REFERENCES,0.5078671328671329,"2CdW(FQ, FQ/P )
(29)"
REFERENCES,0.5087412587412588,"Finally, we can derive the upper limit of coverage difference under concept shift, supv∈Vc |D(v)|, in
446"
REFERENCES,0.5096153846153846,"Eq. (30) at least probability e−2(nϵ2+mρ2).
447"
REFERENCES,0.5104895104895105,"sup
v∈Vc
|D(v)| ≤dK( ˆFQ, ˆFQ/P ) ≤
q"
REFERENCES,0.5113636363636364,"2CdW(FQ, FQ/P ) + ϵ + ρ
(30)"
REFERENCES,0.5122377622377622,"This property is attractive in that the maximum difference in coverage due to concept shift can also
448"
REFERENCES,0.5131118881118881,"be constrained in relation to the W-distance of population score CDFs, denoted as dW(FQ, FQ/P ).
449"
REFERENCES,0.513986013986014,"Despite the unobservability of dW(FQ, FQ/P ), we can still estimate it using its empirical form,
450"
REFERENCES,0.5148601398601399,"dW( ˆFQ, ˆFQ/P ).
451"
REFERENCES,0.5157342657342657,"Even though coverage guarantee on an arbitrary joint shift is almost impossible, Eq. (28) demonstrates
452"
REFERENCES,0.5166083916083916,"robust conformal prediction is attainable if we can train a function reducing the discrepancy between
453"
REFERENCES,0.5174825174825175,"calibration and test conformal score distributions. To be specific, dW(FQ, FQ/P ) can be reduced to
454"
REFERENCES,0.5183566433566433,"zero as far as pV |X = qV |X. In other words, if we regard pXV and qXV as push-forward probability
455"
REFERENCES,0.5192307692307693,"distribution of PXV and QXV by the trained model f, making the concept shift between pV |X and
456"
REFERENCES,0.5201048951048951,"qV |X smaller will reduce coverage difference expectation on test domain.
457"
REFERENCES,0.5209790209790209,"D
Datasets, models, and experiment setups
458"
REFERENCES,0.5218531468531469,"Extensive experiments are conducted under 3 tasks with 7 datasets. Some tasks involve both black-box
459"
REFERENCES,0.5227272727272727,"and physics-informed models to demonstrate the generalizability of NTW and mRCP.
460"
REFERENCES,0.5236013986013986,"D.1
Airfoil self-noise example
461"
REFERENCES,0.5244755244755245,"The airfoil dataset from the UCI Machine Learning Repository [5] consists of 1503 instances of
462"
REFERENCES,0.5253496503496503,"1-dimensional target Y and 5-dimensional feature X = (X1, X2, X3, X4, X5). This dataset is
463"
REFERENCES,0.5262237762237763,"manually separated and modified to create three different domains.
464"
REFERENCES,0.5270979020979021,"Domain separation:
465"
REFERENCES,0.527972027972028,"Step 1. Covariate Shift by Data Separation. The original dataset is initially segmented into three
466"
REFERENCES,0.5288461538461539,"primary subsets A, B, C based on the 33% and 66% quantiles of the first dimension X1. Subsequently,
467"
REFERENCES,0.5297202797202797,"each of these subsets is further divided into three smaller portions at a 7:2:1 ratio, denoted like
468"
REFERENCES,0.5305944055944056,"A0.7, A0.2, A0.1 from A. Finally, we assemble three new datasets with covariate shift as S(e1) =
469"
REFERENCES,0.5314685314685315,"A0.7 ∪B0.2 ∪C0.1, S(e2) = A0.2 ∪B0.1 ∪C0.7, S(e3) = A0.2 ∪B0.1 ∪C0.2.
470"
REFERENCES,0.5323426573426573,"Step 2. Concept Shift by Target Modification. Differently distributed random noises are added to
471"
REFERENCES,0.5332167832167832,"target values to cause concept shifts. For yi from S(e1), yi+ = yi/1000 ∗τ; for yi from S(e2),
472"
REFERENCES,0.5340909090909091,"yi+ = yi/τ; for yi from S(e3), yi+ = τ. τ follows a normal distribution N(0, 102). Since we obtain
473"
REFERENCES,0.534965034965035,"three subsets in the end, |E| = 3.
474"
REFERENCES,0.5358391608391608,"Model selection:
475"
REFERENCES,0.5367132867132867,"We utilize a straightforward multilayer perceptron (MLP) as a trainable model, with an architecture
476"
REFERENCES,0.5375874125874126,"of (input dimension, 64, 64, 1) tailored for the regression task.
477"
REFERENCES,0.5384615384615384,"D.2
Traffic speed prediction
478"
REFERENCES,0.5393356643356644,"The Seattle-loop [9], and PeMSD4, PeMSED8 datasets [16] contain sensor-observed traffic volume
479"
REFERENCES,0.5402097902097902,"and speed data collected in Seattle, San Francisco, and San Bernardino. The snapshots from sensors
480"
REFERENCES,0.541083916083916,"are taken at 5-minute intervals. This task aims to predict the traffic speed of the local road segment in
481"
REFERENCES,0.541958041958042,"the next time step, using the traffic data from local and neighboring segments collected currently.
482"
REFERENCES,0.5428321678321678,"Domain separation:
483"
REFERENCES,0.5437062937062938,"Naturally, instances can be categorized into 24 subsets, |E = 24|, based on the hour they are obtained.
484"
REFERENCES,0.5445804195804196,"It is anticipated that there are joint shifts between the data distribution of every single hour (test
485"
REFERENCES,0.5454545454545454,"domains) and the data distribution of the whole day (calibration domain), as traffic patterns vary over
486"
REFERENCES,0.5463286713286714,"time, making it unnecessary to modify any data. We select the workday data from the three datasets.
487"
REFERENCES,0.5472027972027972,"Model selection:
488"
REFERENCES,0.5480769230769231,"(a) MLP with the same structure (input dimension, 64, 64, 1) is applied to the traffic prediction task.
489"
REFERENCES,0.548951048951049,"(b) The Reaction-Diffusion (RD) model is selected as the physics-informed Partial differential
490"
REFERENCES,0.5498251748251748,"equation (PDE) for traffic speed prediction. Reaction-diffusion mechanism, originally formulated for
491"
REFERENCES,0.5506993006993007,"chemical systems to describe particle dynamics, has been adapted for traffic analysis by [3] to uncover
492"
REFERENCES,0.5515734265734266,"traffic patterns on different road segments, offering an alternative to purely data-driven models like
493"
REFERENCES,0.5524475524475524,"long-short-term memory. [28] further advanced this approach by integrating the RD model into
494"
REFERENCES,0.5533216783216783,"graphical neural networks to capture traffic state interactions among adjacent road segments, with
495"
REFERENCES,0.5541958041958042,"the reaction term accounting for influences against traffic flow and the diffusion term for influences
496"
REFERENCES,0.5550699300699301,"along it. To be specific, for a given sensor i, with N d upstream and N r downstream neighboring
497"
REFERENCES,0.5559440559440559,"sensors, the traffic states from these sensors impact sensor i after δt time through diffusion and
498"
REFERENCES,0.5568181818181818,"reaction effects, respectively. We expand the original RD model in [28] to Eq. (31), where the traffic
499"
REFERENCES,0.5576923076923077,"speed and volume at sensor i at time t is ui(t) and qi(t), respectively. The parameters ρ(i,j) and σ(i,j)
500"
REFERENCES,0.5585664335664335,"represent the diffusion and reaction strengths between sensor i and sensor j, while their superscripts
501"
REFERENCES,0.5594405594405595,"indicate if they serve for speed or volume. Also, di and ri are bias terms for the two components.
502"
REFERENCES,0.5603146853146853,"ui(t + δt) −ui(t) =
X"
REFERENCES,0.5611888111888111,"j∈Nd
(ρu
(i,j)(ui(t) −uj(t)) + ρq
(i,j)(qi(t) −qj(t)) + di"
REFERENCES,0.5620629370629371,"+ tanh(
X"
REFERENCES,0.5629370629370629,"j∈Nr
σu
(i,j)(ui(t) −uj(t)) + σq
(i,j)(qi(t) −qj(t)) + ri).
(31)"
REFERENCES,0.5638111888111889,"D.3
Epidemic spread prediction
503"
REFERENCES,0.5646853146853147,"Three epidemic datasets, US-Regions, US-States, and Japan-Prefectures [10] include the number of
504"
REFERENCES,0.5655594405594405,"patients infected by influenza-like illness (ILI) recorded by U.S. Department of Health and Human
505"
REFERENCES,0.5664335664335665,"Services, Center for Disease Control and Prevention (CDC), and Japan Infectious Diseases Weekly
506"
REFERENCES,0.5673076923076923,"Report. We aim to use the local population, the rise in the number of infected patients observed this
507"
REFERENCES,0.5681818181818182,"week, and the cumulative total of infections as predictive features of the increase in infections for the
508"
REFERENCES,0.5690559440559441,"upcoming week.
509"
REFERENCES,0.5699300699300699,"Domain separation: According to the Pandemic Intervals Framework (PIF) by CDC, samples are
510"
REFERENCES,0.5708041958041958,"divided by four pandemic intervals, Initiation, Acceleration, Declaration, and Subsidence, so |E| = 4.
511"
REFERENCES,0.5716783216783217,"We establish the interval endpoints based on specific percentages of the total infected patient count,
512"
REFERENCES,0.5725524475524476,"specifically at the 15%, 50%, and 85% thresholds.
513"
REFERENCES,0.5734265734265734,"Model selection:
514"
REFERENCES,0.5743006993006993,"(a) MLP with the same architecture is utilized for the epidemic spread forecasting task as well.
515"
REFERENCES,0.5751748251748252,"(b) PDE for this task is the SIR model that categorizes the population into three groups: those
516"
REFERENCES,0.576048951048951,"susceptible to the disease S, those infectious I, and those who have recovered and gained immunity
517"
REFERENCES,0.5769230769230769,"R. It outlines the temporal changes in their populations, as described by [8]. The governing
518"
REFERENCES,0.5777972027972028,"differential equations can be expressed as Eq. 32, where N, λ, and γ represent the total population,
519"
REFERENCES,0.5786713286713286,"infection rate, and recovery rate, respectively.
520 

 
 dS(t)"
REFERENCES,0.5795454545454546,"dt
= −λS(t)I(t)"
REFERENCES,0.5804195804195804,"N
,
dI(t)"
REFERENCES,0.5812937062937062,"dt
= λS(t)I(t)"
REFERENCES,0.5821678321678322,"N
−γI(t) = ( λS(t)"
REFERENCES,0.583041958041958,"N
−γ)I(t),
dR(t)"
REFERENCES,0.583916083916084,"dt
= γI(t).
(32)"
REFERENCES,0.5847902097902098,"We make the assumption that the location is isolated, hence N = S(t) + I(t) + R(t). Additionally,
521"
REFERENCES,0.5856643356643356,"the population of recovered individuals is represented by R(t) = γ
R t
0 I(t)dt. Given this, if to
522"
REFERENCES,0.5865384615384616,"signifies the initial time of the current epidemic and δt denotes the time step, which is a week in the
523"
REFERENCES,0.5874125874125874,"three datasets, we can express the dynamic change of infectious individuals discretely as Eq. (33).
524"
REFERENCES,0.5882867132867133,I(t + δt) −I(t) =
REFERENCES,0.5891608391608392,"λ(N −I(t) −γ Pt
to I(t))
N
−γ !"
REFERENCES,0.590034965034965,"I(t).
(33)"
REFERENCES,0.5909090909090909,"D.4
Experiment setups for NTW and baseline metrics
525"
REFERENCES,0.5917832167832168,"As we only need to validate the positive correlation between NTW and coverage difference ex-
526"
REFERENCES,0.5926573426573427,"pectation, all models are trained by ERM. In the airfoil self-noise example, 100 trials are carried
527"
REFERENCES,0.5935314685314685,"out. For the traffic task, 61 locations from the Seattle-loop, 59 locations from PeMSD4, and 33
528"
REFERENCES,0.5944055944055944,"locations from PeMSD8 are chosen, with 10 trials conducted at each location. For simplicity in the
529"
REFERENCES,0.5952797202797203,"calculation, all selected locations have just one segment upstream and one segment downstream. For
530"
REFERENCES,0.5961538461538461,"epidemic datasets, all locations from US-Regions, US-States, and Japan-Prefectures (49 locations
531"
REFERENCES,0.597027972027972,"in US-States, 10 locations in US-Regions, and 46 locations in Japan-Prefectures) are encompassed
532"
REFERENCES,0.5979020979020979,"in the experiments, with 10 trials implemented on each location. The same experiment setups are
533"
REFERENCES,0.5987762237762237,"operated on all baseline metrics and NTW. σ values for MLP and PDE are 0.8 and 0.95, respectively.
534"
REFERENCES,0.5996503496503497,"The ratio of training, calibration, validation, and testing data on airfoil self-noise datasets, three
535"
REFERENCES,0.6005244755244755,"traffic datasets, and three epidemic datasets are 1:1:1:1, 3:2:2:3, and 1:2:1:1, respectively. Data
536"
REFERENCES,0.6013986013986014,"separation was conducted randomly. Adam optimizer with a learning rate of 0.001 was applied for all
537"
REFERENCES,0.6022727272727273,"experiments. On average, one trial requires one hours on a workstation with double NVIDIA RTX
538"
REFERENCES,0.6031468531468531,"3090 GPU.
539"
REFERENCES,0.6040209790209791,"D.5
Experiment setups for mRCP, V-REx, and DRO
540"
REFERENCES,0.6048951048951049,"We define 1 trial as running a series of experiments of all predefined β values once, except for DRO.
541"
REFERENCES,0.6057692307692307,"For the airfoil self-noise example, 100 trials with random data preprocessing are conducted. For the
542"
REFERENCES,0.6066433566433567,"traffic speed prediction task, we randomly select 10 locations from each of the three traffic datasets
543"
REFERENCES,0.6075174825174825,"and operate one trial on all selected locations. In the epidemic spread prediction task, all locations of
544"
REFERENCES,0.6083916083916084,"the three datasets are included and we operate one trial on each of them. All combinations of models
545"
REFERENCES,0.6092657342657343,"(MLP and PDE) and algorithms (mRCP, DRO, V-REx) share the same experiment setups mentioned
546"
REFERENCES,0.6101398601398601,"above. σ values for MLP and PDE are 0.8 and 0.95, respectively. β values for mRCP and V-REx in
547"
REFERENCES,0.611013986013986,"different experiment setups are shown in Table 3. Each Pareto curve consists of at least 10 β values.
548"
REFERENCES,0.6118881118881119,"For airfoil self-noise datasets and three traffic datasets, the original data is evenly and randomly split
549"
REFERENCES,0.6127622377622378,"for training, calibration, and testing. For three epidemic datasets, we randomly split the original data
550"
REFERENCES,0.6136363636363636,"for training, calibration, and testing with a ratio of 2:1:2. Adam optimizer with a learning rate of
551"
REFERENCES,0.6145104895104895,"0.001 was applied for all experiments. On average, one trial requires 12 hours on a workstation with
552"
REFERENCES,0.6153846153846154,"double NVIDIA RTX 3090 GPU.
553"
REFERENCES,0.6162587412587412,Table 3: β values for mRCP and V-REx in experiment setups
REFERENCES,0.6171328671328671,"Dataset
Model
Algorithm
β Values"
REFERENCES,0.618006993006993,"Airfoil Self-Noise
MLP
mRCP
0.1, 0.2, 0.5, 1, 2, 5, 10, 15, 20, 30, 50, 80, 100.
V-REx
0.1, 1, 2, 2.5, 3, 3.5, 4, 4.5, 5, 6, 7, 8, 9, 10, 15, 20."
REFERENCES,0.6188811188811189,Japan-Prefectures
REFERENCES,0.6197552447552448,"MLP
mRCP
0.1, 0.2, 0.4, 0.8, 1, 2, 5, 10, 20, 40, 100, 200, 500.
V-REx
0.1, 1, 1.5, 2, 3, 4, 5, 7.5, 10, 20, 40, 100, 200, 500."
REFERENCES,0.6206293706293706,"PDE
mRCP
0.1, 0.2, 0.4, 0.6, 0.8, 1, 2, 5, 7, 10.
V-REx
0.1, 0.2, 0.4, 0.6, 0.8, 1, 2, 5, 7, 10."
REFERENCES,0.6215034965034965,US-Regions
REFERENCES,0.6223776223776224,"MLP
mRCP
0.1, 0.2, 0.4, 0.8, 1, 2, 5, 10, 20, 40, 100, 200, 500.
V-REx
0.1, 1, 2, 3, 4, 5, 6, 7, 8, 10, 15, 20, 30, 40, 100."
REFERENCES,0.6232517482517482,"PDE
mRCP
0.1, 0.2, 0.4, 0.6, 0.8, 1, 2, 5, 7, 10.
V-REx
0.1, 0.2, 0.4, 0.6, 0.8, 1, 2, 5, 7, 10."
REFERENCES,0.6241258741258742,US-States
REFERENCES,0.625,"MLP
mRCP
0.1, 0.2, 0.4, 0.8, 1, 2, 5, 10, 20, 40, 100, 200, 500.
V-REx
0.1, 1, 1.2, 1.7, 2, 2.5, 3, 3.5, 4, 5, 7, 10, 15."
REFERENCES,0.6258741258741258,"PDE
mRCP
0.1, 0.2, 0.4, 0.6, 0.8, 1, 2, 5, 7, 10.
V-REx
0.1, 0.2, 0.4, 0.6, 0.8, 1, 2, 5, 7, 10."
REFERENCES,0.6267482517482518,Seattle-loop
REFERENCES,0.6276223776223776,"MLP
mRCP
0.1, 1, 2, 3, 4, 5, 10, 25, 100, 200, 400, 700, 1000.
V-REx
0.1, 1, 1.5, 2, 2.5, 3, 4, 5, 10, 50."
REFERENCES,0.6284965034965035,"PDE
mRCP
0.1, 1, 5, 10, 20, 40, 80, 160, 320, 640.
V-REx
0.1, 0.2, 0.5, 0.8, 1, 1.5, 2, 3, 4, 5."
REFERENCES,0.6293706293706294,PeMSD4
REFERENCES,0.6302447552447552,"MLP
mRCP
0.1, 1, 2, 5, 10, 50, 100, 150, 200, 300, 400, 500.
V-REx
0.1, 1, 2, 3, 4, 5, 7.5, 10, 13, 16, 19, 22, 25."
REFERENCES,0.6311188811188811,"PDE
mRCP
0.1, 1, 5, 10, 50, 100, 200, 500, 1000, 5000, 10000.
V-REx
0.1, 1, 2, 3, 4, 5, 7, 8, 10, 12, 15."
REFERENCES,0.631993006993007,PeMSD8
REFERENCES,0.6328671328671329,"MLP
mRCP
0.1, 1, 2, 5, 10, 50, 100, 150, 250, 300, 400, 500.
V-REx
0.1, 1, 2, 3, 4, 5, 7.5, 10, 20, 30, 40, 75, 80, 150."
REFERENCES,0.6337412587412588,"PDE
mRCP
0.1, 1, 5, 10, 50, 100, 200, 500, 1000, 2000.
V-REx
0.1, 1, 2, 3, 5, 7, 10, 15, 20, 30."
REFERENCES,0.6346153846153846,"E
Additional experiment results
554"
REFERENCES,0.6354895104895105,"E.1
Pearson coefficient definition
555"
REFERENCES,0.6363636363636364,"Here we provide a detailed definition of the Pearson coefficient as follows.
556"
REFERENCES,0.6372377622377622,"Definition 4 (Pearson coefficient). The Pearson correlation coefficient, denoted as r, is calculated as
557"
REFERENCES,0.6381118881118881,"the covariance of the two variables divided by the product of their standard deviations, as follows.
558"
REFERENCES,0.638986013986014,"r =
P(xi −x)(yi −y)
pP(xi −x)2 P(yi −y)2 .
(34)"
REFERENCES,0.6398601398601399,"where xi and yi are the individual sample points of random variables X and Y indexed with i and x
559"
REFERENCES,0.6407342657342657,"and y are the means of their samples, respectively.
560"
REFERENCES,0.6416083916083916,"The Pearson correlation coefficient measures the linear correlation between two variables. It gives a
561"
REFERENCES,0.6424825174825175,"value between -1 and 1 inclusive, where 1 indicates a perfect positive linear relationship, -1 indicates
562"
REFERENCES,0.6433566433566433,"a perfect negative linear relationship, and 0 indicates no linear correlation.
563"
REFERENCES,0.6442307692307693,"E.2
Correlation visualization
564"
REFERENCES,0.6451048951048951,"Figure 3 shows the experimental results of the correlation between NTW and coverage difference
565"
REFERENCES,0.6459790209790209,"expectation, compared with three baselines: total variation, KL divergence, and expectation difference.
566"
REFERENCES,0.6468531468531469,"It is organized into a matrix of subplots, with each column corresponding to a specific dataset and
567"
REFERENCES,0.6477272727272727,"each row depicting the performance of a metric. Within these subplots, individual points represent
568"
REFERENCES,0.6486013986013986,"the conjunction of a metric’s value with the associated coverage difference expectation for a given
569"
REFERENCES,0.6494755244755245,"test domain. A positive trend between NTW and the coverage difference expectation is shown in the
570"
REFERENCES,0.6503496503496503,"top row, showcasing NTW’s strong correlation. In contrast, the other metrics exhibit inconsistent
571"
REFERENCES,0.6512237762237763,"correlations across the varied datasets and models, as seen in the lower three rows of subplots.
572"
REFERENCES,0.6520979020979021,"Figure 4 also illustrates the expected coverage difference’s correlation to NTW, standard W-distance,
573"
REFERENCES,0.652972027972028,"normalized W-distance, and truncated W-distance, proving that normalization and truncation are
574"
REFERENCES,0.6538461538461539,equally important for robust correlations.
REFERENCES,0.6547202797202797,"0.10
0.20
d(e)
NTW"
REFERENCES,0.6555944055944056,Airfoil Self-Noise
REFERENCES,0.6564685314685315,"10
15
d(e)
TV"
REFERENCES,0.6573426573426573,"100 200 300
d(e)
KL"
REFERENCES,0.6582167832167832,"0.1
0.2
[D(e)]"
REFERENCES,0.6590909090909091,"0.08
0.16 (e)"
REFERENCES,0.659965034965035,"0.03
0.06"
REFERENCES,0.6608391608391608,Seattle-loop
REFERENCES,0.6617132867132867,"0.3
0.4
1
2"
REFERENCES,0.6625874125874126,"0.025
0.050
0.075
[D(e)]"
REFERENCES,0.6634615384615384,0.25 0.50 0.75
REFERENCES,0.6643356643356644,"0.06
0.09"
REFERENCES,0.6652097902097902,PeMSD4
REFERENCES,0.666083916083916,"3
4
5
40
60"
REFERENCES,0.666958041958042,"0.04
0.06
0.08
[D(e)]"
REFERENCES,0.6678321678321678,"0.15
0.30"
REFERENCES,0.6687062937062938,"0.04
0.06"
REFERENCES,0.6695804195804196,PeMSD8
REFERENCES,0.6704545454545454,"3
4
20
40
60"
REFERENCES,0.6713286713286714,"0.04
0.06
[D(e)]"
REFERENCES,0.6722027972027972,0.08 0.12
REFERENCES,0.6730769230769231,"MLP
PDE"
REFERENCES,0.673951048951049,"0.08
0.10"
REFERENCES,0.6748251748251748,US-States
REFERENCES,0.6756993006993007,"0.3
0.4
0.5
10
15"
REFERENCES,0.6765734265734266,"0.08
0.10
[D(e)]"
REFERENCES,0.6774475524475524,"5
10
15"
REFERENCES,0.6783216783216783,"0.06
0.08"
REFERENCES,0.6791958041958042,US-Regions
REFERENCES,0.6800699300699301,"0.016
0.024
0.2
0.4"
REFERENCES,0.6809440559440559,"0.070
0.075
[D(e)]"
REFERENCES,0.6818181818181818,"20
40
60"
REFERENCES,0.6826923076923077,"0.10
0.20"
REFERENCES,0.6835664335664335,Japan-Prefectures
REFERENCES,0.6844405594405595,"0.02 0.04 0.06
0.5 1.0 1.5 2.0"
REFERENCES,0.6853146853146853,"0.05
0.10
0.15
[D(e)]"
REFERENCES,0.6861888111888111,"150
300"
REFERENCES,0.6870629370629371,"Figure 3: Experimental results of the correlation between Normalized Truncated Wasserstein distance
(NTW) and coverage difference expectation, compared with total variation, KL divergence, and expec-
tation difference. Each point represents a pair of metric value and coverage difference expectation for a test
domain. The first row of the subplots demonstrates NTW indicates the expectation across different datasets and
models, whereas other baseline metrics, represented in the other three rows, can not consistently capture it. 575"
REFERENCES,0.6879370629370629,"0.10
0.20
d(e)
NTW"
REFERENCES,0.6888111888111889,Airfoil Self-Noise
REFERENCES,0.6896853146853147,"0.1
0.2
d(e)
W"
REFERENCES,0.6905594405594405,"0.010 0.015
d(e)
NW"
REFERENCES,0.6914335664335665,"0.1
0.2
[D(e)]"
REFERENCES,0.6923076923076923,"0.02
0.04
d(e)
TW"
REFERENCES,0.6931818181818182,"0.03
0.06"
REFERENCES,0.6940559440559441,Seattle-loop
REFERENCES,0.6949300699300699,"0.2 0.4 0.6 0.8
0.01
0.02"
REFERENCES,0.6958041958041958,"0.025
0.050
0.075
[D(e)]"
REFERENCES,0.6966783216783217,"0.25
0.50"
REFERENCES,0.6975524475524476,"0.06
0.09"
REFERENCES,0.6984265734265734,PeMSD4
REFERENCES,0.6993006993006993,"0.2 0.3 0.4
0.02 0.03"
REFERENCES,0.7001748251748252,"0.04
0.06
0.08
[D(e)]"
REFERENCES,0.701048951048951,0.10 0.15
REFERENCES,0.7019230769230769,"0.04
0.06"
REFERENCES,0.7027972027972028,PeMSD8
REFERENCES,0.7036713286713286,"0.10
0.15
0.01
0.02
0.03"
REFERENCES,0.7045454545454546,"0.04
0.06
[D(e)]"
REFERENCES,0.7054195804195804,"0.04
0.08"
REFERENCES,0.7062937062937062,"MLP
PDE"
REFERENCES,0.7071678321678322,"0.08
0.10"
REFERENCES,0.708041958041958,US-States
REFERENCES,0.708916083916084,"10 15 20 25
0.04
0.06"
REFERENCES,0.7097902097902098,"0.08
0.10
[D(e)]"
REFERENCES,0.7106643356643356,"5
10
15"
REFERENCES,0.7115384615384616,"0.06
0.08"
REFERENCES,0.7124125874125874,US-Regions
REFERENCES,0.7132867132867133,"40
60
80
0.02
0.03"
REFERENCES,0.7141608391608392,"0.070
0.075
[D(e)] 20
40"
REFERENCES,0.715034965034965,"0.10
0.20"
REFERENCES,0.7159090909090909,Japan-Prefectures
REFERENCES,0.7167832167832168,"200
400
0.08
0.16"
REFERENCES,0.7176573426573427,"0.05
0.10
0.15
[D(e)]"
REFERENCES,0.7185314685314685,"0
150
300"
REFERENCES,0.7194055944055944,"Figure 4: Experimental results of the correlation between Normalized Truncated Wasserstein distance
(NTW) and coverage difference expectation of concept shift, compared with standard, normalized,
and truncated Wasserstein distance. Each point represents a pair of metric value and coverage difference
expectation of a test domain. By comparing the first row with the rest three rows, we validate the necessity of
applying normalization and truncation together."
REFERENCES,0.7202797202797203,"NeurIPS Paper Checklist
576"
CLAIMS,0.7211538461538461,"1. Claims
577"
CLAIMS,0.722027972027972,"Question: Do the main claims made in the abstract and introduction accurately reflect the
578"
CLAIMS,0.7229020979020979,"paper’s contributions and scope?
579"
CLAIMS,0.7237762237762237,"Answer: [Yes]
580"
CLAIMS,0.7246503496503497,"Justification: We focus on robust conformal prediction under joint distribution shift. The
581"
ABSTRACT,0.7255244755244755,"abstract and introduction accurately state our contributions. We propose Normalized Wasser-
582"
ABSTRACT,0.7263986013986014,"stein distance to quantify the coverage difference caused by concept shift and develop
583"
ABSTRACT,0.7272727272727273,"multi-domain robust conformal prediction to make coverage approach confidence when
584"
ABSTRACT,0.7281468531468531,"multiple test domains hold joint shifts with the calibration domain.
585"
ABSTRACT,0.7290209790209791,"Guidelines:
586"
ABSTRACT,0.7298951048951049,"• The answer NA means that the abstract and introduction do not include the claims
587"
ABSTRACT,0.7307692307692307,"made in the paper.
588"
ABSTRACT,0.7316433566433567,"• The abstract and/or introduction should clearly state the claims made, including the
589"
ABSTRACT,0.7325174825174825,"contributions made in the paper and important assumptions and limitations. A No or
590"
ABSTRACT,0.7333916083916084,"NA answer to this question will not be perceived well by the reviewers.
591"
ABSTRACT,0.7342657342657343,"• The claims made should match theoretical and experimental results, and reflect how
592"
ABSTRACT,0.7351398601398601,"much the results can be expected to generalize to other settings.
593"
ABSTRACT,0.736013986013986,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
594"
ABSTRACT,0.7368881118881119,"are not attained by the paper.
595"
LIMITATIONS,0.7377622377622378,"2. Limitations
596"
LIMITATIONS,0.7386363636363636,"Question: Does the paper discuss the limitations of the work performed by the authors?
597"
LIMITATIONS,0.7395104895104895,"Answer: [Yes]
598"
LIMITATIONS,0.7403846153846154,"Justification: We discuss the limitation of our proposed method in Section 6. The proposed
599"
LIMITATIONS,0.7412587412587412,"method requires the training model’s enough capacity to fit complex patterns. Also, as it
600"
LIMITATIONS,0.7421328671328671,"needs to approximate the ratio of covariate likelihood between calibration and test domains,
601"
LIMITATIONS,0.743006993006993,"it requires enough training and calibration samples to conduct accurate kernel density
602"
LIMITATIONS,0.7438811188811189,"estimation.
603"
LIMITATIONS,0.7447552447552448,"Guidelines:
604"
LIMITATIONS,0.7456293706293706,"• The answer NA means that the paper has no limitation while the answer No means that
605"
LIMITATIONS,0.7465034965034965,"the paper has limitations, but those are not discussed in the paper.
606"
LIMITATIONS,0.7473776223776224,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
607"
LIMITATIONS,0.7482517482517482,"• The paper should point out any strong assumptions and how robust the results are to
608"
LIMITATIONS,0.7491258741258742,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
609"
LIMITATIONS,0.75,"model well-specification, asymptotic approximations only holding locally). The authors
610"
LIMITATIONS,0.7508741258741258,"should reflect on how these assumptions might be violated in practice and what the
611"
LIMITATIONS,0.7517482517482518,"implications would be.
612"
LIMITATIONS,0.7526223776223776,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
613"
LIMITATIONS,0.7534965034965035,"only tested on a few datasets or with a few runs. In general, empirical results often
614"
LIMITATIONS,0.7543706293706294,"depend on implicit assumptions, which should be articulated.
615"
LIMITATIONS,0.7552447552447552,"• The authors should reflect on the factors that influence the performance of the approach.
616"
LIMITATIONS,0.7561188811188811,"For example, a facial recognition algorithm may perform poorly when image resolution
617"
LIMITATIONS,0.756993006993007,"is low or images are taken in low lighting. Or a speech-to-text system might not be
618"
LIMITATIONS,0.7578671328671329,"used reliably to provide closed captions for online lectures because it fails to handle
619"
LIMITATIONS,0.7587412587412588,"technical jargon.
620"
LIMITATIONS,0.7596153846153846,"• The authors should discuss the computational efficiency of the proposed algorithms
621"
LIMITATIONS,0.7604895104895105,"and how they scale with dataset size.
622"
LIMITATIONS,0.7613636363636364,"• If applicable, the authors should discuss possible limitations of their approach to
623"
LIMITATIONS,0.7622377622377622,"address problems of privacy and fairness.
624"
LIMITATIONS,0.7631118881118881,"• While the authors might fear that complete honesty about limitations might be used by
625"
LIMITATIONS,0.763986013986014,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
626"
LIMITATIONS,0.7648601398601399,"limitations that aren’t acknowledged in the paper. The authors should use their best
627"
LIMITATIONS,0.7657342657342657,"judgment and recognize that individual actions in favor of transparency play an impor-
628"
LIMITATIONS,0.7666083916083916,"tant role in developing norms that preserve the integrity of the community. Reviewers
629"
LIMITATIONS,0.7674825174825175,"will be specifically instructed to not penalize honesty concerning limitations.
630"
THEORY ASSUMPTIONS AND PROOFS,0.7683566433566433,"3. Theory Assumptions and Proofs
631"
THEORY ASSUMPTIONS AND PROOFS,0.7692307692307693,"Question: For each theoretical result, does the paper provide the full set of assumptions and
632"
THEORY ASSUMPTIONS AND PROOFS,0.7701048951048951,"a complete (and correct) proof?
633"
THEORY ASSUMPTIONS AND PROOFS,0.7709790209790209,"Answer: [Yes]
634"
THEORY ASSUMPTIONS AND PROOFS,0.7718531468531469,"Justification: We refer to Section 3, Appendix B, Appendix C for detailed theoretical work.
635"
THEORY ASSUMPTIONS AND PROOFS,0.7727272727272727,"Guidelines:
636"
THEORY ASSUMPTIONS AND PROOFS,0.7736013986013986,"• The answer NA means that the paper does not include theoretical results.
637"
THEORY ASSUMPTIONS AND PROOFS,0.7744755244755245,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
638"
THEORY ASSUMPTIONS AND PROOFS,0.7753496503496503,"referenced.
639"
THEORY ASSUMPTIONS AND PROOFS,0.7762237762237763,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
640"
THEORY ASSUMPTIONS AND PROOFS,0.7770979020979021,"• The proofs can either appear in the main paper or the supplemental material, but if
641"
THEORY ASSUMPTIONS AND PROOFS,0.777972027972028,"they appear in the supplemental material, the authors are encouraged to provide a short
642"
THEORY ASSUMPTIONS AND PROOFS,0.7788461538461539,"proof sketch to provide intuition.
643"
THEORY ASSUMPTIONS AND PROOFS,0.7797202797202797,"• Inversely, any informal proof provided in the core of the paper should be complemented
644"
THEORY ASSUMPTIONS AND PROOFS,0.7805944055944056,"by formal proofs provided in appendix or supplemental material.
645"
THEORY ASSUMPTIONS AND PROOFS,0.7814685314685315,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7823426573426573,"4. Experimental Result Reproducibility
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7832167832167832,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7840909090909091,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.784965034965035,"of the paper (regardless of whether the code and data are provided or not)?
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7858391608391608,"Answer: [Yes]
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7867132867132867,"Justification: We show detailed experiment setups, including models, datasets, and algo-
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7875874125874126,"rithms, in Section 5 and Appendix D.
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7884615384615384,"Guidelines:
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7893356643356644,"• The answer NA means that the paper does not include experiments.
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7902097902097902,"• If the paper includes experiments, a No answer to this question will not be perceived
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.791083916083916,"well by the reviewers: Making the paper reproducible is important, regardless of
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.791958041958042,"whether the code and data are provided or not.
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7928321678321678,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7937062937062938,"to make their results reproducible or verifiable.
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7945804195804196,"• Depending on the contribution, reproducibility can be accomplished in various ways.
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7954545454545454,"For example, if the contribution is a novel architecture, describing the architecture fully
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7963286713286714,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7972027972027972,"be necessary to either make it possible for others to replicate the model with the same
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7980769230769231,"dataset, or provide access to the model. In general. releasing code and data is often
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.798951048951049,"one good way to accomplish this, but reproducibility can also be provided via detailed
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7998251748251748,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8006993006993007,"of a large language model), releasing of a model checkpoint, or other means that are
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8015734265734266,"appropriate to the research performed.
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8024475524475524,"• While NeurIPS does not require releasing code, the conference does require all submis-
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8033216783216783,"sions to provide some reasonable avenue for reproducibility, which may depend on the
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8041958041958042,"nature of the contribution. For example
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8050699300699301,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8059440559440559,"to reproduce that algorithm.
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8068181818181818,"(b) If the contribution is primarily a new model architecture, the paper should describe
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8076923076923077,"the architecture clearly and fully.
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8085664335664335,"(c) If the contribution is a new model (e.g., a large language model), then there should
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8094405594405595,"either be a way to access this model for reproducing the results or a way to reproduce
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8103146853146853,"the model (e.g., with an open-source dataset or instructions for how to construct
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8111888111888111,"the dataset).
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8120629370629371,"(d) We recognize that reproducibility may be tricky in some cases, in which case
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8129370629370629,"authors are welcome to describe the particular way they provide for reproducibility.
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8138111888111889,"In the case of closed-source models, it may be that access to the model is limited in
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8146853146853147,"some way (e.g., to registered users), but it should be possible for other researchers
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8155594405594405,"to have some path to reproducing or verifying the results.
685"
OPEN ACCESS TO DATA AND CODE,0.8164335664335665,"5. Open access to data and code
686"
OPEN ACCESS TO DATA AND CODE,0.8173076923076923,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
687"
OPEN ACCESS TO DATA AND CODE,0.8181818181818182,"tions to faithfully reproduce the main experimental results, as described in supplemental
688"
OPEN ACCESS TO DATA AND CODE,0.8190559440559441,"material?
689"
OPEN ACCESS TO DATA AND CODE,0.8199300699300699,"Answer: [No]
690"
OPEN ACCESS TO DATA AND CODE,0.8208041958041958,"Justification: We would like to provide open access to the data and code if this submission
691"
OPEN ACCESS TO DATA AND CODE,0.8216783216783217,"is accepted.
692"
OPEN ACCESS TO DATA AND CODE,0.8225524475524476,"Guidelines:
693"
OPEN ACCESS TO DATA AND CODE,0.8234265734265734,"• The answer NA means that paper does not include experiments requiring code.
694"
OPEN ACCESS TO DATA AND CODE,0.8243006993006993,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
695"
OPEN ACCESS TO DATA AND CODE,0.8251748251748252,"public/guides/CodeSubmissionPolicy) for more details.
696"
OPEN ACCESS TO DATA AND CODE,0.826048951048951,"• While we encourage the release of code and data, we understand that this might not be
697"
OPEN ACCESS TO DATA AND CODE,0.8269230769230769,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
698"
OPEN ACCESS TO DATA AND CODE,0.8277972027972028,"including code, unless this is central to the contribution (e.g., for a new open-source
699"
OPEN ACCESS TO DATA AND CODE,0.8286713286713286,"benchmark).
700"
OPEN ACCESS TO DATA AND CODE,0.8295454545454546,"• The instructions should contain the exact command and environment needed to run to
701"
OPEN ACCESS TO DATA AND CODE,0.8304195804195804,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
702"
OPEN ACCESS TO DATA AND CODE,0.8312937062937062,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
703"
OPEN ACCESS TO DATA AND CODE,0.8321678321678322,"• The authors should provide instructions on data access and preparation, including how
704"
OPEN ACCESS TO DATA AND CODE,0.833041958041958,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
705"
OPEN ACCESS TO DATA AND CODE,0.833916083916084,"• The authors should provide scripts to reproduce all experimental results for the new
706"
OPEN ACCESS TO DATA AND CODE,0.8347902097902098,"proposed method and baselines. If only a subset of experiments are reproducible, they
707"
OPEN ACCESS TO DATA AND CODE,0.8356643356643356,"should state which ones are omitted from the script and why.
708"
OPEN ACCESS TO DATA AND CODE,0.8365384615384616,"• At submission time, to preserve anonymity, the authors should release anonymized
709"
OPEN ACCESS TO DATA AND CODE,0.8374125874125874,"versions (if applicable).
710"
OPEN ACCESS TO DATA AND CODE,0.8382867132867133,"• Providing as much information as possible in supplemental material (appended to the
711"
OPEN ACCESS TO DATA AND CODE,0.8391608391608392,"paper) is recommended, but including URLs to data and code is permitted.
712"
OPEN ACCESS TO DATA AND CODE,0.840034965034965,"6. Experimental Setting/Details
713"
OPEN ACCESS TO DATA AND CODE,0.8409090909090909,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
714"
OPEN ACCESS TO DATA AND CODE,0.8417832167832168,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
715"
OPEN ACCESS TO DATA AND CODE,0.8426573426573427,"results?
716"
OPEN ACCESS TO DATA AND CODE,0.8435314685314685,"Answer: [Yes]
717"
OPEN ACCESS TO DATA AND CODE,0.8444055944055944,"Justification: We provide detailed information about data splits and preprocessing, hyper-
718"
OPEN ACCESS TO DATA AND CODE,0.8452797202797203,"parameters, optimizer, black-box model architecture, and physics-informed equations in
719"
OPEN ACCESS TO DATA AND CODE,0.8461538461538461,"Appendix D.
720"
OPEN ACCESS TO DATA AND CODE,0.847027972027972,"Guidelines:
721"
OPEN ACCESS TO DATA AND CODE,0.8479020979020979,"• The answer NA means that the paper does not include experiments.
722"
OPEN ACCESS TO DATA AND CODE,0.8487762237762237,"• The experimental setting should be presented in the core of the paper to a level of detail
723"
OPEN ACCESS TO DATA AND CODE,0.8496503496503497,"that is necessary to appreciate the results and make sense of them.
724"
OPEN ACCESS TO DATA AND CODE,0.8505244755244755,"• The full details can be provided either with the code, in appendix, or as supplemental
725"
OPEN ACCESS TO DATA AND CODE,0.8513986013986014,"material.
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8522727272727273,"7. Experiment Statistical Significance
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8531468531468531,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8540209790209791,"information about the statistical significance of the experiments?
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8548951048951049,"Answer: [Yes]
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8557692307692307,"Justification: Statistical measures of experiment results are shown in Figure 2 and Table ??.
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8566433566433567,"Guidelines:
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8575174825174825,"• The answer NA means that the paper does not include experiments.
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8583916083916084,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8592657342657343,"dence intervals, or statistical significance tests, at least for the experiments that support
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8601398601398601,"the main claims of the paper.
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.861013986013986,"• The factors of variability that the error bars are capturing should be clearly stated (for
737"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8618881118881119,"example, train/test split, initialization, random drawing of some parameter, or overall
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8627622377622378,"run with given experimental conditions).
739"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8636363636363636,"• The method for calculating the error bars should be explained (closed form formula,
740"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8645104895104895,"call to a library function, bootstrap, etc.)
741"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8653846153846154,"• The assumptions made should be given (e.g., Normally distributed errors).
742"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8662587412587412,"• It should be clear whether the error bar is the standard deviation or the standard error
743"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8671328671328671,"of the mean.
744"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.868006993006993,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
745"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8688811188811189,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
746"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8697552447552448,"of Normality of errors is not verified.
747"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8706293706293706,"• For asymmetric distributions, the authors should be careful not to show in tables or
748"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8715034965034965,"figures symmetric error bars that would yield results that are out of range (e.g. negative
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8723776223776224,"error rates).
750"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8732517482517482,"• If error bars are reported in tables or plots, The authors should explain in the text how
751"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8741258741258742,"they were calculated and reference the corresponding figures or tables in the text.
752"
EXPERIMENTS COMPUTE RESOURCES,0.875,"8. Experiments Compute Resources
753"
EXPERIMENTS COMPUTE RESOURCES,0.8758741258741258,"Question: For each experiment, does the paper provide sufficient information on the com-
754"
EXPERIMENTS COMPUTE RESOURCES,0.8767482517482518,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
755"
EXPERIMENTS COMPUTE RESOURCES,0.8776223776223776,"the experiments?
756"
EXPERIMENTS COMPUTE RESOURCES,0.8784965034965035,"Answer: [Yes]
757"
EXPERIMENTS COMPUTE RESOURCES,0.8793706293706294,"Justification: We provide the information about our workstation and computation time for
758"
EXPERIMENTS COMPUTE RESOURCES,0.8802447552447552,"one trial in Appendix D.
759"
EXPERIMENTS COMPUTE RESOURCES,0.8811188811188811,"Guidelines:
760"
EXPERIMENTS COMPUTE RESOURCES,0.881993006993007,"• The answer NA means that the paper does not include experiments.
761"
EXPERIMENTS COMPUTE RESOURCES,0.8828671328671329,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
762"
EXPERIMENTS COMPUTE RESOURCES,0.8837412587412588,"or cloud provider, including relevant memory and storage.
763"
EXPERIMENTS COMPUTE RESOURCES,0.8846153846153846,"• The paper should provide the amount of compute required for each of the individual
764"
EXPERIMENTS COMPUTE RESOURCES,0.8854895104895105,"experimental runs as well as estimate the total compute.
765"
EXPERIMENTS COMPUTE RESOURCES,0.8863636363636364,"• The paper should disclose whether the full research project required more compute
766"
EXPERIMENTS COMPUTE RESOURCES,0.8872377622377622,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
767"
EXPERIMENTS COMPUTE RESOURCES,0.8881118881118881,"didn’t make it into the paper).
768"
CODE OF ETHICS,0.888986013986014,"9. Code Of Ethics
769"
CODE OF ETHICS,0.8898601398601399,"Question: Does the research conducted in the paper conform, in every respect, with the
770"
CODE OF ETHICS,0.8907342657342657,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
771"
CODE OF ETHICS,0.8916083916083916,"Answer: [Yes]
772"
CODE OF ETHICS,0.8924825174825175,"Justification: Our research follows the NeurPIS Code of Ethics.
773"
CODE OF ETHICS,0.8933566433566433,"Guidelines:
774"
CODE OF ETHICS,0.8942307692307693,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
775"
CODE OF ETHICS,0.8951048951048951,"• If the authors answer No, they should explain the special circumstances that require a
776"
CODE OF ETHICS,0.8959790209790209,"deviation from the Code of Ethics.
777"
CODE OF ETHICS,0.8968531468531469,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
778"
CODE OF ETHICS,0.8977272727272727,"eration due to laws or regulations in their jurisdiction).
779"
BROADER IMPACTS,0.8986013986013986,"10. Broader Impacts
780"
BROADER IMPACTS,0.8994755244755245,"Question: Does the paper discuss both potential positive societal impacts and negative
781"
BROADER IMPACTS,0.9003496503496503,"societal impacts of the work performed?
782"
BROADER IMPACTS,0.9012237762237763,"Answer: [Yes]
783"
BROADER IMPACTS,0.9020979020979021,"Justification: We mentioned the positive impact of robust conformal prediction in the
784"
BROADER IMPACTS,0.902972027972028,"introduction. We do not see negative societal impacts.
785"
BROADER IMPACTS,0.9038461538461539,"Guidelines:
786"
BROADER IMPACTS,0.9047202797202797,"• The answer NA means that there is no societal impact of the work performed.
787"
BROADER IMPACTS,0.9055944055944056,"• If the authors answer NA or No, they should explain why their work has no societal
788"
BROADER IMPACTS,0.9064685314685315,"impact or why the paper does not address societal impact.
789"
BROADER IMPACTS,0.9073426573426573,"• Examples of negative societal impacts include potential malicious or unintended uses
790"
BROADER IMPACTS,0.9082167832167832,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
791"
BROADER IMPACTS,0.9090909090909091,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
792"
BROADER IMPACTS,0.909965034965035,"groups), privacy considerations, and security considerations.
793"
BROADER IMPACTS,0.9108391608391608,"• The conference expects that many papers will be foundational research and not tied
794"
BROADER IMPACTS,0.9117132867132867,"to particular applications, let alone deployments. However, if there is a direct path to
795"
BROADER IMPACTS,0.9125874125874126,"any negative applications, the authors should point it out. For example, it is legitimate
796"
BROADER IMPACTS,0.9134615384615384,"to point out that an improvement in the quality of generative models could be used to
797"
BROADER IMPACTS,0.9143356643356644,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
798"
BROADER IMPACTS,0.9152097902097902,"that a generic algorithm for optimizing neural networks could enable people to train
799"
BROADER IMPACTS,0.916083916083916,"models that generate Deepfakes faster.
800"
BROADER IMPACTS,0.916958041958042,"• The authors should consider possible harms that could arise when the technology is
801"
BROADER IMPACTS,0.9178321678321678,"being used as intended and functioning correctly, harms that could arise when the
802"
BROADER IMPACTS,0.9187062937062938,"technology is being used as intended but gives incorrect results, and harms following
803"
BROADER IMPACTS,0.9195804195804196,"from (intentional or unintentional) misuse of the technology.
804"
BROADER IMPACTS,0.9204545454545454,"• If there are negative societal impacts, the authors could also discuss possible mitigation
805"
BROADER IMPACTS,0.9213286713286714,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
806"
BROADER IMPACTS,0.9222027972027972,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
807"
BROADER IMPACTS,0.9230769230769231,"feedback over time, improving the efficiency and accessibility of ML).
808"
SAFEGUARDS,0.923951048951049,"11. Safeguards
809"
SAFEGUARDS,0.9248251748251748,"Question: Does the paper describe safeguards that have been put in place for responsible
810"
SAFEGUARDS,0.9256993006993007,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
811"
SAFEGUARDS,0.9265734265734266,"image generators, or scraped datasets)?
812"
SAFEGUARDS,0.9274475524475524,"Answer: [NA]
813"
SAFEGUARDS,0.9283216783216783,"Justification: Our submission does not pose such risks.
814"
SAFEGUARDS,0.9291958041958042,"Guidelines:
815"
SAFEGUARDS,0.9300699300699301,"• The answer NA means that the paper poses no such risks.
816"
SAFEGUARDS,0.9309440559440559,"• Released models that have a high risk for misuse or dual-use should be released with
817"
SAFEGUARDS,0.9318181818181818,"necessary safeguards to allow for controlled use of the model, for example by requiring
818"
SAFEGUARDS,0.9326923076923077,"that users adhere to usage guidelines or restrictions to access the model or implementing
819"
SAFEGUARDS,0.9335664335664335,"safety filters.
820"
SAFEGUARDS,0.9344405594405595,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
821"
SAFEGUARDS,0.9353146853146853,"should describe how they avoided releasing unsafe images.
822"
SAFEGUARDS,0.9361888111888111,"• We recognize that providing effective safeguards is challenging, and many papers do
823"
SAFEGUARDS,0.9370629370629371,"not require this, but we encourage authors to take this into account and make a best
824"
SAFEGUARDS,0.9379370629370629,"faith effort.
825"
LICENSES FOR EXISTING ASSETS,0.9388111888111889,"12. Licenses for existing assets
826"
LICENSES FOR EXISTING ASSETS,0.9396853146853147,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
827"
LICENSES FOR EXISTING ASSETS,0.9405594405594405,"the paper, properly credited and are the license and terms of use explicitly mentioned and
828"
LICENSES FOR EXISTING ASSETS,0.9414335664335665,"properly respected?
829"
LICENSES FOR EXISTING ASSETS,0.9423076923076923,"Answer: [Yes]
830"
LICENSES FOR EXISTING ASSETS,0.9431818181818182,"Justification: Datasets and models are properly cited in the paper.
831"
LICENSES FOR EXISTING ASSETS,0.9440559440559441,"Guidelines:
832"
LICENSES FOR EXISTING ASSETS,0.9449300699300699,"• The answer NA means that the paper does not use existing assets.
833"
LICENSES FOR EXISTING ASSETS,0.9458041958041958,"• The authors should cite the original paper that produced the code package or dataset.
834"
LICENSES FOR EXISTING ASSETS,0.9466783216783217,"• The authors should state which version of the asset is used and, if possible, include a
835"
LICENSES FOR EXISTING ASSETS,0.9475524475524476,"URL.
836"
LICENSES FOR EXISTING ASSETS,0.9484265734265734,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
837"
LICENSES FOR EXISTING ASSETS,0.9493006993006993,"• For scraped data from a particular source (e.g., website), the copyright and terms of
838"
LICENSES FOR EXISTING ASSETS,0.9501748251748252,"service of that source should be provided.
839"
LICENSES FOR EXISTING ASSETS,0.951048951048951,"• If assets are released, the license, copyright information, and terms of use in the
840"
LICENSES FOR EXISTING ASSETS,0.9519230769230769,"package should be provided. For popular datasets, paperswithcode.com/datasets
841"
LICENSES FOR EXISTING ASSETS,0.9527972027972028,"has curated licenses for some datasets. Their licensing guide can help determine the
842"
LICENSES FOR EXISTING ASSETS,0.9536713286713286,"license of a dataset.
843"
LICENSES FOR EXISTING ASSETS,0.9545454545454546,"• For existing datasets that are re-packaged, both the original license and the license of
844"
LICENSES FOR EXISTING ASSETS,0.9554195804195804,"the derived asset (if it has changed) should be provided.
845"
LICENSES FOR EXISTING ASSETS,0.9562937062937062,"• If this information is not available online, the authors are encouraged to reach out to
846"
LICENSES FOR EXISTING ASSETS,0.9571678321678322,"the asset’s creators.
847"
NEW ASSETS,0.958041958041958,"13. New Assets
848"
NEW ASSETS,0.958916083916084,"Question: Are new assets introduced in the paper well documented and is the documentation
849"
NEW ASSETS,0.9597902097902098,"provided alongside the assets?
850"
NEW ASSETS,0.9606643356643356,"Answer: [NA]
851"
NEW ASSETS,0.9615384615384616,"Justification: We do not introduce new assets.
852"
NEW ASSETS,0.9624125874125874,"Guidelines:
853"
NEW ASSETS,0.9632867132867133,"• The answer NA means that the paper does not release new assets.
854"
NEW ASSETS,0.9641608391608392,"• Researchers should communicate the details of the dataset/code/model as part of their
855"
NEW ASSETS,0.965034965034965,"submissions via structured templates. This includes details about training, license,
856"
NEW ASSETS,0.9659090909090909,"limitations, etc.
857"
NEW ASSETS,0.9667832167832168,"• The paper should discuss whether and how consent was obtained from people whose
858"
NEW ASSETS,0.9676573426573427,"asset is used.
859"
NEW ASSETS,0.9685314685314685,"• At submission time, remember to anonymize your assets (if applicable). You can either
860"
NEW ASSETS,0.9694055944055944,"create an anonymized URL or include an anonymized zip file.
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9702797202797203,"14. Crowdsourcing and Research with Human Subjects
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9711538461538461,"Question: For crowdsourcing experiments and research with human subjects, does the paper
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972027972027972,"include the full text of instructions given to participants and screenshots, if applicable, as
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9729020979020979,"well as details about compensation (if any)?
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9737762237762237,"Answer: [NA]
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9746503496503497,"Justification: This research does not involve crowdsourcing or research with human subjects.
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9755244755244755,"Guidelines:
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9763986013986014,"• The answer NA means that the paper does not involve crowdsourcing nor research with
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772727272727273,"human subjects.
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9781468531468531,"• Including this information in the supplemental material is fine, but if the main contribu-
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9790209790209791,"tion of the paper involves human subjects, then as much detail as possible should be
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798951048951049,"included in the main paper.
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9807692307692307,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9816433566433567,"or other labor should be paid at least the minimum wage in the country of the data
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825174825174825,"collector.
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833916083916084,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842657342657343,"Subjects
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851398601398601,"Question: Does the paper describe potential risks incurred by study participants, whether
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986013986013986,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
880"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868881118881119,"approvals (or an equivalent approval/review based on the requirements of your country or
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9877622377622378,"institution) were obtained?
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9886363636363636,"Answer: [NA]
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895104895104895,"Justification: This research does not involve crowdsourcing or research with human subjects.
884"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9903846153846154,"Guidelines:
885"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912587412587412,"• The answer NA means that the paper does not involve crowdsourcing nor research with
886"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9921328671328671,"human subjects.
887"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.993006993006993,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
888"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938811188811189,"may be required for any human subjects research. If you obtained IRB approval, you
889"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947552447552448,"should clearly state this in the paper.
890"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956293706293706,"• We recognize that the procedures for this may vary significantly between institutions
891"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9965034965034965,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
892"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9973776223776224,"guidelines for their institution.
893"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9982517482517482,"• For initial submissions, do not include any information that would break anonymity (if
894"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991258741258742,"applicable), such as the institution conducting the review.
895"
