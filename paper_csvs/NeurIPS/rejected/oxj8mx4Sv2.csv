Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0022471910112359553,"The ﬁght between discriminative versus generative goes deep, in both the study of
1"
ABSTRACT,0.0044943820224719105,"artiﬁcial and natural intelligence. In our view, both camps have complementary
2"
ABSTRACT,0.006741573033707865,"values. So, we sought to synergistically combine them. Here, we propose a
3"
ABSTRACT,0.008988764044943821,"methodology to convert deep discriminative networks to kernel generative networks.
4"
ABSTRACT,0.011235955056179775,"We leveraged the fact that deep models, including both random forests and deep
5"
ABSTRACT,0.01348314606741573,"networks, learn internal representations which are unions of polytopes with afﬁne
6"
ABSTRACT,0.015730337078651686,"activation functions to conceptualize them both as generalized partitioning rules.
7"
ABSTRACT,0.017977528089887642,"We replace the afﬁne function in each polytope populated by the training data with
8"
ABSTRACT,0.020224719101123594,"Gaussian kernel that results in a generative model. Theoretically, we derive the
9"
ABSTRACT,0.02247191011235955,"conditions under which our generative models are a consistent estimator of the
10"
ABSTRACT,0.024719101123595506,"corresponding class conditional density. Moreover, our proposed models obtain
11"
ABSTRACT,0.02696629213483146,"well calibrated posteriors for in-distribution, and extrapolate beyond the training
12"
ABSTRACT,0.029213483146067417,"data to handle out-of-distribution inputs reasonably. We believe this approach
13"
ABSTRACT,0.03146067415730337,"may be an important step in unifying the thinking and the approaches across the
14"
ABSTRACT,0.033707865168539325,"discriminative and the generative divide.
15"
INTRODUCTION,0.035955056179775284,"1
Introduction
16"
INTRODUCTION,0.038202247191011236,"Machine learning methods, specially deep neural networks and random forests have shown excellent
17"
INTRODUCTION,0.04044943820224719,"performance in many real-world tasks, including drug discovery, autonomous driving and clinical
18"
INTRODUCTION,0.04269662921348315,"surgery. However, calibrating conﬁdence over the whole feature space for these models remains a key
19"
INTRODUCTION,0.0449438202247191,"challenge in the ﬁeld. Although these learning algorithms can achieve near optimal performance at
20"
INTRODUCTION,0.04719101123595506,"inferring on the samples lying in the high density regions of the training data [1–3], they yield highly
21"
INTRODUCTION,0.04943820224719101,"conﬁdent predictions for the samples lying far away from the training data [4]. Calibrated conﬁdence
22"
INTRODUCTION,0.051685393258426963,"within the training or in-distribution (ID) region as well as in the out-of-distribution (OOD) region is
23"
INTRODUCTION,0.05393258426966292,"crucial for safety critical applications like autonomous driving and computer-assisted surgery, where
24"
INTRODUCTION,0.056179775280898875,"any aberrant reading should be detected and taken care of immediately [4, 5]. A well-calibrated model
25"
INTRODUCTION,0.058426966292134834,"capable of quantifying the uncertainty associated with inference for any points from the training
26"
INTRODUCTION,0.060674157303370786,"distribution as well as detecting OOD data can be a life-saver in these cases.
27"
INTRODUCTION,0.06292134831460675,"The approaches to calibrate OOD conﬁdence for learning algorithms described in the literature can
28"
INTRODUCTION,0.0651685393258427,"be roughly divided into two groups: discriminative and generative. Discriminative approaches try to
29"
INTRODUCTION,0.06741573033707865,"scale the posteriors based on OOD detection or modify the learning loss function. Intuitively, the
30"
INTRODUCTION,0.0696629213483146,"easiest solution for OOD conﬁdence calibration is to learn a function that gives higher scores for
31"
INTRODUCTION,0.07191011235955057,"in-distribution samples and lower scores for OOD samples, and thereby re-scale the posterior or
32"
INTRODUCTION,0.07415730337078652,"conﬁdence score from the original model accordingly [6]. There are a number of approaches in the
33"
INTRODUCTION,0.07640449438202247,"literature which try to either modify the loss function [7–9] or adversarially train the network to be
34"
INTRODUCTION,0.07865168539325842,"less conﬁdent on OOD samples [10, 4]. However, one can adversarially manipulate an OOD sample
35"
INTRODUCTION,0.08089887640449438,"where the model is less conﬁdent to ﬁnd another OOD sample where the model is overconﬁdent
36"
INTRODUCTION,0.08314606741573034,"[11, 4, 12]. Recently, as shown by Hein et al. [4], the ReLU networks produce arbitrarily high
37"
INTRODUCTION,0.0853932584269663,"conﬁdence as the inference point moves far away from the training data. Therefore, calibrating
38"
INTRODUCTION,0.08764044943820225,"ReLU networks for the whole OOD region is not possible without fundamentally changing the
39"
INTRODUCTION,0.0898876404494382,"network architecture. As a result, all of the aforementioned algorithms are unable to provide any
40"
INTRODUCTION,0.09213483146067415,"guarantee about the performance of the network through out the whole feature space. On the other end
41"
INTRODUCTION,0.09438202247191012,"of the spectrum, the generative group tries to learn generative models for both the in-distribution as
42"
INTRODUCTION,0.09662921348314607,"well as the out-of-distribution samples. The general idea for the generative group is to get likelihoods
43"
INTRODUCTION,0.09887640449438202,"for a particular sample out of the generative models for both ID and OOD to do likelihood ratio test
44"
INTRODUCTION,0.10112359550561797,"[13] or control the likelihood for training distribution far away from the training data to detect OOD
45"
INTRODUCTION,0.10337078651685393,"samples by thresholding. However, it is not obvious how to control likelihoods far away from the
46"
INTRODUCTION,0.10561797752808989,"training data for powerful generative models like variational autoencoders (VAEs) [14] and generative
47"
INTRODUCTION,0.10786516853932585,"adversarial networks (GAN) [15]. Moreover, Nalisnick et al. [16] and Hendrycks et al. [10] showed
48"
INTRODUCTION,0.1101123595505618,"VAEs and GANs can also yield overconﬁdent likelihoods far away from the training data.
49"
INTRODUCTION,0.11235955056179775,"The algorithms described so far are concerned with OOD conﬁdence calibration for deep-nets only.
50"
INTRODUCTION,0.1146067415730337,"However, in this paper, we show that other approaches which partition the feature space, for example
51"
INTRODUCTION,0.11685393258426967,"random forest, can also suffer from poor conﬁdence calibration both in the ID and the OOD regions.
52"
INTRODUCTION,0.11910112359550562,"Moreover, the algorithms described above are concerned about the conﬁdence of the algorithms in the
53"
INTRODUCTION,0.12134831460674157,"OOD region only and they do not address the conﬁdence calibration within the training distribution
54"
INTRODUCTION,0.12359550561797752,"at all. This issue is addressed separately in a different group of literature [17–19]. In this paper, we
55"
INTRODUCTION,0.1258426966292135,"consider both calibration problem jointly and propose an approach that achieves good calibration
56"
INTRODUCTION,0.12808988764044943,"throughout the whole feature space.
57"
INTRODUCTION,0.1303370786516854,"In this paper, we conceptualize both random forest and ReLU networks as generalized partitioning
58"
INTRODUCTION,0.13258426966292136,"rules with an afﬁne activation over each polytope. We consider replacing the afﬁne functions learned
59"
INTRODUCTION,0.1348314606741573,"over the polytopes with Gaussian kernels. We propose two novel kernel density estimation techniques
60"
INTRODUCTION,0.13707865168539327,"named Kernel Generative Forest (KGF) and Kernel Generative Network (KGN). We theoretically show
61"
INTRODUCTION,0.1393258426966292,"that they asymptotically converge to the true training distribution under certain conditions. At the
62"
INTRODUCTION,0.14157303370786517,"same time, the estimated likelihood from the kernel generative models decreases for samples far away
63"
INTRODUCTION,0.14382022471910114,"from the training samples. By adding a suitable bias to the kernel density estimate, we can achieve
64"
INTRODUCTION,0.14606741573033707,"calibrated posterior over the classes in the OOD region. It completely excludes the need for providing
65"
INTRODUCTION,0.14831460674157304,"OOD training examples to the model. We conduct several simulation and real data studies that show
66"
INTRODUCTION,0.15056179775280898,"both KGF and KGN are robust against OOD samples while they maintain good performance in the
67"
INTRODUCTION,0.15280898876404495,"in-distribution region.
68"
RELATED WORKS AND OUR CONTRIBUTIONS,0.1550561797752809,"2
Related Works and Our Contributions
69"
RELATED WORKS AND OUR CONTRIBUTIONS,0.15730337078651685,"There are a number of approaches in the literature which attempt to learn a generative model and
70"
RELATED WORKS AND OUR CONTRIBUTIONS,0.15955056179775282,"control the likelihoods far away from the training data. For example, Ren et al. [13] employed
71"
RELATED WORKS AND OUR CONTRIBUTIONS,0.16179775280898875,"likelihood ratio test for detecting OOD samples. Wan et al. [8] modify the training loss so that the
72"
RELATED WORKS AND OUR CONTRIBUTIONS,0.16404494382022472,"downstream projected features follow a Gaussian distribution. However, there is no guarantee of
73"
RELATED WORKS AND OUR CONTRIBUTIONS,0.1662921348314607,"performance for OOD detection for the above methods. To the best of our knowledge, only Meinke
74"
RELATED WORKS AND OUR CONTRIBUTIONS,0.16853932584269662,"et al. [5] has proposed an approach to guarantee asymptotic performance for OOD detection. They
75"
RELATED WORKS AND OUR CONTRIBUTIONS,0.1707865168539326,"model the training and the OOD distribution using Gaussian mixture models which enable them
76"
RELATED WORKS AND OUR CONTRIBUTIONS,0.17303370786516853,"to control the class conditional posteriors far away. Compared to the aforementioned methods, our
77"
RELATED WORKS AND OUR CONTRIBUTIONS,0.1752808988764045,"approach differs in several ways:
78"
RELATED WORKS AND OUR CONTRIBUTIONS,0.17752808988764046,"• We address the conﬁdence calibration problems for both ReLU-nets and random forests from
79"
RELATED WORKS AND OUR CONTRIBUTIONS,0.1797752808988764,"a common ground.
80"
RELATED WORKS AND OUR CONTRIBUTIONS,0.18202247191011237,"• We address in-distribution (ID) and out-of-distribution (OOD) calibration problem as a
81"
RELATED WORKS AND OUR CONTRIBUTIONS,0.1842696629213483,"continuum rather than two separate problems.
82"
RELATED WORKS AND OUR CONTRIBUTIONS,0.18651685393258427,"• We provide guarantees for asymptotic convergence of our proposed approach under certain
83"
RELATED WORKS AND OUR CONTRIBUTIONS,0.18876404494382024,"conditions for both ID and OOD regions.
84"
RELATED WORKS AND OUR CONTRIBUTIONS,0.19101123595505617,"• We propose an unsupervised OOD calibration approach, i.e., we do not need to train
85"
RELATED WORKS AND OUR CONTRIBUTIONS,0.19325842696629214,"exhaustively on different OOD samples.
86"
METHODS,0.19550561797752808,"3
Methods
87"
SETTING,0.19775280898876405,"3.1
Setting
88"
SETTING,0.2,"Consider a supervised learning problem with independent and identically distributed training samples
89"
SETTING,0.20224719101123595,"{(xi, yi)}n"
SETTING,0.20449438202247192,"i=1 such that (X, Y ) ⇠PX,Y , where X ⇠PX is a X ✓Rd valued input and Y ⇠PY is
90"
SETTING,0.20674157303370785,"a Y = {1, · · · , K} valued class label. We deﬁne in-distribution region as the high density region
91"
SETTING,0.20898876404494382,"of PX,Y and denote it by S. Here the goal is to learn a conﬁdence score, g : Rd ! [0, 1]K,
92"
SETTING,0.21123595505617979,"g(x) = [g1(x), g2(x), . . . , gK(x)] such that,
93"
SETTING,0.21348314606741572,gy(x) =
SETTING,0.2157303370786517,"⇢PY |X(y|x),
if x 2 S
PY (y),
if x /2 S ,
8y 2 Y
(1)"
SETTING,0.21797752808988763,"where PY |X(y|x) is the posterior probability for class y given by the Bayes formula:
94"
SETTING,0.2202247191011236,"PY |X(y|x) =
PX|Y (x|y)PY (y)
PK"
SETTING,0.22247191011235956,k=1 PX|Y (x|k)PY (k)
SETTING,0.2247191011235955,",
8y 2 Y.
(2)"
SETTING,0.22696629213483147,"Here PX|Y (x|y) is the class conditional density for the training data which we will refer as fy(x)
95"
SETTING,0.2292134831460674,"hereafter for brevity.
96"
BACKGROUND AND MAIN IDEA,0.23146067415730337,"3.2
Background and Main Idea
97"
BACKGROUND AND MAIN IDEA,0.23370786516853934,"Deep discriminative networks partition the feature space Rd into a union of p afﬁne polytopes Qr
98"
BACKGROUND AND MAIN IDEA,0.23595505617977527,such that Sp
BACKGROUND AND MAIN IDEA,0.23820224719101124,"r=1 Qr = Rd, and learn an afﬁne function over each polytope [4, 20]. Mathematically, the
99"
BACKGROUND AND MAIN IDEA,0.24044943820224718,"class-conditional density for the label y estimated by these deep discriminative models at a particular
100"
BACKGROUND AND MAIN IDEA,0.24269662921348314,"point x can be expressed as:
101"
BACKGROUND AND MAIN IDEA,0.2449438202247191,"ˆfy(x) = p
X r=1 (a>"
BACKGROUND AND MAIN IDEA,0.24719101123595505,"r x + br) (x 2 Qr).
(3)"
BACKGROUND AND MAIN IDEA,0.24943820224719102,"For example, in the case of a decision tree, ar = 0, i.e., decision tree assumes uniform distribution
102"
BACKGROUND AND MAIN IDEA,0.251685393258427,"for the class-conditional densities over the leaf nodes. Among these polytopes, the ones that lie on
103"
BACKGROUND AND MAIN IDEA,0.2539325842696629,"the boundary of the training data extend to the whole feature space and hence encompass all the OOD
104"
BACKGROUND AND MAIN IDEA,0.25617977528089886,"samples. Since the posterior probability for a class is determined by the afﬁne activation over each of
105"
BACKGROUND AND MAIN IDEA,0.25842696629213485,"these polytopes, the algorithms tend to be overconﬁdent when making predictions on the OOD inputs.
106"
BACKGROUND AND MAIN IDEA,0.2606741573033708,"Moreover, there exist some polytopes that are not populated with training data. These unpopulated
107"
BACKGROUND AND MAIN IDEA,0.26292134831460673,"polytopes serve to interpolate between the training sample points. If we replace the afﬁne activation
108"
BACKGROUND AND MAIN IDEA,0.2651685393258427,"function of the populated polytopes with Gaussian kernel G learned using maximum likelihood
109"
BACKGROUND AND MAIN IDEA,0.26741573033707866,"approach on the training points within the corresponding polytope and prune the unpopulated ones,
110"
BACKGROUND AND MAIN IDEA,0.2696629213483146,"the tail of the kernel will help interpolate between the training sample points while assigning lower
111"
BACKGROUND AND MAIN IDEA,0.27191011235955054,"likelihood to the low density or unpopulated polytope regions of the feature space. This may result in
112"
BACKGROUND AND MAIN IDEA,0.27415730337078653,"better conﬁdence calibration for the proposed modiﬁed approach.
113"
PROPOSED MODEL,0.27640449438202247,"3.3
Proposed Model
114"
PROPOSED MODEL,0.2786516853932584,"Consider the collection of polytope indices P which contains the indices of total ˜p polytopes populated
115"
PROPOSED MODEL,0.2808988764044944,"by the training data. We consider replacing the afﬁne function over the populated polytopes with a
116"
PROPOSED MODEL,0.28314606741573034,"Gaussian kernel G(·; ˆµr, ˆ⌃r). For a particular inference point x, we consider the Gaussian kernel
117"
PROPOSED MODEL,0.2853932584269663,"with the minimum distance from the center of the kernel to the corresponding point:
118 r⇤"
PROPOSED MODEL,0.2876404494382023,x = argmin
PROPOSED MODEL,0.2898876404494382,"r
kµr −xk,
(4)"
PROPOSED MODEL,0.29213483146067415,"where k · k denotes a suitable distance measure. We use Euclidean distance metric while conducting
119"
PROPOSED MODEL,0.2943820224719101,"the simulation and the benchmark datasets experiments in this paper for simplicity. In short, we
120"
PROPOSED MODEL,0.2966292134831461,"modify Equation 3 from the parent ReLU-net or random forest to estimate the class-conditional density
121"
PROPOSED MODEL,0.298876404494382,"as:
122"
PROPOSED MODEL,0.30112359550561796,˜fy(x) = 1 ny X r2P
PROPOSED MODEL,0.30337078651685395,"nryG(x; µr, ⌃r) (r = r⇤"
PROPOSED MODEL,0.3056179775280899,"x),
(5)"
PROPOSED MODEL,0.30786516853932583,"where ny is the total number of samples with label y and nry is the number of samples from class y
123"
PROPOSED MODEL,0.3101123595505618,"that end up in polytope Qr. We add a bias to the class conditional density ˜fy:
124"
PROPOSED MODEL,0.31235955056179776,"ˆfy(x) = ˜fy(x) +
b
log(n).
(6)"
PROPOSED MODEL,0.3146067415730337,"Note that in Equation 6,
b
log(n) ! 0 as the total training points, n ! 1. The class posterior
125"
PROPOSED MODEL,0.31685393258426964,"probability (conﬁdence) ˆgy(x) of class y for a test point x is estimated using the Bayes rule as
126"
PROPOSED MODEL,0.31910112359550563,"follows:
127"
PROPOSED MODEL,0.32134831460674157,ˆgy(x) =
PROPOSED MODEL,0.3235955056179775,"ˆfy(x) ˆPY (y)
PK"
PROPOSED MODEL,0.3258426966292135,"k=1 ˆfk(x) ˆPY (k) ,
(7)"
PROPOSED MODEL,0.32808988764044944,"where ˆPY (y) is the empirical prior probability of class y estimated from the training data. We
128"
PROPOSED MODEL,0.3303370786516854,"estimate the class for a particular inference point x as:
129"
PROPOSED MODEL,0.3325842696629214,ˆy = argmax y2Y
PROPOSED MODEL,0.3348314606741573,"ˆgy(x).
(8)"
DESIDERATA,0.33707865168539325,"3.4
Desiderata
130"
DESIDERATA,0.3393258426966292,"We desire our proposed model to estimate conﬁdence score ˆgy to satisfy the following two desiderata:
131"
DESIDERATA,0.3415730337078652,1. Asymptotic Performance: We want point-wise convergence for our estimated conﬁdence
DESIDERATA,0.3438202247191011,"as n ! 1, i.e., max"
DESIDERATA,0.34606741573033706,y2Y sup
DESIDERATA,0.34831460674157305,x2Rd |gy(x) −ˆgy(x)| ! 0.
DESIDERATA,0.350561797752809,"2. Finite Sample Performance: We want better posterior calibration for ˆgy(x) both in ID and
132"
DESIDERATA,0.35280898876404493,"OOD region compared to that of its parent model.
133"
DESIDERATA,0.3550561797752809,"We theoretically derive the conditions under which we achieve Desiderata 1 in Section 4. However, we
134"
DESIDERATA,0.35730337078651686,"run extensive experiments on various simulation and benchmark datasets in Section 6 to empirically
135"
DESIDERATA,0.3595505617977528,"verify that our proposed approach achieves Desiderata 2.
136"
THEORETICAL RESULTS,0.36179775280898874,"4
Theoretical Results
137"
THEORETICAL RESULTS,0.36404494382022473,"Theorem 1 (Asymptotic Convergence to the True Distribution). Consider a partition rule that
138"
THEORETICAL RESULTS,0.36629213483146067,"partitions Rd into hypercubes of the same size hn > 0. Formally, let Pn = {Q1, Q2, · · · } be a
139"
THEORETICAL RESULTS,0.3685393258426966,"partition of Rd, that is, it partitions Rd into sets of the type ⇧d"
THEORETICAL RESULTS,0.3707865168539326,"i=1[ ihn, ( i + 1)hn), where  i’s are
140"
THEORETICAL RESULTS,0.37303370786516854,"integers. Let n be the total number of samples and nr be the number of data points within polytope
141"
THEORETICAL RESULTS,0.3752808988764045,"Qr. Consider the probability density f estimated for the samples populating the polytopes using
142"
THEORETICAL RESULTS,0.3775280898876405,"Equation 5, denoted as ˆf. The conditions for choosing the Gaussian kernel parameters are:
143"
THEORETICAL RESULTS,0.3797752808988764,"1. The center of the kernel can be any point zr within the polytope Qr as n ! 1,
144"
THEORETICAL RESULTS,0.38202247191011235,"2. The kernel bandwidth along any dimension σr is any positive number always bounded by
145"
THEORETICAL RESULTS,0.3842696629213483,"the polytope bandwidth hn as n ! 1, i.e., σr = Crhn, where 0 < Cr 1.
146"
THEORETICAL RESULTS,0.3865168539325843,"Consider the following assumptions as well:
147"
THEORETICAL RESULTS,0.3887640449438202,"1. The polytope bandwidth hn ! 0 as n ! 1.
148"
THEORETICAL RESULTS,0.39101123595505616,"2. n grows faster than the shrinkage of hn, i.e., nhn ! 1 as hn ! 0 in probability.
149"
THEORETICAL RESULTS,0.39325842696629215,"Given these assumptions, we have that as n ! 1:
150"
THEORETICAL RESULTS,0.3955056179775281,"sup
x2Rd |f(x) −ˆf(x)| ! 0,"
THEORETICAL RESULTS,0.39775280898876403,"where | · | denotes absolute value of the scalar it operates on.
151"
THEORETICAL RESULTS,0.4,"Proof. Please see Appendix A for the proof.
152"
THEORETICAL RESULTS,0.40224719101123596,"Theorem 2 (Asymptotic OOD Convergence). Given n independent and identically distributed
153"
THEORETICAL RESULTS,0.4044943820224719,"training samples {(xi, yi)}n"
THEORETICAL RESULTS,0.4067415730337079,"i=1, we deﬁne the distance of an inference point x from the training points
154"
THEORETICAL RESULTS,0.40898876404494383,"as:
155"
THEORETICAL RESULTS,0.41123595505617977,"dx =
min
i=1,··· ,n kx −xik.
(9)"
THEORETICAL RESULTS,0.4134831460674157,"Here k · k denotes a suitable distance measure as mentioned in Equation 4. Given non-zero and
156"
THEORETICAL RESULTS,0.4157303370786517,"bounded bandwidth of the Gaussians, then we have almost sure convergence for ˆgy as: ˆgy(x)"
THEORETICAL RESULTS,0.41797752808988764,"as
!
157"
THEORETICAL RESULTS,0.4202247191011236,"ˆ
PY (y) as dx ! 1.
158"
THEORETICAL RESULTS,0.42247191011235957,"Proof. Please see Appendix A for the proof.
159"
THEORETICAL RESULTS,0.4247191011235955,"Corollary 2.1. Given the conditions in Theorem 1 and 2, we have:
160 max"
THEORETICAL RESULTS,0.42696629213483145,y2Y sup
THEORETICAL RESULTS,0.42921348314606744,x2Rd |gy(x) −ˆgy(x)| ! 0.
THEORETICAL RESULTS,0.4314606741573034,"Proof. Using the law of large numbers, we have ˆPY (y) = ny n"
THEORETICAL RESULTS,0.4337078651685393,"as
! PY (y) as ny ! 1. The rest of the
161"
THEORETICAL RESULTS,0.43595505617977526,"proof follows from Theorem 1 and 2.
162"
MODEL PARAMETER ESTIMATION,0.43820224719101125,"5
Model Parameter Estimation
163"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.4404494382022472,"5.1
Gaussian Kernel Parameter Estimation
164"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.44269662921348313,"Theorem 1 implies that the Gaussian kernel parameters need to maintain two key properties. We
165"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.4449438202247191,"use the training data within the polytopes to estimate the Gaussian parameters in a way that we
166"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.44719101123595506,"asymptotically satisfy the above two conditions for consistency. To satisfy the ﬁrst condition, we set
167"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.449438202247191,"the kernel center as:
168"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.451685393258427,"ˆµr = 1 nr n
X i=1"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.45393258426966293,"xi (xi 2 Qr).
(10)"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.45617977528089887,"Note that ˆµr in Equation 10 resides always within the corresponding polytope Qr. For improving the
169"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.4584269662921348,"estimates for the kernel bandwidth, we incorporate the samples from other polytopes Qs based on the
170"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.4606741573033708,"similarity wrs between Qr and Qs. Moreover, We constrain our estimated Gaussian kernels to have
171"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.46292134831460674,"diagonal covariance matrix. We use weighted likelihood estimation to estimate the variance ⌃r for a
172"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.4651685393258427,"particular polytope Qr. For simplicity, we will describe the estimation procedure for wrs later. The
173"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.46741573033707867,"weighted likelihood estimation for ⌃r can be written as:
174"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.4696629213483146,"ˆ⌃r = argmin ⌃ − n
X i=1 X s2P"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.47191011235955055,"wrs (xi 2 Qs) log G(xi; ˆµr, ⌃) + λk⌃−1k2"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.47415730337078654,"F ,
(11)"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.4764044943820225,"where we regularize the Frobenius norm of precision matrix ⌃−1 so that ⌃does not become singular
175"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.4786516853932584,"and λ is the regularization parameter. By solving Equation 11, we ﬁnd:
176 ˆ⌃r = P s2P Pn"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.48089887640449436,"i=1 wrs (xi 2 Qs)(xi −ˆµr)(xi −ˆµr)> + λId
P s2P Pn"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.48314606741573035,"i=1 wrs (xi 2 Qs)
,
(12)"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.4853932584269663,"where, Id is a d dimensional identity matrix. However, we want ⌃r to be estimated based on the
177"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.48764044943820223,"samples within Qr so that the second condition for the Gaussian parameters is satisﬁed. Therefore,
178"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.4898876404494382,"as n ! 1 and hn ! 0, the estimated weights wrs should satisfy the condition:
179 wrs !"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.49213483146067416,"⇢0,
if Qr 6= Qs
1,
if Qr = Qs.
(13)"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.4943820224719101,"We need Condition 13 as we will be only using the data within the polytope Qr as n ! 1 to estimate
180"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.4966292134831461,"the Gaussian bandwidth and the estimated Gaussian bandwidth will be bounded by the polytope
181"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.49887640449438203,"bandwidth. Additionally, we use weighted samples to replace the ratio nry"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.501123595505618,"ny in Equation 5 as:
182 ˜wry ˜wy"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.503370786516854,"=
˜wry
P"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.5056179775280899,r2P ˜wry = P s2P Pn
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.5078651685393258,"i=1 wrs (xi 2 Qs) (yi = y)
P r2P P s2P Pn"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.5101123595505618,"i=1 wrs (xi 2 Qs) (yi = y).
(14)"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.5123595505617977,"Note that if we satisfy Condition 13, then we have ˜
wry"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.5146067415730337,"˜
wy ! nry"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.5168539325842697,"ny as n ! 1. Therefore, we modify
183"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.5191011235955056,"Equation 5 as:
184"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.5213483146067416,˜fy(x) = 1 ˜wy X r2P
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.5235955056179775,"˜wryG(x; ˆµr, ˆ⌃r) (r = ˆr⇤"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.5258426966292135,"x),
(15)"
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.5280898876404494,where ˆr⇤
GAUSSIAN KERNEL PARAMETER ESTIMATION,0.5303370786516854,"x = argminr kˆµr −xk. Below, we describe how we estimate wrs for KGF and KGN .
185"
KERNEL GENERATIVE FOREST,0.5325842696629214,"5.2
Kernel Generative Forest
186"
KERNEL GENERATIVE FOREST,0.5348314606741573,"Consider T number of decision trees in a random forest trained on n i.i.d training samples
187"
KERNEL GENERATIVE FOREST,0.5370786516853933,"{(xi, yi)}n"
KERNEL GENERATIVE FOREST,0.5393258426966292,"i=1. Each tree t partitions the feature space into pt polytopes resulting in a set of polytopes:
188"
KERNEL GENERATIVE FOREST,0.5415730337078651,"{{Qt,r}pt r=1}T"
KERNEL GENERATIVE FOREST,0.5438202247191011,t=1. The intersection of these polytopes gives a new set of polytopes {Qr}p
KERNEL GENERATIVE FOREST,0.5460674157303371,"r=1 for the
189"
KERNEL GENERATIVE FOREST,0.5483146067415731,"forest. For any point xr 2 Qr, we push every other sample point xs 2 Qs down the trees. Now, we
190"
KERNEL GENERATIVE FOREST,0.550561797752809,deﬁne the weight w0
KERNEL GENERATIVE FOREST,0.5528089887640449,"rs as:
191 w0"
KERNEL GENERATIVE FOREST,0.5550561797752809,rs = trs
KERNEL GENERATIVE FOREST,0.5573033707865168,"T ,
(16)"
KERNEL GENERATIVE FOREST,0.5595505617977528,where trs is the total number trees xr and xs end up in the same leaf node. Note that 0 w0
KERNEL GENERATIVE FOREST,0.5617977528089888,"rs 1.
192"
KERNEL GENERATIVE FOREST,0.5640449438202247,"If the two samples end up in the same leaves in all the trees, they belong to the same polytope, i.e.
193"
KERNEL GENERATIVE FOREST,0.5662921348314607,"Qr = Qs.
194"
KERNEL GENERATIVE FOREST,0.5685393258426966,"In short, w0"
KERNEL GENERATIVE FOREST,0.5707865168539326,"rs is the fraction of total trees where the two samples follow the same path from the root to
195"
KERNEL GENERATIVE FOREST,0.5730337078651685,a leaf node. We exponentiate w0
KERNEL GENERATIVE FOREST,0.5752808988764045,"rs with a suitable function of n which grows with n so that Condition
196"
KERNEL GENERATIVE FOREST,0.5775280898876405,"13 is satisﬁed:
197"
KERNEL GENERATIVE FOREST,0.5797752808988764,wrs = (w0
KERNEL GENERATIVE FOREST,0.5820224719101124,"rs)O(n).
(17)"
KERNEL GENERATIVE NETWORK,0.5842696629213483,"5.3
Kernel Generative Network
198"
KERNEL GENERATIVE NETWORK,0.5865168539325842,"Consider a fully connected ReLU-net trained on n i.i.d training samples {(xi, yi)}n"
KERNEL GENERATIVE NETWORK,0.5887640449438202,"i=1. We have the
199"
KERNEL GENERATIVE NETWORK,0.5910112359550562,"set of all nodes denoted by Nl at a particular layer l. We can randomly pick a node nl 2 Nl from Al
200"
KERNEL GENERATIVE NETWORK,0.5932584269662922,"at each layer l, and construct a sequence of nodes starting at the input layer and ending at the output
201"
KERNEL GENERATIVE NETWORK,0.5955056179775281,layer which we call an activation path: m = {nl 2 Nl}L
KERNEL GENERATIVE NETWORK,0.597752808988764,l=1. Note that there are N = ⇧L
KERNEL GENERATIVE NETWORK,0.6,"i=1|Nl|
202"
KERNEL GENERATIVE NETWORK,0.6022471910112359,"possible activation paths for a sample in the ReLU-net, where |·| denotes the cardinality or the number
203"
KERNEL GENERATIVE NETWORK,0.604494382022472,"of elements in the set. We index each path by a unique identiﬁer number z 2 N and construct a
204"
KERNEL GENERATIVE NETWORK,0.6067415730337079,"sequence of activation paths as: M = {mz}z=1,··· ,N. Therefore, M contains all possible activation
205"
KERNEL GENERATIVE NETWORK,0.6089887640449438,"pathways from the input to the output of the network.
206"
KERNEL GENERATIVE NETWORK,0.6112359550561798,"While pushing a training sample xi through the network, we deﬁne the activation from a ReLU unit at
207"
KERNEL GENERATIVE NETWORK,0.6134831460674157,"any node as ‘1’ when it has non-negative input and ‘0’ otherwise. Therefore, the activation indicates
208"
KERNEL GENERATIVE NETWORK,0.6157303370786517,"on which side of the afﬁne function at each node the sample falls. The activation for all nodes in an
209"
KERNEL GENERATIVE NETWORK,0.6179775280898876,"activation path mz for a particular sample creates an activation mode az 2 {0, 1}L. If we evaluate
210"
KERNEL GENERATIVE NETWORK,0.6202247191011236,"the activation mode for all activation paths in M while pushing a sample through the network, we
211"
KERNEL GENERATIVE NETWORK,0.6224719101123596,get a sequence of activation modes: Ar = {ar z}N
KERNEL GENERATIVE NETWORK,0.6247191011235955,"z=1. Here r is the index of the polytope where the
212"
KERNEL GENERATIVE NETWORK,0.6269662921348315,"sample falls in.
213"
KERNEL GENERATIVE NETWORK,0.6292134831460674,"If the two sequences of activation modes for two different training samples are identical, they belong
214"
KERNEL GENERATIVE NETWORK,0.6314606741573033,"to the same polytope. In other words, if Ar = As, then Qr = Qs. This statement holds because the
215"
KERNEL GENERATIVE NETWORK,0.6337078651685393,"above samples will lie on the same side of the afﬁne function at each node in different layers of the
216"
KERNEL GENERATIVE NETWORK,0.6359550561797753,"network. Now, we deﬁne the weight w0"
KERNEL GENERATIVE NETWORK,0.6382022471910113,"rs as:
217 w0 rs = PN"
KERNEL GENERATIVE NETWORK,0.6404494382022472,"z=1
(ar"
KERNEL GENERATIVE NETWORK,0.6426966292134831,z = as
KERNEL GENERATIVE NETWORK,0.6449438202247191,"z)
N
.
(18)"
KERNEL GENERATIVE NETWORK,0.647191011235955,Note that 0 w0
KERNEL GENERATIVE NETWORK,0.6494382022471911,"rs 1. In short, w0"
KERNEL GENERATIVE NETWORK,0.651685393258427,"rs is the fraction of total activation paths which are identically
218"
KERNEL GENERATIVE NETWORK,0.6539325842696629,"activated for two samples in two different polytopes r and s. We exponentiate the weights using
219"
KERNEL GENERATIVE NETWORK,0.6561797752808989,"Equation 17.
220"
KERNEL GENERATIVE NETWORK,0.6584269662921348,"Pseudocodes outlining the two algorithms are provided in Appendix C.
221"
KERNEL GENERATIVE NETWORK,0.6606741573033708,"Figure 1:
Visualization of true and estimated posteriors for class 0 from ﬁve binary class
simulation experiments. Row 1: 10,000 training points with 5,000 samples per class sampled from
5 different simulation setups for binary class classiﬁcation. The class labels are indicated by yellow
and blue colors. Row 2: True class conditional posteriors. Row 3: Estimated posteriors from random
forest. Row 4: Estimated posteriors from KGF. Row 5: Estimated posteriors from Deep-net. Row 6:
Estimated posteriors from KGN. The posteriors estimated from KGN and KGF are better calibrated for
both in- and out-of-distribution regions compared to those of their parent algorithms."
EXPERIMENTAL RESULTS,0.6629213483146067,"6
Experimental Results
222"
EXPERIMENTAL RESULTS,0.6651685393258427,"We conduct several experiments on two dimensional simulated datasets and OpenML-CC18 data
223"
EXPERIMENTAL RESULTS,0.6674157303370787,"suite [21] 1 to gain insights on the ﬁnite sample performance of KGF and KGN. The details of the
224"
EXPERIMENTAL RESULTS,0.6696629213483146,"simulation datasets and hyperparameters used for all the experiments are provided in Appendix B.
225"
EXPERIMENTAL RESULTS,0.6719101123595506,"For the simulation setups, we use classiﬁcation error, hellinger distance [22, 23] from the true class
226"
EXPERIMENTAL RESULTS,0.6741573033707865,"conditional posteriors and mean max conﬁdence or posterior [4] as performance statistics. While
227"
EXPERIMENTAL RESULTS,0.6764044943820224,"measuring in-distribution calibration for the datasets in OpenML-CC18 data suite, as we do not know
228"
EXPERIMENTAL RESULTS,0.6786516853932584,"the true distribution, we used adaptive calibration error as deﬁned by Nixon et al. [24] with a ﬁxed
229"
EXPERIMENTAL RESULTS,0.6808988764044944,"bin number of R = 15 across all the datasets. Given n OOD samples, we deﬁne OOD calibration
230"
EXPERIMENTAL RESULTS,0.6831460674157304,"error to measure OOD performance for the benchmark datasets as:
231 %%%%%"
N,0.6853932584269663,"1
n n
X i=1 max"
N,0.6876404494382022,y2Y ( ˆPY |X(y|xi)) −max
N,0.6898876404494382,y2Y ( ˆPY (y))
N,0.6921348314606741,%%%%% .
SIMULATION STUDY,0.6943820224719102,"6.1
Simulation Study
232"
SIMULATION STUDY,0.6966292134831461,"Figure 1 top row shows 10000 training samples with 5000 samples per class sampled within the
233"
SIMULATION STUDY,0.698876404494382,"region [−1, 1] ⇥[−1, 1] from the ﬁve simulation setups described in Appendix B. Therefore, the
234"
SIMULATION STUDY,0.701123595505618,"empty annular region between [−1, 1] ⇥[−1, 1] and [−2, 2] ⇥[−2, 2] is the low density or OOD
235"
SIMULATION STUDY,0.7033707865168539,1https://www.openml.org/s/99
SIMULATION STUDY,0.7056179775280899,"Figure 2: Classiﬁcation error, Hellinger distance from true posteriors, mean max conﬁdence
or posterior for the simulation experiments. The median performance is shown as a dark curve
with shaded region as error bars showing the 25-th and the 75-th percentile. KGF (Left block) and
KGN (Right block) improve both in- and out-of-distribution calibration of their respective parent
algorithms while maintaining nearly similar classiﬁcation accuracy on the simulation datasets."
SIMULATION STUDY,0.7078651685393258,"region in Figure 1. The corresponding true posteriors P[Y = 0|X = x] are shown in the second
236"
SIMULATION STUDY,0.7101123595505618,"row of Figure 1. As shown in Row 3 and 5, RF and DN are really good at estimating the high density
237"
SIMULATION STUDY,0.7123595505617978,"regions of training distribution. However, they overestimate the posteriors in the low density regions
238"
SIMULATION STUDY,0.7146067415730337,"of training distribution. Row 4 and 6 of Figure 1 show KGF and KGN improves the posterior estimation
239"
SIMULATION STUDY,0.7168539325842697,"specially in the low density of the training distribution or OOD regions of the feature space. Because
240"
SIMULATION STUDY,0.7191011235955056,"of axis aligned split in random forest RF and thereby, KGF are less efﬁcient in learning non-linear
241"
SIMULATION STUDY,0.7213483146067415,"decision boundaries like spiral, circle and sinewave simulations than ReLU-net and KGN. Figure 2
242"
SIMULATION STUDY,0.7235955056179775,"quantiﬁes the performance of the algorithms which are visually represented in Figure 1. KGF and KGN
243"
SIMULATION STUDY,0.7258426966292135,"maintain similar classiﬁcation accuracy to those of their parent algorithms. We measure hellinger
244"
SIMULATION STUDY,0.7280898876404495,"distance from the true distribution for increasing training sample size within [−1, 1] ⇥[−1, 1] region
245"
SIMULATION STUDY,0.7303370786516854,"as an index for in-distribution calibration. Column 2 of left and right block in Figure 2 show KGF
246"
SIMULATION STUDY,0.7325842696629213,"and KGN are better at estimating the high density region of training distribution compared to their
247"
SIMULATION STUDY,0.7348314606741573,"parent methods. For measuring OOD performance, we normalize the training data by the maximum
248"
SIMULATION STUDY,0.7370786516853932,"of their l2 norm so that the training data is conﬁned within a unit circle. For inference, we sample
249"
INFERENCE POINTS UNIFORMLY FROM A CIRCLE WHERE THE CIRCLES HAVE INCREASING RADIUS AND PLOT,0.7393258426966293,"1000 inference points uniformly from a circle where the circles have increasing radius and plot
250"
INFERENCE POINTS UNIFORMLY FROM A CIRCLE WHERE THE CIRCLES HAVE INCREASING RADIUS AND PLOT,0.7415730337078652,"mean max posterior for increasing distance from the origin. Therefore, for distance up to 1 we have
251"
INFERENCE POINTS UNIFORMLY FROM A CIRCLE WHERE THE CIRCLES HAVE INCREASING RADIUS AND PLOT,0.7438202247191011,"in-distribution samples and distances farther than 1 can be considered as OOD region. As shown in
252"
INFERENCE POINTS UNIFORMLY FROM A CIRCLE WHERE THE CIRCLES HAVE INCREASING RADIUS AND PLOT,0.7460674157303371,"Column 3 of Figure 2, mean max posteriors or conﬁdence for KGF and KGN converge to the maximum
253"
INFERENCE POINTS UNIFORMLY FROM A CIRCLE WHERE THE CIRCLES HAVE INCREASING RADIUS AND PLOT,0.748314606741573,"of the class priors, i.e., 0.5 as we go farther away from the training data origin.
254"
BENCHMARK DATA STUDY,0.750561797752809,"6.2
Benchmark Data Study
255"
BENCHMARK DATA STUDY,0.7528089887640449,"We used OpenML-CC18 data suite for benchmark dataset study. We exclude any dataset which
256"
BENCHMARK DATA STUDY,0.755056179775281,"contain categorical features or NaN values and conduct our experiments on 46 datasets with varying
257"
BENCHMARK DATA STUDY,0.7573033707865169,"dimensions and sample sizes. For the OOD experiments, we follow a similar setup as that of the
258"
BENCHMARK DATA STUDY,0.7595505617977528,"simulation data. We normalize the training data by their maximum l2 norm and sample 1000 testing
259"
BENCHMARK DATA STUDY,0.7617977528089888,"samples uniformly from each hypersphere where the hyperspheres have increasing radius starting
260"
BENCHMARK DATA STUDY,0.7640449438202247,"from 1 to 5. Figure 3 shows the summary of performance of the algorithms. The extended results for
261"
BENCHMARK DATA STUDY,0.7662921348314606,"Figure 3: Performance summary of KGF and KGN on OpenML-CC18 data suite. The dark red
curve in the middle shows the median of performance on 46 datasets. The shaded region shows the
error bar consisting of the 25-th and the 75-th percentile of the performance statistics. Left: KGF
and KGN maintains performance close to their parent algorithms for classiﬁcation. Middle: KGF
signiﬁcantly improves the in-distribution calibration for random forest and KGN improves ReLU-net’s
in-distribution calibration for high training sample sizes. Right: Both of the proposed approaches
yield highly calibrated conﬁdence in the OOD region."
BENCHMARK DATA STUDY,0.7685393258426966,"each dataset is shown separately in appendix Figure 4, 5, 6, 7, 8 and 9. Figure 3 left column shows on
262"
BENCHMARK DATA STUDY,0.7707865168539326,"average KGF and KGN has nearly similar classiﬁcation accuracy to their respective parent algorithm.
263"
BENCHMARK DATA STUDY,0.7730337078651686,"However, according to Figure 3 middle column, KGF improves the in-distribution calibration for
264"
BENCHMARK DATA STUDY,0.7752808988764045,"random forest by a huge margin. On the contrary, KGN maintains similar in-distribution calibration
265"
BENCHMARK DATA STUDY,0.7775280898876404,"performance to that of its parent ReLU-net. Most interestingly, Figure 3 right column KGN and KGF
266"
BENCHMARK DATA STUDY,0.7797752808988764,"improves OOD calibration of their respective parent algorithms by a huge margin.
267"
DISCUSSION,0.7820224719101123,"7
Discussion
268"
DISCUSSION,0.7842696629213484,"In this paper, we convert deep discriminative models to deep generative models by replacing the afﬁne
269"
DISCUSSION,0.7865168539325843,"function over the polytopes in the discriminative models with a Gaussian kernel. This replacement of
270"
DISCUSSION,0.7887640449438202,"afﬁne function results in better in- and out-of-distribution calibration for our proposed approaches
271"
DISCUSSION,0.7910112359550562,"while maintaining classiﬁcation accuracy close to the parent algorithm. Theoretically, we show under
272"
DISCUSSION,0.7932584269662921,"certain conditions our approaches asymptotically converge to the true training distribution and this
273"
DISCUSSION,0.7955056179775281,"establishes conﬁdence calibration for learning algorithms in in- and out-of-distribution regions as a
274"
DISCUSSION,0.797752808988764,"continuum rather than two different problems.
275"
DISCUSSION,0.8,"For a feature space densely partitioned with small polytopes, we can use Euclidean distance metric in
276"
DISCUSSION,0.802247191011236,"Equation 4. This is because the Euclidean manifold approximation holds locally for the corresponding
277"
DISCUSSION,0.8044943820224719,polytope with index r⇤
DISCUSSION,0.8067415730337079,"x. On the contrary, for a feature space partitioned with large polytopes,
278"
DISCUSSION,0.8089887640449438,"Euclidean distance measure may be a wrong notion of distance, specially when the underlying
279"
DISCUSSION,0.8112359550561797,"manifold is non-Euclidean. Note that the indicator function in Equation 5 and pruning of the
280"
DISCUSSION,0.8134831460674158,"unpopulated polytopes result in an enlargement of the polytopes in our proposed method compared
281"
DISCUSSION,0.8157303370786517,"to that of the parent model in Equation 3. Therefore, our proposed approach while using Euclidean
282"
DISCUSSION,0.8179775280898877,"metric in Equation 4 may have less classiﬁcation accuracy compared to that of its parent algorithm.
283"
DISCUSSION,0.8202247191011236,"A correct measure of distance in all the cases including the aforementioned non-Euclidean one would
284"
DISCUSSION,0.8224719101123595,"be the geodesic distance as explored by Madhyastha et al. [25]. We will explore convolutional
285"
DISCUSSION,0.8247191011235955,"neural nets (CNN) trained on image and language benchmark datasets using geodesic distance in
286"
DISCUSSION,0.8269662921348314,"Equation 4 in our future work. Additionally, the proposed approach needs benchmarking against
287"
DISCUSSION,0.8292134831460675,"other calibration approaches in the literature which are mainly based on image datasets and we will
288"
DISCUSSION,0.8314606741573034,"pursue the benchmarking task in our future work.
289"
REFERENCES,0.8337078651685393,"References
290"
REFERENCES,0.8359550561797753,"[1] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
291"
REFERENCES,0.8382022471910112,"networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International
292"
REFERENCES,0.8404494382022472,"Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
293"
REFERENCES,0.8426966292134831,"pages 1321–1330. PMLR, 06–11 Aug 2017.
294"
REFERENCES,0.8449438202247191,"[2] Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being bayesian, even just a bit, ﬁxes
295"
REFERENCES,0.8471910112359551,"overconﬁdence in ReLU networks. In Hal Daumé III and Aarti Singh, editors, Proceedings
296"
REFERENCES,0.849438202247191,"of the 37th International Conference on Machine Learning, volume 119 of Proceedings of
297"
REFERENCES,0.851685393258427,"Machine Learning Research, pages 5436–5446. PMLR, 13–18 Jul 2020.
298"
REFERENCES,0.8539325842696629,"[3] Haoyin Xu, Kaleab A. Kinfu, Will LeVine, Sambit Panda, Jayanta Dey, Michael Ainsworth,
299"
REFERENCES,0.8561797752808988,"Yu-Chung Peng, Madi Kusmanov, Florian Engert, Christopher M. White, Joshua T. Vogelstein,
300"
REFERENCES,0.8584269662921349,"and Carey E. Priebe. When are Deep Networks really better than Decision Forests at small
301"
REFERENCES,0.8606741573033708,"sample sizes, and how? arXiv preprint arXiv:2108.13637, 2021.
302"
REFERENCES,0.8629213483146068,"[4] Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield
303"
REFERENCES,0.8651685393258427,"high-conﬁdence predictions far away from the training data and how to mitigate the problem. In
304"
REFERENCES,0.8674157303370786,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
305"
REFERENCES,0.8696629213483146,"41–50, 2019.
306"
REFERENCES,0.8719101123595505,"[5] Alexander Meinke, Julian Bitterwolf, and Matthias Hein. Provably robust detection of out-of-
307"
REFERENCES,0.8741573033707866,"distribution data (almost) for free. arXiv preprint arXiv:2106.04260, 2021.
308"
REFERENCES,0.8764044943820225,"[6] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant.
Enhancing the reliability of out-of-
309"
REFERENCES,0.8786516853932584,"distribution image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.
310"
REFERENCES,0.8808988764044944,"[7] Jay Nandy, Wynne Hsu, and Mong Li Lee. Towards maximizing the representation gap between
311"
REFERENCES,0.8831460674157303,"in-domain & out-of-distribution examples. Advances in Neural Information Processing Systems,
312"
REFERENCES,0.8853932584269663,"33:9239–9250, 2020.
313"
REFERENCES,0.8876404494382022,"[8] Weitao Wan, Yuanyi Zhong, Tianpeng Li, and Jiansheng Chen. Rethinking feature distribution
314"
REFERENCES,0.8898876404494382,"for loss functions in image classiﬁcation. In Proceedings of the IEEE conference on computer
315"
REFERENCES,0.8921348314606742,"vision and pattern recognition, pages 9117–9126, 2018.
316"
REFERENCES,0.8943820224719101,"[9] Terrance DeVries and Graham W Taylor. Learning conﬁdence for out-of-distribution detection
317"
REFERENCES,0.8966292134831461,"in neural networks. arXiv preprint arXiv:1802.04865, 2018.
318"
REFERENCES,0.898876404494382,"[10] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier
319"
REFERENCES,0.9011235955056179,"exposure. arXiv preprint arXiv:1812.04606, 2018.
320"
REFERENCES,0.903370786516854,"[11] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High
321"
REFERENCES,0.9056179775280899,"conﬁdence predictions for unrecognizable images. In Proceedings of the IEEE conference on
322"
REFERENCES,0.9078651685393259,"computer vision and pattern recognition, pages 427–436, 2015.
323"
REFERENCES,0.9101123595505618,"[12] Vikash Sehwag, Arjun Nitin Bhagoji, Liwei Song, Chawin Sitawarin, Daniel Cullina, Mung
324"
REFERENCES,0.9123595505617977,"Chiang, and Prateek Mittal. Better the devil you know: An analysis of evasion attacks using
325"
REFERENCES,0.9146067415730337,"out-of-distribution adversarial examples. arXiv preprint arXiv:1905.01726, 2019.
326"
REFERENCES,0.9168539325842696,"[13] Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon,
327"
REFERENCES,0.9191011235955057,"and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. Advances in
328"
REFERENCES,0.9213483146067416,"neural information processing systems, 32, 2019.
329"
REFERENCES,0.9235955056179775,"[14] Diederik P Kingma, Max Welling, et al. An introduction to variational autoencoders. Founda-
330"
REFERENCES,0.9258426966292135,"tions and Trends® in Machine Learning, 12(4):307–392, 2019.
331"
REFERENCES,0.9280898876404494,"[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
332"
REFERENCES,0.9303370786516854,"Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications
333"
REFERENCES,0.9325842696629213,"of the ACM, 63(11):139–144, 2020.
334"
REFERENCES,0.9348314606741573,"[16] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan.
335"
REFERENCES,0.9370786516853933,"Do deep generative models know what they don’t know? arXiv preprint arXiv:1810.09136,
336"
REFERENCES,0.9393258426966292,"2018.
337"
REFERENCES,0.9415730337078652,"[17] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
338"
REFERENCES,0.9438202247191011,"networks. In International conference on machine learning, pages 1321–1330. PMLR, 2017.
339"
REFERENCES,0.946067415730337,"[18] Richard Guo, Ronak Mehta, Jesus Arroyo, Hayden Helm, Cencheng Shen, and Joshua T
340"
REFERENCES,0.9483146067415731,"Vogelstein. Estimating information-theoretic quantities with uncertainty forests. arXiv, pages
341"
REFERENCES,0.950561797752809,"arXiv–1907, 2019.
342"
REFERENCES,0.952808988764045,"[19] Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song, and Peter
343"
REFERENCES,0.9550561797752809,"Flach. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with
344"
REFERENCES,0.9573033707865168,"dirichlet calibration. Advances in neural information processing systems, 32, 2019.
345"
REFERENCES,0.9595505617977528,"[20] Haoyin Xu, Kaleab A Kinfu, Will LeVine, Sambit Panda, Jayanta Dey, Michael Ainsworth,
346"
REFERENCES,0.9617977528089887,"Yu-Chung Peng, Madi Kusmanov, Florian Engert, Christopher M White, et al. When are deep
347"
REFERENCES,0.9640449438202248,"networks really better than decision forests at small sample sizes, and how? arXiv preprint
348"
REFERENCES,0.9662921348314607,"arXiv:2108.13637, 2021.
349"
REFERENCES,0.9685393258426966,"[21] Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Pieter Gijsbers, Frank Hutter, Michel
350"
REFERENCES,0.9707865168539326,"Lang, Rafael G Mantovani, Jan N van Rijn, and Joaquin Vanschoren. Openml benchmarking
351"
REFERENCES,0.9730337078651685,"suites. arXiv preprint arXiv:1708.03731, 2017.
352"
REFERENCES,0.9752808988764045,"[22] Thomas Kailath. The divergence and bhattacharyya distance measures in signal selection. IEEE
353"
REFERENCES,0.9775280898876404,"transactions on communication technology, 15(1):52–60, 1967.
354"
REFERENCES,0.9797752808988764,"[23] C Radhakrishna Rao. A review of canonical coordinates and an alternative to correspondence
355"
REFERENCES,0.9820224719101124,"analysis using hellinger distance. Qüestiió: quaderns d’estadística i investigació operativa,
356"
REFERENCES,0.9842696629213483,"1995.
357"
REFERENCES,0.9865168539325843,"[24] Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran.
358"
REFERENCES,0.9887640449438202,"Measuring calibration in deep learning. In CVPR workshops, volume 2, 2019.
359"
REFERENCES,0.9910112359550561,"[25] Meghana Madhyastha, Gongkai Li, Veronika Strnadová-Neeley, James Browne, Joshua T
360"
REFERENCES,0.9932584269662922,"Vogelstein, Randal Burns, and Carey E Priebe. Geodesic forests. In Proceedings of the 26th
361"
REFERENCES,0.9955056179775281,"ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages
362"
REFERENCES,0.9977528089887641,"513–523, 2020.
363"
