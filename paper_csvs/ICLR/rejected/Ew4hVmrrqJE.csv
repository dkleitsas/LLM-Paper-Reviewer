Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0007535795026375283,"Actor-critic (AC) algorithms have been widely adopted in decentralized multi-agent
systems to learn the optimal joint control policy. However, existing decentralized
AC algorithms either need to share agents’ sensitive information, e.g., local actions
and policies , or are not sample and communication-efﬁcient. In this work, we
develop two decentralized AC and natural AC (NAC) algorithms that are sample and
communication-efﬁcient and avoid sharing agents’ local actions and policies . In
both algorithms, agents share only noisy rewards and adopt mini-batch local policy
gradient updates to improve sample and communication efﬁciency. Particularly
for decentralized NAC, we develop a decentralized Markovian SGD algorithm
with an adaptive mini-batch size to efﬁciently compute the natural policy gradient.
Under Markovian sampling and linear function approximation, we prove that
the proposed decentralized AC and NAC algorithms achieve the state-of-the-art
sample complexities O(ϵ−2 ln ϵ−1) and O(ϵ−3 ln ϵ−1), respectively, and achieve
an improved communication complexity O(ϵ−1 ln ϵ−1). Numerical experiments
demonstrate that the proposed algorithms achieve lower sample and communication
complexities than the existing decentralized AC algorithm."
INTRODUCTION,0.0015071590052750565,"1
INTRODUCTION"
INTRODUCTION,0.002260738507912585,"Multi-agent reinforcement learning (MARL) has achieved great success in various application
domains, including control (66; 10; 51), robotics (64), wireless sensor networks (24; 67), intelligent
systems (71), etc. In MARL, a set of fully decentralized agents interact with a dynamic environment
following their own policies and collect local rewards, and their goal is to collaboratively learn the
optimal joint policy that achieves the maximum expected accumulated reward."
INTRODUCTION,0.003014318010550113,"Classical policy optimization algorithms have been well developed and studied, e.g., policy gradient
(PG) (49), actor-critic (AC) (23) and natural actor-critic (NAC) (37; 7). In particular, AC-type
algorithms are more computationally tractable and efﬁcient as they take advantages of both policy
gradient and value-based updates. However, in the multi-agent setting, decentralized AC is more
challenging to design compared with the centralized AC, as the algorithm updates involve sensitive
agent information, e.g., local actions, rewards and policies, which must be kept locally in the
decentralized learning process. In the existing designs of decentralized AC, the agents need to share
either their local actions (70; 69; 8; 36; 72; 27; 19; 26; 11) or local rewards (15; 33; 32) with their
neighbors, and hence are not desired. This issue is addressed by Algorithm 2 of (70) at the cost of
learning a parameterized model to estimate the averaged reward, yet this approach requires extra
learning effort and the reward estimation can be inaccurate. Moreover, existing decentralized AC
algorithms are not sample and communication-efﬁcient, and do not have ﬁnite-time convergence
guarantee, especially under the practical Markovian sampling setting. Therefore, we aim to address
the following important question."
INTRODUCTION,0.0037678975131876413,"• Q1: Can we develop a decentralized AC algorithm that is convergent, sample and communication-
efﬁcient, and does not require sharing agents’ local actions and policies ?"
INTRODUCTION,0.00452147701582517,"On the other hand, as an important variant of the decentralized AC, decentralized NAC algorithm has
not been formally developed and rigorously analyzed in the existing literature. In particular, a major
challenge is that we need to develop a fully decentralized and computationally tractable scheme to"
INTRODUCTION,0.0052750565184626974,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.006028636021100226,"compute the inverse of the high dimensional Fisher information matrix, and this scheme must be both
sample and communication efﬁcient. Hence, we want to ask:"
INTRODUCTION,0.006782215523737754,"• Q2: Can we develop a computationally tractable and communication-efﬁcient decentralized NAC
algorithm that has a low ﬁnite-time sample and communication complexity?"
INTRODUCTION,0.007535795026375283,"In this study, we provide afﬁrmative answers to the above two questions by developing fully decen-
tralized AC and NAC algorithms that are sample and communication-efﬁcient, and do not reveal
agents’ local actions and policies. We also develop rigorous ﬁnite-time analysis of these algorithms
under Markovian sampling. Our contributions are summarized as follows."
INTRODUCTION,0.008289374529012811,"Table 1: List of complexities of the existing AC and NAC algorithms for achieving
E[∥∇J(ω)∥2] ≤ϵ and E[J(ω∗) −J(ω))] ≤ϵ, respectively."
INTRODUCTION,0.00904295403165034,"Algorithm
Papers
Share local
Sampling
Sample
Communication
action/policy
scheme
complexity
complexity"
INTRODUCTION,0.009796533534287867,Centralized AC
INTRODUCTION,0.010550113036925395,"(54)
–
i.i.d.
O(ϵ−36)
–
(38)
–
i.i.d.
e
O(ϵ−4)
–
(25)
–
i.i.d.
O(ϵ−2.5)
–
(61)
–
Markovian
O(ϵ−2.5 ln3 ϵ−1)
–
(56)
–
Markovian
e
O(ϵ−2.5)
–
(60)
–
Markovian
O(ϵ−2 ln ϵ−1)
–"
INTRODUCTION,0.011303692539562924,Decentralized AC
INTRODUCTION,0.012057272042200452,"(70; 69; 15)
(72; 27; 26)
×
Markovian
–
–
(70; 47; 33)
✓
Markovian
–
–
This work
✓
Markovian
O(ϵ−2 ln ϵ−1)
O(ϵ−1 ln ϵ−1)"
INTRODUCTION,0.01281085154483798,"Centralized NAC
(54)
–
i.i.d.
O(ϵ−36)
–
(61)
–
Markovian
O(ϵ−4 ln2 ϵ−1)
–
(60)
–
Markovian
O(ϵ−3 ln ϵ−1)
–"
INTRODUCTION,0.013564431047475508,"Decentralized NAC
This work
✓
Markovian
O(ϵ−3 ln ϵ−1)
O(ϵ−1 ln ϵ−1)"
OUR CONTRIBUTIONS,0.014318010550113038,"1.1
OUR CONTRIBUTIONS"
OUR CONTRIBUTIONS,0.015071590052750565,"We develop fully decentralized AC and NAC algorithms and analyze their ﬁnite-time sample and
communication complexities under Markovian sampling. Our results and comparisons to existing
works are summarized in Table 1 1. Our decentralized AC and NAC algorithms adopt the following
novel designs to accurately estimate the policy gradient in an efﬁcient way."
OUR CONTRIBUTIONS,0.015825169555388093,"• Noisy Rewards: In a decentralized setting, local policy gradients (estimated locally by the agents)
involve the average of all agents’ local rewards. To help agents estimate this averaged reward
without revealing the raw local rewards , we let them share Gaussian-corrupted local rewards with
their neighbor, and the variance of the Gaussian noise can be adjusted by each agent to reach its
desired level."
OUR CONTRIBUTIONS,0.016578749058025623,"• Mini-batch Updates: We apply mini-batch Markovian sampling to both the decentralized actor
and critic updates. This approach 1) helps the agents obtain accurate estimations of the corrupted
averaged reward; 2) signiﬁcantly reduces the variance of policy gradient caused by Markovian
sampling; and 3) signiﬁcantly reduces the communication frequency and complexity."
OUR CONTRIBUTIONS,0.01733232856066315,"Moreover, for our decentralized NAC algorithm, we additionally adopt the following design to
compute the inverse of the Fisher information matrix in an efﬁcient and decentralized way."
OUR CONTRIBUTIONS,0.01808590806330068,"• Decentralized Natural Policy Gradient: By reformulating the natural policy gradient as the min-
imizer of a quadratic program, we develop a decentralized SGD with Markovian sampling that
allows the agents to estimate the corresponding local natural gradients by communicating only"
OUR CONTRIBUTIONS,0.018839487565938208,"1In this table, e
O(·) hides all logarithm factors. In (25), the sample complexity has been established for
various AC-type algorithms, and we compare with the best one. In (70), the Algorithm 1 needs to share local
actions while the Algorithm 2 does not."
OUR CONTRIBUTIONS,0.019593067068575734,Under review as a conference paper at ICLR 2022
OUR CONTRIBUTIONS,0.020346646571213264,"scalar variables with their neighbors. In particular, in order to minimize the sample complexity of
the decentralized SGD, we set the batch size to be exponentially increasing."
OUR CONTRIBUTIONS,0.02110022607385079,"Theoretically, for the ﬁrst time, we provide ﬁnite-time convergence analysis of decentralized AC
and NAC algorithms under Markovian sampling. Speciﬁcally, we prove that our decentralized AC
and NAC algorithms achieve the overall sample complexities O(ϵ−2 ln ϵ−1) and O(ϵ−3 ln ϵ−1),
respectively, and both match the state-of-the-art complexities of their centralized versions (60).
Moreover, both decentralized algorithms achieve a signiﬁcantly reduced overall communication
complexity O(ϵ−1 ln ϵ−1). In particular, our analysis involves new technical developments. First, we
need to characterize the bias and variance of (natural) policy gradient and stochastic gradient caused
by the noisy rewards and the inexact local averaging steps, and control them with proper choices of
batch sizes and number of local averaging steps. Second, when using decentralized Markovian SGD
to compute the inverse Fisher information matrix, we need to use an exponentially increasing batch
size to achieve an optimized sample complexity bound. Such a Markovian SGD with adaptive batch
size has not been studied before and can be of independent interest."
RELATED WORK,0.02185380557648832,"1.2
RELATED WORK"
RELATED WORK,0.02260738507912585,"Convergence analysis of AC and NAC. In the centralized setting, the AC algorithm was ﬁrstly
proposed by (23) and later developed into the natural actor-critic (NAC) algorithm (37; 7). Speciﬁcally,
(37) does not provide any convergence result, while (22; 5) and (20; 6; 7) establish the asymptotic
convergence rate of centralized AC and NAC, respectively, which is weaker than our ﬁnite-time
convergence results. Furthermore, (54; 25; 38; 61; 56) and (54) establish the ﬁnite-time convergence
rate of centralized AC and NAC, respectively. Please refer to Table 1 for their sample complexities.
Moreover, (60) improve the ﬁnite-time sample complexities of the above works to the state-of-the-
art result for both centralized AC and NAC by leveraging mini batch sampling, and our sample
complexities match these state-of-the-art results ."
RELATED WORK,0.023360964581763375,"In the decentralized setting, a few works have established the almost sure convergence result of
AC (15; 27; 47; 33), but they do not characterize the ﬁnite-time convergence rate and the sample
complexity. To the best of our knowledge, there is no formally developed decentralized NAC
algorithm."
RELATED WORK,0.024114544084400905,"Decentralized TD-type algorithms. The ﬁnite-time convergence of decentralized TD(0) has been
obtained using i.i.d samples (52; 14; 53; 28) and Markovian samples (46; 53), respectively, without
revealing the agents’ local actions, policies and rewards . Decentralized off-policy TD-type algorithms
have been studied in (34; 45; 9; 12)."
RELATED WORK,0.024868123587038434,"Decentralized AC in other MARL settings. Some works apply decentralized AC to other MARL
settings that are very different from ours. For example, (44; 36; 17; 11; 57) studied adversarial game.
(30) studied a mixed cooperative-competitive environment where each agent maximizes its own Q
function (30). (11) proposed Delay-Aware Markov Game which considers delay in Markov game.
(68; 31) studied linear control system and linear quadratic regulators instead of an MDP. (55) studied
sequential prisoner’s dilemmas."
RELATED WORK,0.02562170308967596,"Policy gradient algorithms. Policy gradient (PG) and natural policy gradient (NPG) are popular
policy optimization algorithms. (1) characterizes the iteration complexity (i.e., number of episodes)
of centralized PG and NPG algorithms by assuming access to exact policy gradient. They also
established a sample complexity result O(ϵ−6) in the i.i.d. setting for NPG, which is worse than the
state-of-the-art result O(ϵ−3 ln ϵ−1) of both centralized NAC (60) and our decentralized NAC with
Markovian samples. (3) proposes decentralized PG in a simple cooperative MARL setting, where all
the agents share one action and the same policy, and they establish a iteration complexity in the order
of O(ϵ−4). (13; 73) apply decentralized PG to Markov games. (2) applies decentralized NPG to a
different cooperative MARL setting where each agent observes its own state, takes its own action and
has access to these information of its neighbors."
RELATED WORK,0.02637528259231349,"Value-based algorithms. Value-based algorithms have also been develop for MARL. Speciﬁcally,
(21; 18) develop distributed Q-learning in a simpliﬁed cooperative MARL setting, where the agents
share a joint action. In particular, (18) characterizes the convergence rate of a value function-based
convergence error, which is a different optimality measure from that of AC-type algorithms. (35)
applies distributed Q-learning to another cooperative MARL setting, where each agent observes"
RELATED WORK,0.027128862094951016,Under review as a conference paper at ICLR 2022
RELATED WORK,0.027882441597588545,"its own state and takes its own action. It establishes an asymptotic convergence guarantee, and no
convergence rate is given. (40) develops a value propagation algorithm that uses primal-dual method
to minimize a soft Bellman error in the MARL setting. Under an assumption that the variance of
the stochastic gradient is uniformly bounded, it establishes a non-asymptotic convergence rate to an
approximate stationary point."
REVIEW OF MULTI-AGENT REINFORCEMENT LEARNING,0.028636021100226075,"2
REVIEW OF MULTI-AGENT REINFORCEMENT LEARNING"
REVIEW OF MULTI-AGENT REINFORCEMENT LEARNING,0.0293896006028636,"In this section, we ﬁrst introduce some standard settings of RL. Consider an agent that starts from
an initial state s0 ∼ξ and collects a trajectory of Markovian samples {st, at, Rt}t ⊂S × A × R
by interacting with an underlying environment (with transition kernel P) following a parameterized
policy πω with induced stationary state distribution µω. The agent aims to learn an optimal policy
that maximizes the expected accumulated reward J(ω) = (1 −γ)E
 P∞
t=0 γtRt

, where γ ∈(0, 1)
is a discount factor. The marginal state distribution is denoted as Pω(st) and the visitation measure
is deﬁned as νω(s) := (1 −γ) P∞
t=0 γtPω(st = s), both of which depend on the policy parameter
ω ∈Ωand the transition kernel P. We also deﬁne the mixed transition kernel Pξ(·|s, a) :=
γP(·|s, a) + (1 −γ)ξ(·), whose stationary state distribution is known to be νω."
REVIEW OF MULTI-AGENT REINFORCEMENT LEARNING,0.03014318010550113,"In the multi-agent RL (MARL) setting, M agents are connected via a fully decentralized network
and interact with a shared environment. The network topology is speciﬁed by a doubly stochastic
communication matrix W ∈RM×M. At any time t, all the agents share a common state st. Then,
every agent m takes an action a(m)
t
following its own current policy π(m)
t
(·|st) parameterized by
ω(m)
t
. After all the actions at := {a(m)
t
}M
m=1 are taken, the global state st transfers to a new state
st+1 and every agent m receives a local reward R(m)
t
. In this MARL setting, each agent m can only
access the global state {st}t, its own actions {a(m)
t
}t and rewards {R(m)
t
}t and policy π(m)
t
. Next,
deﬁne the joint policy πt(at|st) := QM
m=1 π(m)
t
(a(m)
t
|st) parameterized by ωt = [ω(1)
t
; . . . ; ω(M)
t
],
and deﬁne the average reward Rt :=
1
M
PM
m=1 R(m)
t
. The goal of the agents is to collaboratively
learn the optimal joint policy that maximizes the expected accumulated average reward J(ω) :="
REVIEW OF MULTI-AGENT REINFORCEMENT LEARNING,0.03089675960813866,"(1 −γ)E
 P∞
t=0 γtRt
s0 ∼ξ

. Throughout, we consider the setting that the agents keep interacting
with the environment and observing a trajectory of MDP transition samples, which are further used
to learn the optimal joint policy."
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.031650339110776186,"3
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.03240391861341371,"In this section, we propose a decentralized actor-critic (AC) algorithm that is sample and
communication-efﬁcient and avoids revealing agents’ local actions, policies and raw rewards ."
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.033157498116051246,"We ﬁrst consider a direct extension of the centralized AC to the decentralized case. As each agent m
has its own policy π(m), it aims to update the policy parameter ω(m) using the local policy gradient
∇ω(m)J(ω). Under linear approximation of the value function Vθ(s) ≈φ(s)⊤θ where φ(s) is the
feature vector, the local policy gradient has the following stochastic approximation."
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.03391107761868877,"∇ω(m)J(ωt)≈
h"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.0346646571213263,"Rt + γφ(s′
t+1)⊤θ(m)
t
−φ(st)⊤θ(m)
t
i
ψ(m)
t
(a(m)
t
|st),
(1)"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.03541823662396383,"where a(m)
t
∼π(m)
t
(·|st), st+1 ∼Pξ(·|st, at), s′
t+1 ∼P(·|st, at).
(2)"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.03617181612660136,"Here, θ(m)
t
is agent m’s critic parameter and ψ(m)
t
(a(m)
t
|st) = ∇ω(m) ln π(m)
t
(a(m)
t
|st) is the local
score function. It is clear that both θ(m)
t
and ψ(m)
t
(a(m)
t
|st) can be obtained/computed by agent
m using the local information. However, the average reward Rt requires agent m aggregating
the local rewards from all the other agents, which raises concerns. In the existing literature on
decentralized AC, this issue is avoided by either 1) sharing the agents’ actions with each other instead
(70; 69; 8; 36; 72; 27; 19; 26; 11), yet the action information is also highly sensitive; or 2) learning a
parameterized model to estimate the average reward (70), which requires extra learning effort and
does not provide an accurate estimation. Hence, we are motivated to develop a simpler approach that
provides accurate estimation of the average reward while avoids sharing raw local rewards ."
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.03692539562923888,Under review as a conference paper at ICLR 2022
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.037678975131876416,"1. Efﬁcient Policy Gradient Estimation. We propose a decentralized policy gradient estimation
scheme that improves the sample and communication efﬁciency and avoids revealing the agents’ local
actions, policies and raw rewards . First, in order for each agent to estimate the average reward Rt in
eq. (1), we let each agent m generate a noisy local reward eR(m)
t
= R(m)
t
(1 + e(m)
t
) and share with
other agents, where e(m)
t
∼N(0, σ2
m) 2. The noise variance is determined by the agent based on its
desired level. Speciﬁcally, every agent m ﬁrst initializes its local estimation of the averaged reward
R
(m)
t
using its own noisy reward, i.e., R
(m)
t,0 = eR(m)
t
. Then, each agent m performs decentralized
local averaging with its neighbors Nm for T ′ iterations, i.e.,"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.03843255463451394,"R
(m)
t,ℓ+1 = P"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.03918613413715147,"m′∈Nm Wm,m′ R
(m)
t,ℓ,
ℓ= 0, 1, . . . , T ′ −1.
(3)"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.039939713639788994,"After that, agent m obtains the ﬁnal estimate R
(m)
t
:= R
(m)
t,T ′. It can be shown that R
(m)
t
converges"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.04069329314242653,"to the averaged noisy reward
1
M
PM
m=1 eR(m)
t
exponentially fast. Ideally, by averaging these noisy
local rewards over the M agents, the variance of the noise in the ﬁnal estimation will be scaled by a
factor of
1
M . Therefore, to obtain an accurate estimation, the network needs to have a sufﬁciently
large number of agents, which does not always hold in practice."
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.04144687264506405,"To address this issue, we let each agent m collect a mini-batch of N Markovian samples in each
iteration t to estimate the local policy gradient, which then takes the following form."
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.04220045214770158,b∇ω(m)J(ωt) = 1 N
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.04295403165033911,"(t+1)N−1
X i=tN h"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.04370761115297664,"R
(m)
i
+ γφ(s′
i+1)⊤θ(m)
t
−φ(si)⊤θ(m)
t
i
ψ(m)
t
(a(m)
i
|si),
(4)"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.044461190655614165,"where R
(m)
i
is an estimation of Ri obtained by agent m following the process described in eq. (3)."
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.0452147701582517,"Intuitively, each R
(m)
i
is corrupted by a zero-mean noise with variance O( 1"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.045968349660889224,"M ) due to averaging over
the agents. Then, the mini-batch samples further help scale the noise variance by a factor of
1
N .
Consequently, with a sufﬁciently large batch size N, we can obtain an accurate estimation of the
averaged reward and hence the policy gradient. To summarize, our decentralized policy gradient
estimation scheme has the following advantages."
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.04672192916352675,"• Avoid sharing raw rewards: The agents share only noisy rewards eR(m)
t
with their neighbors, and
the noise variance can be adjusted based on the desired level such that R(m)
t
is unknown to the other
agents. This is in contrast to other decentralized AC algorithms where the agents need to either
share local actions, rewards or collaboratively learn an additional parameterized reward model.
• Sample-efﬁcient: The mini-batch updates help greatly suppress the noise variance of the local
policy gradient in (4) and improve its estimation accuracy. On the other hand, mini-batch policy
gradient also helps reduce the optimization variance caused by Markovian sampling and leads to a
good ﬁnite-time sample complexity as we prove later. We note that there is no trade-off between
noise variance and sample efﬁciency here, because for highly noisy local rewards we can choose a
large batch size to suppress the overall estimation error to the desired level.
• Communication-efﬁcient: The mini-batch updates also signiﬁcantly reduce the communication
frequency as well as the complexity as we prove later. In comparison, the existing decentralized
AC requires to perform one communication round per Markovian sample.
Remark. We note that the local mini-batch policy gradient update in eq. (4) can be computed in an
accumulative way by the agent when observing the mini-batch of transition samples on the ﬂy. There
is no need to store all these samples and perform a large batch computation."
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.04747550866616428,"2. Fully Decentralized Critic Update. The critic parameters of the agents are updated following
the standard decentralized TD-type algorithm. Speciﬁcally, consider the t-th local critic update of
each agent m. It ﬁrst collects a mini-batch of Nc Markovian samples. Then, starting from a ﬁxed
initialization θ(m)
t,0 = θ−1, agent m performs Tc iterations of decentralized TD updates as follows,"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.04822908816880181,"where {st}t∈N follows the transition kernel P and a(m)
t
∼π(m)
t
(·|st): for t′ = 0, 1, ..., Tc −1,"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.048982667671439335,"θ(m)
t,t′+1 =
X"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.04973624717407687,"m′∈Nm
Wm,m′ θ(m′)
t,t′
+ β Nc"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.050489826676714394,"(t+1)Nc−1
X i=tNc"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.05124340617935192,"h
R(m)
i
+ γφ(si+1)⊤θ(m)
t,t′ −φ(si)⊤θ(m)
t,t′
i
φ(si). (5)"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.05199698568198945,"2More generally, any noise with zero mean and variance σ2
m will work."
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.05275056518462698,Under review as a conference paper at ICLR 2022
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.053504144687264506,"Then, the updated critic parameter is set to be θ(m)
t
:= θ(m)
t,Tc. To further reduce the consensus error,
we perform additional T ′
c steps of local model averaging, as also adopted in (12). The pseudo code of
the entire decentralized AC algorithm is summarized in Algorithms 1 and 2 below."
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.05425772418990203,"Algorithm 1 Decentralized Actor-Critic
Initialize: Actor-critic parameters ω0, θ−1.
for actor iterations t = 0, 1, . . . , T −1 do"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.055011303692539565,"▶Critic update on θt: by Algorithm 2.
▶Collect N Markovian samples by eq. (2).
for agents m = 1, ..., M in parallel do"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.05576488319517709,"▶Send noisy local rewards and perform
T ′ local average steps following eq. (3).
▶Compute the estimated local policy
gradient b∇ω(m)J(ωt) following eq. (4).
▶Actor update on ωt:
ω(m)
t+1 = ω(m)
t
+ αb∇ω(m)J(ωt).
end
end"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.05651846269781462,"Output: ω e
T with eT
uniform
∼
{1, 2, . . . , T}."
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.05727204220045215,"Algorithm 2 Decentralized TD (critic update)
Initialize: Critic parameter θt,0 = θ−1.
for critic iterations t′ = 0, 1, . . . , Tc −1 do"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.058025621703089676,"▶Collect Nc Markovian samples following
policy πt and transition kernel P.
for agents m = 1, ..., M in parallel do"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.0587792012057272,"▶Send local critic parameters.
▶Decentralized TD update in eq. (5).
end
end
for iterations t′ = Tc, ..., Tc + T ′
c −1 do
for agents m = 1, ..., M in parallel do"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.059532780708364735,"▶θ(m)
t,t′+1 = P"
SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC,0.06028636021100226,"m′∈Nm Wm,m′ θ(m′)
t,t′ .
end
end
Output: θt = θt,Tc+T ′c."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.06103993971363979,"4
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.06179351921627732,"In this section, we analyze the ﬁnite-time convergence of Algorithm 1 and characterize the sample
and communication complexities. All the notations and universal constants are summarized in
Appendices A & F respectively. We ﬁrst introduce the following standard assumptions that have been
widely adopted in the existing literature.
Assumption 1. Regarding the transition kernels P, Pξ, denote µω, νω respectively as their stationary
state distributions under policy πω and denote P, Pξ respectively as their marginal state distributions.
Then, there exist constants κ > 0 and ρ ∈(0, 1) such that for all t ≥0,"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.06254709871891484,"sup
s∈S
dT V
 
P (st | s0 = s) , µω

≤κρt,
sup
s∈S
dT V
 
Pξ (st | s0 = s) , νω

≤κρt
(6)"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.06330067822155237,"where dT V (P, Q) denotes the total-variation distance between probability measures P and Q.
Assumption 2. There exist constants Cψ, Lψ, Lπ > 0 such that for all ω, eω ∈Ω, s ∈S and a ∈A,
∥ψω(a|s)∥≤Cψ, ∥ψeω(a|s) −ψω(a|s)∥≤Lψ∥eω −ω∥and dTV
 
πeω(·|s), πω(·|s)

≤Lπ∥eω −ω∥.
Assumption 3. There exists Rmax > 0 such that for any agent m and any Markovian sample
(s, a, s′), we have 0 ≤R(m)(s, a, s′) ≤Rmax."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.0640542577241899,"Assumption 4. The feature vectors satisfy ∥φ(s)∥≤1 for all s ∈S. There exists a constant λφ > 0
such that λmin
 
Es∼µω[φ(s)φ(s)⊤]

≥λφ for all ω."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.06480783722682742,"Assumption 5. The communication matrix W ∈RM×M of the decentralized network is doubly
stochastic, and its second largest singular value satisﬁes σW ∈[0, 1)."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.06556141672946496,"Assumption 1 has been widely considered in the existing literature (4; 38; 63; 58; 42; 60; 12) and it
holds for any time-homogeneous Markov chains with ﬁnite-state space and any uniformly ergodic
Markov chains. Assumption 2 introduces boundedness and Lipschitzness to the policy and its
associated score function (65; 60), and holds for many parameterized policies such as Gaussian policy
(25) and Boltzman policy (16). Assumption 4 can always hold by normalizing the feature vector φ(s)
Assumption 5 is widely used in decentralized optimization (43; 41) and multi-agent reinforcement
learning (46; 53; 12), which ensures that all the decentralized agents can reach a global consensus."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.06631499623210249,"With the above assumptions, we obtain the following ﬁnite-time convergence result of the decen-
tralized AC algorithm. Throughout, we follow (60; 56) and deﬁne the critic approximation error as
ζcritic
approx := supω Es∼νω(Vω(s)−φ(s)⊤θ∗
ω)2 where θ∗
ω is the optimal critic parameter (see its deﬁnition
right before Lemma D.3 in Appendix D). We also deﬁne sample complexity as the total number of
Markovian samples required for achieving E[∥∇J(ω)∥2] ≤ϵ. All the universal constants are listed
in Appendix F."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.06706857573474001,Under review as a conference paper at ICLR 2022
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.06782215523737754,"Theorem 1. Let Assumptions 1–5 hold and adopt the hyperparameters of the decentralized TD
in Algorithm 2 following Lemma D.4. Choose α ≤
1
4LJ , T ′ ≥
ln M
2 ln σ−1
W . Then, the output of the
decentralized AC in Algorithm 1 has the following convergence rate."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.06857573474001508,"E
h∇J(ω e
T )
2i
≤4Rmax"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.0693293142426526,"Tα
+4(c4σ2T ′
W +c5β2σ2T ′
c
W )+4c6

1−λB"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.07008289374529013,"8 β
Tc
+ 4c7"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.07083647324792766,N + 4c8
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.07159005275056518,"Nc
+64C2
ψζcritic
approx."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.07234363225320271,"Moreover, to achieve E
∇J(ω e
T )
2
≤ϵ for any ϵ ≥128C2
ψζcritic
approx, we can choose T, N, Nc =
O(ϵ−1) and Tc, T ′
c, T ′ = O(ln ϵ−1). Consequently, the overall sample complexity is T(TcNc+N) =
O(ϵ−2 ln ϵ−1), and the communication complexities for synchronizing linear model parameters and
rewards are T(Tc + T ′
c) = O(ϵ−1 ln ϵ−1) and TT ′ = O(ϵ−1 ln ϵ−1), respectively."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.07309721175584025,"Remark. We note that the constraint on ϵ is naturally induced by the critic approximation error. In
particular, if this approximation errors vanish, for example, when the dimension of features equals the
number of states and the parameterized policy space is sufﬁciently expressive, then we can achieve
arbitrarily small target accuracy."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.07385079125847777,"To the best of our knowledge, Theorem 1 provides the ﬁrst ﬁnite-time analysis of decentralized AC
under Markovian sampling. To elaborate, under any pre-speciﬁed variance σ2
m of the reward noise, our
result shows that the gradient norm asymptotically converges to the order O(N −1 + N −1
c
+ ζcritic
approx),
which can be made arbitrarily close to the linear model approximation error ζcritic
approx by choosing
sufﬁciently large batch sizes N, Nc. In particular, exact gradient convergence can be achieved when
there is no model approximation error. The overall sample complexity of our decentralized AC is
O(ϵ−2 ln ϵ−1), matching the state-of-the-art complexity result for centralized AC (60). Moreover,
with proper choices of the batch sizes N, Nc = O(ϵ−1), the overall communication complexity is
signiﬁcantly reduced to O(ϵ−1 ln ϵ−1)."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.0746043707611153,"The proof of Theorem 1 relies on developing several new algorithmic and technical developments
to reduce the communication complexity of both the decentralized actor and critic updates while
establishing tight convergence error bounds for both components. We further elaborate on these novel
technical developments below."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.07535795026375283,"• To achieve an overall reduced communication complexity, we adopt mini-batch updates in both the
actor and critic steps to reduce the communication frequency, as opposed to the single sample-based
update adopted in the existing work on decentralized TD learning (46). Speciﬁcally, in the analysis
of the decentralized TD described in Algorithm 2 (see Lemma D.4), the mini-batch updates with
batch size O(ϵ−1) substantially improve the communication complexity from O(ϵ−1 ln ϵ−1) to
O(ln ϵ−1) while help achieve the state-of-the-art sample complexity. Eventually, this together
with the mini-batch updates in the decentralized actor steps help achieve the desired overall low
communication complexity.
• To achieve the state-of-the-art overall sample complexity, it is critical that the policy gradient
vanishes fast, which further requires a fast convergence of the decentralized TD learning. However,
although the standard Tc decentralized mini-batch TD updates can yield a small convergence error
for the global critic model (i.e., the average of all local critic models), it still suffers from a relatively
large consensus error. To resolve this issue, we introduce an additional T ′
c global consensus steps
in Algorithm 2 to reduce the consensus error. It is proved that a small number O(ln ϵ−1) of such
steps sufﬁces to yield a desired TD error."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC,0.07611152976639035,"• We inject random noises into the local raw rewards R(m)
t
to protect the information. These noises
introduce additional Markovian bias and variance to the local policy gradients in (4). Fortunately,
as proved in Lemma D.6, by applying mini-batch policy gradient updates, we are able to control
the bias and variance induced by the noisy rewards to an acceptable level that does not affect the
overall sample and communication complexities."
DECENTRALIZED NATURAL AC,0.07686510926902788,"5
DECENTRALIZED NATURAL AC"
DECENTRALIZED NATURAL AC,0.07761868877166542,"Natural actor-critic (NAC) is a popular variant of the AC algorithm. It utilizes a Fisher information
matrix to perform a natural policy gradient update, which helps attain the globally optimal solution in
terms of the function value convergence. In this section, we develop a fully decentralized version of
the NAC algorithm that is sample and communication-efﬁcient."
DECENTRALIZED NATURAL AC,0.07837226827430294,Under review as a conference paper at ICLR 2022
DECENTRALIZED NATURAL AC,0.07912584777694047,"Algorithm 3 Decentralized Natural Actor-Critic
Initialize: Actor-critic parameters ω0, θ−1, natural policy gradient h−1.
for actor iterations t = 0, 1, . . . , T −1 do"
DECENTRALIZED NATURAL AC,0.07987942727957799,"▶Critic update on θt: by Algorithm 2.
for agents m = 1, ..., M in parallel do"
DECENTRALIZED NATURAL AC,0.08063300678221552,"for iterations k = 0, 1, . . . , K −1 do"
DECENTRALIZED NATURAL AC,0.08138658628485305,"▶Collect Nk Markovian samples following eq. (2).
▶Send eR(m)
i
and z(m)
i,ℓ
and perform T ′ and Tz local average steps, respectively."
DECENTRALIZED NATURAL AC,0.08214016578749057,"▶Estimate local gradient b∇ω(m)fωt(ht,k) following eqs. (8) and (4).
▶Perform SGD update in eq. (9).
end
▶Actor update on ωt: ω(m)
t+1 = ω(m)
t
+ αh(m)
t
.
end
end"
DECENTRALIZED NATURAL AC,0.0828937452901281,"Output: ω e
T with eT
uniform
∼
{1, 2, . . . , T}."
DECENTRALIZED NATURAL AC,0.08364732479276564,"A major challenge of developing fully decentralized NAC algorithm is computing the inverse Fisher
information matrix-vector product involved in the natural policy gradient update. To explain, ﬁrst
recall the exact natural policy gradient update of the centralized NAC algorithm, i.e., ωt+1 =
ωt + αF(ωt)−1∇J(ωt), where F(ωt) := Est∼νωt,at∼πt(·|st)

ψt(at|st)ψt(at|st)⊤
is the Fisher
information matrix. However, in the multi-agent case, it is challenging to perform the natural policy
gradient update in a decentralized manner. This is because the Fisher information matrix F(ωt) is
based on the concatenated multi-agent score vector ψt(at|st) = [ψ(1)
t
(a(1)
t |st); ...; ψ(M)
t
(a(M)
t
|st)]
and the inverse matrix-vector product F(ωt)−1∇J(ωt) is not separable with regard to each agent’s
policy parameter dimensions. Next, we develop a fully decentralized scheme to implement the
natural policy gradient update in the multi-agent setting."
DECENTRALIZED NATURAL AC,0.08440090429540316,"First, note that the natural policy gradient update h(ωt) := F(ωt)−1∇J(ωt) is equivalent to the
solution of a quadratic program, i.e.,"
DECENTRALIZED NATURAL AC,0.08515448379804069,"h(ωt) = arg min
h fωt(h) := 1"
DECENTRALIZED NATURAL AC,0.08590806330067823,"2h⊤F(ωt)h −∇J(ωt)⊤h.
(7)"
DECENTRALIZED NATURAL AC,0.08666164280331574,"Therefore, we can apply K steps of SGD with Markovian sampling to solve this problem and obtain
an estimated natural policy gradient update. Speciﬁcally, starting from the initialization ht,0 = ht−1
(obtained in the previous iteration), in the k-th SGD step, we sample a mini-batch Bt,k 3 of Nk
Markovian samples to estimate ∇fωt(h) as
1
Nk
P"
DECENTRALIZED NATURAL AC,0.08741522230595328,"i∈Bt,k ψt(ai|si)ψt(ai|si)⊤ht,k −b∇J(ωt; Bt,k),"
DECENTRALIZED NATURAL AC,0.08816880180859081,"where b∇J(ωt; Bt,k) is estimated in the same decentralized way as eq. (4) using the mini-batch
of samples Bt,k. In particular, each agent m needs to compute the corresponding local gradi-
ent
1
Nk
P"
DECENTRALIZED NATURAL AC,0.08892238131122833,"i∈Bt,k ψ(m)
t
(a(m)
i
|si)

ψt(ai|si)⊤ht,k

−b∇ω(m)J(ωt; Bt,k), in which ψ(m)
t
(a(m)
i
|si) and
b∇ω(m)J(ωt; Bt,k) can be computed/estimated by the agent m. Then, it sufﬁces to obtain an estimate
of the scalar ψt(ai|si)⊤ht,k, which can be rewritten as PM
m=1 ψ(m)
t
(a(m)
i
|si)⊤h(m)
t,k . This summa-
tion can be easily estimated by the decentralized agents through local averaging. Speciﬁcally, each
agent m locally computes z(m)
i,0
= ψ(m)
t
(a(m)
i
|si)⊤h(m)
t,k and performs Tz steps of local averaging,"
DECENTRALIZED NATURAL AC,0.08967596081386586,"i.e., z(m)
i,ℓ+1 = P"
DECENTRALIZED NATURAL AC,0.0904295403165034,"m′∈Nm Wm,m′ z(m′)
i,ℓ
,
ℓ= 0, 1, . . . , Tz −1. After that, the quantity Mz(m)
i,Tz can be"
DECENTRALIZED NATURAL AC,0.09118311981914091,"proven to converge to the desired summation PM
m=1 ψ(m)
t
(a(m)
i
|si)⊤h(m)
t,k exponentially fast. Finally,
the local gradient for agent m is approximated as"
DECENTRALIZED NATURAL AC,0.09193669932177845,"b∇ω(m)fωt(ht,k) = M Nk X"
DECENTRALIZED NATURAL AC,0.09269027882441598,"i∈Bt,k
ψ(m)
t
(a(m)
i
|si)z(m)
i,Tz −b∇ω(m)J(ωt; Bt,k).
(8)"
DECENTRALIZED NATURAL AC,0.0934438583270535,"Then, the agent m performs the following SGD updates to obtain h(m)
t
:= h(m)
t,K ."
DECENTRALIZED NATURAL AC,0.09419743782969103,"h(m)
t,k+1 = h(m)
t,k −η b∇ω(m)fωt(ht,k),
k = 0, ..., K −1.
(9)"
DECENTRALIZED NATURAL AC,0.09495101733232857,"3Speciﬁcally, the mini-batch Bt,k contains sample indices

tN + Pk−1
k′=0 Nk′, . . . , tN + Pk
k′=0 Nk′ −1
	
."
DECENTRALIZED NATURAL AC,0.09570459683496609,Under review as a conference paper at ICLR 2022
DECENTRALIZED NATURAL AC,0.09645817633760362,"We emphasize that the above mini-batch SGD updates use Markovian samples. In particular, as
shown in Section 6, we need to develop an adaptive batch size scheduling scheme for this SGD in
order to reduce its sample complexity. We summarize the decentralized NAC in Algorithm 3."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.09721175584024115,"6
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.09796533534287867,"To analyze the decentralized NAC, we introduce the following additional standard assumptions.
Assumption 6. There exists a constant λF > 0 such that λmin
 
F(ω)

≥λF > 0, ∀ω ∈Ω."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.0987189148455162,"Assumption 7. There exists C∗> 0 such that for ω∗= arg maxω∈ΩJ(ω) and any ω ∈Ω,"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.09947249434815374,"Es∼νω,a∼πω(·|s)
hνω∗(s)πω∗(a|s)"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.10022607385079126,νω(s)πω(a|s)
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.10097965335342879,"2i
≤C2
∗."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.10173323285606632,"Assumption 6 ensures that the Fisher information matrix F(ω) is uniformly positive deﬁnite, and
is also considered in (65; 29; 62). Assumption 7 regularizes the discrepancy between the stationary
state-action distributions νω∗(s)πω∗(a|s) and νω(s)πω(a|s) (54; 59)."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.10248681235870384,"We obtain the following ﬁnite-time convergence result of the decentralized NAC algorithm.
Throughout, we follow (54; 60; 62) and deﬁne the actor approximation error as ζactor
approx :="
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.10324039186134137,"supω minhEs∼νω,a∼πω(·|s)
 
ψω(a|s)⊤h −Aω(s, a)
2
. All universal constants are listed in Ap-
pendix F.
Theorem 2. Let Assumptions 1–7 hold and adopt the hyperparameters of the decentralized TD in"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.1039939713639789,"Algorithm 2 following Lemma D.4. Choose hyperparameters α ≤min
 
1,
λ2
F
4LJC2
ψ ,
C2
ψ
2LJ

, β ≤1,"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.10474755086661643,"T ′ ≥
ln M
2 ln σ−1
W , η ≤
1
2C2
ψ , Tz ≥
ln(3DJC2
ψ)"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.10550113036925396,"ln σ−1
W
, K ≥
ln 3
ln(1−ηλF /2)−1 , N ≥
2304C4
ψ(κ+1−ρ)
ηλ5
F (1−ρ)(1−ηλF /2)(K−1)/2"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.10625470987189148,"and Nk ∝(1 −ηλF /2)−k/2. Then, the output of Algorithm 3 satisﬁes"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.10700828937452901,"J(ω∗) −E

J(ω e
T )

≤c17"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.10776186887716654,"Tα + c18

1 −ηλF 2"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.10851544837980406,"(K−1)/4
+ c19σTz
W + c20σT ′
W + c21βσT ′
c
W + c23
√Nc"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.1092690278824416,"+ c22

1 −λB"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.11002260738507913,"8 β
Tc/2
+ Cψ
q"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.11077618688771665,"c16ζcritic
approx + c24ζcritic
approx + C∗q"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.11152976639035418,"ζactor
approx."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.11228334589299171,"Moreover, to achieve J(ω∗) −E

J(ω b
T )

≤ϵ for any ϵ ≥2Cψ
q"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.11303692539562923,"c16ζcritic
approx + 2c24ζcritic
approx +"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.11379050489826677,"2C∗pζactor
approx, we can choose T = O(ϵ−1), N, Nc = O(ϵ−2), Tc, T ′
c, T ′, Tz, K = O(ln ϵ−1). Con-
sequently, the overall sample complexity is T(TcNc + N) = O(ϵ−3 ln ϵ−1), and the communication
complexities for synchronizing linear model parameters and rewards are T(Tc+T ′
c) = O(ϵ−1 ln ϵ−1)
and TT ′ = O(ϵ−1 ln ϵ−1), respectively."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.1145440844009043,"Theorem 2 provides the ﬁrst ﬁnite-time analysis of fully decentralized natural AC algorithm. Our
result proves that the function value optimality gap converges to the order O
 
N −1/2
c
+
q"
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.11529766390354182,"ζcritic
approx +
pζactor
approx

, which can be made arbitrarily close to the actor and critic approximation error by choosing
a sufﬁciently large batch size Nc. In particular, exact global optimum can be achieved when there
is no model approximation error. We note that the overall sample complexity of our decentralized
NAC is O(ϵ−3 ln ϵ−1), matching the state-of-the-art complexity result for centralized NAC (60).
Moreover, with the mini-batch updates, the overall communication complexity is signiﬁcantly reduced
to O(ϵ−1 ln ϵ−1)."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.11605124340617935,"Similar to that of Theorem 1, our analysis of Theorem 2 also leverages the mini-batch decentralized
TD updates to reduce the communication complexity and deal with the bias and variance of the
local policy gradient introduced by noisy rewards. In addition, decentralized NAC uses mini-batch
SGD with Markovian sampling to solve the quadratic problem in eq. (7). Here, we use a special
geometrically increasing batch size scheduling scheme, i.e., Nk ∝(1 −ηλF /2)−k/2, to achieve
the best possible convergence rate under the total sample budget that PK
k=1 Nk = N and obtain
the desired overall sample complexity result. Such an analysis of SGD with Markovian sampling
under adaptive batch size scheduling has not been studied in the literature and can be of independent
interests."
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.11680482290881689,Under review as a conference paper at ICLR 2022
FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC,0.1175584024114544,"Figure 1: Comparison of accumulated discounted reward J(ωt) among decentralized AC-type
algorithms in a ring network with sparse connections ."
EXPERIMENTS,0.11831198191409194,"7
EXPERIMENTS"
EXPERIMENTS,0.11906556141672947,"We simulate a fully decentralized ring network with 6 agents. Please refer to Appendix E for
detailed environment setup. We implement four decentralized AC-type algorithms and compare their
performance, namely, our Algorithms 1 and 3, an existing decentralized AC algorithm (Algorithm 2 of
(70)) that uses a linear model to parameterize the agents’ averaged reward (we name it DAC-RP1 for
decentralized AC with reward parameterization), and our proposed modiﬁed version of DAC-RP1 to
incorporate minibatch, which we refer to as DAC-RP100 with batch size N = 100. For our Algorithm
1, we choose T = 500, Tc = 50, T ′
c = 10, Nc = 10, T ′ = Tz = 5, β = 0.5, {σm}6
m=1 = 0.1, and
consider batch size choices N = 100, 500, 2000. Algorithm 3 uses the same hyperparameters as
those of Algorithm 1 except that T = 2000 in Algorithm 3. For DAC-RP1, we set learning rates
βθ = 2(t + 1)−0.9, βv = 5(t + 1)−0.8 and batch size N = 1 as mentioned in (70). The modiﬁed
DAC-RP100 adopts the same learning rates as Algorithm 1 with N = 100."
EXPERIMENTS,0.11981914091936699,"Figure 1 plots the accumulated reward J(ωt) v.s. communication and sample complexity. Each
curve includes 10 repeated experiments, and its upper and lower envelopes denote the 95% and 5%
percentiles of the 10 repetitions, respectively. For our decentralized AC algorithm (left two ﬁgures),
its communication and sample complexities for achieving a high accumulated reward are signiﬁcantly
reduced under a larger batch size N. This matches our theoretical understanding in Theorem 1 that
a large N helps reduce the communication frequency and policy gradient variance. In comparison,
DAC-RP1 (with N = 1) has almost no improvement on the accumulated reward. Moreover, although
the modiﬁed DAC-RP100 (with N = 100) outperforms DAC-RP1, its performance is much worse
than our Algorithm 1 with N = 100. This performance gap is due to two reasons: (i) Both DAC-RP
algorithms suffer from an inaccurate parameterized estimation of the averaged reward, and their mean
relative reward errors are over 100%. In contrast, our noisy averaged reward estimation achieves a
mean relative error in the range of 10−5 ∼10−4;(ii) Both DAC-RP algorithms apply only a single
TD update per-round, and hence suffers from a large mean relative TD error (about 2% and 1%
for DAC-RP1 and DAC-RP100, respectively)whereas our algorithms perform multiple TD learning
updates per-round and achieve a smaller mean relative TD error (about 0.3%). For our decentralized
NAC algorithm (right two ﬁgures), one can make similar observations and conclusions."
CONCLUSION,0.12057272042200452,"8
CONCLUSION"
CONCLUSION,0.12132629992464206,"We developed fully-decentralized AC and NAC algorithms that are efﬁcient and do not reveal agents’
local actions and policies . The agents share noisy reward information and adopt mini-batch updates
to improve sample and communication efﬁciency. Under Markovian sampling and linear function
approximation, we proved that our decentralized AC and NAC algorithms achieve the state-of-the-art
sample complexities O(ϵ−2 ln ϵ−1) and O(ϵ−3 ln ϵ−1), respectively, and they both achieve a small
communication complexity O(ϵ−1 ln ϵ−1). Numerical experiments demonstrate that our algorithms
achieve better sample and communication complexity than the existing decentralized AC algorithm
that adopts reward parameterization."
CONCLUSION,0.12207987942727958,Under review as a conference paper at ICLR 2022
REFERENCES,0.12283345892991711,REFERENCES
REFERENCES,0.12358703843255464,"[1] A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods:
Optimality, approximation, and distribution shift. ArXiv:1908.00261, 2019."
REFERENCES,0.12434061793519216,"[2] C. Alfano and P. Rebeschini. Dimension-free rates for natural policy gradient in multi-agent
reinforcement learning. ArXiv:2109.11692, 2021."
REFERENCES,0.12509419743782968,"[3] Q. Bai, M. Agarwal, and V. Aggarwal. Joint optimization of multi-objective reinforcement
learning with policy gradient based algorithm. ArXiv:2105.14125, 2021."
REFERENCES,0.12584777694046723,"[4] J. Bhandari, D. Russo, and R. Singal. A ﬁnite time analysis of temporal difference learning with
linear function approximation. In Proc. Conference on Learning Theory (COLT), volume 75,
pages 1691–1692, 2018."
REFERENCES,0.12660135644310475,"[5] S. Bhatnagar. An actor–critic algorithm with function approximation for discounted cost
constrained markov decision processes. Systems & Control Letters, 59(12):760–766, 2010."
REFERENCES,0.12735493594574226,"[6] S. Bhatnagar, M. Ghavamzadeh, M. Lee, and R. S. Sutton. Incremental natural actor-critic
algorithms. In Proc. Advances in Neural Information Processing Systems (NeurIPS), volume 20,
pages 105–112, 2007."
REFERENCES,0.1281085154483798,"[7] S. Bhatnagar, R. S. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor–critic algorithms.
Automatica, 45(11):2471–2482, 2009."
REFERENCES,0.12886209495101733,"[8] G. Bono, J. S. Dibangoye, L. Matignon, F. Pereyron, and O. Simonin. Cooperative multi-agent
policy gradient. In Proc. Joint European Conference on Machine Learning and Knowledge
Discovery in Databases (ECML PKDD), pages 459–476, 2018."
REFERENCES,0.12961567445365485,"[9] L. Cassano, K. Yuan, and A. H. Sayed. Multi-agent fully decentralized value function learning
with linear convergence rates. IEEE Transactions on Automatic Control, 2020."
REFERENCES,0.1303692539562924,"[10] B. Chalaki and A. A. Malikopoulos. A hysteretic q-learning coordination framework for
emerging mobility systems in smart cities. ArXiv:2011.03137, 2020."
REFERENCES,0.13112283345892992,"[11] B. Chen, M. Xu, Z. Liu, L. Li, and D. Zhao. Delay-aware multi-agent reinforcement learning.
ArXiv:2005.05441, 2020."
REFERENCES,0.13187641296156744,"[12] Z. Chen, Y. Zhou, and R. Chen. Multi-agent off-policy td learning: Finite-time analysis with
near-optimal sample complexity and communication complexity. ArXiv:2103.13147, 2021."
REFERENCES,0.13262999246420498,"[13] C. Daskalakis, D. J. Foster, and N. Golowich.
Independent policy gradient methods for
competitive reinforcement learning. ArXiv:2101.04233, 2021."
REFERENCES,0.1333835719668425,"[14] T. Doan, S. Maguluri, and J. Romberg. Finite-time analysis of distributed TD(0) with linear func-
tion approximation on multi-agent reinforcement learning. In Proc. International Conference
on Machine Learning (ICML), volume 97, pages 1626–1635, 09–15 Jun 2019."
REFERENCES,0.13413715146948002,"[15] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent
policy gradients. In Proc. Association for the Advancement of Artiﬁcial Intelligence (AAAI),
volume 32, 2018."
REFERENCES,0.13489073097211757,"[16] A. Ghosh and V. Aggarwal. Model free reinforcement learning algorithm for stationary mean
ﬁeld equilibrium for multiple types of agents. ArXiv:2012.15377, 2020."
REFERENCES,0.1356443104747551,"[17] D. Hennes, D. Morrill, S. Omidshaﬁei, R. Munos, J. Perolat, M. Lanctot, A. Gruslys, J.-B.
Lespiau, P. Parmas, E. Duéñez-Guzmán, et al. Neural replicator dynamics: Multiagent learning
via hedging policy gradients. In Proc. International Conference on Autonomous Agents and
MultiAgent Systems (AAMAS), pages 492–501, 2020."
REFERENCES,0.1363978899773926,"[18] P. Heredia, H. Ghadialy, and S. Mou. Finite-sample analysis of distributed q-learning for
multi-agent networks. In 2020 American Control Conference (ACC), pages 3511–3516, 2020."
REFERENCES,0.13715146948003015,"[19] P. C. Heredia and S. Mou. Distributed multi-agent reinforcement learning by actor-critic method.
IFAC-PapersOnLine, 52(20):363–368, 2019."
REFERENCES,0.13790504898266767,Under review as a conference paper at ICLR 2022
REFERENCES,0.1386586284853052,"[20] S. M. Kakade. A natural policy gradient. In Proc. Advances in Neural Information Processing
Systems (NeurIPS), volume 14, 2001."
REFERENCES,0.13941220798794274,"[21] S. Kar, J. M. F. Moura, and H. V. Poor. Qd-learning: A collaborative distributed strategy for
multi-agent reinforcement learning through consensus + innovations. IEEE Transactions on
Signal Processing, 61(7):1848–1862, 2013."
REFERENCES,0.14016578749058026,"[22] V. Konda. Actor-critic algorithms (ph.d. thesis). Department of Electrical Engineering and
Computer Science, Massachusetts Institute of Technology, 2002."
REFERENCES,0.14091936699321778,"[23] V. R. Konda and J. N. Tsitsiklis.
Actor-critic algorithms.
In Proc. Advances in Neural
Information Processing Systems (NeurIPS), pages 1008–1014, 2000."
REFERENCES,0.14167294649585532,"[24] V. Krishnamurthy, M. Maskery, and G. Yin. Decentralized adaptive ﬁltering algorithms for
sensor activation in an unattended ground sensor network. IEEE Transactions on Signal
Processing, 56(12):6086–6101, 2008."
REFERENCES,0.14242652599849284,"[25] H. Kumar, A. Koppel, and A. Ribeiro. On the sample complexity of actor-critic method for
reinforcement learning with function approximation. ArXiv:1910.08412, 2019."
REFERENCES,0.14318010550113036,"[26] Y. Lin, Y. Luo, K. Zhang, Z. Yang, Z. Wang, T. Basar, R. Sandhu, and J. Liu. An asyn-
chronous multi-agent actor-critic algorithm for distributed reinforcement learning. In NeurIPS
Optimization Foundations for Reinforcement Learning Workshop, 2019."
REFERENCES,0.1439336850037679,"[27] Y. Lin, K. Zhang, Z. Yang, Z. Wang, T. Ba¸sar, R. Sandhu, and J. Liu. A communication-efﬁcient
multi-agent actor-critic algorithm for distributed reinforcement learning. In 2019 IEEE 58th
Conference on Decision and Control (CDC), pages 5562–5567, 2019."
REFERENCES,0.14468726450640543,"[28] R. Liu and A. Olshevsky. Distributed td (0) with almost no communication. ArXiv:2104.07855,
2021."
REFERENCES,0.14544084400904295,"[29] Y. Liu, K. Zhang, T. Basar, and W. Yin. An improved analysis of (variance-reduced) policy gra-
dient and natural policy gradient methods. In Proc. Advances in Neural Information Processing
Systems (NeurIPS), volume 33, pages 7624–7636, 2020."
REFERENCES,0.1461944235116805,"[30] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch. Multi-agent actor-critic for
mixed cooperative-competitive environments. ArXiv:1706.02275, 2017."
REFERENCES,0.146948003014318,"[31] Y. Luo, Z. Yang, Z. Wang, and M. Kolar. Natural actor-critic converges globally for hierarchical
linear quadratic regulator. ArXiv:1912.06875, 2019."
REFERENCES,0.14770158251695553,"[32] X. Lyu, Y. Xiao, B. Daley, and C. Amato. Contrasting centralized and decentralized critics in
multi-agent reinforcement learning. ArXiv:2102.04402, 2021."
REFERENCES,0.14845516201959308,"[33] X. Ma, Y. Yang, C. Li, Y. Lu, Q. Zhao, and Y. Jun. Modeling the interaction between agents in
cooperative multi-agent reinforcement learning. ArXiv:2102.06042, 2021."
REFERENCES,0.1492087415222306,"[34] S. V. Macua, J. Chen, S. Zazo, and A. H. Sayed. Distributed policy evaluation under multiple
behavior strategies. IEEE Transactions on Automatic Control, 60(5):1260–1274, 2014."
REFERENCES,0.14996232102486812,"[35] D. J. Ornia and M. Mazo Jr. Event-based communication in multi-agent distributed q-learning.
ArXiv:2109.01417, 2021."
REFERENCES,0.15071590052750566,"[36] J. Perolat, B. Piot, and O. Pietquin. Actor-critic ﬁctitious play in simultaneous move multistage
games. In Proc. International Conference on Artiﬁcial Intelligence and Statistics, pages 919–
928, 2018."
REFERENCES,0.15146948003014318,"[37] J. Peters and S. Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180–1190, 2008."
REFERENCES,0.1522230595327807,"[38] S. Qiu, Z. Yang, J. Ye, and Z. Wang. On the ﬁnite-time convergence of actor-critic algorithm.
In NeurIPS Optimization Foundations for Reinforcement Learning Workshop, 2019."
REFERENCES,0.15297663903541825,"[39] W. Qiu, X. Wang, R. Yu, R. Wang, X. He, B. An, S. Obraztsova, and Z. Rabinovich. Rmix:
Learning risk-sensitive policies forcooperative reinforcement learning agents. In Proc. Advances
in Neural Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.15373021853805577,Under review as a conference paper at ICLR 2022
REFERENCES,0.1544837980406933,"[40] C. Qu, S. Mannor, H. Xu, Y. Qi, L. Song, and J. Xiong. Value propagation for decentralized
networked deep multi-agent reinforcement learning. In Proc. Advances in Neural Information
Processing Systems (NeurIPS)), pages 1184–1193, 2019."
REFERENCES,0.15523737754333083,"[41] R. Saha, S. Rini, M. Rao, and A. Goldsmith. Decentralized optimization over noisy, rate-
constrained networks: How to agree by talking about how we disagree. ArXiv:2010.11292,
2020."
REFERENCES,0.15599095704596835,"[42] M. Shaocong, Z. Yi, and Z. Shaofeng. Variance-reduced off-policy tdc learning: Non-asymptotic
convergence analysis. In Proc. Advances in Neural Information Processing Systems (NeurIPS),
2020."
REFERENCES,0.15674453654860587,"[43] N. Singh, D. Data, J. George, and S. Diggavi. Squarm-sgd: Communication-efﬁcient momentum
sgd for decentralized optimization. ArXiv:2005.07041, 2020."
REFERENCES,0.15749811605124342,"[44] S. Srinivasan, M. Lanctot, V. Zambaldi, J. Pérolat, K. Tuyls, R. Munos, and M. Bowling. Actor-
critic policy optimization in partially observable multiagent environments. ArXiv:1810.09026,
2018."
REFERENCES,0.15825169555388094,"[45] M. S. Stankovi´c and S. S. Stankovi´c. Multi-agent temporal-difference learning with linear
function approximation: Weak convergence under time-varying network topologies. In Proc.
American Control Conference (ACC), pages 167–172, 2016."
REFERENCES,0.15900527505651846,"[46] J. Sun, G. Wang, G. B. Giannakis, Q. Yang, and Z. Yang. Finite-sample analysis of decentral-
ized temporal-difference learning with linear function approximation. In Proc. International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 4485–4495, 2020."
REFERENCES,0.15975885455915598,"[47] W. Suttle, Z. Yang, K. Zhang, Z. Wang, T. Basar, and J. Liu. A multi-agent off-policy actor-critic
algorithm for distributed reinforcement learning. ArXiv:1903.06372, 2019."
REFERENCES,0.16051243406179352,[48] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction (Second Edition). 2018.
REFERENCES,0.16126601356443104,"[49] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for rein-
forcement learning with function approximation. In Proc. Advances in Neural Information
Processing Systems (NeurIPS), volume 12, 2000."
REFERENCES,0.16201959306706856,"[50] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour, et al. Policy gradient methods for
reinforcement learning with function approximation. In Proc. Advances in Neural Information
Processing Systems (NeurIPS), volume 99, pages 1057–1063, 1999."
REFERENCES,0.1627731725697061,"[51] F. Venturini, F. Mason, F. Pase, F. Chiariotti, A. Testolin, A. Zanella, and M. Zorzi. Distributed
reinforcement learning for ﬂexible and efﬁcient uav swarm control. ArXiv:2103.04666, 2021."
REFERENCES,0.16352675207234363,"[52] H.-T. Wai, Z. Yang, Z. Wang, and M. Hong. Multi-agent reinforcement learning via double
averaging primal-dual optimization. In Proc. Advances in Neural Information Processing
Systems (NeurIPS), pages 9672–9683, 2018."
REFERENCES,0.16428033157498115,"[53] G. Wang, S. Lu, G. Giannakis, G. Tesauro, and J. Sun. Decentralized td tracking with linear
function approximation and its ﬁnite-time analysis. In Proc. Advances in Neural Information
Processing Systems (NeurIPS), volume 33, 2020."
REFERENCES,0.1650339110776187,"[54] L. Wang, Q. Cai, Z. Yang, and Z. Wang. Neural policy gradient methods: Global optimality and
rates of convergence. ArXiv:1909.01150, 2019."
REFERENCES,0.1657874905802562,"[55] W. Wang, J. Hao, Y. Wang, and M. Taylor. Achieving cooperation through deep multiagent
reinforcement learning in sequential prisoner’s dilemmas. In Proc. of International Conference
on Distributed Artiﬁcial Intelligence (DAI), pages 1–7, 2019."
REFERENCES,0.16654107008289373,"[56] Y. F. Wu, W. ZHANG, P. Xu, and Q. Gu. A ﬁnite-time analysis of two time-scale actor-critic
methods. In Proc. Advances in Neural Information Processing Systems (NeurIPS), volume 33,
pages 17617–17628, 2020."
REFERENCES,0.16729464958553128,"[57] B. Xiao, B. Ramasubramanian, and R. Poovendran. Shaping advice in deep multi-agent
reinforcement learning. ArXiv:2103.15941, 2021."
REFERENCES,0.1680482290881688,Under review as a conference paper at ICLR 2022
REFERENCES,0.16880180859080632,"[58] T. Xu and Y. Liang. Sample complexity bounds for two timescale value-based reinforcement
learning algorithms. ArXiv:2011.05053, 2020."
REFERENCES,0.16955538809344387,"[59] T. Xu, Y. Liang, and G. Lan. A primal approach to constrained policy optimization: Global
optimality and ﬁnite-time analysis. ArXiv:2011.05869, 2020."
REFERENCES,0.17030896759608138,"[60] T. Xu, Z. Wang, and Y. Liang. Improving sample complexity bounds for (natural) actor-critic
algorithms. In Proc. Advances in Neural Information Processing Systems (NeurIPS), volume 33,
2020."
REFERENCES,0.1710625470987189,"[61] T. Xu, Z. Wang, and Y. Liang. Non-asymptotic convergence analysis of two time-scale (natural)
actor-critic algorithms. ArXiv:2005.03557, 2020."
REFERENCES,0.17181612660135645,"[62] T. Xu, Z. Yang, Z. Wang, and Y. Liang. Doubly robust off-policy actor-critic: Convergence and
optimality. ArXiv:2102.11866, 2021."
REFERENCES,0.17256970610399397,"[63] T. Xu, S. Zou, and Y. Liang. Two time-scale off-policy td learning: Non-asymptotic analysis over
markovian samples. In Proc. Advances in Neural Information Processing Systems (NeurIPS),
pages 10634–10644, 2019."
REFERENCES,0.1733232856066315,"[64] Z. Yan, N. Jouandeau, and A. A. Cherif. A survey and analysis of multi-robot coordination.
International Journal of Advanced Robotic Systems, 10(12):399, 2013."
REFERENCES,0.17407686510926904,"[65] L. Yang, Q. Zheng, and G. Pan. Sample complexity of policy gradient ﬁnding second-order
stationary points. ArXiv:2012.01491, 2020."
REFERENCES,0.17483044461190655,"[66] E. Yanmaz, M. Quaritsch, S. Yahyanejad, B. Rinner, H. Hellwagner, and C. Bettstetter. Com-
munication and coordination for drone networks. In Proc. International Conference on Ad Hoc
Networks, pages 79–91, 2017."
REFERENCES,0.17558402411454407,"[67] M. Yuan, Q. Cao, M.-o. Pun, and Y. Chen. Towards user scheduling for 6g: A fairness-oriented
scheduler using multi-agent reinforcement learning. ArXiv:2012.15081, 2020."
REFERENCES,0.17633760361718162,"[68] H. Zhang, H. Jiang, Y. Luo, and G. Xiao. Data-driven optimal consensus control for discrete-
time multi-agent systems with unknown dynamics using reinforcement learning method. IEEE
Transactions on Industrial Electronics, 64(5):4091–4100, 2016."
REFERENCES,0.17709118311981914,"[69] K. Zhang, Z. Yang, and T. Basar. Networked multi-agent reinforcement learning in continuous
spaces. In Proc. 2018 IEEE Conference on Decision and Control (CDC), pages 2771–2776.
IEEE, 2018."
REFERENCES,0.17784476262245666,"[70] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar. Fully decentralized multi-agent reinforcement
learning with networked agents. In Proc. International Conference on Machine Learning
(ICML), pages 5872–5881, 2018."
REFERENCES,0.1785983421250942,"[71] W. Zhang, H. Liu, F. Wang, T. Xu, H. Xin, D. Dou, and H. Xiong. Intelligent electric vehicle
charging recommendation based on multi-agent reinforcement learning. ArXiv:2102.07359,
2021."
REFERENCES,0.17935192162773173,"[72] Y. Zhang and M. M. Zavlanos. Distributed off-policy actor-critic reinforcement learning with
policy consensus. In Proc, Conference on Decision and Control (CDC), pages 4674–4679,
2019."
REFERENCES,0.18010550113036924,"[73] Y. Zhao, Y. Tian, J. D. Lee, and S. S. Du. Provably efﬁcient policy gradient methods for
two-player zero-sum markov games. ArXiv:2102.08903, 2021."
REFERENCES,0.1808590806330068,Under review as a conference paper at ICLR 2022
REFERENCES,0.1816126601356443,Appendix
REFERENCES,0.18236623963828183,Table of Contents
REFERENCES,0.18311981914091938,"A Notations
15"
REFERENCES,0.1838733986435569,"B
Proof of Theorem 1
16"
REFERENCES,0.18462697814619441,"C Proof of Theorem 2
17"
REFERENCES,0.18538055764883196,"D Supporting Lemmas
20"
REFERENCES,0.18613413715146948,"E
Experiment Setup and Additional Results
37
E.1
Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
E.2
Gradient Norm Convergence Results in Ring Network . . . . . . . . . . . . . .
38
E.3
Additional Experiments in Fully Connected Network . . . . . . . . . . . . . . .
39
E.4
Two-agent Cliff Navigation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40"
REFERENCES,0.186887716654107,"F
Constant scalars
41"
REFERENCES,0.18764129615674455,"A
NOTATIONS"
REFERENCES,0.18839487565938207,"Norms: For any vector x, we denote ∥x∥as its ℓ2 norm. For any matrix X, we denote ∥X∥, ∥X∥F
as its spectral norm and Frobenius norm, respectively."
REFERENCES,0.18914845516201959,"Difference matrix: ∆:= I −
1
M 11⊤, where 1 denotes a column vector that consists of 1s."
REFERENCES,0.18990203466465713,"Moments of random vectors: For a random vector X, we deﬁne its variance and covariance matrix
as Var(X) := E∥X −EX∥2 and Cov(X) := E
 
[X −EX][X −EX]⊤
, respectively. It is well
known that E∥X∥2 = Var(X) + ∥EX∥2 and that Var(X) = tr[Cov(X)]."
REFERENCES,0.19065561416729465,"Score function: At any time t, The joint score function ψt(at|st) := ∇ω ln πt(at|st) can be de-
composed into individual score functions ψ(m)
t
(a(m)
t
|st) := ∇ω(m) ln π(m)
t
(a(m)
t
|st) as ψt(at|st) =
[ψ(1)
t
(a(1)
t |st), . . . , ψ(M)
t
(a(M)
t
|st)]."
REFERENCES,0.19140919366993217,"Reward functions: At any time t, we denote R(m)
t
:= R(m)(st, at, st+1) and Rt := R(st, at, st+1),
where R(s, a, s′) =
1
M
PM
m=1 R(m)(s, a, s′)."
REFERENCES,0.19216277317256972,Policy gradient: The policy gradient theorem (50) shows that
REFERENCES,0.19291635267520724,"∇J(ω) = Eνω

Aω(s, a)ψω(s, a)

.
(10)"
REFERENCES,0.19366993217784476,"where Aω(s, a) := Qω(s, a) −Vω(s) denotes the advantage function. In the decentralized case,
we have the approximations Vω(st) ≈φ(st)⊤θ, Qω(st, at) ≈Rt + γφ(s′
t+1)⊤θ where s′
t+1 ∼
P(·|st, at). Therefore, we can stochastically approximate the partial policy gradient as eq. (1), i.e.,
for m = 1, ..., M,"
REFERENCES,0.1944235116804823,"∇ω(m)J(ωt)≈
h"
REFERENCES,0.19517709118311982,"Rt + γφ(s′
t+1)⊤θ(m)
t
−φ(st)⊤θ(m)
t
i
ψ(m)
t
(a(m)
t
|st)."
REFERENCES,0.19593067068575734,We also deﬁne the following mini-batch stochastic (partial) policy gradient.
REFERENCES,0.1966842501883949,e∇ω(m)J(ωt) := 1
REFERENCES,0.1974378296910324,"N
P(t+1)N−1
i=tN
h"
REFERENCES,0.19819140919366993,"Ri + γφ(s′
i+1)⊤θ(m)
t
−φ(si)⊤θ(m)
t
i
ψ(m)
t
(a(m)
i
|si)."
REFERENCES,0.19894498869630747,"e∇J(ωt) :=
e∇ω(1)J(ωt); . . . ; e∇ω(M)J(ωt)

."
REFERENCES,0.199698568198945,Filtrations: We deﬁne the following ﬁltrations for Algorithms 1 & 3.
REFERENCES,0.2004521477015825,Under review as a conference paper at ICLR 2022
REFERENCES,0.20120572720422006,"Ft := σ
 
{θ(m)
t′
}m∈M,0≤t′≤t ∪{si, ai, s′
i+1, {e(m)
i
}m∈M}tN−1
i=0
∪{stN}

."
REFERENCES,0.20195930670685758,"F′
t := σ

Ft ∪σ
 
{si, ai, s′
i+1}(t+1)N−1
i=tN+1

."
REFERENCES,0.2027128862094951,"Ft,k = σ

Ft ∪σ
 
{si, ai, si+1, s′
i+1, {e(m)
i
}m∈M}i∈∪k−1
k′=0Bt,k′

."
REFERENCES,0.20346646571213264,"B
PROOF OF THEOREM 1"
REFERENCES,0.20422004521477016,"Theorem 1. Let Assumptions 1–5 hold and adopt the hyperparameters of the decentralized TD
in Algorithm 2 following Lemma D.4. Choose α ≤
1
4LJ , T ′ ≥
ln M
2 ln σ−1
W . Then, the output of the
decentralized AC in Algorithm 1 has the following convergence rate."
REFERENCES,0.20497362471740768,"E
h∇J(ω e
T )
2i
≤4Rmax"
REFERENCES,0.2057272042200452,"Tα
+4(c4σ2T ′
W +c5β2σ2T ′
c
W )+4c6

1−λB"
REFERENCES,0.20648078372268275,"8 β
Tc
+ 4c7"
REFERENCES,0.20723436322532027,N + 4c8
REFERENCES,0.2079879427279578,"Nc
+64C2
ψζcritic
approx."
REFERENCES,0.20874152223059533,"Moreover, to achieve E
∇J(ω e
T )
2
≤ϵ for any ϵ ≥128C2
ψζcritic
approx, we can choose T, N, Nc =
O(ϵ−1) and Tc, T ′
c, T ′ = O(ln ϵ−1). Consequently, the overall sample complexity is T(TcNc+N) =
O(ϵ−2 ln ϵ−1), and the communication complexities for synchronizing linear model parameters and
rewards are T(Tc + T ′
c) = O(ϵ−1 ln ϵ−1) and TT ′ = O(ϵ−1 ln ϵ−1), respectively."
REFERENCES,0.20949510173323285,"Proof. Concatenating all the agents’ actor updates in Algorithm 1, we obtain the joint actor update
ωt+1 = ωt + αb∇J(ωt). Then, the item 7 of Lemma D.5 implies that"
REFERENCES,0.21024868123587037,J(ωt+1) ≥J(ωt) + ∇J(ωt)⊤(ωt+1 −ωt) −LJ 2
REFERENCES,0.21100226073850792,"ωt+1 −ωt
2"
REFERENCES,0.21175584024114544,= J(ωt) + α∇J(ωt)⊤b∇J(ωt) −LJα2 2
REFERENCES,0.21250941974378296,"b∇J(ωt)
2"
REFERENCES,0.2132629992464205,"(i)
≥J(ωt) + α∥∇J(ωt)∥2 + α∇J(ωt)⊤ b∇J(ωt) −∇J(ωt)
"
REFERENCES,0.21401657874905802,"−LJα2b∇J(ωt) −∇J(ωt)
2 −LJα2∇J(ωt)
2"
REFERENCES,0.21477015825169554,"(ii)
≥J(ωt) +
α"
REFERENCES,0.2155237377543331,"2 −LJα2
∥∇J(ωt)∥2 −
α"
REFERENCES,0.2162773172569706,"2 + LJα2b∇J(ωt) −∇J(ωt)
2"
REFERENCES,0.21703089675960813,"(iii)
≥J(ωt) + α"
REFERENCES,0.21778447626224567,"4 ∥∇J(ωt)∥2 −α
b∇J(ωt) −∇J(ωt)
2"
REFERENCES,0.2185380557648832,where (i) and (ii) use the inequalities ∥x∥2 ≤2∥x −y∥2 + 2∥y∥2 and x⊤y ≥−1
REFERENCES,0.2192916352675207,2∥x∥2 −1 2∥y∥2
REFERENCES,0.22004521477015826,"for any x, y ∈Rd, respectively, and (iii) uses the condition that α ≤
1
4LJ . Then, summing up the
inequality above over t = 0, 1, . . . , T −1 yields that"
REFERENCES,0.22079879427279578,J(ωT ) ≥J(ω0) + α 4
REFERENCES,0.2215523737754333,"T −1
X"
REFERENCES,0.22230595327807084,"t=0
∥∇J(ωt)∥2 −α"
REFERENCES,0.22305953278070836,"T −1
X t=0"
REFERENCES,0.22381311228334588,"b∇J(ωt) −∇J(ωt)
2."
REFERENCES,0.22456669178598343,Rearranging the equation above and taking expectation on both sides yields that
REFERENCES,0.22532027128862095,"E
∇J(ω e
T )
2 = 1 T"
REFERENCES,0.22607385079125847,"T −1
X"
REFERENCES,0.22682743029389602,"t=0
E∥∇J(ωt)∥2"
REFERENCES,0.22758100979653353,"≤
4
TαE[J(ωT ) −J(ω0)] + 4 T"
REFERENCES,0.22833458929917105,"T −1
X"
REFERENCES,0.2290881688018086,"t=0
E
hb∇J(ωt) −∇J(ωt)
2i"
REFERENCES,0.22984174830444612,"(i)
≤4Rmax"
REFERENCES,0.23059532780708364,"Tα
+ 4c4σ2T ′
W
+ 4c5β2σ2T ′
c
W
+ 4c6

1 −λB"
REFERENCES,0.23134890730972119,"4 β
Tc + 4c7"
REFERENCES,0.2321024868123587,N + 4c8
REFERENCES,0.23285606631499622,"Nc
+ 64C2
ψζcritic
approx,
(11)"
REFERENCES,0.23360964581763377,"where (i) uses the item 4 of Lemma D.5 and eq. (39) of Lemma D.6 (The condition of Lemma D.6
that T ′ ≥
ln M
2 ln(σ−1) holds). This proves the error bound of Theorem 1."
REFERENCES,0.2343632253202713,Under review as a conference paper at ICLR 2022
REFERENCES,0.2351168048229088,"Finally, for any ϵ ≥128C2
ψζcritic
approx, it can be easily veriﬁed that the following hyperparameter choices
make the error bound in (11) smaller than ϵ and also satisfy the conditions of this Theorem and those
in Lemma D.4 that β ≤min
  λB"
REFERENCES,0.23587038432554636,"8C2
B ,
4
λB , 1−σ"
CB,0.23662396382818388,"2CB

, Nc ≥
  2"
CB,0.2373775433308214,"λB + 2β
 192C2
B[1+(κ−1)ρ]
(1−ρ)λB
."
CB,0.23813112283345894,"α = min

1,
1
4LJ"
CB,0.23888470233609646,"
= O(1)"
CB,0.23963828183873398,"β = min
  λB"
CB,0.24039186134137153,"8C2
B
, 4"
CB,0.24114544084400905,"λB
, 1 −σ"
CB,0.24189902034664656,2CB
CB,0.2426525998492841,"
= O(1)"
CB,0.24340617935192163,"T =
l48Rmax αϵ"
CB,0.24415975885455915,"m
= O(ϵ−1)"
CB,0.2449133383571967,"T ′ =
l
1
2 ln(σ−1) max

ln(48c4ϵ−1), ln M
m
= O
 
ln(ϵ−1)
"
CB,0.24566691785983422,"T ′
c =
lln(48c5β2ϵ−1)"
CB,0.24642049736247174,2 ln(σ−1)
CB,0.24717407686510928,"m
= O
 
ln(ϵ−1)
"
CB,0.2479276563677468,"Tc =
l
ln(48c6ϵ−1)
2 ln[(1 −λBβ/4)−1]"
CB,0.24868123587038432,"m
= O
 
ln(ϵ−1)
"
CB,0.24943481537302187,"N =
l48c7 ϵ"
CB,0.25018839487565936,"m
= O(ϵ−1)"
CB,0.2509419743782969,"Nc =
l
max
h48c7"
CB,0.25169555388093445,"ϵ
,
  2"
CB,0.25244913338357194,"λB
+ 2β
192C2
B[1 + (κ −1)ρ]
(1 −ρ)λB"
CB,0.2532027128862095,"im
= O(ϵ−1)
(12)"
CB,0.25395629238884704,"C
PROOF OF THEOREM 2"
CB,0.25470987189148453,Theorem 2. Let Assumptions 1–7 hold and adopt the hyperparameters of the decentralized TD in
CB,0.2554634513941221,"Algorithm 2 following Lemma D.4. Choose hyperparameters α ≤min
 
1,
λ2
F
4LJC2
ψ ,
C2
ψ
2LJ

, β ≤1,"
CB,0.2562170308967596,"T ′ ≥
ln M
2 ln σ−1
W , η ≤
1
2C2
ψ , Tz ≥
ln(3DJC2
ψ)"
CB,0.2569706103993971,"ln σ−1
W
, K ≥
ln 3
ln(1−ηλF /2)−1 , N ≥
2304C4
ψ(κ+1−ρ)
ηλ5
F (1−ρ)(1−ηλF /2)(K−1)/2"
CB,0.25772418990203466,"and Nk ∝(1 −ηλF /2)−k/2. Then, the output of Algorithm 3 satisﬁes"
CB,0.2584777694046722,"J(ω∗) −E

J(ω e
T )

≤c17"
CB,0.2592313489073097,"Tα + c18

1 −ηλF 2"
CB,0.25998492840994725,"(K−1)/4
+ c19σTz
W + c20σT ′
W + c21βσT ′
c
W + c23
√Nc"
CB,0.2607385079125848,"+ c22

1 −λB"
CB,0.2614920874152223,"8 β
Tc/2
+ Cψ
q"
CB,0.26224566691785983,"c16ζcritic
approx + c24ζcritic
approx + C∗q"
CB,0.2629992464204974,"ζactor
approx."
CB,0.26375282592313487,"Moreover, to achieve J(ω∗) −E

J(ω b
T )

≤ϵ for any ϵ ≥2Cψ
q"
CB,0.2645064054257724,"c16ζcritic
approx + 2c24ζcritic
approx +"
CB,0.26525998492840996,"2C∗pζactor
approx, we can choose T = O(ϵ−1), N, Nc = O(ϵ−2), Tc, T ′
c, T ′, Tz, K = O(ln ϵ−1). Con-
sequently, the overall sample complexity is T(TcNc + N) = O(ϵ−3 ln ϵ−1), and the communication
complexities for synchronizing linear model parameters and rewards are T(Tc+T ′
c) = O(ϵ−1 ln ϵ−1)
and TT ′ = O(ϵ−1 ln ϵ−1), respectively."
CB,0.26601356443104746,"Proof. Concatenating all the agents’ actor updates in Algorithm 3, we obtain the joint actor update
ωt+1 = ωt + αht. Then, the item 7 of Lemma D.5 implies that"
CB,0.266767143933685,J(ωt+1) ≥J(ωt) + ∇J(ωt)⊤(ωt+1 −ωt) −LJ 2
CB,0.26752072343632255,"ωt+1 −ωt
2"
CB,0.26827430293896004,= J(ωt) + α∇J(ωt)⊤ht −LJα2 2
CB,0.2690278824415976,"ht
2"
CB,0.26978146194423513,"(i)
≥J(ωt) + α∇J(ωt)⊤F(ωt)−1∇J(ωt) + α∇J(ωt)⊤[ht −h(ωt)]"
CB,0.2705350414468726,"−LJα2ht −h(ωt)
2 −LJα2F(ωt)−1∇J(ωt)
2"
CB,0.2712886209495102,Under review as a conference paper at ICLR 2022
CB,0.2720422004521477,"(ii)
≥J(ωt) +
 α"
CB,0.2727957799547852,"C2
ψ
−
α
2C2
ψ
−LJα2 λ2
F"
CB,0.27354935945742276,"
∥∇J(ωt)∥2 −
αC2
ψ
2
+ LJα2ht −h(ωt)
2"
CB,0.2743029389600603,"(iii)
≥J(ωt) +
α
4C2
ψ
∥∇J(ωt)∥2 −αC2
ψ
ht −h(ωt)
2"
CB,0.2750565184626978,"where (i) uses the notation that h(ωt)
△= F(ωt)−1∇J(ωt) and the inequality that ∥x∥2 ≤2∥x −
y∥2 + 2∥y∥2 for any x, y ∈Rd, (ii) uses the item 3 of Lemma D.7 and the inequality that x⊤y ≥"
CB,0.27581009796533534,"−
1
2C2
ψ ∥x∥2 −
C2
ψ
2 ∥y∥2 for any x, y ∈Rd, and (iii) uses the condition that α ≤min

λ2
F
4LJC2
ψ ,
C2
ψ
2LJ"
CB,0.2765636774679729,"
.
Taking expectation on both sides of the above inequality, summing over t = 0, 1, . . . , T −1 and
rearranging, we obtain that"
T,0.2773172569706104,"1
T"
T,0.27807083647324793,"T −1
X"
T,0.2788244159758855,"t=0
E∥∇J(ωt)∥2 ≤
4C2
ψ
Tα E[J(ωT ) −J(ω0)] +
4C4
ψ
T"
T,0.27957799547852297,"T −1
X"
T,0.2803315749811605,"t=0
E
ht −h(ωt)
2"
T,0.28108515448379806,"(i)
≤
4C2
ψRmax"
T,0.28183873398643555,"Tα
+ 4C4
ψ
h
c10

1 −ηλF 2"
T,0.2825923134890731,"(K−1)/2
+ c11σ2Tz + c12σ2T ′"
T,0.28334589299171065,"+ c13β2σ2T ′
c + c14

1 −λB"
T,0.28409947249434814,"4 β
Tc
+ c15"
T,0.2848530519969857,"Nc
+ c16ζcritic
approx
i
,
(13)"
T,0.28560663149962323,where (i) uses the item 4 of Lemma D.5 and the item 8 of Lemma D.7.
T,0.2863602110022607,"By Assumption 2, ln πω(s, a) is an Lψ-smooth function of ω. Denote ω∗:= arg minω∈ΩJ(ω) and
denote Eω∗as the unconditional expectation over s ∼νω∗, a ∼πω∗(·|s). We obtain that"
T,0.28711379050489827,"Eω∗
ln πt+1(a|s) −ln πt(a|s)
"
T,0.2878673700075358,"≥Eω∗
h 
∇ωt ln πt(a|s)
⊤(ωt+1 −ωt)
i
−Lψ"
T,0.2886209495101733,2 E∥ωt+1 −ωt∥2
T,0.28937452901281085,"= αEω∗
ψt(a|s)⊤ht

−Lψα2"
E,0.2901281085154484,"2
E

∥ht∥2"
E,0.2908816880180859,"(i)
≥αEω∗
ψt(a|s)⊤ 
ht −h(ωt)

+ αEω∗
ψt(a|s)⊤h(ωt) −Aωt(s, a)

+ αEω∗
Aωt(s, a)
"
E,0.29163526752072344,"−Lψα2E
ht −h(ωt)
2
−Lψα2E
F(ωt)−1∇J(ωt)
2"
E,0.292388847023361,"(ii)
≥−αCψ
q"
E,0.2931424265259985,"E
ht −h(ωt)
2
−αC∗
q"
E,0.293896006028636,"ζactor
approx"
E,0.2946495855312736,"+ αE

J(ω∗) −J(ωt)

−Lψα2E
ht −h(ωt)
2
−Lψα2λ−2
F E
∇J(ωt)
2
,"
E,0.29540316503391106,"where (i) uses the inequality that ∥x∥2 ≤2∥x −y∥2 + 2∥y∥2 for any x, y ∈Rd and the notation"
E,0.2961567445365486,"that h(ωt)
△= F(ωt)−1∇J(ωt), (ii) uses Cauchy-Schwarz inequality, the items 3 & 6 of Lemma"
E,0.29691032403918616,"D.7, the inequality that E∥X∥≤
q"
E,0.29766390354182365,"E

∥X∥2
for any random vector X and the equality that"
E,0.2984174830444612,"Eω∗
Aωt(s, a)

= E

J(ω∗) −J(ωt)

(See its proof in Lemma 3.2 of (1).). Averaging the inequality
above over t = 0, 1, . . . , T −1 and rearranging it yields that"
E,0.29917106254709874,"J(ω∗) −E

J(ω e
T )

= 1 T"
E,0.29992464204973623,"T −1
X"
E,0.3006782215523738,"t=0
E

J(ωt)
"
E,0.30143180105501133,"≤
1
TαEω∗
ln πT (a|s) −ln π0(a|s)

+ C∗
q"
E,0.3021853805576488,"ζactor
approx + Cψ T"
E,0.30293896006028637,"T −1
X t=0 q"
E,0.3036925395629239,"E
ht −h(ωt)
2 + Lψα T"
E,0.3044461190655614,"T −1
X"
E,0.30519969856819895,"t=0
E
ht −h(ωt)
2
+ Lψα Tλ2
F"
E,0.3059532780708365,"T −1
X"
E,0.306706857573474,"t=0
E
∇J(ωt)
2"
E,0.30746043707611154,"(i)
≤
1
TαEs∼νω∗

KL
 
πω∗(·|s)||π0(·|s)

−KL
 
πω∗(·|s)||πT (·|s)

+ C∗q"
E,0.3082140165787491,"ζactor
approx"
E,0.3089675960813866,"+ Cψ
h
c10

1 −ηλF 2"
E,0.3097211755840241,"(K−1)/2
+ c11σ2Tz + c12σ2T ′ + c13β2σ2T ′
c"
E,0.31047475508666167,Under review as a conference paper at ICLR 2022
E,0.31122833458929916,"+ c14

1 −λB"
E,0.3119819140919367,"4 β
Tc
+ c15"
E,0.31273549359457425,"Nc
+ c16ζcritic
approx
i1/2"
E,0.31348907309721175,"+ Lψα
h
c10

1 −ηλF 2"
E,0.3142426525998493,"(K−1)/2
+ c11σ2Tz + c12σ2T ′ + c13β2σ2T ′
c"
E,0.31499623210248684,"+ c14

1 −λB"
E,0.31574981160512433,"4 β
Tc
+ c15"
E,0.3165033911077619,"Nc
+ c16ζcritic
approx
i + Lψα λ2
F"
E,0.3172569706103994,"n4C2
ψRmax"
E,0.3180105501130369,"Tα
+ 4C4
ψ
h
c10

1 −ηλF 2"
E,0.31876412961567446,"(K−1)/2
+ c11σ2Tz + c12σ2T ′ + c13β2σ2T ′
c"
E,0.31951770911831195,"+ c14

1 −λB"
E,0.3202712886209495,"4 β
Tc
+ c15"
E,0.32102486812358705,"Nc
+ c16ζcritic
approx
io"
E,0.32177844762622454,"(ii)
≤
1
TαEs∼νω∗

KL
 
πω∗(·|s)||π0(·|s)

+ C∗q"
E,0.3225320271288621,"ζactor
approx"
E,0.32328560663149963,"+ Cψ
h√c10

1 −ηλF 2"
E,0.3240391861341371,"(K−1)/4
+ √c11σTz + √c12σT ′ + √c13βσT ′
c"
E,0.32479276563677467,"+ √c14

1 −λB"
E,0.3255463451394122,"4 β
Tc/2
+
rc15"
E,0.3262999246420497,"Nc
+
q"
E,0.32705350414468726,"c16ζcritic
approx
i"
E,0.3278070836473248,"+ Lψ

1 +
4C4
ψ
λ2
F"
E,0.3285606631499623,"h
c10

1 −ηλF 2"
E,0.32931424265259984,"(K−1)/4
+ c11σTz + c12σT ′ + c13βσT ′
c"
E,0.3300678221552374,"+ c14

1 −λB"
E,0.3308214016578749,"4 β
Tc/2
+ c15
√Nc
+ c16ζcritic
approx
i
+
4LψC2
ψRmax
Tαλ2
F"
E,0.3315749811605124,"(iii)
= c17"
E,0.33232856066315,"Tα + c18

1 −ηλF 2"
E,0.33308214016578747,"(K−1)/4
+ c19σTz + c20σT ′ + c21βσT ′
c + c22

1 −λB"
E,0.333835719668425,"4 β
Tc/2"
E,0.33458929917106256,"+ c23
√Nc
+ Cψ
q"
E,0.33534287867370005,"c16ζcritic
approx + c24ζcritic
approx + C∗q"
E,0.3360964581763376,"ζactor
approx,
(14)"
E,0.33685003767897514,"where
(i)
uses
the
deﬁnition
of
KL
divergence
that
KL
 
πω∗(·|s)||πω(·|s)

=
Ea∼πω∗(·|s)

ln πω∗(a|s) −ln πω(a|s)
s

and eqs. (13) & (54), (ii) uses the condition that α ≤1 and
the inequality that
pPn
i=1 xi ≤Pn
i=1
√xi for any n ∈N+ and x1, . . . , xn ≥0, (iii) uses the nota-"
E,0.33760361718161264,"tions that c17:=Es∼νω∗

KL
 
πω∗(·|s)||π0(·|s)

+
4LψC2
ψRmax
λ2
F
, c18 := Cψ√c10 +c10Lψ

1+
4C4
ψ
λ2
F 
,"
E,0.3383571966842502,"c19 := Cψ√c11 + c11Lψ

1 +
4C4
ψ
λ2
F"
E,0.33911077618688773,"
, c20 := Cψ√c12 + c12Lψ

1 +
4C4
ψ
λ2
F"
E,0.3398643556895252,"
, c21 := Cψ√c13 +"
E,0.34061793519216277,"c13Lψ

1 +
4C4
ψ
λ2
F"
E,0.3413715146948003,"
, c22 := Cψ√c14 + c14Lψ

1 +
4C4
ψ
λ2
F"
E,0.3421250941974378,"
, c23 := Cψ√c15 + c15Lψ

1 +
4C4
ψ
λ2
F 
,"
E,0.34287867370007535,"c24 := c16Lψ

1 +
4C4
ψ
λ2
F"
E,0.3436322532027129,"
. This proves the error bound of Theorem 2."
E,0.3443858327053504,"Finally, for any ϵ ≥2Cψ
q"
E,0.34513941220798794,"c16ζcritic
approx + 2c24ζcritic
approx + 2C∗pζactor
approx, it can be veriﬁed that the
following hyperparameter choices make the error bound in (14) smaller than ϵ and satisfy all
the conditions of this Theorem and those in Lemma D.4 that β ≤min
  λB"
E,0.3458929917106255,"8C2
B ,
4
λB , 1−σ"
CB,0.346646571213263,"2CB

, Nc ≥
  2"
CB,0.3474001507159005,"λB + 2β
 192C2
B[1+(κ−1)ρ]
(1−ρ)λB
."
CB,0.34815373021853807,"α = min

1,
λ2
F
4LJC2
ψ
,
C2
ψ
2LJ"
CB,0.34890730972117556,"
= O(1)"
CB,0.3496608892238131,"β = min
 
1, λB"
CB,0.35041446872645066,"8C2
B
, 4"
CB,0.35116804822908815,"λB
, 1 −σ"
CB,0.3519216277317257,2CB
CB,0.35267520723436324,"
= O(1) η = 1"
CB,0.35342878673700073,"2C2
ψ
= O(1)"
CB,0.3541823662396383,"T =
l14c17 αϵ"
CB,0.3549359457422758,"m
= O(ϵ−1)"
CB,0.3556895252449133,Under review as a conference paper at ICLR 2022
CB,0.35644310474755087,"K =
l
max
h
ln 3
ln[(1 −ηλF /2)−1],
4 ln(14c18ϵ−1)
ln

(1 −ηλF /2)−1 + 1
im
= O

ln(ϵ−1)
"
CB,0.3571966842501884,"Tz =
l
max
hln(3DJC2
ψ)
ln(σ−1)
, ln(14c19ϵ−1)"
CB,0.3579502637528259,ln(σ−1)
CB,0.35870384325546345,"i
=
m
O

ln(ϵ−1)
"
CB,0.359457422758101,"T ′ =
l
max
h
ln M
2 ln(σ−1), ln(14c20ϵ−1)"
CB,0.3602110022607385,ln(σ−1)
CB,0.36096458176337604,"im
= O

ln(ϵ−1)
"
CB,0.3617181612660136,"T ′
c =
lln(14c21ϵ−1)"
CB,0.3624717407686511,ln(σ−1)
CB,0.3632253202712886,"m
= O

ln(ϵ−1)
"
CB,0.36397889977392617,"Tc =
l
2 ln(14c22ϵ−1)
ln[(1 −λBβ/4)−1]"
CB,0.36473247927656366,"m
= O

ln(ϵ−1)
"
CB,0.3654860587792012,"N =
l
2304C4
ψ(κ + 1 −ρ)"
CB,0.36623963828183875,"ηλ5
F (1 −ρ)(1 −ηλF /2)(K−1)/2"
CB,0.36699321778447624,"m
= O(ϵ−2)"
CB,0.3677467972871138,"Nc =
l
max
h 2"
CB,0.36850037678975134,"λB
+ 2β
192C2
B[1 + (κ −1)ρ]
(1 −ρ)λB
, 196c2
23ϵ−2im
= O(ϵ−2)
(15)"
CB,0.36925395629238883,"D
SUPPORTING LEMMAS"
CB,0.3700075357950264,"First, we extend the Lemma F.3 of (12) to the Lemma D.1 below. The item 1 of Lemma D.1
generalizes the case n = 1 to any n ∈N+, the items 2 & 3 remain unchanged, and the item 4 is
added for convenience of our convergence analysis.
Lemma D.1. The doubly stochastic matrix W and the difference matrix ∆= I −
1
M 11⊤have the
following properties:"
CB,0.3707611152976639,"1. ∆W n = W n∆= W n −
1
M 11⊤for any n ∈N+."
CB,0.3715146948003014,2. The spectral norm of W satisﬁes ∥W∥= 1.
CB,0.37226827430293896,"3. For any x ∈RM and n ∈N+, ∥W n∆x∥≤σn
W ∥∆x∥(σW is the second largest singular value
of W). Hence, for any H ∈RM×M, ∥W n∆H∥F ≤σn
W ∥∆H∥F . 4."
CB,0.3730218538055765,"W n −
1
M 11⊤ ≤σn
W ,
W n −
1
M 11⊤
F ≤σn
W
√"
CB,0.373775433308214,M for any n ∈N+.
CB,0.37452901281085155,Proof. The proof of items 2 & 3 can be found in (12). We prove the item 1 and item 4.
CB,0.3752825923134891,"We prove item 1 by induction. The case n = 1 of the item 1 can be proved by the following two
equalities, as shown in (12)."
CB,0.3760361718161266,"∆W =

I −1"
CB,0.37678975131876413,"M 11⊤
W = W −1"
CB,0.3775433308214017,M 11⊤W = W −1 M 11⊤
CB,0.37829691032403917,"W∆= W

I −1"
CB,0.3790504898266767,"M 11⊤
= W −1"
CB,0.37980406932931426,M W11⊤= W −1 M 11⊤
CB,0.38055764883195176,"Suppose the case of n = k holds for a certain k ∈N+, then the following two equalities proves the
case of n = k + 1 and thus proves the item 1."
CB,0.3813112283345893,"∆W k+1 = (∆W k)W =

W k −1"
CB,0.38206480783722685,"M 11⊤
W = W k+1 −1 M 11⊤"
CB,0.38281838733986434,"W k+1∆= W(W k∆) = W

W k −1"
CB,0.3835719668425019,"M 11⊤
= W k+1 −1 M 11⊤"
CB,0.38432554634513943,"The item 4 can be proved by the following two inequalities.
W n −1"
CB,0.3850791258477769,"M 11⊤
(i)
=
W n∆
 =
sup
x:∥x∥≤1
∥W n∆x∥
(ii)
≤
sup
x:∥x∥≤1
σn
W ∥∆∥∥x∥
(iv)
= σn
W ,
(16)"
CB,0.3858327053504145,Under review as a conference paper at ICLR 2022
CB,0.386586284853052,W n −1
CB,0.3873398643556895,"M 11⊤
F"
CB,0.38809344385832706,"(i)
=
W n∆

F"
CB,0.3888470233609646,"(iii)
≤σn
W ∥∆∥F"
CB,0.3896006028636021,"(iv)
= σn
W r"
CB,0.39035418236623964,"M

1 −1 M"
CB,0.3911077618688772,"2
+ M(M −1)

−1 M"
CB,0.3918613413715147,"2
≤σn
W
√"
CB,0.39261492087415223,"M,
(17)"
CB,0.3933685003767898,"where (i) uses the item 1, (ii) and (iii) use the item 3 (H = I in (iii)), and (iv) uses the fact that
∆has M diagnoal entries 1 −
1
M and M(M −1) off-diagnoal entries −1"
CB,0.39412207987942727,"M , which implies that
∥∆∥= 1."
CB,0.3948756593820648,"Next, we extend the Lemma F.2. of (12) to the Lemma D.2 below."
CB,0.39562923888470236,"Lemma D.2. Suppose the Markovian samples {si, ai}i≥0 are generated following the policy πω and
transition kernel P′ (can be P or Pξ), and s′
i+1 ∼P(·|si, ai). Then, for any deterministic mapping
X : S × A × S × S →Rp×q (p, q ∈N+ are arbitrary.) such that ∥X(s, a, s′, es)∥F ≤Cx and for
any s, s′, es ∈S, a ∈A, we have"
CB,0.39638281838733985,"E
h 1 n"
CB,0.3971363978899774,"n+n′−1
X"
CB,0.39788997739261495,"i=n′
X(si, ai, si+1, s′
i+1) −X

2 F"
CB,0.39864355689525244,"sn′
i
≤9C2
x(κ + 1 −ρ)"
CB,0.39939713639789,"n(1 −ρ)
, ∀n, n′ ∈N+
(18)"
CB,0.40015071590052753,"where X = E

X(si, ai, si+1, s′
i+1)
si

with si ∼µω (or νω) when P′ = P (or Pξ)."
CB,0.400904295403165,"Proof. Denote Y (s, a, s′) := Ees∼P′(·|s,a)

X(s, a, s′, es)
s, a, s′
which satisﬁes ∥Y (s, a, s′)∥≤Cx
and Esi∼νω

Y (si, ai, si+1)

= X. Hence, Lemma F.2 of (12) can be applied to Y (s, a, s′) and
obtain the following inequality"
CB,0.40165787490580257,"E
h 1 n"
CB,0.4024114544084401,"n+n′−1
X"
CB,0.4031650339110776,"i=n′
Y (si, ai, si+1) −X

2 F"
CB,0.40391861341371516,"sn′
i
≤8C2
x(κ + 1 −ρ)"
CB,0.4046721929163527,"n(1 −ρ)
.
(19)"
CB,0.4054257724189902,"Therefore, we obtain that"
CB,0.40617935192162774,"E
h 1 n"
CB,0.4069329314242653,"n+n′−1
X"
CB,0.4076865109269028,"i=n′
X(si, ai, si+1, s′
i+1) −X

2 F"
CB,0.4084400904295403,"{si, ai, si+1}n+n′−1
i=n′
i"
CB,0.4091936699321778,"=
E
h 1 n"
CB,0.40994724943481536,"n+n′
X"
CB,0.4107008289374529,"i=n′
X(si, ai, si+1, s′
i+1) −X
{si, ai, si+1}n+n′−1
i=n′
i
2 F"
CB,0.4114544084400904,"+ Var
h 1 n"
CB,0.41220798794272795,"n+n′−1
X"
CB,0.4129615674453655,"i=n′
X(si, ai, si+1, s′
i+1)
{si, ai, si+1}n+n′−1
i=n′
i"
CB,0.413715146948003,"(i)
=
 1 n"
CB,0.41446872645064053,"n+n′−1
X"
CB,0.4152223059532781,"i=n′
Y (si, ai, si+1) −X

2 F + 1 n2"
CB,0.4159758854559156,"n+n′−1
X"
CB,0.4167294649585531,"i=n′
Var

X(si, ai, si+1, s′
i+1)
{si, ai, si+1}n+n′−1
i=n′
"
CB,0.41748304446119067,"(ii)
≤
 1 n"
CB,0.41823662396382816,"n+n′−1
X"
CB,0.4189902034664657,"i=n′
Y (si, ai, si+1) −X

2"
CB,0.41974378296910325,"F + C2
x
n
(20)"
CB,0.42049736247174074,"where (i) uses the conditional independency among {s′
i+1}(t+1)N−1
i=tN
on {si, ai, si+1}n+n′−1
i=n′
and (ii)
uses the fact that ∥X(si, ai, si+1, s′
i+1)∥F ≤Cx."
CB,0.4212509419743783,"Finally, eq. (18) can be proved via the following inequality."
CB,0.42200452147701584,"E
h 1 n"
CB,0.42275810097965333,"n+n′
X"
CB,0.4235116804822909,"i=n′
X(si, ai, si+1, s′
i+1) −X

2 F sn′
i"
CB,0.4242652599849284,Under review as a conference paper at ICLR 2022
CB,0.4250188394875659,"(i)
≤E
h 1 n"
CB,0.42577241899020346,"n+n′−1
X"
CB,0.426525998492841,"i=n′
Y (si, ai, si+1) −X

2 F"
CB,0.4272795779954785,"sn′
i
+ C2
x
n"
CB,0.42803315749811605,"(ii)
≤8C2
x(κ + 1 −ρ)"
CB,0.4287867370007536,"n(1 −ρ)
+ C2
x
n ≤9C2
x(κ + 1 −ρ)"
CB,0.4295403165033911,"n(1 −ρ)
,"
CB,0.43029389600602863,"where (i) takes the conditional expectation of eq. (20) on s′
n and (ii) uses eq. (19)."
CB,0.4310474755086662,"Next, we prove the following Lemmas D.3 & D.4 on the decentralized TD in Algorithm 2. We ﬁrst
deﬁne the following useful notations."
CB,0.43180105501130367,"λφ := λmin
 
Es∼µω[φ(s)φ(s)⊤]

> 0, see Assumption 4."
CB,0.4325546345139412,"B(s, s′) := φ(s)

γφ(s′) −φ(s)
⊤."
CB,0.43330821401657876,"Bt :=
1
Nc
P(t+1)Nc−1
i=tNc
B(si, si+1)."
CB,0.43406179351921625,"Bω := Es∼µω,a∼πω(·|s),s′∼P(·|s,a)

B(s, s′)

."
CB,0.4348153730218538,"b(m)(s, a, s′) := R(m)(s, a, s′)φ(s)."
CB,0.43556895252449135,"b(s, a, s′) :=
1
M
PM
m=1 b(m)(s, a, s′)."
CB,0.43632253202712884,"b(m)
t
:=
1
Nc
P(t+1)Nc−1
i=tNc
b(m)(si, ai, si+1)."
CB,0.4370761115297664,"bt :=
1
M
PM
m=1 b(m)
t
."
CB,0.43782969103240393,"bω := Es∼µω,a∼πω(·|s),s′∼P(·|s,a)

b(s, a, s′)

."
CB,0.4385832705350414,"θ∗
ω := B−1
ω bω, which is the optimal critic parameter under policy πω."
CB,0.43933685003767897,Lemma D.3. The following bounds hold for Algorithm 2.
CB,0.4400904295403165,"1. ∥B(s, s′)∥F , ∥Bt∥F , ∥Bω∥F ≤CB := 1 + γ,
∥b(m)(s, a, s′)∥, ∥b(s, a, s′)∥, ∥b(m)
t
∥, ∥bt∥, ∥bω∥≤Cb := Rmax."
CB,0.440844009042954,2. θ⊤Bωθ ≤−λB
CB,0.44159758854559156,"2 ∥θ∥2 uniformly for all ω, where λB := 2(1 −γ)λφ > 0."
CB,0.4423511680482291,"3. ∥θ∗
ω∥≤Rθ := 2Cb"
CB,0.4431047475508666,λB uniformly for all ω.
CB,0.44385832705350414,"Proof. We ﬁrst prove the item 1. Notice that for any vectors x, y ∈Rd,"
CB,0.4446119065561417,∥xy⊤∥F =
CB,0.4453654860587792,"v
u
u
t d
X i=1 d
X"
CB,0.4461190655614167,"j=1
(xiyj)2 ="
CB,0.4468726450640543,"v
u
u
t d
X"
CB,0.44762622456669177,"i=1
x2
i"
CB,0.4483798040693293,"v
u
u
t d
X"
CB,0.44913338357196686,"j=1
y2
j = ∥x∥∥y∥."
CB,0.44988696307460435,"Hence, we obtain that"
CB,0.4506405425772419,"∥B(s, s′)∥F =
φ(s)
 
γφ(s′) −φ(s)
⊤
F = ∥φ(s)∥∥γφ(s′) −φ(s)∥≤1 + γ := CB,
(21)"
CB,0.45139412207987945,"∥b(s, a, s′)∥= R(s, a, s′)∥φ(s)∥≤Rmax := Cb.
(22)"
CB,0.45214770158251694,"The other terms listed in the item 1 can be proved by applying the Jensen’s inequality to the convex
function ∥· ∥."
CB,0.4529012810851545,"Next, we prove the item 2, where we use the underlying distribution that s ∼µω, a ∼πω(·|s),
s′ ∼P(·|s, a). We obtain that"
CB,0.45365486058779203,"θ⊤Bωθ = Eω

θ⊤φ(s)

γφ(s′) −φ(s)
⊤θ
"
CB,0.4544084400904295,"= γEω
h 
θ⊤φ(s)
 
θ⊤φ(s′)
i
−Eω
h 
θ⊤φ(s)
2i"
CB,0.45516201959306707,Under review as a conference paper at ICLR 2022 ≤γ 2
CB,0.4559155990957046,"
Eω
h 
θ⊤φ(s)
2i
+ Eω
h 
θ⊤φ(s′)
2i
−Eω
h 
θ⊤φ(s)
2i"
CB,0.4566691785983421,"(i)
= (γ −1)Eω
h 
θ⊤φ(s)
2i"
CB,0.45742275810097965,= −(1 −γ)θ⊤Eω[φ(s)φ(s)⊤]θ
CB,0.4581763376036172,"(ii)
≤−λB"
CB,0.4589299171062547,"2 ∥θ∥2,
(23)"
CB,0.45968349660889224,"where (i) uses the fact that s, s′ ∼µω which is the stationary state distribution with the transition
kernel P and the policy πω, and (ii) uses Assumption 4 and we denote λB := 2(1 −γ)λφ > 0."
CB,0.4604370761115298,"Finally, the item 3 can be proved via the following inequality."
CB,0.4611906556141673,"∥θ∗
ω∥2 (i)
≤−2"
CB,0.4619442351168048,"λB
(θ∗
ω)⊤Bωθ∗
ω ≤2"
CB,0.46269781461944237,"λB
∥θ∗
ω∥∥Bωθ∗
ω∥= 2"
CB,0.46345139412207986,"λB
∥θ∗
ω∥∥bω∥≤2Cb"
CB,0.4642049736247174,"λB
∥θ∗
ω∥,
(24)"
CB,0.46495855312735496,where (i) uses the item 2.
CB,0.46571213262999245,"Lemma D.4. Under Assumptions 1–5 and choosing β ≤min
  λB"
CB,0.46646571213263,"8C2
B ,
4
λB , 1−σW"
CB,0.46721929163526754,"2CB

, Nc ≥
  2 λB +"
CB,0.46797287113790503,"2β
 192C2
B[1+(κ−1)ρ]
(1−ρ)λB
, Algorithm 2 has the following convergence rate. M
X"
CB,0.4687264506405426,"m=1
E
θ(m)
Tc+T ′
c −θ∗
ωt
2ωt

≤σ2T ′
c
W β2c2 + 2M
h
c3

1 −λB"
CB,0.4694800301431801,"8 β
Tc
+ c1 Nc"
CB,0.4702336096458176,"i
.
(25)"
CB,0.47098718914845517,"Moreover, to achieve PM
m=1 E
θ(m)
Tc+T ′c −θ∗
ωt
2ωt

≤ϵ, we can choose Tc, T ′
c = O

ln(ϵ−1)
"
CB,0.4717407686510927,"and Nc = O(ϵ−1). Consequently, the sample complexity is TcNc = O

ϵ−1 ln(ϵ−1)

and the
communication complexity is Tc + T ′
c = O

ln(ϵ−1)

."
CB,0.4724943481537302,"Proof. In Algorithm 2, by averaging the TD update rule (26) over the agents m ∈M, we obtain that
the averaged critic parameter θt,t′ :=
1
M
PM
m=1 θ
(m)
t,t′ follows the following update rule"
CB,0.47324792765636775,"θt,t′+1 = 1 M M
X m=1 h
M
X"
CB,0.4740015071590053,"m′=1
Wm,m′θ(m′)
t,t′
+ β
 
Bt′θ(m)
t,t′ + b(m)
t′
i = 1 M M
X"
CB,0.4747550866616428,"m′=1
θ(m′)
t,t′
+ β 1 M M
X m=1"
CB,0.47550866616428034," 
Bt′θ(m)
t,t′ + b(m)
t′
"
CB,0.4762622456669179,"= θt,t′ + β
 
Bt′θt,t′ + bt′
(26)"
CB,0.4770158251695554,"which can be viewed as a centralized TD update using the Markovian samples {si, ai}i from the
transition kernel P and the joint policy πt. Therefore, Theorem 4 in (60) can be directly applied to
analyze this centralized TD update and obtain the following convergence rate of θt,t′, since all the
conditions of that theorem are met 4."
CB,0.4777694046721929,"E
θt,Tc −θ∗
ωt
2ωt

≤

1 −λB"
CB,0.47852298417483047,"4 β
Tc
E
θt,0 −θ∗
ωt
2ωt
 +
 2"
CB,0.47927656367746796,"λB
+ 2β
192
 
C2
BR2
θ + C2
b

[1 + (κ −1)ρ]
(1 −ρ)λBNc"
CB,0.4800301431801055,"(i)
≤2

1 −λB"
CB,0.48078372268274305,"4 β
Tc  θ−1
2 + R2
θ

+ c1 Nc"
CB,0.48153730218538054,"(ii)
≤c3"
CB,0.4822908816880181,"
1 −λB"
CB,0.48304446119065564,"4 β
Tc
+ c1"
CB,0.48379804069329313,"Nc
.
(27)"
CB,0.4845516201959307,"where (i) uses the condition that β ≤4/λB, the item 3 of Lemma D.3 and the constant that"
CB,0.4853051996985682,"c1 :=
1920(C2
BR2
θ+C2
b)[1+(κ−1)ρ]
(1−ρ)λ2
B
, (ii) uses the constant that c3 := 2
 θ−1
2 + R2
θ

."
CB,0.4860587792012057,4We corrected the typo 1 −λB
CB,0.48681235870384326,"8 β, which should be 1 −λB 4 β."
CB,0.4875659382064808,Under review as a conference paper at ICLR 2022
CB,0.4883195177091183,"Next, we consider the consensus error ∥∆Θt,t′∥2
F = PM
m=1
θ(m)
t,t′ −θt,t′2 where we deﬁne"
CB,0.48907309721175585,"Θt,t′ := [θ(1)
t,t′, . . . , θ(M)
t,t′ ]⊤. Note that the critic-step (26) can be rewritten into the following matrix
form"
CB,0.4898266767143934,"Θt,t′+1 = WΘt,t′ + β
 
Θt,t′B⊤
t′ + [b(1)
t′ ; . . . ; b(M)
t′
]⊤
; t′ = 0, 1, . . . , Tc −1,
(28)"
CB,0.4905802562170309,"which further implies that for any t′ = 0, 1, . . . , Tc −1,"
CB,0.49133383571966843,"∆Θt,t′+1

F"
CB,0.492087415222306,"(i)
≤
W∆Θt,t′

F + β
∆Θt,t′B⊤
t′

F + β
∆[b(1)
t′ ; . . . ; b(M)
t′
]⊤
F"
CB,0.49284099472494347,"(ii)
≤(σW + βCB)
∆Θt,t′
F + β"
CB,0.493594574227581,"v
u
u
tM M
X"
CB,0.49434815373021856,"m=1
∥b(m)
t′
∥2"
CB,0.49510173323285606,"(iii)
≤1 + σW 2"
CB,0.4958553127354936,"∆Θt,t′
F + βMCb,"
CB,0.49660889223813115,"where (i) uses the item 1 of Lemma D.1, (ii) uses the item 3 of Lemma D.1 and the item 1 of Lemma
D.3, (iii) uses the condition that β ≤1−σW"
CB,0.49736247174076864,"2CB and the item 1 of Lemma D.3. Telescoping the inequality
above yields that
∆Θt,Tc

F ≤
1 + σW 2"
CB,0.4981160512434062,"Tc∆Θt,0

F + 2βMCb 1 −σW"
CB,0.49886963074604374,"(i)
= 2βMCb"
CB,0.4996232102486812,"1 −σW
,
(29)"
CB,0.5003767897513187,"where (i) uses the equality that ∆Θ0 = O due to the initial condition that Θt,0 = [θ−1; . . . ; θ−1]⊤."
CB,0.5011303692539563,"On the other hand, the ﬁnal T ′
c local average steps in Algorithm 2 can be rewritten into the following
matrix form"
CB,0.5018839487565938,"Θt,t′+1 = WΘt,t′; t = Tc, Tc + 1, . . . , Tc + T ′
c −1."
CB,0.5026375282592314,"Hence, the average critic parameter θt,t′ does not change in these local average steps, i.e.,"
CB,0.5033911077618689,"θt,Tc+T ′c = 1"
CB,0.5041446872645065,"M Θ⊤
t,Tc+T ′c1 = 1"
CB,0.5048982667671439,"M Θ⊤
t,Tc(W T ′
c)⊤1 = 1"
CB,0.5056518462697814,"M Θ⊤
t,Tc1 = θt,Tc.
(30)"
CB,0.506405425772419,"Therefore, we obtain that M
X m=1"
CB,0.5071590052750565,"θ(m)
t,Tc+T ′c −θt,Tc
2 = M
X m=1"
CB,0.5079125847776941,"θ(m)
t,Tc+T ′c −θt,Tc+T ′c
2 = ∥∆Θt,Tc+T ′c∥2
F = ∥∆W T ′
cΘt,Tc∥2
F"
CB,0.5086661642803316,"(i)
= ∥W T ′
c∆Θt,Tc∥2
F"
CB,0.5094197437829691,"(ii)
≤σ2T ′
c
W ∥∆Θt,Tc∥2
F"
CB,0.5101733232856066,"(iii)
≤σ2T ′
c
W
2βMCb 1 −σW"
CB,0.5109269027882442,"2 (iv)
= σ2T ′
c
W β2c2/2
(31)"
CB,0.5116804822908817,"where (i) and (ii) use the items 1 and 3 of Lemma D.1 respectively, (iii) uses eq. (29), (iv) denotes
that c2 := 2
  2MCb"
CB,0.5124340617935192,"1−σW
2. Combining eqs. (27) & (31) yields that M
X"
CB,0.5131876412961568,"m=1
E
θ(m)
t,Tc+T ′c −θ∗
ωt
2ωt

≤2 M
X"
CB,0.5139412207987942,"m=1
E
θ(m)
t,Tc+T ′c −θt,Tc
2ωt

+ 2ME
θt,Tc −θ∗
ωt
2ωt
"
CB,0.5146948003014318,"≤σ2T ′
c
W β2c2 + 2M
h
c3

1 −λB"
CB,0.5154483798040693,"4 β
Tc
+ c1 Nc i
."
CB,0.5162019593067069,"In the inequality above, replacing θ(m)
t,Tc+T ′c from Algorithm 2 by its corresponding variable θ(m)
t
from
Algorithm 1 proves eq. (25). Finally, it can be easily veriﬁed that the following hyperparameter
choices make the error bound in (25) smaller than ϵ and also satisfy the conditions of Lemma D.4."
CB,0.5169555388093444,"β = min
  λB"
CB,0.517709118311982,"8C2
B
, 4"
CB,0.5184626978146194,"λB
, 1 −σW"
CB,0.519216277317257,2CB
CB,0.5199698568198945,"
= O(1)"
CB,0.520723436322532,"Nc = max
h  2"
CB,0.5214770158251696,"λB
+ 2β
192C2
B[1 + (κ −1)ρ]
(1 −ρ)λB
, 6Mc1ϵ−1i
= O(ϵ−1)"
CB,0.5222305953278071,Under review as a conference paper at ICLR 2022
CB,0.5229841748304446,"Tc =
l
ln(6Mc3ϵ−1)"
CB,0.5237377543330821,"ln
 
1 −λBβ/4
−1
m
= O

ln(ϵ−1)
"
CB,0.5244913338357197,"T ′
c = 2
lln(3β2c2ϵ−1)"
CB,0.5252449133383572,"ln(σ−1
W )"
CB,0.5259984928409948,"m
= O

ln(ϵ−1)
"
CB,0.5267520723436323,"Lemma D.5. For any ω, eω ∈Ω, s ∈S and a(m) ∈Am (Am denotes the action space for the agent
m), the following properties hold."
CB,0.5275056518462697,"1. ∥ψ(m)
ω
(a(m)|s)∥≤Cψ, where ψ(m)
ω
(a(m)|s) := ∇ω(m) ln π(m)
ω
(a(m)|s)."
CB,0.5282592313489073,"2. ∥ψ(m)
eω
(a(m)|s) −ψ(m)
ω
(a(m)|s)∥≤Lψ∥eω(m) −ω(m)∥."
DTV,0.5290128108515448,"3. dTV

π(m)
eω(m)(·|s), π(m)
ω(m)(·|s)

≤Lπ∥eω(m) −ω(m)∥."
DTV,0.5297663903541824,"4. 0 ≤Vω(s), Qω(s, a) ≤(1 −γ)Rmax, 0 ≤J(ω) ≤Rmax."
DTV,0.5305199698568199,"5. dTV

νω(·|s), νeω(·|s)

≤Lν∥ω′ −ω∥where Lν := Lπ[1 + logρ(κ−1) + (1 −ρ)−1]."
DTV,0.5312735493594575,"6. dTV

Qeω(s, a), Qω(s, a)

≤LQ∥eω −ω∥where LQ := 2RmaxLν 1−γ
."
DTV,0.5320271288620949,7. J(ω) is LJ-smooth where LJ := Rmax(4Lν + Lψ)/(1 −γ).
DTV,0.5327807083647325,"8. ∥∇J(ω)∥≤DJ := CψRmax 1−γ
."
DTV,0.53353428786737,9. F(ω) is LF -Lipschitz where LF := 2Cψ(LπCψ + LνCψ + Lψ).
DTV,0.5342878673700076,"10. h(ω) is Lh-Lipschitz where Lh := 2λ−1
F (DJλ−1
F LF + LJ)."
DTV,0.5350414468726451,"Proof. For any ω(m), eω(m) ∈Ωm, s ∈S and a(m) ∈Am, arbitrarily select ω(m′) = eω(m′) ∈Ωm′,
a(m′) ∈Am′ for every m′ ∈{1, ..., M}/{m}. Denote ω = [ω(1); . . . ; ω(M)], eω = [eω(1); . . . ; eω(M)],
a = [a(1), . . . , a(M)]. Notice that the joint score vector has the following decomposition"
DTV,0.5357950263752826,"ψω(a|s) = [ψ(1)
ω (a(1)|s); . . . ; ψ(M)
ω
(a(M)|s)].
(32)"
DTV,0.5365486058779201,"Hence, the items 1 & 2 can be proved via the following two inequalities, respectively."
DTV,0.5373021853805576,"∥ψ(m)
ω
(a(m)|s)∥≤"
DTV,0.5380557648831952,"v
u
u
t M
X"
DTV,0.5388093443858327,"m′=1
∥ψ(m′)
ω
(a(m′)|s)∥2 (i)
= ∥ψω(a|s)∥
(ii)
≤Cψ."
DTV,0.5395629238884703,"∥ψ(m)
eω
(a(m)|s) −ψ(m)
ω
(a(m)|s)∥= ∥ψeω(a|s) −ψω(a|s)∥"
DTV,0.5403165033911078,"(i)
≤Lψ∥eω −ω∥= Lψ∥eω(m) −ω(m)∥"
DTV,0.5410700828937453,where (i) uses Assumption 2.
DTV,0.5418236623963828,"Next, we prove the item 3. Notice that"
DTV,0.5425772418990203,"dTV

πeω(·|s), πω(·|s)
"
DTV,0.5433308214016579,"(i)
= sup
A⊂A
|πeω(A|s) −πω(A|s)|"
DTV,0.5440844009042954,"(ii)
≥
sup
A1⊂A1,...,AM⊂AM  M
Y"
DTV,0.544837980406933,"m′=1
πeω(m′)(Am′|s) − M
Y"
DTV,0.5455915599095704,"m′=1
πω(m′)(Am′|s)"
DTV,0.546345139412208,"(iii)
=
sup
A1⊂A1,...,AM⊂AM  M
Y"
DTV,0.5470987189148455,"m′=1,m′̸=m
πω(m′)(Am′|s)

πeω(m)(Am|s) −πω(m)(Am|s)"
DTV,0.5478522984174831,Under review as a conference paper at ICLR 2022
DTV,0.5486058779201206,"(iv)
=
sup
Am⊂Am"
DTV,0.549359457422758,"πeω(m)(Am|s) −πω(m)(Am|s)
 = dTV

π(m)
eω(m)(·|s), π(m)
ω(m)(·|s)

,"
DTV,0.5501130369253956,"where (i) denotes that πω(A|s) =
R"
DTV,0.5508666164280331,"A πω(a|s)da, (ii) uses the relation that ×m∈MAm ⊂A, (iii) uses
our construction that ω(m′) = eω(m′) ∈Ωm′, ∀m′ ∈{1, ..., M}/{m}, and (iv) uses Am′ = Am′ to
achieve the supremum. Therefore, the item 2 can be proved via the following inequality."
DTV,0.5516201959306707,"dTV

π(m)
eω(m)(·|s), π(m)
ω(m)(·|s)

= dTV

πeω(·|s), πω(·|s)
 ≤
Lπ ∥eω −ω∥= Lπ∥eω(m) −ω(m)∥,"
DTV,0.5523737754333082,where (i) uses Assumption 2.
DTV,0.5531273549359458,The item 4 can be proved by the following three inequalities that use Assumption 3.
DTV,0.5538809344385832,"0 ≤Vω(s) = Eω
h ∞
X"
DTV,0.5546345139412208,"t=0
γtRt
s0 = s
i
≤ ∞
X"
DTV,0.5553880934438583,"t=0
γtRmax = Rmax"
DTV,0.5561416729464959,"1 −γ ,"
DTV,0.5568952524491334,"0 ≤Qω(s, a) = Es′∼P(·|s,a)[R(s, a, s′) + γVω(s′)] ≤Rmax + γ Rmax"
DTV,0.557648831951771,1 −γ = Rmax
DTV,0.5584024114544084,"1 −γ ,"
DTV,0.5591559909570459,"0 ≤J(ω) = (1 −γ)Eω
h ∞
X"
DTV,0.5599095704596835,"t=0
γtRt
i
≤(1 −γ) ∞
X"
DTV,0.560663149962321,"t=0
γtRmax = Rmax."
DTV,0.5614167294649586,"The proof of the items 5 – 7 can be found in the proof of Lemma 3, Lemma 4 and Proposition 1 of
(60), respectively."
DTV,0.5621703089675961,"Next, the item 8 is proved by the following inequality.
∇J(ω)
 =
Es∼νω,a∼πω(·|s)

Qω(s, a)ψω(a|s)
"
DTV,0.5629238884702336,"(i)
≤Es∼νω,a∼πω(·|s)

|Qω(s, a)|
ψω(a|s)
 (ii)
≤CψRmax"
DTV,0.5636774679728711,"1 −γ
,"
DTV,0.5644310474755087,"where (i) applies Jensen’s inequality, (ii) uses Assumption 2 and the item 4."
DTV,0.5651846269781462,"Next, the item 9 is proved by the following inequality.
F(eω) −F(ω)"
DTV,0.5659382064807837,"=
Es∼νπeω ,a∼πeω(·|s)

ψeω(a|s)ψeω(a|s)⊤
−Es∼νπω ,a∼πω(·|s)

ψω(a|s)ψω(a|s)⊤"
DTV,0.5666917859834213,"(i)
≤
Es∼νπeω ,a∼πeω(·|s)

ψeω(a|s)ψeω(a|s)⊤
−Es∼νπω ,a∼πω(·|s)

ψeω(a|s)ψeω(a|s)⊤"
DTV,0.5674453654860587,"+ Es∼νπω ,a∼πω(·|s)
[ψeω(a|s) −ψω(a|s)]ψeω(a|s)⊤"
DTV,0.5681989449886963,"+ Es∼νπω ,a∼πω(·|s)
ψω(a|s)[ψeω(a|s) −ψω(a|s)]⊤"
DTV,0.5689525244913338,"(ii)
≤

Z"
DTV,0.5697061039939714,"S×A
[νeω(s)πeω(a|s) −νω(s)πω(a|s)]

ψeω(a|s)ψeω(a|s)⊤
dsda
 + 2CψLψ∥eω −ω∥ ≤C2
ψ Z"
DTV,0.5704596834966089,"S×A
|νeω(s)πeω(a|s) −νω(s)πω(a|s)|dsda + 2CψLψ∥eω −ω∥ ≤C2
ψ Z"
DTV,0.5712132629992465,"S×A
νeω(s)|πeω(a|s) −πω(a|s)|dsda"
DTV,0.5719668425018839,"+ C2
ψ Z"
DTV,0.5727204220045214,"S×A
πω(a|s)|νeω(s) −νω(s)|dsda + 2CψLψ
eω −ω"
DTV,0.573474001507159,"(iii)
≤2LπC2
ψ
eω −ω
 + 2LνC2
ψ
eω −ω
 + 2CψLψ
eω −ω
 := LF
eω −ω"
DTV,0.5742275810097965,"where (i) applies triangle inequality and then Jensen’s inequality to the norm ∥·∥, (ii) uses Assumption
2, (iii) uses the equality that
R"
DTV,0.5749811605124341,"S νω(s)ds =
R"
DTV,0.5757347400150716,"A πω(a|s)da = 1 as well as the inequlities that
R"
DTV,0.5764883195177091,"A |πeω(a|s) −πω(a|s)|da = 2dTV

πeω(·|s), πω(·|s)

≤2Lπ∥eω −ω∥(based on Assumption 2) and
that
R"
DTV,0.5772418990203466,"S |νeω(s) −νω(s)|ds = 2dTV

νω(·|s), νeω(·|s)

≤2Lν∥ω′ −ω∥(based on the item 5)."
DTV,0.5779954785229842,Under review as a conference paper at ICLR 2022
DTV,0.5787490580256217,"Finally, the item 10 is proved by the following inequality
h(eω) −h(ω)"
DTV,0.5795026375282593,"=
F(eω)−1∇J(eω) −F(ω)−1∇J(ω)"
DTV,0.5802562170308968,"≤2
[F(eω)−1 −F(ω)−1]∇J(eω)
 + 2
F(ω)−1[∇J(eω) −∇J(ω)]∥"
DTV,0.5810097965335342,"(i)
≤2DJ
F(ω)−1[F(ω) −F(eω)]F(eω)−1 + 2LJ
F(ω)−1eω −ω"
DTV,0.5817633760361718,"(ii)
≤2DJλ−2
F LF
eω −ω
 + 2LJλ−1
F
eω −ω
 := Lh
eω −ω
,"
DTV,0.5825169555388093,"where (i) uses the items 7 & 8, and (ii) uses the inequality that ∥F(ω)−1∥= λmax(F(ω)−1) =
λmin[F(ω)]−1 ≤λ−1
F
for all ω (since F(ω) and F(ω)−1 are positive deﬁnite) and the item 9."
DTV,0.5832705350414469,"Next, we bound the approximation error of the following stochastic (partial) policy gradients."
DTV,0.5840241145440844,b∇ω(m)J(ωt) := 1 N
DTV,0.584777694046722,"(t+1)N−1
X i=tN "
DTV,0.5855312735493594,"R
(m)
i
+ γφ(s′
i+1)⊤θ(m)
t
−φ(si)⊤θ(m)
t

ψ(m)
t
(a(m)
i
|si),
(33)"
DTV,0.586284853051997,"b∇J(ωt) :=
b∇ω(1)J(ωt); . . . ; b∇ω(M)J(ωt)

,
(34)"
DTV,0.5870384325546345,"b∇ω(m)J(ωt; Bt,k) := 1 Nk X"
DTV,0.587792012057272,"i∈Bt,k "
DTV,0.5885455915599096,"R
(m)
i
+ γφ(s′
i+1)⊤θ(m)
t
−φ(si)⊤θ(m)
t

ψ(m)
t
(a(m)
i
|si),
(35)"
DTV,0.5892991710625471,"b∇J(ωt; Bt,k) :=
b∇ω(1)J(ωt); . . . ; b∇ω(M)J(ωt)

.
(36)"
DTV,0.5900527505651846,"Lemma D.6. Let Assumptions 1-5 hold and adopt the hyperparameters of the decentralized TD in
Algorithm 2 following Lemma D.4. Choose T ′ ≥
ln M
2 ln(σ−1
W ). Then, the following properties hold."
THE ESTIMATED AVERAGE REWARD R,0.5908063300678221,"1. The estimated average reward R
(m)
i
has the following bias and variance bound. M
X"
THE ESTIMATED AVERAGE REWARD R,0.5915599095704597,"m=1
E
"
THE ESTIMATED AVERAGE REWARD R,0.5923134890730972,"R
(m)
i
−Ri
Ri
2 ≤Mσ2T ′
W R2
max,
(37) M
X"
THE ESTIMATED AVERAGE REWARD R,0.5930670685757348,"m=1
Var
"
THE ESTIMATED AVERAGE REWARD R,0.5938206480783723,"R
(m)
i
Ri

≤4R2
maxσ2,
(38)"
THE ESTIMATED AVERAGE REWARD R,0.5945742275810098,"where Ri := [R(1)
i ; . . . ; R(M)
i
] denotes the joint reward."
THE ESTIMATED AVERAGE REWARD R,0.5953278070836473,2. The stochastic policy gradients have the following error bound.
THE ESTIMATED AVERAGE REWARD R,0.5960813865862848,"E
b∇J(ωt) −∇J(ωt)
2
≤c4σ2T ′
W
+ c5β2σ2T ′
c
W
+ c6

1 −λB"
THE ESTIMATED AVERAGE REWARD R,0.5968349660889224,"8 β
Tc + c7"
THE ESTIMATED AVERAGE REWARD R,0.5975885455915599,N + c8
THE ESTIMATED AVERAGE REWARD R,0.5983421250941975,"Nc
+ 16C2
ψζcritic
approx
(39)"
THE ESTIMATED AVERAGE REWARD R,0.5990957045968349,"E
b∇J(ωt; Bt,k) −∇J(ωt)
2Ft,k

≤c4σ2T ′
W
+ 16C2
ψ M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.5998492840994725,"θ(m)
t
−θ∗
ωt
2 + c7"
THE ESTIMATED AVERAGE REWARD R,0.60060286360211,"Nk
+ 16C2
ψζcritic
approx,
(40)"
THE ESTIMATED AVERAGE REWARD R,0.6013564431047476,"where Ft,k := σ

Ft ∪σ
 
{si, ai, si+1, s′
i+1, {e(m)
i
}m∈M}i∈∪k−1
k′=0Bt,k′

."
THE ESTIMATED AVERAGE REWARD R,0.6021100226073851,Proof. We will ﬁrst prove the item 1.
THE ESTIMATED AVERAGE REWARD R,0.6028636021100227,"When Ri := [R(1)
i ; . . . ; R(M)
i
] is given and ﬁxed, the randomness of eR(m)
i
:= R(m)
i
(1 + e(m)
i
)
and b∇ω(m)J(ωt) deﬁned in eq. (4) only comes from the noises {e(m)
i
}M
m=1. Since {e(m)
i
}M
m=1
are independent noises with zero mean and variances σ2
1, . . . , σ2
M, eRi := [ eR(1)
i ; . . . ; eR(M)
i
] has the
following moments"
THE ESTIMATED AVERAGE REWARD R,0.6036171816126601,"E
 eRi|Ri

= Ri,"
THE ESTIMATED AVERAGE REWARD R,0.6043707611152976,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.6051243406179352,"cov
 eRi|Ri

= diag

(R(1)
i )2σ2
1, . . . , (R(M)
i
)2σ2
M

:= Σi."
THE ESTIMATED AVERAGE REWARD R,0.6058779201205727,"Hence, bRi := [R
(1)
i , . . . , R
(m)
i
]⊤= W T ′ eRi (the second “=” comes from eq. (3) and the nota-
tions that eR(m)
i
:= bR(m)
i,0 and that bR(m)
i
:= bR(m)
i,T ′) has the moment that E
 bRi|Ri

= W T ′Ri and"
THE ESTIMATED AVERAGE REWARD R,0.6066314996232103,"Cov
 bRi|Ri

= W T ′Σi(W T ′)⊤. Therefore, eq. (37) can be proved as follows M
X"
THE ESTIMATED AVERAGE REWARD R,0.6073850791258478,"m=1
E
"
THE ESTIMATED AVERAGE REWARD R,0.6081386586284853,"R
(m)
i
−Ri
Ri
2 =
E
 bRi −Ri1
Ri

2
=
W T ′Ri −1"
THE ESTIMATED AVERAGE REWARD R,0.6088922381311228,"M 11⊤Ri

2"
THE ESTIMATED AVERAGE REWARD R,0.6096458176337604,"≤
W T ′ −1"
THE ESTIMATED AVERAGE REWARD R,0.6103993971363979,"M 11⊤
2
∥Ri∥2 (i)
≤Mσ2T ′
W R2
max,"
THE ESTIMATED AVERAGE REWARD R,0.6111529766390355,"where 1 is a M-dim vector of 1’s, (i) uses the inequality that ∥Ri∥2 = PM
m=1(R(m)
i
)2 ≤MR2
max
(based on Assumption 3) and the item 4 of Lemma D.1. Then, eq. (38) can be proved as follows M
X"
THE ESTIMATED AVERAGE REWARD R,0.611906556141673,"m=1
var
"
THE ESTIMATED AVERAGE REWARD R,0.6126601356443104,"R
(m)
i
Ri

= Var
 bRi|Ri

= tr

(W T ′)⊤ΣiW T ′"
THE ESTIMATED AVERAGE REWARD R,0.613413715146948,"= tr
h
W T ′ −1"
THE ESTIMATED AVERAGE REWARD R,0.6141672946495855,"M 11⊤
Σi

W T ′ −1"
THE ESTIMATED AVERAGE REWARD R,0.6149208741522231,"M 11⊤⊤i
+ tr
h
(W T ′)Σi
 1"
THE ESTIMATED AVERAGE REWARD R,0.6156744536548606,M 11⊤i
THE ESTIMATED AVERAGE REWARD R,0.6164280331574982,"+ tr
h 1"
THE ESTIMATED AVERAGE REWARD R,0.6171816126601356,"M 11⊤
Σi(W T ′)⊤i
+ tr
h 1"
THE ESTIMATED AVERAGE REWARD R,0.6179351921627732,"M 11⊤
Σi
 1"
THE ESTIMATED AVERAGE REWARD R,0.6186887716654107,M 11⊤i
THE ESTIMATED AVERAGE REWARD R,0.6194423511680482,"(i)
≤MR2
maxσ2W T ′ −1"
THE ESTIMATED AVERAGE REWARD R,0.6201959306706858,"M 11⊤
2
+ 2"
THE ESTIMATED AVERAGE REWARD R,0.6209495101733233,"M tr

W T ′Σi11⊤
+
1
M 2 tr[1(1⊤Σi1)1⊤]"
THE ESTIMATED AVERAGE REWARD R,0.6217030896759608,"(ii)
≤MR2
maxσ2σ2T ′
W
+ 2"
THE ESTIMATED AVERAGE REWARD R,0.6224566691785983,"M 1⊤ΣiW T ′1 +
1
M 2 (1⊤Σi1)tr[1⊤1]"
THE ESTIMATED AVERAGE REWARD R,0.6232102486812359,"(iii)
≤R2
maxσ2 + 3"
THE ESTIMATED AVERAGE REWARD R,0.6239638281838734,M 1⊤Σi1
THE ESTIMATED AVERAGE REWARD R,0.624717407686511,"= R2
maxσ2 + 3 M M
X"
THE ESTIMATED AVERAGE REWARD R,0.6254709871891485,"m=1
(R(m)
i
)2σ2
m"
THE ESTIMATED AVERAGE REWARD R,0.6262245666917859,"(iv)
≤4R2
maxσ2,"
THE ESTIMATED AVERAGE REWARD R,0.6269781461944235,"where (i) uses the equality that tr(Y ⊤) = tr(Y ) and the inequality (41) below in which X =
W T ′ −
1
M 11⊤and the m-th entry of vm ∈RM is 1 while its other entries are 0, (ii) uses the item 4
of Lemma D.1 and the equality that tr(xy⊤) = y⊤x for any x, y ∈RM, (iii) uses the condition that
T ′ ≥[ln M]/[2 ln(σ−1
W )] and the item 1 of Lemma D.1, (iv) uses Assumption 3."
THE ESTIMATED AVERAGE REWARD R,0.627731725697061,"tr(XΣiX⊤) =tr(X⊤XΣi) = M
X"
THE ESTIMATED AVERAGE REWARD R,0.6284853051996986,"m=1
v⊤
mX⊤XΣivm ≤ M
X"
THE ESTIMATED AVERAGE REWARD R,0.6292388847023361,"m=1
∥vm∥∥X∥2∥Σivm∥ = M
X"
THE ESTIMATED AVERAGE REWARD R,0.6299924642049737,"m=1
(R(m)
i
)2σ2
m∥X∥2 ≤MR2
maxσ2∥X∥2.
(41)"
THE ESTIMATED AVERAGE REWARD R,0.6307460437076111,"Next, we will prove eq. (39) in the item 2, where the error term can be decomposed as follows
b∇J(ωt) −∇J(ωt)
2 ≤4
b∇J(ωt) −gt
2
|
{z
}
(I)"
THE ESTIMATED AVERAGE REWARD R,0.6314996232102487,"+4
gt −g∗
t
2
|
{z
}
(II)"
THE ESTIMATED AVERAGE REWARD R,0.6322532027128862,"+ 4
g∗
t −g∗
t
2
|
{z
}
(III)"
THE ESTIMATED AVERAGE REWARD R,0.6330067822155238,"+4
g∗
t −∇J(ωt)
2
|
{z
}
(IV )"
THE ESTIMATED AVERAGE REWARD R,0.6337603617181613,",
(42)"
THE ESTIMATED AVERAGE REWARD R,0.6345139412207988,where we use the following notations that
THE ESTIMATED AVERAGE REWARD R,0.6352675207234363,"gt := [g(1)
t
; . . . ; g(M)
t
],
(43)"
THE ESTIMATED AVERAGE REWARD R,0.6360211002260738,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.6367746797287114,"g(m)
t
:= 1 N"
THE ESTIMATED AVERAGE REWARD R,0.6375282592313489,"(t+1)N−1
X i=tN "
THE ESTIMATED AVERAGE REWARD R,0.6382818387339865,"Ri + γφ(s′
i+1)⊤θ(m)
t
−φ(si)⊤θ(m)
t

ψ(m)
t
(a(m)
i
|si),
(44)"
THE ESTIMATED AVERAGE REWARD R,0.6390354182366239,"g∗
t := 1 N"
THE ESTIMATED AVERAGE REWARD R,0.6397889977392615,"(t+1)N−1
X i=tN "
THE ESTIMATED AVERAGE REWARD R,0.640542577241899,"Ri + γφ(s′
i+1)⊤θ∗
ωt −φ(si)⊤θ∗
ωt

ψt(ai|si),
(45)"
THE ESTIMATED AVERAGE REWARD R,0.6412961567445365,"g∗
t := Es∼νωt,a∼πt(·|s),s′∼P(·|s,a)
"
THE ESTIMATED AVERAGE REWARD R,0.6420497362471741,"R(s, a, s′) + γφ(s′)⊤θ∗
ωt −φ(s)⊤θ∗
ωt

ψt(a|s)
ωt

.
(46)"
THE ESTIMATED AVERAGE REWARD R,0.6428033157498116,Conditioned on the following ﬁltration
THE ESTIMATED AVERAGE REWARD R,0.6435568952524491,"F′
t :=σ

Ft ∪σ
 
{si, ai, s′
i+1}(t+1)N−1
i=tN+1
"
THE ESTIMATED AVERAGE REWARD R,0.6443104747550866,"=σ
 
{θ(m)
t′
}m∈M,0≤t′≤t ∪{si, ai, s′
i+1}(t+1)N−1
i=0
∪{s(t+1)N} ∪{{e(m)
i
}m∈M}tN−1
i=0

,"
THE ESTIMATED AVERAGE REWARD R,0.6450640542577242,the error term (I) can be bounded as follows.
THE ESTIMATED AVERAGE REWARD R,0.6458176337603617,"E
hb∇J(ωt) −gt,k
2F′
t
i"
THE ESTIMATED AVERAGE REWARD R,0.6465712132629993,"= E
h
M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.6473247927656368,"b∇ω(m)J(ωt) −g(m)
t,k
2F′
t
i (i)
= M
X"
THE ESTIMATED AVERAGE REWARD R,0.6480783722682742,"m=1
E
h 1 N"
THE ESTIMATED AVERAGE REWARD R,0.6488319517709118,"(t+1)N−1
X i=tN  "
THE ESTIMATED AVERAGE REWARD R,0.6495855312735493,"R
(m)
i
−Ri

ψ(m)
t
(a(m)
i
|si)

2F′
t
i"
THE ESTIMATED AVERAGE REWARD R,0.6503391107761869,"(ii)
≤ M
X m=1 E
h 1 N"
THE ESTIMATED AVERAGE REWARD R,0.6510926902788244,"(t+1)N−1
X i=tN  "
THE ESTIMATED AVERAGE REWARD R,0.651846269781462,"R
(m)
i
−Ri

ψ(m)
t
(a(m)
i
|si)
F′
t
i
2 + M
X"
THE ESTIMATED AVERAGE REWARD R,0.6525998492840994,"m=1
Var
h 1 N"
THE ESTIMATED AVERAGE REWARD R,0.653353428786737,"(t+1)N−1
X i=tN  "
THE ESTIMATED AVERAGE REWARD R,0.6541070082893745,"R
(m)
i
−Ri

ψ(m)
t
(a(m)
i
|si)
F′
t
i"
THE ESTIMATED AVERAGE REWARD R,0.6548605877920121,"(iii)
≤ M
X m=1 E
h 1 N"
THE ESTIMATED AVERAGE REWARD R,0.6556141672946496,"(t+1)N−1
X i=tN  "
THE ESTIMATED AVERAGE REWARD R,0.6563677467972872,"R
(m)
i
−Ri
F′
t
i
ψ(m)
t
(a(m)
i
|si)

2 + 1 N 2 M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.6571213262999246,"(t+1)N−1
X"
THE ESTIMATED AVERAGE REWARD R,0.6578749058025621,"i=tN
Var
 "
THE ESTIMATED AVERAGE REWARD R,0.6586284853051997,"R
(m)
i
−Ri

ψ(m)
t
(a(m)
i
|si)
F′
t
"
THE ESTIMATED AVERAGE REWARD R,0.6593820648078372,"(iv)
≤ M
X m=1 h 1 N"
THE ESTIMATED AVERAGE REWARD R,0.6601356443104748,"(t+1)N−1
X"
THE ESTIMATED AVERAGE REWARD R,0.6608892238131123,"i=tN
E
 "
THE ESTIMATED AVERAGE REWARD R,0.6616428033157498,"R
(m)
i
−Ri
F′
t
i2ψ(m)
t
(a(m)
i
|si)
2 + 1 N 2 M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.6623963828183873,"(t+1)N−1
X i=tN"
THE ESTIMATED AVERAGE REWARD R,0.6631499623210249,"ψ(m)
t
(a(m)
i
|si)
2var
"
THE ESTIMATED AVERAGE REWARD R,0.6639035418236624,"R
(m)
i
−Ri
F′
t
"
THE ESTIMATED AVERAGE REWARD R,0.6646571213263,"(v)
≤
C2
ψ
N M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.6654107008289375,"(t+1)N−1
X i=tN 
E
 "
THE ESTIMATED AVERAGE REWARD R,0.6661642803315749,"R
(m)
i
−Ri
F′
t
2 +
C2
ψ
N 2"
THE ESTIMATED AVERAGE REWARD R,0.6669178598342125,"(t+1)N−1
X i=tN M
X"
THE ESTIMATED AVERAGE REWARD R,0.66767143933685,"m=1
var
"
THE ESTIMATED AVERAGE REWARD R,0.6684250188394876,"R
(m)
i
F′
t
"
THE ESTIMATED AVERAGE REWARD R,0.6691785983421251,"(vi)
≤C2
ψ(Mσ2T ′
W R2
max) +
C2
ψ
N (4R2
maxσ2)"
THE ESTIMATED AVERAGE REWARD R,0.6699321778447627,"= C2
ψR2
max

Mσ2T ′
W
+ 4"
THE ESTIMATED AVERAGE REWARD R,0.6706857573474001,"N σ2
,
(47)"
THE ESTIMATED AVERAGE REWARD R,0.6714393368500376,"where (i) uses the deﬁnitions of b∇ω(m)J(ωt) and g(m)
t
deﬁned in eqs. (33) & (44) respectively, (ii)
uses the relation that E∥X∥2 = Var(X) + ∥EX∥2 for any random vector X, (iii) uses the facts that"
THE ESTIMATED AVERAGE REWARD R,0.6721929163526752,"ψ(m)
t
(a(m)
i
|si), Ri ∈F′
t are ﬁxed while {R
(m)
i
}(t+1)N−1
i=tN
are random and independent given F′
t, (iv)
uses the equality that Var(xY ) = Pd
j=1 var(xyj) = Pd
j=1 y2
j var(x) = ∥y∥2var(x) for any random"
THE ESTIMATED AVERAGE REWARD R,0.6729464958553127,"scalar x and ﬁxed vector Y = [y1, . . . , yd] ∈Rd (Here we denote y = ψ(m)
t
(a(m)
i
|si) ∈F′
t), (v)
applies Jensen’s inequality to the convex function (·)2 and uses the item 1 of Lemma D.5 as well as"
THE ESTIMATED AVERAGE REWARD R,0.6737000753579503,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.6744536548605878,"the fact that Ri ∈F′
t is ﬁxed, (vi) uses eqs. (37) & (38) and the fact that the conditional distribution
of R
(m)
i
on Ri ∈F′
t is the same as that on F′
t since the noise e(m)
i
is independent from any other
variables."
THE ESTIMATED AVERAGE REWARD R,0.6752072343632253,Then we bound the error term (II) of eq. (42) as follows.
THE ESTIMATED AVERAGE REWARD R,0.6759608138658628,"gt −g∗
t
2 = M
X m=1 1 N"
THE ESTIMATED AVERAGE REWARD R,0.6767143933685004,"(t+1)N−1
X i=tN"
THE ESTIMATED AVERAGE REWARD R,0.6774679728711379," 
[γφ(s′
i+1) −φ(si)]⊤(θ(m)
t
−θ∗
ωt)

ψ(m)
t
(a(m)
i
|si)

2"
THE ESTIMATED AVERAGE REWARD R,0.6782215523737755,"(i)
≤1 N"
THE ESTIMATED AVERAGE REWARD R,0.678975131876413,"(t+1)N−1
X i=tN M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.6797287113790504,"γφ(s′
i+1) −φ(si)
2θ(m)
t
−θ∗
ωt
2ψ(m)
t
(a(m)
i
|si)
2"
THE ESTIMATED AVERAGE REWARD R,0.680482290881688,"(ii)
≤
C2
ψ(1 + γ)2 N"
THE ESTIMATED AVERAGE REWARD R,0.6812358703843255,"(t+1)N−1
X i=tN M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.6819894498869631,"θ(m)
t
−θ∗
ωt
2"
THE ESTIMATED AVERAGE REWARD R,0.6827430293896006,"= 4C2
ψ M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.6834966088922382,"θ(m)
t
−θ∗
ωt
2,
(48)"
THE ESTIMATED AVERAGE REWARD R,0.6842501883948756,"where (i) applies Jensen’s inequality to the convex function ∥· ∥2, (ii) uses Assumption 4 and the
item 1 of Lemma D.5."
THE ESTIMATED AVERAGE REWARD R,0.6850037678975132,"To bound the error term (III) of eq. (42), denote that"
THE ESTIMATED AVERAGE REWARD R,0.6857573474001507,"X(s, a, s′, es) =
"
THE ESTIMATED AVERAGE REWARD R,0.6865109269027883,"R(s, a, es) + γφ(es)⊤θ∗
ωt −φ(s)⊤θ∗
ωt

ψt(a|s),
(49)"
THE ESTIMATED AVERAGE REWARD R,0.6872645064054258,"which satisﬁes ∥X(s, a, s′, es)∥≤

|R(s, a, es)| +
γφ(es) + φ(s)
θ∗
ωt
ψt(a|s)
 ≤Cψ(Rmax +
2Rθ) (the second ≤uses the item 3 of Lemma D.3) and X = Esi∼νt

X(si, ai, si+1, s′
i+1)
Ft

= g∗
t
where sN, ωt ∈Ft := σ
 
{θ(m)
t′
}m∈M,0≤t′≤t ∪{si, ai, s′
i+1, {e(m)
i
}m∈M}tN−1
i=0
∪{stN}

are ﬁxed.
Hence, Lemma D.2 yields that"
THE ESTIMATED AVERAGE REWARD R,0.6880180859080633,"E
g∗
t −g∗
t
2Ft

= E
h 1 N"
THE ESTIMATED AVERAGE REWARD R,0.6887716654107008,"(t+1)N−1
X"
THE ESTIMATED AVERAGE REWARD R,0.6895252449133383,"i=tN
X(si, ai, si+1, s′
i+1) −X

2Ft
i"
THE ESTIMATED AVERAGE REWARD R,0.6902788244159759,"≤
9C2
ψ(Rmax + 2Rθ)2(κ + 1 −ρ)"
THE ESTIMATED AVERAGE REWARD R,0.6910324039186134,"N(1 −ρ)
.
(50)"
THE ESTIMATED AVERAGE REWARD R,0.691785983421251,"Next, we bound the error term (IV) of eq. (42). Notice that"
THE ESTIMATED AVERAGE REWARD R,0.6925395629238885,"g∗
t −∇J(ωt)"
THE ESTIMATED AVERAGE REWARD R,0.693293142426526,"= Eωt
h"
THE ESTIMATED AVERAGE REWARD R,0.6940467219291635,"R(s, a, es) + [γφ(es) −φ(s)]⊤θ∗
ωt −
"
THE ESTIMATED AVERAGE REWARD R,0.694800301431801,"R(s, a, es) + γVωt(es) −Vωt(s)

ψt(a|s)
ωt
i"
THE ESTIMATED AVERAGE REWARD R,0.6955538809344386,"= Eωt
h
γ

φ(es)⊤θ∗
ωt −Vωt(es)

−

φ(s)⊤θ∗
ωt −Vωt(s)

ψt(a|s)
ωt
i
.
(51)"
THE ESTIMATED AVERAGE REWARD R,0.6963074604370761,"Hence,"
THE ESTIMATED AVERAGE REWARD R,0.6970610399397137,"∥g∗
t −∇J(ωt)∥2 =
Eωt
h
γ

φ(es)⊤θ∗
ωt −Vωt(es)

−

φ(s)⊤θ∗
ωt −Vωt(s)

ψt(a|s)
ωt
i
2"
THE ESTIMATED AVERAGE REWARD R,0.6978146194423511,"(i)
≤Eωt
h

γ

φ(es)⊤θ∗
ωt −Vωt(es)

−

φ(s)⊤θ∗
ωt −Vωt(s)

ψt(a|s)

2ωt
i"
THE ESTIMATED AVERAGE REWARD R,0.6985681989449887,"(ii)
≤2C2
ψEωt
h
γ2φ(es)⊤θ∗
ωt −Vωt(es)

2
+
φ(s)⊤θ∗
ωt −Vωt(s)

2ωt
i"
THE ESTIMATED AVERAGE REWARD R,0.6993217784476262,"= 2C2
ψγ2
Z S×A×S"
THE ESTIMATED AVERAGE REWARD R,0.7000753579502638,"φ(es)⊤θ∗
ωt −Vωt(es)

2
νt(s)πt(a|s)P(es|s, a)dsdades"
THE ESTIMATED AVERAGE REWARD R,0.7008289374529013,"+ 2C2
ψEωt
hφ(s)⊤θ∗
ωt −Vωt(s)

2ωt
i"
THE ESTIMATED AVERAGE REWARD R,0.7015825169555389,"(iii)
≤2C2
ψγ
Z S×A×S"
THE ESTIMATED AVERAGE REWARD R,0.7023360964581763,"φ(es)⊤θ∗
ωt −Vωt(es)

2
νt(s)πt(a|s)Pξ(es|s, a)dsdades"
THE ESTIMATED AVERAGE REWARD R,0.7030896759608138,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.7038432554634514,"+ 2C2
ψEωt
hφ(s)⊤θ∗
ωt −Vωt(s)

2ωt
i"
THE ESTIMATED AVERAGE REWARD R,0.7045968349660889,"(iv)
= 2C2
ψ(γ + 1)Eωt
hφ(s)⊤θ∗
ωt −Vωt(s)

2ωt
i"
THE ESTIMATED AVERAGE REWARD R,0.7053504144687265,"(v)
≤4C2
ψζcritic
approx,
(52)"
THE ESTIMATED AVERAGE REWARD R,0.706103993971364,"where (i) applies Jensen’s inequality to the convex function ∥· ∥2, (ii) uses the inequality that
∥x + y∥2 ≤2∥x∥2 + 2∥y∥2 for any x, y ∈Rd, (iii) uses the inequality that P(s′|s, a) ≤
γ−1Pξ(s′|s, a); ∀s, s′ ∈S, a ∈A, (iv) uses the equality that
R"
THE ESTIMATED AVERAGE REWARD R,0.7068575734740015,"S×A νt(s)πt(a|s)Pξ(es|s, a)dsda ="
THE ESTIMATED AVERAGE REWARD R,0.707611152976639,"νt(es), and (v) uses the notation that ζcritic
approx := supω Es∼νω
Vω(s) −φ(s)⊤θ∗
ω
2
. Substituting eqs.
(47),(48),(50)&(52) into eq. (42) yields that"
THE ESTIMATED AVERAGE REWARD R,0.7083647324792766,"E
b∇J(ωt) −∇J(ωt)
2Ft
"
THE ESTIMATED AVERAGE REWARD R,0.7091183119819141,"≤4C2
ψR2
max

Mσ2T ′
W
+ 4"
THE ESTIMATED AVERAGE REWARD R,0.7098718914845517,"N σ2
+ 16C2
ψ M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.7106254709871892,"θ(m)
t
−θ∗
ωt
2"
THE ESTIMATED AVERAGE REWARD R,0.7113790504898266,"+
36C2
ψ(Rmax + 2Rθ)2(κ + 1 −ρ)"
THE ESTIMATED AVERAGE REWARD R,0.7121326299924642,"N(1 −ρ)
+ 16C2
ψζcritic
approx"
THE ESTIMATED AVERAGE REWARD R,0.7128862094951017,"= c4σ2T ′
W
+ c7"
THE ESTIMATED AVERAGE REWARD R,0.7136397889977393,"N + 16C2
ψ M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.7143933685003768,"θ(m)
t
−θ∗
ωt
2 + 16C2
ψζcritic
approx,
(53)"
THE ESTIMATED AVERAGE REWARD R,0.7151469480030144,"where θ(m)
t
, ωt ∈Ft are ﬁxed, and we take the conditional expectation of eq. (47) on Ft ⊂F′
t and"
THE ESTIMATED AVERAGE REWARD R,0.7159005275056518,"denote that c4 := 4MC2
ψR2
max, c7 := 16C2
ψR2
maxσ2 +
36C2
ψ(Rmax+2Rθ)2(κ+1−ρ)"
THE ESTIMATED AVERAGE REWARD R,0.7166541070082894,"1−ρ
. Substituting eq.
(25) into the unconditional expectation of eq. (53) yields that"
THE ESTIMATED AVERAGE REWARD R,0.7174076865109269,"E
b∇J(ωt) −∇J(ωt)
2"
THE ESTIMATED AVERAGE REWARD R,0.7181612660135644,"≤c4σ2T ′
W
+ c7"
THE ESTIMATED AVERAGE REWARD R,0.718914845516202,"N + 16C2
ψ

σ2T ′
c
W β2c2 + 2M
h
c3

1 −λB"
THE ESTIMATED AVERAGE REWARD R,0.7196684250188395,"8 β
Tc
+ c1 Nc"
THE ESTIMATED AVERAGE REWARD R,0.720422004521477,"i
+ 16C2
ψζcritic
approx"
THE ESTIMATED AVERAGE REWARD R,0.7211755840241145,"= c4σ2T ′
W
+ c5β2σ2T ′
c
W
+ c6

1 −λB"
THE ESTIMATED AVERAGE REWARD R,0.7219291635267521,"8 β
Tc
+ c7"
THE ESTIMATED AVERAGE REWARD R,0.7226827430293896,N + c8
THE ESTIMATED AVERAGE REWARD R,0.7234363225320272,"Nc
+ 16C2
ψζcritic
approx,"
THE ESTIMATED AVERAGE REWARD R,0.7241899020346647,"where we denote that c5 := 16c2C2
ψ, c6 := 32Mc3C2
ψ, c8 := 32Mc1C2
ψ. This proves eq. (39)."
THE ESTIMATED AVERAGE REWARD R,0.7249434815373021,"Equation (40) can be proved in the same way as that of proving eq. (53). There are two differences.
First, b∇J(ωt; Bt,k) uses the minibatch Bt,k of size Nk while b∇J(ωt) uses batchsize N. Second, eq.
(40) is conditioned on the ﬁltration Ft,k := σ

Ft∪σ
 
si, ai, si+1, s′
i+1, {e(m)
i
}m∈M"
THE ESTIMATED AVERAGE REWARD R,0.7256970610399397,"i∈∪k−1
k′=0Bt,k′
"
THE ESTIMATED AVERAGE REWARD R,0.7264506405425772,"which includes not only the ﬁltration Ft use by eq. (53) but also the minibatches ∪k−1
k′=0Bt,k′ used by
the previous (k −1) SGD steps."
THE ESTIMATED AVERAGE REWARD R,0.7272042200452148,"Lemma D.7. Implementing Algorithm 3 with η ≤
1
2C2
ψ , T ′ ≥
ln M
2 ln(σ−1
W ), Tz ≥
ln(3DJC2
ψ)"
THE ESTIMATED AVERAGE REWARD R,0.7279577995478523,"ln(σ−1
W ) ,"
THE ESTIMATED AVERAGE REWARD R,0.7287113790504898,"K ≥
ln 3
ln[(1−ηλF /2)−1], N ≥
2304C4
ψ(κ+1−ρ)
ηλ5
F (1−ρ)(1−ηλF /2)(K−1)/2 and Nk ∝(1 −ηλF /2)−k/2, the involved
quantities have the following properties, where Eω denotes the expectation under the underlying
distributions that s ∼νω, a ∼πω(·|s)."
THE ESTIMATED AVERAGE REWARD R,0.7294649585531273,"1. λF ≤λmax[F(ω)] = ∥F(ω)∥≤C2
ψ, ∀ω."
THE ESTIMATED AVERAGE REWARD R,0.7302185380557649,"2.
1
2 ≤1 −ηC2
ψ ≤
I −ηF(ω)
 ≤1 −ηλF , so η ≤
1
2λF ."
THE ESTIMATED AVERAGE REWARD R,0.7309721175584024,"3. C−2
ψ
≤∥F(ω)−1∥≤λ−1
F . For any ω, x ∈Rdω, x⊤F(ω)−1x ≥C−2
ψ ∥x∥2. 4."
THE ESTIMATED AVERAGE REWARD R,0.73172569706104,"h(ω)
 ≤
1
λF
∇J(ω)
 ≤DJ λF ."
THE ESTIMATED AVERAGE REWARD R,0.7324792765636775,"5. h(ω) = arg min
h
Eω
 
ψω(a|s)⊤h −Aω(s, a)
2
, so"
THE ESTIMATED AVERAGE REWARD R,0.7332328560663149,"Eω
 
ψω(a|s)⊤h(ω) −Aω(s, a)
2
≤ζactor
approx where s ∼νω, a ∼πω(·|s)."
THE ESTIMATED AVERAGE REWARD R,0.7339864355689525,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.73474001507159,"6. Eω∗
ψω(a|s)⊤h(ω) −Aω(s, a)

≥−C∗
pζactor
approx, ∀ω."
THE ESTIMATED AVERAGE REWARD R,0.7354935945742276,"7. Nk =
N(1−ηλF /2)(K−1−k)/2(1−√"
THE ESTIMATED AVERAGE REWARD R,0.7362471740768651,"1−ηλF /2)
1−(1−ηλF /2)K/2
≥
576C4
ψ(κ+1−ρ)
λ4
F (1−ρ)
."
THE ESTIMATED AVERAGE REWARD R,0.7370007535795027,8. ht approximates the natural gradient h(ωt) with the following error bound.
THE ESTIMATED AVERAGE REWARD R,0.7377543330821401,"E
ht −h(ωt)
2
≤c10

1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.7385079125847777,"(K−1)/2
+ c11σ2Tz
W
+ c12σ2T ′
W
+ c13β2σ2T ′
c
W"
THE ESTIMATED AVERAGE REWARD R,0.7392614920874152,"+ c14

1 −λB"
THE ESTIMATED AVERAGE REWARD R,0.7400150715900528,"8 β
Tc
+ c15"
THE ESTIMATED AVERAGE REWARD R,0.7407686510926903,"Nc
+ c16ζcritic
approx.
(54)"
THE ESTIMATED AVERAGE REWARD R,0.7415222305953278,Proof. The item 1 is proved by the following inequality. λF
THE ESTIMATED AVERAGE REWARD R,0.7422758100979653,"(i)
≤λmin[F(ω)] ≤λmax[F(ω)]
(ii)
= ∥F(ω)∥"
THE ESTIMATED AVERAGE REWARD R,0.7430293896006028,"=
Eω

ψ(a|s)ψ(a|s)⊤ ≤Eω
ψ(a|s)
ψ(a|s)⊤ (iii)
≤C2
ψ,"
THE ESTIMATED AVERAGE REWARD R,0.7437829691032404,"where (i) uses Assumption 6, (ii) uses the fact that F(ω) is positive deﬁnite implied by Assumption 6,
(iii) applies Jensen’s inequality to the convex function ∥· ∥and (iv) uses Assumption 2."
THE ESTIMATED AVERAGE REWARD R,0.7445365486058779,"Next we will prove the item 2. On one hand,"
THE ESTIMATED AVERAGE REWARD R,0.7452901281085155,"λmin

I −ηF(ω)

= 1 −ηλmax

F(ω)
 (i)
≥1 −ηC2
ψ ≥1"
THE ESTIMATED AVERAGE REWARD R,0.746043707611153,"2,
(55)"
THE ESTIMATED AVERAGE REWARD R,0.7467972871137905,"where (i) uses the item 1, (ii) uses the condition that η ≤
1
2C2
ψ . On the other hand,"
THE ESTIMATED AVERAGE REWARD R,0.747550866616428,"λmin

I −ηF(ω)

≤λmax

I −ηF(ω)
 (i)
= ∥I −ηF(ω)∥= I −ηλmin

F(ω)

≤1 −ηλF , (56)"
THE ESTIMATED AVERAGE REWARD R,0.7483044461190655,"where (i) uses the fact that I −ηF(ω) is positive deﬁnite based on eq. (55). Hence, eqs. (55) & (56)
prove the item 2."
THE ESTIMATED AVERAGE REWARD R,0.7490580256217031,"The item 3 can be proved by the fact that F(ω)−1 is positive deﬁnite with minimum eigenvalue
λmax[F(ω)]−1 ≥C−2
ψ
and maximum eigenvalue λmin[F(ω)]−1 ≤λ−1
F
implied by the item 1."
THE ESTIMATED AVERAGE REWARD R,0.7498116051243406,The item 4 can be proved by the following inequality.
THE ESTIMATED AVERAGE REWARD R,0.7505651846269782,"∥h(ω)∥=
F(ω)−1∇J(ω)
 ≤
F(ω−1)
∇J(ω)

(i)
≤λ−1
F
∇J(ω)

(ii)
≤λ−1
F DJ,"
THE ESTIMATED AVERAGE REWARD R,0.7513187641296156,where (i) uses the item 3 and (ii) uses the item 8 of Lemma D.5.
THE ESTIMATED AVERAGE REWARD R,0.7520723436322532,Next we will prove item 5.
THE ESTIMATED AVERAGE REWARD R,0.7528259231348907,Consider the following function of x ∈Rdω.
THE ESTIMATED AVERAGE REWARD R,0.7535795026375283,fω(x) = 1
THE ESTIMATED AVERAGE REWARD R,0.7543330821401658,"2Eω
 
ψω(a|s)⊤x −Aω(s, a)
2 = 1"
THE ESTIMATED AVERAGE REWARD R,0.7550866616428034,"2x⊤Eω

ψω(a|s)ψω(a|s)⊤
x −Eω

Aω(s, a)ψω(a|s)
⊤x + 1"
THE ESTIMATED AVERAGE REWARD R,0.7558402411454408,"2Eω

Aω(s, a)2 = 1"
THE ESTIMATED AVERAGE REWARD R,0.7565938206480783,2x⊤F(ω)x −∇J(ω)⊤x + 1
THE ESTIMATED AVERAGE REWARD R,0.7573474001507159,"2Eω

Aω(s, a)2"
THE ESTIMATED AVERAGE REWARD R,0.7581009796533534,"Since ∇2f(ω) = F(ω) is positive deﬁnite, f is strongly convex quardratic and thus it has unique
minimizer h(ω) = F(ω)−1∇J(ω) obtained by solving h from the equation ∇fω(h) = F(ω)h −
∇J(ω) = 0. Hence,"
THE ESTIMATED AVERAGE REWARD R,0.758854559155991,"Eω
ψω(a|s)⊤h(ω) −Aω(s, a)
2"
THE ESTIMATED AVERAGE REWARD R,0.7596081386586285,"= min
h Eω
 
ψω(a|s)⊤h −Aω(s, a)
2"
THE ESTIMATED AVERAGE REWARD R,0.760361718161266,"≤sup
ω min
h Eω
 
ψω(a|s)⊤h −Aω(s, a)
2
:= ζactor
approx,
(57)"
THE ESTIMATED AVERAGE REWARD R,0.7611152976639035,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.7618688771665411,which proves the item 5.
THE ESTIMATED AVERAGE REWARD R,0.7626224566691786,The item 6 can be proved by the following inequality.
THE ESTIMATED AVERAGE REWARD R,0.7633760361718162,"Eω∗
Aω(s, a) −ψω(a|s)⊤h(ω)
"
THE ESTIMATED AVERAGE REWARD R,0.7641296156744537,"=
Z
νω∗(s)πω∗(a|s)

Aω(s, a) −ψω(a|s)⊤h(ω)

dsda"
THE ESTIMATED AVERAGE REWARD R,0.7648831951770911,"=
Z
νω(s)πω(a|s)νω∗(s)πω∗(a|s)"
THE ESTIMATED AVERAGE REWARD R,0.7656367746797287,"νω(s)πω(a|s)

Aω(s, a) −ψω(a|s)⊤h(ω)

dsda"
THE ESTIMATED AVERAGE REWARD R,0.7663903541823662,"= Eω
hνω∗(s)πω∗(a|s)"
THE ESTIMATED AVERAGE REWARD R,0.7671439336850038,"νω(s)πω(a|s)

Aω(s, a) −ψω(a|s)⊤h(ω)
i ≤ s"
THE ESTIMATED AVERAGE REWARD R,0.7678975131876413,"Eω
hνω∗(s)πω∗(a|s)"
THE ESTIMATED AVERAGE REWARD R,0.7686510926902789,νω(s)πω(a|s) 2iq
THE ESTIMATED AVERAGE REWARD R,0.7694046721929163,"Eω
 
Aω(s, a) −ψω(a|s)⊤h(ω)
2 (i)
≤C∗
q"
THE ESTIMATED AVERAGE REWARD R,0.7701582516955539,"ζactor
approx,
(58)"
THE ESTIMATED AVERAGE REWARD R,0.7709118311981914,"where (i) uses Assumption 7 and the item 5. Multiplying −1 to the above inequality proves the item
6."
THE ESTIMATED AVERAGE REWARD R,0.771665410700829,"Next, the item 7 can be proved as follows."
THE ESTIMATED AVERAGE REWARD R,0.7724189902034665,"Nk
(i)
=N
(1 −ηλF /2)−k/2
PK−1
k′=0(1 −ηλF /2)−k′/2"
THE ESTIMATED AVERAGE REWARD R,0.773172569706104,"=N(1 −ηλF /2)(K−1−k)/2(1 −
p"
THE ESTIMATED AVERAGE REWARD R,0.7739261492087415,"1 −ηλF /2)
1 −(1 −ηλF /2)K/2"
THE ESTIMATED AVERAGE REWARD R,0.774679728711379,"(ii)
≥
2304C4
ψ(κ + 1 −ρ)"
THE ESTIMATED AVERAGE REWARD R,0.7754333082140166,"ηλ5
F (1 −ρ)(1 −ηλF /2)(K−1)/2
(1 −ηλF /2)(K−1)/2(ηλF /2) 1 +
p"
THE ESTIMATED AVERAGE REWARD R,0.7761868877166541,1 −ηλF /2
THE ESTIMATED AVERAGE REWARD R,0.7769404672192917,"≥
576C4
ψ(κ + 1 −ρ)
λ4
F (1 −ρ)
,"
THE ESTIMATED AVERAGE REWARD R,0.7776940467219292,"where (i) uses the conditions that Nk ∝(1 −ηλF /2)−k/2 and PK−1
k=0 Nk = N and (ii) uses the"
THE ESTIMATED AVERAGE REWARD R,0.7784476262245666,"condition that N ≥
2304C4
ψ(κ+1−ρ)
ηλ5
F (1−ρ)(1−ηλF /2)(K−1)/2"
THE ESTIMATED AVERAGE REWARD R,0.7792012057272042,"Finally, we will prove the item 8. Until the end of this proof, we use the underlying distribution that
ai ∼πt(·|si),si+1 ∼Pξ(·|si, ai) for tN ≤i ≤(t + 1)N −1 in the t-th iteration of the multi-agent
NAC algorithm (Algorithm 1)."
THE ESTIMATED AVERAGE REWARD R,0.7799547852298417,"The local averaging steps of zi,ℓ:= [z(1)
i,ℓ, . . . , z(M)
i,ℓ]⊤yield the following consensus error bound. M
X"
THE ESTIMATED AVERAGE REWARD R,0.7807083647324793,"m=1
(z(m)
Tz
−zTz)2 = ∥∆zi,Tz∥2 = ∥∆W Tzzi,0∥2 (i)
= ∥W Tz∆zi,0∥2 (ii)
≤σ2Tz
W ∥∆zi,0∥2"
THE ESTIMATED AVERAGE REWARD R,0.7814619442351168,"(iii)
≤σ2Tz
W M
X"
THE ESTIMATED AVERAGE REWARD R,0.7822155237377544,"m=1
(z(m)
i,0 )2 = σ2Tz
W M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.7829691032403918,"
ψ(m)
t
(a(m)
i
|si)⊤h(m)
t,k
2"
THE ESTIMATED AVERAGE REWARD R,0.7837226827430294,"(iv)
≤C2
ψσ2Tz
W M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.7844762622456669,"h(m)
t,k
2 ≤C2
ψσ2Tz
W
ht,k
2,"
THE ESTIMATED AVERAGE REWARD R,0.7852298417483045,"where zTz :=
1
M
PM
m=1 z(m)
i,Tz, (i) and (ii) use the items 1 and 3 of Lemma D.1 respectively, (iii) uses
the equality that ∥∆∥= 1, and (iv) uses the item 1 of Lemma D.5."
THE ESTIMATED AVERAGE REWARD R,0.785983421250942,"Then, we deﬁne the following stochastic gradients of function fω."
THE ESTIMATED AVERAGE REWARD R,0.7867370007535796,"e∇ω(m)fωt(ht,k) := 1 Nk X"
THE ESTIMATED AVERAGE REWARD R,0.787490580256217,"i∈Bt,k
ψ(m)
t
(a(m)
i
|si)ψt(ai|si)⊤ht,k −b∇ω(m)J(ωt; Bt,k)"
THE ESTIMATED AVERAGE REWARD R,0.7882441597588545,"e∇fωt(ht,k) := 1 Nk X"
THE ESTIMATED AVERAGE REWARD R,0.7889977392614921,"i∈Bt,k
ψt(ai|si)ψt(ai|si)⊤ht,k −b∇J(ωt; Bt,k)"
THE ESTIMATED AVERAGE REWARD R,0.7897513187641296,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.7905048982667672,"=
e∇ω(1)fωt(ht,k); . . . ; e∇ω(M)fωt(ht,k)

,"
THE ESTIMATED AVERAGE REWARD R,0.7912584777694047,"b∇ω(m)fωt(ht,k) := M Nk X"
THE ESTIMATED AVERAGE REWARD R,0.7920120572720422,"i∈Bt,k
ψ(m)
t
(a(m)
i
|si)z(m)
i,Tz −b∇ω(m)J(ωt; Bt,k),"
THE ESTIMATED AVERAGE REWARD R,0.7927656367746797,"b∇fωt(ht,k) :=
b∇ω(1)fωt(ht,k); . . . ; b∇ω(M)fωt(ht,k)
⊤,"
THE ESTIMATED AVERAGE REWARD R,0.7935192162773173,"where b∇ω(m)J(ωt; Bt,k) and b∇J(ωt; Bt,k) are deﬁned in eqs. (35) & (36) respectively. Hence,
b∇fωt(ht,k) −e∇fωt(ht,k)
2 = M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.7942727957799548,"b∇ω(m)fωt(ht,k) −e∇ω(m)fωt(ht,k)
2 = M
X m=1 1 Nk X"
THE ESTIMATED AVERAGE REWARD R,0.7950263752825923,"i∈Bt,k"
THE ESTIMATED AVERAGE REWARD R,0.7957799547852299,"
Mz(m)
i,Tz −ψt(ai|si)⊤ht,k

ψ(m)
t
(ai|si)

2"
THE ESTIMATED AVERAGE REWARD R,0.7965335342878673,"(i)
≤
1
Nk X"
THE ESTIMATED AVERAGE REWARD R,0.7972871137905049,"i∈Bt,k M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.7980406932931424,"M
 
z(m)
i,Tz −zTz

ψ(m)
t
(ai|si)
2"
THE ESTIMATED AVERAGE REWARD R,0.79879427279578,"(ii)
≤
M 2C2
ψ
Nk X"
THE ESTIMATED AVERAGE REWARD R,0.7995478522984175,"i∈Bt,k M
X"
THE ESTIMATED AVERAGE REWARD R,0.8003014318010551,"m=1
(z(m)
i,Tz −zTz)2 ≤M 2C4
ψσ2Tz
W
ht,k
2.
(59)"
THE ESTIMATED AVERAGE REWARD R,0.8010550113036925,"where (i) uses the equality that ψt(ai|si)⊤ht,k = P"
THE ESTIMATED AVERAGE REWARD R,0.80180859080633,"m∈M z(m)
i,Tz = MzTz, (ii) uses the item 1 of
Lemma D.5."
THE ESTIMATED AVERAGE REWARD R,0.8025621703089676,"Since, ωt, ht,k ∈Ft,k while {si, ai}i∈Bt,k are random. Hence,"
THE ESTIMATED AVERAGE REWARD R,0.8033157498116051,"E
b∇fωt(ht,k) −∇fωt(ht,k)
2Ft,k
"
THE ESTIMATED AVERAGE REWARD R,0.8040693293142427,"= E
h 1 Nk X"
THE ESTIMATED AVERAGE REWARD R,0.8048229088168802,"i∈Bt,k"
THE ESTIMATED AVERAGE REWARD R,0.8055764883195177,"
ψt(ai|si)ψt(ai|si)⊤
ht,k −b∇J(ωt; Bt,k) −F(ωt)ht,k + ∇J(ωt)

2Ft,k
i"
THE ESTIMATED AVERAGE REWARD R,0.8063300678221552,"(i)
≤2E
h 1 Nk X"
THE ESTIMATED AVERAGE REWARD R,0.8070836473247928,"i∈Bt,k"
THE ESTIMATED AVERAGE REWARD R,0.8078372268274303,"
ψt(ai|si)ψt(ai|si)⊤
−F(ωt)

2
∥ht,k∥2Ft,k
i"
THE ESTIMATED AVERAGE REWARD R,0.8085908063300679,"+ 2E
b∇J(ωt; Bt,k) −∇J(ωt)
2Ft,k
"
THE ESTIMATED AVERAGE REWARD R,0.8093443858327054,"(ii)
= 2E
h 1 Nk X"
THE ESTIMATED AVERAGE REWARD R,0.8100979653353428,"i∈Bt,k"
THE ESTIMATED AVERAGE REWARD R,0.8108515448379804,"
ψt(ai|si)ψt(ai|si)⊤
−F(ωt)

2Ft,k
i
∥ht,k∥2"
THE ESTIMATED AVERAGE REWARD R,0.8116051243406179,"+ 2E
b∇J(ωt; Bt,k) −∇J(ωt)
2Ft,k
"
THE ESTIMATED AVERAGE REWARD R,0.8123587038432555,"(iii)
≤
18C4
ψ(κ + 1 −ρ)
Nk(1 −ρ)
∥ht,k∥2 + 2c4σ2T ′
W
+ 2c7"
THE ESTIMATED AVERAGE REWARD R,0.813112283345893,"Nk
+ 32C2
ψ M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.8138658628485306,"θ(m)
t
−θ∗
ωt
2 + 32C2
ψζcritic
approx, (60)"
THE ESTIMATED AVERAGE REWARD R,0.814619442351168,"where (i) uses the inequalities that ∥x + y∥2 ≤2∥x∥2 + 2∥y∥2 for any x, y ∈Rd, (ii) uses
the fact that ht,k ∈Ft,k, and (iii) uses eq. (40) and applies Lemma D.2 to the quantity that
X(s, a, s′, es) = ψt(a|s)ψt(a|s)⊤in which ωt ∈Ft,k is ﬁxed and ∥X(s, a, s′, es)∥F ≤C2
ψ."
THE ESTIMATED AVERAGE REWARD R,0.8153730218538056,Combining eqs. (59) & (60) yields that
THE ESTIMATED AVERAGE REWARD R,0.8161266013564431,"E
b∇fωt(ht,k) −∇fωt(ht,k)
2Ft,k
"
THE ESTIMATED AVERAGE REWARD R,0.8168801808590807,"≤2E
b∇fωt(ht,k) −e∇fωt(ht,k)
2Ft,k

+ 2E
e∇fωt(ht,k) −∇fωt(ht,k)
2Ft,k
"
THE ESTIMATED AVERAGE REWARD R,0.8176337603617182,"≤C4
ψ
h
2M 2σ2Tz
W
+ 36(κ + 1 −ρ)"
THE ESTIMATED AVERAGE REWARD R,0.8183873398643556,Nk(1 −ρ)
THE ESTIMATED AVERAGE REWARD R,0.8191409193669932,"i
∥ht,k∥2 + 4c4σ2T ′
W + 4c7"
THE ESTIMATED AVERAGE REWARD R,0.8198944988696307,"Nk
+ 64C2
ψ M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.8206480783722683,"θ(m)
t
−θ∗
ωt
2 + 64C2
ψζcritic
approx.
(61)"
THE ESTIMATED AVERAGE REWARD R,0.8214016578749058,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.8221552373775434,"Therefore,"
THE ESTIMATED AVERAGE REWARD R,0.8229088168801808,"E
ht,k+1 −h(ωt)
2Ft,k
"
THE ESTIMATED AVERAGE REWARD R,0.8236623963828184,"= E
ht,k −η b∇fωt(ht,k) −h(ωt)
2Ft,k
"
THE ESTIMATED AVERAGE REWARD R,0.8244159758854559,"(i)
≤(1 + ηλF )E
ht,k −η∇fωt(ht,k) −h(ωt)
2Ft,k
"
THE ESTIMATED AVERAGE REWARD R,0.8251695553880934,"+

1 + (ηλF )−1
E
η
b∇fωt(ht,k) −∇fωt(ht,k)
2Ft,k
"
THE ESTIMATED AVERAGE REWARD R,0.825923134890731,"(ii)
= (1 + ηλF )
ht,k −ηF(ωt)

ht,k −h(ωt)

−h(ωt)
2"
THE ESTIMATED AVERAGE REWARD R,0.8266767143933685,"+ η
 
η + λ−1
F

E
b∇fωt(ht,k) −∇fωt(ht,k)
2Ft,k
"
THE ESTIMATED AVERAGE REWARD R,0.827430293896006,"= (1 + ηλF )

I −ηF(ωt)

ht,k −h(ωt)
2"
THE ESTIMATED AVERAGE REWARD R,0.8281838733986435,"+ η
 
η + λ−1
F

E
[b∇fωt(ht,k) −∇fωt(ht,k)]
2Ft,k
"
THE ESTIMATED AVERAGE REWARD R,0.8289374529012811,"(iii)
≤(1 + ηλF )(1 −ηλF )2ht,k −h(ωt)
2 + 2η λF"
THE ESTIMATED AVERAGE REWARD R,0.8296910324039186,"
C4
ψ
h
2M 2σ2Tz
W
+ 36(κ + 1 −ρ)"
THE ESTIMATED AVERAGE REWARD R,0.8304446119065562,Nk(1 −ρ)
THE ESTIMATED AVERAGE REWARD R,0.8311981914091937,"i
∥ht,k∥2 + 4c4σ2T ′
W + 4c7"
THE ESTIMATED AVERAGE REWARD R,0.8319517709118311,"Nk
+ 64C2
ψ M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.8327053504144687,"θ(m)
t
−θ∗
ωt
2 + 64C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.8334589299171062,"≤(1 −ηλF )
ht,k −h(ωt)
2 + 2η λF"
THE ESTIMATED AVERAGE REWARD R,0.8342125094197438,"
2C4
ψ
h
2M 2σ2Tz
W
+ 36(κ + 1 −ρ)"
THE ESTIMATED AVERAGE REWARD R,0.8349660889223813,Nk(1 −ρ)
THE ESTIMATED AVERAGE REWARD R,0.8357196684250189,"i
(∥ht,k −h(ωt)∥2 + ∥h(ωt)∥2)"
THE ESTIMATED AVERAGE REWARD R,0.8364732479276563,"+ 4c4σ2T ′
W
+ 4c7"
THE ESTIMATED AVERAGE REWARD R,0.8372268274302939,"Nk
+ 64C2
ψ M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.8379804069329314,"θ(m)
t
−θ∗
ωt
2 + 64C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.838733986435569,"(iv)
≤

1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.8394875659382065,"ht,k −h(ωt)
2 + 2η λF"
THE ESTIMATED AVERAGE REWARD R,0.840241145440844,"
2C4
ψ
h
2M 2σ2Tz
W
+ 36(κ + 1 −ρ)"
THE ESTIMATED AVERAGE REWARD R,0.8409947249434815,Nk(1 −ρ)
THE ESTIMATED AVERAGE REWARD R,0.841748304446119,"iD2
J
λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.8425018839487566,"+ 4c4σ2T ′
W
+ 4c7"
THE ESTIMATED AVERAGE REWARD R,0.8432554634513941,"Nk
+ 64C2
ψ M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.8440090429540317,"θ(m)
t
−θ∗
ωt
2 + 64C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.8447626224566692,"(v)
≤

1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.8455162019593067,"ht,k −h(ωt)
2 + 8η λF"
THE ESTIMATED AVERAGE REWARD R,0.8462697814619442,"
C4
ψM 2σ2Tz
W
+ c9 Nk"
THE ESTIMATED AVERAGE REWARD R,0.8470233609645818,"+ c4σ2T ′
W
+ 16C2
ψ M
X m=1"
THE ESTIMATED AVERAGE REWARD R,0.8477769404672193,"θ(m)
t
−θ∗
ωt
2 + 16C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.8485305199698568,"where (i) uses the inequality that ∥x+y∥2 ≤(1+ηλF )∥x∥2 +[1+(ηλF )−1]∥y∥2 for any x, y ∈Rd,
(ii) uses the notation that ∇fωt(h) = F(ωt)h −∇J(ωt) = F(ωt)[h −h(ωt)] and the fact that
ωt, ht,k ∈Ft,k, (iii) uses eq. (61) and the item 2 of this Lemma, (iv) uses the conditions that Tz ≥
ln(3DJC2
ψ)"
THE ESTIMATED AVERAGE REWARD R,0.8492840994724944,"ln(σ−1
W )
and the item 7 of this Lemma, and (v) uses the notation that c9 :=
18C4
ψD2
J(κ+1−ρ)
λ2
F (1−ρ)
+ c7."
THE ESTIMATED AVERAGE REWARD R,0.8500376789751318,"Then, taking unconditional expectation of the above inequality and iterating it over k = 0, 1, . . . , K−1
yield that"
THE ESTIMATED AVERAGE REWARD R,0.8507912584777694,"E
ht −h(ωt)
2
= E
ht,K −h(ωt)
2"
THE ESTIMATED AVERAGE REWARD R,0.8515448379804069,"≤

1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.8522984174830445,"K
E
ht,0 −h(ωt)
2
+ 8η λF K−1
X k=0"
THE ESTIMATED AVERAGE REWARD R,0.853051996985682,"
1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.8538055764883196,K−1−k
THE ESTIMATED AVERAGE REWARD R,0.854559155990957,"
C4
ψM 2σ2Tz
W
+ c9"
THE ESTIMATED AVERAGE REWARD R,0.8553127354935945,"Nk
+ c4σ2T ′
W
+ 16C2
ψ M
X"
THE ESTIMATED AVERAGE REWARD R,0.8560663149962321,"m=1
E
θ(m)
t
−θ∗
ωt
2
+ 16C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.8568198944988696,"(i)
≤

1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.8575734740015072,"K
E
ht−1 −h(ωt)
2"
THE ESTIMATED AVERAGE REWARD R,0.8583270535041447,"Under review as a conference paper at ICLR 2022 + 16 λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.8590806330067822,"
C4
ψM 2σ2Tz
W
+ c4σ2T ′
W
+ 16C2
ψ M
X"
THE ESTIMATED AVERAGE REWARD R,0.8598342125094197,"m=1
E
θ(m)
t
−θ∗
ωt
2
+ 16C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.8605877920120573,+ 8ηc9[1 −(1 −ηλF /2)K/2]
THE ESTIMATED AVERAGE REWARD R,0.8613413715146948,"NλF (1 −
p"
THE ESTIMATED AVERAGE REWARD R,0.8620949510173324,"1 −ηλF /2) K−1
X k=0"
THE ESTIMATED AVERAGE REWARD R,0.8628485305199699,"
1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.8636021100226073,(K−1−k)/2
THE ESTIMATED AVERAGE REWARD R,0.8643556895252449,"(ii)
≤

1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.8651092690278824,"K
E
ht−1 −h(ωt)
2
+ 16 λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.86586284853052," 
C4
ψM 2σ2Tz
W
+ c4σ2T ′
W
+ 16C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.8666164280331575,"+
256C2
ψ
λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.8673700075357951,"
σ2T ′
c
W β2c2 + 2M
h
c3

1 −λB"
THE ESTIMATED AVERAGE REWARD R,0.8681235870384325,"8 β
Tc
+ c1 Nc"
THE ESTIMATED AVERAGE REWARD R,0.8688771665410701,"i
+
8ηc9
NλF (1 −
p"
THE ESTIMATED AVERAGE REWARD R,0.8696307460437076,1 −ηλF /2)2
THE ESTIMATED AVERAGE REWARD R,0.8703843255463452,"(iii)
≤

1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.8711379050489827,"K
E
ht−1 −h(ωt)
2
+ 16 λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.8718914845516202," 
C4
ψM 2σ2Tz
W
+ c4σ2T ′
W
+ 16C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.8726450640542577,"+
256C2
ψ
λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.8733986435568952,"
σ2T ′
c
W β2c2 + 2M
h
c3

1 −λB"
THE ESTIMATED AVERAGE REWARD R,0.8741522230595328,"8 β
Tc
+ c1 Nc"
THE ESTIMATED AVERAGE REWARD R,0.8749058025621703,"i
+ 128c9"
THE ESTIMATED AVERAGE REWARD R,0.8756593820648079,"Nηλ3
F
(62)"
THE ESTIMATED AVERAGE REWARD R,0.8764129615674454,"(iv)
≤3

1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.8771665410700829,"K
E
ht−1 −h(ωt−1)
2 +
h(ωt−1)
2 +
 −h(ωt)
2 + 16 λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.8779201205727204," 
C4
ψM 2σ2Tz
W
+ c4σ2T ′
W
+ 16C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.8786737000753579,"+
256C2
ψ
λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.8794272795779955,"
σ2T ′
c
W β2c2 + 2M
h
c3

1 −λB"
THE ESTIMATED AVERAGE REWARD R,0.880180859080633,"8 β
Tc
+ c1 Nc"
THE ESTIMATED AVERAGE REWARD R,0.8809344385832706,"i
+ 128c9"
THE ESTIMATED AVERAGE REWARD R,0.881688018085908,"Nηλ3
F
(v)
≤3

1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.8824415975885456,"K
E
ht−1 −h(ωt−1)
2
+ 6D2
J
λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.8831951770911831,"
1 −ηλF 2 K + 16 λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.8839487565938207," 
C4
ψM 2σ2Tz
W
+ c4σ2T ′
W
+ 16C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.8847023360964582,"+
256C2
ψ
λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.8854559155990958,"
σ2T ′
c
W β2c2 + 2M
h
c3

1 −λB"
THE ESTIMATED AVERAGE REWARD R,0.8862094951017332,"8 β
Tc
+ c1 Nc"
THE ESTIMATED AVERAGE REWARD R,0.8869630746043707,"i
+ 128c9"
THE ESTIMATED AVERAGE REWARD R,0.8877166541070083,"Nηλ3
F
,"
THE ESTIMATED AVERAGE REWARD R,0.8884702336096458,"where (i) uses the notation that ht,0
= ht, the item 7 of this Lemma and the inequality
that PK−1
k=0
 
1 −ηλF"
THE ESTIMATED AVERAGE REWARD R,0.8892238131122834,"2
K−1−k
≤
2
ηλF , (ii) uses Lemma D.4, (iii) uses the inequality that"
THE ESTIMATED AVERAGE REWARD R,0.8899773926149209,"1
(1−√"
THE ESTIMATED AVERAGE REWARD R,0.8907309721175584,"1−ηλF /2)2 =
(1+√"
THE ESTIMATED AVERAGE REWARD R,0.8914845516201959,1−ηλF /2)2
THE ESTIMATED AVERAGE REWARD R,0.8922381311228335,"(ηλF /2)2
≤
16
(ηλF )2 implied by the item 2 of this Lemma, (iv) uses"
THE ESTIMATED AVERAGE REWARD R,0.892991710625471,"the inequality that ∥x + y + z∥2 ≤3∥x∥2 + 3∥y∥2 + 3∥z∥2, ∀x, y, z ∈Rd, and (v) uses the items 4
of this Lemma. Taking unconditional expectation of the above inequality and iterating it over t yield
that
E
ht −h(ωt)
2"
THE ESTIMATED AVERAGE REWARD R,0.8937452901281085,"(i)
≤
h
3

1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.8944988696307461,"Kit
E
h0 −h(ω0)
2
+ 12D2
J
λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.8952524491333835,"
1 −ηλF 2 K + 32 λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.8960060286360211," 
C4
ψM 2σ2Tz
W
+ c4σ2T ′
W
+ 16C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.8967596081386586,"+
512C2
ψ
λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.8975131876412962,"
σ2T ′
c
W β2c2 + 2M
h
c3

1 −λB"
THE ESTIMATED AVERAGE REWARD R,0.8982667671439337,"8 β
Tc
+ c1 Nc"
THE ESTIMATED AVERAGE REWARD R,0.8990203466465713,"i
+ 256c9"
THE ESTIMATED AVERAGE REWARD R,0.8997739261492087,"Nηλ3
F
(ii)
≤
h
3

1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.9005275056518462,"Kith
1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.9012810851544838,"K
E
h−1 −h(ω0)
2 + 16 λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.9020346646571213," 
C4
ψM 2σ2Tz
W
+ c4σ2T ′
W
+ 16C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.9027882441597589,"+
256C2
ψ
λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.9035418236623964,"
σ2T ′
c
W β2c2 + 2M
h
c3

1 −λB"
THE ESTIMATED AVERAGE REWARD R,0.9042954031650339,"8 β
Tc
+ c1 Nc"
THE ESTIMATED AVERAGE REWARD R,0.9050489826676714,"i
+ 128c9"
THE ESTIMATED AVERAGE REWARD R,0.905802562170309,"Nηλ3
F i"
THE ESTIMATED AVERAGE REWARD R,0.9065561416729465,"+ 12D2
J
λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.9073097211755841,"
1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.9080633006782216,"K
+ 32 λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.908816880180859," 
C4
ψM 2σ2Tz
W
+ c4σ2T ′
W
+ 16C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.9095704596834966,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.9103240391861341,"+
512C2
ψ
λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.9110776186887717,"
σ2T ′
c
W β2c2 + 2M
h
c3

1 −λB"
THE ESTIMATED AVERAGE REWARD R,0.9118311981914092,"8 β
Tc
+ c1 Nc"
THE ESTIMATED AVERAGE REWARD R,0.9125847776940467,"i
+ 256c9"
THE ESTIMATED AVERAGE REWARD R,0.9133383571966842,"Nηλ3
F
(iii)
≤2

1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.9140919366993218,"Kh−1
2 + D2
J
λ2
F "
THE ESTIMATED AVERAGE REWARD R,0.9148455162019593,"+ 12D2
J
λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.9155990957045969,"
1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.9163526752072344,"K
+ 48 λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.9171062547098718," 
C4
ψM 2σ2Tz
W
+ c4σ2T ′
W
+ 16C2
ψζcritic
approx
"
THE ESTIMATED AVERAGE REWARD R,0.9178598342125094,"+
768C2
ψ
λ2
F"
THE ESTIMATED AVERAGE REWARD R,0.9186134137151469,"
σ2T ′
c
W β2c2 + 2M
h
c3

1 −λB"
THE ESTIMATED AVERAGE REWARD R,0.9193669932177845,"8 β
Tc
+ c1 Nc i"
THE ESTIMATED AVERAGE REWARD R,0.920120572720422,"+ 384c9 ηλ3
F"
THE ESTIMATED AVERAGE REWARD R,0.9208741522230596,"ηλ5
F (1 −ρ)(1 −ηλF /2)(K−1)/2"
THE ESTIMATED AVERAGE REWARD R,0.921627731725697,"2304C4
ψ(κ + 1 −ρ)"
THE ESTIMATED AVERAGE REWARD R,0.9223813112283346,"(iv)
≤c10

1 −ηλF 2"
THE ESTIMATED AVERAGE REWARD R,0.9231348907309721,"(K−1)/2
+ c11σ2Tz
W
+ c12σ2T ′
W
+ c13β2σ2T ′
c
W"
THE ESTIMATED AVERAGE REWARD R,0.9238884702336096,"+ c14

1 −λB"
THE ESTIMATED AVERAGE REWARD R,0.9246420497362472,"8 β
Tc
+ c15"
THE ESTIMATED AVERAGE REWARD R,0.9253956292388847,"Nc
+ c16ζcritic
approx"
THE ESTIMATED AVERAGE REWARD R,0.9261492087415222,"where (i) uses the inequality that 3(1 −ηλF /2)K
≤
1 implied by the condition that
K
≥
ln 3
ln[(1−ηλF /2)−1], (ii) uses eq.
(62) with t = 0, (iii) uses the condition that N
≥"
THE ESTIMATED AVERAGE REWARD R,0.9269027882441597,"2304C4
ψ(κ+1−ρ)
ηλ5
F (1−ρ)(1−ηλF /2)(K−1)/2 as well as the inequalities that
h−1 −h(ω0)
2
≤
2
h−1
2 +"
THE ESTIMATED AVERAGE REWARD R,0.9276563677467973,"2
h(ω0)
2 ∗
≤2
h−1
2 + 2D2
Jλ−2
F
(* uses the item 4 of this Lemma) and that 3(1 −ηλF /2)K ≤1,"
THE ESTIMATED AVERAGE REWARD R,0.9284099472494348,"(iv) denotes that c10 := 2∥h−1∥2 + 14D2
J
λ2
F
+ c9λ2
F
C4
ψ , c11 :=
48C4
ψM 2"
THE ESTIMATED AVERAGE REWARD R,0.9291635267520724,"λ2
F
, c12 := 48c4"
THE ESTIMATED AVERAGE REWARD R,0.9299171062547099,"λ2
F , c13 :=
768c2C2
ψ
λ2
F
,"
THE ESTIMATED AVERAGE REWARD R,0.9306706857573473,"c14 :=
1536Mc3C2
ψ
λ2
F
, c15 :=
1536Mc1C2
ψ
λ2
F
, c16 :=
768C2
ψ
λ2
F
. This proves the item 8 of this Lemma."
THE ESTIMATED AVERAGE REWARD R,0.9314242652599849,"E
EXPERIMENT SETUP AND ADDITIONAL RESULTS"
THE ESTIMATED AVERAGE REWARD R,0.9321778447626224,"In this section, we present all the details of experiment setup and additional experiment results."
THE ESTIMATED AVERAGE REWARD R,0.93293142426526,"E.1
EXPERIMENT SETUP"
THE ESTIMATED AVERAGE REWARD R,0.9336850037678975,"We simulate a fully decentralized ring network with 6 fully decentralized agents, using communication
matrix with diagonal entries 0.4 and off-diagnonal entries 0.3. The shared state space contains 5
states and each agent can take 2 actions. We adopt the softmax policy πω(a|s) ∝eωs,a. The entries
of the transition kernel and the reward functions are independently generated from the standard
Gaussian distribution (with proper normalization of the absolute value for the transition kernel). We
use the rows of a 5-dimensional identity matrix as state features. We set the discount factor γ = 0.95."
THE ESTIMATED AVERAGE REWARD R,0.9344385832705351,"We implement and compare four decentralized AC-type algorithms in this multi-agent MDP: our
decentralized AC in Algorithm 1, our decentralized NAC in Algorithm 3, an existing decentralized
AC algorithm (Algorithm 2 of (70)) that uses a linear model to parameterize the agents’ averaged
reward R(s, a, s′) = P"
THE ESTIMATED AVERAGE REWARD R,0.9351921627731725,"i λifi(s, a, s′) (we name it DAC-RP1 for decentralized AC with reward
parameterization) 5, and our proposed modiﬁed version of DAC-RP1 to incorporate minibatch,
which we refer to as DAC-RP100 with batch size N = 100. For our Algorithm 1, we choose
T = 500, Tc = 50, T ′
c = 10, Nc = 10, T ′ = Tz = 5, β = 0.5, {σm}6
m=1 = 0.1, and consider
batch size choices N = 100, 500, 2000. Algorithm 3 uses the same hyperparameters as those of
Algorithm 1 except that T = 2000 in Algorithm 3. We select α = 10, 50, 200 for Algorithm 1 with
N = 100, 500, 2000 respectively, and Tz = 5, α = 0.1, 0.5, 2, η = 0.04, 0.2, 0.8, K = 50, 100, 200,
Nk ≡2, 5, 10 for Algorithm 3 with N = 100, 500, 2000, respectively. For DAC-RP1 that was
originally designed for discount factor γ = 1, we slightly adjust it to ﬁt our setting where 0 < γ < 16."
THE ESTIMATED AVERAGE REWARD R,0.9359457422758101,"5The original algorithm in (70) uses the parameterization R(s, a) = P"
THE ESTIMATED AVERAGE REWARD R,0.9366993217784476,"i λifi(s, a), and we extend to our
setting where the rewards also depend on the next state s′.
6(70) deﬁned the Q-function Qθ(s, a) = E
"
THE ESTIMATED AVERAGE REWARD R,0.9374529012810852,"rt+1 −J(θ)
 for policy parameter θ and used the temporal
differences δi
t = ri
t+1 −µi
t + Vt+1(vi
t) −Vt(vi
t) and eδi
t = Rt(λi
t) −µi
t + Vt+1(vi
t) −Vt(vi
t) for critic"
THE ESTIMATED AVERAGE REWARD R,0.9382064807837227,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.9389600602863603,Figure 2: Comparison of ∥∇J(ωt)∥2 among decentralized AC-type algorithms for ring network .
THE ESTIMATED AVERAGE REWARD R,0.9397136397889977,"For this adjusted DAC-RP1, we select diminishing stepsizes βθ = 2(t + 1)−0.9, βv = 5(t + 1)−0.8
as recommended in (70) and use the rows of a 1600-dimensional identity matrix as the reward
features {fi(s, a, s′) : s, s′ ∈S, a ∈A} (i = 1, 2, . . . , 1600) to fully express R(s, a, s′) over all the
5 × 26 × 5 = 1600 triplets (s, a, s′). DAC-RP100 has batchsizes 100 and 10 for actor and critic
updates respectively, and selects constant stepsizes βv = 0.5, βθ = 10. This setting is similar to
Algorithm 1 with N = 100 to inspect the reason of performance difference between Algorithm 1 and
DAC-RP1. All the algorithms are repeated 10 times using initial state 0 and the same initial actor
parameter ω0 generated from standard Gaussian distribution."
THE ESTIMATED AVERAGE REWARD R,0.9404672192916352,"E.2
GRADIENT NORM CONVERGENCE RESULTS IN RING NETWORK"
THE ESTIMATED AVERAGE REWARD R,0.9412207987942728,"Figure 2 plots ∥∇J(ωt)∥2 v.s. communication complexity (t(Tc + Tc + T ′) = 65t, t(Tc + Tc +
T ′ + Tz) = 70t and 2t for Algorithms 1 & 3, and both DAC-RP algorithms, respectively)7 and
sample complexity (t(TcNc + N), 2t and 110t for both of our AC-type algorithms, DAC-RP1 and
DAC-RP100, respectively).8 For each curve, its upper and lower envelopes denote the 95% and 5%
percentiles of the 10 repetitions, respectively."
THE ESTIMATED AVERAGE REWARD R,0.9419743782969103,"Similar to the result of accumulative reward J(ωt) shown in Figure 1, it can be seen from Figure 2 that
the communication and sample efﬁciency of both our decentralized AC and NAC algorithms improve
with larger batchsize due to reduced gradient variance, which matches our understanding in Theorems
1 & 2. Our decentralized AC and NAC algorithms signiﬁcantly outperform DAC-RP1 which has
batchsize 1. Using mini-batch, DAC-RP100 outperforms a lot than DAC-RP1, and converges to
critical points earlier than Algorithm 1. However, it can be seen from Figure 1 that such early
convergence turns out to have much lower J(ωt) than Algorithm 1 with N = 100 and Nc = 10.
Such a performance gap is caused by two reasons: (i) Both DAC-RP1 and DAC-RP100 suffer from
an inaccurate parameterized estimation of the averaged reward, and the mean relative estimation
errors of both DAC-RP1 and DAC-RP100 are over 100% 9. In contrast, our noisy averaged reward
estimation achieves a mean relative error in the range of 10−5 ∼10−4. 10 ; (ii) Both DAC-RP1"
THE ESTIMATED AVERAGE REWARD R,0.9427279577995479,"update and actor update respectively. To ﬁt 0 < γ < 1, we use δi
t = ri
t+1 + γVt+1(vi
t) −Vt(vi
t) and
eδi
t = Rt(λi
t) + γVt+1(vi
t) −Vt(vi
t) where µi
t ≈J(θt) is removed since Qθ(s, a) = E(rt+1). In addition, we
used two different chains generated from transition kernels P, Pξ respectively for critic update and actor update
as in our Algorithm 1.
7Each update of our decentralized AC uses Tc + T ′
c and T ′ communication rounds for synchronizing critic
model and rewards, respectively. Each update of our decentralized NAC uses Tc + T ′
c, T ′, T ′
z communication
rounds for synchronizing critic model, rewards and scalar z, respectively. Each update of both DAC-RP1 and
DAC-RP100 uses 1 communication round for synchronizing v and λ respectively.
8DAC-RP1 uses 1 sample for actor and critic updates respectively. DAC-RP100 uses 100 and 10 samples for
actor and critic updates respectively.
9The relative reward estimation error at the t-th iteration of both DAC-RP1 and DAC-RP100 is de-
ﬁned as A/B where A =
1
M|S|2|A|
PM
m=1
P"
THE ESTIMATED AVERAGE REWARD R,0.9434815373021854,"s,s′∈S
P"
THE ESTIMATED AVERAGE REWARD R,0.9442351168048229,"a∈A[R(s, a, s′) −P"
THE ESTIMATED AVERAGE REWARD R,0.9449886963074604,"i λ(m)
i
fi(s, a, s′)]2 and B ="
THE ESTIMATED AVERAGE REWARD R,0.945742275810098,"1
|S|2|A|
P"
THE ESTIMATED AVERAGE REWARD R,0.9464958553127355,"s,s′∈S
P"
THE ESTIMATED AVERAGE REWARD R,0.947249434815373,"a∈A R(s, a, s′)2."
THE ESTIMATED AVERAGE REWARD R,0.9480030143180106,"10At the t-th iteration of Algorithms 1 & 3, we focus on r(m)
t
=
1
N
P(t+1)N−1
i=tN
R
(m)
i
as the estimation of
the batch-averaged reward rt =
1
N
P(t+1)N−1
i=tN
Ri since its estimation error affects the accuracy of the policy
gradient (4). The relative estimation error is deﬁned as
1
Mr2
t
PM
m=1(r(m)
t
−rt)2."
THE ESTIMATED AVERAGE REWARD R,0.948756593820648,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.9495101733232856,"and DAC-RP100 apply only a single TD update per-round, and hence suffers from a larger mean
TD learning error (about 2% and 1% for DAC-RP1 and DAC-RP100, respectively), whereas our
algorithms perform multiple TD learning updates per-round and achieve a smaller mean relative error
(about 0.3% and 0.07% for our decentralized AC and NAC respectively) 11. All these relative errors
are averaged over iterations."
THE ESTIMATED AVERAGE REWARD R,0.9502637528259231,"E.3
ADDITIONAL EXPERIMENTS IN FULLY CONNECTED NETWORK"
THE ESTIMATED AVERAGE REWARD R,0.9510173323285607,"To investigate the effect of network topology on the performance of our algorithms, we also conduct
the above experiments on a fully connected network with 6 fully decentralized agents, using commu-
nication matrix with diagonal entries 0.4 and all the other entries 0.12. The MDP environment and all
the hyperparameters are the same as the above experiments for ring network. Figures 3 & 4 plot the
learning curves of the optimality gap J∗−J(ωt) and ∥∇J(ωt)∥2 respectively for fully connected
network. To make comparison, we plot J∗−J(ωt) and ∥∇J(ωt)∥in Figures 5 & 2 respectively for
the above experiments with ring network. It can be seen by comparing these ﬁgures that network
topology does not much affect the performance of these algorithms, so the conclusions for ring
network that we summarized right before this subsection also holds for fully connected network."
THE ESTIMATED AVERAGE REWARD R,0.9517709118311982,"Figure 3: Comparison of optimality gap J(ω∗) −J(ωt) among decentralized AC-type algorithms in
fully connected network."
THE ESTIMATED AVERAGE REWARD R,0.9525244913338358,"Figure 4: Comparison of ∥∇J(ωt)∥2 among decentralized AC-type algorithms in fully connected
network."
THE ESTIMATED AVERAGE REWARD R,0.9532780708364732,"Figure 5: Comparison of optimality gap J(ω∗) −J(ωt) among decentralized AC-type algorithms in
ring network."
THE ESTIMATED AVERAGE REWARD R,0.9540316503391107,"11The TD error at the t-th iteration is deﬁned as
1
M∥θ∗ωt ∥2
PM
m=1 ∥θ(m)
t
−θ∗
ωt∥2."
THE ESTIMATED AVERAGE REWARD R,0.9547852298417483,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.9555388093443858,"E.4
TWO-AGENT CLIFF NAVIGATION"
THE ESTIMATED AVERAGE REWARD R,0.9562923888470234,"Figure 6: Two-agent cliff navigation. (“S”, “X”,
“D” denote starting point, cliff and destination re-
spectively. The optimal path is shown in red.)"
THE ESTIMATED AVERAGE REWARD R,0.9570459683496609,"In this subsection, we test our algorithms in solv-
ing a two-agent Cliff Navigation problem (39)
in a grid-world environment. This problem is
adapted from its single-agent version (see Ex-
ample 6.6 of (48)). As illustrated in Figure 6,
two agents start from the starting point “S” on
a 3 × 4 grid and aim to reach the destination
“D”. Here, global state is deﬁned as the joint lo-
cation of the two agents, and there are in total
(3 × 4)2 = 144 global states. In most states,
an agent can choose to move up, down, left or
right by one step and receives −1 reward. How-
ever, once an agent falls into the cliff “X”, it
will return to the starting point “S” and receives
−100 reward. When an agent reaches “D”, it
will always stay at “D”, and receives 0 reward
if the other agent also reaches/stays at “D”, or
receives −0.5 reward otherwise. If an agent is not at “X” or “D” and selects a direction that points
outside the grid, then it stays in the previous location and receives −1 reward. The optimal path
for both agents is the red path shown in Figure 6, which has the minimum accumulative reward
J∗= −0.1855 under the discount factor γ = 0.95."
THE ESTIMATED AVERAGE REWARD R,0.9577995478522984,"For our Algorithm 1, we choose T = 500, Tc = 50, T ′
c = 10, Nc = 10, T ′ = Tz = 5, β = 0.5,
{σm}6
m=1 = 0.1, and consider batch size choices N = 100, 500, 2000. Our Algorithm 3 uses the
same hyperparameters as those of Algorithm 1 except that we choose T = 2000. We select α =
1, 5, 20 for Algorithm 1 with N = 100, 500, 2000 respectively, and Tz = 5, α = 0.002, 0.01, 0.04,
η = 0.002, 0.01, 0.04, K = 50, 100, 200, Nk ≡2, 5, 10 for Algorithm 3 with N = 100, 500, 2000,
respectively. For DAC-RP1, we select T = 10000, βv = 10(t + 1)−0.6 and βθ = 5(t + 1)−0.6. For
DAC-RP100, we use T = 2000 and batchsizes 100 and 10 for actor and critic updates respectively,
and selects constant stepsizes βv = 0.5, βθ = 1. This setting is similar to Algorithm 1 with N = 100
to inspect performance difference between Algorithm 1 and DAC-RP1."
THE ESTIMATED AVERAGE REWARD R,0.9585531273549359,"Figure 7: Comparison of optimality gap J(ω∗) −J(ωt) among decentralized AC-type algorithms on
cliff navigation."
THE ESTIMATED AVERAGE REWARD R,0.9593067068575735,"Figure 8: Comparison of optimality gap J(ω∗) −J(ωt) among decentralized AC-type algorithms on
cliff navigation."
THE ESTIMATED AVERAGE REWARD R,0.960060286360211,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.9608138658628486,"We plot J∗−J(ωt) and ∥∇J(ωt)∥in Figures 7 & 8 respectively. It can be seen from these ﬁgures
that both our Algorithm 1 & Algorithm 3 signiﬁcantly reduce the function value gap J∗−J(ωt), and
their convergence is faster with a larger batchsize. In contrast, the function value gaps of DAC-RP1
and DAC-RP100 do not decrease sufﬁciently and converge to a high value. In particular, since
DAC-RP100 achieves a larger function value gap than our Algorithm 1 with N = 100 while their
hyperparameter choices are similar, we attribute this performance gap to the inaccurate average
reward estimation and TD error, as we analyzed in Appendix E.2."
THE ESTIMATED AVERAGE REWARD R,0.9615674453654861,"F
CONSTANT SCALARS"
THE ESTIMATED AVERAGE REWARD R,0.9623210248681235,The following global constants are frequently used.
THE ESTIMATED AVERAGE REWARD R,0.9630746043707611,M: The number of agents.
THE ESTIMATED AVERAGE REWARD R,0.9638281838733986,γ: Discount rate.
THE ESTIMATED AVERAGE REWARD R,0.9645817633760362,"Rmax: The reward bound such that 0 ≤R(m)(s, a, s′) ≤Rmax for any s, s′ ∈S and a ∈A"
THE ESTIMATED AVERAGE REWARD R,0.9653353428786737,"(Assumption 3). Hence, 0 ≤R
(m)(s, a, s′), R(m)
i
, Ri ≤Rmax."
THE ESTIMATED AVERAGE REWARD R,0.9660889223813113,"σW ∈[0, 1): The second largest singular value of W."
THE ESTIMATED AVERAGE REWARD R,0.9668425018839487,ω∗:= maxω J(ω) denotes the optimal policy parameter.
THE ESTIMATED AVERAGE REWARD R,0.9675960813865863,The following constants are deﬁned in Lemma D.3.
THE ESTIMATED AVERAGE REWARD R,0.9683496608892238,CB := 1 + γ.
THE ESTIMATED AVERAGE REWARD R,0.9691032403918614,Cb := Rmax.
THE ESTIMATED AVERAGE REWARD R,0.9698568198944989,"λφ := λmin
 
Es∼µω[φ(s)φ(s)⊤]

> 0 satisﬁes Assumption 4."
THE ESTIMATED AVERAGE REWARD R,0.9706103993971364,λB := 2(1 −γ)λφ > 0. (Assumption 4 implies that λφ > 0.)
THE ESTIMATED AVERAGE REWARD R,0.9713639788997739,Rθ := 2Cb λB .
THE ESTIMATED AVERAGE REWARD R,0.9721175584024114,The policy-related norm bounds and Lipschitz parameters are deﬁned as follows.
THE ESTIMATED AVERAGE REWARD R,0.972871137905049,"Cψ, Lψ, Lπ > 0 deﬁned in Assumption 2: For all s ∈S, a ∈A and ω, eω, ∥ψω(a|s)∥≤Cψ,
∥ψeω(a|s) −ψω(a|s)∥≤Lψ∥eω −ω∥and dTV
 
πeω(·|s), πω(·|s)

≤Lπ∥eω −ω∥."
THE ESTIMATED AVERAGE REWARD R,0.9736247174076865,Lν := Lπ[1 + logρ(κ−1) + (1 −ρ)−1].
THE ESTIMATED AVERAGE REWARD R,0.9743782969103241,"LQ := 2RmaxLν 1−γ
."
THE ESTIMATED AVERAGE REWARD R,0.9751318764129616,LJ := Rmax(4Lν + Lψ)/(1 −γ).
THE ESTIMATED AVERAGE REWARD R,0.975885455915599,"DJ := CψRmax 1−γ
."
THE ESTIMATED AVERAGE REWARD R,0.9766390354182366,LF := 2Cψ(LπCψ + LνCψ + Lψ).
THE ESTIMATED AVERAGE REWARD R,0.9773926149208741,"Lh := 2λ−1
F (DJλ−1
F LF + LJ) where λF := infω∈Ωλmin[F(ω)] > 0 (λmin denotes the minimum
eigenvalue) which satisﬁes Assumption 6."
THE ESTIMATED AVERAGE REWARD R,0.9781461944235117,The following constants are deﬁned to simplify the notations in the proof.
THE ESTIMATED AVERAGE REWARD R,0.9788997739261492,"c1 :=
1920(C2
BR2
θ+C2
b)[1+(κ−1)ρ]
(1−ρ)λ2
B
."
THE ESTIMATED AVERAGE REWARD R,0.9796533534287868,"c2 := 2
  2MCb"
THE ESTIMATED AVERAGE REWARD R,0.9804069329314242,"1−σW
2."
THE ESTIMATED AVERAGE REWARD R,0.9811605124340618,"c3 := 2
 θ−1
2 + R2
θ

where θ−1 is the initial parameter of decentralized TD (Algorithm 2)."
THE ESTIMATED AVERAGE REWARD R,0.9819140919366993,"c4 := 4MC2
ψR2
max."
THE ESTIMATED AVERAGE REWARD R,0.9826676714393369,Under review as a conference paper at ICLR 2022
THE ESTIMATED AVERAGE REWARD R,0.9834212509419744,"c5 := 16c2C2
ψ."
THE ESTIMATED AVERAGE REWARD R,0.984174830444612,"c6 := 32Mc3C2
ψ."
THE ESTIMATED AVERAGE REWARD R,0.9849284099472494,"c7 := 16C2
ψR2
maxσ2 +
36C2
ψ(Rmax+2Rθ)2(κ+1−ρ) 1−ρ
."
THE ESTIMATED AVERAGE REWARD R,0.9856819894498869,"c8 := 32Mc1C2
ψ."
THE ESTIMATED AVERAGE REWARD R,0.9864355689525245,"c9 :=
18C4
ψD2
J(κ+1−ρ)
λ2
F (1−ρ)
+ c7."
THE ESTIMATED AVERAGE REWARD R,0.987189148455162,"c10 := 2∥h−1∥2 + 14D2
J
λ2
F
+ c9λ2
F
C4
ψ where h−1 is the initial natural gradient of Algorithm 3."
THE ESTIMATED AVERAGE REWARD R,0.9879427279577996,"c11 :=
48C4
ψM 2"
THE ESTIMATED AVERAGE REWARD R,0.9886963074604371,"λ2
F
."
THE ESTIMATED AVERAGE REWARD R,0.9894498869630746,c12 := 48c4
THE ESTIMATED AVERAGE REWARD R,0.9902034664657121,"λ2
F ."
THE ESTIMATED AVERAGE REWARD R,0.9909570459683497,"c13 :=
768c2C2
ψ
λ2
F
."
THE ESTIMATED AVERAGE REWARD R,0.9917106254709872,"c14 :=
1536Mc3C2
ψ
λ2
F
."
THE ESTIMATED AVERAGE REWARD R,0.9924642049736248,"c15 :=
1536Mc1C2
ψ
λ2
F
."
THE ESTIMATED AVERAGE REWARD R,0.9932177844762623,"c16 :=
768C2
ψ
λ2
F
."
THE ESTIMATED AVERAGE REWARD R,0.9939713639788997,"c17 := Es∼νω∗

KL
 
πω∗(·|s)||π0(·|s)

+
4LψC2
ψRmax
λ2
F
."
THE ESTIMATED AVERAGE REWARD R,0.9947249434815373,"c18 := Cψ√c10 + c10Lψ

1 +
4C4
ψ
λ2
F 
."
THE ESTIMATED AVERAGE REWARD R,0.9954785229841748,"c19 := Cψ√c11 + c11Lψ

1 +
4C4
ψ
λ2
F 
."
THE ESTIMATED AVERAGE REWARD R,0.9962321024868124,"c20 := Cψ√c12 + c12Lψ

1 +
4C4
ψ
λ2
F 
."
THE ESTIMATED AVERAGE REWARD R,0.9969856819894499,"c21 := Cψ√c13 + c13Lψ

1 +
4C4
ψ
λ2
F 
."
THE ESTIMATED AVERAGE REWARD R,0.9977392614920875,"c22 := Cψ√c14 + c14Lψ

1 +
4C4
ψ
λ2
F 
."
THE ESTIMATED AVERAGE REWARD R,0.9984928409947249,"c23 := Cψ√c15 + c15Lψ

1 +
4C4
ψ
λ2
F 
."
THE ESTIMATED AVERAGE REWARD R,0.9992464204973625,"c24 := c16Lψ

1 +
4C4
ψ
λ2
F 
."
