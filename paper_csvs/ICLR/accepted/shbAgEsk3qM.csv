Section,Section Appearance Order,Paragraph
GOOGLE,0.0,"1Google
2Department of Computing Science, University of Alberta"
ABSTRACT,0.001996007984031936,ABSTRACT
ABSTRACT,0.003992015968063872,"The theory of function approximation in reinforcement learning (RL) typically
considers low capacity representations that incur a tradeoff between approximation
error, stability and generalization. Current deep architectures, however, operate in
an overparameterized regime where approximation error is not necessarily a bottle-
neck. To better understand the utility of deep models in RL we present an analysis
of recursive value estimation using overparameterized linear representations that
provides useful, transferable ﬁndings. First, we show that classical updates such
as temporal difference (TD) learning or ﬁtted-value-iteration (FVI) converge to
different ﬁxed points than residual minimization (RM) in the overparameterized
linear case. We then develop a uniﬁed interpretation of overparameterized lin-
ear value estimation as minimizing the Euclidean norm of the weights subject to
alternative constraints. A practical consequence is that RM can be modiﬁed by
a simple alteration of the backup targets to obtain the same ﬁxed points as FVI
and TD (when they converge), while universally ensuring stability. Further, we
provide an analysis of the generalization error of these methods, demonstrating
per iterate bounds on the value prediction error of FVI, and ﬁxed point bounds for
TD and RM. Given this understanding, we then develop new algorithmic tools for
improving recursive value estimation with deep models. In particular, we extract
two regularizers that penalize out-of-span top-layer weights and co-linearity in
top-layer features respectively. Empirically we ﬁnd that these regularizers dramati-
cally improve the stability of TD and FVI, while allowing RM to match and even
sometimes surpass their generalization performance with assured stability."
INTRODUCTION,0.005988023952095809,"1
INTRODUCTION"
INTRODUCTION,0.007984031936127744,"Model-free value estimation remains a core method of reinforcement learning (RL), lying at the
heart of some of the most prominent achievements in this area (Mnih et al., 2015; Bellemare et al.,
2020). Such success appears paradoxical however, given that value estimation is subject to the deadly
triad: any value update that combines off-policy estimation with Bellman-bootstrapping and function
approximation diverges in the worst case (Sutton and Barto, 2018). Without additional assumptions it
is impossible to ensure the viability of iterative value estimation schemes, yet this remains a dominant
method in RL—its popularity supported by empirical success in many applications. Such a sizable
gap between theory and practice reﬂects limited understanding of such methods, how they behave in
practice, and what accounts for their empirical success (van Hasselt et al., 2018; Achiam et al., 2019)."
INTRODUCTION,0.00998003992015968,"Decomposing the deadly triad indicates that off-policy estimation and bootstrapping are difﬁcult to
forego: off-policy estimation is supported by the empirical effectiveness of action value maximization
and replay buffers, while Bellman-bootstrapping provides signiﬁcant advantages over Monte Carlo
estimation (Sutton, 1988). On the other hand, our understanding of the third factor, the relationship
between function representation and generalization, has evolved dramatically in recent years. Al-
though it was once thought that restrictive function approximation—representations that lack capacity
to ﬁt all data constraints—might be essential for generalization, we now know that this view is
oversimpliﬁed (Belkin et al., 2019). The empirical success of deep learning (Krizhevsky et al., 2012),"
INTRODUCTION,0.011976047904191617,"∗Work performed while an intern at Google Brain. Email: {chenjun,daes}@ualberta.ca"
INTRODUCTION,0.013972055888223553,Published as a conference paper at ICLR 2022
INTRODUCTION,0.015968063872255488,"extremely large models (Brown et al., 2020) and associated theoretical advances (Jacot et al., 2018)
have made it clear that gradient-based training of overparameterized models embodies implicit biases
that encourage generalization even after all data constraints are ﬁt exactly. This success suggests a
new opportunity for breaking the deadly triad: by leveraging overparameterized value representations
one can avoid some of the most difﬁcult tradeoffs in value-based RL (Lu et al., 2018)."
INTRODUCTION,0.017964071856287425,"The use of overparameterized deep models in value-based RL, however, still exhibits mysteries in
stability and performance. Although one might expect larger capacity models to improve the stability
of Bellman-bootstrapping, in fact the opposite appears to occur (van Hasselt et al., 2018). Our own
empirical experience indicates that classical value estimation with deep models eventually diverges
in non-toy problems. It has also been shown that value updating leads to premature rank-collapse
in deep models (Kumar et al., 2021), coinciding with instability and degrading generalization. In
practice, some form of early-stopping is usually necessary to obtaining successful results, a fact
that is not often emphasized in the literature (Agarwal et al., 2021). Meanwhile, there is a long
history of convergent methods being proposed in the RL literature—starting from residual gradient
(Baird, 1995), to gradient-TD (Sutton et al., 2008; Maei et al., 2009), prox gradient TD (Liu et al.,
2015; 2016), and emphatic TD (Yu, 2015; Sutton et al., 2016)—yet none of these has demonstrated
sufﬁcient generalization quality to supplant unstable methods. The current state of development
leaves an awkward tradeoff between stability and generalization. A stable recursive value estimation
method that ensures generalization quality with overparametrization remains elusive."
INTRODUCTION,0.01996007984031936,"In this paper we investigate whether overparameterized value representations might allow the stability-
generalization tradeoff to be better managed, enabling stable estimation methods that break the deadly
triad and generalize well. We ﬁrst consider policy evaluation with overparameterized linear value
representations, a simpliﬁed setting that still imposes the deadly triad (Zhang et al., 2021). Here
we ﬁnd that alternative updates, such as temporal differencing (TD), ﬁtted value iteration (FVI) and
residual minimization (RM) converge to different ﬁxed points in the overparameterized case (when
they converge), even though these updates share a common ﬁxed point when the approximation
error is zero and there are no extra degrees of freedom (Dann et al., 2014). That is, these algorithms
embody implicit biases that only become distinguishable in the overparameterized regime. From this
result, we observe that the ﬁxed points lie in different bases, which we use to develop a uniﬁed view
of iterative value estimation as minimizing the Euclidean norm of the weights subject to alternative
constraint sets. This uniﬁcation allows us to formulate alternative updates that share ﬁxed points
with TD and FVI but guarantee stability without requiring regularization or prox constraints (Zhang
et al., 2021). Next, we analyze the generalization performance of these algorithms and provide a
per-iterate bound on the value estimation error of FVI, and ﬁxed point bounds on the value estimation
error of TD. From these results, we identify two novel regularizers, one that closes the gap between
RM and TD and another that quantiﬁes the effect of the feature representation on the generalization
bound. We deploy these regularizers in a realistic study of deep model training for optimal value
estimation and observe systematic stability and generalization improvements. We also observe that
the performance gap between RM and TD can be closed and in some cases eliminated."
RELATED WORK,0.021956087824351298,"2
RELATED WORK"
RELATED WORK,0.023952095808383235,"Value estimation has a lengthy history throughout RL research. Our main focus is on off-policy value
estimation with parametric function representations and iterative (i.e., gradient based) updates. We
do not consider exploration nor full planning problems (i.e., approximately solving an entire Markov
decision process (MDP)) in the theoretical development, but instead focus on ofﬂine value estimation;
however, we do apply the ﬁndings to policy improvement experiments in the empirical investigation."
RELATED WORK,0.02594810379241517,"Dann et al. (2014) provide a comprehensive survey of value estimation with parametric function
representations. Signiﬁcant attention has been focused on underparameterized representations where
backed up values are not necessarily expressible in the function class, however we focus on the
overparameterized case where any backed up values can be assumed to be exactly representable with
respect to ﬁnite data. This change fundamentally alters the conclusions one can draw about algorithm
behavior, as we see below. One of the key consequences is that classical distinctions (Scherrer, 2010;
Dann et al., 2014) between objectives—e.g., mean squared Bellman error (MSBE), mean squared
projected Bellman error (MSPBE), mean squared temporal difference error (MSTDE), and the norm
of the expected TD update (NEU)—all collapse when the Bellman errors can all be driven to zero.
Despite this collapse, we ﬁnd that algorithms targetting the different objectives—TD and LSTD for
MSPBE (Sutton, 1988; Bradtke and Barto, 1996) and RM without double sampling (DS) for MSTDE"
RELATED WORK,0.027944111776447105,Published as a conference paper at ICLR 2022
RELATED WORK,0.029940119760479042,"(Maei et al., 2009; Dann et al., 2014)—converge to different ﬁxed points given overparameterization,
even when they ultimately satisfy the same set of temporal consistency constraints."
RELATED WORK,0.031936127744510975,"It is well known that classical value updates can diverge given off-policy data and parametric function
representations (Baird, 1995; Tsitsiklis and Van Roy, 1996; 1997). The stability of these methods has
therefore been studied extensively with many mitigations proposed, including restricting the function
representation (Gordon, 1995; Szepesv´ari and Smart, 2004) or adjusting the representation to ensure
contraction (Kolter, 2011; Ghosh and Bellemare, 2020; Wang et al., 2021b), or modifying the updates
to achieve convergent variations, such as LSTD (Bradtke and Barto, 1996; Yu, 2010), FVI (Ernst et al.,
2005; Munos and Szepesv´ari, 2005; Szepesv´ari and Munos, 2008; Lizotte, 2011) or the introduction
of target networks (Mnih et al., 2015; Lillicrap et al., 2016; Zhang et al., 2021; Carvalho et al., 2020).
Others have considered modiﬁed the updates to combat various statistical inefﬁciencies (van Hasselt,
2010; Weng et al., 2020; Konidaris et al., 2011). Another long running trend has been to consider
two time-scale algorithms and analyses, reﬂected in gradient-TD methods (Sutton et al., 2008; Maei
et al., 2009), prox gradient TD (Liu et al., 2015; 2016), primal-dual TD (Dai et al., 2017; Du et al.,
2017), and emphatic TD (Yu, 2015; Sutton et al., 2016). Beyond mere convergence, however, we
discover a greater diversity in ﬁxed points among algorithms in the overparameterized case, which
play a critical but previously unacknowledged role in generalization quality."
RELATED WORK,0.033932135728542916,"The fact that minimizing MSPBE via TD methods still dominates practice appears surprising given the
theoretical superiority of other objectives. It has been argued, for example, that direct policy gradient
methods (Sutton et al., 1999) dominate minimizing Bellman error objectives (Geist et al., 2017). Even
among Bellman based approaches, it is known that MSBE can upper bound the value estimation error
(MSE) whereas MSPBE cannot (Kolter, 2011; Dann et al., 2014), yet MSPBE minimization (via TD
based methods) empirically dominates minimizing MSBE (via residual methods). This dominance
has been thought to be due to the double sampling bias of residual methods (Baird, 1995; Dann et al.,
2014), but we uncover a more interesting ﬁnding that their ﬁxed points lie in different bases in the
overparameterized setting, and that reducing this difference closes the performance gap."
RELATED WORK,0.03592814371257485,"We analyze the convergence of classical updates given ofﬂine data and provide associated generaliza-
tion bounds, with the primary goal of understanding the discrepancy between previous theory and the
empirical success of TD/FVI versus RM. Although this theory sheds new light in exploitable ways, it
cannot overcome theoretical limits on ofﬂine value estimation, such as lower bounds on worst case
error that are exponential in horizon length (Wang et al., 2021a;b; Zanette, 2021; Xiao et al., 2021).
We analyze the convergence of the expected updates, extendible to the stochastic case using known
techniques (Yu, 2010; Bhandari et al., 2018; Dalal et al., 2018; Prashanth et al., 2021; Patil et al.,
2021). We expand the coverage of these earlier works by including alternative updates and focusing
on the overparameterized case, uncovering previously unobserved differences in the ﬁxed points."
RELATED WORK,0.03792415169660679,"There is a growing body of work on linear value estimation and planning that leverages the insight
of (Parr et al., 2008; Taylor and Parr, 2009) that linear value estimation is equivalent to linear
model approximation. A number of works have strived to obtain provably efﬁcient algorithms for
approximating the optimal policy values in this setting, but these generally rely on exploration or
strong assumptions about data coverage (Song et al., 2016; Yang and Wang, 2019; Duan et al., 2020;
Agarwal et al., 2020; Jin et al., 2020; Yang et al., 2020; Hao et al., 2021) that we do not make. Instead
we study linear value estimation to gain insight, but rather than focus on linear planning we leverage
the ﬁndings to improve the empirical performance of value estimation with deep models."
PRELIMINARIES,0.03992015968063872,"3
PRELIMINARIES"
PRELIMINARIES,0.041916167664670656,"Notation
We let R denote the set of real numbers, In an n × n identity matrix, and I the indicator
function. For a ﬁnite set X, we use ∆(X) to denote the set of probability distributions over X. For
a vector µ we let |supp(µ)| denote the size of the support of µ (i.e., the number of nonzero entries
in µ). For a matrix A ∈Rn×m, we let A† be the Moore-Penrose pseudoinverse of A, ∥A∥be its
spectral norm, and λmax(A) and λmin(A) be its maximum and minimum non-zero eigenvalues. We
also use ΠA = A†A to denote the projection matrix to the row space of A. For a vector x ∈Rd, we
let ∥x∥be its l2 norm and ∥x∥A =
√"
PRELIMINARIES,0.043912175648702596,"x⊤Ax be the associated norm for a positive deﬁnite matrix A.
We also use diag(x) ∈Rd×d to denote a diagonal matrix whose diagonal elements are x."
PRELIMINARIES,0.04590818363273453,"Markov reward processes
We consider the problem of predicting the value of a given stationary
policy in a Markov Decision Process (MDP). For a stationary policy, this problem can be formulated"
PRELIMINARIES,0.04790419161676647,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.0499001996007984,"in terms of a Markov reward process M = {S, P, r, γ}, such that S is a ﬁnite set of states, r : S →R
and P : S →∆(S) are the reward and transition functions respectively, and γ ∈[0, 1) is the discount
factor. Let S = |S| be the number of states. For a given state s ∈S, the function r(s) gives the
immediate reward incurred at s, while P(·|s) gives the next-state transition probability of s. The
value function speciﬁes the future discounted total reward obtained from each state, deﬁned as"
PRELIMINARIES,0.05189620758483034,"v(s) = E
 ∞
X"
PRELIMINARIES,0.05389221556886228,"t=0
γtr(st)
s0 = s

.
(1)"
PRELIMINARIES,0.05588822355289421,"To simplify the presentation we identify functions as vectors to allow vector-space operations: the
value function v and reward function r are identiﬁed as vectors v, r ∈RS, the transition P is
identiﬁed as an S × S transition matrix, where the s-th row Ps speciﬁes the transition probability
P(·|s) of state s. These deﬁnitions allow the value function to be expressed using Bellman’s equation"
PRELIMINARIES,0.05788423153692615,"v = r + γP v .
(2)"
PRELIMINARIES,0.059880239520958084,"Linear Function Approximation
It is usually not possible to consider tabular value representations
in practice, since the state set is usually combinatorial or inﬁnite. In our theoretical development we
focus on linear function approximations, where v is approximated by a linear combination of features
describing states; i.e., v(s) ≈φ(s)⊤θ, where θ ∈Rd is a parameter vector and φ : S →Rd maps a
given state s ∈S to a d-dimensional feature vector φ(s) ∈Rd. We let Φ ∈R|S|×d denote the feature
matrix, with the s-th row corresponding to the feature vector φ(s), so that the value approximation
can be written as v ≈Φθ. We assume ∥φ(s)∥≤1 for any s ∈S, and for simplicity we also assume
that there is no redundant or irrelevant features in the feature map; that is, Φ is full rank."
BATCH VALUE ESTIMATION,0.06187624750499002,"3.1
BATCH VALUE ESTIMATION"
BATCH VALUE ESTIMATION,0.06387225548902195,"We consider batch mode (“ofﬂine”) estimation of the value function. Let µ ∈∆(S) be an arbitrary
probability distribution over states and Dµ = diag(µ). The data set consists of {si, ri, s′
i}n
i=1
transition tuples, which are generated by s ∼µ, ri = r(si), s′
i ∼P(·|si). Let n(s) = Pn
i=1 I(si =
s) be the number of counts of state s. We deﬁne the empirical data distribution matrix D = diag(ˆµ),
where ˆµ(s) = n(s)/n is the empirical data distribution over states. The goal is to estimate the value
function by ﬁnding a weight vector θ ∈Rd that minimizes the value prediction error,"
BATCH VALUE ESTIMATION,0.0658682634730539,"E(θ) = ∥Φθ −v∥2
Dµ =
X"
BATCH VALUE ESTIMATION,0.06786427145708583,"s∈S µ(s)(φ(s)⊤θ −v(s))2 .
(3)"
BATCH VALUE ESTIMATION,0.06986027944111776,"Let ˆP be the empirical transition matrix, where the s-th row represents the estimated transition of
state s: if n(s) > 0, ˆPs(s′) = Pn
i=1 I(si = s, s′
i = s′)/n(s); if n(s) = 0, ˆPs(s′) = 0 for all s′ ∈S.
The empirical mean squared Bellman error on the batch data can be deﬁned as"
BATCH VALUE ESTIMATION,0.0718562874251497,MSBE(θ) = 1
BATCH VALUE ESTIMATION,0.07385229540918163,"2
r + γ ˆP Φθ −Φθ
2
D .
(4)"
BATCH VALUE ESTIMATION,0.07584830339321358,"Over vs Underparameterized Features
In this paper we are particularly interested in the over-
parameterized regime d > |supp(ˆµ)| where one can exactly satisfy the temporal consistencies on
all transitions in the batch data set, achieving zero Bellman error. (Obviously this would also be
possible if d = |supp(ˆµ)| but the strictly overparameterized case is more interesting, as we will see
below.) By contrast, in the underparameterized regime d < |supp(ˆµ)|, one can only expect to ﬁnd an
approximate solution that in general has nonzero Bellman error."
BATCH VALUE ESTIMATION,0.07784431137724551,"We consider three core algorithms in our analysis, covering major classical approaches."
BATCH VALUE ESTIMATION,0.07984031936127745,"Residual Minimization (RM)
RM directly minimizes the empirical mean squared Bellman error
Eq. (4) (MSBE) (Baird, 1995). The gradient update (Dann et al., 2014) can be expressed as"
BATCH VALUE ESTIMATION,0.08183632734530938,"θt+1 = θt −η(Φ −γ ˆP Φ)⊤D
 
Φθt −(r + γ ˆP Φθt)

,
(5)"
BATCH VALUE ESTIMATION,0.08383233532934131,"where θt is the estimated weight at step t, and η is the learning rate. As a gradient descent method, the
convergence of this update is robust, and applies to both linear and nonlinear function approximation."
BATCH VALUE ESTIMATION,0.08582834331337326,"Temporal Difference (TD) Learning
The simplest variant of TD (Sutton, 1988), known as TD(0),
also updates weights iteratively using transition data to approximate the value function. Let θt be the
weight vector at step t. Then the so-called “semi-gradient” of Eq. (4) is used to compute the update,"
BATCH VALUE ESTIMATION,0.08782435129740519,"θt+1 = θt −ηΦ⊤D
 
Φθt −
 
r + γ ˆP Φθt

,
(6)"
BATCH VALUE ESTIMATION,0.08982035928143713,Published as a conference paper at ICLR 2022
BATCH VALUE ESTIMATION,0.09181636726546906,"where η is the learning rate. From Eq. (6), it is clear that in the underparameterized (d < |supp(ˆµ)|)
regime, if the system converges, it must converge to parameters θ∗
D such that"
BATCH VALUE ESTIMATION,0.09381237524950099,"Φ⊤Dr −Φ⊤D(Φ −γ ˆP Φ)θ∗
D = 0
⇒
θ∗
D = (Φ⊤D(Φ −γ ˆP Φ))−1Φ⊤Dr ,
(7)
where θ∗
D is the TD ﬁxed point. That is, given limited representational power, the TD ﬁxed point
minimizes the squared projected Bellman error (MSPBE) by solving the projected Bellman equation:"
BATCH VALUE ESTIMATION,0.09580838323353294,"Φθ∗
D = ΠD
Φ
 
r + γ ˆP Φθ∗
D

,
(8)"
BATCH VALUE ESTIMATION,0.09780439121756487,"such that ΠD
Φ = Φ(Φ⊤DΦ)−1Φ⊤D is a weighted projection matrix. It is well-known that TD(0)
can diverge if the data sampling distribution µ is not the stationary distribution of the Markov process.
One can still compute the TD ﬁxed point directly using batch data, for example using the LSTD
algorithm (Bradtke and Barto, 1996), but this requires computation on the order of O(d2) compared
to O(d) of the iterative update algorithm Eq. (6). The value prediction error of TD is discussed in
(Tsitsiklis and Van Roy, 1997; Kolter, 2011; Dann et al., 2014; Bhandari et al., 2018)."
BATCH VALUE ESTIMATION,0.0998003992015968,"Fitted Value Iteration (FVI)
FVI iteratively updates the weight vector by solving a regression
problem where the target is constructed from the current estimate (Ernst et al., 2005; Dann et al.,
2014), which is also known as approximate dynamic programming (Sutton and Barto, 2018). In
particular, given the current weight θt at iteration t, the objective Eq. (9) is minimized to obtain θt+1,"
BATCH VALUE ESTIMATION,0.10179640718562874,FVIt(θ) = 1
BATCH VALUE ESTIMATION,0.10379241516966067,"2
r + γ ˆP Φθt −Φθ
2
D .
(9)
A simple calculation shows the TD ﬁxed point matches the ﬁxed point of FVI whenever θ0 is in
the row-span of DΦ. Although convergence of FVI can be established under strong conditions
(Szepesv´ari and Munos, 2008), the algorithm can be quite unstable in the general batch setting (Chen
and Jiang, 2019; Wang et al., 2021b)."
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.10578842315369262,"4
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.10778443113772455,"In this section, we study the convergence properties of the value estimation algorithms introduced
in Section 3.1 in the overparameterized regime where d > |supp(ˆµ)|. To faciliate analysis, we
ﬁrst introduce additional notation to simplify the derivations. Let {xi}k
i=1 denote the states in the
support of ˆµ, such that n(xi) > 0 for all i = {1, . . . , k} and k = |supp(ˆµ)|. Deﬁne a mask matrix
H ∈Rk×|S| and a truncated empirical data distribution matrix Dk ∈Rk×k according to H =  "
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.10978043912175649,"1⊤
x1...
1⊤
xk "
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.11177644710578842,",
Dk =  "
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.11377245508982035,"ˆµ(x1)
...
ˆµ(xk) "
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.1157684630738523,",
(10)"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.11776447105788423,"where 1xi ∈{0, 1}|S| is an indicator vector such that φ(xi) = Φ⊤1xi. We can then translate between
the full distribution and its support via the following.
Proposition 1. The empirical data distribution matrix D can be decomposed as D = H⊤DkH."
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.11976047904191617,"Let M = HΦ, N = H ˆP Φ and R = Hr denote the state features, the expected next state features
under the empirical transitions, and the rewards on the support of the data distribution respectively."
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.1217564870259481,"Overparameterized Residual Minimization
We ﬁrst study the convergence of RM given a ﬁxed
D. First note that the update Eq. (5) can be re-written as"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.12375249500998003,"θt+1 = (Id −η(M −γN)⊤Dk(M −γN))θt + η(M −γN)⊤DkR .
(11)"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.12574850299401197,"In the overparameterized regime, one can easily verify that there are inﬁnitely many solutions θ ∈Rd
satisfying (M −γN)θ = R. The gradient of Eq. (11) is zero at any of these solutions, which
implies that RM can have inﬁnitely many ﬁxed points. However, given that RM minimizes the MSBE
objective via gradient descent, as we show in the following theorem, the RM update initialized from
θ0 = 0 will converge to a unique ﬁxed point."
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.1277445109780439,"Theorem 1. With η ≤
1
(1+γ)2 and starting from θ0 = 0, RM converges to θRM = (M −γN)† R."
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.12974051896207583,"Remark 1. For simplicity we present the ﬁxed points of RM and TD starting from θ0 = 0. The ﬁxed
points given an arbitrary initial weight vector θ0 ∈Rd are shown in Appendices A.1 and A.2."
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.1317365269461078,"This result parallels similar ﬁndings in the supervised learning literature, that training overparam-
eterized deep models with gradient descent (or related algorithms) encodes implicit regularization"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.13373253493013973,Published as a conference paper at ICLR 2022 M τ φ1 φ2 φ′2
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.13572854291417166,φ′1 = φ2 ΠMφ′2
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.1377245508982036,"Figure 1: An illustrative example showing
the spectrum of W with k = 2 and d = 3.
M = [φ1, φ2]⊤. Without loss of generality,
let φ1 = (cos τ, sin τ, 0) and φ2 = (1, 0, 0).
N = [φ′
1, φ′
2]⊤, where φ′
1 = φ2, φ′
2 =
(−cos τ,
√"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.13972055888223553,"2
2 sin τ,
√"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.14171656686626746,"2
2 sin τ). Then W =
[[0, 1]⊤, [
√"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.1437125748502994,"2
2 , −(1 +
√"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.14570858283433133,"2
2 cos τ)]⊤]. Clearly,
the spectral norm of W increases as the
angle τ between φ1 and φ2 decreases."
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.14770459081836326,"that drives the model solution to particular outcomes in the overparameterized regime (Soudry et al.,
2018; Gunasekar et al., 2018; Neyshabur et al., 2019). Moreover, this implicit regularization is
often associated with generalization beneﬁts. However, unlike the case for supervised learning, RM
solutions do not often generalize well. Below we uncover a key difference between the RM ﬁxed
point and those of TD and FVI that sheds new light on the source of generalization differences."
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.1497005988023952,"Overparameterized TD Learning
We next consider the convergence properties of the TD(0)
update in the overparameterized setting. First, rewrite the TD(0) update formula Eq. (6) as"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.15169660678642716,"θt+1 = (Id −ηM ⊤Dk(M −γN))θt + ηM ⊤DkR .
(12)"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.1536926147704591,"Similar to RM, in the overparameterized regime any solutions θ ∈Rd that satisfy (M −γN)θ = R
are the ﬁxed points of Eq. (12), which implies an inﬁnite set of ﬁxed points. This is quite unlike
the underparameterized case where there is a unique TD ﬁxed point Eq. (7) given by the solution of
projected Bellman equation. However, we now show that in the overparameterized setting, similar to
solving RM using gradient descent, TD also encodes an implicit bias toward a particular ﬁxed point."
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.15568862275449102,"This of course requires TD to converge, which can be assured by a simple condition. Let W = NM †,
which has a geometric interpretation that we will exploit later in Section 5. Observe that"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.15768463073852296,"N = NΠM + N(Id −ΠM) = NM †M + N(Id −ΠM) = W M + N(Id −ΠM) ,
(13)"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.1596806387225549,"i.e., N can be decomposed into its projection onto the row-span of M plus a perpendicular component.
Eq. (13) shows that W projects N onto the row space of M; see Fig. 1 for an illustration. We refer
to W as the core matrix since its norm determines the convergence of TD.
Theorem 2. Choosing η <
1
(1+γ)∥Φ∥and starting from θ0 = 0, if ∥W ∥< 1"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.16167664670658682,"γ , TD(0) converges to
θTD = M †(Ik −γW )−1R. If ∥W ∥≥1"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.16367265469061876,γ there is an initial θ0 for which TD(0) does not converge.
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.1656686626746507,"A few key observations. First, note that the RM ﬁxed point in Theorem 1 and the TD ﬁxed point in
Theorem 2 are not identical. That is, the different value estimation algorithms continue to demonstrate
different preferences for ﬁxed points, but in the overparameterized setting these differences are implicit
in the algorithms and cannot be captured by the MSBE versus MSPBE objectives, since both are zero
for any θ that satisﬁes (M −γN)θ = R. Second, the ﬁxed point of TD lies in different basis than
RM. That is, θTD lies in the row space of the state features M, whereas θRM lies in the row space
of the residual features M −γN, and these two spaces are not identical in general. We revisit the
signiﬁcance of this difference below, but intuitively, the parameter vector θ is being trained to predict
values rather than temporal differences, and the future test states from which value predictions are
made will tend to be closer to the space spanned by M than M −γN."
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.16766467065868262,"Overparameterized Fitted Value Iteration
Finally, we consider the convergence of FVI. Recall
that at iteration t, FVI solves the least squares problem Eq. (9) to compute the next weight vector.
Using the notation established above, the normal equations for this problem can be expressed as
M ⊤DkMθ = M ⊤Dk(R+γNθt), but this system cannot be directly used to compute the solution
since M ⊤DkM is not invertible. Furthermore, just like RM and TD, any θ ∈Rd that satisﬁes
(M −γN)θ = R is a ﬁxed point of FVI. If one solves the least squares problem Eq. (9) using
gradient descent, it is known (Bartlett et al., 2021; Soudry et al., 2018) that the optimization converges
to the minimum norm solution"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.16966067864271456,"θt+1 = M †(R + γNθt) .
(14)"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.17165668662674652,"Interestingly, by choosing this solution, each iteration of FVI corresponds to applying a linear backup
on the current value estimate, where the backup operator is deﬁned by the core matrix."
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.17365269461077845,Published as a conference paper at ICLR 2022
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.17564870259481039,Deﬁnition 1. Deﬁne the core matrix linear operator TW by TW ν = R + γW ν for any ν ∈RS.
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.17764471057884232,"Similar results for the case with underparameterized linear model have been discussed in (Parr et al.,
2008). Using this operator we can characterize the convergence condition of FVI, reaching the
conclusion that whenever TW is a non-expansion, FVI converges to the same ﬁxed point as TD.
Theorem 3. Let θ0 be the initial weight and θt ∈Rd be the output of FVI at iteration t. We have"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.17964071856287425,"θt+1 = M †T t
W (R + γNθ0) .
(15)"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.18163672654690619,"Furthermore, given that ∥W ∥< 1/γ, the algorithm converges to θTD = M †(Ik −γW )−1R. If
∥W ∥≥1"
OVER-PARAMETERIZED LINEAR VALUE FUNCTION APPROXIMATION,0.18363273453093812,γ there is an initial θ0 for which FVI does not converge.
"UNIFIED VIEW OF OVERPARAMETERIZED VALUE ESTIMATORS
WE NOW SHOW THAT THE CONVERGENCE POINTS ABOVE CAN BE CHARACTERIZED AS SOLUTIONS TO RELATED CON-",0.18562874251497005,"4.1
UNIFIED VIEW OF OVERPARAMETERIZED VALUE ESTIMATORS
We now show that the convergence points above can be characterized as solutions to related con-
strained optimization problems, providing a uniﬁed perspective on the respective algorithm biases.
Theorem 4. θRM is the solution of the following constrained optimization,"
"UNIFIED VIEW OF OVERPARAMETERIZED VALUE ESTIMATORS
WE NOW SHOW THAT THE CONVERGENCE POINTS ABOVE CAN BE CHARACTERIZED AS SOLUTIONS TO RELATED CON-",0.18762475049900199,"inf
θ∈Rd
1
2∥θ∥2
s.t. Mθ = R + γNθ ,
(16)"
"UNIFIED VIEW OF OVERPARAMETERIZED VALUE ESTIMATORS
WE NOW SHOW THAT THE CONVERGENCE POINTS ABOVE CAN BE CHARACTERIZED AS SOLUTIONS TO RELATED CON-",0.18962075848303392,"and θTD is the solution of the following constrained optimization,"
"UNIFIED VIEW OF OVERPARAMETERIZED VALUE ESTIMATORS
WE NOW SHOW THAT THE CONVERGENCE POINTS ABOVE CAN BE CHARACTERIZED AS SOLUTIONS TO RELATED CON-",0.19161676646706588,"inf
θ∈Rd
1
2∥θ∥2
s.t. Mθ = R + γNθ , null(M)θ = 0 .
(17)"
"UNIFIED VIEW OF OVERPARAMETERIZED VALUE ESTIMATORS
WE NOW SHOW THAT THE CONVERGENCE POINTS ABOVE CAN BE CHARACTERIZED AS SOLUTIONS TO RELATED CON-",0.1936127744510978,"That is, the convergence points of RM, TD and FVI in the overparameterized case can all be seen
as minimizing the Euclidean norm of the weights θ subject to satisfying the Bellman constraints
Mθ = R + γNθ, where TD and FVI implicitly add the additional constraint that θ must lie in
the row span of M; moreover, this is the only constraint that differentiates TD from RM. From this
perspective, the algorithms can all be seen as iterative procedures for solving a particular form of
quadratic program, when they converge. Of course, proper constrained optimization techniques would
be able to stably compute solutions in scenarios where TD or FVI diverge (Boyd and Vandenberghe,
2004), but a more direct way to ensure convergence is implied by the following corollary.
Corollary 1. θTD is also the solution of the following constrained optimization,"
"UNIFIED VIEW OF OVERPARAMETERIZED VALUE ESTIMATORS
WE NOW SHOW THAT THE CONVERGENCE POINTS ABOVE CAN BE CHARACTERIZED AS SOLUTIONS TO RELATED CON-",0.19560878243512975,"inf
θ∈Rd
1
2∥θ∥2
s.t. Mθ = R + γNΠMθ .
(18)"
"UNIFIED VIEW OF OVERPARAMETERIZED VALUE ESTIMATORS
WE NOW SHOW THAT THE CONVERGENCE POINTS ABOVE CAN BE CHARACTERIZED AS SOLUTIONS TO RELATED CON-",0.19760479041916168,"Note that the right hand side of the constraint simply pre-projects next state value predictions onto
the row space of M before determining the Bellman backed up value. This allows a novel objective
to be formulated whose minimizer recovers the same ﬁxed point as TD,"
"UNIFIED VIEW OF OVERPARAMETERIZED VALUE ESTIMATORS
WE NOW SHOW THAT THE CONVERGENCE POINTS ABOVE CAN BE CHARACTERIZED AS SOLUTIONS TO RELATED CON-",0.1996007984031936,MSCBE(θ) = 1
"UNIFIED VIEW OF OVERPARAMETERIZED VALUE ESTIMATORS
WE NOW SHOW THAT THE CONVERGENCE POINTS ABOVE CAN BE CHARACTERIZED AS SOLUTIONS TO RELATED CON-",0.20159680638722555,"2 ∥R + γNΠMθ −Mθ∥2
D ,
(19)"
"UNIFIED VIEW OF OVERPARAMETERIZED VALUE ESTIMATORS
WE NOW SHOW THAT THE CONVERGENCE POINTS ABOVE CAN BE CHARACTERIZED AS SOLUTIONS TO RELATED CON-",0.20359281437125748,"which stands for mean squared corrected Bellman error. Note that MSCBE is not identical to MSPBE
because the projection is applied before not after the Bellman backup. Gradient descent minimization
of MSCBE yields the same ﬁxed point as θTD, which is essentially equivalent to applying RM to
corrected target values while ensuring stability. Note also that in the linear case the projection matrix
ΠM only needs to be precomputed once."
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.2055888223552894,"4.2
VALUE PREDICTION ERROR BOUNDS
One can also establish generalization bounds on the value estimation error of these methods in the
overparameterized regime. We ﬁrst provide a ﬁnite time analysis of the value prediction error of FVI."
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.20758483033932135,"Theorem 5. Let ˆΣ = M ⊤DkM be the empirical covariance matrix, and θt be the output of FVI
starting from θ0 as deﬁned in Theorem 3. Then for any θ∗∈arg minθ∈Rd E(θ),"
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.20958083832335328,E(θt) −E(θ∗)
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.21157684630738524,"≤
1
kλmin( ˆ
Σ)"
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.21357285429141717,"
ε2 + σ2 t−1
X"
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.2155688622754491,"i=0
(γW )i
2
+
(γW )t−12 ∥Φ∥2∥θ0 −θ∗∥2
+ 1"
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.21756487025948104,"2 ∥θ∗∥2
Id−ΠM ,
(20)"
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.21956087824351297,where ε = ∥N(Id −ΠM)θ∗∥and σ = ∥H( ˆP −P )v∥.
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.2215568862275449,"Intuitively, ε measures the length of next-state features along the direction θ∗, and σ is the expected
value prediction error under the empirical transition model, which can be bounded using standard"
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.22355289421157684,Published as a conference paper at ICLR 2022
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.22554890219560877,"concentration inequalities. The proof of this theorem is given in Appendix A.5. Observe that for
any step t ≥1, the output of FVI θt is within the row-span of M. This allows one to decompose
the prediction error into a component within the row-span, controlled by leveraging the core matrix
linear operator TW , and an orthogonal component that can be bounded by ∥θ∗∥2
Id−ΠM ."
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.2275449101796407,"Under the convergence conditions of Theorems 2 and 3, we also have the following generalization
bound for the value prediction error of θTD."
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.22954091816367264,"Corollary 2. Suppose that ∥W ∥≤1, and the value of any s ∈S is bounded by v(s) ∈[0, vmax].
For any θ∗∈arg minθ∈Rd E(θ),"
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.2315369261477046,"E[E(θTD)] ≤
γ log(|S|/δ)"
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.23353293413173654,"nminE[λmin( ˆΣ)](1 −γ)4 +
4γE[∥θ∗∥2
Id−ΠM ]"
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.23552894211576847,"E[λmin( ˆΣ)](1 −γ)2 + δvmax ,
(21)"
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.2375249500998004,where nmin = mins:n(s)>0 n(s) is the minimum counts given the data set.
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.23952095808383234,"This result automatically implies the requirements for ensuring ofﬂine generalization, accounting
both for distribution shift (Wang et al., 2021b) and policy completeness (Munos and Szepesv´ari,
2005; Duan et al., 2020) in feature space. In particular, for Eq. (20) and Eq. (21), we characterize
the distribution shift using well known concentration bounds in Appendix A.6, which leads to the
denominators kλmin( ˆΣ) and nminE[λmin( ˆΣ)] respectively. In addition, we explicitly characterize
the misalignment between the features of current states and next states using the core matrix, which
can be used to bound misalignment between values, replacing the feature completeness assumption."
"VALUE PREDICTION ERROR BOUNDS
ONE CAN ALSO ESTABLISH GENERALIZATION BOUNDS ON THE VALUE ESTIMATION ERROR OF THESE METHODS IN THE",0.24151696606786427,"We note that if the convergence condition cannot be satisﬁed, that is when ∥W ∥≥1/γ, the estimation
error could be arbitrarily large. The sources of value estimation error are explicit in Corollary 2.
The ﬁrst term measures the error due to sampling (statistical error), while the second term considers
out-of-span components of the optimal weight vector θ∗with respect to M (approximation error).
The smallest eigenvalue of the empirical covariance matrix E[λmin( ˆΣ)], as well as the length of
the orthogonal components E[∥θ∗∥2
Id−ΠM ], can both be controlled using classical techniques for
concentration properties of random matrix. In Appendix A.7 we present the exact approach for
bounding these two terms. Furthermore, by Corollary 1, one can also apply Corollary 2 to an
algorithm that directly optimizes MSCBE. Although a solution of Eq. (18) must exist, its value
prediction error can be arbitrarily large given that ∥W ∥≥1/γ. This also connects to a similar result
for the TD ﬁxed point that minimizes MSPBE in the underparameterized regime (Kolter, 2011)."
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.2435129740518962,"5
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.24550898203592814,"For tractability, the theory in prior sections assumes ﬁxed representations with a linear parameteriza-
tion on only the ﬁnal layer parameters of the value function. However, in practice, deep RL algorithms
also learn the representations in an end-to-end fashion. Inspired by the linear case, we now identify
two novel regularizers that are applicable more generally—one that closes the gap between RM and
TD inspired by the uniﬁed view of different ﬁxed points, and another that quantiﬁes the effect of
feature representation on the generalization bound."
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.24750499001996007,"Two-Part Approximation
Most deep RL algorithms rely on approximating values with a deep
neural network Qω that predicts the future outcome of given state-action pair (Mnih et al., 2015;
Kalashnikov et al., 2018; Lillicrap et al., 2016). In practice, Qω is trained by TD learning that
minimizes the objective P"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.249500998003992,"s,a(r(s, a)+γ ¯Qω(s, a)−Qω(s, a)), where ¯Qω(s, a) is known as the target
network to increase the learning stability. We view Qω as a two part-approximation with ω = (φ, θ),
where the output of the penultimate layer is referred as the feature mapping φ, the weight of last fully
connected layer is referred as θ, and the Q-function is approximated by Qω(s, a) = φ(s, a)⊤θ. Our
goal is to deﬁne regularizers on φ and θ that can be effectively applied to practical algorithms."
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.25149700598802394,"The ﬁrst regularizer directly takes inspiration from Theorem 4: by restricting the linear weight θ
within the row space of M (now deﬁned by exited (s, a) pairs in the data), RM ﬁnds the same ﬁxed
point as TD. We implement this idea by penalizing the norm of the perpendicular component of θ,"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.25349301397205587,"Rθ = ∥θ −ΠMθ∥,
(22)"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.2554890219560878,"In practice we compute this regularizer for each minibatch of data. The projection step is computed
by a least squares algorithm with an additional l2 regularization for numerical stability."
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.25748502994011974,Published as a conference paper at ICLR 2022
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.25948103792415167,"0
100
200
300
400
500 80 60 40 20"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.26147704590818366,"TD
TD + reg
RM
RM + reg"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.2634730538922156,(a) Reacher
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.2654690618762475,"0
100
200
300
400
500 500 400 300 200 100 0"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.26746506986027946,"TD
TD + reg
RM
RM + reg"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.2694610778443114,(b) Acrobot
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.2714570858283433,"0
100
200
300
400
500 50 0 50 100 150 200"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.27345309381237526,"TD
TD + reg
RM
RM + reg"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.2754491017964072,(c) Cartpole
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.2774451097804391,"0
100
200
300
400
500 1400 1200 1000 800 600 400 200"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.27944111776447106,"TD
TD + reg
RM
RM + reg"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.281437125748503,(d) Pendulum
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.2834331337325349,"Figure 2: We show the results with proposed regularization compared to the baseline algorithms.
The algorithms are trained using a ﬁxed ofﬂine data set collected by random initialized policies. The
x-axis shows the training iterations (in thousands) and y-axis shows the performance. All plots are
averaged over 100 runs. The shaded area shows the standard error."
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.28542914171656686,"The second regularizer is designed to address the effect of the feature representation on convergence
and value prediction error. In particular, Theorems 2 and 3 show that TD and FVI converge if the
spectral norm of W be upper bounded by 1/γ, which by Theorem 5 will also reduce the bound on
generalization error. Hence, it is natural to penalize the norm of this matrix using standard automatic
differentiation tools. However, such an approach is prone to numerical difﬁculty, as it involves
differentiation through a matrix pseudo inverse. We instead propose an alternative regularizer inspired
by the geometric interpretation of the core matrix Eq. (13): recall from Fig. 1 that W can be viewed
as the weights that project N onto the row space of M. To ensure that an arbitrary feature vector
can be well approximated using W , it would be ideal if M was orthonomral, which would imply an
ideally-behaved basis to represent N. This intuition justiﬁes the following regularization:"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.2874251497005988,"Rφ =
βId −M ⊤DkM
 ,
(23)"
REGULARIZERS FOR DEEP REINFORCEMENT LEARNING ALGORITHMS,0.2894211576846307,"where β is a scale parameter designed to approximate the column norm. That is, the regularizer forces
the neural network to learn an orthogonal feature embedding by normalizing the empirical feature
covariance matrix. The gradient of Rφ can also approximated using mini-batches. We augment the
original learning objectives by adding both Rθ and Rφ weighted by hyper-parameters."
"EMPIRICAL JUSTIFICATION OF REGULARIZERS
THE GOAL OF OUR EXPERIMENTS IS TO ASSESS THE APPLICABILITY OF THE PROPOSED REGULARIZATION SCHEMES",0.29141716566866266,"5.1
EMPIRICAL JUSTIFICATION OF REGULARIZERS
The goal of our experiments is to assess the applicability of the proposed regularization schemes
based on orthogonality and projection operations to practical deep RL algorithms. To avoid the
confounding effects of exploration, we restrict our study to learning from a frozen batch of data
with a ﬁxed number of transitions collected prior to learning. We use a randomly initialized policy
in this initial collection step. We consider both discrete and continuous control benchmarks in this
analysis. For the discrete action environments, we use DQN (Mnih et al., 2015) as the baseline
algorithm to add our regularizers. For continuous control environments, we use QT-Opt (Kalashnikov
et al., 2018) as the baseline algorithm, which is an actor-critic method that applies the cross-entropy
method to perform policy optimization. Our modiﬁcations add Rφ and Rθ to the standard MSBE
objective on the critic Q-network. Additional details describing the complete experiment setup for
each environment are provided in Appendix B. Experimental results contrasting vanilla TD and RM
with their regularized variants are summarized in Fig. 2. These ﬁndings demonstrate that the proposed
regularization schemes can be used to improve the performance of both vanilla TD learning and RM.
Note that RM is typically less popular than TD due to its worse empirical performance. On Acrobot
and Reacher, the modiﬁcation was able to fully close the gap between RM and TD. On Cartpole,
(where vanilla RM dominates vanilla TD), and on Pendulum, the regularizers also deliver signiﬁcant
improvements to the TD learning baseline and modest improvements to the RM baseline."
CONCLUSION,0.2934131736526946,"6
CONCLUSION"
CONCLUSION,0.2954091816367265,"We have investigated the ﬁxed points of classical updates for value estimation in the overparameterized
setting, where there is sufﬁcient capacity to ﬁt all the Bellman constraints in a given data set. We
ﬁnd that TD and FVI have different ﬁxed points than RM, but in the linear case the difference can
be entirely attributed to a constraint missing from RM that the solution lie in the row space of the
predecessor state features. We devised two novel regularizers based on these ﬁndings, which stabilized
the performance of TD without sacriﬁcing generalization, while improving the generalization of RM,
in the setting of estimating optimal values with a deep model. Characterizing the implicit bias of other
algorithms, such as gradient or emphatic TD variants remains open. Identifying other regularizers
that further close the gap between TD and RM is also an interesting direction for future investigation."
CONCLUSION,0.29740518962075846,Published as a conference paper at ICLR 2022
ACKNOWLEDGEMENT,0.2994011976047904,"7
ACKNOWLEDGEMENT"
ACKNOWLEDGEMENT,0.3013972055888224,"The authors would like to thank Mengjiao Yang, George Tucker, Oﬁr Nachum and Aviral Kumar
for insightful discussions and providing feedback on a draft of this manuscript. Dale Schuurmans
gratefully acknowledges funding from the Canada CIFAR AI Chairs Program, Amii and NSERC."
REFERENCES,0.3033932135728543,REFERENCES
REFERENCES,0.30538922155688625,"Joshua Achiam, Ethan Knight, and Pieter Abbeel. Towards characterizing divergence in deep
Q-learning. arXiv preprint arXiv:1903.08894, 2019."
REFERENCES,0.3073852295409182,"Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. FLAMBE: Structural complex-
ity and representation learning of low rank MDPs. In Advances in Neural Information Processing
Systems (NeurIPS), volume 33, 2020."
REFERENCES,0.3093812375249501,"Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G. Belle-
mare.
Deep reinforcement learning at the edge of the statistical precipice.
arXiv preprint
arXiv:2108.13264, 2021."
REFERENCES,0.31137724550898205,"Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
International Conference on Machine Learning (ICML), pages 30–37, 1995."
REFERENCES,0.313373253493014,"Peter L Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint.
arXiv preprint arXiv:2103.09177, 2021."
REFERENCES,0.3153692614770459,"Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias-variance trade-off. Proceedings of the National Academy of Sciences,
116(32):15849–15854, 2019."
REFERENCES,0.31736526946107785,"Marc G. Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C. Machado,
Subhodeep Moitra, Sameera S. Ponda, and Ziyu Wang. Autonomous navigation of stratospheric
balloons using reinforcement learning. Nature, 588:77–82, 2020."
REFERENCES,0.3193612774451098,"Jalaj Bhandari, Daniel Russo, and Raghav Singal. A ﬁnite time analysis of temporal difference
learning with linear function approximation. In Conference on Learning Theory (COLT), pages
1691–1692, 2018."
REFERENCES,0.3213572854291417,"Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge U Press, 2004."
REFERENCES,0.32335329341317365,"Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference
learning. Machine Learning, 22(1):33–57, 1996."
REFERENCES,0.3253493013972056,"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information
Processing Systems (NeurIPS), volume 33, 2020."
REFERENCES,0.3273453093812375,"Diogo Carvalho, Francisco S Melo, and Pedro Santos. A new convergent variant of Q-learning with
linear function approximation. In Advances in Neural Information Processing Systems (NeurIPS),
volume 33, 2020."
REFERENCES,0.32934131736526945,"Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In
International Conference on Machine Learning (ICML), pages 1042–1051, 2019."
REFERENCES,0.3313373253493014,"Bo Dai, Niao He, Yunpeng Pan, Byron Boots, and Le Song. Learning from conditional distributions
via dual embeddings. In Artiﬁcial Intelligence and Statistics, pages 1458–1467. PMLR, 2017."
REFERENCES,0.3333333333333333,"Gal Dalal, Bal´azs Sz¨or´eni, Gugan Thoppe, and Shie Mannor. Finite sample analysis for TD(0) with
function approximation. In AAAI Conference on Artiﬁcial Intelligence (AAAI), pages 6144–6160,
2018."
REFERENCES,0.33532934131736525,Published as a conference paper at ICLR 2022
REFERENCES,0.3373253493013972,"Christoph Dann, Gerhard Neumann, and Jan Peters. Policy evaluation with temporal differences: A
survey and comparison. Journal of Machine Learning Research, 15:809–883, 2014."
REFERENCES,0.3393213572854291,"Simon S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction
methods for policy evaluation. In International Conference on Machine Learning, pages 1049–
1058. PMLR, 2017."
REFERENCES,0.3413173652694611,"Yaqi Duan, Zeyu Jia, , and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function
approximation. In International Conference on Machine Learning (ICML), pages 2701–2709,
2020."
REFERENCES,0.34331337325349304,"Damien Ernst, P. Guerts, and L. Whenkel. Tree-based batch mode reinforcement learning. Journal of
Machine Learning Research, 6:503–556, 2005."
REFERENCES,0.34530938123752497,"Mattheiu Geist, Bilal Piot, and Olivier Pietquin. Is the Bellman residual a bad proxy? In Advances in
Neural Information Processing Systems (NeurIPS), volume 31, pages 3208–3217, 2017."
REFERENCES,0.3473053892215569,"Dibya Ghosh and Marc G. Bellemare. Representations for stable off-policy reinforcement learning.
In International Conference on Machine Learning (ICML), pages 3556–3565, 2020."
REFERENCES,0.34930139720558884,"Geoffrey J. Gordon. Stable function approximation in dynamic programming. In International
Conference on Machine Learning (ICML), pages 261–268, 1995."
REFERENCES,0.35129740518962077,"Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning (ICML), 2018."
REFERENCES,0.3532934131736527,"Botao Hao, Yaqi Duan, Tor Lattimore, Csaba Szepesv´ari, and Mengdi Wang. Sparse feature selection
makes batch reinforcement learning more sample efﬁcient. In International Conference on Machine
Learning (ICML), 2021."
REFERENCES,0.35528942115768464,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and gener-
alization in neural networks. In Advances in Neural Information Processing Systems (NeurIPS),
volume 31, pages 8571–8580, 2018."
REFERENCES,0.35728542914171657,"Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efﬁcient reinforcement
learning with linear function approximation. Journal of Machine Learning Research, 125:1–7,
2020."
REFERENCES,0.3592814371257485,"Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. QT-Opt: Scalable deep
reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293,
2018."
REFERENCES,0.36127744510978044,"J. Zico Kolter. The ﬁxed points of off-policy TD. In Advances in Neural Information Processing
Systems (NeurIPS), volume 24, pages 2169–2177, 2011."
REFERENCES,0.36327345309381237,"George Konidaris, Scott Niekum, and Philip S. Thomas. TDγ: Re-evaluating complex backups in
temporal difference learning. In Advances in Neural Information Processing Systems (NeurIPS),
volume 24, pages 2402–2410, 2011."
REFERENCES,0.3652694610778443,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classiﬁcation with deep neural
networks. In Advances in Neural Information Processing Systems (NeurIPS), volume 25, 2012."
REFERENCES,0.36726546906187624,"Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-parameterization
inhibits data-efﬁcient deep reinforcement learning. In International Conference on Learning
Representations (ICLR), 2021."
REFERENCES,0.36926147704590817,"Ilja Kuzborskij, Csaba Szepesv´ari, Omar Rivasplata, Amal Rannen-Triki, and Razvan Pascanu. On
the role of optimization in double descent: A least squares study. arXiv preprint arXiv:2107.12685,
2021."
REFERENCES,0.3712574850299401,"Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR,
2016."
REFERENCES,0.37325349301397204,Published as a conference paper at ICLR 2022
REFERENCES,0.37524950099800397,"Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample
analysis of proximal gradient TD. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI),
pages 504–513, 2015."
REFERENCES,0.3772455089820359,"Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Proximal gradient
temporal difference learning algorithms. In International Joint Conference on Artiﬁcial Intelligence
(IJCAI), pages 4195–4199, 2016."
REFERENCES,0.37924151696606784,"Daniel J. Lizotte. Convergent ﬁtted value iteration with linear function approximation. In Advances
in Neural Information Processing Systems (NeurIPS), volume 24, pages 2537–2545, 2011."
REFERENCES,0.3812375249500998,"Tyler Lu, Dale Schuurmans, and Craig Boutilier. Non-delusional Q-learning and value iteration. In
Advances in Neural Information Processing Systems (NeurIPS), volume 32, pages 9971–9981,
2018."
REFERENCES,0.38323353293413176,"Hamid R. Maei, Csaba Szepesv´ari, Shalabh Bhatnagar, Doina Precup, and David Silver. Convergent
temporal-difference learning with arbitrary smooth function approximation. In Advances in Neural
Information Processing Systems (NeurIPS), volume 22, pages 1204–1212, 2009."
REFERENCES,0.3852295409181637,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533, 2015."
REFERENCES,0.3872255489021956,"R´emi Munos and Csaba Szepesv´ari. Finite time bounds for sampling based ﬁtted value iteration. In
International Conference on Machine Learning (ICML), pages 880–887, 2005."
REFERENCES,0.38922155688622756,"Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role
of over-parameterization in generalization of neural networks. In International Conference on
Learning Representations (ICLR), 2019."
REFERENCES,0.3912175648702595,"Ronald Parr, Lihong Li, Gavin Taylor, Christopher Painter-Wakeﬁeld, and Michael L. Littman.
An analysis of linear models, linear value-function approximation, and feature selction for rein-
forcement learning. In International Conference on Machine Learning (ICML), pages 752–759,
2008."
REFERENCES,0.3932135728542914,"Gandharv Patil, L. A. Prashanth, and Doina Precup. Finite time analysis of temporal difference
learning with linear function approximation: the tail averaged case. 2021."
REFERENCES,0.39520958083832336,"L. A. Prashanth, Nathaniel Korda, and R´emi Munos. Concentration bounds for temporal difference
learning with linear function approximation: The case of batch data and uniform sampling. Machine
Learning, 110(3):559–618, 2021."
REFERENCES,0.3972055888223553,"Bruno Scherrer. Should one compute the temporal difference ﬁx point or minimize the Bellman
residual? In International Conference on Machine Learning (ICML), 2010."
REFERENCES,0.3992015968063872,"John Shawe-Taylor, Christopher KI Williams, Nello Cristianini, and Jaz Kandola. On the eigen-
spectrum of the Gram matrix and the generalization error of kernel-PCA. IEEE Transactions on
Information Theory, 51(7):2510–2522, 2005."
REFERENCES,0.40119760479041916,"Zhao Song, Ronald Parr, Xuejun Liao, and Lawrence Carin. Linear feature encoding for reinforcement
learning. In Advances in Neural Information Processing Systems (NeurIPS), 2016."
REFERENCES,0.4031936127744511,"Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. Journal of Machine Learning Research, 19:1–57, 2018."
REFERENCES,0.405189620758483,"Richard S Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3
(1):9–44, 1988."
REFERENCES,0.40718562874251496,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.4091816367265469,"Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Advances in Neural Information
Processing Systems (NeurIPS), volume 12, 1999."
REFERENCES,0.4111776447105788,Published as a conference paper at ICLR 2022
REFERENCES,0.41317365269461076,"Richard S Sutton, Csaba Szepesv´ari, and Hamid Reza Maei. A convergent O(n) algorithm for
off-policy temporal-difference learning with linear function approximation. In Advances in Neural
Information Processing Systems (NeurIPS), volume 21, pages 1609–1616, 2008."
REFERENCES,0.4151696606786427,"Richard S. Sutton, A. Rupam Mahmood, and Martha White. An emphatic approach to the problem
of off-policy temporal-difference learning. Journal of Machine Learning Research, 17:1–29, 2016."
REFERENCES,0.4171656686626746,"Csaba Szepesv´ari and R´emi Munos. Finite-time bounds for ﬁtted value iteration. Journal of Machine
Learning Research, 9(5), 2008."
REFERENCES,0.41916167664670656,"Csaba Szepesv´ari and W. D. Smart. Interpolation-based Q-learning. In International Conference on
Machine Learning (ICML), pages 100–107, 2004."
REFERENCES,0.42115768463073855,"Gavin Taylor and Ronald Parr. Kernelized value function approximation for reinforcement learning.
In International Conference on Machine Learning (ICML), pages 4214–4226, 2009."
REFERENCES,0.4231536926147705,"John N Tsitsiklis and Benjamin Van Roy. Feature-based methods for large scale dynamic program-
ming. Machine Learning, 22(1):59–94, 1996."
REFERENCES,0.4251497005988024,"John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function
approximation. IEEE Transactions on Automatic Control, 42(5):674–690, 1997."
REFERENCES,0.42714570858283435,"Hado van Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems
(NeurIPS), volume 23, 2010."
REFERENCES,0.4291417165668663,"Hado van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil.
Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018."
REFERENCES,0.4311377245508982,"Ruosong Wang, Dean Foster, and Sham M Kakade. What are the statistical limits of ofﬂine RL with
linear function approximation? In International Conference on Learning Representations (ICLR),
2021a."
REFERENCES,0.43313373253493015,"Ruosong Wang, Yifan Wu, Ruslan Salakhutdinov, and Sham M. Kakade. Instabilities of ofﬂine RL
with pre-trained neural representation. In International Conference on Machine Learning (ICML),
pages 10948–10960, 2021b."
REFERENCES,0.4351297405189621,"Wentao Weng, Harsh Gupta, Niao He, and Lei Ying. The mean-squared error of double Q-learning.
In Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.437125748502994,"Chenjun Xiao, Ilbin Lee, Bo Dai, Dale Schuurmans, and Csaba Szepesvari. On the sample complexity
of batch reinforcement learning with policy-induced data. arXiv preprint arXiv:2106.09973, 2021."
REFERENCES,0.43912175648702595,"Lin F. Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features.
In International Conference on Machine Learning (ICML), 2019."
REFERENCES,0.4411177644710579,"Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael I. Jordan. On function approx-
imation in reinforcement learning: Optimism in the face of large state spaces. In Advances in
Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.4431137724550898,"Huizhen Yu. Convergence of least squares temporal difference methods under general conditions. In
International Conference on Machine Learning (ICML), pages 1207–1214, 2010."
REFERENCES,0.44510978043912175,"Huizhen Yu. On convergence of emphatic temporal-difference learning. In Conference on Learning
Theory (COLT), pages 1–28, 2015."
REFERENCES,0.4471057884231537,"Andrea Zanette. Exponential lower bounds for batch reinforcement learning. In International
Conference on Machine Learning (ICML), pages 12287–12297, 2021."
REFERENCES,0.4491017964071856,"Shangtong Zhang, Hengshuai Yao, and Shimon Whiteson. Breaking the deadly triad with a target
network. In International Conference on Machine Learning (ICML), pages 12621–12631, 2021."
REFERENCES,0.45109780439121755,Published as a conference paper at ICLR 2022
REFERENCES,0.4530938123752495,Appendix
REFERENCES,0.4550898203592814,"A
PROOFS"
REFERENCES,0.45708582834331335,"A.1
PROOF OF THEOREM 1"
REFERENCES,0.4590818363273453,"Theorem 6 (Restatement of Theorem 1). Let θ0 ∈Rd be the initial weight vector. With η ≤
1
(1+γ)2 ,"
REFERENCES,0.46107784431137727,RM converges to θRM = (M −γN)† R + (Id −ΠM−γN)θ0.
REFERENCES,0.4630738522954092,"Proof. Let A = M −γN for simplicity. First recall the residual minimization update,"
REFERENCES,0.46506986027944114,"θt+1 =
 
Id −ηA⊤DkA

θt + ηA⊤DkR .
(24)"
REFERENCES,0.46706586826347307,"Let θ∗= A†R. It can be veriﬁed θ∗is one of the feasible solution as Aθ∗= R. Then we use
induction to show that for any θ0 ∈Rd and t ≥0"
REFERENCES,0.469061876247505,"θt+1 −θ∗= (Id −ηA⊤DkA)t+1(θ0 −θ∗) .
(25)"
REFERENCES,0.47105788423153694,"The base case holds by the update rule Eq. (24). Suppose that the statement holds for t, then we have"
REFERENCES,0.47305389221556887,"θt+1 −θ∗=
 
Id −ηA⊤DkA

θt + ηA⊤DkR −θ∗
(26)"
REFERENCES,0.4750499001996008,"=
 
Id −ηA⊤DkA

θt −(Id −ηA⊤DkA)θ∗
(27)"
REFERENCES,0.47704590818363274,"=
 
Id −ηA⊤DkA

(θt −θ∗)
(28)"
REFERENCES,0.47904191616766467,"=
 
Id −ηA⊤DkA
t+1 (θ0 −θ∗) .
(29) Thus,"
REFERENCES,0.4810379241516966,"θt+1 = (Id −ηA⊤DkA)t+1θ0 + (Id −(Id −ηA⊤DkA)t+1)θ∗.
(30)"
REFERENCES,0.48303393213572854,"We let V ΛV ⊤be its eigendecomposition of A⊤DkA, which is the empirical covariance matrix of
residual features. Let V−be the null space of V . Then"
REFERENCES,0.48502994011976047,"Id −(Id −ηA⊤DkA)t+1
(31)"
REFERENCES,0.4870259481037924,"=Id −(V V ⊤−ηV ΛV ⊤+ V−V ⊤
−)t+1
(32)"
REFERENCES,0.48902195608782434,"=Id −(V (Ik −ηΛ)V ⊤+ V−V ⊤
−)t+1
(33)"
REFERENCES,0.49101796407185627,"=Id −(V−V ⊤
−)t+1 −V (Ik −ηΛ)t+1V ⊤
(34)"
REFERENCES,0.4930139720558882,"=V V ⊤−V (Ik −ηΛ)t+1V ⊤
(35)"
REFERENCES,0.49500998003992014,"=V
 
Ik −(Ik −ηΛ)t+1
V ⊤
(36)"
REFERENCES,0.49700598802395207,Let λmax be the largest eigenvalue of A⊤DkA. We now show that λmax ≤1 + γ.
REFERENCES,0.499001996007984,"λmax
 
A⊤DkA

≤ k
X"
REFERENCES,0.500998003992016,"i=1
ˆµ(si)λmax

(φ(si) −γ ¯φ(s′
i))(φ(si) −γ ¯φ(s′
i))⊤
≤(1 + γ)2 ,
(37)"
REFERENCES,0.5029940119760479,"where we use the fact that λmax is a convex function and we assume ∥φ(s)∥≤1 for all s ∈S. Thus,
given that η ≤
1
1+γ , η ≤
1
λmax . Then Id −(Id −ηA⊤DkA)t+1 = V V ⊤as t →∞. Thus"
REFERENCES,0.5049900199600799,"lim
t→∞θt = lim
t→∞(Id −ηA⊤DkA)t+1θ0 + V V ⊤θ∗= lim
t→∞(Id −ηA⊤DkA)t+1θ0 + θ∗,
(38)"
REFERENCES,0.5069860279441117,"where the last equality follows by that θ∗is in the row space of A by deﬁnition. When θ0 = 0, we
have the algorithm converges to θ∗."
REFERENCES,0.5089820359281437,Published as a conference paper at ICLR 2022
REFERENCES,0.5109780439121756,"We next show the result for general θ0. Let θ0 = θ1
0 + θ2
0, where θ1
0 = ΠM−γNθ0 is the component
of θ0 that is in the row space of A, θ2
0 = (Id −ΠM−γN)θ0 is the perpendicular residual. Then,"
REFERENCES,0.5129740518962076,"lim
t→∞(Id −ηA⊤DkA)t+1θ0
(39)"
REFERENCES,0.5149700598802395,"= lim
t→∞(V−V ⊤
−+ V (Ik −ηΛ)t+1V ⊤)(θ1
0 + θ2
0)
(40)"
REFERENCES,0.5169660678642715,"=θ2
0 + lim
t→∞V (Ik −ηΛ)t+1V ⊤θ1
0 = θ2
0 ,
(41)"
REFERENCES,0.5189620758483033,where the last step follows by the choice of η. This ﬁnishes the proof.
REFERENCES,0.5209580838323353,"A.2
PROOF OF THEOREM 2"
REFERENCES,0.5229540918163673,We will need the matrix binomial theorem in the proof.
REFERENCES,0.5249500998003992,"Lemma 1 (Matrix Binomial Theorem). For n ≥0 and two matrices X, Y"
REFERENCES,0.5269461077844312,"(I + XY )nX = X(I + Y X)n .
(42)"
REFERENCES,0.5289421157684631,Proof.
REFERENCES,0.530938123752495,"(I + XY )nX = n
X k=0 n
k"
REFERENCES,0.5329341317365269,"
(XY )kX = X n
X k=0 n
k"
REFERENCES,0.5349301397205589,"
(Y X)k = X(I + Y X)n .
(43)"
REFERENCES,0.5369261477045908,"Theorem 7 (Restatement of Theorem 2). Assuming that M ⊤Dk(M −γN) is diagonalizable
Let θ0 ∈Rd be the initial weight vector. With η <
1
(1+γ)∥Φ∥, if ∥W ∥< 1"
REFERENCES,0.5389221556886228,"γ , TD(0) converges to
θTD = M †(Ik −γW )−1R + β, where β = Q0Q−1
0 θ0, Q0 are eigenvectors of M ⊤Dk(M −γN)
with zero eigenvalues. If ∥W ∥≥1"
REFERENCES,0.5409181636726547,γ there is an initial θ0 for which TD(0) does not converge.
REFERENCES,0.5429141716566867,Proof. We ﬁrst rewrite the TD update formulate as
REFERENCES,0.5449101796407185,"θt+1 = (Id −ηM ⊤Dk(M −γN))θt + ηM ⊤DkR
(44)"
REFERENCES,0.5469061876247505,"A simple recursive argument shows that for any θ0 ∈Rd,"
REFERENCES,0.5489021956087824,"θt+1 = (Id −ηM ⊤Dk(M −γN))t−1θ0 + η t
X"
REFERENCES,0.5508982035928144,"i=0
(Id −ηM ⊤Dk(M −γN))iM ⊤DkR . (45)"
REFERENCES,0.5528942115768463,"By the matrix binomial theorem (Lemma 1),"
REFERENCES,0.5548902195608783,"(Id −ηM ⊤Dk(M −γN))iM ⊤Dk = M ⊤Dk(Ik −η(M −γN)M ⊤Dk)i .
(46)"
REFERENCES,0.5568862275449101,"By writing N as the projection to the row-span of M and the perpendicular component, we have"
REFERENCES,0.5588822355289421,"(M −γN)M ⊤
(47)"
REFERENCES,0.5608782435129741,"=(M −γNM †M −γN(Id −M †M))M ⊤
(48)"
REFERENCES,0.562874251497006,"=(Ik −γW )MM ⊤,
(49)"
REFERENCES,0.564870259481038,where the last step follows by (Id −M †M)M ⊤= 0. Thus we can rewrite θt+1 as
REFERENCES,0.5668662674650699,"θt+1 = (Id −ηM ⊤Dk(M −γN))t−1θ0 + ηM ⊤Dk t
X"
REFERENCES,0.5688622754491018,"i=0
(Ik −η(M −γN)M ⊤Dk)iR (50)"
REFERENCES,0.5708582834331337,"= (Id −ηM ⊤Dk(M −γN))t−1θ0 + ηM ⊤Dk t
X"
REFERENCES,0.5728542914171657,"i=0
(Ik −η(Ik −γW )MM ⊤Dk)iR , (51)"
REFERENCES,0.5748502994011976,Published as a conference paper at ICLR 2022
REFERENCES,0.5768463073852296,"Given ∥W ∥< 1/γ, we have that all eigenvalues of Ik −γW are positive. Let η <
1
(1+γ)∥Φ∥, then"
REFERENCES,0.5788423153692615,"∥η(Ik −γW )MM ⊤Dk∥< η∥(Ik −γW )∥∥MM ⊤Dk∥< 1 ,
(52)"
REFERENCES,0.5808383233532934,otherwise the matrix power series diverges. Thus
REFERENCES,0.5828343313373253,"ηM ⊤Dk t
X"
REFERENCES,0.5848303393213573,"i=0
(Ik −η(Ik −γW )MM ⊤Dk)iR = M †(Ik −γW )−1R .
(53)"
REFERENCES,0.5868263473053892,"Therefore, given that θ0 = 0, we have the algorithm converge to M †(Ik −γW )−1R."
REFERENCES,0.5888223552894212,"We now show the convergence point for an arbitrary θ0. Let QΛQ−1 be the eigen decomposition
of M ⊤Dk(M −γN). By the low rank structure of this matrix, it has at most h ≤k non-zero
eigenvalues. Let Q0 be the eigenvectors with eigenvalue zero. Then"
REFERENCES,0.590818363273453,"lim
t→∞(Id −ηM ⊤Dk(M −γN))tθ0
(54)"
REFERENCES,0.592814371257485,"= lim
t→∞Q(Id −ηΛ)tQ−1θ0
(55)"
REFERENCES,0.5948103792415169,"=Q0Q−1
0 θ0 ,
(56)"
REFERENCES,0.5968063872255489,where the last step follows by the choice of η.
REFERENCES,0.5988023952095808,"A.2.1
CHARACTERIZATION FOR NON-DIAGONALIZABLE CASE"
REFERENCES,0.6007984031936128,"In the above analysis, we assume that the matrix M ⊤Dk(M −γN) is diagonalizable. We now
characterize the convergent point for the general case using Jordan normal form of the matrix. Let
Z = M ⊤Dk(M −γN) and Z = QJQ−1 be the jordan normal form of Z. We still denote Q0
the eigenvectors with eigenvalue zero. Then there is
lim
t→∞(I −ηZ)t = lim
t→∞Q(I −ηJ)tQ−1
(57)"
REFERENCES,0.6027944111776448,"Since I −ηJ has a block diagonal structure, its power can be obtained by ﬁrst computing the power
of each block. Let Ji be the jordan block with eigenvalue λi. We write Ji = λiI + L, where L is a
matrix such that the only non-zero entries of L are on the ﬁrst off-diagonal. Then we can write the
i-th block of J as (1 −ηλi)I −ηL. Using the binomial theorem we get"
REFERENCES,0.6047904191616766,"((1 −ηλi)I −ηL)t = t
X s=0"
REFERENCES,0.6067864271457086," t
s

(1 −ηλi)t−s(−ηL)s .
(58)"
REFERENCES,0.6087824351297405,"Note that Ls is the matrix with ones on the s-th diagonal away from the main diagonal, and Ls = 0
for s larger than the size of L. Therefore, ((1−ηλi)I −ηL)t is a triangular matrix with (1−ηλi)t on
the main diagonal, −ηt(1 −ηλi)t−1 on the ﬁrst off-diagonal, and so on. Therefore, the eigenvalues
of this matrix are all (1 −ηλi)t. Then given a learning rate that η < 1/λmax, for any jordan block
with λi > 0, we have that the matrix power converges. For λi = 0, the jordan block corresponds
to eigenvectors that are in the kernel space of Z. Thus, suppose that all eigenvalues of Z are
non-negative, we have"
REFERENCES,0.6107784431137725,"lim
t→∞Q(I −ηJ)tQ−1θ0 = Q0Q−1
0 θ0 .
(59)"
REFERENCES,0.6127744510978044,"Note that if a negative λi exists, the above derivations can still be used to characterize the convergent
sub-component of θ0. The non-convergent sub-component of θ0 will diverge with an exponential rate
as shown above."
REFERENCES,0.6147704590818364,"A.3
PROOF OF THEOREM 3"
REFERENCES,0.6167664670658682,"Proof. We ﬁrst prove the update formula. Recall the FVI update,"
REFERENCES,0.6187624750499002,θt = M †(R + γNθt−1) .
REFERENCES,0.6207584830339321,Published as a conference paper at ICLR 2022
REFERENCES,0.6227544910179641,For t = 1 the result holds by deﬁnition. Suppose that
REFERENCES,0.624750499001996,"θt = M †
 t−2
X"
REFERENCES,0.626746506986028,"i=0
(γNM)iR + (γNM †)t−1(R + γNθ0) ! (60)"
REFERENCES,0.6287425149700598,We now prove the result for t + 1 by induction.
REFERENCES,0.6307385229540918,"θt+1 = M †(R + γNθt)
(61) = M †"
REFERENCES,0.6327345309381237,"R + γNM †
 t−2
X"
REFERENCES,0.6347305389221557,"i=0
(γNM †)iR + (γNM †)t−1(R + γNθ0) !! (62) = M † R + t−1
X"
REFERENCES,0.6367265469061876,"i=1
(γNM †)iR + (γNM †)t(R + γNθ0) !! (63)"
REFERENCES,0.6387225548902196,"= M †
 t−1
X"
REFERENCES,0.6407185628742516,"i=0
(γNM †)iR + (γNM †)t(R + γNθ0) ! (64)"
REFERENCES,0.6427145708582834,"Clearly the convergence of this algorithm depends on the spectral norm of NM †. In particular, given
that ∥NM †∥< 1/γ, we have the algorithm converges to"
REFERENCES,0.6447105788423154,"M †(Ik −γW )−1R
(65)"
REFERENCES,0.6467065868263473,as t →∞. This ﬁnishes the proof.
REFERENCES,0.6487025948103793,"A.4
PROOF OF THEOREM 4 AND COROLLARY 1"
REFERENCES,0.6506986027944112,"Proof. We ﬁrst prove the result for residual minimization ﬁxed point θRM. The proof is adopted from
characterizing the minimum norm solution of solving least square (Boyd and Vandenberghe, 2004).
Let A = M −γN for simplicity. We write the Lagrange of the optimization problem,"
REFERENCES,0.6526946107784432,"L(θ, α) = inf
θ∈Rd sup
α∈Rk
1
2∥θ∥2 + α⊤(R −Aθ)
(66)"
REFERENCES,0.654690618762475,"= sup
α∈Rk
1
2∥A⊤α∥2 + α⊤R −α⊤AA⊤α
(67)"
REFERENCES,0.656686626746507,"= sup
α∈Rk α⊤R −1"
REFERENCES,0.6586826347305389,"2α⊤AA⊤α .
(68)"
REFERENCES,0.6606786427145709,Solving for α∗and add it to θ∗= A⊤α∗gives that θ∗= A†R.
REFERENCES,0.6626746506986028,"We next prove Corollary 1, which characterizes the TD and FVI ﬁxed point θTD. Let W = NM †.
We write the Lagrange of the optimization problem,"
REFERENCES,0.6646706586826348,"L(θ, α) = inf
θ∈Rd sup
α∈Rk
1
2∥θ∥2 + α⊤(R −(Ik −γW )Mθ)
(69)"
REFERENCES,0.6666666666666666,"= sup
α∈Rk
1
2∥M ⊤(Ik −γW )⊤α∥2 + α⊤R −α⊤(Ik −γW )MM ⊤(Ik −γW )⊤α (70)"
REFERENCES,0.6686626746506986,"= sup
α∈Rk α⊤R −1"
REFERENCES,0.6706586826347305,"2α⊤(Ik −γW )MM ⊤(Ik −γW )⊤α .
(71)"
REFERENCES,0.6726546906187625,"Solving for α∗and add it to θ∗= M ⊤(Ik −γW )⊤α∗gives that θ∗= M †(Ik −γW )−1R. The
second part of Theorem 4 is immediately followed by this."
REFERENCES,0.6746506986027944,"A.5
PROOF OF THEOREM 5"
REFERENCES,0.6766467065868264,"Lemma 2. Let θt be the output of FVI at iteration t with θ0 as the initial parameter. We have that
M †Mθt = θt for any t ≥1."
REFERENCES,0.6786427145708582,Published as a conference paper at ICLR 2022
REFERENCES,0.6806387225548902,"Proof. The claim is implied by the fact that θt is in the row space of M. In particular, by Theorem 3,
θt = M †α for some α ∈Rn. Thus,"
REFERENCES,0.6826347305389222,"M †Mθt = M †MM †α = M †α = θt .
(72)"
REFERENCES,0.6846307385229541,This ﬁnishes the proof.
REFERENCES,0.6866267465069861,Lemma 3. E(θ) is 1-smoothness.
REFERENCES,0.688622754491018,Proof. Recall the prediction error of θ ∈Rd
REFERENCES,0.6906187624750499,E(θ) = 1
REFERENCES,0.6926147704590818,"2 ∥Φθ −v∥2
Dµ = 1"
REFERENCES,0.6946107784431138,"2 ∥θ −θ∗∥2
Σ ,
(73)"
REFERENCES,0.6966067864271457,where Σ = Φ⊤DµΦ. The gradient of θ is E′(θ) = Σ(θ −θ∗). Then
REFERENCES,0.6986027944111777,"∥E′(θ1) −E′(θ2)∥= ∥Σ(θ1 −θ2)∥≤λmax(Σ) ∥(θ1 −θ2)∥≤∥θ1 −θ2∥,
(74)"
REFERENCES,0.7005988023952096,where we use ∥φ(s)∥≤1 for all s ∈S and λmax(Σ) ≤P
REFERENCES,0.7025948103792415,s µ(s)λmax(φ(s)φ(s)⊤) ≤1.
REFERENCES,0.7045908183632734,"Lemma 4. Let εapp = NΠ⊥
Mθ∗and σstat = H(P −ˆP)Φθ∗. We have"
REFERENCES,0.7065868263473054,"Mθ∗= R + γ(εapp + εstat) + γW Mθ∗.
(75)"
REFERENCES,0.7085828343313373,"Proof. Using the deﬁnitions we have,"
REFERENCES,0.7105788423153693,"Mθ∗= R + γHPΦθ∗
(76)"
REFERENCES,0.7125748502994012,"= R + γNθ∗+ γH(P −ˆP)Φθ∗
(77)"
REFERENCES,0.7145708582834331,"= R + γ(W M + NΠ⊥
M)θ∗+ γH(P −ˆP)Φθ∗
(78)"
REFERENCES,0.716566866267465,"= R + γ(εapp + εstat) + γW Mθ∗.
(79)"
REFERENCES,0.718562874251497,"Proof. By Theorem 3, θt = M †T t−1(R + γNθ0) is the output of FVI at iteration t. By noting that
E(θ∗) = 0 and Lemma 3, for any θ ∈Rd."
REFERENCES,0.720558882235529,E(θ) ≤1
REFERENCES,0.7225548902195609,2 ∥θ −θ∗∥2 = 1 2
REFERENCES,0.7245508982035929,"
∥θ −θ∗∥2
M †M + ∥θ −θ∗∥2
Id−M †M

.
(80)"
REFERENCES,0.7265469061876247,"We ﬁrst consider the second term. By Lemma 2,"
REFERENCES,0.7285429141716567,"∥θt −θ∗∥2
Id−M †M = (θt −θ∗)⊤ 
Id −M †M

(θt −θ∗) = ∥θ∗∥2
Id−M †M .
(81)"
REFERENCES,0.7305389221556886,"We now consider the ﬁrst term. By Lemma 4,"
REFERENCES,0.7325349301397206,"M(θ∗−θt) = Mθ∗−T t−1(R + γNθ0)
(82) = t−2
X"
REFERENCES,0.7345309381237525,"i=0
(γW )i (R + γ(εapp + εstat)) + (γW )t−1(Mθ∗) − t−2
X"
REFERENCES,0.7365269461077845,"i=0
(γW )iR −(γW )t−1(R + γNθ0) (83) =γ t−2
X"
REFERENCES,0.7385229540918163,"i=0
(γW )iεapp + t−1
X"
REFERENCES,0.7405189620758483,"i=0
(γW )iεstat + (γW )t−1N(θ∗−θ) ! (84)"
REFERENCES,0.7425149700598802,"Let ˆΣ = M ⊤DkM be the empirical covariance matrix. Note that λmin(M ⊤M)/k ≥λmin(ˆΣ). To
show this, let ¯D = diag( 1"
REFERENCES,0.7445109780439122,"k, . . . , 1"
REFERENCES,0.7465069860279441,"k), and M = USV ⊤be the SVD of M. Then"
REFERENCES,0.7485029940119761,"λ+
min(M ⊤¯DM) = min
∥x∥=1 x⊤M ⊤¯DMx
(85)"
REFERENCES,0.7504990019960079,"= min
∥α∥=1 α⊤SU ⊤¯DUSα
(86)"
REFERENCES,0.7524950099800399,"≥min
∥α∥=1 α⊤SU ⊤DkUSα
(87)"
REFERENCES,0.7544910179640718,"= λ+
min(M ⊤DkM)
(88)"
REFERENCES,0.7564870259481038,Published as a conference paper at ICLR 2022
REFERENCES,0.7584830339321357,"where we replace x = V α since V are orthonormal bases, and mini∈[k] ˆµi ≤1/k. Therefore,"
REFERENCES,0.7604790419161677,"∥M †∥= 1/
q"
REFERENCES,0.7624750499001997,"λ+
min(M †M) ≤1/
q"
REFERENCES,0.7644710578842315,"kλ+
min(ˆΣ)."
REFERENCES,0.7664670658682635,"Combining the results above we have,"
REFERENCES,0.7684630738522954,"∥θt −θ∗∥2
M †M =
M †M(θt −θ∗)
2
(89)"
REFERENCES,0.7704590818363274,"≤∥M †∥2 ∥M(θt −θ∗)∥2
(90) ≤
γ"
REFERENCES,0.7724550898203593,"kλmin(ˆΣ)  t−1
X"
REFERENCES,0.7744510978043913,"i=0
(γW )i(εapp + εstat) + (γW )t−1N(θ∗−θ)  2 (91) ≤
4γ"
REFERENCES,0.7764471057884231,kλmin(ˆΣ) 
REFERENCES,0.7784431137724551,"(ε2 + σ2)  t−1
X"
REFERENCES,0.780439121756487,"i=0
(γW )i 2"
REFERENCES,0.782435129740519,"+
(γW )t−12 ∥Φ∥2∥θ0 −θ∗∥2  . (92)"
REFERENCES,0.7844311377245509,Combine this with Eq. (81) ﬁnishes the proof.
REFERENCES,0.7864271457085829,"A.6
PROOF OF COROLLARY 2"
REFERENCES,0.7884231536926147,Proof. Recall that in the proof of Theorem 5 we have
REFERENCES,0.7904191616766467,"∥θt −θ∗∥2
M †M ≤
4γ"
REFERENCES,0.7924151696606786,kλmin(ˆΣ) 
REFERENCES,0.7944111776447106,"(ε2 + σ2)  t−1
X"
REFERENCES,0.7964071856287425,"i=0
(γW )i 2"
REFERENCES,0.7984031936127745,"+
(γW )t−12 ∥Φ∥2∥θ0 −θ∗∥2  . (93)"
REFERENCES,0.8003992015968064,"Given that ∥W ∥< 1, we have for the ﬁxed point θ∞,"
REFERENCES,0.8023952095808383,"∥θt −θ∗∥2
M †M ≤
4γ(ε2 + σ2)"
REFERENCES,0.8043912175648703,"kλmin(ˆΣ)(1 −γ)2
(94)"
REFERENCES,0.8063872255489022,"We ﬁrst consider σ2 = ∥H(P −ˆP)v∥2. By Hoeffding’s inequality and a union bound we have with
probability at least 1 −δ, for any s ∈supp(D),
( ˆPs −Ps)⊤v
 ≤
1
1 −γ s"
REFERENCES,0.8083832335329342,log(|S|/δ)
REFERENCES,0.810379241516966,"2n(s)
.
(95)"
REFERENCES,0.812375249500998,"Thus, let nmin = mins:n(s)>0 n(s), we have σ2"
REFERENCES,0.8143712574850299,"k ≤
log(|S|/δ)
2(1 −γ)2nmin
.
(96)"
REFERENCES,0.8163672654690619,"Now we consider ε2 = ∥NΠ⊥
Mθ∗∥2. Since NΠ⊥
M is perpendicular to M, and all features have
norm bounded by one, ε2"
REFERENCES,0.8183632734530938,"k ≤∥θ∗∥2
Id−M †M .
(97)"
REFERENCES,0.8203592814371258,"Combine the above we have,"
REFERENCES,0.8223552894211577,E(θ) ≤1
REFERENCES,0.8243512974051896,"2 ∥θ −θ∗∥2
M †M + 1"
REFERENCES,0.8263473053892215,"2 ∥θ∗∥2
Id−M †M
(98) ≤
2γ"
REFERENCES,0.8283433133732535,λmin(ˆΣ)(1 −γ)2
REFERENCES,0.8303393213572854,"
log(|S|/δ)
2(1 −γ)2nmin
+ ∥θ∗∥2
Id−M †M 
+ 1"
REFERENCES,0.8323353293413174,"2 ∥θ∗∥2
Id−M †M
(99)"
REFERENCES,0.8343313373253493,"=
γ log(|S|/δ)"
REFERENCES,0.8363273453093812,"λmin(ˆΣ)(1 −γ)4nmin
+
4γ"
REFERENCES,0.8383233532934131,"λmin(ˆΣ)(1 −γ)2 ∥θ∗∥2
Id−M †M
(100)"
REFERENCES,0.8403193612774451,"Finally, using the tower rule gives the desired result."
REFERENCES,0.8423153692614771,Published as a conference paper at ICLR 2022
REFERENCES,0.844311377245509,"A.7
CONCENTRATION OF EIGENVALUES AND BOUNDING THE ORTHOGONAL COMPLEMENT"
REFERENCES,0.846307385229541,"We will need the following result (Kuzborskij et al., 2021, Theorem 6), which is concerned with
the magnitude of projection onto the eigenspace of a covariance matrix. The result is based on
(Shawe-Taylor et al., 2005, Theorem 1)"
REFERENCES,0.8483033932135728,"Lemma 5. Let ˆΣ = 1 n
P"
REFERENCES,0.8502994011976048,"i xix⊤
i be the covariance matrix of i.i.d. data xi ∈Rd. Denote the h-“tail”
of eigenvalues of a covariance matrix ˆΣ = as λ>h = n
X"
REFERENCES,0.8522954091816367,"i=h+1
λi .
(101)"
REFERENCES,0.8542914171656687,"Let Ur be the ﬁrst r eigenbasis for r ∈[n]. Then for any z ∈Rd, with probability at least 1 −δ,"
REFERENCES,0.8562874251497006,"E

∥Π⊥
Urz∥2
2

≤min
h∈[r] 
"
REFERENCES,0.8582834331337326,"
1
nλ>h + 1 +
√ h
√n"
REFERENCES,0.8602794411177644,"v
u
u
t 2 n n
X"
REFERENCES,0.8622754491017964,"i=1
∥xi∥2 
"
REFERENCES,0.8642714570858283,"+ ∥z∥2
2 s 18"
REFERENCES,0.8662674650698603,"n ln
2n δ"
REFERENCES,0.8682634730538922,"
.
(102)"
REFERENCES,0.8702594810379242,"The next lemma gives a non-asymptotic result to understand the behaviour of ˆλmin (Kuzborskij et al.,
2021, Lemma 1).
Lemma 6. Let X = [X1, . . . , Xn] ∈Rd×n be a random matrix with i.i.d. columns, such that
maxi ∥Xi∥2 ≤K, and let ˆΣ = XX⊤/n, and Σ = E[X1X⊤
1 ]. Then, for every α ≥0, with
probability at least 1 −2e−α, we have"
REFERENCES,0.872255489021956,"λ+
min(ˆΣ) ≥λ+
min(Σ)  1 −K2 c r"
REFERENCES,0.874251497005988,"d
n +
rα n !!2"
REFERENCES,0.8762475049900199,"+
for n ≥d ,
(103)"
REFERENCES,0.8782435129740519,"and furthermore, assuming that ∥Xi∥Σ† =
√"
REFERENCES,0.8802395209580839,"d, for all i ∈[n], we have"
REFERENCES,0.8822355289421158,"λ+
min(ˆΣ) ≥λ+
min(Σ) r"
REFERENCES,0.8842315369261478,"d
n −K2

c + 6
rα n !2"
REFERENCES,0.8862275449101796,"+
for n < d ,
(104)"
REFERENCES,0.8882235528942116,where we have an absolute constant c = 23.5√ ln 9.
REFERENCES,0.8902195608782435,"B
EXPERIMENT SETUP"
REFERENCES,0.8922155688622755,"In this section, we provide additional details about the experimental setup and hyper-parameters
used for each of the environments. For all of these environments the regularization weights were
considered as tunable hyper-parameters. In addition, for Rφ (see Eq 23), the scale factor β was also
considered as a parameter to be tuned in order to approximate the feature matrix norm."
REFERENCES,0.8942115768463074,"B.1
ACROBOT"
REFERENCES,0.8962075848303394,"• Replay buffer with 10k tuples sampled using a random policy across trajectories with
maximum episode length of 64.
• A DQN with hidden units consisting of fully connected layers with (100, 100) units.
• Batch size 64.
• Learning rate of 1e-3.
• Regularized RM with weight of 2e-2 on Rφ and 1e-4 on Rw.
• Regularized TD with weight of 0 on Rφ and 1e-4 on Rw."
REFERENCES,0.8982035928143712,"B.2
REACHER"
REFERENCES,0.9001996007984032,"• Replay buffer with 10k tuples sampled from a random policy across trajectories with
maximum steps per episode of length 50."
REFERENCES,0.9021956087824351,Published as a conference paper at ICLR 2022
REFERENCES,0.9041916167664671,• Learning rate 1e-4.
REFERENCES,0.906187624750499,"• A value network for the continuous action inputs with a fc observation layer with params
(50,), a fc action layer with params (50,) and a joint fc layer with params (100,)."
REFERENCES,0.908183632734531,• Batch size 64.
REFERENCES,0.9101796407185628,• Gradient clipping with a norm of 10.0
REFERENCES,0.9121756487025948,• Regularized RM with weight of 0.15 on Rw and 0 on Rφ.
REFERENCES,0.9141716566866267,• Regularized TD with weight of 2e-2 on Rw and 7e-3 on Rφ.
REFERENCES,0.9161676646706587,"B.3
CARTPOLE"
REFERENCES,0.9181636726546906,"• Replay buffer with 10k tuples sampled using a random policy across trajectories with
maximum steps per episode of length 50."
REFERENCES,0.9201596806387226,"• A DQN with hidden units consisting of fully connected layers with (100, 100) units."
REFERENCES,0.9221556886227545,• Batch size 64.
REFERENCES,0.9241516966067864,• Learning rate 1e-3.
REFERENCES,0.9261477045908184,• Regularized RM with weight of 0.29 on Rw and 0 on Rφ.
REFERENCES,0.9281437125748503,• Regularized TD with weight of 1.5e-3 on Rw and 5e-3 on Rφ.
REFERENCES,0.9301397205588823,"B.4
PENDULUM"
REFERENCES,0.9321357285429142,"• Replay buffer with 1k tuples obtained by sampling directly from a ﬁxed initial state distribu-
tion using a random policy."
REFERENCES,0.9341317365269461,"• A value network for the continuous action inputs with a fc observation layer with params
(50,), a fc action layer with params (50,) and a joint fc layer with params (100,)."
REFERENCES,0.936127744510978,• Batch size 64.
REFERENCES,0.93812375249501,• Learning rate 1e-3.
REFERENCES,0.9401197604790419,• Regularized RM with weight of 1.0 on Rw and 5.4e-4 on Rφ.
REFERENCES,0.9421157684630739,• Regularized TD with weight of 0 on Rw and 1.0 on Rφ.
REFERENCES,0.9441117764471058,"B.5
EXTRA EXPERIMENT RESULTS"
REFERENCES,0.9461077844311377,"We provide extra experiment results on four Mujoco control problems to assess the applicability of
the proposed regularization Rφ: HalfCheetah, Hopper, Ant, and Walker2d. The results are provided
in Fig. 3. All results are averaged over 100 runs with different random seeds. The hyper-parameters
are provided below."
REFERENCES,0.9481037924151696,"• The Q-function is approximated by two hidden layer fully neural networks, where the hidden
layer size is 256."
REFERENCES,0.9500998003992016,• Batch size 256.
REFERENCES,0.9520958083832335,• Learning rate 3e-4.
REFERENCES,0.9540918163672655,"• Regularized TD weight are tuned from {1e −4, 1e −3, 1e −2, 1e −1, 1}"
REFERENCES,0.9560878243512974,Published as a conference paper at ICLR 2022
REFERENCES,0.9580838323353293,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps (1e6) 0 2000 4000 6000 8000 10000"
REFERENCES,0.9600798403193613,Returns
REFERENCES,0.9620758483033932,HalfCheetah-v2
REFERENCES,0.9640718562874252,"ddpg
reg_ddpg"
REFERENCES,0.9660678642714571,(a) HalfCheetah
REFERENCES,0.9680638722554891,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps (1e6) 1000 1500 2000 2500 3000 3500 4000"
REFERENCES,0.9700598802395209,Returns
REFERENCES,0.9720558882235529,Hopper-v2
REFERENCES,0.9740518962075848,"ddpg
reg_ddpg"
REFERENCES,0.9760479041916168,(b) Hopper
REFERENCES,0.9780439121756487,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps (1e6) 0 1000 2000 3000 4000 5000"
REFERENCES,0.9800399201596807,Returns
REFERENCES,0.9820359281437125,Ant-v2
REFERENCES,0.9840319361277445,"ddpg
reg_ddpg"
REFERENCES,0.9860279441117764,(c) Ant
REFERENCES,0.9880239520958084,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps (1e6) 0 1000 2000 3000 4000"
REFERENCES,0.9900199600798403,Returns
REFERENCES,0.9920159680638723,Walker2d-v2
REFERENCES,0.9940119760479041,"ddpg
reg_ddpg"
REFERENCES,0.9960079840319361,(d) Walker2d
REFERENCES,0.998003992015968,"Figure 3: We show the results with proposed regularization compared to the baseline algorithms.
The algorithms are trained using a ﬁxed ofﬂine data set collected by random initialized policies. The
x-axis shows the training iterations (in thousands) and y-axis shows the performance. All plots are
averaged over 100 runs. The shaded area shows the standard error."
