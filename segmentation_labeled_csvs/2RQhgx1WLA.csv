Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0019230769230769232,"This paper proposes a new easy-to-implement parameter-free gradient-based op-
timizer: DoWG (Distance over Weighted Gradients). We prove that DoWG is
efficient—matching the convergence rate of optimally tuned gradient descent in
convex optimization up to a logarithmic factor without tuning any parameters,
and universal—automatically adapting to both smooth and nonsmooth problems.
While popular algorithms following the AdaGrad framework compute a running
average of the squared gradients to use for normalization, DoWG maintains a new
distance-based weighted version of the running average, which is crucial to achieve
the desired properties. To complement our theory, we also show empirically that
DoWG trains at the edge of stability, and validate its effectiveness on practical
machine learning tasks."
INTRODUCTION,0.0038461538461538464,"1
Introduction"
INTRODUCTION,0.0057692307692307696,We study the fundamental optimization problem
INTRODUCTION,0.007692307692307693,"min
x∈X f(x),
(OPT)"
INTRODUCTION,0.009615384615384616,"where f is a convex function, and X is a convex, closed, and bounded subset of Rd. We assume
f has at least one minimizer x∗∈X. We focus on gradient descent and its variants, as they are
widely adopted and scale well when the model dimensionality d is large (Bottou et al., 2018). The
optimization problem (OPT) finds many applications: in solving linear systems, logistic regression,
support vector machines, and other areas of machine learning (Boyd and Vandenberghe, 2004).
Equally important, methods designed for (stochastic) convex optimization also influence the intuition
for and design of methods for nonconvex optimization– for example, momentum (Polyak, 1964),
AdaGrad (Duchi et al., 2010), and Adam (Kingma and Ba, 2015) were all first analyzed in the convex
optimization framework."
INTRODUCTION,0.011538461538461539,"As models become larger and more complex, the cost and environmental impact of training have
rapidly grown as well (Sharir et al., 2020; Patterson et al., 2021). Therefore, it is vital that we develop
more efficient and effective methods of solving machine learning optimization tasks. One of the
chief challenges in applying gradient-based methods is that they often require tuning one or more
stepsize parameters (Goodfellow et al., 2016), and the choice of stepsize can significantly influence
a method’s convergence speed as well as the quality of the obtained solutions, especially in deep
learning (Wilson et al., 2017)."
INTRODUCTION,0.013461538461538462,"The cost and impact of hyperparameter tuning on the optimization process have led to significant
research activity in designing parameter-free and adaptive optimization methods in recent years, see
e.g. (Orabona and Cutkosky, 2020; Carmon and Hinder, 2022) and the references therein."
INTRODUCTION,0.015384615384615385,"We say an algorithm is universal if it adapts to many different problem geometries or regularity
conditions on the function f (Nesterov, 2014; Levy et al., 2018; Grimmer, 2022). In this work,
we focus on two regularity conditions: (a) f Lipschitz and (b) f smooth. Lipschitz functions"
INTRODUCTION,0.01730769230769231,"have a bounded rate of change, that is, there exists some G > 0 such that for all x, y ∈X we
have |f(x) −f(y)| ≤G∥x −y∥. The Lipschitz property is beneficial for the convergence of
gradient-based optimization algorithms. They converge even faster on smooth functions, which
have continuous derivatives; that is, there exists some L > 0 such that for all x, y ∈X we have
∥∇f(x) −∇f(y)∥≤L∥x −y∥. Smoothness leads to faster convergence of gradient-based methods
than the Lipschitz property. Universality is a highly desirable property because in practice the same
optimization algorithms are often used for both smooth and nonsmooth optimization (e.g. optimizing
both ReLU and smooth networks)."
INTRODUCTION,0.019230769230769232,The main question of our work is as follows:
INTRODUCTION,0.021153846153846155,"Can we design a universal, parameter-free gradient descent method for (OPT)?"
INTRODUCTION,0.023076923076923078,"Existing universal variants of gradient descent either rely on line search (Nesterov, 2014; Grimmer,
2022), bisection subroutines (Carmon and Hinder, 2022), or are not parameter-free (Hazan and
Kakade, 2019; Levy et al., 2018; Kavis et al., 2019). Line search algorithms are theoretically strong,
achieving the optimal convergence rates in both the nonsmooth and smooth settings with only an extra
log factor. Through an elegant application of bisection search, Carmon and Hinder (2022) design
a parameter-free method whose convergence is only double-logarithmically worse than gradient
descent with known problem parameters. However, this method requires resets, i.e. restarting the
optimization process many times, which can be very expensive in practice. Therefore, we seek a
universal, parameter-free gradient descent method for (OPT) with no search subroutines."
INTRODUCTION,0.025,"Our contributions. We provide a new algorithm that meets the above requirements. Our main contri-
bution is a new universal, parameter-free gradient descent method with no search subroutines.
Building upon the recently proposed Distance-over-Gradients (DoG) algorithm (Ivgi et al., 2023), we
develop a new method, DoWG (Algorithm 1), that uses a different stepsize with adaptively weighted
gradients. We show that DoWG automatically matches the performance of gradient descent on (OPT)
up to logarithmic factors with no stepsize tuning at all. This holds in both the nonsmooth setting
(Theorem 3) and the smooth setting (Theorem 4). Finally, we show that DoWG is competitive on real
machine learning tasks (see Section 4)."
INTRODUCTION,0.026923076923076925,"Algorithm
No search
Parameter-free
Universal
GD framework"
INTRODUCTION,0.028846153846153848,"Polyak stepsize (Polyak, 1987; Hazan and Kakade, 2019)
✓
✗
✓
✓"
INTRODUCTION,0.03076923076923077,"Coin betting with normalization
✓
✓
✓(*)
✗
(Orabona and Pál, 2016; Orabona and Cutkosky, 2020; Orabona, 2023)"
INTRODUCTION,0.032692307692307694,"Nesterov line search (Nesterov, 2014)
✗
✓
✓
✓"
INTRODUCTION,0.03461538461538462,"AdaGrad
✓
✗
✓
✓
(Duchi et al., 2010; Levy et al., 2018; Ene et al., 2021)"
INTRODUCTION,0.03653846153846154,"Adam
✓
✗
✓
✓
(Kingma and Ba, 2015; Li et al., 2023)"
INTRODUCTION,0.038461538461538464,"Bisection search (Carmon and Hinder, 2022)
✗
✓
✓
✓"
INTRODUCTION,0.04038461538461539,"D-Adaptation (Defazio and Mishchenko, 2023)
✓
✓
✗
✓"
INTRODUCTION,0.04230769230769231,"DoG (Ivgi et al., 2023)
✓
✓
✓(*)
✓"
INTRODUCTION,0.04423076923076923,"DoWG (new, this paper!)
✓
✓
✓
✓"
INTRODUCTION,0.046153846153846156,"(*) Result appeared after the initial release of this paper.
Table 1: A comparison of different adaptive algorithms for solving (OPT). ""Universal"" means that
the algorithm can match the rate of gradient descent on both smooth and nonsmooth objectives up to
polylogarithmic factors. ""No search"" means the algorithm does not reset. ""GD framework"" refers to
algorithms that follow the framework of Gradient Descent."
LIT REVIEW,0.04807692307692308,"2
Related Work"
LIT REVIEW,0.05,"There is a lot of work on adaptive and parameter-free approaches for optimization. We summarize the
main properties of the algorithms we compare against in Table 1. We enumerate some of the major
approaches below:"
LIT REVIEW,0.051923076923076926,"Polyak stepsize. When f∗= f(x∗) is known, the Polyak stepsize (Polyak, 1987) is a theoretically-
grounded, adaptive, and universal method (Hazan and Kakade, 2019). When f∗is not known,
Hazan and Kakade (2019) show that an adaptive re-estimation procedure can recover the optimal
convergence rate up to a log factor when f is Lipschitz. Loizou et al. (2021) study the Polyak
stepsize in stochastic non-convex optimization. Orvieto et al. (2022) show that a variant of the Polyak
stepsize with decreasing stepsizes can recover the convergence rate of gradient descent, provided the
stepsize is initialized properly. Unfortunately, this initialization requirement makes the method not
parameter-free."
LIT REVIEW,0.05384615384615385,"The doubling trick. The simplest way to make an algorithm parameter-free is the doubling-trick.
For example, for gradient descent for L-smooth and convex optimization, the stepsize η = 1"
LIT REVIEW,0.05576923076923077,"L results
in the convergence rate of"
LIT REVIEW,0.057692307692307696,"f(ˆx) −f∗= O
D0L2 T"
LIT REVIEW,0.05961538461538462,"
,
(1)"
LIT REVIEW,0.06153846153846154,"where D0 = ∥x0 −x∗∥. We may therefore start with a small estimate L0 of the smoothness constant
L, run gradient descent for T steps, and return the average point. We restart and repeat this for
N times, and return the point with the minimum function value. So long as N ≥log L"
LIT REVIEW,0.06346153846153846,"L0 , we will
return a point with loss satisfying eq. (1) at the cost of only an additional logarithmic factor. This
trick and similar variants of it appear in the literature on prediction with expert advice and online
learning (Cesa-Bianchi et al., 1997; Cesa-Bianchi and Lugosi, 2006; Hazan and Megiddo, 2007). It is
not even needed to estimate N in some cases, as the restarting can be done adaptively (Streeter and
McMahan, 2012). In practice, however, the performance of doubling trick suffers from restarting the
optimization process and throwing away useful that could be used to guide the algorithm."
LIT REVIEW,0.06538461538461539,"Parameter-free methods. Throughout this paper, we use the term “parameter-free algorithms” to
describe optimization algorithms that do not have any tuning parameters. We specifically consider
only the deterministic setting with a compact domain. As mentioned before, Carmon and Hinder
(2022) develop an elegant parameter-free and adaptive method based on bisection search. Bisection
search, similar to grid search, throws away the progress of several optimization runs and restarts,
which may hinder their practical performance. Ivgi et al. (2023); Defazio and Mishchenko (2023)
recently developed variants of gradient descent that are parameter-free when f is Lipschitz. However,
D-Adaptation (Defazio and Mishchenko, 2023) has no known guarantee under smoothness, while
DoG (Ivgi et al., 2023) was only recently (after the initial release of this paper) shown to adapt to
smoothness. We compare against the convergence guarantees of DoG in Section 3.2. For smooth
functions, Malitsky and Mishchenko (2020) develop AdGD, a method that efficiently estimates the
smoothness parameter on-the-fly from the training trajectory. AdGD is parameter-free and matches
the convergence of gradient descent but has no known guarantees for certain classes of Lipschitz
functions. A proximal extension of this method has been proposed by Latafat et al. (2023)."
LIT REVIEW,0.0673076923076923,"Parameter-free methods in online learning. In the online learning literature, the term “parameter-
free algorithms” was originally used to describe another class of algorithms that adapt to the unknown
distance to the optimal solution (but can still have other tuning parameters such as Lipschitz constant).
When the Lipschitz parameter is known, approaches from online convex optimization such as coin
betting (Orabona and Pál, 2016), exponentiated gradient (Streeter and McMahan, 2012; Orabona,
2013), and others (McMahan and Orabona, 2014; Orabona and Cutkosky, 2020; Orabona and
Pál, 2021; Orabona and Tommasi, 2017) yield rates that match gradient descent up to logarithmic
factors. Knowledge of the Lipschitz constant can be removed either by using careful restarting
schemes (Mhammedi et al., 2019; Mhammedi and Koolen, 2020), or adaptive clipping on top of
coin betting (Cutkosky, 2019). For optimization in the deterministic setting, it is later clarified that,
by leveraging the normalization techniques developed in (Levy, 2017), the aforementioned online
learning algorithms can be used without knowing other tuning parameters (i.e., achieve “parameter-
free” in the sense of this paper) for optimizing both Lipschitz functions (Orabona and Pál, 2021) and
smooth functions (Orabona, 2023). Concretely, as shown in Orabona (2023) (which appears after the
initial release of this paper), combining algorithms in Streeter and McMahan (2012); Orabona and Pál
(2016) with normalization techniques (Levy, 2017) yields new algorithms that are also search-free,
parameter-free (in the sense of this paper), and universal. However, these algorithms are rather
different from DoWG in algorithmic style: these algorithms only use normalized gradients while
DoWG does use the magnitudes of the gradients; DoWG falls in the category of gradient descent
algorithms with adaptive learning rate, while these algorithms do not."
LIT REVIEW,0.06923076923076923,"Line search. As mentioned before, line-search-based algorithms are universal and theoretically
grounded (Nesterov, 2014) but are often expensive in practice (Malitsky and Mishchenko, 2020)."
LIT REVIEW,0.07115384615384615,"AdaGrad family of methods. Li and Orabona (2019) study a variant of the AdaGrad stepsizes
in the stochastic convex and non-convex optimization and show convergence when the stepsize is
tuned to depend on the smoothness constant. Levy et al. (2018) show that when the stepsize is tuned
properly to the diameter of the domain X in the constrained convex case, AdaGrad-Norm adapts to
smoothness. Ene et al. (2021) extend this to AdaGrad and other algorithms, and also to variational
inequalities. Ward et al. (2019); Traoré and Pauwels (2021) show the convergence of AdaGrad-Norm
for any stepsize for non-convex (resp. convex) optimization, but in the worst case the dependence on
the smoothness constant is worse than gradient descent. Liu et al. (2022) show that AdaGrad-Norm
converges in the unconstrained setting when f is quasi-convex, but their guarantee is worse than
gradient descent. We remark that all AdaGrad-style algorithms mentioned above require tuning
stepsizes, and are thus not parameter-free."
LIT REVIEW,0.07307692307692308,"Alternative justifications for normalization. There are other justifications for why adaptive methods
work outside of universality. Zhang et al. (2020a) study a generalized smoothness condition and
show that in this setting tuned clipped gradient descent can outperform gradient descent. Because the
effective stepsize used in clipped gradient descent is only a constant factor away from the effective
stepsize in normalized gradient descent, (Zhang et al., 2020a), also show that this improvement holds
for NGD. Zhang et al. (2020b) observe that gradient clipping and normalization methods outperform
SGD when the stochastic gradient noise distribution is heavy-tailed. However, Kunstner et al. (2023)
later observe that adaptive methods still do well even when the effect of the noise is limited."
IMPLEMENTATION/METHODS,0.075,"3
Algorithms and theory"
IMPLEMENTATION/METHODS,0.07692307692307693,"In this section we first review the different forms of adaptivity in gradient descent and normalized
gradient descent, and then introduce our proposed algorithm DoWG. The roadmap for the rest of
the paper is as follows: we first review the convergence of gradient descent in the Lipschitz and
smooth settings, and highlight the problem of divergence under stepsize misspecification, and how
normalization fixes that. Then, we introduce our main new algorithm, DoWG, and give our main
theoretical guarantees for the algorithm. Finally, we evaluate the performance of DoWG on practical
machine learning problems."
IMPLEMENTATION/METHODS,0.07884615384615384,"3.1
Baselines: gradient descent and normalized gradient descent"
IMPLEMENTATION/METHODS,0.08076923076923077,We start our investigation with the standard Gradient Descent (GD) algorithm:
IMPLEMENTATION/METHODS,0.08269230769230769,"xt+1 = ΠX (xt −η∇f(xt)),
(GD)"
IMPLEMENTATION/METHODS,0.08461538461538462,"where ΠX is the projection on X (when X = Rd, this is just the identity operator). The iterations (GD)
require specifying the stepsize η > 0. When f is G-Lipschitz, gradient descent achieves the following
standard convergence guarantee:"
IMPLEMENTATION/METHODS,0.08653846153846154,"Theorem 1. Suppose that f is convex with minimizer x∗. Let f∗= f(x∗). Let D0
def
= ∥x0 −x∗∥be
the initial distance to the optimum. Denote by ˆxT = 1"
IMPLEMENTATION/METHODS,0.08846153846153847,"T
PT −1
t=0 xt the average iterate returned by GD.
Then:"
IMPLEMENTATION/METHODS,0.09038461538461538,"• (Bubeck, 2015) If f is G-Lipschitz, the average iterate satisfies for any stepsize η > 0:"
IMPLEMENTATION/METHODS,0.09230769230769231,"f(¯xT ) −f∗≤D2
0
ηT + ηG2"
IMPLEMENTATION/METHODS,0.09423076923076923,"2 ,
(2)"
IMPLEMENTATION/METHODS,0.09615384615384616,"• (Nesterov, 2018) If f is L-smooth, then for all η < 2"
IMPLEMENTATION/METHODS,0.09807692307692308,L the average iterate satisfies
IMPLEMENTATION/METHODS,0.1,"f(ˆxT ) −f∗≤
2LD2
0
4 + TηL(2 −Lη).
(3)"
IMPLEMENTATION/METHODS,0.10192307692307692,"Minimizing eq. (2) over η gives f(¯xT ) −f∗= O

D0G
√ T"
IMPLEMENTATION/METHODS,0.10384615384615385,"
with η =
D0
G
√"
IMPLEMENTATION/METHODS,0.10576923076923077,"T . We have several remarks
to make about this rate for gradient descent. First, the optimal stepsize depends on both the distance"
IMPLEMENTATION/METHODS,0.1076923076923077,"to the optimum D0 and the Lipschitz constant G, and in fact, this rate is in general optimal (Nesterov,
2018, Theorem 3.2.1). Moreover, if we misspecify D0 or G while tuning η, this does not in general
result in divergence but may result in a slower rate of convergence. On the other hand, for the smooth
setting the optimal stepsize is η = 1"
IMPLEMENTATION/METHODS,0.10961538461538461,"L for which f(xT ) −f∗≤O

LD2
0
T

. Unfortunately, to obtain"
IMPLEMENTATION/METHODS,0.11153846153846154,this rate we have to estimate the smoothness constant L in order to choose a stepsize η < 2
IMPLEMENTATION/METHODS,0.11346153846153846,"L, and this
dependence is hard: if we overshoot the upper bound 2"
IMPLEMENTATION/METHODS,0.11538461538461539,"L, the iterations of gradient descent can diverge
very quickly, as shown by Figure 1. Therefore, GD with a constant stepsize cannot be universal: we
have to set the stepsize differently for smooth and nonsmooth objectives."
IMPLEMENTATION/METHODS,0.11730769230769231,"0.75
0.50
0.25
0.00
0.25
0.50
0.75
x 0 5 10 15 20 25 30 35 40 f(x)"
IMPLEMENTATION/METHODS,0.11923076923076924,"f(x)
Initial Point
Trajectory
Final Point"
IMPLEMENTATION/METHODS,0.12115384615384615,(a) Convergence with η = 1.9 L
IMPLEMENTATION/METHODS,0.12307692307692308,"1.0
0.5
0.0
0.5
1.0
1.5
x 0 20 40 60 80 100 f(x)"
IMPLEMENTATION/METHODS,0.125,"f(x)
Initial Point
Trajectory
Final Point"
IMPLEMENTATION/METHODS,0.12692307692307692,(b) Divergence with η = 2.1 L
IMPLEMENTATION/METHODS,0.12884615384615383,Figure 1: Two trajectories of gradient descent on the one-dimensional quadratic f(x) = Lx2
IMPLEMENTATION/METHODS,0.13076923076923078,"2 , with
L = 100."
IMPLEMENTATION/METHODS,0.1326923076923077,"Normalized Gradient Descent (NGD) (Shor, 2012) consists of iterates of the form"
IMPLEMENTATION/METHODS,0.1346153846153846,xt+1 = ΠX
IMPLEMENTATION/METHODS,0.13653846153846153,"
xt −η ∇f(xt)"
IMPLEMENTATION/METHODS,0.13846153846153847,∥∇f(xt)∥
IMPLEMENTATION/METHODS,0.14038461538461539,"
.
(NGD)"
IMPLEMENTATION/METHODS,0.1423076923076923,"The projection step above is not necessary, and the results for NGD also hold in the unconstrained
setting where the projection on X = Rd is just the identity. NGD has many benefits: it can escape
saddle points that GD may take arbitrarily long times to escape (Murray et al., 2019), and can
minimize functions that are quasi-convex and only locally Lipschitz (Hazan et al., 2015). One of
the main benefits of normalized gradient descent is that normalization makes the method scale-free:
multiplying f by a constant factor α > 0 and minimizing αf does not change the method’s trajectory
at all. This allows it to adapt to the Lipschitz constant G in nonsmooth optimization as well as the
smoothness constant L for smooth objectives, as the following theorem states:
Theorem 2. Under the same conditions as Theorem 1, the iterations generated by generated
by (NGD) satisfy after T steps satisfy:"
IMPLEMENTATION/METHODS,0.14423076923076922,"• (Nesterov, 2018) If f is G-Lipschitz, the minimal function suboptimality satisfies"
IMPLEMENTATION/METHODS,0.14615384615384616,"min
k∈{0,1,...,T −1} [f(xk) −f∗] ≤G
 D2
0
2ηT + η 2"
IMPLEMENTATION/METHODS,0.14807692307692308,"
,
(4)"
IMPLEMENTATION/METHODS,0.15,"where D0
def
= ∥x0 −x∗∥."
IMPLEMENTATION/METHODS,0.1519230769230769,"• (Levy, 2017; Grimmer, 2019) If f is L-Lipschitz, the minimal function suboptimality satisfies"
IMPLEMENTATION/METHODS,0.15384615384615385,"min
k=0,...,T −1 [f(xk) −f∗] ≤L 2"
IMPLEMENTATION/METHODS,0.15576923076923077," D2
0
2ηT + η 2"
IMPLEMENTATION/METHODS,0.1576923076923077,"2
.
(5)"
IMPLEMENTATION/METHODS,0.1596153846153846,"Tuning eq. (4) in η gives η =
D0
√"
IMPLEMENTATION/METHODS,0.16153846153846155,"T , and the stepsize is also optimal for eq. (5). This gives a"
IMPLEMENTATION/METHODS,0.16346153846153846,"convergence rate of D0G
√"
IMPLEMENTATION/METHODS,0.16538461538461538,"T when f is Lipschitz and D2
0L
T
when f is smooth. Observe that NGD
matches the dependence of gradient descent on G and L without any knowledge of it. Furthermore
that, unlike GD where the optimal stepsize is 1"
IMPLEMENTATION/METHODS,0.1673076923076923,"L in the smooth setting and
D0
G
√"
IMPLEMENTATION/METHODS,0.16923076923076924,T in the nonsmooth
IMPLEMENTATION/METHODS,0.17115384615384616,"setting. The optimal stepsize for NGD is the same in both cases. Therefore, NGD is universal:
the same method with the same stepsize adapts to nonsmooth and smooth objectives. Moreover,
misspecification of the stepsize in NGD does not result in divergence, but just slower convergence.
Another interesting property is that we only get a guarantee on the best iterate: this might be because
NGD is non-monotonic, as Figure 2 (a) shows."
IMPLEMENTATION/METHODS,0.17307692307692307,"0
50
100
150
200
250
Steps 10
1 100"
IMPLEMENTATION/METHODS,0.175,"f(x)
f *"
IMPLEMENTATION/METHODS,0.17692307692307693,Function suboptimality
IMPLEMENTATION/METHODS,0.17884615384615385,"0
50
100
150
200
250
Steps 10
5"
IMPLEMENTATION/METHODS,0.18076923076923077,Effective step size
IMPLEMENTATION/METHODS,0.18269230769230768,"Effective stepsize
2/L"
IMPLEMENTATION/METHODS,0.18461538461538463,"Figure 2: NGD iterations on ℓ2-regularized linear regression on the mushrooms dataset from Lib-
SVM (Chang and Lin, 2011) with η = 0.1. Top (a) shows the function suboptimality over time.
Observe that as the number of iterations grow, the method becomes non-monotonic. Bottom (b)
shows the effective stepsize ηeff,t =
0.1
∥∇f(xt)∥over time."
IMPLEMENTATION/METHODS,0.18653846153846154,"Edge of Stability Phenomena. We may reinterpret NGD with stepsize η as simply GD with a time-
varying “effective stepsize” ηeff,t =
η
∥∇f(xt)∥. We plot this effective stepsize for an ℓ2-regularized
linear regression problem in Figure 2 (b). Observe that the stepsize sharply increases, then decreases
until it starts oscillating around 2"
IMPLEMENTATION/METHODS,0.18846153846153846,L. Recall that 2
IMPLEMENTATION/METHODS,0.19038461538461537,"L is the edge of stability for gradient descent: its
iterates diverge when the stepsize crosses this threshold. Arora et al. (2022) observe this phenomenon
for NGD, and give a detailed analysis of it under several technical assumptions and when the iterates
are close to the manifold of local minimizers."
IMPLEMENTATION/METHODS,0.19230769230769232,"Theorem 2 offers an alternative, global, and less explicit explanation of this phenomenon: NGD
matches the optimal gradient descent rate, and in order to do so it must drive the effective stepsize
to be large. Specifically, suppose that we use the optimal stepsize η = D0
√"
IMPLEMENTATION/METHODS,0.19423076923076923,"T , and call the best iterate"
IMPLEMENTATION/METHODS,0.19615384615384615,"returned by NGD xτ. Then xτ satisfies f(xτ) −f∗≤D2
0L
T
and therefore by smoothness"
IMPLEMENTATION/METHODS,0.19807692307692307,"∥∇f(xτ)∥≤
p"
IMPLEMENTATION/METHODS,0.2,2L (f(xτ) −f∗) ≤ r
IMPLEMENTATION/METHODS,0.20192307692307693,"L2D2
0
T
= LD0
√"
IMPLEMENTATION/METHODS,0.20384615384615384,"T
= Lη."
IMPLEMENTATION/METHODS,0.20576923076923076,"This implies ηeff,τ =
η
∥∇f(xt)∥≥
1
L. Therefore the effective stepsize at convergence is forced
to grow to Ω
 1"
IMPLEMENTATION/METHODS,0.2076923076923077,"L

. But if the effective stepsize increases too much and crosses the threshold 2"
IMPLEMENTATION/METHODS,0.20961538461538462,"L,
the gradient norms start diverging, forcing the effective stepsize back down. Thus, NGD is self-
stabilizing. We note that Edge of Stability phenomenon is not unique to NGD, and GD itself trains
at the edge of stability for more complicated models where the smoothness also varies significantly
over training (Cohen et al., 2021; Damian et al., 2023)."
IMPLEMENTATION/METHODS,0.21153846153846154,"3.2
DoWG"
IMPLEMENTATION/METHODS,0.21346153846153845,"We saw in the last section that NGD adapts to both the Lipschitz constant G and the smoothness L,
but we have to choose η to vary with the distance to the optimum D0 = ∥x0 −x∗∥. In this section,
we develop a novel algorithm that adaptively estimates the distance to the optimum, and attains the
optimal convergence rate of gradient descent for constrained convex and smooth optimization up to a
logarithmic factor. Our algorithm builds upon the recently proposed Distance over Gradients (DoG)"
IMPLEMENTATION/METHODS,0.2153846153846154,"algorithm developed by Ivgi et al. (2023). We call the new method DoWG (Distance over Weighted
Gradients), and we describe it as Algorithm 1 below."
IMPLEMENTATION/METHODS,0.2173076923076923,Algorithm 1: DoWG: Distance over Weighted Gradients
IMPLEMENTATION/METHODS,0.21923076923076923,1 Input: initial point x0 ∈X Initial distance estimate rϵ > 0.
IMPLEMENTATION/METHODS,0.22115384615384615,"2 Initialize v−1 = 0, r−1 = rϵ."
IMPLEMENTATION/METHODS,0.2230769230769231,"3 for t = 0, 1, 2, . . . , T −1 do"
IMPLEMENTATION/METHODS,0.225,"4
Update distance estimator: ¯rt ←max (∥xt −x0∥, ¯rt−1)"
IMPLEMENTATION/METHODS,0.22692307692307692,"5
Update weighted gradient sum: vt ←vt−1 + ¯r2
t ∥∇f(xt)∥2"
IMPLEMENTATION/METHODS,0.22884615384615384,"6
Set the stepsize: ηt ←
¯r2
t
√vt
7
Gradient descent step: xt+1 ←ΠX (xt −ηt∇f(xt))"
IMPLEMENTATION/METHODS,0.23076923076923078,8 end
IMPLEMENTATION/METHODS,0.2326923076923077,"DoWG uses the same idea of estimating the distance from the optimum by using the distance from
the initial point as a surrogate, but instead of using the square root of the running gradient sum
Gt = Pt
k=0 ∥∇f(xk)∥2 as the normalization, DoWG uses the square root of the weighted gradient
sum vt = Pt
k=0 ¯r2
k∥∇f(xk)∥2. Observe that because the estimated distances ¯rt are monotonically
increasing, later gradients have a larger impact on vt than earlier ones compared to Gt. Therefore,
we may expect this to aid the method in adapting to the local properties of the problem once
far away from the initialization x0. We note that using a weighted sum of gradients is not new:
AcceleGrad (Levy et al., 2018) uses time-varying polynomial weights and Adam (Kingma and Ba,
2015) uses exponentially decreasing weights. The difference is that DoWG chooses the weights
adaptively based on the running distance from the initial point. This use of distance-based weighted
averaging is new, and we are not aware of any previous methods that estimate the running gradient
sum in this manner."
IMPLEMENTATION/METHODS,0.23461538461538461,"Nonsmooth analysis. The next theorem shows that DoWG adapts to the Lipschitz constant G and
the diameter D of the set X if the function f is nonsmooth but G-Lipschitz. We use the notation
log+ x = log x + 1 following (Ivgi et al., 2023).
Theorem 3. (DoWG, Lipschitz f). Suppose that the function f is convex, G-Lipschitz, and has a
minimizer x∗∈X. Suppose that the domain X is a closed convex set of (unknown) diameter D > 0.
Let rϵ < D. Then the output of Algorithm 1 satisfies for some t ∈{0, 1, . . . , T −1}"
IMPLEMENTATION/METHODS,0.23653846153846153,"f(¯xt) −f∗= O
GD
√"
IMPLEMENTATION/METHODS,0.23846153846153847,"T
log+
D
rϵ 
,"
IMPLEMENTATION/METHODS,0.2403846153846154,"where ¯xt
def
=
1
Pt−1
k=0 r2
k
Pt−1
k=0 r2
kxk is a weighted average of the iterates returned by the algorithm."
IMPLEMENTATION/METHODS,0.2423076923076923,"Discussion of convergence rate. DoWG matches the optimal O

DG
√ T"
IMPLEMENTATION/METHODS,0.24423076923076922,"
rate of tuned GD and tuned
NGD up to an extra logarithmic factor. We note that the recently proposed algorithms DoG (Ivgi
et al., 2023) and D-Adaptation (Defazio and Mishchenko, 2023) achieve a similar rate in this setting."
IMPLEMENTATION/METHODS,0.24615384615384617,"Comparison with DoG. As we discussed before, DoWG uses an adaptively weighted sum of
gradients for normalization compared to the simple sum used by DoG. In addition, DoG uses the
stepsize
¯rt
√Pt
k=0 ∥∇f(xk)∥2 , whereas the DoWG stepsize is pointwise larger: since ¯r2
k is monotonically"
IMPLEMENTATION/METHODS,0.24807692307692308,increasing in k we have
IMPLEMENTATION/METHODS,0.25,"ηDoWG,t =
¯r2
t
qPt
k=0 ¯r2
k∥∇f(xk)∥2 ≥
¯r2
t"
IMPLEMENTATION/METHODS,0.2519230769230769,"¯rt
qPt
k=0 ∥∇f(xk)∥2 =
¯rt
qPt
k=0 ∥∇f(xk)∥2 = ηDoG,t."
IMPLEMENTATION/METHODS,0.25384615384615383,"Of course, the pointwise comparison may not reflect the practical performance of the algorithms,
since after the first iteration the sequence of iterates x2, x3, . . . generated by the two algorithms can
be very different. We observe in practice that DoWG is in general more aggressive, and uses larger
stepsizes than both DoG and D-Adaptation (see Section 4)."
IMPLEMENTATION/METHODS,0.25576923076923075,"Smooth analysis. Our next theorem shows that DoWG adapts to the smoothness constant and the
diameter D of the set X."
IMPLEMENTATION/METHODS,0.25769230769230766,"Theorem 4. (DoWG, Smooth f). Suppose that the function f is L-smooth, convex, and has a
minimizer x∗∈X. Suppose that the domain X is a closed convex set of diameter D > 0. Let rϵ < D.
Then the output of Algorithm 1 satisfies for some t ∈{0, 1, . . . , T −1}"
IMPLEMENTATION/METHODS,0.25961538461538464,"f(¯xt) −f∗= O
LD2"
IMPLEMENTATION/METHODS,0.26153846153846155,"T
log+
D
rϵ 
,"
IMPLEMENTATION/METHODS,0.26346153846153847,"where ¯xt
def
=
1
Pt−1
k=0 r2
k
Pt−1
k=0 r2
kxk is a weighted average of the iterates returned by the algorithm."
IMPLEMENTATION/METHODS,0.2653846153846154,"The proof of this theorem and all subsequent results is relegated to the supplementary material.
We note that the proof of Theorem 4 uses the same trick used to show the adaptivity of NGD
to smoothness: we use the fact that ∥∇f(x)∥≤
p"
IMPLEMENTATION/METHODS,0.2673076923076923,"2L(f(x) −f∗) for all x ∈X applied to a
carefully-chosen weighted sum of gradients."
IMPLEMENTATION/METHODS,0.2692307692307692,"Comparison with GD/NGD. Both well-tuned gradient descent and normalized gradient descent
achieve the convergence rate O

LD2
0
T

where D0 = ∥x0 −x∗∥≤D for the constrained convex
minimization problem. Theorem 4 shows that DoWG essentially attains the same rate up to the
difference between D0 and D and an extra logarithmic factor. In the worst case, if we initialize
far from the optimum, we have D0 ≃D and hence the difference is not significant. We note that
DoG (Ivgi et al., 2023) suffers from a similar dependence on the diameter D of X, and can diverge in
the unconstrained setting, where X is not compact. This can be alleviated by making the stepsize
smaller by a polylogarithmic factor. A similar reduction of the stepsize also works for DoWG, and
we provide the proof in Section 7 in the supplementary."
IMPLEMENTATION/METHODS,0.27115384615384613,"Comparison with DoG. After the initial version of this paper, Ivgi et al. (2023) reported a con-
vergence guarantee for the unweighted average ˆxT = 1"
IMPLEMENTATION/METHODS,0.27307692307692305,"T
PT −1
k=0 xk returned by DoG. In particular,
Proposition 3 in their work gives the rate"
IMPLEMENTATION/METHODS,0.275,f(ˆxT ) −f∗= O
IMPLEMENTATION/METHODS,0.27692307692307694,"L(D0 log+
¯rT"
IMPLEMENTATION/METHODS,0.27884615384615385,rϵ + ¯rT )2 T !
IMPLEMENTATION/METHODS,0.28076923076923077,"= O
LD2"
IMPLEMENTATION/METHODS,0.2826923076923077,"T
log2
+
D ¯rϵ 
."
IMPLEMENTATION/METHODS,0.2846153846153846,"where D0 = ∥x0 −x∗∥, and where in the second step we used the bound D0 ≤D and ¯rT ≤D.
This rate is the same as that achieved by the weighted average of the DoWG iterates up to an
extra logarithmic factor log+
D
¯rϵ . We note that DoG also has a guarantee in the stochastic setting,
provided the gradients are bounded locally with a known constant, while in this work we have focused
exclusively on the deterministic setting."
IMPLEMENTATION/METHODS,0.2865384615384615,"0
50
100
150
200
250
Steps 10
2 10
1 100"
IMPLEMENTATION/METHODS,0.28846153846153844,"f(x)
f *"
IMPLEMENTATION/METHODS,0.2903846153846154,Function suboptimality
IMPLEMENTATION/METHODS,0.2923076923076923,"0
50
100
150
200
250
Steps 10
10 10
8 10
6"
IMPLEMENTATION/METHODS,0.29423076923076924,Effective step size
IMPLEMENTATION/METHODS,0.29615384615384616,"Effective stepsize
2/L"
IMPLEMENTATION/METHODS,0.2980769230769231,"Figure 3: DoWG iterations on ℓ2-regularized linear regression on the mushrooms dataset from
LibSVM (Chang and Lin, 2011) with rϵ = 10−6. Top (a) shows the function suboptimality over
time. Observe that as the number of iterations grow, the method becomes non-monotonic. Bottom (b)
shows the DoWG stepsize over time."
IMPLEMENTATION/METHODS,0.3,"Edge of Stability. Like NGD, DoWG also tends to increase the stepsize and train at the edge of
stability. The intuition from NGD carries over: in order to preserve the convergence rate of GD,
DoWG tends to drive the stepsize larger. However, once it overshoots, the gradients quickly diverge,
forcing the stepsize back down. Figure 3 shows the performance of DoWG and its stepsize on the
same regularized linear regression problem as in Figure 2. Comparing the two figures, we observe
that DoWG is also non-monotonic and trains close to the edge of stability, but its stepsize oscillates
less than NGD’s effective stepsize."
IMPLEMENTATION/METHODS,0.3019230769230769,"Universality. Theorems 4 and 3 together show that DoWG is universal, i.e. it almost recovers the
convergence of gradient descent with tuned stepsizes in both the smooth and nonsmooth settings.
As the optimal stepsize for gradient descent can differ significantly between the two settings, we
believe that achieving both rates simultaneously without any parameter-tuning or search procedures
is a significant strength of DoWG."
RESULTS/EXPERIMENTS,0.3038461538461538,"4
Experimental results"
RESULTS/EXPERIMENTS,0.3057692307692308,"0
50
100
150
200
Epoch 80 82 84 86 88 90 92"
RESULTS/EXPERIMENTS,0.3076923076923077,Test accuracy
RESULTS/EXPERIMENTS,0.3096153846153846,"Tuned Adagrad-norm
Adam
DoG
L-DoG
DoWG
DoG-ave
L-DoG-ave
DoWG-ave"
RESULTS/EXPERIMENTS,0.31153846153846154,"0
50
100
150
200
Epoch 80 85 90 95 100"
RESULTS/EXPERIMENTS,0.31346153846153846,Train accuracy
RESULTS/EXPERIMENTS,0.3153846153846154,"Tuned Adagrad-norm
Adam
DoG
L-DoG
DoWG
DoG-ave
L-DoG-ave
DoWG-ave"
RESULTS/EXPERIMENTS,0.3173076923076923,"0
20
40
60
80
100
Epoch 10
5 10
3 10
1 10
1"
RESULTS/EXPERIMENTS,0.3192307692307692,Step size
RESULTS/EXPERIMENTS,0.3211538461538462,"Adagrad tuned
DoG
DoWG
D-Adapt-norm"
RESULTS/EXPERIMENTS,0.3230769230769231,"0
50
100
150
200
Epoch 50 60 70 80 90"
RESULTS/EXPERIMENTS,0.325,Test accuracy
RESULTS/EXPERIMENTS,0.3269230769230769,"Tuned AdaGrad-norm
Adam
DoG
L-DoG
DoWG
DoG-ave
L-DoG-ave
DoWG-ave"
RESULTS/EXPERIMENTS,0.32884615384615384,"0
50
100
150
200
Epoch 50 60 70 80 90 100"
RESULTS/EXPERIMENTS,0.33076923076923076,Train accuracy
RESULTS/EXPERIMENTS,0.3326923076923077,"Tuned AdaGrad-norm
Adam
DoG
L-DoG
DoWG
DoG-ave
L-DoG-ave
DoWG-ave"
RESULTS/EXPERIMENTS,0.3346153846153846,"0
20
40
60
80
100
Epoch 10
3 10
2 10
1 10
0 10
1"
RESULTS/EXPERIMENTS,0.33653846153846156,Step size
RESULTS/EXPERIMENTS,0.3384615384615385,"DoG
DoWG"
RESULTS/EXPERIMENTS,0.3403846153846154,"Figure 4: VGG11 (top) and ResNet-50 (bottom) training on CIFAR10. Left: test accuracy, middle:
train loss, right: step sizes."
RESULTS/EXPERIMENTS,0.3423076923076923,"We compare DoWG to DoG, L-DoG from Ivgi et al. (2023), for all of which we also report perfor-
mance of the polynomially-averaged iterate with power 8 as recommended by Ivgi et al. (2023). We
also add comparison against Adam (Kingma and Ba, 2015) with cosine annealing and the standard
step size 10−3. All methods are used with batch size 256 with no weight decay on a single RTX3090
GPU. We plot the results in Figure 4 with the results averaged over 8 random seeds. We train the
VGG11 (Simonyan and Zisserman, 2015) and ResNet-50 (He et al., 2016) neural network architec-
tures on CIFAR10 (Krizhevsky, 2009) using PyTorch (Paszke et al., 2019), and implement1 DoWG
on top of the DoG code2. Unsurprisingly, DoWG’s estimates of the step size are larger than that
of DoG and D-Adapt-norm, which also makes it less stable on ResNet-50. While the last iterate of
DoWG gives worse test accuracy than Adam, the average iterate of DoWG often performs better."
RESULTS/EXPERIMENTS,0.34423076923076923,"Finally, we note that while both neural networks tested are generally nonsmooth, recent work shows
local smoothness can significantly influence and be influenced by a method’s trajectory (Cohen
et al., 2022; Pan and Li, 2022). We believe this adaptivity to smoothness might explain the empirical
difference between DoWG and DoG, but leave a rigorous discussion of adaptivity to local smoothness
to future work."
RESULTS/EXPERIMENTS,0.34615384615384615,"1https://github.com/rka97/dowg
2https://github.com/formll/dog"
REFERENCES,0.34807692307692306,References
REFERENCES,0.35,"Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on edge
of stability in deep learning. arXiv preprint arXiv:2205.09745, abs/2205.09745, 2022. URL
https://arXiv.org/abs/2205.09745. (Cited on page 6)"
REFERENCES,0.35192307692307695,"Léon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223–311, 2018. doi: 10.1137/16M1080173. URL https://doi.
org/10.1137/16M1080173. (Cited on page 1)"
REFERENCES,0.35384615384615387,"Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
doi: 10.1017/CBO9780511804441. (Cited on page 1)"
REFERENCES,0.3557692307692308,"Sébastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends® in
Machine Learning, 8(3-4):231–357, 2015. doi: 10.1561/2200000050. URL https://doi.org/
10.1561/2200000050. (Cited on page 4)"
REFERENCES,0.3576923076923077,"Yair Carmon and Oliver Hinder. Making SGD parameter-free. In Po-Ling Loh and Maxim Raginsky,
editors, Conference on Learning Theory, 2-5 July 2022, London, UK, volume 178 of Proceedings
of Machine Learning Research, pages 2360–2389. PMLR, 2022. URL https://proceedings.
mlr.press/v178/carmon22a.html. (Cited on pages 1, 2, 3, 17, and 18)"
REFERENCES,0.3596153846153846,"Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University
Press, USA, 2006. ISBN 0521841089. (Cited on page 3)"
REFERENCES,0.36153846153846153,"Nicolò Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and
Manfred K. Warmuth. How to use expert advice. J. ACM, 44(3):427–485, may 1997. ISSN
0004-5411. doi: 10.1145/258128.258179. URL https://doi.org/10.1145/258128.258179.
(Cited on page 3)"
REFERENCES,0.36346153846153845,"Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM
transactions on intelligent systems and technology (TIST), 2(3):1–27, 2011. (Cited on pages 6 and 8)"
REFERENCES,0.36538461538461536,"Jeremy M. Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, and Ameet Talwalkar. Gradient descent
on neural networks typically occurs at the edge of stability. In ICLR. OpenReview.net, 2021. (Cited
on page 6)"
REFERENCES,0.36730769230769234,"Jeremy M. Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati, Michal
Badura, Daniel Suo, David Cardoze, Zachary Nado, George E. Dahl, and Justin Gilmer. Adaptive
gradient methods at the edge of stability. arXiv preprint arXiv:2207.14484, abs/2207.14484, 2022.
URL https://arXiv.org/abs/2207.14484. (Cited on page 9)"
REFERENCES,0.36923076923076925,"Ashok Cutkosky. Artificial constraints and hints for unbounded online learning. In Alina Beygelzimer
and Daniel Hsu, editors, Proceedings of the Thirty-Second Conference on Learning Theory,
volume 99 of Proceedings of Machine Learning Research, pages 874–894. PMLR, 25–28 Jun
2019. URL https://proceedings.mlr.press/v99/cutkosky19a.html. (Cited on page 3)"
REFERENCES,0.37115384615384617,"Alex Damian, Eshaan Nichani, and Jason D. Lee. Self-stabilization: The implicit bias of gra-
dient descent at the edge of stability. In The Eleventh International Conference on Learning
Representations, 2023. URL https://openreview.net/forum?id=nhKHA59gXz. (Cited on
page 6)"
REFERENCES,0.3730769230769231,"Aaron Defazio and Konstantin Mishchenko. Learning-rate-free learning by D-adaptation. In Andreas
Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan
Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume
202 of Proceedings of Machine Learning Research, pages 7449–7479. PMLR, 23–29 Jul 2023.
(Cited on pages 2, 3, 7, and 16)"
REFERENCES,0.375,"John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. In Adam Tauman Kalai and Mehryar Mohri, editors, COLT 2010 - The
23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pages 257–269. Omnipress,
2010. URL http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#
page=265. (Cited on pages 1 and 2)"
REFERENCES,0.3769230769230769,"Alina Ene, Huy L. Nguyen, and Adrian Vladu. Adaptive gradient methods for constrained convex
optimization and variational inequalities. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 35, pages 7314–7321, 2021. (Cited on pages 2 and 4)"
REFERENCES,0.37884615384615383,"Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Practical Methodology, chapter 11. MIT
Press, 2016. URL http://www.deeplearningbook.org. (Cited on page 1)"
REFERENCES,0.38076923076923075,"Benjamin Grimmer. Convergence rates for deterministic and stochastic subgradient methods without
lipschitz continuity. SIAM Journal on Optimization, 29(2):1350–1365, 2019. doi: 10.1137/
18M117306X. URL https://doi.org/10.1137/18M117306X. (Cited on page 5)"
REFERENCES,0.38269230769230766,"Benjamin Grimmer. On optimal universal first-order methods for minimizing heterogeneous sums.
arXiv preprint arXiv:2208.08549, abs/2208.08549, 2022. URL https://arXiv.org/abs/2208.
08549. (Cited on pages 1 and 2)"
REFERENCES,0.38461538461538464,"Vineet Gupta, Tomer Koren, and Yoram Singer. A unified approach to adaptive regularization in
online and stochastic optimization. arXiv preprint arXiv:1706.06569, abs/1706.06569, 2017. URL
https://arXiv.org/abs/1706.06569. (Cited on page 17)"
REFERENCES,0.38653846153846155,"Elad Hazan and Sham Kakade. Revisiting the Polyak step size. arXiv preprint arXiv:1905.00313,
abs/1905.00313, 2019. URL https://arXiv.org/abs/1905.00313. (Cited on pages 2 and 3)"
REFERENCES,0.38846153846153847,"Elad Hazan and Nimrod Megiddo. Online learning with prior knowledge. In Nader H. Bshouty and
Claudio Gentile, editors, Learning Theory, pages 499–513, Berlin, Heidelberg, 2007. Springer
Berlin Heidelberg. ISBN 978-3-540-72927-3. (Cited on page 3)"
REFERENCES,0.3903846153846154,"Elad Hazan, Kfir Y. Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex
optimization. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and
Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Confer-
ence on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec,
Canada, pages 1594–1602, 2015. URL https://proceedings.neurips.cc/paper/2015/
hash/934815ad542a4a7c5e8a2dfa04fea9f5-Abstract.html. (Cited on page 5)"
REFERENCES,0.3923076923076923,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pages 770–778, 2016. doi: 10.1109/CVPR.2016.90. (Cited on page 9)"
REFERENCES,0.3942307692307692,"Maor Ivgi, Oliver Hinder, and Yair Carmon. DoG is SGD’s best friend: A parameter-free dynamic
step size schedule. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on
Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 14465–14499.
PMLR, 23–29 Jul 2023. (Cited on pages 2, 3, 7, 8, 9, 15, 16, 17, and 22)"
REFERENCES,0.39615384615384613,"Ali Kavis, Kfir Y. Levy, Francis R. Bach, and Volkan Cevher. UniXGrad: A universal, adaptive
algorithm with optimal guarantees for constrained optimization. In Hanna M. Wallach, Hugo
Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, ed-
itors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada, pages 6257–6266, 2019. URL https://proceedings.neurips.cc/paper/2019/
hash/88855547570f7ff053fff7c54e5148cc-Abstract.html. (Cited on page 2)"
REFERENCES,0.39807692307692305,"Ahmed Khaled and Peter Richtárik. Better theory for SGD in the nonconvex world. arXiv preprint
arXiv:2002.03329, abs/2002.03329, 2020. URL https://arXiv.org/abs/2002.03329. (Cited
on page 15)"
REFERENCES,0.4,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL
http://arXiv.org/abs/1412.6980. (Cited on pages 1, 2, 7, and 9)"
REFERENCES,0.40192307692307694,"Alex Krizhevsky. Learning multiple layers of features from tiny images. pages 32–33, 2009.
URL https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf. (Cited on
page 9)"
REFERENCES,0.40384615384615385,"Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt. Noise is not
the main factor behind the gap between SGD and Adam on transformers, but sign descent might
be. In The Eleventh International Conference on Learning Representations, 2023. URL https:
//openreview.net/forum?id=a65YK0cqH8g. (Cited on page 4)"
REFERENCES,0.40576923076923077,"Puya Latafat, Andreas Themelis, Lorenzo Stella, and Panagiotis Patrinos.
Adaptive proximal
algorithms for convex optimization under local Lipschitz continuity of the gradient. arXiv preprint
arXiv:2301.04431, 2023. (Cited on page 3)"
REFERENCES,0.4076923076923077,"Kfir Y. Levy. Online to offline conversions, universality and adaptive minibatch sizes. In Isabelle
Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan,
and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,
CA, USA, pages 1613–1622, 2017. URL https://proceedings.neurips.cc/paper/2017/
hash/ce5140df15d046a66883807d18d0264b-Abstract.html. (Cited on pages 3, 5, and 15)"
REFERENCES,0.4096153846153846,"Kfir Yehuda Levy, Alp Yurtsever, and Volkan Cevher. Online adaptive methods, universality and
acceleration. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò
Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems
31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December
3-8, 2018, Montréal, Canada, pages 6501–6510, 2018. URL https://proceedings.neurips.
cc/paper/2018/hash/b0169350cd35566c47ba83c6ec1d6f82-Abstract.html.
(Cited on
pages 1, 2, 4, 7, 15, and 18)"
REFERENCES,0.4115384615384615,"Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin. Convergence of adam under relaxed assumptions.
arXiv preprint arXiv:2304.13972, abs/2304.13972, 2023. URL https://arXiv.org/abs/2304.
13972. (Cited on page 2)"
REFERENCES,0.41346153846153844,"Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. In Kamalika Chaudhuri and Masashi Sugiyama, editors, The 22nd International Confer-
ence on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa,
Japan, volume 89 of Proceedings of Machine Learning Research, pages 983–992. PMLR, 2019.
URL http://proceedings.mlr.press/v89/li19c.html. (Cited on page 4)"
REFERENCES,0.4153846153846154,"Zijian Liu, Ta Duy Nguyen, Alina Ene, and Huy L. Nguyen. On the convergence of AdaGrad(norm)
on Rd: Beyond convexity, non-asymptotic rate and acceleration. arXiv preprint arXiv:2209.14827,
abs/2209.14827, 2022. URL https://arXiv.org/abs/2209.14827. (Cited on page 4)"
REFERENCES,0.4173076923076923,"Nicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic polyak
step-size for SGD: an adaptive learning rate for fast convergence. In Arindam Banerjee and Kenji
Fukumizu, editors, The 24th International Conference on Artificial Intelligence and Statistics,
AISTATS 2021, April 13-15, 2021, Virtual Event, volume 130 of Proceedings of Machine Learning
Research, pages 1306–1314. PMLR, 2021. URL http://proceedings.mlr.press/v130/
loizou21a.html. (Cited on page 3)"
REFERENCES,0.41923076923076924,"Yura Malitsky and Konstantin Mishchenko. Adaptive gradient descent without descent. In Pro-
ceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July
2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 6702–6712.
PMLR, 2020. URL http://proceedings.mlr.press/v119/malitsky20a.html. (Cited on
pages 3 and 4)"
REFERENCES,0.42115384615384616,"H. Brendan McMahan and Francesco Orabona. Unconstrained online linear learning in hilbert spaces:
Minimax algorithms and normal approximations. In Maria-Florina Balcan, Vitaly Feldman, and
Csaba Szepesvári, editors, Proceedings of The 27th Conference on Learning Theory, COLT 2014,
Barcelona, Spain, June 13-15, 2014, volume 35 of JMLR Workshop and Conference Proceedings,
pages 1020–1039. JMLR.org, 2014. URL http://proceedings.mlr.press/v35/mcmahan14.
html. (Cited on page 3)"
REFERENCES,0.4230769230769231,"Zakaria Mhammedi and Wouter M Koolen. Lipschitz and comparator-norm adaptivity in online
learning. In Conference on Learning Theory, pages 2858–2887. PMLR, 2020. (Cited on page 3)"
REFERENCES,0.425,"Zakaria Mhammedi, Wouter M Koolen, and Tim Van Erven. Lipschitz adaptivity with multiple
learning rates in online learning. In Conference on Learning Theory, pages 2490–2511. PMLR,
2019. (Cited on page 3)"
REFERENCES,0.4269230769230769,"Ryan Murray, Brian Swenson, and Soummya Kar. Revisiting normalized gradient descent: Fast
evasion of saddle points. IEEE Transactions on Automatic Control, 64(11):4818–4824, 2019. doi:
10.1109/TAC.2019.2914998. (Cited on page 5)"
REFERENCES,0.4288461538461538,"Yurii Nesterov. Universal gradient methods for convex optimization problems. Mathematical
Programming, 152(1-2):381–404, 2014. doi: 10.1007/s10107-014-0790-0. URL https://doi.
org/10.1007/s10107-014-0790-0. (Cited on pages 1, 2, and 4)"
REFERENCES,0.4307692307692308,"Yurii Nesterov. Lectures on Convex Optimization. Springer Publishing Company, Incorporated, 2nd
edition, 2018. ISBN 3319915770. (Cited on pages 4 and 5)"
REFERENCES,0.4326923076923077,"Francesco Orabona. Dimension-free exponentiated gradient. In C.J. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems,
volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_
files/paper/2013/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf.
(Cited on
page 3)"
REFERENCES,0.4346153846153846,"Francesco Orabona. Normalized gradients for all. arXiv preprint arXiv:2308.05621, abs/2308.05621,
2023. URL https://arXiv.org/abs/2308.05621. (Cited on pages 2 and 3)"
REFERENCES,0.43653846153846154,"Francesco Orabona and Ashok Cutkosky. ICML 2020 tutorial on parameter-free online optimiza-
tion. ICML Tutorials, 2020. URL https://parameterfree.com/icml-tutorial/. (Cited on
pages 1, 2, 3, 15, and 18)"
REFERENCES,0.43846153846153846,"Francesco Orabona and Dávid Pál. Coin betting and parameter-free online learning. In Proceedings
of the 30th International Conference on Neural Information Processing Systems, NIPS’16, page
577–585, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819. (Cited on
pages 2 and 3)"
REFERENCES,0.4403846153846154,"Francesco Orabona and Dávid Pál. Parameter-free stochastic optimization of variationally coherent
functions. arXiv preprint arXiv:2102.00236, abs/2102.00236, 2021. URL https://arXiv.org/
abs/2102.00236. (Cited on page 3)"
REFERENCES,0.4423076923076923,"Francesco Orabona and Tatiana Tommasi. Training deep networks without learning rates through coin
betting. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus,
S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA, pages 2160–2170, 2017. URL https://proceedings.neurips.
cc/paper/2017/hash/7c82fab8c8f89124e2ce92984e04fb40-Abstract.html.
(Cited on
page 3)"
REFERENCES,0.4442307692307692,"Antonio Orvieto, Simon Lacoste-Julien, and Nicolas Loizou. Dynamics of sgd with stochastic
polyak stepsizes: Truly adaptive variants and convergence to exact solution. arXiv preprint
arXiv:2205.04583, abs/2205.04583, 2022. URL https://arXiv.org/abs/2205.04583. (Cited
on page 3)"
REFERENCES,0.4461538461538462,"Yan Pan and Yuanzhi Li. Toward understanding why Adam converges faster than SGD for trans-
formers. OPT2023: 14th Annual Workshop on Optimization for Machine Learning, 2022. URL
https://openreview.net/pdf?id=Sf1NlV2r6PO. (Cited on page 9)"
REFERENCES,0.4480769230769231,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.
Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems 32,
pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf. (Cited on page 9)"
REFERENCES,0.45,"David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild,
David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training.
arXiv preprint arXiv:2104.10350, abs/2104.10350, 2021. URL https://arXiv.org/abs/2104.
10350. (Cited on page 1)"
REFERENCES,0.4519230769230769,"Boris Polyak. Introduction to optimization. Optimization Software, 1987. (Cited on pages 2 and 3)"
REFERENCES,0.45384615384615384,"Boris T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR
Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964. ISSN 0041-5553.
doi: https://doi.org/10.1016/0041-5553(64)90137-5. URL https://www.sciencedirect.com/
science/article/pii/0041555364901375. (Cited on page 1)"
REFERENCES,0.45576923076923076,"Or Sharir, Barak Peleg, and Yoav Shoham. The cost of training NLP models: a concise overview.
arXiv preprint arXiv:2004.08900, abs/2004.08900, 2020. URL https://arXiv.org/abs/2004.
08900. (Cited on page 1)"
REFERENCES,0.4576923076923077,"Naum Zuselevich Shor. Minimization methods for non-differentiable functions, volume 3. Springer
Science & Business Media, 2012. (Cited on page 5)"
REFERENCES,0.4596153846153846,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,
2015. URL http://arxiv.org/abs/1409.1556. (Cited on page 9)"
REFERENCES,0.46153846153846156,"Matthew Streeter and H. Brendan McMahan. No-regret algorithms for unconstrained online convex
optimization. In Proceedings of the 25th International Conference on Neural Information Process-
ing Systems - Volume 2, NIPS’12, page 2402–2410, Red Hook, NY, USA, 2012. Curran Associates
Inc. (Cited on page 3)"
REFERENCES,0.4634615384615385,"Cheik Traoré and Edouard Pauwels. Sequential convergence of AdaGrad algorithm for smooth
convex optimization. Operations Research Letters, 49(4):452–458, 2021. (Cited on page 4)"
REFERENCES,0.4653846153846154,"Rachel Ward, Xiaoxia Wu, and Léon Bottou. AdaGrad stepsizes: sharp convergence over nonconvex
landscapes.
In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the
36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA, volume 97 of Proceedings of Machine Learning Research, pages 6677–6686.
PMLR, 2019. URL http://proceedings.mlr.press/v97/ward19a.html. (Cited on page 4)"
REFERENCES,0.4673076923076923,"Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht.
The
marginal value of adaptive gradient methods in machine learning. In Isabelle Guyon, Ulrike
von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Ro-
man Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Confer-
ence on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA, pages 4148–4158, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
81b3833e2504647f9d794f7d7b9bf341-Abstract.html. (Cited on page 1)"
REFERENCES,0.46923076923076923,"Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: A theoretical justification for adaptivity. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020a.
URL https://openreview.net/forum?id=BJgnXpVYwS. (Cited on page 4)"
REFERENCES,0.47115384615384615,"Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J. Reddi,
Sanjiv Kumar, and Suvrit Sra.
Why are adaptive methods good for attention models?
In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin, editors, Advances in Neural Information Processing Systems 33:
Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-
12, 2020, virtual, 2020b.
URL https://proceedings.neurips.cc/paper/2020/hash/
b05b57f6add810d3b7490866d74c0053-Abstract.html. (Cited on page 4)"
OTHER,0.47307692307692306,Supplementary material
OTHER,0.475,Contents
OTHER,0.47692307692307695,"1
Introduction
1"
OTHER,0.47884615384615387,"2
Related Work
2"
OTHER,0.4807692307692308,"3
Algorithms and theory
4"
OTHER,0.4826923076923077,"3.1
Baselines: gradient descent and normalized gradient descent . . . . . . . . . . . .
4"
OTHER,0.4846153846153846,"3.2
DoWG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6"
OTHER,0.48653846153846153,"4
Experimental results
9"
OTHER,0.48846153846153845,"5
Algorithm-independent results
15"
OTHER,0.49038461538461536,"6
Proofs for DoWG
16"
OTHER,0.49230769230769234,"6.1
Smooth case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18"
OTHER,0.49423076923076925,"6.2
Nonsmooth case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20"
OTHER,0.49615384615384617,"7
Unconstrained domain extension
22"
OTHER,0.4980769230769231,"5
Algorithm-independent results"
OTHER,0.5,"In this section we collect different results that are algorithm-independent, the first is a consequence of
smoothness:
Fact 1. Suppose that f is smooth and lower bounded by f∗. Then for all x ∈Rd we have,"
OTHER,0.5019230769230769,∥∇f(x)∥2 ≤2L (f(x) −f∗) .
OTHER,0.5038461538461538,"Proof. This is a common result in the literature, and finds applications in convex and non-convex
optimization see e.g. (Levy, 2017; Levy et al., 2018; Orabona and Cutkosky, 2020; Khaled and
Richtárik, 2020). We include the proof for completeness. Let x ∈Rd and define x+ = x −1"
OTHER,0.5057692307692307,"L∇f(x).
Then by smoothness"
OTHER,0.5076923076923077,"f(x+) ≤f(x) + ⟨∇f(x), x+ −x⟩+ L"
OTHER,0.5096153846153846,2 ∥x+ −x∥2
OTHER,0.5115384615384615,= f(x) −1
OTHER,0.5134615384615384,L∥∇f(x)∥2 + 1
OTHER,0.5153846153846153,2L∥∇f(x)∥2
OTHER,0.5173076923076924,= f(x) −1
OTHER,0.5192307692307693,2L∥∇f(x)∥2.
OTHER,0.5211538461538462,Because f is lower bounded by f∗we thus have
OTHER,0.5230769230769231,f∗≤f(x+) ≤f(x) −1
OTHER,0.525,2L∥∇f(x)∥2.
OTHER,0.5269230769230769,Rearranging gives ∥∇f(x)∥2 ≤2L (f(x) −f∗).
OTHER,0.5288461538461539,"The next two results are helpful algebraic identities that will be useful for the proof of DoWG.
Lemma 1. (Ivgi et al., 2023, Lemma 4). Let a0, .., at be a nondecreasing sequence of nonnegative
numbers. Then
t
X k=1"
OTHER,0.5307692307692308,"ak −ak−1
√ak
≤2 (√at −√a0) ."
OTHER,0.5326923076923077,"Proof. This is (Ivgi et al., 2023, Lemma 4). We include the proof for completeness: t
X k=1"
OTHER,0.5346153846153846,"ak −ak−1
√ak
= t
X k=1"
OTHER,0.5365384615384615,"√ak −√ak−1

(√ak + √ak−1)
√ak ≤2 t
X k=1"
OTHER,0.5384615384615384,"√ak −√ak−1
"
OTHER,0.5403846153846154,= 2 (√at −√a0) .
OTHER,0.5423076923076923,"Lemma 2. ((Ivgi et al., 2023, Lemma 3), similar to (Defazio and Mishchenko, 2023, Lemma 11)).
Let s0, s1, . . . , sT be a positive increasing sequence. Then"
OTHER,0.5442307692307692,"max
t≤T X i<t"
OTHER,0.5461538461538461,"si
st
≥1 e"
OTHER,0.5480769230769231,"
T
log+(sT /s0) −1

,"
OTHER,0.55,"where log+ x
def
= log x + 1."
OTHER,0.551923076923077,"Proof. This is (Ivgi et al., 2023, Lemma 3). We include the proof for completeness: Define K =
⌈log sT"
OTHER,0.5538461538461539,"s0 ⌉and n =
 T"
OTHER,0.5557692307692308,"K

. Then,"
OTHER,0.5576923076923077,"log
sT s0 
≥ K−1
X"
OTHER,0.5596153846153846,"k=0
log
sn(k+1) snk"
OTHER,0.5615384615384615,"
≥K min
k<K log sn(k+1) snk
."
OTHER,0.5634615384615385,Rearranging and using K = ⌈log sT
OTHER,0.5653846153846154,s0 ⌉gives
OTHER,0.5673076923076923,"min
k<K log sn(k+1)"
OTHER,0.5692307692307692,"snk
≤
log sT"
OTHER,0.5711538461538461,"s0
K
≤1."
OTHER,0.573076923076923,"Therefore,"
OTHER,0.575,"min
k<K
sn(k+1)"
OTHER,0.5769230769230769,"snk
≤e. Thus,"
OTHER,0.5788461538461539,"max
t≤T X i≤t"
OTHER,0.5807692307692308,"si
st
≥max
t≤T nst−n st"
OTHER,0.5826923076923077,"≥max
k≤K nsn(k−1)"
OTHER,0.5846153846153846,"snk
≥e−1n"
OTHER,0.5865384615384616,"= e−1
 T K "
OTHER,0.5884615384615385,"≥e−1
 T"
OTHER,0.5903846153846154,"K −1
 ≥e−1  
T"
OTHER,0.5923076923076923,"log

sT s0"
OTHER,0.5942307692307692,"
+ 1
−1  ."
OTHER,0.5961538461538461,"6
Proofs for DoWG"
OTHER,0.5980769230769231,"This section collects proofs for DoWG. First, we give the following lemma, which holds under
convexity alone (regardless of whether f is smooth or Lipschitz)."
OTHER,0.6,"Lemma 3. Suppose that f is convex and has minimizer x∗. For the iterations generated by Algo-
rithm 1, we have t−1
X"
OTHER,0.6019230769230769,"k=0
¯r2
k ⟨∇f(xk), xk −x∗⟩≤2¯rt
 ¯dt + ¯rt
 √vt−1,
(6)"
OTHER,0.6038461538461538,where ¯dt = maxk≤t dk.
OTHER,0.6057692307692307,"Proof. This proof follows the proof of DoG (Ivgi et al., 2023, Lemma 1), itself a modification of
the standard proof for adaptive cumulative gradient normalization methods (Gupta et al., 2017)
incorporating insights from (Carmon and Hinder, 2022). We specifically modify the proof to handle
the weighting scheme we use in DoWG. By the nonexpansivity of the projection we have"
OTHER,0.6076923076923076,"d2
k+1 = ∥xk+1 −x∗∥2"
OTHER,0.6096153846153847,≤∥xk −ηk∇f(xk) −x∗∥2
OTHER,0.6115384615384616,"= ∥xk −x∗∥2 −2ηk ⟨∇f(xk), xk −x∗⟩+ η2
k∥∇f(xk)∥2"
OTHER,0.6134615384615385,"= d2
k −2ηk ⟨∇f(xk), xk −x∗⟩+ η2
k∥∇f(xk)∥2."
OTHER,0.6153846153846154,Rearranging and dividing by 2ηk we get
OTHER,0.6173076923076923,"⟨∇f(xk), xk −x∗⟩≤d2
k −d2
k+1
2ηk
+ ηk"
OTHER,0.6192307692307693,2 ∥∇f(xk)∥2.
OTHER,0.6211538461538462,"Multiplying both sides by ¯r2
k we get"
OTHER,0.6230769230769231,"¯r2
k ⟨∇f(xk), xk −x∗⟩≤1"
OTHER,0.625,"2
¯r2
k
ηk"
OTHER,0.6269230769230769,"
d2
k −d2
k+1

+ 1"
OTHER,0.6288461538461538,"2 ¯r2
kηk∥∇f(xk)∥2."
OTHER,0.6307692307692307,"Summing up as k varies from 0 to t −1 we get t−1
X"
OTHER,0.6326923076923077,"k=0
¯r2
k ⟨∇f(xk), xk −x∗⟩≤1 2"
OTHER,0.6346153846153846,"""t−1
X k=0"
OTHER,0.6365384615384615,"¯r2
k
ηk"
OTHER,0.6384615384615384,"
d2
k −d2
k+1

#"
OTHER,0.6403846153846153,"|
{z
}
(A) +1 2"
OTHER,0.6423076923076924,"""t−1
X"
OTHER,0.6442307692307693,"k=0
¯r2
kηk∥∇f(xk)∥2
#"
OTHER,0.6461538461538462,"|
{z
}
(B) .
(7)"
OTHER,0.6480769230769231,"We shall now bound each of the terms (A) and (B). We have (A) = t−1
X k=0"
OTHER,0.65,"¯r2
k
ηk"
OTHER,0.6519230769230769,"
d2
k −d2
k+1
 = t−1
X k=0"
OTHER,0.6538461538461539,"√vk

d2
k −d2
k+1

(8)"
OTHER,0.6557692307692308,"= d2
0
√v0 −d2
t
√vt−1 + t−1
X"
OTHER,0.6576923076923077,"k=1
d2
k
√vk −√vk−1

(9)"
OTHER,0.6596153846153846,"≤¯d2
t
√v0 −d2
t
√vt−1 + ¯d2
t t−1
X k=1"
OTHER,0.6615384615384615,"√vk −√vk−1

(10)"
OTHER,0.6634615384615384,"= √vt−1
 ¯d2
t −d2
t

(11)"
OTHER,0.6653846153846154,"≤4¯rt ¯dt
√vt−1,
(12)"
OTHER,0.6673076923076923,"where eq. (8) holds by definition of the DoWG stepsize ηk, eq. (9) holds by telescoping, eq. (10)
holds because vk = vk−1 + ¯r2
k∥∇f(xk)∥2 ≥vk−1 and hence √vk ≥√vk−1, and d2
k ≤¯d2
t by
definition. Equation (11) just follows by telescoping. Finally observe that ¯d2
t −d2
t = d2
s −d2
t for
some s ∈[t], and d2
s −d2
t = (ds −dt)(ds +dt). Then by the triangle inequality and that the sequence"
OTHER,0.6692307692307692,¯rk is monotonically nondecreasing we have
OTHER,0.6711538461538461,"ds −dt = ∥xs −x∗∥−∥xt −x∗∥
≤∥xs −xt∥
≤∥xs −x0∥+ ∥xt −x0∥
= rs + rt
≤¯rs + ¯rt
≤2¯rt."
OTHER,0.6730769230769231,"Therefore d2
s −d2
t ≤(¯rs + ¯rt)(ds + dt) ≤4¯rt ¯dt. This explains eq. (12)."
OTHER,0.675,"For the second term in eq. (7), we have (B) = t−1
X"
OTHER,0.676923076923077,"k=0
¯r2
kηk∥∇f(xk)∥2 = t−1
X k=1"
OTHER,0.6788461538461539,"¯r4
k
√vk
∥∇f(xk)∥2"
OTHER,0.6807692307692308,"= r2
0
√v0 + t−1
X k=1"
OTHER,0.6826923076923077,"¯r4
k
√vk
∥∇f(xk)∥2"
OTHER,0.6846153846153846,"≤¯r2
t
√v0 + ¯r2
t t−1
X k=1"
OTHER,0.6865384615384615,"¯r2
k∥∇f(xk)∥2 √vk"
OTHER,0.6884615384615385,"= ¯r2
t
√v0 + ¯r2
t t−1
X k=1"
OTHER,0.6903846153846154,"vk −vk−1
√vk"
OTHER,0.6923076923076923,"= ¯r2
t
√v0 + ¯r2
t t−1
X k=1"
OTHER,0.6942307692307692,"vk −vk−1
√vk"
OTHER,0.6961538461538461,"≤¯r2
t
√v0 + 2¯r2
t
√vt−1 −√v0

(13)"
OTHER,0.698076923076923,"= 2¯r2
t
√vt−1.
(14)"
OTHER,0.7,"where eq. (13) is by Lemma 1. Plugging eqs. (12) and (14) in eq. (7) gives t−1
X"
OTHER,0.7019230769230769,"k=0
¯r2
k ⟨∇f(xk), xk −x∗⟩≤2¯rt ¯dt
√vt−1 + ¯r2
t
√vt−1"
OTHER,0.7038461538461539,"≤2¯rt
 ¯dt + ¯rt
 √vt−1."
OTHER,0.7057692307692308,"6.1
Smooth case"
OTHER,0.7076923076923077,"We now prove the convergence of DoWG under smoothness. In particular, we shall use Fact 1 and
the DoWG design to bound the weighted cumulative error St = Pt−1
k=0 r2
k [f(xk) −f∗] by its square
root multiplied by a problem-dependent constant. We note that a similar trick is used in the analysis
of AdaGrad-Norm (Levy et al., 2018), in reductions from online convex optimization to stochastic
smooth optimization (Orabona and Cutkosky, 2020), and in the method of (Carmon and Hinder,
2022). However, in all the mentioned cases, the unweighted error Mt = Pt−1
k=0 [f(xk) −f∗] is
bounded by its square root. Here, DoWG’s design allows us to bound the weighted errors St instead."
OTHER,0.7096153846153846,"Proof of Theorem 4. We start with Lemma 3. Let t ∈[T]. By eq. (6) we have t−1
X"
OTHER,0.7115384615384616,"k=0
¯r2
k ⟨∇f(xk), xk −x∗⟩≤2¯rt
 ¯dt + ¯rt
 √vt−1.
(15)"
OTHER,0.7134615384615385,Observe that by convexity we have
OTHER,0.7153846153846154,"⟨∇f(xk), xk −x∗⟩≥f(xk) −f∗."
OTHER,0.7173076923076923,"Using this to lower bound the left-hand side of eq. (15) gives t−1
X"
OTHER,0.7192307692307692,"k=0
¯r2
k [f(xk) −f∗] ≤ t−1
X"
OTHER,0.7211538461538461,"k=0
¯r2
k ⟨∇f(xk), xk −x∗⟩"
OTHER,0.7230769230769231,"≤2¯rt
 ¯dt + ¯rt
 √vt−1.
(16)"
OTHER,0.725,"We have by smoothness that ∥∇f(x)∥2 ≤2L(f(x) −f∗) for all x ∈Rd, therefore"
OTHER,0.7269230769230769,"vt−1 = t−1
X"
OTHER,0.7288461538461538,"k=0
¯r2
k∥∇f(xk)∥2 ≤2L t−1
X"
OTHER,0.7307692307692307,"k=0
¯r2
k [f(xk) −f∗] ."
OTHER,0.7326923076923076,Taking square roots we get
OTHER,0.7346153846153847,"√vt−1 ≤
√"
OTHER,0.7365384615384616,2L
OTHER,0.7384615384615385,"v
u
u
t t−1
X"
OTHER,0.7403846153846154,"k=0
¯r2
k [f(xk) −f∗].
(17)"
OTHER,0.7423076923076923,"Using eq. (17) in eq. (16) gives t−1
X"
OTHER,0.7442307692307693,"k=0
¯r2
k [f(xk) −f∗] ≤2
√"
OTHER,0.7461538461538462,"2L¯rt
 ¯dt + ¯rt

v
u
u
t t−1
X"
OTHER,0.7480769230769231,"k=0
¯r2
k [f(xk) −f∗]."
OTHER,0.75,"If f(xk) −f∗= 0 for some k ∈[t −1] then the statement of the theorem is trivial. Otherwise, we
can divide both sides by the latter square root to get
v
u
u
t t−1
X"
OTHER,0.7519230769230769,"k=0
¯r2
k [f(xk) −f∗] ≤2
√"
OTHER,0.7538461538461538,"2L¯rt
 ¯dt + ¯rt

."
OTHER,0.7557692307692307,"Squaring both sides gives t−1
X"
OTHER,0.7576923076923077,"k=0
¯r2
k [f(xk) −f∗] ≤8L¯r2
t
 ¯dt + ¯rt
2 ."
OTHER,0.7596153846153846,"Dividing both sides by Pt−1
k=0 ¯r2
k we get"
OTHER,0.7615384615384615,"1
Pt−1
k=0 ¯r2
k t−1
X"
OTHER,0.7634615384615384,"k=0
¯r2
k [f(xk) −f∗] ≤8L¯r2
t
 ¯dt + ¯rt
2
Pt−1
k=0 ¯r2
k"
OTHER,0.7653846153846153,"= 8L
 ¯dt + ¯rt
2"
OTHER,0.7673076923076924,"Pt−1
k=0
¯r2
k
¯r2
t ."
OTHER,0.7692307692307693,By convexity we have
OTHER,0.7711538461538462,"f(¯xt) −f∗≤
1
Pt−1
k=0 ¯r2
k t−1
X"
OTHER,0.7730769230769231,"k=0
¯r2
k [f(xk) −f∗]"
OTHER,0.775,"≤8L
 ¯dt + ¯rt
2"
OTHER,0.7769230769230769,"Pt−1
k=0
¯r2
k
¯r2
t"
OTHER,0.7788461538461539,".
(18)"
OTHER,0.7807692307692308,"By Lemma 2 applied to the sequence sk = ¯r2
k we have that for some t ∈[T] t−1
X k=0"
OTHER,0.7826923076923077,"¯r2
k
¯r2
t
≥1 e  
T"
OTHER,0.7846153846153846,"log+
¯r2
T
¯r2
0 −1  ."
OTHER,0.7865384615384615,"Because X has diameter D we have ¯r2
T ≤D2 and therefore t−1
X k=0"
OTHER,0.7884615384615384,"¯r2
k
¯r2
t
≥1 e"
OTHER,0.7903846153846154,"T
log+
D2"
OTHER,0.7923076923076923,"¯r2
0
−1 !"
OTHER,0.7942307692307692,".
(19)"
OTHER,0.7961538461538461,We now have two cases:
OTHER,0.7980769230769231,"• If T ≥2 log+
D2"
OTHER,0.8,"¯r2
0 then
T
log D2"
OTHER,0.801923076923077,"¯r2
0
−1 ≥
T
2 log D2"
OTHER,0.8038461538461539,"¯r2
0
and we use this in eqs. (18) and (19) to get"
OTHER,0.8057692307692308,"f(¯xt) −f∗≤16eL
 ¯dt + ¯rt
2"
OTHER,0.8076923076923077,"T
log ¯r2
T
¯r2
0
."
OTHER,0.8096153846153846,"Observe that because X has diameter at most D we have ¯dt + ¯rt ≤2D, therefore"
OTHER,0.8115384615384615,f(¯xt) −f∗≤64eLD2
OTHER,0.8134615384615385,"T
log+
¯r2
T
¯r2
0
= O
LD2"
OTHER,0.8153846153846154,"T
log+
D
¯r0 
."
OTHER,0.8173076923076923,"• If T < 2 log+
D2"
OTHER,0.8192307692307692,"¯r2
0 , then 1 <
2 log+
D2"
OTHER,0.8211538461538461,"¯r2
0
T
. Let t ∈[T]. Using smoothness and this fact we have"
OTHER,0.823076923076923,f(¯xt) −f∗≤L
OTHER,0.825,2 ∥¯xt −x∗∥2 ≤L∥¯xt −x∗∥2
OTHER,0.8269230769230769,"T
log+
D2"
OTHER,0.8288461538461539,"¯r2
0
."
OTHER,0.8307692307692308,"Observe ¯xt, x∗∈X and X has diameter D, hence ∥¯xt −x∗∥2 ≤D2 and we get"
OTHER,0.8326923076923077,f(¯xt) −f∗≤LD2
OTHER,0.8346153846153846,"T
log+
D2"
OTHER,0.8365384615384616,"¯r2
0
= O
LD2"
OTHER,0.8384615384615385,"T
log+
D
¯r0 
."
OTHER,0.8403846153846154,"Thus in both cases we have that f(¯xt) −f∗= O

LD2"
OTHER,0.8423076923076923,"T
log+
D
¯r0"
OTHER,0.8442307692307692,"
, this completes our proof."
OTHER,0.8461538461538461,"6.2
Nonsmooth case"
OTHER,0.8480769230769231,We now give the proof of DoWG’s convergence when f is Lipschitz.
OTHER,0.85,"Proof of Theorem 3. We start with Lemma 3. Let t ∈[T]. By eq. (6) we have t−1
X"
OTHER,0.8519230769230769,"k=0
¯r2
k ⟨∇f(xk), xk −x∗⟩≤2¯rt
 ¯dt + ¯rt
 √vt−1.
(20)"
OTHER,0.8538461538461538,Observe that by convexity we have
OTHER,0.8557692307692307,"⟨∇f(xk), xk −x∗⟩≥f(xk) −f∗."
OTHER,0.8576923076923076,"Using this to lower bound the left-hand side of eq. (20) gives t−1
X"
OTHER,0.8596153846153847,"k=0
¯r2
k [f(xk) −f∗] ≤ t−1
X"
OTHER,0.8615384615384616,"k=0
¯r2
k ⟨∇f(xk), xk −x∗⟩"
OTHER,0.8634615384615385,"≤2¯rt
 ¯dt + ¯rt
 √vt−1.
(21)"
OTHER,0.8653846153846154,"We have by the fact that f is G-Lipschitz that ∥∇f(x)∥2 ≤G2 for all x ∈X. Therefore,"
OTHER,0.8673076923076923,"vt−1 = t−1
X"
OTHER,0.8692307692307693,"k=0
¯r2
k∥∇f(xk)∥2"
OTHER,0.8711538461538462,"≤¯r2
t t−1
X"
OTHER,0.8730769230769231,"k=0
∥∇f(xk)∥2"
OTHER,0.875,"≤¯r2
t G2T."
OTHER,0.8769230769230769,"Taking square roots and plugging into eq. (21) gives t−1
X"
OTHER,0.8788461538461538,"k=0
¯r2
k [f(xk) −f∗] ≤2¯r2
t
 ¯dt + ¯rt

G
√ T."
OTHER,0.8807692307692307,"Dividing both sides by Pt−1
k=0 ¯r2
k we get"
OTHER,0.8826923076923077,"1
Pt−1
k=0 ¯r2
k t−1
X"
OTHER,0.8846153846153846,"k=0
¯r2
k [f(xk) −f∗] ≤2
 ¯dt + ¯rt

G
√"
OTHER,0.8865384615384615,"T
Pt−1
k=0
¯r2
k
¯r2
t"
OTHER,0.8884615384615384,".
(22)"
OTHER,0.8903846153846153,"By Lemma 2 applied to the sequence sk = ¯r2
k we have that for some t ∈[T] t−1
X k=0"
OTHER,0.8923076923076924,"¯r2
k
¯r2
t
≥1 e  
T"
OTHER,0.8942307692307693,"log+
¯r2
T
¯r2
0 −1  ."
OTHER,0.8961538461538462,"Because ¯rT ≤D we further have t−1
X k=0"
OTHER,0.8980769230769231,"¯r2
k
¯r2
t
≥1 e"
OTHER,0.9,"T
log+
D2"
OTHER,0.9019230769230769,"¯r2
0
−1 !"
OTHER,0.9038461538461539,".
(23)"
OTHER,0.9057692307692308,We now have two cases:
OTHER,0.9076923076923077,"• If T ≥2 log+
D2"
OTHER,0.9096153846153846,"¯r2
0 : then
T"
OTHER,0.9115384615384615,"log
¯r2
T
¯r2
0
−1 ≥
T"
OTHER,0.9134615384615384,"2 log
¯r2
T
¯r2
0
. We can use this in eq. (22) alongside eq. (23) and"
OTHER,0.9153846153846154,"the fact that log+ x2 = max(log x2, 1) = max(2 log x, 1) ≤2 log+ x to get"
OTHER,0.9173076923076923,"1
Pt−1
k=0 ¯r2
k t−1
X"
OTHER,0.9192307692307692,"k=0
¯r2
k [f(xk) −f∗] ≤8
 ¯dt + ¯rt

G
√"
OTHER,0.9211538461538461,"T
log+
D
¯r0
."
OTHER,0.9230769230769231,"Because the diameter of X is bounded by D we have ¯rT ≤D and ¯dt ≤D, using this and
convexity we get"
OTHER,0.925,"f(¯xt) −f∗≤
1
Pt−1
k=0 ¯r2
k t−1
X"
OTHER,0.926923076923077,"k=0
¯r2
k [f(xk) −f∗]"
OTHER,0.9288461538461539,"≤8
 ¯dt + ¯rt

G
√"
OTHER,0.9307692307692308,"T
log ¯rT ¯r0"
OTHER,0.9326923076923077,"≤16DG
√"
OTHER,0.9346153846153846,"T
log D r0
."
OTHER,0.9365384615384615,"• If T < 2 log+
D2"
OTHER,0.9384615384615385,"¯r2
0 : then"
OTHER,0.9403846153846154,"1 <
2 log+
D2"
OTHER,0.9423076923076923,"¯r2
0
T
≤
4 log+
D
¯r0
T
.
(24)"
OTHER,0.9442307692307692,"By convexity and Cauchy-Schwartz we have
f(¯xt) −f∗≤⟨∇f(¯xt), ¯xt −x∗⟩
≤∥∇f(¯xt)∥∥¯xt −x∗∥.
(25)
Because f is G-Lipschitz then ∥∇f(¯xt)∥≤G and because X has diameter D we have
∥¯xt −x∗∥≤D. Using this and eq. (24) in eq. (25) gives
f(¯xt) −f∗≤DG"
OTHER,0.9461538461538461,"<
4DG log+
D
¯r0
T
."
OTHER,0.948076923076923,"Now because T ≥1 we have
√"
OTHER,0.95,T ≤T and hence
OTHER,0.9519230769230769,"f(¯xt) −f∗≤
4DG log+
D
¯r0
√ T
."
OTHER,0.9538461538461539,"In both cases, we have that f(¯xt) −f∗= O

4DG log+
D
¯r0
√ T"
OTHER,0.9557692307692308,"
, and this completes our proof."
OTHER,0.9576923076923077,"7
Unconstrained domain extension"
OTHER,0.9596153846153846,"In this section we consider the case where the domain set is unbounded, and we seek dependence
only on d0 = ∥x0 −x∗∥. We use the same technique for handling the unconstrained problem as (Ivgi
et al., 2023) in this section and consider DoWG iterates with the reduced stepsizes"
OTHER,0.9615384615384616,"ηt =
¯r2
t
√vt log 2vt"
OTHER,0.9634615384615385,"v0
vt = vt−1 + ¯r2
t ∥∇f(xt)∥2.
(26)"
OTHER,0.9653846153846154,"We prove that with this stepsize, the iterates do not venture far from the initialization. The proof
follows (Ivgi et al., 2023).
Lemma 4. (Stability). For the iterates xt+1 = xt −ηt∇f(xt) following the stepsize scheme given
by (26) we have ¯d2
t ≤12d0 and ¯r2
t ≤32d2
0 provided that r0 ≤d0."
OTHER,0.9673076923076923,Proof. By expanding the square and convexity
OTHER,0.9692307692307692,"d2
k+1 −d2
k ≤η2
t ∥gt∥2."
OTHER,0.9711538461538461,"Summing up as t varies from k = 1 to k = t and using (Ivgi et al., 2023, Lemma 6)"
OTHER,0.9730769230769231,"d2
t −d2
1 ≤ t
X k=1"
OTHER,0.975,"¯r4
k
vk"
OTHER,0.9769230769230769,∥∇f(xk)∥2
OTHER,0.9788461538461538,4 log2 2vk
OTHER,0.9807692307692307,"v0
≤¯r2
t
4 t
X k=1"
OTHER,0.9826923076923076,"vk −vk−1
vk log2
+
vk
v0
≤¯r2
t
4 ."
OTHER,0.9846153846153847,"Therefore we have d2
t ≤d2
1 + ¯r2
t
4 . Now suppose for the sake of induction that ¯r2
t ≤8d2
1, then applying
the last equation we get d2
t ≤3d2
1. Taking square roots gives dt ≤
√"
OTHER,0.9865384615384616,"3d1. By the triangle inequality
we then get"
OTHER,0.9884615384615385,"∥xt+1 −x0∥≤∥xt+1 −x∗∥+ ∥x∗−x0∥≤(1 +
√ 3)d1."
OTHER,0.9903846153846154,"Squaring both sides gives ∥xt+1 −x0∥2 ≤(1 +
√"
OTHER,0.9923076923076923,"3)2d2
1 ≤8d2
1. This completes our induction and
we have ¯r2
t ≤8d2
1 for all t. Finally, observe that"
OTHER,0.9942307692307693,d1 = ∥x1 −x∗∥≤∥x1 −x0∥+ ∥x0 −x∗∥= rϵ + d0 ≤2d0.
OTHER,0.9961538461538462,"It follows that ¯r2
t ≤8d2
1 ≤32d2
0 for all t. Finally, we have d2
t ≤d2
1 + ¯r2
t
4 ≤3d2
1 ≤12d2
0. This
completes our proof."
OTHER,0.9980769230769231,"Therefore the iterates stay bounded. The rest of the proof then follows Theorems 3 and 4 and is
omitted for simplicity.. In both cases it gives the same results with D0 = ∥x0 −x∗∥instead of D, up
to extra constants and polylogarithmic factors."
