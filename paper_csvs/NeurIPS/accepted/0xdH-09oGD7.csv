Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002688172043010753,"In this paper, we study both multi-armed and contextual bandit problems in cen-
sored environments. Our goal is to estimate the performance loss due to censorship
in the context of classical algorithms designed for uncensored environments. Our
main contributions include the introduction of a broad class of censorship models
and their analysis in terms of the effective dimension of the problem – a natural
measure of its underlying statistical complexity and main driver of the regret bound.
In particular, the effective dimension allows us to maintain the structure of the
original problem at ﬁrst order, while embedding it in a bigger space, and thus
naturally leads to results analogous to uncensored settings. Our analysis involves a
continuous generalization of the Elliptical Potential Inequality, which we believe
is of independent interest. We also discover an interesting property of decision-
making under censorship: a transient phase during which initial misspeciﬁcation
of censorship is self-corrected at an extra cost; followed by a stationary phase that
reﬂects the inherent slowdown of learning governed by the effective dimension.
Our results are useful for applications of sequential decision-making models where
the feedback received depends on strategic uncertainty (e.g., agents’ willingness to
follow a recommendation) and/or random uncertainty (e.g., loss or delay in arrival
of information)."
INTRODUCTION,0.005376344086021506,"1
Introduction"
INTRODUCTION,0.008064516129032258,"Bandit problems are prototypical models of sequential decision-making under uncertainty. They
are widely studied due to their applications in recommender systems, online advertising, medical
treatment assignment, revenue management, network routing and control [26, 39]. Our work is
motivated by settings in which the feedback received by the decision-maker in each round of decision
is censored by a stochastic process that depends on the current action as well as past history of
feedbacks and actions. For instance, in typical missing data problems, the decision-maker needs to
deal with frequent losses of information (or delays in arrival of information) due to exogeneous failures
such as faulty and/or unreliable communication. Missing observations in dynamical interactions
with the environment are a common concern in diverse ﬁelds ranging from operations management
to health sciences to physical sciences [45, 20, 29]. In other settings, such as AI-driven platforms
for health alerts, route guidance, and product recommendations [7, 46], the reception of feedback
depends on whether or not the decision (or recommendation) is adopted by strategic agents (e.g.
patients, customers or drivers) with private valuations. Thus, from the platform’s viewpoint, the
adoption behavior of heterogeneous agents can be regarded as a stochastic censorship process."
INTRODUCTION,0.010752688172043012,"⇤Work done prior to joining Amazon.
†Dept. of Civil and Environmental Engineering & Laboratory for Information and Decision Systems.
‡Dept. of Electrical Engineering and Computer Science & Laboratory for Information and Decision Systems."
INTRODUCTION,0.013440860215053764,"In static environments, the bias induced by the presence of randomly missing information has been
thoroughly studied [29, 33]. However, in online settings, the dynamics of learning and acting are
inherently coupled: since censorship mediates current information of the environment, it impacts
the outcome of data-driven decision process; this in turn conditions the future decisions and future
censored feedback, creating a complex and endogenous joint temporal dependency. Our work
contributes to the analysis of such phenomena for a broad classes of decision and censorship models.
Importantly, it is the ﬁrst normative inquiry of how censorship impacts the statistical complexity of
bandit problems. We develop an analysis approach that is useful for both estimating the performance
loss due to censorship and reﬁning the classical algorithms designed for uncensored environments."
RELATED WORK,0.016129032258064516,"1.1
Related Work"
RELATED WORK,0.01881720430107527,"Within the extensive bandits literature, well-surveyed in [26, 39], our work is most closely related
to stochastic delayed bandits. Initially, this line of work focused on the joint evolution of actions
and information in settings where the reception of the latter is delayed [15]. Of particular interest is
the packet loss model recently introduced in [24], which provides the regret bound O( 1"
RELATED WORK,0.021505376344086023,"pRT ) where
RT is the uncensored regret and p the censorship probability. Analogous results have been shown
in the context of Combinatorial Multi-Armed Bandits with probabilistically triggered arms; see for
example, [11] and [43]. Our work provides a systematic approach to study more general censorship
models, and sheds light on how the impact of coupled feedback and censorship realizations on the
expected regret can be evaluated in terms of the effective dimension of the problem."
RELATED WORK,0.024193548387096774,"Importantly, we also tackle the contextual bandit problems, where relatively few results are available
on the regret under missing or censored feedback. A notable exception is the work of [42], who
focus on a different information structure and obtain a scaling of 1/p (see Remark 4.4). A related
contribution by [2] provides both a potential-based analysis of the Upper Conﬁdence Bound algorithm
(UCB) for multi-armed bandits and an algorithmic variant leveraging the Kaplan-Meier estimator,
although their censorship setting is different than ours. In particular, our results are applicable to
settings when delay is signiﬁcantly large (possibly inﬁnite). This is in contrast to prior results on
bandits with delayed information structure which assume either that the delay is constant, upper
bounded, has a ﬁnite mean, or simply provide regret guarantees that are linear in the cumulative
delays up to time T [15, 22, 30, 47, 34]. Under such assumptions on delay, one usually gets a second
order additive dependency of the regret in terms of delay parameters, which practically says that delay
is benign for bandits. On the other hand, we show that censorship leads to a ﬁrst order multiplicative
dependency on regret and we provide a complete characterization of this dependency for a wide range
of bandits and censorship models."
RELATED WORK,0.026881720430107527,"Moreover, the abovementioned works primarily focus on modifying well-known bandit algorithms
to account for delays, or propose new delay-robust algorithms which may be difﬁcult to implement
in practice; a notable exception includes [44] but it focuses on Thompson Sampling. In our work,
we instead focus on estimating the performance loss due to censorship and derive insights on the
behavior of well-known UCB class of algorithms [27, 13, 1]. These algorithms are widely used in
practice; moreover, their theoretical study has been shown to be useful for analysis of broader class
of algorithms (notably Thompson Sampling [3, 36] and Information-Directed Sampling [37, 23])."
RELATED WORK,0.02956989247311828,"On a somewhat related note, the literature on non-stochastic multi-armed bandit problems with
delays [32, 9, 21] also tackles multiplicative dependency, although in a different setting than ours.
Another related line of work is Partial Monitoring [6, 25] which deals with generic categorization of
learnability, rather than a ﬁne-grained analysis of dimensionality in relation to censorship, which is
our current focus."
RELATED WORK,0.03225806451612903,"Our work contributes to the Generalized Linear Contextual Bandits literature [16, 28] in two ways:
ﬁrstly, through the use of these models in a sequential decision-making framework on which the
impact of censorship is assessed in Sec. 4. Secondly, by showing that our multi-threshold censorship
model MT induces, at ﬁrst order, a non-linear structure that closely mirrors such models. Our results
provide new tools to study this structure. It is useful to note that the notion of effective dimension has
been well-studied in the statistical learning and kernels literature [40, 41] (where it is deﬁned for a
Gram matrix Kn and regularization λ as dn"
RELATED WORK,0.03494623655913978,"eff(λ) = tr(Kn(Kn + λId)−1)). Our work shows that an
analogous quantity governs the regret bound of bandit problems in censored settings."
RELATED WORK,0.03763440860215054,"Finally, there is a rich literature on classical missing and censored data problems [29, 33]. Although
conditional on the choice of a given action the missing data/censorship process we study is an instance
of missing-completely-at-random (MCAR), the online action generating process adds a signiﬁcant
difﬁculty to the problem: whereas MCAR is typically studied under a well-deﬁned distributional
assumption (e.g. i.i.d. generation of action), our problem needs to deal with adaptive (hence non
i.i.d.) data generation process with respect to the ﬁltration of past information. In particular, the
structure of missing data set results from strong endogenous dependencies with past realization of the
censorship (see Sec. 2)."
SUMMARY OF RESULTS,0.04032258064516129,"1.2
Summary of Results"
SUMMARY OF RESULTS,0.043010752688172046,"In Sec. 3, we consider Multi-Armed Bandit (MAB) models and prove that the regret scales as"
SUMMARY OF RESULTS,0.0456989247311828,˜O(de↵ p
SUMMARY OF RESULTS,0.04838709677419355,"T) (Thm. 3.1), where de↵is the effective dimension with value P a2[d]"
SUMMARY OF RESULTS,0.051075268817204304,"1
pa . In doing so, we
recover and generalize related results from [24, 11] to more complex regularized settings and noise
models. In particular, we prove that the effective dimension results from characterizing the so-called
censored cumulative potential V↵. Interestingly, we also show that the adaptive nature of censorship
on V↵plays only a second order role (Prop. 3.7), that is, impact of censorship can be treated in an
ofﬂine manner at ﬁrst order."
SUMMARY OF RESULTS,0.053763440860215055,"Importantly, our study of MAB under censorship instantiates an analysis framework which extends
to Linear Contextual Bandits (LCB) (Sec. 4). Our main result provides that regret is still governed
by the effective dimension, but now with a dependency of ˜O(σ p"
SUMMARY OF RESULTS,0.056451612903225805,d · de↵ p
SUMMARY OF RESULTS,0.05913978494623656,"T) (Thm. 4.1). To the
best of our knowledge, these regret bounds provide the ﬁrst theoretical characterization in LCB with
censorship, and contribute to the literature by evaluating the impact of censorship on the performance
of UCB-type algorithms. Our second main contribution is identifying the effective dimension for
a broad class of multi-threshold models MT as well as a precise understanding of the dynamic
behavior induced by these models (Thm. 4.6). In particular, we ﬁnd that censorship introduces a two-
phase behavior: a transient phase during which the initial censoring misspeciﬁcation is self-corrected
at an additional cost; followed by a stationary phase that reﬂects the inherent slowdown of learning
governed by the effective dimension. In extending our analysis from MAB to LCB, we also develop
a continuous generalization of the widely used Elliptical Potential Inequality (Prop. 4.3), which we
believe is also of independent interest. Finally, our results (Thm. 3.1 and Prop. 3.2 for MAB and
Thm. 4.1 for LCB) suggest that the UCB class of algorithms is indeed a reliable method for stochastic
bandits problems under censorship."
PROBLEM SETUP AND BACKGROUND,0.06182795698924731,"2
Problem Setup and Background"
PROBLEM SETUP AND BACKGROUND,0.06451612903225806,"Bandit Model:
We successively consider stochastic multi-armed bandits (Sec. 3) and Linear
Contextual Bandits (LCB) (Sec. 4) in censored environments. In both settings, at each round t T,
the agent observes an action set At ⇢A. She then selects an action at 2 At (i.e. an arm) to
which a noisy feedback r(at) + ✏t is associated, where r(at) is a bounded reward and ✏t is an i.i.d.
sub-Gaussian noise of pseudo-variance σ2. For action a, the sub-optimality gap at time t is denoted
∆t(a) , max˜a2At r(˜a) −r(a), and the maximal gap ∆max , maxa,t ∆t(a). We now recall the"
PROBLEM SETUP AND BACKGROUND,0.06720430107526881,speciﬁcs of each model:
PROBLEM SETUP AND BACKGROUND,0.06989247311827956,"• MAB: There is a ﬁnite number of actions d, enumerated as A , [d], each having a scalar"
PROBLEM SETUP AND BACKGROUND,0.07258064516129033,reward ✓?
PROBLEM SETUP AND BACKGROUND,0.07526881720430108,a. Arms are independent: playing one arm gives no information about the others.
PROBLEM SETUP AND BACKGROUND,0.07795698924731183,"• LCB: The action set At is a subset of the unit ball Bd, possibly inﬁnite. Unless explicitly"
PROBLEM SETUP AND BACKGROUND,0.08064516129032258,"mentioned, the reward is assumed to be linear with respect to a latent unknown vector
✓? 2 Rd, i.e. r(a) = ha, ✓?i. Non-stochastic contexts are modeled by the fact that At
is drawn by an oblivious adversary. Here one does not need to rely on the typical i.i.d
assumption on their generating process [47, 16]."
PROBLEM SETUP AND BACKGROUND,0.08333333333333333,"Information Structure:
In the classical uncensored setting, the noisy feedback is immediately
observed post-decision and utilized to make decisions in the next round. We introduce the following
censorship model: an independent Bernoulli random variable of parameter p(at) denoted as xat is
drawn after each decision at and the feedback is observed, i.e., realized, if and only if xat = 1; else
the feedback is said to be censored. We recover the uncensored setting when p(a) ⌘1. Henceforth,"
PROBLEM SETUP AND BACKGROUND,0.08602150537634409,"in both ﬁnite and linear settings, the Bernoulli parameter corresponding to the censorship probability
depends on the action chosen i.e. our model allows the censorship to be heterogeneous across actions.
Given that the action chosen at time t is a random variable, p(at) refers to a random variable as well."
PROBLEM SETUP AND BACKGROUND,0.08870967741935484,"Algorithm 1: Generic UCB
Input: Total time T, Regularization λ, Precision δ
for t = 1, . . . , T do"
PROBLEM SETUP AND BACKGROUND,0.0913978494623656,Provide reward estimator ˜rλ
PROBLEM SETUP AND BACKGROUND,0.09408602150537634,"t verifying w.p.
1 −δ:"
PROBLEM SETUP AND BACKGROUND,0.0967741935483871,"8a 2 At, r(a) ˜rλ"
PROBLEM SETUP AND BACKGROUND,0.09946236559139784,"t (a);
Play action at = argmaxa2At ˜rλ"
PROBLEM SETUP AND BACKGROUND,0.10215053763440861,"t (a) ;
if (at, r(at) + ✏t) is realized i.e. xat = 1 then"
PROBLEM SETUP AND BACKGROUND,0.10483870967741936,Update ˜rλ
PROBLEM SETUP AND BACKGROUND,0.10752688172043011,"t ;
end
end"
PROBLEM SETUP AND BACKGROUND,0.11021505376344086,"Algorithms:
To study the impact of cen-
sorship on bandit problems, we consider the
class of high-probability index algorithms
based on the optimism under uncertainty prin-
ciple, commonly referred as UCB-algorithms.
Following [23], Algorithm 1 summarizes the
generic UCB design framework. We detail
in App.A the speciﬁc instances of UCB for
MAB (resp. LCB) used in Sec.3 (resp. Sec.4).
Moreover, this family of algorithms strongly
relies on regularized reward estimators ˜rλ"
PROBLEM SETUP AND BACKGROUND,0.11290322580645161,"t ,
where the regularizer is mostly used to pre-
vent an artiﬁcial cold-start exploratory phase."
PROBLEM SETUP AND BACKGROUND,0.11559139784946236,"Performance Criterion:
The frequentist performance of the agent is measured by the notion of
pseudo regret, i.e., the difference between the algorithm’s cumulative reward and the best total reward.
More formally, we introduce for any policy ⇡2 ⇧:"
PROBLEM SETUP AND BACKGROUND,0.11827956989247312,"R(T, ⇡) = T
X t=1"
PROBLEM SETUP AND BACKGROUND,0.12096774193548387,"max
a2At r(a) − T
X t=1"
PROBLEM SETUP AND BACKGROUND,0.12365591397849462,"r(at) = T
X t=1"
PROBLEM SETUP AND BACKGROUND,0.12634408602150538,∆t(at).
PROBLEM SETUP AND BACKGROUND,0.12903225806451613,"We aim to provide guarantees on E[R(T, ⇡)] with respect to the number of rounds T and quantities
that govern the complexity of the problem (for example number of arms, ambient dimension d,
parameters of censorship model or smoothness properties of the reward r). Here, the expectation is
with respect to the noise induced by the feedback, the censorship and a possibly randomized policy."
NOTATIONS,0.13172043010752688,"2.1
Notations"
NOTATIONS,0.13440860215053763,"Transpose of a vector u is denoted by u>, classical Euclidean inner product by h., .i and trace operator
by Tr. For positive semi-deﬁnite matrix ⌃2 Rd⇥d and for any vector u 2 Rd, notation kuk⌃refers
to p"
NOTATIONS,0.13709677419354838,"u>⌃u. We use notation Id to denote the d ⇥d identity matrix. Bd is the unit ball in Rd. [n]
is the set of integers {1, 2, · · · , n}. For a given function f, we note f (i) the ith derivative of f. To
avoid confusion with the dimension d, we use @x instead of dx to denote an inﬁnitesimal increase of
x. We use the asymptotic notations ⇠, O, ⇥and ˜O (O when log factors are removed). Finally, for an
event H, we use ¬H to denote its complement."
MULTI-ARMED BANDITS,0.13978494623655913,"3
Multi-Armed Bandits"
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.1424731182795699,"3.1
Effective Dimension and Regret Bounds"
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.14516129032258066,"The main result of this section is that censorship effectively enlarges the dimension of the problem.
We deﬁne the effective dimension as de↵, P a2[d]"
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.1478494623655914,"1
pa and our result (Thm. 3.1) shows that, at ﬁrst
order, the regret is guaranteed to be the same as the uncensored problem with de↵arms instead of d.
Theorem 3.1. Under censorship, the UCB algorithm with regularization λ has an instance-
independent expected regret of:"
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.15053763440860216,"E[R(T, ⇡UCB)] ˜O(σ p"
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.1532258064516129,de↵T).
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.15591397849462366,"Furthermore, we obtain analogous regret guarantees for instance-dependent cases where, at ﬁrst order,
the uncensored dimension P"
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.1586021505376344,a6=a? σ2
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.16129032258064516,∆a enlarges to P
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.1639784946236559,"a6=a?
σ2
pa∆a :"
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.16666666666666666,"Proposition 3.2. For a ﬁxed action set At ⌘[d] and for a-priori known action gap ∆a , max˜a ✓?"
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.1693548387096774,"˜a −
✓?"
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.17204301075268819,"a, the UCB algorithm with regularization λ has the instance-dependent expected regret:"
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.17473118279569894,"E[R(T, ⇡UCB)] O ⇣"
EFFECTIVE DIMENSION AND REGRET BOUNDS,0.1774193548387097,log(T) X a6=a?
PA,0.18010752688172044,"1
pa"
PA,0.1827956989247312,"max( σ2 ∆a , ∆a) ⌘ ."
PA,0.18548387096774194,"On one hand, a preliminary understanding of censorship posits an increase of the average ""regret per
information gain"" [23] (as it takes longer on average to get the same amount of information) but does
not change the underlying complexity of the problem. One the other hand, our results (Thm. 3.1 and
Prop. 3.2) postulate that the censored problem is equivalent at ﬁrst order to a higher dimensional
problem but explored with the same regret per information gain."
PA,0.1881720430107527,"The abovementioned results extends to a-priori known heteroskedasticity (see Rem. 3 and 4 in App.
B). For this general setting, the effective dimension for instance-independent (resp. dependent) case is
given by P a σ2"
PA,0.19086021505376344,"a
pa (resp. P a6=a? σ2"
PA,0.1935483870967742,"a
pa∆a ), where σ2"
PA,0.19623655913978494,"a is the variance proxy of arm a. Although the scaling
in P a"
PA,0.1989247311827957,"1
∆apa was already mentioned in [24] for unregularized setting with homogeneous variance σ2"
PA,0.20161290322580644,"and proven to be optimal, our results generalize these ﬁndings."
CUMULATIVE CENSORED POTENTIAL,0.20430107526881722,"3.2
Cumulative Censored Potential"
CUMULATIVE CENSORED POTENTIAL,0.20698924731182797,"We now provide a proof sketch of Thm. 3.1, and in doing so, we instantiate an analysis framework
that will be extended in Sec. 4. This proof consists in the successive elimination of the noise induced
by the feedback and censorship. This leads to regret guarantees on a resulting deterministic quantity
by characterizing worst-case learning conditions. The ﬁrst step of the proof is a variant of the classical
reduction of the UCB regret to another quantity we refer to as the expected cumulative censored
potential. Before stating it, we deﬁne at the end of a round t 2 [T], the random number of times an
arm a has been pulled as ⌧a(t) , Pt"
CUMULATIVE CENSORED POTENTIAL,0.20967741935483872,"l=1 1{al = a}. Similarly, the number of times an action a has
been realized at the end of round t is denoted Na(t) , Pt"
CUMULATIVE CENSORED POTENTIAL,0.21236559139784947,"l=1 1{al = a, xal = 1}. We then have:
Lemma 3.3. Given an uniform regularization of λ > 0, the UCB algorithm veriﬁes:"
CUMULATIVE CENSORED POTENTIAL,0.21505376344086022,"E[R(T, ⇡UCB)] 2 p"
CUMULATIVE CENSORED POTENTIAL,0.21774193548387097,6σ2 log(T)E[V 1
CUMULATIVE CENSORED POTENTIAL,0.22043010752688172,"2 (T, ⇡UCB)] + 2λk✓?k1E[V1(T, ⇡UCB)] + 2d∆max"
CUMULATIVE CENSORED POTENTIAL,0.22311827956989247,"T
where, for any ↵> 0 and ⇡2 ⇧, the cumulative potential under censorship is given by:"
CUMULATIVE CENSORED POTENTIAL,0.22580645161290322,"V↵(T, ⇡) = T
X t=1"
CUMULATIVE CENSORED POTENTIAL,0.22849462365591397,(Nat(t −1) + λ)−↵.
CUMULATIVE CENSORED POTENTIAL,0.23118279569892472,"Without censorship, the cumulative potential translates the average rate of decay of uncertainty on the
reward of different arms and is closely linked to the divergence between the true reward distribution
and the empirical distribution of observed rewards [38]. Introducing censorship transforms the
classical deterministic decay rate into a stochastic one. For a typical reward distribution, the rate of
decay is proportional to a term in n−↵or can be upper bounded by such a term (see for e.g. [38]),
where n is the number of observed rewards. Therefore, a higher ↵corresponds to faster learning."
CUMULATIVE CENSORED POTENTIAL,0.23387096774193547,"In contrast to the classical non-regularized analysis or to the LCB case of Sec. 4, we observe two
different orders of ↵(1/2 and 1) coming from the use of the L1-norm instead of the L2-norm. Taken
independently, they lead to respective contributions of O(de↵log(T)) and O( p"
CUMULATIVE CENSORED POTENTIAL,0.23655913978494625,"de↵T). Note that
by working with a general ↵, our analysis naturally extends beyond sub-Gaussian noise to more
general assumptions about the Laplace transform of noise (e.g., lighter or heavier tails), as discussed
in Rem.2. To further study V↵, we introduce the following property:
Proposition 3.4. For all ↵> 0, δ 2]0, 1] and given  ↵a primitive of x 7! x−↵, we have: max"
CUMULATIVE CENSORED POTENTIAL,0.239247311827957,"⇡2⇧E[V↵(T, ⇡)] 
de↵
(1 −δ)↵  ↵( T de↵"
CUMULATIVE CENSORED POTENTIAL,0.24193548387096775,"+
λ
1 −δ ) − ↵(
λ
1 −δ ) '"
CUMULATIVE CENSORED POTENTIAL,0.2446236559139785,+ 24de↵log(T) + d
CUMULATIVE CENSORED POTENTIAL,0.24731182795698925,"λ↵
+
4de↵
λ↵δ2T 12δ2 ."
CUMULATIVE CENSORED POTENTIAL,0.25,"The proof of this proposition involves two steps: ﬁrstly, we remove the stochastic dependence induced
by the censorship through concentration properties (See App. B), and we then solve the resulting
policy maximization problem (Lemma 3.5). In the ﬁrst step, we consider for a given δ 2]0, 1] the
event:"
CUMULATIVE CENSORED POTENTIAL,0.25268817204301075,"HCEN(δ) = {9a 2 [d], t 2 [T], Na(t) < (1 −δ)pa⌧a(t)
and
⌧a(t) ≥T0(a)} ,"
CUMULATIVE CENSORED POTENTIAL,0.2553763440860215,"where T0(a) , 24 log(T)/pa + 1 and claim that P(HCEN(δ)) 4de↵"
CUMULATIVE CENSORED POTENTIAL,0.25806451612903225,"δ2 T −12δ2, improving a result of
[24]. Here HCEN denotes the event where there is a signiﬁcant gap between the realized and expected"
CUMULATIVE CENSORED POTENTIAL,0.260752688172043,"number of observed rewards. We consider its complement in our analysis of the principal order of
regret. This allows us to lower bound for each action, the realized number of reward observations by
a multiple of the number of times that action was selected, thus eliminating the randomness induced
by censoring."
CUMULATIVE CENSORED POTENTIAL,0.26344086021505375,"Our second step makes use of the following lemma (also known as a water-ﬁlling process in
information theory [14]):
Lemma 3.5. For  ↵a primitive of x 7! x−↵where ↵2]0, 1], regularization (λa)a2[d] 2 (R>0)d"
CUMULATIVE CENSORED POTENTIAL,0.2661290322580645,"and censorship vector (pa)a2[d], the solution of the optimization problem:"
CUMULATIVE CENSORED POTENTIAL,0.26881720430107525,"max
⌧1...,⌧d≥0 X a2[d]"
PA,0.271505376344086,"1
pa ⇣"
PA,0.27419354838709675,↵(pa⌧a + λa) − ↵(λa) ⌘ s.t. X a2[d]
PA,0.2768817204301075,⌧a = T
PA,0.27956989247311825,is given by ⌧?
PA,0.28225806451612906,"a =
1
pa [C −λa]+, where C ensures the total budget constraint P"
PA,0.2849462365591398,a2[d] ⌧?
PA,0.28763440860215056,"a = T. In
particular, with λeff ,
1
de↵ P a2[d]"
PA,0.2903225806451613,"λa
pa and λ0"
PA,0.29301075268817206,"a , de↵(λa −λeff), the optimal solution is given by ⌧?"
PA,0.2956989247311828,"a ,
1
pade↵(T −λ0"
PA,0.29838709677419356,"a) for T ≥max a
λ0"
PA,0.3010752688172043,a and the optimal value is de↵ ↵( T
PA,0.30376344086021506,de↵+λeff)−P a2[d]
PA,0.3064516129032258,"1
pa  ↵(λa)."
PA,0.30913978494623656,"For unregularized algorithms, this framework can be easily applied to provide instances-dependent
guarantees by adding constraints of type ⌧a f(∆a) within Lemma 3.5. Optimal guarantees under
regularization such as the ones given in Prop. 3.2 require however to consider both orders of V↵(1/2
and 1) simultaneously and not independently, leading to slight variations as shown in the proof of
Prop. 3.2. Next, we further discuss the properties of V↵given its importance in our analysis."
EVALUATING ADAPTIVITY GAIN,0.3118279569892473,"3.3
Evaluating Adaptivity Gain"
EVALUATING ADAPTIVITY GAIN,0.31451612903225806,"It is well known that adaptivity is a key feature of sequential decision problems: optimal policies
use feedback from previous decisions to decide the next action to take based on the data, and in
comparison non-adaptive policies can be quite suboptimal. Somewhat interestingly, the main result
of this section is that adaptivity in the context of censoring does not provide a signiﬁcant advantage
to the decision maker. More precisely, being able to observe which decisions have been censored and
adapting to this information does not bring more than a second order gain. In proving this result, we
quantify and gain insight into the expected performance of policies that are adaptive to the realization
of the censorship process, in comparison to a class of non-adaptive (i.e., ofﬂine) policies."
EVALUATING ADAPTIVITY GAIN,0.3172043010752688,"In fact, through the introduction of HCEN(δ) and for any ↵2 [0, 1], δ 2]0, 1], we showed in Prop.
3.4 the upper bound
de↵
(1−δ)↵ ↵( T"
EVALUATING ADAPTIVITY GAIN,0.31989247311827956,"de↵+
λ
1−δ) for the learning complexity max E[V↵(T, ⇡)] where the
maximum is taken over the class of adaptive policies ⇧adapt, i.e., measurable with respect to the
censorship. Note that the exact value of such maximum is notoriously difﬁcult to study due to the
adaptive nature of censorship induced by the decision-making process. Next, we introduce ⇧o↵, the
class of policies that are not adaptive with respect to the censorship and we prove that :"
EVALUATING ADAPTIVITY GAIN,0.3225806451612903,"Lemma 3.6. For ↵2]0, 1] and λ > 0, we have max"
EVALUATING ADAPTIVITY GAIN,0.32526881720430106,"⇡2⇧off E[V↵(T, ⇡)] ⇠de↵ ↵( T de↵ + λ)."
EVALUATING ADAPTIVITY GAIN,0.3279569892473118,"In other words, restricting attention to ofﬂine policies is sufﬁcient to obtain the correct scaling. The
next step to complete our claim is the asymptotic expansion:"
EVALUATING ADAPTIVITY GAIN,0.33064516129032256,"Proposition 3.7. For ↵2]0, 1], by denoting γ↵(p) ,
↵
2d1−↵ e↵ X a2[d]"
PA,0.3333333333333333,"1
pa ⇣X ˜a6=a"
PA,0.33602150537634407,1 −p˜a p˜a ⌘
PA,0.3387096774193548,", we have:"
PA,0.34139784946236557,"max
⇡2⇧adapt E[V↵(T, ⇡)] −max"
PA,0.34408602150537637,"⇡2⇧off E[V↵(T, ⇡)] = γ↵(p) 1"
PA,0.3467741935483871,T ↵+ o( 1
PA,0.34946236559139787,"T ↵).
(?)"
PA,0.3521505376344086,"Moreover, if for a given β 2]0, 1[, we introduce ⇧single(βT) the policy class whose censorship
information set has a single updating at time bβTc, we have:"
PA,0.3548387096774194,"max
⇡2⇧single(βT ) E[V↵(T, ⇡)] −max"
PA,0.3575268817204301,"⇡2⇧off E[V↵(T, ⇡)] = γ↵(p) β"
PA,0.3602150537634409,T ↵+ o( 1
PA,0.3629032258064516,"T ↵).
(??)"
PA,0.3655913978494624,"Thus, γ↵(p) can be viewed as an adaptivity gain resulting from the continuous correction of the
cumulative variance induced by the action selection process. Essentially, it is closely related to"
PA,0.3682795698924731,"the Jensen Gap of an appropriate random variable and the proof involves the study of the Taylor
expansion of the potential function  ↵. (??) tells us that a single observation of the censorship
realization is sufﬁcient to obtain a near-optimal gain in adaptivity. We present a proof sketch of Prop.
3.7 in App. B. This shows that censorship in MAB can be treated in an ofﬂine manner at ﬁrst order."
CONTEXTUAL BANDIT,0.3709677419354839,"4
Contextual Bandit"
CONTEXTUAL BANDIT,0.3736559139784946,"In this section, we study Linear Contextual Bandits (LCBs) under censorship. The regret analysis for
the generic censorship model in Sec. 2 is signiﬁcantly more complex for LCB than for MAB. This is
due to the fact that different actions contribute differently to the information acquisition, leading to a
non-linear phenomenon governing the trade-off between reward and information gain (see Sec.4.4)."
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.3763440860215054,"4.1
Multi-threshold Models and Regret Bounds"
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.3790322580645161,"To address the abovementioned challenge, we now introduce a simple multi-threshold censorship
model, which enables a precise regret analysis. In particular, we consider that feedback is censored
according to the following action-dependant probability:"
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.3817204301075269,"p : a 2 Bd 7! k
X j=0"
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.3844086021505376,"1{sin(φj) ha, ui < sin(φj+1)}pj,
(MT )"
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.3870967741935484,where (φj)jk+1 is an increasing sequence verifying φ0 = −⇡
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.3897849462365591,"2 , φk+1 = ⇡"
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.3924731182795699,"2 and u 2 Rd is a unit
vector. We assume that (pj)jk is decreasing, i.e. the censorship is increasing with j in direction u.
Henceforth, we refer to the interval [sin(φj), sin(φj+1)[ as region j. Note that simple models such as
uniform censorship are subsumed by this family (for k equals 0)."
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.3951612903225806,"Figure 1: Example of a multi-threshold model for
k = 2 (Green). Logistic censorship model (Red)"
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.3978494623655914,"The two main features of the multi-threshold
model are: the radial aspect (the censorship
probability depends on the action through a
scalar product with a given vector) and the mono-
tonicity (the censorship is monotone in the value
of this scalar product).
Note that MT can
be seen as a piecewise constant approximation
of any Generalized Linear Model (GLM) [31].
Thus, the simplicity of this censorship model is
not an inherently limiting factor on the general-
ity of our subsequent results."
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.40053763440860213,"Moreover, MT admits a natural behavioral in-
terpretation: Such a distribution can be seen
as induced by a population model of hetero-
geneous random-utility maximizing agents. A
single threshold model (i.e. k equals 1) cor-
responds to a given agent type, and the multi-
threshold model naturally results from aggregate
responses of heterogeneous population [4]."
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.4032258064516129,"We now state the main result of this section:
Theorem 4.1. For a given multi-threshold censorship model MT , there exits de↵such that the UCB
algorithm with regularization λ has an instance-independent expected regret of:"
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.40591397849462363,"E[R(T, ⇡UCB)] ˜O(σ p"
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.40860215053763443,d · de↵ p T).
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.4112903225806452,"Importantly, note the mapping from the original dimension d to the enlarged p"
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.41397849462365593,"d · de↵, in contrast
to the previous dilation d 7! de↵for the case of MAB problems. An extension to Generalized
Linear Contextual Bandits is provided in App. C.6 where we show that the dimension is governed by
p"
MULTI-THRESHOLD MODELS AND REGRET BOUNDS,0.4166666666666667,"d · de↵/, with corresponding to a minimum of the derivative of the link function (encompassing
the smoothness of the GLM at its maximum) [28, 16]. We conjecture that this result still holds if we
relax the monotonicity property of MT although it will require some modiﬁcations in the proofs of
section D. On the other hand, we believe that the radial property is necessary, considering the related
literature on GLMs (further discussed in App. C.6) where it appears prominently."
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.41935483870967744,"4.2
Generalized Cumulative Censored Potential"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.4220430107526882,"Analogous to the MAB case, we now introduce for LCB the random matrices corresponding to
the effective realization WC"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.42473118279569894,"t , λId + Pt"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.4274193548387097,n=1 xatata>
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.43010752688172044,"t and the expected realization Wt , λId +
Pt"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.4327956989247312,n=1 p(at)ata>
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.43548387096774194,"t . We also introduce the continuous counterpart of Wt deﬁned as W(t) , λId +
R t"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.4381720430107527,"u=0 p(a(u))a(u)a(u)>@u, where (a(u))uT is an integrable deterministic path.4 We emphasise
that the use of continuous counterpart is key in enabling our next results. As in the MAB case, we
bound the regret although now using a generalization of V↵:"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.44086021505376344,"Lemma 4.2. For all δ 2]0, 1], there exists a constant ˜βδ(T) = ⇥( p"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.4435483870967742,d log(T)) such that
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.44623655913978494,"E[R(T, ⇡UCB)] 2 ˜βδ(T) p"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.4489247311827957,"TE[V1(T, ⇡UCB)] + δT∆max,"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.45161290322580644,"where, for ↵> 0 and ⇡2 ⇧, the linear extension of the cumulative censored potential is given by:"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.4543010752688172,"V↵(T, ⇡) , T
X t=1 katk2 (WC"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.45698924731182794,"t−1)−↵= T
X t=1"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.4596774193548387,Tr((WC
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.46236559139784944,t−1)−↵ata> t ).
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.4650537634408602,"The proof idea is analogous (albeit more complex) than in the ﬁnite action case (see App. C). In order
to get a handle on V↵, we again leverage a two-step approach: ﬁrst we eliminate the randomness
due to censorship (here, we utilize matrix martingale inequalities) and then optimize the resulting
deterministic quantity seen through a continuous lens. The ﬁrst step requires the following result:
Proposition 4.3. For any δ 2]0, 1], λ > 0, ↵> 0 and policy ⇡2 ⇧, we have:"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.46774193548387094,"E[V↵(T, ⇡)] δ"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.47043010752688175,λ↵+ C(δ)↵Tr ⇣Z T 0
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.4731182795698925,"W(t)−↵a(t)a(t)>@t ⌘ ,"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.47580645161290325,"where C(δ) , 8(λ + 1) max(log(d/δ))/λ, 1)/λ."
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.478494623655914,"The key idea of this result is to observe that the telescopic sum on which the classical Elliptical
Potential lemma [1, 35, 8] heavily relies on is, in fact, the discrete approximation of an integral over
a matrix path. This critical methodological contribution is further discussed in Rem. 1 and 5.
Remark 1. One way to fully appreciate the generality of this result is to consider the simpler case of
classical uncensored environment for which we obtain for ↵> 0, ↵6= 1: T
X t=1 katk2 W−↵ t−1 "
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.48118279569892475,⇣λ + 1 λ ⌘↵Tr ⇣R T
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.4838709677419355,0 @W(t)1−↵⌘
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.48655913978494625,"1 −↵
="
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.489247311827957,⇣λ + 1 λ
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.49193548387096775,⌘↵Tr(W1−↵
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.4946236559139785,"T
−W1−↵"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.49731182795698925,"0
)
1 −↵
."
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.5,"For ↵= 1, a similar reasoning is applied using the formula Tr(log(A)) = log(det A): T
X t=1 katk2 W−1"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.5026881720430108,t−1 λ + 1 λ Z T 0
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.5053763440860215,@ log det(W(t))
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.5080645161290323,"@t
@t = λ + 1"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.510752688172043,"λ
Tr(log WT −log W0) = λ + 1"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.5134408602150538,"λ
log det WT"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.5161290322580645,det W0 .
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.5188172043010753,A deeper study of the eigenvalues of W1−↵
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.521505376344086,"T
then yields the worst-case upper bound d↵(dλ +
T)1−↵/(1 −↵) for ↵< 1 and dλ1−↵/(↵−1) for ↵> 1, recovering more naturally and ex-
tending the results of [8]. Thus, analogous to the water ﬁlling process highlighted in the MAB case in
Lemma 3.5, we now consider a spectral water-ﬁlling process [14] optimizing over the eigenvalues of
 ↵(WT ) with a slight abuse of notations ( W1−↵"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.5241935483870968,"T
and log WT in this discussion)."
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.5268817204301075,"Following Rem.1, for the general censored case the challenge now becomes to identify a suitable
matrix operator on which the aforementioned spectral maximization can be performed. By applying
Lemma 4.2, we henceforth focus on the case of ↵= 1 for which Prop. 4.3 implies that for any policy: Tr ⇣Z T 0"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.5295698924731183,W(t)−1a(t)a(t)>@t ⌘ = Z T 0
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.532258064516129,"1
p(a(t))"
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.5349462365591398,@ log det(W(t))
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.5376344086021505,"@t
@t."
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.5403225806451613,"Next, we focus on maximizing this integral over the policy class ⇧and again recover the notion of
effective dimension."
GENERALIZED CUMULATIVE CENSORED POTENTIAL,0.543010752688172,"4In this section, the generic notation X(t) is used for continuous time quantities and Xt for discrete time."
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5456989247311828,"4.3
Effective Dimension in Linear Settings"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5483870967741935,"We now highlight immediate properties of the effective dimension, and then present its general study
for the multi-threshold model MT .
Lemma 4.4. Let us consider an uniform censorship model p : a 7! ¯p. By leveraging the case of
equality in the Arithmetic-Geometric inequality applied to the eigenvalues of WT , we then simply
deduce the associated effective dimension de↵, d/¯p: max ⇡2⇧ Z T 0 1 ¯p"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5510752688172043,@ log det(W(t))
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.553763440860215,"@t
@t = de↵log(1 +
T
λde↵ )."
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5564516129032258,"In fact, the logarithmic scaling of this quantity persists while moving beyond the uniform censorship
assumption. This also highlights the importance of the leading dimension factor, crudely upper
bounded by d/pmin in the next lemma:
Lemma 4.5. For any censorship function p, by introducing lower and upper bounds (pmin, pmax) of
p, we have:"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5591397849462365,"d
pmax"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5618279569892473,log(1 + pminT
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5645161290322581,"dλ
) max ⇡2⇧ Z T 0"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5672043010752689,"1
p(a(t))"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5698924731182796,@ log det(W(t))
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5725806451612904,"@t
@t 
d
pmin"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5752688172043011,"log(1 + pmaxT dλ
)."
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5779569892473119,"Related problems in the Generalized Linear Models literature [47, 28, 16] are implicitly solved in
the spirit of Lemma 4.5, where a minimum of the derivative of the link function plays the role of
pmin above. However, when the function p varies with action a, a more careful analysis is required
to derive useful dimensional bounds. Our next major result addresses this gap in the literature by
improving the bounds provided in Lemma 4.5:
Theorem 4.6. For a multi-threshold censorship model MT , we have: max ⇡2⇧ Z T 0"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5806451612903226,"1
p(a(t))"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5833333333333334,@ log det(W(t))
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5860215053763441,"@t
@t = de↵log(T) + o(log(T)),
(P)"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5887096774193549,"where de↵is the effective dimension. Furthermore, de↵is characterized by two cases:"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5913978494623656,"• Case 1: Single region j effective dimension de↵=
d
pj ."
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5940860215053764,"• Case 2: Bi-region (i, j) effective dimension, with i < j:"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5967741935483871,"de↵= 1 pj """
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.5994623655913979,"(d −1) 1 −l(i, j)"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6021505376344086,"pi
pj −l(i, j) + u(i, j) −1"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6048387096774194,"u(i, j) −pi pj # < d pj .
(D)"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6075268817204301,"where l(i, j) , sin2(φi)"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6102150537634409,"sin2(φj) and u(i, j) , cos2(φi)"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6129032258064516,cos2(φj).
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6155913978494624,"The implications of these cases are further discussed in Fig.2 in App. D. Notice that a necessary
condition for the bi-region (i, j) effective dimension to arise is the constraint on pi pj :"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6182795698924731,"max(1,
dl(i, j)u(i, j)
u(i, j) + (d −1)l(i, j)
|
{z
}"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6209677419354839,",s?(i,j)"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6236559139784946,) < pi pj
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6263440860215054,"< (d −1)u(i, j) + l(i, j)"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6290322580645161,"d
|
{z
}"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6317204301075269,",r?(i,j)"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6344086021505376,In the limit pi
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6370967741935484,"pj ! r?(i, j), de↵goes again to d/pj. We interpret this limiting case as locally hard
in the sense that censorship in region j is sufﬁciently important in comparison to all other regions
to impose a maximal effective dimension to the problem, irrespective of the values of pi, matching
Lemma 4.5. On the other hand, for the other limiting case (under additional mild assumptions),
we ﬁnd that de↵also goes to d/pj, but now for a uniformly hard reason: that is, censorship is
approximately constant and equal to pj, recovering the Lemma 4.4. Finally, in between these two
extremes lies the minimum effective dimension for a given value of pi pj ."
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6397849462365591,"4.4
Temporal dynamics of W(t)"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6424731182795699,"The proof of Thm. 4.6 requires the characterization of the dynamics of the optimal policy of (P).
Importantly, we discover that the evolution of W(t) is described by two qualitatively different regimes
as outlined next. It turns out that our continuous approach to analyzing cumulative censored potential
is an important tool to obtaining this result."
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6451612903225806,"Transient Regime:
There exists a decreasing sequence of censorship regions {i1 = k, . . . , il} of
length l 2 [k+1] and associated time sequence {t0 , 0, t1, . . . , tl} such that whenever tj t tj+1
for a given index j l −1, the evolution of W(t) is given by:"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6478494623655914,"W(t) = pij+1(t −tj)Wij+1 + W(tj) = pij+1(t −tj)Wij+1 + j
X n=1"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6505376344086021,"pin(tn −tn−1)Win + λId,"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6532258064516129,where Wi denotes the d ⇥d diagonal matrix diag( cos2(φi)
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6559139784946236,"d−1
, . . . , cos2(φi)"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6586021505376344,"d−1
, sin2(φi)). Interestingly, the
initial misspeciﬁcation of censorship is self-corrected during this transient step but at an extra cost.
This characterization of transient regime highlights an important consequence of using classical
algorithms in censored environments."
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6612903225806451,"Steady State Regime:
Post-transient regime, the dynamics of W(t) enter a steady state regime,
where one of the two cases necessarily arise:5."
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6639784946236559,• Case 1: Single region il. This case arises when the last element of the time sequence tl is
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6666666666666666,equal to +1 and we have the single region evolution for all t ≥tl−1:
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6693548387096774,"W(t) = pil(t −tl−1)Wil + W(tl−1) = pil(t −tl−1)Wil + l−1
X n=1"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6720430107526881,pin(tn −tn−1)Win + λId.
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6747311827956989,"The effective dimension corresponding to this dynamics is d/pil, with the following equality
for T ≥tl−1:
Z T 0"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6774193548387096,"1
p(a(t))"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6801075268817204,@ log det(W(t))
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6827956989247311,"@t
@t = 1 pil"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6854838709677419,"log det(W(T)) + l−1
X n=1 ( 1 pin"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6881720430107527,"−
1
pin+1"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6908602150537635,) log det W(tn).
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6935483870967742,"• Case 2: Bi-region (il+1, il). This case arises when the steady-state dynamics of W(t) span"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.696236559139785,"the two regions (il+1, il) with il+1 < il. For all t ≥tl, we have the evolution:"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.6989247311827957,W(t) / pil+1(t + λ?) 
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.7016129032258065,"cos2(φil)(u(il+1, il) − pil+1"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.7043010752688172,"pil )Id−1
(0)"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.706989247311828,"(0)
sin2(φil)( pil+1"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.7096774193548387,"pj
−l(il+1, il)) ! ."
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.7123655913978495,"where λ? and the proportionality factor are speciﬁed in SI. The corresponding effective
dimension is given by (D) and the following equality holds for all T ≥tl:
Z T 0"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.7150537634408602,"1
p(a(t))"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.717741935483871,@ log det(W(t))
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.7204301075268817,"@t
@t = de↵log(1 + T −tl"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.7231182795698925,"tl + λ? ) + l
X n=1 ( 1 pin"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.7258064516129032,"−
1
pin+1"
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.728494623655914,) log det W(tn).
EFFECTIVE DIMENSION IN LINEAR SETTINGS,0.7311827956989247,"For further discussions on transient and steady state regimes, we refer to Fig.3, 4 and 5. in App. D."
CONCLUDING REMARKS,0.7338709677419355,"5
Concluding Remarks"
CONCLUDING REMARKS,0.7365591397849462,"In this work, we demonstrate that the complexity of bandit learning under censorship is governed by
the notion of effective dimension. To do so, we developed a novel analysis framework which enables
us to precisely estimate this quantity for a broad class of multi-threshold censorship models. An
important future work would be to extend our model and approach to Bayesian settings, which will
likely provide us with useful insights on the cumulative censored potential V↵, as initiated by [18].
Future work also includes relaxing the Missing Completely at Random (MCAR) property in favor
of time-dependent censorship models such as Markov Decision Processes (MDPs). We believe that
tools similar to those developed in our potential-based analysis can be applied in this case. Finally,
the contributions of our work may be of interest to the recent value alignment literature, where the
question of learnability under humain-AI interactions is central. [10, 17, 12]."
CONCLUDING REMARKS,0.739247311827957,"We do not envision any negative societal impacts of our work other than that of bandits algorithms
deployed in AI-driven platforms."
CONCLUDING REMARKS,0.7419354838709677,"5These cases are fully characterized in terms of parameters of censorship model in Lemmas D.1, D.2, D.3
and Cor. D.1.1."
CONCLUDING REMARKS,0.7446236559139785,Acknowledgments and Disclosure of Funding
CONCLUDING REMARKS,0.7473118279569892,"This research project is supported by the AFOSR FA9550-19-1-0263 “Building attack resilience
into complex networks” Grant. The authors would like to thank Prem Talwai and the anonymous
reviewers for providing insightful comments and suggestions."
REFERENCES,0.75,References
REFERENCES,0.7526881720430108,"[1] Yasin Abbasi-yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear"
REFERENCES,0.7553763440860215,"stochastic bandits. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger,
editors, Advances in Neural Information Processing Systems, volume 24. Curran Associates,
Inc., 2011."
REFERENCES,0.7580645161290323,"[2] Jacob Abernethy, Kareem Amin, and Ruihao Zhu. Threshold bandit, with and without cen-"
REFERENCES,0.760752688172043,"sored feedback. In Proceedings of the 30th International Conference on Neural Information
Processing Systems, NIPS’16, page 4896–4904, Red Hook, NY, USA, 2016. Curran Associates
Inc."
REFERENCES,0.7634408602150538,[3] Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit
REFERENCES,0.7661290322580645,"problem. In Conference on learning theory, pages 39–1. JMLR Workshop and Conference
Proceedings, 2012."
REFERENCES,0.7688172043010753,[4] Victor Aguirregabiria and Pedro mira. Dynamic Discrete Choice Structural Models: A Survey.
REFERENCES,0.771505376344086,"Technical Report tecipa-297, University of Toronto, Department of Economics, July 2007."
REFERENCES,0.7741935483870968,"[5] Sylvain Arlot. Rééchantillonnage et Sélection de modèles. Theses, Université Paris Sud - Paris"
REFERENCES,0.7768817204301075,"XI, December 2007."
REFERENCES,0.7795698924731183,[6] Jean-Yves Audibert and Sébastien Bubeck. Regret bounds and minimax policies under partial
REFERENCES,0.782258064516129,"monitoring. Journal of Machine Learning Research, 11(94):2785–2836, 2010."
REFERENCES,0.7849462365591398,"[7] Xueying Bai, Jian Guan, and Hongning Wang. A model-based reinforcement learning with"
REFERENCES,0.7876344086021505,"adversarial training for online recommendation. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.7903225806451613,"[8] Alexandra Carpentier, Claire Vernade, and Yasin Abbasi-Yadkori. The elliptical potential lemma"
REFERENCES,0.793010752688172,"revisited, 2020."
REFERENCES,0.7956989247311828,"[9] Nicolo Cesa-Bianchi, Claudio Gentile, Yishay Mansour, and Alberto Minora. Delay and"
REFERENCES,0.7983870967741935,"cooperation in nonstochastic bandits. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir,
editors, 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine
Learning Research, pages 605–622, Columbia University, New York, New York, USA, 23–26
Jun 2016. PMLR."
REFERENCES,0.8010752688172043,"[10] Lawrence Chan, Dylan Hadﬁeld-Menell, Siddhartha Srinivasa, and Anca Dragan. The assistive"
REFERENCES,0.803763440860215,"multi-armed bandit. In Proceedings of the 14th ACM/IEEE International Conference on
Human-Robot Interaction, HRI ’19, page 354–363. IEEE Press, 2019."
REFERENCES,0.8064516129032258,"[11] Wei Chen, Yajun Wang, Yang Yuan, and Qinshi Wang. Combinatorial multi-armed bandit"
REFERENCES,0.8091397849462365,"and its extension to probabilistically triggered arms. Journal of Machine Learning Research,
17(50):1–33, 2016."
REFERENCES,0.8118279569892473,"[12] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei."
REFERENCES,0.8145161290322581,"Deep reinforcement learning from human preferences, 2017."
REFERENCES,0.8172043010752689,"[13] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff"
REFERENCES,0.8198924731182796,"functions. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence
and Statistics, volume 15 of Proceedings of Machine Learning Research, pages 208–214, Fort
Lauderdale, FL, USA, 11–13 Apr 2011. PMLR."
REFERENCES,0.8225806451612904,[14] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in
REFERENCES,0.8252688172043011,"Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006."
REFERENCES,0.8279569892473119,"[15] Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev"
REFERENCES,0.8306451612903226,"Reyzin, and Tong Zhang. Efﬁcient optimal learning for contextual bandits. arXiv preprint
arXiv:1106.2369, 2011."
REFERENCES,0.8333333333333334,"[16] Sarah Filippi, Olivier Cappe, Aurélien Garivier, and Csaba Szepesvári. Parametric bandits: The"
REFERENCES,0.8360215053763441,"generalized linear case. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta,
editors, Advances in Neural Information Processing Systems, volume 23. Curran Associates,
Inc., 2010."
REFERENCES,0.8387096774193549,"[17] Dylan Hadﬁeld-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell. Cooperative inverse"
REFERENCES,0.8413978494623656,"reinforcement learning, 2016."
REFERENCES,0.8440860215053764,[18] Nima Hamidi and Mohsen Bayati. The randomized elliptical potential lemma with an application
REFERENCES,0.8467741935483871,"to linear thompson sampling, 2021."
REFERENCES,0.8494623655913979,"[19] Hoda Heidari, Michael Kearns, and Aaron Roth. Tight policy regret bounds for improving"
REFERENCES,0.8521505376344086,"and decaying bandits. In Proceedings of the Twenty-Fifth International Joint Conference on
Artiﬁcial Intelligence, IJCAI’16, page 1562–1570. AAAI Press, 2016."
REFERENCES,0.8548387096774194,[20] James Honaker and Gary King. What to do about missing values in time series cross-section
REFERENCES,0.8575268817204301,"data. American Journal of Political Science, 54(3):561–581, 2010 2010."
REFERENCES,0.8602150537634409,"[21] Shinji Ito, Daisuke Hatano, Hanna Sumita, Kei Takemura, Takuro Fukunaga, Naonori Kakimura,"
REFERENCES,0.8629032258064516,"and Ken-Ichi Kawarabayashi. Delay and cooperation in nonstochastic linear bandits. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural
Information Processing Systems, volume 33, pages 4872–4883. Curran Associates, Inc., 2020."
REFERENCES,0.8655913978494624,"[22] Pooria Joulani, Andras Gyorgy, and Csaba Szepesvari. Online learning under delayed feedback."
REFERENCES,0.8682795698924731,"In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International
Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research,
pages 1453–1461, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR."
REFERENCES,0.8709677419354839,[23] Johannes Kirschner and Andreas Krause. Information directed sampling and bandits with
REFERENCES,0.8736559139784946,"heteroscedastic noise. In Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors,
Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine
Learning Research, pages 358–384. PMLR, 06–09 Jul 2018."
REFERENCES,0.8763440860215054,"[24] Tal Lancewicki, Shahar Segal, Tomer Koren, and Y. Mansour. Stochastic multi-armed bandits"
REFERENCES,0.8790322580645161,"with unrestricted delay distributions. In ICML, 2021."
REFERENCES,0.8817204301075269,[25] Tor Lattimore and Csaba Szepesvari. An information-theoretic approach to minimax regret in
REFERENCES,0.8844086021505376,"partial monitoring. In COLT, 2019."
REFERENCES,0.8870967741935484,"[26] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.8897849462365591,"[27] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to"
REFERENCES,0.8924731182795699,"personalized news article recommendation. In Proceedings of the 19th international conference
on World wide web, pages 661–670, 2010."
REFERENCES,0.8951612903225806,"[28] Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear"
REFERENCES,0.8978494623655914,"contextual bandits. In International Conference on Machine Learning, pages 2071–2080.
PMLR, 2017."
REFERENCES,0.9005376344086021,[29] R.J.A. Little and D.B. Rubin. Statistical analysis with missing data. Wiley series in probability
REFERENCES,0.9032258064516129,"and mathematical statistics. Probability and mathematical statistics. Wiley, 2002."
REFERENCES,0.9059139784946236,"[30] Travis Mandel, Yun-En Liu, Emma Brunskill, and Zoran Popovi´c. The queue method: Handling"
REFERENCES,0.9086021505376344,"delay, heuristics, prior data, and evaluation in bandits. In Proceedings of the Twenty-Ninth AAAI
Conference on Artiﬁcial Intelligence, AAAI’15, page 2849–2856. AAAI Press, 2015."
REFERENCES,0.9112903225806451,"[31] P. McCullagh and J.A. Nelder. Generalized Linear Models, Second Edition. Chapman and"
REFERENCES,0.9139784946236559,"Hall/CRC Monographs on Statistics and Applied Probability Series. Chapman & Hall, 1989."
REFERENCES,0.9166666666666666,"[32] Gergely Neu, Andras Antos, András György, and Csaba Szepesvári. Online markov decision"
REFERENCES,0.9193548387096774,"processes under bandit feedback. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and
A. Culotta, editors, Advances in Neural Information Processing Systems, volume 23. Curran
Associates, Inc., 2010."
REFERENCES,0.9220430107526881,"[33] Therese D. Pigott. A review of methods for missing data. Educational Research and Evaluation,"
REFERENCES,0.9247311827956989,"7(4):353–383, 2001."
REFERENCES,0.9274193548387096,"[34] Ciara Pike-Burke, Shipra Agrawal, Csaba Szepesvari, and Steffen Grunewalder. Bandits with"
REFERENCES,0.9301075268817204,"delayed, aggregated anonymous feedback. In International Conference on Machine Learning,
pages 4105–4113. PMLR, 2018."
REFERENCES,0.9327956989247311,"[35] Chao Qin and Daniel Russo. Adaptivity and confounding in multi-armed bandit experiments, 2022."
REFERENCES,0.9354838709677419,[36] Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of thompson sampling.
REFERENCES,0.9381720430107527,"Journal of Machine Learning Research, 17(68):1–30, 2016."
REFERENCES,0.9408602150537635,[37] Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling.
REFERENCES,0.9435483870967742,"In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances
in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014."
REFERENCES,0.946236559139785,"[38] Shubhanshu Shekhar, Tara Javidi, and Mohammad Ghavamzadeh. Adaptive sampling for"
REFERENCES,0.9489247311827957,"estimating probability distributions. In Hal Daumé III and Aarti Singh, editors, Proceedings
of the 37th International Conference on Machine Learning, volume 119 of Proceedings of
Machine Learning Research, pages 8687–8696. PMLR, 13–18 Jul 2020."
REFERENCES,0.9516129032258065,[39] Aleksandrs Slivkins. Introduction to multi-armed bandits. Foundations and Trends® in Machine
REFERENCES,0.9543010752688172,"Learning, 12(1-2):1–286, 2019."
REFERENCES,0.956989247311828,"[40] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process"
REFERENCES,0.9596774193548387,"optimization in the bandit setting: No regret and experimental design. In Proceedings of the
27th International Conference on International Conference on Machine Learning, ICML’10,
page 1015–1022, Madison, WI, USA, 2010. Omnipress."
REFERENCES,0.9623655913978495,"[41] Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Information-"
REFERENCES,0.9650537634408602,"theoretic regret bounds for gaussian process optimization in the bandit setting. IEEE Transac-
tions on Information Theory."
REFERENCES,0.967741935483871,"[42] Claire Vernade, Alexandra Carpentier, Tor Lattimore, Giovanni Zappella, Beyza Ermis, and"
REFERENCES,0.9704301075268817,"Michael Brückner. Linear bandits with stochastic delayed feedback. In Hal Daumé III and Aarti
Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume
119 of Proceedings of Machine Learning Research, pages 9712–9721. PMLR, 13–18 Jul 2020."
REFERENCES,0.9731182795698925,[43] Qinshi Wang and Wei Chen. Improving regret bounds for combinatorial semi-bandits with
REFERENCES,0.9758064516129032,"probabilistically triggered arms and its applications. In I. Guyon, U. Von Luxburg, S. Ben-
gio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems, volume 30. Curran Associates, Inc., 2017."
REFERENCES,0.978494623655914,[44] Han Wu and Stefan Wager. Thompson sampling with unrestricted delays. arXiv preprint
REFERENCES,0.9811827956989247,"arXiv:2202.12431, 2022."
REFERENCES,0.9838709677419355,[45] Fuwen Yang and Yongmin Li. Set-membership ﬁltering for systems with sensor saturation.
REFERENCES,0.9865591397849462,"Automatica, 45(8):1896–1902, 2009."
REFERENCES,0.989247311827957,"[46] Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare:"
REFERENCES,0.9919354838709677,"A survey. ACM Computing Surveys (CSUR), 55(1):1–36, 2021."
REFERENCES,0.9946236559139785,"[47] Zhengyuan Zhou, Renyuan Xu, and Jose Blanchet. Learning in generalized linear contextual"
REFERENCES,0.9973118279569892,"bandits with stochastic delays. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,
E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32.
Curran Associates, Inc., 2019."
