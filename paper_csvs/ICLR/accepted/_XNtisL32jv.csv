Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0037174721189591076,"Recently,
brain-inspired spiking neuron networks (SNNs) have attracted
widespread research interest because of their event-driven and energy-efﬁcient
characteristics. Still, it is difﬁcult to efﬁciently train deep SNNs due to the non-
differentiability of its activation function, which disables the typically used gradi-
ent descent approaches for traditional artiﬁcial neural networks (ANNs). Although
the adoption of surrogate gradient (SG) formally allows for the back-propagation
of losses, the discrete spiking mechanism actually differentiates the loss landscape
of SNNs from that of ANNs, failing the surrogate gradient methods to achieve
comparable accuracy as for ANNs. In this paper, we ﬁrst analyze why the current
direct training approach with surrogate gradient results in SNNs with poor gen-
eralizability. Then we introduce the temporal efﬁcient training (TET) approach
to compensate for the loss of momentum in the gradient descent with SG so that
the training process can converge into ﬂatter minima with better generalizabil-
ity. Meanwhile, we demonstrate that TET improves the temporal scalability of
SNN and induces a temporal inheritable training for acceleration. Our method
consistently outperforms the SOTA on all reported mainstream datasets, includ-
ing CIFAR-10/100 and ImageNet. Remarkably on DVS-CIFAR10, we obtained
83% top-1 accuracy, over 10% improvement compared to existing state of the
art. Codes are available at https://github.com/Gus-Lab/temporal_
efficient_training."
INTRODUCTION,0.007434944237918215,"1
INTRODUCTION"
INTRODUCTION,0.011152416356877323,"The advantages of Spiking neuron networks (SNNs) lie in their energy-saving and fast-inference
computation when embedded on neuromorphic hardware such as TrueNorth (DeBole et al., 2019)
and Loihi (Davies et al., 2018). Such advantages originate from the biology-inspired binary spike
transmitted mechanism, by which the networks avoid multiplication during inference. On the other
hand, this mechanism also leads to difﬁculty in training very deep SNNs from scratch because
the non-differentiable spike transmission hinders the powerful back-propagation approaches like
gradient descents. Recently, many studies on converting artiﬁcial neuron networks (ANNs) to SNNs
have demonstrated SNNs’ comparable power in feature representation as ANNs (Han & Roy, 2020;
Deng & Gu, 2020; Li et al., 2021a). Nevertheless, it is commonly agreed that the direct training
method for high-performance SNN is still crucial since it distinguishes SNNs from converted ANNs,
especially on neuromorphic datasets."
INTRODUCTION,0.01486988847583643,"The output layer’s spike frequency or the average membrane potential increment is commonly used
as inference indicators in SNNs (Shrestha & Orchard, 2018; Kim et al., 2019). The current standard
direct training (SDT) methods regard the SNN as RNN and optimize inference indicators’ distri-
bution (Wu et al., 2018). They adopt surrogate gradients (SG) to relieve the non-differentiability
(Lee et al., 2016; Wu et al., 2018; Zheng et al., 2021). However, the gradient descent with SG does
not match with the loss landscape in SNN and is easy to get trapped in a local minimum with low
generalizability. Although using suitable optimizers and weight decay help ease this problem, the"
INTRODUCTION,0.01858736059479554,B Corresponding author
INTRODUCTION,0.022304832713754646,"Published as a conference paper at ICLR 2022 FC
…"
INTRODUCTION,0.026022304832713755,Class 1
INTRODUCTION,0.02973977695167286,Class 2
INTRODUCTION,0.03345724907063197,Class 3
INTRODUCTION,0.03717472118959108,Class 4
INTRODUCTION,0.040892193308550186,"𝐿(𝑂𝑚𝑒𝑎𝑛, 𝑦) Label 1
𝑇෍ 𝑡"
INTRODUCTION,0.04460966542750929,"𝐿(𝑂𝑡, 𝑦) 𝑂𝑚𝑒𝑎𝑛 𝑂(𝑡)"
INTRODUCTION,0.048327137546468404,Leaky Integrate-Fire
INTRODUCTION,0.05204460966542751,Surrogate Gradient
INTRODUCTION,0.055762081784386616,Output Integrate
INTRODUCTION,0.05947955390334572,Standard Direct Training
INTRODUCTION,0.06319702602230483,Temporal  Efficient Training
INTRODUCTION,0.06691449814126393,Post-spike
INTRODUCTION,0.07063197026022305,"𝜕𝐿
𝜕𝑂(𝑡) = 1"
INTRODUCTION,0.07434944237918216,"𝑇𝐿′(𝑂𝑚𝑒𝑎𝑛, 𝑦)"
INTRODUCTION,0.07806691449814127,"𝜕𝐿
𝜕𝑂(𝑡) = 1"
INTRODUCTION,0.08178438661710037,"𝑇𝐿′(𝑂𝑡, 𝑦)"
INTRODUCTION,0.08550185873605948,Pre-synaptic Input
INTRODUCTION,0.08921933085501858,"Forward
Backward
Temporal Efficient Training Forward"
INTRODUCTION,0.09293680297397769,"Figure 1: Workﬂow of temporal efﬁcient training (TET). To obtain a more generalized SNN, we
modify the optimization target to adjust each moment’s output distribution."
INTRODUCTION,0.09665427509293681,"performance of deep SNNs trained from scratch still suffers a big deﬁcit compared to that of ANNs
Deng et al. (2020). Another training issue is the memory and time consumption, which increases
linearly with the simulation time. Rathi & Roy (2020) initializes the target network by a converted
SNN to shorten the training epochs, indicating the possibility of high-performance SNN with limited
activation time. The training problem due to the non-differentiable activation function has become
the main obstruction of spiking neural network development."
INTRODUCTION,0.10037174721189591,"In this work, we examine the limitation of the traditional direct training approach with SG and
propose the temporal efﬁcient training (TET) algorithm. Instead of directly optimizing the integrated
potential, TET optimizes every moment’s pre-synaptic inputs. As a result, it avoids the trap into
local minima with low prediction error but a high second-order moment. Furthermore, since the
TET applies optimization on each time point, the network naturally has more robust time scalability.
Based on this characteristic, we propose the time inheritance training (TIT), which reduces the
training time by initializing the SNN with a smaller simulation length. With the help of TET, the
performance of SNNs has improved on both static datasets and neuromorphic datasets. Figure 1
depicts the workﬂow of our approach."
INTRODUCTION,0.10408921933085502,The following summarizes our main contributions:
INTRODUCTION,0.10780669144981413,"• We analyze the problem of training SNN with SG and propose the TET method, a new loss
and gradient descent regime that succeeds in obtaining more generalizable SNNs.
• We analyze the feasibility of TET and picture the loss landscape under both the SDT and
TET setups to demonstrate TET’s advantage in better generalization.
• Our sufﬁcient experiments on both static datasets and neuromorphic datasets prove the
effectiveness of the TET method. Especially on DVS-CIFAR10, we report 83.17% top-1
accuracy for the ﬁrst time, which is over 10% better than the current state-of-the-art result."
RELATED WORK,0.11152416356877323,"2
RELATED WORK"
RELATED WORK,0.11524163568773234,"In recent years, SNNs have developed rapidly and received more and more attention from the re-
search community. However, lots of challenging problems remain to be unsolved. In general, most
works on SNN training have been carried out in two strategies: ANN-to-SNN conversion and direct
training from scratch."
RELATED WORK,0.11895910780669144,"ANN-to-SNN Conversion. Conversion approaches avoid the training problem by trading high ac-
curacy through high latency. They convert a high-performing ANN to SNN and adjust the SNN
parameters w.r.t the ANN activation value layer-by-layer (Diehl et al., 2015; 2016). Some special
techniques have been proposed to reduce the inference latency, such as the subtraction mechanism"
RELATED WORK,0.12267657992565056,Published as a conference paper at ICLR 2022
RELATED WORK,0.12639405204460966,"(Rueckauer et al., 2016; Han et al., 2020), robust normalization Rueckauer et al. (2016), spike-norm
(Sengupta et al., 2018), and channel-wise normalization (Kim et al., 2019). Recently, Deng & Gu
(2020) decompose the conversion error to each layer and reduce it by bias shift. Li et al. (2021a)
suggest using adaptive threshold and layer-wise calibration to obtain high-performance SNNs that
require a simulation length of less than 50. However, converted methods signiﬁcantly extend the
inference latency, and they are not suitable for neuromorphic data (Deng et al., 2020)."
RELATED WORK,0.13011152416356878,"Direct training. In this area, SNNs are regarded as special RNNs and training with BPTT (Neftci
et al., 2019). On the backpropagation process, The non-differentiable activation term is replaced
with a surrogate gradient (Lee et al., 2016). Compared with ANN-to-SNN conversion, direct train-
ing achieves high accuracy with few time steps but suffers more training costs (Deng et al., 2020).
Several studies suggest that surrogate gradient (SG) is helpful to obtain high-performance SNNs
on both static datasets and neuromorphic datasets (Wu et al., 2019; Shrestha & Orchard, 2018; Li
et al., 2021b). On the backpropagation process, SG replaces the Dirac function with various shapes
of curves. Exceptionally, Wu et al. (2018) ﬁrst propose the STBP method and train SNNs on the
ANN programming platform, which signiﬁcantly promotes direct training development. Zheng et al.
(2021) further proposes the tdBN algorithm to smooth the loss function and ﬁrst realize training a
large-scale SNN on ImageNet. Zhang & Li (2020) proposes TSSL-BP to break down error back-
propagation across two types of inter-neuron and intra-neuron dependencies and achieve low-latency
and high accuracy SNNs. Recently, Yang et al. (2021) designed a neighborhood aggregation (NA)
method to use the multiple perturbed membrane potential waveforms in the neighborhood to com-
pute the ﬁnite difference gradients and guide the weight updates. They signiﬁcantly decrease the
required training iterations and improve the SNN performance."
PRELIMINARY,0.13382899628252787,"3
PRELIMINARY"
ITERATIVE LIF MODEL,0.137546468401487,"3.1
ITERATIVE LIF MODEL"
ITERATIVE LIF MODEL,0.1412639405204461,"We adopt the Leaky Integrate-and-Fire (LIF) model and translate it to an iterative expression with
the Euler method (Wu et al., 2019). Mathematically, the membrane potential is updated as"
ITERATIVE LIF MODEL,0.1449814126394052,"u(t + 1) = τu(t) + I(t),
(1)"
ITERATIVE LIF MODEL,0.14869888475836432,"where τ is the constant leaky factor, u(t) is the membrane potential at time t, and I(t) denotes
the pre-synaptic inputs, which is the product of synaptic weight W and spiking input x(t). Given
a speciﬁc threshold Vth, the neuron ﬁres a spike and u(t) reset to 0 when the u(t) exceeds the
threshold. So the ﬁring function and hard reset mechanism can be described as"
ITERATIVE LIF MODEL,0.1524163568773234,"a(t + 1) = Θ(u(t + 1) −Vth)
(2)"
ITERATIVE LIF MODEL,0.15613382899628253,"u(t + 1) = u(t + 1) · (1 −a(t + 1)),
(3)
where Θ denotes the Heaviside step function. The output spike a(t + 1) will become the post
synaptic spike and propagate to the next layer. In this study, we set the starting membrane u(0) to
0, the threshold Vth to 1, and the leaky factor τ to 0.5 for all experiments."
ITERATIVE LIF MODEL,0.15985130111524162,"The last layer’s spike frequency is typically used as the ﬁnal classiﬁcation index. However, adopting
the LIF model on the last layer will lose information on the membrane potential and damage the
performance, especially on complex tasks (Kim et al., 2019). Instead, we integrate the pre-synaptic
inputs I(t) with no decay or ﬁring (Rathi & Roy, 2020; Fang et al., 2021). Finally, we set the average
membrane potential as the classiﬁcation index and calculate the cross-entropy loss for training."
SURROGATE GRADIENT,0.16356877323420074,"3.2
SURROGATE GRADIENT"
SURROGATE GRADIENT,0.16728624535315986,"Following the concept of direct training, we regard the SNN as RNN and calculate the gradients
through spatial-temporal backpropagation (STBP) (Wu et al., 2018):"
SURROGATE GRADIENT,0.17100371747211895,"∂L
∂W =
X t"
SURROGATE GRADIENT,0.17472118959107807,"∂L
∂a(t)
∂a(t)
∂u(t)
∂u(t)
∂I(t)
∂I(t)"
SURROGATE GRADIENT,0.17843866171003717,"∂W ,
(4)"
SURROGATE GRADIENT,0.1821561338289963,where the term ∂a(t)
SURROGATE GRADIENT,0.18587360594795538,"∂u(t) is the gradient of the non-differentiability step function involving the derivative
of Dirac’s δ-function that is typically replaced by surrogate gradients with a derivable curve. So far,"
SURROGATE GRADIENT,0.1895910780669145,Published as a conference paper at ICLR 2022
SURROGATE GRADIENT,0.19330855018587362,"there are various shapes of surrogate gradients, such as rectangular (Wu et al., 2018; 2019), triangle
(Esser et al., 2016; Rathi & Roy, 2020), and exponential (Shrestha & Orchard, 2018) curve. In this
work, we choose the surrogate gradients shaped like triangles. Mathematically, it can describe as"
SURROGATE GRADIENT,0.1970260223048327,"∂a(t)
∂u(t) = 1"
SURROGATE GRADIENT,0.20074349442379183,"γ2 max(0, γ −|u(t) −Vth|),
(5)"
SURROGATE GRADIENT,0.20446096654275092,where the γ denotes the constraint factor that determines the sample range to activate the gradient.
BATCH NORMALIZATION FOR SNN,0.20817843866171004,"3.3
BATCH NORMALIZATION FOR SNN"
BATCH NORMALIZATION FOR SNN,0.21189591078066913,"Batch Normalization (BN) (Ioffe & Szegedy, 2015) is beneﬁcial to accelerate training and increase
performance since it can smooth the loss landscape during training (Santurkar et al., 2018). Zheng
et al. (2021) modiﬁed the forward time loop form and proposed threshold-dependent Batch Nor-
malization (tdBN) to normalize the pre-synaptic inputs I in both spatial and temporal paradigms
so that the BN can support spatial-temporal input. We adopt this setup with the extension of the
time dimension to batch dimension 1. In the inference process, the BN layer will be merged into the
pre-convolutional layer, thus the inference rule of SNN remain the same but with modiﬁed weight:"
BATCH NORMALIZATION FOR SNN,0.21561338289962825,"ˆ
W ←W γ"
BATCH NORMALIZATION FOR SNN,0.21933085501858737,"α, ˆb ←β + (b −µ) γ"
BATCH NORMALIZATION FOR SNN,0.22304832713754646,"α,
(6)"
BATCH NORMALIZATION FOR SNN,0.22676579925650558,"where µ, α are the running mean and standard deviation on both spatial and temporal paradigm, γ, β
are the afﬁne transformation parameters, and W, b are the parameters of the pre-convolutional layer."
METHODOLOGY,0.23048327137546468,"4
METHODOLOGY"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.2342007434944238,"4.1
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.2379182156133829,"Standard Direct Training. We use O(t) to represent pre-synaptic input I(t) of the output layer
and calculate the cross-entropy loss. The loss function of standard direct training LSDT is:"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.241635687732342,"LSDT = LCE( 1 T T
X"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.24535315985130113,"t=1
O(t), y),
(7)"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.24907063197026022,"where T is the total simulation time, LCE denotes the cross-entropy loss, and y represents the target
label. Following the chain rule, we obtain the gradient of W with softmax S(·) inference function : ∂LSDT"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.2527881040892193,"∂W
= 1 T T
X"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.25650557620817843,"t=1
[S(Omean) −ˆy]∂O(t)"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.26022304832713755,"∂W ,
(8)"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.26394052044609667,"where Omean denotes the average of the output O(t) over time, and ˆy is the one-hot coding of y."
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.26765799256505574,"Temporal Efﬁcient Training. In this section, we come up with a new kind of loss function LTET
to realize temporal efﬁcient training (TET). It constrains the output (pre-synaptic inputs) at each
moment to be close to the target distribution. It is described as:"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.27137546468401486,"LTET = 1 T · T
X"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.275092936802974,"t=1
LCE[O(t), y].
(9)"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.2788104089219331,"Recalculate the gradient of weights under the loss function LTET, and we have: ∂LTET"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.2825278810408922,"∂W
= 1 T T
X"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.2862453531598513,"t=1
[S(O(t)) −ˆy] · ∂O(t)"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.2899628252788104,"∂W .
(10)"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.2936802973977695,1https://github.com/fangwei123456/spikingjelly
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.29739776951672864,Published as a conference paper at ICLR 2022
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.30111524163568776,"4.2
CONVERGENCE OF GRADIENT DESCENT FOR SDT V.S. TET"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.3048327137546468,"In the case of SDT, the gradient consists of two parts, the error term (S(Omean) −ˆy) and the partial
derivative of output ∂O(t)/∂W. When the training process reaches near a local minimum, the term
(S(Omean)−ˆy) approximates 0 for all t = 1, ..., T, ignorant of the term ∂O(t)/∂W. For traditional
ANNs, the accumulated momentum may help get out of the local minima (e.g. saddle point) that
typically implies bad generalizability (Kingma & Ba, 2014; Kidambi et al., 2018). However, when
the SNN is trained with surrogate gradients, the accumulated momentum could be extremely small,
considering the mismatch of gradients and losses. The fact that the activation function is a step one
while the SG is bounded with integral constraints. This mismatch dissipates the momentum around
a local minimum and stops the SDT from searching for a ﬂatter minimum that may suggest better
generalizability."
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.30855018587360594,"In the case of TET, this issue of mismatch is relieved by reweighting the contribution of ∂O(t)/∂W.
Indeed, considering the fact that the ﬁrst term (S(O(t)) −ˆy) is impossible to be 0 at every moment
of SNN since the early output accuracy on the training set is not 100%. So TET needs the second
term ∂O(t)/∂W close to 0 to make the LTET convergence. This mechanism increases the norm of
gradients around sharp local minima and drives the TET to search for a ﬂat local minimum where
the disturbance of weight does not cause a huge change in O(t)."
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.31226765799256506,"Further, to ensure that the convergence with TET implies the convergence of SDT, we prove the
following lemma:"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.3159851301115242,Lemma 4.1. LSDT is upper bounded by LTET.
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.31970260223048325,"Proof. Suppose Oi(t) and ˆyi denote the i-th component of O(t) and ˆy, respectively. Expand Eqn.9,
we have:"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.32342007434944237,"LTET = −1 T T
X t=1 n
X"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.3271375464684015,"i=1
ˆyi log S(Oi(t)) = −1 T n
X"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.3308550185873606,"i=1
ˆyi log( T
Y"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.3345724907063197,"t=1
S(Oi(t))) = − n
X"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.3382899628252788,"i=1
ˆyi log( T
Y"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.3420074349442379,"t=1
S(Oi(t)))
1
T ≥− n
X"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.34572490706319703,"i=1
ˆyi log( 1 T T
X"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.34944237918215615,"t=1
S(Oi(t))) ≥− n
X"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.35315985130111527,"i=1
ˆyi log(S( 1 T T
X"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.35687732342007433,"t=1
Oi(t))) = LSDT,
(11)"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.36059479553903345,"where the ﬁrst inequality is given by the Arithmetic Mean-Geometric Mean Inequality, and the
second one is given by Jensen Inequality since the softmax function is convex. As a corollary, once
the LTET gets closed to zero, the original loss function LSDT also approaches zero."
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.3643122676579926,"Furthermore, the network output O(t) at a particular time point may be a particular outlier that dra-
matically affects the total output since the output of the SNN has the same weight at every moment
under the rule of integration. Thus it is necessary to add a regularization term like LMSE loss to
conﬁne each moment’s output to reduce the risk of outliers:"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.3680297397769517,"LMSE = 1 T T
X"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.37174721189591076,"t=1
MSE(O(t), φ),
(12)"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.3754646840148699,"where φ is a constant used to regularize the membrane potential distribution. And we set φ = Vth
in our experiments. In practice, we use a hyperparameter λ to adjust the proportion of the regular
term, we have:"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.379182156133829,"LTOTAL = (1 −λ)LTET + λLMSE.
(13)"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.3828996282527881,"It is worth noting that we only changed the loss function in the training process and did not change
SNN’s inference rules in the testing phase for a fair comparison. This algorithm is detailed in Algo.1."
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.38661710037174724,Published as a conference paper at ICLR 2022
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.3903345724907063,"Algorithm 1: Temporal efﬁcient training for one epoch
Input: SNN model; Simulation length: T; Threshold: Vth; Training dataset; Validation dataset;
total training iteration in one epoch: Itrain; total validation iteration in one epoch: Ival
for all i = 1, 2, ...Itrain iteration do"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.3940520446096654,"Get mini-batch training data, and class label: Y i;
Compute the SNN output Oi(t) of eatch time step;
Calculate loss function: LTOTAL = (1 −λ)LTET + λLMSE =
(1 −λ) · 1"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.39776951672862454,"T
PT
t=1 LCE(Oi(t), Y i) + λ · 1"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.40148698884758366,"T
PT
t=1 MSE(Oi(t), φ);
Backpropagation and update model parameters;
end
for all i = 1, 2, ...Ival iteration do"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.4052044609665427,"Get mini-batch validation data, and class label: Y i;
Compute the SNN average output Oi
mean = 1"
FORMULA OF TRAINING SNN WITH SURROGATE GRADIENTS,0.40892193308550184,"T
PT
t=1 Oi(t) over all time step;
Compare the classiﬁcation factor Oi
mean and Y i for classiﬁcation;
end"
TIME INHERITANCE TRAINING,0.41263940520446096,"4.3
TIME INHERITANCE TRAINING"
TIME INHERITANCE TRAINING,0.4163568773234201,"SNN demands simulation length long enough to obtain a satisfying performance, but the training
time consumption will increase linearly as the simulation length grows. So how to shorten the
training time is also an essential problem in the direct training ﬁeld. Traditional loss function LSDT
only optimizes the whole network output under a speciﬁc T, so its temporal scalability is poor.
Unlike the standard training, TET algorithm optimizes each moment’s output, enabling us to extend
the simulation time naturally. We introduce Time Inheritance Training (TIT) to alleviate the training
time problem. We ﬁrst use long epochs to train an SNN with a short simulation time T, e.g., 2. Then,
we increase the simulation time to the target value and retrain with short epochs. We discover that
TIT performs better than training from scratch on accuracy and signiﬁcantly saves the training time.
Assuming that training an SNN with simulation length T = 1 cost ts time per epoch, the SNN needs
300 epochs to train from scratch, and the TIT needs 50 epochs for ﬁnetuning. So we need 1800ts
time to train an SNN with T = 6 from scratch, but following the TIT pipeline with the initial T = 2
only requires 900ts. As a result, the TIT can reduce the training time cost by half."
EXPERIMENTS,0.4200743494423792,"5
EXPERIMENTS"
EXPERIMENTS,0.42379182156133827,"We validate our proposed TET algorithm and compare it with existing works on both static and
neuromorphic datasets. The network architectures in this paper include ResNet-19 (Zheng et al.,
2021), Spiking-ResNet34 (Zheng et al., 2021), SEW-ResNet34 (Fang et al., 2021), SNN-5, and
VGGSNN. SNN-5 (16C3-64C5-AP2-128C5-AP2-256C5-AP2-512C3-AP2-FC) is a simple convo-
lutional SNN suitable for multiple runs to discover statistical rules (Figure A. 7). The architec-
ture of VGGSNN (64C3-128C3-AP2-256C3-256C3-AP2-512C3-512C3-AP2-512C3-512C3-AP2-
FC) is based on VGG11 with two fully connected layers removed as we found that additional fully
connected layers were unnecessary for neuromorphic datasets."
MODEL VALIDATION AND ABLATION STUDY,0.4275092936802974,"5.1
MODEL VALIDATION AND ABLATION STUDY"
MODEL VALIDATION AND ABLATION STUDY,0.4312267657992565,"Effectiveness of TET over SDT with SG. We ﬁrst examine whether the mismatch between SG and
loss causes the convergence problem. For this purpose, we set the simulation length to 4 and change
the spike function Θ in Eqn.2 to Sigmoid σ(k · input). We ﬁnd that the TET and SDT achieved
similar accuracy (Table 2) when k = 1, 10, 20. This indicates that both TET and SDT work when
the gradient and loss function match each other. Next, we compare the results training with LSDT and
LTET on SNNs (ResNet-19 on CIFAR100) training with surrogate gradient for three runs. As shown
in Table 1, our proposed new TET training strategy dramatically increases the accuracy by 3.25%
when the simulation time is 4 and 3.53% when the simulation time is 6. These results quantitatively
support the effectiveness of TET in solving the mismatch between gradient and loss in training SNNs
with SG."
MODEL VALIDATION AND ABLATION STUDY,0.4349442379182156,Published as a conference paper at ICLR 2022
MODEL VALIDATION AND ABLATION STUDY,0.43866171003717475,"A
B
C
D"
MODEL VALIDATION AND ABLATION STUDY,0.4423791821561338,"Standard Direct Training
SDT Loss
Standard Direct Training
TET Loss"
MODEL VALIDATION AND ABLATION STUDY,0.44609665427509293,Temporal Efficient Training
MODEL VALIDATION AND ABLATION STUDY,0.44981412639405205,SDT Loss
MODEL VALIDATION AND ABLATION STUDY,0.45353159851301117,Temporal Efficient Training
MODEL VALIDATION AND ABLATION STUDY,0.45724907063197023,TET Loss
MODEL VALIDATION AND ABLATION STUDY,0.46096654275092935,"Figure 2: Loss landscape of VGGSNN. The 2D landscape of LSDT and LTET from two different
training methods."
MODEL VALIDATION AND ABLATION STUDY,0.4646840148698885,"Table 1: Comparison between SDT and TET.
We adopt the SNN architecture ResNet-19 with
SG on CIFAR100 and record the results with
three different simulation lengths 2, 4, and 6."
MODEL VALIDATION AND ABLATION STUDY,0.4684014869888476,"Method
T=2
T=4
T = 6
Direct training
69.41±0.08
70.86±0.22
71.12±0.57
TET
72.37±0.21
74.11±0.18
74.65±0.12"
MODEL VALIDATION AND ABLATION STUDY,0.4721189591078067,"Table 2: Comparison of SDT and TET with sig-
moid function σ(k·input). We ﬁx the simulation
length to 4 and record the results of CNN-5 un-
der three different k on CIFAR10."
MODEL VALIDATION AND ABLATION STUDY,0.4758364312267658,"Method
k=1
k=10
k=20
Direct training
88.00±0.15
88.83±0.32
88.50±0.32
TET
87.63±0.38
89.31±0.15
88.64±0.28"
MODEL VALIDATION AND ABLATION STUDY,0.4795539033457249,"Loss Landscape around Local Minima. We further inspect the 2D landscapes (Li et al., 2018) of
LSDT and LTET around their local minima (see Figure. 2) to demonstrate why TET generalizes better
than SDT and how TET helps the training process jump out of the sharp local minima typically
found by SDT. First, comparing Figure. 2 A and C, we can see that although the values of local
minima achieved by SDT and TET are similar in LSDT, the local minima of TET (Figure. 2 C)
is ﬂatter than that of SDT (Figure. 2 A). This indicates that the TET is effective in ﬁnding ﬂatter
minima that are typically more generalizable even w.r.t the original loss in TET. Next, we examine
the two local minima under LTET to see how it helps jump out the local minima found by SDT.
When comparing Figure. 2 B and D, we observe that the local minima found by SDT (Figure. 2 B)
is not only sharper than that found by TET (Figure. 2 D) under LSDT but also maintains a higher loss
value. This supports our claim that TET loss cannot be easily minimized around sharp local minima
(Figure. 2 B), thus preferable to converge into ﬂatter local minima (Figure. 2 D). Put together, the
results here provide evidence for our reasoning in Section 4.2."
MODEL VALIDATION AND ABLATION STUDY,0.483271375464684,"Training from SDT to TET. In this part, we further validate the ability of TET to escape from the
local minimum found by SDT. We adopt the VGGSNN with 300 epochs training on DVS-CIFAR10.
First, we optimize LSDT for 200 epochs and then change the loss function to LTET after epoch 200.
Figure 3 demonstrates the accuracy and loss change on the test set. After 200 epochs training, SDT"
MODEL VALIDATION AND ABLATION STUDY,0.48698884758364314,"SDT Loss (SDT)
TET Loss (SDT)
SDT Loss (Change to TET)
TET Loss (Change to TET)"
MODEL VALIDATION AND ABLATION STUDY,0.49070631970260226,Test Loss
MODEL VALIDATION AND ABLATION STUDY,0.4944237918215613,"Epochs
Epochs"
MODEL VALIDATION AND ABLATION STUDY,0.49814126394052044,Test Accuracy
MODEL VALIDATION AND ABLATION STUDY,0.5018587360594795,"SDT
Change to TET A
B"
MODEL VALIDATION AND ABLATION STUDY,0.5055762081784386,"Figure 3: TET helps to jump out the local minimum point. We provide the test accuracy (A) and
loss (B) change after changing the SDT to TET at epoch 200. TET efﬁciently improves the test
performance and reduces the two kinds of loss."
MODEL VALIDATION AND ABLATION STUDY,0.5092936802973977,Published as a conference paper at ICLR 2022
MODEL VALIDATION AND ABLATION STUDY,0.5130111524163569,Simulation length (T)
MODEL VALIDATION AND ABLATION STUDY,0.516728624535316,Accuracy 70.5 71.0 71.5 72.0 72.5 73.0 73.5 74.0 74.5 75.0 75.5
MODEL VALIDATION AND ABLATION STUDY,0.5204460966542751,"2
3
4
5
6
7
8"
MODEL VALIDATION AND ABLATION STUDY,0.5241635687732342,Energy (mJ)
MODEL VALIDATION AND ABLATION STUDY,0.5278810408921933,Accuracy
MODEL VALIDATION AND ABLATION STUDY,0.5315985130111525,"Direct training
Indirect training 69.0 70.0 71.0 72.0 73.0 74.0 75.0"
MODEL VALIDATION AND ABLATION STUDY,0.5353159851301115,"40
60
80
100
120
140
160
180 T = 2 T = 3"
MODEL VALIDATION AND ABLATION STUDY,0.5390334572490706,"T = 4
T = 5
T = 6 T = 2 T = 3"
MODEL VALIDATION AND ABLATION STUDY,0.5427509293680297,"T = 4
T = 5
T = 6 A
B"
MODEL VALIDATION AND ABLATION STUDY,0.5464684014869888,"Training with T = 2
Training with T = 3
Training with T = 4
Training from scratch"
MODEL VALIDATION AND ABLATION STUDY,0.550185873605948,"Figure 4: Time scalability robustness and network efﬁciency of ResNet-19 on CIFAR100. (A) The
comparison of training from scratch (dots) and inheriting from a small simulation length (lines). (B)
SNN network performance changes with energy consumption."
MODEL VALIDATION AND ABLATION STUDY,0.5539033457249071,"gets trapped into a local minimum, and the LSDT no longer decreases. The LTET is much higher
than LSDT since SDT does not optimize it. Nevertheless, after we change the loss function to LTET,
the LTET and LSDT on the test set both have a rapid decline. This phenomenon illustrates the TET
ability to help the SNN efﬁciently jump out of the local minimum with poor generalization and ﬁnd
another ﬂatter local minimum."
MODEL VALIDATION AND ABLATION STUDY,0.5576208178438662,"Time Scalability Robustness. Here, we study the time scalability robustness of SNNs trained with
TET (LTET). First, we use 300 epochs to train a small simulation length ResNet-19 on CIFAR100
as the initial SNN. Then, we directly change the simulation length from 2 to 8 without ﬁnetuning
and report the network accuracy on the test set. Figure. 4. A displays the results after changing the
simulation length. We use 2, 3, and 4, respectively, as the simulation length of the initial network.
When we increase the simulation length, the accuracy of all networks gradually increases. After the
simulation time reaches a certain value, the network performance will slightly decrease. Interest-
ingly, SNNs trained from scratch (T=4 and T=6) are not as good as those trained following the TIT
procedure."
MODEL VALIDATION AND ABLATION STUDY,0.5613382899628253,"Network Efﬁciency. In this section, we measure the relationship between energy consumption
and network performance. SNN avoids multiplication on the inference since its binary activation
and event-based operation. The addition operation in SNN costs 0.9pJ energy while multiplication
operation consumes 4.6pJ measured in 45nm CMOS technology (Rathi & Roy, 2020). In our
SNN model, the ﬁrst layer has multiplication operations, while the other layers only have addition
operations. Figure 4. B summarizes the results of different simulation times. In all cases, the SNN
obtained by TET has higher efﬁciency."
COMPARISON TO EXITING WORKS,0.5650557620817844,"5.2
COMPARISON TO EXITING WORKS"
COMPARISON TO EXITING WORKS,0.5687732342007435,"In this section, we compare our experimental results with previous works. We validate the full TIT
algorithm (LTOTAL) both on the static dataset and neuromorphic dataset. All of the experiment results
are summarized in Table 5.2. We specify all the training details in the appendix A.1."
COMPARISON TO EXITING WORKS,0.5724907063197026,"CIFAR. We apply TET and TIT algorithm on CIFAR (Krizhevsky et al., 2009), and report the mean
and standard deviation of 3 runs under different random seeds. The λ is set to 0.05. On CIFAR10,
our TET method achieves the highest accuracy above all existing approaches. Even when T = 2,
there is a 1.82% increment compare to STBP-tdBN with simulation length T = 6. It is worth noting
that our method is only 0.47% lower than the ANN performance. TET algorithm demonstrates
a more excellent ability on CIFAR100. It has an accuracy increase greater than 3% on all report
simulation lengths. In addition, when T = 6, the reported accuracy is only 0.63% lower than that
of ANN. We can see that the proposed TET’s improvement is even higher on complex data like
CIFAR100, where the generalizability of the model distinguishes a lot among minima with different
ﬂatness."
COMPARISON TO EXITING WORKS,0.5762081784386617,Published as a conference paper at ICLR 2022
COMPARISON TO EXITING WORKS,0.5799256505576208,"Table 3: Compare with existing works. Our method improves network performance across all tasks.
* denotes self-implementation results. † denotes data augmentation (Li et al., 2022)."
COMPARISON TO EXITING WORKS,0.5836431226765799,"Dataset
Model
Methods
Architecture
Simulation Length
Accuracy"
COMPARISON TO EXITING WORKS,0.587360594795539,CIFAR10
COMPARISON TO EXITING WORKS,0.5910780669144982,"Rathi et al. (2019)
Hybrid training
ResNet-20
250
92.22
Rathi & Roy (2020)
Diet-SNN
ResNet-20
10
92.54
Wu et al. (2018)
STBP
CIFARNet
12
89.83
Wu et al. (2019)
STBP NeuNorm
CIFARNet
12
90.53
Zhang & Li (2020)
TSSL-BP
CIFARNet
5
91.41"
COMPARISON TO EXITING WORKS,0.5947955390334573,"Zheng et al. (2021)
STBP-tdBN
ResNet-19
6
93.16
4
92.92
2
92.34"
COMPARISON TO EXITING WORKS,0.5985130111524164,"our model
TET
ResNet-19
6
94.50±0.07
4
94.44±0.08
2
94.16±0.03
ANN*
ANN
ResNet-19
1
94.97"
COMPARISON TO EXITING WORKS,0.6022304832713755,CIFAR100
COMPARISON TO EXITING WORKS,0.6059479553903345,"Rathi et al. (2019)
Hybrid training
VGG-11
125
67.87
Rathi & Roy (2020)
Diet-SNN
ResNet-20
5
64.07"
COMPARISON TO EXITING WORKS,0.6096654275092936,"Zheng et al. (2021)*
STBP-tdBN
ResNet-19
6
71.12±0.57
4
70.86±0.22
2
69.41±0.08"
COMPARISON TO EXITING WORKS,0.6133828996282528,"our model
TET
ResNet-19
6
74.72±0.28
4
74.47±0.15
2
72.87±0.10
ANN*
ANN
ResNet-19
1
75.35"
COMPARISON TO EXITING WORKS,0.6171003717472119,ImageNet
COMPARISON TO EXITING WORKS,0.620817843866171,"Rathi et al. (2019)
Hybrid training
ResNet-34
250
61.48
Sengupta et al. (2018)
SPIKE-NORM
ResNet-34
2500
69.96
Zheng et al. (2021)
STBP-tdBN
Spiking-ResNet-34
6
63.72
Fang et al. (2021)
SEW ResNet
SEW-ResNet-34
4
67.04"
COMPARISON TO EXITING WORKS,0.6245353159851301,"our model
TET
Spiking-ResNet-34
6
64.79
TET
SEW-ResNet-34
4
68.00"
COMPARISON TO EXITING WORKS,0.6282527881040892,DVS-CIFAR10
COMPARISON TO EXITING WORKS,0.6319702602230484,"Zheng et al. (2021)
STBP-tdBN
ResNet-19
10
67.8
Kugele et al. (2020)
Streaming Rollout
DenseNet
10
66.8
Wu et al. (2021)
Conv3D
LIAF-Net
10
71.70
Wu et al. (2021)
LIAF
LIAF-Net
10
70.40"
COMPARISON TO EXITING WORKS,0.6356877323420075,"our model
TET
VGGSNN
10
77.33±0.21
TET†
VGGSNN
10
83.17±0.15"
COMPARISON TO EXITING WORKS,0.6394052044609665,"ImageNet. The training set of ImageNet (Krizhevsky et al., 2012) provides 1.28k training samples
for each label. We choose the two most representative ResNet-34 to verify our algorithm on Ima-
geNet with λ = 0.001. SEW-ResNet34 is not a typical SNN since it adopts the IF model and mod-
iﬁes the Residual structure. Although we only train our model for 120 epochs, the TET algorithm
achieves a 1.07% increment on Spiking-ResNet-34 and a 0.96% increment on SEW-ResNet34."
COMPARISON TO EXITING WORKS,0.6431226765799256,"DVS-CIFAR10. The neuromorphic datasets suffer much more noise than static datasets. Thus the
well-trained SNN is easier to overﬁt on these datasets than static datasets. DVS-CIFAR10 (Li et al.,
2017), which provides each label with 0.9k training samples, is the most challenging mainstream
neuromorphic dataset. Recent works prefer to deal with this dataset by complex architectures, which
are more susceptible to overﬁtting and do not result in very high accuracy. Here, we adopt VGGSNN
on the DVS-CIFAR10 dataset, set λ = 0.001, and report the mean and standard deviation of 3
runs under different random seeds. Along with data augmentation methods, VGGSNN can achieve
an accuracy of 77.4%. Then we apply the TET method to obtain a more generalizable optima.
The accuracy rises to 83.17%. Our TET method outperforms existing state-of-the-art by 11.47%
accuracy. Without data augmentation methods, VGGSNN obtains 73.3% accuracy by SDT and
77.3% accuracy by TET."
CONCLUSION,0.6468401486988847,"6
CONCLUSION"
CONCLUSION,0.6505576208178439,"This paper focuses on the SNN generalization problem, which is described as the direct training
SNN performs well on the training set but poor on the test set. We ﬁnd this phenomenon is due to
the incorrect SG that makes the SNN easily trapped into a local minimum with poor generalization.
To solve this problem, we propose the temporal efﬁcient training algorithm (TET). Extensive ex-
periments verify that our proposed method consistently achieves better performance than the SDT
process. Furthermore, TET signiﬁcantly improves the time scalability robustness of SNN, which"
CONCLUSION,0.654275092936803,Published as a conference paper at ICLR 2022
CONCLUSION,0.6579925650557621,"enables us to propose the time inheritance training (TIT) to signiﬁcantly reduce the training time
consumption by almost a half."
ACKNOWLEDGMENT,0.6617100371747212,"7
ACKNOWLEDGMENT"
ACKNOWLEDGMENT,0.6654275092936803,"This project is supported by NSFC 61876032 and JCYJ20210324140807019. Y. Li completed this
work during his prior research assistantship in UESTC."
REFERENCES,0.6691449814126395,REFERENCES
REFERENCES,0.6728624535315985,"Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha
Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic
manycore processor with on-chip learning. Ieee Micro, 38(1):82–99, 2018."
REFERENCES,0.6765799256505576,"Michael V DeBole, Brian Taba, Arnon Amir, Filipp Akopyan, Alexander Andreopoulos, William P
Risk, Jeff Kusnitz, Carlos Ortega Otero, Tapan K Nayak, Rathinakumar Appuswamy, et al.
Truenorth: Accelerating from zero to 64 million neurons in 10 years. Computer, 52(5):20–29,
2019."
REFERENCES,0.6802973977695167,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.6840148698884758,"Lei Deng, Yujie Wu, Xing Hu, Ling Liang, Yufei Ding, Guoqi Li, Guangshe Zhao, Peng Li, and
Yuan Xie. Rethinking the performance comparison between snns and anns. Neural Networks,
121:294 – 307, 2020."
REFERENCES,0.6877323420074349,"Shikuang Deng and Shi Gu. Optimal conversion of conventional artiﬁcial neural networks to spiking
neural networks. In International Conference on Learning Representations, 2020."
REFERENCES,0.6914498141263941,"Peter U. Diehl, Daniel Neil, Jonathan Binas, Matthew Cook, and Shih Chii Liu. Fast-classifying,
high-accuracy spiking deep networks through weight and threshold balancing. In Neural Net-
works (IJCNN), 2015 International Joint Conference on, 2015."
REFERENCES,0.6951672862453532,"Peter U Diehl, Guido Zarrella, Andrew Cassidy, Bruno U Pedroni, and Emre Neftci. Conversion
of artiﬁcial recurrent neural networks to spiking neural networks for low-power neuromorphic
hardware. In 2016 IEEE International Conference on Rebooting Computing (ICRC), pp. 1–8.
IEEE, 2016."
REFERENCES,0.6988847583643123,"Steven K Esser, Paul A Merolla, John V Arthur, Andrew S Cassidy, Rathinakumar Appuswamy,
Alexander Andreopoulos, David J Berg, Jeffrey L McKinstry, Timothy Melano, Davis R Barch,
et al. Convolutional networks for fast, energy-efﬁcient neuromorphic computing. Proceedings of
the national academy of sciences, 113(41):11441–11446, 2016."
REFERENCES,0.7026022304832714,"Wei Fang, Zhaofei Yu, Yanqi Chen, Tiejun Huang, Timoth´ee Masquelier, and Yonghong Tian. Deep
residual learning in spiking neural networks. arXiv preprint arXiv:2102.04159, 2021."
REFERENCES,0.7063197026022305,"Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017."
REFERENCES,0.7100371747211895,"Bing Han and Kaushik Roy. Deep spiking neural network: Energy efﬁciency through time based
coding. In European Conference on Computer Vision, 2020."
REFERENCES,0.7137546468401487,"Bing Han, Gopalakrishnan Srinivasan, and Kaushik Roy. Rmp-snn: Residual membrane potential
neuron for enabling deeper high-accuracy and low-latency spiking neural network. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13558–13567,
2020."
REFERENCES,0.7174721189591078,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015."
REFERENCES,0.7211895910780669,Published as a conference paper at ICLR 2022
REFERENCES,0.724907063197026,"Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham Kakade. On the insufﬁciency of ex-
isting momentum schemes for stochastic optimization. In 2018 Information Theory and Applica-
tions Workshop (ITA), pp. 1–9. IEEE, 2018."
REFERENCES,0.7286245353159851,"Seijoon Kim, Seongsik Park, Byunggook Na, and Sungroh Yoon. Spiking-yolo: Spiking neural
network for energy-efﬁcient object detection. arXiv preprint arXiv:1903.06530, 2019."
REFERENCES,0.7323420074349443,"Youngeun Kim and Priyadarshini Panda. Visual explanations from spiking neural networks using
inter-spike intervals. Scientiﬁc reports, 11(1):1–14, 2021."
REFERENCES,0.7360594795539034,"Youngeun Kim, Yeshwanth Venkatesha, and Priyadarshini Panda.
Privatesnn: Fully privacy-
preserving spiking neural networks. arXiv preprint arXiv:2104.03414, 2021."
REFERENCES,0.7397769516728625,"Youngeun Kim, Yuhang Li, Hyoungseob Park, Yeshwanth Venkatesha, and Priyadarshini Panda.
Neural architecture search for spiking neural networks. arXiv preprint arXiv:2201.10355, 2022."
REFERENCES,0.7434944237918215,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.7472118959107806,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.7509293680297398,"Alex Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural
networks. Advances in neural information processing systems, 25(2), 2012."
REFERENCES,0.7546468401486989,"Alexander Kugele, Thomas Pfeil, Michael Pfeiffer, and Elisabetta Chicca. Efﬁcient processing of
spatio-temporal data streams with spiking neural networks. Frontiers in Neuroscience, 14:439,
2020."
REFERENCES,0.758364312267658,"Jun Haeng Lee, Tobi Delbruck, and Michael Pfeiffer. Training deep spiking neural networks using
backpropagation. Frontiers in neuroscience, 10:508, 2016."
REFERENCES,0.7620817843866171,"Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-
scape of neural nets. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, pp. 6391–6401, 2018."
REFERENCES,0.7657992565055762,"Hongmin Li, Hanchao Liu, Xiangyang Ji, Guoqi Li, and Luping Shi. Cifar10-dvs: an event-stream
dataset for object classiﬁcation. Frontiers in neuroscience, 11:309, 2017."
REFERENCES,0.7695167286245354,"Yuhang Li, Shikuang Deng, Xin Dong, Ruihao Gong, and Shi Gu. A free lunch from ann: Towards
efﬁcient, accurate spiking neural networks calibration. arXiv preprint arXiv:2106.06984, 2021a."
REFERENCES,0.7732342007434945,"Yuhang Li, Yufei Guo, Shanghang Zhang, Shikuang Deng, Yongqing Hai, and Shi Gu. Differ-
entiable spike: Rethinking gradient-descent for training spiking neural networks. Advances in
Neural Information Processing Systems, 34, 2021b."
REFERENCES,0.7769516728624535,"Yuhang Li, Youngeun Kim, Hyoungseob Park, Tamar Geller, and Priyadarshini Panda. Neuromor-
phic data augmentation for training spiking neural networks. arXiv preprint arXiv:2203.06145,
2022."
REFERENCES,0.7806691449814126,"Emre O Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking
neural networks: Bringing the power of gradient-based optimization to spiking neural networks.
IEEE Signal Processing Magazine, 36(6):51–63, 2019."
REFERENCES,0.7843866171003717,"Nitin Rathi and Kaushik Roy. Diet-snn: Direct input encoding with leakage and threshold optimiza-
tion in deep spiking neural networks. arXiv preprint arXiv:2008.03658, 2020."
REFERENCES,0.7881040892193308,"Nitin Rathi, Gopalakrishnan Srinivasan, Priyadarshini Panda, and Kaushik Roy.
Enabling deep
spiking neural networks with hybrid conversion and spike timing dependent backpropagation. In
International Conference on Learning Representations, 2019."
REFERENCES,0.79182156133829,"Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, and Michael Pfeiffer. Theory and tools for
the conversion of analog to spiking convolutional neural networks. arXiv: Statistics/Machine
Learning, (1612.04052):0–0, 2016."
REFERENCES,0.7955390334572491,Published as a conference paper at ICLR 2022
REFERENCES,0.7992565055762082,"Ali Samadzadeh, Fatemeh Sadat Tabatabaei Far, Ali Javadi, Ahmad Nickabadi, and Morteza Haghir
Chehreghani. Convolutional spiking neural networks for spatio-temporal feature extraction. arXiv
preprint arXiv:2003.12346, 2020."
REFERENCES,0.8029739776951673,"Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch nor-
malization help optimization?
In Proceedings of the 32nd international conference on neural
information processing systems, pp. 2488–2498, 2018."
REFERENCES,0.8066914498141264,"Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy. Going deeper in spiking
neural networks: Vgg and residual architectures. Frontiers in Neuroence, 13, 2018."
REFERENCES,0.8104089219330854,"Sumit Bam Shrestha and Garrick Orchard. Slayer: Spike layer error reassignment in time. In
Advances in Neural Information Processing Systems, pp. 1412–1421, 2018."
REFERENCES,0.8141263940520446,"Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. Spatio-temporal backpropagation for
training high-performance spiking neural networks. Frontiers in neuroscience, 12:331, 2018."
REFERENCES,0.8178438661710037,"Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, Yuan Xie, and Luping Shi. Direct training for spiking neural
networks: Faster, larger, better. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 33, pp. 1311–1318, 2019."
REFERENCES,0.8215613382899628,"Zhenzhi Wu, Hehui Zhang, Yihan Lin, Guoqi Li, Meng Wang, and Ye Tang. Liaf-net: Leaky inte-
grate and analog ﬁre network for lightweight and efﬁcient spatiotemporal information processing.
IEEE Transactions on Neural Networks and Learning Systems, 2021."
REFERENCES,0.8252788104089219,"Yukun Yang, Wenrui Zhang, and Peng Li. Backpropagated neighborhood aggregation for accu-
rate training of spiking neural networks. In International Conference on Machine Learning, pp.
11852–11862. PMLR, 2021."
REFERENCES,0.828996282527881,"Wenrui Zhang and Peng Li. Temporal spike sequence learning via backpropagation for deep spiking
neural networks. arXiv preprint arXiv:2002.10085, 2020."
REFERENCES,0.8327137546468402,"Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu, and Guoqi Li. Going deeper with directly-trained
larger spiking neural networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 35, pp. 11062–11070, 2021."
REFERENCES,0.8364312267657993,Published as a conference paper at ICLR 2022
REFERENCES,0.8401486988847584,"A
APPENDIX"
REFERENCES,0.8438661710037175,"A.1
DATASET AND TRAINING DETAIL"
REFERENCES,0.8475836431226765,"CIFAR. The CIFAR dataset (Krizhevsky et al., 2009) consists of 50k training images and 10k testing
images with the size of 32 × 32. We use ResNet-19 for both CIFAR10 and CIFAR100. Moreover,
random horizontal ﬂip and crop are applied to the training images the augmentation. First, we use
300 epoch to train the SNN with the simulation length T = 2. We use an Adam optimizer with
a learning rate of 0.01 and cosine decay to 0. Next, following the TIT algorithm, we increase the
simulation time (to 4 and 6) and continue training the SNN for only 50 epochs, with the learning
rate changing to 1e −4."
REFERENCES,0.8513011152416357,"ImageNet. ImageNet (Deng et al., 2009) contains more than 1250k training images and 50k valida-
tion images. We crop the images to 224×224 and using the standard augmentation for the training
data. We use an SGD optimizer with 0.9 momentum and weight decay 4e −5. The learning rate is
set to 0.1 and cosine decay to 0. We train the SEW-ResNet34 (Fang et al., 2021) with T = 4 for 120
epochs. As for the Spiking-ResNet34 (Zheng et al., 2021), we use TIT algorithm to train 90 epochs
with T = 4 ﬁrst, then change the simulation time to 6 and ﬁnetune the network for 30 epochs.
We adopt an Adam optimizer on the ﬁnetune phase and change the learning rate to 1e −4. TIT
algorithm signiﬁcantly reduces the training time consumption since training the Spiking-ResNet34
is extremely slow."
REFERENCES,0.8550185873605948,"DVS-CIFAR10. DVS-CIFAR10 (Li et al., 2017), the most challenging mainstream neuromor-
phic data set, is converted from CIFAR10. It has 10k images with the size 128×128. Following
Samadzadeh et al. (2020), we divide the data stream into 10 blocks by time and accumulate the
spikes in each block. Then, we split the dataset into 9k training images and 1k test images and
reduce the spatial resolution to 48×48. Random horizontal ﬂip and random roll within 5 pixels are
taken as augmentation (Li et al., 2022). We adopt VGGSNN architecture with 300 epochs training
on this classiﬁcation task. And we use an Adam optimizer with the learning rate 1e −3 and cosine
decay to 0. As for the case that does not apply any augmentation, we add a weight decay of 5e-4 to
the optimizer."
REFERENCES,0.8587360594795539,"A.2
LSDT LOSS LANDSCAPE OF RESNET-19"
REFERENCES,0.862453531598513,"Here we compare the classiﬁcation loss (LSDT) landscapes of ResNet-19 on CIFAR100. The po-
sition around the local minimal value found by the SDT (LSDT) is very sharp. However, the area
around the local minimum found by TET (LTET) is much smoother (Figure 5), which indicates that
TET effectively improves the network generalization. Such improvements could be further utilized
to other techniques like privacy-preserving data generalization (Kim et al., 2021) and neural archi-
tecture search (Kim et al., 2022)."
REFERENCES,0.8661710037174721,"Standard Direct Training
SDT Loss
Temporal Efficient Training
SDT Loss"
REFERENCES,0.8698884758364313,Figure 5: STD loss landscape of ResNet-19 on CIFAR100 from different training approaches.
REFERENCES,0.8736059479553904,"A.3
EFFECT OF LMSE"
REFERENCES,0.8773234200743495,"In this part, we examine the effect of the regular term LMSE with 5 different levels of λ. Figure
6 Summarizes the ﬁnal results. The regular term LMSE effectively increases the performance of"
REFERENCES,0.8810408921933085,Published as a conference paper at ICLR 2022
REFERENCES,0.8847583643122676,"both ResNet-19 on CIFAR100 and VGGSNN on DVS-CIFAR10. The static dataset CIFAR100 is
more suitable for larger λ, while smaller λ is suitable for DVS-CIFAR10. Theoretically, it is hard to
obtain satisfying performance at the early simulation moment due to the sparseness of neuromorphic
datasets. So too large regular term LMSE is not suitable for the neuromorphic dataset. Furthermore,
we ﬁnd that a high λ may harm the early training phase on ImageNet, especially if zero-initialize
(Goyal et al., 2017) is not performed. As a result, we set λ to 5e −2 for CIFAR10 and CIFAR100,
1e −3 for ImageNet and DVS-CIFAR10."
REFERENCES,0.8884758364312267,"λ
0
1e-3
1e-2
5e-2
1e-1"
REFERENCES,0.8921933085501859,Accuracy
REFERENCES,0.895910780669145,"λ
0
1e-3
1e-2
5e-2
1e-1"
REFERENCES,0.8996282527881041,Accuracy
REFERENCES,0.9033457249070632,"ResNet-19 on CIFAR100
VGGSNN on DVS-CIFAR10"
REFERENCES,0.9070631970260223,Figure 6: The accuracy under different levels of λ.
REFERENCES,0.9107806691449815,"A.4
STATISTICAL RESULTS"
REFERENCES,0.9144981412639405,"Here we provide statistical results (Figure 7) to prove that the total SNN accuracy is positively
associated with every average of moment’s output test accuracy. We train CNN-5 on CIFAR10 for
a total of 20 runs with SDT and 5 runs with TET."
REFERENCES,0.9182156133828996,"86.5
87
87.5
88
88.5
89
89.5 89.2 89.4 89.6 89.8 90 90.2 90.4 90.6"
REFERENCES,0.9219330855018587,"r = 0.899
p = 4.302e-10"
REFERENCES,0.9256505576208178,Average accuracy of single time steps
REFERENCES,0.929368029739777,SNN accuracy
REFERENCES,0.9330855018587361,"Figure 7: Statistical results. The overall performance of SNN is highly positively associated with
the average accuracy of each moment. The standard training obtains the green dots, while the red
dots are trained by the TET method."
REFERENCES,0.9368029739776952,"A.5
TIME SCALABILITY ROBUSTNESS OF SDT AND TET."
REFERENCES,0.9405204460966543,"Here we ﬁrst show the test accuracy (ResNet19 on CIFAR100) of the membrane potential increment
at each moment instead of the integrated membrane potential. We set the initial simulation length of
the SNNs to 3 or 4 and trained them for a full 300 epochs. Then we expand their simulation length
to 8. As shown in table 4, TET (LTET) makes the membrane potential increment at each moment
have a higher classiﬁcation ability than SDT (LSDT). And TET (1.41 and 0.08) also acquires a low
accuracy variance than SDT (3.81 and 4.04)."
REFERENCES,0.9442379182156134,"Then we compare the time scalability robustness between SDT (LSDT) and TET (LTET). We set the
initial simulation length of ResNet19 SNNs to 2, 3, 4 and train with SDT or TET. Then we gradually
increase SNN simulation length to 64 and record test accuracy of the integrated membrane potential."
REFERENCES,0.9479553903345725,Published as a conference paper at ICLR 2022
REFERENCES,0.9516728624535316,Simulation Length
REFERENCES,0.9553903345724907,Accuracy
REFERENCES,0.9591078066914498,"0
10
20
30
40
50
60
70
62.0 64.0 66.0 68.0 70.0 72.0 74.0 76.0"
REFERENCES,0.9628252788104089,"SDT with T = 4
SDT with T = 3
SDT with T = 2
TET with T = 4
TET with T = 3
TET with T = 2"
REFERENCES,0.966542750929368,"SDT with T = 4
SDT with T = 3
SDT with T = 2
TET with T = 4
TET with T = 3
TET with T = 2"
REFERENCES,0.9702602230483272,Simulation Length
REFERENCES,0.9739776951672863,"0
10
20
30
40
50
60
70"
REFERENCES,0.9776951672862454,Accuracy  relative growth rate 0.88 0.90 0.92 0.94 0.96 0.98 1.00 1.02 1.04 1.06
"A
B",0.9814126394052045,"1.08
A
B"
"A
B",0.9851301115241635,"Figure 8: The accuracy after increasing the simulation length. We ﬁrst train the SNN with TET (only
use LTET) and SDT (LSDT) with simulation length (T) is 2, 3, or 4. Then, we increase the simulation
to 64 without ﬁnetuning and record the test the classiﬁcation accuracy (A) and the accuracy relative
growth rate (B) of the total SNN output (integrate membrane potential) at each simulation time."
"A
B",0.9888475836431226,"As we increase the simulation length, all the SNNs’ accuracy will ﬁrst increase and then be stable
in a certain area. Meanwhile, TET (1.80) has a small accuracy variance than the SDT (11.13) after
increasing the simulation length. This phenomenon indicates that the initialization steps of TIT only
need a small simulation length SNN for TET but a sufﬁciently large simulation (or enough epochs
for ﬁnetuning step) for SDT."
"A
B",0.9925650557620818,"Table 4: Accuracy of each moment’s membrane potential increment. We use LSDT or LTET to train
the networks with simulation length 3 or 4. Then directly increase their simulation length to 8 and
record each moment’s potential increment test accuracy."
"A
B",0.9962825278810409,"Method
T=1
T=2
T=3
T=4
T=5
T=6
T=7
T=8
SDT (T=3)
55.61
57.95
56.87
55.09
57.56
53.54
57.72
54.04
SDT (T=4)
37.96
61.78
55.03
56.64
57.47
54.24
58.74
55.48
TET (T=3)
65.97
72.22
71.78
70.55
71.90
69.57
72.15
69.78
TET (T=4)
62.17
71.57
71.05
72.08
71.77
71.23
71.81
71.36"
