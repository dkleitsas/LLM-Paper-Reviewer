Section,Section Appearance Order,Paragraph
WESTLAKE UNIVERSITY,0.0,"1Westlake University
2Zhejiang University
3Tsinghua University
4Westlake Institute for Advanced Study
5Shanghai Qi Zhi Institute
6Shanghai AI Lab"
ABSTRACT,0.0017006802721088435,Abstract
ABSTRACT,0.003401360544217687,"In this paper, we present ContExtual Imitation Learning (CEIL), a general and
broadly applicable algorithm for imitation learning (IL). Inspired by the formula-
tion of hindsight information matching, we derive CEIL by explicitly learning a
hindsight embedding function together with a contextual policy using the hindsight
embeddings. To achieve the expert matching objective for IL, we advocate for
optimizing a contextual variable such that it biases the contextual policy towards
mimicking expert behaviors. Beyond the typical learning from demonstrations
(LfD) setting, CEIL is a generalist that can be effectively applied to multiple settings
including: 1) learning from observations (LfO), 2) offline IL, 3) cross-domain IL
(mismatched experts), and 4) one-shot IL settings. Empirically, we evaluate CEIL
on the popular MuJoCo tasks (online) and the D4RL dataset (offline). Compared to
prior state-of-the-art baselines, we show that CEIL is more sample-efficient in most
online IL tasks and achieves better or competitive performances in offline tasks."
INTRODUCTION,0.00510204081632653,"1
Introduction"
INTRODUCTION,0.006802721088435374,"Imitation learning (IL) allows agents to learn from expert demonstrations. Initially developed with a
supervised learning paradigm [58, 63], IL can be extended and reformulated with a general expert
matching objective, which aims to generate policies that produce trajectories with low distributional
distances to expert demonstrations [30]. This formulation allows IL to be extended to various
new settings: 1) online IL where interactions with the environment are allowed, 2) learning from
observations (LfO) where expert actions are absent, 3) offline IL where agents learn from limited
expert data and a fixed dataset of sub-optimal and reward-free experience, 4) cross-domain IL where
the expert demonstrations come from another domain (i.e., environment) that has different transition
dynamics, and 5) one-shot IL which expects to recover the expert behaviors when only one expert
trajectory is observed for a new IL task."
INTRODUCTION,0.008503401360544218,"Modern IL algorithms introduce various designs or mathematical principles to cater to the expert
matching objective in a specific scenario. For example, the LfO setting requires particular considera-
tions regarding the absent expert actions, e.g., learning an inverse dynamics function [5, 65]. Besides,
out-of-distribution issues in offline IL require specialized modifications to the learning objective,
such as introducing additional policy/value regularization [32, 72]. However, such a methodology,
designing an individual formulation for each IL setting, makes it difficult to scale up a specific IL
algorithm to more complex tasks beyond its original IL setting, e.g., online IL methods often suffer
severe performance degradation in offline IL settings. Furthermore, realistic IL tasks are often not
subject to a particular IL setting but consist of a mixture of them. For example, we may have access"
INTRODUCTION,0.01020408163265306,∗Equal contributions. Corresponding author: Donglin Wang <wangdonglin@westlake.edu.cn>
INTRODUCTION,0.011904761904761904,"Table 1: A coarse summary of IL methods demonstrating 1) different expert data modalities they
can handle (learning from demonstrations or observations), 2) disparate task settings they consider
(learning from online environment interactions or pre-collected offline static dataset), 3) the specific
cross-domain setting they assume (the transition dynamics between the learning environment and that
of the expert behaviors are different), and 4) the unique one-shot merit they desire (the learned policy
is capable of one-shot transfer to new imitation tasks). We highlight that our contextual imitation
learning (CEIL) method can naturally be applied to all the above IL settings."
INTRODUCTION,0.013605442176870748,"Expert data
Task setting
Cross-domain
One-shot
LfD
LfO
Online
Offline"
INTRODUCTION,0.015306122448979591,"S-on-LfD [9, 13, 21, 30, 38, 52, 57, 61, 77]
""
%
""
%
%
%
S-on-LfO [7, 54, 65, 66, 75]
""
""
""
%
%
%
S-off-LfD [19, 32, 33, 39, 55, 70, 72, 73]
""
%
%
""
%
%
S-off-LfO [78]
""
""
%
""
%
%
C-on-LfD [18, 69, 79]
""
%
""
%
""
%
C-on-LfO [20, 25, 26, 48, 59, 60]
""
""
""
%
""
%
C-off-LfD [34]
""
%
%
""
""
%
C-off-LfO [56, 68]
""
""
%
""
""
%"
INTRODUCTION,0.017006802721088437,"S-on/off-LfO [28]
""
""
""
""
%
%
Online one-shot [14, 16, 40]
""
%
""
%
%
""
Offline one-shot [24, 71]
""
%
%
""
%
""
CEIL (ours)
""
""
""
""
""
"""
INTRODUCTION,0.01870748299319728,"to both demonstrations and observation-only data in offline robot tasks; however, it could require
significant effort to adapt several specialized methods to leverage such mixed/hybrid data. Hence,
a problem naturally arises: How can we accommodate various design requirements of different IL
settings with a general and practically ready-to-deploy IL formulation?"
INTRODUCTION,0.02040816326530612,"Hindsight information matching, a task-relabeling paradigm in reinforcement learning (RL), views
control tasks as analogous to a general sequence modeling problem, with the goal to produce a
sequence of actions that induces high returns [12]. Its generality and simplicity enable it to be
extended to both online and offline settings [17, 42]. In its original RL context, an agent directly uses
known extrinsic rewards to bias the hindsight information towards task-related behaviors. However,
when we attempt to retain its generality in IL tasks, how to bias the hindsight towards expert behaviors
remains a significant barrier as the extrinsic rewards are missing."
INTRODUCTION,0.022108843537414966,"To design a general IL formulation and tackle the above problems, we propose ContExtual Imitation
Learning (CEIL), which readily incorporates the hindsight information matching principle within a
bi-level expert matching objective. In the inner-level optimization, we explicitly learn a hindsight
embedding function to deal with the challenges of unknown rewards. In the outer-level optimization,
we perform IL expert matching via inferring an optimal embedding (i.e., hindsight embedding
biasing), replacing the naive reward biasing in hindsight. Intuitively, we find that such a bi-level
objective results in a spectrum of expert matching objectives from the embedding space to the
trajectory space. To shed light on the applicability and generality of CEIL, we instantiate CEIL to
various IL settings, including online/offline IL, LfD/LfO, cross-domain IL, and one-shot IL settings."
INTRODUCTION,0.023809523809523808,"In summary, this paper makes the following contributions: 1) We propose a bi-level expert match-
ing objective ContExtual Imitation Learning (CEIL), inheriting the spirit of hindsight information
matching, which decouples the learning policy into a contextual policy and an optimal embedding.
2) CEIL exhibits high generality and adaptability and can be instantiated over a range of IL tasks.
3) Empirically, we conduct extensive empirical analyses showing that CEIL is more sample-efficient
in online IL and achieves better or competitive results in offline IL tasks."
RELATED WORK,0.025510204081632654,"2
Related Work"
RELATED WORK,0.027210884353741496,"Recent advances in decision-making have led to rapid progress in IL settings (Table 1), from typical
learning from demonstrations (LfD) to learning from observations (LfO) [7, 9, 35, 54, 62, 66],
from online IL to offline IL [11, 15, 33, 53, 73], and from single-domain IL to cross-domain
IL [34, 48, 56, 68]. Targeting a specific IL setting, individual works have shown their impressive"
RELATED WORK,0.02891156462585034,"ability to solve the exact IL setting. However, it is hard to retrain their performance in new unprepared
IL settings. In light of this, it is tempting to consider how we can design a general and broadly
applicable IL method. Indeed, a number of prior works have studied part of the above IL settings, such
as offline LfO [78], cross-domain LfO [48, 60], and cross-domain offline IL [56]. While such works
demonstrate the feasibility of tackling multiple IL settings, they still rely on standard online/offline
RL algorithmic advances to improve performance [25, 32, 44, 47, 50, 51, 55, 72, 76]. Our objective
diverges from these works, as we strive to minimize the reliance on the RL pipeline by replacing it
with a simple supervision objective, thus avoiding the dependence on the choice of RL algorithms."
RELATED WORK,0.030612244897959183,"Our approach to IL is most closely related to prior hindsight information-matching methods [2, 8, 24,
49], both learning a contextual policy and using a contextual variable to guide policy improvement.
However, these prior methods typically require additional mechanisms to work well, such as extrinsic
rewards in online RL [4, 42, 64] or a handcrafted target return in offline RL [12, 17]. Our method
does not require explicit handling of these components. By explicitly learning an embedding space
for both expert and suboptimal behaviors, we can bias the contextual policy with an inferred optimal
embedding (contextual variable), thus avoiding the need for explicit reward biasing in prior works.
Our method also differs from most prior offline transformer-based RL/IL algorithms that explicitly
model a long sequence of transitions [10, 12, 31, 36, 43, 71]. We find that simple fully-connected
networks can also elicit useful embeddings and guide expert behaviors when conditioned on a well-
calibrated embedding. In the context of the recently proposed prompt-tuning paradigm in large
language tasks or multi-modal tasks [27, 45, 74], our method can be interpreted as a combination of
IL and prompting-tuning, with the main motivation that we tune the prompt (the optimal contextual
variable) with an expert matching objective in IL settings."
BACKGROUND,0.03231292517006803,"3
Background"
BACKGROUND,0.034013605442176874,"Before discussing our method, we briefly introduce the background for IL, including learning from
demonstrations (LfD), learning from observations (LfO), online IL, offline IL, and cross-domain
settings in Section 3.1, and introduce the hindsight information matching in Section 3.2."
IMITATION LEARNING,0.03571428571428571,"3.1
Imitation Learning"
IMITATION LEARNING,0.03741496598639456,"Consider a control task formulated as a discrete-time Markov decision process (MDP)2 M =
{S, A, T , r, γ, p0}, where S is the state (observation) space, A is the action space, T : S×A×S →R
is the transition dynamics function, r : S ×A →R is the reward function, γ is the discount factor, and
p0 is the distribution of initial states. The goal in a reinforcement learning (RL) control task is to learn
a policy πθ(a|s) maximizing the expected sum of discounted rewards Eπθ(τ)
hPT −1
t=0 γtr(st, at)
i
,"
IMITATION LEARNING,0.0391156462585034,"where τ := {s0, a0, · · · , sT −1, aT −1} denotes the trajectory and the generated trajectory distribution
πθ(τ) = p0(s0)πθ(a0|s0) QT −1
t=1 πθ(at|st)T (st|st−1, at−1)."
IMITATION LEARNING,0.04081632653061224,"In IL, the ground truth reward function (i.e., r in M) is not observed. Instead, we have access to a set
of demonstrations (or observations) {τ|τ ∼πE(τ)} that are collected by an unknown expert policy
πE(a|s). The goal of IL tasks is to recover a policy that matches the corresponding expert policy.
From the mathematical perspective, IL achieves the plain expert matching objective by minimizing
the divergence of trajectory distributions between the learner and the expert:"
IMITATION LEARNING,0.04251700680272109,"min
πθ
D(πθ(τ), πE(τ)),
(1)"
IMITATION LEARNING,0.04421768707482993,"where D is a distance measure. Meanwhile, we emphasize that the given expert data {τ|τ ∼
πE(τ)} may not contain the corresponding expert actions. Thus, in this work, we consider two IL
cases where the given expert data τ consists of a set of state-action demonstrations {(st, at, st+1)}
(learning from demonstrations, LfD), as well as a set of state-only transitions {(st, st+1)} (learning
from observations, LfO). When it is clear from context, we abuse notation πE(τ) to denote both
demonstrations in LfD and observations in LfO for simplicity."
IMITATION LEARNING,0.04591836734693878,"Besides, we can also divide IL settings into two orthogonal categories: online IL and offline IL. In
online IL, the learning policy πθ can interact with the environment and generate online trajectories
τ ∼πθ(τ). In offline IL, the agent cannot interact with the environment but has access to an offline"
IMITATION LEARNING,0.047619047619047616,"2In this paper, we use environment and MDP interchangeably, and use state and observation interchangeably."
IMITATION LEARNING,0.04931972789115646,"static dataset {τ|τ ∼πβ(τ)}, collected by some unknown (sub-optimal) behavior policies πβ. By
leveraging the offline data {πβ(τ)} ∪{πE(τ)} without any interactions with the environment, the
goal of offline IL is to learn a policy recovering the expert behaviors (demonstrations or observations)
generated by πE. Note that, in contrast to the typical offline RL problem [46], the offline data
{πβ(τ)} in offline IL does not contains any reward signal."
IMITATION LEARNING,0.05102040816326531,"Cross-domain IL. Beyond the above two IL branches (online/offline and LfD/LfO), we can also
divide IL into: 1) single-domain IL and 2) cross-domain IL, where 1) the single-domain IL assumes
that the expert behaviors are collected in the same MDP in which the learning policy is to be learned,
and 2) the cross-domain IL studies how to imitate expert behaviors when discrepancies exist between
the expert and the learning MDPs (e.g., differing in their transition dynamics or morphologies)."
HINDSIGHT INFORMATION MATCHING,0.05272108843537415,"3.2
Hindsight Information Matching"
HINDSIGHT INFORMATION MATCHING,0.05442176870748299,"In typical goal-conditioned RL problems, hindsight experience replay (HER) [3] proposes to leverage
the rich repository of the failed experiences by replacing the desired (true) goals of training trajectories
with the achieved goals of the failed experiences:"
HINDSIGHT INFORMATION MATCHING,0.05612244897959184,"Alg(πθ; g, τg) →Alg(πθ; fHER(τg), τg),
where the learner Alg(πθ; ·, ·) could be any RL methods, τg ∼πθ(τg|g) denotes the trajectory
generated by a goal-conditioned policy πθ(at|st, g), and fHER denotes a pre-defined (hindsight
information extraction) function, e.g., returning the last state in trajectory τg."
HINDSIGHT INFORMATION MATCHING,0.05782312925170068,"HER can also be applied to the (single-goal) reward-driven online/offline RL tasks, setting the return
(sum of the discounted rewards) of a trajectory as an implicit goal for the corresponding trajectory.
Thus, we can reformulate the (single-goal) reward-driven RL task, learning policy πθ(at|st) that
maximize the return, as a multi-goal RL task, learning a return-conditioned policy πθ(at|st, ·) that
maximize the following log-likelihood:"
HINDSIGHT INFORMATION MATCHING,0.05952380952380952,"max
πθ
ED(τ) [log πθ(a|s, fR(τ))] ,
(2)"
HINDSIGHT INFORMATION MATCHING,0.061224489795918366,"where fR(τ) denotes the return of trajectory τ. At test, we can then condition the contextual policy
πθ(a|s, ·) on a desired target return. In offline RL, the empirical distribution D(τ) in Equation 2 can
be naturally set as the offline data distribution; in online RL, D(τ) can be set as the replay/experience
buffer, and will be updated and biased towards trajectories that have high expected returns."
HINDSIGHT INFORMATION MATCHING,0.06292517006802721,"Intuitively, biasing the sampling distribution (D(τ) towards higher returns) leads to an implicit policy
improvement operation. However, such an operator is non-trivial to obtain in the IL problem, where
we do not have access to a pre-defined function fR(τ) to bias the learning policy towards recovering
the given expert data {πE(τ)} (demonstrations or observations)."
METHOD,0.06462585034013606,"4
Method"
METHOD,0.0663265306122449,"In this section, we will formulate IL as a bi-level optimization problem, which will allow us to
derive our method, contextual imitation learning (CEIL). Instead of attempting to train the learning
policy πθ(a|s) with the plain expert matching objective (Equation 1), our approach introduces an
additional contextual variable z for a contextual IL policy πθ(a|s, ·). The main idea of CEIL is to
learn a contextual policy πθ(a|s, z) and an optimal contextual variable z∗such that the given expert
data (demonstrations in LfD or observations in LfO) can be recovered by the learned z∗-conditioned
policy πθ(a|s, z∗). We begin by describing the overall framework of CEIL in Section 4.1, and make
a connection between CEIL and the plain expert matching objective in Section 4.2, which leads to a
practical implementation under various IL settings in Section 4.3."
METHOD,0.06802721088435375,"4.1
Contextual Imitation Learning (CEIL)"
METHOD,0.06972789115646258,"Motivated by the hindsight information matching in online/offline RL (Section 3.2), we propose to
learn a general hindsight embedding function fϕ, which encodes trajectory τ (with window size T)
into a latent variable z ∈Z, |Z| ≪T ∗|S|. Formally, we learn the embedding function fϕ and a
corresponding contextual policy πθ(a|s, z) by minimizing the trajectory self-consistency loss:"
METHOD,0.07142857142857142,"πθ, fϕ = min
πθ,fϕ −ED(τ) [log πθ(τ|fϕ(τ))] = min
πθ,fϕ −Eτ∼D(τ)E(s,a)∼τ [log πθ(a|s, fϕ(τ))] , (3)"
METHOD,0.07312925170068027,"where in the online setting, we sample trajectory τ from buffer D(τ), known as the experience replay
buffer in online RL; in the offline setting, we sample trajectory τ directly from the given offline data."
METHOD,0.07482993197278912,"If we can ensure that the learned contextual policy πθ and the embedding function fϕ are accurate on
the empirical data D(τ), then we can convert the IL policy optimization objective (in Equation 1)
into a bi-level expert matching objective:"
METHOD,0.07653061224489796,"min
z∗
D(πθ(τ|z∗), πE(τ)),
(4)"
METHOD,0.0782312925170068,"s.t. πθ, fϕ = min
πθ,fϕ −ED(τ) [log πθ(τ|fϕ(τ))] −R(fϕ), and z∗∈fϕ ◦supp(D),
(5)"
METHOD,0.07993197278911565,"where R(fϕ) is an added regularization over the embedding function (we will elaborate on it later),
and supp(D) denotes the support of the trajectory distribution {τ|D(τ) > 0}. Here fϕ is employed
to map the trajectory space to the latent variable space (Z). Intuitively, by optimizing Equation 4,
we expect the induced trajectory distribution of the learned πθ(a|s, z∗) will match that of the expert.
However, in the offline IL setting, the contextual policy can not interact with the environment. If we
directly optimize the expert matching objective (Equation 4), such an objective can easily exploit
generalization errors in the contextual policy model to infer a mistakenly overestimated z∗that
achieves low expert-matching loss but does not preserve the trajectory self-consistency (Equation 3).
Therefore, we formalize CEIL into a bi-level optimization problem, where, in Equation 5, we explicitly
constrain the inferred z∗lies in the (fϕ-mapped) support of the training trajectory distribution."
METHOD,0.08163265306122448,"At a high level, CEIL decouples the learning policy into two parts: an expressive contextual policy
πθ(a|s, ·) and an optimal contextual variable z∗. By comparing CEIL with the plain expert match-
ing objective, minπθ D(πθ(τ), πE(τ)), in Equation 1, we highlight two merits: 1) CEIL’s expert
matching loss (Equation 4) does not account for updating πθ and is only incentivized to update the
low-dimensional latent variable z∗, which enjoys efficient parameter learning similar to the prompt
tuning in large language models [74], and 2) we learn πθ by simply performing supervised regression
(Equation 5), which is more stable compared to vanilla inverse-RL/adversarial-IL methods."
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.08333333333333333,"4.2
Connection to the Plain Expert Matching Objective"
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.08503401360544217,"To gain more insight into Equation 4 that captures the quality of IL (the degree of similarity to
the expert data), we define D(·, ·) as the sum of reverse KL and forward KL divergence3, i.e.,
D(q, p) = DKL(q∥p) + DKL(p∥q), and derive an alternative form for Equation 4:"
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.08673469387755102,"arg min
z∗
D(πθ(τ|z∗), πE(τ)) = arg max
z∗
I(z∗; τE) −I(z∗; τθ)
|
{z
}
JMI"
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.08843537414965986,"−D(πθ(τ), πE(τ))
|
{z
}
JD ,
(6)"
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.09013605442176871,"where I(x; y) denotes the mutual information (MI) between x and y, which measures the predictive
power of y on x (or vice-versa), the latent variables are defined as τE := τ ∼πE(τ), τθ := τ ∼
p(z∗)πθ(τ|z∗), and πθ(τ) = Ez∗[πθ(τ|z∗)]."
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.09183673469387756,"Intuitively, the second term JD on RHS of Equation 6 is similar to the plain expert matching objective
in Equation 1, except that here we optimize a latent variable z∗over this objective. Regarding the MI
terms JMI, we can interpret the maximization over JMI as an implicit policy improvement, which
incentivizes the optimal latent variable z∗for having high predictive power of the expert data τE and
having low predictive power of the non-expert data τθ."
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.0935374149659864,"Further, we can rewrite the MI term (JMI in Equation 6) in terms of the learned embedding function
fϕ, yielding an approximate embedding inference objective JMI(fϕ):"
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.09523809523809523,"JMI = EπE(z∗,τE) log p(z∗|τE) −Eπθ(z∗,τθ) log p(z∗|τθ)"
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.09693877551020408,"≈Ep(z∗)πE(τE)πθ(τθ|z∗)

−∥z∗−fϕ(τE)∥2 + ∥z∗−fϕ(τθ)∥2
≜JMI(fϕ),"
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.09863945578231292,"where we approximate the logarithmic predictive power of z∗on τ with −∥z∗−fϕ(τ)∥2, by taking
advantage of the learned embedding function fϕ in Equation 5."
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.10034013605442177,"3DKL(p∥q) := Ep(x)
h
log p(x)"
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.10204081632653061,"q(x)
i
denotes the (forward) KL divergences. It is well known that reverse KL
ensures that the learned distribution is mode-seeking and forward KL exhibits a mode-covering behavior [37].
For analysis purposes, here we define D(·, ·) as the sum of reverse KL and forward KL, and set the weights of
both reverse KL and forward KL to 1."
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.10374149659863946,"Algorithm 1 Training CEIL: Online or Offline IL Setting
Require: Expert demonstrations {πE(τ)}, empty buffer D for online IL or reward-free offline data
D for offline IL, training iteration K, and batch size N."
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.1054421768707483,"1: Initialize contextual policy πθ(a|s, ·), embedding function fϕ(z|τ), and latent variable z∗.
2: for k = 1, · · · , K do
3:
(Online only) Run policy πθ(a|s, z∗) in environment and store experience into buffer D.
4:
Sample a batch of data {τ}n
1 from D for online IL or D for offline IL.
5:
Learn πθ and fϕ over sampled {τ}n
1 using the trajectory self-consistency loss.
6:
Update z∗and fϕ over sampled {τ}n
1 by maximizing JMI(fϕ) −αJD.
7:
(Offline only) Update z∗by minimizing R(z∗).
# eliminating the offline OOD issues.
8: end for
Return: the learned contextual policy πθ(a|s, ·) and the optimal latent variable z∗."
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.10714285714285714,"By maximizing JMI(fϕ), the learned optimal z∗will be induced to converge towards the embeddings of
expert data and avoid trivial solutions (as shown in Figure 1). Intuitively, JMI(fϕ) can also be thought
of as an instantiation of contrastive loss, which manifests two facets we consider significant in IL:"
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.10884353741496598,"0
100
200
Rollout steps (k) 10
1 100"
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.11054421768707483,L2 distance
CONNECTION TO THE PLAIN EXPERT MATCHING OBJECTIVE,0.11224489795918367,Hopper-v2 0
K,0.11394557823129252,1k
K,0.11564625850340136,2k
K,0.11734693877551021,3k
K,0.11904761904761904,Return
K,0.12074829931972789,"0
100
200
Rollout steps (k) 10
1 100"
K,0.12244897959183673,L2 distance
K,0.12414965986394558,Ant-v2 0
K,0.12585034013605442,2k
K,0.12755102040816327,4k
K,0.1292517006802721,6k
K,0.13095238095238096,Return
K,0.1326530612244898,"Return
||z *
f (
E)||2
||z *
f (
)||2"
K,0.13435374149659865,"Figure 1: During learning, the distance between z∗
and fϕ(τE) decreases rapidly (green lines). Mean-
while, as policy πθ(·|·, z∗) gets better (blue lines),
fϕ(τθ) gradually approaches z∗(red lines)."
K,0.1360544217687075,"1) the ""anchor"" variable4 z∗is unknown and
must be estimated, and 2) it is necessary to
ensure that the estimated z∗lies in the support
set of training distribution, as specified by the
support constraints in Equation 5."
K,0.1377551020408163,"In summary, by comparing JMI(fϕ) and JD,
we can observe that JMI(fϕ) actually encour-
ages expert matching in the embedding space,
while JD encourages expert matching in the
original trajectory space. In the next section,
we will see that such an embedding-level ex-
pert matching objective naturally lends itself
to cross-domain IL settings."
PRACTICAL IMPLEMENTATION,0.13945578231292516,"4.3
Practical Implementation"
PRACTICAL IMPLEMENTATION,0.141156462585034,"In this section, we describe how we can convert the bi-level IL problem above (Equations 4 and 5)
into a feasible online/offline IL objective and discuss some practical implementation details in LfO,
offline IL, cross-domain IL, and one-shot IL settings (see more details5 in Appendix 9.3)."
PRACTICAL IMPLEMENTATION,0.14285714285714285,"As shown in Algorithm 1 (best viewed in colors), CEIL alternates between solving the bi-level
problem with respect to the support constraint (Line 3 for online IL or Line 7 for offline IL), the
trajectory self-consistency loss (Line 5), and the optimal embedding inference (Line 6)."
PRACTICAL IMPLEMENTATION,0.1445578231292517,"To satisfy the support constraint in Equation 5, for online IL (Line 3), we directly roll out the
z∗-conditioned policy πθ(a|s, z∗) in the environment; for offline IL (Line 7), we minimize a simple
regularization6 over z∗, bearing a close resemblance to the one used in TD3+BC [23]:"
PRACTICAL IMPLEMENTATION,0.14625850340136054,"R(z∗) = min
 
∥z∗−f ¯ϕ(τE)∥2, ∥z∗−f ¯ϕ(τD)∥2
, τE := τ ∼πE(τ), τD := τ ∼D(τ), (7)
where we apply a stop-gradient operation to f ¯ϕ. To ensure the optimal embedding inference
(maxz∗JMI(fϕ) −JD) retaining the flexibility of seeking z∗across different instances of fϕ, we
jointly update the optimal embedding z∗and the embedding function fϕ with"
PRACTICAL IMPLEMENTATION,0.14795918367346939,"max
z∗,fϕ JMI(fϕ) −αJD,
(8)"
PRACTICAL IMPLEMENTATION,0.14965986394557823,where we use α to control the weight on JD.
PRACTICAL IMPLEMENTATION,0.15136054421768708,"LfO. In the LfO setting, as expert actions are missing, we apply our expert matching objective only
over the observations. Note that even though expert data contains no actions in LfO, we can still"
THE TRIPLET CONTRASTIVE LOSS ENFORCES THE DISTANCE BETWEEN THE ANCHOR AND THE POSITIVE TO BE SMALLER THAN THAT,0.15306122448979592,"4The triplet contrastive loss enforces the distance between the anchor and the positive to be smaller than that
between the anchor and the negative. Thus, we can view z∗in JMI(fϕ) as an instance of the anchor.
5Our code will be released at https://github.com/wechto/GeneralizedCEIL.
6In other words, the offline support constraint in Equation 5 is achieved through minimizing R(z∗)."
THE TRIPLET CONTRASTIVE LOSS ENFORCES THE DISTANCE BETWEEN THE ANCHOR AND THE POSITIVE TO BE SMALLER THAN THAT,0.15476190476190477,"leverage a large number of suboptimal actions presented in online/offline D(τ). Thus, we can learn
the contextual policy πθ(a|s, z) using the buffer data in online IL or the offline data in offline IL,
much owing to the fact that we do not directly use the plain expert matching objective to update πθ."
THE TRIPLET CONTRASTIVE LOSS ENFORCES THE DISTANCE BETWEEN THE ANCHOR AND THE POSITIVE TO BE SMALLER THAN THAT,0.1564625850340136,"Cross-domain IL. Cross-domain IL considers the case in which the expert’s and learning agent’s
MDPs are different. Due to the domain shift, the plain idea of min JD may not be a sufficient proxy
for the expert matching objective, as there may never exist a trajectory (in the learning MDP) that
matches the given expert data. Thus, we can set (the weight of JD) α to 0."
THE TRIPLET CONTRASTIVE LOSS ENFORCES THE DISTANCE BETWEEN THE ANCHOR AND THE POSITIVE TO BE SMALLER THAN THAT,0.15816326530612246,"Further, to make embedding function fϕ useful for guiding the expert matching in latent space (i.e.,
max JMI(fϕ)), we encourage fϕ to capture the task-relevant embeddings and ignore the domain-
specific factors. To do so, we generate a set of pseudo-random transitions {τE′} by independently
sampling trajectories from expert data {πE(τE)} and adding random noise over these sampled
trajectories, i.e., τE′ = τE + noise. Then, we couple each trajectory τ in {τE} ∪{τE′} with a
label n ∈{0, 1}, indicating whether it is noised, and then generate a new set of {(τ, n)}, where
τ ∈{τE} ∪{τE′} and n ∈{0, 1}. Thus, we can set the regularization R(fϕ) in Equation 5 to be:"
THE TRIPLET CONTRASTIVE LOSS ENFORCES THE DISTANCE BETWEEN THE ANCHOR AND THE POSITIVE TO BE SMALLER THAN THAT,0.1598639455782313,"R(fϕ) = I(fϕ(τ); n).
(9)"
THE TRIPLET CONTRASTIVE LOSS ENFORCES THE DISTANCE BETWEEN THE ANCHOR AND THE POSITIVE TO BE SMALLER THAN THAT,0.16156462585034015,"Intuitively, maximizing R(fϕ) encourages embeddings to be domain-agnostic and task-relevant:
fϕ(τE) has high predictive power over expert data (n = 0) and low that over noised data (n = 1)."
THE TRIPLET CONTRASTIVE LOSS ENFORCES THE DISTANCE BETWEEN THE ANCHOR AND THE POSITIVE TO BE SMALLER THAN THAT,0.16326530612244897,"One-shot IL. Benefiting from the separate design of the contextual policy learning and the optimal
embedding inference, CEIL also enjoys another advantage — one-shot generalization to new IL
tasks. For new IL tasks, given the corresponding expert data τnew, we can use the learned embedding
function fϕ to generate a corresponding latent embedding znew. When conditioning on such an
embedding, we can directly roll out πθ(a|s, znew) to recover the one-shot expert behavior."
EXPERIMENTS,0.1649659863945578,"5
Experiments"
EXPERIMENTS,0.16666666666666666,"In this section, we conduct experiments across a variety of IL problem domains: single/cross-domain
IL, online/offline IL, and LfD/LfO IL settings. By arranging and combining these IL domains, we
obtain 8 IL tasks in all: S-on-LfD, S-on-LfO, S-off-LfD, S-off-LfO, C-on-LfD, C-on-LfO, C-off-LfD,
and C-off-LfO, where S/C denotes single/cross-domain IL, on/off denotes online/offline IL, and
LfD/LfO denote learning from demonstrations/observations respectively. Moreover, we also verify
the scalability of CEIL on the challenging one-shot IL setting."
EXPERIMENTS,0.1683673469387755,"Our experiments are conducted in four popular MuJoCo environments:
Hopper-v2 (Hop.),
HalfCheetah-v2 (Hal.), Walker2d-v2 (Wal.), and Ant-v2 (Ant.). In the single-domain IL setting, we
train a SAC policy in each environment and use the learned expert policy to collect expert trajecto-
ries (demonstrations/observations). To investigate the cross-domain IL setting, we assume the two
domains (learning MDP and the expert-data collecting MDP) have the same state space and action
space, while they have different transition dynamics. To achieve this, we modify the torso length of
the MuJoCo agents (see details in Appendix 9.2). Then, for each modified agent, we train a separate
expert policy and collect expert trajectories. For the offline IL setting, we directly take the reward-free
D4RL [22] as the offline dataset, replacing the online rollout experience in the online IL setting."
EVALUATION RESULTS,0.17006802721088435,"5.1
Evaluation Results"
EVALUATION RESULTS,0.1717687074829932,"To demonstrate the versatility of the CEIL idea, we collect 20 expert trajectories (demonstrations
in LfD or state-only observations in LfO) for each environment and compare CEIL to GAIL [30],
AIRL [21], SQIL [61], IQ-Learn [28], ValueDICE [41], GAIfO [66], ORIL [78], DemoDICE [39],
and SMODICE [56] (see their implementation details in Appendix 9.4). Note that these baseline
methods cannot be applied to all the IL task settings (S/C-on/off-LfD/LfO), thus we only provide
experimental comparisons with compatible baselines in each IL setting."
EVALUATION RESULTS,0.17346938775510204,"Online IL. In Figure 2, we provide the return (cumulative rewards) curves of our method and baselines
on 4 online IL settings: S-on-LfD (top-left), S-on-LfO (top-right), C-on-LfD (bottom-left), and C-
on-LfO (bottom-right) settings. As can be seen, CEIL quickly achieves expert-level performance
in S-on-LfD. When extended to S-on-LfO, CEIL also yields better sample efficiency compared to
baselines. Further, considering the complex cross-domain setting, we can see those baselines SQIL"
EVALUATION RESULTS,0.17517006802721088,"0
100 200 300 400 500"
EVALUATION RESULTS,0.17687074829931973,Rollout steps (k) 0
K,0.17857142857142858,1.5k
K,0.18027210884353742,3k
K,0.18197278911564627,Return (a)
K,0.1836734693877551,Hopper-v2
K,0.18537414965986396,"0
100 200 300 400 500"
K,0.1870748299319728,Rollout steps (k) 0
K,0.18877551020408162,2k
K,0.19047619047619047,4k 
K,0.1921768707482993,"Ant-v2
Single-domain online LfD (S-on-LfD)"
K,0.19387755102040816,"0
100 200 300 400 500"
K,0.195578231292517,Rollout steps (k) 0
K,0.19727891156462585,1.5k
K,0.1989795918367347,3k
K,0.20068027210884354,Return (b)
K,0.20238095238095238,Hopper-v2
K,0.20408163265306123,"0
100 200 300 400 500"
K,0.20578231292517007,Rollout steps (k) 0
K,0.20748299319727892,2k
K,0.20918367346938777,4k 
K,0.2108843537414966,"Ant-v2
Single-domain online LfO (S-on-LfO)"
K,0.21258503401360543,"0
100 200 300 400 500"
K,0.21428571428571427,Rollout steps (k) 0
K,0.21598639455782312,0.5k
K,0.21768707482993196,1k
K,0.2193877551020408,1.5k
K,0.22108843537414966,Return (c)
K,0.2227891156462585,Hopper-v2
K,0.22448979591836735,"0
100 200 300 400 500"
K,0.2261904761904762,Rollout steps (k) 0
K,0.22789115646258504,2k
K,0.22959183673469388,4k 
K,0.23129251700680273,"Ant-v2
Cross-domain online LfD (C-on-LfD)"
K,0.23299319727891157,"0
100 200 300 400 500"
K,0.23469387755102042,Rollout steps (k) 0
K,0.23639455782312926,0.4k
K,0.23809523809523808,0.8k
K,0.23979591836734693,1.2k
K,0.24149659863945577,Return (d)
K,0.24319727891156462,Hopper-v2
K,0.24489795918367346,"0
100 200 300 400 500"
K,0.2465986394557823,Rollout steps (k) 0
K,0.24829931972789115,2k 
K,0.25,"Ant-v2
Cross-domain online LfO (C-on-LfO)"
K,0.25170068027210885,"CEIL
AIRL"
K,0.2534013605442177,"GAIL
SQIL"
K,0.25510204081632654,"IQ-Learn
ValueDICE
(legend for S/C-on-LfD)"
K,0.2568027210884354,"CEIL
AIRL (state only)"
K,0.2585034013605442,"GAIfO
IQ-Learn
(legend for S/C-on-LfO)"
K,0.2602040816326531,"Figure 2: Return curves on 4 online IL settings: (a) S-on-LfD, (b) S-on-LfO, (c) C-on-LfD, and (d) C-on-LfO,
where the shaded area represents a 95% confidence interval over 30 trails. Note that baselines cannot be applied
to all the IL task settings, thus we only provide comparisons with compatible baselines (two separate legends)."
K,0.2619047619047619,"Table 2: Normalized scores (averaged over 30 trails for each task) on 4 offline IL settings: S-off-LfD, S-off-LfO,
C-off-LfD, and C-off-LfO. Scores within two points of the maximum score are highlighted"
K,0.26360544217687076,"Offline IL settings
Hopper-v2
Halfcheetah-v2
Walker2d-v2
Ant-v2
sum
m
mr
me
m
mr
me
m
mr
me
m
mr
me S-LfD"
K,0.2653061224489796,"ORIL (TD3+BC)
50.9
22.1
72.7
44.7
30.2
87.5
47.1
26.7
102.6
46.5
31.4
61.9
624.3
SQIL (TD3+BC)
32.6
60.6
25.5
13.2
25.3
14.4
25.6
15.6
8.0
63.6
58.4
44.3
387.1
IQ-Learn
21.3
19.9
24.9
5.0
7.5
7.5
22.3
19.6
18.5
38.4
24.3
55.3
264.5
ValueDICE
73.8
83.6
50.8
1.9
2.4
3.2
24.6
26.4
44.1
79.1
82.4
75.2
547.5
DemoDICE
54.8
32.7
65.4
42.8
37.0
55.6
68.1
39.7
95.0
85.6
69.0
108.8
754.6
SMODICE
56.1
28.7
68.0
42.7
37.7
66.9
66.2
40.7
58.2
87.4
69.9
113.4
735.9
CEIL (ours)
110.4
103.0
106.8
40.0
30.3
63.9
118.6
110.8
117.0
126.3
122.0
114.3
1163.5 S-LfO"
K,0.26700680272108845,"ORIL (TD3+BC)
43.4
25.7
73.0
44.9
2.4
81.8
58.9
16.8
78.2
33.7
29.6
67.1
555.4
SMODICE
54.5
26.4
73.7
42.7
37.9
66.2
60.6
38.5
70.9
85.7
68.3
116.3
741.7
CEIL (ours)
54.2
51.4
90.4
43.5
40.1
47.7
78.5
20.5
110.0
97.0
67.8
120.5
821.7 C-LfD"
K,0.2687074829931973,"ORIL (TD3+BC)
52.8
27.6
46.5
38.3
8.0
74.0
25.3
28.4
26.3
26.0
17.6
11.9
382.6
SQIL (TD3+BC)
34.4
19.1
11.4
19.2
25.1
19.9
15.8
16.5
8.8
21.8
23.2
21.2
236.2
IQ-Lean
37.3
35.4
25.9
27.4
27.1
31.2
27.7
22.2
31.7
63.7
63.3
55.8
448.8
ValueDICE
22.0
18.3
18.9
14.0
11.7
8.7
11.5
10.0
8.6
24.1
21.4
19.2
188.4
DemoDICE
52.9
15.2
77.2
42.8
38.9
53.8
58.4
26.4
77.8
87.8
69.3
114.9
715.6
SMODICE
55.4
21.4
71.2
42.7
38.0
64.6
68.4
34.2
80.4
87.4
70.4
115.7
749.7
CEIL (ours)
58.4
39.8
81.6
42.6
38.3
46.6
76.5
21.1
81.1
91.6
88.0
115.3
780.9 C-LfO"
K,0.27040816326530615,"ORIL (TD3+BC)
55.5
18.2
55.5
40.6
2.9
73.0
26.9
19.4
22.7
11.2
21.3
10.8
358.0
SMODICE
53.7
18.3
64.2
42.6
38.0
63.0
68.9
37.5
60.7
87.5
75.1
115.0
724.4
CEIL (ours)
44.7
44.2
48.2
42.4
36.5
46.9
76.2
31.7
77.0
95.9
71.0
112.7
727.3"
K,0.272108843537415,"and IQ-Learn (in C-on-LfD and C-on-LfO) suffer from the domain mismatch, leading to performance
degradation at late stages of training, while CEIL can still achieve robust performance."
K,0.27380952380952384,"Offline IL. Next, we evaluate CEIL on the other 4 offline IL settings: S-off-LfD, S-off-LfO, C-off-LfD,
and C-off-LfO. In Table 2, we provide the normalized return of our method and baseline methods on
reward-free D4RL [22] medium (m), medium-replay (mr), and medium-expert (me) datasets. We can 0 100"
K,0.2755102040816326,(online) (a)
K,0.27721088435374147,"1
2
3
4
5
8
10
15
20
0 100"
K,0.2789115646258503,(offline)
K,0.28061224489795916,Demonstration Num.
K,0.282312925170068,Normalized Return
K,0.28401360544217685,Hopper-v2 0 100
K,0.2857142857142857,(online) (b)
K,0.28741496598639454,"1
2
3
4
5
8
10
15
20
0 100"
K,0.2891156462585034,(offline)
K,0.29081632653061223,Demonstration Num.
K,0.2925170068027211,Normalized Return
K,0.2942176870748299,Ant-v2
K,0.29591836734693877,"2
4
8
16
32
Window Size 0 100"
K,0.2976190476190476,Normalized Return
K,0.29931972789115646,"(c)
Hopper-v2"
K,0.3010204081632653,"S-on-LfD (CEIL)
S-off-LfD (CEIL)
C-on-LfD (CEIL)
C-off-LfD (CEIL)"
K,0.30272108843537415,"2
4
8
16
32
Window Size 0 100"
K,0.304421768707483,Normalized Return
K,0.30612244897959184,"(d)
Ant-v2"
K,0.3078231292517007,"S-on-LfD (CEIL)
S-off-LfD (CEIL)
C-on-LfD (CEIL)
C-off-LfD (CEIL)"
K,0.30952380952380953,"CEIL
GAIL
IQ-Learn
SQIL
ORIL
ValueDICE
SMODICE   (legend for varying Demo Num.)"
K,0.3112244897959184,"Figure 3: Ablating (a, b) the number of expert demonstrations and (c, d) the trajectory window size."
K,0.3129251700680272,"observe that CEIL achieves a significant improvement over the baseline methods in both S-off-LfD and
S-off-LfO settings. Compared to the state-of-the-art offline baselines, CEIL also shows competitive
results on the challenging cross-domain offline IL settings (C-off-LfD and C-off-LfO)."
K,0.31462585034013607,"One-shot IL. Then, we explore CEIL on the one-shot IL tasks, where we expect CEIL can adapt its
behavior to new IL tasks given only one trajectory for each task (mismatched MDP, see Appendix 9.2)."
K,0.3163265306122449,"One-shot IL
Hop.
Hal.
Wal.
Ant."
K,0.31802721088435376,Online
K,0.3197278911564626,"SQIL
16.8
1.1
3.5
4.2
IQ-Learn
4.6
0.2
1.7
7.5
CEIL (LfD)
29.9
2.5
31.7
20.5
CEIL (LfO)
17.8
3.2
5.6
29.7"
K,0.32142857142857145,Offine
K,0.3231292517006803,"ORIL
14.7
0.2
6.9
17.4
SQIL
7.4
0.8
4.6
12.5
IQ-Learn
18.8
1.2
4.0
19.3
DemoDICE
76.5
-0.5
-0.1
19.5
SMODICE
78.0
1.1
8.1
24.6
CEIL (LfD)
85.6
5.6
67.1
24.3
CEIL (LfO)
72.2
5.1
70.0
19.4"
K,0.32482993197278914,"Table 3: Normalized results on one-shot IL,
where CEIL shows prominent transferability."
K,0.32653061224489793,"We first pre-train an embedding function and a contex-
tual policy in the training domain (online/offline IL),
then infer a new contextual variable and evaluate it on
the new task. To facilitate comparison to baselines, we
similarly pre-train a policy network (using baselines)
and run BC on top of the pre-trained policy by using
the provided demonstration. Consequently, such a base-
line+BC procedure cannot be applied to the (one-shot)
LfO tasks. The results in Table 3 show that baseline+BC
struggles to transfer their expertise to new tasks. Bene-
fiting from the hindsight framework, CEIL shows better
one-shot transfer learning performance on 7 out of 8
one-shot LfD tasks and retains higher scalability and
generality for both one-shot LfD and LfO IL tasks."
ANALYSIS OF CEIL,0.3282312925170068,"5.2
Analysis of CEIL"
ANALYSIS OF CEIL,0.3299319727891156,"Hybrid offline IL settings
Hop.
Hal.
Wal.
Ant."
ANALYSIS OF CEIL,0.33163265306122447,"S-LfD
29.4
69.9
42.8
84.9
S-LfD + S-LfO
30.4
68.6
42.3
91.6
S-LfD + S-LfO + C-LfD
30.7
71.7
42.9
89.2
S-LfD + S-LfO + C-LfD + C-LfO
58.6
79.6
43.7
98.0"
ANALYSIS OF CEIL,0.3333333333333333,"Table 4: The normalized results of CEIL, showing that CEIL can
consistently digest useful (task-relevant) information and boost
its performance, even under a hybrid of offline IL settings."
ANALYSIS OF CEIL,0.33503401360544216,"Hybrid IL settings.
In real-world,
many IL tasks do not correspond to one
specific IL setting, and instead consist of
a hybrid of several IL settings, each of
which passes a portion of task-relevant
information to the IL agent. For exam-
ple, we can provide the agent with both
demonstrations and state-only observa-
tions and, in some cases, cross-domain
demonstrations (S-LfD+S-LfO+C-LfD).
To examine the versatility of CEIL, we collect a separate expert trajectory for each of the four offline
IL settings, and study CEIL’s performance under hybrid IL settings. As shown in Table 4, we can see
that by adding new expert behaviors on top of LfD, even when carrying relatively less supervision
(e.g., actions are absent in LfO), CEIL can still improve the performance."
ANALYSIS OF CEIL,0.336734693877551,"Varying the number of demonstrations. In Figure 3 (a, b), we study the effect of the number
of expert demonstrations on CEIL’s performance. Empirically, we reduce the number of training
demonstrations from 20 to 1, and report the normalized returns at 1M training steps. We can
observe that across both online and offline (D4RL *-medium) IL settings, CEIL shows more robust
performance with respect to different numbers of demonstrations compared to baseline methods."
ANALYSIS OF CEIL,0.33843537414965985,"Varying the window size of trajectory. Next we assess the effect of the trajectory window size (i.e.,
the length of trajectory τ used for the embedding function fϕ in Equation 3). In Figure 3 (b, c), we
ablate the number of the window size in 4 LfD IL instantiations. We can see that across a range of
window sizes, CEIL remains stable and achieves expert-level performance."
ANALYSIS OF CEIL,0.3401360544217687,"0
100 200 300 400 500"
ANALYSIS OF CEIL,0.34183673469387754,Rollout steps (k) 0
K,0.3435374149659864,1.5k
K,0.34523809523809523,3k
K,0.3469387755102041,Return (a)
K,0.3486394557823129,Hopper-v2
K,0.35034013605442177,"CEIL
CEIL(ablating f )"
K,0.3520408163265306,CEIL(ablating JMI)
K,0.35374149659863946,"0
100 200 300 400 500"
K,0.3554421768707483,Rollout steps (k) 0
K,0.35714285714285715,2k
K,0.358843537414966,4k 
K,0.36054421768707484,Ant-v2
K,0.3622448979591837,"CEIL
CEIL(ablating f )"
K,0.36394557823129253,CEIL(ablating JMI)
K,0.3656462585034014,Single-domain online LfD (S-on-LfD)
K,0.3673469387755102,"0
100 200 300 400 500"
K,0.36904761904761907,Rollout steps (k) 0
K,0.3707482993197279,0.5k
K,0.37244897959183676,1k
K,0.3741496598639456,1.5k
K,0.3758503401360544,Return (b)
K,0.37755102040816324,Hopper-v2
K,0.3792517006802721,"CEIL
CEIL(ablating f )"
K,0.38095238095238093,CEIL(ablating JMI)
K,0.3826530612244898,"0
100 200 300 400 500"
K,0.3843537414965986,Rollout steps (k) 0
K,0.38605442176870747,2k
K,0.3877551020408163,4k 
K,0.38945578231292516,Ant-v2
K,0.391156462585034,"CEIL
CEIL(ablating f )"
K,0.39285714285714285,CEIL(ablating JMI)
K,0.3945578231292517,Cross-domain online LfD (C-on-LfD)
K,0.39625850340136054,"Figure 4: Ablation studies on the optimization of fϕ (ablating fϕ) and the objective of JMI (ablating JMI),
where the shaded area represents 95% CIs over 5 trails. See ablation results for offline IL tasks in Table 5."
K,0.3979591836734694,"Table 5: Ablation studies on the optimization of fϕ (ablating fϕ) and the objective of JMI (ablating JMI),
where scores (averaged over 5 trails for each task) within two points of the maximum score are highlighted."
K,0.39965986394557823,"Offline IL settings
Hopper-v2
HalfCheetah-v2
Walker2d-v2
Ant-v2
sum
m
mr
me
m
mr
me
m
mr
me
m
mr
me S-LfD"
K,0.4013605442176871,"CEIL (ablating fϕ)
97.9
92.5
99.3
41.3
30.3
66.7
103.6
88.1
114.4
97.6
98.4
100.7
1030.8
CEIL (ablating JMI)
83.2
89.0
98.7
27.1
28.3
53.5
107.4
68.0
75.6
116.9
97.8
105.9
951.4
CEIL
110.4
103.0
106.8
40.0
30.3
63.9
118.6
110.8
117.0
126.3
122.0
114.3
1163.5 S-LfO"
K,0.4030612244897959,"CEIL (ablating fϕ)
51.5
41.1
83.3
43.8
40.1
63.7
76.3
20.3
103.0
78.0
52.5
105.5
759.2
CEIL (ablating JMI)
54.3
44.9
84.7
42.2
39.9
51.6
77.4
22.7
94.0
92.1
67.9
118.4
792.0
CEIL
54.2
51.4
90.4
43.5
40.1
47.7
78.5
20.5
110.0
97.0
67.8
120.5
821.7"
K,0.40476190476190477,"Ablation studies on the optimization of fϕ and the objective of JMI. In Figure 4 and Table 5, we
carried out ablation experiments on the loss of fϕ and JMI in both online IL and offline IL settings.
We can see that ablating the fϕ loss (optimizing with Equation 5) does degrade the performance
in both online and offline IL tasks, demonstrating the effectiveness of optimizing with Equation 8.
Intuitively, Equation 8 encourages the embedding function to be task-relevant, and thus we use
the expert matching loss to update fϕ. We can also see that ablating JMI does lead to degraded
performance, further verifying the effectiveness of our expert matching objective in the latent space."
CONCLUSION,0.4064625850340136,"6
Conclusion"
CONCLUSION,0.40816326530612246,"In this paper, we present CEIL, a novel and general Imitation Learning framework applicable to a
wide range of IL settings, including C/S-on/off-LfD/LfO and few-shot IL settings. This is achieved by
explicitly decoupling the imitation policy into 1) a contextual policy, learned with the self-supervised
hindsight information matching objective, and 2) a latent variable, inferred by performing the IL expert
matching objective. Compared to prior baselines, our results show that CEIL is more sample-efficient
in most of the online IL tasks and achieves better or competitive performances in offline tasks."
CONCLUSION,0.4098639455782313,"Limitations and future work. Our primary aim behind this work is to develop a simple and scalable
IL method. We believe that CEIL makes an important step in that direction. Admittedly, we also find
some limitations of CEIL: 1) Offline results generally outperform online results, especially in the
LfO setting. The main reason is that CEIL lacks explicit exploration bounds, thus future work could
explore the exploration ability of online CEIL. 2) The trajectory self-consistency cannot be applied to
cross-embodiment agents once the two embodiments/domains have different state spaces or action
spaces. Considering such a cross-embodiment setting, a typical approach is to serialize state/action
from different modalities into a flat sequence of tokens. We also remark that CEIL is compatible with
such a tokenization approach, and thus suitable for IL tasks with different action/state spaces. Thus,
we encourage the future exploration of generalized IL methods across different embodiments."
CONCLUSION,0.41156462585034015,Acknowledgments and Disclosure of Funding
CONCLUSION,0.413265306122449,"We sincerely thank the anonymous reviewers for their insightful suggestions. This work was
supported by the National Science and Technology Innovation 2030 - Major Project (Grant No.
2022ZD0208800), and NSFC General Program (Grant No. 62176215)."
REFERENCES,0.41496598639455784,References
REFERENCES,0.4166666666666667,"[1] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Belle-
mare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural
information processing systems, 34:29304–29320, 2021."
REFERENCES,0.41836734693877553,"[2] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit
Agrawal. Is conditional generative modeling all you need for decision-making? arXiv preprint
arXiv:2211.15657, 2022."
REFERENCES,0.4200680272108844,"[3] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,
Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience
replay. Advances in neural information processing systems, 30, 2017."
REFERENCES,0.4217687074829932,"[4] Kai Arulkumaran, Dylan R Ashley, Jürgen Schmidhuber, and Rupesh K Srivastava. All you
need is supervised learning: From imitation learning to meta-rl with upside down rl. arXiv
preprint arXiv:2202.11960, 2022."
REFERENCES,0.42346938775510207,"[5] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon
Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching
unlabeled online videos. Advances in Neural Information Processing Systems, 35:24639–24654,
2022."
REFERENCES,0.42517006802721086,"[6] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio,
Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv
preprint arXiv:1801.04062, 2018."
REFERENCES,0.4268707482993197,"[7] Damian Boborzi, Christoph-Nikolas Straehle, Jens S Buchner, and Lars Mikelsons. Imitation
learning by state-only distribution matching. arXiv preprint arXiv:2202.04332, 2022."
REFERENCES,0.42857142857142855,"[8] David Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna.
When does return-conditioned supervised learning work for offline reinforcement learning?
arXiv preprint arXiv:2206.01079, 2022."
REFERENCES,0.4302721088435374,"[9] Daniel S Brown, Wonjoon Goo, and Scott Niekum. Better-than-demonstrator imitation learning
via automatically-ranked demonstrations. In Conference on robot learning, pages 330–359.
PMLR, 2020."
REFERENCES,0.43197278911564624,"[10] Micah Carroll, Orr Paradise, Jessy Lin, Raluca Georgescu, Mingfei Sun, David Bignell,
Stephanie Milani, Katja Hofmann, Matthew Hausknecht, Anca Dragan, et al. Unimask: Unified
inference in sequential decision problems. arXiv preprint arXiv:2211.10869, 2022."
REFERENCES,0.4336734693877551,"[11] Jonathan Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating
covariate shift in imitation learning via offline data with partial coverage. Advances in Neural
Information Processing Systems, 34:965–979, 2021."
REFERENCES,0.43537414965986393,"[12] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
via sequence modeling. Advances in neural information processing systems, 34:15084–15097,
2021."
REFERENCES,0.4370748299319728,"[13] Robert Dadashi, Léonard Hussenot, Matthieu Geist, and Olivier Pietquin. Primal wasserstein
imitation learning. arXiv preprint arXiv:2006.04678, 2020."
REFERENCES,0.4387755102040816,"[14] Christopher R Dance, Julien Perez, and Théo Cachet. Conditioned reinforcement learning
for few-shot imitation. In International Conference on Machine Learning, pages 2376–2387.
PMLR, 2021."
REFERENCES,0.44047619047619047,"[15] Branton DeMoss, Paul Duckworth, Nick Hawes, and Ingmar Posner. Ditto: Offline imitation
learning with world models. arXiv preprint arXiv:2302.03086, 2023."
REFERENCES,0.4421768707482993,"[16] Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya
Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. Advances in
neural information processing systems, 30, 2017."
REFERENCES,0.44387755102040816,"[17] Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential
for offline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021."
REFERENCES,0.445578231292517,"[18] Arnaud Fickinger, Samuel Cohen, Stuart Russell, and Brandon Amos. Cross-domain imitation
learning via optimal transport. arXiv preprint arXiv:2110.03684, 2021."
REFERENCES,0.44727891156462585,"[19] Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs,
Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning.
In Conference on Robot Learning, pages 158–168. PMLR, 2022."
REFERENCES,0.4489795918367347,"[20] Tim Franzmeyer, Philip HS Torr, and João F Henriques. Learn what matters: cross-domain
imitation learning with task-relevant embeddings. arXiv preprint arXiv:2209.12093, 2022."
REFERENCES,0.45068027210884354,"[21] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse
reinforcement learning. arXiv preprint arXiv:1710.11248, 2017."
REFERENCES,0.4523809523809524,"[22] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for
deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.45408163265306123,"[23] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.
Advances in neural information processing systems, 34:20132–20145, 2021."
REFERENCES,0.4557823129251701,"[24] Hiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for
offline hindsight information matching. arXiv preprint arXiv:2111.10364, 2021."
REFERENCES,0.4574829931972789,"[25] Tanmay Gangwani and Jian Peng. State-only imitation with transition dynamics mismatch.
arXiv preprint arXiv:2002.11879, 2020."
REFERENCES,0.45918367346938777,"[26] Tanmay Gangwani, Yuan Zhou, and Jian Peng. Imitation learning from observations under
transition model disparity. arXiv preprint arXiv:2204.11446, 2022."
REFERENCES,0.4608843537414966,"[27] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng
Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. arXiv
preprint arXiv:2110.04544, 2021."
REFERENCES,0.46258503401360546,"[28] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. Iq-learn:
Inverse soft-q learning for imitation. Advances in Neural Information Processing Systems, 34:
4028–4039, 2021."
REFERENCES,0.4642857142857143,"[29] Adam Gleave, Mohammad Taufeeque, Juan Rocamonde, Erik Jenner, Steven H. Wang, Sam
Toyer, Maximilian Ernestus, Nora Belrose, Scott Emmons, and Stuart Russell. imitation: Clean
imitation learning implementations, 2022."
REFERENCES,0.46598639455782315,"[30] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural
information processing systems, 29, 2016."
REFERENCES,0.467687074829932,"[31] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big
sequence modeling problem. Advances in neural information processing systems, 34:1273–
1286, 2021."
REFERENCES,0.46938775510204084,"[32] Firas Jarboui and Vianney Perchet. Offline inverse reinforcement learning. arXiv preprint
arXiv:2106.05068, 2021."
REFERENCES,0.4710884353741497,"[33] Daniel Jarrett, Ioana Bica, and Mihaela van der Schaar. Strictly batch imitation learning by
energy-based distribution matching. Advances in Neural Information Processing Systems, 33:
7354–7365, 2020."
REFERENCES,0.47278911564625853,"[34] Shengyi Jiang, Jingcheng Pang, and Yang Yu. Offline imitation learning with a misspecified
simulator. Advances in neural information processing systems, 33:8510–8520, 2020."
REFERENCES,0.4744897959183674,"[35] Kshitij Judah, Alan Fern, Prasad Tadepalli, and Robby Goetschalckx. Imitation learning with
demonstrations and shaping rewards. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 28, 2014."
REFERENCES,0.47619047619047616,"[36] Yachen Kang, Diyuan Shi, Jinxin Liu, Li He, and Donglin Wang. Beyond reward: Offline
preference-guided policy optimization. arXiv preprint arXiv:2305.16217, 2023."
REFERENCES,0.477891156462585,"[37] Liyiming Ke, Sanjiban Choudhury, Matt Barnes, Wen Sun, Gilwoo Lee, and Siddhartha Srini-
vasa. Imitation learning as f-divergence minimization. In International Workshop on the
Algorithmic Foundations of Robotics, pages 313–329. Springer, 2020."
REFERENCES,0.47959183673469385,"[38] Liyiming Ke, Sanjiban Choudhury, Matt Barnes, Wen Sun, Gilwoo Lee, and Siddhartha Srini-
vasa. Imitation learning as f-divergence minimization. In Algorithmic Foundations of Robotics
XIV: Proceedings of the Fourteenth Workshop on the Algorithmic Foundations of Robotics 14,
pages 313–329. Springer International Publishing, 2021."
REFERENCES,0.4812925170068027,"[39] Geon-Hyeong Kim, Seokin Seo, Jongmin Lee, Wonseok Jeon, HyeongJoo Hwang, Hongseok
Yang, and Kee-Eung Kim. Demodice: Offline imitation learning with supplementary imperfect
demonstrations. In International Conference on Learning Representations, 2022."
REFERENCES,0.48299319727891155,"[40] Kuno Kim, Yihong Gu, Jiaming Song, Shengjia Zhao, and Stefano Ermon. Domain adaptive
imitation learning. In International Conference on Machine Learning, pages 5286–5295. PMLR,
2020."
REFERENCES,0.4846938775510204,"[41] Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribu-
tion matching. arXiv preprint arXiv:1912.05032, 2019."
REFERENCES,0.48639455782312924,"[42] Aviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned policies. arXiv preprint
arXiv:1912.13465, 2019."
REFERENCES,0.4880952380952381,"[43] Yao Lai, Jinxin Liu, Zhentao Tang, Bin Wang, HAO Jianye, and Ping Luo. Chipformer:
Transferable chip placement via offline decision transformer. ICML, 2023. URL https:
//openreview.net/pdf?id=j0miEWtw87."
REFERENCES,0.4897959183673469,"[44] Youngwoon Lee, Andrew Szot, Shao-Hua Sun, and Joseph J Lim. Generalizable imitation learn-
ing from observation via inferring goal proximity. Advances in neural information processing
systems, 34:16118–16130, 2021."
REFERENCES,0.4914965986394558,"[45] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient
prompt tuning. arXiv preprint arXiv:2104.08691, 2021."
REFERENCES,0.4931972789115646,"[46] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning:
Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.49489795918367346,"[47] Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from
visual demonstrations. Advances in Neural Information Processing Systems, 30, 2017."
REFERENCES,0.4965986394557823,"[48] Fangchen Liu, Zhan Ling, Tongzhou Mu, and Hao Su. State alignment-based imitation learning.
arXiv preprint arXiv:1911.10947, 2019."
REFERENCES,0.49829931972789115,"[49] Jinxin Liu, Donglin Wang, Qiangxing Tian, and Zhengyu Chen. Learn goal-conditioned
policy with intrinsic motivation for deep reinforcement learning. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 36, pages 7558–7566, 2022."
REFERENCES,0.5,"[50] Jinxin Liu, Hongyin Zhang, and Donglin Wang. Dara: Dynamics-aware reward augmentation
in offline reinforcement learning. arXiv preprint arXiv:2203.06662, 2022."
REFERENCES,0.5017006802721088,"[51] Jinxin Liu, Ziqi Zhang, Zhenyu Wei, Zifeng Zhuang, Yachen Kang, Sibo Gai, and Donglin
Wang. Beyond ood state actions: Supported cross-domain offline reinforcement learning. arXiv
preprint arXiv:2306.12755, 2023."
REFERENCES,0.5034013605442177,"[52] Minghuan Liu, Tairan He, Minkai Xu, and Weinan Zhang. Energy-based imitation learning.
arXiv preprint arXiv:2004.09395, 2020."
REFERENCES,0.5051020408163265,"[53] Minghuan Liu, Hanye Zhao, Zhengyu Yang, Jian Shen, Weinan Zhang, Li Zhao, and Tie-Yan
Liu. Curriculum offline imitating learning. Advances in Neural Information Processing Systems,
34:6266–6277, 2021."
REFERENCES,0.5068027210884354,"[54] YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation:
Learning to imitate behaviors from raw video via context translation. In 2018 IEEE International
Conference on Robotics and Automation (ICRA), pages 1118–1125. IEEE, 2018."
REFERENCES,0.5085034013605442,"[55] Yicheng Luo, Zhengyao Jiang, Samuel Cohen, Edward Grefenstette, and Marc Peter Deisenroth.
Optimal transport for offline imitation learning. arXiv preprint arXiv:2303.13971, 2023."
REFERENCES,0.5102040816326531,"[56] Yecheng Jason Ma, Andrew Shen, Dinesh Jayaraman, and Osbert Bastani. Smodice: Versatile
offline imitation learning via state occupancy matching. arXiv e-prints, pages arXiv–2202,
2022."
REFERENCES,0.5119047619047619,"[57] Tianwei Ni, Harshit Sikchi, Yufei Wang, Tejus Gupta, Lisa Lee, and Ben Eysenbach. f-irl:
Inverse reinforcement learning via state marginal matching. In Conference on Robot Learning,
pages 529–551. PMLR, 2021."
REFERENCES,0.5136054421768708,"[58] Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation.
Neural computation, 3(1):88–97, 1991."
REFERENCES,0.5153061224489796,"[59] Yiwen Qiu, Jialong Wu, Zhangjie Cao, and Mingsheng Long. Out-of-dynamics imitation
learning from multimodal demonstrations. In Conference on Robot Learning, pages 1071–1080.
PMLR, 2023."
REFERENCES,0.5170068027210885,"[60] Dripta S Raychaudhuri, Sujoy Paul, Jeroen Vanbaar, and Amit K Roy-Chowdhury. Cross-
domain imitation from observations. In International Conference on Machine Learning, pages
8902–8912. PMLR, 2021."
REFERENCES,0.5187074829931972,"[61] Siddharth Reddy, Anca D Dragan, and Sergey Levine. Sqil: Imitation learning via reinforcement
learning with sparse rewards. arXiv preprint arXiv:1905.11108, 2019."
REFERENCES,0.5204081632653061,"[62] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and
structured prediction to no-regret online learning. In Proceedings of the fourteenth interna-
tional conference on artificial intelligence and statistics, pages 627–635. JMLR Workshop and
Conference Proceedings, 2011."
REFERENCES,0.5221088435374149,"[63] Stefan Schaal. Learning from demonstration. Advances in neural information processing
systems, 9, 1996."
REFERENCES,0.5238095238095238,"[64] Rupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja´skowski, and Jürgen
Schmidhuber. Training agents using upside-down reinforcement learning. arXiv preprint
arXiv:1912.02877, 2019."
REFERENCES,0.5255102040816326,"[65] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. arXiv
preprint arXiv:1805.01954, 2018."
REFERENCES,0.5272108843537415,"[66] Faraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observa-
tion. arXiv preprint arXiv:1807.06158, 2018."
REFERENCES,0.5289115646258503,"[67] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in
neural information processing systems, 30, 2017."
REFERENCES,0.5306122448979592,"[68] Luca Viano, Yu-Ting Huang, Parameswaran Kamalaruban, Craig Innes, Subramanian Ra-
mamoorthy, and Adrian Weller. Robust learning from observation with model misspecification.
arXiv preprint arXiv:2202.06003, 2022."
REFERENCES,0.532312925170068,"[69] Tianyu Wang, Nikhil Karnwal, and Nikolay Atanasov. Latent policies for adversarial imitation
learning. arXiv preprint arXiv:2206.11299, 2022."
REFERENCES,0.5340136054421769,"[70] Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin. Discriminator-weighted offline
imitation learning from suboptimal demonstrations. In International Conference on Machine
Learning, pages 24725–24742. PMLR, 2022."
REFERENCES,0.5357142857142857,"[71] Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang
Gan. Prompting decision transformer for few-shot policy generalization. In International
Conference on Machine Learning, pages 24631–24645. PMLR, 2022."
REFERENCES,0.5374149659863946,"[72] Sheng Yue, Guanbo Wang, Wei Shao, Zhaofeng Zhang, Sen Lin, Ju Ren, and Junshan Zhang.
Clare: Conservative model-based reward learning for offline inverse reinforcement learning.
arXiv preprint arXiv:2302.04782, 2023."
REFERENCES,0.5391156462585034,"[73] Wenjia Zhang, Haoran Xu, Haoyi Niu, Peng Cheng, Ming Li, Heming Zhang, Guyue Zhou, and
Xianyuan Zhan. Discriminator-guided model-based offline imitation learning. In Conference
on Robot Learning, pages 1266–1276. PMLR, 2023."
REFERENCES,0.5408163265306123,"[74] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for
vision-language models. International Journal of Computer Vision, 130(9):2337–2348, 2022."
REFERENCES,0.5425170068027211,"[75] Zhuangdi Zhu, Kaixiang Lin, Bo Dai, and Jiayu Zhou. Off-policy imitation learning from
observations. Advances in Neural Information Processing Systems, 33:12402–12413, 2020."
REFERENCES,0.54421768707483,"[76] Zifeng Zhuang, Kun Lei, Jinxin Liu, Donglin Wang, and Yilang Guo. Behavior proximal policy
optimization. arXiv preprint arXiv:2302.11312, 2023."
REFERENCES,0.5459183673469388,"[77] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy
inverse reinforcement learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008."
REFERENCES,0.5476190476190477,"[78] Konrad Zolna, Alexander Novikov, Ksenia Konyushkova, Caglar Gulcehre, Ziyu Wang, Yusuf
Aytar, Misha Denil, Nando de Freitas, and Scott Reed. Offline learning from demonstrations
and unlabeled experience. arXiv preprint arXiv:2011.13885, 2020."
REFERENCES,0.5493197278911565,"[79] Konrad Zolna, Scott Reed, Alexander Novikov, Sergio Gomez Colmenarejo, David Budden,
Serkan Cabi, Misha Denil, Nando de Freitas, and Ziyu Wang. Task-relevant adversarial imitation
learning. In Conference on Robot Learning, pages 247–263. PMLR, 2021."
REFERENCES,0.5510204081632653,Appendix
ADDITIONAL DERIVATION,0.5527210884353742,"7
Additional Derivation"
ADDITIONAL DERIVATION,0.5544217687074829,"(Repeat from the main paper.) To gain more insight into Equation 4 that captures the quality of IL (the
degree of similarity to the expert data), we define D(·, ·) as the sum of reverse KL and forward KL
divergence, i.e., D(q, p) = DKL(q∥p) + DKL(p∥q), and derive an alternative form for Equation 4:
arg min
z∗
D(πθ(τ|z∗), πE(τ)) = arg max
z∗
I(z∗; τE) −I(z∗; τθ)
|
{z
}
JMI"
ADDITIONAL DERIVATION,0.5561224489795918,"−DKL(πθ(τ), πE(τ))
|
{z
}
JD ,"
ADDITIONAL DERIVATION,0.5578231292517006,"where I(x; y) denotes the mutual information (MI) between x and y, which measures the predictive
power of y on x (or vice-versa), the latent variables are defined as τE := τ ∼πE(τ), τθ := τ ∼
p(z∗)πθ(τ|z∗), and πθ(τ) = Ez∗[πθ(τ|z∗)]."
ADDITIONAL DERIVATION,0.5595238095238095,"Below is our derivation:
min
z∗
D(πθ(τ|z∗), πE(τ))"
ADDITIONAL DERIVATION,0.5612244897959183,"= min
z∗
Ep(z∗) [DKL(πθ(τ|z∗)∥πE(τ)) + DKL(πE(τ)∥πθ(τ|z∗))]"
ADDITIONAL DERIVATION,0.5629251700680272,"= min
z∗
Ep(z∗)πθ(τ|z∗) [log πθ(τ|z∗) −log πE(τ)]"
ADDITIONAL DERIVATION,0.564625850340136,+ Ep(z∗)πE(τ) [log πE(τ) −log πθ(τ|z∗)]
ADDITIONAL DERIVATION,0.5663265306122449,"= min
z∗
Ep(z∗)πθ(τ|z∗)"
ADDITIONAL DERIVATION,0.5680272108843537,"
log p(z∗|τ)πθ(τ)"
ADDITIONAL DERIVATION,0.5697278911564626,"p(z∗)
−log πE(τ)
"
ADDITIONAL DERIVATION,0.5714285714285714,+ Ep(z∗)πE(τ)
ADDITIONAL DERIVATION,0.5731292517006803,"
log πE(τ) −log p(z∗|τ)πθ(τ) p(z∗) "
ADDITIONAL DERIVATION,0.5748299319727891,"= min
z∗
Ep(z∗)πθ(τ|z∗)"
ADDITIONAL DERIVATION,0.576530612244898,"
log p(z∗|τ)"
ADDITIONAL DERIVATION,0.5782312925170068,"p(z∗)
+ log πθ(τ) πE(τ)"
ADDITIONAL DERIVATION,0.5799319727891157,"
−Ep(z∗)πE(τ)"
ADDITIONAL DERIVATION,0.5816326530612245,"
log p(z∗|τ)"
ADDITIONAL DERIVATION,0.5833333333333334,"p(z∗)
+ log πθ(τ) πE(τ) "
ADDITIONAL DERIVATION,0.5850340136054422,"= max
z∗
I(z∗; τE) −I(z∗; τθ) −D(πθ(τ), πE(τ)),"
ADDITIONAL DERIVATION,0.5867346938775511,"where τE := τ ∼πE(τ), τθ := τ ∼p(z∗)πθ(τ|z∗)."
MORE COMPARISONS AND ABLATION STUDIES,0.5884353741496599,"8
More Comparisons and Ablation Studies"
MORE COMPARISONS AND ABLATION STUDIES,0.5901360544217688,"8.1
Offline Comparison on D4RL Expert Domain Dataset"
MORE COMPARISONS AND ABLATION STUDIES,0.5918367346938775,"In Table 6, we provide the normalized return of our method and baseline methods on the reward-free
D4RL [22] expert dataset. Consistently, we can observe that CEIL achieves a significant improvement
over the baseline methods in both S-off-LfD and S-off-LfO settings. Compared to the state-of-the-art
offline IL baselines, CEIL also shows competitive results on the challenging cross-domain offline IL
settings (C-off-LfD and C-off-LfO)."
GENERALIZABILITY ON CROSS-DOMAIN OFFLINE IL SETTINGS,0.5935374149659864,"8.2
Generalizability on Cross-domain Offline IL Settings"
GENERALIZABILITY ON CROSS-DOMAIN OFFLINE IL SETTINGS,0.5952380952380952,"In the standard cross-domain IL setting, the goal is to extract expert-relevant information from the
mismatched expert demonstrations/observations (expert domain) and to mimic such expert behaviors
in the training environment (training domain). Thus, we validate the performance of the learned
policy in the training environment (i.e., the environment where the offline data was collected). Here,
we also study the generalizability of the learned policy by evaluating the learned policy in the expert
environment (i.e., the environment where the mismatched expert data was collected). We provide
the normalized scores (evaluated in the expert domain) in Table 7. We can find that across a range
of cross-domain offline IL tasks, CEIL consistently demonstrates better (zero-shot) generalizability
compared to baselines."
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.5969387755102041,"8.3
Ablating the Cross-domain Regularization"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.5986394557823129,"We now conduct ablation studies to evaluate the importance of cross-domain regularization in
Equation 9 (in the main paper). In Figure 5, we provide the performance improvement when we"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6003401360544217,"Table 6: Normalized scores (averaged over 30 trails for each task) on D4RL expert dataset. Scores
within two points of the maximum score are highlighted. hop: Hopper-v2. hal: HalfCheetah-v2. wal:
Walker2d-v2. ant: Ant-v2."
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6020408163265306,"hop
hal
wal
ant
sum
expert
expert
expert
expert"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6037414965986394,S-off-LfD
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6054421768707483,"ORIL (TD3+BC)
97.5
91.8
14.5
76.8
280.6
SQIL (TD3+BC)
25.5
14.4
8.0
44.3
92.1
IQ-Learn
37.3
9.9
46.6
85.9
179.7
ValueDICE
65.6
2.9
28.2
90.5
187.1
DemoDICE
107.3
87.1
104.8
114.2
413.3
SMODICE
111.0
93.5
108.2
122.0
434.7
CEIL
106.0
96.0
115.6
117.8
435.4"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6071428571428571,"S-off-LfO
ORIL (TD3+BC)
64.2
92.1
12.2
44.3
212.8
SMODICE
111.3
93.7
108.0
122.0
435.0
CEIL
103.3
96.8
110.0
126.4
436.5"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.608843537414966,C-off-LfD
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6105442176870748,"ORIL (TD3+BC)
24.4
78.3
29.3
32.1
164.1
SQIL (TD3+BC)
12.2
19.9
8.8
21.2
62.0
IQ-Learn
25.9
31.2
31.7
55.8
144.6
ValueDICE
18.6
9.8
8.3
22.3
59.0
DemoDICE
111.5
88.7
107.9
122.5
430.6
SMODICE
111.1
93.8
108.2
120.9
434.0
CEIL
105.8
97.1
108.6
112.2
423.7"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6122448979591837,"C-off-LfO
ORIL (TD3+BC)
22.5
76.6
11.2
28.2
138.6
SMODICE
111.2
93.7
108.1
117.7
430.7
CEIL
113.0
90.1
108.7
125.2
437.0"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6139455782312925,"Table 7: Normalized scores (evaluated on the expert dataset over 30 trails for each task) on 2
cross-domain offline IL settings: C-off-LfD and C-off-LfO. Scores within two points of the maximum
score are highlighted. m: medium. mr: medium-replay. me: medium-expert. e: expert."
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6156462585034014,"Hopper-v2
HalfCheetah-v2
sum
m
mr
me
e
m
mr
me
e"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6173469387755102,C-off-LfD
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6190476190476191,"ORIL (TD3+BC)
74.7
16.7
45.0
21.4
2.2
0.8
-0.3
-2.2
158.3
SQIL (TD3+BC)
33.6
21.6
14.5
14.5
18.2
7.5
20.9
20.9
151.8
IQ-Learn
11.8
9.7
17.1
17.1
7.7
7.8
9.5
9.5
90.2
ValueDICE
49.5
24.2
55.7
49.3
32.2
32.9
38.7
28.7
311.2
DemoDICE
83.2
31.5
81.6
28.5
0.9
-1.1
-1.7
-2.4
220.6
SMODICE
80.1
26.1
78.0
54.3
2.8
-1.0
1.0
-2.3
239.1
CEIL
87.4
74.3
81.2
82.4
44.0
30.4
25.0
17.1
441.9"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6207482993197279,"C-off-LfO
ORIL (TD3+BC)
62.3
18.7
57.0
28.2
0.2
1.1
-0.3
-2.3
165.0
SMODICE
77.6
22.5
80.2
71.0
2.0
-0.9
0.8
-2.3
250.9
CEIL
56.4
58.6
56.7
65.2
5.5
36.5
5.0
5.0
288.7"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6224489795918368,"Walker2d-v2
Ant-v2
sum
m
mr
me
e
m
mr
me
e"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6241496598639455,C-off-LfD
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6258503401360545,"ORIL (TD3+BC)
22.0
24.5
23.9
33.1
16.0
18.6
2.5
0.4
141.0
SQIL (TD3+BC)
32.4
14.9
10.3
10.3
71.4
63.6
60.1
60.1
323.1
IQ-Learn
8.4
5.0
10.2
10.2
19.4
18.4
16.1
16.1
103.8
ValueDICE
31.7
21.9
22.9
27.7
70.5
68.5
69.3
68.5
380.9
DemoDICE
12.8
31.5
12.9
86.9
15.7
24.2
2.3
1.4
187.7
SMODICE
43.6
16.1
62.0
85.3
23.7
22.9
2.3
-5.9
249.9
CEIL
102.8
94.8
101.9
100.7
82.0
77.0
76.4
79.8
715.3"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6275510204081632,"C-off-LfO
ORIL (TD3+BC)
22.4
15.2
17.8
12.6
13.6
20.7
5.5
-6.2
101.6
SMODICE
42.4
17.0
55.5
88.7
15.7
22.6
2.5
-6.3
238.1
CEIL
67.9
12.0
68.4
50.8
31.7
57.0
18.0
-1.9
304.0 hop-m"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6292517006802721,hop-mr
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6309523809523809,hop-me hop-e hal-m
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6326530612244898,hal-mr
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6343537414965986,hal-me hal-e wal-m
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6360544217687075,wal-mr
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6377551020408163,wal-me wal-e ant-m
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6394557823129252,ant-mr
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.641156462585034,ant-me ant-e 6 4 2 0 2
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6428571428571429,Improvement
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6445578231292517,C-off-LfD hop-m
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6462585034013606,hop-mr
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6479591836734694,hop-me hop-e hal-m
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6496598639455783,hal-mr
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6513605442176871,hal-me hal-e wal-m
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6530612244897959,wal-mr
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6547619047619048,wal-me wal-e ant-m
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6564625850340136,ant-mr
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6581632653061225,ant-me ant-e 4 3 2 1 0 1
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6598639455782312,Improvement
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6615646258503401,C-off-LfO
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6632653061224489,"Figure 5: Normalized performance improvement (left: C-off-LfD, right: C-off-LfO) when we ablate
the cross-domain regularization (Equation 9 in the main paper) in cross-domain IL settings. We can
observe the general trend (in 26 out of 32 tasks) that ablating the cross-domain regularization causes
negative performance improvement. hop: Hopper-v2. hal: HalfCheetah-v2. wal: Walker2d-v2. ant:
Ant-v2. m: medium. me: medium-expert. mr: medium-replay. e: expert."
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6649659863945578,"30
60
90
normalized return"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6666666666666666,"CEIL
SMODICE
DemoDICE"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6683673469387755,"ValueDICE
ORIL(TD3+BC)
SQIL(TD3+BC)"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6700680272108843,IQ-Learn
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6717687074829932,Median
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.673469387755102,"25
50
75
100
normalized return IQM"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6751700680272109,"25
50
75
100
normalized return Mean"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6768707482993197,"20
40
60
80
normalized return"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6785714285714286,"(a) S-off-LfD
Optimality Gap"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6802721088435374,"45
60
75
normalized return"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6819727891156463,"CEIL
SMODICE
ORIL(TD3+BC)"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6836734693877551,Median
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.685374149659864,"45
60
75
normalized return IQM"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6870748299319728,"50
60
70
normalized return Mean"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6887755102040817,"30
40
50
60
normalized return"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6904761904761905,"(b) S-off-LfO
Optimality Gap"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6921768707482994,"25
50
75
normalized return"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6938775510204082,"CEIL
SMODICE
DemoDICE"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6955782312925171,"ValueDICE
ORIL(TD3+BC)
SQIL(TD3+BC)"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6972789115646258,IQ-Learn
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.6989795918367347,Median
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7006802721088435,"20
40
60
80
normalized return IQM"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7023809523809523,"20
40
60
80
normalized return Mean"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7040816326530612,"40
60
80
normalized return"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.70578231292517,"(c) C-off-LfD
Optimality Gap"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7074829931972789,"20
40
60
80
normalized return"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7091836734693877,"CEIL
SMODICE
ORIL(TD3+BC)"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7108843537414966,Median
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7125850340136054,"30
45
60
normalized return IQM"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7142857142857143,"30
45
60
normalized return Mean"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7159863945578231,"45
60
normalized return"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.717687074829932,"(d) C-off-LfO
Optimality Gap"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7193877551020408,"Figure 6: Aggregate median, IQM, mean, and optimality gap over 16 offline IL tasks. Higher
median, higher IQM, and higher mean and lower optimality gap are better. The shaded bar shows
95% stratified bootstrap confidence intervals. We can see that CEIL achieves consistently better
performance across a wide range of offline IL settings."
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7210884353741497,"ablate the cross-domain regularization in two cross-domain offline IL tasks (C-off-LfD and C-off-LfO).
We can find that in 26 out of 32 cross-domain tasks, ablating the regularization can cause performance
to decrease (negative performance improvement), thus verifying the benefits of encouraging task-
relevant embeddings."
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7227891156462585,"0
100 200 300 400 500"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7244897959183674,Rollout steps (k) 0 2000 4000
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7261904761904762,Return
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7278911564625851,S-on-LfD
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7295918367346939,"0
100 200 300 400 500"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7312925170068028,Rollout steps (k) 0 2000 4000
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7329931972789115,Return
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7346938775510204,C-on-LfD
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7363945578231292,"0
100 200 300 400 500"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7380952380952381,Rollout steps (k) 0 250 500 750 1000
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7397959183673469,Return
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7414965986394558,S-on-LfO
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7431972789115646,"0
100 200 300 400 500"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7448979591836735,Rollout steps (k) 0 250 500 750 1000
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7465986394557823,Return
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7482993197278912,C-on-LfO
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.75,"CEIL
AIRL"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7517006802721088,"GAIL
SQIL"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7534013605442177,"IQ-Learn
ValueDICE
(legend for S/C-on-LfD)"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7551020408163265,"CEIL
AIRL (state only)"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7568027210884354,"GAIfO
IQ-Learn
(legend for S/C-on-LfO)"
ABLATING THE CROSS-DOMAIN REGULARIZATION,0.7585034013605442,"Figure 7: Return curves in Walker2d-v2 (from left to right: S-on-LfD, C-on-LfD, S-on-LfO, and
C-on-LfO), where the shaded area represents a 95% confidence interval over 30 trails. We can see
that CEIL consistently achieves expert-level performance in LfD (S-on-LfD and C-on-LfD) tasks.
Due to the lack of explicit exploration in online LfO settings, CEIL exhibits drastic performance
degradation (in S-on-LfO and C-on-LfO) under the same environmental interaction steps."
AGGREGATE RESULTS,0.7602040816326531,"8.4
Aggregate Results"
AGGREGATE RESULTS,0.7619047619047619,"According to Agarwal et al. [1], we report the aggregate statistics (for 16 offline IL tasks) in Figure 6.
We can find that CEIL provides competitive performance consistently across a range of offline IL
settings (S-off-LfD, S-off-LfO, C-off-LfD, and C-off-LfO) and outperforms prior offline baselines."
AGGREGATE RESULTS,0.7636054421768708,"Table 8: Normalized scores (averaged over 30 trails for each task) when we vary the number of the expert
demonstrations (#5, #10, #15, and #20). Scores within two points of the maximum score are highlighted"
AGGREGATE RESULTS,0.7653061224489796,"Offline IL settings
Hopper-v2
Halfcheetah-v2
Walker2d-v2
Ant-v2
sum
m
mr
me
m
mr
me
m
mr
me
m
mr
me"
AGGREGATE RESULTS,0.7670068027210885,S-off-LfD #5
AGGREGATE RESULTS,0.7687074829931972,"ORIL (TD3+BC)
42.1
26.7
51.2
45.1
2.7
79.6
44.1
22.9
38.3
25.6
24.5
6.0
408.8
SQIL (TD3+BC)
45.2
27.4
5.9
14.5
15.7
11.8
12.2
7.2
13.6
20.6
23.6
-5.7
192.0
IQ-Learn
17.2
15.4
21.7
6.4
4.8
6.2
13.1
10.6
5.1
22.8
27.2
18.7
169.2
ValueDICE
59.8
80.1
72.6
2.0
0.9
1.2
2.8
0.0
7.4
27.3
32.7
30.2
316.9
DemoDICE
50.2
26.5
63.7
41.9
38.7
59.5
66.3
38.8
101.6
82.8
68.8
112.4
751.2
SMODICE
54.1
34.9
64.7
42.6
38.4
63.8
62.2
40.6
55.4
86.0
69.7
112.4
724.7
CEIL
94.5
45.1
80.8
45.1
43.3
33.9
103.1
81.1
99.4
99.8
101.4
85.0
912.5"
AGGREGATE RESULTS,0.7704081632653061,S-off-LfD #10
AGGREGATE RESULTS,0.7721088435374149,"ORIL (TD3+BC)
42.0
21.6
53.4
45.0
2.1
82.1
44.1
27.4
80.4
47.3
24.0
44.9
514.1
SQIL (TD3+BC)
50.0
34.2
7.4
8.8
10.9
8.2
20.0
15.2
9.7
35.3
36.2
11.9
247.6
IQ-Learn
11.3
18.6
20.1
4.1
6.5
6.6
18.3
12.8
12.2
30.7
53.9
23.7
218.7
ValueDICE
56.0
64.1
54.2
-0.2
2.6
2.4
4.7
4.0
0.9
31.4
72.3
49.5
341.8
DemoDICE
53.6
25.8
64.9
42.1
36.9
60.6
64.7
36.1
100.2
87.4
67.1
114.3
753.5
SMODICE
55.6
30.3
66.6
42.6
38.0
66.0
64.5
44.6
53.8
86.9
69.5
113.4
731.8
CEIL
113.2
53.0
96.3
64.0
43.6
44.0
120.4
82.3
104.2
119.3
70.0
90.1
1000.4"
AGGREGATE RESULTS,0.7738095238095238,S-off-LfD #15
AGGREGATE RESULTS,0.7755102040816326,"ORIL (TD3+BC)
38.9
22.3
46.8
44.7
1.9
83.8
37.9
4.2
69.9
59.4
22.3
12.4
444.6
SQIL (TD3+BC)
42.8
44.4
5.2
6.8
17.1
9.1
16.9
13.5
6.9
21.2
17.2
12.6
213.6
IQ-Learn
14.6
8.2
29.3
4.0
3.4
5.1
7.3
14.5
11.4
54.2
15.2
61.6
228.6
ValueDICE
66.3
58.3
53.6
2.3
2.3
1.2
5.2
-0.1
17.0
45.2
72.0
74.3
397.8
DemoDICE
52.2
29.6
67.3
41.9
37.6
58.1
66.4
42.9
103.5
86.6
68.3
114.3
768.7
SMODICE
55.9
25.7
72.7
42.5
37.6
66.4
67.0
43.2
55.1
86.7
69.7
118.2
740.6
CEIL
116.4
56.7
103.7
80.4
43.0
43.8
120.3
84.8
103.8
126.8
87.0
90.6
1057.3"
AGGREGATE RESULTS,0.7772108843537415,S-off-LfD #20
AGGREGATE RESULTS,0.7789115646258503,"ORIL (TD3+BC)
50.9
22.1
72.7
44.7
30.2
87.5
47.1
26.7
102.6
46.5
31.4
61.9
624.3
SQIL (TD3+BC)
32.6
60.6
25.5
13.2
25.3
14.4
25.6
15.6
8.0
63.6
58.4
44.3
387.1
IQ-Learn
21.3
19.9
24.9
5.0
7.5
7.5
22.3
19.6
18.5
38.4
24.3
55.3
264.5
ValueDICE
73.8
83.6
50.8
1.9
2.4
3.2
24.6
26.4
44.1
79.1
82.4
75.2
547.5
DemoDICE
54.8
32.7
65.4
42.8
37.0
55.6
68.1
39.7
95.0
85.6
69.0
108.8
754.6
SMODICE
56.1
28.7
68.0
42.7
37.7
66.9
66.2
40.7
58.2
87.4
69.9
113.4
735.9
CEIL (ours)
110.4
103.0
106.8
40.0
30.3
63.9
118.6
110.8
117.0
126.3
122.0
114.3
1163.5"
VARYING THE NUMBER OF EXPERT TRAJECTORIES,0.7806122448979592,"8.5
Varying the Number of Expert Trajectories"
VARYING THE NUMBER OF EXPERT TRAJECTORIES,0.782312925170068,"As a complement to the experimental results in the main paper, we continue to compare the per-
formance of CEIL and baselines on more tasks when we vary the number of expert trajectories.
Considering offline IL settings, we provide the results in Table 8 for the number of expert trajectories
of 5, 10, 15, and 20 respectively. We can find that when varying the number of expert behaviors,
CEIL can still obtain higher scores compared to baselines, which is consistent with the findings in
Figure 3 in the main paper."
VARYING THE NUMBER OF EXPERT TRAJECTORIES,0.7840136054421769,"8.6
Limitation (Failure Modes in Online LfO Setting)"
VARYING THE NUMBER OF EXPERT TRAJECTORIES,0.7857142857142857,"Meanwhile, we find that in the online LfO settings, CEIL’s performance deteriorates severely on a
few tasks, as shown in Figure 7 (Walker2d). In LfD (either on single-domain or on cross-domain
IL) settings, CEIL can consistently achieve expert-level performance, but when migrating to LfO
settings, CEIL suffers collapsing performance under the same number of environmental interactions.
We believe that this is due to the lack of expert actions in LfO settings, which causes the agent to stay
in the collapsed state region and therefore deteriorates performance. Thus, we believe a rich direction
for future research is to explore the online exploration ability."
IMPLEMENTATION DETAILS,0.7874149659863946,"9
Implementation Details"
IMITATION LEARNING TASKS,0.7891156462585034,"9.1
Imitation Learning Tasks"
IMITATION LEARNING TASKS,0.7908163265306123,"In our paper, we conduct experiments across a variety of IL problem domains: single/cross-domain
IL, online/offline IL, and LfD/LfO IL settings. By arranging and combining these IL domains, we
obtain 8 IL tasks in all: S-on-LfD, S-on-LfO, S-off-LfD, S-off-LfO, C-on-LfD, C-on-LfO, C-off-LfD,
and C-off-LfO, where S/C denotes single/cross-domain IL, on/off denotes online/offline IL, and
LfD/LfO denote learning from demonstrations/observations respectively."
IMITATION LEARNING TASKS,0.7925170068027211,"S-on-LfD. We have access to a limited number of expert demonstrations and an online interactive
training environment. The goal of S-on-LfD is to learn an optimal policy that mimics the provided
demonstrations in the training environment."
IMITATION LEARNING TASKS,0.79421768707483,"S-on-LfO. We have access to a limited number of expert observations (state-only demonstrations)
and an online interactive training environment. The goal of S-on-LfO is to learn an optimal policy
that mimics the provided observations in the training environment."
IMITATION LEARNING TASKS,0.7959183673469388,"S-off-LfD. We have access to a limited number of expert demonstrations and a large amount of
pre-collected offline (reward-free) data. The goal of S-off-LfD is to learn an optimal policy that
mimics the provided demonstrations in the environment in which the offline data was collected. Note
that here the environment that was used to collect the expert demonstrations and the environment that
was used to collect the offline data are the same environment."
IMITATION LEARNING TASKS,0.7976190476190477,"S-off-LfO. We have access to a limited number of expert observations and a large amount of pre-
collected offline (reward-free) data. The goal of S-off-LfO is to learn an optimal policy that mimics
the provided observations in the environment in which the offline data was collected. Note that here
the environment that was used to collect the expert observations and the environment that was used to
collect the offline data are the same environment."
IMITATION LEARNING TASKS,0.7993197278911565,"C-on-LfD. We have access to a limited number of expert demonstrations and an online interactive
training environment. The goal of C-on-LfD is to learn an optimal policy that mimics the provided
demonstrations in the training environment. Note that here the environment that was used to collect
the expert demonstrations and the online training environment are not the same environment."
IMITATION LEARNING TASKS,0.8010204081632653,"C-on-LfO. We have access to a limited number of expert observations (state-only demonstrations)
and an online interactive training environment. The goal of C-on-LfO is to learn an optimal policy
that mimics the provided observations in the training environment. Note that here the environment
that was used to collect the expert observations and the online training environment are not the same
environment."
IMITATION LEARNING TASKS,0.8027210884353742,"C-off-LfD. We have access to a limited number of expert demonstrations and a large amount of
pre-collected offline (reward-free) data. The goal of C-off-LfD is to learn an optimal policy that
mimics the provided demonstrations in the environment in which the offline data was collected. Note
that here the environment that was used to collect the expert demonstrations and the environment that
was used to collect the offline data are not the same environment."
IMITATION LEARNING TASKS,0.8044217687074829,"C-off-LfO. We have access to a limited number of expert observations and a large amount of pre-
collected offline (reward-free) data. The goal of C-off-LfO is to learn an optimal policy that mimics
the provided observations in the environment in which the offline data was collected. Note that here
the environment that was used to collect the expert observations and the environment that was used to
collect the offline data are not the same environment."
IMITATION LEARNING TASKS,0.8061224489795918,"Figure 8: MuJoCo environments and our modified versions. From left to right: Ant-v2, HalfCheetah-
v2, Hopper-v2, Walker2d-v2, our modified Ant-v2, our modified HalfCheetah-v2, our modified
Hopper-v2, and our modified Walker2d-v2."
IMITATION LEARNING TASKS,0.8078231292517006,"9.2
Online IL Environments, Offline IL Datasets, and One-shot tasks"
IMITATION LEARNING TASKS,0.8095238095238095,"Our experiments are conducted in four popular MuJoCo environments (Figure 8): Hopper-v2,
HalfCheetah-v2, Walker2d-v2, and Ant-v2. For offline IL tasks, we take the standard (reward-free)
D4RL dataset [22] (medium, medium-replay, medium-expert, and expert domains) as the offline
dataset. For cross-domain (online/offline) IL tasks, we collect the expert behaviors (demonstrations
or observations) on a modified MuJoCo environment. Specifically, we change the height of the
agent’s torso (as shown in Figure 8). We refer the reader to our code submission, which includes our
modified MuJoCo assets. For one-shot IL tasks, we train the policy only in the single-domain IL
settings (S-on-LfD, S-on-LfO, S-off-LfD, and S-off-LfO). Then we collect only one expert trajectory
in the modified MuJoCo environment, and roll out the fine-tuned/inferred policy in the modified
environment to test the one-shot performance."
IMITATION LEARNING TASKS,0.8112244897959183,"Collecting expert behaviors. In our implementation, we use the publicly available rlkit7 imple-
mentation of SAC to learn an expert policy and use the learned policy to collect expert behaviors
(demonstrations in LfD or observations in LfO)."
CEIL IMPLEMENTATION DETAILS,0.8129251700680272,"9.3
CEIL Implementation Details"
CEIL IMPLEMENTATION DETAILS,0.814625850340136,"Trajectory self-consistency loss. To learn the embedding function fϕ and a corresponding contextual
policy πθ(a|s, z), we minimize the following trajectory self-consistency loss:"
CEIL IMPLEMENTATION DETAILS,0.8163265306122449,"πθ, fϕ = min
πθ,fϕ −Eτ1:T ∼D(τ1:T )E(s,a)∼τ1:T [log πθ(a|s, fϕ(τ1:T ))] ,"
CEIL IMPLEMENTATION DETAILS,0.8180272108843537,"where τ1:T denotes a trajectory segment with window size of T. In the online setting, we sample
trajectory τ from the experience replay buffer D(τ); in the offline setting, we sample trajectory τ
directly from the given offline data D(τ). Meanwhile, if we can access the expert actions (i.e., LfD
settings), we also incorporate the expert demonstrations into the empirical expectation (i.e., storing
the expert demonstrations into the online/offline experience D(τ))."
CEIL IMPLEMENTATION DETAILS,0.8197278911564626,"In our implementation, we use a 4-layer MLP (with ReLU activation) to encode the tra-
jectory τ1:T and a 4-layer MLP (with ReLU activation ) to predict the action respectively.
To regularize the learning of the encoder function fϕ, we additionally introduce a decoder
network (4-layer MLP with ReLU activation) π′
θ(s′|s, fϕ(τ1:T )) to predict the next states:
minπ′
θ,fϕ −Eτ1:T ∼D(τ1:T )E(s,a,s′)∼τ1:T [log π′
θ(s′|s, fϕ(τ1:T ))]. Further, to circumvent issues of
""posterior collapse"" [67], we encourage learning quantized latent embeddings. In a similar spirit
to VQ-VAE [67], we incorporate ideas from vector quantization (VQ) and introduce the following
regularization: minfϕ ||sg[ze(τ1:T )] −e||2 + ||ze(τ1:T ) −sg[e]||2, where e is a dictionary of vector
quantization embeddings (we set the size of this embedding dictionary to be 4096), ze(τ1:T ) is
defined as the nearest dictionary embedding to fϕ(τ1:T ), and sg[·] denotes the stop-gradient operator."
CEIL IMPLEMENTATION DETAILS,0.8214285714285714,"Out-level embedding inference. In Section 4.2 (main paper), we approximate JMI with JMI(fϕ) ≜
Ep(z∗)πE(τE)πθ(τθ|z∗)

−∥z∗−fϕ(τE)∥2 + ∥z∗−fϕ(τθ)∥2
, where we replace the mutual infor-
mation with −∥z∗−fϕ(τ)∥2 by leveraging the learned embedding function fϕ. Empirically, we
find that we can ignore the second loss ∥z∗−fϕ(τθ)∥2, and directly conduct outer-level embedding
inference with maxz∗,fϕ Ep(z∗)πE(τE)

−∥z∗−fϕ(τE)∥2
. Meanwhile, this simplification makes
the support constraints (R(z∗) in Equation 7 in the main paper) for the offline OOD issues naturally
satisfied, since maxz∗Ep(z∗)πE(τE)

−∥z∗−fϕ(τE)∥2
and minz∗R(z∗) are equivalent."
CEIL IMPLEMENTATION DETAILS,0.8231292517006803,"Cross-domain IL regularization.
To encourage fϕ to capture the task-relevant embeddings
and ignore the domain-specific factors, we set the regularization R(fϕ) in Equation 5 to be:"
CEIL IMPLEMENTATION DETAILS,0.8248299319727891,7https://github.com/rail-berkeley/rlkit.
CEIL IMPLEMENTATION DETAILS,0.826530612244898,"R(fϕ) = I(fϕ(τ); n), where we couple each trajectory τ in {τE} ∪{τE′} with a label n ∈{0, 1},
indicating whether it is noised.
In our implementation, we apply MINE [6] to estimate the
mutual information and conduct encoder regularization. Specifically, we estimate I(z; n) with
ˆI(z; n) := supδ Ep(z,n) [fδ(z, n)] −log Ep(z)p(n) [exp (fδ(z, n))] and regularize the encoder fϕ
with maxfϕ ˆI(fϕ(τ); n), where we model fδ with a 4-layer MLP (using ReLU activations)."
CEIL IMPLEMENTATION DETAILS,0.8282312925170068,"Hyper-parameters. In Table 9, we list the hyper-parameters used in the experiments. For the
size of the embedding dictionary, we selected it from a range of [512, 1024, 2048, 4096]. We
found 4096 to almost uniformly attain good performance across IL tasks, thus selecting it as the
default. For the size of the embedding dimension, we tried four values [4, 8, 16, 32] and selected
16 as the default. For the trajectory window size, we tried five values [2, 4, 8, 16, 32] but we
did not observe a significant difference in performance across these values. Thus we selected 2
as the default value. For the learning rate scheduler, we tried the default Pytorch scheduler and
CosineAnnealingWarmRestarts, and found CosineAnnealingWarmRestarts enables better results
(thus we selected it). For other hyperparameters, they are consistent with the default values of most
RL implementations, e.g. learning rate 3e-4 and the MLP policy."
CEIL IMPLEMENTATION DETAILS,0.8299319727891157,Table 9: CEIL hyper-parameters.
CEIL IMPLEMENTATION DETAILS,0.8316326530612245,"Parameter
Value"
CEIL IMPLEMENTATION DETAILS,0.8333333333333334,"size of the embedding dictionary
4096
size of the embedding dimension
16
trajectory window size
2"
CEIL IMPLEMENTATION DETAILS,0.8350340136054422,"encoder: optimizer
Adam
encoder: learning rate
3e-4
encoder: learning rate scheduler
CosineAnnealingWarmRestarts(T_0 = 1000,T_mult=1, eta_min=1e-5)
encoder: number of hidden layers
4
encoder: number of hidden units per layer
512
encoder: nonlinearity
ReLU"
CEIL IMPLEMENTATION DETAILS,0.8367346938775511,"policy: optimizer
Adam
policy: learning rate
3e-4
policy: learning rate scheduler
CosineAnnealingWarmRestarts(T_0 = 1000,T_mult=1, eta_min=1e-5)
policy: number of hidden layers
4
policy: number of hidden units per layer
512
policy: nonlinearity
ReLU"
CEIL IMPLEMENTATION DETAILS,0.8384353741496599,"decoder: optimizer
Adam
decoder: learning rate
3e-4
decoder: learning rate scheduler
CosineAnnealingWarmRestarts(T_0 = 1000,T_mult=1, eta_min=1e-5)
decoder: number of hidden layers
4
decoder: number of hidden units per layer
512
decoder: nonlinearity
ReLU"
CEIL IMPLEMENTATION DETAILS,0.8401360544217688,Table 10: Baseline methods and their code-bases.
CEIL IMPLEMENTATION DETAILS,0.8418367346938775,"Baselines
Code-bases"
CEIL IMPLEMENTATION DETAILS,0.8435374149659864,"GAIL, GAIfO, AIRL
https://github.com/HumanCompatibleAI/imitation
SAIL
https://github.com/FangchenLiu/SAIL
IQ-Learn, SQIL
https://github.com/Div99/IQ-Learn
ValueDICE
https://github.com/google-research/google-research/tree/master/value_dice
DemoDICE
https://github.com/KAIST-AILab/imitation-dice
SMODICE, ORIL
https://github.com/JasonMa2016/SMODICE"
BASELINES IMPLEMENTATION DETAILS,0.8452380952380952,"9.4
Baselines Implementation Details"
BASELINES IMPLEMENTATION DETAILS,0.8469387755102041,"We summarize our code-bases of our baseline implementations in Table 10 and describe each baseline
as follows:"
BASELINES IMPLEMENTATION DETAILS,0.8486394557823129,"Generative Adversarial Imitation Learning (GAIL). GAIL [30] is a GAN-based online LfD
method that trains a policy (generator) to confuse a discriminator trained to distinguish between
generated transitions and expert transitions. While the goal of the discriminator is to maximize the"
BASELINES IMPLEMENTATION DETAILS,0.8503401360544217,"objective below, the policy is optimized via an RL algorithm to match the expert occupancy measure
(minimize the objective below):"
BASELINES IMPLEMENTATION DETAILS,0.8520408163265306,"J (π, D) = Eπ [log(D(s, a))] + EπE [1 −log(D(s, a))] −λH(π)."
BASELINES IMPLEMENTATION DETAILS,0.8537414965986394,"We used the implementation by Gleave et al. [29] on the GitHub page8, where there are two modifica-
tions introduced with respect to the original paper: 1) a higher output of the discriminator represents
better, 2) PPO is used to optimize the policy instead of TRPO."
BASELINES IMPLEMENTATION DETAILS,0.8554421768707483,"Generative Adversarial Imitation from Observations (GAIfO). GAIfO [66] is an online LfO
method that applies the principle of GAIL and utilizes a state-only discriminator to judge whether
the generated trajectory matches the expert trajectory in terms of states. We provide the objective of
GAIfO as follows:"
BASELINES IMPLEMENTATION DETAILS,0.8571428571428571,"J (π, D) = Eπ [log(D(s, s′))] + EπE [1 −log(D(s, s′))] −λH(π)."
BASELINES IMPLEMENTATION DETAILS,0.858843537414966,"Based on the implementation of GAIL, we implement GAIfO by changing the input of the discrimi-
nator to state transitions."
BASELINES IMPLEMENTATION DETAILS,0.8605442176870748,"Adversarial Inverse Reinforcement Learning (AIRL). AIRL [21] is an online LfD/LfO method
using an adversarial learning framework similar to GAIL. It modifies the form of the discriminator to
explicitly disentangle the task-relevant information from the transition dynamics. To make the policy
more generalized and less sensitive to dynamics, AIRL proposes to learn a parameterized reward
function using the output of the discriminator:"
BASELINES IMPLEMENTATION DETAILS,0.8622448979591837,"fθ,ϕ(s, a, s′) = gθ(s, a) + λhϕ(s′) −hϕ(s),"
BASELINES IMPLEMENTATION DETAILS,0.8639455782312925,"Dθ,ϕ(s, a, s′) =
exp(fθ,ϕ(s, a, s′))
exp(fθ,ϕ(s, a, s′)) + π(a|s)."
BASELINES IMPLEMENTATION DETAILS,0.8656462585034014,"Similarly to GAIL, we used the code provided by Gleave et al. [29], and the RL algorithm is also
PPO."
BASELINES IMPLEMENTATION DETAILS,0.8673469387755102,"State Alignment-based Imitation Learning (SAIL). SAIL [48] is an online LfO method capable of
solving cross-domain tasks. SAIL aims to minimize the divergence between the policy rollout and
the expert trajectory from both local and global perspectives: 1) locally, a KL divergence between the
policy action and the action predicted by a state planner and an inverse dynamics model, 2) globally, a
Wasserstein divergence of state occupancy between the policy and the expert. The policy is optimized
using:"
BASELINES IMPLEMENTATION DETAILS,0.8690476190476191,J (π) = −DW(π(s)∥πE(s)) −λDKL(π(·|st)∥πE(·|st))
BASELINES IMPLEMENTATION DETAILS,0.8707482993197279,"= Eπ(st,at,st+1) 
T
X t=1"
BASELINES IMPLEMENTATION DETAILS,0.8724489795918368,D(st+1) −EπE(s)D(s) T
BASELINES IMPLEMENTATION DETAILS,0.8741496598639455,"
−λDKL"
BASELINES IMPLEMENTATION DETAILS,0.8758503401360545,"
π
 
· |st

∥ginv
 
· |st, f(st)

,"
BASELINES IMPLEMENTATION DETAILS,0.8775510204081632,"where D is a state-based discriminator trained via J (D) = EπE [D(s)] −Eπ [D(s)], f is the
pretrained VAE-based state planner, and ginv is the inverse dynamics model trained by supervised
regression."
BASELINES IMPLEMENTATION DETAILS,0.8792517006802721,"In the online setting, we use the official implementation published by the authors9, where SAIL is
optimized using PPO with the reward definition: r(st, st+1) = 1"
BASELINES IMPLEMENTATION DETAILS,0.8809523809523809,"T

D(st+1) −EπE(s)D(s)

. Besides,
we further implement SAIL in the offline setting by using TD3+BC [23] to maximize the reward
defined above."
BASELINES IMPLEMENTATION DETAILS,0.8826530612244898,"In our experiments, we empirically discover that SAIL is computationally expensive. While SAIL
is able to learn tasks in the typical IL setting (S-on-LfD), our early experimental results find that
SAIL(TD3+BC) with heavy hyperparameter tuning failed on the offline setting. This indicates that
SAIL is rather sensitive to the dataset composition, which also coincides with the results gathered
in Ma et al. [56]. Thus, we do not include SAIL in our comparison results."
BASELINES IMPLEMENTATION DETAILS,0.8843537414965986,"Soft-Q Imitation Learning (SQIL). SQIL [61] is a simple but effective single-domain LfD IL
algorithm that is easy to implement with both online and offline Q-learning algorithms. The main"
BASELINES IMPLEMENTATION DETAILS,0.8860544217687075,"8https://github.com/HumanCompatibleAI/imitation
9https://github.com/FangchenLiu/SAIL"
BASELINES IMPLEMENTATION DETAILS,0.8877551020408163,"idea of SQIL is to give sparse rewards (+1) only to those expert transitions and zero rewards (0) to
those experiences in the replay buffer. The Q-function of SQIL is updated using the squared soft
Bellman Error:"
BASELINES IMPLEMENTATION DETAILS,0.8894557823129252,"δ2(D, r) ≜
1
|D| X"
BASELINES IMPLEMENTATION DETAILS,0.891156462585034,"(s,a,s′)∈D"
BASELINES IMPLEMENTATION DETAILS,0.8928571428571429,"
Q(s, a) −

r + γ log
  X"
BASELINES IMPLEMENTATION DETAILS,0.8945578231292517,"a′∈A
exp(Q(s′, a′))
2
."
BASELINES IMPLEMENTATION DETAILS,0.8962585034013606,The overall objective of the Q-function is to maximize the following objective:
BASELINES IMPLEMENTATION DETAILS,0.8979591836734694,"J (Q) = −δ2(DE, 1) −δ2(Dπ, 0)."
BASELINES IMPLEMENTATION DETAILS,0.8996598639455783,"In our experiments, the online imitation policy is optimized using SAC which is also used in the
original paper. To make a fair comparison among the offline IL baselines, the offline policy is
optimized via TD3+BC."
BASELINES IMPLEMENTATION DETAILS,0.9013605442176871,"Offline Reinforced Imitation Learning (ORIL). ORIL [78] is an offline single-domain IL method
that solves both LfD and LfO tasks. To relax the hard-label assumption (like the sparse rewards
made in SQIL), ORIL treats the experiences stored in the replay buffer as unlabelled data that could
potentially include both successful and failed trajectories. More specifically, ORIL aims to train a
reward function to distinguish between the expert and the suboptimal data without explicitly knowing
the negative labels. By incorporating Positive-unlabeled learning (PU-learning), the objective of the
reward model can be written as follows (for the LfD setting):"
BASELINES IMPLEMENTATION DETAILS,0.9030612244897959,"J (R) = ηEπE(s,a) [log(R(s, a))] + Eπ(s,a) [log(1 −R(s, a))] −ηEπE(s,a) [log(1 −R(s, a))] ,"
BASELINES IMPLEMENTATION DETAILS,0.9047619047619048,"where η is the relative proportion of the expert data and we set it as 0.5 throughout our experiments.
In the original paper, the policy learning algorithm of ORIL is Critic Regularized Regression (CRR),
while in this paper, we implemented ORIL using TD3+BC for fair comparisons. Besides, we adapted
ORIL to the LfO setting by learning a state-only reward function:"
BASELINES IMPLEMENTATION DETAILS,0.9064625850340136,"J (R) = ηEπE(s,s′) [log(R(s, s′))] + Eπ(s,s′) [log(1 −R(s, s′))] −ηEπE(s,s′) [log(1 −R(s, s′))] ."
BASELINES IMPLEMENTATION DETAILS,0.9081632653061225,"Inverse soft-Q learning (IQ-Learn). IQ-Learn [28] is an IRL-based method that can solve IL
tasks in the online/offline and LfD/LfO settings. It proposes to directly learn a Q-function from
demonstrations and avoid the intermediate step of reward learning. Unlike GAIL optimizing a
min-max objective defined in the reward-policy space, IQ-Learn solves the expert matching problem
directly in the policy-Q space. The Q-function is trained to maximize the objective:"
BASELINES IMPLEMENTATION DETAILS,0.9098639455782312,"EπE(s,a,s′) [Q(s, a) −γV π(s′)] −Eπ(s,a,s′) [Q(s, a) −γV π(s′)] −ψ(r),"
BASELINES IMPLEMENTATION DETAILS,0.9115646258503401,"where V π(s) ≜Ea∼π(·|s) [Q(s, a) −log π(a|s)], ψ(r) is a regularization term calculated over the
expert distribution. Then, the policy is learned by SAC."
BASELINES IMPLEMENTATION DETAILS,0.9132653061224489,"We use the code provided in the official IQ-learn repository10 and reproduce the online-LfD results
reported in the original paper. For online tasks, we empirically find that penalizing the Q-value on the
initial states gives the best and most stabilized performance. The learning objective of the Q-function
for the online tasks is:"
BASELINES IMPLEMENTATION DETAILS,0.9149659863945578,"J (Q) = EπE(s,a,s′) [Q(s, a) −γV π(s′)] −(1 −γ)Eρ0 [V π(s0)] −ψ(r)."
BASELINES IMPLEMENTATION DETAILS,0.9166666666666666,"In the offline setting, we find that using the above objective easily leads to an overfitting issue, causing
collapsed performance. Thus, we follow the instruction provided in the paper and only penalize the
expert samples:"
BASELINES IMPLEMENTATION DETAILS,0.9183673469387755,"J (Q) = EπE(s,a,s′) [Q(s, a) −γV π(s′)] −EπE(s,a,s′) [V π(s) −γV π(s′)] −ψ(r)"
BASELINES IMPLEMENTATION DETAILS,0.9200680272108843,"= EπE(s,a,s′) [Q(s, a) −V π(s)] −ψ(r)."
BASELINES IMPLEMENTATION DETAILS,0.9217687074829932,"Imitation Learning via Off-Policy Distribution Matching (ValueDICE). ValueDICE [41] is a
DICE-based11 LfD algorithm which minimizes the divergence of state-action distributions between
the policy and the expert. In contrast to the state-conditional distribution of actions π(·|s) used in the"
BASELINES IMPLEMENTATION DETAILS,0.923469387755102,"10https://github.com/Div99/IQ-Learn
11DICE refers to stationary DIstribution Estimation Correction"
BASELINES IMPLEMENTATION DETAILS,0.9251700680272109,"above methods, the state-action distribution, dπ(s, a) : S × A →[0, 1], can uniquely characterize a
one-to-one correspondence,"
BASELINES IMPLEMENTATION DETAILS,0.9268707482993197,"dπ(s, a) ≜(1 −γ) ∞
X"
BASELINES IMPLEMENTATION DETAILS,0.9285714285714286,"t=0
γtPr(st = s, at = a |s0 ∼ρ0, at ∼π(st), st+1 ∼P(st, at))."
BASELINES IMPLEMENTATION DETAILS,0.9302721088435374,"Thus, the plain expert matching objective can be reformulated and expressed in the Donsker-Varadhan
representation:"
BASELINES IMPLEMENTATION DETAILS,0.9319727891156463,"J (π) = −DKL(dπ(s, a)∥dπE(s, a))
=
min
x:S×A→R log E(s,a)∼dπE [exp(x(s, a))] −E(s,a)∼dπ [x(s, a)] ."
BASELINES IMPLEMENTATION DETAILS,0.9336734693877551,"The objective above can be expanded further by defining x(s, a) = v(s, a) −Bπv(s, a) and using a
zero-reward Bellman operator Bπ to derive the following (adversarial) objective:"
BASELINES IMPLEMENTATION DETAILS,0.935374149659864,"JDICE(π, v) = log E(s,a)∼dπE

exp
 
v(s, a) −Bπv(s, a)

−(1 −γ)Es0∼ρ0,a0∼π(·|s0) [v(s0, a0)] ."
BASELINES IMPLEMENTATION DETAILS,0.9370748299319728,"We use the official Tensorflow implementation12 in our experiments. In the online setting, the rollouts
collected are used as an additional replay regularization. The overall objective in the online setting is:"
BASELINES IMPLEMENTATION DETAILS,0.9387755102040817,"J mix
DICE(π, v)"
BASELINES IMPLEMENTATION DETAILS,0.9404761904761905,"= −DKL
 
(1 −α)dπ(s, a) + αdRB(s, a)∥(1 −α)dπE(s, a) + αdRB(s, a)
"
BASELINES IMPLEMENTATION DETAILS,0.9421768707482994,"= log E(s,a)∼dmix 
exp
 
v(s, a) −Bπv(s, a)

−(1 −α)(1 −γ) Es0∼ρ0, a0∼π(·|s0) [v(s0, a0)]
−α E(s,a)∼dRB [v(s, a) −Bπv(s, a)] ,"
BASELINES IMPLEMENTATION DETAILS,0.9438775510204082,"where dmix ≜(1 −α)dπE + αdRB and α is a non-negative regularization coefficient (we set α as
0.1 following the specification of the paper)."
BASELINES IMPLEMENTATION DETAILS,0.9455782312925171,"In the offline setting, ValueDICE only differs in the source of sampling data. We change the online
replay buffer to the offline pre-collected dataset."
BASELINES IMPLEMENTATION DETAILS,0.9472789115646258,"Offline Imitation Learning with Supplementary Imperfect Demonstrations (DemoDICE).
DemoDICE [39] is a DICE-based offline LfD method that assumes to have access to an offline dataset
collected by a behavior policy πβ. Using this supplementary dataset, the expert matching objective of
DemoDICE is instantiated over ValueDICE:"
BASELINES IMPLEMENTATION DETAILS,0.9489795918367347,"−DKL(dπ(s, a)∥dπE(s, a)) −αDKL(dπ(s, a)∥dπβ(s, a)),"
BASELINES IMPLEMENTATION DETAILS,0.9506802721088435,where α is a positive weight for the constraint.
BASELINES IMPLEMENTATION DETAILS,0.9523809523809523,"The above optimization objective can be transformed into three tractable components: 1) a reward
function r(s, a) derived by pre-training a binary discriminator D : S × A →[0, 1]:"
BASELINES IMPLEMENTATION DETAILS,0.9540816326530612,"r(s, a) = −log(
1
D∗(s, a) −1),"
BASELINES IMPLEMENTATION DETAILS,0.95578231292517,"D∗(s, a) = arg max
D
= EdπE [log D(s, a)] + Edπβ [log(1 −D(s, a))] ,"
BASELINES IMPLEMENTATION DETAILS,0.9574829931972789,2) a value function optimization objective:
BASELINES IMPLEMENTATION DETAILS,0.9591836734693877,"J (v) = −(1 −γ)Es∼ρ0 [v(s)] −(1 + α) log E(s,a)∼dπβ

exp(r(s, a) + Es′∼P (s,a)(v(s′)) −v(s)"
BASELINES IMPLEMENTATION DETAILS,0.9608843537414966,"1 + α
)

,"
BASELINES IMPLEMENTATION DETAILS,0.9625850340136054,and 3) a policy optimization step:
BASELINES IMPLEMENTATION DETAILS,0.9642857142857143,"J (π) = E(s,a)∼dπβ [v∗(s, a) log π(a|s)] ,"
BASELINES IMPLEMENTATION DETAILS,0.9659863945578231,"v∗(s, a) = arg max
v
J (v)."
BASELINES IMPLEMENTATION DETAILS,0.967687074829932,We report the offline results using the official Tensorflow implementation13.
BASELINES IMPLEMENTATION DETAILS,0.9693877551020408,"12https://github.com/google-research/google-research/tree/master/value_dice
13https://github.com/KAIST-AILab/imitation-dice"
BASELINES IMPLEMENTATION DETAILS,0.9710884353741497,"State Matching Offline DIstribution Correction Estimation (SMODICE). SMODICE [56] pro-
poses to solve offline IL tasks in LfO and cross-domain settings and it optimizes the following state
occupancy objective:"
BASELINES IMPLEMENTATION DETAILS,0.9727891156462585,−DKL(dπ(s)∥dπE(s)).
BASELINES IMPLEMENTATION DETAILS,0.9744897959183674,"To incorporate the offline dataset, SMODICE derives an f-divergence regularized state-occupancy
objective:"
BASELINES IMPLEMENTATION DETAILS,0.9761904761904762,Es∼dπ(s)
BASELINES IMPLEMENTATION DETAILS,0.9778911564625851,"
log( dπβ(s)"
BASELINES IMPLEMENTATION DETAILS,0.9795918367346939,"dπE(s))

+ −Df(dπ(s, a)∥dπβ(s, a))."
BASELINES IMPLEMENTATION DETAILS,0.9812925170068028,"Intuitively, the first term can be interpreted as matching the offline states towards the expert states,
while the second regularization term constrains the policy close to the offline distribution of state-
action occupancy. Similarly, we can divide the objective into three steps: 1) deriving a state-based
reward by learning a state-based discriminator:"
BASELINES IMPLEMENTATION DETAILS,0.9829931972789115,"r(s, a) = −log(
1
D∗(s) −1),"
BASELINES IMPLEMENTATION DETAILS,0.9846938775510204,"D∗(s, a) = arg max
D
= EdπE [log D(s)] + Edπβ [log(1 −D(s))] ,"
BASELINES IMPLEMENTATION DETAILS,0.9863945578231292,2) learning a value function using the learned reward:
BASELINES IMPLEMENTATION DETAILS,0.9880952380952381,"J (v) = −(1 −γ)Es∼ρ0 [v(s)] −log E(s,a)∼dπβ

f∗(r(s, a) + Es′∼P (s,a)(v(s′)) −v(s))

,"
BASELINES IMPLEMENTATION DETAILS,0.9897959183673469,and 3) training the policy via weighted regression:
BASELINES IMPLEMENTATION DETAILS,0.9914965986394558,"J (π) = E(s,a)∼dπβ

f ′
∗(r(s, a) + Es′∼P (s,a)(v∗(s′)) −v∗(s)) log π(a|s)

,"
BASELINES IMPLEMENTATION DETAILS,0.9931972789115646,"v∗(s, a) = arg max
v
J (v),"
BASELINES IMPLEMENTATION DETAILS,0.9948979591836735,where f∗is the Fenchel conjugate of f-divergence (please refer to Ma et al. [56] for more details).
BASELINES IMPLEMENTATION DETAILS,0.9965986394557823,"We conduct experiments using the official Pytorch implementation 14, where the f-divergence used is
X 2-divergence. On the LfD tasks, we change the input of the discriminator to state-action pairs."
BASELINES IMPLEMENTATION DETAILS,0.9982993197278912,14https://github.com/JasonMa2016/SMODICE
