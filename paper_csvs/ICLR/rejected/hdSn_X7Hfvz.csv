Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001610305958132045,"Reliable probability estimation is of crucial importance in many real-world appli-
cations where there is inherent uncertainty, such as weather forecasting, medical
prognosis, or collision avoidance in autonomous vehicles. Probability-estimation
models are trained on observed outcomes (e.g. whether it has rained or not, or
whether a patient has died or not), because the ground-truth probabilities of the
events of interest are typically unknown. The problem is therefore analogous to
binary classiﬁcation, with the important difference that the objective is to esti-
mate probabilities rather than predicting the speciﬁc outcome. The goal of this
work is to investigate probability estimation from high-dimensional data using
deep neural networks. There exist several methods to improve the probabilities
generated by these models but they mostly focus on classiﬁcation problems where
the probabilities are related to model uncertainty. In the case of problems with
inherent uncertainty, it is challenging to evaluate performance without access to
ground-truth probabilities. To address this, we build a synthetic dataset to study
and compare different computable metrics. We evaluate existing methods on the
synthetic data as well as on three real-world probability estimation tasks, all of
which involve inherent uncertainty: precipitation forecasting from radar images,
predicting cancer patient survival from histopathology images, and predicting car
crashes from dashcam videos. Finally, we also propose a new method for prob-
ability estimation using neural networks, which modiﬁes the training process to
promote output probabilities that are consistent with empirical probabilities com-
puted from the data. The method outperforms existing approaches on most metrics
on the simulated as well as real-world data."
INTRODUCTION,0.00322061191626409,"1
INTRODUCTION"
INTRODUCTION,0.004830917874396135,"We consider the problem of building models that answer questions such as: Will it rain? Will a
patient survive? Will a car collide with another vehicle? Due to the inherently-uncertain nature
of these real-world phenomena, this requires performing probability estimation, i.e. estimating the
probability of each possible outcome of the phenomenon of interest. Models for probability pre-
diction must be trained on observed outcomes (e.g. whether it rained, a patient died, or a collision
occurred), because the ground-truth probabilities are unknown. The problem is therefore analogous
to binary classiﬁcation, with the important difference that the objective is to estimate probabili-
ties rather than predicting speciﬁc outcomes. In probability estimation, two identical inputs (e.g.
histopathology images from cancer patients) can potentially result in two different outcomes (death
vs. survival). In contrast, in classiﬁcation the class label is usually completely determined by the
data (a picture either shows a cat or it does not)."
INTRODUCTION,0.00644122383252818,"The goal of this work is to investigate probability estimation from high-dimensional data using
deep neural networks. Deep networks trained for classiﬁcation often generate probabilities, which
quantify the uncertainty of the estimate (i.e. how likely the network is to classify correctly). This
quantiﬁcation has been observed to be inaccurate, and several methods have been developed to
improve it (Platt, 1999; Guo et al., 2017; Szegedy et al., 2016; Zhang et al., 2020; Thulasidasan
et al., 2020; Mukhoti et al., 2020; Thagaard et al., 2020), including Bayesian neural networks (Gal &
Ghahramani, 2016; Wang et al., 2016; Shekhovtsov & Flach, 2019; Postels et al., 2019). However,
these works restrict their attention almost exclusively to classiﬁcation in datasets (e.g. CIFAR-
10/100 Krizhevsky (2009), or ImageNet (Deng et al., 2009)) where the label is not uncertain, and
therefore the uncertainty is completely tied to the model: it quantiﬁes the conﬁdence of the model
in its own prediction, not the probability of an event of interest."
INTRODUCTION,0.008051529790660225,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.00966183574879227,"P(survival)=?
Trained"
INTRODUCTION,0.011272141706924315,"Inference
Training"
INTRODUCTION,0.01288244766505636,P(survival)=70% Death
INTRODUCTION,0.014492753623188406,"Figure 1: The probability-estimation problem. In probability estimation, we assume that each ob-
served outcome yi (e.g. death or survival in cancer patients) in the training set is randomly generated
from a latent unobserved probability pi associated to the corresponding data xi (e.g. histopathology
images). Training (left): Only xi and yi can be used for training, because pi is not observed. In-
ference (right): Given new data x, the trained network f produces a probability estimate ˆp ∈[0, 1]."
INTRODUCTION,0.01610305958132045,"Probability estimation from high-dimensional data is crucial in medical prognostics (Wulczyn et al.,
2020), weather prediction (Agrawal et al., 2019), and autonomous driving (Kim et al., 2019). In or-
der to advance deep-learning methodology for probability estimation it is crucial to build appropriate
benchmark datasets. Here we build a synthetic dataset and gather three real-world datasets, which
we use to systematically evaluate existing methodology. In addition, we propose a novel approach,
which outperforms current state-of-the-art methods. Our contributions are the following:"
INTRODUCTION,0.017713365539452495,"• We introduce a new synthetic dataset for probability estimation where a population of people may
have a certain disease connected to age. The task is to predict the probability that they contract the
disease from a picture of their face. The data are generated based on the UTKFaces dataset (Zhang
et al., 2017a), which contains age information. The dataset contains multiple versions of the
synthetic labels, which are generated according to different distributions designed to mimic real-
world probability-prediction datasets. The dataset serves two objectives. First, it allows us to
evaluate existing methodology. Second, it enables us to evaluate different metrics in a controlled
scenario where we have access to ground-truth probabilities.
• We have used publicly available data to build probability-estimation benchmark datasets for three
real-world applications: (1) precipitation forecasting from radar images, (2) prediction of cancer-
patient survival from histopathology images, and (3) prediction of vehicle collisions from dashcam
videos. We use these datasets to systematically evaluate existing approaches, which have been
previously tested mainly on classiﬁcation datasets.
• We propose Calibrated Probability Estimation (CaPE), a novel technique which modiﬁes the train-
ing process so that output probabilities are consistent with empirical probabilities computed from
the data. CaPE outperforms existing approaches on most metrics on synthetic and real-world data."
INTRODUCTION,0.01932367149758454,"2
PROBLEM FORMULATION: PROBABILITY ESTIMATION"
INTRODUCTION,0.020933977455716585,"The goal of probability estimation is to evaluate the likelihood of a certain event of interest, based
on observed data. The available training data consists of n examples xi, 1 ≤i ≤n, each associated
with a corresponding outcome yi. In our applications of interest, the input data are high dimensional:
each xi corresponds to an image or a video. The corresponding label yi is either 0 or 1 depending
on whether or not the event in question occurred. For example, in the cancer-survival application
xi is a histopathology image of a patient, and yi equals 1 if the patient survived for 5 years after
xi was collected. The data have inherent uncertainty: yi, the patient’s survival, does not depend
deterministically on the histopathology image (due e.g. to comorbidities and other health factors).
Instead, we assume that yi equals 1 with a certain probability pi associated with xi,, as illustrated in
Figure 1, because the input data provides key information about the patient’s survival chances."
INTRODUCTION,0.02254428341384863,"At inference, a probability-estimation model aims to generate an estimate ˆp of the underlying proba-
bility p, associated with a new input data point x (e.g. the probability of survival for over 5 years for
new patients based on their histopathology data). To summarize, this is not a classiﬁcation problem,
because the labels are not completely predictable. Instead, the goal is to predict the probability of
the outcome, which is critical in choosing a course of treatment for the patient."
INTRODUCTION,0.024154589371980676,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02576489533011272,(a) ✓Accurate probability estimation
INTRODUCTION,0.027375201288244767,✓Calibration
INTRODUCTION,0.028985507246376812,(b)  Inaccurate probability estimation
INTRODUCTION,0.030595813204508857,✓Calibration
INTRODUCTION,0.0322061191626409,"Figure 2: Calibration is not enough. Uncolored/colored markers denote y = 0/1 outcomes, re-
spectively. Blue/red stand for two classes with different associated ground-truth probabilities (1/4
and 3/4 respectively). (a) The model f retrieves the true probabilities, which requires discriminating
between inputs with low and high probability. (b) The model f has no discriminative power, it just
assigns the same probability to all outputs. However, the model is perfectly calibrated because out
of all outcomes assigned 0.5 by the model, the fraction that are equal to 1 is 50%."
EVALUATION METRICS,0.033816425120772944,"3
EVALUATION METRICS"
EVALUATION METRICS,0.03542673107890499,"Probability estimation shares similar target labels and network outputs with binary classiﬁcation.
However, classiﬁcation accuracy is not an appropriate metric for evaluating probability-estimation
models due to the inherent uncertainty of the outcomes. This is illustrated by the example in Fig-
ure 2a where a perfect probability estimate would result in a classiﬁcation accuracy of just 75%.1"
EVALUATION METRICS,0.037037037037037035,"Metrics when ground-truth probabilities are available. For synthetic datasets, we have access
to the ground truth probability labels and can use them to evaluate performance. Two reasonable
metrics are the mean squared error or ℓ2 distance MSEp, and the Kullback–Leibler divergence KLp
between the estimated and ground-truth probabilities:"
EVALUATION METRICS,0.03864734299516908,"MSEp = 1 N N
X"
EVALUATION METRICS,0.040257648953301126,"i=1
(ˆpi −pi)2, and KLp = 1 N N
X i=1"
EVALUATION METRICS,0.04186795491143317,"
ˆpi log
 ˆpi pi"
EVALUATION METRICS,0.043478260869565216,"
+ (1 −ˆpi) log
1 −ˆpi 1 −pi"
EVALUATION METRICS,0.04508856682769726,"
. (1)"
EVALUATION METRICS,0.04669887278582931,"N is the number of data, and pi, ˆpi are the ground-truth and predicted probabilities respectively."
EVALUATION METRICS,0.04830917874396135,"Calibration metrics. In real-world data, ground-truth probabilities are not available. In order to
evaluate the probabilities estimated by a model, we need to compare them to the observed probabil-
ities. To this end, we aggregate the examples for which the model output equals a certain value (e.g.
0.5), and verify what fraction of them have outcomes equal to 1. If the fraction is close to the model
output, then the model is said to be well calibrated.
Deﬁnition 3.1. A model f is well calibrated if
P (y = 1 | f(x) ∈I(q)) = q,
∀0 ≤q ≤1,
(2)
where y is the observed outcome, f(x) is the probability predicted by model f for input x, and I(q)
is a small interval around q."
EVALUATION METRICS,0.0499194847020934,"Model calibration can be evaluated using the expected calibration error (ECE) (Guo et al., 2017)
(note however that the deﬁnition Guo et al. (2017) is speciﬁc to classiﬁcation). Given a probability-
estimation model f and a dataset of input data xi and associated outcomes yi, 1 ≤i ≤N, we
partition the examples into B bins, I1I2, · · · , IB, according to the probabilities assigned to the
examples by the model. Let Q1,..., QB−1 the B-quantiles of the set {f(x1), . . . , f(xN)}, we have
Ib := [Qb−1, Qb] ∩{f(xi)}N
i=1 (setting Q0 = 0). For each bin, we compute the mean predicted and
empirical probabilities,"
EVALUATION METRICS,0.05152979066022544,"p(b)
emp = E (y | f(x) ∈Ib) =
1
|Ib| X"
EVALUATION METRICS,0.05314009661835749,"i∈Index(Ib)
yi,
(3)"
EVALUATION METRICS,0.05475040257648953,"q(b) =
1
|Ib| X"
EVALUATION METRICS,0.05636070853462158,"i∈Index(Ib)
f(xi),
(4)"
EVALUATION METRICS,0.057971014492753624,"1A perfect model (in terms of probability estimation), assigns 0.25 to the blue class and 0.75 to the red
class. To maximize classiﬁcation accuracy, we predict when the model outputs 0.75 (red examples) and 0
when it outputs 0.25 (blue examples). However, 25% of red examples have an outcome of 0, and 25% of blue
examples have an outcome of 1. As a result, the model would only have 75% accuracy."
EVALUATION METRICS,0.05958132045088567,Under review as a conference paper at ICLR 2022
EVALUATION METRICS,0.061191626409017714,"0.01
0.02
0.03
0.04
MSEp 0.55 0.60 0.65 0.70 AUC"
EVALUATION METRICS,0.06280193236714976,Correlation: -0.750
EVALUATION METRICS,0.0644122383252818,"0.01
0.02
0.03
0.04
MSEp 0.04 0.08 0.12 ECE"
EVALUATION METRICS,0.06602254428341385,Correlation: 0.712
EVALUATION METRICS,0.06763285024154589,"0.01
0.02
0.03
0.04
MSEp 0.12 0.16 0.20 MCE"
EVALUATION METRICS,0.06924315619967794,Correlation - 0.679
EVALUATION METRICS,0.07085346215780998,"0.01
0.02
0.03
0.04
MSEp 0.04 0.08 0.12"
EVALUATION METRICS,0.07246376811594203,KS-Error
EVALUATION METRICS,0.07407407407407407,Correlation: 0.652
EVALUATION METRICS,0.07568438003220612,"0.01
0.02
0.03
0.04
MSEp 0.19 0.20 0.21 0.22 Brier"
EVALUATION METRICS,0.07729468599033816,Correlation: 0.996
EVALUATION METRICS,0.07890499194847021,"CE Early-stop
Infinite Data"
EVALUATION METRICS,0.08051529790660225,"Temperature
Platt Scaling"
EVALUATION METRICS,0.0821256038647343,"Dirichlet Cal.
Mix-n-Match"
EVALUATION METRICS,0.08373590982286634,"Focal Loss
MMCE Reg."
EVALUATION METRICS,0.0853462157809984,"Entropy Reg.
CaPE (bin)"
EVALUATION METRICS,0.08695652173913043,CaPE (kernel)
EVALUATION METRICS,0.08856682769726248,"Figure 3: Evaluating evaluation metrics. We use synthetic data to compare different metrics to
the gold-standard MSEp that uses ground-truth probabilities. Brier score is highly correlated with
MSEp, in contrast to the classiﬁcation metric AUC and the calibration metrics ECE, MCE and KS-
Error. The graphs show the results of the proposed method CaPE, as well as the baselines described
in Section 6.3 on the Linear scenario (see Section 6.1. Other scenarios and a similar comparison
with KLp are included in Appendix D.)"
EVALUATION METRICS,0.09017713365539452,where Index(Ib) = {i | f(xi) ∈Ib}.
EVALUATION METRICS,0.09178743961352658,"The pairs (q(b), p(b)
emp) can be plotted as a reliability diagram, shown in the second row of Figure 4
and in Figure 6. ECE is then deﬁned as"
EVALUATION METRICS,0.09339774557165861,"ECE = 1 B B
X b=1"
EVALUATION METRICS,0.09500805152979067,"p(b)
emp −q(b) .
(5)"
EVALUATION METRICS,0.0966183574879227,Other metrics for calibration include the maximum calibration error (MCE) deﬁned as
EVALUATION METRICS,0.09822866344605476,"MCE =
max
b=1,...,B"
EVALUATION METRICS,0.0998389694041868,"p(b)
emp −q(b) ,"
EVALUATION METRICS,0.10144927536231885,"and the Kolmogorov-Smirnov error (KS-error) (Gupta et al., 2021), a metric based on the cumulative
distribution function, which is described in more detail in Appendix B."
EVALUATION METRICS,0.10305958132045089,"Brier score. Crucially, a model without any discriminative power can be perfectly calibrated (see
Figure 2b). The Brier score is a metric designed to evaluate both calibration and discriminative
ability. It is the mean squared error between the predicted probability and the observed outcomes:"
EVALUATION METRICS,0.10466988727858294,"Brier = 1 N N
X"
EVALUATION METRICS,0.10628019323671498,"i=1
(ˆpi −yi)2.
(6)"
EVALUATION METRICS,0.10789049919484701,"This score can be decomposed into two terms associated to calibration and discrimination ability, as
shown in Appendix C. Using the synthetic data in Section 6.1, where the ground-truth probabilities
are known, we show that Brier score is indeed a reliable proxy for gold-standard MSE metric based
on ground-truth probabilities MSEp, in contrast to calibration metrics such as ECE, MCE or KS-
error, and to classiﬁcation metrics such as AUC (see Figure 3 and Appendix D)."
EVALUATION METRICS,0.10950080515297907,"4
PROPOSED METHOD: CALIBRATED PROBABILITY ESTIMATION (CAPE)"
EVALUATION METRICS,0.1111111111111111,"Prediction models based on deep learning are typically trained by minimizing the cross entropy
between the model output and the training labels (Goodfellow et al., 2016). This cost function is
a proper scoring rule, which means that it evaluates probability estimates in a consistent manner
and is therefore guaranteed to be well calibrated in an inﬁnite-data regime (Buja et al., 2005), as
illustrated by Figure 4 (ﬁrst column)."
EVALUATION METRICS,0.11272141706924316,"Unfortunately, in practice prediction models are trained on ﬁnite data. This is crucial in the case
of deep neural networks, because these models are highly overparametrized and therefore prone to
overﬁtting (Goodfellow et al., 2016). In classiﬁcation, networks have been shown to be capable of
ﬁtting arbitrary random labels (Zhang et al., 2017a). In probability estimation, we observe that neu-
ral networks indeed eventually overﬁt the observed outcomes completely. Moreover, the estimated
probabilities collapse to 0 or 1 (Figure 4, second column), a phenomenon that has also been reported
in classiﬁcation (Mukhoti et al., 2020). However, calibration is preserved during the ﬁrst stages of
training (Figure 4, third column). This is reminiscent of the early-learning phenomenon observed"
EVALUATION METRICS,0.1143317230273752,Under review as a conference paper at ICLR 2022
EVALUATION METRICS,0.11594202898550725,"Figure 4: Miscalibration due to overﬁtting and how to avoid it. When trained on inﬁnite data
(i.e. resampling outcome labels at each epoch according to ground-truth probabilities), models min-
imizing cross-entropy are well calibrated (ﬁrst column). The top row shows results for the synthetic
Discrete scenario (see Section 6.1) (top). The bottom row shows results for the Linear scenario
(dashed line indicates perfect calibration). However, when trained on ﬁxed observed outcomes, the
model eventually overﬁts and the probabilities collapse to either 0 or 1 (second column). This is
mitigated via early stopping (i.e. selecting the model based on validation cross-entropy loss), which
yields relatively good calibration (third column). The proposed Calibration Probability Estimation
(CaPE) method exploits this to further improve the model while ensuring that the output remains
well calibrated. Appendix A.3 shows plots for all synthetic data scenarios."
EVALUATION METRICS,0.11755233494363929,"for classiﬁcation from partially corrupted labels (Yao et al., 2020; Xia et al., 2020), where neural
networks learn from the correct labels before eventually overﬁtting the false ones (Liu et al., 2020)."
EVALUATION METRICS,0.11916264090177134,"Here, we propose to exploit the training dynamics of cross-entropy minimization through a method
that we name Calibrated Probability Estimation (CaPE). Our starting point is a model obtained
via early stopping using validation data on the cross-entropy loss. CaPE is designed to further
improve the discrimination ability of the model, while ensuring that it remains well calibrated. This
is achieved by alternatively minimizing the following two loss functions:"
EVALUATION METRICS,0.12077294685990338,"Discrimination loss: Cross entropy between the model output and the observed binary outcomes,"
EVALUATION METRICS,0.12238325281803543,"LD = − N
X"
EVALUATION METRICS,0.12399355877616747,"i=1
[yi log(f(xi)) + (1 −yi) log(1 −f(xi))]."
EVALUATION METRICS,0.12560386473429952,"Calibration loss: Cross entropy between the output probability of the model and the empirical
probability of the outcomes conditioned on the model output:"
EVALUATION METRICS,0.12721417069243157,"LC = − N
X i=1"
EVALUATION METRICS,0.1288244766505636,"
pi
emp log(f(xi)) + (1 −pi
emp) log(1 −f(xi))

,"
EVALUATION METRICS,0.13043478260869565,"where pi
emp is an estimate of the conditional probability P[y = 1|f(x) ∈I(f(xi))] where I(f(xi))
is a small interval centered at f(xi). As explained in Section 3 if f(xi) is close to this value, then the
model is well calibrated. We consider two approaches for estimating pi
emp. (1) CaPE (bin) where we"
EVALUATION METRICS,0.1320450885668277,"divide the training set into bins, select the bin bi containing f(xi) and set pi
emp = p(bi)
emp in equation 3.
(2) CaPE (kernel) where pi
emp is estimated through a moving average with a kernel function (see
Appendix E for more details). Both methods are efﬁciently computed by sorting the predictions
ˆpi. The calibration loss requires a reasonable estimation of the empirical probabilities p(i)
emp, which
can be obtained from the model after early learning. Therefore using the calibration loss from the
beginning is counterproductive, as demonstrated in Section J."
EVALUATION METRICS,0.13365539452495975,Under review as a conference paper at ICLR 2022
EVALUATION METRICS,0.13526570048309178,"Cross-Entropy minimization
CaPE
CaPE begins"
EVALUATION METRICS,0.13687600644122383,"Training loss
Validation loss"
EVALUATION METRICS,0.13848631239935588,"Figure 5: Calibrated Probability Estimation prevents overﬁtting. Comparison between the learn-
ing curves of cross-entropy (CE) minimization and the proposed calibrated probability estimation
(CaPE), smoothed with a 5-epoch moving average. After an early-learning stage where both train-
ing and validation losses decrease, CE minimization overﬁts (ﬁrst and second graph), with disastrous
consequences in terms of probability estimation (third and fourth graph). In contrast, CaPE prevents
overﬁtting, continuing to improve the model while maintaining calibration (see Figure 4)."
EVALUATION METRICS,0.14009661835748793,Algorithm 1 Pseudocode for CaPE
EVALUATION METRICS,0.14170692431561996,"Require: f
▷early stopped model
Require: m
▷freq. of training with LC
Require: {xi, yi}N
i=1
▷training set
Require: K(p, q) := exp

−(p −q)2 /σ2
▷Gaussian kernel
for t = 1 to num epochs do"
EVALUATION METRICS,0.143317230273752,if t mod m = 0 then
EVALUATION METRICS,0.14492753623188406,"ˆpi ←f(xi), ∀i
Update pi
emp, ∀i, with BIN or KERNEL
L ←LC
▷compute discrimination loss
else"
EVALUATION METRICS,0.14653784219001612,"L ←LD
▷compute calibration loss
end if
f ←backprop with L
▷train with loss
end for"
EVALUATION METRICS,0.14814814814814814,"function BIN(B)
▷B-number of bins
I1, · · · IB ←partitions by quantile of {ˆpj}N
j=1
Find b such that ˆpi ∈Ib
Index(Ib) ←{j|ˆpj ∈Ib}
▷get indices in bin b
pi
emp ←
1
|Ib|
P"
EVALUATION METRICS,0.1497584541062802,"i∈Index(Ib)
yi
▷empirical mean of bin b"
EVALUATION METRICS,0.15136876006441224,end function
EVALUATION METRICS,0.1529790660225443,"function KERNEL(r, K)
▷r-window size; kernel
Nr(i) ←r-nearest neighbor of ˆpi (output probability space)
Z ←
P"
EVALUATION METRICS,0.15458937198067632,"ˆ
pj ∈Nr(i)
K (ˆpi, ˆpj)
▷normalization factor"
EVALUATION METRICS,0.15619967793880837,"pi
emp ←
P"
EVALUATION METRICS,0.15780998389694043,"ˆ
pj ∈Nr(i)
K (ˆpi, ˆpj) yj/Z
▷kernel smooth"
EVALUATION METRICS,0.15942028985507245,end function
EVALUATION METRICS,0.1610305958132045,"CaPE is summarized in Algorithm 1. Figures 4 and 5 show that incorporating the calibration-loss
minimization step indeed preserves calibration as training proceeds (this is not necessarily expected
because CaPE minimizes a calibration loss on the training data), and prevents the model from
overﬁtting the observed outputs. This is beneﬁcial also for the discriminative ability of the model,
because it enables it to further reduce the cross-entropy loss without overﬁtting, as shown in Figure 5.
The experiments with synthetic and real-world data reported in Section 6 suggest that this approach
results in accurate probability estimates across a variety of realistic scenarios."
RELATED WORK,0.16264090177133655,"5
RELATED WORK"
RELATED WORK,0.1642512077294686,"Neural networks trained for classiﬁcation often generate a probability associated with their predic-
tion which quantiﬁes its uncertainty. These estimates are often found to be inaccurate (Mukhoti
et al., 2020; Guo et al., 2017). Techniques mitigating this issue are often described as calibration
methods, and broadly fall into three categories depending on whether they: (1) postprocess the
outputs of a trained model, (2) combine multiple model outputs, or (3) modify the training process."
RELATED WORK,0.16586151368760063,"Post-processing methods transform the output probabilities in order to improve calibration on held-
out data (Zadrozny & Elkan, 2001; Gupta et al., 2021; Kull et al., 2017; 2019). For example, Platt
scaling (Platt, 1999) ﬁts a logistic function that minimizes the negative log-likelihood loss. Temper-
ature scaling (Guo et al., 2017) does the same with a temperature parameter augmenting the softmax
function. In contrast to these methods, CaPE enforces calibration during training, which has the
advantage of enabling further improvements in the discriminative abilities of the model."
RELATED WORK,0.16747181964573268,"Ensembling methods combine multiple models to improve generalization. Mix-n-Match (Zhang
et al., 2020) uses a single model, and ensembles predictions using multiple temperature scaling
transformations. Other methods (Lakshminarayanan et al., 2017; Maddox et al., 2019) ensemble"
RELATED WORK,0.16908212560386474,Under review as a conference paper at ICLR 2022
RELATED WORK,0.1706924315619968,"Methods
Linear
Sigmoid
Centered
Skewed
Discrete
(×10−2)
MSEp
KLp
MSEp
KLp
MSEp
KLp
MSEp
KLp
MSEp
KLp"
RELATED WORK,0.1723027375201288,"Inﬁnite Data*
1.13
2.81
5.35
14.86
0.20
0.41
0.22
0.92
1.52
3.64"
RELATED WORK,0.17391304347826086,"CE early-stop
4.21
10.93
6.16
17.16
0.48
0.98
0.40
1.79
2.24
5.26
Temperature
2.71
6.70
6.13
17.05
0.48
0.98
0.40
1.75
2.20
5.12
Platt Scaling
2.48
6.06
5.78
16.12
0.41
0.83
0.39
1.71
2.06
4.82
Dirichlet Cal.
3.56
9.06
8.71
25.33
0.46
0.94
0.48
2.31
2.74
6.52
Mix-n-match
2.69
6.70
6.13
17.10
0.48
0.98
0.39
1.74
2.20
5.12
Focal Loss
4.13
10.51
6.89
19.51
0.48
0.97
1.27
11.61
2.92
6.76
Entropy Reg.
2.84
7.37
7.03
21.19
0.42
0.87
1.17
10.57
2.84
6.62
MMCE Reg.
2.22
5.65
5.33
15.03
0.44
0.90
0.54
2.43
2.08
4.90
Deep Ensemble
1.90
4.55
5.85
16.43
0.44
0.89
0.55
2.58
1.97
4.61"
RELATED WORK,0.17552334943639292,"CaPE (bin)
1.83
4.46
5.29
14.59
0.38
0.78
0.40
1.72
1.83
4.31
CaPE (kernel)
1.81
4.41
5.22
14.47
0.40
0.81
0.39
1.70
1.85
4.36"
RELATED WORK,0.17713365539452497,"Table 1: Results on synthetic data. Appendix A.1 shows a table with conﬁdence intervals using
bootstrapping. * is a model trained with inﬁnite data obtained by continuous label resampling."
RELATED WORK,0.178743961352657,"multiple models obtained using different initializations. These approaches are compatible with the
proposed method CaPE; how to combine them effectively is an interesting future research direction."
RELATED WORK,0.18035426731078905,"Modiﬁed training methods can be divided into two groups. The ﬁrst group smooths the target 0/1
labels in order to prevent output estimates from collapsing to 0/1 (Mukhoti et al., 2020; Szegedy
et al., 2016; Zhang et al., 2018; Thulasidasan et al., 2020). The second group, attaches additional
calibration penalties to a cross entropy loss (Kumar et al., 2018; Pereyra et al., 2017; Liang et al.,
2020). CaPE is most similar in spirit to the latter methods, although its data-driven calibration loss
is different to the penalties used in these techniques."
RELATED WORK,0.1819645732689211,"Datasets for evaluation The methods discussed in this section were developed for calibration in
classiﬁcation, and tested on datasets such as CIFAR-10/100 (Krizhevsky, 2009), SVHN (Netzer
et al., 2011), and ImageNet (Deng et al., 2009) where the relationship between labels and input
data is completely deterministic. Here, we evaluate these methods on synthetic and real-world
probability-estimation problems with inherent uncertainty."
EXPERIMENTS,0.18357487922705315,"6
EXPERIMENTS"
EXPERIMENTS,0.18518518518518517,"6.1
SYNTHETIC DATASET: FACE-BASED RISK PREDICTION"
EXPERIMENTS,0.18679549114331723,"To benchmark probability-estimation methods, we build a synthetic dataset based on UTK-
Face (Zhang et al., 2017b), containing face images and associated ages. We use the age of the ith per-
son zi to assign them a risk of contracting a disease pi = ψ(zi) for a ﬁxed function ψ : N →[0, 1].
Then we simulate whether the person actually contracts the illness (label yi = 1) or not (yi = 0)
with probability pi. The probability-estimation task is to estimate the ground-truth probability pi
from the face image xi, which requires learning to discriminate age and map it to the corresponding
risk. We design ψ to create ﬁve scenarios, inspired by real-world data (see Appendix G)):"
EXPERIMENTS,0.18840579710144928,"• Linear: Equally-spaced, inspired by weather forecasting: ψ(z) = z/100
• Sigmoid: Concentrated near two extremes: ψ(z) = σ(25(z/100 −0.29))
• Skewed: Clustered close to zero, inspired by vehicle-collision detection: ψ(z) = z/250
• Centered: Clustered in the center, inspired by cancer-survival prediction: ψ(z) = z/300 + 0.35
• Discrete: Discretized: ψ(z) = 0.2

1{z>20} + 1{z>40} + 1{z>60} + 1{z>80}

+ 0.1"
REAL-WORLD DATASETS,0.19001610305958133,"6.2
REAL-WORLD DATASETS"
REAL-WORLD DATASETS,0.19162640901771336,"We propose to use three open-source, real-world datasets to benchmark probability-estimation ap-
proaches (see Appendix H for further details on the datasets and experiments)."
REAL-WORLD DATASETS,0.1932367149758454,Under review as a conference paper at ICLR 2022
REAL-WORLD DATASETS,0.19484702093397746,"Method
Cancer Survival
Weather forecasting
Collision Prediction
(×10−2)
AUC
ECE
Brier
AUC
ECE
Brier
AUC
ECE
Brier"
REAL-WORLD DATASETS,0.1964573268921095,"CE early-stop
58.88
12.25
23.96
77.64
10.91
20.57
85.68
4.36
8.59
Temperature
58.88
12.07
23.73
77.64
8.66
20.21
85.68
4.56
8.51
Platt Scaling
58.91
10.28
23.33
77.65
6.97
19.53
85.76
3.04
8.23
Dirichlet Cal.
49.89
13.83
24.08
77.51
14.29
21.89
83.36
5.78
8.78
Mix-n-match
58.88
12.16
23.67
77.64
8.65
20.21
85.68
4.40
8.52
Focal Loss
55.02
12.15
23.31
76.18
8.32
20.27
82.21
9.07
9.82
Entropy Reg.
56.29
11.73
23.62
79.01
10.53
19.77
83.15
14.54
11.10
MMCE Reg.
48.45
11.84
23.73
76.69
8.46
20.12
85.18
2.94
8.48
Deep Ensemble
52.46
9.99
23.47
79.86
7.41
18.82
85.27
3.15
8.55"
REAL-WORLD DATASETS,0.19806763285024154,"CaPE (bin)
61.44
12.31
23.20
78.99
5.16
18.37
85.70
3.16
8.18
CaPE (kernel)
61.22
9.48
23.18
79.00
5.08
18.39
85.95
3.22
8.13"
REAL-WORLD DATASETS,0.1996779388083736,"Table 2: Results on cancer-survival prediction, weather forecasting, and collision prediction. Tables
with all the metrics described in Section 3 are provided in Appendix A.2"
REAL-WORLD DATASETS,0.20128824476650564,"Survival of Cancer Patients. Histopathology aims to identify tumor cells, cancer subtypes, and
the stage and level of differentiation of cancer. Hematoxylin and Eosin (H&E)-stained slides are
the most common type of histopathology data used for clinical decision making. In particular,
they can be used for survival prediction (Wulczyn et al., 2020), which is critical in evaluating the
prognosis of patients. Treatments assigned to patients after diagnosis are not personalized and their
impact on cancer trajectory is complex, so the survival status of a patient is not deterministic. In
this work, we use the H&E slides of non-small cell lung cancers from The Cancer Genome Atlas
Program (TCGA)2 to estimate the the 5-year survival probability of cancer patients. The outcome
distribution is similar to the Centered scenario in our synthetic data."
REAL-WORLD DATASETS,0.2028985507246377,"Weather Forecasting. The atmosphere is governed by nonlinear dynamics, hence weather forecast
models possess inherent uncertainties (Richardson, 2007). Nowcasting, weather prediction in the
near future, is of great operational signiﬁcance, especially with increasing number of extreme in-
clement weather conditions (Agrawal et al., 2019; Ravuri et al., 2021). We use the German Weather
service dataset3, which contains quality-controlled rainfall-depth composites from 17 operational
Doppler radars. We use 30 minutes of precipitation data to predict if the mean precipitation will
increase or decrease after one hour. Three precipitation maps from the past 30 minutes serve as an
input. The outcome distribution is similar to the Linear scenario in our synthetic data."
REAL-WORLD DATASETS,0.20450885668276972,"Collision Prediction. Vehicle collision is one of the leading causes of death in the world. Reliable
collision prediction systems are therefore instrumental in saving human lives. These systems predict
potential collisions from dashcam cameras. Collisions are inﬂuenced by many unknown factors,
and hence are not deterministic. Following Kim et al. (2019), we use 0.3 seconds of real dashcam
videos from YouTubeCrash dataset as input, and predict the probability of a collision in the next
2 seconds. The data are very imbalanced as the number of collisions is very low, so the outcome
distribution is similar to the Skewed scenario in our synthetic data."
BASELINES,0.20611916264090177,"6.3
BASELINES"
BASELINES,0.20772946859903382,"We apply existing calibration methods developed for classiﬁcation to probability estimation (as well
as cross-entropy minimization with early-stopping): (1) Three post-processing methods: Temper-
ature Scaling (Guo et al., 2017), Platt Scaling (Platt, 1999), and Dirichlet Calibration (Kull et al.,
2019) applied to the best CE model, (2) Two Ensemble Methods: Mix-n-Match (Zhang et al., 2020)
applied to best CE model, and Deep Ensemble (Lakshminarayanan et al., 2017) with 5 networks,
and (3) Three Modiﬁed Training methods: focal loss (Mukhoti et al., 2020), entropy-maximizing
loss (Pereyra et al., 2017), and MMCE regularization (Kumar et al., 2018). Appendix F provides
a detailed description. For our experiments on synthetic data, we also compare against a model
trained on an inﬁnite amount of data by repeatedly sampling new outcomes from the ground-truth
probabilities at each epoch. This provides a best-case reference for each scenario."
BASELINES,0.20933977455716588,"2https://www.cancer.gov/tcga
3https://opendata.dwd.de/weather/radar/"
BASELINES,0.2109500805152979,Under review as a conference paper at ICLR 2022
BASELINES,0.21256038647342995,"0.0
0.2
0.4
0.6
0.8
1.0
Predicted Probability 0.0 0.2 0.4 0.6 0.8 1.0"
BASELINES,0.214170692431562,Empirical Probability
BASELINES,0.21578099838969403,Cancer Survival
BASELINES,0.21739130434782608,"CE Early-stop
Deep Ensemble
CaPE (kernel)"
BASELINES,0.21900161030595813,"0.0
0.2
0.4
0.6
0.8
1.0
Predicted Probability 0.0 0.2 0.4 0.6 0.8 1.0"
BASELINES,0.22061191626409019,Empirical Probability
BASELINES,0.2222222222222222,Weather Forecast
BASELINES,0.22383252818035426,"CE Early-stop
Deep Ensemble
CaPE (kernel)"
BASELINES,0.22544283413848631,"0.0
0.2
0.4
0.6
0.8
1.0
Predicted Probability 0.0 0.2 0.4 0.6 0.8 1.0"
BASELINES,0.22705314009661837,Empirical Probability
BASELINES,0.2286634460547504,Collision Prediction
BASELINES,0.23027375201288244,"CE Early-stop
MMCE
CaPE (kernel)"
BASELINES,0.2318840579710145,"Figure 6: Reliability diagrams for real-world data. Reliability diagrams computed on test data for
cross-entropy minimization with early stopping, the proposed method (CaPE) and the best baseline
for each dataset. CaPE produces better calibrated outputs. Appendix A.3 shows additional diagrams."
RESULTS AND DISCUSSION,0.23349436392914655,"7
RESULTS AND DISCUSSION"
RESULTS AND DISCUSSION,0.23510466988727857,"Table 1 shows that calibration methods developed for classiﬁcation can be effective for probability
estimation. However, the performance of some methods is not consistent across all scenarios. For
instance, regularization with negative entropy, which penalizes very high/low conﬁdence, performs
worse than CE when the ground-truth probability is close to 0 or 1. In contrast, methods that do
not make strong assumptions tend to generalize better to multiple scenarios (e.g. Platt scaling con-
sistently beats CE). The proposed method CaPE outperforms other techniques in most scenarios,
and even matches the performance of the inﬁnite data baseline for the Sigmoid scenario. Finally, we
observe that the Skewed scenario is very challenging: most methods barely improve the CE baseline."
RESULTS AND DISCUSSION,0.23671497584541062,"Table 2 compares the baseline methods and CaPE on the three real-world datasets. We present AUC,
ECE for 15 equally-sized bins, and Brier score, as complementary metrics since the underlying
ground-truth probabilities are unobserved. As discussed in Section 3, Brier score is the metric
that best captures the quality of probability estimates. CaPE has the lowest Brier score in all three
datasets, while also achieving lower ECE values and higher AUC values than the other methods. This
demonstrates that enforcing calibration during training also yields a more discriminative model. The
reliability diagrams in Figure 6 depict the probability estimates produced by CE, CaPE and the best
baseline method on the three datasets, demonstrating that CaPE produces a well-calibrated outputs."
RESULTS AND DISCUSSION,0.23832528180354268,"Figure 6 also shows that each real-world dataset closely aligns with a particular synthetic scenario:
cancer survival with Centered; weather forecasting with Linear; collision prediction with Skewed.
This supports the signiﬁcance of our synthetic benchmark dataset, and provides insights in the dif-
ferences among baseline models. For example, model averaging with deep ensemble performs well
on weather forecasting but has higher Brier scores than Platt scaling on the other two datasets (see
Appendix I for further analysis based on pathological stages). Accordingly, deep ensemble also
underperforms in the synthetic scenarios where ground-truth probabilities are clustered closely (Sig-
moid, Linear), but is effective for Linear. Finally, as in the synthetic Skewed scenario, all methods
had similar performance on the collision prediction task. This highlights the importance of consid-
ering different scenarios when evaluating methodology for probability estimation."
CONCLUSION,0.23993558776167473,"8
CONCLUSION"
CONCLUSION,0.24154589371980675,"In this work we evaluate existing approaches to improve the output probabilities of neural net-
works on probability-estimation problems. To this end, we introduce a new synthetic benchmark
dataset designed to reproduce several realistic scenarios, and also gather three real-world datasets
relevant to medicine, climatology, and self-driving cars. In addition, we propose a novel approach
for probability-estimation via deep learning that outperforms existing approaches for most datasets.
An important application for probability estimation is in the context of survival analysis, which can
be recast as estimation of conditional probabilities (Lee et al., 2018; Shamout et al., 2020; Gold-
stein et al., 2021). An interesting research direction is to consider problems with several possible
uncertain outcomes (analogous to multiclass classiﬁcation)."
CONCLUSION,0.2431561996779388,Under review as a conference paper at ICLR 2022
REFERENCES,0.24476650563607086,REFERENCES
REFERENCES,0.2463768115942029,"Shreya Agrawal, Luke Barrington, Carla Bromberg, John Burge, Cenk Gazen, and Jason
Hickey.
Machine learning for precipitation nowcasting from radar images.
arXiv preprint
arXiv:1912.12132, 2019."
REFERENCES,0.24798711755233493,"Georgy Ayzel. Rainnet: a convolutional neural network for radar-based precipitation nowcasting.
https://github.com/hydrogo/rainnet, 2020."
REFERENCES,0.249597423510467,"Andreas Buja, Werner Stuetzle, and Yi Shen.
Loss functions for binary class probability
estimation and classiﬁcation:
Structure and applications,” manuscript, available at www-
stat.wharton.upenn.edu/ buja, 2005."
REFERENCES,0.25120772946859904,"Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momen-
tum contrastive learning. CoRR, abs/2003.04297, 2020. URL https://arxiv.org/abs/
2003.04297."
REFERENCES,0.2528180354267311,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848."
REFERENCES,0.25442834138486314,"Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning, 2016."
REFERENCES,0.2560386473429952,"Mark Goldstein, Xintian Han, Aahlad Puli, Adler J. Perotte, and Rajesh Ranganath. X-cal: Explicit
calibration for survival analysis, 2021."
REFERENCES,0.2576489533011272,"Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016."
REFERENCES,0.25925925925925924,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321–1330. PMLR, 2017."
REFERENCES,0.2608695652173913,"Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu,
and Richard Hartley. Calibration of neural networks using splines. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
eQe8DEWNN2W."
REFERENCES,0.26247987117552335,"Maximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learn-
ing.
In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Con-
ference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp.
2127–2136. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/
ilse18a.html."
REFERENCES,0.2640901771336554,"Hoon Kim, Kangwook Lee, Gyeongjo Hwang, and Changho Suh. Crash to not crash: Learn to
identify dangerous vehicles using a simulator. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 33, pp. 978–985, 2019."
REFERENCES,0.26570048309178745,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.2673107890499195,"Meelis Kull, Telmo M. Silva Filho, and Peter Flach.
Beyond sigmoids: How to obtain well-
calibrated probabilities from binary classiﬁers with beta calibration. Electronic Journal of Statis-
tics, 11(2):5052 – 5080, 2017. doi: 10.1214/17-EJS1338SI. URL https://doi.org/10.
1214/17-EJS1338SI."
REFERENCES,0.2689210950080515,"Meelis Kull, Miquel Perell´o-Nieto, Markus K¨angsepp, Telmo de Menezes e Silva Filho, Hao Song,
and Peter A. Flach. Beyond temperature scaling: Obtaining well-calibrated multiclass probabili-
ties with dirichlet calibration. In NeurIPS, 2019."
REFERENCES,0.27053140096618356,"Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks
from kernel mean embeddings. In ICML, 2018."
REFERENCES,0.2721417069243156,"Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles, 2017."
REFERENCES,0.27375201288244766,Under review as a conference paper at ICLR 2022
REFERENCES,0.2753623188405797,"Changhee Lee, William Zame, Jinsung Yoon, and Mihaela van der Schaar. Deephit: A deep learning
approach to survival analysis with competing risks. Proceedings of the AAAI Conference on Arti-
ﬁcial Intelligence, 32(1), Apr. 2018. URL https://ojs.aaai.org/index.php/AAAI/
article/view/11842."
REFERENCES,0.27697262479871176,"Gongbo Liang, Yu Zhang, Xiaoqin Wang, and Nathan Jacobs. Improved trainable calibration method
for neural networks on medical imaging classiﬁcation. In British Machine Vision Conference
(BMVC), 2020."
REFERENCES,0.2785829307568438,"Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning
regularization prevents memorization of noisy labels. Advances in Neural Information Processing
Systems, 33, 2020."
REFERENCES,0.28019323671497587,"Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry P. Vetrov, and Andrew Gordon Wilson. A
simple baseline for bayesian uncertainty in deep learning. CoRR, abs/1902.02476, 2019. URL
http://arxiv.org/abs/1902.02476."
REFERENCES,0.28180354267310787,"Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, S. Golodetz, Philip H. S. Torr, and P. Dokania.
Calibrating deep neural networks using focal loss. ArXiv, abs/2002.09437, 2020."
REFERENCES,0.2834138486312399,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning.
2011.
URL http://ufldl.
stanford.edu/housenumbers/nips2011_housenumbers.pdf."
REFERENCES,0.28502415458937197,"Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey E. Hinton. Regular-
izing neural networks by penalizing conﬁdent output distributions. CoRR, abs/1701.06548, 2017.
URL http://arxiv.org/abs/1701.06548."
REFERENCES,0.286634460547504,"J. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood
methods. Advances in Large Margin Classiﬁers, 10(3), 1999."
REFERENCES,0.2882447665056361,"Janis Postels, Francesco Ferroni, Huseyin Coskun, Nassir Navab, and Federico Tombari. Sampling-
free epistemic uncertainty estimation using approximated variance propagation.
CoRR,
abs/1908.00598, 2019. URL http://arxiv.org/abs/1908.00598."
REFERENCES,0.2898550724637681,"Suman Ravuri, Karel Lenc, Matthew Willson, Dmitry Kangin, Remi Lam, Piotr Mirowski, Megan
Fitzsimons, Maria Athanassiadou, Sheleem Kashem, Sam Madge, et al. Skillful precipitation
nowcasting using deep generative models of radar. arXiv preprint arXiv:2104.00954, 2021."
REFERENCES,0.2914653784219002,"Lewis Fry Richardson. Weather prediction by numerical process. Cambridge university press, 2007."
REFERENCES,0.29307568438003223,"Farah E. Shamout, Yiqiu Shen, Nan Wu, Aakash Kaku, Jungkyu Park, Taro Makino, Stanislaw Jas-
trzebski, Duo Wang, Ben Zhang, Siddhant Dogra, Meng Cao, Narges Razavian, David Kudlowitz,
Lea Azour, William Moore, Yvonne W. Lui, Yindalon Aphinyanaphongs, Carlos Fernandez-
Granda, and Krzysztof J. Geras. An artiﬁcial intelligence system for predicting the deteriora-
tion of COVID-19 patients in the emergency department. CoRR, abs/2008.01774, 2020. URL
https://arxiv.org/abs/2008.01774."
REFERENCES,0.2946859903381642,"Alexander Shekhovtsov and Boris Flach. Feed-forward propagation in probabilistic neural networks
with categorical and max layers. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=SkMuPjRcKQ."
REFERENCES,0.2962962962962963,"Christian Szegedy, V. Vanhoucke, S. Ioffe, Jonathon Shlens, and Z. Wojna. Rethinking the incep-
tion architecture for computer vision. 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 2818–2826, 2016."
REFERENCES,0.29790660225442833,"Jeppe Thagaard, Søren Hauberg, Bert van der Vegt, Thomas Ebstrup, Johan D Hansen, and Anders B
Dahl. Can you trust predictive uncertainty under real dataset shifts in digital pathology?
In
International Conference on Medical Image Computing and Computer-Assisted Intervention, pp.
824–833. Springer, 2020."
REFERENCES,0.2995169082125604,"Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.
On mixup training: Improved calibration and predictive uncertainty for deep neural networks,
2020."
REFERENCES,0.30112721417069244,Under review as a conference paper at ICLR 2022
REFERENCES,0.3027375201288245,"Hao Wang, Xingjian Shi, and Dit-Yan Yeung. Natural-parameter networks: A class of probabilistic
neural networks.
CoRR, abs/1611.00448, 2016.
URL http://arxiv.org/abs/1611.
00448."
REFERENCES,0.30434782608695654,"Ellery Wulczyn, David F. Steiner, Zhaoyang Xu, Apaar Sadhwani, Hongwu Wang, I. Flament,
C. Mermel, Po-Hsuan Cameron Chen, Yun Liu, and Martin C. Stumpe. Deep learning-based
survival prediction for multiple cancer types using histopathology images. PLoS ONE, 15, 2020."
REFERENCES,0.3059581320450886,"Xiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, Nannan Wang, Zongyuan Ge, and Yi Chang.
Robust early-learning: Hindering the memorization of noisy labels. In International Conference
on Learning Representations, 2020."
REFERENCES,0.3075684380032206,"Quanming Yao, Hansi Yang, Bo Han, Gang Niu, and James Tin-Yau Kwok. Searching to exploit
memorization effect in learning with noisy labels. In International Conference on Machine Learn-
ing, pp. 10789–10798. PMLR, 2020."
REFERENCES,0.30917874396135264,"B. Zadrozny and C. Elkan. Obtaining calibrated probability estimates from decision trees and naive
bayesian classiﬁers. In ICML, 2001."
REFERENCES,0.3107890499194847,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. ICLR, 2017a."
REFERENCES,0.31239935587761675,"Hongyi Zhang, Moustapha Ciss´e, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. ArXiv, abs/1710.09412, 2018."
REFERENCES,0.3140096618357488,"Jize Zhang, B. Kailkhura, and T. Y. Han. Mix-n-match: Ensemble and compositional methods for
uncertainty calibration in deep learning. In ICML, 2020."
REFERENCES,0.31561996779388085,"Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial
autoencoder. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE,
2017b."
REFERENCES,0.3172302737520129,Under review as a conference paper at ICLR 2022
REFERENCES,0.3188405797101449,"A
ADDITIONAL RESULTS"
REFERENCES,0.32045088566827695,We present here supplementary results to the ones presented in Section 7.
REFERENCES,0.322061191626409,"A.1
FACE-BASED RISK PREDICTION"
REFERENCES,0.32367149758454106,"Full evaluation with conﬁdence intervals derived using 1000 bootstraps for the ﬁve simulated sce-
narios are examined: Linear (Table.3); Sigmoid (Table.4); Centered (Table.5); Skewed (Table.6);
Discrete (Table.7). Note that all numbers are downscaled by 10−2 in the tables."
REFERENCES,0.3252818035426731,"Linear
ECE
MCE
KS
Brier
MSEp
KLp
Inﬁnite Data
4.14±0.81
12.07±3.29
2.24±0.88
18.97±0.33 1.14±0.04
2.82±0.11"
REFERENCES,0.32689210950080516,"CE early-stop
12.32±0.83 21.79±1.97 12.16±0.83 21.82±0.51 4.21±0.15 10.94±0.36
Temperature
5.7±0.74
13.82±2.71
2.36±0.74
20.47±0.37 2.73±0.11
6.75±0.25
Platt Scaling
4.29±0.77
10.94±2.55
1.3±0.45
20.18±0.36 2.48±0.09
6.07±0.22
Dirichlet Cal.
7.38±1.12
22.58±7.24
3.78±0.46
21.32±0.33 3.56±0.13
9.08±0.29
Focal Loss
5.34±0.68
13.31±2.67
3.56±0.85
21.99±0.28 4.13±0.11 10.52±0.28
Mix-n-match
5.46±0.84
12.9±2.51
1.92±0.44
20.43±0.35
2.7±0.11
6.72±0.24
Entropy Reg.
5.7±0.74
13.52±1.67
4.94±0.9
20.42±0.3
2.58±0.09
6.65±0.21
MMCE Reg.
4.89±0.74
12.57±2.27
1.92±0.46
20.04±0.38 2.24±0.08
5.68±0.2
Deep Ensemble
4.26±0.72
11.33±2.38
1.95±0.61
19.88±0.32
1.9±0.07
4.55±0.18"
REFERENCES,0.3285024154589372,"CaPE (bin)
4.58±0.75
11.85±2.49
1.71±0.51
19.68±0.36 1.78±0.07
4.35±0.16
CaPE (kernel)
4.62±0.62
12.25±2.31
1.65±0.38
19.71±0.34 1.74±0.07
4.3±0.17"
REFERENCES,0.33011272141706927,Table 3: Performance on Face-based Risk Prediction. Linear scenario.
REFERENCES,0.33172302737520126,"Sigmoid
ECE
MCE
KS
Brier
MSEp
KLp
Inﬁnite Data
6.4±0.71
20.63±3.44 2.74±0.45 16.28±0.44
5.34±0.2
14.82±0.51"
REFERENCES,0.3333333333333333,"CE early-stop
6.19±0.75
17.0±3.68
5.86±0.8
16.68±0.42 6.16±0.17 17.16±0.48
Temperature
5.57±0.71 15.32±3.09 5.02±0.83 16.58±0.34 6.13±0.17 17.09±0.43
Platt Scaling
3.45±0.68 10.32±2.79
1.3±0.43
16.33±0.34 5.78±0.19 16.15±0.47
Dirichlet Cal.
14.5±1.15 25.68±3.02 4.67±0.32 19.21±0.43 8.64±0.26 25.18±0.58
Focal Loss
4.65±0.7
11.84±2.78 2.66±0.77 16.96±0.34 6.86±0.21
19.46±0.5
Mix-n-match
5.65±0.76 15.32±3.41 5.09±0.94
16.6±0.36
6.12±0.17 17.08±0.46
Entropy Reg.
9.51±0.79 18.77±2.38 7.26±0.78 17.17±0.31 7.02±0.17 21.16±0.42
MMCE Reg.
4.67±0.76 13.63±2.59
2.5±0.53
15.9±0.51
5.35±0.18 15.06±0.49
Deep Ensemble
5.17±0.74 16.12±3.11 2.04±0.44 16.39±0.45 5.86±0.22
16.46±0.6"
REFERENCES,0.33494363929146537,"CaPE (bin)
3.78±0.6
11.96±2.59
2.22±0.7
15.84±0.43
5.17±0.2
14.27±0.49
CaPE (kernel)
3.9±0.75
11.73±2.79 2.05±0.54 15.85±0.41
5.16±0.2
14.34±0.49"
REFERENCES,0.3365539452495974,Table 4: Performance on Face-based Risk Prediction. Sigmoid scenario.
REFERENCES,0.33816425120772947,Under review as a conference paper at ICLR 2022
REFERENCES,0.3397745571658615,"Centered
ECE
MCE
KS
Brier
MSEp
KLp
Inﬁnite Data
4.29±0.74
12.38±2.92
2.68±0.8
24.22±0.13
0.2±0.01
0.41±0.01"
REFERENCES,0.3413848631239936,"CE early-stop
5.76±0.84
15.32±3.07
4.19±1.02
24.68±0.08
0.48±0.01
0.98±0.03
Temperature
6.09±0.82
15.83±2.91
4.74±0.96
24.74±0.06
0.48±0.01
0.98±0.03
Platt Scaling
4.57±0.76
11.85±2.5
2.79±0.85
24.62±0.08
0.41±0.01
0.83±0.03
Dirichlet Cal.
4.84±1.15
13.13±7.61
2.16±0.86
24.7±0.1
0.46±0.01
0.94±0.03
Mix-n-match
6.05±0.83
15.71±2.92
4.68±0.98
24.74±0.06
0.48±0.01
0.98±0.02
Focal Loss
5.09±0.83
13.4±2.87
3.44±1.02
24.8±0.05
0.48±0.01
0.97±0.03
Entropy Reg.
5.02±0.86
12.69±3.42
3.27±0.96
24.74±0.06
0.45±0.01
0.92±0.03
MMCE Reg.
5.56±0.86
13.59±2.65
2.71±0.93
24.7±0.08
0.44±0.01
0.9±0.03
Deep Ensemble
4.84±0.78
12.39±2.52
2.64±0.71
24.69±0.07
0.44±0.01
0.89±0.03"
REFERENCES,0.34299516908212563,"CaPE (bin)
4.73±0.82
11.81±2.54
2.07±0.6
24.56±0.11
0.38±0.01
0.78±0.03
CaPE (kernel)
5.41±0.87
12.71±2.5
2.39±0.78
24.59±0.11
0.4±0.01
0.81±0.03"
REFERENCES,0.3446054750402576,Table 5: Performance on Face-based Risk Prediction. Centered scenario.
REFERENCES,0.3462157809983897,"Skewed
ECE
MCE
KS
Brier
MSEp
KLp
Inﬁnite Data
2.7±0.46
7.64±2.14
1.05±0.39
11.0±0.51
0.22±0.01
0.92±0.03"
REFERENCES,0.34782608695652173,"CE early-stop
3.07±0.57
7.88±1.88
1.28±0.41
11.18±0.5
0.4±0.01
1.79±0.06
Temperature
3.14±0.49
7.92±1.84
1.12±0.33 11.22±0.47
0.4±0.02
1.76±0.06
Platt Scaling
2.99±0.53
7.73±1.59
1.07±0.37
11.1±0.54
0.39±0.01
1.72±0.06
Dirichlet Cal.
3.04±0.73
7.81±2.43
0.97±0.3
11.22±0.42 0.47±0.02
2.31±0.07
Focal Loss
8.29±0.67 14.93±1.43 6.16±0.67 12.01±0.41 1.28±0.03
1.63±0.66
Mix-n-match
2.99±0.53
7.78±1.78
1.08±0.32 11.18±0.49
0.4±0.01
1.75±0.05
Entropy Reg.
7.67±0.57
14.43±1.5
5.2±0.71
11.94±0.45 1.18±0.03 10.74±0.65
MMCE Reg.
3.68±0.59 10.94±2.76 1.47±0.31 11.14±0.44 0.54±0.02
2.44±0.08
Deep Ensemble
2.87±0.5
7.21±1.63
1.36±0.44
11.28±0.5
0.55±0.02
2.58±0.07"
REFERENCES,0.3494363929146538,"CaPE (bin)
3.29±0.5
8.18±1.51
1.17±0.34 11.07±0.47
0.4±0.02
1.73±0.06
CaPE (kernel)
3.16±0.5
8.14±1.58
1.09±0.33 11.17±0.53 0.39±0.01
1.69±0.06"
REFERENCES,0.35104669887278583,Table 6: Performance on Face-based Risk Prediction. Skewed scenario.
REFERENCES,0.3526570048309179,"Discrete
ECE
MCE
KS
Brier
MSEp
KLp
Inﬁnite Data
4.23±0.74
11.16±2.5
1.45±0.49
20.38±0.35
1.52±0.05
3.63±0.12"
REFERENCES,0.35426731078904994,"CE early-stop
6.7±0.86
18.62±3.52
2.61±0.53
21.91±0.36
2.24±0.08
5.27±0.17
Temperature
6.12±0.87
16.82±3.56
3.37±0.86
21.76±0.35
2.21±0.08
5.15±0.18
Platt Scaling
4.7±0.72
11.69±2.44
1.67±0.51
21.44±0.32
2.06±0.08
4.83±0.17
Dirichlet Cal.
7.13±0.86
22.67±5.08
3.18±0.68
22.1±0.34
2.74±0.1
6.53±0.22
Focal Loss
5.7±0.75
13.68±2.32
4.62±0.91
21.77±0.28
2.92±0.09
6.77±0.21
Mix-n-match
6.27±0.76
16.83±2.95
3.47±0.93
21.77±0.33
2.21±0.08
5.14±0.18
Entropy Reg.
6.69±0.87
15.38±2.43
6.03±1.13
21.79±0.31
2.84±0.08
6.62±0.19
MMCE Reg.
3.96±0.7
10.4±2.4
1.51±0.47
21.12±0.35
2.09±0.08
4.92±0.18
Deep Ensemble
4.76±0.74
11.49±2.23
2.04±0.61
21.17±0.31
1.97±0.08
4.61±0.17"
REFERENCES,0.355877616747182,"CaPE (bin)
5.41±0.74
14.45±3.15
2.24±0.59
21.33±0.36
1.81±0.08
4.28±0.18
CaPE (kernel)
4.96±0.8
12.97±2.63
2.18±0.58
21.21±0.42
1.84±0.08
4.35±0.17"
REFERENCES,0.357487922705314,Table 7: Performance on Face-based Risk Prediction. Discrete scenario.
REFERENCES,0.35909822866344604,Under review as a conference paper at ICLR 2022
REFERENCES,0.3607085346215781,"A.2
SUPPLEMENTARY METRICS ON REAL-WORLD DATASET"
REFERENCES,0.36231884057971014,"We present here additional metrics on the real world data: Cancer Survival (Table 8); Climate Fore-
casting (Table 9); Collision Prediction (Table 10)."
REFERENCES,0.3639291465378422,"Methods
(×10−2)
AUC
ECE
MCE
NLL
Brier
KS"
REFERENCES,0.36553945249597425,"CE Early-stop
58.88
12.25
25.35
67.92
23.96
6.44
Temperature
58.88
12.07
24.65
67.11
23.73
6.92
Platt Scaling
58.91
10.28
27.69
66.11
23.33
4.91
Dirichlet Cal.
49.89
13.83
35.52
67.57
24.08
6.00
Mix-n-match
58.88
12.16
24.52
66.89
23.67
7.18
Focal loss
55.02
12.15
26.34
65.92
23.31
6.38
Entropy Reg.
56.29
11.73
30.81
66.49
23.62
6.83
MMCE Reg.
48.45
11.84
37.36
66.83
23.73
3.64
Deep Ensemble
52.26
9.99
28.30
66.22
23.47
5.02"
REFERENCES,0.3671497584541063,"CaPE (bin)
61.44
12.31
25.27
65.75
23.20
2.59
CaPE (kernel)
61.22
9.48
32.40
65.70
23.18
3.70"
REFERENCES,0.3687600644122383,Table 8: Baselines with full metrics for cancer survival
REFERENCES,0.37037037037037035,"Methods
(×10−2)
AUC
ECE
MCE
NLL
Brier
KS"
REFERENCES,0.3719806763285024,"CE Early-stop
77.64
10.91
25.50
59.97
20.57
11.03
Temperature
77.64
8.66
23.56
58.77
20.21
7.41
Platt Scaling
77.65
6.97
16.47
57.38
19.53
3.26
Dirichlet Cal.
77.51
14.29
30.09
62.83
21.89
5.21
Mix-n-match
77.64
8.65
23.58
58.77
20.21
7.39
Focal Loss
76.18
8.32
21.25
59.01
20.27
4.45
Entropy Reg
79.01
10.53
20.72
57.83
19.77
5.00
MMCE Reg
76.69
8.46
19.73
59.25
20.12
7.31
Deep Ensemble
79.86
7.41
18.24
55.28
18.82
7.57"
REFERENCES,0.37359098228663445,"CaPE (bin)
78.99
5.16
15.09
79.00
18.37
2.34
CaPE (kernel)
79.00
5.08
13.28
54.32
18.39
2.34"
REFERENCES,0.3752012882447665,Table 9: Baselines with full metrics for weather prediction
REFERENCES,0.37681159420289856,"Methods
(×10−2)
AUC
ECE
MCE
NLL
Brier
KS"
REFERENCES,0.3784219001610306,"CE Early-stop
85.68
4.36
19.87
31.67
8.59
1.54
Temperature
85.68
4.56
16.79
30.36
8.52
2.9
Platt Scaling
85.76
3.04
12.39
29.42
8.23
1.52
Dirichlet Cal.
83.36
5.78
18.13
30.90
8.77
1.60
Mix-n-match
85.68
4.40
17.41
30.25
8.52
2.60
Focal Loss
82.21
9.07
19.85
34.41
9.82
8.72
Entropy Reg
83.15
14.54
21.27
38.74
11.10
13.44
MMCE Reg.
85.18
2.94
8.95
30.65
8.48
2.44
Deep Ensemble
85.27
3.15
16.53
30.20
8.54
2.01"
REFERENCES,0.38003220611916266,"CaPE (bin)
8.57
3.16
12.21
30.61
8.18
2.13
CaPE (kernel)
85.95
3.22
13.32
30.44
8.13
2.10"
REFERENCES,0.38164251207729466,Table 10: Baselines with full metrics for collision prediction
REFERENCES,0.3832528180354267,Under review as a conference paper at ICLR 2022
REFERENCES,0.38486312399355876,"A.3
ADDITIONAL RELIABILITY DIAGRAM"
REFERENCES,0.3864734299516908,We present here additional reliability curves to the ones illustrated in Figure 6
REFERENCES,0.38808373590982287,"Cancer Survival
Weather Forecasting
Collision Prediction"
REFERENCES,0.3896940418679549,"Temperature
Entropy Reg."
REFERENCES,0.391304347826087,"Platt Scaling
MMCE"
REFERENCES,0.392914653784219,"Dirichlet Cal.
Deep Ensemble"
REFERENCES,0.394524959742351,"Mix-n-match
CaPE(bin)"
REFERENCES,0.3961352657004831,"Focal Loss
CaPE(kernal)"
REFERENCES,0.3977455716586151,CE Early-stop
REFERENCES,0.3993558776167472,"Temperature
Entropy Reg."
REFERENCES,0.40096618357487923,"Platt Scaling
MMCE"
REFERENCES,0.4025764895330113,"Dirichlet Cal.
Deep Ensemble"
REFERENCES,0.40418679549114334,"Mix-n-match
CaPE(bin)"
REFERENCES,0.4057971014492754,"Focal Loss
CaPE(kernal)"
REFERENCES,0.4074074074074074,CE Early-stop
REFERENCES,0.40901771336553944,"Temperature
Entropy Reg."
REFERENCES,0.4106280193236715,"Platt Scaling
MMCE"
REFERENCES,0.41223832528180354,"Dirichlet Cal.
Deep Ensemble"
REFERENCES,0.4138486312399356,"Mix-n-match
CaPE(bin)"
REFERENCES,0.41545893719806765,CaPE(kernal)
REFERENCES,0.4170692431561997,CE Early-stop 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0
REFERENCES,0.41867954911433175,Focal Loss
REFERENCES,0.42028985507246375,"0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.4219001610305958,0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.42351046698872785,"0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.4251207729468599,"0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.42673107890499196,"0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.428341384863124,"0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.42995169082125606,"0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.43156199677938806,"0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.4331723027375201,"0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.43478260869565216,"0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.4363929146537842,0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2
REFERENCES,0.43800322061191627,"0.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2"
REFERENCES,0.4396135265700483,"0.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2"
REFERENCES,0.44122383252818037,"0.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2"
REFERENCES,0.4428341384863124,"0.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2"
REFERENCES,0.4444444444444444,"0.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.8 0.6 0.4 0.2"
REFERENCES,0.44605475040257647,"0.0
0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.4476650563607085,"Figure 7: The reliability diagrams of all the baselines on real-world datasets. We train all base-
line methods on each of the datasets and plot the empirical probability(y-axis) against predicted
probability(x-axis). The axis labels are removed due to space constraints."
REFERENCES,0.4492753623188406,Under review as a conference paper at ICLR 2022
REFERENCES,0.45088566827697263,"We present here additional reliability curves for the different synthetic data scenarios, mentioned in
Figure 4."
REFERENCES,0.4524959742351047,"B
KOLMOGOROV-SMIRNOV ERROR"
REFERENCES,0.45410628019323673,"We derive the KS-error, mentioned in Section 3."
REFERENCES,0.4557165861513688,"For a calibrated estimator
P[y = 1|f(x) ∈I(q)] = q, ∀0 ≤q ≤1,
for some small interval I(q) around q."
REFERENCES,0.4573268921095008,"Hence
P[y = 1, f(x) ∈I(q)] = P[f(x) ∈I(q)]q, ∀0 ≤q ≤1
. Similarly to the Kolmogorov-Smirnov (KS) test for distribution functions, we can recast this
property in integral form"
REFERENCES,0.45893719806763283,"φ1(σ) = σ
Z"
REFERENCES,0.4605475040257649,"0
P[y = 1, f(x) ∈I(q)]dq,
φ2(σ) = σ
Z"
REFERENCES,0.46215780998389694,"0
P[f(x) ∈I(q)]qdq"
REFERENCES,0.463768115942029,"We can evaluate φ1, φ2 from a ﬁnite sample (xi, yi), i = 1 . . . n,"
REFERENCES,0.46537842190016104,"φ1(σ) = 1 n n
X"
REFERENCES,0.4669887278582931,"i=1
1(yi = 1, f(xi) ≤σ),
φ2(σ) = 1 n n
X"
REFERENCES,0.46859903381642515,"i=1
1(f(xi) ≤σ)f(xi)"
REFERENCES,0.47020933977455714,"The KS error is deﬁned as
KS = max
1≤σ≤1 |φ1(σ) −φ2(σ)|"
REFERENCES,0.4718196457326892,"φ1, φ2 can be efﬁciently computed by sorting the data points with respect to their conﬁdence scores
f(xi). The KS error has the advantage of being independent of binning conﬁgurations, unlike ECE
and MCE."
REFERENCES,0.47342995169082125,"C
BRIER SCORE DECOMPOSITION"
REFERENCES,0.4750402576489533,"We present here a decomposition of the Brier score into two components, discussed in Section 3."
REFERENCES,0.47665056360708535,"The Brier score can be interpreted as a sum of two terms, calibration and reﬁnement. Assume the
network can output one of K distinct possible predictions, i.e., ˆp ∈{ˆq1, . . . , ˆqK}."
REFERENCES,0.4782608695652174,"Denote Sk, the set of all inputs with output pk and ¯qk the empirical probability over Sk, i.e.,"
REFERENCES,0.47987117552334946,"Sk = {x|f(x) = ˆqk},
|Sk| = nk,
¯qk = 1 nk X"
REFERENCES,0.48148148148148145,"xi∈Sk
yi"
REFERENCES,0.4830917874396135,Then we can write
REFERENCES,0.48470209339774556,"Brier = 1 N N
X"
REFERENCES,0.4863123993558776,"i=1
(ˆpi −yi)2 = 1 N K
X"
REFERENCES,0.48792270531400966,"k=1
nk(ˆqk −¯qk)2 + 1 N K
X"
REFERENCES,0.4895330112721417,"k=1
nk¯qk(1 −¯qk),"
REFERENCES,0.49114331723027377,"The ﬁrst term on the RHS, calibration, is similar to MSEp, with the empirical probabilities ¯qk sub-
stituting for the true labels. The second term, reﬁnement, is an estimate of the conﬁdence in deter-
mining ¯qk. It is related to the area under curve (AUC), which measures to the achievable accuracy of
the network as a classiﬁer. The term is smaller as the prediction classes fi tend towards 0 or 1. Thus,
this term penalizes empirically calibrated predictors, with low discriminative power, as in Figure 2b."
REFERENCES,0.4927536231884058,Under review as a conference paper at ICLR 2022
REFERENCES,0.4943639291465378,Linear
REFERENCES,0.49597423510466987,"0.0
0.5
1.0
Predicted probability 0.0 0.5 1.0"
REFERENCES,0.4975845410628019,Empirical probability pi pi
REFERENCES,0.499194847020934,Infinite Data
REFERENCES,0.500805152979066,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5024154589371981,1.0 pi pi
PI,0.5040257648953301,Overfitting
PI,0.5056360708534622,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5072463768115942,1.0 pi pi
PI,0.5088566827697263,Early Stopping
PI,0.5104669887278583,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5120772946859904,1.0 pi pi CaPE
PI,0.5136876006441223,Sigmoid
PI,0.5152979066022544,"0.0
0.5
1.0
Predicted probability 0.0 0.5 1.0"
PI,0.5169082125603864,Empirical probability pi pi
PI,0.5185185185185185,Infinite Data
PI,0.5201288244766505,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5217391304347826,1.0 pi pi
PI,0.5233494363929146,Overfitting
PI,0.5249597423510467,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5265700483091788,1.0 pi pi
PI,0.5281803542673108,Early Stopping
PI,0.5297906602254429,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5314009661835749,1.0 pi pi CaPE
PI,0.533011272141707,Centered
PI,0.534621578099839,"0.0
0.5
1.0
Predicted probability 0.0 0.5 1.0"
PI,0.5362318840579711,Empirical probability pi pi
PI,0.537842190016103,Infinite Data
PI,0.5394524959742351,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5410628019323671,1.0 pi pi
PI,0.5426731078904992,Overfitting
PI,0.5442834138486312,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5458937198067633,1.0 pi pi
PI,0.5475040257648953,Early Stopping
PI,0.5491143317230274,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5507246376811594,1.0 pi pi CaPE
PI,0.5523349436392915,Skewed
PI,0.5539452495974235,"0.0
0.5
1.0
Predicted probability 0.0 0.5 1.0"
PI,0.5555555555555556,Empirical probability pi pi
PI,0.5571658615136876,Infinite Data
PI,0.5587761674718197,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5603864734299517,1.0 pi pi
PI,0.5619967793880838,Overfitting
PI,0.5636070853462157,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5652173913043478,1.0 pi pi
PI,0.5668276972624798,Early Stopping
PI,0.5684380032206119,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5700483091787439,1.0 pi pi CaPE
PI,0.571658615136876,Discrete
PI,0.573268921095008,"0.0
0.5
1.0
Predicted probability 0.0 0.5 1.0"
PI,0.5748792270531401,Empirical probability pi pi
PI,0.5764895330112721,Infinite Data
PI,0.5780998389694042,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5797101449275363,1.0 pi pi
PI,0.5813204508856683,Overfitting
PI,0.5829307568438004,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5845410628019324,1.0 pi pi
PI,0.5861513687600645,Early Stopping
PI,0.5877616747181964,"0.0
0.5
1.0
Predicted probability 0.0 0.5"
PI,0.5893719806763285,1.0 pi pi CaPE
PI,0.5909822866344605,"Figure 8: Reliability diagrams for different synthetic data scenarios. We can see that CaPE out-
performs early stopping, prevents overﬁtting, and achieves a performance on par with training on
inﬁnite resampled data."
PI,0.5925925925925926,Under review as a conference paper at ICLR 2022
PI,0.5942028985507246,Linear
PI,0.5958132045088567,"0.01
0.02
0.03
0.04
MSEp 0.55 0.60 0.65 0.70 AUC"
PI,0.5974235104669887,Correlation: -0.750
PI,0.5990338164251208,"0.01
0.02
0.03
0.04
MSEp 0.04 0.08 0.12 ECE"
PI,0.6006441223832528,Correlation: 0.712
PI,0.6022544283413849,"0.01
0.02
0.03
0.04
MSEp 0.12 0.16 0.20 MCE"
PI,0.6038647342995169,Correlation - 0.679
PI,0.605475040257649,"0.01
0.02
0.03
0.04
MSEp 0.04 0.08 0.12"
PI,0.607085346215781,KS-Error
PI,0.6086956521739131,Correlation: 0.652
PI,0.6103059581320451,"0.01
0.02
0.03
0.04
MSEp 0.19 0.20 0.21 0.22 Brier"
PI,0.6119162640901772,Correlation: 0.996
PI,0.6135265700483091,Sigmoid
PI,0.6151368760064412,"0.05
0.06
0.07
0.08
MSEp 0.83 0.84 AUC"
PI,0.6167471819645732,Correlation: -0.710
PI,0.6183574879227053,"0.05
0.06
0.07
0.08
MSEp 0.04 0.08 0.12 ECE"
PI,0.6199677938808373,Correlation: 0.869
PI,0.6215780998389694,"0.05
0.06
0.07
0.08
MSEp 0.12 0.16 0.20 0.24 MCE"
PI,0.6231884057971014,Correlation - 0.667
PI,0.6247987117552335,"0.05
0.06
0.07
0.08
MSEp 0.02 0.04 0.06"
PI,0.6264090177133655,KS-Error
PI,0.6280193236714976,Correlation: 0.528
PI,0.6296296296296297,"0.05
0.06
0.07
0.08
MSEp 0.16 0.17 0.18 0.19 Brier"
PI,0.6312399355877617,Correlation: 0.976
PI,0.6328502415458938,Centered
PI,0.6344605475040258,"0.002
0.004
MSEp 0.52 0.56 AUC"
PI,0.6360708534621579,Correlation: -0.769
PI,0.6376811594202898,"0.002
0.004
MSEp 0.05 0.06 ECE"
PI,0.6392914653784219,Correlation: 0.692
PI,0.6409017713365539,"0.002
0.004
MSEp 0.12 0.14 0.16 MCE"
PI,0.642512077294686,Correlation - 0.570
PI,0.644122383252818,"0.002
0.004
MSEp 0.02 0.03 0.04"
PI,0.6457326892109501,KS-Error
PI,0.6473429951690821,Correlation: 0.495
PI,0.6489533011272142,"0.002
0.004
MSEp 0.242 0.244 0.246 0.248 Brier"
PI,0.6505636070853462,Correlation: 0.979
PI,0.6521739130434783,Skewed
PI,0.6537842190016103,"0.005
0.010
MSEp 0.56 0.60 0.64 AUC"
PI,0.6553945249597424,Correlation: -0.973
PI,0.6570048309178744,"0.005
0.010
MSEp 0.04 0.06 0.08 ECE"
PI,0.6586151368760065,Correlation: 0.990
PI,0.6602254428341385,"0.005
0.010
MSEp 0.08 0.10 0.12 0.14 MCE"
PI,0.6618357487922706,Correlation - 0.968
PI,0.6634460547504025,"0.005
0.010
MSEp 0.02 0.04 0.06"
PI,0.6650563607085346,KS-Error
PI,0.6666666666666666,Correlation: 0.979
PI,0.6682769726247987,"0.005
0.010
MSEp 0.112 0.116 0.120 Brier"
PI,0.6698872785829307,Correlation: 0.982
PI,0.6714975845410628,Discrete 0.02 MSEp 0.55 0.60 0.65 AUC
PI,0.6731078904991948,Correlation: -0.669 0.02 MSEp 0.04 0.06 ECE
PI,0.6747181964573269,Correlation: 0.673 0.02 MSEp 0.12 0.16 0.20 MCE
PI,0.6763285024154589,Correlation - 0.503 0.02 MSEp 0.04
PI,0.677938808373591,KS-Error
PI,0.679549114331723,Correlation: 0.827 0.02 MSEp 0.21 0.22 Brier
PI,0.6811594202898551,Correlation: 0.794
PI,0.6827697262479872,"CE Early-stop
Infinite Data"
PI,0.6843800322061192,"Temperature
Platt Scaling"
PI,0.6859903381642513,"Dirichlet Cal.
Mix-n-Match"
PI,0.6876006441223832,"Focal Loss
MMCE Reg."
PI,0.6892109500805152,"Entropy Reg.
CaPE (bin)"
PI,0.6908212560386473,CaPE (kernel)
PI,0.6924315619967794,"Figure 9: The comparison between MSEp and other metrics on synthetic data. Brier score presents
the most consistent correlation with MSEp."
PI,0.6940418679549114,"D
METRIC COMPARISON"
PI,0.6956521739130435,"We present here the correlation between different calibration and accuracy metrics and metrics that
have access to the ground truth probabilities, MSEp and KL-divergence, eveluated over all ﬁve
scenarios in our Face-based Risk Prediction synthetic dataset, referred to in Section 3."
PI,0.6972624798711755,"E
ESTIMATION OF EMPIRICAL PROBABILITY IN CAPE"
PI,0.6988727858293076,"We describe in further detail the two ways to estimate the conditional probability P[y = 1|f(x) ∈
I(q)], introduced in Section 4."
PI,0.7004830917874396,Under review as a conference paper at ICLR 2022
PI,0.7020933977455717,Linear
PI,0.7037037037037037,"0.04
0.06
0.08
0.10
KLp 0.55 0.60 0.65 0.70 AUC"
PI,0.7053140096618358,Correlation: -0.737
PI,0.7069243156199678,"0.04
0.06
0.08
0.10
KLp 0.04 0.08 0.12 ECE"
PI,0.7085346215780999,Correlation: 0.731
PI,0.7101449275362319,"0.04
0.06
0.08
0.10
KLp 0.12 0.16 0.20 MCE"
PI,0.711755233494364,Correlation - 0.694
PI,0.7133655394524959,"0.04
0.06
0.08
0.10
KLp 0.04 0.08 0.12"
PI,0.714975845410628,KS-Error
PI,0.71658615136876,Correlation: 0.677
PI,0.7181964573268921,"0.04
0.06
0.08
0.10
KLp 0.19 0.20 0.21 0.22 Brier"
PI,0.7198067632850241,Correlation: 0.995
PI,0.7214170692431562,Sigmoid
PI,0.7230273752012882,"0.16
0.20
0.24
KLp 0.83 0.84 AUC"
PI,0.7246376811594203,Correlation: -0.658
PI,0.7262479871175523,"0.16
0.20
0.24
KLp 0.04 0.08 0.12 ECE"
PI,0.7278582930756844,Correlation: 0.883
PI,0.7294685990338164,"0.16
0.20
0.24
KLp 0.12 0.16 0.20 0.24 MCE"
PI,0.7310789049919485,Correlation - 0.673
PI,0.7326892109500805,"0.16
0.20
0.24
KLp 0.02 0.04 0.06"
PI,0.7342995169082126,KS-Error
PI,0.7359098228663447,Correlation: 0.557
PI,0.7375201288244766,"0.16
0.20
0.24
KLp 0.16 0.17 0.18 0.19 Brier"
PI,0.7391304347826086,Correlation: 0.963
PI,0.7407407407407407,Centered
PI,0.7423510466988728,"0.004
0.006
0.008
0.010
KLp 0.51 0.54 0.57 AUC"
PI,0.7439613526570048,Correlation: -0.771
PI,0.7455716586151369,"0.004
0.006
0.008
0.010
KLp 0.05 0.06 ECE"
PI,0.7471819645732689,Correlation: 0.693
PI,0.748792270531401,"0.004
0.006
0.008
0.010
KLp 0.12 0.13 0.14 0.15 0.16 MCE"
PI,0.750402576489533,Correlation - 0.569
PI,0.7520128824476651,"0.004
0.006
0.008
0.010
KLp 0.02 0.03 0.04"
PI,0.7536231884057971,KS-Error
PI,0.7552334943639292,Correlation: 0.491
PI,0.7568438003220612,"0.004
0.006
0.008
0.010
KLp 0.242 0.244 0.246 0.248 Brier"
PI,0.7584541062801933,Correlation: 0.979
PI,0.7600644122383253,Skewed
PI,0.7616747181964574,0.02 0.04 0.06 0.08 0.10 0.12 KLp 0.54 0.57 0.60 0.63 AUC
PI,0.7632850241545893,Correlation: -0.962
PI,0.7648953301127214,0.02 0.04 0.06 0.08 0.10 0.12 KLp
PI,0.7665056360708534,"0.03
0.04
0.05
0.06
0.07
0.08 ECE"
PI,0.7681159420289855,Correlation: 0.997
PI,0.7697262479871175,0.02 0.04 0.06 0.08 0.10 0.12 KLp 0.08 0.10 0.12 0.14 MCE
PI,0.7713365539452496,Correlation - 0.958
PI,0.7729468599033816,0.02 0.04 0.06 0.08 0.10 0.12 KLp 0.01 0.02 0.03 0.04 0.05 0.06
PI,0.7745571658615137,KS-Error
PI,0.7761674718196457,Correlation: 0.993
PI,0.7777777777777778,0.02 0.04 0.06 0.08 0.10 0.12 KLp 0.112 0.116 0.120 Brier
PI,0.7793880837359098,Correlation: 0.988
PI,0.7809983896940419,Discrete
PI,0.782608695652174,"0.04
0.05
0.06
KLp 0.52 0.56 0.60 0.64 0.68 AUC"
PI,0.784219001610306,Correlation: -0.696
PI,0.785829307568438,"0.04
0.05
0.06
KLp 0.04 0.05 0.06 0.07 ECE"
PI,0.7874396135265701,Correlation: 0.681
PI,0.789049919484702,"0.04
0.05
0.06
KLp 0.12 0.16 0.20 MCE"
PI,0.7906602254428341,Correlation - 0.525
PI,0.7922705314009661,"0.04
0.05
0.06
KLp 0.02 0.03 0.04 0.05 0.06"
PI,0.7938808373590982,KS-Error
PI,0.7954911433172303,Correlation: 0.814
PI,0.7971014492753623,"0.04
0.05
0.06
KLp 0.204 0.208 0.212 0.216 0.220 Brier"
PI,0.7987117552334944,Correlation: 0.794
PI,0.8003220611916264,"CE Early-stop
Infinite Data"
PI,0.8019323671497585,"Temperature
Platt Scaling"
PI,0.8035426731078905,"Dirichlet Cal.
Mix-n-Match"
PI,0.8051529790660226,"Focal Loss
MMCE Reg."
PI,0.8067632850241546,"Entropy Reg.
CaPE (bin)"
PI,0.8083735909822867,CaPE (kernel)
PI,0.8099838969404187,"Figure 9: Comparison between KLp and other metrics on synthetic data. Brier score presents the
most consistent correlation with KLp."
PI,0.8115942028985508,"We wish to estimate the conditional probability of an output y given a network prediction f(x),
P[y = 1|f(x) ∈I(q)] We can approximate the probability by averaging over points ˆp ∈I(q),"
PI,0.8132045088566827,"P[y = 1|f(x) ∈I(q)] ≈
1
|I(q)| X"
PI,0.8148148148148148,"ˆp∈I(q)
P[y = 1|f(x) = p]
(7)"
PI,0.8164251207729468,An empirical estimate of P[y = 1|f(x) ∈I(q)] would be
PI,0.8180354267310789,"P[y = 1|f(x) ∈I(q)] ≈
1
|Index(I(q))| X"
PI,0.8196457326892109,"f(xi)∈I(q)
yi,
(8)"
PI,0.821256038647343,where Index(Iq) = {i|f(xi) ∈I(q)}.
PI,0.822866344605475,Under review as a conference paper at ICLR 2022
PI,0.8244766505636071,"Alternatively, we can use kernel estimation:"
PI,0.8260869565217391,P[y = 1|f(x) ∈I(q)] ≈1 Z X
PI,0.8276972624798712,"ˆp∈I(q)
P[y = 1|f(x) = ˆp] · exp "
PI,0.8293075684380032,"−(p −q)2 σ2 ! ,
(9)"
PI,0.8309178743961353,where Z = P
PI,0.8325281803542673,"p∈I(q) exp

−(p−q)2"
PI,0.8341384863123994,"σ2

is the normalization factor. An empirical estimate of the
conditional probability would then be"
PI,0.8357487922705314,P[y = 1|f(x) ∈I(q)] ≈1 Z X
PI,0.8373590982286635,"f(xi)∈I(q)
yi exp "
PI,0.8389694041867954,−(f(xi) −q)2 σ2 !
PI,0.8405797101449275,".
(10)"
PI,0.8421900161030595,"Based on these two approximation methods, we can design an algorithm to estimate pi
emp."
PI,0.8438003220611916,"Bin We divide our data into B bins of equal size. Q1, . . . , QB are the data B-quantiles. We wish
to estimate P[y = 1|f(x) ∈[Qb−1, Qb]], b = 1, . . . , B, Q0 = 0. Denote Ib := [Qb−1, Qb] ∩
{f(xi)}N
i=1, set of all predictions in [Qb−1, Qb], and Index(Ib) = {i|f(xi) ∈Ib}. We have,"
PI,0.8454106280193237,"P[y = 1|f(x) ∈[Qb−1, Qb]] ≈p(b)
emp =
1
|Ib| X"
PI,0.8470209339774557,"i∈Index(Ib)
yi"
PI,0.8486312399355878,"We assign p(b)
emp to all data points i in the b-th quantile"
PI,0.8502415458937198,"pi
emp = p(b)
emp
∀i ∈Index(Ib)"
PI,0.8518518518518519,Kernel In this case we use kernel estimation:
PI,0.8534621578099839,"pi
emp = P"
PI,0.855072463768116,"k∈NN(i,r) K (i, k) yk
P"
PI,0.856682769726248,"k∈NN(i,r) K (i, k) .
(11)"
PI,0.8582930756843801,"NN(i, r) deﬁnes r data points whose predictions are nearest to ˆpi = f(xi). K(i, j) is the Gaussian
kernel"
PI,0.8599033816425121,"K(i, j) = exp "
PI,0.8615136876006442,"−(ˆpi −ˆpj)2 σ2 ! ,"
PI,0.8631239935587761,with hyperparameter σ.
PI,0.8647342995169082,"F
CALIBRATION BASELINES"
PI,0.8663446054750402,"This section includes a review of the baseline methods, discussed in Section 6"
PI,0.8679549114331723,"Post-processing
Postprocessing for calibration requires ﬁnding a function f : [0, 1] →[0, 1],
that augments the output of a the neural network ˆpi →f(ˆpi) in order to achieve better calibration
properties"
PI,0.8695652173913043,"• Platt scaling (Platt, 1999) optimizes f on validation set within the following family,"
PI,0.8711755233494364,"f1(ˆpi) = σ
 
W T ˆpi + b

(12)"
PI,0.8727858293075684,"where W ∈R2, b ∈R and σ is the Sigmoid function. The non-probabilistic predictions
of a classiﬁer are used as features for a logistic regression model, which is trained on the
validation set to return probabilities.
• Temperature scaling (Guo et al., 2017) is a single parameter variant of Platt Scaling where
we only change the temperature of the softmax to obtain the calibrated probabilities.
f(ˆpi) = Softmax (ˆpi/T)
(13)
where T ∈R minimizes the negative log-likelihood of validation set.
• Beta/Dirchlet calibration (Dir-ODIR) (Kull et al., 2017; 2019) assumes that the probabili-
ties can be parametrized by a Beta/Dirchlet distribution i.e."
PI,0.8743961352657005,"fj ∼Beta(α(j), β(j))
(14)
Assume the prior to be p(y = j) = πj, πj ∈[0, 1], we have P(y|fj) ∝πjfj, and then
α(j), β(j) are estimated by maximizing the posterior."
PI,0.8760064412238325,Under review as a conference paper at ICLR 2022
PI,0.8776167471819646,"Ensembling
These calibration methods simultaneously train several neural networks from end to
end, varying parameters in the training process. The ﬁnal output is some function of all the different
outputs."
PI,0.8792270531400966,"• Mix-n-Match (Zhang et al., 2020) improves calibration by ensembling parametric and non-
parametric calibrators. Denote the temperature scaling function with g(ˆyi, T). Then Mix-
n-Match ensembles different temperatures
fj(ˆpi) = w1gj(ˆpi, T) + w2gj(ˆpi, 0) + w3gj(ˆpi, ∞)
(15)
After ensembling the parametric temperature scaling, Mix-n-Match applies non-parametric
isotonic regression.
• Deep ensemble (Lakshminarayanan et al., 2017) trains M copies of the neural network
with different initialization. The probability estimation is the average of all single model
estimations"
PI,0.8808373590982287,"p(yi | xi) = 1 M M
X"
PI,0.8824476650563607,"j
pθj(yi | xi)
(16)"
PI,0.8840579710144928,"Modiﬁed training
These calibration methods train the neural networks from end to end, modify-
ing the training process to improve calibration."
PI,0.8856682769726248,"• Conﬁdence penalty (Pereyra et al., 2017) Penalizeslow entropy output distributions (conﬁ-
dence penalty). Label smoothing improve state-of-the-art models across benchmarks."
PI,0.8872785829307569,"L(θ) = −
X"
PI,0.8888888888888888,"i
log pθ(yi|xi) −βH(pθ(yi|xi))
(17)"
PI,0.8904991948470209,"• Focal loss (Mukhoti et al., 2020) maximizes entropy while minimizing the KL divergence
between the predicted and the target distributions. It also regularizes the weights of the
model to avoid overﬁtting."
PI,0.8921095008051529,"L(θ) = −
X"
PI,0.893719806763285,"i
(1 −pθ(yi|xi))γ log pθ(yi|xi),
γ ∈R.
(18)"
PI,0.895330112721417,"• Kernel MMCE (Kumar et al., 2018) is a reproducing kernel Hilbert space (RKHS) kernel
based measure of calibration that is efﬁciently trainable, alongside the negative likelihood
loss. Given data samples D = {(ci, ri)}m
i=0, where ci = χ{ˆyi=yi} and ri = P(ci = 1|ˆyi),
MMCE is computed on samples D as following,"
PI,0.8969404186795491,"MMCE2(D) =
X i,j"
PI,0.8985507246376812,"(ci −ri)(cj −rj)k(ri, rj)"
PI,0.9001610305958132,"m2
(19)"
PI,0.9017713365539453,"where k(ri, rj) is a kernel function. MMCE is optimized together with the cross entropy
loss as a regularization term. The strength of calibration can be adjusted by a scale λ ∈R."
PI,0.9033816425120773,"L(θ) = −
X"
PI,0.9049919484702094,"i
log pθ(yi|xi) + λ
 
MMCE2(D)
 1"
PI,0.9066022544283414,"2
(20)"
PI,0.9082125603864735,"G
SYNTHETIC DATA EXPERIMENTS"
PI,0.9098228663446055,We use ResNet-18 model for all our experiments with synthetic data.
PI,0.9114331723027376,"The synthetic data is split into training, validation, and test sets with 16641, 4738, and 2329 samples,
respectively. The training and validation sets contain only images xi and 0-1 labels yi for training
and tuning the model. In order to evaluate the performance of the model for probability estimation,
the held-out test set contains the ground truth probabilities pi, in addition to xi and yi. Note that we
do not use the ground-truth probability labels pi values during training or inference - we only use
them to compare the performance of different models."
PI,0.9130434782608695,"Ground Truth Probability Generation
The ground truth probability associated with example i
is simulated by pi = ψ(zi) where zi is age of the person."
PI,0.9146537842190016,Under review as a conference paper at ICLR 2022
PI,0.9162640901771336,"0
25
50
75
100
z 0.0 0.2 0.4 0.6 0.8 1.0 p"
PI,0.9178743961352657,Linear
PI,0.9194847020933977,"0
25
50
75
100
z 0.0 0.2 0.4 0.6 0.8 1.0 p"
PI,0.9210950080515298,Sigmoid
PI,0.9227053140096618,"0
25
50
75
100
z 0.0 0.2 0.4 0.6 0.8 1.0 p"
PI,0.9243156199677939,Centered
PI,0.9259259259259259,"0
25
50
75
100
z 0.0 0.2 0.4 0.6 0.8 1.0 p"
PI,0.927536231884058,Skewed
PI,0.92914653784219,"0
25
50
75
100
z 0.0 0.2 0.4 0.6 0.8 1.0 p"
PI,0.9307568438003221,Discrete
PI,0.9323671497584541,Figure 10: Illustration of the function ψ(z) used to generate the different synthetic-data scenarios.
PI,0.9339774557165862,"Label distribution
After determining the probability pi using ψ(z), the label yi is sampled from
a Bernoulli distribution parametrized by pi, so that it takes the value 1 with probability pi. The
distributions of yi under ﬁve different scenarios are illustrated in Fig.11."
PI,0.9355877616747182,"0
50
100
Age 0 200 400 600 800 1000 Count"
PI,0.9371980676328503,"Linear 1
0"
PI,0.9388083735909822,"0
50
100
Age 0 200 400 600 800 1000 Count"
PI,0.9404186795491143,"Sigmoid 1
0"
PI,0.9420289855072463,"0
50
100
Age 0 200 400 600 800 Count"
PI,0.9436392914653784,"Centered 1
0"
PI,0.9452495974235104,"0
50
100
Age 0 250 500 750 1000 1250 Count"
PI,0.9468599033816425,"Skewed 1
0"
PI,0.9484702093397746,"0
50
100
Age 0 200 400 600 800 1000 Count"
PI,0.9500805152979066,"Discrete 1
0"
PI,0.9516908212560387,Figure 11: Histograms of the outcomes (yi) for the different synthetic-data scenarios.
PI,0.9533011272141707,Under review as a conference paper at ICLR 2022
PI,0.9549114331723028,"H
REAL-WORLD DATA AND EXPERIMENT DETAILS"
PI,0.9565217391304348,We present here supplementary information for the real-world datasets used in our experiments.
PI,0.9581320450885669,"Cancer Survival
Histopathological features are useful in identiﬁcation of tumor cells, cancer sub-
types, and the stage and level of differentiation of the cancer. Hematoxylin and Eosin (H&E)-stained
slides are the most common type of histopathology data and the basis for decision making in the clin-
ics. With these properties, H&E are used for mortality prediction of cancer (Wulczyn et al., 2020).
In this experiment, we use the H&E slides of non-small cell lung cancers from The Cancer Genome
Atlas Program (TCGA)4 to predict the 5-year survival. The dataset has 1512 whole slide images
from 1009 patients, and 352 of them died in 5-years. We split the samples by patients and source
institutions into training, validation, and test set, which has 1203, 151, and 158 samples respectively."
PI,0.9597423510466989,"The whole slide images contain numerous pixels, so we cropped the slides into tiles at 20x mag-
niﬁcation with 1/4 overlapping, resized them to 299 × 299 with bicubic interpolation, and ﬁltered
out the tiles with more than 85% area covered by the background. The representations of each tile
are trained with self-supervised momentum contrastive learning (MoCo) (Chen et al., 2020), and
the slide-level prediction is obtained from a multiple-instance learning network (Ilse et al., 2018)
trained with the binary label of survival in 5 years."
PI,0.961352657004831,"Weather Forecasting
We use the German Weather service dataset5, which contains quality-
controlled rainfall-depth composites from 17 operational Doppler radars. Three precipitation maps
from the past 30 minutes serve as an input. The training labels are the 0/1 events indicating whether
the mean precipitation increases (1) or not (0)."
PI,0.9629629629629629,"The German Weather service (DWD - Deutshce Wetter Dienst) dataset https://opendata.
dwd.de/weather/radar/ contains quality-controlled rainfall-depth composites from 17 op-
erational DWD Doppler radars. It has a spatial extent of 900x900 km, and covers the entirety of
Germany. Data exists since 2006, with a spatial and temporal resolution of 1x1 km and 5 minutes,
respectively. The dataset has been used to train RainNet, a pricipitation nowcasting model (Ayzel,
2020)."
PI,0.964573268921095,"The network architecture is ResNet18, with 3 input channels and 2 output channels. The input to
the network are 3 precipitation maps which cover a ﬁxed area of 300km×300 km in the center of
the grid (300× 300 pixels), set 10 minutes apart. The training, validation and test datasets consist of
20000, 6000 and 3000 samples, respectively, all separated temporally, over the span of 15 years."
PI,0.966183574879227,"Collision Prediction
Vehicle collision is one of the leading causes of death in the world. Reliable
collision prediction systems which can warn drivers about potential collisions can save a signiﬁcant
number of lives. A standard way to design such a system is to train a convolutional model for
identifying if a particular vehicle in the dash-cam video feed might collide with the car in next few
seconds. More formally, at time t = T the system tries to predict if any car in the video might
collide with our given car in time t ∈[T, T + Tlook-ahead]. Each labelled training sample consists of
features X = (XT −δ, XT −2δ, . . . , XT −dδ) and a binary label Y ∈{0, 1} denoting if an accident
will occur in t ∈[T, T + Tlook-ahead]. Each Xt is a tensor with 4 channels where the ﬁrst 3 channels
corresponds to an RGB image of the dashcam view at time t = t, and the fourth channel consists of
a mask with a bounding box on a particular vehicle of interest. In this work, we use YouTubeCrash
dataset (Kim et al., 2019) to train and test our model, which uses δ = 0.1s,Tlook-head = 18δ = 1.8s,
and d = 3. Following Kim et al. (2019) we used a VGG-16 network architecture."
PI,0.9677938808373591,"The dataset contains 122 accident scenes, and 2096 non-accident scenes, which after feature extrac-
tion gives us 2096 positive samples, and 11486 negative samples (the dataset is severely imbalanced,
and similar to the Skewed situation in Section 6.1). We further split the dataset into train (6453 sam-
ples for label 0, and 1023 samples for label 1), validation (2348 samples for label 0, and 545 samples
for label 1), and test (2685 samples for label 0, and 528 samples for label 1) sets. The samples in
train, validation and test sets are generated from disjoint scenes/dashcam videos."
PI,0.9694041867954911,"4https://www.cancer.gov/tcga
5https://opendata.dwd.de/weather/radar/"
PI,0.9710144927536232,Under review as a conference paper at ICLR 2022
PI,0.9726247987117552,"I
ANALYSIS OF CANCER SURVIVAL RESULTS"
PI,0.9742351046698873,"I
II
III/IV
Pathological stage 0.2 0.4 0.6 0.8 1.0"
PI,0.9758454106280193,Estimated Probability
PI,0.9774557165861514,"CE Early Stop
DeepEns
CaPE (kernel)"
PI,0.9790660225442834,"Figure 12: Estimated probability of survival grouped by pathological stages. The plot shows median,
samples between 25th to 75th percentile in the box, samples between 0th and 100th percentile on
the line, and the outliers as dots. Deep ensemble produces similar probability estimates for patients
across all the stages; CE is more discriminative but has a very large variance; CaPE achieves a
trade-off between the two baselines."
PI,0.9806763285024155,"For cancer survival prediction, we visualize the estimated probabilities on the test set in different
pathological stages in Figure 12. In general, patients in earlier stages should have higher prob-
abilities of survival. Deep ensemble produces similar probability estimates for all stages (i.e the
model is less discriminative). Cross-entropy minimization (CE) is more discriminative, but has very
wide conﬁdence intervals. CaPE is more discriminative than deep ensemble, while having narrower
conﬁdence intervals than CE."
PI,0.9822866344605475,"J
CALIBRATING FROM THE BEGINNING"
PI,0.9838969404186796,"CaPE exploits a calibration-based cost function to improve its probability estimates without over-
ﬁtting. The empirical probabilities in this loss are computed from the model itself. Consequently,
applying this strategy from the beginning of training can be counterproductive, because the model
predictions are essentially random. This is demonstrated in the following table, which compares
CaPE with a model trained using the calibration loss from the beginning (in the same way as CaPE,
alternating with cross-entropy minimization)."
PI,0.9855072463768116,"Methods
Linear
Sigmoid
Centered
Skewed
Discrete
(×10−2)
MSEp
KLp
MSEp
KLp
MSEp
KLp
MSEp
KLp
MSEp
KLp"
PI,0.9871175523349437,"Bin (start)
2.59
6.81
8.07
22.10
0.48
0.98
0.51
2.37
2.74
6.36
Kernel (start)
2.23
5.68
7.60
21.15
0.54
1.10
0.68
2.84
2.40
5.63"
PI,0.9887278582930756,"Bin (CaPE)
1.83
4.46
5.29
14.59
0.38
0.78
0.40
1.72
1.83
4.31
Kernel (CaPE)
1.81
4.41
5.22
14.47
0.40
0.81
0.39
1.70
1.85
4.36"
PI,0.9903381642512077,"Table 11: Comparison between CaPE and a model that uses the calibration loss from the beginning
(in the same way as CaPE, alternating with cross-entropy minimization) on synthetic data."
PI,0.9919484702093397,Under review as a conference paper at ICLR 2022
PI,0.9935587761674718,"K
COMPARISON OF REAL-WORLD DATASETS WITH DIFFERENT SCENARIOS
OF THE SIMULATED DATASET"
PI,0.9951690821256038,"Figure 13 illustrates the similarity between the empirical probability curves of different real-world
datasets and the different scenarios of our synthetic dataset. For the cancer survival dataset, the
empirical probabilities are clustered in the center (0.4-0.6) similar to the Centered scenario. For the
weather forecasting dataset, the probabilities are uniformly distributed across 0.1-0.8 similar to the
Linear scenario. For the collision prediction dataset, the majority of the data points are clustered in
the lower probability region which makes it similar to the Skewed scenario."
PI,0.9967793880837359,Under review as a conference paper at ICLR 2022
PI,0.998389694041868,"Figure 13: Comparison of reliability diagrams for real-world data with different scenarios of
simulated data. For the cancer survival dataset, the empirical probabilities are clustered in around
(0.4-0.6), similar to the Centered scenario. For the weather forecasting dataset, the probabilities
are uniformly distributed across 0.1-0.8, similar to the Linear scenario. For the collision prediction
dataset, the majority of the output probabilities are clustered in the lower probability region, similar
to the Skewed scenario."
