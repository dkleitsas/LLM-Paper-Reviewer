Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002544529262086514,"Designing a suitable representation for code-reasoning tasks is challenging in
aspects such as the kinds of program information to model, how to combine them,
and how much context to consider. We propose CodeTrek, a deep learning approach
that addresses these challenges by representing codebases as databases that conform
to rich relational schemas. The relational representation not only allows CodeTrek
to uniformly represent diverse kinds of program information, but also to leverage
program-analysis queries to derive new semantic relations, which can be readily
incorporated without further architectural engineering. CodeTrek embeds this
relational representation using a set of walks that can traverse different relations
in an unconstrained fashion, and incorporates all relevant attributes along the way.
We evaluate CodeTrek on four diverse and challenging Python tasks: variable
misuse, exception prediction, unused deﬁnition, and variable shadowing. CodeTrek
achieves an accuracy of 91%, 63%, 98%, and 94% on these tasks respectively, and
outperforms state-of-the-art neural models by 2-19% points."
INTRODUCTION,0.005089058524173028,"1
INTRODUCTION"
INTRODUCTION,0.007633587786259542,"Deep learning techniques are increasingly applied to code-reasoning tasks, including bug detection
(Allamanis et al., 2018), type inference (Hellendoorn et al., 2018), code summarization (Alon et al.,
2019b), program repair (Dinella et al., 2020), and code generation (Alon et al., 2020), among many
others. The successful application of these techniques to a given task depends heavily on the program
representation that encompasses relevant program features and model architecture."
INTRODUCTION,0.010178117048346057,"There are many crucial choices involved in designing a suitable representation for a new task.
Consider the example in Figure 1 which depicts an instance of an exception-prediction task (Kanade
et al., 2020), whose goal is to predict the exception type in the placeholder “[??]” in the highlighted
statement. Predicting the correct type AssertionError from a range of 20 pre-deﬁned exception types
in Python requires understanding the implementation of the check() function, which is deﬁned in a
different class (TestObject). Thus, the desirable context for the model goes beyond the immediate
lexical neighborhood of the placeholder, possibly inside a chain of called functions."
INTRODUCTION,0.01272264631043257,"The richness of information and extended scope inevitably imply that the relevant context may be very
large. Models tackling such tasks are challenged to either reduce the scope of a task—e.g., a single
function, or a few contiguous lines of code text—or heuristically sample from larger scope to produce
a small enough input to ﬁt inside the memory of a GPU. For example, Transformers learn to reason
about code from a sequence of tokens in the program; and GNNs with n layers—a hyper-parameter
which, for message-passing architectures, determines how much of the graph is reachable from some
immediately adjacent context to the task example—prune all nodes that are further than n graph hops
away from the placeholder. However, distance is not always the determining factor in collecting
relevant information. For instance, when trying to decide which exception type is applicable, it may
be more important to follow control ﬂow edges until they meet the raising of an exception than, say,
fetching all adjacent statements without exception handling."
INTRODUCTION,0.015267175572519083,"We outline two key design goals motivated by these considerations. First, we observe that pro-
gramming languages have well-deﬁned semantics, which ensures that relevant information (module
imports such as pickle, class inheritance, inter-procedural control ﬂow and data ﬂow, deeper analyses"
INTRODUCTION,0.017811704834605598,Published as a conference paper at ICLR 2022
INTRODUCTION,0.020356234096692113,"ID
Kind
CID
s1 except
c1
s2
assert
c2
s4
assign
c1 stmt"
INTRODUCTION,0.022900763358778626,"ID
Scope
c1 test.py:130-138
c2 test.py:5-10"
INTRODUCTION,0.02544529262086514,"scope
Base Relations"
INTRODUCTION,0.027989821882951654,Derived Relations
INTRODUCTION,0.030534351145038167,"SID
Var
s4
o def"
INTRODUCTION,0.03307888040712468,project
INTRODUCTION,0.035623409669211195,module
INTRODUCTION,0.03816793893129771,dependency
INTRODUCTION,0.04071246819338423,Program Analysis Queries
INTRODUCTION,0.043256997455470736,(derived relations)
INTRODUCTION,0.04580152671755725,btorrent
INTRODUCTION,0.04834605597964377,pickle 
INTRODUCTION,0.05089058524173028,"class SyncTestCase:
  ...
  def call_chk():
    o = TestObject()
    try:
      ...
      o.check()
    except [??]:     
      errs.append(...) ..."
INTRODUCTION,0.05343511450381679,"1:
 2:
 5: 100:"
INTRODUCTION,0.05597964376590331,defr.py
INTRODUCTION,0.058524173027989825,log.py
INTRODUCTION,0.061068702290076333,test.py ... ...
INTRODUCTION,0.06361323155216285,"TABLE            (
        ID         VARCHAR(5)     PRIMARY KEY,
        Scope  VARCHAR(128)  NOT NULL
   )"
INTRODUCTION,0.06615776081424936,"TABLE            (
        ID         VARCHAR(5)    PRIMARY KEY,
        Kind     VARCHAR(20)   NOT NULL,
        CID      VARCHAR(5)     NOT NULL,
        FOREIGN KEY (CID) REFERENCES scope(ID)
   )"
INTRODUCTION,0.06870229007633588,"VIEW          [ID, Caller, Callee] AS  ..."
INTRODUCTION,0.07124681933842239,Semmle Compiler
INTRODUCTION,0.0737913486005089,"VIEW          [SID, Var] AS
       SELECT s, v
        FROM stmt s, var v
        WHERE exists(expr e | e.defines(v)
             and s.getSubEx() = e"
INTRODUCTION,0.07633587786259542,"130:
131:
132:"
INTRODUCTION,0.07888040712468193,"136:
137:
138:"
INTRODUCTION,0.08142493638676845,"import pickle
class TestObject:
  def check(self):
    ...
    assert self.z == 0
 10:"
INTRODUCTION,0.08396946564885496,Schema of Python Programs
INTRODUCTION,0.08651399491094147,"(base relations)
Input Python Program stmt"
INTRODUCTION,0.089058524173028,"scope
Relational Representation def call"
INTRODUCTION,0.0916030534351145,"ID
Caller Callee
a1
f2
f1 call ..."
INTRODUCTION,0.09414758269720101,"Figure 1: Example showing how CODETREK translates an exception-prediction sample in a Python
program into a feature-rich representation that consists of base relations that capture the program’s
syntax and derived relations that capture semantic information computed by program analysis queries."
INTRODUCTION,0.09669211195928754,"like def/use chains and object escape, etc.) can be extracted via a number of deterministic analyses.
So, instead of learning this kind of information indirectly from labeled data, we can make it directly
available to the model. The model can thus focus on learning the information that can only be
discovered from rich contexts. Second, even when rich information is easily accessible, making
well-informed predictions in code-reasoning tasks requires intelligent context collection to ﬁt the
needs of the task. So, instead of solely considering the model’s technical constraints, we must capture
relevant context in a task-speciﬁc manner."
INTRODUCTION,0.09923664122137404,"In this paper, we propose CODETREK, a deep learning approach that realizes these goals. CODETREK
leverages a declarative program analysis framework to produce a rich, easily extensible representation
of context as a relational database, and a biased graph-walk mechanism for pruning that context in
a task-speciﬁc way before presenting it to a model based on Transformers and DeepSets (Zaheer
et al., 2017). CODETREK builds upon Semmle (Avgustinov et al., 2016), which converts codebases
in C, Java, Python, etc., into relational databases that capture the underlying structure and semantics
of code, as well as a query language, CodeQL, for specifying program analyses to compute new
semantic information. CODETREK brings little modelling innovation—the architecture is reminiscent
of neural techniques for knowledge-graph reasoning (Das et al., 2018; Perozzi et al., 2014). However,
it is useful both as a relation generator and a task generator. As the former, it can harness existing
and new analyses to add more inductive bias for hard tasks. As the latter, it can help benchmark
neural techniques on more challenging tasks, and generate auxiliary training objectives to pre-train
unsupervised code-understanding models (Feng et al., 2020; Guo et al., 2020; Kanade et al., 2020)."
INTRODUCTION,0.10178117048346055,"We evaluate CODETREK on four diverse tasks on real-world Python programs. They include two
existing tasks, variable misuse and exception prediction, as well as two newer ones, unused deﬁnition
and variable shadowing. The newer tasks are sophisticated CodeQL queries, written by program
analysis experts, and enable testing the power of neural models: they both involve complex logical
reasoning, and only 1.6% of the unused deﬁnition samples contain bugs, which is more in line with
real-world settings. CODETREK achieves an accuracy of 91%, 63%, 98%, and 94% on these tasks
respectively, which is 2-19% points higher than state-of-the-art neural models CuBERT, GREAT,
GGNN, and Code2Seq. We also demonstrate the robustness of CODETREK in two out-of-distribution
scenarios: real-world variable misuse samples from GitHub and unused deﬁnition samples involving
subtle code perturbations introduced using a systematic test-generation framework, Skeletal Program
Enumeration (Zhang et al., 2017). CODETREK achieves an accuracy of 57% and ROC-AUC of 78%,
respectively in these scenarios, which is 6–11% points and 14–36% points higher than the baselines."
INTRODUCTION,0.10432569974554708,"In summary, this paper makes the following contributions:"
WE PROPOSE TO REPRESENT PROGRAMS AS RELATIONAL DATABASES THAT MAKE RICH CONTEXT READILY AVAILABLE,0.10687022900763359,"1. We propose to represent programs as relational databases that make rich context readily available
for code-reasoning tasks using deep learning.
2. We present a graph-walk mechanism that prunes the unrelated context in a task-speciﬁc manner.
3. We propose techniques to enable task designers to easily tailor and stress-test their models via
program analysis queries, walk speciﬁcations, and systematic test-program generation."
WE PROPOSE TO REPRESENT PROGRAMS AS RELATIONAL DATABASES THAT MAKE RICH CONTEXT READILY AVAILABLE,0.10941475826972011,Published as a conference paper at ICLR 2022
WE PROPOSE TO REPRESENT PROGRAMS AS RELATIONAL DATABASES THAT MAKE RICH CONTEXT READILY AVAILABLE,0.11195928753180662,"4. We identify two new challenging tasks for neural code reasoning, unused deﬁnition and variable
shadowing; although sophisticated, non-neural static-analysis tools can solve them, these tasks
pose a useful litmus test for neural code-reasoning frameworks and demonstrate the ability of
CODETREK to generate hard tasks that follow real-world program distributions with modest effort.
5. We extensively evaluate our approach and demonstrate that deeper relational information about
code helps neural models outperform the state-of-the-art in terms of accuracy and robustness."
WE PROPOSE TO REPRESENT PROGRAMS AS RELATIONAL DATABASES THAT MAKE RICH CONTEXT READILY AVAILABLE,0.11450381679389313,CODETREK is publicly available at https://github.com/ppashakhanloo/CodeTrek.
THE CODETREK FRAMEWORK,0.11704834605597965,"2
THE CODETREK FRAMEWORK"
BACKGROUND,0.11959287531806616,"2.1
BACKGROUND"
BACKGROUND,0.12213740458015267,"Inspired by the idea of storing codebases as databases, CODETREK represents a program as a
relational database. Speciﬁcally, CODETREK leverages the per-language schema deﬁned by Semmle
to uniformly store lexical, syntactic, and semantic program information as base relations in the
database—we focus on Python in this paper, but the approach is language-agnostic, as long as
Semmle supports the language. Each relation contains information—in the form of tuples—about
a particular kind of program element, such as expressions, statements, and so on. The columns of
a relation specify its attributes. For instance, in Figure 1, tuple (s1, except, c1) in the stmt relation
speciﬁes that s1 is an except statement contained in a scope with identiﬁer c1, and tuple (s4, o) in the
def relation speciﬁes that variable o is deﬁned in some statement with identiﬁer s4. The schema also
deﬁnes referential integrity constraints of the form R.A →S.B where A is called a foreign key of
referencing relation R, and B is a unique attribute (e.g. a primary key) of referenced relation S. For
example, in Figure 1, we have stmt.CID →scope.ID."
BACKGROUND,0.12468193384223919,"2.2
A BIRD’S-EYE VIEW OF CODETREK"
BACKGROUND,0.1272264631043257,"Facilitated by CODETREK’s uniform representation of programs, task developers can easily obtain
new semantic information by writing program-analysis queries in CodeQL, an SQL-like language.
The newly derived information is also in the form of derived relations, which maintains the uniformity
of the relational representation. The derived information is stored in def, which, together with call,
can bias the prediction of the best variable to replace a placeholder. A task developer need not be a
machine-learning expert to bring in more semantic information about programs: All they need do is
write a CodeQL query, and the resulting derived information will be added to the existing richness of
the program’s available features in CODETREK."
BACKGROUND,0.1297709923664122,scope_stmt
BACKGROUND,0.13231552162849872,call.caller_func.id
BACKGROUND,0.13486005089058525,func_scope
BACKGROUND,0.13740458015267176,call.callee_func.id
BACKGROUND,0.13994910941475827,"func_scope
scope_stmt"
BACKGROUND,0.14249363867684478,expr_stmt func
BACKGROUND,0.1450381679389313,"id: f1
name: check
scope: c2 scope . . . . . . . . ."
BACKGROUND,0.1475826972010178,"id: c2
scope:
test.py:5-10
stmt"
BACKGROUND,0.15012722646310434,"id: s3
kind: try
scope: c1 scope"
BACKGROUND,0.15267175572519084,"scope_stmt
expr"
BACKGROUND,0.15521628498727735,"id: e1
kind: call
scope: c1 call"
BACKGROUND,0.15776081424936386,"id: a1
caller: f2
callee: f1
stmt"
BACKGROUND,0.16030534351145037,"id: s2
kind: assert
scope: c2
stmt"
BACKGROUND,0.1628498727735369,"id: s1
kind: except
scope: c1"
BACKGROUND,0.16539440203562342,"id: c1
scope:
test.py:130-138 func"
BACKGROUND,0.16793893129770993,"id: f2
 name: call_chk
 scope: c1"
BACKGROUND,0.17048346055979643,Figure 2: A partial illustration of a graph generated by CODETREK.
BACKGROUND,0.17302798982188294,"CODETREK translates a relational database to a graph whose nodes correspond to tuples, and whose
edges follow referential integrity constraints. An example of such a graph is illustrated in Figure 2
where each node is depicted as a circle along with its type (e.g., func) in white font. Orange and green
nodes correspond to tuples of base and derived relations, respectively. The attributes (e.g., name, kind,
etc.) of the node are shown in a box at the corner of the node. For each referential integrity constraint
R.A →S.B, an edge type R.A S.B is deﬁned, connecting the tuples of the two relations with the same
value on the edge attributes R.A and S.B. For brevity of presentation in Figure 2, when there is a single
such constraint between a pair of relations R and S, we omit the attributes from the edge type. But
we do not omit them when there are multiple such constraints, such as in the case between relations
func and call, namely, call.caller →func.id and call.callee →func.id. This graph view of program
semantics helps extract succinct context as input to a model. Context extraction from the resulting
CODETREK graph is done via biased random walks of the graph, in a fashion speciﬁed by the task
deﬁnition. The starting node—which we call an anchor node—may be example-speciﬁc (e.g., the
node containing the placeholder) or task-speciﬁc (e.g., all nodes holding a variable declaration). In
Figure 2, the node that represents tuple stmt(s1, except, c1)—which corresponds to the statement on"
BACKGROUND,0.17557251908396945,Published as a conference paper at ICLR 2022
BACKGROUND,0.178117048346056,"stmt
scope
func
call
...
except
test.py
call_chk
...
scope_stmt func_scope
call_func
call_func
..."
BACKGROUND,0.1806615776081425,"node types
node values
edge types"
BACKGROUND,0.183206106870229,subtokenize
BACKGROUND,0.18575063613231552,"except test
py
call
chk
...
summation
& embed using Ev"
BACKGROUND,0.18829516539440203,"node vocabulary (En)
edge vocabulary (Ee)"
BACKGROUND,0.19083969465648856,Pooling
BACKGROUND,0.19338422391857507,positional
BACKGROUND,0.19592875318066158,encoding (1) (2) (3)
BACKGROUND,0.1984732824427481,Transformer Encoder
BACKGROUND,0.2010178117048346,concat
BACKGROUND,0.2035623409669211,positional
BACKGROUND,0.20610687022900764,encoding
BACKGROUND,0.20865139949109415,positional
BACKGROUND,0.21119592875318066,encoding
BACKGROUND,0.21374045801526717,"(N)
(N)
(N - 1)"
BACKGROUND,0.21628498727735368,(3N - 1)
BACKGROUND,0.21882951653944022,Figure 3: Embedding of the walk highlighted in Figure 2.
BACKGROUND,0.22137404580152673,"line 137 in Figure 1 (left)—is the anchor because the goal of the task is to predict a suitable exception
type in the except statement. The walk generator traverses the graph by biasing traversal of edges
according to each neighbor’s node type. If no bias is speciﬁed, walks are simply fair random walks.
Different probability mixes for different node types encourage the model to sample walks more
relevant to a task. An example of such a walk is shown in Figure 2 using circles with thicker borders.
This walk reaches the “assert” node which in fact determines the exception type that should be used
in the except statement. For instance, to spend more time traversing longer-range dependencies in
other functions, the developer can assign a higher value to call nodes. In our evaluation, we assign
higher probabilities to nodes of types stmt and expr. We could achieve improved accuracy for the
tasks compared to baselines by only modifying the probabilities of up to 4 types of nodes. Learning
the walk speciﬁcation given the task without human input is exciting future work."
BACKGROUND,0.22391857506361323,"Finally, to convert random walks to a distributed representation, CODETREK embeds each walk
(including the types and attributes of each node and edge in the walk) using a Transformer encoder,
and then produces an order-invariant representation of the set of walks using the Deep Set architec-
ture (Zaheer et al., 2017). The resulting hidden representation can then be used by to make predictions
for the particular code-reasoning task."
BACKGROUND,0.22646310432569974,"Random walks to embed a graph are perhaps a regressive choice, compared to more modern solutions
such as GNNs or relational Transformers. For instance, DeepWalk (Perozzi et al., 2014) uses random
walks for distributed-representation learning in a transductive setting, and Code2Seq (Alon et al.,
2019a) uses shortest paths between pairs of AST leaf nodes. We chose biased random walks for
several reasons. First, this enables CODETREK to choose task-speciﬁc strategies to heuristically
fetch relevant context for a task, rather than choosing to embed all tokens of a function or class in
lexical order. Second, in contrast to GNNs, this enables CODETREK to potentially follow much
longer chains through the semantic graph than what would be possible in a message-passing GNN
of a tractable number of layers—this was, in fact, the motivation behind the model architectures by
Hellendoorn et al. (2020). Finally, in contrast to other walk-based approaches such as Code2Seq and
AnyCodeGen, which share some of our motivation, our graph structure has much richer connectivity
and is larger. For instance, those approaches only consider paths ending at token-bearing leaf nodes,
with only AST interior nodes in between, whereas CODETREK admits arbitrary paths through the
graph. In our example, paths with more than two token nodes, e.g., the walk illustrated in Figure 2
using circles with thicker borders is admissible for CODETREK but not for Code2Seq. CODETREK
builds upon the above techniques and extends to relational graphs, with different sampling strategies
and neural architectures that would suit database representation better."
BUILDING BLOCKS OF CODETREK,0.22900763358778625,"2.3
BUILDING BLOCKS OF CODETREK
We now describe brieﬂy the building blocks of CODETREK. Pre-processing consists of two steps:
(a) turning the codebase into a relational database (Code2Rel), using a pre-existing set of analyses
and the developer’s own analyses, and (b) mapping the relational database to a graph (Rel2Graph).
Then, a graph is processed into sets of graph walks to present to a model. Finally, we train using a
cross-entropy loss to implement a particular task."
BUILDING BLOCKS OF CODETREK,0.23155216284987276,"Codebases as Relational Databases.
Code2Rel applies to the codebase the system’s base program
analysis queries, which make up the base relations, as well as those provided by the developer, which"
BUILDING BLOCKS OF CODETREK,0.2340966921119593,Published as a conference paper at ICLR 2022
BUILDING BLOCKS OF CODETREK,0.2366412213740458,"form the derived relations. The result is a database comprising of a number of named tuples for each
relation type. For Python, we collect 95 base relations and 277 derived relations, although adding
more derived relations is simply an exercise in writing a few lines of CodeQL. Optionally, this step
can limit the resulting database to only those relations that are reachable (through query-to-query
dependencies) from some program analyses marked as required by the developer. Appendix D.1
provides further details."
BUILDING BLOCKS OF CODETREK,0.23918575063613232,"Constructing Relational Graphs from Databases.
Rel2Graph interprets the relations produced
by Code2Rel as a graph, as follows. Each named tuple is represented by a node with the values of the
tuple attributes as its features. Edges are added between these nodes as described in subsection 2.2
such that the edge type R.A S.B is deﬁned for each referential integrity constraint R.A →S.B between
nodes representing tuples of relations R and S."
BUILDING BLOCKS OF CODETREK,0.24173027989821882,"Representing Code as a Set of Walks.
Given a code database that is converted to graph G via
the above Rel2Graph, we propose to represent it by the embedding of a set of walks W, via the
procedure Graph2Walks. Graph2Walks projects a code graph as produced by Rel2Graph to a set of
walks, according to the task-speciﬁc walk speciﬁcation (anchor node predicate, traversal bias, and
target walk length). Graph2Walks samples from the distribution of such random walks, by repeatedly
picking a node satisfying the anchor predicate, and traversing up to a maximum number of neighbors,
following the transition probabilities speciﬁed. The resulting walks are collected as token sequences
of relation types of nodes, their attribute values, and the edge types traversed, in the order of traversal.
Figure 3 row (1) shows such a walk representation corresponding to the walk highlighted in Figure 2.
Appendix D.3 speciﬁes Graph2Walks precisely."
BUILDING BLOCKS OF CODETREK,0.24427480916030533,"Embedding the Set of Sampled Walks.
Given a walk w = [n0, e0, n1, e1, . . . , nN−1] of length
N steps consisting of N nodes and N −1 edges, we produce an initial embedding Xw ∈R(3N−1)×d,
where d is the embedding dimension. It consists of three segments. The ﬁrst N rows of the embedding
tensor represents the N node types (relation names), using an embedding lookup in En ∈RR×d,
where R is the number of relations. The next N rows represent the attribute values of the N nodes;
we subtokenize attribute values (using a V -sized WordPiece vocabulary for attribute values), embed
each subtoken using Ev ∈RV ×d, and mean-pool the subtoken embeddings into each node’s attribute
embedding. The last segment represents the N −1 edge types (recall that an edge type is a tuple of two
relation names and primary-key/foreign-key attributes), using an embedding lookup in Ee ∈RI×d,
where I is the number of referential-integrity constraints in the database. All three embedding
matrices En, Ev, Ee are learnable parameters. Each individual part of the embedding tensor gets its
own sinusoidal positional encoding (denoted as PEw ∈R(3N−1)×d). We use a Transformer encoder to
represent the embedding of walk w as ew = pooling

Transformer(Xw+PEw)

: R(3N−1)×d 7→Rd,
where after the last layer of Transformer we do mean-pooling over all 3N −1 elements of the walk,
to obtain a d-dimensional ew. The steps for embedding of a walk sampled from the graph in Figure 2
is illustrated in Figure 3."
BUILDING BLOCKS OF CODETREK,0.24681933842239187,"Training and Inference.
An example consists of a set of walks and a ground-truth label, (W, ˆy).
Given an unordered set of walk embeddings {ew}w∈W , we build a classiﬁer by using the construction
y = MLP
 
DeepSet({ew}w∈W )

, where y denotes the predicted label and we optimize for cross
entropy loss. However, for a binary classiﬁcation task, we can obtain a more interpretable model via
y = P"
BUILDING BLOCKS OF CODETREK,0.24936386768447838,"w∈W αwσ(MLP(ew)), where σ(·) is the sigmoid function, and αw =
exp MLP(ew)
P w′∈W exp MLP(ew′).
This way, we can inspect the individual walks that contributed the most (i.e., the highest αw) to the
positive or negative predictions, and see if that aligns with human reasoning. We refer to αw as
the walk score. We train using the Adam optimizer with 8 GPUs for distributed synchronized SGD
training (see Appendix A for details)."
EVALUATION,0.25190839694656486,"3
EVALUATION"
EVALUATION,0.2544529262086514,"Tasks.
We consider two main criteria in selecting tasks. The ﬁrst is locality, which is determined
by whether reasoning within a function typically sufﬁces, or whether inter-procedural reasoning is
required. The second is declarativity—whether the task can be stated as a logic problem that can
be solved using declarative queries. For declarative tasks, we write queries in CodeQL (detailed in
Appendix C); for non-declarative tasks, we rely on available datasets. We treat the following tasks:"
EVALUATION,0.25699745547073793,"1. VARMISUSE. Given a function and a variable accessed in it, predict whether the variable is
misused. We also consider a variation of this task, VARMISUSE-FUN (Kanade et al., 2020), that"
EVALUATION,0.2595419847328244,Published as a conference paper at ICLR 2022
EVALUATION,0.26208651399491095,"Task
CODETREK
GGNN
Code2Seq
GREAT
CuBERT
VARMISUSE
0.91 ± 0.003
0.69 ± 0.004
–
0.82 ± 0.002
0.89 ± 0.003
VARMISUSE-FUN
0.70 ± 0.004
0.54 ± 0.004
0.52 ± 0.005
0.89 ± 0.003
0.84 ± 0.003
EXCEPTION
0.63 ± 0.003
0.28 ± 0.02
0.30 ± 0.01
0.44 ± 0.008
0.42 ± 0.008
EXCEPTION-FUN
0.65 ± 0.01
0.51 ± 0.02
0.51 ± 0.008
0.68 ± 0.007
0.69 ± 0.007
DEFUSE ∗
0.98 ± 0.002
0.76 ± 0.07
–
0.84 ± 0.05
0.76 ± 0.01
DEFUSE-FUN ∗
0.91 ± 0.005
0.77 ± 0.07
0.66 ± 0.01
0.82 ± 0.007
0.71 ± 0.01
VARSHADOW
0.94 ± 0.007
0.71 ± 0.01
0.70 ± 0.01
0.93 ± 0.008
0.91 ± 0.008"
EVALUATION,0.26463104325699743,"Table 1: Accuracy results of CODETREK. Rows that are marked by ∗are measured by ROC-AUC,
and the rest are measured by accuracy. The best performance in each row is denoted in boldface."
EVALUATION,0.26717557251908397,"takes only a function and predicts whether all variables are used correctly in the function. Note that
neither variation is declarative: given a well-formed program, no logic query can deterministically
decide that a variable is misused, since that decision depends on the intended semantics.
2. EXCEPTION. Given a module containing a masked exception type in an except clause, predict
the most appropriate built-in exception type out of 20 choices. We also consider a variation of
this task, EXCEPTION-FUN (Kanade et al., 2020), that is similar to EXCEPTION but takes a single
function as scope. Although EXCEPTION needs inter-procedural reasoning, neither variation is
declarative, since the choice of the appropriate exception type is subjective in Python.
3. DEFUSE. Given a function and a variable deﬁnition in its scope, predict whether the deﬁnition is
used. We also consider a variation of this task, DEFUSE-FUN, that takes a function as its input
and predicts whether any deﬁnitions are unused. This task is especially interesting because the
real-world distribution of programs that contain unused deﬁnitions is skewed. Both variations are
declarative (see Appendix C.1) and require only intra-procedural reasoning.
4. VARSHADOW. Given a module, predict whether any variable deﬁned within a certain scope has
the same name as a variable deﬁned in an enclosing outer scope, thereby shadowing that latter
variable. Similar to EXCEPTION, this task requires inter-procedural analysis in order to reason
over both local as well as global variables. It has a declarative query (see Appendix C.2)."
EVALUATION,0.2697201017811705,"Benchmark.
We use the ETH Py150 Open corpus consisting of 125K Python modules1. It is a
de-duplicated and redistributable subset of ETH Py1502. Speciﬁcally, for the non-declarative tasks,
we use the datasets released by Kanade et al. (2020). Since these are function-level samples but the
EXCEPTION task is module-level, we augment the function in each sample with the entire containing
module for this task. For the declarative tasks, we use analyses written in CodeQL to annotate all
functions (or modules, as applicable) in ETH Py150 Open. All datasets consist of real examples,
except for VARMISUSE-FUN and VARMISUSE where variable misuses are synthetically introduced
into real code. We collected a number of apparent variable misuses from GitHub commits to test our
models and baselines on a realistic dataset. Dataset details are available in Appendix H."
EVALUATION,0.272264631043257,"Baselines.
To compare CODETREK’s performance with state-of-the-art techniques, we select four
baselines: we implement GGNN by Allamanis et al. (2018) and Code2Seq by Alon et al. (2019a),
build classiﬁers on top of the GREAT encoder by Hellendoorn et al. (2020), and ﬁne-tune the
pre-trained Python model for CuBERT by Kanade et al. (2020), which is essentially the Transformer-
based classiﬁer implementation of BERT. For Code2Seq we use ASTs as base program structures
as described by Alon et al. (2019a). We sample leaf-to-leaf paths from these ASTs. The number of
paths we sample is the same as the number of walks we sample for training CODETREK models.
For GGNN and GREAT, we compute the data ﬂow, control ﬂow, and lexical information described
by Allamanis et al. (2018) and Hellendoorn et al. (2020), respectively, using Semmle CodeQL and
augment program ASTs with those edges. We detail baseline hyperparameters in Appendix A, and
ﬁne-grained information about the size of AST-based versus CODETREK graphs in Appendix G.1."
ACCURACY OF CODETREK,0.2748091603053435,"3.1
ACCURACY OF CODETREK"
ACCURACY OF CODETREK,0.27735368956743,"We evaluate the performance of CODETREK and the baseline techniques on all the tasks described in
section 3, all presented as classiﬁcation tasks. We perform 10-fold cross-validation and report the
average of the metric that we use to measure the performance of each task. We use ROC-AUC as the
metric for DEFUSE and DEFUSE-FUN tasks due to their unbalanced datasets, and accuracy for the"
ACCURACY OF CODETREK,0.27989821882951654,"1https://github.com/google-research-datasets/eth_py150_open
2https://www.sri.inf.ethz.ch/py150"
ACCURACY OF CODETREK,0.2824427480916031,Published as a conference paper at ICLR 2022
ACCURACY OF CODETREK,0.28498727735368956,"Task
CODETREK
GGNN
Code2Seq
GREAT
CuBERT
VARMISUSE-REAL
0.57
0.51
0.50
0.49
0.46
DEFUSE-SPE ∗
0.78
0.53
0.63
0.41
0.47"
ACCURACY OF CODETREK,0.2875318066157761,Table 2: Robustness results of CODETREK.
ACCURACY OF CODETREK,0.2900763358778626,"remaining tasks. The results are reported in Table 1. In 5 out of 7 tasks, CODETREK outperforms
GGNN, Code2Seq, GREAT, and CuBERT by 2–19% points."
ACCURACY OF CODETREK,0.2926208651399491,"There are various reasons why CODETREK performs better than these approaches. First, declarative
tasks such as DEFUSE-FUN (or DEFUSE) require complex reasoning about the interactions between
program variables. For instance, one needs to reason about the uses and deﬁnitions of variables in
a ﬂow-sensitive manner to determine whether any unused deﬁnitions exist in a program. Consider
the code snippet in Figure 4. The deﬁnition of the variable month on line 2 is unused but that at
line 3 is used in line 4. CODETREK gives a majority of the walks sampled using the deﬁnition
at line 3 a high score (around 0.99), indicating the existence of a use of that deﬁnition. However,
most of the walks sampled from the deﬁnition of month at line 2 were given a lower score, and so
CODETREK determines that this deﬁnition is unused. We observe that both CuBERT and GREAT
fail to distinguish between the deﬁnitions on lines 2 and 3, and so they predict both to be used."
ACCURACY OF CODETREK,0.2951653944020356,"1 def get_month(self, t):"
ACCURACY OF CODETREK,0.29770992366412213,"2
month, _, _ = t"
ACCURACY OF CODETREK,0.30025445292620867,"3
def validate(month):"
ACCURACY OF CODETREK,0.30279898218829515,"4
return is_valid(month)"
ACCURACY OF CODETREK,0.3053435114503817,"5
return self.month"
ACCURACY OF CODETREK,0.30788804071246817,Figure 4: Example DEFUSE-FUN task.
ACCURACY OF CODETREK,0.3104325699745547,"Additionally, some tasks such as EXCEPTION require rea-
soning beyond the boundaries of a single function to make
informed predictions. Functions in the chain of function
calls can be lexically far from each other, thus rendering
the limited context size of CuBERT and GREAT insufﬁ-
cient. We observe that GGNN fails in the presence of such
long call chains on par with ﬁndings of Alon & Yahav
(2021). CODETREK addresses this kind of mispredictions by readily using a call graph relation to
connect the chain of function calls. This enables CODETREK to traverse a long distance without the
need to consider other statements in the program that have no effect in raising some exception."
ACCURACY OF CODETREK,0.31297709923664124,"However, CODETREK performs worse than CuBERT in EXCEPTION-FUN. This could be attributed
to the fact that CuBERT is pre-trained on around 7 million Python programs, and therefore is able
to memorize tokens from several instances of try-except blocks. An example of a heuristic that it
learns is that in presence of tokens such as request or response in the context, it suggests catching
HTTPError, which is usually the correct choice. However, its prediction is not robust against changes
in the variable names. For instance, changing the names of a few nearby variables to request or
response forces CuBERT to predict HTTPError regardless of the semantics. CODETREK on the other
hand, does not rely on memorizing the tokens, but learns to assign high probabilities to walks that
correctly traverse a chain of function calls starting from the try blocks to locations in programs (or
their libraries) where the exception is originally raised."
ACCURACY OF CODETREK,0.3155216284987277,"CODETREK also performs worse than GREAT in VARMISUSE-FUN. This is because every node that
corresponds to a variable is selected to be an anchor for this task. The total number of walks (500
in this task) is divided among these variables. However, there can be hundreds of variables in some
programs, resulting in few walks generated for each variable in such cases, diminishing the ability of
CODETREK to learn sufﬁcient information about each variable."
"ROBUSTNESS OF CODETREK
WE EVALUATE THE ROBUSTNESS OF CODETREK ON ADDITIONAL TEST DATA THAT DOES NOT FOLLOW THE DISTRIBUTION",0.31806615776081426,"3.2
ROBUSTNESS OF CODETREK
We evaluate the robustness of CODETREK on additional test data that does not follow the distribution
of the training data. This data includes two new datasets: one representing real-world bugs for
the VARMISUSE-FUN task and the other consisting of programs mutated using a systematic test-
generation framework for the DEFUSE-FUN task. The results are reported in Table 2.
Real-world bugs.
We manually collect 199 real-world instances containing a VARMISUSE-FUN
bug and their corrected counterparts (a total of 398 samples) from commits on GitHub and use them
as testing data for the VARMISUSE-FUN task. We deﬁne a VARMISUSE-FUN bug as the occurrence
of a misused variable that is changed to another in-scope variable in the commit. We evaluate the
baselines using this real-world set of bugs. CODETREK outperforms baselines in detecting real-world
variable misuse bugs (VARMISUSE-REAL) by achieving an accuracy of 57% which is 6% points
better than the second best result obtained by GGNN."
"ROBUSTNESS OF CODETREK
WE EVALUATE THE ROBUSTNESS OF CODETREK ON ADDITIONAL TEST DATA THAT DOES NOT FOLLOW THE DISTRIBUTION",0.32061068702290074,"Mutated programs.
There are several approaches to mutating existing datasets, including trans-
forming existing data (Yang et al. (1992)), generating synthetic programs, and fuzzing. A representa-"
"ROBUSTNESS OF CODETREK
WE EVALUATE THE ROBUSTNESS OF CODETREK ON ADDITIONAL TEST DATA THAT DOES NOT FOLLOW THE DISTRIBUTION",0.3231552162849873,Published as a conference paper at ICLR 2022
"ROBUSTNESS OF CODETREK
WE EVALUATE THE ROBUSTNESS OF CODETREK ON ADDITIONAL TEST DATA THAT DOES NOT FOLLOW THE DISTRIBUTION",0.3256997455470738,"tive approach that has been used to systematically evaluate the robustness of compilers is Skeletal
Program Enumeration (SPE), proposed by Zhang et al. (2017). SPE parameterizes each program
by a set of its variables, and replaces each variable name exhaustively with other in-scope variable
names. We generate variations of the DEFUSE-FUN testing data using this technique, and evaluate
the baselines on this mutated dataset (DEFUSE-SPE). CODETREK outperforms all baselines in
classifying these perturbed programs by achieving the ROC-AUC score of 78% which is 15% points
better than the second best result obtained by Code2Seq."
"ROBUSTNESS OF CODETREK
WE EVALUATE THE ROBUSTNESS OF CODETREK ON ADDITIONAL TEST DATA THAT DOES NOT FOLLOW THE DISTRIBUTION",0.3282442748091603,"The poor performance of the baselines can be explained by the fact that the code generated by
SPE is out-of-distribution. For example, the assignment a = a + a is unusual in real code, but
occurs frequently in SPE-generated samples. Despite this, the inductive bias borne by rich relational
information during training remains applicable and prevails over the unusual-looking token sequences,
thus explaining CODETREK’s performance."
"ROBUSTNESS OF CODETREK
WE EVALUATE THE ROBUSTNESS OF CODETREK ON ADDITIONAL TEST DATA THAT DOES NOT FOLLOW THE DISTRIBUTION",0.33078880407124683,"These results suggest that sampling walks can be a promising strategy for robustness. Interestingly,
the runner-up in this study is Code2Seq—another walk-based approach. We inspected the paths in
both approaches to understand the reason behind the difference in performance of Code2Seq and
CODETREK despite their similarities. We identiﬁed two reasons: 1) the kinds of program information
that can be captured from an AST are limited compared to the program graph we propose, and 2)
several walks that CODETREK prioritizes for this task cannot be embedded by Code2Seq."
EFFECTIVENESS ON LONGER-RANGE TASKS,0.3333333333333333,"3.3
EFFECTIVENESS ON LONGER-RANGE TASKS"
EFFECTIVENESS ON LONGER-RANGE TASKS,0.33587786259541985,"We evaluate the effectiveness of CODETREK on tasks that require reasoning beyond function bound-
aries. CODETREK achieves this ability by readily incorporating relations that capture inter-procedural
or inter-modular dependencies such as call graphs. To demonstrate this, we compare CODETREK’s
performance on EXCEPTION with vs. without incorporating the call graph information at training
time. CODETREK achieves an accuracy of 52% when call graph information is not provided, which
increases to 63% after providing the call graph information between functions within a module."
EFFECTIVENESS ON LONGER-RANGE TASKS,0.3384223918575064,"1 class ZipFile:
2
def __init__(...):
3
self.__check_compression(...)
4
def __check_compression(...):
5
raise NotImplementedError
6 # ...2000 lines of code...
7 class TestZipFile:
8
def test(path):
9
try:
10
zf = ZipFile(path)
11
except [??]:
12
log.warning()"
EFFECTIVENESS ON LONGER-RANGE TASKS,0.34096692111959287,Figure 5: Example EXCEPTION task.
EFFECTIVENESS ON LONGER-RANGE TASKS,0.3435114503816794,"To illustrate the kinds of mistakes that the baselines (and
also CODETREK in the absence of call graph information)
make, consider the representative example in Figure 5,
snipped and simpliﬁed from the zipﬁle package. In this
example, the model predicts the exception type that should
be caught on line 11. However, to make an informed pre-
diction, the model must consider the exceptions that may
be raised when calling the ZipFile constructor (line 10).
Hence, the deﬁnition of the constructor (line 2) must be
taken into consideration. This constructor calls another
function
check compression in which a NotImplement-
edError is raised on line 5. This chain of function dependence can be easily represented using a call
graph. Therefore, CODETREK, once provided with call graph, will eventually traverse the path that
reaches this raise statement from the exception statement through the call graph edges."
SENSITIVITY TO NUMBER OF SAMPLED WALKS,0.3460559796437659,"3.4
SENSITIVITY TO NUMBER OF SAMPLED WALKS"
SENSITIVITY TO NUMBER OF SAMPLED WALKS,0.3486005089058524,"We evaluate the sensitivity of CODETREK at test time to the number of walks that are sampled
from program graphs. All the models for the considered tasks are trained on 100 sampled walks
per program. The results are reported in Figure 6. We observe that the accuracies of the models
increase with the number of sampled walks. In some tasks, such as DEFUSE and VARMISUSE that
involve local reasoning about one point in the program, reducing the number of walks from 100 to
50 reduces the accuracies of the models by a very small amount. On the other hand, for tasks that
require reasoning about numerous points in the program (e.g., DEFUSE-FUN) or reasoning globally
(e.g., EXCEPTION) decreasing the number of sampled walks has a bigger impact on the accuracy."
IMPACT OF PROGRAM REPRESENTATION,0.3511450381679389,"3.5
IMPACT OF PROGRAM REPRESENTATION
To evaluate the impact of different code representations, we train two models for each task using
CODETREK’s architecture: for one set of models, the walks are sampled from relational graphs
whereas for the other set of models, the walks are sampled from ASTs. The performance results are
reported in Table 3. Notably, the models trained on walks sampled from relational graphs are about
3–35% points more accurate than models trained on walks sampled from ASTs."
IMPACT OF PROGRAM REPRESENTATION,0.35368956743002544,Published as a conference paper at ICLR 2022
IMPACT OF PROGRAM REPRESENTATION,0.356234096692112,"25
50
75
100
Number of Sampled Walks 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
IMPACT OF PROGRAM REPRESENTATION,0.35877862595419846,Performance
IMPACT OF PROGRAM REPRESENTATION,0.361323155216285,"VarMisuse
Exception
ExceptionFun
DefUse
DefUseFun
VarShadow"
IMPACT OF PROGRAM REPRESENTATION,0.3638676844783715,Figure 6: Sensitivity to the number of walks.
IMPACT OF PROGRAM REPRESENTATION,0.366412213740458,"Task
Relational
AST
VARMISUSE
0.91
0.63
VARMISUSE-FUN
0.70
0.55
EXCEPTION
0.63
0.37
EXCEPTION-FUN
0.65
0.62
DEFUSE ∗
0.98
0.63
DEFUSE-FUN ∗
0.91
0.67
VARSHADOW
0.94
0.73"
IMPACT OF PROGRAM REPRESENTATION,0.36895674300254455,"Table 3: Impact of Program Representa-
tion on Accuracy. (Rows marked by ∗are
evaluated by ROC-AUC score.)"
IMPACT OF PROGRAM REPRESENTATION,0.37150127226463103,"We also evaluate the usefulness of the ability to bias random walks in CODETREK. The accuracy
results that are reported in Table 1 are all trained on biased random walks. Speciﬁcally, in all of the
tasks, nodes with types stmt, expr, and variable are biased such that they are 5 times more likely to
be traversed compared to other kinds of neighboring nodes. In addition, in the EXCEPTION task,
we decrease the bias assigned to nodes of type module to 0 to avoid traveling from one function to
another through the module node they have in common. (See Appendix E for more details about
speciﬁcation of the tasks.) This forces the walks to only go to other functions by taking call graph
edges between them. We select one of the tasks, EXCEPTION-FUN, to measure the accuracy in the
absence of said biases. We re-train EXCEPTION-FUN using uniformly sampled walks and observe
that the accuracy reduces from 65% to 58% as a result."
RELATED WORK,0.37404580152671757,"4
RELATED WORK"
RELATED WORK,0.37659033078880405,"Learning to represent code. There is a rich literature on using neural networks for code reasoning.
At the token sequence level, the Transformer and its variants (Hellendoorn et al., 2020; Dowdell &
Zhang, 2020) have been widely used (Berabi et al., 2021; Ahmad et al., 2020; Z¨ugner et al., 2021; Kim
et al., 2021; Wang et al., 2020). Their performance can be further boosted via pretraining (Feng et al.,
2020; Guo et al., 2020; Kanade et al., 2020; Wang et al., 2021; Peng et al., 2021; Liu et al., 2020).
Others have proposed to represent programs with ASTs and additional semantic edges (Allamanis
et al., 2018; Brockschmidt et al., 2018) or learned abstract relations (Johnson et al., 2020), using GNN
or leaf-to-leaf sequence embeddings (Alon et al., 2019a;b). Our work enables adding much richer
semantic information while reducing the dependency on syntax structures. Additionally, CODETREK
can take advantage of program analysis queries on relational databases to eliminate the engineering
burden of augmenting program graphs with additional semantic edges."
RELATED WORK,0.3791348600508906,"Graph representation learning. Our work on learning program representations via relational
databases is closely related to inductive representation learning on graphs (Hamilton et al., 2017)
with graph neural networks (GNNs) (Xu et al., 2018) or Transformers (Ying et al., 2021). Although
scalable GNNs via sampling (Chen et al., 2017; Zhou et al., 2020) have been proposed in the
transductive setting, it is still challenging to represent large database graphs with 100k nodes in this
inductive setting (Clement et al., 2021; Yang & Kuang, 2021). Techniques from transductive graph
embedding based on skip-gram (Perozzi et al., 2014; Grover & Leskovec, 2016) or general knowledge
graph embedding (Das et al., 2017; Hamilton et al., 2018; Zheng et al., 2020) are scalable but not
directly applicable for inductive setting. CODETREK achieves a good balance between modeling for
large codebases and efﬁciency."
RELATED WORK,0.3816793893129771,"Feature selection techniques. There are efforts in the data-mining literature to minimize human
effort in feature augmentation and selection. Chepurko et al. (2020) discover joins that can improve
the prediction accuracy for a single data table whereas CODETREK operates on multiple tables."
CONCLUSIONS,0.3842239185750636,"5
CONCLUSIONS"
CONCLUSIONS,0.38676844783715014,"We proposed CODETREK, a technique that represents programs as relational databases to make rich
semantic information available to deep learning models for code-reasoning tasks. We also introduced
a ﬂexible walk-based mechanism to sample relevant contexts from large graphs which are constructed
from relational databases. We evaluated CODETREK on a variety of real-world tasks and datasets,
and showed that it outperforms state-of-the-art neural models."
CONCLUSIONS,0.3893129770992366,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.39185750636132316,ACKNOWLEDGMENTS
CONCLUSIONS,0.3944020356234097,"We thank the anonymous reviewers, David Bieber, Rishabh Singh, Charles Sutton, and Daniel Tarlow
for their valuable feedback. This research was supported by grants from ONR (#N00014-18-1-2021)
and NSF (#2107429 and #1836936)."
REFERENCES,0.3969465648854962,REFERENCES
REFERENCES,0.3994910941475827,"Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. A transformer-based
approach for source code summarization. arXiv preprint arXiv:2005.00653, 2020."
REFERENCES,0.4020356234096692,"Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs
with graphs. In International Conference on Learning Representations (ICLR). OpenReview.net,
2018."
REFERENCES,0.40458015267175573,"Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications.
In International Conference on Learning Representations (ICLR). OpenReview.net, 2021."
REFERENCES,0.4071246819338422,"Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. code2seq: Generating sequences from
structured representations of code. In International Conference on Learning Representations
(ICLR). OpenReview.net, 2019a."
REFERENCES,0.40966921119592875,"Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec: Learning distributed rep-
resentations of code. Proceedings of the ACM on Programming Languages, 3(POPL):1–29,
2019b."
REFERENCES,0.4122137404580153,"Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Structural language models of code. In
International Conference on Machine Learning, pp. 245–256. PMLR, 2020."
REFERENCES,0.41475826972010177,"Pavel Avgustinov, Oege de Moor, Michael Peyton Jones, and Max Sch¨afer. QL: object-oriented
queries on relational data. In Shriram Krishnamurthi and Benjamin S. Lerner (eds.), European
Conference on Object-Oriented Programming (ECOOP), volume 56 of LIPIcs, pp. 2:1–2:25.
Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik, 2016."
REFERENCES,0.4173027989821883,"Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. Tﬁx: Learning to ﬁx coding errors
with a text-to-text transformer. In International Conference on Machine Learning, pp. 780–791.
PMLR, 2021."
REFERENCES,0.4198473282442748,"Marc Brockschmidt, Miltiadis Allamanis, Alexander L Gaunt, and Oleksandr Polozov. Generative
code modeling with graphs. arXiv preprint arXiv:1805.08490, 2018."
REFERENCES,0.4223918575063613,"Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with
variance reduction. arXiv preprint arXiv:1710.10568, 2017."
REFERENCES,0.42493638676844786,"Nadiia Chepurko, Ryan Marcus, Emanuel Zgraggen, Raul Castro Fernandez, Tim Kraska, and David
Karger. Arda: Automatic relational data augmentation for machine learning. arXiv preprint
arXiv:2003.09758, 2020."
REFERENCES,0.42748091603053434,"Colin B Clement, Shuai Lu, Xiaoyu Liu, Michele Tufano, Dawn Drain, Nan Duan, Neel Sundaresan,
and Alexey Svyatkovskiy. Long-range modeling of source code ﬁles with ewash: Extended window
access by syntax hierarchy. arXiv preprint arXiv:2109.08780, 2021."
REFERENCES,0.4300254452926209,"Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishna-
murthy, Alex Smola, and Andrew McCallum. Go for a walk and arrive at the answer: Reasoning
over paths in knowledge bases using reinforcement learning. arXiv preprint arXiv:1711.05851,
2017."
REFERENCES,0.43256997455470736,"Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishna-
murthy, Alex Smola, and Andrew McCallum. Go for a walk and arrive at the answer: Reasoning
over paths in knowledge bases using reinforcement learning. In International Conference on
Learning Representations, 2018."
REFERENCES,0.4351145038167939,Published as a conference paper at ICLR 2022
REFERENCES,0.43765903307888043,"Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. Hoppity: Learning
graph transformations to detect and ﬁx bugs in programs. In International Conference on Learning
Representations (ICLR). OpenReview.net, 2020."
REFERENCES,0.4402035623409669,"Thomas Dowdell and Hongyu Zhang. Language modelling for source code with transformer-xl.
arXiv preprint arXiv:2007.15813, 2020."
REFERENCES,0.44274809160305345,"Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural
languages. arXiv preprint arXiv:2002.08155, 2020."
REFERENCES,0.44529262086513993,"Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
855–864, 2016."
REFERENCES,0.44783715012722647,"Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with
data ﬂow. arXiv preprint arXiv:2009.08366, 2020."
REFERENCES,0.45038167938931295,"William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Proceedings of the 31st International Conference on Neural Information Processing Systems,
pp. 1025–1035, 2017."
REFERENCES,0.4529262086513995,"William L Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. Embedding
logical queries on knowledge graphs. arXiv preprint arXiv:1806.01445, 2018."
REFERENCES,0.455470737913486,"Vincent J. Hellendoorn, Christian Bird, Earl T. Barr, and Miltiadis Allamanis. Deep learning type
inference. In Proceedings of the ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (ESEC/FSE), 2018."
REFERENCES,0.4580152671755725,"Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global
relational models of source code. In International Conference on Learning Representations (ICLR).
OpenReview.net, 2020."
REFERENCES,0.46055979643765904,"Daniel D Johnson, Hugo Larochelle, and Daniel Tarlow. Learning graph structure with a ﬁnite-state
automaton layer. arXiv preprint arXiv:2007.04929, 2020."
REFERENCES,0.4631043256997455,"Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating
contextual embedding of source code. In Proceedings of the International Conference on Machine
Learning (ICML), volume 119 of Proceedings of Machine Learning Research, pp. 5110–5121.
PMLR, 2020."
REFERENCES,0.46564885496183206,"Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. Code prediction by feeding trees to
transformers. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),
pp. 150–162. IEEE, 2021."
REFERENCES,0.4681933842239186,"Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. Multi-task learning based pre-trained language model for
code completion. In Proceedings of the 35th IEEE/ACM International Conference on Automated
Software Engineering, pp. 473–485, 2020."
REFERENCES,0.4707379134860051,"Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. How could neural
networks understand programs? arXiv preprint arXiv:2105.04297, 2021."
REFERENCES,0.4732824427480916,"Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery
and data mining, pp. 701–710, 2014."
REFERENCES,0.4758269720101781,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems, 30, 2017."
REFERENCES,0.47837150127226463,"Wenhua Wang, Yuqun Zhang, Zhengran Zeng, and Guandong Xu. trans3: A transformer-based
framework for unifying code summarization and code search. arXiv preprint arXiv:2003.03238,
2020."
REFERENCES,0.48091603053435117,Published as a conference paper at ICLR 2022
REFERENCES,0.48346055979643765,"Xin Wang, Yasheng Wang, Pingyi Zhou, Meng Xiao, Yadao Wang, Li Li, Xiao Liu, Hao Wu, Jin Liu,
and Xin Jiang. Clsebert: Contrastive learning for syntax enhanced code pre-trained model. arXiv
preprint arXiv:2108.04556, 2021."
REFERENCES,0.4860050890585242,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018."
REFERENCES,0.48854961832061067,"Hao Yang and Li Kuang. Ccmc: Code completion with a memory mechanism and a copy mechanism.
In Evaluation and Assessment in Software Engineering, pp. 129–138. 2021."
REFERENCES,0.4910941475826972,"Wuu Yang, Susan Horwitz, and Thomas Reps. A program integration algorithm that accommo-
dates semantics-preserving transformations. ACM Transactions on Software Engineering and
Methodology (TOSEM), 1(3):310–354, 1992."
REFERENCES,0.49363867684478374,"Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen,
and Tie-Yan Liu. Do transformers really perform bad for graph representation? arXiv preprint
arXiv:2106.05234, 2021."
REFERENCES,0.4961832061068702,"Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and
Alexander Smola. Deep sets. arXiv preprint arXiv:1703.06114, 2017."
REFERENCES,0.49872773536895676,"Qirun Zhang, Chengnian Sun, and Zhendong Su. Skeletal program enumeration for rigorous compiler
testing. In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and
Implementation (PLDI), pp. 347–361. ACM, 2017."
REFERENCES,0.5012722646310432,"Da Zheng, Xiang Song, Chao Ma, Zeyuan Tan, Zihao Ye, Jin Dong, Hao Xiong, Zheng Zhang, and
George Karypis. Dgl-ke: Training knowledge graph embeddings at scale. In Proceedings of the
43rd International ACM SIGIR Conference on Research and Development in Information Retrieval,
pp. 739–748, 2020."
REFERENCES,0.5038167938931297,"Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,
Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications.
AI Open, 1:57–81, 2020."
REFERENCES,0.5063613231552163,"Daniel Z¨ugner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan G¨unnemann.
Language-agnostic representation learning of source code from structure and context. arXiv
preprint arXiv:2103.11318, 2021."
REFERENCES,0.5089058524173028,Published as a conference paper at ICLR 2022
REFERENCES,0.5114503816793893,"A
TRAINING PARAMETERS AND HYPERPARAMETERS"
REFERENCES,0.5139949109414759,"In this section, we describe details on parameters and hyperparameters we used."
REFERENCES,0.5165394402035624,"CodeTrek
We train CODETREK models with a learning rate of 10−4, 4 transformer layers, an
embedding size of 256, 8 attention heads, and 512 hidden units. We sample 100 walks with lengths
of up to 24 in each graph for every task, except for the VARMISUSE-FUN task for which we sample
500 such walks per graph. The reason is that the anchors we select for VARMISUSE-FUN task are all
the variables in the given program which can be well over 100 variables. So, we increase the total
number of walks to include more random walks starting from each variable."
REFERENCES,0.5190839694656488,"CuBERT
We ﬁne-tune the CuBERT pre-trained model that is provided by Kanade et al. (2020)
with a learning rate of 10−4, 4 transformer layers, and 512 hidden units. We use the checkpoint that
is pre-trained on examples of size 512 tokens."
REFERENCES,0.5216284987277354,"GREAT
We train GREAT models with a learning rate of 10−4, 4 transformer layers, 8 attention
heads and 512 hidden units. The example size in 512 tokens."
REFERENCES,0.5241730279898219,"Code2Seq
We train Code2Seq models with a learning rate of 10−3, 4 layers, 512 hidden units, and
embedding size of 256. We sample 100 paths in each AST."
REFERENCES,0.5267175572519084,"GGNN
We train GGNN models with a learning rate of 10−4, 5 layers, a latent dimension of size
128, and a message dimension of size 128."
REFERENCES,0.5292620865139949,"B
MORE EXPERIMENNTS"
REFERENCES,0.5318066157760815,"Increasing the Number of Layers in Baselines
We repeat the experiments that are reported in
Table 1 for GREAT and GNN with 10 layers. We report the results in Table 4"
REFERENCES,0.5343511450381679,"Task
GNN
GREAT
VARMISUSE
0.72
0.84
VARMISUSE-FUN
0.58
0.86
EXCEPTION
0.30
0.45
EXCEPTION-FUN
0.53
0.68
DEFUSE*
0.78
0.87
DEFUSE-FUN*
0.77
0.84
VARSHADOW
0.74
0.94"
REFERENCES,0.5368956743002544,Table 4: The performance of GNN and GREAT with 10 layers.
REFERENCES,0.539440203562341,"Ablation Study
We measure the contribution of the positional encoding which is used in embedding
walks, the contribution of derived relations in improving the accuracy, and the effect of the biases
assigned to node types. We report the results in Table 5. Every row in this table shows a different
conﬁguration indicated by 1-4."
REFERENCES,0.5419847328244275,"We train a model for the EXCEPTION task using the Positional Encoding in embedding the components
of each walk, the call relation that shows the relationships between functions and their callers, and
the biases which are assigned to nodes of type stmt, expr”, and variable (Conﬁg 1). This setting is
similar to that of Table 1 in the paper. With this setting, CODETREK achieves an accuracy of 63.83%
on the EXCEPTION task. If we remove the Positional Encoding (Conﬁg 2), we see a small drop of
1.77% points in the accuracy. The effect of further removing the biases (Conﬁg 3) is much higher:
CODETREK ’s accuracy drops 6.3% points from 62.06% to 55.76% points. This aligns with our
intuition that adding biases to the aforementioned node types results in generating walks that are
more relevant to the task. Finally, we obtain the largest drop in the accuracy by further removing the
derived call relation (Conﬁg 4). This component contributes a signiﬁcant amount of 10.57% points
to the accuracy of the task, and it obtains a low accuracy of 45.19% points in absence of all three
components."
REFERENCES,0.544529262086514,Published as a conference paper at ICLR 2022
REFERENCES,0.5470737913486005,"Conﬁg #
Positional Encoding
Derived Relations
Biases
Accuracy (%)
1
✓
✓
✓
63.83
2
×
✓
✓
62.06
3
×
✓
×
55.76
4
×
×
×
45.19"
REFERENCES,0.549618320610687,Table 5: Contribution of different factors to the accuracy of EXCEPTION task.
REFERENCES,0.5521628498727735,"# Steps
4
6
12
18
24
30
Accuracy
0.41
0.49
0.60
0.63
0.64
0.65"
REFERENCES,0.55470737913486,Table 6: Sensitivity to the length of walks.
REFERENCES,0.5572519083969466,"Different Pooling Mechanisms.
We examine the effect of mean pooling versus attention pool-
ing on the performance of CODETREK models. Attention pooling increases the accuracy of the
EXCEPTION task from 63.83% to 66.43%."
REFERENCES,0.5597964376590331,"Different Positional Encoding Techniques.
We also explore different positional encoding options.
The current setting of CODETREK gives an accuracy of 63.83% in the EXCEPTION task. Substituting
the sinusoidal positional encoding with a learned one improves the accuracy less than 1% points.
This result is in line with ﬁndings of Vaswani et al. (2017), which report nearly identical results using
both positional encoding techniques."
REFERENCES,0.5623409669211196,"Sensitivity to the Length of Walks
To measure the sensitivity of CODETREK to the length (i.e.,
number of steps) of walks, we train a number of models for the EXCEPTION task with walks of
length 4–30 steps. We report accuracy changes in Table 6. Longer walks tend to improve accuracy.
Walks that are too short (4 or 6 hops) result in models with low accuracy (42% and 51%, respectively)
because they are not able to capture enough information to make predictions. There is, however,
a point when enough context is captured (e.g., 24 hop walks) and longer walks do not improve
performance signiﬁcantly."
REFERENCES,0.5648854961832062,"C
CODEQL QUERIES USED FOR LABELING"
REFERENCES,0.5674300254452926,"In this section, we present the CodeQL queries that we used to label the examples for newly added
tasks. Both queries are adapted from CodeQL’s query repository at https://github.com/
github/codeql."
REFERENCES,0.5699745547073791,"C.1
DEFUSE-FUN QUERY"
IMPORT PYTHON,0.5725190839694656,"1 import python
2 import Definition
3
4 predicate unused_local(Name unused, LocalVariable v) {
5
forex(Definition def | def.getNode() = unused |
6
def.getVariable() = v
7
and def.isUnused()
8
and not exists(def.getARedef())
9
and not exists(annotation_without_assignment(v))
10
and def.isRelevant()
11
and not v = any(Nonlocal n).getAVariable()
12
and not exists(def.getNode().getParentNode().
13
(FunctionDef).getDefinedFunction().getADecorator())
14
and not exists(def.getNode().getParentNode().
15
(ClassDef).getDefinedClass().getADecorator())
16
)
17 }
18
19 private AnnAssign annotation_without_assignment(LocalVariable v) {
20
result.getTarget() = v.getAStore()
21
and not exists(result.getValue())
22 }
23
24 from Name unused, LocalVariable v"
IMPORT PYTHON,0.5750636132315522,Published as a conference paper at ICLR 2022
WHERE,0.5776081424936387,"25 where
26
unused_local(unused, v) and
27
forall(Name el | el = unused.getParentNode().(Tuple).getAnElt() | unused_local(el, _))
28 select unused, v.getId()"
WHERE,0.5801526717557252,"C.2
VARSHADOW QUERY"
IMPORT PYTHON,0.5826972010178118,"1 import python
2 import semmle.python.types.Builtins
3
4 predicate optimizing_parameter(Parameter p) {
5
exists(string name, Name glob | p.getDefault() = glob
6
| glob.getId() = name
7
and p.asName().getId() = name
8
)
9 }
10
11 predicate shadows(Name d, GlobalVariable g, Function scope, int line) {
12
g.getScope() = scope.getScope()
13
and d.getScope() = scope
14
and exists(LocalVariable l |
15
d.defines(l) and
16
l.getId() = g.getId()
17
)
18
and not exists(Import il, Import ig, Name gd | il.contains(d)
19
and gd.defines(g)
20
and ig.contains(gd))
21
and not exists(Assign a | a.getATarget() = d
22
and a.getValue() = g.getAnAccess())
23
and not exists(Builtin::builtin(g.getId()))
24
and d.getLocation().getStartLine() = line
25
and exists(Name defn | defn.defines(g)
26
| not exists(If i | i.isNameEqMain()
27
| i.contains(defn)))
28
and not optimizing_parameter(d)
29 }
30
31 AttrNode pytest_fixture_attr() {
32
exists(ModuleValue pytest | result.getObject(""fixture"").pointsTo(pytest))
33 }
34
35 Value pytest_fixture() {
36
exists(CallNode call |
37
call.getFunction() = pytest_fixture_attr()
38
or call.getFunction().(CallNode).getFunction() = pytest_fixture_attr()
39
| call.pointsTo(result)
40
)
41 }
42
43 predicate assigned_pytest_fixture(GlobalVariable v) {
44
exists(NameNode def |
45
def.defines(v) and def.(DefinitionNode).getValue().pointsTo(pytest_fixture())
46
)
47 }
48
49 predicate first_shadowing_definition(Name d, GlobalVariable g) {
50
exists(int first, Scope scope |
51
shadows(d, g, scope, first)
52
and first = min(int line | shadows(_, g, scope, line))
53
)
54 }
55
56 from Name d, GlobalVariable g, Name def
57 where
58
first_shadowing_definition(d, g)
59
and not exists(Name n | n.deletes(g))
60
and def.defines(g)
61
and not assigned_pytest_fixture(g)
62
and not g.getId() = ""_""
63 select d, g.getId(), def"
IMPORT PYTHON,0.5852417302798982,Published as a conference paper at ICLR 2022
IMPORT PYTHON,0.5877862595419847,"D
CODEBASES AS DATABASES"
IMPORT PYTHON,0.5903307888040712,"D.1
TRANSLATING CODE TO RELATIONAL DATABASE"
IMPORT PYTHON,0.5928753180661578,"CODETREK views program information as relations. ?? shows the number of relations in three
common programming languages. There is a directed acyclic graph F that represents the dependencies
between these relations. For example, in FP ython represents the dependencies between 95 base
relations plus 277 derived relations."
IMPORT PYTHON,0.5954198473282443,"Example D.1. A certain Python task requires base relations {A, B} and derived relation {E}.
However, in FP ython, E depends on derived relation D, which in turn depends on base relations
{B, C}. Therefore, Code2Rel computes all ﬁve relations: A, B, C, D, E in order."
IMPORT PYTHON,0.5979643765903307,"Algorithm 1 (Code2Rel) Given a program P, a set of base relation names RB, and a set of derived
relation names RQ, construct and return database D. Note that the term node in this algorithm refers
to nodes in the relation dependency graph, not to nodes in the program graph (e.g., in Algorithm 2)
that the model will see."
IMPORT PYTHON,0.6005089058524173,1. Initialize D to the set of all base relations in RB by translating program P.
IMPORT PYTHON,0.6030534351145038,2. Let RS be the set of relation names reachable from RQ in relation dependency graph F:
IMPORT PYTHON,0.6055979643765903,"(a) RQ ⊆RS
(b) if r ∈RS and r →r′ ∈Edges(F) then r′ ∈RS
3. Let F be the sub-graph of F induced by set of nodes RS:"
IMPORT PYTHON,0.6081424936386769,"(a) Nodes(F) = RS
(b) Edges(F) = { r →r′ ∈Edges(F) | r, r′ ∈Nodes(F) }"
IMPORT PYTHON,0.6106870229007634,"4. Compute a topological ordering L = [r1, ..., r|Nodes(F )|] of F."
IMPORT PYTHON,0.6132315521628499,5. For each r in L in order:
IMPORT PYTHON,0.6157760814249363,Evaluate the query for computing relation r on database D and add the result to D.
IMPORT PYTHON,0.6183206106870229,"D.2
TRANSLATING RELATIONS TO GRAPH"
IMPORT PYTHON,0.6208651399491094,"Algorithm 2 (Rel2Graph) Given a database D, construct a program graph G."
IMPORT PYTHON,0.6234096692111959,Construct an undirected and labeled graph G as follows:
IMPORT PYTHON,0.6259541984732825,"(a) Nodes(G) = D
(b) Add to Edges(G) each (t1, t2, l) that satisﬁes the following conditions:"
IMPORT PYTHON,0.628498727735369,"i. l : R.[a1, ...ak] →S.[b1, ..., bk] is a referential integrity constraint in the schema of
database D
ii. t1 is a tuple of relation named R in D
iii. t2 is a tuple of relation named S in D"
IMPORT PYTHON,0.6310432569974554,iv. for all i ∈[1..k] : t1.ai = t2.bi
IMPORT PYTHON,0.6335877862595419,"D.3
TRANSLATING GRAPH TO A SET OF WALKS"
IMPORT PYTHON,0.6361323155216285,"Deﬁnition D.1 (Walk speciﬁcation). A walk speciﬁcation S = ⟨C, B, min, max⟩is a tuple in which
C is a conditional expression that ﬁlters walk anchors from the set of nodes Nodes(G), B is a map of
bias values that correspond to each relation name, and min, max ∈R+ specify the minimum and
maximum length of the walks generated by the speciﬁcation."
IMPORT PYTHON,0.638676844783715,Example D.2. An example of a walk speciﬁcation is as follows.
IMPORT PYTHON,0.6412213740458015,"C = {t|t ∈Nodes(G) ∧t ∈expr ∧t.kind = name}
B = {stmt : 5, expr : 5}
min = 3, max = 16"
IMPORT PYTHON,0.6437659033078881,Published as a conference paper at ICLR 2022
IMPORT PYTHON,0.6463104325699746,"Algorithm 3 (Graph2Walks) Given a program graph G, a walk speciﬁcation S = ⟨C, B, min, max⟩,
and the number of walks w, sample a set of walks W."
IMPORT PYTHON,0.648854961832061,1. Initialize the set of walks W = ∅.
IMPORT PYTHON,0.6513994910941476,2. Compute the set of anchors A = {t|t ∈Nodes(G) ∧t conforms to S.C}.
IMPORT PYTHON,0.6539440203562341,3. While |W| ≤w:
IMPORT PYTHON,0.6564885496183206,"(a) Pick a random tuple tcurr from A.
(b) Construct walk by repeating the following steps between S.min and S.max times:"
IMPORT PYTHON,0.6590330788804071,"i. Set tprev := tcurr.
ii. Set tcurr to a t ∈Neighbors(G, tprev) with prob. proportionate to S.B[type(t)].
iii. Let ecurr = (tprev, tcurr, l) ∈Edges(G)"
IMPORT PYTHON,0.6615776081424937,"iv. If ecurr /∈walk then extend walk by ecurr. Otherwise set tcurr := tprev.
(c) If walk /∈W then add it to W."
IMPORT PYTHON,0.6641221374045801,"D.4
BRINGING ALL THE PIECES TOGETHER"
IMPORT PYTHON,0.6666666666666666,"Deﬁnition D.2 (Task speciﬁcation). A task speciﬁcation T = ⟨RB, RQ, S, n⟩is a tuple in which
RB is a set of base relation names, RQ is a set of derived relation names, S is a walk speciﬁcation as
described in Deﬁnition D.1, and n is the number of walks to be generated."
IMPORT PYTHON,0.6692111959287532,"Algorithm 4 (Code2Walks) Given a program P and a task speciﬁcation T = ⟨RB, RQ, S, n⟩,
generate a set of walks W."
IMPORT PYTHON,0.6717557251908397,"1. D = Code2Rel(P, T.RB, T.RQ)"
IMPORT PYTHON,0.6743002544529262,2. G = Rel2Graph(D)
IMPORT PYTHON,0.6768447837150128,"3. W = Graph2Walks(G, T.S, T.n)"
IMPORT PYTHON,0.6793893129770993,"E
TASK SPECIFICATIONS"
IMPORT PYTHON,0.6819338422391857,"Using the notations deﬁned in Appendix D, we describe the speciﬁcations of each task that we
used for evaluating CODETREK. In the following speciﬁcations, ellipsis (...) indicate the rest of the
universe of base relations as designed in Semmle framework."
IMPORT PYTHON,0.6844783715012722,TVARMISUSE-FUN = {
IMPORT PYTHON,0.6870229007633588,"RB = {stmt, expr, variable, ssa-defn, ssa-use, successor, ...},
RQ = {},
S = {"
IMPORT PYTHON,0.6895674300254453,"C = {t | t ∈Nodes(G) ∧t ∈expr ∧t.Kind = name},
B = {stmt : 5, expr : 5, variable : 5},
min = 4, max = 16
},
n = 500 }"
IMPORT PYTHON,0.6921119592875318,TEXCEPTION-FUN = {
IMPORT PYTHON,0.6946564885496184,"RB = {stmt, expr, variable, ssa-defn, ssa-use, successor, ...},
RQ = {},
S = {"
IMPORT PYTHON,0.6972010178117048,"C = {t | t ∈Nodes(G) ∧t ∈stmt ∧t.Kind = except ∧t.Type = HoleException},
B = {stmt : 5, expr : 5, variable : 5},
min = 4, max = 16"
IMPORT PYTHON,0.6997455470737913,Published as a conference paper at ICLR 2022
IMPORT PYTHON,0.7022900763358778,"},
n = 100 }"
IMPORT PYTHON,0.7048346055979644,TEXCEPTION = {
IMPORT PYTHON,0.7073791348600509,"RB = {stmt, expr, variable, ssa-defn, ssa-use, successor, ...},
RQ = {call-graph},
S = {"
IMPORT PYTHON,0.7099236641221374,"C = {t | t ∈Nodes(G) ∧t ∈stmt ∧t.Kind = except ∧t.Type = HoleException},
B = {stmt : 5, expr : 5, module : 0},
min = 10, max = 24
},
n = 100 }"
IMPORT PYTHON,0.712468193384224,TDEFUSE-FUN = {
IMPORT PYTHON,0.7150127226463104,"RB = {stmt, expr, variable, ssa-defn, ssa-use, successor, ...},
RQ = {variable-defs, local-variables},
S = {"
IMPORT PYTHON,0.7175572519083969,"C = {t | t ∈Nodes(G)∧t ∈expr∧t.Kind = name∧t.Context ∈{write, param}},
B = {stmt : 5, expr : 5, variable : 5},
min = 4, max = 16
},
n = 100 }"
IMPORT PYTHON,0.7201017811704835,"TDEFUSE and TVARMISUSE are deﬁned similar to TDEFUSE-FUN and TVARMISUSE-FUN, respectively. However,
in both TDEFUSE and TVARMISUSE their C has an additional restriction t.location = user selection.
Also, TVARMISUSE is evaluated with n = 100."
IMPORT PYTHON,0.72264631043257,TVARSHADOW = {
IMPORT PYTHON,0.7251908396946565,"RB = {stmt, expr, variable, ssa-defn, ssa-use, successor, ...},
RQ = {call-graph},
S = {"
IMPORT PYTHON,0.727735368956743,"C = {t | t ∈Nodes(G) ∧t ∈var ∧t.Kind = name ∧t.is global = True},
B = {stmt : 5, expr : 5, variable : 5},
min = 4, max = 16
},
n = 100 }"
IMPORT PYTHON,0.7302798982188295,"F
WALK FORMAT"
IMPORT PYTHON,0.732824427480916,"We describe the format of a walk as the model views it before encoding using an example. Each walk
consists of three lists for node types, node values, and edges."
IMPORT PYTHON,0.7353689567430025,"{
""anchor"": ""py_stmts(415098,6,415072,2)""
""trajectory"": {
""node_types"": [""py_stmts"", ""py_scopes"",
""py_Functions"", ""py_scopes"", ""py_stmts"",
""py_exprs"", ""py_exprs"", ""py_scopes"",
""py_exprs"", ""py_variables"", ""v_8""],"
IMPORT PYTHON,0.7379134860050891,Published as a conference paper at ICLR 2022
IMPORT PYTHON,0.7404580152671756,"""node_values"": [[""ExceptStmt""], """",
[""pipe"",""line""], """", [""Assign""],
[""Call""], [""Attribute""],
[""Name""], [""metr"",""ic""]],
""edges"": [""(py_scopes.node,py_stmts.id)"",
""(py_Functions.id,py_scopes.scope)"",
""(py_Functions.id,py_scopes.scope)"",
""(py_scopes.node,py_stmts.id)"",
""(py_exprs.parent,py_stmts.id)"",
""(py_exprs.id,py_exprs.parent)"",
""(py_exprs.id,py_scopes.node)"",
""(py_exprs.id,py_scopes.node)"",
""(py_exprs.id,py_variables.parent)"",
""(py_variables.id,variable.id)""]
},
}"
IMPORT PYTHON,0.7430025445292621,"The node values can consist of the value of any attribute of a node. For instance, ExceptStmt above
is the kind of the node with type py stmts, and pipeline is the name of the function that corresponds
to the function which is illustrated by node with type py Functions. The values that are identiﬁers,
function names, etc. are subtokenized using a subword tokenizer (the tensor2tensor package). For
instance, pipeline will break into [pipe,line]. Therefore, each value is encoded using the corresponding
vector in the subword dictionary. If the relation name of a tuple is variable, we assign an id to it to be
able to distinguish between different variables. So, the type of a variable node is determined by an id
(e.g., v 8)."
IMPORT PYTHON,0.7455470737913485,"G
ADDITIONAL INFORMATION ABOUT GRAPHS"
IMPORT PYTHON,0.7480916030534351,"G.1
GRAPH SIZES"
IMPORT PYTHON,0.7506361323155216,"Relational
EXCEPTION-FUN
EXCEPTION
VARSHADOW
VARMISUSE-FUN
DEFUSE-FUN
average
5,278
802,231
62,863
1,482
1,829
std
31,550
504,758
93,248
14,623
27,503
min
55
585
53
130
139
max
492,970
4,422,586
1,285,178
483,499
612,343"
IMPORT PYTHON,0.7531806615776081,"Table 7: Number of tuples (i.e., nodes) across relations of each ﬁle in the dataset used for each task."
IMPORT PYTHON,0.7557251908396947,"AST
EXCEPTION-FUN
EXCEPTION
VARSHADOW
VARMISUSE-FUN
DEFUSE-FUN
average
176
3,202
667
93
177
std
244
4,117
1432
126
268
min
20
20
10
10
10
max
13,035
36,808
36,786
30,729
5,243"
IMPORT PYTHON,0.7582697201017812,Table 8: Number of AST nodes for each ﬁle in the dataset used for each task.
IMPORT PYTHON,0.7608142493638677,"G.2
EDGES IN BASELINE GRAPHS"
IMPORT PYTHON,0.7633587786259542,"GGNN Graphs.
The edges that are represented to GGNN models are borrowed from Allamanis
et al. (2018). They include"
AST EDGES,0.7659033078880407,1. AST edges
NEXTTOKEN EDGES,0.7684478371501272,2. NextToken edges
NEXTTOKEN EDGES,0.7709923664122137,3. LastRead/LastWrite/ComputedFrom/LastLexicalUse edges among variable accesses
NEXTTOKEN EDGES,0.7735368956743003,Published as a conference paper at ICLR 2022
NEXTTOKEN EDGES,0.7760814249363868,"4. GuardedBy/GuardedByNegation edges between variables used in branches and their corre-
sponding conditional expressions"
NEXTTOKEN EDGES,0.7786259541984732,"5. ReturnsTo edges from the return statement to the method declaration, and"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.7811704834605598,"6. FormalArgName edges between method call arguments and their corresponding formal
parameters."
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.7837150127226463,"GREAT Graphs.
For GREAT models, similar to Hellendoorn et al. (2020), we borrow the edge
types from Allamanis et al. (2018) and augment them with function calls."
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.7862595419847328,"H
DATASET SIZES"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.7888040712468194,"The number of samples in each dataset is shown in Table 9. The VARSHADOW dataset consists of
49% samples with positive labels. The DEFUSE-FUN dataset is more skewed, with 15% positive
labels, while the DEFUSE dataset has 1.6% positive labels."
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.7913486005089059,"Task
# Training
# Validation
# Testing
Avg. LoC (<max)
VARMISUSE
700,683
75,468
378,401
13 (<235)
VARMISUSE-FUN
700,683
75,468
378,401
13 (<235)
EXCEPTION
18,456
2,086
10,334
528 (<7,624)
EXCEPTION-FUN
18,456
2,086
10,334
32 (<1,835)
DEFUSE
217,591
52,598
104,111
12 (<528)
DEFUSE-FUN
33,182
8,149
16,296
12 (<528)
VARSHADOW
70,183
21,794
39,845
149 (<27,228)"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.7938931297709924,"Table 9: The number of samples used for training, validation, and testing and the lines of code that
they contain. Lines of code (LoC) is reported as the average lines of code across samples in each
dataset after removing the highest 0.1% and lowest 0.1% of the data."
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.7964376590330788,"I
QUALITATIVE STUDY"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.7989821882951654,"We qualitatively discuss a few examples on speciﬁc code snippets and describe the walks that
contribute the most to the predictions made by the respective CODETREK models. In each example,
we explain how the relations and the semantic edges between them enable CODETREK’s models to
predict accurately."
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8015267175572519,"I.1
EXAMPLE 1 (DEFUSE)"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8040712468193384,"1 def write_random_to_file():
2
no = random.randint(1, 10)
3
with open(""random.txt"", ""w"") as file:
4
file.write(str(no))
5
return no
6
7 def write_random():
8
random_no = write_random_to_file()
9
print ""A random number was written to random.txt"""
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.806615776081425,Figure 7: A sample code snippet for DEFUSE. var
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8091603053435115,"id: 4249
name: no
parent: 5320"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.811704834605598,"access_var
ssa-var
ssa-use
access"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8142493638676844,"ex: 5051
var: 4249
ctx: write"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.816793893129771,"ssa-var_var
ssa-use_var-var"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8193384223918575,"id: 4326
var: 4249"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.821882951653944,"id: 3420
svar: 4326"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8244274809160306,"expr
expr_access"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8269720101781171,"id: 5051
kind: name
parent: 4217"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8295165394402035,Figure 8: The most important walk in a simple instance of DEFUSE.
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8320610687022901,Published as a conference paper at ICLR 2022 var
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8346055979643766,"id: 4252
name: file
parent: 5520"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8371501272264631,"access_var
ssa-var
ssa-use
access"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8396946564885496,"ex: 4380
var: 4252
ctx: write"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8422391857506362,"ssa-var_var
ssa-use_var-var"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8447837150127226,"id: 4318
var: 4252"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8473282442748091,"id: 4309
svar: 4318"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8498727735368957,"expr
expr_access"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8524173027989822,"id: 4254
kind: name
parent: 4380"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8549618320610687,Figure 9: The most important walk in a challenging instance of DEFUSE.
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8575063613231552,"To understand how CODETREK uses semantic relations to determine whether a deﬁned variable is
used, consider the code snippet listed in Figure 7. In this snippet, the local variable ﬁle is deﬁned on
line 3, and then used on line 4. Intuitively, one would start from the variable deﬁnition and follow the
code to ﬁnd an access of it to prove that it is indeed a used variable. More speciﬁcally, a programmer
starts with the expression on line 3 in which variable ﬁle is deﬁned. She then tries to ﬁnd another
access to this variable that reads it, such as on line 4."
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8600508905852418,"CODETREK determines that the walk illustrated in Figure 8 has the highest score among a set of
randomly generated walks. Interestingly, this walk shows a similar behavior to that of a programmer:
it starts at the anchor node (an expr node corresponding to the variable deﬁnition) which corresponds
to the expression that deﬁnes ﬁle. Then, it traverses the graph towards a node that corresponds to a
use of this variable (a ssa-use node corresponding to the variable use)."
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8625954198473282,"1 def construct_file_handle():
2
file = Handler.initialize()
3
def check_handle(file):
4
if file.id < MIN_H:
5
return False
6
return True
7
file = Handler.initialize()
8
return Handler.default()"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8651399491094147,Figure 10: A challenging code snippet for DEFUSE.
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8676844783715013,"Even the models that do not embed semantic edges (e.g., CuBERT) are able to correctly predict that
ﬁle is used, in such simple cases. However, in more complicated cases, such as the code snippet listed
in Figure 10, semantic edges are needed to be able to distinguish between different deﬁnitions of
variable ﬁle and to not confuse various uses of them. In this snippet, ﬁle is deﬁned on line 2, and then
on line 7. The ﬁle deﬁned on lines 2 and 7 are never used. To make matters more complicated, there
is a function check handle that is deﬁned inside the top-level function construct ﬁle handle. This
function takes an argument which is named ﬁle and uses it on line 4."
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8702290076335878,"In the absence of edges that make the relationship between uses and deﬁnitions of variable explicit, it
is challenging for a model to determine that the variable named ﬁle on line 2 is different from the
variable of the same name on line 4. As a result, we see that GREAT and CuBERT fail to label the
variable deﬁnition on line 2 as unused. CODETREK, however, takes advantage of the relationship
between the variable deﬁnition and its use (an ssa-use node) and makes a robust prediction. Among
the set of randomly generated walks starting from the deﬁnition on line 2 (expr node with id 4244)
there are no walks with the following pattern which only occurs when a variable is used after being
deﬁned: “expr →access →var →ssa-var →ssa-use”. Therefore, CODETREK predicts that this
variable is never used. On the other hand, as illustrated in Figure 9, a walk with the mentioned pattern
exists between the deﬁnition on line 3 (expr node with id 4254) to its use on line 4 (ssa-use node
with id 4309). So, CODETREK predicts that this variable is used. It is worth emphasizing that the
walks illustrated in Figure 8 and Figure 9 are very similar although they correspond to completely
different code snippets."
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8727735368956743,"I.2
EXAMPLE 2 (EXCEPTION)"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8753180661577609,"For the EXCEPTION task, we choose a code snippet from the test dataset, which is listed in Figure 11.
In this code snippet, CODETREK must predict the exception to be caught by the except statement
at line 36 (represented by HoleException). The correct exception is ValidationError. We know this"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8778625954198473,Published as a conference paper at ICLR 2022
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8804071246819338,"1 class Admission:
2
3
# ...
4
5
def admit_car(self, car):
6
if not str(car.get_id()).isdecimal():
7
raise ValidationError(""Index is not valid"")
8
name = car.get_number()
9
if name.upper() != name:
10
raise ValidationError(""Number not in capslock"")
11
if name.count("" "") < 2:
12
raise ValidationError(""Number should have 3 parts"")
13
check_name = name.split("" "")
14
if not check_name[0].isalpha():
15
raise ValidationError(""First part is not alpha"")
16
if not check_name[1].isdecimal():
17
raise ValidationError(""Second part is not decimal"")
18
if not check_name[2].isalpha():
19
raise ValidationError(""Third part is not alpha"")
20
owner = car.get_owner().replace(""-"", "" "")
21
if not owner.isalpha() or not owner.istitle():
22
raise ValidationError(""Owner's name is not written correctly"")
23
if len(owner) > 40:
24
raise ValidationError(""Name too long"")
25
26 # a number of other unit test functions removed here only for presentation purposes ...
27
28 def test_car_admit():
29
admit = Admission.get_instance()
30
car1 = Car(1, ""ag 12 BOB"", ""Dan"")
31
car4 = Car(4, ""A"", ""Ian"")
32
33
try:
34
admit.admit_car(car1)
35
assert False
36
except HoleException:
37
assert True"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8829516539440203,Figure 11: A sample code snippet for EXCEPTION.
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8854961832061069,"scope_stmt
call.caller_func.id
func_scope
call.callee_func.id"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8880407124681934,func_scope
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8905852417302799,scope_stmt scope stmt stmt
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8931297709923665,"id: 4506
kind: except
scope: 3801"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8956743002544529,"id: 3801
scope:
src.py:93-102"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.8982188295165394,access
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9007633587786259,"ex: 4264
var: 4265
ctx: read var"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9033078880407125,"id: 4265
name: ValidationError
parent: 4226"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.905852417302799,access_var
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9083969465648855,"func
call
func scope"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.910941475826972,"id: 4452
 name: test_car_admit
 scope: 3801"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9134860050890585,"id: 5253
 caller: 4452
 callee: 4236"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.916030534351145,"id: 4436
 name: admit_car
 scope: 3923"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9185750636132316,"id: 3923
scope:
src.py:20-44"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9211195928753181,"id: 4262
kind: raise
scope: 3923"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9236641221374046,"expr
access_expr
expr_stmt"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.926208651399491,"id: 4264
kind: name
parent: 4262"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9287531806615776,Figure 12: The most important walk in an instance of EXCEPTION.
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9312977099236641,"because we observe that the function admit car is called in the corresponding try block, and upon
inspecting its deﬁnition, we see that it raises the ValidationError exception."
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9338422391857506,"Out of the walks sampled by CODETREK for predicting the correct exception, we illustrate the most
important (highest scoring) walk in Figure 12. This walk represents the aforementioned intuitive
reasoning for predicting the exception. It starts at the anchor node, which is the node of type stmt with
id 4506, corresponding to the except statement on line 36. It traverses to the function deﬁnition of
admit car by ﬁrst traversing to the call node for admit car with id 5253, representing the call on line 34,
and then following the corresponding call graph edge to the deﬁnition of admit car. These call graph
edges allow for such inter-procedural reasoning. The walk then traverses to the stmt node for the
raise statement, then to its expression, and reaches the ValidationError exception via its corresponding
access node."
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9363867684478372,Published as a conference paper at ICLR 2022
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9389312977099237,"I.3
EXAMPLE 3 (VARSHADOW)"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9414758269720102,"1 env_vars = env.vars
2
3 class SystemReq:
4
# ...
5
6 # ...
7
8 class Utils:
9
@staticmethod
10
def rev(s):
11
for i in range(len(s)//2):
12
tmp = s[i]
13
s[i] = s[-(i+1)]
14
s[-(i+1)] = tmp
15
16
@staticmethod
17
def env_check():
18
env_vars = environ.vars
19
return env_vars"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9440203562340967,Figure 13: A sample code snippet for VARSHADOW.
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9465648854961832,access
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9491094147582697,"ex: 5051
var: 4238
ctx: write var"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9516539440203562,"id: 4238
name: env_vars
parent: 4226"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9541984732824428,"expr
module"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9567430025445293,id: 4226 
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9592875318066157,scope- loc
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9618320610687023,"id: 1937
scope: 8456
parent: 4226"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9643765903307888,"func
var"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9669211195928753,"id: 4347
name: env_vars
parent: 4337"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9694656488549618,access
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9720101781170484,"ex: 4346
var: 4347
ctx: write"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9745547073791349,"id: 5051
kind: name
parent: 6327"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9770992366412213,"id: 4337
 name: env_check
 scope: 8456 scope"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9796437659033079,"id: 8456
scope:
src.py:93-95"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9821882951653944,"access_expr
access_var
module_var"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9847328244274809,module_scope-loc
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9872773536895675,scope_scope-loc
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.989821882951654,"func_scope
func_var
access_var"
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9923664122137404,Figure 14: The most important walk in an instance of VARSHADOW.
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9949109414758269,"VARSHADOW is an example of a long-range task in which the model has to be able to distinguish
between the global and the local scopes in order to predict whether a global variable is shadowed
by another variable with the same name that is deﬁned in a local scope. We use the code snippet in
Figure 13 to explain how semantic relations help in such tasks."
FORMALARGNAME EDGES BETWEEN METHOD CALL ARGUMENTS AND THEIR CORRESPONDING FORMAL,0.9974554707379135,"To determine whether a global variable is shadowed by a local variable, a programmer would look
for variables that are deﬁned in local scopes and have the same name as the global variable. The
walk which is illustrated in Figure 14 captures the relationship between the global variable deﬁnition
(expr node with id 5051) and a local re-deﬁnition with the same name (access node with id 4346)
by visiting a local scope (scope node with id 8456) of the module (the module node) along the way.
Interestingly, CODETREK assigns the highest importance to this walk among a number of randomly
generated walks, and can therefore correctly predict that env vars is a shadowed global variable."
