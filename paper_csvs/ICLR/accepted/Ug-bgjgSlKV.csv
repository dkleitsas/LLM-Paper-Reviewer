Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0026455026455026454,"Recent research has shown that numerous human-interpretable directions exist in
the latent space of GANs. In this paper, we develop an automatic procedure for
Ô¨Ånding directions that lead to foreground-background image separation, and we
use these directions to train an image segmentation model without human super-
vision. Our method is generator-agnostic, producing strong segmentation results
with a wide range of different GAN architectures. Furthermore, by leveraging
GANs pretrained on large datasets such as ImageNet, we are able to segment
images from a range of domains without further training or Ô¨Ånetuning. Eval-
uating our method on image segmentation benchmarks, we compare favorably
to prior work while using neither human supervision nor access to the training
data. Broadly, our results demonstrate that automatically extracting foreground-
background structure from pretrained deep generative models can serve as a re-
markably effective substitute for human supervision."
INTRODUCTION,0.005291005291005291,"1
INTRODUCTION"
INTRODUCTION,0.007936507936507936,"Recent years have seen rapid progress in the Ô¨Åeld of deep generative modeling of images, driven by
a proliferation of research into Generative Adversaial Networks (GANs) (Goodfellow et al., 2014).
Nowadays, it is possible to generate high-resolution images of realistic objects and scenes (Karras
et al., 2019; Brock et al., 2019). However, with the exception of generation for computer graphics,
there has been limited research into how we might be able to leverage the representations learned
by these powerful generative models to enhance other tasks, particularly those involving semantic
understanding (e.g. classiÔ¨Åcation or segmentation)."
INTRODUCTION,0.010582010582010581,"Generally, deep generative models learn to map latent codes to images, imposing simple statistical
structures on the distribution of the latent codes, such as assuming an i.i.d. Gaussian distribution.
Due to this structure, in some cases code dimensions acquire speciÔ¨Åc meanings which can be related
to human-interpretable concepts (e.g., the rotation or size of an object); however, the code space
in high-quality generators (e.g., BigGAN (Brock et al., 2019), BigBiGANs (Donahue & Simonyan,
2019), StyleGAN (Karras et al., 2019)) is usually not easily interpretable. Nonetheless, it is intuitive
that an efÔ¨Åcient generative process should account for the structure of natural images, including for
example the fact that images often comprise distinct foreground and background regions."
INTRODUCTION,0.013227513227513227,"In this paper, we validate this hypothesis by learning to separate foreground and background image
regions from generator networks. Our approach starts from an arbitrary, off-the-shelf high-quality
generator network trained on a large corpus of (unlabeled) images. While these generator networks
are not explicitly trained for foreground/background segmentation, we show that such a separation
emerges implicitly as a step to efÔ¨Åciently encode realistically-looking images. SpeciÔ¨Åcally, we de-
sign a probing scheme that can extract such foreground/background information automatically, i.e.
without manual supervision, from the generated images."
INTRODUCTION,0.015873015873015872,"This scheme works as follows (cf. Fig. 1). We start from a random code in latent space and learn
a Ô¨Åxed, global offset that results in a change in the generated images. The offset is learned to alter
the appearance of foreground and background such that a mask can be extracted from the changes
in image space."
INTRODUCTION,0.018518518518518517,Published as a conference paper at ICLR 2022
INTRODUCTION,0.021164021164021163,Generated images
INTRODUCTION,0.023809523809523808,"and extracted masks
Real-world images and"
INTRODUCTION,0.026455026455026454,predicted masks
INTRODUCTION,0.0291005291005291,"Figure 1: We automatically Ô¨Ånd a universal latent direction in a GAN that can separate the foreground from
the background without any supervision. We can then generate unlimited samples with masks to train a seg-
mentation network that achieves state-of-the-art unsupervised segmentation performance."
INTRODUCTION,0.031746031746031744,"The resulting masks provide segmentation maps for the generated images, but they cannot yet be
used to segment images from the real world. Given a natural image, the obvious approach would
be to Ô¨Ånd the corresponding code in the latent space of the generator, and then obtain a mask with
our method. Unfortunately, this inversion process is less than trivial. In fact, recent work provides
strong evidence that the expressiveness of GANs is insufÔ¨Åcient to encode arbitrary images (Bau
et al., 2019), meaning that the inversion problem has no solution in general."
INTRODUCTION,0.03439153439153439,"As we aim to build a general-purpose segmentation method, we take a different approach: we gen-
erate a labelled image dataset with foreground/background segmentations and use the generated
dataset to train a standard segmentation network. With this, we show that our method can suc-
cessfully learn accurate foreground-background segmentation networks with no manually provided
labels at all. Differently from prior work in GAN-based image segmentation, we neither design
a new GAN architecture speciÔ¨Åcally for the task of segmentation nor use manual supervision to
extract segmentation information from an existing GAN. Thus, we can discover meaningful latent
directions for any GAN with no need for model-speciÔ¨Åc manual intervention."
INTRODUCTION,0.037037037037037035,"Extensive experiments on Ô¨Åve segmentation datasets across twelve different GANs demonstrate the
effectiveness and generalizability of our approach. Moreover, by constructing our image segmenters
from generator networks trained on a generic large-scale datasets such as ImageNet, our method can
learn to generically segment objects from a wide range of visual domains. SpeciÔ¨Åcally, when we
apply our generic segmenters to the CUB200 (Welinder et al., 2010) and Oxford Flowers (Nilsback,
2009) datasets, we attain very strong foreground-background results despite not training on this
data. Similarly, when we apply our generic segmenters to three saliency detection benchmarks, our
method approaches and sometimes even exceeds the performance of supervised and handcrafted
saliency detection methods. An analysis of our results also shows that segmentation performance
directly correlates with the quality of the underlying GAN, suggesting that foreground/background
separation is an important concept in learning generative models."
INTRODUCTION,0.03968253968253968,"Finally, we demonstrate that our method may be used as a drop-in replacement for saliency networks
for the purpose of learning pixel-wise semantic image representations. These pixel-wise represen-
tations can then be clustered to obtain fully unsupervised semantic segmentations, extending our
method beyond foreground-background segmentation. Thus, we not only demonstrate for the Ô¨Årst
time that it is possible to obtain semantic segmentations using GANs, but also that this information
may be extracted from a wide range of generic GANs trained on general-purpose image datasets."
RELATED WORK,0.042328042328042326,"2
RELATED WORK"
RELATED WORK,0.04497354497354497,"Below, we describe how our method relates to recent work in interpreting generative models and
object segmentation. Due to space constraints, we include additional related work in the Appendix."
RELATED WORK,0.047619047619047616,"Interpreting Deep Generative Models.
Several works have proposed methods for decomposing
the latent space of a generative model into interpretable or disentangled directions. Early work
included Beta-VAE (Higgins et al., 2017), which modiÔ¨Åed the variational ELBO in the original VAE
formulation, and InfoGAN (Chen et al., 2016), which maximized the mutual information between
a subset of the latent code and the generated data. Later work has sought to disentangle factors
of variation by mixing latent codes (Hu et al., 2018), adding additional adversarial losses (Mathieu
et al., 2016), and using contrastive learning (Ren et al., 2021)."
RELATED WORK,0.05026455026455026,Published as a conference paper at ICLR 2022
RELATED WORK,0.05291005291005291,"Our work follows a recent line of research that looks for structure in large, pretrained generative
models. Shen & Zhou (2020) perform a direct decomposition of model weights to Ô¨Ånd disentangled
directions, while Peebles et al. (2020) penalize nonzero second-order interactions between different
latent dimensions, and Voynov & Babenko (2020) Ô¨Ånd interpretable directions by introducing an
additional reconstruction network."
RELATED WORK,0.05555555555555555,"Differently from the works above, we conduct a deep study of one speciÔ¨Åc type of structure (fore-
ground/background separation) encoded in the latent space. Other works have taken this approach
in the context of extracting 3D structure from 2D images; for example, IG-GAN (Lunz et al., 2020)
uses a neural renderer to recover 3D (voxel-based) representations of scenes, and GAN2Shape (Pan
et al., 2021) exploits viewpoint and lighting variations in generated images to recover 3D shapes."
RELATED WORK,0.0582010582010582,"Unsupervised Object Segmentation.
Prior work on unsupervised object segmentation can be
divided into two categories: those that employ generative models to obtain segmentation masks and
those that employ purely discriminative methods such as contrastive learning (Ji et al., 2019a; Ouali
et al., 2020). Here, we focus on generative approaches."
RELATED WORK,0.06084656084656084,"Nearly all generative approaches are based on the idea of decomposing the generative process in
a layer-wise fashion; in general, the foreground and background of an image are generated sepa-
rately and then combined to obtain a Ô¨Ånal image. SpeciÔ¨Åcally, ReDo (Chen et al., 2019) trains a
generator to re-draw new objects on top of old objects, and enforces realism through adversarial
training. (Bielski & Favaro, 2019) generates a background, a foreground, and a foreground mask
separately and composite them together; they prevent degenerate outputs (i.e. the foreground and
background being the same) by randomly shifting the foreground relative to the background. Copy-
Paste GAN (Arandjelovi¬¥c & Zisserman, 2019) receives two images as input and copies parts of one
image onto the other. OneGAN (Benny & Wolf, 2020) learns to simultaneously generate, cluster,
and segment images with a combination of GANs, VAEs, and additional encoders. Equivariant Lay-
ered GAN (Yang et al., 2021) Ô¨Årst trains a new layerwise GAN and then trains a segmentation net-
work on synthetic data. Labels4Free (Abdal et al., 2021) proposes a new layerwise network inspired
by StyleGAN for foreground-background segmenation. Common difÔ¨Åculties with these layer-wise
approaches above include the challenges of training new GANs and scaling beyond simple datasets
(e.g. CUB, Flowers)."
RELATED WORK,0.06349206349206349,"One work along different lines is by Voynov et al. (2021), which uses a pretrained BigBiGAN (Don-
ahue & Simonyan, 2019) generator rather than proposing a new layer-wise GAN. Voynov et al.
(2021) use the method from Voynov & Babenko (2020) to decompose the latent space into inter-
pretable directions. A recent version of their method also picks out this direction without supervi-
sion using the idea that a foreground-background separating direction should be decomposable into
two afÔ¨Åne operators acting on different sets of pixels."
RELATED WORK,0.06613756613756613,"Our approach is based on generative modeling, but it differs from other approaches in that we seek
a general-purpose method to Ô¨Ånd foreground/background structure implicitly encoded in standard,
non-layer-wise GANs rather than encoding it explicitly or searching for it manually. This enables us
to leverage any of the numerous existing generators that have already been pretrained on millions
of high-resolution images, rather than developing a new GAN architecture for this speciÔ¨Åc task.
Differently from layer-wise approaches, our approach does not require training new GANs and it
does not rely on the assumption that the foreground and background of an image are independent.
Differently from Voynov et al. (2021), we show through experiments that our method can be applied
to a wide range of different GANs and Ô¨Ånd that it delivers superior performance on four out of Ô¨Åve
object segmentation and saliency detection benchmarks. Finally, unlike any of these previous works,
we demonstrate that our foreground-background segmentation network can be used as a drop-in
replacement for saliency networks for the task of learning dense semantic image representations.
By clustering these representations, we are able to extend our method from foregound-background
segmentation to semantic segmentation."
METHOD,0.06878306878306878,"3
METHOD"
METHOD,0.07142857142857142,"Let x ‚ààR3√óH√óW be a (color) image. A generator (network) is a function G : RD ‚ÜíR3√óH√óW
that maps code variables z to images x = G(z). Optionally, some generative models come with an
encoder function E : R3√óH√óW ‚ÜíRD which computes an approximate inverse of the generator
(i.e. G(E(x)) ‚âàx)."
METHOD,0.07407407407407407,Published as a conference paper at ICLR 2022
METHOD,0.07671957671957672,"Sobel 
Filter"
METHOD,0.07936507936507936,"Sobel 
Filter
G"
METHOD,0.082010582010582,Step 1: Finding informative code variations z G
METHOD,0.08465608465608465,z + vl
METHOD,0.0873015873015873,‚Ñíconsistency
METHOD,0.08994708994708994,‚Ñísegmentation
METHOD,0.09259259259259259,"Perturbation 
in direction  v
G z G"
METHOD,0.09523809523809523,z + vl G
METHOD,0.09788359788359788,z + vd
METHOD,0.10052910052910052,"S
‚Ñícross-"
METHOD,0.10317460317460317,entropy
METHOD,0.10582010582010581,Prediction
METHOD,0.10846560846560846,"Synthetic 
Ground Truth"
METHOD,0.1111111111111111,Brightness mask
METHOD,0.11375661375661375,Step 2: Learning a segmentation model
METHOD,0.1164021164021164,"Perturbation 
in direction  vd"
METHOD,0.11904761904761904,"Perturbation 
in direction  vl"
METHOD,0.12169312169312169,"Figure 2: (Typo Ô¨Åxed) Our unsupervised segmentation pipeline. First (left), a direction is identiÔ¨Åed
in the latent space of a deep generative model (G) that separates the foreground and background
of generated images by changing their relative brightness. Second (right), a synthetic dataset is
generated using this direction (or two of these directions) and is used to train a separate segmentation
network (S). This network can then be applied to unseen real-world data without further training."
METHOD,0.12433862433862433,"A challenge in generating images is that individual pixels exhibit complex correlations, caused by the
fact that the images are obtained as the composition of a number of different objects. For example,
all pixels that belong to a dog have a similar color, characteristic of dog‚Äôs instance. However, the
correlation is much less strong between pixels that belong to different objects. This is because,
while object in a scene are not entirely independent, their correlation is much weaker than within
the structure of objects."
METHOD,0.12698412698412698,"Intuitively, an image generator must learn to account for such correlations in order to generate
realistically-looking images. In particular, we expect the generator to capture the idea that pixels
that belong to the same object have a related appearance, whereas the appearance of pixels that be-
long to different objects or, as it may be, to a foreground object and its background, should be much
more statistically independent."
METHOD,0.12962962962962962,"Given a generator function G, it is then natural to ask whether such correlations can be extracted
and used not just for the purpose of generating images, but also for analyzing them. In order to
explore this idea, we consider perturbing the code z via a small increment œµv ‚ààRD, where œµ ‚ààR
and v ‚ààSD‚àí1 is a unit vector. Because the dimension D of the embedding space is typically
much smaller than the dimension 3HW of the generated images, codes provide highly-compressed
views of the data (for example, D = 120 for BigBiGAN (Donahue & Simonyan, 2019) and the
self-conditioned GAN). As such, most changes in the code are likely to affect most if not all pixels
in the image. However, if the generator did in fact learn to compose objects, then one could hope
too Ô¨Ånd speciÔ¨Åc variations v that only affect only portions of the image, corresponding to individual
objects, and use the latter to highlight and segment them."
METHOD,0.13227513227513227,"Empirically, we Ô¨Ånd that the situation is not as simple. SpeciÔ¨Åcally, it is not easy to Ô¨Ånd changes
in the code that leave part of the pixels exactly constant while changing other pixels. However,
we Ô¨Ånd that there are directions that affect foreground and background regions in a systematic and
characteristic manner. Furthermore, we show that these directions are ‚Äòuniversal‚Äô, in the sense that
the same v works for all codes z, and are thus characteristic of a given generator network G."
FINDING INFORMATIVE CODE VARIATIONS,0.1349206349206349,"3.1
FINDING INFORMATIVE CODE VARIATIONS"
FINDING INFORMATIVE CODE VARIATIONS,0.13756613756613756,"Next, we introduce an automated criterion to select informative changes v in code space. To this
end, we consider an image x = G(z) generated from a random code z ‚àºZ, where Z is the code
distribution characteristic of the generator (e.g. an i.i.d. Gaussian). We then consider a modiÔ¨Åed
image x‚Ä≤ = G(z + œµv) and observe the change x ‚Üíx‚Ä≤."
FINDING INFORMATIVE CODE VARIATIONS,0.1402116402116402,"We compare the two images using two criteria. The Ô¨Årst one preserves the structure of the image
x. We capture the latter by imposing that x and x‚Ä≤ generate approximately the same edges when
fed to a simple edge detector. The intuition is that we wish v to affect the appearance of objects
without changing their shape. By preventing objects from ‚Äòmoving around‚Äô the image or deforming,
we make it signiÔ¨Åcantly easier to extract an image segmentation from the change x ‚Üíx‚Ä≤. This loss"
FINDING INFORMATIVE CODE VARIATIONS,0.14285714285714285,Published as a conference paper at ICLR 2022
FINDING INFORMATIVE CODE VARIATIONS,0.1455026455026455,"(a) A comparison of generated images for different values
of the perturbation length œµ, using the BigBiGAN genera-
tor. A value of œµ = 0.0 corresponds to the original image,
with a random Gaussian latent vector z ‚àºN(0, 1)."
FINDING INFORMATIVE CODE VARIATIONS,0.14814814814814814,Original
FINDING INFORMATIVE CODE VARIATIONS,0.15079365079365079,"Image
Foreground"
FINDING INFORMATIVE CODE VARIATIONS,0.15343915343915343,Darker
FINDING INFORMATIVE CODE VARIATIONS,0.15608465608465608,Foreground
FINDING INFORMATIVE CODE VARIATIONS,0.15873015873015872,"Lighter
Mask (  ùíól  )
Mask (  ùíób  )"
FINDING INFORMATIVE CODE VARIATIONS,0.16137566137566137,"Mask
( ùíól  and   ùíób )"
FINDING INFORMATIVE CODE VARIATIONS,0.164021164021164,"(b) A comparison of perturbed images and their
masks for vb (foreground darker), vl, (foreground
lighter), and the combination vb and vl. Using both
directions yields visually superior segmentations."
FINDING INFORMATIVE CODE VARIATIONS,0.16666666666666666,takes the form:
FINDING INFORMATIVE CODE VARIATIONS,0.1693121693121693,"Ls(v) = 1 N N
X"
FINDING INFORMATIVE CODE VARIATIONS,0.17195767195767195,"i=1
‚à•S(G(zi + œµv)) ‚àíS(G(zi))‚à•2"
FINDING INFORMATIVE CODE VARIATIONS,0.1746031746031746,where zi ‚àºZ and S is the Sobel-Feldman operator:
FINDING INFORMATIVE CODE VARIATIONS,0.17724867724867724,[S(x)]ij =
X,0.17989417989417988,"3
X"
X,0.18253968253968253,"c=1
(g ‚àóxc::)2
ij + (g‚ä§‚àóxc::)2
ij,"
X,0.18518518518518517,"and g = [1
2
1] ¬∑ [1
0
‚àí1]‚ä§."
X,0.18783068783068782,"This loss encourages x and x‚Ä≤ to be similar. We thus also need a loss that encourages the direction v
to explore a non-zero change of the image. We consider an image contrast variation and additionally
exploit the photographer bias, that objects are often placed in the middle of the image. This is
captured by the loss:"
X,0.19047619047619047,"Lc(v) = 1 N N
X i=1"
X,0.1931216931216931,"3
X"
X,0.19576719576719576,"c=1
‚ü®G(z + œµv), r‚ü©"
X,0.1984126984126984,where r ‚ààRH√óW is a ‚Äòradial‚Äô prior:
X,0.20105820105820105,rij = 1 ‚àí1 Œ±
X,0.2037037037037037,"s
i ‚àíH + 1 2"
X,0.20634920634920634,"2
+

j ‚àíW + 1 2 2"
X,0.20899470899470898,with normalization factor Œ± = 1
P,0.21164021164021163,"4
p"
P,0.21428571428571427,"(H ‚àí1)2 + (W ‚àí1)2 that linearly interpolates from 1 in the
center of the image to ‚àí1 at the boundary. This encourages Ô¨Ånding a direction v that changes the
brightness in the center of the image opposite to the border. In order to learn v, the two losses are
combined with a weighting factor Œª."
P,0.21693121693121692,"L(v) = ŒªLc(v) + Ls(v)
(1)"
P,0.21957671957671956,"Given this fully-automatic procedure, the latent code direction v may be thought of as a function of
the generator G and the weighting factor Œª."
COMBINING INFORMATIVE CODES,0.2222222222222222,"3.1.1
COMBINING INFORMATIVE CODES"
COMBINING INFORMATIVE CODES,0.22486772486772486,"Optimizing Eq. (1) with Œª > 0 encourages the network to produce a shift v that brightens the
foreground and darkens the background of an image. However, there is no constraint that Œª need be
positive; by negating Œª and optimizing a second time, we obtain another direction v that shifts the
foreground dark and the background light."
COMBINING INFORMATIVE CODES,0.2275132275132275,"Although using only one direction sufÔ¨Åces for our method, we Ô¨Ånd that we can improve performance
by using both. As a result, for the remainder of the paper, let vl represent the direction that shifts the
foreground lighter, and vd represent the direction that shifts the foreground darker."
COMBINING INFORMATIVE CODES,0.23015873015873015,Published as a conference paper at ICLR 2022
LEARNING A SEGMENTATION MODEL,0.2328042328042328,"3.2
LEARNING A SEGMENTATION MODEL"
LEARNING A SEGMENTATION MODEL,0.23544973544973544,"Once the latent directions vd and vl have been found, the process of extracting a segmentation mask
is straightforward: we label as foreground regions the pixels in which the image generated with the
foreground-lighter shifted latent code is lighter than the image generated with the foreground-darker
shifted latent code. That is, for a generated image x = G(z), we have:"
LEARNING A SEGMENTATION MODEL,0.23809523809523808,"M(z) = sign(G(z + œµvl) ‚àíG(z + œµvd))
(2)"
LEARNING A SEGMENTATION MODEL,0.24074074074074073,"Alternatively, if we use only a single direction vl or vd, M(z) is set to either:"
LEARNING A SEGMENTATION MODEL,0.24338624338624337,"G(z + œµvl) ‚àíG(z) or G(z) ‚àíG(z + œµvd)
(3)"
LEARNING A SEGMENTATION MODEL,0.24603174603174602,"Given the learned direction v, we use it to generate a training set as follows:"
LEARNING A SEGMENTATION MODEL,0.24867724867724866,"D = {(G(zi), M(zi)) : zi ‚àºZ, i = 1, . . .}."
LEARNING A SEGMENTATION MODEL,0.25132275132275134,"This dataset may then be used to train any dense segmentation network Œ® (i.e., a UNet (Ronneberger
et al., 2015)) in the standard fashion. That is, we minimize the pixel-wise binary cross-entropy loss
between the segmentation output Œ®(G(z)) ‚ààRH√óW and the (synthesized) mask M(z):"
LEARNING A SEGMENTATION MODEL,0.25396825396825395,"L(Œ®|z) = ‚àí
1
HW X"
LEARNING A SEGMENTATION MODEL,0.2566137566137566,"u‚àà[H]√ó[W ]
log p(sign(Mu(z))|Œ®u(G(z)))"
LEARNING A SEGMENTATION MODEL,0.25925925925925924,"where p(m|s) = mœÉ(s)+(1‚àím)œÉ(‚àís), u is a pixel index and sign is the sign function. Unlike pre-
vious object segmentation methods, our method requires no additional losses or constraints to ensure
the stability of training. By sampling z, we can generate an ‚ÄòinÔ¨Ånite‚Äô dataset for learning the network
Œ®. Although we described the procedure above for unconditional GANs, our method applies just as
well for weakly-supervised conditional GANs, where the generator G(z, y) also depends on a class
label; we simply sample a label y uniformly at random for each generated image."
REFINING THE GENERATED DATASET,0.2619047619047619,"3.2.1
REFINING THE GENERATED DATASET"
REFINING THE GENERATED DATASET,0.26455026455026454,"An advantage of training with GAN-generated data is that the dataset size is inÔ¨Ånite, which means
that one is free to curate one‚Äôs dataset and discard uninformative training examples. In our case,
we found that it was helpful to reÔ¨Åne the dataset by (1) discarding images with masks that were
too large, (2) discarding images for which the latent code shift did not produce a signiÔ¨Åcant change
in brightness, and (3) removing small connected components from the mask. The exact details are
given in the Supplementary Material."
EXPERIMENTS,0.2671957671957672,"4
EXPERIMENTS"
EXPERIMENTS,0.2698412698412698,"In this section, we present an extensive set of experiments demonstrating the method‚Äôs performance,
its wide applicability across image datasets, and its generalizability across GAN architectures."
EXPERIMENTAL SETUP,0.2724867724867725,"4.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.2751322751322751,"As our method is generator-agnostic, we apply our method to twelve generators, including three
unconditional and nine conditional GANs. For the three unconditional GANs (BigBiGAN (Donahue
& Simonyan, 2019), SelfCondGAN (Liu et al., 2020), and UncondGAN (Liu et al., 2020)), our
procedure is completely unsupervised. For conditional GANs, our method is still unsupervised but
the GAN naturally relies on class supervision for training."
EXPERIMENTAL SETUP,0.2777777777777778,"To demonstrate the efÔ¨Åcacy of our method across resolutions and datasets, we implement both,
GANs trained on ImageNet (Deng et al., 2009) at a resolution of 128px, and GANs trained on the
smaller TinyImageNet dataset (100,000 images split into 200 classes) at a resolution of 64px. All ex-
periments performed across all GANs utilize the same set of hyperparameters for both optimization
and segmentation. This is a key advantage of our method relative to other unsupervised/weakly-
supervised image segmentation methods (Chen et al., 2019; Bielski & Favaro, 2019; Benny & Wolf,
2020; Arandjelovi¬¥c & Zisserman, 2019), which are sensitive to dataset-speciÔ¨Åc hyperparameters."
EXPERIMENTAL SETUP,0.2804232804232804,Published as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.2830687830687831,"Table 1: (Updated for Voynov et al. (2021)) Performance on three saliency detection benchmarks (DUTS, EC-
SSD, DUT-OMRON) and two object segmentation benchmarks (CUB, Flowers). ‚àó‚àóinitializes with a pretrained
supervised network. ‚Ä† CRF post-processing. ‚ãÑour implementation."
EXPERIMENTAL SETUP,0.2857142857142857,"DUTS
ECSSD
Acc
IoU
FŒ≤
Acc
IoU
FŒ≤"
EXPERIMENTAL SETUP,0.28835978835978837,Supervised Methods
EXPERIMENTAL SETUP,0.291005291005291,"(Hou et al., 2019)
0.924
-
0.729 0.930
-
0.880
(Luo et al., 2017)
0.920
-
0.736 0.934
-
0.891
(Zhang et al., 2017b)
0.902
-
0.693 0.939
-
0.883
(Zhang et al., 2017c)
0.868
-
0.660 0.920
-
0.852
(Wang et al., 2017)
0.915
-
0.672 0.908
-
0.826
(Li et al., 2016)
0.924
-
0.605 0.840
-
0.759"
EXPERIMENTAL SETUP,0.29365079365079366,Handcrafted Methods
EXPERIMENTAL SETUP,0.2962962962962963,"RBD (Zhu et al., 2014)
0.799
-
0.510 0.817
-
0.652
DSR (Li et al., 2013)
0.863
-
0.558 0.826
-
0.639
MC (Jiang et al., 2013)
0.814
-
0.529 0.796
-
0.611
HS (Zou & Komodakis, 2015) 0.773
-
0.521 0.772
-
0.623"
EXPERIMENTAL SETUP,0.29894179894179895,Deep Ensembles of Handcrafted Methods
EXPERIMENTAL SETUP,0.30158730158730157,"SBF (Zhang et al., 2017a)
0.865
-
0.583 0.915
-
0.787
USD‚àó‚àó(Zhang et al., 2018)
0.914
-
0.716 0.930
-
0.878
USPS‚àó‚àó‚Ä†(Nguyen et al., 2019) 0.938
-
0.736 0.937
-
0.874"
EXPERIMENTAL SETUP,0.30423280423280424,Unsupervised Methods
EXPERIMENTAL SETUP,0.30687830687830686,"(Voynov et al., 2021)
0.878 0.498
-
0.899 0.672
-
(Voynov et al., 2021) ‚ãÑ
0.881 0.508 0.600 0.906 0.685 0.790"
EXPERIMENTAL SETUP,0.30952380952380953,"Ours
0.893 0.528 0.614 0.915 0.713 0.806"
EXPERIMENTAL SETUP,0.31216931216931215,"CUB
Flowers
Acc
IoU maxFŒ≤ Acc
IoU maxFŒ≤"
EXPERIMENTAL SETUP,0.3148148148148148,Unsupervised Methods
EXPERIMENTAL SETUP,0.31746031746031744,"PertGAN (Bielski & Favaro, 2019)
-
0.380
-
-
-
-
ReDO (Chen et al., 2019)
0.845 0.426
-
0.879 0.764
-
WNet‚Ä†(Xia & Kulis, 2017)
-
0.248
-
-
-
-
UISB (Kanezaki, 2018)
-
0.442
-
-
-
-
IIC-seg (Ji et al., 2019b)
-
0.365
-
-
-
-
OneGAN (Benny & Wolf, 2020)
-
0.555
-
-
-
-
(Voynov et al., 2021)
0.930 0.683 0.794 0.765 0.540 0.760
(Voynov et al., 2021)‚ãÑ
0.931 0.693 0.807 0.777 0.529 0.672"
EXPERIMENTAL SETUP,0.3201058201058201,"Ours (BigBiGAN - ImageNet)
0.921 0.664 0.783 0.796 0.541 0.723"
EXPERIMENTAL SETUP,0.32275132275132273,"DUT-OMRON
Acc
IoU
FŒ≤"
EXPERIMENTAL SETUP,0.3253968253968254,"(Voynov et al., 2021)‚àó
0.856
0.453
-
(Voynov et al., 2021)‚àó‚ãÑ
0.859
0.460
0.533"
EXPERIMENTAL SETUP,0.328042328042328,"Ours
0.883
0.509
0.583"
EXPERIMENTAL SETUP,0.3306878306878307,"Table 2: A comparison of segmentation model performance across a wide range of generator architectures,
using a foreground-lighter shift (vl). All hyperparameters are kept constant across generators. IN-128px refers
to ImageNet at resolution 128px, and TinyIN-64px refers to TinyImageNet at resolution 64px."
EXPERIMENTAL SETUP,0.3333333333333333,"CUB
Flowers
DUT-OMRON
DUTS
ECSSD
Dataset Res.
Acc
IoU
Acc
IoU
Acc
IoU
Acc
IoU
Acc
IoU"
EXPERIMENTAL SETUP,0.335978835978836,"ACGAN (Odena et al., 2017)
TinyIN 64px
0.682
0.265
0.572
0.266
0.642
0.190
0.647
0.191
0.652
0.276
BigGAN (Brock et al., 2019)
TinyIN 64px
0.853
0.257
0.723
0.284
0.844
0.213
0.842
0.224
0.811
0.332
GGAN (Lim & Ye, 2017)
TinyIN 64px
0.818
0.366
0.697
0.315
0.782
0.221
0.783
0.235
0.766
0.316
SAGAN (Zhang et al., 2019)
TinyIN 64px
0.828
0.376
0.732
0.351
0.808
0.235
0.806
0.246
0.788
0.327
SNGAN (Miyato et al., 2018)
TinyIN 64px
0.849
0.357
0.751
0.374
0.816
0.216
0.814
0.217
0.795
0.292
SAGAN (Zhang et al., 2019)
IN 128px
0.871
0.336
0.608
0.085
0.856
0.250
0.860
0.282
0.814
0.340
SNGAN (Miyato et al., 2018)
IN 128px
0.881
0.378
0.703
0.304
0.860
0.305
0.854
0.300
0.837
0.432
ContraGAN (Kang & Park, 2020)
IN 128px
0.857
0.159
0.661
0.088
0.858
0.075
0.870
0.149
0.805
0.204
UnCondGAN (Liu et al., 2020)
IN 128px
0.734
0.217
0.494
0.049
0.698
0.127
0.729
0.158
0.681
0.198
SelfCondGAN (Liu et al., 2020)
IN 128px
0.869
0.459
0.670
0.238
0.800
0.280
0.806
0.297
0.806
0.412
BigGAN (Brock et al., 2019)
IN 128px
0.886
0.367
0.731
0.318
0.883
0.316
0.876
0.303
0.848
0.424
BigBiGAN (Donahue & Simonyan, 2019)
IN 128px
0.912
0.601
0.773
0.479
0.878
0.451
0.890
0.486
0.905
0.663"
EVALUATION DATA,0.3386243386243386,"4.2
EVALUATION DATA"
EVALUATION DATA,0.3412698412698413,"We evaluate the performance of our model on three standard saliency detection benchmarks (DUT-
Omrom (Yang et al., 2013), DUTS (Wang et al., 2017), ECSSD (Shi et al., 2016)) and two standard
object segmentation benchmarks (CUB (Welinder et al., 2010), and Flowers-102 (Nilsback & Zisser-
man, 2009)). For the saliency datasets, we evaluate using (pixel-wise) accuracy, mean intersection-
over-union (IoU), and FŒ≤-score with Œ≤2 = 0.3. For the object segmentation datasets, we evaluate
using accuracy, IoU, and maxFŒ≤ (the maximum FŒ≤ score over a range of 255 uniformly distributed
binarization thresholds between 0 and 1)."
RESULTS,0.3439153439153439,"4.3
RESULTS"
RESULTS,0.34656084656084657,"Performance on Benchmarks.
In Table 1, we compare our method to other recent work. We em-
phasize that our method uses the same model for all datasets and has not seen any of the (training or
evaluation) data for these datasets before. Our method delivers strong performance across datasets,
approaching and even outperforming some supervised/handcrafted saliency detection methods. In"
RESULTS,0.3492063492063492,Published as a conference paper at ICLR 2022
RESULTS,0.35185185185185186,"comparison to Voynov et al. (2021), we perform better on four out of Ô¨Åve benchmarks (all except
CUB), even though we do not rely on humans to hand-pick latent directions. In comparison to
layerwise GANs (Chen et al., 2019; Bielski & Favaro, 2019; Benny & Wolf, 2020; Arandjelovi¬¥c
& Zisserman, 2019), we perform similarly on CUB and Flowers-102, but we cannot compare our
method with layerwise GANs on complex datasets (e.g. DUTS) because they do not produce any
meaningful results. Due to the difÔ¨Åculty of training GANs, they are only ever trained on datasets
consisting of images from a single domain with a single main subject, such as birds or Ô¨Çowers. By
contrast, our ability to leverage pre-trained generators means that our method scales to complex and
diverse datasets, such as those used for saliency detection."
RESULTS,0.3544973544973545,"Figure 4: A plot of Frechet Inception Distance (FID)
versus average segmentation accuracy across all Ô¨Åve
evaluation datasets (CUB, Flowers, DUT-OMRON,
DUTS, ECSSD) for nine different GAN architectures.
Lower FIDs are better (note that the x-axis is reversed).
Lower FID scores correlate with improved Ô¨Ånal seg-
mentation accuracy."
RESULTS,0.35714285714285715,"Performance across Generators.
We inves-
tigate the generality of our method by perform-
ing the same optimization and training pipeline
with twelve different GANs. For each gener-
ator, we optimize to obtain a latent direction
vl, train a segmentation model using this di-
rection, and evaluate its performance across the
Ô¨Åve datasets above.
The same hyperparame-
ters are kept constant for all GANs, including
Œª = 5.0 during the optimization phase."
RESULTS,0.35978835978835977,"Results are shown in Table 2; BigBiGAN per-
forms best, but all networks, even those using
relatively weak TinyImageNet-trained GANs
(e.g., GGAN (Lim & Ye, 2017)), deliver rea-
sonable segmentation performance. This high-
lights the beneÔ¨Åts of our fully-automatic seg-
mentation pipeline; our method performs well
across a wide range of generators trained on different datasets at different resolutions."
RESULTS,0.36243386243386244,"Naturally, the quality of a Ô¨Ånal segmentation network produced by our method is related to the
quality of the underlying generator. Figure 4 plots the Frechet Inception Distance (FID) score of
nine conditional GANs versus the average accuracy of the corresponding segmentation networks
produced by our method. Lower FID scores, which correspond to better GANs, correlate with
improved accuracy. This correlation suggests that as better GANs architectures are developed, our
method will continue to produce better unsupervised segmentation networks."
RESULTS,0.36507936507936506,"Ablation: Comparing latent directions.
We compare the performance of a segmentation masks
using the two latent directions vd and vl together (Eq. (2)), or each of them individually (Eq. (3))
visually in Fig. 3a. In Table 6, we quantitatively compare the results of these three methods along
with a fourth method in which we ensemble the Ô¨Ånal segmentation networks produced by vd and vl
individually. The foreground-lighter (vl) and foreground-darker (vd) directions yield similar results
when used individually. The combination (vl and vd) provides superior results, on par with the
ensemble. Unlike the ensemble, which requires training two networks, the combination of vd and vl
adds minimal overhead compared to training with one direction."
RESULTS,0.36772486772486773,"Ablation: Varying Œª, œµ, the Central Prior, Random Initializations, Amount of Generated Data.
The two hyperparameters in the optimization stage of our method are Œª, which controls the trade-off
between brightness and consistency, and œµ, which controls the magnitude of the perturbation. We
Ô¨Ånd that the process is only modestly sensitive to changes in these hyperparameters. We also Ô¨Ånd
that using the central prior compared to a spatially-agnostic loss term is better, but only moderately.
Detailed numerical results, including the ablation on the central prior, are given in Section A.3.
In Appendix B.3, we also investigate how segmentation performance varies with the number of
generated images, Ô¨Ånding a clear log-linear relationship."
RESULTS,0.37037037037037035,"Qualitative Results.
By inspection, we Ô¨Ånd that our optimization procedure is able to edit images
in such a manner that the foreground becomes lighter and the background becomes darker. In nu-
merous cases, the network appears to convert the scene from daytime to nighttime. Furthermore,
better GANs generally produce qualitativley better segmentations. Please refer to Appendix A and
Section A.1.3 for illustrations."
RESULTS,0.373015873015873,Published as a conference paper at ICLR 2022
RESULTS,0.37566137566137564,"Table 3: A comparison of semantic segmentation performance on Pascal-VOC obtained from K-means clus-
tering of pixelwise semantic features. The mIoU is computed over the 20 classes by performing Hungarian
matching between the clusters obtained from K-means and the ground truth. All networks use a ResNet
backbone. We compare with numerous baselines, including using self-supervised features directly (i.e. MoCo,
SwaV) and IIC. Compared to Van Gansbeke et al. (2021), we achieve competitive performance, but our pipeline
is entirely unsupervised, whereas theirs uses a saliency network which was initialized with a supervised network
pretrained for semantic segmentation on CityScapes."
RESULTS,0.3783068783068783,"Method
Saliency Network
Saliency Net. PT
Sem. Seg. PT
mIoU"
RESULTS,0.38095238095238093,"Colorization
Proxy task
-
-
Colorization
4.9
IIC
Clustering
-
-
IIC
9.8
MoCo
Image Contrast -
-
Moco
4.3
Swav
Image Contrast -
-
Swav
4.4
ImageNet Sup.
Image Contrast -
-
Sup. ImageNet
4.4
MaskContrast
Pixel Contrast
DeepUSPS + BAS-Net Cityscapes (Sup.)
MoCo
35.0
MaskContrast
Pixel Contrast
DeepUSPS + BAS-Net Cityscapes + DUTS (Sup.) MoCo
38.9
Ours (BigBiGAN) Pixel Contrast
Our method
Our method (Unsup.)
MoCo
36.5"
EXTENSION TO SEMANTIC SEGMENTATION,0.3835978835978836,"4.4
EXTENSION TO SEMANTIC SEGMENTATION"
EXTENSION TO SEMANTIC SEGMENTATION,0.3862433862433862,"Finally, we demonstrate that our network may be extended from binary foreground-background
segmentation to semantic segmentation, the task of assigning each pixel in an image into one of K
semantic categories. Due to the challenging nature of this task, it has not been attempted by any
previous works in the GAN-based segmentation space."
EXTENSION TO SEMANTIC SEGMENTATION,0.3888888888888889,"We extend our method by following the dense contrastive learning approach proposed by Van Gans-
beke et al. (2021). In this approach, binary masks are extracted from a set of images using a
foreground-background segmentation model, sometimes called a ‚Äúmid-level visual prior.‚Äù Then,
a network is trained to generate pixel-wise features using a mask-based contrastive loss: features
corresponding to pixels in the foreground of the image are pulled toward the features of other pix-
els in the mask and pushed away from the features of background pixels. After training, semantic
segmentations may be extracted by clustering these pixel-wise features across an entire dataset."
EXTENSION TO SEMANTIC SEGMENTATION,0.3915343915343915,"Importantly, Van Gansbeke et al. (2021) uses saliency detection networks to generate their object
masks. Although these saliency detectors are sometimes called ‚Äúunsupervised,‚Äù they are actually
initialized using pretrained semantic segmentation networks1. We propose to use our object seg-
mentation network as a drop-in replacement for these saliency networks, making the entire process
entirely unsupervised. We leave the rest of their method (i.e. the dense contrastive learning and the
evaluation procedure) unchanged."
EXTENSION TO SEMANTIC SEGMENTATION,0.3941798941798942,"We perform experiments on the PASCAL VOC dataset, which contains 20 semantic classes. Exper-
imental details are included in Section A.1.3 and K-Means clustering results are shown in Table 3.
We compare to a range of baselines, along with two models from Van Gansbeke et al. (2021) us-
ing different levels of supervised pretraining. Our network is competitive with Van Gansbeke et al.
(2021) despite being entirely unsupervised. Furthermore, in Table 4 in Section A.1.3 we show that
this procedure works for a wide range of GANs, demonstrating the generalizability of our approach."
CONCLUSIONS,0.3968253968253968,"5
CONCLUSIONS"
CONCLUSIONS,0.3994708994708995,"We Ô¨Ånd that extracting a salient object segmentation from the latent space of a GAN is not only
possible without supervision but also leads to state-of-the-art unsupervised segmentation perfor-
mance on several benchmark datasets. In contrast to existing methods that have been engineered
speciÔ¨Åcally for this task, we extract segmentations from a network trained for a very different pur-
pose ‚Äî generating images. Surprisingly, we are able to generalize to a wide range of segmentation
benchmarks without directly training on any real images, and even extend our results to semantic
segmentation. Our Ô¨Åndings directly prompt future research questions about what other concepts of
the physical world can be automatically extracted from generative models, and to what extent we
can use such extracted concepts to replace human supervision in other computer vision tasks."
CONCLUSIONS,0.4021164021164021,"1In the case of Van Gansbeke et al. (2021), they use DeepUSPS(Nguyen et al., 2019), which is initialized
using a supervised network pretrained for semantic segmentation on Cityscapes."
CONCLUSIONS,0.40476190476190477,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.4074074074074074,"6
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.41005291005291006,"We aim to ensure that our experiments are entirely and easily reproducible. We upload code to the
Supplementary Material to fully reproduce all experiments. This code contains a README Ô¨Åle
with a detailed description of the code structure, which should help enable others to reproduce and
later extend upon our work. We also take care to describe all hyperparameters and implementation
details in the Appendix. Our results do not require extremely large amounts of compute; they can
be reproduced with a single GPU by researchers with computational constraints."
ETHICS STATEMENT,0.4126984126984127,"7
ETHICS STATEMENT"
ETHICS STATEMENT,0.41534391534391535,"It is important to discuss the potential ethical issues involved with training large-scale generative
models and segmentation networks along the lines proposed by our paper."
ETHICS STATEMENT,0.41798941798941797,"First of all, the task of segmentation is predicated upon classifying objects into predetermined (either
binary or semantic) categories; the deÔ¨Ånition of these categories, especially in the case of semantic
segmentation, may introduce biases into the task itself. Second, when training models on large-
scale datasets, it is essential to consider the biases and data privacy issues introduced in the data
collection process. For example, the PASCAL-VOC dataset, which we use for the task of semantic
segmentation, is composed of images scraped from Flickr. As a result, it is composed primarily of
photographs from the United States and Europe, and the ‚Äúperson‚Äù class contains primarily images of
white individuals. Additionally, it is not clear whether the individuals in these photographs consent
to being used to train image segmentation models, bringing up the issue of data privacy."
ETHICS STATEMENT,0.42063492063492064,"From an ethical perspective, our method is slightly different from standard segmentation models
because it is trained solely on GAN-generated images; this is not to say that it is ethically better
or worse, but that it involves different ethical considerations. On the one hand, this might alleviate
some data privacy concerns, as the segmentation training data is synthetic. However, since this
training data is generated by a GAN, one has to examine the data and methodology originally used
to pretrain the GAN; any biases present in this data will likely be reproduced or ampliÔ¨Åed by the
GAN . For example, if one uses a GAN trained on ImageNet to perform object segmentation, it
may perform better on white individuals than individuals of other races due to the disproportionate
percentage of white individuals in the training data (Steed & Caliskan, 2021). Investigating biases
introduced by GANs remains an active area of research in the machine learning community (Jain
et al., 2020; Tan et al., 2020), and these ethical discussions extend to our GAN-based segmentation
method."
REFERENCES,0.42328042328042326,REFERENCES
REFERENCES,0.42592592592592593,"Rameen Abdal, Peihao Zhu, Niloy Mitra, and Peter Wonka. Labels4free: Unsupervised segmenta-
tion using stylegan. 2021."
REFERENCES,0.42857142857142855,"Relja Arandjelovi¬¥c and Andrew Zisserman.
Object discovery with a copy-pasting gan.
arXiv
preprint arXiv:1905.11369, 2019."
REFERENCES,0.4312169312169312,"David Bau, Jun-Yan Zhu, Jonas Wulff, William S. Peebles, Bolei Zhou, Hendrik Strobelt, and An-
tonio Torralba. Seeing what a GAN cannot generate. In Proc. ICCV, 2019."
REFERENCES,0.43386243386243384,"Yaniv Benny and Lior Wolf. Onegan: Simultaneous unsupervised learning of conditional image
generation, foreground segmentation, and Ô¨Åne-grained clustering. Lecture Notes in Computer
Science, pp. 514‚Äì530, 2020. ISSN 1611-3349. doi: 10.1007/978-3-030-58574-7 31."
REFERENCES,0.4365079365079365,"Adam Bielski and Paolo Favaro. Emergence of Object Segmentation in Perturbed Generative Mod-
els. In Proc. NeurIPS, volume 32, 2019."
REFERENCES,0.43915343915343913,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large Scale GAN Training for High Fidelity
Natural Image Synthesis. In Proc. ICLR, 2019."
REFERENCES,0.4417989417989418,"Micka¬®el Chen, Thierry Arti`eres, and Ludovic Denoyer. Unsupervised Object Segmentation by Re-
drawing. In Proc. NeurIPS, volume 32, 2019."
REFERENCES,0.4444444444444444,Published as a conference paper at ICLR 2022
REFERENCES,0.4470899470899471,"Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
interpretable representation learning by information maximizing generative adversarial nets. In
Neural Information Processing Systems (NIPS), 2016."
REFERENCES,0.4497354497354497,"J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In Proc. CVPR, pp. 248‚Äì255, 2009. doi: 10.1109/CVPR.2009.5206848."
REFERENCES,0.4523809523809524,"Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. In Proc. ICLR,
2019."
REFERENCES,0.455026455026455,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. volume 27, 2014."
REFERENCES,0.4576719576719577,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for
unsupervised visual representation learning. arXiv.cs, abs/1911.05722, 2019."
REFERENCES,0.4603174603174603,"Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner.
beta-vae: Learning basic visual concepts with a
constrained variational framework. Proc. ICLR, 2017."
REFERENCES,0.46296296296296297,"Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A. Efros,
and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In Proc. ICML,
2018."
REFERENCES,0.4656084656084656,"Q. Hou, M. Cheng, X. Hu, A. Borji, Z. Tu, and P. H. S. Torr. Deeply supervised salient object
detection with short connections.
In PAMI, pp. 815‚Äì828, 2019.
doi: 10.1109/TPAMI.2018.
2815688."
REFERENCES,0.46825396825396826,"Qiyang Hu, Attila Szab¬¥o, Tiziano Portenier, Paolo Favaro, and Matthias Zwicker. Disentangling
factors of variation by mixing them. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3399‚Äì3407, 2018."
REFERENCES,0.4708994708994709,"Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia Manikonda, and Subbarao Kambhampati. Im-
perfect imaganation: Implications of gans exacerbating biases on facial data augmentation and
snapchat selÔ¨Åe lenses. arXiv preprint arXiv:2001.09528, 2020."
REFERENCES,0.47354497354497355,"Xu Ji, Joao F. Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised
image classiÔ¨Åcation and segmentation. In Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV), October 2019a."
REFERENCES,0.47619047619047616,"Xu Ji, JoÀúao F. Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised im-
age classiÔ¨Åcation and segmentation. In Proceedings of the International Conference on Computer
Vision (ICCV), 2019b."
REFERENCES,0.47883597883597884,"B. Jiang, L. Zhang, H. Lu, C. Yang, and M. Yang. Saliency detection via absorbing markov chain.
In Proc. ICCV, pp. 1665‚Äì1672, 2013. doi: 10.1109/ICCV.2013.209."
REFERENCES,0.48148148148148145,"A. Kanezaki. Unsupervised Image Segmentation by Backpropagation. In Proc. ICASSP, pp. 1543‚Äì
1547, 2018. doi: 10.1109/ICASSP.2018.8462533."
REFERENCES,0.48412698412698413,"Minguk Kang and Jaesik Park. Contragan: Contrastive learning for conditional image generation.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Proc. NeurIPS, vol-
ume 33, pp. 21357‚Äì21369. Curran Associates, Inc., 2020."
REFERENCES,0.48677248677248675,"Tero Karras, Samuli Laine, and Timo Aila. A Style-Based Generator Architecture for Generative
Adversarial Networks. In Proc. CVPR, 2019."
REFERENCES,0.4894179894179894,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.49206349206349204,"X. Li, H. Lu, L. Zhang, X. Ruan, and M. Yang. Saliency detection via dense and sparse reconstruc-
tion. In Proc. ICCV, pp. 2976‚Äì2983, 2013. doi: 10.1109/ICCV.2013.370."
REFERENCES,0.4947089947089947,Published as a conference paper at ICLR 2022
REFERENCES,0.4973544973544973,"X. Li, L. Zhao, L. Wei, M. Yang, F. Wu, Y. Zhuang, H. Ling, and J. Wang. Deepsaliency: Multi-task
deep neural network model for salient object detection. In IEEE Trans. on Image Processing, pp.
3919‚Äì3930, 2016. doi: 10.1109/TIP.2016.2579306."
REFERENCES,0.5,"Jae Hyun Lim and Jong Chul Ye. Geometric gan. In arXiv.cs, 2017."
REFERENCES,0.5026455026455027,"Steven Liu, Tongzhou Wang, David Bau, Jun-Yan Zhu, and Antonio Torralba. Diverse image gen-
eration via self-conditioned gans. In Proc. CVPR, 2020."
REFERENCES,0.5052910052910053,"Sebastian Lunz, Yingzhen Li, Andrew Fitzgibbon, and Nate Kushman. Inverse graphics gan: Learn-
ing to generate 3d shapes from unstructured 2d data. arXiv preprint arXiv:2002.12674, 2020."
REFERENCES,0.5079365079365079,"Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin Eichel, Shaozi Li, and Pierre-Marc Jodoin.
Non-local deep features for salient object detection. In Proc. CVPR, July 2017."
REFERENCES,0.5105820105820106,"Michael Mathieu, Junbo Zhao, Pablo Sprechmann, Aditya Ramesh, and Yann Le Cun. Disentan-
gling factors of variation in deep representations using adversarial training. Advances in Neural
Information Processing Systems, pp. 5047‚Äì5055, 2016."
REFERENCES,0.5132275132275133,"Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In Proc. ICLR, 2018."
REFERENCES,0.5158730158730159,"Duc Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi
Hoai Phuong Nguyen, Zhongyu Lou, and Thomas Brox. DeepUSPS: Deep Robust Unsupervised
Saliency Prediction With Self-Supervision. In Proc. NeurIPS, 2019."
REFERENCES,0.5185185185185185,"M.-E. Nilsback. An Automatic Visual Flora ‚Äì Segmentation and ClassiÔ¨Åcation of Flowers Images.
PhD thesis, University of Oxford, 2009."
REFERENCES,0.5211640211640212,"Maria-Elena Nilsback and Andrew Zisserman. Delving deeper into the whorl of Ô¨Çower segmenta-
tion. Image and Vision Computing, 2009."
REFERENCES,0.5238095238095238,"Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with aux-
iliary classiÔ¨Åer GANs. In Doina Precup and Yee Whye Teh (eds.), Proc. ICML, volume 70 of
Proceedings of Machine Learning Research, pp. 2642‚Äì2651, International Convention Centre,
Sydney, Australia, 06‚Äì11 Aug 2017. PMLR."
REFERENCES,0.5264550264550265,"Yassine Ouali, C¬¥eline Hudelot, and Myriam Tami. Autoregressive Unsupervised Image Segmenta-
tion. In Proc. ECCV, 2020."
REFERENCES,0.5291005291005291,"Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, and Ping Luo. Do 2d gans know 3d shape?
unsupervised 3d shape reconstruction from 2d image gans. Proc. ICLR, 2021."
REFERENCES,0.5317460317460317,"William Peebles, John Peebles, Jun-Yan Zhu, Alexei Efros, and Antonio Torralba. The hessian
penalty: A weak prior for unsupervised disentanglement. arXiv preprint arXiv:2008.10599, 2020."
REFERENCES,0.5343915343915344,"Xuanchi Ren, Tao Yang, Yuwang Wang, and Wenjun Zeng. Do generative models know disentan-
glement? contrastive learning is all you need. arXiv preprint arXiv:2102.10543, 2021."
REFERENCES,0.5370370370370371,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In Proc. MICCAI, 2015."
REFERENCES,0.5396825396825397,"Yujun Shen and Bolei Zhou. Closed-form factorization of latent semantics in gans. arXiv preprint
arXiv:2007.06600, 2020."
REFERENCES,0.5423280423280423,"J. Shi, Q. Yan, L. Xu, and J. Jia. Hierarchical image saliency detection on extended cssd. In PAMI,
2016."
REFERENCES,0.544973544973545,"Ashish Shrivastava, Tomas PÔ¨Åster, Oncel Tuzel, Joshua Susskind, Wenda Wang, and Russell Webb.
Learning from simulated and unsupervised images through adversarial training. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 2107‚Äì2116, 2017."
REFERENCES,0.5476190476190477,"Ryan Steed and Aylin Caliskan. Image representations learned with unsupervised pre-training con-
tain human-like biases. In Proceedings of the 2021 ACM Conference on Fairness, Accountability,
and Transparency, pp. 701‚Äì713, 2021."
REFERENCES,0.5502645502645502,Published as a conference paper at ICLR 2022
REFERENCES,0.5529100529100529,"Shuhan Tan, Yujun Shen, and Bolei Zhou. Improving the fairness of deep generative models without
retraining. arXiv preprint arXiv:2012.04842, 2020."
REFERENCES,0.5555555555555556,"Marco Toldo, Andrea Maracani, Umberto Michieli, and Pietro Zanuttigh. Unsupervised domain
adaptation in semantic segmentation: a review. In arXiv.cs, 2020."
REFERENCES,0.5582010582010583,"Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan
Chandraker. Learning to adapt structured output space for semantic segmentation. In Proc. CVPR,
June 2018."
REFERENCES,0.5608465608465608,"Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, and Luc Van Gool. Unsuper-
vised semantic segmentation by contrasting object mask proposals. In International Conference
on Computer Vision, 2021."
REFERENCES,0.5634920634920635,"Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the GAN
latent space. In Proc. ICML, 2020."
REFERENCES,0.5661375661375662,"Andrey Voynov, Stanislav Morozov, and Artem Babenko. Object segmentation without labels with
large-scale generative models. In International Conference on Machine Learning, pp. 10596‚Äì
10606. PMLR, 2021."
REFERENCES,0.5687830687830688,"Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Perez. Advent: Ad-
versarial entropy minimization for domain adaptation in semantic segmentation. In Proc. CVPR,
June 2019."
REFERENCES,0.5714285714285714,"Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, and Xiang Ruan.
Learning to detect salient objects with image-level supervision. In Proc. CVPR, 2017."
REFERENCES,0.5740740740740741,"T. Wang, A. Borji, L. Zhang, P. Zhang, and H. Lu. A stagewise reÔ¨Ånement model for detecting
salient objects in images. In Proc. ICCV, pp. 4039‚Äì4048, 2017. doi: 10.1109/ICCV.2017.433."
REFERENCES,0.5767195767195767,"P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010."
REFERENCES,0.5793650793650794,"Xide Xia and Brian Kulis. W-net: A deep model for fully unsupervised image segmentation. In
arXiv.cs, 2017."
REFERENCES,0.582010582010582,"Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. Saliency detection via
graph-based manifold ranking. In Proc. CVPR, pp. 3166‚Äì3173. IEEE, 2013."
REFERENCES,0.5846560846560847,"Yu Yang, Hakan Bilen, Qiran Zou, Wing Yin Cheung, and Xiangyang Ji. Unsupervised foreground-
background segmentation with equivariant layered gans. arXiv preprint arXiv:2104.00483, 2021."
REFERENCES,0.5873015873015873,"Yu Zeng, Yunzhi Zhuge, Huchuan Lu, Lihe Zhang, Mingyang Qian, and Yizhou Yu. Multi-Source
Weak Supervision for Saliency Detection. In Proc. CVPR, pp. 6074‚Äì6083, 2019."
REFERENCES,0.58994708994709,"Dingwen Zhang, Junwei Han, and Yu Zhang. Supervision by fusion: Towards unsupervised learning
of deep salient object detector. In Proc. ICCV, Oct 2017a."
REFERENCES,0.5925925925925926,"Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena.
Self-attention generative
adversarial networks. In Proc. ICML, pp. 7354‚Äì7363, 2019."
REFERENCES,0.5952380952380952,"Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash Harandi, and Richard Hartley. Deep Unsupervised
Saliency Detection: A Multiple Noisy Labeling Perspective. In Proc. CVPR, 2018."
REFERENCES,0.5978835978835979,"Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang, and Xiang Ruan. Amulet: Aggregating
multi-level convolutional features for salient object detection. In Proc. ICCV, Oct 2017b."
REFERENCES,0.6005291005291006,"Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang, and Baocai Yin. Learning uncertain
convolutional features for accurate saliency detection. In Proc. ICCV, Oct 2017c."
REFERENCES,0.6031746031746031,"W. Zhu, S. Liang, Y. Wei, and J. Sun. Saliency optimization from robust background detection. In
Proc. CVPR, pp. 2814‚Äì2821, 2014. doi: 10.1109/CVPR.2014.360."
REFERENCES,0.6058201058201058,Published as a conference paper at ICLR 2022
REFERENCES,0.6084656084656085,"W. Zou and N. Komodakis. Harf: Hierarchy-associated rich features for salient object detection. In
Proc. ICCV, pp. 406‚Äì414, 2015. doi: 10.1109/ICCV.2015.54."
REFERENCES,0.6111111111111112,"Yang Zou, Zhiding Yu, B.V.K. Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation
for semantic segmentation via class-balanced self-training. In Proc. ECCV, September 2018."
REFERENCES,0.6137566137566137,Published as a conference paper at ICLR 2022
REFERENCES,0.6164021164021164,"A
APPENDIX"
REFERENCES,0.6190476190476191,"A.1
IMPLEMENTATION DETAILS"
REFERENCES,0.6216931216931217,"A.1.1
OPTIMIZATION"
REFERENCES,0.6243386243386243,"First, we optimize for the latent code vectors vd and vl. We generate latent codes z ‚àºN(0, 1) and
optimize the vector vl (or vd) by gradient descent with the Adam (Kingma & Ba, 2014) optimizer
and learning rate 0.05. We use Œª = 5 for the light direction vl and Œª = ‚àí5 for the dark direction vb.
We perform 1000 optimization steps, by which point vl (or vd) has converged."
REFERENCES,0.626984126984127,"A.1.2
OBJECT SEGMENTATION"
REFERENCES,0.6296296296296297,"To generate data, we sample z ‚àºN(0, 1), produce the mask m = M(z), and reÔ¨Åne the mask as
described in the main paper. We use a simple UNet (Ronneberger et al., 2015) with bilinear down/up-
sampling as our segmentation network. Following (Voynov et al., 2021), we train for 12000 steps
using Adam with learning rate 10‚àí3 and batch size 95, decaying the learning rate by a factor of 0.2
at iteration 8000."
REFERENCES,0.6322751322751323,"A.1.3
SEMANTIC SEGMENTATION"
REFERENCES,0.6349206349206349,"We follow the procedure from (Van Gansbeke et al., 2021), but we use binary masks produced by our
object segmentation model as a drop-in replacement for their saliency detection masks. Under this
procedure, we begin by applying our segmentation network to extract masks from the trainaug
split of PASCAL-VOC (10582 images). We then train a dilated ResNet-50 model with DeepLab-
v3 head, initialized with a pretrained MoCo ResNet-50. As in (Van Gansbeke et al., 2021), this
network has a main head and an auxiliary head: the main head predicts dense pixel-wise features
of dimension 32, and the auxiliary head predicts the object segmentation mask for the purpose of
regularization. We train the output of the main head using a pixel-wise version of the InfoNCE loss.
SpeciÔ¨Åcally, for a feature zi corresponding to a pixel i in the foreground mask Mi, we have:"
REFERENCES,0.6375661375661376,"Li = ‚àílog
exp(zi ¬∑ z+/œÑ)
PK
k=0 exp(zi ¬∑ z‚àí/œÑ)"
REFERENCES,0.6402116402116402,"where z+ is the average embedding of other pixels in the mask Mi, and z‚àíis the average embeding
of pixels in the background:"
REFERENCES,0.6428571428571429,"z+ =
1
|Mi| X"
REFERENCES,0.6455026455026455,"i‚ààMi
zi,
z‚àí=
1
N ‚àí|Mi| X"
REFERENCES,0.6481481481481481,"iÃ∏‚ààMi
zi"
REFERENCES,0.6507936507936508,where N is the number of pixels in the image.
REFERENCES,0.6534391534391535,"We use the same hyperparameters as (Van Gansbeke et al., 2021) for the contrastive learning pro-
cedure. We train for 40 epochs using random crops of size 224, learning rate 0.01, and output
dimension 32. After training, we evaluate with K-Means clustering with 21 clusters (20 seman-
tic categories and 1 background category). These clusters are computed globally over the entire
validation set and matched to the ground truth labels using the Hungarian algorithm."
REFERENCES,0.656084656084656,"We repeat this same procedure identically for a large range of GANs. Figure 9 compares the result
of Ô¨Ånal segmentation networks for different GANs: all images produce sensible segmentations, with
better GANs generally producing better segmentations."
REFERENCES,0.6587301587301587,"A.2
DATASETS"
REFERENCES,0.6613756613756614,"Below, we give statistics of our evaluation datasets. Note that these are only used for evaluation, as
only GAN-generated images are seen during training:"
REFERENCES,0.6640211640211641,Published as a conference paper at ICLR 2022
REFERENCES,0.6666666666666666,"GAN
Dataset
Resolution
mIoU"
REFERENCES,0.6693121693121693,"BigBiGAN
ImageNet
128px
0.345
SAGAN
ImageNet
128px
0.259
SelfCondGAN
ImageNet
128px
0.228
UncondGAN
ImageNet
128px
0.157
ContraGAN
ImageNet
128px
0.153
BigGAN
Tiny-ImageNet
64px
0.124
SNGAN
Tiny-ImageNet
64px
0.095"
REFERENCES,0.671957671957672,"Table 4: Semantic segmentation performance on Pascal-VOC obtained from K-means clustering
of pixelwise semantic features. mIoU is computed over the 20 classes by performing Hungarian
matching between the clusters obtained from K-means and the ground truth. We see that, as with
object segmentation, better GANs (i.e. BigBiGAN, SAGAN) yield better downstream semantic
segmentation performance."
REFERENCES,0.6746031746031746,"CUB
Flowers
DUT-OMRON
DUTS
ECSSD
Acc
IoU
Acc
IoU
Acc
IoU
Acc
IoU
Acc
IoU"
REFERENCES,0.6772486772486772,"vl only
0.912
0.601
0.773
0.479
0.878
0.451
0.890
0.486
0.905
0.663
vd only
0.912
0.631
0.806
0.572
0.842
0.442
0.864
0.478
0.899
0.672
vl and vd
0.921
0.664
0.796
0.541
0.883
0.509
0.893
0.528
0.915
0.713
Ensemble
0.921
0.650
0.799
0.544
0.881
0.492
0.894
0.524
0.917
0.713"
REFERENCES,0.6798941798941799,"Table 6: A comparison of segmentation performance when different directions in the latent space
are used to construct the training segmentation masks."
REFERENCES,0.6825396825396826,"Dataset
Num. Images
Type
Crop"
REFERENCES,0.6851851851851852,"CUB
1000
Object seg.

Flowers
1020
Object seg.

OMRON
5168
Saliency det.

DUTS
5019
Saliency det.

ECSSD
1000
Saliency det.
"
REFERENCES,0.6878306878306878,Table 5: Evaluation dataset statistics
REFERENCES,0.6904761904761905,"A.3
ADDITIONAL ABLATIONS"
REFERENCES,0.6931216931216931,"Ablation: Comparing latent directions.
In addition to comparing the networks resulting from
vl and vb, we also compare the actual latent directions vl and vb. Due to the nonlinearity of the
generator function, the optimal unit directions vl and vd are not necessarily negations of one an-
other; indeed, we found in practice that they were close to but not exactly antiparallel. Table 9 in
Section A.3 gives exact numbers for a variety of different generator architectures."
REFERENCES,0.6957671957671958,"Ablation over Œª.
The most notable hyperparameter in the optimization stage of our method is Œª,
which controls the trade-off between brightness and consistency. Table 7 compares segmentation
results for BigBiGAN with Œª = 10, 5, 2.5, and 1.25, with 5 performing best."
REFERENCES,0.6984126984126984,"Ablation over œµ.
In Table 8, we show ablation results for changing œµ during the optimization
process. Note that since the GAN used in this set of experiments (BigBiGAN) has a 120-dimensional
latent space, the distribution of the norm of the N(0, 1) latent vectors used to train the GAN is
concentrated around (approximately) 11. That is to say, a shift of magnitude œµ = 6 in the latent
space is very large."
REFERENCES,0.701058201058201,"Ablation: Random initialization.
Given the importance of the latent direction optimization pro-
cedure in our segmentation pipeline, it is natural to ask whether different initializations yield mate-
rially different directions in the latent space. To answer this question, we optimized ten vectors v on"
REFERENCES,0.7037037037037037,Published as a conference paper at ICLR 2022
REFERENCES,0.7063492063492064,"CUB
Flowers
DUT-OMRON
DUTS
ECSSD
Acc
IoU
Acc
IoU
Acc
IoU
Acc
IoU
Acc
IoU"
REFERENCES,0.708994708994709,"Œª = 10
0.911
0.631
0.794
0.550
0.849
0.455
0.874
0.498
0.899
0.677
Œª = 5
0.919
0.658
0.782
0.506
0.880
0.498
0.891
0.524
0.912
0.703
Œª = 2.5
0.818
0.418
0.728
0.456
0.762
0.311
0.765
0.311
0.792
0.467
Œª = 1.25
0.791
0.385
0.713
0.449
0.740
0.296
0.743
0.296
0.771
0.446"
REFERENCES,0.7116402116402116,"Table 7: A comparison of segmentation performance for a BigBiGAN model when different values
of Œª are used to Ô¨Ånd the latent vectors vl and vb in the optimization stage. Higher values of Œª
yield latent directions v that produce shifted images with greater variance in brightness between the
center and outside pixels. Conversely, lower values of Œª yield latent directions v that produce shifted
images that align better to the original images."
REFERENCES,0.7142857142857143,"CUB
Flowers
DUT-OMRON
DUTS
ECSSD
Acc
IoU
Acc
IoU
Acc
IoU
Acc
IoU
Acc
IoU"
REFERENCES,0.716931216931217,"œµ = 1
0.911
0.600
0.744
0.600
0.867
0.454
0.880
0.479
0.897
0.650
œµ = 2
0.912
0.601
0.773
0.479
0.878
0.451
0.890
0.486
0.905
0.663
œµ = 4
0.843
0.435
0.617
0.435
0.763
0.290
0.775
0.297
0.779
0.419
œµ = 6
0.761
0.347
0.602
0.347
0.714
0.236
0.709
0.238
0.724
0.349"
REFERENCES,0.7195767195767195,"Table 8: A comparison of segmentation performance for a BigBiGAN-based model when differ-
ent values of œµ are used to Ô¨Ånd the latent vector vl in the optimization stage. Higher values of œµ
correspond to a greater-magnitude shift in the latent space during optimization."
REFERENCES,0.7222222222222222,"BigBiGAN from different initializations and measured their pairwise cosine similarities. We found
that the cosine similarities had mean 0.9866 with standard deviation 0.0044, which is extremely
small relative to the values in Table 9. Thus, the random initialization of the optimization procedure
does not have a large impact on the Ô¨Ånal direction."
REFERENCES,0.7248677248677249,"Ablation: On the necessity of the center prior.
Here, we conduct a small experiment to inves-
tigate the necessity of the central prior in the optimization step of our segmentation pipeline. We
replace it with a spatially-agnostic loss term with no central prior: it simply encourages the shifted
image to have high variance in brightness. We Ô¨Ånd that this spatially-agnostic loss still yields good
results, although not quite as good as the spatial prior loss. Numerical results are shown in Table 10."
REFERENCES,0.7275132275132276,"A.4
ADDITIONAL RELATED WORK"
REFERENCES,0.7301587301587301,"Saliency Detection.
Object segmentation is closely related to saliency detection, the problem of
Ô¨Ånding signiÔ¨Åcant (salient) objects in an image. Although most saliency detection models are trained
with pixel-level supervision (or human eye movement data), the past few years have seen some re-
search into unsupervised/weakly-supervised saliency detection (Zhang et al., 2018; Nguyen et al.,
2019; Zeng et al., 2019). These methods work by ensembling strong hand-crafted priors and dis-
tilling them into a deep network. In practice, they also initialize their networks with pretrained
(supervised) image classiÔ¨Åers or semantic segmentation networks."
REFERENCES,0.7328042328042328,"Learning from Synthetic Data.
Finally, our method can be viewed from the perspective of learn-
ing from synthetic data. Motivated by the costly and time-consuming nature of data labeling, there
has been a plethora of recent work on synthesizing, training with, and adapting from synthetic
datasets. For example, one widely-studied line of research (Hoffman et al., 2018; Tsai et al., 2018;
Zou et al., 2018; Vu et al., 2019; Toldo et al., 2020) tackles the task of semantic segmentation by
training on data generated from video games (e.g. GTA5). With regard to adversarially-generated
training data speciÔ¨Åcally (Shrivastava et al., 2017) uses a GAN-like network to enhance the realism
of synthetic images while preserving label information."
REFERENCES,0.7354497354497355,"Although we train our segmentation network using generated images only, we show in the exper-
iments below that it generalizes to real-world images without the need for additional adaptation.
Such additional adaptation could be an interesting avenue for future research."
REFERENCES,0.7380952380952381,Published as a conference paper at ICLR 2022
REFERENCES,0.7407407407407407,"GAN
vT
s vb
BigBiGAN
-0.4376
SelfCondGAN
-0.7854
UncondGAN
-0.3522
ContraGAN
-0.3297
SAGAN
-0.4648"
REFERENCES,0.7433862433862434,"Table 9: The dot product of the optimized foreground-lighter (vl) and foreground-darker latent di-
rections (vb) for different generators, all of which have a 120-dimensional latent space. Across the
board, the directions are almost but not exactly antiparallel (random vectors in this space have an
expected dot product of 0 with variance
1
120)."
REFERENCES,0.746031746031746,"CUB
Flowers
DUT-O.
DUTS
ECSSD"
REFERENCES,0.7486772486772487,"Acc.
0.885
0.788
0.825
0.849
0.876
IoU
0.535
0.553
0.379
0.416
0.609
maxFŒ≤
0.672
0.689
0.473
0.526
0.728"
REFERENCES,0.7513227513227513,Table 10: Ablation results for the spatially-agnostic spatial mask
REFERENCES,0.753968253968254,"A.5
ADDITIONAL EXAMPLES"
REFERENCES,0.7566137566137566,"A.5.1
EXAMPLES ACROSS DATASETS"
REFERENCES,0.7592592592592593,"In Figure 6, Figure 7,aand Figure 8, we show the results of applying our Ô¨Ånal segmentation network
to random images from each of the Ô¨Åve datasets on which we evaluated."
REFERENCES,0.7619047619047619,"A.5.2
EXAMPLES ACROSS GENERATORS"
REFERENCES,0.7645502645502645,"In Figure 9, we show examples of pairs of generated images and the corresponding extracted samples
for a range of different GANs."
REFERENCES,0.7671957671957672,"A.5.3
SEMANTIC SEGMENTATION EXAMPLES"
REFERENCES,0.7698412698412699,"In Figure 10, we provide randomly selected examples of our semantic segmentations after K-Means
clustering on the PASCAL-VOC dataset. We see that our masks are of similar visual quality to
those from MaskContrast (Van Gansbeke et al., 2021), despite the fact that our method is entirely
unsupervised."
REFERENCES,0.7724867724867724,Published as a conference paper at ICLR 2022
REFERENCES,0.7751322751322751,BigBiGAN
REFERENCES,0.7777777777777778,SelfCondGAN
REFERENCES,0.7804232804232805,ContraGAN SAGAN
REFERENCES,0.783068783068783,StyleGAN
REFERENCES,0.7857142857142857,(AHFQ)
REFERENCES,0.7883597883597884,Perturbation Amount ( ùú∫ )
REFERENCES,0.791005291005291,0.0                2.0                 4.0                 8.0
REFERENCES,0.7936507936507936,"Figure 5: Examples of perturbed images generated by our method for Ô¨Åve different generators
(GANs). Note that the Ô¨Ånal generator, StyleGAN (Karras et al., 2019), is only trained on close-up
portraits of animals, and thus cannot be used for general-perpose image segmentation. Nonetheless,
our method successfully identiÔ¨Åes the foreground and background of the generated animal portraits."
REFERENCES,0.7962962962962963,Published as a conference paper at ICLR 2022
REFERENCES,0.798941798941799,"CUB
Flowers"
REFERENCES,0.8015873015873016,"Figure 6: Examples of the Ô¨Ånal segmentation network across evaluation datasets. From left to right:
original image, ground truth, prediction."
REFERENCES,0.8042328042328042,Published as a conference paper at ICLR 2022
REFERENCES,0.8068783068783069,"DUT-OMRON
DUTS"
REFERENCES,0.8095238095238095,"Figure 7: Examples of the Ô¨Ånal segmentation network across evaluation datasets. From left to right:
original image, ground truth, prediction."
REFERENCES,0.8121693121693122,Published as a conference paper at ICLR 2022 ECSSD
REFERENCES,0.8148148148148148,"Figure 8: Examples of the Ô¨Ånal segmentation network across evaluation datasets. From left to right:
original image, ground truth, prediction."
REFERENCES,0.8174603174603174,Published as a conference paper at ICLR 2022
REFERENCES,0.8201058201058201,"BigBiGAN  (ImageNet)
SNGAN (ImageNet)"
REFERENCES,0.8227513227513228,"SAGAN (ImageNet)
BigGAN (ImageNet)"
REFERENCES,0.8253968253968254,"SelfCondGAN (ImageNet)
ContraGAN  (ImageNet)
SNGAN (TinyImageNet)"
REFERENCES,0.828042328042328,SAGAN (TinyImageNet)
REFERENCES,0.8306878306878307,BigGAN  (TinyImageNet)
REFERENCES,0.8333333333333334,"Figure 9: A comparison of perturbed images and their corresponding masks for many different
generators."
REFERENCES,0.8359788359788359,Published as a conference paper at ICLR 2022
REFERENCES,0.8386243386243386,"Image
Ground Truth
MaskContrast
Ours
Image
Ground Truth
MaskContrast
Ours"
REFERENCES,0.8412698412698413,"Figure 10: Examples of semantic segmentations after K-Means clustering on the PASCAL-VOC
dataset. We see that our masks are of similar visual quality to those from MaskContrast (Van Gans-
beke et al., 2021), despite the fact that our method is entirely unsupervised. Note that these images
are randomly selected, not cherry-picked."
REFERENCES,0.843915343915344,Published as a conference paper at ICLR 2022
REFERENCES,0.8465608465608465,"B
FURTHER EXPERIMENTS"
REFERENCES,0.8492063492063492,"Based on the helpful comments of our reviewers, we have conducted new experiments and created
more visualizations to better understand how our method functions and performs."
REFERENCES,0.8518518518518519,"B.1
ADDITIONAL EXPERIMENTS ON FLOWERS"
REFERENCES,0.8544973544973545,"In this section, we provide a comprehensive explanation and analysis of our performance on the
Flowers dataset."
REFERENCES,0.8571428571428571,"The Flowers dataset is different from the other datasets investigated in our paper (CUB, DUTS,
DUT-OMRON, ECSSD, PASCAL VOC) in that the ‚Äúground-truth‚Äù masks for Flowers were obtained
using an automated segmentation method rather than human annotations. As a result, the ground
truth are very unreliable. In fact, there are many instances in the Flowers dataset in which the
ground truth mask is completely or nearly completely empty despite there clearly being a Ô¨Çower
in the image. This issue is also mentioned by Chen et al. (2019), who claim that their method
‚Äúprovide[s] better masks‚Äù than the ground-truth in cases of disagreement."
REFERENCES,0.8597883597883598,"Below, we present an additional experiment to improve our performance on Flowers. We switch
the underlying GAN in our method from BigBiGAN (trained on ImageNet) to an unconditional
StyleGAN2 trained on Flowers."
REFERENCES,0.8624338624338624,"This experiment aims to show that our method works well on in-domain data (i.e. using a GAN
trained on Flowers to segment Flowers) in addition to out-of-domain data (i.e. using a GAN trained
on ImageNet to segment Flowers)."
REFERENCES,0.8650793650793651,"SpeciÔ¨Åcally, we found publicly available weights on GitHub for an (unconditional) StyleGAN2
model Karras et al. (2019) trained on the Flowers dataset at a resolution of 256px, and we applied
our method directly to this model. We use perturbation radius r = 0.2 rather than r = 2.0 as
in BigBiGAN because the latent space of StyleGAN2 has a different scale. Apart from this sin-
gular parameter, however we apply our method without any hyperparameter tuning or additional
modiÔ¨Åcations for the new StyleGAN2."
REFERENCES,0.8677248677248677,"Results are shown in Table 11. We Ô¨Ånd that using StyleGAN2 dramatically improves the visual
quality of our segmentations both quantitatively and qualitatively. Quantitatively, our performance
under this setup is similar to other unsupervised methods which have been extensively Ô¨Ånetuned for
this task, such as Chen et al. (2019)."
REFERENCES,0.8703703703703703,"We show examples of failure cases in Fig. 13. The most severe failure cases involve empty ground-
truth masks. Similarly to ReDo, we Ô¨Ånd that when our method and the ground truth disagree, our
segmentations are often visually superior. At this level of performance, the high level of label noise
means that exact mIoU/accuracy numbers are not meaningful. However, there is no doubt that
switching generators to StyleGAN tremendously improves our Ô¨Çower segmentation performance."
REFERENCES,0.873015873015873,"Additionally, we sought to identify the extent to which this improvement was due purely to the
increased resolution (256px) as opposed to the underlying GAN. We train a segmentation model
128px-downsampled versions of the StyleGAN2 generated images and show the results in Table 15.
Higher resolution training improves performance modestly on the Flowers dataset, while the bulk
of the improvement is due to the use of StyleGAN. This result supports our hypothesis that better
GANs correspond to better unsupervised object segmentation performance under our model."
REFERENCES,0.8756613756613757,"B.2
ADDITIONAL EXPERIMENTS ON FACES"
REFERENCES,0.8783068783068783,"In this section, we provide additional experiments on the domain of face images."
REFERENCES,0.8809523809523809,"This section is designed with comparison to Abdal et al. (2021) in mind. However, it is difÔ¨Åcult
to compare with Abdal et al. (2021) because they evaluate on non-standard datasets, they do not
release any information about their data splits, and they do not release code or pretrained models.
In particular, they conduct evaluations on CelebA-HQ-Mask using 1000 randomly selected images
from the dataset, but they do not specify which images these are. Nevertheless, we tried to recreate
their setup as best as possible to establish a comparison."
REFERENCES,0.8835978835978836,Published as a conference paper at ICLR 2022
REFERENCES,0.8862433862433863,"We applied our method to a StyleGAN2 trained on FFHQ. We use resolution 256px due to time
constraints (and 1024px would likely improve results)."
REFERENCES,0.8888888888888888,"We use a perturbation radius r = 0.2, which is copied directly from the experiment in the section
above (because both experiments use StyleGAN2). Again, with the exception of this single param-
eter, we did not change a single other hyperparameter in our entire learning setup to demonstrate
the generalizability of our approach."
REFERENCES,0.8915343915343915,"Results are shown in Table 12. Qualitatively, we Ô¨Ånd that our model tends to segment the hair and
face but not the clothes, which is an equally valid segmentation but does not align with the ground
truth where the person‚Äôs body is included in the foreground mask. This result could mean that
StyleGAN2 internally models the clothing in a different manner from the face which would lead to
this behavior in latent space."
REFERENCES,0.8941798941798942,"B.3
DATA SIZE"
REFERENCES,0.8968253968253969,"In this section, we investigate the relationship between the amount of synthetic training data and
segmentation performance. All experiments in the main paper use 1,000,000 generated examples."
REFERENCES,0.8994708994708994,"We train models for the same number of total iterations using a varying number of synthetic training
images from 100 to 1,000,000. Results are shown in Table 14 and Fig. 11. We Ô¨Ånd consistently
across all datasets that - similar to supervised learning - performance scales logarithmically with the
dataset size. Differently from supervised learning, our labels are entirely free to generate, so we can
scale our synthetic dataset as large as desired."
REFERENCES,0.9021164021164021,"B.4
SEMI-SUPERVISED LEARNING"
REFERENCES,0.9047619047619048,"In this section, we investigate whether our synthetic data can be used to augment existing supervised
datasets in a semi-supervised learning setup."
REFERENCES,0.9074074074074074,"We consider a subset of the CUB dataset with 1000 labeled examples along with synthetic data from
our BigBiGAN (ImageNet-pretrained) model. Results are shown in Table 13."
REFERENCES,0.91005291005291,"First, we train a supervised model on the 1000 images with ground truth segmentations as a baseline.
Similarly, we train a fully unsupervised model on our generated data alone. Now, in the semi-
supervised setting, we have access to 1000 labeled images. When training on the combination of
both datasets we can already see an improvement over both baselines in IoU (which is the harder
and less saturated metric). With access to some in-domain data, we can also take another step to
reduce the domain gap between the generated and real images: we can Ô¨Ålter our generated images
by their distance to the supervised samples. SpeciÔ¨Åcally, we select the 50000 nearest neighbors in
our generated set (according to cosine similarity using a self-supervised ResNet-50 He et al. (2019)),
and we refer to these as the ‚ÄúkNN generated images‚Äù in Table 13."
REFERENCES,0.9126984126984127,"We see that a model trained on the combined ground truth and kNN samples surpasses both baselines
and the supervised model. This result demonstrates that our pipeline can play a valuable role in
semi-supervised object segmentation in addition to unsupervised object segmentation."
REFERENCES,0.9153439153439153,"B.4.1
FAILURE CASES"
REFERENCES,0.917989417989418,"In this section, we describe common failure cases of our models."
REFERENCES,0.9206349206349206,"Fig. 12, Fig. 13, and Fig. 14 show examples of failure cases of our model."
REFERENCES,0.9232804232804233,"One of the main failure cases of our method is to segment additional foreground objects beyond
the ‚Äúmain object‚Äù in the image. For example, on the CUB dataset (Fig. 12), we often segment
objects such as branches and bird feeders alongside the bird in the image. Across all datasets, one
of the other main sources of errors is the object boundary that is not as precise as the object. This
could potentially be improved in the future through the use of higher-resolution GANs or by post-
processing (e.g. with a CRF)."
REFERENCES,0.9259259259259259,"The Flowers dataset is a special case in which the largest source of error is label noise in the ground
truth annotations. Occasionally, our method misses to segment a Ô¨Çower in the background or seg-
ments too much of the stem."
REFERENCES,0.9285714285714286,Published as a conference paper at ICLR 2022
REFERENCES,0.9312169312169312,"With regard to failure cases of mask generation, since there is no ground-truth associated with these
generated images, these failure cases were found by manually Ô¨Åltering approximately 500 generated
images (Fig. 14). Common failure cases include segmenting too much or too little of the image
and are often attributable to low-quality images generated by the GAN. Thus, improvements in the
generation quality of GANs and the performance of our method are closely linked."
REFERENCES,0.9338624338624338,Published as a conference paper at ICLR 2022
REFERENCES,0.9365079365079365,"Data
Acc
IoU"
REFERENCES,0.9391534391534392,"(Chen et al., 2019)
Flowers
0.879
0.764
Ours (BigBiGAN)
ImageNet
0.796
0.541
Ours (StyleGAN)
Flowers
0.882
0.723"
REFERENCES,0.9417989417989417,"Table 11: Performance on the Flowers dataset compared to prior methods. It is important to em-
phasize the fact that the ‚Äúground truth‚Äù of the Flowers dataset was generated using an automatic
procedure and is extremely noisy. For example, a signiÔ¨Åcant fraction of the ground truth masks are
entirely empty, and those that are not empty often do not properly reÔ¨Çect the content of the image. As
a result, quantitative numbers on the Flowers dataset should be heavily discounted. We encourage
the reader to see Fig. 13 for visualizations of failure cases."
REFERENCES,0.9444444444444444,"Acc
IoU"
REFERENCES,0.9470899470899471,"Labels4Free (StyleGAN - UNet)
0.91
0.82
Ours (StyleGAN - UNet)
0.85
0.80"
REFERENCES,0.9497354497354498,"Table 12: Performance on 1000 random images from the CelebA-HQ-Mask dataset. Performance is
not exactly comparable because these models were evaluated using different random subsets."
REFERENCES,0.9523809523809523,"Real Images
Gen. Images
Acc
IoU
Real images only
1000
0
0.924
0.586
All generated images only
0
1000000
0.906
0.616
kNN generated images only
0
50000
0.908
0.588
Combined: real images and all generated images
1000
1000000
0.914
0.628
Combined: real images and kNN generated images
1000
50000
0.931
0.665"
REFERENCES,0.955026455026455,"Table 13: Results of combining real images with generated images on the CUB dataset. We use a
subset of 1000 training images from CUB along with 1,000,000 generated images. Additionally, we
evaluate using a Ô¨Åltered subset of our generated images containing the 50,000 nearest neighbors to
the real images. The distance between images was computed using a self-supervised ResNet-50 He
et al. (2019). We see that combining real data points with nearby generated data points gives the
best results."
REFERENCES,0.9576719576719577,Published as a conference paper at ICLR 2022
REFERENCES,0.9603174603174603,"Num. Images
CUB
Flowers
DUTS
OMRON
ECSSD
Acc
IoU
Acc
IoU
Acc
IoU
Acc
IoU
Acc
IoU"
REFERENCES,0.9629629629629629,"1000000
0.923
0.653
0.798
0.540
0.890
0.524
0.875
0.492
0.914
0.709
300000
0.906
0.616
0.769
0.480
0.876
0.498
0.859
0.460
0.901
0.674
200000
0.903
0.606
0.784
0.508
0.868
0.473
0.852
0.440
0.898
0.659
100000
0.900
0.596
0.762
0.467
0.861
0.443
0.845
0.408
0.881
0.617
10000
0.874
0.486
0.740
0.412
0.849
0.374
0.835
0.345
0.848
0.505
1000
0.847
0.419
0.700
0.343
0.821
0.311
0.811
0.299
0.802
0.410
100
0.752
0.256
0.650
0.284
0.727
0.203
0.728
0.206
0.710
0.271"
REFERENCES,0.9656084656084656,"Table 14: Performance of our method for different numbers of generated images. All models are
trained for the same number of iterations (20000). For a visual representation of these numbers, see
Fig. 11."
REFERENCES,0.9682539682539683,"Resolution
Acc
max FŒ≤
IoU
128
0.878
0.795
0.715
256
0.882
0.798
0.723"
REFERENCES,0.9708994708994709,"Table 15: Performance of our method on the Flowers dataset using a StyleGAN 2 model trained
on Flowers for different input image resolutions. Using a higher resolution improves performance
slightly."
REFERENCES,0.9735449735449735,"Figure 11: Here we plot the average average mIoU score of a segmentation model as we vary the
number of synthetic training images and masks generated using our method. We see that model
performance consistently improves with the number of generated training images, following a log-
linear scale."
REFERENCES,0.9761904761904762,Published as a conference paper at ICLR 2022
REFERENCES,0.9788359788359788,"Image
Prediction
Ground Truth
Image
Prediction
Ground Truth"
REFERENCES,0.9814814814814815,"Figure 12: Examples of failure cases of our method on the test set of CUB. Differently from the
ground truth, our method frequently segments foreground objects that are not birds, such as branches
and bird feeders. This reÔ¨Çects the fact that our model captures general foreground structure in images
and has not been trained on CUB."
REFERENCES,0.9841269841269841,Published as a conference paper at ICLR 2022
REFERENCES,0.9867724867724867,"Image
Prediction
Ground Truth
Image
Prediction
Ground Truth"
REFERENCES,0.9894179894179894,Figure 13: Examples of failure cases on Flowers in which the ground-truth mask is not empty.
REFERENCES,0.9920634920634921,Published as a conference paper at ICLR 2022
REFERENCES,0.9947089947089947,"Image
Mask
Image
Mask
Image
Mask"
REFERENCES,0.9973544973544973,"Figure 14: Examples of failure cases for GAN-generated images and masks. As there is no ground-
truth associated with these generated images, these failure cases were found by manually Ô¨Åltering
approximately 500 generations."
