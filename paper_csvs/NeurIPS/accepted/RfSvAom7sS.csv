Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0022727272727272726,"Causal discovery is a fundamental problem with applications spanning various areas
in science and engineering. It is well understood that solely using observational
data, one can only orient the causal graph up to its Markov equivalence class,
necessitating interventional data to learn the complete causal graph. Most works in
the literature design causal discovery policies with perfect interventions, i.e., they
have access to infinite interventional samples. This study considers a Bayesian
approach for learning causal graphs with limited interventional samples, mirroring
real-world scenarios where such samples are usually costly to obtain. By leveraging
the recent result of Wienöbst et al. [2023] on uniform DAG sampling in polynomial
time, we can efficiently enumerate all the cut configurations and their corresponding
interventional distributions of a target set, and further track their posteriors. Given
any number of interventional samples, our proposed algorithm randomly intervenes
on a set of target vertices that cut all the edges in the graph and returns a causal graph
according to the posterior of each target set. When the number of interventional
samples is large enough, we show theoretically that our proposed algorithm will
return the true causal graph with high probability. We compare our algorithm
against various baseline methods on simulated datasets, demonstrating its superior
accuracy measured by the structural Hamming distance between the learned DAG
and the ground truth. Additionally, we present a case study showing how this
algorithm could be modified to answer more general causal questions without
learning the whole graph. As an example, we illustrate that our method can be used
to estimate the causal effect of a variable that cannot be intervened."
INTRODUCTION,0.004545454545454545,"1
Introduction"
INTRODUCTION,0.006818181818181818,"Causal discovery refers to learning the underlying causal graph of a data generating process using a
combination of observational and interventional data. This is a fundamental problem with applications
across various areas including economics, genomics, meteorology, could computing, and etc. [Pearl,
2009, Hoover, 1990, King et al., 2004, Ikram et al., 2022, Runge et al., 2019]. In recent decades, the
technological progress has facilitated the accumulation of vast quantities of observational data, i.e.,
data collected without perturbing the underling causal mechanisms. However, with only observational
data, one can only recover the causal graph up to its Markov equivalence class (MEC) [Verma, 1991,
Andersson et al., 1997]. In general, interventional data, i.e., data collected after a perturbation in"
INTRODUCTION,0.00909090909090909,"the system, is needed to learn the whole graph. Therefore, many recent works aim to use both
observational and interventional data [Choo et al., 2022, Squires et al., 2020, Shanmugam et al., 2015,
He and Geng, 2008, Hauser and Bühlmann, 2014]."
INTRODUCTION,0.011363636363636364,"These works use perfect do(X) intervention [Pearl, 2009] on a set of intervention targets X and
can be categorized along various dimensions. One such dimension is whether they are adaptive or
non-adaptive. Non-adaptive approaches [Shanmugam et al., 2015], determine the intervention targets
before any interventions (experiments), whereas adaptive algorithms [Squires et al., 2020, Choo et al.,
2022] can suggest the next intervention targets based on previous experiments. Non-adaptive methods
can parallelize experiments, while adaptive ones are sequential but demand fewer interventions to
learn the entire graph [Choo and Shiragur, 2023]. These works provide theoretic guarantees that
the experiments are sufficient to learn the graph. Jiang and Aragam [2024] established conditions
under which latent causal graphs are nonparametrically identifiable and can be learned from unknown
interventions."
INTRODUCTION,0.013636363636363636,"Nevertheless, these studies operate under the assumption that an infinite amount of interventional
data can be gathered after each intervention. However, in most real-world scenarios, collecting
interventional data is considerably more challenging. For instance, while modern single-cell RNA
sequencing technologies have facilitated the collection of vast amounts of gene expression data
for downstream tasks [Stuart and Satija, 2019, Zhou et al., 2022], accurately perturbing each gene
remains difficult [Chandrasekaran et al., 2024, Uhler and Shivashankar, 2017, Sharma et al., 2022].
Consequently, practical applications tend to favor interventional methods that require fewer data,
although this aspect receives less attention. To bridge this disparity, our work assumes access to an
infinite oracle of observational data, while only a finite number of interventional samples can be
acquired for each intervention."
INTRODUCTION,0.015909090909090907,"Among those that consider a limited number of interventional samples, the majority employ a
Bayesian approach. This method offers several advantages, including the ability to make finer
distinctions among different graphs and resilience to erroneous categorical decisions in the face of
limited interventional samples. These studies can be categorized based on whether they assume a
parametric structural causal model (SCM). Many works, such as those by Heckerman et al. [1997],
Annadani et al. [2024], Toth et al. [2022], make certain assumptions about the SCM, such as additive
Gaussian noise or linear causal models. However, their results will be inaccurate if the underlying
SCM does not adhere to these assumptions. In contrast, the only non-parametric work [Greenewald
et al., 2019] assumes that the ground truth causal graph is a forest of trees. Each undirected
component of a tree with n nodes contains n Directed Acyclic Graphs (DAGs) in its MEC, which
facilitates the tracking of the posterior of each possible DAG. For general undirected graphs, the MEC
size could be exponential to n [He et al., 2015, Meek, 1995] which makes the Bayesian posterior
updating intractable. In this study, we adopt a non-parametric approach to mitigate the risk of
erroneous outcomes resulting from assumptions about the SCM while mediating the problem of
exponentially many DAGs in the MEC. We additionally assume causal sufficiency and faithfulness.
Causal sufficiency asserts the absence of latent confounders, while faithfulness implies that every
conditional independence relation that holds in the probabilistic model is entailed by the d-separation
property of the causal graph. Given these assumptions, the PC algorithm [Spirtes et al., 2001] can
be employed to identify all v-structures in the causal graph, followed by the application of Meek
Rules [Meek, 1995] to derive the essential graph. The subgraph induced by the unoriented edges in
the essential graph comprises multiple chordal chain components and can be oriented independently
from other unoriented chain components and the oriented subgraph [Andersson et al., 1997]. Given
our assumption of infinite observational data and that PC algorithm and Meek Rules are complete,
i.e., they orient all the edges that can be identified from the data, our focus lies on fully orienting the
undirected chordal chain graphs (UCCG) of the essential graph of the ground truth causal graph. The
main contributions of our work are listed below:"
INTRODUCTION,0.01818181818181818,"• We study the causal discovery problem with limited interventional samples and propose an
algorithm to solve this problem in an Bayesian approach. We analyze this algorithm and
show in theory that it can learn the true causal graph with a high probability given a large
enough number of interventional samples and calculate the convergence rate."
INTRODUCTION,0.020454545454545454,"• We conduct experiments on simulated datasets with random chordal DAGs to compare our
algorithm with other baseline methods. The results show that our algorithm outperforms
other baselines in that it takes fewer interventional samples to receive the same accuracy."
INTRODUCTION,0.022727272727272728,"Additionally, the performance of our algorithm is stable across different settings (order and
density of the random DAGs)."
INTRODUCTION,0.025,"• We discuss how we can modify our algorithm to answer more general causal questions
without learning the whole graph using DAG sampling from the MEC. We show how we
can estimate the posterior of a set to be the backdoor adjustment set of a causal query
given the essential graph and some interventional data as an example. We further show
through simulated experiments that we can modify our algorithm to estimate causal effects
of variables that cannot be intervened."
INTRODUCTION,0.02727272727272727,"Outline of the paper: In Section 2, we summarize the related works. In Section 3, we state the
preliminary notations and background knowledge. In Section 4, we describe how we setup the
initialization steps of the main algorithm. In Section 5, we describe the algorithm with details and
analysis on the convergence to the ground truth. In Section 6, we discuss how our algorithm can be
modified to answer more general causal questions with an example. In Section 7, we present the
experiment results on simulated datasets and compare with baselines. In Section 8, we conclude with
discussion of limitations and future extensions."
RELATED WORKS,0.029545454545454545,"2
Related Works"
RELATED WORKS,0.031818181818181815,"We can roughly divide the causal discovery methods into 3 categories: non-adaptive, adaptive, and
Bayesian. Non-adaptive methods design intervention targets for each experiment before retrieving
interventional samples. Since no information is shared between experiments, they can be conducted
in parallel. Eberhardt [2007] discussed non-adaptive methods as fixed searching strategies and
they show that in the worst-case scenaris O(log n) size n"
INTERVENTIONS ARE NECESSARY TO ORIENT THE,0.03409090909090909,"2 interventions are necessary to orient the
whole causal graph, regardless of the adaptivity of the algorithm. Hyttinen et al. [2013] show the
connection between orienting causal graph via intervention and the concept of separating system
from combinatorics. Ghassami et al. [2018] studied the problem of orienting the maximum expected
number of edges with a budgeted number of size 1 interventions. Kocaoglu et al. [2017], Lindgren
et al. [2018] studied the problem of non-adaptive causal graph learning while minimizing intervention
costs when each vertex is assigned with a different intervention cost. Many of the early works
in causal discovery are non-adaptive, and therefore they construct the theoretical basis of causal
discovery algorithm. Separating system is also used in our proposed method."
INTERVENTIONS ARE NECESSARY TO ORIENT THE,0.03636363636363636,"Adaptive methods make the decision of next intervention based on the results of previous experiments.
Less interventions are usually required than non-adaptive counterparts. Eberhardt [2007] compares
adaptive strategies with non-adaptive ones and propose a heuristic algorithm. He and Geng [2008]
proposed MinMaxMEC and MaxEntropy algorithms that select the next intervention target that
minimizes the maximum I-MEC size or maximizes the intervention entropy respectively. Hauser
and Bühlmann [2014] proposed OptSingle which calculates the target that minimizes the maximum
number of unoriented edges in the interventional graph. Squires et al. [2020] show that any algorithm
would take at least ⌈ω"
INTERVENTIONS ARE NECESSARY TO ORIENT THE,0.038636363636363635,"2 ⌉size 1 interventions to recover the whole graph and proposed a two-stage
active learning algorithm. In the first stage, it finds the directed clique tree representation of the
unoriented graph and orient the residuals separately in the second stage. Choo et al. [2022] show
that a size 1 invervention set can orient the whole graph if and only if it cuts all the covered edges in
the ground truth DAG, and propose a learning algorithm that intervenes on a 1/2−separator in each
experiment to iteratively decrease the size of the currently unoriented components."
INTERVENTIONS ARE NECESSARY TO ORIENT THE,0.04090909090909091,"The aforementioned works all assume the existence of an infinite intervention oracle. The Bayesian
approach is usually used when this assumption is untenable. Heckerman et al. [1997] investigated
learning Structural Causal Models (SCMs) under the assumption that each local likelihood is a
combination of multinomial distributions and all variables are discrete. Similarly, Buntine [1994]
discussed simple linear local likelihoods involving both discrete and continuous variables. Tong
and Koller [2001] proposed an active learning algorithm for Bayesian network learning. Previous
studies typically employed Markov Chain Monte Carlo (MCMC) for sampling Directed Acyclic
Graphs (DAGs) to estimate posterior distributions over DAGs and function parameters, which grow
exponentially with the size of the model. Acharya et al. [2018] studied the problem of distinguishing
two causal models on the same graph with limited interventional samples. Nishikawa-Toomey et al.
[2022] uses variational inference and GFlowNets [Bengio et al., 2023] to learn and parameterize
the joint posterior distribution over DAGs and mechanisms while assuming linear model and equal"
INTERVENTIONS ARE NECESSARY TO ORIENT THE,0.04318181818181818,"noise variances. Recent researches [Lorch et al., 2021, Charpentier et al., 2022, Cundy et al., 2021,
Toth et al., 2022, Hägele et al., 2023] started leveraging gradient information for more efficient
inference, while they suffer from poor inference quality. These studies all make specific parametric
assumptions regarding functions or noises in the SCM, which lead to incorrect outcomes when these
assumptions are violated. Kuipers et al. [2022] combines constraint-based methods and MCMC
methods to efficiently search DAGs, but no guarantee of convergence is provided. To circumvent
such issues, Greenewald et al. [2019] directly update the posterior of a specific subgraph containing
the root when the graph is a tree without any constraint on the SCM. However in practice, causal
graphs are usually more complicated than trees, which motivates our work."
PRELIMINARIES,0.045454545454545456,"3
Preliminaries"
PRELIMINARIES,0.04772727272727273,"A causal graph D = (V, E) is a directed acyclic graph (DAG) where the vertex set V represents
a group of random variables. In the context of D, a directed edge (X, Y ) ∈E from variable X to
variable Y (denoted as X →Y ) signifies that X serves as an immediate parent of Y . We denote the
parent set of variable Y by pa(Y ). The cut at vertex set X, denoted as E[X, V \ X], constitutes the
set of edges between X and some vertex in the set V \ X. According to the Markov assumption,
the joint distribution can be decomposed as P(v) = Qn
i=1 P(vi|pa(Xi)). A causal graph entails
specific conditional independence (CI) relationships among variables via d-separation statements. The
d-separation serves as a criterion to determine whether a set of variables X is independent of another
set Y, given a third set Z. This approach links dependence to connectedness, meaning the existence
of a connecting path, while it associates independence with the absence of connection or separation.
A set of DAGs is deemed Markov equivalent when they entail identical conditional independence (CI)
statements. All Markov equivalent DAGs must have the same adjacencies and unshielded colliders
(or v-structures). An unshielded collider is a graph structure of the form A →B ←C where A and
C are non-adjacent vertices. The set of all Markov equivalent DAGs is called a Markov Equivalence
Class (MEC). We denote the set of all DAGs that is Markov equivalent to D as [D].
Definition 1 (Faithfulness [Zhang and Spirtes, 2012]). If the population distribution exhibits a
conditional independence relation only when there exists a corresponding d-separation statement in
the causal graph, then we say that the population distribution is faithful to the causal graph."
PRELIMINARIES,0.05,"The positivity assumption is fundamental for making causal inferences. It asserts that, in theory,
every individual possesses a non-zero chance of being both exposed and unexposed [Hernan and
Robins, 2020]. In many cases, we have access to abundant observational data, which allows for
precise estimation of the true observational distribution. The no positivity violation assumption is
merely needed for theoretical guarantees to hold while not required by the algorithm. We assume
access to the observational distribution, and the observational distribution is faithful to the true causal
graph, with no positivity violations."
PRELIMINARIES,0.05227272727272727,"A partially directed acyclic graph (PDAG) is a partially directed graph free from directed cycles.
All Markov equivalent DAGs can be represented by a completed partially directed acyclic graph
(CPDAG), denoted by C. A DAG D can be represented by a CPDAG when they share the same set
of adjacencies and unshielded colliders, and every oriented edge in the CPDAG is also present in D
[Meek, 2013]. CPDAGs are chain graphs with chordal chain components [Andersson et al., 1997].
In graph theory, a chordal graph is one in which cycles of four or more vertices always contain an
additional edge, called a chord. We denote the set of all chain components of a PDAG P as CC(P)."
PRELIMINARIES,0.05454545454545454,"An intervention on a subset of variables W ⊆V, denoted by the do-operator do(W = w), involves
setting each Wj ∈W to wj. For every intervention, we have an induced post-interventional graph
denoted by DS with incoming edges to vertices in S removed. We denote the interventional and
observational distributions as P D
w (v) and P D(v) respectively for a given DAG D. Using the truncated
factorization formula over the post-interventional graph (DS), we have the following:"
PRELIMINARIES,0.056818181818181816,"Pw(V) := P(V | do(W = w)) =
Y"
PRELIMINARIES,0.05909090909090909,"v/∈W
P(v|pa(v))
(1)"
PRELIMINARIES,0.06136363636363636,"Where v must be consistent with the intervention do(W = w). Consider a set of intervention targets
S = {S1, S2, ..., Sn} where each Si ⊆V for all i ∈[n]. For every target Si ∈S, we define the
collection of all possible cut configurations, i.e., all possible orientations of edges in E[Si, V \ Si], as
Ck(Si) for all k ∈[nSi]. Also note that nSi ≤2|Si|dm where dm is the maximum degree of the graph."
PRELIMINARIES,0.06363636363636363,"Additionally, for bounded size intervention targets i.e., |Si| ≤k, we have nSi ≤2kdm. We use the
notation P Ck(Si)
si
(V) to represent the interventional distribution in E(D) with the cut configuration
Ck(Si). Also we use the notation C∗(Si) for the cut configuration in the true DAG D∗. Given
an intervention set Si, if we assume perfect intervention, one can recognize all the edges adjacent
to vertices in Si and further apply Meek Rules. The resulting PDAG is called an interventional
essential (I-essential) graph of D denoted as ESi(D). When Si is empty, it is the observational
essential graph. I-essential graphs are also known to be maximally oriented partially directed acyclic
graphs (MPDAGs). We denote an MPDAG as M and the I-essential graph with Ck(Si) as MCk(Si).
MPDAGs are also chain graphs with chordal components [Hauser and Bühlmann, 2012]. We denote
the KL divergence between two distributions p, q as DKL(p||q)."
ALGORITHM INITIALIZATIONS,0.0659090909090909,"4
Algorithm Initializations"
ALGORITHM INITIALIZATIONS,0.06818181818181818,"Assume the true DAG is D∗, our algorithm aims to return a most ‘probable’ causal graph D given N
interventional samples. With the access to infinite observational data, we can retrieve the essential
graph G = E(D∗) by PC algorithm [Spirtes et al., 2001] and joint distribution P(V). Here we
describe how we compute the separating system of G and the causal effect of a given intervention set."
SEPARATING SYSTEM,0.07045454545454545,"4.1
Separating System"
SEPARATING SYSTEM,0.07272727272727272,"As mentioned in Section 2, separating system plays the key role for non-adaptive intervention design
of causal discovery. Roughly speaking, a separating system on a set of elements is a collection
of subsets such that for every pair of elements from the set, there exists at least one subset which
contains exactly one element from the pair. Consider an undirected complete graph G of n vertices
indexed as 1, ..., n, a separating system S on [n] would cut every edge of G. In the worst case, a
separating system is needed to learn the causal graph with G as its essential graph [Shanmugam
et al., 2015]. If we further bound the size of each S ∈S such that |S| ≤k, k < n"
SEPARATING SYSTEM,0.075,"2 , the resulting set is
called an (n, k)-separating system. We provide the formal definition below:"
SEPARATING SYSTEM,0.07727272727272727,"Definition 2 ((n, k)-Separating System [Katona, 1966, Wegener, 1979]). An (n, k)-separating system
on [n] is a set of subsets S = {S1, S2, ..., Sm} such that |Si| ≤k and for every pair i, j there is a
subset S ∈S such that either i ∈S, j /∈S or i /∈S, j ∈S.
We discuss the detailed steps of how we construct the (n, k)-separating system in Appendix D."
ENUMERATING CAUSAL EFFECTS,0.07954545454545454,"4.2
Enumerating Causal Effects"
ENUMERATING CAUSAL EFFECTS,0.08181818181818182,"To use a Bayesian approach, we need to construct a set of disjoint events and track their posteriors.
Here we consider the events as interventional causal effects when we intervene on a set of vertices. By
the assumption of faithfulness, Lemma 1 shows that the post-interventional distribution is determined
by the edge configurations that are adjacent to vertices in the intervention set. Perkovic [2020]
proposed a formula is to identify any causal effect in an MPDAG. One can enumerate all valid edge
cuts and use the identification formula to calculate the post-interventional distributions. Here we
propose a simple Algorithm 3 to enumerate through all possible configurations of a given set S
and calculate the post-interventional distribution P C(S)
s
(V) via DAG sampling. For each candidate
configuration Ck(S), we check if it contains invalid structures, i.e. unshielded colliders or cycles. If it
is valid, we apply Meek Rules to get the MPDAG MCk(S) which is an I-essential graph ES(D∗) for
some DAG DCk(S) ∈[D∗] that is consistent with Ck(S). The chain components of M are chordal
graphs and can be oriented independently [Hauser and Bühlmann, 2012]. To efficiently calculate
the interventional distribution, we use a DAG sampler [Wienöbst et al., 2023] to sample a DAG for
each chain component and replace the edges in MCk(S) with the arcs in the sampled DAG to get a
fully oriented graph D. The sampling process takes linear time. Given the DAG D, the interventional
distribution could be calculated by using the Equation 1 over the DAG."
ALGORITHM DESIGN AND ANALYSIS,0.08409090909090909,"5
Algorithm Design and Analysis"
ALGORITHM DESIGN AND ANALYSIS,0.08636363636363636,"In the previous section, we discuss two key steps for the algorithm initialization, which include the
construction of (n, k)-separating systems and enumerating causal effects for all possible cutting edge"
ALGORITHM DESIGN AND ANALYSIS,0.08863636363636364,"configurations for all the intervention targets in the separating system. For a given intervention target
Si, we utilize the following result that under the faithfulness assumption, shows that we have a unique
interventional distribution for every cutting edge orientation Cj(Si)."
ALGORITHM DESIGN AND ANALYSIS,0.09090909090909091,"Lemma 1. [Elahi et al., 2024] Assume that the faithfulness assumption holds and D∗is the true
DAG. For any DAG D1 ̸= D∗, if P D1
s
= P D∗
s
for some S ⊆V, they must share the same cutting
edge orientation C(S)."
ALGORITHM DESIGN AND ANALYSIS,0.09318181818181819,"Lemma 1 allows us to uniquely orient cutting edges for every intervention target in the separating
system, which in turn orients every edge and gives us the true DAG. We use the notation Dsi
ab :=
DKL( P Ca(Si)
si
(V)||P Cb(Si)
si
(V) ) > 0 ∀a ̸= b , ∀a, b ∈[nSi] and Dsi := min∀a̸=b , ∀a,b∈[nSi] Dsi
ab.
The idea is to use the interventional data do(Si = si) to determine the true cutting edge con-
figuration C∗(Si) and repeat this for all intervention targets in the separating system. Suppose
we have access to msi i.i.d. samples from the intervention do(Si = si), which we denote as
Datado(si) = {v1, v2, ..., vmsi}. We define the posterior probabilities of Cj(Si) for all possible
cutting configurations at Si, i.e., for all j ∈[nSi] where nSi ≤2kdm as follows:"
ALGORITHM DESIGN AND ANALYSIS,0.09545454545454546,"Definition 3. (Posterior Probabilities of Cutting Edge Configurations) Consider an intervention
target Si and a collection of all possible cutting edge configurations Cj(Si) for all possible cutting
configurations at Si, i.e., for all j ∈[nSi] and the interventional dataset of i.i.d. samples Datado(si) =
{v1, v2, ..., vmsi}. We define probabilities of Cutting Edge Configurations as:"
ALGORITHM DESIGN AND ANALYSIS,0.09772727272727273,"P(Cj(Si) | Datado(si)) =
P Cj(Si)
si
(v1, v2, ..., vmsi) pj
PnSi
a=1 P Ca(Si)
si
(v1, v2, ..., vmsi) pa
∀j ∈[nSi] , ∀Si ∈S
(2)"
ALGORITHM DESIGN AND ANALYSIS,0.1,"where pa for all a ∈[nSi] are the set of priors such that PnSi
a=1 pa = 1."
ALGORITHM DESIGN AND ANALYSIS,0.10227272727272728,Given the fact that the interventional samples are i.i.d we can rewrite the posterior probabilities as
ALGORITHM DESIGN AND ANALYSIS,0.10454545454545454,"P(Cj(Si) | Datado(si)) =
Qmsi
k=1 P Cj(Si)
si
(vk) pj
nSi
P a=1"
ALGORITHM DESIGN AND ANALYSIS,0.10681818181818181,"Qmsi
k=1 P Ca(Si)
si
(vk) pa"
ALGORITHM DESIGN AND ANALYSIS,0.10909090909090909,"∀j ∈[nSi] , ∀Si ∈S
(3)"
ALGORITHM DESIGN AND ANALYSIS,0.11136363636363636,Assumption 1. We assume access to the observational distribution that is faithful to the true causal
ALGORITHM DESIGN AND ANALYSIS,0.11363636363636363,"graph. Furthermore, we have
 log
P
Ca(Si)
si
(v)"
ALGORITHM DESIGN AND ANALYSIS,0.1159090909090909,"P
Cb(Si)
si
(v)"
ALGORITHM DESIGN AND ANALYSIS,0.11818181818181818,≤β for every intervention set Si ∈S and for all
ALGORITHM DESIGN AND ANALYSIS,0.12045454545454545,"a, b ∈[nSi]."
ALGORITHM DESIGN AND ANALYSIS,0.12272727272727273,"The second half of the assumption can be seen as a version of the positivity assumption commonly
used in causal discovery and many other applications. We show that for the problem of learning the
true cutting edge configuration, the posterior is consistent, i.e., as the number of samples msi →∞,
the posterior of the true cutting edge configuration given the data converges to 1 with high probability."
ALGORITHM DESIGN AND ANALYSIS,0.125,"Lemma 2. (Posterior Consistency) Consider an intervention target Si ∈S and the corresponding
true cutting-edge configuration C∗(Si). As the number of samples msi →∞in Datado(si) =
{v1, v2, . . . , vmsi}, the posterior of the true cutting-edge configuration P(C∗(Si) | Datado(si))
converges to 1 with high probability. More precisely, we have the following high probability lower
bound on the posterior probability of the true cutting-edge configuration."
ALGORITHM DESIGN AND ANALYSIS,0.12727272727272726,"P(C∗(Si) | Datado(si)) ≥1 −
1"
ALGORITHM DESIGN AND ANALYSIS,0.12954545454545455,"1 + α1 exp

O(msi) −α2O
 q"
ALGORITHM DESIGN AND ANALYSIS,0.1318181818181818,msi ln 1
ALGORITHM DESIGN AND ANALYSIS,0.1340909090909091,"δ
 w.p. at least 1 −δ (4)"
ALGORITHM DESIGN AND ANALYSIS,0.13636363636363635,"Where α1 and α2 are constants depending on the prior and the problem instance. Thus, for any small
choice of the probability δ, with a sufficiently large number of samples msi, the posterior of the true
cutting-edge configuration P(C∗(Si) | Datado(si)), converges to 1 with a probability at least 1 −δ."
ALGORITHM DESIGN AND ANALYSIS,0.13863636363636364,"Note that the randomness in the high-probability result in Equation 4 arises from the finite sample
size. Lemma 2 guarantees that with a sufficiently large number of samples from interventions on Si,
the posterior of the true cutting edge configuration P(C∗(Si) | Datado(si)) converges to 1 with high
probability. However, in scenarios where we want to know ahead of time how many interventional
samples we need from every target Si ∈S to ensure that the posterior of the corresponding true
cutting orientations P(C∗(Si) | Datado(si)) ≥1 −γ with high probability, we need to determine the
required number of samples. The parameter γ ≥0 represents the error tolerance. We extend Lemma
2 to provide the minimum number of samples msi required for an intervention on the target set Si to
ensure that the posterior of the corresponding true cutting orientations is greater than some threshold.
Theorem 3. Given that the Assumption1 hold, consider an intervention target Si ∈S such that
|Si| ≤k and the corresponding true cutting edge configuration C∗(Si). If the number of samples
msi in Datado(si) = {v1, v2, ..., vmsi} satisfies the following:"
ALGORITHM DESIGN AND ANALYSIS,0.1409090909090909,"msi =
2β2"
ALGORITHM DESIGN AND ANALYSIS,0.1431818181818182,(Dsi)2 ln 2(k+1)dm
ALGORITHM DESIGN AND ANALYSIS,0.14545454545454545,"δ
+
2
Dsi ln 2kdm(1 −γ)(1 −p∗)"
ALGORITHM DESIGN AND ANALYSIS,0.14772727272727273,"p∗γ
(5)"
ALGORITHM DESIGN AND ANALYSIS,0.15,"where p∗is the prior assigned to the true cutting-edge configuration C∗(Si), then we have
P(C∗(Si) | Datado(si)) ≥1 −γ with a probability at least 1 −δ."
ALGORITHM DESIGN AND ANALYSIS,0.15227272727272728,"Since we have multiple intervention targets in the (n, k)-separating system of the form S =
{S1, S2, ..Sp} such that |Si| ≤k for all i ∈[p]. We can have a total of p bad events one for
every target set Si ∈S where posterior of corresponding true cutting edge configuration C∗(Si) is
not greater than the desired threshold 1 −γ. Thus we extend the Theorem 3 as follows:
Corollary 4. Given that Assumption1 hold, consider a separating system of the form S =
{S1, S2, ..Sp} such that |Si| ≤k for all i ∈[p]. If the number of samples msi in Datado(si) =
{v1, v2, ..., vmsi} for every target set Si ∈S satisfies the following:"
ALGORITHM DESIGN AND ANALYSIS,0.15454545454545454,"msi =
2β2"
ALGORITHM DESIGN AND ANALYSIS,0.15681818181818183,"(Dsi)2
 
ln 2(k+1)dm"
ALGORITHM DESIGN AND ANALYSIS,0.1590909090909091,"δ′

+
2
Dsi
 
ln 2kdm(1 −γ)(1 −p∗)"
ALGORITHM DESIGN AND ANALYSIS,0.16136363636363638,"p∗γ

(6)"
ALGORITHM DESIGN AND ANALYSIS,0.16363636363636364,where p∗is the prior assigned to the true cutting edge configuration C∗(Si) and δ′ = δ
ALGORITHM DESIGN AND ANALYSIS,0.16590909090909092,"p, then we
have P(C∗(Si) | Datado(si)) ≥1 −γ with probability at least 1 −δ for every Si ∈S."
ALGORITHM DESIGN AND ANALYSIS,0.16818181818181818,"To ensure that for every target set Si ∈S, we have the posterior probability of the corresponding true
cutting-edge configuration P(C∗(Si) | Datado(si)) ≥1 −γ, we need msi samples (as in Equation 6)
from every target. Therefore, for the causal discovery problem with p targets in a separating system,
we require a total of p · msi samples."
ALGORITHM DESIGN AND ANALYSIS,0.17045454545454544,"Based on the analysis, we propose Algorithm 1 as the main algorithm. We start by calculating a
(n, k)-separating system S using the labeling procedure described in Appendix D for the essential
graph G = E(D∗), and them identify all valid configurations C(S), ∀S ∈S using Algorithm 3. For
each valid configuration, one can simply assume that their priors are uniform. In our algorithm,
we assume that each possible DAG in [D∗] are equally likely to be the true DAG. Therefore we
can calculate the prior more accurately using the MEC counting algorithm [Wienöbst et al., 2023].
Specifically, for each valid configuration Ck(S), we find the MPDAG MCk(S) that matches with it
by applying Meek Rules. MCk(S) is a chain graph with chordal components that can be oriented
independently. Thus, the interventional MEC size of MCk(S) could be calculated by:"
ALGORITHM DESIGN AND ANALYSIS,0.17272727272727273,"|[MCk(S)]| =
Y"
ALGORITHM DESIGN AND ANALYSIS,0.175,"H∈CC(MCk(S))
|[H]|
(7)"
ALGORITHM DESIGN AND ANALYSIS,0.17727272727272728,The prior of each configuration could then be estimated by:
ALGORITHM DESIGN AND ANALYSIS,0.17954545454545454,"P(Ck(S)) =
|[MCk(S)]|
P"
ALGORITHM DESIGN AND ANALYSIS,0.18181818181818182,"Cj∈C(S) |[MCj(S)]|
(8)"
ALGORITHM DESIGN AND ANALYSIS,0.18409090909090908,"If we are allowed to collect N total interventional samples, we can randomly choose a target set from
the (n, k)-separating system for each sample. Algorithm 3 iterates all valid configurations to calculate"
ALGORITHM DESIGN AND ANALYSIS,0.18636363636363637,Algorithm 1: Sample efficient causal discovery in Bayesian approach
ALGORITHM DESIGN AND ANALYSIS,0.18863636363636363,"Data: Input UCCG G = (V, E), (n, k)-separating system S, and observational joint distribution Pobs(V),
prior of each configuration P(C(S)), number of samples N
Result: Output the posterior P C(S)
S
(C(S)|vdo(S)) of each configuration to be true and a candidate DAG D
for i ∈[N] do"
ALGORITHM DESIGN AND ANALYSIS,0.19090909090909092,"Randomly choose a target set S ∈S;
Sample vdo(S) from PS;
for Ck(S) ∈C(S) do"
ALGORITHM DESIGN AND ANALYSIS,0.19318181818181818,Calculate likelihood P(vdo(S)|Ck(S)) using Algorithm 3;
ALGORITHM DESIGN AND ANALYSIS,0.19545454545454546,for Ck(S) ∈C(S) do
ALGORITHM DESIGN AND ANALYSIS,0.19772727272727272,Update posterior and prior ;
ALGORITHM DESIGN AND ANALYSIS,0.2,"P Ck(S)
S
(Ck(S)|vdo(S)) ←
P (Ck(S))×P (vdo(S)|Ck(S))
P
Ck(S)∈C(S) P (Ck(S))×P (vdo(S)|Ck(S));"
ALGORITHM DESIGN AND ANALYSIS,0.20227272727272727,"P(Ck(S)) ←P Ck(S)
S
(Ck(S)|vdo(S))"
ALGORITHM DESIGN AND ANALYSIS,0.20454545454545456,"D ←G, Svisit = ∅;
while S ̸= Svisit do"
ALGORITHM DESIGN AND ANALYSIS,0.20681818181818182,"Ck(S) ←arg maxCj(S)∈C(S),S∈S P Ck(S)
S
(Ck(S)|vdo(S));
if Ck(S) not compatible with Svisit then"
ALGORITHM DESIGN AND ANALYSIS,0.20909090909090908,"Remove Ck(S) from C(S);
Pass;"
ALGORITHM DESIGN AND ANALYSIS,0.21136363636363636,"Replace edges in D with arcs in Ck(S);
Svisit ←Svisit ∪{S};"
ALGORITHM DESIGN AND ANALYSIS,0.21363636363636362,"return P C(S)
S
(C(S)|vdo(S)), D"
ALGORITHM DESIGN AND ANALYSIS,0.2159090909090909,"the likelihood P(vdo(S)|Ck(S)) by sampling DAG from the interventional MEC. After calculating
all the likelihoods, we can update the posteriors and priors. For the last step, we need to combine all
the configurations of each (n, k)-separating set to return a DAG. We consider a greedy approach. At
each step, we choose the configuration with the highest posterior across all unvisited target sets that
is compatible with chosen configurations. Given the anytime nature of our algorithm, we can propose
a candidate DAG after each interventional sample. With a large enough sample number, according to
Theorem 3, our algorithm will return the true DAG with a high probability."
ALGORITHM DESIGN AND ANALYSIS,0.21818181818181817,"6
Case Study: Estimating Causal Effects of Non-Intervenable Vertices"
ALGORITHM DESIGN AND ANALYSIS,0.22045454545454546,"Here we show how to use DAG sampler to estimate the causal effect when some vertex cannot be
intervened in the causal graph. The detailed problem setup is as follow. D = (V, E) is an undirected
underlying causal graph, X, Y ∈V are non-adjacent vertices. We want to estimate p(y|do(x)) using
interventional data while X is not intervenable in the graph. Thus we cannot directly use the method
in Perkovic [2020] to estimate the causal effect. With the DAG sampler, we can first intervene on
X’s neighborhood Ne(X) and then sample DAG to estimate the likelihood of each configuration
being the true one in the causal graph. When we have large sample size, the posterior of the true
configuration will go to 1 with a high probability according to Lemma 2. Then, we iterate through
each configuration and their posterior to calculate the average divergence D between the estimated
and true causal effect. The divergence D here can be KL divergence or Total Variation Distance
(TVD). If the ground truth causal effect is p∗, the interventional data is Datado(Ne(X)), then D is
formally defined as: D =
X"
ALGORITHM DESIGN AND ANALYSIS,0.22272727272727272,"Ci∈C(Ne(X))
D(p∗||P Ci
Ne(X)(y|do(x))) · P(Ci|Datado(Ne(X)))
(9)"
ALGORITHM DESIGN AND ANALYSIS,0.225,"In this experiment, we randomly create 50 causal graphs with n = 5, 6, 7, ρ = 0.3, 0.6 using a similar
approach described in Section 7. Then we randomly choose a pair of non-adjacent vertices X, Y . We
then intervene on X and collect 100,000 samples. We plot DKL and DT V D between estimated causal
effect and the ground truth given different number of interventional samples. The mean and standard
deviation are plotted in Figure 1. We can see that as the number of interventional samples increases,
the estimated causal effect gets close to the ground truth and the variance is also decreasing. The differ-
ence decreases sharply w.r.t. the number of samples, showing the efficiency of our proposed approach."
ALGORITHM DESIGN AND ANALYSIS,0.22727272727272727,"(a) n = 5, ρ = 0.3
(b) n = 6, ρ = 0.3
(c) n = 7, ρ = 0.3"
ALGORITHM DESIGN AND ANALYSIS,0.22954545454545455,"(d) n = 5, ρ = 0.6
(e) n = 6, ρ = 0.6
(f) n = 7, ρ = 0.6"
ALGORITHM DESIGN AND ANALYSIS,0.2318181818181818,"Figure 1: Average KL divergence and TVD between estimated causal effect and ground truth vs
number of interventional samples for random causal graphs."
EXPERIMENTS,0.2340909090909091,"7
Experiments"
EXPERIMENTS,0.23636363636363636,"We compare the proposed Bayesian Causal Discovery algorithm with 3 existing baselines. The first
baseline, Random Intervention, intervenes randomly on the graph separating system. In each step,
one interventional sample is collected, and then we perform independence tests to learn the cuts at the
targets based on the collected samples from each intervention target. The second baseline is Active
Structure Learning of Causal DAGs via Directed Clique Trees (DCTs) [Squires et al., 2020]. The
third baseline is the adaptivity-sensitive search algorithm proposed in Choo and Shiragur [2023]. It
chooses the intervention target based on the clique tree representation of a chordal graph."
EXPERIMENTS,0.23863636363636365,"We generate random connected moral DAGs with order n and density ρ using a modified Erd˝os–Rényi
sample approach, similar to the process in Squires et al. [2020]. We start by generating a random
ordering τ over vertices. Then, for the ith vertex, we sample its in-degree as Xi = max(1, Bin(n −
1, ρ)). The vertices precede it in the ordering are uniformly assgined as its parents. In the last step,
we apply the elimination algorithm in Koller and Friedman [2009] to chordalize the graph. The
elimination algorithm uses an ordering that is the reverse of τ. Based on the generated graph, we
randomly sample the conditional probability table (CPT) that is consistent with the graph and strictly
positive. The essential graph of the generated DAG is then fed into the causal discovery algorithms
together with the Bayes network that is consistent with the DAG."
EXPERIMENTS,0.2409090909090909,"To measure the performance of the algorithms, we plot the Structural Hamming Distance (SHD)
between the ground truth and learned DAGs with respect to the number of collected interventional
samples. SHD counts the number of edge insertions, deletions, or flips to transform from one DAG
into another. For each setting of n and ρ, we sample 50 random DAGs and calculate the mean and
standard deviation of SHD by each causal discovery algorithm. The mean is plotted as a curve and
the shaded area around the curve represents the standard deviation. We use Chi-Square independence
test from the Causal Discovery Toolbox [Kalainathan et al., 2020] to perform statistical tests with
limited samples."
EXPERIMENTS,0.2431818181818182,"The results in Figure 2 and 3 show that our algorithm outperforms other causal discovery algorithms.
Figure 2 shows the performance of causal discovery algorithms on complete graphs with order 5,
6, and 7 respectively. Our algorithm uses significantly less samples to reach a low SHD and the
number of samples required does not increase much with the order of the graph compared to the
other algorithms. Figure 3 shows the results of graphs with the same order but varying densities.
Similarly, our algorithm uses comparable number of samples across different densities. In contrast,
the baseline algorithms require much more samples in denser graphs to reach the same SHD. To wrap
up, in both cases, our algorithm reaches a low SHD using much fewer samples than the baselines"
EXPERIMENTS,0.24545454545454545,"and is more stable across different settings. The results on larger graphs (n = 20) are provided
in Appendix H.1. The algorithm code is provided at https://github.com/CausalML-Lab/
Bayesian_SampleEfficient_Discovery."
EXPERIMENTS,0.24772727272727274,"(a) n = 5, ρ = 1
(b) n = 6, ρ = 1
(c) n = 7, ρ = 1"
EXPERIMENTS,0.25,Figure 2: SHD vs number of interventional samples for random complete graphs
EXPERIMENTS,0.25227272727272726,"(a) n = 10, ρ = 0.1
(b) n = 10, ρ = 0.15
(c) n = 10, ρ = 0.2"
EXPERIMENTS,0.2545454545454545,Figure 3: SHD vs number of interventional samples for random sparse chordal graphs
CONCLUSION,0.25681818181818183,"8
Conclusion"
CONCLUSION,0.2590909090909091,"In this work, we discuss the problem of learning causal graphs with minimum number of interventional
samples. We propose an algorithm that solves this problem in a Bayesian approach. Specifically, we
keep track of the posteriors of interventional distributions of each target set in the (n, k)-separating
system and then combine the configurations that have high posteriors to propose a DAG. We show
in theory that given enough samples, our algorithm can return the true causal graph with a high
probability. Also, according to the Bayesian nature of our algorithm, we can stop the algorithm
anytime and check the output graph. Experiments on simulated datasets show that compared with
the baselines, our algorithm use significantly fewer interventional samples to achieve the same SHD.
Additionally in the case study, we demonstrate that we can modify our algorithm to answer special
causal queries with DAG sampling. For future extensions, it is of great interest to decrease the space of
posteriors being tracked, since on large dense graphs, it will be intractable to track all interventional
distributions of a target set. Besides, we can try to remove/weaken the assumptions like causal
sufficiency and faithfulness while maintaining theoretical guarantees. Our work can potentially have
some societal consequences including ethical considerations related to performing interventions and
the risk of biased or partial understandings which may cause misled decision-making in real-world
cases."
CONCLUSION,0.26136363636363635,Acknowledgement
CONCLUSION,0.2636363636363636,"This research has been supported in part by NSF CAREER 2239375, IIS 2348717, Amazon Research
Award and Adobe Research."
REFERENCES,0.26590909090909093,References
REFERENCES,0.2681818181818182,"Jayadev Acharya, Arnab Bhattacharyya, Constantinos Daskalakis, and Saravanan Kandasamy. Learn-
ing and testing causal models with interventions. Advances in Neural Information Processing
Systems, 31, 2018."
REFERENCES,0.27045454545454545,"Steen A Andersson, David Madigan, and Michael D Perlman. A characterization of markov equiva-
lence classes for acyclic digraphs. The Annals of Statistics, 25(2):505–541, 1997."
REFERENCES,0.2727272727272727,"Yashas Annadani, Nick Pawlowski, Joel Jennings, Stefan Bauer, Cheng Zhang, and Wenbo Gong.
Bayesdag: Gradient-based posterior inference for causal discovery. Advances in Neural Information
Processing Systems, 36, 2024."
REFERENCES,0.275,"Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio.
Gflownet foundations. Journal of Machine Learning Research, 24(210):1–55, 2023."
REFERENCES,0.2772727272727273,"Wray L Buntine. Operations for learning with graphical models. Journal of artificial intelligence
research, 2:159–225, 1994."
REFERENCES,0.27954545454545454,"Srinivas Niranj Chandrasekaran, Beth A Cimini, Amy Goodale, Lisa Miller, Maria Kost-Alimova,
Nasim Jamali, John G Doench, Briana Fritchman, Adam Skepner, Michelle Melanson, et al. Three
million images and morphological profiles of cells treated with matched chemical and genetic
perturbations. Nature Methods, pages 1–8, 2024."
REFERENCES,0.2818181818181818,"Bertrand Charpentier, Simon Kibler, and Stephan Günnemann. Differentiable dag sampling. arXiv
preprint arXiv:2203.08509, 2022."
REFERENCES,0.2840909090909091,"Davin Choo and Kirankumar Shiragur. Adaptivity complexity for causal graph discovery. In
Uncertainty in Artificial Intelligence, pages 391–402. PMLR, 2023."
REFERENCES,0.2863636363636364,"Davin Choo, Kirankumar Shiragur, and Arnab Bhattacharyya. Verification and search algorithms for
causal dags. Advances in Neural Information Processing Systems, 35:12787–12799, 2022."
REFERENCES,0.28863636363636364,"Chris Cundy, Aditya Grover, and Stefano Ermon. Bcd nets: Scalable variational approaches for
bayesian causal discovery. Advances in Neural Information Processing Systems, 34:7095–7110,
2021."
REFERENCES,0.2909090909090909,"Frederick Eberhardt. Causation and intervention. Unpublished doctoral dissertation, Carnegie
Mellon University, 93, 2007."
REFERENCES,0.29318181818181815,"Muhammad Qasim Elahi, Lai Wei, Murat Kocaoglu, and Mahsa Ghasemi. Adaptive online experi-
mental design for causal discovery. arXiv preprint, 2024."
REFERENCES,0.29545454545454547,"AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, and Elias Bareinboim. Budgeted
experiment design for causal structure learning. In International Conference on Machine Learning,
pages 1724–1733. PMLR, 2018."
REFERENCES,0.29772727272727273,"Kristjan Greenewald, Dmitriy Katz, Karthikeyan Shanmugam, Sara Magliacane, Murat Kocaoglu,
Enric Boix Adsera, and Guy Bresler. Sample efficient active learning of causal trees. Advances in
Neural Information Processing Systems, 32, 2019."
REFERENCES,0.3,"Alexander Hägele, Jonas Rothfuss, Lars Lorch, Vignesh Ram Somnath, Bernhard Schölkopf, and
Andreas Krause. Bacadi: Bayesian causal discovery with unknown interventions. In International
Conference on Artificial Intelligence and Statistics, pages 1411–1436. PMLR, 2023."
REFERENCES,0.30227272727272725,"Alain Hauser and Peter Bühlmann. Characterization and greedy learning of interventional markov
equivalence classes of directed acyclic graphs. The Journal of Machine Learning Research, 13(1):
2409–2464, 2012."
REFERENCES,0.30454545454545456,"Alain Hauser and Peter Bühlmann. Two optimal strategies for active learning of causal models from
interventional data. International Journal of Approximate Reasoning, 55(4):926–939, 2014."
REFERENCES,0.3068181818181818,"Yang-Bo He and Zhi Geng. Active learning of causal networks with intervention experiments and
optimal designs. Journal of Machine Learning Research, 9(Nov):2523–2547, 2008."
REFERENCES,0.3090909090909091,"Yangbo He, Jinzhu Jia, and Bin Yu. Counting and exploring sizes of markov equivalence classes of
directed acyclic graphs. The Journal of Machine Learning Research, 16(1):2589–2609, 2015."
REFERENCES,0.31136363636363634,"David Heckerman, Christopher Meek, and Gregory Cooper. A bayesian approach to causal discovery.
Technical report, Technical report msr-tr-97-05, Microsoft Research, 1997."
REFERENCES,0.31363636363636366,"MA Hernan and J Robins. Causal inference: What if. boca raton: Chapman & hill/crc. Taylor and
Francis, 2020."
REFERENCES,0.3159090909090909,"Kevin D Hoover. The logic of causal inference: Econometrics and the conditional analysis of
causation. Economics & Philosophy, 6(2):207–234, 1990."
REFERENCES,0.3181818181818182,"Antti Hyttinen, Frederick Eberhardt, and Patrik O Hoyer. Experiment selection for causal discovery.
The Journal of Machine Learning Research, 14(1):3041–3071, 2013."
REFERENCES,0.32045454545454544,"Azam Ikram, Sarthak Chakraborty, Subrata Mitra, Shiv Saini, Saurabh Bagchi, and Murat Kocaoglu.
Root cause analysis of failures in microservices through causal discovery. Advances in Neural
Information Processing Systems, 35:31158–31170, 2022."
REFERENCES,0.32272727272727275,"Yibo Jiang and Bryon Aragam. Learning nonparametric latent causal graphs with unknown interven-
tions. Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.325,"Diviyan Kalainathan, Olivier Goudet, and Ritik Dutta. Causal discovery toolbox: Uncovering causal
relationships in python. Journal of Machine Learning Research, 21(37):1–5, 2020."
REFERENCES,0.32727272727272727,"Gyula Katona. On separating systems of a finite set. Journal of Combinatorial Theory, 1(2):174–194,
1966."
REFERENCES,0.32954545454545453,"Ross D King, Kenneth E Whelan, Ffion M Jones, Philip GK Reiser, Christopher H Bryant, Stephen H
Muggleton, Douglas B Kell, and Stephen G Oliver. Functional genomic hypothesis generation and
experimentation by a robot scientist. Nature, 427(6971):247–252, 2004."
REFERENCES,0.33181818181818185,"Murat Kocaoglu, Alex Dimakis, and Sriram Vishwanath. Cost-optimal learning of causal graphs. In
International Conference on Machine Learning, pages 1875–1884. PMLR, 2017."
REFERENCES,0.3340909090909091,"Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT
press, 2009."
REFERENCES,0.33636363636363636,"Jack Kuipers, Polina Suter, and Giusi Moffa. Efficient sampling and structure learning of bayesian
networks. Journal of Computational and Graphical Statistics, 31(3):639–650, 2022."
REFERENCES,0.3386363636363636,"Erik Lindgren, Murat Kocaoglu, Alexandros G Dimakis, and Sriram Vishwanath. Experimental
design for cost-aware learning of causal graphs. Advances in Neural Information Processing
Systems, 31, 2018."
REFERENCES,0.3409090909090909,"Lars Lorch, Jonas Rothfuss, Bernhard Schölkopf, and Andreas Krause. Dibs: Differentiable bayesian
structure learning. Advances in Neural Information Processing Systems, 34:24111–24123, 2021."
REFERENCES,0.3431818181818182,"Lars Lorch, Scott Sussex, Jonas Rothfuss, Andreas Krause, and Bernhard Schölkopf. Amortized
inference for causal structure learning. Advances in Neural Information Processing Systems, 35:
13104–13118, 2022."
REFERENCES,0.34545454545454546,"Daniel Malinsky. A cautious approach to constraint-based causal model selection. arXiv preprint
arXiv:2404.18232, 2024."
REFERENCES,0.3477272727272727,"Christopher Meek.
Causal inference and causal explanation with background knowledge.
In
Proceedings of the Eleventh conference on Uncertainty in artificial intelligence, pages 403–410,
1995."
REFERENCES,0.35,"Christopher Meek. Causal inference and causal explanation with background knowledge. arXiv
preprint arXiv:1302.4972, 2013."
REFERENCES,0.3522727272727273,"Mizu Nishikawa-Toomey, Tristan Deleu, Jithendaraa Subramanian, Yoshua Bengio, and Laurent
Charlin. Bayesian learning of causal structure and mechanisms with gflownets and variational
bayes. arXiv preprint arXiv:2211.02763, 2022."
REFERENCES,0.35454545454545455,"Judea Pearl. Causality. Cambridge university press, 2009."
REFERENCES,0.3568181818181818,"Emilija Perkovic. Identifying causal effects in maximally oriented partially directed acyclic graphs.
In Conference on Uncertainty in Artificial Intelligence, pages 530–539. PMLR, 2020."
REFERENCES,0.35909090909090907,"Jakob Runge, Sebastian Bathiany, Erik Bollt, Gustau Camps-Valls, Dim Coumou, Ethan Deyle, Clark
Glymour, Marlene Kretschmer, Miguel D Mahecha, Jordi Muñoz-Marí, et al. Inferring causation
from time series in earth system sciences. Nature communications, 10(1):2553, 2019."
REFERENCES,0.3613636363636364,"Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G Dimakis, and Sriram Vishwanath. Learning
causal graphs with small interventions. Advances in Neural Information Processing Systems, 28,
2015."
REFERENCES,0.36363636363636365,"Atul Sharma, Pranjal Jain, Ashraf Mahgoub, Zihan Zhou, Kanak Mahadik, and Somali Chaterji.
Lerna: transformer architectures for configuring error correction tools for short-and long-read
genome sequencing. BMC bioinformatics, 23(1):25, 2022."
REFERENCES,0.3659090909090909,"Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, prediction, and search. MIT press,
2001."
REFERENCES,0.36818181818181817,"Chandler Squires, Sara Magliacane, Kristjan Greenewald, Dmitriy Katz, Murat Kocaoglu, and
Karthikeyan Shanmugam. Active structure learning of causal dags via directed clique trees.
Advances in Neural Information Processing Systems, 33:21500–21511, 2020."
REFERENCES,0.3704545454545455,"Tim Stuart and Rahul Satija. Integrative single-cell analysis. Nature reviews genetics, 20(5):257–272,
2019."
REFERENCES,0.37272727272727274,"Robert E Tarjan and Mihalis Yannakakis. Simple linear-time algorithms to test chordality of graphs,
test acyclicity of hypergraphs, and selectively reduce acyclic hypergraphs. SIAM Journal on
computing, 13(3):566–579, 1984."
REFERENCES,0.375,"Simon Tong and Daphne Koller. Active learning for structure in bayesian networks. In International
joint conference on artificial intelligence, volume 17, pages 863–869. Citeseer, 2001."
REFERENCES,0.37727272727272726,"Christian Toth, Lars Lorch, Christian Knoll, Andreas Krause, Franz Pernkopf, Robert Peharz, and
Julius Von Kügelgen. Active bayesian causal inference. Advances in Neural Information Processing
Systems, 35:16261–16275, 2022."
REFERENCES,0.3795454545454545,"Caroline Uhler and GV Shivashankar. Regulation of genome organization and gene expression by
nuclear mechanotransduction. Nature reviews Molecular cell biology, 18(12):717–727, 2017."
REFERENCES,0.38181818181818183,"Ts Verma. Equivalence and synthesis of causal models. In Proceedings of the Sixth Conference on
Uncertainty in Artificial Intelligence, 1991. Elsevier, 1991."
REFERENCES,0.3840909090909091,"Ingo Wegener. On separating systems whose elements are sets of at most k elements. Discrete
Mathematics, 28(2):219–222, 1979."
REFERENCES,0.38636363636363635,"Marcel Wienöbst, Max Bannach, and Maciej Li´skiewicz. Polynomial-time algorithms for counting
and sampling markov equivalent dags with applications. Journal of Machine Learning Research,
24(213):1–45, 2023."
REFERENCES,0.3886363636363636,"Jiji Zhang and Peter L Spirtes. Strong faithfulness and uniform consistency in causal inference. arXiv
preprint arXiv:1212.2506, 2012."
REFERENCES,0.39090909090909093,"Zihan Zhou, Zijia Du, and Somali Chaterji. Kratos: Context-aware cell type classification and
interpretation using joint dimensionality reduction and clustering. In Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2616–2625, 2022."
REFERENCES,0.3931818181818182,Supplemental Material
REFERENCES,0.39545454545454545,"A
Proof of Lemma 2:"
REFERENCES,0.3977272727272727,"Consider an intervention target Si and a collection of all possible cutting edge configurations Cj(Si)
for all possible cutting configurations at Si, i.e., for all j ∈[nSi] and the interventional dataset of i.i.d.
samples Datado(si) = {v1, v2, ..., vmsi}. We revisit some notations from the main paper. We denote"
REFERENCES,0.4,"the interventional distribution with cut configuration Cj(Si) by P Cj(Si)
si
. we have nSi different cut
configurations which we denote by C1(Si), C2(Si) .... CnSi(S). Under faithfulness assumption we"
REFERENCES,0.4022727272727273,"have P Ca(Si)
si
̸= P Cb(Si)
si
for all a ̸= b , a, b ∈[nSi]. Also note that for with (n, k) separating system
we have nSi ≤2kdm. Without loss of generality assume C1(Si) is the true cutting edge configuration.
We use the notation Dsi
ab := DKL( P Ca(Si)
si
(V) | | P Cb(Si)
si
(V) ) > 0 ∀a ̸= b , ∀a, b ∈[nSi] and
Dsi := min∀a̸=b , ∀a,b∈[nSi] Dsi
ab."
REFERENCES,0.40454545454545454,"Let us consider P C1(Si)
si
and P C2(Si)
si
only for now."
REFERENCES,0.4068181818181818,"Let −L = −log
P C2(Si)
si
(v1, v2, ..., vmsi)"
REFERENCES,0.4090909090909091,"P C1(Si)
si
(v1, v2, ..., vmsi)
(10)"
REFERENCES,0.4113636363636364,Since interventional dataset is composed of i.i.d. samples we have:
REFERENCES,0.41363636363636364,"−L = − msi
X"
REFERENCES,0.4159090909090909,"j=1
log P C2(Si)
si
(vj)"
REFERENCES,0.41818181818181815,"P C1(Si)
si
(vj)
(11)"
REFERENCES,0.42045454545454547,"Let Lj = −log P C2(Si)
si
(vj)"
REFERENCES,0.42272727272727273,"P C1(Si)
si
(vj)
(12) −L = msi
X"
REFERENCES,0.425,"j=1
Lj
(13)"
N,0.42727272727272725,"1
n msi
X"
N,0.42954545454545456,"j=1
Lj = −L"
N,0.4318181818181818,"n
(14)"
N,0.4340909090909091,"Since C1(Si) is the true cutting edge configuration so the interventional samples are actually sampled
from P C1(Si)
si
. So we have the following:"
N,0.43636363636363634,"E[Lj] = Evj∼P
C1(Si)
si"
N,0.43863636363636366,"
−log P C2(Si)
si
(vj)"
N,0.4409090909090909,"P C1(Si)
si
(vj)"
N,0.4431818181818182,"
(15)"
N,0.44545454545454544,"E[Lj] = Evj∼P
C1(Si)
si """
N,0.44772727272727275,"log P C1(Si)
si
(vj)"
N,0.45,"P C2(Si)
si
(vj) # (16)"
N,0.45227272727272727,"E[Lj] = DKL( P C1(Si)
si
(V) | | P C2(Si)
si
(V) ) = Dsi
12 > 0
(17)"
N,0.45454545454545453,"Under the Assumption | Lj | ≤β ∀j ∈[msi] for all Lj = −log
P
C2(Si)
si
(vj)"
N,0.45681818181818185,"P
C1(Si)
si
(vj). using hoeffding"
N,0.4590909090909091,inequality with msi samples we have:
N,0.46136363636363636,"P( | −L −msiDsi
12 | ≥ϵ) ≤2e −2ϵ2"
N,0.4636363636363636,"4β2msi
(18)"
N,0.4659090909090909,"Suppose
δ
2kdm = 2e −2ϵ2"
N,0.4681818181818182,"4β2msi which implies ϵ =
q"
N,0.47045454545454546,"2β2msi ln
2
δ
2kdm =
q"
N,0.4727272727272727,2β2msi ln 2×2kdm δ
N,0.475,Thus we have the following:
N,0.4772727272727273,P( | −L −msiDsi | ≥ r
N,0.47954545454545455,2β2msi ln 2 × 2kdm
N,0.4818181818181818,"δ
) ≤
δ
2kdm
(19)"
N,0.48409090909090907,"which implies that with probability of at least 1 −
δ
2kdm we have:"
N,0.4863636363636364,"−L ≥msiDsi
12 − r"
N,0.48863636363636365,2β2msi ln 2 × 2kdm
N,0.4909090909090909,"δ
(20)"
N,0.49318181818181817,Using the definition of L we have:
N,0.4954545454545455,"−log
P C2(Si)
si
(v1, v2, ..., vmsi)"
N,0.49772727272727274,"P C1(Si)
si
(v1, v2, ..., vmsi)
≥msiDsi
12− r"
N,0.5,2β2msi ln 2 × 2kdm
N,0.5022727272727273,"δ
(w.p. at least
1−
δ
2kdm ) (21)"
N,0.5045454545454545,"P C2(Si)
si
(v1, v2, ..., vmsi)"
N,0.5068181818181818,"P C1(Si)
si
(v1, v2, ..., vmsi)
≤e−
 
msiD
si
12−
q"
N,0.509090909090909,2β2msi ln 2×2kdm
N,0.5113636363636364,"δ

(w.p. at least
1 −
δ
2kdm ) (22)"
N,0.5136363636363637,"P C2(Si)
si
(v1, v2, ..., vmsi) ≤e−
 
msiD
si
12−
q"
N,0.5159090909090909,2β2msi ln 2×2kdm
N,0.5181818181818182,"δ

P C1(Si)
si
(v1, v2, ..., vmsi)
(23)"
N,0.5204545454545455,"The equation 23 holds with probability at least 1 −
δ
2kdm . Similarly using the union bound we have
with probability at least 1 −δ we have the following ∀j ̸= 1 , j ∈[nSi]:"
N,0.5227272727272727,"P Cj(Si)
si
(v1, v2, ..., vmsi) ≤e−
 
msiD
si
1j−
q"
N,0.525,2β2msi ln 2×2kdm
N,0.5272727272727272,"δ

P C1(Si)
si
(v1, v2, ..., vmsi)
(24)"
N,0.5295454545454545,"Now, suppose we assign a non-zero prior pj > 0 to the all cut configuration Cj(Si) ∀j ∈[nSI] such
that Pn
j=1 pi = 1. The posterior distribution can be written as:"
N,0.5318181818181819,"P(C1(Si) | Datado(si)) =
P C1(Si)
si
(v1, v2, ..., vmsi) p1
PnSi
j=1 P Cj(Si)
si
(v1, v2, ..., vmsi) pj
(25)"
N,0.5340909090909091,From the result in Equation 24 with probability at least 1 −δ we have:
N,0.5363636363636364,"P(C1(Si) | Datado(si)) ≥
P
C1(Si)
si
(v1,v2,...,vmsi ) p1"
N,0.5386363636363637,"P
C1(Si)
si
(v1,v2,...,vmsi ) p1+PnSi
j=2 P
C1(Si)
si
(v1,v2,...,vmsi ) e
−
 "
N,0.5409090909090909,"msi Dsi
1j − r"
N,0.5431818181818182,"2β2msi ln 2×2kdm δ
"
N,0.5454545454545454,"pj
(26)"
N,0.5477272727272727,With probability at least 1 −δ we have:
N,0.55,"P(C1(Si) | Datado(si)) ≥
p1"
N,0.5522727272727272,"p1 + PnSi
j=2 e−
 
msiD
si
1j−
q"
N,0.5545454545454546,2β2msi ln 2×2kdm
N,0.5568181818181818,"δ

pj
(27)"
N,0.5590909090909091,"Since nSi ≤2kdm and Dsi := min∀a̸=b , ∀a,b∈[nSi] Dsi
ab. With probability at least 1 −δ we have:"
N,0.5613636363636364,"P(C1(Si) | Datado(si)) ≥
p1"
N,0.5636363636363636,"p1 + (1 −p1)2kdme−
 
msiDsi−
q"
N,0.5659090909090909,2β2msi ln 2×2kdm
N,0.5681818181818182,"δ

(28)"
N,0.5704545454545454,"P(C1(Si) | Datado(si)) ≥1−
1"
N,0.5727272727272728,"1 +
p1
(1−p1)2kdm e
 
msiDsi−
q"
N,0.575,2β2msi ln 2×2kdm
N,0.5772727272727273,"δ
 (w.p. at least 1−δ) (29)"
N,0.5795454545454546,which can equivalently written as follows:
N,0.5818181818181818,"P(C∗(Si) | Datado(si)) ≥1 −
1"
N,0.5840909090909091,"1 + α1 exp

O(msi) −α2O
 q"
N,0.5863636363636363,msi ln 1
N,0.5886363636363636,"δ
 (w.p. at least 1 −δ) (30)"
N,0.5909090909090909,"Where α1 and α2 are constants depending on the priors used and the problem instance. Note the in
our proof the true cutting edge configuration C∗(Si) = C1(Si). This completes the proof of Lemma
2."
N,0.5931818181818181,"B
Proof of Theorem 3:"
N,0.5954545454545455,"The Proof of the Theorem 3 is the continuation of the proof of Lemma 2. Suppose we want
P(C1(Si) | Datado(si)) ≥1 −γ we have:"
N,0.5977272727272728,"(1 −p1)2kdme−
 
msiDsi−
q"
N,0.6,2β2msi ln 2×2kdm
N,0.6022727272727273,"δ

≤p1γ"
N,0.6045454545454545,"1 −γ
(31)"
N,0.6068181818181818,"e−
 
msiDsi−
q"
N,0.6090909090909091,2β2msi ln 2×2kdm
N,0.6113636363636363,"δ

≤
p1γ
2kdm(1 −γ)(1 −p1)
(32)"
N,0.6136363636363636,msiDsi − r
N,0.615909090909091,2β2msi ln 2 × 2kdm
N,0.6181818181818182,"δ
≥log 2kdm(1 −γ)(1 −p1)"
N,0.6204545454545455,"p1γ
(33)"
N,0.6227272727272727,Solving the above equation for number of interventional samples msi we have:
N,0.625,"msi ≥
4β2 ln 2(k+1)dm"
N,0.6272727272727273,"δ
+4Dsi ln 2kdm (1−γ)(1−p1) p1γ
+ r"
N,0.6295454545454545,16β4 ln2 2(k+1)dm
N,0.6318181818181818,"δ
+32β2Dsi ln 2(k+1)dm"
N,0.634090909090909,"δ
ln 2kdm (1−γ)(1−p1)"
N,0.6363636363636364,"p1γ
4(Dsi)2
(34)"
N,0.6386363636363637,In order to simplify the expression we select the number of interventional samples msi as follows: msi =
N,0.6409090909090909,4β2 ln 2(k+1)dm
N,0.6431818181818182,"δ
+4Dsi ln 2kdm (1−γ)(1−p1) p1γ
+"
N,0.6454545454545455,"v
u
u
u
t "
N,0.6477272727272727,4β2 ln 2(k+1)dm
N,0.65,"δ
+4Dsi ln 2kdm (1−γ)(1−p1) p1γ 2"
N,0.6522727272727272,"4(Dsi)2
(35)"
N,0.6545454545454545,"msi =
4β2 ln 2(k+1)dm"
N,0.6568181818181819,"δ
+ 4Dsi ln 2kdm(1−γ)(1−p1) p1γ
+"
N,0.6590909090909091,"2(Dsi)2
(36)"
N,0.6613636363636364,"msi =
2β2"
N,0.6636363636363637,(Dsi)2 ln 2(k+1)dm
N,0.6659090909090909,"δ
+
2
Dsi ln 2kdm(1 −γ)(1 −p1)"
N,0.6681818181818182,"p1γ
(37)"
N,0.6704545454545454,for the above choice of number of samples msi we have the following result:
N,0.6727272727272727,"P(C1(Si) | Datado(si)) ≥1 −γ
(w.p. at least
1 −δ)
(38)"
N,0.675,"Now, suppose p∗is the prior assigned to the true cutting-edge configuration C∗(Si). Consequently,
for the following choice of number of samples msi:"
N,0.6772727272727272,"msi =
2β2"
N,0.6795454545454546,(Dsi)2 ln 2(k+1)dm
N,0.6818181818181818,"δ
+
2
Dsi ln 2kdm(1 −γ)(1 −p∗)"
N,0.6840909090909091,"p∗γ
(39)"
N,0.6863636363636364,"We have P(C∗(Si) | Datado(si)) ≥1 −γ with probability at least 1 −δ. Note the in our proof the
true cutting edge configuration C∗(Si) = C1(Si). This completes the proof of Theorem 3."
N,0.6886363636363636,"C
Proof of the Corollary 4:"
N,0.6909090909090909,"Consider a separating system of the form S = {S1, S2, ..Sp} such that |si| ≤k for all i ∈
[p]. The Proof of the Corollary is simple application of union to result in Theorem 3. if replace
δ by δ"
N,0.6931818181818182,"p in the Equation 39 for number of samples msi then for a particular Si ∈S we have
P(C∗(Si) | Datado(si)) ≥1 −γ with probability at least 1 −δ"
N,0.6954545454545454,"p. Taking union bound we have the
following result:"
N,0.6977272727272728,"If the number of samples msi in Datado(si) = {v1, v2, ..., vmsi} for every target set Si ∈S satisfies
the following:"
N,0.7,"msi =
2β2"
N,0.7022727272727273,(Dsi)2 ln 2(k+1)dm
N,0.7045454545454546,"δ′
+
2
Dsi ln 2kdm(1 −γ)(1 −p∗)"
N,0.7068181818181818,"p∗γ
(40)"
N,0.7090909090909091,where p∗is the prior assigned to the true cutting edge configuration C∗(Si) and δ′ = δ
N,0.7113636363636363,"p. Then we
have with P(C∗(Si) | Datado(si)) ≥1 −γ with probability at least 1 −δ for every target set Si ∈S.
This completes the proof of Corollary 4."
N,0.7136363636363636,"D
Procedure to Construct Separating Systems"
N,0.7159090909090909,"D.1
(n, k)-Separating System"
N,0.7181818181818181,"Lemma 5 ( [Shanmugam et al., 2015]). There exists a labeling procedure that gives distinct labels of
length l for all elements in [n] using letter from the integer alphabet {0, 1, ..., a}, where l = ⌈loga n⌉.
Furthermore, in every position, any integer letter is used at most ⌈n"
N,0.7204545454545455,a ⌉times.
N,0.7227272727272728,"The labeling method in Lemma 5 from Shanmugam et al. [2015] is described as:
Labelling Procedure: Let a > 1 be a positive integer. Let x be the integer such that ax < n ≤ax+1.
x + 1 = ⌈loga n⌉. Every element j ∈[n] is given a label L(j) which is a string of integers
of length x + 1 drawn from the alphabet {0, 1, ..., a} of size a + 1. Let n = pdad + rd and
n = pd−1ad−1 + rd−1 for any integers pd, pd−1, rd, rd−1, where rd < ad and rd−1 < ad−1. Now,
we describe the sequence of the d-th digit across the string labels of all elements from 1 to n:"
N,0.725,"1. Repeat the integer 0 a total of ad−1 times, and then repeat the subsequent integer, 1, also
ad−1 times from {0, 1, ..., a −1} till pdad.
2. Following this, repeat the integer o a number of times equal to ⌈rd"
N,0.7272727272727273,"a ⌉, and the repeat the
integer 1 ⌈rd"
N,0.7295454545454545,"a ⌉times, continuing this pattern until we reach the nth position. It is evident
that the nth integer in the sequence will not exceed n −1.
3. Each integer that appears beyond the position ad−1pd−1 is incremented by 1."
N,0.7318181818181818,"Once we have a set of n string labels, we can easily construct a (n, k)-separating system using the
following Lemma:
Lemma 6 ( [Shanmugam et al., 2015]). Consider an alphabet A = [0 : ⌈n"
N,0.7340909090909091,k ⌉] of size ⌈n
N,0.7363636363636363,"k ⌉+ 1 where
k < n"
N,0.7386363636363636,"2 . Label every element of an n element set using a distinct string of letters from A of length
l = ⌈log⌈n"
N,0.740909090909091,k ⌉n⌉using the labeling procedure in Lemma 5 with a = ⌈n
N,0.7431818181818182,"k ⌉. For every 1 ≤a ≤l and
1 ≤b ≤⌈n"
N,0.7454545454545455,"k ⌉, we choose the subset Ia,b of vertices whose string’s a-th letter is b. The set of all such
subsets S = {sa,b} is a k-separating system on n elements and |S| ≤(⌈n"
N,0.7477272727272727,k ⌉)⌈log⌈n
N,0.75,k ⌉n⌉.
N,0.7522727272727273,"D.2
G-Separating System"
N,0.7545454545454545,"While an (n, k)-separating system separates every edge in the complete graph of order n, usually
the causal graphs are sparse with much fewer edges. Thusly, we also provide a simple algorithm to
construct a separating system that cuts all the edges in a graph."
N,0.7568181818181818,"Definition 4 (G-Separating System, Definition 3 of Kocaoglu et al. [2017]). Given an undirected
graph G = (V, E), a set of subsets I ⊆2V is a G-separating system if for every edge {u, v} ∈E,
there exists I ∈I such that either (u ∈Ii and v /∈Ii)or (u /∈Ii and v ∈Ii)."
N,0.759090909090909,"Kocaoglu et al. [2017] show that an intervention set I learns every DAG entailed by G if and only if
it is a G-separating system. For a graph with n vertices, a G-separating system S = S1, S2, ..., Sm,
such that |Si| ≤k; ∀i ∈[m], is called a (n, k)-separating system. There are many ways to get a
G-separating system. Algorithm 2 gives a simple algorithm that uses vertex coloring. We first find the
perfect elimination order (PEO) L of G. PEO can be found by maximum cardinality search [Tarjan
and Yannakakis, 1984]. Then based on the reverse PEO, we apply greedy coloring to assign each
vertex with a number i ∈[ω] where ω is the clique number of G. ω colors are guaranteed to color G
since G is a perfect graph. In each step, we assign a vertex with the minimum color number that is
not used by its visited neighbors, thus no adjacent vertices will have the same color. For each vertex,
the union of it and its visited neighbors induce a clique, meaning that its color number is bounded by
ω. Consequently, Algorithm 2 returns a G-separating system with ω intervention sets as it cuts every
edge in G. We use f(·) to represent a function that maps a set of vertices to a set of colors that are
used by the vertices. N(v) denotes the neighborhood of a vertex v ∈V."
N,0.7613636363636364,Algorithm 2: G-separating system of a given unoriented graph G
N,0.7636363636363637,"Data: Input UCCG G = (V, E), |V| = n
Result: Output the G-separating system S
Initialization: Find the PEO L and ω of G by Maximum Cardinality Search;
L ←Reverse L, Si = ∅, ∀i ∈[ω], f(V) = {0};
for i = 1 to n do"
N,0.7659090909090909,"ci ←min{[ω] −f(N(Li))};
f(Li) ←{ci};
Sci ←Sci ∪{Li};"
N,0.7681818181818182,"S = ∪i∈[ω]{Si};
return S;"
N,0.7704545454545455,"E
Algorithm for Enumerating Causal Effects"
N,0.7727272727272727,"Algorithm 3: Enumerating all interventional distributions and priors of an intervention set I on
an undirected graph G"
N,0.775,"Data: Input UCCG G = (V, E), intervention set S ⊆V
Result: Output all possible interventional distribution P C(S)
S
(V) and P(C(S))
Initialization: Enumerate all possible edge configurations C(S) that are adjacent to vertices in S;
for Ck(S) in C(S) do"
N,0.7772727272727272,if Ck(S) not valid then
N,0.7795454545454545,"Remove Ck(S) from C(S);
Pass;"
N,0.7818181818181819,"Get the MPDAG MCk(S) by applying Meek Rules;
for H ∈CC(MCk(S)) do"
N,0.7840909090909091,"Sample a DAG DH from [H];
Replace edges in G with arcs in DH;"
N,0.7863636363636364,"Calculate P Ck(S)
S
(V) with Equation 1;
Calculate the prior P(Ck(S)) with Equation 8;"
N,0.7886363636363637,"return C(S), P C(S)
S
(V), P(C(S));"
N,0.7909090909090909,"F
Discussion on Computational Complexity"
N,0.7931818181818182,"For Bayesian approach of learning causal graph, we mainly care about the space complexity because
for run time, all the steps (e.g. calculating likelihood, updating posteriors) are efficient. For dense
graphs, a fully Bayesian method and our algorithm are both computationally expensive. For example,
for a complete graph with order n, Fully Bayesian method needs to store O(n!) distributions because
there are such many DAGs in the MEC. Our method needs to track O(2n−1) posteriors, which is less
than fully Bayesian but still exponentially many and would be intractable when n is large. However,
our method would be a lot better when the given graph is sparse, i.e. the maximum degree d in the
UCCG G is around log n. If we consider atomic interventions, the valid configurations around a
vertex v ∈V, |C(v)| < 2d = n. While the MEC size of a UCCG with order n is at least n, i.e.,
|[G]| ≥n. The equality holds if and only if G is a tree. Thus, our method is at least as good as a fully
Bayesian method. However, even for sparse graphs, its MEC size could be exponentially many in n.
Let’s consider the following example. Given a sparse UCCG G with order n. G consists of [
n
[log n]]
small cliques. Each small clique contains [log n] vertices. The cliques are connected together to form
a ’clique line’. The MEC size of G would be:"
N,0.7954545454545454,"|[G]| ≥([log n]!)[
n
[log n] ]
(41)"
N,0.7977272727272727,"According to Stirling’s Approximation, we have:"
N,0.8,|[G]| ∈O(([log n]
N,0.8022727272727272,"e
)[log n](log n)−1"
N,0.8045454545454546,"2 )
(42)"
N,0.8068181818181818,∈O([log n][log n]n−1[log n]−1
N,0.8090909090909091,"2 )
(43)"
N,0.8113636363636364,∈O(elog[log n]·[log n]n−1[log n]−1
N,0.8136363636363636,"2 )
(44)"
N,0.8159090909090909,∈O(nlog[log n]−1[log n]−1
N,0.8181818181818182,"2 )
(45)"
N,0.8204545454545454,"Which is approximately O(nlog[log n]). If we consider an interventional target of size k for our algo-
rithm, there are at most 2kd = nk configurations to track. If n is large enough, k < log[log n], ∀k > 0.
Thus, the MEC size for G is greater than any polynomial complexity, while our algorithm tracks at
most polynomially many configurations."
N,0.8227272727272728,"G
Further Discussion on Case Study"
N,0.825,"In Section 4.2, we mentioned that the Causal Identification Formula for MPDAG from Perkovic
[2020] could also be used when enumerating the causal effects. Here we show that our use of DAG
sampler provides a more flexible and general approach of causal discovery with special queries that
do not rely on the cut configurations of the MPDAG. In many real-world settings, our focus is on
specific queries rather than understanding the entire graph. For instance, when a cloud computing
system encounters an error, we aim to efficiently identify the root cause. Similarly, when a patient
exhibits symptoms of cancer, we want to determine if certain attributes of the patient are direct causes
of the cancer. Here, we consider the scenario mentioned in Malinsky [2024], where the goal is to
estimate the posterior probability of a set of variables being the adjustment set of a causal query
given the essential graph. The adjustment set is crucial in estimating the causal effect. Given the
essential graph G, denote the variable of a set S being the adjustment set as AS. AS = 1 if S is the
adjustment set and 0 otherwise. We want to estimate P(AS|v), where v could be any data collected,
both observational and interventional, Denote the true DAG as D∗, the posterior could be expressed as:"
N,0.8272727272727273,"P(AS|v) ∝
X"
N,0.8295454545454546,"D∈[E(D∗)]
P(AS|v, D)P(v|D)P(D)
(46)"
N,0.8318181818181818,"Where P(AS|v, D) = P(AS|D) is an indicator function. Equation 46 could be rewritten as:"
N,0.8340909090909091,"ES = ED∼[E(D∗)][P(AS|D)P(v|D)P(D)]
(47)"
N,0.8363636363636363,"Which can be approximated by sampling DAGs from the MEC [D∗]. If we have M DAG samples,
define S as S = M
X"
N,0.8386363636363636,"i=1
P(AS|Di)P(v|Di)P(Di); Di ∈[E(D∗)]
(48)"
N,0.8409090909090909,"According to Hoeffding Inequality, the difference between S and ES could be bounded by"
N,0.8431818181818181,"P(

S
M −ES"
N,0.8454545454545455,"≥ϵ) ≤exp(−2Mϵ2)
(49)"
N,0.8477272727272728,"By estimating the posteriors for both P(AS = 1|v) and P(AS = 0|v), we can normalize them to get
the exact posterior. For large and dense graphs which are hard to iterate through each DAG, we could
estimate efficiently by sampling DAGs according to intended accuracy. With enough DAG samples,
it shows that the estimation converges to the true posterior with a high probability."
N,0.85,"H
Additional Experiments"
N,0.8522727272727273,"H.1
Additional Experiments on Large Graphs"
N,0.8545454545454545,"We further test our proposed algorithm against the baseline methods on larger random chordal graphs.
The results are shown in Figure 4. We can see that our algorithm converges very fast to a low SHD.
The performance of our algorithm does not differ much from that in small graphs. The baselines, on
the other hand, require a lot more samples to reach the same SHD as in small graphs.
For extremely large graphs, it would be intractable to store the whole observational distribution,
instead we could all the conditional distributions of the factorizations. Given large enough observa-
tional samples, the conditional distributions will be accurate and then we can perform the Algorithm 1
for Bayesian learning."
N,0.8568181818181818,"(a) n = 20, ρ = 0.05
(b) n = 20, ρ = 0.1
(c) n = 20, ρ = 0.15"
N,0.8590909090909091,Figure 4: SHD vs number of interventional samples for large random Erd˝os-Rényi chordal graphs
N,0.8613636363636363,"H.2
Additional Experiment Scale-Free Graphs"
N,0.8636363636363636,"In this experiment, we demonstrate the performance of our proposed algorithm on scale-free graphs.
For each experiment settings, we generate 50 random DAGs from Barabási-Albert (BA) model. The
results are plotted in Figure 5. Our proposed method still outperforms other methods."
N,0.865909090909091,"(a) n = 7, m = 2
(b) n = 7, m = 4"
N,0.8681818181818182,"Figure 5: SHD vs number of interventional samples for scale-free graphs generated from Barabási-
Albert (BA) model. We generated 50 random DAGs under two settings and plot the average SHD and
standard deviation."
N,0.8704545454545455,"H.3
Additional Experiment on Other Baseline"
N,0.8727272727272727,"In this experiment, we compare our proposed algorithm with a Bayesian causal discovery method,
AVICI, proposed in Lorch et al. [2022]. For AVICI, we fine-tuned the pretrained models, scm-v0 and
neurips-rff, with 50 random complete graphs, each with 1000 observational samples. The results are
plotted in Figure 6. We used SHD to measure our algorithm and expected SHD for AVICI. Here, since
AVICI can not take in too many samples, we just show the performance with 1000 interventional
samples. At each step for both methods, we choose the interventional target randomly. For AVICI,
we feed in 1000 observational samples together with the interventional samples. The results show
that our algorithm performs better, while AVICI failed to learn the causal structure with high expected
SHD. AVICI’s bad performance in this setting may be caused by its parametric model on the SCM or
noise. Also, there is no theoretical guarantee on AVICI’s performance and convergence to the ground
truth. Furthermore, our algorithm benefits has the advantage of being able to update the posterior
given any number of samples, while AVICI would fail when the number of samples is large."
N,0.875,"(a) n = 5, ρ = 1"
N,0.8772727272727273,"Figure 6: SHD vs number of interventional samples for random complete graphs. We generate
50 random DAGs and plot the average SHD and standard deviation. scm-v0 and neurips-rff are
pretrained models in the paper."
N,0.8795454545454545,NeurIPS Paper Checklist
CLAIMS,0.8818181818181818,1. Claims
CLAIMS,0.884090909090909,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.8863636363636364,Answer: [Yes]
CLAIMS,0.8886363636363637,Justification: We discuss this in Section 1.
CLAIMS,0.8909090909090909,Guidelines:
CLAIMS,0.8931818181818182,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.8954545454545455,2. Limitations
LIMITATIONS,0.8977272727272727,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9,Answer: [Yes]
LIMITATIONS,0.9022727272727272,Justification: We discuss this in Section 8.
LIMITATIONS,0.9045454545454545,Guidelines:
LIMITATIONS,0.9068181818181819,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9090909090909091,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9113636363636364,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9136363636363637,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9159090909090909,"Justification: We discuss this in Section 5, Section 3, and Appendix 8.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9181818181818182,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.9204545454545454,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We discuss this in Section7.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9227272727272727,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
THEORY ASSUMPTIONS AND PROOFS,0.925,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
THEORY ASSUMPTIONS AND PROOFS,0.9272727272727272,"Answer: [Yes]
Justification: We discuss this in Section 7.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9295454545454546,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
THEORY ASSUMPTIONS AND PROOFS,0.9318181818181818,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We discuss this in Section 7.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9340909090909091,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
THEORY ASSUMPTIONS AND PROOFS,0.9363636363636364,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We discuss this in Section 7.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9386363636363636,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean."
THEORY ASSUMPTIONS AND PROOFS,0.9409090909090909,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
THEORY ASSUMPTIONS AND PROOFS,0.9431818181818182,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We discuss this in Appendix8.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9454545454545454,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
THEORY ASSUMPTIONS AND PROOFS,0.9477272727272728,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We conform with the NeurIPS Code of Ethics.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.95,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
THEORY ASSUMPTIONS AND PROOFS,0.9522727272727273,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We discuss this in Section 8.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9545454545454546,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to"
THEORY ASSUMPTIONS AND PROOFS,0.9568181818181818,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9590909090909091,11. Safeguards
SAFEGUARDS,0.9613636363636363,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9636363636363636,Answer: [NA]
SAFEGUARDS,0.9659090909090909,Justification: We do not use any dataset or model that has this concern.
SAFEGUARDS,0.9681818181818181,Guidelines:
SAFEGUARDS,0.9704545454545455,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9727272727272728,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.975,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9772727272727273,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9795454545454545,"Justification: We cite the assets and mention the license explicitly in the paper and Readme
of the code."
LICENSES FOR EXISTING ASSETS,0.9818181818181818,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9840909090909091,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9863636363636363,13. New Assets
NEW ASSETS,0.9886363636363636,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We provide the descriptions in the Readme file.
Guidelines:"
NEW ASSETS,0.990909090909091,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
NEW ASSETS,0.9931818181818182,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our work does not involve crowdsourcing or human subjects.
Guidelines:"
NEW ASSETS,0.9954545454545455,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Our work does not involve crowdsourcing or human subjects.
Guidelines:"
NEW ASSETS,0.9977272727272727,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
