Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002717391304347826,"Robotic manipulation planning is the problem of ﬁnding a sequence of robot con-
ﬁgurations that involves interactions with objects in the scene, e.g., grasp, place-
ment, tool-use, etc. To achieve such interactions, traditional approaches require
hand-designed features and object representations, and it still remains an open
question how to describe such interactions with arbitrary objects in a ﬂexible and
efﬁcient way. Inspired by recent advances in 3D modeling, e.g. NeRF, we pro-
pose a method to represent objects as neural implicit functions upon which we
can deﬁne and jointly train interaction constraint functions. The proposed pixel-
aligned representation is directly inferred from camera images with known camera
geometry, naturally acting as a perception component in the whole manipulation
pipeline, while at the same time enabling sequential robot manipulation planning."
INTRODUCTION,0.005434782608695652,"1
INTRODUCTION"
INTRODUCTION,0.008152173913043478,"Intelligent agents should be able to interact with objects in the environment, such as grasping and
placing an object, or more general tool-use, to achieve a certain goal. In robotics, such instances
are formalized as manipulation planning, a type of a motion planning problem that solves not only
for the robot’s own movement but also for the objects’ motions subject to interaction constraints.
Traditional approaches represent objects using meshes or combinations of shape primitives and de-
scribe interactions as hand-crafted constraints in terms of that representation. The approach of using
such traditional geometric representations has long-standing limitations in terms of their perception
and generalizing to large varieties of objects and interaction modes: (i) The representations have to
be inferred from raw sensory inputs like images or point clouds – raising the fundamental problem
of perception and shape estimation. However, if the aim is manipulation skills, the hard problem
of precise shape estimation might be unnecessary to predict accurate interaction features1, and an
end-to-end object representation might be more appropriate than a standard perception pipeline. (ii)
With increasing generality of object shapes and interaction, the complexity of representations grows
and hand-engineering of the interaction features becomes inefﬁcient."
INTRODUCTION,0.010869565217391304,"What is a good representation of an object? Considering the representation will be used to predict
interaction features, we expect it to encode primarily task-speciﬁc information rather than only geo-
metric. And some of the information should to be shared across different interaction modes. In other
words, good representations should be task-speciﬁc so that the feature prediction can be simpliﬁed
and, at the same time, be task-agnostic to enable synergies between the features. E.g., mug handles
are called handles because we can handle the mug through them and also, once we learn the notion
of a handle, we can interact with the mug through them in many different ways. From the perception
aspect, good representations should be easy to infer from raw sensory inputs and should be able to
trade their accuracy off in favor of the feature prediction."
INTRODUCTION,0.01358695652173913,"To this end, we propose a novel data-driven approach to learning interaction features. The proposed
feature prediction scheme is illustrated in Fig. 1. The whole pipeline is trained end-to-end directly
with the task supervisions so as to make the representation and perception task-speciﬁc and thus
to simplify the interaction prediction. The object representation acts as a bottleneck and is shared
across multiple feature predictions so that the task-agnostic representations can emerge. Particularly,
the object representation is a neural implicit function over the 3D space (Park et al., 2019; Milden-
hall et al., 2020) upon which equality constraint features are trained. The proposed neural implicit"
INTRODUCTION,0.016304347826086956,"1We call an interaction constraint function an interaction feature; when used as equality constraints, the
interaction features, analogous to energy potentials, return zero when feasible and non-zero otherwise."
INTRODUCTION,0.019021739130434784,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.021739130434782608,"robot
frame’s pose "
INTRODUCTION,0.024456521739130436,"
p1
...
pN  "
INTRODUCTION,0.02717391304347826,"obj images,
cam poses,
intrinsics ... PIFO PIFO "
INTRODUCTION,0.029891304347826088,"
y1
...
yN  "
INTRODUCTION,0.03260869565217391,"shared
(Sec.3.1)"
INTRODUCTION,0.035326086956521736,"feature
head
h"
INTRODUCTION,0.03804347826086957,"task speciﬁc
(Sec.3.2)"
INTRODUCTION,0.04076086956521739,Figure 1: The interaction feature prediction scheme
INTRODUCTION,0.043478260869565216,"function is pixel-aligned: The function takes images from multiple cameras as input (e.g. stereo)
and, assuming known camera poses and intrinsics, the latent representation at a certain spatial loca-
tion is directly related to pixels of the images. Once learned, the interaction features can be used by a
typical constrained optimal control framework to plan dexterous object-robot interaction. We adopt
Logic-Geometric Programming (LGP) (Toussaint et al., 2018) as an optimization-based manipu-
lation planning framework and show that this learned-feature based planning enables to compute
trajectories that involve various types of interaction modes only from images. Due to the represen-
tations’ generalization, the learned features are directly applicable to manipulation tasks involving
unseen objects. To summarize, our main contributions are"
INTRODUCTION,0.04619565217391304,"• To represent objects as neural implicit functions upon which interaction features are trained,
• An image-based manipulation planning framework with the learned features as constraints,
• Comparison to non pixel-aligned, non implicit function, and geometric representations,
• Demonstration in various manipulation scenarios ranging from simple pick-and-
hang [videos] to longer-horizon manipulations [videos] and zero-shot imitations [videos]."
RELATED WORK,0.04891304347826087,"2
RELATED WORK"
RELATED WORK,0.051630434782608696,"2.1
NEURAL IMPLICIT REPRESENTATIONS IN 3D MODELING AND VIEW SYNTHESIS"
RELATED WORK,0.05434782608695652,"Neural implicit representations have recently gained increasing attention in 3D modeling. The core
idea is to encode an object or a scene in the weights of a neural network, where the network acts
as a direct mapping from 3D spatial location to an implicit representation of the model, such as
occupancy measures (Mescheder et al., 2019; Songyou Peng, 2020) or signed distance ﬁelds (Park
et al., 2019; Gropp et al., 2020; Atzmon & Lipman, 2020). In contrast to explicit representations like
voxels, meshes or point clouds, the implicit representations don’t require discretization of the 3D
space nor ﬁxed shape topology but rather continuously represent the 3D geometry, thereby allowing
for capturing complex shape geometry at high resolutions in a memory efﬁcient way."
RELATED WORK,0.057065217391304345,"There have been attempts to associate these 3D representations with 2D images using the principle
of camera geometry. Exploiting the camera geometry in a forward direction, i.e., 2D projection
of 3D representations, yields a differentiable image rendering procedure and this idea can be used
to get rid of 3D supervisions. For example, Sitzmann et al. (2019); Niemeyer et al. (2020); Yariv
et al. (2020); Mildenhall et al. (2020); Henzler et al. (2021); Reizenstein et al. (2021) showed that
the representation networks can be trained without the 3D supervision by deﬁning a loss function
to be difference between the rendered images and the ground-truth. Another notable application of
this idea is view synthesis. Based on the differentiable rendering, Park et al. (2020); Chen et al.
(2020); Yen-Chen et al. (2021) addressed unseen object pose estimation problems, where the goal
is to ﬁnd object’s pose relative to the camera that produces a rendered image closest to the ground
truth. By conditioning 3D representations on 2D input images, one can expect the amortized encoder
network to directly generalize to novel 3D geometries without requiring any test-time optimization.
This can be done by introducing a bottleneck of a ﬁnite-dimensional global latent vector between
the images and representations, but these global features often fail to capture ﬁne-grained details of
the 3D models (Songyou Peng, 2020). To address this, the camera geometry can be exploited in
the inverse direction to obtain pixel-aligned local representations, i.e., 3D reprojection of 2D image
features. Saito et al. (2019) and Xu et al. (2019) showed that the pixel-aligned methods can establish
rich latent features because they can easily preserve high-frequency components in the input images.
Also, Yu et al. (2021) and Trevithick & Yang (2021) incorporated this idea within the view-synthesis
framework and showed that their convolutional encoders have strong generalizations."
RELATED WORK,0.059782608695652176,Under review as a conference paper at ICLR 2022
RELATED WORK,0.0625,"While the above work investigates neural implicit functions to model shapes or appearances, we
train them to model physical interaction feasibility and thereby to provide a differentiable constraint
model for robot manipulation planning."
RELATED WORK,0.06521739130434782,"2.2
OBJECT/SCENE REPRESENTATIONS FOR ROBOTIC MANIPULATIONS"
RELATED WORK,0.06793478260869565,"Several works have proposed data-driven approaches to learning object representations and/or in-
teraction features which are conditioned on raw sensory inputs, especially for grasping of diverse
objects. One popular approach is to train discriminative models for grasp assessments. For example,
ten Pas et al. (2017); Mahler et al. (2017); Van der Merwe et al. (2020) trained a neural network that,
for given candidate grasp poses, predicts their grasp qualities from point clouds. In addition, Breyer
et al. (2020); Jiang et al. (2021) proposed 3D convolutional networks that take as inputs a truncated
SDF and candidate grasp poses and return the grasp affordances. Similarly, Zeng et al. (2020b;a)
addressed more general manipulation scenarios such as throwing or pick-and-place, where a con-
volutional network outputs a task score image. On the other hand, neural networks also have been
used as generative models. For example, Mousavian et al. (2019) and Murali et al. (2020) adopted
the approach of conditional variational autoencoders to model the feasible grasp pose distribution
conditioned on the point cloud. Sundermeyer et al. (2021) proposed a somewhat hybrid method,
where the network densely generates grasp candidates by assigning grasp scores and orientations
to the point cloud. You et al. (2021) addressed the object hanging tasks from point clouds where
the framework ﬁrst makes dense predictions of the candidate poses among which one is picked and
reﬁned. Compared to these works, our framework takes advantage of a trajectory optimization to
jointly optimize an interaction pose sequence instead of relying on exhaustive search or heuristic
sampling schemes, thus not suffering from the high dimensionality nor the combinatorial complex-
ity of long-horizon planning problems."
RELATED WORK,0.07065217391304347,"Another important line of research is learning and utilizing keypoint object representations.
Manuelli et al. (2019); Gao & Tedrake (2021); Qin et al. (2020); Turpin et al. (2021) represented
objects using a set of 3D semantic keypoints and formulated manipulation problems in terms of
such the keypoints. Similarly, Manuelli et al. (2020) learned the object dynamics as a function of
keypoints upon which a model predictive controller is implemented. Despite their strong gener-
alizations to unseen objects, the keypoint representations require semantics of the keypoints to be
predeﬁned. The representation part of our framework is closely related to dense object descriptions
proposed by Florence et al. (2018; 2019). The idea is to train fully-convolutional neural networks
that maps a raw input image to pixelwise object representations which directly generalize to un-
seen objects. Our proposed framework can be seen as an extension of this pixelwise representation
to dense representations over the 3D space which is learned by the task supervisions and can be
seamlessly integrated into general sequential manipulation planning problems. Another recent re-
lated work was proposed by Yuan et al. (2021), where the learned object-centric representations are
used to predict the symbolic predicates of the scene which in turn enables symbolic-level task plan-
ning. In contrast, our framework predicts the task feasibility given a robot conﬁguration and enables
trajectory optimization of the lower-level continuous motions."
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.07336956521739131,"3
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.07608695652173914,"Given Nview images with their camera poses/intrinsics, {(I1, T 1, K1), ..., (INview, T Nview, KNview)},
we deﬁne an interaction feature as a neural implicit function:"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.07880434782608696,"h = φtask(q; {(I1, T 1, K1), ..., (INview, T Nview, KNview)}),
(1)"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.08152173913043478,"where q ∈SE(3) is the pose of the robot frame interacting with the object. As shown in Fig.
1, the feature prediction framework consists of two parts: the representation network, which we
call a backbone, and the feature head networks. The backbone serves as an implicit functional
representation of an object, which, conditioned on a set of posed images, outputs d-dimensional
representation vectors at queried 3D spatial locations. The interaction feature predictions are made
through the feature heads, where each head is fed on a set of representation vectors obtained by
querying the backbone at a set of key interaction points. While the multiple feature heads separately
model different interactions, the backbone is shared across the tasks, making it learn more general
object representations. The rest of this section will be devoted to introduce each module in detail."
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.08423913043478261,"Under review as a conference paper at ICLR 2022 64 64 64
64 32"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.08695652173913043,"128
128 16"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.08967391304347826,"256
256 8"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.09239130434782608,"512
512 4"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.09510869565217392,Bottleneck Conv
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.09782608695652174,"256
256
256 8"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.10054347826086957,"128
128
128 16"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.10326086956521739,"64
64
64 32"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.10597826086956522,64 64 64 64 1 64
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.10869565217391304,"local
img
feat 1 3 p 1 3 z
1 32"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.11141304347826086,"coord
feat × 1 256"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.11413043478260869,local2 1 128
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.11684782608695653,local3
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.11956521739130435,projection
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.12228260869565218,Figure 2: Image encoder and 3D reprojector of the backbone network.
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.125,"3.1
PIXEL-ALIGNED IMPLICIT FUNCTIONAL OBJECT REPRESENTATION (PIFO)"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.12771739130434784,"Given Nview posed images, we deﬁne a functional representation of an object as a mapping:"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.13043478260869565,"ψ(p; {(I1, T 1, K1), ..., (INview, T Nview, KNview)}) = y,
(2)"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.1331521739130435,"where p ∈R3 and y ∈Rd are a queried 3D position and a representation vector at that point,
respectively. The representation network consists of three parts: image encoder, 3D reprojector,
and feature aggregator. The ﬁrst two modules compute a representation vector for each image, as
depicted in Fig. 2, and the last one combines them into one vector."
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.1358695652173913,"Image Encoder: This module takes as input an image and computes a feature image. In order to
capture both local and global information in the image, we adopted the U-net architecture (Ron-
neberger et al., 2015), especially with ResNet-34 (He et al., 2016) as its downward path and two
residual 3 × 3 convolutions followed by up-convolution as the upward path, i.e.,"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.13858695652173914,"Fn = UNet(In), ∀n ∈{1, ..., Nview}.
(3)"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.14130434782608695,"3D Reprojector: To endow the network with the multi-view consistency, all the 3D operations are
performed in the view space. The 3D reprojector ﬁrst converts a queried point, p, to the image
coordinate including depth, π(p; T , K) = z ∈R3 and then uses the projected point to extract
the local image feature from the feature image, F, via bilinear interpolation. Finally, the extracted
feature and the coordinate feature, which is necessary to handle out-of-image query points, are
passed to a couple of fully connected layers to get a representation vector for a single image, i.e.,"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.14402173913043478,"yn = MLP(Fn(zn), zn), zn = π(p; T n, Kn), ∀n ∈{1, ..., Nview}.
(4)"
INTERACTION FEATURE PREDICTION VIA IMPLICIT REPRESENTATION,0.14673913043478262,"Feature Aggregator: The feature aggregation from the multiple views should be permutation-
invariant. While there are many options, like summation, averaging, or more sophisticated attention
mechanisms, we simply take the averaging operation for it, i.e. y =
1
Nview
P
n yn."
FEATURE PREDICTION,0.14945652173913043,"3.2
FEATURE PREDICTION"
FEATURE PREDICTION,0.15217391304347827,"A feature head network predicts the interaction constraint value of a robot frame’s pose via the object
representation. We ﬁrst attach a set of keypoints to the robot frame and query the backbone at those
keypoint positions, i.e., ∀i ∈{1, ..., Nkeypoint},"
FEATURE PREDICTION,0.15489130434782608,"yi = ψ(pi; {(In, T n, Kn)|n ∈{1, ..., Nview}}), pi = R(q)ˆpi + t(q),
(5)"
FEATURE PREDICTION,0.15760869565217392,"where ˆpi is ith keypoint’s local coordinate, and R(q) and t(q) denote the rotation matrix and the
translation vector, respectively. The feature head then takes as input the resulting representation
vectors and predicts a feature value through a couple of fully connected layers, i.e.,"
FEATURE PREDICTION,0.16032608695652173,"h = MLP(y1, ..., yNkeypoint).
(6)"
FEATURE PREDICTION,0.16304347826086957,"Note that when the considered feature is an SDF, only a single point needs to be queried, i.e.,
Nkeypoint = 1, so the whole architecture reduces to the surface reconstruction in Saito et al. (2019)."
FEATURE PREDICTION,0.16576086956521738,Under review as a conference paper at ICLR 2022
TRAINING,0.16847826086956522,"4
TRAINING"
TRAINING,0.17119565217391305,"In this paper, we consider manipulation scenarios where a robot arm, Franka Emika Panda, or two
have to pick and hang mugs on hooks. The environment contains mugs having different shapes
and hooks on which a mug can be hung. To formulate this manipulation problem, three interaction
features are considered: an SDF feature for collision avoidance and grasping/hanging features."
DATA GENERATION,0.17391304347826086,"4.1
DATA GENERATION"
DATA GENERATION,0.1766304347826087,"We took 131 mesh models of mugs from ShapeNet (Chang et al., 2015) and convex-decomposed
those meshes. The meshes are translated and randomly scaled so that they can ﬁt in a bounding
sphere with a radius of 10 ∼15 cm at the origin. For each mug, we created datasets of the posed
images and each interaction as follows."
DATA GENERATION,0.1793478260869565,"Posed Images: The posed image data consists of 100 images (128 × 128) with the corresponding
camera poses and intrinsic matrices generated by an OpenGL renderer. Azimuths and elevations of
the cameras are sampled such that they can uniformly be distributed on the surface of a sphere, while
their distances from the object center are randomly chosen. The azimuth, elevation and distance
fully determine the camera’s positions and making them upright and face the object center gives
the camera’s orientations. For the intrinsics, we used fov = 2 arcsin(d/r), where d is the camera
distance from the object center and r is the radius of the object’s bounding sphere, so that the entire
object appears in the image. Lighting is also randomized."
DATA GENERATION,0.18206521739130435,"SDF: We sampled 12,500 3D positions and computed their signed distance values using the mesh-to-
sdf library (Kleineberg, 2021). Following the approach of DeepSDF (Park et al., 2019), we sampled
more aggressively near the object surface to foster the learning of the object geometry."
DATA GENERATION,0.18478260869565216,"Grasping & Hanging: We obtained 1,000 feasible grasping and hanging poses of the gripper and
the hook, respectively. For grasping, we used an antipodal sampling scheme, similarly to Eppner
et al. (2021), to create candidate gripper poses and checked their feasibility using Bullet (Coumans
& Bai, 2016–2021). For hanging, we randomly sampled collision-free hook poses and checked if
it’s kinematically trapped by the mug in the directions perpendicular to the hook’s main axis."
DATA GENERATION,0.1875,"Fig. 8 shows some rough looks of the generated data. In the end, we have a dataset of:"
DATA GENERATION,0.19021739130434784,"D =
n 
I1:100, T 1:100, K1:100, p1:12500, SDF 1:12500, q1:1000
grasp , q1:1000
hang
(i)o131"
DATA GENERATION,0.19293478260869565,"i=1 ,
(7)"
DATA GENERATION,0.1956521739130435,"which we divided into 78 training, 25 validation and 28 test sets."
DATA AUGMENTATION AND LOSS FUNCTION,0.1983695652173913,"4.2
DATA AUGMENTATION AND LOSS FUNCTION"
DATA AUGMENTATION AND LOSS FUNCTION,0.20108695652173914,"Data Augmentation: While randomizing the azimuth, elevation and distance of the camera provides
all possible appearances of the object, it still cannot account for the roll angles of the camera and
off-centered images. To show the network all possible images that it can encounter when deployed
later and to mitigate the size-ambiguity issue, we propose to use a data augmentation technique
based on Homography warping: In each iteration, for a randomly sampled set of images, we arti-
ﬁcially perturb the roll angle of each camera and the estimated object center position (at which the
cameras are looking). Also, fov is modiﬁed as if the radius of the bounding sphere is 15 cm so that
smaller objects can appear smaller in the transformed images. This results in new rotation matrices,
ˆR, and intrinsic matrices, ˆ
K, of the cameras. We then compute the corresponding Homography
transformation and warp the images accordingly (details in Appendix A.1):"
DATA AUGMENTATION AND LOSS FUNCTION,0.20380434782608695,"W( ˆR, ˆ
K) :"
DATA AUGMENTATION AND LOSS FUNCTION,0.20652173913043478,"""u
v
1 #"
DATA AUGMENTATION AND LOSS FUNCTION,0.20923913043478262,"7→w ˆ
K ˆRT RK−1
""u
v
1 # .
(8)"
DATA AUGMENTATION AND LOSS FUNCTION,0.21195652173913043,"Random cutouts are also applied to address the object occlusion. Fig. 9 depicts how this image
augmentation works. For grasping and hanging, we generate 6D random poses ˆqtask ∼P in each
iteration2 and, similarly to Atzmon & Lipman (2020), set the training target to be unsinged distances
in SE(3) to the set of the feasible poses: d(q; q1:Ntask
task
) = mini=1,...Ntask ||q −qi
task||2."
DATA AUGMENTATION AND LOSS FUNCTION,0.21467391304347827,"2To encourage more precise prediction around the constraint manifolds, we used a weighted sum of a
feasible pose and a random pose ˆqtask = tqfeasible + (1 −t)qrand, t ∼U(0, 1) where the position of qrand is from
the normal distribution and its quaternion is sampled uniformly."
DATA AUGMENTATION AND LOSS FUNCTION,0.21739130434782608,Under review as a conference paper at ICLR 2022
DATA AUGMENTATION AND LOSS FUNCTION,0.22010869565217392,"Training: For the overall network training, we ﬁrst choose a minibatch of mugs from which a
subset of augmented images with their camera poses and intrinsics,

ˆI1:Nviews, ˆT 1:Nviews, ˆ
K1:Nviews

,"
DATA AUGMENTATION AND LOSS FUNCTION,0.22282608695652173,"a subset of SDF data,
 
p1:NSDF, SDF 1:NSDF
, and the grasping/hanging data,

ˆq1:Ntask
task
, d1:Ntask
task

, are
sampled. The images are encoded only once per iteration and then the SDF, grasping, hanging
features are queried at the sampled points and poses. The overall loss is given as Ltotal = Lsdf +
Lgrasp + Lhang, where a typical L1 loss is used for SDFs, i.e. Lsdf =
1
NSDF
PNSDF
i=1 |φsdf(pi) −SDF i|,
and the sign-agnostic L1 loss in (Atzmon & Lipman, 2020) for grasping and hanging, i.e., Ltask =
1
Ntask
PNtask
i=1
φtask(ˆqi
task)
 −di
task
 , ∀task ∈{grasp, hang}.3 The feature head and backbone are
trained end-to-end. Speciﬁcally, we used Nviews = 4, NSDF = 300, Ngrasp = 100, Nhang = 100. As
shown in Fig. 11, the grasp and hang interaction points are deﬁned as (3 × 3 × 3) grid points around
the gripper center and as 5 points along the hook’s main axis, respectively."
SEQUENTIAL MANIPULATION PLANNING WITH LEARNED FEATURES,0.22554347826086957,"5
SEQUENTIAL MANIPULATION PLANNING WITH LEARNED FEATURES"
SEQUENTIAL MANIPULATION PLANNING WITH LEARNED FEATURES,0.22826086956521738,"robot
conﬁguration"
SEQUENTIAL MANIPULATION PLANNING WITH LEARNED FEATURES,0.23097826086956522,"forward
kinematics"
SEQUENTIAL MANIPULATION PLANNING WITH LEARNED FEATURES,0.23369565217391305,"robot
frame’s pose "
SEQUENTIAL MANIPULATION PLANNING WITH LEARNED FEATURES,0.23641304347826086,"
p1
...
pN  "
SEQUENTIAL MANIPULATION PLANNING WITH LEARNED FEATURES,0.2391304347826087,"raw images & masks,
camera poses,
intrinsics"
SEQUENTIAL MANIPULATION PLANNING WITH LEARNED FEATURES,0.2418478260869565,"multi-view
processing
(Sec.5.2)"
SEQUENTIAL MANIPULATION PLANNING WITH LEARNED FEATURES,0.24456521739130435,"obj images,
cam poses,
intrinsics"
SEQUENTIAL MANIPULATION PLANNING WITH LEARNED FEATURES,0.24728260869565216,"interaction
feature
prediction
(Sec.3) h"
SEQUENTIAL MANIPULATION PLANNING WITH LEARNED FEATURES,0.25,Figure 3: The proposed interaction feature prediction scheme for manipulation planning
SEQUENTIAL MANIPULATION PLANNING WITH LEARNED FEATURES,0.25271739130434784,"In order to compute a full trajectory of the robot and the objects that it interacts with, the learned
features can be integrated into any constraint-based trajectory optimization and manipulation plan-
ning framework, using the features as differentiable interaction constraints. In this work, we adopt
Logic-Geometric Programming (LGP) (Toussaint et al., 2018) as an optimization-based manipu-
lation planning framework. In typical manipulation scenes, cameras are equipped such that their
views cover a wide range of the environment. As shown in Fig. 3, we warp raw images of the entire
scene to get object-centric images and corresponding camera extrinsics/intrinsics and compute the
robot frame’s pose via forward kinematics to feed them into the network. Section 5.1 presents how
the learned constraint functions are integrated into sequential manipulation planning problems and
Section 5.2 discusses the proposed warping procedure."
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING,0.2554347826086957,"5.1
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING"
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING,0.25815217391304346,"The core concept of manipulation planning is the rigid transformations of objects. For an object
transformed by δq ∈SE(3), we deﬁne a rigid transformation of the interaction feature as:"
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING,0.2608695652173913,"T(δq)[φtask](·) := φtask
 
δq−1·

,
(9)"
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING,0.26358695652173914,"which is equivalent to transforming the representation as T(δq)[ψ](·) = ψ
 
R(δq)T (· −t(δq))

.
By composing the forward kinematics with the feature as"
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING,0.266304347826087,"Htask(x, δq) := (T(δq)[φtask] ◦FK) (x),
(10)"
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING,0.26902173913043476,"we obtain an interaction feature as a function of a robot joint conﬁguration x and object’s rigid
transformation that can be equipped into manipulation planning."
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING,0.2717391304347826,"Now we are ready to formally formulate manipulation planning problems. For an n-joint robot
and m rigid objects, LGP is a hybrid optimization problem over the number of phases K ∈N, a
sequence of discrete actions a1:K and sequences of the robot joint conﬁgurations x1:KT , x ∈Rn
and the object’s rigid transformations δq1:KT , δq ∈SE(3)m. The trajectory is discretized into
T steps per phase. A discrete action ak describes which interaction should be fulﬁlled at the end
of the phase k, i.e., which mug to pick or on which hook to hang a grasped mug, and uniquely"
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING,0.27445652173913043,"3For simplicity’s sake, we abuse the notation of Equation 1 as φtask(q) when it doesn’t lead to confusion."
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING,0.27717391304347827,Under review as a conference paper at ICLR 2022
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING,0.2798913043478261,"determines an interaction mode sk = succ(sk−1, ak), i.e., whether each mug is grasped or hung on
a particular hook. Suppose that a discrete action sequence a1:K and the corresponding modes s1:K
with sK ∈Sgoal are proposed by a logic tree search. We deﬁne the geometric path problem as a 2nd
order Markov optimization (Toussaint, 2017):"
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING,0.2826086956521739,"min
x1:KT
δq1:KT KT
X"
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING,0.28532608695652173,"t=1
f (xt−2:t) , s.t.∀k∈{1,...,K}∀H∈H(sk,ak) : H
 
(xt−2:t, δqi
t−2:t)(t,i)∈IH(sk,ak)

= 0, (11)"
LOGIC-GEOMETRIC PROGRAMMING FOR MANIPULATION PLANNING,0.28804347826086957,"where the initial joint states x−1:0 and objects’ transformations δq−1:0 = 04 are given. f is a path
cost that penalizes squared accelerations of the robot joints. H(sk, ak) is a set of path constraints the
discrete state and action impose on the geometric level at each phase k(t) = ⌊t/T⌋; these constraints
include physical consistency, collision avoidance, and the learned interaction constraints that ensure
the success of the discrete action ak. Lastly, IH(sk, ak) decides the time slice and object index that
are subject to the constraint H. Appendix A.2 introduces the set of imposed constraints in detail. As
all the cost and constraint terms are differentiable and their Jacobians/Hessians are sparse, we can
solve this optimization problem efﬁciently using the Gauss-Newton optimization method."
MULTI-VIEW PREPROCESSING,0.2907608695652174,"5.2
MULTI-VIEW PREPROCESSING"
MULTI-VIEW PREPROCESSING,0.29347826086956524,"Let Mn ∈{0, 1}W ×H be the object masks available along with the raw images In, ∀n =
1, ..., Ncam. We ﬁrst solve the following optimization to ﬁnd a position and radius of the minimal
bounding sphere such that the warped images contain all the object pixels in the original images:"
MULTI-VIEW PREPROCESSING,0.296195652173913,"min
p∈R3,r∈R+ r,
s.t. ∀(u,v)∈{(u′,v′);Mn(u′,v′)=1,∀n∈{1,...,Ncam}} : ||W( ˆR, ˆ
K)(u, v)||2 < 1,
(12)"
MULTI-VIEW PREPROCESSING,0.29891304347826086,"where ˆR can be obtained from the sphere center p and the camera position t, and ˆ
K is computed as
fov = 2 arcsin(||t −p||2/r) from which the warping W is deﬁned as in A.1 or Equation 8. After
solving the optimization above, we ﬁx the camera orientations ˆR, change the intrinsics as if the
bounding sphere has a radius of 15 cm and ﬁnally warp the raw images accordingly. Fig. 12 shows
the raw images from an example environment and the images warped by the multi-view processing."
EXPERIMENTS,0.3016304347826087,"6
EXPERIMENTS"
PERFORMANCE OF LEARNED FEATURES,0.30434782608695654,"6.1
PERFORMANCE OF LEARNED FEATURES"
PERFORMANCE OF LEARNED FEATURES,0.3070652173913043,"Baselines: The key techniques of the proposed framework are threefold: the pixel-aligned local im-
age features, the implicit function over the 3D space as representations and the task guided learning
scheme. To examine the beneﬁts from each component, three baselines are considered. (i) Global
image features: The ﬁrst baseline still represent an object as an implicit function but the image
encoder outputs a global image feature as shown in Fig. 13(b) rather than having the pixel-aligned
local feature extraction; we used the ResNet-34 architecture as the image encoder and ﬁxed the other
model speciﬁcations. (ii) Vector object representations: The second baseline represents an object as
a ﬁnite-dimensional vector instead of an implicit function; as shown in Fig. 13(c), the representation
network ﬁrst computes the image features from the images using ResNet-34 and the camera features
from the camera parameters using a couple of fully connected layers. Two features are then passed
to another couple of fully connected layers to produce the object representation vector. The feature
heads take as input the frame’s pose as well as the object representation vector. (iii) SDF represen-
tations: The last baseline uses SDFs as object representations; the network architecture for the SDF
feature remains the same, but the grasping and hanging heads take as inputs a set of the keypoints’
SDF values instead of the d-dimensional representation vectors. The SDF values are detached when
passed to the grasp/hang heads so the backbone is only trained by the geometry (SDF) data."
PERFORMANCE OF LEARNED FEATURES,0.30978260869565216,"Evaluation Metric:
Regarding the shape reconstruction, we report the Volumetric IoU and the
Chamfer distance. To measure these metrics, we ﬁrst randomly sampled 4 images from the dataset
and reconstructed the meshes from the learned SDF feature using the marching cube algorithm (See
Fig. 10). The volumetric IoU is the ratio between the intersection and the union of the reconstructed
and ground-truth meshes which is (approximately) computed on the 1003 grid points around the
objects. To compute the Chamfer distance, we sampled 10,000 surface points from each mesh and"
PERFORMANCE OF LEARNED FEATURES,0.3125,"4Note that δq denotes rigid transformations applied to object’s implicit representation, not absolute poses."
PERFORMANCE OF LEARNED FEATURES,0.31521739130434784,Under review as a conference paper at ICLR 2022
PERFORMANCE OF LEARNED FEATURES,0.3179347826086957,"Figure 4: SDFs predicted by PIFO and the global
image feature model."
PERFORMANCE OF LEARNED FEATURES,0.32065217391304346,"Figure 5:
Sequential manipulation scenarios:
Single-, three-mug hanging and handover."
PERFORMANCE OF LEARNED FEATURES,0.3233695652173913,"IoU
Chamfer-L1 (×10−3)
Grasp+c (%)
Hang+c (%)
PIFO
0.816 / 0.656
5.26 / 6.90
88.1 / 82.5
94.0 / 78.9
Global Img. Feat.
0.697 / 0.581
7.42 / 9.49
82.7 / 75.7
91.2 / 78.2
Vector Obj. Repr.
0.036 / 0.014
38.6 / 39.7
0.5 / 0.4
0.0 / 0.0
SDF Obj. Repr.
0.845 / 0.667
4.90 / 6.83
67.9 / 64.3
3.7 / 4.3
PIFO (2 views)
0.760 / 0.577
6.14 / 8.84
82.9 / 77.1
88.2 / 72.1
PIFO (8 views)
0.851 / 0.683
4.78 / 6.34
88.7 / 85.0
96.5 / 82.5"
PERFORMANCE OF LEARNED FEATURES,0.32608695652173914,Table 1: Individual Feature Evaluation with 4 views (Training / Test).
PERFORMANCE OF LEARNED FEATURES,0.328804347826087,"averaged the forward and backward closest pair distances. To evaluate the learned task features,
we solved the unconstrained optimization ˆq∗= arg minq ||φtask(q)||2, task ∈{grasp, hang} using
the Gauss-Newton method. Starting from this solution, we then solved the second optimization
problem by including the collision feature (details in Appendix A.3), q∗= arg minq ||φtask(q)||2 +
wcoll||φcoll(q)||2. Because the local optimization method can be stuck at local optima, we ran the
algorithm from 10 random initial guesses in parallel and picked the best one. The optimized pose is
then tested in simulation and the success rates are reported in Table 1."
PERFORMANCE OF LEARNED FEATURES,0.33152173913043476,"Result: Table 1 shows that the SDF representation has the best shape reconstruction performance;
PIFO is slightly worse, followed by the other two frameworks. On the other hand, the task perfor-
mances of PIFO are signiﬁcantly better than the others. The SDF representation is especially worse
in the hanging task, which implies that SDFs along the line are not sufﬁcient for the feature predic-
tion and our task-guided representation simpliﬁes the feature prediction. Fig. 4 depicts SDF values
of an unseen mug with a complex shape handle predicted by PIFO and the global image feature
model; one can observe that the global image feature model reconstructed the handle shape as being
more “typical” and the pixel-aligned representation was better able to capture ﬁne-grained details.
PIFO was also tested with the different numbers of input images and it can be seen from the last
two rows of the table that the more images we put in, the better performance the network shows.
Tables 2 and 3 report all combinations of the metrics and the number of views."
SEQUENTIAL MANIPULATION PLANNING VIA LGP,0.3342391304347826,"6.2
SEQUENTIAL MANIPULATION PLANNING VIA LGP"
SEQUENTIAL MANIPULATION PLANNING VIA LGP,0.33695652173913043,"As shown in Fig. 5, we considered three manipulation scenarios: Basic pick & hang, long-horizon
three-mug hanging, and handover. In the ﬁrst scenario, the environment contains one robot arm, one
hook, one mug and 4 cameras (as in Fig. 12(a)), and the interaction modes are constrained by the
discrete action sequence of [(GRASP, gripper, mug), (HANG, hook, mug)]. 10 mugs were picked
from each of the training and test data sets and their initial poses are randomized.5 When executed
the optimized trajectory in the Bullet simulation, the success rates on the train and test mugs were
50 % and 40 %, respectively. If we allow the method to re-plan and execute when it failed, the
success rates increased to 90% and 70%, respectively [playlist1]. The three-mug scenario consists
of 6 discrete phases with [(GRASP, gripper, mug1), (HANG, M hook, mug1), (GRASP, gripper,
mug2), (HANG, U hook, mug2), (GRASP, gripper, mug3), (HANG, L hook, mug3)]. The handover
scenario has two arms at different heights and the target hook is placed very high, requiring two
arms to coordinate a handover motion; the corresponding discrete actions are [(GRASP, R gripper,
mug), (GRASP, L gripper, mug), (HANG, U hook, mug)]. Fig. 5 shows the last conﬁgurations of
the optimized plans; we refer the readers to Figs. 16–17 and the videos [playlist2] for clearer views."
SEQUENTIAL MANIPULATION PLANNING VIA LGP,0.33967391304347827,"Inverse Kinematics with Generative Models: One important attribute of our framework is that,
while most existing works train generative models that directly produce the interaction poses, ours
models interactions as equality constraints which can jointly be optimized with other planning fea-
tures. To see the beneﬁts of such joint optimization, we considered the following inverse kinematics
problem with a generative model: For the basic pick & hang and handover scenarios, we optimized"
SEQUENTIAL MANIPULATION PLANNING VIA LGP,0.3423913043478261,"5Before solving the full trajectory optimization, we ﬁrst optimized each feature as in Sec. 6.1 and added
small regularization terms using the optimized poses to guide the optimizer away from local optima."
SEQUENTIAL MANIPULATION PLANNING VIA LGP,0.3451086956521739,Under review as a conference paper at ICLR 2022
SEQUENTIAL MANIPULATION PLANNING VIA LGP,0.34782608695652173,"ICP
ICP2
FCP
F+ICP2
0.000 0.025 0.050 0.075 0.100 0.125"
SEQUENTIAL MANIPULATION PLANNING VIA LGP,0.35054347826086957,"ICP
ICP2
FCP
F+ICP2
0 1 2 3"
SEQUENTIAL MANIPULATION PLANNING VIA LGP,0.3532608695652174,"Figure 6: Position and Orientation Errors
Figure 7: (a) Reference (b-c) Imitation"
SEQUENTIAL MANIPULATION PLANNING VIA LGP,0.35597826086956524,"each interaction pose separately as in Sec. 6.1 and checked if these individually optimized poses are
kinematically feasible, i.e. whether or not the inverse kinematics problems have a solution. Even
though the mug’s initial pose was given such that the ﬁrst gasping is feasible, 53 out of 100 pairs
of grasp and hang poses were infeasible for the pick & hang scenario and 86 out of 100 sets for the
handover scenario, i.e., many of the sampled poses led to a collision or an infeasible robot conﬁgu-
ration for hanging/handover. Some failure cases are depicted in Figs. 18–19. This will become even
worse when the whole trajectory is optimized or the mug’s initial pose is given arbitrarily. As the
sequence length gets longer, not only should an exponentially larger number of planning problems
be solved to ﬁnd a set of feasible poses, but also the found poses are not guaranteed to be optimal.
Again, the joint optimization with our constraint models doesn’t raise such issues."
SEQUENTIAL MANIPULATION PLANNING VIA LGP,0.358695652173913,"6.3
6D POSE ESTIMATION AND ZERO-SHOT IMITATION"
SEQUENTIAL MANIPULATION PLANNING VIA LGP,0.36141304347826086,"Figs. 14 and 15 visualize some principal components of the image features and of the representation
vectors, respectively. It can be seen that each component represents a certain property of the objects,
such as inside vs. outside, handle vs. other parts, or above vs. below. This enables the image-
based pose estimation which we call feature-based closest point (FCP) matching, i.e., the problem
of ﬁnding the relative pose of a target mesh w.r.t. a model mesh whose pose is given, without
deﬁning any canonical coordinate of the objects. To this end, we ﬁrst queried the backbone at 103
and 53 grid points around the target and the model, respectively, (as shown in Fig. 20(d)) with
their own images. For each model grid point, the corresponding target point is obtained such that
their representations are closest. Finally, FCP computes the rigid transformation that minimizes the
sum of their Euclidean distances. We compared this to the conventional iterative closest point (ICP)
algorithm on point clouds, i.e., the problem of ﬁnding the pose minimizing the Euclidean distance
of two sets of point clouds obtained from depth cameras. Instead of using depth images, the point
clouds can be obtained from the reconstructed meshed via the learned SDF features and we call it
ICP2. The point clouds’ size was 1000. Fig. 6 shows the position and orientation errors of the 6D
pose estimation. FCP performs much better especially in the orientation because, as already widely
known, ICP easily gets stuck at the local optima; ICP2 was similarly as worse as ICP. Speciﬁcally, a
signiﬁcant improvement was observed in F+ICP2 where we used the FCP results as starting points
of ICP2 (which is performed without depth images). Fig. 21 depicts some of the results."
SEQUENTIAL MANIPULATION PLANNING VIA LGP,0.3641304347826087,"Another observation from the PCAs is that the semantics of the representation are consistent across
different objects, e.g. the handle parts of different mugs have similar representations, which im-
plies a pose of one object can be transferred into another through it. We therefore considered
an image-based zero-shot imitation scenario, where the environment contains one robot arm, one
target mug (ﬁlled with small balls) and 4 cameras as shown in Fig. 7. We manually designed
the pouring motion and captured the camera images of pre- and post-pouring postures of the mug,
Ppre = (Ipre, Tpre, Kpre) and Ppost = (Ipost, Tpost, Kpost), respectively. For a new mug, we solved the
sequential manipulation planning with [(GRASP, gripper, mug), (POSEFCP, Ppre, mug), (POSEFCP,
Ppost, mug)], where (POSEFCP, ·, ·) imposes the aforementioned FCP constraint. That is, the tra-
jectory optimizer tries to match each part of the new object to the corresponding part of the target
while coordinating the global consistency of the full trajectory, thereby resulting in the imitation of
the reference motions only from the posed images. Fig. 7 shows the optimized post-pouring posture;
Figs. 22 – 23 and the videos [playlist3] show the reference and its imitations more clearly."
CONCLUSION,0.36684782608695654,"7
CONCLUSION"
CONCLUSION,0.3695652173913043,"This work transferred the idea of neural implicit representations to robotic manipulation applica-
tions. The pixel-aligned nature enables the proposed representations to be inferred from posed cam-
era images and easily capture ﬁne-grained details of the objects. We demonstrated that the learned
representations and interaction features allowed for formulating and solving general sequential ma-
nipulation problems only from images within the LGP framework. The learned representations
generalize to unseen objects, enabling manipulation planning and zero-shot imitations for them."
CONCLUSION,0.37228260869565216,Under review as a conference paper at ICLR 2022
REFERENCES,0.375,REFERENCES
REFERENCES,0.37771739130434784,"Matan Atzmon and Yaron Lipman. SAL: Sign agnostic learning of shapes from raw data. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2565–
2574, 2020."
REFERENCES,0.3804347826086957,"Michel Breyer, Jen Jen Chung, Lionel Ott, Siegwart Roland, and Nieto Juan. Volumetric grasping
network: Real-time 6 dof grasp detection in clutter. In Conference on Robot Learning, 2020."
REFERENCES,0.38315217391304346,"Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,
Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu.
ShapeNet: An Information-Rich 3D Model Repository.
Technical Report arXiv:1512.03012
[cs.GR], Stanford University — Princeton University — Toyota Technological Institute at
Chicago, 2015."
REFERENCES,0.3858695652173913,"Xu Chen, Zijian Dong, Jie Song, Andreas Geiger, and Otmar Hilliges. Category level object pose
estimation via neural analysis-by-synthesis. In European Conference on Computer Vision, pp.
139–156. Springer, 2020."
REFERENCES,0.38858695652173914,"Erwin Coumans and Yunfei Bai.
Pybullet, a python module for physics simulation for games,
robotics and machine learning. http://pybullet.org, 2016–2021."
REFERENCES,0.391304347826087,"Danny Driess, Jung-Su Ha, Marc Toussaint, and Russ Tedrake. Learning models as functionals of
signed-distance ﬁelds for manipulation planning. In 5th Annual Conference on Robot Learning,
2021. URL https://openreview.net/forum?id=FS30JeiGG3h."
REFERENCES,0.39402173913043476,"Clemens Eppner, Arsalan Mousavian, and Dieter Fox. ACRONYM: A large-scale grasp dataset
based on simulation. In 2021 IEEE Int. Conf. on Robotics and Automation, ICRA, 2021."
REFERENCES,0.3967391304347826,"Peter Florence, Lucas Manuelli, and Russ Tedrake. Dense object nets: Learning dense visual object
descriptors by and for robotic manipulation. Conference on Robot Learning, 2018."
REFERENCES,0.39945652173913043,"Peter Florence, Lucas Manuelli, and Russ Tedrake. Self-supervised correspondence in visuomotor
policy learning. IEEE Robotics and Automation Letters, 5(2):492–499, 2019."
REFERENCES,0.40217391304347827,"Wei Gao and Russ Tedrake. kPAM 2.0: Feedback control for category-level robotic manipulation.
IEEE Robotics and Automation Letters, 6(2):2962–2969, 2021."
REFERENCES,0.4048913043478261,"Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regular-
ization for learning shapes. In International Conference on Machine Learning, pp. 3789–3799.
PMLR, 2020."
REFERENCES,0.4076086956521739,"Jung-Su Ha, Danny Driess, and Marc Toussaint. A probabilistic framework for constrained ma-
nipulations and task and motion planning under uncertainty. In Proc. of the IEEE Int. Conf. on
Robotics and Automation (ICRA), 2020. doi: 10.1109/LRA.2020.3010462."
REFERENCES,0.41032608695652173,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition, 2016."
REFERENCES,0.41304347826086957,"Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Roman Shapovalov, Tobias Ritschel, Andrea
Vedaldi, and David Novotny. Unsupervised learning of 3d object categories from videos in the
wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 4700–4709, 2021."
REFERENCES,0.4157608695652174,"Zhenyu Jiang, Yifeng Zhu, Maxwell Svetlik, Kuan Fang, and Yuke Zhu. Synergies between affor-
dance and geometry: 6-dof grasp detection via implicit representations. In Robotics: Science and
Systems (RSS), 2021."
REFERENCES,0.41847826086956524,"Marian Kleineberg. mesh-to-sdf: Calculate signed distance ﬁelds for arbitrary meshes. https:
//github.com/marian42/mesh_to_sdf, 2021."
REFERENCES,0.421195652173913,"Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Apari-
cio Ojea, and Ken Goldberg. Dex-net 2.0: Deep learning to plan robust grasps with synthetic
point clouds and analytic grasp metrics. In Robotics: Science and Systems (RSS), 2017."
REFERENCES,0.42391304347826086,Under review as a conference paper at ICLR 2022
REFERENCES,0.4266304347826087,"Lucas Manuelli, Wei Gao, Peter Florence, and Russ Tedrake. kPAM: Keypoint affordances for
category-level robotic manipulation. In International Symposium on Robotics Research (ISRR),
2019."
REFERENCES,0.42934782608695654,"Lucas Manuelli, Yunzhu Li, Pete Florence, and Russ Tedrake. Keypoints into the future: Self-
supervised correspondence in model-based reinforcement learning. Conference on Robot Learn-
ing, 2020."
REFERENCES,0.4320652173913043,"Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Oc-
cupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 4460–4470, 2019."
REFERENCES,0.43478260869565216,"Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and
Ren Ng. NeRF: Representing scenes as neural radiance ﬁelds for view synthesis. In European
conference on computer vision, pp. 405–421. Springer, 2020."
REFERENCES,0.4375,"Arsalan Mousavian, Clemens Eppner, and Dieter Fox. 6-dof GraspNet: Variational grasp generation
for object manipulation. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 2901–2910, 2019."
REFERENCES,0.44021739130434784,"Adithyavairavan Murali, Arsalan Mousavian, Clemens Eppner, Chris Paxton, and Dieter Fox. 6-dof
grasping for target-driven object manipulation in clutter. In 2020 IEEE International Conference
on Robotics and Automation (ICRA), pp. 6232–6238. IEEE, 2020."
REFERENCES,0.4429347826086957,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumet-
ric rendering: Learning implicit 3d representations without 3d supervision. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3504–3515, 2020."
REFERENCES,0.44565217391304346,"Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.
DeepSDF: Learning continuous signed distance functions for shape representation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 165–174,
2019."
REFERENCES,0.4483695652173913,"Keunhong Park, Arsalan Mousavian, Yu Xiang, and Dieter Fox. LatentFusion: End-to-end differ-
entiable reconstruction and rendering for unseen object pose estimation. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pp. 10710–10719, 2020."
REFERENCES,0.45108695652173914,"Zengyi Qin, Kuan Fang, Yuke Zhu, Li Fei-Fei, and Silvio Savarese. KETO: Learning keypoint
representations for tool manipulation. In 2020 IEEE International Conference on Robotics and
Automation (ICRA), pp. 7278–7285. IEEE, 2020."
REFERENCES,0.453804347826087,"Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and
David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d cat-
egory reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 10901–10911, 2021."
REFERENCES,0.45652173913043476,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234–241. Springer, 2015."
REFERENCES,0.4592391304347826,"Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao
Li.
PIFu: Pixel-aligned implicit function for high-resolution clothed human digitization.
In
International Conference on Computer Vision (ICCV), pp. 2304–2314, 2019."
REFERENCES,0.46195652173913043,"Vincent Sitzmann, Michael Zollhoefer, and Gordon Wetzstein. Scene representation networks: Con-
tinuous 3d-structure-aware neural scene representations. Advances in Neural Information Pro-
cessing Systems, 32:1121–1132, 2019."
REFERENCES,0.46467391304347827,"Lars Mescheder Marc Pollefeys Andreas Geiger Songyou Peng, Michael Niemeyer. Convolutional
occupancy networks. In European Conference on Computer Vision (ECCV), 2020."
REFERENCES,0.4673913043478261,"Martin Sundermeyer, Arsalan Mousavian, Rudolph Triebel, and Dieter Fox. Contact-GraspNet: Ef-
ﬁcient 6-dof grasp generation in cluttered scenes. In IEEE International Conference on Robotics
and Automation (ICRA), 2021."
REFERENCES,0.4701086956521739,Under review as a conference paper at ICLR 2022
REFERENCES,0.47282608695652173,"Andreas ten Pas, Marcus Gualtieri, Kate Saenko, and Robert Platt. Grasp pose detection in point
clouds. The International Journal of Robotics Research, 36(13-14):1455–1473, 2017."
REFERENCES,0.47554347826086957,"Marc Toussaint. A tutorial on Newton methods for constrained trajectory optimization and relations
to SLAM, Gaussian Process smoothing, optimal control, and probabilistic inference. In Jean-Paul
Laumond (ed.), Geometric and Numerical Foundations of Movements. Springer, 2017."
REFERENCES,0.4782608695652174,"Marc Toussaint, Kelsey Allen, Kevin A Smith, and Joshua B Tenenbaum. Differentiable physics and
stable modes for tool-use and manipulation planning. In Robotics: Science and Systems, 2018."
REFERENCES,0.48097826086956524,"Marc Toussaint, Jung-Su Ha, and Danny Driess. Describing physics for physical reasoning: Force-
based sequential manipulation planning.
IEEE Robotics and Automation Letters, 2020.
doi:
10.1109/LRA.2020.3010462."
REFERENCES,0.483695652173913,"Alex Trevithick and Bo Yang. GRF: Learning a general radiance ﬁeld for 3d scene representation
and rendering. In International Conference on Computer Vision (ICCV), 2021."
REFERENCES,0.48641304347826086,"Dylan Turpin, Liquan Wang, Stavros Tsogkas, Sven Dickinson, and Animesh Garg. GIFT: Gener-
alizable interaction-aware functional tool affordances without labels. In Robotics: Science and
Systems, 2021."
REFERENCES,0.4891304347826087,"Mark Van der Merwe, Qingkai Lu, Balakumar Sundaralingam, Martin Matak, and Tucker Hermans.
Learning continuous 3d reconstructions for geometrically aware grasping. In 2020 IEEE Interna-
tional Conference on Robotics and Automation (ICRA), pp. 11516–11522. IEEE, 2020."
REFERENCES,0.49184782608695654,"Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich Neumann. DISN: Deep
implicit surface network for high-quality single-view 3d reconstruction. Advances in Neural In-
formation Processing Systems, 32:492–502, 2019."
REFERENCES,0.4945652173913043,"Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lip-
man. Multiview neural surface reconstruction by disentangling geometry and appearance. Ad-
vances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.49728260869565216,"Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi
Lin. iNeRF: Inverting neural radiance ﬁelds for pose estimation. In IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), 2021."
REFERENCES,0.5,"Yifan You, Lin Shao, Toki Migimatsu, and Jeannette Bohg. OmniHang: Learning to hang arbitrary
objects using contact point correspondences and neural collision estimation. In IEEE Interna-
tional Conference on Robotics and Automation (ICRA). IEEE, 2021."
REFERENCES,0.5027173913043478,"Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance ﬁelds
from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2021."
REFERENCES,0.5054347826086957,"Wentao Yuan, Chris Paxton, Karthik Desingh, and Dieter Fox. SORNet: Spatial object-centric
representations for sequential manipulation. In 5th Annual Conference on Robot Learning, 2021.
URL https://openreview.net/forum?id=mOLu2rODIJF."
REFERENCES,0.5081521739130435,"Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian,
Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, and Johnny Lee.
Transporter
networks: Rearranging the visual world for robotic manipulation. Conference on Robot Learning
(CoRL), 2020a."
REFERENCES,0.5108695652173914,"Andy Zeng, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. Tossingbot:
Learning to throw arbitrary objects with residual physics. IEEE Transactions on Robotics, 36(4):
1307–1319, 2020b."
REFERENCES,0.5135869565217391,Under review as a conference paper at ICLR 2022
REFERENCES,0.5163043478260869,"A
APPENDIX"
REFERENCES,0.5190217391304348,"A.1
HOMOGRAPHY TRANSFORMATION"
REFERENCES,0.5217391304347826,"The idea of the Homography warping is that two images taken by cameras at the same position but
with different orientations and intrinsics can be transformed into each other. Suppose that we have
a source image I with the camera position t, rotation matrix R and projection matrix K and that
an object is inside a bounding sphere at p ∈R3 with a radius r ∈R+. An image focusing on
the bounding sphere can be taken from a (synthetic) camera at the same position t with the view
direction as t −p and the ﬁeld of view angle as 2 arcsin(||t −p||2/r), from which we can compute
the new camera rotation matrix ˆR and the intrinsic ˆ
K."
REFERENCES,0.5244565217391305,"Given ˆR and ˆ
K, the new ﬁeld warped by the corresponding Homography can be obtained as follows:
First, a pixel in the source image, p1 = (u, v, 1), is reprojected into a ray in the 3D space: P1 =
K−1p1. Next, the ray is viewed in the new camera coordinate: P2 = ˆRT RP1. Lastly, this ray is
projected back into a pixel in the new camera: p2 = ˆKP2. Putting all together, the Homography
warping is given as:"
REFERENCES,0.5271739130434783,"W( ˆR, ˆ
K) :"
REFERENCES,0.529891304347826,"""u
v
1 #"
REFERENCES,0.532608695652174,"7→w ˆ
K ˆRT RK−1
""u
v
1 #"
REFERENCES,0.5353260869565217,",
(13)"
REFERENCES,0.5380434782608695,"where w is the parameter that makes the last element of the output homogeneous coordinate 1, which"
REFERENCES,0.5407608695652174,"results in the warped image ˆI with its camera pose ˆT =
 ˆR
t
0
1"
REFERENCES,0.5434782608695652,"
and intrinsic matrix ˆ
K."
REFERENCES,0.5461956521739131,"A.2
MANIPULATION CONSTRAINTS"
REFERENCES,0.5489130434782609,"In this work, we consider two discrete actions, (GRASP, gripper, mug) and (HANG, hook, mug), for
grasping and hanging, respectively. Each action imposes three constraints on the path as follows."
REFERENCES,0.5516304347826086,"• The action ak = (GRASP, gripper, mug) ﬁrst imposes the learned grasping constraint at the
end of its phase, Hi
grasp(xt, δqi
t) = 0, t = kT, i.e.,
 
T(δqi
t)[φi
grasp] ◦FKj

(xt) = 0,
(14)
where i and j are indices of the mug and the gripper, respectively. It also imposes the
zero-impact switching constraint at t = kT, i.e.,
ˆvt = 0,
(15)
where ˆvt is a joint velocity computed from xt−1 and xt via ﬁnite difference. Lastly, it
introduces an equality constraint on the gripper’s approaching direction for collision-free
grasping; more precisely, the constraint is imposed at t ∈{kT −2, kT −1, kT} as:"
REFERENCES,0.5543478260869565,"j ˆai
t = aapproach"
REFERENCES,0.5570652173913043,""" 0
0
−1 #"
REFERENCES,0.5597826086956522,",
(16)"
REFERENCES,0.5625,"where j ˆai
t is the mug’s acceleration in the gripper’s coordinate computed from jt(δqi
t−2),
jt(δqi
t−1) and jt(δqi
t) via ﬁnite difference, and aapproach ∈R+ is the predeﬁned approach-
ing acceleration magnitude. The gripper’s z axis is depicted in Fig. 11(a) as a blue arrow.
Combined with the above zero-impact constraint, this constraint enforces the gripper to
approach the mug in the gripper’s -z axis direction and to stop moving at the end of the
phase.
• Similarly, the action ak = (HANG, hook, mug) consists of the learned hanging constraint,
the zero-impact and hanging approaching constraints as
 
T(δqi
kT )[φhang] ◦FKj

(xkT ) = 0,
(17)
ˆvkT = 0,
(18)"
REFERENCES,0.5652173913043478,"j ˆai
t = aapproach"
REFERENCES,0.5679347826086957,"""0
0
1 #"
REFERENCES,0.5706521739130435,", ∀t ∈{kT −2, kT −1, kT},
(19)"
REFERENCES,0.5733695652173914,"where i and j are indices of the mug and the hook, respectively, and the hook’s z axis is the
blue arrow in Fig. 11(b) (or outer product of the red and green arrow)."
REFERENCES,0.5760869565217391,Under review as a conference paper at ICLR 2022
REFERENCES,0.5788043478260869,"The discrete actions above affect the consecutive symbolic states. While sk indicates a mug is
grasped by a gripper or hung on a hook at the phase k, we impose the following path constraint:"
REFERENCES,0.5815217391304348,"δqi
t −δqi
t−1 = FKj(xt) −FKj(xt−1), ∀t ∈{(k −1)T + 1, · · · , kT}
(20)"
REFERENCES,0.5842391304347826,"where i and j are indices of the mug and the gripper/hook, respectively. Effectively this introduces
a static joint between the two frames (Toussaint et al., 2018) so the mug moves along with its parent
frame (the gripper or hook). The collision constraints are also imposed along the trajectory, where
the pair collisions with the mug are computed by the learned SDF feature. We introduce the collision
feature in the following section."
REFERENCES,0.5869565217391305,"We would like to emphasize that our manipulation planning framework is not limited by the con-
straints we introduced above, but it can incorporate any existing other constraint models and meth-
ods, e.g., (Toussaint et al., 2018; Ha et al., 2020; Toussaint et al., 2020; Driess et al., 2021)."
REFERENCES,0.5896739130434783,"A.3
DEFINING PAIR-COLLISION CONSTRAINTS WITH SDFS"
REFERENCES,0.592391304347826,"For manipulation planning problems written only by convex meshes, the distance or penetration of
two objects, which we call pair-collision features, are computed with either Gilbert-Johnson-Keerthi
(GJK) for non-penetrating objects or Minkowski Portal Reﬁnement (MPR) for penetrating objects.
In this section, we introduce how to deﬁne pair-collision features when one or both objects are given
as SDFs."
REFERENCES,0.595108695652174,"SDF vs. Sphere:
Let δqi, qj and rj be the rigid transformation of PIFO, the sphere’s pose and
radius, respectively. Then the pair-collision feature is simply given by:"
REFERENCES,0.5978260869565217,"dij = T(δqi)[φSDF](t(qj)) −rj.
(21)"
REFERENCES,0.6005434782608695,"SDF vs. Capsule: Let δqi, qj, hj and rj be the rigid transformation of PIFO, the capsule’s pose,
height and radius, respectively. The pair-collision feature is given by the solution of the following
optimization:"
REFERENCES,0.6032608695652174,"dij =
min
−hj/2≤z≤hj/2 T(δqi)[φSDF]  R(qj)"
REFERENCES,0.6059782608695652,"""0
0
z #"
REFERENCES,0.6086956521739131,+ t(qj) !
REFERENCES,0.6114130434782609,"−rj.
(22)"
REFERENCES,0.6141304347826086,"SDF vs. Mesh: Let δqi and qj be the rigid transformation of PIFO, the mesh’s pose, respectively."
REFERENCES,0.6168478260869565,"dij =
min
p1∈R3,p2∈R3"
REFERENCES,0.6195652173913043,"T (δqi)[φSDF](p1)=0
dj(p2)=0"
REFERENCES,0.6222826086956522,"nT
1 (p2 −p1),
(23)"
REFERENCES,0.625,"where n1 is the normal vector of φSDF at p1 and dj(p2) is the signed distance of p2 to the mesh
computed by GJK/MPR."
REFERENCES,0.6277173913043478,SDF vs. SDF: Let δqi and δqj be the rigid transformations of two PIFOs.
REFERENCES,0.6304347826086957,"dij =
min
p1∈R3,p2∈R3"
REFERENCES,0.6331521739130435,"T (δqi)[φi
SDF](p1)=0
T (δqj)[φj
SDF](p2)=0"
REFERENCES,0.6358695652173914,"nT
1 (p2 −p1).
(24)"
REFERENCES,0.6385869565217391,"The optimizations in Equation 22–Equation 24 should be run multiple times from different initial
guesses because the object shape represented as SDF can be non-convex. In practice, we found
approximating the meshes by a number of spheres and computing the collision feature much more
efﬁcient because querying the network φSDF at multiple points can be done in parallel on GPUs."
REFERENCES,0.6413043478260869,"A.4
NETWORK PARAMETERS"
REFERENCES,0.6440217391304348,"Image encoder has the U-net architecture (Ronneberger et al., 2015), especially with the headless
ResNet-34 (He et al., 2016) as its downward path and two residual 3 × 3 convolutions followed by
up-convolution as the upward path. The number of output channels is 64."
REFERENCES,0.6467391304347826,"3D reprojector computes the coordinate feature as 32-dimensional vector using one linear+ReLU
layer and concatenate it with the local image feature. They are passed to two hidden layers with the"
REFERENCES,0.6494565217391305,Under review as a conference paper at ICLR 2022
REFERENCES,0.6521739130434783,"width of (256, 128) followed by ReLUs. Therefore, the dimension of the representation vector is
128."
REFERENCES,0.654891304347826,"SDF head takes as input one representation vector and computes the output through one hidden
layer with the width of 128 followed by ReLU."
REFERENCES,0.657608695652174,"Grasp and hang heads take as input 27 and 5 representation vectors at their interaction points
(depicted in Fig. 11) and predict the feature through two hidden layers with the widths of (256, 128)
followed by ReLUs."
REFERENCES,0.6603260869565217,"As shown in Figure 13, the network structures for comparison in Section 6.1 was kept similar to the
above as possible. Image encoders of the global image feature and vector representation networks
are the ResNet-34 returning 64-dimensional vector. The feature head structures remain the same,
but, because the vector representation scheme doesn’t represent objects as implicit functions, the in-
put of their feature head is the frame’s pose as 7-dimensional vector (3D translation+ 4D quaternion).
The grasp and hang heads of the SDF representation scheme take as input 27- and 5-dimensional
vectors of their interaction points SDF values."
REFERENCES,0.6630434782608695,"# of views
Method
SDF error (×10−3)
Volumetric IoU
Chamfer-L1 (×10−3)"
PIFO,0.6657608695652174,"2
PIFO
2.91 / 4.63
0.760 / 0.577
6.14 / 8.84
Global Image Feature
3.58 / 4.96
0.642 / 0.515
8.50 / 10.8
Vector Representation
15.6 / 15.8
0.045 / 0.046
39.1 / 40.4
SDF Representation
2.11 / 3.48
0.786 / 0.622
5.78 / 8.13"
PIFO,0.6684782608695652,"4
PIFO
2.20 / 3.38
0.816 / 0.656
5.26 / 6.90
Global Image Feature
2.82 / 3.93
0.697 / 0.581
7.42 / 9.49
Vector Representation
15.0 / 15.2
0.036 / 0.014
38.6 / 39.7
SDF Representation
1.43 / 2.73
0.845 / 0.667
4.90 / 6.83"
PIFO,0.6711956521739131,"8
PIFO
1.68 / 2.72
0.851 / 0.683
4.78 / 6.34
Global Image Feature
2.31 / 3.51
0.728 / 0.607
6.75 / 8.80
Vector Representation
14.6 / 15.3
0.033 / 0.006
38.7 / 40.6
SDF Representation
1.07 / 2.07
0.878 / 0.703
4.51 / 6.06"
PIFO,0.6739130434782609,"Table 2: SDF Feature Evaluation (Training / Test). The SDF errors were also measured at the same
grid points as IoU."
PIFO,0.6766304347826086,Under review as a conference paper at ICLR 2022
PIFO,0.6793478260869565,"# of views
Method
Grasp (%)
Grasp+c (%)
Hang (%)
Hang+c (%)"
PIFO,0.6820652173913043,"2
PIFO
65.8 / 55.4
82.9 / 77.1
87.2 / 71.4
88.2 / 72.1
Global Image Feature
67.6 / 63.9
80.9 / 70.4
88.3 / 70.4
86.3 / 71.8
Vector Representation
13.2 / 12.9
0.8 / 0.4
25.6 / 21.8
0.0 / 0.0
SDF Representation
41.2 / 55.3
49.6/ 45.7
2.6 / 1.1
3.3 / 2.1"
PIFO,0.6847826086956522,"4
PIFO
69.0 / 63.9
88.1 / 82.5
88.7 / 75.4
94.0 / 78.9
Global Image Feature
62.3 / 61.8
82.7 / 75.7
90.3 / 75.7
91.2 / 78.2
Vector Representation
21.2 / 22.5
0.5 / 0.4
55.1 / 46.4
0.0 / 0.0
SDF Representation
49.1 / 46.1
67.9 / 64.3
3.3 / 2.9
3.7 / 4.3"
PIFO,0.6875,"8
PIFO
71.9 / 69.3
88.7 / 85.0
91.7 / 80.4
96.5 / 82.5
Global Image Feature
71.3 / 67.1
84.0 / 79.3
91.3 / 77.5
92.9 / 80.4
Vector Representation
29.0 / 23.9
0.5 / 0.7
65.9 / 49.6
0.0 / 0.0
SDF Representation
51.4 / 52.1
75.5 / 70.4
4.6 / 6.1
6.3/ 5.7"
PIFO,0.6902173913043478,Table 3: Task Feature Evaluation (Training / Test).
PIFO,0.6929347826086957,"(a) SDF
(b) Grasp
(c) Hang"
PIFO,0.6956521739130435,"(d) Camera
(e) Image"
PIFO,0.6983695652173914,Figure 8: Data Generation
PIFO,0.7010869565217391,Under review as a conference paper at ICLR 2022
PIFO,0.7038043478260869,(a) Before augmentation
PIFO,0.7065217391304348,(b) After augmentation
PIFO,0.7092391304347826,Figure 9: Image Data Augmentation
PIFO,0.7119565217391305,"(a) Train Mugs
(b) Test Mugs"
PIFO,0.7146739130434783,"Figure 10: Reconstruction via marching cube. Red: ground truth, Blue: reconstructed"
PIFO,0.717391304347826,Under review as a conference paper at ICLR 2022
PIFO,0.720108695652174,"Figure 11: Key interaction points on the gripper and hook. Before passed to PIFO, their global
positions are computed from the gripper’s or hook’s pose q, i.e., pi = R(q)ˆpi + t(q), ∀i ∈
{1, ..., Nkeypoint}."
PIFO,0.7228260869565217,Under review as a conference paper at ICLR 2022
PIFO,0.7255434782608695,"(a) Scene. Four cameras’ poses are depicted as coordinate systems where the
origin is the camera location, −z axis (blue) is pointing the view direction, and
x and −y axes (red and blue) are the directions of (u, v) coordinate of images."
PIFO,0.7282608695652174,(b) Raw images and masks
PIFO,0.7309782608695652,(c) Warped images (via the multi-view processing)
PIFO,0.7336956521739131,Figure 12: Multi-view processing.
PIFO,0.7364130434782609,"Under review as a conference paper at ICLR 2022 64 64 64
64 32"
PIFO,0.7391304347826086,"128
128 16"
PIFO,0.7418478260869565,"256
256 8"
PIFO,0.7445652173913043,"512
512 4"
PIFO,0.7472826086956522,Bottleneck Conv
PIFO,0.75,"256
256
256 8"
PIFO,0.7527173913043478,"128
128
128 16"
PIFO,0.7554347826086957,"64
64
64 32"
PIFO,0.7581521739130435,64 64 64 64 1 64
PIFO,0.7608695652173914,"local
img
feat 1 3 p 1 3 z
1 32"
PIFO,0.7635869565217391,"coord
feat × 1 256"
PIFO,0.7663043478260869,local2 1 128
PIFO,0.7690217391304348,local3
PIFO,0.7717391304347826,projection
PIFO,0.7744565217391305,"(a) PIFO 64 64 64
64 32"
PIFO,0.7771739130434783,"128
128 16"
PIFO,0.779891304347826,"256
256 8"
PIFO,0.782608695652174,"512
512 4 1 64"
PIFO,0.7853260869565217,"global
img
feat 1 3 p 1 3 z
1 32"
PIFO,0.7880434782608695,"coord
feat × 1 256"
PIFO,0.7907608695652174,local2 1 128
PIFO,0.7934782608695652,local3
PIFO,0.7961956521739131,projection
PIFO,0.7989130434782609,"(b) Global Image Feature 64 64 64
64 32"
PIFO,0.8016304347826086,"128
128 16"
PIFO,0.8043478260869565,"256
256 8"
PIFO,0.8070652173913043,"512
512 4 1 64"
PIFO,0.8097826086956522,"img
feat 1 6"
PIFO,0.8125,"cam
params
1 32"
PIFO,0.8152173913043478,"cam
feat × 1 256 1 128"
PIFO,0.8179347826086957,(c) Vector Object Represnetation
PIFO,0.8206521739130435,Figure 13: Baseline Networks used for comparison.
PIFO,0.8233695652173914,Under review as a conference paper at ICLR 2022
PIFO,0.8260869565217391,"(a)
(b)
(c)"
PIFO,0.8288043478260869,"Figure 14: First 5 principal components from PCA on image features. The ﬁrst component indicates
the object vs. non-object areas, the second component distinguishes the handle parts, and the third
one spots the above vs. below of the mugs, etc. Note that the network is trained only via the task
feature supervisions."
PIFO,0.8315217391304348,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8342391304347826,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8369565217391305,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8396739130434783,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.842391304347826,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.845108695652174,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8478260869565217,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8505434782608695,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8532608695652174,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8559782608695652,(a) Train mugs
PIFO,0.8586956521739131,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8614130434782609,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8641304347826086,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8668478260869565,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8695652173913043,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8722826086956522,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.875,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8777173913043478,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8804347826086957,"0.10.0 0.1
0.1 0.0 0.1 0.1 0.0 0.1"
PIFO,0.8831521739130435,(b) Test mugs
PIFO,0.8858695652173914,"Figure 15: The ﬁrst principal component from PCA on representation vectors of the 3D surface
points. It distinguishes the handles of the mugs from the other parts and is consistent across different
mugs."
PIFO,0.8885869565217391,Under review as a conference paper at ICLR 2022
PIFO,0.8913043478260869,"(a) t=5
(b) t=8.5
(c) t=10"
PIFO,0.8940217391304348,"(d) t=15
(e) t=18.5
(f) t=20"
PIFO,0.8967391304347826,"(g) t=25
(h) t=28.5
(i) t=30"
PIFO,0.8994565217391305,"Figure 16: The three-mug scenario. 60 steps of robot conﬁgurations and rigid transformations of
three mugs are jointly optimized via the proposed manipulation framework. This optimization is a
1071-dimensional decision problem (one 7DOF arm for 60 steps and one 7DOF mug for 51, 31, 11
steps = 1071, the mug’s rigid transformations before grasped are not included in optimization) and
is solved within 1 minute on a standard laptop."
PIFO,0.9021739130434783,Under review as a conference paper at ICLR 2022
PIFO,0.904891304347826,"(a) t=0
(b) t=5
(c) t=10"
PIFO,0.907608695652174,"(d) t=13.5
(e) t=15"
PIFO,0.9103260869565217,"Figure 17: The handover scenario. 30 steps of the two arms’ conﬁgurations and rigid transformations
of the mug are jointly optimize dvia the proposed manipulation framework. This optimization is a
567-dimensional decision problem (two 7DOF arms for 30 steps and one 7DOF mug for 21 steps =
567, the mug’s rigid transformations at the ﬁrst phase are not included in optimization) and is solved
within 1 minute on a standard laptop."
PIFO,0.9130434782608695,Under review as a conference paper at ICLR 2022
PIFO,0.9157608695652174,"(a) Sampled grasp pose
(b) Sampled hang pose
(c) IK result (inﬁseable)"
PIFO,0.9184782608695652,"(d) Sampled grasp pose
(e) Sampled hang pose
(f) IK result (inﬁseable)"
PIFO,0.9211956521739131,"(g) Sampled grasp pose
(h) Sampled hang pose
(i) IK result (inﬁseable)"
PIFO,0.9239130434782609,"(j) Sampled grasp pose
(k) Sampled hang pose
(l) IK result (inﬁseable)"
PIFO,0.9266304347826086,"Figure 18: IK with generative models - Pick & Hang. Separately generated poses often can not
be coordinated due to the kinematic infeasibility, i.e., the robot joint angle limits, or the collision
constraints."
PIFO,0.9293478260869565,Under review as a conference paper at ICLR 2022
PIFO,0.9320652173913043,"(a) Sampled grasp1 pose
(b) Sampled grasp2 pose
(c) IK result (inﬁseable)"
PIFO,0.9347826086956522,"(d) Sampled grasp1 pose
(e) Sampled grasp2 pose
(f) IK result (inﬁseable)"
PIFO,0.9375,"(g) Sampled grasp1 pose
(h) Sampled grasp2 pose
(i) IK result (inﬁseable)"
PIFO,0.9402173913043478,"(j) Sampled grasp1 pose
(k) Sampled grasp2 pose
(l) IK result (inﬁseable)"
PIFO,0.9429347826086957,"Figure 19: IK with generative models - Handover. Separately generated poses often can not be
coordinated due to the kinematic infeasibility, i.e., the robot joint angle limits, or the collision con-
straints."
PIFO,0.9456521739130435,Under review as a conference paper at ICLR 2022
PIFO,0.9483695652173914,"(a) Model (right) and target (left) mugs
(b) Point clouds for ICP"
PIFO,0.9510869565217391,"(c) Point clouds for ICP2 obtained from meshes
reconstructed via φSDF"
PIFO,0.9538043478260869,(d) Grid points for FCP
PIFO,0.9565217391304348,"Figure 20: 6D Pose Estimation. (b) Point clouds for ICP are obtained from depth cameras at the
same locations/orientations as the RGB cameras. The size of the point clouds is 1000. (c) Point
clouds for ICP are sampled from the surfaces of the meshes reconstructed via the learned φSDF. The
size of the point clouds is 1000. (d) FCP uses 103 grid points for the target and 53 grid points (in
smaller area) for the model, respectively."
PIFO,0.9592391304347826,Under review as a conference paper at ICLR 2022
PIFO,0.9619565217391305,"(a) ICP
(b) ICP2
(c) FCP
(d) F+ICP2"
PIFO,0.9646739130434783,"(e) ICP
(f) ICP2
(g) FCP
(h) F+ICP2"
PIFO,0.967391304347826,"(i) ICP
(j) ICP2
(k) FCP
(l) F+ICP2"
PIFO,0.970108695652174,"(m) ICP
(n) ICP2
(o) FCP
(p) F+ICP2"
PIFO,0.9728260869565217,"Figure 21: 6D Pose Estimation Results - the estimated poses are applied to the green meshes. ICP
easily gets stuck at local optima while FCP produces fairly accurate poses which help F+ICP2 escape
the local optima; note that FCP does not iterate to get the results."
PIFO,0.9755434782608695,Under review as a conference paper at ICLR 2022
PIFO,0.9782608695652174,"(a) t=5
(b) t=10
(c) t=15"
PIFO,0.9809782608695652,"Figure 22: Zero-shot Imitation - reference motion. Two sets of posed images are obtained at t =
10, 15."
PIFO,0.9836956521739131,Under review as a conference paper at ICLR 2022
PIFO,0.9864130434782609,"(a) t=5
(b) t=10
(c) t=15"
PIFO,0.9891304347826086,"(d) t=5
(e) t=10
(f) t=15"
PIFO,0.9918478260869565,"(g) t=5
(h) t=10
(i) t=15"
PIFO,0.9945652173913043,"(j) t=5
(k) t=10
(l) t=15"
PIFO,0.9972826086956522,"Figure 23: Zero-shot imitation - optimized motions. The FCP constraints are imposed at t = 10, 15.
The imitations are achieved only from images, without deﬁning the canonical coordinate/pose of the
objects."
