Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0011933174224343676,"Transformers are becoming mainstream solutions for various tasks like NLP and
Computer vision. Despite their success, the quadratic complexity of their attention
mechanism hinders them from applying to latency-sensitive tasks. Tremendous
efforts have been made to alleviate this problem, and many of them successfully
reduce the asymptotic complexity to linear. Nevertheless, few of them achieve
practical speedup over the original full attention, especially under the moderate
sequence length. In this paper, we present DFSSATTEN, an attention mechanism
that dynamically prunes the full attention weight matrix to the 50% fine-grained
structured sparse pattern used by the sparse tensor core on NVIDIA A100 GPU.
We provide both theoretical and empirical evidence that demonstrate DFSSATTEN
is a good approximation of the full attention mechanism and can achieve speedups
in wall-clock time under arbitrary sequence length. We evaluate our method on
tasks from various domains under different sequence lengths from 256 to 4096.
DFSSATTEN achieves 1.27 ∼1.89× speedups over the full-attention mechanism
with no accuracy loss on A100 GPU."
INTRODUCTION,0.002386634844868735,"1
INTRODUCTION"
INTRODUCTION,0.003579952267303103,"Transformers (Vaswani et al., 2017) have achieved competitive performance across various domains
like NLP (Ott et al., 2018) and Computer Vision (Dosovitskiy et al., 2021). The key feature that sets
them apart from traditional neural network architectures is the attention mechanism (Vaswani et al.,
2017), which allows the transformers to gather information from the embeddings of elements in the
input sequence in an adaptive and learnable manner."
INTRODUCTION,0.00477326968973747,"Nevertheless, the high computation cost and memory footprint brought by the attention mechanism
make it difﬁcult to apply transformers to latency-sensitive tasks. Many efﬁcient attention mecha-
nisms(Tay et al., 2020b; Zaheer et al., 2020; Beltagy et al., 2020; Tay et al., 2020a; Roy et al., 2021;
Kitaev et al., 2020) have been proposed to address this issue. However, most of them drastically
modify the original full attention mechanism and introduce a handful of hyper-parameters to tune.
Therefore, they require tremendous engineering effort to deploy and optimize. Besides, they usually
need to be trained from scratch instead of exploiting pretrained models like BERT (Devlin et al.,
2019). Some of them rely on ﬁxed sparse patterns or extremely high sparsity to achieve wall-clock
time speedup. Therefore, these methods usually require thousands of pretraining or ﬁne-tuning steps
on speciﬁc tasks and toilsome tuning of several hyper-parameters to reach good accuracy. Last but
not least, previous methods usually introduce additional operators like top-k, sort that cause large
overheads and offset their beneﬁts at moderate sequence length."
INTRODUCTION,0.0059665871121718375,"In this paper, we present DFSSATTEN, a simple and effective sparse attention mechanism that ad-
dress the limitations mentioned above. DFSSATTEN dynamically prunes the full attention score
matrix using 50% fine-grained structured sparse patterns (NVIDIA, 2020). This pattern is GPU
friendly and can leverage the new sparse tensor core on NVIDIA A100 GPU (Mishra et al., 2021).
Our DFSSATTEN offers several advantages over existing studies:"
INTRODUCTION,0.007159904534606206,"• It requires minimal changes to the original full-attention with no hyper-parameters to tune. This
makes it a drop-in replacement of the full attention that only requires to change a few lines of
code. Moreover, it can directly exploit existing pretrained models like BERT (Devlin et al., 2019)
and RoBERTa (Liu et al., 2020)."
INTRODUCTION,0.008353221957040573,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.00954653937947494,"• We dynamically prune the attention scores based on their magnitude under only 50% sparsity.
This allows the pruned attention matrix to reserve the important entries, achieving on par model
accuracy with full attention even without ﬁne-tuning."
INTRODUCTION,0.010739856801909307,"• Our method introduces zero overhead on existing GPU hardware. As a result, we are able to
achieve wall-clock time speedup and memory footprint reduction over the full attention in arbitrary
sequence length."
INTRODUCTION,0.011933174224343675,"To conclude, our main contributions are summarized below:"
INTRODUCTION,0.013126491646778043,"• We propose DFSSATTEN, a dynamic sparse attention mechanism that is a drop-in proxy of the
full attention mechanism. Its effectiveness is justiﬁed by both empirical and theoretical evidence."
INTRODUCTION,0.014319809069212411,"• We present a dedicated CUDA kernel design to completely remove the pruning overhead. The
pruning is implemented as an epilogue of the dense matrix multiplication which produces the
attention score matrix."
INTRODUCTION,0.015513126491646777,"• We evaluate DFSSATTEN on tasks cross various domains and sequence lengths. It achieves 1.27 ∼
1.89× speedup over the full attention with no accuracy loss."
BACKGROUND AND MOTIVATION,0.016706443914081145,"2
BACKGROUND AND MOTIVATION"
BACKGROUND AND MOTIVATION,0.017899761336515514,"We ﬁrst introduce the preliminaries, notations, and background of our paper."
FULL ATTENTION MECHANISM,0.01909307875894988,"2.1
FULL ATTENTION MECHANISM"
FULL ATTENTION MECHANISM,0.02028639618138425,"Given an input sequence X =(x1,..,xn) ∈Rn×d, the full attention mechanism can be deﬁned as"
FULL ATTENTION MECHANISM,0.021479713603818614,"O = Softmax(QKT /
√"
FULL ATTENTION MECHANISM,0.022673031026252982,"d)V ,
(1)"
FULL ATTENTION MECHANISM,0.02386634844868735,"where Q = XWq, K = XWk, and V = XWv are query, key, and value matrices. QKT
forms a full-quadratic adjacency matrix, whose edge weights are the dot-product similarity between
all the elements in the sequence. This adjacency matrix is standardized with 1/
√"
FULL ATTENTION MECHANISM,0.025059665871121718,"d to keep the
unit second moment and then normalized with softmax. At last, the row feature vectors in V are
aggregated according to the normalized adjacency matrix by multiplying them together. In the rest
of this paper, we denote A = Softmax(QKT /
√"
FULL ATTENTION MECHANISM,0.026252983293556086,"d) for simplicity. We refer QKT as the attention
score matrix and A as the attention weight matrix."
EFFICIENT ATTENTION MECHANISM,0.027446300715990454,"2.2
EFFICIENT ATTENTION MECHANISM"
EFFICIENT ATTENTION MECHANISM,0.028639618138424822,"The high computation cost and memory footprint in the full attention mechanism come from A,
whose size grows quadratically with the sequence length n. To address this issue, various efﬁcient
attention mechanisms have been proposed (Tay et al., 2020b)."
EFFICIENT ATTENTION MECHANISM,0.029832935560859187,"Fixed Sparse Patterns. Zaheer et al. (2020); Beltagy et al. (2020) apply a set of ﬁxed sparse
attention patterns on A, like global attention and sliding window attention. These patterns are
constructed from empirical observations and designed GPU-friendly to achieve wall-time speedup.
However, as these patterns are designed empirically and ﬁxed during inference, there is no guarantee
that they can always capture the important entries in A or transfer easily across different tasks."
EFFICIENT ATTENTION MECHANISM,0.031026252983293555,"Dynamic Sparse Patterns. Ham et al. (2021) dynamically generate ﬁne-grained sparse attention
patterns on A with low-cost binary hashing. However, this technique requires specialized hardware
to achieve speedup, so it is not available on general-purpose hardware like GPU. Tay et al. (2020a);
Roy et al. (2021); Kitaev et al. (2020) apply various clustering methods and only compute the at-
tention within each cluster. Although computing full attention in each cluster is more friendly to
GPU compared with ﬁne-grained sparsity, the clustering methods contain several GPU-unfriendly
operators like top-k and sorting that offsets their beneﬁts under moderate sequence length."
EFFICIENT ATTENTION MECHANISM,0.032219570405727926,"Low Rank / Kernel. Wang et al. (2020) project A from n × n to n × k with linear projection.
Choromanski et al. (2021) introduce the FAVOR+ which approximates the softmax with kernel
method. This allows them to change the computation order and reduce the asymptotic complexity"
EFFICIENT ATTENTION MECHANISM,0.03341288782816229,Under review as a conference paper at ICLR 2022
EFFICIENT ATTENTION MECHANISM,0.034606205250596656,Select X
EFFICIENT ATTENTION MECHANISM,0.03579952267303103,Dot-product
EFFICIENT ATTENTION MECHANISM,0.03699284009546539,Sparse Tensor Core
EFFICIENT ATTENTION MECHANISM,0.03818615751789976,"Fine-grained
Structured Pruning"
EFFICIENT ATTENTION MECHANISM,0.03937947494033413,1:2/2:4 sparsity
EFFICIENT ATTENTION MECHANISM,0.0405727923627685,Compress
EFFICIENT ATTENTION MECHANISM,0.041766109785202864,NonzerosMetadata 2:4 1:2
EFFICIENT ATTENTION MECHANISM,0.04295942720763723,"Figure 1: A100 GPU Fine-Grained Structured Sparsity Pruning. (NVIDIA, 2020)"
EFFICIENT ATTENTION MECHANISM,0.0441527446300716,"to linear. However, the low-rank projection and kernel construction also introduce considerable
overhead. This makes these methods only effective under long sequence length."
EFFICIENT ATTENTION MECHANISM,0.045346062052505964,"Besides, the previous studies drastically change the attention mechanisms, tens of thousands pre-
training or ﬁnetuning steps are required to reach a comparable performance with the origin full
attention mechanism. So they require tremendous engineering effort to deploy."
EFFICIENT ATTENTION MECHANISM,0.046539379474940336,"2.3
FINE-GRAINED STRUCTURED SPARSITY IN NVIDIA A100 GPU"
EFFICIENT ATTENTION MECHANISM,0.0477326968973747,"NVIDIA introduces the ﬁne-grained structured sparsity in the A100 GPU. As shown in Figure 1,
the dense input matrix is pruned with ﬁne-grained structured pruning. If the data type is ﬂoat, 1:2
sparsity is used which selects the larger one in two consecutive entries. If the data type is bﬂoat16
or ﬂoat16, the 2:4 sparsity is used which selects two larger ones among four consecutive elements.
After the pruning, the result is compressed to nonzeros and metadata. The nonzeros contain the
value of reserved data that is 50% smaller than the original one. The metadata records the index of
the nonzeros in the origins matrix. It takes 4 bit metadata to record the decision of each 1:2 or 2:4
selection. Therefore, the metadata is only 1/16 of the original dense matrix in terms of bits. This
compressed sparse matrix can be multiplied with a dense matrix under the support of the sparse
tensor core to achieve signiﬁcant speedup."
EFFICIENT ATTENTION MECHANISM,0.04892601431980907,"This ﬁne-grained structured sparsity has been applied to the static weight matrices in various neural
network models including transformer (Mishra et al., 2021). It can effectively accelerate the feed-
forward part of the transformer up to 1.9×. However, to the best of our knowledge, no previous
studies use it in the attention mechanism where the attention weight matrix is dynamically generated
for each sequence. One plausible explanation is that during pruning, GPU must read the whole
dense matrix to be pruned from the memory. Then, after selecting the elements to be reserved under
the 1:2 and 2:4 pattern, it must also generate the metadata encoded in a special format such that
the metadata can be used efﬁciently later. All these overheads will offset the beneﬁt brought by the
pruning if we do it on the ﬂy."
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM,0.050119331742243436,"3
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM"
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM,0.0513126491646778,"In this section, we ﬁrst give an overview of our DFSSATTEN method. Then, we discuss the design
considerations of exploring sparsity in attention and the choice of sparse granularity in our method
for GPU-friendly implementation and effectiveness. Finally, we brieﬂy introduce our GPU kernel
design to remove pruning overhead."
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM,0.05250596658711217,"Our proposed DFSSATTEN mechanism is simple and effective, as illustrated in Figure 2. Com-
pared with the full-quadratic attention mechanism, our method dynamically prunes attention scores
without incurring storage or computation overhead, while maintaining the effectiveness of atten-
tion. More importantly, our method can achieve practical speedups of attention on existing GPU
hardware with customized CUDA kernels. Listing 1 shows all the modiﬁcations to be made to use
DFSSATTEN."
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM,0.05369928400954654,"Under review as a conference paper at ICLR 2022 𝑛𝑄 𝐾!
𝑉 𝑌"
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM,0.05489260143198091,𝑛/2 𝑛/16 𝑛 𝑑 𝑑 𝑛 𝑑
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM,0.05608591885441527,"𝑄𝐾!
𝜎(𝑄𝐾!)
𝜎 𝑂
𝑛𝑄 𝐾!
𝑉 𝑌 𝑛 𝑛 𝑑 𝑑 𝑛 𝑑 𝜎"
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM,0.057279236276849645,"𝑂
𝑄𝐾!
𝜎(𝑄𝐾!)"
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM,0.05847255369928401,"(A) Full Attention
(B) DFSSATTEN"
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM,0.059665871121718374,"Figure 2: Overview of our Dynamic Fine-grained Structured Sparse Attention Mechanism. σ repre-
sents softmax,
1
√"
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM,0.060859188544152745,d is omitted for simplicity.
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM,0.06205250596658711,"# Full attention mechanism
def full_attention(q,k,v):
attn_weight = torch.bmm(q, k.transpose(1, 2))
attn_weight = torch.nn.functional.softmax(attn_weight, -1)
return torch.bmm(attn_weight, v)"
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM,0.06324582338902147,"# DFSS attention mechanism
import dspattn
def dfss_attention(q,k,v):
attn_weight, metadata = dspattn.bmm(q, k)
attn_weight = torch.nn.functional.softmax(attn_weight, -1)
return dspattn.spmm(attn_weight, metadata, v)"
DYNAMIC FINE-GRAINED STRUCTURED SPARSE ATTENTION MECHANISM,0.06443914081145585,Listing 1: Example of using DFSSATTEN. The “dspattn” is the package we developed.
DESIGN CONSIDERATIONS FOR EXPLOITING ATTENTION SPARSITY,0.06563245823389022,"3.1
DESIGN CONSIDERATIONS FOR EXPLOITING ATTENTION SPARSITY"
DESIGN CONSIDERATIONS FOR EXPLOITING ATTENTION SPARSITY,0.06682577565632458,"𝑄𝐾!
S𝑜𝑓𝑡𝑚𝑎𝑥
𝐴𝑉
0
1
2"
DESIGN CONSIDERATIONS FOR EXPLOITING ATTENTION SPARSITY,0.06801909307875895,Figure 3: Attention Stages.
DESIGN CONSIDERATIONS FOR EXPLOITING ATTENTION SPARSITY,0.06921241050119331,"As illustrated in Figure 3, the attention mechanism can be considered
as three stages: QKT , Softmax, and AV . To design a sparse atten-
tion mechanism, the ﬁrst decision to make is where should we induce
pruning to sparsify the attention."
DESIGN CONSIDERATIONS FOR EXPLOITING ATTENTION SPARSITY,0.07040572792362769,"If we start from 0 , all the three stages will be beneﬁted from the sparsity given effective imple-
mentation on GPU: the dense matrix multiplication between Q and K will be replaced with the
sampled dense-dense matrix multiplication (SDDMM) which only computes the entries identiﬁed
by the sparse pattern. The Softmax only operates on the nonzero values in each row. The original
dense matrix multiplication between A and V will be replaced with a sparse matrix-matrix multipli-
cation (SpMM) which multiplies a sparse matrix with a dense matrix. However, as it is not possible
to exactly know which entry in QKT has higher magnitude before computing QKT , starting from"
DESIGN CONSIDERATIONS FOR EXPLOITING ATTENTION SPARSITY,0.07159904534606205,0 usually requires some additional components to predict the location of important entries.
DESIGN CONSIDERATIONS FOR EXPLOITING ATTENTION SPARSITY,0.07279236276849642,"Starting from 1 requires us to compute a dense matrix multiplication between Q and K. The
beneﬁt is that we can explicitly select important entries from QKT without prediction. As the
softmax is a monotonically increasing function, starting from 2 does not offer any beneﬁts over 1
but throws away the opportunity to accelerate Softmax."
DESIGN CONSIDERATIONS FOR EXPLOITING ATTENTION SPARSITY,0.07398568019093078,"In this paper, we choose to start from 1 based on two considerations. First, replacing the dense
matrix multiplication with SDDMM at QKT offers limited speedup even at high sparsity. Chen
et al. (2021b) show that it is difﬁcult for SDDMM to achieve speedup over its dense counterpart
under 80% sparsity even with some structured design. Second, starting from 1 allows us to keep
our design simple such that it does not introduce additional overhead or hyper-parameters to tune."
ON THE GRANULARITY OF SPARSE ATTENTION PATTERNS,0.07517899761336516,"3.2
ON THE GRANULARITY OF SPARSE ATTENTION PATTERNS"
ON THE GRANULARITY OF SPARSE ATTENTION PATTERNS,0.07637231503579953,"The second decision to make is what sparse pattern to use as it will tremendously affect the latency of
SpMM as well as the overhead to encode the sparse QKT . Existing studies exploit various sparse
encoding schemes. For instance, the compressed sparse row (CSR) is popular for encoding ﬁne-
grained sparsity. However, CSR-based SpMM requires over 95% sparsity to be on par with its dense
counterpart (Chen et al., 2021b). Block sparsity is also widely used as it can bring considerable
wall-time speedup at moderate sparsity given large block size. However, it cannot capture some"
ON THE GRANULARITY OF SPARSE ATTENTION PATTERNS,0.07756563245823389,Under review as a conference paper at ICLR 2022
ON THE GRANULARITY OF SPARSE ATTENTION PATTERNS,0.07875894988066826,"ﬁne-grained attention patterns. Moreover, these patterns require data comparisons within the entire
row, which is difﬁcult to execute in parallel and unfriendly to GPUs."
ON THE GRANULARITY OF SPARSE ATTENTION PATTERNS,0.07995226730310262,"We ﬁnd the ﬁne-grained structured sparsity mentioned in Section 2.3 is a good choice as long as we
address the pruning overhead. On one hand, the 1:2 and 2:4 selections are performed locally and
are easy to execute in parallel. On the other hand, the size of compressed nonzeros is half of the
original dense attention matrix, so the softmax only needs half of the computations. Powered by the
NVIDIA Sparse Tensor Core, the SpMM between the compressed A and V can also achieve 1.7×
speedup."
EMPIRICAL RESULTS OF DFSSATTEN MECHANISM,0.081145584725537,"3.3
EMPIRICAL RESULTS OF DFSSATTEN MECHANISM"
EMPIRICAL RESULTS OF DFSSATTEN MECHANISM,0.08233890214797136,Table 1: F1 Score w/o Finetune on SQuAD v1.1
EMPIRICAL RESULTS OF DFSSATTEN MECHANISM,0.08353221957040573,"Full
1:2
2:4
93.17 ± 0.27
92.86 ± 0.22
93.00 ± 0.16"
EMPIRICAL RESULTS OF DFSSATTEN MECHANISM,0.08472553699284009,"Empirically, we ﬁnd this pattern can well ap-
proximate the full attention mechanism.
We
ﬁrst ﬁnetune a BERT-large model on SQuAD
v1.1 under full attention. Then, we directly replace the full attention with the 1:2 and 2:4 attention
without additional ﬁnetuning. The F1-scores are summarized in Table 1 under Cl = 95%. It is
obvious that the accuracy loss is only around one sigma even without ﬁnetuning."
REMOVING PRUNING OVERHEAD,0.08591885441527446,"3.4
REMOVING PRUNING OVERHEAD"
REMOVING PRUNING OVERHEAD,0.08711217183770883,"As mentioned in Section 2.3, the major challenge that hinders us from using the ﬁne-grained struc-
tured sparse attention is the pruning overhead. We observe that when computing QKT , the results
are ﬁrst accumulated in GPU registers and written to memory when all the computations are done.
Therefore, we can implement the pruning as an epilogue of the matrix multiplication: after the accu-
mulation is ﬁnished, we compare the data stored in the registers, select the larger ones and generate
the metadata. Then, we only write the reserved non-zeros and metadata to memory. This design
brings two beneﬁts. First, it completely removes the overhead caused by reading the matrix to be
pruned from memory, so it has zero overhead. Second, the memory footprint caused by the attention
weight matrix is reduced from n2 × 32-bit to n2"
REMOVING PRUNING OVERHEAD,0.0883054892601432,2 × 32-bit + n2
REMOVING PRUNING OVERHEAD,0.08949880668257756,"16 × 32-bit as the original n × n full
attention weight matrix is not written to memory. The more detailed description of the CUDA kernel
design including how to encode the metadata on the ﬂy is summarized in Appendix A.1."
THEORETICAL RESULTS,0.09069212410501193,"4
THEORETICAL RESULTS"
THEORETICAL RESULTS,0.09188544152744631,"In this section, we provide more theoretical and empirical evidence that justify our DFSSATTN as a
good proxy of the full attention mechanism. The strategy is to ﬁrst derive the theoretical value of 1)
quality of the approximation with different sparse patterns 2) speedup can be achieved under certain
sparsity. Then, we compare the quality of different methods under the same speedup."
ATTENTION LOTTERY TICKET,0.09307875894988067,"4.1
ATTENTION LOTTERY TICKET"
ATTENTION LOTTERY TICKET,0.09427207637231504,"We borrow the lottery ticket hypothesis (Frankle & Carbin, 2019) and extend it to the attention
mechanism. The last step AV in the attention mechanism can be viewed as the aggregation in the
graph neural network. Following the Generalized Attention Mechanism (Zaheer et al., 2020), we
describe it with a weighted directed graph G = (A, X). A is the adjacent matrix and Au,v > 0
indicates that element xu attends to xv. Inspired by the Graph Lottery Tickets (Chen et al., 2021a),
we propose the Attention Lottery Ticket as follows."
ATTENTION LOTTERY TICKET,0.0954653937947494,"Attention Lottery Ticket (ALT). Given a fully connected d graph G = {A, X} constructed from
the full quadratic attention mechanism (Vaswani et al., 2017), the associated sub-graph can be de-
ﬁned as Gs = {m ⊙A, X}, where m is a binary mask. If a Gs has the performance matching
or surpassing the original full quadratic attention mechanism, then we deﬁne the sparse attention
mechanism with Gs as an attention lottery ticket."
ATTENTION LOTTERY TICKET,0.09665871121718377,"Zaheer et al. (2020) have proved the existence of lottery tickets by showing 1) sparse attention
mechanisms are universal approximators of sequence to sequence functions when being used as
encoder 2) sparse encoder-decoder transformers are Turing Complete. So the remaining problem is
how to identify the winning tickets Gs at runtime."
ATTENTION LOTTERY TICKET,0.09785202863961814,Under review as a conference paper at ICLR 2022
QUALITY OF THE LOTTERY TICKET,0.09904534606205251,"4.2
QUALITY OF THE LOTTERY TICKET"
QUALITY OF THE LOTTERY TICKET,0.10023866348448687,"A popular strategy that empirically works well is selecting the top-k neighborhood in G based on
the magnitude of edge weight. We refer it as Top-k Sparsity. Intuitively, this strategy is based on the
hypothesis that the edges with larger edge weight are more important. It has been widely adapted in
existing studies (Frankle & Carbin, 2019; Chen et al., 2021a; Ham et al., 2021; Wang et al., 2021)
and demonstrated its ability to preserve model accuracy at a high sparsity ratio. Following this trend
of work, we deﬁne the Quality of Attention Lottery Ticket as follows:
Deﬁnition 4.1. (Lp-Quality of Attention Lottery Ticket) The quality of attention lottery ticket Gs =
{m ⊙A, X} under density s =
1
n2
Pn
j=1
Pn
i=1 mj,i is deﬁned as"
QUALITY OF THE LOTTERY TICKET,0.10143198090692124,"Qp = 1 n n
X j=1"
QUALITY OF THE LOTTERY TICKET,0.1026252983293556,"Pn
i=1 (m ⊙A)p
j,i
Pn
i=1 Ap
j,i
.
(2)"
QUALITY OF THE LOTTERY TICKET,0.10381861575178998,"The above deﬁnition computes the expectation of normalized Lp norm in each row of the attention
score matrix. The p is a task-dependent factor that indicates how the accuracy depends on the edges
with higher magnitude. In this paper, we compare the Lp-Quality of tickets yield by three types of
sparse patterns: Top-K, ﬁxed, and our dynamic 1:2 and 2:4 sparse pattern. Particularly, we have the
proposition below:"
QUALITY OF THE LOTTERY TICKET,0.10501193317422435,"Proposition 4.1. Under the assumption that the entries in QKT /
√"
QUALITY OF THE LOTTERY TICKET,0.10620525059665871,"d follow i.i.d. N(µ, σ), we have"
QUALITY OF THE LOTTERY TICKET,0.10739856801909307,"Qp
topk|s ≈
1 + erf

pσ
√"
QUALITY OF THE LOTTERY TICKET,0.10859188544152745,"2 −erfinv(1 −2s)
"
QUALITY OF THE LOTTERY TICKET,0.10978520286396182,"2
, Qp
fix|s = s,
Qp
2:4 ≥Qp
1:2 = 1 + erf
  pσ 2
 2
(3)"
QUALITY OF THE LOTTERY TICKET,0.11097852028639618,(Proof: Appendix A.2)
QUALITY OF THE LOTTERY TICKET,0.11217183770883055,"It is obvious that the Qp
topk achieves the upper bound of Qp under s. Besides, the pσ is always
positive. Therefore, we also have Qp
2:4 ≥Qp
1:2 > Qp
fix|s=0.5 = 1/2."
EFFICIENCY OF THE LOTTERY TICKET,0.11336515513126491,"4.3
EFFICIENCY OF THE LOTTERY TICKET"
EFFICIENCY OF THE LOTTERY TICKET,0.11455847255369929,"A lottery ticket with high quality does not necessarily mean that it is also efﬁcient to execute for
wall-clock time speedup. In this section, we analyze the efﬁciency of the three sparse patterns."
EFFICIENCY OF THE LOTTERY TICKET,0.11575178997613365,"Top-K Sparsity. Zhao et al. (2019) explicitly select k neighbors in each row of A based on their
magnitude. However, as shown in their Table 4, the explicit sparse transformer has lower inference
throughput despite k ≪n. On one hand, the top-k operator is difﬁcult to parallel and introduces
high overhead. On the other hand, even if an oracle top-k sparsity mask m were provided with
zero overhead, it would still be difﬁcult for the explicit Top-K sparse attention to beat its dense
counterpart. We provide a theoretical upper bound for density s in Proposition 4.2.
Proposition 4.2. Given embedding size d and the maximum tiling size supported by GPU T, the
upper bound of the speedup achieved by Top-K Sparsity under density s is (Proof: Appendix A.3)"
EFFICIENCY OF THE LOTTERY TICKET,0.11694510739856802,"Speedup <
4d + 3T
2d + T + (d + 2T + dT)s.
(4)"
EFFICIENCY OF THE LOTTERY TICKET,0.11813842482100238,"As typical values for the dimension d and tiling size T are d = 64, T = 128, s < 4.5% is a
necessary and insufﬁcient condition to have Speedup > 1. Notably, this is not a strict upper bound
as we did not take the overhead of identifying top-k entries into consideration. Therefore, the strict
upper bounder should be even smaller."
EFFICIENCY OF THE LOTTERY TICKET,0.11933174224343675,"Fixed Sparsity. As the ﬁxed sparse pattern are designed or learned before inference, they can be
designed to be GPU-friendly and have the same tiling size with the dense matrix multiplication.
Therefore, we can derive the upper bound of the speedup under density s with the same strategy in
Proposition 4.2:"
EFFICIENCY OF THE LOTTERY TICKET,0.12052505966587113,"Speedup =
n2   2d"
EFFICIENCY OF THE LOTTERY TICKET,0.12171837708830549,"T + 1

+ 2n2 + nd
  2n"
EFFICIENCY OF THE LOTTERY TICKET,0.12291169451073986,"T + 1
"
EFFICIENCY OF THE LOTTERY TICKET,0.12410501193317422,sn2   2d
EFFICIENCY OF THE LOTTERY TICKET,0.12529832935560858,"T + 1

+ 2n2s + nd

(1+s)n"
EFFICIENCY OF THE LOTTERY TICKET,0.12649164677804295,"T
+ 1
 n≫d
=
4d + 3T
(1 + 3s)d + 3sT .
(5)"
EFFICIENCY OF THE LOTTERY TICKET,0.1276849642004773,Under review as a conference paper at ICLR 2022
EFFICIENCY OF THE LOTTERY TICKET,0.1288782816229117,"Dynamic 1:2 / 2:4 Sparsity. Similarly, we can derive the theoretical speedup with 1:2 and 2:4
sparsity as follows"
EFFICIENCY OF THE LOTTERY TICKET,0.13007159904534607,"Speedup =
n2   2d"
EFFICIENCY OF THE LOTTERY TICKET,0.13126491646778043,"T + 1

+ 2n2 + nd
  2n"
EFFICIENCY OF THE LOTTERY TICKET,0.1324582338902148,"T + 1
"
EFFICIENCY OF THE LOTTERY TICKET,0.13365155131264916,n2   2d T + 1 2 + 1
EFFICIENCY OF THE LOTTERY TICKET,0.13484486873508353,"16

+ n2 + nd
  n"
EFFICIENCY OF THE LOTTERY TICKET,0.1360381861575179,"T +
n
2T +
n
16T + 1
 n≫d
=
64d + 48T
57d + 25T .
(6)"
QUALITY OF THE LOTTERY TICKETS UNDER THE SAME EFFICIENCY,0.13723150357995226,"4.4
QUALITY OF THE LOTTERY TICKETS UNDER THE SAME EFFICIENCY"
QUALITY OF THE LOTTERY TICKETS UNDER THE SAME EFFICIENCY,0.13842482100238662,"With the theoretical conclusions above, we compare the quality of the lottery ticket under our dy-
namic 1:2 and 2:4 sparsity with the other two methods under the same efﬁciency."
QUALITY OF THE LOTTERY TICKETS UNDER THE SAME EFFICIENCY,0.13961813842482101,Comparison with Top-K Sparsity. The Top-K sparsity achieves the same efﬁciency with ours at
QUALITY OF THE LOTTERY TICKETS UNDER THE SAME EFFICIENCY,0.14081145584725538,"s <
(4d + 3T)(57d + 25T)
(64d + 48T)(d + 2T + dT) −
2d + T
(d + 2T + dT).
(7)"
QUALITY OF THE LOTTERY TICKETS UNDER THE SAME EFFICIENCY,0.14200477326968974,"With typical values T = 128, d = 64, we have s < 0.02. We can substitute it to Proposition 4.1
and get Qp
topk < Qp
1:2 when pσ < 7. On the other hand, when pσ > 7, although the Top-K sparsity
produces tickets with higher quality, Qp
1:2|pσ=7 ≈0.9999996 is already very close to 1."
QUALITY OF THE LOTTERY TICKETS UNDER THE SAME EFFICIENCY,0.1431980906921241,Comparison with Fixed Sparsity. The ﬁxed sparsity achieves the same efﬁciency with ours when
QUALITY OF THE LOTTERY TICKETS UNDER THE SAME EFFICIENCY,0.14439140811455847,s = (4d + 3T)(64d + 48T)
QUALITY OF THE LOTTERY TICKETS UNDER THE SAME EFFICIENCY,0.14558472553699284,"(57d + 25T)(3d + 3T) −
d
3d + 3T .
(8)"
QUALITY OF THE LOTTERY TICKETS UNDER THE SAME EFFICIENCY,0.1467780429594272,"With typical values T = 128, d = 64, we have s ≈0.63. On the other hand, we have theoretical
value of σ ≈1 and p ≥1 . The p ≥1 is based on the observation that the edges with higher
magnitude are more inﬂuential. Therefore, we have pσ ≥1 and Qp
1:2 ≥0.76 > 0.63 = Qp
fix|s=0.63."
QUALITY OF THE LOTTERY TICKETS UNDER THE SAME EFFICIENCY,0.14797136038186157,"To conclude, compared with both top-k sparsity and ﬁxed sparsity, our method can always yield
lottery tickets with higher quality under the same efﬁciency. To support this conclusion, we further
provide some empirical studies in Appendix A.4. Besides, we found both theoretically and em-
pirically that our method is a good complementary to the kernel-based transformers like Performer
(Choromanski et al., 2021). We add more discussions about it in Appendix A.5."
EVALUATION,0.14916467780429593,"5
EVALUATION"
EVALUATION,0.15035799522673032,"In this section, we ﬁrst evaluate the accuracy of our dynamic ﬁne-grained structured sparse attention
mechanism on tasks across different domains. Then, we proﬁle our methods on NVIDIA A100 GPU
under different sequence lengths from 256 to 4096 to show that we can achieve practical speedup in
arbitrary sequence length."
MODEL ACCURACY,0.1515513126491647,"5.1
MODEL ACCURACY"
MODEL ACCURACY,0.15274463007159905,"To show that our method is effective in comprehensive scenarios, we ﬁrst evaluate the model accu-
racy on tasks in different domains and sequence length. For models under “bﬂoat16” data type, we
ﬁrst ﬁnetune them from the pretrained model under “ﬂoat” data type as “ﬂoat” provides more pre-
cise gradient that helps convergence. After the ﬁnetuning, we directly cast all the parameters in the
model to “bﬂoat16” and test it on the test dataset. For Question Answering and Masked Language
Modeling tasks, we report the results averaged over 8 runs under different random seeds."
MODEL ACCURACY,0.15393794749403342,"Question Answering. We evaluate the BERT-large on SQuAD v1.1 under sequence length 384.
We use the “bert-large-uncased-whole-word-masking” in Huggingface (Wolf et al., 2020) as the
pretrained model and ﬁnetune it with the default conﬁguration in Huggingface 1. The F1-scores of
“1:2(ﬂoat)” and “2:4(bﬂoat16)” without ﬁnetuning are obtained by directly using the checkpoints
from “Transformer(ﬂoat)”. The F1-scores of “Transformer (ﬂoat)” and “Transformer (bﬂoat16)”
without ﬁnetuning are obtained by directly using the checkpoints from “DFSSATTEN 1:2 (ﬂoat)”"
MODEL ACCURACY,0.15513126491646778,Under review as a conference paper at ICLR 2022
MODEL ACCURACY,0.15632458233890215,Table 2: F1 score on BERT-large SQuAD v1.1 (Cl=95%)
MODEL ACCURACY,0.1575178997613365,"Model
w/o ﬁnetune
w/ ﬁnetune
Transformer (ﬂoat)
93.22 ± 0.15
93.17 ± 0.27
Transformer (bﬂoat16)
93.34 ± 0.31
93.18 ± 0.27
DFSSATTEN 1:2 (ﬂoat)
92.86 ± 0.22
93.07 ± 0.17
DFSSATTEN 2:4 (bﬂoat16)
93.00 ± 0.16
93.28 ± 0.29"
MODEL ACCURACY,0.15871121718377088,"and “DFSSATTEN 2:4 (bﬂoat16)”, respectively, and running inference with dense attention mecha-
nism."
MODEL ACCURACY,0.15990453460620524,"As shown in Table 2, with ﬁnetuning, our 1:2 sparsity has only 0.1 F1 score loss that is smaller
than the standard deviation. Our 2:4 sparsity even achieves a little bit of performance improvement
over the dense baseline. One plausible explanation is that while the 2:4 sparsity can keep most of
the important edges, it also occasionally drops a small fraction of important edges which acts like
the attention dropout technique (Zehui et al., 2019). Besides, directly applying our methods to the
dense transformer without ﬁnetuning also achieves comparable results, it justiﬁes that our method
can well approximate the dense attention mechanism."
MODEL ACCURACY,0.1610978520286396,"Masked Language Modeling. We also evaluate our models on the masked modeling tasks on
Wikitext-2 and Wikitext-103 under sequence length 512. Similar to the question-answering tasks,
we choose the “roberta-large” as the pretrained model and ﬁnetune it under the default conﬁguration
in Huggingface 2. The results are summarized in Table 3. Similarly, the perplexities achieved by our
methods are on par with the dense transformer."
MODEL ACCURACY,0.162291169451074,Table 3: Perplexity on roBERTa-large (Cl=95%)
MODEL ACCURACY,0.16348448687350836,"Model
Wikitext-2
Wikitext-103
w/o ﬁnetune
w/ ﬁnetune
w/o ﬁnetune
w/ ﬁnetune
Transformer (ﬂoat)
2.85 ± 0.09
2.83 ± 0.09
2.63 ± 0.03
2.62 ± 0.04
Transformer (bﬂoat16)
2.85 ± 0.05
2.85 ± 0.07
2.62 ± 0.08
2.63 ± 0.05
DFSSATTEN 1:2 (ﬂoat)
2.88 ± 0.06
2.88 ± 0.07
2.64 ± 0.06
2.64 ± 0.06
DFSSATTEN 2:4 (bﬂoat16)
2.88 ± 0.07
2.84 ± 0.04
2.63 ± 0.03
2.61 ± 0.04"
MODEL ACCURACY,0.16467780429594273,"Long Range Arena. For sequence length longer than 512, we incorporate four tasks from the Long
Range Arena (Tay et al., 2021), including ListOps, Text Classiﬁcation, Document Retrieval, and
Image Classiﬁcation under sequence lengths 2048, 2048, 4096, and 1024, respectively. We omit the
Pathﬁnder (1K) task as we cannot replicate the results, which was also reported in Lu et al. (2021).
For a fair comparison with other efﬁcient transformers, the model is trained from scratch under the
default conﬁgurations. The results are summarized in Table 4. Our method achieves comparable
accuracy on all the three benchmarks for long sequence."
MODEL ACCURACY,0.1658711217183771,"Table 4: Accuracy of different transformer models on LRA benchmark. We follow the training
instructions from Tay et al. (2021) to reuse the results from this paper."
MODEL ACCURACY,0.16706443914081145,"Model
ListOps (n=2048)
Text (n=2048)
Retrieval (n=4000)
Image (n=1024)
Avg
Transformer (ﬂoat)
35.91
65.05
61.72
42.15
51.21
Transformer (bﬂoat16)
35.92
65.03
61.73
42.17
51.21
Local Attention
15.82
52.98
53.39
41.46
40.91
Sparse Trans.
17.07
63.58
59.59
44.24
46.12
Longformer
35.63
62.85
56.89
42.22
49.40
Linformer
35.70
53.94
52.27
38.56
45.12
Reformer
37.27
56.10
53.40
38.07
46.21
Sinkhorn Trans.
33.67
61.20
53.83
41.23
47.48
Synthesizer
36.99
61.68
54.67
41.61
48.74
BigBird
36.05
64.02
59.29
40.83
50.05
Linear Trans.
16.13
65.90
53.09
42.34
44.37
Performer
18.01
65.40
53.82
42.77
45.00
DFSSATTEN 1:2 (ﬂoat)
36.85
64.95
61.83
42.02
51.41
DFSSATTEN 2:4 (bﬂoat16)
37.19
64.91
62.26
42.31
51.67"
SPEEDUP,0.16825775656324582,"5.2
SPEEDUP"
SPEEDUP,0.16945107398568018,"In this section, we demonstrate the speedup achieved by our method across different sequence
lengths. For a fair comparison, we only show the speedup achieved on the attention mechanism"
SPEEDUP,0.17064439140811455,"1https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering
2https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling"
SPEEDUP,0.1718377088305489,Under review as a conference paper at ICLR 2022
SPEEDUP,0.1730310262529833,"0
0.2
0.4
0.6
0.8"
SPEEDUP,0.17422434367541767,"1
1.2
1.4
1.6
1.8 2"
SPEEDUP,0.17541766109785203,Transformer Ours
SPEEDUP,0.1766109785202864,Performer
SPEEDUP,0.17780429594272076,Reformer
SPEEDUP,0.17899761336515513,Routing
SPEEDUP,0.1801909307875895,Sinkhorn
SPEEDUP,0.18138424821002386,Nystrom
SPEEDUP,0.18257756563245822,Transformer Ours
SPEEDUP,0.18377088305489261,Performer
SPEEDUP,0.18496420047732698,Reformer
SPEEDUP,0.18615751789976134,Routing
SPEEDUP,0.1873508353221957,Sinkhorn
SPEEDUP,0.18854415274463007,Nystrom
SPEEDUP,0.18973747016706444,Transformer Ours
SPEEDUP,0.1909307875894988,Performer
SPEEDUP,0.19212410501193317,Reformer
SPEEDUP,0.19331742243436753,Routing
SPEEDUP,0.19451073985680192,Sinkhorn
SPEEDUP,0.1957040572792363,Nystrom
SPEEDUP,0.19689737470167065,Transformer Ours
SPEEDUP,0.19809069212410502,Performer
SPEEDUP,0.19928400954653938,Reformer
SPEEDUP,0.20047732696897375,Routing
SPEEDUP,0.2016706443914081,Sinkhorn
SPEEDUP,0.20286396181384247,Nystrom
SPEEDUP,0.20405727923627684,Transformer Ours
SPEEDUP,0.2052505966587112,Performer
SPEEDUP,0.2064439140811456,Reformer
SPEEDUP,0.20763723150357996,Routing
SPEEDUP,0.20883054892601433,Sinkhorn
SPEEDUP,0.2100238663484487,Nystrom
SPEEDUP,0.21121718377088305,"256
512
1024
2048
4096"
SPEEDUP,0.21241050119331742,bfloat16
SPEEDUP,0.21360381861575178,"Series1
Series2
Series3
Series4 0 0.5 1 1.5 2"
SPEEDUP,0.21479713603818615,Transformer Ours
SPEEDUP,0.2159904534606205,Performer
SPEEDUP,0.2171837708830549,Reformer
SPEEDUP,0.21837708830548927,Routing
SPEEDUP,0.21957040572792363,Sinkhorn
SPEEDUP,0.220763723150358,Nystrom
SPEEDUP,0.22195704057279236,Transformer Ours
SPEEDUP,0.22315035799522673,Performer
SPEEDUP,0.2243436754176611,Reformer
SPEEDUP,0.22553699284009546,Routing
SPEEDUP,0.22673031026252982,Sinkhorn
SPEEDUP,0.22792362768496421,Nystrom
SPEEDUP,0.22911694510739858,Transformer Ours
SPEEDUP,0.23031026252983294,Performer
SPEEDUP,0.2315035799522673,Reformer
SPEEDUP,0.23269689737470167,Routing
SPEEDUP,0.23389021479713604,Sinkhorn
SPEEDUP,0.2350835322195704,Nystrom
SPEEDUP,0.23627684964200477,Transformer Ours
SPEEDUP,0.23747016706443913,Performer
SPEEDUP,0.2386634844868735,Reformer
SPEEDUP,0.2398568019093079,Routing
SPEEDUP,0.24105011933174225,Sinkhorn
SPEEDUP,0.24224343675417662,Nystrom
SPEEDUP,0.24343675417661098,Transformer Ours
SPEEDUP,0.24463007159904535,Performer
SPEEDUP,0.2458233890214797,Reformer
SPEEDUP,0.24701670644391407,Routing
SPEEDUP,0.24821002386634844,Sinkhorn
SPEEDUP,0.2494033412887828,Nystrom
SPEEDUP,0.25059665871121717,"256
512
1024
2048
4096 float"
SPEEDUP,0.25178997613365156,Sequence Length
SPEEDUP,0.2529832935560859,Latency normalized to Transformer
SPEEDUP,0.2541766109785203,"Figure 4: Latency breakdown of different attention mechanism. For each conﬁguration, we normal-
ize the latency to Transformer with full attention mechanism and cut off the axis at 2 for clarity."
SPEEDUP,0.2553699284009546,"declared in equation 1. This is because the end-to-end speedup can be affected by various other
factors including the quantization and pruning strategies applied to the other parts of the transformer
models, the embedding sizes, and the efﬁciency of the code implementation (e.g. some operators
could be fused for lower latency). We present the end-to-end speedup and memory footprint reduc-
tion under different sequence length, number of heads, and hidden dimension in Appendix A.6. For
models in previous studies, We also apply the PyTorch JIT script when possible in case that their
implementations are not efﬁcient. The conﬁguration we use is as follows: Each layer contains 4
heads, the feature dimension per head is 64. The batch size is set to be large enough to keep the
GPU busy. We summarize the proﬁling results in Figure 4. We normalize the latency to the Trans-
former with full attention mechanism under each conﬁguration. We also cut the y axis off at 2 for
clarity, because some methods designed for long sequence could be more than 20× slower than the
dense transformer at moderate sequence length."
SPEEDUP,0.256563245823389,"First of all, our method achieves 1.27∼1.89x speedup over the transformer with full attention. It
is the only method that brings consistent speedup across different sequence lengths, while other
methods from previous papers suffer from high overhead at moderate and short sequence lengths.
Second, under the ﬂoat data type, our method achieves speedup in all the three stages with zero
overhead. This accords with our arguments in Section 3. Under the data type bﬂoat16, the QKT in
our method is a little bit slower than the dense baseline. The reason is that selecting 2 larger ones
from 4 elements requires more comparisons, which results in more warp divergence."
CONCLUSION AND DISCUSSION,0.2577565632458234,"6
CONCLUSION AND DISCUSSION"
CONCLUSION AND DISCUSSION,0.25894988066825775,"In this paper, we present DFSSATTEN, a dynamic ﬁne-grained structured sparse attention mecha-
nism that dynamically prunes the QKT on the ﬂy to 1:2 and 2:4 structured sparsity. As it only
requires 50% sparsity, it can achieve no accuracy loss compared with the full attention mechanism
across tasks in various domains and sequence lengths. Besides, it only requires modifying a few
lines of code, which makes it a drop-in replacement of the full-attention mechanism. Moreover,
powered by our customized CUDA kernels and the new sparse tensor core on Ampere GPU, we
achieve 1.27∼1.89× speedup over the full attention in arbitrary sequence length. All of these pieces
of evidence demonstrate that our method can be a good replacement for the full attention mechanism.
Our method is also orthogonal to many existing efﬁcient attention mechanisms and can potentially
be applied jointly for further speedup. We present two examples in Appendix A.7."
CONCLUSION AND DISCUSSION,0.26014319809069214,Under review as a conference paper at ICLR 2022
REFERENCES,0.2613365155131265,REFERENCES
REFERENCES,0.26252983293556087,"Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150, 2020."
REFERENCES,0.2637231503579952,"Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. A uniﬁed lottery
ticket hypothesis for graph neural networks. In International Conference on Machine Learning,
pp. 1695–1706. PMLR, 2021a."
REFERENCES,0.2649164677804296,"Zhaodong Chen, Zheng Qu, Liu Liu, Yufei Ding, and Yuan Xie. Efﬁcient tensor core-based gpu
kernels for structured sparsity under reduced precision. 2021b."
REFERENCES,0.26610978520286394,"Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea
Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser,
David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with per-
formers.
In International Conference on Learning Representations, 2021.
URL https:
//openreview.net/forum?id=Ua6zuk0WRH."
REFERENCES,0.26730310262529833,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL-HLT (1), pp. 4171–4186, 2019.
URL https://aclweb.org/anthology/papers/N/N19/N19-1423/."
REFERENCES,0.2684964200477327,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby.
An image is worth 16x16 words: Transformers for image recogni-
tion at scale. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=YicbFdNTTy."
REFERENCES,0.26968973747016706,"Andrew Kerr et al. Cutlass. https://github.com/NVIDIA/cutlass, 2021."
REFERENCES,0.27088305489260145,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations, 2019. URL https://
openreview.net/forum?id=rJl-b3RcF7."
REFERENCES,0.2720763723150358,"Tae Jun Ham, Yejin Lee, Seong Hoon Seo, Soosung Kim, Hyunji Choi, Sung Jun Jung, and Jae W
Lee. Elsa: Hardware-software co-design for efﬁcient, lightweight self-attention mechanism in
neural networks. In 2021 ACM/IEEE 48th Annual International Symposium on Computer Archi-
tecture (ISCA), pp. 692–705. IEEE, 2021."
REFERENCES,0.2732696897374702,"Andrew Kerr. Gtc 2020: Developing cuda kernels to push tensor cores to the absolute limit on nvidia
a100. https://developer.nvidia.com/gtc/2020/video/s21745, 2020."
REFERENCES,0.2744630071599045,"Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
Reformer: The efﬁcient transformer.
In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=rkgNKkHtvB."
REFERENCES,0.2756563245823389,"Franc¸ois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M Rush. Block pruning for faster
transformers. arXiv preprint arXiv:2109.04838, 2021."
REFERENCES,0.27684964200477324,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Ro{bert}a: A robustly optimized {bert} pre-
training approach, 2020. URL https://openreview.net/forum?id=SyxS0T4tvS."
REFERENCES,0.27804295942720764,"Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao
Xiang, and Li Zhang. Soft: Softmax-free transformer with linear complexity. arXiv preprint
arXiv:2110.11945, 2021."
REFERENCES,0.27923627684964203,"Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh,
Chong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint
arXiv:2104.08378, 2021."
REFERENCES,0.28042959427207637,"NVIDIA.
Nvidia
a100
tensor
core
gpu
architecture.
https://
images.nvidia.com/aem-dam/en-zz/Solutions/data-center/
nvidia-ampere-architecture-whitepaper.pdf, 2020."
REFERENCES,0.28162291169451076,Under review as a conference paper at ICLR 2022
REFERENCES,0.2828162291169451,"Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In
Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 1–9, 2018."
REFERENCES,0.2840095465393795,"Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse
attention with routing transformers. Transactions of the Association for Computational Linguis-
tics, 9:53–68, 2021."
REFERENCES,0.2852028639618138,"Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobile-
bert: a compact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984,
2020."
REFERENCES,0.2863961813842482,"Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In
International Conference on Machine Learning, pp. 9438–9447. PMLR, 2020a."
REFERENCES,0.28758949880668255,"Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. arXiv
preprint arXiv:2009.06732, 2020b."
REFERENCES,0.28878281622911695,"Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efﬁcient
transformers. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=qVyeW-grC2k."
REFERENCES,0.28997613365155134,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.2911694510739857,"Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efﬁcient sparse attention architecture with
cascade token and head pruning. In 2021 IEEE International Symposium on High-Performance
Computer Architecture (HPCA), pp. 97–110. IEEE, 2021."
REFERENCES,0.29236276849642007,"Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention
with linear complexity. arXiv preprint arXiv:2006.04768, 2020."
REFERENCES,0.2935560859188544,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-
ger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art
natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. As-
sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/
2020.emnlp-demos.6."
REFERENCES,0.2947494033412888,"Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and
Vikas Singh. Nystr¨omformer: A nystr¨om-based algorithm for approximating self-attention. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pp. 14138–14148,
2021."
REFERENCES,0.29594272076372313,"Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. arXiv
preprint arXiv:1910.06188, 2019."
REFERENCES,0.2971360381861575,"Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for
longer sequences. In NeurIPS, 2020."
REFERENCES,0.29832935560859186,"Lin Zehui, Pengfei Liu, Luyao Huang, Junkun Chen, Xipeng Qiu, and Xuanjing Huang. Dropat-
tention: A regularization method for fully-connected self-attention networks.
arXiv preprint
arXiv:1907.11065, 2019."
REFERENCES,0.29952267303102625,"Guangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi Su, and Xu Sun.
Ex-
plicit sparse transformer: Concentrated attention through explicit selection.
arXiv preprint
arXiv:1912.11637, 2019."
REFERENCES,0.30071599045346065,"Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hong-
sheng Li. Learning n: M ﬁne-grained structured sparse neural networks from scratch. In Interna-
tional Conference on Learning Representations, 2020."
REFERENCES,0.301909307875895,Under review as a conference paper at ICLR 2022 R 0 .. 7 8 15 .. 31 .. 24 23 16 ..
B,0.3031026252983294,"32B
32B
R 0 .. 7
8 15 .. 31 .. 24 23 16 .."
B,0.3042959427207637,"2B
2B"
B,0.3054892601431981,"0-0
0-1"
B,0.30668257756563244,"7-0
7-1"
B,0.30787589498806683,"8-0
8-1"
B,0.30906921241050117,"15-0
15-1"
B,0.31026252983293556,"16-0
16-1"
B,0.31145584725536996,"23-0
23-1"
B,0.3126491646778043,"24-0
24-1"
B,0.3138424821002387,"31-0
31-1 28 29 30"
B,0.315035799522673,"7-0
7-1
15-0
15-1"
B,0.3162291169451074,"23-0
23-1"
B,0.31742243436754175,"31-0
31-1
31 R 0 1 2"
B,0.31861575178997614,"2B
2B"
B,0.3198090692124105,"0-0
0-1"
B,0.32100238663484487,"8-0
8-1"
B,0.3221957040572792,"16-0
16-1
24-0
24-1
3 .. R 28
29 30"
B,0.3233890214797136,"7-0
15-0"
B,0.324582338902148,"7-1
15-1"
B,0.32577565632458233,"23-0
31-0"
B,0.3269689737470167,"23-1
31-1
31 0 1 2"
B,0.32816229116945106,"2B
2B"
B,0.32935560859188545,"0-0
8-0
0-1
8-1"
B,0.3305489260143198,"16-0
24-1"
B,0.3317422434367542,"16-1
24-1
3 .."
B,0.3329355608591885,"0-0
8-0
0-1
8-1
16-0 24-1 16-1 24-0
7-0
15-0 7-1
15-1 23-0 31-0 23-1 31-1
…
0               4                8               12                             112            116            120            124     127 R 0 .. 7 8 15 .. 31 .. 24 23 16 .."
B,0.3341288782816229,"32B
0
0
1 2 3"
B,0.3353221957040573,"Dense Data
Nonzeros"
B,0.33651551312649164,Metadata 0x4 0x8
XC,0.33770883054892603,0xc 0x9
XD,0.33890214797136037,0xd
XE,0.34009546539379476,0xe
XE,0.3412887828162291,"2B×4
4bit"
XE,0.3424821002386635,"0x4
0x8
0xd
0xe
Metadata:2B
LSB
MSB
0
2
3
1
4
5
7
6
9
a
8
b
e
c
d
f
Dense Data:32B
0
1
4
6
9
b
e
f
Nonzeros:16B
(a) 0-0 (b)"
XE,0.3436754176610978,"Figure 5: Prune dense data and generate nonzeros, metadata."
XE,0.3448687350835322,"A
APPENDIX"
XE,0.3460620525059666,"A.1
KERNEL DESIGN DETAILS"
XE,0.34725536992840095,"In this section, we ﬁrst demonstrate how a dense matrix is pruned and compressed under the 50%
structured sparsity on Ampere GPU in Section A.1.1. Then, we detail the design and implementation
of the SDDMM, Softmax, and SpMM kernels in Section A.1.2 and A.1.3."
XE,0.34844868735083534,"A.1.1
STRUCTURED PRUNING OF DENSE MATRIX"
XE,0.3496420047732697,"We ﬁrst illustrate how to dynamically prune a dense matrix with 50% structured sparsity. Under data
type ﬂoat, we select the larger one in every two consecutive elements. If the data type is bﬂoat16,
we select two larger ones in every four consecutive elements. We compress the pruned dense matrix
to nonzeros and metadata following CUTLASS et al (2021) as there are two beneﬁts. First, it can
be directly used by high-performance SpMM kernels in CUTLASS. Second, as we will show in
Section A.1.2, it can be dynamically generated from the SDDMM kernel with neglectable overhead."
XE,0.35083532219570407,"As shown in Figure 5, the basic tile size to prune is 32 × 64-byte, this corresponds to a 32 × 32
block under bﬂoat16 or 32 × 16 block under ﬂoat. There are four major steps: 0 Prune 50% of
each consecutive 8B data, generate nonzeros and metadata; 1 Interleave the metadata rows by 8;"
XE,0.3520286396181384,2 Switch the metadata along sub-diagonal. 3 Write metadata and nonzeros to global memory.
XE,0.3532219570405728,"In detail, 2 out of 4 2-byte data are select based on their magnitude and a unique 4-bit metadata is
assigned to each combination in 0 . The correspondence between selection pattern and metadata is
enumerated in Figure 5 (b). Notably, with ﬂoat32 data type, each 32-bit data occupies two consec-
utive 2-byte slots. Therefore, it only supports the patterns under 0x4 and 0xe. After generating the
4-bit metadata, consecutive four of them are concatenated to a 2B metadata block. Then, the rows
of metadata are interleaved by 8 in 1 following"
XE,0.35441527446300713,"dst row = ⌊row/32⌋× 32 + (row%8) × 4 + ⌊(row%32)/8⌋.
(9)"
XE,0.3556085918854415,"In 2 , the metadata blocks at upper right and lower left of each 2 × 2 grid are switched. At last, in"
XE,0.3568019093078759,"3 , the metadata produced by 2 is written into global memory following the interleaved column-"
XE,0.35799522673031026,Under review as a conference paper at ICLR 2022
XE,0.35918854415274465,"Blocked GEMM
Thread Block Tile A B C
A B M N K K Mtile Ntile"
XE,0.360381861575179,"Warp0
Warp1"
XE,0.3615751789976134,"Warp2
Warp3"
XE,0.3627684964200477,Warp Tile 16 16
XE,0.3639618138424821,float32bfloat16 32 32
XE,0.36515513126491644,Figure 6: Dense Matrix Multiplication (GEMM) Tiling Design.
XE,0.36634844868735084,"major format under stride 4-byte. This can be realized by interpreting two consecutive metadata as
an int object and then write it to DRAM in column-major. The nonzeros are simply writen to global
memory under row-major."
XE,0.36754176610978523,"A.1.2
SDDMM KERNEL DESIGN"
XE,0.36873508353221957,"Our strategy for dynamically pruning the attention score matrix has two steps. First, perform a
conventional dense GEMM. Second, prune the GEMM output with procedures described in Section
A.1.1. However, if the second step is implemented as a separate GPU kernel, we need to write the
dense attention score matrix to DRAM and read it back. This not only introduces high overhead,
but also prevents us from reducing global memory footprint. To address this issue, we implement
the pruning step as an epilogue attached to the conventional GEMM kernel: the results of the dense
GEMM are stored in the registers, the epilogue processes the results and then writes nonzeros and
metadata to global memory."
XE,0.36992840095465396,"Dense GEMM. The GEMM step is no different from conventional GEMM kernels, and all the
existing optimizations can be used. The tiling is shown in Figure 6: each thread block processes
a Mtile × Ntiles output tile, which is further partitioned to several warp tiles. Each warp tile is
composed of a grid of 16 × 16 blocks that matches the tensor core output size. In each thread block,
all the threads jointly load Mtile×Ktile and Ktile×Ntile input tiles from matrix A and B into the
shared memory. We use the new synchronize copy feature on Ampere architecture to fully utilize the
memory bandwidth and reduce register usage. To fully annihilate shared memory bank conﬂict, we
use the XOR layout. Once the load is completed, the warps fetch their source operands from shared
memory with ldmatrix and perform a (16 × 32B) · (32B × 16) warp matrix multiply accumulate
(wmma) with tensor core. Notably, ﬂoat data will be converted to tensorﬂoat-32 before wmma. To
reduce accumulation error, we accumulate the partial sum as ﬂoat regardless of the source operand
data type. Besides, software 2-stage pipeline is used to overlap memory access and computation
with double buffering (et al, 2021). Although deeper software pipeline can be built on Ampere, we
ﬁnd 2 stages is enough as the inner-product dimension K is usually very small (e.g. 64). More
detailed explanation of the above techniques can be found in this GTC 2020 talk (Kerr, 2020)."
XE,0.3711217183770883,"Pruning the GEMM result. In the pruning step, the warp tile is partitioned to a grid of 32 × 64B
blocks that are processed by the warp one at a time."
XE,0.3723150357995227,"Under data type ﬂoat, the register layout of the 32×16 block is illustrated in Figure 7 (a). It consists
of two 16 × 16 wmma blocks, so each thread has sixteen 32-bit registers to hold the results. The
registers are annotated with “Tthread id{register id}”. As the adjacent two data are held by the
same thread, we can simply compare them and the larger one is retained."
XE,0.373508353221957,"Under data type bﬂoat16, we need to select 2 larger ones from adjacent 4 entries. However, under
the naive mapping shown in Figure 8 (a), these 4 entries are held by 2 thread. Therefore, we need
additional warp shufﬂe to ﬁrst pass these 4 entries to the same thread, then compare them and obtain
the 2 larger ones. This will introduce additional overhead. To solve this problem, we propose
to interleave the columns when loading matrix B to shared memory by simply manipulating the
pointer to the global memory at the beginning. The resulted mapping to the registers is shown in
Figure 8 (b) which is equivalent with Figure 7 (a) bﬂoat16. After the interleaving, consecutive four"
XE,0.3747016706443914,"Under review as a conference paper at ICLR 2022 R\C 0
1 2
.. 7
8 9 10 .. 15 T0{0} .. 26 .. 31 25
24 23 18 16
17 T4{0} T8{0}"
XE,0.37589498806682575,T28{0} T0{1} T4{1} T8{1}
XE,0.37708830548926014,T28{1} T1{2} T5{2} T9{2}
XE,0.37828162291169454,T29{2} T1{3} T5{3} T9{3}
XE,0.3794749403341289,T29{3} T2{4} T6{4}
XE,0.38066825775656327,T10{4}
XE,0.3818615751789976,T30{4} T3{6} T7{6}
XE,0.383054892601432,T11{6}
XE,0.38424821002386633,T31{6} T2{5} T6{5}
XE,0.3854415274463007,T10{5}
XE,0.38663484486873506,T30{5} T3{7} T7{7}
XE,0.38782816229116945,T11{7}
XE,0.38902147971360385,"T31{7} 11
10 8 9"
XE,0.3902147971360382,"T8{0} 
T9{2}"
XE,0.3914081145584726,"T8{1} 
T9{3}"
XE,0.3926014319809069,"T10{4} 
T11{6}"
XE,0.3937947494033413,"T10{5} 
T11{7} R\C 0
1 2 3 6 4 5 7 T0{0} 29 31 28 30 T1{2}"
XE,0.39498806682577564,"T4{0} 
T5{2}"
XE,0.39618138424821003,"T28{0} 
T29{2}"
XE,0.39737470167064437,"T0{1} 
T1{3}"
XE,0.39856801909307876,"T4{1} 
T5{3}"
XE,0.3997613365155131,"T28{1} 
T29{3}"
XE,0.4009546539379475,"T2{4} 
T3{6}"
XE,0.4021479713603819,"T6{4} 
T7{6}"
XE,0.4033412887828162,"T30{4} 
T31{6}"
XE,0.4045346062052506,T30{5}
XE,0.40572792362768495,"T2{5} 
T3{7}"
XE,0.40692124105011934,"T6{5} 
T7{7}"
XE,0.4081145584725537,"T31{7} .. 11
10 8 9"
XE,0.40930787589498807,"T8{0,1}"
XE,0.4105011933174224,"T9{2,3}"
XE,0.4116945107398568,"T10{4,5}"
XE,0.4128878281622912,"T11{6,7} R\C 0
1 2 3 6 4 5 7"
XE,0.41408114558472553,"T0{0,1} 29 31 28 30"
XE,0.4152744630071599,"T5{2,3}"
XE,0.41646778042959426,"T28{0,1}"
XE,0.41766109785202865,"T1{2,3}"
XE,0.418854415274463,"T4{0,1}"
XE,0.4200477326968974,"T29{2,3}"
XE,0.4212410501193317,"T2{4,5}"
XE,0.4224343675417661,"T6{4,5}"
XE,0.4236276849642005,T30{4.5}
XE,0.42482100238663484,"T31{6,7}"
XE,0.42601431980906923,"T3{6,7}"
XE,0.42720763723150357,"T7{6,7} .. R\C 11
10 8 9"
XE,0.42840095465393796,"T8{0,1}"
XE,0.4295942720763723,"T9{0,1}"
XE,0.4307875894988067,"T10{0,1}"
XE,0.431980906921241,"T11{0,1} 0
1 2 3 6 4 5 7"
XE,0.4331742243436754,"T0{0,1} 29 31 28 30"
XE,0.4343675417661098,"T5{0,1}"
XE,0.43556085918854415,"T28{0,1}"
XE,0.43675417661097854,"T1{0,1}"
XE,0.4379474940334129,"T4{0,1}"
XE,0.43914081145584727,"T29{0,1}"
XE,0.4403341288782816,"T2{0,1}"
XE,0.441527446300716,"T6{0,1}"
XE,0.44272076372315033,"T30{0,1}"
XE,0.4439140811455847,"T31{0,1}"
XE,0.4451073985680191,"T3{0,1}"
XE,0.44630071599045346,"T7{0,1} .. R\C 0
1 2
.. 7
8 9 10 .. 15"
XE,0.44749403341288785,"T0{0}[0]     T1{0}[1]    T2{0}[2]     T3{0}[3] .. 26 .. 31 25
24 23 18 16
17"
XE,0.4486873508353222,T4{0}[0]     T5{0}[1]    T6{0}[2]     T7{0}[3]
XE,0.4498806682577566,T28{0}[0]  T29{0}[1]  T30{0}[2]  T31{0}[3]
XE,0.4510739856801909,T0{2}[0]     T1{2}[1]    T2{2}[2]     T3{2}[3]
XE,0.4522673031026253,T4{2}[0]     T5{2}[1]    T6{2}[2]     T7{2}[3]
XE,0.45346062052505964,T28{2}[0]  T29{2}[1]  T30{2}[2]  T31{2}[3]
XE,0.45465393794749404,T0{1}[0]     T1{1}[1]    T2{1}[2]     T3{1}[3]
XE,0.45584725536992843,T4{1}[0]     T5{1}[1]    T6{1}[2]     T7{1}[3]
XE,0.45704057279236276,T28{1}[0]  T29{1}[1]  T30{1}[2]  T31{1}[3]
XE,0.45823389021479716,T0{3}[0]     T1{3}[1]    T2{3}[2]     T3{3}[3]
XE,0.4594272076372315,T4{3}[0]     T5{3}[1]    T6{3}[2]     T7{3}[3]
XE,0.4606205250596659,T28{3}[0]  T29{3}[1]  T30{3}[2]  T31{3}[3]
XE,0.4618138424821002,T0{4}[0]     T1{4}[1]    T2{4}[2]     T3{4}[3]
XE,0.4630071599045346,T4{4}[0]     T5{4}[1]    T6{4}[2]     T7{4}[3]
XE,0.46420047732696895,T28{4}[0]  T29{4}[1]  T30{4}[2]  T31{4}[3]
XE,0.46539379474940334,T0{6}[0]     T1{6}[1]    T2{6}[2]     T3{6}[3]
XE,0.4665871121718377,T4{6}[0]     T5{6}[1]    T6{6}[2]     T7{6}[3]
XE,0.4677804295942721,T28{6}[0]  T29{6}[1]  T30{6}[2]  T31{6}[3]
XE,0.46897374701670647,T0{5}[0]     T1{5}[1]    T2{5}[2]     T3{5}[3]
XE,0.4701670644391408,T4{5}[0]     T5{5}[1]    T6{5}[2]     T7{5}[3]
XE,0.4713603818615752,T28{5}[0]  T29{5}[1]  T30{5}[2]  T31{5}[3]
XE,0.47255369928400953,T0{7}[0]     T1{7}[1]    T2{7}[2]     T3{7}[3]
XE,0.4737470167064439,T4{7}[0]     T5{7}[1]    T6{7}[2]     T7{7}[3]
XE,0.47494033412887826,T28{7}[0]  T29{7}[1]  T30{7}[2]  T31{7}[3] R\C 0
XE,0.47613365155131265,"1
2
.. 7
8 9
10 .. 15"
XE,0.477326968973747,"T0{0,1,4,5}  T1{0,1,4,5}  T2{0,1,4,5}   T3{0,1,4,5}"
XE,0.4785202863961814,"T4{0,1,4,5}  T5{0,1,4,5}  T6{0,1,4,5}   T7{0,1,4,5}"
XE,0.4797136038186158,"T28{0,1,4,5} T29{0,1,4,5} T30{0,1,4,5} T31{0,1,4,5} .. 26 .. 31 25
24 23 18 16
17"
XE,0.4809069212410501,"T0{2,3,6,7}  T1{2,3,6,7}  T2{2,3,6,7}   T3{2,3,6,7}"
XE,0.4821002386634845,"T4{2,3,6,7}  T5{2,3,6,7}  T6{2,3,6,7}   T7{2,3,6,7}"
XE,0.48329355608591884,"T28{2,3,6,7} T29{2,3,6,7} T30{2,3,6,7} T31{2,3,6,7}"
XE,0.48448687350835323,"T0{0,1,4,5}  T1{0,1,4,5}  T2{0,1,4,5}   T3{0,1,4,5}"
XE,0.48568019093078757,"T4{0,1,4,5}  T5{0,1,4,5}  T6{0,1,4,5}   T7{0,1,4,5}"
XE,0.48687350835322196,"T28{0,1,4,5} T29{0,1,4,5} T30{0,1,4,5} T31{0,1,4,5}"
XE,0.4880668257756563,"T0{2,3,6,7}  T1{2,3,6,7}  T2{2,3,6,7}   T3{2,3,6,7}"
XE,0.4892601431980907,"T4{2,3,6,7}  T5{2,3,6,7}  T6{2,3,6,7}   T7{2,3,6,7}"
XE,0.4904534606205251,"T28{2,3,6,7} T29{2,3,6,7} T30{2,3,6,7} T31{2,3,6,7}"
XE,0.4916467780429594,bfloat16
XE,0.4928400954653938,0-3          4-7        8-11        12-15        16-19    20-23      24-27      28-31
XE,0.49403341288782815,"T0{0,1,4,5}  T1{0,1,4,5}  T2{0,1,4,5}   T3{0,1,4,5}"
XE,0.49522673031026254,"T4{0,1,4,5}  T5{0,1,4,5}  T6{0,1,4,5}   T7{0,1,4,5}"
XE,0.4964200477326969,"T28{0,1,4,5} T29{0,1,4,5} T30{0,1,4,5} T31{0,1,4,5}"
XE,0.49761336515513127,"T0{2,3,6,7}  T1{2,3,6,7}  T2{2,3,6,7}   T3{2,3,6,7}"
XE,0.4988066825775656,"T4{2,3,6,7}  T5{2,3,6,7}  T6{2,3,6,7}   T7{2,3,6,7}"
XE,0.5,"T28{2,3,6,7} T29{2,3,6,7} T30{2,3,6,7} T31{2,3,6,7}"
XE,0.5011933174224343,"T0{0,1,4,5}  T1{0,1,4,5}  T2{0,1,4,5}   T3{0,1,4,5}"
XE,0.5023866348448688,"T4{0,1,4,5}  T5{0,1,4,5}  T6{0,1,4,5}   T7{0,1,4,5}"
XE,0.5035799522673031,"T28{0,1,4,5} T29{0,1,4,5} T30{0,1,4,5} T31{0,1,4,5}"
XE,0.5047732696897375,"T0{2,3,6,7}  T1{2,3,6,7}  T2{2,3,6,7}   T3{2,3,6,7}"
XE,0.5059665871121718,"T4{2,3,6,7}  T5{2,3,6,7}  T6{2,3,6,7}   T7{2,3,6,7}"
XE,0.5071599045346062,"T28{2,3,6,7} T29{2,3,6,7} T30{2,3,6,7} T31{2,3,6,7}"
XE,0.5083532219570406,"R\C 0   1  2   3   4    5 
6  7 
8    9   10  11  12  13   14  15 0
1 2
.. 7
8 9
10 .. 15"
XE,0.5095465393794749,"T0{0,1}       T1{0,1}       T2{0,1}       T3{0,1}"
XE,0.5107398568019093,"T4{0,1}       T5{0,1}       T6{0,1}       T7{0,1}"
XE,0.5119331742243437,"T28{0,1}    T29{0,1}    T30{0,1}     T31{0,1}"
XE,0.513126491646778,"T0{4,5}       T1{4,5}       T2{4,5}       T3{4,5}"
XE,0.5143198090692124,"T4{4,5}       T5{4,5}       T6{4,5}       T7{4,5}"
XE,0.5155131264916468,"T28{4,5}    T29{4,5}    T30{4,5}     T31{4,5}"
XE,0.5167064439140812,"T0{2,3}       T1{2,3}       T2{2,3}       T3{2,3}"
XE,0.5178997613365155,"T4{2,3}       T5{2,3}       T6{2,3}       T7{2,3}"
XE,0.5190930787589498,"T28{2,3}    T29{2,3}    T30{2,3}     T31{2,3}"
XE,0.5202863961813843,"T0{6,7}       T1{6,7}       T2{6,7}       T3{6,7}"
XE,0.5214797136038186,"T4{6,7}       T5{6,7}       T6{6,7}       T7{6,7}"
XE,0.522673031026253,"T28{6,7}    T29{6,7}    T30{6,7}     T31{6,7} .. 26 .. 31"
XE,0.5238663484486874,"T0{0,1}       T1{0,1}       T2{0,1}       T3{0,1}"
XE,0.5250596658711217,"T4{0,1}       T5{0,1}       T6{0,1}       T7{0,1}"
XE,0.5262529832935561,"T28{0,1}    T29{0,1}    T30{0,1}     T31{0,1}"
XE,0.5274463007159904,"T0{4,5}       T1{4,5}       T2{4,5}       T3{4,5}"
XE,0.5286396181384249,"T4{4,5}       T5{4,5}       T6{4,5}       T7{4,5}"
XE,0.5298329355608592,"T28{4,5}    T29{4,5}    T30{4,5}     T31{4,5}"
XE,0.5310262529832935,"T0{2,3}       T1{2,3}       T2{2,3}       T3{2,3}"
XE,0.5322195704057279,"T4{2,3}       T5{2,3}       T6{2,3}       T7{2,3}"
XE,0.5334128878281623,"T28{2,3}    T29{2,3}    T30{2,3}     T31{2,3}"
XE,0.5346062052505967,"T0{6,7}       T1{6,7}       T2{6,7}       T3{6,7}"
XE,0.535799522673031,"T4{6,7}       T5{6,7}       T6{6,7}       T7{6,7}"
XE,0.5369928400954654,"T28{6,7}    T29{6,7}    T30{6,7}     T31{6,7} 25
24 23 18 16
17"
XE,0.5381861575178998,float32
XE,0.5393794749403341,"(b)
(c)
(d)                       (e)                  (f) (a)"
XE,0.5405727923627685,"Figure 7: Mapping between the registers and data, metadata."
XE,0.5417661097852029,"data are naturally held by the same thread, and we select 2 larger ones from them. To reduce branch
divergence, the selection is done by comparing the sum of any two data."
XE,0.5429594272076372,"Generate Metadata and Nonzeros. For both ﬂoat and bﬂoat16 data type, each comparison pro-
duces a 4-bit metadata. Next, following the procedures described in Section A.1.1, we need to
concatenate consecutive four metadata to a 16-bit metadata block. This is done in two steps. First,
put the 4-bit metadata to the correct position of a int16 register with bit shift. Second, share these
int16 registers cross threads with warp shufﬂe, and concatenate them with bitwise OR. As con-
secutive four metadata are held by thread 4t to 4t+3, we put the 4-bit metadata of thread 4t+k to
[k×4:k×4 + 3] bits in the int16 object in the ﬁrst step. The detailed layout is shown in Figure 8 (b),
where we denote each 4-bit metadata as “Tthread id{register id}[bit id]”. The result of the second
step is shown in Figure 7 (c). Figure 7 (d) and (e) illustrate the result after 1 and 2 in Figure 5.
Notably, these two step only change the logic mapping of the metadata and the register allocation is
not affected. So no code is required for these two steps. At last, we need to write the metadata and
nonzeros to global memory following 3 in Figure 5. As shown in Figure 7 (e), each row is held
by consecutive two int16 registers of the same thread, so we can simply reinterpret it as an =int32
object and write the metadata to global memory in column major. For the nonzeros, we simply
coalesce them in the shared memory and then write to global memory in row-major."
XE,0.5441527446300716,"Batched Kernel. The self-attention layer in transformer usually has multiple independent attention
heads. Instead of launching one CUDA kernel for each attention head, using a batched kernel that"
XE,0.545346062052506,Under review as a conference paper at ICLR 2022
XE,0.5465393794749404,"0   1  2   3   4    5 
6  7 
8    9   10  11  12  13   14  15   16 … 31
R\C 0
1 2
.. 7
8 31 .."
XE,0.5477326968973747,"T0{0,1}       T1{0,1}       T2{0,1}       T3{0,1}"
XE,0.548926014319809,"T4{0,1}       T5{0,1}       T6{0,1}       T7{0,1}"
XE,0.5501193317422435,"T28{0,1}    T29{0,1}    T30{0,1}     T31{0,1}"
XE,0.5513126491646778,"T0{4,5}       T1{4,5}       T2{4,5}       T3{4,5}"
XE,0.5525059665871122,"T4{4,5}       T5{4,5}       T6{4,5}       T7{4,5}"
XE,0.5536992840095465,"T28{4,5}    T29{4,5}    T30{4,5}     T31{4,5}"
XE,0.5548926014319809,"0   1  4
5
8
9
12 13 2
3
6    7  10  11   14  15      16 … 31
R\C 0
1 2
.. 7 8 31 .."
XE,0.5560859188544153,"T0{0,1}       T1{0,1}       T2{0,1}       T3{0,1}"
XE,0.5572792362768496,"T4{0,1}       T5{0,1}       T6{0,1}       T7{0,1}"
XE,0.5584725536992841,"T28{0,1}    T29{0,1}    T30{0,1}     T31{0,1}"
XE,0.5596658711217184,"T0{4,5}       T1{4,5}       T2{4,5}       T3{4,5}"
XE,0.5608591885441527,"T4{4,5}       T5{4,5}       T6{4,5}       T7{4,5}"
XE,0.5620525059665871,"T28{4,5}    T29{4,5}    T30{4,5}     T31{4,5}"
XE,0.5632458233890215,"𝒅𝒔𝒕_𝒄𝒐𝒍=
𝒄𝒐𝒍"
XE,0.5644391408114559,"𝟐
𝒎𝒐𝒅 𝟐×𝟐+ 𝒄𝒐𝒍 𝒎𝒐𝒅 𝟐+
𝒄𝒐𝒍"
XE,0.5656324582338902,"𝟖
𝒎𝒐𝒅 𝟐×𝟐+ 𝒄𝒐𝒍 𝟏𝟔×𝟏𝟔 (a) (b)"
XE,0.5668257756563246,"Figure 8: Interleave the columns for matrix B to reduce cross-lane data sharing during pruning for
bﬂoat16."
XE,0.568019093078759,"processes all the heads can better utilize the GPU resources and reduce kernel launching overhead.
We support the batched computation by using the blockIdx.z to index the heads in the batch and
update the pointers to the input and output based on the index."
XE,0.5692124105011933,"Blocked-ELL Sparsity. Under long sequence length, higher sparsity is desired to reduce compu-
tation cost and memory footprint. Our kernel support hybrid blocked-ELL sparsity Zaheer et al.
(2020) and 50% structured sparsity. To support this feature, we set the block size in blocked-ELL to
the thread block tile size of the GEMM. Therefore, we can simply skip those pruned blocks during
the execution."
XE,0.5704057279236276,"A.1.3
SOFTMAX AND SPMM KERNEL"
XE,0.5715990453460621,"In this section, we detail the implementation of the softmax and SpMM kernels."
XE,0.5727923627684964,"Softmax Kernel. To improve numerical stability, the softmax on GPU is computed with"
XE,0.5739856801909308,"softmax(x)i =
exi−max(x)
P"
XE,0.5751789976133651,"j exj−max(x) .
(10)"
XE,0.5763723150357996,"Therefore, each element in x has to be loaded for three times. 1) compute c = max(x); 2) compute
s = P"
XE,0.5775656324582339,"j exj−c; 3) compute exi−c/s. Instead of loading xi from global memory in each time, we
cache it in the register when the whole row ﬁts in the register ﬁle capacity. Besides, the ordinary
softmax kernel in libraries like PyTorch can also be used."
XE,0.5787589498806682,"SpMM Kernel. As we encode the nonzeros and metadata following the CUTLASSet al (2021), we
directly construct the SpMM kernels from the CUTLASS APIs. To support the hybrid blocked-ELL
and structured 50% sparsity, we modify the PredictedTileAccessIterator class in CUTLASS to skip
the tiles masked out by the blocked-ELL sparsity."
XE,0.5799522673031027,"A.2
PROOF OF PROPOSITION 4.1"
XE,0.581145584725537,"Proof. Under the assumption that the entries in QKT /
√"
XE,0.5823389021479713,"d follow i.i.d. N(µ, σ), we denote xi,j =
eµ+σzi,j, where z ∼i.i.d. N(0, 1). Then we can substitute it into the deﬁnition of the softmax and"
XE,0.5835322195704057,Under review as a conference paper at ICLR 2022
XE,0.5847255369928401,"get
Au,v =
xu,v
Pn
i=1 xu,i
.
(11)"
XE,0.5859188544152745,We substitute the above equation into the deﬁnition of Lp-Quality and get
XE,0.5871121718377088,"Qp = 1 n n
X j=1"
XE,0.5883054892601431,"Pn
i=1 (m ⊙A)p
j,i
Pn
i=1 Ap
j,i
= 1 n n
X j=1"
"N
PN",0.5894988066825776,"1
n
Pn
i=1 mj,ixp
j,i
1
n
Pn
i=1 xp
j,i
(12)"
"N
PN",0.5906921241050119,"With n →∞, the denominator can be approximated with"
N,0.5918854415274463,"1
n n
X"
N,0.5930787589498807,"i=1
xp
j,i ≈
Z ∞ −∞"
N,0.594272076372315,epµ+pσz √
N,0.5954653937947494,"2π
exp

−z2 2"
N,0.5966587112171837,"
dz = exp

pµ + p2σ2 2"
N,0.5978520286396182,"
(13)"
N,0.5990453460620525,"Top-K Sparsity. When the sequence is long enough such that we have n →∞, the numerator can
be approximated with"
N,0.6002386634844868,"1
n n
X"
N,0.6014319809069213,"i=1
mj,ixp
j,i ≈
Z ∞ √"
N,0.6026252983293556,2erfinv(1−2s)
N,0.60381861575179,epµ+pσz √
N,0.6050119331742243,"2π
exp

−z2 2 
dz"
N,0.6062052505966588,"= exp

pµ + p2σ2 2"
N,0.6073985680190931," 1 + erf

pσ
√"
N,0.6085918854415274,"2 −erfinv(1 −2s)
 2 (14)"
N,0.6097852028639618,"Therefore, the Lp-Quality of Top-K sparsity is"
N,0.6109785202863962,"Qp
topk ≈
1 + erf

pσ
√"
N,0.6121718377088305,"2 −erfinv(1 −2s)
"
N,0.6133651551312649,"2
.
(15)"
N,0.6145584725536993,"Fixed Sparsity. Without any assumption on the distribution of important edges in A, applying a
ﬁxed pattern is equivalent with uniformly sampling with probability s and we have"
N,0.6157517899761337,"1
n n
X"
N,0.616945107398568,"i=1
mj,ixp
j,i ≈exp

pµ + p2σ2 2"
N,0.6181384248210023,"
s.
(16)"
N,0.6193317422434368,"Therefore, the Lp-Quality of the ﬁxed sparsity is
Qp
fix ≈s.
(17)"
N,0.6205250596658711,"2-to-1 Sparsity: This sparsity pattern select the larger one in every two elements. We denote adja-
cent two elements with
X = eµ+σZ1, Y = eµ+σZ2; Z1, Z2 ∼N(0, 1),
(18)
Z1 and Z2 are independent. Then we have"
N,0.6217183770883055,"1
n n
X"
N,0.6229116945107399,"i=1
mj,ixp
j,i ≈1"
N,0.6241050119331742,"2E [max(Xp, Y p)] = 1
2 ZZ"
N,0.6252983293556086,"z1≥z2
epµ+pσz1 1"
N,0.6264916467780429,"2π exp

−z2
1+z2
2
2"
N,0.6276849642004774,"
dz1dz2+
ZZ"
N,0.6288782816229117,"z1<z2
epµ+pσz2 1"
N,0.630071599045346,"2π exp

−z2
1+z2
2
2"
N,0.6312649164677804,"
dz1dz2 "
N,0.6324582338902148,"(19)
We denote
x = z1 −z2
√"
N,0.6336515513126492,"2
,
y = z1 + z2
√"
N,0.6348448687350835,"2
,
(20)"
N,0.636038186157518,"then we have
ZZ"
N,0.6372315035799523,"z1≥z2
epµ+pσz1 1"
N,0.6384248210023866,"2π exp

−z2
1 + z2
2
2"
N,0.639618138424821,"
dz1dz2 =
Z ∞ −∞ Z ∞"
N,0.6408114558472554,"0
epµ+ pσ
√"
N,0.6420047732696897,2 (x+y) 1
N,0.6431980906921241,"2π exp

−x2 + y2 2"
N,0.6443914081145584,"
dxdy =
Z ∞ −∞ Z ∞"
N,0.6455847255369929,"0
epµ+ p2σ2"
N,0.6467780429594272,"2
1
2π exp "" −1 2"
N,0.6479713603818615,"
x −pσ
√ 2 2
−1 2"
N,0.649164677804296,"
y −pσ
√ 2 2# dxdy"
N,0.6503579952267303,"= epµ+ p2σ2 2
Z ∞ 0 1
√"
N,0.6515513126491647,"2π exp "" −1 2"
N,0.652744630071599,"
x −σ
√ 2 2#"
N,0.6539379474940334,"dx = epµ+ p2σ2 2
2"
N,0.6551312649164678,"h
1 + erf
pσ 2 i
. (21)"
N,0.6563245823389021,Under review as a conference paper at ICLR 2022
N,0.6575178997613366,"With the conclusion above, we have"
N,0.6587112171837709,"1
n n
X"
N,0.6599045346062052,"i=1
mj,ixp
j,i = exp

pµ + p2σ2 2"
N,0.6610978520286396," 1 + erf
  pσ 2
"
N,0.662291169451074,"2
.
(22)"
N,0.6634844868735084,The LP -Quality of 1:2 sparsity can be computed with
N,0.6646778042959427,"Qp
1:2 = 1 + erf
  pσ 2
"
N,0.665871121718377,"2
.
(23)"
N,0.6670644391408115,"2:4 Sparsity: This sparsity pattern select the largest two elements in consecutive four elements.
While it is more challenging to ﬁnd an explicit expression for Qp
4−to−2, a trivial lower bound can be
found with"
N,0.6682577565632458,"1
n n
X"
N,0.6694510739856802,"i=1
mj,ixp
j,i ≈1"
N,0.6706443914081146,"4E [max(Xp + Y p, Xp + U p, Xp + V p, Y p + U p, Y p + V p, U p + V p)] ≥1"
N,0.6718377088305489,"4 (E[max(Xp, Y p)] + E[max(U p, V p)]) = 1"
N,0.6730310262529833,"2E[max(Xp, Y p)], (24)"
N,0.6742243436754176,where we have
N,0.6754176610978521,"X = eµ+σZ1, Y = eµ+σZ2, U = eµ+σZ3, V = eµ+σZ4; Z1, Z2, Z3, Z4 ∼N(0, 1),
(25)"
N,0.6766109785202864,"Z1, ..., Z4 are independent. Therefore, the lower-bound of Qp
2:4 is"
N,0.6778042959427207,"Qp
2:4 ≥1 + erf
  pσ 2
"
N,0.6789976133651552,"2
.
(26)"
N,0.6801909307875895,"A.3
PROOF OF PROPOSITION 4.2"
N,0.6813842482100239,"Proof. First of all, thanks to the Tensor Core in latest GPUs, the latency of matrix multiplication op-
erations, both sparse and dense, are bounded by the memory access. Therefore, instead of counting
the number of MACs (multiply-accumulate operations), the amount of memory access is a better
metric to estimate the latency. 𝑨 𝑽 n d n n T T r r n d n k 1 T r r … 𝑨"
N,0.6825775656324582,"𝑽
(A)
(B)"
N,0.6837708830548926,Figure 9: Tiling Matrix-Matrix Multiply
N,0.684964200477327,"Tiling is a basic optimization applied to optimize matrix matrix multiply on GPU. As shown in
Figure 9 (A), the original n × n output is partitioned to independent blocks with size T × T. When
computing each block, operands with size T × r and r × T are loaded from A and V T to the fast
memory, respectively. Then, these two operand are multiplied and accumulated to the partial sum
stored in the registers. After applying the top-k, as shown in Figure 9 (B), the k elements in each row
of A correspond to different rows in A. Therefore, we can only partition the output to independent"
N,0.6861575178997613,Under review as a conference paper at ICLR 2022 0 0.5 1 1.5 2 2.5 3
N,0.6873508353221957,"0
0.1
0.2
0.3
0.4
0.5
0.6
0.7"
N,0.6885441527446301,Speedup Over Full Attention
N,0.6897374701670644,Density (s) Top-K
N,0.6909307875894988,Top-K Theory Fixed
N,0.6921241050119332,Fixed Theory 1:2
N,0.6933174224343676,1:2 Theory
N,0.6945107398568019,Figure 10: Theoretical and actual speedup achieved by different sparse patterns on A100 GPU.
N,0.6957040572792362,"vectors with size 1 × T. During the computation, operands with size 1 × r and r × T are loaded
from A and V T to the fast memory, respectively. Then, the loaded operands are multiplied and
accumulated to the partial sum stored in the registers."
N,0.6968973747016707,"With the tiling strategy mentioned above, we can summarize the amount of memory access in dif-
ferent attentions in the table below."
N,0.698090692124105,"Table 5: Amount of Memory Access in Different Operations in Attention. s = k/n: density of the
sparse attention; T: tiling size."
N,0.6992840095465394,"QKT
Softmax
AV
Full Attention
n2   2d"
N,0.7004773269689738,"T + 1
"
N,0.7016706443914081,"2n2
nd
  2n"
N,0.7028639618138425,"T + 1
"
N,0.7040572792362768,"Explicit Top-k Attention
n2   2d"
N,0.7052505966587113,"T + 1
"
N,0.7064439140811456,"2n2s
nd
 
sn + sn"
N,0.7076372315035799,"T + 1
"
N,0.7088305489260143,"For QKT , as we need to compute all of it before getting the top-k elements, it is a dense matrix
matrix multiplication for both full and explicit top-k attention. The Softmax needs to read the n × n
QKT in, normalizes it, and write the result A back. As the intermediate values can be stored in
registers, we only need to count reading QKT in and writing A out. Therefore, its memory access
is 2n2 for full attention and 2n2s for explicit top-k attention. For AV in full attention, the output
size is nd. As each output element is generated from the inner product between two vectors with
length n, the total data read equals nd × 2n. However, with the tiling in Figure 9 (A), each operand
is reused for T times. Therefore, the total memory access for AV in full attention is nd( 2n"
N,0.7100238663484487,"T + 1).
For AV in explicit top-k attention, as shown in Figure 9 (B), each left-hand-side data is reused for
T times while each right-hand-side data is used only for once. Therefore, its memory access equals
to nd( sn"
N,0.711217183770883,T + sn + 1).
N,0.7124105011933174,The theoretical speedup can be computed with
N,0.7136038186157518,"Speedup ≤
n2   2d"
N,0.7147971360381862,"T + 1

+ 2n2 + nd
  2n"
N,0.7159904534606205,"T + 1
"
N,0.7171837708830548,n2   2d
N,0.7183770883054893,"T + 1

+ 2n2s + nd
 
sn + sn"
N,0.7195704057279236,"T + 1

n≫d
≈
4d + 3T
2d + T + (d + 2T + dT)s
(27)"
N,0.720763723150358,"A.4
QUALITY OF THE LOTTERY TICKETS UNDER THE SAME EFFICIENCY"
N,0.7219570405727923,"In this section, we provide more empirical evidences to support our conclusions. We ﬁrst compare
the theoretical speedup of different sparsity predicted in equation 4, equation 5, and equation 6 and
the actual speedup measured on A100 GPU in Figure 10."
N,0.7231503579952268,Under review as a conference paper at ICLR 2022 0.0 0.5 1.0 p=1
N,0.7243436754176611,"Top-K
Fix
1:2
2:4 0.0 0.5 1.0 p=2 0.0 0.5 1.0 p=3"
N,0.7255369928400954,"0.02
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.63 0.0 0.5 1.0 p=7"
N,0.7267303102625299,"0.5
0.5
0.0
0.2
0.4
0.6
0.8
1.0
Density (s) 0.0 0.2 0.4 0.6 0.8 1.0 p"
N,0.7279236276849642,"Figure 11: Qp under different density s and sparsity strategies. Box plot: Empirical results from
BERT-large on SQuAD v1.1; Solid line: Theoretical results from Proposition 4.1."
N,0.7291169451073986,"First of all, the Top-K sparsity is well bounded by the theoretical value, and our method achieves
better speedup than the Top-K sparsity when the density s > 0.02. This is because gathering top-k
elements in each row of the attention weight matrix and sorting them to compressed row format
introduce huge overhead."
N,0.7303102625298329,"Second, the speedup achieved by the ﬁxed sparsity is well predicted by our theoretical value. The
speedup it achieved is lower than ours when density s ≥0.63, which accords with our theoretical
conclusion. Notably, the speedup of ﬁxed sparsity we used here is simply truncate the number of
columns of the attention weight matrix based on the density. The actual speedup will be even lower
when more ﬁne-grained pattern is involved."
N,0.7315035799522673,"Our method delivers speedup a little bit higher than the theoretical value. This is because the softmax
kernel has different implementations under different sequence length. When the sequence length is
moderate, as mentioned in Appendix A.3, the data loaded from the attention score matrix can be
explicitly cached in fast memory like registers or shared memory for reuse. When sequence length
too long for the fast memory to cache, it has to be implicitly reused through lower-level cache or
even global memory. The second implementation is slower than the ﬁrst one as lower-level cache
has longer access latency and lower throughput. As our method reduces the sequence length by half,
it can use the implementation for moderate sequence length while the full attention is handled by
the long sequence version."
N,0.7326968973747017,"In Figure 11, we compute the theoretical value (solid line) and empirical value (box plot) of Qp over
attention matrix A in BERT-Large on SQuAD v1.1. As p is a task-dependent value that is hard to
obtain, we instead sweep through several typical values."
N,0.733890214797136,"Compared with the top-k sparsity, when p < 7, our 1:2 and 2:4 sparsity always achieve better
performance than the top-k sparsity when s < 0.05. Besides, when p = 7, the Qp
1:2 and Qp
2:4 are"
N,0.7350835322195705,Under review as a conference paper at ICLR 2022
N,0.7362768496420048,"0.70
0.75
0.80
0.85
0.90
0.95
1.00
Qp = 6.5 90.5 91.0 91.5 92.0 92.5 93.0"
N,0.7374701670644391,F1 Score on BERT-large SQuAD v1.1 (a)
N,0.7386634844868735,"Top-k Sparsity
Fixed Sparsity
1:2 Sparsity
2:4 Sparsity"
N,0.7398568019093079,"0.65
0.70
0.75
0.80
0.85
0.90
0.95
||A
(m
A)||2
F
||A||2
F 90.5 91.0 91.5 92.0 92.5 93.0 (b)"
N,0.7410501193317423,"Top-k Sparsity
Fixed Sparsity
1:2 Sparsity
2:4 Sparsity"
N,0.7422434367541766,"Figure 12: Qp under different density s and sparsity strategies. Box plot: Empirical results from
BERT-large on SQuAD v1.1; Solid line: Theoretical results from Proposition 4.1."
N,0.7434367541766109,"very close to 1. These observations accord with our conclusion that our 1:2 and 2:4 sparsity can
obtain tickets with better quality than Top-K sparsity at the same efﬁciency."
N,0.7446300715990454,"Compared with the ﬁxed sparsity, our Qp
1:2 and Qp
2:4 are also similar or better than Qp
fix across
different ps. This supports our conclusion that our method achieves better performance than the
ﬁxed sparsity patterns under the same efﬁciency."
N,0.7458233890214797,"To show that our Qp is a good metric to compare the performance of different sparse patterns, we
plot the Qp and F1 score on BERT-large SQuAD v1.1 in Figure 12. As we mentioned before,
p is a task-speciﬁc value used to model tasks with different degree of dependency on the largest
few elements. In order to identify the p for our target task, we tune the value of p until the data
points from Top-K sparsity and Fixed sparsity form a monotonically increasing line. We found that
p = 6.5 is a good choice. This large p accords our observation that the Top-K sparsity works well
even under 5.4% density. After anchored the p, we put the data points from 1:2 and 2:4 sparsity
into the plot and verify if the line is still monotonically increasing. Figure 12 shows that the data
points from our 1:2/2:4 sparsity perfectly ﬁlls in the monotonically increasing line. Oppositely, The
traditional F-norm based metric cannot explain why the 1:2 sparsity has better F1-score than some
Fixed Sparsity even though it has lower score. This demonstrates that our Qp is a better metric than
existing metrices."
N,0.747016706443914,"A.5
COMPARISON WITH PERFORMER"
N,0.7482100238663485,"In this section, we add more discussions on how our method compared with kernel based trans-
former, i.e. Performer (Choromanski et al., 2021). As our Deﬁnition 4.1 is designed to characterize
how well the sparse pattern could reserve the important edges in A, so it is not suitable for kernel-
based attention mechanisms that do not involve sparsity. For example, an approximation of A with
high positive approximation error can have QP ≥1 under Deﬁnition 4.1. Therefore, we instead
compare the mean squared error (MSE) following Choromanski et al. (2021). Given the query
and two adjacent key vectors q, k, and k′ ∈N(0, Id), we denote the softmax kernel between
them as SM(q, k) = exp(qT k/
√"
N,0.7494033412887828,"d). And the softmax approximated by our dynamic 1:2 sparsity
\
SM1:2(q, k) is deﬁned as"
N,0.7505966587112172,"\
SM1:2(q, k) ="
N,0.7517899761336515,"(
exp

qT k
√ d"
N,0.752983293556086,"
if qT k > qT k′"
ELSE,0.7541766109785203,"0
else
.
(28)"
ELSE,0.7553699284009546,"Then, we can compute its MSE as follows"
ELSE,0.7565632458233891,"MSE( \
SM1:2(q, k)) =
Z"
ELSE,0.7577565632458234,"qT k<qT k′ exp
2qT k
√ d"
ELSE,0.7589498806682577,"
2π−d/2exp

−||k′||2
2
2"
ELSE,0.7601431980906921,"
dk′.
(29)"
ELSE,0.7613365155131265,Under review as a conference paper at ICLR 2022
ELSE,0.7625298329355609,"Because qT k′ = Pd
i=1 qik′
i is the weighted sum of i.i.d variables following N(0, 1), we have
x = qT k′ ∼N(0, ||q||2
2). We can substitute it into equation 29 and get"
ELSE,0.7637231503579952,"MSE( \
SM1:2(q, k)) = exp
2qT k
√ d  Z"
ELSE,0.7649164677804295,x>qT k
P,0.766109785202864,"1
p"
P,0.7673031026252983,"2π||q||2
2
exp

−
x2"
P,0.7684964200477327,"2||q||2
2 
dx"
P,0.7696897374701671,"= exp
2qT k
√ d"
P,0.7708830548926014," 1 −erf

qT k
||q||2
√ 2 "
P,0.7720763723150358,"2
= SM 2(q, k)
1 −erf

√"
P,0.7732696897374701,"d
||q||2
√"
P,0.7744630071599046,"2ln (SM(q, k))
 2
. (30)"
P,0.7756563245823389,"With Lemma 2 and Theorem 2 in Choromanski et al. (2021), the MSE of their positive softmax
kernel with orthogonal random features has an upper bound as follows"
P,0.7768496420047732,"MSE
 \
SM ort+
m
(q, k))

≤1"
P,0.7780429594272077,"mexp
2qT k
√ d"
P,0.779236276849642," 
exp
||q + k||2 √ d"
P,0.7804295942720764,"
−1 −

1 −1 m"
P,0.7816229116945107,"
2
d + 2  = 1"
P,0.7828162291169452,"mSM 2(q, k)

exp
||q||2
2 + ||k||2
2
√ d"
P,0.7840095465393795,"
SM 2(q, k)−1 −

1 −1 m"
P,0.7852028639618138,"
2
d + 2"
P,0.7863961813842482,"
.
(31)"
P,0.7875894988066826,"First of all, when SM(q, k) →0, both MSE( \
SM1:2(q, k)) and MSE
 \
SM ort+
m
(q, k))

con-"
P,0.788782816229117,"verge to 0. However, for large SM(q, k)s that are potentially be critical for the model accuracy,"
P,0.7899761336515513,"the exp

||q||2
2+||k||2
2
√ d"
P,0.7911694510739857,"
SM 2(q, k) term in the positive softmax kernel in Performer could greatly"
P,0.7923627684964201,"increases the MSE. Oppositely, the 1 −erf

√"
P,0.7935560859188544,"d
||q||2
√"
P,0.7947494033412887,"2ln (SM(q, k))

term in our method reduces
the MSE. To conclude, while both the positive softmax kernel and ours has low MSE error when ap-
proximating small edge weights, our method can better approximate the edges with high magnitude."
P,0.7959427207637232,"From the empirical perspective, as shown in Table 2 and 3, our method can achieve good accuracy
even without ﬁnetuning. Whereas the Performer still requires tens of thousands steps of ﬁnetuning
(e.g. Figure 5 in Choromanski et al. (2021)). Table 4 also reveals that Performer has poor accuracy
on certain tasks like byte-level document retrieval, while ours consistently achieve accuracy on par
with the dense transformer. All this observations suggest that our method can better approximate
the full attention mechanism than Performer."
P,0.7971360381861575,"In terms of wall-clock time speedup, Figure 4 illustrates that the Performer can only achieve good
speedup at long sequence length. The similar phenomenon is also observed in multiple online fo-
rums 3. Certainly, the PyTorch JIT script does not yield the optimal implementation of the compu-
tation graph, but it reveals that tremendous engineering efforts are required for Performer to achieve
good speedup under moderate sequence length."
P,0.7983293556085919,"Following Section 4.3, we also compare the theoretical speedup achieved by ours and the Performer."
P,0.7995226730310262,"T (1)
n×m = Qn×d 4√"
P,0.8007159904534606,"d
Pd×m, T (2)
n×1 =
1
2
√ d d
X"
P,0.801909307875895,"i=1
[Qn×d ⊙Qn×d]:,i"
P,0.8031026252983293,"T (3)
n×1 = maxi[T (1)
n×m]:i, φ(Qn×m) =
1
√mexp

T (1)
n×m −T (2)
n×1 −T (3)
n×1 + ϵ
"
P,0.8042959427207638,"T (4)
n×m = Kn×d 4√"
P,0.8054892601431981,"d
Pd×m, T (5)
n×1 =
1
2
√ d d
X"
P,0.8066825775656324,"i=1
[Kn×d ⊙Kn×d]:,i"
P,0.8078758949880668,"T (6)
n×1 = maxi[T (4)
n×m]:i, φ(Kn×m) =
1
√mexp

T (4)
n×m −T (5)
n×1 −T (6)
n×1 + ϵ
"
P,0.8090692124105012,"T (7)
m×1 = n
X"
P,0.8102625298329356,"i=1
[φ(K)n×m]i,:, T (8)
n×1 = 1/

φ(Q)n×m × T (7)
m×1
"
P,0.8114558472553699,"T (9)
m×d = φ(K)T
n×m × Vn×d, T (10)
n×d = φ(Q)n×m × T (9)
m×d ⊙T (8)
n×1. (32)"
P,0.8126491646778043,"The computation steps of Performer are listed in equation 32 where each equation denotes a sub
computation graph that can potentially be fused. Notably, this is more complex than the original"
P,0.8138424821002387,3https://github.com/huggingface/transformers/issues/7675
P,0.815035799522673,Under review as a conference paper at ICLR 2022
P,0.8162291169451074,"512
1024
2048
4096 0.5 1.0 1.5 2.0 2.5"
P,0.8174224343675418,"dtype=float, #head=4, hidden=256"
P,0.8186157517899761,"Ours
Perfomer
Reformer
Routing
Sinkhorn
Nystrom"
P,0.8198090692124105,"512
1024
2048
4096 0.5 1.0 1.5 2.0 2.5"
P,0.8210023866348448,"dtype=float, #head=4, hidden=512"
P,0.8221957040572793,"512
1024
2048
4096 0.5 1.0 1.5 2.0"
P,0.8233890214797136,"dtype=float, #head=4, hidden=1024"
P,0.8245823389021479,"512
1024
2048
4096 0.5 1.0 1.5 2.0 2.5"
P,0.8257756563245824,"dtype=float, #head=4, hidden=256"
P,0.8269689737470167,"512
1024
2048
4096 0.5 1.0 1.5 2.0 2.5"
P,0.8281622911694511,"dtype=float, #head=4, hidden=512"
P,0.8293556085918854,"512
1024
2048
4096 0.5 1.0 1.5 2.0 2.5"
P,0.8305489260143198,"dtype=float, #head=4, hidden=1024"
P,0.8317422434367542,"512
1024
2048
4096 0.5 1.0 1.5 2.0 2.5"
P,0.8329355608591885,"dtype=bfloat16, #head=4, hidden=256"
P,0.834128878281623,"512
1024
2048
4096 0.5 1.0 1.5 2.0 2.5"
P,0.8353221957040573,"dtype=bfloat16, #head=4, hidden=512"
P,0.8365155131264916,"512
1024
2048
4096 0.5 1.0 1.5 2.0 2.5"
P,0.837708830548926,"dtype=bfloat16, #head=4, hidden=1024"
P,0.8389021479713604,"512
1024
2048
4096 0.5 1.0 1.5 2.0 2.5"
P,0.8400954653937948,"dtype=bfloat16, #head=4, hidden=256"
P,0.8412887828162291,"512
1024
2048
4096 0.5 1.0 1.5 2.0 2.5"
P,0.8424821002386634,"dtype=bfloat16, #head=4, hidden=512"
P,0.8436754176610979,"512
1024
2048
4096 0.5 1.0 1.5 2.0 2.5"
P,0.8448687350835322,"dtype=bfloat16, #head=4, hidden=1024"
P,0.8460620525059666,"0.0
0.2
0.4
0.6
0.8
1.0
Sequence Length 0.0 0.2 0.4 0.6 0.8 1.0"
P,0.847255369928401,End-to-End Speedup over Dense Transformer
P,0.8484486873508353,Figure 13: End-to-end inference speedup of different efﬁcient transformers over dense transformer.
P,0.8496420047732697,"mathematical expression to handle the numerical instability of exp. The total memory access can be
computed with"
P,0.850835322195704,"Speedup={2

nm
2d"
P,0.8520286396181385,"T + 1

+n(d+1)+n(m + 1)+n(m + 3)

+m(n + 1)+n(m"
P,0.8532219570405728,T + m + 1)
P,0.8544152744630071,"+ md
2n"
P,0.8556085918854416,"T + 1

+ nd
2m"
P,0.8568019093078759,"T
+ 1

+ n}/

n2
2d"
P,0.8579952267303103,"T + 1

+ 2n2 + nd
2n"
P,0.8591885441527446,"T + 1

. (33)"
P,0.860381861575179,"We have m = dln(d) following Theorem 4 in Choromanski et al. (2021). We can substitute m =
266, d = 64, and T = 128 into equation 33 and get Speedup > 1 when n > 672. On the other
hand, the performer achieves the same speedup with ours with n > 1002."
P,0.8615751789976134,"To conclude, our method is a good complementary to performer. With delicately optimized com-
putation graph, performer can achieve good speedup and relatively good accuracy under long se-
quence scenario. In contrary, our method has better speedup and accuracy under moderate and short
sequence length. Besides, our method delivers lower approximation error on important edges so it
is more friendly to ﬁnetuning."
P,0.8627684964200477,"A.6
END-TO-END SPEEDUP AND MEMORY FOOTPRINT REDUCTION"
P,0.863961813842482,"In this section, we present the end-to-end speedup achieved by our method under different conﬁg-
urations. We use the 4-layer dense transformer model of Text Classiﬁcation task in Long Range
Arena Tay et al. (2021). The dimension of each head is 64. We explore different combination of
number of heads (4, 8), sequence length (512, 1024, 2048, 4096), and hidden dimension of the feed
forward layer (256, 512, 1024). The end-to-end speedup over the dense transformer under different
conﬁgurations are plotted in Figure 13."
P,0.8651551312649165,"Our method achieves 1.11 ∼1.52× and 1.08 ∼1.47× end-to-end speedup over the dense trans-
former, it is the only method that deliver end-to-end speedup under all conﬁgurations. Under se-
quence length ≤2048, our method achieves higher speedup than most of the baselines. Although"
P,0.8663484486873508,Under review as a conference paper at ICLR 2022
P,0.8675417661097852,"Sinkhorn transformer (Tay et al., 2020a) has higher speedup than ours at sequence length 2048, as
shown in Table 4, its accuracy is less satisfying. This result justiﬁes that our method delivers good
speedup under short and moderate sequence length. Notably, this speedup is almost a free lunch.
On one hand, Section 5 demonstrates that our method achieves comparable accuracy across differ-
ent tasks and sequence length, so the model accuracy is not sacriﬁced. On the other hand, unlike
previous efﬁcient transformers, our method has no hyper-parameters and only requires lightweight
ﬁnetuning process."
X,0.8687350835322196,"1.10 x
1.09 x
1.08 x
1.10 x
1.09 x
1.08 x"
X,0.869928400954654,"1.41 x
1.38 x
1.37 x
1.44 x
1.43 x
1.41 x"
X,0.8711217183770883,"0
0.2
0.4
0.6
0.8"
X,0.8723150357995226,"1
1.2
1.4
1.6 Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours"
X,0.8735083532219571,"256
512
1024
256
512
1024
256
512
1024
256
512
1024"
X,0.8747016706443914,"512
1024
2048
4096"
X,0.8758949880668258,"Attention
Others
Speedup"
X,0.8770883054892601,"1.10 x
1.09 x
1.09 x
1.15 x
1.15 x
1.14 x"
X,0.8782816229116945,"1.45 x
1.44 x
1.41 x
1.47 x
1.46 x
1.44 x"
X,0.8794749403341289,"0
0.2
0.4
0.6
0.8"
X,0.8806682577565632,"1
1.2
1.4
1.6 Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours Dense Ours"
X,0.8818615751789977,"256
512
1024
256
512
1024
256
512
1024
256
512
1024"
X,0.883054892601432,"512
1024
2048
4096"
X,0.8842482100238663,Latency Normalized to Dense
X,0.8854415274463007,# Head = 4
X,0.8866348448687351,# Head = 8
X,0.8878281622911695,Hidden
X,0.8890214797136038,Seq Len
X,0.8902147971360382,Hidden
X,0.8914081145584726,Seq Len
X,0.8926014319809069,Figure 14: End-to-end inference latency break down under bﬂoat16.
X,0.8937947494033412,"To study how our method contributes to the end-to-end speedup, we further break down the end-to-
end inference time to the attention mechanism and other components under bﬂoat16. The results are
illustrated in Figure 14. Under moderate and short sequence length like 1024 and 512, the “Others”
contributes over 70% of the total latency. This is because the size of the matrix multiplications in the
feed-forward network and query/key/value projection are comparable with the attention mechanism."
X,0.8949880668257757,"However, unlike the attention mechanism that has limited time budget for compression, the feed-
forward network and query/key/value projection use a static weight matrix during inference, so they
can be compressed ofﬂine. Tons of methods have been proposed in the literature to do that even
before the transformers are proposed. For instance, Mishra et al. (2021); Zhou et al. (2020) show
that pruning the weights to 2:4 sparsity can deliver 1.3 ∼1.6× speedup and 2× fewer parameters in
the feed-forward and projection layers 4 without accuracy loss on BERT-large. Lagunas et al. (2021)
apply structured pruning and achieve 2.4× speedup on SQuAD v1.1 with 1% drop of F1. The
MobileBERT (Sun et al., 2020), on the other hand, redesign the network architecture that reduce the
hidden dimension of feed-forward network in BERT from 4096 to 512. In terms of quantization,
previous work (Zafrir et al., 2019) have shown that the linear layers can be quantized to 8 bit integer."
X,0.89618138424821,"Besides the linear layers, there are also techniques to accelerate other components in transform-
ers. For instance, the MobileBERT (Sun et al., 2020) replaces the layer normalization to a simple
element-wise linear transformation. The input embedding table is also compressed with smaller
embedding dimension along with an 1D convolution."
X,0.8973747016706444,4https://developer.nvidia.com/blog/exploiting-ampere-structured-sparsity-with-cusparselt/
X,0.8985680190930787,Under review as a conference paper at ICLR 2022
X,0.8997613365155132,"512
1024
2048
4096
0 1 2 3 4"
X,0.9009546539379475,"dtype=float, #head=4, hidden=256"
X,0.9021479713603818,"Ours
Perfomer
Reformer
Routing
Sinkhorn
Nystrom"
X,0.9033412887828163,"512
1024
2048
4096
0 1 2 3 4"
X,0.9045346062052506,"dtype=float, #head=4, hidden=512"
X,0.905727923627685,"512
1024
2048
4096
0 1 2 3 4"
X,0.9069212410501193,"dtype=float, #head=4, hidden=1024"
X,0.9081145584725537,"512
1024
2048
4096
0 1 2 3 4"
X,0.9093078758949881,"dtype=float, #head=4, hidden=256"
X,0.9105011933174224,"512
1024
2048
4096
0 1 2 3 4"
X,0.9116945107398569,"dtype=float, #head=4, hidden=512"
X,0.9128878281622912,"512
1024
2048
4096
0 1 2 3 4"
X,0.9140811455847255,"dtype=float, #head=4, hidden=1024"
X,0.9152744630071599,"512
1024
2048
4096
0 1 2 3 4"
X,0.9164677804295943,"dtype=bfloat16, #head=4, hidden=256"
X,0.9176610978520287,"512
1024
2048
4096
0 1 2 3 4"
X,0.918854415274463,"dtype=bfloat16, #head=4, hidden=512"
X,0.9200477326968973,"512
1024
2048
4096
0 1 2 3 4"
X,0.9212410501193318,"dtype=bfloat16, #head=4, hidden=1024"
X,0.9224343675417661,"512
1024
2048
4096
0 1 2 3 4"
X,0.9236276849642004,"dtype=bfloat16, #head=4, hidden=256"
X,0.9248210023866349,"512
1024
2048
4096
0 1 2 3 4"
X,0.9260143198090692,"dtype=bfloat16, #head=4, hidden=512"
X,0.9272076372315036,"512
1024
2048
4096
0 1 2 3 4"
X,0.9284009546539379,"dtype=bfloat16, #head=4, hidden=1024"
X,0.9295942720763724,"0.0
0.2
0.4
0.6
0.8
1.0
Sequence Length 0.0 0.2 0.4 0.6 0.8 1.0"
X,0.9307875894988067,Peak Memory Allocation Normalized to Dense Transformer
X,0.931980906921241,Figure 15: Peak memory allocation normalized to dense transformer under different conﬁgurations.
X,0.9331742243436754,"With all these techniques in the literature, it should not be hard to achieve 2× speedup in the non-
attention part of transformer models. Then our method could deliver 1.13 ∼1.41× speedup under
sequence length ≤1024."
X,0.9343675417661098,"We also measure the peak memory allocation of different models and conﬁgurations, the results
are summarized in Figure 15. Our method achieves 1.41 ∼1.82× memory reduction, which is
comparable with or better than most existing efﬁcient transformers when sequence length ≤1024."
X,0.9355608591885441,"A.7
COMBINATION WITH THE EXISTING EFFICIENT TRANSFORMERS"
X,0.9367541766109785,"Existing efﬁcient transformers usually sparsify the full attention mechanism to densely connected
clusters (Tay et al., 2020a; Roy et al., 2021; Kitaev et al., 2020; Zaheer et al., 2020) or approximate
it with low-rank projection (Wang et al., 2020). As our method is a good approximation of the full
attention mechanism and brings wall time speedup at arbtrary sequence length, it can potentially be
combined with the existing efﬁcient transformers."
X,0.9379474940334129,"We ﬁrst demonstrate the combination of our method with Xiong et al. (2021). Xiong et al. (2021)
propose a Nystrom-based self-attention mechanism that approximate standard self-attention with
O(n) complexity. The Nystromformer is illustrated in Figure 16. We observe that the computation
circled in Figure 16 is identical to the standard attention mechanism, so it can be further accelerated
with our method. More importantly, the two matrix multipliation involved are the two of the three
largest m × n matrices. It will be very beneﬁcial to reduce their complexity."
X,0.9391408114558473,"We report the accuracy on Image (1K) on LRA (Tay et al., 2021) in Table 6. We ﬁrst pretrain a
standard Nystromformer from the scratch for 35,000 iterations following Xiong et al. (2021). Then,
we ﬁnetune it for 3,500 iterations (1/10 of the training process) under standard Nystromformer,
Nystromformer + DFSSATTEN 1:2, and Nystromformer + DFSSATTEN 2:4. It is obvious that
by combining DFSSATTEN and Nystromformer, we can achieve higher accuracy on LRA with
lightweight ﬁnetuning."
X,0.9403341288782816,Under review as a conference paper at ICLR 2022
X,0.9415274463007159,"𝑋:
𝑛×𝑑
𝐾!: 𝑑""×𝑛
𝐾'!: 𝑑""×𝑚"
X,0.9427207637231504,"𝑄:
𝑛×𝑑"""
X,0.9439140811455847,"𝑉:
𝑛×𝑑#"
X,0.9451073985680191,"𝑄+:
m×𝑑"" ×
𝑚×𝑚 n×𝑚 m×𝑛 n×𝑚 𝑚×𝑑#"
X,0.9463007159904535,"n×𝑑#
𝑂∶
n×𝑑#"
X,0.9474940334128878,"n×𝑑#
DConv k×1"
X,0.9486873508353222,sMEANS
X,0.9498806682577565,sMEANS
X,0.951073985680191,"𝑚×𝑚
pINV × × × × ×
+ SDDMM SDDMM SpMM SpMM"
X,0.9522673031026253,"Figure 16: Combination of our method with Nystromformer (Xiong et al., 2021). The two red
matrices are stored under 1:2/2:4 structured sparsity."
X,0.9534606205250596,"Table 6: Accuracy on Image (1K) on LRA (Tay et al., 2021) under the combination of DFSSATTEN
and Nystromformer (Xiong et al., 2021)."
X,0.954653937947494,"Pretraining
Finetuning
Nystromformer (ﬂoat)
41.17
41.52
Nystromformer (bﬂoat16)
-
41.59
Nystromformer + DFSSATTEN 1:2 (ﬂoat)
-
41.91
Nystromformer + DFSSATTEN 2:4 (bﬂoat16)
-
42.54"
X,0.9558472553699284,"Then we provide a complexity analysis of the combination following Xiong et al. (2021). The
landmark selection with segement-means takes O(n), iterative approximation of the pseudoinverse
takes O(m3). The matrix multiplication complexity of the standard Nystromformer takes O(nm2 +
mndv+m3+nmdv). After applying our method, it can be reduced to O( nm2"
X,0.9570405727923628,2 + nmdv
X,0.9582338902147971,"2
+m3+nmdv).
The memory footprint can be reduced from O(mdq + nm + m2 + nm + ndv) to O(mdq + nm +
m2 + ndv). Given n ≫m > dv ≈dp, this could be a signiﬁcant improvement that allows us to use
more landmarks m to better approximate the full attention mechanism."
X,0.9594272076372315,"Besides Nystromformer, we also illustrate two possible combinations with BigBird (Zaheer et al.,
2020) and Linformer Wang et al. (2020) that can be explored in the future work."
X,0.9606205250596659,"As shown in Figure 17 (A), Zaheer et al. (2020) use block sparsity with block size 64 and compute
a full attention within each block. We can apply the 1:2 or 2:4 sparsity within each block to bring
further speedup."
X,0.9618138424821002,"Figure 17 (B) gives another example on how to combine our method with Linformer (Wang et al.,
2020). Linformer uses low-rank approximation on the attention mechanism as follows:"
X,0.9630071599045346,O = softmax
X,0.964200477326969,Q (EK)T √ d !
X,0.9653937947494033,"F V ,
(34)"
X,0.9665871121718377,"where E, F ∈Rn×k are linear projection matrices and k ≪n. We can ﬁrst prune E and F along
with other weight matrices to have 1:2 or 2:4 sparsity ofﬂine following Mishra et al. (2021). Then
we compute EK and F V with Sparse Matrix-Matrix multiplication. Next, we multiply Q and
(EK)T and the result is pruned to 50% structured ﬁne-grained sparsity on the ﬂy. After applying
softmax to the nonzeros, we multiply it with F V ."
X,0.9677804295942721,"A.8
VISUALIZE ATTENTION DISTRIBUTION"
X,0.9689737470167065,"To illustrate that our DFSSATTEN can well capture the ﬁne-grained sparsity in attention, we vi-
sualize the attention weight matrices in BERT-large on SQuAD v1.1 in Figure 18. In detail, we"
X,0.9701670644391408,"Under review as a conference paper at ICLR 2022 𝑄 𝑉 𝑌 𝑘
2 𝑘
16 𝑛 𝑑 𝑛 𝑑"
X,0.9713603818615751,"𝑄𝐾!𝐸! 𝜎
𝑂"
X,0.9725536992840096,(B) Linformer: 𝐸! 𝐾!
X,0.9737470167064439,"𝑛
𝑑
  𝐾!𝐸! 𝑘 𝑛/2 𝑛/16"
X,0.9749403341288783,𝜎(𝑄𝐾!𝐸!)
X,0.9761336515513126,"𝐹𝑉
𝐹
𝑘"
X,0.977326968973747,𝑛/2 𝑛/16
X,0.9785202863961814,"(A) BigBird 𝑛 𝑛 𝑑 𝑑
𝐾 𝑄 𝑏 𝑏"
X,0.9797136038186157,"Figure 17: Combination of our method with BigBird (Zaheer et al., 2020) and Linformer (Wang
et al., 2020)"
X,0.9809069212410502,"run inference of the same input sample in BERT-large model pretrained under dense, 1:2, and 2:4
settings, then collect the attention weight matrix in the ﬁrst layer. It is obvious that the pattern in
dense transformer and our DFSSATTEN are quite similar. The magnitude of nonzero values in DF-
SSATTEN are a little bit higher than dense attention. This is because the softmax normalizes the
values in each row with the exponential sum of each entry. After removing 50% smaller entries, the
magnitude of remaining entries would be relatively higher. Nevertheless, we ﬁnd that this does not
inﬂuence the model accuracy, as the forthcoming normalization layers will take care of it."
X,0.9821002386634845,Under review as a conference paper at ICLR 2022
X,0.9832935560859188,"Input 0
Input 1
Input 0"
X,0.9844868735083532,Head 0
X,0.9856801909307876,Input 1
X,0.986873508353222,"Head 1
Head 0
Head 1"
X,0.9880668257756563,"0.0000
0.0025
0.0050
0.0075
0.0100
0.0125
0.0150
0.0175
0.0200"
X,0.9892601431980907,"0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.2 0.4 0.6 0.8"
X,0.9904534606205251,"1.0
Dense (float)"
X,0.9916467780429594,"0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.2 0.4 0.6 0.8"
X,0.9928400954653938,"1.0
1:2 (float)"
X,0.9940334128878282,"0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.2 0.4 0.6 0.8 1.0"
X,0.9952267303102625,Dense (bfloat16)
X,0.9964200477326969,"0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.2 0.4 0.6 0.8 1.0"
X,0.9976133651551312,2:4 (bfloat16)
X,0.9988066825775657,Figure 18: Visualization of attention weight in dense transformer and DFSSATTEN
