Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029239766081871343,"Recent research has shown the existence of significant redundancy in large Trans-
former models. One can prune the redundant parameters without significantly
sacrificing the generalization performance. However, we question whether the
redundant parameters could have contributed more if they were properly trained.
To answer this question, we propose a novel training strategy that encourages all
parameters to be trained sufficiently. Specifically, we adaptively adjust the learning
rate for each parameter according to its sensitivity, a robust gradient-based measure
reflecting this parameter’s contribution to the model performance. A parameter
with low sensitivity is redundant, and we improve its fitting by increasing its
learning rate. In contrast, a parameter with high sensitivity is well-trained, and
we regularize it by decreasing its learning rate to prevent further overfitting. We
conduct extensive experiments on natural language understanding, neural machine
translation, and image classification to demonstrate the effectiveness of the pro-
posed schedule. Analysis shows that the proposed schedule indeed reduces the
redundancy and improves generalization performance.1"
INTRODUCTION,0.005847953216374269,"1
INTRODUCTION"
INTRODUCTION,0.008771929824561403,"Large-scale Transformer models have achieved remarkable success in various fields. Performance
of these models scales with their number of parameters, which can be up to hundreds of millions,
e.g., BERT (Devlin et al., 2018), DeBERTa (He et al., 2020), GPT-3 (Brown et al., 2020). Recent
research, however, has shown the existence of significant redundancy in the Transformer models
(Michel et al., 2019; Fan et al., 2019; Wang et al., 2019; Chen et al., 2020; Sanh et al., 2020). For
example, Sanh et al. (2020) removes around 90% of the parameters, and the models exhibit only a
marginal performance drop."
INTRODUCTION,0.011695906432748537,"100
80
60
Percentage of Weight Remaining (%) 60.0 62.5 65.0 67.5 70.0 72.5"
INTRODUCTION,0.014619883040935672,Accuracy
INTRODUCTION,0.017543859649122806,"Figure 1: Validation results
of fine-tuning BERT-base at
different sparsity levels on
the RTE dataset (Wang et al.,
2018) in Liang et al. (2021).
Solid black curve represents
the full model performance."
INTRODUCTION,0.02046783625730994,"The existence of redundancy can hurt the model performance. Re-
cent works have demonstrated that the removal of the redundant
parameters can lead to better generalization performance, a phe-
nomenon observed in both small-scale models (Mozer & Smolensky,
1989; Rasmussen & Ghahramani, 2001; Grünwald & Grunwald,
2007) and large-scale Transformer models (Bartoldson et al., 2019;
Voita et al., 2019; Hou et al., 2020; Liang et al., 2021). As illus-
trated in Figure 1, with up to 20% of the parameters pruned, the
generalization performance boosts up to 1%."
INTRODUCTION,0.023391812865497075,"As a result, we aim to improve model generalization through re-
dundancy elimination. However, the existence of redundancy has
long been regarded as inevitable. The common belief is that, in
each network, there always exists a set of parameters “born” to be
useless (Frankle & Carbin, 2018; Liu et al., 2018). Following this
belief, pruning, where redundant parameters are directly zeroed out,
becomes one of the most widely adopted solutions to redundancy
elimination. However, we ask a critical question here:"
INTRODUCTION,0.02631578947368421,1Our code has been released at https://github.com/cliang1453/SAGE
INTRODUCTION,0.029239766081871343,Published as a conference paper at ICLR 2022
INTRODUCTION,0.03216374269005848,"Are these parameters really redundant, or just insufficiently trained by commonly used
training strategies?"
INTRODUCTION,0.03508771929824561,"Our question is motivated by empirical observations, which show that training strategies indeed play
a role in causing redundancy. For example, different learning rates (Table 1), random seeds and
optimizers (Morcos et al., 2019) can produce models with similar performance but different sets
of redundant parameters. This suggests that the redundancy of parameters depends on the training
strategy: A training strategy often prefers specific parameters and provides them with sufficient
training. In contrast, the other parameters receive insufficient training and become under-fitted. As
a result, these parameters become redundant, such that they fail to contribute to the generalization
and prevent the model from achieving its ideal performance. Therefore, we hypothesize that with
a desirable training strategy, these redundant parameters can receive more sufficient training and
become useful ultimately."
INTRODUCTION,0.038011695906432746,"OVLP Among
Avg % OVLP"
MODELS,0.04093567251461988,"2 Models
59.8%
3 Models
46.5%
5 Models
35.7%"
MODELS,0.043859649122807015,"Table 1: Percentage of overlapping
between the 30% most redundant
parameters in 5 BERT-base models
fine-tuned using {1, 5, 8, 10, 20} ×
10−5 as learning rates on SST-2."
MODELS,0.04678362573099415,"We verify the hypothesis by proposing a novel training strat-
egy, which encourages all parameters to be trained sufficiently.
Throughout the training process, we simultaneously excite the
under-fitted parameters to reduce redundancy and regularize
the well-fitted parameters to prevent overfitting."
MODELS,0.049707602339181284,"More specifically, we propose an adaptive learning rate sched-
ule – SAGE (Sensitivity-guided Adaptive learninG ratE), where
each parameter learns at its own pace guided by its sensitivity.
Sensitivity originated in model pruning, where it is used to mea-
sure the redundancy of the parameters (Molchanov et al., 2016;
2019; Theis et al., 2018; Lee et al., 2018; Ding et al., 2019).
In pruning literature, parameters with low sensitivity are considered redundant. Since a redundant
parameter could be insufficiently trained and under-fitted, we promote its training by increasing its
learning rate. In contrast, for a parameter with high sensitivity, i.e., it is considered sufficiently trained
and well-fitted, we slow down its training by decreasing its learning rate to prevent overfitting."
MODELS,0.05263157894736842,"Moreover, we introduce a local temporal variation of the sensitivity as a second factor to further guide
the learning rate. The local temporal variation essentially measures the uncertainty of sensitivity,
which mainly comes from two sources: (1) The sensitivity can have large variance due to data
sampling. This is because during training, the sensitivity is evaluated using a randomly sampled
mini-batch instead of all the training data. (2) The sensitivity of a parameter may not be stable and can
vary drastically among iterations, which introduces extra uncertainty. We define the local temporal
variation of a parameter as the absolute difference between its sensitivity and an exponential moving
average of its sensitivity from all previous iterations. A large local temporal variation implies high
uncertainty in the sensitivity at the current iteration, and therefore it is not yet a reliable indicator of
redundancy. Accordingly, we should avoid significantly decreasing its learning rate even though its
sensitivity at the current iteration might be large."
MODELS,0.05555555555555555,"Therefore, we eventually require the overall learning rate schedule for each parameter to be pro-
portional to the ratio between the local temporal variation and the sensitivity. This can effectively
account for the uncertainty issue in sensitivity."
MODELS,0.05847953216374269,"We conduct experiments on a wide range of tasks and models to demonstrate the effectiveness of
SAGE. In natural language understanding, the fine-tuning performance of BERT-base (Devlin et al.,
2018) and RoBERTa-large (Liu et al., 2019b) improves 1.4 and 0.6 task-average score on the dev
set of the GLUE benchmark (Wang et al., 2018), respectively. Furthermore, SAGE improves neural
machine translation performance using Transformer-base (Vaswani et al., 2017) on two datasets,
suggesting it also benefits training-from-scratch. SAGE also boost the image classification accuracy
on ImageNet dataset (Deng et al., 2009) with Vision Transformer models (Dosovitskiy et al., 2020).
Furthermore, our experiments demonstrate SAGE is complementary to various types of optimizers,
e.g., SGD (Robbins & Monro, 1951), Adam, and Adamax (Kingma & Ba, 2014)."
MODELS,0.06140350877192982,"Moreover, we observe several favorable proprieties of SAGE. First, it leads to balanced and sufficient
training on all parameters and produces a better-generalized model. Second, SAGE is complementary
to state-of-the-art training methods. Specifically, we show that SAGE achieves better performance on
GLUE when combined with adversarial regularization (Jiang et al., 2019)."
MODELS,0.06432748538011696,Published as a conference paper at ICLR 2022
PRELIMINARY,0.06725146198830409,"2
PRELIMINARY"
PRELIMINARY,0.07017543859649122,We briefly review the sensitivity of the parameters and adaptive learning rate methods.
SENSITIVITY OF THE PARAMETERS,0.07309941520467836,"2.1
SENSITIVITY OF THE PARAMETERS"
SENSITIVITY OF THE PARAMETERS,0.07602339181286549,"The sensitivity of a parameter essentially approximates the change in the loss magnitude when this
parameter is completely zeroed-out (LeCun et al., 1990; Mozer & Smolensky, 1989). If the removal
of a parameter causes a large influence on the loss, then the model is sensitive to it. More specifically,
we define a deep neural network with parameters Θ = [θ1, ..., θJ] ∈RJ, where for j = 1, ..., J,
θj ∈R denotes each parameter. We further define Θj,−j = [0, ..., 0, θj, 0, ..., 0] ∈RJ. We denote
the loss of the model as L(Θ), and the gradients of the loss with respect to Θ as ∇ΘL(Θ). The
sensitivity of the j-th parameter is defined as the magnitude of the gradient-weight product:"
SENSITIVITY OF THE PARAMETERS,0.07894736842105263,"Ij = |Θ⊤
j,−j∇ΘL(Θ)|.
(1)"
SENSITIVITY OF THE PARAMETERS,0.08187134502923976,"This definition is derived from the first-order Taylor expansion of L(·) with respect to θj at Θ.
Specifically, Ij approximates the absolute change of the loss given the removal of θj:"
SENSITIVITY OF THE PARAMETERS,0.0847953216374269,"Θ⊤
j,−j∇ΘL(Θ) ≈L(Θ) −L(Θ −Θj,−j)."
SENSITIVITY OF THE PARAMETERS,0.08771929824561403,"The sensitivity was originally introduced for model pruning (Molchanov et al., 2016; 2019; Theis
et al., 2018; Lee et al., 2018; Ding et al., 2019; Xiao et al., 2019), and it was commonly used as an
“importance score” for model weights. The parameters with high sensitivity are of high importance
and should be kept (Lubana & Dick, 2020). Parameters with low sensitivity are considered redundant,
and they can be safely pruned with only marginal influence on the model loss."
ADAPTIVE LEARNING RATE METHODS,0.09064327485380116,"2.2
ADAPTIVE LEARNING RATE METHODS"
ADAPTIVE LEARNING RATE METHODS,0.0935672514619883,"Adaptive learning rate methods adjust the learning rate of each individual parameter based on
the training progress. Most of these methods focus on adapting the training to the optimization
landscape, e.g., AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012), RMSProp (Hinton et al.,
2012), Adam(Kingma & Ba, 2014) and RAdam (Liu et al., 2019a). Their purpose is to make the
model converge faster to the first-order stationary solutions. Specifically, these methods prefer
updating the weights with smaller second-order moments, as the loss function is generally flat along
directions corresponding to such weights."
ADAPTIVE LEARNING RATE METHODS,0.09649122807017543,"There are also some adaptive learning rate methods focusing on the perspective of improving model
generalization (Loshchilov & Hutter, 2018; Foret et al., 2020). For example, AdamW (Loshchilov &
Hutter, 2018) propose to decouple the weight decay and gradient update to avoid regularizing weights
that have larger gradient magnitudes with a weaker strength."
METHOD,0.09941520467836257,"3
METHOD"
METHOD,0.1023391812865497,"We introduce our proposed adaptive learning rate schedule, SAGE. Our method customizes a specific
learning rate for each parameter at each iteration. A parameter’s learning rate at a certain iteration is
determined by two factors: sensitivity and its local temporal variation."
METHOD,0.10526315789473684,"Sensitivity of the parameters. At the t-th iteration, following Eq. (1), we define the sensitivity of
θ(t)
j
as"
METHOD,0.10818713450292397,"I(t)
j
= |Θ(t)⊤
j,−j∇Θ(t)L(Θ(t))|,
(2)"
METHOD,0.1111111111111111,"which reflects the influence of removing θ(t)
j
in the model loss. In previous literature, θ(t)
j
is considered"
METHOD,0.11403508771929824,"redundant when I(t)
j
is small. In contrast, we hypothesize that θ(t)
j
is just insufficiently trained and
under-fitted, and can become less redundant when receiving further training."
METHOD,0.11695906432748537,"Local temporal variation. Recall that the sensitivity measure involves excessive uncertainty, which
comes from: (1) Sensitivity is measured based on a randomly sampled mini-batch of the training data
at each iteration, which leads to a large variance; (2) Sensitivity can be unstable and vary drastically,
as changes of the model introduce extra uncertainty to the measure."
METHOD,0.11988304093567251,"One way to measure the uncertainty of sensitivity of θj is the absolute change of sensitivity, i.e.,
|I(t)
j
−I(t−1)
j
|. Such a quantity often has a large variance in practice. Therefore, we propose to keep"
METHOD,0.12280701754385964,Published as a conference paper at ICLR 2022
METHOD,0.12573099415204678,"track of an exponential moving average of I(t)
j
as"
METHOD,0.1286549707602339,"bI(t)
j
= β0bI(t−1)
j
+ (1 −β0)I(t)
j ,"
METHOD,0.13157894736842105,"where bI(0)
j
= 0 and β0 ∈(0, 1) is a hyper-parameter. Based on bI(t)
j , we measure the uncertainty of
the j-th parameter’s sensitivity using the local temporal variation defined as:"
METHOD,0.13450292397660818,"U (t)
j
= |I(t)
j
−bI(t)
j |.
(3)"
METHOD,0.13742690058479531,"We remark that a large U (t)
j
implies that there exists high uncertainty in I(t)
j , and therefore it is not"
METHOD,0.14035087719298245,"yet a reliable indicator of the redundancy of θ(t)
j ."
METHOD,0.14327485380116958,"Algorithm. We denote the learning rate at the t-th iteration as η(t) under the original schedule. Then
the sensitivity-guided learning rate for the j-th parameter at the t-th iteration can be computed as"
METHOD,0.14619883040935672,"η(t)
j
= η(t) ·
U (t)
j
+ ϵ
bI(t)
j
+ ϵ
= η(t) ·
|I(t)
j
−bI(t)
j | + ϵ
bI(t)
j
+ ϵ
,
(4)"
METHOD,0.14912280701754385,"where 0 < ϵ ≪1 prevents zero learning rate and zero denominator. Algorithm 2 shows the SAGE
algorithm for SGD, and extensions to other algorithms, such as Adam (Kingma & Ba, 2014), are
straightforward (Appendix A.4.1)."
METHOD,0.15204678362573099,"In Eq. (4), we place bI(t)
j
in the denominator, as one of our goals is to encourage all parameters to be"
METHOD,0.15497076023391812,"sufficiently trained. If bI(t)
j
is small, we promote its training by increasing its learning rate. If bI(t)
j
is
large, we slow down its training to prevent overfitting by decreasing its learning rate."
METHOD,0.15789473684210525,"We place U (t)
j
in the numerator to measure the uncertainty in the sensitivity. A large U (t)
j
implies"
METHOD,0.1608187134502924,"I(t)
j
is not yet a reliable indicator of the redundancy in θ(t)
j . We thus avoid significantly decreasing its
learning rate."
METHOD,0.16374269005847952,Algorithm 1 SGD-SAGE (⊙denotes Hadamard product and ⊘denotes Hadamard division)
METHOD,0.16666666666666666,"Input: Model parameters Θ ∈RJ; Data D; Learning rate schedule η(·); Total training iteration T;
Moving average coefficient β0.
1: Initialize bI(0) = 0 ∈RJ.
2: for t = 1, ..., T do
3:
Sample a minibatch b(t) from D.
4:
Compute gradient ∇Θ(t)L(b(t), Θ(t)).
5:
I(t) = |Θ(t) ⊙∇Θ(t)L(b(t), Θ(t))|.
6:
bI(t) = β0bI(t−1) + (1 −β0)I(t).
7:
U (t) = |I(t) −bI(t)|.
8:
Θ(t+1) = Θ(t) −η(t)(U (t) + ϵ) ⊘(bI(t) + ϵ) ⊙∇Θ(t)L(b(t), Θ(t)).
9: end for"
METHOD,0.1695906432748538,"Computation and memory usage. SAGE adds a marginal cost to computation and memory usage.
At each iteration, we only perform an extra element-wise multiplication between the weight matrix
and the corresponding gradient matrix obtained through back-propagation. The only memory cost is
to store the exponential moving average of sensitivity."
EXPERIMENTS,0.17251461988304093,"4
EXPERIMENTS"
EXPERIMENTS,0.17543859649122806,"We evaluate SAGE on widely used benchmarks for natural language understanding (NLU), neural
machine translation (NMT), and image classification."
NATURAL LANGUAGE UNDERSTANDING,0.1783625730994152,"4.1
NATURAL LANGUAGE UNDERSTANDING"
NATURAL LANGUAGE UNDERSTANDING,0.18128654970760233,"Model and data. We evaluate the fine-tuning performance of the pre-trained language models,
BERT-base (Devlin et al., 2018) and RoBERTa-large (Liu et al., 2019b), on the General Language
Understanding Evaluation (GLUE, Wang et al. (2018)) benchmark. GLUE contains nine NLU tasks,
including textual entailment, question answering, sentiment analysis, and text similarity. Details
about the benchmark are deferred to Appendix A.1.1."
NATURAL LANGUAGE UNDERSTANDING,0.18421052631578946,Published as a conference paper at ICLR 2022
NATURAL LANGUAGE UNDERSTANDING,0.1871345029239766,"Implementation Details. We implement our method using the MT-DNN code-base2. We follow the
suggested training and hyper-parameters settings from Liu et al. (2020). Specifically, we adopt Adam
and Adamax (Kingma & Ba, 2014) with corrected weight decay (Loshchilov & Hutter, 2018) as the
baseline optimizer and we set β = (0.9, 0.999). We use a linear-decay learning rate schedule, and
we apply SAGE to both Adam and Adamax."
NATURAL LANGUAGE UNDERSTANDING,0.19005847953216373,"We select learning rates in range of {1, 2, 3, 5, 8} × {10−5, 10−4}. We select β0 in range of [0.6, 0.9]
with an increment of 0.05. Other training details are reported in Appendix A.1.2."
NATURAL LANGUAGE UNDERSTANDING,0.19298245614035087,"Main results. Table 2 and Table 3 show the evaluation results on the GLUE benchmark. The dev
results are averaged over 5 different random seeds, and all gains are statistically significant3. We
select the best single task model for test evaluation."
NATURAL LANGUAGE UNDERSTANDING,0.195906432748538,"Model
Optimizer
RTE
MRPC
CoLA
SST-2
STS-B
QNLI
QQP
MNLI-m/mm
Average
Acc
Acc/F1
Mcc
Acc
P/S Corr
Acc
Acc/F1
Acc
Score"
NATURAL LANGUAGE UNDERSTANDING,0.19883040935672514,BERTBASE
NATURAL LANGUAGE UNDERSTANDING,0.20175438596491227,"Devlin et al. (2018)
-
-/86.7
-
92.7
-/-
88.4
-/-
84.4/-
-"
NATURAL LANGUAGE UNDERSTANDING,0.2046783625730994,"Adam
63.5
84.1/89.0
54.7
92.9
89.2/88.8
91.1
90.9/88.1
84.5/84.4
81.5
Adam-SAGE
73.3
87.0/90.9
60.3
93.5
90.3/89.9
91.7
91.2/88.1
84.7/84.8
84.0"
NATURAL LANGUAGE UNDERSTANDING,0.20760233918128654,"Adamax
69.2
86.2/90.4
57.8
92.9
89.7/89.2
91.2
90.9/88.0
84.5/84.4
82.8
Adamax-SAGE
74.0
87.3/91.0
59.7
93.8
90.3/89.8
91.8
91.2/88.2
85.0/85.2
84.2"
NATURAL LANGUAGE UNDERSTANDING,0.21052631578947367,RoBERTaLARGE
NATURAL LANGUAGE UNDERSTANDING,0.2134502923976608,"Liu et al. (2019b)
86.6
-/90.9
68.0
96.4
92.4/-
94.7
92.2/-
90.2/90.2
-"
NATURAL LANGUAGE UNDERSTANDING,0.21637426900584794,"Adamax
86.6
90.4/93.1
67.5
96.4
92.4/92.2
94.7
92.1/89.3
90.4/90.3
88.7
Adamax-SAGE
87.8
91.5/93.9
68.7
96.7
92.7/92.4
94.9
92.2/89.4
90.8/90.4
89.3"
NATURAL LANGUAGE UNDERSTANDING,0.21929824561403508,"Table 2: Single task fine-tuning dev results on GLUE. All results are from our implementations. ‘-’
denotes missing results."
NATURAL LANGUAGE UNDERSTANDING,0.2222222222222222,"Our method gains 1.4 on dev and 1.1 on test of the task-average score on BERT-base. In large
datasets, i.e., MNLI (392K) and QNLI (108K), SAGE improves around 0.5 points. In small datasets,
i.e., RTE (2.5K) and CoLA (8.5K), we obtain more than 2 points of improvements. Such observations
indicate that SAGE is very effective on the small datasets. Furthermore, SAGE improves upon
RoBERTa-large by 0.6 average scores, suggesting SAGE can still achieve significant improvements
for larger and more adequately pre-trained models than BERT-base."
NATURAL LANGUAGE UNDERSTANDING,0.22514619883040934,"RTE
MRPC
CoLA
SST-2
STS-B
QNLI
QQP
MNLI-m/mm
Average
Acc
F1
Mcc
Acc
P/S Corr
Acc
F1
Acc
Score"
NATURAL LANGUAGE UNDERSTANDING,0.22807017543859648,"BERTBASE (Devlin et al., 2018)
66.4
88.9
52.1
93.5
85.8
90.5
71.2
84.6/83.4
79.6
BERTBASE, Adamax
66.8
88.6
54.0
93.4
86.6
90.6
71.1
84.7/83.6
79.9
BERTBASE, Adamax-SAGE
69.8
89.7
54.5
94.1
87.1
90.8
71.3
84.9/83.8
80.7"
NATURAL LANGUAGE UNDERSTANDING,0.2309941520467836,Table 3: Single task fine-tuning test results from the GLUE evaluation server.
NATURAL LANGUAGE UNDERSTANDING,0.23391812865497075,"Model
Optimizer
IWSLT’14 De-En
WMT’16 En-De"
NATURAL LANGUAGE UNDERSTANDING,0.23684210526315788,"TransformerBASE
Adam
34.5
27.3
Adam-SAGE
35.1
27.7"
NATURAL LANGUAGE UNDERSTANDING,0.23976608187134502,Table 4: Neural machine translation BLEU scores on test set. All results are from our implementation.
NATURAL LANGUAGE UNDERSTANDING,0.24269005847953215,"Model
Optimizer
CIFAR100
ImageNet"
NATURAL LANGUAGE UNDERSTANDING,0.24561403508771928,"ViT-B/32
SGD∗
91.97
81.28
SGD-SAGE
92.68
81.72"
NATURAL LANGUAGE UNDERSTANDING,0.24853801169590642,"ViT-L/32
SGD∗
93.04
80.99
SGD-SAGE
93.74
81.90"
NATURAL LANGUAGE UNDERSTANDING,0.25146198830409355,"Table 5: Image classification test accuracy. Results with ∗are from Dosovitskiy et al. (2020). ViT-
B/32 and ViT-L/32 each denotes ViT-base and ViT-large model with 32 × 32 input patch size."
NATURAL LANGUAGE UNDERSTANDING,0.2543859649122807,"2https://github.com/namisan/mt-dnn
3The dev results on RoBERTa-large are averaged over 3 different random seeds. All results have passed a
paired student t-test with p-values less than 0.05. The detailed statistics are summarized in Appendix A.1.3."
NATURAL LANGUAGE UNDERSTANDING,0.2573099415204678,Published as a conference paper at ICLR 2022
NEURAL MACHINE TRANSLATION,0.260233918128655,"4.2
NEURAL MACHINE TRANSLATION"
NEURAL MACHINE TRANSLATION,0.2631578947368421,"Model and Data. We evaluate SAGE on the Transformer-base NMT models (Vaswani et al., 2017)
using two widely used NMT datasets, IWSLT’14 De-En (Cettolo et al., 2015)4 and WMT’16 En-De
(Bojar et al., 2016)5. IWSLT’14 De-En is a low-resource dataset, which contains 160K sentence
pairs. WMT’16 En-De is a rich-resource dataset, which contains 4.5M sentence pairs. Dataset and
pre-processing details are deferred to Appendix A.2.1."
NEURAL MACHINE TRANSLATION,0.26608187134502925,"Implementation Details. We implement the algorithms using the fairseq code-base and follow the
training and hyper-parameters settings from Ott et al. (2018; 2019). Specifically, we adopt the inverse
square root learning rate schedule and we employ Adam (Kingma & Ba, 2014) as the optimizer with
β = (0.9, 0.98). We apply SAGE to the same setting."
NEURAL MACHINE TRANSLATION,0.26900584795321636,"We select learning rates in range of {5, 7} × 10−5 ∪{1, 2} × 10−4 and select β0 in range of
{0.5, 0.6, 0.7, 0.8, 0.9}. Comprehensive training details are reported in Appendix A.2.2."
NEURAL MACHINE TRANSLATION,0.2719298245614035,"Main results. Table 4 shows the BLEU scores on the IWSLT’14 De-En and the WMT’16 En-De
test set, where SAGE improves around 0.6 and 0.4 points, respectively. This suggests that other
than fine-tuning, SAGE can also improve the generalization of trained-from-scratch models in both
low-resource and rich-resource settings."
IMAGE CLASSIFICATION,0.27485380116959063,"4.3
IMAGE CLASSIFICATION"
IMAGE CLASSIFICATION,0.2777777777777778,"Model and data. We evaluate SAGE using Vision Transformer models (ViT) on the CIFAR100
(Krizhevsky et al., 2009) and ILSVRC-2012 ImageNet dataset (Deng et al., 2009). Specifically, we
evaluate the fine-tuning performance of the ViT-base and ViT-large pre-trained using ImageNet-21k,
a superset of ImageNet dataset with 21k classes and 14M images. Data and pre-processing details
are deferred to Appendix A.3.1."
IMAGE CLASSIFICATION,0.2807017543859649,"Implementation details. All experiments follow the suggested training configuration of Dosovitskiy
et al. (2020) and a jax-implemented code base 6. We adopt SGD as the baseline optimizer with
a momentum factor 0.9. We fine-tune the models for 100K steps for CIFAR100, and 200K steps
for ImageNet. We select learning rates in range of {0.02, 0.05, 0.08, 0.1} and select β0 in range of
{0.85, 0.90, 0.95}. Comprehensive training details are reported in Appendix A.3.2."
IMAGE CLASSIFICATION,0.28362573099415206,"Main results. Table 5 shows the evaluation results on CIFAR100 and ImageNet. SAGE outperforms
baselines by a significant margin. This demonstrates that SAGE is quite general, and can be applied
to various tasks (e.g., NLP and computer vision) and optimizers (e.g., Adam, Adamax and SGD)."
ANALYSIS,0.28654970760233917,"5
ANALYSIS"
ANALYSIS,0.2894736842105263,"We verify that SAGE leads to more sufficient training (Section 5.1), better generalization performance
(Section 5.2), and is complementary to existing state-of-the-art regularization methods (Section 5.3).
We also provide ablation studies in Appendix A.4.4."
SAGE LEADS TO MORE SUFFICIENT TRAINING,0.29239766081871343,"5.1
SAGE LEADS TO MORE SUFFICIENT TRAINING"
SAGE LEADS TO MORE SUFFICIENT TRAINING,0.2953216374269006,"Recall that SAGE adjusts the learning rate for each parameter according to two factors: the sensitivity
of parameters and the local temporal variation of sensitivity. By inspecting these factors, we verify
that SAGE leads to more sufficient training."
SAGE LEADS TO MORE SUFFICIENT TRAINING,0.2982456140350877,"The sensitivity distribution is more concentrated. Figure 2 shows the sensitivity distribution of
parameters in the SAGE optimized models and the baseline models. We select the hyper-parameters
that yield the best generalization performance on the BERT-base model, and we evaluate the sensitivity
of each parameter using the entire training set. See Appendix A.4.2 for implementation details."
SAGE LEADS TO MORE SUFFICIENT TRAINING,0.30116959064327486,"We observe that the sensitivity distribution exhibits a lower variance in the SAGE optimized models
than the baseline models. This suggests that the sensitivity of parameters becomes more concentrated.
In other words, the amount of each parameter’s contribution is more balanced, and the model is more
sufficiently trained."
SAGE LEADS TO MORE SUFFICIENT TRAINING,0.30409356725146197,"4https://wit3.fbk.eu/
5http://data.statmt.org/wmt16/translation-task/
6https://github.com/google-research/vision_transformer"
SAGE LEADS TO MORE SUFFICIENT TRAINING,0.30701754385964913,Published as a conference paper at ICLR 2022 MNLI 0.0 0.5 1.0 1.5
SAGE LEADS TO MORE SUFFICIENT TRAINING,0.30994152046783624,Distribution of Sensitivity 1e 6
SAGE LEADS TO MORE SUFFICIENT TRAINING,0.3128654970760234,Schedule
SAGE LEADS TO MORE SUFFICIENT TRAINING,0.3157894736842105,"Adam 
Adam-SAGE QNLI 0.0 0.5 1.0 1.5 1e 6 SST-2 0 2 4 1e 8 MRPC 0 2 4 6 1e 8"
SAGE LEADS TO MORE SUFFICIENT TRAINING,0.31871345029239767,"Figure 2: The sensitivity distribution of the BERT-base models fine-tuned on GLUE tasks. Note that
we drop some outliers to ease visualization."
SAGE LEADS TO MORE SUFFICIENT TRAINING,0.3216374269005848,"Even the most redundant parameters contribute to the model performance. Recall that sensitivity
is a type of importance score in pruning, which is a straightforward approach to measure each
parameter’s contribution. Therefore, we conduct an unstructured, one-shot pruning experiment on
the fine-tuned BERT-base models. Specifically, we remove up to 40% parameters7 with the lowest
sensitivity scores and evaluate the pruned models’ performance. We average the results over 5
models trained with different random seeds. Figure 3 Upper shows the generalization performance
of the pruned models. To ease the comparison, Figure 3 Lower shows the change in generalization
performance with respect to the un-pruned models."
SAGE LEADS TO MORE SUFFICIENT TRAINING,0.32456140350877194,"100
90 
 80 
 70 
Percentage of Weight Remaining (%) 88 90 92 94 96"
SAGE LEADS TO MORE SUFFICIENT TRAINING,0.32748538011695905,Accuracy SST-2 77.5 80.0 82.5 85.0 87.5 90.0 MRPC 60 65 70 75
"RTE
SCHEDULE",0.3304093567251462,"80
RTE
Schedule"
"RTE
SCHEDULE",0.3333333333333333,"Adam 
Adam-SAGE"
"RTE
SCHEDULE",0.3362573099415205,"100
100
90 
 80 
 70 
Percentage of Weight Remaining (%)
90 
 80 
 70 
Percentage of Weight Remaining (%)"
"RTE
SCHEDULE",0.3391812865497076,"100
90 
 80 
 70 
Percentage of Weight Remaining (%) 6 4 2 0 2"
"RTE
SCHEDULE",0.34210526315789475,Change in Accuracy 10.0 7.5 5.0 2.5 0.0 2.5 15 10 5 0 5
"RTE
SCHEDULE",0.34502923976608185,"100
90 
 80 
 70 
Percentage of Weight Remaining (%)
100
90 
 80 
 70 
Percentage of Weight Remaining (%)"
"RTE
SCHEDULE",0.347953216374269,"Figure 3: Upper: Model generalization performance at different pruning ratios; Lower: Change in
generalization performance with respect to the full model. Pruning is conducted on the fine-tuned
BERT-base models."
"RTE
SCHEDULE",0.3508771929824561,We have the following observations:
"RTE
SCHEDULE",0.3538011695906433,"• The pruning performance of the SAGE optimized models remains higher than that of the baseline
models (Figure 3 Upper)."
"RTE
SCHEDULE",0.3567251461988304,"• Even the most redundant parameters in the SAGE optimized models makes contributions (Figure 3
Lower). When there are over 80% of weights remaining, the pruning performance of the baseline
models is comparable or even superior than their un-pruned alternatives. In contrast, the performance
of the SAGE optimized models consistently deteriorates. This suggests that the most redundant
parameters in the baseline models fail to contribute, while those in the SAGE optimized models are
trained more sufficiently and are able to make contributions."
"RTE
SCHEDULE",0.35964912280701755,"Sensitivity is a reliable indicator of redundancy. We visualize the local temporal variation (Figure 4)
to verify that sensitivity indeed becomes a more reliable indicator of redundancy in SAGE than in the
baselines. We track the variation for all parameters in the BERT-base model at each iteration, and"
"RTE
SCHEDULE",0.36257309941520466,7Embedding weights are excluded.
"RTE
SCHEDULE",0.3654970760233918,Published as a conference paper at ICLR 2022
"RTE
SCHEDULE",0.3684210526315789,"we evaluate the variation based on the current mini-batch of training data. See Appendix A.4.2 for
implementation details."
"RTE
SCHEDULE",0.3713450292397661,"0
5
10
15
20
25
30
35
MNLI 1 2 3 4 5"
"RTE
SCHEDULE",0.3742690058479532,Local Temporal Variation 1e 7
"RTE
SCHEDULE",0.37719298245614036,"Schedule
Adam 
Adam-SAGE"
"RTE
SCHEDULE",0.38011695906432746,"0
2
4
6
8
10
QNLI 1 2 3 4 5 1e 7"
"RTE
SCHEDULE",0.3830409356725146,"0
2
4
6
8
10
12
SST-2 
 Number of Training Updates (K) 0 2 4 6"
"RTE
SCHEDULE",0.38596491228070173,Local Temporal Variation 1e 7
"RTE
SCHEDULE",0.3888888888888889,"0.0
0.5
1.0
1.5
2.0
2.5
MRPC 
 Number of Training Updates (K) 0 2 4"
"RTE
SCHEDULE",0.391812865497076,"6
1e 7"
"RTE
SCHEDULE",0.39473684210526316,"Figure 4: The local temporal variation of sensitivity (with β0 = 0.7) during training.
We observe that the local temporal variation in SAGE remains lower or decreases faster than in the
baselines for all tasks. For example, the variation in the baseline approach remains large in QNLI. In
contrast, the variation in SAGE decreases, suggesting the sensitivity indeed stabilizes and becomes a
reliable indicator of redundancy."
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.39766081871345027,"5.2
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE"
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.40058479532163743,"We verify that SAGE leads to better generalization performance through inspecting the learning
curves, decision boundary and hyper-parameter search space."
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.40350877192982454,"Figure 5: Decision boundary predicted on the
Spiral dataset. The white curve on Adam-
SAGE corresponds the decision boundary of
Adam."
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4064327485380117,"Learning Curves. Figure 6 shows the training loss,
validation loss, learning rate, and sensitivity score
obtained by fine-tuning BERT-base on SST-2. All
experiment details are deferred to Appendix A.4.3.
We have two major observations: 1) SAGE’s valida-
tion loss descends faster and SAGE is less prone to
overfitting. This observation suggests that SAGE has
a regularization effect and reduces the model vari-
ance. 2) SAGE’s variance of the sensitivity score
becomes lower through training, aligning with our
observation in Figure 2. This suggests that SAGE
gives rise to a more balanced and sufficient training.
Both observations agree with our initial motivation
(Figure 1) that redundancy elimination can lead to
better generalization."
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4093567251461988,"0
5000
10000
Training Step 0.1 0.2 0.3 0.4 0.5"
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.41228070175438597,Training Loss
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4152046783625731,Schedule
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.41812865497076024,"Adam 
Adam-SAGE"
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.42105263157894735,"0
5000
10000
Training Step 0.3 0.4 0.5"
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4239766081871345,Validation Loss
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4269005847953216,"0
5000
10000
Training Step"
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4298245614035088,0.0000
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4327485380116959,0.0001
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.43567251461988304,0.0002
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.43859649122807015,0.0003
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4415204678362573,Learning Rate
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4444444444444444,"0
5000
10000
Training Step 0.04 0.05 0.06 0.07 0.08"
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4473684210526316,Sensitivity Score
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4502923976608187,Figure 6: Learning curves obtained by fine-tuning BERT-base on SST-2 dataset.
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.45321637426900585,"Hyper-parameter Study. Figure 7 shows the validation accuracy heatmap obtained by fine-tuning
BERT-base on the RTE dataset. We plot the accuracy obtained by training with different learning rates,
Adam’s βs and SAGE’s β0s. We can observe that SAGE consistently achieves a better generalization
performance within a larger region of hyper-parameter search space under different β0s. We also
provide a hyper-parameter study for more datasets in Appendix A.4.5."
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.45614035087719296,Published as a conference paper at ICLR 2022
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4590643274853801,"1e-5
8e-5
2e-4
Learning Rate 
 Adam 0.95 0.85 0.75 0.65 0.55 0.45"
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4619883040935672,"β = (·, 0.999)"
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4649122807017544,"1e-5
8e-5
2e-4
Learning Rate 
 Adam-SAGE, β0 = 0.8"
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4678362573099415,"1e-5
8e-5
2e-4
Learning Rate 
 Adam-SAGE, β0 = 0.7"
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.47076023391812866,"1e-5
8e-5
2e-4
Learning Rate 
 Adam-SAGE, β0 = 0.6 55 60 65 70 55 60 65 70 55 60 65 70 55 60 65 70"
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.47368421052631576,"Figure 7: Validation accuracy obtained by fine-tuning BERT-base on RTE dataset with a wide range
of hyper-parameters."
SAGE LEADS TO BETTER GENERALIZATION PERFORMANCE,0.4766081871345029,"Decision Boundary. Figure 5 shows the decision boundary predicted with Adam and SAGE on the
Spiral dataset. Specifically, we train a multi-layer perceptron with 3 hidden layers, each with a hidden
dimension of 100. The decision boundary predicted with SAGE is smoother and has a larger margin
than with Adam, suggesting SAGE produces a better generalized model."
COMBINE WITH STATE-OF-THE-ART METHODS,0.47953216374269003,"5.3
COMBINE WITH STATE-OF-THE-ART METHODS"
COMBINE WITH STATE-OF-THE-ART METHODS,0.4824561403508772,"We further show that SAGE is complementary to existing state-of-the-art regularization methods.
Specifically, we apply SAGE to SMART (Jiang et al., 2019), a state-of-the-art smoothness-inducing
adversarial regularization method. As shown in Table 6, SAGE can further improve upon SMART,
suggesting the two techniques are complementary."
COMBINE WITH STATE-OF-THE-ART METHODS,0.4853801169590643,"Model
Optimizer
RTE
MRPC
CoLA
SST-2
STS-B
QNLI
QQP
MNLI-m/mm
Average
Acc
Acc/F1
Mcc
Acc
P/S Corr
Acc
Acc/F1
Acc
Score"
COMBINE WITH STATE-OF-THE-ART METHODS,0.48830409356725146,"BERTBASE
Adamax
69.2
86.2/90.4
57.8
92.9
89.7/89.2
91.2
90.9/88.0
84.5/84.4
82.8"
COMBINE WITH STATE-OF-THE-ART METHODS,0.49122807017543857,"SMARTBASE
Adamax
72.5
87.7/91.4
59.5
93.5
90.0/89.6
91.9
91.7/88.9
85.2/85.7
84.1
Adamax-SAGE
75.1
89.0/92.8
60.8
94.3
90.1/89.7
92.2
91.9/89.1
85.9/86.0
85.0"
COMBINE WITH STATE-OF-THE-ART METHODS,0.49415204678362573,Table 6: Single task fine-tuning dev results on GLUE.
DISCUSSION,0.49707602339181284,"6
DISCUSSION"
DISCUSSION,0.5,"SAGE is complementary to Adaptive Gradient Methods. Our proposed method and the mainstream
adaptive gradient methods (e.g., Adam and AdaGrad) are for fundamentally different purposes. The
mainstream adaptive gradient methods aim to improve optimization by adapting to the optimization
landscape, while SAGE aims to improve generalization by eliminating the weight redundancy. The
quantities of our interest (i.e., Eq. (2) and Eq. (3)) are related to the weight redundancy. They are
not directly related to the moduli of the objective function, e.g., smoothness, curvature (which are of
the interests for optimization). As shown in our experiments (See Section 4), we do not observe any
conflicts between the two methods, as SAGE improves the model generalization performance when
being combined with several adaptive gradient methods (e.g., Adam)."
DISCUSSION,0.5029239766081871,"Redundant Weights vs. Insufficiently Trained Weights. Lottery Ticket Hypothesis (Frankle
& Carbin, 2018) suggests that, in a randomly initialized network, there exists a well-initialized
subnetwork, which outperforms any other subnetworks and matches the full model’s performance.
This suggests the rest parameters contribute marginally to the model performance. Although the
initialization of these parameters may not be satisfactory, SAGE provides them sufficient training so
that they can learn to contribute."
CONCLUSION,0.5058479532163743,"7
CONCLUSION"
CONCLUSION,0.5087719298245614,"We begin with a hypothesis that the redundant parameters can become useful if they are sufficiently
trained by desirable optimization strategies. We verify this hypothesis by proposing an adaptive
learning schedule – SAGE, which excites the under-fitted parameters to reduce redundancy and
regularize the well-fitted parameters to prevent overfitting. We demonstrate that SAGE can benefit
model generalization in a wide range of tasks and strengthen various types of optimizers."
CONCLUSION,0.5116959064327485,Published as a conference paper at ICLR 2022
REFERENCES,0.5146198830409356,REFERENCES
REFERENCES,0.5175438596491229,"Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. The second PASCAL
recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges
Workshop on Recognising Textual Entailment, 01 2006."
REFERENCES,0.52046783625731,"Brian R Bartoldson, Ari S Morcos, Adrian Barbu, and Gordon Erlebacher. The generalization-stability
tradeoff in neural network pruning. arXiv preprint arXiv:1906.03728, 2019."
REFERENCES,0.5233918128654971,"Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The fifth
pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference (TAC’09),
2009."
REFERENCES,0.5263157894736842,"Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias
Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, et al. Findings
of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine
Translation: Volume 2, Shared Task Papers, pp. 131–198, 2016."
REFERENCES,0.5292397660818714,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.5321637426900585,"Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1:
Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the
11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1–14, 2017."
REFERENCES,0.5350877192982456,"Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa Bentivogli, Roldano Cattoni, and Marcello
Federico. The iwslt 2015 evaluation campaign. In IWSLT 2015, International Workshop on Spoken
Language Translation, 2015."
REFERENCES,0.5380116959064327,"Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and
Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. arXiv preprint
arXiv:2007.12223, 2020."
REFERENCES,0.5409356725146199,"Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment
challenge. In Proceedings of the First International Conference on Machine Learning Chal-
lenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual
Entailment, MLCW’05, pp. 177–190, Berlin, Heidelberg, 2006. Springer-Verlag. ISBN 3-540-
33427-0, 978-3-540-33427-9. doi: 10.1007/11736790_9. URL http://dx.doi.org/10.
1007/11736790_9."
REFERENCES,0.543859649122807,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.5467836257309941,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.5497076023391813,"Xiaohan Ding, Guiguang Ding, Xiangxin Zhou, Yuchen Guo, Jungong Han, and Ji Liu. Global sparse
momentum sgd for pruning very deep neural networks. arXiv preprint arXiv:1909.12778, 2019."
REFERENCES,0.5526315789473685,"William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005."
REFERENCES,0.5555555555555556,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020."
REFERENCES,0.5584795321637427,"John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011."
REFERENCES,0.5614035087719298,"Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with
structured dropout. arXiv preprint arXiv:1909.11556, 2019."
REFERENCES,0.564327485380117,Published as a conference paper at ICLR 2022
REFERENCES,0.5672514619883041,"Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020."
REFERENCES,0.5701754385964912,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635, 2018."
REFERENCES,0.5730994152046783,"Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing
textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pp. 1–9, Prague, June 2007. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/W07-1401."
REFERENCES,0.5760233918128655,Peter D Grünwald and Abhijit Grunwald. The minimum description length principle. 2007.
REFERENCES,0.5789473684210527,"Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert
with disentangled attention. arXiv preprint arXiv:2006.03654, 2020."
REFERENCES,0.5818713450292398,"Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture
6a overview of mini-batch gradient descent. Cited on, 14(8), 2012."
REFERENCES,0.5847953216374269,"Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic bert
with adaptive width and depth. arXiv preprint arXiv:2004.04037, 2020."
REFERENCES,0.5877192982456141,"Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. Smart:
Robust and efficient fine-tuning for pre-trained natural language models through principled regu-
larized optimization. arXiv preprint arXiv:1911.03437, 2019."
REFERENCES,0.5906432748538012,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.5935672514619883,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.5964912280701754,"Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing systems, pp. 598–605, 1990."
REFERENCES,0.5994152046783626,"Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning
based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018."
REFERENCES,0.6023391812865497,"Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao,
and Weizhu Chen. Super tickets in pre-trained language models: From model compression to
improving generalization. arXiv preprint arXiv:2105.12002, 2021."
REFERENCES,0.6052631578947368,"Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019a."
REFERENCES,0.6081871345029239,"Xiaodong Liu, Yu Wang, Jianshu Ji, Hao Cheng, Xueyun Zhu, Emmanuel Awa, Pengcheng He,
Weizhu Chen, Hoifung Poon, Guihong Cao, et al. The microsoft toolkit of multi-task deep neural
networks for natural language understanding. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics: System Demonstrations, pp. 118–126, 2020."
REFERENCES,0.6111111111111112,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019b."
REFERENCES,0.6140350877192983,"Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of
network pruning. arXiv preprint arXiv:1810.05270, 2018."
REFERENCES,0.6169590643274854,Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. 2018.
REFERENCES,0.6198830409356725,"Ekdeep Singh Lubana and Robert P Dick. A gradient flow framework for analyzing network pruning.
arXiv preprint arXiv:2009.11839, 2020."
REFERENCES,0.6228070175438597,"Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? arXiv
preprint arXiv:1905.10650, 2019."
REFERENCES,0.6257309941520468,Published as a conference paper at ICLR 2022
REFERENCES,0.6286549707602339,"Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016."
REFERENCES,0.631578947368421,"Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation
for neural network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 11264–11272, 2019."
REFERENCES,0.6345029239766082,"Ari S Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian. One ticket to win them all: general-
izing lottery ticket initializations across datasets and optimizers. arXiv preprint arXiv:1906.02773,
2019."
REFERENCES,0.6374269005847953,"Michael C Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a
network via relevance assessment. In Advances in neural information processing systems, pp.
107–115, 1989."
REFERENCES,0.6403508771929824,"Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation.
arXiv preprint arXiv:1806.00187, 2018."
REFERENCES,0.6432748538011696,"Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of
NAACL-HLT 2019: Demonstrations, 2019."
REFERENCES,0.6461988304093568,"Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016. Association for
Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.org/
anthology/D16-1264."
REFERENCES,0.6491228070175439,"Carl Edward Rasmussen and Zoubin Ghahramani. Occam’s razor. Advances in neural information
processing systems, pp. 294–300, 2001."
REFERENCES,0.652046783625731,"Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pp. 400–407, 1951."
REFERENCES,0.6549707602339181,"Victor Sanh, Thomas Wolf, and Alexander M Rush. Movement pruning: Adaptive sparsity by
fine-tuning. arXiv preprint arXiv:2005.07683, 2020."
REFERENCES,0.6578947368421053,"Rico Sennrich, Barry Haddow, and Alexandra Birch. Edinburgh neural machine translation systems
for wmt 16. arXiv preprint arXiv:1606.02891, 2016."
REFERENCES,0.6608187134502924,"Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 conference on empirical methods in natural language processing, pp.
1631–1642, 2013."
REFERENCES,0.6637426900584795,"Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Huszár. Faster gaze prediction with
dense networks and fisher pruning. arXiv preprint arXiv:1801.05787, 2018."
REFERENCES,0.6666666666666666,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017."
REFERENCES,0.6695906432748538,"Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head
self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint
arXiv:1905.09418, 2019."
REFERENCES,0.672514619883041,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint
arXiv:1804.07461, 2018."
REFERENCES,0.6754385964912281,"Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models. arXiv
preprint arXiv:1910.04732, 2019."
REFERENCES,0.6783625730994152,"Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.
Transactions of the Association for Computational Linguistics, 7:625–641, 2019."
REFERENCES,0.6812865497076024,Published as a conference paper at ICLR 2022
REFERENCES,0.6842105263157895,"Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers), pp. 1112–1122. Association for Computational Linguistics, 2018.
URL http://aclweb.org/anthology/N18-1101."
REFERENCES,0.6871345029239766,"Xia Xiao, Zigeng Wang, and Sanguthevar Rajasekaran. Autoprune: Automatic network pruning by
regularizing auxiliary parameters. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.6900584795321637,"Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012."
REFERENCES,0.6929824561403509,Published as a conference paper at ICLR 2022
REFERENCES,0.695906432748538,"A
APPENDIX"
REFERENCES,0.6988304093567251,"A.1
NATURAL LANGUAGE UNDERSTANDING"
REFERENCES,0.7017543859649122,"A.1.1
DATA"
REFERENCES,0.7046783625730995,"GLUE is a collection of nine NLU tasks. The benchmark includes question answering (Rajpurkar
et al., 2016), linguistic acceptability (CoLA, Warstadt et al. 2019), sentiment analysis (SST, Socher
et al. 2013), text similarity (STS-B, Cer et al. 2017), paraphrase detection (MRPC, Dolan & Brockett
2005), and natural language inference (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. 2006;
Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018) tasks. Details of the GLUE
benchmark, including tasks, statistics, and evaluation metrics, are summarized in Table 13."
REFERENCES,0.7076023391812866,"All the texts were tokenized using wordpieces, and were chopped to spans no longer than 512 tokens."
REFERENCES,0.7105263157894737,"A.1.2
TRAINING DETAILS"
REFERENCES,0.7134502923976608,"To fine-tune BERT-base and RoBERTa-large models on individual tasks, we append a task-specific
fully-connected classification layer to them as in Devlin et al. (2018)."
REFERENCES,0.716374269005848,"Table 7 present the hyper-parameter configurations. We tune this set of hyper-parameters on a single
seed, and report the averaged results obtained with the same configuration over all seeds. For SAGE
experiments, We slightly tune β0 within a range of 0.1 on different seeds. We apply a linear weight
decay rate of 0.01 and a gradient norm clipping threshold of 1 for all experiments. All experiments
are conducted on Nvidia V100 GPUs."
REFERENCES,0.7192982456140351,"Hyper-param
Experiment
RTE MRPC CoLA SST-2 STS-B QNLI QQP MNLI"
REFERENCES,0.7222222222222222,Learning Rate
REFERENCES,0.7251461988304093,"BERTBASE, Adam
1e-5
1e-5
1e-5
1e-5
1e-5
1e-5
2e-5
2e-5
BERTBASE, Adam-SAGE
1e-4
8e-5
8e-5
3e-5
1e-4
8e-5
4e-5
5e-5
BERTBASE, Adamax
1e-4
1e-4
1e-4
5e-5
1e-4
1e-4
1e-4
8e-5
BERTBASE, Adamax-SAGE
3e-4
3e-4
2e-4
2e-4
5e-4
5e-4
3e-4
2e-4
RoBERTaLARGE, Adamax
5e-5
5e-5
3e-5
1e-5
5e-5
1e-5
1e-4
1e-5
RoBERTaLARGE, Adamax-SAGE
6e-5
2e-4
8e-5
2e-5
8e-5
3e-5
2e-4
8e-5 β0"
REFERENCES,0.7280701754385965,"BERTBASE, Adam-SAGE
0.60
0.80
0.70
0.80
0.60
0.70
0.75
0.70
BERTBASE, Adamax-SAGE
0.65
0.80
0.75
0.70
0.75
0.70
0.75
0.85
RoBERTaLARGE, Adamax-SAGE
0.75
0.65
0.70
0.75
0.80
0.80
0.65
0.60"
REFERENCES,0.7309941520467836,"Batch Size
BERTBASE
16
8
32
32
32
32
32
32
RoBERTaLARGE
16
8
32
32
32
32
32
32"
REFERENCES,0.7339181286549707,"Epoch
BERTBASE
6
6
6
6
6
3
6
3
RoBERTaLARGE
15
6
6
6
10
10
15
3"
REFERENCES,0.7368421052631579,"Dropout
BERTBASE
0.1
0.1
0.1
0.1
0.1
0.1
0.0
0.3
RoBERTaLARGE
0.1
0.1
0.1
0.1
0.1
0.1
0.0
0.3"
REFERENCES,0.7397660818713451,"Warmup
BERTBASE
0.1
0.1
0.1
0.1
0.1
0.1
0.0
0.1
RoBERTaLARGE
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1"
REFERENCES,0.7426900584795322,"Table 7: Hyper-parameter configurations for GLUE experiments. “Epoch” refers to the total training
epochs; we adopt early-stopping strategy in practice. “Dropout” refers to classification layer dropout
ratio. “Warmup” refers to the ratio of learning rate linear warmup iterations to total training iterations."
REFERENCES,0.7456140350877193,"A.1.3
EVALUATION RESULTS"
REFERENCES,0.7485380116959064,Statistics of the dev set results. Table 8 shows the standard deviation of the dev set results.
REFERENCES,0.7514619883040936,"Average score computation formula. For dev set results, we first obtain a score for each task by
averaging the scores of all metrics (e.g., Acc and F1) and test sets (e.g., MNLI-m and MNLI-mm)
within this task, then compute a task-average score. For test set results, we directly averages scores of
all reported metrics following Devlin et al. (2018)."
REFERENCES,0.7543859649122807,Published as a conference paper at ICLR 2022
REFERENCES,0.7573099415204678,"Model
Optimizer
RTE MRPC CoLA SST-2 STS-B QNLI QQP MNLI"
REFERENCES,0.7602339181286549,"BERTBASE
Adam-SAGE
0.35
0.32
0.85
0.25
0.12
0.06
0.05
0.06"
REFERENCES,0.7631578947368421,"Adamax-SAGE
0.56
0.69
0.12
0.23
0.03
0.06
0.08
0.10"
REFERENCES,0.7660818713450293,"RoBERTaLARGE
Adamax-SAGE
0.51
0.78
0.50
0.19
0.08
0.00
0.05
0.05"
REFERENCES,0.7690058479532164,Table 8: Standard deviation of the dev set results.
REFERENCES,0.7719298245614035,"A.2
NEURAL MACHINE TRANSLATION"
REFERENCES,0.7748538011695907,"A.2.1
DATA"
REFERENCES,0.7777777777777778,"Table 9 shows the number of sentence pairs in each dataset. We use the standard newstest-2013 and
newstest-2014 as dev and test set for WMT’16 En-De. We follow Ott et al. (2019) to split the dev/test
sets for IWSLT’14 De-En."
REFERENCES,0.7807017543859649,"All datasets are encoded using byte-pair encoding (BPE, Sennrich et al. (2016)). We preprocess
IWSLT’14 De-En data following fairseq8 and adopt the preprocessed WMT’16 En-De from Google9."
REFERENCES,0.783625730994152,"Data
Train Dev
Test"
REFERENCES,0.7865497076023392,"IWSLT’14 De-En
160K 7283 6750
WMT’16 En-De
4.5M 1061 1019"
REFERENCES,0.7894736842105263,Table 9: The number of parallel sentences in NMT datasets.
REFERENCES,0.7923976608187134,"A.2.2
TRAINING DETAILS"
REFERENCES,0.7953216374269005,"We adopt the Transformer-base model for both datasets. For IWSLT’14 De-En, we share the decoder
and encoder output embeddings. For WMT’16 En-De, we share all the embeddings."
REFERENCES,0.7982456140350878,"Table 10 presents the hyper-parameter configurations for the best models. We apply a linear weight
decay rate of 1 × 10−4 and a label smoothing ratio of 0.1 for all experiments. All experiments are
conducted on Nvidia V100 GPUs."
REFERENCES,0.8011695906432749,"For IWSLT’14 De-En, we report the BLEU score of the best checkpoint using a beam size of 5 and
length penalty of 1. For WMT’16 En-De, we report the average of the last 10 checkpoints with a
beam size of 4 and length penalty of 0.6."
REFERENCES,0.804093567251462,"Hyper-param
Experiment
IWSLT’14 De-En WMT’16 En-De"
REFERENCES,0.8070175438596491,"Learning Rate
Adam
5e-4
7e-4
Adam-SAGE
1e-3
2e-3"
REFERENCES,0.8099415204678363,"β0
Adam-SAGE
0.8
0.4"
REFERENCES,0.8128654970760234,"Batch size
Both
4096
32768"
REFERENCES,0.8157894736842105,"Epoch
Both
60
40"
REFERENCES,0.8187134502923976,"Dropout
Both
0.3
0.1"
REFERENCES,0.8216374269005848,"Warmup
Both
8000
4000"
REFERENCES,0.8245614035087719,"Table 10: Hyper-parameter configurations for NMT experiments. “Warmup” refers to the learning
rate linear warmup iterations."
REFERENCES,0.827485380116959,"8https://github.com/pytorch/fairseq/blob/master/examples/translation
9https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/datasets/wmt.html"
REFERENCES,0.8304093567251462,Published as a conference paper at ICLR 2022
REFERENCES,0.8333333333333334,"A.3
IMAGE CLASSIFICATION"
REFERENCES,0.8362573099415205,"A.3.1
DATA"
REFERENCES,0.8391812865497076,"For CIFAR100, we apply random cropping and random horizontal flipping to the training data."
REFERENCES,0.8421052631578947,"A.3.2
TRAINING DETAILS"
REFERENCES,0.8450292397660819,"Table 11 present the hyper-parameter configurations for the best models. All experiments are
conducted on Nvidia V100 GPUs."
REFERENCES,0.847953216374269,"Hyper-param
Experiment
CIFAR100 ImageNet"
REFERENCES,0.8508771929824561,"Learning Rate
ViT-B/32, SGD-SAGE
0.02
0.05
ViT-L/32, SGD-SAGE
0.02
0.08"
REFERENCES,0.8538011695906432,"β0
ViT-B/32, SGD-SAGE
0.95
0.95
ViT-L/32, SGD-SAGE
0.85
0.95"
REFERENCES,0.8567251461988304,"Training Steps
All
10000
20000"
REFERENCES,0.8596491228070176,"Dropout
All
0.0
0.0"
REFERENCES,0.8625730994152047,Table 11: Hyper-parameter configurations for ViT experiments on CIFAR100 and ImageNet.
REFERENCES,0.8654970760233918,"A.4
SUPPLEMENTS FOR METHOD AND ANALYSIS"
REFERENCES,0.868421052631579,"A.4.1
ADAM-SAGE ALGORITHM"
REFERENCES,0.8713450292397661,Algorithm 2 Adam-SAGE (⊙denotes Hadamard product and ⊘denotes Hadamard division)
REFERENCES,0.8742690058479532,"Input: Model parameters Θ ∈RJ; Data D; Learning rate schedule η(·); Total training iteration T;
Moving average coefficient β0, β1, β2.
1: Initialize bI(0), m(0), v(0) = 0 ∈RJ.
2: for t = 1, ..., T do
3:
Sample a minibatch b(t) from D.
4:
Compute gradient g(t) = ∇Θ(t)L(b(t), Θ(t)).
5:
Compute sensitivity I(t) = |Θ(t) ⊙g(t)|.
6:
m(t) = β1m(t−1) + (1 −β1)g(t)"
REFERENCES,0.8771929824561403,"7:
v(t) = β2v(t−1) + (1 −β2)(g(t))2"
REFERENCES,0.8801169590643275,"8:
bI(t) = β0bI(t−1) + (1 −β0)I(t).
9:
bm(t) = m(t)/(1 −β1)
10:
bv(t) = v(t)/(1 −β2)
11:
bI(t) = bI(t)/(1 −β0)
12:
U (t) = |I(t) −bI(t)|.
13:
Update Θ(t+1) = Θ(t) −η(t)((U (t) + ϵ) ⊙bm(t)) ⊘((bI(t) + ϵ) ⊙(
√"
REFERENCES,0.8830409356725146,"bv(t) + ϵ)) ⊙g(t).
14: end for"
REFERENCES,0.8859649122807017,"A.4.2
IMPLEMENTATION DETAILS FOR SECTION 5.1"
REFERENCES,0.8888888888888888,"Figure 2 experiments: Due to the extremely large model size, we only sample 110K parameters per
layer (in total 12×110K parameters) to calculate the distribution. We select the hyper-parameters that
yield the best generalization performance on the BERT-base model, and we evaluate the sensitivity of
each parameter using the entire training set."
REFERENCES,0.8918128654970761,"Figure 4 experiments: Following previous experiment’s practice, we randomly sample 110K parame-
ters per layer (in total 12 × 110K parameters), and for visualization purposes, we plot 60 randomly
selected iterations. We adopt the learning rate corresponding to the best training performance for
both SAGE and the baselines."
REFERENCES,0.8947368421052632,Published as a conference paper at ICLR 2022
REFERENCES,0.8976608187134503,"A.4.3
IMPLEMENTATION DETAILS FOR SECTION 5.2"
REFERENCES,0.9005847953216374,"Plotting the parameter sensitivity distribution throughout training can be computational expensive.
The distribution varies significantly throughout training and often fails to provide a meaningful
visualisation. As a result, we compute the structured sensitivity score instead of the parameter
sensitivity score. Specifically, we compute a single sensitivity score for each Transformer weight
block Θ at iteration t using the structured counterpart of the parameter sensitivity metric widely
adopted in the existing structured pruning literature (Michel et al., 2019; Liang et al., 2021). Following
common structured pruning practice, we split Transformer models into 12 feed-forward weight
modules and 12 multi-head attention weight modules, and plot the average and variance of the
sensitivity of these modules’ sensitivity scores throughout the training."
REFERENCES,0.9035087719298246,"We present the results obtained with the hyper-parameters that yield the best generalization perfor-
mance on the BERT-base model for both Adamax (Baseline) and Adamax-SAGE (SAGE)."
REFERENCES,0.9064327485380117,"A.4.4
ABLATION STUDY"
REFERENCES,0.9093567251461988,"To further interpret the role of the parameter sensitivity I and the local temporal variation U, we
conduct an ablation study on these two factors. Specifically, we check five variants of Eq. (4):"
REFERENCES,0.9122807017543859,"Variant 1.
η(t)
j
= η(t)(bI(t)
j
+ ϵ)(U (t)
j
+ ϵ)"
REFERENCES,0.9152046783625731,"Variant 2.
η(t)
j
= η(t)(bI(t)
j
+ ϵ)/(U (t)
j
+ ϵ)"
REFERENCES,0.9181286549707602,"Variant 3.
η(t)
j
= η(t)(bI(t)
j
+ ϵ)"
REFERENCES,0.9210526315789473,"Variant 4.
η(t)
j
= η(t)/(bI(t)
j
+ ϵ)"
REFERENCES,0.9239766081871345,"Variant 5.
η(t)
j
= η(t)(U (t)
j
+ ϵ)"
REFERENCES,0.9269005847953217,"For Variants 1,2 and 3, we aim to check the performance of giving a high/low-sensitive parameter a
high/low, instead of low/high learning rate. Specifically, we place (bI(t)
j
+ ϵ) in the numerator, so that
the learning rates increase for the high sensitive parameters and decrease for low sensitive parameters."
REFERENCES,0.9298245614035088,"For Variants 4 and 5, we aim to check the performance of eliminating the influence of one of these
factors. Specifically, we fix the local temporal variation term to 1 in Variant 4 and fix the sensitivity
term to 1 in Variant 5."
REFERENCES,0.9327485380116959,"A.4.5
HYPER-PARAMETER STUDY"
REFERENCES,0.935672514619883,"We investigate the influence of hyper-parameters learning rate and β0 on the performance of SAGE
(Figure 8). As can be seen, SAGE requires a larger learning rate than the baselines to offset the small
scale of the modulation term (the optimal baseline learning rate lies in 5 × 10−5 ∼1 × 10−4 for
MNLI, 5 × 10−4 ∼7 × 10−4 for IWSLT 14 De-En and 0.1 ∼0.2 for CIFAR10). Furthermore,
switching to a larger learning rate requires a lower β0 to maintain the same level of performance."
REFERENCES,0.9385964912280702,"0.6
0.7
0.8
0.9
β0
 MNLI 84.6 84.8 85.0 85.2 Acc"
REFERENCES,0.9415204678362573,Learning Rate
REFERENCES,0.9444444444444444,"2e-4
3e-4"
REFERENCES,0.9473684210526315,"0.76
0.78
0.80
0.82
0.84
β0
 IWSLT 14 De-En 34.6 34.7 34.8 34.9 35.0 35.1 BLEU"
REFERENCES,0.9502923976608187,Learning Rate
REFERENCES,0.9532163742690059,"8e-4
1e-3"
REFERENCES,0.956140350877193,"0.3
0.4
0.5
0.6
0.7
0.8
β0
 CIFAR 10 90.5 91.0 91.5 92.0 92.5 Acc"
REFERENCES,0.9590643274853801,Learning Rate
REFERENCES,0.9619883040935673,"0.3
0.4
0.5
0.6"
REFERENCES,0.9649122807017544,Figure 8: Parameter study on learning rate and β0.
REFERENCES,0.9678362573099415,"All five variants show no clear gain upon the baseline on both RTE and SST-2 datasets after careful
hyper-parameter tuning. Specifically, we observe that the Variants 1 and 3 converge very fast at the
early stage of training, and then quickly start overfitting. In Variants 2 and 4, the training collapses
due to gradient explosion or vanishing."
REFERENCES,0.9707602339181286,Published as a conference paper at ICLR 2022
REFERENCES,0.9736842105263158,"Variant Name
Learning Rate Modulating Term
RTE
SST-2"
REFERENCES,0.9766081871345029,"Adam
1
63.5
92.9
Adam-SAGE
(U (t)
j
+ ϵ)/(bI(t)
j
+ ϵ)
73.3
93.5"
REFERENCES,0.97953216374269,"Variant 1.
(bI(t)
j
+ ϵ)(U (t)
j
+ ϵ)
63.5
91.2
Variant 2.
(bI(t)
j
+ ϵ)/(U (t)
j
+ ϵ)
Unconverged
Unconverged
Variant 3.
bI(t)
j
+ ϵ
63.8
91.1
Variant 4.
1/(bI(t)
j
+ ϵ)
Unconverged
Unconverged
Variant 5.
U (t)
j
+ ϵ
63.8
91.1"
REFERENCES,0.9824561403508771,Table 12: Ablation study on parameter sensitivity and local temporal variations.
REFERENCES,0.9853801169590644,"Corpus
Task
#Train
#Dev
#Test
#Label
Metrics"
REFERENCES,0.9883040935672515,"Single-Sentence Classification (GLUE)
CoLA
Acceptability
8.5k
1k
1k
2
Matthews corr
SST
Sentiment
67k
872
1.8k
2
Accuracy"
REFERENCES,0.9912280701754386,"Pairwise Text Classification (GLUE)
MNLI
NLI
393k
20k
20k
3
Accuracy
RTE
NLI
2.5k
276
3k
2
Accuracy
QQP
Paraphrase
364k
40k
391k
2
Accuracy/F1
MRPC
Paraphrase
3.7k
408
1.7k
2
Accuracy/F1
QNLI
QA/NLI
108k
5.7k
5.7k
2
Accuracy"
REFERENCES,0.9941520467836257,"Text Similarity (GLUE)
STS-B
Similarity
7k
1.5k
1.4k
1
Pearson/Spearman corr"
REFERENCES,0.9970760233918129,Table 13: Summary of the GLUE benchmark.
