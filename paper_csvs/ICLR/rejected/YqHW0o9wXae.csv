Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003194888178913738,"We develop an assisted learning framework for assisting organization-level learners
to improve their learning performance with limited and imbalanced data. In partic-
ular, learners at the organization level usually have sufﬁcient computation resource,
but are subject to stringent data sharing and collaboration policies. Their limited
imbalanced data often cause biased inference and sub-optimal decision-making.
In our assisted learning framework, an organizational learner purchases assistance
service from a service provider and aims to enhance its model performance within
a few assistance rounds. We develop effective stochastic training algorithms for
assisted deep learning and assisted reinforcement learning. Different from exist-
ing distributed algorithms that need to frequently transmit gradients or models,
our framework allows the learner to only occasionally share information with
the service provider, and still achieve a near-oracle model as if all the data were
centralized."
INTRODUCTION,0.006389776357827476,"1
INTRODUCTION"
INTRODUCTION,0.009584664536741214,"Modern distributed learning frameworks such as federated learning (Shokri & Shmatikov, 2015;
Konecny et al., 2016; McMahan et al., 2017) aim to improve the learning performance for a large
number of learners that have limited data and computation/communication resources. These learning
frameworks are well suited for cloud systems and IoT systems (Ray, 2016; Gomathi et al., 2018) that
manage numerous smart devices through wireless communication."
INTRODUCTION,0.012779552715654952,"Over the past decade, many organizations, e.g., government agencies, hospitals, schools, and com-
panies, have integrated machine learning models into their work pipeline to facilitate data analysis
and decision making. For example, according to a recent survey (Financesonline, 2021), 49% of
companies worldwide are exploring or planning to use machine learning, 51% of organizations
claim to be early adopters of machine learning, and the estimated productivity improvement from
the learning models is 40%. However, the performance of their machine learning models critically
depends on the quality of the data, which typically is of a limited population and is biased toward
certain distributions. Unfortunately, the existing learning frameworks cannot help big organizations
improve their learning performance due to the following major restrictions."
INTRODUCTION,0.01597444089456869,"• Unlike smart devices in the conventional federated learning, organizational learners typically
cooperate with a single external service provider under a rigorous contract. Moreover, the service
provider is often presumed to have more data with better quality than the organization.
• Conventional distributed learners achieve the performance goal by frequently exchanging informa-
tion with other learners. In comparison, each learning round for organizational learners is costly, as
they need to pay the provider for assistance and need to exchange a large amount of information
with the provider. Hence, organizational learners desire to achieve a signiﬁcant performance
improvement within limited assistance rounds."
INTRODUCTION,0.019169329073482427,"Therefore, there is an emerging need to develop a modern machine learning framework for organiza-
tional learners that can signiﬁcantly improve the model performance by purchasing limited assistance
services from external providers without data sharing. This constitutes the goal of this work."
INTRODUCTION,0.022364217252396165,"In this work, we develop an assisted learning framework in the ‘horizontal-splitting’ setting, where
the learner and the service provider possess different datasets that are utilized for training a common
model. In our context, the learner’s data is assumed to be limited and imbalanced, while the provider’s"
INTRODUCTION,0.025559105431309903,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02875399361022364,"data is supposed to be big and complements the learner’s data. Our learning framework nicely suits
the organizational learners’ characteristics: they have a very limited budget for purchasing external
assistance services, yet they exchange a large amount of side information with the provider per
assistance round to maximize the performance gain. This is opposite to federated learning, where
smart devices are equipped with only a limited communication budget but can endlessly learn through
interacting with the cloud. We summarize our contributions as follows."
OUR CONTRIBUTIONS,0.03194888178913738,"1.1
OUR CONTRIBUTIONS"
OUR CONTRIBUTIONS,0.03514376996805112,"We identify the need for developing an assisted learning framework for facilitating the deployment
of general machine learning in large organizations. This learning framework addresses the unique
challenges as explained previously."
OUR CONTRIBUTIONS,0.038338658146964855,"We ﬁrst develop an assisted deep learning framework for organizational learners with limited and
imbalanced data, and propose a stochastic training algorithm named AssistSGD. Speciﬁcally, every
assistance round of AssistSGD consists of two phases. In the ﬁrst phase, the learner performs local
SGD training for multiple iterations and sends the generated trajectory of models together with their
corresponding local loss values to the service provider. In the second phase, the provider utilizes
the learner’s information to evaluate the global loss of the received models, and uses the best model
with the smallest global loss as an initialization. Then, the provider performs local SGD training for
multiple iterations and sends the generated trajectory of models together with their corresponding
local loss values to the learner. Finally, the learner utilizes the provider’s information to evaluate the
global loss of the received models, and outputs the best model with the smallest global loss. Under
mild technical assumptions, we formally prove that AssistSGD with full batch gradient updates is
guaranteed to ﬁnd a critical point of the global loss function in general nonconvex optimization."
OUR CONTRIBUTIONS,0.04153354632587859,"We further generalize the framework to enable assisted reinforcement learning, and develop a policy
gradient training algorithm named AssistPG, which has the same training logic as that of AssistSGD."
OUR CONTRIBUTIONS,0.04472843450479233,"Through extensive experiments with deep learning and reinforcement learning, we demonstrate that
the learner can achieve a near-oracle performance with AssistSGD and AssistPG as if all the data
were centralized. In particular, as the learner data’s level of imbalance increases, AssistSGD can help
the learner achieve a higher performance gain. Moreover, data are never exchanged in the assisted
learning process for both participants."
RELATED WORK,0.04792332268370607,"1.2
RELATED WORK"
RELATED WORK,0.051118210862619806,"Assisted learning. Earlier work on assisted learning (Xian et al., 2020) considers organizations
that collect different features from the same cohort. This is in contrast with our context where
organizations hold the same features but imbalanced data distributions or environments. Also, our
method applies to general deep learning and reinforcement learning tasks, which are beyond the
previously studied regression task. Consequently, our algorithm designs and application scenarios are
signiﬁcantly different from the prior work."
RELATED WORK,0.054313099041533544,"Distributed optimization. In conventional distributed optimization, the data is evenly distributed
among workers, which collaboratively solve a large-scale problem by exchanging local information
(gradients, models.) via either decentralized networks (Xie et al., 2016; Lian et al., 2017; 2018)
or centralized networks (Ho et al., 2013; Li et al., 2014; Richtarik & Takavc, 2016; Zhou et al.,
2016; 2018). As a comparison, our AssistSGD only requires a few transmission rounds between the
learner and provider. This is particularly appealing for organizational learners, who can employ a
sophisticated optimization process locally while restricting the rounds of assistance."
RELATED WORK,0.05750798722044728,"Federated learning. Federated learning is an emerging distributed learning framework (Shokri &
Shmatikov, 2015; Konecny et al., 2016; McMahan et al., 2017; Zhao et al., 2018; Li et al., 2020;
Diao et al., 2021), which aims to learn a global model using the average of local models trained by
numerous smart devices with heterogeneous data. The existing federated learning algorithms require
frequent transmissions of local model parameters. This is different from our solution designed for the
organizational learning scenarios, where each learner is an organization that often has unconstrained
communication and computation resources, but is restricted to interact with other external service
providers. Our solution aims to help the learner improve learning performance within ten rounds,
while federated learning needs many more rounds."
RELATED WORK,0.06070287539936102,Under review as a conference paper at ICLR 2022
ASSISTED DEEP LEARNING,0.06389776357827476,"2
ASSISTED DEEP LEARNING"
ASSISTED DEEP LEARNING,0.0670926517571885,"In this section, we introduce the assisted deep learning framework. Throughout the paper, L denotes
a learner who seeks assistance, and P denotes a service provider who provides assistance to L."
PROBLEM FORMULATION,0.07028753993610223,"2.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.07348242811501597,"We consider the case where the learner L aims to train a machine learning model θ that performs well
on its own dataset D(L) and generalizes well to unseen data. In general, L can train a machine learning
model by solving the empirical risk minimization problem minθ∈Θ f(θ; D(L)), where f(·; D(L)) is
the loss on D(L) and Θ is the parameter space. Standard statistical learning theories show that
the obtained model can generalize well to intact test samples under suitable constraints of model
parsimoniousness (Ding et al., 2018). However, when the user’s data D(L) contains a limited number
of samples that are highly imbalanced, the learned model will suffer from overﬁtting or deteriorated
generalization capability to the unseen test data."
PROBLEM FORMULATION,0.07667731629392971,"To overcome data deﬁciency, the learner L intends to connect with an external service provider P
(e.g., a commercialized data company), who possesses data D(P) that are sufﬁcient or complementary
to the learner’s data D(L). Ideally, the user L would improve the model by solving the following
data-augmented problem, where D(L,P) := D(L) ∪D(P) denotes the centralized data."
PROBLEM FORMULATION,0.07987220447284345,"θ(L,P) = arg min
θ∈Θ
f(θ; D(L,P)),
where D(L,P) = D(L) ∪D(P).
(1)"
PROBLEM FORMULATION,0.08306709265175719,"We note that f(θ; D(L,P)) = f(θ, D(L)) + f(θ, D(P)). If D(P) is generated from a distribution that
is close to the underlying data distribution, then it is expected that θ(L,P) will achieve signiﬁcantly
improved performance on unseen data. However, it is unrealistic to centralize the data since the
interactions between the learner L and the provider P are often restricted by various regulations.
Some representative regulations that formally deﬁne the assisted learning framework are listed below."
PROBLEM FORMULATION,0.08626198083067092,Assisted Learning Protocols
PROBLEM FORMULATION,0.08945686900958466,1. No data sharing: Neither the learner L nor the provider P will share data with each other.
PROBLEM FORMULATION,0.0926517571884984,"2. Limited assistance: The learner L has a limited budget for purchasing assistance service.
The learner desires to maximize the performance gain with only a few assistance rounds."
PROBLEM FORMULATION,0.09584664536741214,"3. Unlimited communication bandwidth: In each assistance round, the learner and the
provider can exchange unlimited information. For example, the learner (resp. provider)
can send an employee (resp. technician) to deliver a large-capacity hard drive to the other."
PROBLEM FORMULATION,0.09904153354632587,"The above assisted learning framework is different from the existing learning frameworks. For
example, in federated learning, many devices collaboratively train a global model via a large number
of learning rounds with limited communication bandwidth. In comparison, the organizational learner
in assisted learning can only query a few rounds of assistance from the provider, but can exchange
unlimited information with it."
PROBLEM FORMULATION,0.10223642172523961,"Hence, we need to develop a training algorithm that can substantially improve the learner’s model
quality using limited interactions with the provider for assisted learning. Next, we present an assisted
stochastic gradient descent (AssistSGD) algorithm for this purpose."
ASSISTSGD FOR ASSISTED DEEP LEARNING,0.10543130990415335,"2.2
ASSISTSGD FOR ASSISTED DEEP LEARNING"
ASSISTSGD FOR ASSISTED DEEP LEARNING,0.10862619808306709,"We propose AssistSGD in Algorithm 1 for assisted deep learning. The learning process consists of R
rounds, each consisting of the following interactions between the learner L and the provider P."
ASSISTSGD FOR ASSISTED DEEP LEARNING,0.11182108626198083,"(1) First, the learner L initiates a local learning process. It initializes a model θ(L)
0
and applies SGD
with learning rate η to update it for T iterations using the local dataset D(L). Then, the learner
evaluates the local loss f(·; D(L)) in a subset T of the iterations t = 0, 1, ...T −1. Lastly, the learner
sends this subset of models and their corresponding local loss to the provider P ."
ASSISTSGD FOR ASSISTED DEEP LEARNING,0.11501597444089456,Under review as a conference paper at ICLR 2022
ASSISTSGD FOR ASSISTED DEEP LEARNING,0.1182108626198083,"Algorithm 1 AssistSGD
Input: Initialization model θ0, learning rate η, assistance
rounds R, local iterations T.
for assistance rounds r = 1, . . . , R do"
ASSISTSGD FOR ASSISTED DEEP LEARNING,0.12140575079872204,"Learner L :
▶Initialize θ(L)
0
= θr−1.
▶Local SGD training to generate {θ(L)
t
}T −1
t=0 .
▶Send {θ(L)
t
, f(θ(L)
t
; D(L))}t∈T to provider P .
————————————————————
Provider P :
▶Initialize θ(P)
0
= arg minθ∈{θ(L)
t
}t∈T f(θ; D(L,P))."
ASSISTSGD FOR ASSISTED DEEP LEARNING,0.12460063897763578,"▶Local SGD training to generate {θ(P)
t
}T ′−1
t=0 .
▶Send {θ(P)
t
, f(θ(P)
t
; D(P))}t∈T ′ to learner L .
————————————————————
Learner L :
▶Output θr = arg minθ∈{θ(P)
t
}t∈T ′ f(θ; D(L,P))."
ASSISTSGD FOR ASSISTED DEEP LEARNING,0.12779552715654952,"end
Output: The best model in {θr}R
r=1."
ASSISTSGD FOR ASSISTED DEEP LEARNING,0.13099041533546327,"(2) Upon receiving the information
from the learner L, the provider
P ﬁrst evaluates the global loss
f(·; D(L,P)) of the received set of
models {θ(L)
t
, t ∈T } and picks the
best one (denoted by θ(P)
0 ) for initial-
ization. Note that the global loss can
be evaluated because the local loss
{f(θ(L)
t
; D(L)), t ∈T } are provided
by the learner L , and the provider P
just needs to evaluate the local loss
{f(θ(L)
t
; D(P)), t ∈T }. After that,
the provider applies SGD with learn-
ing rate η to update the model for T ′"
ASSISTSGD FOR ASSISTED DEEP LEARNING,0.134185303514377,"iterations on the local dataset D(P).
Then, the provider evaluates the local
loss f(·; D(P)) in a subset T ′ of the it-
erations t = 0, 1, ...T ′ −1, and sends
the subset of models and their corre-
sponding local loss to the learner L."
ASSISTSGD FOR ASSISTED DEEP LEARNING,0.13738019169329074,"(3) Once the learner L receives feedback from the provider P, it evaluates the global loss f(·; D(L,P))
of the received set of models {θ(P)
t
, t ∈T ′} and picks the best model as the output model of this
assistance round."
ASSISTSGD FOR ASSISTED DEEP LEARNING,0.14057507987220447,"Discussions. The above algorithm works for general deep learning tasks. It does not require data
sharing between the learner and the provider. Moreover, in each learning round, the learner and the
provider exchange a small number of their local training models. As we show later in the experimental
studies, it sufﬁces to sample the iterations in T , T ′ at a low frequency. Such an assisted learning
process is very different from, for example, the federated learning process. Particularly, in each round
of federated learning, all learners perform a small number of local SGD updates and send their last
output models to the cloud due to limited communication bandwidths. Consequently, the global
model needs a large number of learning rounds to achieve a desirable performance."
CONVERGENCE ANALYSIS,0.14376996805111822,"2.3
CONVERGENCE ANALYSIS"
CONVERGENCE ANALYSIS,0.14696485623003194,"In this subsection, we show that AssistSGD provably converges to a stationary point in smooth
nonconvex optimization. For simplicity, we consider the deterministic setting, where the local training
of AssistSGD uses full gradient updates. We also make the following reasonable assumptions.
Assumption 1. We assume that the assisted learning problem in equation 1 satisﬁes the following
conditions."
CONVERGENCE ANALYSIS,0.1501597444089457,"1. The global loss f(θ; D(L,P)) has L-Lipschitz gradients. Moreover, infθ f(θ; D(L,P)) > −∞;"
CONVERGENCE ANALYSIS,0.15335463258785942,"2. There exists a G > 0 such that max{∥∇f(θ; D(L))∥, ∥∇f(θ; D(P))∥, ∥∇f(θ; D(L,P))∥} ≤G for
all θ generated by AssistSGD. This holds when the generated model trajectory is bounded."
CONVERGENCE ANALYSIS,0.15654952076677317,"Note that in each assistance round r, both the provider and the learner pick the best model via the
arg min operation to initialize and ﬁnalize their local training. This guarantees that AssistSGD
continuously makes optimization progress. Speciﬁcally, we let θ(L),r
0
, θ(P),r
0
denote learner L’s and
provider P’s initialization models, respectively, in the round r. Recall that θr is the output model
of learner L (see Algorithm 1). Then, the two arg min operations guarantee that the following
proposition holds."
CONVERGENCE ANALYSIS,0.1597444089456869,"Proposition 1. The sequence of global loss {f(θr; D(L,P))}r achieved by AssistSGD monotonically
decreases to a ﬁnite limit, namely, in any round r,"
CONVERGENCE ANALYSIS,0.16293929712460065,"f(θr, D(L,P)) ≤f(θ(P),r
0
, D(L,P)) ≤f(θ(L),r
0
, D(L,P)) = f(θr−1, D(L,P))."
CONVERGENCE ANALYSIS,0.16613418530351437,"Next, we prove that the output model θr asymptotically converges to a stationary point."
CONVERGENCE ANALYSIS,0.16932907348242812,Under review as a conference paper at ICLR 2022
CONVERGENCE ANALYSIS,0.17252396166134185,"Theorem 1. Let Assumption 1 hold and run AssistSGD with full gradient updates for R assistance
rounds. With η = O((RLTG2)−0.5), we have that min0≤r≤R−1 ∥∇fL,P(θr)∥→0 as R →∞."
CONVERGENCE ANALYSIS,0.1757188498402556,"Therefore, with a proper choice of the learning rate η, assisted learning is guaranteed to ﬁnd a
stationary point in general nonconvex optimization. We note that it is generally hard to establish a
tight convergence complexity bound for AssistSGD due to the uncertainty of the arg min operations.
Still, our experiments show that it can often achieve the performance of centralized SGD training."
ASSISTED REINFORCEMENT LEARNING,0.17891373801916932,"3
ASSISTED REINFORCEMENT LEARNING"
ASSISTED REINFORCEMENT LEARNING,0.18210862619808307,"In this section, we extend our assisted learning framework to Reinforcement Learning (RL) scenarios
to enhance the model generalizability. We ﬁrst introduce some basic setup of RL."
ASSISTED REINFORCEMENT LEARNING,0.1853035143769968,"Markov Decision Process (MDP). We consider a standard ﬁnite-horizon MDP that is denoted by
a tuple M = (S, A, P, r, π, ρ0, T), where S is the state space, A corresponds to the action space,
P : S × A × S →[0, 1] denotes the underlying state transition kernel that drives the new state given
the previous state and action, r : S × A 7→R is the reward function, π : S →A is the policy, ρ0
denotes the initial state distribution, and T is the episode length. Given a policy πθ parameterized
with θ, the goal of RL, also known as on-policy learning, is to learn an optimal policy parameter θ∗"
ASSISTED REINFORCEMENT LEARNING,0.18849840255591055,"that maximizes the expected accumulated reward, namely θ∗= arg maxθ J(θ) := E[PT
t=1 γt−1rt],
where the expectation is taken with respect to the ﬁnite-length episode."
PROBLEM FORMULATION,0.19169329073482427,"3.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.19488817891373802,"We assume that an RL learner L has collected a small amount of Markovian data D(L) by interacting
with a certain environment. It wants to train a policy that generalizes well to other similar environ-
ments. However, the data and environment that the learner L can access are limited. In assisted
reinforcement learning, the learner L aims to enhance its policy’s generalizability to unseen environ-
ments by querying assistance from a service provider P. For example, autonomous-driving startup
companies typically own limited data that are insufﬁcient for training good autonomous driving
models that perform well in heterogeneous environments, and they can purchase assistance service
from big companies (who own massive data) to improve the model performance and generalizability."
PROBLEM FORMULATION,0.19808306709265175,"Formally, we assume that there is an underlying distribution of transition kernel that models the
variability of the environment. Speciﬁcally, denote Eβ as an environment with the transition kernel
Pβ parameterized by β, which follows an underlying distribution q. Let Jβ(θ) denote the expected
accumulated reward collected from the environment Eβ following the policy πθ. The learner L’s
ultimate goal is to learn a good policy that applies to the underlying distribution of environment,
namely, maxθ Eβ∼q

Jβ(θ)

. In practice, the learner L only has training data collected from a
limited number of environment instances, say β(L) = {β(L)
1 , . . . , β(L)
nL }. On the other hand, the
service provider may have rich experience interacting with a more diverse set of environments, say
β(P) = {β(P)
1 , . . . , β(P)
nP }. Consequently, the learner aims to solve the following assisted RL problem."
PROBLEM FORMULATION,0.2012779552715655,"max
θ
Jβ(L,P)(θ) :=
X"
PROBLEM FORMULATION,0.20447284345047922,"β∈β(L)
Jβ(θ) +
X"
PROBLEM FORMULATION,0.20766773162939298,"β′∈β(P)
Jβ′(θ)
(2)"
PROBLEM FORMULATION,0.2108626198083067,"which is similar to the formulation of assisted deep learning. In the following subsection, we will
develop a policy gradient (PG)-type algorithm for solving the assisted RL problem in equation 2."
ASSISTPG FOR ASSISTED REINFORCEMENT LEARNING,0.21405750798722045,"3.2
ASSISTPG FOR ASSISTED REINFORCEMENT LEARNING"
ASSISTPG FOR ASSISTED REINFORCEMENT LEARNING,0.21725239616613418,"Policy gradient (PG) is a classic RL algorithm for policy optimization. The PG algorithm estimates the
policy gradient ∇J(θ) via the policy gradient theorem, and applies it to update the policy. Speciﬁcally,
given one episode τ with length T that is collected under the current policy πθ, the corresponding
policy gradient takes the following form, where R(τ) = PT
t=1 γt−1rt is the discounted accumulated"
ASSISTPG FOR ASSISTED REINFORCEMENT LEARNING,0.22044728434504793,Under review as a conference paper at ICLR 2022
ASSISTPG FOR ASSISTED REINFORCEMENT LEARNING,0.22364217252396165,"reward over this episode. In practice, a mini batch of episodes are used to estimate the policy gradient."
ASSISTPG FOR ASSISTED REINFORCEMENT LEARNING,0.2268370607028754,"∇J(θ) ≈R(τ) T
X"
ASSISTPG FOR ASSISTED REINFORCEMENT LEARNING,0.23003194888178913,"t=1
∇log πθ
 
a(i)
t |s(i)
t
"
ASSISTPG FOR ASSISTED REINFORCEMENT LEARNING,0.23322683706070288,"In Algorithm 2, we present Assisted Policy Gradient (AssistPG)–a policy gradient-type algorithm
for solving the assisted RL problem in equation 2. The main logic of the AssistPG algorithm is the
same as that of the AssistSGD for assisted deep learning."
ASSISTPG FOR ASSISTED REINFORCEMENT LEARNING,0.2364217252396166,"Algorithm 2 AssistPG
Input: Initialization model θ0, learning rate η, assistance rounds R, local itera-
tions T.
for assistance rounds r = 1, . . . , R do"
ASSISTPG FOR ASSISTED REINFORCEMENT LEARNING,0.23961661341853036,"Learner L :
▶Initialize θ(L)
0
= θr−1.
▶Local PG training to generate {θ(L)
t
}T −1
t=0 .
▶Send {θ(L)
t
, P
β∈β(L) Jβ(θ(L)
t
)}t∈T to provider P .
————————————————————
Provider P :
▶Initialize θ(P)
0
= arg maxθ∈{θ(L)
t
}t∈T Jβ(L,P)(θ)."
ASSISTPG FOR ASSISTED REINFORCEMENT LEARNING,0.24281150159744408,"▶Local PG training to generate {θ(P)
t
}T ′−1
t=0 .
▶Send {θ(P)
t
, P"
ASSISTPG FOR ASSISTED REINFORCEMENT LEARNING,0.24600638977635783,"β∈β(P) Jβ(θ(P)
t
)}t∈T ′ to learner L .
————————————————————
Learner L :
▶Output θr = arg maxθ∈{θ(P)
t
}t∈T ′ Jβ(L,P)(θ)."
ASSISTPG FOR ASSISTED REINFORCEMENT LEARNING,0.24920127795527156,"end
Output: The best model in {θr}R
r=1."
EXPERIMENTS,0.2523961661341853,"4
EXPERIMENTS"
EXPERIMENTS,0.25559105431309903,"Non-assisted local 
solution (70.0%)"
EXPERIMENTS,0.25878594249201275,"Assisted at ! = #
(82.4%)"
EXPERIMENTS,0.26198083067092653,"Assisted at ! = $
(81.9%)"
EXPERIMENTS,0.26517571884984026,"(a) Assisted solutions
(b) Oracle solution (82.5%)"
EXPERIMENTS,0.268370607028754,"Figure 1: Visualization of AssistSGD: (a) the
learner’s classiﬁers after being assisted by the
provider at different rounds, and (b) oracle classi-
ﬁer obtained by using centralized data. The test
accuracies are shown in the parentheses."
EXPERIMENTS,0.2715654952076677,"In this section, we ﬁrst visualize AssistSGD
training to help understand the mechanism of
assisted learning. Then, we provide extensive
experiments of deep learning and reinforcement
learning to demonstrate the effectiveness of the
proposed assisted learning algorithms."
"VISUALIZATION
OF ASSISTSGD TRAINING",0.2747603833865815,"4.1
VISUALIZATION
OF ASSISTSGD TRAINING"
"VISUALIZATION
OF ASSISTSGD TRAINING",0.2779552715654952,"We visualize the learning process of As-
sistSGD through a simple logistic regression
task.
Consider two classes of data samples:
Class A data contains 50 points drawn from
N([−1, 1], 1.52I2) , and Class B data con-
tains 50 points drawn from N([1, −1], 1.52I2),
where N and I denote Gaussian distribution and identity matrix, respectively. Suppose that a learner
L observes 90% class A samples and 10% class B samples. Another service provider P observes a
similar number of data samples consisting of 10% class A samples and 90% class B samples. The
learning process of AssisSGD is illustrated in Figure 1 (Left), and we also show the oracle solution
trained by SGD with centralized data in Figure 1 (Right)."
"VISUALIZATION
OF ASSISTSGD TRAINING",0.28115015974440893,"It can be seen that without any assistance, the learner can only achieve 70% accuracy and the
corresponding classiﬁer performs poorly on the samples in class B. In comparison, after a single
round of assistance, the classiﬁcation performance is signiﬁcantly improved to 81.9% accuracy. After"
"VISUALIZATION
OF ASSISTSGD TRAINING",0.28434504792332266,Under review as a conference paper at ICLR 2022
"VISUALIZATION
OF ASSISTSGD TRAINING",0.28753993610223644,"three rounds of assistance, the corresponding classiﬁer is relatively close to the oracle classiﬁer,
which achieves an accuracy of 82.5%. Hence, it can be seen that AssistSGD has the potential to
achieve a near-oracle performance. We also present the visualization of another regression example
in Section A.2 of the Appendix."
ASSISTED DEEP LEARNING EXPERIMENTS,0.29073482428115016,"4.2
ASSISTED DEEP LEARNING EXPERIMENTS"
ASSISTED DEEP LEARNING EXPERIMENTS,0.2939297124600639,"We test the performance of AssistSGD by comparing it with three baselines: standard SGD (using
centralized data D(L,P)), Learner-SGD (using only the learner’s data D(L)), and the FedAvg algo-
rithm (McMahan et al., 2017) for federated learning. We implement these algorithms to train an
AlexNet (with learning rate 0.01 and batch size 256) and a ResNet-18 (with learning rate 0.1 and
batch size 256) on the CIFAR-10 dataset (Krizhevsky, 2009)."
ASSISTED DEEP LEARNING EXPERIMENTS,0.2971246006389776,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
ASSISTED DEEP LEARNING EXPERIMENTS,0.3003194888178914,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60"
ASSISTED DEEP LEARNING EXPERIMENTS,0.3035143769968051,"80
CIFAR-10, AlexNet, ρ=1/9, γL=0.1"
ASSISTED DEEP LEARNING EXPERIMENTS,0.30670926517571884,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
ASSISTED DEEP LEARNING EXPERIMENTS,0.30990415335463256,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60"
ASSISTED DEEP LEARNING EXPERIMENTS,0.31309904153354634,"80
CIFAR-10, AlexNet, ρ=1/1, γL=0.1"
ASSISTED DEEP LEARNING EXPERIMENTS,0.31629392971246006,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
ASSISTED DEEP LEARNING EXPERIMENTS,0.3194888178913738,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60"
ASSISTED DEEP LEARNING EXPERIMENTS,0.3226837060702875,"80
CIFAR-10, ResNet-18, ρ=1/9, γL=0.1"
ASSISTED DEEP LEARNING EXPERIMENTS,0.3258785942492013,"0
1
2
3
4
5
6
7
8
9 10
0 1 2 3 4"
ASSISTED DEEP LEARNING EXPERIMENTS,0.329073482428115,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60"
ASSISTED DEEP LEARNING EXPERIMENTS,0.33226837060702874,"80
CIFAR-10, ResNet-18, ρ=1/1, γL=0.1"
ASSISTED DEEP LEARNING EXPERIMENTS,0.3354632587859425,"Figure 2: Comparison of AssistSGD, SGD, Learner-SGD and FedAvg with balanced learner’s data
using AlexNet (top row) and ResNet-18 (bottom row)."
ASSISTED DEEP LEARNING EXPERIMENTS,0.33865814696485624,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
ASSISTED DEEP LEARNING EXPERIMENTS,0.34185303514376997,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60"
ASSISTED DEEP LEARNING EXPERIMENTS,0.3450479233226837,"80
CIFAR-10, AlexNet, ρ=1/9, γL=1"
ASSISTED DEEP LEARNING EXPERIMENTS,0.34824281150159747,"0
1
2
3
4
5
6
7
8
9 10
0 1 2 3"
ASSISTED DEEP LEARNING EXPERIMENTS,0.3514376996805112,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80"
ASSISTED DEEP LEARNING EXPERIMENTS,0.3546325878594249,"CIFAR-10, AlexNet, ρ=1/1, γL=1"
ASSISTED DEEP LEARNING EXPERIMENTS,0.35782747603833864,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
ASSISTED DEEP LEARNING EXPERIMENTS,0.3610223642172524,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80"
ASSISTED DEEP LEARNING EXPERIMENTS,0.36421725239616615,"CIFAR-10, ResNet-18, ρ=1/9, γL=1"
ASSISTED DEEP LEARNING EXPERIMENTS,0.36741214057507987,"0
1
2
3
4
5
6
7
8
9 10
0 1 2 3 4"
ASSISTED DEEP LEARNING EXPERIMENTS,0.3706070287539936,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80"
ASSISTED DEEP LEARNING EXPERIMENTS,0.3738019169329074,"CIFAR-10, ResNet-18, ρ=1/1, γL=1"
ASSISTED DEEP LEARNING EXPERIMENTS,0.3769968051118211,"Figure 3: Comparison of AssistSGD, SGD, Learner-SGD and FedAvg with imbalanced learner’s data
using AlexNet (top row) and ResNet-18 (bottom row)."
ASSISTED DEEP LEARNING EXPERIMENTS,0.3801916932907348,"For AssistSGD, we distribute the entire training set of CIFAR-10 (50k samples) to the learner and
provider according to two parameters: 1) data size ratio ρ := |D(L)|/|D(P)|, and 2) data imbalance
ratio γL ∈(0, 1) that speciﬁes the imbalance level of the learner’s data. Speciﬁcally, we ﬁrst randomly"
ASSISTED DEEP LEARNING EXPERIMENTS,0.38338658146964855,Under review as a conference paper at ICLR 2022
ASSISTED DEEP LEARNING EXPERIMENTS,0.3865814696485623,"assign one sample class as the primary class of the learner’s data. Then, γL ·|D(L)| number of samples
are sampled from the primary class, and the rest (1 −γL) · |D(L)| number of samples are drawn
from the remaining classes uniformly at random. The provider’s data are sampled from all classes
uniformly at random and are balanced. We also ﬁx the number of assistance rounds to be 10. The
total number of local SGD iterations in each assistance round is ﬁxed to be 2000. We assign the SGD
iteration budget to the learner and provider in proportion to their data samples’ sizes. Both the learner
and provider record their local training models and local loss values for every I = 50 SGD iterations,
which is the sampling period. For FedAvg, we treat the learner and provider as two federated learning
agents, and they inherit the same local data and local SGD iteration budgets from AssistSGD."
ASSISTED DEEP LEARNING EXPERIMENTS,0.38977635782747605,"We ﬁrst compare these algorithms with balanced learner’s data γL = 0.1 and varying data size ratios
ρ = 1/9, 1/1. We plot the training loss (on centralized data D(L,P)) and the test accuracy (on the
10k test data) against the number of assistance rounds in Figure 2 (top row AlexNet, bottom row
ResNet-18). Here, one assistance round on the x-axis is interpreted as 2000 SGD iterations for
SGD, Learner-SGD and FedAvg. The training loss of Learner-SGD is not reported as it is trained on
D(L) only. It can be seen that AssistSGD achieves a comparable performance to that of SGD with
centralized data. In particular, when ρ = 1/9 and the learner has limited data, the test performance
of AssistSGD is signiﬁcantly better than Learner-SGD, demonstrating the effectiveness of querying
assistance from the service provider. When ρ = 1/1 and the learner has more data, AssistSGD still
achieves a near-oracle performance. In particular, it converges much faster and achieves a slightly
better test performance than FedAvg."
ASSISTED DEEP LEARNING EXPERIMENTS,0.3929712460063898,"We further test and compare these algorithms with imbalanced learner’s data γL = 1 and ρ = 1/9, 1/1.
The results are shown in Figure 3. It can be seen that when ρ = 1/9 and the learner has limited data,
AssistSGD achieves a comparable performance to that of SGD, and slightly outperforms FedAvg.
Moreover, when ρ = 1/1, AssistSGD converges slower than SGD due to the large amount of highly
imbalanced data D(L). Nevertheless, it still achieves a comparable test performance to that of SGD.
On the other hand, AssistSGD signiﬁcantly outperforms FedAvg, demonstrating the robustness of
assisted learning to data heterogeneity. Comparing the results in Figure 3 with those in Figure 2,
we conclude that AssistSGD improves the test performance more (compare to Learner-SGD and
FedAvg) when the learner’s data are more imbalanced."
ASSISTED DEEP LEARNING EXPERIMENTS,0.3961661341853035,"Due to space limitation, we present other CIFAR-10 training results of both network models under the
parameters ρ = 1/3 and γL = 0.1, 1 and the corresponding SVHN training results in Section A.3.1
of the Appendix, where one can make the same conclusions for both datasets. We also explore the
effect of sampling period (Section A.3.2) on the performance of AssistSGD in the Appendix."
ASSISTED REINFORCEMENT LEARNING EXPERIMENTS,0.3993610223642173,"4.3
ASSISTED REINFORCEMENT LEARNING EXPERIMENTS"
ASSISTED REINFORCEMENT LEARNING EXPERIMENTS,0.402555910543131,"We demonstrate the effectiveness of AssistPG via solving two reinforcement learning problems: the
CartPole (Barto et al., 1983) and the LunarLander provided by the OpenAI Gym library (Brockman
et al., 2016). In the CartPole problem, a controller aims to stabilize a pole attached to a cart by
applying left or right force to the cart (see the ﬁrst ﬁgure in Figure 4), and we show that AssistPG
can help the controller stabilize the pole with different pole lengths. For the LunarLander problem, a
lander initializes its landing from top left of the sky and aims to land on a landing pad by controlling
its engine (see the ﬁrst ﬁgure in Figure 5). We show that AssistPG can help land the lander with
different engine powers."
ASSISTED REINFORCEMENT LEARNING EXPERIMENTS,0.4057507987220447,"We assume that the learner and the provider can query episode data by interacting with diverse
environments. Speciﬁcally, for the CartPole problem, we parameterize the environment using the pole
length. Both the learner and the provider train their control policies by playing 5 Cartpole games with
the pole length randomly generated from Uniform(4, 5) (for the learner) and Uniform(0, 1) (for the
provider). For the LunarLander problem, we parameterize the environment using the engine power.
Both the learner and provider train their control policies by playing 10 LunarLander games with the
engine power randomly generated from Uniform(10, 15) (for the learner) and Uniform(35, 40) (for
the provider). Moreover, we consider two sets of testing environments that are uniform (“Test I”) and
non-uniform (“Test II”), respectively. For the CartPole problem, Test I environments randomly gener-
ate the pole length from Uniform(0, 5), and Test II environments randomly generate the pole length
from Beta(1, 5) with probability 0.2 and Uniform(0, 5) with probability 0.8. For the LunarLander"
ASSISTED REINFORCEMENT LEARNING EXPERIMENTS,0.40894568690095845,Under review as a conference paper at ICLR 2022
ASSISTED REINFORCEMENT LEARNING EXPERIMENTS,0.41214057507987223,"Figure 4: Comparison of AssistPG, PG, Learner-PG and FedAvg in the CartPole game."
ASSISTED REINFORCEMENT LEARNING EXPERIMENTS,0.41533546325878595,"Figure 5: Comparison of AssistPG, PG, Learner-PG and FedAvg in the LunarLander game."
ASSISTED REINFORCEMENT LEARNING EXPERIMENTS,0.4185303514376997,"problem, Test I environments randomly generate the engine power from Uniform(10, 40), and Test II
environments randomly generate the engine power as 30r + 10, where r ∼Beta(5, 1)."
ASSISTED REINFORCEMENT LEARNING EXPERIMENTS,0.4217252396166134,"We test AssistPG on both RL problems and compare its performance with three baselines: the
standard PG (using centralized episode data), the Learner-PG (using only learner’s episode data),
and the FedAvg algorithm that uses policy gradient updates. All these algorithms are implemented
with learning rate 5 × 10−3 and episode batch size 32. We model the policy using a three-layer feed-
forward neural network with 4 and 32 hidden neurons for CartPole and LunarLander, respectively.
Moreover, for AssistPG, we ﬁx the total number of assistance rounds to be 10 and 5 for CartPole and
LunarLander, respectively. The total number of local PG iterations in each assistance round is ﬁxed
to be 20 for both problems. We also set the sampling period to be four, namely, the learner and the
provider record their local model and discounted training reward for every four local PG iterations.
Figures 4 and 5 plot the discounted training rewards (collected in local environment only), Test I
cumulative rewards, and Test II cumulative rewards against the assistance round obtained by all these
algorithms, for solving the CartPole and LunarLander problems, respectively. Here, one assistant
round is interpreted as 20 local PG iterations for algorithms other than AssistPG."
ASSISTED REINFORCEMENT LEARNING EXPERIMENTS,0.4249201277955272,"Figure 4 indicates that AssistPG outperforms Learner-PG and Fed-Avg, when the testing environments
include diverse lengths of poles. Also, AssistPG can achieve a comparable performance to that of the
PG with centralized data. Moreover, Figure 5 indicates that AssistPG can swiftly adapt to scenarios
out of their comfort zone (namely the training environments) in only a few rounds. These experiments
demonstrate that our assisted learning framework can help the learner signiﬁcantly improve the quality
of the policy for handling complex RL problems. Extensions and more details of these reinforcement
learning experiments as well as video demonstrations are included in Section A.4 of the Appendix."
CONCLUSION,0.4281150159744409,"5
CONCLUSION"
CONCLUSION,0.43130990415335463,"This work develops a new learning framework for assisting organizational learners to improve their
learning performance with limited imbalanced data. In particular, the proposed AssistSGD and
AssistPG allow the provider to assist the learner’s training process and signiﬁcantly improve its model
quality within only a few assistance rounds. We demonstrate the effectiveness of both assisted learning
algorithms through experimental studies. In the future, we expect that this learning framework can be
integrated with other learning frameworks such as meta-learning and multi-task learning. A limitation
of this study is that it only considers a pair of learner and provider. An interesting future direction is
to emulate the current assisted learning framework to allow multiple learners or service providers."
CONCLUSION,0.43450479233226835,"The Appendix document contains the proof of the technical result, more experimental details, and
video demonstrations of the reinforcement learning performance."
CONCLUSION,0.43769968051118213,Under review as a conference paper at ICLR 2022
REFERENCES,0.44089456869009586,REFERENCES
REFERENCES,0.4440894568690096,"Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can
solve difﬁcult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):
834–846, 1983."
REFERENCES,0.4472843450479233,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.4504792332268371,"Enmao Diao, Jie Ding, and Vahid Tarokh. HeteroFL: Computation and communication efﬁcient
federated learning for heterogeneous clients. Proc. ICLR, 2021."
REFERENCES,0.4536741214057508,"Jie Ding, Vahid Tarokh, and Yuhong Yang. Model selection techniques–an overview. IEEE Signal
Process. Mag., 35(6):16–34, 2018."
REFERENCES,0.45686900958466453,"Financesonline.
Market share & data analysis.
https://financesonline.com/
machine-learning-statistics/, 2021. Accessed: 2021-01-18."
REFERENCES,0.46006389776357826,"RM Gomathi, G Hari Satya Krishna, E Brumancia, and Y Mistica Dhas. A survey on iot technologies,
evolution and architecture. In Proc. ICCCSP, pp. 1–5. IEEE, 2018."
REFERENCES,0.46325878594249204,"Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu Kim, Phillip B. Gibbons, Garth A
Gibson, Greg Ganger, and Eric P Xing. More effective distributed ML via a stale synchronous
parallel parameter server. In Proc. NeurIPS, pp. 1223–1231. 2013."
REFERENCES,0.46645367412140576,"Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efﬁciency. arXiv
preprint arXiv:1610.05492, 2016."
REFERENCES,0.4696485623003195,"A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.4728434504792332,"Mu Li, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja Josifovski,
James Long, Eugene J. Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the
parameter server. In Proc. OSDI, pp. 583–598, 2014."
REFERENCES,0.476038338658147,"Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In Proc. ICLR, 2020."
REFERENCES,0.4792332268370607,"Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic
gradient descent. In Proc. NeurIPS, 2017."
REFERENCES,0.48242811501597443,"Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic
gradient descent. In Proc. ICML, volume 80, pp. 3043–3052, 2018."
REFERENCES,0.48562300319488816,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In Proc. AISTATS,
pp. 1273–1282. PMLR, 2017."
REFERENCES,0.48881789137380194,"Partha Pratim Ray. A survey of iot cloud platforms. Future Computing and Informatics Journal, 1
(1-2):35–46, 2016."
REFERENCES,0.49201277955271566,"P. Richtarik and M. Takavc. Distributed Coordinate Descent Method for Learning with Big Data. J.
Mach. Learn. Res., 2016."
REFERENCES,0.4952076677316294,"Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proc. CCS, pp. 1310–1321.
ACM, 2015."
REFERENCES,0.4984025559105431,"Xun Xian, Xinran Wang, Jie Ding, and Reza Ghanadan. Assisted learning: A framework for
multi-organization learning. Proc. NeurIPS, 2020."
REFERENCES,0.5015974440894568,"Pengtao Xie, Jin Kyu Kim, Yi Zhou, Qirong Ho, Abhimanu Kumar, Yaoliang Yu, and Eric Xing.
Lighter-communication distributed machine learning via sufﬁcient factor broadcasting. In Proc.
UAI, pp. 795–804, 2016."
REFERENCES,0.5047923322683706,Under review as a conference paper at ICLR 2022
REFERENCES,0.5079872204472844,"Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. ArXiv:1806.00582, 2018."
REFERENCES,0.5111821086261981,"Y. Zhou, Y.L. Yu, W. Dai, Y.B. Liang, and E.P. Xing. On convergence of model parallel proximal
gradient algorithm for stale synchronous parallel system. In Proc. AISTATS, 2016."
REFERENCES,0.5143769968051118,"Yi Zhou, Yingbin Liang, Yaoliang Yu, Wei Dai, and Eric P. Xing. Distributed proximal gradient
algorithm for partially asynchronous computer clusters. J. Mach. Learn. Res., 19(19):1–32, 2018."
REFERENCES,0.5175718849840255,Under review as a conference paper at ICLR 2022
REFERENCES,0.5207667731629393,"A
APPENDIX"
REFERENCES,0.5239616613418531,"A.1
PROOF OF THEOREM 1"
REFERENCES,0.5271565495207667,"For brevity, throughout the proof, we denote the loss functions f(·; D(L)), f(·; D(P)), f(·; D(L,P)) as
fL, fP, fL,P, respectively."
REFERENCES,0.5303514376996805,"We consider any assistance round r, and ﬁrst study the local training of the learner L. Recall
that the learner L initializes with the output model obtained from the previous round, namely
θ(L),r
0
= θr−1. In the local training, learner L performs T local gradient descent steps and generates
the trajectory {θ(L),r
t
}T
t=0. Then, provider P picks the best model from this trajectory that achieves
the minimum global loss, and we denote this model as θ(L),r
k
for certain k ∈{0, ..., T}. It is clear
that fL,P(θ(L),r
k
) ≤fL,P(θ(L),r
t
) for all t. By smoothness of the global loss and the gradient descent
update rule, we have that"
REFERENCES,0.5335463258785943,"fL,P(θ(L),r
k+1 ) ≤fL,P(θ(L),r
k
) + ⟨θ(L),r
k+1 −θ(L),r
k
, ∇fL,P(θ(L),r
k
)⟩+ L"
REFERENCES,0.536741214057508,"2 ∥θ(L),r
k+1 −θ(L),r
k
∥2"
REFERENCES,0.5399361022364217,"= fL,P(θ(L),r
k
) + ⟨−η∇fL(θ(L),r
k
), ∇fL,P(θ(L),r
k
)⟩+ Lη2"
REFERENCES,0.5431309904153354,"2 ∥∇fL(θ(L),r
k
)∥2."
REFERENCES,0.5463258785942492,"Since fL,P(θ(L),r
k
) ≤fL,P(θ(L),r
k+1 ), the above inequality further implies that"
REFERENCES,0.549520766773163,"⟨∇fL(θ(L),r
k
), ∇fL,P(θ(L),r
k
)⟩≤Lη"
REFERENCES,0.5527156549520766,"2 ∥∇fL(θ(L),r
k
)∥2 ≤LG2η"
REFERENCES,0.5559105431309904,"2
.
(3)"
REFERENCES,0.5591054313099042,"Next, we consider the local training of the provider P. Recall that the provider P will initialize
with the best model sent by the learner, namely θ(P),r
0
= θ(L),r
k
. In the local training, the provider P
performs T ′ local gradient descent steps and generates the trajectory {θ(P),r
t
}T ′
t=0. According to the
smoothness of the global loss, we have that"
REFERENCES,0.5623003194888179,"fL,P(θ(P),r
1
) ≤fL,P(θ(P),r
0
) + ⟨θ(P),r
1
−θ(P),r
0
, ∇fL,P(θ(P),r
0
)⟩+ L"
REFERENCES,0.5654952076677316,"2 ∥θ(P),r
1
−θ(P),r
0
∥2"
REFERENCES,0.5686900958466453,"= fL,P(θ(P),r
0
) + ⟨−η∇fP(θ(P),r
0
), ∇fL,P(θ(P),r
0
)⟩+ Lη2"
REFERENCES,0.5718849840255591,"2 ∥∇fP(θ(P),r
0
)∥2"
REFERENCES,0.5750798722044729,"= fL,P(θ(P),r
0
) + ⟨−η∇fP(θ(L),r
k
), ∇fL,P(θ(L),r
k
)⟩+ Lη2"
REFERENCES,0.5782747603833865,"2 ∥∇fP(θ(P),r
0
)∥2"
REFERENCES,0.5814696485623003,"= fL,P(θ(P),r
0
) −η
 
∥∇fL,P(θ(L),r
k
)∥2 −⟨∇fL(θ(L),r
k
), ∇fL,P(θ(L),r
k
)⟩

+ Lη2"
REFERENCES,0.5846645367412141,"2 ∥∇fP(θ(P),r
0
)∥2"
REFERENCES,0.5878594249201278,"≤fL,P(θ(P),r
0
) −η∥∇fL,P(θ(L),r
k
)∥2 + LG2η2"
REFERENCES,0.5910543130990416,"2
+ LG2η2"
REFERENCES,0.5942492012779552,"2
,
(4)"
REFERENCES,0.597444089456869,"where the last inequality utilizes equation 3 and the boundedness of the gradient. Denote θ(P),r
k
as the
best model from this trajectory that achieves the minimum global loss. The above Inequality (4) and
Proposition 1 further imply that"
REFERENCES,0.6006389776357828,"fL,P(θr) = fL,P(θ(P),r
k′
)
(for some k′ ∈{0, . . . , T ′})"
REFERENCES,0.6038338658146964,"≤fL,P(θ(P),r
1
)"
REFERENCES,0.6070287539936102,"≤fL,P(θ(P),r
0
) −η∥∇fL,P(θ(L),r
k
)∥2 + LG2η2"
REFERENCES,0.610223642172524,"≤fL,P(θr−1) −η∥∇fL,P(θ(L),r
k
)∥2 + LG2η2"
REFERENCES,0.6134185303514377,"≤fL,P(θr−1) −η∥∇fL,P(θ(L),r
0
)∥2 −η∥∇fL,P(θ(L),r
k
) −∇fL,P(θ(L),r
0
)∥2+"
REFERENCES,0.6166134185303515,"+ 2η∥∇fL,P(θ(L),r
k
) −∇fL,P(θ(L),r
0
)∥∥∇fL,P(θ(L),r
0
)∥+ LG2η2"
REFERENCES,0.6198083067092651,"(i)
≤fL,P(θr−1) −η∥∇fL,P(θ(L),r
0
)∥2 + 2η2LTG2 + LG2η2"
REFERENCES,0.6230031948881789,"≤fL,P(θr−1) −η∥∇fL,P(θr−1)∥2 + 3η2LTG2,
(5)"
REFERENCES,0.6261980830670927,Under review as a conference paper at ICLR 2022
REFERENCES,0.6293929712460063,where the inequality (i) uses the fact that
REFERENCES,0.6325878594249201,"2η∥∇fL,P(θ(L),r
k
) −∇fL,P(θ(L),r
0
)∥∥∇fL,P(θ(L),r
0
)∥≤2ηG∥∇fL,P(θ(L),r
k
) −∇fL,P(θ(L),r
0
)∥"
REFERENCES,0.6357827476038339,"≤2ηGL∥θ(L),r
k
−θ(L),r
0
∥"
REFERENCES,0.6389776357827476,"= 2η2GL∥ k−1
X"
REFERENCES,0.6421725239616614,"j=0
∇fL(θ(L),r
j
)∥"
REFERENCES,0.645367412140575,≤2η2LTG2.
REFERENCES,0.6485623003194888,"Telescoping the inequality in equation 5 over r = 1, ..., R and rearranging them, we obtain that"
R,0.6517571884984026,"1
R R−1
X"
R,0.6549520766773163,"r=0
∥∇fL,P(θr)∥2 ≤fL,P(θ0) −infθ fL,P(θ)"
R,0.65814696485623,"ηR
+ 3ηLTG2."
R,0.6613418530351438,"Choosing η =
q"
R,0.6645367412140575,"fL,P(θ0)−infθ fL,P(θ)"
R,0.6677316293929713,"3RLT G2
, we ﬁnally obtain that"
R,0.670926517571885,"min
0≤r≤R−1 ∥∇fL,P(θr)∥2 ≤1 R R−1
X"
R,0.6741214057507987,"r=0
∥∇fL,P(θr)∥2 ≤ r"
R,0.6773162939297125,12LTG2
R,0.6805111821086262,"R
 
fL,P(θ0) −inf
θ fL,P(θ)

."
R,0.6837060702875399,"Consequently, min0≤r≤R−1 ∥∇fL,P(θr)∥→0 as R →∞. This completes the proof. We note the
above complexity bound may not be tight due to the two arg min operations. We conjecture that
the assisted gradient descent can achieve the same order of convergence rate as the gradient descent
algorithm in nonconvex optimization."
R,0.6869009584664537,"Remark: If the learning rate η is small, the Inequality (3) shows that the gradient ∇fL(θ(L),r
k
) should
not be well aligned with the gradient of the global loss ∇fL,P(θ(L),r
k
). Intuitively, this is because
θ(L),r
k
is the best model chosen from the training trajectory {θ(L),r
t
}t that achieves the minimum
global loss, and therefore the subsequent gradient update ∇fL(θ(L),r
k
) must be badly correlated with
∇fL,P(θ(L),r
k
) so that the global loss actually increases after the next gradient descent iteration."
R,0.6900958466453674,"A.2
VISUALIZATION OF ASSISTSGD TRAINING: A REGRESSION EXAMPLE"
R,0.6932907348242812,"We apply the AssistSGD to solve a regression problem with simulated data a = [−1, −1], b =
[1, −1.25] and hyperparameters T = 10, η = 0.9r for both the learner and the provider. We run
the algorithm for R = 10 assistance rounds. Figure 6 shows the learning trajectory of θr for
r = 0, 1, . . . , 9. It can be seen that at the beginning, the learner L’s learning trajectory moves toward
the oracle solution since the directions of two local optima are roughly the same; then, it oscillates in
between two opposite directions and converges to the oracle solution."
R,0.6964856230031949,"1.5
1.0
0.5
0.0
0.5
1.0
1.5
1.50 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.50"
R,0.6996805111821086,L's initial
R,0.7028753993610224,"r trajectory
L's local opt
P's local opt
Global opt"
R,0.7060702875399361,Figure 6: Learning trajectory of AssistSGD in regression.
R,0.7092651757188498,Under review as a conference paper at ICLR 2022
R,0.7124600638977636,"A.3
ADDITIONAL EXPERIMENTS OF ASSISTED DEEP LEARNING"
R,0.7156549520766773,"A.3.1
EFFECT OF LEARNER’S DATASIZE AND IMBALANCE LEVEL"
R,0.7188498402555911,CIFAR-10 Dataset
R,0.7220447284345048,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.7252396166134185,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60"
R,0.7284345047923323,"80
CIFAR-10, AlexNet, ρ=1/3, γL=0.1"
R,0.731629392971246,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.7348242811501597,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80"
R,0.7380191693290735,"CIFAR-10, AlexNet, ρ=1/3, γL=1"
R,0.7412140575079872,"Figure 7: Comparison of AssistSGD, SGD, Learner-SGD and FedAvg with ρ = 1/3 using AlexNet."
R,0.744408945686901,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.7476038338658147,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80"
R,0.7507987220447284,"CIFAR-10, ResNet-18, ρ=1/3, γL=0.1"
R,0.7539936102236422,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.7571884984025559,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80"
R,0.7603833865814696,"CIFAR-10, ResNet-18, ρ=1/3, γL=1"
R,0.7635782747603834,"Figure 8: Comparison of AssistSGD, SGD, Learner-SGD and FedAvg with ρ = 1/3 using ResNet-18."
R,0.7667731629392971,"In this subsection, we present more CIFAR-10 experimental results of training AlexNet (Figure 7)
and ResNet-18 (Figure 8) under the data size ratio ρ = 1/3 and different levels of data imbalance.
We test and compare our AssistSGD with SGD, Learner-SGD, and FedAvg with balanced (γL = 0.1)
and imbalanced (γL = 1) learner’s data. From both ﬁgures, we can draw the same conclusions as
those made in Section 4.2."
R,0.7699680511182109,SVHN Dataset
R,0.7731629392971247,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.7763578274760383,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80"
R,0.7795527156549521,"100
SVHN, AlexNet, ρ=1/9, γL=0.1"
R,0.7827476038338658,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.7859424920127795,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80 100"
R,0.7891373801916933,"SVHN, AlexNet, ρ=1/1, γL=0.1"
R,0.792332268370607,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.7955271565495208,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80 100"
R,0.7987220447284346,"SVHN, ResNet-18, ρ=1/9, γL=0.1"
R,0.8019169329073482,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.805111821086262,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80 100"
R,0.8083067092651757,"SVHN, ResNet-18, ρ=1/1, γL=0.1"
R,0.8115015974440895,"Figure 9: Comparison of AssistSGD, SGD, Learner-SGD and FedAvg on SVHN with balanced
learner’s data using AlexNet (top row) and ResNet-18 (bottom row)."
R,0.8146964856230032,Under review as a conference paper at ICLR 2022
R,0.8178913738019169,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.8210862619808307,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80 100"
R,0.8242811501597445,"SVHN, AlexNet, ρ=1/9, γL=1"
R,0.8274760383386581,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.8306709265175719,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80 100"
R,0.8338658146964856,"SVHN, AlexNet, ρ=1/1, γL=1"
R,0.8370607028753994,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.8402555910543131,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80 100"
R,0.8434504792332268,"SVHN, ResNet-18, ρ=1/9, γL=1"
R,0.8466453674121406,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.8498402555910544,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80 100"
R,0.853035143769968,"SVHN, ResNet-18, ρ=1/1, γL=1"
R,0.8562300319488818,"Figure 10: Comparison of AssistSGD, SGD, Learner-SGD and FedAvg on SVHN with imbalanced
learner’s data using AlexNet (top row) and ResNet-18 (bottom row)."
R,0.8594249201277955,"In this subsection, we present the SVHN experimental results with balanced learner’s data γL = 0.1
(Figure 9) and imbalanced learner’s data γL = 1 (Figure 10) corresponding to the CIFAR-10 results
in Section 4.2. From both ﬁgures, we can draw the same conclusions as those made in Section 4.2,
which proves that our proposed AssistSGD can work well on a wide range of dataset types."
R,0.8626198083067093,"A.3.2
EFFECT OF SAMPLING PERIOD"
R,0.865814696485623,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.8690095846645367,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60"
R,0.8722044728434505,"80
CIFAR-10, AlexNet, ρ=1/9, γL=0.1"
R,0.8753993610223643,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.8785942492012779,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80"
R,0.8817891373801917,"CIFAR-10, ResNet-18, ρ=1/9, γL=0.1"
R,0.8849840255591054,"Figure 11: Comparison for AlexNet and ResNet-18 with balanced learner’s data (γL = 0.1) under
different sampling periods."
R,0.8881789137380192,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.8913738019169329,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60"
R,0.8945686900958466,"80
CIFAR-10, AlexNet, ρ=1/9, γL=0.5"
R,0.8977635782747604,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.9009584664536742,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80"
R,0.9041533546325878,"CIFAR-10, ResNet-18, ρ=1/9, γL=0.5"
R,0.9073482428115016,"Figure 12: Comparison for AlexNet and ResNet-18 with imbalanced learner’s data (γL = 0.5) under
different sampling periods."
R,0.9105431309904153,Under review as a conference paper at ICLR 2022
R,0.9137380191693291,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.9169329073482428,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60 80"
R,0.9201277955271565,"CIFAR-10, AlexNet, ρ=1/9, γL=1"
R,0.9233226837060703,"0
1
2
3
4
5
6
7
8
9 10
0 0.5 1 1.5 2 2.5"
R,0.9265175718849841,"0
1
2
3
4
5
6
7
8
9 10
0 20 40 60"
R,0.9297124600638977,"80
CIFAR-10, ResNet-18, ρ=1/9, γL=1"
R,0.9329073482428115,"Figure 13: Comparison for AlexNet and ResNet-18 with imbalanced learner’s data (γL = 1) under
different sampling periods."
R,0.9361022364217252,"We explore whether increasing the sampling frequency of the model (AlexNet and ResNet-18) and
loss value can improve the performance of AssistSGD. Under data size ratio ρ = 1/9, we test and
compare AssistSGD and SGD with balanced and imbalanced learner’s data, i.e., γL = 0.1, 0.5, 1.
We set the sampling period to be 20 and 50. The comparison results are shown in Figures 11, 12 and
13. It can be seen that using a low sampling frequency for AssistSGD already achieves the baseline
performance of SGD. It implies that AssistSGD does not require much information exchange between
the learner and provider. This helps save computation resources and reduce information leakage."
R,0.939297124600639,"A.4
VISUALIZATIONS OF THE REINFORCEMENT LEARNING GAMES"
R,0.9424920127795527,"In this section, we visualize the landing trace of the LunarLander trained by AssistPG and Leaner-PG
in different test environments."
R,0.9456869009584664,"Speciﬁcally, we consider a ﬁxed map and set the engine power of the lander to be 10, 20, 30, and
40, respectively. In each setting, we train the lander using both AssistPG and Learner-PG for R = 5
rounds. After each round of training, we let the lander play an episode using the trained model and
plot the corresponding landing trace. These traces are plotted in Figures 14, 15, 16, 17. From these
ﬁgures, it can be seen that the lander with engine power 20-40 trained by AssistPG can successfully
land to the landpad after 5 rounds of assisted learning. As a comparison, the lander trained by
Learner-PG cannot even land after 5 rounds of training. This demonstrates the advantage of AssistPG.
On the other hand, when the lander has a small engine power 10, it is challenging for both algorithms
to land the lander properly, as the engine cannot provide sufﬁcient acceleration."
R,0.9488817891373802,"Round 1
Round 2
Round 3
Round 4
Round 5"
R,0.952076677316294,"Figure 14: Comparison of landing traces of LunarLander with engine power = 10 trained by AssistPG
and Learner-PG."
R,0.9552715654952076,"Round 1
Round 2
Round 3
Round 4
Round 5"
R,0.9584664536741214,"Figure 15: Comparison of landing traces of LunarLander with engine power = 20 trained by AssistPG
and Learner-PG."
R,0.9616613418530351,Under review as a conference paper at ICLR 2022
R,0.9648562300319489,"Round 1
Round 2
Round 4
Round 3
Round 5"
R,0.9680511182108626,"Figure 16: Comparison of landing traces of LunarLander with engine power = 30 trained by AssistPG
and Learner-PG."
R,0.9712460063897763,"Round 1
Round 2
Round 3
Round 4
Round 5"
R,0.9744408945686901,"Figure 17: Comparison of landing traces of LunarLander with engine power = 40 trained by AssistPG
and Learner-PG."
R,0.9776357827476039,"Moreover, after 5 rounds of training (using both AssistPG and Learner-PG), we test the lander in both
the test environment I (“Test I”) and II (“Test II”), and plot the landing traces in Figures 18 and 19,
respectively. Here, for each test, we consider a ﬁxed map and randomly generate 10 different engine
powers from Uniform(10, 40) (for Test I) and 30 ∗Beta(5, 1) + 10 (for Test II)."
R,0.9808306709265175,"From both ﬁgures, it can be seen that the lander trained by the AssisPG lands more smoothly in all
test environments under diverse engine powers than that trained by the Learner-PG."
R,0.9840255591054313,"Figure 18: Comparison of landing traces of LunarLander with engine power ∼Uniform(10, 40)
trained by AssistPG and Learner-PG."
R,0.987220447284345,"Figure 19: Comparison of landing traces of LunarLander with engine power ∼30 ∗Beta(5, 1) + 10
trained by AssistPG and Learner-PG."
R,0.9904153354632588,"The video version for the CartPole and LunarLander games can be accessed from
the
anonymous
link
https://www.dropbox.com/sh/oz2jswj36li4lkh/
AADaQn4Nj67v9mdIHKDLN6nAa?dl=0.
In the CartPole game, four videos record the
performance of AssistPG and Learner-PG against the ﬁrst ﬁve rounds with pole length equaling
1, 2, 3, and 4, respectively. Another two videos record 10 plays in the test environment I and II,
respectively. In all the plays, both AssistPG and Learner-PG use the model trained from the ﬁfth"
R,0.9936102236421726,Under review as a conference paper at ICLR 2022
R,0.9968051118210862,"round. In the LunarLander game, four videos record the performance of AssistPG and Learner-PG
against the ﬁrst ﬁve rounds with engine power equaling 10, 20, 30, and 40, respectively. Another two
videos record 10 plays in the test environment I and II, respectively. In all the plays, both AssistPG
and Learner-PG use the model trained from the ﬁfth round. The videos show that with the assistance
from the provider, the user can quickly generalize its model to more diverse environments."
