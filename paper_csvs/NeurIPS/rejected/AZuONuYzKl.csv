Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009442870632672333,"A grand challenge in biology is to discover evolutionary traits—features of organ-
1"
ABSTRACT,0.0018885741265344666,"isms common to a group of species with a shared ancestor in the tree of life (also
2"
ABSTRACT,0.0028328611898017,"referred to as phylogenetic tree). With the growing availability of image repositories
3"
ABSTRACT,0.003777148253068933,"in biology, there is a tremendous opportunity to discover evolutionary traits directly
4"
ABSTRACT,0.004721435316336166,"from images in the form of a hierarchy of prototypes. However, current prototype-
5"
ABSTRACT,0.0056657223796034,"based methods are mostly designed to operate over a flat structure of classes and
6"
ABSTRACT,0.0066100094428706326,"face several challenges in discovering hierarchical prototypes, including the issue of
7"
ABSTRACT,0.007554296506137866,"learning over-specific features at internal nodes. To overcome these challenges, we
8"
ABSTRACT,0.0084985835694051,"introduce the framework of Hierarchy aligned Commonality through Prototypical
9"
ABSTRACT,0.009442870632672332,"Networks (HComP-Net). We empirically show that HComP-Net learns prototypes
10"
ABSTRACT,0.010387157695939566,"that are accurate, semantically consistent, and generalizable to unseen species in
11"
ABSTRACT,0.0113314447592068,"comparison to baselines on birds, butterflies, and fishes datasets.
12"
INTRODUCTION,0.012275731822474031,"1
Introduction
13"
INTRODUCTION,0.013220018885741265,"A central goal in biology is to discover the observable characteristics of organisms, or traits (e.g., beak
14"
INTRODUCTION,0.014164305949008499,"color, stripe pattern, and fin curvature), that help in discriminating between species and understanding
15"
INTRODUCTION,0.015108593012275733,"Lion
Bobcat
Mangrove Cuckoo
Pied Billed Grebe
Long tail, Long neck : Not common to both"
INTRODUCTION,0.016052880075542966,Cuckoo and Grebe
INTRODUCTION,0.0169971671388102,Over-specific
INTRODUCTION,0.01794145420207743,Prototype-1
INTRODUCTION,0.018885741265344664,(Long tail)
INTRODUCTION,0.019830028328611898,"Prototype-2
(Long neck)"
INTRODUCTION,0.02077431539187913,Common Phylogenetic Ancestor
INTRODUCTION,0.021718602455146365,"Red bellied woodpecker
Long beak, Spotted back : Common to Red"
INTRODUCTION,0.0226628895184136,bellied woodpecker and Northern Flicker
INTRODUCTION,0.023607176581680833,Common trait
INTRODUCTION,0.024551463644948063,"Prototype-1
(Spotted back)"
INTRODUCTION,0.025495750708215296,"Prototype-2
(Sharp beak)"
INTRODUCTION,0.02644003777148253,Common Phylogenetic Ancestor
INTRODUCTION,0.027384324834749764,"Northern Flicker
Mane, Spotted back : Not common to all"
INTRODUCTION,0.028328611898016998,species of Felidae family
INTRODUCTION,0.02927289896128423,Over-specific
INTRODUCTION,0.030217186024551465,Prototype-1
INTRODUCTION,0.031161473087818695,(Mane)
INTRODUCTION,0.03210576015108593,"Prototype-2
(Spotted back)"
INTRODUCTION,0.033050047214353166,Family: Felidae
INTRODUCTION,0.0339943342776204,"Figure 2: Examples to illustrate the problem of learning “over-specific” prototypes at internal nodes, which
only cover one descendant species of the node instead of learning prototypes common to all descendants."
INTRODUCTION,0.03493862134088763,"how organisms evolve and adapt to their environment [1]. For example, discovering traits inherited by
16"
INTRODUCTION,0.03588290840415486,"a group of species that share a common ancestor on the tree of life (also referred to as the phylogenetic
17"
INTRODUCTION,0.036827195467422094,"tree, see Figure 1) is of great interest to biologists to understand how organisms diversify and evolve
18"
INTRODUCTION,0.03777148253068933,"[2]. The measurement of such traits with evolutionary signals, termed evolutionary traits, is not
19"
INTRODUCTION,0.03871576959395656,"straightforward and often relies on subjective and labor-intensive human expertise and definitions
20"
INTRODUCTION,0.039660056657223795,"[3, 4], hindering rapid scientific advancement [5].
21"
INTRODUCTION,0.04060434372049103,"With the growing availability of large-scale image repositories in biology containing millions of
22"
INTRODUCTION,0.04154863078375826,"images of organisms [6, 7, 8], there is an opportunity for machine learning (ML) methods to discover
23"
INTRODUCTION,0.042492917847025496,"evolutionary traits automatically from images [5, 9]. This is especially true in light of recent advances
24"
INTRODUCTION,0.04343720491029273,"in the field of explainable ML, such as the seminal work of ProtoPNet [10] and its variants [11, 12, 13]
25"
INTRODUCTION,0.044381491973559964,"which find representative patches in training images (termed prototypes) capturing discriminatory
26"
INTRODUCTION,0.0453257790368272,"features for every class. We can thus cast the problem of discovering evolutionary traits into asking
27"
INTRODUCTION,0.04627006610009443,"the following question: what image features or prototypes are common across a group of species
28"
INTRODUCTION,0.047214353163361665,"with a shared ancestor in the tree of life that are absent in species with a different shared ancestor?
29"
INTRODUCTION,0.04815864022662889,"For example, in Figure 1, we can see that the four species of birds on the left descending from the
30"
INTRODUCTION,0.049102927289896126,"blue node show the common feature of having “long tails,” unlike any of the descendant species of
31"
INTRODUCTION,0.05004721435316336,"the red node. Learning such common features at every internal node as a hierarchy of prototypes can
32"
INTRODUCTION,0.05099150141643059,"help biologists generate novel hypotheses of species diversification (e.g., the splitting of blue and red
33"
INTRODUCTION,0.05193578847969783,"nodes) and accumulation of evolutionary trait changes.
34"
INTRODUCTION,0.05288007554296506,"Despite the success of ProtoPNet [10] and its variants in learning prototypes over a flat structure of
35"
INTRODUCTION,0.053824362606232294,"classes, applying them to discover a hierarchy of prototypes is challenging for three main reasons.
36"
INTRODUCTION,0.05476864966949953,"First, existing methods that learn multiple prototypes for every class are prone to learning “over-
37"
INTRODUCTION,0.05571293673276676,"specific” prototypes at internal nodes of a tree, which cover only one (or a few) of its descendant
38"
INTRODUCTION,0.056657223796033995,"species. Figure 2 shows a few examples to illustrate the concept of over-specific prototypes. Consider
39"
INTRODUCTION,0.05760151085930123,"the problem of learning prototypes common to descendant species of the Felidae family: Lion and
40"
INTRODUCTION,0.05854579792256846,"Bobcat. If we learn one prototype focusing on the feature of the mane (specific only to Lion) and
41"
INTRODUCTION,0.059490084985835696,"another prototype focusing on the feature of spotted back (specific only to Bobcat), then these two
42"
INTRODUCTION,0.06043437204910293,"prototypes taken together can classify all images from the Felidae family. However, they do not
43"
INTRODUCTION,0.061378659112370164,"represent common features shared between Lion and Bobcat and hence are not useful for discovering
44"
INTRODUCTION,0.06232294617563739,"evolutionary traits. Such over-specific prototypes should be instead pushed down to be learned at
45"
INTRODUCTION,0.06326723323890462,"lower levels of the tree (e.g., the species leaf nodes of Lion and Bobcat).
46"
INTRODUCTION,0.06421152030217187,"Second, while existing methods such as ProtoPShare [11], ProtoPool [12], and ProtoTree [13] allow
47"
INTRODUCTION,0.06515580736543909,"prototypes to be shared across classes for re-usability and sparsity, in the problem of discovering
48"
INTRODUCTION,0.06610009442870633,"evolutionary traits, we want to learn prototypes at an internal node n that are not just shared across
49"
INTRODUCTION,0.06704438149197356,"all it descendant species but are also absent in the contrasting set of species (i.e., species descending
50"
INTRODUCTION,0.0679886685552408,"from sibling nodes of n representing alternate paths of diversification). Third, at higher levels of the
51"
INTRODUCTION,0.06893295561850803,"tree, finding features that are common across a large number of diverse species is challenging [14, 15].
52"
INTRODUCTION,0.06987724268177525,"In such cases, we should be able to abstain from finding common prototypes without hampering
53"
INTRODUCTION,0.0708215297450425,"accuracy at the leaf nodes—a feature missing in existing methods.
54"
INTRODUCTION,0.07176581680830972,"To address these challenges, we present Hierarchy aligned Commonality through Prototypical
55"
INTRODUCTION,0.07271010387157696,"Networks (HComP-Net), a framework to learn hierarchical prototypes over the tree of life for
56"
INTRODUCTION,0.07365439093484419,"discovering evolutionary traits. Here are the main contributions of our work:
57"
HCOMP-NET LEARNS COMMON TRAITS SHARED BY ALL DESCENDANT SPECIES OF AN INTERNAL NODE AND,0.07459867799811143,"1. HComP-Net learns common traits shared by all descendant species of an internal node and
58"
HCOMP-NET LEARNS COMMON TRAITS SHARED BY ALL DESCENDANT SPECIES OF AN INTERNAL NODE AND,0.07554296506137866,"avoids the learning of over-specific prototypes in contrast to baseline methods using a novel
59"
HCOMP-NET LEARNS COMMON TRAITS SHARED BY ALL DESCENDANT SPECIES OF AN INTERNAL NODE AND,0.0764872521246459,"overspecificity loss.
60"
HCOMP-NET USES A NOVEL DISCRIMINATIVE LOSS TO ENSURE THAT THE PROTOTYPES LEARNED AT AN,0.07743153918791312,"2. HComP-Net uses a novel discriminative loss to ensure that the prototypes learned at an
61"
HCOMP-NET USES A NOVEL DISCRIMINATIVE LOSS TO ENSURE THAT THE PROTOTYPES LEARNED AT AN,0.07837582625118036,"internal node are absent in the contrasting set of species with different ancestry.
62"
HCOMP-NET INCLUDES A NOVEL MASKING MODULE TO ALLOW FOR THE EXCLUSION OF OVER-SPECIFIC,0.07932011331444759,"3. HComP-Net includes a novel masking module to allow for the exclusion of over-specific
63"
HCOMP-NET INCLUDES A NOVEL MASKING MODULE TO ALLOW FOR THE EXCLUSION OF OVER-SPECIFIC,0.08026440037771483,"prototypes at higher levels of the tree without hampering classification performance.
64"
HCOMP-NET INCLUDES A NOVEL MASKING MODULE TO ALLOW FOR THE EXCLUSION OF OVER-SPECIFIC,0.08120868744098206,"4. We empirically show that HComP-Net learns prototypes that are accurate, semantically
65"
HCOMP-NET INCLUDES A NOVEL MASKING MODULE TO ALLOW FOR THE EXCLUSION OF OVER-SPECIFIC,0.0821529745042493,"consistent, and generalizable to unseen species compared to baselines on data from 190
66"
HCOMP-NET INCLUDES A NOVEL MASKING MODULE TO ALLOW FOR THE EXCLUSION OF OVER-SPECIFIC,0.08309726156751653,"species of birds (CUB-200-2011 dataset) [8], 38 species of fishes [9], and 30 species of
67"
HCOMP-NET INCLUDES A NOVEL MASKING MODULE TO ALLOW FOR THE EXCLUSION OF OVER-SPECIFIC,0.08404154863078375,"butterflies [16]. We show the ability of HComP-Net to generate novel hypotheses about
68"
HCOMP-NET INCLUDES A NOVEL MASKING MODULE TO ALLOW FOR THE EXCLUSION OF OVER-SPECIFIC,0.08498583569405099,"evolutionary traits at different levels of the phylogenetic tree of organisms.
69"
RELATED WORKS,0.08593012275731822,"2
Related Works
70"
RELATED WORKS,0.08687440982058546,"One of the seminal lines of work in the field of prototype-based interpretability methods is the
71"
RELATED WORKS,0.08781869688385269,"framework of ProtoPNet [10] that learns a set of “prototypical patches” from training images of every
72"
RELATED WORKS,0.08876298394711993,"class to enable case-based reasoning. Following this work, several variants have been developed
73"
RELATED WORKS,0.08970727101038715,"such as ProtoPShare [11], ProtoPool [12], ProtoTree [13], and HPnet [17] suiting to different
74"
RELATED WORKS,0.0906515580736544,"interpretability requirements. Among all these approaches, our work is closely related to HPnet [17],
75"
RELATED WORKS,0.09159584513692162,"the hierarchical extension of ProtoPNet that learns a prototype layer for every parent node in the
76"
RELATED WORKS,0.09254013220018886,"tree. Despite sharing a similar motivation as our work, HPnet is not designed to avoid the learning of
77"
RELATED WORKS,0.09348441926345609,"over-specific prototypes or to abstain from learning common prototypes at higher levels of the tree.
78"
RELATED WORKS,0.09442870632672333,"Another related line of work is the framework of PIPNet [18], which uses self-supervised learning
79"
RELATED WORKS,0.09537299338999056,"methods to reduce the “semantic gap” [19, 20] between the latent space of prototypes and the space
80"
RELATED WORKS,0.09631728045325778,"of images, such that the prototypes in latent space correspond to the same visual concept in the image
81"
RELATED WORKS,0.09726156751652502,"space. In HComP-Net, we build upon the idea of self-supervised learning introduced in PIPNet to
82"
RELATED WORKS,0.09820585457979225,"learn semantically consistent hiearchy of prototypes. Our work is also related to ProtoTree [13],
83"
RELATED WORKS,0.09915014164305949,"which structures the prototypes as nodes in a decision tree to offer more granular interpretability.
84"
RELATED WORKS,0.10009442870632672,"However, ProtoTree differs from our work in that it learns the tree-based structure of prototypes
85"
RELATED WORKS,0.10103871576959396,"automatically from data and cannot handle a known hierarchy. Moreover, the prototypes learned in
86"
RELATED WORKS,0.10198300283286119,"ProtoTree are purely discriminative and allow for negative reasoning, which is not aligned with our
87"
RELATED WORKS,0.10292728989612843,"objective of finding common traits of descendant species.
88"
RELATED WORKS,0.10387157695939565,"Other related works that focus on finding shared features are ProtoPShare [11] and ProtoPool [12].
89"
RELATED WORKS,0.1048158640226629,"Both approaches aim to find common features among classes, but their primary goal is to reduce
90"
RELATED WORKS,0.10576015108593012,"the prototype count by exploiting similarities among classes, leading to a sparser network. This is
91"
RELATED WORKS,0.10670443814919736,"different from our goal of finding a hiearchy of prototypes to find evolutionary traits common to a
92"
RELATED WORKS,0.10764872521246459,"group of species (that are absent from other species).
93"
RELATED WORKS,0.10859301227573183,"Outside the realm of prototype-based methods, the framework of Phylogeny-guided Neural Networks
94"
RELATED WORKS,0.10953729933899906,"(PhyloNN) [9] shares a similar motivation as our work to discover evolutionary traits by representing
95"
RELATED WORKS,0.11048158640226628,"biological images in feature spaces structured by tree-based knowledge (i.e., phylogeny). However,
96"
RELATED WORKS,0.11142587346553352,"PhyloNN primarily focuses on the tasks of image generation and translation rather than interpretability.
97"
RELATED WORKS,0.11237016052880075,"Additionally, PhyloNN can only work with discretized trees with fixed number of ancestor levels per
98"
RELATED WORKS,0.11331444759206799,"leaf node, unlike our work that does not require any discretization of the tree.
99"
PROPOSED METHODOLOGY,0.11425873465533522,"3
Proposed Methodology
100"
HCOMP-NET MODEL ARCHITECTURE,0.11520302171860246,"3.1
HComP-Net Model Architecture
101"
HCOMP-NET MODEL ARCHITECTURE,0.11614730878186968,"Given a phylogenetic tree with N internal nodes, the goal of HComP-Net is to jointly learn a set of
102"
HCOMP-NET MODEL ARCHITECTURE,0.11709159584513693,"prototype vectors Pn for every internal node n ∈{1, . . . , N}. Our architecture as shown in Figure 3
103"
HCOMP-NET MODEL ARCHITECTURE,0.11803588290840415,"begins with a CNN that acts as a common feature extractor f(x; θ) for all nodes, where θ represents
104"
HCOMP-NET MODEL ARCHITECTURE,0.11898016997167139,"the learnable parameters of f. f converts an image x into a latent representation Z ∈RH×W ×C,
105"
HCOMP-NET MODEL ARCHITECTURE,0.11992445703493862,"where each “patch” at location (h, w) is, zh,w ∈RC. Following the feature extractor, for every node
106"
HCOMP-NET MODEL ARCHITECTURE,0.12086874409820586,"n, we initialize a set of Kn prototype vectors Pn = {pi}Kn
i=1, where pi ∈RC. Here, the number of
107 K"
HCOMP-NET MODEL ARCHITECTURE,0.12181303116147309,Node 1 Prototype
HCOMP-NET MODEL ARCHITECTURE,0.12275731822474033,vectors
HCOMP-NET MODEL ARCHITECTURE,0.12370160528800755,"Child 
classes"
HCOMP-NET MODEL ARCHITECTURE,0.12464589235127478,Prototype
HCOMP-NET MODEL ARCHITECTURE,0.125590179414542,"Scores
Softmax &"
HCOMP-NET MODEL ARCHITECTURE,0.12653446647780925,"Global 
Max-pooling H W C CNN x’ H W H W C"
HCOMP-NET MODEL ARCHITECTURE,0.1274787535410765,Masking module sg
HCOMP-NET MODEL ARCHITECTURE,0.12842304060434373,"Child 
classes"
HCOMP-NET MODEL ARCHITECTURE,0.12936732766761094,"Child 
classes"
HCOMP-NET MODEL ARCHITECTURE,0.13031161473087818,Masking module
HCOMP-NET MODEL ARCHITECTURE,0.13125590179414542,Masking module
HCOMP-NET MODEL ARCHITECTURE,0.13220018885741266,"Inner 
Product K"
HCOMP-NET MODEL ARCHITECTURE,0.13314447592067988,Node 2 Prototype
HCOMP-NET MODEL ARCHITECTURE,0.13408876298394712,vectors
HCOMP-NET MODEL ARCHITECTURE,0.13503305004721436,"Node N 
Prototype vectors sg sg"
HCOMP-NET MODEL ARCHITECTURE,0.1359773371104816,"stop 
gradient
Gumbel 
Softmax"
HCOMP-NET MODEL ARCHITECTURE,0.1369216241737488,"Multiplication
Learnable parameter"
HCOMP-NET MODEL ARCHITECTURE,0.13786591123701605,Overspecificity score
HCOMP-NET MODEL ARCHITECTURE,0.1388101983002833,Masking module
HCOMP-NET MODEL ARCHITECTURE,0.1397544853635505,Augmented image x’’
HCOMP-NET MODEL ARCHITECTURE,0.14069877242681775,"Inner 
Product"
HCOMP-NET MODEL ARCHITECTURE,0.141643059490085,"Inner 
Product"
HCOMP-NET MODEL ARCHITECTURE,0.14258734655335223,Figure 3: Schematic illustration of HComP-Net model architecture.
HCOMP-NET MODEL ARCHITECTURE,0.14353163361661944,"prototypes Kn learned at node n varies in proportion to the number of children of node n, with β
108"
HCOMP-NET MODEL ARCHITECTURE,0.14447592067988668,"as the proportionality constant, i.e., at each node n we assign β prototypes for every child node. To
109"
HCOMP-NET MODEL ARCHITECTURE,0.14542020774315392,"simplify notations, we drop the subscript n in Pn and Kn while discussing the operations occurring
110"
HCOMP-NET MODEL ARCHITECTURE,0.14636449480642116,"in node n.
111"
HCOMP-NET MODEL ARCHITECTURE,0.14730878186968838,"We consider the following sequence of operations at every node n. We first compute the similarity
112"
HCOMP-NET MODEL ARCHITECTURE,0.14825306893295562,"score between every prototype in P and every patch in Z. This results in a matrix ˆZ ∈RH×W ×K,
113"
HCOMP-NET MODEL ARCHITECTURE,0.14919735599622286,"where every element represents a similarity score between image patches and prototype vectors. We
114"
HCOMP-NET MODEL ARCHITECTURE,0.1501416430594901,"apply a softmax operation across the K channels of ˆZ such that the vector ˆzh,w ∈RK at spatial
115"
HCOMP-NET MODEL ARCHITECTURE,0.1510859301227573,"location (h, w) in ˆZ represents the probability that the corresponding patch zh,w is similar to the K
116"
HCOMP-NET MODEL ARCHITECTURE,0.15203021718602455,"prototypes. Furthermore, the ith channel of ˆZ serves as a prototype score map for the prototype
117"
HCOMP-NET MODEL ARCHITECTURE,0.1529745042492918,"vector pi, indicating the presence of pi in the image. We perform global max-pooling across the
118"
HCOMP-NET MODEL ARCHITECTURE,0.153918791312559,"spatial dimensions H × W of ˆZ to obtain a vector g ∈RK, where the ith element represents the
119"
HCOMP-NET MODEL ARCHITECTURE,0.15486307837582625,"highest similarity score of the prototype vector pi across the entire image. g is then fed to a linear
120"
HCOMP-NET MODEL ARCHITECTURE,0.1558073654390935,"classification layer with weights ϕ to produce the final classification scores for every child node of
121"
HCOMP-NET MODEL ARCHITECTURE,0.15675165250236073,"node n. We restrict the connections in the classification layer so that every child node nc is connected
122"
HCOMP-NET MODEL ARCHITECTURE,0.15769593956562794,"to a distinct set of β prototypes, to ensure that every prototype uniquely maps to a child node. ϕ is
123"
HCOMP-NET MODEL ARCHITECTURE,0.15864022662889518,"restricted to be non-negative to ensure that the classification is done solely through positive reasoning,
124"
HCOMP-NET MODEL ARCHITECTURE,0.15958451369216242,"similar to the approach used in PIP-Net [18]. We borrow the regularization scheme of PIP-Net to
125"
HCOMP-NET MODEL ARCHITECTURE,0.16052880075542966,"induce sparsity in ϕ by computing the logit of child node nc as log((gϕ)2 + 1). g and ϕ here are
126"
HCOMP-NET MODEL ARCHITECTURE,0.16147308781869688,"again unique to each node.
127"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.16241737488196412,"3.2
Loss Functions Used to Train HComP-Net
128"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.16336166194523136,"Contrastive Losses for Learning Hierarchical Prototypes: PIP-Net [18] introduced the idea of
129"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.1643059490084986,"using self-supervised contrastive learning to learn semantically meaningful prototypes. We build
130"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.1652502360717658,"upon this idea in our work to learn semantically meaningful hierarchical prototypes at every node
131"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.16619452313503305,"in the tree as follows. For every input image x, we pass in two augmentations of the image, x′ and
132"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.1671388101983003,"x′′ to our framework. The prototype score maps for the two augmentations, ˆZ
′ and ˆZ
′′, are then
133"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.1680830972615675,"considered as positive pairs. Since ˆzh,w ∈RK represents the probabilities of patch zh,w being similar
134"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.16902738432483475,"to the prototypes from P, we align the probabilities from the two augmentations ˆz
′
h,w and ˆz
′′
h,w to be
135"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.16997167138810199,"similar using the following alignment loss:
136"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.17091595845136923,"LA = −
1
HW X"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.17186024551463644,"(h,w)∈H×W
log(ˆz
′
h,w · ˆz
′′
h,w)
(1)"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.17280453257790368,"Since PK
i=1 ˆzh,w,i = 1 due to softmax operation, LA is minimum (i.e., LA = 0) when both ˆz
′
h,w
137"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.17374881964117092,"and ˆz
′′
h,w are identical one-hot encoded vectors. A trivial solution that minimizes LA is when all
138"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.17469310670443816,"patches across all images are similar to the same prototype. To avoid such representation collapse, we
139"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.17563739376770537,"use the following tanh-loss LT of PIP-Net [18], which serves the same purpose as uniformity losses
140"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.17658168083097261,"in [21] and [22]:
141"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.17752596789423986,"LT = −1 K K
X"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.17847025495750707,"i=1
log(tanh( B
X"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.1794145420207743,"b=1
gb,i)),
(2)"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.18035882908404155,"where gb,i is the prototype score for prototype i with respect to image b of mini-batch. LT encourages
142"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.1813031161473088,"each prototype pi to be activated at least once in a given mini-batch of B images, thereby helping to
143"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.182247403210576,"avoid the possibility of representation collapse. The use of tanh ensures that only the presence of a
144"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.18319169027384324,"prototype is taken into account and not its frequency.
145"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.18413597733711048,"Over-specificity Loss: To achieve the goal of learning prototypes common to all descendant species
146"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.18508026440037773,"of an internal node, we introduce a novel loss, termed over-specificity loss Lovsp that avoids learning
147"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.18602455146364494,"over-specific prototypes at any node n. Lovsp is formulated as a modification of the tanh-loss such
148"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.18696883852691218,"that prototype pi is encouraged to be activated at least once in every one of the descendant species
149"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.18791312559017942,"d ∈{1, . . . , Di} of its corresponding child node in the mini-batch of images fed to the model, as
150"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.18885741265344666,"follows:
151"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.18980169971671387,"Lovsp = −1 K K
X i=1 Di
X"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.1907459867799811,"d=1
log(tanh(
X"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.19169027384324835,"b∈Bd
gb,i)),
(3)"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.19263456090651557,"where Bd is the subset of images in the mini-batch that belong to species d.
152"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.1935788479697828,"Discriminative loss: In order to ensure that a learned prototype for a child node nc is not activated
153"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.19452313503305005,"by any of its contrasting set of species (i.e., species that are descendants of child nodes of n other
154"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.1954674220963173,"than nc), we introduce another novel loss function, Ldisc, defined as follows:
155"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.1964117091595845,"Ldisc = 1 K K
X i=1 X"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.19735599622285174,"d∈f
Di"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.19830028328611898,"max
b∈Bd(gb,i),
(4)"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.19924457034938622,"where f
Di is the contrasting set of all descendant species of child nodes of n other than nc. This is
156"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.20018885741265344,"similar to the seperation loss used in other prototype-based methods such as [10], [13], and [23].
157"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.20113314447592068,"Orthogonality loss: We also apply kernel orthogonality as introduced in [24] to the prototype vectors
158"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.20207743153918792,"at every node n, so that the learned prototypes are orthogonal and capture diverse features:
159"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.20302171860245516,"Lorth = ∥ˆPˆP⊤−I∥2
F
(5)"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.20396600566572237,"where ˆP is the matrix of normalized prototype vectors of size C × K, I is an identity matrix, and
160"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.2049102927289896,"∥.∥2
F is the Frobenius norm. Each prototype ˆpi in ˆP is normalized as, ˆpi =
pi
∥pi∥.
161"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.20585457979225685,"Classification loss: Finally, we apply cross entropy loss for classification at each internal node as
162"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.20679886685552407,"follows:
163"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.2077431539187913,"LCE = − B
X"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.20868744098205855,"b
yb log(ˆyb)
(6)"
LOSS FUNCTIONS USED TO TRAIN HCOMP-NET,0.2096317280453258,"where y is ground truth label and ˆy is the prediction at every node of the tree.
164"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.210576015108593,"3.3
Masking Module to Identify Over-specific Prototypes
165"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.21152030217186024,"We employ an additional masking module at every node n to identify over-specific prototypes without
166"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.21246458923512748,"hampering their training. The learned mask for prototype pi simply serves as an indicator of whether
167"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.21340887629839472,"pi is over-specific or not, enabling our approach to abstain from finding common prototypes if there
168"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.21435316336166194,"are none, especially at higher levels of the tree. To obtain the mask values, we first calculate the
169"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.21529745042492918,"over-specificity score for prototype pi as the product of the maximum prototype scores obtained
170"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.21624173748819642,"across all images in the mini-batch belonging to every descendant species d as:
171"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.21718602455146366,"Oi = − Di
Y"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.21813031161473087,"d=1
max
(b∈Bd)(gb,i)
(7)"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.2190745986779981,"where gb,i is the prototype score for prototype pi with respect to image b of mini-batch and Bd
172"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.22001888574126535,"is the subset of images in the mini-batch that belong to descendant species d. Since gb,i takes a
173"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.22096317280453256,"value between 0 to 1 due to the softmax operation, Oi ranges from -1 to 0, where -1 denotes least
174"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.2219074598677998,"over-specificity and 0 denotes the most over-specificity. The multiplication of the prototype scores
175"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.22285174693106705,"ensures that even when the score is less with respect to only one descendant species, the prototype
176"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.2237960339943343,"will be assigned a high over-specificity score (close to 0).
177"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.2247403210576015,"As shown in Figure 3, Oi is then fed into the masking module, which includes a learned mask value
178"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.22568460812086874,"Mi for every prototype pi. We generate Mi from a Gumbel-softmax distribution [25] so that the
179"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.22662889518413598,"values are skewed to be very close to either 0 or 1, i.e., Mi = Gumbel-Softmax(γi, τ), where γi are
180"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.22757318224740322,"the learnable parameters of the distribution and τ is temperature. We then compute the masking loss,
181"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.22851746931067043,"Lmask, as:
182"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.22946175637393768,"Lmask = K
X"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.23040604343720492,"i=1
(λmaskMi ◦stopgrad(Oi) + λL1∥Mi∥1)
(8)"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.23135033050047216,"where λmask and λL1 are trade-off coefficients, ∥.∥1 is the L1 norm added to induce sparsity in
183"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.23229461756373937,"the masks, and stopgrad represents the stop gradient operation applied over Oi to ensure that the
184"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.2332389046270066,"gradient of Lmask does not flow back to the learning of prototype vectors and impact their training.
185"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.23418319169027385,"Note that the learned masks are not used for pruning the prototypes during training, they are only
186"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.23512747875354106,"used during inference to determine which of the learned prototypes are over-specific and likely to not
187"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.2360717658168083,"represent evolutionary traits. Therefore, even if all the prototypes are identified as over-specific by
188"
MASKING MODULE TO IDENTIFY OVER-SPECIFIC PROTOTYPES,0.23701605288007555,"the masking module at an internal node, it will not affect the classification performance at that node.
189"
TRAINING HCOMP-NET,0.23796033994334279,"3.4
Training HComP-Net
190"
TRAINING HCOMP-NET,0.23890462700661,"We first pre-train the prototypes at every internal node in a self-supervised learning manner using
191"
TRAINING HCOMP-NET,0.23984891406987724,"alignment and tanh-losses as LSS = λALA+λT LT . We then fine-tune the model using the following
192"
TRAINING HCOMP-NET,0.24079320113314448,"combined loss: (λCELCE +LSS +λovspLovsp +λdiscLdisc +λorthLorth +Lmask), where λ’s are
193"
TRAINING HCOMP-NET,0.24173748819641172,"trade-off parameters. Note that the loss is applied over every node in the tree. We show an ablation of
194"
TRAINING HCOMP-NET,0.24268177525967893,"key loss terms in our framework in Table 6 in the Supplementary Section.
195"
EXPERIMENTAL SETUP,0.24362606232294617,"4
Experimental Setup
196"
EXPERIMENTAL SETUP,0.24457034938621341,"Dataset: In our experiments, we primarily focus on the 190 species of birds (Bird) from the CUB-200-
197"
EXPERIMENTAL SETUP,0.24551463644948066,"2011 [8] dataset for which the phylogenetic relationship [26] is known. The tree is quite large with a
198"
EXPERIMENTAL SETUP,0.24645892351274787,"total of 184 internal nodes. We removed the background from the images to avoid the possibility of
199"
EXPERIMENTAL SETUP,0.2474032105760151,"learning prototypes corresponding to background information such as the bird’s habitat as we are
200"
EXPERIMENTAL SETUP,0.24834749763928235,"only interested in the traits corresponding to the body of the organism. We also apply our method on
201"
EXPERIMENTAL SETUP,0.24929178470254956,"a fish dataset with 38 species (Fish) [9] along with its associated phylogeny [9] and 30 subspecies
202"
EXPERIMENTAL SETUP,0.2502360717658168,"of Heliconius butterflies (Butterfly) from the Jiggins Heliconius Collection dataset [16] collected
203"
EXPERIMENTAL SETUP,0.251180358829084,"from various sources 1 along with its phylogeny [52, 53]. The qualitative results of Butterfly and
204"
EXPERIMENTAL SETUP,0.2521246458923513,"Fish datasets are provided in the supplementary materials. The complete details of hyper-parameter
205"
EXPERIMENTAL SETUP,0.2530689329556185,"settings and training strategy are also provided in the Supplementary Section E.
206"
EXPERIMENTAL SETUP,0.25401322001888577,"Baselines: We compare HComP-Net to ResNet-50 [54], INTR (Interpretable Transformer) [55] and
207"
EXPERIMENTAL SETUP,0.254957507082153,"HPnet [17]. For HPnet, we used the same hyperparameter settings and training strategy as used by
208"
EXPERIMENTAL SETUP,0.2559017941454202,"ProtoPNet for CUB-200-2011 dataset. For a fair comparison, we also set the number of prototypes
209"
EXPERIMENTAL SETUP,0.25684608120868746,"for each child in HPnet to be equal to 10 similar to our implementation. We follow the same training
210"
EXPERIMENTAL SETUP,0.2577903682719547,"strategy as provided by ProtoPNet for CUB-200-2011 dataset.
211"
RESULTS,0.2587346553352219,"5
Results
212"
FINE-GRAINED ACCURACY,0.25967894239848915,"5.1
Fine-grained Accuracy
213"
FINE-GRAINED ACCURACY,0.26062322946175637,"Similar to HPnet [17], we calculate the fine-grained accuracy for each leaf node by calculating the
214"
FINE-GRAINED ACCURACY,0.2615675165250236,"path probability over every image. During inference, the final probability for leaf class Y given
215"
FINE-GRAINED ACCURACY,0.26251180358829085,"an image X is calculated as, P(Y |X) = P(Y (1), Y (2), ..., Y (L)|X) = QL
l=1 P(Y (l)|X), where
216"
FINE-GRAINED ACCURACY,0.26345609065155806,"P(Y (l)|X) is the probability of assigning image X to a node at level l, and L is the depth of the
217"
FINE-GRAINED ACCURACY,0.26440037771482533,"leaf node. Every image is assigned to the leaf class with maximum path probability, which is used
218"
FINE-GRAINED ACCURACY,0.26534466477809254,"to compute the fine-grained accuracy. The comparison of the fine-grained accuracy calculated for
219"
FINE-GRAINED ACCURACY,0.26628895184135976,"1Sources: [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]"
FINE-GRAINED ACCURACY,0.267233238904627,"(a) HPnet
(b) HComP-Net"
FINE-GRAINED ACCURACY,0.26817752596789424,"Figure 4: Comparing the part consistency of HPnet and HComP-Net for their prototype learned at an internal
node in the bird dataset that corresponds to 3 descendant species (names shown on the rows). For every species,
we are visualizing the top-3 images with highest prototype score for both HPnet and HComP-Net, shown as the
four columns with zoomed in views of their discovered prototypes. We can see that HPnet highlights varying
parts of the bird across the 3 species and across multiple images of the same species, making it difficult to
associate a consistent semantic meaning to its learned prototype. In contrast, HComP-Net consistently highlights
the head region of the bird across all four species and their images."
FINE-GRAINED ACCURACY,0.26912181303116145,"HComP-Net and the baselines are given in Table 1. We can see that HComP-Net performs better
220"
FINE-GRAINED ACCURACY,0.2700661000944287,"than the other interpretable methods, such as INTR and HPNet, and is also able to nearly match the
221"
FINE-GRAINED ACCURACY,0.27101038715769593,"performance of non-interpretable models, such as ResNet-50, even outperforming it for the Fish
222"
FINE-GRAINED ACCURACY,0.2719546742209632,"and Butterfly dataset. This shows the ability of our proposed framework to achieve competitive
223"
FINE-GRAINED ACCURACY,0.2728989612842304,"classification accuracy along with serving the goal of discovering evolutionary traits.
224"
FINE-GRAINED ACCURACY,0.2738432483474976,"Table 1: % Accuracy
Model
Hierarchy Bird Butterfly Fish"
FINE-GRAINED ACCURACY,0.2747875354107649,"ResNet-50
No
74.18
95.76
86.63
INTR
69.22
95.53
86.73"
FINE-GRAINED ACCURACY,0.2757318224740321,"HPnet
Yes
36.18
94.69
77.51
HComP-Net
70.01
97.35
90.80"
FINE-GRAINED ACCURACY,0.2766761095372993,"Table 2: % Accuracy (on unseen species)
Species Name
HComP-Net HPnet"
FINE-GRAINED ACCURACY,0.2776203966005666,"Fish Crow
53.33
10.55
Rock Wren
53.33
10.22
Indigo Bunting
96.67
49.2
Bohemian Waxwing
70.00
44.9"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.2785646836638338,"5.2
Generalizing to Unseen Species in the Phylogeny
225"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.279508970727101,"We analyze the performance of HComP-Net in generalizing to unseen species that the model hasn’t
226"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.2804532577903683,"seen during training. The biological motivation for this experiment is to evaluate if HComP-Net
227"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.2813975448536355,"can situate newly discovered species at its appropriate position in the phylogeny by identifying its
228"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.28234183191690276,"common ancestors shared with the known species. An added advantage of our work is that along with
229"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.28328611898017,"identifying the ancestor of an unseen species, we can also identify the common traits shared by the
230"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.2842304060434372,"novel species with known species in the phylogeny. Since unseen species cannot be classified to the
231"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.28517469310670446,"finest levels (i.e., up to the leaf node corresponding to the unseen species), we analyze the ability of
232"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.28611898016997167,"HComP-Net to classify unseen species accurately up to one level above the leaf level in the hierarchy.
233"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.2870632672332389,"With this consideration, the final probability of an unseen species for a given image is calculated
234"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.28800755429650615,"as, P(Y |Xunseen) = P(Y (1), Y (2), ..., Y (L−1)|X) = QL−1
l=1 P(Y (l)|X). Note that we leave out the
235"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.28895184135977336,"class probability at the Lth level, since we do not take into account the class probability of the leaf
236"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.2898961284230406,"level. We leave four species from the Bird training set and calculate their accuracy during inference
237"
GENERALIZING TO UNSEEN SPECIES IN THE PHYLOGENY,0.29084041548630785,"in Table 2. We can see that HComP-Net is able to generalize better than HPnet for all four species.
238"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.29178470254957506,"5.3
Analyzing the Semantic Quality of Prototypes
239"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.2927289896128423,"Following the method introduced in PIPNet [18], we assess the semantic quality of our learned
240"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.29367327667610954,"prototypes by evaluating their part purity. A prototype with high part purity (close to 1) is one that
241"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.29461756373937675,"consistently highlights the same image region in the score maps (corresponding to consistent local
242"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.295561850802644,"features such as the eye or wing of a bird) across images belonging to the same class. The part
243"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.29650613786591123,Table 3: Part purity of prototypes on Bird dataset.
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.29745042492917845,"Model
Lovsp Masking Part purity % masked"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.2983947119924457,"HPnet
-
-
0.14 ± 0.09
-
HComP-Net
-
-
0.68 ± 0.22
-
HComP-Net
-
✓
0.75 ± 0.17
21.42%
HComP-Net
✓
-
0.72 ± 0.19
-
HComP-Net
✓
✓
0.77 ± 0.16
16.53%"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.29933899905571293,"purity is calculated using the part locations of
244"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3002832861189802,"15 parts that are provided in the CUB dataset.
245"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3012275731822474,"For each prototype, we take the top-10 im-
246"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3021718602455146,"ages from each leaf descendant. We con-
247"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3031161473087819,"sider the 32×32 image patch that is centered
248"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3040604343720491,"around the max activation location of the pro-
249"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3050047214353163,"totype from the top-10 images. With these
250"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3059490084985836,"top-10 image patches, we calculate for each
251"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3068932955618508,"part how frequently the part is present inside
252"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.307837582625118,"the image patch. For example, a part that is found inside the image patch 8 out of 10 times is given a
253"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3087818696883853,"score of 0.8. In PIP-Net, the highest value among the values calculated for each part is given as the
254"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3097261567516525,"part purity of the prototype. In our approach, since we are dealing with a hierarchy and taking the
255"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.31067044381491976,"top-10 from each leaf descendant, a particular part, let’s say the eye, might have a score of 0.5 for
256"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.311614730878187,"one leaf descendant and 0.7 for a different leaf descendant. Since we want the prototype to represent
257"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3125590179414542,"the same part for all the leaf descendants, we take the lowest score (the weakest link) among all the
258"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.31350330500472146,"leaf descendants as the score of the part. By following this method, for a given prototype we can
259"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.31444759206798867,"arrive at a value for each part and finally take the maximum among the values as the purity of the
260"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3153918791312559,"prototype. We take the mean of the part purity across all the prototypes and report the results in Table
261"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.31633616619452315,"3 for different ablations of HComP-Net and also HPnet, which is the only baseline method that can
262"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.31728045325779036,"learn hierarchical prototypes.
263"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3182247403210576,"We can see that HComP-Net, even without the use of over-specificity loss performs much better than
264"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.31916902738432484,"HPnet due to the contrastive learning approach we have adopted from PIPNet [18]. The addition
265"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.32011331444759206,"of over-specificity loss improves the part purity because over-specific prototypes tend to have poor
266"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3210576015108593,"part purity for some of the leaf descendants which will affect their overall part purity score. Further,
267"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.32200188857412654,"for both ablations with and without over-specificity loss, we apply the masking module and remove
268"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.32294617563739375,"masked (over-specific) prototypes during the calculation of part purity. We see that the part purity goes
269"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.323890462700661,"higher by applying the masking module, demonstrating its effectiveness in identifying over-specific
270"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.32483474976392823,"prototypes. We further compute the purity of masked-out prototypes and notice that the masked-out
271"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.32577903682719545,"prototypes have drastically lower part purity (0.29 ± 0.17) compared to non-masked prototypes
272"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3267233238904627,"(0.77 ± 0.16). An alternative approach to learning the masking module is to identify over-specific
273"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3276676109537299,"prototypes using a fixed global threshold over Oi. We show in Table 9 of Supplementary Section F,
274"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3286118980169972,"that given the right choice of such a threshold, we can identify over-specific prototypes. However,
275"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3295561850802644,"selecting the ideal threshold can be non-trivial. On the other hand, our masking module learns the
276"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3305004721435316,"appropriate threshold dynamically as part of the training process.
277"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3314447592067989,"Figure 4 visualizes the part consistency of prototypes discovered by HComP-Net in comparison to
278"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3323890462700661,"HPnet for the bird dataset. We can see that HComP-Net is finding a consistent region in the image
279"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3333333333333333,"(corresponding to the head region) across all three descendant species and all images of a species, in
280"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3342776203966006,"contrast to HPnet. Futhermore, thanks to the alignment loss, every patch ˆzh,w is encoded as nearly
281"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3352219074598678,"a one-hot encoding with respect to the K prototypes which causes the prototype score maps to be
282"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.336166194523135,"highly localized. The concise and focused nature of the prototype score maps makes the interpretation
283"
ANALYZING THE SEMANTIC QUALITY OF PROTOTYPES,0.3371104815864023,"much more effective compared to baselines.
284"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3380547686496695,"5.4
Analyzing Evolutionary Traits Discovered by HComP-Net
285"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.33899905571293676,"We now qualitatively analyze some of the hypothesized evolutionary traits discovered in the hierarchy
286"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.33994334277620397,"of prototypes learned by HComP-Net. Figure 5 shows the hierarchy of prototypes discovered over
287"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3408876298394712,"a small subtree of the phylogeny from Bird (four species) and Fish (three species) dataset. In the
288"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.34183191690273845,"visualization of bird prototypes, we can see that the two Pelican species share a consistent region in the
289"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.34277620396600567,"learned Prototype labeled 2, which corresponds to the head region of the birds. We can hypothesize
290"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3437204910292729,"this prototype to be capturing the white colored crown common to the two species. On the other hand,
291"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.34466477809254015,"Prototype 1 finds the shared trait of similar beak morphology (e.g., sharpness of beaks) across the
292"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.34560906515580736,"two Cormorant species. We can see that HComP-Net avoids the learning of over-specific prototypes
293"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3465533522190746,"at internal nodes, which are pushed down to individual leaf nodes, as shown in visualizations of
294"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.34749763928234184,"Prototype 3, 4, 5, and 6. Similarly, in the visualization of the fish prototypes, we can see that Prototype
295"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.34844192634560905,"1 is highlighting a specific fin (dorsal fin) of the Carassius auratus and Notropis hudsonius species,
296"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3493862134088763,"possibly representing their pigmentation and structure, which is noticeably different compared to
297"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.35033050047214354,"the contrasting species of Alosa chrysochloris. Note that while HComP-Net identifies the common
298"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.35127478753541075,"Figure 5: Visualizing the hierarchy of prototypes discovered by HComP-Net for birds and fishes. *Note that
the textual descriptions of the hypothesized traits shown for every prototype are based on human interpretation."
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.352219074598678,"Figure 6: We trace the prototypes learned for Western Grebe at three different levels in the phylogenetic tree
(corresponding to different periods of time in evolution). Text in blue is the interpretation of common traits of
descendants found by HComP-Net at every ancestor node of Western Grebe."
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.35316336166194523,"regions corresponding to each prototype (shown as heatmaps), the textual descriptions of the traits
299"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.35410764872521244,"provided in Figure 5 are based on human interpretation.
300"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3550519357884797,"Figure 6 shows another visualization of the sequence of prototypes learned by HComP-Net for the
301"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3559962228517469,"Western Grebe species at different levels of the phylogeny. We can see that at level 0, we are capturing
302"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.35694050991501414,"features closer to the neck region, indicating the likely difference between the length of necks between
303"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3578847969782814,"Grebe species and other species (Cuckoo, Albatross, and Fulmar) that diversify at an earlier time in
304"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3588290840415486,"the process of evolution. At level 1, the prototype is focusing on the eye region, potentially indicating
305"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3597733711048159,"to difference in the color of red and black patterns around the eyes. At level 2, we are differentiating
306"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3607176581680831,"Western Grebe from Horned Grebe based on the feature of bills. We also validate our prototypes by
307"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3616619452313503,"comparing them with the multi-head cross-attention maps learned by INTR [55]. We can see that
308"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3626062322946176,"some of the prototypes discovered by HComP-Net can be mapped to equivalent attention heads of
309"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3635505193578848,"INTR. However, while INTR is designed to produce a flat structure of attention maps, we are able
310"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.364494806421152,"to place these maps on the tree of life. This shows the power of HComP-Net in generating novel
311"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3654390934844193,"hypotheses about how trait changes may have evolved and accumulated across different branches of
312"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.3663833805476865,"the phylogeny. Additional visualizations of discovered evolutionary traits for butterfly species and
313"
ANALYZING EVOLUTIONARY TRAITS DISCOVERED BY HCOMP-NET,0.36732766761095376,"fish species are provided in the supplementary section in Figures 7 to 16.
314"
CONCLUSION,0.36827195467422097,"6
Conclusion
315"
CONCLUSION,0.3692162417374882,"We introduce a novel approach for learning hierarchy-aligned prototypes while avoiding the learning
316"
CONCLUSION,0.37016052880075545,"of over-specific features at internal nodes of the phylogenetic tree, enabling the discovery of novel
317"
CONCLUSION,0.37110481586402266,"evolutionary traits. Our empirical analysis on birds, fishes, and butterflies, demonstrates the efficacy
318"
CONCLUSION,0.3720491029272899,"of HComP-Net over baseline methods. Furthermore, HComP-Net demonstrates a unique ability
319"
CONCLUSION,0.37299338999055714,"to generate novel hypotheses about evolutionary traits, showcasing its potential in advancing our
320"
CONCLUSION,0.37393767705382436,"understanding of evolution. We discuss the limitations of our work in Supplementary Section I. While
321"
CONCLUSION,0.37488196411709157,"we focus on the biological problem of discovering evolutionary traits, our work can be applied in
322"
CONCLUSION,0.37582625118035884,"general to domains involving a hierarchy of classes, which can be explored in future research.
323"
REFERENCES,0.37677053824362605,"References
324"
REFERENCES,0.3777148253068933,"[1] David Houle and Daniela M Rossoni. Complexity, evolvability, and the process of adaptation.
325"
REFERENCES,0.37865911237016053,"Annual Review of Ecology, Evolution, and Systematics, 53, 2022.
326"
REFERENCES,0.37960339943342775,"[2] Maureen A O’Leary and Seth Kaufman. Morphobank: phylophenomics in the “cloud”. Cladis-
327"
REFERENCES,0.380547686496695,"tics, 27(5):529–537, 2011.
328"
REFERENCES,0.3814919735599622,"[3] Tiago R Simões, Michael W Caldwell, Alessandro Palci, and Randall L Nydam. Giant taxon-
329"
REFERENCES,0.38243626062322944,"character matrices: quality of character constructions remains critical regardless of size. Cladis-
330"
REFERENCES,0.3833805476864967,"tics, 33(2):198–219, 2017.
331"
REFERENCES,0.3843248347497639,"[4] Paul C Sereno. Logical basis for morphological characters in phylogenetics. Cladistics,
332"
REFERENCES,0.38526912181303113,"23(6):565–587, 2007.
333"
REFERENCES,0.3862134088762984,"[5] Moritz D Lürig, Seth Donoughe, Erik I Svensson, Arthur Porto, and Masahito Tsuboi. Computer
334"
REFERENCES,0.3871576959395656,"vision, machine learning, and the promise of phenomics in ecology and evolutionary biology.
335"
REFERENCES,0.3881019830028329,"Frontiers in Ecology and Evolution, 9:642774, 2021.
336"
REFERENCES,0.3890462700661001,"[6] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig
337"
REFERENCES,0.3899905571293673,"Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection
338"
REFERENCES,0.3909348441926346,"dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition,
339"
REFERENCES,0.3918791312559018,"pages 8769–8778, 2018.
340"
REFERENCES,0.392823418319169,"[7] Randal A Singer, Kevin J Love, and Lawrence M Page. A survey of digitized data from us fish
341"
REFERENCES,0.3937677053824363,"collections in the idigbio data aggregator. PloS one, 13(12):e0207636, 2018.
342"
REFERENCES,0.3947119924457035,"[8] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The
343"
REFERENCES,0.39565627950897075,"caltech-ucsd birds-200-2011 dataset. 2011.
344"
REFERENCES,0.39660056657223797,"[9] Mohannad Elhamod, Mridul Khurana, Harish Babu Manogaran, Josef C Uyeda, Meghan A
345"
REFERENCES,0.3975448536355052,"Balk, Wasila Dahdul, Yasin Bakis, Henry L Bart Jr, Paula M Mabee, Hilmar Lapp, et al.
346"
REFERENCES,0.39848914069877245,"Discovering novel biological traits from images using phylogeny-guided neural networks. In
347"
REFERENCES,0.39943342776203966,"Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,
348"
REFERENCES,0.4003777148253069,"pages 3966–3978, 2023.
349"
REFERENCES,0.40132200188857414,"[10] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su.
350"
REFERENCES,0.40226628895184136,"This looks like that: deep learning for interpretable image recognition. Advances in neural
351"
REFERENCES,0.40321057601510857,"information processing systems, 32, 2019.
352"
REFERENCES,0.40415486307837584,"[11] Dawid Rymarczyk, Łukasz Struski, Jacek Tabor, and Bartosz Zieli´nski. Protopshare: Prototypi-
353"
REFERENCES,0.40509915014164305,"cal parts sharing for similarity discovery in interpretable image classification. In Proceedings of
354"
REFERENCES,0.4060434372049103,"the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 1420–1430,
355"
REFERENCES,0.40698772426817753,"2021.
356"
REFERENCES,0.40793201133144474,"[12] Dawid Rymarczyk, Łukasz Struski, Michał Górszczak, Koryna Lewandowska, Jacek Tabor, and
357"
REFERENCES,0.408876298394712,"Bartosz Zieli´nski. Interpretable image classification with differentiable prototypes assignment.
358"
REFERENCES,0.4098205854579792,"In European Conference on Computer Vision, pages 351–368. Springer, 2022.
359"
REFERENCES,0.41076487252124644,"[13] Meike Nauta, Ron Van Bree, and Christin Seifert. Neural prototype trees for interpretable
360"
REFERENCES,0.4117091595845137,"fine-grained image recognition. In Proceedings of the IEEE/CVF Conference on Computer
361"
REFERENCES,0.4126534466477809,"Vision and Pattern Recognition, pages 14933–14943, 2021.
362"
REFERENCES,0.41359773371104813,"[14] Luke J Harmon, Jonathan B Losos, T Jonathan Davies, Rosemary G Gillespie, John L Gittleman,
363"
REFERENCES,0.4145420207743154,"W Bryan Jennings, Kenneth H Kozak, Mark A McPeek, Franck Moreno-Roark, Thomas J Near,
364"
REFERENCES,0.4154863078375826,"et al. Early bursts of body size and shape evolution are rare in comparative data. Evolution,
365"
REFERENCES,0.4164305949008499,"64(8):2385–2396, 2010.
366"
REFERENCES,0.4173748819641171,"[15] Matthew W Pennell, Richard G FitzJohn, William K Cornwell, and Luke J Harmon. Model
367"
REFERENCES,0.4183191690273843,"adequacy and the macroevolution of angiosperm functional traits. The American Naturalist,
368"
REFERENCES,0.4192634560906516,"186(2):E33–E50, 2015.
369"
REFERENCES,0.4202077431539188,"[16] Christopher Lawrence and Elizabeth G. Campolongo. Heliconius collection (cambridge butter-
370"
REFERENCES,0.421152030217186,"fly), 2024.
371"
REFERENCES,0.42209631728045327,"[17] Peter Hase, Chaofan Chen, Oscar Li, and Cynthia Rudin. Interpretable image recognition with
372"
REFERENCES,0.4230406043437205,"hierarchical prototypes. In Proceedings of the AAAI Conference on Human Computation and
373"
REFERENCES,0.42398489140698775,"Crowdsourcing, volume 7, pages 32–40, 2019.
374"
REFERENCES,0.42492917847025496,"[18] Meike Nauta, Jörg Schlötterer, Maurice van Keulen, and Christin Seifert. Pip-net: Patch-based
375"
REFERENCES,0.4258734655335222,"intuitive prototypes for interpretable image classification. In Proceedings of the IEEE/CVF
376"
REFERENCES,0.42681775259678945,"Conference on Computer Vision and Pattern Recognition, pages 2744–2753, 2023.
377"
REFERENCES,0.42776203966005666,"[19] Adrian Hoffmann, Claudio Fanconi, Rahul Rade, and Jonas Kohler. This looks like that... does
378"
REFERENCES,0.42870632672332387,"it? shortcomings of latent space prototype interpretability in deep networks. arXiv preprint
379"
REFERENCES,0.42965061378659114,"arXiv:2105.02968, 2021.
380"
REFERENCES,0.43059490084985835,"[20] Sunnie SY Kim, Nicole Meister, Vikram V Ramaswamy, Ruth Fong, and Olga Russakovsky.
381"
REFERENCES,0.43153918791312557,"Hive: Evaluating the human interpretability of visual explanations. In European Conference on
382"
REFERENCES,0.43248347497639283,"Computer Vision, pages 280–298. Springer, 2022.
383"
REFERENCES,0.43342776203966005,"[21] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through
384"
REFERENCES,0.4343720491029273,"alignment and uniformity on the hypersphere. In International Conference on Machine Learning,
385"
REFERENCES,0.43531633616619453,"pages 9929–9939. PMLR, 2020.
386"
REFERENCES,0.43626062322946174,"[22] Thalles Silva and Adín Ramírez Rivera. Representation learning via consistent assignment of
387"
REFERENCES,0.437204910292729,"views to clusters. In Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing,
388"
REFERENCES,0.4381491973559962,"pages 987–994, 2022.
389"
REFERENCES,0.43909348441926344,"[23] Jiaqi Wang, Huafeng Liu, Xinyue Wang, and Liping Jing. Interpretable image recognition
390"
REFERENCES,0.4400377714825307,"by constructing transparent embedding space. In Proceedings of the IEEE/CVF International
391"
REFERENCES,0.4409820585457979,"Conference on Computer Vision, pages 895–904, 2021.
392"
REFERENCES,0.44192634560906513,"[24] Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, and Stella X Yu. Orthogonal convolutional
393"
REFERENCES,0.4428706326723324,"neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern
394"
REFERENCES,0.4438149197355996,"recognition, pages 11505–11515, 2020.
395"
REFERENCES,0.4447592067988669,"[25] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.
396"
REFERENCES,0.4457034938621341,"arXiv preprint arXiv:1611.01144, 2016.
397"
REFERENCES,0.4466477809254013,"[26] W. Jetz, G. H. Thomas, J. B. Joy, K. Hartmann, and A. O. Mooers. The global diversity of birds
398"
REFERENCES,0.4475920679886686,"in space and time. Nature, 491:444–448, 2012.
399"
REFERENCES,0.4485363550519358,"[27] Gabriela Montejo-Kovacevich, Eva van der Heijden, Nicola Nadeau, and Chris Jiggins. Cam-
400"
REFERENCES,0.449480642115203,"bridge butterfly wing collection batch 10, November 2020.
401"
REFERENCES,0.45042492917847027,"[28] Patricio A. Salazar, Nicola Nadeau, Gabriela Montejo-Kovacevich, and Chris Jiggins. Sheffield
402"
REFERENCES,0.4513692162417375,"butterfly wing collection - Patricio Salazar, Nicola Nadeau, Ikiam broods batch 1 and 2, Novem-
403"
REFERENCES,0.4523135033050047,"ber 2020.
404"
REFERENCES,0.45325779036827196,"[29] Gabriela Montejo-Kovacevich, Chris Jiggins, and Ian Warren. Cambridge butterfly wing
405"
REFERENCES,0.4542020774315392,"collection batch 2, May 2019.
406"
REFERENCES,0.45514636449480644,"[30] Chris Jiggins, Gabriela Montejo-Kovacevich, Ian Warren, and Eva Wiltshire. Cambridge
407"
REFERENCES,0.45609065155807366,"butterfly wing collection batch 3, May 2019.
408"
REFERENCES,0.45703493862134087,"[31] Gabriela Montejo-Kovacevich, Chris Jiggins, and Ian Warren. Cambridge butterfly wing
409"
REFERENCES,0.45797922568460814,"collection batch 4, May 2019.
410"
REFERENCES,0.45892351274787535,"[32] Gabriela Montejo-Kovacevich, Chris Jiggins, Ian Warren, and Eva Wiltshire. Cambridge
411"
REFERENCES,0.45986779981114256,"butterfly wing collection batch 5, May 2019.
412"
REFERENCES,0.46081208687440983,"[33] Ian Warren and Chris Jiggins. Miscellaneous Heliconius wing photographs (2001-2019) Part 1,
413"
REFERENCES,0.46175637393767704,"February 2019.
414"
REFERENCES,0.4627006610009443,"[34] Ian Warren and Chris Jiggins. Miscellaneous Heliconius wing photographs (2001-2019) Part 3,
415"
REFERENCES,0.4636449480642115,"February 2019.
416"
REFERENCES,0.46458923512747874,"[35] Gabriela Montejo-Kovacevich, Chris Jiggins, Ian Warren, and Eva Wiltshire. Cambridge
417"
REFERENCES,0.465533522190746,"butterfly wing collection batch 6, May 2019.
418"
REFERENCES,0.4664778092540132,"[36] Chris Jiggins and Ian Warren. Cambridge butterfly wing collection - Chris Jiggins 2001/2
419"
REFERENCES,0.46742209631728043,"broods batch 1, January 2019.
420"
REFERENCES,0.4683663833805477,"[37] Chris Jiggins and Ian Warren. Cambridge butterfly wing collection - Chris Jiggins 2001/2
421"
REFERENCES,0.4693106704438149,"broods batch 2, January 2019.
422"
REFERENCES,0.4702549575070821,"[38] Joana I. Meier, Patricio Salazar, Gabriela Montejo-Kovacevich, Ian Warren, and Chris Jggins.
423"
REFERENCES,0.4711992445703494,"Cambridge butterfly wing collection - Patricio Salazar PhD wild specimens batch 3, October
424"
REFERENCES,0.4721435316336166,"2020.
425"
REFERENCES,0.4730878186968839,"[39] Gabriela Montejo-Kovacevich, Chris Jiggins, and Ian Warren. Cambridge butterfly wing
426"
REFERENCES,0.4740321057601511,"collection batch 1- version 2, May 2019.
427"
REFERENCES,0.4749763928234183,"[40] Gabriela Montejo-Kovacevich, Chris Jiggins, Ian Warren, Camilo Salazar, Marianne Elias,
428"
REFERENCES,0.47592067988668557,"Imogen Gavins, Eva Wiltshire, Stephen Montgomery, and Owen McMillan. Cambridge and
429"
REFERENCES,0.4768649669499528,"collaborators butterfly wing collection batch 10, May 2019.
430"
REFERENCES,0.47780925401322,"[41] Patricio Salazar, Gabriela Montejo-Kovacevich, Ian Warren, and Chris Jiggins. Cambridge
431"
REFERENCES,0.47875354107648727,"butterfly wing collection - Patricio Salazar PhD wild and bred specimens batch 1, December
432"
REFERENCES,0.4796978281397545,"2018.
433"
REFERENCES,0.4806421152030217,"[42] Gabriela Montejo-Kovacevich, Chris Jiggins, Ian Warren, and Eva Wiltshire. Cambridge
434"
REFERENCES,0.48158640226628896,"butterfly wing collection batch 7, May 2019.
435"
REFERENCES,0.4825306893295562,"[43] Patricio Salazar, Gabriela Montejo-Kovacevich, Ian Warren, and Chris Jiggins. Cambridge
436"
REFERENCES,0.48347497639282344,"butterfly wing collection - Patricio Salazar PhD wild and bred specimens batch 2, January 2019.
437"
REFERENCES,0.48441926345609065,"[44] Erika Pinheiro de Castro, Christopher Jiggins, Karina Lucas da Silva-Brand˘00e3o, Andre Victor
438"
REFERENCES,0.48536355051935787,"Lucci Freitas, Marcio Zikan Cardoso, Eva Van Der Heijden, Joana Meier, and Ian Warren.
439"
REFERENCES,0.48630783758262514,"Brazilian Butterflies Collected December 2020 to January 2021, February 2022.
440"
REFERENCES,0.48725212464589235,"[45] Gabriela Montejo-Kovacevich, Chris Jiggins, Ian Warren, and Eva Wiltshire. Cambridge
441"
REFERENCES,0.48819641170915956,"butterfly wing collection batch 8, May 2019.
442"
REFERENCES,0.48914069877242683,"[46] Gabriela Montejo-Kovacevich, Chris Jiggins, Ian Warren, Eva Wiltshire, and Imogen Gavins.
443"
REFERENCES,0.49008498583569404,"Cambridge butterfly wing collection batch 9, May 2019.
444"
REFERENCES,0.4910292728989613,"[47] Gabriela Montejo-Kovacevich, Eva van der Heijden, and Chris Jiggins. Cambridge butterfly
445"
REFERENCES,0.4919735599622285,"collection - GMK Broods Ikiam 2018, November 2020.
446"
REFERENCES,0.49291784702549574,"[48] Gabriela Montejo-Kovacevich, Quentin Paynter, and Amin Ghane. Heliconius erato cyrbia,
447"
REFERENCES,0.493862134088763,"Cook Islands (New Zealand) 2016, 2019, 2021, September 2021.
448"
REFERENCES,0.4948064211520302,"[49] Ian Warren and Chris Jiggins. Miscellaneous Heliconius wing photographs (2001-2019) Part 2,
449"
REFERENCES,0.49575070821529743,"February 2019.
450"
REFERENCES,0.4966949952785647,"[50] Camilo Salazar, Gabriela Montejo-Kovacevich, Chris Jiggins, Ian Warren, and Imogen Gavins.
451"
REFERENCES,0.4976392823418319,"Camilo Salazar and Cambridge butterfly wing collection batch 1, May 2019.
452"
REFERENCES,0.4985835694050991,"[51] Anniina Mattila, Chris Jiggins, and Ian Warren. University of Helsinki butterfly collection -
453"
REFERENCES,0.4995278564683664,"Anniina Mattila bred specimens, February 2019.
454"
REFERENCES,0.5004721435316336,"[52] OpenTreeOfLife, Benjamin Redelings, Luna Luisa Sanchez Reyes, Karen A. Cranston, Jim
455"
REFERENCES,0.5014164305949008,"Allman, Mark T. Holder, and Emily Jane McTavish. Open tree of life synthetic tree, 2019.
456"
REFERENCES,0.502360717658168,"[53] Francois Michonneau, Joseph W. Brown, and David J. Winter. rotl: an r package to interact
457"
REFERENCES,0.5033050047214354,"with the open tree of life data. Methods in Ecology and Evolution, 7(12):1476–1481, 2016.
458"
REFERENCES,0.5042492917847026,"[54] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
459"
REFERENCES,0.5051935788479698,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
460"
REFERENCES,0.506137865911237,"pages 770–778, 2016.
461"
REFERENCES,0.5070821529745042,"[55] Dipanjyoti Paul, Arpita Chowdhury, Xinqi Xiong, Feng-Ju Chang, David Carlyn, Samuel
462"
REFERENCES,0.5080264400377715,"Stevens, Kaiya Provost, Anuj Karpatne, Bryan Carstens, Daniel Rubenstein, et al. A simple
463"
REFERENCES,0.5089707271010387,"interpretable transformer for fine-grained image classification and analysis. arXiv preprint
464"
REFERENCES,0.509915014164306,"arXiv:2311.04157, 2023.
465"
REFERENCES,0.5108593012275732,"[56] Abien Fred Agarap.
Deep learning using rectified linear units (relu).
arXiv preprint
466"
REFERENCES,0.5118035882908404,"arXiv:1803.08375, 2018.
467"
REFERENCES,0.5127478753541076,"[57] Samuel G Müller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-art data
468"
REFERENCES,0.5136921624173749,"augmentation. In Proceedings of the IEEE/CVF international conference on computer vision,
469"
REFERENCES,0.5146364494806421,"pages 774–782, 2021.
470"
REFERENCES,0.5155807365439093,"[58] R. Farrell. Cub-200-2011 segmentations (1.0) [data set], 2024.
471"
REFERENCES,0.5165250236071766,"NeurIPS Paper Checklist
472"
CLAIMS,0.5174693106704438,"1. Claims
473"
CLAIMS,0.5184135977337111,"Question: Do the main claims made in the abstract and introduction accurately reflect the
474"
CLAIMS,0.5193578847969783,"paper’s contributions and scope?
475"
CLAIMS,0.5203021718602455,"Answer: [Yes]
476"
CLAIMS,0.5212464589235127,"Justification: We claim that HComP-Net can generate novel hypotheses for potential evo-
477"
CLAIMS,0.52219074598678,"lutionary traits (shared traits among species due to common ancestry in the phylogeny)
478"
CLAIMS,0.5231350330500472,"from image by learning prototypes at each internal node in the phylogenetic tree. We show
479"
CLAIMS,0.5240793201133145,"through various visualizations of the prototypes in Figures 5, 6, and 7 to 16, that the learned
480"
CLAIMS,0.5250236071765817,"prototypes at the internal nodes can identify possible evolutionary traits from images. We
481"
CLAIMS,0.5259678942398489,"also evaluate the improved interpretability of our approach quantitatively in Table 3 by
482"
CLAIMS,0.5269121813031161,"computing part purity metric on Bird dataset.
483"
CLAIMS,0.5278564683663833,"Guidelines:
484"
CLAIMS,0.5288007554296507,"• The answer NA means that the abstract and introduction do not include the claims
485"
CLAIMS,0.5297450424929179,"made in the paper.
486"
CLAIMS,0.5306893295561851,"• The abstract and/or introduction should clearly state the claims made, including the
487"
CLAIMS,0.5316336166194523,"contributions made in the paper and important assumptions and limitations. A No or
488"
CLAIMS,0.5325779036827195,"NA answer to this question will not be perceived well by the reviewers.
489"
CLAIMS,0.5335221907459868,"• The claims made should match theoretical and experimental results, and reflect how
490"
CLAIMS,0.534466477809254,"much the results can be expected to generalize to other settings.
491"
CLAIMS,0.5354107648725213,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
492"
CLAIMS,0.5363550519357885,"are not attained by the paper.
493"
LIMITATIONS,0.5372993389990557,"2. Limitations
494"
LIMITATIONS,0.5382436260623229,"Question: Does the paper discuss the limitations of the work performed by the authors?
495"
LIMITATIONS,0.5391879131255902,"Answer: [Yes]
496"
LIMITATIONS,0.5401322001888574,"Justification: We discuss the limitations of our work in Supplementary Section I.
497"
LIMITATIONS,0.5410764872521246,"Guidelines:
498"
LIMITATIONS,0.5420207743153919,"• The answer NA means that the paper has no limitation while the answer No means that
499"
LIMITATIONS,0.5429650613786591,"the paper has limitations, but those are not discussed in the paper.
500"
LIMITATIONS,0.5439093484419264,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
501"
LIMITATIONS,0.5448536355051936,"• The paper should point out any strong assumptions and how robust the results are to
502"
LIMITATIONS,0.5457979225684608,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
503"
LIMITATIONS,0.546742209631728,"model well-specification, asymptotic approximations only holding locally). The authors
504"
LIMITATIONS,0.5476864966949953,"should reflect on how these assumptions might be violated in practice and what the
505"
LIMITATIONS,0.5486307837582625,"implications would be.
506"
LIMITATIONS,0.5495750708215298,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
507"
LIMITATIONS,0.550519357884797,"only tested on a few datasets or with a few runs. In general, empirical results often
508"
LIMITATIONS,0.5514636449480642,"depend on implicit assumptions, which should be articulated.
509"
LIMITATIONS,0.5524079320113314,"• The authors should reflect on the factors that influence the performance of the approach.
510"
LIMITATIONS,0.5533522190745986,"For example, a facial recognition algorithm may perform poorly when image resolution
511"
LIMITATIONS,0.554296506137866,"is low or images are taken in low lighting. Or a speech-to-text system might not be
512"
LIMITATIONS,0.5552407932011332,"used reliably to provide closed captions for online lectures because it fails to handle
513"
LIMITATIONS,0.5561850802644004,"technical jargon.
514"
LIMITATIONS,0.5571293673276676,"• The authors should discuss the computational efficiency of the proposed algorithms
515"
LIMITATIONS,0.5580736543909348,"and how they scale with dataset size.
516"
LIMITATIONS,0.559017941454202,"• If applicable, the authors should discuss possible limitations of their approach to
517"
LIMITATIONS,0.5599622285174694,"address problems of privacy and fairness.
518"
LIMITATIONS,0.5609065155807366,"• While the authors might fear that complete honesty about limitations might be used by
519"
LIMITATIONS,0.5618508026440038,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
520"
LIMITATIONS,0.562795089707271,"limitations that aren’t acknowledged in the paper. The authors should use their best
521"
LIMITATIONS,0.5637393767705382,"judgment and recognize that individual actions in favor of transparency play an impor-
522"
LIMITATIONS,0.5646836638338055,"tant role in developing norms that preserve the integrity of the community. Reviewers
523"
LIMITATIONS,0.5656279508970727,"will be specifically instructed to not penalize honesty concerning limitations.
524"
THEORY ASSUMPTIONS AND PROOFS,0.56657223796034,"3. Theory Assumptions and Proofs
525"
THEORY ASSUMPTIONS AND PROOFS,0.5675165250236072,"Question: For each theoretical result, does the paper provide the full set of assumptions and
526"
THEORY ASSUMPTIONS AND PROOFS,0.5684608120868744,"a complete (and correct) proof?
527"
THEORY ASSUMPTIONS AND PROOFS,0.5694050991501416,"Answer: [NA]
528"
THEORY ASSUMPTIONS AND PROOFS,0.5703493862134089,"Justification: The assumptions made in our work do not require explicit theoretical proofs.
529"
THEORY ASSUMPTIONS AND PROOFS,0.5712936732766761,"Instead, for the key loss terms that we introduce in this work, we provide ablation results in
530"
THEORY ASSUMPTIONS AND PROOFS,0.5722379603399433,"Supplementary Table 6 to show empirically the importance of each component.
531"
THEORY ASSUMPTIONS AND PROOFS,0.5731822474032106,"Guidelines:
532"
THEORY ASSUMPTIONS AND PROOFS,0.5741265344664778,"• The answer NA means that the paper does not include theoretical results.
533"
THEORY ASSUMPTIONS AND PROOFS,0.5750708215297451,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
534"
THEORY ASSUMPTIONS AND PROOFS,0.5760151085930123,"referenced.
535"
THEORY ASSUMPTIONS AND PROOFS,0.5769593956562795,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
536"
THEORY ASSUMPTIONS AND PROOFS,0.5779036827195467,"• The proofs can either appear in the main paper or the supplemental material, but if
537"
THEORY ASSUMPTIONS AND PROOFS,0.5788479697828139,"they appear in the supplemental material, the authors are encouraged to provide a short
538"
THEORY ASSUMPTIONS AND PROOFS,0.5797922568460812,"proof sketch to provide intuition.
539"
THEORY ASSUMPTIONS AND PROOFS,0.5807365439093485,"• Inversely, any informal proof provided in the core of the paper should be complemented
540"
THEORY ASSUMPTIONS AND PROOFS,0.5816808309726157,"by formal proofs provided in appendix or supplemental material.
541"
THEORY ASSUMPTIONS AND PROOFS,0.5826251180358829,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
542"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5835694050991501,"4. Experimental Result Reproducibility
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5845136921624173,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5854579792256847,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5864022662889519,"of the paper (regardless of whether the code and data are provided or not)?
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5873465533522191,"Answer: [Yes]
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5882908404154863,"Justification: We provide details of hyperparameters in Supplementary Section E. Further-
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5892351274787535,"more, we also provide the full code, data, and necessary data preprocessing pipelines to
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5901794145420207,"reproduce all the experiments.
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.591123701605288,"Guidelines:
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5920679886685553,"• The answer NA means that the paper does not include experiments.
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5930122757318225,"• If the paper includes experiments, a No answer to this question will not be perceived
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5939565627950897,"well by the reviewers: Making the paper reproducible is important, regardless of
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5949008498583569,"whether the code and data are provided or not.
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5958451369216242,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5967894239848914,"to make their results reproducible or verifiable.
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5977337110481586,"• Depending on the contribution, reproducibility can be accomplished in various ways.
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5986779981114259,"For example, if the contribution is a novel architecture, describing the architecture fully
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.5996222851746931,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6005665722379604,"be necessary to either make it possible for others to replicate the model with the same
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6015108593012276,"dataset, or provide access to the model. In general. releasing code and data is often
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6024551463644948,"one good way to accomplish this, but reproducibility can also be provided via detailed
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.603399433427762,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6043437204910292,"of a large language model), releasing of a model checkpoint, or other means that are
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6052880075542965,"appropriate to the research performed.
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6062322946175638,"• While NeurIPS does not require releasing code, the conference does require all submis-
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.607176581680831,"sions to provide some reasonable avenue for reproducibility, which may depend on the
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6081208687440982,"nature of the contribution. For example
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6090651558073654,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6100094428706326,"to reproduce that algorithm.
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6109537299339,"(b) If the contribution is primarily a new model architecture, the paper should describe
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6118980169971672,"the architecture clearly and fully.
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6128423040604344,"(c) If the contribution is a new model (e.g., a large language model), then there should
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6137865911237016,"either be a way to access this model for reproducing the results or a way to reproduce
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6147308781869688,"the model (e.g., with an open-source dataset or instructions for how to construct
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.615675165250236,"the dataset).
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6166194523135033,"(d) We recognize that reproducibility may be tricky in some cases, in which case
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6175637393767706,"authors are welcome to describe the particular way they provide for reproducibility.
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6185080264400378,"In the case of closed-source models, it may be that access to the model is limited in
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.619452313503305,"some way (e.g., to registered users), but it should be possible for other researchers
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6203966005665722,"to have some path to reproducing or verifying the results.
582"
OPEN ACCESS TO DATA AND CODE,0.6213408876298395,"5. Open access to data and code
583"
OPEN ACCESS TO DATA AND CODE,0.6222851746931067,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
584"
OPEN ACCESS TO DATA AND CODE,0.623229461756374,"tions to faithfully reproduce the main experimental results, as described in supplemental
585"
OPEN ACCESS TO DATA AND CODE,0.6241737488196412,"material?
586"
OPEN ACCESS TO DATA AND CODE,0.6251180358829084,"Answer: [Yes]
587"
OPEN ACCESS TO DATA AND CODE,0.6260623229461756,"Justification: We provide the full code, data, and necessary data preprocessing pipelines to
588"
OPEN ACCESS TO DATA AND CODE,0.6270066100094429,"reproduce the experiments.
589"
OPEN ACCESS TO DATA AND CODE,0.6279508970727101,"Guidelines:
590"
OPEN ACCESS TO DATA AND CODE,0.6288951841359773,"• The answer NA means that paper does not include experiments requiring code.
591"
OPEN ACCESS TO DATA AND CODE,0.6298394711992445,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
592"
OPEN ACCESS TO DATA AND CODE,0.6307837582625118,"public/guides/CodeSubmissionPolicy) for more details.
593"
OPEN ACCESS TO DATA AND CODE,0.6317280453257791,"• While we encourage the release of code and data, we understand that this might not be
594"
OPEN ACCESS TO DATA AND CODE,0.6326723323890463,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
595"
OPEN ACCESS TO DATA AND CODE,0.6336166194523135,"including code, unless this is central to the contribution (e.g., for a new open-source
596"
OPEN ACCESS TO DATA AND CODE,0.6345609065155807,"benchmark).
597"
OPEN ACCESS TO DATA AND CODE,0.6355051935788479,"• The instructions should contain the exact command and environment needed to run to
598"
OPEN ACCESS TO DATA AND CODE,0.6364494806421152,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
599"
OPEN ACCESS TO DATA AND CODE,0.6373937677053825,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
600"
OPEN ACCESS TO DATA AND CODE,0.6383380547686497,"• The authors should provide instructions on data access and preparation, including how
601"
OPEN ACCESS TO DATA AND CODE,0.6392823418319169,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
602"
OPEN ACCESS TO DATA AND CODE,0.6402266288951841,"• The authors should provide scripts to reproduce all experimental results for the new
603"
OPEN ACCESS TO DATA AND CODE,0.6411709159584513,"proposed method and baselines. If only a subset of experiments are reproducible, they
604"
OPEN ACCESS TO DATA AND CODE,0.6421152030217187,"should state which ones are omitted from the script and why.
605"
OPEN ACCESS TO DATA AND CODE,0.6430594900849859,"• At submission time, to preserve anonymity, the authors should release anonymized
606"
OPEN ACCESS TO DATA AND CODE,0.6440037771482531,"versions (if applicable).
607"
OPEN ACCESS TO DATA AND CODE,0.6449480642115203,"• Providing as much information as possible in supplemental material (appended to the
608"
OPEN ACCESS TO DATA AND CODE,0.6458923512747875,"paper) is recommended, but including URLs to data and code is permitted.
609"
OPEN ACCESS TO DATA AND CODE,0.6468366383380547,"6. Experimental Setting/Details
610"
OPEN ACCESS TO DATA AND CODE,0.647780925401322,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
611"
OPEN ACCESS TO DATA AND CODE,0.6487252124645893,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
612"
OPEN ACCESS TO DATA AND CODE,0.6496694995278565,"results?
613"
OPEN ACCESS TO DATA AND CODE,0.6506137865911237,"Answer: [Yes]
614"
OPEN ACCESS TO DATA AND CODE,0.6515580736543909,"Justification: The details of the data splits for each dataset used are provided in Table 8 and
615"
OPEN ACCESS TO DATA AND CODE,0.6525023607176582,"overview of phylogeny is given in Table 7. We also give details of key hyperparameters
616"
OPEN ACCESS TO DATA AND CODE,0.6534466477809254,"and the way they were chosen in Supplementary Section E. Full code also provided for
617"
OPEN ACCESS TO DATA AND CODE,0.6543909348441926,"reproducibility.
618"
OPEN ACCESS TO DATA AND CODE,0.6553352219074599,"Guidelines:
619"
OPEN ACCESS TO DATA AND CODE,0.6562795089707271,"• The answer NA means that the paper does not include experiments.
620"
OPEN ACCESS TO DATA AND CODE,0.6572237960339944,"• The experimental setting should be presented in the core of the paper to a level of detail
621"
OPEN ACCESS TO DATA AND CODE,0.6581680830972616,"that is necessary to appreciate the results and make sense of them.
622"
OPEN ACCESS TO DATA AND CODE,0.6591123701605288,"• The full details can be provided either with the code, in appendix, or as supplemental
623"
OPEN ACCESS TO DATA AND CODE,0.660056657223796,"material.
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6610009442870632,"7. Experiment Statistical Significance
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6619452313503305,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6628895184135978,"information about the statistical significance of the experiments?
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.663833805476865,"Answer: [Yes]
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6647780925401322,"Justification: We have done multiple runs of our model on Bird dataset with different
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6657223796033994,"random weight initialization, and report the mean and standard deviation of accuracy in
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6666666666666666,"Supplementary Section D
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.667610953729934,"Guidelines:
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6685552407932012,"• The answer NA means that the paper does not include experiments.
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6694995278564684,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6704438149197356,"dence intervals, or statistical significance tests, at least for the experiments that support
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6713881019830028,"the main claims of the paper.
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.67233238904627,"• The factors of variability that the error bars are capturing should be clearly stated (for
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6732766761095373,"example, train/test split, initialization, random drawing of some parameter, or overall
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6742209631728046,"run with given experimental conditions).
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6751652502360718,"• The method for calculating the error bars should be explained (closed form formula,
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.676109537299339,"call to a library function, bootstrap, etc.)
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6770538243626062,"• The assumptions made should be given (e.g., Normally distributed errors).
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6779981114258735,"• It should be clear whether the error bar is the standard deviation or the standard error
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6789423984891407,"of the mean.
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6798866855524079,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
645"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6808309726156752,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
646"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6817752596789424,"of Normality of errors is not verified.
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6827195467422096,"• For asymmetric distributions, the authors should be careful not to show in tables or
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6836638338054769,"figures symmetric error bars that would yield results that are out of range (e.g. negative
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6846081208687441,"error rates).
650"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6855524079320113,"• If error bars are reported in tables or plots, The authors should explain in the text how
651"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6864966949952785,"they were calculated and reference the corresponding figures or tables in the text.
652"
EXPERIMENTS COMPUTE RESOURCES,0.6874409820585458,"8. Experiments Compute Resources
653"
EXPERIMENTS COMPUTE RESOURCES,0.6883852691218131,"Question: For each experiment, does the paper provide sufficient information on the com-
654"
EXPERIMENTS COMPUTE RESOURCES,0.6893295561850803,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
655"
EXPERIMENTS COMPUTE RESOURCES,0.6902738432483475,"the experiments?
656"
EXPERIMENTS COMPUTE RESOURCES,0.6912181303116147,"Answer: [Yes]
657"
EXPERIMENTS COMPUTE RESOURCES,0.6921624173748819,"Justification: Details of computer resources used are provided in Supplementary Section E
658"
EXPERIMENTS COMPUTE RESOURCES,0.6931067044381491,"Guidelines:
659"
EXPERIMENTS COMPUTE RESOURCES,0.6940509915014165,"• The answer NA means that the paper does not include experiments.
660"
EXPERIMENTS COMPUTE RESOURCES,0.6949952785646837,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
661"
EXPERIMENTS COMPUTE RESOURCES,0.6959395656279509,"or cloud provider, including relevant memory and storage.
662"
EXPERIMENTS COMPUTE RESOURCES,0.6968838526912181,"• The paper should provide the amount of compute required for each of the individual
663"
EXPERIMENTS COMPUTE RESOURCES,0.6978281397544853,"experimental runs as well as estimate the total compute.
664"
EXPERIMENTS COMPUTE RESOURCES,0.6987724268177526,"• The paper should disclose whether the full research project required more compute
665"
EXPERIMENTS COMPUTE RESOURCES,0.6997167138810199,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
666"
EXPERIMENTS COMPUTE RESOURCES,0.7006610009442871,"didn’t make it into the paper).
667"
CODE OF ETHICS,0.7016052880075543,"9. Code Of Ethics
668"
CODE OF ETHICS,0.7025495750708215,"Question: Does the research conducted in the paper conform, in every respect, with the
669"
CODE OF ETHICS,0.7034938621340887,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
670"
CODE OF ETHICS,0.704438149197356,"Answer: [Yes]
671"
CODE OF ETHICS,0.7053824362606232,"Justification: The work abides by NeurIPS Code of Ethics in every aspect.
672"
CODE OF ETHICS,0.7063267233238905,"Guidelines:
673"
CODE OF ETHICS,0.7072710103871577,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
674"
CODE OF ETHICS,0.7082152974504249,"• If the authors answer No, they should explain the special circumstances that require a
675"
CODE OF ETHICS,0.7091595845136922,"deviation from the Code of Ethics.
676"
CODE OF ETHICS,0.7101038715769594,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
677"
CODE OF ETHICS,0.7110481586402266,"eration due to laws or regulations in their jurisdiction).
678"
BROADER IMPACTS,0.7119924457034938,"10. Broader Impacts
679"
BROADER IMPACTS,0.7129367327667611,"Question: Does the paper discuss both potential positive societal impacts and negative
680"
BROADER IMPACTS,0.7138810198300283,"societal impacts of the work performed?
681"
BROADER IMPACTS,0.7148253068932956,"Answer: [NA]
682"
BROADER IMPACTS,0.7157695939565628,"Justification: There is no negative societal impact of the work performed to the best of our
683"
BROADER IMPACTS,0.71671388101983,"knowledge.
684"
BROADER IMPACTS,0.7176581680830972,"Guidelines:
685"
BROADER IMPACTS,0.7186024551463644,"• The answer NA means that there is no societal impact of the work performed.
686"
BROADER IMPACTS,0.7195467422096318,"• If the authors answer NA or No, they should explain why their work has no societal
687"
BROADER IMPACTS,0.720491029272899,"impact or why the paper does not address societal impact.
688"
BROADER IMPACTS,0.7214353163361662,"• Examples of negative societal impacts include potential malicious or unintended uses
689"
BROADER IMPACTS,0.7223796033994334,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
690"
BROADER IMPACTS,0.7233238904627006,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
691"
BROADER IMPACTS,0.724268177525968,"groups), privacy considerations, and security considerations.
692"
BROADER IMPACTS,0.7252124645892352,"• The conference expects that many papers will be foundational research and not tied
693"
BROADER IMPACTS,0.7261567516525024,"to particular applications, let alone deployments. However, if there is a direct path to
694"
BROADER IMPACTS,0.7271010387157696,"any negative applications, the authors should point it out. For example, it is legitimate
695"
BROADER IMPACTS,0.7280453257790368,"to point out that an improvement in the quality of generative models could be used to
696"
BROADER IMPACTS,0.728989612842304,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
697"
BROADER IMPACTS,0.7299338999055713,"that a generic algorithm for optimizing neural networks could enable people to train
698"
BROADER IMPACTS,0.7308781869688386,"models that generate Deepfakes faster.
699"
BROADER IMPACTS,0.7318224740321058,"• The authors should consider possible harms that could arise when the technology is
700"
BROADER IMPACTS,0.732766761095373,"being used as intended and functioning correctly, harms that could arise when the
701"
BROADER IMPACTS,0.7337110481586402,"technology is being used as intended but gives incorrect results, and harms following
702"
BROADER IMPACTS,0.7346553352219075,"from (intentional or unintentional) misuse of the technology.
703"
BROADER IMPACTS,0.7355996222851747,"• If there are negative societal impacts, the authors could also discuss possible mitigation
704"
BROADER IMPACTS,0.7365439093484419,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
705"
BROADER IMPACTS,0.7374881964117092,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
706"
BROADER IMPACTS,0.7384324834749764,"feedback over time, improving the efficiency and accessibility of ML).
707"
SAFEGUARDS,0.7393767705382436,"11. Safeguards
708"
SAFEGUARDS,0.7403210576015109,"Question: Does the paper describe safeguards that have been put in place for responsible
709"
SAFEGUARDS,0.7412653446647781,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
710"
SAFEGUARDS,0.7422096317280453,"image generators, or scraped datasets)?
711"
SAFEGUARDS,0.7431539187913125,"Answer: [NA]
712"
SAFEGUARDS,0.7440982058545798,"Justification: The work poses no risk of misuse.
713"
SAFEGUARDS,0.7450424929178471,"Guidelines:
714"
SAFEGUARDS,0.7459867799811143,"• The answer NA means that the paper poses no such risks.
715"
SAFEGUARDS,0.7469310670443815,"• Released models that have a high risk for misuse or dual-use should be released with
716"
SAFEGUARDS,0.7478753541076487,"necessary safeguards to allow for controlled use of the model, for example by requiring
717"
SAFEGUARDS,0.7488196411709159,"that users adhere to usage guidelines or restrictions to access the model or implementing
718"
SAFEGUARDS,0.7497639282341831,"safety filters.
719"
SAFEGUARDS,0.7507082152974505,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
720"
SAFEGUARDS,0.7516525023607177,"should describe how they avoided releasing unsafe images.
721"
SAFEGUARDS,0.7525967894239849,"• We recognize that providing effective safeguards is challenging, and many papers do
722"
SAFEGUARDS,0.7535410764872521,"not require this, but we encourage authors to take this into account and make a best
723"
SAFEGUARDS,0.7544853635505193,"faith effort.
724"
LICENSES FOR EXISTING ASSETS,0.7554296506137866,"12. Licenses for existing assets
725"
LICENSES FOR EXISTING ASSETS,0.7563739376770539,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
726"
LICENSES FOR EXISTING ASSETS,0.7573182247403211,"the paper, properly credited and are the license and terms of use explicitly mentioned and
727"
LICENSES FOR EXISTING ASSETS,0.7582625118035883,"properly respected?
728"
LICENSES FOR EXISTING ASSETS,0.7592067988668555,"Answer: [Yes]
729"
LICENSES FOR EXISTING ASSETS,0.7601510859301227,"Justification: All datasets used have been cited along with their source wherever necessary.
730"
LICENSES FOR EXISTING ASSETS,0.76109537299339,"We also mention the license details for each dataset used in Supplementary Section E
731"
LICENSES FOR EXISTING ASSETS,0.7620396600566572,"Guidelines:
732"
LICENSES FOR EXISTING ASSETS,0.7629839471199245,"• The answer NA means that the paper does not use existing assets.
733"
LICENSES FOR EXISTING ASSETS,0.7639282341831917,"• The authors should cite the original paper that produced the code package or dataset.
734"
LICENSES FOR EXISTING ASSETS,0.7648725212464589,"• The authors should state which version of the asset is used and, if possible, include a
735"
LICENSES FOR EXISTING ASSETS,0.7658168083097262,"URL.
736"
LICENSES FOR EXISTING ASSETS,0.7667610953729934,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
737"
LICENSES FOR EXISTING ASSETS,0.7677053824362606,"• For scraped data from a particular source (e.g., website), the copyright and terms of
738"
LICENSES FOR EXISTING ASSETS,0.7686496694995278,"service of that source should be provided.
739"
LICENSES FOR EXISTING ASSETS,0.7695939565627951,"• If assets are released, the license, copyright information, and terms of use in the
740"
LICENSES FOR EXISTING ASSETS,0.7705382436260623,"package should be provided. For popular datasets, paperswithcode.com/datasets
741"
LICENSES FOR EXISTING ASSETS,0.7714825306893296,"has curated licenses for some datasets. Their licensing guide can help determine the
742"
LICENSES FOR EXISTING ASSETS,0.7724268177525968,"license of a dataset.
743"
LICENSES FOR EXISTING ASSETS,0.773371104815864,"• For existing datasets that are re-packaged, both the original license and the license of
744"
LICENSES FOR EXISTING ASSETS,0.7743153918791312,"the derived asset (if it has changed) should be provided.
745"
LICENSES FOR EXISTING ASSETS,0.7752596789423984,"• If this information is not available online, the authors are encouraged to reach out to
746"
LICENSES FOR EXISTING ASSETS,0.7762039660056658,"the asset’s creators.
747"
NEW ASSETS,0.777148253068933,"13. New Assets
748"
NEW ASSETS,0.7780925401322002,"Question: Are new assets introduced in the paper well documented and is the documentation
749"
NEW ASSETS,0.7790368271954674,"provided alongside the assets?
750"
NEW ASSETS,0.7799811142587346,"Answer: [Yes]
751"
NEW ASSETS,0.7809254013220018,"Justification: We provide the full code and the necessary documentation to reproduce the
752"
NEW ASSETS,0.7818696883852692,"results.
753"
NEW ASSETS,0.7828139754485364,"Guidelines:
754"
NEW ASSETS,0.7837582625118036,"• The answer NA means that the paper does not release new assets.
755"
NEW ASSETS,0.7847025495750708,"• Researchers should communicate the details of the dataset/code/model as part of their
756"
NEW ASSETS,0.785646836638338,"submissions via structured templates. This includes details about training, license,
757"
NEW ASSETS,0.7865911237016053,"limitations, etc.
758"
NEW ASSETS,0.7875354107648725,"• The paper should discuss whether and how consent was obtained from people whose
759"
NEW ASSETS,0.7884796978281398,"asset is used.
760"
NEW ASSETS,0.789423984891407,"• At submission time, remember to anonymize your assets (if applicable). You can either
761"
NEW ASSETS,0.7903682719546742,"create an anonymized URL or include an anonymized zip file.
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7913125590179415,"14. Crowdsourcing and Research with Human Subjects
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7922568460812087,"Question: For crowdsourcing experiments and research with human subjects, does the paper
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7932011331444759,"include the full text of instructions given to participants and screenshots, if applicable, as
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7941454202077431,"well as details about compensation (if any)?
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7950897072710104,"Answer: [NA]
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7960339943342776,"Justification: No crowdsourcing or human subject involved in this research.
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7969782813975449,"Guidelines:
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7979225684608121,"• The answer NA means that the paper does not involve crowdsourcing nor research with
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7988668555240793,"human subjects.
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.7998111425873465,"• Including this information in the supplemental material is fine, but if the main contribu-
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8007554296506137,"tion of the paper involves human subjects, then as much detail as possible should be
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8016997167138811,"included in the main paper.
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8026440037771483,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8035882908404155,"or other labor should be paid at least the minimum wage in the country of the data
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8045325779036827,"collector.
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8054768649669499,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8064211520302171,"Subjects
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8073654390934845,"Question: Does the paper describe potential risks incurred by study participants, whether
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8083097261567517,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8092540132200189,"approvals (or an equivalent approval/review based on the requirements of your country or
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8101983002832861,"institution) were obtained?
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8111425873465533,"Answer: [NA]
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8120868744098206,"Justification: No crowdsourcing or human subject involved in this research.
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8130311614730878,"Guidelines:
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8139754485363551,"• The answer NA means that the paper does not involve crowdsourcing nor research with
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8149197355996223,"human subjects.
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8158640226628895,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8168083097261567,"may be required for any human subjects research. If you obtained IRB approval, you
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.817752596789424,"should clearly state this in the paper.
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8186968838526912,"• We recognize that the procedures for this may vary significantly between institutions
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8196411709159585,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8205854579792257,"guidelines for their institution.
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8215297450424929,"• For initial submissions, do not include any information that would break anonymity (if
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8224740321057602,"applicable), such as the institution conducting the review.
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8234183191690274,"A
Ablation of Over-specificity Loss Trade-off Hyperparameter
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8243626062322946,"We have provided an ablation for the over-specificity loss trade-off hyperparameter (λovsp) in Table 4.
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8253068932955618,"We can observe that increasing the weight of over-specificity loss reduces the model’s classification
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.826251180358829,"performance, as the model struggles to find any commonality especially at internal nodes where the
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8271954674220963,"number of leaf descendant species are large in number and quite diverse. It is natural that species that
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8281397544853636,"are diverse and distantly related may share fewer characteristics with each other, in comparison to a
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8290840415486308,"set of species that diverged more recently from a common ancestor [14, 15]. Therefore, forcing the
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.830028328611898,"model to learn common traits with a strong Lovsp constraint can cause the model to perform bad in
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8309726156751652,"terms of accuracy.
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8319169027384324,Table 4: Ablation of over-specificity loss trade-off hyperparameter (λovsp). Done on Bird dataset.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8328611898016998,"λovsp
Part purity
Part purity with mask applied
% masked
% Accuracy"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.833805476864967,"w/o Lovsp
0.68 ± 0.22
0.75 ± 0.17
21.42%
58.32
0.05
0.72 ± 0.19
0.77 ± 0.16
16.53%
70.01
0.1
0.71 ± 0.18
0.74 ± 0.16
11.31%
70.97
0.5
0.71 ± 0.19
0.72 ± 0.18
4.2%
68.23
1.0
0.70 ± 0.19
0.70 ± 0.2
2.13%
62.68
2.0
0.69 ± 0.19
0.69 ± 0.19
0.55%
53.16"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8347497639282342,"B
Ablation of Number of Prototypes
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8356940509915014,"In Table 5 we vary the number of prototypes per child β for a node to see the impact on model’s
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8366383380547686,"performance. We note that while the accuracy increases marginally with increasing the number of
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8375826251180358,"prototypes per child (β) from 10 to 15, it also considerably increases the overall number of prototypes
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8385269121813032,"initialized. Therefore we continue to work with β = 10 for all of our experiments.
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8394711992445704,Table 5: Ablation of number of prototypes per child for a node (β). Done on Bird dataset.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8404154863078376,"Number of Prototypes (β)
% Accuracy"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8413597733711048,"10
70.01
15
70.92
20
67.93"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.842304060434372,"C
Ablation of Individual Losses
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8432483474976393,"In Table 6, we perform an ablation of the various loss terms used in our methodology. As it can be
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8441926345609065,"observed, removal of Lovsp and Ldisc degrades performance in terms of both semantic consistency
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8451369216241738,"(part purity) and accuracy. On the other hand, removal of self supervised contrastive loss LSS
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.846081208687441,"improves accuracy but at the cost of drastically decreasing the semantic consistency.
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8470254957507082,"Table 6: Ablation of individual losses. Done on Bird dataset.
Model
Part purity
Part purity with mask applied
% masked
% Accuracy"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8479697828139755,"HComP-Net
0.72 ± 0.19
0.77 ± 0.16
16.53%
70.01
HComP-Net w/o Lovsp
0.68 ± 0.22
0.75 ± 0.17
21.42%
58.32
HComP-Net w/o Ldisc
0.69 ± 0.19
0.72 ± 0.17
10.95%
65.99
HComP-Net w/o LSS
0.53 ± 0.18
0.57 ± 0.15
8.36%
81.62"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8489140698772427,"D
Consistency of Classification Performance Over Multiple Runs
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8498583569405099,"We trained the model using five distinct random weight initializations. The results showed that the
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8508026440037771,"model’s fine-grained accuracy averaged 70.63% with a standard deviation of 0.18%.
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8517469310670444,"E
Implementation Details
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8526912181303116,"We have included all the source code and dataset along with the comprehensive instructions to
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8536355051935789,"reproduce the results, in the supplementary material (.zip file).
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8545797922568461,"Model hyper-parameters: We build HComP-Net on top of a ConvNeXt-tiny architecture as the
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8555240793201133,"backbone feature extractor. We have modified the stride of the max pooling layers of later stages
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8564683663833805,"of the backbone from 2 to 1 similar to PIP-Net such that the backbone produces feature maps of
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8574126534466477,"increased height and width, in order to get more fine-grained prototype score maps. We implement
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8583569405099151,"and experiment our method on ConvNeXt-tiny backbones with 26 × 26 feature maps. The length
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8593012275731823,"of prototype vectors C is 768. The weights ϕ at every node n of HComP-Net are constrained to be
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8602455146364495,"non-negative by the use of ReLU activation function [56]. Further, the prototype activation nodes are
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8611898016997167,"connected with non-negative weights only to their respective child classes in W while their weights
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8621340887629839,"to other classes are made zero and non-trainable.
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8630783758262511,"Training details: All models were trained with images resized and appropriately padded to 224×224
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8640226628895185,"pixel resolution and augmented using TrivialAugment [57] for contrastive learning. The prototypes
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8649669499527857,"are pretrained with self-supervised learning similar to PIP-Net for 10 epochs, following which the
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8659112370160529,"model is trained with the entire set of loss functions for 60 epochs. We use a batch size of 256 for
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8668555240793201,"Bird dataset and 64 for Butterfly and Fish dataset. The masking module is trained in parallel and its
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8677998111425873,"training is continued for 15 additional epochs after the training of rest of the model is completed. The
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8687440982058546,"trade-off hyper-parameters for the loss functions are set to be λCE = 2; λA = 5; λT = 2; λovsp =
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8696883852691218,"0.05; λdisc = 0.1; λorth = 0.1; λmask = 2.0; λL1 = 0.5. λCE, λT and λA were borrowed from
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8706326723323891,"PIP-Net [18]. Ablations to arrive at suitable λovsp is provided in Table 4. λdisc and λorth were
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8715769593956563,"chosen empirically and found to work well on all three datasets. Experiment on unseen species was
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8725212464589235,"done by leaving out certain classes from the datasets, so that they are not considered during training.
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8734655335221907,"Dataset and Phylogeny Details: Dataset statistics and phylogeny statistics are provided in Table
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.874409820585458,"8 and Table 7 respectively. Bird dataset is created by choosing 190 species from CUB-200-2011
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8753541076487252,"2 [8] dataset, which were part of the phylogeny. Background from all images were filtered using
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8762983947119924,"the associated segmentation metadata [58]. For Butterfly dataset we considered each subspecies
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8772426817752597,"as an individual class and considered only the subspecies of genus Heliconius from the Heliconius
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8781869688385269,"Collection (Cambridge Butterfly)3 [16]. There is substantial variation among subspecies of Heliconius
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8791312559017942,"species. Furthermore, we balanced the dataset by filtering out the subspecies which did not have
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8800755429650614,"20 or more images. We also sampled a subset of 100 images from each subspecies that had more
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8810198300283286,"than 100 images. For Fish 4 dataset, we followed the exact same preprocessing steps as outlined in
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8819641170915958,"PhyloNN [9].
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.882908404154863,"Compute Resources: The models for Bird dataset were trained on two NVIDIA A100 GPUs with
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8838526912181303,"80GB of RAM each. Butterfly and Fish models were trained on single A100 GPU. As a rough
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8847969782813976,"estimate the execution time for training model on Bird dataset is around 2.5 hours. For Butterfly and
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8857412653446648,"Fish datasets, the training completes under 1 hour. We used a single A100 GPU during inference
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.886685552407932,"stage for all other analysis.
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8876298394711992,Table 7: High level statistics of the phylogenies used for different datasets.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8885741265344664,"Phylogeny
# Internal nodes
Max-depth
Min-depth"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8895184135977338,"Bird
184
25
3
Butterfly
13
5
2
Fish
20
11
2"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.890462700661001,"2License: CC BY
3Note that this dataset is a compilation of images from 25 Zenodo records by the Butterfly Genetics Group at
Cambridge University, licensed under Creative Commons Attribution 4.0 International ([27, 28, 29, 30, 31, 32,
33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]).
4License: CC BY-NC"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8914069877242682,Table 8: Dataset statistics (# train and validation images).
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8923512747875354,"Dataset
# Classes
Train set
Validation set"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8932955618508026,"Bird
190
5695
5512
Butterfly
30
1418
358
Fish
38
4140
1294"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8942398489140698,Table 9: Part purity with post-hoc thresholding approach. Done on Bird dataset.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8951841359773371,"Threshold
Part purity with mask applied
% masked"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8961284230406044,"0.2
0.74 ± 0.28
12.28%
0.3
0.75 ± 0.27
13.47%
0.4
0.76 ± 0.26
14.97%
0.5
0.77 ± 0.15
16.66%
0.6
0.77 ± 0.26
17.43%"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8970727101038716,"F
Post-hoc Thresholding to Identify Over-specific Prototypes
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8980169971671388,"An alternative approach to learning masking module is to calculate the over-specificity score for each
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.898961284230406,"prototype on the test set after training the model. We calculate the over-specificity scores for the
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8999055712936733,"prototypes of a trained model as follows,
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9008498583569405,"Oi = − Di
Y d=1"
TOPK,0.9017941454202077,"1
topk"
TOPK,0.902738432483475,"topk
X"
TOPK,0.9036827195467422,"i=1
(gi)
(9)"
TOPK,0.9046270066100094,"For a given prototype, we choose the topk images with the highest prototype scores from each
861"
TOPK,0.9055712936732767,"leaf descendant. After taking mean of the topk prototype score, we multiply the values from each
862"
TOPK,0.9065155807365439,"descendant to arrive at the over-specificity score for the particular prototype. Subsequently we choose
863"
TOPK,0.9074598677998111,"a threshold to determine which prototypes are over-specific. We provide the results of post-hoc
864"
TOPK,0.9084041548630784,"thresholding approach that can also be used to identify overspecific prototypes in Table 9. While we
865"
TOPK,0.9093484419263456,"can note that this approach can also be effective, validating the threshold particularly in scenarious
866"
TOPK,0.9102927289896129,"where there is no part annotations available (such as part location annotation of CUB-200-2011) can
867"
TOPK,0.9112370160528801,"be an ardous task. In such cases directly identifying over-specific prototypes as part of the training
868"
TOPK,0.9121813031161473,"through masking module can be the more feasible option.
869"
TOPK,0.9131255901794145,"G
Additional Visualizations of the Hierarchical Prototypes Discovered by
870"
TOPK,0.9140698772426817,"HComP-Net
871"
TOPK,0.9150141643059491,"We provide more visualizations of the hierarchical prototypes discovered by HComP-Net for Butterfly
872"
TOPK,0.9159584513692163,"(Figures 7 and 8) and Fish (Figure 9) datasets in this section. For ease of visualization, in each figure
873"
TOPK,0.9169027384324835,"we visualize the prototypes learned over a small sub-tree from the phylogeny. The prototypes at the
874"
TOPK,0.9178470254957507,"lowest level capture traits that are species-specific whereas the prototypes at internal nodes capture the
875"
TOPK,0.9187913125590179,"commonality between its descendant species. For Fish dataset, we have provided textual descriptions
876"
TOPK,0.9197355996222851,"purely based on human interpretation for the traits that are captured by prototypes at different levels.
877"
TOPK,0.9206798866855525,"For Butterfly dataset, since the prototypes are capturing different wing patterns, assigning textual
878"
TOPK,0.9216241737488197,"description for them is not straightforward. Therefore, we refrain from providing any text description
879"
TOPK,0.9225684608120869,"for the highlighted regions of the learned prototypes and leave it to the reader’s interpretation.
880"
TOPK,0.9235127478753541,"H
Additional Top-K Visualizations of HComP-Net Prototypes
881"
TOPK,0.9244570349386213,"We provide additional top-K visualizations of the prototypes from Butterfly (Figures 10 to 13) and
882"
TOPK,0.9254013220018886,"Fish (Figures 14 to 16) datasets, where every row corresponds to a descendant species and the
883"
TOPK,0.9263456090651558,"columns corresponds to the top-K images from the species with the largest prototype activation scores.
884"
TOPK,0.927289896128423,"A requirement of a semantically meaningful prototype is that it should consistently highlight the
885"
TOPK,0.9282341831916903,"same part of the organisms in various images, provided that the part is visible. We can see in the
886"
TOPK,0.9291784702549575,"figures that the prototypes learned by HComP-Net consistently highlight the same part across all
887"
TOPK,0.9301227573182247,"top-K images of a species, and across all descendant species. We additionally show that HComP-Net
888"
TOPK,0.931067044381492,"can find common traits at internal nodes with varying number of descendant species, including 4
889"
TOPK,0.9320113314447592,"species (Figure 10), 5 species (Figures 11 and 12), and 10 species (Figure 13) of butterflies, and
890"
TOPK,0.9329556185080264,"5 species (Figure 14), 8 species (Figure 15) and 18 species (Figure 16) for fish. We also provide
891"
TOPK,0.9338999055712937,"several top-k visualizations of prototypes learned for bird species in Figures 17 to 25. This shows the
892"
TOPK,0.9348441926345609,"ability of HComP-Net to discover common prototypes at internal nodes of the phylogenetic tree that
893"
TOPK,0.9357884796978282,"consistently highlight the same regions in the descendant species images even when the number of
894"
TOPK,0.9367327667610954,"descendants is large.
895"
TOPK,0.9376770538243626,"I
Limitations of Our Work
896"
TOPK,0.9386213408876298,"A fundamental challenge of every prototype-based interpretability method (including ours) is the
897"
TOPK,0.939565627950897,"difficulty in associating a semantic interpretation to the underlying visual concept of a prototype.
898"
TOPK,0.9405099150141643,"While some prototypes can be interpreted easily based on visual inspection of prototype activation
899"
TOPK,0.9414542020774316,"maps, other prototypes are harder to interpret and require additional domain expertise of biologists.
900"
TOPK,0.9423984891406988,"Also, while we have considered large phylogenies as that of the 190 species from CUB dataset, it may
901"
TOPK,0.943342776203966,"still not be representative of all bird species. This limited scope may cause our method to identify
902"
TOPK,0.9442870632672332,"apparent homologous evolutionary traits that could differ with the inclusion of more species into the
903"
TOPK,0.9452313503305004,"phylogeny. Therefore, our method can be seen as a system that generates potential hypotheses about
904"
TOPK,0.9461756373937678,"evolutionary traits discovered in the form of hierarchical prototypes.
905"
TOPK,0.947119924457035,Heliconius sara sara
TOPK,0.9480642115203022,Heliconius eleuchia primularis
TOPK,0.9490084985835694,Heliconius eleuchia eleuchia
TOPK,0.9499527856468366,Heliconius erato amalfreda
TOPK,0.9508970727101038,Heliconius erato lativitta
TOPK,0.9518413597733711,Heliconius erato notabilis
TOPK,0.9527856468366384,Heliconius telesiphe sotericus
TOPK,0.9537299338999056,Traits common
TOPK,0.9546742209631728,to species
TOPK,0.95561850802644,Traits common
TOPK,0.9565627950897073,to species
TOPK,0.9575070821529745,Traits specific
TOPK,0.9584513692162417,to species
TOPK,0.959395656279509,"Figure 7: Visualizing the hierarchy of prototypes discovered by HComP-Net over three levels in
the phylogeny of seven species from Butterfly dataset. For each prototype we visualize one image
from each of its leaf descendant. Therefore, for prototypes at species level ( rightmost column) we
show only one image whereas for prototypes at internal nodes we show multiple images (equal to the
number of leaf descendants). For each image, we show the zoomed in view of the original image as
well as the heatmap overlayed image in the region of the learned prototype. The prototypes appear to
be capturing different wing patterns of the butterflies."
TOPK,0.9603399433427762,Heliconius timareta linaresi
TOPK,0.9612842304060434,Heliconius cydno alithea
TOPK,0.9622285174693107,Heliconius cydno chioneus
TOPK,0.9631728045325779,Heliconius cydno cydnides
TOPK,0.9641170915958451,Heliconius melpomene plesseni
TOPK,0.9650613786591123,Heliconius melpomene melpomene
TOPK,0.9660056657223796,Heliconius melpomene rosina
TOPK,0.9669499527856469,Traits common
TOPK,0.9678942398489141,to species
TOPK,0.9688385269121813,Traits common
TOPK,0.9697828139754485,to species
TOPK,0.9707271010387157,Traits specific
TOPK,0.9716713881019831,to species
TOPK,0.9726156751652503,"Figure 8: Visualizing the hierarchy of prototypes discovered by HComP-Net over three levels in the
phylogeny of seven species from Butterfly dataset."
TOPK,0.9735599622285175,"Esox 
americanus"
TOPK,0.9745042492917847,Gambusia
TOPK,0.9754485363550519,affinis
TOPK,0.9763928234183191,Lepomis
TOPK,0.9773371104815864,gulosus
TOPK,0.9782813975448537,Trait: Dorsal fin morphology*
TOPK,0.9792256846081209,Trait: Head morphology*
TOPK,0.9801699716713881,Trait: Dorsal fin upper back
TOPK,0.9811142587346553,shape*
TOPK,0.9820585457979226,Trait: Snout shape*
TOPK,0.9830028328611898,"Traits common to species
Traits specific to species"
TOPK,0.983947119924457,"Figure 9: Visualizing the hierarchy of prototypes discovered by HComP-Net for a sub-trees with
three species from Fish dataset. *Note that the textual descriptions of the hypothesized traits shown
for every prototype are based on human interpretation."
TOPK,0.9848914069877243,"Figure 10: Top-K visualization of a prototype finding commonality between four species of butterfly
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.9858356940509915,"Figure 11: Top-K visualization of a prototype finding commonality between nine species of butterfly
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.9867799811142587,"Figure 12: Top-K visualization of a prototype finding commonality between twelve species of
butterfly sharing a common ancestor. Each row represents the top 3 images from the respective
species. For each image we show the zoomed in view of the original image as well as the heatmap
overlayed image."
TOPK,0.987724268177526,"Figure 13: Top-K visualization of a prototype finding commonality between four species of butterfly
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.9886685552407932,"Figure 14: Top-K visualization of a prototype finding commonality between five species of fish
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.9896128423040604,"Figure 15: Top-K visualization of a prototype finding commonality between eight species of fish
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.9905571293673276,"Figure 16: Top-K visualization of a prototype finding commonality between eighteen species of fish
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.9915014164305949,"Figure 17: Top-K visualization of a prototype finding commonality between seven species of birds
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.9924457034938622,"Figure 18: Top-K visualization of a prototype finding commonality between eight species of birds
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.9933899905571294,"Figure 19: Top-K visualization of a prototype finding commonality between nine species of birds
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.9943342776203966,"Figure 20: Top-K visualization of a prototype finding commonality between thirteen species of birds
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.9952785646836638,"Figure 21: Top-K visualization of a prototype finding commonality between five species of birds
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.996222851746931,"Figure 22: Top-K visualization of a prototype finding commonality between five species of birds
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.9971671388101983,"Figure 23: Top-K visualization of a prototype finding commonality between sixteen species of birds
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.9981114258734656,"Figure 24: Top-K visualization of a prototype finding commonality between four species of birds
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
TOPK,0.9990557129367328,"Figure 25: Top-K visualization of a prototype finding commonality between three species of birds
sharing a common ancestor. Each row represents the top 3 images from the respective species. For
each image we show the zoomed in view of the original image as well as the heatmap overlayed
image."
