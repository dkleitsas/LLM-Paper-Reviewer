Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0025575447570332483,"As Deep Neural Networks (DNNs) are trained to perform tasks of increasing
1"
ABSTRACT,0.005115089514066497,"complexity, their size grows, presenting several challenges when it comes to
2"
ABSTRACT,0.0076726342710997444,"deploying them on edge devices that have limited resources. To cope with this,
3"
ABSTRACT,0.010230179028132993,"a recently proposed approach hinges on substituting the classical Multiply-and-
4"
ABSTRACT,0.01278772378516624,"Accumulate (MAC) neurons in the hidden layers of a DNN with other neurons
5"
ABSTRACT,0.015345268542199489,"called Multiply-And-Max/min (MAM) whose selective behaviour helps identifying
6"
ABSTRACT,0.017902813299232736,"important interconnections and allows extremely aggressive pruning. Hybrid
7"
ABSTRACT,0.020460358056265986,"structures with MAC and MAM neurons promise a reduction in the number of
8"
ABSTRACT,0.023017902813299233,"interconnections that outperforms what can be achieved with MAC-only structures
9"
ABSTRACT,0.02557544757033248,"by more than an order of magnitude. However, by now, the lack of any theoretical
10"
ABSTRACT,0.028132992327365727,"demonstration of their ability to work as universal approximators limits their
11"
ABSTRACT,0.030690537084398978,"diffusion. Here, we take a first step in the theoretical characterization of the
12"
ABSTRACT,0.03324808184143223,"capabilities of MAM&MAC networks. In details, we prove two theorems that
13"
ABSTRACT,0.03580562659846547,"confirm that they are universal approximators providing that two hidden MAM
14"
ABSTRACT,0.03836317135549872,"layers are followed either by a MAC neuron without nonlinearity or by a normalized
15"
ABSTRACT,0.04092071611253197,"variant of the same. Approximation quality is measured either in terms of the first-
16"
ABSTRACT,0.043478260869565216,"order Lp Sobolev norm or by the L∞norm.
17"
INTRODUCTION,0.04603580562659847,"1
Introduction
18"
INTRODUCTION,0.04859335038363171,"Deep Neural Networks (DNNs) solve complex tasks leveraging a massive number of trainable
19"
INTRODUCTION,0.05115089514066496,"parameters. Yet, thanks to the recent increasing interest in mobile Artificial Intelligence, there
20"
INTRODUCTION,0.05370843989769821,"has been a growing emphasis on designing lightweight structures able to run on devices with
21"
INTRODUCTION,0.056265984654731455,"constrained resources. This can be obtained by removing parameters that do not appreciably influence
22"
INTRODUCTION,0.058823529411764705,"performance by means of one of the many pruning techniques that have been proposed. Some
23"
INTRODUCTION,0.061381074168797956,"approaches entail removing, in a single shot, individual interconnections or entire neurons once the
24"
INTRODUCTION,0.0639386189258312,"DNN has been trained, while others methods are applied iteratively, and require multiple rounds of
25"
INTRODUCTION,0.06649616368286446,"training. These techniques eliminate interconnections but do not alter the underlying Multiply-and-
26"
INTRODUCTION,0.06905370843989769,"ACcumulate (MAC) paradigm that governs the neuron’s inner functioning.
27"
INTRODUCTION,0.07161125319693094,"In [1, 2], the authors address the challenge of designing neural networks that can have a smaller
28"
INTRODUCTION,0.0741687979539642,"memory footprint presenting a novel neuron model based on the Multiply-And-Max/min (MAM)
29"
INTRODUCTION,0.07672634271099744,"paradigm that can be substituted to classical MAC neurons in the hidden layers of a DNN to
30"
INTRODUCTION,0.0792838874680307,"allow a more aggressive pruning of interconnections, while substantially preserving the network
31"
INTRODUCTION,0.08184143222506395,"performance. In a standard MAC-based neuron, inputs are modulated independently of each other
32"
INTRODUCTION,0.08439897698209718,"through multiplication with their respective weights, and the resulting products are then summed into
33"
INTRODUCTION,0.08695652173913043,"a single quantity. As MAC neurons, MAM neurons multiply each input by a weight but then only the
34"
INTRODUCTION,0.08951406649616368,"maximum and the minimum quantity of the products are summed together.
35"
INTRODUCTION,0.09207161125319693,"In formulas, if v1,v2,... are the inputs after being multiplied by their respective weights, the output
36"
INTRODUCTION,0.09462915601023018,"u of a MAM neuron is
37"
INTRODUCTION,0.09718670076726342,"u = [max
j
vj + min
j
vj + b] +
(1)"
INTRODUCTION,0.09974424552429667,"where b is the bias and [⋅]+ = max{0,⋅} represents the nowadays common ReLU nonlinearity.
38"
INTRODUCTION,0.10230179028132992,"It is shown empirically that, starting from an architecture originally designed using MAC neurons, one
39"
INTRODUCTION,0.10485933503836317,"may substitute them with MAM neurons in several hidden layers and use a proper training strategy to
40"
INTRODUCTION,0.10741687979539642,"achieve the same performances as the corresponding MAC-only network. Yet, in the resulting hybrid
41"
INTRODUCTION,0.10997442455242967,"network, one may leverage the extremely selective behaviour of min and max operations to reduce
42"
INTRODUCTION,0.11253196930946291,"very aggressively the number of weights. MAM neurons can be pruned with almost every technique
43"
INTRODUCTION,0.11508951406649616,"proposed in the literature with little to no modifications. As a motivating example, Table 1 reports
44"
INTRODUCTION,0.11764705882352941,"some of the results described in [1] showing cases in which, once the quality level is set (in this case
45"
INTRODUCTION,0.12020460358056266,"to 3% less accuracy than the original non-pruned network), MAM neuron substitution, retraining
46"
INTRODUCTION,0.12276214833759591,"and pruning reduce the number of weights 1 to 2 orders of magnitude more than what is obtained
47"
INTRODUCTION,0.12531969309462915,"by pruning the original MAC-only network. Moreover, these neurons can also be pruned iteratively
48"
INTRODUCTION,0.1278772378516624,"requiring less training iterations to guarantee a given accuracy compared to standard MAC neurons.
49"
INTRODUCTION,0.13043478260869565,"Table 1: Approximate remaining interconnections in the hidden fully-connected layers with one-shot
global magnitude pruning built either with MAC or MAM neurons."
INTRODUCTION,0.1329923273657289,"AlexNet + Cifar-10
AlexNet + Cifar-100
VGG-16 + ImageNet"
INTRODUCTION,0.13554987212276215,"Top-1 accuracy (3% lower
than non-pruned network)
87.69%
63.89%
61.03%"
INTRODUCTION,0.13810741687979539,"Surviving interconnections
(MAC)
1.01%
25.01%
10.82%"
INTRODUCTION,0.14066496163682865,"Surviving interconnections
(MAM)
0.06%
0.26%
0.04%"
INTRODUCTION,0.1432225063938619,"Though the equivalence between MAC-only and MAM&MAC networks has been demonstrated in
50"
INTRODUCTION,0.14578005115089515,"practice, a change in the model of some neurons opens the problem of the abstract capability of such
51"
INTRODUCTION,0.1483375959079284,"hybrid architectures. This contribution is a step forward in clarifying that, despite the locally different
52"
INTRODUCTION,0.15089514066496162,"input-output relationships, also hybrid MAM&MAC networks enjoy some universal approximation
53"
INTRODUCTION,0.1534526854219949,"capabilities analogous to those of the MAC-only networks.
54"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.15601023017902813,"1.1
Brief background on universal approximation properties
55"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.1585677749360614,"The development of models with universal approximation properties has been a significant break-
56"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.16112531969309463,"through in many fields of science and engineering. In 1989 [3] proved that a network with a single
57"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.1636828644501279,"hidden layer could approximate any continuous function, given enough hidden neurons. Some years
58"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.16624040920716113,"later, [4] and [5] showed that also fuzzy systems could approximate any continuous function to
59"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.16879795396419436,"arbitrary accuracy. These works were later extended to multiple inputs and outputs, demonstrating
60"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.17135549872122763,"the universal approximation properties of fuzzy systems more broadly ([6, 7]). In the following
61"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.17391304347826086,"years, a large number of researchers have studied the universal approximation properties of neural
62"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.17647058823529413,"networks with MAC neurons in the case of bounded depth and arbitrary width ([8, 9]), bounded width
63"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.17902813299232737,"and arbitrary depth ([10, 11, 12]) and bounded width and depth ([13, 14]). In the recent work [15],
64"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.1815856777493606,"authors obtained the optimal minimum width bound of a neural network with arbitrary depth to retain
65"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.18414322250639387,"universal approximation capabilities.
66"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.1867007672634271,"The research in this field is still very active and aims at proving the universal approximation capa-
67"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.18925831202046037,"bilities of networks with different architectural or computational paradigm choices, such as deep
68"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.1918158567774936,"convolutional neural networks [16], dropout neural networks [17], networks representing probability
69"
BRIEF BACKGROUND ON UNIVERSAL APPROXIMATION PROPERTIES,0.19437340153452684,"distributions [18] and spiking neural networks [19].
70"
MATHEMATICAL MODEL,0.1969309462915601,"2
Mathematical model
71"
MATHEMATICAL MODEL,0.19948849104859334,"We indicate with L(⋅) a fully connected layer in which all neurons are based on the MAM paradigm
72"
MATHEMATICAL MODEL,0.2020460358056266,"(1). We consider networks with N inputs collected in the vector x = (x1,...,xN), two MAM hidden
73"
MATHEMATICAL MODEL,0.20460358056265984,"layers producing a vector z(x) = (z1(x),z2(x),...) = L′′ (L′ (x)) and a single output Z (x) ∈R
74"
MATHEMATICAL MODEL,0.2071611253196931,"produced by a final layer that computes either the normalized linear combination
75"
MATHEMATICAL MODEL,0.20971867007672634,Z (x) = ∑k ckzk(x)
MATHEMATICAL MODEL,0.21227621483375958,"∑k zk(x)
(2)"
MATHEMATICAL MODEL,0.21483375959079284,"or the linear combination
76"
MATHEMATICAL MODEL,0.21739130434782608,"Z (x) = ∑
k
ckzk(x)
(3)"
MATHEMATICAL MODEL,0.21994884910485935,"We normalize the input domain by assuming xi ∈X = [0,1] for i = 1,...,N and indicate with Z∗
77"
MATHEMATICAL MODEL,0.22250639386189258,"the family of functions in (2) while with Z the analogous family of functions in (3). Smoothness
78"
MATHEMATICAL MODEL,0.22506393861892582,"conditions on our target functions f ∶XN ↦R is formalized by assuming that they belong to
79"
MATHEMATICAL MODEL,0.22762148337595908,"Cd (XN), i.e., that their d-th order derivatives are continuous. Distances between functions are
80"
MATHEMATICAL MODEL,0.23017902813299232,"measured by means of the norms defined as
81"
MATHEMATICAL MODEL,0.23273657289002558,"∥ϕ∥k,p =
⎡⎢⎢⎢⎣
∫XN ∣ϕ(x)∣p dx + k
N
∑
j=1∫XN ∣∂ϕ"
MATHEMATICAL MODEL,0.23529411764705882,"xj
(x)∣"
MATHEMATICAL MODEL,0.23785166240409208,"p
dx
⎤⎥⎥⎥⎦ 1/p"
MATHEMATICAL MODEL,0.24040920716112532,"with k = {0,1} and p ≥1.
82"
MAIN RESULTS,0.24296675191815856,"3
Main results
83"
MAIN RESULTS,0.24552429667519182,"Within the above framework, we prove two theorems that describe the universal approximation
84"
MAIN RESULTS,0.24808184143222506,"properties of DNNs using MAM neurons in the hidden layers.
85"
MAIN RESULTS,0.2506393861892583,"Theorem 1. For any function f ∈C0 (XN) and any prescribed level of tolerance ϵ > 0, there is a
86"
MAIN RESULTS,0.2531969309462916,"Z ∈Z∗such that ∥f −Z∥0,∞≤ϵ.
87"
MAIN RESULTS,0.2557544757033248,"Theorem 2. For any function f ∈C2 (XN), any prescribed level of tolerance ϵ > 0 and finite p ≥1,
88"
MAIN RESULTS,0.25831202046035806,"there is a Z ∈Z such that ∥f −Z∥1,p ≤ϵ.
89"
MAIN RESULTS,0.2608695652173913,"The proofs of both theorems are reported in Section 6 and are constructive. In particular, subnetworks
90"
MAIN RESULTS,0.26342710997442453,"in the cascade z(x) = L′′ (L′ (x)) are identified and programmed to make each zk(x) a weakly
91"
MAIN RESULTS,0.2659846547314578,"unimodal piecewise-linear function of the inputs, whose maximum is 1 and is reached in a hyper-
92"
MAIN RESULTS,0.26854219948849106,"rectangular subset of the domain, while the function vanishes for points far from the center of
93"
MAIN RESULTS,0.2710997442455243,"that hyper-rectangle. The shapes and positions of these functions can then be designed along with
94"
MAIN RESULTS,0.27365728900255754,"the values of the weights ck so that their combination by means of either (2) or (3) is capable of
95"
MAIN RESULTS,0.27621483375959077,"approximating the target function arbitrarily well as measured either by ∥⋅∥1,p or ∥⋅∥0,∞.
96"
EXAMPLES,0.27877237851662406,"4
Examples
97"
EXAMPLES,0.2813299232736573,"Figure 1 proposes a visual representation of the constructions behind Theorem 1 and Theorem 2 for
98"
EXAMPLES,0.28388746803069054,"N = 2. From left to right, we report the target function f ∶X2 →R
99"
EXAMPLES,0.2864450127877238,"f(x1,x2) = (4x1 −2)(4x2 −2)(4x1 + 1 2)"
EXAMPLES,0.289002557544757,"1 + (4x1 −2)2 + (4x2 −2)2
+ 3
(4)"
EXAMPLES,0.2915601023017903,"and its approximation Z ∈Z∗implied by the proof of Theorem 1 and its approximation X ∈Z
100"
EXAMPLES,0.29411764705882354,"implied by the proof of Theorem 2. In both cases the parameter n used in Section 6 is set to n = 7.
101 x1
x1"
EXAMPLES,0.2966751918158568,"f(x1, x2)
Z(x1, x2)
X(x1, x2) x1"
EXAMPLES,0.29923273657289,"x2
x2
x2"
EXAMPLES,0.30179028132992325,"Figure 1: Three dimensional plot of a target function f(x1,x2) and of its two approximations
Z(x1,x2) ∈Z∗implied by Theorem 1 and X(x1,x2) ∈Z by Theorem 2."
LIMITATIONS,0.30434782608695654,"5
Limitations
102"
LIMITATIONS,0.3069053708439898,"Theorem 1 and Theorem 2 rely on networks in which constraints are put neither on the layer width
103"
LIMITATIONS,0.309462915601023,"nor on the total number of neurons. Hence, despite proving universal approximation capabilities, they
104"
LIMITATIONS,0.31202046035805625,"do not imply efficient approximation. Yet, such theoretical limitation is never strongly experienced
105"
LIMITATIONS,0.3145780051150895,"in practice, since MAM networks are able to guarantee acceptable performance in real use cases.
106"
LIMITATIONS,0.3171355498721228,"Nevertheless, a deeper look at universal approximation aimed at meeting efficiency will be the focus
107"
LIMITATIONS,0.319693094629156,"of future analysis.
108"
NETWORK CONSTRUCTION AND PROOFS OF THEOREMS,0.32225063938618925,"6
Network construction and proofs of Theorems
109"
NETWORK CONSTRUCTION,0.3248081841432225,"6.1
Network construction
110"
NETWORK CONSTRUCTION,0.3273657289002558,"The aim of this subsection is to show that our network can be programmed to make the outputs of the
111"
NETWORK CONSTRUCTION,0.329923273657289,"second hidden layer specific weakly unimodal piecewise-linear functions zk (x) of the inputs.
112"
NETWORK CONSTRUCTION,0.33248081841432225,"Lemma 1. Let z be any of the outputs of the second hidden layer. For N > 1 and any choice of
113"
NETWORK CONSTRUCTION,0.3350383631713555,"the quantities ω1,...,ωN ∈[0,1], l1,...,lN ≥0, δL
1,...,δL
N ≥0, and δR
1,...,δR
N ≥0, the two MAM
114"
NETWORK CONSTRUCTION,0.3375959079283887,"hidden layers can be programmed to yield
115"
NETWORK CONSTRUCTION,0.340153452685422,"z (x) = [1 −∆(x)]+
(5)"
NETWORK CONSTRUCTION,0.34271099744245526,"where
116"
NETWORK CONSTRUCTION,0.3452685421994885,"∆(x) =
max
i∈{1,..,N}"
NETWORK CONSTRUCTION,0.34782608695652173,⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
NETWORK CONSTRUCTION,0.35038363171355497,"0,
∣xi −ωi∣−li"
NETWORK CONSTRUCTION,0.35294117647058826,"{δL
i
if xi < ωi
δR
i
if xi ≥ωi"
NETWORK CONSTRUCTION,0.3554987212276215,⎫⎪⎪⎪⎪⎪⎪⎪⎬⎪⎪⎪⎪⎪⎪⎪⎭ (6)
NETWORK CONSTRUCTION,0.35805626598465473,"Proof of Lemma 1. We
assume
that
neurons
in
the
first
hidden
layer
come
in
pairs
117"
NETWORK CONSTRUCTION,0.36061381074168797,"(yL
1,yR
1,yL
2,yR
2,...) = L′(x) and the output of a pair depends on only one of the inputs.
118"
NETWORK CONSTRUCTION,0.3631713554987212,"Without any loss of generality, we assume that yL
i and yR
i depend only on xi for i = 1,...,N while all
119"
NETWORK CONSTRUCTION,0.3657289002557545,"the other N −1 input weights are set to 0. The other outputs of the first hidden layer are involved in
120"
NETWORK CONSTRUCTION,0.36828644501278773,"the computation of the outputs of the second hidden layer further to the z we are considering.
121"
NETWORK CONSTRUCTION,0.37084398976982097,"For yL
i the non-null input weight is equal to −1/δL
i and the bias is (ωi−li)/δL
i, while for yR
i the non-null
122"
NETWORK CONSTRUCTION,0.3734015345268542,"input weight is equal to 1/δR
i and bias is (−ωi−li)/δR
i. By recalling (1) one gets
123"
NETWORK CONSTRUCTION,0.37595907928388744,"y
L
i = [−xi + ωi −li"
NETWORK CONSTRUCTION,0.37851662404092073,"δL
i
]"
NETWORK CONSTRUCTION,0.38107416879795397,"+
and
y
R
i = [xi −ωi −li"
NETWORK CONSTRUCTION,0.3836317135549872,"δR
i
] + (7)"
NETWORK CONSTRUCTION,0.38618925831202044,"In the second hidden layer, the neuron computing the z we consider has all input weights equal to 0
124"
NETWORK CONSTRUCTION,0.3887468030690537,"but those connecting to yL
1,yR
1,...,yL
N,yR
N. Non-null input weights are equal to −1 and the bias is 1
125"
NETWORK CONSTRUCTION,0.391304347826087,"so that
126 δR
1 δL
2 x1"
NETWORK CONSTRUCTION,0.3938618925831202,"zω(x1, x2) x2 2ℓ2 δR
2"
NETWORK CONSTRUCTION,0.39641943734015345,"2ℓ1
δL
1"
NETWORK CONSTRUCTION,0.3989769820971867,"(ω1, ω2)"
NETWORK CONSTRUCTION,0.40153452685422,"Figure 2: Three dimensional plot of a generic zω (x) for N = 2 and its contour plot showing the role
of the various parameters."
NETWORK CONSTRUCTION,0.4040920716112532,"z = [ max
i∈{1,..,N}{0,−y
L
i,−y
R
i} +
min
i∈{1,..,N}{0,−y
L
i,−y
R
i} + 1]"
NETWORK CONSTRUCTION,0.40664961636828645,"+
= [1 −
max
i∈{1,..,N}{y
L
i,y
R
i}] +
(8)"
NETWORK CONSTRUCTION,0.4092071611253197,"Considering the last expression, note that, if xi ≥ωi then yR
i ≥0 and yL
i = 0 while, if xi < ωi then
127"
NETWORK CONSTRUCTION,0.4117647058823529,"yR
i = 0 and yL
i ≥0. Hence, without loss of generality, we may assume that xi ≥ωi for i = 1,...,N,
128"
NETWORK CONSTRUCTION,0.4143222506393862,"being all other cases a variation of this one by suitable symmetry and scaling. With this, yL
i = 0 for
129"
NETWORK CONSTRUCTION,0.41687979539641945,"i = 1,...,N and (8) becomes
130"
NETWORK CONSTRUCTION,0.4194373401534527,"z = [1 −max
i=1,...,N [xi −ωi −li"
NETWORK CONSTRUCTION,0.4219948849104859,"δR
i
] +
]"
NETWORK CONSTRUCTION,0.42455242966751916,"+
= [1 −max
i=1,...,N {0, xi −ωi −li"
NETWORK CONSTRUCTION,0.42710997442455245,"δR
i
}] +
(9)"
NETWORK CONSTRUCTION,0.4296675191815857,"that is equivalent to the thesis.
131"
NETWORK CONSTRUCTION,0.4322250639386189,"To interpret Lemma 1 note that ∆(x) is a scaled measure of how far the input vector x is from the
132"
NETWORK CONSTRUCTION,0.43478260869565216,"hyper-rectangle centered at ω = (ω1,...,ωN) with sides 2l1,...,2lN. Hence, z (x) is maximum
133"
NETWORK CONSTRUCTION,0.4373401534526854,"and equal to 1 if x belongs to such a hyper-rectangle and has a piecewise-linear decreasing profile
134"
NETWORK CONSTRUCTION,0.4398976982097187,"when x gets further from ω. Figure 2 reports an example of a z (x) when N = 2.
135"
NETWORK CONSTRUCTION,0.4424552429667519,"In the following, we will assume that each neuron in the second hidden layer matches a whole
136"
NETWORK CONSTRUCTION,0.44501278772378516,"subnetwork as implied by Lemma 1. With this, we may re-index the outputs of the second hidden
137"
NETWORK CONSTRUCTION,0.4475703324808184,"layer as zω (x) associating each of them with the center of the hyper-rectangle in which zω (x) = 1.
138"
NETWORK CONSTRUCTION,0.45012787723785164,"The same is done with the corresponding weights cω in the output layers.
139"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.45268542199488493,"6.2
Universal approximation properties with normalized linear output neuron
140"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.45524296675191817,"Given a positive integer n, define Ω= {0, 1 n, 2"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.4578005115089514,"n,...,1}
N and include in the two hidden layers all the
141"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.46035805626598464,"subnetworks implied by Lemma 1 to implement the function zω (x) for each ω ∈Ω.
142"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.4629156010230179,"In each of these subnetworks set δL
i = δR
i = δ = 1/n for i = 1,...,N and li = 0 for i = 1,...,N.
143"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.46547314578005117,"With this, zω (x) is and (N + 1)-dimensional pyramid whose base is an N-dimensional hypercube
144"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.4680306905370844,"with sides of length 2δ and center in ω.
145"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.47058823529411764,"Proof of Theorem 1. Note first that for any given x ∈XN, only a limited number of functions zω (x)
146"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.4731457800511509,"are not null. In particular, if ki = ⌊nxi⌋for i = 1,...,N is the largest integer not exceeding nxi, then
147"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.47570332480818417,"zω (x) > 0 only if ω belongs to the set Ωx = {k1δ,(k1 + 1)δ} × ⋅⋅⋅× {kNδ,(kN + 1)δ} that contains
148"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.4782608695652174,"the 2N corners of the N-dimensional hypercube Cx = [k1δ,(k1 + 1)δ] × ⋅⋅⋅× [kNδ,(kN + 1)δ].
149"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.48081841432225064,"Hence, we may evaluate Z(x) focusing on functions zω (x) with ω ∈Ωx.
150"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.4833759590792839,"Define the functions
151"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.4859335038363171,"ζω (x) =
zω (x)
∑ω′∈Ωzω′ (x)
(10)"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.4884910485933504,"that are such that ∑ω∈Ωζω (x) = ∑ω∈Ωx ζω (x) = 1 for any x ∈XN, and set cω = f (ω) for each
152"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.49104859335038364,"ω ∈Ω.
153"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.4936061381074169,"The error ∥f (x) −Z (x)∥0,∞in Theorem 1 can be written as
154"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.4961636828644501,"XXXXXXXXXXX
f (x) −∑
ω∈Ωx
f (ω)ζω (x)
XXXXXXXXXXX0,∞
=
XXXXXXXXXXX
∑
ω∈Ωx
[f (x) −f (ω)]ζω (x)
XXXXXXXXXXX0,∞
≤max
x∈XN max
ξ∈Cx
ω∈Ωx
∣f (ξ) −f (ω)∣"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.49872122762148335,"Since f ∶XN ↦R is continuous on the compact domain XN, it is also uniformly continuous and,
155"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.5012787723785166,"for any given level of tolerance ϵ > 0, there is a ∆x such that for any x′,x′′ ∈XN with distance
156"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.5038363171355499,"∥x′ −x′′∥2 ≤∆x we have ∣f(x′) −f(x′′)∣≤ϵ. For a given x, the distance between any ξ ∈Cx and
157"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.5063938618925832,"any ω ∈Ωx is ∥ξ −ω∥2 ≤δ
√"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.5089514066496164,"N. Since δ = 1/n we can select n so that
158"
UNIVERSAL APPROXIMATION PROPERTIES WITH NORMALIZED LINEAR OUTPUT NEURON,0.5115089514066496,"∥f (x) −Z (x)∥0,∞≤max
x∈XN max
ξ∈Cx
ω∈Ωx
∣f (ξ) −f (ω)∣≤ϵ 159"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5140664961636828,"6.3
Universal approximation properties with linear output neuron
160"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5166240409207161,"In this case, the approximation capabilities of our network over the whole domain depend on the
161"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5191815856777494,"local behaviour of subnetworks converging not in a single second-hidden-layer neuron but in 2N
162"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5217391304347826,"second-hidden-layer neurons.
163"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5242966751918159,"Formally, given a center ω ∈XN we include in a subnetwork neurons of the second hidden layer with
164"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5268542199488491,"outputs labelled zω1−,zω1+,...,zωN−,zωN+ as well as all the previous neurons needed to compute
165"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5294117647058824,"such outputs.
166"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5319693094629157,"The expression of each zωj± is given by Lemma 1 and thus is defined by the center point ωj± =
167"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5345268542199488,"(ωj±
1 ,...,ωj±
N ), by the slopes δ
L,j±
1
,...,δ
L,j±
N
and δ
R,j±
1
,...,δ
R,j±
N
, as well as by the side lengths
168"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5370843989769821,"lj±
1 ,...,lj±
N .
169"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5396419437340153,"In a subnetwork, everything depends on two quantities δ,ℓ≥0 that are used to set
170"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5421994884910486,"ωj±
i
= {ωi
if i ≠j
ωi ± ℓ
if i = j
lj±
i
= {ℓ
if i ≠j
0
if i = j"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5447570332480819,"δ
R,j−
i
= δ"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5473145780051151,"δ
L,j−
i
= {δ
if i ≠j
2ℓ
if i = j"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5498721227621484,"δ
R,j+
i
= {δ
if i ≠j
2ℓ
if i = j"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5524296675191815,"δ
L,j+
i
= δ"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5549872122762148,"for i,j = 1,...,N.
171"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5575447570332481,"To give some intuitive grounding to the above definitions, Figure 3 reports example profiles for 4
172"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5601023017902813,"output functions zω1−,zω1+,zω2−,zω2+ with N = 2.
173"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5626598465473146,"Given a center ω, the same quantities δ and ℓallow to define the two domain subsets
174"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5652173913043478,"X
∎
ω = {x ∈XN ∣
max
i=1,...,N {∣xi −ωi∣} ≤ℓ}
X
◻
ω = {x ∈XN ∣ℓ<
max
i=1,...,N {∣xi −¯ωi∣} ≤ℓ+ δ }"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5677749360613811,"as well as Xω = X ∎
ω ∪X ◻
ω.
175 x1 x1
x1 ω1− ω2− ω2+ ω ω
ω ω1+ x1"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5703324808184144,"zω1−(x1, x2)
zω1+(x1, x2)"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5728900255754475,"zω2−(x1, x2)
zω2+(x1, x2) x2
x2 x2
x2 ω1−ω ω1+"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5754475703324808,"ω2+
ω2−"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.578005115089514,"Figure 3: Three dimensional plots of the functions zω1−,zω1+,zω2−,zω2+ with N = 2."
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5805626598465473,"The approximation capabilities depend on the behaviour of the output of the subnetworks in the three
176"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5831202046035806,"disjoint domains X ∎
ω, X ◻
ω, and XN ∖Xω.
177"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5856777493606138,"It is easy to see that if x ∈XN ∖Xω then zωj± = 0 for j = 1,...,N.
178"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5882352941176471,"For x ∈X ∎
ω the following Lemma holds.
179"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5907928388746803,"Lemma 2. Given any choice of N + 1 coefficients a and bj for j = 1,...,N, one may choose 2N
180"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5933503836317136,"weights cj± with j = 1,...,N such that
181"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.5959079283887468,"Zω (x) ≡
N
∑
j=1
cj±zωj± (x) = a +
N
∑
j=1
bjxj
(11)"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.59846547314578,"for any x ∈X ∎
ω, where Zω (x) remains implicitly defined.
182"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6010230179028133,"Proof of Lemma 2. Due to the definition of ωj± we have
183"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6035805626598465,"X
∎
ω
=
[ω1 −ℓ,ω1 + ℓ] × ⋯× [ωN −ℓ,ωN + ℓ] = [ω1−
1 ,ω1+
1 ] × ⋯× [ωN−
N ,ωN+
N ]"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6061381074168798,"Hence, if x ∈X ∎
ω we know that ωj−
j
≤xj ≤ωj+
j
for j = 1,...,N.
184"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6086956521739131,"Moreover, since by definition for any i,j = 1,...,N and i ≠j we have ωj+
i −ωj−
i
= 2ℓand ωj−
i +ωj+
i
=
185"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6112531969309463,"2ωi , then ∣xi −ωj±
i ∣≤ℓwhen i ≠j. Therefore, one can apply Lemma 1 and compute ∆(x), for
186"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6138107416879796,"which all the terms in (6) but ∣xj −ωj±
j ∣are non-positive, thus yielding zωj± (x) = 1 −∣xj−ωj±
j ∣/(2ℓ).
187"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6163682864450127,"Without any loss of generality, translate Xω so that ω = (ℓ,...,ℓ). This implies ωj−
j
= 0 and ωj+
j
= 2ℓ
188"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.618925831202046,"for j = 1,...,N, thus yielding zωj−(x) = 1 −xj"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6214833759590793,2ℓand zωj+ (x) = xj
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6240409207161125,"2ℓ. With this,
189"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6265984654731458,"N
∑
j=1
cj±zωj± (x) =
N
∑
j=1
[cj−(1 −xj"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.629156010230179,2ℓ) + cj+ xj
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6317135549872123,"2ℓ]
=
N
∑
j=1
cj−+
N
∑
j=1
(cj+ −cj−) xj 2ℓ"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6342710997442456,"that can yield any affine function f(x) = a + ∑N
j=1 bjxj by setting, for j = 1,...,N,
190"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6368286445012787,cj−= a
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.639386189258312,"N
and
cj+ = cj−+ 2ℓbj
(12) 191"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6419437340153452,"Finally, what happens for x ∈X ◻
ω is described by the following Lemma.
192"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6445012787723785,"Lemma 3. If the 2N weights cj± with j = 1,...,N are set according to Lemma 2 so that Zω (x) =
193"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6470588235294118,"a+∑N
j=1 bjxj for any x ∈X ∎
ω, for coefficients satisfying ∣a∣,∣bj∣≤M for some M > 0 and j = 1,...,N,
194"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.649616368286445,"then ∣Zω (x)∣≤3MN for any x ∈Xω and thus for any x ∈X ◻
ω.
195"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6521739130434783,"Proof of Lemma 3. From ∣a∣,∣bj∣≤M and from (12) we get ∣cj−∣≤M/N and ∣cj+∣≤M/N + 2ℓM.
196"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6547314578005116,"Overall, since ℓ≤1 and N ≥1 we have ∣cj±∣≤3M.
Since 0 ≤zωj± ≤1 and Zω (x) =
197"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6572890025575447,"∑N
j=1 cj±zωj± (x) we finally get the thesis.
198"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.659846547314578,"The above characterization of the output of Z-subnetworks allows to prove their local approximation
199"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6624040920716112,"capabilities.
200"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6649616368286445,"Lemma 4. Given any function f ∈C2 (XN), there are two constants P,Q > 0 such that
201"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6675191815856778,"Eω
≡
∫Xω
∣f (x) −Zω (x)∣p dx +
N
∑
j=1∫Xω
∣∂f"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.670076726342711,"∂xj
(x) −∂Zω"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6726342710997443,"∂xj
(x)∣ p
dx"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6751918158567775,"≤
(2ℓ+ 2δ)N{Pℓp [1 −o(δ/ℓ)] + Qo(δ/ℓ)}"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6777493606138107,"with o(⋅) = 1 −1/(1+⋅)N
202"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.680306905370844,"Proof of Lemma 4. Since f ∈C2 (XN) and XN is compact, M0,M1,M2 ≥0 exists such that
203"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6828644501278772,"∣f (x)∣≤M0,
∣∂f"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6854219948849105,"∂xi
(x)∣≤M1,
∣∂2f"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6879795396419437,"∂xixj
(x)∣≤M2
(13)"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.690537084398977,"for any x ∈XM and i,j = 1,...,N.
204"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6930946291560103,"Assuming x ∈X ∎
ω, and thus ∣xi −ωi∣≤ℓ, the above bounds can be used jointly with the Taylor
205"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6956521739130435,"expansions of f and its derivatives around ω
206"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.6982097186700768,"f (x)
=
f (ω) +
N
∑
i=1"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.7007672634271099,"∂f
∂xi
(ω)(xi −ωi) +
N
∑
i=1"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.7033248081841432,"N
∑
j=1
Ri,j (x)(xi −ωi)(xj −ωj)
(14)"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.7058823529411765,"∂f
∂xi
(x)
=
∂f
∂xi
(ω) +
N
∑
j=1
Si,j (x)(xj −ωj)
i = 1,...,N
(15)"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.7084398976982097,"to ensure that their error terms satisfy
207"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.710997442455243,RRRRRRRRRRR
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.7135549872122762,"N
∑
i=1"
UNIVERSAL APPROXIMATION PROPERTIES WITH LINEAR OUTPUT NEURON,0.7161125319693095,"N
∑
j=1
Ri,j (x)(xi −ωi)(xj −ωj)
RRRRRRRRRRR
≤N 2ℓ2 1"
MAX,0.7186700767263428,"2
max
k,l=1,...,N max
ξ∈XN ∣∂2f"
MAX,0.7212276214833759,"∂xkxl
(ξ)∣≤1"
MAX,0.7237851662404092,"2M2N 2ℓ2
(16)"
MAX,0.7263427109974424,"and
208"
MAX,0.7289002557544757,RRRRRRRRRRR
MAX,0.731457800511509,"N
∑
j=1
Si,j (x)(xj −ωj)
RRRRRRRRRRR
≤N 2ℓ2 1"
MAX,0.7340153452685422,"2
max
j=1,...,N max
ξ∈XN ∣∂2f"
MAX,0.7365728900255755,"∂xixj
(ξ)∣≤1"
MAX,0.7391304347826086,"2M2Nℓ
i = 1,...,N
(17)"
MAX,0.7416879795396419,"Again focusing on x ∈X ∎
ω, exploit Lemma 2 to set the weights cj± yielding
209"
MAX,0.7442455242966752,"Zω (x) = f (ω) +
N
∑
i=1"
MAX,0.7468030690537084,"∂f
∂xi
(ω)(xi −ωi) = [f (ω) −
N
∑
i=1"
MAX,0.7493606138107417,"∂f
∂xi
(ω)ωi] +
N
∑
i=1"
MAX,0.7519181585677749,"∂f
∂xi
(ω)xi
(18)"
MAX,0.7544757033248082,which is also such that ∂Zω
MAX,0.7570332480818415,"∂xi (x) =
∂f
∂xu (ω).
210"
MAX,0.7595907928388747,"Hence, we may program Zω to reproduce the beaviour of f and its derivatives in X ∎
ω, and the
211"
MAX,0.7621483375959079,"approximation errors can be derived exploiting (14) with (16) and (15) with (17) to obtain
212"
MAX,0.7647058823529411,∣Zω (x) −f (x)∣≤1
MAX,0.7672634271099744,"2M2N 2ℓ2,
∣∂Zω"
MAX,0.7698209718670077,"∂xi
(x) −∂f"
MAX,0.7723785166240409,"∂xi
(x)∣≤1"
MAX,0.7749360613810742,"2M2Nℓ
(19)"
MAX,0.7774936061381074,"To address the case x ∈X ◻
ω, we may apply Lemma 3. By matching (18) with (13) we get that
213"
MAX,0.7800511508951407,"∣a∣≤M0 + M1N and ∣bi∣≤M1 ≤M0 + M1N for i = 1,...,N. Hence, if x ∈X ◻
¯ω, then if
214"
MAX,0.782608695652174,"M3 = M0(1 + 3N) + 3M1N 2 we have
215"
MAX,0.7851662404092071,"∣Zω (x) −f (x)∣≤M3,
∣∂Zω"
MAX,0.7877237851662404,"∂xi
(x) −∂f"
MAX,0.7902813299232737,"∂xi
(x)∣= ∣∂f"
MAX,0.7928388746803069,"∂xi
(ω) −∂f"
MAX,0.7953964194373402,"∂xi
(x)∣≤2M1
(20)"
MAX,0.7979539641943734,"Since we have different error bounds in X ∎
ω and X ◻
¯ω, we bound the overall error Eω by splitting
216"
MAX,0.8005115089514067,"Eω
=
∫X∎
ω
∣f (x) −Zω (x)∣p dx +
N
∑
j=1∫X∎
ω
∣∂f"
MAX,0.80306905370844,"∂xj
(x) −∂Zω"
MAX,0.8056265984654731,"∂xj
(x)∣"
MAX,0.8081841432225064,"p
dx +"
MAX,0.8107416879795396,"∫X◻
ω
∣f (x) −Zω (x)∣p dx +
N
∑
j=1∫X◻
ω
∣∂f"
MAX,0.8132992327365729,"∂xj
(x) −∂Zω"
MAX,0.8158567774936062,"∂xj
(x)∣ p
dx+"
MAX,0.8184143222506394,"and apply (19) and (20) to bound each integrand. Adding the fact that the measure of X ∎
ω is (2ℓ)N,
217"
MAX,0.8209718670076727,"while the measure of X ◻
ω is (2ℓ+ 2δ)N −(2ℓ)N we obtain
218"
MAX,0.8235294117647058,Eω ≤[(1
MAX,0.8260869565217391,2M2N 2ℓ2)
MAX,0.8286445012787724,"p
+ (1"
MAX,0.8312020460358056,2M2Nℓ)
MAX,0.8337595907928389,"p
](2ℓ)N + [M p
3 + (2M1)p][(2ℓ+ 2δ)N −(2ℓ)N]"
MAX,0.8363171355498721,from which we may set P = ( 1
MAX,0.8388746803069054,"2M2N 2)
p + ( 1"
MAX,0.8414322250639387,"2M2N)
p and Q = M p
3 + (2M1)p to get the thesis.
219"
MAX,0.8439897698209718,"We are now in the position of proving our second result.
220"
MAX,0.8465473145780051,"Proof of Theorem 2. For n > 0 integer define δ and ℓsuch that δ = ℓ2 and 2ℓ+ 2δ = 1/n. Let also
221"
MAX,0.8491048593350383,"Ω= { 1 2n, 3"
MAX,0.8516624040920716,"2n,..., 2n−1"
MAX,0.8542199488491049,"2n }
N so that XN is partitioned in nN hyper-cubes Xω with centers ω ∈Ωand
222"
MAX,0.8567774936061381,"side 2ℓ+ 2δ. The output of the whole network is Z (x) = ∑ω∈ΩZω (x).
223"
MAX,0.8593350383631714,"Since Zω (x) is null for x /∈Xω, the error measure over XN can be decomposed into
224"
MAX,0.8618925831202046,"∥f −Z∥p
1,p = ∑
ω∈Ω"
MAX,0.8644501278772379,"⎧⎪⎪⎨⎪⎪⎩
∫Xω
∣f (x) −Zω (x)∣p dx +
N
∑
j=1∫Xω
∣∂f"
MAX,0.8670076726342711,"∂xj
(x) −∂Zω"
MAX,0.8695652173913043,"∂xj
(x)∣"
MAX,0.8721227621483376,"p
dx
⎫⎪⎪⎬⎪⎪⎭"
MAX,0.8746803069053708,"Each of the terms in the last sum can be bounded using Lemma 4 in which we may also substitute
225"
MAX,0.8772378516624041,"2ℓ+ 2δ = 1/n and δ = ℓ2 to yield
226"
MAX,0.8797953964194374,"∥f −Z∥p
1,p ≤∑
ω∈Ω"
MAX,0.8823529411764706,"1
nN {Pℓp [1 −o(ℓ)] + Qo(ℓ)} = Pℓp [1 −o(ℓ)] + Qo(ℓ)"
MAX,0.8849104859335039,"Since when n →∞we have ℓ→0 and thus o(ℓ) →0 the thesis is proven.
227"
CONCLUSIONS,0.887468030690537,"7
Conclusions
228"
CONCLUSIONS,0.8900255754475703,"We established that neural networks in which hidden MAC neurons are substituted with MAM
229"
CONCLUSIONS,0.8925831202046036,"neurons to obtain more aggressively prunable architectures are still universal approximators.
230"
REFERENCES,0.8951406649616368,"References
231"
REFERENCES,0.8976982097186701,"[1] L. Prono, P. Bich, M. Mangia, F. Pareschi, R. Rovatti, and G. Setti, “A Multiply-And-Max/min Neuron
232"
REFERENCES,0.9002557544757033,"Paradigm for Aggressively Prunable Deep Neural Networks,” Apr. 2023, preprint available on TechRxiv.
233"
REFERENCES,0.9028132992327366,"[2] P. Bich, L. Prono, M. Mangia, F. Pareschi, R. Rovatti, and G. Setti, “Aggressively prunable MAM²-based
234"
REFERENCES,0.9053708439897699,"Deep Neural Oracle for ECG acquisition by Compressed Sensing,” in 2022 IEEE Biomedical Circuits and
235"
REFERENCES,0.907928388746803,"Systems Conference (BioCAS), Oct. 2022, pp. 163–167.
236"
REFERENCES,0.9104859335038363,"[3] G. Cybenko, “Approximation by superpositions of a sigmoidal function,” Mathematics of Control, Signals
237"
REFERENCES,0.9130434782608695,"and Systems, vol. 2, no. 4, pp. 303–314, Dec. 1989.
238"
REFERENCES,0.9156010230179028,"[4] L.-X. Wang, “Fuzzy systems are universal approximators,” in [1992 Proceedings] IEEE International
239"
REFERENCES,0.9181585677749361,"Conference on Fuzzy Systems, 1992, pp. 1163–1170.
240"
REFERENCES,0.9207161125319693,"[5] B. Kosko, “Fuzzy systems as universal approximators,” IEEE Transactions on Computers, vol. 43, no. 11,
241"
REFERENCES,0.9232736572890026,"pp. 1329–1333, 1994.
242"
REFERENCES,0.9258312020460358,"[6] J. Castro, “Fuzzy logic controllers are universal approximators,” IEEE Transactions on Systems, Man, and
243"
REFERENCES,0.928388746803069,"Cybernetics, vol. 25, no. 4, pp. 629–635, Apr. 1995.
244"
REFERENCES,0.9309462915601023,"[7] R. Rovatti, “Fuzzy piecewise multilinear and piecewise linear systems as universal approximators in
245"
REFERENCES,0.9335038363171355,"sobolev norms,” IEEE Transactions on Fuzzy Systems, vol. 6, no. 2, pp. 235–249, 1998.
246"
REFERENCES,0.9360613810741688,"[8] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward networks are universal approximators,”
247"
REFERENCES,0.9386189258312021,"Neural Networks, vol. 2, no. 5, pp. 359–366, 1989.
248"
REFERENCES,0.9411764705882353,"[9] A. Pinkus, “Approximation theory of the MLP model in neural networks,” Acta Numerica, vol. 8, pp.
249"
REFERENCES,0.9437340153452686,"143–195, Jan. 1999.
250"
REFERENCES,0.9462915601023018,"[10] G. Gripenberg, “Approximation by neural networks with a bounded number of nodes at each level,” Journal
251"
REFERENCES,0.948849104859335,"of Approximation Theory, vol. 122, no. 2, pp. 260–266, Jun. 2003.
252"
REFERENCES,0.9514066496163683,"[11] Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang, “The Expressive Power of Neural Networks: A View from the
253"
REFERENCES,0.9539641943734015,"Width,” in Advances in Neural Information Processing Systems, vol. 30.
Curran Associates, Inc., 2017.
254"
REFERENCES,0.9565217391304348,"[12] B. Hanin and M. Sellke, “Approximating Continuous Functions by ReLU Nets of Minimal Width,” Mar.
255"
REFERENCES,0.959079283887468,"2018.
256"
REFERENCES,0.9616368286445013,"[13] V. Maiorov and A. Pinkus, “Lower bounds for approximation by MLP neural networks,” Neurocomputing,
257"
REFERENCES,0.9641943734015346,"vol. 25, no. 1, pp. 81–91, Apr. 1999.
258"
REFERENCES,0.9667519181585678,"[14] N. J. Guliyev and V. E. Ismailov, “On the approximation by single hidden layer feedforward neural
259"
REFERENCES,0.969309462915601,"networks with fixed weights,” Neural Networks, vol. 98, pp. 296–304, Feb. 2018.
260"
REFERENCES,0.9718670076726342,"[15] Y. Cai, “Achieve the Minimum Width of Neural Networks for Universal Approximation,” in The Eleventh
261"
REFERENCES,0.9744245524296675,"International Conference on Learning Representations, Feb. 2023.
262"
REFERENCES,0.9769820971867008,"[16] D.-X. Zhou, “Universality of deep convolutional neural networks,” Applied and Computational Harmonic
263"
REFERENCES,0.979539641943734,"Analysis, vol. 48, no. 2, pp. 787–794, 2020.
264"
REFERENCES,0.9820971867007673,"[17] O. A. Manita, M. A. Peletier, J. W. Portegies, J. Sanders, and A. Senen-Cerda, “Universal approximation
265"
REFERENCES,0.9846547314578005,"in dropout neural networks,” J. Mach. Learn. Res., vol. 23, no. 1, jan 2022.
266"
REFERENCES,0.9872122762148338,"[18] Y. Lu and J. Lu, “A universal approximation theorem of deep neural networks for expressing probability
267"
REFERENCES,0.989769820971867,"distributions,” in Advances in Neural Information Processing Systems, vol. 33.
Curran Associates, Inc.,
268"
REFERENCES,0.9923273657289002,"2020, pp. 3094–3105.
269"
REFERENCES,0.9948849104859335,"[19] S.-Q. Zhang and Z.-H. Zhou, “Theoretically provable spiking neural networks,” in Advances in Neural
270"
REFERENCES,0.9974424552429667,"Information Processing Systems, vol. 35.
Curran Associates, Inc., 2022, pp. 19 345–19 356.
271"
