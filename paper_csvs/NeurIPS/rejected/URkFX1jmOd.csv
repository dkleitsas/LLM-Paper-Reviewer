Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001098901098901099,"Night-to-Day translation (Night2Day) aims to achieve day-like vision for nighttime
1"
ABSTRACT,0.002197802197802198,"scenes. However, processing night images with complex degradations remains a
2"
ABSTRACT,0.0032967032967032967,"significant challenge under unpaired conditions. Previous methods that uniformly
3"
ABSTRACT,0.004395604395604396,"mitigate these degradations have proven inadequate in simultaneously restoring
4"
ABSTRACT,0.005494505494505495,"daytime domain information and preserving underlying semantics. In this paper,
5"
ABSTRACT,0.006593406593406593,"we propose N2D3 (Night-to-Day via Degradation Disentanglement) to identify
6"
ABSTRACT,0.007692307692307693,"different degradation patterns in nighttime images. Specifically, our method com-
7"
ABSTRACT,0.008791208791208791,"prises a degradation disentanglement module and a degradation-aware contrastive
8"
ABSTRACT,0.00989010989010989,"learning module. Firstly, we extract physical priors from a photometric model
9"
ABSTRACT,0.01098901098901099,"based on Kubelka-Munk theory. Then, guided by these physical priors, we design a
10"
ABSTRACT,0.012087912087912088,"disentanglement module to discriminate among different illumination degradation
11"
ABSTRACT,0.013186813186813187,"regions. Finally, we introduce the degradation-aware contrastive learning strategy
12"
ABSTRACT,0.014285714285714285,"to preserve semantic consistency across distinct degradation regions. Our method
13"
ABSTRACT,0.015384615384615385,"is evaluated on two public datasets, demonstrating a significant improvement of
14"
ABSTRACT,0.016483516483516484,"5.4 FID on BDD100K and 10.3 FID on Alderley.
15"
INTRODUCTION,0.017582417582417582,"1
Introduction
16"
INTRODUCTION,0.01868131868131868,"Nighttime images often suffer from severe information loss, posing significant challenges to both
17"
INTRODUCTION,0.01978021978021978,"human visual recognition and computer vision tasks including detection, segmentation, etc. [14].
18"
INTRODUCTION,0.020879120879120878,"In contrast, daylight images exhibit rich content and intricate details. Achieving day-like nighttime
19"
INTRODUCTION,0.02197802197802198,"vision remains a primary objective in nighttime perception, sparking numerous pioneering works [30].
20"
INTRODUCTION,0.023076923076923078,"Night-to-Day image translation (Night2Day) offers a comprehensive solution to achieve day-like
21"
INTRODUCTION,0.024175824175824177,"vision at night. The primary goal is to transform images from nighttime to daytime while maintaining
22"
INTRODUCTION,0.025274725274725275,"their underlying semantic structure. However, achieving this goal is challenging. It requires to process
23"
INTRODUCTION,0.026373626373626374,"complex degraded images using unpaired data, which raises additional difficulties compared to other
24"
INTRODUCTION,0.027472527472527472,"image translation tasks.
25"
INTRODUCTION,0.02857142857142857,"Recently, explorations have been made in Night2Day. Early approaches, such as ToDayGAN,
26"
INTRODUCTION,0.02967032967032967,"demonstrated the effectiveness of cycle-consistent learning in maintaining semantic structure [1].
27"
INTRODUCTION,0.03076923076923077,"Subsequent methods incorporated auxiliary structure regularization techniques, including perceptual
28"
INTRODUCTION,0.031868131868131866,"loss and uncertainty regularization, to better preserve the original structure [33, 18]. Furthermore,
29"
INTRODUCTION,0.03296703296703297,"some methods utilized daytime images with nearby GPS locations to aid in coarse structure regular-
30"
INTRODUCTION,0.03406593406593406,"ization [26]. However, these methods often neglect the complex degradations at nighttime, applying
31"
INTRODUCTION,0.035164835164835165,"structure regularization uniformly and resulting in severe artifacts. To address this issue, more recent
32"
INTRODUCTION,0.03626373626373627,"approaches adopt auxiliary human annotations to maintain semantic consistency, such as segmenta-
33"
INTRODUCTION,0.03736263736263736,"tion maps and bounding boxes [16, 22]. Despite their potential, these methods are labor-intensive
34"
INTRODUCTION,0.038461538461538464,"and challenging, especially since many nighttime scenes are beyond human cognition.
35"
INTRODUCTION,0.03956043956043956,Physical Prior
INTRODUCTION,0.04065934065934066,w Disentangled Regularization
INTRODUCTION,0.041758241758241756,w/o Disentangled Regularization
INTRODUCTION,0.04285714285714286,"(c) Visual Comparison
(b) Disentangled Regularization 
(a) Disentanglement Process"
INTRODUCTION,0.04395604395604396,Darkness
INTRODUCTION,0.045054945054945054,Well-lit
INTRODUCTION,0.046153846153846156,Generated Patches
INTRODUCTION,0.04725274725274725,High-light
INTRODUCTION,0.04835164835164835,Light Effects
INTRODUCTION,0.04945054945054945,"Figure 1: Illustration of our motivation. (a) The disentanglement process leverages physical priors. (b)
The image patches are restored individually for each degradation type. (c) The proposed Disentangled
Regularization improves the overall performance."
INTRODUCTION,0.05054945054945055,"The critical limitation of the aforementioned methods is the disregard for complex degraded regions.
36"
INTRODUCTION,0.051648351648351645,"Specifically, different regions in nighttime images possess varying characteristics, such as extreme
37"
INTRODUCTION,0.05274725274725275,"darkness, well-lit regions, light effects, etc. Treating all these degraded regions equally could adversely
38"
INTRODUCTION,0.05384615384615385,"impact the results. As illustrated in Figure 1, our key insight emphasizes that nighttime images suffer
39"
INTRODUCTION,0.054945054945054944,"from various degradations, necessitating customizing restoration for different degradation types.
40"
INTRODUCTION,0.056043956043956046,"Intuitively, we manage to disentangle nighttime images into patches according to the recognized
41"
INTRODUCTION,0.05714285714285714,"degradation type and learn individual restoration patterns for them to enhance the overall performance.
42"
INTRODUCTION,0.05824175824175824,"Motivated by this point, we propose N2D3 (Night to Day via Degradation Disentanglement), which
43"
INTRODUCTION,0.05934065934065934,"utilizes Generative Adversarial Networks (GANs) to bridge the domain gap between nighttime and
44"
INTRODUCTION,0.06043956043956044,"daytime in a degradation-aware manner, as illustrated in Figure 2. There are two modules in N2D3,
45"
INTRODUCTION,0.06153846153846154,"including physical-informed degradation disentanglement and degradation-aware contrastive learning,
46"
INTRODUCTION,0.06263736263736264,"which are employed to preserve the semantic structure of nighttime images. In the disentanglement
47"
INTRODUCTION,0.06373626373626373,"of nighttime degradation, a photometric model tailored to nighttime scenes is conducted to extract
48"
INTRODUCTION,0.06483516483516484,"physical priors. Subsequently, the illuminance and physical priors are integrated to disentangle
49"
INTRODUCTION,0.06593406593406594,"regions into darkness, well-lit, high-light, and light effects. Building on this, degradation-aware
50"
INTRODUCTION,0.06703296703296703,"contrastive learning is designed to constrain the similarity of the source and generated images in
51"
INTRODUCTION,0.06813186813186813,"different regions. It comprises disentanglement-guided sampling and reweighting strategies. The
52"
INTRODUCTION,0.06923076923076923,"sampling strategy mines valuable anchors and hard negative examples, while the reweighting process
53"
INTRODUCTION,0.07032967032967033,"assigns their weights. They enhance vanilla contrastive learning by prioritizing valuable patches with
54"
INTRODUCTION,0.07142857142857142,"appropriate attention. Ultimately, our method yields highly faithful results that are visually pleasing
55"
INTRODUCTION,0.07252747252747253,"and beneficial for downstream vision tasks including keypoint matching and semantic segmentation.
56"
INTRODUCTION,0.07362637362637363,"Our contributions are summarized as follows:
57"
INTRODUCTION,0.07472527472527472,"(1) We propose the N2D3 translation method based on the illumination degradation disentanglement
58"
INTRODUCTION,0.07582417582417582,"module, which enables degradation-aware restoration of nighttime images.
59"
INTRODUCTION,0.07692307692307693,"(2) We present a novel degradation-aware contrastive learning module to preserve the semantic
60"
INTRODUCTION,0.07802197802197802,"structure of generated results. The core design incorporates disentanglement-guided sampling and
61"
INTRODUCTION,0.07912087912087912,"reweighting strategies, which greatly enhance the performance of vanilla contrastive learning.
62"
INTRODUCTION,0.08021978021978023,"(3) Experimental results on two public datasets underscore the significance of considering distinct
63"
INTRODUCTION,0.08131868131868132,"degradation types in nighttime scenes. Our method achieves state-of-the-art performance in visual
64"
INTRODUCTION,0.08241758241758242,"effects and downstream tasks.
65"
RELATED WORK,0.08351648351648351,"2
Related Work
66"
RELATED WORK,0.08461538461538462,"Unpaired Image-to-Image Translation. Unpaired image-to-image translation addresses the chal-
67"
RELATED WORK,0.08571428571428572,"lenge of lacking paired data, providing an effective self-supervised learning strategy. To overcome the
68"
RELATED WORK,0.08681318681318681,"efficiency limitations of traditional cycle-consistency learning, Park et al., first introduces contrastive
69"
RELATED WORK,0.08791208791208792,"learning to this domain, achieving efficient one-sided learning[20]. Following this work, several stud-
70"
RELATED WORK,0.08901098901098901,"ies have improved the contrastive learning by generating hard negative examples [24], re-weighting
71"
RELATED WORK,0.09010989010989011,"positive-negative pairs [31], and selecting key samples [9]. Furthermore, other constraints, such as
72"
RELATED WORK,0.0912087912087912,"density [27] and path length [28], have been explored in unpaired image translation. However, all
73"
RELATED WORK,0.09230769230769231,"these works neglect physical priors in the nighttime, leading to suboptimal results in Night2Day.
74"
RELATED WORK,0.09340659340659341,Physical
RELATED WORK,0.0945054945054945,"Prior 
Darkness
Light Effect
Well-lit"
RELATED WORK,0.0956043956043956,ùë•1ùë¶1 ùë•1ùë¶2 ùë•1ùë¶3 ùë•1ùë¶4 ùë•1ùë¶5 ùë•1ùë¶6
RELATED WORK,0.0967032967032967,ùë•2ùë¶1 ùë•2ùë¶2 ùë•2ùë¶3 ùë•2ùë¶4 ùë•2ùë¶5 ùë•2ùë¶6
RELATED WORK,0.0978021978021978,ùë•3ùë¶1 ùë•3ùë¶2 ùë•3ùë¶3 ùë•3ùë¶4 ùë•3ùë¶5 ùë•3ùë¶6
RELATED WORK,0.0989010989010989,ùë•4ùë¶1 ùë•4ùë¶2 ùë•4ùë¶3 ùë•4ùë¶4 ùë•4ùë¶5 ùë•4ùë¶6
RELATED WORK,0.1,ùë•5ùë¶1 ùë•5ùë¶2 ùë•5ùë¶3 ùë•5ùë¶4 ùë•5ùë¶5 ùë•5ùë¶6
RELATED WORK,0.1010989010989011,ùë•6ùë¶1 ùë•6ùë¶2 ùë•6ùë¶3 ùë•6ùë¶4 ùë•6ùë¶5 ùë•6ùë¶6
RELATED WORK,0.1021978021978022,"ùë•1ùë¶1
ùë§12
0
0
0
0"
RELATED WORK,0.10329670329670329,"ùë§21
ùë•2ùë¶2
0
0
0
0"
RELATED WORK,0.1043956043956044,"0
0
ùë•3ùë¶3
ùë§34
0
0"
RELATED WORK,0.1054945054945055,"0
0
ùë§43
ùë•4ùë¶4
0
0"
RELATED WORK,0.10659340659340659,"0
0
0
0
ùë•5ùë¶5
ùë§56"
RELATED WORK,0.1076923076923077,"0
0
0
0
ùë§65
ùë•6ùë¶6"
RELATED WORK,0.1087912087912088,"min
wij,i,j‚àà[1,N][‡∑ç i N ‡∑ç j N"
RELATED WORK,0.10989010989010989,wij ‚ãÖexiyi/Œ≤ ]
RELATED WORK,0.11098901098901098,s. t.‡∑ç i=1 N
RELATED WORK,0.11208791208791209,"wij = 1, ‡∑ç j=1 N"
RELATED WORK,0.11318681318681319,wij = 1
RELATED WORK,0.11428571428571428,(b)  Training Phase ùë¶1 ùë¶2 ùë¶3 ùë¶4 ùë¶5 ùë¶6
RELATED WORK,0.11538461538461539,"ùë•1
ùë•2
ùë•3
ùë•4
ùë•5
ùë•6 ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶"
RELATED WORK,0.11648351648351649,(a) Inference Phase GAN Loss
RELATED WORK,0.11758241758241758,"Deg. Aware 
Reweighting"
RELATED WORK,0.11868131868131868,Optimal Transport
RELATED WORK,0.11978021978021978,"Block Diagonal SM
Similarity Matrix(SM)"
RELATED WORK,0.12087912087912088,Deg. Aware
RELATED WORK,0.12197802197802197,Sampling
RELATED WORK,0.12307692307692308,"Disentangled Map
Disentangled Categories"
RELATED WORK,0.12417582417582418,Generator
RELATED WORK,0.12527472527472527,"Shared 
Encoder
Deg. Aware 
Contrastive Loss"
RELATED WORK,0.12637362637362637,"Figure 2:
The overall architecture of the proposed N2D3 method. The training phase contains
the physical prior informed degradation disentanglement module and degradation-aware contrastive
learning module. They are utilized to optimize the ResNet-based generator which is the main part in
the inference phase."
RELATED WORK,0.12747252747252746,"Nighttime Domain Translation. Domain translation techniques have been applied to address adverse
75"
RELATED WORK,0.12857142857142856,"nighttime conditions. An early contribution is made by Anoosheh et al., which demonstrates the
76"
RELATED WORK,0.12967032967032968,"effectiveness of cycle-consistent learning in Night2Day[1]. Following this, many works incorporate
77"
RELATED WORK,0.13076923076923078,"different modules into cycle-consistent learning to enhance structural modeling capabilities. Zheng et
78"
RELATED WORK,0.13186813186813187,"al. incorporate a fork-shaped encoder to enhance visual perceptual quality[33]. AUGAN employs
79"
RELATED WORK,0.13296703296703297,"uncertainty estimation to mine useful features in nighttime images[18]. Fan et al. explore inter-
80"
RELATED WORK,0.13406593406593406,"frequency relation knowledge to streamline the Night2Day process[5]. Xia et al. utilize nearby GPS
81"
RELATED WORK,0.13516483516483516,"locations to form paired night and daytime images, providing weak supervision[26]. Some other
82"
RELATED WORK,0.13626373626373625,"studies incorporate human annotations to impose structural constraints, overlooking the practical
83"
RELATED WORK,0.13736263736263737,"difficulty of acquiring such annotations at nighttime with multiple degradations [11][16] [22]. To
84"
RELATED WORK,0.13846153846153847,"address the concerns of the aforementioned methods, the proposed N2D3 explores patch-wise
85"
RELATED WORK,0.13956043956043956,"contrastive learning with physical guidance, so as to achieve degradation-aware Night2Day. N2D3 is
86"
RELATED WORK,0.14065934065934066,"free of human annotations and offers comprehensive structural modeling to provide faithful translation
87"
RELATED WORK,0.14175824175824175,"results.
88"
METHODS,0.14285714285714285,"3
Methods
89"
METHODS,0.14395604395604394,"Given nighttime image IN ‚ààN and daytime image ID ‚ààD, the goal of Night2Day is to translate
90"
METHODS,0.14505494505494507,"images from nighttime to daytime while preserving content semantic consistency. This involves the
91"
METHODS,0.14615384615384616,"construction of a mapping function F with parameters Œ∏, which can be formulated as FŒ∏ : IN ‚ÜíID.
92"
METHODS,0.14725274725274726,"Our method N2D3 is illustrated in Figure 2. To train a generator for Night2Day, we employ GANs as
93"
METHODS,0.14835164835164835,"the overall learning framework to bridge the domain gap between nighttime and daytime. Our core
94"
METHODS,0.14945054945054945,"design, consisting of the degradation disentanglement module and the degradation-aware contrastive
95"
METHODS,0.15054945054945054,"learning module, aims to preserve the structure from the source images and suppress artifacts.
96"
METHODS,0.15164835164835164,"In this section, we first introduce physical priors in the nighttime environment, and then describe
97"
METHODS,0.15274725274725276,"the degradation disentanglement module and the degradation-aware contrastive learning module,
98"
METHODS,0.15384615384615385,"respectively.
99"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.15494505494505495,"3.1
Physical Priors for Nighttime Environment
100"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.15604395604395604,"The illumination degradations at night are primarily categorized as darkness, well-lit regions, high-
101"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.15714285714285714,"light regions, and light effects. As shown in Figure 3, well-lit represents the diffused reflectance under
102"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.15824175824175823,"normal light, while the light effects denote phenomena such as flare, glow, and specular reflections.
103"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.15934065934065933,"Intuitively, these regions can be disentangled through the analysis of illumination distribution. Among
104"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.16043956043956045,"these degradation types, darkness and high-light are directly correlated with illuminance and can be
105"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.16153846153846155,"effectively disentangled through illumination estimation.
106"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.16263736263736264,"As a common practice, we estimate the illuminance map L by utilizing the maximum RGB channel
107"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.16373626373626374,"of image IN as L = maxc‚ààR,G,B Ic
N . Then k-nearest neighbors [4] is employed to acquire three
108"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.16483516483516483,"clusters representing darkness, well-lit, and high-light regions. These clusters are aggregated as
109"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.16593406593406593,"masks Md, Mn, Mh. However, the challenge arises with light effects that are mainly related to
110"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.16703296703296702,"Figure 3: The first row displays nighttime images, while the second row shows the corresponding
degradation disentanglement results. The color progression from blue, light blue, green to yellow
corresponds to the following regions: darkness, well-lit, light effects, and high-light, respectively."
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.16813186813186815,"the illumination. Light effects regions tend to intertwine with well-lit regions when using only the
111"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.16923076923076924,"illumination map, as they often share similar illumination densities. To disentangle light effects from
112"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.17032967032967034,"well-lit regions, we need to introduce additional physical priors.
113"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.17142857142857143,"To extract the physical priors for disentangling light effects, we develop a photometric model derived
114"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.17252747252747253,"from Kubelka-Munk theory [17]. This model characterizes the spectrum of light E reflected from an
115"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.17362637362637362,"object as follows:
116"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.17472527472527472,"E(Œª, x) = e(Œª, x)(1 ‚àíœÅf(x))2R‚àû(Œª, x) + e(Œª, x)œÅf(x),
(1)
here x represents the horizontal component for analysis, while the analysis of the vertical component
117"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.17582417582417584,"y is the same as the horizontal component. Œª corresponds to the wavelength of light. e(Œª, x) signifies
118"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.17692307692307693,"the spectrum, representing the illumination density and color. œÅf stands for the Fresnel reflectance
119"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.17802197802197803,"coefficient. R‚àûis the material reflectivity function, formulated as follows at a specific location
120"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.17912087912087912,"x = x0:
121"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.18021978021978022,"R(Œª) = a(Œª) ‚àí
p"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.1813186813186813,"a(Œª)2 ‚àí1, a(Œª) = 1 + k(Œª)"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.1824175824175824,"s(Œª) ,
(2)"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.1835164835164835,"where k(Œª) and s(Œª) denote the absorption and scattering coefficients, respectively. This formulation
122"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.18461538461538463,"implies that for any local pixels, the material reflectivity is determined if the material is given.
123"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.18571428571428572,"Assuming C is the material distribution function, which describes the material type varying across
124"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.18681318681318682,"locations, the material reflectivity R‚àûcan be formulated as:
125"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.1879120879120879,"R‚àû(Œª, x) = R(Œª)C(x).
(3)
Since the mixture of light effects and well-lit regions has been obtained previously, the core of
126"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.189010989010989,"disentangling light effects from well-lit regions lies in separating the illumination e(Œª, x) and re-
127"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.1901098901098901,"flectance components R(Œª)C(x). Note that the Fresnel reflectance coefficient œÅf(x) approaches 0 in
128"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.1912087912087912,"reflectance-dominating well-lit regions, while œÅf(x) approaches 1 in illumination-dominating light
129"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.19230769230769232,"effects regions. According to Equation (1), the photometric model for the mixture of light effects and
130"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.1934065934065934,"well-lit regions is formulated as:
131"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.1945054945054945,"E(Œª, x) =
e(Œª, x),
if x /‚àà‚Ñ¶
e(Œª, x)R(Œª)C(x),
if x ‚àà‚Ñ¶,
(4)"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.1956043956043956,"where ‚Ñ¶denotes the reflectance-dominating well-lit regions.
132"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.1967032967032967,"Subsequently, we observe that the following color invariant response to the regions with high color
133"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.1978021978021978,"saturation, which is suitable to extract the illumination:
134"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.1989010989010989,"NŒªmxn =
‚àÇm+n‚àí1"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.2,"‚àÇŒªm‚àí1‚àÇxn {
1
E(Œª, x)
‚àÇE(Œª, x)"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.2010989010989011,"‚àÇŒª
},
(5)"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.2021978021978022,"This invariant has the following characteristics:
135"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.2032967032967033,"NŒªmxn =
‚àÇm+n‚àí2"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.2043956043956044,"‚àÇŒªm‚àí1‚àÇxn‚àí1
‚àÇ
‚àÇx"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.20549450549450549,"
1
E(Œª, x)
‚àÇE(Œª, x) ‚àÇŒª "
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.20659340659340658,"=
‚àÇm+n‚àí2"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.2076923076923077,"‚àÇŒªm‚àí1‚àÇxn‚àí1
‚àÇ
‚àÇx"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.2087912087912088,"
1
e(Œª, x)
‚àÇe(Œª, x)"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.2098901098901099,"‚àÇŒª
+
1
R(Œª)C(x)
‚àÇR(Œª)C(x) ‚àÇŒª "
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.210989010989011,"=
‚àÇm+n‚àí1"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.21208791208791208,‚àÇŒªm‚àí1‚àÇxn
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.21318681318681318,"
1
e(Œª, x)
‚àÇe(Œª, x) ‚àÇŒª 
. (6)"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.21428571428571427,"Equation (5) to Equation (6) demonstrate that the invariant NŒªmxn captures the features only related
136"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.2153846153846154,"to illumination e(Œª, x). Consequently, we assert that NŒªmxn functions as a light effects detector
137"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.2164835164835165,"because light effects are mainly related to the illumination. It allows us to design the illumination
138"
PHYSICAL PRIORS FOR NIGHTTIME ENVIRONMENT,0.2175824175824176,"disentanglement module based on this physical prior.
139"
DEGRADATION DISENTANGLEMENT MODULE,0.21868131868131868,"3.2
Degradation Disentanglement Module
140"
DEGRADATION DISENTANGLEMENT MODULE,0.21978021978021978,"In this subsection, we will elucidate how to incorporate the invariant for extracting light effects into
141"
DEGRADATION DISENTANGLEMENT MODULE,0.22087912087912087,"the disentanglement in computation. As common practice, the following second and third-order
142"
DEGRADATION DISENTANGLEMENT MODULE,0.22197802197802197,"components, both horizontally and vertically, are taken into account in the practical calculation of the
143"
DEGRADATION DISENTANGLEMENT MODULE,0.2230769230769231,"final invariant, which is denoted as N:
144 N =
q"
DEGRADATION DISENTANGLEMENT MODULE,0.22417582417582418,"N 2
Œªx + N 2
ŒªŒªx + N 2
Œªy + N 2
ŒªŒªy.
(7)"
DEGRADATION DISENTANGLEMENT MODULE,0.22527472527472528,"here NŒªx and NŒªŒªx can be computed through E(Œª, x) by simplifying Equation (5). The calculation
145"
DEGRADATION DISENTANGLEMENT MODULE,0.22637362637362637,"of NŒªy and NŒªŒªy are the same. Specifically,
146"
DEGRADATION DISENTANGLEMENT MODULE,0.22747252747252747,NŒªx = EŒªxE ‚àíEŒªEx
DEGRADATION DISENTANGLEMENT MODULE,0.22857142857142856,"E2
, NŒªŒªx
= EŒªŒªxE2 ‚àíEŒªŒªExE ‚àí2EŒªxEŒªE + 2E2
ŒªEx
E3
,
(8)"
DEGRADATION DISENTANGLEMENT MODULE,0.22967032967032966,"where Ex and EŒª denote the partial derivatives of x and Œª.
147"
DEGRADATION DISENTANGLEMENT MODULE,0.23076923076923078,"To compute each component in the invariant N, we develop a computation scheme starting with the
148"
DEGRADATION DISENTANGLEMENT MODULE,0.23186813186813188,"estimation of E and its partial derivatives EŒª and EŒªŒª using the Gaussian color model:
149"
DEGRADATION DISENTANGLEMENT MODULE,0.23296703296703297,""" E(x, y)
EŒª(x, y)
EŒªŒª(x, y) # ="
DEGRADATION DISENTANGLEMENT MODULE,0.23406593406593407,"""
0.06,
0.63,
0.27
0.3,
0.04,
‚àí0.35
0.34,
‚àí0.6,
0.17"
DEGRADATION DISENTANGLEMENT MODULE,0.23516483516483516,"# ""R(x, y)
G(x, y)
B(x, y) # ,
(9)"
DEGRADATION DISENTANGLEMENT MODULE,0.23626373626373626,"where x, y are pixel locations of the image. Then, the spatial derivatives Ex and Ey are calculated by
150"
DEGRADATION DISENTANGLEMENT MODULE,0.23736263736263735,"convolving E with Gaussian derivative kernel g and standard deviation œÉ:
151"
DEGRADATION DISENTANGLEMENT MODULE,0.23846153846153847,"Ex(x, y, œÉ) =
X"
DEGRADATION DISENTANGLEMENT MODULE,0.23956043956043957,"t‚ààZ
E(t, y)‚àÇg(x ‚àít, œÉ)"
DEGRADATION DISENTANGLEMENT MODULE,0.24065934065934066,"‚àÇx
,
(10)"
DEGRADATION DISENTANGLEMENT MODULE,0.24175824175824176,"where t denotes the index of the horizontal component x and Z represents set of integers. The spatial
152"
DEGRADATION DISENTANGLEMENT MODULE,0.24285714285714285,"derivatives for EŒªx and EŒªŒªx are obtained by applying Equation (10) to EŒª and EŒªŒª. Then invariant
153"
DEGRADATION DISENTANGLEMENT MODULE,0.24395604395604395,"N can be obtained following Equation (8) and Equation (7).
154"
DEGRADATION DISENTANGLEMENT MODULE,0.24505494505494504,"To extract the light effects, ReLU and normalization functions are first applied to filter out minor
155"
DEGRADATION DISENTANGLEMENT MODULE,0.24615384615384617,"disturbances. Then, by filtering invariant N with the well-lit mask Mn, we obtain the light effects
156"
DEGRADATION DISENTANGLEMENT MODULE,0.24725274725274726,"from the well-lit regions. The operations above can be formulated as:
157"
DEGRADATION DISENTANGLEMENT MODULE,0.24835164835164836,Mle = ReLU(N ‚àí¬µ(N)
DEGRADATION DISENTANGLEMENT MODULE,0.24945054945054945,"œÉ(N)
) ‚äôMn,
(11)"
DEGRADATION DISENTANGLEMENT MODULE,0.25054945054945055,"while the well-lit mask are refined: Mn ‚ÜêMn ‚àíMle.
158"
DEGRADATION DISENTANGLEMENT MODULE,0.25164835164835164,"With the initial disentanglement in Section 3.1, we obtain the final disentanglement: Md, Mn, Mh
159"
DEGRADATION DISENTANGLEMENT MODULE,0.25274725274725274,"and Mle. All the masks are stacked to obtain the disentanglement map. Through the employment of
160"
DEGRADATION DISENTANGLEMENT MODULE,0.25384615384615383,"the aforementioned techniques and processes, we successfully achieve the disentanglement of various
161"
DEGRADATION DISENTANGLEMENT MODULE,0.2549450549450549,"degradation regions.
162"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.256043956043956,"3.3
Degradation-Aware Contrastive Learning
163"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2571428571428571,"For unpaired image translation, contrastive learning has validated its effectiveness for the preservation
164"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.25824175824175827,"of content. It targets to maximize the mutual information between patches in the same spatial location
165"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.25934065934065936,"from the generated image and the source image as below:
166"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.26043956043956046,"‚Ñì(v, v+, v‚àí) = ‚àílog
exp(v ¬∑ v+/œÑ)"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.26153846153846155,"exp(v ¬∑ v+/œÑ) + PQ
n=1 exp(v ¬∑ v‚àí
n /œÑ)
,
(12)"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.26263736263736265,"v is the anchor that denotes the patch from the generated image. The positive example v+ corresponds
167"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.26373626373626374,"to the source image patch with the same location as the anchor v. The negative examples v‚àírepresent
168"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.26483516483516484,"patches with locations distinct from that of the anchor v. Q denotes the total number of negative
169"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.26593406593406593,"examples. In our work, the key insight of degradation-aware contrastive learning lies in two folds: (1)
170"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.26703296703296703,"How to sample the anchor, positive, and negative examples. (2) How to manage the focus on different
171"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2681318681318681,"negative examples.
172"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2692307692307692,"Degradation-Aware Sampling. In this paper, N2D3 selects the anchor, positive, and negative patches
173"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2703296703296703,"under the guidance of the disentanglement results. Initially, based on the disentanglement mask
174"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2714285714285714,"obtained in the Section 3.2, we compute the patch count for different degradation types, denoting as
175"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2725274725274725,"Ks, s ‚àà[1, 4]. Then, within each degradation region, the anchors v are randomly selected from the
176"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.27362637362637365,"patches of generated daytime images IN ‚ÜíD. The positive examples v+ are sampled from the same
177"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.27472527472527475,"locations with the anchors in the source nighttime images IN , and the negative examples v‚àíare
178"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.27582417582417584,"randomly selected from other locations of IN . For each anchor, there is one corresponding positive
179"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.27692307692307694,"example and Ks negative examples. Subsequently, the sample set with the same degradation type
180"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.27802197802197803,"will be assigned weights and the contrastive loss will be computed in the following steps.
181"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.27912087912087913,"Degradation-Aware Reweighting. Despite the careful selection of anchor, positive, and negative
182"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2802197802197802,"examples, the importance of anchor-negative pairs still differs within the same degradation. A known
183"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2813186813186813,"principle of designing contrastive learning is that the hard anchor-negative pairs (i.e., the pairs with
184"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2824175824175824,"high similarity) should assign higher attention. Thus, weighted contrastive learning can be formulated
185"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2835164835164835,"as:
186"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2846153846153846,"‚Ñì(v, v+, v‚àí, wn) = ‚àílog
exp(v ¬∑ v+/œÑ)"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2857142857142857,"exp(v ¬∑ v+/œÑ) + PQ
n=1 wn exp(v ¬∑ v‚àí
n /œÑ)
,
(13)"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2868131868131868,"wn denotes the weight of the n-th anchor-negative pairs.
187"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2879120879120879,"The contrastive objective is depicted in the Similarity Matrix in Figure 2. The patches in different
188"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.289010989010989,"regions are obviously easy examples. We suppress their weights to 0, which transforms the similarity
189"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.29010989010989013,"matrix into a blocked diagonal matrix with diag(A1, . . . , A4). Within each degradation matrix
190"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.29120879120879123,"As, s ‚àà[1, 4], a soft reweighting strategy is implemented. Specifically, for each anchor-negative
191"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2923076923076923,"pair, we apply optimal transport to yield an optimal transport plan, serving as a reweighting matrix
192"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2934065934065934,"associated with the disentangled results. It can adaptively optimize and avoid manual design. The
193"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2945054945054945,"reweight matrix for each degradation type is formulated as:
194"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2956043956043956,"min
wij,i,j‚àà[1,Ks][ Ks
X i=1 Ks
X"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2967032967032967,"j=1,iÃ∏=j
wij ¬∑ exp (vi ¬∑ v‚àí
j /œÑ)], Ks
X"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2978021978021978,"i=1
wij = 1, Ks
X"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.2989010989010989,"j=1
wij = 1, i, j ‚àà[1, Ks], (14)"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.3,"The aforementioned operations transform the contrastive objective to the Block Diagonal Similarity
195"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.3010989010989011,"Matrix depicted in Figure 2. As a common practice, our degradation-aware contrastive loss is applied
196"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.3021978021978022,"to the S layers of the CNN feature extractor, formulated as:
197"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.3032967032967033,"LDegNCE(F) = S
X"
DEGRADATION-AWARE CONTRASTIVE LEARNING,0.30439560439560437,"l=1
‚Ñì(v, v+, v‚àí, wn).
(15)"
OTHER REGULARIZATIONS,0.3054945054945055,"3.4
Other Regularizations
198"
OTHER REGULARIZATIONS,0.3065934065934066,"As a common practice, GANs are employed to bridge the domain gap between daytime and nighttime.
199"
OTHER REGULARIZATIONS,0.3076923076923077,"The adversarial loss is formulated as:
200"
OTHER REGULARIZATIONS,0.3087912087912088,"Ladv(F) = ||D(IN ‚ÜíD) ‚àí1||2
2,"
OTHER REGULARIZATIONS,0.3098901098901099,"Ladv(D) = ||D(ID) ‚àí1||2
2 + ||D(IN ‚ÜíD)||2
2,
(16)"
OTHER REGULARIZATIONS,0.310989010989011,"where D denotes the discriminator network. The final loss function is formatted as :
201"
OTHER REGULARIZATIONS,0.3120879120879121,"L(F) = Ladv(F) + LDegNCE(F),
L(D) = Ladv(D).
(17)"
EXPERIMENTS,0.3131868131868132,"4
Experiments
202"
EXPERIMENTAL SETTINGS,0.3142857142857143,"4.1
Experimental Settings
203"
EXPERIMENTAL SETTINGS,0.3153846153846154,"Datasets. Experiments are conducted on the two public datasets BDD100K [29] and Alderley [19].
204"
EXPERIMENTAL SETTINGS,0.31648351648351647,"Alderley dataset consists of images captured along the same route twice: once on a sunny day and
205"
EXPERIMENTAL SETTINGS,0.31758241758241756,"another time during a stormy rainy night. The nighttime images in this dataset are often blurry due to
206"
EXPERIMENTAL SETTINGS,0.31868131868131866,"the rainy conditions, which makes Night2Day challenging. BDD100K dataset is a large-scale high-
207"
EXPERIMENTAL SETTINGS,0.31978021978021975,"resolution autonomous driving dataset. It comprises 100,000 video clips under various conditions.
208"
EXPERIMENTAL SETTINGS,0.3208791208791209,"For each video, a keyframe is selected and meticulously annotated with details. We reorganized this
209"
EXPERIMENTAL SETTINGS,0.321978021978022,"dataset based on its annotations, resulting in 27,971 night images for training and 3,929 night images
210"
EXPERIMENTAL SETTINGS,0.3230769230769231,"for evaluation.
211"
EXPERIMENTAL SETTINGS,0.3241758241758242,"Evaluation Metric. Following common practice, we utilize the Fr√©chet Inception Distance (FID)
212"
EXPERIMENTAL SETTINGS,0.3252747252747253,"scores [7] to assess whether the generated images align with the target distribution. This assessment
213"
EXPERIMENTAL SETTINGS,0.3263736263736264,"helps determine if a model effectively transforms images from the night domain to the day domain.
214"
EXPERIMENTAL SETTINGS,0.3274725274725275,"Additionally, we seek to understand the extent to which the generated daytime images maintain
215"
EXPERIMENTAL SETTINGS,0.32857142857142857,"structural consistency compared to the original inputs. To measure this, we employ SIFT scores,
216"
EXPERIMENTAL SETTINGS,0.32967032967032966,"mIoU scores and LPIPS distance [32].
217"
EXPERIMENTAL SETTINGS,0.33076923076923076,"DownStream Vision Task. Two downstream tasks are conducted. In the Alderley dataset, GPS
218"
EXPERIMENTAL SETTINGS,0.33186813186813185,"annotations indicate the locations of two images, one in the nighttime and the other in the daytime,
219"
EXPERIMENTAL SETTINGS,0.33296703296703295,"as the same. We calculate the number of SIFT-detected key points between the generated daytime
220"
EXPERIMENTAL SETTINGS,0.33406593406593404,"images and their corresponding daytime images to measure if the two images represent the same
221"
EXPERIMENTAL SETTINGS,0.33516483516483514,"location. The BDD100K dataset includes 329 night images with semantic annotations. We employ
222"
EXPERIMENTAL SETTINGS,0.3362637362637363,"Deeplabv3 pretrained on the Cityscapes dataset as the semantic segmentation model [2], then perform
223"
EXPERIMENTAL SETTINGS,0.3373626373626374,"inference on our generated daytime images without any additional training and compute the mIoU
224"
EXPERIMENTAL SETTINGS,0.3384615384615385,"(mean Intersection over Union).
225"
EXPERIMENTAL SETTINGS,0.3395604395604396,"Table 1: The quantitative results on Alderley and BDD100k. ‚Üìmeans lower result is better. ‚Üëmeans
higher is better."
EXPERIMENTAL SETTINGS,0.34065934065934067,"Dataset
Alderley
BDD100k
Methods
FID‚Üì
LPIPS‚Üì
SIFT‚Üë
FID‚Üì
LPIPS‚Üì
mIoU‚Üë
Original
Conf./Jour.
210
-
3.12
101
-
15.63
CycleGAN[34]
ICCV 2017
167
0.706
3.36
51.7
0.477
13.42
StarGAN[3]
CVPR 2018
117
-
3.28
68.3
-
-
ToDayGAN[1]
ICRA 2019
104
0.770
4.14
43.8
0.577
16.77
UGATIT[15]
ICLR 2020
170
-
2.51
72.2
-
-
CUT[20]
ECCV 2020
64.7
0.707
6.78
55.5
0.583
9.30
ForkGAN[33]
ECCV 2020
61.2
0.759
12.1
37.6
0.581
11.81
AUGAN[18]
BMVC 2021
65.2
-
-
38.6
-
-
MoNCE[31]
CVPR 2022
72.7
0.737
6.35
40.2
0.502
17.21
Decent[27]
NIPS 2022
76.5
0.768
6.31
40.3
0.582
10.49
Santa[28]
CVPR 2023
67.1
0.757
6.93
36.9
0.559
11.03
N2D-LPNet[5]
CVPR 2023
-
-
-
69.1
-
-
EnlightenGAN [13]
TIP 2021
209.8
-
2.00
103.5
-
16.10
Zero-DCE [6]
TPAMI 2022
246.4
-
4.34
90.5
-
15.90
DeLight [21]
ECCV 2022
222.9
-
3.07
113.8
-
14.48
LLformer [23]
AAAI 2023
275.6
-
7.62
123.1
-
15.28
WCDM [12]
ToG 2023
239.6
-
7.10
124.3
-
16.32
GSAD [8]
NIPS 2023
214.7
-
6.29
116.0
-
15.76
N2D3(Ours)
-
50.9
0.650
16.62
31.5
0.466
21.58"
RESULTS ON ALDERLEY,0.34175824175824177,"4.2
Results on Alderley
226"
RESULTS ON ALDERLEY,0.34285714285714286,"We first apply Night2Day on the Alderley dataset, a challenging collection of nighttime images
227"
RESULTS ON ALDERLEY,0.34395604395604396,"captured on rainy nights. In Figure 4, we present a visual comparison of the results. CycleGAN [34]
228"
RESULTS ON ALDERLEY,0.34505494505494505,"and CUT [20] manage to preserve the general structural information of the entire image but often
229"
RESULTS ON ALDERLEY,0.34615384615384615,"lose many fine details. ToDayGAN [1], ForkGAN [33], Decent [27], and Santa [28] tend to miss
230"
RESULTS ON ALDERLEY,0.34725274725274724,"important elements such as cars in their results.
231"
RESULTS ON ALDERLEY,0.34835164835164834,"In Table 1, thirteen translation methods and three enhancement methods are compared, considering
232"
RESULTS ON ALDERLEY,0.34945054945054943,"both visual effects and keypoint matching metrics. Our method showcases an improvement of 10.3
233"
RESULTS ON ALDERLEY,0.3505494505494505,"Real Night
CycleGAN
ForkGAN
ToDayGAN"
RESULTS ON ALDERLEY,0.3516483516483517,"N2D3 (ours)
Decent
Santa
CUT"
RESULTS ON ALDERLEY,0.35274725274725277,Figure 4: The qualitative comparison results on the Alderley dataset.
RESULTS ON ALDERLEY,0.35384615384615387,"Real Night
CycleGAN
ForkGAN
ToDayGAN"
RESULTS ON ALDERLEY,0.35494505494505496,"N2D3 (ours)
Decent
Santa
CUT"
RESULTS ON ALDERLEY,0.35604395604395606,Figure 5: The qualitative comparison results on the BDD100K dataset.
RESULTS ON ALDERLEY,0.35714285714285715,"in FID scores and 4.52 in SIFT scores compared to the previous state-of-the-art. This suggests that
234"
RESULTS ON ALDERLEY,0.35824175824175825,"N2D3 successfully achieves photorealistic daytime image generation, underscoring its potential for
235"
RESULTS ON ALDERLEY,0.35934065934065934,"robotic localization applications. The qualitative comparison results are demonstrated in Figure 4. In
236"
RESULTS ON ALDERLEY,0.36043956043956044,"conclusion, N2D3 achieves top scores in both FID and LPIPS metrics, demonstrating its superiority
237"
RESULTS ON ALDERLEY,0.36153846153846153,"in the Night2Day task. N2D3 excels in generating photorealistic daytime images while effectively
238"
RESULTS ON ALDERLEY,0.3626373626373626,"preserving structures, even in challenging scenarios such as rainy nights in the Alderley.
239"
RESULTS ON ALDERLEY,0.3637362637362637,"4.3
Results on BDD100K
240"
RESULTS ON ALDERLEY,0.3648351648351648,"We conducted experiments on a larger-scale dataset, BDD100K, focusing on more general night
241"
RESULTS ON ALDERLEY,0.3659340659340659,"scenes. The qualitative results can be found in Figure 5. CycleGAN, ToDayGAN, and CUT succeed
242"
RESULTS ON ALDERLEY,0.367032967032967,"in preserving the structure in well-lit regions. ForkGAN, Santa, and Decent demonstrate poor
243"
RESULTS ON ALDERLEY,0.36813186813186816,"performance in such challenging scenes. Regretfully, none of them excel in handling light effects and
244"
RESULTS ON ALDERLEY,0.36923076923076925,"exhibit weak performance in maintaining global structures. With a customized design specifically
245"
RESULTS ON ALDERLEY,0.37032967032967035,"addressing light effects, our method successfully preserves the structure in all regions.
246"
RESULTS ON ALDERLEY,0.37142857142857144,"The quantitative results are presented in Table 1. As the scale of the dataset increases, all the
247"
RESULTS ON ALDERLEY,0.37252747252747254,"compared methods show an improvement in their performance. Notably, N2D3 demonstrates the best
248"
RESULTS ON ALDERLEY,0.37362637362637363,"performance with a significant improvement of 5.4 in FID scores, showcasing its ability to handle a
249"
RESULTS ON ALDERLEY,0.3747252747252747,"broader range of nighttime scenes and establishing itself as the most advanced method in this domain.
250"
RESULTS ON ALDERLEY,0.3758241758241758,"We also investigate the potential of Night2Day in enhancing downstream vision tasks in nighttime
251"
RESULTS ON ALDERLEY,0.3769230769230769,"environments using the BDD100K dataset. The quantitative results are summarized in Table 1.
252"
RESULTS ON ALDERLEY,0.378021978021978,"The enhancement methods demonstrate a slight improvement in segmentation results, while some
253"
RESULTS ON ALDERLEY,0.3791208791208791,"image-to-image translation methods have a negative impact on performance. N2D3 exhibits the best
254"
RESULTS ON ALDERLEY,0.3802197802197802,"performance in enhancing nighttime semantic segmentation with a remarkable improvement of
255"
RESULTS ON ALDERLEY,0.3813186813186813,"5.95 in mIoU compared to inferring the segmentation model directly on nighttime images.
256"
RESULTS ON ALDERLEY,0.3824175824175824,"In conclusion, N2D3 achieves top scores in both FID and LPIPS metrics, establishing itself as the
257"
RESULTS ON ALDERLEY,0.38351648351648354,"most advanced method for the Night2Day task. It excels in generating photorealistic daytime images
258"
RESULTS ON ALDERLEY,0.38461538461538464,"while preserving local and global structures. Moreover, the substantial improvement in nighttime
259"
RESULTS ON ALDERLEY,0.38571428571428573,"semantic segmentation highlights its benefits for downstream tasks and its potential for wide-ranging
260"
RESULTS ON ALDERLEY,0.3868131868131868,"applications.
261 50.8 45.4 31.5"
RESULTS ON ALDERLEY,0.3879120879120879,"35.4
36.7 65.0 59.4 50.9 58.2 53.9 20 30 40 50 60 70"
RESULTS ON ALDERLEY,0.389010989010989,"64
128
256
512
1024 FID"
RESULTS ON ALDERLEY,0.3901098901098901,num samping
RESULTS ON ALDERLEY,0.3912087912087912,"BDD100K
Alderley"
RESULTS ON ALDERLEY,0.3923076923076923,"0.582
0.579 0.466"
RESULTS ON ALDERLEY,0.3934065934065934,"0.508
0.491"
RESULTS ON ALDERLEY,0.3945054945054945,"0.681
0.673
0.650 0.685 0.739 0.4 0.5 0.6 0.7 0.8"
RESULTS ON ALDERLEY,0.3956043956043956,"64
128
256
512
1024 LPIPS"
RESULTS ON ALDERLEY,0.3967032967032967,num samping
RESULTS ON ALDERLEY,0.3978021978021978,"BDD100K
Alderley"
RESULTS ON ALDERLEY,0.3989010989010989,"(a) Ablation in FID
(b) Ablation in LPIPS"
RESULTS ON ALDERLEY,0.4,"Figure 6: The quantitative results of ablation on the number of patches of the degradation-aware
sampling.
Table 2: The quantitative results of ablation on the main component of degradation-aware con-
trastive learning. (a) denotes the degradation-aware sampling, and (b) denotes the degradation-aware
reweighting. L and N denotes the invariant types."
RESULTS ON ALDERLEY,0.4010989010989011,"Main Component
BDD100K
Alderley
(a)
(b)
FID
LPIPS
FID
LPIPS
SIFT
%
%
55.5
0.583
64.7
0.707
6.78
!
%
36.9
0.495
56.6
0.698
16.52
!
!
31.5
0.466
50.9
0.650
16.62"
RESULTS ON ALDERLEY,0.4021978021978022,"Invariant Type
BDD100K
Alderley
L
N
FID
LPIPS
FID
LPIPS
SIFT
%
%
55.5
0.583
64.7
0.707
6.78
!
%
49.1
0.592
62.9
0.726
9.83
!
!
31.5
0.466
50.9
0.650
16.62"
ABLATION STUDY,0.4032967032967033,"4.4
Ablation Study
262"
ABLATION STUDY,0.4043956043956044,"Ablation on the main component of degradation-aware contrastive learning. The core design of
263"
ABLATION STUDY,0.4054945054945055,"the degradation-aware contrastive learning module relies on two main components: (a) degradation-
264"
ABLATION STUDY,0.4065934065934066,"aware sampling, and (b) degradation-aware reweighting. As shown in Table 2, when degradation-
265"
ABLATION STUDY,0.4076923076923077,"aware sampling is exclusively activated, there is a noticeable decrease in FID on both datasets
266"
ABLATION STUDY,0.4087912087912088,"compared to the baseline (no components activated). Notably, the combination of degradation-aware
267"
ABLATION STUDY,0.4098901098901099,"sampling and reweighting achieves the lowest FID on both BDD100K and Alderley, indicating the
268"
ABLATION STUDY,0.41098901098901097,"effectiveness of degradation-aware sampling in conjunction with degradation-aware reweighting.
269"
ABLATION STUDY,0.41208791208791207,"Ablation on the number of patches in the degradation-aware sampling. To explore the impact
270"
ABLATION STUDY,0.41318681318681316,"of the number of sampling patches in our method, we conduct an ablation study on the number of
271"
ABLATION STUDY,0.4142857142857143,"sampling patches with settings of 64, 128, 256, 512, and 1024 for degradation-aware sampling. The
272"
ABLATION STUDY,0.4153846153846154,"FID and LPIPS scores are evaluated, as shown in Figure 6. The optimal performance is achieved with
273"
ABLATION STUDY,0.4164835164835165,"256 patches, and increasing the number of sampling patches beyond this point leads to a degradation
274"
ABLATION STUDY,0.4175824175824176,"in performance.
275"
ABLATION STUDY,0.4186813186813187,"Ablation on the type of the invariant in disentanglement. To explore different invariants for
276"
ABLATION STUDY,0.4197802197802198,"obtaining degradation-disentangled prototypes, we conduct an ablation study on the type of invariant.
277"
ABLATION STUDY,0.4208791208791209,"As shown in Table 2, when L is enabled, the FID decreases from 55.5 to 49.1 on BDD100K and
278"
ABLATION STUDY,0.421978021978022,"from 64.7 to 62.9 on Alderley. This suggests that incorporating illuminance maps helps in reducing
279"
ABLATION STUDY,0.4230769230769231,"the perceptual gap between generated and source nighttime images. When N is activated, there
280"
ABLATION STUDY,0.42417582417582417,"is a consistent improvement in FID on both datasets, indicating that considering physical priors
281"
ABLATION STUDY,0.42527472527472526,"invariant contributes to more realistic image generation. The combination of both illuminance map
282"
ABLATION STUDY,0.42637362637362636,"and physical prior invariant results in the lowest FID on both datasets, showcasing the complementary
283"
ABLATION STUDY,0.42747252747252745,"nature of these degradation types in improving contrastive learning.
284"
CONCLUSION,0.42857142857142855,"5
Conclusion
285"
CONCLUSION,0.42967032967032964,"This paper introduces a novel solution for the Night2Day image translation task, focusing on trans-
286"
CONCLUSION,0.4307692307692308,"lating nighttime images to their corresponding daytime counterparts while preserving semantic
287"
CONCLUSION,0.4318681318681319,"consistency. To achieve this objective, the proposed method begins by disentangling the degradation
288"
CONCLUSION,0.432967032967033,"presented in nighttime images, which is the key insight of our method. To achieve this, we contribute
289"
CONCLUSION,0.4340659340659341,"a degradation disentanglement module and a degradation-aware contrastive learning module. Our
290"
CONCLUSION,0.4351648351648352,"method outperforms the existing state-of-the-art, which shows the effectiveness of N2D3 and the
291"
CONCLUSION,0.43626373626373627,"superiority of the insight to disentangle the degradation.
292"
REFERENCES,0.43736263736263736,"References
293"
REFERENCES,0.43846153846153846,"[1] Asha Anoosheh, Torsten Sattler, Radu Timofte, Marc Pollefeys, and Luc Van Gool. Night-to-day
294"
REFERENCES,0.43956043956043955,"image translation for retrieval-based localization. In 2019 International Conference on Robotics
295"
REFERENCES,0.44065934065934065,"and Automation (ICRA), pages 5958‚Äì5964. IEEE, 2019.
296"
REFERENCES,0.44175824175824174,"[2] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous
297"
REFERENCES,0.44285714285714284,"convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.
298"
REFERENCES,0.44395604395604393,"[3] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo.
299"
REFERENCES,0.44505494505494503,"Stargan: Unified generative adversarial networks for multi-domain image-to-image translation.
300"
REFERENCES,0.4461538461538462,"In Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages 8789‚Äì8797,
301"
REFERENCES,0.4472527472527473,"2018.
302"
REFERENCES,0.44835164835164837,"[4] T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE Transactions on Information
303"
REFERENCES,0.44945054945054946,"Theory, 13(1):21‚Äì27, 1967.
304"
REFERENCES,0.45054945054945056,"[5] Zhentao Fan, Xianhao Wu, Xiang Chen, and Yufeng Li. Learning to see in nighttime driving
305"
REFERENCES,0.45164835164835165,"scenes with inter-frequency priors. In Proceedings of the IEEE/CVF Conference on Computer
306"
REFERENCES,0.45274725274725275,"Vision and Pattern Recognition, pages 4217‚Äì4224, 2023.
307"
REFERENCES,0.45384615384615384,"[6] Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and
308"
REFERENCES,0.45494505494505494,"Runmin Cong. Zero-reference deep curve estimation for low-light image enhancement. In Proc.
309"
REFERENCES,0.45604395604395603,"IEEE Conference on Computer Vision and Pattern Recognition, pages 1780‚Äì1789, 2020.
310"
REFERENCES,0.45714285714285713,"[7] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
311"
REFERENCES,0.4582417582417582,"Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
312"
REFERENCES,0.4593406593406593,"Neural Information Processing Systems, 30, 2017.
313"
REFERENCES,0.4604395604395604,"[8] Jinhui Hou, Zhiyu Zhu, Junhui Hou, Hui Liu, Huanqiang Zeng, and Hui Yuan. Global structure-
314"
REFERENCES,0.46153846153846156,"aware diffusion process for low-light image enhancement. Advances in Neural Information
315"
REFERENCES,0.46263736263736266,"Processing Systems, 36, 2024.
316"
REFERENCES,0.46373626373626375,"[9] Xueqi Hu, Xinyue Zhou, Qiusheng Huang, Zhengyi Shi, Li Sun, and Qingli Li. Qs-attn: Query-
317"
REFERENCES,0.46483516483516485,"selected attention for contrastive learning in i2i translation. In Proceedings of the IEEE/CVF
318"
REFERENCES,0.46593406593406594,"Conference on Computer Vision and Pattern Recognition, pages 18291‚Äì18300, 2022.
319"
REFERENCES,0.46703296703296704,"[10] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with
320"
REFERENCES,0.46813186813186813,"conditional adversarial networks. In Proc. IEEE Conference on Computer Vision and Pattern
321"
REFERENCES,0.46923076923076923,"Recognition, pages 1125‚Äì1134, 2017.
322"
REFERENCES,0.4703296703296703,"[11] Somi Jeong, Youngjung Kim, Eungbean Lee, and Kwanghoon Sohn. Memory-guided unsuper-
323"
REFERENCES,0.4714285714285714,"vised image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer
324"
REFERENCES,0.4725274725274725,"Vision and Pattern Recognition, pages 6558‚Äì6567, 2021.
325"
REFERENCES,0.4736263736263736,"[12] Hai Jiang, Ao Luo, Haoqiang Fan, Songchen Han, and Shuaicheng Liu. Low-light image
326"
REFERENCES,0.4747252747252747,"enhancement with wavelet-based diffusion models. ACM Transactions on Graphics (TOG),
327"
REFERENCES,0.4758241758241758,"42(6):1‚Äì14, 2023.
328"
REFERENCES,0.47692307692307695,"[13] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan
329"
REFERENCES,0.47802197802197804,"Zhou, and Zhangyang Wang. Enlightengan: Deep light enhancement without paired supervision.
330"
REFERENCES,0.47912087912087914,"IEEE Transactions on Image Processing, 30:2340‚Äì2349, 2021.
331"
REFERENCES,0.48021978021978023,"[14] Mikhail Kennerley, Jian-Gang Wang, Bharadwaj Veeravalli, and Robby T Tan. 2pcnet: Two-
332"
REFERENCES,0.48131868131868133,"phase consistency training for day-to-night unsupervised domain adaptive object detection. In
333"
REFERENCES,0.4824175824175824,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
334"
REFERENCES,0.4835164835164835,"11484‚Äì11493, 2023.
335"
REFERENCES,0.4846153846153846,"[15] Junho Kim, Minjae Kim, Hyeonwoo Kang, and Kwanghee Lee. U-gat-it: Unsupervised
336"
REFERENCES,0.4857142857142857,"generative attentional networks with adaptive layer-instance normalization for image-to-image
337"
REFERENCES,0.4868131868131868,"translation. arXiv preprint arXiv:1907.10830, 2019.
338"
REFERENCES,0.4879120879120879,"[16] Soohyun Kim, Jongbeom Baek, Jihye Park, Gyeongnyeon Kim, and Seungryong Kim. In-
339"
REFERENCES,0.489010989010989,"staformer: Instance-aware image-to-image translation with transformer. In Proceedings of
340"
REFERENCES,0.4901098901098901,"the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18321‚Äì18331,
341"
REFERENCES,0.4912087912087912,"2022.
342"
REFERENCES,0.49230769230769234,"[17] Paul Kubelka. Ein beitrag zur optik der farbanstriche (contribution to the optic of paint).
343"
REFERENCES,0.49340659340659343,"Zeitschrift fur technische Physik, 12:593‚Äì601, 1931.
344"
REFERENCES,0.4945054945054945,"[18] Jeong-gi Kwak, Youngsaeng Jin, Yuanming Li, Dongsik Yoon, Donghyeon Kim, and Hanseok
345"
REFERENCES,0.4956043956043956,"Ko. Adverse weather image translation with asymmetric and uncertainty-aware gan. arXiv
346"
REFERENCES,0.4967032967032967,"preprint arXiv:2112.04283, 2021.
347"
REFERENCES,0.4978021978021978,"[19] Michael J. Milford and Gordon. F. Wyeth. Seqslam: Visual route-based navigation for sunny
348"
REFERENCES,0.4989010989010989,"summer days and stormy winter nights. In 2012 IEEE International Conference on Robotics
349"
REFERENCES,0.5,"and Automation, pages 1643‚Äì1649, 2012.
350"
REFERENCES,0.5010989010989011,"[20] Taesung Park, Alexei Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired
351"
REFERENCES,0.5021978021978022,"image-to-image translation. In European Conference on Computer Vision, pages 319‚Äì345,
352"
REFERENCES,0.5032967032967033,"2020.
353"
REFERENCES,0.5043956043956044,"[21] Aashish Sharma and Robby T Tan. Nighttime visibility enhancement by increasing the dynamic
354"
REFERENCES,0.5054945054945055,"range and suppression of light effects. In Proceedings of the IEEE/CVF Conference on Computer
355"
REFERENCES,0.5065934065934066,"Vision and Pattern Recognition, pages 11977‚Äì11986, 2021.
356"
REFERENCES,0.5076923076923077,"[22] Seokbeom Song, Suhyeon Lee, Hongje Seong, Kyoungwon Min, and Euntai Kim. Shunit: Style
357"
REFERENCES,0.5087912087912088,"harmonization for unpaired image-to-image translation. Proceedings of the AAAI Conference
358"
REFERENCES,0.5098901098901099,"on Artificial Intelligence, 37(2):2292‚Äì2302, Jun. 2023.
359"
REFERENCES,0.510989010989011,"[23] Tao Wang, Kaihao Zhang, Tianrun Shen, Wenhan Luo, Bjorn Stenger, and Tong Lu. Ultra-
360"
REFERENCES,0.512087912087912,"high-definition low-light image enhancement: A benchmark and transformer-based method. In
361"
REFERENCES,0.5131868131868131,"Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 2654‚Äì2662,
362"
REFERENCES,0.5142857142857142,"2023.
363"
REFERENCES,0.5153846153846153,"[24] Weilun Wang, Wengang Zhou, Jianmin Bao, Dong Chen, and Houqiang Li. Instance-wise hard
364"
REFERENCES,0.5164835164835165,"negative example generation for contrastive learning in unpaired image-to-image translation.
365"
REFERENCES,0.5175824175824176,"In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14020‚Äì
366"
REFERENCES,0.5186813186813187,"14029, 2021.
367"
REFERENCES,0.5197802197802198,"[25] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:
368"
REFERENCES,0.5208791208791209,"from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600‚Äì
369"
REFERENCES,0.521978021978022,"612, 2004.
370"
REFERENCES,0.5230769230769231,"[26] Youya Xia, Josephine Monica, Wei-Lun Chao, Bharath Hariharan, Kilian Q Weinberger, and
371"
REFERENCES,0.5241758241758242,"Mark Campbell. Image-to-image translation for autonomous driving from coarsely-aligned
372"
REFERENCES,0.5252747252747253,"image pairs. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages
373"
REFERENCES,0.5263736263736264,"7756‚Äì7762. IEEE, 2023.
374"
REFERENCES,0.5274725274725275,"[27] Shaoan Xie, Qirong Ho, and Kun Zhang. Unsupervised image-to-image translation with density
375"
REFERENCES,0.5285714285714286,"changing regularization. In Advances in Neural Information Processing Systems, 2022.
376"
REFERENCES,0.5296703296703297,"[28] Shaoan Xie, Yanwu Xu, Mingming Gong, and Kun Zhang. Unpaired image-to-image translation
377"
REFERENCES,0.5307692307692308,"with shortest path regularization. In Proceedings of the IEEE/CVF Conference on Computer
378"
REFERENCES,0.5318681318681319,"Vision and Pattern Recognition (CVPR), pages 10177‚Äì10187, June 2023.
379"
REFERENCES,0.532967032967033,"[29] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht
380"
REFERENCES,0.5340659340659341,"Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous mul-
381"
REFERENCES,0.5351648351648352,"titask learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern
382"
REFERENCES,0.5362637362637362,"recognition, pages 2636‚Äì2645, 2020.
383"
REFERENCES,0.5373626373626373,"[30] Zhenjie Yu, Shuang Li, Yirui Shen, Chi Harold Liu, and Shuigen Wang. On the difficulty of
384"
REFERENCES,0.5384615384615384,"unpaired infrared-to-visible video translation: Fine-grained content-rich patches transfer. In
385"
REFERENCES,0.5395604395604395,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
386"
REFERENCES,0.5406593406593406,"pages 1631‚Äì1640, June 2023.
387"
REFERENCES,0.5417582417582417,"[31] Fangneng Zhan, Jiahui Zhang, Yingchen Yu, Rongliang Wu, and Shijian Lu. Modulated contrast
388"
REFERENCES,0.5428571428571428,"for versatile image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision
389"
REFERENCES,0.5439560439560439,"and Pattern Recognition (CVPR), pages 18280‚Äì18290, June 2022.
390"
REFERENCES,0.545054945054945,"[32] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unrea-
391"
REFERENCES,0.5461538461538461,"sonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE
392"
REFERENCES,0.5472527472527473,"conference on computer vision and pattern recognition, pages 586‚Äì595, 2018.
393"
REFERENCES,0.5483516483516484,"[33] Ziqiang Zheng, Yang Wu, Xinran Han, and Jianbo Shi. Forkgan: Seeing into the rainy night. In
394"
REFERENCES,0.5494505494505495,"European conference on computer vision, pages 155‚Äì170. Springer, 2020.
395"
REFERENCES,0.5505494505494506,"[34] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired image-to-image transla-
396"
REFERENCES,0.5516483516483517,"tion using cycle-consistent adversarial networks. In Proc. IEEE International Conference on
397"
REFERENCES,0.5527472527472528,"Computer Vision, pages 2223‚Äì2232, 2017.
398"
REFERENCES,0.5538461538461539,"A
Overview
399"
REFERENCES,0.554945054945055,"This supplementary material is organized as follows. Appendix B provides additional details about
400"
REFERENCES,0.5560439560439561,"the proof that the invariant NŒªmxn is exclusively related to the illumination. Appendix C outlines the
401"
REFERENCES,0.5571428571428572,"limitations and failure case of N2D3. Appendix D illustrates the implementation details, including
402"
REFERENCES,0.5582417582417583,"N2D3 and other methods used in the experiments. Appendix E presents additional visualization
403"
REFERENCES,0.5593406593406594,"results.
404"
REFERENCES,0.5604395604395604,"B
More Proof Details
405"
REFERENCES,0.5615384615384615,"We provide a detailed proof process to demonstrate how the invariant NŒªmxn is exclusively related
406"
REFERENCES,0.5626373626373626,"to the illumination and can function as the light effect detector. First, consider the following
407"
REFERENCES,0.5637362637362637,"equations, corresponding to Equation (5) in the main paper:
408"
REFERENCES,0.5648351648351648,"NŒªmxn =
‚àÇm+n‚àí2"
REFERENCES,0.5659340659340659,"‚àÇŒªm‚àí1‚àÇxn‚àí1
‚àÇ
‚àÇx{
1
E(Œª, x)
‚àÇE(Œª, x) ‚àÇŒª
}"
REFERENCES,0.567032967032967,"=
‚àÇm+n‚àí2"
REFERENCES,0.5681318681318681,"‚àÇŒªm‚àí1‚àÇxn‚àí1
‚àÇ
‚àÇx{
1
e(Œª, x)
‚àÇe(Œª, x)"
REFERENCES,0.5692307692307692,"‚àÇŒª
+
1
R(Œª)C(x)
‚àÇR(Œª)C(x)"
REFERENCES,0.5703296703296703,"‚àÇŒª
},
(18)"
REFERENCES,0.5714285714285714,"by applying the additivity of linear differential operators, the first term represents the invariants only
409"
REFERENCES,0.5725274725274725,"related to the illumination. The second term can be simplified by applying the chain rule as follows:
410"
REFERENCES,0.5736263736263736,"‚àÇ
‚àÇx{
1
R(Œª)C(x)
‚àÇR(Œª)C(x) ‚àÇŒª
}"
REFERENCES,0.5747252747252747,"=
1
R(Œª)2C(x)2 (‚àÇ2{R(Œª)C(x)}"
REFERENCES,0.5758241758241758,"‚àÇŒª‚àÇx
¬∑ R(Œª)C(x) ‚àí‚àÇ{R(Œª)C(x)}"
REFERENCES,0.5769230769230769,"‚àÇŒª
¬∑ ‚àÇ{R(Œª)C(x)} ‚àÇx
)"
REFERENCES,0.578021978021978,"=
1
R(Œª)2C(x)2 (‚àÇR(Œª)"
REFERENCES,0.5791208791208792,"‚àÇŒª
‚àÇC(x)"
REFERENCES,0.5802197802197803,"‚àÇx
¬∑ R(Œª)C(x) ‚àí‚àÇR(Œª)"
REFERENCES,0.5813186813186814,"‚àÇŒª
C(x) ¬∑ R(Œª)‚àÇC(x)"
REFERENCES,0.5824175824175825,"‚àÇx
) = 0. (19)"
REFERENCES,0.5835164835164836,"Finally, we conclude that the invariant NŒªmxn is exclusively related to the illumination and can be
411"
REFERENCES,0.5846153846153846,"formulated as follows:
412"
REFERENCES,0.5857142857142857,"NŒªmxn =
‚àÇm+n‚àí2"
REFERENCES,0.5868131868131868,"‚àÇŒªm‚àí1‚àÇxn‚àí1
‚àÇ
‚àÇx{
1
E(Œª, x)
‚àÇE(Œª, x) ‚àÇŒª
}"
REFERENCES,0.5879120879120879,"=
‚àÇm+n‚àí1"
REFERENCES,0.589010989010989,"‚àÇŒªm‚àí1‚àÇxn {
1
e(Œª, x)
‚àÇe(Œª, x)"
REFERENCES,0.5901098901098901,"‚àÇŒª
}.
(20)"
REFERENCES,0.5912087912087912,Figure 7: Failure Cases of N2D3: Our method struggles to handle various other types of degradation.
REFERENCES,0.5923076923076923,"C
Limitations and Failure Case
413"
REFERENCES,0.5934065934065934,"Despite the superior performance of N2D3 in Night2Day, it still exhibits certain limitations. On the
414"
REFERENCES,0.5945054945054945,"one hand, this work focuses solely on addressing light degradation, while nighttime environments
415"
REFERENCES,0.5956043956043956,"encompass various other types of degradation, including blur caused by rain, motion, and other
416"
REFERENCES,0.5967032967032967,"Figure 8: More disentanglement results. The first and third rows display nighttime images, while
the second and fourth rows show the corresponding degradation disentanglement results. The color
progression from blue, light blue, green to yellow corresponds to the following regions: darkness,
well-lit, light effects, and high-light."
REFERENCES,0.5978021978021978,"w/o N
w/o reweighting
N2D3 (ours)
Real Night"
REFERENCES,0.5989010989010989,Figure 9: Qualitative comparison abalation results.
REFERENCES,0.6,"factors. Our method currently struggles to handle these situations effectively. On the other hand, the
417"
REFERENCES,0.6010989010989011,"limitations of visible imaging in night vision arise from the scarcity of photos captured in low-light
418"
REFERENCES,0.6021978021978022,"conditions, as illustrated by the failure cases presented inFigure 7. Future advancements in night
419"
REFERENCES,0.6032967032967033,"vision will likely incorporate additional modalities, such as infrared images, radar, and other sensor
420"
REFERENCES,0.6043956043956044,"data, to overcome these challenges and improve performance.
421"
REFERENCES,0.6054945054945055,"D
Implementation Details
422"
REFERENCES,0.6065934065934065,"Training Details. We adopt the resnet 9blocks, a ResNetbased model with nine residual blocks, as
423"
REFERENCES,0.6076923076923076,"the backbone for generator G. Additionally, we utilize the patch-wise discriminator D following
424"
REFERENCES,0.6087912087912087,"PatchGAN[10]. To conduct degradation-aware contrastive learning on multiple layers, we extract
425"
REFERENCES,0.6098901098901099,"features from 5 layers of the generator G encoder, as done in [20]. These layers include RGB pixels,
426"
REFERENCES,0.610989010989011,"the first and second downsampling convolution, and the first and fifth residual block. For the features
427"
REFERENCES,0.6120879120879121,"of each layer, we apply a 2-layer MLP to acquire final 256-dimensional features. These features are
428"
REFERENCES,0.6131868131868132,"then utilized in our degradation-aware contrastive learning.
429"
REFERENCES,0.6142857142857143,"All the comparison methods are reproduced using their released source code with default settings.
430"
REFERENCES,0.6153846153846154,"Training procedures are consistent across all methods. All models are trained using the Adaptive
431"
REFERENCES,0.6164835164835165,"Moment Estimation optimizer with an initial learning rate of 10‚àí4, a momentum of 0.9, and weight
432"
REFERENCES,0.6175824175824176,"decay of 10‚àí4. For the BDD100K dataset, training consists of 10 epochs with the initial learning
433"
REFERENCES,0.6186813186813187,"rate, followed by another 10 epochs with a decreased learning rate using the polynomial annealing
434"
REFERENCES,0.6197802197802198,"procedure with a power of 0.9. On the Alderley dataset, given the limited training data compared
435"
REFERENCES,0.6208791208791209,"to BDD100K, we extend the training to 20 epochs with the initial learning rate and an additional
436"
REFERENCES,0.621978021978022,"CUT
N2D3 (ours)
Decent
Santa"
REFERENCES,0.6230769230769231,"Real Night
CycleGAN
ForkGAN
ToDayGAN"
REFERENCES,0.6241758241758242,Figure 10: More qualitative comparison results on the Alderley dataset.
REFERENCES,0.6252747252747253,"20 epochs with the decayed learning rate. All the experiments are run on a single A100 GPU with
437"
REFERENCES,0.6263736263736264,"80GB of memory. Training our method with a smaller patch size and batch size on a device with less
438"
REFERENCES,0.6274725274725275,"memory is feasible.
439"
REFERENCES,0.6285714285714286,"Evaluation Details. In the evaluation, we compute the Fr√©chet Inception Distance (FID) [7],
440"
REFERENCES,0.6296703296703297,"Structural Similarity Index (SSIM) [25], and Learned Perceptual Image Patch Similarity (LPIPS)
441"
REFERENCES,0.6307692307692307,"[32] scores on 256 √ó 512 images. Partial FID scores are provided by ForkGAN [33], and all SSIM
442"
REFERENCES,0.6318681318681318,"and LPIPS scores are reproduced by us.
443"
REFERENCES,0.6329670329670329,"Semantic segmentation evaluation are conducted as follows. First, we use Deeplabv3 pretrained
444"
REFERENCES,0.634065934065934,"on the Cityscapes dataset as the semantic segmentation model [2]. The model is provided by
445"
REFERENCES,0.6351648351648351,"https://github.com/open-mmlab/mmsegmentation with an R-18-D8 backbone and trained at
446"
REFERENCES,0.6362637362637362,"a resolution of 512 √ó 1024. Second, we perform 512 √ó 1024 Night2Day translation to obtain the
447"
REFERENCES,0.6373626373626373,"generation results. Finally, we infer the semantic segmentation on the generated daytime images.
448"
REFERENCES,0.6384615384615384,"E
More Visualization Results
449"
REFERENCES,0.6395604395604395,"More Ablation Visualization Results. We provide ablation visualization results on both Alderley
450"
REFERENCES,0.6406593406593407,"and BDD100K in Figure 9. The complete method is presented along with ablation studies on the
451"
REFERENCES,0.6417582417582418,"invariant N and without degradation-aware reweighting. All the modules contribute to improving the
452"
REFERENCES,0.6428571428571429,"ability to maintain semantic consistency.
453"
REFERENCES,0.643956043956044,"More Disentanglement Results. We provide additional disentanglement results in Figure 8. Our
454"
REFERENCES,0.6450549450549451,"disentanglement methods offer a comprehensive representation of different illumination degradation
455"
REFERENCES,0.6461538461538462,"types in various nighttime scenes.
456"
REFERENCES,0.6472527472527473,"More Qualitative Comparison. We present more qualitative comparisons in Figure 10 and Figure 11
457"
REFERENCES,0.6483516483516484,"alongside other methods.Our method demonstrates visually pleasing results under various nighttime
458"
REFERENCES,0.6494505494505495,"conditions.
459"
REFERENCES,0.6505494505494506,"Real Night
CycleGAN
ToDayGAN
ForkGAN"
REFERENCES,0.6516483516483517,"CUT
N2D3 (ours)
Decent
Santa"
REFERENCES,0.6527472527472528,Figure 11: More qualitative comparison results on the BDD100K dataset.
REFERENCES,0.6538461538461539,"NeurIPS Paper Checklist
460"
CLAIMS,0.654945054945055,"1. Claims
461"
CLAIMS,0.656043956043956,"Question: Do the main claims made in the abstract and introduction accurately reflect the
462"
CLAIMS,0.6571428571428571,"paper‚Äôs contributions and scope?
463"
CLAIMS,0.6582417582417582,"Answer: [Yes]
464"
CLAIMS,0.6593406593406593,"Justification: We claim our main contribution as N2D3, which achieves SOTA performance
465"
CLAIMS,0.6604395604395604,"by bridging the domain gap between nighttime and daytime in a degradation-aware manner.
466"
CLAIMS,0.6615384615384615,"Guidelines:
467"
CLAIMS,0.6626373626373626,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
468"
CLAIMS,0.6637362637362637,"made in the paper.
469"
CLAIMS,0.6648351648351648,"‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
470"
CLAIMS,0.6659340659340659,"contributions made in the paper and important assumptions and limitations. A No or
471"
CLAIMS,0.667032967032967,"NA answer to this question will not be perceived well by the reviewers.
472"
CLAIMS,0.6681318681318681,"‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
473"
CLAIMS,0.6692307692307692,"much the results can be expected to generalize to other settings.
474"
CLAIMS,0.6703296703296703,"‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
475"
CLAIMS,0.6714285714285714,"are not attained by the paper.
476"
LIMITATIONS,0.6725274725274726,"2. Limitations
477"
LIMITATIONS,0.6736263736263737,"Question: Does the paper discuss the limitations of the work performed by the authors?
478"
LIMITATIONS,0.6747252747252748,"Answer: [Yes]
479"
LIMITATIONS,0.6758241758241759,"Justification: We discuss our limitation in degradations beyond light and low-light image
480"
LIMITATIONS,0.676923076923077,"scarcity in the appendix.
481"
LIMITATIONS,0.6780219780219781,"Guidelines:
482"
LIMITATIONS,0.6791208791208792,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
483"
LIMITATIONS,0.6802197802197802,"the paper has limitations, but those are not discussed in the paper.
484"
LIMITATIONS,0.6813186813186813,"‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
485"
LIMITATIONS,0.6824175824175824,"‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
486"
LIMITATIONS,0.6835164835164835,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
487"
LIMITATIONS,0.6846153846153846,"model well-specification, asymptotic approximations only holding locally). The authors
488"
LIMITATIONS,0.6857142857142857,"should reflect on how these assumptions might be violated in practice and what the
489"
LIMITATIONS,0.6868131868131868,"implications would be.
490"
LIMITATIONS,0.6879120879120879,"‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
491"
LIMITATIONS,0.689010989010989,"only tested on a few datasets or with a few runs. In general, empirical results often
492"
LIMITATIONS,0.6901098901098901,"depend on implicit assumptions, which should be articulated.
493"
LIMITATIONS,0.6912087912087912,"‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
494"
LIMITATIONS,0.6923076923076923,"For example, a facial recognition algorithm may perform poorly when image resolution
495"
LIMITATIONS,0.6934065934065934,"is low or images are taken in low lighting. Or a speech-to-text system might not be
496"
LIMITATIONS,0.6945054945054945,"used reliably to provide closed captions for online lectures because it fails to handle
497"
LIMITATIONS,0.6956043956043956,"technical jargon.
498"
LIMITATIONS,0.6967032967032967,"‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
499"
LIMITATIONS,0.6978021978021978,"and how they scale with dataset size.
500"
LIMITATIONS,0.6989010989010989,"‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
501"
LIMITATIONS,0.7,"address problems of privacy and fairness.
502"
LIMITATIONS,0.701098901098901,"‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
503"
LIMITATIONS,0.7021978021978021,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
504"
LIMITATIONS,0.7032967032967034,"limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
505"
LIMITATIONS,0.7043956043956044,"judgment and recognize that individual actions in favor of transparency play an impor-
506"
LIMITATIONS,0.7054945054945055,"tant role in developing norms that preserve the integrity of the community. Reviewers
507"
LIMITATIONS,0.7065934065934066,"will be specifically instructed to not penalize honesty concerning limitations.
508"
THEORY ASSUMPTIONS AND PROOFS,0.7076923076923077,"3. Theory Assumptions and Proofs
509"
THEORY ASSUMPTIONS AND PROOFS,0.7087912087912088,"Question: For each theoretical result, does the paper provide the full set of assumptions and
510"
THEORY ASSUMPTIONS AND PROOFS,0.7098901098901099,"a complete (and correct) proof?
511"
THEORY ASSUMPTIONS AND PROOFS,0.710989010989011,"Answer: [Yes]
512"
THEORY ASSUMPTIONS AND PROOFS,0.7120879120879121,"Justification: We provide the full set of assumptions and complete proofs in both Section 3.1
513"
THEORY ASSUMPTIONS AND PROOFS,0.7131868131868132,"and Appendix B .
514"
THEORY ASSUMPTIONS AND PROOFS,0.7142857142857143,"Guidelines:
515"
THEORY ASSUMPTIONS AND PROOFS,0.7153846153846154,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
516"
THEORY ASSUMPTIONS AND PROOFS,0.7164835164835165,"‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
517"
THEORY ASSUMPTIONS AND PROOFS,0.7175824175824176,"referenced.
518"
THEORY ASSUMPTIONS AND PROOFS,0.7186813186813187,"‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
519"
THEORY ASSUMPTIONS AND PROOFS,0.7197802197802198,"‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
520"
THEORY ASSUMPTIONS AND PROOFS,0.7208791208791209,"they appear in the supplemental material, the authors are encouraged to provide a short
521"
THEORY ASSUMPTIONS AND PROOFS,0.721978021978022,"proof sketch to provide intuition.
522"
THEORY ASSUMPTIONS AND PROOFS,0.7230769230769231,"‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
523"
THEORY ASSUMPTIONS AND PROOFS,0.7241758241758242,"by formal proofs provided in appendix or supplemental material.
524"
THEORY ASSUMPTIONS AND PROOFS,0.7252747252747253,"‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
525"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7263736263736263,"4. Experimental Result Reproducibility
526"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7274725274725274,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
527"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7285714285714285,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
528"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7296703296703296,"of the paper (regardless of whether the code and data are provided or not)?
529"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7307692307692307,"Answer: [Yes]
530"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7318681318681318,"Justification: All the information needed to reproduce the main experimental results is
531"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7329670329670329,"included in the Section 3 and Appendix D.
532"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.734065934065934,"Guidelines:
533"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7351648351648352,"‚Ä¢ The answer NA means that the paper does not include experiments.
534"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7362637362637363,"‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
535"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7373626373626374,"well by the reviewers: Making the paper reproducible is important, regardless of
536"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7384615384615385,"whether the code and data are provided or not.
537"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7395604395604396,"‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
538"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7406593406593407,"to make their results reproducible or verifiable.
539"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7417582417582418,"‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
540"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7428571428571429,"For example, if the contribution is a novel architecture, describing the architecture fully
541"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.743956043956044,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
542"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7450549450549451,"be necessary to either make it possible for others to replicate the model with the same
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7461538461538462,"dataset, or provide access to the model. In general. releasing code and data is often
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7472527472527473,"one good way to accomplish this, but reproducibility can also be provided via detailed
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7483516483516484,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7494505494505495,"of a large language model), releasing of a model checkpoint, or other means that are
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7505494505494505,"appropriate to the research performed.
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7516483516483516,"‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7527472527472527,"sions to provide some reasonable avenue for reproducibility, which may depend on the
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7538461538461538,"nature of the contribution. For example
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7549450549450549,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.756043956043956,"to reproduce that algorithm.
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7571428571428571,"(b) If the contribution is primarily a new model architecture, the paper should describe
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7582417582417582,"the architecture clearly and fully.
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7593406593406593,"(c) If the contribution is a new model (e.g., a large language model), then there should
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7604395604395604,"either be a way to access this model for reproducing the results or a way to reproduce
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7615384615384615,"the model (e.g., with an open-source dataset or instructions for how to construct
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7626373626373626,"the dataset).
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7637362637362637,"(d) We recognize that reproducibility may be tricky in some cases, in which case
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7648351648351648,"authors are welcome to describe the particular way they provide for reproducibility.
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.765934065934066,"In the case of closed-source models, it may be that access to the model is limited in
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7670329670329671,"some way (e.g., to registered users), but it should be possible for other researchers
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7681318681318682,"to have some path to reproducing or verifying the results.
564"
OPEN ACCESS TO DATA AND CODE,0.7692307692307693,"5. Open access to data and code
565"
OPEN ACCESS TO DATA AND CODE,0.7703296703296704,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
566"
OPEN ACCESS TO DATA AND CODE,0.7714285714285715,"tions to faithfully reproduce the main experimental results, as described in supplemental
567"
OPEN ACCESS TO DATA AND CODE,0.7725274725274726,"material?
568"
OPEN ACCESS TO DATA AND CODE,0.7736263736263737,"Answer: [No]
569"
OPEN ACCESS TO DATA AND CODE,0.7747252747252747,"Justification: Code will be released latter.
570"
OPEN ACCESS TO DATA AND CODE,0.7758241758241758,"Guidelines:
571"
OPEN ACCESS TO DATA AND CODE,0.7769230769230769,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
572"
OPEN ACCESS TO DATA AND CODE,0.778021978021978,"‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
573"
OPEN ACCESS TO DATA AND CODE,0.7791208791208791,"public/guides/CodeSubmissionPolicy) for more details.
574"
OPEN ACCESS TO DATA AND CODE,0.7802197802197802,"‚Ä¢ While we encourage the release of code and data, we understand that this might not be
575"
OPEN ACCESS TO DATA AND CODE,0.7813186813186813,"possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
576"
OPEN ACCESS TO DATA AND CODE,0.7824175824175824,"including code, unless this is central to the contribution (e.g., for a new open-source
577"
OPEN ACCESS TO DATA AND CODE,0.7835164835164835,"benchmark).
578"
OPEN ACCESS TO DATA AND CODE,0.7846153846153846,"‚Ä¢ The instructions should contain the exact command and environment needed to run to
579"
OPEN ACCESS TO DATA AND CODE,0.7857142857142857,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
580"
OPEN ACCESS TO DATA AND CODE,0.7868131868131868,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
581"
OPEN ACCESS TO DATA AND CODE,0.7879120879120879,"‚Ä¢ The authors should provide instructions on data access and preparation, including how
582"
OPEN ACCESS TO DATA AND CODE,0.789010989010989,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
583"
OPEN ACCESS TO DATA AND CODE,0.7901098901098901,"‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
584"
OPEN ACCESS TO DATA AND CODE,0.7912087912087912,"proposed method and baselines. If only a subset of experiments are reproducible, they
585"
OPEN ACCESS TO DATA AND CODE,0.7923076923076923,"should state which ones are omitted from the script and why.
586"
OPEN ACCESS TO DATA AND CODE,0.7934065934065934,"‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
587"
OPEN ACCESS TO DATA AND CODE,0.7945054945054945,"versions (if applicable).
588"
OPEN ACCESS TO DATA AND CODE,0.7956043956043956,"‚Ä¢ Providing as much information as possible in supplemental material (appended to the
589"
OPEN ACCESS TO DATA AND CODE,0.7967032967032966,"paper) is recommended, but including URLs to data and code is permitted.
590"
OPEN ACCESS TO DATA AND CODE,0.7978021978021979,"6. Experimental Setting/Details
591"
OPEN ACCESS TO DATA AND CODE,0.798901098901099,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
592"
OPEN ACCESS TO DATA AND CODE,0.8,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
593"
OPEN ACCESS TO DATA AND CODE,0.8010989010989011,"results?
594"
OPEN ACCESS TO DATA AND CODE,0.8021978021978022,"Answer: [Yes]
595"
OPEN ACCESS TO DATA AND CODE,0.8032967032967033,"Justification: The training details and dataset information are provided in Section 4.
596"
OPEN ACCESS TO DATA AND CODE,0.8043956043956044,"Guidelines:
597"
OPEN ACCESS TO DATA AND CODE,0.8054945054945055,"‚Ä¢ The answer NA means that the paper does not include experiments.
598"
OPEN ACCESS TO DATA AND CODE,0.8065934065934066,"‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
599"
OPEN ACCESS TO DATA AND CODE,0.8076923076923077,"that is necessary to appreciate the results and make sense of them.
600"
OPEN ACCESS TO DATA AND CODE,0.8087912087912088,"‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
601"
OPEN ACCESS TO DATA AND CODE,0.8098901098901099,"material.
602"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.810989010989011,"7. Experiment Statistical Significance
603"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8120879120879121,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
604"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8131868131868132,"information about the statistical significance of the experiments?
605"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8142857142857143,"Answer: [No]
606"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8153846153846154,"Justification: Error bars are not reported because it would be too computationally expensive.
607"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8164835164835165,"We report our results using a fixed random seed.
608"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8175824175824176,"Guidelines:
609"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8186813186813187,"‚Ä¢ The answer NA means that the paper does not include experiments.
610"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8197802197802198,"‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
611"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8208791208791208,"dence intervals, or statistical significance tests, at least for the experiments that support
612"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8219780219780219,"the main claims of the paper.
613"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.823076923076923,"‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
614"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8241758241758241,"example, train/test split, initialization, random drawing of some parameter, or overall
615"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8252747252747252,"run with given experimental conditions).
616"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8263736263736263,"‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
617"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8274725274725274,"call to a library function, bootstrap, etc.)
618"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8285714285714286,"‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
619"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8296703296703297,"‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
620"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8307692307692308,"of the mean.
621"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8318681318681319,"‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
622"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.832967032967033,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
623"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8340659340659341,"of Normality of errors is not verified.
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8351648351648352,"‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8362637362637363,"figures symmetric error bars that would yield results that are out of range (e.g. negative
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8373626373626374,"error rates).
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8384615384615385,"‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8395604395604396,"they were calculated and reference the corresponding figures or tables in the text.
629"
EXPERIMENTS COMPUTE RESOURCES,0.8406593406593407,"8. Experiments Compute Resources
630"
EXPERIMENTS COMPUTE RESOURCES,0.8417582417582418,"Question: For each experiment, does the paper provide sufficient information on the com-
631"
EXPERIMENTS COMPUTE RESOURCES,0.8428571428571429,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
632"
EXPERIMENTS COMPUTE RESOURCES,0.843956043956044,"the experiments?
633"
EXPERIMENTS COMPUTE RESOURCES,0.845054945054945,"Answer: [Yes]
634"
EXPERIMENTS COMPUTE RESOURCES,0.8461538461538461,"Justification: We report the compute resources in Appendix D.
635"
EXPERIMENTS COMPUTE RESOURCES,0.8472527472527472,"Guidelines:
636"
EXPERIMENTS COMPUTE RESOURCES,0.8483516483516483,"‚Ä¢ The answer NA means that the paper does not include experiments.
637"
EXPERIMENTS COMPUTE RESOURCES,0.8494505494505494,"‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
638"
EXPERIMENTS COMPUTE RESOURCES,0.8505494505494505,"or cloud provider, including relevant memory and storage.
639"
EXPERIMENTS COMPUTE RESOURCES,0.8516483516483516,"‚Ä¢ The paper should provide the amount of compute required for each of the individual
640"
EXPERIMENTS COMPUTE RESOURCES,0.8527472527472527,"experimental runs as well as estimate the total compute.
641"
EXPERIMENTS COMPUTE RESOURCES,0.8538461538461538,"‚Ä¢ The paper should disclose whether the full research project required more compute
642"
EXPERIMENTS COMPUTE RESOURCES,0.8549450549450549,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
643"
EXPERIMENTS COMPUTE RESOURCES,0.856043956043956,"didn‚Äôt make it into the paper).
644"
CODE OF ETHICS,0.8571428571428571,"9. Code Of Ethics
645"
CODE OF ETHICS,0.8582417582417582,"Question: Does the research conducted in the paper conform, in every respect, with the
646"
CODE OF ETHICS,0.8593406593406593,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
647"
CODE OF ETHICS,0.8604395604395605,"Answer: [Yes]
648"
CODE OF ETHICS,0.8615384615384616,"Justification: The research conducted in this paper conforms, in every respect, with the
649"
CODE OF ETHICS,0.8626373626373627,"NeurIPS Code of Ethics.
650"
CODE OF ETHICS,0.8637362637362638,"Guidelines:
651"
CODE OF ETHICS,0.8648351648351649,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
652"
CODE OF ETHICS,0.865934065934066,"‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
653"
CODE OF ETHICS,0.8670329670329671,"deviation from the Code of Ethics.
654"
CODE OF ETHICS,0.8681318681318682,"‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
655"
CODE OF ETHICS,0.8692307692307693,"eration due to laws or regulations in their jurisdiction).
656"
BROADER IMPACTS,0.8703296703296703,"10. Broader Impacts
657"
BROADER IMPACTS,0.8714285714285714,"Question: Does the paper discuss both potential positive societal impacts and negative
658"
BROADER IMPACTS,0.8725274725274725,"societal impacts of the work performed?
659"
BROADER IMPACTS,0.8736263736263736,"Answer: [Yes]
660"
BROADER IMPACTS,0.8747252747252747,"Justification: The societal impacts are discussed in the manuscript and appendix.
661"
BROADER IMPACTS,0.8758241758241758,"Guidelines:
662"
BROADER IMPACTS,0.8769230769230769,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
663"
BROADER IMPACTS,0.878021978021978,"‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
664"
BROADER IMPACTS,0.8791208791208791,"impact or why the paper does not address societal impact.
665"
BROADER IMPACTS,0.8802197802197802,"‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
666"
BROADER IMPACTS,0.8813186813186813,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
667"
BROADER IMPACTS,0.8824175824175824,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
668"
BROADER IMPACTS,0.8835164835164835,"groups), privacy considerations, and security considerations.
669"
BROADER IMPACTS,0.8846153846153846,"‚Ä¢ The conference expects that many papers will be foundational research and not tied
670"
BROADER IMPACTS,0.8857142857142857,"to particular applications, let alone deployments. However, if there is a direct path to
671"
BROADER IMPACTS,0.8868131868131868,"any negative applications, the authors should point it out. For example, it is legitimate
672"
BROADER IMPACTS,0.8879120879120879,"to point out that an improvement in the quality of generative models could be used to
673"
BROADER IMPACTS,0.889010989010989,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
674"
BROADER IMPACTS,0.8901098901098901,"that a generic algorithm for optimizing neural networks could enable people to train
675"
BROADER IMPACTS,0.8912087912087913,"models that generate Deepfakes faster.
676"
BROADER IMPACTS,0.8923076923076924,"‚Ä¢ The authors should consider possible harms that could arise when the technology is
677"
BROADER IMPACTS,0.8934065934065935,"being used as intended and functioning correctly, harms that could arise when the
678"
BROADER IMPACTS,0.8945054945054945,"technology is being used as intended but gives incorrect results, and harms following
679"
BROADER IMPACTS,0.8956043956043956,"from (intentional or unintentional) misuse of the technology.
680"
BROADER IMPACTS,0.8967032967032967,"‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
681"
BROADER IMPACTS,0.8978021978021978,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
682"
BROADER IMPACTS,0.8989010989010989,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
683"
BROADER IMPACTS,0.9,"feedback over time, improving the efficiency and accessibility of ML).
684"
SAFEGUARDS,0.9010989010989011,"11. Safeguards
685"
SAFEGUARDS,0.9021978021978022,"Question: Does the paper describe safeguards that have been put in place for responsible
686"
SAFEGUARDS,0.9032967032967033,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
687"
SAFEGUARDS,0.9043956043956044,"image generators, or scraped datasets)?
688"
SAFEGUARDS,0.9054945054945055,"Answer: [NA]
689"
SAFEGUARDS,0.9065934065934066,"Justification: Our model does not have such risks, and all the datasets used in the experiments
690"
SAFEGUARDS,0.9076923076923077,"are open-source benchmarks in this field.
691"
SAFEGUARDS,0.9087912087912088,"Guidelines:
692"
SAFEGUARDS,0.9098901098901099,"‚Ä¢ The answer NA means that the paper poses no such risks.
693"
SAFEGUARDS,0.910989010989011,"‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
694"
SAFEGUARDS,0.9120879120879121,"necessary safeguards to allow for controlled use of the model, for example by requiring
695"
SAFEGUARDS,0.9131868131868132,"that users adhere to usage guidelines or restrictions to access the model or implementing
696"
SAFEGUARDS,0.9142857142857143,"safety filters.
697"
SAFEGUARDS,0.9153846153846154,"‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
698"
SAFEGUARDS,0.9164835164835164,"should describe how they avoided releasing unsafe images.
699"
SAFEGUARDS,0.9175824175824175,"‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
700"
SAFEGUARDS,0.9186813186813186,"not require this, but we encourage authors to take this into account and make a best
701"
SAFEGUARDS,0.9197802197802197,"faith effort.
702"
LICENSES FOR EXISTING ASSETS,0.9208791208791208,"12. Licenses for existing assets
703"
LICENSES FOR EXISTING ASSETS,0.921978021978022,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
704"
LICENSES FOR EXISTING ASSETS,0.9230769230769231,"the paper, properly credited and are the license and terms of use explicitly mentioned and
705"
LICENSES FOR EXISTING ASSETS,0.9241758241758242,"properly respected?
706"
LICENSES FOR EXISTING ASSETS,0.9252747252747253,"Answer: [Yes]
707"
LICENSES FOR EXISTING ASSETS,0.9263736263736264,"Justification: The code and data are properly credited, and the license and terms of use are
708"
LICENSES FOR EXISTING ASSETS,0.9274725274725275,"explicitly mentioned and properly documented.
709"
LICENSES FOR EXISTING ASSETS,0.9285714285714286,"Guidelines:
710"
LICENSES FOR EXISTING ASSETS,0.9296703296703297,"‚Ä¢ The answer NA means that the paper does not use existing assets.
711"
LICENSES FOR EXISTING ASSETS,0.9307692307692308,"‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
712"
LICENSES FOR EXISTING ASSETS,0.9318681318681319,"‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
713"
LICENSES FOR EXISTING ASSETS,0.932967032967033,"URL.
714"
LICENSES FOR EXISTING ASSETS,0.9340659340659341,"‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
715"
LICENSES FOR EXISTING ASSETS,0.9351648351648352,"‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
716"
LICENSES FOR EXISTING ASSETS,0.9362637362637363,"service of that source should be provided.
717"
LICENSES FOR EXISTING ASSETS,0.9373626373626374,"‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
718"
LICENSES FOR EXISTING ASSETS,0.9384615384615385,"package should be provided. For popular datasets, paperswithcode.com/datasets
719"
LICENSES FOR EXISTING ASSETS,0.9395604395604396,"has curated licenses for some datasets. Their licensing guide can help determine the
720"
LICENSES FOR EXISTING ASSETS,0.9406593406593406,"license of a dataset.
721"
LICENSES FOR EXISTING ASSETS,0.9417582417582417,"‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
722"
LICENSES FOR EXISTING ASSETS,0.9428571428571428,"the derived asset (if it has changed) should be provided.
723"
LICENSES FOR EXISTING ASSETS,0.9439560439560439,"‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
724"
LICENSES FOR EXISTING ASSETS,0.945054945054945,"the asset‚Äôs creators.
725"
NEW ASSETS,0.9461538461538461,"13. New Assets
726"
NEW ASSETS,0.9472527472527472,"Question: Are new assets introduced in the paper well documented and is the documentation
727"
NEW ASSETS,0.9483516483516483,"provided alongside the assets?
728"
NEW ASSETS,0.9494505494505494,"Answer: [Yes]
729"
NEW ASSETS,0.9505494505494505,"Justification: The code introduced in the paper is well-documented, and the documentation
730"
NEW ASSETS,0.9516483516483516,"is provided alongside it.
731"
NEW ASSETS,0.9527472527472527,"Guidelines:
732"
NEW ASSETS,0.9538461538461539,"‚Ä¢ The answer NA means that the paper does not release new assets.
733"
NEW ASSETS,0.954945054945055,"‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
734"
NEW ASSETS,0.9560439560439561,"submissions via structured templates. This includes details about training, license,
735"
NEW ASSETS,0.9571428571428572,"limitations, etc.
736"
NEW ASSETS,0.9582417582417583,"‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
737"
NEW ASSETS,0.9593406593406594,"asset is used.
738"
NEW ASSETS,0.9604395604395605,"‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
739"
NEW ASSETS,0.9615384615384616,"create an anonymized URL or include an anonymized zip file.
740"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9626373626373627,"14. Crowdsourcing and Research with Human Subjects
741"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9637362637362638,"Question: For crowdsourcing experiments and research with human subjects, does the paper
742"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9648351648351648,"include the full text of instructions given to participants and screenshots, if applicable, as
743"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9659340659340659,"well as details about compensation (if any)?
744"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967032967032967,"Answer: [NA]
745"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9681318681318681,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
746"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9692307692307692,"Guidelines:
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9703296703296703,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9714285714285714,"human subjects.
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9725274725274725,"‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9736263736263736,"tion of the paper involves human subjects, then as much detail as possible should be
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9747252747252747,"included in the main paper.
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758241758241758,"‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769230769230769,"or other labor should be paid at least the minimum wage in the country of the data
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978021978021978,"collector.
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791208791208791,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802197802197802,"Subjects
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813186813186813,"Question: Does the paper describe potential risks incurred by study participants, whether
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824175824175824,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835164835164835,"approvals (or an equivalent approval/review based on the requirements of your country or
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846153846153847,"institution) were obtained?
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9857142857142858,"Answer: [NA]
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868131868131869,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987912087912088,"Guidelines:
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.989010989010989,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9901098901098901,"human subjects.
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912087912087912,"‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923076923076923,"may be required for any human subjects research. If you obtained IRB approval, you
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934065934065934,"should clearly state this in the paper.
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945054945054945,"‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956043956043956,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967032967032967,"guidelines for their institution.
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978021978021978,"‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989010989010989,"applicable), such as the institution conducting the review.
774"
