Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001941747572815534,"Benefiting from the impressive diffusion models, conditional generative models
1"
ABSTRACT,0.003883495145631068,"have exhibited exceptional capabilities in various generation tasks, for example,
2"
ABSTRACT,0.005825242718446602,"image or short video generation based on text description. In this work, we
3"
ABSTRACT,0.007766990291262136,"focus on the task of generating a series of coherent images based on a given
4"
ABSTRACT,0.009708737864077669,"storyline, denoted as open-ended visual storytelling. We make the following three
5"
ABSTRACT,0.011650485436893204,"contributions: (i) to fulfill the task of visual storytelling, we introduce two modules
6"
ABSTRACT,0.013592233009708738,"into a pre-trained stable diffusion model, and construct an auto-regressive image
7"
ABSTRACT,0.015533980582524271,"generator, termed as StoryGen, that enables to generate the current frame by
8"
ABSTRACT,0.017475728155339806,"conditioning on a text prompt and preceding frame; (ii) to train our proposed
9"
ABSTRACT,0.019417475728155338,"model, we collect paired image and text samples by sourcing from various online
10"
ABSTRACT,0.021359223300970873,"sources, such as videos, E-books, and establish a data processing pipeline for
11"
ABSTRACT,0.02330097087378641,"constructing a diverse dataset, named StorySalon, with a far larger vocabulary
12"
ABSTRACT,0.02524271844660194,"than existing animation-specific datasets; (iii) we adopt a three-stage curriculum
13"
ABSTRACT,0.027184466019417475,"training strategy, that enables style transfer, visual context conditioning, and human
14"
ABSTRACT,0.02912621359223301,"feedback alignment, respectively. Quantitative experiments and human evaluation
15"
ABSTRACT,0.031067961165048542,"have validated the superiority of our proposed model, in terms of image quality,
16"
ABSTRACT,0.03300970873786408,"style consistency, content consistency, and visual-language alignment. We will
17"
ABSTRACT,0.03495145631067961,"make the code, model, and dataset publicly available to the research community.
18"
INTRODUCTION,0.036893203883495145,"1
Introduction
19"
INTRODUCTION,0.038834951456310676,"""Mirror, mirror, here I stand! Who is the fairest in the land?""
20"
INTRODUCTION,0.040776699029126215,"—- Grimms’ Fairy Tales
21"
INTRODUCTION,0.04271844660194175,"This paper explores an exciting yet challenging task of visual storytelling, with the goal of training
22"
INTRODUCTION,0.04466019417475728,"a model that can effectively capture the relationship between visual elements in images and their
23"
INTRODUCTION,0.04660194174757282,"corresponding language descriptions, to generate a sequence of images that tell a visual coherent
24"
INTRODUCTION,0.04854368932038835,"story. The ultimate goal is to generate a sequence of images that tell a coherent story, provided
25"
INTRODUCTION,0.05048543689320388,"in the form of natural language. The outcome of this task has significant potential for education,
26"
INTRODUCTION,0.05242718446601942,"providing children with an engaging and interactive way to learn complex visual concepts, and
27"
INTRODUCTION,0.05436893203883495,"develop imagination, creativity, emotional intelligence, and language skills, as evidenced by the
28"
INTRODUCTION,0.05631067961165048,"research in psychology [4, 41].
29"
INTRODUCTION,0.05825242718446602,"In the recent literature, there has been significant progress in image generation, particularly with
30"
INTRODUCTION,0.06019417475728155,"the guidance of text, such as stable diffusion [35], DALL·E [33] and Imagen [9]. However, these
31"
INTRODUCTION,0.062135922330097085,"models are not sufficient for visual storytelling for two reasons: (i) existing models generate images
32"
INTRODUCTION,0.06407766990291262,"independently without considering the context of previous frames or the overall narrative, resulting
33"
INTRODUCTION,0.06601941747572816,"in visual inconsistencies and a lack of coherence in the visual story; (ii) generating images by only
34"
INTRODUCTION,0.06796116504854369,"conditioning on text can lead to ambiguities or require unnecessarily long descriptions, particularly
35"
INTRODUCTION,0.06990291262135923,StoryGen
INTRODUCTION,0.07184466019417475,"(1) Once upon a time, 
there was a yellow dog, 
with ears that flopped and"
INTRODUCTION,0.07378640776699029,a tail that wagged…
INTRODUCTION,0.07572815533980583,(2) The yellow dog loved
INTRODUCTION,0.07766990291262135,"to run and play, jumping 
over fences and chasing"
INTRODUCTION,0.07961165048543689,after butterflies…
INTRODUCTION,0.08155339805825243,"(3) Curious, he followed 
his nose and it led him to a"
INTRODUCTION,0.08349514563106795,hidden cave in the
INTRODUCTION,0.0854368932038835,woods…
INTRODUCTION,0.08737864077669903,"(4) Inside the cave, the"
INTRODUCTION,0.08932038834951456,"yellow dog found a 
treasure chest filled with"
INTRODUCTION,0.0912621359223301,shiny gold coins and
INTRODUCTION,0.09320388349514563,sparkling jewels...
INTRODUCTION,0.09514563106796116,"(5) So, the yellow dog"
INTRODUCTION,0.0970873786407767,"took a few coins and 
jewels and ran to the"
INTRODUCTION,0.09902912621359224,nearby town…
INTRODUCTION,0.10097087378640776,(6) The mayor rewarded
INTRODUCTION,0.1029126213592233,"the yellow dog with a 
medal of honor and a 
lifetime supply of bones…"
INTRODUCTION,0.10485436893203884,"(7) From that day on, the 
yellow dog became a hero 
in the town. People would"
INTRODUCTION,0.10679611650485436,come to visit him…
INTRODUCTION,0.1087378640776699,"ChatGPT: OK. Here 
is a story: Once upon 
a time, there was a 
{yellow dog}…"
INTRODUCTION,0.11067961165048544,"User: Hey ChatGPT, 
please give me a story 
about a {yellow dog}."
INTRODUCTION,0.11262135922330097,"Figure 1: An illustration of open-ended visual storytelling. In practise, one user can prompt a large
larnguage model, for example, ChatGPT, to generate a unique and engaging story, which is then fed
into our proposed StoryGen model, to generate a sequence of images that are not only aligning to
the given storyline, but also coherent. We recommend the reader to zoom in and read the story."
INTRODUCTION,0.1145631067961165,"when dealing with subtle differences, for example, distinguishing between animals from the same
36"
INTRODUCTION,0.11650485436893204,"category but different breeds. To address the limitations, we introduce a novel auto-regressive
37"
INTRODUCTION,0.11844660194174757,"architecture, termed as StoryGen, that builds upon pre-trained stable diffusion model, with two extra
38"
INTRODUCTION,0.1203883495145631,"modules serving for style transfer and visual context conditioning. At inference time, StoryGen takes
39"
INTRODUCTION,0.12233009708737864,"the preceding frames and text prompts as conditions to synthesize the current frame, i.e., iteratively
40"
INTRODUCTION,0.12427184466019417,"creating visual sequences that are not only aligning to language description, but also coherent.
41"
INTRODUCTION,0.1262135922330097,"In practise, visual storytelling is faced with significant challenges due to the lack of high-quality
42"
INTRODUCTION,0.12815533980582525,"image-text data. Most existing works are limited to train on a few specific animations, for example,
43"
INTRODUCTION,0.13009708737864079,"StoryGAN [17], StoryDALL-E [22] and AR-LDM [28], resulting in a small and restricted vocabulary
44"
INTRODUCTION,0.13203883495145632,"and characters. To overcome this limitation, we have created a dataset called StorySalon, that features
45"
INTRODUCTION,0.13398058252427184,"a rich source of coherent images and stories, primarily comprising children’s storybooks collected
46"
INTRODUCTION,0.13592233009708737,"from three different sources: videos, E-books, and synthesized sample data from our StoryGen
47"
INTRODUCTION,0.1378640776699029,"model with human verification. As a result, our dataset includes a diverse vocabulary with different
48"
INTRODUCTION,0.13980582524271845,"characters, storylines, and artistic styles.
49"
INTRODUCTION,0.141747572815534,"We follow a three-stage training procedure: Firstly, we insert a LoRA-like architecture into text
50"
INTRODUCTION,0.1436893203883495,"conditioning module in stable diffusion model, i.e., on top of the image-to-text cross-attention, in
51"
INTRODUCTION,0.14563106796116504,"order to adapt the pre-trained diffusion model on our collected dataset; Secondly, we introduce a
52"
INTRODUCTION,0.14757281553398058,"visual context module that enables the generation process to condition on preceding image; Lastly,
53"
INTRODUCTION,0.14951456310679612,"we finetune the model with data after human verification, i.e., leveraging the feedback to further align
54"
INTRODUCTION,0.15145631067961166,"the model with human preference. As a result, the scale and diversity of our collected dataset enable
55"
INTRODUCTION,0.1533980582524272,"the model to acquire the ability of open-vocabulary visual storytelling, by that we mean, our model
56"
INTRODUCTION,0.1553398058252427,"can generate new image sequences that are not limited to pre-defined storylines, characters, or scenes.
57"
INTRODUCTION,0.15728155339805824,"For example, we can prompt a large language model to create unique and engaging stories, then feed
58"
INTRODUCTION,0.15922330097087378,"into StoryGen for generation, as shown in Figure 1.
59"
INTRODUCTION,0.16116504854368932,"To summarise, we make the following contributions in this paper: (i) we propose the task of open-
60"
INTRODUCTION,0.16310679611650486,"ended visual storytelling, that involves generating engaging image sequence that is aligning to a
61"
INTRODUCTION,0.1650485436893204,"given storyline, for example, written by a large language model; (ii) we develop a novel architecture
62"
INTRODUCTION,0.1669902912621359,"based on stable diffusion, termed as StoryGen, which can generate image sequences in an auto-
63"
INTRODUCTION,0.16893203883495145,"regressive manner, taking both preceding image and text prompt of current frame as condition;
64"
INTRODUCTION,0.170873786407767,"(iii) we initiate a data collection pipeline and collect a large-scale, diverse datasets of storybooks,
65"
INTRODUCTION,0.17281553398058253,"from online videos, E-books and synthesized samples, including paired image-text samples of a
66"
INTRODUCTION,0.17475728155339806,"diverse vocabulary with different characters, storylines, and artistic styles; (iv) we adopt a three-stage
67"
INTRODUCTION,0.1766990291262136,"curriculum training strategy, that enables style transfer, visual context conditioning, and human
68"
INTRODUCTION,0.1786407766990291,"feedback alignment, respectively. Experimentally, we conduct both quantitative comparison and
69"
INTRODUCTION,0.18058252427184465,"human evaluation, showing that the outputs from ou proposed model are more preferred, in terms of
70"
INTRODUCTION,0.1825242718446602,"image quality, style consistency, and image coherence.
71"
RELATED WORKS,0.18446601941747573,"2
Related Works
72"
RELATED WORKS,0.18640776699029127,"Diffusion Models learn to model a data distribution via iterative denoising and are trained with
73"
RELATED WORKS,0.1883495145631068,"denoising score matching. Drawing from the principles of Langevin dynamics and physical diffusion
74"
RELATED WORKS,0.19029126213592232,"processes, diffusion models have undergone refinement through a multitude of works [39, 27, 45].
75"
RELATED WORKS,0.19223300970873786,"Notably, DDPM [10] has demonstrated improved performance over other generative models, while
76"
RELATED WORKS,0.1941747572815534,"DDIM [40] has significantly boosted generation efficiency. In view of their superior generative
77"
RELATED WORKS,0.19611650485436893,"capabilities, diffusion models have found extensive utility in various downstream applications besides
78"
RELATED WORKS,0.19805825242718447,"image generation, such as video generation [46, 5, 12, 9, 38], image manipulation [2, 24, 14, 7],
79"
RELATED WORKS,0.2,"grounded generation [18], 3D texturing [34], and image inpainting [47, 26, 19, 1].
80"
RELATED WORKS,0.20194174757281552,"Text-to-image Generation involves the creation of images from textual descriptions. The task has
81"
RELATED WORKS,0.20388349514563106,"been tackled using various generative models, with Generative Adversarial Networks (GANs) [6]
82"
RELATED WORKS,0.2058252427184466,"being the first widely-used model. Several GAN-based architectures such as StackGAN [50],
83"
RELATED WORKS,0.20776699029126214,"StackGAN++ [51], and AttnGAN [48] have achieved notable success in this area. Additionally,
84"
RELATED WORKS,0.20970873786407768,"pre-trained auto-regressive transformers [43] such as DALL·E [33] have demonstrated the ability
85"
RELATED WORKS,0.21165048543689322,"to generate high-quality images in response to textual prompts. Recently, diffusion models have
86"
RELATED WORKS,0.21359223300970873,"emerged as a popular approach to text-to-image generation. New images can be sampled under text
87"
RELATED WORKS,0.21553398058252426,"guidance from the data distribution learned by diffusion models with iterative denoising process.
88"
RELATED WORKS,0.2174757281553398,"DALL·E 2 [32] leverages CLIP [29] features to achieve well-aligned text-image latent space, while
89"
RELATED WORKS,0.21941747572815534,"Imagen [37] relies on large language models like T5 [31] to encode text. Stable Diffusion (or Latent
90"
RELATED WORKS,0.22135922330097088,"Diffusion) Model [35] performs diffusion process in latent space, and it can generate impressive
91"
RELATED WORKS,0.22330097087378642,"images after pre-training on a large-scale text-image datasets.
92"
RELATED WORKS,0.22524271844660193,"Story Synthesis is first introduced as the task of story visualization (SV) by StoryGAN [17],
93"
RELATED WORKS,0.22718446601941747,"which presents a GAN-based framework and the inaugural dataset named Pororo, derived from
94"
RELATED WORKS,0.229126213592233,"cartoons. Subsequently, some other works also follow the GAN-based framework, such as DUCO-
95"
RELATED WORKS,0.23106796116504855,"StoryGAN [21] and VLC-StoryGAN [20]. In the case of word-level SV [16], more emphasis is placed
96"
RELATED WORKS,0.23300970873786409,"on the representation of text, whereas VP-CSV [3] employs VQ-VAE [42] and a transformer-based
97"
RELATED WORKS,0.23495145631067962,"language model to conserve character appearance and enhance visual quality. StoryDALL-E [22]
98"
RELATED WORKS,0.23689320388349513,"extends the story visualization task to story continuation with the initial image given and recommends
99"
RELATED WORKS,0.23883495145631067,"using a pre-trained DALL·E generative model [33] to produce coherent images. AR-LDM [28]
100"
RELATED WORKS,0.2407766990291262,"introduces an auto-regressive latent diffusion model that can generate highly realistic images, but with
101"
RELATED WORKS,0.24271844660194175,"only a limited vocabulary. NUWA-XL [49] is a concurrent work that exploits hierarchical diffusion
102"
RELATED WORKS,0.2446601941747573,"model to synthesize long videos, with the keyframes generated first, followed by frame interpolation.
103"
RELATED WORKS,0.24660194174757283,"Existing models are mostly developed on specific scenes, which limits their ability for generating
104"
RELATED WORKS,0.24854368932038834,"image sequences for diverse stories. In this paper, we target for more ambitious applications, to
105"
RELATED WORKS,0.2504854368932039,"develop an open-ended visual storytelling model, that can digest stories of arbitrary length and diverse
106"
RELATED WORKS,0.2524271844660194,"topics, and synthesize a sequence of coherent images in terms of both style and semantic.
107"
METHOD,0.2543689320388349,"3
Method
108"
METHOD,0.2563106796116505,"To be self-contained, we first present a brief overview to diffusion model in Section 3.1; then, we
109"
METHOD,0.258252427184466,"detail our proposed model for storybook generation in Section 3.2, starting from problem formulation,
110"
METHOD,0.26019417475728157,"then architecture details, and lastly on training details.
111"
PRELIMINARIES ON DIFFUSION MODELS,0.2621359223300971,"3.1
Preliminaries on Diffusion Models
112"
PRELIMINARIES ON DIFFUSION MODELS,0.26407766990291265,"Diffusion models are a type of generative models that undergo a denoising process, converting input
113"
PRELIMINARIES ON DIFFUSION MODELS,0.26601941747572816,"noise into meaningful data samples. Diffusion models comprise a forward diffusion process that
114"
PRELIMINARIES ON DIFFUSION MODELS,0.26796116504854367,"incorporates Gaussian noise into an image sample x0, accomplished via a Markovian process over
115"
PRELIMINARIES ON DIFFUSION MODELS,0.26990291262135924,"T steps. If we denote the noisy image at step t as xt, the transition function q(xt|xt−1) connecting
116"
PRELIMINARIES ON DIFFUSION MODELS,0.27184466019417475,"xt−1 and xt can be expressed as follows:
117"
PRELIMINARIES ON DIFFUSION MODELS,0.2737864077669903,"q(xt|xt−1) = N(xt;
p"
PRELIMINARIES ON DIFFUSION MODELS,0.2757281553398058,"1 −βtxt−1, βtI)
q(x1:T |x0) = T
Y"
PRELIMINARIES ON DIFFUSION MODELS,0.27766990291262134,"t=1
q(xt|xt−1)
(1)"
PRELIMINARIES ON DIFFUSION MODELS,0.2796116504854369,"where βt ∈(0, 1) is the variance schedule controlling the step size.
118"
PRELIMINARIES ON DIFFUSION MODELS,0.2815533980582524,"Using Gaussian distribution property and reparametrization, if we define αt = 1 −βt and ¯αt =
119
Qt
i=1 αi, we can write equation 1 as follows:
120"
PRELIMINARIES ON DIFFUSION MODELS,0.283495145631068,"q(xt|x0) = N(xt; √¯αtx0, (1 −¯αt)I)
(2)"
PRELIMINARIES ON DIFFUSION MODELS,0.2854368932038835,"Diffusion models also comprise a reverse diffusion process that learns to restore the initial image
121"
PRELIMINARIES ON DIFFUSION MODELS,0.287378640776699,"sample from noise. A UNet-based model [36] is utilized in the diffusion model to learn the reverse
122"
PRELIMINARIES ON DIFFUSION MODELS,0.28932038834951457,"Figure 2: Architecture Overview. The left figure illustrates the complete procedure of visual
storytelling. Our StoryGen model utilizes contextual information from previous frame and the text
description at current step, to generate an image. The right figure displays the structure of our
proposed modules, (i) style transfer module that is inserted into the text-conditioning module, with
a LoRA-like achitecture; (ii) visual context module that enables the model to also condition on the
features from preceding image for generation."
PRELIMINARIES ON DIFFUSION MODELS,0.2912621359223301,"diffusion process pθ. The process pθ can be expressed using the following equation.
123"
PRELIMINARIES ON DIFFUSION MODELS,0.29320388349514565,"pθ(x0:T ) = p(xT ) T
Y"
PRELIMINARIES ON DIFFUSION MODELS,0.29514563106796116,"t=1
pθ(xt−1|xt)
pθ(xt−1|xt) = N(xt−1; µθ(xt, t), Σθ(xt, t))
(3)"
PRELIMINARIES ON DIFFUSION MODELS,0.2970873786407767,"where µθ is the predicted Gaussian distribution mean value.
124"
PRELIMINARIES ON DIFFUSION MODELS,0.29902912621359223,"As we compute the loss function by taking the mean absolute error of the noise term ϵθ into account,
125"
PRELIMINARIES ON DIFFUSION MODELS,0.30097087378640774,"we can express the mean value µθ in terms of the noise term ϵθ as follows:
126"
PRELIMINARIES ON DIFFUSION MODELS,0.3029126213592233,"µθ(xt, t) =
1
√αt"
PRELIMINARIES ON DIFFUSION MODELS,0.3048543689320388,"
xt −1 −αt
√1 −¯αt
ϵθ(xt, t)

(4)"
PRELIMINARIES ON DIFFUSION MODELS,0.3067961165048544,"Therefore, the objective can be written as:
127"
PRELIMINARIES ON DIFFUSION MODELS,0.3087378640776699,"Lt = Et∼[1,T ],x0,ϵt
h
∥ϵt −ϵθ(xt, t)∥2i
(5)"
STORYGEN MODEL,0.3106796116504854,"3.2
StoryGen Model
128"
STORYGEN MODEL,0.312621359223301,"In this section, we start by defining the problem for visual storytelling, then we introduce the core
129"
STORYGEN MODEL,0.3145631067961165,"components in our proposed architecture, namely, Style Transfer Module and Visual Context Module,
130"
STORYGEN MODEL,0.31650485436893205,"lastly, we present details for training the model with a curriculum learning regime.
131"
PROBLEM FORMULATION,0.31844660194174756,"3.2.1
Problem Formulation
132"
PROBLEM FORMULATION,0.32038834951456313,"In visual storytelling, our goal is to generate a sequence of coherent and consistent images that corre-
133"
PROBLEM FORMULATION,0.32233009708737864,"spond to a given story in the form of natural language. To achieve this, we propose an auto-regressive
134"
PROBLEM FORMULATION,0.32427184466019415,"image generation model, called StoryGen, that generates the current frame Ik by conditioning on
135"
PROBLEM FORMULATION,0.3262135922330097,"both current text description Tk and the previous frame Ik−1, as illustrated in Figure 2. The model is
136"
PROBLEM FORMULATION,0.32815533980582523,"formulated as follows:
137"
PROBLEM FORMULATION,0.3300970873786408,"{ˆI1, ˆI2, . . . , ˆIL} = ΦStoryGen({T1, T2, . . . , TL}; Θ),"
PROBLEM FORMULATION,0.3320388349514563,"= ψSDM(ˆI1|T1; θ) . . . ψSDM(ˆIk|ˆIk−1, Tk; θ) . . . ψSDM(ˆIL|ˆIL−1, TL; θ)"
PROBLEM FORMULATION,0.3339805825242718,"Here, {T1, T2, . . . , TL} represents the given story in text sentences, {ˆI1, ˆI2, . . . , ˆIL|ˆIi ∈RH×W ×3}
138"
PROBLEM FORMULATION,0.3359223300970874,"denotes the generated storybook, H, W refer to the width and height, respectively. ψSDM(·) refers
139"
PROBLEM FORMULATION,0.3378640776699029,"to a semi-frozen stable diffusion model (SDM), with a small number of newly-added trainable
140"
PROBLEM FORMULATION,0.33980582524271846,"parameters (θ). It takes randomly sampled gaussian noise, text description and preceding image as
141"
PROBLEM FORMULATION,0.341747572815534,"input, and generate coherent image sequence that align with the story’s narrative. In the following
142"
PROBLEM FORMULATION,0.34368932038834954,"sections, we present the architecture detail for one-step generation conditioned on text and image.
143"
ARCHITECTURE DETAILS,0.34563106796116505,"3.2.2
Architecture Details
144"
ARCHITECTURE DETAILS,0.34757281553398056,"Generally speaking, our model is built upon the foundation of a pre-trained stable diffusion
145"
ARCHITECTURE DETAILS,0.34951456310679613,"model (SDM), that has been pre-trained on large number of paired image-caption samples, to
146"
ARCHITECTURE DETAILS,0.35145631067961164,"gradually transform the noisy latent into an image. To tackle the problem of open-ended storytelling,
147"
ARCHITECTURE DETAILS,0.3533980582524272,"we introduce two computational modules, namely, Style Transfer Module, and Visual Context Mod-
148"
ARCHITECTURE DETAILS,0.3553398058252427,"ule, that enables the model to condition on not only text descriptions, but also the preceding RGB
149"
ARCHITECTURE DETAILS,0.3572815533980582,"image, as shown in Figure 2. Formally, we can express the generation procedure as:
150"
ARCHITECTURE DETAILS,0.3592233009708738,"Ik = ψSDM(ˆIk|ˆIk−1, Tk) = ψSDM(x, ϕtext(Tk), ϕvis(ˆIk−1))"
ARCHITECTURE DETAILS,0.3611650485436893,"where x, ϕtext(·) and ϕvis(·) denote the noisy latent, encoded text description and preceding image.
151"
ARCHITECTURE DETAILS,0.36310679611650487,"Style Transfer Module. To steer a pre-trained stable diffusion model towards the style of children’s
152"
ARCHITECTURE DETAILS,0.3650485436893204,"storybooks, we propose to insert a lightweight, LoRA-like [13] architecture into the text conditioning
153"
ARCHITECTURE DETAILS,0.36699029126213595,"module, effectively acting as style transfer. This can be expressed as:
154"
ARCHITECTURE DETAILS,0.36893203883495146,"Q = W Q · x + ∆Q, K = W K · Ctext
k
+ ∆K, V = W V · Ctext
k
+ ∆V , where Ctext
k
= ϕtext(Tk)"
ARCHITECTURE DETAILS,0.37087378640776697,"W Q, W K and W V denote the projection matrices, adopted from the text conditioning module in
155"
ARCHITECTURE DETAILS,0.37281553398058254,"pre-trained stable diffusion model. ϕtext(·) and Ctext
k
refer to the pre-trained CLIP text encoder and
156"
ARCHITECTURE DETAILS,0.37475728155339805,"extracted text embedding respectively. ∆Q, ∆K and ∆V are calculated by a learnable projection of
157"
ARCHITECTURE DETAILS,0.3766990291262136,"x, Ctext
k
and Ctext
k , respectively, resembling LoRA operations.
158"
ARCHITECTURE DETAILS,0.3786407766990291,"Visual Context Module. In order to generate visually coherent images, we insert a visual context
159"
ARCHITECTURE DETAILS,0.38058252427184464,"module after the text conditioning, specifically, it is a transformer decoder comprising a self-attention
160"
ARCHITECTURE DETAILS,0.3825242718446602,"layer, a cross-attention layer, and a feed-forward network, where the cross-attention layer employs a
161"
ARCHITECTURE DETAILS,0.3844660194174757,"casual attention mechanism by using the noisy latent as query and the visual features of the previous
162"
ARCHITECTURE DETAILS,0.3864077669902913,"frame as key and value, which can be formally denoted as:
163"
ARCHITECTURE DETAILS,0.3883495145631068,"Q = W Q · x,
K = W K · Cvis
k ,
V = W V · Cvis
k ,
where Cvis
k = ϕvis(Ik−1)"
ARCHITECTURE DETAILS,0.39029126213592236,"W Q, W K and W V refer to three learnable projection matrices, ϕvis(·) denotes the visual feature
164"
ARCHITECTURE DETAILS,0.39223300970873787,"extracted by a pre-trained CLIP visual encoder. It is worth noting that the visual context module can
165"
ARCHITECTURE DETAILS,0.3941747572815534,"also extend to multiple condition frames by concatenating their CLIP features as visual contexts.
166"
ARCHITECTURE DETAILS,0.39611650485436894,"Training Objective. At training stage, we randomly sample a triplet each time, i.e., {Ik, Ik−1, Tk},
167"
ARCHITECTURE DETAILS,0.39805825242718446,"and the objective defined in Equation 5 can now be transformed into:
168"
ARCHITECTURE DETAILS,0.4,"Lt = Et∼[1,T ],x0,ϵt,Cvis
k ,Ctext
k"
ARCHITECTURE DETAILS,0.40194174757281553,"h
∥ϵt −ϵθ(xt, t, Cvis
k , Ctext
k )∥2i
(6)"
ARCHITECTURE DETAILS,0.40388349514563104,"and as we adopt classifier-free guidance [11] in inference, the predicted noise can be expressed as:
169"
ARCHITECTURE DETAILS,0.4058252427184466,"¯ϵθ(xt, t, Cvis
k , Ctext
k ) = (w + 1)ϵθ(xt, t, Cvis
k , Ctext
k ) −wϵθ(xt, t)
(7)"
ARCHITECTURE DETAILS,0.4077669902912621,"where w is the guidance scale.
170"
CURRICULUM LEARNING,0.4097087378640777,"3.2.3
Curriculum Learning
171"
CURRICULUM LEARNING,0.4116504854368932,"In this section, we describe the three-stage training strategy, that includes single-frame pre-training,
172"
CURRICULUM LEARNING,0.41359223300970877,"multiple-frame fine-tuning, and alignment with human feedback. This curriculum learning approach
173"
CURRICULUM LEARNING,0.4155339805825243,"enables the model to gradually learn from simple to complex tasks, ultimately improving its ability
174"
CURRICULUM LEARNING,0.4174757281553398,"to generate high-quality images that align with the given story narratives. Details for our proposed
175"
CURRICULUM LEARNING,0.41941747572815535,"curriculum learning are presented below.
176"
CURRICULUM LEARNING,0.42135922330097086,"Single-frame Pre-training. We start by training the style transfer module, which has been inserted
177"
CURRICULUM LEARNING,0.42330097087378643,"into the text conditioning module of a pre-trained stable diffusion model, in a single-frame manner. At
178"
CURRICULUM LEARNING,0.42524271844660194,"this pre-training stage, we do not introduce the visual context module, and freeze all other parameters
179"
CURRICULUM LEARNING,0.42718446601941745,"except for the LoRA-like plug-in. This training approach allows us to quickly adjust to the desired
180"
CURRICULUM LEARNING,0.429126213592233,"visual style and character appearance in storybooks, while also maintaining the generation ability of
181"
CURRICULUM LEARNING,0.43106796116504853,"the pre-trained stable diffusion model.
182"
CURRICULUM LEARNING,0.4330097087378641,"Multiple-frame Fine-tuning. Here, we fine-tune the visual context module while freezing other
183"
CURRICULUM LEARNING,0.4349514563106796,"parameters of the model. Till this point, this allows the generation procedure to utilize information
184"
CURRICULUM LEARNING,0.4368932038834951,"from either the text description or the preceding frame. To avoid over-fitting to the text descriptions,
185"
CURRICULUM LEARNING,0.4388349514563107,"we adopt a technique inspired by BERT training [15], randomly dropping some words in the text with
186"
CURRICULUM LEARNING,0.4407766990291262,"certain probability. The entire visual context module is fine-tuned during this stage.
187"
CURRICULUM LEARNING,0.44271844660194176,"Fine-tuning with Human Feedback. After multiple-frame fine-tuning, the model has developed
188"
CURRICULUM LEARNING,0.4446601941747573,"basic storybook generation capabilities, to avoid from generating, potentially scary, toxic or biased
189"
CURRICULUM LEARNING,0.44660194174757284,"content, we also propose to align the model with human preference. Specifically, we prompt ChatGPT
190"
CURRICULUM LEARNING,0.44854368932038835,"to generate approximately 200 stories and use our model to synthesize images. After manually
191"
CURRICULUM LEARNING,0.45048543689320386,"filtering around 100 high-quality storybooks from this corpus, we add them to the training set for
192"
CURRICULUM LEARNING,0.4524271844660194,"further fine-tuning. As future work, we aim to add more books into this human feedback step.
193"
CURRICULUM LEARNING,0.45436893203883494,"Inference.
With the three-stage training regime, we can streamline the entire inference process
194"
CURRICULUM LEARNING,0.4563106796116505,"into a unified generation framework. As shown in the Figure 1, at inference time, we can prompt
195"
CURRICULUM LEARNING,0.458252427184466,"the ChatGPT to generate engaging, yet educational storylines, and synthesize the first image using a
196"
CURRICULUM LEARNING,0.4601941747572815,"single-frame approach with only style transfer module involved; the previously synthesized frames,
197"
CURRICULUM LEARNING,0.4621359223300971,"along with story description at current step, are treated as condition to generate image sequence in an
198"
CURRICULUM LEARNING,0.4640776699029126,"auto-regressive manner. Experimentally, our proposed StoryGen is shown to generate images that
199"
CURRICULUM LEARNING,0.46601941747572817,"align with the storyline, as well as maintaining consistency with previously generated frames.
200"
DATASET PREPARATION,0.4679611650485437,"4
Dataset Preparation
201"
DATASET PREPARATION,0.46990291262135925,"For training visual storytelling model, we collect a dataset called StorySalon, that contains approxi-
202"
DATASET PREPARATION,0.47184466019417476,"mately 2K storybooks and more than 30K well-aligned text-image pairs. This dataset is comprised of
203"
DATASET PREPARATION,0.47378640776699027,"storybooks with potentially aligned text and image pairs, sourced from three different sources: video
204"
DATASET PREPARATION,0.47572815533980584,"data, E-book data, and additional data from human feedback.
205"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.47766990291262135,"4.1
Image-text Data from Videos & E-books
206"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.4796116504854369,"Here, we elaborate the procedure of extracting paired image-text samples from YouTube videos and
207"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.4815533980582524,"E-books (pdf and corresponding audios available).
208"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.48349514563106794,"Visual Frame Extraction. To begin with, we download a significant amount of videos and subtitles
209"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.4854368932038835,"from YouTube, by querying keywords related to children story, for example, storytime. We then
210"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.487378640776699,"extract the keyframes from the videos, along with the corresponding subtitles and their timestamps.
211"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.4893203883495146,"To remove duplicate frames, we extract ViT features for each frame using pre-trained DINO [23], for
212"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.4912621359223301,"the image groups with high similarity score, we only keep one of them. Next, we use YOLOv7 [44]
213"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.49320388349514566,"to segment and remove person frames and headshots, as they often correspond to the story-teller and
214"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.49514563106796117,"are unrelated to the content of the storybook. Finally, we manually screen out frames that are entirely
215"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.4970873786407767,"white or black. Similarly, we also acquire a number of electronic storybooks from the Internet,
216"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.49902912621359224,"and extract images from E-book, except for those with extraneous information, for example, the
217"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5009708737864078,"authorship page. We acquire the corresponding text description with Whisper [30] from the audio
218"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5029126213592233,"file. For E-books that do not have corresponding audio files, we use OCR algorithms, to directly
219"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5048543689320388,"recognize the text on each page.
220"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5067961165048543,"Visual-Language Alignment.
Here, for each image, we acquire two types of text description,
221"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5087378640776699,"namely, story-level description, and visual description. This is based on our observation that there
222"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5106796116504855,"actually exists semantic gap between story narrative and descriptive text, for example, the same image
223"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.512621359223301,"can be well described as ""The cat is isolated by others, sitting alone in front of a village."" in story, or
224"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5145631067961165,"""A black cat sits in front of a number of houses."" as visual description, therefore, directly finetuning
225"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.516504854368932,"stable diffusion model with story narrative maybe detrimental to its pre-trained text-image alignment.
226"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5184466019417475,"In practise, to get story-level paired image-text samples, we align the subtitles with visual frames by
227"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5203883495145631,"using Dynamic Time Warping (DTW) algorithm [25]. To get visual descriptions, we use ChatCap-
228"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5223300970873787,"tioner [52] to generate captions for each image, as shown in Figure 3. s At training time, this allows
229"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5242718446601942,"us to substitute the original story with more accurate and descriptive captions.
230"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5262135922330097,"Video
E-book
Synthetic
Text"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5281553398058253,Caption 1
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5300970873786408,Caption 2
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5320388349514563,Caption 3
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5339805825242718,Unstructured
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5359223300970873,Metadada
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.537864077669903,"Step 2
Visual-language"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5398058252427185,Alignment
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.541747572815534,"Aligned
Image-text Pairs"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5436893203883495,"Step 1
Visual Frame"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.545631067961165,Extraction
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5475728155339806,"Step 3
Visual Frame
Post-processing"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5495145631067961,"Text: Santa waved down 
with a cry of delight: 
merry Christmas, my dears, 
and good night. Let 
YouTube know you're 
interested in videos."
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5514563106796116,"Story: Santa waved down 
with a cry of delight: 
merry Christmas, my 
dears, and good night. 
Caption: Two mice are 
sleeping in the bed…"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5533980582524272,"Figure 3: Dataset Pipeline Overview. The left figure provides an overview of the complete dataset
collection pipeline. Unstructured metadata sourced from the Internet undergoes a series of steps
including frame extraction, visual-language alignment and image inpainting, resulting in properly
aligned image-text pairs. The right figure displays examples of video data, E-book data, and synthetic
samples. The accompanying texts represent their corresponding textual content, respectively."
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5553398058252427,"Visual Frame Post-processing. In practice, we discovered that books in the frames can potentially
231"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5572815533980583,"interfere with our image generation model by having story texts printed on them. To address this, we
232"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5592233009708738,"use an OCR detector to identify the text regions in the frames and an image inpainting model to fill in
233"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5611650485436893,"the text and headshot regions. This process results in more precise image-text pairs that can be fed
234"
IMAGE-TEXT DATA FROM VIDEOS & E-BOOKS,0.5631067961165048,"into the diffusion model.
235"
ADDITIONAL DATA FROM HUMAN FEEDBACK,0.5650485436893203,"4.2
Additional Data from Human Feedback
236"
ADDITIONAL DATA FROM HUMAN FEEDBACK,0.566990291262136,"As outlined in Section 3.2.3, we use the model (trained after two stages) to generate a set of new
237"
ADDITIONAL DATA FROM HUMAN FEEDBACK,0.5689320388349515,"storybooks, and incorporate human feedback into the fine-tuning process. Following a rigorous
238"
ADDITIONAL DATA FROM HUMAN FEEDBACK,0.570873786407767,"manual review, we carefully select the best pieces, and add them into the training dataset. This
239"
ADDITIONAL DATA FROM HUMAN FEEDBACK,0.5728155339805825,"allows us to continually improve the quality of our model and ensure that it produces engaging, yet
240"
ADDITIONAL DATA FROM HUMAN FEEDBACK,0.574757281553398,"educational storybooks, that align with human’s preference.
241"
DISCUSSION,0.5766990291262136,"4.3
Discussion
242"
DISCUSSION,0.5786407766990291,"Given the diversity of our data sources and data types, the StorySalon dataset exhibits a significantly
243"
DISCUSSION,0.5805825242718446,"broader range of visual styles and character appearances over other animation-specific datasets.
244"
DISCUSSION,0.5825242718446602,"Moreover, our dataset surpasses others in terms of vocabulary coverage by a substantial margin.
245"
DISCUSSION,0.5844660194174758,"Notably, our texts seamlessly integrate narrative storylines and descriptive visual prompts, ensuring
246"
DISCUSSION,0.5864077669902913,"the preservation of text-image alignment while adapting to the art style of storybooks.
247"
EXPERIMENT,0.5883495145631068,"5
Experiment
248"
EXPERIMENT,0.5902912621359223,"In this section, we start by describing our experimental settings, then compare with other models
249"
EXPERIMENT,0.5922330097087378,"from three different perspectives: style, quality and coherence with quantitative and subjective human
250"
EXPERIMENT,0.5941747572815534,"evaluation. Additionally, we present the results of our ablation experiments to prove the effectiveness
251"
EXPERIMENT,0.596116504854369,"of our proposed training regime.
252"
TRAINING SETTINGS,0.5980582524271845,"5.1
Training Settings
253"
TRAINING SETTINGS,0.6,"Our model is based on publicly released stable diffusion checkpoints, with a learning rate of 1×10−5
254"
TRAINING SETTINGS,0.6019417475728155,"and a batch size of 512. We begin with a single-frame pre-training stage, which involves 10,000
255"
TRAINING SETTINGS,0.6038834951456311,"iterations on 8 NVIDIA RTX3090. In the multiple-frame fine-tuning stage, we fine-tune the model for
256"
TRAINING SETTINGS,0.6058252427184466,"40,000 iterations using a single condition image. To improve the robustness of the training procedure,
257"
TRAINING SETTINGS,0.6077669902912621,"we apply a 10% ∼30% words dropout with a probability to the texts in the current frames. We also
258"
TRAINING SETTINGS,0.6097087378640776,"fine-tune the model with human feedback on the training set with 100 manually generated storybooks
259"
TRAINING SETTINGS,0.6116504854368932,"in addition for 5,000 iterations. During inference, we utilize DDIM sampling and classifier-free
260"
TRAINING SETTINGS,0.6135922330097088,"guidance with a weight of 6.0.
261"
TRAINING SETTINGS,0.6155339805825243,"Model
FID ↓
Alignment ↑Style ↑Content ↑Quality ↑Preference
GT
-
4.22
4.68
4.34
4.32
-
StoryGen
120.01
4.02
3.82
3.67
3.53
70.42%
Prompt-SDM 167.21
3.45
2.25
2.64
3.21
16.11%
SDM
184.01
3.28
2.56
2.57
3.49
13.47%"
TRAINING SETTINGS,0.6174757281553398,"Table 1: Comparison result of human evaluation and FID. GT stands
for the ground truth from the training set. SDM denotes Stable Diffusion
and Prompt-SDM denotes SDM with cartoon-style-directed prompts."
TRAINING SETTINGS,0.6194174757281553,"Model
FID ↓
with HF
66.41
without HF
66.60
Prompt-SDM
101.23
SDM
115.43"
TRAINING SETTINGS,0.6213592233009708,"Table 2:
Ablation
study
on
human
feedback."
QUANTITATIVE RESULTS,0.6233009708737864,"5.2
Quantitative Results
262"
QUANTITATIVE RESULTS,0.625242718446602,"To evaluate the quality of our generated image sequence, we adopt the widely-used Fréchet Inception
263"
QUANTITATIVE RESULTS,0.6271844660194175,"Distance (FID) score [8]. However, as there is no standardized metric for evaluating the consistency
264"
QUANTITATIVE RESULTS,0.629126213592233,"of images, we include human evaluation for comparison.
265"
QUANTITATIVE RESULTS,0.6310679611650486,"Fréchet Inception Distance (FID). We present a comparison of the FID scores between our model
266"
QUANTITATIVE RESULTS,0.6330097087378641,"and other existing ones, including SDM and Propmt-SDM, which conditions on an additional cartoon-
267"
QUANTITATIVE RESULTS,0.6349514563106796,"style-directed prompt ""A cartoon style image"". Specifically, we calculate the FID scores between
268"
QUANTITATIVE RESULTS,0.6368932038834951,"the distribution of the generated results from these models and the distribution of the our proposed
269"
QUANTITATIVE RESULTS,0.6388349514563106,"StorySalon testset. As shown in Table 1, we evaluate the generated image sequences from 100
270"
QUANTITATIVE RESULTS,0.6407766990291263,"storylines, obtained by prompting ChatGPT. Our StoryGen model outperforms the original stable
271"
QUANTITATIVE RESULTS,0.6427184466019418,"diffusion models (SDM) and Prompt-SDM by a large margin, demonstrating the effectiveness of our
272"
QUANTITATIVE RESULTS,0.6446601941747573,"model in generating high-quality coherent images.
273"
QUANTITATIVE RESULTS,0.6466019417475728,"Human Evaluation. We conduct two types of human evaluation experiments to assess the quality
274"
QUANTITATIVE RESULTS,0.6485436893203883,"of our generated storybooks. In the first experiment, we randomly select an equal number of
275"
QUANTITATIVE RESULTS,0.6504854368932039,"groundtruth storybooks, the results of our StoryGen and the generation results of SDM and Propmt
276"
QUANTITATIVE RESULTS,0.6524271844660194,"SDM. Participants are then invited to rate these four categories of storybooks on a score ranging from
277"
QUANTITATIVE RESULTS,0.654368932038835,"1 to 5, taking into account text-image alignment, style consistency, content consistency, and image
278"
QUANTITATIVE RESULTS,0.6563106796116505,"quality, higher scores indicate better samples. In the second experiment, we prompt ChatGPT to
279"
QUANTITATIVE RESULTS,0.658252427184466,"produce a number of storylines and use our StoryGen along with the two variations of stable diffusion
280"
QUANTITATIVE RESULTS,0.6601941747572816,"to generate corresponding image sequences. Participants are asked to choose the preferred results of
281"
QUANTITATIVE RESULTS,0.6621359223300971,"each storyline. To mitigate bias, participants are unaware of the type of storybooks they are evaluating
282"
QUANTITATIVE RESULTS,0.6640776699029126,"during these two human evaluation experiments. In both experiments, we have invited approximately
283"
QUANTITATIVE RESULTS,0.6660194174757281,"30 participants in total.
284"
QUANTITATIVE RESULTS,0.6679611650485436,"Table 1 presents the results of our human evaluation. As can be seen, our model has shown significant
285"
QUANTITATIVE RESULTS,0.6699029126213593,"performance improvement in its overall score compared to stable diffusion models, especially in
286"
QUANTITATIVE RESULTS,0.6718446601941748,"terms of consistency and alignment, indicating that it can generate images that are highly consistent
287"
QUANTITATIVE RESULTS,0.6737864077669903,"with the given text prompts and visual contexts, thus better exploiting the contextual information.
288"
QUANTITATIVE RESULTS,0.6757281553398058,"Ablation Studies. Our study compares the performance of our model with and without fine-tuning
289"
QUANTITATIVE RESULTS,0.6776699029126214,"using human feedback. We evaluate the FID score of between our generated image sequence and
290"
QUANTITATIVE RESULTS,0.6796116504854369,"the test set of our StorySalon dataset. The results demonstrate that fine-tuning with human feedback
291"
QUANTITATIVE RESULTS,0.6815533980582524,"slightly improves performance, we conjecture that this could be attributed to the fact that the number
292"
QUANTITATIVE RESULTS,0.683495145631068,"of human-verified samples are limited, due to a result of resource limitation. In future work, we
293"
QUANTITATIVE RESULTS,0.6854368932038835,"intend to address this quantity constrain by continually augmenting the dataset with new samples and
294"
QUANTITATIVE RESULTS,0.6873786407766991,"closely monitoring the progressive advancements.
295"
QUALITATIVE RESULTS,0.6893203883495146,"5.3
Qualitative Results
296"
QUALITATIVE RESULTS,0.6912621359223301,"In Figure 4, we present the visualization results, showing that our model can generate storybooks
297"
QUALITATIVE RESULTS,0.6932038834951456,"with a broad vocabulary, while maintaining coherence and consistency throughout the narrative. The
298"
QUALITATIVE RESULTS,0.6951456310679611,"generated images successfully maintain the consistency of the artistic style and character appearance,
299"
QUALITATIVE RESULTS,0.6970873786407767,"whereas the results from SDM and Prompt-SDM fail to do so. Moreover, style of the generated
300"
QUALITATIVE RESULTS,0.6990291262135923,"results from SDM’s are also incongruent with the requirements of visual storytelling for childrens.
301"
CONCLUSION,0.7009708737864078,"6
Conclusion
302"
CONCLUSION,0.7029126213592233,"In this paper, we consider the exciting yet challenging task known as open-ended visual storytelling,
303"
CONCLUSION,0.7048543689320388,"which involves generating a sequence of consistent images that tell a coherent visual story based
304"
CONCLUSION,0.7067961165048544,"(a) A story of a {white dog}: (1) Once upon a time, in a small village, there lived a white dog. It had pure white fur that sparkled in the sunlight. (2) The white dog 
was loved by all the villagers. Children would play with it, and adults would often walk it around the village. (3) One day, while the white dog was taking a walk, it 
heard a cry for help. It followed the sound and found a young boy who had fallen into a deep pit. The white dog quickly sprang into action and started barking loudly 
to get the attention of the villagers. (4) Within minutes, a group of villagers gathered around the pit and lowered a rope to rescue the boy. (5) From that day onwards, 
the white dog was regarded as a hero in the village. (6) Eventually, the white dog became old. But even in its old age, the white dog would still wag its tail whenever 
it saw a child or heard someone call its name. (7) The white dog passed away peacefully, surrounded by the love and affection of the entire village."
CONCLUSION,0.7087378640776699,"(b) A story of a {boy with yellow hair}: (1) Once upon a time, there was a boy with yellow hair as bright as the sun. (2) The boy loved his yellow hair, even though 
other children teased him for looking different. (3) One day, as the boy was walking through the forest, he stumbled upon a magical vase. When he touched the vase, 
he suddenly shrunk down to the size of a bug. (4) Scared and confused, the boy tried to find his way back to his normal size. He wandered through the forest, until he 
came across a wise old rabbit.  (5) The rabbit taught him a special spell to grow back to his normal size. The boy was grateful for the rabbit’s kindness. (6) As the 
boy regained his size, he realized that his yellow hair had grown even brighter because of the magical vase. He ran back to his village, excited to show off his hair. 
(7) But when the children saw his hair, they no longer wanted to tease him. They were amazed by the boy‘s bold and beautiful yellow hair. From that day forward he 
had grown to understand that being different is what makes us special, and he embraced his uniqueness with pride. 1 1 1"
CONCLUSION,0.7106796116504854,"2
3
4
5
6
7"
CONCLUSION,0.7126213592233009,"2
3
4
5
6
7"
CONCLUSION,0.7145631067961165,"2
3
4
5
6
7 1 1 1"
CONCLUSION,0.7165048543689321,"2
3
4
5
6
7"
CONCLUSION,0.7184466019417476,"2
3
4
5
6
7"
CONCLUSION,0.7203883495145631,"2
3
4
5
6
7"
CONCLUSION,0.7223300970873786,"Figure 4: Qualitative Comparison with other baselines. The images in green, orange and blue
boxes are generated SDM, Prompt-SDM and StoryGen respectively. Our results have superior style
and content consistency, text-image alignment, and image quality."
CONCLUSION,0.7242718446601941,"on the given storyline. Our proposed StoryGen architecture can take input from the preceding
305"
CONCLUSION,0.7262135922330097,"frame along with the text prompt to generate the current frame in an auto-regressive manner. A
306"
CONCLUSION,0.7281553398058253,"three-stage curriculum training strategy has been introduced for effective training and alignment with
307"
CONCLUSION,0.7300970873786408,"human preference. Due to the limitations of dataset in previous works, we have also collected a
308"
CONCLUSION,0.7320388349514563,"large-scale, diverse dataset named StorySalon that includes paired image-text samples sourced from
309"
CONCLUSION,0.7339805825242719,"storybook data from videos, e-books and synthesized samples. The StorySalon dataset possesses
310"
CONCLUSION,0.7359223300970874,"a diverse vocabulary of storyline, character appearances and artistic styles. While comparing with
311"
CONCLUSION,0.7378640776699029,"stable diffusion models with quantitative experiment and human evaluation, our proposed model
312"
CONCLUSION,0.7398058252427184,"substantially outperform existing models, from the perspective of image quality, style consistency,
313"
CONCLUSION,0.7417475728155339,"content consistency, and image-language alignment.
314"
REFERENCES,0.7436893203883496,"References
315"
REFERENCES,0.7456310679611651,"[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural
316"
REFERENCES,0.7475728155339806,"images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
317"
REFERENCES,0.7495145631067961,"18208–18218, 2022. 3
318"
REFERENCES,0.7514563106796116,"[2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing
319"
REFERENCES,0.7533980582524272,"instructions. arXiv preprint arXiv:2211.09800, 2022. 3
320"
REFERENCES,0.7553398058252427,"[3] Hong Chen, Rujun Han, Te-Lin Wu, Hideki Nakayama, and Nanyun Peng. Character-centric story
321"
REFERENCES,0.7572815533980582,"visualization via visual planning and token alignment. arXiv preprint arXiv:2210.08465, 2022. 3
322"
REFERENCES,0.7592233009708738,"[4] K. Dickinson David, A. Griffith Julie, Golinkoff Roberta, Michnick, and Hirsh-Pasek Kathy. How reading
323"
REFERENCES,0.7611650485436893,"books fosters language development around the world. Child Development Research, 2012. 1
324"
REFERENCES,0.7631067961165049,"[5] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis.
325"
REFERENCES,0.7650485436893204,"Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011,
326"
REFERENCES,0.7669902912621359,"2023. 3
327"
REFERENCES,0.7689320388349514,"[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
328"
REFERENCES,0.7708737864077669,"Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 2020. 3
329"
REFERENCES,0.7728155339805826,"[7] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-
330"
REFERENCES,0.7747572815533981,"prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 3
331"
REFERENCES,0.7766990291262136,"[8] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
332"
REFERENCES,0.7786407766990291,"trained by a two time-scale update rule converge to a local nash equilibrium.
Advances in Neural
333"
REFERENCES,0.7805825242718447,"Information Processing Systems, 2017. 8
334"
REFERENCES,0.7825242718446602,"[9] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P
335"
REFERENCES,0.7844660194174757,"Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video
336"
REFERENCES,0.7864077669902912,"generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1, 3
337"
REFERENCES,0.7883495145631068,"[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural
338"
REFERENCES,0.7902912621359224,"Information Processing Systems, 2020. 2
339"
REFERENCES,0.7922330097087379,"[11] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.
340 5
341"
REFERENCES,0.7941747572815534,"[12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.
342"
REFERENCES,0.7961165048543689,"Video diffusion models. arXiv preprint arXiv:2204.03458, 2022. 3
343"
REFERENCES,0.7980582524271844,"[13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
344"
REFERENCES,0.8,"Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,
345"
REFERENCES,0.8019417475728156,"2021. 5
346"
REFERENCES,0.8038834951456311,"[14] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal
347"
REFERENCES,0.8058252427184466,"Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210.09276,
348"
REFERENCES,0.8077669902912621,"2022. 3
349"
REFERENCES,0.8097087378640777,"[15] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional
350"
REFERENCES,0.8116504854368932,"transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171–4186, 2019. 6
351"
REFERENCES,0.8135922330097087,"[16] Bowen Li. Word-level fine-grained story visualization. In Proceedings of the European Conference on
352"
REFERENCES,0.8155339805825242,"Computer Vision, 2022. 3
353"
REFERENCES,0.8174757281553398,"[17] Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson,
354"
REFERENCES,0.8194174757281554,"and Jianfeng Gao. Storygan: A sequential conditional gan for story visualization. In Proceedings of the
355"
REFERENCES,0.8213592233009709,"IEEE Conference on Computer Vision and Pattern Recognition, 2019. 2, 3
356"
REFERENCES,0.8233009708737864,"[18] Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Guiding text-to-image
357"
REFERENCES,0.8252427184466019,"diffusion model towards grounded generation. arXiv preprint arXiv:2301.05221, 2023. 3
358"
REFERENCES,0.8271844660194175,"[19] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool.
359"
REFERENCES,0.829126213592233,"Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF
360"
REFERENCES,0.8310679611650486,"Conference on Computer Vision and Pattern Recognition, pages 11461–11471, 2022. 3
361"
REFERENCES,0.8330097087378641,"[20] Adyasha Maharana and Mohit Bansal. Integrating visuospatial, linguistic, and commonsense structure into
362"
REFERENCES,0.8349514563106796,"story visualization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
363"
REFERENCES,0.8368932038834952,"Processing, 2021. 3
364"
REFERENCES,0.8388349514563107,"[21] Adyasha Maharana, Darryl Hannan, and Mohit Bansal. Improving generation and evaluation of visual
365"
REFERENCES,0.8407766990291262,"stories via semantic consistency. In Proceedings of the 2021 Conference of the North American Chapter of
366"
REFERENCES,0.8427184466019417,"the Association for Computational Linguistics: Human Language Technologies, 2021. 3
367"
REFERENCES,0.8446601941747572,"[22] Adyasha Maharana, Darryl Hannan, and Mohit Bansal. Storydall-e: Adapting pretrained text-to-image
368"
REFERENCES,0.8466019417475729,"transformers for story continuation. In Proceedings of the European Conference on Computer Vision, 2022.
369"
REFERENCES,0.8485436893203884,"2, 3
370"
REFERENCES,0.8504854368932039,"[23] Caron Mathilde, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand
371"
REFERENCES,0.8524271844660194,"Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International
372"
REFERENCES,0.8543689320388349,"Conference on Computer Vision, 2021. 6
373"
REFERENCES,0.8563106796116505,"[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit:
374"
REFERENCES,0.858252427184466,"Guided image synthesis and editing with stochastic differential equations. In International Conference on
375"
REFERENCES,0.8601941747572815,"Learning Representations, 2021. 3
376"
REFERENCES,0.8621359223300971,"[25] Meinard Müller. Dynamic time warping. Information retrieval for music and motion, 2007. 6
377"
REFERENCES,0.8640776699029126,"[26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya
378"
REFERENCES,0.8660194174757282,"Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided
379"
REFERENCES,0.8679611650485437,"diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3
380"
REFERENCES,0.8699029126213592,"[27] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In
381"
REFERENCES,0.8718446601941747,"International Conference on Machine Learning, 2021. 2
382"
REFERENCES,0.8737864077669902,"[28] Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, and Wenhu Chen. Synthesizing coherent story with
383"
REFERENCES,0.8757281553398059,"auto-regressive latent diffusion models. arXiv preprint arXiv:2211.10950, 2022. 2, 3
384"
REFERENCES,0.8776699029126214,"[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
385"
REFERENCES,0.8796116504854369,"Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
386"
REFERENCES,0.8815533980582524,"natural language supervision. In Proceedings of the International Conference on Machine Learning, 2021.
387 3
388"
REFERENCES,0.883495145631068,"[30] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust
389"
REFERENCES,0.8854368932038835,"speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356, 2022. 6
390"
REFERENCES,0.887378640776699,"[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
391"
REFERENCES,0.8893203883495145,"Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
392"
REFERENCES,0.8912621359223301,"The Journal of Machine Learning Research, 2020. 3
393"
REFERENCES,0.8932038834951457,"[32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional
394"
REFERENCES,0.8951456310679612,"image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 3
395"
REFERENCES,0.8970873786407767,"[33] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and
396"
REFERENCES,0.8990291262135922,"Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning,
397"
REFERENCES,0.9009708737864077,"2021. 1, 3
398"
REFERENCES,0.9029126213592233,"[34] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided
399"
REFERENCES,0.9048543689320389,"texturing of 3d shapes. arXiv preprint arXiv:2302.01721, 2023. 3
400"
REFERENCES,0.9067961165048544,"[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution
401"
REFERENCES,0.9087378640776699,"image synthesis with latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision
402"
REFERENCES,0.9106796116504854,"and Pattern Recognition, 2022. 1, 3
403"
REFERENCES,0.912621359223301,"[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
404"
REFERENCES,0.9145631067961165,"image segmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015:
405"
REFERENCES,0.916504854368932,"18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, 2015. 3
406"
REFERENCES,0.9184466019417475,"[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
407"
REFERENCES,0.920388349514563,"Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-
408"
REFERENCES,0.9223300970873787,"image diffusion models with deep language understanding. Advances in Neural Information Processing
409"
REFERENCES,0.9242718446601942,"Systems, 2022. 3
410"
REFERENCES,0.9262135922330097,"[38] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,
411"
REFERENCES,0.9281553398058252,"Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv
412"
REFERENCES,0.9300970873786408,"preprint arXiv:2209.14792, 2022. 3
413"
REFERENCES,0.9320388349514563,"[39] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning
414"
REFERENCES,0.9339805825242719,"using nonequilibrium thermodynamics. In International Conference on Machine Learning, 2015. 2
415"
REFERENCES,0.9359223300970874,"[40] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint
416"
REFERENCES,0.9378640776699029,"arXiv:2010.02502, 2020. 2
417"
REFERENCES,0.9398058252427185,"[41] Gabrielle A. Strouse, Angela Nyhout, and Patricia A. Ganea. The role of book features in young children’s
418"
REFERENCES,0.941747572815534,"transfer of information from picture books to real-world contexts. Frontiers in Psychology, 2018. 1
419"
REFERENCES,0.9436893203883495,"[42] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural
420"
REFERENCES,0.945631067961165,"information processing systems, 2017. 3
421"
REFERENCES,0.9475728155339805,"[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
422"
REFERENCES,0.9495145631067962,"Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,
423"
REFERENCES,0.9514563106796117,"2017. 3
424"
REFERENCES,0.9533980582524272,"[44] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-freebies
425"
REFERENCES,0.9553398058252427,"sets new state-of-the-art for real-time object detectors. arXiv preprint arXiv:2207.02696, 2022. 6
426"
REFERENCES,0.9572815533980582,"[45] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings
427"
REFERENCES,0.9592233009708738,"of the 28th international conference on machine learning (ICML-11), 2011. 2
428"
REFERENCES,0.9611650485436893,"[46] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu
429"
REFERENCES,0.9631067961165048,"Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video
430"
REFERENCES,0.9650485436893204,"generation. arXiv preprint arXiv:2212.11565, 2022. 3
431"
REFERENCES,0.9669902912621359,"[47] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape guided
432"
REFERENCES,0.9689320388349515,"object inpainting with diffusion model. arXiv preprint arXiv:2212.05034, 2022. 3
433"
REFERENCES,0.970873786407767,"[48] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.
434"
REFERENCES,0.9728155339805825,"Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In
435"
REFERENCES,0.974757281553398,"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. 3
436"
REFERENCES,0.9766990291262136,"[49] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang,
437"
REFERENCES,0.9786407766990292,"Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video
438"
REFERENCES,0.9805825242718447,"generation. arXiv preprint arXiv:2303.12346, 2023. 3
439"
REFERENCES,0.9825242718446602,"[50] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N
440"
REFERENCES,0.9844660194174757,"Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks.
441"
REFERENCES,0.9864077669902913,"In Proceedings of the International Conference on Computer Vision, 2017. 3
442"
REFERENCES,0.9883495145631068,"[51] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N
443"
REFERENCES,0.9902912621359223,"Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial networks. IEEE
444"
REFERENCES,0.9922330097087378,"transactions on pattern analysis and machine intelligence, 2018. 3
445"
REFERENCES,0.9941747572815534,"[52] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, and Mohamed Elhoseiny.
446"
REFERENCES,0.996116504854369,"Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. arXiv preprint
447"
REFERENCES,0.9980582524271845,"arXiv:2303.06594, 2023. 6
448"
