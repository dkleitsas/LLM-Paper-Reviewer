Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00186219739292365,"This paper is concerned with representing and learning the optimal control law
for the linear quadratic Gaussian (LQG) optimal control problem. In recent years,
there is a growing interest in re-visiting this classical problem, in part due to the
successes of reinforcement learning (RL). The main question of this body of re-
search (and also of our paper) is to approximate the optimal control law without
explicitly solving the Riccati equation. For this purpose, a novel simulation-based
algorithm, namely an ensemble Kalman ﬁlter (EnKF), is introduced in this paper.
The algorithm is used to obtain formulae for optimal control, expressed entirely in
terms of the EnKF particles. For the general partially observed LQG problem, the
proposed EnKF is combined with a standard EnKF (for the estimation problem)
to obtain the optimal control input based on the use of the separation principle.
The theoretical results and algorithms are illustrated with numerical experiments."
INTRODUCTION,0.0037243947858473,"1
INTRODUCTION"
INTRODUCTION,0.00558659217877095,"This paper is concerned with the problem of reinforcement learning (RL) in continuous-time and
continuous (Euclidean) state-space settings. A special case is the linear quadratic Gaussian (LQG)
problem where the dynamic model is a linear system, the cost terms are quadratic, and the distribu-
tions – of the random initial condition and the noise – are Gaussian."
INTRODUCTION,0.0074487895716946,"The LQG problem has a rich and storied history in modern control theory going back to the very
origin of the subject (Kalman, 1960). To obtain the optimal control, the bottleneck is to solve the
Riccati equation – the differential Riccati equation (DRE) in ﬁnite-horizon settings or the algebraic
Riccati equation (ARE) in the inﬁnite-horizon settings of the problem. There is a body of literature
devoted to the study of these equations (Bittanti et al., 2012; Lancaster & Rodman, 1995) with
specialized numerical techniques to compute the solution (Laub, 1991; Benner & Bujanovi´c, 2016)."
INTRODUCTION,0.00931098696461825,"There are two issues which makes the LQG and related problems a topic of recent research interest:
(i) In high-dimensions, the matrix-valued nature of the DRE or ARE means that any algorithm is
O(d2) in the dimension d of the state-space; and (ii) the model parameters may not be explicitly
available to write down the DRE (or the ARE) let alone solve it. The latter is a concern, e.g., when
the model exists only in the form of a black-box numerical simulator."
INTRODUCTION,0.0111731843575419,"These two issues have motivated the recent research on the inﬁnite-horizon linear quadratic regulator
(LQR) problem (Fazel et al., 2018; Mohammadi et al., 2021b; Tu & Recht, 2019). In LQR, the
bottleneck is to solve an ARE. The algorithms studied in the recent papers seek to bypass solving
an ARE by directly searching over the space of stabilizing gain matrices. Global convergence rate
estimates are established for both discrete-time (Fazel et al., 2018; Dean et al., 2020; Malik et al.,
2020; Mohammadi et al., 2021a) and continuous-time (Mohammadi et al., 2020a;b; 2019; 2021b)
settings of the LQR problem. Extensions to the H∞regularized LQR (Zhang et al., 2020) and
Markov jump linear systems (Jansch-Porto et al., 2020) have also been carried out."
INTRODUCTION,0.01303538175046555,"The motivation and goals of the present work are related to these recent papers, albeit the proposed
solution approach is very different. Our inspiration comes from data assimilation (Reich & Cotter,
2015), where in practical applications, e.g., in weather prediction, (i) only simulation-based models
are available, and (ii) these are very high-dimensional. The ensemble Kalman ﬁlter (EnKF) is an
efﬁcient simulation-based algorithm to assimilate sensor data in these applications without the need
to explicitly solve a DRE (Evensen, 1994; 2006; Houtekamer & Mitchell, 2001)."
INTRODUCTION,0.0148975791433892,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.01675977653631285,"Our contribution:
A novel extension of the EnKF algorithm is proposed to learn the optimal
control law for the general (stochastic and partially observed) setting of the LQG optimal control
problem. The proposed algorithm is simulation-based (using particles) and, in particular, avoids the
need to solve the DRE. Assuming full-state feedback (e.g, setting of the LQR problem), the optimal
control law is directly obtained. For the partially observed case, the proposed EnKF is combined
with a standard EnKF (for the estimation problem) to obtain the optimal control input, based on the
use of the separation principle."
INTRODUCTION,0.0186219739292365,"The algorithm is shown to be exact in its mean-ﬁeld (when the number of particles N = ∞) limit
(Prop. 2). For the ﬁnite-N approximation, an error bound is obtained (Eq. (14)). An extensive
discussion is included to provide an intuitive explanation of the algorithm, situate the algorithm in
the RL landscape (see discussion after Eq. (12) in Sec. 2.2), and to compare and contrast the algo-
rithm with the state-of-the-art (Sec. 2.5). For this purpose, numerical comparisons with competing
algorithms are also included (Fig. 2 (c))."
INTRODUCTION,0.020484171322160148,"The outline of the remainder of this paper is as follows. The LQG optimal control problem and
its simulation-based solution is described in Sec. 2. The algorithms are illustrated with numerical
examples in Sec. 3 where comparisons with the state-of-the-art algorithms are also described."
INTRODUCTION,0.0223463687150838,"2
LINEAR QUADRATIC GAUSSIAN (LQG) PROBLEM"
INTRODUCTION,0.024208566108007448,"Problem:
The partially observed linear Gaussian model is expressed using the Itˆo-stochastic dif-
ferential equations (SDE) as"
INTRODUCTION,0.0260707635009311,"dXt = AXt dt + BUt dt + dξt,
X0 ∼N(m0, Σ0)
(1a)
dZt = HXt dt + dζt,
Z0 = 0
(1b)"
INTRODUCTION,0.027932960893854747,"where X = {Xt ∈Rd : t ≥0} is the hidden state process, U = {Ut ∈Rm : t ≥0} is the control
input, and Z = {Zt ∈Rp : t ≥0} is the observation process. The model parameters A, B, H are
matrices of appropriate dimensions, the process noise ξ = {ξt ∈Rd : t ≥0} and the observation
noise ζ = {ζt ∈Rp : t ≥0} are Wiener processes (w.p.) with covariance Q and R, respectively. It
is assumed that X0, ξ and ζ are mutually independent and R ≻0."
INTRODUCTION,0.0297951582867784,The LQG optimal control objective is to minimize
INTRODUCTION,0.03165735567970205,J(U) = E Z T 0
INTRODUCTION,0.0335195530726257,"1
2|CXt|2 + 1"
INTRODUCTION,0.035381750465549346,"2U ⊤
t RUt dt + 1"
INTRODUCTION,0.037243947858473,"2X⊤
T PT XT ! (2)"
INTRODUCTION,0.03910614525139665,"It is assumed that (A, B) is controllable, (A, C) and (A, H) are observable, and the matrices PT ≻0
and R ≻0. In the fully observed settings, Ut is allowed to be a function of Xt. In the partially
observed settings, Ut is a function of the past observations {Zs : 0 ≤s ≤t}. For this purpose, it is
convenient to denote Zt := σ({Zs : 0 ≤s ≤t}) as the sigma-ﬁeld generated by the observations,
and consider control inputs U which are adapted to the ﬁltration Z := {Zt : t ≥0}."
INTRODUCTION,0.040968342644320296,"Classical solution:
Using the seperation principle1 the LQG controller is obtained in three steps:"
INTRODUCTION,0.04283054003724395,"Step 1. Filter design:
The objective of the ﬁlter design step is to compute the causal estimate
ˆXt := E(Xt|Zt) for t ≥0. The evolution equation for { ˆXt : t ≥0} is the Kalman-Bucy ﬁlter.
Notably, the optimal gain matrix for the ﬁlter is obtained by solving a forward (in time) DRE."
INTRODUCTION,0.0446927374301676,"Step 2. Control design:
The objective of the control design step is to compute the optimal feed-
back control law {ut(x) : 0 ≤t ≤T, x ∈Rd} for the fully observed LQG problem. It is well
known to be of the linear form
ut(x) = Ktx,
0 ≤t ≤T"
INTRODUCTION,0.04655493482309125,where the optimal gain matrix Kt is obtained by solving a backward (in time) DRE.
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.048417132216014895,"1The separation principle hinges on the assumption that the control input Ut does not change the observation
sigma-ﬁeld Zt. This is valid under very mild assumptions on the control policy, e.g., Lipschitz with respect to
estimate ˆ
Xt (Van Handel, 2007, Sec. 7.3), (Georgiou & Lindquist, 2013)."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.05027932960893855,Under review as a conference paper at ICLR 2022
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.0521415270018622,"Step 3. Certainty equivalence:
The optimal control input is obtained by combining the results
from steps 1 and 2:
Ut = ut( ˆXt) = Kt ˆXt,
0 ≤t ≤T"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.054003724394785846,"The bottleneck is to solve the DREs – the forward DRE for the optimal ﬁlter gain and the backward
DRE for the optimal control gain2. In the following three sections, we describe a simulation-based
algorithm for these three steps which avoids the need to explicitly solve the DREs."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.055865921787709494,"2.1
STEP 1. FILTER DESIGN USING ENKF"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.05772811918063315,"The ﬁlter design objective is to compute the causal estimate ˆXt = E(Xt|Zt). In the linear Gaussian
settings, the conditional distribution of the Xt is Gaussian whose mean and variance are denoted as
mt and Σt, respectively. These evolve according to the Kalman-Bucy ﬁlter:"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.0595903165735568,"dmt = Amt dt + BUt dt + Lt( dZt −Hmt dt),
m0 = E(X0)
(3a)
d
dtΣt = AΣt + ΣtA⊤+ Q −ΣtH⊤R−1HΣt,
Σ0 = var(X0)
(3b)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.061452513966480445,"where Lt = ΣtH⊤R−1 is the Kalman gain. Note that (3b) is a forward (in time) DRE and its
solution Σt is used to compute the optimal Kalman gain Lt."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.0633147113594041,"The EnKF is a simulation-based algorithm to approximate the Kalman ﬁlter, that does not require
an explicit solution of the DRE (3b). The design of an EnKF proceeds in two steps:"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.06517690875232775,"1.
Construct a stochastic process, denoted by ¯X := { ¯Xt ∈Rd : t ≥0}, such that the conditional
distribution (given Zt) of ¯Xt is equal to the conditional distribution of Xt;"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.0670391061452514,"2.
Simulate N stochastic processes, denoted by {Xi
t : t ≥0, 1 ≤i ≤N}, to empirically approxi-
mate the distribution of ¯Xt."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.06890130353817504,"The process ¯X is referred to as the mean-ﬁeld process. The N processes in the step 2 are referred to
as particles. The construction ensures that the EnKF is exact in the mean-ﬁeld (N = ∞) limit. That
is, for any bounded and continuous function f,"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.07076350093109869,"E[f(Xt)|Zt]
Step 1
= E[f( ¯Xt)|Zt]
|
{z
}
exactness condition"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.07262569832402235,"Step 2
≈
1
N N
X"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.074487895716946,"i=1
f(Xi
t)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.07635009310986965,The details of the two steps are as follows:
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.0782122905027933,"Mean-ﬁeld process:
The mean-ﬁeld process is constructed as"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.08007448789571694,d ¯Xt = A ¯Xt dt + BUt dt + d¯ξt + ¯Lt( dZt −H ¯Xt + H ¯mt
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.08193668528864059,"2
dt),
¯X0 ∼N(m0, Σ0)
(4)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.08379888268156424,"where ¯ξ := {¯ξt : t ≥0} is an independent copy of the process noise ξ, ¯Lt := ¯ΣtH⊤R−1 is the
Kalman gain and"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.0856610800744879,"¯mt := E[ ¯Xt|Zt],
¯Σt := E[( ¯Xt −¯mt)( ¯Xt −¯mt)⊤|Zt]"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.08752327746741155,"are the conditional mean and the conditional covariance, respectively, of ¯Xt. The right-hand side
of (4) depends upon both the process ( ¯Xt) as well as the statistics of the process ( ¯mt, ¯Σt). Such an
SDE is an example of a McKean-Vlasov SDE. The proof of the following proposition is included in
the Appendix A (see also Taghvaei & Mehta (2020, Theorem 1))."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.0893854748603352,"Proposition 1 (Exactness of EnKF). Consider the mean-ﬁeld EnKF (4) initialized with a Gaussian
initial condition ¯X0 ∼N(m0, Σ0). Suppose the control input U is a Z-adapted stochastic process.
Then its solution ¯Xt is a Gaussian random variable whose conditional mean and variance"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.09124767225325885,"¯mt = mt,
¯Σt = Σt,
a.s., t > 0"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.0931098696461825,evolve the same as the Kalman ﬁlter (3).
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.09497206703910614,"2In the steady-state or inﬁnite horizon settings (as T →∞), one may replace the optimal ﬁlter gain and the
optimal control gain by their steady-state values. These are directly obtained by solving the respective AREs."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.09683426443202979,Under review as a conference paper at ICLR 2022
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.09869646182495345,"Finite-N approximation:
The mean-ﬁeld process is simulated as an interacting particle system:"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1005586592178771,"dXi
t = AXi
t dt + BUt dt + dξi
t
|
{z
}
i-th copy of model (1a)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.10242085661080075,"+ L(N)
t
( dZt −HXi
t + H ˆX(N)
t
2
dt)
|
{z
}
data assimilation step"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1042830540037244,",
Xi
0
i.i.d
∼N(m0, Σ0) (5)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.10614525139664804,"where ˆX(N)
t
:= 1"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.10800744878957169,"N
PN
i=1 Xi
t is the empirical mean and"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.10986964618249534,"L(N)
t
=
1
N −1 N
X"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.11173184357541899,"i=1
(Xi
t)(HXi
t −H ˆX(N)
t
)⊤R−1
(6)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.11359404096834265,"is the empirical approximation of the optimal Kalman gain matrix. The system (5) is referred to
as the square root form of the EnKF (Bergemann & Reich, 2012, Eq (3.3)). Note that the gain is
approximated entirely in terms of particles without solving the DRE (3b)."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1154562383612663,"The EnKF (5) is an example of a simulation-based algorithm in the sense that N copies of the
model (1a) are simulated in parallel. The simulations are coupled through a term which is referred
to as the data assimilation step. This term has a gain times error feedback control structure."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.11731843575418995,"2.2
STEP 2. CONTROL DESIGN USING ENKF"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1191806331471136,"This section contains the main contribution of this paper. Assuming full-state feedback, the optimal
control law for the LQG problem (1a)-(2) is"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.12104283054003724,"ut(x) = Ktx
where
Kt = −R−1B⊤Pt"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.12290502793296089,is the optimal gain matrix and Pt is a solution of the backward (in time) DRE −d
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.12476722532588454,"dtPt = A⊤Pt + PtA + C⊤C −PtBR−1B⊤Pt,
PT (given)
(7)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1266294227188082,"It is known that Pt ≻0 for 0 ≤t ≤T whenever PT ≻0 (Brockett, 2015, Sec. 24). Therefore,
St = P −1
t
is well-deﬁned. It is straightforward to verify that St also solves a backward DRE"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.12849162011173185,"d
dtSt = ASt + StA⊤−BR−1B⊤+ StC⊤CSt,
ST = P −1
T
(8)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1303538175046555,"The bottleneck is to solve the DRE (7). In the following, an EnKF is proposed to obtain a simulation-
based approximation of the optimal control law ut. As before, the construction proceeds in two
steps: (i) deﬁnition of an exact mean-ﬁeld process and (ii) its ﬁnite-N approximation."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.13221601489757914,"Mean-ﬁeld process:
The objective is to construct a stochastic process, denoted ¯Yt ∈Rd at time
t, whose variance equals Pt, the solution of the DRE (7). This is done by constructing ¯Y = { ¯Yt ∈
Rd : 0 ≤t ≤T} as a solution of the following backward (in time) McKean-Vlasov SDE:"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1340782122905028,"d ¯Yt = A ¯Yt dt + B d
 ¯ηt + 1"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.13594040968342644,"2
¯StC⊤(C ¯Yt + C¯nt) dt,
¯YT ∼N(0, ST )
(9)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1378026070763501,"and deﬁning
¯Yt := ¯S−1
t
( ¯Yt −¯nt)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.13966480446927373,"where ¯η = {¯ηt ∈Rm : t ≥0} is a w.p. with covariance matrix R−1, d
 ¯η in (9) denotes the
backward Itˆo-integral (Nualart & Pardoux, 1988, Sec. 4.2), and"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.14152700186219738,"¯nt := E[ ¯Yt],
¯St := E[( ¯Yt −¯nt)( ¯Yt −¯nt)⊤]
(10)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.14338919925512103,The proof of the following proposition is included in Appendix B.
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1452513966480447,"Proposition 2. Consider the mean-ﬁeld EnKF (9) initialized with a Gaussian initial condition ¯YT ∼
N(0, ST ). Then its solution ¯Yt is a Gaussian random variable whose mean and variance"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.14711359404096835,"¯nt = 0,
¯St = St,
0 ≤t ≤T"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.148975791433892,Under review as a conference paper at ICLR 2022
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.15083798882681565,"Consequently, ¯Yt is also a Gaussian random variable with mean and variance"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1527001862197393,"E( ¯Yt) = 0,
E( ¯Yt ¯Y ⊤
t ) = Pt,
0 ≤t ≤T"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.15456238361266295,and therefore (i) If the matrix B is explicitly known then the optimal gain matrix
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1564245810055866,"Kt = −R−1B⊤E( ¯Yt ¯Y ⊤
t )"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.15828677839851024,"or else (when it is not) then (ii) deﬁne the Hamiltonian3 (or the Q-function) (Mehta & Meyn, 2009)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1601489757914339,"H(x, a, t) := 1"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.16201117318435754,2|Cx|2 + 1
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.16387337057728119,"2a⊤Ra + x⊤E( ¯Yt ¯Y ⊤
t )(Ax + Ba)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.16573556797020483,"from which the optimal control law is obtained as ut(x) = arg mina∈Rm H(x, a, t)."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.16759776536312848,"Finite-N approximation:
The mean-ﬁeld process is empirically approximated by simulating a
system of interacting particles {Yi
t ∈Rd : 0 ≤t ≤T, i = 1, . . . , N} according to"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.16945996275605213,"dYi
t = AYi
t dt + B d
 η
i
t
|
{z
}
i-th copy of model (1a)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1713221601489758,"+ S(N)
t
C⊤(CYi
t + Cn(N)
t
2
) dt
|
{z
}
RL step"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.17318435754189945,",
Yi
T
i.i.d
∼N(0, P −1
T )
(11)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1750465549348231,"Y i
t = (S(N)
t
)−1(Yi
t −n(N)
t
)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.17690875232774675,"where ηi is a copy of ¯η, and"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1787709497206704,"n(N)
t
= N −1
N
X"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.18063314711359404,"i=1
Yi
t,
S(N)
t
=
1
N −1 N
X"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1824953445065177,"i=1
(Yi
t −n(N)
t
)(Yi
t −n(N)
t
)⊤.
(12)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.18435754189944134,"Relationship to RL:
The following intuitive explanation is included to situate the proposed
simulation-based algorithm in the RL landscape:"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.186219739292365,"Representation: In designing any RL algorithm, the ﬁrst issue is representation – how does one rep-
resent the unknown value function (Pt in the linear case)? Our novel idea – the ﬁrst key innovation
of this paper – is to represent Pt is in terms of statistics (variance) of the particles. Such a represen-
tation is fundamentally distinct from representing the value function, or its proxies, such as the Q
function, in terms of a set of basis functions (Bradtke et al., 1994; Devraj et al., 2020; Maei et al.,
2010; Fujimoto et al., 2018; Melo et al., 2008)."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.18808193668528864,"The algorithm is entirely simulation based: N copies of the model (1a) are simulated in parallel
where the terms on the righthand-side of (11) have the following intuitive explanation:"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.18994413407821228,"Dynamics: The ﬁrst term “AYi
t dt” on the right-hand side of (11) is simply a copy of uncontrolled
dynamics in the model (1a)."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.19180633147113593,"Control: The second term is the control input “BUt dt” implemented as “B dηi
t”. That is, the control
input U for the i-th particle is a white noise process with covariance R−1. One may interpret this as
an exploration step whereby the cheaper control directions are explored more."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.19366852886405958,"In summary, for the i-th particle, the dynamics and control are the same as any RL algorithm (white
noise is used for exploration). The difference arises due to the third term (which is the second key
innovation of this paper)."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.19553072625698323,"RL step: The third term indicated as the RL step engenders a particle ﬂow that effectively imple-
ments the value iteration step of the RL. There are several points to be made:"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.1973929236499069,"1. The RL step is a function of the state cost term in (2). This is most easily seen by writing
the equation for the empirical mean"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.19925512104283055,"dn(N)
t
= (A + S(N)
t
C⊤C)n(N)
t
dt + B d
 η
(N)
t
,
n(N)
T
= 0"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.2011173184357542,"where η(N)
t
= N −1 P"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.20297951582867785,"i ηi
t. Noting that the state cost term is x⊤C⊤Cx, the RL step imple-
ments a gradient with S(N)
t
C⊤as the gain matrix."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.2048417132216015,"3The Hamiltonian H(x, a, t) is in the form of an oracle because (Ax + Ba) is the right-hand side of the
simulation model (1a)."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.20670391061452514,Under review as a conference paper at ICLR 2022
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.2085661080074488,"2. The RL step has a linear feedback control structure and serves to couple the particles.
Without the RL step, the particles are independent of each other.
3. This form of the RL step is possible only if one has access to a simulator and an ability
to add additional terms outside the control channel (same as the data assimilation step in
estimation). For example, this is not possible when the system exists only as an experiment."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.21042830540037244,"Arrow of simulation time: The particles are simulated backward – from terminal time t = T to initial
time t = 0. This is consistent with the dynamic programming (DP) equation which also proceeds
backward in time. However, because the focus of the RL is on the inﬁnite-horizon problem, this
important property of DP is not considered in algorithm design. It is noted that the inﬁnite-horizon
problem is easily handled in our formulation by taking T as suitably large (which is how the inﬁnite-
horizon limit is deﬁned in DP)."
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.2122905027932961,It remains to use the ﬁnite-N system to approximate the optimal control law.
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.21415270018621974,"Optimal control:
There are two cases to consider: (i) If the matrix B is explicitly known then4"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.21601489757914338,"K(N)
t
= −
1
N −1 N
X"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.21787709497206703,"i=1
R−1(B⊤Y i
t )(Y i
t )⊤
(13)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.21973929236499068,or else (when it is not) then (ii) approximate the Hamiltonian as
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.22160148975791433,"H(N)(x, a, t) := 1"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.22346368715083798,2|Cx|2 + 1
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.22532588454376165,"2a⊤Ra +
1
N −1 N
X"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.2271880819366853,"i=1
(x⊤Y i
t )(Y i
t )⊤(Ax + Ba)
|
{z
}
model (1a)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.22905027932960895,from which the optimal control law is obtained as
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.2309124767225326,"u(N)
t
(x) = arg min
a∈Rm H(N)(x, a, t)"
THE SEPARATION PRINCIPLE HINGES ON THE ASSUMPTION THAT THE CONTROL INPUT UT DOES NOT CHANGE THE OBSERVATION,0.23277467411545624,"There are several zeroth-order approaches to solve the minimization problem, e.g., by constructing
2-point estimators for the gradient. Since the objective function is quadratic and the matrix R is
known, m queries of H(N)(x, ·, t) are sufﬁcient to compute u(N)
t
(x)."
ENKF-BASED ALGORITHM FOR THE LQG CONTROLLER,0.2346368715083799,"2.3
ENKF-BASED ALGORITHM FOR THE LQG CONTROLLER"
ENKF-BASED ALGORITHM FOR THE LQG CONTROLLER,0.23649906890130354,"By combining the result of steps 1 and 2, the optimal control input Ut = 
 "
ENKF-BASED ALGORITHM FOR THE LQG CONTROLLER,0.2383612662942272,"K(N)
t
ˆX(N)
t
if B is known"
ENKF-BASED ALGORITHM FOR THE LQG CONTROLLER,0.24022346368715083,"u(N)
t
( ˆX(N)
t
)
o.w."
ENKF-BASED ALGORITHM FOR THE LQG CONTROLLER,0.24208566108007448,"The overall algorithm including steps 1, 2 and 3 is tabulated in the Appendix C where additional
remarks on numerical approximation of backward SDEs also appear."
CONVERGENCE AND ERROR ANALYSIS,0.24394785847299813,"2.4
CONVERGENCE AND ERROR ANALYSIS"
CONVERGENCE AND ERROR ANALYSIS,0.24581005586592178,"The mean-ﬁeld process (9) represents the mean-ﬁeld limit of the ﬁnite-N system (11), as the number
of particles N →∞. The convergence analysis is a delicate matter based on the propagation of
chaos (Bishop & Del Moral, 2018). In Appendix D, under certain additional assumptions on system
matrices, the following error bound is derived:"
CONVERGENCE AND ERROR ANALYSIS,0.24767225325884543,"E[∥S(N)
t
−¯St∥2
F ] ≤3∥¯ST ∥2
F
N
e−(4µ−1"
CONVERGENCE AND ERROR ANALYSIS,0.24953445065176907,N )(T −t) + cvar
CONVERGENCE AND ERROR ANALYSIS,0.25139664804469275,"N ,
0 ≤t ≤T
(14)"
CONVERGENCE AND ERROR ANALYSIS,0.2532588454376164,where µ and cvar are positive constants and ∥· ∥F is the Frobenius (matrix) norm.
CONVERGENCE AND ERROR ANALYSIS,0.25512104283054005,"The computational complexity of an ENKF is O(Nd). EnKF is a workhorse in applications such as
weather prediction where models are simulation-based (Evensen, 2006; Reich & Cotter, 2015). In
these applications, the number of simulations N << d."
CONVERGENCE AND ERROR ANALYSIS,0.2569832402234637,"4Eq. (13) for the optimal control gain matrix K(N)
t
is dual to the eq. (6) for optimal ﬁlter gain matrix L(N)
t
."
CONVERGENCE AND ERROR ANALYSIS,0.25884543761638734,Under review as a conference paper at ICLR 2022
COMPARISON WITH RELATED WORK,0.260707635009311,"2.5
COMPARISON WITH RELATED WORK"
COMPARISON WITH RELATED WORK,0.26256983240223464,"Classical RL algorithms for the LQR problem are based on a linear function approximation, us-
ing quadratic basis functions, of the value function or its surrogate the Q-function (Bradtke et al.,
1994). Convergence guarantees typically require (i) a persistence of excitation condition, and (ii)
use of the on policy methods whereby the parameters are learned for a given ﬁxed policy (which is
subsequently improved). For the deterministic LQR problem, the persistence of excitation condi-
tion is difﬁcult to justify theoretically, and in practice can lead to poor performance related to slow
convergence rates. These limitations have spurred recent research on the LQR problem."
COMPARISON WITH RELATED WORK,0.2644320297951583,"Our algorithm is compared to the model-free policy optimization based methods of Mohammadi
et al. (2021a) and Fazel et al. (2018). In policy optimization, the value J(U) is minimized over
the search space of stabilizing gain matrices, using a gradient-descent approach, starting from a
stabilizing controller (which is not straightforward to obtain in a model-free setting). For each
perturbation of the gain matrix, the gradient is estimated by simulating N particles (trajectories)
over a time-horizon [0, T]. Multiple iterations are needed to converge to the minimizer."
COMPARISON WITH RELATED WORK,0.26629422718808193,"In contrast, using the EnKF, the matrix Pt is approximated by simulating N particles once over the
time-horizon [0, T]. The optimal control is obtained from an application of the minimum principle
that only requires m evaluations at each t (the minimization of Hamiltonian is an online calculation)."
COMPARISON WITH RELATED WORK,0.2681564245810056,"The trade-off between the two approaches is as follows: While policy optimization methods require
multiple iterations with a small number of particles, the EnKF requires only a single iteration with
relatively larger number of particles. As illustrated with the aid of numerical examples in Sec. 3,
this can lead to an order of magnitude gain in the computation time."
COMPARISON WITH RELATED WORK,0.27001862197392923,"As a ﬁnal point, our paper provides a simulation-based algorithm for the most general class of linear
quadratic problem: fully or partially observed, ﬁnite or inﬁnite-horizon, stochastic or deterministic,
and furthermore does not require an initial stabilizing controller."
COMPARISON WITH RELATED WORK,0.2718808193668529,"Duality:
In Appendix E, it is shown that the Gaussian density of the random variable ¯Yt equals a
log transformation of the value function (at time t) of the LQG optimal control problem. Additional
remarks are also included to expound the duality roots of the proposed algorithm, including a survey
of the relevant literature on this beautiful subject."
NUMERICAL SIMULATIONS,0.2737430167597765,"3
NUMERICAL SIMULATIONS"
NUMERICAL SIMULATIONS,0.2756052141527002,"In all the following three numerical examples, we consider the inﬁnite-horizon fully observed deter-
ministic LQR problem. Although our algorithms are more generally applied, such a choice allows
us to compare and contrast with the recent RL work on the LQR problem. For the LQR problem,
the solution of the ARE is denoted P∞and the associated optimal feedback gain is denoted K∞."
NUMERICAL SIMULATIONS,0.2774674115456238,"In each of the three numerical examples, a time-horizon T is ﬁxed. Over the time-horizon, the ﬁnite-
N EnKF algorithm (11) is run to obtain an empirical approximation {P (N)
t
∈Rd×d : 0 ≤t ≤T}.
For the sake of comparison, the exact {Pt ∈Rd×d : 0 ≤t ≤T} is obtained by numerically solving
the backward DRE (7). Typically, we chose PT = I, the identity matrix."
EXPONENTIAL CONVERGENCE,0.27932960893854747,"3.1
EXPONENTIAL CONVERGENCE"
EXPONENTIAL CONVERGENCE,0.2811918063314711,"An attractive feature of the proposed EnKF algorithm is that the mean-ﬁeld limit is exact. This means
the EnKF recovers the properties of the DRE in the limit. One such property is the exponential
convergence: For any ﬁxed t, Pt →P∞as T →∞, starting from any initialization PT (Ocone
& Pardoux, 1996, Remark 2.1). Moreover, because of the error formula (14), P (N)
t
→P∞with a
small additional bias that decays as O( 1 N )."
EXPONENTIAL CONVERGENCE,0.28305400372439476,"For a d = 2 dimensional system, Fig. 1 depicts this exponential convergence of the four entries
of the symmetric P (N)
t
matrix (using N = 100 particles). As part of Appendix F.1, additional
ﬁgures are included to depict exponential convergence of the 100 entries of the 10×10 matrix P (N)
t
for a d = 10 dimensional system (using N = 1000 particles). In these evaluations, the system
(A, B) is chosen to be in its controllable canonical form where the appropriate entries (last row)
of the A matrix are randomly generated. Because these are randomly generated, the matrix A is"
EXPONENTIAL CONVERGENCE,0.2849162011173184,Under review as a conference paper at ICLR 2022
EXPONENTIAL CONVERGENCE,0.28677839851024206,"−0.5
0.0
(a) −0.50 −0.25 0.00 0.25 0.50 OL
CL"
EXPONENTIAL CONVERGENCE,0.2886405959031657,"0
4
8
(b) 1.0 1.2 1.4 1.6 1.8 P11"
EXPONENTIAL CONVERGENCE,0.2905027932960894,"EnKF
DRE
ARE"
EXPONENTIAL CONVERGENCE,0.29236499068901306,"0
4
8
(c) 1.00 1.25 1.50 1.75 2.00 2.25 P22"
EXPONENTIAL CONVERGENCE,0.2942271880819367,"T −t
0
4
8
(d) 0.0 0.2 0.4 0.6 0.8"
IM,0.29608938547486036,"1.0
Im Re"
IM,0.297951582867784,P21 = P12
IM,0.29981378026070765,"Figure 1: The d = 2 example: (a) Open-loop (OL) and closed-loop (CL) eigenvalues; (b)-(d)
Convergence of P N
t : Plot of the entries of the matrix P (N)
t
shows exponential convergence, with
a small (random) error, to the ARE limit P∞which is also depicted together with the exact DRE
solution. (Note the x-axis of the plots (b)-(d) is T −t so convergence is easy to see)."
IM,0.3016759776536313,"often unstable (see also Fig. 1 (a)) which does not pose any problem with our algorithm. Additional
details on the numerics appear as part of Appendix F.1."
IM,0.30353817504655495,"3.2
EVALUATION AND COMPARISON WITH LITERATURE: MASS SPRING DAMPER SYSTEM"
IM,0.3054003724394786,"In order to evaluate and compare the performance of the proposed algorithm, we describe next the
results for the coupled mass-spring system studied in Mohammadi et al. (2019). For each mass,
there are two states, position and velocity. A system with d/2 masses is d-dimensional. To evaluate
the performance of the ﬁnite-N algorithm, we consider the following error metric:"
IM,0.30726256983240224,MSE := 1 T E Z T 0
IM,0.3091247672253259,"∥Pt −P (N)
t
∥2
F
∥Pt∥2
F
dt !"
IM,0.31098696461824954,"The expectation is approximated empirically by averaging over 100 simulation runs. See Appendix
F.2 for details on modeling, parameter values, and the numerical discretization. Fig. 2 depicts the
results of the numerical experiments showing the O( 1"
IM,0.3128491620111732,N ) decay of MSE as N increases (for d ﬁxed).
IM,0.31471135940409684,"Part (c) of the ﬁgure depicts a comparison with the two state-of-the-art algorithms, namely Moham-
madi et al. (2021b) and Fazel et al. (2018). In the ﬁgure, computation time is plotted against the
relative error in approximating the LQR gain K∞(plots are qualitatively similar using other error
metrics). The computation time is obtained using the Python process time() function from
the time library for measuring execution time. The result depicted in the ﬁgure is for d = 10 but
qualitatively similar results are also obtained for other values of d. See Appendix F.3 for details
on numerical implementation of the algorithms. We also implemented the classical RL algorithm
in Bradtke et al. (1994) but its convergence was not reliable because of the persistence of excitation
issues."
IM,0.3165735567970205,"102
103
Number of particles (N) 10−3 10−2 10−1 100 101 102 MSE"
IM,0.31843575418994413,"d = 2
d = 10
d = 20
d = 50
d = 80"
IM,0.3202979515828678,"0
20
40
60
80
Dimensions (d) 10−2 10−1 100 101 102 MSE"
IM,0.3221601489757914,"N = 100
N = 300
N = 1000"
IM,0.3240223463687151,"0.05
0.10
0.15
Relative error in gain 100 101 102 103 104 105"
IM,0.3258845437616387,Comp. Time (s)
IM,0.32774674115456237,O(1/N)
IM,0.329608938547486,"EnKF
[M21] [F18]"
IM,0.33147113594040967,"Figure 2: (a)-(b) Plot of MSE as a function of the number of particles N and system dimension d;
(c) Comparison with algorithms in Fazel et al. (2018) (labeled [F18]) and Mohammadi et al. (2021b)
(labeled [M21]). The comparisons depict the computation time (in a Python implementation) as a
function of the relative error in approximating the LQR gain K∞."
IM,0.3333333333333333,Under review as a conference paper at ICLR 2022
IM,0.33519553072625696,"Based on the preliminary analysis, the main reason for the order of magnitude improvement in
computational time is as follows: An EnKF requires only a single iteration over a ﬁxed time-horizon
[0, T]. Although the number of particles (N) is an order of magnitude larger for the EnKF algorithm,
certain vectorisation features of numpy yield signiﬁcant gains in computational time. Because the
other algorithms require multiple iterations which must necessarily be carried out serially, these
computations are slower. In our comparisons, we used the same time-horizon T and discretization
time-step δt for all the algorithms. It is certainly possible that some of these parameters can be
optimized to improve the performance of the other algorithms. In particular, one may consider
shorter or longer time-horizon T or use parallelization to speed up the gradient calculation."
EVALUATION OF ENKF FOR A NONLINEAR CART-POLE SYSTEM,0.3370577281191806,"3.3
EVALUATION OF ENKF FOR A NONLINEAR CART-POLE SYSTEM"
EVALUATION OF ENKF FOR A NONLINEAR CART-POLE SYSTEM,0.33891992551210426,"It is a time-honored practice to design the optimal control law for a (linearized) LQR model and
then implement this control law on a nonlinear system. By the principle of linearization, the control
law works well (in theory) for small perturbations and often (in practice) for even large ones. We
consider the nonlinear conservative cart pole model (Tedrake; Rawlik et al., 2013). The control acts
as external force applied to the cart. The four-dimensional state for the system is (θ, x, ω, v), where
θ ∈S1 (the circle) is the angle of the pole (pendulum) as measured from the stable equilibrium,
x ∈R is the displacement of cart along the horizontal, and (ω, v) := ( ˙θ, ˙x) ∈R2 is the velocity
vector. The control objective is to balance the pole – stabilize the system at the inverted equilibrium
(π, 0, 0, 0), assuming full state feedback. For the purposes of control design, the nonlinear system
is ﬁrst linearized at the desired equilibrium and an LQR problem is formulated (see Appendix F.4
for details). For the purposes of evaluation, the optimal control is applied to the nonlinear model.
Figure 3 depict the results of numerical experiments for three different choices of N. It is seen that
as few as N = 10 particles are sufﬁcient to stabilize the equilibrium. Using N = 1000 particles, the
closed-loop trajectories are virtually indistinguishable from the DRE-based solution."
EVALUATION OF ENKF FOR A NONLINEAR CART-POLE SYSTEM,0.3407821229050279,Figure 3: Trajectories of the closed-loop nonlinear cart pole system.
CONCLUSIONS,0.3426443202979516,"4
CONCLUSIONS"
CONCLUSIONS,0.34450651769087526,"In recent years, there has been a concerted effort to revisit the LQR problem in the context of
RL (Fazel et al., 2018; Mohammadi et al., 2021b; Tu & Recht, 2019; Dean et al., 2020; Malik et al.,
2020; Zhang et al., 2020; Jansch-Porto et al., 2020). The effort is largely in response to the well
known issues that arise when the state and action space are continuous (Lu et al., 2021)."
CONCLUSIONS,0.3463687150837989,"In this paper, we present a new paradigm for RL. There are two key innovations: (i) the representa-
tion of the unknown value function in terms of the statistics (variance) of the particles; and (ii) design
of interactions between simulations to solve the optimal control problem. For the LQR problem, this
is shown to yield a learning rate that closely approximates the exponential rate of convergence of the
solution of the DRE. In numerical examples, this property is shown to lead to an order of magnitude
better performance than the state-of-the-art algorithms. Given the enormous success of EnKF in
data assimilation (Evensen, 2006; Reich & Cotter, 2015), the contributions of this paper potentially
open up new opportunities for RL. It is our hope that the paper will engender new synergies between
the data assimilation and the RL communities."
CONCLUSIONS,0.34823091247672255,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.3500931098696462,"5
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.35195530726256985,"The implementable pseudo-code of the algorithm is tabulated in Appendix C. For the three exam-
ples in Sec. 3, the numerical parameters are tabulated in Appendix F. The codes for all numerical
simulations are provided as part of the supplementary material. All the codes have a README ﬁle
associated with it. Table 1 provides a list of the Appendix containing the simulation parameters and
location of codes for each numerical example."
REPRODUCIBILITY STATEMENT,0.3538175046554935,Table 1: Location of supplementary material
REPRODUCIBILITY STATEMENT,0.35567970204841715,"Section
Appendix
Code Directory"
REPRODUCIBILITY STATEMENT,0.3575418994413408,"3.1
F.1
Section 3-1"
REPRODUCIBILITY STATEMENT,0.35940409683426444,"3.2
F.2, F.3
Section 3-2"
REPRODUCIBILITY STATEMENT,0.3612662942271881,"3.3
F.4
Section 3-3"
REFERENCES,0.36312849162011174,REFERENCES
REFERENCES,0.3649906890130354,"P. Benner and Z. Bujanovi´c. On the solution of large-scale algebraic Riccati equations by using
low-dimensional invariant subspaces. Linear Algebra and its Applications, 488:430–459, 2016."
REFERENCES,0.36685288640595903,"K. Bergemann and S. Reich. An ensemble Kalman-Bucy ﬁlter for continuous data assimilation.
Meteorologische Zeitschrift, 21(3):213–219, 2012. doi: 10.1127/0941-2948/2012/0307."
REFERENCES,0.3687150837988827,"A. N. Bishop and P. Del Moral. On the stability of matrix-valued Riccati diffusions. arXiv preprint
arXiv:1808.00235, 2018. URL https://arxiv.org/abs/1808.00235."
REFERENCES,0.37057728119180633,"A. N. Bishop, P. Del Moral, K. Kamatani, B. Remillard, et al. On one-dimensional riccati diffusions.
Annals of Applied Probability, 29(2):1127–1187, 2019."
REFERENCES,0.37243947858473,"Sergio Bittanti, Alan J Laub, and Jan C Willems. The Riccati Equation. Springer Science & Business
Media, 2012."
REFERENCES,0.3743016759776536,"S.J. Bradtke, B.E. Ydstie, and A.G. Barto. Adaptive linear quadratic control using policy iteration.
In Proceedings of 1994 American Control Conference - ACC ’94, volume 3, pp. 3475–3479 vol.3,
1994. doi: 10.1109/ACC.1994.735224."
REFERENCES,0.3761638733705773,"R. W Brockett. Finite dimensional linear systems. SIAM, 2015."
REFERENCES,0.3780260707635009,"S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu.
On the Sample Complexity of the
Linear Quadratic Regulator.
Found Comput Math, 20(4):633–679, August 2020.
ISSN
1615-3383.
doi:
10.1007/s10208-019-09426-y.
URL https://doi.org/10.1007/
s10208-019-09426-y."
REFERENCES,0.37988826815642457,"P. Del Moral and J. Tugaut.
On the stability and the uniform propagation of chaos proper-
ties of ensemble KalmanBucy ﬁlters.
Ann. Appl. Probab., 28(2):790–850, 04 2018.
doi:
10.1214/17-AAP1317. URL https://doi.org/10.1214/17-AAP1317."
REFERENCES,0.3817504655493482,"A. M. Devraj, A. Busic, and S. Meyn. Fundamental design principles for reinforcement learning
algorithms. In K. G. Vamvoudakis, Y.Wan, F. L. Lewis, and D. Cansever (eds.), Handbook on
Reinforcement Learning and Control. Springer, 2020."
REFERENCES,0.38361266294227186,"G. Evensen. Sequential data assimilation with a nonlinear quasi-geostrophic model using Monte
Carlo methods to forecast error statistics. Journal of Geophysical Research: Oceans, 99(C5):
10143–10162, 1994. doi: 10.1029/94JC00572."
REFERENCES,0.3854748603351955,"G. Evensen. Data Assimilation. The Ensemble Kalman Filter. Springer-Verlag, New York, 2006."
REFERENCES,0.38733705772811916,"M. Fazel, R. Ge, S. Kakade, and M. Mesbahi. Global Convergence of Policy Gradient Methods for
the Linear Quadratic Regulator. In International Conference on Machine Learning, pp. 1467–
1476. PMLR, July 2018.
URL http://proceedings.mlr.press/v80/fazel18a.
html. ISSN: 2640-3498."
REFERENCES,0.3891992551210428,Under review as a conference paper at ICLR 2022
REFERENCES,0.39106145251396646,"W. H. Fleming. Exit probabilities and optimal stochastic control. Applied Mathematics and Opti-
mization, 4(1):329–346, 1977."
REFERENCES,0.3929236499068901,"W. H. Fleming and S. K. Mitter.
Optimal Control and Nonlinear Filtering for Nondegenerate
Diffusion Processes. Stochastics, 8(1):63–77, January 1982. ISSN 0090-9491. doi: 10.1080/
17442508208833228. URL https://doi.org/10.1080/17442508208833228."
REFERENCES,0.3947858472998138,"S. Fujimoto, H. van Hoof, and D. Meger. Addressing function approximation error in actor-critic
methods.
In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,
pp. 1587–1596. PMLR, 10–15 Jul 2018. URL http://proceedings.mlr.press/v80/
fujimoto18a.html."
REFERENCES,0.39664804469273746,"T. T. Georgiou and A. Lindquist. The separation principle in stochastic control, redux. IEEE Trans-
actions on Automatic Control, 58(10):2481–2494, 2013."
REFERENCES,0.3985102420856611,"C. Hartmann and C. Sch¨utte. Efﬁcient rare event simulation by optimal nonequilibrium forcing.
Journal of Statistical Mechanics: Theory and Experiment, 2012(11):P11004, 2012."
REFERENCES,0.40037243947858475,"C. Hoffmann and P. Rostalski. Linear optimal control on factor graphs a message passing per-
spective .
IFAC-PapersOnLine, 50(1):6314–6319, 2017.
ISSN 2405-8963.
doi: https://doi.
org/10.1016/j.ifacol.2017.08.914. URL https://www.sciencedirect.com/science/
article/pii/S2405896317313800. 20th IFAC World Congress."
REFERENCES,0.4022346368715084,"P. L. Houtekamer and H. L. Mitchell. A sequential ensemble Kalman ﬁlter for atmospheric data
assimilation. Monthly Weather Review, 129(1):123–137, 2001."
REFERENCES,0.40409683426443205,"J. P. Jansch-Porto, B. Hu, and G. E. Dullerud.
Convergence guarantees of policy optimization
methods for Markovian jump linear systems. In 2020 American Control Conference (ACC), pp.
2882–2887. IEEE, 2020."
REFERENCES,0.4059590316573557,"R. E. Kalman. Contributions to the theory of optimal control. Boletin de la Sociedad Matematica
Mexicana (2), 5:102–109, 1960."
REFERENCES,0.40782122905027934,"H. J. Kappen and H. C. Ruiz. Adaptive importance sampling for control and inference. Journal of
Statistical Physics, 162(5):1244–1266, 2016."
REFERENCES,0.409683426443203,"H. J. Kappen, V. G´omez, and M. Opper.
Optimal control as a graphical model inference
problem.
Machine Learning, 87(2):159–182, May 2012.
ISSN 1573-0565.
doi: 10.1007/
s10994-012-5278-7. URL https://doi.org/10.1007/s10994-012-5278-7."
REFERENCES,0.41154562383612664,"J. W. Kim and P. G. Mehta. An optimal control derivation of nonlinear smoothing equations. In
Proceedings of the Workshop on Dynamics, Optimization and Computation held in honor of the
60th birthday of Michael Dellnitz, pp. 295–311. Springer, 2020."
REFERENCES,0.4134078212290503,"Peter E Kloeden and Eckhard Platen.
Numerical Solution of Stochastic Differential Equations.
Springer, Applications of Mathematics, 1999."
REFERENCES,0.41527001862197394,"Huibert Kwakernaak and Raphael Sivan. Linear optimal control systems. Wiley Interscience, New
York, 1972. ISBN 0471511102."
REFERENCES,0.4171322160148976,"P. Lancaster and L. Rodman. Algebraic Riccati Equations. Clarendon press, 1995."
REFERENCES,0.41899441340782123,"A. J. Laub. Invariant subspace methods for the numerical solution of Riccati equations. In The
Riccati Equation, pp. 163–196. Springer, 1991."
REFERENCES,0.4208566108007449,"Daniel Liberzon. Calculus of Variations and Optimal Control Theory. Princeton University Press,
Princeton, NJ, 2012. ISBN 978-0-691-15187-8. A concise introduction."
REFERENCES,0.4227188081936685,"F. Lu, P. G. Mehta, S. P. Meyn, and G. Neu. Convex q-learning. In 2021 American Control Confer-
ence (ACC), pp. 4749–4756, 2021. doi: 10.23919/ACC50511.2021.9483244."
REFERENCES,0.4245810055865922,Under review as a conference paper at ICLR 2022
REFERENCES,0.4264432029795158,"H. R. Maei, C. Szepesv´ari, S. Bhatnagar, and R. S. Sutton. Toward off-policy learning control with
function approximation. In Proceedings of the 27th International Conference on International
Conference on Machine Learning, ICML’10, pp. 719726, Madison, WI, USA, 2010. Omnipress.
ISBN 9781605589077."
REFERENCES,0.42830540037243947,"D. Malik, A.n Pananjady, K. Bhatia, K. Khamaru, P. L. Bartlett, and M. J. Wainwright. Derivative-
Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems. Journal of
Machine Learning Research, 21(21):1–51, 2020. ISSN 1533-7928. URL http://jmlr.org/
papers/v21/19-198.html."
REFERENCES,0.4301675977653631,"P. G. Mehta and S. P. Meyn. Q-learning and Pontryagin’s minimum principle. In Proceedings of
the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese
Control Conference, pp. 3598–3605. IEEE, 2009."
REFERENCES,0.43202979515828677,"F. S. Melo, S. P. Meyn, and M. I. Ribeiro.
An analysis of reinforcement learning with func-
tion approximation. In Proceedings of the 25th International Conference on Machine Learn-
ing, ICML ’08, pp. 664671, New York, NY, USA, 2008. Association for Computing Machin-
ery. ISBN 9781605582054. doi: 10.1145/1390156.1390240. URL https://doi.org/10.
1145/1390156.1390240."
REFERENCES,0.4338919925512104,"S. K. Mitter and N. J. Newton. A variational approach to nonlinear estimation. SIAM journal on
control and optimization, 42(5):1813–1833, 2003."
REFERENCES,0.43575418994413406,"H. Mohammadi, A. Zare, M. Soltanolkotabi, and M. R. Jovanovic. Global exponential convergence
of gradient methods over the nonconvex landscape of the linear quadratic regulator. In 2019
IEEE 58th Conference on Decision and Control (CDC), pp. 7474–7479, December 2019. doi:
10.1109/CDC40024.2019.9029985. ISSN: 2576-2370."
REFERENCES,0.4376163873370577,"H. Mohammadi, M. R. Jovanovic, and M. Soltanolkotabi. Learning the model-free linear quadratic
regulator via random search. In Learning for Dynamics and Control, pp. 531–539. PMLR, July
2020a. URL http://proceedings.mlr.press/v120/mohammadi20a.html. ISSN:
2640-3498."
REFERENCES,0.43947858472998136,"H. Mohammadi, M. Soltanolkotabi, and M. R. Jovanovic. Random search for learning the linear
quadratic regulator. In 2020 American Control Conference (ACC), pp. 4798–4803, July 2020b.
doi: 10.23919/ACC45564.2020.9147749. ISSN: 2378-5861."
REFERENCES,0.441340782122905,"H. Mohammadi, M. Soltanolkotabi, and M. R. Jovanovic. On the Linear Convergence of Random
Search for Discrete-Time LQR. IEEE Control Systems Letters, 5(3):989–994, July 2021a. ISSN
2475-1456. doi: 10.1109/LCSYS.2020.3006256. Conference Name: IEEE Control Systems
Letters."
REFERENCES,0.44320297951582865,"Hesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo R. Jovanovic. Conver-
gence and sample complexity of gradient methods for the model-free linear quadratic regulator
problem. IEEE Transactions on Automatic Control, pp. 1–1, 2021b. doi: 10.1109/TAC.2021.
3087455."
REFERENCES,0.4450651769087523,"Hesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo R. Jovanovi. Conver-
gence and sample complexity of gradient methods for the model-free linear quadratic regula-
tor problem. arXiv:1912.11899 [physics], March 2021c. URL http://arxiv.org/abs/
1912.11899. arXiv: 1912.11899."
REFERENCES,0.44692737430167595,"D. Nualart and ´E. Pardoux. Stochastic calculus with anticipating integrands. Probability Theory and
Related Fields, 78(4):535–581, 1988."
REFERENCES,0.44878957169459965,"D. Ocone and E. Pardoux.
Asymptotic stability of the optimal ﬁlter with respect to its initial
condition. SIAM Journal on Control and Optimization, 34(1):226–243, 1996. doi: 10.1137/
s0363012993256617."
REFERENCES,0.4506517690875233,"K. Rawlik, M. Toussaint, and S. Vijayakumar. On stochastic optimal control and reinforcement
learning by approximate inference. In Twenty-third international joint conference on artiﬁcial
intelligence, 2013."
REFERENCES,0.45251396648044695,Under review as a conference paper at ICLR 2022
REFERENCES,0.4543761638733706,"S. Reich and C. Cotter. Probabilistic forecasting and Bayesian data assimilation. Cambridge Uni-
versity Press, 2015."
REFERENCES,0.45623836126629425,"C. Sch¨utte, S. Winkelmann, and C. Hartmann. Optimal control of molecular dynamics using markov
state models. Mathematical programming, 134(1):259–282, 2012."
REFERENCES,0.4581005586592179,"T. Sutter, A. Ganguly, and H. Koeppl. A variational approach to path estimation and parameter
inference of hidden diffusion processes. Journal of Machine Learning Research, 17:6544–80,
2016."
REFERENCES,0.45996275605214154,"A. Taghvaei and P. G. Mehta. An optimal transport formulation of the ensemble Kalman ﬁlter. IEEE
Transactions on Automatic Control, pp. 1–1, 2020. doi: 10.1109/TAC.2020.3015410."
REFERENCES,0.4618249534450652,"Amirhossein Taghvaei. Design and analysis of particle-based algorithms for nonlinear ﬁltering and
sampling. PhD thesis, University of Illinois at Urbana-Champaign, 2019."
REFERENCES,0.46368715083798884,"Amirhossein Taghvaei and Prashant G Mehta. Error analysis for the linear feedback particle ﬁlter. In
2018 Annual American Control Conference (ACC), pp. 4261–4266. IEEE, 2018. doi: 10.23919/
ACC.2018.8430867."
REFERENCES,0.4655493482309125,"R. Tedrake. Underactuated Robotics: Algorithms for Walking, Running, Swimming, Flying, and
Manipulation (course Notes for MIT 6.832). URL http://underactuated.mit.edu/.
Last accessed on 16 May 2021."
REFERENCES,0.46741154562383613,"E. Theodorou, J. Buchli, and S. Schaal. A generalized path integral control approach to reinforce-
ment learning. The Journal of Machine Learning Research, 11:3137–3181, 2010."
REFERENCES,0.4692737430167598,"E. Todorov. General duality between optimal control and estimation. In 2008 47th IEEE Conference
on Decision and Control, pp. 4286–4292, Dec 2008."
REFERENCES,0.47113594040968343,"E. Todorov. Efﬁcient computation of optimal actions. Proceedings of the national academy of
sciences, 106(28):11478–11483, 2009."
REFERENCES,0.4729981378026071,"M. Toussaint and A. Storkey.
Probabilistic inference for solving discrete and continuous state
markov decision processes.
ICML ’06, pp. 945952, New York, NY, USA, 2006. Associa-
tion for Computing Machinery.
ISBN 1595933832.
doi: 10.1145/1143844.1143963.
URL
https://doi.org/10.1145/1143844.1143963."
REFERENCES,0.4748603351955307,"S. Tu and B. Recht.
The Gap Between Model-Based and Model-Free Methods on the Linear
Quadratic Regulator: An Asymptotic Viewpoint. In Conference on Learning Theory, pp. 3036–
3083. PMLR, June 2019. URL http://proceedings.mlr.press/v99/tu19a.html.
ISSN: 2640-3498."
REFERENCES,0.4767225325884544,"R. Van Handel. Filtering, stability, and robustness. PhD thesis, California Institute of Technology,
2006."
REFERENCES,0.478584729981378,"R. Van Handel. Stochastic calculus, ﬁltering, and stochastic control. Course notes., URL http://www.
princeton. edu/rvan/acm217/ACM217. pdf, 14, 2007."
REFERENCES,0.48044692737430167,"J. Watson and J. Peters. Advancing trajectory optimization with approximate inference: Exploration,
covariance control and adaptive risk. In 2021 American Control Conference (ACC), pp. 1231–
1236, 2021. doi: 10.23919/ACC50511.2021.9482657."
REFERENCES,0.4823091247672253,"J. Watson, H. Abdulsamad, and J. Peters. Stochastic optimal control as approximate input inference.
In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura (eds.), Proceedings of the Confer-
ence on Robot Learning, volume 100 of Proceedings of Machine Learning Research, pp. 697–
716. PMLR, 30 Oct–01 Nov 2020.
URL https://proceedings.mlr.press/v100/
watson20a.html."
REFERENCES,0.48417132216014896,"J. Watson, H. Abdulsamad, R. Findeisen, and J. Peters. Stochastic control through approximate
bayesian input inference, 2021."
REFERENCES,0.4860335195530726,Under review as a conference paper at ICLR 2022
REFERENCES,0.48789571694599626,"K. Zhang, B. Hu, and T. Basar. On the Stability and Convergence of Robust Adversarial Reinforce-
ment Learning: A Case Study on Linear Quadratic Systems. Advances in Neural Information
Processing Systems, 33:22056–22068, 2020. URL https://proceedings.neurips.cc/
/paper/2020/hash/fb2e203234df6dee15934e448ee88971-Abstract.html."
REFERENCES,0.4897579143389199,APPENDIX
REFERENCES,0.49162011173184356,"A
PROOF OF PROP. 1"
REFERENCES,0.4934823091247672,"It is ﬁrst shown that the conditional mean ¯mt and the conditional covariance ¯Σt evolve according
to the Kalman ﬁlter equations (3). The SDE for the mean is obtained by taking the conditional
expectation of (4),
d ¯mt = A ¯mt dt + BUt dt + ¯Lt( dZt −H ¯mt dt)
where we used the fact that Ut is assumed to be Zt measurable."
REFERENCES,0.49534450651769085,"To obtain the equation for the covariance, deﬁne the error process et = ¯Xt −¯mt which evolves
according to"
REFERENCES,0.4972067039106145,det = (A −1
REFERENCES,0.49906890130353815,"2
¯LtH)et dt + d¯ξt"
REFERENCES,0.5009310986964618,"Then, upon an application of the Itˆo rule"
REFERENCES,0.5027932960893855,"d(ete⊤
t ) = (A −1"
REFERENCES,0.5046554934823091,"2
¯LtH)(ete⊤
t ) dt + (ete⊤
t )(A −1"
REFERENCES,0.5065176908752328,"2
¯LtH)⊤dt + Q dt + d¯ξte⊤
t + et d¯ξ⊤
t"
REFERENCES,0.5083798882681564,"and taking the conditional expectation, the equation for the conditional covariance ¯Σt = E[ete⊤
t |Zt]
is obtained as
d
dt
¯Σt = A¯Σt + ¯ΣtA⊤+ Q −¯ΣtH⊤H ¯Σt"
REFERENCES,0.5102420856610801,where we used the deﬁnition ¯Lt = ¯ΣtH⊤.
REFERENCES,0.5121042830540037,"The equation for the conditional covariance ¯Σt is identical to the DRE (3b). Therefore, if the initial
condition ¯Σ0 = Σ0 then ¯Σt = Σt for all t > 0. This also implies ¯Lt = Lt, which in turn implies
that the SDE for the conditional mean ¯mt is identical to Kalman ﬁlter equation for mt. If ¯m0 = m0
then ¯mt = mt for all t > 0."
REFERENCES,0.5139664804469274,"By replacing the mean-ﬁeld terms ¯mt and ¯Σt by exogenous processes mt and Σt, the McKean-
Vlasov SDE (4) simpliﬁes to an Ornstein-Uhlenbeck SDE. Because the distribution of the initial
condition ¯X0 is Gaussian, the distribution of ¯Xt is also Gaussian and given by N(mt, Σt)."
REFERENCES,0.515828677839851,"B
PROOF OF PROP. 2"
REFERENCES,0.5176908752327747,"The proof is similar to the proof of Prop. 1. The equation for the mean ¯nt is obtained by taking the
expectation of SDE (9),
d¯nt = (A + ¯StC⊤C)¯nt dt
Because ¯nT = 0, we have ¯nt = 0 for all t ∈[0, T]."
REFERENCES,0.5195530726256983,The equation for the covariance ¯St is obtained by writing the SDE for the error et := ¯Yt −¯nt:
REFERENCES,0.521415270018622,det = (A + 1
REFERENCES,0.5232774674115456,"2
¯StC⊤C)et dt + B d
 ¯ηt,"
REFERENCES,0.5251396648044693,"Using the Itˆo rule for ete⊤
t ,"
REFERENCES,0.527001862197393,"d(ete⊤
t ) = (A+1"
REFERENCES,0.5288640595903166,"2
¯StC⊤C)(ete⊤
t ) dt+(ete⊤
t )(A+1"
REFERENCES,0.5307262569832403,"2
¯StC⊤C)⊤−BR−1B⊤+B d
 ¯ηte⊤
t +et(B d
 ¯ηt)⊤"
REFERENCES,0.5325884543761639,"The Itˆo correction term appears with a negative sign because the SDE involves a backward Wiener
process
 ¯ηt (Nualart & Pardoux, 1988, Sec. 4.2). Taking an expectation yields the following equation
for ¯St:
d
dt
¯St = (A + 1"
REFERENCES,0.5344506517690876,"2
¯StC⊤C) ¯St + ¯St(A + 1"
REFERENCES,0.5363128491620112,"2
¯StC⊤C)⊤−BR−1B⊤"
REFERENCES,0.5381750465549349,Under review as a conference paper at ICLR 2022
REFERENCES,0.5400372439478585,"The SDE is identical to the SDE for St. Because ¯ST = ST , we have ¯St = St for all t ∈[0, T]. The
conclusion that ¯Yt is Gaussian follows from the fact that with ¯nt = nt and ¯St = St, the SDE for ¯Yt
is an Ornstein-Uhlenbeck SDE with a Gaussian terminal condition."
REFERENCES,0.5418994413407822,"The proof for the rest of proposition is straightforward. By deﬁnition,"
REFERENCES,0.5437616387337058,"E[ ¯Yt] = E[ ¯S−1
t
( ¯Yt −¯nt)] = ¯S−1
t
(¯nt −¯nt) = 0"
REFERENCES,0.5456238361266295,"E[ ¯Yt ¯Y ⊤
t ] = E[ ¯S−1
t
( ¯Yt −¯nt)( ¯Yt −¯nt)⊤¯S−1
t
] = ¯S−1
t
= S−1
t
= Pt"
REFERENCES,0.547486033519553,"Since E[ ¯Yt ¯Y ⊤
t ] = Pt, the optimal gain matrix Kt = −R−1B⊤E[ ¯Yt ¯Y ⊤
t ]. For a given x ∈Rd, the
optimal control ut(x) = Ktx = −R−1B⊤E[ ¯Yt ¯Y ⊤
t ]x is the unique minimizer of the Hamiltonian.
This is referred to as the minimum principle of optimal control."
REFERENCES,0.5493482309124768,"C
DETAILS OF THE ALGORITHMS TO SOLVE THE PARTIALLY OBSERVED
LQG PROBLEM IN SEC. 2"
REFERENCES,0.5512104283054003,"In this section, the implementation details of the EnKF-based algorithms are presented to numeri-
cally solve the partially observed LQG problem (1)-(2) introduced in the main body of the paper.
For better readability, the overall algorithm is broken down into three separate algorithms:"
REFERENCES,0.553072625698324,"1. Algorithm 1 is an ofﬂine algorithm. It is based on the ﬁnite-N approximation of the (con-
trol) EnKF as described in Sec. 2.2 of the main body of the paper. The algorithm is run
ofﬂine to obtain an approximation of the {Pt : 0 ≤t ≤T}."
REFERENCES,0.5549348230912476,"2. Algorithm 2 is an online algorithm. It is based on the ﬁnite-N approximation of the (ﬁlter)
EnKF as described in Sec. 2.1 of the main body of the paper. The algorithm is run online in
a real-time manner. At each time step, it processes the sensor measurements from the true
system (plant) and computes the optimal control input by calling Algorithm 3."
REFERENCES,0.5567970204841713,"3. Algorithm 3 computes the optimal control input. It is based upon minimization of the
Hamiltonian function. In an online implementation, the optimal control input is applied to
the plant at each time step."
REFERENCES,0.5586592178770949,"The input structure of each of the three algorithms is clearly delineated. In particular, the algorithms
require only the simulator in the form of the function evaluator f(x, u) = Ax+Bu for the dynamics,
c(x) = Cx for the cost function, and h(x) = Hx for the observation function. The algorithms do
not require solution of the DRE."
REFERENCES,0.5605214152700186,"Remark 1. In a numerical implementation of step 1 and step 2, there are two sources of error:
(i) error on account of ﬁnite-N approximation; and (ii) error on account of time discretization
which depends upon the step size ∆t. The ﬁnite-N approximation error has been studied in the
EnKF literature where it is shown that the error in approximating ∥Pt −P (N)
t
∥(in step 1) and the
error in approximating the gain the mean ∥ˆXt −ˆX(N)
t
∥(in step 2) converges to zero with the rate
O(
1
√"
REFERENCES,0.5623836126629422,"N ) Del Moral & Tugaut (2018); Bishop et al. (2019); Bishop & Del Moral (2018); Taghvaei
& Mehta (2018); Taghvaei (2019). The time discretization error is also expected to be bounded and
converges to zero with the rate O(∆t) when the system is stable Kloeden & Platen (1999)."
REFERENCES,0.5642458100558659,Under review as a conference paper at ICLR 2022
REFERENCES,0.5661080074487895,"Algorithm 1 [ofﬂine] EnKF algorithm to approximate {Pt : 0 ≤t ≤T}
Input: Simulation time T, simulation step-size ∆t, number of particles N, simulator f(x, u) =
Ax + Bu, terminal cost PT , cost function c(x) = Cx, and control cost matrix R.
Output: {P (N)
k
: k = 0, 1, 2, . . . , T"
REFERENCES,0.5679702048417132,"∆t}.
1: TF =
T
∆t
2: P (N)
TF
= PT"
REFERENCES,0.5698324022346368,"3: Initialize {Yi
TF }N
i=1
i.i.d
∼N(0, P −1
T )"
REFERENCES,0.5716945996275605,"4: calculate n(N)
TF = N −1 PN
i=1 Yi
TF
5: for k = TF to 1 do
6:
Calculate ˆc(N)
k
= N −1 PN
i=1 c(Yi
k)"
REFERENCES,0.5735567970204841,"7:
Calculate M (N)
k
= (N −1)−1 PN
i=1(Yi
k −n(N)
k
)(c(Yi
k) −ˆc(N)
k
)⊤"
REFERENCES,0.5754189944134078,"8:
for i = 1 : N do
9:
∆ηi
k
i.i.d
∼N(0,
1
∆tR−1)"
REFERENCES,0.5772811918063314,"10:
∆Yi
k = f(Yi
k, ∆ηi
k)∆t + 1"
REFERENCES,0.5791433891992551,"2M (N)
k
(c(Yi
k) + ˆc(N)
k
)∆t
11:
Yi
k−1 = Yi
k −∆Yi
k
12:
end for
13:
Calculate n(N)
k−1 = N −1 PN
i=1 Yi
k−1
14:
Calculate S(N)
k−1 = (N −1)−1 PN
i=1(Yi
k−1 −n(N)
k−1)(Yi
k−1 −n(N)
k−1)⊤"
REFERENCES,0.5810055865921788,"15:
P (N)
k−1 = (S(N)
k−1)−1"
REFERENCES,0.5828677839851024,16: end for
REFERENCES,0.5847299813780261,"Algorithm 2 [online] EnKF algorithm to approximate state estimate ˆXt and optimal control ut
Input: Simulation time T, simulation step-size ∆t, number of particles N, simulator f(x, u) =
Ax + Bu, initial distribution N(m0, Σ0), process noise covariance R, observation function
h(x) = Hx, {P (N)
k
: k = 0, 1, 2, . . . , T"
REFERENCES,0.5865921787709497,"∆t} from the ofﬂine algorithm F.1.
Output: estimate { ˆX(N)
k
: k = 0, 1, 2, . . . , T"
REFERENCES,0.5884543761638734,"∆t} and optimal control input {u(N)
k
: k =
0, 1, 2, . . . , T"
REFERENCES,0.590316573556797,"∆t −1}.
1: Deﬁne TF :=
T
∆t
2: Initialize particles {Xi
0}N
i=1
i.i.d
∼N(m0, Σ0)
3: for k = 0 to TF −1 do
4:
Calculate ˆX(N)
k
= 1"
REFERENCES,0.5921787709497207,"N
PN
i=1 Xi
k
5:
Calculate u(N)
k
= arg mina H( ˆX(N)
k
, P (N)
k
ˆX(N)
k
, a) from algorithm 3"
REFERENCES,0.5940409683426443,"6:
Apply control u(N)
k
to the true system and obtain the observation ∆Zk = Z(k+1)∆t −Zk∆t"
REFERENCES,0.595903165735568,"7:
Calculate ˆh(N)
k
= 1"
REFERENCES,0.5977653631284916,"N
PN
i=1 h(Xi
k)"
REFERENCES,0.5996275605214153,"8:
Calculate L(N)
k
=
1
N−1
PN
i=1(Xi
k −ˆX(N)
k
)(h(Xi
k) −ˆh(N)
k
)⊤R−1"
REFERENCES,0.6014897579143389,"9:
for i = 1 to N do
10:
∆ξi
k
i.i.d
∼N(0, Q∆t)"
REFERENCES,0.6033519553072626,"11:
∆Xi
k = f(Xi
k, u(N)
k
)∆t + ∆ξi
k + L(N)
k
(∆Zk −h(Xi
k)+ˆh(N)
k
2
∆t)
12:
Xi
k+1 = Xi
k + ∆Xi
k
13:
end for
14: end for"
REFERENCES,0.6052141527001862,Under review as a conference paper at ICLR 2022
REFERENCES,0.6070763500931099,"Algorithm 3 Computation of optimal control
Input: state x, momentum y, control cost matrix R,
Hamiltonian H(x, y, α) = yT (a(x) + b(x)α) + 1"
REFERENCES,0.6089385474860335,2|c(x)|2 + 1
REFERENCES,0.6108007448789572,"2α⊤Rα.
Output: optimal control u = arg minα H(x, y, α)."
REFERENCES,0.6126629422718808,"1: if b(x) is known then
2:
u = −R−1b(x)⊤y
3: else
4:
for k = 1 to m do
5:
ek = [0, . . . , 0,
1
|{z}
k-th component
, 0, . . . , 0] ∈Rm"
REFERENCES,0.6145251396648045,"6:
uk = H(x, y, R−1ek) −H(x, y, 0) −1"
REFERENCES,0.6163873370577281,"2(R−1)kk
7:
end for
8: end if"
REFERENCES,0.6182495344506518,"D
ERROR ANALYSIS"
REFERENCES,0.6201117318435754,"The objective of the error analysis is to study the convergence of the empirical variance of the
particles S(N)
t
to its mean-ﬁeld limit ¯St as N →∞. According to the Proposition 2, the mean-ﬁeld
limit ¯St = St, hence it evolves backward in time according to"
REFERENCES,0.6219739292364991,"d
dt
¯St = A ¯St + ¯StA⊤+ ¯StC⊤C ¯St −BR−1B⊤,
¯ST = PT
(15)"
REFERENCES,0.6238361266294227,"In order to study the error S(N)
t
−¯St, we obtain a stochastic differential equation for the evolution
of S(N)
t
."
REFERENCES,0.6256983240223464,"Lemma 1. The empirical covariance matrix S(N)
t
, deﬁned in (12), evolves according to the stochas-
tic differential equation"
REFERENCES,0.62756052141527,"dS(N)
t
=

AS(N)
t
+ S(N)
t
A⊤+ S(N)
t
C⊤CS(N)
t
−BR−1B⊤
dt + dMt,
(16)"
REFERENCES,0.6294227188081937,where Mt is a Martingale given by
REFERENCES,0.6312849162011173,"dMt =
1
N −1 N
X"
REFERENCES,0.633147113594041,"i=1
ei
t(B d
 η
i
t)⊤+ B d
 η
i
t(ei
t)⊤,
ei
t := Yi
t −n(N)
t"
REFERENCES,0.6350093109869647,with quadratic variation
REFERENCES,0.6368715083798883,"d⟨M⟩t =
1
N −1"
REFERENCES,0.638733705772812,"h
Tr(BR−1B⊤)S(N)
t
+ BR−1B⊤Tr(S(N)
t
) + BR−1B⊤S(N)
t
+ S(N)
t
BR−1B⊤i"
REFERENCES,0.6405959031657356,"Proof. The evolution for the error process ei
t := Yi
t −n(N)
t
is obtained by subtracting the evolution
for the empirical mean n(N)
t
from (11):"
REFERENCES,0.6424581005586593,"dei
t = Aei
t + B( d
 η
i
t −1 N N
X"
REFERENCES,0.6443202979515829,"j=1
B d
 η
j
t) + S(N)
t
Cei
t dt"
REFERENCES,0.6461824953445066,"The evolution for the empirical covariance S(N)
t
=
1
N−1
PN
i=1 ei
t(ei
t)T is obtained by application
of the Itˆo rule."
REFERENCES,0.6480446927374302,"Note that the ﬁrst term in the evolution of S(N)
t
in (16) is exactly identical to that of ¯St in (15), while
the second term is an additional stochastic term that converges to zero as N →∞. Even though
the ﬂuctuations scale as O(N −1"
REFERENCES,0.6499068901303539,"2 ), the analysis is challenging as has been noted in literature (see
the remark after Theorem 3.1 in (Del Moral & Tugaut, 2018)). Error analysis of the (stochastic)
ensemble Kalman ﬁlter has been carried out in the literature (Del Moral & Tugaut, 2018; Bishop &
Del Moral, 2018) under additional assumptions which we also make below:
Assumption 1. The matrix C⊤C is identity. And the control matrix B is full rank."
REFERENCES,0.6517690875232774,Under review as a conference paper at ICLR 2022
REFERENCES,0.6536312849162011,"The assumption regarding the cost matrix C can be relaxed to CT C being non-singular by a change
of coordinates. The assumption regarding the control matrix is strong. It is an open problem in
the literature regarding stability of EnKF to carry out the error analysis under the more natural
assumption that the system is controllable and observable."
REFERENCES,0.6554934823091247,"Under assumption 1, we can prove the main result stated in the following proposition."
REFERENCES,0.6573556797020484,"Proposition 3. Let ¯St be the mean-ﬁeld covariance deﬁned in (10) and SN
t
be the empirical co-
variance of the particles deﬁned in(16). Then, under Assumption 1, the error between S(N)
t
and ¯St
satisﬁes the upper-bound"
REFERENCES,0.659217877094972,"E[∥S(N)
t
−¯St∥2
F ] ≤3cµ∥PT ∥2
F
N
e−(4µ∞−1"
REFERENCES,0.6610800744878957,N )(T −t) + cvar
REFERENCES,0.6629422718808193,"N ,
0 ≤t ≤T
(17)"
REFERENCES,0.664804469273743,"where µ∞, cµ, cvar are positive constants that are independent of time t."
REFERENCES,0.6666666666666666,"In the remainder of this section, we prove the proposition 3. To simplify the presentation, we use the
time-reversed quantitative Ω(N)
t
:= S(N)
T −t and Ωt := ¯ST −t which evolve forward in time according
to"
REFERENCES,0.6685288640595903,"dΩt = (−AΩt −ΩtA⊤−ΩtC⊤CΩt + BR−1B⊤)dt
(18)"
REFERENCES,0.6703910614525139,"dΩ(N)
t
= (−AΩ(N)
t
−Ω(N)
t
A⊤−Ω(N)
t
C⊤CΩ(N)
t
+ BR−1B⊤) dt + dMt
(19)"
REFERENCES,0.6722532588454376,where Mt is deﬁned in Lemma 1.
REFERENCES,0.6741154562383612,"First, we show some basic results regarding the asymptotic properties of the Ricatti equation that
governs Ωt. These results are straightforward application of the classical stability theory of the
Ricatti equation."
REFERENCES,0.6759776536312849,"Lemma 2. Consider the Ricatti equation (18) that governs Ωt. Then,"
REFERENCES,0.6778398510242085,1. There exists a positive deﬁnite matrix solution Ω∞to the ARE
REFERENCES,0.6797020484171322,0 = −AΩ∞−Ω∞A⊤−Ω∞C⊤CΩ∞+ BR−1B⊤
REFERENCES,0.6815642458100558,such that −A −Ω∞C⊤C is Hurwitz.
REFERENCES,0.6834264432029795,2. Ωt converges to Ω∞exponentially fast
REFERENCES,0.6852886405959032,3. Let µt denote the minimum eigenvalue for the matrix Ft := A + 1
REFERENCES,0.6871508379888268,"2C⊤CΩt. Then, there
exists a positive constant µ∞> 0 and cµ such that"
REFERENCES,0.6890130353817505,"e−
R t
0 µτ dτ < cµe−µ∞t,
∀t ≥0
(20)"
REFERENCES,0.6908752327746741,"Proof. Under Assumption 1, the pair (A, BR−1"
REFERENCES,0.6927374301675978,"2 ) is controllable and the pair (A, C) is observable.
Part (1) follows from (Kwakernaak & Sivan, 1972, Theorem 3.7) and part (2) follows from (Ocone
& Pardoux, 1996, Lemma 2.2 and Theorem 2.3). The proof of part (3) follows from the following
two facts (i) (−A −1"
REFERENCES,0.6945996275605214,2Ω∞C⊤C) is Hurwitz because
REFERENCES,0.6964618249534451,−BR−1B⊤= (−A −1
REFERENCES,0.6983240223463687,2Ω∞C⊤C)Ω∞+ Ω∞(−A −1
REFERENCES,0.7001862197392924,2Ω∞C⊤C)⊤
REFERENCES,0.702048417132216,"is a Lyapunov equation, with BR−1B⊤positive deﬁnite since B is assumed to be full rank; and
(ii) Ωt converges to Ω∞exponentially fast. Therefore, there exists a time τ1 > 0 such that (−A −
1
2ΩtC⊤C is Hurwitz for t ≥τ1. As a result, the minimum eigenvalue µt of Ft = A + 1"
REFERENCES,0.7039106145251397,"2ΩtC⊤C is
lower-bounded µt ≥µ∞> 0 for t ≥τ1. This concludes the bound (20) where cµ = e−
R τ1
0
µτ dτ"
REFERENCES,0.7057728119180633,"Next, we obtain a bound for E[Tr(Ω(N)
t
)]. Upon taking the expectation and trace of (19)"
REFERENCES,0.707635009310987,"d
dtE[tr(Ω(N)
t
)] = −tr(AE[Ω(N)
t
]) −tr(E[Ω(N)
t
]A⊤) −tr(E[Ω(N)
t
C⊤CΩ(N)
t
]) + tr(BR−1B⊤)"
REFERENCES,0.7094972067039106,"≤−tr(AE[Ω(N)
t
]) −tr(E[Ω(N)
t
]A⊤) −tr(E[Ω(N)
t
]C⊤CE[Ω(N)
t
]) + tr(BR−1B⊤)"
REFERENCES,0.7113594040968343,Under review as a conference paper at ICLR 2022
REFERENCES,0.7132216014897579,"where we used E[Tr(X⊤X)] ≥Tr(E[X]E[X]⊤) for X = Ω(N)
t
C⊤to conclude the inequality. Then,
by inspecting the equation for Tr(Ωt), it follows that"
REFERENCES,0.7150837988826816,"E[Tr(Ω(N)
t
)] ≤Tr(Ωt) ≤E0
(21)
where E0 is a uniformly in time bounded constant, because Ωt converges to Ω∞."
REFERENCES,0.7169459962756052,"Next, we obtain the bound for the error Ω(N)
t
−Ωt. Subtracting (18) from (19) yields"
REFERENCES,0.7188081936685289,"d(Ω(N)
t
−Ωt) = (−A(Ω(N)
t
−Ωt) −(Ω(N)
t
−Ωt)A⊤−(Ω(N)
t
−Ωt)C⊤C(Ω(N)
t
−Ωt)"
REFERENCES,0.7206703910614525,"−(Ω(N)
t
−Ωt)C⊤CΩt −ΩtC⊤C(Ω(N)
t
−Ωt))dt + dMt"
REFERENCES,0.7225325884543762,"Then, by application of the Itˆo rule to (Ω(N)
t
−Ωt)⊤(Ω(N)
t
−Ωt) = (Ω(N)
t
−Ωt)2 and taking the
trace,"
REFERENCES,0.7243947858472998,"dTr((Ω(N)
t
−Ωt)2) = 2tr((Ω(N)
t
−Ωt)2)(−A −A⊤−C⊤CΩt −ΩtC⊤C))dt"
REFERENCES,0.7262569832402235,"−2tr(C⊤C(Ω(N)
t
−Ωt)3))dt + tr((dMt + dM ⊤
t )(Ω(N)
t
−Ωt)) + 2 N"
REFERENCES,0.7281191806331471,"
tr(Ω(N)
t
BR−1B⊤) + tr(Ω(N)
t
)tr(BR−1B⊤)

dt"
REFERENCES,0.7299813780260708,"Upon taking the expectation and deﬁning Θt = E[tr((Ω(N)
t
−Ωt)2)],
d
dtΘt ≤2E
h
tr(−A −A⊤−C⊤CΩt)(Ω(N)
t
−Ωt)2)
i
−2E
h
tr(C⊤CΩ(N)
t
(Ω(N)
t
−Ωt)2)
i + 1 N"
REFERENCES,0.7318435754189944,"
Θt + 2tr((BR−1B⊤)2) + tr(Ω2
t) + 2tr(BR−1B⊤)tr(Ωt))"
REFERENCES,0.7337057728119181,"where we used the bound (21), and Tr(XY ) ≤1"
REFERENCES,0.7355679702048417,2Tr(X⊤X) + 1
REFERENCES,0.7374301675977654,"2Tr(Y ⊤Y ) for X = Ω(N)
t
−Ωt and
Y = BR−1B⊤."
REFERENCES,0.7392923649906891,"In order to continue with proving the bound, we use the assumption C⊤C = I to conclude
d
dtΘt ≤2E
h
tr(−A −A⊤−Ωt)(Ω(N)
t
−Ωt)2)
i + 1 N"
REFERENCES,0.7411545623836127,"
Θt + 2tr((BR−1B⊤)2) + 2tr(Ωt)tr(BR−1B⊤) + tr(Ω2
t))"
REFERENCES,0.7430167597765364,"because tr(Ω(N)
t
(Ω(N)
t
−Ωt)2) ≥0. Morevoer, due to exponential convergence of Ωt to Ω∞,
tr(Ω2
t) ≤E1 uniformly. Thus,
d
dtΘt ≤4µtΘt + 1"
REFERENCES,0.74487895716946,N (Θt + 2tr((BR−1B⊤)2) + 2E0tr(BR−1B⊤) + E1)
REFERENCES,0.7467411545623837,where µt is the minimum eigenvalue for A + 1
REFERENCES,0.7486033519553073,"2Ωt. Therefore,"
REFERENCES,0.750465549348231,"Θt ≤Θ0e−4
R t
0 µt′dt′+ 1"
REFERENCES,0.7523277467411545,N t + cvar N
REFERENCES,0.7541899441340782,≤Θ0cµe−4µ∞t+ 1
REFERENCES,0.7560521415270018,N t + cvar
REFERENCES,0.7579143389199255,"N
where cvar = 2tr((BR−1B⊤)2) + 2E0tr(BR−1B⊤) + E1 and we used (20). The proof of the
proposition follows by substitution S(N)
T −t −St = Ω(N)
t
−Ωt and the bound Θ0 ≤3∥PT ∥F N
."
REFERENCES,0.7597765363128491,"E
DUALITY ROOTS OF THE PROPOSED ALGORITHM"
REFERENCES,0.7616387337057728,"For t ∈(0, T), the LQG value function is deﬁned as follows"
REFERENCES,0.7635009310986964,"vt(x) :=
min
{Us:t≤s≤T } E Z T t   1"
REFERENCES,0.7653631284916201,2|CXs|2 + 1
REFERENCES,0.7672253258845437,"2U ⊤
s RUs

ds + X⊤
T PT XT"
REFERENCES,0.7690875232774674,Xt = x ! (22)
REFERENCES,0.770949720670391,"From the DP optimality principle, it is easily shown Liberzon (2012) that"
REFERENCES,0.7728119180633147,• The value function vt(x) = 1
REFERENCES,0.7746741154562383,"2x⊤Ptx + (constant) is quadratic where
• The matrix-valued process {Pt ∈Rd×d : 0 ≤t ≤T} solves the DRE with terminal
condition PT at time t = T."
REFERENCES,0.776536312849162,Under review as a conference paper at ICLR 2022
REFERENCES,0.7783985102420856,"Log transformation:
The log transformation is a manifestation of the duality between optimal
control and optimal estimation Mitter & Newton (2003); Todorov (2008). Deﬁne the probability
density pt(x) ∝e−vt(x) Fleming & Mitter (1982); Fleming (1977). For the LQG problem, vt is
quadratic and therefore, pt is a Gaussian density with variance P −1
t
= St. Indeed, the Gaussian
density of the random variable ¯Yt equals pt. The following proposition gives the precise connection:"
REFERENCES,0.7802607076350093,"Proposition 4. Suppose pt ∝e−vt where vt is the value function deﬁned in (22). Let ¯pt be the
density of the random variable ¯Yt deﬁned in (9). Then, provided pT = ¯pT ,"
REFERENCES,0.7821229050279329,"pt(x) = ¯pt(x),
∀x ∈Rd,
0 ≤t ≤T"
REFERENCES,0.7839851024208566,"Proof. By Proposition 2, ¯Yt is a Gaussian random variable N(0, St) where St = P −1
t
by deﬁnition.
Therefore,"
REFERENCES,0.7858472998137802,−log(¯pt(x)) = 1
REFERENCES,0.7877094972067039,2x⊤Ptx + (constant) = vt(x) + (constant) = −log(pt(x)) + (constant)
REFERENCES,0.7895716945996276,"Therefore, since pt(x) and ¯pt(x) are both normalized to one, the constant is zero and ¯pt(x) = pt(x)."
REFERENCES,0.7914338919925512,"In the past, duality has been used to:"
REFERENCES,0.7932960893854749,"(i) Obtain linearly solvable sampling algorithms to solve optimal control problems Kappen & Ruiz
(2016); Rawlik et al. (2013); Theodorou et al. (2010); Sch¨utte et al. (2012); Todorov (2009); and"
REFERENCES,0.7951582867783985,"(ii) Set up optimal control problems for the purposes of estimation and simulation Sutter et al.
(2016); Hartmann & Sch¨utte (2012); Van Handel (2006); Kim & Mehta (2020)."
REFERENCES,0.7970204841713222,"(iii) A parallel approach to duality involves posing optimal control problems as problems of prob-
abilistic inference, and using methods like expectation maximisation (Toussaint & Storkey, 2006),
message passing (Hoffmann & Rostalski, 2017; Watson & Peters, 2021; Watson et al., 2021) or
minimisation of KL divergence (Kappen et al., 2012; Rawlik et al., 2013; Watson et al., 2020)."
REFERENCES,0.7988826815642458,"Although novel and distinct from the algorithms described in these earlier works – which instead rely
on the use of a Feynman-Kac representation – the proposed algorithm is an example of a sampling
algorithm to solve the optimal control problem. In a related work, we have extended the results
presented here to a class of nonlinear optimal control problems. These nonlinear generalization more
clearly reveals the duality roots of the proposed algorithm. The EnKF algorithm arises as a special
case in the LQG settings. The nonlinear generalization will be a subject of future publication."
REFERENCES,0.8007448789571695,"F
DETAILS OF THE NUMERICAL EXAMPLES IN SEC. 3"
REFERENCES,0.8026070763500931,"Notation: In ∈Rn×n denotes the identity matrix and 1d ∈Rd denotes a vector with all entries
equal to 1."
REFERENCES,0.8044692737430168,"F.1
EXPONENTIAL CONVERGENCE"
REFERENCES,0.8063314711359404,"The EnKF algorithm is implemented for the linear system (1) to approximate the optimal gain ma-
trix. The model matrices A and B are in the control canonical form A =  "
REFERENCES,0.8081936685288641,"0
1
0
0
. . .
0
0
0
1
0
. . .
0
...
...
a1
a2
a3
a4
. . .
ad "
REFERENCES,0.8100558659217877,",
B =  "
REFERENCES,0.8119180633147114,"0
0
...
1  "
REFERENCES,0.813780260707635,"where the entries (a1, . . . , ad) ∈Rd is selected randomly from N(1.651d, Id). Additional model
and simulation parameters are summarized in Table 2 and Table 3."
REFERENCES,0.8156424581005587,"The numerical result for a single realization of the algorithm, for d = 2, is depicted in Figure 1.
Part-(a) depicts the spectral properties of the open-loop ( ˙x = Ax) and the closed-loop ( ˙x = (A +
BK(N)
0
)x) dynamics (recall that due to the backward in time nature of the EnKF, K0 is the terminal"
REFERENCES,0.8175046554934823,Under review as a conference paper at ICLR 2022
REFERENCES,0.819366852886406,"gain which the algorithm yields). It is observed that the algorithm learns the gain matrix that make
the closed-loop dynamics stable, while the open loop dynamics A is unstable."
REFERENCES,0.8212290502793296,"The entry-wise convergence of the matrix P (N)
t
as t goes from T to 0 is depicted in part (b)-(c)-(d).
The result is compared with the exact solution to the DRE Pt and the asymptomatic limit, as the
time horizon T →∞, which is the solution to the ARE."
REFERENCES,0.8230912476722533,Figure 4: Open-loop (o) and closed loop eigenvalues (x) for the 10 dimensional system.
REFERENCES,0.8249534450651769,"Additional numerical results, for the case where d = 10, are depicted in Fig. 4 for the open and
closed-loop eigenvalues and in Fig. 5 for the convergence of the 100 entries of the 10 × 10 matrix
P (N)
t
."
REFERENCES,0.8268156424581006,Table 2: Model parameters for the LQR with random dynamics
REFERENCES,0.8286778398510242,"Model parameter
Numerical value"
REFERENCES,0.8305400372439479,"m0
0d×1
Σ0
0.1Id
C for d = 2
√"
ID,0.8324022346368715,"5Id
C for d > 2
Id
R
1.0 PT
Id"
ID,0.8342644320297952,Table 3: Simulation parameters for LQR with random dynamics
ID,0.8361266294227188,"Simulation parameter name
Symbol
Numerical value"
ID,0.8379888268156425,"Simulation time
T
10"
ID,0.839851024208566,"Step size
∆t
0.02"
ID,0.8417132216014898,Under review as a conference paper at ICLR 2022
ID,0.8435754189944135,"Figure 5: Convergence of the 100 entries of the matrix P (N)
t
for d = 10 dimensional system.
Solution of the ARE is depicted as the red dashed line. As in Fig. 1 for the 2-dimensional system,
these plots are depicted with respect to T −t. The initialization is PT = I, the identity matrix."
ID,0.845437616387337,"F.2
COUPLED MASS SPRING DAMPER SYSTEM"
ID,0.8472998137802608,This system is taken from Mohammadi et al. (2019). The matrices A and B are as follows:
ID,0.8491620111731844,"A =

0ds×ds
Ids
−T
−T"
ID,0.851024208566108,"
,
B =

0ds×ds
Ids "
ID,0.8528864059590316,where ds = d
ID,0.8547486033519553,"2 is the number of masses and T ∈Rds×ds is a Toeplitz matrix with 2 on the main
diagonal and −1 on the ﬁrst sub-diagonal and ﬁrst super-diagonal. Additional model and simulation
parameters are listed in Table 4 and 5, respectively."
ID,0.8566108007448789,Table 4: Model parameters for the coupled mass spring damper system
ID,0.8584729981378026,"Model parameter
Numerical value"
ID,0.8603351955307262,"m0
0d×1
Σ0
0.1Id
C for d = 2
√"
ID,0.8621973929236499,"5Id
C for d > 2
Id
R
Ids
PT
Id"
ID,0.8640595903165735,Table 5: Simulation parameters for the coupled mass spring damper system
ID,0.8659217877094972,"Simulation parameter name
Symbol
Numerical value"
ID,0.8677839851024208,"Simulation time
T
10"
ID,0.8696461824953445,"Step size
∆t
0.02"
ID,0.8715083798882681,Under review as a conference paper at ICLR 2022
ID,0.8733705772811918,"F.3
COMPARISON BETWEEN ENKF AND POLICY-GRADIENT METHODS"
ID,0.8752327746741154,"The performance of the EnKF algorithm is compared with policy gradient algorithms presented
in Mohammadi et al. (2021c) (denoted as [M21]) and Fazel et al. (2018) (denoted as [F18]). These
two algorithms are based on gradient free optimization for the gain matrix, with the difference that
[M21] is for continuous-time systems while [F18] is for discrete-time systems. We used the dis-
cretized linear system xt+∆t = (I + A∆t)xt + But∆t to implement [F18]. The two algorithms
involve similar set of hyper-parameters that are selected optimally for each experiment for fair com-
parison."
ID,0.8770949720670391,"The three algorithms, EnKF, [M21], and [F18] are implemented for the mass-spring system of Ap-
pendix F.2 for d = 2, 4, 10. The comparison is made by studying the relationship between the
computational time and the error in approximating the optimal gain matrix. The error is measured
with the formula"
ID,0.8789571694599627,"error = 
 "
ID,0.8808193668528864,"∥K(N)
0
−K∞∥F
∥K∞∥F
;
for EnKF"
ID,0.88268156424581,"∥K(N)
T
−K∞∥F
∥K∞∥F
;
for [M21] and [F18]"
ID,0.8845437616387337,"where K(N)
0
is the terminal gain output by EnKF, K(N)
T
is the terminal gain output by the [M21]
and [F18] algorithms, and K∞is the analytical gain obtained through the ARE."
ID,0.8864059590316573,"In order to study the error vs computational time relationship for EnKF algorithm, the number of
particles N is varied, while for [M21] and [F18] the number of iterations is varied. The simulation
time horizon T = 10, and the step-size ∆t = 0.01 is the same for all algorithms. The hyper-
parameters of the [M21], and [F18] algorithms are presented in Table 6. The initial guess K0 = 0,
initial distribution D0 = N(0, Id), and gradient descent step α = 0.0001 for both [M21] and [F18].
The values of the other hyper parameters are in Table 7. The numerical results for d = 10 are
depicted in Figure 2 and for d = 2, 4 in Figure 6. Additionally, Figure 7 shows comparison when
the error is measured as"
ID,0.888268156424581,"error = 

 
"
ID,0.8901303538175046,"c(N)
0
−c∞
c(N)
T
−c∞;
for EnKF"
ID,0.8919925512104283,"c(N)
T
−c∞
c(N)
0
−c∞;
for [M21] and [F18]"
ID,0.8938547486033519,"where c(N)
0
is the optimal control cost corresponding to the terminal gainas per EnKF, and by the
initial gain as per [M21] and [F18] algorithms; and c(N)
T
is the optimal control cost corresponding
to the initial gain as per EnKF, and terminal gain as per the [M21] and [F18] algorithms; and c∞is
the optimal control cost achieved by analytical gain obtained through the ARE."
ID,0.8957169459962756,"The simulations are implemented in Python 3 on a Intel Xeon E3-1240 V2 3.40 Ghz CPU, and the
process time()
function from the time module is used to evaluate the execution time."
ID,0.8975791433891993,Table 6: Hyper-parameters for policy gradient
ID,0.8994413407821229,"Hyper-parameter
Symbol"
ID,0.9013035381750466,"Initial guess
K0"
ID,0.9031657355679702,"Distribution of initial state
D0
Smoothing parameter
r"
ID,0.9050279329608939,"Gradient descent step
α"
ID,0.9068901303538175,"Simulation horizon
T"
ID,0.9087523277467412,"Averaging for gradient
N"
ID,0.9106145251396648,Under review as a conference paper at ICLR 2022
ID,0.9124767225325885,"0.05
0.10 10−1 100 101 102 103"
ID,0.9143389199255121,Comp. Time (s)
ID,0.9162011173184358,"EnKF
[M21]
[F18]"
ID,0.9180633147113594,"0.02
0.04
0.06
0.08 100 101 102 103"
ID,0.9199255121042831,"d = 2
d = 4"
ID,0.9217877094972067,"Relative error in gain
Figure 6: Comparison between EnKF, [M21], [F18]"
ID,0.9236499068901304,"0.000
0.002
0.004
0.006
0.008
0.010 10−1 100 101 102 103"
ID,0.925512104283054,Comp. Time (s)
ID,0.9273743016759777,"0.000
0.002
0.004
0.006
0.008
Error in cost 100 101 102 103"
ID,0.9292364990689013,"EnKF
[M21]
[F18]"
ID,0.931098696461825,"0.000
0.002
0.004
0.006 100 101 102 103 104"
ID,0.9329608938547486,"d = 2
d = 4
d = 10"
ID,0.9348230912476723,"Figure 7: Comparison between EnKF, [M21], [F18]"
ID,0.9366852886405959,"F.4
CART-POLE SYSTEM"
ID,0.9385474860335196,"The nonlinear model is taken from (Tedrake, Chapter 3.2.1):"
ID,0.9404096834264432,˙θ = ω
ID,0.9422718808193669,"˙ω =
1
l(M + m sin2(θ))"
ID,0.9441340782122905," 
−F cos(θ) −mlω2 cos(θ) sin(θ) −(m + M)g sin(θ)
"
ID,0.9459962756052142,˙x = v
ID,0.9478584729981379,"˙v =
1
M + m sin2(θ)"
ID,0.9497206703910615," 
F + m sin(θ)(lω2 + g cos(θ))
"
ID,0.9515828677839852,"Table 7: Hyper-parameter values for policy gradient
Hyper-param.
[M21]
[F18]"
ID,0.9534450651769087,"d
2
4
10
2
4
10"
ID,0.9553072625698324,"r
10−1
10−1
10−3
10−1
10−1
10−1"
ID,0.957169459962756,"N
2
4
10
2
4
10"
ID,0.9590316573556797,Under review as a conference paper at ICLR 2022
ID,0.9608938547486033,"For the LQG control design, we linearize the equations about the desired inverted equilibrium
(π, 0, 0, 0). The associated A and B matrices are as follows: A =  "
ID,0.962756052141527,"0
0
1
0"
ID,0.9646182495344506,"0
0
0
1"
ID,0.9664804469273743,(M+m)g
ID,0.9683426443202979,"Ml
0
0
0 mg"
ID,0.9702048417132216,"M
0
0
0 "
ID,0.9720670391061452,"
,
B =   0"
ML,0.9739292364990689,"1
Ml
0"
M,0.9757914338919925,"1
M  "
M,0.9776536312849162,The model parameters are listed in Table 8 and the simulation parameters are in Table 9.
M,0.9795158286778398,"Table 8: Model parameters for the cart-pole system
Model parameter name
Symbol
Numerical value"
M,0.9813780260707635,"Mass of ball
m
0.08"
M,0.9832402234636871,"Mass of cart
M
1"
M,0.9851024208566108,"Length of rod
l
0.7"
M,0.9869646182495344,"Gravity
g
9.81"
M,0.9888268156424581,"Unstable equilibrium
(¯θ, ¯x, ¯ω, ¯v)
(π, 0, 0, 0)"
M,0.9906890130353817,"Initial condition
(θ(0), x(0), ω(0), v(0))
(1.25π, −0.1, 0, 0)"
M,0.9925512104283054,"LQG parameters
C
diag([10, 10, 1, 1]) R
10 PT
I4"
M,0.994413407821229,"Table 9: Simulation parameters for the cart-pole system
Simulation parameter name
Symbol
Numerical value"
M,0.9962756052141527,"Simulation time
T
10"
M,0.9981378026070763,"Step size
∆t
0.0002"
