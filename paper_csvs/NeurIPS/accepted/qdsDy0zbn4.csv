Section,Section Appearance Order,Paragraph
NSF AI INSTITUTE FOR ARTIFICIAL INTELLIGENCE AND FUNDAMENTAL INTERACTIONS,0.0,"1NSF AI Institute for Artificial Intelligence and Fundamental Interactions
2Center for Theoretical Physics, Massachusetts Institue of Technology
3Department of Physics, Massachusetts Institute of Technology
4Department of Mathematics, Massachusetts Institute of Technology
5Department of Physics, Harvard University
{chenzhuo,lakern,ezchen,diluo,soljacic}@mit.edu"
ABSTRACT,0.002061855670103093,Abstract
ABSTRACT,0.004123711340206186,"Quantum many-body physics simulation has important impacts on understanding
fundamental science and has applications to quantum materials design and quan-
tum technology. However, due to the exponentially growing size of the Hilbert
space with respect to the particle number, a direct simulation is intractable. While
representing quantum states with tensor networks and neural networks are the two
state-of-the-art methods for approximate simulations, each has its own limitations
in terms of expressivity and inductive bias. To address these challenges, we develop
a novel architecture, Autoregressive Neural TensorNet (ANTN), which bridges
tensor networks and autoregressive neural networks. We show that Autoregressive
Neural TensorNet parameterizes normalized wavefunctions, allows for exact sam-
pling, generalizes the expressivity of tensor networks and autoregressive neural
networks, and inherits a variety of symmetries from autoregressive neural net-
works. We demonstrate our approach on quantum state learning as well as finding
the ground state of the challenging 2D J1-J2 Heisenberg model with different
systems sizes and coupling parameters, outperforming both tensor networks and
autoregressive neural networks. Our work opens up new opportunities for quan-
tum many-body physics simulation, quantum technology design, and generative
modeling in artificial intelligence."
INTRODUCTION,0.006185567010309278,"1
Introduction"
INTRODUCTION,0.008247422680412371,"Quantum many-body physics is fundamental to our understanding of the universe. It appears in
high energy physics where all the fundamental interactions in the Standard Model, such as quantum
electrodynamics (QED) and quantum chromodynamics (QCD), are described by quantum mechanics.
In condensed matter physics, quantum many-body physics has led to a number of rich phenomena
and exotic quantum matters, including superfluids, superconductivity, the quantum Hall effect, and
topological ordered states (Girvin & Yang, 2019). As an application, quantum many-body physics is
crucial for new materials design. The electronic structure problem and chemical reactions in quantum
chemistry are governed by quantum many-body physics. The recent development of quantum
computers is also deeply connected to quantum many-body physics. A multi-qubit quantum device is
intrinsically a quantum many-body system, such that progress on quantum computer engineering is
tied to our understanding of quantum many-body physics (Preskill, 2021)."
INTRODUCTION,0.010309278350515464,"‚àóequal contribution
‚Ä†corresponding author: diluo@mit.edu"
INTRODUCTION,0.012371134020618556,"ANTN
Bridging the gap"
INTRODUCTION,0.01443298969072165,"ùúìùúì(ùë•ùë•1, ùë•ùë•2, ùë•ùë•3, ùë•ùë•4)"
INTRODUCTION,0.016494845360824743,"ùë•ùë•1
ùë•ùë•2
ùë•ùë•3
ùë•ùë•4
ùë•ùë•1
ùë•ùë•2
ùë•ùë•3
ùë•ùë•4"
INTRODUCTION,0.018556701030927835,"ùë•ùë•1
ùë•ùë•2
ùë•ùë•3
ùë•ùë•4"
INTRODUCTION,0.020618556701030927,"ùúìùúì(ùë•ùë•1, ùë•ùë•2, ùë•ùë•3, ùë•ùë•4)"
INTRODUCTION,0.02268041237113402,"ùë•ùë•1
ùë•ùë•2
ùë•ùë•3
ùë•ùë•4"
INTRODUCTION,0.024742268041237112,"ùë•ùë•1
ùë•ùë•2
ùë•ùë•3
ùë•ùë•4"
INTRODUCTION,0.026804123711340205,"ùúìùúì(ùë•ùë•1, ùë•ùë•2, ùë•ùë•3, ùë•ùë•4)"
INTRODUCTION,0.0288659793814433,"TN
Pro: Physics inductive bias
Con: Limited expressivity"
INTRODUCTION,0.030927835051546393,"ARNN
Pro: High expressivity
Con: No physics prior"
INTRODUCTION,0.032989690721649485,"Figure 1: Diagrammatic representation of autoregres-
sive neural network (ARNN), tensor network (TN)
and our Autoregressive Neural TensorNet (ANTN)."
INTRODUCTION,0.03505154639175258,"All information of a closed quantum many-
body system is captured by the wavefunc-
tion, whose properties are described by the
famous Schr√∂dinger equation.
An impor-
tant tool to study and understand quantum
many-body physics is to classically simu-
late the wavefunction. However, the wave-
function is a high dimensional function in
Hilbert space, whose dimension grows expo-
nentially with the number of particles. For
example, for qubit systems where each qubit
has two degrees of freedom), the wavefunc-
tion of 300 qubits will have dimension 2300,
which is larger than the number of atoms in
the observable universe. Furthermore, the
Schr√∂dinger equation is a complex-valued
high-dimensional equation, which is challenging to solve or simulate in general."
INTRODUCTION,0.03711340206185567,"A number of algorithms have been developed to simulate quantum many-body physics, including
quantum Monte Carlo, tensor networks, neural network quantum states, and quantum computation.
In particular, computing the ground state of quantum many-body systems is of great interest. One
important approach is the variational principle, which provides an upper bound for the ground
state energy. To apply the variational principle successfully, one must design an ansatz that can
represent and optimize the wavefunction efficiently. Tensor networks and neural network quantum
states (Carleo & Troyer, 2017) are the two main state-of-the-art methods that can be applied with the
variational principle for quantum many-body simulation. However, tensor networks usually suffer
from an expressivity issue in systems with more than one dimension, while neural network quantum
states usually lack inductive bias from the underlying physics structure and have sign structure
challenges in the representation."
INTRODUCTION,0.03917525773195876,"In this paper, we develop a novel architecture, Autoregressive Neural TensorNet (ANTN), to bridge
neural network quantum states and tensor networks, achieving the best of both worlds. In particular,
our contributions are threefold:"
INTRODUCTION,0.041237113402061855,"‚Ä¢ Develop ANTN with two variants called ‚Äúelementwise‚Äù and ‚Äúblockwise,‚Äù which each natu-
rally generalize the two state-of-the-arts ansatzes, tensor networks (TN) and autoregressive
neural networks (ARNN), to provide proper inductive bias and high expressivity.
‚Ä¢ Prove that ANTN is normalized with exact sampling, has generalized expressivity of TN
and ARNN, and inherits multiple symmetries from ARNN.
‚Ä¢ Demonstrate our methods on quantum state learning and variationally finding the ground
state of the challenging 2D J1-J2 Heisenberg model, outperforming both TN and ARNN."
RELATED WORK,0.04329896907216495,"2
Related Work"
RELATED WORK,0.04536082474226804,"Tensor Networks (TN) represent high-dimensional wavefunctions using low-rank tensor decompo-
sition, notably matrix product state (MPS) (Vidal, 2003, 2004), PEPS (Verstraete & Cirac, 2004),
and MERA (Vidal, 2007). They capture the entanglement structure of physical systems and, with
algorithms like the density matrix renormalization group (DMRG) (White, 1992), are used for state
simulations and real-time dynamics. However, their expressivity can be limited in systems with more
than one dimension and systems with high entanglement. In machine learning, TN appears as tensor
train (Oseledets, 2011) and CP decomposition methods."
RELATED WORK,0.04742268041237113,"Neural Network Quantum State (NNQS) leverages neural networks for high dimensional wave-
function representation (Carleo & Troyer, 2017). It‚Äôs been demonstrated that many quantum states
can be approximated or represented by NNQS (Sharir et al., 2021; Gao & Duan, 2017; Lu et al., 2019;
Levine et al., 2019a; Luo et al., 2021b,a; Deng et al., 2017; Huang & Moore, 2021; Vieijra et al.,
2020), and it has yielded state-of-the-art results in computing quantum system properties (Guti√©rrez &
Mendl, 2020; Schmitt & Heyl, 2020; Vicentini et al., 2019; Yoshioka & Hamazaki, 2019; Hartmann
& Carleo, 2019; Nagy & Savona, 2019; Luo et al., 2021b, 2022a). Key advancements in NNQS
include ARNN that improves sample efficiency and gradient estimation, and the development of"
RELATED WORK,0.049484536082474224,"neural networks that adhere to the underlying symmetries of quantum systems (Luo et al., 2021b;
Hibat-Allah et al., 2020; Choo et al., 2019; Luo & Clark, 2019; Hermann et al., 2019; Pfau et al., 2020;
Luo et al., 2021a, 2022b, 2021b; Chen et al., 2022). Despite its potential, NNQS faces challenges
such as the lack of physics prior and the sign structure problem. While there are attempts to integrate
NNQS with TN, including matrix product state with neural network backflow (Lami et al., 2022) and
generalizing MPS to RNN (Wu et al., 2022), the former can not produce normalized wavefunctions,
while the latter only builds on tensor contractions that lacks the nonlinear activation functions of
standard neural network wavefunctions."
BACKGROUND,0.05154639175257732,"3
Background"
QUANTUM PRELIMINARIES,0.05360824742268041,"3.1
Quantum Preliminaries"
QUANTUM PRELIMINARIES,0.05567010309278351,"In this work, we focus on qubit systems in quantum many-body physics. The wavefunction or
quantum state œà of the system is a normalized function œà : Zn
2 ‚ÜíC with P
x |œà(x)|2 = 1, where
n is the system size. The input to the wavefunction is an n-bit string x ‚àà{0, 1}n. Therefore,
the wavefunction œà can be viewed as a complex-valued vector of size 2n, with Dirac notation ‚ü®œà|
and |œà‚ü©correspond to a conjugate row vector and a column vector respectively, and œà(x) = ‚ü®x|œà‚ü©.
Because the size of the wavefunction grows exponentially with the system size n, a direct computation
quickly becomes intractable as the system size increases. The goal of NNQS is to design a compact
architecture that can approximate and optimize the wavefunction efficiently."
QUANTUM STATE LEARNING,0.0577319587628866,"3.2
Quantum State Learning"
QUANTUM STATE LEARNING,0.05979381443298969,"Given a quantum state |œï‚ü©, we are often interested in finding |œàŒ∏‚ü©that is closest to |œï‚ü©given the
variational ansatz. In quantum mechanics, the closeness of two quantum states is measured by
the quantum fidelity F = |‚ü®œï|œàŒ∏‚ü©|2 = |P"
QUANTUM STATE LEARNING,0.061855670103092786,"x œï‚àó(x)œàŒ∏(x)|2 where ‚àórefers to complex conjugation.
Quantum fidelity satisfies 0 ‚â§F ‚â§1, with F = 1 corresponding to exact match, and F = 0
orthogonal quantum states. Finding Fmax for a given ansatz allows us to quantify how good the
ansatz can be used to approximate that state. In practice, minimizing ‚àílog F is usually better than
maximizing F itself. This can be achieved by enumerating all the basis x in small systems and
stochastically in large systems such as quantum state tomography (see Appendix A.1)."
VARIATIONAL MONTE CARLO,0.06391752577319587,"3.3
Variational Monte Carlo"
VARIATIONAL MONTE CARLO,0.06597938144329897,"For a given quantum system with n qubits, the Hamiltonian ÀÜH can be written as a Hermitian matrix
of size 2n √ó 2n. The ground state energy Eg of the system is the smallest eigenvalue of ÀÜH and the
ground state is the corresponding eigenvector. For large system sizes, finding the ground state directly
is usually impossible. In this case, the variational principle in quantum mechanics provides an upper
bound on the ground state energy Eg. For all (normalized) wavefunctions |œà‚ü©, it is evidental that
Eg ‚â§‚ü®œà| ÀÜH|œà‚ü©. Therefore, finding the |œàŒ∏‚ü©that minimizes EŒ∏ = ‚ü®œàŒ∏| ÀÜH|œàŒ∏‚ü©gives the lowest upper
bound of the ground state energy. For large system sizes, we can stochastically evaluate and minimize"
VARIATIONAL MONTE CARLO,0.06804123711340206,"‚ü®œàŒ∏| ÀÜH|œàŒ∏‚ü©=
X"
VARIATIONAL MONTE CARLO,0.07010309278350516,"xx‚Ä≤
|œàŒ∏(x)|2 Hx,x‚Ä≤œàŒ∏(x‚Ä≤)"
VARIATIONAL MONTE CARLO,0.07216494845360824,"œàŒ∏(x)
= Ex‚àº|œàŒ∏|2
P"
VARIATIONAL MONTE CARLO,0.07422680412371134,"x‚Ä≤ Hx,x‚Ä≤œàŒ∏(x‚Ä≤)"
VARIATIONAL MONTE CARLO,0.07628865979381444,"œàŒ∏(x)
,
(1)"
VARIATIONAL MONTE CARLO,0.07835051546391752,"where Hx,x‚Ä≤ refers to the matrix element of ÀÜH and we interpret |œàŒ∏(x)|2 as a probability distribution.
The summation over x‚Ä≤ can be efficiently computed given x since the Hamiltonian is usually sparse.
The gradient ‚àáŒ∏EŒ∏ can also be calculated stochastically in a similar fashion (see Appendix A.2)."
METHOD,0.08041237113402062,"4
Method"
PRELIMINARIES,0.08247422680412371,"4.1
Preliminaries"
PRELIMINARIES,0.08453608247422681,"Autoregressive Neural Network Wavefunction. The autoregressive neural network (ARNN) (Fig. 1
left) parameterizes the full probability distribution as a product of conditional probability distributions as"
PRELIMINARIES,0.0865979381443299,"p(x) = n
Y"
PRELIMINARIES,0.088659793814433,"i=1
p(xi|x<i),
(2)"
PRELIMINARIES,0.09072164948453608,"where x = (x1, . . . , xn) is a configuration of n qubits and x<i = (x1, . . . , xi‚àí1) is any configura-
tion before xi. The normalization of the full probability can be guaranteed from the normalization
of individual conditional probabilities. The ARNN also allows for exact sampling from the full
distribution by sampling sequentially from the conditional probabilities (see Appendix B.1). The
autoregressive constructions for probabilities can be easily modified to represent quantum wavefunc-
tion by (1) replacing p(xi|x<i) with a complex-valued conditional wavefunction œà(xi|x<i) and (2)
using the following normalization condition for the conditional wavefunctions: P"
PRELIMINARIES,0.09278350515463918,"x |œà(xi|x<i)|2 = 1
(Sharir et al., 2020; Hibat-Allah et al., 2020; Luo et al., 2021b). Similar to the case of probabilities,
ARNN automatically preserves the normalization of wavefunctions and allows for exact sampling
(see Appendix B.1). Because of this, ARNN is often more efficient when training and computing
various quantum observables compared to other generative neural networks."
PRELIMINARIES,0.09484536082474226,"Matrix Product State (MPS). MPS (also known as tensor train) is a widely used TN architecture to
study quantum many-body physics (Vidal, 2003, 2004). The MPS defines a wavefunction using n
rank-3 tensors M Œ±i‚àí1Œ±i
xi
with Œ±i‚àí1 (or Œ±i) the index of the left (or right) bond dimensions and xi the
configuration of the ith qubit. Then, the full wavefunction is generated by first choosing a particular
configuration x = (x1, . . . , xn) and then contracting the tensors selected by this configuration (Fig. 1
right) as"
PRELIMINARIES,0.09690721649484536,"œà(x) =
X"
PRELIMINARIES,0.09896907216494845,"Œ±1,...,Œ±n‚àí1
M Œ±0Œ±1
x1
¬∑ ¬∑ ¬∑ M Œ±n‚àí1Œ±n
xn
=
X Œ± n
Y"
PRELIMINARIES,0.10103092783505155,"i=1
M Œ±i‚àí1Œ±i
xi
,
(3)"
PRELIMINARIES,0.10309278350515463,"where the left-and-right-most bond dimensions are assumed to be D(Œ±0) = D(Œ±n+1) = 1 so are not
summed."
PRELIMINARIES,0.10515463917525773,"4.2
Autoregressive Neural TensorNet (ANTN)"
PRELIMINARIES,0.10721649484536082,"Elementwise and Blockwise Construction for ANTN. Both ARNN and MPS are powerful ansatzes
for parameterizing quantum many-body wavefunctions. Albeit very expressive, ARNN lacks the
physics prior of the system of interest. In addition, since wavefunctions are complex-valued in general,
learning the sign structure can be a nontrivial task (Westerhout et al., 2020). MPS, on the other hand,
contains the necessary physics prior and can efficiently represent local or quasi-local sign structures,
but its expressivity is severely limited. The internal bond dimension needs to grow exponentially to
account for a linear increase in entanglement. It turns out that MPS representation is not unique in
that many MPS actually represent the same wavefunction; and if we choose a particular constraint
(without affecting the expressivity), MPS allows for efficient evaluation of conditional probability
and exact sampling (Ferris & Vidal, 2012) (see Appendix B.2). Because both these ansatzes allow
for exact sampling, it is natural to combine them to produce a more powerful ansatz. Therefore,
we develop the Autoregressive Neural TensorNet (ANTN) (Fig. 1 (middle)). In the last layer of
the ANTN, instead of outputting the conditional wavefunction œà(xi|x<i), we output a conditional
wavefunction tensor ÀúœàŒ±i‚àí1Œ±i(xi|x<i) for each site. Defining the left partially contracted tensor up to
qubit j as ÀúœàŒ±j
L (x‚â§j) := P"
PRELIMINARIES,0.10927835051546392,"Œ±<j
Qj
i=1 ÀúœàŒ±i‚àí1Œ±i(xi|x<i), we can define the (unnormalized) marginal
probability distribution as"
PRELIMINARIES,0.11134020618556702,"q(x‚â§j) :=
X Œ±j"
PRELIMINARIES,0.1134020618556701,"ÀúœàŒ±j
L (x‚â§j) ÀúœàŒ±j
L (x‚â§j)‚àó,
(4)"
PRELIMINARIES,0.1154639175257732,"where ‚àódenotes complex conjugation. Then, the (normalized) conditional probability can be obtained
as q(xj|x<j) = q(x‚â§j)/ P"
PRELIMINARIES,0.11752577319587629,"xj q(x‚â§j). We construct the overall wavefunction by defining both its
amplitude and phase according to"
PRELIMINARIES,0.11958762886597939,"œà(x) :=
p"
PRELIMINARIES,0.12164948453608247,"q(x)eiœï(x),
(5)"
PRELIMINARIES,0.12371134020618557,"with q(x) =: Qn
i=1 q(xi|x<i) and the phase œï(x) =: Arg P"
PRELIMINARIES,0.12577319587628866,"Œ±
Qn
i=1 ÀúœàŒ±i‚àí1,Œ±i(xi|x<i). In other
words, we define the amplitude of the wavefunction through the conditional probability distributions
and define the phase analogous to the standard MPS."
PRELIMINARIES,0.12783505154639174,"We develop two different constructions for ANTN that differ in the last layer on how to construct
conditional wavefunction tensors."
PRELIMINARIES,0.12989690721649486,The elementwise ANTN is given by
PRELIMINARIES,0.13195876288659794,"ÀúœàŒ±i‚àí1Œ±i(xi|x<i) = M Œ±i‚àí1Œ±i
xi
+ fNN(xi, Œ±i‚àí1, Œ±i|x<i),
(6)"
PRELIMINARIES,0.13402061855670103,"where fNN(xi, Œ±i‚àí1, Œ±i|x<i) is the complex-valued output. The blockwise ANTN is given by"
PRELIMINARIES,0.1360824742268041,"ÀúœàŒ±i‚àí1Œ±i(xi|x<i) = M Œ±i‚àí1Œ±i
xi
+ fNN(xi|x<i),
(7)"
PRELIMINARIES,0.13814432989690723,"where the complex-valued output fNN(xi|x<i) is broadcasted over Œ±. This results in a lower
complexity that allows us to use a larger maximum bond dimension."
PRELIMINARIES,0.1402061855670103,"Transformer and PixelCNN Based ANTN. Our construction above is general and can be applied to
any standard ARNN. Depending on the application, we can use different ARNN architectures. In this
work, we choose the transformer and PixelCNN depending on the specific tasks. The transformer
(Vaswani et al., 2017) used here is similar to a decoder-only transformer implemented in (Luo et al.,
2020), and The PixelCNN we use is the gated PixelCNN (Van den Oord et al., 2016) implemented in
(Chen et al., 2022)."
PRELIMINARIES,0.1422680412371134,"MPS Initialization. Since our ANTN generalizes from TN, both the elementwise and the blockwise
can take advantage of the optimized MPS from algorithms such as DMRG (White, 1992) as an
initialization. In practice, we can initialize the TN component with the optimized DMRG results of
the same bond dimension (similar to (Lami et al., 2022; Wu et al., 2022)). The MPS Initialization
can also be thought of as a pretraining of the ANTN. This is a nice feature since it provides a good
sign structure and inductive bias from the physics structure, which does not exist in the conventional
ARNN."
PRELIMINARIES,0.14432989690721648,"Limitations. In this work, we only integrated MPS into the ANTN, where MPS may not be the best
TN in various settings. Besides MPS, many other TNs also allow efficient evaluation of conditional
probabilities and exact sampling, such as MERA (Vidal, 2007) and PEPS (Verstraete & Cirac, 2004),
or cylindrical MPS for periodic boundary conditions. In fact, the recently developed TensorRNN
(Wu et al., 2022) can also be viewed as a type of TN and can be integrated into our construction for
future work. In addition, while the ANTN construction is general in terms of the base ARNN used,
our current study only focuses on transformer and PixelCNN. Lastly, as shown later in Sec. 5.1, the
current implementation of ANTN has an additional sampling overhead that is linear in system size
which can be avoided."
THEORETICAL RESULTS,0.1463917525773196,"5
Theoretical Results"
EXACT SAMPLING AND COMPLEXITY ANALYSIS OF ANTN,0.14845360824742268,"5.1
Exact Sampling and Complexity Analysis of ANTN"
EXACT SAMPLING AND COMPLEXITY ANALYSIS OF ANTN,0.15051546391752577,"Theorem 5.1. Autoregressive Neural TensorNet wavefunction is automatically normalized and allows
for exact sampling."
EXACT SAMPLING AND COMPLEXITY ANALYSIS OF ANTN,0.15257731958762888,"Proof. This is a direct consequence that we defined the amplitude of the wavefunction through
normalized conditional probability distributions q(xi|x<i). (See Appendix B.1 for the detailed
sampling procedure.)"
EXACT SAMPLING AND COMPLEXITY ANALYSIS OF ANTN,0.15463917525773196,"Complexity Analysis. We first note that for MPS, the number of parameters and computational
complexity for evaluating a bitstring x scales as O(nœá2), where n is the number of particles
and œá = D(Œ±) is the (maximum) bond dimension of MPS. The sampling complexity scales as
O(n2œá2). The DMRG algorithm has a computational complexity of O(nœá3) (White, 1992). The
number of parameters and computational complexity of ARNN depends on the specific choice.
Assuming the ARNN has nARNN parameters and a single pass (evaluation) of the ARNN has a
computational complexity of cARNN, then the sampling complexity of the ARNN is O(ncARNN) in
our current implementation. The ANTN based on such ARNN would have O(nARNN + nœá2hdim)
parameters for the elementwise construction, and O(nARNN + nœá2 + nhdim) parameters for the
blockwise construction. The computational complexity for evaluating and sampling bitstrings scales
as O(ns(cARNN +nœá2hdim)) for elementwise construction and O(ns(cARNN +nœá2)) for blockwise
construction with s = 0 for evaluation and s = 1 for sampling."
EXACT SAMPLING AND COMPLEXITY ANALYSIS OF ANTN,0.15670103092783505,"We note that the complexity analysis above is based on our current implementation, where the
sampling procedure for all algorithms has an O(n) overhead compared to evaluation. It is possible to
remove it by storing the partial results during the sampling procedure."
EXACT SAMPLING AND COMPLEXITY ANALYSIS OF ANTN,0.15876288659793814,"Then, each gradient update of the variational Monte Carlo algorithm requires sampling once and
evaluating Ncon times where Ncon is the number of connected (non-zero) x‚Ä≤‚Äôs in Hx,x‚Ä≤ given x, which
usually scales linearly with the system size. Then, the overall complexity is O(Ns(Nconceva+csamp))
with Ns the batch size, ceva the evaluation cost and csamp the sampling cost. Usually, the first part
dominates the cost."
EXACT SAMPLING AND COMPLEXITY ANALYSIS OF ANTN,0.16082474226804125,"The difference in computational complexities and number of parameters between elementwise ANTN
and blockwise ANTN implies that blockwise ANTN is usually more economical than elementwise
ANTN for the same hidden dimsnieon hdim and bond dimension œá. Therefore, for small bond
dimensions, we use the elementwise ANTN for a more flexible parameterization with a higher cost.
In contrast, for large bond dimensions, we use the blockwise ANTN, which saves computational
complexity and improves the initial performance (with DMRG initialization) of the blockwise ANTN
at the cost of less flexible modifications from the ARNN. We also note that compared to the state-of-
the-art MPS simulation, even for our blockwise ANTN, a small bond dimension usually suffices. For
example, in the later experimental section, we use bond dimension 70 for blockwise ANTN while
the best MPS results use bond dimension 1024, which has much more parameters than our ANTN.
In general, our ANTN has much fewer parameters compared to MPS due to the O(œá2) parameters
scaling which dominates at large bond dimension."
EXPRESSIVITY RESULTS OF ANTN,0.16288659793814433,"5.2
Expressivity Results of ANTN"
EXPRESSIVITY RESULTS OF ANTN,0.16494845360824742,"Theorem 5.2. Autoregressive Neural TensorNet can have volume law entanglement, which is strictly
beyond the expressivity of matrix product states."
EXPRESSIVITY RESULTS OF ANTN,0.1670103092783505,"Proof. We proved in Thm. 5.3 that ANTN can be reduced to an ARNN, which has been shown to
have volume law entanglement that cannot be efficiently represented by TN (Sharir et al., 2021;
Levine et al., 2019b). Hence, ANTN can represent states that cannot in general be represented by
MPS efficiently; thus it has strictly greater expressivity.
Theorem 5.3. Autoregressive Neural TensorNet has generalized expressivity over tensor networks
and autoregressive neural networks."
EXPRESSIVITY RESULTS OF ANTN,0.16907216494845362,"Proof. Thm B.2of Appendix B.4 shows that both TN and ARNN are special cases as ANTN and
Thm B.3of Appendix B.4 shows that ANTN can be written as either a TN or an ARNN with an
exponential (in system size) number of parameters. Thus, ANTN generalizes the expressivity over
both TN and ARNN."
SYMMETRY RESULTS OF ANTN,0.1711340206185567,"5.3
Symmetry Results of ANTN"
SYMMETRY RESULTS OF ANTN,0.1731958762886598,"Symmetry plays an important role in quantum many-body physics and quantum chemistry. Many of
the symmetries can be enforced in ARNN via the two classes of the symmetries‚Äîmask symmetry
and function symmetry.
Definition 5.1 (Mask Symmetry). A conditional wavefunction œà(xi|x<i) has a mask symmetry if
œà(xi|x<i) = 0 for some xi given x<i.
Definition 5.2 (Function Symmetry). A conditional wavefunction tensor œà(xi|x<i) has a func-
tion symmetry over a function F if œà(xi|x<i) = œà(F(xi; x<i)|F (x<i)) where F (x‚â§i) :=
{F(x1), F(x2; x1), . . . , F(xi; x<i)}."
SYMMETRY RESULTS OF ANTN,0.17525773195876287,"Here, we list several symmetries that ANTN inherits from ARNN and show the proofs in Ap-
pendix B.4.
Theorem 5.4. Autoregressive Neural TensorNet inherits mask symmetry and function symmetry from
autoregressive neural networks.
Corollary 5.4.1 (Global U(1) Symmetry). Autoregressive Neural TensorNet can realize global U(1)
symmetry, which conserves particle number.
Corollary 5.4.2 (Z2 Spin Flip Symmetry). Autoregressive Neural TensorNet can realize Z2 spin flip
symmetry such that the wavefunction is invariant under conjugation of the input."
SYMMETRY RESULTS OF ANTN,0.177319587628866,MPS (4)
SYMMETRY RESULTS OF ANTN,0.17938144329896907,MPS(16)
SYMMETRY RESULTS OF ANTN,0.18144329896907216,MPS (64)
SYMMETRY RESULTS OF ANTN,0.18350515463917524,MPS (256) ARNN
SYMMETRY RESULTS OF ANTN,0.18556701030927836,ANTN (4) 0.0 0.2 0.4 0.6 0.8 1.0
SYMMETRY RESULTS OF ANTN,0.18762886597938144,Fidelity
SYMMETRY RESULTS OF ANTN,0.18969072164948453,"(a)
Random Bell State"
SYMMETRY RESULTS OF ANTN,0.19175257731958764,MPS (4)
SYMMETRY RESULTS OF ANTN,0.19381443298969073,MPS(16)
SYMMETRY RESULTS OF ANTN,0.1958762886597938,MPS (64)
SYMMETRY RESULTS OF ANTN,0.1979381443298969,MPS (256) ARNN
SYMMETRY RESULTS OF ANTN,0.2,ANTN (4) 0.0 0.2 0.4 0.6 0.8 1.0
SYMMETRY RESULTS OF ANTN,0.2020618556701031,Fidelity
SYMMETRY RESULTS OF ANTN,0.20412371134020618,"(b)
Shallow Random Circuit"
SYMMETRY RESULTS OF ANTN,0.20618556701030927,"no sign rule
sign rule"
SYMMETRY RESULTS OF ANTN,0.20824742268041238,"Figure 2: Fidelity ‚Üëon quantum state learning with 16 qubits for TN (MPS), ARNN (transformer)
and ANTN (elementwise construction with transformer+MPS). (a) Learning random Bell states. (b)
Learning real-valued depth-4 random circuit with and without sign rule. The error bar denotes the
standard deviation (not the standard error of the mean) of the fidelities over the random states sampled
from the corresponding distribution. The mean and standard deviation are calculated from 10 random
states. The numbers inside the parentheses denote the bond dimension."
SYMMETRY RESULTS OF ANTN,0.21030927835051547,"Corollary 5.4.3 (Discrete Abelian and Non-Abelian Symmetries). Autoregressive Neural TensorNet
can realize discrete Abelian and Non-Abelian symmetries."
EXPERIMENTS,0.21237113402061855,"6
Experiments"
QUANTUM STATE LEARNING,0.21443298969072164,"6.1
Quantum State Learning"
QUANTUM STATE LEARNING,0.21649484536082475,"As stated previously, ANTN generalizes both ARNN and TN to take advantage of both the expressivity
and the inductive bias. Here, we test the ANTN on learning physically important quantum states to
demonstrate this ability. In this task, we use the transformer neural network for ARNN and ANTN
due to the 1D structure of the system."
QUANTUM STATE LEARNING,0.21855670103092784,"Experiments on expressivity. We first test the expressivity of the ANTN by learning a class of
well-known high-entangled states‚Äîrandom permuted Bell states. A 2-qubit Bell state is defined as
(|00‚ü©+ |11‚ü©)/
‚àö"
QUANTUM STATE LEARNING,0.22061855670103092,"2, which has 1 bit of entanglement. For a system size of n, we randomly pair qubits
in the first half of the system with the qubits in the second half of the system to be Bell states. This
process creates quantum states with n/2 bit of entanglement between the first and second half of
the system. It can be shown that a bond dimension of 2n/2 is required for MPS to fully represent
such a system. In Fig. 2 (a), we plot the quantum fidelity of learning 16-qubit random Bell states. As
the figure shows, MPS cannot represent the states without reaching the required bond dimension of
256 = 28. The ARNN and ANTN, on the other hand, can represent such states without limitations
from the bond dimension."
QUANTUM STATE LEARNING,0.22268041237113403,"Experiments on inductive bias. We then test the physics inductive bias of the ANTN. One of the
physics inductive biases of MPS is that it can represent wavefunctions with local or quasi-local sign
structures that can be hard for neural networks to learn. These wavefunctions can have a fluctuating
sign depending on the configuration x. Here, we use (real-valued) shallow random circuits to mimic
the short-range interaction and generate states with sign structures. We test the algorithms on these
states both with and without the ‚Äúsign rule‚Äù, which means that we explicitly provide the sign structure
to the algorithm. As shown in Fig. 2 (b), the fidelity of MPS only depends weakly on the sign rule,
whereas the fidelity of ARNN can change drastically. Our ANTN inherits the property of MPS and
is thus not affected by the sign structure. Furthermore, being more expressive, ANTN with a bond
dimension of 4 already performs better than MPS with a bond dimension of 16."
VARIATIONAL MONTE CARLO,0.22474226804123712,"6.2
Variational Monte Carlo"
VARIATIONAL MONTE CARLO,0.2268041237113402,"We further test our algorithm on finding the ground state of the challenging 2D J1-J2 Heisenberg
model with open boundary condition. The model has a rich phase diagram with at least three different
phases across different J2/J1 values (Capriotti et al., 2004) (Lante & Parola, 2006). In addition, the"
VARIATIONAL MONTE CARLO,0.2288659793814433,"Energy per site ‚Üì8 √ó 8
Algorithms
J2 = 0.2
J2 = 0.5
J2 = 0.8
RBM (NS)
-1.9609(16)
-1.5128(20)
-1.7508(19)
RBM (S)
-2.1944(17)
-1.8625(20)
-0.7267(29)
PixelCNN (NS)
-2.21218(16)
-1.77058(29)
-1.93825(16)
PixelCNN (S)
-2.23171(9)
-1.88902(19)
-1.83672(26)
Elementwise (8 NS)
-2.23690(4)
-1.93018(8)
-2.00036(16)
Elementwise (8 S)
-2.23688(4)
-1.93102(7)
-2.00262(11)
Blockwise (70 NS)
-2.23484(7)
-1.93000(7)
-1.99148(13)
Blockwise (70 S)
-2.23517(6)
-1.92880(9)
-1.99246(13)"
VARIATIONAL MONTE CARLO,0.2309278350515464,"Table 1: Energy per site ‚Üìfor 8 √ó 8 system with
various algorithms where elementwise and blockwise
are two constructions of ANTN (with PixelCNN +
MPS). The bond dimensions for ANTN are labeled
inside the parentheses. For each algorithm, we test it
both with the sign rule (S) and without the sign rule
(NS) The best energy is highlighted in boldface and
the second in italic."
VARIATIONAL MONTE CARLO,0.2329896907216495,"16
36
64
100
144
Number of qubits 0.01 0.00 0.01"
VARIATIONAL MONTE CARLO,0.23505154639175257,"Elementwise 
 DMRG"
VARIATIONAL MONTE CARLO,0.23711340206185566,J2 = 0.2
VARIATIONAL MONTE CARLO,0.23917525773195877,J2 = 0.5
VARIATIONAL MONTE CARLO,0.24123711340206186,J2 = 0.8
VARIATIONAL MONTE CARLO,0.24329896907216494,"Figure 3: Energy per site difference
‚Üìbetween elementwise ANTN with
bond dimension 8 and MPS with
bond dimension 1024 optimized us-
ing DMRG algorithm for various J2
and different system sizes from 4 √ó 4
to 12 √ó 12."
VARIATIONAL MONTE CARLO,0.24536082474226803,"Energy per site ‚Üì10 √ó 10
Algorithms
J2 = 0.2
J2 = 0.3
J2 = 0.4
J2 = 0.5
J2 = 0.6
J2 = 0.7
J2 = 0.8
MPS (8)
-1.997537
-1.893753
-1.797675
-1.734326
-1.716253
-1.768225
-1.869871
MPS (70)
-2.191048
-2.069029
-1.956480
-1.866159
-1.816249
-1.854296
-2.007845
MPS (1024)
-2.255633
-2.138591
-2.031681
-1.938770
-1.865561
-1.894371
-2.062730
PixelCNN
-2.22462(24)
-2.12873(14) -2.02053(14) -1.74098(29) -1.71885(27) -1.81800(13)
-1.98331(17)
Elementwise (8) -2.26034(6)
-2.14450(4) -2.03727(7) -1.94001(6) -1.85684(10) -1.88643(7)
-2.05707(43)
Blockwise (70)
-2.25755(8)
-2.14152(8) -2.03319(10) -1.93842(42) -1.85270(12) -1.87853(13)
-2.05088(14)
Energy per site ‚Üì12 √ó 12
Algorithms
J2 = 0.2
J2 = 0.3
J2 = 0.4
J2 = 0.5
J2 = 0.6
J2 = 0.7
J2 = 0.8
MPS (8)
-1.998207
-1.887531
-1.800784
-1.735906
-1.720619
-1.788652
-1.893916
MPS (70)
-2.185071
-2.059443
-1.944832
-1.851954
-1.812450
-1.853650
-2.030131
MPS (1024)
-2.264084
-2.141043
-2.027736
-1.931243
-1.858846
-1.913483
-2.093013
PixelCNN
-2.24311(102) -2.12616(23) -2.01768(21) -1.74282(30) -1.72637(16) -1.85239(13)
-2.03226(59)
Elementwise (8) -2.27446(27) -2.15537(6) -2.04437(7) -1.94686(6) -1.85443(15) -1.91391(10) -2.09457(10)
Blockwise (70)
-2.26152(50)
-2.15395(7) -2.04225(8) -1.94298(43) -1.85176(15) -1.90571(12)
-2.09113(43)"
VARIATIONAL MONTE CARLO,0.24742268041237114,"Table 2: Energy per site ‚Üìfor 10√ó10 and 12√ó12 system with various algorithms where elementwise
and blockwise are two constructions of ANTN (with PixelCNN + MPS). The bond dimensions for
MPS and ANTN are labeled in parentheses. The MPS is optimized with DMRG algorithm. The best
energy is highlighted in boldface and the second best value is highlighted in italic."
VARIATIONAL MONTE CARLO,0.24948453608247423,"complicated structure of its ground state makes it a robust model on which to test state-of-the-art
methods. Here, we use the PixelCNN for ARNN and ANTN due to the 2D geometry of the system."
VARIATIONAL MONTE CARLO,0.2515463917525773,The 2D J1-J2 Hamiltonian is given by
VARIATIONAL MONTE CARLO,0.2536082474226804,"ÀÜH = J1
X"
VARIATIONAL MONTE CARLO,0.2556701030927835,"‚ü®i,j‚ü©
‚ÉóœÉi ¬∑ ‚ÉóœÉj + J2
X"
VARIATIONAL MONTE CARLO,0.25773195876288657,"‚ü®‚ü®i,j‚ü©‚ü©
‚ÉóœÉi ¬∑ ‚ÉóœÉj,
(8)"
VARIATIONAL MONTE CARLO,0.2597938144329897,"where subscript refers to the site of the qubits, ‚ü®¬∑, ¬∑‚ü©is the nearest neighbour and ‚ü®‚ü®¬∑, ¬∑‚ü©‚ü©is the next
nearest neighbour. ‚ÉóœÉi ¬∑ ‚ÉóœÉj = Xi ‚äóXj + Yi ‚äóYj + Zi ‚äóZj with X, Y and Z the Pauli matrices"
VARIATIONAL MONTE CARLO,0.2618556701030928,"X =

0
1
1
0"
VARIATIONAL MONTE CARLO,0.2639175257731959,"
,
Y =

0
‚àíi
i
0"
VARIATIONAL MONTE CARLO,0.26597938144329897,"
,
Z =

1
0
0
‚àí1"
VARIATIONAL MONTE CARLO,0.26804123711340205,"
.
(9)"
VARIATIONAL MONTE CARLO,0.27010309278350514,We will fix J1 = 1 and vary J2 in our studies.
VARIATIONAL MONTE CARLO,0.2721649484536082,Experiments on inductive bias of J1-J2 Heisenberg Model.
VARIATIONAL MONTE CARLO,0.27422680412371137,"As shown previously, it can be challenging for neural networks to learn the sign structures. The
ground state of the J1-J2 model also has a sign structure, which can be partially captured by the
Marshall sign rule(Marshall, 1955). The sign rule is exact at J2 = 0 and becomes worse for large J2.
This could introduce bias to the neural network wavefunction if directly applied. We compare the
restricted Boltzmann machine (RBM) (from NetKet (Vicentini et al., 2022)), our implementation of
gated PixelCNN, and the two different constructions of ANTN with and without the sign rule."
VARIATIONAL MONTE CARLO,0.27628865979381445,"The result is shown in Table 1. Since our approach is based on the variational principle discussed
in Sec. 3, it provides an upper bound on the exact ground state energy; therefore, a lower energy
implies a better state. As shown in the results, both RBM and PixelCNN improve significantly with
the application of the sign rule at J2 = 0.2 and J2 = 0.5, but the results deteriorate at J2 = 0.8.
This is expected because the sign rule becomes increasingly inaccurate as J2 increases, especially
past J2 = 0.5. Our ANTN, on the other hand, does not require the sign rule in both constructions.
As an additional comparison, we note that both of our ANTN constructions achieved better results
compared to the recently developed matrix product backflow state (Lami et al., 2022), which uses the
sign rule and has a per site energy of ‚àí1.9267 at J2 = 0.5."
VARIATIONAL MONTE CARLO,0.27835051546391754,"Ablation study on J1-J2 Heisenberg Model. We scan across many J2 for the model without
using the sign rule. In this experiment, we compare the performance of six models to compute the
ground state energy of the J1-J2 Hamiltonian system with J1 = 1 fixed and J2 = 0.2-0.8 with 0.1
increments, covering three different phases of the model. The first three models are TN models, using
the MPS with bond dimensions of 8, 70, and 1024. For up to 4 √ó 4 system, the MPS results with bond
dimension 1024 are exact and can be regarded as a benchmark. The fourth model is a PixelCNN pure
neural network; the fifth and sixth models are elementwise and blockwise ANTN models. Thus the
experiment compares the ANTN against the two previous state-of-the-art techniques."
VARIATIONAL MONTE CARLO,0.2804123711340206,"Table 2 summarizes the per site ground state energy computed by different models at different J2‚Äôs
in three distinct phases. Our results provide strong evidence that ANTN integrating TN and ARNN
outperforms each base model, achieving state-of-the-art results."
VARIATIONAL MONTE CARLO,0.2824742268041237,"Comparison between ANTN and MPS. In all cases the ANTN surpasses the MPS with the
corresponding bond dimension on which they are based. For example, even in the 10 √ó 10 system
with J2 > 0.5, where MPS with a bond dimension of 1024 outperforms the ANTN, the elementwise
ANTN still significantly improves on the base MPS with a bond dimension of 8 and the blockwise
ANTN improves on the base MPS with a bond dimension of 70. It is consistent with Theorem. 5.3
and Theorem. 5.2 that ANTN is more expressive than TN."
VARIATIONAL MONTE CARLO,0.2845360824742268,"In addition, the ANTN models scale well for larger systems. Figure 3 visualizes the scalability of
ANTN compared to MPS. The figure plots the difference in per site energy between the elementwise
ANTN with bond dimension 8 and MPS with bond dimension 1024 for J2 = 0.2, 0.5, 0.8 where
lower energies signify better performance for the elementwise ANTN. Compared to MPS, the ANTN
models compute better ground state energies as the system size grows larger. As the system size
increases, the elementwise ANTN with bond dimension 8 starts to outperform MPS with bond
dimension 1024. Even at J2 = 0.6, where the elementwise ANTN is slightly worse than MPS, the
difference gets significantly smaller at 12√ó12 compared to 10√ó10 (Table 2). In fact, the elementwise
ANTN achieves such a performance using only ‚àº1% the number of parameters of MPS. We note
that for large J2, the system goes into a stripped phase (Nomura & Imada, 2021), which can be less
challenging for MPS to represent. Nevertheless, in almost all cases our ANTN still outperforms MPS
on the 12 √ó 12 system. MPS has a cost dominated by the bond dimension (quadratic for memory
and cubic for computational), which limits its use in practice for large system sizes that require large
bond dimensions. According to the complexity analysis in Sec. 5.1, ANTN has lower complexity
than TN and thus scales better for larger systems."
VARIATIONAL MONTE CARLO,0.2865979381443299,"Comparison between ANTN and ARNN. The elementwise and blockwise ANTN models also
consistently outperform the pure neural network PixelCNN model. This agrees with Theorem 5.3
that both elementwise and blockwise ANTN have a generalized expressivity compared to ARNN. In
Appendix C we further compare ANTN with ARNN and find that: (a) it is challenging to surpass
ANTN by ARNN even with an increased number of parameters; (b) the improvement from ANTN
mainly comes from the effective inductive bias of MPS structure instead of the DMRG initialization;
(c) ANTN has a favorable scaling compared to ARNN and MPS for reaching accurate results."
VARIATIONAL MONTE CARLO,0.28865979381443296,The details of all experiment setups and hyperparameters can be found in Appendix D.
CONCLUSION,0.2907216494845361,"7
Conclusion"
CONCLUSION,0.2927835051546392,"In this paper, we developed Autoregressive Neural TensorNet, bridging the two state-of-the-art meth-
ods in the field, tensor networks, and autoregressive neural networks. We proved that Autoregressive
Neural TensorNet is self-normalized with exact sampling, naturally generalizes the expressivity"
CONCLUSION,0.2948453608247423,"of both tensor networks and autoregressive neural networks, and inherits proper physics inductive
bias (e.g. sign structures) from tensor networks and various symmetries from autoregressive neural
networks. We demonstrated our approach on quantum state learning and the challenging 2D J1-J2
Heisenberg model with different system sizes and couplings. Our approach achieves better perfor-
mance than both the original tensor network and autoregressive neural network while surpassing
tensor networks with large bond dimensions as the system size increases. In addition, our approach
is robust, independent of sign rule. Besides scientific applications, since both tensor networks and
autoregressive neural networks have been applied to machine learning tasks such as supervised
learning and generative modeling, our novel approach holds promise for better performance in these
domains due to its exact sampling, expressivity, and symmetries."
CONCLUSION,0.29690721649484536,Broader Impact
CONCLUSION,0.29896907216494845,"Our Autoregressive Neural TensorNet, blending tensor networks and autoregressive neural networks,
could advance our grasp of quantum phenomena, potentially fueling scientific breakthroughs. It
enhances quantum material design and quantum computing through improved simulations and control
of quantum states. This technology may also inspire new machine learning models for handling
high-dimensional data. However, possible ethical and societal impacts, such as the use for chemical
weapon development, require careful scrutiny."
CONCLUSION,0.30103092783505153,Acknowledgement
CONCLUSION,0.3030927835051546,"The authors acknowledge support from the National Science Foundation under Cooperative Agree-
ment PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions,
http://iaifi.org/). This material is based upon work supported by the U.S. Department of
Energy, Office of Science, National Quantum Information Science Research Centers, Co-design
Center for Quantum Advantage (C2QA) under contract number DE-SC0012704. This material is
also in part based upon work supported by the Air Force Office of Scientific Research under the
award number FA9550-21-1-0317. The authors acknowledge the MIT SuperCloud (Reuther et al.,
2018) and Lincoln Laboratory Supercomputing Center for providing (HPC, database, consultation)
resources that have contributed to the research results reported within this paper. Some computations
in this paper were run on the FASRC cluster supported by the FAS Division of Science Research
Computing Group at Harvard University. ZC acknowledges the DARPA 134371-5113608 award and
NSF 10434."
REFERENCES,0.30515463917525776,References
REFERENCES,0.30721649484536084,"Barrett, T. D., Malyshev, A., and Lvovsky, A. Autoregressive neural-network wavefunctions for ab
initio quantum chemistry. Nature Machine Intelligence, 4(4):351‚Äì358, 2022."
REFERENCES,0.30927835051546393,"Capriotti, L., Fubini, A., Roscilde, T., and Tognetti, V. Ising transition in the two-dimensional
quantum j 1- j 2 heisenberg model. Physical review letters, 92(15):157202, 2004."
REFERENCES,0.311340206185567,"Carleo, G. and Troyer, M. Solving the quantum many-body problem with artificial neural networks.
Science, 355(6325):602‚Äì606, 2017. doi: 10.1126/science.aag2302. URL https://www.science.
org/doi/abs/10.1126/science.aag2302."
REFERENCES,0.3134020618556701,"Chen, Z., Luo, D., Hu, K., and Clark, B. K. Simulating 2+ 1d lattice quantum electrodynamics at
finite density with neural flow wavefunctions. arXiv preprint arXiv:2212.06835, 2022."
REFERENCES,0.3154639175257732,"Choo, K., Neupert, T., and Carleo, G. Two-dimensional frustrated J1‚àíJ2 model studied with neural
network quantum states. Phys. Rev. B, 100:125124, Sep 2019. doi: 10.1103/PhysRevB.100.125124.
URL https://link.aps.org/doi/10.1103/PhysRevB.100.125124."
REFERENCES,0.31752577319587627,"Deng, D.-L., Li, X., and Das Sarma, S. Quantum entanglement in neural network states. Physical
Review X, 7(2), May 2017. ISSN 2160-3308. doi: 10.1103/physrevx.7.021021. URL http:
//dx.doi.org/10.1103/PhysRevX.7.021021."
REFERENCES,0.31958762886597936,"Ferris, A. J. and Vidal, G. Perfect sampling with unitary tensor networks. Phys. Rev. B, 85:165146,
Apr 2012. doi: 10.1103/PhysRevB.85.165146. URL https://link.aps.org/doi/10.1103/
PhysRevB.85.165146."
REFERENCES,0.3216494845360825,"Gao, X. and Duan, L.-M. Efficient representation of quantum many-body states with deep neural
networks.
Nature Communications, 8(1):662, Sep 2017.
ISSN 2041-1723.
doi: 10.1038/
s41467-017-00705-2. URL https://www.nature.com/articles/s41467-017-00705-2."
REFERENCES,0.3237113402061856,"Girvin, S. M. and Yang, K. Modern condensed matter physics. Cambridge University Press, 2019."
REFERENCES,0.32577319587628867,"Greensmith, E., Bartlett, P. L., and Baxter, J. Variance reduction techniques for gradient estimates in
reinforcement learning. J. Mach. Learn. Res., 5:1471‚Äì1530, December 2004. ISSN 1532-4435."
REFERENCES,0.32783505154639175,"Guti√©rrez, I. L. and Mendl, C. B. Real time evolution with neural-network quantum states, 2020."
REFERENCES,0.32989690721649484,"Hartmann, M. J. and Carleo, G. Neural-network approach to dissipative quantum many-body
dynamics. Phys. Rev. Lett., 122:250502, Jun 2019. doi: 10.1103/PhysRevLett.122.250502. URL
https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.122.250502."
REFERENCES,0.3319587628865979,"Hermann, J., Sch√§tzle, Z., and No√©, F. Deep neural network solution of the electronic schr√∂dinger
equation, 2019."
REFERENCES,0.334020618556701,"Hibat-Allah, M., Ganahl, M., Hayward, L. E., Melko, R. G., and Carrasquilla, J.
Recurrent
neural network wave functions.
Phys. Rev. Research, 2:023358, Jun 2020.
doi: 10.1103/
PhysRevResearch.2.023358. URL https://journals.aps.org/prresearch/abstract/10.
1103/PhysRevResearch.2.023358."
REFERENCES,0.33608247422680415,"Huang, Y. and Moore, J. E. Neural network representation of tensor network and chiral states.
Phys. Rev. Lett., 127:170601, Oct 2021. doi: 10.1103/PhysRevLett.127.170601. URL https:
//link.aps.org/doi/10.1103/PhysRevLett.127.170601."
REFERENCES,0.33814432989690724,"Lami, G., Carleo, G., and Collura, M. Matrix product states with backflow correlations. Physical
Review B, 106(8), aug 2022. doi: 10.1103/physrevb.106.l081111. URL https://doi.org/10.
1103%2Fphysrevb.106.l081111."
REFERENCES,0.3402061855670103,"Lante, V. and Parola, A. The ising phase in the j1-j2 heisenberg model. Physical Review, 2006."
REFERENCES,0.3422680412371134,"Levine, Y., Sharir, O., Cohen, N., and Shashua, A. Quantum entanglement in deep learning architec-
tures. Physical Review Letters, 122(6), Feb 2019a. ISSN 1079-7114. doi: 10.1103/physrevlett.122.
065301.
URL https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.122.
065301."
REFERENCES,0.3443298969072165,"Levine, Y., Sharir, O., Cohen, N., and Shashua, A. Quantum entanglement in deep learning architec-
tures. Physical review letters, 122(6):065301, 2019b."
REFERENCES,0.3463917525773196,"Lu, S., Gao, X., and Duan, L.-M. Efficient representation of topologically ordered states with restricted
boltzmann machines. Phys. Rev. B, 99:155136, Apr 2019. doi: 10.1103/PhysRevB.99.155136.
URL https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.155136."
REFERENCES,0.34845360824742266,"Luo, D. and Clark, B. K.
Backflow transformations via neural networks for quantum many-
body wave functions. Physical Review Letters, 122(22), Jun 2019. ISSN 1079-7114. doi: 10.
1103/physrevlett.122.226401. URL https://journals.aps.org/prl/abstract/10.1103/
PhysRevLett.122.226401."
REFERENCES,0.35051546391752575,"Luo, D., Chen, Z., Carrasquilla, J., and Clark, B. K. Autoregressive neural network for simulating
open quantum systems via a probabilistic formulation, 2020."
REFERENCES,0.3525773195876289,"Luo, D., Carleo, G., Clark, B. K., and Stokes, J. Gauge equivariant neural networks for quantum
lattice gauge theories. Physical review letters, 127(27):276402, 2021a."
REFERENCES,0.354639175257732,"Luo, D., Chen, Z., Hu, K., Zhao, Z., Hur, V. M., and Clark, B. K. Gauge invariant autoregressive neural
networks for quantum lattice models, 2021b. URL https://arxiv.org/abs/2101.07243."
REFERENCES,0.35670103092783506,"Luo, D., Chen, Z., Carrasquilla, J., and Clark, B. K. Autoregressive neural network for simulating open
quantum systems via a probabilistic formulation. Phys. Rev. Lett., 128:090501, Feb 2022a. doi: 10.
1103/PhysRevLett.128.090501. URL https://link.aps.org/doi/10.1103/PhysRevLett.
128.090501."
REFERENCES,0.35876288659793815,"Luo, D., Yuan, S., Stokes, J., and Clark, B. K. Gauge equivariant neural networks for 2+ 1d u (1)
gauge theory simulations in hamiltonian formulation. arXiv preprint arXiv:2211.03198, 2022b."
REFERENCES,0.36082474226804123,"Marshall, W.
Antiferromagnetism.
Proceedings of the Royal Society of London. Series A,
Mathematical and Physical Sciences, 232(1188):48‚Äì68, 1955. ISSN 00804630. URL http:
//www.jstor.org/stable/99682."
REFERENCES,0.3628865979381443,"Nagy, A. and Savona, V. Variational quantum monte carlo method with a neural-network ansatz for
open quantum systems. Phys. Rev. Lett., 122:250501, Jun 2019. doi: 10.1103/PhysRevLett.122.
250501.
URL https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.122.
250501."
REFERENCES,0.3649484536082474,"Nomura, Y. and Imada, M. Dirac-type nodal spin liquid revealed by refined quantum many-body
solver using neural-network wave function, correlation ratio, and level spectroscopy. Physical
Review X, 11(3), aug 2021. doi: 10.1103/physrevx.11.031034. URL https://doi.org/10.
1103%2Fphysrevx.11.031034."
REFERENCES,0.3670103092783505,"Oseledets, I. V. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):2295‚Äì2317,
2011. doi: 10.1137/090752286. URL https://doi.org/10.1137/090752286."
REFERENCES,0.36907216494845363,"Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In
Advances in neural information processing systems, pp. 8026‚Äì8037, 2019."
REFERENCES,0.3711340206185567,"Pfau, D., Spencer, J. S., Matthews, A. G. D. G., and Foulkes, W. M. C. Ab initio solution of the many-
electron schr√∂dinger equation with deep neural networks. Phys. Rev. Research, 2:033429, Sep 2020.
doi: 10.1103/PhysRevResearch.2.033429. URL https://journals.aps.org/prresearch/
abstract/10.1103/PhysRevResearch.2.033429."
REFERENCES,0.3731958762886598,"Preskill, J. Quantum computing 40 years later. arXiv preprint arXiv:2106.10522, 2021."
REFERENCES,0.3752577319587629,"Reuther, A., Kepner, J., Byun, C., Samsi, S., Arcand, W., Bestor, D., Bergeron, B., Gadepally, V.,
Houle, M., Hubbell, M., Jones, M., Klein, A., Milechin, L., Mullen, J., Prout, A., Rosa, A., Yee,
C., and Michaleas, P. Interactive supercomputing on 40,000 cores for machine learning and data
analysis. In 2018 IEEE High Performance extreme Computing Conference (HPEC), pp. 1‚Äì6. IEEE,
2018."
REFERENCES,0.37731958762886597,"Schmitt, M. and Heyl, M. Quantum many-body dynamics in two dimensions with artificial neural net-
works. Physical Review Letters, 125(10), Sep 2020. ISSN 1079-7114. doi: 10.1103/physrevlett.125.
100503.
URL https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.125.
100503."
REFERENCES,0.37938144329896906,"Sharir, O., Levine, Y., Wies, N., Carleo, G., and Shashua, A. Deep autoregressive models for
the efficient variational simulation of many-body quantum systems. Physical Review Letters,
124(2), jan 2020. doi: 10.1103/physrevlett.124.020503. URL https://doi.org/10.1103%
2Fphysrevlett.124.020503."
REFERENCES,0.38144329896907214,"Sharir, O., Shashua, A., and Carleo, G. Neural tensor contractions and the expressive power of deep
neural quantum states, 2021."
REFERENCES,0.3835051546391753,"Van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A., et al. Conditional image
generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016."
REFERENCES,0.38556701030927837,"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., and
Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30:
5998‚Äì6008, 2017."
REFERENCES,0.38762886597938145,"Verstraete, F. and Cirac, J. I. Renormalization algorithms for quantum-many body systems in two
and higher dimensions, 2004. URL https://arxiv.org/abs/cond-mat/0407066."
REFERENCES,0.38969072164948454,"Vicentini, F., Biella, A., Regnault, N., and Ciuti, C. Variational neural-network ansatz for steady
states in open quantum systems. Physical Review Letters, 122(25), Jun 2019. ISSN 1079-7114.
doi: 10.1103/physrevlett.122.250503. URL https://journals.aps.org/prl/abstract/10.
1103/PhysRevLett.122.250503."
REFERENCES,0.3917525773195876,"Vicentini, F., Hofmann, D., Szab√≥, A., Wu, D., Roth, C., Giuliani, C., Pescia, G., Nys, J., Vargas-
Calder√≥n, V., Astrakhantsev, N., and Carleo, G. NetKet 3: Machine learning toolbox for many-body
quantum systems. SciPost Physics Codebases, aug 2022. doi: 10.21468/scipostphyscodeb.7. URL
https://doi.org/10.21468%2Fscipostphyscodeb.7."
REFERENCES,0.3938144329896907,"Vidal, G. Efficient classical simulation of slightly entangled quantum computations. Physical Review
Letters, 91(14), oct 2003. doi: 10.1103/physrevlett.91.147902. URL https://doi.org/10.
1103%2Fphysrevlett.91.147902."
REFERENCES,0.3958762886597938,"Vidal, G. Efficient simulation of one-dimensional quantum many-body systems. Physical Review
Letters, 93(4), jul 2004. doi: 10.1103/physrevlett.93.040502. URL https://doi.org/10.1103%
2Fphysrevlett.93.040502."
REFERENCES,0.3979381443298969,"Vidal, G.
Entanglement renormalization.
Phys. Rev. Lett., 99:220405, Nov 2007.
doi:
10.1103/PhysRevLett.99.220405. URL https://link.aps.org/doi/10.1103/PhysRevLett.
99.220405."
REFERENCES,0.4,"Vieijra, T., Casert, C., Nys, J., De Neve, W., Haegeman, J., Ryckebusch, J., and Verstraete, F.
Restricted boltzmann machines for quantum states with non-abelian or anyonic symmetries.
Physical Review Letters, 124(9), Mar 2020. ISSN 1079-7114. doi: 10.1103/physrevlett.124.097201.
URL https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.124.097201."
REFERENCES,0.4020618556701031,"Westerhout, T., Astrakhantsev, N., Tikhonov, K. S., Katsnelson, M. I., and Bagrov, A. A. General-
ization properties of neural network approximations to frustrated magnet ground states. Nature
Communications, 11(1):1593, Mar 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-15402-w.
URL https://doi.org/10.1038/s41467-020-15402-w."
REFERENCES,0.4041237113402062,"White, S. R. Density matrix formulation for quantum renormalization groups. Physical review letters,
69(19):2863, 1992."
REFERENCES,0.4061855670103093,"Wu, D., Rossi, R., Vicentini, F., and Carleo, G. From tensor network quantum states to tensorial
recurrent neural networks. arXiv preprint arXiv:2206.12363, 2022."
REFERENCES,0.40824742268041236,"Yoshioka, N. and Hamazaki, R. Constructing neural stationary states for open quantum many-
body systems. Phys. Rev. B, 99:214306, Jun 2019. doi: 10.1103/PhysRevB.99.214306. URL
https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.214306."
REFERENCES,0.41030927835051545,Appendices
REFERENCES,0.41237113402061853,"A
Additional Background"
REFERENCES,0.4144329896907217,"A.1
Quantum State Learning"
REFERENCES,0.41649484536082476,"In this section, we discuss more details about quantum state learning. As mentioned in the main
paper, the quantum fidelity"
REFERENCES,0.41855670103092785,F = |‚ü®œï|œàŒ∏‚ü©|2 =  X
REFERENCES,0.42061855670103093,"x
œï‚àó(x)œàŒ∏(x)  2 (A.1)"
REFERENCES,0.422680412371134,"measures the closeness of the two (normalized) quantum states |œï‚ü©and |œàŒ∏‚ü©, with ‚àódenoting complex
conjugation. By minimizing ‚àílog F, one can obtain the |œàŒ∏‚ü©closest to the target state |œï‚ü©(see
Appendix D for optimization details). For small system sizes (‚â≤20 qubits), it is possible to enumerate
all the basis x and compute ‚àílog F exactly as in the case of this work. As mentioned in the main
paper, we learn the random Bell state and shallow random circuit to test the expressivity and physics
inductive bias of the ansatz. More specifically, the states are generated as follows"
REFERENCES,0.4247422680412371,"Random Bell State. A two-qubit Bell state is defined to be the state (|00‚ü©+ |11‚ü©)/
‚àö"
WE USE THE,0.4268041237113402,"2. We use the
following algorithm to generate a n-qubit random Bell state (n is a multiple of 2)."
WE USE THE,0.4288659793814433,Algorithm 1 Random Bell State Generation
WE USE THE,0.4309278350515464,"a ‚Üêshuffle([1, . . . n"
WE USE THE,0.4329896907216495,"2 ])
b ‚Üêshuffle([ n"
WE USE THE,0.4350515463917526,"2 + 1, . . . n])
Œ® ‚Üê[]
i ‚Üê1
while i ‚â§n/2 do"
WE USE THE,0.43711340206185567,"Œ®.append(bell_state(ai, bi))
i ‚Üêi + 1
end while
|œà‚ü©‚Üêproduct_state(Œ®)
return |œà‚ü©"
WE USE THE,0.43917525773195876,"where bell_state(¬∑, ¬∑) creates a Bell state given the two qubits and product_state([]) creates the
tensor product state for a list of individual states. Each two-qubit Bell state in the random Bell state
forms between the left half and right half of the system, allowing a maximum entanglement across a
cut in the middle of the system."
WE USE THE,0.44123711340206184,"Shallow Random Circuit. The shallow random circuit generates random states using the following
algorithm."
WE USE THE,0.44329896907216493,Algorithm 2 Shallow Random Circuit State Generation
WE USE THE,0.44536082474226807,|œà‚ü©‚Üê|0‚ü©‚äón
WE USE THE,0.44742268041237115,"l ‚Üê1
while l ‚â§nl do"
WE USE THE,0.44948453608247424,"i ‚Üê1
while i < n do"
WE USE THE,0.4515463917525773,"U ‚Üêrandom_gate(i, i + 1)
|œà‚ü©‚ÜêU |œà‚ü©
i ‚Üêi + 2
end while
i ‚Üê2
while i < n do"
WE USE THE,0.4536082474226804,"U ‚Üêrandom_gate(i, i + 1)
|œà‚ü©‚ÜêU |œà‚ü©
i ‚Üêi + 2
end while
end while
return |œà‚ü©"
WE USE THE,0.4556701030927835,"where nl is the number of layers, and random_gate(¬∑, ¬∑) generates a random unitary gate on the
two qubits. The random_gate(¬∑, ¬∑) function is realized by first generating a (real-valued) Gaussian
random matrix of size 4 √ó 4, following a QR decomposition of the matrix to obtain the unitary part
as the random unitary gate."
WE USE THE,0.4577319587628866,"Each of the above algorithms defines a distribution of states, which we average over multiple
realizations to compute the mean and standard deviation of the learned quantum fidelity."
WE USE THE,0.45979381443298967,"However, for large system sizes, the Hilbert space is too large, so one has to evaluate the fidelity
stochastically. This can be achieved by rewriting"
WE USE THE,0.4618556701030928,"f :=
X"
WE USE THE,0.4639175257731959,"x
œï‚àó(x)œàŒ∏(x) =
X"
WE USE THE,0.465979381443299,"x
|œï(x)|2 œàŒ∏(x)"
WE USE THE,0.46804123711340206,œï(x) = Ex‚àº|œï|2 œàŒ∏(x)
WE USE THE,0.47010309278350515,"œï(x) .
(A.2)"
WE USE THE,0.47216494845360824,"Which allows the fidelity to be evaluated by sampling from |œï|2. The gradient of ‚àílog F can be
written as"
WE USE THE,0.4742268041237113,‚àáŒ∏ ‚àílog F = ‚àí2‚Ñú‚àáŒ∏f
WE USE THE,0.4762886597938144,"f
= ‚àí2‚Ñú
Ex‚àº|œï|2 ‚àáŒ∏œàŒ∏(x)"
WE USE THE,0.47835051546391755,"œï(x)
Ex‚àº|œï|2 œàŒ∏(x)"
WE USE THE,0.48041237113402063,"œï(x)
= ‚àí2‚ÑúEx‚àº|œï|2 œàŒ∏(x)"
WE USE THE,0.4824742268041237,"œï(x)
Ex‚Ä≤‚àº|œï|2 œàŒ∏(x‚Ä≤)"
WE USE THE,0.4845360824742268,"œï(x‚Ä≤)
‚àáŒ∏ log œàŒ∏(x)."
WE USE THE,0.4865979381443299,"(A.3)
Alternatively, one can sample from œàŒ∏(x) by writing the complex conjugated"
WE USE THE,0.488659793814433,"f ‚àó=
X"
WE USE THE,0.49072164948453606,"x
|œàŒ∏(x)|2 œï(x)"
WE USE THE,0.4927835051546392,œàŒ∏(x) = Ex‚àº|œàŒ∏|2 œï(x)
WE USE THE,0.4948453608247423,"œàŒ∏(x).
(A.4)"
WE USE THE,0.49690721649484537,"This time, we have to be careful with"
WE USE THE,0.49896907216494846,"‚àáŒ∏f ‚àó=
X"
WE USE THE,0.5010309278350515,"x
œï(x)‚àáŒ∏œà‚àó
Œ∏(x) =
X"
WE USE THE,0.5030927835051546,"x
|œàŒ∏(x)|2 œï(x)"
WE USE THE,0.5051546391752577,"œàŒ∏(x)‚àáŒ∏ log œà‚àó
Œ∏(x) = Ex‚àº|œàŒ∏|2 œï(x)"
WE USE THE,0.5072164948453608,"œàŒ∏(x)‚àáŒ∏ log œà‚àó
Œ∏(x)."
WE USE THE,0.5092783505154639,"(A.5)
Then,"
WE USE THE,0.511340206185567,‚àáŒ∏ ‚àílog F = ‚àí2‚Ñú‚àáŒ∏f ‚àó
WE USE THE,0.51340206185567,"f ‚àó
= ‚àí2‚ÑúEx‚àº|œàŒ∏|2"
WE USE THE,0.5154639175257731,"œï(x)
œàŒ∏(x)
Ex‚Ä≤‚àº|œàŒ∏|2 œï(x‚Ä≤)"
WE USE THE,0.5175257731958763,"œàŒ∏(x‚Ä≤)
‚àáŒ∏ log œà‚àó
Œ∏(x).
(A.6)"
WE USE THE,0.5195876288659794,"We can further use the variance reduction technique (Greensmith et al., 2004) from reinforcement
learning and write"
WE USE THE,0.5216494845360825,‚àáŒ∏ ‚àílog F = ‚àí2‚ÑúEx‚àº|œàŒ∏|2 Ô£Æ Ô£∞
WE USE THE,0.5237113402061856,"œï(x)
œàŒ∏(x)
Ex‚Ä≤‚àº|œàŒ∏|2 œï(x‚Ä≤)"
WE USE THE,0.5257731958762887,"œàŒ∏(x‚Ä≤)
‚àí1 Ô£π"
WE USE THE,0.5278350515463918,"Ô£ª‚àáŒ∏ log œà‚àó
Œ∏(x),
(A.7)"
WE USE THE,0.5298969072164949,"where the minus 1 comes from the mean of the coefficient in front of ‚àáŒ∏ log œà‚àó
Œ∏(x)"
WE USE THE,0.5319587628865979,Ex‚àº|œàŒ∏|2 œï(x)
WE USE THE,0.534020618556701,"œàŒ∏(x)
Ex‚Ä≤‚àº|œàŒ∏|2 œï(x‚Ä≤)"
WE USE THE,0.5360824742268041,"œàŒ∏(x‚Ä≤)
= 1.
(A.8)"
WE USE THE,0.5381443298969072,"One can verify that without the assumption that |œàŒ∏‚ü©is normalized (i.e. F = |‚ü®œï|œàŒ∏‚ü©|2/ ‚ü®œàŒ∏|œàŒ∏‚ü©),
Eq. A.7 can also be obtained without applying the variance reduction formula."
WE USE THE,0.5402061855670103,"In the main paper, due to the small system size N = 16, we enumerated all the bases when evaluating
the quantum fidelity."
WE USE THE,0.5422680412371134,"A.2
Variational Monte Carlo"
WE USE THE,0.5443298969072164,"As shown in the main paper, given a Hamiltonian ÀÜH (and its matrix elements Hx,x‚Ä≤)"
WE USE THE,0.5463917525773195,"‚ü®œàŒ∏| ÀÜH|œàŒ∏‚ü©=
X"
WE USE THE,0.5484536082474227,"x,x‚Ä≤
Hx,x‚Ä≤œà‚àó
Œ∏(x)œàŒ∏(x‚Ä≤) =
X"
WE USE THE,0.5505154639175258,"x,x‚Ä≤
|œàŒ∏(x)|2 Hx,x‚Ä≤œàŒ∏(x‚Ä≤)"
WE USE THE,0.5525773195876289,"œàŒ∏(x)
= Ex‚àº|œàŒ∏|2
P"
WE USE THE,0.554639175257732,"x‚Ä≤ Hx,x‚Ä≤œàŒ∏(x‚Ä≤)"
WE USE THE,0.5567010309278351,"œàŒ∏(x)
."
WE USE THE,0.5587628865979382,"(A.9)
Since the Hamiltonian ÀÜH is usually sparse, give x, one only needs to sum up a small number of
x‚Ä≤‚Äôs for the numerator (usually linear in system size), allowing Eq. A.9 to be evaluated efficiently.
Analogously, we can evaluate the gradient as"
WE USE THE,0.5608247422680412,"‚àáŒ∏ ‚ü®œàŒ∏| ÀÜH|œàŒ∏‚ü©= 2‚Ñú
X"
WE USE THE,0.5628865979381443,"x,x‚Ä≤
Hx,x‚Ä≤ [‚àáŒ∏œà‚àó
Œ∏(x)] œàŒ∏(x‚Ä≤)"
WE USE THE,0.5649484536082474,"= 2‚Ñú
X"
WE USE THE,0.5670103092783505,"x,x‚Ä≤
|œàŒ∏(x)|2Hx,x‚Ä≤ ‚àáŒ∏œà‚àó
Œ∏(x)
œà‚àó
Œ∏(x)
œàŒ∏(x‚Ä≤) œàŒ∏(x)"
WE USE THE,0.5690721649484536,"= 2‚ÑúEx‚àº|œàŒ∏|2
P
x‚Ä≤ Hx,x‚Ä≤œàŒ∏(x‚Ä≤)"
WE USE THE,0.5711340206185567,"œàŒ∏(x)
‚àáŒ∏ log œà‚àó
Œ∏(x)."
WE USE THE,0.5731958762886598,(A.10)
WE USE THE,0.5752577319587628,"Furthermore, it is possible to reduce the variance by either directly applying the variance reduction
formula (Greensmith et al., 2004), or explicitly normalizing |œàŒ∏‚ü©(minimizing ‚ü®œàŒ∏| ÀÜH|œàŒ∏‚ü©/ ‚ü®œàŒ∏|œàŒ∏‚ü©)
to obtain"
WE USE THE,0.5773195876288659,"‚àáŒ∏ ‚ü®œàŒ∏| ÀÜH|œàŒ∏‚ü©= 2‚ÑúEx‚àº|œàŒ∏|2
P"
WE USE THE,0.5793814432989691,"x‚Ä≤ Hx,x‚Ä≤œàŒ∏(x‚Ä≤)"
WE USE THE,0.5814432989690722,"œàŒ∏(x)
‚àí‚ü®œàŒ∏| ÀÜH|œàŒ∏‚ü©

‚àáŒ∏ log œà‚àó
Œ∏(x),
(A.11)"
WE USE THE,0.5835051546391753,"where ‚ü®œàŒ∏| ÀÜH|œàŒ∏‚ü©can be approximated by applying Eq. A.9 on the same batch x. In this work, we
use Eq. A.11 as the gradient of the loss function (see Appendix D for optimization details)."
WE USE THE,0.5855670103092784,"B
Additional Theoretical Results"
WE USE THE,0.5876288659793815,"B.1
Exact Sampling from Conditional Probabilities"
WE USE THE,0.5896907216494846,"Suppose a full probability distribution over multiple qubits is written as a product of conditional
probabilities as"
WE USE THE,0.5917525773195876,"p(x) = p(x1, . . . xn) = n
Y"
WE USE THE,0.5938144329896907,"i=1
p(xi|x<i)
(B.1)"
WE USE THE,0.5958762886597938,"with x<i = (x1, . . . xi‚àí1), then sampling a bitstring x from the probability distribution can be
obtained by sequentially sampling each conditional probability as Since each sample is sampled
independently using this algorithm, a batch of samples can be obtained in parallel, allowing for an
efficient implementation on GPUs. In addition, the exact sampling nature of the algorithm makes it
not suffer from the autocorrelation time problem of standard Markov chain Monte Carlo."
WE USE THE,0.5979381443298969,"This same algorithm applies to sampling from |œà(x)|2 with a simple replacement p(xi|x<i) ‚Üí
|œà(xi|x<i)|2. This is a direct consequence of"
WE USE THE,0.6,"œà(x) = n
Y"
WE USE THE,0.6020618556701031,"i=1
œà(xi|x<i) ‚áí|œà(x)|2 = n
Y"
WE USE THE,0.6041237113402061,"i=1
|œà(xi|x<i)|2,
(B.2)"
WE USE THE,0.6061855670103092,and the fact that we require P
WE USE THE,0.6082474226804123,xi |œà(xi|x<i)|2 = 1.
WE USE THE,0.6103092783505155,Algorithm 3 Exact Sampling from Conditional Probabilities/Wavefunctions
WE USE THE,0.6123711340206186,"x ‚Üê[]
i ‚Üê1
while i ‚â§n do"
WE USE THE,0.6144329896907217,"x<i ‚Üêx
xi ‚àºp(xi|x<i)
x.append(xi)
i ‚Üêi + 1
end while
return x"
WE USE THE,0.6164948453608248,"B.2
Exact Sampling from MPS"
WE USE THE,0.6185567010309279,"As mentioned in the main text, matrix product state (MPS) with particular constraints, also allows for
exact sampling. Recall that MPS defines a wavefunction as"
WE USE THE,0.6206185567010309,"œà(x) =
X"
WE USE THE,0.622680412371134,"Œ±1,...,Œ±n‚àí1
M Œ±0Œ±1
x1
¬∑ ¬∑ ¬∑ M Œ±n‚àí1Œ±n
xn
=
X Œ± n
Y"
WE USE THE,0.6247422680412371,"i=1
M Œ±i‚àí1Œ±i
xi
.
(B.3)"
WE USE THE,0.6268041237113402,"Let‚Äôs first assume that the wavefunction is normalized. It is easy to notice that any gauge transforma-
tion
(Mxi, Mxi+1) ‚Üí(MxiA, A‚àí1Mxi+1),
(B.4)
with A being an invertible matrix leaves the resulting wavefunction invariant. Here, we suppress
the Œ± indices and view them as the indices for matrices. It has been shown that by fixing this gauge
redundancy (Vidal, 2003, 2004), we can restrict all the M matrices to satisfy the following condition
X"
WE USE THE,0.6288659793814433,"xi,Œ±i
M Œ±i‚àí1Œ±i
xi

M
Œ±‚Ä≤
i‚àí1Œ±i
xi
‚àó
= Œ¥Œ±i‚àí1,Œ±‚Ä≤
i‚àí1,
(B.5)"
WE USE THE,0.6309278350515464,"where Œ¥¬∑,¬∑ is the Kronecker delta function. The matrices M that satisfy this condition are called
isometries, and the MPS is called in the right canonical form (to be distinguished from the left canon-
ical form). In the right canonical form, each M Œ±i‚àí1Œ±i
xi
can be interpreted as a basis transformation
(xi, Œ±i) ‚ÜíŒ±i‚àí1 that is part of a unitary matrix."
WE USE THE,0.6329896907216495,Theorem B.1. Defining the left partially contracted tensors
WE USE THE,0.6350515463917525,"ML
Œ±j
x‚â§j :=
X Œ±‚â§j jY"
WE USE THE,0.6371134020618556,"i=1
M Œ±i‚àí1Œ±i
xi
,
(B.6)"
WE USE THE,0.6391752577319587,"If the matrix product state is in the right canonical form, then the marginal probability of the
wavefunction satisfies"
WE USE THE,0.6412371134020619,"q(x‚â§i) :=
X"
WE USE THE,0.643298969072165,"x>i
|œà(x)|2 =
X"
WE USE THE,0.6453608247422681,"Œ±j
ML
Œ±j
x‚â§j

ML
Œ±j
x‚â§j
‚àó
.
(B.7)"
WE USE THE,0.6474226804123712,Proof. Let‚Äôs define the right partially contracted tensors analogously
WE USE THE,0.6494845360824743,"MR
Œ±j‚àí1
x‚â•j :=
X Œ±‚â•j n
Y"
WE USE THE,0.6515463917525773,"i=j
M Œ±i‚àí1Œ±i
xi
.
(B.8)"
WE USE THE,0.6536082474226804,"We will first show that
X"
WE USE THE,0.6556701030927835,"x‚â•j
MR
Œ±j‚àí1
x‚â•j

MR
Œ±‚Ä≤
j‚àí1
x‚â•j
‚àó
= Œ¥Œ±j‚àí1,Œ±‚Ä≤
j‚àí1.
(B.9)"
WE USE THE,0.6577319587628866,We can prove this by induction:
WE USE THE,0.6597938144329897,"‚Ä¢ Base case:
X"
WE USE THE,0.6618556701030928,"xn
MR
Œ±n‚àí1
xn
 
MR
Œ±n‚àí1
xn
‚àó=
X"
WE USE THE,0.6639175257731958,"xn
M Œ±n‚àí1Œ±n
xn

M
Œ±‚Ä≤
n‚àí1Œ±‚Ä≤
n
xn
‚àó
= Œ¥Œ±n‚àí1,Œ±‚Ä≤
n‚àí1,
(B.10)"
WE USE THE,0.6659793814432989,which directly comes from Eq. B.5 by noticing that D(Œ±n) = 1 so it can be ignored.
WE USE THE,0.668041237113402,‚Ä¢ Inductive step: Write
WE USE THE,0.6701030927835051,"MR
Œ±j‚àí1
x‚â•j =
X"
WE USE THE,0.6721649484536083,"Œ±j
M Œ±j‚àí1Œ±j
xj
MR
Œ±j
x‚â•j+1.
(B.11)"
WE USE THE,0.6742268041237114,"Assuming
X"
WE USE THE,0.6762886597938145,"x‚â•j+1
MR
Œ±j
x‚â•j+1

MR
Œ±‚Ä≤
j
x‚â•j+1
‚àó
= Œ¥Œ±j,Œ±‚Ä≤
j,
(B.12)"
WE USE THE,0.6783505154639176,"then
X"
WE USE THE,0.6804123711340206,"x‚â•j
MR
Œ±j‚àí1
x‚â•j

MR
Œ±‚Ä≤
j‚àí1
x‚â•j
‚àó
=
X xj X"
WE USE THE,0.6824742268041237,"Œ±j,Œ±‚Ä≤
j
M Œ±j‚àí1Œ±j
xj

M
Œ±‚Ä≤
j‚àí1Œ±‚Ä≤
j
xj
‚àóX"
WE USE THE,0.6845360824742268,"x‚â•j+1
MR
Œ±j
x‚â•j+1

MR
Œ±‚Ä≤
j
x‚â•j+1
‚àó =
X xj X"
WE USE THE,0.6865979381443299,"Œ±j,Œ±‚Ä≤
j
M Œ±j‚àí1Œ±j
xj

M
Œ±‚Ä≤
j‚àí1Œ±‚Ä≤
j
xj
‚àó
Œ¥Œ±j,Œ±‚Ä≤
j =
X"
WE USE THE,0.688659793814433,"xj,Œ±j
M Œ±j‚àí1Œ±j
xj

M
Œ±‚Ä≤
j‚àí1Œ±j
xj
‚àó"
WE USE THE,0.6907216494845361,"= Œ¥Œ±j‚àí1,Œ±‚Ä≤
j‚àí1.
(B.13)"
WE USE THE,0.6927835051546392,"Using this result, we can then prove our desired result about the marginal probability. Let‚Äôs write"
WE USE THE,0.6948453608247422,"œà(x) =
X Œ± n
Y"
WE USE THE,0.6969072164948453,"i=1
M Œ±i‚àí1Œ±i
xi
=
X"
WE USE THE,0.6989690721649484,"Œ±j
ML
Œ±j
x‚â§jMR
Œ±j
x>j,
(B.14) then,"
WE USE THE,0.7010309278350515,"q(x‚â§i) =
X"
WE USE THE,0.7030927835051546,"x>i
|œà(x)|2 =
X x>j X"
WE USE THE,0.7051546391752578,"Œ±j,Œ±‚Ä≤
j
ML
Œ±j
x‚â§jMR
Œ±j
x>j

ML
Œ±‚Ä≤
j
x‚â§jMR
Œ±‚Ä≤
j
x>j
‚àó =
X"
WE USE THE,0.7072164948453609,"Œ±j,Œ±‚Ä≤
j
ML
Œ±j
x‚â§j

ML
Œ±‚Ä≤
j
x‚â§j
‚àó
Œ¥Œ±j,Œ±‚Ä≤
j =
X"
WE USE THE,0.709278350515464,"Œ±j
ML
Œ±j
x‚â§j

ML
Œ±j
x‚â§j
‚àó
."
WE USE THE,0.711340206185567,(B.15)
WE USE THE,0.7134020618556701,Corollary B.1.1. Matrix product state allows for efficient exact sampling.
WE USE THE,0.7154639175257732,"Proof. Thm. B.1 shows that marginal probability distribution q(x‚â§i) can be evaluated efficiently.
The conditional probability can be evaluated efficiently either as q(xi|x<i) = q(x‚â§i)/q(x<i) or
(equivalently) as an explicit normalization of the marginal probability distribution q(xi|x<i) =
q(x‚â§i)/ P"
WE USE THE,0.7175257731958763,"xi q(x‚â§i). Then, Algorithm 3 can be used to sample from the full distribution."
WE USE THE,0.7195876288659794,"B.3
Autoregressive Sampling Order"
WE USE THE,0.7216494845360825,"Algorithm 3 samples from conditional probabilities, which requires an ordering of the system to be
defined. For 1D systems, we use a linear ordering such that qubit 1 corresponds to the leftmost qubit,
and qubit n corresponds to the rightmost qubit. This ordering is natural for 1D MPS and transformer,
as well as the transformer-based ANTN. For 2D systems, we use a snake (zig-zag) ordering. In this
ordering, the qubit at the 2D location (i, j) is defined to be the i √ó Ly + j th qubit, where Ly is the
number of qubits along the second dimension. This ordering is inherently defined from the masked
convolution for PixelCNN and PixelCNN-based ANTN. The MPS is reshaped to satisfy the same
ordering for 2D systems. The same 2D ordering can be generalized to other tensor networks as well."
WE USE THE,0.7237113402061855,"B.4
Additional Details of Expressivity and Symmetry Results of ANTN"
WE USE THE,0.7257731958762886,"Theorem B.2. Both tensor networks and autoregressive neural networks are special cases of Autore-
gressive Neural TensorNet."
WE USE THE,0.7278350515463917,Proof. Recall that the ANTN defines the wavefunction from its amplitude and phase
WE USE THE,0.7298969072164948,"œà(x) :=
p"
WE USE THE,0.7319587628865979,"q(x)eiœï(x),
(B.16) where"
WE USE THE,0.734020618556701,"q(x) = n
Y"
WE USE THE,0.7360824742268042,"j=1
q(xj|x<j)
(B.17)"
WE USE THE,0.7381443298969073,"with q(xj|x<j) =
q(x‚â§j)
P"
WE USE THE,0.7402061855670103,"xj q(x‚â§j)
(B.18)"
WE USE THE,0.7422680412371134,"q(x‚â§j) :=
X Œ±j"
WE USE THE,0.7443298969072165,"ÀúœàŒ±j
L (x‚â§j) ÀúœàŒ±j
L (x‚â§j)‚àó
(B.19)"
WE USE THE,0.7463917525773196,"ÀúœàŒ±j
L (x‚â§j) :=
X Œ±<j jY"
WE USE THE,0.7484536082474227,"i=1
ÀúœàŒ±i‚àí1Œ±i(xi|x<i)
(B.20) and"
WE USE THE,0.7505154639175258,"œï(x) =: Arg
X Œ± n
Y"
WE USE THE,0.7525773195876289,"i=1
ÀúœàŒ±i‚àí1,Œ±i(xi|x<i).
(B.21)"
WE USE THE,0.7546391752577319,"‚Ä¢ ANTN reduces to MPS. If we don‚Äôt modify the tensors with neural networks,
ÀúœàŒ±i‚àí1,Œ±i(xi|x<i) reduces to M Œ±i‚àí1,Œ±i
xi
and ÀúœàŒ±j
L (x‚â§j) reduces to ML
Œ±j
x‚â§j. It is then straight-
forward that Eq. B.19 reduces to Eq. B.15 and thus the probability distribution of ANTN
reduces to that of MPS. Analogously, Eq. B.21 reduces to the phase of an MPS, and thus
ANTN wavefunction reduces to an MPS wavefunction.
‚Ä¢ ANTN reduces to ARNN. If we set all D(Œ±) = 1, then ÀúœàŒ±i‚àí1Œ±i(xi|x<i) reduces to
œà(xi|x<i) and all the summations over Œ± can be dropped. In this case, q(x‚â§j) reduces
to Qj
i=1 |œà(xi|x<i)|2 and q(xj|x<j) reduces to |œà(xj|x<j)|2, which is the same as the
standard ARNN. In addition, the phase œï(x) reduces to Arg Qn
i=1 œà(xi|x<i) which is also
the same as ARNN. Thus, ANTN wavefunction reduces to ARNN wavefunction."
WE USE THE,0.756701030927835,"Notice that we only showed one type of TN, i.e. MPS, but the proof can be easily generalized to any
TN that allows evaluation of marginal probabilities.
Theorem B.3. Autoregressive Neural TensorNet can be written as either a tensor network or an
autoregressive neural network with an exponential (in system size) number of parameters."
WE USE THE,0.7587628865979381,Proof.
WE USE THE,0.7608247422680412,"‚Ä¢ ANTN as TN. In ANTN, the base ARNN outputs the conditional tensors ÀúœàŒ±i‚àí1Œ±i(xi|x<i)
to form the final wavefunctions. These conditional tensors can be written as ÀúœàŒ±i‚àí1Œ±i
x1,x2...,xi in
tensor notation. It directly follows from the shape of these tensors that they contain œá2 ¬∑ 2i
elements (where œá2 is from the two bond dimensions and 2i is from all the qubits at or before
the current qubit i). According to Theorem 5.2 ARNN has volume law entanglement while
TN doesn‚Äôt. The conditional tensors generated from ARNN do not permit efficient tensor
decomposition. Therefore, a tensor network almost has to parameterize all full conditional
tensors, resulting in PN
i=1 œá2 ¬∑ 2i ‚àºO(œá2 ¬∑ 2N) parameters in total.
‚Ä¢ ANTN as ARNN. In ANTN, the conditional probability is calculated from the marginal
probability as shown in Eq. 4 as"
WE USE THE,0.7628865979381443,"q(x‚â§i) =
X Œ±i"
WE USE THE,0.7649484536082474,"ÀúœàŒ±i
L (x‚â§i) ÀúœàŒ±i
L (x‚â§i)‚àó =
X"
WE USE THE,0.7670103092783506,"Œ±1,Œ±‚Ä≤
1,...,Œ±i‚àí1,Œ±‚Ä≤
i‚àí1Œ±i"
WE USE THE,0.7690721649484537,"ÀúœàŒ±1(x1) ÀúœàŒ±‚Ä≤
1(x1)‚àó¬∑ ¬∑ ¬∑ ÀúœàŒ±i‚àí1Œ±i(xi|x<i) ÀúœàŒ±‚Ä≤
i‚àí1Œ±i(xi|x<i)‚àó."
WE USE THE,0.7711340206185567,"(B.22)
The summation over Œ±i‚Äôs contains œá2i‚àí1 terms, each of which can be viewed as a marginal
(quasi-)probability qŒ±1,Œ±‚Ä≤
1,...,Œ±i(x‚â§i) generated by the underlying ARNN of ANTN. This"
WE USE THE,0.7731958762886598,"effectively creates a weight matrix on top of a conventional ARNN, whose shape is hdim √ó
œá2i‚àí1 √ó 2 (where hdim is the hidden dimension, and 2 is the local dimension of each qubit)
on top of a conventional ARNN, resulting in PN
i=1 2 ¬∑ hdim ¬∑ œá2i‚àí1 ‚àºO(hdim ¬∑ œá2N)
parameters."
WE USE THE,0.7752577319587629,"Definition B.1 (Mask Symmetry). A conditional wavefunction œà(xi|x<i) has a mask symmetry if
œà(xi|x<i) = 0 for some xi given x<i."
WE USE THE,0.777319587628866,"Theorem B.4. Autoregressive Neural TensorNet inherits mask symmetry from autoregressive neural
network."
WE USE THE,0.7793814432989691,"Proof. The mask symmetry results in œà(x) = 0 for certain x satisfying the condition while preserving
the normalization of œà(x). For ANTN, we can directly set q(xi|x<i) = 0 for the xi given x<i. This
results in œà(x) = 0 for the same x."
WE USE THE,0.7814432989690722,"Corollary B.4.1 (Global U(1) Symmetry). Autoregressive Neural TensorNet can realize global U(1)
symmetry, which conserves particle number."
WE USE THE,0.7835051546391752,"Proof. A global U(1) symmetry in a qubit system manifests as a conservation of the number of 0‚Äôs
and number of 1‚Äôs in x. Equivalently, œà(x) = 0 for x violates such conservation. (Hibat-Allah et al.,
2020) has shown that autoregressive neural networks can preserve global U(1) symmetry as a mask
symmetry, which ANTN inherits."
WE USE THE,0.7855670103092783,"Definition B.2 (Function Symmetry). A conditional wavefunction tensor œà(xi|x<i) has a func-
tion symmetry over a function F if œà(xi|x<i) = œà(F(xi; x<i)|F (x<i)) where F (x‚â§i) :=
{F(x1), F(x2; x1), . . . , F(xi; x<i)}."
WE USE THE,0.7876288659793814,"Theorem B.5. Autoregressive Neural TensorNet inherits function symmetry from autoregressive
neural networks."
WE USE THE,0.7896907216494845,"Proof. We can apply the function symmetry on the left partially contracted tensors instead of the
conditional wavefunctions. Here, we show that this produces the desired conditional probabilities
and phase. Applying the function symmetry on the left partially contracted tensors results in"
WE USE THE,0.7917525773195876,"ÀúœàŒ±j
L (x‚â§j) = ÀúœàŒ±j
L (F(x‚â§j)).
(B.23)"
WE USE THE,0.7938144329896907,"This implies that the marginal probability satisfies q(x‚â§j) = q(F(x‚â§j)) after contracting over the
index Œ±j, which then results in the correct symmetry on the conditional probabilities"
WE USE THE,0.7958762886597938,"q(xj|x<j) = q(F(xj)|F(x<j)),
(B.24)"
WE USE THE,0.797938144329897,"as the conditional probability is just the marginal probability normalized at site j. The phase, on the
other hand,"
WE USE THE,0.8,"œï(x) = Arg
X Œ± n
Y"
WE USE THE,0.8020618556701031,"i=1
ÀúœàŒ±i‚àí1,Œ±i(xi|x<i) = Arg ÀúœàŒ±n
L (x‚â§n),
(B.25)"
WE USE THE,0.8041237113402062,which satisfies the same function symmetry from Eq. B.23.
WE USE THE,0.8061855670103093,"Corollary B.5.1 (Z2 Spin Flip Symmetry). Autoregressive Neural TensorNet can realize Z2 spin flip
symmetry such that the ANTN wavefunction is invariant under conjugation of the input."
WE USE THE,0.8082474226804124,"Proof. Spin flip symmetry exists for quantum chemistry systems that do not couple spin up and spin
down electron, so that the wavefunction is invariant to inputs when spin up and down are flipped.
(Barrett et al., 2022) has shown that autoregressive neural network can preserve Z2 spin flip symmetry
as a function symmetry, which ANTN inherits."
WE USE THE,0.8103092783505155,"Corollary B.5.2 (Discrete Abelian and Non-Abelian Symmetries). Autoregressive Neural TensorNet
can realize discrete Abelian and Non-Abelian symmetries."
WE USE THE,0.8123711340206186,"Proof. Gauge symmetry is a local symmetry such that the function is invariant under local transfor-
mation. (Luo et al., 2021b) has shown that autoregressive neural networks can preserve discrete
Abelian and non-Abelian symmetries as either the mask symmetry or the function symmetry, which
ANTN inherits."
WE USE THE,0.8144329896907216,"C
Additional Experiments"
WE USE THE,0.8164948453608247,"In this section, we perform additional experiments to further compare ANTN and ARNN, and show
that"
WE USE THE,0.8185567010309278,‚Ä¢ It is challenging to surpass ANTN by ARNN even with increased number of parameters.
WE USE THE,0.8206185567010309,"‚Ä¢ The improvement from ANTN mainly comes from the effective inductive bias of MPS
structure instead of the DMRG initialization."
WE USE THE,0.822680412371134,‚Ä¢ ANTN has a favorable scaling compared to ARNN and MPS for reaching accurate results.
WE USE THE,0.8247422680412371,"Energy per site ‚Üìand additional information for 10 √ó 10 system
Algorithms
J2 = 0.2
J2 = 0.5
J2 = 0.8
Nparams,TN Nparams,NN Nparams
Ttraining
MPS (8)
-1.997537
-1.734326
-1.869871
1.2 √ó 104
0
1.2 √ó 104 < 1 hr
MPS (70)
-2.191048
-1.866159
-2.007845
8.8 √ó 105
0
8.8 √ó 105 < 1 hr
MPS (1024)
-2.255633
-1.938770
-2.062730
1.7 √ó 108
0
1.7 √ó 108 242 hrs
PixelCNN (7-48)
-2.22462(24) -1.74098(29) -1.98331(17) 0
5.0 √ó 105
5.0 √ó 105 43 hrs
PixelCNN (P) (7-48) -2.1920(9)
-1.6458(10)
-1.9468(6)
0
5.0 √ó 105
5.0 √ó 105 19 hrs‚Ä†"
WE USE THE,0.8268041237113402,"PixelCNN (8-48)
-2.23581(14) -1.86922(16) -2.01093(16) 0
5.7 √ó 105
5.7 √ó 105 59 hrs
EW (7-48-8)
-2.26034(6) -1.94001(6)
-2.05707(43) 1.2 √ó 106
5.0 √ó 105
1.7 √ó 106 89 hrs
EW (RI) (7-48-8)
-2.25946(6)
-1.94030(9) -2.05125(11) 1.2 √ó 106
5.0 √ó 105
1.7 √ó 106 46 hrs‚Ä†"
WE USE THE,0.8288659793814434,"BW (7-48-70)
-2.25755(8)
-1.93842(42) -2.05088(14) 8.9 √ó 105
5.0 √ó 105
1.4 √ó 106 136 hrs"
WE USE THE,0.8309278350515464,"Table 3: Energy per site ‚Üì, tensor network parameters Nparams,TN, neural network parameters
Nparams,NN, total parameters Nparams and runtime Ttraining for 10 √ó 10 system with various algo-
rithms. Elementwise (EW) and blockwise (BW) are two constructions of ANTN with PixelCNN
as the underlying ARNN. For MPS, the number in the parenthesis labels the bond dimension. For
PixelCNN, the two numbers label the number of layers and hidden dimension respectively. For
ANTN, the numbers label (in this order) the number of layers, hidden dimension, and bond dimension.
For PixelCNN, we also test pertaining (P) using MPS result (with bond dimension the last number
in parenthesis), and for EW, we in addition test random initialization (RI) of the MPS part. The
algorithms in boldface are new experiments not previously shown in the main paper. Best energies for
each J2 are also highlighted in boldface. The MPS with DMRG algorithm is run on CPUs, while the
PixelCNN, elementwise (EW) and blockwise (BW) ANTN are trained using GPUs, with PixelCNN
(P) (7-48) and EW (RI) (7-48-8) using A100 GPUs (labeled with ‚Ä†), and the rest using V100 GPUs."
WE USE THE,0.8329896907216495,"104
105
106
107
108"
WE USE THE,0.8350515463917526,Number of parameters 1.95 1.90 1.85 1.80 1.75
WE USE THE,0.8371134020618557,Energy per site
WE USE THE,0.8391752577319588,"DMRG
PixelCNN
EW
BW
10 √ó 10
12 √ó 12"
WE USE THE,0.8412371134020619,"0
50
100
150
200
250
300
Runtime [hrs]"
WE USE THE,0.843298969072165,"DMRG
PixelCNN
EW
BW
10 √ó 10
12 √ó 12"
WE USE THE,0.845360824742268,"Figure 4: Energy per site vs total number of parameters and runtime (in hours) for various algorithms
(MPS with DMRG algorithm, PixelCNN, elementwise (EW), and blockwise (BW) ANTNs) and
system sizes (10 √ó 10 and 12 √ó 12) for J2 = 0.5. The ANTN construction uses PixelCNN as the
underlying ARNN. The total number of parameters includes parameters from both the TN part and
the ARNN part."
WE USE THE,0.8474226804123711,"C.1
Comparison between ANTN and ARNN with More Layers"
WE USE THE,0.8494845360824742,In Table 3 and 4 we include additional results.
WE USE THE,0.8515463917525773,"Energy per site ‚Üìand additional information for 10 √ó 10 system at J2 = 0.5
Algorithms
J2 = 0.5
Nparams,TN Nparams,NN Nparams
Ttraining
MPS (8)
-1.734326
1.2 √ó 104
0
1.2 √ó 104 < 1 hr
MPS (70)
-1.866159
8.8 √ó 105
0
8.8 √ó 105 < 1 hr
MPS (1024)
-1.938770
1.7 √ó 108
0
1.7 √ó 108 242 hrs
PixelCNN (7-48)
-1.74098(29) 0
5.0 √ó 105
5.0 √ó 105 19 hrs (A100) or 43 hrs (V100)
PixelCNN (8-48)
-1.86922(16) 0
5.7 √ó 105
5.7 √ó 105 59 hrs (V100)
PixelCNN (9-48)
-1.90021(15) 0
6.3 √ó 105
6.3 √ó 105 21 hrs (A100)
PixelCNN (10-48)
-1.90440(14) 0
7.0 √ó 105
7.0 √ó 105 23 hrs (A100)
PixelCNN (11-48)
-1.92826(11) 0
7.7 √ó 105
7.7 √ó 105 26 hrs (A100)
PixelCNN (12-48)
-1.92986(10) 0
8.4 √ó 105
8.4 √ó 105 30 hrs (A100)
PixelCNN (15-48)
-1.92869(11) 0
1.0 √ó 106
1.0 √ó 106 51 hrs (A100)
PixelCNN (20-48)
-1.91537(13) 0
1.4 √ó 106
1.4 √ó 106 66 hrs (A100)
PixelCNN (8-72)
-1.88536(19) 0
1.3 √ó 106
1.3 √ó 106 35 hrs (A100)
PixelCNN (9-72)
-1.89266(15) 0
1.4 √ó 106
1.4 √ó 106 40 hrs (A100)
PixelCNN (10-72)
-1.90718(15) 0
1.6 √ó 106
1.6 √ó 106 47 hrs (A100)
PixelCNN (11-72)
-1.92965(13) 0
1.7 √ó 106
1.7 √ó 106 52 hrs (A100)
PixelCNN (12-72)
-1.91186(14) 0
1.9 √ó 106
1.9 √ó 106 57 hrs (A100)
PixelCNN (S) (11-48) -1.92915(12) 0
7.7 √ó 105
7.7 √ó 105 31 hrs (A100)
PixelCNN (S) (12-48) -1.93112(11) 0
8.4 √ó 105
8.4 √ó 105 34 hrs (A100)
PixelCNN (S) (15-48) -1.93233(9)
0
1.0 √ó 106
1.0 √ó 106 47 hrs (A100)
PixelCNN (S) (20-48) -1.93058(10) 0
1.4 √ó 106
1.4 √ó 106 64 hrs (A100)
PixelCNN (S) (11-72) -1.93211(10) 0
1.7 √ó 106
1.7 √ó 106 54 hrs (A100)
PixelCNN (S) (12-72) -1.92812(12) 0
1.9 √ó 106
1.9 √ó 106 59 hrs (A100)
EW (7-48-8)
-1.94001(6) 1.2 √ó 106
5.0 √ó 105
1.7 √ó 106 46 hrs (A100) or 89 hrs (V100)
BW (7-48-70)
-1.93842(42) 8.9 √ó 105
5.0 √ó 105
1.4 √ó 106 136 hrs (V100)"
WE USE THE,0.8536082474226804,"Table 4: Energy per site ‚Üì, tensor network parameters Nparams,TN, neural network parameters
Nparams,NN, total parameters Nparams and runtime Ttraining for 10 √ó 10 system at J2 = 0.5 with
various algorithms. Elementwise (EW) and blockwise (BW) are two constructions of ANTN with
PixelCNN as the underlying ARNN. For MPS, the number in the parenthesis labels the bond
dimension. For PixelCNN, the two numbers label the number of layers and hidden dimension
respectively. For ANTN, the numbers label (in this order) the number of layers, hidden dimension,
and bond dimension. For PixelCNN, we also test the sign rule (S) The algorithms in boldface are new
experiments not previously shown in the main paper. Best energies for each J2 are also highlighted
in boldface. The MPS with DMRG algorithm is run on CPUs, while the PixelCNN, elementwise
(EW) and blockwise (BW) ANTN are trained using GPUs."
WE USE THE,0.8556701030927835,"Additional information for 12 √ó 12 system
Algorithms
Nparams,TN Nparams,NN Nparams
Ttraining
MPS (8)
1.8 √ó 104
0
1.8 √ó 104 < 1 hr
MPS (70)
1.3 √ó 106
0
1.3 √ó 106 < 1 hr
MPS (1024)
2.6 √ó 108
0
2.6 √ó 108 329 hrs
PixelCNN (7-48) 0
5.0 √ó 105
5.0 √ó 105 82 hrs
EW (7-48-8)
1.7 √ó 106
5.0 √ó 105
2.2 √ó 106 183 hrs
BW (7-48-70)
1.3 √ó 106
5.0 √ó 105
1.8 √ó 106 287 hrs"
WE USE THE,0.8577319587628865,"Table 5: Additional information for the number of parameters and runtime of the 12 √ó 12 experiment
in the paper for various algorithms. The runtime is average over different J2‚Äôs. The MPS with DMRG
algorithm is run on CPUs, while the PixelCNN, elementwise (EW), and blockwise (BW) ANTN
are trained using V100 GPUs. The ANTN uses PixelCNN as the underlying ARNN. For MPS, the
number in the parenthesis labels the bond dimension. For PixelCNN, the two numbers label the
number of layers and hidden dimension respectively. For ANTN, the numbers label (in this order) the
number of layers, hidden dimension, and bond dimension."
WE USE THE,0.8597938144329897,"In Table 3, we perform 2 addition tests: a) pretrain pure PixelCNN by learning MPS results; b) train
ANTN (elementwise) without DMRG initialization. We find that DMRG pretraining negatively affects
the result, while ANTN without DMRG initialization performs as good as with DMRG initialization.
This is understandable, as learning MPS results with complex sign structure is essentially the same
task as learning a quantum state from shallow random circuit, which has been shown a difficult task
for ARNN in Figure 2(b) of the original paper. Since the MPS result is an approximation of the
actual ground state, learning this state actually makes a worse initialization for PixelCNN. (We note
that the PixelCNN was originally initialized from the result of smaller system sizes, as we described
in Appendix D under transfer learning.) On the other hand, the ANTN, despite not using DMRG
initialization, still retains the ability to represent flexible sign structures. In addition, since DMRG is
just an optimization algorithm to train MPS, the gradient-descent-based algorithm should still train
the MPS part of ANTN reasonably well without DMRG initialization."
WE USE THE,0.8618556701030928,"In Table 4, we test a series of ARNNs with different layers (up to 20 layers) and hidden dimensions
(up to 72 hidden dimensions) such that the largest ARNN tested has more parameters than ANTN. We
found that all the ARNN energies are not as good at ANTN (which is only based on 7 layers and 48
hidden dimensions), despite that ARNNs have more parameters and take a longer time to train using
the same GPU (see new data below). Furthermore, we observe that an increase of parameters can help
ARNN to improve energy in general, but this improvement hits a diminishing return potentially due
to difficulties in optimization as the number of parameters increases. In addition, we further tested
the (approximate) sign rule and found that it could help ARNN obtain better energies but still worse
than ANTN without the sign rule. This indicates that the ANTN has the advantage of inheriting the
flexible sign structure from MPS, avoiding the manual bias of adding the sign rule. The new results
have provided strong support for our expectation, both from an expressivity perspective and from
the fact that ANTN has a better physics inductive bias than ARNN for optimization. We have also
added a plot for the new benchmark in the updated manuscript. Although the added layer improves
the result to some extent, the PixelCNN fails to beat ANTN in terms of energy calculation. This is
consistent with Theorem 5.3 that ANTN is more expressive than ARNN."
WE USE THE,0.8639175257731959,"C.2
Comparison of Runtime and Number of Parameters"
WE USE THE,0.865979381443299,"In Table 3, Table 4, Table 5 and Figure 4, we show the number of parameters and runtime of each
algorithm. In summary, MPS has the most parameters, and is the slowest with DMRG algorithm,
taking as long as 2 weeks. The ANTN is much more efficient compared to MPS while obtaining
better energies, and it is comparable to pure ARNN while obtaining much better energies. Within the
two types of ANTN, the elementwise construction, despite using more parameters, runs faster than
the blockwise construction. We note that since TNs and ARNNs use the parameters very differently,
a direct comparison of the two types of parameters may be unfair. Therefore, we list both the total
number of parameters and the parameters for the individual parts (TN part and ARNN part)."
WE USE THE,0.8680412371134021,"D
Experimental Details and Reproducibility"
WE USE THE,0.8701030927835052,"We note that while the following details can be used to reproduce our results, additional adjustments
may be possible to further improve our results. The code associated with this paper is available at
https://github.com/antn2023/ANTN"
WE USE THE,0.8721649484536083,"Implementation. The transformer (Vaswani et al., 2017) used in this work is a decoder-only
transformer implemented in (Luo et al., 2020), and the PixelCNN used is the gated PixelCNN
(Van den Oord et al., 2016) implemented in (Chen et al., 2022) but without the channelwise mask."
WE USE THE,0.8742268041237113,"Hyperparameters. For the transformer in quantum state learning, we use 2 layers, 32 hidden
dimensions, and 16 attention heads, whereas for the PixelCNN in variational Monte Carlo, we use 7
layers with dilations 1, 2, 1, 4, 1, 2, and 1 for each layer and 48 hidden dimensions. For additional
experiments with more layers, we always use dilation of 1 for the layer 8 and beyond."
WE USE THE,0.8762886597938144,"Initialization. Throughout the experiment, we initialize the weight matrices of the last fully connected
layer of the ARNN and ANTN to 0, and initialize the bias according to the following rule:"
WE USE THE,0.8783505154639175,‚Ä¢ The bias of ARNN is always randomly initialized.
WE USE THE,0.8804123711340206,"‚Ä¢ For the random Bell state learning, the bias of ANTN is randomly initialized"
WE USE THE,0.8824742268041237,"‚Ä¢ For the rest of the experiment, the bias of ANTN is set to 0."
WE USE THE,0.8845360824742268,"The rest of the parameters of ARNN and ANTN are randomly initialized according to PyTorch
(Paszke et al., 2019)‚Äôs default initialization scheme."
WE USE THE,0.8865979381443299,"In addition, for the random Bell state learning experiment, we do not initialize the ANTN with
MPS (and hence the random bias), whereas for the shallow random circuit learning experiment, we
initialize the ANTN with an MPS that comes from explicit truncation of the full wavefunction, and
for the variational Monte Carlo experiment, we initialize the ANTN with DMRG optimized MPS."
WE USE THE,0.8886597938144329,"Optimization. Through the experiment, we use the Adam optimizer with an initial learning rate of
0.01. For quantum state learning experiments, we train the ARNN and ANTN with full basis for 2000
iterations, where the learning rate is halved at iterations 600, 1200, and 1800. In case the full basis
cannot be computed in one pass due to GPU memory constraint, we divide the full basis into several
batches and accumulates the gradient in each iteration. For variational Monte Carlo experiments, we
train the ARNN and ANTN stochastically until the energy converges. During the training, we halve
the learning rate at iterations 100, 500, 1000, 1800, 2500, and 4000. In each experiment, we choose
the maximum batch size that can be fed into the GPU memory, while using accumulation steps to
keep the effective batch size around 10000 throughout the training."
WE USE THE,0.8907216494845361,"Transfer Learning. We in addition take advantage of transfer learning. For J2 = 0.2, 0.5, and
0.8, we begin from scratch with the 4 √ó 4 system. Then, an N √ó N system is initialized from an
(N ‚àí2) √ó (N ‚àí2) system by keeping all but the weights in the last layer. Then, for other J2, the
neural network is initialized from one of J2 = 0.2, 0.5, and 0.8 that is closest to the current J2 of the
same system size by keeping all but the weights in the last layer. We find this allows the optimization
to converge faster than always starting from scratch."
WE USE THE,0.8927835051546392,"Computational Resources. The models are mainly trained using NVIDIA V100 GPUs with 32 GB
memory and Intel Xeon Gold 6248 CPUs, with some models trained using NVIDIA A100 GPUs
with 80 GB memory."
REFERENCES,0.8948453608247423,References
REFERENCES,0.8969072164948454,"Barrett, T. D., Malyshev, A., and Lvovsky, A. Autoregressive neural-network wavefunctions for ab
initio quantum chemistry. Nature Machine Intelligence, 4(4):351‚Äì358, 2022."
REFERENCES,0.8989690721649485,"Capriotti, L., Fubini, A., Roscilde, T., and Tognetti, V. Ising transition in the two-dimensional
quantum j 1- j 2 heisenberg model. Physical review letters, 92(15):157202, 2004."
REFERENCES,0.9010309278350516,"Carleo, G. and Troyer, M. Solving the quantum many-body problem with artificial neural networks.
Science, 355(6325):602‚Äì606, 2017. doi: 10.1126/science.aag2302. URL https://www.science.
org/doi/abs/10.1126/science.aag2302."
REFERENCES,0.9030927835051547,"Chen, Z., Luo, D., Hu, K., and Clark, B. K. Simulating 2+ 1d lattice quantum electrodynamics at
finite density with neural flow wavefunctions. arXiv preprint arXiv:2212.06835, 2022."
REFERENCES,0.9051546391752577,"Choo, K., Neupert, T., and Carleo, G. Two-dimensional frustrated J1‚àíJ2 model studied with neural
network quantum states. Phys. Rev. B, 100:125124, Sep 2019. doi: 10.1103/PhysRevB.100.125124.
URL https://link.aps.org/doi/10.1103/PhysRevB.100.125124."
REFERENCES,0.9072164948453608,"Deng, D.-L., Li, X., and Das Sarma, S. Quantum entanglement in neural network states. Physical
Review X, 7(2), May 2017. ISSN 2160-3308. doi: 10.1103/physrevx.7.021021. URL http:
//dx.doi.org/10.1103/PhysRevX.7.021021."
REFERENCES,0.9092783505154639,"Ferris, A. J. and Vidal, G. Perfect sampling with unitary tensor networks. Phys. Rev. B, 85:165146,
Apr 2012. doi: 10.1103/PhysRevB.85.165146. URL https://link.aps.org/doi/10.1103/
PhysRevB.85.165146."
REFERENCES,0.911340206185567,"Gao, X. and Duan, L.-M. Efficient representation of quantum many-body states with deep neural
networks.
Nature Communications, 8(1):662, Sep 2017.
ISSN 2041-1723.
doi: 10.1038/
s41467-017-00705-2. URL https://www.nature.com/articles/s41467-017-00705-2."
REFERENCES,0.9134020618556701,"Girvin, S. M. and Yang, K. Modern condensed matter physics. Cambridge University Press, 2019."
REFERENCES,0.9154639175257732,"Greensmith, E., Bartlett, P. L., and Baxter, J. Variance reduction techniques for gradient estimates in
reinforcement learning. J. Mach. Learn. Res., 5:1471‚Äì1530, December 2004. ISSN 1532-4435."
REFERENCES,0.9175257731958762,"Guti√©rrez, I. L. and Mendl, C. B. Real time evolution with neural-network quantum states, 2020."
REFERENCES,0.9195876288659793,"Hartmann, M. J. and Carleo, G. Neural-network approach to dissipative quantum many-body
dynamics. Phys. Rev. Lett., 122:250502, Jun 2019. doi: 10.1103/PhysRevLett.122.250502. URL
https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.122.250502."
REFERENCES,0.9216494845360824,"Hermann, J., Sch√§tzle, Z., and No√©, F. Deep neural network solution of the electronic schr√∂dinger
equation, 2019."
REFERENCES,0.9237113402061856,"Hibat-Allah, M., Ganahl, M., Hayward, L. E., Melko, R. G., and Carrasquilla, J.
Recurrent
neural network wave functions.
Phys. Rev. Research, 2:023358, Jun 2020.
doi: 10.1103/
PhysRevResearch.2.023358. URL https://journals.aps.org/prresearch/abstract/10.
1103/PhysRevResearch.2.023358."
REFERENCES,0.9257731958762887,"Huang, Y. and Moore, J. E. Neural network representation of tensor network and chiral states.
Phys. Rev. Lett., 127:170601, Oct 2021. doi: 10.1103/PhysRevLett.127.170601. URL https:
//link.aps.org/doi/10.1103/PhysRevLett.127.170601."
REFERENCES,0.9278350515463918,"Lami, G., Carleo, G., and Collura, M. Matrix product states with backflow correlations. Physical
Review B, 106(8), aug 2022. doi: 10.1103/physrevb.106.l081111. URL https://doi.org/10.
1103%2Fphysrevb.106.l081111."
REFERENCES,0.9298969072164949,"Lante, V. and Parola, A. The ising phase in the j1-j2 heisenberg model. Physical Review, 2006."
REFERENCES,0.931958762886598,"Levine, Y., Sharir, O., Cohen, N., and Shashua, A. Quantum entanglement in deep learning architec-
tures. Physical Review Letters, 122(6), Feb 2019a. ISSN 1079-7114. doi: 10.1103/physrevlett.122.
065301.
URL https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.122.
065301."
REFERENCES,0.934020618556701,"Levine, Y., Sharir, O., Cohen, N., and Shashua, A. Quantum entanglement in deep learning architec-
tures. Physical review letters, 122(6):065301, 2019b."
REFERENCES,0.9360824742268041,"Lu, S., Gao, X., and Duan, L.-M. Efficient representation of topologically ordered states with restricted
boltzmann machines. Phys. Rev. B, 99:155136, Apr 2019. doi: 10.1103/PhysRevB.99.155136.
URL https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.155136."
REFERENCES,0.9381443298969072,"Luo, D. and Clark, B. K.
Backflow transformations via neural networks for quantum many-
body wave functions. Physical Review Letters, 122(22), Jun 2019. ISSN 1079-7114. doi: 10.
1103/physrevlett.122.226401. URL https://journals.aps.org/prl/abstract/10.1103/
PhysRevLett.122.226401."
REFERENCES,0.9402061855670103,"Luo, D., Chen, Z., Carrasquilla, J., and Clark, B. K. Autoregressive neural network for simulating
open quantum systems via a probabilistic formulation, 2020."
REFERENCES,0.9422680412371134,"Luo, D., Carleo, G., Clark, B. K., and Stokes, J. Gauge equivariant neural networks for quantum
lattice gauge theories. Physical review letters, 127(27):276402, 2021a."
REFERENCES,0.9443298969072165,"Luo, D., Chen, Z., Hu, K., Zhao, Z., Hur, V. M., and Clark, B. K. Gauge invariant autoregressive neural
networks for quantum lattice models, 2021b. URL https://arxiv.org/abs/2101.07243."
REFERENCES,0.9463917525773196,"Luo, D., Chen, Z., Carrasquilla, J., and Clark, B. K. Autoregressive neural network for simulating open
quantum systems via a probabilistic formulation. Phys. Rev. Lett., 128:090501, Feb 2022a. doi: 10.
1103/PhysRevLett.128.090501. URL https://link.aps.org/doi/10.1103/PhysRevLett.
128.090501."
REFERENCES,0.9484536082474226,"Luo, D., Yuan, S., Stokes, J., and Clark, B. K. Gauge equivariant neural networks for 2+ 1d u (1)
gauge theory simulations in hamiltonian formulation. arXiv preprint arXiv:2211.03198, 2022b."
REFERENCES,0.9505154639175257,"Marshall, W.
Antiferromagnetism.
Proceedings of the Royal Society of London. Series A,
Mathematical and Physical Sciences, 232(1188):48‚Äì68, 1955. ISSN 00804630. URL http:
//www.jstor.org/stable/99682."
REFERENCES,0.9525773195876288,"Nagy, A. and Savona, V. Variational quantum monte carlo method with a neural-network ansatz for
open quantum systems. Phys. Rev. Lett., 122:250501, Jun 2019. doi: 10.1103/PhysRevLett.122.
250501.
URL https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.122.
250501."
REFERENCES,0.954639175257732,"Nomura, Y. and Imada, M. Dirac-type nodal spin liquid revealed by refined quantum many-body
solver using neural-network wave function, correlation ratio, and level spectroscopy. Physical
Review X, 11(3), aug 2021. doi: 10.1103/physrevx.11.031034. URL https://doi.org/10.
1103%2Fphysrevx.11.031034."
REFERENCES,0.9567010309278351,"Oseledets, I. V. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):2295‚Äì2317,
2011. doi: 10.1137/090752286. URL https://doi.org/10.1137/090752286."
REFERENCES,0.9587628865979382,"Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In
Advances in neural information processing systems, pp. 8026‚Äì8037, 2019."
REFERENCES,0.9608247422680413,"Pfau, D., Spencer, J. S., Matthews, A. G. D. G., and Foulkes, W. M. C. Ab initio solution of the many-
electron schr√∂dinger equation with deep neural networks. Phys. Rev. Research, 2:033429, Sep 2020.
doi: 10.1103/PhysRevResearch.2.033429. URL https://journals.aps.org/prresearch/
abstract/10.1103/PhysRevResearch.2.033429."
REFERENCES,0.9628865979381444,"Preskill, J. Quantum computing 40 years later. arXiv preprint arXiv:2106.10522, 2021."
REFERENCES,0.9649484536082474,"Reuther, A., Kepner, J., Byun, C., Samsi, S., Arcand, W., Bestor, D., Bergeron, B., Gadepally, V.,
Houle, M., Hubbell, M., Jones, M., Klein, A., Milechin, L., Mullen, J., Prout, A., Rosa, A., Yee,
C., and Michaleas, P. Interactive supercomputing on 40,000 cores for machine learning and data
analysis. In 2018 IEEE High Performance extreme Computing Conference (HPEC), pp. 1‚Äì6. IEEE,
2018."
REFERENCES,0.9670103092783505,"Schmitt, M. and Heyl, M. Quantum many-body dynamics in two dimensions with artificial neural net-
works. Physical Review Letters, 125(10), Sep 2020. ISSN 1079-7114. doi: 10.1103/physrevlett.125.
100503.
URL https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.125.
100503."
REFERENCES,0.9690721649484536,"Sharir, O., Levine, Y., Wies, N., Carleo, G., and Shashua, A. Deep autoregressive models for
the efficient variational simulation of many-body quantum systems. Physical Review Letters,
124(2), jan 2020. doi: 10.1103/physrevlett.124.020503. URL https://doi.org/10.1103%
2Fphysrevlett.124.020503."
REFERENCES,0.9711340206185567,"Sharir, O., Shashua, A., and Carleo, G. Neural tensor contractions and the expressive power of deep
neural quantum states, 2021."
REFERENCES,0.9731958762886598,"Van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A., et al. Conditional image
generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016."
REFERENCES,0.9752577319587629,"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., and
Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30:
5998‚Äì6008, 2017."
REFERENCES,0.977319587628866,"Verstraete, F. and Cirac, J. I. Renormalization algorithms for quantum-many body systems in two
and higher dimensions, 2004. URL https://arxiv.org/abs/cond-mat/0407066."
REFERENCES,0.979381443298969,"Vicentini, F., Biella, A., Regnault, N., and Ciuti, C. Variational neural-network ansatz for steady
states in open quantum systems. Physical Review Letters, 122(25), Jun 2019. ISSN 1079-7114.
doi: 10.1103/physrevlett.122.250503. URL https://journals.aps.org/prl/abstract/10.
1103/PhysRevLett.122.250503."
REFERENCES,0.9814432989690721,"Vicentini, F., Hofmann, D., Szab√≥, A., Wu, D., Roth, C., Giuliani, C., Pescia, G., Nys, J., Vargas-
Calder√≥n, V., Astrakhantsev, N., and Carleo, G. NetKet 3: Machine learning toolbox for many-body
quantum systems. SciPost Physics Codebases, aug 2022. doi: 10.21468/scipostphyscodeb.7. URL
https://doi.org/10.21468%2Fscipostphyscodeb.7."
REFERENCES,0.9835051546391752,"Vidal, G. Efficient classical simulation of slightly entangled quantum computations. Physical Review
Letters, 91(14), oct 2003. doi: 10.1103/physrevlett.91.147902. URL https://doi.org/10.
1103%2Fphysrevlett.91.147902."
REFERENCES,0.9855670103092784,"Vidal, G. Efficient simulation of one-dimensional quantum many-body systems. Physical Review
Letters, 93(4), jul 2004. doi: 10.1103/physrevlett.93.040502. URL https://doi.org/10.1103%
2Fphysrevlett.93.040502."
REFERENCES,0.9876288659793815,"Vidal, G.
Entanglement renormalization.
Phys. Rev. Lett., 99:220405, Nov 2007.
doi:
10.1103/PhysRevLett.99.220405. URL https://link.aps.org/doi/10.1103/PhysRevLett.
99.220405."
REFERENCES,0.9896907216494846,"Vieijra, T., Casert, C., Nys, J., De Neve, W., Haegeman, J., Ryckebusch, J., and Verstraete, F.
Restricted boltzmann machines for quantum states with non-abelian or anyonic symmetries.
Physical Review Letters, 124(9), Mar 2020. ISSN 1079-7114. doi: 10.1103/physrevlett.124.097201.
URL https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.124.097201."
REFERENCES,0.9917525773195877,"Westerhout, T., Astrakhantsev, N., Tikhonov, K. S., Katsnelson, M. I., and Bagrov, A. A. General-
ization properties of neural network approximations to frustrated magnet ground states. Nature
Communications, 11(1):1593, Mar 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-15402-w.
URL https://doi.org/10.1038/s41467-020-15402-w."
REFERENCES,0.9938144329896907,"White, S. R. Density matrix formulation for quantum renormalization groups. Physical review letters,
69(19):2863, 1992."
REFERENCES,0.9958762886597938,"Wu, D., Rossi, R., Vicentini, F., and Carleo, G. From tensor network quantum states to tensorial
recurrent neural networks. arXiv preprint arXiv:2206.12363, 2022."
REFERENCES,0.9979381443298969,"Yoshioka, N. and Hamazaki, R. Constructing neural stationary states for open quantum many-
body systems. Phys. Rev. B, 99:214306, Jun 2019. doi: 10.1103/PhysRevB.99.214306. URL
https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.214306."
