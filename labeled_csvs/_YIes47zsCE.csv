Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0031446540880503146,"We address the challenge of identifying multiple change points in a group of independent
time series, assuming these change points occur simultaneously in all series and their number is
unknown. The search for the best segmentation can be expressed as a minimization problem
over a given cost function. We focus on dynamic programming algorithms that solve this
problem exactly. When the number of changes is proportional to data length, an inequality-based
pruning rule encoded in the PELT algorithm leads to a linear time complexity. Another type of
pruning, called functional pruning, gives a close-to-linear time complexity whatever the number
of changes, but only for the analysis of univariate time series. We propose a few extensions of
functional pruning for multiple independent time series based on the use of simple geometric
shapes (balls and hyperrectangles). We focus on the Gaussian case, but some of our rules can be
easily extended to the exponential family. In a simulation study we compare the computational
efficiency of different geometric-based pruning rules. We show that for a small number of time
series some of them ran significantly faster than inequality-based approaches in particular when
the underlying number of changes is small compared to the data length."
ABSTRACT,0.006289308176100629,"Keywords: multivariate time series, multiple change point detection, dynamic programming, func-
tional pruning, computational geometry"
OTHER,0.009433962264150943,Contents
OTHER,0.012578616352201259,"Introduction
2"
OTHER,0.015723270440251572,"1
Functional Pruning for Multiple Time Series
3
1.1
Model and Cost
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Functional Pruning Optimal Partitioning Algorithm . . . . . . . . . . . . . . . . . .
4
1.3
Geometric Formulation of Functional Pruning . . . . . . . . . . . . . . . . . . . . .
6"
OTHER,0.018867924528301886,"2
Geometric Functional Pruning Optimal Partitioning
8
2.1
General Principle of GeomFPOP . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8"
OTHER,0.0220125786163522,1Corresponding author: liudmila.pishchagina@univ-evry.fr
OTHER,0.025157232704402517,"3
Approximation Operators â‹‚Ìƒğ‘and â§µÌƒğ‘
10
3.1
S-type Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
3.2
R-type Approximation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11"
OTHER,0.02830188679245283,"4
Simulation Study of GeomFPOP
13
4.1
The Number of Change Point Candidates stored over Time . . . . . . . . . . . . . .
14
4.2
Empirical Time Complexity of GeomFPOP . . . . . . . . . . . . . . . . . . . . . . .
15
4.3
Empirical Time Complexity of a Randomized GeomFPOP . . . . . . . . . . . . . . .
15
4.4
Empirical Complexity of the Algorithm as a Function of ğ‘
. . . . . . . . . . . . . .
16
4.5
Run Time as a Function of the Number of Segments . . . . . . . . . . . . . . . . . .
17"
OTHER,0.031446540880503145,"Acknowledgments
18"
OTHER,0.03459119496855346,"5
Supplements
18
5.1
Examples of Likelihood-Based Cost Functions
. . . . . . . . . . . . . . . . . . . . .
18
5.2
Intersection and Inclusion of Two p-balls . . . . . . . . . . . . . . . . . . . . . . . .
18
5.3
Intersection and Inclusion Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
5.4
Proof of Proposition 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
5.5
Optimization Strategies for GeomFPOP (R-type) . . . . . . . . . . . . . . . . . . . .
21
5.6
The number of change point candidates in time: GeomFPOP vs. PELT . . . . . . . .
22
5.7
Run time of the algorithm by multivariate time series with changes in subset of
dimension
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22"
OTHER,0.03773584905660377,"References
22"
INTRODUCTION,0.040880503144654086,Introduction
INTRODUCTION,0.0440251572327044,"A National Research Council report (Data et al. 2013) has identified change point detection as one
of the â€œinferential giantsâ€ in massive data analysis. Detecting change points, either a posteriori or
online, is important in areas as diverse as bioinformatics (Olshen et al. 2004; Picard et al. 2005),
econometrics (Bai and Perron 2003; Aue et al. 2006), medicine (Bosc et al. 2003; Staudacher et al.
2005; Malladi, Kalamangalam, and Aazhang 2013), climate and oceanography (Reeves et al. 2007;
DucrÃ©-Robitaille, Vincent, and Boulet 2003; Killick, Fearnhead, and Eckley 2012; Naoki and Kurths
2010), finance (Andreou and Ghysels 2002; Fryzlewicz 2014), autonomous driving (Galceran et al.
2017), entertainment (Rybach et al. 2009; Radke et al. 2005; Davis, Lee, and Rodriguez-Yam 2006),
computer vision (Ranganathan 2012) or neuroscience (Jewell, Fearnhead, and Witten 2019). The most
common and prototypical change point detection problem is that of detecting changes in mean of a
univariate Gaussian signal and a large number of approaches have been proposed to perform this task
(see among many others (Yao 1984; Lebarbier 2005; Harchaoui and LÃ©vy-Leduc 2010; Frick, Munk,
and Sieling 2013; Anastasiou and Fryzlewicz 2022) and the reviews (Truong, Oudre, and Vayatis 2020;
Aminikhanghahi and Cook 2017))."
INTRODUCTION,0.04716981132075472,"Penalized cost methods. Some of these methods optimize a penalized cost function (see for example
(Lebarbier 2005; Auger and Lawrence 1989; Jackson et al. 2005; Killick, Fearnhead, and Eckley 2012;
Rigaill 2015; Maidstone et al. 2017). These methods have good statistical guarantees (Yao 1984;
Lavielle and Moulines 2000; Lebarbier 2005) and have shown good performances in benchmark
simulation (Fearnhead, Maidstone, and Letchford 2018) and on many applications (Lai et al. 2005;
Liehrmann, Rigaill, and Hocking 2021). From a computational perspective, they rely on dynamic
programming algorithms that are at worst quadratic in the size of the data, ğ‘›. However using
inequality-based and functional pruning techniques (Rigaill 2015; Killick, Fearnhead, and Eckley
2012; Maidstone et al. 2017) the average run times are typically much smaller allowing to process"
INTRODUCTION,0.050314465408805034,"very large profiles (ğ‘›> 105) in a matter of seconds or minutes. In detail, for one time series:"
INTRODUCTION,0.05345911949685535,"â€¢ if the number of change points is proportional to ğ‘›both PELT (Killick, Fearnhead, and Eckley
2012) (a version of OP which uses inequality-based pruning) and FPOP (Maidstone et al. 2017)
(a version of OP which uses functional pruning as in (Rigaill 2015)) are on average linear
(Killick, Fearnhead, and Eckley 2012; Maidstone et al. 2017);
â€¢ if the number of change points is fixed, FPOP is quasi-linear (on simulations) while PELT is
quadratic (Maidstone et al. 2017)."
INTRODUCTION,0.05660377358490566,"Multivariate extensions. This paper focuses on identifying multiple change points in a multivariate
independent time series. We assume that changes occur simultaneously in all dimensions, their
number is unknown, and the cost function or log-likelihood of a segment (denoted as ğ’) can be
expressed as a sum across all dimensions ğ‘. Informally, that is,"
INTRODUCTION,0.059748427672955975,ğ’(ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡) =
INTRODUCTION,0.06289308176100629,"ğ‘
âˆ‘
ğ‘˜=1
ğ’(ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡, time series ğ‘˜) ."
INTRODUCTION,0.0660377358490566,"In this context, the PELT algorithm can easily be extended for multiple time series. However, as for
the univariate case, it will be algorithmically efficient only if the number of non-negligible change
points is comparable to ğ‘›. In this paper, we study the extension of functional pruning techniques
(and more specifically FPOP) to the multivariate case."
INTRODUCTION,0.06918238993710692,"At each iteration, FPOP updates the set of parameter values for which a change position ğœis optimal.
As soon as this set is empty the change is pruned. For univariate time series, this set is a union of
intervals in â„. For parametric multivariate models, this set is equal to the intersection and difference
of convex sets in â„ğ‘(Runge 2020). It is typically non-convex, hard to update, and deciding whether
it is empty or not is not straightforward."
INTRODUCTION,0.07232704402515723,"In this work, we present a new algorithm, called Geometric Functional Pruning Optimal Partitioning
(GeomFPOP). The idea of our method consists in approximating the sets that are updated at each
iteration of FPOP using simpler geometric shapes. Their simplicity of description and simple updating
allow for a quick emptiness test."
INTRODUCTION,0.07547169811320754,"The paper has the following structure. In Section 1 we introduce the penalized optimization problem
for segmented multivariate time series in case where the number of changes is unknown. We then
review the existing pruned dynamic programming methods for solving this problem. We define the
geometric problem that occurs when using functional pruning. The new method, called GeomFPOP,
is described in Section 2 and based on approximating intersection and exclusion set operators. In
Section 3 we introduce two approximation types (sphere-like and rectangle-like) and define the
approximation operators for each of them. We then compare in Section 4 the empirical efficiency of
GeomFPOP with PELT on simulated data."
IMPLEMENTATION/METHODS,0.07861635220125786,"1
Functional Pruning for Multiple Time Series"
IMPLEMENTATION/METHODS,0.08176100628930817,"1.1
Model and Cost"
IMPLEMENTATION/METHODS,0.08490566037735849,"We consider the problem of change point detection in multiple independent time series of length ğ‘›
and dimension ğ‘, while assuming simultaneous changes in all univariate time series and an unknown
number of changes. Our aim is to partition data into segments, such that in each segment the
parameter associated to each time series is constant. For a time series ğ‘¦we write ğ‘¦= ğ‘¦1âˆ¶ğ‘›=
(ğ‘¦1, â€¦ , ğ‘¦ğ‘›) âˆˆ(â„ğ‘)ğ‘›with ğ‘¦ğ‘˜
ğ‘–the ğ‘˜-th component of the ğ‘-dimensional point ğ‘¦ğ‘–âˆˆâ„ğ‘in position ğ‘–
in vector ğ‘¦1âˆ¶ğ‘›. We also use the notation ğ‘¦ğ‘–âˆ¶ğ‘—= (ğ‘¦ğ‘–, â€¦ , ğ‘¦ğ‘—) to denote points from index ğ‘–to ğ‘—. If we"
IMPLEMENTATION/METHODS,0.0880503144654088,"assume that there are ğ‘€change points in a time series, this corresponds to time series splits into
ğ‘€+ 1 distinct segments. The data points of each segment ğ‘šâˆˆ{1, â€¦ , ğ‘€+ 1} are generated by
independent random variables from a multivariate distribution with the segment-specific parameter
ğœƒğ‘š= (ğœƒ1ğ‘š, â€¦ , ğœƒğ‘
ğ‘š) âˆˆâ„ğ‘. A segmentation with ğ‘€change points is defined by the vector of integers
ğœ= (ğœ0 = 0, ğœ1, â€¦ , ğœğ‘€, ğœğ‘€+1 = ğ‘›). Segments are given by the sets of indices {ğœğ‘–+ 1, â€¦ , ğœğ‘–+1} with ğ‘–in
{0, 1, â€¦ , ğ‘€}."
IMPLEMENTATION/METHODS,0.09119496855345911,"We define the set ğ‘†ğ‘€
ğ‘›of all possible change point locations related to the segmentation of data points
between positions 1 to ğ‘›in ğ‘€+ 1 segments as"
IMPLEMENTATION/METHODS,0.09433962264150944,"ğ‘†ğ‘€
ğ‘›= {ğœ= (ğœ0, ğœ1, â€¦ , ğœğ‘€, ğœğ‘€+1) âˆˆâ„•ğ‘€+2|0 = ğœ0 < ğœ1 < â‹¯< ğœğ‘€< ğœğ‘€+1 = ğ‘›} ."
IMPLEMENTATION/METHODS,0.09748427672955975,"For any segmentation ğœin ğ‘†ğ‘€
ğ‘›we define its size as |ğœ| = ğ‘€. We denote ğ’®âˆ
ğ‘›as the set of all possible
segmentations of ğ‘¦1âˆ¶ğ‘›:"
IMPLEMENTATION/METHODS,0.10062893081761007,"ğ’®âˆ
ğ‘›= â‹ƒ
ğ‘€<ğ‘›
ğ‘†ğ‘€
ğ‘›,"
IMPLEMENTATION/METHODS,0.10377358490566038,"and take the convention that ğ‘†âˆâˆ’1
ğ‘›
= ğ‘†âˆ
ğ‘›. In our case the number of changes ğ‘€is unknown, and has
to be estimated."
IMPLEMENTATION/METHODS,0.1069182389937107,"Many approaches to detecting change points define a cost function for segmentation using the
negative log-likelihood (times two). Here the negative log-likelihood (times two) calculated at the
data point ğ‘¦ğ‘—is given by function ğœƒâ†¦Î©(ğœƒ, ğ‘¦ğ‘—), where ğœƒ= (ğœƒ1, â€¦ , ğœƒğ‘) âˆˆâ„ğ‘. Over a segment from ğ‘–to
ğ‘¡, the parameter remains the same and the segment cost ğ’is given by"
IMPLEMENTATION/METHODS,0.11006289308176101,"ğ’(ğ‘¦ğ‘–âˆ¶ğ‘¡) = min
ğœƒâˆˆâ„ğ‘"
IMPLEMENTATION/METHODS,0.11320754716981132,"ğ‘¡
âˆ‘
ğ‘—=ğ‘–
Î©(ğœƒ, ğ‘¦ğ‘—) = min
ğœƒâˆˆâ„ğ‘"
IMPLEMENTATION/METHODS,0.11635220125786164,"ğ‘¡
âˆ‘
ğ‘—=ğ‘–
("
IMPLEMENTATION/METHODS,0.11949685534591195,"ğ‘
âˆ‘
ğ‘˜=1
ğœ”(ğœƒğ‘˜, ğ‘¦ğ‘˜
ğ‘—)) ,
(1)"
IMPLEMENTATION/METHODS,0.12264150943396226,"with ğœ”the atomic likelihood function associated with Î© for each univariate time series. This
decomposition is made possible by the independence hypothesis between univariate time series}.
Notice that it could have been dimension-dependent with a mixture of different distributions (Gauss,
Poisson, negative binomial, etc.). In our study, we use the same data model for all dimensions."
IMPLEMENTATION/METHODS,0.12578616352201258,"In summary, the methodology we propose relies on the assumption that:"
IMPLEMENTATION/METHODS,0.1289308176100629,"1. the cost is point additive (see first equality in equation (1));
2. the per-point cost Î© has a simple decomposition : Î©(ğœƒ) = âˆ‘ğ‘ğœ”(ğœƒğ‘);
3. the ğœ”is convex."
IMPLEMENTATION/METHODS,0.1320754716981132,"We get that for any ğœâˆˆğ’®âˆ
ğ‘›its segmentation cost is the sum of segment cost functions:"
IMPLEMENTATION/METHODS,0.13522012578616352,"|ğœ|
âˆ‘
ğ‘–=0
ğ’(ğ‘¦(ğœğ‘–+1)âˆ¶ğœğ‘–+1) ."
IMPLEMENTATION/METHODS,0.13836477987421383,"We consider a penalized version of the segment cost by a penalty ğ›½> 0, as the zero penalty case would
lead to segmentation with ğ‘›segments. The optimal penalized cost associated with our segmentation
problem is then defined by"
IMPLEMENTATION/METHODS,0.14150943396226415,Ì‚ğ‘„ğ‘›= min
IMPLEMENTATION/METHODS,0.14465408805031446,"ğœâˆˆğ‘†âˆ
ğ‘›"
IMPLEMENTATION/METHODS,0.14779874213836477,"|ğœ|
âˆ‘
ğ‘–=0
{ğ’(ğ‘¦(ğœğ‘–+1)âˆ¶ğœğ‘–+1) + ğ›½} .
(2)"
IMPLEMENTATION/METHODS,0.1509433962264151,The optimal segmentation ğœis obtained by the argminimum in equation (2).
IMPLEMENTATION/METHODS,0.1540880503144654,"Various penalty forms have been proposed in the literature (Yao 1984; Killick, Fearnhead, and Eckley
2012; Zhang and Siegmund 2007; Lebarbier 2005; Verzelen et al. 2020). Summing over all segments
in Equation (2), we end up with a global penalty of the form ğ›½(ğ‘€+ 1). Hence, our model only allows
penalties that are proportional to the number of segments (Yao 1984; Killick, Fearnhead, and Eckley
2012). Penalties such as (Zhang and Siegmund 2007; Lebarbier 2005; Verzelen et al. 2020) cannot be
considered with our algorithm."
IMPLEMENTATION/METHODS,0.15723270440251572,"By default, we set the penalty ğ›½for ğ‘-variate time series of length ğ‘›using the Schwarz Information
Criterion from (Yao 1984) (calibrated to the ğ‘dimensions), as ğ›½= 2ğ‘ğœ2 log ğ‘›. In practice, if the
variance ğœ2 is unknown, it is replaced by an appropriate estimation (e.g. (Hampel 1974; Hall, Kay,
and Titterington 1990) as in (Lavielle and Lebarbier 2001; Liehrmann et al. 2023))."
IMPLEMENTATION/METHODS,0.16037735849056603,"1.2
Functional Pruning Optimal Partitioning Algorithm"
IMPLEMENTATION/METHODS,0.16352201257861634,"The idea of the Optimal Partitioning (OP) method (Jackson et al. 2005) is to search for the last
change point defining the last segment in data ğ‘¦1âˆ¶ğ‘¡at each iteration (with ğ‘„0 = 0), which leads to the
recursion:"
IMPLEMENTATION/METHODS,0.16666666666666666,"ğ‘„ğ‘¡=
min
ğ‘–âˆˆ{0,â€¦,ğ‘¡âˆ’1} (ğ‘„ğ‘–+ ğ’(ğ‘¦(ğ‘–+1âˆ¶ğ‘¡) + ğ›½) ."
IMPLEMENTATION/METHODS,0.16981132075471697,"The Pruned Exact Linear Time (PELT) method, introduced in (Killick, Fearnhead, and Eckley 2012),
uses inequality-based pruning. It essentially relies on the assumption that splitting a segment in two
is always beneficial in terms of cost, this is ğ¶(ğ‘¦(ğ‘–+1)âˆ¶ğ‘—) + ğ¶(ğ‘¦(ğ‘—+1)âˆ¶ğ‘¡) â‰¤ğ¶(ğ‘¦(ğ‘–+1)âˆ¶ğ‘¡). This assumption is
always true in our setting. PELT considers each change point candidate sequentially and decides
whether ğ‘–can be excluded from the set of changepoint candidates if Ì‚ğ‘„ğ‘–+ ğ’(ğ‘¦(ğ‘–+1)âˆ¶ğ‘¡) â‰¥
Ì‚ğ‘„ğ‘¡, as ğ‘–cannot
appear as the optimal change point in future iterations."
IMPLEMENTATION/METHODS,0.17295597484276728,"Functional description. In the FPOP method we introduce a last segment parameter ğœƒ= (ğœƒ1, â€¦ , ğœƒğ‘) in
â„ğ‘and define a functional cost ğœƒâ†¦ğ‘„ğ‘¡(ğœƒ) depending on ğœƒ, that takes the following form:"
IMPLEMENTATION/METHODS,0.1761006289308176,"ğ‘„ğ‘¡(ğœƒ) = min
ğœâˆˆğ‘†ğ‘¡
(
ğ‘€âˆ’1
âˆ‘
ğ‘–=0
{ğ’(ğ‘¦(ğœğ‘–+1)âˆ¶ğœğ‘–+1) + ğ›½} +
ğ‘¡
âˆ‘
ğ‘—=ğœğ‘€+1
Î©(ğœƒ, ğ‘¦ğ‘—) + ğ›½) ."
IMPLEMENTATION/METHODS,0.1792452830188679,"As explained in (Maidstone et al. 2017), we can compute the function ğ‘„ğ‘¡+1(â‹…) based only on the
knowledge of ğ‘„ğ‘¡(â‹…) for each integer ğ‘¡from 0 to ğ‘›âˆ’1. We have:"
IMPLEMENTATION/METHODS,0.18238993710691823,"ğ‘„ğ‘¡+1(ğœƒ) = min{ğ‘„ğ‘¡(ğœƒ), Ì‚ğ‘„ğ‘¡+ ğ›½} + Î©(ğœƒ, ğ‘¦ğ‘¡+1) ,
(3)"
IMPLEMENTATION/METHODS,0.18553459119496854,"for all ğœƒâˆˆâ„ğ‘, with
Ì‚ğ‘„ğ‘¡= minğœƒğ‘„ğ‘¡(ğœƒ) (ğ‘¡â‰¥1) and the initialization ğ‘„0(ğœƒ) = ğ›½,
Ì‚ğ‘„0 = 0 so that
ğ‘„1(ğœƒ) = Î©(ğœƒ, ğ‘¦1) + ğ›½. By looking closely at this relation, we see that each function ğ‘„ğ‘¡is a piece-wise
continuous function consisting of at most ğ‘¡different functions on â„ğ‘, denoted ğ‘ğ‘–ğ‘¡:"
IMPLEMENTATION/METHODS,0.18867924528301888,"ğ‘„ğ‘¡(ğœƒ) =
min
ğ‘–âˆˆ{1,â€¦,ğ‘¡} {ğ‘ğ‘–ğ‘¡(ğœƒ)} ,"
IMPLEMENTATION/METHODS,0.1918238993710692,where the ğ‘ğ‘–ğ‘¡functions are given by explicit formulas:
IMPLEMENTATION/METHODS,0.1949685534591195,"ğ‘ğ‘–ğ‘¡(ğœƒ) =
Ì‚ğ‘„ğ‘–âˆ’1 + ğ›½+
ğ‘¡
âˆ‘
ğ‘—=ğ‘–
Î©(ğœƒ, ğ‘¦ğ‘—) ,
ğœƒâˆˆâ„ğ‘,
ğ‘–= 1, â€¦ , ğ‘¡. and"
IMPLEMENTATION/METHODS,0.19811320754716982,"Ì‚ğ‘„ğ‘–âˆ’1 = min
ğœƒâˆˆâ„ğ‘ğ‘„ğ‘–âˆ’1(ğœƒ) =
min
ğ‘—âˆˆ{1,â€¦,ğ‘–âˆ’1} {min
ğœƒâˆˆâ„ğ‘ğ‘ğ‘—
ğ‘–âˆ’1(ğœƒ)} .
(4)"
IMPLEMENTATION/METHODS,0.20125786163522014,"It is important to notice that each ğ‘ğ‘–ğ‘¡function is associated with the last change point ğ‘–âˆ’1 and the
last segment is given by indices from ğ‘–to ğ‘¡. Consequently, the last change point at step ğ‘¡in ğ‘¦1âˆ¶ğ‘¡is
denoted as Ì‚ğœğ‘¡( Ì‚ğœğ‘¡â‰¤ğ‘¡âˆ’1) and is given by"
IMPLEMENTATION/METHODS,0.20440251572327045,"Ì‚ğœğ‘¡= ğ´ğ‘Ÿğ‘”min
ğ‘–âˆˆ{1,â€¦,ğ‘¡}
{min
ğœƒâˆˆâ„ğ‘ğ‘ğ‘–ğ‘¡(ğœƒ)} âˆ’1."
IMPLEMENTATION/METHODS,0.20754716981132076,"Backtracking. Knowing the values of Ì‚ğœğ‘¡for all ğ‘¡= 1, â€¦ , ğ‘›, we can always restore the optimal
segmentation at time ğ‘›for ğ‘¦1âˆ¶ğ‘›. This procedure is called backtracking. The vector ğ‘ğ‘(ğ‘›) of ordered
change points in the optimal segmentation of ğ‘¦1âˆ¶ğ‘›is determined recursively by the relation ğ‘ğ‘(ğ‘›) =
(ğ‘ğ‘( Ì‚ğœğ‘›), Ì‚ğœğ‘›) with stopping rule ğ‘ğ‘(0) = âˆ…."
IMPLEMENTATION/METHODS,0.21069182389937108,"Parameter space description. Applying functional pruning requires a precise analysis of the recursion
(3) that depends on the property of the cost function Î©. In what follows we consider three choices
based on a Gaussian, Poisson, and negative binomial distribution for data distribution. The exact
formulas of these cost functions are given in Section 5.1."
IMPLEMENTATION/METHODS,0.2138364779874214,We denote the set of parameter values for which the function ğ‘ğ‘–ğ‘¡(â‹…) is optimal as:
IMPLEMENTATION/METHODS,0.2169811320754717,"ğ‘ğ‘–ğ‘¡= {ğœƒâˆˆâ„ğ‘|ğ‘„ğ‘¡(ğœƒ) = ğ‘ğ‘–ğ‘¡(ğœƒ)} ,
ğ‘–= 1, â€¦ , ğ‘¡."
IMPLEMENTATION/METHODS,0.22012578616352202,"also called the living zone. The key idea behind functional pruning is that the ğ‘ğ‘–ğ‘¡are nested (ğ‘ğ‘–
ğ‘¡+1 âŠ‚ğ‘ğ‘–ğ‘¡)
thus as soon as we can prove the emptiness of one set ğ‘ğ‘–ğ‘¡, we delete its associated ğ‘ğ‘–ğ‘¡function and
do not have to consider its minimum anymore at any further iteration (proof in Section 1.3). In
dimension ğ‘= 1 this is reasonably easy. In this case, the sets ğ‘ğ‘–ğ‘¡(ğ‘–= 1, â€¦ , ğ‘¡) are unions of intervals
and an efficient functional pruning rule is possible by updating a list of these intervals for ğ‘„ğ‘¡. This
approach is implemented in FPOP (Maidstone et al. 2017)."
IMPLEMENTATION/METHODS,0.22327044025157233,"In dimension ğ‘â‰¥2 it is not so easy anymore to keep track of the emptiness of the sets ğ‘ğ‘–ğ‘¡. We illustrate
the dynamics of the ğ‘ğ‘–ğ‘¡sets in Figure 1 in the bivariate Gaussian case. Each color is associated with a
set ğ‘ğ‘–ğ‘¡(corresponding to a possible change at ğ‘–âˆ’1) for ğ‘¡equal 1 to 5. This plot shows in particular
that sets ğ‘ğ‘–ğ‘¡can be non-convex."
IMPLEMENTATION/METHODS,0.22641509433962265,"1.3
Geometric Formulation of Functional Pruning"
IMPLEMENTATION/METHODS,0.22955974842767296,"To build an efficient pruning strategy for dimension ğ‘â‰¥2 we need to test the emptiness of the sets
ğ‘ğ‘–ğ‘¡at each iteration. Note that to get ğ‘ğ‘–ğ‘¡we need to compare the functional cost ğ‘ğ‘–ğ‘¡with any other
functional cost ğ‘ğ‘—
ğ‘¡, ğ‘—= 1, â€¦ , ğ‘¡, ğ‘—â‰ ğ‘–. This leads to the definition of the following sets."
IMPLEMENTATION/METHODS,0.23270440251572327,"Definition 1.1. We define ğ‘†-type set ğ‘†ğ‘–
ğ‘—using the function Î© as"
IMPLEMENTATION/METHODS,0.2358490566037736,"ğ‘†ğ‘–
ğ‘—= {ğœƒâˆˆâ„ğ‘|"
IMPLEMENTATION/METHODS,0.2389937106918239,"ğ‘—âˆ’1
âˆ‘
ğ‘¢=ğ‘–
Î©(ğœƒ, ğ‘¦ğ‘¢) â‰¤
Ì‚ğ‘„ğ‘—âˆ’1 âˆ’Ì‚ğ‘„ğ‘–âˆ’1} , when ğ‘–< ğ‘—"
IMPLEMENTATION/METHODS,0.24213836477987422,"Figure 1: The sets ğ‘ğ‘–ğ‘¡over time for the bivariate independent Gaussian model on time series without
change ğ‘¦= ((0.29, 1.93), (1.86, âˆ’0.02), (0.9, 2.51), (âˆ’1.26, 0.91), (1.22, 1.11)). From left to right we
represent at time ğ‘¡= 1, 2, 3, 4, and 5 the parameter space (ğœƒ1, ğœƒ2). Each ğ‘ğ‘–ğ‘¡is represented by a color.
The change 1 associated with quadratics 2 is pruned at ğ‘¡= 3. Notice that each time sequence of ğ‘ğ‘–ğ‘¡
with ğ‘–fixed is a nested sequence of sets."
IMPLEMENTATION/METHODS,0.24528301886792453,"and ğ‘†ğ‘–
ğ‘–= â„ğ‘. We denote the set of all possible S-type sets as S."
IMPLEMENTATION/METHODS,0.24842767295597484,"To ease some of our calculations, we now introduce some additional notations. For ğœƒ= (ğœƒ1, â€¦ , ğœƒğ‘) in
â„ğ‘, 1 â‰¤ğ‘–< ğ‘—â‰¤ğ‘›we define ğ‘univariate functions ğœƒğ‘˜â†¦ğ‘ ğ‘˜
ğ‘–ğ‘—(ğœƒğ‘˜) associated to the ğ‘˜-th time series as"
IMPLEMENTATION/METHODS,0.25157232704402516,"ğ‘ ğ‘˜
ğ‘–ğ‘—(ğœƒğ‘˜) ="
IMPLEMENTATION/METHODS,0.25471698113207547,"ğ‘—âˆ’1
âˆ‘
ğ‘¢=ğ‘–
ğœ”(ğœƒğ‘˜, ğ‘¦ğ‘˜ğ‘¢),
ğ‘˜= 1, â€¦ , ğ‘.
(5)"
IMPLEMENTATION/METHODS,0.2578616352201258,"We introduce a constant Î”ğ‘–ğ‘—and a function ğœƒâ†¦ğ‘ ğ‘–ğ‘—(ğœƒ): â§ â¨
â©"
IMPLEMENTATION/METHODS,0.2610062893081761,"Î”ğ‘–ğ‘—=
Ì‚ğ‘„ğ‘—âˆ’1 âˆ’Ì‚ğ‘„ğ‘–âˆ’1 ,"
IMPLEMENTATION/METHODS,0.2641509433962264,ğ‘ ğ‘–ğ‘—(ğœƒ) =
IMPLEMENTATION/METHODS,0.2672955974842767,"ğ‘
âˆ‘
ğ‘˜=1
ğ‘ ğ‘˜
ğ‘–ğ‘—(ğœƒğ‘˜) âˆ’Î”ğ‘–ğ‘—,
(6)"
IMPLEMENTATION/METHODS,0.27044025157232704,"where Ì‚ğ‘„ğ‘–âˆ’1 and Ì‚ğ‘„ğ‘—âˆ’1 are defined as in (4). The sets ğ‘†ğ‘–
ğ‘—for ğ‘–< ğ‘—can thus be written as"
IMPLEMENTATION/METHODS,0.27358490566037735,"ğ‘†ğ‘–
ğ‘—= ğ‘ âˆ’1
ğ‘–ğ‘—(âˆ’âˆ, 0] .
(7)"
IMPLEMENTATION/METHODS,0.27672955974842767,"In Figure 2 we present the level curves for three different parametric models given by ğ‘ âˆ’1
ğ‘–ğ‘—({ğ‘¤}) with
ğ‘¤a real number. Each of these curves encloses an S-type set, which, according to the definition of
the function ğœ”, is convex."
IMPLEMENTATION/METHODS,0.279874213836478,"At time ğ‘¡= 1, â€¦ , ğ‘›we define the following sets associated to the last change point index ğ‘–âˆ’1:"
IMPLEMENTATION/METHODS,0.2830188679245283,-past set ğ’«ğ‘–
IMPLEMENTATION/METHODS,0.2861635220125786,"ğ’«ğ‘–= {ğ‘†ğ‘¢
ğ‘–, ğ‘¢= 1, â€¦ , ğ‘–âˆ’1} ."
IMPLEMENTATION/METHODS,0.2893081761006289,"-future set â„±ğ‘–(ğ‘¡)
â„±ğ‘–(ğ‘¡) = {ğ‘†ğ‘–ğ‘£, ğ‘£= ğ‘–, â€¦ , ğ‘¡} ."
IMPLEMENTATION/METHODS,0.29245283018867924,"We denote the cardinal of a set ğ’œas |ğ’œ|. Using these two sets of sets, the ğ‘ğ‘–ğ‘¡have the following
description."
IMPLEMENTATION/METHODS,0.29559748427672955,"Proposition 1.1. At iteration ğ‘¡, the living zones ğ‘ğ‘–ğ‘¡(ğ‘–= 1, â€¦ , ğ‘¡) are defined by the functional cost ğ‘„ğ‘¡(â‹…),
with each of them being formed as the intersection of sets in â„±ğ‘–(ğ‘¡) excluding the union of sets in ğ’«ğ‘–."
IMPLEMENTATION/METHODS,0.29874213836477986,"Figure 2: Three examples of the level curves of a function ğ‘ ğ‘–ğ‘—for bivariate time series {ğ‘¦1, ğ‘¦2}. We use
the following simulations for univariate time series : (a) ğ‘¦1 âˆ¼ğ’©(0, 1), ğ‘¦2 âˆ¼ğ’©(0, 1), (b) ğ‘¦1 âˆ¼ğ’«(1),
ğ‘¦2 âˆ¼ğ’«(3), (c) ğ‘¦1 âˆ¼ğ’©â„¬(0.5, 1), ğ‘¦2 âˆ¼ğ’©â„¬(0.8, 1)."
IMPLEMENTATION/METHODS,0.3018867924528302,ğ‘ğ‘–ğ‘¡= ( â‹‚
IMPLEMENTATION/METHODS,0.3050314465408805,"ğ‘†âˆˆâ„±ğ‘–(ğ‘¡)
ğ‘†) â§µ(âˆªğ‘†âˆˆğ’«ğ‘–ğ‘†) ,
ğ‘–= 1, â€¦ , ğ‘¡.
(8)"
IMPLEMENTATION/METHODS,0.3081761006289308,"Proof. Based on the definition of the set ğ‘ğ‘–ğ‘¡, the proof is straightforward. Parameter value ğœƒis in ğ‘ğ‘–ğ‘¡
if and only if ğ‘ğ‘–ğ‘¡(ğœƒ) â‰¤ğ‘ğ‘¢ğ‘¡(ğœƒ) for all ğ‘¢â‰ ğ‘–; these inequalities define the past set (when ğ‘¢< ğ‘–) and the
future set (when ğ‘¢â‰¥ğ‘–)."
IMPLEMENTATION/METHODS,0.3113207547169811,"Proposition 1.1 states that regardless of the value of i, the living zone ğ‘ğ‘–ğ‘¡is formed through intersection
and elimination operations on ğ‘¡S-type sets. Notably, one of these sets, ğ‘†ğ‘–
ğ‘–, always represents the
entire space â„ğ‘."
IMPLEMENTATION/METHODS,0.31446540880503143,Corollary 1.1. The sequence ğœğ‘–= (ğ‘ğ‘–ğ‘¡)ğ‘¡â‰¥ğ‘–is a nested sequence of sets.
IMPLEMENTATION/METHODS,0.31761006289308175,"Indeed, ğ‘ğ‘–
ğ‘¡+1 is equal to ğ‘ğ‘–ğ‘¡with an additional intersection in the future set. Based on Corollary 1.1,
as soon as we prove that the set ğ‘ğ‘–ğ‘¡, is empty, we delete its associated ğ‘ğ‘–ğ‘¡function and, consequently,
we can prune the change point ğ‘–âˆ’1. In this context, functional and inequality-based pruning have a
simple geometric interpretation."
IMPLEMENTATION/METHODS,0.32075471698113206,"Functional pruning geometry. The position ğ‘–âˆ’1 is pruned at step ğ‘¡, in ğ‘„ğ‘¡(â‹…), if the intersection set of
â‹‚ğ‘†âˆˆâ„±ğ‘–(ğ‘¡) ğ‘†is covered by the union set âˆªğ‘†âˆˆğ’«ğ‘–ğ‘†."
IMPLEMENTATION/METHODS,0.3238993710691824,"Inequality-based pruning geometry. The inequality-based pruning of PELT is equivalent to the
geometric rule: position ğ‘–âˆ’1 is pruned at step ğ‘¡if the set ğ‘†ğ‘–ğ‘¡is empty. In that case, the intersection set
â‹‚ğ‘†âˆˆâ„±ğ‘–(ğ‘¡) ğ‘†is empty, and therefore ğ‘ğ‘–ğ‘¡is also empty using (8). This shows that if a change is pruned"
IMPLEMENTATION/METHODS,0.3270440251572327,"using inequality-based pruning it is also pruned using functional pruning. For the dimension ğ‘= 1
this claim was theoretically proved in (Maidstone et al. 2017)."
IMPLEMENTATION/METHODS,0.330188679245283,"According to Proposition 1.1, beginning with ğ‘ğ‘–
ğ‘–= â„ğ‘, the set ğ‘ğ‘–ğ‘¡is derived by iteratively applying
two types of operations: intersection with an S-type set ğ‘†from â„±ğ‘–(ğ‘¡) or subtraction of an S-type set
ğ‘†from ğ’«ğ‘–. The construction of set ğ‘ğ‘–ğ‘¡using Proposition 1.1 is illustrated in Figure 3 for a bivariate
independent Gaussian case: we have the intersection of three S-type sets and the subtraction of three
S-type sets. This simple example highlights that the set ğ‘ğ‘–ğ‘¡is typically non-convex, posing challenge
in studying its emptiness."
IMPLEMENTATION/METHODS,0.3333333333333333,"Figure 3: Examples of building a living zone ğ‘ğ‘–ğ‘¡with |ğ’«ğ‘–| = |â„±ğ‘–(ğ‘¡)| = 3 for the Gaussian case in 2-D
(ğœ‡= 0, ğœ= 1). The green disks are S-type sets of the past set ğ’«ğ‘–. The blue disks are S-type sets of
the future set â„±ğ‘–(ğ‘¡). The shaded area is the set ğ‘ğ‘–ğ‘¡."
IMPLEMENTATION/METHODS,0.33647798742138363,"2
Geometric Functional Pruning Optimal Partitioning"
IMPLEMENTATION/METHODS,0.33962264150943394,"2.1
General Principle of GeomFPOP"
IMPLEMENTATION/METHODS,0.34276729559748426,"Rather than considering an exact representation of the ğ‘ğ‘–ğ‘¡, our idea is to consider a hopefully slightly
larger set that is easier to update. To be specific, for each ğ‘ğ‘–ğ‘¡we introduce
Ìƒğ‘ğ‘–ğ‘¡, called testing set,
such that ğ‘ğ‘–ğ‘¡âŠ‚
Ìƒğ‘ğ‘–ğ‘¡. If at time ğ‘¡
Ìƒğ‘ğ‘–ğ‘¡is empty thus is ğ‘ğ‘–ğ‘¡and thus change ğ‘–âˆ’1 can be pruned. From
Proposition 1.1 we have that starting from ğ‘ğ‘–
ğ‘–= â„ğ‘the set ğ‘ğ‘–ğ‘¡is obtained by successively applying
two types of operations: intersection with an S-type set ğ‘†(ğ‘â‹‚ğ‘†) or subtraction of an S-type set ğ‘†
(ğ‘â§µğ‘†). Similarly, starting from
Ìƒğ‘ğ‘–
ğ‘–= â„ğ‘we obtain
Ìƒğ‘ğ‘–ğ‘¡by successively applying approximation of
these intersection and subtraction operations. Intuitively, the complexity of the resulting algorithm
is a combination of the efficiency of the pruning and the easiness of updating the testing set."
IMPLEMENTATION/METHODS,0.34591194968553457,"A Generic Formulation of GeomFPOP. In what follows we will generically describe GeomFPOP, that
is, without specifying the precise structure of the testing set Ìƒğ‘ğ‘–ğ‘¡. We call ÌƒZ the set of all possible Ìƒğ‘ğ‘–ğ‘¡
and assume the existence of two operators â‹‚Ìƒğ‘and â§µÌƒğ‘. We have the following assumptions for these
operators."
IMPLEMENTATION/METHODS,0.3490566037735849,Definition 2.1. The two operators â‹‚Ìƒğ‘and â§µÌƒğ‘are such that:
IMPLEMENTATION/METHODS,0.3522012578616352,"1. the left input is a Ìƒğ‘-type set (that is an element of ÌƒZ);
2. the right input is a ğ‘†-type set;"
IMPLEMENTATION/METHODS,0.3553459119496855,"3. the output is a Ìƒğ‘-type set;
4.
Ìƒğ‘â‹‚ğ‘†âŠ‚
Ìƒğ‘â‹‚Ìƒğ‘ğ‘†and Ìƒğ‘â§µğ‘†âŠ‚
Ìƒğ‘â§µÌƒğ‘ğ‘†."
IMPLEMENTATION/METHODS,0.3584905660377358,"We give a proper description of two types of testing sets and their approximation operators in
Section 3."
IMPLEMENTATION/METHODS,0.36163522012578614,"At each iteration ğ‘¡GeomFPOP will construct Ìƒğ‘ğ‘–ğ‘¡(with ğ‘–< ğ‘¡) from Ìƒğ‘ğ‘–
ğ‘¡âˆ’1, ğ’«ğ‘–and â„±ğ‘–(ğ‘¡) iteratively using
the two operators â‹‚Ìƒğ‘and â§µÌƒğ‘. To be specific, we define ğ‘†ğ¹
ğ‘—the j-th element of â„±ğ‘–(ğ‘¡) and ğ‘†ğ‘—
ğ‘ƒthe j-th
element of ğ’«ğ‘–, we use the following iterations:"
IMPLEMENTATION/METHODS,0.36477987421383645,"{
ğ´0 =
Ìƒğ‘ğ‘–ğ‘¡,
ğ´ğ‘—= ğ´ğ‘—âˆ’1 â‹‚
Ìƒğ‘
ğ‘†ğ¹
ğ‘—,
ğ‘—= 1, â€¦ , |â„±ğ‘–(ğ‘¡)| ,"
IMPLEMENTATION/METHODS,0.36792452830188677,"ğµ0 = ğ´|â„±ğ‘–(ğ‘¡)| ,
ğµğ‘—= ğµğ‘—âˆ’1 â§µÌƒğ‘ğ‘†ğ‘—
ğ‘ƒ,
ğ‘—= 1, â€¦ , |ğ’«ğ‘–| ,"
IMPLEMENTATION/METHODS,0.3710691823899371,"and define Ìƒğ‘ğ‘–ğ‘¡= ğµ|ğ’«ğ‘–|. Using the fourth property of Definition 2.1 and Proposition 1.1, we get that at
any time of the algorithm Ìƒğ‘ğ‘–ğ‘¡contains ğ‘ğ‘–ğ‘¡."
IMPLEMENTATION/METHODS,0.3742138364779874,"The pseudo-code of this procedure is described in Algorithm 1. The select(ğ’œ) step in Algorithm 1,
where ğ’œâŠ‚S, returns a subset of ğ’œin S. By default, select(ğ’œ) âˆ¶= ğ’œ."
IMPLEMENTATION/METHODS,0.37735849056603776,Algorithm 1 Geometric update rule of Ìƒğ‘ğ‘–ğ‘¡
IMPLEMENTATION/METHODS,0.3805031446540881,"procedure updateZone( Ìƒğ‘ğ‘–
ğ‘¡âˆ’1, ğ’«ğ‘–, â„±ğ‘–(ğ‘¡), ğ‘–< ğ‘¡)
Ìƒğ‘ğ‘–ğ‘¡â†
Ìƒğ‘ğ‘–
ğ‘¡âˆ’1
for ğ‘†âˆˆselect(â„±ğ‘–(ğ‘¡âˆ’1)) do"
IMPLEMENTATION/METHODS,0.3836477987421384,"Ìƒğ‘ğ‘–ğ‘¡â†
Ìƒğ‘ğ‘–ğ‘¡â‹‚Ìƒğ‘ğ‘†
for ğ‘†âˆˆselect(ğ’«ğ‘–) do"
IMPLEMENTATION/METHODS,0.3867924528301887,"Ìƒğ‘ğ‘–ğ‘¡â†
Ìƒğ‘ğ‘–ğ‘¡â§µÌƒğ‘ğ‘†
return Ìƒğ‘ğ‘–ğ‘¡"
IMPLEMENTATION/METHODS,0.389937106918239,"We denote the set of candidate change points at time ğ‘¡as ğœğ‘¡. Note that for any (ğ‘–âˆ’1) âˆˆğœğ‘¡the sum of |ğ’«ğ‘–|
and |â„±ğ‘–(ğ‘¡)| is |ğœğ‘¡|. With the default select procedure we do ğ’ª(ğ‘|ğœğ‘¡|) operations in the updateZone
procedure. By limiting the number of elements returned by select we can reduce the complexity of
the updateZone procedure."
IMPLEMENTATION/METHODS,0.39308176100628933,"Remark. For example, if the operator ğ’œâ†¦select(ğ’œ), regardless of |ğ’œ|, always returns a subset of
constant size, then the overall complexity of GeomFPOP is at worst âˆ‘ğ‘›
ğ‘¡=1 ğ’ª(ğ‘|ğœğ‘¡|)."
IMPLEMENTATION/METHODS,0.39622641509433965,"Using this updateZone procedure we can now informally describe the GeomFPOP algorithm. At
each iteration the algorithm will"
IMPLEMENTATION/METHODS,0.39937106918238996,"1. find the minimum value for ğ‘„ğ‘¡, ğ‘šğ‘¡and the best position for last change point Ì‚ğœğ‘¡(note that this
step is standard: as in the PELT algorithm we need to minimize the cost of the last segment
defined in equation (1));
2. compute all sets Ìƒğ‘ğ‘–ğ‘¡using Ìƒğ‘ğ‘–
ğ‘¡âˆ’1, ğ’«ğ‘–, and â„±ğ‘–(ğ‘¡) with the updateZone procedure;
3. remove changes such that Ìƒğ‘ğ‘–ğ‘¡is empty."
IMPLEMENTATION/METHODS,0.4025157232704403,"To simplify the pseudo-code of GeomFPOP, we also define the following operators:"
IMPLEMENTATION/METHODS,0.4056603773584906,"1. bestCost&Tau(ğ‘¡) operator returns two values: the minimum value of ğ‘„ğ‘¡, ğ‘šğ‘¡, and the best
position for last change point Ì‚ğœğ‘¡at time ğ‘¡(see Section 1.2);
2. getPastFutureSets(ğ‘–, ğ‘¡) operator returns a pair of sets (ğ’«ğ‘–, â„±ğ‘–(ğ‘¡)) for change point candidate
ğ‘–âˆ’1 at time ğ‘¡;
3. backtracking( Ì‚ğœ, ğ‘›) operator returns the optimal segmentation for ğ‘¦1âˆ¶ğ‘›."
IMPLEMENTATION/METHODS,0.4088050314465409,The pseudo-code of GeomFPOP is presented in Algorithm 2.
IMPLEMENTATION/METHODS,0.4119496855345912,Algorithm 2 GeomFPOP algorithm
IMPLEMENTATION/METHODS,0.41509433962264153,"procedure GeomFPOP(ğ‘¦, Î©(â‹…, â‹…), ğ›½)"
IMPLEMENTATION/METHODS,0.41823899371069184,"Ì‚ğ‘„0 â†0,
ğ‘„0(ğœƒ) â†ğ›½,
ğœ0 â†âˆ…,
{ Ìƒğ‘ğ‘–
ğ‘–}ğ‘–âˆˆ{1,â€¦,ğ‘›} â†â„ğ‘"
IMPLEMENTATION/METHODS,0.42138364779874216,"for ğ‘¡= 1, â€¦ , ğ‘›do"
IMPLEMENTATION/METHODS,0.42452830188679247,"ğ‘„ğ‘¡(ğœƒ) â†min{ğ‘„ğ‘¡âˆ’1(ğœƒ), Ì‚ğ‘„ğ‘¡âˆ’1 + ğ›½} + Î©(ğœƒ, ğ‘¦ğ‘¡)
( Ì‚ğ‘„ğ‘¡, Ì‚ğœğ‘¡) â†bestCost&Tau(ğ‘¡)
for ğ‘–âˆ’1 âˆˆğœğ‘¡âˆ’1 do"
IMPLEMENTATION/METHODS,0.4276729559748428,"(ğ’«ğ‘–, â„±ğ‘–(ğ‘¡)) â†getPastFutureSets(ğ‘–, ğ‘¡)"
IMPLEMENTATION/METHODS,0.4308176100628931,"Ìƒğ‘ğ‘–ğ‘¡â†updateZone( Ìƒğ‘ğ‘–
ğ‘¡âˆ’1, ğ’«ğ‘–, â„±ğ‘–(ğ‘¡), ğ‘–, ğ‘¡)
if Ìƒğ‘ğ‘–ğ‘¡= âˆ…then"
IMPLEMENTATION/METHODS,0.4339622641509434,"ğœğ‘¡âˆ’1 â†ğœğ‘¡âˆ’1\{ğ‘–âˆ’1}
ğœğ‘¡â†(ğœğ‘¡âˆ’1, ğ‘¡âˆ’1)
return ğ‘ğ‘(ğ‘›) â†backtracking( Ì‚ğœ= ( Ì‚ğœ1, â€¦ , Ì‚ğœğ‘›), ğ‘›)"
IMPLEMENTATION/METHODS,0.4371069182389937,"Remark. Whatever the number of elements returned by the select operator for computing Ìƒğ‘ğ‘–ğ‘¡, we
can guarantee the exactness of the GeomFPOP algorithm, since the approximate living zone (the
testing set) includes the living zone (8), as we consider less intersections and set subtractions."
IMPLEMENTATION/METHODS,0.44025157232704404,"3
Approximation Operators â‹‚Ìƒğ‘and â§µÌƒğ‘"
IMPLEMENTATION/METHODS,0.44339622641509435,"The choice of the geometric structure and the way it is constructed directly affects the computational
cost of the algorithm. We consider two types of testing set Ìƒğ‘âˆˆÌƒZ, a S-type set Ìƒğ‘†âˆˆS (see Definition 1.1)
and a hyperrectangle Ìƒğ‘…âˆˆR defined below."
IMPLEMENTATION/METHODS,0.44654088050314467,"Definition 3.1. Given two vectors in â„ğ‘, Ìƒğ‘™and Ìƒğ‘Ÿwe define the set Ìƒğ‘…, called hyperrectangle, as:"
IMPLEMENTATION/METHODS,0.449685534591195,"Ìƒğ‘…= [ Ìƒğ‘™1, Ìƒğ‘Ÿ1] Ã— â‹¯Ã— [ Ìƒğ‘™ğ‘, Ìƒğ‘Ÿğ‘] ."
IMPLEMENTATION/METHODS,0.4528301886792453,We denote the set of all possible sets Ìƒğ‘…as R.
IMPLEMENTATION/METHODS,0.4559748427672956,"To update the testing sets we need to give a strict definition of the operators â‹‚Ìƒğ‘and â§µÌƒğ‘for each
type of testing set. To facilitate the following discussion, we rename them. For the first type of
geometric structure, we rename the testing set
Ìƒğ‘as Ìƒğ‘†, the operators â‹‚Ìƒğ‘and â§µÌƒğ‘as â‹‚ğ‘†and â§µğ‘†and
Ìƒğ‘-type approximation as S-type approximation. And, likewise, we rename the testing set
Ìƒğ‘as
Ìƒğ‘…,
the operators â‹‚Ìƒğ‘and â§µÌƒğ‘as â‹‚ğ‘…and â§µğ‘…and Ìƒğ‘-type approximation as R-type approximation for the
second type of geometric structure."
IMPLEMENTATION/METHODS,0.4591194968553459,"3.1
S-type Approximation"
IMPLEMENTATION/METHODS,0.46226415094339623,"With this approach, our goal is to keep track of the fact that at time ğ‘¡= 1, â€¦ , ğ‘›there is a pair of
changes (ğ‘¢1, ğ‘¢2), with ğ‘¢1 < ğ‘–< ğ‘¢2 â‰¤ğ‘¡such that ğ‘†ğ‘–ğ‘¢2 âŠ‚ğ‘†ğ‘¢1
ğ‘–
or there is a pair of changes (ğ‘£1, ğ‘£2), with
ğ‘–< ğ‘£1 < ğ‘£2 â‰¤ğ‘¡such that ğ‘†ğ‘–ğ‘£1 â‹‚ğ‘†ğ‘–ğ‘£2 is empty. If at time ğ‘¡at least one of these conditions is met, we can
guarantee that the set Ìƒğ‘†is empty, otherwise, we propose to keep as the result of approximation the
last future S-type set ğ‘†ğ‘–ğ‘¡, because it always includes the set ğ‘ğ‘–ğ‘¡. This allows us to quickly check and
prove (if Ìƒğ‘†= âˆ…) the emptiness of set ğ‘ğ‘–ğ‘¡."
IMPLEMENTATION/METHODS,0.46540880503144655,"We consider two generic S-type sets, ğ‘†and Ìƒğ‘†from S, described as in Definition 1.1 by the functions ğ‘ 
and Ìƒğ‘ :"
IMPLEMENTATION/METHODS,0.46855345911949686,ğ‘ (ğœƒ) =
IMPLEMENTATION/METHODS,0.4716981132075472,"ğ‘
âˆ‘
ğ‘˜=1
ğ‘ ğ‘˜(ğœƒğ‘˜) âˆ’Î” ,
Ìƒğ‘ (ğœƒ) ="
IMPLEMENTATION/METHODS,0.4748427672955975,"ğ‘
âˆ‘
ğ‘˜=1
Ìƒğ‘ ğ‘˜(ğœƒğ‘˜) âˆ’ÌƒÎ” ."
IMPLEMENTATION/METHODS,0.4779874213836478,Definition 3.2. For all ğ‘†and Ìƒğ‘†in S we define the operators â‹‚ğ‘†and â§µğ‘†as:
IMPLEMENTATION/METHODS,0.4811320754716981,"Ìƒğ‘†â‹‚
ğ‘†
ğ‘†
= {
âˆ…,
if Ìƒğ‘†â‹‚ğ‘†= âˆ…,
Ìƒğ‘†,
otherwise ."
IMPLEMENTATION/METHODS,0.48427672955974843,"Ìƒğ‘†â§µğ‘†ğ‘†
= {
âˆ…,
if Ìƒğ‘†âŠ‚ğ‘†,
Ìƒğ‘†,
otherwise ."
IMPLEMENTATION/METHODS,0.48742138364779874,"As a consequence, we only need an easy way to detect any of these two geometric configurations:
Ìƒğ‘†â‹‚ğ‘†and Ìƒğ‘†âŠ‚ğ‘†."
IMPLEMENTATION/METHODS,0.49056603773584906,"In the Gaussian case, the S-type sets are ğ‘-balls and an easy solution exists based on comparing radii
(see Section 5.2 for details). In the case of other models (as Poisson or negative binomial), intersection
and inclusion tests can be performed based on a solution using separative hyperplanes and iterative
algorithms for convex problems (see Section 5.3). We propose another type of testing set solving all
types of models with the same method."
IMPLEMENTATION/METHODS,0.4937106918238994,"3.2
R-type Approximation"
IMPLEMENTATION/METHODS,0.4968553459119497,"Here, we approximate the sets ğ‘ğ‘–ğ‘¡by hyperrectangles Ìƒğ‘…ğ‘–ğ‘¡âˆˆR. A key insight of this approximation is
that given a hyperrectangle ğ‘…and an S-type set ğ‘†we can efficiently (in ğ’ª(ğ‘) using Proposition 3.2)
recover the best hyperrectangle approximation of ğ‘…âˆªğ‘†and ğ‘…â§µğ‘†. Formally we define these operators
as follows."
IMPLEMENTATION/METHODS,0.5,"Definition 3.3. For all ğ‘…, Ìƒğ‘…âˆˆR and ğ‘†âˆˆS we define the operators â‹‚ğ‘…and â§µğ‘…as:"
IMPLEMENTATION/METHODS,0.5031446540880503,"ğ‘…â‹‚
ğ‘…
ğ‘†=
â‹‚"
IMPLEMENTATION/METHODS,0.5062893081761006,"{ Ìƒğ‘…|ğ‘…â‹‚ğ‘†âŠ‚R} Ìƒğ‘…,"
IMPLEMENTATION/METHODS,0.5094339622641509,"ğ‘…â§µğ‘…ğ‘†=
â‹‚"
IMPLEMENTATION/METHODS,0.5125786163522013,{ Ìƒğ‘…|ğ‘…â§µğ‘†âŠ‚R} Ìƒğ‘….
IMPLEMENTATION/METHODS,0.5157232704402516,"We now explain how we compute these two operators. First, we note that they can be recovered by
solving 2ğ‘one-dimensional optimization problems."
IMPLEMENTATION/METHODS,0.5188679245283019,"Proposition 3.1. The ğ‘˜-th minimum coordinates Ìƒğ‘™ğ‘˜and maximum coordinates Ìƒğ‘Ÿğ‘˜of Ìƒğ‘…= ğ‘…â‹‚ğ‘…ğ‘†(resp.
Ìƒğ‘…= ğ‘…â§µğ‘…ğ‘†) is obtained as"
IMPLEMENTATION/METHODS,0.5220125786163522,"Ìƒğ‘™ğ‘˜or Ìƒğ‘Ÿğ‘˜=
â§âª
â¨âªâ©"
IMPLEMENTATION/METHODS,0.5251572327044025,"min
ğœƒğ‘˜âˆˆâ„or max
ğœƒğ‘˜âˆˆâ„ğœƒğ‘˜,"
IMPLEMENTATION/METHODS,0.5283018867924528,"subject to ğœ€ğ‘ (ğœƒ) â‰¤0 ,"
IMPLEMENTATION/METHODS,0.5314465408805031,"ğ‘™ğ‘—â‰¤ğœƒğ‘—â‰¤ğ‘Ÿğ‘—,
ğ‘—= 1, â€¦ , ğ‘, (9)"
IMPLEMENTATION/METHODS,0.5345911949685535,with ğœ€= 1 (resp. ğœ€= âˆ’1).
IMPLEMENTATION/METHODS,0.5377358490566038,"To solve the previous problems (ğœ€= 1 or âˆ’1), we define the following characteristic points."
IMPLEMENTATION/METHODS,0.5408805031446541,"Definition 3.4. Let ğ‘†âˆˆS, described by function ğ‘ (ğœƒ) = âˆ‘ğ‘
ğ‘˜=1 ğ‘ ğ‘˜(ğœƒğ‘˜) âˆ’Î” from the family of functions
(6), with ğœƒâˆˆâ„ğ‘. We define the minimal point c âˆˆâ„ğ‘of ğ‘†as:"
IMPLEMENTATION/METHODS,0.5440251572327044,"c = {cğ‘˜}ğ‘˜=1,â€¦,ğ‘,
with
cğ‘˜= ğ´ğ‘Ÿğ‘”min
ğœƒğ‘˜âˆˆâ„
{ğ‘ ğ‘˜(ğœƒğ‘˜)} .
(10)"
IMPLEMENTATION/METHODS,0.5471698113207547,"Moreover, with ğ‘…âˆˆR defined through vectors ğ‘™, ğ‘Ÿâˆˆâ„ğ‘, we define two points of ğ‘…, the closest point
m âˆˆâ„ğ‘and the farthest point M âˆˆâ„ğ‘relative to ğ‘†as"
IMPLEMENTATION/METHODS,0.550314465408805,"m = {mğ‘˜}ğ‘˜=1,â€¦,ğ‘,
with
mğ‘˜= ğ´ğ‘Ÿğ‘”min
ğ‘™ğ‘˜â‰¤ğœƒğ‘˜â‰¤ğ‘Ÿğ‘˜{ğ‘ ğ‘˜(ğœƒğ‘˜)} ,"
IMPLEMENTATION/METHODS,0.5534591194968553,"M = {Mğ‘˜}ğ‘˜=1,â€¦,ğ‘,
with
Mğ‘˜= ğ´ğ‘Ÿğ‘”max
ğ‘™ğ‘˜â‰¤ğœƒğ‘˜â‰¤ğ‘Ÿğ‘˜{ğ‘ ğ‘˜(ğœƒğ‘˜)} ."
IMPLEMENTATION/METHODS,0.5566037735849056,"Remark. In the Gaussian case, ğ‘†is a ball in â„ğ‘and"
IMPLEMENTATION/METHODS,0.559748427672956,"â€¢ c is the center of the ball;
â€¢ m is the closest point to c inside ğ‘…;
â€¢ M is the farthest point to c in ğ‘…."
IMPLEMENTATION/METHODS,0.5628930817610063,"Figure 4: Three examples of minimal point c, closest point m and farthest point M for bivariate
Gaussian case: (a) ğ‘…âŠ‚ğ‘†; (b) ğ‘…â‹‚ğ‘†â‰ âˆ…; (c) ğ‘…â‹‚ğ‘†= âˆ…."
IMPLEMENTATION/METHODS,0.5660377358490566,"Proposition 3.2. Let Ìƒğ‘…= ğ‘…â‹‚ğ‘…ğ‘†(resp. ğ‘…â§µğ‘…ğ‘†), with ğ‘…âˆˆR and ğ‘†âˆˆS. We compute the boundaries
( Ìƒğ‘™, Ìƒğ‘Ÿ) of Ìƒğ‘…using the following rule:"
IMPLEMENTATION/METHODS,0.5691823899371069,"1. We define the point Ìƒğœƒâˆˆâ„ğ‘as the closest point m (resp. farthest M). For all ğ‘˜= 1, â€¦ ğ‘we find the
roots ğœƒğ‘˜1 and ğœƒğ‘˜2 of the one-variable (ğœƒğ‘˜) equation"
IMPLEMENTATION/METHODS,0.5723270440251572,"ğ‘ ğ‘˜(ğœƒğ‘˜) + âˆ‘
ğ‘—â‰ ğ‘˜
ğ‘ ğ‘—( Ìƒğœƒğ‘—) âˆ’Î” = 0 ."
IMPLEMENTATION/METHODS,0.5754716981132075,"If the roots are real-valued we consider that ğœƒğ‘˜1 â‰¤ğœƒğ‘˜2, otherwise we write [ğœƒğ‘˜1, ğœƒğ‘˜2] = âˆ…."
IMPLEMENTATION/METHODS,0.5786163522012578,2. We compute the boundary values Ìƒğ‘™ğ‘˜and Ìƒğ‘Ÿğ‘˜of Ìƒğ‘…as:
IMPLEMENTATION/METHODS,0.5817610062893082,"â€¢ For ğ‘…â‹‚ğ‘…ğ‘†(ğ‘˜= 1, â€¦ , ğ‘):"
IMPLEMENTATION/METHODS,0.5849056603773585,"[ Ìƒğ‘™ğ‘˜, Ìƒğ‘Ÿğ‘˜] = [ğœƒğ‘˜1, ğœƒğ‘˜2] â‹‚[ğ‘™ğ‘˜, ğ‘Ÿğ‘˜] .
(11)"
IMPLEMENTATION/METHODS,0.5880503144654088,"â€¢ For ğ‘…â§µğ‘…ğ‘†(ğ‘˜= 1, â€¦ , ğ‘):"
IMPLEMENTATION/METHODS,0.5911949685534591,"[ Ìƒğ‘™ğ‘˜, Ìƒğ‘Ÿğ‘˜] = {
[ğ‘™ğ‘˜, ğ‘Ÿğ‘˜] â§µ[ğœƒğ‘˜1, ğœƒğ‘˜2] ,
if
[ğœƒğ‘˜1, ğœƒğ‘˜2] âŠ„[ğ‘™ğ‘˜, ğ‘Ÿğ‘˜] ,"
IMPLEMENTATION/METHODS,0.5943396226415094,"[ğ‘™ğ‘˜, ğ‘Ÿğ‘˜] ,
otherwise ."
IMPLEMENTATION/METHODS,0.5974842767295597,"If there is a dimension ğ‘˜for which [ Ìƒğ‘™ğ‘˜, Ìƒğ‘Ÿğ‘˜] = âˆ…, then the set Ìƒğ‘…is empty."
IMPLEMENTATION/METHODS,0.60062893081761,The proof of Proposition 3.2 is presented in Section 5.4.
IMPLEMENTATION/METHODS,0.6037735849056604,"As a partial conclusion to this theoretical study, those ideas could be extended to some other models
with missing values or dependencies between dimensions (e.g. piece-wise constant regression).
However, it would require introducing new approximation operators of potential high complexity."
RESULTS/EXPERIMENTS,0.6069182389937107,"4
Simulation Study of GeomFPOP"
RESULTS/EXPERIMENTS,0.610062893081761,"In this section, we study the efficiency of GeomFPOP using simulations of multivariate independent
time series. For this, we implemented GeomFPOP (with S and R types) and PELT for the Multivariate
Independent Gaussian Model in the R-package â€˜GeomFPOPâ€™ https://github.com/lpishchagina/Geom
FPOP written in R/C++. By default, the value of penalty ğ›½for each simulation was defined by the
Schwarz Information Criterion proposed in (Yao 1984) as ğ›½= 2ğ‘ğœ2 log ğ‘›with ğœ= 1 known. As long
as the per-dimension variance is known (or appropriately estimated) we can make this assumption
(ğœ= 1 known) without loss of generality by rescaling the data by the standard deviation."
RESULTS/EXPERIMENTS,0.6132075471698113,"Overview of our simulations. First, for 2 â‰¤ğ‘â‰¤10 we generated ğ‘-variate independent time series
(multivariate independent Gaussian model with fixed variance) with ğ‘›= 104 data points and number
of segments: 1, 5, 10, 50 and 100. The segment-specific parameter (mean) was set to 1 for even
segments, and 0 for odd segments. As a quality control measure, we verified that PELT and GeomFPOP
produced identical outputs on these simulated profiles. Second, we studied cases where the PELT
approach is not efficient, that is when the data has no or few changes relative to ğ‘›. Indeed, it was
shown in (Killick, Fearnhead, and Eckley 2012) and (Maidstone et al. 2017) that the run time of
PELT is close to ğ’ª(ğ‘›2) in such cases. So we considered simulations of multivariate time series
without change (only one segment). By these simulations we evaluated the pruning efficiency of
GeomFPOP (using S and R types) for dimension 2 â‰¤ğ‘â‰¤10 (see Figure 5 in Section 4.1). For small
dimensions we also evaluated the run time of GeomFPOP and PELT and compare them (see Figure 6
in Section 4.2). In addition, we considered another approximation of the ğ‘ğ‘–ğ‘¡where we applied our
â‹‚ğ‘…and â§µğ‘…operators only for a randomly selected subset of the past and future balls. In practice,
this strategy turned out to be faster computationally than the full/original GeomFPOP and PELT
(see Figure 7 in Section 4.3). For this strategy we also generated time series of a fixed size (106 data
points) and varying number of segments and evaluated how the run time vary with the number of
segments for small dimensions (2 â‰¤ğ‘â‰¤4). Our empirical results confirmed that the GeomFPOP
(R-type: random/random) approach is computationally comparable to PELT when the number of
changes is large (see Figure 9 in Section 4.5)."
RESULTS/EXPERIMENTS,0.6163522012578616,"4.1
The Number of Change Point Candidates stored over Time"
RESULTS/EXPERIMENTS,0.6194968553459119,"We evaluate the functional pruning efficiency of the GeomFPOP method using ğ‘-variate independent
Gaussian noise of length ğ‘›= 104 data points. For such series, PELT typically does not pruned (e.g. for
ğ‘¡= 104, ğ‘= 2 it stores almost always ğ‘¡candidates)."
RESULTS/EXPERIMENTS,0.6226415094339622,"We report in Figure 5 the percentage of candidates that are kept by GeomFPOP as a function of ğ‘›, ğ‘
and the type of pruning (R or S). Regardless of the type of approximation and contrary to PELT, we"
RESULTS/EXPERIMENTS,0.6257861635220126,"observe that there is some pruning. However when increasing the dimension ğ‘, the quality of the
pruning decreases."
RESULTS/EXPERIMENTS,0.6289308176100629,"Comparing the left plot of Figure 5 with the right plot we see that for dimensions ğ‘= 2 to ğ‘= 5
R-type prunes more than the S-type, while for larger dimensions the S-type prunes more than the
R-type. For example, for ğ‘= 2 at time ğ‘¡= 104 by GeomFPOP (R-type) the number of candidates
stored over ğ‘¡does not exceed 1% versus 3% by GeomFPOP (S-type). This intuitively makes sense.
One the one hand, the R-type approximation of a sphere deteriorates as the dimension increases. On
the other hand with R-type approximation every new approximation is included in the previous one.
For small dimensions this memory effect outweighs the roughness of the approximation."
RESULTS/EXPERIMENTS,0.6320754716981132,"Figure 5: Percentage of candidate change points stored over time by GeomFPOP with R (left) or S
(right) type pruning for dimension ğ‘= 2, â€¦ , 10. Averaged over 100 data sets."
RESULTS/EXPERIMENTS,0.6352201257861635,"Based on these results we expect that R-type pruning GeomFPOP will be more efficient than S-type
pruning for small dimensions."
RESULTS/EXPERIMENTS,0.6383647798742138,"4.2
Empirical Time Complexity of GeomFPOP"
RESULTS/EXPERIMENTS,0.6415094339622641,"We studied the run time of GeomFPOP (S and R-type) and compared it to PELT for small dimensions.
We simulated data generated by a ğ‘-variate i.i.d. Gaussian noise and saved their run times with a three
minutes limit. The results are presented in Figure 6. We observe that GeomFPOP is faster than PELT
only for ğ‘= 2. For ğ‘= 3 run times are comparable and for ğ‘= 4 GeomFPOP is slower. This is not in
line with the fact that GeomFPOP prunes more than PELT. However, as explained in Section 2.1, the
computational complexity of GeomFPOP and PELT is affected by both the efficiency of pruning and
the number of comparisons conducted at each step. For PELT at time ğ‘¡, all candidates are compared
to the last change, resulting in a complexity of order ğ’ª(ğ‘|ğœğ‘ƒğ¸ğ¿ğ‘‡
ğ‘¡
|). On the other hand, GeomFPOP
compares all candidates to each other (refer to Algorithm 1 and the remark from Section 2.1), leading
to a complexity of order ğ’ª(ğ‘|ğœğºğ‘’ğ‘œğ‘šğ¹ğ‘ƒğ‘‚ğ‘ƒ
ğ‘¡
|2). In essence, the complexity of GeomFPOP is governed
by the square of the number of candidates. Therefore, GeomFPOP is expected to be more efficient
than PELT only if its square number of candidates is smaller than the number of candidates for
PELT. Based on the information presented in} Figure 6, we argue that this condition holds true only
for dimensions ğ‘= 2 and 3. Indeed, analysis of the number of comparisons between PELT and
GeomFPOP (see Section 5.6) supports this claim, revealing that GeomFPOP (S-type) outperforms
PELT only when ğ‘â‰¤2 and GeomFPOP (R-type) outperforms PELT only when ğ‘â‰¤3} (see Figure 12
in Section 5.6). This leads us to consider a randomized version of GeomFPOP."
RESULTS/EXPERIMENTS,0.6446540880503144,"Figure 6: Run time of GeomFPOP (S and R types) and PELT using multivariate time series without
change points. The maximum run time of the algorithms is 3 minutes. Averaged over 100 data sets."
RESULTS/EXPERIMENTS,0.6477987421383647,"4.3
Empirical Time Complexity of a Randomized GeomFPOP"
RESULTS/EXPERIMENTS,0.6509433962264151,"R-type GeomFPOP is designed in such a way that at each iteration we need to consider all past and
future spheres of change ğ‘–. In practice, it is often sufficient to consider just a few of them to get an
empty set. Having this in mind, we propose a further approximation of the ğ‘ğ‘–ğ‘¡where we apply our
â‹‚ğ‘…and â§µğ‘…operators only for a randomly selected subset of the past and future sets. In detail, we
propose to redefine the output of the select() function in Algorithm 1 for any sets ğ’«ğ‘–and â„±ğ‘–(ğ‘¡) as:"
RESULTS/EXPERIMENTS,0.6540880503144654,"â€¢ select(ğ’«ğ‘–) returns one random set from ğ’«ğ‘–.
â€¢ select(â„±ğ‘–(ğ‘¡)) returns the last set ğ‘†ğ‘–ğ‘¡and one random set from â„±ğ‘–(ğ‘¡)."
RESULTS/EXPERIMENTS,0.6572327044025157,"Thus, we consider the following geometric update rule:"
RESULTS/EXPERIMENTS,0.660377358490566,â€¢ (random/random) At time ğ‘¡we update hyperrectangle:
RESULTS/EXPERIMENTS,0.6635220125786163,"1. by only two intersection operations: one with the last S-type set ğ‘†ğ‘–ğ‘¡from â„±ğ‘–(ğ‘¡), and one
with a random S-type set from â„±ğ‘–(ğ‘¡);
2. by only one exclusion operation with a random S-type set from ğ’«ğ‘–."
RESULTS/EXPERIMENTS,0.6666666666666666,"In this approach, at time ğ‘¡we need no more than three operations to update the testing set Ìƒğ‘ğ‘–ğ‘¡for
each (ğ‘–âˆ’1) âˆˆğœğ‘¡. As can be seen in Figure Figure 11 of Section 5.5, by making less comparisons, we
prune less change points than in the general GeomFPOP (R-type) case, but still more than PELT.
It is important to note that in this randomization, we compare each change point candidate with
only two other change point candidates (rather than all in the general case of GeomFPOP (R-type)).
Therefore, informally our complexity at time step ğ‘¡is only ğ’ª(ğ‘|ğœğºğ‘’ğ‘œğ‘šğ¹ğ‘ƒğ‘‚ğ‘ƒ(random/random)
ğ‘¡
|). According
to the remark from Section 2.1 and the discussion in Section 4.2, even with large values of ğ‘, the
overall complexity of GeomFPOP should not be worse than that of PELT. We investigated other
randomized strategies (see Section 5.5) but this simple one was sufficient to significantly improve
run times. The run time of our optimization approach and PELT in dimension (ğ‘= 2, â€¦ , 10, 100) are
presented in Figure 7. As in Section 4.2, run times were limited to three minutes and were recorded
for simulations of length ranging ğ‘›from 210 to 223 data points (ğ‘-variate i.i.d. Gaussian noise)."
RESULTS/EXPERIMENTS,0.6698113207547169,"Although the (random/random) approach reduces the quality of pruning (see Section 5.5), it gives a
significant gain in run time compared to PELT in small dimensions. To be specific, with a run time of
five minutes GeomFPOP, on average, processes a time series with a length of about 8 Ã— 106, 106 and
2, 5 Ã— 105 data points in the dimensions ğ‘= 2, 3 and 4, respectively. At the same time, PELT manages
to process time series with a length of at most 6, 5 Ã— 104 data points in these dimensions."
RESULTS/EXPERIMENTS,0.6729559748427673,"Figure 7: Run time of the (random/random) approach of { GeomFPOP} (R-type) and PELT using
p-variate time series without change points (ğ‘= 2, â€¦ , 10, 100). The maximum run time of the
algorithms is 3 minutes. Averaged over 100 data sets."
RESULTS/EXPERIMENTS,0.6761006289308176,"4.4
Empirical Complexity of the Algorithm as a Function of ğ‘"
RESULTS/EXPERIMENTS,0.6792452830188679,"We also evaluate the slope coefficient ğ›¼of the run time curve of GeomFPOP with random sampling
of the past and future candidates for all considered dimensions. In Figure 8 we can see that already
for ğ‘â‰¥7 ğ›¼is close to 2."
RESULTS/EXPERIMENTS,0.6823899371069182,"Figure 8: Run time dependence of (random/random) approach of GeomFPOP (R-type) on dimension
ğ‘."
RESULTS/EXPERIMENTS,0.6855345911949685,"4.5
Run Time as a Function of the Number of Segments"
RESULTS/EXPERIMENTS,0.6886792452830188,"For small dimensions we also generated time series with ğ‘›= 106 data points with increasing number
of segments. We have considered the following number of segments: (1, 2, 5) Ã— 10ğ‘–(for ğ‘–= 0, â€¦ , 3)
and 104. The mean was equal to 1 for even segments, and 0 for odd segments. In Figure 9 we can
see the run time dependence of the (random/random) approach of GeomFPOP (R-type) and PELT on
the number of segments for this type of time series. For smaller number of segments (the threshold
between small and large numbers of segments is around 5 Ã— 103 for all considered dimensions ğ‘)
GeomFPOP (random/random) is an order of magnitude faster. But for large number of segments, it
can be seen that the run times (both PELT and GeomFPOP) are larger. This might be a bit counter-"
RESULTS/EXPERIMENTS,0.6918238993710691,"intuitive. However, it is essential to recall that a similar trend of increased run time for a large number
of segments was already noted in the one-dimensional case, as demonstrated in (Maidstone et al.
2017). This observation is explained as follows. When the number of segments becomes excessively
large, the algorithm (both PELT and GeomFPOP) tends to interpret this abundance as an indication
of no change, resulting in reduced pruning. As a conclusion of this simulation study, in Section 5.7
we make a similar analysis, but using time series in which changes are present only in a subset of
dimensions. We observe that in this case GeomFPOP (random/random) will be slightly less effective
but no worse than no change (see Figure 13)."
RESULTS/EXPERIMENTS,0.6949685534591195,"Figure 9: Run time dependence of (random/random) approach of GeomFPOP (R-type) on the number
of segments in time series with 106 data points."
OTHER,0.6981132075471698,Acknowledgments
OTHER,0.7012578616352201,We thank Paul Fearnhead for fruitful discussions.
OTHER,0.7044025157232704,"5
Supplements"
OTHER,0.7075471698113207,"5.1
Examples of Likelihood-Based Cost Functions"
OTHER,0.710691823899371,"We define a cost function for segmentation as in Equation 1 by the function Î©(â‹…, â‹…) (the opposite log-
likelihood (times two)). Below is the expression of this function linked to data point ğ‘¦ğ‘–= (ğ‘¦1
ğ‘–, â€¦ , ğ‘¦ğ‘
ğ‘–)
in â„ğ‘for three examples of parametric multivariate models:"
OTHER,0.7138364779874213,"Î©(ğœƒ, ğ‘¦ğ‘–) = â§
âªâªâª"
OTHER,0.7169811320754716,"â¨
âªâªâª
â©"
OTHER,0.720125786163522,"ğ‘
âˆ‘
ğ‘˜=1
(ğ‘¦ğ‘˜
ğ‘–âˆ’ğœƒğ‘˜)2 ,
if ğ‘¦ğ‘–âˆ¼ğ’©ğ‘(ğœƒ, ğœ2ğ•€ğ‘) , 2"
OTHER,0.7232704402515723,"ğ‘
âˆ‘
ğ‘˜=1
{ğœƒğ‘˜âˆ’log ((ğœƒğ‘˜)ğ‘¦ğ‘˜
ğ‘–"
OTHER,0.7264150943396226,"ğ‘¦ğ‘˜
ğ‘–!
)} ,
if ğ‘¦ğ‘–âˆ¼ğ’«(ğœƒ) , âˆ’2"
OTHER,0.7295597484276729,"ğ‘
âˆ‘
ğ‘˜=1
log ((ğœƒğ‘˜)ğ‘¦ğ‘˜
ğ‘–(1 âˆ’ğœƒğ‘˜)ğœ™(ğ‘¦ğ‘˜
ğ‘–+ ğœ™âˆ’1
ğ‘¦ğ‘˜
ğ‘–
)) ,
if ğ‘¦ğ‘–âˆ¼ğ’©â„¬(ğœƒ, ğœ™) . (12)"
OTHER,0.7327044025157232,"We suppose that the over-dispersion parameter ğœ™of the multivariate negative binomial distribution
is known."
OTHER,0.7358490566037735,"5.2
Intersection and Inclusion of Two p-balls"
OTHER,0.7389937106918238,"We define two ğ‘-balls, ğ‘†and ğ‘†â€² in â„ğ‘using their centers ğ‘, ğ‘â€² âˆˆâ„ğ‘and radius ğ‘…, ğ‘…â€² âˆˆâ„+ as"
OTHER,0.7421383647798742,"ğ‘†= {ğ‘¥âˆˆâ„ğ‘, ||ğ‘¥âˆ’ğ‘||2 â‰¤ğ‘…2} and ğ‘†â€² = {ğ‘¥âˆˆâ„ğ‘, ||ğ‘¥âˆ’ğ‘â€²||2 â‰¤ğ‘…â€²2},"
OTHER,0.7452830188679245,"where ||ğ‘¥âˆ’ğ‘||2 = âˆ‘ğ‘
ğ‘˜=1(ğ‘¥ğ‘˜âˆ’ğ‘ğ‘˜)2, with ğ‘¥= (ğ‘¥1, ..., ğ‘¥ğ‘) âˆˆâ„ğ‘, is the Euclidean norm. The distance
between centers ğ‘and ğ‘â€² is defined as ğ‘‘(ğ‘, ğ‘â€²) = âˆš||ğ‘âˆ’ğ‘â€²||2. We have the following simple results:"
OTHER,0.7484276729559748,"ğ‘†âˆ©ğ‘†â€² = âˆ…âŸºğ‘‘(ğ‘, ğ‘â€²) > ğ‘…+ ğ‘…â€² ,"
OTHER,0.7515723270440252,"ğ‘†âŠ‚ğ‘†â€² or ğ‘†â€² âŠ‚ğ‘†âŸºğ‘‘(ğ‘, ğ‘â€²) â‰¤|ğ‘…âˆ’ğ‘…â€²| ."
OTHER,0.7547169811320755,"5.3
Intersection and Inclusion Tests"
OTHER,0.7578616352201258,"Remark. For any ğ‘†ğ‘–
ğ‘—âˆˆS its associated function ğ‘ can be redefine after normalization by constant
ğ‘—âˆ’ğ‘–+ 1 as:"
OTHER,0.7610062893081762,"ğ‘ (ğœƒ) = ğ‘(ğœƒ) + âŸ¨ğ‘, ğœƒâŸ©+ ğ‘,"
OTHER,0.7641509433962265,"with ğ‘(â‹…) is some convex function depending on ğœƒ, ğ‘= {ğ‘ğ‘˜}ğ‘˜=1,â€¦,ğ‘âˆˆâ„ğ‘and ğ‘âˆˆâ„."
OTHER,0.7672955974842768,"For example, in the Gaussian case, the elements have the following form:"
OTHER,0.7704402515723271,"ğ‘âˆ¶ğœƒâ†¦ğœƒ2 ,
ğ‘ğ‘˜= 2 Ì„ğ‘Œğ‘˜
ğ‘–âˆ¶ğ‘—,
ğ‘= Ì„ğ‘Œ2
ğ‘–âˆ¶ğ‘—âˆ’Î”ğ‘–ğ‘—,"
OTHER,0.7735849056603774,"where Ì„ğ‘Œğ‘˜
ğ‘–âˆ¶ğ‘—=
1
ğ‘—âˆ’ğ‘–+1 âˆ‘ğ‘—
ğ‘¢=ğ‘–+1 ğ‘¦ğ‘˜ğ‘¢and Ì„ğ‘Œ2
ğ‘–âˆ¶ğ‘—=
1
ğ‘—âˆ’ğ‘–+1 âˆ‘ğ‘—
ğ‘¢=ğ‘–+1 âˆ‘ğ‘
ğ‘˜=1(ğ‘¦ğ‘˜ğ‘¢)2."
OTHER,0.7767295597484277,"Definition 5.1. For all ğœƒâˆˆâ„ğ‘and ğ‘†1, ğ‘†2 âˆˆS with their associated functions, ğ‘ 1 and ğ‘ 2, we define a
function â„12 and a hyperplane ğ»12 as:"
OTHER,0.779874213836478,"â„12(ğœƒ) âˆ¶= ğ‘ 2(ğœƒ) âˆ’ğ‘ 1(ğœƒ) ,
ğ»12 âˆ¶= {ğœƒâˆˆâ„ğ‘|â„12(ğœƒ) = 0} ."
OTHER,0.7830188679245284,"We denote by ğ»+
12 âˆ¶= {ğœƒâˆˆâ„ğ‘|â„12(ğœƒ) > 0} and ğ»âˆ’
12 âˆ¶= {ğœƒâˆˆâ„ğ‘|â„12(ğœƒ) < 0} the positive and negative
half-spaces of ğ»12, respectively. We call H the set of hyperplanes."
OTHER,0.7861635220125787,For all ğ‘†âˆˆS and ğ»âˆˆH we introduce a half âˆ’space operator.
OTHER,0.789308176100629,Definition 5.2. The operator half âˆ’space is such that:
OTHER,0.7924528301886793,"1. the left input is an S-type set ğ‘†;
2. the right input is a hyperplane ğ»;
3. the output is the half-spaces of ğ», such that ğ‘†lies in those half-spaces."
OTHER,0.7955974842767296,"Definition 5.3. We define the output of half âˆ’space(ğ‘†, ğ») by the following rule:"
OTHER,0.7987421383647799,"1. We find two points, ğœƒ1, ğœƒ2 âˆˆâ„ğ‘, as: â§âª â¨âª
â©"
OTHER,0.8018867924528302,"ğœƒ1 =
ğ´ğ‘Ÿğ‘”min ğ‘ (ğœƒ),"
OTHER,0.8050314465408805,"ğœƒ2 =
{
ğ´ğ‘Ÿğ‘”min
ğœƒâˆˆğ‘†â„(ğœƒ),
if ğœƒ1 âˆˆğ»+,"
OTHER,0.8081761006289309,"ğ´ğ‘Ÿğ‘”max
ğœƒâˆˆğ‘†â„(ğœƒ),
if ğœƒ1 âˆˆğ»âˆ’."
OTHER,0.8113207547169812,2. We have:
OTHER,0.8144654088050315,"half âˆ’space(ğ‘†, ğ») =
â§ â¨
â©"
OTHER,0.8176100628930818,"{ğ»+},
if ğœƒ1, ğœƒ2 âˆˆğ»+,"
OTHER,0.8207547169811321,"{ğ»âˆ’},
if ğœƒ1, ğœƒ2 âˆˆğ»âˆ’,"
OTHER,0.8238993710691824,"{ğ»+, ğ»âˆ’},
otherwise."
OTHER,0.8270440251572327,"Lemma 5.1. ğ‘†1 âŠ‚ğ»âˆ’
12 â‡”ğœ•ğ‘†1 âŠ‚ğ»âˆ’
12, where ğœ•(â‹…) denote the frontier operator."
OTHER,0.8301886792452831,The proof of Lemma 5.1 follows from the convexity of ğ‘†1.
OTHER,0.8333333333333334,"Lemma 5.2. ğ‘†1 âŠ‚ğ‘†2 (resp. ğ‘†2 âŠ‚ğ‘†1) â‡”ğ‘†1, ğ‘†2 âŠ‚ğ»âˆ’
12 (resp. ğ‘†1, ğ‘†2 âŠ‚ğ»+
12)."
OTHER,0.8364779874213837,"Proof. We have the hypothesis â„‹0 âˆ¶{ğ‘†1 âŠ‚ğ‘†2}, then"
OTHER,0.839622641509434,"âˆ€ğœƒâˆˆğœ•ğ‘†1
{ğ‘ 1(ğœƒ) = 0,
[by Definition 1.1]"
OTHER,0.8427672955974843,"ğ‘ 2(ğœƒ) â‰¤0,
[by â„‹0]
â‡’ğœƒâˆˆğ»âˆ’
12
â‡’ğœ•ğ‘†1 âŠ‚ğ»âˆ’
12."
OTHER,0.8459119496855346,"Thus, according to Lemma 5.1, ğ‘†1 âŠ‚ğ»âˆ’
12."
OTHER,0.8490566037735849,"We have now the hypothesis â„‹0 âˆ¶{ğ‘†1, ğ‘†2 âŠ‚ğ»âˆ’
12}, then"
OTHER,0.8522012578616353,"âˆ€ğœƒâˆˆğ‘†1
{ ğ‘ 1(ğœƒ) â‰¤0,
[by Definition 1.1]"
OTHER,0.8553459119496856,"â„12(ğœƒ) < 0,
[by â„‹0, Definitions 5.1 and 1.1]
â‡’ğœƒâˆˆğ‘†2
â‡’ğ‘†1 âŠ‚ğ‘†2."
OTHER,0.8584905660377359,"Similarly, it is easy to show that ğ‘†2 âŠ‚ğ‘†1 â‡”ğ‘†1, ğ‘†2 âŠ‚ğ»+
12."
OTHER,0.8616352201257862,Lemma 5.3. ğ‘†1 âˆ©ğ‘†2 = âˆ…â‡”ğ»12 is a separating hyperplane of ğ‘†1 and ğ‘†2.
OTHER,0.8647798742138365,"Proof. We have the hypothesis â„‹0 âˆ¶{ğ‘†1 âŠ‚ğ»+
12, ğ‘†2 âŠ‚ğ»âˆ’
12}. Thus, ğ»12 is a separating hyperplane of
ğ‘†1 and ğ‘†2 then, according to its definition, ğ‘†1 âˆ©ğ‘†2 = âˆ…."
OTHER,0.8679245283018868,We have now the hypothesis â„‹0 âˆ¶{ğ‘†1 âˆ©ğ‘†2 = âˆ…} then
OTHER,0.8710691823899371,"âˆ€ğœƒâˆˆğ‘†1
{ğ‘ 1(ğœƒ) â‰¤0,
[by Definition 1.1]"
OTHER,0.8742138364779874,"ğ‘ 2(ğœƒ) > 0,
[by â„‹0, Definition 1.1]
â‡’ğœƒâˆˆğ»+
12."
OTHER,0.8773584905660378,"âˆ€ğœƒâˆˆğ‘†2
{ğ‘ 1(ğœƒ) > 0,
[by â„‹0, Definition 1.1]"
OTHER,0.8805031446540881,"ğ‘ 2(ğœƒ) â‰¤0,
[by Definition 1.1]
â‡’ğœƒâˆˆğ»âˆ’
12."
OTHER,0.8836477987421384,"Consequently, ğ»12 is a separating hyperplane of ğ‘†1 and ğ‘†2."
OTHER,0.8867924528301887,"Proposition 5.1. To detect set inclusion ğ‘†1 âŠ‚ğ‘†2 and emptiness of set intersection ğ‘†1 âˆ©ğ‘†2, it is necessary:"
OTHER,0.889937106918239,"1. build the hyperplane ğ»12;
2. apply the half âˆ’space operator for couples (ğ‘†1, ğ»12) and (ğ‘†2, ğ»12) to know in which half-space(s)
ğ‘†1 and ğ‘†2 are located;
3. check the conditions in Lemmas 5.2 and 5.3."
OTHER,0.8930817610062893,"5.4
Proof of Proposition 3.2"
OTHER,0.8962264150943396,"Proof. Let c = {cğ‘˜}ğ‘˜=1,â€¦,ğ‘is the minimal point of ğ‘†, defined as in Equation 10. In the intersection case,
we consider solving the optimization problem (9) for the boundaries Ìƒğ‘™ğ‘˜and Ìƒğ‘Ÿğ‘˜, removing constraint
ğ‘™ğ‘˜â‰¤ğœƒğ‘˜â‰¤ğ‘Ÿğ‘˜. If ğ‘…intersects ğ‘†, the optimal solution ğœƒğ‘˜belongs to the boundary of ğ‘†due to our simple
(axis-aligned rectangular) inequality constraints and we get"
OTHER,0.89937106918239,"ğ‘ ğ‘˜(ğœƒğ‘˜) = âˆ’âˆ‘
ğ‘—â‰ ğ‘˜
ğ‘ ğ‘—(ğœƒğ‘—) + Î” .
(13)"
OTHER,0.9025157232704403,We are looking for minimum and maximum values in ğœƒğ‘˜for this equation with constraints ğ‘™ğ‘—â‰¤ğœƒğ‘—â‰¤ğ‘Ÿğ‘—
OTHER,0.9056603773584906,"(ğ‘—â‰ ğ‘˜). Using the convexity of ğ‘ ğ‘˜and ğ‘ ğ‘—, we need to maximize the quantity in the right-hand side.
Thus, the solution Ìƒğœƒğ‘—for each ğœƒğ‘—is the minimal value of âˆ‘ğ‘—â‰ ğ‘˜ğ‘ ğ‘—(ğœƒğ‘—) under constraint ğ‘™ğ‘—â‰¤ğœƒğ‘—â‰¤ğ‘Ÿğ‘—and
the result can only be ğ‘™ğ‘—, ğ‘Ÿğ‘—or cğ‘—. Looking at all coordinates at the same time, the values for Ìƒğœƒâˆˆâ„ğ‘"
OTHER,0.9088050314465409,"corresponds to the closest point m = {mğ‘˜}ğ‘˜=1,â€¦,ğ‘. Having found ğœƒğ‘˜1 and ğœƒğ‘˜2 using Ìƒğœƒthe result in
Equation 11 is obvious considering current boundaries ğ‘™ğ‘˜and ğ‘Ÿğ‘˜."
OTHER,0.9119496855345912,"In exclusion case, we remove from ğ‘…the biggest possible rectangle included into ğ‘†âˆ©{ğ‘™ğ‘—â‰¤ğœƒğ‘—â‰¤ğ‘Ÿğ‘—, ğ‘—â‰ ğ‘˜},
which correspond to minimizing the right hand side of Equation 13, that is maximizing âˆ‘ğ‘—â‰ ğ‘˜ğ‘ ğ‘—(ğœƒğ‘—)
under constraint ğ‘™ğ‘—â‰¤ğœƒğ‘—â‰¤ğ‘Ÿğ‘—(ğ‘—â‰ ğ‘˜). In that case, the values for Ìƒğœƒcorrespond to the greatest value
returned by âˆ‘ğ‘—â‰ ğ‘˜ğ‘ ğ‘—(ğœƒğ‘—) on interval boundaries. With convex functions ğ‘ ğ‘—, it corresponds to the
farthest point M = {Mğ‘˜}ğ‘˜=1,â€¦,ğ‘."
OTHER,0.9150943396226415,"5.5
Optimization Strategies for GeomFPOP (R-type)"
OTHER,0.9182389937106918,"In GeomFPOP(R-type) at each iteration, we need to consider all past and future spheres of change ğ‘–.
As it was said in Section 4, in practice it is often sufficient to consider just a few of them to get an
empty set. Thus, we propose to limit the number of operations âˆ©ğ‘…no more than two:"
OTHER,0.9213836477987422,"â€¢ last. At time ğ‘¡we update hyperrectangle by only one operation, this is an intersection with
the last S-type set ğ‘†ğ‘–ğ‘¡from â„±ğ‘–(ğ‘¡).
â€¢ random. At time ğ‘¡we update the hyperrectangle by only two operations. First, this is an
intersection with the last S-type set ğ‘†ğ‘–ğ‘¡from â„±ğ‘–(ğ‘¡), and second, this is an intersection with
other random S-type set from â„±ğ‘–(ğ‘¡)."
OTHER,0.9245283018867925,The number of operations â§µğ‘…we limit no more than one:
OTHER,0.9276729559748428,"â€¢ empty. At time ğ‘¡we do not perform â§µğ‘…operations.
â€¢ random. At time ğ‘¡we update hyperrectangle by only one operation: exclusion with a random
S-type set from ğ’«ğ‘–."
OTHER,0.9308176100628931,"According to these notations, the approach presented in the original GeomFPOP (R-type) has the
form (all/all). We show the impact of introduced limits on the number of change point candidates
retained over time and evaluate their run times. The results are presented in Figures 10 and 11."
OTHER,0.9339622641509434,"Even though the (random/random) approach reduces the quality of pruning in dimensions ğ‘= 2, 3
and 4, it gives a significant gain in the run time compared to the original GeomFPOP (R-type) and is
at least comparable to the (last/random) approach."
OTHER,0.9371069182389937,"Figure 10: Ratio number of candidate change point over time by different optimization approaches of
GeomFPOP (R-type) in dimension ğ‘= 2, 3 and 4. Averaged over 100 data sets without changes with
104 data points."
OTHER,0.940251572327044,"Figure 11: Run time of different optimization approaches of GeomFPOP (R-type) using multivariate
time series without change points. The maximum run time of the algorithms is 3 minutes. Averaged
over 100 data sets."
OTHER,0.9433962264150944,"5.6
The number of change point candidates in time: GeomFPOP vs. PELT"
OTHER,0.9465408805031447,"In this appendix we compare the square of the number of candidates stored by GeomFPOP (S and
R-type) to the corresponding number of candidates stored by PELT over time. Indeed, the complexity
of GeomFPOP at each time step is a function of the square of the number of candidates, while, for
PELT, of the number of candidates (see Section 4.2). Figure 12 shows the ratios of these computed
quantities for dimension 2 â‰¤ğ‘â‰¤10. It is noteworthy that for both S-type and R-type for ğ‘= 2
this ratio is almost always less than 1 and decreases with time. This is coherent with the fact that
GeomFPOP is faster than PELT (see Figure 6). At ğ‘= 3 for the R-type this ratio is approximately 1,
while for the S-type it is greater than 1 and continues to increase with increasing ğ‘¡value. For sizes
3 < ğ‘â‰¤10, this ratio remains consistently greater than 1 for both S-type and R-type, showing a
continuous increasing trend with time. This is coherent with the fact that GeomFPOP is almost as
fast as PELT for ğ‘= 3 and slower than PELT for ğ‘â‰¤4 (see Figure 6)."
OTHER,0.949685534591195,"5.7
Run time of the algorithm by multivariate time series with changes in subset
of dimension"
OTHER,0.9528301886792453,"We expect GeomFPOP (random/random) to be slightly less effective (but no worse than in the absence
of changes) if changes are only present in a subset of dimensions. To this end, in this appendix for"
OTHER,0.9559748427672956,"Figure 12: The ratio of the square of the number of candidates stored in GeomFPOP to the number of
candidates stored in PELT over time. The horizontal black line corresponds to the value 1."
OTHER,0.9591194968553459,"dimension 2 â‰¤ğ‘â‰¤4 we examine the run time of GeomFPOP (random/random) as in Section 4.5
(see Figure 9) but removing all changes in the last ğ‘˜dimensions (with ğ‘˜= 0, â€¦ , ğ‘âˆ’1). The results
are presented in Figure 13. There are two regimes. For a small number of segments (the threshold
between small and large numbers of segments is around 2 Ã— 103 for all considered dimensions ğ‘),
the run time decreases with the number of segments and the difference between the run time of
GeomFPOP (random/random) for ğ‘˜= 0 (this case corresponds to changes in all dimensions) and
ğ‘˜> 0 is very small. For larger number of segments, the run time increases with the number of
segments, as in Section 4.5 and also increases with ğ‘˜. Importantly, in this regime the run time is
never lower than for 1 segment."
OTHER,0.9622641509433962,"p = 2
p = 3
p = 4"
OTHER,0.9654088050314465,"100
101
102
103
104 100
101
102
103
104 100
101
102
103
104 101 102 103"
OTHER,0.9685534591194969,Number of segments into a time series with 10â¶ data points
OTHER,0.9716981132075472,Seconds
OTHER,0.9748427672955975,"k
0
1
2
3"
OTHER,0.9779874213836478,"Figure 13: Dependence of the run time of the (random/random) approach of GeomFPOP (R-type) on
the number of segments in a ğ‘-variable time series with 106 data points where all changes in the last
ğ‘˜dimensions have been removed."
REFERENCES,0.9811320754716981,References
REFERENCES,0.9842767295597484,"Aminikhanghahi, Samaneh, and Diane J Cook. 2017. â€œA Survey of Methods for Time Series Change
Point Detection.â€ Knowledge and Information Systems 51 (2): 339â€“67.
Anastasiou, Andreas, and Piotr Fryzlewicz. 2022. â€œDetecting Multiple Generalized Change-Points by"
REFERENCES,0.9874213836477987,"Isolating Single Ones.â€ Metrika 85 (February). https://doi.org/10.1007/s00184-021-00821-6.
Andreou, Elena, and Eric Ghysels. 2002. â€œDetecting Multiple Breaks in Financial Market Volatility
Dynamics.â€ Journal of Applied Econometrics 17 (5): 579â€“600. http://www.jstor.org/stable/4129273.
Aue, Alexander, Lajos HorvÃ¡th, Marie HuÅ¡kovÃ¡, and Piotr Kokoszka. 2006. â€œChange-Point Monitoring
in Linear Models.â€ The Econometrics Journal 9 (3): 373â€“403. http://www.jstor.org/stable/23114925.
Auger, Ivan E., and Charles E. Lawrence. 1989. â€œAlgorithms for the Optimal Identification of Segment
Neighborhoods.â€ Bulletin of Mathematical Biology 51 (1): 39â€“54. https://doi.org/10.1007/BF0245
8835.
Bai, Jushan, and Pierre Perron. 2003. â€œComputation and Analysis of Multiple Structural-Change.â€
Journal of Applied Econometrics 18 (January).
Bosc, Marcel, Fabrice Heitz, Jean-Paul Armspach, Izzie Namer, Daniel Gounot, and Lucien Rumbach.
2003. â€œAutomatic Change Detection in Multimodal Serial MRI: Application to Multiple Sclerosis
Lesion Evolution.â€ NeuroImage 20(2), 643â€“56. https://doi.org/https://doi.org/10.1016/S1053-
8119(03)00406-3.
Data, Committee, Committee Statistics, Board Applications, Division Sciences, and National Council.
2013. Frontiers in Massive Data Analysis. Frontiers in Massive Data Analysis. The National
Academies Press. https://doi.org/10.17226/18374.
Davis, Richard A., Thomas C. M. Lee, and Gabriel A. Rodriguez-Yam. 2006. â€œStructural Break
Estimation for Nonstationary Time Series Models.â€ Journal of the American Statistical Association
101: 223â€“39. https://EconPapers.repec.org/RePEc:bes:jnlasa:v:101:y:2006:p:223-239.
DucrÃ©-Robitaille, Jean-FranÃ§ois, Lucie A. Vincent, and Gilles Boulet. 2003. â€œComparison of Techniques
for Detection of Discontinuities in Temperature Series.â€ International Journal of Climatology 23.
Fearnhead, Paul, Robert Maidstone, and Adam Letchford. 2018. â€œDetecting Changes in Slope with an
L0 Penalty.â€ Journal of Computational and Graphical Statistics, 1â€“11.
Frick, Klaus, Axel Munk, and Hannes Sieling. 2013. â€œMultiscale Change-Point Inference.â€ arXiv."
REFERENCES,0.9905660377358491,"https://doi.org/10.48550/ARXIV.1301.7212.
Fryzlewicz, Piotr. 2014. â€œWild Binary Segmentation for Multiple Change-Point Detection.â€ The
Annals of Statistics 42 (6). https://doi.org/10.1214/14-aos1245.
Galceran, Enric, Alexander Cunningham, Ryan Eustice, and Edwin Olson. 2017. â€œMultipolicy
Decision-Making for Autonomous Driving via Changepoint-Based Behavior Prediction: Theory
and Experiment.â€ Autonomous Robots 41 (August). https://doi.org/10.1007/s10514-017-9619-z.
Hall, Peter, J. W. Kay, and D. M. Titterington. 1990. â€œAsymptotically Optimal Difference-Based
Estimation of Variance in Nonparametric Regression.â€ Biometrika 77 (3): 521â€“28. http://www.js
tor.org/stable/2336990.
Hampel, Frank R. 1974. â€œThe Influence Curve and Its Role in Robust Estimation.â€ Journal of the
American Statistical Association 69 (346): 383â€“93. http://www.jstor.org/stable/2285666.
Harchaoui, Z., and C. LÃ©vy-Leduc. 2010. â€œMultiple Change-Point Estimation with a Total Variation
Penalty.â€ Journal of the American Statistical Association. 105 (492): 1480â€“93. http://www.jstor.or
g/stable/27920180.
Jackson, Brad, Jeffrey D Scargle, David Barnes, Sundararajan Arabhi, Alina Alt, Peter Gioumousis,
Elyus Gwin, Paungkaew Sangtrakulcharoen, Linda Tan, and Tun Tao Tsai. 2005. â€œAn Algorithm
for Optimal Partitioning of Data on an Interval.â€ IEEE Signal Processing Letters 12 (2): 105â€“8.
Jewell, Sean, Paul Fearnhead, and Daniela Witten. 2019. â€œTesting for a Change in Mean After
Changepoint Detection.â€ arXiv. https://doi.org/10.48550/ARXIV.1910.04291.
Killick, Rebecca, Paul Fearnhead, and Idris A. Eckley. 2012. â€œOptimal Detection of Changepoints with
a Linear Computational Cost.â€ Journal of the American Statistical Association 107 (500): 1590â€“98.
Lai, Weil R, Mark D Johnson, Raju Kucherlapati, and Peter J Park. 2005. â€œComparative Analysis of
Algorithms for Identifying Amplifications and Deletions in Array CGH Data.â€ Bioinformatics 21
(19): 3763â€“70.
Lavielle, Marc, and Ã‰milie Lebarbier. 2001. â€œAn Application of MCMC Methods for the Multiple"
REFERENCES,0.9937106918238994,"Change-Points Problem.â€ Signal Processing 81: 39â€“53. https://api.semanticscholar.org/CorpusID:
9866087.
Lavielle, Marc, and Eric Moulines. 2000. â€œLeast-Squares Estimation of an Unknown Number of Shifts
in a Time Series.â€ Journal of Time Series Analysis 21 (1): 33â€“59.
Lebarbier, Emilie. 2005. â€œDetecting Multiple Change-Points in the Mean of Gaussian Process by Model
Selection.â€ Signal Processing 85 (April): 717â€“36. https://doi.org/10.1016/j.sigpro.2004.11.012.
Liehrmann, Arnaud, Etienne Delannoy, Alexandra Launay-Avon, Elodie Gilbault, Olivier Loudet,
BenoÃ®t Castandet, and Guillem Rigaill. 2023. â€œDiffSegR: an RNA-seq data driven method for
differential expression analysis using changepoint detection.â€ NAR Genomics and Bioinformatics
5 (4): lqad098. https://doi.org/10.1093/nargab/lqad098.
Liehrmann, Arnaud, Guillem Rigaill, and Toby Dylan Hocking. 2021. â€œIncreased Peak Detection
Accuracy in over-Dispersed ChIP-Seq Data with Supervised Segmentation Models.â€ BMC Bioin-
formatics 22 (1): 1â€“18.
Maidstone, Robert, Toby Hocking, Guillem Rigaill, and Paul Fearnhead. 2017. â€œOn Optimal Multiple
Changepoint Algorithms for Large Data.â€ Statistics and Computing 27 (2): 519â€“33.
Malladi, Rakesh, Giridhar P. Kalamangalam, and Behnaam Aazhang. 2013. â€œOnline Bayesian Change
Point Detection Algorithms for Segmentation of Epileptic Activity.â€ 2013 Asilomar Conference on
Signals, Systems and Computers, 1833â€“37.
Naoki, Itoh, and Juergen Kurths. 2010. â€œChange-Point Detection of Climate Time Series by Nonpara-
metric Method.â€ Lecture Notes in Engineering and Computer Science 2186 (October).
Olshen, Adam, E. S. Venkatraman, Robert Lucito, and Michael Wigler. 2004. â€œCircular Binary
Segmentation for the Analysis of Array-Based DNA Copy Number Data.â€ Biostatistics (Oxford,
England) 5 (November): 557â€“72. https://doi.org/10.1093/biostatistics/kxh008.
Picard, Franck, Stephane Robin, Marc Lavielle, Christian Vaisse, and Jean-Jacques Daudin. 2005.
â€œA Statistical Approach for Array CGH Data Analysis.â€ BMC Bioinformatics 6: np. https:
//doi.org/10.1186/1471-2105-6-27.
Radke, R. J., S. Andra, O. Al-Kofahi, and B. Roysam. 2005. â€œImage Change Detection Algorithms: A
Systematic Survey.â€ IEEE Transactions on Image Processing 14 (3): 294â€“307. https://doi.org/10.110
9/TIP.2004.838698.
Ranganathan, Ananth. 2012. â€œPLISS: Labeling Places Using Online Changepoint Detection.â€ Auton.
Robots 32 (4): 351â€“68. https://doi.org/10.1007/s10514-012-9273-4.
Reeves, Jaxk, Jien Chen, Xiaolan L. Wang, Robert Lund, and Qi Qi Lu. 2007. â€œA Review and Compari-
son of Changepoint Detection Techniques for Climate Data.â€ Journal of Applied Meteorology and
Climatology 46 (6): 900â€“915. https://doi.org/10.1175/JAM2493.1.
Rigaill, Guillem. 2015. â€œA Pruned Dynamic Programming Algorithm to Recover the Best Segmenta-
tions with 1 to ğ¾ğ‘šğ‘ğ‘¥Change-Points.â€ Journal de La SociÃ©tÃ© FranÃ§aise de Statistique 156 (4): 180â€“205.
http://www.numdam.org/item/JSFS_2015__156_4_180_0/.
Runge, Vincent. 2020. â€œIs a Finite Intersection of Balls Covered by a Finite Union of Balls in Euclidean
Spaces?â€ Journal of Optimization Theory and Applications 187 (2): 431â€“47.
Rybach, David, Christian Gollan, Ralf Schluter, and Hermann Ney. 2009. â€œAudio Segmentation for
Speech Recognition Using Segment Features.â€ In 2009 IEEE International Conference on Acoustics,
Speech and Signal Processing, 4197â€“4200. https://doi.org/10.1109/ICASSP.2009.4960554.
Staudacher, Martin, Stefan Telser, Anton Amann, Hartmann Hinterhuber, and Monika Ritsch-Marte.
2005. â€œA New Method for Change-Point Detection Developed for on-Line Analysis of the Heart
Beat Variability During Sleep.â€ Physica A-Statistical Mechanics and Its Applications 349: 582â€“96.
Truong, Charles, Laurent Oudre, and Nicolas Vayatis. 2020. â€œSelective Review of Offline Change
Point Detection Methods.â€ Signal Processing 167: 107299.
Verzelen, Nicolas, Magalie Fromont, Matthieu Lerasle, and Patricia Reynaud-Bouret. 2020. â€œOptimal
Change-Point Detection and Localization.â€ arXiv. https://doi.org/10.48550/ARXIV.2010.11470.
Yao, Yi-Ching. 1984. â€œEstimation of a Noisy Discrete-Time Step Function: Bayes and Empirical Bayes"
REFERENCES,0.9968553459119497,"Approaches.â€ The Annals of Statistics 12 (4): 1434â€“47. https://doi.org/10.1214/aos/1176346802.
Zhang, Nancy, and David Siegmund. 2007. â€œA Modified Bayes Information Criterion with Applications
to the Analysis of Comparative Genomic Hybridization Data.â€ Biometrics 63 (April): 22â€“32.
https://doi.org/10.1111/j.1541-0420.2006.00662.x."
