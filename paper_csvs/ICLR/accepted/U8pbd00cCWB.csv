Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0020833333333333333,"Implicit neural shape functions, e.g. occupancy fields or signed distance func-
tions, are promising 3D representations for modeling arbitrary 3D surfaces. How-
ever, existing approaches that use these representations for single-view 3D re-
construction require 3D supervision signals at every location in the scene, posing
difficulties when extending to real-world scenes where ideal watertight geome-
try necessary to compute dense supervision is difficult to obtain. In such cases,
constraints on the spatial gradient of the implicit field, rather than the value it-
self, can provide a training signal, but this has not been employed as a source of
supervision for single-view reconstruction in part due to the difficulties of differ-
entiably sampling a spatial gradient from a feature map. In this paper, we derive a
novel closed-form Differentiable Gradient Sampling (DGS) solution that enables
backpropagation of the loss on spatial gradients to the feature maps, thus allow-
ing training on large-scale scenes without dense 3D supervision. As a result, we
demonstrate single view implicit 3D surface reconstructions on real-world scenes
via learning directly from a scanned dataset. Our model performs well when gen-
eralizing to unseen images from Pix3D or downloaded directly from the Internet
(Fig. 1). Extensive quantitative analysis confirms that our proposed DGS module
plays an essential role in our learning framework. Video and code are available at
https://github.com/zhusz/ICLR22-DGS."
ABSTRACT,0.004166666666666667,"View 0
View 1
View 2
View 3
View 4
View 5"
ABSTRACT,0.00625,"Figure 1: Given a single RGB image as the input (the first column), our model can predict its 3D
implicit surface reconstruction (shown in six novel views in the last six columns). The test images
for the first 3 rows are downloaded from the Internet and the last 2 rows are from the pix3d dataset."
ABSTRACT,0.008333333333333333,Published as a conference paper at ICLR 2022
INTRODUCTION,0.010416666666666666,"1
INTRODUCTION"
INTRODUCTION,0.0125,"Recent studies have shown promising learning capabilities of implicit models for 3D geometry rep-
resentations (Park et al., 2019; Mescheder et al., 2019; Mildenhall et al., 2020; Sitzmann et al., 2019;
Gropp et al., 2020; Saito et al., 2019; Yu et al., 2020). Rather than explicitly representing 3D geome-
try with a textured mesh (Kato et al., 2018) or point cloud (Fan et al., 2017), implicit representations
express a function of points in the space. Among many recent promising implicit models, the oc-
cupancy field (Mescheder et al., 2019; Popov et al., 2020; Chen & Zhang, 2019b) and the Signed
Distance Field (SDF) (Park et al., 2019; Xu et al., 2019; Li & Zhang, 2021) are particularly suitable
for high resolution topology free surface reconstruction. Given a query point (x, y, z) âˆˆR3 in the
space, the occupancy function returns the occupancy status of the point o(x, y, z) âˆˆ{+, âˆ’}, while
the SDF value returns the closest distance from a point to the surface of the given 3D geometry,
as well as its sign for its occupancy. The surface is defined by the classification boundary of the
occupancy field or the zero level-set of the SDF (Lorensen & Cline, 1987)."
INTRODUCTION,0.014583333333333334,"Despite the strong representational power of these models and their success on predicting 3D objects
from single image, learning to predict complex 3D geometry, such as scenes, from a real world
image remains a challenge for this class of methods. Obtaining the occupancy label, or the sign,
from noisy non-watertight meshes is non-trivial (Atzmon & Lipman, 2020) (Fig. 2). Furthermore,
producing the label on the fly from a large number of triangle meshes, as encountered in typical
real-world scenes with complex geometry, can be computationally prohibitive (Straub et al., 2019;
Chang et al., 2017; Dai et al., 2017; Song et al., 2017). Storing dense query points sufficiently with
pre-computed occupancies or SDF for complex geometries, or meta data of meshes like octrees or
hashing, is a possibility, but incurs severe engineering and computational burdens on storage and
runtime loading (Yu et al., 2021). The challenge is exacerbated for predicting scenes in particular,
since unlike objects scenes in the view frustrum typically cannot be enclosed by a pre-defined range
of space and do not have a categorical canonical coordinate system (Popov et al., 2020)."
INTRODUCTION,0.016666666666666666,"Recent studies on fitting a single scene geometry to partial 3D observations (Gropp et al., 2020;
Sitzmann et al., 2020) have pointed out the value of the 3D spatial gradient of the implicit field. For
instance, the gradients of the SDF on surface points are actually the surface normals and the 2-norm
of the spatial gradients for any point in the space is 1, known as the Eikonal equation (Crandall
& Lions, 1983). Similarly, in this work, we propose to regularize the gradient of the occupancy
field to be zero away from the surface (Fig. 3(c)). Thus, one can train the full occupancy field or
SDF with only the surface point clouds, thanks to these constraints on the gradients of the implicit
shape models, even when the value of the occupancy or the SDF itself is not available everywhere.
However, existing approaches (Gropp et al., 2020; Sitzmann et al., 2020) have only demonstrated
this benefit in the cases of geometry inputs only, where a network is fit to a particular scene per
model without any conditioning on input images. When locally sampled image features are used in
the feed-forward prediction, it is necessary to derive a differentiable gradient sampling solution over
the feature pixel map."
INTRODUCTION,0.01875,"In this paper, we tackle this challenge by deriving a closed-form differentiable gradient sampling
(DGS) solution for learning a single-view 3D implicit reconstruction model (Fig. 4). Our novel
propagation of the loss gradient of the spatial gradient back to the feature maps (Eq. 6) serves in
addition to the existing spatial gradient sampling (Eq. 5) and loss gradient back-propagation (Eq. 4)
used in existing deep learning frameworks (Paszke et al., 2019; Abadi et al., 2016). The resulting
end-to-end learning scheme with supervision over the spatial gradients opens the potential to train a
model generalizable to unseen test cases of complex scenes with only surface point cloud supervision
â€” a typical setting for real-world data â€” without requiring ground truth per-query-point labels."
INTRODUCTION,0.020833333333333332,"Our contributions include a framework to propagate spatial gradients through a spatial feature sam-
pling procedure, and a novel closed-form DGS solution. Experiments on real-scanned data (Scan-
netV2 (Dai et al., 2017)) shows that DGS enables training a single-image 3D implicit reconstruction
model that can generalize to unseen scenes, with only imperfect surface annotations as the supervi-
sion (e.g. non-watertight meshes). Experiments on synthetic data (ShapeNet Chang et al. (2015))
indicate that our learning framework without using dense per-query-point training labels demon-
strates competitive performance compared to the oracle scenario where dense occupancy labels are
available. To the best of our knowledge, DGS-enabled shape inference provides the first single-
view implicit shape reconstruction on real scene datasets which can generalize accurately to unseen
scenes from different datasets or domains (see Figure 1)."
INTRODUCTION,0.022916666666666665,Published as a conference paper at ICLR 2022
INTRODUCTION,0.025,Input Image
INTRODUCTION,0.027083333333333334,DISN + TSDFVox DGS
INTRODUCTION,0.029166666666666667,View 0
INTRODUCTION,0.03125,View 1
INTRODUCTION,0.03333333333333333,View 0
INTRODUCTION,0.035416666666666666,View 1
INTRODUCTION,0.0375,"Ground Truth Mesh
TSDF Voxelization
Ground Truth Mesh"
INTRODUCTION,0.03958333333333333,"TSDF 
Voxelization"
INTRODUCTION,0.041666666666666664,"(a)
(b)
(c)"
INTRODUCTION,0.04375,Input Image
INTRODUCTION,0.04583333333333333,"Figure 2: (a, b) Truncated SDF (TSDF) (Curless & Levoy, 1996) Voxelization results of the non-
watertight ground truth meshes (each shown in two views). (a) is a simple sphere and (b) is a real
scene from the ScannetV2 training data. After depth-fusion and internal-filling (Popov et al., 2020),
the inside space of both geometries (a, b) remains empty (red arrows), causing severe noise for
training the occupancy field or the SDF prediction model. This type of noise particularly affects
the single-view prediction problem, as no additional predicted depth surface from other views are
available. (c) As a result of learning the implicit prediction directly from the inaccurate and low-
resolution TSDF voxels (due to engineering constraints on runtime loading and memory bottleneck
for the sufficiently dense pre-computed query point occupancy labels), the prediction result (DISN
+ TSDFVox) is clearly inferior compared to our results (DGS). The surface color denotes the evalu-
ation of the â€œprecisionâ€, with the larger blue region, the higher â€œprecisionâ€."
RELATED WORKS,0.04791666666666667,"2
RELATED WORKS + - + +
+ + + + +
+ + + - - - - - - - + +
+ +
- - -"
RELATED WORKS,0.05,"-
-
-
-
-
- - - -
- - - + + +
+ +
+ + - + + - +
+ +"
RELATED WORKS,0.052083333333333336,"+
-
âˆ‡
Occupied
Unoccupied
âˆ‡!,#,$o% = 0"
RELATED WORKS,0.05416666666666667,(a) Learning with
RELATED WORKS,0.05625,ideal meshes
RELATED WORKS,0.058333333333333334,"(b) Learning with the 
non-watertight meshes 
with TSDF voxelization"
RELATED WORKS,0.06041666666666667,(c) Learning with the
RELATED WORKS,0.0625,proposed DGS
RELATED WORKS,0.06458333333333334,"Illustration: âˆ‡ âˆ‡ âˆ‡ âˆ‡ âˆ‡
âˆ‡"
RELATED WORKS,0.06666666666666667,"Figure 3: Illustration of the loss imposition for the
occupancy prediction scenario. (a) When learning
from the ideal mesh for ShapeNet objects, we can
directly supervise the training with the accurate
occupancy labels. (b) On scans of real scenes with
imperfections (Fig. 2(b)), the TSDF voxelization
produces severe noise for training. Specifically, a
considerable fraction of the objects are â€œemptyâ€
inside. (c) Our learning scheme with DGS allevi-
ates these issues via enabling imposition of losses
on the gradients all the way back to image fea-
tures."
D IMPLICIT REPRESENTATIONS AMONG ALL THE,0.06875,"3D Implicit Representations Among all the
3D representations, implicit models are advan-
tageous for arbitrarily high resolution model-
ing (unlike voxels with fixed resolution and
no detailed surface modeling) and easy learn-
ing (unlike meshes which assume a fixed topol-
ogy). Implicit models typically learn to estab-
lish a mapping between a query point and the
prediction of the point.
Groueix et al.
pro-
posed to learn a mapping from the uv tex-
ture map to the 3D surface point.
More re-
cent works (Park et al., 2019; Mescheder et al.,
2019; Xu et al., 2019; Popov et al., 2020; Chen
& Zhang, 2019a) focus on mapping the query
point coordinates to the signed distance field or
the occupancy field. In addition to these pure
geometry modeling, recent works like (Saito
et al., 2019) also model the surface texture via
learning a mapping from the surface point to the
RGB value, or use the RGB loss as the supervi-
sion signal (Niemeyer et al., 2020; Yariv et al., 2020). In contrast to defining textures explicitly on
the surface, Mildenhall et al. (2020) proposed the volumetric rendering representation which maps
the point coordinate and the viewing angle to volumetric textures. Oechsle et al. (2021); Wang et al.
(2021); Yariv et al. (2021) use volume renderings for fitting high quality geometry of single scenes,
but these are not generalizable to unseen scenes. In multi-view stereo setting, Murez et al. (2020);
Sun et al. (2021); BoË‡ziË‡c et al. (2021) proposed to accumulate the multi-view predictions in a TSDF
voxel volume. We believe our proposed DGS module is necessary when extending most of the exist-
ing implicit surface representations into feed-forward models that are conditioned on input image(s),
and training these models with a loss on the gradient of the predicted field. In this work, we focus
on 3D geometry reconstruction reconstructions from single view, as an example of the application
of the proposed DGS module."
D IMPLICIT REPRESENTATIONS AMONG ALL THE,0.07083333333333333,"Differentiable Operations End-to-end deep learning requires all modules in a computation path to
be differentiable. Many differentiable modules are proposed to serve a particular functionality. The"
D IMPLICIT REPRESENTATIONS AMONG ALL THE,0.07291666666666667,Published as a conference paper at ICLR 2022
D IMPLICIT REPRESENTATIONS AMONG ALL THE,0.075,"spatial transformer layer (Jaderberg et al., 2015) is one of the early approaches for differentiable
sampling of the feature map. Kato et al. (2018) proposed to render the mesh into image in a dif-
ferentiable manner to optimize the mesh or the camera parameters to fit to a particular image. A
similar idea has also been applied for rendering point clouds (Wiles et al., 2020; Yifan et al., 2019).
Tulsiani et al. (2017) and Mildenhall et al. (2020) proposed to compute the ray rendering in an ac-
cumulating manner. One common feature of these differentiable operations is a strategy to soften
the existing explicit modeling (Liu et al., 2019), and replace the gradient of the loss with approxi-
mate signals, making sure those signals provide guidance in the right direction. Our differentiable
gradient sampler also seeks to soften the spatial gradient between the adjacent pixels."
LEARNING FRAMEWORK,0.07708333333333334,"3
LEARNING FRAMEWORK"
LEARNING FRAMEWORK,0.07916666666666666,"Problem definition and notations. We aim to train a feed-forward deep model for predicting the 3D
implicit surface reconstruction conditioned on a single image. We denote the input RGB image as
I âˆˆRmÃ—nÃ—3. Like most feed-forward models, our model employs an image encoder. We denote the
extracted 2D features as Ï•. Our model Ë†fÎ˜(Â·), parameterized by Î˜, takes in the image feature Ï• and
predicts for each 3D location (x, y, z) the implicit value Ë†fÎ˜(Ï•; x, y, z). We denote the ground truth
value as f(Ï•; x, y, z). For the occupancy field, the implicit field value f represents the occupancy
probability. We denote the predicted occupancy probability as Ë†oÎ˜(Ï•; x, y, z) âˆˆ[0, 1] and the ground
truth binary occupancy label as o(Ï•; x, y, z) âˆˆ{â€œ âˆ’â€, â€œ + â€}, where â€œâˆ’â€ represents unoccupied
space and â€œ+â€ vice versa. For SDF, the implicit field value f represents the signed distance between
the query point and its projection on the surface. We denote the predicted and the ground truth
signed distance as Ë†sÎ˜(Ï•; x, y, z) âˆˆR and s(Ï•; x, y, z) âˆˆR respectively."
LEARNING FRAMEWORK,0.08125,"Background. Typical fully supervised training (Xu et al., 2019; Popov et al., 2020) imposes the
loss to each individual sampled query points by comparing the predicted field value Ë†fÎ˜(Ï•; x, y, z)
with the ground truth value f(Ï•; x, y, z).
For occupancy predictions, we compute the loss as
P"
LEARNING FRAMEWORK,0.08333333333333333,"{I,PI}âˆˆD
P"
LEARNING FRAMEWORK,0.08541666666666667,"(x,y,z)âˆˆPI BCE(Ë†oÎ˜(Ï•; x, y, z), oÎ˜(Ï•; x, y, z)) (Fig. 3(a)), where â€œBCEâ€ represents
the binary cross entropy loss, D represents the whole single-view image training set, and PI repre-
sents the set of all the possible query point sampling locations (x, y, z) within the view frustum. For
SDF, we compute the loss as P"
LEARNING FRAMEWORK,0.0875,"{I,PI}âˆˆD
P"
LEARNING FRAMEWORK,0.08958333333333333,"(x,y,z)âˆˆPI |Ë†sÎ˜(Ï•; x, y, z) âˆ’s(Ï•; x, y, z)|."
PROPOSED FRAMEWORK,0.09166666666666666,"3.1
PROPOSED FRAMEWORK
As obtaining the full labels f(Ï•; x, y, z) for query points (x, y, z) from every location in PI is non-
trivial for complex geometry from real-world (Fig. 2) and due to additional engineering constraints
on runtime loading and memory bottleneck for the sufficiently dense pre-computed query point
occupancy labels, we propose to incorporate the loss w.r.t. the spatial gradients. For occupancy of
the implicit field, L =
X"
PROPOSED FRAMEWORK,0.09375,"{I,PI}âˆˆD
(
X"
PROPOSED FRAMEWORK,0.09583333333333334,"(x,y,z)âˆˆPIâˆ’P0
I"
PROPOSED FRAMEWORK,0.09791666666666667,"Î»orâˆ¥âˆ‡x,y,zË†oÎ˜(Ï•; x, y, z)âˆ¥ +
X"
PROPOSED FRAMEWORK,0.1,"(x,y,z)âˆˆP0+
I"
PROPOSED FRAMEWORK,0.10208333333333333,"BCE(Ë†oÎ˜(Ï•; x, y, z), â€œ + â€) +
X"
PROPOSED FRAMEWORK,0.10416666666666667,"(x,y,z)âˆˆP0âˆ’
I"
PROPOSED FRAMEWORK,0.10625,"BCE(Ë†oÎ˜(Ï•; x, y, z), â€œ âˆ’â€))
(1)"
PROPOSED FRAMEWORK,0.10833333333333334,"where âˆ‡x,y,zË†oÎ˜(Ï•; x, y, z) = [âˆ‡xË†oÎ˜(Ï•; x, y, z), âˆ‡yË†oÎ˜(Ï•; x, y, z), âˆ‡zË†oÎ˜(Ï•; x, y, z)]âŠ¤denotes the
spatial gradient of the occupancy prediction, P0+
I
and P0âˆ’
I
denote inward and outward near-surface
query points, and Î»or represents the loss weight of the occupancy geometric regularization. The
facing direction of the mesh surface (determining inward or outward) can be determined by normals
(if available), or via rendering the surface in all views as in the RGBD captures (Dai et al., 2017)
(with the surface facing toward camera as the â€œoutwardâ€ side). The three terms in Eq. 1 correspond
to the three types of losses in Fig. 3(c) (â€œâˆ‡â€, â€œ+â€, â€œâˆ’â€) respectively. For SDF, we set the loss similar
to Gropp et al. (2020): L =
X"
PROPOSED FRAMEWORK,0.11041666666666666,"{I,PI}âˆˆD
(
X"
PROPOSED FRAMEWORK,0.1125,"(x,y,z)âˆˆPI
Î»sr|âˆ¥âˆ‡x,y,zË†sÎ˜(Ï•; x, y, z)âˆ¥2 âˆ’1| +
X"
PROPOSED FRAMEWORK,0.11458333333333333,"(x,y,z)âˆˆP0
I"
PROPOSED FRAMEWORK,0.11666666666666667,"|Ë†sÎ˜(Ï•; x, y, z)| +
X"
PROPOSED FRAMEWORK,0.11875,"(x,y,z)âˆˆP0
I"
PROPOSED FRAMEWORK,0.12083333333333333,"Î»snâˆ¥âˆ‡x,y,zË†sÎ˜(Ï•; x, y, z) âˆ’âˆ‡x,y,zs(Ï•; x, y, z)âˆ¥)
(2)"
PROPOSED FRAMEWORK,0.12291666666666666,Published as a conference paper at ICLR 2022
PROPOSED FRAMEWORK,0.125,"Image 
Encoder"
PROPOSED FRAMEWORK,0.12708333333333333,"Input Image I
2D Feature Maps ğœ™ ğ‘¥
ğ‘¦
ğ‘§ ğ‘–
ğ‘—"
PROPOSED FRAMEWORK,0.12916666666666668,"âˆ‡!,#,$ğœ™%,& ğœ™%,&"
PROPOSED FRAMEWORK,0.13125,"Query
Location"
PROPOSED FRAMEWORK,0.13333333333333333,"âˆ‡!,#,$ğ‘“%(ğœ™; ğ‘¥, ğ‘¦, ğ‘§)"
PROPOSED FRAMEWORK,0.13541666666666666,"ğ‘“%(ğœ™; ğ‘¥, ğ‘¦, ğ‘§)
â€¦
Loss ğœ™! ğœ™! ğœ™"" ğœ™#
ğœ™$"
PROPOSED FRAMEWORK,0.1375,"ğœ™!,#
ğœ™"" ğœ™#
ğœ™$"
PROPOSED FRAMEWORK,0.13958333333333334,"ğ›½ğ‘¤
(1 âˆ’ğ›½)ğ‘¤ ğ›¼â„"
PROPOSED FRAMEWORK,0.14166666666666666,"(1 âˆ’ğ›¼)â„ ğœ™!
ğœ™"" ğœ™#
ğœ™$ ğœ™%,' ğœ™!
ğœ™"" ğœ™#
ğœ™$ ğœ™!
ğœ™"" ğœ™#
ğœ™$"
PROPOSED FRAMEWORK,0.14375,"âˆ‡%ğœ™%,'
âˆ‡'ğœ™%,'
ğœ™! âˆ’ğœ™""
ğœ™# âˆ’ğœ™$"
PROPOSED FRAMEWORK,0.14583333333333334,"ğœ™$ âˆ’ğœ™"""
PROPOSED FRAMEWORK,0.14791666666666667,ğœ™# âˆ’ğœ™!
PROPOSED FRAMEWORK,0.15,"(b) 2D Feature Map Pixels ğœ™
(c) Zoom-in"
PROPOSED FRAMEWORK,0.15208333333333332,(d) Existing differentiable
PROPOSED FRAMEWORK,0.15416666666666667,"sampling (Eq. 3, 4)"
PROPOSED FRAMEWORK,0.15625,(e) Proposed differentiable gradient
PROPOSED FRAMEWORK,0.15833333333333333,"sampling (Eq. 5, 6)"
PROPOSED FRAMEWORK,0.16041666666666668,"ResNetFC
ResNetFC"
PROPOSED FRAMEWORK,0.1625,(a) Framework of our training pipeline
PROPOSED FRAMEWORK,0.16458333333333333,"Differentiable Gradient Sampling
ğ‘–= ğ‘¦ğ‘“#/ğ‘§
ğ‘—= ğ‘¥ğ‘“#/ğ‘§"
PROPOSED FRAMEWORK,0.16666666666666666,"Figure 4: Overview of our learning framework (a) and differentiable gradient sampling (b, c, d, e)."
PROPOSED FRAMEWORK,0.16875,"where P0
I denotes the query points on the ground truth surface only, and Î»sr and Î»sn represents the
loss weight for the signed distance geometric regularization term and normal term respectively. The
three terms are the Eikonal regularization (Crandall & Lions, 1983), surface zero-SDF loss, and the
surface normal loss respectively, where the last term is optional (Gropp et al., 2020). Note now the
loss functions in Eq. 1, 2 no longer require the implicit ground truth label for query points far away
from the ground truth surface, enabling training with the real-world imperfect scanned data. In both
cases, the availability of surface normals is optional."
PROPOSED FRAMEWORK,0.17083333333333334,"Practically, we found only the proposed loss for the occupancy model (Fig. 3(c), Eq. 1) to be effec-
tive in the real-world single-view feed-forward scenario, and use Eq. 1 rather than Eq. 2 in all of our
experiments. There are a few reasons. First, learning with the Eikonal term (Eq. 2) suffers severely
from its sensitivity of model initialization. We found either the SDF or the Truncated SDF (TSDF)
representation model cannot easily converge during training. Specifically, we found it poses numer-
ical difficulties when the model is learned to predict a large distance value or a constant truncation
value for the majority of the query points in the air that is far from any surfaces. This becomes
a major problem when we depart from the single objects scenario (Mescheder et al., 2019; Popov
et al., 2020) or single scene fitting (Gropp et al., 2020) to large schale scenes, which is our focus.
Second, our loss function (Eq. 1) can significantly save memory footprint during training and enable
large batch size training, which is crucial in our learning framework. Unlike in Eq. 2 where all the
query points require spatial gradient computation, which leads to 3Ã— higher of memory footprint,
in our loss function (Eq. 1), only the non-surface query points do. We set the query point batch size
of the non-surface points (512 in practice) to be much smaller than the critical near-surface points
(4096 in practice), enabling learning with the image batch size of 32 in our real-scene training."
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.17291666666666666,"3.2
SAMPLING WITH DIFFERENTIABLE GRADIENTS"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.175,"A distinct difference compared to existing works is that our spatial gradient âˆ‡x,y,z Ë†fÎ˜(Ï•; x, y, z) is
also conditioned on the pixels in Ï•. Gropp et al. (2020) devised the model fÎ˜ to fit to a single scene,
and the spatial gradient âˆ‡x,y,z Ë†fÎ˜(x, y, z) can be conveniently computed because it does not involve
the image sampling procedure. Jiang et al. (2020); Lin et al. (2020) used a non-spatial global feature
for inference and hence bypassed 2D sampling. In our learning framework, the spatial gradient
computation must undergo the sampling procedure."
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.17708333333333334,"We name our gradient computation involving the sampling operation as the Pixel Conditioned Gra-
dients and derive a closed-form solution, Differentiable Gradient Sampling (DGS), for handling
forward and backward propagation. Figure 4(a) provides an illustration of our training pipeline.
Each layer in our network tracks both the response of the layer as well as its spatial gradient w.r.t.
(x, y, z). While it is well established to track the layer-wise spatial gradient for fully-connected
(FC) layers or convolutions in existing works (Gropp et al., 2020), tracking the spatial gradients and
back-propagating the loss to the feature map pixels Ï• through the sampling module has not been
studied. To this end, we derive the closed-form sampling scheme for tracking and propagating the
spatial gradients âˆ‡x,y,zÏ• = [âˆ‡xÏ•, âˆ‡yÏ•, âˆ‡zÏ•]âŠ¤through the sampling layer."
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.17916666666666667,"Background - 2D Differentiable Sampling. Differentiably sampling pixel values from a grid of
2D feature map with the given pixel locations (i, j) is a common operation. Throughout our paper,
we define the pixel coordinates in the normalized coordinate system that ranges from -1 to 1. As"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.18125,Published as a conference paper at ICLR 2022
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.18333333333333332,"illustrated in Fig. 4(d), given the feature map Ï• and the sampling locations (i, j), the resulting
sampled value Ï•(i, j) is"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.18541666666666667,"Ï•(i, j) = (1 âˆ’Î±)((1 âˆ’Î²)Ï•A + Î²Ï•B) + Î±((1 âˆ’Î²)Ï•C + Î²Ï•D)
(3)"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.1875,"Please refer to Fig. 4(c) for the definitions of Î±, Î² and Ï•A, Ï•B, Ï•C, Ï•D. Without loss of generality
we use bilinear interpolation. During training, the gradient from the loss can be back-propagated via"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.18958333333333333,"âˆ‚L
âˆ‚Ï•A
= (1 âˆ’Î±)(1 âˆ’Î²)
âˆ‚L
âˆ‚Ï•(i, j).
(4)"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.19166666666666668,Equation 4 is for Pixel A and similarly to the other 3 pixels (please refer Eq. 9).
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.19375,"2D Differentiable Gradient Sampling. Our learning framework (Sec. 3.1, Fig. 4(a)) requires the
extension of the sampling capability from just the feature value response Ï•i,j to its spatial gradient
âˆ‡i,jÏ•(i, j) = [âˆ‡iÏ•(i, j), âˆ‡jÏ•(i, j)]âŠ¤. During the forward and the backward propagation, both the
sampled feature response Ï•i,j and its spatial gradient âˆ‡i,jÏ•(i, j) are recorded for further propagation
(Fig. 4(e)):"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.19583333333333333,"âˆ‡iÏ•(i, j) = (1 âˆ’Î²)(Ï•C âˆ’Ï•A) + Î²(Ï•D âˆ’Ï•B)"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.19791666666666666,"h
.
(5)"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.2,"Please refer to Eq. 10 for âˆ‡jÏ•(i, j). Hence, during the forward pass, we compute the spatial gradient
via Eq. 5 in addition to the existing value sampling (Eq. 3). During the backward pass, we compute
the loss gradient over the spatial gradient via"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.20208333333333334,"âˆ‚L
âˆ‚Ï•A
=
âˆ‚L
âˆ‚Ï•(i, j)
âˆ‚Ï•(i, j)"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.20416666666666666,"âˆ‚Ï•A
+
âˆ‚L
âˆ‚âˆ‡iÏ•(i, j)
âˆ‚âˆ‡iÏ•(i, j)"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.20625,"âˆ‚Ï•A
+
âˆ‚L
âˆ‚âˆ‡jÏ•(i, j)
âˆ‚âˆ‡jÏ•(i, j) âˆ‚Ï•A"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.20833333333333334,"= (1 âˆ’Î±)(1 âˆ’Î²)
âˆ‚L
âˆ‚Ï•(i, j) + (âˆ’1 âˆ’Î²"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.21041666666666667,"h
)
âˆ‚L
âˆ‚âˆ‡iÏ•(i, j) + (âˆ’1 âˆ’Î±"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.2125,"w
)
âˆ‚L
âˆ‚âˆ‡jÏ•(i, j),
(6)"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.21458333333333332,"where w and h are the width and height of a pixel in normalized coordinate system, s.t. w = 2/W
and h = 2/H for the feature map with the size W Ã— H. Please refer to Eq. 11 for
âˆ‚L
âˆ‚Ï•B , âˆ‚L"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.21666666666666667,âˆ‚Ï•C and
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.21875,"âˆ‚L
âˆ‚Ï•D ."
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.22083333333333333,"3D Differentiable Gradient Sampling. We now extend the sampling to 3D. We model the camera
as the pin-hole camera. For any point (x, y, z) in the camera space, we seek for its projected 2D
locations (i, j) based on the focal length f0 via i = yf0"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.22291666666666668,"z , j = xf0"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.225,z . The feed forward pass is
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.22708333333333333,"âˆ‡xÏ•(x, y, z) =âˆ‡iÏ•(i, j) Â· âˆ‚i"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.22916666666666666,"âˆ‚x + âˆ‡jÏ•(i, j) Â· âˆ‚j"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.23125,"âˆ‚x = âˆ‡jÏ•(i, j) Â· f0 z ,"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.23333333333333334,"âˆ‡yÏ•(x, y, z) =âˆ‡iÏ•(i, j) Â· âˆ‚i"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.23541666666666666,"âˆ‚y + âˆ‡jÏ•(i, j) Â· âˆ‚j"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.2375,"âˆ‚y = âˆ‡iÏ•(i, j) Â· f0 z ,"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.23958333333333334,"âˆ‡zÏ•(x, y, z) =âˆ‡iÏ•(i, j) Â· âˆ‚i"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.24166666666666667,"âˆ‚z + âˆ‡jÏ•(i, j) Â· âˆ‚j"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.24375,"âˆ‚z = âˆ‡iÏ•(i, j) Â· (âˆ’yf0"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.24583333333333332,"z2 ) + âˆ‡jÏ•(i, j) Â· (âˆ’xf0 z2 ). (7)"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.24791666666666667,"Note that here, our notation of the sampled 2D image feature Ï•(x, y, z) refers to the same as Ï•(i, j),
as (i, j) is the projected pixel coordinates of (x, y, z) that represent the exact projected 2D locations
when extracting the 2D features."
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.25,"During the backward propagation procedure, the DGS accumulates the gradient via"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.2520833333333333,"âˆ‚L
âˆ‚Ï•A
=
âˆ‚L
âˆ‚Ï•(x, y, z)
âˆ‚Ï•(x, y, z)"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.25416666666666665,"âˆ‚Ï•A
+
âˆ‚L
âˆ‚âˆ‡xÏ•(x, y, z)
âˆ‚âˆ‡xÏ•(x, y, z) âˆ‚Ï•A"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.25625,"+
âˆ‚L
âˆ‚âˆ‡yÏ•(x, y, z)
âˆ‚âˆ‡yÏ•(x, y, z)"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.25833333333333336,"âˆ‚Ï•A
+
âˆ‚L
âˆ‚âˆ‡zÏ•(x, y, z)
âˆ‚âˆ‡zÏ•(x, y, z) âˆ‚Ï•A"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.2604166666666667,"=
âˆ‚L
âˆ‚Ï•(x, y, z) Â· (1 âˆ’Î±)(1 âˆ’Î²) +
âˆ‚L
âˆ‚âˆ‡xÏ•(x, y, z) Â· (âˆ’1 âˆ’Î±"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.2625,"w
) Â· f0 z"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.26458333333333334,"+
âˆ‚L
âˆ‚âˆ‡yÏ•(x, y, z) Â· (âˆ’1 âˆ’Î²"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.26666666666666666,"h
) Â· f0"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.26875,"z +
âˆ‚L
âˆ‚âˆ‡zÏ•(x, y, z) Â· (1 âˆ’Î²"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.2708333333333333,"h
Â· yf0"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.27291666666666664,z2 + 1 âˆ’Î±
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.275,"w
Â· xf0 z2 ). (8)"
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.27708333333333335,Published as a conference paper at ICLR 2022
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.2791666666666667,"Table 1: Intersection over Union % (IoU â†‘) benchmarking result on the high-realism ShapeNet.
Our approach (the last 4 rows) demonstrates competitive performance compared to state-of-the-art
approaches (the top 4 rows) even trained without the dense occupancy labels as used in the oracle
settings of these existing works. Our comparisons with the state-of-the-art approaches are direct ab-
lations as we maintain exactly the same experimental setups except for the loss function. In addition,
our approach also comfortably outperforms the ablation baselines (the middle 5 approaches)."
SAMPLING WITH DIFFERENTIABLE GRADIENTS,0.28125,"Category
Craft
Rifle
Disp.
Lamp
Spk.
Box
Chair
Bench
Car
Plane
Sofa
Table
Phone
Mean
OccNet
49.6
39.7
49.7
33.3
49.3
42.1
42.8
30.9
57.2
41.7
60.7
42.4
64.8
46.5
DISN
54.5
52.5
50.2
39.2
53.3
46.0
50.6
37.1
58.6
48.5
64.9
48.4
67.6
51.6
CoReNet
60.5
67.5
61.0
46.9
56.8
51.3
59.7
47.1
61.1
58.4
68.7
56.9
77.3
59.5
DISN(Res50)+DVR
61.1
64.5
61.9
46.9
58.2
54.4
59.6
48.0
59.4
58.4
69.5
57.2
78.7
59.8
Abl.-NoGrad
11.9
4.9
23.6
15.4
31.4
30.6
25.1
10.1
20.8
7.7
28.3
20.1
21.2
19.3
Abl.-NoGrad (10 %)
23.7
19.9
33.1
26.5
40.0
35.2
35.6
18.6
33.0
15.7
38.8
30.7
41.0
30.1
Abl.-NoGrad (30 %)
26.4
15.5
36.8
23.6
40.6
36.0
36.8
19.5
38.5
19.1
47.8
30.9
39.4
31.6
Abl.-NoGrad (50 %)
39.7
30.0
39.7
28.4
45.0
37.5
39.0
23.6
50.6
29.0
53.5
36.2
49.7
38.6
Abl.-FixedE
49.6
52.5
44.4
33.0
46.3
39.9
43.4
27.5
57.5
46.9
58.8
39.1
68.0
46.7
OccNet w/ Eq. 1
50.7
42.6
50.4
33.0
50.1
43.7
44.4
32.9
56.8
41.6
60.9
44.0
68.4
47.7
DISN w/ Eq. 1
53.3
51.2
51.9
38.7
52.1
43.8
50.8
36.2
58.8
47.4
64.6
47.4
66.6
51.0
CoReNet w/ Eq. 1 (DGS)
61.1
67.5
62.7
44.2
54.8
49.6
59.5
45.4
59.4
59.9
69.8
55.1
78.0
59.0
DISN(Res50)+DVR
w/ Eq. 1 (DGS Best)
60.8
62.6
62.3
47.1
57.7
53.5
59.8
47.4
59.2
58.6
70.7
57.4
77.0
59.6"
EXPERIMENTS,0.2833333333333333,"4
EXPERIMENTS"
EXPERIMENTS,0.28541666666666665,"4.1
LEARNING FROM SYNTHETIC DATA (SHAPENET)"
EXPERIMENTS,0.2875,"Dataset. Following Popov et al. (2020), we conduct experiments on ShapeNet with the low-realism
and the high-realsim renderings. For low-realism, we use the renderings of various models from
Choy et al. (2016). Following Mescheder et al. (2019), we split the dataset into the training set of
30661 models, the validation set of 4371 models, and the test set of 8751 models. During testing, we
follow Mescheder et al. (2019) to only test the first rendering of each model. Similar to all the prior
works, we use the same 13 classes to report any performance via grouping all the test cases into the
same class. For high-realism, we follow the split and evaluation protocol of the single-object scenes
Popov et al. (2020) - we only use the 13 categories used by Choy et al. (2016), filter out repeated
samples, and finally construct the data with 666,565 models from the training set, 96,084 from the
validation set, and 189,748 from the test set. Following Popov et al. (2020), we only evaluate the
first 1% test cases of the test set (1898 samples).
Metrics. We use the Intersection-of-Union (IoU) for evaluating the model performances. For low-
realism renderings, we follow the protocol of IoU benchmarking from Mescheder et al. (2019) that
evaluating occupancy of the specified 100K query points in the space. For high-realism renderings,
we follow the protocol from Popov et al. (2020) to evaluate occupancy prediction of the 128Ã—128Ã—
128 grid. Following Popov et al. (2020), the grid cube is a unit cube that spans from âˆ’0.5 to 0.5 for
x and y, and from f/2 to 1 + f/2 for z in the camera coordinate system.
Comparison with state-of-the-art approaches. On high-realism ShapeNet, we build our model on
top of the representative state-of-the-art models - OccNet (Mescheder et al., 2019), DISN (Xu et al.,
2019), CoReNet (Popov et al., 2020) for their 3D volumetric implicit modeling capability. We also
incorporate a strong baseline named â€œDISN (ResNet50) + DVR (Niemeyer et al., 2020)â€, where
we replace the originally used VGG (Simonyan & Zisserman, 2014) with ResNet-50 as the image
encoder as used in CoReNet (Popov et al., 2020), and devised the 5-layer fully connected ResNet
as the decoder as used in DVR (Niemeyer et al., 2020). In each DGS experiment, we maintain
exactly the same encoder and decoder architecture, optimization parameters (e.g. the learning rate
and the epsilon of Adam Kingma & Ba (2014)) and the prediction format (per-query occupancy
probability). We set Î»or to be 0.01 in our loss function (Eq. 1). Note that the DGS experiments
can only access nearsurface training signals, in contrast to the oracle learning setting as in the state-
of-the-art approaches where models were trained with dense occupancy labels. We name the DGS
version of the CoReNet model as the DGS model, and the DISN+DVR counterpart as the DGS-
Best model. Quantitative results are reported in Tab. 4 for the low-realism evaluation setting, and in
Tab. 1 for the high-realism setting1. Our competitive experimental results indicate that our learning"
EXPERIMENTS,0.28958333333333336,"1We noticed that in our original submission version of the paper, our experiments were conducted with
evaluating the 64 Ã— 64 Ã— 64 volume grids due to an error. We thus revised our results in Tab. 1 with the
evaluation results returned from the 128 Ã— 128 Ã— 128 volume grid, and attach our previous results in Tab. 3.
Please refer to Sec. A for detailed explanation."
EXPERIMENTS,0.2916666666666667,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.29375,"Input
OccNet +"
EXPERIMENTS,0.29583333333333334,GeoReg
EXPERIMENTS,0.29791666666666666,"DISN + 
TSDFVox
CoReNet
DGS
GT
MiDaS
AdelaiDepth"
EXPERIMENTS,0.3,View 0
EXPERIMENTS,0.3020833333333333,View 1
EXPERIMENTS,0.30416666666666664,View 2
EXPERIMENTS,0.30625,View 3
EXPERIMENTS,0.30833333333333335,"Figure 5: Qualitative comparisons on one challenging test case on ScannetV2. For each predicted
surface with red and sky-blue colors, sky-blue indicates â€œpositive precisionâ€ for that surface region,
while red indicates â€œnegative precisionâ€. The ground truth surface is shown on the top-right corner
of each prediction with gold and navy-blue colors, navy-blue indicates â€œpositive recallâ€, while gold
indicates â€œnegative recallâ€. The larger the blue region is, the higher the F1 score would be."
EXPERIMENTS,0.3104166666666667,Table 2: Benchmarking results of single view 3D surface reconstruction on ScannetV2 test set.
EXPERIMENTS,0.3125,"Acc (â†“)
Compl (â†“)
Chamfer (â†“)
Prec (â†‘)
Recall (â†‘)
F1-score (â†‘)
OccNet + GeoReg
18.3
18.5
18.4
30.4
28.5
26.8
DISN + TSDFVox
13.0
53.3
33.1
25.3
10.2
12.4
CoReNet
19.5
15.6
17.6
30.9
36.3
29.9
MiDaS
13.9
18.5
16.2
41.9
30.1
34.4
AdelaiDepth
9.7
18.8
14.2
46.4
33.2
37.9
Ablation-NoGrad
17.0
11.7
14.3
34.1
47.0
36.5
Ablation-FixedE
15.0
20.3
17.7
31.6
29.8
27.3
DGS
14.3
11.5
12.9
39.7
49.4
41.6"
EXPERIMENTS,0.3145833333333333,"framework along with the differentiable gradient sampling layer implementation plays the critical
role when learning with only the near-surface training labels, and can achieve similar performance
without the dense occupancy training labels.
Please refer to Sec. A for ablation baselines details and low-realism ShapeNet experimental settings."
EXPERIMENTS,0.31666666666666665,"4.2
LEARNING FROM REAL SCANNED DATASETS (SCANNETV2)"
EXPERIMENTS,0.31875,"Dataset. We use ScannetV2 (Dai et al., 2017) for training and evaluating the performance of the
models on the real images. We follow the standard training / testing split as used in Sun et al. (2021)
and Murez et al. (2020), where 1513 scenes are for trainval (with 1201 for training and 312 for
validation), and 100 for testing. Each scene is provided with multiple image capture as well as the
associated camera pose. We train the models with all the views given in the training / validation set
(2423872 frames in total, after filtering out frames with invalid extrinsic poses), while for testing, we
select 10 frames with different extrinsic poses for each test scene. Practically, since all the frames
of the scenes are in the form of video clips, with adjacent frames associated with similar extrinsic
cameras poses, we select the 10 frames for each frame via extracting every 100 frames from each
scene video (e.g. Frame 1, 101, 201, ..., 901), resulting in 1000 frames in total in our test set (10
frames per scene, with 100 test scenes)."
EXPERIMENTS,0.32083333333333336,"Metrics. We use the same evaluation metrics following Sun et al. (2021) and Murez et al. (2020).
Since we are the first, to our knowledge, to evaluate single view 3D implicit reconstruction on the
ScannetV2 benchmark, and here we only evaluate the geometries within the camera view frustum
rather than the whole scene geometries as in Sun et al. (2021); Murez et al. (2020). In addition,
we also only evaluate the geometries in front of the amodal depth for each pixel ray, where only
the space in front of the wall, bounded within the ceiling and the floor, is evaluated. We define the
amodal depth for a pixel ray to be delineated by the minimum between the closest structure-category
surface (e.g. walls and doors, etc) and the farthest surface. In practice, in order to accommodate the
evaluation of the surfaces right on the amodal depth, we slack the evaluation scope with a factor Î»
(1.05 in our case) multiplied with the amodal depth. This evaluation protocol would be equivalent
to the â€œsingle-layeredâ€ protocol used in Sun et al. (2021), within our single-view scenario."
EXPERIMENTS,0.3229166666666667,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.325,"Input
OccNet + GeoReg CoReNet
DGS
AdelaiDepth
NoGrad
FixedE"
EXPERIMENTS,0.32708333333333334,View 0
EXPERIMENTS,0.32916666666666666,View 1
EXPERIMENTS,0.33125,View 2
EXPERIMENTS,0.3333333333333333,View 3
EXPERIMENTS,0.33541666666666664,Figure 6: Qualitative comparisons on an unseen test image downloaded from the internet.
EXPERIMENTS,0.3375,"View 0
View 2
View 1
View 3
View 4
View 5"
EXPERIMENTS,0.33958333333333335,Figure 7: Two representative failure cases of our approach.
EXPERIMENTS,0.3416666666666667,"Due to the inherent ambiguity of the scaling and shifting of the predicted 3D single view geometry
(Ranftl et al., 2019; Yin et al., 2021), we follow them by computing the best scale and shift com-
paring the predicted depth with the ground truth depth. For approaches that predict 3D surfaces, the
rendered depth map from the predicted mesh would be used for calculating the scale and shift.
Please refer to Sec. B for details on baselines.
Results. We provide quantitative comparison in Tab. 2 and qualitative comparison in Fig. 5 re-
spectively. Our model outperforms all state-of-the-art approaches as well as the ablation models.
Compared to the synthetic data scenario in Tab. 1 and 4, our model demonstrates large advantages
when compared to existing state-of-the-art approaches, as our motivation stems from addressing
learning from imperfect 3D labels directly from the real scan data. Compared to single-view depth
prediction approaches trained on massive data (Lasinger et al., 2019; Yin et al., 2021), our approach
does not prevail on â€œAccâ€ and â€œPrecâ€. This is due to the fact that these two metrics only project the
predicted surface to the ground truth surface, giving advantages to approaches that only predict the
visible surface. Our approach still prevails for other metrics (Chamfer Distance and F1), which are
considered as the most important metrics (BoË‡ziË‡c et al., 2021; Sun et al., 2021; Murez et al., 2020)."
EXPERIMENTS,0.34375,"Generalization to Unseen Scenes (Pix3d and Open-Domain Images). We further test our model
without further finetuning directly on unseen scenes for evaluating the generalizability of the learned
model. We run our model on Pix3d Sun et al. (2018) as well as test images downloaded directly from
the internet. We provide a detailed qualitative comparison in Fig. 6 and more results in Fig. 1. The
results further indicate our learning framework exhibits promise for unseen scene generalization."
EXPERIMENTS,0.3458333333333333,"Failure Cases.
We provide two representative failure cases of our approach in Fig. 7. The first
case (the first row) demonstrates difficulties in predicting the floor occupancy as a result of the noisy
and non-watertight mesh during training. The second case (the second row) shows that our model
cannot identify small objects (e.g. chairs) and not predict the invisible partition of these objects."
CONCLUSIONS,0.34791666666666665,"5
CONCLUSIONS"
CONCLUSIONS,0.35,"We have presented our learning framework for real-world 3D implicit surface reconstruction from a
single view image. Owing to our unique learning framework that directly trains the model from the
raw scan data and our novel occupancy loss function over the gradients, we are able to go beyond the
existing works for single-objects reconstruction or single view fitting. Thanks to our differentiable
gradient sampling module we enable efficient and memory-efficient end-to-end training from images
and demonstrated single view 3D surface reconstruction results on scenes for the first time, to our
knowledge."
CONCLUSIONS,0.35208333333333336,Published as a conference paper at ICLR 2022
REFERENCES,0.3541666666666667,REFERENCES
REFERENCES,0.35625,"MartÂ´Ä±n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th {USENIX} symposium on operating systems design and imple-
mentation ({OSDI} 16), pp. 265â€“283, 2016."
REFERENCES,0.35833333333333334,"Matan Atzmon and Yaron Lipman. Sal: Sign agnostic learning of shapes from raw data. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2565â€“2574,
2020."
REFERENCES,0.36041666666666666,"AljaË‡z BoË‡ziË‡c, Pablo Palafox, Justus Thies, Angela Dai, and Matthias NieÃŸner. Transformerfusion:
Monocular rgb scene reconstruction using transformers. arXiv preprint arXiv:2107.02191, 2021."
REFERENCES,0.3625,"Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva,
Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor
environments. arXiv preprint arXiv:1709.06158, 2017."
REFERENCES,0.3645833333333333,"Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,
Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d
model repository. arXiv preprint arXiv:1512.03012, 2015."
REFERENCES,0.36666666666666664,"Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5939â€“5948,
2019a."
REFERENCES,0.36875,"Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In IEEE
Computer Vision and Pattern Recognition (CVPR), 2019b."
REFERENCES,0.37083333333333335,"Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A
unified approach for single and multi-view 3d object reconstruction. In European conference on
computer vision, pp. 628â€“644. Springer, 2016."
REFERENCES,0.3729166666666667,"Michael G Crandall and Pierre-Louis Lions.
Viscosity solutions of hamilton-jacobi equations.
Transactions of the American mathematical society, 277(1):1â€“42, 1983."
REFERENCES,0.375,"Brian Curless and Marc Levoy. A volumetric method for building complex models from range
images. In Proceedings of the 23rd annual conference on Computer graphics and interactive
techniques, pp. 303â€“312, 1996."
REFERENCES,0.3770833333333333,"Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias
NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 5828â€“5839, 2017."
REFERENCES,0.37916666666666665,"Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object recon-
struction from a single image. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 605â€“613, 2017."
REFERENCES,0.38125,"Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regu-
larization for learning shapes. arXiv preprint arXiv:2002.10099, 2020."
REFERENCES,0.38333333333333336,"T Groueix, M Fisher, VG Kim, BC Russell, and M Aubry. Atlasnet: a papier-mË†achÂ´e approach to
learning 3d surface generation (2018). arXiv preprint arXiv:1802.05384, 11."
REFERENCES,0.3854166666666667,"Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Advances
in neural information processing systems, 28:2017â€“2025, 2015."
REFERENCES,0.3875,"Yue Jiang, Dantong Ji, Zhizhong Han, and Matthias Zwicker. Sdfdiff: Differentiable rendering of
signed distance fields for 3d shape optimization. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 1251â€“1261, 2020."
REFERENCES,0.38958333333333334,"Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3d mesh renderer. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pp. 3907â€“3916, 2018."
REFERENCES,0.39166666666666666,Published as a conference paper at ICLR 2022
REFERENCES,0.39375,"Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.3958333333333333,"Katrin Lasinger, RenÂ´e Ranftl, Konrad Schindler, and Vladlen Koltun.
Towards robust monoc-
ular depth estimation: Mixing datasets for zero-shot cross-dataset transfer.
arXiv preprint
arXiv:1907.01341, 2019."
REFERENCES,0.39791666666666664,"Manyi Li and Hao Zhang. D2im-net: Learning detail disentangled implicit fields from single images.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
10246â€“10255, 2021."
REFERENCES,0.4,"Chen-Hsuan Lin, Chaoyang Wang, and Simon Lucey. Sdf-srn: Learning signed distance 3d object
reconstruction from static images. arXiv preprint arXiv:2010.10505, 2020."
REFERENCES,0.40208333333333335,"Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft rasterizer: A differentiable renderer for
image-based 3d reasoning. The IEEE International Conference on Computer Vision (ICCV), Oct
2019."
REFERENCES,0.4041666666666667,"William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface construction
algorithm. ACM siggraph computer graphics, 21(4):163â€“169, 1987."
REFERENCES,0.40625,"Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.
Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 4460â€“4470, 2019."
REFERENCES,0.4083333333333333,"Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis, 2020."
REFERENCES,0.41041666666666665,"Zak Murez, Tarrence van As, James Bartolozzi, Ayan Sinha, Vijay Badrinarayanan, and Andrew
Rabinovich. Atlas: End-to-end 3d scene reconstruction from posed images. In Computer Visionâ€“
ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part
VII 16, pp. 414â€“431. Springer, 2020."
REFERENCES,0.4125,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumet-
ric rendering: Learning implicit 3d representations without 3d supervision. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3504â€“3515, 2020."
REFERENCES,0.41458333333333336,"Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces
and radiance fields for multi-view reconstruction. arXiv preprint arXiv:2104.10078, 2021."
REFERENCES,0.4166666666666667,"Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.
Deepsdf: Learning continuous signed distance functions for shape representation. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 165â€“174, 2019."
REFERENCES,0.41875,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in neural information processing systems, pp.
8026â€“8037, 2019."
REFERENCES,0.42083333333333334,"Stefan Popov, Pablo Bauszat, and Vittorio Ferrari. Corenet: Coherent 3d scene reconstruction from
a single rgb image. In European Conference on Computer Vision, pp. 366â€“383. Springer, 2020."
REFERENCES,0.42291666666666666,"RenÂ´e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust
monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. arXiv preprint
arXiv:1907.01341, 2019."
REFERENCES,0.425,"Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li.
Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proceed-
ings of the IEEE International Conference on Computer Vision, pp. 2304â€“2314, 2019."
REFERENCES,0.4270833333333333,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.42916666666666664,Published as a conference paper at ICLR 2022
REFERENCES,0.43125,"Vincent Sitzmann, Michael ZollhÂ¨ofer, and Gordon Wetzstein. Scene representation networks: Con-
tinuous 3d-structure-aware neural scene representations. arXiv preprint arXiv:1906.01618, 2019."
REFERENCES,0.43333333333333335,"Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-
plicit neural representations with periodic activation functions. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.4354166666666667,"Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser.
Semantic scene completion from a single depth image. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 1746â€“1754, 2017."
REFERENCES,0.4375,"Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J.
Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge,
Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler
Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat,
Renzo De Nardi, Michael Goesele, Steven Lovegrove, and Richard Newcombe. The Replica
dataset: A digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019."
REFERENCES,0.4395833333333333,"Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time
coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 15598â€“15607, 2021."
REFERENCES,0.44166666666666665,"Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue,
Joshua B Tenenbaum, and William T Freeman. Pix3d: Dataset and methods for single-image
3d shape modeling. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2974â€“2983, 2018."
REFERENCES,0.44375,"Shubham Tulsiani, Tinghui Zhou, Alexei A Efros, and Jitendra Malik.
Multi-view supervision
for single-view reconstruction via differentiable ray consistency. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 2626â€“2634, 2017."
REFERENCES,0.44583333333333336,"Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus:
Learning neural implicit surfaces by volume rendering for multi-view reconstruction.
arXiv
preprint arXiv:2106.10689, 2021."
REFERENCES,0.4479166666666667,"Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view
synthesis from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 7467â€“7477, 2020."
REFERENCES,0.45,"Saining Xie, Ross Girshick, Piotr DollÂ´ar, Zhuowen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 1492â€“1500, 2017."
REFERENCES,0.45208333333333334,"Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich Neumann. Disn: Deep
implicit surface network for high-quality single-view 3d reconstruction. In Advances in Neural
Information Processing Systems, pp. 490â€“500, 2019."
REFERENCES,0.45416666666666666,"Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, and Yaron Lip-
man. Multiview neural surface reconstruction by disentangling geometry and appearance. arXiv
preprint arXiv:2003.09852, 2020."
REFERENCES,0.45625,"Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces.
arXiv preprint arXiv:2106.12052, 2021."
REFERENCES,0.4583333333333333,"Wang Yifan, Felice Serena, Shihao Wu, Cengiz Â¨Oztireli, and Olga Sorkine-Hornung. Differentiable
surface splatting for point-based geometry processing. ACM Transactions on Graphics (TOG),
38(6):1â€“14, 2019."
REFERENCES,0.46041666666666664,"Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua
Shen. Learning to recover 3d scene shape from a single image. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 204â€“213, 2021."
REFERENCES,0.4625,"Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from
one or few images. arXiv preprint arXiv:2012.02190, 2020."
REFERENCES,0.46458333333333335,Published as a conference paper at ICLR 2022
REFERENCES,0.4666666666666667,"Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for
real-time rendering of neural radiance fields. arXiv preprint arXiv:2103.14024, 2021."
REFERENCES,0.46875,Published as a conference paper at ICLR 2022
REFERENCES,0.4708333333333333,APPENDIX
REFERENCES,0.47291666666666665,"In this document, we provide additional experimental details and results on ShapeNet in Sec. A,
additional qualitative results on ScannetV2 in Sec. B, analysis of numerical gradient approximation
in Sec. C, additional results generalizing to external unseen scenes in Sec. D and E, as well as the
details of the formulation and derivations of DGS in Sec. F."
REFERENCES,0.475,"A
ADDITIONAL DETAILS AND RESULTS ON SHAPENET"
REFERENCES,0.47708333333333336,"Additional Quantitative Results. We provide our original results on high-realism ShapeNet in
Tab. 3. These results are evaluated with the 64 Ã— 64 Ã— 64 volume grids due to an error in our prior
code base. Based on the results in both Tab. 1 and Tab. 3, we found our proposed learning framework
performs competitively with the oracle training setting, even though our approach does not utilize
dense occupancy labels as used in the oracle state-of-the-art approaches (OccNet, DISN, CoReNet
and DISN (ResNet50) + DVR). On the other hand, we observe that our model demonstrates slight
improvements when evaluated with the resolution of 64Ã—64Ã—64, while evaluating the same models
with the resolution of 128 Ã— 128 Ã— 128 would reverse the course. In particular, we found categories
like â€œcarâ€ exhibited major performance decrease on all the approaches with the higher resolution
evaluating setting. This might be due to the fact that these categories demonstrate empty ground
truth inside the objects, and and at a higher resolution, the metrics are biased to penalize models
that predict â€œoccupieâ€ inside the object. However, the competitive performance in both resolution
settings indicates that our learning with only the surface labels does not deteriorate the performance,
and can be faithfully applied toward the real-scene settings where the dense occupancy labels are
truly unavailable."
REFERENCES,0.4791666666666667,"Table 3: Intersection over Union % (IoU â†‘) benchmarking result on the high-realism ShapeNet with
the resolution of 64 Ã— 64 Ã— 64. Please refer to Tab. 1 (the resolution of 128 Ã— 128 Ã— 128) for details."
REFERENCES,0.48125,"Category
Craft
Rifle
Disp.
Lamp
Spk.
Box
Chair
Bench
Car
Plane
Sofa
Table
Phone
Mean
OccNet
54.7
48.7
55.5
38.6
57.1
52.0
48.9
39.7
70.6
48.9
63.6
49.2
71.7
53.8
DISN
60.9
63.5
56.9
46.4
61.5
55.2
57.4
46.7
72.5
57.2
68.2
55.7
74.5
59.7
CoReNet
62.5
65.5
63.2
48.2
63.0
56.7
60.6
48.3
73.7
58.1
69.8
55.0
75.1
61.5
DISN(Res50)+DVR
66.9
74.4
67.3
54.5
66.1
63.2
66.2
58.6
71.2
67.2
72.5
63.1
82.6
67.2
Abl.-NoGrad
14.1
6.1
27.3
18.1
37.3
40.4
28.7
13.0
26.2
9.6
30.5
23.4
25.1
23.1
Abl.-NoGrad (10 %)
28.2
25.8
39.3
32.4
48.2
46.5
42.0
24.6
42.4
20.0
42.7
37.1
49.4
36.8
Abl.-NoGrad (30 %)
34.1
25.1
46.6
30.8
49.8
47.9
45.1
28.1
51.4
26.5
54.9
38.0
52.7
40.8
Abl.-NoGrad (50 %)
47.8
41.5
47.6
35.1
54.1
49.2
46.8
32.1
66.0
37.9
59.6
44.4
60.2
47.9
Abl.-FixedE
52.6
57.0
47.8
37.4
52.6
49.7
47.4
33.0
69.2
51.0
60.2
42.9
69.2
51.5
OccNet w/ Eq. 1
56.8
52.6
56.8
39.3
58.1
53.6
51.1
42.4
71.7
49.7
65.5
50.9
76.0
55.7
DISN w/ Eq. 1
60.4
63.9
59.6
46.2
61.1
55.8
58.3
46.6
73.2
57.3
68.7
55.5
76.7
60.2
CoReNet w/ Eq. 1 (DGS)
63.3
70.4
65.7
49.0
61.2
57.0
62.1
50.9
70.1
62.8
70.3
56.9
78.3
62.9
DISN(Res50)+DVR
w/ Eq. 1 (DGS Best)
66.8
75.1
68.7
55.1
65.7
63.7
66.5
58.2
71.5
67.4
72.9
63.5
84.2
67.6"
REFERENCES,0.48333333333333334,"Ablation Study Details. We conduct ablation studies to further validate the importance of derived
DGS module, via attempting work-around training methods without DGS."
REFERENCES,0.48541666666666666,"i) NoGrad - To test the performance of the baseline when only the surface data are provided, we
train this ablation model in exactly the same way compared to its original model, with the only
exceptions that only the near-surface points are equipped with training labels, and we do not use any
training labels from the non-surface query points (where it is not necessary to backpropagate the
loss gradient of the spatial gradient using DGS). To further evaluate how the rate of known voxels
affects the learning performance, we enlarge the near-surface region and evaluate when the rate is
10%, 30% and 50%. An increase in the performances among these baselines would indicate the
importance of knowing more voxel labels if our proposed gradient loss (Eq. 1) is not imposed.
ii) FixedE - We train with both the near-surface as well as the spatial gradient supervision, without
DGS - meaning the loss gradient of the spatial gradient would not back-propagate to any module
before the sampling module - in our case, the feature encoder network. Note all the other losses
without gradient sampling can still back-propagated to the feature maps.
We report the ablation results in Tab. 1. Both experiments are conducted with the high-realism
ShapeNet data. We comfortably outperform all the in-house ablation baselines, validating the essen-
tial roles of DGS in our learning framework."
REFERENCES,0.4875,Published as a conference paper at ICLR 2022
REFERENCES,0.4895833333333333,"Experiments on low-realism ShapeNet. For our low-realism evaluation setting, all the baseline
approaches reported their results in the papers. For OccNet and DISN, the reported results are
based on knowing the category canonical view prior, which demonstrates considerable privilege
with respect to accuracy Popov et al. (2020). Hence, we mark the results from the literature as
OccNet-Privilege and DISN-Privilege (OccNet-Priv. and DISN-Priv. for short). Compared to their
privileged setting, our results demonstrate superior results even without the category canonical view
prior privilege. To further provide the the baseline results where these two approaches are without
the category canonical view prior, we retrain their models with the released codes, and report the
results in the â€œOccNetâ€ and â€œDISNâ€ row respectively. The results further provide evidence that
the category canonical view prior demonstrates privilege on accuracy, as observed in Popov et al.
(2020)."
REFERENCES,0.49166666666666664,"Qualitative Results. We provide qualitative results in Fig. 9-13 as the additional illustration of the
performance of all the approaches on ShapeNet."
REFERENCES,0.49375,"Table 4: Intersection over Union % (IoU â†‘) benchmarking result on the low-realism ShapeNet. Our
proposed DGS learning advances state-of-the-art approaches (OccNet, DISN, CoReNet and D2IM-
Net) compared to the reported performance from the literatures.
Category
Craft
Rifle
Disp.
Lamp
Spk.
Box
Chair
Bench
Car
Plane
Sofa
Table
Phone
Mean
OccNet-Priv.
53.0
47.4
47.1
37.1
64.7
73.3
50.1
48.5
73.7
57.1
68.0
50.6
72.0
57.1
DISN-Priv.
60.2
68.0
57.7
39.7
55.9
53.1
54.9
54.2
77.0
61.7
67.1
48.9
73.6
59.4
OccNet
49.9
48.0
55.4
39.5
57.0
43.8
58.5
45.1
54.0
45.8
68.0
50.7
68.3
52.6
DISN
49.0
44.4
55.5
39.0
67.3
71.7
49.0
41.2
64.7
45.0
66.4
50.7
70.5
55.0
CoReNet
54.0
64.6
57.2
42.1
60.8
50.9
63.0
50.8
57.3
53.0
70.6
55.5
73.1
57.9
D2IM-Net
63.4
68.1
52.7
42.1
51.8
48.6
56.1
55.0
79.8
55.8
65.4
53.7
76.2
59.1
DGS
57.0
65.7
58.6
45.5
58.6
55.2
59.8
48.2
71.6
56.8
68.1
55.6
78.4
60.0"
REFERENCES,0.49583333333333335,"B
ADDITIONAL RESULTS ON SCANNETV2"
REFERENCES,0.4979166666666667,"Baselines. Since we demonstrate the first attempt to predict 3D implicit surface of scenes from a
single image, very few exiting works provide a direct baseline performance to our task. For Occ-
Net (Mescheder et al., 2019), since its image feature extraction is not local, and its gradient propa-
gation does not require sampling, we use its direct application with Gropp et al. (2020) â€œOccNet +
GeoRegâ€ as one baseline. We train DISN (Xu et al., 2019) with the TSDF voxelization labels (Murez
et al., 2020; Sun et al., 2021; BoË‡ziË‡c et al., 2021). For CoReNet (Popov et al., 2020), we stick to its
own voxelization and internal filling toolbox for obtaining the training labels. Since CoReNet can
only predict geometries within the fixed range of space, we tried our best to pick the best cube loca-
tion based on the dataset statistics. We also incorporate depth approaches (Ranftl et al., 2019; Yin
et al., 2021) for comparison by finetuning their weights on ScannetV2. Lastly, we compare with the
two ablation models NoGrad and FixedE as introduced in Sec. 4.1."
REFERENCES,0.5,"Implementation details for DGS. Since the DISN variant architectures (Xu et al., 2019; Niemeyer
et al., 2020; Yu et al., 2020) demonstrate higher degress of flexibility of representing any point
in the space, in contrast to 3D convolutions that are fixed for a particular range of space (Popov
et al., 2020), we build our model based on the former architectures. Similar to Niemeyer et al.
(2020); Yu et al. (2020), we use 5 residual blocks in the fully connected part with the first 3 blocks
receiving the 2D features and its spatial gradients. Our image encoder uses the same RexNext101
architecture (Xie et al., 2017) as in Yin et al. (2021) and starts the training with the pre-trained
weights. We use a batch size of 32 images with 2048 near-surface points and 512 non-surface points
per image. Similar to the ShapeNet experiments, we set Î»or to be 0.01. We found in the real-world
setting, this parameter setting is crucial for the convergence of the learning procedure."
REFERENCES,0.5020833333333333,"Quantitative Results for 2.5D evaluation metrics. We provide the quantitative 2.5D evaluation
results in Tab. 5. Our approach constantly outperforms all other implicit based baselines. While our
approach falls short slightly when compared to the existing depth-based approaches, we claim that
our approach is not directly trained with the massive depth training data. Our results indicate that
our depth performance is still on par with the state-of-the-art depth prediciton approaches."
REFERENCES,0.5041666666666667,Additional qualitative results. We provide additional qualitative results on ScannetV2 Fig. 14-18.
REFERENCES,0.50625,Published as a conference paper at ICLR 2022
REFERENCES,0.5083333333333333,"Table 5: Benchmarking results of the depth metrics on ScannetV2.
AbsRel (â†“)
AbsDiff (â†“)
SqRel (â†“)
RMSE (â†“)
LogRMSE (â†“)
Î´1 (â†‘)
Î´2 (â†‘)
Î´3 (â†‘)
OccNet + GeoReg
13.5
23.7
7.2
33.2
17.3
82.2
95.5
99.0
DISN + TSDFVox
15.8
26.9
9.2
34.9
18.8
77.3
94.1
98.5
CoReNet
12.6
21.4
6.6
30.4
16.3
84.5
96.0
98.9
MiDaS
9.3
15.8
3.1
21.2
11.7
91.4
98.7
99.8
AdelaiDepth
6.1
10.5
1.9
15.7
8.6
95.3
99.1
99.8
Ablation-Fair
10.5
17.6
5.3
26.6
14.9
88.2
96.4
98.9
Ablation-FixedE
14.4
24.5
7.6
32.9
17.9
81.4
95.4
98.9
DGS
8.9
14.7
4.1
22.7
12.7
91.1
97.5
99.3"
REFERENCES,0.5104166666666666,"C
ANALYSIS WITH NUMERICAL GRADIENT APPROXIMATION"
REFERENCES,0.5125,"As part of our proposed learning framework with the loss imposed on the spatial gradient, the dif-
ferentiable gradient sampling module (Sec. 3.2) has a numerical alternative where we can perturb
the query points to get its simulated numerical gradients. We further compare the closed-form per-
formance with the numerical counterpart in Tab. 6. Please find qualitative comparison in Fig. 9-13."
REFERENCES,0.5145833333333333,"We provide a closer look at our comparison to the simulated gradient baseline. We found in our
experiments that the simulated gradient baseline demonstrates relatively severe convergence diffi-
culties. As shown in Fig. 8, the simulated gradient model (red) does not observe loss drop in the
first 10k training iterations. Despite its subsequent loss drop, which indicates the simulated gradient
model can still learn to predict the geometry in moderate accuracy, its converged training loss is
still higher than our DGS (blue). This aligns with our benchmarking evaluations in Tab. 6 that the
closed-form variant (DGS) achieve better performance."
REFERENCES,0.5166666666666667,"Table 6: Intersection over Union % (IoU â†‘) benchmarking comparison between the closed-form
solution and the numerical gradients on the high-realism ShapeNet. We report the performance
comparison with both the resolution settings of 64 Ã— 64 Ã— 64 and 128 Ã— 128 Ã— 128 (Please refer to
Sec. A for details).
Category
Craft
Rifle
Disp.
Lamp
Spk.
Box
Chair
Bench
Car
Plane
Sofa
Table
Phone
Mean
Numerical (64)
62.4
68.3
63.7
46.2
60.6
54.9
61.0
49.0
70.7
61.1
69.2
55.6
77.2
61.5
Closed-form (64)
63.3
70.4
65.7
49.0
61.2
57.0
62.1
50.9
70.1
62.8
70.3
56.9
78.3
62.9
Numerical (128)
59.7
66.9
61.3
41.8
54.1
48.5
58.2
43.8
59.8
59.0
68.5
54.3
77.0
57.9
Closed-form (128)
61.1
67.5
62.7
44.2
54.8
49.6
59.5
45.4
59.4
59.9
69.8
55.1
78.0
59.0"
REFERENCES,0.51875,"0
0.5
1
1.5
2
2.5
3
Iterations
105 0.3 0.4 0.5 0.6 0.7 0.8"
REFERENCES,0.5208333333333334,The Total Loss
REFERENCES,0.5229166666666667,"Numerical
Closed-Form"
REFERENCES,0.525,"Figure 8: Convergence Analysis for the comparison between the closed-form (blue) and the numer-
ical counterpart (red). Notably, the numerical counterpart does not observe loss drop in the first 10k
iterations."
REFERENCES,0.5270833333333333,"D
ADDITIONAL GENERALIZABILITY QUALITATIVE RESULTS"
REFERENCES,0.5291666666666667,"We provide more qualitative results on our model generalizing to unseen images downloaded from
the Internet in Fig. 19. Each result is visualized in 6 views. Once again, these additional visual
results further indicate that our model demonstrates good generalizability capability for handling
unseen indoor scene images."
REFERENCES,0.53125,Published as a conference paper at ICLR 2022
REFERENCES,0.5333333333333333,"Table 7: Benchmarking results of single view 3D surface reconstruction on Matterport3D test set
(trained by the ScannetV2 dataset)."
REFERENCES,0.5354166666666667,"Acc (â†“)
Compl (â†“)
Chamfer (â†“)
Prec (â†‘)
Recall (â†‘)
F1-score (â†‘)
OccNet + GeoReg
24.8
33.3
29.0
21.9
27.7
23.4
DISN + TSDFVox
32.7
25.6
29.1
19.1
33.3
22.9
CoReNet
35.2
23.3
29.3
20.8
35.5
24.9
AdelaiDepth
14.1
32.9
23.5
33.7
28.7
29.9
Ablation-NoGrad
39.5
20.7
30.1
22.9
43.6
28.7
Ablation-FixedE
22.7
34.7
28.7
19.5
24.0
20.2
DGS
24.4
22.1
23.2
25.7
41.8
30.5"
REFERENCES,0.5375,"E
EVALUATION OF THE GENERALIZABILITY TO MATTERPORT3D DATASET"
REFERENCES,0.5395833333333333,"To quantatively evaluate our generaizability to new datasets, we provide our results on the Matter-
port3D dataset (Chang et al., 2017) using our model trained from ScannetV2 as used in Sec. 4.2."
REFERENCES,0.5416666666666666,"The Matterport3d dataset (Chang et al., 2017) collects indoor scenes of 90 full houses. Textured
meshes are provided along with the scanned real images. Since the Matterport3D dataset does not
provide an official train/test split, and our model is not trained on Matterport3d, we evaluate our
model over all the 90 scenes in the dataset. In particular, we use the 129600 images (without the
elevated pitch views where the camera is looking up at the ceiling) from the dataset, use the 10
first images from each scene (resulting in 900 test images in total), and test their reconstruction
performance in the same way as we tested on ScannetV2. We provide the quantative evaluation
result in Tab. 7. We can see from the table that the results align with our evaluation in Tab. 2 that
our model demonstrates clear advantages. We also noticed that the AdelaiDepth Yin et al. (2021)
baseline (finetuned on ScannetV2) performance is closer to our results (F1) score compared to our
ScannetV2 evaluation. This is probably due to the fact that AdelaiDepth was directly pre-trained
from a massive number of training images and potentially gives it some privilege when generalize
to a new dataset."
REFERENCES,0.54375,"F
DETAILS OF THE FORMULATION AND DERIVATION OF DGS"
REFERENCES,0.5458333333333333,"In this section, we provide the full formulation as well as the derivation procedure of our DGS in
this section as the supplementary to Sec. 3.2."
REFERENCES,0.5479166666666667,"In Eq. 4, we provided the backward gradient for differentiable sampling for Pixel A. The backward
gradient for Pixel B, C and D are"
REFERENCES,0.55,"âˆ‚L
âˆ‚Ï•B
= (1 âˆ’Î±)Î² Â·
âˆ‚L
âˆ‚Ï•(i, j)
âˆ‚L
âˆ‚Ï•C
= Î±(1 âˆ’Î²) Â·
âˆ‚L
âˆ‚Ï•(i, j)
âˆ‚L
âˆ‚Ï•D
= Î±Î² Â·
âˆ‚L
âˆ‚Ï•(i, j) (9)"
REFERENCES,0.5520833333333334,"In Eq. 5, we provided the forward gradient in the vertical pixel direction. The forward gradient of
the horizontal pixel direction is"
REFERENCES,0.5541666666666667,"âˆ‚Ï•(i, j)"
REFERENCES,0.55625,"âˆ‚j
= (1 âˆ’Î±)(Ï•B âˆ’Ï•A) + Î±(Ï•D âˆ’Ï•C)"
REFERENCES,0.5583333333333333,"w
(10)"
REFERENCES,0.5604166666666667,"In Eq. 6, we provided the backward gradient for only Pixel A. The backward gradient for Pixel B, C
and D are"
REFERENCES,0.5625,Published as a conference paper at ICLR 2022
REFERENCES,0.5645833333333333,"âˆ‚L
âˆ‚Ï•B
= (1 âˆ’Î±)Î² Â·
âˆ‚L
âˆ‚Ï•(i, j) + (âˆ’Î²"
REFERENCES,0.5666666666666667,"h) Â·
âˆ‚L
âˆ‚âˆ‡iÏ•(i, j) + 1 âˆ’Î±"
REFERENCES,0.56875,"w
Â·
âˆ‚L
âˆ‚âˆ‡jÏ•(i, j)
âˆ‚L
âˆ‚Ï•C
= Î±(1 âˆ’Î²) Â·
âˆ‚L
âˆ‚Ï•(i, j) + 1 âˆ’Î²"
REFERENCES,0.5708333333333333,"h
Â·
âˆ‚L
âˆ‚âˆ‡iÏ•(i, j) + (âˆ’Î±"
REFERENCES,0.5729166666666666,"w) Â·
âˆ‚L
âˆ‚âˆ‡jÏ•(i, j)
âˆ‚L
âˆ‚Ï•D
= Î±Î² Â·
âˆ‚L
âˆ‚Ï•(i, j) + Î²"
REFERENCES,0.575,"h Â·
âˆ‚L
âˆ‚âˆ‡iÏ•(i, j) + Î±"
REFERENCES,0.5770833333333333,"w Â·
âˆ‚L
âˆ‚âˆ‡jÏ•(i, j) (11)"
REFERENCES,0.5791666666666667,"In Eq. 8, we provided the backward gradient for only Pixel A in the 3D setting. The backward
gradient for Pixel B, C and D are"
REFERENCES,0.58125,"âˆ‚L
âˆ‚Ï•B
=
âˆ‚L
âˆ‚Ï•(x, y, z) Â· (1 âˆ’Î±)Î² +
âˆ‚L
âˆ‚âˆ‡xÏ•(x, y, z) Â· 1 âˆ’Î±"
REFERENCES,0.5833333333333334,"w
Â· f0"
REFERENCES,0.5854166666666667,"z +
âˆ‚L
âˆ‚âˆ‡yÏ•(x, y, z) Â· (âˆ’Î²"
REFERENCES,0.5875,h) Â· f0 z
REFERENCES,0.5895833333333333,"+
âˆ‚L
âˆ‚âˆ‡zÏ•(x, y, z) Â· (Î²"
REFERENCES,0.5916666666666667,h Â· yf0
REFERENCES,0.59375,z2 âˆ’1 âˆ’Î±
REFERENCES,0.5958333333333333,"w
Â· xf0 z2 )"
REFERENCES,0.5979166666666667,"âˆ‚L
âˆ‚Ï•C
=
âˆ‚L
âˆ‚Ï•(x, y, z) Â· Î±(1 âˆ’Î²) +
âˆ‚L
âˆ‚âˆ‡xÏ•(x, y, z) Â· (âˆ’Î±"
REFERENCES,0.6,w) Â· f0
REFERENCES,0.6020833333333333,"z +
âˆ‚L
âˆ‚âˆ‡yÏ•(x, y, z) Â· 1 âˆ’Î²"
REFERENCES,0.6041666666666666,"h
Â· f0 z"
REFERENCES,0.60625,"+
âˆ‚L
âˆ‚âˆ‡zÏ•(x, y, z) Â· (âˆ’1 âˆ’Î²"
REFERENCES,0.6083333333333333,"h
Â· yf0"
REFERENCES,0.6104166666666667,z2 + Î±
REFERENCES,0.6125,w Â· xf0 z2 )
REFERENCES,0.6145833333333334,"âˆ‚L
âˆ‚Ï•D
=
âˆ‚L
âˆ‚Ï•(x, y, z) Â· Î±Î² +
âˆ‚L
âˆ‚âˆ‡xÏ•(x, y, z) Â· Î±"
REFERENCES,0.6166666666666667,w Â· f0
REFERENCES,0.61875,"z +
âˆ‚L
âˆ‚âˆ‡yÏ•(x, y, z) Â· Î²"
REFERENCES,0.6208333333333333,h Â· f0 z
REFERENCES,0.6229166666666667,"+
âˆ‚L
âˆ‚âˆ‡zÏ•(x, y, z) Â· (âˆ’Î²"
REFERENCES,0.625,h Â· yf0 z2 âˆ’Î±
REFERENCES,0.6270833333333333,w Â· xf0 z2 ) (12)
REFERENCES,0.6291666666666667,"As an illustration of the derivation for Eq. 8, we can obtain the backward gradient via"
REFERENCES,0.63125,"âˆ‚L
âˆ‚Ï•A
= âˆ‚L"
REFERENCES,0.6333333333333333,âˆ‚Ï• Â· âˆ‚Ï•
REFERENCES,0.6354166666666666,"âˆ‚Ï•A
+
âˆ‚L
âˆ‚âˆ‡xÏ• Â· âˆ‚âˆ‡xÏ•"
REFERENCES,0.6375,"âˆ‚Ï•A
+
âˆ‚L
âˆ‚âˆ‡yÏ• Â· âˆ‚âˆ‡yÏ•"
REFERENCES,0.6395833333333333,"âˆ‚Ï•A
+
âˆ‚L
âˆ‚âˆ‡zÏ• Â· âˆ‚âˆ‡zÏ•"
REFERENCES,0.6416666666666667,"âˆ‚Ï•A
(13)"
REFERENCES,0.64375,"The four partial derivatives with respect to Ï•A in Eq. 13 can be determined based on the sampling
rules. For example, based on Eq. 3, we can quickly obtain the first derivative with respect to Ï•A via"
REFERENCES,0.6458333333333334,"âˆ‚Ï•
âˆ‚Ï•A
= (1 âˆ’Î±)(1 âˆ’Î²)
(14)"
REFERENCES,0.6479166666666667,"For the other three derivatives with respect to Ï•A in Eq. 13, we can substitute Eq. 5, 10 into Eq. 7,
and obtain them via âˆ‚âˆ‡xÏ•"
REFERENCES,0.65,"âˆ‚Ï•A
= âˆ’f0"
REFERENCES,0.6520833333333333,z Â· 1 âˆ’Î±
REFERENCES,0.6541666666666667,"w
âˆ‚âˆ‡yÏ•"
REFERENCES,0.65625,"âˆ‚Ï•A
= âˆ’f0"
REFERENCES,0.6583333333333333,z Â· 1 âˆ’Î²
REFERENCES,0.6604166666666667,"h
âˆ‚âˆ‡zÏ•"
REFERENCES,0.6625,"âˆ‚Ï•A
= yf0"
REFERENCES,0.6645833333333333,z2 Â· 1 âˆ’Î²
REFERENCES,0.6666666666666666,"h
+ xf0"
REFERENCES,0.66875,z2 Â· 1 âˆ’Î± w (15)
REFERENCES,0.6708333333333333,"We can then obtain Eq. 8 via substituting Eq. 14, 15 into Eq. 13."
REFERENCES,0.6729166666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.675,View 0
REFERENCES,0.6770833333333334,View 1
REFERENCES,0.6791666666666667,View 2
REFERENCES,0.68125,View 3
REFERENCES,0.6833333333333333,View 4
REFERENCES,0.6854166666666667,View 5
REFERENCES,0.6875,View 0
REFERENCES,0.6895833333333333,View 1
REFERENCES,0.6916666666666667,View 2
REFERENCES,0.69375,View 3
REFERENCES,0.6958333333333333,View 4
REFERENCES,0.6979166666666666,View 5
REFERENCES,0.7,"Input
OccNet
DISN
CoReNet NoGrad FixedE
DGS
Numerical Gradient
DGS"
REFERENCES,0.7020833333333333,Closed-form Gradient GT
REFERENCES,0.7041666666666667,"Figure 9: Quantitative comparison on the high-realism ShapeNet (without handpick: test case num-
ber 0 and 100). The reconstruction result of each approach is visualized in six different views, with
the first view the same as the camera view, the first three views the same elevation as the camera
view, and the last three view horizontal view."
REFERENCES,0.70625,Published as a conference paper at ICLR 2022
REFERENCES,0.7083333333333334,View 0
REFERENCES,0.7104166666666667,View 1
REFERENCES,0.7125,View 2
REFERENCES,0.7145833333333333,View 3
REFERENCES,0.7166666666666667,View 4
REFERENCES,0.71875,View 5
REFERENCES,0.7208333333333333,View 0
REFERENCES,0.7229166666666667,View 1
REFERENCES,0.725,View 2
REFERENCES,0.7270833333333333,View 3
REFERENCES,0.7291666666666666,View 4
REFERENCES,0.73125,View 5
REFERENCES,0.7333333333333333,"Input
OccNet
DISN
CoReNet NoGrad FixedE
DGS
Numerical Gradient
DGS"
REFERENCES,0.7354166666666667,Closed-form Gradient GT
REFERENCES,0.7375,"Figure 10: Quantitative comparison on the high-realism ShapeNet (without handpick: test case
number 200 and 300). The reconstruction result of each approach is visualized in six different
views, with the first view the same as the camera view, the first three views the same elevation as the
camera view, and the last three view horizontal view."
REFERENCES,0.7395833333333334,Published as a conference paper at ICLR 2022
REFERENCES,0.7416666666666667,View 0
REFERENCES,0.74375,View 1
REFERENCES,0.7458333333333333,View 2
REFERENCES,0.7479166666666667,View 3
REFERENCES,0.75,View 4
REFERENCES,0.7520833333333333,View 5
REFERENCES,0.7541666666666667,View 0
REFERENCES,0.75625,View 1
REFERENCES,0.7583333333333333,View 2
REFERENCES,0.7604166666666666,View 3
REFERENCES,0.7625,View 4
REFERENCES,0.7645833333333333,View 5
REFERENCES,0.7666666666666667,"Input
OccNet
DISN
CoReNet NoGrad FixedE
DGS
Numerical Gradient
DGS"
REFERENCES,0.76875,Closed-form Gradient GT
REFERENCES,0.7708333333333334,"Figure 11: Quantitative comparison on the high-realism ShapeNet (without handpick: test case
number 400 and 500). The reconstruction result of each approach is visualized in six different
views, with the first view the same as the camera view, the first three views the same elevation as the
camera view, and the last three view horizontal view."
REFERENCES,0.7729166666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.775,View 0
REFERENCES,0.7770833333333333,View 1
REFERENCES,0.7791666666666667,View 2
REFERENCES,0.78125,View 3
REFERENCES,0.7833333333333333,View 4
REFERENCES,0.7854166666666667,View 5
REFERENCES,0.7875,View 0
REFERENCES,0.7895833333333333,View 1
REFERENCES,0.7916666666666666,View 2
REFERENCES,0.79375,View 3
REFERENCES,0.7958333333333333,View 4
REFERENCES,0.7979166666666667,View 5
REFERENCES,0.8,"Input
OccNet
DISN
CoReNet NoGrad FixedE
DGS
Numerical Gradient
DGS"
REFERENCES,0.8020833333333334,Closed-form Gradient GT
REFERENCES,0.8041666666666667,"Figure 12: Quantitative comparison on the high-realism ShapeNet (without handpick: test case
number 600 and 700). The reconstruction result of each approach is visualized in six different
views, with the first view the same as the camera view, the first three views the same elevation as the
camera view, and the last three view horizontal view."
REFERENCES,0.80625,Published as a conference paper at ICLR 2022
REFERENCES,0.8083333333333333,View 0
REFERENCES,0.8104166666666667,View 1
REFERENCES,0.8125,View 2
REFERENCES,0.8145833333333333,View 3
REFERENCES,0.8166666666666667,View 4
REFERENCES,0.81875,View 5
REFERENCES,0.8208333333333333,View 0
REFERENCES,0.8229166666666666,View 1
REFERENCES,0.825,View 2
REFERENCES,0.8270833333333333,View 3
REFERENCES,0.8291666666666667,View 4
REFERENCES,0.83125,View 5
REFERENCES,0.8333333333333334,"Input
OccNet
DISN
CoReNet NoGrad FixedE
DGS
Numerical Gradient
DGS"
REFERENCES,0.8354166666666667,Closed-form Gradient GT
REFERENCES,0.8375,"Figure 13: Quantitative comparison on the high-realism ShapeNet (without handpick: test case
number 800 and 900). The reconstruction result of each approach is visualized in six different
views, with the first view the same as the camera view, the first three views the same elevation as the
camera view, and the last three view horizontal view."
REFERENCES,0.8395833333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.8416666666666667,"Input
OccNet + GeoReg DISN + TSDFVox Ablation - NoGrad
DGS
GT
Ablation - FixedE"
REFERENCES,0.84375,View 0
REFERENCES,0.8458333333333333,View 1
REFERENCES,0.8479166666666667,View 2
REFERENCES,0.85,View 3
REFERENCES,0.8520833333333333,View 4
REFERENCES,0.8541666666666666,View 5
REFERENCES,0.85625,View 0
REFERENCES,0.8583333333333333,View 1
REFERENCES,0.8604166666666667,View 2
REFERENCES,0.8625,View 3
REFERENCES,0.8645833333333334,View 4
REFERENCES,0.8666666666666667,View 5
REFERENCES,0.86875,"Figure 14: Quantitative comparison on the ScannetV2 (without handpick: the first frame of the
1st and 2nd test scene in ScannetV2). The reconstruction result of each approach is visualized in
six different views, with the first view the same as the camera view, the first three views the same
elevation as thecamera view, and the last three view elevated view."
REFERENCES,0.8708333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.8729166666666667,"Input
OccNet + GeoReg
CoReNet
MiDaS
DGS
GT
AdelaiDepth"
REFERENCES,0.875,View 0
REFERENCES,0.8770833333333333,View 1
REFERENCES,0.8791666666666667,View 2
REFERENCES,0.88125,View 3
REFERENCES,0.8833333333333333,View 4
REFERENCES,0.8854166666666666,View 5
REFERENCES,0.8875,View 0
REFERENCES,0.8895833333333333,View 1
REFERENCES,0.8916666666666667,View 2
REFERENCES,0.89375,View 3
REFERENCES,0.8958333333333334,View 4
REFERENCES,0.8979166666666667,View 5
REFERENCES,0.9,"Figure 15: Quantitative comparison on the ScannetV2 (without handpick: the first frame of the
3rd and 4th test scene in ScannetV2). The reconstruction result of each approach is visualized in
six different views, with the first view the same as the camera view, the first three views the same
elevation as thecamera view, and the last three view elevated view."
REFERENCES,0.9020833333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.9041666666666667,"Input
OccNet + GeoReg DISN + TSDFVox Ablation - NoGrad
DGS
GT
Ablation - FixedE"
REFERENCES,0.90625,View 0
REFERENCES,0.9083333333333333,View 1
REFERENCES,0.9104166666666667,View 2
REFERENCES,0.9125,View 3
REFERENCES,0.9145833333333333,View 4
REFERENCES,0.9166666666666666,View 5
REFERENCES,0.91875,View 0
REFERENCES,0.9208333333333333,View 1
REFERENCES,0.9229166666666667,View 2
REFERENCES,0.925,View 3
REFERENCES,0.9270833333333334,View 4
REFERENCES,0.9291666666666667,View 5
REFERENCES,0.93125,"Figure 16: Quantitative comparison on the ScannetV2 (without handpick: the first frame of the
5th and 6th test scene in ScannetV2). The reconstruction result of each approach is visualized in
six different views, with the first view the same as the camera view, the first three views the same
elevation as thecamera view, and the last three view elevated view."
REFERENCES,0.9333333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.9354166666666667,"Input
OccNet + GeoReg
CoReNet
MiDaS
DGS
GT
AdelaiDepth"
REFERENCES,0.9375,View 0
REFERENCES,0.9395833333333333,View 1
REFERENCES,0.9416666666666667,View 2
REFERENCES,0.94375,View 3
REFERENCES,0.9458333333333333,View 4
REFERENCES,0.9479166666666666,View 5
REFERENCES,0.95,View 0
REFERENCES,0.9520833333333333,View 1
REFERENCES,0.9541666666666667,View 2
REFERENCES,0.95625,View 3
REFERENCES,0.9583333333333334,View 4
REFERENCES,0.9604166666666667,View 5
REFERENCES,0.9625,"Figure 17: Quantitative comparison on the ScannetV2 (without handpick: the first frame of the
7th and 8th test scene in ScannetV2). The reconstruction result of each approach is visualized in
six different views, with the first view the same as the camera view, the first three views the same
elevation as thecamera view, and the last three view elevated view."
REFERENCES,0.9645833333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.9666666666666667,"Input
OccNet + GeoReg DISN + TSDFVox Ablation - NoGrad
DGS
GT
Ablation - FixedE"
REFERENCES,0.96875,View 0
REFERENCES,0.9708333333333333,View 1
REFERENCES,0.9729166666666667,View 2
REFERENCES,0.975,View 3
REFERENCES,0.9770833333333333,View 4
REFERENCES,0.9791666666666666,View 5
REFERENCES,0.98125,View 0
REFERENCES,0.9833333333333333,View 1
REFERENCES,0.9854166666666667,View 2
REFERENCES,0.9875,View 3
REFERENCES,0.9895833333333334,View 4
REFERENCES,0.9916666666666667,View 5
REFERENCES,0.99375,"Figure 18: Quantitative comparison on the ScannetV2 (without handpick: the first frame of the
9th and 10th test scene in ScannetV2). The reconstruction result of each approach is visualized in
six different views, with the first view the same as the camera view, the first three views the same
elevation as thecamera view, and the last three view elevated view."
REFERENCES,0.9958333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.9979166666666667,"Figure 19: Additional Qualitative results of our model generalizing to unseen test images down-
loaded from the Internet."
