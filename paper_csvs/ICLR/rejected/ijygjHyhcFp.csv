Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0008438818565400844,"Present-day federated learning (FL) systems deployed over edge networks consists
of a large number of workers with high degrees of heterogeneity in data and/or
computing capabilities, which call for ﬂexible worker participation in terms of
timing, effort, data heterogeneity, etc. To achieve these goals, in this work, we
propose a new FL paradigm called “Anarchic Federated Learning” (AFL). In stark
contrast to conventional FL models, each worker in AFL has complete freedom to
choose i) when to participate in FL, and ii) the number of local steps to perform in
each round based on its current situation (e.g., battery level, communication chan-
nels, privacy concerns). However, AFL also introduces signiﬁcant challenges in
algorithmic design because the server needs to handle the chaotic worker behaviors.
Toward this end, we propose two Anarchic Federated Averaging (AFA) algorithms
with two-sided learning rates for both cross-device and cross-silo settings, which
are named AFA-CD and AFA-CS, respectively. Somewhat surprisingly, even with
general worker information arrival processes, we show that both AFL algorithms
achieve the same convergence rate order as the state-of-the-art algorithms for
conventional FL. Moreover, they retain the highly desirable linear speedup effect
in the new AFL paradigm. We validate the proposed algorithms with extensive
experiments on real-world datasets."
INTRODUCTION,0.0016877637130801688,"1
INTRODUCTION"
INTRODUCTION,0.002531645569620253,"Federated Learning (FL) has recently emerged as an important distributed learning framework that
leverages numerous workers to collaboratively learn a joint model (Li et al., 2019a; Yang et al., 2019;
Kairouz et al., 2019). Since its inception, FL algorithms have become increasingly powerful and
have been able to handle various heterogeneity in data, network environments, worker computing
capabilities, etc. Moreover, most of the prevailing FL algorithms (e.g., FedAvg (McMahan et al.,
2016) and its variants (Li et al., 2018; Zhang et al., 2020b; Karimireddy et al., 2020b;a; Acar et al.,
2021)) enjoy so-called “linear speedup effect,” i.e., the convergence time of an FL algorithm decreases
linearly as the number of workers increases (Stich, 2018; Yu et al., 2019; Wang & Joshi, 2018; Khaled
et al., 2019; Karimireddy et al., 2020b; Yang et al., 2021; Qu et al., 2020)."
INTRODUCTION,0.0033755274261603376,"To achieve these salient features, most of the existing FL algorithms have adopted a server-centric
approach, i.e., the worker behaviors are tightly “dictated” by the server. For example, the server in
these FL algorithms can i) determine either all or a subset of workers to participate in each round of
FL update; ii) fully control the timing for synchronization and whether to accept/reject information
sent from the workers; iii) precisely specify the algorithmic operations (e.g., the number of local
steps performed at each worker before communicating with the server), etc."
INTRODUCTION,0.004219409282700422,"Despite achieving strong performance guarantees, such a server-centric approach introduces several
limitations. Speciﬁcally, these server-centric FL algorithms often implicitly rely on the following
assumptions: (1) each worker is available for training upon the server’s request and throughout a
complete round; (2) all participating workers are willing to execute the same number of local updates
and communicate with the server in a synchronous manner following a common clock. Unfortunately,
in edge networks where many FL systems are deployed, these assumptions are restrictive or even
problematic due to the following reasons. First, many requested edge devices on the worker side may
not be available in each round because of, e.g., communication errors or battery outages. Second,
the use of synchronous communication and an identical number of local updates across all workers
ignores the fact that worker devices in edge-based FL systems are heterogeneous in computation
and communication capabilities. As a result, stragglers (i.e., slow workers) could signiﬁcantly slow"
INTRODUCTION,0.005063291139240506,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.00590717299578059,"down the training process. To mitigate the straggler effect, various robust FL algorithms have been
developed. For example, the server in FedAvg (McMahan et al., 2016) can simply ignore and drop
the information from the stragglers to speedup learning. However, this may lead to other problems
such as wasted computation/energy (Wang et al., 2019), slower convergence (Li et al., 2018), or
biased/unfair uses of worker data (Kairouz et al., 2019). Moreover, the synchronous nature of the
server-centric approaches implies many networking problems (e.g., interference between workers,
periodic trafﬁc spikes, high complexity in maintaining a network-wide common clock)."
INTRODUCTION,0.006751054852320675,"The above limitations of the current server-centric FL approaches motivate us to propose a new
paradigm in FL, which we call Anarchic Federated Learning (AFL). In stark contrast to server-
centric FL, workers in AFL are completely free of the “dictation” from the server. Speciﬁcally, each
worker has complete freedom to choose when and how long to participate in FL without following
any control signals from the server. As a result, the information fed back from workers is inherently
asynchronous. Also, each worker can independently determine the number of local update steps
to perform in each round based on its current local situation (e.g., battery level, communication
channels, privacy concerns). In other words, the amount of local computation at each worker is
time-varying, device-dependent, and fully controlled by the worker itself. Clearly, AFL has a much
lower server-worker coordination complexity and avoids the aforementioned pitfalls in server-centric
FL approaches. However, AFL also introduces signiﬁcant challenges in algorithmic design on the
server-side because the server needs to work much harder to handle the chaotic worker behaviors in
AFL (e.g., asynchrony, spatial and temporal heterogeneity in computing). Toward this end, several
fundamental questions naturally arise: 1) Is it possible to design algorithms that converge under
AFL? 2) If the answer to the previous question is yes, how fast could the algorithms converge? 3)
Can the new AFL-based algorithms still achieve the desired “linear speedup effect?”"
INTRODUCTION,0.007594936708860759,"In this paper, we answer the above fundamental questions of AFL afﬁrmatively. Our main contribu-
tions and key results are summarized as follows:"
INTRODUCTION,0.008438818565400843,"• We propose a new FL paradigm called Anarchic Federated Learning (AFL), where the workers are
allowed to engage in training at will and choose the number of local update steps based on their
own time-varying situations (computing resources, energy levels, etc.). This loose worker-server
coupling signiﬁcantly simpliﬁes the implementations and renders AFL particularly suitable for
FL deployments in edge computing environments. For any AFL algorithms under general worker
information arrival processes and non-i.i.d. data across workers, we ﬁrst establish a fundamental
convergence error lower bound that depends on the data heterogeneity in the AFL system. Then,
we propose two Anarchic Federated Averaging (AFA) algorithms with two-sided learning rates for
two classes of FL problems(cross-device and cross-silo) (Kairouz et al., 2019; Wang et al., 2021).
• For AFL in the cross-device (CD) setting, our AFA-CD algorithm converges to an error ball whose
size matches the fundamental lower bound, with an O(1/
√"
INTRODUCTION,0.009282700421940928,"mT) convergence rate where m is the
number of collected workers in each round of update and T is the total number of rounds. We
note that this convergence rate retains the highly desirable “linear speedup effect” under AFL.1
Moreover, under the special case with uniform workers’ participation (equivalent to uniform
workers sampling in conventional FL (Li et al., 2019c; Karimireddy et al., 2020b;a; Acar et al.,
2021)), AFA-CD can further converge to a stationary point (i.e., a singleton) at a convergence rate
that matches the state-of-the-art of conventional distributed and federated learning.
• For AFL in the cross-silo (CS) setting, our proposed AFA-CS algorithm achieves an enhanced
convergence rate of O(1/
√"
INTRODUCTION,0.010126582278481013,"MT) by leveraging historical feedback and variance reduction tech-
niques, where M is the total number of workers. This suggests that, not only can “linear speedup”
be achieved under AFL-CS, the speedup factor also depends on the total number of workers M
instead of the number of collected workers m in each round (M > m). To our knowledge, this
result is new in the FL literature.
• We validate the proposed algorithms with extensive experiments on CV and NLP tasks and further
explore the effect of the asynchrony and local step number in AFL. We also numerically show that
our AFL is a general algorithmic framework in the sense that various advanced FL techniques (e.g.,
FedProx (Li et al., 2018) and SCAFFOLD (Karimireddy et al., 2020b)) can be integrated as the
optimizers in our AFA framework to further enhance the AFL performance."
INTRODUCTION,0.010970464135021098,"1To attain ϵ-accuracy, it takes O(1/ϵ2) steps for an algorithm with an O(1/
√"
INTRODUCTION,0.01181434599156118,"T) convergence rate, while
needing O(1/mϵ2) steps for another algorithm with an O(1/
√"
INTRODUCTION,0.012658227848101266,"mT) convergence rate (the hidden constant in
Big-O is the same). In this sense, O(1/
√"
INTRODUCTION,0.01350210970464135,mT) implies a linear speedup with respect to the number of workers.
INTRODUCTION,0.014345991561181435,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.015189873417721518,"The rest of the paper is organized as follows. In Section 2, we review related work. In Section 3,
we introduce AFL and our AFA algorithms, which are followed by their convergence analysis in
Section 4. We present the numerical results in Section 5 and conclude the work in Section 6."
RELATED WORK,0.016033755274261603,"2
RELATED WORK"
RELATED WORK,0.016877637130801686,"Server-Centric Federated Learning Algorithms: To date, one of the prevailing FL algorithms is
Federated Averaging (FedAvg), which was ﬁrst proposed in (McMahan et al., 2016) as a heuristic to
improve communication efﬁciency and data privacy for FL. Since then, there have been substantial
follow-ups of FedAvg that focus on non-i.i.d. (heterogeneous) data (see, e.g., FedProx (Li et al.,
2018), FedPD (Zhang et al., 2020b), SCAFFOLD (Karimireddy et al., 2020b), FedNova (Wang
et al., 2020), FedDyn (Acar et al., 2021), and MIME (Karimireddy et al., 2020a)), which are closely
related to our work. The main idea for these algorithms is to control the “model drift” (due to
heterogeneous datasets and the use of multiple local update steps on the worker side of FedAvg).
While these algorithms achieved various degrees of success in dealing with data heterogeneity, they
are all server-centric synchronous algorithms that are not easy to implement in edge-based FL due to
straggler issues (see discussions in Section 1)."
RELATED WORK,0.017721518987341773,"Federated Learning with Flexible Worker Participation: Recently, some attempts have been
made to alleviate the strict requirements on worker’s participation, such as allowing different local
steps (Ruan et al., 2021; Wang et al., 2020) and asynchronous FL (Avdiukhin & Kasiviswanathan,
2021; Xie et al., 2019). However, most of these works either lack theoretical performance guarantees
or require strong assumptions. For example, Ruan et al. (2021) assumed strongly convex loss function
and bounded aggregation coefﬁcient; Avdiukhin & Kasiviswanathan (2021) assumed bounded
gradients and same computation time per iteration for all workers. Our AFL paradigm considered in
this paper is more general and subsumes all the above settings as special cases. We note, however, that
AFL differs from conventional FL with ﬂexible worker participation in that the worker’s participation
in AFL and its local optimization process are completely determined by the workers, and not by the
sampling requests from the server. This is more practical since it allows workers to participate in
FL under drastically different situations in network, charging/idle cycles, etc. Due to the complex
couplings between various sources of randomness and multiple layers of heterogeneity in spatial and
temporal domains in AFL, the training algorithm design for AFL and its theoretical analysis is far
from a straightforward combination of existing FL techniques for ﬂexible worker participation."
RELATED WORK,0.018565400843881856,"Asynchronous Distributed Optimization: The asynchrony in AFL also shares some similarity with
asynchronous distributed optimization. The basic idea of asynchronous distributed optimization is to
forgo the common clock in the system to lower the system implementation complexity in distributed
optimization. However, due to extra noise introduced by asynchrony, it is highly non-trivial to
establish the convergence performance of asynchronous distributed optimization algorithms. To
address this challenge, asynchronous distributed optimization has been studied extensively in the
machine learning and optimization literature (see, e.g., Lian et al. (2018); Niu et al. (2011); Agarwal
& Duchi (2012); Paine et al. (2013); Xie et al. (2019); Zhang et al. (2020a) and references therein).
We note that the AFL paradigm considered in this paper is more general and subsumes asynchronous
distributed optimization as a special case. To see this, note that in addition to the asynchronous
updates at the server, the workers in AFL could further have different numbers of local update steps.
Moreover, the workers may not even need to be work-conserving (i.e., workers could be idle between
rounds of updates). As a result, the convergence analysis of AFL is much more challenging."
ANARCHIC FEDERATED LEARNING,0.019409282700421943,"3
ANARCHIC FEDERATED LEARNING"
ANARCHIC FEDERATED LEARNING,0.020253164556962026,"1) Overview of Anarchic Federated Learning: The goal of FL is to solve an optimization problem
in the form of minx∈Rd f(x) :=
1
M
PM
i=1 fi(x), where fi(x) ≜Eξi∼Di[fi(x, ξi)] is the local (non-
convex) loss function associated with a local data distribution Di and M is the total number of workers.
For the setting with heterogeneous (non-i.i.d.) datasets at the workers, we have Di ̸= Dj, if i ̸= j. In
terms of the assumption on the size of workers, FL can be classiﬁed as cross-device FL and cross-silo
FL (Kairouz et al., 2019; Wang et al., 2021). Cross-device FL is designed for large-scale FL with
a massive number of mobile or IoT devices (M is large). As a result, the server can only afford to
collect information from a subset of workers in each round of update and does not have sufﬁcient"
ANARCHIC FEDERATED LEARNING,0.02109704641350211,Under review as a conference paper at ICLR 2022
ANARCHIC FEDERATED LEARNING,0.021940928270042195,Algorithm 1 The Basic Framework of Anarchic Federated Learning (AFL).
ANARCHIC FEDERATED LEARNING,0.02278481012658228,At the Server (Concurrently with Workers):
ANARCHIC FEDERATED LEARNING,0.02362869198312236,"1. (Concurrent Thread) Collect local updates returned from the workers.
2. (Concurrent Thread) Aggregate local update returned from the collected workers and
update global model following some server-side optimization process.
At Each Worker (Concurrently with Server):"
ANARCHIC FEDERATED LEARNING,0.024472573839662448,"1. Once decided to participate in the training, pull the global model with current timestamp.
2. Perform (multiple) local update steps following some worker-side optimization process.
3. Return the computation result and the associated pulling timestamp to the server, with
extra processing if so desired."
ANARCHIC FEDERATED LEARNING,0.02531645569620253,"memory space to store workers’ information across rounds. In comparison, the number of workers
in cross-silo FL is relatively small. Although the server in cross-silo FL may still have to collect
information only from a subset of workers in each round, it has enough memory space to store each
worker’s most recent information."
ANARCHIC FEDERATED LEARNING,0.026160337552742614,"The basic process of AFL is illustrated in Algorithm 1. Here, the server is responsible for: 1)
collecting the local updates returned from workers, and 2) aggregating the obtained updates once
certain conditions are satisﬁed (e.g., upon collecting m ∈(0, M] local updates from workers)
to update the global model. In AFL, each worker could be non-work-conserving (i.e., idling is
allowed between each two successive participations in training). Whenever a worker intends to
participate in the training, it ﬁrst pulls the current model parameters from the server. Then, after
ﬁnishing multiple local update steps (more on this next) with some worker-side optimization process
(e.g., following stochastic gradients or additional information such as variance-reduced and/or
momentum adjustments), the worker reports the computation results to the server (potentially with
extra processing if so desired, such as rescaling, compression for communication-efﬁciency, etc.).
Note that the above operations can happen concurrently on the server and worker sides, i.e., the
processings at the workers and the server are independent of each other in the temporal domain."
ANARCHIC FEDERATED LEARNING,0.0270042194092827,"We remark that AFL is a general computing architecture that subsumes the conventional FL and
asynchronous distributed optimization as special cases. From an optimization perspective, the server
and the workers may adopt independent optimization processes, thus enabling a much richer set of
learning “control knobs” (e.g., separated learning rates, separated batch sizes). More importantly,
each worker is able to completely take control of its own optimization process, even using a time-
varying number of local update steps and optimizers, which depend on its local dataset and/or its
device status (e.g., battery level, privacy preference)."
ANARCHIC FEDERATED LEARNING,0.027848101265822784,"2) A Fundamental Convergence Error Lower Bound of AFL: Before developing training algo-
rithms for AFL, it is insightful to ﬁrst obtain a fundamental understanding of the performance limit of
any AFL training algorithms. Toward this end, we ﬁrst state several assumptions that are needed for
our theoretical analysis throughout the rest of this paper."
ANARCHIC FEDERATED LEARNING,0.02869198312236287,"Assumption 1. (L-Lipschitz Continuous Gradient) There exists a constant L > 0, such that
∥∇fi(x) −∇fi(y)∥≤L∥x −y∥, ∀x, y ∈Rd, and i ∈[M]."
ANARCHIC FEDERATED LEARNING,0.029535864978902954,"Assumption 2. (Unbiased Local Stochastic Gradient) Let ξi be a random local data sample at
worker i. The local stochastic gradient is unbiased, i.e., E[∇fi(x, ξi)] = ∇fi(x), ∀i ∈[m], where
the expectation is taken over the local data distribution Di."
ANARCHIC FEDERATED LEARNING,0.030379746835443037,"Assumption 3. (Bounded Local and Global Variances) There exist two constants σL ≥0 and σG ≥0,
such that the variance of each local stochastic gradient estimator is bounded by E[∥∇fi(x, ξi) −
∇fi(x)∥2] ≤σ2
L, ∀i ∈[M], and the global variability of the local gradient of the cost function is
bounded by ∥∇fi(x) −∇f(x)∥2 ≤σ2
G, ∀i ∈[M], ∀k."
ANARCHIC FEDERATED LEARNING,0.031223628691983123,"The ﬁrst two assumptions are standard in the convergence analysis of non-convex optimization (see,
e.g., (Ghadimi & Lan, 2013; Bottou et al., 2018)). For Assumption 3, the bounded local variance
is also a standard assumption. We utilize a universal bound σG to quantify the data heterogeneity
among different workers. This assumption is also frequently used in the literature of FL with non-
i.i.d. datasets (Reddi et al., 2020; Wang et al., 2019; Yang et al., 2021) as well as in decentralized
optimization (Kairouz et al., 2019)."
ANARCHIC FEDERATED LEARNING,0.032067510548523206,Under review as a conference paper at ICLR 2022
ANARCHIC FEDERATED LEARNING,0.03291139240506329,"To establish a fundamental convergence error lower bound, we consider the most general case where
no assumption on the arrival processes of the worker information is made, except that each worker’s
participation in FL is independent of each other. In such general worker information arrival processes,
we prove the following fundamental lower bound of convergence error by constructing a worst-case
scenario (see Appendix B.1 for proof details):
Theorem 1 (Convergence Error Lower Bound for General Worker Information Arrival Processes).
For any level of heterogeneity characterized by σG, there exists loss functions and worker participation
process satisfying Assumptions 1- 3 for which the output ˆx of any randomized FL algorithm satisﬁes:"
ANARCHIC FEDERATED LEARNING,0.03375527426160337,"E∥∇f(ˆx)∥2 = Ω(σ2
G),
(1)"
ANARCHIC FEDERATED LEARNING,0.03459915611814346,"Remark 1. The lower bound in Theorem 1 indicates that no algorithms for AFL could converge to a
stationary point under general worker information arrival processes, due to the signiﬁcant system
heterogeneity and randomness caused by such general worker information arrivals. The possible
system heterogeneity coupled with non-i.i.d. data result in objective value drifts, which further lead
to an inevitable error in convergence. We note that our lower bound also holds for non-i.i.d. FL
including synchronous FedAvg and its variants, thus also providing insights for conventional FL. To
ensure convergence to a stationary point, extra assumptions for the worker information arrivals have
to be made, e.g., uniformly distributed arrivals (see Theorem 3) and bounded delays (see Theorem 4)."
ANARCHIC FEDERATED LEARNING,0.035443037974683546,"4
THE ANARCHIC FEDERATED AVERAGING (AFA) ALGORITHMS"
ANARCHIC FEDERATED LEARNING,0.036286919831223625,"Upon obtaining a fundamental understanding of the training algorithm performance limit from the
convergence error in Theorem 1, we are now in a position to develop AFL algorithms that are guided
by our fundamental lower bound. Toward this end, in this section, we will develop two anarchic
federated averaging (AFA) algorithms for the cross-device (CD) and cross-silo (CS) settings in
Section 4.1 and 4.2, respectively."
THE AFA-CD ALGORITHM FOR CROSS-DEVICE AFL,0.03713080168776371,"4.1
THE AFA-CD ALGORITHM FOR CROSS-DEVICE AFL"
THE AFA-CD ALGORITHM FOR CROSS-DEVICE AFL,0.0379746835443038,"1) The AFA-CD Algorithm: First, we propose the AFA-CD algorithm for the cross-device AFL
setting. As mentioned earlier, cross-device AFL is suitable for cases with a massive number of
edge devices. In each round of global model update, only a small subset of workers are used in
the training. The server is assumed to have no historical information of the workers. As shown
in Algorithm 2, AFA-CD closely follows the AFL architecture shown in Algorithm 1. Here, we
use standard stochastic gradient descent (SGD) as the server- and worker-side optimizer. In each
update t, t = 1, . . . , T, the server waits until collecting m local updates {Gi(xt−τt,i)} from workers
to form a set Mt with |Mt| = m, where τt,i represents the random delay of the local update of
worker i, i ∈Mt (Server Code, Line 1). Once Mt is formed, the server aggregates all local updates
Gi(xt−τt,i), i ∈Mt and updates global model (Server Code, Line 2). We count each global model
update as one communication round. Meanwhile, for each worker, it pulls the current global model
parameters with time stamp µ once it decides to participate in training (Worker Code, Line 1). Each
worker can then choose a desired number of local update steps Kt,i (could be time-varying and
device-dependent) to perform SGD updates for Kt,i times, and then return the rescaled sum of all
stochastic gradients with timestamp µ to the server (Worker Code, Lines 2–3)."
THE AFA-CD ALGORITHM FOR CROSS-DEVICE AFL,0.038818565400843885,"2) Convergence Analysis of the AFA-CD Algorithm: We ﬁrst conduct the convergence of AFA-CD
under general worker information arrival processes. We let K be the maximum local update steps,
which is deﬁned as K := maxt∈[T ],i∈[M]{Kt,i}. We let τ be the maximum delay of workers’
returned information, which is deﬁned as τ := maxt∈[T ],i∈Mt{τt,i}. Also, we use f0 = f(x0) and
f∗to denote the initial and the optimal objective values, respectively. Then, we have the following
convergence result for the AFA-CD algorithm (see proof details in Appendix B.2):
Theorem 2 (AFA-CD with General Worker Information Arrival Processes). Under Assumptions 1- 3,
choose server-side and worker-side learning rates η and ηL such that the following relationships
hold: 180η2
LK2L2τ < 1 and 2LηηL + 3τ 2L2η2η2
L ≤1. Then, the output sequence {xt} generated
by AFedAvg-TSLR-CD with general worker information arrival processes satisﬁes:"
T,0.039662447257383965,"1
T"
T,0.04050632911392405,"T −1
X"
T,0.04135021097046414,"t=0
E∥∇f(xt)∥2 ≤4(f0 −f∗)"
T,0.04219409282700422,"ηηLT
+ 4
 
αLσ2
L + αGσ2
G

,
(2)"
T,0.043037974683544304,Under review as a conference paper at ICLR 2022
T,0.04388185654008439,Algorithm 2 The AFA-CD Algorithm for Cross-Device AFL.
T,0.04472573839662447,At the Server (Concurrently with Workers):
T,0.04556962025316456,"1. In the t−th update round, collect m local updates {Gi(xt−τt,i), i ∈Mt} returned from
the workers to form the set Mt, where τt,i represents the random delay of the worker i’s
local update, i ∈Mt.
2. Aggregate and update: Gt = 1 m
P"
T,0.046413502109704644,"i∈Mt Gi(xt−τt,i),
xt+1 = xt −ηGt.
At Each Worker (Concurrently with Server):"
T,0.04725738396624472,"1. Once decided to participate in the training, retrieve the parameter xµ from the server and
its timestamp, set the local model: xi
µ,0 = xµ.
2. Choose a number of local steps Kt,i, which can be time-varying and device-dependent.
Let xi
µ,k+1 = xi
µ,k −ηLgi
k, where gi
k = ∇fi(xi
µ,k, ξi
k), k = 0, . . . , Kt,i −1."
T,0.04810126582278481,"3. Sum and rescale the stochastic gradients: Gi(xµ) =
1
Kt,i
PKt,i−1
j=0
gi
j. Return Gi(xµ)."
T,0.048945147679324896,"where αL ≜
 2ηηL"
T,0.049789029535864976,"m
+ 3τ 2L2η2η2
L
2m
+ 15η2
LKL2"
T,0.05063291139240506,"2

and αG ≜
 3"
T,0.05147679324894515,"2 + 45K2L2η2
L

."
T,0.05232067510548523,"With Theorem 2, we immediately have the following convergence rate result for AFA-CD, which
implies the highly desirable “linear speedup effect” can be achieved by AFA-CD."
T,0.053164556962025315,"Corollary 1 (Linear Speedup to Error Ball). By setting ηL =
1
√"
T,0.0540084388185654,"T , and η = √m, the convergence
rate of AFA-CD with general worker information arrival processes is:"
T,0.05485232067510549,"1
T"
T,0.05569620253164557,"T −1
X"
T,0.056540084388185655,"t=0
E∥∇f(xt)∥2 = O

1
m1/2T 1/2"
T,0.05738396624472574,"
+ O
τ 2 T"
T,0.05822784810126582,"
+ O
K2 T"
T,0.05907172995780591,"
+ O(σ2
G)."
T,0.059915611814345994,"Remark 2. First, we note that the non-vanishing error term O(σ2
G) in Corollary 1 matches the lower
bound in Theorem 1. This implies that the convergence error of AFL-CD is order-optimal. Recall
that this non-vanishing convergence error is a direct consequence of objective function drift under the
general worker information arrivals (no assumption on the arrivals of the worker participation in each
round of update), which is independent of the choices of learning rates, the number of local update
steps, and the number of global update rounds (more discussion in the supplementary material). Also,
for a sufﬁciently large T, the dominant term O(
1
m1/2T 1/2 ) implies that AFA-CD achieves the linear
speedup effect before reaching a constant error neighborhood with size O(σ2
G)."
T,0.060759493670886074,"Remark 3. The conditions for the learning rates η and ηL are a natural extension of SGD. When
τ = 0 (synchronous setting) and K = 1 (single local update), the condition becomes ηηL ≤
1
2L,
which recovers the same learning rate condition for SGD in (Ghadimi & Lan, 2013). Also, the
suggested worker-side learning rate ηL is independent of local update steps. This is different from
previous work that ηL depends heavily on local update steps (Wang et al., 2020; Yang et al., 2021),
thus making it more practical for implementation."
T,0.06160337552742616,"Given the weak convergence result implied by the lower bound in Theorem 1 under general workers’
information arrivals, it is important to understand what extra assumptions on the worker information
arrivals are needed in order to guarantee convergence. Toward this end, we consider a special case
where the arrivals of worker returned information in each round for global update is uniformly dis-
tributed among the workers. In this setting, Mt can be viewed as a subset with size m independently
and uniformly sampled from [M] without replacement. For FL systems with a massive number
of workers, this assumption of uniformly distributed arrivals is a good approximation for worker
participation in cross-device FL (McMahan et al., 2016; Li et al., 2019a). Also, one can map this
setting into the conventional cross-device FL systems with uniform worker sampling in partial worker
participation, which is a widely-used assumption (McMahan et al., 2016; Li et al., 2019c; 2018; Yang
et al., 2021; Wang et al., 2020). For this special case, the convergence performance of AFA-CD can
be improved as follows (see proof details in Appendix B.3):"
T,0.06244725738396625,"Theorem 3. Under Assumptions 1- 3, choose server-side and worker-side learning rates η and ηL
such that the following relationships hold: η2
L[6(2K2 −3K +1)L2] ≤1, 120L2K2η2
Lτ +4(LηηL +
L2η2η2
Lτ 2)
M−m
m(M−1)(90K2L2η2
Lτ + 3τ) < 1. Then, the output sequence {xt} generated by AFA-CD"
T,0.06329113924050633,Under review as a conference paper at ICLR 2022
T,0.06413502109704641,Algorithm 3 The AFA-CS Algorithm for Cross-Silo AFL.
T,0.06497890295358649,At the Server (Concurrently with Workers):
T,0.06582278481012659,"1. In the t−th update round, collect m local updates {Gi(xt−τti)} from the workers to form
the set Mt.
2. Update worker i’s information in the memory using the returned local update Gi.
3. Aggregate and update: Gt =
1
M
P"
T,0.06666666666666667,"i∈[M] Gi,
xt+1 = xt −ηGt.
At Each Worker (Concurrently with Server): Same as AFA-CD Worker Code."
T,0.06751054852320675,with uniformly distributed worker information arrivals satisﬁes:
T,0.06835443037974684,"1
T"
T,0.06919831223628692,"T −1
X"
T,0.070042194092827,"t=0
E∥∇f(xt)∥2
2 ≤4(f0 −f∗)"
T,0.07088607594936709,"ηηLT
+ 4
 
αLσ2
L + αGσ2
G

,
(3)"
T,0.07172995780590717,where αL and αG are constants that are deﬁned as:
T,0.07257383966244725,"αL ≜
LηηL"
T,0.07341772151898734,"m
+ τ 2L2η2η2
L
m
+ 5KL2η2
L"
T,0.07426160337552742,"
+ (LηηL + L2η2η2
Lτ 2) M −m"
T,0.0751054852320675,"m(M −1)(15KL2η2
L)

,"
T,0.0759493670886076,"αG ≜

30K2L2η2
L + (LηηL + L2η2η2
Lτ 2) M −m"
T,0.07679324894514768,"m(M −1)(90K2L2η2
L + 3)

."
T,0.07763713080168777,"Furthermore, by choosing the server- and worker-side learning rates appropriately, we immediately
have the following linear speedup convergence result for AFA-CD:
Corollary 2 (Linear Speedup to Stationary Point). By setting ηL =
1
√"
T,0.07848101265822785,"T and η = √m, the conver-
gence rate of AFA-CD with uniformly distributed worker information arrivals is:"
T,0.07932489451476793,"1
T"
T,0.08016877637130802,"T −1
X"
T,0.0810126582278481,"t=0
E∥∇f(xt)∥2
2 = O(
1
m1/2T 1/2 ) + O
τ 2 T"
T,0.08185654008438818,"
+ O
K2 T"
T,0.08270042194092828,"
+ O

K2"
T,0.08354430379746836,m1/2T 3/2
T,0.08438818565400844,"
+ O
K2τ 2 T 2 
."
T,0.08523206751054853,"Remark 4. For a sufﬁciently large T, the linear speedup effect to a stationary point (rather than
a constant error neighborhood) can be achieved with any ﬁnitely bounded maximum local update
steps K and maximum delay τ, i.e., O(
1
√"
T,0.08607594936708861,"mT ). Note this rate does not depend on the delay τ and
local computation steps K after sufﬁciently many rounds T, the negative effect of using outdated
information in such asynchronous setting vanishes asymptotically. Further, for σG = 0 (i.i.d. data)
and K = 1 (single local update step), our AFA-CD algorithm is exactly the same as the AsySG-con
algorithm (Lian et al., 2018) in asynchronous parallel distributed optimization. It can be readily
veriﬁed that AFA-CD achieves the same rate as that of the AsySG-con algorithm. When τ = 0
(synchronous setting) and Kt,i = K (same number of local update steps across workers), AFA-CD
becomes the generalized synchronous FedAvg algorithm with two-sided learning rates (Yang et al.,
2021; Karimireddy et al., 2020b; Reddi et al., 2020). Remarkably, the result of AFA-CD shows that
we can achieve a faster convergence rate than O(
√ K/
√"
T,0.08691983122362869,"mT) in (Yang et al., 2021) and the same
rate as FedAvg analysis in (Karimireddy et al., 2020b)."
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.08776371308016878,"4.2
THE AFA-CS ALGORITHM FOR CROSS-SILO FL"
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.08860759493670886,"1) The AFA-CS Algorithm: As mentioned earlier, cross-silo FL is suitable for collaborative learning
among a relatively small number of (organizational) workers. Thanks to the relatively small number
of workers, each worker’s feedback can be stored at the server. As a result, the server could reuse the
historical information of each speciﬁc worker in each round of global update."
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.08945147679324894,"As shown in Algorithm 3, the AFA-CS algorithm also closely follows the AFL architecture as shown
in Algorithm 1. In each round of global model update, a subset of workers could participate in the
training (Server Code, Line 1). Compared to AFA-CD, the key difference in AFA-CS is in Line 2 of
the Server Code, where the server stores the collected local updates {Gi} for each worker i ∈Mt
into the memory space at the server (Server Code, Line 2). As a result, whenever a worker i returns a
local update to the server upon ﬁnishing its local update steps, the server will update the memory
space corresponding to worker i to replace the old information with this newly received update from"
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.09029535864978903,Under review as a conference paper at ICLR 2022
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.09113924050632911,"worker i. Similar to AFA-CD, every m new updates in the AFA-CS algorithm trigger the server
to aggregate all the Gi, i ∈[M] (newly received information if i ∈Mt or stored information if
i /∈Mt) and update the global model. The Worker Code in AFA-CS is exactly the same as AFA-CD
and its description is omitted for brevity."
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.0919831223628692,"2) Convergence Analysis of the AFA-CS Algorithm: For cross-silo AFL, our AFA-CS algorithm
achieves the following convergence performance (see proof details in Appendix C):
Theorem 4. Under Assumptions 1- 3, choose sever- and worker-side learning rates η and ηL in such
a way that there exists a non-negative constant series {βµ}τ−1
u=0 satisfying the following relationship:"
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.09282700421940929,12LηηL + 540(M −m′)2
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.09367088607594937,"M 2
(1 + LηηL)K2L2η2
L(1 + τ) + 180K2L2η2
L + 320L3K2ηη3
L <1, (4) ηηL"
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.09451476793248945,9(M −m′)2
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.09535864978902954,"2M 2
(1 + LηηL)

3τL2 + (βu+1 −βu) ≤0,
(5) ηηL"
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.09620253164556962,9(M −m′)2
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.0970464135021097,"2M 2
(1 + LηηL)

3τL2 −βτ−1 ≤0,
(6)"
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.09789029535864979,"3
2M σ2
L ≤(1"
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.09873417721518987,"2 −β0ηηL)E∥Gt∥2
2,
(7)"
THE AFA-CS ALGORITHM FOR CROSS-SILO FL,0.09957805907172995,"the output sequence {xt} generated by the AFA-CS algorithm for general worker information arrival
processes with bounded delay (τ := maxt∈[T ],i∈[M]{τt,i}) satisﬁes:"
T,0.10042194092827005,"1
T"
T,0.10126582278481013,"T −1
X"
T,0.1021097046413502,"t=0
E∥∇f(xt)∥2
2 ≤4(V (x0) −V (x∗))"
T,0.1029535864978903,"ηηLT
+ 4
 
αLσ2
L + αGσ2
G

,
(8)"
T,0.10379746835443038,where αL and αG are constants deﬁned as follows:
T,0.10464135021097046,αL ≜[3LηηL
M,0.10548523206751055,"2M
+ 5KL2η2
L(9(M −m′)2"
M,0.10632911392405063,"M 2
(1 + LηηL) + (3"
M,0.10717299578059072,"2 + 3LηηL))],"
M,0.1080168776371308,αG ≜(9(M −m′)2
M,0.10886075949367088,"M 2
(1 + LηηL) + (3"
M,0.10970464135021098,"2 + 3LηηL))(30K2L2η2
L),"
M,0.11054852320675106,"and V (·) is deﬁned as V (xt) ≜f(xt) + Pτ−1
u=0 βu∥xt−u −xt−u−1∥2, m′ is the number of updates
in the memory space with no time delay (τt,i = 0).
Remark 5. To see how stringent the conditions for learning rates in Theorem 4 are, note that Eq. (4)
implies that O(ηηL + ηη3
LK2 + τK2η2
L + τK2ηη3
L) < 1. With a sufﬁciently small learning rate ηL
for given bounded time delay τ and maximum local steps K, Eq. (4) can be satisﬁed. Eqs. (5)–(6)
imply that {βµ}τ−1
u=0 is a non-negative decreasing series with difference O(ηηLτ + η2η2
Lτ). As a
result, β0 = Ω(ηηLτ 2 + η2η2
Lτ 2). Plugging it into Eq. (7) yields that the update term Gt should be
larger than the variance term σ2
L/M, which can be satisﬁed if the number of workers M is sufﬁciently
large or σ2
L is sufﬁciently small (i.e., workers use a sufﬁciently large batch size)."
M,0.11139240506329114,"By choosing appropriate learning rates, we immediately have stronger linear speedup convergence:"
M,0.11223628691983123,"Corollary 3 (Linear Speedup). By setting ηL =
1
√"
M,0.11308016877637131,"T , and η =
√"
M,0.11392405063291139,"M, the convergence rate of the
AFA-CS algorithm for general worker information arrival processes with bounded delay is:"
T,0.11476793248945148,"1
T"
T,0.11561181434599156,"T −1
X"
T,0.11645569620253164,"t=0
E∥∇f(xt)∥2
2 = O

1
M 1/2T 1/2"
T,0.11729957805907174,"
+ O
K2 T"
T,0.11814345991561181,"
+ O
K2M 1/2 T 3/2 
."
T,0.1189873417721519,"Remark 6. Compared to Corollary 1, we can see that, by reusing historical data, the AFA-CS
algorithm can eliminate the non-vanishing O(σ2
G) error term even for general worker information
arrival processes with bounded delay. The bounded delay implicitly requires each workers at least
participate in the training process, eliminating the worse-case scenario shown in Theorem 1. On the
other hand, although the server only collects m workers’ feedback in each round of global model
update, the server leverages all M workers’ feedback by reusing historical information. Intuitively,
this translates the potential objection function drift originated from general worker information arrival
process into the negative effect of delayed returns G(xt−τt,i) from workers. It can be shown that
such a negative effect of delayed returns vanishes asymptotically as the number of communication
rounds T gets large and in turn diminishes the convergence error."
T,0.11983122362869199,Under review as a conference paper at ICLR 2022
T,0.12067510548523207,"0
20
40
60
80
100 120 140
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
T,0.12151898734177215,Test Accuracy
T,0.12236286919831224,"FedAvg
AFA-CD
AFA-CS"
T,0.12320675105485232,(a) p = 1.
T,0.1240506329113924,"0
20
40
60
80
100 120 140
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
T,0.1248945147679325,Test Accuracy
T,0.1257383966244726,"FedAvg
AFA-CD
AFA-CS"
T,0.12658227848101267,(b) p = 2.
T,0.12742616033755275,"0
20
40
60
80
100 120 140
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
T,0.12827004219409283,Test Accuracy
T,0.1291139240506329,"FedAvg
AFA-CD
AFA-CS"
T,0.12995780590717299,(c) p = 5.
T,0.1308016877637131,"0
20
40
60
80
100 120 140
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
T,0.13164556962025317,Test Accuracy
T,0.13248945147679325,"FedAvg
AFA-CD
AFA-CS"
T,0.13333333333333333,(d) p = 10.
T,0.1341772151898734,Figure 1: Test accuracy for logistic regression on non-i.i.d. MNIST with different p-values.
T,0.1350210970464135,"Remark 7. AFA-CS achieves a stronger linear speedup O(1/
√"
T,0.1358649789029536,"MT), which is new in the FL
literature to our knowledge. Speciﬁcally, even though partial (m) workers participation is used in
each round, AFA-CS is able to achieve a surprising speedup with respect to total number of workers
M (M > m). Also, AFA-CS generalizes the lazy aggregation strategy in distributed learning (e.g.,
LAG (Chen et al., 2018)) by setting K = 1 (single local update), τ = 0 (synchronous setting) and
σL = 0 (using full gradient descents instead of stochastic gradients) and further improve the rate of
LSAG (Chen et al., 2020) from O(1/
√"
T,0.13670886075949368,"T) to O(1/
√ MT)."
NUMERICAL RESULTS,0.13755274261603376,"5
NUMERICAL RESULTS"
NUMERICAL RESULTS,0.13839662447257384,"In this section, we conduct experiments to verify our theoretical results. We use i) logistic regression
(LR) on manually partitioned non-i.i.d. MNIST dataset (LeCun et al., 1998), ii) convolutional neural
network (CNN) for manually partitioned CIFAR-10 (Krizhevsky, 2009), and iii) recurrent neural
network (RNN) on natural non-i.i.d. dataset Shakespeare (McMahan et al., 2016). In order to impose
data heterogeneity in MNIST and CIFAR-10 data, we distribute the data evenly into each worker
in label-based partition following the same process in the literature (e.g., McMahan et al. (2016);
Yang et al. (2021); Li et al. (2019c)). Therefore, we can use a parameter p to represent the classes of
labels in each worker’s dataset, which signiﬁes data heterogeneity: the smaller the p-value, the more
heterogeneous the data across workers (cf. Yang et al. (2021); Li et al. (2019c) for details). Due to
space limitation, we relegate the details of models, datasets and hyper-parameters, and further results
of CNN and RNN to the appendix."
NUMERICAL RESULTS,0.13924050632911392,"In Figure 1, we illustrate the test accuracy for LR on MNIST with different p-values. We use
the classical FedAvg algorithm (McMahan et al., 2016) for conventional FL with uniform worker
sampling as a baseline, since it corresponds to the most ideal scenario where workers are fully
cooperative with the server. We examine the learning performance degradation of AFA algorithms
(due to anarchic worker behaviors) compared to this ideal baseline. For our AFA-CD and AFA-CS
with general worker information arrival processes, the test accuracy is comparable to or nearly the
same as that of FedAvg. This conﬁrms our theoretical results and validates the effectiveness of our
AFA algorithms. We further evaluate the impacts of various factors in AFL, including asynchrony,
heterogeneous computing, worker’s arrival process, and non-i.i.d. datasets, on convergence rate
of our proposed AFA algorithms. Note that AFL subsumes FedAvg and many variants when the
above hyper-parameters are set as constant. Also, AFL coupled with other FL algorithms such as
FedProx (Li et al., 2018) and SCAFFOLD (Karimireddy et al., 2020b) is tested. Our results show
that the AFA algorithms are robust against all asynchrony and heterogeneity factors in AFL. Due to
space limitation, we refer readers to the appendix for all these experimental results."
CONCLUSIONS,0.140084388185654,"6
CONCLUSIONS"
CONCLUSIONS,0.1409282700421941,"In this paper, we propose a new paradigm in FL called “Anarchic Federated Learning” (AFL). In
stark contrast to conventional FL models where the server and the worker are tightly coupled, AFL
has a much lower sever-worker coordination complexity, allowing a ﬂexible worker participation.
We propose two Anarchic Federated Averaging algorithms with two-sided learning rates for both
cross-device and cross-silo settings, which are named AFA-CD and AFA-CS, respectively. We show
that both algorithms retain the highly desirable linear speedup effect in the new AFL paradigm.
Moreover, we show that our AFL framework works well numerically by employing advance FL
algorithms FedProx and SCAFFOLD as the optimizer in worker’s side."
CONCLUSIONS,0.14177215189873418,Under review as a conference paper at ICLR 2022
REFERENCES,0.14261603375527426,REFERENCES
REFERENCES,0.14345991561181434,"Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N Whatmough,
and Venkatesh Saligrama. Federated learning based on dynamic regularization. In International
Conference on Learning Representations, 2021."
REFERENCES,0.14430379746835442,"Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In 2012 IEEE 51st
IEEE Conference on Decision and Control (CDC), pp. 5451–5452. IEEE, 2012."
REFERENCES,0.1451476793248945,"Dmitrii Avdiukhin and Shiva Kasiviswanathan. Federated learning under arbitrary communication
patterns. In International Conference on Machine Learning, pp. 425–435. PMLR, 2021."
REFERENCES,0.1459915611814346,"Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223–311, 2018."
REFERENCES,0.1468354430379747,"Tianyi Chen, Georgios B Giannakis, Tao Sun, and Wotao Yin. Lag: Lazily aggregated gradient for
communication-efﬁcient distributed learning. In NeurIPS, 2018."
REFERENCES,0.14767932489451477,"Tianyi Chen, Yuejiao Sun, and Wotao Yin.
Lasg: Lazily aggregated stochastic gradients for
communication-efﬁcient distributed learning. arXiv preprint arXiv:2002.11360, 2020."
REFERENCES,0.14852320675105485,"Aaron Defazio and Léon Bottou. On the ineffectiveness of variance reduced optimization for deep
learning. arXiv preprint arXiv:1812.04529, 2018."
REFERENCES,0.14936708860759493,"Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013."
REFERENCES,0.150210970464135,"Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019."
REFERENCES,0.15105485232067511,"Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U
Stich, and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in
federated learning. arXiv preprint arXiv:2008.03606, 2020a."
REFERENCES,0.1518987341772152,"Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020b."
REFERENCES,0.15274261603375527,"Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. First analysis of local gd on heteroge-
neous data. arXiv preprint arXiv:1909.04715, 2019."
REFERENCES,0.15358649789029535,"Jakub Konecn`y, H Brendan McMahan, Daniel Ramage, and Peter Richtárik. Federated optimization:
Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016."
REFERENCES,0.15443037974683543,Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.15527426160337554,"Nicolas Le Roux, Mark W Schmidt, and Francis R Bach. A stochastic gradient method with an
exponential convergence rate for ﬁnite training sets. In NIPS, 2012."
REFERENCES,0.15611814345991562,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.1569620253164557,"Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018."
REFERENCES,0.15780590717299578,"Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. arXiv preprint arXiv:1908.07873, 2019a."
REFERENCES,0.15864978902953586,"Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated
learning. arXiv preprint arXiv:1905.10497, 2019b."
REFERENCES,0.15949367088607594,"Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019c."
REFERENCES,0.16033755274261605,Under review as a conference paper at ICLR 2022
REFERENCES,0.16118143459915613,"Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic
gradient descent. In International Conference on Machine Learning, pp. 3043–3052. PMLR, 2018."
REFERENCES,0.1620253164556962,"H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efﬁcient
learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016."
REFERENCES,0.16286919831223629,"Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Interna-
tional Conference on Machine Learning, pp. 4615–4625. PMLR, 2019."
REFERENCES,0.16371308016877636,"Feng Niu, Benjamin Recht, Christopher Ré, and Stephen J Wright. Hogwild!: A lock-free approach
to parallelizing stochastic gradient descent. arXiv preprint arXiv:1106.5730, 2011."
REFERENCES,0.16455696202531644,"Thomas Paine, Hailin Jin, Jianchao Yang, Zhe Lin, and Thomas Huang. Gpu asynchronous stochastic
gradient descent to speed up neural network training. arXiv preprint arXiv:1312.6186, 2013."
REFERENCES,0.16540084388185655,"Zhaonan Qu, Kaixiang Lin, Jayant Kalagnanam, Zhaojian Li, Jiayu Zhou, and Zhengyuan Zhou.
Federated learning’s blessing: Fedavg has linear speedup. arXiv preprint arXiv:2007.05690, 2020."
REFERENCES,0.16624472573839663,"Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint
arXiv:2003.00295, 2020."
REFERENCES,0.1670886075949367,"Yichen Ruan, Xiaoxi Zhang, Shu-Che Liang, and Carlee Joe-Wong. Towards ﬂexible device partici-
pation in federated learning. In International Conference on Artiﬁcial Intelligence and Statistics,
pp. 3403–3411. PMLR, 2021."
REFERENCES,0.1679324894514768,"Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing ﬁnite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83–112, 2017."
REFERENCES,0.16877637130801687,"Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767,
2018."
REFERENCES,0.16962025316455695,"Jianyu Wang and Gauri Joshi. Cooperative sgd: A uniﬁed framework for the design and analysis of
communication-efﬁcient sgd algorithms. arXiv preprint arXiv:1808.07576, 2018."
REFERENCES,0.17046413502109706,"Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective
inconsistency problem in heterogeneous federated optimization. arXiv preprint arXiv:2007.07481,
2020."
REFERENCES,0.17130801687763714,"Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated
optimization. arXiv preprint arXiv:2107.06917, 2021."
REFERENCES,0.17215189873417722,"Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting He, and
Kevin Chan. Adaptive federated learning in resource constrained edge computing systems. IEEE
Journal on Selected Areas in Communications, 37(6):1205–1221, 2019."
REFERENCES,0.1729957805907173,"Cong Xie, Sanmi Koyejo, and Indranil Gupta. Asynchronous federated optimization. arXiv preprint
arXiv:1903.03934, 2019."
REFERENCES,0.17383966244725738,"Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation
in non-{iid} federated learning. In International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=jDdzh5ul-d."
REFERENCES,0.17468354430379746,"Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and
applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1–19, 2019."
REFERENCES,0.17552742616033756,"Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efﬁcient
momentum sgd for distributed non-convex optimization. In International Conference on Machine
Learning, pp. 7184–7193. PMLR, 2019."
REFERENCES,0.17637130801687764,"Xin Zhang, Jia Liu, and Zhengyuan Zhu. Taming convergence for asynchronous stochastic gradient
descent with unbounded delay in non-convex learning. In 2020 59th IEEE Conference on Decision
and Control (CDC), pp. 3580–3585. IEEE, 2020a."
REFERENCES,0.17721518987341772,Under review as a conference paper at ICLR 2022
REFERENCES,0.1780590717299578,"Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. Fedpd: A federated learning
framework with optimal rates and adaptivity to non-iid data. arXiv preprint arXiv:2005.11418,
2020b."
REFERENCES,0.17890295358649788,Under review as a conference paper at ICLR 2022
REFERENCES,0.17974683544303796,Appendix
REFERENCES,0.18059071729957807,"In this supplementary material, we provide the detailed proofs for all theoretical results in this
paper. Before presenting the proofs, we introduce some notations that will be used subsequently..
We assume there exists M workers in total in the FL systems. In each communication round, we
assume a subset Mt of workers to be used, with |Mt| = m. We use Gi(xt) to represent the local
update returned from worker i, i ∈[M] given global model parameter x0
t = xt. Also, we deﬁne
Gi(xt) ≜
1
Kt,i
PKt,i−1
j=0
∇fi(xj
t, ξt,i), where xj
t represents the trajectory of the local model in the
worker. We use ∆i to denote the average of the full gradients long the trajectory of local updates, i.e.,
∆i(xt) =
1
Kt,i
PKt,i−1
j=0
∇fi(xj
t). With the above notations, we are now in a position to present the
proofs of the theoretical results in this paper."
REFERENCES,0.18143459915611815,"A
PROOFS OF LEMMA 1 AND LEMMA 2"
REFERENCES,0.18227848101265823,"We start with proving two results stated in the following two lemmas, which will be useful in the rest
of the proofs."
REFERENCES,0.1831223628691983,"Lemma 1. E[Gi(xt)] = ∆i(xt), E[∥Gi(xt) −∆i(xt)∥2] ≤σ2
L, ∀i ∈[M]."
REFERENCES,0.1839662447257384,"Proof. Taking the expectation of Gi(xt), we have:"
REFERENCES,0.1848101265822785,"E

Gi(xt)

= E
 1 Kt,i"
REFERENCES,0.18565400843881857,"Kt,i−1
X"
REFERENCES,0.18649789029535865,"j=0
∇fi(xj
t, ξt,i)
"
REFERENCES,0.18734177215189873,"=
1
Kt,i"
REFERENCES,0.1881856540084388,"Kt,i−1
X"
REFERENCES,0.1890295358649789,"j=0
E∇fi(xj
t, ξt,i)"
REFERENCES,0.189873417721519,= ∆i(xt).
REFERENCES,0.19071729957805908,"Also, by computing the mean square error between Gi(xt) and ∆i(xt), we have:"
REFERENCES,0.19156118143459916,"E[∥Gi(xt) −∆i(xt)∥2] = E

∥1 Kt,i"
REFERENCES,0.19240506329113924,"Kt,i−1
X"
REFERENCES,0.19324894514767932,"j=0
∇fi(xj
t, ξt,i) −"
REFERENCES,0.1940928270042194,"Kt,i−1
X"
REFERENCES,0.1949367088607595,"j=0
∇fi(xj
t)∥2"
REFERENCES,0.19578059071729959,"=
1
K2
t,i
E

∥"
REFERENCES,0.19662447257383966,"Kt,i−1
X"
REFERENCES,0.19746835443037974,"j=0
∇fi(xj
t, ξt,i) −"
REFERENCES,0.19831223628691982,"Kt,i−1
X"
REFERENCES,0.1991561181434599,"j=0
∇fi(xj
t)∥2"
REFERENCES,0.2,"≤
1
Kt,i"
REFERENCES,0.2008438818565401,"Kt,i−1
X"
REFERENCES,0.20168776371308017,"j=0
E

∥∇fi(xj
t, ξt,i) −"
REFERENCES,0.20253164556962025,"Kt,i−1
X"
REFERENCES,0.20337552742616033,"j=0
∇fi(xj
t)∥2"
REFERENCES,0.2042194092827004,"≤σ2
L."
REFERENCES,0.20506329113924052,This completes the proof of Lemma 1.
REFERENCES,0.2059071729957806,"Lemma 2.
For a ﬁxed set Mt
with cardinality m,
E
 P"
REFERENCES,0.20675105485232068,"i∈Mt Gi(xt−τt,i)
2

≤
 P"
REFERENCES,0.20759493670886076,"i∈Mt ∆i(xt−τt,i)
2 + mσ2
L."
REFERENCES,0.20843881856540084,"Proof. By the deﬁnition of variance (E[(X −E[X])2] = E[X2] −[E[X]]2), we have:"
REFERENCES,0.20928270042194091,"E
 X"
REFERENCES,0.21012658227848102,"i∈Mt
Gi(xt−τt,i)
2

= E
 X"
REFERENCES,0.2109704641350211,"i∈Mt
Gi(xt−τt,i) −∆i(xt−τt,i)
2

+
 X"
REFERENCES,0.21181434599156118,"i∈Mt
∆i(xt−τt,i)
2"
REFERENCES,0.21265822784810126,"≤mσ2
L +
 X"
REFERENCES,0.21350210970464134,"i∈Mt
∆i(xt−τt,i)
2."
REFERENCES,0.21434599156118145,"Here {Gi(xt−τt,i) −∆i(xt−τt,i)} forms a martingale difference sequence."
REFERENCES,0.21518987341772153,Under review as a conference paper at ICLR 2022
REFERENCES,0.2160337552742616,"B
PROOF OF THE PERFORMANCE OF THE AFA-CD ALGORITHM"
REFERENCES,0.2168776371308017,"In this section, we provide the proofs of the theoretical results of the AFA-CD algorithm. We consider
two cases: i) general worker information arrival processes and ii) uniformly distributed worker
information arrivals. As mentioned earlier, for general worker information arrival processes, we do
not make any assumptions on the worker information arrival processes except the independence of
workers’ participation. For uniformly distributed worker information arrivals, Mt can be viewed as a
subset with size m independently and uniformly sampled from [M] without replacement. The similar
convergence analysis for independently and uniformly sampling with replacement can be derived in
the same way following the techniques in (Yang et al., 2021; Li et al., 2019c)."
REFERENCES,0.21772151898734177,"B.1
LOWER BOUND FOR GENERAL WORKER INFORMATION ARRIVAL PROCESSES"
REFERENCES,0.21856540084388185,"Theorem 1 (Convergence Error Lower Bound for General Worker Information Arrival Processes).
For any level of heterogeneity characterized by σG, there exists loss functions and worker participation
process satisfying Assumptions 1- 3 for which the output ˆx of any randomized FL algorithm satisﬁes:"
REFERENCES,0.21940928270042195,"E∥∇f(ˆx)∥2 = Ω(σ2
G),
(1)"
REFERENCES,0.22025316455696203,"Proof. We prove the lower bound by considering a worst-case scenario for simple one-dimensional
functions. Let the FL system has two workers with the following loss functions: f1(x) = (x +
G)2, f2(x) = (x −G)2, f(x) = 1"
REFERENCES,0.2210970464135021,"2(f1(x) + f2(x)) = x2 + G2. It is easy to verify that ∥∇f1(x) −
∇f(x)∥2 ≤4G2 = σ2
G and ∥∇f2(x) −∇f(x)∥2 ≤4G2 = σ2
G. We consider a special case for
the general arrival process when only the ﬁrst one worker participates in the training, equivalent to
optimizing f1(x) rather than f(x). In such case, any decent algorithm would return ˆx = −G + ϵ,
where ϵ is a small error term. As a result, E∥∇f(ˆx)∥2 = Ω(σ2
G)."
REFERENCES,0.2219409282700422,"B.2
GENERAL WORKER INFORMATION ARRIVAL PROCESSES"
REFERENCES,0.22278481012658227,"Theorem 2 (AFA-CD with General Worker Information Arrival Processes). Under Assumptions 1- 3,
choose server-side and worker-side learning rates η and ηL such that the following relationships
hold: 180η2
LK2L2τ < 1 and 2LηηL + 3τ 2L2η2η2
L ≤1. Then, the output sequence {xt} generated
by AFedAvg-TSLR-CD with general worker information arrival processes satisﬁes:"
T,0.22362869198312235,"1
T"
T,0.22447257383966246,"T −1
X"
T,0.22531645569620254,"t=0
E∥∇f(xt)∥2 ≤4(f0 −f∗)"
T,0.22616033755274262,"ηηLT
+ 4
 
αLσ2
L + αGσ2
G

,
(2)"
T,0.2270042194092827,"where αL ≜
 2ηηL"
T,0.22784810126582278,"m
+ 3τ 2L2η2η2
L
2m
+ 15η2
LKL2"
T,0.22869198312236286,"2

and αG ≜
 3"
T,0.22953586497890296,"2 + 45K2L2η2
L

."
T,0.23037974683544304,"Proof. Due to the L-smoothness assumption, taking expectation of f(xt+1) over the randomness in
communication round t, we have:"
T,0.23122362869198312,"E[f(xt+1)] ≤f(xt) +

∇f(xt), E[xt+1 −xt]"
T,0.2320675105485232,"|
{z
}
A1 +L"
T,0.23291139240506328,"2 E[∥xt+1 −xt∥2
|
{z
}
A2 ."
T,0.23375527426160336,"First, we bound the term A2 as follows:"
T,0.23459915611814347,A2 = E∥xt+1 −xt∥2
T,0.23544303797468355,"= η2η2
LE∥1 m m
X"
T,0.23628691983122363,"i=1
Gi(xt−τt,i)∥2"
T,0.2371308016877637,"(a1)
≤η2η2
L
m2 ∥ m
X"
T,0.2379746835443038,"i=1
∆i(xt−τt,i)∥2 + η2η2
L
m σ2
L,"
T,0.23881856540084387,"where (a1) is due to Lemma 2. Next, we bound the term A1 as follows:"
T,0.23966244725738398,"A1 =

∇f(xt), E[xt+1 −xt]"
T,0.24050632911392406,Under review as a conference paper at ICLR 2022
T,0.24135021097046414,"= −ηηL

∇f(xt), E 1 m m
X"
T,0.24219409282700421,"i=1
Gi(xt−τt,i)"
T,0.2430379746835443,"(a2)
= −1"
T,0.2438818565400844,2ηηL∥∇f(xt)∥2 −1
T,0.24472573839662448,"2ηηL∥1 m m
X"
T,0.24556962025316456,"i=1
∆i(xt−τt,i)∥2 + 1"
T,0.24641350210970464,"2ηηL ∥∇f(xt) −1 m m
X"
T,0.24725738396624472,"i=1
∆i(xt−τt,i)∥2"
T,0.2481012658227848,"|
{z
}
A3 ,"
T,0.2489451476793249,"where (a2) is due to Lemma 1 and the fact that ⟨x, y⟩= 1"
T,0.249789029535865,"2(∥x∥2 + ∥y∥2 −∥x −y∥2). To further
bound the term A3, we have:"
T,0.25063291139240507,"A3 = ∥∇f(xt) −1 m m
X"
T,0.2514767932489452,"i=1
∆i(xt−τt,i)∥2 ≤1 m m
X"
T,0.2523206751054852,"i=1
∥∇f(xt) −∆i(xt−τt,i)∥2 = 1 m m
X"
T,0.25316455696202533,"i=1
∥∇f(xt) −∇f(xt−τt,i) + ∇f(xt−τt,i) −∇fi(xt−τt,i) + ∇fi(xt−τt,i) −∆i(xt−τt,i)∥2"
T,0.2540084388185654,"(a3)
≤
1
m m
X i=1"
T,0.2548523206751055,"
3∥∇f(xt) −∇f(xt−τt,i)∥2 + 3∥∇f(xt−τt,i) −∇fi(xt−τt,i)∥2"
T,0.25569620253164554,"+ 3∥∇fi(xt−τt,i) −∆i(xt−τt,i)∥2
"
T,0.25654008438818565,"(a4)
≤3L2 m m
X"
T,0.25738396624472576,"i=1
∥xt −xt−τt,i∥2"
T,0.2582278481012658,"|
{z
}
A4"
T,0.2590717299578059,"+3σ2
G + 3 m m
X"
T,0.25991561181434597,"i=1
∥∇fi(xt−τt,i) −∆i(xt−τt,i)∥2
|
{z
}
A5 ,"
T,0.2607594936708861,"where (a3) followings from the inequality ∥x1 + x2 + · · · + xn∥2 ≤n Pn
i=1 ∥xi∥2, and (a4)
is due to the L-smoothness assumption (Assumption 1) and bounded global variance assumption
(Assumption 3)."
T,0.2616033755274262,"To further bound the term A4, we have:"
T,0.26244725738396624,A4 = 1 m X
T,0.26329113924050634,"i∈[m]
∥xt −xt−τt,i∥2"
T,0.2641350210970464,"(a5)
≤∥xt −xt−τt,u∥2 = ∥ t−1
X"
T,0.2649789029535865,"k=t−τt,u
xk+1 −xk∥2 = E∥ t−1
X"
T,0.26582278481012656,"k=t−τt,u"
T,0.26666666666666666,"1
mηηL
X"
T,0.26751054852320677,"i∈Mk
Gi(xk−τk,i)∥2"
T,0.2683544303797468,"= E
η2η2
L
m2 ∥ t−1
X"
T,0.26919831223628693,"k=t−τt,u X"
T,0.270042194092827,"i∈Mk
Gi(xk−τk,i)∥2
"
T,0.2708860759493671,"(a6)
≤E
η2η2
L
m2 τ t−1
X"
T,0.2717299578059072,"k=t−τt,u
∥
X"
T,0.27257383966244725,"i∈Mk
Gi(xk−τk,i)∥2
"
T,0.27341772151898736,"(a7)
≤E
η2η2
Lτ
m2
[( t−1
X"
T,0.2742616033755274,"k=t−τt,u
∥
X"
T,0.2751054852320675,"i∈Mk
∆i(xk−τk,i)∥2) + τmσ2
L]

."
T,0.2759493670886076,Under review as a conference paper at ICLR 2022
T,0.2767932489451477,"In the derivations above, we let u := argmaxi∈[M] ∥xt −xt−τt,i∥2, which yields (a5). Note also
that the maximum delay assumption τ ≥τk,i, ∀i ∈[M] implies (a6). Lastly, (a7) follows from
Lemma 2."
T,0.2776371308016878,"To further bound the term A5, we have:"
T,0.27848101265822783,"A5 = ∥∇fi(xt−τt,i) −∆i(xt−τt,i)∥2"
T,0.27932489451476794,"= ∥∇fi(xt−τt,i) −
1
Kt,i"
T,0.280168776371308,"Kt,i−1
X"
T,0.2810126582278481,"j=0
∇fi(xj
t−τt,i)∥2"
T,0.2818565400843882,"=
1
Kt,i"
T,0.28270042194092826,"Kt,i−1
X"
T,0.28354430379746837,"j=0
∥∇fi(xt−τt,i) −∇fi(xj
t−τt,i)∥2"
T,0.2843881856540084,"(a8)
≤
L2 Kt,i"
T,0.2852320675105485,"Kt,i−1
X"
T,0.28607594936708863,"j=0
∥xt−τt,i −xj
t−τt,i∥2
|
{z
}
A6"
T,0.2869198312236287,"(a9)
≤5Kt,iL2η2
L(σ2
L + 6Kt,iσ2
G) + 30K2
t,iL2η2
L∥∇f(xt−τt,i)∥2"
T,0.2877637130801688,"(a10)
≤
5KL2η2
L(σ2
L + 6Kσ2
G) + 30K2L2η2
L∥∇f(xt−τt,i)∥2,"
T,0.28860759493670884,"where (a8) is due to the L-smoothness assumption (Assumption 1), and (a9) follows from the bound
of A6 shown below. Here, we denote maximum number of local steps of all workers as K, i.e.,
Kt,i ≤K, ∀t, i. This deﬁnition of K implies (a10)."
T,0.28945147679324895,"Now, it remains to bound term A6 in the derivations above. Note that the bounding proof of A6 in
what follows is the same as Lemma 4 in (Reddi et al., 2020). we restate the proof here in order for
this paper to be self-contained. For any worker i in the k-th local step, we have the following results
for the norm of parameter changes for one local computation:"
T,0.290295358649789,"A6 = E[∥xi
t,k −xt∥2] = E[∥xi
t,k−1 −xt −ηLgi
t,k−1∥2]"
T,0.2911392405063291,"≤E[∥xi
t,k−1 −xt −ηL(gi
t,k−1 −∇fi(xi
t,k−1) + ∇fi(xi
t,k−1) −∇fi(xt)"
T,0.2919831223628692,+ ∇fi(xt) −∇f(xt) + ∇f(xt))∥2]
T,0.29282700421940927,"≤(1 +
1
2K −1)E[∥xi
t,k−1 −xt∥2] + E[∥ηL(gi
t,k−1 −∇fi(xi
t,k−1))∥2]"
T,0.2936708860759494,"+ 6KE[∥ηL(∇fi(xi
t,k−1) −∇fi(xt))∥2] + 6KE[∥ηL(∇fi(xt) −∇f(xt)))∥2]"
T,0.29451476793248943,+ 6K∥ηL∇f(xt))∥2
T,0.29535864978902954,"≤(1 +
1
2K −1)E[∥xi
t,k−1 −xt∥2] + η2
Lσ2
L + 6Kη2
LL2E[∥xi
t,k−1 −xt∥2]"
T,0.29620253164556964,"+ 6Kη2
Lσ2
G + 6K∥ηL∇f(xt))∥2"
T,0.2970464135021097,"= (1 +
1
2K −1 + 6Kη2
LL2)E[∥xi
t,k−1 −xt∥2] + η2
Lσ2
L + 6Kη2
Lσ2
G + 6K∥ηL∇f(xt))∥2"
T,0.2978902953586498,"(a11)
≤
(1 +
1
K −1)E[∥xi
t,k−1 −xt∥2] + η2
Lσ2
L + 6Kη2
Lσ2
G + 6K∥ηL∇f(xt))∥2,"
T,0.29873417721518986,"where (a11) follows from the fact that
1
2K−1 + 6Kη2
LL2 ≤
1
K−1 if η2
L ≤
1
6(2K2−3K+1)L2 ."
T,0.29957805907172996,"Unrolling the recursion, we obtain:"
T,0.30042194092827,"E[∥xi
t,k −xt∥2] ≤ k−1
X"
T,0.3012658227848101,"p=0
(1 +
1
K −1)p[η2
Lσ2
L + 6Kσ2
G + 6Kη2
L∥ηL∇f(xt))∥2]"
T,0.30210970464135023,"≤(K −1)[(1 +
1
K −1)K −1][η2
Lσ2
L + 6Kη2
Lσ2
G + 6K∥ηL∇f(xt))∥2]"
T,0.3029535864978903,"≤5Kη2
L(σ2
L + 6Kσ2
G) + 30K2η2
L∥∇f(xt)∥2.
(9)"
T,0.3037974683544304,Under review as a conference paper at ICLR 2022
T,0.30464135021097044,"With the above results of the terms A1 through A5, we have:"
T,0.30548523206751055,"E[f(xt+1)] −f(xt) ≤

∇f(xt), E[xt+1 −xt]"
T,0.30632911392405066,"|
{z
}
A1 +L"
T,0.3071729957805907,"2 E[∥xt+1 −xt∥2
|
{z
}
A2 = −1"
T,0.3080168776371308,2ηηL∥∇f(xt)∥2 −1
T,0.30886075949367087,"2ηηL∥1 m m
X"
T,0.309704641350211,"i=1
∆i(xt−τt,i)∥2 + 1"
T,0.3105485232067511,"2ηηL ∥∇f(xt) −1 m m
X"
T,0.31139240506329113,"i=1
∆i(xt−τt,i)∥2"
T,0.31223628691983124,"|
{z
}
A3"
T,0.3130801687763713,"+ Lη2η2
L
m2
∥ m
X"
T,0.3139240506329114,"i=1
∆i(xt−τt,i)∥2 + Lη2η2
L
m
σ2
L ≤−1"
T,0.31476793248945145,2ηηL∥∇f(xt)∥2 −1
T,0.31561181434599156,"2ηηL∥1 m m
X"
T,0.31645569620253167,"i=1
∆i(xt−τt,i)∥2 + Lη2η2
L
m2
∥ m
X"
T,0.3172995780590717,"i=1
∆i(xt−τt,i)∥2 + Lη2η2
L
m
σ2
L + 3"
T,0.3181434599156118,"2ηηLσ2
G + 3L2 2 ηηL  1 m m
X"
T,0.3189873417721519,"i=1
∥xt −xt−τt,i∥2"
T,0.319831223628692,"|
{z
}
A4"
T,0.3206751054852321,"
+ 3ηηL"
M,0.32151898734177214,"2m m
X"
M,0.32236286919831225,"i=1
∥∇fi(xt−τt,i) −∆i(xt−τt,i)∥2
|
{z
}
A5 ≤−1"
M,0.3232067510548523,2ηηL∥∇f(xt)∥2 −1
M,0.3240506329113924,"2ηηL∥1 m m
X"
M,0.32489451476793246,"i=1
∆i(xt−τt,i)∥2 + Lη2η2
L
m2
∥ m
X"
M,0.32573839662447257,"i=1
∆i(xt−τt,i)∥2 + Lη2η2
L
m
σ2
L + 3"
M,0.3265822784810127,"2ηηLσ2
G + 3L2 2 ηηL"
M,0.32742616033755273,"η2η2
Lτ
m2
( t−1
X"
M,0.32827004219409284,"k=t−τt,u
∥ m
X"
M,0.3291139240506329,"i=1
∆i(xk−τk,i)∥2 + τmσ2
L)
"
M,0.329957805907173,+ 3ηηL
M,0.3308016877637131,"2
[5KL2η2
L(σ2
L + 6Kσ2
G) + 30K2L2η2
L
1
m m
X"
M,0.33164556962025316,"i=1
∥∇f(xt−τt,i)∥2] ≤−1"
M,0.33248945147679326,"2ηηL∥∇f(xt)∥2 + 45ηη3
LK2L2 1 m m
X"
M,0.3333333333333333,"i=1
∥∇f(xt−τt,i)∥2"
M,0.3341772151898734,"+

−ηηL"
M,0.33502109704641353,"2m2 + Lη2η2
L
m2 
∥ m
X"
M,0.3358649789029536,"i=1
∆i(xt−τt,i)∥2 + 3τη3η3
L
2m2 t−1
X"
M,0.3367088607594937,"k=t−τt,u
∥ m
X"
M,0.33755274261603374,"i=1
∆i(xk−τk,i)∥2"
M,0.33839662447257385,"+
Lη2η2
L
m
+ 3τ 2L2η3η3
L
2m
+ 15ηη3
LKL2 2"
M,0.3392405063291139,"
σ2
L +
3"
M,0.340084388185654,"2ηηL + 45K2L2ηη3
L"
M,0.3409282700421941,"
σ2
G."
M,0.34177215189873417,Summing the above inequality from t = 0 to t = T −1 yields:
M,0.3426160337552743,Ef(xT ) −f(x0) ≤
M,0.3434599156118143,"T −1
X t=0 
−1"
M,0.34430379746835443,"2ηηL∥∇f(xt)∥2 + 45ηη3
LK2L2 1 m m
X"
M,0.34514767932489454,"i=1
∥∇f(xt−τt,i)∥2
 +"
M,0.3459915611814346,"T −1
X t=0"
M,0.3468354430379747,"
[−ηηL"
M,0.34767932489451475,"2m2 + Lη2η2
L
m2
]∥ m
X"
M,0.34852320675105486,"i=1
∆i(xt−τt,i)∥2 + 3τL2η3η3
L
2m2 t−1
X"
M,0.3493670886075949,"k=t−τt,u
∥ m
X"
M,0.350210970464135,"i=1
∆i(xk−τk,i)∥2
"
M,0.3510548523206751,"+ T
Lη2η2
L
m
+ 3τ 2L2η3η3
L
2m
+ 15ηη3
LKL2 2"
M,0.3518987341772152,"
σ2
L + T
3"
M,0.3527426160337553,"2ηηL + 45K2L2ηη3
L"
M,0.35358649789029534,"
σ2
G"
M,0.35443037974683544,"(a12)
≤"
M,0.35527426160337555,"T −1
X t=0 
−1"
M,0.3561181434599156,"2ηηL + 45ηη3
LK2L2τ

∥∇f(xt)∥2 +"
M,0.3569620253164557,"T −1
X t=0"
M,0.35780590717299576,"
−ηηL"
M,0.35864978902953587,"2m2 + Lη2η2
L
m2
+ 3τ 2L2η3η3
L
2m2 
∥ m
X"
M,0.3594936708860759,"i=1
∆i(xt−τt,i)∥2"
M,0.36033755274261603,"+ T
Lη2η2
L
m
+ 3τ 2L2η3η3
L
2m
+ 15ηη3
LKL2 2"
M,0.36118143459915614,"
σ2
L + T
3"
M,0.3620253164556962,"2ηηL + 45K2L2ηη3
L"
M,0.3628691983122363,"
σ2
G"
M,0.36371308016877635,Under review as a conference paper at ICLR 2022
M,0.36455696202531646,"(a13)
≤"
M,0.36540084388185656,"T −1
X"
M,0.3662447257383966,"t=0
−1"
M,0.3670886075949367,4ηηL∥∇f(xt)∥2
M,0.3679324894514768,+ TηηL LηηL
M,0.3687763713080169,"m
+ 3τ 2L2η2η2
L
2m
+ 15η2
LKL2 2"
M,0.369620253164557,"
σ2
L + TηηL 3"
M,0.37046413502109704,"2 + 45K2L2η2
L"
M,0.37130801687763715,"
σ2
G"
M,0.3721518987341772,"(a14)
="
M,0.3729957805907173,"T −1
X"
M,0.37383966244725736,"t=0
−1"
M,0.37468354430379747,"4ηηL∥∇f(xt)∥2 + TηηL

αLσ2
L + αGσ2
G

,"
M,0.3755274261603376,"where (a12) is due to maximum time delay τ in the system, (a13) holds if 1"
M,0.3763713080168776,4 ≤[ 1
M,0.37721518987341773,"2 −45η2
LK2L2τ],"
M,0.3780590717299578,"i.e., 180η2
LK2L2τ < 1, and

−ηηL"
M,0.3789029535864979,"2m2 + Lη2η2
L
m2
+ 3L2τ 2η3η3
L
2m2"
M,0.379746835443038,"
≤0, i.e., 2LηηL + 3τ 2L2η2η2
L ≤1."
M,0.38059071729957805,"Lastly, (a14) follows from the following deﬁnitions: αL =
 2ηηL"
M,0.38143459915611816,"m
+ 3τ 2L2η2η2
L
2m
+ 15η2
LKL2"
M,0.3822784810126582,"2

, αG =
 3"
M,0.3831223628691983,"2 + 45K2L2η2
L

. Rearranging terms, we have:"
T,0.38396624472573837,"1
T"
T,0.3848101265822785,"T −1
X"
T,0.3856540084388186,"t=0
∥∇f(xt)∥2 ≤4(f0 −f∗)"
T,0.38649789029535864,"ηηLT
+ 4

αLσ2
L + αGσ2
G

,"
T,0.38734177215189874,and the proof is complete.
T,0.3881856540084388,"Corollary 1 (Linear Speedup to Error Ball). By setting ηL =
1
√"
T,0.3890295358649789,"T , and η = √m, the convergence
rate of AFA-CD with general worker information arrival processes is:"
T,0.389873417721519,"1
T"
T,0.39071729957805906,"T −1
X"
T,0.39156118143459917,"t=0
E∥∇f(xt)∥2 = O

1
m1/2T 1/2"
T,0.3924050632911392,"
+ O
τ 2 T"
T,0.39324894514767933,"
+ O
K2 T"
T,0.39409282700421944,"
+ O(σ2
G)."
T,0.3949367088607595,"Proof. Let ηL =
1
√"
T,0.3957805907172996,"T , and η = √m. It then follows that:"
T,0.39662447257383965,"αL = O(
1
m1/2T 1/2 ) + O(τ 2"
T,0.39746835443037976,T ) + O(K T ).
T,0.3983122362869198,"αG = O(σ2
G) + O(K2 T )."
T,0.3991561181434599,This completes the proof.
T,0.4,"B.3
UNIFORMLY DISTRIBUTED WORKER INFORMATION ARRIVALS"
T,0.4008438818565401,"Now, we consider the special case where all workers have a statistically identical speed so that the
worker information arrivals are uniformly distributed. As mentioned earlier, this special case acts as
a widely-used assumption in FL and could deepen our understanding on the AFA-CD algorithm’s
performance in large-scale AFL systems.
Theorem 3. Under Assumptions 1- 3, choose server-side and worker-side learning rates η and ηL
such that the following relationships hold: η2
L[6(2K2 −3K +1)L2] ≤1, 120L2K2η2
Lτ +4(LηηL +
L2η2η2
Lτ 2)
M−m
m(M−1)(90K2L2η2
Lτ + 3τ) < 1. Then, the output sequence {xt} generated by AFA-CD
with uniformly distributed worker information arrivals satisﬁes:"
T,0.4016877637130802,"1
T"
T,0.40253164556962023,"T −1
X"
T,0.40337552742616034,"t=0
E∥∇f(xt)∥2
2 ≤4(f0 −f∗)"
T,0.40421940928270045,"ηηLT
+ 4
 
αLσ2
L + αGσ2
G

,
(3)"
T,0.4050632911392405,where αL and αG are constants that are deﬁned as:
T,0.4059071729957806,"αL ≜
LηηL"
T,0.40675105485232066,"m
+ τ 2L2η2η2
L
m
+ 5KL2η2
L"
T,0.40759493670886077,"
+ (LηηL + L2η2η2
Lτ 2) M −m"
T,0.4084388185654008,"m(M −1)(15KL2η2
L)

,"
T,0.4092827004219409,"αG ≜

30K2L2η2
L + (LηηL + L2η2η2
Lτ 2) M −m"
T,0.41012658227848103,"m(M −1)(90K2L2η2
L + 3)

."
T,0.4109704641350211,Under review as a conference paper at ICLR 2022
T,0.4118143459915612,"Proof. The one-step update can be rewritten as: xt+1 −xt = −ηηLGt. For cross-device FL,
Gt =
1
m
P"
T,0.41265822784810124,"i∈Mt Gi(xt−τt,i), where τt,i is the delay for client i in terms of the current global
communication round t. When τt,i = 0, ∀i ∈Mt, it degenerates to synchronous FL with partial
worker participation."
T,0.41350210970464135,"Due to the L-smoothness in Assumption 1 , taking expectation of f(xt+1) over the randomness in
communication round t, we have:"
T,0.41434599156118146,"E[f(xt+1)] ≤f(xt) +

∇f(xt), E[xt+1 −xt]"
T,0.4151898734177215,"|
{z
}
A1 +L"
T,0.4160337552742616,"2 E[∥xt+1 −xt∥2
|
{z
}
A2"
T,0.41687763713080167,We ﬁrst bound A2 as follows:
T,0.4177215189873418,A2 = E∥xt+1 −xt∥2
T,0.41856540084388183,"= η2η2
LE∥1 m X"
T,0.41940928270042194,"i∈Mt
Gi(xt−τt,i)∥2"
T,0.42025316455696204,"(b1)
≤η2η2
L
m2 E

∥ m
X"
T,0.4210970464135021,"i=1
∆i(xt−τt,i)∥2 + mσ2
L "
T,0.4219409282700422,"(b2)
≤η2η2
L
m2 E∥ M
X"
T,0.42278481012658226,"i=1
I{i ∈Mt}∆i(xt−τt,i)∥2 + η2η2
L
m σ2
L,"
T,0.42362869198312236,"where (b1) is due to Lemma 2 and (b2) is due to the uniformly independent information arrival
assumption."
T,0.42447257383966247,"To bound the term A1, we have:"
T,0.4253164556962025,"A1 =

∇f(xt), E[xt+1 −xt]"
T,0.42616033755274263,"= −ηηL

∇f(xt), E 1 m X"
T,0.4270042194092827,"i∈Mt
Gi(xt−τt,i)"
T,0.4278481012658228,"(b3)
= −ηηL

∇f(xt), 1 M X"
T,0.4286919831223629,"i∈[M]
∆i(xt−τt,i)"
T,0.42953586497890295,"(b4)
= −1"
T,0.43037974683544306,2ηηL∥∇f(xt)∥2 −1
T,0.4312236286919831,2ηηL∥1 M X
T,0.4320675105485232,"i∈[M]
∆i(xt−τt,i)∥2 + 1 2ηηL"
T,0.43291139240506327,∇f(xt) −1 M X
T,0.4337552742616034,"i∈[M]
∆i(xt−τt,i) 2"
T,0.4345991561181435,"|
{z
}
A3 ,"
T,0.43544303797468353,"where (b3) is due to the uniformly independent worker information arrival assumption and Lemma 1,
(b4) is due to the fact that ⟨x, y⟩= 1"
T,0.43628691983122364,2(∥x∥2 + ∥y∥2 −∥x −y∥2).
T,0.4371308016877637,"To further bound the term A3, we have:"
T,0.4379746835443038,A3 = ∥∇f(xt) −1 M X
T,0.4388185654008439,"i∈[M]
∆i(xt−τt,i)∥2"
T,0.43966244725738396,"(b5)
=
 1 M X"
T,0.44050632911392407,"i∈[M]
[∇fi(xt) −∆i(xt−τt,i)]
2 ≤1 M X i∈[M]"
T,0.4413502109704641,"∇fi(xt) −∆i(xt−τt,i)
2 = 1 M X i∈[M]"
T,0.4421940928270042,"∇fi(xt) −∇fi(xt−τt,i) + ∇fi(xt−τt,i) −∆i(xt−τt,i)
2"
T,0.4430379746835443,Under review as a conference paper at ICLR 2022
T,0.4438818565400844,"(b6)
≤
1
M X i∈[M]"
T,0.4447257383966245,"
2∥∇fi(xt) −∇fi(xt−τt,i)∥2 + 2∥∇fi(xt−τt,i) −∆i(xt−τt,i)∥2
"
T,0.44556962025316454,"(b7)
≤2L2 M M
X i=1"
T,0.44641350210970465,"xt −xt−τt,i
2"
T,0.4472573839662447,"|
{z
}
A4 + 2 M M
X i=1"
T,0.4481012658227848,"∇fi(xt−τt,i) −∆i(xt−τt,i)
2
|
{z
}
A5 ,"
T,0.4489451476793249,"where (b5) is due to the fact that ∇f(x) =
1
M
P"
T,0.44978902953586497,"i∈[M] ∇fi(x), (b6) follows from the inequality
∥x1 + x2 + · · · + xn∥2 ≤n Pn
i=1 ∥xi∥2, and (b7) follows from the L-smoothness assumption
(Assumption 1)."
T,0.4506329113924051,"For A4 and A5, we have the same bounds as in the case of general worker information arrival
processes:"
T,0.45147679324894513,"A4 ≤E
η2η2
Lτ
m2
[( t−1
X"
T,0.45232067510548524,"k=t−τt,µ
∥
X"
T,0.4531645569620253,"i∈Mk
∆i(xk−τk,i)∥2) + τmσ2
L]
"
T,0.4540084388185654,"≤η2η2
Lτ
m2 
( t−1
X"
T,0.4548523206751055,"k=t−τt,µ
E∥ M
X"
T,0.45569620253164556,"i=1
I{i ∈Mk}∆i(xk−τk,i)
2) + τmσ2
L 
."
T,0.45654008438818566,"A5 ≤5KL2η2
L(σ2
L + 6Kσ2
G) + 30K2L2η2
L∥∇f(xt−τt,i)∥2,"
T,0.4573839662447257,"With the above results of the term A1 through A5, we have:"
T,0.4582278481012658,"Et[f(xt+1)] −f(xt) ≤

∇f(xt), Et[xt+1 −xt]"
T,0.45907172995780593,"|
{z
}
A1 +L"
T,0.459915611814346,"2 Et[∥xt+1 −xt∥2
|
{z
}
A2 = −1"
T,0.4607594936708861,2ηηL∥∇f(xt)∥2 −1
T,0.46160337552742614,2ηηL∥1 M X
T,0.46244725738396625,"i∈[M]
∆i(xt−τt,i)∥2 + 1"
T,0.46329113924050636,2ηηL ∥∇f(xt) −1 M X
T,0.4641350210970464,"i∈[M]
∆i(xt−τt,i)∥2"
T,0.4649789029535865,"|
{z
}
A3"
T,0.46582278481012657,"+ Lη2η2
L
m2
E∥ M
X"
T,0.4666666666666667,"i=1
I{i ∈Mt}∆i(xt−τt,i)∥2 + Lη2η2
L
m
σ2
L ≤−1"
T,0.4675105485232067,2ηηL∥∇f(xt)∥2 −1
T,0.46835443037974683,2ηηL∥1 M X
T,0.46919831223628694,"i∈[M]
∆i(xt−τt,i)∥2 + Lη2η2
L
m2
E∥ M
X"
T,0.470042194092827,"i=1
I{i ∈Mt}∆i(xt−τt,i)∥2 + 1 2ηηL  2L2 M M
X"
T,0.4708860759493671,"i=1
∥xt −xt−τt,i∥2"
T,0.47172995780590715,"|
{z
}
A4 + 2 M M
X"
T,0.47257383966244726,"i=1
∥∇fi(xt−τt,i) −∆i(xt−τt,i)∥2
|
{z
}
A5"
T,0.47341772151898737,"
+ Lη2η2
L
m
σ2
L ≤−1"
T,0.4742616033755274,2ηηL∥∇f(xt)∥2 −1
T,0.4751054852320675,2ηηL∥1 M X
T,0.4759493670886076,"i∈[M]
∆i(xt−τt,i)∥2 + Lη2η2
L
m2
E∥ M
X"
T,0.4767932489451477,"i=1
I{i ∈Mt}∆i(xt−τt,i)∥2"
T,0.47763713080168774,"+ ηηLL2
η2η2
Lτ
m2
( t−1
X"
T,0.47848101265822784,"k=t−τt,µ
E∥ M
X"
T,0.47932489451476795,"i=1
I{i ∈Mk}∆i(xk−τk,i)∥2 + τmσ2
L)
 + ηηL"
T,0.480168776371308,"
5KL2η2
L(σ2
L + 6Kσ2
G) + 30K2L2η2
L
1
M X"
T,0.4810126582278481,"i∈[M]
∥∇f(xt−τt,i)∥2

+ Lη2η2
L
m
σ2
L"
T,0.48185654008438816,"(b8)
≤

−1"
T,0.48270042194092827,"2ηηL∥∇f(xt)∥2 + (30ηK2L2η3
L)∥∇f(xt−τt,j)∥2
"
T,0.4835443037974684,"+

−ηηL"
T,0.48438818565400843,"2M 2 ∥ M
X"
T,0.48523206751054854,"i=1
∆i(xt−τt,i)∥2 + Lη2η2
L
m2
E∥ M
X"
T,0.4860759493670886,"i=1
I{i ∈Mt}∆i(xt−τt,i)∥2"
T,0.4869198312236287,Under review as a conference paper at ICLR 2022
T,0.4877637130801688,"+ L2η3η3
Lτ
m2 t−1
X"
T,0.48860759493670886,"k=t−τt,µ
E∥ M
X"
T,0.48945147679324896,"i=1
I{i ∈Mk}∆i(xk−τk,i)∥2
"
T,0.490295358649789,"+ σ2
L"
T,0.4911392405063291,"Lη2η2
L
m
+ τ 2L2η3η3
L
m
+ 5KηL2η3
L"
T,0.4919831223628692,"
+ 30ηK2L2η3
Lσ2
G,"
T,0.4928270042194093,"where (b8) follows from j := argmaxi∈[M] ∥∇f(xt−τt,i)∥2. Note j is dependent on t but we omit
it for brevity."
T,0.4936708860759494,"Summing the above inequality from t = 0 to t = T −1 yields:
Ef(xT ) −f(x0) ≤"
T,0.49451476793248944,"T −1
X t=0 
−1"
T,0.49535864978902955,"2ηηL∥∇f(xt)∥2 + (30ηK2L2η3
L)∥∇f(xt−τt,j)∥2
 +"
T,0.4962025316455696,"T −1
X t=0"
T,0.4970464135021097,"
−ηηL"
T,0.4978902953586498,"2M 2 ∥
X"
T,0.49873417721518987,"i∈[M]
∆i(xt−τt,i)∥2 + Lη2η2
L
m2
E∥ M
X"
T,0.49957805907173,"i=1
I{i ∈Mt}∆i(xt−τt,i)∥2"
T,0.5004219409282701,"+ L2η3η3
Lτ
m2 t−1
X"
T,0.5012658227848101,"k=t−τt,µ
E∥
X"
T,0.5021097046413502,"i∈[M]
I{i ∈Mk}∆i(xk−τk,i)∥2
"
T,0.5029535864978903,"+ T

σ2
L(Lη2η2
L
m
+ τ 2L2η3η3
L
m
+ 5KηL2η3
L) + 30ηK2L2η3
Lσ2
G "
T,0.5037974683544304,"(b9)
≤"
T,0.5046413502109705,"T −1
X t=0 
−1"
T,0.5054852320675105,"2ηηL∥∇f(xt)∥2 + (30ηK2L2η3
Lτ)∥∇f(xt)∥2
 +"
T,0.5063291139240507,"T −1
X t=0"
T,0.5071729957805907,"
−ηηL"
T,0.5080168776371308,"2M 2 ∥
X"
T,0.5088607594936709,"i∈[M]
∆i(xt−τt,i)∥2 + Lη2η2
L
m2
E∥
X"
T,0.509704641350211,"i∈[M]
I{i ∈Mt}∆i(xt−τt,i)∥2"
T,0.510548523206751,"+ L2η3η3
Lτ 2"
T,0.5113924050632911,"m2
E∥
X"
T,0.5122362869198313,"i∈[M]
I{i ∈Mt}∆i(xt−τt,i)∥2
"
T,0.5130801687763713,"+ T

σ2
L(Lη2η2
L
m
+ τ 2L2η3η3
L
m
+ 5KηL2η3
L) + 30ηK2L2η3
Lσ2
G 
,"
T,0.5139240506329114,where (b9) is due to the fact that the delay in the system is less than τ.
T,0.5147679324894515,"By letting zi = ∆i(xt−τt,i) (omitting the communication round index t for notation simplicity), we
have that: ∥ M
X"
T,0.5156118143459916,"i=1
zi∥2 =
X"
T,0.5164556962025316,"i∈[M]
∥zi∥2 +
X"
T,0.5172995780590718,"i̸=j
⟨zi, zj⟩,"
T,0.5181434599156118,"(b10)
=
X"
T,0.5189873417721519,"i∈[M]
M∥zi∥2 −1 2 X"
T,0.5198312236286919,"i̸=j
∥zi −zj∥2, E∥ M
X"
T,0.5206751054852321,"i=1
I{i ∈Mt}zi∥2 =
X"
T,0.5215189873417722,"i∈[M]
P{i ∈Mt}∥zi∥2 +
X"
T,0.5223628691983122,"i̸=j
P{i, j ∈Mt}⟨zi, zj⟩"
T,0.5232067510548524,"(b11)
=
m
M X"
T,0.5240506329113924,"i∈[M]
∥zi∥2 + m(m −1)"
T,0.5248945147679325,M(M −1) X
T,0.5257383966244725,"i̸=j
⟨zi, zj⟩"
T,0.5265822784810127,"(b12)
=
m2 M X"
T,0.5274261603375527,"i∈[M]
∥zi∥2 −m(m −1)"
T,0.5282700421940928,2M(M −1) X
T,0.529113924050633,"i̸=j
∥zi −zj∥2,"
T,0.529957805907173,"where (b10) and (b12) are due to the fact that

x, y

= 1"
T,0.5308016877637131,2[∥x∥2+∥y∥2−∥x−y∥2] ≤1
T,0.5316455696202531,"2[∥x∥2+∥y∥2],
(b11) follows from the fact that P{i ∈Mt} = m"
T,0.5324894514767933,"M and P{i, j ∈Mt} =
m(m−1)
M(M−1). It then follows"
T,0.5333333333333333,Under review as a conference paper at ICLR 2022 that: −ηηL
T,0.5341772151898734,"2M 2 ∥ M
X"
T,0.5350210970464135,"i=1
zi∥2 + Lη2η2
L
m2
E∥ M
X"
T,0.5358649789029536,"i=1
I{i ∈Mt}zi∥2 + L2η3η3
Lτ 2 m2
E∥ M
X"
T,0.5367088607594936,"i=1
I{i ∈Mt}zi∥2"
T,0.5375527426160338,"=

−ηηL"
T,0.5383966244725739,"2M + (Lη2η2
L
M
+ L2η3η3
Lτ 2"
T,0.5392405063291139,"M
)
 M
X"
T,0.540084388185654,"i=1
∥zi∥2"
T,0.5409282700421941,"+
 ηηL"
T,0.5417721518987342,"4M 2 −(Lη2η2
L
m2
+ L2η3η3
Lτ 2"
T,0.5426160337552742,"m2
) m(m −1)"
T,0.5434599156118144,2M(M −1)  X
T,0.5443037974683544,"i̸=j
∥zi −zj∥2"
T,0.5451476793248945,"≤

−ηηL"
T,0.5459915611814345,"2M + (Lη2η2
L
M
+ L2η3η3
Lτ 2"
T,0.5468354430379747,"M
) + (ηηL"
T,0.5476793248945148,"2M −(Lη2η2
L
m2
+ L2η3η3
Lτ 2"
T,0.5485232067510548,"m2
)m(m −1)"
T,0.549367088607595,"(M −1) )
 M
X"
T,0.550210970464135,"i=1
∥zi∥2"
T,0.5510548523206751,"=

(Lη2η2
L
M
+ L2η3η3
Lτ 2"
T,0.5518987341772152,"M
) −(Lη2η2
L
m2
+ L2η3η3
Lτ 2"
T,0.5527426160337553,"m2
)m(m −1)"
T,0.5535864978902953,"(M −1)  M
X"
T,0.5544303797468354,"i=1
∥zi∥2"
T,0.5552742616033756,"=

(Lη2η2
L + L2η3η3
Lτ 2)
M −m
mM(M −1)  M
X"
T,0.5561181434599156,"i=1
∥zi∥2."
T,0.5569620253164557,"Note also that:
∥zi∥2 = ∥∆i(xt−τt,i)∥2"
T,0.5578059071729958,"= ∥∆i(xt−τt,i) −∇fi(xt−τt,i) + ∇fi(xt−τt,i) −∇f(xt−τt,i) + ∇f(xt−τt,i)∥2"
T,0.5586497890295359,"≤3∥∆i(xt−τt,i) −∇fi(xt−τt,i)∥2 + 3∥∇fi(xt−τt,i) −∇f(xt−τt,i)∥2 + 3∥∇f(xt−τt,i)∥2"
T,0.5594936708860759,"≤3 ∥∇fi(xt−τt,i) −∆i(xt−τt,i)∥2
|
{z
}
A5"
T,0.560337552742616,"+3σ2
G + 3∥∇f(xt−τt,i)∥2."
T,0.5611814345991561,"Using the above results, we ﬁnally have:
Ef(xT ) −f(x0) ≤"
T,0.5620253164556962,"T −1
X t=0 
−1"
T,0.5628691983122363,"2ηηL∥∇f(xt)∥2 + (30ηK2L2η3
Lτ)∥∇f(xt)∥2
 +"
T,0.5637130801687764,"T −1
X t=0"
T,0.5645569620253165,"
−ηηL"
T,0.5654008438818565,"2M 2 ∥ M
X"
T,0.5662447257383966,"i=1
∆i(xt−τt,i)∥2 + Lη2η2
L
m2
E∥ M
X"
T,0.5670886075949367,"i=1
I{i ∈Mt}∆i(xt−τt,i)∥2"
T,0.5679324894514768,"+ L2η3η3
Lτ 2 m2
E∥ M
X"
T,0.5687763713080168,"i=1
I{i ∈Mt}∆i(xk−τk,i)∥2
"
T,0.569620253164557,"+ T

σ2
L(Lη2η2
L
m
+ τ 2L2η3η3
L
m
+ 5KηL2η3
L) + 30ηK2L2η3
Lσ2
G  ≤"
T,0.570464135021097,"T −1
X t=0 
−1"
T,0.5713080168776371,"2ηηL∥∇f(xt)∥2 + (30ηK2L2η3
Lτ)∥∇f(xt)∥2
 +"
T,0.5721518987341773,"T −1
X t=0"
T,0.5729957805907173,"
(Lη2η2
L + L2η3η3
Lτ 2) M −m"
T,0.5738396624472574,"m(M −1)

(15KL2η2
L(σ2
L + 6Kσ2
G)"
T,0.5746835443037974,"+ 90K2L2η2
L
1
M X"
T,0.5755274261603376,"i∈[M]
∥∇f(xt−τt,i)∥2 + 3σ2
G + 3 M M
X"
T,0.5763713080168776,"i=1
∥∇f(xt−τt,i)∥2)
"
T,0.5772151898734177,"+ T

σ2
L(Lη2η2
L
m
+ τ 2L2η3η3
L
m
+ 5KηL2η3
L) + 30ηK2L2η3
Lσ2
G  ≤"
T,0.5780590717299579,"T −1
X"
T,0.5789029535864979,"t=0
ηηL∥∇f(xt)∥2×"
T,0.579746835443038,"Under review as a conference paper at ICLR 2022 
−1"
T,0.580590717299578,"2 + 30L2K2η2
Lτ +

(LηηL + L2η2η2
Lτ 2) M −m"
T,0.5814345991561182,"m(M −1)

(90K2L2η2
Lτ + 3τ)
"
T,0.5822784810126582,"+ TηηLσ2
L×

(LηηL"
T,0.5831223628691983,"m
+ τ 2L2η2η2
L
m
+ 5KL2η2
L) + (LηηL + L2η2η2
Lτ 2) M −m"
T,0.5839662447257384,"m(M −1)(15KL2η2
L)
"
T,0.5848101265822785,"+

30K2L2η2
L + (LηηL + L2η2η2
Lτ 2) M −m"
T,0.5856540084388185,"m(M −1)(90K2L2η2
L + 3)

σ2
G "
T,0.5864978902953587,"(b13)
≤"
T,0.5873417721518988,"T −1
X"
T,0.5881856540084388,"t=0
−1"
T,0.5890295358649789,"4ηηL∥∇f(xt)∥2 + TηηL

αLσ2
L + αGσ2
G

,"
T,0.589873417721519,where (b13) follows from the fact that
T,0.5907172995780591,"1
4 ≤
1"
T,0.5915611814345991,"2 −30L2K2η2
Lτ −

(LηηL + L2η2η2
Lτ 2) M −m"
T,0.5924050632911393,"m(M −1)

(90K2L2η2
Lτ + 3τ)
"
T,0.5932489451476793,"if 120L2K2η2
Lτ + 4(LηηL + L2η2η2
Lτ 2)
M−m
m(M−1)

(90K2L2η2
Lτ + 3τ) < 1,"
T,0.5940928270042194,"αL =

(LηηL"
T,0.5949367088607594,"m
+ τ 2L2η2η2
L
m
+ 5KL2η2
L) + (LηηL + L2η2η2
Lτ 2) M −m"
T,0.5957805907172996,"m(M −1)(15KL2η2
L)

,"
T,0.5966244725738397,"and
αG =

30K2L2η2
L + (LηηL + L2η2η2
Lτ 2) M −m"
T,0.5974683544303797,"m(M −1)(90K2L2η2
L + 3)

."
T,0.5983122362869199,"Lastly, by rearranging and telescoping, we have"
T,0.5991561181434599,"1
T"
T,0.6,"T −1
X"
T,0.60084388185654,"t=0
E∥∇f(xt)∥2 ≤4(f0 −f∗)"
T,0.6016877637130802,"ηηLT
+ 4

αLσ2
L + αGσ2
G

."
T,0.6025316455696202,This completes the proof.
T,0.6033755274261603,"Corollary 2 (Linear Speedup to Stationary Point). By setting ηL =
1
√"
T,0.6042194092827005,"T and η = √m, the conver-
gence rate of AFA-CD with uniformly distributed worker information arrivals is:"
T,0.6050632911392405,"1
T"
T,0.6059071729957806,"T −1
X"
T,0.6067510548523207,"t=0
E∥∇f(xt)∥2
2 = O(
1
m1/2T 1/2 ) + O
τ 2 T"
T,0.6075949367088608,"
+ O
K2 T"
T,0.6084388185654008,"
+ O

K2"
T,0.6092827004219409,m1/2T 3/2
T,0.610126582278481,"
+ O
K2τ 2 T 2 
."
T,0.6109704641350211,"Proof. Let ηL =
1
√"
T,0.6118143459915611,"T , and η = √m. It then follows that:"
T,0.6126582278481013,"αL = O(
1
m1/2T 1/2 ) + O(τ 2"
T,0.6135021097046414,T ) + O(K
T,0.6143459915611814,"T ) + O(
K
m1/2T 3/2 ) + O(Kτ 2"
T,0.6151898734177215,"T 2 ),"
T,0.6160337552742616,"αG = O(
1
m1/2T 1/2 ) + O(τ 2"
T,0.6168776371308017,T ) + O(K2
T,0.6177215189873417,"T ) + O(
K2"
T,0.6185654008438819,m1/2T 3/2 ) + O(K2τ 2
T,0.619409282700422,"T 2 ),"
T,0.620253164556962,and the proof is complete.
T,0.6210970464135022,"C
PROOF OF THE PERFORMANCE RESULTS OF THE AFA-CS ALGORITHM"
T,0.6219409282700422,"Theorem 4. Under Assumptions 1- 3, choose sever- and worker-side learning rates η and ηL in such
a way that there exists a non-negative constant series {βµ}τ−1
u=0 satisfying the following relationship:"
T,0.6227848101265823,12LηηL + 540(M −m′)2
T,0.6236286919831223,"M 2
(1 + LηηL)K2L2η2
L(1 + τ) + 180K2L2η2
L + 320L3K2ηη3
L <1, (4) ηηL"
T,0.6244725738396625,9(M −m′)2
T,0.6253164556962025,"2M 2
(1 + LηηL)

3τL2 + (βu+1 −βu) ≤0,
(5)"
T,0.6261603375527426,Under review as a conference paper at ICLR 2022 ηηL
T,0.6270042194092827,9(M −m′)2
T,0.6278481012658228,"2M 2
(1 + LηηL)

3τL2 −βτ−1 ≤0,
(6)"
T,0.6286919831223629,"3
2M σ2
L ≤(1"
T,0.6295358649789029,"2 −β0ηηL)E∥Gt∥2
2,
(7)"
T,0.6303797468354431,"the output sequence {xt} generated by the AFA-CS algorithm for general worker information arrival
processes with bounded delay (τ := maxt∈[T ],i∈[M]{τt,i}) satisﬁes:"
T,0.6312236286919831,"1
T"
T,0.6320675105485232,"T −1
X"
T,0.6329113924050633,"t=0
E∥∇f(xt)∥2
2 ≤4(V (x0) −V (x∗))"
T,0.6337552742616034,"ηηLT
+ 4
 
αLσ2
L + αGσ2
G

,
(8)"
T,0.6345991561181434,where αL and αG are constants deﬁned as follows:
T,0.6354430379746835,αL ≜[3LηηL
M,0.6362869198312237,"2M
+ 5KL2η2
L(9(M −m′)2"
M,0.6371308016877637,"M 2
(1 + LηηL) + (3"
M,0.6379746835443038,"2 + 3LηηL))],"
M,0.6388185654008439,αG ≜(9(M −m′)2
M,0.639662447257384,"M 2
(1 + LηηL) + (3"
M,0.640506329113924,"2 + 3LηηL))(30K2L2η2
L),"
M,0.6413502109704642,"and V (·) is deﬁned as V (xt) ≜f(xt) + Pτ−1
u=0 βu∥xt−u −xt−u−1∥2, m′ is the number of updates
in the memory space with no time delay (τt,i = 0)."
M,0.6421940928270042,"Proof. We divide the stochastic gradient returns { Gi} into two groups, one is for those without delay
(Gi(xt), i ∈Mt, |Mt| = m′) and the other is for those with delay (Gi(xt−τt,i), i ∈Mc
t, |Mc
t| =
M −m′)."
M,0.6430379746835443,"Then, the update step can be written as follows:"
M,0.6438818565400843,xt+1 −xt = −ηηL M  X
M,0.6447257383966245,"i∈Mt
Gi(xt) +
X"
M,0.6455696202531646,"i∈Mc
t
Gi(xt−τt,i)

(10) = −η M  X"
M,0.6464135021097046,"i∈[M]
Gi(xt) +
X"
M,0.6472573839662448,"i∈Mc
t"
M,0.6481012658227848,"
Gi(xt−τt,i) −Gi(xt)

.
(11)"
M,0.6489451476793249,"Due to the L-smoothness assumption, taking expectation of f(xt+1) over the randomness in commu-
nication round t, we have:"
M,0.6497890295358649,"E[f(xt+1)] ≤f(xt) +

∇f(xt), E[xt+1 −xt]"
M,0.6506329113924051,"|
{z
}
A1 +L"
M,0.6514767932489451,"2 E[∥xt+1 −xt∥2
|
{z
}
A2"
M,0.6523206751054852,We ﬁrst bound A2 as follows:
M,0.6531645569620254,A2 = E[∥xt+1 −xt∥2]
M,0.6540084388185654,"= η2η2
L
M 2 E

X"
M,0.6548523206751055,"i∈Mt
Gi(xt) +
X"
M,0.6556962025316456,"i∈Mc
t
Gi(xt−τt,i) 2"
M,0.6565400843881857,"= η2η2
L
M 2 E

X"
M,0.6573839662447257,"i∈Mt
[Gi(xt) −∆i(xt)] +
X"
M,0.6582278481012658,"i∈Mc
t
[Gi(xt−τt,i) −∆i(xt−τt,i) + ∆i(xt−τt,i) −∆i(xt)] +
X"
M,0.6590717299578059,"i∈[M]
∆i(xt) 2"
M,0.659915611814346,"= 3η2η2
L
M 2"
M,0.660759493670886,"
E

X"
M,0.6616033755274262,"i∈Mt
[Gi(xt) −∆i(xt)] +
X"
M,0.6624472573839663,"i∈Mc
t
[Gi(xt−τt,i) −∆i(xt−τt,i)] 2"
M,0.6632911392405063,"+ E

X"
M,0.6641350210970464,"i∈Mc
t
[∆i(xt−τt,i) −∆i(xt)]"
M,0.6649789029535865,"2
+ E

X"
M,0.6658227848101266,"i∈[M]
∆i(xt) 2"
M,0.6666666666666666,"≤3η2η2
L
M
σ2
L + 3(M −m′)"
M,0.6675105485232068,"M 2
η2η2
L
X"
M,0.6683544303797468,"i∈Mc
t
∥∆i(xt) −∆i(xt−τt,i)∥2 + 3η2η2
L
M 2 ∥
X"
M,0.6691983122362869,"i∈[M]
∆i(xt)∥2"
M,0.6700421940928271,Under review as a conference paper at ICLR 2022
M,0.6708860759493671,"≤3η2η2
L
M
σ2
L + 3(M −m′)"
M,0.6717299578059072,"M 2
η2η2
L
X"
M,0.6725738396624472,"i∈Mc
t
∥∆i(xt) −∆i(xt−τt,i)∥2"
M,0.6734177215189874,"+ 6η2η2
L
M X"
M,0.6742616033755274,"i∈[M]
∥∆i(xt) −∇fi(xt)∥2 + 6η2η2
L∥∇f(xt)∥2."
M,0.6751054852320675,"To bound the term A1, we have:"
M,0.6759493670886076,"A1 =E

∇f(xt), xt+1 −xt"
M,0.6767932489451477,"=E

∇f(xt), −ηηL M  X"
M,0.6776371308016877,"i∈Mt
Gi(xt) +
X"
M,0.6784810126582278,"i∈Mc
t
Gi(xt−τt,i)
"
M,0.679324894514768,"= −ηηLE
1"
M,0.680168776371308,"2∥∇f(xt)∥2 +
1
2M 2 X"
M,0.6810126582278481,"i∈Mt
Gi(xt) +
X"
M,0.6818565400843882,"i∈Mc
t
Gi(xt−τt,i) 2 −1 2"
M,0.6827004219409283,"∇f(xt) −1 M [
X"
M,0.6835443037974683,"i∈Mt
Gi(xt) +
X"
M,0.6843881856540084,"i∈Mc
t
Gi(xt−τt,i)] 2"
M,0.6852320675105485,"(c1)
= −ηηL"
M,0.6860759493670886,2 ∥∇f(xt)∥2 −ηηL 2 E∥1
M,0.6869198312236287,"ηηL
(xt+1 −xt)∥2 + ηηL"
M,0.6877637130801688,"2M 2 E∥
X"
M,0.6886075949367089,"i∈Mt
[∇fi(xt) −Gi(xt)] +
X"
M,0.6894514767932489,"i∈Mc
t
[∇fi(xt) −Gi(xt−τt,i)]∥2"
M,0.6902953586497891,= −ηηL
M,0.6911392405063291,"2 ∥∇f(xt)∥2 −
1
2ηηL
E∥xt+1 −xt∥2 + ηηL"
M,0.6919831223628692,"2M 2 E

X"
M,0.6928270042194092,"i∈Mt
[∇fi(xt) −∆i(xt) + ∆i(xt) −Gi(xt)] +
X"
M,0.6936708860759494,"i∈Mc
t
[∇fi(xt) −∆i(xt) + ∆i(xt) −∆i(xt−τt,i) + ∆i(xt−τt,i) −Gi(xt−τt,i)] 2"
M,0.6945147679324895,= −ηηL
M,0.6953586497890295,"2 ∥∇f(xt)∥2 −
1
2ηηL
E∥xt+1 −xt∥2 + ηηL"
M,0.6962025316455697,"2M 2 E

X"
M,0.6970464135021097,"i∈[M]
[∇fi(xt) −∆i(xt)] +
X"
M,0.6978902953586498,"i∈Mt
[∆i(xt) −Gi(xt)] +
X"
M,0.6987341772151898,"i∈Mc
t
[∆i(xt) −∆i(xt−τt,i) + ∆i(xt−τt,i) −Gi(xt−τt,i)] 2 ≤−ηηL"
M,0.69957805907173,"2 ∥∇f(xt)∥2 −
1
2ηηL
E∥xt+1 −xt∥2 + 3ηηL"
M,0.70042194092827,"2M 2 E

X"
M,0.7012658227848101,"i∈Mc
t
[∆i(xt) −∆i(xt−τt,i)] 2"
M,0.7021097046413503,+ 3ηηL 2M 2 X
M,0.7029535864978903,"i∈[M]
∇fi(xt) −∆i(xt) 2"
M,0.7037974683544304,+ 3ηηL
M,0.7046413502109705,"2M 2 E

X"
M,0.7054852320675106,"i∈Mt
[∆i(xt) −Gi(xt)] +
X"
M,0.7063291139240506,"i∈Mc
t
∆i(xt−τt,i) −Gi(xt−τt,i) 2 ≤−ηηL"
M,0.7071729957805907,"2 ∥∇f(xt)∥2 −
1
2ηηL
E∥xt+1 −xt∥2 + 3(M −m′)ηηL"
M,0.7080168776371308,"2M 2
X"
M,0.7088607594936709,"i∈Mc
t
E
∆i(xt) −∆i(xt−τt,i) 2"
M,0.7097046413502109,+ 3ηηL
M,0.7105485232067511,2M X i∈[M]
M,0.7113924050632912,∇fi(xt) −∆i(xt)
M,0.7122362869198312,"2
+ 3ηηL"
M,0.7130801687763713,"2M σ2
L"
M,0.7139240506329114,where (c1) follows from the update step of the algorithm speciﬁed in Eq. 10.
M,0.7147679324894515,Under review as a conference paper at ICLR 2022
M,0.7156118143459915,"Combining A1 abd A2, we have:"
M,0.7164556962025317,"E[f(xt+1)] −f(xt) ≤

∇f(xt), E[xt+1 −xt]"
M,0.7172995780590717,"|
{z
}
A1 +L"
M,0.7181434599156118,"2 E[∥xt+1 −xt∥2
|
{z
}
A2"
M,0.7189873417721518,≤f(xt) −ηηL
M,0.719831223628692,"2 ∥∇f(xt)∥2 −
1
2ηηL
E∥xt+1 −xt∥2"
M,0.7206751054852321,"+
3(M −m′)ηηL"
M,0.7215189873417721,"2M 2
+ 3L(M −m′)η2η2
L
2M 2  X"
M,0.7223628691983123,"i∈Mc
t
E
∆i(xt) −∆i(xt−τt,i) 2"
M,0.7232067510548523,"|
{z
}
C1"
M,0.7240506329113924,"+
3ηηL"
M,0.7248945147679325,"2M + 3Lη2η2
L
M  X i∈[M]"
M,0.7257383966244726,∇fi(xt) −∆i(xt) 2
M,0.7265822784810126,"|
{z
}
C2"
M,0.7274261603375527,+ 3ηηL
M,0.7282700421940929,"2M σ2
L + 3Lη2η2
L
2M
σ2
L + 3Lη2η2
L∥∇f(xt)∥2
(12)"
M,0.7291139240506329,"For each worker i, we have:"
M,0.729957805907173,C2 = ∥∇fi(xt) −∆i(xt)∥2
M,0.7308016877637131,"= ∥∇fi(xt) −
1
Kt,i"
M,0.7316455696202532,"Kt,i−1
X"
M,0.7324894514767932,"j=0
∇fi(xj
t)∥2"
M,0.7333333333333333,"=
1
Kt,i"
M,0.7341772151898734,"Kt,i−1
X"
M,0.7350210970464135,"j=0
∥∇fi(xt) −∇fi(xj
t)∥2 ≤L2 Kt,i"
M,0.7358649789029535,"Kt,i−1
X"
M,0.7367088607594937,"j=0
∥xt −xj
t∥2"
M,0.7375527426160338,"(c2)
≤5KL2η2
L(σ2
L + 6Kσ2
G) + 30K2L2η2
L∥∇f(xt)∥2,"
M,0.7383966244725738,where (c2) follows from the same bound of A6 speciﬁed in Eq. (9).
M,0.739240506329114,"Also, note that:"
M,0.740084388185654,"C1 = ∥∆i(xt) −∆i(xt−τt,i)∥"
M,0.7409282700421941,"≤3∥∆i(xt) −∇fi(xt)∥2 + 3∥∇fi(xt) −∇fi(xt−τt,i)∥2 + 3∥∇fi(xt−τt,i) −∆i(xt−τt,i)∥2"
M,0.7417721518987341,"≤3∥∆i(xt) −∇fi(xt)∥2 + 3∥∇fi(xt−τt,i) −∆i(xt−τt,i)∥2 + 3L2∥xt −xt−τt,i∥2"
M,0.7426160337552743,"≤3∥∆i(xt) −∇fi(xt)∥2 + 3∥∇fi(xt−τt,i) −∆i(xt−τt,i)∥2 + 3L2∥"
M,0.7434599156118143,"τt,i−1
X"
M,0.7443037974683544,"u=0
xt−u −xt−u−1∥2"
M,0.7451476793248946,"≤3∥∆i(xt) −∇fi(xt)∥2 + 3∥∇fi(xt−τt,i) −∆i(xt−τt,i)∥2 + 3τL2
τ−1
X"
M,0.7459915611814346,"u=0
∥xt−u −xt−u−1∥2,"
M,0.7468354430379747,"where τ is the maximum delay, i.e., τ = max{τt,i}, ∀t ∈[T], i ∈[M]."
M,0.7476793248945147,"Plugging C1 and C2 into the inequality in Eq. (12), we have:"
M,0.7485232067510549,"E[f(xt+1)] −f(xt) ≤

∇f(xt), E[xt+1 −xt]"
M,0.7493670886075949,"|
{z
}
A1 +L"
M,0.750210970464135,"2 E[∥xt+1 −xt∥2
|
{z
}
A2"
M,0.7510548523206751,≤f(xt) −ηηL
M,0.7518987341772152,"2 ∥∇f(xt)∥2 −
1
2ηηL
E∥xt+1 −xt∥2"
M,0.7527426160337553,"+
9(M −m′)ηηL"
M,0.7535864978902953,"2M 2
+ 9L(M −m′)η2η2
L
2M 2  X"
M,0.7544303797468355,"i∈Mc
t
E
∇fi(xt) −∆i(xt) 2"
M,0.7552742616033755,Under review as a conference paper at ICLR 2022
M,0.7561181434599156,"+
9(M −m′)ηηL"
M,0.7569620253164557,"2M 2
+ 9L(M −m′)η2η2
L
2M 2  X"
M,0.7578059071729958,"i∈Mc
t
E
∇fi(xt−τt,i) −∆i(xt−τt,i) 2"
M,0.7586497890295358,"+
9(M −m′)2ηηL"
M,0.759493670886076,"2M 2
+ 9L(M −m′)2η2η2
L
2M 2"
M,0.760337552742616,"
3τL2
τ−1
X"
M,0.7611814345991561,"u=0
E
xt−u −xt−u−1  2"
M,0.7620253164556962,"+
3ηηL"
M,0.7628691983122363,"2M + 3Lη2η2
L
M  X i∈[M]"
M,0.7637130801687764,∇fi(xt) −∆i(xt) 2
M,0.7645569620253164,+ 3ηηL
M,0.7654008438818566,"2M σ2
L + 3Lη2η2
L
2M
σ2
L + 3Lη2η2
L∥∇f(xt)∥2"
M,0.7662447257383966,≤f(xt) −ηηL
M,0.7670886075949367,"2 ∥∇f(xt)∥2 −
1
2ηηL
E∥xt+1 −xt∥2 + ηηL"
M,0.7679324894514767,9(M −m′)2
M,0.7687763713080169,"2M 2
(1 + LηηL) + (3"
M,0.769620253164557,"2 + 3LηηL)

(5KL2η2
L(σ2
L + 6Kσ2
G) + 30K2L2η2
L∥∇f(xt)∥2) + ηηL"
M,0.770464135021097,9(M −m′)2
M,0.7713080168776372,"2M 2
(1 + LηηL)

(5KL2η2
L(σ2
L + 6Kσ2
G) + 30K2L2η2
L
1
M −m X"
M,0.7721518987341772,"i∈Mc
t
∥∇f(xt−τt,i)∥2) + ηηL"
M,0.7729957805907173,9(M −m′)2
M,0.7738396624472574,"2M 2
(1 + LηηL)

3τL2
τ−1
X"
M,0.7746835443037975,"u=0
E
xt−u −xt−u−1  2"
M,0.7755274261603375,+ 3ηηL
M,0.7763713080168776,"2M σ2
L + 3Lη2η2
L
2M
σ2
L + 3Lη2η2
L∥∇f(xt)∥2"
M,0.7772151898734178,=f(xt) −ηηL 1
M,0.7780590717299578,"2 −
9(M −m′)2"
M,0.7789029535864979,"2M 2
(1 + LηηL) + (3"
M,0.779746835443038,"2 + 3LηηL)

30K2L2η2
L −3LηηL"
M,0.7805907172995781,"
∥∇f(xt)∥2"
M,0.7814345991561181,"−
1
2ηηL
E∥xt+1 −xt∥2 + ηηL"
M,0.7822784810126582,9(M −m′)2
M,0.7831223628691983,"M 2
(1 + LηηL) + (3"
M,0.7839662447257384,"2 + 3LηηL)

[5KL2η2
L(σ2
L + 6Kσ2
G)] + ηηL"
M,0.7848101265822784,9(M −m′)2
M,0.7856540084388186,"2M 2
(1 + LηηL)

(30K2L2η2
L
1
M −m X"
M,0.7864978902953587,"i∈Mc
t
∥∇f(xt−τt,i)∥2) + ηηL"
M,0.7873417721518987,9(M −m′)2
M,0.7881856540084389,"2M 2
(1 + LηηL)

3τL2
τ−1
X"
M,0.7890295358649789,"u=0
E
xt−u −xt−u−1 "
M,0.789873417721519,"2
+ 3ηηL"
M,0.790717299578059,"2M σ2
L + 3Lη2η2
L
2M
σ2
L."
M,0.7915611814345992,"Now, deﬁne V (xt) = f(xt) + Pτ−1
u=0 βu∥xt−u −xt−u−1∥2.
Based on the above bound of
E[f(xt+1)] −f(xt), it then follows that:"
M,0.7924050632911392,"EV (xt+1) −V (xt)
=Ef(xt+1) −f(xt) + τ−2
X"
M,0.7932489451476793,"u=0
(βu+1 −βu)∥xt−u −xt−u−1∥2 + β0∥xt+1 −xt∥2 −βτ−1∥xt−τ−1 −xt−τ−2∥2 ≤−ηηL 1"
M,0.7940928270042195,"2 −
9(M −m′)2"
M,0.7949367088607595,"2M 2
(1 + LηηL) + (3"
M,0.7957805907172996,"2 + 3LηηL)

30K2L2η2
L −3LηηL"
M,0.7966244725738396,"
∥∇f(xt)∥2 + ηηL"
M,0.7974683544303798,9(M −m′)2
M,0.7983122362869198,"M 2
(1 + LηηL) + (3"
M,0.7991561181434599,"2 + 3LηηL)

[5KL2η2
L(σ2
L + 6Kσ2
G)] + ηηL"
M,0.8,9(M −m′)2
M,0.8008438818565401,"2M 2
(1 + LηηL)

(30K2L2η2
L
1
M −m X"
M,0.8016877637130801,"i∈Mc
t
∥∇f(xt−τt,i)∥2)"
M,0.8025316455696202,+ [ηηL
M,0.8033755274261604,9(M −m′)2
M,0.8042194092827004,"2M 2
(1 + LηηL)

3τL2 + (βu+1 −βu)] τ−2
X"
M,0.8050632911392405,"u=0
E
xt−u −xt−u−1  2"
M,0.8059071729957806,Under review as a conference paper at ICLR 2022
M,0.8067510548523207,+ [ηηL
M,0.8075949367088607,9(M −m′)2
M,0.8084388185654009,"2M 2
(1 + LηηL)

3τL2 −βτ−1]∥xt−τ−1 −xt−τ−2∥2"
M,0.809282700421941,"+ (β0 −
1
2ηηL
)∥xt+1 −xt∥2 + 3ηηL"
M,0.810126582278481,"2M σ2
L + 3Lη2η2
L
2M
σ2
L."
M,0.810970464135021,Telescoping the above inequality from t = 0 to T −1 of the above inequality yields:
M,0.8118143459915612,EV (xT ) −V (x0) ≤−ηηL
M,0.8126582278481013,"T −1
X t=0 1"
M,0.8135021097046413,"2 −
9(M −m′)2"
M,0.8143459915611815,"2M 2
(1 + LηηL) + (3"
M,0.8151898734177215,"2 + 3LηηL)

30K2L2η2
L −3LηηL"
M,0.8160337552742616,"
∥∇f(xt)∥2 + ηηL"
M,0.8168776371308016,9(M −m′)2
M,0.8177215189873418,"2M 2
(1 + LηηL)

(30K2L2η2
L"
M,0.8185654008438819,"T −1
X t=0"
M,0.8194092827004219,"1
M −m X"
M,0.8202531645569621,"i∈Mc
t
∥∇f(xt−τt,i)∥2)"
M,0.8210970464135021,"+ ηηLT
9(M −m′)2"
M,0.8219409282700422,"M 2
(1 + LηηL) + (3"
M,0.8227848101265823,"2 + 3LηηL)

[5KL2η2
L(σ2
L + 6Kσ2
G)] +"
M,0.8236286919831224,"T −1
X"
M,0.8244725738396624,"t=0
[ηηL"
M,0.8253164556962025,9(M −m′)2
M,0.8261603375527427,"2M 2
(1 + LηηL)

3τL2 + (βu+1 −βu)]
|
{z
}
C3 τ−2
X"
M,0.8270042194092827,"u=0
E
xt−u −xt−u−1  2 +"
M,0.8278481012658228,"T −1
X"
M,0.8286919831223629,"t=0
[ηηL"
M,0.829535864978903,9(M −m′)2
M,0.830379746835443,"2M 2
(1 + LηηL)

3τL2 −βτ−1]
|
{z
}
C4"
M,0.8312236286919831,∥xt−τ−1 −xt−τ−2∥2
M,0.8320675105485232,"+ (β0 −
1
2ηηL
)"
M,0.8329113924050633,"T −1
X"
M,0.8337552742616033,"t=0
∥xt+1 −xt∥2 + 3ηηLT"
M,0.8345991561181435,"2M
σ2
L
|
{z
}
C5"
M,0.8354430379746836,"+3LTη2η2
L
2M
σ2
L"
M,0.8362869198312236,"(c3)
≤−1 4ηηL"
M,0.8371308016877637,"T −1
X"
M,0.8379746835443038,"t=0
∥∇f(xt)∥2 + ηηL"
M,0.8388185654008439,3LTηηL
M,0.8396624472573839,"2M
+ 5TKL2η2
L"
M,0.8405063291139241,9(M −m′)2
M,0.8413502109704641,"M 2
(1 + LηηL) + (3"
M,0.8421940928270042,"2 + 3LηηL)

σ2
L"
M,0.8430379746835444,"+ ηηLT
9(M −m′)2"
M,0.8438818565400844,"M 2
(1 + LηηL) + (3"
M,0.8447257383966245,"2 + 3LηηL)

(30K2L2η2
L)σ2
G,"
M,0.8455696202531645,where (c3) holds if the following conditions are satisﬁed:
M,0.8464135021097047,"1
4 ≤
1"
M,0.8472573839662447,"2 −
9(M −m′)2"
M,0.8481012658227848,"2M 2
(1 + LηηL) + (3"
M,0.8489451476793249,"2 + 3LηηL)

30K2L2η2
L −3LηηL "
M,0.849789029535865,"−
9(M −m′)2"
M,0.850632911392405,"2M 2
(1 + LηηL)

(30τK2L2η2
L),"
M,0.8514767932489451,C3 = [ηηL
M,0.8523206751054853,9(M −m′)2
M,0.8531645569620253,"2M 2
(1 + LηηL)

3τL2 + (βu+1 −βu)] ≤0,"
M,0.8540084388185654,C4 = [ηηL
M,0.8548523206751055,9(M −m′)2
M,0.8556962025316456,"2M 2
(1 + LηηL)

3τL2 −βτ−1] ≤0,"
M,0.8565400843881856,"C5 ≤0 ←
3
2M σ2
L ≤(1"
M,0.8573839662447258,2 −β0ηηL)E∥Gt∥2.
M,0.8582278481012658,"By rearranging the inequality, we have:"
M,0.8590717299578059,"1
4ηηL"
M,0.859915611814346,"T −1
X"
M,0.8607594936708861,"t=0
∥∇f(xt)∥2"
M,0.8616033755274262,Under review as a conference paper at ICLR 2022
M,0.8624472573839662,≤V (x0) −EV (xT )
M,0.8632911392405064,"+ ηηLT
3LηηL"
M,0.8641350210970464,"2M
+ 5KL2η2
L"
M,0.8649789029535865,9(M −m′)2
M,0.8658227848101265,"M 2
(1 + LηηL) + (3"
M,0.8666666666666667,"2 + 3LηηL)

σ2
L"
M,0.8675105485232067,"+ ηηLT
9(M −m′)2"
M,0.8683544303797468,"M 2
(1 + LηηL) + (3"
M,0.869198312236287,"2 + 3LηηL)

(30K2L2η2
L)σ2
G,"
M,0.870042194092827,"That is,"
T,0.8708860759493671,"1
T"
T,0.8717299578059071,"T −1
X"
T,0.8725738396624473,"t=0
E∥∇f(xt)∥2 ≤4(V (x0) −V (x∗))"
T,0.8734177215189873,"ηηLT
+ 4[αLσ2
L + αGσ2
G],"
T,0.8742616033755274,"where αL =

3LηηL"
M,0.8751054852320675,"2M
+ 5KL2η2
L"
M,0.8759493670886076,"
9(M−m′)2"
M,0.8767932489451477,"M 2
(1 + LηηL) + ( 3"
M,0.8776371308016878,"2 + 3LηηL)

, αG =

9(M−m′)2"
M,0.8784810126582279,"M 2
(1 +"
M,0.8793248945147679,LηηL) + ( 3
M,0.880168776371308,"2 + 3LηηL)

(30K2L2η2
L). This completes the proof."
M,0.8810126582278481,"Corollary 3 (Linear Speedup). By setting ηL =
1
√"
M,0.8818565400843882,"T , and η =
√"
M,0.8827004219409282,"M, the convergence rate of the
AFA-CS algorithm for general worker information arrival processes with bounded delay is:"
T,0.8835443037974684,"1
T"
T,0.8843881856540085,"T −1
X"
T,0.8852320675105485,"t=0
E∥∇f(xt)∥2
2 = O

1
M 1/2T 1/2"
T,0.8860759493670886,"
+ O
K2 T"
T,0.8869198312236287,"
+ O
K2M 1/2 T 3/2 
."
T,0.8877637130801688,"Proof. Let ηL =
1
√"
T,0.8886075949367088,"T , and η =
√"
T,0.889451476793249,M. It then follows that:
T,0.890295358649789,"αL = O(
1
M 1/2T 1/2 ) + O(K"
T,0.8911392405063291,T ) + O(KM 1/2
T,0.8919831223628693,"T 3/2
)."
T,0.8928270042194093,αG = O(K2
T,0.8936708860759494,T ) + O(K2M 1/2
T,0.8945147679324894,"T 3/2
)."
T,0.8953586497890296,This completes the proof.
T,0.8962025316455696,"D
DISCUSSION"
T,0.8970464135021097,"Convergence Error: The case with uniformly distributed worker information arrivals under AFL can
be viewed as a uniformly independent sampling process from total workers [M] under conventional
FL. Also, the case with general worker information arrival processes under AFL can be equivalently
mapped to an arbitrarily independent sampling under conventional FL. In each communication round,
the surrogate objection function for partial worker participation in FL is ˜f(x) :=
1
|Mt|
P"
T,0.8978902953586498,"i∈Mt fi(x).
For uniformly independent sampling, the surrogate object function approximately equals to f(x) :=
1
M
PM
i=1 fi(x) in expectation, i.e., E[ ˜f(x)] = f(x). However, the surrogate object function ˜f(x)
may deviate from f(x) with arbitrarily independent sampling. More speciﬁcally, for uniformly
independent sampling, the bound of ∥∇f(xt) −˜f(xt)∥2 is independent of σG (A3 term in B.3). On
the other hand, for arbitrarily independent sampling, ∥∇f(xt)−˜f(xt)∥2 ≤O(σ2
G) (A3 term in B.2).
This deviation may happen in every communication round, so it is non-vanishing even with inﬁnity
communication rounds. As a result, such deviation is originated from the arbitrary sampling coupling
with non-i.i.d. datasets. In other words, it is irrelevant to the optimization hyper-parameters such as
the learning rate, local steps and others, which is different from the objective inconsistency due to
different local steps shown in Wang et al. (2020). When we set τ = 0 and Kt,i = K, ∀t, i, AFA-CD
generalizes FedAvg. In such sense, the convergence error also exists in currently synchronous FL
algorithms with such arbitrarily independent sampling and non-i.i.d. dataset. Moreover, this sampling
process coupling with non-i.i.d. dataset not only results in convergence issue but also potentially
induces a new source of bias/unfairness (Mohri et al., 2019; Li et al., 2019b). So how to model the
practical worker participation process in practice and in turn tackle these potential bias are worth
further exploration."
T,0.8987341772151899,Under review as a conference paper at ICLR 2022
T,0.8995780590717299,"Variance Reduction: If we view the derivation between local loss function and global loss function
as global variance, i.e., ∥∇fi(xt) −∇f(xt)∥2 ≤σ2
G, ∀i ∈[m], ∀t as shown in Assumption 3, the
AFA-CS algorithm is indeed a variance reduction (VR) method, akin to SAG (Le Roux et al., 2012;
Schmidt et al., 2017). SAG maintains an estimate stochastic gradient vi, i ∈[n] for each data point
(n is the size of the dataset). In each iteration, SAG only samples one data point (say, j) and update
the stochastic gradient on latest model (vj = ∇fj(xt)) stored in the memory space, but then use
the average of all stored stochastic gradients as the estimate of a full gradient to update the model
(xt+1 = xt −ηtgt, gt = 1"
T,0.90042194092827,"n
Pn
i=1 vi). In such way, SAG is able to have a faster convergence rate
by reducing the local variance due to the stochastic gradient. AFA-CS algorithm performs in the
similar way. The server in the AFA-CS algorithm maintains a parameter for each worker as an
estimate of the returned stochastic gradient. In each communication round, the server only receives
m updates in the memory space but updates the global model by the average of all the M parameters.
As a result, not only can it diminish the convergence error derived from the non-i.i.d. dataset and
general worker information arrival processes (arbitrarily independent sampling), but also accelerate
the convergence rate with a linear speedup factor M. Previous works have applied VR methods in
FL, notably SCAFFOLD (Karimireddy et al., 2020b) and FedSVRG (Konecn`y et al., 2016). The key
difference is that we apply the VR on the server side to control the global variance while previous
works focus on the worker side in order to tackle the model drift due to local update steps. Applying
VR methods on server and worker side are orthogonal, and thus can be used simultaneously. We
believe other variance reduction methods could be similarly extended on the server side in a similar
fashion as what we do in AFA-CD. This will be left for future research."
T,0.9012658227848102,"E
EXPERIMENTS"
T,0.9021097046413502,"In this section, we provide the detailed experiment settings as well as extra experimental results that
cannot ﬁt in the page limit of the main paper."
T,0.9029535864978903,"E.1
MODEL AND DATASETS"
T,0.9037974683544304,"We run three models on three different datasets, including i) multinomial logistic regression (LR)
on manually partitioned non-i.i.d. MNIST, ii) convolutional neural network (CNN) for manually
partitioned non-i.i.d. CIFAR-10, and iii) recurrent neural network (RNN) on natural non-i.i.d.
Shakespeare datasets. These dataset are curated from previous FL papers (McMahan et al., 2016;
Li et al., 2018) and are now widely used as benchmarks in FL studies (Li et al., 2019c; Yang et al.,
2021)."
T,0.9046413502109705,"For MNIST and CIFAR-10, each dataset has ten classes of images. To impose statistical heterogeneity,
we split the data based on the classes (p) of images each worker contains. We distribute the data to
M = 10(or 100) workers such that each worker contains only certain classes with the same number
of training/test samples. Speciﬁcally, each worker randomly chooses p classes of labels and evenly
samples training/testing data points only with these p classes labels from the overall dataset without
replacement. For example, for p = 2, each worker only has training/testing samples with two classes,
which causes heterogeneity among different workers. For p = 10, each worker has samples with ten
classes, which is nearly i.i.d. case. In this way, we can use the classes (p) in worker’s local dataset to
represent the non-i.i.d. degree qualitatively."
T,0.9054852320675105,"The Shakespeare dataset is built from The Complete Works of William Shakespeare (McMahan et al.,
2016). We use a two-layer LSTM classiﬁer containing 100 hidden units with an embedding layer.
The learning task is the next-character prediction, and there are 80 classes of characters in total.
The model takes as input a sequence of 80 characters, embeds each of the characters into a learned
8-dimensional space and outputs one character per training sample after two LSTM layers and a
densely-connected layer. The dataset and model are taken from LEAF (Li et al., 2018)."
T,0.9063291139240506,"For MNIST and CIFAR-10, we use global learning rate η = 1.0 and local learning rate ηL = 0.1. For
MNIST, the batch size is 64 and the total communication round is 150. For CIFAR-10, the batch size
is 500 and the total communication round is 10000. For the Shakespeare dataset, the global learning
rate is η = 50, the local learning rate is ηL = 0.8, batch size is b = 10, and the total communication
round is 300. In the following tables and ﬁgure captions, we use “m/M” to denote that, in each
communication round, we randomly choose m workers from [M] to participate in the training."
T,0.9071729957805907,Under review as a conference paper at ICLR 2022
T,0.9080168776371308,"We study the asynchrony and heterogeneity factors in AFL, including asynchrony, heterogeneous
computing, worker’s arrival process, and data heterogeneity. To simulate the asynchrony, each partici-
pated worker choose one global model from the last recent ﬁve models instead of only using the latest
global model for synchronous case. To mimic the heterogeneous computing, we simulate two cases:
constant and dynamic local steps. For constant local steps, each participated worker performs a ﬁxed c
local update steps. In contrast, each worker takes a random local update steps uniformly sampled from
[1, 2 × c] for dynamic local steps. To emulate the effect of various worker’s arrival processes, we use
uniform sampling without replacement to simulate the uniformly distributed worker information ar-
rivals, and we use biased sampling with probability [0.19, 0.19, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01, 0.01]
without replacement for total 10 workers to investigate potential biases with general worker informa-
tion arrival processes. To study the data heterogeneity, we use the value p as a proxy to represent the
non-i.i.d. degree for MNIST and CIFAR-10."
T,0.9088607594936708,Table 1: CNN Architecture for CIFAR-10.
T,0.909704641350211,"Layer Type
Size
Convolution + ReLu
5 × 5 × 32
Max Pooling
2 × 2
Convolution + ReLu
5 × 5 × 64
Max Pooling
2 × 2
Fully Connected + ReLU
1024 × 512
Fully Connected + ReLU
512 × 128
Fully Connected
128 × 10"
T,0.9105485232067511,"E.2
FURTHER EXPERIMENTAL RESULTS"
T,0.9113924050632911,Table 2: Test Accuracy for comparison of asynchrony and local steps.
T,0.9122362869198313,"Models/
Dataset"
T,0.9130801687763713,"Non-i.i.d.
index (p)"
T,0.9139240506329114,"Worker
number"
T,0.9147679324894514,"Local
steps"
T,0.9156118143459916,"Synchrony
Asynchrony
Constant
steps"
T,0.9164556962025316,"Dynamic
steps"
T,0.9172995780590717,"Constant
Steps"
T,0.9181434599156119,"Dynamic
Steps"
T,0.9189873417721519,"LR/
MNIST"
T,0.919831223628692,"p = 1
5/10
5
0.8916
0.8915
0.8888
0.8868
p = 2
5/10
5
0.8906
0.8981
0.8901
0.8931
p = 5
5/10
5
0.9072
0.9075
0.9059
0.9048
p = 10
5/10
5
0.9114
0.9111
0.9129
0.9143
p = 1
5/10
10
0.8743
0.8786
0.8701
0.8734
p = 2
5/10
10
0.8687
0.8813
0.8661
0.8819
p = 5
5/10
10
0.9016
0.9050
0.9034
0.9065
p = 10
5/10
10
0.9124
0.9135
0.9112
0.9111
p = 1
20/100
5
0.8898
0.8973
0.8909
0.8938
p = 2
20/100
5
0.8968
0.9007
0.8955
0.9000
p = 5
20/100
5
0.9088
0.9088
0.9097
0.9078
p = 10
20/100
5
0.9111
0.9106
0.9126
0.9125"
T,0.920675105485232,"CNN/
CIFAR-10"
T,0.9215189873417722,"p = 1
5/10
5
0.7474
0.7606
0.7319
0.7350
p = 2
5/10
5
0.7677
0.7944
0.7662
0.777
p = 5
5/10
5
0.7981
0.802
0.8065
0.799
p = 10
5/10
5
0.8081
0.8072
0.8065
0.8119
RNN/
Shakespeare
-
72/143
50
0.4683
0.4831
0.4606
0.4687"
T,0.9223628691983122,"Effect of asynchrony, local update steps, and non-i.i.d. level. In table 2, we examine three factors
by comparing the top-1 test accuracy: synchrony versus asynchrony, constant steps versus dynamic
steps and different levels of non-i.i.d. dataset. The worker sampling process is uniformly random
sampling to simulate the uniformly distributed worker information arrivals. The baseline is synchrony
with constant steps. When using asynchrony or/and dynamic local steps, the top-1 test accuracy shows"
T,0.9232067510548523,Under review as a conference paper at ICLR 2022
T,0.9240506329113924,"no obvious differences. This observation can be observed in all these three tasks. Asynchrony and
dynamic local update steps enable each worker to participate ﬂexibly and loosen the coupling between
workers and the server. As a result, asynchrony and dynamic local steps introduce extra heterogeneity
factors, but the performance of the model is as good as that of the synchronous approaches with
constant local steps. Instead, the data heterogeneity is an important factor for the model performance.
As the non-i.i.d. level increases (smaller p value), the top-1 test accuracy decreases."
T,0.9248945147679325,"Next, we study convergence speed of the test accuracy for the model training under different settings.
Figure 2 illustrates the test accuracy for LR on MNIST with different non-i.i.d. levels. We can see that
asynchrony and dynamic local steps result in zigzagging convergence curves, but the ﬁnal accuracy
results have negligible differences. The zigzagging phenomenon is more dramatic as the non-i.i.d.
level gets higher. Interestingly, from Figure 3 and Figure 4, we can see that for less non-i.i.d. settings
such as p = 10 and p = 5, the curves of all algorithms are almost identical. Speciﬁcally, in Figure 4,
the test accuracy curves of the LSTM model oscillates under asynchrony and dynamic local steps.
Another observation is that it takes more rounds to converge as the non-i.i.d. level of the datasets
increases. This trend can be clearly observed in Figure 3."
T,0.9257383966244725,"0
20
40
60
80
100 120 140
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
T,0.9265822784810127,Test Accuracy
T,0.9274261603375528,"Synchrony + Constant
Asynchrony + Constant
Synchrony + Dynamic
Asynchrony + Dynamic"
T,0.9282700421940928,(a) p = 1.
T,0.9291139240506329,"0
20
40
60
80
100 120 140
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
T,0.929957805907173,Test Accuracy
T,0.9308016877637131,"Synchrony + Constant
Asynchrony + Constant
Synchrony + Dynamic
Asynchrony + Dynamic"
T,0.9316455696202531,(b) p = 2.
T,0.9324894514767933,"0
20
40
60
80
100 120 140
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
T,0.9333333333333333,Test Accuracy
T,0.9341772151898734,"Synchrony + Constant
Asynchrony + Constant
Synchrony + Dynamic
Asynchrony + Dynamic"
T,0.9350210970464135,(c) p = 5.
T,0.9358649789029536,"0
20
40
60
80
100 120 140
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
T,0.9367088607594937,Test Accuracy
T,0.9375527426160337,"Synchrony + Constant
Asynchrony + Constant
Synchrony + Dynamic
Asynchrony + Dynamic"
T,0.9383966244725739,(d) p = 10.
T,0.9392405063291139,"Figure 2: Test accuracy for LR on MNIST with worker number 5/10, local steps 5."
T,0.940084388185654,Under review as a conference paper at ICLR 2022
T,0.9409282700421941,"0
2000
4000
6000
8000 10000
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8"
T,0.9417721518987342,Test Accuracy
T,0.9426160337552743,"Synchrony + Constant
Asynchrony + Constant
Synchrony + Dynamic
Asynchrony + Dynamic"
T,0.9434599156118143,(a) p = 1.
T,0.9443037974683545,"0
2000
4000
6000
8000 10000
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8"
T,0.9451476793248945,Test Accuracy
T,0.9459915611814346,"Synchrony + Constant
Asynchrony + Constant
Synchrony + Dynamic
Asynchrony + Dynamic"
T,0.9468354430379747,(b) p = 2.
T,0.9476793248945148,"0
2000
4000
6000
8000 10000
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8"
T,0.9485232067510548,Test Accuracy
T,0.9493670886075949,"Synchrony + Constant
Asynchrony + Constant
Synchrony + Dynamic
Asynchrony + Dynamic"
T,0.950210970464135,(c) p = 5.
T,0.9510548523206751,"0
2000
4000
6000
8000 10000
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8"
T,0.9518987341772152,Test Accuracy
T,0.9527426160337553,"Synchrony + Constant
Asynchrony + Constant
Synchrony + Dynamic
Asynchrony + Dynamic"
T,0.9535864978902954,(d) p = 10.
T,0.9544303797468354,"Figure 3: Test accuracy for CNN on CIFAR-10 with worker number 5/10, local steps 5."
T,0.9552742616033755,"0
50
100
150
200
250
300
Communication Round 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55"
T,0.9561181434599156,Test Accuracy
T,0.9569620253164557,"Synchrony + Constant
Asynchrony + Constant
Synchrony + Dynamic
Asynchrony + Dynamic"
T,0.9578059071729957,"Figure 4: Test accuracy for LSTM on Shakespeare with worker number 72/143, local steps 50."
T,0.9586497890295359,Under review as a conference paper at ICLR 2022
T,0.959493670886076,Table 3: Test Accuracy of FedProx and SCAFFOLD.
T,0.960337552742616,"Models/
Dataset"
T,0.9611814345991562,"Non-i.i.d.
index (p)"
T,0.9620253164556962,"Worker
number"
T,0.9628691983122363,"Local
steps
FedProx SCAFFOLD
AFL +
FedProx"
T,0.9637130801687763,"AFL +
SCAFFOLD"
T,0.9645569620253165,"LR/
MNIST"
T,0.9654008438818565,"p = 1
5/10
5
0.8893
0.8928
0.8775
0.8946
p = 2
5/10
5
0.8868
0.8970
0.8832
0.8954
p = 5
5/10
5
0.9036
0.9032
0.9004
0.9019
p = 10
5/10
5
0.9075
0.9057
0.9054
0.9022
p = 1
5/10
10
0.8752
0.8789
0.8669
0.8838
p = 2
5/10
10
0.8685
0.8967
0.8789
0.8978
p = 5
5/10
10
0.9019
0.9047
0.8998
0.9029
p = 10
5/10
10
0.9072
0.9071
0.9052
0.9038"
T,0.9662447257383966,"CNN/
CIFAR-10"
T,0.9670886075949368,"p = 1
5/10
5
0.7488
0.1641
0.7415
0.3935
p = 2
5/10
5
0.7728
0.6315
0.7890
0.6971
p = 5
5/10
5
0.7931
0.7828
0.8031
0.7884
p = 10
5/10
5
0.8150
0.8083
0.8143
0.8051
RNN/
Shakespeare
-
72/143
50
0.4690
0.4794
0.4550
0.4515"
T,0.9679324894514768,"Utilizing FedProx and SCAFFOLD as the optimizer on the worker-side. Here, we choose
FedProx and SCAFFOLD as two classes of algorithms in existing FL algorithms. FedProx represents
these algorithms that modiﬁes the local objective function. Other algorithms belonging to this
category includes FedPD (Zhang et al., 2020b) and FedDyn (Acar et al., 2021). In such algorithms,
no extra information exchange between worker and server is needed. On the other hand, SCAFFOLD
represents VR-based (variance reduction) algorithms. It needs an extra control variate to perform
the “variance reduction” step, so extra parameters are required in each communication round. Other
algorithms in this class includes FedSVRG (Konecn`y et al., 2016)."
T,0.9687763713080169,"In Table 3, we show the effectiveness of utilizing existing FL algorithms, FedProx and SCAFFOLD,
in the AFL framework. For FedProx and SCAFFOLD, we examine synchrony and constant local
steps settings. When incorporating these two advanced FL algorithms in the AFL framework, we
study the effects of asynchrony and dynamic local steps. We set µ = 0.1 as default in FedProx
algorithm. We can see from Table 3 that FedProx performs as good as FedAvg does (compare with
the results in Table 2). Also, there is no performance degradation in AFL framework by utilizing
FedProx as the worker’s optimizer. However, while SCAFFOLD performs well for LR on MNIST,
it dose not work well for CNN on CIFAR-10, especially in cases with higher non-i.i.d. levels. One
possible reason is that the control variates can become stale in partial worker participation and in turn
degrade the performance. Previous work also showed similar results (Acar et al., 2021; Reddi et al.,
2020). If we view the SCAFFOLD ( in synchrony and constant steps setting) as the baseline, no
obvious performance degradation happens under AFL with SCAFFOLD being used as the worker’s
optimizer."
T,0.9696202531645569,"Effects of different worker information arrival processes. In order to generate different workers’
arrival processes, we use uniform sampling without replacement to simulate the uniformly distributed
worker information arrivals and use biased sampling to simulate the potential bias in general worker
information arrival processes. In Figures 5 and 6, we illustrate the effect of the sampling process
for LR on MNIST and CNN on CIFAR-10 with asynchrony and dynamic local steps. For highly
non-i.i.d. datasets (p = 1), the biased sampling process degrades the model performance. This
is consistent with the larger convergence error as shown in our theoretical analysis. On the other
hand, for other non-i.i.d. cases with p = 2, 5, 10, such biased sampling dose not lead to signiﬁcant
performance degradation. When applying variance reduction on such biased sampling process by
reusing old gradients as shown in AFA-CS, we can see that AFA-CS performs well on MNIST, but
not on CIFAR-10. We conjecture that AFA-CS, as a variance reduction method, does not always
perform well in practice. This observation is consistent with the previous work (Defazio & Bottou,
2018; Reddi et al., 2020), which also demonstrated the ineffectiveness of variance reduction methods
in deep learning and some cases of FL."
T,0.9704641350210971,Under review as a conference paper at ICLR 2022
T,0.9713080168776371,"0
20
40
60
80
100 120 140
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
T,0.9721518987341772,Test Accuracy
T,0.9729957805907173,"uniform sampling
biased sampling
VR on biased sampling"
T,0.9738396624472574,(a) p = 1.
T,0.9746835443037974,"0
20
40
60
80
100 120 140
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
T,0.9755274261603376,Test Accuracy
T,0.9763713080168777,"uniform sampling
biased sampling
VR on biased sampling"
T,0.9772151898734177,(b) p = 2.
T,0.9780590717299578,"0
20
40
60
80
100 120 140
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
T,0.9789029535864979,Test Accuracy
T,0.979746835443038,"uniform sampling
biased sampling
VR on biased sampling"
T,0.980590717299578,(c) p = 5.
T,0.9814345991561182,"0
20
40
60
80
100 120 140
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
T,0.9822784810126582,Test Accuracy
T,0.9831223628691983,"uniform sampling
biased sampling
VR on biased sampling"
T,0.9839662447257383,(d) p = 10.
T,0.9848101265822785,Figure 5: Test accuracy for LR on MNIST with asynchrony and dynamic local steps.
T,0.9856540084388186,"0
2000
4000
6000
8000 10000
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8"
T,0.9864978902953586,Test Accuracy
T,0.9873417721518988,"uniform sampling
biased sampling
VR on biased sampling"
T,0.9881856540084388,(a) p = 1.
T,0.9890295358649789,"0
2000
4000
6000
8000 10000
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8"
T,0.9898734177215189,Test Accuracy
T,0.9907172995780591,"uniform sampling
biased sampling
VR on biased sampling"
T,0.9915611814345991,(b) p = 2.
T,0.9924050632911392,"0
2000
4000
6000
8000 10000
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8"
T,0.9932489451476794,Test Accuracy
T,0.9940928270042194,"uniform sampling
biased sampling
VR on biased sampling"
T,0.9949367088607595,(c) p = 5.
T,0.9957805907172996,"0
2000
4000
6000
8000 10000
Communication Round 0.3 0.4 0.5 0.6 0.7 0.8"
T,0.9966244725738397,Test Accuracy
T,0.9974683544303797,"uniform sampling
biased sampling
VR on biased sampling"
T,0.9983122362869198,(d) p = 10.
T,0.99915611814346,Figure 6: Test accuracy for CNN on CIFAR-10 with asynchrony and dynamic local steps.
