Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0038314176245210726,"Recent efforts in interpretable deep learning models have shown that concept-
based explanation methods achieve competitive accuracy with standard end-to-
end models and enable reasoning and intervention about extracted high-level
visual concepts from images, e.g., identifying the wing color and beak length
for bird-species classiﬁcation. However, these concept bottleneck models rely
on a domain expert providing a necessary and sufﬁcient set of concepts–which
is intractable for complex tasks such as video classiﬁcation. For complex tasks,
the labels and the relationship between visual elements span many frames, e.g.,
identifying a bird ﬂying or catching prey–necessitating concepts with various levels
of abstraction. To this end, we present CODEX, an automatic Concept Discovery
and Extraction module that rigorously composes a necessary and sufﬁcient set
of concept abstractions for concept-based video classiﬁcation. CoDEx identiﬁes
a rich set of complex concept abstractions from natural language explanations
of videos–obviating the need to predeﬁne the amorphous set of concepts. To
demonstrate our method’s viability, we construct two new public datasets that
combine existing complex video classiﬁcation datasets with short, crowd-sourced
natural language explanations for their labels. Our method elicits inherent complex
concept abstractions in natural language to generalize concept-bottleneck methods
to complex tasks."
INTRODUCTION,0.007662835249042145,"1
INTRODUCTION"
INTRODUCTION,0.011494252873563218,"Deep neural networks (DNNs) provide unparalleled performance when applied to application domains,
including video classiﬁcation and activity recognition. However, the inherent black-box nature of the
DNNs inhibits the ability to explain the output decisions of a model. While opaque decision-making
may be sufﬁcient for certain tasks, several critical and sensitive applications force model developers
to face a dilemma between selecting the best-performing solution or one that is inherently explainable.
For example, in the healthcare domain (Yeung et al. (2019)), a life-or-death diagnosis compels the use
of the best performing model, yet accepting an automated prediction without justiﬁcation is wholly
insufﬁcient. Ideally, one could take advantage of the power of deep learning while still providing a
sufﬁcient understanding of why a model is making a particular decision, especially if the situation
demands trust in a decision that can have severe impacts."
INTRODUCTION,0.01532567049808429,"To address the need for model interpretability, researchers have sought to enable model intervention
by leveraging concept bottleneck-based explanations. Unlike post hoc explanation methods–where
techniques are used to extract an explanation for a given input for an inference by a trained black-box
model (Chakraborty et al. (2017); Jeyakumar et al. (2020)), concept bottleneck models are inherently
interpretable and take a human reasoning-inspired approach to explaining a model inference based
on an underlying set of concepts that deﬁne the decisions within an application. Thus far, prior
works have focused on concept-based explanation models for image (Kumar et al. (2009); Koh et al.
(2020)) and text classiﬁcation (Murty et al. (2020)). However, the concepts are assumed to be given a
priori by a domain expert–a process that may not result in a necessary and sufﬁcient set of concepts.
For instance, for bird species identiﬁcation, an expert may provide two redundant concepts that are
possibly correlated, such as wing color and beak color. More critically, prior works have considered
simple concepts with the same level of abstraction, e.g., visual elements present in a single image. For
more complex tasks such as video activity classiﬁcation, a label may span multiple frames. Thus, the
composing set of concepts will have various levels of abstraction representing relationships of various"
INTRODUCTION,0.019157088122605363,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.022988505747126436,"visual elements spanning multiple frames, e.g., a bird ﬂapping its wings. Unlike the prior works, we
aim to exploit the complex abstractions inherent in natural language explanations to conceptualize
such complex events."
INTRODUCTION,0.02681992337164751,"Research Questions. In summary, this paper seeks to answer the following research questions:"
INTRODUCTION,0.03065134099616858,"• How can a machine automatically elicit the inherent complex concepts from natural language
to construct a necessary and sufﬁcient set of concepts for video classiﬁcation tasks?"
INTRODUCTION,0.034482758620689655,"• Given that a machine can extract such concepts, are they informative and meaningful enough
to be detected in videos by DNNs for downstream prediction tasks?"
INTRODUCTION,0.038314176245210725,"• Are the machine extracted concepts perceived by humans as good explanations for the
correct classiﬁcations?"
INTRODUCTION,0.0421455938697318,"Approach. This paper introduces an automatic concept extraction module for concept bottleneck-
based video classiﬁcation. The bottleneck architecture equips a standard video classiﬁcation model
with an intermediate concept prediction layer that identiﬁes concepts spanning multiple video frames.
To compose the concepts that will be predicted by the model, we propose a natural language processing
(NLP) based automatic Concept Discovery and Extraction module, CODEX, to extract a rich set of
concepts from natural language explanations of a video classiﬁcation. NLP tools are leveraged to
elicit inherent complex concept abstractions in natural language. CODEX identiﬁes and groups short
textual fragments relating to events, thereby capturing the complex concepts from videos. Thus, we
amortize the effort required to deﬁne and label the necessary and sufﬁcient set of concepts. Moreover,
we employ an attention mechanism to highlight and quantify which concepts are most important for
a given decision."
INTRODUCTION,0.04597701149425287,"To demonstrate the efﬁcacy of our approach, we construct two new datasets–MLB V2E (Video to
Explanations) for baseball activity classiﬁcation and MSR-V2E for video category classiﬁcation–
that combine complex video classiﬁcation datasets with short, crowd-sourced natural language
explanations for their corresponding labels. We ﬁrst compare our model against the existing standard
end-to-end deep-learning methods for video classiﬁcation and show that our architecture provides
additional beneﬁts of an inherently interpretable model with a marginal impact on performance (less
than 0.3% accuracy loss on classiﬁcation tasks). A subsequent user study showed that the extracted
concepts were perceived by humans as good explanations for the classiﬁcation on both the MLB-V2E
and MSR-V2E datasets."
INTRODUCTION,0.04980842911877394,Contributions. We summarize our contributions as follows.
INTRODUCTION,0.05363984674329502,"• We propose CoDEx, a concept discovery and extraction module that leverages NLP tech-
niques to automatically extract complex concept abstractions from crowd-sourced, natural
language explanations for a given video and label–obviating the need to manually deﬁne a
necessary and sufﬁcient set of concepts."
INTRODUCTION,0.05747126436781609,"• We evaluate our approach on complex video classiﬁcation datasets and show that our
model attains high concept accuracies while maintaining competitive task performance with
standard end-to-end video classiﬁcation models."
INTRODUCTION,0.06130268199233716,"• We also augment the concept-based explanation architecture to include an attention mech-
anism that highlights the importance of each concept for a given decision. We show that
users prefer our concept extraction method over baseline methods to explain a given label."
INTRODUCTION,0.06513409961685823,"• We construct two new public datasets, MLB-V2E and MSR-V2E, that combine complex
video classiﬁcation datasets with short, crowd-sourced natural language explanations and
labels."
RELATED WORK,0.06896551724137931,"2
RELATED WORK"
RELATED WORK,0.07279693486590039,"There is a wide array of works in explainable deep learning for various applications. This work
focuses on the concepts-based explanations for video classiﬁcation, and this section provides an
overview of the existing literature for overlapping domains.
Concept-Based Explanations for Images and Text. A number of existing works consider concept-
bottleneck architectures where models are trained to interact with high-level concepts. Generally, the"
RELATED WORK,0.07662835249042145,Under review as a conference paper at ICLR 2022
RELATED WORK,0.08045977011494253,"Explanation
Corpus ( Ɛ )"
RELATED WORK,0.0842911877394636,Concept Matrix ( C )
RELATED WORK,0.08812260536398467,CoDEx Module
RELATED WORK,0.09195402298850575,Concept Bottleneck Model
RELATED WORK,0.09578544061302682,Model Outputs
RELATED WORK,0.09961685823754789,"Figure 1: The overall pipeline showing the automatic concept extraction framework from natural
language explanations and the concept bottleneck classiﬁcation model training framework."
RELATED WORK,0.10344827586206896,"approaches are multi-task architectures, where the model ﬁrst identiﬁes a human-understandable set
of concepts and then reasons about the identiﬁed concepts. Until now, the applications have been
limited to static image and text applications. Koh et al. (2020) used pre-labeled concepts provided
by the dataset to train a model that predicts the concepts, which is then used to predict the ﬁnal
classiﬁcation. However, the caveat is that the concepts had to be manually provided. Ghorbani et al.
(2019) and Yeh et al. (2020) proposed approaches that automatically extract groups of pixels from
the input image that represent meaningful concepts for the prediction. They were designed largely
for image classiﬁcation and extract concepts directly from the dataset. Kim et al. (2018) propose a
post-hoc explanation method that returns the importance of user-deﬁned concepts for a classiﬁcation.
In the mentioned works, the concepts have been limited to simple concepts and are not suited for
complex tasks such as video classiﬁcation where we have complex concepts that may span multiple
frames with various levels of abstraction.
Explanations for Video Classiﬁcation Other approaches have been considered to explain video
classiﬁcation and activity recognition. Chattopadhay et al. (2018) applied GradCAM and GradCAM++
to video classiﬁcation, where for each frame, the important region of the frame to the model is
highlighted as a heatmap. Hiley et al. (2020) extract both spatial and temporal explanations from
input videos by highlighting the relevant pixels. However, these are post-hoc techniques that focus on
explaining black-box models, whereas our approach enables concept-bottleneck methods for video
classiﬁcation that are intended to be inherently interpretable and intervenable.
Video Captioning. In recent years, there is a large number of works (Pan et al. (2017); Gao et al.
(2017); Wang et al. (2018); Yan et al. (2019); Zhou et al. (2018); Chen & Jiang (2021); Yu et al.
(2017)) on video captioning. While they also employ natural language techniques, these works are
tangential to generating text explanations for classiﬁcations, since they are merely describing the
video. Our model provides an explanation justifying the classiﬁcation of the video. Similarly, the
associated datasets such as MSR-VTT (Xu et al. (2016)) only have videos with ground truth captions
that only describe the video without the context of a classiﬁcation–which often results in concepts
that do not pertain to a classiﬁcation.
Semantic Concept Video Classiﬁcation. The closest works to this paper is the body of work in
semantic concept video classiﬁcation (Fan et al. (2004; 2007)), where the concepts are deﬁned as
salient objects that are visually distinguishable video components. The concepts in these works are
simple objects detected in the videos and are not complex enough to capture the semantics of events
that happen over multiple frames of the videos. These works typically used traditional SVM-based
video classiﬁers. Assari et al. (2014) represent a video category based on the co-occurrences of the
semantic concepts and classify based on the co-occurrences, but their method requires a predeﬁned
set of concepts. Thus, we now present the methodology behind our automatic concept extraction for
concept bottleneck video classiﬁcation."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.10727969348659004,"3
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.1111111111111111,"This work introduces CoDEx, an automatic concept extraction method from natural language ex-
planations for concept-based video classiﬁcation. Figure 1 depicts the overall concept-bottleneck
pipeline, composed of CoDEx and the concept bottleneck model. CoDEx extracts a set of concepts"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.11494252873563218,Under review as a conference paper at ICLR 2022
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.11877394636015326,"from natural language explanations that will comprise the bottleneck layer for the video classiﬁcation
model. We ﬁrst formalize the overall problem and then provide the methodology for both modules."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.12260536398467432,"Problem Formalization. We assume that we have a training dataset {(xn, ln)}N
n=1 = D of videos
xn with a label ln ∈L, where L is a predeﬁned set of possible class labels for the video. Each
video is represented as a sequence of frames f ∈F where F is the set of video frames. Thus video
xn = ⟨fn0, fn1, . . . , fnT ⟩, where fnt represents frame t of video n. For each video xn, we form a
label-explanation pair (ln, en), where en is a (short) natural language document explaining the given
label ln. If multiple annotators contribute to an explanation for video-label pair, (xn, ln) , then these
are concatenated to form en. The full set of pairs E = {(ln, en)}N
n=1 is the explanation corpus. Thus,
the design goals are:"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.12643678160919541,"• Concept Discovery and Extraction (CODEX) Module: Given the explanation corpus,
ﬁrst produce an N × K concept matrix, C, where the (n, k)th element is 1 if the nth
explanation contains discovered concept k and 0 otherwise. We call the nth row cn, the
concept vector for video xn. K is the total number of discovered concepts."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.13026819923371646,"• Concept Bottleneck Model: Given a concept matrix, C, the second goal is to train a
concept bottleneck model such that for a given video xi, we predict a concept vector ci–
which indicates the presence or absence of concepts and their importance scores. The model
then makes use of ci to make the ﬁnal video classiﬁcation."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.13409961685823754,"label, ln
explanation, en"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.13793103448275862,"strike
The batter did not swing. The ball was in the strike zone."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.1417624521072797,"foul
The batter hit the ball into the stands and it landed in foul territory."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.14559386973180077,"ball
The hitter didn’t swing. The ball was outside the strike zone."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.14942528735632185,"none
The video did not load."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.1532567049808429,"out
the batter hit the ball and it was caught by the fielder"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.15708812260536398,"label, ln
concepts, cn"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.16091954022988506,"strike
{the batter did not swing, the ball was in the strike zone}"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.16475095785440613,"foul
{the batter hit the ball, it landed in foul territory}"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.1685823754789272,"ball
{the batter did not swing, the ball was outside the strike zone}"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.1724137931034483,"out
{the batter hit the ball, it was caught by the fielder}"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.17624521072796934,"Grouped concepts
Completed concepts
Text Pruned concepts
Text Removed text"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.18007662835249041,Explanation Corpus Ɛ
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.1839080459770115,Extracted Concepts
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.18773946360153257,"Figure 2: Running example for all six stages of the discovery pipeline module. The left table is
the explanation corpus, with highlighted fragments to be modiﬁed. The right table contains the
discovered concepts. The detailed step-by-step modiﬁcations are provided in Appendix A.1."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.19157088122605365,"3.1
CODEX: CONCEPT DISCOVERY AND EXTRACTION MODULE"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.19540229885057472,"We now describe CoDEx, that extracts concepts from the explanation corpus, E. The automatic
extraction of the signiﬁcant concepts is done in 6 steps, as outlined in Fig. 1. These are: cleaning,
extraction, grouping, completion, pruning, and vectorization, which produce the ﬁnal concept
matrix, C. Each of these steps are described below and illustrated with an example corpus depicted
in Figure 2."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.19923371647509577,"Cleaning. We remove explanations associated with corrupted or unlabeled videos from the explana-
tion corpus. In Figure 2, this phase would remove the fourth row with the ""none"" label."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.20306513409961685,"Extraction. The objective of this phase is to identify sentence constituents relevant to explaining the
label. These text fragments, short sequences of words that appear in the document, are referred to
as raw concepts. To achieve this, the cleaned explanation corpus is tokenized then passed through
a pretrained constituency parser to recursively decompose the sentences. At each level of the
constituency hierarchy, the text fragments are evaluated to determine whether they constitute a
candidate raw concept. The rules for candidate raw concepts include the inclusion and exclusion
rules below and follow the widely adopted Universal POS tag naming convention for token types
(Petrov et al. (2012)). Every constituency parsed phrase that satisﬁes one of the two inclusion rules
and not the exclusion rule is considered a candidate concept."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.20689655172413793,"rule name
rule
Inclusion 1.
noun/pronoun →auxillary (optional) →particle (optional) →verb (optional)
Inclusion 2.
noun/pronoun →auxiliary whose lemma is ‘be’ →any token
Exclusion
subordinating conjunction"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.210727969348659,Table 1: Inclusion and exclusion rules for candidate concepts.
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.21455938697318008,Under review as a conference paper at ICLR 2022
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.21839080459770116,"After the extraction process is completed, we have a set of raw concepts, eK, and each video is
associated with a subset of these raw concepts. An example of extracted raw concepts, eK, can be
found in Appendix A.1."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.2222222222222222,"Completion. There are instances where the pretrained constituency parser will split sentences
midway through a text fragment in one sentence that was kept whole in another. For instance, in
Figure 2, the constituency parser splits the explanation for ""foul"" such that ""the batter hit the ball"" is
incorrectly excluded from the raw concepts. To ensure that those concepts are captured, we perform
a substring lookup of each raw concept through all documents of the explanation corpus and count an
explanation as containing a raw concept if it contains the corresponding raw concept as a substring.
This does not change the number of raw concepts identiﬁed but increases their frequency counts."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.2260536398467433,"Grouping (similar raw concepts). When identical text fragments are identiﬁed in different expla-
nations, they are counted directly as the same raw concept. However, we would ideally like to
treat superﬁcially different concepts as the same if they essentially carry the same meaning, e.g.,
Figure 2 highlights two different raw concepts that carry the same meaning and hence can be grouped.
For this, we use agglomerative clustering (Müllner (2011)) approach that measures the degree of
difference between pairs of raw concepts and groups them together if they are similar enough. Our key
contribution here is the distance metric used in clustering which is a novel measure of meta-distance
between raw concepts. This measures the difference between concepts based on two aspects of the
raw concepts: their linguistic difference and their difference in terms of the label categories with
which they are associated."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.22988505747126436,"We deﬁne meta-metric, d, as combining a linguistic distance, dtext (capturing linguistic difference) as
well as a meta-metric, dlabel (capturing the difference in the labels associated with each raw concept).
More formally, for two raw concepts κi, κj ∈eK our distance is linear combination:"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.23371647509578544,"d(κi, κj) = dtext(vi, vj) + λdlabel(ni, nj)
(1)"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.23754789272030652,"where vi is a sentence embedding for the text fragment of concept κi (e.g., based on the BERT
model Devlin et al. (2019)), dtext is a standard distance measure between vectors (e.g., cosine distance),
dlabel is a meta-distance which aims to capture the similarity between two label count vectors, and
λ is a hyperparameter controlling the relative importance between textual and label distance. The
inclusion of a label distance helps to distinguish between concepts that are superﬁcially linguistically
very simlar, but have very distinct meanings within the domain of interest. For instance, without the
dlabel, the concepts “the ball passed inside the strike zone” and “the ball passed outside the strike
zone” will be grouped together though they are very different concepts as they have a very small
dtext. We provide a more formal deﬁnition of the meta-metric dlabel more formally and provide some
intuition behind its construction in Appendix A.3. We also exclude concept groups which occur very
rarely in the explanation corpus, with frequency less than some small threshold, t."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.2413793103448276,"Pruning. Here, we seek a compact subset of concepts that, together, capture a high degree of
information about the label while maintaining interpretability. More formally, after grouping, we
have a set of raw concepts eK = {κ1, . . . , κJ}, and we seek some subset of maximally informative
concepts K⋆= {κj1 . . . , κjK} ⊆eK."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.24521072796934865,"To see what is meant by maximally informative, consider a randomly selected entry in the explanation
corpus (l, e). We deﬁne a binary random variable, Cj for each raw concept κj, and for any concept
set K = {κj1, . . . , κjK}, random vector CK = [C1, . . . , CK], such that Cj = 1 if κj ∈e and 0
otherwise. Y is the random variable which takes label l. We wish to choose the smallest subset of
concepts such that the mutual information (MI)1 between chosen concepts, K, and label, Y , given by
I(Y ; CK), is greater than a threshold fraction, γ < 1 of the MI between the label and the complete
concept vector, I(Y ; C e
K). That is to say we wish to ﬁnd K which satisﬁes:"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.24904214559386972,"I(Y ; CK) ≥γI(Y ; C e
K)
(2)"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.25287356321839083,"and where there is no subset K′ ⊆eK with |K′| < |K| which also satisﬁes Equation equation 2. In
practice, this is infeasible as the problem is combinatorial. However, we note that f(K) = I(Y ; CK)
is a monotone submodular set function of eK. Given this, if we recursively construct a set of size K,
by greedily adding single concepts that most improve the MI, the resulting set will be at least 1 −1 e"
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.2567049808429119,1We use the standard deﬁnition of mutual information (MI) for discrete random variables (MacKay (2003)).
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.26053639846743293,Under review as a conference paper at ICLR 2022
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.26436781609195403,"as good as the most informative set of size K (Nemhauser et al. (1978)). Therefore, we guarantee a
highly-informative set K⋆by iteratively adding concepts to those previously selected, greedy with
respect to the MI, until we have a set that satisﬁes Equation 2."
CONCEPT DISCOVERY AND BOTTLENECK VIDEO CLASSIFICATION,0.2681992337164751,"Vectorization. Each concept κjk ∈K⋆is given a unique index k ∈{1, . . . , K}, and each data-
point, xn is associated with a concept vector cn = (cn1, . . . , cnK), where cnk = 1 if κjk ∈en
and 0 otherwise, indicating the presence or absence of the kth concept in the nth explanation. The
collection of all the concept vectors gives an N × K concept matrix, C."
CONCEPT BOTTLENECK MODEL,0.2720306513409962,"3.2
CONCEPT BOTTLENECK MODEL"
CONCEPT BOTTLENECK MODEL,0.27586206896551724,"We use the videos, the extracted concepts from CoDEx, and the labels to train an interpretable
concept-bottleneck model to predict the activity and the corresponding concepts. Figure 1 shows
the overview of our bottleneck architecture. The activity label, the concepts, and the corresponding
concept scores are the outputs of the interpretable model and are indicated by dotted arrows in
Figure 1."
CONCEPT BOTTLENECK MODEL,0.2796934865900383,"Our bottleneck model architecture is based on the standard end-to-end video classiﬁcation models
where we use convolutional neural network-based feature extractors pretrained on the Imagenet
dataset Deng et al. (2009) to extract the spatial features from the videos. The features are then
passed through temporal layers that can capture features across multiple frames which in turn is
bottle-necked to predict the concepts. Lastly, we deploy an additive attention module (Bahdanau et al.
(2014) that gives the concept score αc indicating the importance of every concept to the classiﬁcation.
The attention module also improves the interpretability of the the bottleneck model by indicating the
key concepts for classiﬁcation and this is evaluated in section 5. More details regarding the model
architecture and hyper-parameters are in the Appendix A.5"
CONCEPT BOTTLENECK MODEL,0.2835249042145594,"Model loss function. The entire bottleneck classiﬁcation model is trained in an end-to-end manner.
Since the concepts are represented as binary vectors, we use sigmoid activation on the concept
bottleneck layer and binary categorical loss function as the concept loss. The ﬁnal layer of the
classiﬁer has softmax activations and categorical cross-entropy as the classiﬁcation loss function.
Thus, the overall loss function of the model is the sum of concept loss and the classiﬁcation loss. The
hyperparameter β controls the tradeoff between concept loss, LC, versus classiﬁcation loss, LY as
shown in equation 3. The full expansion of the equation is located in Appendix A.5."
CONCEPT BOTTLENECK MODEL,0.28735632183908044,"Loss(L) = 1 N N
X"
CONCEPT BOTTLENECK MODEL,0.29118773946360155,"n=1
(LYn + β × LCn)
where
β > 0
(3)"
CONCEPT BOTTLENECK MODEL,0.2950191570881226,"Testing phase. Given an input test video, the model provides us with the activity prediction (label
of the video), a concept vector indicating the relevant concepts that induced this classiﬁcation and
the concept importance score for each concept. By retrieving the phrase representing the concepts
present in the video, the result obtained is a human-understandable explanation of the classiﬁcation."
IMPLEMENTATION,0.2988505747126437,"4
IMPLEMENTATION"
IMPLEMENTATION,0.30268199233716475,"To demonstrate our automatic concept extraction method, we construct two new datasets - MLB-V2E
(Video to Explanations) and MSR-V2E, which combines short video clips with crowd-sourced
classiﬁcation labels and corresponding natural language explanations. For both datasets, we obtained
a video activity label and natural language explanations for that label by crowd-sourcing on Amazon
Mechanical Turk and used unrestricted text explanations to extract concepts automatically. For IRB
exemption and compensation information, please refer to the Ethics Statement.
MLB-V2E Dataset: We used a subset of the MLB-Youtube video activity classiﬁcation dataset
introduced by Piergiovanni & Ryoo (2018)–which had segmented video clips containing the ﬁve
primary activities in baseball: strike, ball, foul, out, in play. We preprocessed the dataset and
extracted 2000 segmented video clips where each video was 12 seconds long, 224×224 in resolution,
and recorded at 30 fps. To ensure that the quality of explanations is good, we screened over 450
participants. Based on their baseball knowledge, 150 participants were qualiﬁed to provide the natural
language text explanations for our video clips. We have included a sample of our screening survey,
the primary survey, and the explanations collected in the supplementary materials."
IMPLEMENTATION,0.3065134099616858,Under review as a conference paper at ICLR 2022
IMPLEMENTATION,0.3103448275862069,"Dataset
Number of Concepts after each Phase"
IMPLEMENTATION,0.31417624521072796,"Extraction
Completion
Grouping
Pruning"
IMPLEMENTATION,0.31800766283524906,"MLB-V2E
1885
1885
225
80
MSR-V2E
1678
1678
104
62"
IMPLEMENTATION,0.3218390804597701,"Table 2: The number of concepts extracted by the Concept Discovery module from the explanation
corpus after every phase."
IMPLEMENTATION,0.32567049808429116,"(a) MLB-V2E Dataset
(b) MSR-V2E Dataset"
IMPLEMENTATION,0.32950191570881227,"Figure 3: The number of concepts versus performance trade-off for the (a) the MLB-V2E dataset and
(b) the MSR-V2E dataset."
IMPLEMENTATION,0.3333333333333333,"MSR-V2E Dataset: For this dataset, we used 2020 video clips from the MSR VTT dataset introduced
by Xu et al. (2016). The MSR-VTT dataset has general videos from everyday life and descriptions of
these videos associated with them. Each video clip is between 10-30 seconds long, and approximately
200 participants provided the labels and explanations to construct the MSR-V2E dataset. The videos
are classiﬁed into ten categories: Automobiles, Cooking, Beauty and Fashion, News, Science and
Technology, Eating, Playing Sports, Music, Animals, and Family (more details in Appendix A.11).
Training: All our models were trained on 2 × Titan GTX GPUs using Adam optimizer. A summary
of our entire model architecture and a trained model is provided in the supplementary materials."
RESULTS,0.3371647509578544,"5
RESULTS"
RESULTS,0.34099616858237547,"Number of extracted concepts. Table 2 shows that the system extracted 80 concepts and 62 concepts
from the explanation corpus of MLB-V2E and MSR-V2E respectively. The number of concepts
remaining after the pruning phase is determined by the cumulative Mutual Information(MI) threshold.
To identify the best threshold, we plotted the number of concepts at different thresholds versus
performance of the model as shown in Figure 3. We found that the task classiﬁcation performance
did not increase after a certain number of concepts and that optimal spot for the number of concepts
corresponded to 90% Mutual Information (Appendix A.6)."
RESULTS,0.3448275862068966,"Comparing concept-bottleneck models to baselines. We adopt model architectures and hyperpa-
rameters from standard well-performing approaches that fall under 3 categories: 1) without concept
bottleneck, 2) with concept bottleneck, 3) with concept bottleneck and attention. We compared the
performance of models with the bottleneck layer with standard video classiﬁcation models without
a concept bottleneck layer. We ﬁnd that, though the latent space was constrained to the limited set
of concepts extracted from the explanation corpus, concept models performed as well as the uncon-"
RESULTS,0.3486590038314176,"Dataset
Feature
Extractor
Model Type
Task Classiﬁcation
Concepts"
RESULTS,0.3524904214559387,"Accuracy(%)
F1-score
AUC"
RESULTS,0.3563218390804598,"MLB-V2E
Inception V3
Standard
68.46 ± 1.27
0.68 ± 0.011
-
Bottleneck
68.16 ± 1.12
0.68 ± 0.004
0.85 ± 0.003
Bottleneck + Attn.
68.38 ± 1.34
0.68 ± 0.004
0.88 ± 0.001"
RESULTS,0.36015325670498083,"MSR-V2E
Inception V3
Standard
61.79 ± 1.42
0.60 ± 0.012
-
Bottleneck
61.42 ± 1.18
0.60 ± 0.013
0.83 ± 0.006
Bottleneck + Attn.
61.68 ± 1.23
0.60 ± 0.009
0.86 ± 0.004"
RESULTS,0.36398467432950193,Table 3: Performance of Models. The full table can be found in Appendix A.7.
RESULTS,0.367816091954023,Under review as a conference paper at ICLR 2022
RESULTS,0.3716475095785441,"Dataset
Concepts Extraction
Task Accuracy(%)
Task F1-score
Concept AUC"
RESULTS,0.37547892720306514,MLB-V2E
RESULTS,0.3793103448275862,"CoDEx
68.38
0.6802
0.8801
w/o Extraction
63.47
0.5823
0.8185
w/o Grouping
67.80
0.6772
0.8122
w/o Pruning
68.36
0.6802
0.8419
w/o Grouping and Pruning
65.29
0.6526
0.7821"
RESULTS,0.3831417624521073,MSR-V2E
RESULTS,0.38697318007662834,"CoDEx
61.68
0.6010
0.8600
w/o Extraction
58.02
0.5214
0.7830
w/o Grouping
59.31
0.5745
0.7888
w/o Pruning
61.68
0.6010
0.8131
w/o Grouping and Pruning
54.70
0.5178
0.7467"
RESULTS,0.39080459770114945,Table 4: Ablation Studies: Shows the effect of each step in CODEX on model performance
RESULTS,0.3946360153256705,"strained models, on both datasets. We also ﬁnd that the addition of the attention layer improves the
concept prediction of the models. Table 3 shows that concept bottleneck models achieved comparable
task accuracy to standard black-box models on both tasks, despite the bottleneck constraint while
achieving high concept prediction performance. Appendix A.7 shows the performance with other
feature extractors."
RESULTS,0.39846743295019155,"(a) MLB-V2E Dataset
(b) MSR-V2E Dataset"
RESULTS,0.40229885057471265,"Figure 4: Explanation offered by the model indicating the predicted class concepts present and their
corresponding scores for (a) the MLB-V2E dataset (b) the MSR-V2E dataset."
RESULTS,0.4061302681992337,"Ablation Study of CoDEx. We performed an ablation study to highlight the impact of CODEX’s
components. For sentence-level concept extraction, we replaced the extraction phase with word-level
concepts extracted from the explanation corpus. We also evaluated how removing the grouping and
pruning phases would impact performance. Table 4 shows the results of our study. We ﬁnd that using
word-level concepts signiﬁcantly reduced the performance of predicting the task and the concepts.
The grouping and pruning stages had a greater impact on the concept prediction performance that
affected the explainability of the model."
RESULTS,0.4099616858237548,"Concept scores for interpretability. Not only does the attention module increase performance in
concept prediction, but it also improves the explainability of the bottleneck model by providing an
importance score for the concepts. Figure 4 shows the explanation from the concept bottleneck model
with attention on a test sample from the two datasets. More examples can be found in Appendix A.10.
The title shows the classiﬁcation label, the y-axis indicates the top-3 concepts predicted as present
in the video clip, and the x-axis corresponds to the concept score. Others refers to the sum of the
importance of all the remaining concepts."
RESULTS,0.41379310344827586,"Human study to evaluate concepts’ explainability. We performed a Mechanical Turk study to
evaluate the explainability of our extracted complex concepts to the end-users. The participants were
asked to select from four different options (presented in random) of what they consider to be the best
possible Explanation for the classiﬁcation of a given video. The four options are: Complex concepts
predicted models without attention, Complex concepts predicted by models with attention, Concepts
of a random video not belonging to the same predicted class and a Random set of 2-5 concepts from
the set of the most frequent concepts. The methodology of this study was inspired by Chang et al.
(2009)’s paper. Figure 5 presents the aggregated results of the Mechanical Turk study. The complex
concepts predicted by the concept bottleneck model with attention was considered as the preferred
explanation by 68% and 57% of the responses in the MLB-V2E and MSR-V2E datasets respectively
followed by the concepts bottleneck models without attention in 20% and 28% of the responses for"
RESULTS,0.41762452107279696,Under review as a conference paper at ICLR 2022
RESULTS,0.421455938697318,"(a) MLB-V2E Dataset
(b) MSR-V2E Dataset
Figure 5: Survey responses with 95% bootstrap conﬁdence interval for the two datasets"
RESULTS,0.42528735632183906,"the two datasets. The presented conﬁdence intervals are calculated using the bootstrap method as
described by DiCiccio & Efron (1996) for 95% conﬁdence."
DISCUSSION,0.42911877394636017,"6
DISCUSSION"
DISCUSSION,0.4329501915708812,"Annotation effort. Although CODEX requires collecting a large explanation corpus, prior work
requires two studies: one study to identify the set of essential concepts, and a second study to
annotate the videos with the essential concepts–whereas CODEX only requires a single study.
Further, if the vocabulary of concepts is large, the annotation process would be inconvenient for the
user. Moreover, natural language explanations can express richer compositions of concepts rather
than simply identifying the presence or absence of an individual concept. Thus, CODEX’s annotation
efforts are more expressive, less expensive and less cumbersome than prior works.
Representative Concept. Our concept extraction method selects the most frequent concept in a
grouped cluster as the representative concept. In general, the most frequent concept sufﬁces to
explain a particular component of the complex activity. However, there were some instances where
the most frequent concept would have a speciﬁc terminology rather than a general term, e.g., ""left
ﬁelder"" is a subclass of ""outﬁelder."" Future work can strive towards generating the representative
concept for a cluster, as opposed to opting for the most frequent or popular phrasing.
Preserving Spatial-temporal Semantics. Our model’s output explanation currently provides a set
of activated concepts along with their score. However, they do not capture the spatial and temporal
relationships between concepts. Some rich concepts implicitly embed spatial and temporal properties,
e.g., “the batter hit the ball on the ground"" implies the following sequence: a batter swung at a ball,
made contact with the ball, and the ball landed on the ground. However, if the generated set of
concepts is limited to less informative concepts, e.g., “the batter,"" the spatial and temporal ordering
of concepts matters. Future work can generalize the architecture to generate concept-based natural
language explanations that explicitly preserve spatial-temporal semantics.
Neural-symbolic Reasoning. Our model’s reasoning layers are inherently black-box in nature, i.e.,
the concept vectors are fed into a fully connected network. To further bolster human-machine teaming
and interpretability, the ﬁnal classiﬁcation model can be replaced with a rule-based model–analogous
to prior works that fuse deep learning inferences with symbolic reasoning layers for complex event
detection (Xing et al. (2020); Vilamala et al. (2019))."
CONCLUSION,0.4367816091954023,"7
CONCLUSION"
CONCLUSION,0.44061302681992337,"The remarkable performance of deep neural networks is only limited by the stark limitation in
clearly explaining their inner workings. While researchers have introduced feature highlighting
explanation techniques to provide insight into these black-box models, concept-bottleneck models
offer a promising new approach to explanation by decomposing application tasks into a set of
underlying concepts. We build upon concept-based explanations by introducing an automatic concept
extraction module, CoDEx, to a general concept bottleneck architecture for identifying, training,
and explaining video classiﬁcation tasks. In coalescing concept deﬁnitions across crowd-sourced
explanations, CoDEx amortizes the expertise of concept deﬁnition while removing the burden from
the model developer. We also show that our method provides reasonable explanations for classiﬁcation
without compromising performance compared to standard end-to-end video classiﬁcation models."
CONCLUSION,0.4444444444444444,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.4482758620689655,ETHICS STATEMENT
ETHICS STATEMENT,0.4521072796934866,"IRB Exemption and Compensation. This research study has been certiﬁed as exempt from review
by the IRB and the participants were compensated at a rate of 15 USD per hour for a total of 920.36
USD spent."
ETHICS STATEMENT,0.4559386973180077,"Dataset privacy. There was no personally identiﬁable information collected at anytime during the
turk study. The responses provided by the mechanical turkers that are present in the dataset are
completely anonymous."
REPRODUCIBILITY STATEMENT,0.45977011494252873,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.46360153256704983,"The entire code with detailed comments are provided in the supplementary materials. The model
architectures and hyper-parameters used are discussed in Appendix A.5. All the plots and graphs can
be obtained by running the code without modiﬁcations."
REFERENCES,0.4674329501915709,REFERENCES
REFERENCES,0.47126436781609193,"Shayan Modiri Assari, Amir Roshan Zamir, and Mubarak Shah. Video classiﬁcation using semantic
concept co-occurrences. In 2014 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2529–2536. IEEE, 2014."
REFERENCES,0.47509578544061304,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014."
REFERENCES,0.4789272030651341,"Supriyo Chakraborty, Richard Tomsett, Ramya Raghavendra, Daniel Harborne, Moustafa Alzan-
tot, Federico Cerutti, Mani Srivastava, Alun Preece, Simon Julier, Raghuveer M Rao, et al.
Interpretability of deep learning models: a survey of results. In 2017 IEEE smartworld, ubiq-
uitous intelligence & computing, advanced & trusted computed, scalable computing & com-
munications, cloud & big data computing, Internet of people and smart city innovation (smart-
world/SCALCOM/UIC/ATC/CBDcom/IOP/SCI), pp. 1–6. IEEE, 2017."
REFERENCES,0.4827586206896552,"Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L Boyd-Graber, and David M Blei. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing
systems, pp. 288–296, 2009."
REFERENCES,0.48659003831417624,"Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Grad-
cam++: Generalized gradient-based visual explanations for deep convolutional networks. In 2018
IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 839–847. IEEE, 2018."
REFERENCES,0.4904214559386973,"Shaoxiang Chen and Yu-Gang Jiang. Towards bridging event captioner and sentence localizer
for weakly supervised dense event captioning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 8425–8435, 2021."
REFERENCES,0.4942528735632184,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.49808429118773945,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding, 2019."
REFERENCES,0.5019157088122606,"Thomas J DiCiccio and Bradley Efron. Bootstrap conﬁdence intervals. Statistical science, 11(3):
189–228, 1996."
REFERENCES,0.5057471264367817,"Jianping Fan, Hangzai Luo, Jing Xiao, and Lide Wu. Semantic video classiﬁcation and feature subset
selection under context and concept uncertainty. In Proceedings of the 2004 Joint ACM/IEEE
Conference on Digital Libraries, 2004., pp. 192–201. IEEE, 2004."
REFERENCES,0.5095785440613027,"Jianping Fan, Hangzai Luo, Yuli Gao, and Ramesh Jain. Incorporating concept ontology for hierar-
chical video classiﬁcation, annotation, and visualization. IEEE Transactions on Multimedia, 9(5):
939–957, 2007."
REFERENCES,0.5134099616858238,Under review as a conference paper at ICLR 2022
REFERENCES,0.5172413793103449,"Lianli Gao, Zhao Guo, Hanwang Zhang, Xing Xu, and Heng Tao Shen. Video captioning with
attention-based lstm and semantic consistency. IEEE Transactions on Multimedia, 19(9):2045–
2055, 2017."
REFERENCES,0.5210727969348659,"Amirata Ghorbani, James Wexler, James Zou, and Been Kim. Towards automatic concept-based
explanations. arXiv preprint arXiv:1902.03129, 2019."
REFERENCES,0.524904214559387,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016."
REFERENCES,0.5287356321839081,"Liam Hiley, Alun Preece, Yulia Hicks, Supriyo Chakraborty, Prudhvi Gurram, and Richard Tomsett.
Explaining motion relevance for activity recognition in video deep learning models. arXiv preprint
arXiv:2003.14285, 2020."
REFERENCES,0.5325670498084292,"Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.5363984674329502,"Jeya Vikranth Jeyakumar, Joseph Noor, Yu-Hsi Cheng, Luis Garcia, and Mani Srivastava. How can i
explain this to you? an empirical study of deep neural network explanation methods. Advances in
Neural Information Processing Systems, 33, 2020."
REFERENCES,0.5402298850574713,"Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al.
Interpretability beyond feature attribution: Quantitative testing with concept activation vectors
(tcav). In International conference on machine learning, pp. 2668–2677. PMLR, 2018."
REFERENCES,0.5440613026819924,"Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and
Percy Liang. Concept bottleneck models. In International Conference on Machine Learning, pp.
5338–5348. PMLR, 2020."
REFERENCES,0.5478927203065134,"Neeraj Kumar, Alexander C Berg, Peter N Belhumeur, and Shree K Nayar. Attribute and simile
classiﬁers for face veriﬁcation. In 2009 IEEE 12th international conference on computer vision,
pp. 365–372. IEEE, 2009."
REFERENCES,0.5517241379310345,"Colin Lea, Michael D Flynn, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutional
networks for action segmentation and detection. In proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 156–165, 2017."
REFERENCES,0.5555555555555556,"David J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University
Press, 2003."
REFERENCES,0.5593869731800766,"Daniel Müllner.
Modern hierarchical, agglomerative clustering algorithms.
arXiv preprint
arXiv:1109.2378, 2011."
REFERENCES,0.5632183908045977,"Shikhar Murty, Pang Wei Koh, and Percy Liang. Expbert: Representation engineering with natural
language explanations. arXiv preprint arXiv:2005.01932, 2020."
REFERENCES,0.5670498084291188,"G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing
submodular set functions i. Mathematical Programming, 14:265–294, 1978."
REFERENCES,0.5708812260536399,"Yingwei Pan, Ting Yao, Houqiang Li, and Tao Mei. Video captioning with transferred semantic
attributes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017."
REFERENCES,0.5747126436781609,"Slav Petrov, Dipanjan Das, and Ryan McDonald. A universal part-of-speech tagset. In Proceedings
of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pp.
2089–2096, 2012."
REFERENCES,0.578544061302682,"AJ Piergiovanni and Michael S. Ryoo. Fine-grained activity recognition in baseball videos. In CVPR
Workshop on Computer Vision in Sports, 2018."
REFERENCES,0.5823754789272031,"Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, 11 2019. URL http://arxiv.org/abs/1908.
10084."
REFERENCES,0.5862068965517241,Under review as a conference paper at ICLR 2022
REFERENCES,0.5900383141762452,"Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108, 2019."
REFERENCES,0.5938697318007663,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818–2826, 2016."
REFERENCES,0.5977011494252874,"Marc Roig Vilamala, Liam Hiley, Yulia Hicks, Alun Preece, and Federico Cerutti. A pilot study
on detecting violence in videos fusing proxy models. In 2019 22th International Conference on
Information Fusion (FUSION), pp. 1–8. IEEE, 2019."
REFERENCES,0.6015325670498084,"Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Reconstruction network for video captioning. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7622–7631,
2018."
REFERENCES,0.6053639846743295,"Tianwei Xing, Luis Garcia, Marc Roig Vilamala, Federico Cerutti, Lance Kaplan, Alun Preece,
and Mani Srivastava. Neuroplex: learning to detect complex events in sensor networks through
knowledge injection. In Proceedings of the 18th Conference on Embedded Networked Sensor
Systems, pp. 489–502, 2020."
REFERENCES,0.6091954022988506,"Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging
video and language. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 5288–5296, 2016."
REFERENCES,0.6130268199233716,"Chenggang Yan, Yunbin Tu, Xingzheng Wang, Yongbing Zhang, Xinhong Hao, Yongdong Zhang, and
Qionghai Dai. Stat: Spatial-temporal attention mechanism for video captioning. IEEE transactions
on multimedia, 22(1):229–241, 2019."
REFERENCES,0.6168582375478927,"Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pﬁster, and Pradeep Ravikumar. On
completeness-aware concept-based explanations in deep neural networks. Advances in Neural
Information Processing Systems, 33, 2020."
REFERENCES,0.6206896551724138,"Serena Yeung, Francesca Rinaldo, Jeffrey Jopling, Bingbin Liu, Rishab Mehra, N Lance Downing,
Michelle Guo, Gabriel M Bianconi, Alexandre Alahi, Julia Lee, et al. A computer vision system
for deep learning-based detection of patient mobilization activities in the icu. NPJ digital medicine,
2(1):1–5, 2019."
REFERENCES,0.6245210727969349,"Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee Kim. End-to-end concept word detection
for video captioning, retrieval, and question answering. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 3165–3173, 2017."
REFERENCES,0.6283524904214559,"Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end dense
video captioning with masked transformer. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 8739–8748, 2018."
REFERENCES,0.632183908045977,"A
APPENDIX"
REFERENCES,0.6360153256704981,"A.1
RUNNING EXAMPLE TO DESCRIBE THE CONCEPT DISCOVERY PIPELINE"
REFERENCES,0.6398467432950191,"Example (A simple Explanation corpus). Consider using baseball domain with a few meaningful
labels, a null label L = {strike, ball, foul, out, none} and ﬁve entries in the explanation corpus E:"
REFERENCES,0.6436781609195402,"id, n
label, ln
explanation, en
1
strike
The batter did not swing. The ball was in the strike zone.
2
foul
the batter hit the ball into the stands and it landed in foul territory
3
ball
The hitter didn’t swing. The ball was outside the strike zone.
4
none
The video did not load.
5
out
the batter hit the ball and it was caught by the ﬁelder"
REFERENCES,0.6475095785440613,Example (After Cleaning Phase). The none label entry is removed after the Cleaning phase
REFERENCES,0.6513409961685823,Under review as a conference paper at ICLR 2022
REFERENCES,0.6551724137931034,"id, n
label, ln
explanation, en
1
strike
The batter did not swing. The ball was in the strike zone.
2
foul
the batter hit the ball into the stands and it landed in foul territory.
3
ball
The hitter didn’t swing. The ball was outside the strike zone.
4
out
the batter hit the ball and it was caught by the ﬁelder"
REFERENCES,0.6590038314176245,"Example (After Extraction Phase). The raw concepts are extracted based on the deﬁned rules
discussed in Table 1 corresponding to each explanation with the help of a pre-trained constituency
parser."
REFERENCES,0.6628352490421456,"id, n
label, ln
raw concepts, eK
1
strike
The batter did not swing, The ball was in the strike zone
2
foul
the ball into the stands, it landed in foul territory
3
ball
The hitter didn’t swing, The ball was outside the strike zone
4
out
the batter hit the ball, it was caught by the ﬁelder"
REFERENCES,0.6666666666666666,"Example (After Completion Phase). The concept ’the batter hit the ball’ was not extracted by the
Extraction phase for the Id 2 explanation in this corpus. This missing concept is retrieved through
the sub-string matching."
REFERENCES,0.6704980842911877,"id, n
label, ln
raw concepts, eK
1
strike
The batter did not swing, The ball was in the strike zone
2
foul
the batter hit the ball, the ball into the stands, it landed in foul territory
3
ball
The hitter didn’t swing, The ball was outside the strike zone
4
out
the batter hit the ball, it was caught by the ﬁelder"
REFERENCES,0.6743295019157088,Example (After Grouping Phase). We show the grouped concepts for this example corpus
REFERENCES,0.6781609195402298,"Concept-index, i
Concept groups
1
[The batter did not swing, The hitter didn’t swing]
2
[the batter hit the ball, the batter hit the ball]
3
[The ball was in the strike zone]
4
[The ball into the stands]
5
[it landed in foul territory]
6
[The ball was outside the strike zone]
7
[it was caught by the ﬁelder]"
REFERENCES,0.6819923371647509,"Example (After Pruning Phase). The concept ’the ball into the stands’ of Concept-Index 4 from
previous table was not contributing much and hence was pruned."
REFERENCES,0.685823754789272,"Concept-index, i
Concept groups
1
[The batter did not swing, The hitter didn’t swing]
2
[the batter hit the ball, the batter hit the ball]
3
[The ball was in the strike zone]
4
[it landed in foul territory]
5
[The ball was outside the strike zone]
6
[it was caught by the ﬁelder]"
REFERENCES,0.6896551724137931,"Example (After Vectorization). After pruning, each sample is mapped to their corresponding concept
vector. The value of concept vector at index i is 1 if en has the concept with index i, else, it’s 0. The
Matrix containing all the concept vectors is called the Concept Matric, C"
REFERENCES,0.6934865900383141,"id, n
label, ln
Concept Vector, cn
1
strike
[1,0,1,0,0,0]
2
foul
[0,1,0,1,0,0]
3
ball
[1,0,0,0,1,0]
4
out
[0,1,0,0,0,1]"
REFERENCES,0.6973180076628352,Under review as a conference paper at ICLR 2022
REFERENCES,0.7011494252873564,Figure 6: Constituency tree for explanation e1 from Example A.1
REFERENCES,0.7049808429118773,"A.2
EXAMPLE CONSTITUENCY TREE"
REFERENCES,0.7088122605363985,"The explanation n = 1 from Example A.1 is decomposed into the constituency tree shown in Figure
6. The parser gives a hierarchy of constituents. Our method traverses through this tree and selects the
constituents that satisfy the rules discussed in Table 1. It is important to note that rules can be added
or deleted at this step based on the requirements."
REFERENCES,0.7126436781609196,"A.3
META-DISTANCE FOR LABEL BASED PROXIMITY"
REFERENCES,0.7164750957854407,"At the end of the Completion Phase we deﬁne a count for each raw concept, κi ∈eK, given
by Mi. And for each label category l ∈L, we deﬁne a label count for raw concept κi as mil
where i is the index of the concept. These count the presence of raw concepts explanations, and
P"
REFERENCES,0.7203065134099617,"l∈L mil = Mi. Finally we group together raw concept κi’s label counts into a label count vector
mi = [mi1, . . . , mi|L|]"
REFERENCES,0.7241379310344828,"Now we describe the meta-metric dlabel used in the Grouping phase more formally and provide some
intuition behind its construction. Consider that we have two raw concepts κi, κj ∈eK and label count
vectors mi and mj. We next assume that vector mi constitutes Mi i.i.d. draws from a categorical
distribution with unknown parameters µi = (µil)|L|
l=1, where µil is the probability that a randomly
selected occurrence of raw concept i belongs to an entry in the explanation corpus with label category
l. Our label distance dlabel is the evidence ratio between the count vectors, mi and mj, being drawn
from independent categorical distributions (model Mindp) versus them being drawn from the same
distribution (model Mcomb). More precisely,"
REFERENCES,0.7279693486590039,"dlabel(mi, mj) = p(mi, mj|Mindp)"
REFERENCES,0.7318007662835249,"p(mi, mj|Mcomb)"
REFERENCES,0.735632183908046,"Note that this is not a true distance between count vectors as two identical count vectors do not
have a distance of zero. Nonetheless, it satisﬁes the other requirements of a metric: non-negativity,
symmetry and the triangle inequality, and two vectors that are more (less) likely to come from the
same multinomial will have a distance less (more) than 1."
REFERENCES,0.7394636015325671,"To evaluate the label distance we must calculate the evidence for various categorical samples given
the model p(m|µ, M) . For simplicity, we assume total count M is known and deﬁne a Dirichlet
prior p(µ|α1) where 1 is the vector of all 1s (this makes the simplifying assumption that the prior is
symmetric). The evidence for m is then:"
REFERENCES,0.7432950191570882,"p(m|α) =
Z
p(m|µ, M)p(µ|α1)dµ"
REFERENCES,0.7471264367816092,The label meta-metric is the evidence ratio given by:
REFERENCES,0.7509578544061303,"dlabel(mi, mj) = p(mi|α)p(mj|α)"
REFERENCES,0.7547892720306514,p(mi + mj|α)
REFERENCES,0.7586206896551724,Under review as a conference paper at ICLR 2022
REFERENCES,0.7624521072796935,"For computational efﬁciency (and since it did not appear to affect results measurably) we use an
approximation for p(m|α) in our calculation of dl."
REFERENCES,0.7662835249042146,"We ﬁrst evaluate the expected parameter of the posterior distribution given count vector m, namely"
REFERENCES,0.7701149425287356,"˜µ = E[µ|α, m]"
REFERENCES,0.7739463601532567,"then evaluate the evidence for m conditioned on ˜µ, i.e."
REFERENCES,0.7777777777777778,"p(m|˜µ) = K
Y k=1"
REFERENCES,0.7816091954022989, mil + α
REFERENCES,0.7854406130268199,Mi + Kα mil
REFERENCES,0.789272030651341,"We then calculate log dlabel(mi, mj) and exponentiate to improve precision. After grouping, the raw
concept within a cluster with highest frequency is identiﬁed as the representative concept of that
cluster."
REFERENCES,0.7931034482758621,"A.4
LANGUAGE MODELS"
REFERENCES,0.7969348659003831,"A.4.1
CONCEPT EXTRACTION"
REFERENCES,0.8007662835249042,"After obtaining the free form textual explanations for both, we ﬁrst cleaned them by removing explana-
tions associated with corrupted video ﬁles and the videos which were labelled incorrectly. We then con-
sidered three different Spacy’s pretrained constituency parsers: en_core_web_lg, en_core_web_md,
en_core_web_sm to parse the explanations and extract raw concepts based on the rules discussed
in section 3.1. We found that, the parser en_core_web_lg was more accurate in identifying the
constituents and resulted in better concept extraction."
REFERENCES,0.8045977011494253,"A.4.2
CONCEPT GROUPING"
REFERENCES,0.8084291187739464,"For text distance we embedded the raw concepts with a sentence encoder and we exper-
imented with two models:
paraphrase-distilroberta-base-v1 (distil) Sanh et al.
(2019) and stsb-roberta-base (stsb) Reimers & Gurevych (2019) both using the
sentence_transformer python library, and evaluated a variety of distance metrics within
the resulting 768 dimensional space, including: Chebyshev (inﬁnity norm), manhattan, Euclidean
and cosine distances."
REFERENCES,0.8122605363984674,"And to cluster the semantically similar concepts together using agglomerative clustering Müllner
(2011), we evaluated a variety of distance metrics within the resulting 768-dimensional space,
including: Our proposed meta-distance metric, Chebyshev (inﬁnity norm), manhattan, Euclidean
and cosine distances. To select hyperparameters, including: choice of sentence embedding model,
distance metric for sentence embeddings, prior α for label distance, and relative importance factor λ
we performed a grid search and selected the values that resulted in well-formed clusters. Based on our
experiments (Provided in supplementary materials), we found that stsb encoder with our proposed
meta-distance metric resulted in the best grouping of concepts. Note: dtext can be either cosine or
manhattan distance as they gave similar clusters."
REFERENCES,0.8160919540229885,"After clustering, we set the frequency occurrence threshold of 3 and removed the rare concept groups
which occurred less than this threshold. Then, pruning was done using 90% of mutual information
score as discussed in section 3.1 which resulted in 80 signiﬁcant concepts for our MLB-V2E dataset
and 62 concepts for MSR-V2E dataset as shown in Table 2 and Figure 7. Therefore, each video was
associated with a binary concept vector of shape [1×k] where k is the number of concepts, indicating
the presence and absence of each concept."
REFERENCES,0.8199233716475096,"A.5
THE CLASSIFICATION MODELS"
REFERENCES,0.8237547892720306,"We considered three different feature extractors : Resnet 50v2 He et al. (2016), Resnet 101v2 He
et al. (2016) and InceptionV3 Szegedy et al. (2016) models and pretrained on the Imagenet dataset to
extract features from each frame of our video clips. We excluded the ﬁnal classiﬁcation layer from
these models and did a global maxpool across the width and height such that we get a 2048 size
feature vector for every frame. We then concatenate the features together, resulting in a [2048 × 360]
feature matrix for every video where 360 is the number of frames per video."
REFERENCES,0.8275862068965517,Under review as a conference paper at ICLR 2022
REFERENCES,0.8314176245210728,"(a) MLB-V2E Dataset
(b) MSR-V2E Dataset
Figure 7: Selecting concepts based on the Mutual Information. (a)MLB-V2E dataset (b)MSR-V2E
Dataset"
REFERENCES,0.8352490421455939,"For the Temporal Layer, we considered both temporal convolution Lea et al. (2017) and LSTM Hochre-
iter & Schmidhuber (1997) based architectures which are good at extracting temporal features and
found that Temporal CNNs outperformed LSTM by a signiﬁcant amount. And the Bottleneck Layer
is a dense layer with k neurons and hence the output is a vector of shape [1 × k] where k is the
number of signiﬁcant concepts. We introduced an attention layer in the concept-bottleneck model
that gives the concept score for each concept. The ﬁnal fully connected layer is implemented with L
neurons (L classes) which predicts the class from the video."
REFERENCES,0.8390804597701149,Model Loss function Assuming s is the feature vectors obtained from the Feature extractors
REFERENCES,0.842911877394636,"Loss(L) = 1 N N
X"
REFERENCES,0.8467432950191571,"n=1
(LYn + β × LCn)
(4) = 1 n n
X i=1  β l
X k=1"
REFERENCES,0.8505747126436781,"
−ci
k log(fσ(sk)) −(1 −ck log(1 −fσ(sk)))

− m
X"
REFERENCES,0.8544061302681992,"j=1
yj log fS(sj)  "
REFERENCES,0.8582375478927203,"where
fσ(si) =
1
1 + e−si
and
fS(si) =
esi
Pm
j=1 esj
and
β > 0"
REFERENCES,0.8620689655172413,"A.6
SELECTING CONCEPTS BASED ON THE MUTUAL INFORMATION(MI)"
REFERENCES,0.8659003831417624,"Figure 7 shows the plot between cumulative MI and the number of concepts after pruning. As
discussed in Section 5 the sweet spot for the number of concepts corresponded to 90% of the
cumulative MI beyond which there was no gain in classiﬁcation performance as we increased the
number of concepts."
REFERENCES,0.8697318007662835,"A.7
PERFORMANCE OF MODELS"
REFERENCES,0.8735632183908046,"Table 5 shows the performance of all the models with different feature extractors. Each model was
trained thrice and the mean and standard deviations are reported. We ﬁnd that models with Inception
V3 as the feature extractor performed the best. Adding attention mechanism greatly improved the
performance of concepts prediction and also achieved higher accuracies than the concept bottleneck
models without attention."
REFERENCES,0.8773946360153256,"A.8
THE RELATIONSHIP BETWEEN CONCEPTS AND THE CLASSIFICATION TASK"
REFERENCES,0.8812260536398467,"To understand the relationship between the extracted the concepts and the task classiﬁcation, we
compared the performance of the Concept Bottleneck models with a) MLP classiﬁer b)Linear
Classiﬁer as the ﬁnal classiﬁcation layers. The results showed that there was approximately 2% drop
in classiﬁcation performance when using a Linear Classiﬁer instead of an MLP. This indicates that,
the classiﬁcation task is not a simple linear combination of the extracted concepts and the composition
of concepts is important."
REFERENCES,0.8850574712643678,Under review as a conference paper at ICLR 2022
REFERENCES,0.8888888888888888,"Dataset
Feature
Extractor
Model Type
Task Classiﬁcation
Concepts"
REFERENCES,0.89272030651341,"Accuracy(%)
F1-score
AUC"
REFERENCES,0.896551724137931,MLB-V2E
REFERENCES,0.9003831417624522,"Resnet 50V2
Standard
67.92 ± 0.78
0.68 ± 0.003
-
Bottleneck
67.83 ± 0.74
0.68 ± 0.001
0.85 ± 0.005
Bottleneck + Attn.
67.96 ± 0.65
0.68 ± 0.002
0.88 ± 0.002"
REFERENCES,0.9042145593869731,"Resnet 101V2
Standard
68.18 ± 0.88
0.68 ± 0.005
-
Bottleneck
68.01 ± 1.02
0.68 ± 0.013
0.85 ± 0.004
Bottleneck + Attn.
68.26 ± 1.12
0.68 ± 0.009
0.88 ± 0.000"
REFERENCES,0.9080459770114943,"Inception V3
Standard
68.46 ± 1.27
0.68 ± 0.011
-
Bottleneck
68.16 ± 1.12
0.68 ± 0.004
0.85 ± 0.003
Bottleneck + Attn.
68.38 ± 1.34
0.68 ± 0.004
0.88 ± 0.001"
REFERENCES,0.9118773946360154,MSR-V2E
REFERENCES,0.9157088122605364,"Resnet 50V2
Standard
61.52 ± 1.20
0.59 ± 0.008
-
Bottleneck
61.23 ± 1.71
0.59 ± 0.008
0.82 ± 0.009
Bottleneck + Attn
61.28 ± 1.54
0.59 ± 0.007
0.86 ± 0.004"
REFERENCES,0.9195402298850575,"Resnet 101V2
Standard
61.56 ± 1.31
0.60 ± 0.008
-
Bottleneck
61.38 ± 1.24
0.60 ± 0.008
0.83 ± 0.006
Bottleneck + Attn.
61.44 ± 1.32
0.60 ± 0.009
0.86 ± 0.003"
REFERENCES,0.9233716475095786,"Inception V3
Standard
61.79 ± 1.42
0.60 ± 0.012
-
Bottleneck
61.42 ± 1.18
0.60 ± 0.013
0.83 ± 0.006
Bottleneck + Attn.
61.68 ± 1.23
0.60 ± 0.009
0.86 ± 0.004"
REFERENCES,0.9272030651340997,Table 5: Performance of Models
REFERENCES,0.9310344827586207,"Dataset
Accuracy (%)
Difference
(%)
MLP
Linear Classiﬁer"
REFERENCES,0.9348659003831418,"MLB-V2E
68.38
66.94
-1.44
MSR-V2E
61.68
60.02
-1.66"
REFERENCES,0.9386973180076629,Table 6: Performance of MLP classiﬁer vs Linear Classiﬁer
REFERENCES,0.9425287356321839,Under review as a conference paper at ICLR 2022
REFERENCES,0.946360153256705,"Dataset
Task Accuracy(%)
Task F-1 score"
REFERENCES,0.9501915708812261,"MLB-V2E
75.68
0.7585
MSR-V2E
65.23
0.6422"
REFERENCES,0.9540229885057471,Table 7: Performance of MLP classiﬁer trained only on concepts (without videos)
REFERENCES,0.9578544061302682,"A.9
A CLASSIFIER MODEL TRAINED ONLY ON CONCEPTS"
REFERENCES,0.9616858237547893,"The MLP model trained on only the concepts (without using video information) achieves a higher
accuracy (7% for MLB-V2E and 4% for MSR-V2E) than the video classiﬁcation model. This result
indicates that the concepts extracted are meaningful for the classiﬁcation task and there’s still an
headroom available if the concept bottleneck network was able to predict these initial concepts better."
REFERENCES,0.9655172413793104,"A.10
MORE EXPLANATIONS FROM THE PREDICTION MODEL"
REFERENCES,0.9693486590038314,"Here are a few more examples of the prediction by the concept bottleneck model with attention.
Figure 8 shows examples from the MLB-V2E dataset and Figure 9 shows examples from the MSR-
V2E dataset. The example videos are provided in the supplementary materials."
REFERENCES,0.9731800766283525,"Figure 8: Examples of the model prediction and their corresponding concepts and their importance
scores for MLB-V2E dataset"
REFERENCES,0.9770114942528736,"A.11
MORE DETAILS ON THE DATASETS"
REFERENCES,0.9808429118773946,"Figure 10 shows the number of videos belonging to each category in the MLB-V2E and the MSR-
V2E datasets. Though the original MSR-VTT dataset had descriptions of videos, they were general
captions and didn’t explain any particular class. Since they didn’t have classiﬁcation labels and
text-based explanations corresponding to the labels for the videos, we collected the video labels and
natural language explanations by crowd-sourcing on Amazon Mechanical Turk. The explanations
obtained for these videos and the concepts extracted using CoDEx for both the datasets are provided
in the supplementary materials. Since the MSR-V2E dataset is imbalanced, we do some weighted
oversampling while training to ensure that the models learn to predict all the classes."
REFERENCES,0.9846743295019157,Under review as a conference paper at ICLR 2022
REFERENCES,0.9885057471264368,"Figure 9: Examples of the model prediction and their corresponding concepts concepts and their
importance scores for MSR-V2E dataset"
REFERENCES,0.9923371647509579,"(a) MLB-V2E Dataset
(b) MSR-V2E Dataset"
REFERENCES,0.9961685823754789,"Figure 10: The number of videos belonging to each category on (a) the MLB-V2E dataset and (b) the
MSR-V2E dataset"
