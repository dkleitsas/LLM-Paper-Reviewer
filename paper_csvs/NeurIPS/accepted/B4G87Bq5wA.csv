Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002053388090349076,"Constructing a similarity graph from a set X of data points in Rd is the first step of
many modern clustering algorithms. However, typical constructions of a similarity
graph have high time complexity, and a quadratic space dependency with respect
to |X|. We address this limitation and present a new algorithmic framework that
constructs a sparse approximation of the fully connected similarity graph while
preserving its cluster structure. Our presented algorithm is based on the kernel
density estimation problem, and is applicable for arbitrary kernel functions. We
compare our designed algorithm with the well-known implementations from the
scikit-learn library and the FAISS library, and find that our method significantly
outperforms the implementation from both libraries on a variety of datasets."
INTRODUCTION,0.004106776180698152,"1
Introduction"
INTRODUCTION,0.006160164271047228,"Given a set X = {x1, . . . , xn} ‚äÇRd of data points and a similarity function k : Rd √ó Rd ‚ÜíR‚â•0
for any pair of data points xi and xj, the objective of clustering is to partition these n data points into
clusters such that similar points are in the same cluster. As a fundamental data analysis technique,
clustering has been extensively studied in different disciplines ranging from algorithms and machine
learning, social network analysis, to data science and statistics."
INTRODUCTION,0.008213552361396304,"One prominent approach for clustering data points in Euclidean space consists of two simple steps:
the first step is to construct a similarity graph K = (V, E, w) from X, where every vertex vi of
G corresponds to xi ‚ààX, and different vertices vi and vj are connected by an edge with weight
w(vi, vj) if their similarity k(xi, xj) is positive, or higher than some threshold. Secondly, we apply
spectral clustering on G, and its output naturally corresponds to some clustering on X [19]. Because
of its out-performance over traditional clustering algorithms like k-means, this approach has become
one of the most popular modern clustering algorithms."
INTRODUCTION,0.01026694045174538,"On the other side, different constructions of similarity graphs have significant impact on the quality
and time complexity of spectral clustering, which is clearly acknowledged and appropriately discussed
by von Luxburg [31]. Generally speaking, there are two types of similarity graphs:"
INTRODUCTION,0.012320328542094456,"‚Ä¢ the first one is the k-nearest neighbour graph (k-NN graph), in which every vertex vi
connects to vj if vj is among the k-nearest neighbours of vi. A k-NN graph is sparse by
construction, but loses some of the structural information in the dataset since k is usually
small and the added edges are unweighted."
INTRODUCTION,0.014373716632443531,"‚Ä¢ the second one is the fully connected graph, in which different vertices vi and vj are
connected with weight w(vi, vj) = k(xi, xj). While a fully connected graph maintains
most of the distance information about X, this graph is dense and storing such graphs
requires quadratic memory in n."
INTRODUCTION,0.01642710472279261,"Taking the pros and cons of the two constructions into account, one would naturally ask the question:"
INTRODUCTION,0.018480492813141684,"Is it possible to directly construct a sparse graph that preserves the cluster structure of a fully
connected similarity graph?"
INTRODUCTION,0.02053388090349076,"We answer this question affirmatively, and present a fast algorithm that constructs an approximation
of the fully connected similarity graph. Our constructed graph consists of only eO(n) edges1, and
preserves the cluster structure of the fully connected similarity graph."
OUR RESULT,0.022587268993839837,"1.1
Our Result"
OUR RESULT,0.024640657084188913,"Given any set X = {x1, . . . , xn} ‚äÇRd and a kernel function k : Rd √óRd ‚ÜíR‚â•0, a fully connected
similarity graph K = (V, E, w) of X consists of n vertices, and every vi ‚ààV corresponds to xi ‚ààX;
we set w(vi, vj) ‚âúk(xi, xj) for any different vi and vj. We introduce an efficient algorithm that
constructs a sparse graph G directly from X, such that K and G share the same cluster-structure, and
the graph matrices for K and G have approximately the same eigen-gap. This ensures that spectral
clustering from G and K return approximately the same result."
OUR RESULT,0.026694045174537988,"The design of our algorithm is based on a novel reduction from the approximate construction of
similarity graphs to the problem of Kernel Density Estimation (KDE). This reduction shows that any
algorithm for the KDE can be employed to construct a sparse representation of a fully connected
similarity graph, while preserving the cluster-structure of the input data points. This is summarised
as follows:
Theorem 1 (Informal Statement of Theorem 2). Given a set of data points X = {x1, . . . , xn} ‚äÇRd
as input, there is a randomised algorithm that constructs a sparse graph G of X, such that it holds
with probability at least 9/10 that"
OUR RESULT,0.028747433264887063,"1. graph G has eO(n) edges,"
OUR RESULT,0.030800821355236138,2. graph G has the same cluster structure as the fully connected similarity graph K of X.
OUR RESULT,0.03285420944558522,"The algorithm uses an approximate KDE algorithm as a black-box, and has running time
eO(TKDE(n, n, œµ)) for œµ ‚â§1/(6 log(n)), where TKDE(n, n, œµ) is the running time of solving the
KDE problem for n data points up to a (1 + œµ)-approximation."
OUR RESULT,0.03490759753593429,"This result builds a novel connection between the KDE and the fast construction of similarity graphs,
and further represents a state-of-the-art algorithm for constructing similarity graphs. For instance,
when employing the fast Gauss transform [10] as the KDE solver, Theorem 1 shows that a sparse
representation of the fully connected similarity graph with the Gaussian kernel can be constructed
in eO(n) time when d is constant. As such, in the case of low dimensions, spectral clustering runs
as fast (up to a poly-logarithmic factor) as the time needed to read the input data points. Moreover,
any improved algorithm for the KDE would result in a faster construction of approximate similarity
graphs."
OUR RESULT,0.03696098562628337,"To demonstrate the significance of this work in practice, we compare the performance of our
algorithm with five competing algorithms from the well-known scikit-learn library [20] and
FAISS library [11]: the algorithm that constructs a fully connected Gaussian kernel graph, and
four algorithms that construct different variants of k-nearest neighbour graphs. We apply spectral
clustering on the six constructed similarity graphs, and compare the quality of the resulting clustering.
For a typical input dataset of 15,000 points in R2, our algorithm runs in 4.7 seconds, in comparison
with between 16.1 ‚Äì 128.9 seconds for the five competing algorithms from scikit-learn and
FAISS libraries. As shown in Figure 1, all the six algorithms return reasonable output."
OUR RESULT,0.039014373716632446,"We further compare the quality of the six algorithms on the BSDS image segmentation dataset [2],
and our algorithm presents clear improvement over the other five algorithms based on the output‚Äôs
average Rand Index. In particular, due to its quadratic memory requirement in the input size, one
would need to reduce the resolution of every image down to 20,000 pixels in order to construct the
fully connected similarity graph with scikit-learn on a typical laptop. In contrast, our algorithm
is able to segment the full-resolution image with over 150,000 pixels. Our experimental result on"
OUR RESULT,0.04106776180698152,1We use eO(n) to represent O (n ¬∑ logc(n)) for some constant c.
OUR RESULT,0.043121149897330596,Figure 1: Output of spectral clustering with different similarity graph constructions.
OUR RESULT,0.045174537987679675,"the BSDS dataset is showcased in Figure 2 and demonstrates that, in comparison with SKLEARN
GK, our algorithm identifies a more detailed pattern on the butterfly‚Äôs wing. In contrast, none of
the k-nearest neighbour based algorithms from the two libraries is able to identify the wings of the
butterfly."
OUR RESULT,0.04722792607802875,"(a) Original Image
(b) SKLEARN GK
(c) OUR ALGORITHM"
OUR RESULT,0.049281314168377825,"(d) SKLEARN k-NN
(e) FAISS EXACT
(f) FAISS HNSW
(g) FAISS IVF"
OUR RESULT,0.0513347022587269,"Figure 2: Comparison on the performance of spectral clustering with different similarity graph
constructions. Here, SKLEARN GK is based on the fully connected similarity graph construction,
and (d) ‚Äì (g) are based on different k-nearest neighbour graph constructions from the two libraries."
RELATED WORK,0.053388090349075976,"1.2
Related Work"
RELATED WORK,0.055441478439425054,"There are a number of works on efficient constructions of Œµ-neighbourhood graphs and k-NN graphs.
For instance, Dong et al. [9] presents an algorithm for approximate k-NN graph construction, and
their algorithm is based on local search. Wang et al. [32] presents an LSH-based algorithm for
constructing an approximate k-NN graph, and employs several sampling and hashing techniques to
reduce the computational and parallelisation cost. These two algorithms [9, 32] have shown to work
very well in practice, but lack a theoretical guarantee on the performance."
RELATED WORK,0.057494866529774126,"Our work also relates to a large and growing number of KDE algorithms. Charikar and Siminelakis [7]
study the KDE problem through LSH, and present a class of unbiased estimators for kernel density in
high dimensions for a variety of commonly used kernels. Their work has been improved through
the sketching technique [24], and a revised description and analysis of the original algorithm [4].
Charikar et al. [6] presents a data structure for the KDE problem, and their result essentially matches
the query time and space complexity for most studied kernels in the literature. In addition, there are
studies on designing efficient KDE algorithms based on interpolation of kernel density estimators [29],
and coresets [12]."
RELATED WORK,0.059548254620123205,"Our work further relates to efficient constructions of spectral sparsifiers for kernel graphs. Quan-
rud [22] studies smooth kernel functions, and shows that an explicit (1 + Œµ)-approximate spectral
approximation of the geometric graph with eO(n/Œµ2) edges can be computed in eO(n/Œµ2) time. Bakshi
et al. [5] proves that, under the strong exponential time hypothesis, constructing an O(1)-approximate
spectral sparsifier with O(n2‚àíŒ¥) edges for the Gaussian kernel graph requires ‚Ñ¶

n ¬∑ 2log(1/œÑ)0.32"
RELATED WORK,0.061601642710472276,"time, where Œ¥ < 0.01 is a fixed universal constant and œÑ is the minimum entry of the kernel matrix.
Compared with their results, we show that, when the similarity graph with the Gaussian kernel
presents a well-defined structure of clusters, an approximate construction of this similarity graph can
be constructed in nearly-linear time."
PRELIMINARIES,0.06365503080082136,"2
Preliminaries"
PRELIMINARIES,0.06570841889117043,"Let G = (V, E, wG) be an undirected graph with weight function wG : E ‚ÜíR‚â•0, and n ‚âú|V |.
The degree of any vertex v is defined as degG(v) ‚âúP"
PRELIMINARIES,0.06776180698151951,"u‚àºv wG(u, v), where we write u ‚àºv if
{u, v} ‚ààE(G). For any S ‚äÇV , the volume of S is defined by volG(S) ‚âúP"
PRELIMINARIES,0.06981519507186858,"v‚ààS degG(v), and the
conductance of S is defined by"
PRELIMINARIES,0.07186858316221766,œïG(S) ‚âú‚àÇG(S)
PRELIMINARIES,0.07392197125256673,"volG(S),"
PRELIMINARIES,0.07597535934291581,where ‚àÇG(S) ‚âúP
PRELIMINARIES,0.07802874743326489,"u‚ààS,vÃ∏‚ààS wG(u, v). For any k ‚â•2, we call subsets of vertices A1, . . . , Ak a k-way"
PRELIMINARIES,0.08008213552361396,"partition if Ai Ã∏= ‚àÖfor any 1 ‚â§i ‚â§k, Ai ‚à©Aj = ‚àÖfor any i Ã∏= j, and Sk
i=1 Ai = V . Moreover, we
define the k-way expansion constant by"
PRELIMINARIES,0.08213552361396304,"œÅG(k) ‚âú
min
partition A1,...,Ak max
1‚â§i‚â§k œïG(Ai)."
PRELIMINARIES,0.08418891170431211,"Note that a lower value of œÅG(k) ensures the existence of k clusters A1, . . . , Ak of low conductance,
i.e, G has at least k clusters."
PRELIMINARIES,0.08624229979466119,"For any undirected graph G, the adjacency matrix AG of G is defined by AG(u, v) = wG(u, v) if
u ‚àºv, and AG(u, v) = 0 otherwise. We write DG as the diagonal matrix defined by DG(v, v) =
degG(v), and the normalised Laplacian of G is defined by NG ‚âúI‚àíD‚àí1/2
G
AGD‚àí1/2
G
. For any PSD
matrix B ‚ààRn√ón, we write the eigenvalues of B as Œª1(B) ‚â§. . . ‚â§Œªn(B)."
PRELIMINARIES,0.08829568788501027,"It is well-known that, while computing œÅG(k) exactly is NP-hard, œÅG(k) is closely related to Œªk
through the higher-order Cheeger inequality [13]: it holds for any k that"
PRELIMINARIES,0.09034907597535935,"Œªk(NG)/2 ‚â§œÅG(k) ‚â§O(k3)
p"
PRELIMINARIES,0.09240246406570841,Œªk(NG).
FULLY CONNECTED SIMILARITY GRAPHS,0.0944558521560575,"2.1
Fully Connected Similarity Graphs"
FULLY CONNECTED SIMILARITY GRAPHS,0.09650924024640657,"We use X ‚âú{x1, . . . xn} to represent the set of input data points, where every xi ‚ààRd. Given X and
some kernel function k : Rd√óRd ‚ÜíR‚â•0, we use K = (VK, EK, wK) to represent the fully connected
similarity graph from X, which is constructed as follows: every vi ‚ààVK corresponds to xi ‚ààX, and
any pair of different vi and vj is connected by an edge with weight wK(vi, vj) = k(xi, xj). One of
the most common kernels used for clustering is the Gaussian kernel, which is defined by"
FULLY CONNECTED SIMILARITY GRAPHS,0.09856262833675565,"k(xi, xj) = exp

‚àí‚à•xi ‚àíxj‚à•2
2
œÉ2 "
FULLY CONNECTED SIMILARITY GRAPHS,0.10061601642710473,"for some bandwidth parameter œÉ. Other popular kernels include the Laplacian kernel and the
exponential kernel which use ‚à•xi ‚àíxj‚à•1 and ‚à•xi ‚àíxj‚à•2 in the exponent respectively."
KERNEL DENSITY ESTIMATION,0.1026694045174538,"2.2
Kernel Density Estimation"
KERNEL DENSITY ESTIMATION,0.10472279260780287,"Our work is based on algorithms for kernel density estimation (KDE), which is defined as follows.
Given a kernel function k : Rd √ó Rd ‚ÜíR‚â•0 with n source points x1, . . . , xn ‚ààRd and m target
points y1, . . . , ym ‚ààRd, the KDE problem is to compute g[1,n](y1), . . . g[1,n](ym), where"
KERNEL DENSITY ESTIMATION,0.10677618069815195,"g[a,b](yi) ‚âú b
X"
KERNEL DENSITY ESTIMATION,0.10882956878850103,"j=a
k(yi, xj)
(1)"
KERNEL DENSITY ESTIMATION,0.11088295687885011,"for 1 ‚â§i ‚â§m. While a direct computation of the m values g[1,n](y1), . . . g[1,n](ym) requires mn
operations, there is substantial research to develop faster algorithms approximating these m quantities."
KERNEL DENSITY ESTIMATION,0.11293634496919917,"In this paper we are interested in the algorithms that approximately compute g[1,n](yi) for all
1 ‚â§i ‚â§m up to a (1 ¬± œµ)-multiplicative error, and use TKDE(m, n, œµ) to denote the asymptotic
complexity of such a KDE algorithm. We also require that TKDE(m, n, œµ) is superadditive in m and
n; that is, for m = m1 + m2 and n = n1 + n2, we have"
KERNEL DENSITY ESTIMATION,0.11498973305954825,"TKDE(m1, n1, œµ) + TKDE(m2, n2, œµ) ‚â§TKDE(m, n, œµ);"
KERNEL DENSITY ESTIMATION,0.11704312114989733,"it is known that such property holds for many KDE algorithms (e.g., [1, 6, 10])."
CLUSTER-PRESERVING SPARSIFIERS,0.11909650924024641,"3
Cluster-Preserving Sparsifiers"
CLUSTER-PRESERVING SPARSIFIERS,0.12114989733059549,"A graph sparsifier is a sparse representation of an input graph that inherits certain properties of the
original dense graph. The efficient construction of sparsifiers plays an important role in designing a
number of nearly-linear time graph algorithms. However, most algorithms for constructing sparsifiers
rely on the recursive decomposition of an input graph [26], sampling with respect to effective
resistances [15, 25], or fast SDP solvers [14]; all of these need the explicit representation of an input
graph, requiring ‚Ñ¶(n2) time and space complexity for a fully connected graph."
CLUSTER-PRESERVING SPARSIFIERS,0.12320328542094455,"Sun and Zanetti [27] study a variant of graph sparsifiers that mainly preserve the cluster structure of
an input graph, and introduce the notion of cluster-preserving sparsifier defined as follows:
Definition 1 (Cluster-preserving sparsifier). Let K = (V, E, wK) be any graph, and {Ai}k
i=1 the
k-way partition of K corresponding to œÅK(k). We call a re-weighted subgraph G = (V, F ‚äÇE, wG)
a cluster-preserving sparsifier of K if œïG(Ai) = O(k ¬∑ œïK(Ai)) for 1 ‚â§i ‚â§k, and Œªk+1(NG) =
‚Ñ¶(Œªk+1(NK))."
CLUSTER-PRESERVING SPARSIFIERS,0.12525667351129363,"Notice that graph K has exactly k clusters if (i) K has k disjoint subsets A1, . . . , Ak of low conduc-
tance, and (ii) any (k + 1)-way partition of K would include some A ‚äÇV of high conductance,
which would be implied by a lower bound on Œªk+1(NK) due to the higher-order Cheeger inequality.
Together with the well-known eigen-gap heuristic [13, 31] and theoretical analysis on spectral cluster-
ing [16, 21], the two properties in Definition 1 ensures that spectral clustering returns approximately
the same output from K and H.2"
CLUSTER-PRESERVING SPARSIFIERS,0.1273100616016427,"Now we present the algorithm in [27] for constructing a cluster-preserving sparsifier, and we call it
the SZ algorithm for simplicity. Given any input graph K = (V, E, wK) with weight function wK, the
algorithm computes"
CLUSTER-PRESERVING SPARSIFIERS,0.1293634496919918,"pu(v) ‚âúmin

C ¬∑ log n"
CLUSTER-PRESERVING SPARSIFIERS,0.13141683778234087,"Œªk+1
¬∑ wK(u, v)"
CLUSTER-PRESERVING SPARSIFIERS,0.13347022587268995,"degK(u) , 1

,
and
pv(u) ‚âúmin

C ¬∑ log n"
CLUSTER-PRESERVING SPARSIFIERS,0.13552361396303902,"Œªk+1
¬∑ wK(v, u)"
CLUSTER-PRESERVING SPARSIFIERS,0.1375770020533881,"degK(v) , 1

,"
CLUSTER-PRESERVING SPARSIFIERS,0.13963039014373715,"for every edge e = {u, v}, where C ‚ààR+ is some constant. Then, the algorithm samples every edge
e = {u, v} with probability"
CLUSTER-PRESERVING SPARSIFIERS,0.14168377823408623,"pe ‚âúpu(v) + pv(u) ‚àípu(v) ¬∑ pv(u),"
CLUSTER-PRESERVING SPARSIFIERS,0.1437371663244353,"and sets the weight of every sampled e = {u, v} in G as wG(u, v) ‚âúwK(u, v)/pe. By setting F as
the set of the sampled edges, the algorithm returns G = (V, F, wG) as output. It is shown in [27] that,
with high probability, the constructed G has eO(n) edges and is a cluster-preserving sparsifier of K."
CLUSTER-PRESERVING SPARSIFIERS,0.1457905544147844,"2The most interesting regime for this definition is k = eO(1) and Œªk+1(NK) = ‚Ñ¶(1), and we assume this in
the rest of the paper."
CLUSTER-PRESERVING SPARSIFIERS,0.14784394250513347,"We remark that a cluster-preserving sparsifier is a much weaker notion than a spectral sparsifier, which
approximately preserves all the cut values and the eigenvalues of the graph Laplacian matrices. On
the other side, while a cluster-preserving sparsifier is sufficient for the task of graph clustering, the SZ
algorithm runs in ‚Ñ¶(n2) time for a fully connected input graph, since it‚Äôs based on the computation
of the vertex degrees as well as the sampling probabilities pu(v) for every pair of vertices u and v."
ALGORITHM,0.14989733059548255,"4
Algorithm"
ALGORITHM,0.15195071868583163,"This section presents our algorithm that directly constructs an approximation of a fully connected
similarity graph from X ‚äÜRd with |X| = n. As the main theoretical contribution, we demonstrate
that neither the quadratic space complexity for directly constructing a fully connected similarity graph
nor the quadratic time complexity of the SZ algorithm is necessary when approximately constructing
a fully connected similarity graph for the purpose of clustering. The performance of our algorithm is
as follows:
Theorem 2 (Main Result). Given a set of data points X = {x1, . . . , xn} ‚äÇRd as input, there is a
randomised algorithm that constructs a sparse graph G of X, such that it holds with probability at
least 9/10 that"
ALGORITHM,0.1540041067761807,"1. graph G has eO(n) edges,"
ALGORITHM,0.15605749486652978,"2. graph G has the same cluster structure as the fully connected similarity graph K of X;
that is, if K has k well-defined clusters, then it holds that œÅG(k) = O(k ¬∑ œÅK(k)) and
Œªk+1(NG) = ‚Ñ¶(Œªk+1(NK))."
ALGORITHM,0.15811088295687886,"The algorithm uses an approximate KDE algorithm as a black-box, and has running time
eO(TKDE(n, n, œµ)) for œµ ‚â§1/(6 log(n))."
ALGORITHM DESCRIPTION,0.1601642710472279,"4.1
Algorithm Description"
ALGORITHM DESCRIPTION,0.162217659137577,"At a very high level, our designed algorithm applies a KDE algorithm as a black-box, and constructs a
cluster-preserving sparsifier by simultaneous sampling of the edges from a non-explicitly constructed
fully connected graph. To explain our technique, we first claim that, for an arbitrary xi, a random xj
can be sampled with probability k(xi, xj)/ degK(vi) through O(log n) queries to a KDE algorithm.
To see this, notice that we can apply a KDE algorithm to compute the probability that the sampled
neighbour is in some set X1 ‚äÇX, i.e.,"
ALGORITHM DESCRIPTION,0.16427104722792607,"P [z ‚ààX1] =
X xj‚ààX1"
ALGORITHM DESCRIPTION,0.16632443531827515,"k(xi, xj)
degK(vi) = gX1(xi)"
ALGORITHM DESCRIPTION,0.16837782340862423,"gX(xi) ,"
ALGORITHM DESCRIPTION,0.1704312114989733,"where we use gX(yi) to denote that the KDE is taken with respect to the set of source points X.
Based on this, we recursively split the set of possible neighbours in half and choose between the
two subsets with the correct probability. The sampling procedure is summarised as follows, and is
illustrated in Figure 3. We remark that the method of sampling a random neighbour of a vertex in K
through KDE and binary search also appears in Backurs et al. [5]"
ALGORITHM DESCRIPTION,0.17248459958932238,"1. Set the feasible neighbours to be X = {x1, . . . , xn}.
2. While |X| > 1:"
ALGORITHM DESCRIPTION,0.17453798767967146,"‚Ä¢ Split X into X1 and X2 with |X1| = ‚åä|X|/2‚åãand |X2| = ‚åà|X|/2‚åâ.
‚Ä¢ Compute gX(xi) and gX1(xi); set X ‚ÜêX1 with probability gX1(xi)/gX(xi), and
X ‚ÜêX2 with probability 1 ‚àígX1(xi)/gX(xi).
3. Return the remaining element in X as the sampled neighbour."
ALGORITHM DESCRIPTION,0.17659137577002054,"Next we generalise this idea and show that, instead of sampling a neighbour of one vertex at a time, a
KDE algorithm allows us to sample neighbours of every vertex in the graph ‚Äúsimultaneously‚Äù. Our
designed sampling procedure is formalised in Algorithm 1."
ALGORITHM DESCRIPTION,0.17864476386036962,"Finally, to construct a cluster-preserving sparsifier, we apply Algorithm 1 to sample O(log n) neigh-
bours for every vertex vi, and set the weight of every sampled edge vi ‚àºvj as"
ALGORITHM DESCRIPTION,0.1806981519507187,"wG(vi, vj) = k(xi, xj)"
ALGORITHM DESCRIPTION,0.18275154004106775,"bp(i, j) ,
(2)"
ALGORITHM DESCRIPTION,0.18480492813141683,Step 1
ALGORITHM DESCRIPTION,0.1868583162217659,"r ‚àºUnif[0, 1]"
ALGORITHM DESCRIPTION,0.188911704312115,"|
{z
}
g[0,n/2](xi)"
ALGORITHM DESCRIPTION,0.19096509240246407,"|
{z
}
g[n/2,n](xi)"
ALGORITHM DESCRIPTION,0.19301848049281314,Step 2
ALGORITHM DESCRIPTION,0.19507186858316222,"|
{z
}
g[0,n/4](xi)"
ALGORITHM DESCRIPTION,0.1971252566735113,"|
{z
}
g[n/4,n/2](xi) ..."
ALGORITHM DESCRIPTION,0.19917864476386038,Step log2(n)
ALGORITHM DESCRIPTION,0.20123203285420946,"|
{z
}
k(xi, xj)"
ALGORITHM DESCRIPTION,0.2032854209445585,"|
{z
}
k(xi, xj+1)"
ALGORITHM DESCRIPTION,0.2053388090349076,"Figure 3: The procedure of sampling a neighbour vj of vi with probability k(xi, xj)/ degK(vi). Our
algorithm performs a binary search to find the sampled neighbour. At each step, the value of two
kernel density estimates are used to determine where the sample lies. Notice that the algorithm
doesn‚Äôt compute any edge weights directly until the last step."
ALGORITHM DESCRIPTION,0.20739219712525667,Algorithm 1 SAMPLE
ALGORITHM DESCRIPTION,0.20944558521560575,"1: Input: set S of {yi}
set X of {xi}
2: Output:
E = {(yi, xj) for some i and j}
3: if |X| = 1 then
4:
return S √ó X
5: else
6:
X1 = {xj : j < |X| /2}
7:
X2 = {xj : j ‚â•|X| /2}
8:
Compute gX1(yi) for all i with a KDE al-
gorithm
9:
Compute gX2(yi) for all i with a KDE al-
gorithm"
ALGORITHM DESCRIPTION,0.21149897330595482,"10:
S1 = S2 = ‚àÖ
11:
for yi ‚ààS do
12:
r ‚àºUnif[0, 1]
13:
if r ‚â§gX1(yi)/(gX1(yi) + gX2(yi))
then
14:
S1 = S1 ‚à™{yi}
15:
else
16:
S2 = S2 ‚à™{yi}
17:
end if
18:
end for
19:
return SAMPLE(S1, X1)‚à™SAMPLE(S2, X2)
20: end if"
ALGORITHM DESCRIPTION,0.2135523613963039,"where bp(i, j) ‚âúbpi(j) + bpj(i) ‚àíbpi(j) ¬∑ bpj(i) is an estimate of the sampling probability of edge
vi ‚àºvj, and"
ALGORITHM DESCRIPTION,0.21560574948665298,"bpi(j) ‚âúmin

6C ¬∑ log n ¬∑ k(xi, xj)"
ALGORITHM DESCRIPTION,0.21765913757700206,"g[1,n](xi), 1
"
ALGORITHM DESCRIPTION,0.21971252566735114,for some constant C ‚ààR+; see Algorithm 2 for the formal description.
ALGORITHM ANALYSIS,0.22176591375770022,"4.2
Algorithm Analysis"
ALGORITHM ANALYSIS,0.22381930184804927,"(S0, X0)"
ALGORITHM ANALYSIS,0.22587268993839835,"(S1,1, X1,1)
(S1,2, X1,2)"
ALGORITHM ANALYSIS,0.22792607802874743,"(S2,1, X2,1)"
ALGORITHM ANALYSIS,0.2299794661190965,Figure 4: The recursion tree for Algorithm 1.
ALGORITHM ANALYSIS,0.23203285420944558,"Now we analyse Algorithm 2, and sketch the proof
of Theorem 2. We first analyse the running time of
Algorithm 2. Since it involves O(log n) executions
of Algorithm 1 in total, it is sufficient to examine the
running time of Algorithm 1."
ALGORITHM ANALYSIS,0.23408624229979466,"We visualise the recursion of Algorithm 1 with re-
spect to S and X in Figure 4. Notice that, although
the number of nodes doubles at each level of the
recursion tree, the total number of samples S and
data points X remain constant for each level of the
tree: it holds for any i that S2i"
ALGORITHM ANALYSIS,0.23613963039014374,"j=1 Si,j = S0 and"
ALGORITHM ANALYSIS,0.23819301848049282,Algorithm 2 FASTSIMILARITYGRAPH
ALGORITHM ANALYSIS,0.2402464065708419,"1: Input: data point set X = {x1, . . . , xn}
2: Output: similarity graph G
3: E = ‚àÖ, L = 6C ¬∑ log(n)/Œªk+1
4: for ‚Ñì‚àà[1, L] do
5:
E = E ‚à™SAMPLE(X, X)
6: end for
7: Compute g[1,n](xi) for each i with a KDE
algorithm"
ALGORITHM ANALYSIS,0.24229979466119098,"8: for (vi, vj) ‚ààE do
9:
bpi(j) = min

L ¬∑ k(xi, xj)/g[1,n](xi), 1"
ALGORITHM ANALYSIS,0.24435318275154005,"10:
bpj(i) = min

L ¬∑ k(xi, xj)/g[1,n](xj), 1"
ALGORITHM ANALYSIS,0.2464065708418891,"11:
bp(i, j) = bpi(j) + bpj(i) ‚àíbpi(j) ¬∑ bpj(i)
12:
Set wG(vi, vj) = k(xi, xj)/bp(i, j)
13: end for
14: return graph G = (X, E, wG) S2i"
ALGORITHM ANALYSIS,0.24845995893223818,"j=1 Xi,j = X0. Since the running time of the KDE algorithm is superadditive, the combined
running time of all nodes at level i of the tree is Ti ="
"I
X",0.25051334702258726,"2i
X"
"I
X",0.25256673511293637,"j=1
TKDE(|Si,j|, |Xi,j|, œµ) ‚â§TKDE Ô£´"
"I
X",0.2546201232032854,"Ô£≠
2i
X"
"I
X",0.25667351129363447,"j=1
|Si,j|,"
"I
X",0.2587268993839836,"2i
X"
"I
X",0.26078028747433263,"j=1
|Xi,j|, œµ Ô£∂"
"I
X",0.26283367556468173,"Ô£∏= TKDE(n, n, œµ)."
"I
X",0.2648870636550308,"Hence, the total running time of Algorithm 2 is eO(TKDE(n, n, œµ)) as the tree has log2(n) levels.
Moreover, when applying the Fast Gauss Transform (FGT) [10] as the KDE algorithm, the total
running time of Algorithm 1 is eO(n) when d = O(1)."
"I
X",0.2669404517453799,"Finally, we prove that the output of Algorithm 2 is a cluster preserving sparsifier of a fully connected
similarity graph, and has eO(n) edges. Notice that, comparing with the sampling probabilities pu(v)
and pv(u) used in the SZ algorithm, Algorithm 2 samples each edge through O(log n) recursive
executions of a KDE algorithm, each of which introduces a (1 + œµ)-multiplicative error. We carefully
examine these (1 + œµ)-multiplicative errors and prove that the actual sampling probability ep(i, j)
used in Algorithm 2 is an O(1)-approximation of pe for every e = {vi, vj}. As such the output of
Algorithm 2 is a cluster-preserving sparsifier of a fully connected similarity graph, and satisfies the
two properties of Theorem 2. The total number of edges in G follows from the sampling scheme. We
refer the reader to Appendix A for the complete proof of Theorem 2."
EXPERIMENTS,0.26899383983572894,"5
Experiments"
EXPERIMENTS,0.27104722792607805,"In this section, we empirically evaluate the performance of spectral clustering with our new algorithm
for constructing similarity graphs. We compare our algorithm with the algorithms for similarity
graph construction provided by the scikit-learn library [20] and the FAISS library [11] which are
commonly used machine learning libraries for Python. In the remainder of this section, we compare
the following six spectral clustering methods."
EXPERIMENTS,0.2731006160164271,"1. SKLEARN GK: spectral clustering with the fully connected Gaussian kernel similarity
graph constructed with the scikit-learn library.
2. SKLEARN k-NN: spectral clustering with the k-nearest neighbour similarity graph con-
structed with the scikit-learn library.
3. FAISS EXACT: spectral clustering with the exact k-nearest neighbour graph constructed
with the FAISS library.
4. FAISS HNSW: spectral clustering with the approximate k-nearest neighbour graph con-
structed with the ‚ÄúHierarchical Navigable Small World‚Äù method [18].
5. FAISS IVF: spectral clustering with the approximate k-nearest neighbour graph constructed
with the ‚ÄúInvertex File Index‚Äù method [3].
6. OUR ALGORITHM: spectral clustering with the Gaussian kernel similarity graph constructed
by Algorithm 2."
EXPERIMENTS,0.2751540041067762,"We implement Algorithm 2 in C++, using the implementation of the Fast Gauss Transform provided by
Yang et al. [33], and use the Python SciPy [30] and stag [17] libraries for eigenvector computation
and graph operations respectively. The scikit-learn and FAISS libraries are well-optimised"
EXPERIMENTS,0.27720739219712526,"and use C, C++, and FORTRAN for efficient implementation of core algorithms. We first employ
classical synthetic clustering datasets to clearly compare how the running time of different algorithms
is affected by the number of data points. This experiment highlights the nearly-linear time complexity
of our algorithm. Next we demonstrate the applicability of our new algorithm for image segmentation
on the Berkeley Image Segmentation Dataset (BSDS) [2]."
EXPERIMENTS,0.2792607802874743,"For each experiment, we set k = 10 for the approximate nearest neighbour algorithms. For the
synthetic datasets, we set the œÉ value of the Gaussian kernel used by SKLEARN GK and OUR
ALGORITHM to be 0.1, and for the BSDS experiment we set œÉ = 0.2. This choice follows the
heuristic suggested by von Luxburg [31] to choose œÉ to approximate the distance from a point to
its kth nearest neighbour. All experiments are performed on an HP ZBook laptop with an 11th Gen
Intel(R) Core(TM) i7-11800H @ 2.30GHz processor and 32 GB RAM. The code to reproduce our
results is available at https://github.com/pmacg/kde-similarity-graph."
SYNTHETIC DATASET,0.2813141683778234,"5.1
Synthetic Dataset"
SYNTHETIC DATASET,0.28336755646817247,"In this experiment we use the scikit-learn library to generate 15,000 data points in R2 from a
variety of classical synthetic clustering datasets. The data is generated with the make_circles,
make_moons, and make_blobs methods of the scikit-learn library with a noise parameter of
0.05. The linear transformation {{0.6, ‚àí0.6}, {‚àí0.4, 0.8}} is applied to the blobs data to create
asymmetric clusters. As shown in Figure 1, all of the algorithms return approximately the same
clustering, but our algorithm runs much faster than the ones from scikit-learn and FAISS."
SYNTHETIC DATASET,0.28542094455852157,"We further compare the speedup of our algorithm against the five competitors on the two_moons
dataset with an increasing number of data points, and our result is reported in Figure 5. Notice that
the running time of the scikit-learn and FAISS algorithms scales roughly quadratically with the
size of the input, while the running time of our algorithm is nearly linear. Furthermore, we note that
on a laptop with 32 GB RAM, the SKLEARN GK algorithm could not scale beyond 20,000 data
points due to its quadratic memory requirement, while our new algorithm can cluster 1,000,000 data
points in 240 seconds under the same memory constraint."
SYNTHETIC DATASET,0.2874743326488706,"0
5,000
10,000
15,000
20,000
Number of data points 0 10 20 30 40 50 60 70 80"
SYNTHETIC DATASET,0.28952772073921973,Running time (s)
SYNTHETIC DATASET,0.2915811088295688,"0
50,000
100,000
Number of data points 0 10 20 30 40 50 60 70 80"
SYNTHETIC DATASET,0.2936344969199179,Running time (s)
SYNTHETIC DATASET,0.29568788501026694,"SKLEARN k-NN
SKLEARN GK
FAISS EXACT
FAISS HNSW
FAISS IVF
OUR ALGORITHM"
SYNTHETIC DATASET,0.29774127310061604,"Figure 5: Comparison of the running time of spectral clustering on the two moons dataset. In every
case, all algorithms return the correct clusters."
BSDS IMAGE SEGMENTATION DATASET,0.2997946611909651,"5.2
BSDS Image Segmentation Dataset"
BSDS IMAGE SEGMENTATION DATASET,0.30184804928131415,"Finally we study the application of spectral clustering for image segmentation on the BSDS dataset.
The dataset contains 500 images with several ground-truth segmentations for each image. Given an
input image, we consider each pixel to be a point (r, g, b, x, y)‚ä∫‚ààR5 where r, g, b are the red, green,
blue pixel values and x, y are the coordinates of the pixel in the image. Then, we construct a similarity
graph based on these points, and apply spectral clustering, for which we set the number of clusters
to be the median number of clusters in the provided ground truth segmentations. Our experimental
result is reported in Table 1, where the ‚ÄúDownsampled‚Äù version is employed to reduce the resolution
of the image to 20,000 pixels in order to run the SKLEARN GK and the ‚ÄúFull Resolution‚Äù one is to
apply the original image of over 150,000 pixels as input. This set of experiments demonstrates our
algorithm produces better clustering results with repsect to the average Rand Index [23]."
BSDS IMAGE SEGMENTATION DATASET,0.30390143737166325,"Table 1: The average Rand Index of the output from the six algorithms. Due to the quadratic space
complexity constraint, the SKLEARN GK cannot be applied to the full resolution image."
BSDS IMAGE SEGMENTATION DATASET,0.3059548254620123,"Algorithm
Downsampled
Full Resolution"
BSDS IMAGE SEGMENTATION DATASET,0.3080082135523614,"SKLEARN GK
0.681
-
SKLEARN k-NN
0.675
0.682
FAISS EXACT
0.675
0.680
FAISS HNSW
0.679
0.677
FAISS IVF
0.675
0.680
OUR ALGORITHM
0.680
0.702"
CONCLUSION,0.31006160164271046,"6
Conclusion"
CONCLUSION,0.31211498973305957,"In this paper we develop a new technique that constructs a similarity graph, and show that an
approximation algorithm for the KDE can be employed to construct a similarity graph with proven
approximation guarantee. Applying the publicly available implementations of the KDE as a black-
box, our algorithm outperforms five competing ones from the well-known scikit-learn and FAISS
libraries for the classical datasets of low dimensions. We believe that our work will motivate more
research on faster algorithms for the KDE in higher dimensions and their efficient implementation, as
well as more efficient constructions of similarity graphs."
CONCLUSION,0.3141683778234086,Acknowledgements
CONCLUSION,0.3162217659137577,"We would like to thank the anonymous reviewers for their valuable comments on the paper. This
work is supported by an EPSRC Early Career Fellowship (EP/T00729X/1)."
REFERENCES,0.3182751540041068,References
REFERENCES,0.3203285420944558,"[1] Josh Alman, Timothy Chu, Aaron Schild, and Zhao Song. Algorithms and hardness for linear
algebra on geometric graphs. In 61st Annual IEEE Symposium on Foundations of Computer
Science (FOCS‚Äô20), pages 541‚Äì552, 2020."
REFERENCES,0.32238193018480493,"[2] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection
and hierarchical image segmentation. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 33(5):898‚Äì916, 2011."
REFERENCES,0.324435318275154,"[3] Artem Babenko and Victor Lempitsky. The inverted multi-index. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 37(6):1247‚Äì1260, 2014."
REFERENCES,0.3264887063655031,"[4] Arturs Backurs, Piotr Indyk, and Tal Wagner. Space and time efficient kernel density estimation
in high dimensions. In 33rd Advances in Neural Information Processing Systems (NeurIPS‚Äô19),
pages 15773‚Äì15782, 2019."
REFERENCES,0.32854209445585214,"[5] Ainesh Bakshi, Piotr Indyk, Praneeth Kacham, Sandeep Silwal, and Samson Zhou. Subquadratic
algorithms for kernel matrices via kernel density estimation. In 11th International Conference
on Learning Representations (ICLR ‚Äô23), 2023."
REFERENCES,0.33059548254620125,"[6] Moses Charikar, Michael Kapralov, Navid Nouri, and Paris Siminelakis.
Kernel density
estimation through density constrained near neighbor search. In 61st Annual IEEE Symposium
on Foundations of Computer Science (FOCS‚Äô20), pages 172‚Äì183, 2020."
REFERENCES,0.3326488706365503,"[7] Moses Charikar and Paris Siminelakis. Hashing-based-estimators for kernel density in high
dimensions. In 58th Annual IEEE Symposium on Foundations of Computer Science (FOCS‚Äô17),
pages 1032‚Äì1043, 2017."
REFERENCES,0.3347022587268994,"[8] Fan Chung and Linyuan Lu. Concentration inequalities and martingale inequalities: a survey.
Internet mathematics, 3(1):79‚Äì127, 2006."
REFERENCES,0.33675564681724846,"[9] Wei Dong, Moses Charikar, and Kai Li. Efficient k-nearest neighbor graph construction for
generic similarity measures. In 20th International Conference on World Wide Web (WWW ‚Äô11),
pages 577‚Äì586, 2011."
REFERENCES,0.33880903490759756,"[10] Leslie Greengard and John Strain. The fast gauss transform. SIAM Journal on Scientific &
Statistical Computing, 12(1):79‚Äì94, 1991."
REFERENCES,0.3408624229979466,"[11] Jeff Johnson, Matthijs Douze, and Herv√© J√©gou. Billion-scale similarity search with GPUs.
IEEE Transactions on Big Data, 7(3):535‚Äì547, 2021."
REFERENCES,0.34291581108829566,"[12] Zohar Karnin and Edo Liberty. Discrepancy, coresets, and sketches in machine learning. In
32nd Conference on Learning Theory (COLT ‚Äô19), pages 1975‚Äì1993, 2019."
REFERENCES,0.34496919917864477,"[13] James R. Lee, Shayan Oveis Gharan, and Luca Trevisan. Multiway spectral partitioning and
higher-order Cheeger inequalities. Journal of the ACM, 61(6):37:1‚Äì37:30, 2014."
REFERENCES,0.3470225872689938,"[14] Yin Tat Lee and He Sun. An SDP-based algorithm for linear-sized spectral sparsification. In
49th Annual ACM Symposium on Theory of Computing (STOC‚Äô17), pages 678‚Äì687, 2017."
REFERENCES,0.3490759753593429,"[15] Yin Tat Lee and He Sun. Constructing linear-sized spectral sparsification in almost-linear time.
SIAM Journal on Computing, 47(6):2315‚Äì2336, 2018."
REFERENCES,0.351129363449692,"[16] Peter Macgregor and He Sun. A tighter analysis of spectral clustering, and beyond. In 39th
International Conference on Machine Learning (ICML‚Äô22), pages 14717‚Äì14742, 2022."
REFERENCES,0.3531827515400411,"[17] Peter Macgregor and He Sun. Spectral toolkit of algorithms for graphs: Technical report (1).
CoRR, abs/2304.03170, 2023."
REFERENCES,0.35523613963039014,"[18] Yu A Malkov and Dmitry A Yashunin. Efficient and robust approximate nearest neighbor search
using hierarchical navigable small world graphs. IEEE transactions on Pattern Analysis and
Machine Intelligence, 42(4):824‚Äì836, 2018."
REFERENCES,0.35728952772073924,"[19] Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an
algorithm. In 14th Advances in Neural Information Processing Systems (NeurIPS‚Äô01), pages
849‚Äì856, 2001."
REFERENCES,0.3593429158110883,"[20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825‚Äì2830, 2011."
REFERENCES,0.3613963039014374,"[21] Richard Peng, He Sun, and Luca Zanetti. Partitioning well-clustered graphs: Spectral clustering
works! SIAM Journal on Computing, 46(2):710‚Äì743, 2017."
REFERENCES,0.36344969199178645,"[22] Kent Quanrud. Spectral sparsification of metrics and kernels. In 32nd Annual ACM-SIAM
Symposium on Discrete Algorithms (SODA‚Äô21), pages 1445‚Äì1464, 2021."
REFERENCES,0.3655030800821355,"[23] William M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the
American Statistical Association, 66(336):846‚Äì850, 1971."
REFERENCES,0.3675564681724846,"[24] Paris Siminelakis, Kexin Rong, Peter Bailis, Moses Charikar, and Philip Levis. Rehashing
kernel evaluation in high dimensions. In 36th International Conference on Machine Learning
(ICML‚Äô19), Proceedings of Machine Learning Research, pages 5789‚Äì5798, 2019."
REFERENCES,0.36960985626283366,"[25] Daniel A. Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. SIAM
Journal on Computing, 40(6):1913‚Äì1926, 2011."
REFERENCES,0.37166324435318276,"[26] Daniel A. Spielman and Shang-Hua Teng. Spectral sparsification of graphs. SIAM Journal on
Computing, 40(4):981‚Äì1025, 2011."
REFERENCES,0.3737166324435318,"[27] He Sun and Luca Zanetti. Distributed graph clustering and sparsification. ACM Transactions on
Parallel Computing, 6(3):17:1‚Äì17:23, 2019."
REFERENCES,0.3757700205338809,"[28] Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-
tional Mathematics, 12(4):389‚Äì434, 2012."
REFERENCES,0.37782340862423,"[29] Paxton Turner, Jingbo Liu, and Philippe Rigollet. Efficient interpolation of density estimators.
In 24th International Conference on Artificial Intelligence and Statistics (AISTATS ‚Äô21), pages
2503‚Äì2511, 2021."
REFERENCES,0.3798767967145791,"[30] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David
Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St√©fan J.
van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew
R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ÀôIlhan Polat, Yu Feng, Eric W.
Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A.
Quintero, Charles R. Harris, Anne M. Archibald, Ant√¥nio H. Ribeiro, Fabian Pedregosa, Paul"
REFERENCES,0.38193018480492813,"van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific
Computing in Python. Nature Methods, 17:261‚Äì272, 2020.
[31] Ulrike von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395‚Äì416,
2007.
[32] Yiqiu Wang, Anshumali Shrivastava, Jonathan Wang, and Junghee Ryu. Randomized algorithms
accelerated over CPU-GPU for ultra-high dimensional similarity search. In 2018 International
Conference on Management of Data (SIGMOD ‚Äô18), pages 889‚Äì903, 2018.
[33] Changjiang Yang, Ramani Duraiswami, Nail A. Gumerov, and Larry Davis. Improved fast
Gauss transform and efficient kernel density estimation. In 9th International Conference on
Computer Vision (ICCV‚Äô03), pages 664‚Äì671, 2003."
REFERENCES,0.3839835728952772,"A
Proof of Theorem 2"
REFERENCES,0.3860369609856263,"This section presents the complete proof of Theorem 2. Let yi,1, . . . , yi,L be random variables which
are equal to the indices of the L points sampled for xi. Recall that by the SZ algorithm, the ‚Äúideal‚Äù
sampling probability for xj from xi is"
REFERENCES,0.38809034907597534,"pi(j) ‚âúmin

C ¬∑ log(n)"
REFERENCES,0.39014373716632444,"Œªk+1
¬∑ k(xi, xj)"
REFERENCES,0.3921971252566735,"degK(vi), 1

."
REFERENCES,0.3942505133470226,We denote the actual sampling probability that xj is sampled from xi under Algorithm 2 to be
REFERENCES,0.39630390143737165,"epi(j) ‚âúP [xj ‚àà{yi,1, . . . yi,L}] ."
REFERENCES,0.39835728952772076,"Finally, for each added edge, Algorithm 2 also computes an estimate of pi(xj) which we denote"
REFERENCES,0.4004106776180698,"bpi(j) ‚âúmin

6C ¬∑ log(n)"
REFERENCES,0.4024640657084189,"Œªk+1
¬∑ k(xi, xj)"
REFERENCES,0.40451745379876797,"g[1,n](xi), 1

."
REFERENCES,0.406570841889117,"Similar with the definition of pe in Section 3, we define"
REFERENCES,0.4086242299794661,"‚Ä¢ p(i, j) = pi(j) + pj(i) ‚àípi(j) ¬∑ pj(i),"
REFERENCES,0.4106776180698152,"‚Ä¢ ep(i, j) = epi(j) + epj(i) ‚àíepi(j) ¬∑ epj(i), and"
REFERENCES,0.4127310061601643,"‚Ä¢ bp(i, j) = bpi(j) + bpj(i) ‚àíbpi(j) ¬∑ bpj(i)."
REFERENCES,0.41478439425051333,"Notice that, following the convention of [27], we use pi(j) to refer to the probability that a given edge
is sampled from the vertex xi and p(i, j) is the probability that the given edge {vi, vj} is sampled at
all by the algorithm. We use the same convention for epi(j) and bpi(j)."
REFERENCES,0.41683778234086244,"We first prove a sequence of lemmas showing that these probabilities are all within a constant factor
of each other."
REFERENCES,0.4188911704312115,"Lemma 1. For any point xi, the probability that a given sampled neighbour yi,l is equal to j is given
by
k(xi, xj)
2 degK(vi) ‚â§P [yi,l = j] ‚â§2k(xi, xj)"
REFERENCES,0.4209445585215606,degK(vi) .
REFERENCES,0.42299794661190965,"Proof. Let X = {x1, . . . , xn} be the input data points for Algorithm 2, and [n] = {1, . . . n} be the
indices of the input data points. Furthermore, let [a, b] = {a, . . . , b} be the set of indices between
a and b. Then, in each recursive call to Algorithm 1, we are given a range of indices [a, b] as input
and assign yi,l to one half of it: either [a, ‚åäb/2‚åã] or [‚åäb/a‚åã+ 1, b]. By Algorithm 1, we have that the
probability of assigning yi,l to [a, ‚åäb/2‚åã] is"
REFERENCES,0.42505133470225875,"P [yi,l ‚àà[a, ‚åäb/2‚åã] | yi,l ‚àà[a, b]] = g[a,‚åäb/2‚åã](xi)"
REFERENCES,0.4271047227926078,"g[a,b](xi)
."
REFERENCES,0.42915811088295686,"By the performance guarantee of the KDE algorithm, we have that g[a,b](xi) ‚àà(1 ¬± œµ) deg[a,b](vi),
where we define"
REFERENCES,0.43121149897330596,"deg[a,b](xi) ‚âú b
X"
REFERENCES,0.433264887063655,"j=a
k(xi, xj)."
REFERENCES,0.4353182751540041,"This gives
1 ‚àíœµ 1 + œµ"
REFERENCES,0.43737166324435317," deg[a,‚åäb/2‚åã](vi)"
REFERENCES,0.4394250513347023,"deg[a,b](vi)
‚â§P

yi,l ‚ààX[a,‚åäb/2‚åã] |yi,l ‚ààX[a,b]

‚â§
1 + œµ 1 ‚àíœµ"
REFERENCES,0.4414784394250513," deg[a,‚åäb/2‚åã](vi)"
REFERENCES,0.44353182751540043,"deg[a,b](vi)
. (3)"
REFERENCES,0.4455852156057495,"Next, notice that we can write for a sequence of intervals [a1, b1] ‚äÇ[a2, b2] ‚äÇ. . . ‚äÇ[1, n] that"
REFERENCES,0.44763860369609854,"P [yi,l = j] = P [yi,l = j|yi,l ‚àà[a1, b1]] √ó P [yi,l ‚àà[a1, b1]|yi,l ‚àà[a2, b2]]
√ó . . . √ó P [yi,l ‚àà[ak, bk]|yi,l ‚àà[1, n]] ,"
REFERENCES,0.44969199178644764,"where each term corresponds to one level of recursion of Algorithm 1 and there are at most ‚åàlog2(n)‚åâ
terms. Then, by (3) and the fact that the denominator and numerator of adjacent terms cancel out, we
have
1 ‚àíœµ 1 + œµ"
REFERENCES,0.4517453798767967,"‚åàlog2(n)‚åâk(xi, xj)"
REFERENCES,0.4537987679671458,"degK(vi) ‚â§P [yi,l = j] ‚â§
1 + œµ 1 ‚àíœµ"
REFERENCES,0.45585215605749485,"‚åàlog2(n)‚åâk(xi, xj)"
REFERENCES,0.45790554414784396,"degK(vi)
since deg[j,j](vi) = k(xi, xj) and deg[1,n](vi) = degK(vi)."
REFERENCES,0.459958932238193,"For the lower bound, we have that
1 ‚àíœµ 1 + œµ"
REFERENCES,0.4620123203285421,"‚åàlog2(n)‚åâ
‚â•(1 ‚àí2œµ)‚åàlog2(n)‚åâ‚â•1 ‚àí3 log2(n)œµ ‚â•1/2,"
REFERENCES,0.46406570841889117,where the final inequality follows by the condition of œµ that œµ ‚â§1/(6 log2(n)).
REFERENCES,0.46611909650924027,"For the upper bound, we similarly have
1 + œµ 1 ‚àíœµ"
REFERENCES,0.4681724845995893,"‚åàlog2(n)‚åâ
‚â§(1 + 3œµ)‚åàlog2(n)‚åâ‚â§exp (3‚åàlog2(n)‚åâœµ) ‚â§e2/3 ‚â§2,"
REFERENCES,0.4702258726899384,where the first inequality follows since œµ < 1/(6 log2(n)).
REFERENCES,0.4722792607802875,"The next lemma shows that the probability of sampling each edge {vi, vj} is approximately equal to
the ‚Äúideal‚Äù sampling probability pi(j).
Lemma 2. For every i and j Ã∏= i, we have"
REFERENCES,0.47433264887063653,"9
10 ¬∑ pi(j) ‚â§epi(j) ‚â§12 ¬∑ pi(j)."
REFERENCES,0.47638603696098564,"Proof. Let Y = {yi,1, . . . , yi,L} be the neighbours of xi sampled by Algorithm 2, where L =
6C log(n)/Œªk+1. Then,"
REFERENCES,0.4784394250513347,"P [j ‚ààY ] = 1 ‚àí L
Y"
REFERENCES,0.4804928131416838,"l=1
(1 ‚àíP [yi,l = j]) ‚â•1 ‚àí

1 ‚àík(xi, xj)"
REFERENCES,0.48254620123203285,2 degK(vi)
REFERENCES,0.48459958932238195,"L
‚â•1 ‚àíexp

‚àíL ¬∑ k(xi, xj)"
REFERENCES,0.486652977412731,2 degK(vi) 
REFERENCES,0.4887063655030801,The proof proceeds by case distinction.
REFERENCES,0.49075975359342916,"Case 1: pi(j) ‚â§9/10. In this case, we have"
REFERENCES,0.4928131416837782,P [j ‚ààY ] ‚â•1 ‚àíexp (‚àí6pi(j)/2) ‚â•pi(j).
REFERENCES,0.4948665297741273,"Case 2: pi(j) > 9/10. In this case, we have"
REFERENCES,0.49691991786447637,"P [j ‚ààY ] ‚â•1 ‚àíexp

‚àí9 ¬∑ 6 20 
‚â•9 10,"
REFERENCES,0.4989733059548255,which completes the proof on the lower bound of epi(j).
REFERENCES,0.5010266940451745,"For the upper bound, we have"
REFERENCES,0.5030800821355236,"P [j ‚ààY ] ‚â§1 ‚àí

1 ‚àí2k(xi, xj)"
REFERENCES,0.5051334702258727,degK(vi)
REFERENCES,0.5071868583162218,"L
‚â§2k(xi, xj)"
REFERENCES,0.5092402464065708,degK(vi) ¬∑ L = 12C ¬∑ log(n)
REFERENCES,0.5112936344969199,"Œªk+1
¬∑ k(xi, xj)"
REFERENCES,0.5133470225872689,"degK(vi),"
REFERENCES,0.5154004106776181,from which the statement follows.
REFERENCES,0.5174537987679672,"An immediate corollary of Lemma 2 is as follows.
Corollary 1. For all different i, j ‚àà[n], it holds that"
REFERENCES,0.5195071868583162,"9
10 ¬∑ p(i, j) ‚â§ep(i, j) ‚â§12 ¬∑ p(i, j)"
REFERENCES,0.5215605749486653,"and
6
7 ¬∑ p(i, j) ‚â§bp(i, j) ‚â§36"
REFERENCES,0.5236139630390144,"5 ¬∑ p(i, j)."
REFERENCES,0.5256673511293635,"Proof. For the upper bound of the first statement, we can assume that pi(j) ‚â§1/12 and pj(i) ‚â§1/12,
since otherwise we have ep(i, j) ‚â§1 ‚â§12¬∑p(i, j) and the statement holds trivially. Then, by Lemma 2,
we have"
REFERENCES,0.5277207392197125,"ep(i, j) = epi(j) + epj(i) ‚àíepi(j) ¬∑ epj(i)
‚â§12pi(j) + 12pj(i) ‚àí12pi(j) ¬∑ 12pj(i)
‚â§12 (pi(j) + pj(i) ‚àípi(j) ¬∑ pj(i))
= 12 ¬∑ p(i, j) and"
REFERENCES,0.5297741273100616,"ep(i, j) = epi(j) + epj(i) ‚àíepi(j) ¬∑ epj(i) ‚â•9"
REFERENCES,0.5318275154004107,10 ¬∑ pi(j) + 9
REFERENCES,0.5338809034907598,10 ¬∑ pj(i) ‚àí9
REFERENCES,0.5359342915811088,10pi(j) ¬∑ 9
REFERENCES,0.5379876796714579,10pj(i) ‚â•9
REFERENCES,0.5400410677618069,10 (pi(j) + pj(i) ‚àípi(j)pj(i)) = 9
REFERENCES,0.5420944558521561,"10 ¬∑ p(i, j)."
REFERENCES,0.5441478439425051,"For the second statement, notice that"
REFERENCES,0.5462012320328542,"bpi(j) = min
6C log(n)"
REFERENCES,0.5482546201232033,"Œªk+1
¬∑
k(i, j)
g[1,n](xi), 1
"
REFERENCES,0.5503080082135524,"‚â•min

1
1 + Œµ
C log(n)"
REFERENCES,0.5523613963039015,"Œªk+1
¬∑
k(i, j)
degK(vi), 1
"
REFERENCES,0.5544147843942505,"‚â•
1
1 + Œµ ¬∑ min
C log(n)"
REFERENCES,0.5564681724845996,"Œªk+1
¬∑
k(i, j)
degK(vi), 1
"
REFERENCES,0.5585215605749486,"=
1
1 + Œµ ¬∑ pi(j) ‚â•6"
REFERENCES,0.5605749486652978,"7 ¬∑ pi(j),"
REFERENCES,0.5626283367556468,"since g[1,n](xi) is a (1 ¬± Œµ) approximation of degK(vi) and Œµ ‚â§1/6. Similarly,"
REFERENCES,0.5646817248459959,"bpi(j) ‚â§
6
1 ‚àíŒµ ¬∑ pi(j) ‚â§36"
REFERENCES,0.5667351129363449,5 ¬∑ pi(j).
REFERENCES,0.5687885010266941,"Then, for the upper bound of the second statement, we can assume that pi(j) ‚â§5/36 and pj(i) ‚â§
5/36, since otherwise bp(i, j) ‚â§1 ‚â§(36/5) ¬∑ ep(i, j) and the statement holds trivially. This implies
that"
REFERENCES,0.5708418891170431,"bp(i, j) = bpi(j) + bpj(i) ‚àíbpi(j) ¬∑ bpj(i) ‚â§36"
REFERENCES,0.5728952772073922,5 pi(j) + 36
REFERENCES,0.5749486652977412,5 pj(i) ‚àí36
REFERENCES,0.5770020533880903,5 pi(j) ¬∑ 36
REFERENCES,0.5790554414784395,5 pj(i) ‚â§36
REFERENCES,0.5811088295687885,5 (pi(j) + pj(i) ‚àípi(j) ¬∑ pj(i)) = 36
REFERENCES,0.5831622176591376,"5 ¬∑ p(i, j) and"
REFERENCES,0.5852156057494866,"bp(i, j) ‚â•6"
REFERENCES,0.5872689938398358,7pi(j) + 6
REFERENCES,0.5893223819301848,7pj(i) ‚àí6
REFERENCES,0.5913757700205339,7pi(j) ¬∑ 6
REFERENCES,0.5934291581108829,7pj(i) ‚â•6
REFERENCES,0.5954825462012321,7 (pi(j) + pj(i) ‚àípi(j) ¬∑ pj(i)) = 6
REFERENCES,0.5975359342915811,"7 ¬∑ p(i, j),"
REFERENCES,0.5995893223819302,which completes the proof.
REFERENCES,0.6016427104722792,"We are now ready to prove Theorem 2. It is important to note that, although some of the proofs
below are parallel to that of [27], our analysis needs to carefully take into account the approximation
ratios introduced by the approximate KDE algorithm, which makes our analysis more involved. The
following concentration inequalities will be used in our proof."
REFERENCES,0.6036960985626283,"Lemma 3 (Bernstein‚Äôs Inequality [8]). Let X1, . . . , Xn be independent random variables such that
|Xi| ‚â§M for any i ‚àà{1, . . . , n}. Let X = Pn
i=1 Xi, and R = Pn
i=1 E

X2
i

. Then, it holds that"
REFERENCES,0.6057494866529775,"P [|X ‚àíE [X] | ‚â•t] ‚â§2 exp

‚àí
t2"
REFERENCES,0.6078028747433265,"2(R + Mt/3) 
."
REFERENCES,0.6098562628336756,"Lemma 4 (Matrix Chernoff Bound [28]). Consider a finite sequence {Xi} of independent, random,
PSD matrices of dimension d that satisfy ‚à•Xi‚à•‚â§R. Let ¬µmin ‚âúŒªmin(E [P"
REFERENCES,0.6119096509240246,"i Xi]) and ¬µmax ‚âú
Œªmax(E [P"
REFERENCES,0.6139630390143738,"i Xi]). Then, it holds that P "" Œªmin X i
Xi !"
REFERENCES,0.6160164271047228,‚â§(1 ‚àíŒ¥)¬µmin #
REFERENCES,0.6180698151950719,"‚â§d

e‚àíŒ¥"
REFERENCES,0.6201232032854209,(1 ‚àíŒ¥)1‚àíŒ¥
REFERENCES,0.62217659137577,¬µmin/R
REFERENCES,0.6242299794661191,"for Œ¥ ‚àà[0, 1], and P "" Œªmax X i
Xi !"
REFERENCES,0.6262833675564682,‚â•(1 + Œ¥)¬µmax #
REFERENCES,0.6283367556468172,"‚â§d

eŒ¥"
REFERENCES,0.6303901437371663,(1 + Œ¥)1+Œ¥
REFERENCES,0.6324435318275154,¬µmax/R
REFERENCES,0.6344969199178645,forr Œ¥ ‚â•0.
REFERENCES,0.6365503080082136,"Proof of Theorem 2. We first show that the degrees of all the vertices in the similarity graph K are
preserved with high probability in the sparsifier G. For any vertex vi, let yi,1, . . . , yi,L be the indices
of the neighbours of vi sampled by Algorithm 2."
REFERENCES,0.6386036960985626,"For every pair of indices i Ã∏= j, and for every 1 ‚â§l ‚â§L, we define the random variable Zi,j,l to be
the weight of the sampled edge if j is the neighbour sampled from i at iteration l, and 0 otherwise:"
REFERENCES,0.6406570841889117,"Zi,j,l ‚âú

k(xi,xj)"
REFERENCES,0.6427104722792608,"bp(i,j)
if yi,l = j
0
otherwise."
REFERENCES,0.6447638603696099,"Then, fixing an arbitrary vertex xi, we can write"
REFERENCES,0.6468172484599589,"degG(vi) = L
X l=1 X"
REFERENCES,0.648870636550308,"iÃ∏=j
Zi,j,l + Zj,i,l."
REFERENCES,0.6509240246406571,We have
REFERENCES,0.6529774127310062,"E [degG(vi)] = L
X l=1 X"
REFERENCES,0.6550308008213552,"jÃ∏=i
E [Zi,j,l] + E [Zj,i,l] = L
X l=1 X jÃ∏=i"
REFERENCES,0.6570841889117043,"
P [yi,l = j] ¬∑ k(xi, xj)"
REFERENCES,0.6591375770020534,"bp(i, j)
+ P [yj,l = i] ¬∑ k(xi, xj)"
REFERENCES,0.6611909650924025,"bp(i, j) 
."
REFERENCES,0.6632443531827515,"By Lemmas 1 and 2 and Corollary 1, we have"
REFERENCES,0.6652977412731006,"E [degG(vi)] ‚â•
X jÃ∏=i"
REFERENCES,0.6673511293634496,"k(xi, xj)"
REFERENCES,0.6694045174537988,"bp(i, j)"
REFERENCES,0.6714579055441479,"L ¬∑ k(xi, xj)"
REFERENCES,0.6735112936344969,"2 degK(vi) + L ¬∑ k(xi, xj)"
REFERENCES,0.675564681724846,"2 degK(vj)  =
X iÃ∏=j"
REFERENCES,0.6776180698151951,"3k(xi, xj)"
REFERENCES,0.6796714579055442,"bp(i, j)
(pi(j) + pj(i)) ‚â•
X iÃ∏=j"
REFERENCES,0.6817248459958932,"5k(xi, xj)"
REFERENCES,0.6837782340862423,"12
= 5 degK(vi) 12
,"
REFERENCES,0.6858316221765913,"where the last inequality follows by the fact that bp(i, j) ‚â§(36/5) ¬∑ p(i, j) ‚â§(36/5) ¬∑ (pi(j) + pj(i)).
Similarly, we have"
REFERENCES,0.6878850102669405,"E [degG(vi)] ‚â§
X jÃ∏=i"
REFERENCES,0.6899383983572895,"k(xi, xj)"
REFERENCES,0.6919917864476386,"bp(i, j)"
REFERENCES,0.6940451745379876,"2 ¬∑ L ¬∑ k(xi, xj)"
REFERENCES,0.6960985626283368,"degK(vi)
+ 2 ¬∑ L ¬∑ k(xi, xj)"
REFERENCES,0.6981519507186859,"degK(vj)  =
X jÃ∏=i"
REFERENCES,0.7002053388090349,"12 ¬∑ k(xi, xj)"
REFERENCES,0.702258726899384,"bp(i, j)
(pi(j) + pj(i)) ‚â§
X"
REFERENCES,0.704312114989733,"jÃ∏=i
28 ¬∑ k(xi, xj) = 28 ¬∑ degK(vi),"
REFERENCES,0.7063655030800822,where the inequality follows by the fact that
REFERENCES,0.7084188911704312,"bp(i, j) ‚â•6"
REFERENCES,0.7104722792607803,"7 ¬∑ p(i, j) = 6"
REFERENCES,0.7125256673511293,7 ¬∑ (pi(j) + pj(i) ‚àípi(j) ¬∑ pj(i)) ‚â•3
REFERENCES,0.7145790554414785,7 ¬∑ (pi(j) + pj(i)) .
REFERENCES,0.7166324435318275,"In order to prove a concentration bound on this degree estimate, we would like to apply the Bernstein
inequality for which we need to bound R = L
X l=1 X"
REFERENCES,0.7186858316221766,"jÃ∏=i
E

Z2
i,j,l

+ E

Z2
j,i,l
 = L
X l=1 X"
REFERENCES,0.7207392197125256,"jÃ∏=i
P [yi,l = j] ¬∑ k(xi, xj)2"
REFERENCES,0.7227926078028748,"bp(i, j)2
+ P [yj,l = i] ¬∑ k(xi, xj)2"
REFERENCES,0.7248459958932238,"bp(i, j)2 ‚â§
X jÃ∏=i"
REFERENCES,0.7268993839835729,"12 ¬∑ k(xi, xj)2"
REFERENCES,0.728952772073922,"bp(i, j)2
¬∑ (pi(j) + pj(i)) ‚â§
X"
REFERENCES,0.731006160164271,"jÃ∏=i
28 ¬∑ k(xi, xj)2"
REFERENCES,0.7330595482546202,"bp(i, j) ‚â§
X jÃ∏=i 98"
REFERENCES,0.7351129363449692,"3 ¬∑ k(xi, xj)2 pi(j) =
X jÃ∏=i 98"
REFERENCES,0.7371663244353183,"3 ¬∑ k(xi, xj) ¬∑ degK(vi) ¬∑ Œªk+1"
REFERENCES,0.7392197125256673,C log(n)
REFERENCES,0.7412731006160165,= 98 ¬∑ degK(vi)2 ¬∑ Œªk+1
REFERENCES,0.7433264887063655,"3 ¬∑ C log(n)
,"
REFERENCES,0.7453798767967146,where the third inequality follows by the fact that
REFERENCES,0.7474332648870636,"bp(i, j) ‚â•6"
REFERENCES,0.7494866529774127,"7 ¬∑ p(i, j) ‚â•6"
REFERENCES,0.7515400410677618,7 ¬∑ pi(j).
REFERENCES,0.7535934291581109,"Then, by applying Bernstein‚Äôs inequality we have for any constant C2 that"
REFERENCES,0.75564681724846,"P

|degG(vi) ‚àíE[degG(vi)]| ‚â•1"
REFERENCES,0.757700205338809,"C2
degK(vi)

‚â§2 exp Ô£´"
REFERENCES,0.7597535934291582,"Ô£≠‚àí
degK(vi)2/(2 ¬∑ C2
2)"
REFERENCES,0.7618069815195072,98 degK(vi)2Œªk+1
REFERENCES,0.7638603696098563,"3C log(n)
+ 7 degK(vi)2Œªk+1"
REFERENCES,0.7659137577002053,6CC2¬∑log(n) Ô£∂ Ô£∏
REFERENCES,0.7679671457905544,"‚â§2 exp

‚àí
C ¬∑ log(n)
((196/3) ¬∑ C2
2 + (7/3) ¬∑ C2) ¬∑ Œªk+1 "
REFERENCES,0.7700205338809035,"= o(1/n),"
REFERENCES,0.7720739219712526,where we use the fact that
REFERENCES,0.7741273100616016,"Zi,j,l ‚â§7k(xi, xj)"
REFERENCES,0.7761806981519507,"6pi(j)
= 7 degK(vi) ¬∑ Œªk+1"
REFERENCES,0.7782340862422998,"6C ¬∑ log(n)
."
REFERENCES,0.7802874743326489,"Therefore, by taking C to be sufficiently large and by the union bound, it holds with high probability
that the degree of all the nodes in G are preserved up to a constant factor. For the remainder of the"
REFERENCES,0.7823408624229979,"proof, we assume that this is the case. Note in particular that this implies volG(S) = Œò(volK(S)) for
any subset S ‚äÜV ."
REFERENCES,0.784394250513347,"Next, we prove it holds for G that œïG(Si) = O (k ¬∑ œïK(Si)) for any 1 ‚â§i ‚â§k, where S1, . . . , Sk
form an optimal clustering in K."
REFERENCES,0.7864476386036962,"By the definition of Zi,j,l, it holds for any 1 ‚â§i ‚â§k that"
REFERENCES,0.7885010266940452,"E [‚àÇG(Si)] = E Ô£Æ Ô£∞X a‚ààSi X bÃ∏‚ààSi L
X"
REFERENCES,0.7905544147843943,"l=1
Za,b,l + Zb,a,l Ô£π Ô£ª ‚â§
X a‚ààSi X bÃ∏‚ààSi"
REFERENCES,0.7926078028747433,"12k(xa, xb)"
REFERENCES,0.7946611909650924,"bp(a, b)
¬∑ (pa(b) + pb(a))"
REFERENCES,0.7967145790554415,= O (‚àÇK(Si))
REFERENCES,0.7987679671457906,"where the last line follows by Corollary 1. By Markov‚Äôs inequality and the union bound, with constant
probability it holds for all i = 1, . . . , k that"
REFERENCES,0.8008213552361396,‚àÇG(Si) = O(k ¬∑ ‚àÇK(Si)).
REFERENCES,0.8028747433264887,"Therefore, it holds with constant probability that"
REFERENCES,0.8049281314168378,"œÅG(k) ‚â§max
1‚â§i‚â§k œïG(Si) = max
1‚â§i‚â§k O(k ¬∑ œïK(Si)) = O(k ¬∑ œÅK(k))."
REFERENCES,0.8069815195071869,"Finally, we prove that Œªk+1(NG) = ‚Ñ¶(Œªk+1(NK)). Let NK be the projection of NK on its top n ‚àík
eigenspaces, and notice that NK can be written NK = n
X"
REFERENCES,0.8090349075975359,"i=k+1
Œªi(NK)fif ‚ä∫
i"
REFERENCES,0.811088295687885,"where f1, . . . , fn are the eigenvectors of NK. Let N
‚àí1/2
K
be the square root of the pseudoinverse of
NK."
REFERENCES,0.813141683778234,"We prove that the top n ‚àík eigenvalues of NK are preserved, which implies that Œªk+1(NG) =
Œò(Œªk+1(NK)). To prove this, for each data point xi and sample 1 ‚â§l ‚â§L, we define a random
matrix Xi,l ‚ààRn√ón by"
REFERENCES,0.8151950718685832,"Xi,l = wG(vi, vj) ¬∑ N
‚àí1/2
K
beb‚ä∫
eN
‚àí1/2
K
;"
REFERENCES,0.8172484599589322,"where j = yi,l, be ‚âúœávi ‚àíœávj is the edge indicator vector, and xvi ‚ààRn is defined by"
REFERENCES,0.8193018480492813,"œávi(a) ‚âú (
1
‚àö"
REFERENCES,0.8213552361396304,"degK(vi)
if a = i"
REFERENCES,0.8234086242299795,"0
otherwise."
REFERENCES,0.8254620123203286,"Notice that n
X i=1 L
X"
REFERENCES,0.8275154004106776,"l=1
Xi,l =
X"
REFERENCES,0.8295687885010267,"sampled edges e={vi,vj}
wG(vi, vj) ¬∑ N
‚àí1/2
K
beb‚ä∫
eN
‚àí1/2
K
= N
‚àí1/2
K
N
‚Ä≤
GN
‚àí1/2
K"
REFERENCES,0.8316221765913757,"where
N
‚Ä≤
G =
X"
REFERENCES,0.8336755646817249,"sampled edges e={vi,vj}
wG(vi, vj) ¬∑ beb‚ä∫
e"
REFERENCES,0.8357289527720739,"is the Laplacian matrix of G normalised with respect to the degrees of the nodes in K. We prove that,
with high probability, the top n ‚àík eigenvectors of N
‚Ä≤
G and NK are approximately the same. Then,
we show the same for NG and N
‚Ä≤
G which implies that Œªk+1(NG) = ‚Ñ¶(Œªk+1(NK))."
REFERENCES,0.837782340862423,"We begin by looking at the first moment of Pn
i=1
PL
l=1 Xi,l, and have that Œªmin  E "" n
X i=1 L
X"
REFERENCES,0.839835728952772,"l=1
Xi,l #!"
REFERENCES,0.8418891170431212,"= Œªmin Ô£´ Ô£¨
Ô£¨
Ô£≠ n
X i=1 L
X l=1 X"
REFERENCES,0.8439425051334702,"jÃ∏=i
e={vi,vj}"
REFERENCES,0.8459958932238193,"P [yi,l = j] ¬∑ k(xi, xj)"
REFERENCES,0.8480492813141683,"bp(i, j)
¬∑ N
‚àí1/2
K
beb‚ä∫
eN
‚àí1/2
K Ô£∂ Ô£∑
Ô£∑
Ô£∏ ‚â•Œªmin Ô£´ Ô£¨
Ô£¨
Ô£≠ n
X i=1 X"
REFERENCES,0.8501026694045175,"jÃ∏=i
e={vi,vj}"
REFERENCES,0.8521560574948666,"3pi(j) ¬∑ k(xi, xj)"
REFERENCES,0.8542094455852156,"bp(i, j)
¬∑ N
‚àí1/2
K
beb‚ä∫
eN
‚àí1/2
K Ô£∂ Ô£∑
Ô£∑
Ô£∏ ‚â•Œªmin  5"
REFERENCES,0.8562628336755647,"12 ¬∑ N
‚àí1/2
K
NKN
‚àí1/2
K 
= 5 12,"
REFERENCES,0.8583162217659137,where the last inequality follows by the fact that
REFERENCES,0.8603696098562629,"bp(i, j) ‚â§36"
REFERENCES,0.8624229979466119,"5 ¬∑ p(i, j) ‚â§36"
REFERENCES,0.864476386036961,5 ¬∑ (pi(j) + pj(i)) .
REFERENCES,0.86652977412731,"Similarly, Œªmax  E "" n
X i=1 L
X"
REFERENCES,0.8685831622176592,"l=1
Xi,l #!"
REFERENCES,0.8706365503080082,"= Œªmax Ô£´ Ô£¨
Ô£¨
Ô£≠ n
X i=1 L
X l=1 X"
REFERENCES,0.8726899383983573,"jÃ∏=i
e={vi,vj}"
REFERENCES,0.8747433264887063,"P [yi,l = j] ¬∑ k(xi, xj)"
REFERENCES,0.8767967145790554,"bp(i, j)
¬∑ N
‚àí1/2
K
beb‚ä∫
eN
‚àí1/2
K Ô£∂ Ô£∑
Ô£∑
Ô£∏ ‚â§Œªmax Ô£´ Ô£¨
Ô£¨
Ô£≠ n
X i=1 X"
REFERENCES,0.8788501026694046,"jÃ∏=i
e={vi,vj}"
REFERENCES,0.8809034907597536,"12 ¬∑ pi(j) ¬∑ k(xi, xj)"
REFERENCES,0.8829568788501027,"bp(i, j)
¬∑ N
‚àí1/2
K
beb‚ä∫
eN
‚àí1/2
K Ô£∂ Ô£∑
Ô£∑
Ô£∏"
REFERENCES,0.8850102669404517,"‚â§Œªmax

28 ¬∑ N
‚àí1/2
K
NKN
‚àí1/2
K

= 28,"
REFERENCES,0.8870636550308009,where the last inequality follows by the fact that
REFERENCES,0.8891170431211499,"bp(i, j) ‚â•6"
REFERENCES,0.891170431211499,"7 ¬∑ p(i, j) ‚â•3"
REFERENCES,0.893223819301848,7 ¬∑ (pi(j) + pj(i)).
REFERENCES,0.8952772073921971,"Additionally, for any i and j = yi,l and e = {vi, vj}, we have that"
REFERENCES,0.8973305954825462,"‚à•Xi,l‚à•‚â§wG(vi, vj) ¬∑ b‚ä∫
eN
‚àí1/2
K
N
‚àí1/2
K
be"
REFERENCES,0.8993839835728953,"= k(xi, xj)"
REFERENCES,0.9014373716632443,"bp(i, j)
¬∑ b‚ä∫
eN
‚àí1
K be"
REFERENCES,0.9034907597535934,"‚â§k(xi, xj)"
REFERENCES,0.9055441478439425,"bp(i, j)
¬∑
1
Œªk+1
‚à•be‚à•2"
REFERENCES,0.9075975359342916,"‚â§
7 ¬∑ Œªk+1"
REFERENCES,0.9096509240246407,"3C log(n)

1
degK(vi) +
1
degK(vj)
 ¬∑
1
Œªk+1"
REFERENCES,0.9117043121149897,"
1
degK(vi) +
1
degK(vj) "
REFERENCES,0.9137577002053389,"‚â§
7
3C log(n)."
REFERENCES,0.9158110882956879,"Now, we apply the matrix Chernoff bound and have that P "" Œªmax n
X i=1 L
X"
REFERENCES,0.917864476386037,"l=1
Xi,l ! ‚â•42 #"
REFERENCES,0.919917864476386,"‚â§n

e1/2"
REFERENCES,0.9219712525667351,(1 + 1/2)3/2
REFERENCES,0.9240246406570842,"12C¬∑log(n)
= O(1/nc)"
REFERENCES,0.9260780287474333,"for some constant c. The other side of the matrix Chernoff bound gives us that P "" Œªmin n
X i=1 L
X"
REFERENCES,0.9281314168377823,"l=1
Xi,l ! ‚â§5/24 #"
REFERENCES,0.9301848049281314,‚â§O(1/nc).
REFERENCES,0.9322381930184805,"Combining these, with probability 1 ‚àíO(1/nc) it holds for any non-zero x ‚ààRn in the space
spanned by fk+1, . . . , fn that"
REFERENCES,0.9342915811088296,"x‚ä∫N
‚àí1/2
K
N
‚Ä≤
GN
‚àí1/2
K
x
x‚ä∫x
‚àà(5/24, 42)."
REFERENCES,0.9363449691991786,"By setting y = N
‚àí1/2
K
x, we can rewrite this as"
REFERENCES,0.9383983572895277,"y‚ä∫N
‚Ä≤
Gy"
REFERENCES,0.9404517453798767,"y‚ä∫N
1/2
K N
1/2
K y
= y‚ä∫N
‚Ä≤
Gy
y‚ä∫NKy = y‚ä∫N
‚Ä≤
Gy
y‚ä∫y
y‚ä∫y
y‚ä∫NKy ‚àà(5/24, 42)."
REFERENCES,0.9425051334702259,"Since dim(span{fk+1, . . . , fn}) = n ‚àík, we have shown that there exist n ‚àík orthogonal vectors
whose Rayleigh quotient with respect to N
‚Ä≤
G is ‚Ñ¶(Œªk+1(NK)). By the Courant-Fischer Theorem, we
have Œªk+1(N
‚Ä≤
G) = ‚Ñ¶(Œªk+1(NK))."
REFERENCES,0.944558521560575,"It only remains to show that Œªk+1(NG) = ‚Ñ¶(Œªk+1(N
‚Ä≤
G)), which implies that Œªk+1(NG) =
‚Ñ¶(Œªk+1(NK)). By the definition of N
‚Ä≤
G, we have that NG = D‚àí1/2
G
D1/2
K N
‚Ä≤
GD1/2
K D‚àí1/2
G
. Therefore,
for any x ‚ààRn and y = D1/2
K D‚àí1/2
G
x, it holds that x‚ä∫NGx"
REFERENCES,0.946611909650924,"x‚ä∫x
= y‚ä∫N
‚Ä≤
Gy
x‚ä∫x
= ‚Ñ¶"
REFERENCES,0.9486652977412731,"y‚ä∫N
‚Ä≤
Gy
y‚ä∫y ! ,"
REFERENCES,0.9507186858316222,"where the final guarantee follows from the fact that the degrees in G are preserved up to a constant
factor. The conclusion of the theorem follows by the Courant-Fischer Theorem."
REFERENCES,0.9527720739219713,"Finally, we bound the running time of Algorithm 2 which is dominated by the recursive calls to
Algorithm 1. We note that, although the number of nodes doubles at each level of the recursion tree
(visualised in Figure 4), the total number of samples S and data points X remain constant for each
level of the tree. Then, since the running time of the KDE algorithm is superadditive, the total running
time of the KDE algorithms at level i of the tree is Ti ="
"I
X",0.9548254620123203,"2i
X"
"I
X",0.9568788501026694,"j=1
TKDE(|Si,j|, |Xi,j|, œµ) ‚â§TKDE Ô£´"
"I
X",0.9589322381930184,"Ô£≠
2i
X"
"I
X",0.9609856262833676,"j=1
|Si,j|,"
"I
X",0.9630390143737166,"2i
X"
"I
X",0.9650924024640657,"j=1
|Xi,j|, œµ Ô£∂"
"I
X",0.9671457905544147,"Ô£∏= TKDE(|S|, |X|, œµ)."
"I
X",0.9691991786447639,"Since there are O(log(n)) levels of the tree, the total running time of Algorithm 1 is
eO(TKDE(|S|, |X|, œµ)). This completes the proof."
"I
X",0.971252566735113,"B
Additional Experimental Results"
"I
X",0.973305954825462,"In this section, we include in Figures 6 and 7 some additional examples of the performance of the six
spectral clustering algorithms on the BSDS image segmentation dataset. Due to its quadratic memory
requirement, the SKLEARN GK algorithm cannot be used on the full-resolution image. Therefore,
we present its results on each image downsampled to 20,000 pixels. For every other algorithm, we
show the results on the full-resolution image. In every case, we find that our algorithm is able to
identify more refined detail of the image when compared with the alternative algorithms."
"I
X",0.9753593429158111,"(a) Original Image
(b) SKLEARN GK
(c) OUR ALGORITHM"
"I
X",0.9774127310061602,"(d) SKLEARN k-NN
(e) FAISS EXACT
(f) FAISS HNSW
(g) FAISS IVF"
"I
X",0.9794661190965093,"(h) Original Image
(i) SKLEARN GK
(j) OUR ALGORITHM"
"I
X",0.9815195071868583,"(k) SKLEARN k-NN
(l) FAISS EXACT
(m) FAISS HNSW
(n) FAISS IVF"
"I
X",0.9835728952772074,"(o) Original Image
(p) SKLEARN GK
(q) OUR ALGORITHM"
"I
X",0.9856262833675564,"(r) SKLEARN k-NN
(s) FAISS EXACT
(t) FAISS HNSW
(u) FAISS IVF"
"I
X",0.9876796714579056,"Figure 6: More examples on the performance of the spectral clustering algorithms for image segmen-
tation."
"I
X",0.9897330595482546,"(a) Original Image
(b) SKLEARN GK
(c) OUR ALGORITHM"
"I
X",0.9917864476386037,"(d) SKLEARN k-NN
(e) FAISS EXACT
(f) FAISS HNSW
(g) FAISS IVF"
"I
X",0.9938398357289527,"(h) Original Image
(i) SKLEARN GK
(j) OUR ALGORITHM"
"I
X",0.9958932238193019,"(k) SKLEARN k-NN
(l) FAISS EXACT
(m) FAISS HNSW
(n) FAISS IVF"
"I
X",0.997946611909651,"Figure 7: More examples on the performance of the spectral clustering algorithms for image segmen-
tation."
