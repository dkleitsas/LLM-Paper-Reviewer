Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018552875695732839,"Context. Mixup is a highly successful technique to improve generalization of
1"
ABSTRACT,0.0037105751391465678,"neural networks by augmenting the training data with combinations of random
2"
ABSTRACT,0.0055658627087198514,"pairs. Selective mixup is a family of methods that apply mixup to specific pairs,
3"
ABSTRACT,0.0074211502782931356,"e.g. only combining examples across classes or domains. These methods have
4"
ABSTRACT,0.00927643784786642,"claimed remarkable improvements on benchmarks with distribution shifts, but their
5"
ABSTRACT,0.011131725417439703,"mechanisms and limitations remain poorly understood.
6"
ABSTRACT,0.012987012987012988,"Findings. We examine an overlooked aspect of selective mixup that explains its
7"
ABSTRACT,0.014842300556586271,"success in a completely new light. We find that the non-random selection of pairs
8"
ABSTRACT,0.016697588126159554,"affects the training distribution and improve generalization by means completely
9"
ABSTRACT,0.01855287569573284,"unrelated to the mixing. For example in binary classification, mixup across classes
10"
ABSTRACT,0.02040816326530612,"implicitly resamples the data for a uniform class distribution — a classical solution
11"
ABSTRACT,0.022263450834879406,"to label shift. We show empirically that this implicit resampling explains much of
12"
ABSTRACT,0.02411873840445269,"the improvements in prior work. Theoretically, these results rely on a “regression
13"
ABSTRACT,0.025974025974025976,"toward the mean”, an accidental property that we identify in several datasets.
14"
ABSTRACT,0.027829313543599257,"Takeaways. We have found a new equivalence between two successful methods:
15"
ABSTRACT,0.029684601113172542,"selective mixup and resampling. We identify limits of the former, confirm the
16"
ABSTRACT,0.03153988868274583,"effectiveness of the latter, and find better combinations of their respective benefits.
17"
INTRODUCTION,0.03339517625231911,"1
Introduction
18"
INTRODUCTION,0.03525046382189239,"Mixup and its variants are some of the few methods that improve generalization across tasks and
19"
INTRODUCTION,0.03710575139146568,"modalities with no domain-specific information [36]. Standard mixup replaces training data with
20"
INTRODUCTION,0.03896103896103896,"linear combinations of random pairs of examples, proving successful e.g. for image classification [35],
21"
INTRODUCTION,0.04081632653061224,"semantic segmentation [9], natural language processing [30], and speech processing [21].
22"
INTRODUCTION,0.04267161410018553,"This paper focuses on scenarios of distribution shift and on variants of mixup that improve out-
23"
INTRODUCTION,0.04452690166975881,"of-distribution (OOD) generalization. We examine the family of methods that apply mixup on
24"
INTRODUCTION,0.04638218923933209,"selected pairs of examples, which we refer to as selective mixup [7, 15, 19, 22, 28, 31, 33]. Each
25"
INTRODUCTION,0.04823747680890538,"of these method uses a predefined criterion.1 For example, some methods combine examples
26"
INTRODUCTION,0.05009276437847866,"across classes [33] (Figure 1) or across domains [31, 15, 19]. These simple heuristics have claimed
27"
INTRODUCTION,0.05194805194805195,"remarkable improvements on benchmarks such as DomainBed [5], WILDS [12], and Wild-Time [32].
28"
INTRODUCTION,0.05380333951762523,"Despite impressive empirical performance, the theoretical mechanisms of selective mixup remain
29"
INTRODUCTION,0.055658627087198514,"obscure. For example, the selection criteria proposed in [33] include the selection of pairs of the
30"
INTRODUCTION,0.0575139146567718,"same class / different domains, but also the exact opposite.
31"
INTRODUCTION,0.059369202226345084,"1We focus on the basic implementation of selective mixup as described by Yao et al. [33], i.e. without
additional regularizers or modifications of the learning objective described in various other papers."
INTRODUCTION,0.061224489795918366,Original
INTRODUCTION,0.06307977736549165,training
INTRODUCTION,0.06493506493506493,"data{
}
,       ,       ,       ...
{
}
,       ,       ,       ..."
INTRODUCTION,0.06679035250463822,Class 1              2                  1                   3
INTRODUCTION,0.0686456400742115,1⊕2              1⊕3             1⊕2              2⊕3
INTRODUCTION,0.07050092764378478,"More
uniform"
INTRODUCTION,0.07235621521335807,"Proportions of
classes used Mixup"
INTRODUCTION,0.07421150278293136,"across
classes"
INTRODUCTION,0.07606679035250463,"Figure 1: Selective mixup is a family of methods that replace the training data with combined pairs
of examples fulfilling a predefined criterion, e.g. pairs of different classes. As an overlooked side
effect, this modifies the training distribution: in this case, sampling classes more uniformly. This
effect is responsible for much of the resulting improvements in OOD generalization."
INTRODUCTION,0.07792207792207792,"This raises questions:
32"
INTRODUCTION,0.07977736549165121,"1.
What mechanisms are responsible for the improvements of selective mixup?
33"
INTRODUCTION,0.08163265306122448,"2.
What makes each selection criterion suitable to any specific dataset?
34"
INTRODUCTION,0.08348794063079777,"This paper presents surprising answers by highlighting an overlooked side effect of selective mixup.
35"
INTRODUCTION,0.08534322820037106,"The non-random selection of pairs implicitly biases the training distribution and improve
36"
INTRODUCTION,0.08719851576994433,"generalization by means completely unrelated to the mixing. We observe empirically that simply
37"
INTRODUCTION,0.08905380333951762,"concatenating – rather than mixing – the selected pairs along the mini-batch dimension often produces
38"
INTRODUCTION,0.09090909090909091,"the same improvements as mixing them. This critical ablation was absent from prior studies.
39"
INTRODUCTION,0.09276437847866419,"We also analyze theoretically the resampling induced by different selection criteria. We find that
40"
INTRODUCTION,0.09461966604823747,"conditioning on a “different attribute” (e.g. combining examples across classes or domains) brings
41"
INTRODUCTION,0.09647495361781076,"the training distribution of this attribute closer to a uniform one. Consequently, the imbalances in
42"
INTRODUCTION,0.09833024118738404,"the data often “regress toward the mean” with selective mixup. We verify empirically that several
43"
INTRODUCTION,0.10018552875695733,"datasets do indeed shift toward a uniform class distribution in their test split (see Figure 10). We also
44"
INTRODUCTION,0.10204081632653061,"find remarkable correlation between improvements in performance and the reduction in divergence
45"
INTRODUCTION,0.1038961038961039,"of training/test distributions due to selective mixup. This also predicts an unknown failure mode of
46"
INTRODUCTION,0.10575139146567718,"selective mixup when the above property does not hold.
47"
INTRODUCTION,0.10760667903525047,"Our contributions are summarized as follows.
48"
INTRODUCTION,0.10946196660482375,"• We point out an overlooked resampling effect when applying selective mixup (Section 3).
49"
INTRODUCTION,0.11131725417439703,"• We show theoretically that certain selection criteria induce a bias in the distribution of features
50"
INTRODUCTION,0.11317254174397032,"and/or classes equivalent to a “regression toward the mean” (Theorem 3.1). In binary classification
51"
INTRODUCTION,0.1150278293135436,"for example, selecting pairs across classes is equivalent to sampling uniformly over classes, the
52"
INTRODUCTION,0.11688311688311688,"standard approach to address label shift and imbalanced data.
53"
INTRODUCTION,0.11873840445269017,"• We verify empirically that multiple datasets indeed contain a regression toward a uniform class
54"
INTRODUCTION,0.12059369202226346,"distribution across training and test splits (Section 4.6). We also find that improvements from
55"
INTRODUCTION,0.12244897959183673,"selective mixup correlate with reductions in divergence of training/test distributions over labels
56"
INTRODUCTION,0.12430426716141002,"and/or covariates. This strongly suggests that resampling is the main driver for these improvements.
57"
INTRODUCTION,0.1261595547309833,"• We compare many selection criteria and resampling baselines on five datasets. In all cases,
58"
INTRODUCTION,0.1280148423005566,"improvements with selective mixup are partly or fully explained by resampling effects (Section 4).
59"
INTRODUCTION,0.12987012987012986,"The implications for future research are summarized as follows.
60"
INTRODUCTION,0.13172541743970315,"• We connect two areas of the literature by showing that selective mixup is sometimes equivalent to
61"
INTRODUCTION,0.13358070500927643,"resampling, a classical strategy for distribution shifts [3, 8]. This hints at possible benefits from
62"
INTRODUCTION,0.13543599257884972,"advanced methods for label shift and domain adaptation on benchmarks with distribution shifts.
63"
INTRODUCTION,0.137291280148423,"• The resampling explains why different criteria in selective mixup benefit different datasets: they
64"
INTRODUCTION,0.1391465677179963,"affect the distribution of features and/or labels and therefore address covariate and/or label shift.
65"
INTRODUCTION,0.14100185528756956,"• There is a risk of overfitting to the benchmarks: we show that much of the observed improvements
66"
INTRODUCTION,0.14285714285714285,"rely on the accidental property of a “regression toward the mean” in the datasets examined.
67"
INTRODUCTION,0.14471243042671614,"2
Background: mixup and selective mixup
68"
INTRODUCTION,0.14656771799628943,"Notations. We consider a classification model fθ : Rd →[0, 1]C of learned parameters θ. It maps
69"
INTRODUCTION,0.14842300556586271,"an input vector x ∈Rd to a vector y of scores over C classes. The training data for such a model
70"
INTRODUCTION,0.150278293135436,"is typically a set of labeled examples D = {(xi, yi, di)}n
i=1 where yi are one-hot vectors encoding
71"
INTRODUCTION,0.15213358070500926,"ground-truth labels, and di ∈N are optional discrete domain indices. Domain labels are sometimes
72"
INTRODUCTION,0.15398886827458255,"available e.g. in datasets with different image styles [14] or collected over different time periods [12].
73"
INTRODUCTION,0.15584415584415584,"Training with ERM. Standard empirical risk minimization (ERM) optimizes the model’s parameters
74"
INTRODUCTION,0.15769944341372913,"for minθ R(fθ, D) where the expected training risk, for a chosen loss function L, is defined as:
 \"
INTRODUCTION,0.15955473098330242,"label  { e qErm} \m a
t
hcal { R
}
(f_\btheta , \mathcal {D}) \;=\; \expectation _{(\bx , \by ) \in \mathcal {D}} \, \mathcal {L}\big (f_\btheta (\bx ), \by \big ).
(1)
An empirical estimate is obtained with an arithmetic mean over instances of the dataset D.
76"
INTRODUCTION,0.1614100185528757,"Training with mixup. Standard mixup essentially replaces training examples with linear combina-
77"
INTRODUCTION,0.16326530612244897,"tions of random pairs in both input and label space. We formalize it by redefining the training risk.
78
 \"
INTRODUCTION,0.16512059369202226,"label {eqM ix u p} \math c
a
l { R}_\textrm  {mixup} (f_
\
bth"
INTRODUCTION,0.16697588126159554,"eta , \mat hcal {D}) \;=\; \expectation _{(\bx , \by ) \in \mathcal {D}} \, \mathcal {L}\big (f( c\,\bx \!+\! (1\!-\!c) \widetilde {\bx }, \;c\,\by \!+\! (1\!-\!c)\,\widetilde {\by })\big )\\ \textrm {with mixing coefficients}~ c \sim \mathcal {B}(2,2)~~~ \textrm {and paired examples}~ (\widetilde {\bx }, \widetilde {\by }) \sim \mathcal {D}.s
(3)"
INTRODUCTION,0.16883116883116883,"The expectation is approximated by sampling different coefficients and pairs at every training iteration.
80"
INTRODUCTION,0.17068645640074212,"Selective mixup. While standard mixup combines random pairs, selective mixup only combines pairs
81"
INTRODUCTION,0.1725417439703154,"that fulfill a predefined criterion. To select these pairs, the method starts with the original data D, then
82"
INTRODUCTION,0.17439703153988867,"for every (x, y, d) ∈D it selects a (ex, ey, ed) ∈D such that they fulfill the criterion represented by
83"
INTRODUCTION,0.17625231910946196,"the predicate Paired
 
·, ·). For example, the criterion same class / different domain a.k.a. “intra-label
84"
INTRODUCTION,0.17810760667903525,"LISA” in [33] is implemented as follows:
 \"
INTRODUCTION,0.17996289424860853,"label 
{
eqPr edi cate Examp les}  \op
e
rator nam e {Pai r ed}\!\b ig ( (\bx _ i, \b y _i, d
_i),
 (\wi detilde {"
INTRODUCTION,0.18181818181818182,"\bx }_
i
, \w ide tild e {\b y }_ i, \
w
ideti lde  {d}_i)
 \big )\!= \!\tex
tit"
INTRODUCTION,0.1836734693877551,"{true}
 
~~\t ext rm { iff}~ ~ (\ wide
t
ilde {\b y } \!
=\! \ by ) \land (\widetilde {d} \!\neq \! d)~~ &\textit {(same class\,/\,diff. domain)}\\ \hspace {-12pt}\textrm {Other examples:}\nonumber \\ \operatorname {Paired}\!\big ( (\bx _i, \by _i, d_i), (\widetilde {\bx }_i, \widetilde {\by }_i, \widetilde {d}_i) \big )\!=\!\textit {true} ~~\textrm {iff}~~ (\widetilde {\by } \!\neq \! \by ) &\textit {(different class)}\\ \label {eqPredicateExamplesLast} \operatorname {Paired}\!\big ( (\bx _i, \by _i, d_i), (\widetilde {\bx }_i, \widetilde {\by }_i, \widetilde {d}_i) \big )\!=\!\textit {true} ~~\textrm {iff}~~ (\widetilde {d} \!=\! d) &\textit {(same domain)}
(4c)"
SELECTIVE MIXUP MODIFIES THE TRAINING DISTRIBUTION,0.18552875695732837,"3
Selective mixup modifies the training distribution
86"
SELECTIVE MIXUP MODIFIES THE TRAINING DISTRIBUTION,0.18738404452690166,"The new claims of this paper comprise two parts.
87"
SELECTIVE MIXUP MODIFIES THE TRAINING DISTRIBUTION,0.18923933209647495,"1. Estimating the training risk with selective mixup (Eq. 2) uses a different sampling of examples
88"
SELECTIVE MIXUP MODIFIES THE TRAINING DISTRIBUTION,0.19109461966604824,"from D than ERM (Eq. 1). We demonstrate it theoretically in this section.
89"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.19294990723562153,"2. We hypothesize that the biased sampling of training examples influences the generalization
90"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.19480519480519481,"properties of the learned model, regardless of the mixing operation. We verify this empirically in
91"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.19666048237476808,"Section 4 using ablations of selective mixup that omit the mixing operation — a critical baseline
92"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.19851576994434136,"absent from previous studies.
93"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.20037105751391465,"Training distribution. This distribution refers to the examples sampled from D to estimate the
94"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.20222634508348794,"training risk (Eq. 1 or 2) — whether these are then mixed or not. The following discussion focuses
95"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.20408163265306123,"on distributions over classes (y) but analogous arguments apply to covariates (x) and domains (d).
96"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.20593692022263452,"With ERM, the training distribution equals the dataset distribution because the expectation in Eq. (1)
97"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.2077922077922078,"is over uniform samples of D. We obtain an empirical estimate by averaging all one-hot labels, giving
98"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.20964749536178107,"the vector of discrete probabilities pY(D) = ⊕(x,y)∈D y / |D| where ⊕is the element-wise sum.
99"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.21150278293135436,"With selective mixup, evaluating the risk (Eq. 2) requires pairs of samples. The first element of
100"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.21335807050092764,"a pair is sampled uniformly, yielding the same pY(D) as ERM. The second element is selected as
101"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.21521335807050093,"described above, using the first element and one chosen predicate Paired(·, ·) e.g. from (4a–4c). For
102"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.21706864564007422,"our analysis, we denote these “second elements” of the pairs as the virtual data:
103"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.2189239332096475,"\l
ab e
l
 {eqV irtu alDa t a }  \wide
t
ilde  {\ math cal { D}} ~=~ 
\
b ig \{
 ( \ wi d e t i lde
 
{\bx }_i, \widetilde {\by }_i, \widetilde {d}_i) \sim \mathcal {D}:~~ \operatorname {Paired}\!\big ( (\bx _i, \by _i, d_i), (\widetilde {\bx }_i, \widetilde {\by }_i, \widetilde {d}_i) \big )\!=\!\operatorname {}{true}, \,~~\forall \,i=1,\ldots ,|\mathcal {D}| \big \}\,.
(5)
We can now analyze the overall training distribution of selective mixup. An empirical estimate is
105"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.22077922077922077,"obtained by combining the distributions resulting from the two elements of the pairs, which gives the
106"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.22263450834879406,"vector pY(D ∪eD) = (pY(D) ⊕pY( eD)) / 2.
107"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.22448979591836735,"Regression toward the mean. With the criterion same class, it is obvious that pY( eD) = pY(D).
108"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.22634508348794063,"Therefore these variants of selective mixup are not concerned with resampling effects.2 In contrast,
109"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.22820037105751392,"the criteria different class or different domain do bias the sampling. In the case of binary classification,
110"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.2300556586270872,"we have pY( eD)=1−pY(D) and therefore pY(D ∪eD) is uniform. This means that selective mixup
111"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.23191094619666047,"with the different class criterion has the side effect of balancing the training distribution of classes, a
112"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.23376623376623376,"classical mitigation of class imbalance [10, 13]. For multiple classes, we have a more general result.
113"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.23562152133580705,"Theorem 3.1. Given a dataset D={(xi, yi)}i and paired data eD sampled according to the “different
114"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.23747680890538034,"class” criterion, i.e. eD = {(exi, eyi) ∼D s.t. eyi ̸=yi}, then the distribution of classes in D ∪eD is
115"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.23933209647495363,"more uniform than in D. Formally, the entropy H
 
pY(D)

≤H
 
pY(D ∪eD)

.
116"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.24118738404452691,"Proof: see Appendix C.
117"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.24304267161410018,"Theorem 3.1 readily extends in two ways. First, the same effect also results from the different domain
118"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.24489795918367346,"criterion: if each domain contains a different class distribution, the resampling from this criterion
119"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.24675324675324675,"averages them out, yielding a more uniform aggregated training distribution. Second, this averaging
120"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.24860853432282004,"applies not only to class labels (y) but also covariates (x). An analysis using distributions is ill-suited
121"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.2504638218923933,"but the mechanism similarly affects the sampling of covariates when training with selective mixup.
122"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.2523191094619666,"When does one benefit from the resampling (regardless of mixup)? The above results mean that
123"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.2541743970315399,"selective mixup can implicitly reduce imbalances (a.k.a. biases) in the training data. When these are
124"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.2560296846011132,"not spurious and also exist in the test data, the effect on predictive performance could be detrimental.
125"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.25788497217068646,"We expect benefits (which we verify in Section 4) on datasets with distribution shifts, whose train-
126"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.2597402597402597,"ing/test splits contain different imbalances by definition. Softening imbalances in the training data is
127"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.26159554730983303,"then likely to bring the training and test distributions closer to one another, in particular with extreme
128"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.2634508348794063,"shifts such as the complete reversal of a spurious correlation (e.g. waterbirds [24], Section 4.1).
129"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.2653061224489796,"We also expect a benefit on worst-group metrics (e.g. with civilComments [12] in Section 4.4). The
130"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.26716141001855287,"challenge in these datasets comes from the imbalance of class/domain combinations in the training
131"
WE HYPOTHESIZE THAT THE BIASED SAMPLING OF TRAINING EXAMPLES INFLUENCES THE GENERALIZATION,0.2690166975881262,"data, and previous work has indeed shown that balancing is a competitive baseline [8, 24].
132"
EXPERIMENTS,0.27087198515769945,"4
Experiments
133"
EXPERIMENTS,0.2727272727272727,"We performed a large number of experiments to understand the contribution of the different effects of
134"
EXPERIMENTS,0.274582560296846,"selective mixup and other resampling baselines (see Appendix B for complete results).
135"
EXPERIMENTS,0.2764378478664193,"Datasets. We focus on five datasets that previously showed improvements with selective mixup. We
136"
EXPERIMENTS,0.2782931354359926,"selected them to cover a range of modalities (vision, NLP, tabular), settings (binary, multiclass), and
137"
EXPERIMENTS,0.28014842300556586,"types of distribution shifts (covariate, label, and subpopulation shifts).
138"
EXPERIMENTS,0.2820037105751391,"• Waterbirds [24] is a popular artificial dataset used to study distribution shifts. The task is to
139"
EXPERIMENTS,0.28385899814471244,"classify images of birds into two types. The image backgrounds are also of two types, and the
140"
EXPERIMENTS,0.2857142857142857,"correlation between birds and backgrounds is reversed across the training and test splits. The type
141"
EXPERIMENTS,0.287569573283859,"of background in each image serves as its domain label.
142"
EXPERIMENTS,0.2894248608534323,"• CivilComments [12] is a widely-used dataset of online text comments to be classified as toxic
143"
EXPERIMENTS,0.2912801484230056,"or not. Each example is labeled with a topical attribute (e.g. Christian, male, LGBT, etc.) that
144"
EXPERIMENTS,0.29313543599257885,"is spuriously associated with ground truth labels in the training data. These attributes serve as
145"
EXPERIMENTS,0.2949907235621521,"domain labels. The target metric is the worst-group accuracy where the groups correspond to all
146"
EXPERIMENTS,0.29684601113172543,"toxicity/attribute combinations.
147"
EXPERIMENTS,0.2987012987012987,"• Wild-Time Yearbook [32] contains yearbook portraits to be classified as male or female. It is part
148"
EXPERIMENTS,0.300556586270872,"of the Wild-Time benchmark, which is a collection of real-world datasets captured over time. Each
149"
EXPERIMENTS,0.30241187384044527,"example belongs to a discrete time period that serves as its domain label. Distinct time periods are
150"
EXPERIMENTS,0.3042671614100185,"assigned to the training and OOD test splits (see Figure 10).
151"
EXPERIMENTS,0.30612244897959184,"• Wild-Time arXiv [32] contains titles of arXiv preprints. The task is to predict each paper’s
152"
EXPERIMENTS,0.3079777365491651,"primary category among 172 classes. Time periods serve as domain labels.
153"
EXPERIMENTS,0.3098330241187384,"• Wild-Time MIMIC-Readmission [32] contains hospital records (sequences of codes representing
154"
EXPERIMENTS,0.3116883116883117,"diagnoses and treatments) to be classified into two classes. The positive class indicates the
155"
EXPERIMENTS,0.313543599257885,"readmission of the patient at the hospital within 15 days. Time periods serve as domain labels.
156"
EXPERIMENTS,0.31539888682745826,"2The absence of resampling effects holds for same class and same domain alone, but not in conjunction with
other criteria. See e.g. the differences between same domain / diff. class and any domain / diff. class in Figure 3."
EXPERIMENTS,0.3172541743970315,"Methods. We train standard architectures suited to each dataset with the methods below (details
157"
EXPERIMENTS,0.31910946196660483,"in Appendix A). We perform early stopping i.e. recording metrics for each run at the epoch of
158"
EXPERIMENTS,0.3209647495361781,"highest ID or worst-group validation performance (for Wild-Time and waterbirds/civilComments
159"
EXPERIMENTS,0.3228200371057514,"datasets respectively). We plot average metrics in bar charts over 9 different seeds with error bars
160"
EXPERIMENTS,0.3246753246753247,"representing ± one standard deviation. ERM and vanilla mixup are the standard baselines. Baseline
161"
EXPERIMENTS,0.32653061224489793,"resampling uses training examples with equal probability from each class, domain, or combinations
162"
EXPERIMENTS,0.32838589981447125,"thereof as in [8, 24]. Selective mixup (■) includes all possible selection criteria based on classes
163"
EXPERIMENTS,0.3302411873840445,"and domains. We avoid ambiguous terminology from earlier works because of inconsistent usage
164"
EXPERIMENTS,0.3320964749536178,"(e.g. “intra-label LISA” means “different domain” in [12] but not in [32]). Selective sampling (■) is
165"
EXPERIMENTS,0.3339517625231911,"a novel ablation of selective mixup where the selected pairs are not mixed, but concatenated along
166"
EXPERIMENTS,0.3358070500927644,"the mini-batch dimension. Half of the pairs are dropped at random to keep the size of mini-batches
167"
EXPERIMENTS,0.33766233766233766,"constant. Therefore any difference between selective sampling and ERM is attributable only to
168"
EXPERIMENTS,0.3395176252319109,"resampling effects. We also include novel combinations (■) of sampling and mixup. Code to
169"
EXPERIMENTS,0.34137291280148424,"reproduce our experiments and figures: https://github.com/<anonymized>/<anonymized>.
170"
RESULTS ON THE WATERBIRDS DATASET,0.3432282003710575,"4.1
Results on the waterbirds dataset
171"
RESULTS ON THE WATERBIRDS DATASET,0.3450834879406308,"The target metric for this dataset is the worst-group accuracy, with groups defined as the four
172"
RESULTS ON THE WATERBIRDS DATASET,0.3469387755102041,"class/domain combinations. The two difficulties are (1) a class imbalance (77 / 23%) and (2) a
173"
RESULTS ON THE WATERBIRDS DATASET,0.34879406307977734,"correlation shift (spurious class/domain association reversed at test time). See discussion in Figure 2.
174"
RESULTS ON THE WATERBIRDS DATASET,0.35064935064935066,"50
60
70
80
waterbirds: OOD worst-group accuracy (%)"
RESULTS ON THE WATERBIRDS DATASET,0.3525046382189239,"Same domain + Diff. class
Diff. domain + Same class"
RESULTS ON THE WATERBIRDS DATASET,0.35435992578849723,Diff. domain
RESULTS ON THE WATERBIRDS DATASET,0.3562152133580705,Same class
RESULTS ON THE WATERBIRDS DATASET,0.3580705009276438,"Diff. class
Same domain
Diff. domain + Diff. class
Same domain + Diff. class
Diff. domain + Same class"
RESULTS ON THE WATERBIRDS DATASET,0.35992578849721707,"Diff. class
Diff. domain + Diff. class"
RESULTS ON THE WATERBIRDS DATASET,0.36178107606679033,Diff. domain
RESULTS ON THE WATERBIRDS DATASET,0.36363636363636365,"Same class
Same domain
Resampling (uniform combinations)"
RESULTS ON THE WATERBIRDS DATASET,0.3654916512059369,"Resampling (uniform classes)
Resampling (uniform domains)"
RESULTS ON THE WATERBIRDS DATASET,0.3673469387755102,"Vanilla mixup
Baseline (ERM)"
RESULTS ON THE WATERBIRDS DATASET,0.3692022263450835,Figure 2: Main results on waterbirds.
RESULTS ON THE WATERBIRDS DATASET,0.37105751391465674,"We first observe that vanilla mixup is detrimen-
tal compared to ERM. Resampling with uniform
class/domain combinations is hugely beneficial,
for the reasons explained in Figure 3. The rank-
ing of various criteria for selective sampling is
similar whether with or without mixup. Most in-
terestingly, the best criterion performs similarly,
but no better than the best resampling."
RESULTS ON THE WATERBIRDS DATASET,0.37291280148423006,"■Baselines
■Selective sampling without mixup
■Selective mixup"
RESULTS ON THE WATERBIRDS DATASET,0.3747680890538033,"This suggest that the excellent performance of the best version of selective mixup is here entirely
175"
RESULTS ON THE WATERBIRDS DATASET,0.37662337662337664,"due to resampling. Note that the efficacy of resampling on this dataset is not a new finding [8, 24].
176"
RESULTS ON THE WATERBIRDS DATASET,0.3784786641929499,"What is new is its equivalence with the best variant of selective mixup. We further explain this claim
177"
RESULTS ON THE WATERBIRDS DATASET,0.3803339517625232,"in Figure 3 by examining the proportions of classes and domains sampled by each training method.
178"
RESULTS ON THE WATERBIRDS DATASET,0.3821892393320965,Baseline     ———— Simple resampling ————   ——— Selective sampling or selective mixup ————
RESULTS ON THE WATERBIRDS DATASET,0.38404452690166974,Domains
RESULTS ON THE WATERBIRDS DATASET,0.38589981447124305,Classes ERM 1% 4% 22% 73%
RESULTS ON THE WATERBIRDS DATASET,0.3877551020408163,Domains
RESULTS ON THE WATERBIRDS DATASET,0.38961038961038963,"Uniform
 domains 49% 1% 8% 43%"
RESULTS ON THE WATERBIRDS DATASET,0.3914656771799629,Domains
RESULTS ON THE WATERBIRDS DATASET,0.39332096474953615,Uniform
RESULTS ON THE WATERBIRDS DATASET,0.39517625231910947,classes 48% 2% 3% 47%
RESULTS ON THE WATERBIRDS DATASET,0.3970315398886827,Domains
RESULTS ON THE WATERBIRDS DATASET,0.39888682745825604,↑ Best
RESULTS ON THE WATERBIRDS DATASET,0.4007421150278293,"Uniform
 combinations 24% 24% 26% 26%"
RESULTS ON THE WATERBIRDS DATASET,0.4025974025974026,Domains
RESULTS ON THE WATERBIRDS DATASET,0.4044526901669759,"Diff. domain,"
RESULTS ON THE WATERBIRDS DATASET,0.40630797773654914,any class 49% 1% 8% 43%
RESULTS ON THE WATERBIRDS DATASET,0.40816326530612246,Domains
RESULTS ON THE WATERBIRDS DATASET,0.4100185528756957,"Any domain,"
RESULTS ON THE WATERBIRDS DATASET,0.41187384044526903,diff. class 48% 3% 3% 46%
RESULTS ON THE WATERBIRDS DATASET,0.4137291280148423,Domains
RESULTS ON THE WATERBIRDS DATASET,0.4155844155844156,"Diff. domain,"
RESULTS ON THE WATERBIRDS DATASET,0.4174397031539889,same class 39% 12% 37% 12%
RESULTS ON THE WATERBIRDS DATASET,0.41929499072356213,Domains
RESULTS ON THE WATERBIRDS DATASET,0.42115027829313545,↑ Best
RESULTS ON THE WATERBIRDS DATASET,0.4230055658627087,Same domain
RESULTS ON THE WATERBIRDS DATASET,0.424860853432282,diff. class 36% 38% 13% 13%
RESULTS ON THE WATERBIRDS DATASET,0.4267161410018553,"Resampling uniform combinations gives them all equal weights, just like the worst-group target metric.
Selective mixup with same domain / diff. class also gives equal weights to the classes, while breaking
the spurious pattern between groups and classes, unlike any other criterion."
RESULTS ON THE WATERBIRDS DATASET,0.42857142857142855,"Figure 3:
The sam-
pling ratios of each
class/domain clearly
explain
the
perfor-
mance of the best
methods (waterbirds)."
RESULTS ON THE YEARBOOK DATASET,0.43042671614100186,"4.2
Results on the yearbook dataset
179"
RESULTS ON THE YEARBOOK DATASET,0.4322820037105751,"The difficulty of this dataset comes from a slight class imbalance and the presence of covariate/label
180"
RESULTS ON THE YEARBOOK DATASET,0.43413729128014844,"shift (see Figure 10). The test split contains several domains (time periods). The target metric is the
181"
RESULTS ON THE YEARBOOK DATASET,0.4359925788497217,"worst-domain accuracy. Figure 4 shows that vanilla mixup is slightly detrimental compared to ERM.
182"
RESULTS ON THE YEARBOOK DATASET,0.437847866419295,"Resampling for uniform classes gives a clear improvement because of the class imbalance. With
183"
RESULTS ON THE YEARBOOK DATASET,0.4397031539888683,"selective sampling (no mixup), the only criteria that improve over ERM contain “different class”.
184"
RESULTS ON THE YEARBOOK DATASET,0.44155844155844154,"This is expected because this criterion implicitly resamples for a uniform class distribution.
185"
RESULTS ON THE YEARBOOK DATASET,0.44341372912801486,"64
66
68
70
yearbook: OOD worst-domain accuracy (%)"
RESULTS ON THE YEARBOOK DATASET,0.4452690166975881,Resampling (uniform cl.) + Same class
RESULTS ON THE YEARBOOK DATASET,0.44712430426716143,Same domain + Same class
RESULTS ON THE YEARBOOK DATASET,0.4489795918367347,"Same class
Diff. domain + Same class"
RESULTS ON THE YEARBOOK DATASET,0.45083487940630795,"Diff. class
Same domain
Same domain + Diff. class"
RESULTS ON THE YEARBOOK DATASET,0.45269016697588127,Diff. domain + Diff. class
RESULTS ON THE YEARBOOK DATASET,0.45454545454545453,"Diff. domain
Diff. domain + Diff. class"
RESULTS ON THE YEARBOOK DATASET,0.45640074211502785,"Diff. class
Same domain + Diff. class
Same domain + Same class"
RESULTS ON THE YEARBOOK DATASET,0.4582560296846011,Same domain
RESULTS ON THE YEARBOOK DATASET,0.4601113172541744,"Same class
Diff. domain
Diff. domain + Same class
Resampling (uniform classes)
Resampling (uniform domains)"
RESULTS ON THE YEARBOOK DATASET,0.4619666048237477,"Vanilla mixup
Baseline (ERM)
Figure 4: Main results on yearbook."
RESULTS ON THE YEARBOOK DATASET,0.46382189239332094,"With selective mixup, the “different class”
criterion is not useful, but “same class” per-
forms significantly better than ERM. Since
this criterion alone does not have resampling
effects, it indicates a genuine benefit from
mixup restricted to pairs of the same class."
RESULTS ON THE YEARBOOK DATASET,0.46567717996289426,"■Baselines
■Selective sampling without mixup
■Selective mixup
■Novel combinations of sampling and mixup"
RESULTS ON THE YEARBOOK DATASET,0.4675324675324675,"To investigate whether some of the improvements are due to resampling, we measure the divergence
186"
RESULTS ON THE YEARBOOK DATASET,0.46938775510204084,"between training and test distributions of classes and covariates (details in Appendix A). Figure 5)
187"
RESULTS ON THE YEARBOOK DATASET,0.4712430426716141,"shows first that there is a clear variation among different criteria (• blue dots) i.e. some bring the
188"
RESULTS ON THE YEARBOOK DATASET,0.47309833024118736,"training/test distributions closer to one another. Second, there is a remarkable correlation between the
189"
RESULTS ON THE YEARBOOK DATASET,0.4749536178107607,"test accuracy and the divergence, on both classes and covariates.3 This means that resampling effects
190"
RESULTS ON THE YEARBOOK DATASET,0.47680890538033394,"do occur and also play a part in the best variants of selective mixup.
191"
RESULTS ON THE YEARBOOK DATASET,0.47866419294990725,Distance between training/test distributions of inputs (top row; average cosine distance) and classes (bottom row; KL divergence)
RESULTS ON THE YEARBOOK DATASET,0.4805194805194805,Accuracy (%)
RESULTS ON THE YEARBOOK DATASET,0.48237476808905383,on each OOD test domain 80.5 82.5 65.5 67.5 68.5 70.5 79.0 80.5 87.5 88.5 92.5 94.0 91.5 92.5 86.5 88.0 85.5 87.0
RESULTS ON THE YEARBOOK DATASET,0.4842300556586271,"Domain 1
80.5 82.5"
RESULTS ON THE YEARBOOK DATASET,0.48608534322820035,"Domain 2
65.5 67.5"
RESULTS ON THE YEARBOOK DATASET,0.48794063079777367,"Domain 3
68.5 70.5"
RESULTS ON THE YEARBOOK DATASET,0.4897959183673469,"Domain 4
79.0 80.5"
RESULTS ON THE YEARBOOK DATASET,0.49165120593692024,"Domain 5
87.5 88.5"
RESULTS ON THE YEARBOOK DATASET,0.4935064935064935,"Domain 6
92.5 94.0"
RESULTS ON THE YEARBOOK DATASET,0.49536178107606677,"Domain 7
91.5 92.5"
RESULTS ON THE YEARBOOK DATASET,0.4972170686456401,"Domain 8
86.5 88.0"
RESULTS ON THE YEARBOOK DATASET,0.49907235621521334,"Domain 9
85.5 87.0"
RESULTS ON THE YEARBOOK DATASET,0.5009276437847866,"Baseline (ERM)
Variants of selective sampling/selective mixup
Linear fit"
RESULTS ON THE YEARBOOK DATASET,0.5027829313543599,"Figure 5: Different selection criteria (•) modify the distribution of both covariates and labels (upper
and lower rows). The resulting reductions in divergence between training and test distributions
correlate remarkably well with test performance.3 This confirms the contribution of resampling to the
overall performance of selective mixup."
RESULTS ON THE YEARBOOK DATASET,0.5046382189239332,"Finally, the improvements from simple resampling and the best variant of selective mixup suggest
192"
RESULTS ON THE YEARBOOK DATASET,0.5064935064935064,"a new combination. We train a model with uniform class sampling and selective mixup using the
193"
RESULTS ON THE YEARBOOK DATASET,0.5083487940630798,"“same class” criterion, and obtain performance superior to all existing results (last row in Figure 5).
194"
RESULTS ON THE YEARBOOK DATASET,0.5102040816326531,"This confirms the complementarity of the effects of resampling and within-class selective mixup.
195"
RESULTS ON THE ARXIV DATASET,0.5120593692022264,"4.3
Results on the arXiv dataset
196"
RESULTS ON THE ARXIV DATASET,0.5139146567717996,"This dataset has difficulties similar to yearbook and also many more classes (172). Simple resampling
197"
RESULTS ON THE ARXIV DATASET,0.5157699443413729,"for uniform classes is very bad (literally off the chart in Figure 6) because it overcorrects the imbalance
198"
RESULTS ON THE ARXIV DATASET,0.5176252319109462,"(the test distribution being closer to the training than to a uniform one). Uniform domains is much
199"
RESULTS ON THE ARXIV DATASET,0.5194805194805194,"better since its effect is similar but milder.
200"
RESULTS ON THE ARXIV DATASET,0.5213358070500927,"All variants of selective mixup (■) perform very well, but they improve over ERM even without
201"
RESULTS ON THE ARXIV DATASET,0.5231910946196661,"mixup (■). And the selection criteria rank similarly with or without mixup, suggesting that parts of
202"
RESULTS ON THE ARXIV DATASET,0.5250463821892394,"the improvements of selective mixup is due to the resampling. Given that vanilla mixup also clearly
203"
RESULTS ON THE ARXIV DATASET,0.5269016697588126,"improves over ERM, the performance of selective mixup is explained by cumulative effects of
204"
RESULTS ON THE ARXIV DATASET,0.5287569573283859,"vanilla mixup and resampling effects. This also suggests new combinations of methods (■) among
205"
RESULTS ON THE ARXIV DATASET,0.5306122448979592,"which we find one version marginally better than the best variant of selective mixup (last row).
206"
RESULTS ON THE ARXIV DATASET,0.5324675324675324,"3As expected, the correlation is reversed for the first two test domains in Figure 5 since they are even further
from a uniform class distribution than the average of the training data, as seen in Figure 10."
RESULTS ON THE ARXIV DATASET,0.5343228200371057,"41
42
43
arxiv: OOD worst-domain accuracy (%)"
RESULTS ON THE ARXIV DATASET,0.536178107606679,"Resampling (uniform dom.) + Same class
Resampling (uniform dom.) + Same domain
Resampling (uniform dom.) + Same domain + Diff. class"
RESULTS ON THE ARXIV DATASET,0.5380333951762524,"Resampling (uniform dom.) + Diff. domain + Diff. class
Resampling (uniform dom.) + Diff. domain + Same class"
RESULTS ON THE ARXIV DATASET,0.5398886827458256,"Resampling (uniform dom.) + Diff. class
Resampling (uniform dom.) + Diff. domain"
RESULTS ON THE ARXIV DATASET,0.5417439703153989,Diff. domain + Diff. class
RESULTS ON THE ARXIV DATASET,0.5435992578849722,"Same class
Diff. domain
Same domain
Same domain + Diff. class"
RESULTS ON THE ARXIV DATASET,0.5454545454545454,"Diff. class
Diff. domain + Same class"
RESULTS ON THE ARXIV DATASET,0.5473098330241187,"Diff. domain
Diff. domain + Diff. class"
RESULTS ON THE ARXIV DATASET,0.549165120593692,"Diff. class
Same domain
Same domain + Diff. class"
RESULTS ON THE ARXIV DATASET,0.5510204081632653,"Same class
Diff. domain + Same class
Resampling (uniform domains)"
RESULTS ON THE ARXIV DATASET,0.5528756957328386,Resampling (uniform classes)
RESULTS ON THE ARXIV DATASET,0.5547309833024119,"Vanilla mixup
Baseline (ERM)"
RESULTS ON THE ARXIV DATASET,0.5565862708719852,Figure 6: Main results on arXiv.
RESULTS ON THE ARXIV DATASET,0.5584415584415584,"■Baselines
■Selective sampling without mixup
■Selective mixup
■Novel combinations of sampling and mixup"
RESULTS ON THE ARXIV DATASET,0.5602968460111317,"To investigate the contribution of resam-
pling, we measure the divergence be-
tween training/test class distributions
and plot them against the test accuracy
(Figure 7). We observe a strong correla-
tion across methods. Mixup essentially
offsets the performance by a constant
factor. This suggests again the inde-
pendence of the effects of mixup and
resampling."
RESULTS ON THE ARXIV DATASET,0.562152133580705,"0
0.5
1
1.5
KL div. between training/test distributions of classes 41 42 43 44 45 46 47 48 49"
RESULTS ON THE ARXIV DATASET,0.5640074211502782,OOD Test accuracy (%)
RESULTS ON THE ARXIV DATASET,0.5658627087198516,"Baseline (ERM)
Selective sampling
Selective sampling (oracle class distribution)
Vanilla mixup
Selective mixup
Resampling (uniform domains)
Resampling (uniform classes)
Linear fit to selective sampling"
RESULTS ON THE ARXIV DATASET,0.5677179962894249,Figure 7: Divergence of tr./test class distributions vs. test accuracy.
RESULTS ON THE ARXIV DATASET,0.5695732838589982,"The resampling baselines (••) also roughy agree with a linear fit
to the “selective sampling” points. We therefore hypothesize that
all these methods are mostly addressing label shift. We verify
this hypothesis with the remarkable fit of an additional point (▲)
of a model trained by resampling according to the test set class
distribution, i.e. cheating."
RESULTS ON THE ARXIV DATASET,0.5714285714285714,"It represents an upper bound that might be achievable in future
work with methods for label shift [1, 17]."
RESULTS ON THE ARXIV DATASET,0.5732838589981447,"We replicated these observations on every test domain of this
dataset (Figure 14 in the appendix)."
RESULTS ON THE CIVILCOMMENTS DATASET,0.575139146567718,"4.4
Results on the civilComments dataset
207"
RESULTS ON THE CIVILCOMMENTS DATASET,0.5769944341372912,"This dataset mimics a subpopulation shift because the worst-group metric requires high accuracy on
208"
RESULTS ON THE CIVILCOMMENTS DATASET,0.5788497217068646,"classes and domains under-represented in the training data. It also contains an implicit correlation
209"
RESULTS ON THE CIVILCOMMENTS DATASET,0.5807050092764379,"shift because any class/domain association (e.g. “Christian” comments labeled as toxic more often
210"
RESULTS ON THE CIVILCOMMENTS DATASET,0.5825602968460112,"than not) becomes spurious when evaluating individual class/domain combinations.
211"
RESULTS ON THE CIVILCOMMENTS DATASET,0.5844155844155844,"40
50
60
70
civilComments: OOD worst-group accuracy (%)"
RESULTS ON THE CIVILCOMMENTS DATASET,0.5862708719851577,Resampling (uniform comb.) + Same domain + Diff. class
RESULTS ON THE CIVILCOMMENTS DATASET,0.588126159554731,"Resampling (uniform comb.) + Diff. domain
Resampling (uniform comb.) + Diff. domain + Same class"
RESULTS ON THE CIVILCOMMENTS DATASET,0.5899814471243042,Resampling (uniform comb.) + Diff. domain + Diff. class
RESULTS ON THE CIVILCOMMENTS DATASET,0.5918367346938775,"Resampling (uniform comb.) + Diff. class
Resampling (uniform comb.) + Same class
Resampling (uniform comb.) + Same domain"
RESULTS ON THE CIVILCOMMENTS DATASET,0.5936920222634509,Same domain + Diff. class
RESULTS ON THE CIVILCOMMENTS DATASET,0.5955473098330241,Same class
RESULTS ON THE CIVILCOMMENTS DATASET,0.5974025974025974,"Diff. class
Same domain"
RESULTS ON THE CIVILCOMMENTS DATASET,0.5992578849721707,"Diff. domain
Diff. domain + Diff. class
Diff. domain + Same class
Resampling (uniform comb.) + Same domain + Diff. class
Resampling (uniform comb.) + Diff. domain + Same class"
RESULTS ON THE CIVILCOMMENTS DATASET,0.601113172541744,"Resampling (uniform comb.) + Same class
Resampling (uniform comb.) + Diff. domain + Diff. class"
RESULTS ON THE CIVILCOMMENTS DATASET,0.6029684601113172,Resampling (uniform comb.) + Diff. class
RESULTS ON THE CIVILCOMMENTS DATASET,0.6048237476808905,"Same domain + Diff. class
Resampling (uniform comb.) + Diff. domain
Resampling (uniform comb.) + Same domain"
RESULTS ON THE CIVILCOMMENTS DATASET,0.6066790352504638,"Diff. class
Diff. domain + Diff. class"
RESULTS ON THE CIVILCOMMENTS DATASET,0.608534322820037,"Same class
Same domain"
RESULTS ON THE CIVILCOMMENTS DATASET,0.6103896103896104,"Diff. domain
Diff. domain + Same class
Resampling (uniform combinations)"
RESULTS ON THE CIVILCOMMENTS DATASET,0.6122448979591837,"Resampling (uniform classes)
Resampling (uniform domains)"
RESULTS ON THE CIVILCOMMENTS DATASET,0.614100185528757,"Vanilla mixup
Baseline (ERM)
Figure 8: Main results on civilComments."
RESULTS ON THE CIVILCOMMENTS DATASET,0.6159554730983302,"For the above reasons, it makes sense that
resampling for uniform classes or combi-
nations greatly improves performance, as
shown in prior work [8]."
RESULTS ON THE CIVILCOMMENTS DATASET,0.6178107606679035,"With selective mixup (■), some criterion
(same domain/diff. class) performs clearly
above all others. But it works even better
without mixup! (■) Among many other
variations, none surpasses the uniform-
combinations baseline."
RESULTS ON THE CIVILCOMMENTS DATASET,0.6196660482374768,"■Baselines
■Selective sampling without mixup
■Selective mixup
■Novel combinations of sampling and mixup"
RESULTS ON THE MIMIC-READMISSION DATASET,0.62152133580705,"4.5
Results on the MIMIC-Readmission dataset
212"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6233766233766234,"This dataset contains a class imbalance (about 78/22% in training data), label shift (the distribution
213"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6252319109461967,"being more balanced in the test split), and possibly covariate shift. It is unclear whether the task
214"
RESULTS ON THE MIMIC-READMISSION DATASET,0.62708719851577,"is causal or anticausal (labels causing the features) because the inputs contain both diagnoses and
215"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6289424860853432,"treatments. The target metric is the area under the ROC curve (AUROC) which gives equal importance
216"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6307977736549165,"to both classes. We report the worst-domain AUROC, i.e. the lowest value across test time periods.
217"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6326530612244898,"Vanilla mixup performs a bit better than ERM. Because of the class imbalance, resampling for uniform
218"
RESULTS ON THE MIMIC-READMISSION DATASET,0.634508348794063,"classes also improves ERM. As expected, this is perfectly equivalent to the selective sampling criterion
219"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6363636363636364,"“diffClass” and they perform therefore equally well. Adding mixup is yet a bit better, which suggests
220"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6382189239332097,"again that the performance of selective mixup is merely the result of the independent effects of
221"
RESULTS ON THE MIMIC-READMISSION DATASET,0.640074211502783,"vanilla mixup and resampling. We further verify this explanation with the novel combination of
222"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6419294990723562,"simple resampling and vanilla mixup, and observe almost no difference whether the mixing operation
223"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6437847866419295,"is performed or not (last two rows in Figure 9).
224"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6456400742115028,"52
53
54
55
56
57
MIMIC: OOD worst-d. AUROC (%)"
RESULTS ON THE MIMIC-READMISSION DATASET,0.647495361781076,"Resampling (uniform cl.) + vanilla mixup
Resampling (uniform cl.) + concatenated pairs"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6493506493506493,Diff. domain + Diff. class
RESULTS ON THE MIMIC-READMISSION DATASET,0.6512059369202227,"Diff. class
Same domain + Diff. class
Same domain + Diff. class"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6530612244897959,"Diff. class
Diff. domain + Diff. class
Resampling (uniform classes)"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6549165120593692,"Vanilla mixup
Baseline (ERM)"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6567717996289425,Figure 9: Main results on MIMIC-Readmission.
RESULTS ON THE MIMIC-READMISSION DATASET,0.6586270871985158,"■Baselines
■Selective sampling without mixup
■Selective mixup
■Novel combinations of sampling and mixup"
RESULTS ON THE MIMIC-READMISSION DATASET,0.660482374768089,"To further support the claim that these methods mostly address label shift, we report in Table 1 the
225"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6623376623376623,"proportion of the majority class in the training and test data. We observe that the distribution sampled
226"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6641929499072357,"by the best training methods brings it much closer to that of the test data.
227"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6660482374768089,"Proportion of majority class
(%)"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6679035250463822,"In the dataset (training)
78.2
In the dataset (validation)
77.8
In the dataset (OOD test)
66.5"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6697588126159555,"Sampled by different training methods
Resampling (uniform classes)
50.0
Diff. domain + diff. class
50.0
Diff. class
50.1
Same domain + Diff. class
49.9
Resampling (uniform cl.) + concatenated pairs
64.3
Resampling (uniform cl.) + vanilla mixup
64.3"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6716141001855288,"Table 1: The performance of the various methods
on MIMIC-Readmission is explained by their correc-
tion of a class imbalance. The best training methods
(boxed numbers) sample the majority class in a pro-
portion much closer to that of the test data."
RESULTS ON THE MIMIC-READMISSION DATASET,0.673469387755102,"4.6
Evidence of a “regression toward the mean” in the data
228"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6753246753246753,"We hypothesized in Section 3 that the resampling benefits are due to a “regression toward the mean”
229"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6771799628942486,"across training and test splits. We now check for this property and find indeed a shift toward uniform
230"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6790352504638218,"class distributions in all datasets studied. For the Wild-Time datasets, we plot in Figure 10 the ratio
231"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6808905380333952,"of the minority class (binary tasks: yearbook, MIMIC) and class distribution entropy (multiclass task:
232"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6827458256029685,"arxiv). Finding this property in all three datasets agrees with the proposed explanation and the fact
233"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6846011131725418,"that we selected them because they previously showed improvements with selective mixup in [32].
234"
RESULTS ON THE MIMIC-READMISSION DATASET,0.686456400742115,"In the waterbirds and civilComments datasets, the shift toward uniformity¨also holds, but artificially.
235"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6883116883116883,"The training data contains imbalanced groups (class/domain combinations) whereas the evaluation
236"
RESULTS ON THE MIMIC-READMISSION DATASET,0.6901669758812616,"with worst-group accuracy implicitly gives uniform importance to all groups.
237"
RELATED WORK,0.6920222634508348,"5
Related work
238"
RELATED WORK,0.6938775510204082,"Mixup and variants. Mixup was originally introduced in [36] and numerous variants followed [2].
239"
RELATED WORK,0.6957328385899815,"Many propose modality-specific mixing operations: CutMix [34] replaces linear combinations
240"
RELATED WORK,0.6975881261595547,"with collages of image patches, Fmix [6] combines image regions based on frequency contents,
241"
RELATED WORK,0.699443413729128,"AlignMixup [29] combines images after spatial alignment. Manifold-mixup [30] replaces the mixing
242"
RELATED WORK,0.7012987012987013,"in input space with the mixing of learned representations, making it applicable to text embeddings.
243"
RELATED WORK,0.7031539888682746,"1930
1970
2010
Years (yearbook dataset) 0 0.2 0.4 0.6"
RELATED WORK,0.7050092764378478,Ratio of minority class
RELATED WORK,0.7068645640074211,(0.5=uniform)
RELATED WORK,0.7087198515769945,"Training domains (ID)
Test domains (OOD)"
RELATED WORK,0.7105751391465677,"2008
2014
2019
Years (MIMIC-readmission dataset) 0 0.2 0.4 0.6"
RELATED WORK,0.712430426716141,Ratio of minority class
RELATED WORK,0.7142857142857143,(0.5=uniform)
RELATED WORK,0.7161410018552876,"Training domains (ID)
Test domains (OOD)"
RELATED WORK,0.7179962894248608,"2007
2014
2019
Years (arxiv dataset) 0.7 0.8 0.9 1"
RELATED WORK,0.7198515769944341,Normalized entropy of class
RELATED WORK,0.7217068645640075,distribution (1.0=uniform)
RELATED WORK,0.7235621521335807,"Training domains (ID)
Test domains (OOD)"
RELATED WORK,0.725417439703154,"Figure 10:
The class
distribution shifts toward
uniformity in these Wild-
Time
datasets,
which
agrees with the explana-
tion that resampling ben-
efits are due to a “regres-
sion toward the mean”."
RELATED WORK,0.7272727272727273,"Mixup for OOD generalization. Mixup has been integrated into existing techniques for domain
244"
RELATED WORK,0.7291280148423006,"adaptation (DomainMix [31]), domain generalization (FIXED [20]), and with meta learning (Reg-
245"
RELATED WORK,0.7309833024118738,"Mixup [23]). This paper focuses on variants we call “selective mixup” that use non-uniform sampling
246"
RELATED WORK,0.7328385899814471,"of the pairs of mixed examples. LISA [33] proposes two heuristics, same-class/different-domain and
247"
RELATED WORK,0.7346938775510204,"vice versa, used in proportions tuned by cross-validation on each dataset. Palakkadavath et al. [22]
248"
RELATED WORK,0.7365491651205937,"use same-class pairs and an additional objective to encourage invariance of the representations to the
249"
RELATED WORK,0.738404452690167,"mixing. CIFair [28] uses same-class pairs with a contrastive objective to improve algorithmic fairness.
250"
RELATED WORK,0.7402597402597403,"SelecMix [7] proposes a selection heuristic to handle biased training data: same class/different
251"
RELATED WORK,0.7421150278293135,"biased attribute, or vice versa. DomainMix [31] uses different-domain pairs for domain adaptation.
252"
RELATED WORK,0.7439703153988868,"DRE [15] uses same-class/different-domain pairs and regularize their Grad-CAM explanations to
253"
RELATED WORK,0.7458256029684601,"improve OOD generalization. SDMix [19] applies mixup on examples from different domains with
254"
RELATED WORK,0.7476808905380334,"other improvements to improve cross-domain generalization for activity recognition.
255"
RELATED WORK,0.7495361781076066,"Explaining the benefits of mixup has invoked regularization [37] and augmentation [11] effects,
256"
RELATED WORK,0.75139146567718,"the introduction of label noise [18], and the learning of rare features [38]. These works focus on the
257"
RELATED WORK,0.7532467532467533,"mixing and in-domain generalization, whereas we focus on the selection and OOD generalization.
258"
RELATED WORK,0.7551020408163265,"Training on resampled data. We find that selective mixup is sometimes equivalent to training
259"
RELATED WORK,0.7569573283858998,"on resampled or reweighted data. Both are standard tools [10, 13] to handle distribution shifts
260"
RELATED WORK,0.7588126159554731,"in a domain adaptation setting, also known as importance-weighted empirical risk minimization
261"
RELATED WORK,0.7606679035250464,"(IW-ERM) [25, 4]. For covariate shift, IW-ERM trains a model with a weight or sampling probability
262"
RELATED WORK,0.7625231910946196,"on each training point x as its likelihood ratio ptarget(x)/psource(x). Likewise with labels y and
263"
RELATED WORK,0.764378478664193,"ptarget(y)/psource(y) for label shift [1, 17], Recently, [8, 24] showed that reweighting and resampling
264"
RELATED WORK,0.7662337662337663,"are competitive with the state of the art on multiple OOD and label-shift benchmarks [3].
265"
CONCLUSIONS AND OPEN QUESTIONS,0.7680890538033395,"6
Conclusions and open questions
266"
CONCLUSIONS AND OPEN QUESTIONS,0.7699443413729128,"Conclusions. This paper helps understand selective mixup, which is one of the most successful and
267"
CONCLUSIONS AND OPEN QUESTIONS,0.7717996289424861,"general methods for distribution shifts. We showed unambiguously that much of the improvements
268"
CONCLUSIONS AND OPEN QUESTIONS,0.7736549165120594,"were actually unrelated to the mixing operation and could be obtained with much simpler, well-known
269"
CONCLUSIONS AND OPEN QUESTIONS,0.7755102040816326,"resampling methods. On datasets where mixup does bring benefits, we could then obtain even better
270"
CONCLUSIONS AND OPEN QUESTIONS,0.7773654916512059,"results by combining the independent effects of the best mixup and resampling variants.
271"
CONCLUSIONS AND OPEN QUESTIONS,0.7792207792207793,"Limitations. We focused on the simplest version selective mixup as described by Yao et al. [33].
272"
CONCLUSIONS AND OPEN QUESTIONS,0.7810760667903525,"Many papers combine the principle with modifications to the learning objective [7, 15, 19, 22, 28, 31].
273"
CONCLUSIONS AND OPEN QUESTIONS,0.7829313543599258,"Resampling likely plays a role in these methods too but this claim requires further investigation.
274"
CONCLUSIONS AND OPEN QUESTIONS,0.7847866419294991,"We evaluated “only” five datasets. Since we introduced simple ablations that can single out the effects
275"
CONCLUSIONS AND OPEN QUESTIONS,0.7866419294990723,"of resampling, we hope to see future re-evaluations of other datasets.
276"
CONCLUSIONS AND OPEN QUESTIONS,0.7884972170686456,"Because we selected datasets that had previously shown benefits with selective mixup, we could not
277"
CONCLUSIONS AND OPEN QUESTIONS,0.7903525046382189,"verify the predicted failure mode when there is no “regression toward the mean” in the data.
278"
CONCLUSIONS AND OPEN QUESTIONS,0.7922077922077922,"Finally, this work is not about designing new algorithms to surpass the state of the art. Our focus is
279"
CONCLUSIONS AND OPEN QUESTIONS,0.7940630797773655,"on improving the scientific understanding of existing mixup strategies and their limitations.
280"
CONCLUSIONS AND OPEN QUESTIONS,0.7959183673469388,"Open questions. Our results leave open the question of the applicability of selective mixup to
281"
CONCLUSIONS AND OPEN QUESTIONS,0.7977736549165121,"real situations. The “regression toward the mean” explanation indicates that much of the observed
282"
CONCLUSIONS AND OPEN QUESTIONS,0.7996289424860853,"improvements are accidental since they rely on an artefact of some datasets. In real deployments,
283"
CONCLUSIONS AND OPEN QUESTIONS,0.8014842300556586,"distribution shifts cannot be foreseen in nature nor magnitude. This is a reminder of the relevance of
284"
CONCLUSIONS AND OPEN QUESTIONS,0.8033395176252319,"Goodhart’s law to machine learning [26] and of the risk of overfitting to popular benchmarks [16].
285"
REFERENCES,0.8051948051948052,"References
286"
REFERENCES,0.8070500927643784,"[1] Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized
287"
REFERENCES,0.8089053803339518,"learning for domain adaptation under label shifts. arXiv preprint arXiv:1903.09734, 2019.
288"
REFERENCES,0.8107606679035251,"[2] Chengtai Cao, Fan Zhou, Yurou Dai, and Jianping Wang. A survey of mix-based data augmen-
289"
REFERENCES,0.8126159554730983,"tation: Taxonomy, methods, applications, and explainability. arXiv preprint arXiv:2212.10888,
290"
REFERENCES,0.8144712430426716,"2022.
291"
REFERENCES,0.8163265306122449,"[3] Saurabh Garg, Nick Erickson, James Sharpnack, Alex Smola, Sivaraman Balakrishnan, and
292"
REFERENCES,0.8181818181818182,"Zachary C Lipton. Rlsbench: Domain adaptation under relaxed label shift. arXiv preprint
293"
REFERENCES,0.8200371057513914,"arXiv:2302.03020, 2023.
294"
REFERENCES,0.8218923933209648,"[4] Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and
295"
REFERENCES,0.8237476808905381,"Bernhard Schölkopf. Covariate shift by kernel mean matching. Dataset shift in machine
296"
REFERENCES,0.8256029684601113,"learning, 2009.
297"
REFERENCES,0.8274582560296846,"[5] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint
298"
REFERENCES,0.8293135435992579,"arXiv:2007.01434, 2020.
299"
REFERENCES,0.8311688311688312,"[6] Ethan Harris, Antonia Marcu, Matthew Painter, Mahesan Niranjan, Adam Prügel-Bennett,
300"
REFERENCES,0.8330241187384044,"and Jonathon Hare. Fmix: Enhancing mixed sample data augmentation. arXiv preprint
301"
REFERENCES,0.8348794063079777,"arXiv:2002.12047, 2020.
302"
REFERENCES,0.8367346938775511,"[7] Inwoo Hwang, Sangjun Lee, Yunhyeok Kwak, Seong Joon Oh, Damien Teney, Jin-Hwa Kim,
303"
REFERENCES,0.8385899814471243,"and Byoung-Tak Zhang. Selecmix: Debiased learning by contradicting-pair sampling. arXiv
304"
REFERENCES,0.8404452690166976,"preprint arXiv:2211.02291, 2022.
305"
REFERENCES,0.8423005565862709,"[8] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data
306"
REFERENCES,0.8441558441558441,"balancing achieves competitive worst-group-accuracy. In Conference on Causal Learning and
307"
REFERENCES,0.8460111317254174,"Reasoning, 2022.
308"
REFERENCES,0.8478664192949907,"[9] Md Amirul Islam, Matthew Kowal, Konstantinos G Derpanis, and Neil DB Bruce. Segmix: Co-
309"
REFERENCES,0.849721706864564,"occurrence driven mixup for semantic segmentation and adversarial robustness. International
310"
REFERENCES,0.8515769944341373,"Journal of Computer Vision, 131(3):701–716, 2023.
311"
REFERENCES,0.8534322820037106,"[10] Nathalie Japkowicz. The class imbalance problem: Significance and strategies. In Proc. of the
312"
REFERENCES,0.8552875695732839,"Int’l Conf. on artificial intelligence, volume 56, pages 111–117, 2000.
313"
REFERENCES,0.8571428571428571,"[11] Masanari Kimura. Why mixup improves the model performance. In International Conference
314"
REFERENCES,0.8589981447124304,"on Artificial Neural Networks (ICANN), 2021.
315"
REFERENCES,0.8608534322820037,"[12] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay
316"
REFERENCES,0.862708719851577,"Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al.
317"
REFERENCES,0.8645640074211502,"Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine
318"
REFERENCES,0.8664192949907236,"Learning, 2021.
319"
REFERENCES,0.8682745825602969,"[13] Miroslav Kubat, Stan Matwin, et al. Addressing the curse of imbalanced training sets: one-sided
320"
REFERENCES,0.8701298701298701,"selection. In Icml, volume 97, page 179. Citeseer, 1997.
321"
REFERENCES,0.8719851576994434,"[14] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier
322"
REFERENCES,0.8738404452690167,"domain generalization. In IEEE International Conference on Computer Vision, pages 5542–
323"
REFERENCES,0.87569573283859,"5550, 2017.
324"
REFERENCES,0.8775510204081632,"[15] Tang Li, Fengchun Qiao, Mengmeng Ma, and Xi Peng. Are data-driven explanations robust
325"
REFERENCES,0.8794063079777366,"against out-of-distribution data? arXiv preprint arXiv:2303.16390, 2023.
326"
REFERENCES,0.8812615955473099,"[16] Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, and Ludwig Schmidt. Are we learning
327"
REFERENCES,0.8831168831168831,"yet? a meta review of evaluation failures across machine learning. In Thirty-fifth Conference on
328"
REFERENCES,0.8849721706864564,"Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.
329"
REFERENCES,0.8868274582560297,"[17] Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift
330"
REFERENCES,0.8886827458256029,"with black box predictors. In International conference on machine learning, 2018.
331"
REFERENCES,0.8905380333951762,"[18] Zixuan Liu, Ziqiao Wang, Hongyu Guo, and Yongyi Mao. Over-training with mixup may hurt
332"
REFERENCES,0.8923933209647495,"generalization. arXiv preprint arXiv:2303.01475, 2023.
333"
REFERENCES,0.8942486085343229,"[19] Wang Lu, Jindong Wang, Yiqiang Chen, Sinno Jialin Pan, Chunyu Hu, and Xin Qin. Semantic-
334"
REFERENCES,0.8961038961038961,"discriminative mixup for generalizable sensor-based cross-domain activity recognition. Pro-
335"
REFERENCES,0.8979591836734694,"ceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2022.
336"
REFERENCES,0.8998144712430427,"[20] Wang Lu, Jindong Wang, Han Yu, Lei Huang, Xiang Zhang, Yiqiang Chen, and Xing Xie.
337"
REFERENCES,0.9016697588126159,"Fixed: Frustratingly easy domain generalization with mixup. arXiv preprint arXiv:2211.05228,
338"
REFERENCES,0.9035250463821892,"2022.
339"
REFERENCES,0.9053803339517625,"[21] Linghui Meng, Jin Xu, Xu Tan, Jindong Wang, Tao Qin, and Bo Xu. Mixspeech: Data
340"
REFERENCES,0.9072356215213359,"augmentation for low-resource automatic speech recognition. In ICASSP 2021-2021 IEEE
341"
REFERENCES,0.9090909090909091,"International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7008–
342"
REFERENCES,0.9109461966604824,"7012. IEEE, 2021.
343"
REFERENCES,0.9128014842300557,"[22] Ragja Palakkadavath, Thanh Nguyen-Tang, Sunil Gupta, and Svetha Venkatesh. Improving
344"
REFERENCES,0.9146567717996289,"domain generalization with interpolation robustness. In NeurIPS 2022 Workshop on Distribution
345"
REFERENCES,0.9165120593692022,"Shifts: Connecting Methods and Applications, 2022.
346"
REFERENCES,0.9183673469387755,"[23] Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip HS Torr, and Puneet K Dokania. Regmixup:
347"
REFERENCES,0.9202226345083488,"Mixup as a regularizer can surprisingly improve accuracy and out distribution robustness. arXiv
348"
REFERENCES,0.922077922077922,"preprint arXiv:2206.14502, 2022.
349"
REFERENCES,0.9239332096474954,"[24] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally
350"
REFERENCES,0.9257884972170687,"robust neural networks for group shifts: On the importance of regularization for worst-case
351"
REFERENCES,0.9276437847866419,"generalization. arXiv preprint arXiv:1911.08731, 2019.
352"
REFERENCES,0.9294990723562152,"[25] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the
353"
REFERENCES,0.9313543599257885,"log-likelihood function. Journal of statistical planning and inference, 2000.
354"
REFERENCES,0.9332096474953617,"[26] Damien Teney, Ehsan Abbasnejad, Kushal Kafle, Robik Shrestha, Christopher Kanan, and
355"
REFERENCES,0.935064935064935,"Anton Van Den Hengel. On the value of out-of-distribution testing: An example of goodhart’s
356"
REFERENCES,0.9369202226345084,"law. Advances in Neural Information Processing Systems, 33:407–417, 2020.
357"
REFERENCES,0.9387755102040817,"[27] Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Abbasnejad. Id and ood performance are
358"
REFERENCES,0.9406307977736549,"sometimes inversely correlated on real-world datasets. arXiv preprint arXiv:2209.00613, 2022.
359"
REFERENCES,0.9424860853432282,"[28] Huan Tian, Bo Liu, Tianqing Zhu, Wanlei Zhou, and S Yu Philip. Cifair: Constructing
360"
REFERENCES,0.9443413729128015,"continuous domains of invariant features for image fair classifications. Knowledge-Based
361"
REFERENCES,0.9461966604823747,"Systems, 2023.
362"
REFERENCES,0.948051948051948,"[29] Shashanka Venkataramanan, Ewa Kijak, Laurent Amsaleg, and Yannis Avrithis. Alignmixup:
363"
REFERENCES,0.9499072356215214,"Improving representations by interpolating aligned features. In IEEE/CVF Conference on
364"
REFERENCES,0.9517625231910947,"Computer Vision and Pattern Recognition, pages 19174–19183, 2022.
365"
REFERENCES,0.9536178107606679,"[30] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-
366"
REFERENCES,0.9554730983302412,"Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states.
367"
REFERENCES,0.9573283858998145,"In International Conference on Machine Learning, 2019.
368"
REFERENCES,0.9591836734693877,"[31] Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang.
369"
REFERENCES,0.961038961038961,"Adversarial domain adaptation with domain mixup. In AAAI Conference on Artificial Intelli-
370"
REFERENCES,0.9628942486085343,"gence, 2020.
371"
REFERENCES,0.9647495361781077,"[32] Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei W Koh, and Chelsea Finn.
372"
REFERENCES,0.9666048237476809,"Wild-time: A benchmark of in-the-wild distribution shift over time. Proc. Advances in Neural
373"
REFERENCES,0.9684601113172542,"Inf. Process. Syst., 2022.
374"
REFERENCES,0.9703153988868275,"[33] Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Im-
375"
REFERENCES,0.9721706864564007,"proving out-of-distribution robustness via selective augmentation. In International Conference
376"
REFERENCES,0.974025974025974,"on Machine Learning, 2022.
377"
REFERENCES,0.9758812615955473,"[34] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon
378"
REFERENCES,0.9777365491651205,"Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In
379"
REFERENCES,0.9795918367346939,"IEEE/CVF International Conference on Computer Vision, pages 6023–6032, 2019.
380"
REFERENCES,0.9814471243042672,"[35] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon
381"
REFERENCES,0.9833024118738405,"Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In
382"
REFERENCES,0.9851576994434137,"Proceedings of the IEEE/CVF international conference on computer vision, pages 6023–6032,
383"
REFERENCES,0.987012987012987,"2019.
384"
REFERENCES,0.9888682745825603,"[36] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond
385"
REFERENCES,0.9907235621521335,"empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.
386"
REFERENCES,0.9925788497217068,"[37] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does
387"
REFERENCES,0.9944341372912802,"mixup help with robustness and generalization? arXiv preprint arXiv:2010.04819, 2020.
388"
REFERENCES,0.9962894248608535,"[38] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. The benefits of mixup for feature learning.
389"
REFERENCES,0.9981447124304267,"arXiv preprint arXiv:2303.08433, 2023.
390"
