Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009328358208955224,"Many tasks in explainable machine learning, such as data valuation and feature
attribution, perform expensive computation for each data point and are intractable
for large datasets. These methods require efficient approximations, and although
amortizing the process by learning a network to directly predict the desired output
is a promising solution, training such models with exact labels is often infeasible.
We therefore explore training amortized models with noisy labels, and we find
that this is inexpensive and surprisingly effective. Through theoretical analysis
of the label noise and experiments with various models and datasets, we show
that this approach tolerates high noise levels and significantly accelerates several
feature attribution and data valuation methods, often yielding an order of magnitude
speedup over existing approaches."
INTRODUCTION,0.0018656716417910447,"1
Introduction"
INTRODUCTION,0.002798507462686567,"Many tasks in explainable machine learning (XML) perform some form of costly computation for
every data point in a dataset. For example, common tasks include assessing individual data points’
impact on a model’s accuracy [33], or quantifying each input feature’s influence on individual model
predictions [68]. Many of these techniques are prohibitively expensive: in particular, those with
game-theoretic formulations have exponential complexity in the number of features or data points,
making their exact calculation intractable [89, 4]."
INTRODUCTION,0.0037313432835820895,"Accelerating these methods is therefore a topic of great practical importance. This has been addressed
primarily with Monte Carlo approximations [96, 16, 74], which are faster than brute-force calculations
but can be slow to converge and impractical for large datasets. Alternatively, a promising idea is to
amortize the computation, or to approximate each data point’s output with a learned model, typically
a deep neural network [2]. For example, in the feature attribution context, we can train an explainer
model to predict Shapley values that describe how each feature affects a classifier’s prediction [50]."
INTRODUCTION,0.0046641791044776115,"There are several reasons why amortization is appealing, particularly with neural networks: similar
data points often have similar outputs, pretrained networks extract relevant features and can be
efficiently fine-tuned, and if the combined training and inference time is low then amortization can
be faster than computing the object of interest (e.g., data valuation scores) for the entire dataset.
However, it is not obvious how to train such amortized models, because standard supervised learning
requires a dataset of ground truth labels that can be intractable to generate. Our goal here is therefore
to explore efficiently training amortized models when exact labels are costly. Our main insight is that"
INTRODUCTION,0.005597014925373134,∗Equal contribution. †Equal advising.
INTRODUCTION,0.0065298507462686565,"Figure 1: Diagram of stochastic amortization. Left: using a dataset with noisy labels ˜a(b) (e.g.,
images and data valuation estimates), we can train an amortized model that accurately estimates the
true outputs a(b) (e.g., data valuation scores). Right: the default approach of running an expensive
approximation algorithm for each example (e.g., a Monte Carlo estimator with many samples [33])."
INTRODUCTION,0.007462686567164179,"amortization is surprisingly effective with noisy labels: we train with inexpensive estimates of the
true labels, and we find that this is theoretically justified when the estimates are unbiased."
INTRODUCTION,0.008395522388059701,"We refer to this approach as stochastic amortization (Figure 1), and we find that it is applicable
to a variety of XML tasks. In particular, we show that it is effective for feature attribution with
Shapley values [68], Banzhaf values [11] and LIME [83]; for several formulations of data valuation
[33, 35, 60, 103]; and to data attribution with datamodels [47]. Our experiments demonstrate
significant speedups for several of these tasks: we find that amortizing across an entire dataset with
noisy labels is often more efficient than current per-example approximations, especially for large
datasets, and that amortized feature and data attribution models generalize well to unseen examples."
INTRODUCTION,0.009328358208955223,Our contributions in this work are the following:
INTRODUCTION,0.010261194029850746,"• We present the idea of stochastic amortization, or training amortized models with noisy labels.
We analyze the role of noise from the label generation process and show theoretically that it is
sufficient to use unbiased estimates of the ground truth labels (Section 3). We find that non-zero
bias in the labels leads to learning an incorrect function, but that variance in the labels plays a more
benign role of slowing optimization."
INTRODUCTION,0.011194029850746268,"• We identify a range of applications for stochastic amortization in XML. Our theory only requires
unbiased estimation of the task’s true labels, and we find that such estimators exist for several
feature attribution and data valuation methods (Section 4)."
INTRODUCTION,0.012126865671641791,"• Experimentally, we test multiple estimators for Shapley value feature attributions and find that
amortization works when the labels are unbiased (Section 5). We also verify that amortization is
effective for Banzhaf values and LIME. For data valuation, we apply amortization to Data Shapley
and show that it allows us to scale this approach to larger datasets than in previous works."
INTRODUCTION,0.013059701492537313,"• Throughout our experiments, we also analyze the scaling behavior with respect to the amount
of training data and the quality of the noisy labels. In general, we find that amortization is more
efficient than per-example computation when the training set used for amortization contains at
least a moderate number of data points (e.g., >1K for data valuation)."
INTRODUCTION,0.013992537313432836,"Overall, our work shows the potential for accelerating many computationally intensive XML tasks
with the same simple approach: amortizing with noisy, unbiased labels."
BACKGROUND,0.014925373134328358,"2
Background"
BACKGROUND,0.01585820895522388,"We first introduce the basic idea of amortization in ML, which we discuss in a general setting before
considering any specific XML tasks. Consider a scenario where we repeat similar computation for
a large number of data points. We represent this general setup with a context variable b ∈B and a
per-context output a(b) ∈A. For example, these can be an image and its data valuation score, or an
image and its feature attributions. Amortization can be used with arbitrary domains, but we assume
Euclidean spaces, or A ⊆Rm and B ⊆Rd, because many XML tasks involve real-valued outputs."
BACKGROUND,0.016791044776119403,"The computation performed for each context b ∈B can be arbitrary as well. In some cases a(b)
is an intractable expectation, in which case we would typically approximate it with a Monte Carlo
estimator [96, 33]. In other situations it is the solution to an optimization problem [91, 2], in which
case we can define a(b) via a parametric objective h : A × B 7→R:"
BACKGROUND,0.017723880597014924,"a(b) ≡arg min
a′∈A
h(a′; b).
(1)"
BACKGROUND,0.018656716417910446,"We do not require a specific formulation for a(b) in this work, but we will see that both the expectation
and optimization setups provide useful perspectives on our proposal of training with noisy labels."
BACKGROUND,0.01958955223880597,"In situations with repeated computation, our two options are generally to (i) perform the computation
separately for each b ∈B, or (ii) amortize the computation by predicting the output with a learned
model. We typically implement the latter with a neural network a(b; θ), and our goal is to train it
such that a(b; θ) ≈a(b). In training these models, the main challenge occurs when the per-instance
computation is costly: specifically, it is not obvious how to train a(b; θ) without a dataset of ground
truth solutions a(b), which can be too slow to generate for many XML methods [68, 33]. We address
this challenge in Section 3, where we prove that amortization tolerates training with noisy labels."
RELATED WORK,0.020522388059701493,"2.1
Related work"
RELATED WORK,0.021455223880597014,"The general idea of amortized computation captures many tasks arising in physics, engineering,
control and ML [2]. For example, amortization is prominent in variational inference [56, 82], meta
learning [38, 29, 80] and reinforcement learning [45, 64, 39]. In the XML context, many recent works
have explored amortization to accelerate costly per-datapoint calculations [12, 108, 49, 50, 18, 19].
Some of these works are reviewed by [13], and we offer a detailed overview in Appendix A."
RELATED WORK,0.022388059701492536,"For feature attributions, two works propose predicting Shapley values by training with a custom
weighted least squares loss [50, 18]; our simpler approach can use any unbiased estimator and
resembles a standard regression task. Two other works suggest modeling feature attributions with
supervised learning [87, 14]; these recommend training with exact or high-quality labels, whereas
we recognize the potential to use noisy labels that can be generated orders of magnitude faster.
Concurrently, Zhang et al. [110] proposed training with a custom estimator for Shapley value feature
attributions; our work is similar but derives stochastic amortization algorithms for a range of settings,
including the use of any unbiased estimator (Section 3) and usage for various XML tasks including
data valuation (Section 4)."
RELATED WORK,0.02332089552238806,"For data valuation, two works consider predicting data valuation scores with supervised learning
[35, 36], but these also use near-exact labels that limit the applicability to large datasets. Concurrently,
Li and Yu [65] propose a family of data valuation estimators and a learning-based approach analogous
to [50] but for data valuation; our approach uses a simpler training loss that works with any unbiased
estimator, and unlike [65] its memory usage does not scale with the dataset size. Separately, another
line of work focuses on accelerating the model retraining step underlying most data valuation methods
[57, 104, 107], and these are complementary to our approach. Finally, while there are works that
accelerate data attribution with datamodels [78, 27], we are not aware of any that use amortization.2"
RELATED WORK,0.024253731343283583,"More broadly, our proposal to train amortized models with noisy labels can be viewed as a version of
stochastic optimization, or training with noisy gradients. This fundamental idea is widely used in
machine learning [5], but to our knowledge we are the first to study the broad applicability of noisy
labels for accelerating diverse XML tasks."
STOCHASTIC AMORTIZATION,0.025186567164179104,"3
Stochastic Amortization"
STOCHASTIC AMORTIZATION,0.026119402985074626,"We now discuss how to efficiently train amortized models with noisy labels. Following Section 2, we
present this as a general approach before focusing on a specific XML task. One natural idea is to treat
amortization like a standard supervised learning problem: we can parameterize a model a(b; θ), adopt
a distribution p(b) over the context variable, and then train our model with the following objective,"
STOCHASTIC AMORTIZATION,0.027052238805970148,"Lreg(θ) = E
h
∥a(b; θ) −a(b)∥2i
.
(2)"
STOCHASTIC AMORTIZATION,0.027985074626865673,"2Datamodels [47] performs data attribution by fitting a linear regression model, but this is not amortization
because the model cannot predict attributions for new data points (see Appendix B)."
STOCHASTIC AMORTIZATION,0.028917910447761194,"This approach is called regression-based amortization [2] because it reduces the problem to a simple
regression task. The challenge is that this approach cannot be used when we lack a large dataset of
exact labels a(b), which is common for computationally intensive XML methods (Section 4)."
STOCHASTIC AMORTIZATION,0.029850746268656716,"A relaxation of this idea is to train the model with inexact labels (see Figure 1). We assume that
these are generated by a noisy oracle ˜a(b), which is characterized by a distribution of outputs for
each context b ∈B. For example, the noisy oracle could be a statistical estimator of a data valuation
score [33]. With this, we can train the model using a modified version of Eq. (2), where we consider
the loss in expectation over both p(b) and the noisy labels ˜a(b):"
STOCHASTIC AMORTIZATION,0.030783582089552237,"˜Lreg(θ) = E
h
∥a(b; θ) −˜a(b)∥2i
.
(3)"
STOCHASTIC AMORTIZATION,0.03171641791044776,"It is not immediately obvious when this approach is worthwhile: if the noisy oracle is too inaccurate
we will learn the wrong function, so it is important to choose the oracle carefully. We find that there
are two properties of the oracle that matter, and these relate to its systematic error and noise level, or
more intuitively its bias and variance. We denote these quantities as follows for a specific value b,"
STOCHASTIC AMORTIZATION,0.03264925373134328,"B(˜a | b) = ∥a(b) −E[˜a(b) | b]∥2 ,
N(˜a | b) = E
h
∥˜a(b) −E[˜a(b) | b]∥2 | b
i
,"
STOCHASTIC AMORTIZATION,0.033582089552238806,"and based on these we can also define the global measures B(˜a) ≡Ep[B(˜a | b)] and N(˜a) ≡
Ep[N(˜a | b)] for the distribution over context variables p(b).3 These terms are useful because they
reveal a relationship between the two amortization objectives. In general, the objectives are related
by the following two-sided bound (see proof in Appendix D):
q"
STOCHASTIC AMORTIZATION,0.03451492537313433,"˜Lreg(θ) −N(˜a) −
p"
STOCHASTIC AMORTIZATION,0.03544776119402985,"B(˜a)
2
≤Lreg(θ) ≤
q"
STOCHASTIC AMORTIZATION,0.036380597014925374,"˜Lreg(θ) −N(˜a) +
p"
STOCHASTIC AMORTIZATION,0.03731343283582089,"B(˜a)
2
.
(4)"
STOCHASTIC AMORTIZATION,0.03824626865671642,"This relationship shows that reducing ˜Lreg(θ) towards its minimum value N(˜a) is similar to training
with Lreg(θ), only with a disconnect introduced by the bias B(˜a). The bias represents a source of
irreducible error, because in the limit ˜Lreg(θ) −N(˜a) →0 we have Lreg(θ) = B(˜a). On the other
hand, when B(˜a) = 0 we can see that Lreg(θ) = ˜Lreg(θ) −N(˜a), which means that training with the
noisy loss is equivalent and will recover the correct function asymptotically. This last equality is easy
to see given an unbiased noisy oracle ˜a(b), but the more general relationship in Eq. (4) emphasizes
how non-zero bias can be problematic and lead to learning an incorrect function."
STOCHASTIC AMORTIZATION,0.03917910447761194,"Aside from the bias, the variance plays a role as well, not in determining the function we learn but
in making the model’s optimization unstable or require more noisy labels. To illustrate the role of
variance, we present a theoretical result considering the simplest case of a linear model a(b; θ) trained
with SGD, which shows that high label noise slows convergence (see proof in Appendix D)."
STOCHASTIC AMORTIZATION,0.04011194029850746,Theorem 1. Consider a noisy oracle ˜a(b) that satisfies E[˜a(b) | b] = ˜θb with parameters ˜θ ∈Rm×d
STOCHASTIC AMORTIZATION,0.041044776119402986,"such that ∥˜θ∥F ≤D. Given a distribution p(b), define the norm-weighted distribution q(b) ∝
p(b) · ∥b∥2 and the terms Σp ≡Ep[bb⊤] and Σq ≡Eq[bb⊤]. If we train a linear model a(θ; b) = θb
with the noisy objective ˜Lreg(θ) using SGD with step size ηt =
2
α(t+1), then the averaged iterate
¯θT = PT
t=1
2t
T (T +1)θt at step T satisfies"
STOCHASTIC AMORTIZATION,0.04197761194029851,"E[ ˜Lreg(¯θT )] −N(˜a) ≤4 Tr(Σp)
 
Nq(˜a) + 4λmax(Σq)D2"
STOCHASTIC AMORTIZATION,0.04291044776119403,"λmin(Σp)(T + 1)
,"
STOCHASTIC AMORTIZATION,0.043843283582089554,"where Nq(˜a) ≡Eq[N(˜a | b)] is the noisy oracle’s norm-weighted variance, and λmax(·), λmin(·) are
the maximum and minimum eigenvalues."
STOCHASTIC AMORTIZATION,0.04477611940298507,"The bound in Theorem 1 shows that noise slows convergence by its presence in the numerator, and
interestingly, it appears in the form of a weighted version Nq(˜a) that puts more weight on values
with large norm ∥b∥; this is a consequence of assuming a linear model, but we expect the general
conclusion of label variance slowing convergence to hold even for neural networks. As a corollary,
we can see that the rate in Theorem 1 applies directly to Lreg(θ) when the noisy oracle is unbiased
(see Appendix D). We note that very high noise levels can in principle prevent effective optimization,
and this can be mitigated by either reducing the noisy oracle’s variance or taking more steps T."
STOCHASTIC AMORTIZATION,0.0457089552238806,"3N(˜a | b) is equal to the trace of the conditional covariance Cov(˜a | b), and N(˜a) = Tr(E[Cov(˜a | b)])."
STOCHASTIC AMORTIZATION,0.04664179104477612,"Overall, our analysis shows that amortization with noisy labels is possible, although perhaps more
difficult to optimize than training with exact labels. We next show that unbiased estimates are available
for many XML tasks (Section 4), and we later find that this form of amortization is consistently
effective with the noise levels observed in practice (Section 5), even providing better accuracy than
per-example estimation in compute-matched comparisons. As a shorthand, we refer to training with
the noisy objective ˜Lreg(θ) in Eq. (3) as stochastic amortization."
APPLICATIONS TO EXPLAINABLE ML,0.04757462686567164,"4
Applications to Explainable ML"
APPLICATIONS TO EXPLAINABLE ML,0.048507462686567165,"We now consider XML tasks that can be accelerated with stochastic amortization. Rather than using
generic variables b ∈B and a(b) ∈A, this section uses an input variable x ∈X, a response variable
y ∈Y, and a model f or a measure of its performance. As we describe below, each application of
stochastic amortization is instantiated by a noisy oracle that generates labels for the given task."
SHAPLEY VALUE FEATURE ATTRIBUTION,0.049440298507462684,"4.1
Shapley value feature attribution"
SHAPLEY VALUE FEATURE ATTRIBUTION,0.05037313432835821,"One of the most common tasks in XML is feature attribution, which aims to quantify each feature’s
influence on an individual prediction. The Shapley value has gained popularity because of its origins
in game theory [89, 68], and like many feature attribution methods is based on querying the model
while removing different feature sets [17]. Given a model f and input x that consists of d separate
features x = (x1, . . . , xd), we assume that we can calculate the prediction f(xS) ∈R for any feature
set S ⊆[d].4 With this setup, the Shapley values ϕi(x) ∈R for each feature i ∈[d] are defined as:"
SHAPLEY VALUE FEATURE ATTRIBUTION,0.051305970149253734,ϕi(x) = 1 d X
SHAPLEY VALUE FEATURE ATTRIBUTION,0.05223880597014925,S⊆[d]\{i}
SHAPLEY VALUE FEATURE ATTRIBUTION,0.05317164179104478,"d −1
|S|"
SHAPLEY VALUE FEATURE ATTRIBUTION,0.054104477611940295,"−1  
f(xS∪{i}) −f(xS)

.
(5)"
SHAPLEY VALUE FEATURE ATTRIBUTION,0.05503731343283582,"These scores satisfy several desirable properties [89], but they are impractical to calculate due to
the exponential summation over feature subsets. Our goal is therefore to learn an amortized model
ϕ(x; θ) ∈Rd to directly predict feature attribution scores, and for this we require a noisy oracle."
SHAPLEY VALUE FEATURE ATTRIBUTION,0.055970149253731345,"Many recent works have studied efficient Shapley value estimation [10], and we first consider
noisy oracles derived from Eq. (5), which defines the attribution as the feature’s expected marginal
contribution. There are several unbiased statistical estimators that rely on sampling feature subsets
or permutations [6, 96, 77, 74, 58], and following Section 3 we can use any of these for stochastic
amortization. Our experiments use the classic permutation sampling estimator [96, 74], which
approximates the values ϕi(x) as an expectation across feature orderings. We defer the precise
definition of this noisy oracle to Appendix E, along with the other estimators used in our experiments."
SHAPLEY VALUE FEATURE ATTRIBUTION,0.05690298507462686,"Next, we also consider noisy oracles derived from an optimization perspective on the Shapley value.
A famous result from Charnes et al. [7] shows that the Shapley values are the solution to the following
problem (with abuse of notation we discard the solution’s intercept term),"
SHAPLEY VALUE FEATURE ATTRIBUTION,0.05783582089552239,"ϕ(x) = arg min
a∈Rd+1 X"
SHAPLEY VALUE FEATURE ATTRIBUTION,0.058768656716417914,"S⊆[d]
µ(S) "
SHAPLEY VALUE FEATURE ATTRIBUTION,0.05970149253731343,"f(xS) −a0 −
X"
SHAPLEY VALUE FEATURE ATTRIBUTION,0.06063432835820896,"i∈S
ai !2 ,
(6)"
SHAPLEY VALUE FEATURE ATTRIBUTION,0.061567164179104475,"where we use a least squares weighting kernel defined as µ−1(S) =
  d
|S|

|S|(d −|S|). Several
works have proposed approximating Shapley values by solving this problem with sampled subsets,
either using projected gradient descent [92] or analytic solutions [68, 16]. Among these, we use
KernelSHAP [68] and SGD-Shapley [92] as noisy oracles in our experiments. The first is an M-
estimator whose bias shrinks as the number of sampled subsets grows [99], so we expect it to lead to
effective amortization; the latter has been shown to have non-negligible bias [10], so our theory in
Section 3 suggests that it should lead to learning an incorrect function when used for amortization."
ALTERNATIVE FEATURE ATTRIBUTIONS,0.0625,"4.2
Alternative feature attributions"
ALTERNATIVE FEATURE ATTRIBUTIONS,0.06343283582089553,"Next, we consider two alternative feature attribution methods: Banzhaf values [4, 11] and LIME [83].
These are closely related to Shapley values and are similarly intractable [26, 40], but we find that
they offer statistical estimators that can be used for stochastic amortization."
ALTERNATIVE FEATURE ATTRIBUTIONS,0.06436567164179105,"4There are many ways to do so [17], e.g., we can set features to their mean or use masked self-attention."
ALTERNATIVE FEATURE ATTRIBUTIONS,0.06529850746268656,"First, Banzhaf values assign the following scores to each feature for a prediction f(x) [4]:"
ALTERNATIVE FEATURE ATTRIBUTIONS,0.06623134328358209,"ϕi(x) =
1
2d−1
X"
ALTERNATIVE FEATURE ATTRIBUTIONS,0.06716417910447761,S⊆[d]\{i}
ALTERNATIVE FEATURE ATTRIBUTIONS,0.06809701492537314," 
f(xS∪{i}) −f(xS)

.
(7)"
ALTERNATIVE FEATURE ATTRIBUTIONS,0.06902985074626866,"These differ from Shapley values only in their choice of weighting function, and they admit a range
of similar statistical estimators. One option is the MSR estimator from [103], which is unbiased and
re-uses all model evaluations for each feature attribution estimate. We adopt this as a noisy oracle in
our experiments (see a precise definition in Appendix E), but several other options are available."
ALTERNATIVE FEATURE ATTRIBUTIONS,0.06996268656716417,"Second, LIME defines its attribution scores ϕi(x) as the solution to the following optimization
problem, given a weighting kernel π(S) and penalty term Ω[83]:5"
ALTERNATIVE FEATURE ATTRIBUTIONS,0.0708955223880597,"arg min
a∈Rd+1 X"
ALTERNATIVE FEATURE ATTRIBUTIONS,0.07182835820895522,"S⊆[d]
π(S) "
ALTERNATIVE FEATURE ATTRIBUTIONS,0.07276119402985075,"f(xS) −a0 −
X"
ALTERNATIVE FEATURE ATTRIBUTIONS,0.07369402985074627,"i∈S
ai !2"
ALTERNATIVE FEATURE ATTRIBUTIONS,0.07462686567164178,"+ Ω(a).
(8)"
ALTERNATIVE FEATURE ATTRIBUTIONS,0.07555970149253731,"As our noisy oracle for LIME, we use the popular approach of solving the above problem for subsets
sampled according to π(S). Similar to KernelSHAP [68], this is an M-estimator whose bias shrinks
to zero as the sample size grows [99], so we expect it to lead to successful amortization."
ALTERNATIVE FEATURE ATTRIBUTIONS,0.07649253731343283,"Aside from these methods, other costly feature interpretation methods rely on unbiased statistical
estimators and can be amortized in a similar fashion [98, 62, 32]. We leave further investigation of
these methods to future work."
DATA VALUATION,0.07742537313432836,"4.3
Data valuation"
DATA VALUATION,0.07835820895522388,"Next, data valuation aims to quantify how much each training example affects a model’s accuracy.
We consider labeled examples z = (x, y) and a training dataset D = {zi}n
i=1, and we analyze each
data point’s value by fitting models to subsampled datasets DT ⊆D with T ⊆[n] and calculating a
measure of the model’s performance v(DT ) ∈R (e.g., its 0-1 accuracy). This general approach was
introduced by Ghorbani and Zou [33], who defined the Data Shapley scores ψ(zi) ∈R as follows:"
DATA VALUATION,0.07929104477611941,ψ(zi) = 1 n X
DATA VALUATION,0.08022388059701492,T ⊆[n]\{i}
DATA VALUATION,0.08115671641791045,"n −1
|T|"
DATA VALUATION,0.08208955223880597,"−1
(v(DT ∪{zi}) −v(DT )) .
(9)"
DATA VALUATION,0.0830223880597015,"Subsequent work generalized the approach in different ways, which we briefly summarize before
considering amortization. For example, Wang and Jia [103] used the Banzhaf value rather than
Shapley value, and Kwon and Zou [60] considered the case of arbitrary semivalues [75]. These
correspond to adopting a different weighting over subsets in Eq. (9), and the general case can be
written as follows for a normalized weighting function w(k):6"
DATA VALUATION,0.08395522388059702,"ψ(zi) =
X"
DATA VALUATION,0.08488805970149253,"T ⊆[n]\{i}
w(|T|) (v(DT ∪{zi}) −v(DT )) .
(10)"
DATA VALUATION,0.08582089552238806,"Next, another extension is the case of distributional data valuation. Ghorbani et al. [35] incorporate
an expectation over the original dataset D, which they show leads to well defined scores even for data
points z = (x, y) outside the training set. Given a distribution over datasets of size |D| = n −1 and
a weighting function w(k), this version defines the score ψ(z) ∈R for arbitrary z as follows:"
DATA VALUATION,0.08675373134328358,"ψ(z) = ED
X"
DATA VALUATION,0.08768656716417911,"T ⊆[n−1]
w(|T|) (v(DT ∪{z}) −v(DT )) .
(11)"
DATA VALUATION,0.08861940298507463,"When using any of these methods in practice, the scores are difficult to calculate due to the intractable
expectation across datasets. However, a crucial property they share is that they can all be estimated
in an unbiased fashion, and these estimates can therefore be used as noisy labels for stochastic
amortization. We focus on Data Shapley and Distributional Data Shapley in our experiments [33, 35],
and we use the Monte Carlo estimator from Ghorbani and Zou [33] as a noisy oracle (see the precise"
DATA VALUATION,0.08955223880597014,"5Following [17], we only consider the version of LIME with a binary interpretable representation.
6For this to be a valid expectation, semivalues require that Pn−1
k=0
 n−1
k

w(k) = 1. Note that Data Shapley
adopts w(k) =
 n−1
k
−1/n and Data Banzhaf adopts w(k) = 1/2n−1."
DATA VALUATION,0.09048507462686567,"definition in Appendix E). Doing so allows us to train an amortized valuation model ψ(z; θ) ∈R that
accelerates valuation within our training dataset, and that can also be applied to external data, e.g.,
when selecting informative new data points for active learning [36]."
DATA VALUATION,0.0914179104477612,"Finally, Appendix B discusses amortization for the datamodels data attribution technique [47]. This
method measures how much each training data point zi ∈D affects the prediction for an inference
example x ∈X, and we show that the scores are equivalent to a simple expectation that can be
estimated in an unbiased fashion. The datamodels scores can therefore be amortized by adopting
these estimates as noisy labels, but we leave further investigation of this approach to future work."
EXPERIMENTS,0.09235074626865672,"5
Experiments"
EXPERIMENTS,0.09328358208955224,"Our experiments apply stochastic amortization to several of the tasks discussed in Section 4. We
consider both feature attribution and data valuation, for image and tabular datasets, and using multiple
architectures for our amortized models, including fully-connected networks (FCNs), ResNets [43] and
ViTs [24]. Full details are provided in Appendix F, including our exact models and hyperparameters."
EXPERIMENTS,0.09421641791044776,"Our goal in each experiment is to perform feature attribution or data valuation for an entire dataset,
and to compare the accuracy of stochastic amortization to running existing estimators on each point.
We adopt a noisy oracle for each task (e.g., a Monte Carlo estimator of data valuation scores), we
then fit an amortized network with one noisy label per training example, and our evaluation focuses
on the accuracy of the amortized predictions relative to the ground truth. Our ground truth is obtained
by running the noisy oracle to near-convergence for a large number of samples: for example, we
run KernelSHAP [68] for feature attribution with 1M samples, and the TMC estimator [33] for data
valuation with 10K samples. We test amortization when using different numbers of training examples,
and for both training and unseen external data to evaluate the model’s generalization. We find that
amortization often denoises and strongly improves upon the noisy labels, leading to a significant
accuracy improvement for the same computational budget."
FEATURE ATTRIBUTION,0.09514925373134328,"5.1
Feature attribution"
FEATURE ATTRIBUTION,0.0960820895522388,Cassette player
FEATURE ATTRIBUTION,0.09701492537313433,Context b
FEATURE ATTRIBUTION,0.09794776119402986,"Parachute
Church
Golf ball"
FEATURE ATTRIBUTION,0.09888059701492537,Noisy label a(b)
FEATURE ATTRIBUTION,0.09981343283582089,Prediction
FEATURE ATTRIBUTION,0.10074626865671642,"a(b;
)"
FEATURE ATTRIBUTION,0.10167910447761194,Ground Truth a(b)
FEATURE ATTRIBUTION,0.10261194029850747,"Figure 2: Stochastic amortization for Shapley
value feature attributions. We compare the pre-
dicted attributions to the noisy labels and ground
truth, which are generated using KernelSHAP with
512 and 1M samples, respectively."
FEATURE ATTRIBUTION,0.10354477611940298,"We first consider Shapley value feature attribu-
tions. This task offers a diverse set of noisy
oracles, and we consider three options: Ker-
nelSHAP [68], permutation sampling [74] and
SGD-Shapley [92]. Among these, our theory
from Section 3 suggests that the first two will
be effective for amortization, while the third
may not because it is not an unbiased estimator.
We follow the setup from [18] and implement
our amortized network with a pretrained ViT-B
architecture [24], and we use the ImageNette
dataset [46] with 224 × 224 images partitioned
into 196 patches of size 14 × 14."
FEATURE ATTRIBUTION,0.1044776119402985,"As a first result, Figure 2 compares our train-
ing targets to the amortized model’s predictions.
The predicted attributions are significantly more
accurate than the labels, even for the noisiest
setting with just 512 KernelSHAP samples. Fig-
ure 3 (left) quantifies the improvement from
amortization, and we observe similar results for
both KernelSHAP and permutation sampling
(see Appendix G): in both cases the error is sig-
nificantly lower than that of the noisy labels,
and it remains lower and improves as the labels
become more accurate. To contextualize our
amortized model’s accuracy, we find that the error is similar to that of running KernelSHAP for
10-40K samples, even though our labels use an order of magnitude fewer samples (see Appendix G).
We also find that the model generalizes to external data points (see Appendix G). In addition, we"
FEATURE ATTRIBUTION,0.10541044776119403,"10
3
10
2
10
1
10
0"
FEATURE ATTRIBUTION,0.10634328358208955,"Error (Prediction) 10
3 10
2 10
1 10
0"
FEATURE ATTRIBUTION,0.10727611940298508,Error (Label)
FEATURE ATTRIBUTION,0.10820895522388059,KernelSHAP
FEATURE ATTRIBUTION,0.10914179104477612,"512
1024
2048
3072"
FEATURE ATTRIBUTION,0.11007462686567164,"10
16
10
17
10
18
10
19 FLOPs 10
4 10
3 10
2 Error Error"
FEATURE ATTRIBUTION,0.11100746268656717,"512
2048
KernelSHAP"
FEATURE ATTRIBUTION,0.11194029850746269,"1024
3072 10
3"
FEATURE ATTRIBUTION,0.11287313432835822,"# Training Datapoints (Amortized) 10
2 Error Error"
FEATURE ATTRIBUTION,0.11380597014925373,"Amortized
KernelSHAP"
FEATURE ATTRIBUTION,0.11473880597014925,"Figure 3: Amortized Shapley value feature attributions using KernelSHAP as a noisy oracle. Left:
squared error relative to the ground truth attributions when using noisy labels with different numbers of
samples (different noise levels). Center: estimation error as a function of FLOPs, where KernelSHAP
incurs FLOPs via classifier predictions used to estimate the attributions, and amortization incurs
additional FLOPs from training (training appears as a vertical line because the FLOPs are relatively
low, and endpoints represent results from the final epoch). Right: estimation error with different
training dataset sizes given equivalent compute per data point (matched by using fewer KernelSHAP
samples when generating noisy labels for amortization and allowing up to 50 epochs of training)."
FEATURE ATTRIBUTION,0.11567164179104478,"show results for SGD-Shapley, where we confirm that it leads to poor amortization results due to its
non-negligible bias (see Appendix G)."
FEATURE ATTRIBUTION,0.1166044776119403,"Next, we investigate the compute trade-off between calculating attributions separately (e.g., with
KernelSHAP) and using amortization. Figure 3 (center-right) shows two results regarding this tradeoff.
First, we measure the error as a function of FLOPs, where we account for the cost of generating labels
and training the amortized model (see Appendix F). The FLOPs incurred by training are negligible
compared to the KernelSHAP estimates, and we find that stopping at any time to fit an amortized
model yields significantly better estimates. This suggests that amortization is an inexpensive final
denoising step, regardless of how much compute was used for the noisy estimates."
FEATURE ATTRIBUTION,0.11753731343283583,"Second, we test the effectiveness of amortization for different dataset sizes. We match the compute
between the two scenarios, using 2440 KernelSHAP samples for per-example computation7 and 2257
for amortization to account for the cost of training. This compute-matched comparison shows that
amortization achieves lower estimation error for datasets ranging from 250-10K data points (Figure 3
right); it becomes more effective as the dataset grows, but it is useful even for small datasets."
FEATURE ATTRIBUTION,0.11847014925373134,"Finally, Appendix G shows a comparison between stochastic amortization and FastSHAP [50, 18],
an existing approach to amortized Shapley value estimation. We observe similar estimation accuracy
in compute-matched comparisons, and find that both methods are significantly more accurate than
per-example estimation (similar to Figure 3 center). Appendix G also show results for amortizing
Banzhaf values and LIME: we find that amortization is more difficult for these methods due to the
inconsistent scale of attributions between inputs, but we nonetheless observe an improvement in our
amortized estimates versus the noisy labels."
DATA VALUATION,0.11940298507462686,"5.2
Data valuation"
DATA VALUATION,0.12033582089552239,"Next, we consider data valuation with Data Shapley. For our noisy oracle, we obtain training labels
by running the TMC estimator with different numbers of samples [33]. We first test our approach
with the adult census and MiniBooNE particle physics datasets [25, 84], and following prior work
we conduct experiments using versions of each dataset with different numbers of data points [53].
Our valuation model must predict scores for each example z = (x, y), so we train FCNs that output
scores for all classes and use only the relevant output for each data point."
DATA VALUATION,0.12126865671641791,"As a first result, Figure 4 (left-center) shows the estimation accuracy for the MiniBooNE dataset
when using 1K and 10K data points. The noisy estimates converge as we use more Monte Carlo
samples, but we see that the amortized estimates are always more accurate in terms of both squared
error and correlation with the ground truth. The improvement is largest for the noisiest estimates,
where the amortized predictions have correlation >0.9 when using only 50 samples. Amortization"
DATA VALUATION,0.12220149253731344,"7This is the default number of samples for 196 features in the official repository: https://github.com/
shap/shap."
DATA VALUATION,0.12313432835820895,"0
50
100
150
200
250
# Samples / Point 10
1 100 Error"
DATA VALUATION,0.12406716417910447,MiniBooNE Error
DATA VALUATION,0.125,"0
50
100
150
200
250
# Samples / Point 0.2 0.4 0.6 0.8 1.0"
DATA VALUATION,0.1259328358208955,Pearson Correlation
DATA VALUATION,0.12686567164179105,MiniBooNE Correlation
DATA VALUATION,0.12779850746268656,Amortized (1000 Points)
DATA VALUATION,0.1287313432835821,Amortized (10000 Points)
DATA VALUATION,0.1296641791044776,Monte Carlo (1000 Points)
DATA VALUATION,0.13059701492537312,Monte Carlo (10000 Points)
DATA VALUATION,0.13152985074626866,Amortized (1000 Points)
DATA VALUATION,0.13246268656716417,Amortized (10000 Points)
DATA VALUATION,0.1333955223880597,Monte Carlo (1000 Points)
DATA VALUATION,0.13432835820895522,Monte Carlo (10000 Points)
DATA VALUATION,0.13526119402985073,"103
104"
DATA VALUATION,0.13619402985074627,# Datapoints 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
DATA VALUATION,0.13712686567164178,Pearson Correlation
DATA VALUATION,0.13805970149253732,Data Valuation Scaling
DATA VALUATION,0.13899253731343283,Amortized (MiniBooNE)
DATA VALUATION,0.13992537313432835,Amortized (Adult)
DATA VALUATION,0.14085820895522388,Monte Carlo (MiniBooNE)
DATA VALUATION,0.1417910447761194,Monte Carlo (Adult)
DATA VALUATION,0.14272388059701493,"Figure 4: Amortized data valuation accuracy for tabular datasets. Left: mean squared error relative
to the ground truth for the MiniBooNE dataset, normalized so that the mean valuation score has
error equal to 1 (for 1K and 10K data points). The x-axis indicates how many Monte Carlo samples
were used for each data point. Center: Pearson correlation with the ground truth for the MiniBooNE
dataset (for 1K and 10K data points). Right: estimation accuracy for the MiniBooNE and adult
census datasets as a function of dataset size (250 to 10K data points); we use 50 Monte Carlo samples
per data point for all results and show the Pearson correlation with the ground truth."
DATA VALUATION,0.14365671641791045,"5
10
15
20
25
# Samples / Point 0.5 1.0 1.5 2.0 Error"
DATA VALUATION,0.14458955223880596,CIFAR-10 Error
DATA VALUATION,0.1455223880597015,Monte Carlo
DATA VALUATION,0.146455223880597,Amortized
DATA VALUATION,0.14738805970149255,"5
10
15
20
25
# Samples / Point 0.6 0.7 0.8 0.9"
DATA VALUATION,0.14832089552238806,Pearson Correlation
DATA VALUATION,0.14925373134328357,Monte Carlo
DATA VALUATION,0.1501865671641791,Amortized
DATA VALUATION,0.15111940298507462,CIFAR-10 Correlation
DATA VALUATION,0.15205223880597016,"103
104"
DATA VALUATION,0.15298507462686567,# Datapoints 0.5 1.0 1.5 2.0 Error
DATA VALUATION,0.15391791044776118,CIFAR-10 Error
DATA VALUATION,0.15485074626865672,Monte Carlo (Internal)
DATA VALUATION,0.15578358208955223,Amortized (Internal)
DATA VALUATION,0.15671641791044777,Amortized (External)
DATA VALUATION,0.15764925373134328,"Figure 5: Distributional data valuation for CIFAR-10. Left: estimation error when using different
numbers of samples for the noisy label estimates. Center: Pearson correlation with the ground truth
for different numbers of noisy samples. Right: estimation error as a function of dataset size, where
all results use 5 Monte Carlo samples per data points; we compare the error for amortized estimates
on internal (training) and external (unseen) data points, demonstrating strong generalization."
DATA VALUATION,0.15858208955223882,"is more beneficial for the 10K dataset, which suggests that training with more noisy labels can be a
substitute for high-quality labels. Appendix G shows similar results with the adult census dataset."
DATA VALUATION,0.15951492537313433,"Next, Figure 4 (right) considers the role of training dataset size for the estimates with 50 Monte Carlo
samples. For both the adult census and MiniBooNE datasets, we see that the benefits of amortization
are small with 250 data points but grow as we approach 10K data points. This number of samples is
enough to maintain 0.9 correlation with the ground truth when using amortization, whereas the raw
estimates become increasingly inaccurate. Stochastic amortization is therefore promising to scale
data valuation beyond previous works, which typically focus on <1K data points [34, 60, 103]."
DISTRIBUTIONAL DATA VALUATION,0.16044776119402984,"5.3
Distributional data valuation"
DISTRIBUTIONAL DATA VALUATION,0.16138059701492538,"Finally, we consider Distributional Data Shapley [35], which is similar to the previous experiments
but defines valuation scores even for points outside the training dataset; this allows us to test the
generalization to unseen data. We use the CIFAR-10 dataset [59], which contains 50K training
examples, and for our valuation model we train a ResNet-18 that outputs scores for each class."
DISTRIBUTIONAL DATA VALUATION,0.1623134328358209,"Similar to the previous experiments, Figure 5 (left-center) evaluates the estimation accuracy for
different numbers of Monte Carlo samples. We observe that the distributional scores converge faster
because we use a smaller maximum dataset cardinality (see Appendix F), but that amortization still
provides a significant benefit. The improvement is largest for the noisiest estimates, which in this
case use just 5 samples: amortization achieves correlation 0.81 with the ground truth, versus 0.58
for the Monte Carlo estimates. The improvement is consistent across several measures of accuracy,
including squared error, Pearson correlation and Spearman correlation (see Appendix G)."
DISTRIBUTIONAL DATA VALUATION,0.16324626865671643,"Next, we study generalization and the role of dataset size. We focus on the noisiest estimates with
5 Monte Carlo samples, because this low-sample regime is most relevant for larger datasets with
millions of examples. We train the valuation model with different portions of the 50K training set,
and we measure the estimation accuracy for both internal and unseen external data. Figure 5 (right)
shows large improvements in squared error with as few as 1K data points, and we also observe
improvement in correlation when we use at least 5K data points (10% of the dataset, see Appendix G).
We additionally observe a small generalization gap, suggesting that the valuation model can be trained
with a subset of the data and reliably applied to unseen examples."
DISTRIBUTIONAL DATA VALUATION,0.16417910447761194,"Lastly, we test the usage of amortized valuation scores in downstream tasks. Following [53], the tasks
we consider are identifying mislabeled examples and improving the model by removing low-value
examples. These results are shown in Appendix G, and we find that the amortized estimates identify
mislabeled examples more reliably than Monte Carlo estimates, and that filtering the dataset based on
our estimates leads to improved performance."
CONCLUSION,0.16511194029850745,"6
Conclusion"
CONCLUSION,0.166044776119403,"This work explored the idea of stochastic amortization, or training amortized models with noisy
labels. Our main finding is that fast, noisy supervision provides substantial compute and accuracy
gains over existing XML approximations. This approach makes several feature attribution and data
valuation methods more practical for large datasets and real-time applications, and it may have
broader applications to amortization beyond XML [2]. Our proposal has certain limitations, including
that stochastic amortization may become ineffective with sufficiently high noise levels, and that it is
difficult to know a priori how much compute is necessary for label generation to achieve a desired
error level in the amortized predictions."
CONCLUSION,0.1669776119402985,"Our work suggests multiple directions for future research. One direction is to study the trade-off
between using a larger number of noisy labels or a smaller number of more accurate labels, which is
a key difference from prior work that uses near-exact labels for amortization [14]. Other directions
include scaling to datasets with millions of examples to test the limits of noisy supervision, leveraging
more sophisticated data valuation estimators [106], using alternative model retraining primitives
[57, 63, 37], and exploring amortization for other methods like datamodels (discussed in Appendix B). Code"
CONCLUSION,0.16791044776119404,We provide two repositories to reproduce each our results:
CONCLUSION,0.16884328358208955,"Feature attribution
https://github.com/chanwkimlab/amortized-attribution"
CONCLUSION,0.16977611940298507,"Data valuation
https://github.com/iancovert/amortized-valuation"
CONCLUSION,0.1707089552238806,Acknowledgements
CONCLUSION,0.17164179104477612,"The authors thank Yongchan Kwon for helpful discussions and advice on using OpenDataVal. We
also thank Mukund Sudarshan and Neil Jethani for early conversations about amortizing data valua-
tion. Chanwoo Kim and Su-In Lee were supported by the National Science Foundation (CAREER
DBI-1552309 and DBI-1759487) and the National Institutes of Health (R35 GM 128638 and R01
AG061132)."
REFERENCES,0.17257462686567165,References
REFERENCES,0.17350746268656717,"[1] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. arXiv preprint
arXiv:2005.00928, 2020."
REFERENCES,0.17444029850746268,"[2] Brandon Amos. Tutorial on amortized optimization for learning to optimize over continuous
domains. arXiv preprint arXiv:2202.00665, 2022."
REFERENCES,0.17537313432835822,"[3] Marco Ancona, Enea Ceolini, Cengiz Öztireli, and Markus Gross. Towards better under-
standing of gradient-based attribution methods for deep neural networks. In International
Conference on Learning Representations, 2018."
REFERENCES,0.17630597014925373,"[4] John F Banzhaf. Weighted voting doesn’t work: a mathematical analysis. Rutgers Law Review,
19:317, 1964."
REFERENCES,0.17723880597014927,"[5] Sébastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and
Trends® in Machine Learning, 8(3-4):231–357, 2015."
REFERENCES,0.17817164179104478,"[6] Javier Castro, Daniel Gómez, and Juan Tejada. Polynomial calculation of the Shapley value
based on sampling. Computers & Operations Research, 36(5):1726–1730, 2009."
REFERENCES,0.1791044776119403,"[7] A Charnes, B Golany, M Keane, and J Rousseau. Extremal principle solutions of games
in characteristic function form: core, Chebychev and Shapley value generalizations. In
Econometrics of Planning and Efficiency, pages 123–133. Springer, 1988."
REFERENCES,0.18003731343283583,"[8] Aditya Chattopadhyay, Stewart Slocum, Benjamin D Haeffele, Rene Vidal, and Donald
Geman. Interpretable by design: Learning predictors by composing interpretable queries.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022."
REFERENCES,0.18097014925373134,"[9] Aditya Chattopadhyay, Kwan Ho Ryan Chan, Benjamin D Haeffele, Donald Geman, and
René Vidal. Variational information pursuit for interpretable predictions. arXiv preprint
arXiv:2302.02876, 2023."
REFERENCES,0.18190298507462688,"[10] Hugh Chen, Ian C Covert, Scott M Lundberg, and Su-In Lee. Algorithms to estimate Shapley
value feature attributions. arXiv preprint arXiv:2207.07605, 2022."
REFERENCES,0.1828358208955224,"[11] Jianbo Chen and Michael Jordan. LS-Tree: Model interpretation when the data are linguistic.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3454–3461,
2020."
REFERENCES,0.1837686567164179,"[12] Jianbo Chen, Le Song, Martin J Wainwright, and Michael I Jordan. Learning to explain: An
information-theoretic perspective on model interpretation. arXiv preprint arXiv:1802.07814,
2018."
REFERENCES,0.18470149253731344,"[13] Yu-Neng Chuang, Guanchu Wang, Fan Yang, Zirui Liu, Xuanting Cai, Mengnan Du, and Xia
Hu. Efficient XAI techniques: A taxonomic survey. arXiv preprint arXiv:2302.03225, 2023."
REFERENCES,0.18563432835820895,"[14] Yu-Neng Chuang, Guanchu Wang, Fan Yang, Quan Zhou, Pushkar Tripathi, Xuanting Cai,
and Xia Hu. CoRTX: Contrastive framework for real-time explanation. arXiv preprint
arXiv:2303.02794, 2023."
REFERENCES,0.1865671641791045,"[15] R Dennis Cook and Sanford Weisberg. Characterizations of an empirical influence function
for detecting influential cases in regression. Technometrics, 22(4):495–508, 1980."
REFERENCES,0.1875,"[16] Ian Covert and Su-In Lee. Improving KernelSHAP: Practical Shapley value estimation using
linear regression. In International Conference on Artificial Intelligence and Statistics, pages
3457–3465. PMLR, 2021."
REFERENCES,0.1884328358208955,"[17] Ian Covert, Scott Lundberg, and Su-In Lee. Explaining by removing: A unified framework for
model explanation. Journal of Machine Learning Research, 22(209):1–90, 2021."
REFERENCES,0.18936567164179105,"[18] Ian Covert, Chanwoo Kim, and Su-In Lee. Learning to estimate Shapley values with vision
transformers. arXiv preprint arXiv:2206.05282, 2022."
REFERENCES,0.19029850746268656,"[19] Ian Covert, Wei Qiu, Mingyu Lu, Nayoon Kim, Nathan White, and Su-In Lee. Learning to
maximize mutual information for dynamic feature selection. arXiv preprint arXiv:2301.00557,
2023."
REFERENCES,0.1912313432835821,"[20] Ian C Covert, Scott Lundberg, and Su-In Lee. Shapley feature utility. In Machine Learning in
Computational Biology, 2019."
REFERENCES,0.1921641791044776,"[21] Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. In Advances
in Neural Information Processing Systems, pages 6967–6976, 2017."
REFERENCES,0.19309701492537312,"[22] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online
prediction using mini-batches. Journal of Machine Learning Research, 13(1), 2012."
REFERENCES,0.19402985074626866,"[23] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition,
pages 248–255. IEEE, 2009."
REFERENCES,0.19496268656716417,"[24] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929, 2020."
REFERENCES,0.1958955223880597,"[25] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.19682835820895522,"[26] Pradeep Dubey and Lloyd S Shapley. Mathematical properties of the Banzhaf power index.
Mathematics of Operations Research, 4(2):99–131, 1979."
REFERENCES,0.19776119402985073,"[27] Logan Engstrom, Axel Feldmann, and Aleksander Madry. DsDm: Model-aware dataset
selection with datamodels. arXiv preprint arXiv:2401.12926, 2024."
REFERENCES,0.19869402985074627,"[28] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering
the long tail via influence estimation. Advances in Neural Information Processing Systems, 33:
2881–2891, 2020."
REFERENCES,0.19962686567164178,"[29] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast
adaptation of deep networks. In International Conference on Machine Learning, pages
1126–1135. PMLR, 2017."
REFERENCES,0.20055970149253732,"[30] Ruth Fong, Mandela Patrick, and Andrea Vedaldi. Understanding deep networks via extremal
perturbations and smooth masks. In Proceedings of the IEEE International Conference on
Computer Vision, pages 2950–2958, 2019."
REFERENCES,0.20149253731343283,"[31] Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful
perturbation. In Proceedings of the IEEE International Conference on Computer Vision, pages
3429–3437, 2017."
REFERENCES,0.20242537313432835,"[32] Fabian Fumagalli, Maximilian Muschalik, Patrick Kolpaczki, Eyke Hüllermeier, and Barbara
Hammer. SHAP-IQ: Unified approximation of any-order Shapley interactions. arXiv preprint
arXiv:2303.01179, 2023."
REFERENCES,0.20335820895522388,"[33] Amirata Ghorbani and James Zou. Data Shapley: Equitable valuation of data for machine
learning. In International Conference on Machine Learning, pages 2242–2251. PMLR, 2019."
REFERENCES,0.2042910447761194,"[34] Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3681–3688,
2019."
REFERENCES,0.20522388059701493,"[35] Amirata Ghorbani, Michael Kim, and James Zou. A distributional framework for data valuation.
In International Conference on Machine Learning, pages 3535–3544. PMLR, 2020."
REFERENCES,0.20615671641791045,"[36] Amirata Ghorbani, James Zou, and Andre Esteva. Data Shapley valuation for efficient batch
active learning. In 2022 56th Asilomar Conference on Signals, Systems, and Computers, pages
1456–1462. IEEE, 2022."
REFERENCES,0.20708955223880596,"[37] Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini,
Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model
generalization with influence functions. arXiv preprint arXiv:2308.03296, 2023."
REFERENCES,0.2080223880597015,"[38] David Ha, Andrew M Dai, and Quoc V Le. HyperNetworks. In International Conference on
Learning Representations, 2016."
REFERENCES,0.208955223880597,"[39] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan,
Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms
and applications. arXiv preprint arXiv:1812.05905, 2018."
REFERENCES,0.20988805970149255,"[40] Peter L Hammer and Ron Holzman. Approximations of pseudo-boolean functions; applications
to game theory. Zeitschrift für Operations Research, 36(1):3–21, 1992."
REFERENCES,0.21082089552238806,"[41] Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends® in
Optimization, 2(3-4):157–325, 2016."
REFERENCES,0.21175373134328357,"[42] He He, Paul Mineiro, and Nikos Karampatziakis. Active information acquisition. arXiv
preprint arXiv:1602.02181, 2016."
REFERENCES,0.2126865671641791,"[43] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for
image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 770–778, 2016."
REFERENCES,0.21361940298507462,"[44] Weijie He, Xiaohao Mao, Chao Ma, Yu Huang, José Miguel Hernàndez-Lobato, and Ting
Chen. BSODA: a bipartite scalable framework for online disease diagnosis. In Proceedings of
the ACM Web Conference 2022, pages 2511–2521, 2022."
REFERENCES,0.21455223880597016,"[45] Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa.
Learning continuous control policies by stochastic value gradients. Advances in Neural
Information Processing Systems, 28, 2015."
REFERENCES,0.21548507462686567,"[46] Jeremy Howard and Sylvain Gugger. FastAI: A layered API for deep learning. Information,
11(2):108, 2020."
REFERENCES,0.21641791044776118,"[47] Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry.
Datamodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622,
2022."
REFERENCES,0.21735074626865672,"[48] Saachi Jain, Hadi Salman, Eric Wong, Pengchuan Zhang, Vibhav Vineet, Sai Vemprala, and
Aleksander Madry. Missingness bias in model debugging. In International Conference on
Learning Representations, 2021."
REFERENCES,0.21828358208955223,"[49] Neil Jethani, Mukund Sudarshan, Yindalon Aphinyanaphongs, and Rajesh Ranganath. Have
we learned to explain?: How interpretability methods can learn to encode predictions in their
interpretations. In International Conference on Artificial Intelligence and Statistics, pages
1459–1467. PMLR, 2021."
REFERENCES,0.21921641791044777,"[50] Neil Jethani, Mukund Sudarshan, Ian Connick Covert, Su-In Lee, and Rajesh Ranganath.
FastSHAP: Real-time Shapley value estimation. In International Conference on Learning
Representations, 2021."
REFERENCES,0.22014925373134328,"[51] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li, Ce Zhang,
Costas J Spanos, and Dawn Song. Efficient task-specific data valuation for nearest neighbor
algorithms. arXiv preprint arXiv:1908.08619, 2019."
REFERENCES,0.22108208955223882,"[52] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve Gürel,
Bo Li, Ce Zhang, Dawn Song, and Costas J Spanos. Towards efficient data valuation based
on the Shapley value. In The 22nd International Conference on Artificial Intelligence and
Statistics, pages 1167–1176. PMLR, 2019."
REFERENCES,0.22201492537313433,"[53] Kevin Fu Jiang, Weixin Liang, James Zou, and Yongchan Kwon. OpenDataVal: a unified
benchmark for data valuation. arXiv preprint arXiv:2306.10577, 2023."
REFERENCES,0.22294776119402984,"[54] Hoang Anh Just, Feiyang Kang, Jiachen T Wang, Yi Zeng, Myeongseob Ko, Ming Jin, and
Ruoxi Jia. Lava: Data valuation without pre-specified learning algorithms. arXiv preprint
arXiv:2305.00054, 2023."
REFERENCES,0.22388059701492538,"[55] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980, 2014."
REFERENCES,0.2248134328358209,"[56] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013."
REFERENCES,0.22574626865671643,"[57] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions.
In International conference on machine learning, pages 1885–1894. PMLR, 2017."
REFERENCES,0.22667910447761194,"[58] Patrick Kolpaczki, Viktor Bengs, Maximilian Muschalik, and Eyke Hüllermeier. Approxi-
mating the shapley value without marginal contributions. arXiv preprint arXiv:2302.00736,
2023."
REFERENCES,0.22761194029850745,"[59] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.228544776119403,"[60] Yongchan Kwon and James Zou. Beta Shapley: a unified and noise-reduced data valuation
framework for machine learning. arXiv preprint arXiv:2110.14049, 2021."
REFERENCES,0.2294776119402985,"[61] Yongchan Kwon and James Zou. Data-OOB: Out-of-bag estimate as a simple and efficient
data value. arXiv preprint arXiv:2304.07718, 2023."
REFERENCES,0.23041044776119404,"[62] Yongchan Kwon and James Y Zou. WeightedSHAP: analyzing and improving Shapley based
feature attributions. Advances in Neural Information Processing Systems, 35:34363–34376,
2022."
REFERENCES,0.23134328358208955,"[63] Yongchan Kwon, Eric Wu, Kevin Wu, and James Zou. DataInf: Efficiently estimating data
influence in lora-tuned llms and diffusion models. arXiv preprint arXiv:2310.00902, 2023."
REFERENCES,0.23227611940298507,"[64] Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on
Machine Learning, pages 1–9. PMLR, 2013."
REFERENCES,0.2332089552238806,"[65] Weida Li and Yaoliang Yu. Faster approximation of probabilistic and distributional values via
least squares. In International Conference on Learning Representations, 2024."
REFERENCES,0.23414179104477612,"[66] Chris Lin, Ian Covert, and Su-In Lee. On the robustness of removal-based feature attributions.
arXiv preprint arXiv:2306.07462, 2023."
REFERENCES,0.23507462686567165,"[67] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
Conference on Learning Representations, 2019."
REFERENCES,0.23600746268656717,"[68] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In
Advances in Neural Information Processing Systems, pages 4765–4774, 2017."
REFERENCES,0.23694029850746268,"[69] Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M. Prutkin, Bala Nair,
Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. From local explanations
to global understanding with explainable AI for trees. Nature Machine Intelligence, 2(1):
2522–5839, 2020."
REFERENCES,0.23787313432835822,"[70] Chao Ma, Sebastian Tschiatschek, Konstantina Palla, Jose Miguel Hernandez-Lobato, Sebas-
tian Nowozin, and Cheng Zhang. EDDI: Efficient dynamic discovery of high-value information
with partial VAE. In International Conference on Machine Learning, pages 4234–4243. PMLR,
2019."
REFERENCES,0.23880597014925373,"[71] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016."
REFERENCES,0.23973880597014927,"[72] Divyat Mahajan, Chenhao Tan, and Amit Sharma. Preserving causal constraints in coun-
terfactual explanations for machine learning classifiers. arXiv preprint arXiv:1912.03277,
2019."
REFERENCES,0.24067164179104478,"[73] Jean-Luc Marichal and Pierre Mathonet. Weighted Banzhaf power and interaction indexes
through weighted approximations of games. European Journal of Operational Research, 211
(2):352–358, 2011."
REFERENCES,0.2416044776119403,"[74] Rory Mitchell, Joshua Cooper, Eibe Frank, and Geoffrey Holmes. Sampling permutations for
Shapley value estimation. arXiv preprint arXiv:2104.12199, 2021."
REFERENCES,0.24253731343283583,"[75] Dov Monderer, Dov Samet, et al. Variations on the Shapley value. Handbook of Game Theory,
3:2055–2076, 2002."
REFERENCES,0.24347014925373134,"[76] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):
1574–1609, 2009."
REFERENCES,0.24440298507462688,"[77] Ramin Okhrati and Aldo Lipani. A multilinear sampling algorithm to estimate shapley values.
In 2020 25th International Conference on Pattern Recognition (ICPR), pages 7992–7999.
IEEE, 2021."
REFERENCES,0.2453358208955224,"[78] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry.
TRAK: Attributing model behavior at scale. arXiv preprint arXiv:2303.14186, 2023."
REFERENCES,0.2462686567164179,"[79] Kaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical
University of Denmark, 7(15):510, 2008."
REFERENCES,0.24720149253731344,"[80] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with
implicit gradients. Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.24813432835820895,"[81] Samrudhdhi B Rangrej and James J Clark. A probabilistic hard attention model for sequentially
observed scenes. arXiv preprint arXiv:2111.07534, 2021."
REFERENCES,0.2490671641791045,"[82] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. In International Conference on Machine
Learning, pages 1278–1286. PMLR, 2014."
REFERENCES,0.25,"[83] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ""Why should I trust you?"" Explaining
the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pages 1135–1144, 2016."
REFERENCES,0.25093283582089554,"[84] BP Roe, HJ Yand, J Zhu, Y Lui, I Stancu, et al. Boosted decision trees, an alternative to
artificial neural networks. Nucl. Instrm. Meth. A, 543:577–584, 2005."
REFERENCES,0.251865671641791,"[85] Nikunj Saunshi, Arushi Gupta, Mark Braverman, and Sanjeev Arora. Understanding influence
functions and datamodels via harmonic analysis. arXiv preprint arXiv:2210.01072, 2022."
REFERENCES,0.25279850746268656,"[86] Patrick Schwab and Walter Karlen. CXPlain: Causal explanations for model interpretation
under uncertainty. In Advances in Neural Information Processing Systems, pages 10220–10230,
2019."
REFERENCES,0.2537313432835821,"[87] Robert Schwarzenberg, Nils Feldhus, and Sebastian Möller. Efficient explanations from
empirical explainers. arXiv preprint arXiv:2103.15429, 2021."
REFERENCES,0.25466417910447764,"[88] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi
Parikh, and Dhruv Batra. Grad-CAM: Visual explanations from deep networks via gradient-
based localization. In Proceedings of the IEEE International Conference on Computer Vision,
pages 618–626, 2017."
REFERENCES,0.2555970149253731,"[89] Lloyd S Shapley. A value for n-person games. Contributions to the Theory of Games, 2(28):
307–317, 1953."
REFERENCES,0.25652985074626866,"[90] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features
through propagating activation differences. In International Conference on Machine Learning,
pages 3145–3153. PMLR, 2017."
REFERENCES,0.2574626865671642,"[91] Rui Shu.
Amortized optimization, 2017.
URL http://ruishu.io/2017/11/07/
amortized-optimization/."
REFERENCES,0.2583955223880597,"[92] Grah Simon and Thouvenot Vincent. A projected stochastic gradient algorithm for estimating
Shapley value applied in attribute importance. In International Cross-Domain Conference for
Machine Learning and Knowledge Extraction, pages 97–115. Springer, 2020."
REFERENCES,0.2593283582089552,"[93] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034,
2013."
REFERENCES,0.26026119402985076,"[94] Sumedha Singla, Brian Pollack, Junxiang Chen, and Kayhan Batmanghelich. Explanation by
progressive exaggeration. arXiv preprint arXiv:1911.00483, 2019."
REFERENCES,0.26119402985074625,"[95] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. Smooth-
grad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017."
REFERENCES,0.2621268656716418,"[96] Erik Štrumbelj and Igor Kononenko. An efficient explanation of individual classifications
using game theory. Journal of Machine Learning Research, 11:1–18, 2010."
REFERENCES,0.2630597014925373,"[97] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages
3319–3328. JMLR. org, 2017."
REFERENCES,0.26399253731343286,"[98] Mukund Sundararajan, Kedar Dhamdhere, and Ashish Agarwal. The Shapley taylor interaction
index. In International Conference on Machine Learning, pages 9259–9268. PMLR, 2020."
REFERENCES,0.26492537313432835,"[99] Aad W Van der Vaart. Asymptotic Statistics, volume 3. Cambridge University Press, 2000."
REFERENCES,0.2658582089552239,"[100] Sahil Verma, John Dickerson, and Keegan Hines. Counterfactual explanations for machine
learning: A review. arXiv preprint arXiv:2010.10596, 2020."
REFERENCES,0.2667910447761194,"[101] Sahil Verma, Keegan Hines, and John P Dickerson. Amortized generation of sequential
algorithmic recourses for black-box models. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 36, pages 8512–8519, 2022."
REFERENCES,0.2677238805970149,"[102] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without
opening the black box: Automated decisions and the GDPR. Harvard Journal of Law &
Technology, 31:841, 2017."
REFERENCES,0.26865671641791045,"[103] Tianhao Wang and Ruoxi Jia. Data Banzhaf: A data valuation framework with maximal
robustness to learning stochasticity. arXiv preprint arXiv:2205.15466, 2022."
REFERENCES,0.269589552238806,"[104] Tianhao Wang, Yu Yang, and Ruoxi Jia. Improving cooperative game theory-based data
valuation via data utility learning. arXiv preprint arXiv:2107.06336, 2021."
REFERENCES,0.27052238805970147,"[105] Brian Williamson and Jean Feng. Efficient nonparametric statistical inference on population
feature importance using Shapley values. In International Conference on Machine Learning,
pages 10282–10291. PMLR, 2020."
REFERENCES,0.271455223880597,"[106] Mengmeng Wu, Ruoxi Jia, Changle Lin, Wei Huang, and Xiangyu Chang. Variance reduced
Shapley value estimation for trustworthy data valuation. Computers & Operations Research,
page 106305, 2023."
REFERENCES,0.27238805970149255,"[107] Zhaoxuan Wu, Yao Shu, and Bryan Kian Hsiang Low. Davinz: Data valuation using deep
neural networks at initialization. In International Conference on Machine Learning, pages
24150–24176. PMLR, 2022."
REFERENCES,0.2733208955223881,"[108] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. INVASE: Instance-wise variable
selection using neural networks. In International Conference on Learning Representations,
2018."
REFERENCES,0.27425373134328357,"[109] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European Conference on Computer Vision, pages 818–833. Springer, 2014."
REFERENCES,0.2751865671641791,"[110] Borui Zhang, Baotong Tian, Wenzhao Zheng, Jie Zhou, and Jiwen Lu. Exploring unified
perspective for fast Shapley value estimation. arXiv preprint arXiv:2311.01010, 2023."
REFERENCES,0.27611940298507465,"[111] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan
Sclaroff. Top-down neural attention by excitation backprop. International Journal of Computer
Vision, 126(10):1084–1102, 2018."
REFERENCES,0.27705223880597013,"A
Extended Related Work"
REFERENCES,0.27798507462686567,"This section provides a more detailed review of amortization in XML. Many XML tasks involve
analyzing individual data points, e.g., to determine the most important features or concepts in a
model, or a data point’s influence on the model’s accuracy or an individual prediction. Among these
methods, many require expensive per-example algorithms, and a trend in recent years has been to
amortize this computation using deep learning. We discuss these works below, grouping them into
several main XML problems that they address."
REFERENCES,0.2789179104477612,"Within this discussion, we differentiate works not only based on their goal (e.g., feature attribution or
data valuation) but also based on how they calculate the object of interest. For those that perform
per-example computation, we distinguish between methods that solve a parametric optimization
problem of the form a(b) = arg mina′ h(a′; b) (e.g., with gradient descent) from those that exploit
an analytic solution, either by directly calculating the result or using a Monte Carlo approximation.
For those that use amortization, we distinguish those that use regression-based amortization with
Lreg(θ) from those that perform objective-based amortization with Lobj(θ) (see Appendix C for
more details on objective-based amortization [2])."
REFERENCES,0.2798507462686567,"Table 1 summarizes the methods we discuss, including the various tasks they solve, the context and
output domains (A, B) for each problem, and the computational approach. This perspective highlights
the significant role of amortization in XML, and it also reveals opportunities for new applications,
some of which we discuss in Section 4. Some of the same works are discussed by Chuang et al.
[13], but we cover a wider range of methods, and we adopt the framing of Amos [2] by outlining the
various parametric optimization problems and different amortization approaches."
REFERENCES,0.28078358208955223,"Feature attribution. These methods aim to quantify the importance of a model’s input features,
typically for individual predictions (such methods are called local rather than global feature attribu-
tions, see Lundberg et al. [69]). The context is therefore a data example x ∈X, and the output is a
vector of attributions in Rd when we have d features. Feature attribution algorithms can be efficient
to calculate, particularly when they are based on simple propagation rules [93, 97, 90, 95, 88, 111, 3]
or a transformer’s attention values [1]. However, another family of approaches are based on feature
removal [17], and querying the model with many feature subsets is typically less efficient."
REFERENCES,0.28171641791044777,"Among the feature removal-based methods, one of the most famous is Shapley value feature attribu-
tions [68, 89]. Two related methods are LIME [83] and Banzhaf values [11, 4], see our discussion of
their similarity in Section 4. Many works have focused on efficiently calculating these attributions,
because their computational complexity is exponential in the number of features. As discussed by
Chen et al. [10], there are many stochastic estimators derived from an analytic expression for the attri-
butions [6, 96, 77, 16, 74, 103, 58], and these are typically unbiased Monte Carlo estimators. Others
are based on solving an optimization problem, for example with either M-estimation [83, 68, 105, 16]
or gradient descent [92]."
REFERENCES,0.2826492537313433,"Lastly, there are also methods based on amortization, which offer the possibility of real-time feature
attribution without a significant loss in accuracy. Two works consider objective-based amortization,
which avoids the need for labels during training [50, 18]. Others consider regression-based amortiza-
tion [86, 87, 14], and these recommend using high-quality labels; for example, Chuang et al. [14] use
exact labels when possible. Our perspective is most similar to the concurrent work by Zhang et al.
[110], in that we highlight the potential for regression-based amortization with inexpensive, noisy
labels; but our work is more general in that we highlight the potential to use noisy labels from any
unbiased estimator, and the applicability of this approach to other XML tasks like data valuation."
REFERENCES,0.2835820895522388,"Instance-wise feature selection. A related set of methods select the most important features for
individual predictions. The context in this case is an input x ∈X, and the output is a set of
feature indices S ∈P([d]) where we use P(·) to denote the power set. The optimal subset can
be defined either based on the prediction f(xS) or f(x[d]\S) for a specific class [31], or based
on the deviation from the original prediction f(x) [12]. Among these methods, some solve the
underlying optimization problem on a per-input basis [31, 30], and others solve it using objective-
based amortization [21, 12, 108, 49]. These methods all require differentiating through discrete
subsets, so they employ various continuous relaxations (e.g., the Concrete distribution [71]), as well
as other tricks to constrain or penalize the subset size. These methods are sometimes thought of
as attribution methods due to the continuous relaxations [31, 30], but we describe them as feature"
REFERENCES,0.28451492537313433,Table 1: Summary of amortized methods in XML.
REFERENCES,0.28544776119402987,"Problem
Context
Domain
Analytic
Per-example optimization
Lreg(θ)
Lobj(θ)"
REFERENCES,0.28638059701492535,Shapley value attribution
REFERENCES,0.2873134328358209,"x ∈X
Rd"
REFERENCES,0.28824626865671643,"Štrumbelj and Kononenko [96]
Okhrati and Lipani [77]
Mitchell et al. [74]"
REFERENCES,0.2891791044776119,"Lundberg and Lee [68]
Simon and Vincent [92]
Covert and Lee [16]"
REFERENCES,0.29011194029850745,"Schwarzenberg et al. [87]
Chuang et al. [14]
Zhang et al. [110]"
REFERENCES,0.291044776119403,"Jethani et al. [50]
Covert et al. [18]"
REFERENCES,0.29197761194029853,"Leave-one-out attribution
Zeiler and Fergus [109]
Schwab and Karlen [86]"
REFERENCES,0.292910447761194,"LIME attribution
Ribeiro et al. [83]"
REFERENCES,0.29384328358208955,"Banzhaf value attribution
Covert et al. [17]
Chen and Jordan [11]"
REFERENCES,0.2947761194029851,Instance-wise selection
REFERENCES,0.2957089552238806,"x ∈X
P([d])"
REFERENCES,0.2966417910447761,"Chen et al. [12]
Yoon et al. [108]
Jethani et al. [49]"
REFERENCES,0.29757462686567165,"Image masking
Fong and Vedaldi [31]
Fong et al. [30]
Dabkowski and Gal [21]"
REFERENCES,0.29850746268656714,"Dynamic feature selection
xS ∈X × P([d])
[d]
Ma et al. [70]
He et al. [42]
Chattopadhyay et al. [9]
Covert et al. [19]"
REFERENCES,0.2994402985074627,"Counterfactual explanation
x ∈X
X
Wachter et al. [102]
Mahajan et al. [72]
Verma et al. [101]"
REFERENCES,0.3003731343283582,Data Shapley
REFERENCES,0.30130597014925375,"zi ∈D
R"
REFERENCES,0.30223880597014924,"Ghorbani and Zou [33]
Ghorbani et al. [35, 36]
Li and Yu [65]"
REFERENCES,0.3031716417910448,"Beta Shapley
Kwon and Zou [60]"
REFERENCES,0.3041044776119403,"Data Banzhaf
Wang and Jia [103]"
REFERENCES,0.3050373134328358,"Datamodels
(zi, x) ∈D × X
R"
REFERENCES,0.30597014925373134,"selection methods because the final scores for each feature are restricted to the range [0, 1] and are
made continuous mainly for optimization purposes."
REFERENCES,0.3069029850746269,"Dynamic feature selection. Next, other works select features separately for each prediction to
achieve high accuracy with a small feature acquisition budget. Each selection is made based on
a partially observed input, so the context is a feature subset xS, which we write as belonging to
the domain X × P([d]), and the output is an index i ∈[d]. There are several ways to define the
optimal selection at each step, but one common approach is to define it as the feature with maximum
conditional mutual information, or i∗= arg maxi I(y; xi | xS), where the response variable y and
unobserved features xi are random variables and xS has a fixed observed value. Given access to this
objective, it is trivial to solve by enumerating the possible indices; however, the mutual information
is typically unavailable in practice and must be approximated. One approach is therefore to fit a
proxy for the mutual information (e.g., via a generative model) and then optimize this proxy for each
selection [70, 81, 8, 44]. An alternative is to learn a network that predicts the optimal selection at each
step: He et al. [42] do so using imitation learning, which resembles regression-based amortization,
and Chattopadhyay et al. [9] and Covert et al. [19] do so with an optimization-based view of the
mutual information I(y; xi | xS), which is objective-based amortization."
REFERENCES,0.30783582089552236,"Counterfactual explanation. The goal of counterfactual explanations (also known as recourse
explanations) is to identify small changes to an input that cause a desired change in a model’s
prediction. The context is an input x ∈X, and the output is a modified version x′ ∈X that is
typically not too different from the original input. This family of methods was introduced by Wachter
et al. [102] and is generally framed as an optimization problem involving the prediction f(x′) and
a measure of the perturbation strength between x′ and x. Verma et al. [100] provide a review of
these methods and the various choices for the optimization formulation. When computing these
explanations, the most common approach is to solve the problem separately for each input, e.g., using
gradient descent [102]. Other works have explored learning models that directly output the modified
example x′, and these are typically implemented using objective-based amortization [72, 94, 101, 13]."
REFERENCES,0.3087686567164179,"Data valuation. The goal of data valuation is to assign scores to each training example that represent
their contribution to a model’s performance. The context is therefore a labeled training example
z ∈D, and the output is a real-valued score. These methods are typically not defined via an
optimization problem, but instead as a measure of the example’s expected impact on performance
across a distribution of preceding datasets [33, 35, 60, 103]. Existing methods therefore rely on
analytic expressions for the valuation scores, which require Monte Carlo approximation because
they are intractable. Two works have considered predicting data valuation scores given a dataset
of near-exact estimates [35, 36], which is regression-based amortization. Our work is similar, but
we use inexpensive estimates to reduce the cost of amortization and scale more efficiently to larger
datasets. Concurrently, Li and Yu [65] proposed a learning-based approach analogous to FastSHAP
for feature attribution [50], but whose memory requirements scale with the dataset size and limit
applicability to large datasets. Besides these methods, there are also data valuation approaches that
can be calculated without any approximations [51, 54, 61]."
REFERENCES,0.30970149253731344,"Data attribution. Finally, data attribution methods aim to quantify the effect of training examples on
individual predictions. The context is therefore a training example z ∈D paired with an inference
example x ∈X, and the output is a real-valued attribution score. One classic approach to this problem
is influence functions [15], which use a gradient-based approximation to avoid the cost of retraining
[57, 37, 63]. Another class of methods involve measuring the effect of training with subsampled
datasets [28, 47]. For datamodels, the main existing approximation algorithm is based on solving a
global least squares problem [47], which does not correspond to any of the computational approaches
listed in Table 1; notably, it is not a form of amortization because there is no model that can predict
the attribution given a new tuple (z, x). As an alternative to the global least squares problem, TRAK
calculates similar scores to datamodels using a gradient-based approximation [78, 27]. We are not
aware of any work investigating amortization for datamodels, but our results in Appendix B show
how to approximate the scores based on their analytic solution, and how to amortize the computation
by adopting Monte Carlo estimates as noisy training labels."
REFERENCES,0.310634328358209,"B
Datamodels"
REFERENCES,0.31156716417910446,"The datamodels technique [47] aims to quantify how much each training data point zi ∈D affects
the prediction for an inference example x ∈X. Similar to data valuation, the inference example’s
output given a training dataset DT is represented by a function vx(DT ), and the attribution scores
ζ(zi, x) ∈R are then defined as the solution to a joint least squares problem that can be solved after
training many models with different datasets, as we show below.8 For the inference example’s output
vx(DT ), Ilyas et al. [47] focus on the loss rather than the raw prediction, but our perspective can
accommodate any definition of this output."
REFERENCES,0.3125,"Our insight on the datamodels technique is twofold. First, we show that the datamodels scores are
equal to a simple expectation and can be estimated in an unbiased fashion. Second, we show that
these calculations can be amortized by using the noisy Monte Carlo estimates as training targets.
Our findings rely on a slight reformulation of the datamodels scores, where we deviate from Ilyas
et al. [47] by using an intercept term and a subtly different dataset distribution: our distribution
is biased towards size nq for a value q ∈(0, 1) rather than using a fixed size, which was also
considered in recent work by Saunshi et al. [85]. In the following result, we use the shorthand
ζ(x) ≡[ζ(z1, x), . . . , ζ(zn, x)] for the vector of data attributions. Given a probability q ∈(0, 1), we
also define the weighting function u(k) = qk(1 −q)n−1−k. With this setup, our reformulation is the
following (see the proof in Appendix D).
Proposition 1. Given a subset distribution that includes each data point zi ∈D with probability
q ∈(0, 1), the data attribution scores defined by"
REFERENCES,0.31343283582089554,"ζ(x) ≡arg min
a∈Rn+1 ET   "
REFERENCES,0.314365671641791,"a0 +
X"
REFERENCES,0.31529850746268656,"i∈T
ai −vx(DT ) !2 "
REFERENCES,0.3162313432835821,can be expressed as the following expectation:
REFERENCES,0.31716417910447764,"ζ(zi, x) =
X"
REFERENCES,0.3180970149253731,"T ⊆[n]\i
u(|T|) (vx(DT ∪{zi}) −vx(DT )) ."
REFERENCES,0.31902985074626866,"Perhaps surprisingly, this corresponds to a game-theoretic semivalue and reduces to the Banzhaf
value when q = 1/2 [73]. Based on this perspective, we can estimate the score ζ(zi, x) without
solving the regression problem from Ilyas et al. [47]. For example, we can simply sample k datasets
DT from the distribution with q ∈(0, 1), and then calculate the empirical average as follows:"
REFERENCES,0.3199626865671642,"ˆζ(zi, x) = 1 k k
X"
REFERENCES,0.3208955223880597,"j=1
vx(DTj ∪{zi}) −vx(DTj \ {zi})."
REFERENCES,0.3218283582089552,"Following Proposition 1, we have the unbiasedness property E[ˆζ(zi, x)] = ζ(zi, x). Furthermore,
rather than repeating this estimation for each attribution score, we can amortize the process by fitting
an attribution model ζ(zi, x; θ) using the noisy estimates ˆζ(zi, x) as training targets. Our experiments
do not test this approach, which we leave as a direction for future work. Implementing this requires
a more complex architecture to handle two model inputs x and z, but we speculate that if trained
effectively, amortization could accelerate attribution within the training set and generalize to new
inference examples x ∈X with no additional overhead."
REFERENCES,0.32276119402985076,"8As we noted in the main text, although datamodels performs data attribution by fitting a linear regression
model, it is not a form of amortization because the resulting model cannot predict attribution scores ζ(z, x) for
new datapoints. The attribution scores are given by the model’s coefficients, not its predictions."
REFERENCES,0.32369402985074625,"C
Connection to Objective-Based Amortization"
REFERENCES,0.3246268656716418,"Recall that many tasks with repeated computation have an optimization view where a(b) ≡
arg mina′ h(a′; b) [91, 2] (see Section 2). For completeness, this section describes an interpretation
of stochastic amortization from this optimization perspective. An alternative to regression-based
amortization when we have an objective h(a′; b) that defines a(b) is to train a(b; θ) directly with the
h objective:"
REFERENCES,0.3255597014925373,"Lobj(θ) = E[h(a(b; θ); b)].
(12)"
REFERENCES,0.32649253731343286,"This approach provides an alternative when exact labels are costly, but it can be unappealing because
(i) the task may not offer a natural objective h(a′; b), and (ii) minimizing h(a′; b) may seem discon-
nected from the error ∥a(b; θ) −a(b)∥. We discuss these issues below and how they are related to
stochastic amortization."
REFERENCES,0.32742537313432835,"For issue (i), certain problems like Shapley values have a natural optimization characterization (see
Section 4.1), but this is not always the case: for example, data valuation scores are framed as an
expectation rather than via optimization (see Section 4.3 or [33]), so they lack an optimization view.
We always have the trivial optimization perspective h(a′; b) = ∥a′ −a(b)∥2, but this is not useful
because it reduces Lobj(θ) to Lreg(θ) and requires the exact outputs a(b). Regardless of whether
the task offers a natural objective h(a′; b), stochastic amortization can be viewed as defining a new
objective that does not require the exact outputs: given a noisy oracle ˜a(b), stochastic amortization is
equivalent to Lobj(θ) with the objective h(a′; b) = E

∥a′ −˜a(b)∥2
, and if the oracle is unbiased
then the optimal predictions are a(b; θ) = a(b)."
REFERENCES,0.3283582089552239,"Next, issue (ii) is a concern when h(a′; b) is available but lacks a clear connection to the estimation
error, which is directly reflected by Lreg(θ). The squared error is in many cases a more meaningful
accuracy measure, and it is commonly used to evaluate estimation accuracy for feature attribution
and data valuation [50, 103, 10]. Certain works on objective-based amortization have considered non-
convex objectives h(a′; b) where the exact output a(b) is not well-defined [2], but we can understand
the potential disconnect by focusing on a class of well behaved objectives. In particular, if we
assume that h(a′; b) is α-strongly convex and β-smooth in a′ for all b, then the Lreg(θ) and Lobj(θ)
objectives bound one other as follows, α"
REFERENCES,0.3292910447761194,"2 Lreg(θ) ≤Lobj(θ) −L∗
obj ≤β"
REFERENCES,0.3302238805970149,"2 Lreg(θ),
(13)"
REFERENCES,0.33115671641791045,"where we define L∗
obj ≡E[h(a(b); b)] (see proof in Appendix D.1). Eq. (13) shows that by mini-
mizing Lobj(θ), we effectively minimize both upper and lower bounds on Lreg(θ). However, these
bounds can be loose: for example, because we have Lreg(θ) ≤2"
REFERENCES,0.332089552238806,"αLobj(θ) and the strong convexity
constant α shrinks to zero for Shapley values as the number of features grows [18], optimizing
Lobj(θ) may become less effective in high dimensions. Stochastic amortization resolves this potential
disconnect between Lobj(θ) and Lreg(θ) as follows: if we use an unbiased noisy oracle ˜a(b), then
training with ˜Lreg(θ) is equivalent to using Lobj(θ) with h(a′; b) = E

∥a′ −˜a(b)∥2
, so we have
α = β = 1 in Eq. (13) and the regression objectives Lreg(θ) and ˜Lreg(θ) are equal up to a constant
L∗
obj = N(˜a)."
REFERENCES,0.33302238805970147,"D
Proofs"
REFERENCES,0.333955223880597,"This section provides proofs for our claims in the main text. Appendix D.1 shows proofs for our
results related to stochastic amortization in Section 3 and Appendix C, and then Appendix D.2 shows
the proof for Proposition 1 related to datamodels."
REFERENCES,0.33488805970149255,"D.1
Amortization proofs"
REFERENCES,0.3358208955223881,"We first derive the inequality in Eq. (4) from the main text, which is the following: q"
REFERENCES,0.33675373134328357,"˜Lreg(θ) −N(˜a) −
p"
REFERENCES,0.3376865671641791,"B(˜a)
2
≤Lreg(θ) ≤
q"
REFERENCES,0.33861940298507465,"˜Lreg(θ) −N(˜a) +
p"
REFERENCES,0.33955223880597013,"B(˜a)
2
."
REFERENCES,0.34048507462686567,"Proof. We begin by decomposing a per-input version of the noisy oracle loss ˜Lreg(b; θ), which takes
an expectation over the noisy oracle ˜a(b) for a fixed b:"
REFERENCES,0.3414179104477612,"˜Lreg(b; θ) = E

∥a(b; θ) −˜a(b)∥2 | b
"
REFERENCES,0.3423507462686567,= ∥a(b; θ) −E[˜a(b) | b]∥2 + E[∥˜a(b) −E[˜a(b) | b]∥2 | b]
REFERENCES,0.34328358208955223,= ∥a(b; θ) −E[˜a(b) | b]∥2 + N(˜a | b).
REFERENCES,0.34421641791044777,"Next, we can also decompose a per-input version of the original regression loss Lreg(b; θ) as follows
using triangle inequality:"
REFERENCES,0.3451492537313433,Lreg(b; θ) = ∥a(b; θ) −a(b)∥2
REFERENCES,0.3460820895522388,"≤(∥a(b; θ) −E[˜a(b) | b]∥+ ∥a(b) −E[˜a(b) | b]∥)2 =
q"
REFERENCES,0.34701492537313433,"˜Lreg(b; θ) −N(˜a | b) +
p"
REFERENCES,0.34794776119402987,"B(˜a | b)
2
."
REFERENCES,0.34888059701492535,"Taking both sides of the inequality in expectation over p(b), we arrive at the following upper bound:"
REFERENCES,0.3498134328358209,"Lreg(θ) ≤E ""q"
REFERENCES,0.35074626865671643,"˜Lreg(b; θ) −N(˜a | b) +
p"
REFERENCES,0.3516791044776119,"B(˜a | b)
2# ."
REFERENCES,0.35261194029850745,"One side of the bound in Eq. (4) comes from developing the square and applying Cauchy-Schwarz to
the cross term:"
REFERENCES,0.353544776119403,"Lreg(θ) ≤E ""q"
REFERENCES,0.35447761194029853,"˜Lreg(b; θ) −N(˜a | b) +
p"
REFERENCES,0.355410447761194,"B(˜a | b)
2#"
REFERENCES,0.35634328358208955,"= E
h
˜Lreg(b; θ) −N(˜a | b)
i
+ E [B(˜a | b)] + 2E ""r"
REFERENCES,0.3572761194029851,"B(˜a | b)

˜Lreg(b; θ) −N(˜a | b)
#"
REFERENCES,0.3582089552238806,≤˜Lreg(θ) −N(˜a) + B(˜a) + 2 r
REFERENCES,0.3591417910447761,"B(˜a)

˜Lreg(θ) −N(˜a)
 =
q"
REFERENCES,0.36007462686567165,"˜Lreg(θ) −N(˜a) +
p"
REFERENCES,0.36100746268656714,"B(˜a)
2
."
REFERENCES,0.3619402985074627,"This proves the upper bound in Eq. (4). For the lower bound, we return to the decomposition of the
per-input noisy oracle loss ˜Lreg(b; θ) and apply triangle inequality as follows:"
REFERENCES,0.3628731343283582,˜Lreg(b; θ) = ∥a(b; θ) −E[˜a(b) | b]∥2 + N(˜a | b)
REFERENCES,0.36380597014925375,"≤(∥a(b; θ) −a(b)∥+ ∥a(b) −E[˜a(b) | b]∥)2 + N(˜a | b) =
q"
REFERENCES,0.36473880597014924,"Lreg(b; θ) +
p"
REFERENCES,0.3656716417910448,"B(˜a | b)
2
+ N(˜a | b)."
REFERENCES,0.3666044776119403,"Rearranging terms, we have"
REFERENCES,0.3675373134328358,"Lreg(b; θ) ≥
q"
REFERENCES,0.36847014925373134,"˜Lreg(b; θ) −N(˜a | b) −
p"
REFERENCES,0.3694029850746269,"B(˜a | b)
2
,"
REFERENCES,0.37033582089552236,"and applying the same logic as above with Cauchy-Schwarz, we arrive at:"
REFERENCES,0.3712686567164179,"Lreg(θ) ≥
q"
REFERENCES,0.37220149253731344,"˜Lreg(θ) −N(˜a) −
p"
REFERENCES,0.373134328358209,"B(˜a)
2
."
REFERENCES,0.37406716417910446,"Putting the two results together, this yields the two-sided bound in Eq. (4): q"
REFERENCES,0.375,"˜Lreg(θ) −N(˜a) −
p"
REFERENCES,0.37593283582089554,"B(˜a)
2
≤Lreg(θ) ≤
q"
REFERENCES,0.376865671641791,"˜Lreg(θ) −N(˜a) +
p"
REFERENCES,0.37779850746268656,"B(˜a)
2
."
REFERENCES,0.3787313432835821,"Next, we prove our SGD result for stochastic amortization in Theorem 1."
REFERENCES,0.37966417910447764,Theorem 1. Consider a noisy oracle ˜a(b) that satisfies E[˜a(b) | b] = ˜θb with parameters ˜θ ∈Rm×d
REFERENCES,0.3805970149253731,"such that ∥˜θ∥F ≤D. Given a distribution p(b), define the norm-weighted distribution q(b) ∝
p(b) · ∥b∥2 and the terms Σp ≡Ep[bb⊤] and Σq ≡Eq[bb⊤]. If we train a linear model a(θ; b) = θb
with the noisy objective ˜Lreg(θ) using SGD with step size ηt =
2
α(t+1), then the averaged iterate
¯θT = PT
t=1
2t
T (T +1)θt at step T satisfies"
REFERENCES,0.38152985074626866,"E[ ˜Lreg(¯θT )] −N(˜a) ≤4 Tr(Σp)
 
Nq(˜a) + 4λmax(Σq)D2"
REFERENCES,0.3824626865671642,"λmin(Σp)(T + 1)
,"
REFERENCES,0.3833955223880597,"where Nq(˜a) ≡Eq[N(˜a | b)] is the noisy oracle’s norm-weighted variance, and λmax(·), λmin(·) are
the maximum and minimum eigenvalues."
REFERENCES,0.3843283582089552,"Proof. Given our assumption about the noisy oracle, we can re-write the noisy objective ˜Lreg(θ) as
follows:"
REFERENCES,0.38526119402985076,"˜Lreg(θ) = E
h
∥(θ −˜θ)b∥2i
+ N(˜a) = Tr

(θ −˜θ)⊤Σp(θ −˜θ)

+ N(˜a)."
REFERENCES,0.38619402985074625,"It is clear that the minimizer is θ = ˜θ and that the minimum achievable error is ˜Lreg(˜θ) = N(˜a).
Because the objective composes a linear function with a convex function, it is convex in θ and we
can analyze its convergence using a standard SGD result. Specifically, because we assume that
∥˜θ∥F ≤D, we consider a projected SGD algorithm where each step is followed by a projection into
the D-ball, or θ ←θ · min(1, D/∥θ∥F ). Note that it is common to assume a bounded solution space
when proving SGD’s convergence [76, 22, 5]."
REFERENCES,0.3871268656716418,"Before proceeding to the main convergence result, we must derive several properties of this objective
and its stochastic gradients. The first is the strong convexity constant, and we begin by noting that the
gradient is the following (see Section 2.5 of Petersen et al. [79] for a review of matrix derivatives):"
REFERENCES,0.3880597014925373,∇˜Lreg(θ) = 2(θ −˜θ)E[bb⊤] = 2(θ −˜θ)Σp.
REFERENCES,0.38899253731343286,"For the strong convexity constant, we require a value α > 0 such that the following is satisfied for all
parameters θ, θ′:"
REFERENCES,0.38992537313432835,"˜Lreg(θ) ≥˜Lreg(θ′) + Tr

(θ −θ′)⊤∇˜Lreg(θ′)

+ α"
REFERENCES,0.3908582089552239,"2 ∥θ −θ′∥2
F .
(14)"
REFERENCES,0.3917910447761194,"For the first two terms on the right side of the inequality, we can write:"
REFERENCES,0.3927238805970149,"˜Lreg(θ′) + Tr

(θ −θ′)⊤∇˜Lreg(θ′)

= N(˜a) + Tr

(θ′ −˜θ)Σp(θ′ −˜θ)⊤
+ 2 Tr

(θ′ −˜θ)Σp(θ −θ′)⊤"
REFERENCES,0.39365671641791045,"= N(˜a) + Tr

(θ −˜θ)Σp(θ −˜θ)⊤
−Tr
 
(θ′ −θ)Σp(θ′ −θ)⊤
."
REFERENCES,0.394589552238806,"By using the minimum eigenvalue of Σp, we can write the following,"
REFERENCES,0.39552238805970147,"Tr
 
(θ′ −θ)Σp(θ′ −θ)⊤
≥λmin(Σp)∥θ −θ′∥2
F ,"
REFERENCES,0.396455223880597,and therefore satisfy Eq. (14) as follows:
REFERENCES,0.39738805970149255,"˜Lreg(θ) = N(˜a) + Tr

(θ −˜θ)Σp(θ −˜θ)⊤"
REFERENCES,0.3983208955223881,"= ˜Lreg(θ′) + Tr

(θ −θ′)⊤∇˜Lreg(θ′)

+ Tr
 
(θ′ −θ)Σp(θ′ −θ)⊤"
REFERENCES,0.39925373134328357,"≥˜Lreg(θ′) + Tr

(θ −θ′)⊤∇˜Lreg(θ′)

+ λmin(Σp)∥θ −θ′∥2
F ."
REFERENCES,0.4001865671641791,We can therefore conclude that ˜Lreg(θ) is 2λmin(Σp)-strongly convex.
REFERENCES,0.40111940298507465,"The next property to derive is the expected gradient norm. When running SGD, we make updates
using the following stochastic gradient estimate:"
REFERENCES,0.40205223880597013,g(θ) = 2 (θb −˜a(b)) b⊤.
REFERENCES,0.40298507462686567,"We require an upper bound on the stochastic gradient norm, which can be written as follows:"
REFERENCES,0.4039179104477612,"E

∥g(θ)∥2
F

= E
h
∥g(θ) −∇˜Lreg(θ)∥2
F
i
+ ∥∇˜Lreg(θ)∥2
F .
(15)"
REFERENCES,0.4048507462686567,"For the first term in Eq. (15), which represents the gradient variance, we have:"
REFERENCES,0.40578358208955223,"E
h
∥g(θ) −∇˜Lreg(θ)∥2
F
i
= 4E
h
∥(θb −˜a(b))b⊤−(θ −˜θ)Σp∥2
F
i
."
REFERENCES,0.40671641791044777,"To find a simple expression for this term, we first consider the expectation over the distribution ˜a(b)
with fixed b ∈B, which isolates label variation due to the noisy oracle:"
REFERENCES,0.4076492537313433,"E˜a|b
h
∥(θb −˜a(b))b⊤−(θ −˜θ)Σp∥2
F
i
= E˜a|b
h
∥(˜a(b) −˜θb)b⊤∥2
F
i
+ ∥(θ −˜θ)(bb⊤−Σp)∥2
F"
REFERENCES,0.4085820895522388,"= N(˜a | b) · ∥b∥2 + ∥(θ −˜θ)(bb⊤−Σp)∥2
F ."
REFERENCES,0.40951492537313433,"When we take this in expectation over p(b), the first term can be understood as a norm-weighted
average of the noisy oracle’s conditional variance:"
REFERENCES,0.41044776119402987,"E

N(˜a | b) · ∥b∥2
=
Z
p(b)N(˜a | b) · ∥b∥2db"
REFERENCES,0.41138059701492535,"= Tr(Σp)
Z p(b) · ∥b∥2"
REFERENCES,0.4123134328358209,Tr(Σp)
REFERENCES,0.41324626865671643,"
N(˜a | b)db"
REFERENCES,0.4141791044776119,= Tr(Σp)Nq(˜a).
REFERENCES,0.41511194029850745,The second term can be rewritten as follows using Σp and Σq:
REFERENCES,0.416044776119403,"E
h
∥(θ −˜θ)(bb⊤−Σp)∥2
F
i
= E
h
Tr

(bb⊤−Σp)(θ −˜θ)⊤(θ −˜θ)(bb⊤−Σp)
i"
REFERENCES,0.41697761194029853,"= E
h
Tr

(θ −˜θ)(bb⊤−Σp)(bb⊤−Σp)(θ −˜θ)⊤i"
REFERENCES,0.417910447761194,"= Tr

(θ −˜θ)E

(bb⊤−Σp)(bb⊤−Σp)

(θ −˜θ)⊤"
REFERENCES,0.41884328358208955,"= Tr

(θ −˜θ)(Tr(Σp)Σq −Σ2
p)(θ −˜θ)⊤
."
REFERENCES,0.4197761194029851,"For the deterministic gradient upper bound in Eq. (15), we have:"
REFERENCES,0.4207089552238806,"∥∇˜Lreg(θ)∥2
F = 4 Tr

(θ −˜θ)Σ2
p(θ −˜θ)⊤
."
REFERENCES,0.4216417910447761,"Putting these results together, we can upper bound the expected gradient norm for any parameter
values ∥θ∥F ≤D as"
REFERENCES,0.42257462686567165,"E

∥g(θ)∥2
F

= 4 Tr(Σp)Nq(˜a) + 4 Tr(Σp) Tr

(θ −˜θ)Σq(θ −˜θ)⊤"
REFERENCES,0.42350746268656714,"≤4 Tr(Σp)Nq(˜a) + 4 Tr(Σp)λmax(Σq)∥θ −˜θ∥2
F
≤4 Tr(Σp)Nq(˜a) + 16 Tr(Σp)λmax(Σq)D2."
REFERENCES,0.4244402985074627,"Using our results for the objective’s strong convexity and expected gradient norm, we can now invoke
a standard SGD convergence result. Following Theorem 6.3 from Bubeck et al. [5], if we make T
updates with the specified step size, we have the following upper bound on the expected objective
value:"
REFERENCES,0.4253731343283582,E[ ˜Lreg(¯θT )] −N(˜a) ≤4 Tr(Σp)Nq(˜a) + 16 Tr(Σp)λmax(Σq)D2
REFERENCES,0.42630597014925375,"λmin(Σp)(T + 1)
.
(16)"
REFERENCES,0.42723880597014924,"Next, we prove a simple consequence of this result, which is that the rate from Theorem 1 applies to
the original objective Lreg(θ) when we assume an unbiased noisy oracle."
REFERENCES,0.4281716417910448,"Corollary 1. Following the setup from Theorem 1, if the noisy oracle is unbiased, then the averaged
iterate at step T satisfies"
REFERENCES,0.4291044776119403,E[Lreg(¯θT )] ≤4 Tr(Σp)Nq(˜a) + 16 Tr(Σp)λmax(Σq)D2
REFERENCES,0.4300373134328358,"λmin(Σp)(T + 1)
."
REFERENCES,0.43097014925373134,"If the oracle is also noiseless, or if we assume access to the exact oracle ˜a(b) = a(b), then the
averaged iterate at step T satisfies"
REFERENCES,0.4319029850746269,E[Lreg(¯θT )] ≤16 Tr(Σp)λmax(Σq)D2
REFERENCES,0.43283582089552236,"λmin(Σp)(T + 1)
."
REFERENCES,0.4337686567164179,"Proof. The first result follows from combining Theorem 1 with the relationship between Lreg(θ)
and ˜Lreg(θ): if we assume the noisy oracle is unbiased, or B(˜a) = 0, then we have Lreg(θ) =
˜Lreg(θ) −N(˜a), which implies the first inequality. The second inequality follows from setting
Nq(˜a) = 0 in the upper bound."
REFERENCES,0.43470149253731344,"Next, we provide a proof for Eq. (13) regarding the connection between regression- and objective-
based amortization, which is the following: α"
REFERENCES,0.435634328358209,"2 Lreg(θ) ≤Lobj(θ) −L∗
obj ≤β"
REFERENCES,0.43656716417910446,2 Lreg(θ).
REFERENCES,0.4375,"Proof. This result relies on well known properties from convex optimization [41]. Consider a fixed
context variable b ∈B. Strong convexity with α > 0 means that for all a, a′ ∈A we have:"
REFERENCES,0.43843283582089554,h(a; b) ≥h(a′; b) + (a −a′)⊤∇ah(a′; b) + α
REFERENCES,0.439365671641791,2 ∥a −a′∥2.
REFERENCES,0.44029850746268656,"Considering the optimal value a′ = a(b), this simplifies to: α"
REFERENCES,0.4412313432835821,"2 ∥a −a(b)∥2 ≤h(a; b) −h(a(b); b).
(17)"
REFERENCES,0.44216417910447764,"Next, smoothness with β > 0 means that for all a, a′ ∈Rd we have:"
REFERENCES,0.4430970149253731,∥∇ah(a; b) −∇ah(a′; b)∥2 ≤β∥a −a′∥.
REFERENCES,0.44402985074626866,Lemma 3.4 in Bubeck et al. [5] shows that β-smoothness also implies the following:
REFERENCES,0.4449626865671642,"h(a; b) −h(a′; b) −(a −a′)⊤∇ah(a′; b)
 ≤β"
REFERENCES,0.4458955223880597,2 ∥a −a′∥2.
REFERENCES,0.4468283582089552,"Considering the optimal value a′ = a(b), this simplifies to:"
REFERENCES,0.44776119402985076,h(a; b) −h(a(b); b) ≤β
REFERENCES,0.44869402985074625,"2 ∥a −a(b)∥2.
(18)"
REFERENCES,0.4496268656716418,"The bounds in Eq. (13) follow from substituting a for predictions from a model a(b; θ), and consider-
ing Eq. (17) and Eq. (18) in expectation across the distribution p(b)."
REFERENCES,0.4505597014925373,"D.2
Datamodels proofs"
REFERENCES,0.45149253731343286,"Before proving our main claim for datamodels in Proposition 1, we first prove a more general result.
This version considers an arbitrary symmetric distribution p(T) over datasets, rather than the specific
distribution parameterized by q ∈(0, 1) used in Proposition 1. By a symmetric distribution, we mean
one that satifies p(T) = p(T ′) whenever |T| = |T ′|."
REFERENCES,0.45242537313432835,"As setup for our derivation, notice that when we assume a symmetric distribution p(T), we can define
the following probabilities that are identical for all indices i, j ∈[n]:"
REFERENCES,0.4533582089552239,"p1 ≡Pr(i ∈T)
for i ∈[n]
p2 ≡Pr(i, j ∈T)
for i ̸= j
p3 ≡Pr(i ∈T, j /∈T)
for i ̸= j."
REFERENCES,0.4542910447761194,"Note that we have p1 = p2 + p3. For example, given a uniform distribution p(T) over subsets with
size |T| = k, which is used by Ilyas et al. [47], we have: p1 ="
REFERENCES,0.4552238805970149," n−1
k−1
"
REFERENCES,0.45615671641791045," n
k

= k"
REFERENCES,0.457089552238806,"n
p2 ="
REFERENCES,0.45802238805970147," n−2
k−1
"
REFERENCES,0.458955223880597," n
k

= k(k −1)"
REFERENCES,0.45988805970149255,"n(n −1)
p3 ="
REFERENCES,0.4608208955223881," n−2
k−1
"
REFERENCES,0.46175373134328357," n
k

= k(n −k)"
REFERENCES,0.4626865671641791,n(n −1).
REFERENCES,0.46361940298507465,"Alternatively, if we have each data point included independently with probability q ∈(0, 1), which is
used by Saunshi et al. [85] and Proposition 1, then we have p1 = q, p2 = q2, and p3 = q(1 −q)."
REFERENCES,0.46455223880597013,Our more general claim about datamodels with a symmetric distribution p(T) is the following.
REFERENCES,0.46548507462686567,"Lemma 1. Given a symmetric distribution p(T), the data attribution scores defined by"
REFERENCES,0.4664179104477612,"ζ(x) ≡arg min
a∈Rn+1 Ep(T )   "
REFERENCES,0.4673507462686567,"a0 +
X"
REFERENCES,0.46828358208955223,"i∈T
ai −vx(DT ) !2 "
REFERENCES,0.46921641791044777,"can be expressed as the expectation ζ(zi, x) = E[ci(T)vx(DT )], where we define the weighting
function ci(T) as follows:"
REFERENCES,0.4701492537313433,ci(T) ≡1 p3
REFERENCES,0.4710820895522388,"
1(i ∈T) −
p2 −p2
1
p3 + n(p2 −p2
1)|T| −
p1p3
p3 + n(p2 −p2
1) 
."
REFERENCES,0.47201492537313433,"Proof. We can write the problem’s partial derivatives as follows, both for the intercept term a0 and
the coefficients ai for i ∈[n]:"
REFERENCES,0.47294776119402987,"∂
∂a0
E
h 
a0 + a⊤1T −vx(DT )
2i
= 2
 
a0 + E

1⊤
T a

−E [vx(DT )]
"
REFERENCES,0.47388059701492535,"= 2
 
a0 + p11⊤
n a −E [vx(DT )]
"
REFERENCES,0.4748134328358209,"∂
∂ai
E
h 
a0 + a⊤1T −vx(DT )
2i
= 2
 
E

1(i ∈T)(1⊤
T a + a0)

−E [1(i ∈T)vx(DT )]
"
REFERENCES,0.47574626865671643,"= 2
 
p1a0 + p21⊤
n a + (p1 −p2)ai −p1E [vx(DT ) | i ∈T]

."
REFERENCES,0.4766791044776119,"We use the shorthand notation ¯v ∈Rn+1 for a vector with entries ¯v0 = E[vx(DT )] and ¯vi =
E[vx(DT ) | i ∈T] for i ∈[n]. Combining this with the partial derivatives, we can derive an analytic
solution a∗∈Rn+1 by setting the derivative to zero,"
REFERENCES,0.47761194029850745,"
1
p11⊤
n
p11n
p21n1⊤
n + p3In"
REFERENCES,0.478544776119403,"
a∗−

1
0
0
p1In"
REFERENCES,0.47947761194029853,"
¯v = 0,"
REFERENCES,0.480410447761194,which yields the following equation for the solution:
REFERENCES,0.48134328358208955,"a∗=

1
p11⊤
n
p11n
p21n1⊤
n + p3In"
REFERENCES,0.4822761194029851,"−1 
1
0
0
p1In 
¯v."
REFERENCES,0.4832089552238806,"To find the required matrix inverse, we can combine the Sherman-Morrison formula with the formula
for block matrix inversion. The formula for block matrix inversion is the following [79]:"
REFERENCES,0.4841417910447761,"
A
B
C
D"
REFERENCES,0.48507462686567165,"−1
=

A−1 + A−1B(D −CA−1B)−1CA−1
−A−1B(D −CA−1B)−1"
REFERENCES,0.48600746268656714,"−(D −CA−1B)−1CA−1
(D −CA−1B)−1 
."
REFERENCES,0.4869402985074627,"We are mainly interested in the lower rows of this matrix because we do not use the learned intercept
term. For the lower right matrix, we have the following:"
REFERENCES,0.4878731343283582,"(D −CA−1B)−1 =
 
p21n1⊤
n + p3In −p2
11n1⊤
n
−1"
REFERENCES,0.48880597014925375,"=
 
(p2 −p2
1)1n1⊤
n + p3In
−1 = 1"
REFERENCES,0.48973880597014924,"p3
In −
p2 −p2
1
p3(p3 + n(p2 −p2
1))1n1⊤
n ."
REFERENCES,0.4906716417910448,"Next, for the lower left vector, we have the following:"
REFERENCES,0.4916044776119403,"−(D −CA−1B)−1CA−1 = −
 1"
REFERENCES,0.4925373134328358,"p3
In −
p2 −p2
1
p3(p3 + n(p2 −p2
1))1n1⊤
n"
REFERENCES,0.49347014925373134,"
p11n = −p1"
REFERENCES,0.4944029850746269,"p3
1n + np1 p3"
REFERENCES,0.49533582089552236,"p2 −p2
1
p3 + n(p2 −p2
1)1n"
REFERENCES,0.4962686567164179,"= −
p1
p3 + n(p2 −p2
1)1n."
REFERENCES,0.49720149253731344,"This yields the following solution for the optimal coefficients a∗
i with i ∈[n]:"
REFERENCES,0.498134328358209,"a∗
i = p1"
REFERENCES,0.49906716417910446,"p3
¯vi −
p1(p2 −p2
1)
p3(p3 + n(p2 −p2
1)) X"
REFERENCES,0.5,"j∈[n]
¯vj −
p1
p3 + n(p2 −p2
1) ¯v0."
REFERENCES,0.5009328358208955,"Based on this solution, we can design a function ci(T) so that we have the expectation
E[ci(T)vx(DT )] = a∗
i . We define the function as follows,"
REFERENCES,0.5018656716417911,ci(T) ≡1 p3
REFERENCES,0.5027985074626866,"
1(i ∈T) −
p2 −p2
1
p3 + n(p2 −p2
1)|T| −
p1p3
p3 + n(p2 −p2
1) 
,"
REFERENCES,0.503731343283582,and it can be verified that this satisfies the required expectation.
REFERENCES,0.5046641791044776,"A similar derivation to Lemma 1 is possible when we omit an intercept term in the datamodels
optimization problem, but we do not show the result here."
REFERENCES,0.5055970149253731,"Finally, we prove the special case of Lemma 1 considered in Proposition 1.
Proposition 1. Given a subset distribution that includes each data point zi ∈D with probability
q ∈(0, 1), the data attribution scores defined by"
REFERENCES,0.5065298507462687,"ζ(x) ≡arg min
a∈Rn+1 ET   "
REFERENCES,0.5074626865671642,"a0 +
X"
REFERENCES,0.5083955223880597,"i∈T
ai −vx(DT ) !2 "
REFERENCES,0.5093283582089553,can be expressed as the following expectation:
REFERENCES,0.5102611940298507,"ζ(zi, x) =
X"
REFERENCES,0.5111940298507462,"T ⊆[n]\i
u(|T|) (vx(DT ∪{zi}) −vx(DT )) ."
REFERENCES,0.5121268656716418,"Proof. Consider the weighting function ci(T) introduced in the proof for Lemma 1. In this case, we
can use the fact that p2 = p2
1 and write the weighting function as follows:"
REFERENCES,0.5130597014925373,ci(T) = 1
REFERENCES,0.5139925373134329,"p3
(1(i ∈T) −p1) ="
REFERENCES,0.5149253731343284,( 1−p1
REFERENCES,0.5158582089552238,"p3
i ∈T
−p1"
REFERENCES,0.5167910447761194,"p3
i /∈T."
REFERENCES,0.5177238805970149,"Using the fact that p(T) = q|T |(1 −q)n−|T | and the coefficient values (1 −p1)/p3 = q−1 and
−p1/p3 = −(1 −q)−1, we arrive at the result in Proposition 1."
REFERENCES,0.5186567164179104,"E
Noisy Oracles for XML Methods"
REFERENCES,0.519589552238806,"This section describes the statistical estimators used for each XML task, which are briefly introduced
in Section 4. Each estimator serves as a noisy oracle for the given task, which allows us to train
amortized models with inexpensive supervision."
REFERENCES,0.5205223880597015,"Shapley values. We use three statistical estimators for Shapley value feature attributions. The
simplest is permutation sampling, where we average each feature’s contribution across a set of
sampled orderings [6, 96]. For this approach, we use ρ to denote a permutation of the indices [d],
where we let ρ(i) ⊆[d] \ {i} denote the elements appearing before i in the permutation. The Shapley
value can be understood as the marginal contribution f(xρ(i)∪{i}) −f(xρ(i)) averaged across all
possible permutations [89], so our first estimator involves sampling k permutations ρ1, . . . , ρk, and
then calculating the following average for each feature:"
REFERENCES,0.5214552238805971,"ˆϕi(x) = 1 k k
X"
REFERENCES,0.5223880597014925,"j=1
f(xρj(i)∪{i}) −f(xρj(i)).
(19)"
REFERENCES,0.523320895522388,"Next, we consider two estimators based on the Shapley value’s least squares view. Recall that the
Shapley value is the solution to the following problem (where we discard the intercept term),"
REFERENCES,0.5242537313432836,"ϕ(x) = arg min
a∈Rd+1 X"
REFERENCES,0.5251865671641791,"S⊆[d]
µ(S) "
REFERENCES,0.5261194029850746,"f(xS) −a0 −
X"
REFERENCES,0.5270522388059702,"i∈S
ai !2"
REFERENCES,0.5279850746268657,",
(20)"
REFERENCES,0.5289179104477612,"where we use a weighting kernel defined as µ−1(S) =
  d
|S|

|S|(d −|S|) [7]. The first estimator
we use based on this perspective is KernelSHAP [68], which solves this problem using k subsets
Sj ⊆[d] sampled according to µ(S). The problem is convex but constrained due to the weighting
terms µ([d]) = µ(∅) = ∞, so it must be solved via the KKT conditions; we refer readers to
Covert and Lee [16] for the closed-form solution. We also consider the SGD-Shapley approach
from Simon and Vincent [92]: rather than solving Eq. (20) exactly given sampled subsets, this
approach solves the problem iteratively with projected stochastic gradient descent. Unlike the other
estimators, SGD-Shapley has been shown to have non-negligible bias [10]. We used a custom
implementation for SGD-Shapley, and we used an open-source implementation of permutation
sampling and KernelSHAP.9"
REFERENCES,0.5298507462686567,"Banzhaf values. When calculating Banzhaf value feature attributions, we adapt the maximum sample
re-use (MSR) estimator from Wang and Jia [103], which was originally used for data valuation but is
equally applicable to feature attribution. For a single example x ∈X, we generate predictions using
k subsets Sj ⊆[d] sampled uniformly at random. We then split the subsets into those that include or
exclude each feature i ∈[d], and we estimate the Banzhaf value as follows:"
REFERENCES,0.5307835820895522,"ˆϕi(x) =
1
|{j : i ∈Sj}| X"
REFERENCES,0.5317164179104478,"j:i∈Sj
f(xSj) −
1
|{j : i /∈Sj}| X"
REFERENCES,0.5326492537313433,"j:i/∈Sj
f(xSj).
(21)"
REFERENCES,0.5335820895522388,"The re-use of samples across all features makes this estimator more efficient than independent Monte
Carlo estimates [103], and re-using samples in this way is simpler for Banzhaf values than for other
methods like Shapley values [20, 58]. We used a custom implementation for this approach."
REFERENCES,0.5345149253731343,"LIME. Similar to KernelSHAP, the most popular estimator for LIME feature attributions is based
on approximately solving its weighted least squares problem. Following the problem formulation in
Section 4.2, which we simplify by omitting the regularization term Ω, we sample k subsets Sj ⊆[d]
uniformly at random and solve the following importance sampling version of the objective:"
REFERENCES,0.5354477611940298,"ˆϕ1(x), . . . , ˆϕd(x) = arg min
a∈Rd+1 k
X"
REFERENCES,0.5363805970149254,"j=1
π(Sj) "
REFERENCES,0.5373134328358209,"a0 +
X"
REFERENCES,0.5382462686567164,"i∈Sj
ai −f(xSj)   2"
REFERENCES,0.539179104477612,".
(22)"
REFERENCES,0.5401119402985075,9https://github.com/iancovert/shapley-regression
REFERENCES,0.5410447761194029,"When doing so, we discard the intercept term, we ensure that k is large enough to avoid singular
matrix inversion, and we use the default weighting kernel π(S) for images in the official LIME
implementation.10 We opt to sample subsets uniformly rather than according to π(S) because this
approach is used in the official implementation, and because the weighting kernel is similar to
sampling subsets uniformly at random [66]. We used a custom implementation of this approach."
REFERENCES,0.5419776119402985,"Data valuation. For the various data valuation methods discussed in Section 4.3, we use the simplest
unbiased approximation, which is a Monte Carlo estimator that averages the performance difference
across a set of sampled datasets. Given a labeled data point z, we sample k datasets Dj ⊆D from
the appropriate distribution and calculate the following empirical average:"
REFERENCES,0.542910447761194,"ˆψ(z) = 1 k k
X"
REFERENCES,0.5438432835820896,"j=1
v(Dj ∪{z}) −v(Dj).
(23)"
REFERENCES,0.5447761194029851,"This is similar to the TMC algorithm from Ghorbani and Zou [33], but we do not employ truncation;
our approach can be used with truncation, but the noisy labels and valuation model predictions would
both become biased. Similar to previous works, we also adopt a minimum subset cardinality to avoid
training models with an insufficient number of data points. We implemented this approach using
the OpenDataVal package [53]. More sophisticated estimators are available, like those that exploit
stratification [106], re-use samples across all data points [103], or assume sparse attribution scores
[52], but we leave exploration of these estimators to future work."
REFERENCES,0.5457089552238806,10https://github.com/marcotcr/lime
REFERENCES,0.5466417910447762,"F
Experiment Details"
REFERENCES,0.5475746268656716,"Model architectures. For the feature attribution experiments, we used the ViT-Base architecture [24]
for the classifier, and we used a modified version of the architecture for the amortized attribution
model: following the approach from Covert et al. [18], we added an extra self-attention layer and
three fully-connected layers that operate on each token, so that the output contains an attribution score
for each class. When estimating feature attributions, we make predictions with subsets of patches
by setting the held-out patches to zero, and the classifier was fine-tuned with random masking to
accommodate missing patches [17, 48, 50, 18]."
REFERENCES,0.5485074626865671,"For the data valuation experiments, we used a FCN with two hidden layers of size 128 for the tabular
datasets, and we used a ResNet-18 architecture [43] for CIFAR-10. Valuation scores are defined for
labeled examples z = (x, y), so the valuation model must account for both the features x and the
class y when making predictions; rather than passing y as a model input, our architecture makes
predictions simultaneously for all classes, and we only use the estimate for the relevant class. When
generating noisy labels using the TMC estimator [33], we trained a logistic regression model on
raw input features for the tabular datasets, and for CIFAR-10 we trained a logistic regression on
pre-trained ResNet-50 features whose dimensionality was reduced with PCA."
REFERENCES,0.5494402985074627,"Hyperparameters. For the feature attribution experiments, we optimized the models using the
AdamW optimizer [67] with a linearly decaying learning rate schedule. The maximum learning
rate was tuned using the validation loss, we trained for up to 100 epochs, and we selected the best
model based on the validation loss. Due to the small scale of the noisy labels for Banzhaf and LIME
feature attributions, we re-scaled the labels during training to improve the model’s stability (see
Appendix G.2), but this re-scaling was not necessary for Shapley values."
REFERENCES,0.5503731343283582,"For the data valuation experiments, we optimized the models using Adam [55] with a cosine learning
rate schedule. The maximum learning rate, the number of training epochs and the best model from
the training run were determined using the validation loss. Due to the small scale of the noisy labels,
we found it helpful to re-scale them during training to have standard deviation approximately equal
to 1. The cost of performing multiple training runs is negligible compared to generating the noisy
training labels, so it is not reflected in our plots comparing amortization to the Monte Carlo estimator
(e.g., Figure 4)."
REFERENCES,0.5513059701492538,"Pretraining. We found that training the amortized models was faster and more stable when we
initialized using pretrained architectures. For the feature attribution experiments, we initialized
the model using the existing ViT-Base classifier weights. For data valuation, we initialized from a
classifier trained using the entire dataset, where we used a FCN for the tabular datasets and a ResNet-
18 for CIFAR-10. When adapting these models to their respective amortization tasks, the feature
attribution models had freshly initialized output layers (one self-attention and three fully-connected),
and the data valuation models had identical output layers that we re-initialized to zero."
REFERENCES,0.5522388059701493,"Validation loss. For the feature attribution experiments, we calculated the validation loss using
independent estimates generated with same estimator and the same number of samples used for the
noisy training labels. For the data valuation experiments, we implemented the validation loss using
independent Monte Carlo estimates of the data valuation scores. Our independent estimates use only
10 samples for the tabular datasets, and just one sample for CIFAR-10. Amortization provides a
significant benefit over per-example calculations even after accounting for the cost of the validation
loss."
REFERENCES,0.5531716417910447,"Noisy oracle details. The estimators used for each task are described in detail in Appendix E. For the
feature attribution experiments, the permutation sampling and KernelSHAP estimators only require
the number of samples as hyperparameters, and we tested multiple values in our experiments (e.g.,
Figure 3). For SGD-Shapley, we tuned the algorithm in several ways to improve its performance:
we used a constant rather than decaying learning rate, we took a uniform average over iterates, we
calculated gradients using multiple subsets, and we used the paired sampling trick from Covert and
Lee [16] to reduce the gradient variance. We also tuned the learning rate to a value that did not cause
divergence for any examples (5e-4). As described in Appendix G.1, we observed that SGD-Shapley
often had the lowest squared error among the three Shapley value estimators (Figure 11), but that it
did not lead to successful amortization due to its non-negligible bias (Figure 10)."
REFERENCES,0.5541044776119403,"For the data valuation experiments, models were trained on each subsampled dataset by fitting a
logistic regression model, either on raw input features for the tabular datasets or on pre-trained
ResNet-50 features for CIFAR-10. For the Data Shapley experiments in Section 5.2 with tabular
datasets, we sampled datasets without replacement, we used a minimum cardinality of 5 points
and a maximum cardinality equal to the number of points n. For the Distributional Data Shapley
experiments in Section 5.3 with CIFAR-10, we sampled datasets with replacement, and we used a
minimum cardinality of 100 and maximum cardinality of 1000."
REFERENCES,0.5550373134328358,"Ground truth. In all our experiments, the ground truth is obtained by running an existing per-
instance estimator for a large number of samples, and we use enough samples to ensure that it has
approximately converged to the exact result. For the Shapley value feature attribution experiments,
we run KernelSHAP for 1M samples. For Banzhaf values, we run the MSR estimator for 1M samples.
For LIME, we run the least squares estimator for 1M samples. For the data valuation experiments,
we run the Monte Carlo estimator for 10K samples. These estimators are costly to run for a large
number of samples, so we do so for only a small portion of the dataset: we calculate the ground truth
for 100 examples for the feature attribution experiments. For the data valuation experiments, we use
250 examples for the tabular datasets and 500 for CIFAR-10."
REFERENCES,0.5559701492537313,"Metrics. The performance metrics used throughout our experiments are related to the estimation
accuracy, which we evaluate using our ground truth values. Other works have evaluated Shapley
value feature attributions against other methods [50, 18] or used data valuation scores in a range
of downstream tasks [53], but our focus is on efficient and accurate estimation. For the feature
attribution experiments, we used the squared error distance, which we calculate across all attributions;
we used Pearson and Spearman correlation, which are calculated individually for each flattened vector
of attribution scores and then averaged across data points; and we used sign agreement, which is
averaged across all attributions. For the data valuation experiments, we used squared error distance,
which we normalized so the mean ground truth valuation score has error equal to 1 (i.e., we report
the error divided by the variance in ground truth scores); and we used Pearson correlation, Spearman
correlation and sign agreement, which were calculated using the vectors of estimated and ground
truth valuation scores."
REFERENCES,0.5569029850746269,"Datasets. We used publicly available, open-source datasets for our experiments. For the feature
attribution experiments, we used the ImageNette dataset, a natural image dataset consisting of ten
ImageNet classes [46, 23]. We partitioned the 224 × 224 inputs into 196 patches of size 14 × 14, and
the dataset was split into a training set with 9469 examples, a validation set with 1962 examples, and
a test set with 1963 examples. The validation set was used to perform early stopping, and the test set
was only used to evaluate the model’s performance on external data."
REFERENCES,0.5578358208955224,"For the data valuation experiments, we used two tabular datasets from the UCI repository: the
MiniBooNE particle classification dataset [84] and the adult census income classification dataset
[25]. We obtained these using the OpenDataVal package [53], we used variable numbers of training
examples ranging from 250 to 10K, and we reserved 100 examples for validation in each case; these
examples are only used to evaluate the performance of models trained on subsampled datasets. We
also used OpenDataVal to add label noise to 20% of the training examples. For CIFAR-10, we
used 50K examples for training and 1K for validation. We tested multiple levels of label noise for
CIFAR-10 (Appendix G.3), and our main text results use 10% label noise."
REFERENCES,0.558768656716418,"Compute resources. For the feature attribution experiments, we used a single GeForce RTX 2080Ti
GPU to train the amortized models. Training an amortized model on ImageNette dataset for 50
epochs required roughly 4 hours. For the data valuation experiments, we used a single RTX A6000 to
train amortized models, and these each trained in under an hour."
REFERENCES,0.5597014925373134,"FLOPs profiling. When profiling compute for our feature attribution experiments (see Figure 3), we
first measured the following constants using the DeepSpeed FLOPs profiler, all using the ViT-Base
architecture:"
REFERENCES,0.5606343283582089,"• The classifier’s forward pass requires 17,563,067,904 FLOPs ≈17.6 GFLOPs per prediction."
REFERENCES,0.5615671641791045,"• The amortized model’s forward pass requires 21,335,153,664 FLOPs ≈21.3 GFLOPs per
prediction, due to the model having an extra transformer block for fine-tuning."
REFERENCES,0.5625,"• The amortized model has 104,730,000 ≈105M trainable parameters."
REFERENCES,0.5634328358208955,"Next, for the FLOPs comparison shown in Figure 3 we calculated total FLOPs for the two approaches
as follows. For KernelSHAP, the total FLOPs = 17.6 GFLOPs × (number of subset samples) ×
(number of training datapoints). For the amortized models, the total FLOPs are the sum of multiple
terms:"
REFERENCES,0.5643656716417911,"1. FLOPs for obtaining noisy labels = 17.6 GFLOPs × (number of subset samples) × (number of
training datapoints)
2. FLOPs for the forward pass during training = 21.3 GFLOPs × (number of epochs) × (number
of training datapoints)
3. FLOPs for the backward pass during training = 21.3 GFLOPs × 2 × (number of epochs) ×
(number of training datapoints), where 2 is a standard multiplier for calculating FLOPs during
the backward pass
4. FLOPs for updating parameters = 105 MFLOPs × (number of epochs) × (number of training
datapoints) / (batch size) × 19, where 19 is due to the optimizer state"
REFERENCES,0.5652985074626866,"In the calculation above, 1) dominates due to the large number of subset samples used for each
datapoint (e.g., >512 for KernelSHAP). Meanwhile, the training epochs in 2), 3) and 4) are relatively
low due to our models’ fast convergence (<25 epochs). In the FLOPs calculation above, training
an amortized model for one epoch is equivalent to obtaining predictions for roughly 3.65 additional
subset samples per training datapoint. This is because (3 × 21,335,153,664 + 104,730,000 ×
19/64)/17,563,067,904 ≈3.65. In other words, the amount of compute for training an amortized
model for up to 50 epochs translates to obtaining predictions for just 182.5 subset samples per
datapoint, which is not enough to meaningfully improve the estimates by running KernelSHAP for
more iterations. This explains why the lines for amortized models appear almost vertical in Figure 3
when we compared amortization to KernelSHAP. Our procedure for calculating FLOPs is similar to
other works that profile computation when training neural networks.11"
REFERENCES,0.566231343283582,"11We consulted the following resources: (i) https://www.lesswrong.com/posts/jJApGWG95495pYM7C/
how-to-measure-flop-s-for-neural-networks-empirically, (ii) https://www.lesswrong.com/
posts/fnjKpBoWJXcSDwhZk/what-s-the-backward-forward-flop-ratio-for-neural-networks,
and (iii) https://www.adamcasson.com/posts/transformer-flops"
REFERENCES,0.5671641791044776,"G
Additional Results"
REFERENCES,0.5680970149253731,"This section provides additional experimental results involving Shapley value feature attributions
(Appendix G.1), Banzhaf value and LIME feature attributions (Appendix G.2), and data valuation
(Appendix G.3)."
REFERENCES,0.5690298507462687,"G.1
Shapley value amortization"
REFERENCES,0.5699626865671642,"First, Figure 6 shows that the performance of our amortized models is comparable to running
KernelSHAP for 10-40k samples. For example, an amortized model trained on noisy KernelSHAP
labels generated with 512 samples produces outputs of similar quality to running KernelSHAP for
about 20k samples in terms of MSE, which is equivalent to a speedup of roughly 40x."
REFERENCES,0.5708955223880597,"0
50,000
100,000
# Samples / Point (KernelSHAP) 10
4 10
3 10
2 Error Error"
REFERENCES,0.5718283582089553,"KernelSHAP
512
1024
2048
3072"
REFERENCES,0.5727611940298507,"0
50,000
100,000
# Samples / Point (KernelSHAP) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5736940298507462,Pearson Correlation
REFERENCES,0.5746268656716418,Correlation
REFERENCES,0.5755597014925373,"0
50,000
100,000
# Samples / Point (KernelSHAP) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5764925373134329,Spearman Correlation
REFERENCES,0.5774253731343284,Rank correlation
REFERENCES,0.5783582089552238,"0
50,000
100,000
# Samples / Point (KernelSHAP) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5792910447761194,Sign Agreement
REFERENCES,0.5802238805970149,Sign Agreement
REFERENCES,0.5811567164179104,Figure 6: Comparison of the estimation accuracy between KernelSHAP and amortized predictions.
REFERENCES,0.582089552238806,"Next, Figure 7 shows an expanded version of Figure 3 (right) from the main text, where we compare
amortization to per-instance estimation given a fixed amount of compute per data point. We observe
that across four metrics, amortization provides a benefit over KernelSHAP even when training with a
small portion of the ImageNette dataset. 10
3"
REFERENCES,0.5830223880597015,"# Training Datapoints (Amortized) 10
2 Error Error"
REFERENCES,0.5839552238805971,"Amortized
KernelSHAP"
REFERENCES,0.5848880597014925,"0
1000
2000
3000
4000
5000
# Training Datapoints (Amortized) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.585820895522388,Pearson Correlation
REFERENCES,0.5867537313432836,Correlation
REFERENCES,0.5876865671641791,"0
1000
2000
3000
4000
5000
# Training Datapoints (Amortized) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5886194029850746,Spearman Correlation
REFERENCES,0.5895522388059702,Rank Correlation
REFERENCES,0.5904850746268657,"0
1000
2000
3000
4000
5000
# Training Datapoints (Amortized) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5914179104477612,Sign Agreement
REFERENCES,0.5923507462686567,Sign Agreement
REFERENCES,0.5932835820895522,"Figure 7: Estimation accuracy for amortization and KernelSHAP with different dataset sizes given
equivalent compute."
REFERENCES,0.5942164179104478,"Figure 8 shows a similar result, but the amortized model’s performance is evaluated with external data
points (i.e., points that are not seen during training). The benefits of amortization remain significant
for most dataset sizes, reflecting that the model generalizes beyond the training data and can be used
for real-time feature attribution with new examples. 10
3"
REFERENCES,0.5951492537313433,"# Training Datapoints (Amortized) 10
2 Error Error"
REFERENCES,0.5960820895522388,"Amortized
KernelSHAP"
REFERENCES,0.5970149253731343,"0
1000
2000
3000
4000
5000
# Training Datapoints (Amortized) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5979477611940298,Pearson Correlation
REFERENCES,0.5988805970149254,Correlation
REFERENCES,0.5998134328358209,"0
1000
2000
3000
4000
5000
# Training Datapoints (Amortized) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6007462686567164,Spearman Correlation
REFERENCES,0.601679104477612,Rank Correlation
REFERENCES,0.6026119402985075,"0
1000
2000
3000
4000
5000
# Training Datapoints (Amortized) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6035447761194029,Sign Agreement
REFERENCES,0.6044776119402985,Sign Agreement
REFERENCES,0.605410447761194,"Figure 8: Estimation accuracy for amortization and KernelSHAP with different dataset sizes given
equivalent compute (external data points)."
REFERENCES,0.6063432835820896,"Figure 9 shows an expanded version of Figure 3 (center) where we can see the benefits of amortization
across multiple numbers of KernelSHAP samples when we account for the number of FLOPs."
REFERENCES,0.6072761194029851,"10
16
10
17
10
18
10
19 FLOPs 10
4 10
3 10
2 Error Error"
REFERENCES,0.6082089552238806,"512
2048
KernelSHAP"
REFERENCES,0.6091417910447762,"1024
3072"
REFERENCES,0.6100746268656716,"10
16
10
17
10
18
10
19 FLOPs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6110074626865671,Pearson Correlation
REFERENCES,0.6119402985074627,Correlation
REFERENCES,0.6128731343283582,"10
16
10
17
10
18
10
19 FLOPs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6138059701492538,Spearman Correlation
REFERENCES,0.6147388059701493,Rank Correlation
REFERENCES,0.6156716417910447,"10
16
10
17
10
18
10
19 FLOPs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6166044776119403,Sign Agreement
REFERENCES,0.6175373134328358,Sign Agrement
REFERENCES,0.6184701492537313,Figure 9: Error of amortization and KernelSHAP as a function of FLOPs.
REFERENCES,0.6194029850746269,"Figure 10 shows an expanded version of Figure 3, where we can see the benefits of amortization for
KernelSHAP and permutation sampling across four metrics. We also see that SGD-Shapley does not
lead to successful amortization, because the predictions are worse than the noisy labels across all
four metrics. This is perhaps surprising, because Figure 11 shows that SGD-Shapley has the lowest
squared error among the three noisy oracles. The crucial issue with SGD-Shapley is that its estimates
are not unbiased (see Section 3), an issue that has been shown in prior work [10]."
REFERENCES,0.6203358208955224,"10
3
10
2
10
1
10
0"
REFERENCES,0.621268656716418,"Error (Prediction) 10
3 10
2 10
1 10
0"
REFERENCES,0.6222014925373134,Error (Label)
REFERENCES,0.6231343283582089,Error (KernelSHAP)
REFERENCES,0.6240671641791045,"512
1024
2048
3072"
REFERENCES,0.625,"10
3
10
2
10
1
10
0"
REFERENCES,0.6259328358208955,"Error (Prediction) 10
3 10
2 10
1 10
0"
REFERENCES,0.6268656716417911,Error (Label)
REFERENCES,0.6277985074626866,Error (Permutation Sampling)
REFERENCES,0.628731343283582,"196
392
588
1176
3136"
REFERENCES,0.6296641791044776,"10
3
10
2
10
1
10
0"
REFERENCES,0.6305970149253731,"Error (Prediction) 10
3 10
2 10
1 10
0"
REFERENCES,0.6315298507462687,Error (Label)
REFERENCES,0.6324626865671642,Error (SGD-Shapley) 9986
REFERENCES,0.6333955223880597,"0.0
0.2
0.4
0.6
0.8
1.0
Pearson Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6343283582089553,Pearson Corr. (Label)
REFERENCES,0.6352611940298507,Correlation (KernelSHAP)
REFERENCES,0.6361940298507462,"0.0
0.2
0.4
0.6
0.8
1.0
Pearson Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6371268656716418,Pearson Corr. (Label)
REFERENCES,0.6380597014925373,Correlation (Permutation Sampling)
REFERENCES,0.6389925373134329,"0.0
0.2
0.4
0.6
0.8
1.0
Pearson Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6399253731343284,Pearson Corr. (Label)
REFERENCES,0.6408582089552238,Correlation (SGD-Shapley)
REFERENCES,0.6417910447761194,"0.0
0.2
0.4
0.6
0.8
1.0
Spearman Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6427238805970149,Spearman Corr. (Label)
REFERENCES,0.6436567164179104,Rank Correlation (KernelSHAP)
REFERENCES,0.644589552238806,"0.0
0.2
0.4
0.6
0.8
1.0
Spearman Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6455223880597015,Spearman Corr. (Label)
REFERENCES,0.6464552238805971,Rank Correlation (Permutation Sampling)
REFERENCES,0.6473880597014925,"0.0
0.2
0.4
0.6
0.8
1.0
Spearman Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.648320895522388,Spearman Corr. (Label)
REFERENCES,0.6492537313432836,Rank Correlation (SGD-Shapley)
REFERENCES,0.6501865671641791,"0.0
0.2
0.4
0.6
0.8
1.0
Sign Agreement (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6511194029850746,Sign Agreement (Label)
REFERENCES,0.6520522388059702,Sign Agreement (KernelSHAP)
REFERENCES,0.6529850746268657,"0.0
0.2
0.4
0.6
0.8
1.0
Sign Agreement (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6539179104477612,Sign Agreement (Label)
REFERENCES,0.6548507462686567,Sign Agreement (Permutation Sampling)
REFERENCES,0.6557835820895522,"0.0
0.2
0.4
0.6
0.8
1.0
Sign Agreement (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6567164179104478,Sign Agreement (Label)
REFERENCES,0.6576492537313433,Sign Agreement (SGD-Shapley)
REFERENCES,0.6585820895522388,"Figure 10: Comparison of the estimation error between noisy labels and amortized predictions for
Shapley value feature attributions. Top: noisy labels generated using KernelSHAP with different
numbers of samples. Middle: noisy labels generated using permutation sampling with different
numbers of samples. Bottom: noisy labels generated using SGD-Shapley with different numbers of
samples."
REFERENCES,0.6595149253731343,"Figure 12 is similar to Figure 10, only the performance metrics are calculated using external points
not seen during training. Like Figure 12, this result emphasizes that the amortized attribution model
generalizes well and can be reliably used with new data points."
REFERENCES,0.6604477611940298,"0
500
1000
1500
2000
2500
3000
# Samples / Point 0.0 0.2 0.4 Error Error"
REFERENCES,0.6613805970149254,"KernelSHAP
Permutation
SGD-Shapley"
REFERENCES,0.6623134328358209,"0
500
1000
1500
2000
2500
3000
# Samples / Point 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6632462686567164,Pearson Correlation
REFERENCES,0.664179104477612,Correlation
REFERENCES,0.6651119402985075,"0
500
1000
1500
2000
2500
3000
# Samples / Point 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6660447761194029,Spearman Correlation
REFERENCES,0.6669776119402985,Rank correlation
REFERENCES,0.667910447761194,"0
500
1000
1500
2000
2500
3000
# Samples / Point 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6688432835820896,Sign Agreement
REFERENCES,0.6697761194029851,Sign Agreement
REFERENCES,0.6707089552238806,"Figure 11: Comparison of the estimation error between different per-example estimators for Shapley
value feature attributions across varying numbers of samples."
REFERENCES,0.6716417910447762,"10
3
10
2
10
1
10
0"
REFERENCES,0.6725746268656716,"Error (Prediction) 10
3 10
2 10
1 10
0"
REFERENCES,0.6735074626865671,Error (Label)
REFERENCES,0.6744402985074627,Error (KernelSHAP)
REFERENCES,0.6753731343283582,"512
1024
2048
3072"
REFERENCES,0.6763059701492538,"10
3
10
2
10
1
10
0"
REFERENCES,0.6772388059701493,"Error (Prediction) 10
3 10
2 10
1 10
0"
REFERENCES,0.6781716417910447,Error (Label)
REFERENCES,0.6791044776119403,Error (Permutation Sampling)
REFERENCES,0.6800373134328358,"196
392
588
1176
3136"
REFERENCES,0.6809701492537313,"10
3
10
2
10
1
10
0"
REFERENCES,0.6819029850746269,"Error (Prediction) 10
3 10
2 10
1 10
0"
REFERENCES,0.6828358208955224,Error (Label)
REFERENCES,0.683768656716418,Error (SGD-Shapley) 9986
REFERENCES,0.6847014925373134,"0.0
0.2
0.4
0.6
0.8
1.0
Pearson Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6856343283582089,Pearson Corr. (Label)
REFERENCES,0.6865671641791045,Correlation (KernelSHAP)
REFERENCES,0.6875,"0.0
0.2
0.4
0.6
0.8
1.0
Pearson Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6884328358208955,Pearson Corr. (Label)
REFERENCES,0.6893656716417911,Correlation (Permutation Sampling)
REFERENCES,0.6902985074626866,"0.0
0.2
0.4
0.6
0.8
1.0
Pearson Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.691231343283582,Pearson Corr. (Label)
REFERENCES,0.6921641791044776,Correlation (SGD-Shapley)
REFERENCES,0.6930970149253731,"0.0
0.2
0.4
0.6
0.8
1.0
Spearman Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6940298507462687,Spearman Corr. (Label)
REFERENCES,0.6949626865671642,Rank Correlation (KernelSHAP)
REFERENCES,0.6958955223880597,"0.0
0.2
0.4
0.6
0.8
1.0
Spearman Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6968283582089553,Spearman Corr. (Label)
REFERENCES,0.6977611940298507,Rank Correlation (Permutation Sampling)
REFERENCES,0.6986940298507462,"0.0
0.2
0.4
0.6
0.8
1.0
Spearman Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6996268656716418,Spearman Corr. (Label)
REFERENCES,0.7005597014925373,Rank Correlation (SGD-Shapley)
REFERENCES,0.7014925373134329,"0.0
0.2
0.4
0.6
0.8
1.0
Sign Agreement (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7024253731343284,Sign Agreement (Label)
REFERENCES,0.7033582089552238,Sign Agreement (KernelSHAP)
REFERENCES,0.7042910447761194,"0.0
0.2
0.4
0.6
0.8
1.0
Sign Agreement (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7052238805970149,Sign Agreement (Label)
REFERENCES,0.7061567164179104,Sign Agreement (Permutation Sampling)
REFERENCES,0.707089552238806,"0.0
0.2
0.4
0.6
0.8
1.0
Sign Agreement (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7080223880597015,Sign Agreement (Label)
REFERENCES,0.7089552238805971,Sign Agreement (SGD-Shapley)
REFERENCES,0.7098880597014925,"Figure 12: Comparison of the estimation error between noisy labels and amortized predictions
for Shapley value feature attributions (external data points). Top: noisy labels generated using
KernelSHAP with different numbers of samples. Middle: noisy labels generated using permutation
sampling with different numbers of samples. Bottom: noisy labels generated using SGD-Shapley
with different numbers of samples."
REFERENCES,0.710820895522388,"Finally, Figure 13 provides a comparison between stochastic amortization and FastSHAP [50, 18], one
of the main existing approaches to amortized Shapley value estimation. For stochastic amortization,
we use our previous results with permutation sampling as the noisy oracle and five different numbers
of samples. For FastSHAP, we train the same ViT-Base architecture following the approach from
Covert et al. [18], using 32 subset samples per gradient step. We monitor FastSHAP’s estimation
accuracy at the end of each epoch while training for a total of 100 epochs. We observe that FastSHAP
and stochastic amortization achieve similar error at each total FLOPs budget, where both methods
incur FLOPs from training the amortized model, stochastic amortization incurs FLOPs upfront when
generating noisy labels, and FastSHAP incurs FLOPs during training while sampling subsets for
each gradient step. FastSHAP is slightly more accurate at the largest computational budget, and both
are significantly more accurate than per-sample estimation with KernelSHAP. Due to their similar
performance, the main advantage of our approach is its simplicity (a standard regression objective
rather than the custom FastSHAP objective), and the flexibility to use any unbiased estimator as the
noisy oracle; future work may find that stochastic amortization is more effective with other noisy
oracles that we did not try here."
REFERENCES,0.7117537313432836,"10
16
10
17
10
18 FLOPs 10
4 10
3 10
2 10
1 Error Error"
REFERENCES,0.7126865671641791,"Amortized (196)
Amortized (392)
Amortized (588)
Amortized (1176)"
REFERENCES,0.7136194029850746,"Amortized (3136)
KernelSHAP
FastSHAP"
REFERENCES,0.7145522388059702,"Figure 13: Comparison between stochastic amortization, FastSHAP and KernelSHAP as a function
of total FLOPs."
REFERENCES,0.7154850746268657,"G.2
Banzhaf value and LIME amortization"
REFERENCES,0.7164179104477612,"As we discussed in Section 4.2, Banzhaf value and LIME feature attributions are two XML tasks
closely related to Shapley values that can be amortized in a similar fashion. Both are intractable to
calculate exactly, but they have (approximately) unbiased estimators that can be used as noisy labels
for stochastic amortization. Appendix E describes these estimators, namely the MSR estimator for
Banzhaf values [103] and the least squares estimator for LIME [83]. Prior work has also shown that
these approaches generate similar outputs [66], although the standard computational approaches are
quite different."
REFERENCES,0.7173507462686567,"In amortizing these methods, one trend we observed is that Banzhaf value and LIME feature attri-
butions have a very different scale from Shapley values. Figure 14 shows that they not only have
smaller norm, but that the distribution is concentrated at small magnitudes with a long tail of larger
magnitudes. This is troublesome, because our theory is focused on the squared error ∥a(b; θ)−a(b)∥2
(see Section 3), which is dominated by the small number of examples with large magnitudes. As a
result, an amortized attribution model can appear to train well by accurately estimating attributions
with a large norm, and predicting the remaining attributions to be roughly zero; in doing so it may
fail to provide the correct relative feature ordering for most examples, which is more important for
practical usage of feature attributions."
REFERENCES,0.7182835820895522,"0.10
0.15
0.20
0.25
0.30
0.35
L2 norm 0 5 10 15 20 25 Count"
REFERENCES,0.7192164179104478,Shapley values
REFERENCES,0.7201492537313433,"0.00
0.05
0.10
0.15
0.20
L2 norm 0 20 40 60 80 Count"
REFERENCES,0.7210820895522388,Banzhaf values
REFERENCES,0.7220149253731343,"0.00
0.05
0.10
0.15
0.20
L2 norm 0 20 40 60 80 Count LIME"
REFERENCES,0.7229477611940298,"Figure 14: Comparison of the distribution of norms between different feature attribution methods.
We plotted the L2 norm of the 100 ground truths for each feature attribution."
REFERENCES,0.7238805970149254,"The issue described above is precisely what occurs when we amortize Banzhaf value feature attribu-
tions, as shown in the top row of Figure 15. We observe that the squared error from our predictions
is lower than that of the noisy labels, which is consistent with our theory from Section 3 and our
results for Shapley values (Appendix G.1). However, the amortized estimates perform worse than the
noisy labels on the remaining metrics, particularly for the correlation scores that evaluate the relative
feature ordering in each example’s attributions (see details for the metrics in Appendix F)."
REFERENCES,0.7248134328358209,"To alleviate this issue, we experimented with a per-label normalization that eliminates the long tail of
large attribution norms that dominate training: we simply normalized each example’s attributions
for each class to have a norm of 1. The results are shown in the bottom row of Figure 15, where we
see that the predictions have higher correlation scores than the noisy labels. The squared error is
higher for the amortized predictions, and the sign agreement is roughly the same. One issue with this
heuristic is that the normalized noisy labels are biased: the normalized noisy label is not an unbiased
estimator of the normalized exact label, because the normalization constant is not known a priori and
must be calculated using the noisy label. However, the results show that amortization can to some
extent denoise inexact labels even with this non-zero bias."
REFERENCES,0.7257462686567164,"Figure 16 shows similar results as Figure 15 but with LIME feature attributions. We observe the
same improvement in squared error from amortization, and a similar degradation in the correlation
metrics (Figure 16 top). We then apply the same per-label normalization trick, and we observe a
similar modest improvement in the correlation metrics, at least for noisier settings of the least squares
estimator (Figure 16 bottom)."
REFERENCES,0.726679104477612,"Overall, these results show that for the purpose of amortization, the consistency in scale of Shapley
values is an unexpected advantage over Banzhaf values and LIME. This property is due in part to the
Shapley value’s efficiency axiom [89], which guarantees that the Shapley values sum to the difference
between the prediction with all features and no features [68]. In comparison, LIME and Banzhaf
values focus on marginal contributions involving roughly half of the features, which in many cases
are near zero when the prediction is saturated; for example, previous works have observed that for"
REFERENCES,0.7276119402985075,"10
5
10
3
10
1
10
1"
REFERENCES,0.7285447761194029,"Error (Prediction) 10
5 10
4 10
3 10
2 10
1 10
0 10
1"
REFERENCES,0.7294776119402985,Error (Label) Error
REFERENCES,0.730410447761194,"10
100
500"
REFERENCES,0.7313432835820896,"10
5
10
3
10
1
10
1"
REFERENCES,0.7322761194029851,"Error (Prediction) 10
5 10
4 10
3 10
2 10
1 10
0 10
1"
REFERENCES,0.7332089552238806,Error (Label)
REFERENCES,0.7341417910447762,Error (Per-Label Scaling)
REFERENCES,0.7350746268656716,"0.0
0.2
0.4
0.6
0.8
1.0
Pearson Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7360074626865671,Pearson Corr. (Label)
REFERENCES,0.7369402985074627,Correlation
REFERENCES,0.7378731343283582,"0.0
0.2
0.4
0.6
0.8
1.0
Pearson Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7388059701492538,Pearson Corr. (Label)
REFERENCES,0.7397388059701493,Correlation (Per-Label Scaling)
REFERENCES,0.7406716417910447,"0.0
0.2
0.4
0.6
0.8
1.0
Spearman Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7416044776119403,Spearman Corr. (Label)
REFERENCES,0.7425373134328358,Rank Correlation
REFERENCES,0.7434701492537313,"0.0
0.2
0.4
0.6
0.8
1.0
Spearman Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7444029850746269,Spearman Corr. (Label)
REFERENCES,0.7453358208955224,Rank Correlation (Per-Label Scaling)
REFERENCES,0.746268656716418,"0.0
0.2
0.4
0.6
0.8
1.0
Sign Agreement (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7472014925373134,Sign Agreement (Label)
REFERENCES,0.7481343283582089,Sign Agreement
REFERENCES,0.7490671641791045,"0.0
0.2
0.4
0.6
0.8
1.0
Sign Agreement (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.75,Sign Agreement (Label)
REFERENCES,0.7509328358208955,Sign Agreement (Per-Label Scaling)
REFERENCES,0.7518656716417911,"Figure 15: Comparison of the estimation error between noisy labels and amortized predictions for
Banzhaf value feature attributions. Noisy labels were generated using the MSR estimator with
different numbers of samples. Top: using raw estimates as noisy training labels. Bottom: normalizing
each noisy training label separately to have unit norm for each class."
REFERENCES,0.7527985074626866,"natural images containing relatively large objects, a large portion of the patches must be removed
before we observe significant changes in the prediction [48, 18]."
REFERENCES,0.753731343283582,"10
5
10
4
10
3
10
2
10
1
10
0"
REFERENCES,0.7546641791044776,"Error (Prediction) 10
5 10
4 10
3 10
2 10
1 10
0"
REFERENCES,0.7555970149253731,Error (Label) Error
REFERENCES,0.7565298507462687,"256
512"
REFERENCES,0.7574626865671642,"10
5
10
4
10
3
10
2
10
1
10
0"
REFERENCES,0.7583955223880597,"Error (Prediction) 10
5 10
4 10
3 10
2 10
1 10
0"
REFERENCES,0.7593283582089553,Error (Label)
REFERENCES,0.7602611940298507,Error (Per-Label Scaling)
REFERENCES,0.7611940298507462,"0.0
0.2
0.4
0.6
0.8
1.0
Pearson Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7621268656716418,Pearson Corr. (Label)
REFERENCES,0.7630597014925373,Correlation
REFERENCES,0.7639925373134329,"0.0
0.2
0.4
0.6
0.8
1.0
Pearson Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7649253731343284,Pearson Corr. (Label)
REFERENCES,0.7658582089552238,Correlation (Per-Label Scaling)
REFERENCES,0.7667910447761194,"0.0
0.2
0.4
0.6
0.8
1.0
Spearman Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7677238805970149,Spearman Corr. (Label)
REFERENCES,0.7686567164179104,Rank Correlation
REFERENCES,0.769589552238806,"0.0
0.2
0.4
0.6
0.8
1.0
Spearman Corr. (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7705223880597015,Spearman Corr. (Label)
REFERENCES,0.7714552238805971,Rank Correlation (Per-Label Scaling)
REFERENCES,0.7723880597014925,"0.0
0.2
0.4
0.6
0.8
1.0
Sign Agreement (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.773320895522388,Sign Agreement (Label)
REFERENCES,0.7742537313432836,Sign Agreement
REFERENCES,0.7751865671641791,"0.0
0.2
0.4
0.6
0.8
1.0
Sign Agreement (Prediction) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7761194029850746,Sign Agreement (Label)
REFERENCES,0.7770522388059702,Sign Agreement (Per-Label Scaling)
REFERENCES,0.7779850746268657,"Figure 16: Comparison of the estimation error between noisy labels and amortized predictions for
LIME feature attributions. Noisy labels were generated using LIME’s least squares estimator with
different numbers of samples. Top: using raw estimates as noisy training labels. Bottom: normalizing
each noisy training label separately to have unit norm for each class."
REFERENCES,0.7789179104477612,"G.3
Data valuation"
REFERENCES,0.7798507462686567,"For the data valuation experiments, our first additional result compares amortization to the Monte
Carlo estimator when using the MiniBooNE and adult datasets with different numbers of data points.
The results are shown in Figure 17 and Figure 18, where we use datasets ranging from 1K to 10K
points, and we see that the benefits of amortization increase with the size of the dataset."
REFERENCES,0.7807835820895522,"0
50
100
150
200
250
# Samples / Point 10
1 100 Error"
REFERENCES,0.7817164179104478,MiniBooNE 1000 Error
REFERENCES,0.7826492537313433,Monte Carlo
REFERENCES,0.7835820895522388,Amortized
REFERENCES,0.7845149253731343,"0
50
100
150
200
250
# Samples / Point 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.7854477611940298,Pearson Correlation
REFERENCES,0.7863805970149254,MiniBooNE 1000 Correlation
REFERENCES,0.7873134328358209,"0
50
100
150
200
250
# Samples / Point 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.7882462686567164,Spearman Correlation
REFERENCES,0.789179104477612,MiniBooNE 1000 Rank Correlation
REFERENCES,0.7901119402985075,"0
50
100
150
200
250
# Samples / Point 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.7910447761194029,Sign Agreement
REFERENCES,0.7919776119402985,MiniBooNE 1000 Sign Agreement
REFERENCES,0.792910447761194,"0
50
100
150
200
250
# Samples / Point 10
1 100 Error"
REFERENCES,0.7938432835820896,MiniBooNE 2500 Error
REFERENCES,0.7947761194029851,Monte Carlo
REFERENCES,0.7957089552238806,Amortized
REFERENCES,0.7966417910447762,"0
50
100
150
200
250
# Samples / Point 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.7975746268656716,Pearson Correlation
REFERENCES,0.7985074626865671,MiniBooNE 2500 Correlation
REFERENCES,0.7994402985074627,"0
50
100
150
200
250
# Samples / Point 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.8003731343283582,Spearman Correlation
REFERENCES,0.8013059701492538,MiniBooNE 2500 Rank Correlation
REFERENCES,0.8022388059701493,"0
50
100
150
200
250
# Samples / Point 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.8031716417910447,Sign Agreement
REFERENCES,0.8041044776119403,MiniBooNE 2500 Sign Agreement
REFERENCES,0.8050373134328358,"0
50
100
150
200
250
# Samples / Point 10
1 100 Error"
REFERENCES,0.8059701492537313,MiniBooNE 5000 Error
REFERENCES,0.8069029850746269,Monte Carlo
REFERENCES,0.8078358208955224,Amortized
REFERENCES,0.808768656716418,"0
50
100
150
200
250
# Samples / Point 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.8097014925373134,Pearson Correlation
REFERENCES,0.8106343283582089,MiniBooNE 5000 Correlation
REFERENCES,0.8115671641791045,"0
50
100
150
200
250
# Samples / Point 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.8125,Spearman Correlation
REFERENCES,0.8134328358208955,MiniBooNE 5000 Rank Correlation
REFERENCES,0.8143656716417911,"0
50
100
150
200
250
# Samples / Point 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.8152985074626866,Sign Agreement
REFERENCES,0.816231343283582,MiniBooNE 5000 Sign Agreement
REFERENCES,0.8171641791044776,"0
50
100
150
200
250
# Samples / Point 10
1 100 Error"
REFERENCES,0.8180970149253731,MiniBooNE 10000 Error
REFERENCES,0.8190298507462687,Monte Carlo
REFERENCES,0.8199626865671642,Amortized
REFERENCES,0.8208955223880597,"0
50
100
150
200
250
# Samples / Point 0.2 0.4 0.6 0.8"
REFERENCES,0.8218283582089553,Pearson Correlation
REFERENCES,0.8227611940298507,MiniBooNE 10000 Correlation
REFERENCES,0.8236940298507462,"0
50
100
150
200
250
# Samples / Point 0.2 0.4 0.6 0.8"
REFERENCES,0.8246268656716418,Spearman Correlation
REFERENCES,0.8255597014925373,MiniBooNE 10000 Rank Correlation
REFERENCES,0.8264925373134329,"0
50
100
150
200
250
# Samples / Point 0.2 0.4 0.6 0.8"
REFERENCES,0.8274253731343284,Sign Agreement
REFERENCES,0.8283582089552238,MiniBooNE 10000 Sign Agreement
REFERENCES,0.8292910447761194,"Figure 17: Amortized data valuation for the MiniBooNE dataset with different numbers of training
data points (1K to 10K). We show four metrics: squared error (normalized so that the mean valuation
scores has error equal to 1), Pearson correlation, Spearman correlation, and sign agreement."
REFERENCES,0.8302238805970149,"0
50
100
150
200
250
# Samples / Point 10
1 100 Error"
REFERENCES,0.8311567164179104,Adult 1000 Error
REFERENCES,0.832089552238806,Monte Carlo
REFERENCES,0.8330223880597015,Amortized
REFERENCES,0.8339552238805971,"0
50
100
150
200
250
# Samples / Point 0.6 0.7 0.8 0.9"
REFERENCES,0.8348880597014925,Pearson Correlation
REFERENCES,0.835820895522388,Adult 1000 Correlation
REFERENCES,0.8367537313432836,"0
50
100
150
200
250
# Samples / Point 0.6 0.7 0.8 0.9"
REFERENCES,0.8376865671641791,Spearman Correlation
REFERENCES,0.8386194029850746,Adult 1000 Rank Correlation
REFERENCES,0.8395522388059702,"0
50
100
150
200
250
# Samples / Point 0.6 0.7 0.8 0.9"
REFERENCES,0.8404850746268657,Sign Agreement
REFERENCES,0.8414179104477612,Adult 1000 Sign Agreement
REFERENCES,0.8423507462686567,"0
50
100
150
200
250
# Samples / Point 10
1 100 Error"
REFERENCES,0.8432835820895522,Adult 2500 Error
REFERENCES,0.8442164179104478,Monte Carlo
REFERENCES,0.8451492537313433,Amortized
REFERENCES,0.8460820895522388,"0
50
100
150
200
250
# Samples / Point 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.8470149253731343,Pearson Correlation
REFERENCES,0.8479477611940298,Adult 2500 Correlation
REFERENCES,0.8488805970149254,"0
50
100
150
200
250
# Samples / Point 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.8498134328358209,Spearman Correlation
REFERENCES,0.8507462686567164,Adult 2500 Rank Correlation
REFERENCES,0.851679104477612,"0
50
100
150
200
250
# Samples / Point 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.8526119402985075,Sign Agreement
REFERENCES,0.8535447761194029,Adult 2500 Sign Agreement
REFERENCES,0.8544776119402985,"0
50
100
150
200
250
# Samples / Point 10
1 100 Error"
REFERENCES,0.855410447761194,Adult 5000 Error
REFERENCES,0.8563432835820896,Monte Carlo
REFERENCES,0.8572761194029851,Amortized
REFERENCES,0.8582089552238806,"0
50
100
150
200
250
# Samples / Point 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.8591417910447762,Pearson Correlation
REFERENCES,0.8600746268656716,Adult 5000 Correlation
REFERENCES,0.8610074626865671,"0
50
100
150
200
250
# Samples / Point 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.8619402985074627,Spearman Correlation
REFERENCES,0.8628731343283582,Adult 5000 Rank Correlation
REFERENCES,0.8638059701492538,"0
50
100
150
200
250
# Samples / Point 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.8647388059701493,Sign Agreement
REFERENCES,0.8656716417910447,Adult 5000 Sign Agreement
REFERENCES,0.8666044776119403,"0
50
100
150
200
250
# Samples / Point 100 Error"
REFERENCES,0.8675373134328358,Adult 10000 Error
REFERENCES,0.8684701492537313,Monte Carlo
REFERENCES,0.8694029850746269,Amortized
REFERENCES,0.8703358208955224,"0
50
100
150
200
250
# Samples / Point 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.871268656716418,Pearson Correlation
REFERENCES,0.8722014925373134,Adult 10000 Correlation
REFERENCES,0.8731343283582089,"0
50
100
150
200
250
# Samples / Point 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.8740671641791045,Spearman Correlation
REFERENCES,0.875,Adult 10000 Rank Correlation
REFERENCES,0.8759328358208955,"0
50
100
150
200
250
# Samples / Point 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
REFERENCES,0.8768656716417911,Sign Agreement
REFERENCES,0.8777985074626866,Adult 10000 Sign Agreement
REFERENCES,0.878731343283582,"Figure 18: Amortized data valuation for the adult dataset with different numbers of training data
points (1K to 10K). We show four metrics: squared error (normalized so that the mean valuation
scores has error equal to 1), Pearson correlation, Spearman correlation, and sign agreement."
REFERENCES,0.8796641791044776,"5
10
15
20
25
# Samples / Point 0.5 1.0 1.5 2.0 Error"
REFERENCES,0.8805970149253731,CIFAR-10 Error
REFERENCES,0.8815298507462687,Monte Carlo
REFERENCES,0.8824626865671642,Amortized
REFERENCES,0.8833955223880597,"5
10
15
20
25
# Samples / Point 0.6 0.7 0.8 0.9"
REFERENCES,0.8843283582089553,Pearson Correlation
REFERENCES,0.8852611940298507,Monte Carlo
REFERENCES,0.8861940298507462,Amortized
REFERENCES,0.8871268656716418,CIFAR-10 Correlation
REFERENCES,0.8880597014925373,"5
10
15
20
25
# Samples / Point 0.4 0.5 0.6 0.7"
REFERENCES,0.8889925373134329,Spearman Correlation
REFERENCES,0.8899253731343284,CIFAR-10 Rank Correlation
REFERENCES,0.8908582089552238,"5
10
15
20
25
# Samples / Point 0.65 0.70 0.75 0.80 0.85 0.90"
REFERENCES,0.8917910447761194,Sign Agreement
REFERENCES,0.8927238805970149,CIFAR-10 Sign Agreement
REFERENCES,0.8936567164179104,"Figure 19: Distributional data valuation for CIFAR-10 with 50K data points. We generate Monte
Carlo estimates and amortized estimates with different numbers of samples per data point, and the
scores are compared to ground truth values using four metrics."
REFERENCES,0.894589552238806,"We also provide more detailed performance metrics for CIFAR-10: Figure 19 shows an expanded
version of Figure 5, and we observe that amortization improves upon the Monte Carlo estimator
across all four performance metrics."
REFERENCES,0.8955223880597015,"Next, we consider the use of CIFAR-10 data valuation scores in two downstream tasks. First, we
attempt to identify mislabeled examples, which we expect should have large negative valuation scores.
Figure 20 shows how quickly each method identifies negative examples when we sort the scores from
lowest to highest: our amortized estimates that train with just 5 samples provide the highest accuracy,
outperforming Monte Carlo estimates that use as many as 100 samples. This result is consistent
across different levels of label noise, where we randomly flip either 10%, 25% or 50% of the labels."
REFERENCES,0.8964552238805971,"Table 2 performs a similar analysis involving mislabeled examples: we expect these examples to
have lower scores than correctly labeled examples, so we use the estimated scores to calculate the
AUROC, the AUPR, and the portion of mislabeled examples with negative scores. Across the different
levels of label noise, our amortized estimates achieve the best performance according to all three
metrics. The strong performance of our amortized data valuation scores is largely due to the improved
estimation accuracy, but we expect that it is also due to the valuation network being trained on raw
images: the noisy labels are derived from training logistic regression models on pretrained ResNet-50
embeddings, which can lead to somewhat arbitrary valuation score differences between semantically
similar examples, and the valuation network may learn a more generalizable notion of data value."
REFERENCES,0.8973880597014925,"0
5000
10000
15000
20000
25000
# Examples Identified 0 1000 2000 3000 4000 5000"
REFERENCES,0.898320895522388,# Mislabeled Examples
REFERENCES,0.8992537313432836,CIFAR-10 (10% Noise)
REFERENCES,0.9001865671641791,Ground Truth
REFERENCES,0.9011194029850746,Amortized (5)
REFERENCES,0.9020522388059702,Monte Carlo (5)
REFERENCES,0.9029850746268657,Monte Carlo (10)
REFERENCES,0.9039179104477612,Monte Carlo (25)
REFERENCES,0.9048507462686567,Monte Carlo (50)
REFERENCES,0.9057835820895522,Monte Carlo (100)
REFERENCES,0.9067164179104478,"0
5000
10000
15000
20000
25000
# Examples Identified 0 2000 4000 6000 8000 10000 12000"
REFERENCES,0.9076492537313433,# Mislabeled Examples
REFERENCES,0.9085820895522388,CIFAR-10 (25% Noise)
REFERENCES,0.9095149253731343,"0
5000
10000
15000
20000
25000
# Examples Identified 0 5000 10000 15000 20000 25000"
REFERENCES,0.9104477611940298,# Mislabeled Examples
REFERENCES,0.9113805970149254,CIFAR-10 (50% Noise)
REFERENCES,0.9123134328358209,"Figure 20: CIFAR-10 mislabeled example identification with different amounts of label noise. We
compare the number of mislabeled examples among the lowest-scoring data points, where the
amortized model uses just 5 samples for training and the Monte Carlo estimator uses between 5 and
100 samples."
REFERENCES,0.9132462686567164,"Table 2: Mislabeled example identification accuracy for CIFAR-10 with different amounts of label
noise."
REFERENCES,0.914179104477612,"10% Noise
25% Noise
50% Noise"
REFERENCES,0.9151119402985075,"AUROC
AUPRC
Negative
AUROC
AUPRC
Negative
AUROC
AUPRC
Negative"
REFERENCES,0.9160447761194029,"Amortized (5)
0.981
0.917
0.974
0.985
0.964
0.973
0.972
0.973
0.915
Monte Carlo (5)
0.782
0.515
0.757
0.802
0.687
0.758
0.815
0.829
0.708
Monte Carlo (10)
0.844
0.619
0.826
0.867
0.784
0.821
0.883
0.892
0.769
Monte Carlo (25)
0.906
0.738
0.883
0.931
0.877
0.888
0.943
0.947
0.841
Monte Carlo (50)
0.934
0.794
0.909
0.956
0.918
0.918
0.965
0.967
0.878
Monte Carlo (100)
0.941
0.808
0.918
0.960
0.923
0.922
0.965
0.967
0.878"
REFERENCES,0.9169776119402985,"Finally, we experiment with using the data valuation scores to improve the dataset. Using the version
with 25% label noise, we experiment with several possible modifications to the dataset, where in
each case we train 5 models and report the mean cross entropy loss. First, we consider a perfect
filtering of the data where we remove all mislabeled examples. Next, we remove different numbers
of examples chosen uniformly at random. We also consider removing the examples with the lowest
scores according to the Monte Carlo estimates with 10 samples. Finally, we test two possible
approaches based on the amortized valuation model: (i) we filter out the lowest scoring examples
(“Amortized filtering”), similar to how we use the noisy Monte Carlo estimates; (ii) we attempt to
correct the lowest scoring examples using the class that the valuation model predicts to be most
valuable (“Amortized cleaning”), which is a unique capability enabled by our valuation model that
predicts valuation scores simultaneously for all possible classes (see Appendix F). The results of
this experiment are shown in Figure 21. We see that removing samples at random hurts the model’s
performance relative to using all the data, that filtering with the amortized estimates outperforms
filtering with the Monte Carlo estimates, and that cleaning the estimates is the best approach by a
narrow margin."
REFERENCES,0.917910447761194,"38000
40000
42000
44000
46000
48000
50000
# Datapoints 0.35 0.40 0.45 0.50 0.55 0.60"
REFERENCES,0.9188432835820896,Log Loss
REFERENCES,0.9197761194029851,CIFAR-10 Performance (25% Noise)
REFERENCES,0.9207089552238806,Noisy labels
REFERENCES,0.9216417910447762,Exact filtering
REFERENCES,0.9225746268656716,Random
REFERENCES,0.9235074626865671,Monte Carlo (10)
REFERENCES,0.9244402985074627,Amortized filtering
REFERENCES,0.9253731343283582,Amortized cleaning
REFERENCES,0.9263059701492538,"Figure 21: CIFAR-10 performance when training data is removed or adjusted based on estimated
valuation scores."
REFERENCES,0.9272388059701493,"H
Broader Impact"
REFERENCES,0.9281716417910447,"Our proposed method aims to make many computationally challenging XML tasks feasible and
scalable to large datasets. We expect our research to contribute to a better understanding of ML models,
enhancing their transparency and explainability. Additionally, our research could help understand
model fairness properties if our method is used to identify undesirable or unfair dependencies of ML
models on input features related to protected attributes like race or gender. The potential downside is
that that if users overly trust our method and rely solely on it for these tasks without careful usage, it
could lead to misunderstanding of models and result in harmful outcomes."
REFERENCES,0.9291044776119403,NeurIPS Paper Checklist
CLAIMS,0.9300373134328358,1. Claims
CLAIMS,0.9309701492537313,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our claim made in the abstract and introduction is that training amortized
models with unbiased noisy oracles is effective and provides significant speed-ups for
various XML tasks. This is supported by theoretical and experimental results in Section 3,
Section 4, and Section 5.
Guidelines:"
CLAIMS,0.9319029850746269,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.9328358208955224,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations of our work are discussed in Section 6.
Guidelines:"
CLAIMS,0.933768656716418,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.9347014925373134,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
CLAIMS,0.9356343283582089,Answer: [Yes]
CLAIMS,0.9365671641791045,Justification: The proofs for all theoretical results are provided in Appendix D.
CLAIMS,0.9375,Guidelines:
CLAIMS,0.9384328358208955,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9393656716417911,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9402985074626866,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.941231343283582,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9421641791044776,"Justification: The details for reproducing the experimental results are described in Section 5
and Appendix F."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9430970149253731,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9440298507462687,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9449626865671642,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9458955223880597,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9468283582089553,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9477611940298507,"Justification: Links to our code are provided in the paper. Our experiments use only
open-source software and datasets, all of which are referenced in the text."
OPEN ACCESS TO DATA AND CODE,0.9486940298507462,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9496268656716418,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9505597014925373,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9514925373134329,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9524253731343284,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9533582089552238,Justification: Details of the experimental settings are discussed in Section 5 and Appendix F.
OPEN ACCESS TO DATA AND CODE,0.9542910447761194,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9552238805970149,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9561567164179104,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.957089552238806,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9580223880597015,Answer: [No]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9589552238805971,"Justification: We performed experiments across multiple XML tasks, noisy oracles, noise
levels, model architectures, and datasets, so performing multiple runs for each experiment
would be too computationally expensive."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9598880597014925,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.960820895522388,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9617537313432836,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9626865671641791,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Information on compute resources is provided in Appendix F.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9636194029850746,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9645522388059702,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: Our proposed method aims to make computationally challenging XML tasks
feasible, so our research promotes transparency and interpretability in ML.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9654850746268657,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9664179104477612,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9673507462686567,"Justification: The potential positive and negative societal impacts of our work are discussed
in Appendix H.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9682835820895522,• The answer NA means that there is no societal impact of the work performed.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9692164179104478,"• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9701492537313433,11. Safeguards
SAFEGUARDS,0.9710820895522388,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9720149253731343,Answer: [NA]
SAFEGUARDS,0.9729477611940298,Justification: Our paper does not release a new dataset or model.
SAFEGUARDS,0.9738805970149254,Guidelines:
SAFEGUARDS,0.9748134328358209,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9757462686567164,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.976679104477612,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9776119402985075,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9785447761194029,Justification: The open-source packages and datasets we used are cited in Appendix F.
LICENSES FOR EXISTING ASSETS,0.9794776119402985,Guidelines:
LICENSES FOR EXISTING ASSETS,0.980410447761194,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided."
LICENSES FOR EXISTING ASSETS,0.9813432835820896,"• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9822761194029851,13. New Assets
NEW ASSETS,0.9832089552238806,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9841417910447762,Answer: [Yes]
NEW ASSETS,0.9850746268656716,"Justification: The code we provide contains usage instructions and scripts for running
experiments."
NEW ASSETS,0.9860074626865671,Guidelines:
NEW ASSETS,0.9869402985074627,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9878731343283582,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888059701492538,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897388059701493,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9906716417910447,"Justification: Our paper does not involve crowdsourcing experiments or research with human
subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916044776119403,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925373134328358,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934701492537313,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9944029850746269,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9953358208955224,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996268656716418,Justification: Our paper does not involve human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972014925373134,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9981343283582089,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990671641791045,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
