Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0014992503748125937,"In this paper, we present Language Model as Visual Explainer (LVX), a systematic
approach for interpreting the internal workings of vision models using a tree-
structured linguistic explanation, without the need for model training. Central
to our strategy is the collaboration between vision models and LLM to craft
explanations. On one hand, the LLM is harnessed to delineate hierarchical visual
attributes, while concurrently, a text-to-image API retrieves images that are most
aligned with these textual concepts. By mapping the collected texts and images to
the vision model‚Äôs embedding space, we construct a hierarchy-structured visual
embedding tree. This tree is dynamically pruned and grown by querying the LLM
using language templates, tailoring the explanation to the model. Such a scheme
allows us to seamlessly incorporate new attributes while eliminating undesired
concepts based on the model‚Äôs representations. When applied to testing samples,
our method provides human-understandable explanations in the form of attribute-
laden trees. Beyond explanation, we retrained the vision model by calibrating it
on the generated concept hierarchy, allowing the model to incorporate the refined
knowledge of visual attributes. To access the effectiveness of our approach, we
introduce new benchmarks and conduct rigorous evaluations, demonstrating its
plausibility, faithfulness, and stability."
INTRODUCTION,0.0029985007496251873,"1
Introduction"
INTRODUCTION,0.004497751124437781,"Unlocking the secrets of deep neural networks is akin to navigating through an intricate, ever-shifting
maze, as the intricate decision flow within the networks is, in many cases, extremely difficult for
humans to fully interpret. In this quest, extracting clear, understandable explanations from these
perplexing mazes has become an imperative task."
INTRODUCTION,0.005997001499250375,"While efforts have been made to explain computer vision models, these approaches often fall
short of providing direct and human-understandable explanations. Standard techniques, such as
attribution methods [46, 55, 90, 68, 1, 62, 65, 64], mechanical interpretability [22] and prototype
analysis [10, 49], only highlight certain pixels or features that are deemed important by the model. As
such, these methods often require the involvement of experts to verify or interpret the outputs for non-
technical users. Natural language explanations [27, 7, 41, 35], on the other hand, present an attractive
alternative, since the produced texts are better aligned with human understanding. Nevertheless, these
approaches typically rely on labor-intensive and biased manual annotation of textual rationales for
model training."
INTRODUCTION,0.0074962518740629685,"In this study, we attempt to explain AI decision in a human-understandable manner, for example,
tree-structured language. We call this task visual explanatory tree parsing. To implement this, we
present a systematic approach, Language Model as Visual Explainer (LVX), for interpreting vision
models structured natural language, without model training."
INTRODUCTION,0.008995502248875561,‚àóCorresponding author
INTRODUCTION,0.010494752623688156,ChatGPT
INTRODUCTION,0.01199400299850075,"Train
Samples"
INTRODUCTION,0.013493253373313344,Concept Prompt
INTRODUCTION,0.014992503748125937,Query Images
INTRODUCTION,0.01649175412293853,Construct Tree
INTRODUCTION,0.017991004497751123,"Query Tree
Refine Prompt"
INTRODUCTION,0.019490254872563718,Query Images
INTRODUCTION,0.020989505247376312,"Refine Tree
Tree Refinement
Construction"
INTRODUCTION,0.022488755622188907,"Test
Samples
Query Tree"
INTRODUCTION,0.0239880059970015,Testing
INTRODUCTION,0.025487256371814093,Explanation User
INTRODUCTION,0.026986506746626688,"Vision
Model"
INTRODUCTION,0.02848575712143928,"Text-to-
Image API
Parse"
INTRODUCTION,0.029985007496251874,"Tree
I can recognize dog. What"
INTRODUCTION,0.031484257871064465,attribute a dog has?
INTRODUCTION,0.03298350824587706,Dog has tails and
INTRODUCTION,0.034482758620689655,four legs.
INTRODUCTION,0.035982008995502246,"Wow, I can recognize legs,"
INTRODUCTION,0.037481259370314844,not tails.
INTRODUCTION,0.038980509745127435,A dog's legs are muscular
INTRODUCTION,0.04047976011994003,or slim.
INTRODUCTION,0.041979010494752625,"I see mostly muscular legs,"
INTRODUCTION,0.043478260869565216,not slim legs.
INTRODUCTION,0.044977511244377814,"Vision Model
ChatGPT
Vision
Model Parse Tree"
INTRODUCTION,0.046476761619190406,√óùíïiters
INTRODUCTION,0.047976011994003,"Figure 1: General workflow of LVX. (Left) A toy example that LLM interacts with vision model
to examine its capability. (Mid) It combines vision, language, and visual-language APIs to create a
parse tree for each visual model. (Right) In testing, embeddings navigate this tree, and the traversed
path provides a personalized explanation for the model‚Äôs prediction."
INTRODUCTION,0.049475262368815595,"A key challenge is that vision models, trained solely on pixel data, inherently lack comprehension
of textual concepts within an image. For example, if a model labels an image as a ‚Äúdog‚Äù, it is
unclear whether it truly recognizes the features like the wet nose or floppy ear, or if it is merely
making ungrounded guesses. To address this challenge, we link the vision model with a powerful
external knowledge provider, to establish connections between textual attributes and image patterns.
Specifically, we leverage large language models (LLM) such as ChatGPT and GPT4 as our knowledge
providers, combining them with the visual recognition system. Figure 1 (Left) describes a toy case,
where the LLM is interacts with the vision model to explore its capability boundaries. By doing so,
we gain insights into the what visual attributes can be recognized by the model."
INTRODUCTION,0.050974512743628186,"The pipeline of our approach is illustrated in Figure 1, which comprises two main stages, the
construction phase and the test phase."
INTRODUCTION,0.05247376311844078,"In the construction phase, our goal is to create an attribute tree for each category, partitioning the
feature space of a visual model via LLM-defined hierarchy. We begin by extracting commonsense
knowledge about each category and its visual attributes from LLMs using in-context prompting [45].
This information is naturally organized as a tree for better organization and clarity. Utilizing a text-
to-image API, we gather corresponding images for each tree node. These images are subsequently
inputted into the vision model to extract prototype embeddings, which are then mapped to the tree."
INTRODUCTION,0.053973013493253376,"Once created, the tree is dynamically adjusted, based on the properties of the training set. Specifically,
each embedding of the training sample is extracted by the vision model.Such embedding then
navigates the parse tree based on their proximity to prototype embeddings. Infrequently visited nodes,
representing attributes less recognizable, are pruned. Conversely, nodes often visited by the model
indicate successful concept recognition. Those nodes are growned, as the LLM introduces more
detailed concepts. Consequently, LVX yields human-understandable attribute trees that mirror the
model‚Äôs understanding of each concept."
INTRODUCTION,0.05547226386806597,"In the test phase, we input a test sample into the model to extract its feature. The feature is then routed
in the parse tree, by finding its nearest neighbors. The root-to-leaf path serves as a sample-specific
rationale for the model, offering an explanation of how the model arrived at its decision."
INTRODUCTION,0.05697151424287856,"To assess our method, we compiled new annotations and developed novel metrics. Subsequently, we
test LVX on these self-collected real-world datasets to access its effectiveness."
INTRODUCTION,0.05847076461769116,"Beyond interpretation, our study proposes to calibrate the vision model by utilizing the generated
explanation results. The utilization of tree-structured explanations plays a key role in enhancing the
model‚Äôs performance, thereby facilitating more reliable and informed decision-making processes.
Experimental results confirm the effectiveness of our method over existing interpretability techniques,
highlighting its potential for advancing explainable AI."
INTRODUCTION,0.05997001499250375,"To summarize, our main contributions are:"
INTRODUCTION,0.06146926536731634,"‚Ä¢ The paper introduces a novel task, visual explanatory tree parsing, that interprets vision
models using tree-structured language explanations."
INTRODUCTION,0.06296851574212893,"‚Ä¢ We introduce the Language Model as Visual Explainer (LVX) to carry out this task, without
model training. The proposed LVX is the first dedicated approach to leverage LLM to explain
the visual recognition system."
INTRODUCTION,0.06446776611694154,"‚Ä¢ Our study proposes utilizing the generated explanations to calibrate the vision model, leading
to enhanced performance and improved reliability for decision-making."
INTRODUCTION,0.06596701649175413,"‚Ä¢ We introduce new benchmarks and metrics for a concise evaluation of the LVX method.
These tools assess its plausibility, faithfulness, and stability in real-world datasets."
PROBLEM DEFINITION,0.06746626686656672,"2
Problem Definition"
PROBLEM DEFINITION,0.06896551724137931,"We first define our specialized task, called visual explanatory tree parsing, which seeks to unravel
the decision-making process of a vision model through a tree."
PROBLEM DEFINITION,0.0704647676161919,Visual Tree Parsing
PROBLEM DEFINITION,0.07196401799100449,"Classifier
‚ÄúDog‚Äù Dog"
PROBLEM DEFINITION,0.0734632683658171,Vehicles
PROBLEM DEFINITION,0.07496251874062969,Mammal
PROBLEM DEFINITION,0.07646176911544228,two ears
PROBLEM DEFINITION,0.07796101949025487,floppy ears that
PROBLEM DEFINITION,0.07946026986506746,hang down.
PROBLEM DEFINITION,0.08095952023988005,"Pointed ears
A tail Snout long short"
PROBLEM DEFINITION,0.08245877061469266,"wmm
Figure 2: The illustration of visual explanatory
tree parsing. Each input sample is interpreted as a
parse tree to represent the model‚Äôs logical process."
PROBLEM DEFINITION,0.08395802098950525,"Let us consider the trained vision model f, de-
fined as a function f : X ‚ÜíY, where X rep-
resents the input image space and Y denotes
the output label space. In this study, our focus
lies on the classification task, where f = g ‚ó¶h
is decomposed into a feature extractor g and a
linear classification head h. The output space
is Y ‚ààRn, where n signifies the number of
classes. The model is trained on a labeled train-
ing set Dtr = {xj, yj}M
j=1, and would be eval-
uated a test set Dts = {xj}L
j=1."
PROBLEM DEFINITION,0.08545727136431784,"The ultimate objective of our problem is to generate an explanation T for each model-input pair
(f, x) on the test set, illuminating the reasoning behind the model‚Äôs prediction ÀÜy = f(x). This unique
explanation manifests as a tree of attributes, denoted as T = (V, E), comprising a set of Nv nodes
V = {vi}Nv
i=1 and Ne edges E = {ei}Ne
i=1. The root of the tree is the predicted category, ÀÜy, while each
node vi encapsulates a specific attribute description of the object. These attributes are meticulously
organized, progressing from the holistic to the granular, and from the general to the specific. Figure 2
provides an example of the parse tree."
PROBLEM DEFINITION,0.08695652173913043,"Unlike existing approaches [53, 3] that explaining visual-language models [48, 47, 51, 86, 93], we
address the more challenging scenario, on explaining vision models trained solely on pixel data.
While some models can dissect and explain hierarchical clustering of feature embeddings [67, 77],
they lack the ability to associate each node with a textual attribute. It is important to note that our
explanations primarily focus on examining the properties of the established network, going beyond
training vision model or visual-language model [2, 44] for reasoning hierarchy [20] and attributes [31]
from the image. In other words, visual-language model, that tells the content in the image, can not
explain the inner working inside another model. Notably, our approach achieves this objective without
supervision and in open-vocabulary manner, without predefined explanations for model training."
LANGUAGE MODEL AS VISUAL EXPLAINER,0.08845577211394302,"3
Language Model as Visual Explainer"
LANGUAGE MODEL AS VISUAL EXPLAINER,0.08995502248875563,"This section dives deeper into the details of LVX. At the heart of our approach is the interaction
between the LLM and the vision model to construct the parsing tree. Subsequently, we establish a
rule to route through these trees, enabling the creation of coherent text explanations."
TREE CONSTRUCTION VIA LLM,0.09145427286356822,"3.1
Tree Construction via LLM"
TREE CONSTRUCTION VIA LLM,0.09295352323838081,"Before constructing our trees, let‚Äôs take a moment to reflect how humans do this task. Typically,
we already hold a hierarchy of concepts in our minds. When presented with visual stimuli, we
instinctively compare the data to our existing knowledge tree, confirming the presence of distinct
traits. We recognize familiar traits and, for unfamiliar ones, we expand our knowledge base. For
example, when we think of a dog, we typically know that it has a furry tail. Upon observing a dog,
we naturally check for the visibility of its tail. If we encounter a hairless tail, previously unknown"
TREE CONSTRUCTION VIA LLM,0.0944527736131934,In Context Example
TREE CONSTRUCTION VIA LLM,0.095952023988006,"Q: this is a deer because
A: 
{"
TREE CONSTRUCTION VIA LLM,0.09745127436281859,"""Concepts"": ‚Ä¶,
""Substance"": [""four legs"", ""two ears"","
TREE CONSTRUCTION VIA LLM,0.09895052473763119,"""two eyes""],
""Attributes"": {"
TREE CONSTRUCTION VIA LLM,0.10044977511244378,"""Head"": {"
TREE CONSTRUCTION VIA LLM,0.10194902548725637,"""Ears"": [""pointy"", ""sensitive""],
""Eyes"": [""large"", ""brown""]
},
""Body"": {"
TREE CONSTRUCTION VIA LLM,0.10344827586206896,"""Tail"": [""short"", ""furry""],
""Legs"": [""long"", ""slender"", 
""hooves""]"
TREE CONSTRUCTION VIA LLM,0.10494752623688156,"}
},
‚ÄúEnvironment‚Äù: ‚Ä¶
}"
TREE CONSTRUCTION VIA LLM,0.10644677661169415,"Q: this is a seal because
A: {"
TREE CONSTRUCTION VIA LLM,0.10794602698650675,"""Concepts"": ‚Ä¶,
""Substance"": [""flippers"", ""fur"", 
""teeth""],"
TREE CONSTRUCTION VIA LLM,0.10944527736131934,"""Attributes"": {"
TREE CONSTRUCTION VIA LLM,0.11094452773613193,"""Head"": {"
TREE CONSTRUCTION VIA LLM,0.11244377811094453,"""shape"": ""round"",
""Nose"": [""small"", ""black‚Äù],
""Eyes"": [""large"", ""dark""]
},
""Body"": {"
TREE CONSTRUCTION VIA LLM,0.11394302848575712,"""Tail"": [""short"", ""rounded""]
}
},
""Environment"": [""coastal areas""]
}"
TREE CONSTRUCTION VIA LLM,0.11544227886056972,"Instruction: Generate the all sentences that 
describe a concept according to the each
attribute"
TREE CONSTRUCTION VIA LLM,0.11694152923538231,<---- In Context Example ---->
TREE CONSTRUCTION VIA LLM,0.1184407796101949,"Category
Attribute Tree"
TREE CONSTRUCTION VIA LLM,0.1199400299850075,Prompt Tree {
TREE CONSTRUCTION VIA LLM,0.12143928035982009,"‚ÄúConcepts‚Äù: ‚Ä¶,
""Substance"": [""A seal has flippers."", ""A seal has fur."", ""A seal has 
teeth.""],"
TREE CONSTRUCTION VIA LLM,0.12293853073463268,"""Attributes"": {"
TREE CONSTRUCTION VIA LLM,0.12443778110944528,"""Head"": {"
TREE CONSTRUCTION VIA LLM,0.12593703148425786,"""shape"": ""A seal's head is round in shape."",
""Nose"": [""A seal's nose is small."", ""A seal's nose is black‚Äù],
""Eyes"": [""A seal's eyes are large."", ""A seal's eyes are dark.""]
},
""Body"": {"
TREE CONSTRUCTION VIA LLM,0.12743628185907047,"""Tail"": [""A seal's tail is short."", ""A seal's tail is 
rounded.""]"
TREE CONSTRUCTION VIA LLM,0.12893553223388307,"}
},
""Environment"": [""A seal lives in coastal areas.""]
}"
TREE CONSTRUCTION VIA LLM,0.13043478260869565,"A seal's eyes are dark.
A seal's tail is short.
A seal lives in coastal areas. CLIP"
TREE CONSTRUCTION VIA LLM,0.13193403298350825,"Stable
Diffusion"
TREE CONSTRUCTION VIA LLM,0.13343328335832083,"Search
Engine"
TREE CONSTRUCTION VIA LLM,0.13493253373313344,"ChatGPT
ChatGPT"
TREE CONSTRUCTION VIA LLM,0.136431784107946,A seal‚Äòs nose is small.
TREE CONSTRUCTION VIA LLM,0.13793103448275862,"Figure 3: Crafting text-image pairs for visual concepts. Through in-context prompting, we extract
knowledge from the LLM, yielding visual attributes for each category. These attributes guide the
collection of text-image pairs that encapsulate the essence of each visual concept."
TREE CONSTRUCTION VIA LLM,0.13943028485757122,"to us, we incorporate it into our knowledge base, ready to apply it to other dogs. This process is
typically termed Predictive Coding Theory [15] in cognitive science."
TREE CONSTRUCTION VIA LLM,0.1409295352323838,"Our LVX mirrors this methodology. We employ LLM as a ‚Äúknowledge provider‚Äù to construct the
initial conceptual tree. Subsequently, we navigate through the visual model‚Äôs feature space to assess
the prevalence of each node. If a specific attribute is rarely observed, we remove the corresponding
nodes from the tree. Conversely, if the model consistently recognizes an attribute, we enrich the tree
by integrating more nuanced, next-level concepts. This iterative process ensures the refinement and
adaptation of the conceptual tree within our pipeline, which gives rise to our LVX."
TREE CONSTRUCTION VIA LLM,0.1424287856071964,"Generating Textual Descriptions for Visual Concepts. We leverage a large language model
(LLM) as our ‚Äúcommonsense knowledge provider‚Äù [42, 95] to generate textual descriptions of visual
attributes corresponding to each category. The LLM acts as an external database, providing a rich
source of diverse visual concept descriptions. The process is illustrated in Figure 3."
TREE CONSTRUCTION VIA LLM,0.14392803598200898,"Formally, assume we have a set of category names, denoted as C = {ci}n
i=1, where i represents
the class index. For each of these classes, we prompt an LLM L to produce visual attribute tree.
We represent these attributes as di = L(ci, P), where di is a nested JSON text containing textual
descriptions associated with class ci. To help generate di, we use example input-output pairs, P, as
in-context prompts. The process unfolds in two stages:"
TREE CONSTRUCTION VIA LLM,0.1454272863568216,"‚Ä¢ Initial Attribute Generation: We initially generate keywords that embody the attributes of each
class. This prompt follows a predefined template that instructs the LLM to elaborate on the attributes
of a visual object. The template is phrased as ‚ÄúThis is a <CLSNAME> because‚Äù . The output JSON
contains four primary nodes: Concepts, Substances, Attributes, and Environments. As
such, the LLM is prompted to return the attributes of that concept. Note that the initial attributes
tree may not accurately represent the model; refinements will be made in the refinement stage."
TREE CONSTRUCTION VIA LLM,0.1469265367316342,"‚Ä¢ Description Composition: Next, we guide the LLM to create descriptions based on these
attributes.
Again we showcase an in-context example and instruct the model to output
‚ÄúGenerate sentences that describe a concept according to each attribute.‚Äù ."
TREE CONSTRUCTION VIA LLM,0.14842578710644677,"Once the LLM generates the structured attributes di, we parse them into an initial tree, represented as
T (0)
i
= (V (0)
i
, E(0)
i
), using the key-value pairs of the JSON text. Those generated JSON tree is then
utilized to query images corresponding to each factor."
TREE CONSTRUCTION VIA LLM,0.14992503748125938,"Visual Embeddings Tree from Retrieved Images. In order to enable the vision model to understand
attributes generated by the LLM, we employ a two-step approach. The primary step involves the
conversion of textual descriptions, outputted by the LLM, into images. Then, these images are
deployed to investigate the feature region that symbolizes specific attributes within the model."
TREE CONSTRUCTION VIA LLM,0.15142428785607195,"The transition from linguistic elements to images is facilitated by the use of arbitrary text-to-image
API. This instrumental API enables the generation of novel images or retrieval of existing images that
bear strong relevance to the corresponding textual descriptions. An initial parse tree node, denoted by"
TREE CONSTRUCTION VIA LLM,0.15292353823088456,"Dog
Attributes
Head"
TREE CONSTRUCTION VIA LLM,0.15442278860569716,"round
Shape Nose wet ‚Ä¶ ‚Ä¶"
TREE CONSTRUCTION VIA LLM,0.15592203898050974,"colour
black"
TREE CONSTRUCTION VIA LLM,0.15742128935532235,"shape
button"
TREE CONSTRUCTION VIA LLM,0.15892053973013492,"Dog
Attributes
Head"
TREE CONSTRUCTION VIA LLM,0.16041979010494753,"round
Shape Nose wet ‚Ä¶ ‚Ä¶"
TREE CONSTRUCTION VIA LLM,0.1619190404797601,colour black
TREE CONSTRUCTION VIA LLM,0.1634182908545727,"shape
button pink brown"
TREE CONSTRUCTION VIA LLM,0.16491754122938532,"mottled ùëá!""#"
TREE CONSTRUCTION VIA LLM,0.1664167916041979,"(%&')
Tree Prune
Tree Grow"
TREE CONSTRUCTION VIA LLM,0.1679160419790105,"Instruction:
Add more visual attribute for the 
<nose colour> of a <dog>, to the json.
ChatGPT"
TREE CONSTRUCTION VIA LLM,0.16941529235382308,"Dog
Attributes
Head"
TREE CONSTRUCTION VIA LLM,0.17091454272863568,"round
Shape Nose wet ‚Ä¶ ‚Ä¶"
TREE CONSTRUCTION VIA LLM,0.1724137931034483,"colour
black"
TREE CONSTRUCTION VIA LLM,0.17391304347826086,"shape
button"
TREE CONSTRUCTION VIA LLM,0.17541229385307347,Muzzle broad
TREE CONSTRUCTION VIA LLM,0.17691154422788605,narrow
TREE CONSTRUCTION VIA LLM,0.17841079460269865,"Rarely Visited Nodes ùëá!""# (%)"
TREE CONSTRUCTION VIA LLM,0.17991004497751126,New Nodes
TREE CONSTRUCTION VIA LLM,0.18140929535232383,Figure 4: Tree refinement by traversing the embedding tree and querying the LLM model.
TREE CONSTRUCTION VIA LLM,0.18290854572713644,"v, containing a textual attribute, is inputted into the API to yield a corresponding set of K support
images, represented as {exi}K
i=1 = T2I(v). The value of K is confined to a moderately small range,
typically between 5 to 30. The full information of the collected dataset will be introduced in Section 4."
TREE CONSTRUCTION VIA LLM,0.18440779610194902,"Our research incorporates the use of search engines such as Bing, or text-to-image diffusion models
like Stable-Diffusion [56], to derive images that correspond accurately to the provided attributes."
TREE CONSTRUCTION VIA LLM,0.18590704647676162,"Following this, the images are presented to the visual model to extract their respective embeddings,
represented as pi = g(exi). As such, each tree node contains a set of support visual features
P = {pk}K
k=1. This procedure allows for the construction of an embedding tree, consisting of paired
text and visual features. These pairs are arranged in a tree structure prescribed by the LLM. It is
important to note that the collected images are not employed in training the model. Instead, they
serve as a support set to assist the model in understanding and representing the disentangled attributes
effectively. As such, the visual model uses these embeddings as a map to navigate through the
vast feature space, carving out territories of attributes, and laying down the groundwork for further
exploration and explanation of a particular input."
TREE CONSTRUCTION VIA LLM,0.1874062968515742,"Tree Refinement Via Refine Prompt. Upon construction, the parse tree structure is refined to better
align with the model‚Äôs feature spaces. This stage, termed Tree Refinement, is achieved through passing
training data as a query to traverse the tree. Nodes that are seldom visited indicate that the model
infrequently recognizes their associated attributes. Therefore, we propose a pruning mechanism that
selectively eliminates these attributes, streamlining the tree structure. For nodes that frequently appear
during the traversal, we further grow the tree by introducing additional or more detailed attributes,
enriching the overall context and depth of the tree. The procedure is demonstrated in Figure 4."
TREE CONSTRUCTION VIA LLM,0.1889055472263868,"Initially, we treat the original training samples, denoted as (xj, yj) ‚ààDtr, as our query set. Each
sample is passed to the visual model to extract a feature, represented as qj = g(xj)."
TREE CONSTRUCTION VIA LLM,0.1904047976011994,"Next, the extracted feature traverses the yj-corresponding tree. Its aim is to locate the closest semantic
neighbors among the tree nodes. We define a distance metric between qj to support set P as the
point-to-set distance D(qj, P). This metric represents the greatest lower bound of the set of distances
from qj to prototypes in P. It is resilient to outliers and effectively suppresses non-maximum nodes."
TREE CONSTRUCTION VIA LLM,0.191904047976012,"D(qj, P) = inf{d(qj, p)|p ‚ààP}
(1)"
TREE CONSTRUCTION VIA LLM,0.1934032983508246,"In our paper, similar to [10, 58], we set d(q, p) = ‚àílog(1 +
1
||q‚àíp||2 )2. It emphasizes close points
while moderating the impact of larger distances. Following this, we employ a Depth-First Search
(DFS) algorithm to locate the tree node closest to the query point qj. After finding this node, each
training point (xj, yj) is assigned to a specific node of the tree. Subsequently, we count the number
of samples assigned to a particular node v‚àó, using the following formula: Cv‚àó= M
X"
TREE CONSTRUCTION VIA LLM,0.19490254872563717,"j=1
1{v‚àó= argmin"
TREE CONSTRUCTION VIA LLM,0.19640179910044978,"v‚ààV (0)
yj
D(qj, Pv)}
(2)"
TREE CONSTRUCTION VIA LLM,0.19790104947526238,"In this formula, 1 is the indicator function and Pv denotes the support feature for node v. Following
this, we rank each node based on the sample counter, which results in two operations to update the
tree architecture T (t+1)
i
= Grow(Prune(T (t)
i
)), where t stands as the iteration number"
TREE CONSTRUCTION VIA LLM,0.19940029985007496,"‚Ä¢ Tree Pruning. Nodes with the least visits are pruned from the tree, along with their child nodes."
TREE CONSTRUCTION VIA LLM,0.20089955022488756,"2In practice, we implement d(q, p) = ‚àílog( ||q‚àíp||2+1"
TREE CONSTRUCTION VIA LLM,0.20239880059970014,"||q‚àíp||2+œµ ), incorporating œµ > 0 to ensure numerical stability."
TREE CONSTRUCTION VIA LLM,0.20389805097451275,"‚Ä¢ Tree Growing. For the top-ranked node, we construct a new inquiry to prompt the LLM to
generate attributes with finer granularity. The inquiry is constructed with an instruction template"
TREE CONSTRUCTION VIA LLM,0.20539730134932535,"‚ÄúAdd visual attributes for the <NodeName> of a <ClassName>, to the json‚Äù ."
TREE CONSTRUCTION VIA LLM,0.20689655172413793,"‚Ä¢ Common Node Discrimination. In cases where different categories share common nodes (e.g.
‚Äúhuman‚Äù and ‚Äúdog‚Äù both have ‚Äúear‚Äù), we execute a targeted growth step aimed at distinguishing be-
tween these shared elements. To achieve this differentiation, we utilize a contrasting question posed
to the LLM ‚ÄúThe <NodeName> of <ClassName1> is different from <Classname2> because‚Äù ."
TREE CONSTRUCTION VIA LLM,0.20839580209895053,"The revised concept tree generated by the LLM provides a comprehensive and detailed represen-
tation of the visual attribute. To refine the attribute further, we employ an iterative procedure that
involves image retrieval and the extraction of visual embeddings, as illustrated in Figure 1. This
iterative process enhances the parse tree by incorporating new elements. As each new element
is introduced, the attribute areas within the feature space become increasingly refined, leading to
improved interpretability. In our experiment, we performed five rounds of tree refinement."
ROUTING IN THE TREE,0.2098950524737631,"3.2
Routing in the Tree"
ROUTING IN THE TREE,0.21139430284857572,"Once the tree is established, the model predicts the class of a new test sample x‚Ä≤ and provides an
explanation for this decision by finding the top-k nearest neighbor nodes."
ROUTING IN THE TREE,0.2128935532233883,"Specifically, the model predicts the category ÀÜy for the test instance x‚Ä≤ as ÀÜy = f(x‚Ä≤). The extracted
image feature q‚Ä≤ corresponding to x‚Ä≤ is routed through the tree. Starting from the root, the tree is
traversed to select the top-k nearest neighbor nodes {vi}k
i=1 based on the smallest D(q‚Ä≤, Pvi) values,
representing the highest semantic similarity between q‚Ä≤ and the visual features in the tree‚Äôs nodes.
The paths from the root to the selected nodes are merged to construct the explanatory tree T for the
model‚Äôs prediction."
ROUTING IN THE TREE,0.2143928035982009,"This parse tree structure reveals the sequence of visual attributes that influenced the model‚Äôs classifica-
tion of x‚Ä≤ as ÀÜy. It facilitates the creation of precise, tree-structured justifications for these predictions.
Importantly, the routing process involves only a few feature similarity computations per node and
does not require queries to the large language model, resulting in exceptionally fast computation."
CALIBRATING THROUGH EXPLAINING,0.2158920539730135,"3.3
Calibrating through Explaining"
CALIBRATING THROUGH EXPLAINING,0.21739130434782608,"The created parse tree offers a two-fold advantage. Not only does it illustrates the logic of a
specific prediction, but it also serves as a by-product to refine the model‚Äôs predictions by introducing
hierarchical regularization for learned representation. Our goal is to use the parse tree as pseudo-labels,
embedding this hierarchical knowledge into the model."
CALIBRATING THROUGH EXPLAINING,0.21889055472263869,"To operationalize this, we employ a hierarchical multi-label contrastive loss (HiMulCon) [92], denoted
as LHMC, to fine-tune the pre-trained neural network. This approach enhances the model by infusing
structured explanations into the learning process, thus enriching the representation."
CALIBRATING THROUGH EXPLAINING,0.22038980509745126,"Specifically, we apply the LVX on all training samples. The explanatory path ÀÜTj provides a hierarchical
annotation for each training sample xj. The model is trained with both the cross-entropy loss LCE
and LHMC as follows: min M
X"
CALIBRATING THROUGH EXPLAINING,0.22188905547226387,"j=1
LCE

f(xj), yj

+ ŒªLHMC

g(xj), ÀÜTj

(3)"
CALIBRATING THROUGH EXPLAINING,0.22338830584707647,"Here, Œª is a weighting coefficient. The explanation ÀÜTj is updated every 10 training epochs to ensure
its alignment with the network‚Äôs evolving parameters and learning progress. Notably, the support set
isn‚Äôt used in model training, maintaining a fair comparison with the baselines."
EXPERIMENT,0.22488755622188905,"4
Experiment"
EXPERIMENT,0.22638680659670166,"This section offers an in-depth exploration of our evaluation process for the proposed LVX framework
and explains how it can be utilized to gain insights into the behavior of a trained visual recognition
model, potentially leading to performance and transparency improvements."
EXPERIMENT,0.22788605697151423,"Table 1: Data annotation statistics. The ‚àóindicates the number of video frames. We compare the
statistics of category, attributes, image and tree depth across different explanatory datasets. Our
dataset stands out as the first hierarchical dataset, offering a wide range of attributes."
EXPERIMENT,0.22938530734632684,"Dataset Name
No. Class No. Attr No. Images Avg. Tree Depth Rationales Hierarchy Validation Only
AWA2 [81]
50
85
37,322
N/A
‚úì
√ó
√ó
CUB [76]
200
N/A
11,788
N/A
‚úì
√ó
√ó
BDD-X [35]
906
1,668
26,000*
N/A
‚úì
√ó
√ó
VAW [52]
N/A
650
72,274
N/A
√ó
√ó
√ó
COCO Attr [50]
29
196
180,000
N/A
√ó
√ó
√ó
DR-CIFAR-10 [47]
10
63
2,201
N/A
‚úì
√ó
√ó
DR-CIFAR-100 [47]
100
540
18,318
N/A
‚úì
√ó
√ó
DR-ImageNet [47]
1,000
5,810
271,016
N/A
‚úì
√ó
√ó
H-CIFAR-10
10
289
10,000
4.3
‚úì
‚úì
‚úì
H-CIFAR-100
100
2,359
10,000
4.5
‚úì
‚úì
‚úì
H-ImageNet
1,000
26,928
50,000
4.8
‚úì
‚úì
‚úì"
EXPERIMENT,0.23088455772113944,"Figure 5: Plausibility comparison on three visual tree parsing benchmarks. We plot the mean¬±std
across all networks architectures. For both scores, higher values indicate better performance."
EXPERIMENTAL SETUP,0.23238380809595202,"4.1
Experimental Setup"
EXPERIMENTAL SETUP,0.23388305847076463,"Data Annotation and Collection. To assess explanation plausibility, data must include human
annotations. Currently, no large-scale vision dataset with hierarchical annotations is available to
facilitate reasoning for visual predictions. To address this, we developed annotations for three
recognized benchmarks: CIFAR10, CIFAR100 [39], and ImageNet [57], termed as H-CIFAR10,
H-CIFAR100, and H-ImageNet. These annotations, detailed in Table 8, serve as ground truth for
model evaluation, highlighting our dataset‚Äôs unique support for hierarchical attributes and diverse
visual concepts. Note that, we evaluate on hierarchical datasets only, as our method is specifically
designed for structured explanations."
EXPERIMENTAL SETUP,0.2353823088455772,"As an additional outcome of our framework, we have gathered three support sets to facilitate model
explanation. In these datasets, each attribute generated by the LLM corresponds to a collection of
images that showcase the specified visual concepts. These images are either retrieved from Bing
search engine 3 using attributes as queries or are generated using Stable-diffusion. We subsequently
filter the mismatched pairs with the CLIP model, with the threshold of 0.5. Due to the page limit,
extensive details on data collection, false positive removal, limitations, and additional evaluation of
user study and on medical data, such as X-ray diagnoses, are available in the supplementary material."
EXPERIMENTAL SETUP,0.2368815592203898,"Evaluation Metrics. In this paper, we evaluate the quality of our explanation from three perspectives:
Plausibility, Faithfulness and Stability."
EXPERIMENTAL SETUP,0.2383808095952024,"‚Ä¢ Plausibility measures how reasonable the machine explanation is compared to the human explana-
tion. We measure this by the graph distance between the predicted and ground-truth trees, using
two metrics: Maximum Common Subgraph (MCS) [54, 33], and Tree Kernels (TK) [71]. We
calculate their normalized scores respectively. Specifically, given a predicted tree Tpred and the
ground-truth Tgt, the MCS score is computed as
|MCS|√ó100
‚àö"
EXPERIMENTAL SETUP,0.239880059970015,"|Tpred||Tgt|, and the TK score is computed as"
EXPERIMENTAL SETUP,0.2413793103448276,"T K(Tpred,Tgt)√ó100
‚àö"
EXPERIMENTAL SETUP,0.24287856071964017,"T K(Tpred,Tpred)T K(Tgt,Tgt). Here, | ¬∑ | represents the number of nodes in a tree, and TK(¬∑, ¬∑)"
EXPERIMENTAL SETUP,0.24437781109445278,denotes the unnormalized TK score. We report the average score across all validation samples.
EXPERIMENTAL SETUP,0.24587706146926536,"‚Ä¢ Faithfulness states that the explanations should reflect the inner working of the model. We
introduce Model-induced Sample-Concept Distance (MSCD) to evaluate this, calculated as the"
EXPERIMENTAL SETUP,0.24737631184407796,3https://www.bing.com/images/
EXPERIMENTAL SETUP,0.24887556221889057,"Table 2: Stability comparison in CIFAR10 un-
der input perturbations."
EXPERIMENTAL SETUP,0.25037481259370314,Method Network
EXPERIMENTAL SETUP,0.2518740629685157,"Clean
Gaussian
(œÉ = 0.05)
Gaussian
(œÉ = 0.1)
Cutout
(nholes = 1)"
EXPERIMENTAL SETUP,0.25337331334332835,"MCS TK MCS
TK
MCS TK MCS
TK"
EXPERIMENTAL SETUP,0.25487256371814093,"TrDec
RN-18
100 100 65.3
86.4
56.2 82.5 65.4
86.0
LVX
RN-18
100 100 69.7
90.8
62.1 86.5 68.1
88.3"
EXPERIMENTAL SETUP,0.2563718140929535,"TrDec
RN-50
100 100 68.3
88.5
59.3 84.2 66.2
86.9
LVX
RN-50
100 100 71.9
92.1
65.6 88.3 69.3
90.1"
EXPERIMENTAL SETUP,0.25787106446776614,"Table 3: Faithfulness comparison by computing
the MSCD score. Smaller the better."
EXPERIMENTAL SETUP,0.2593703148425787,"Network
CIFAR-10
CIFAR-100
ImageNet"
EXPERIMENTAL SETUP,0.2608695652173913,TrDecSubTree LVX TrDecSubTree LVX TrDecSubTree LVX
EXPERIMENTAL SETUP,0.2623688155922039,"RN-18
-0.224 -0.393 -0.971-0.246 -0.446 -0.574-0.298 -0.548 -0.730"
EXPERIMENTAL SETUP,0.2638680659670165,"RN-50
-0.236 -0.430 -1.329-0.256 -0.500 -1.170-0.317 -0.588 -1.186"
EXPERIMENTAL SETUP,0.2653673163418291,ViT-S 16-0.244 -0.467 -1.677-0.266 -0.527 -1.073-0.330 -0.626 -1.792
EXPERIMENTAL SETUP,0.26686656671664166,"average of point-to-set distances
1
Nv
P"
EXPERIMENTAL SETUP,0.2683658170914543,"v‚ààV D(qj, Pv) between all test samples and tree nodes,
reflecting the alignment between generated explanation and model‚Äôs internal logic. The concept
is simple: if the explanation tree aligns with the model‚Äôs internal representation, the MSCD is
minimized, indicating high faithfulness."
EXPERIMENTAL SETUP,0.2698650674662669,"‚Ä¢ Stability evaluates the resilience of the explanation graph to minor input variation, expecting
minimal variations in explanations. The MCS/TK metrics are used to assess stability by comparing
explanations derived from clean and slightly modified inputs. We include 3 perturbations, including
Gaussian additive noise with œÉ ‚àà{0.05, 0.1} and Cutout [18] augmentation."
EXPERIMENTAL SETUP,0.27136431784107945,"Baselines. We construct three baselines for comparisons: Constant, using the full category template
tree; Random, which selects a subtree randomly from the template; and Subtree, choosing the most
common subtree in the test set for explanations. Additionally, we consider TrDec Baseline [79],
a strategy utilizing a tree-topology RNN decoder on top of image encoder. Given the absence of
hierarchical annotations, the CLIP model verifies nodes in the template trees, serving as pseudo-labels
for training. We only update the decoder parameters for interpretation purposes. These models
provide a basic comparison for the performance of LVX. More details are in the appendix."
EXPERIMENTAL SETUP,0.272863568215892,"For classification performance, we compare LVX-calibrated model with neural-tree based solutions, in-
cluding a Decision Tree (DT) trained on the neural network‚Äôs final layer, DNDF [38], and NBDT [77]."
EXPERIMENTAL SETUP,0.27436281859070466,"Models to be Explained. Our experiments cover a wide range of neural networks, including
various convolutional neural networks (CNN) and transformers. These models consist of VGG [66],
ResNet [26], DenseNet [29], GoogLeNet [72], Inceptionv3 [73], MobileNet-v2 [59], and Vision
Transformer (ViT) [19]. In total, we utilize 12 networks for CIFAR-10, 11 networks for CIFAR-100,
and 8 networks for ImageNet. For each model, we perform the tree refinement for 5 iterations."
EXPERIMENTAL SETUP,0.27586206896551724,"Calibration Model Training. As described in Section 3.3, we finetune the pre-trained neural net-
works with the hierarchical contrastive loss based on the explanatory results. The model is optimized
with SGD for 50 epochs on the training sample, with an initial learning rate in {0.001, 0.01, 0.03}
and a momentum term of 0.9. The weighting factor is set to 0.1. We compare the calibrated models
with the original ones in terms of accuracy and explanation faithfulness."
LLM HELPS VISUAL INTERPREBILITY,0.2773613193403298,"4.2
LLM helps Visual Interprebility"
LLM HELPS VISUAL INTERPREBILITY,0.27886056971514245,"Plausibility Results. We evaluated LVX against human annotations across three datasets, using
different architectures, and calculating MCS and TK scores. The results, shown in Figure 5, reveal
LVX outperforms baselines, providing superior explanations. Notably, TrDec, even when trained on
CLIP induced labels, fails to generate valid attributes in deeper tree layers‚Äîa prevalent issue in long
sequence and structure generation tasks. Meanwhile, SubTree lacks adaptability in its explanations,
leading to lower scores. More insights are mentioned in the appendix."
LLM HELPS VISUAL INTERPREBILITY,0.280359820089955,"Faithfulness Results. We present the MSCD scores for ResNet-18(RN-18), ResNet-50(RN-50), and
ViT-S, contrasting them with SubTree and TrDec in Table 3. Thanks to the incorporation of tree
refinement that explicitly minimizes MSCD, our LVX method consistently surpasses benchmarks,
demonstrating lowest MSCD values, indicating its enhanced alignment with model reasoning."
LLM HELPS VISUAL INTERPREBILITY,0.2818590704647676,"Stability Results. The stability of our model against minor input perturbations on the CIFAR-10
dataset is showcased in Table 2, where MCS/TK are computed. The ‚ÄúClean‚Äù serves as the oracle
baseline. Our method, demonstrating robustness to input variations, retains consistent explanation
results (MCS>60, TK>85). In contrast, TrDec, dependent on an RNN-parameterized decoder,
exhibits higher sensitivity to feature variations."
LLM HELPS VISUAL INTERPREBILITY,0.28335832083958024,Ground-Truth Tench
LLM HELPS VISUAL INTERPREBILITY,0.2848575712143928,"Pred: 
Sturgeon"
LLM HELPS VISUAL INTERPREBILITY,0.2863568215892054,Concepts
LLM HELPS VISUAL INTERPREBILITY,0.28785607196401797,Substance
LLM HELPS VISUAL INTERPREBILITY,0.2893553223388306,Attributes
LLM HELPS VISUAL INTERPREBILITY,0.2908545727136432,Aquatic
LLM HELPS VISUAL INTERPREBILITY,0.29235382308845576,Cartilaginous
LLM HELPS VISUAL INTERPREBILITY,0.2938530734632684,Prehistoric Head Mouth
LLM HELPS VISUAL INTERPREBILITY,0.29535232383808097,toothless ‚àö ‚àö
LLM HELPS VISUAL INTERPREBILITY,0.29685157421289354,"Long 
snout√ó √ó ‚àö ‚àö √ó
√ó"
LLM HELPS VISUAL INTERPREBILITY,0.2983508245877061,"Ground-Truth
Great While Shark"
LLM HELPS VISUAL INTERPREBILITY,0.29985007496251875,"Pred: 
Killer
Whale"
LLM HELPS VISUAL INTERPREBILITY,0.30134932533733133,Concepts
LLM HELPS VISUAL INTERPREBILITY,0.3028485757121439,Substance Body
LLM HELPS VISUAL INTERPREBILITY,0.30434782608695654,Aquatic
LLM HELPS VISUAL INTERPREBILITY,0.3058470764617691,"Black and white 
smooth skin with 
patches ‚àö √ó
√ó"
LLM HELPS VISUAL INTERPREBILITY,0.3073463268365817,"‚àö
Pectoral"
LLM HELPS VISUAL INTERPREBILITY,0.30884557721139433,"Fin
‚àö
Fin"
LLM HELPS VISUAL INTERPREBILITY,0.3103448275862069,Tall ‚àö
LLM HELPS VISUAL INTERPREBILITY,0.3118440779610195,Dorsal Fin ‚àö
LLM HELPS VISUAL INTERPREBILITY,0.31334332833583206,"Tail 
flukes√ó Wide√ó"
LLM HELPS VISUAL INTERPREBILITY,0.3148425787106447,Ground-Truth
LLM HELPS VISUAL INTERPREBILITY,0.31634182908545727,Green lizard
LLM HELPS VISUAL INTERPREBILITY,0.31784107946026985,"Pred: 
Green"
LLM HELPS VISUAL INTERPREBILITY,0.3193403298350825,lizard
LLM HELPS VISUAL INTERPREBILITY,0.32083958020989506,Substances
LLM HELPS VISUAL INTERPREBILITY,0.32233883058470764,Environment
LLM HELPS VISUAL INTERPREBILITY,0.3238380809595202,Attributes
LLM HELPS VISUAL INTERPREBILITY,0.32533733133433285,Scales Tail Head
LLM HELPS VISUAL INTERPREBILITY,0.3268365817091454,Slender ‚àö ‚àö
LLM HELPS VISUAL INTERPREBILITY,0.328335832083958,"Open
Woodland"
LLM HELPS VISUAL INTERPREBILITY,0.32983508245877063,Jaws ‚àö
LLM HELPS VISUAL INTERPREBILITY,0.3313343328335832,"‚àö
Eyes ‚àö"
LLM HELPS VISUAL INTERPREBILITY,0.3328335832083958,Ovular‚àö ‚àö
LLM HELPS VISUAL INTERPREBILITY,0.3343328335832084,Ground-Truth
LLM HELPS VISUAL INTERPREBILITY,0.335832083958021,Snowmobile
LLM HELPS VISUAL INTERPREBILITY,0.3373313343328336,"Pred: 
Snow
mobile"
LLM HELPS VISUAL INTERPREBILITY,0.33883058470764615,Concepts
LLM HELPS VISUAL INTERPREBILITY,0.3403298350824588,Substance
LLM HELPS VISUAL INTERPREBILITY,0.34182908545727136,Attribute
LLM HELPS VISUAL INTERPREBILITY,0.34332833583208394,For Sport Skis ‚àö
LLM HELPS VISUAL INTERPREBILITY,0.3448275862068966,"‚àö
Frozen Lake"
LLM HELPS VISUAL INTERPREBILITY,0.34632683658170915,Narrow ‚àö
LLM HELPS VISUAL INTERPREBILITY,0.34782608695652173,Skis ‚àö Seats
LLM HELPS VISUAL INTERPREBILITY,0.3493253373313343,"1 or 2
person ‚àö ‚àö
‚àö"
LLM HELPS VISUAL INTERPREBILITY,0.35082458770614694,Environment
LLM HELPS VISUAL INTERPREBILITY,0.3523238380809595,Ground-Truth Hook
LLM HELPS VISUAL INTERPREBILITY,0.3538230884557721,"Pred: 
Nematode"
LLM HELPS VISUAL INTERPREBILITY,0.3553223388305847,Concepts
LLM HELPS VISUAL INTERPREBILITY,0.3568215892053973,Attribute
LLM HELPS VISUAL INTERPREBILITY,0.3583208395802099,Substances
LLM HELPS VISUAL INTERPREBILITY,0.3598200899550225,Parasite
LLM HELPS VISUAL INTERPREBILITY,0.3613193403298351,No joint legs Body Head Mouth Small Shape
LLM HELPS VISUAL INTERPREBILITY,0.36281859070464767,Cylindrical
LLM HELPS VISUAL INTERPREBILITY,0.36431784107946025,"Elongated √ó √ó √ó √ó
‚àö ‚àö"
LLM HELPS VISUAL INTERPREBILITY,0.3658170914542729,Ground-Truth
LLM HELPS VISUAL INTERPREBILITY,0.36731634182908546,Band Aid
LLM HELPS VISUAL INTERPREBILITY,0.36881559220389803,"Pred:
Piggy 
Bank"
LLM HELPS VISUAL INTERPREBILITY,0.37031484257871067,Concepts
LLM HELPS VISUAL INTERPREBILITY,0.37181409295352325,Substance
LLM HELPS VISUAL INTERPREBILITY,0.3733133433283358,Attribute
LLM HELPS VISUAL INTERPREBILITY,0.3748125937031484,Object ‚àö
LLM HELPS VISUAL INTERPREBILITY,0.37631184407796103,"Made of 
ceramics √ó"
LLM HELPS VISUAL INTERPREBILITY,0.3778110944527736,"‚àö
Pink
Colour"
LLM HELPS VISUAL INTERPREBILITY,0.3793103448275862,"Slot for 
coin"
LLM HELPS VISUAL INTERPREBILITY,0.3808095952023988,Opening√ó ‚àö ‚àö
LLM HELPS VISUAL INTERPREBILITY,0.3823088455772114,"(a) Correct predictions, 
reasonable explanations."
LLM HELPS VISUAL INTERPREBILITY,0.383808095952024,"(b) Error predictions,
but partially correct explanations."
LLM HELPS VISUAL INTERPREBILITY,0.3853073463268366,"(b) Noisy annotations,
reasonable explanations. (c)"
LLM HELPS VISUAL INTERPREBILITY,0.3868065967016492,"Figure 6: Explanation visualization for ViT-B on ImageNet-1K. ‚úìand √ó means that the node is
aligned or misaligned with the image. Zoom in for better view."
LLM HELPS VISUAL INTERPREBILITY,0.38830584707646176,"Method
Network
Expl.
CIFAR10
CIFAR100
ImageNet"
LLM HELPS VISUAL INTERPREBILITY,0.38980509745127434,"NN
ResNet18
√ó
94.97%
75.92%
69.76%
DT
ResNet18
‚úì
93.97%
64.45%
63.45%
DNDF
ResNet18
‚úì
94.32%
67.18%
N/A
NBDT
ResNet18
‚úì
94.82%
77.09%
65.27%"
LLM HELPS VISUAL INTERPREBILITY,0.391304347826087,"LVX (Ours)
ResNet18
‚úì
95.14%
77.33%
70.28%
Table 4: Performance comparison of neural de-
cision tree-based methods. Expl. stands for
whether the prediction is explainable."
LLM HELPS VISUAL INTERPREBILITY,0.39280359820089955,"Table 5:
Performance and interpretability
comparison with/without model calibration on
CIFAR-100. Higher MCS means better."
LLM HELPS VISUAL INTERPREBILITY,0.39430284857571213,"Model and Data Diagnosis with Explanation. We visualize the sample explanatory parse tree on
ImageNet validation set induced by ViT-B in Figure 6. The explanations fall into three categories: (1)
correct predictions with explanations, (2) incorrect predictions with explanations, and (3) noisy label
predictions with explanations. We‚Äôve also displayed the 5 nearest neighbor node for each case."
LLM HELPS VISUAL INTERPREBILITY,0.39580209895052476,"What‚Äôs remarkable about LVX is that, even when the model‚Äôs prediction is wrong, it can identify
correct attributes. For instance, in a case where a ‚Äúwhite shark‚Äù was misidentified as a ‚Äúkiller
whale‚Äù (b-Row 2), LVX correctly identified ‚Äúfins‚Äù, a shared attribute of both species. Moreover, the
misrecognition of the attribute ‚Äúwide tail flukes‚Äù indicates a potential error in the model, that
could be later addressed to enhance its performance."
LLM HELPS VISUAL INTERPREBILITY,0.39730134932533734,"Surprisingly, LVX is able to identify certain noisy labels in the data, as shown in c-Row 2. In
such cases, even experienced human observers might struggle to decide whether a ‚Äúpig bank
with band‚Äù should be classified ‚Äúpiggy bank‚Äù or ‚Äúband aid‚Äù. It again underscores the superior
capabilities of our LVX system in diagnosing the errors beyond model, but also within the data itself."
LLM HELPS VISUAL INTERPREBILITY,0.3988005997001499,"Calibration Enhances Interpretability and Performance. Our approach involves fine-tuning a
pre-trained model with the loss function outlined in Section 3.3, using parsed explanatory trees to
improve model performance. Table 4 compares the classification performance of our model with that
of other neural tree methods. Our model clearly outperforms the rest."
LLM HELPS VISUAL INTERPREBILITY,0.4002998500749625,"Neural tree models often face challenges in balancing interpretability with performance. In contrast,
LVX achieves strong performance without relying on a strict decision tree. Instead, decisions are
handled by the neural network, with concepts guided by the LLM through Equation 3. This approach
enhances the model‚Äôs ability to disentangle visual concepts while preserving high performance."
LLM HELPS VISUAL INTERPREBILITY,0.4017991004497751,"In addition, we compared the quality of the generated parsed tree with or without calibration, in
Figure 5. The calibration process not only improved model performance, but also led to more precise
tree predictions, indicating enhanced interpretability. We also test the calibrated model on OOD
evaluations in Appendix, where we observe notable improvements."
ABLATION STUDY AND ANALYSIS,0.4032983508245877,"5
Ablation Study and Analysis"
ABLATION STUDY AND ANALYSIS,0.4047976011994003,"In this section, we present an ablation study on the refinement stage of LVX. We also apply the method
to different neural networks to observe variations in model‚Äôs behavior."
ABLATION STUDY AND ANALYSIS,0.4062968515742129,"Table 6: Performance comparison on CIFAR-10 and CIFAR-100 with and without refinement. Higher
MCS and lower MSCD indicate better performance."
ABLATION STUDY AND ANALYSIS,0.4077961019490255,"Network
Method
CIFAR-10
CIFAR-100"
ABLATION STUDY AND ANALYSIS,0.40929535232383807,"MCS
MSCD
MCS
MSCD"
ABLATION STUDY AND ANALYSIS,0.4107946026986507,"ResNet-18
w/o Refine
27.73
-0.645
23.18
-0.432
LVX
30.24
-0.971
25.10
-0.574"
ABLATION STUDY AND ANALYSIS,0.4122938530734633,"ResNet-50
w/o Refine
28.09
-0.822
23.44
-0.698
LVX
31.09
-1.329
26.90
-1.170"
ABLATION STUDY AND ANALYSIS,0.41379310344827586,"Ablation 1: No Refinement. To study the impact of refinement stage, we present a baseline called
w/o Refine. In this setup, the initial tree generated by LLMs is kept fixed. We evaluate the method
using the MSCD for faithfulness and MCS for plausibility on the CIFAR-10 and CIFAR-100 datasets."
ABLATION STUDY AND ANALYSIS,0.41529235382308843,"The results show in Table 6 that incorporating image model feedback indeed improves tree alignment
with the classifier‚Äôs internal representation, as reflected in higher MCS scores. The refined trees also
better match human-annotations."
ABLATION STUDY AND ANALYSIS,0.41679160419790107,"Ablation 2: Refinement Criteria. In our original method, tree refinement is based on feature
similarity to the training set. To explore an alternative, we use average activation magnitude on
generated data as the criterion for concept familiarity. Concepts with activation magnitudes ‚â§Œ∑ are
pruned. This method, referred to as ActMag, is evaluated on CIFAR-10. We report the MCS, MSCD
for performance, and average tree depth as an indicator of tree complexity."
ABLATION STUDY AND ANALYSIS,0.41829085457271364,"Table 7 shows that feature similarity achieves better results than ActMag. Specifically, setting a
threshold is challenging for ActMag, leading shallow trees (Œ∑ = 0.3) or too deep ones (Œ∑ = 0.01)."
ABLATION STUDY AND ANALYSIS,0.4197901049475262,Table 7: Performance comparison on CIFAR-10 with different refinement criteria.
ABLATION STUDY AND ANALYSIS,0.42128935532233885,"Network
LVX
ActMag(Œ∑ = 0.01)
ActMag(Œ∑ = 0.1)
ActMag(Œ∑ = 0.3)"
ABLATION STUDY AND ANALYSIS,0.42278860569715143,"MCS
MSCD
Depth
MCS
MSCD
Depth
MCS
MSCD
Depth
MCS
MSCD
Depth"
ABLATION STUDY AND ANALYSIS,0.424287856071964,"ResNet-50
31.1
-1.3
4.2
23.4
-0.3
6.0
26.9
-0.8
3.7
25.3
-0.5
1.4
ViT-S
31.9
-1.7
4.3
24.2
-0.4
6.2
27.4
-0.9
3.3
26.1
-0.6
1.8"
ABLATION STUDY AND ANALYSIS,0.4257871064467766,"Analysis: CNN vs. Transformer. We use our LVX to compare CNN and Transformer models and
identify which concepts they miss. We compared ConvNeXt-T (CNN) and DeiT-B (Transformer) on
26,928 concepts we collected on ImageNet, from sub-categories of Concepts, Substances, Attributes,
and Environments. We measured accuracy across 4 sub-categories and tree depths."
ABLATION STUDY AND ANALYSIS,0.4272863568215892,"Results show that ConvNeXt-T is better at local patterns (Attributes, Substances), while DeiT-B
perform better on Environments which needs global semantics. Additionally, DeiT-B is more accurate
at shallow depths, whereas ConvNeXt-T performs better at deeper levels. These findings aligns with
earlier research showing that CNN are biased towards textures over shape [23, 84]."
CONCLUSION,0.4287856071964018,"6
Conclusion"
CONCLUSION,0.4302848575712144,"In this study, we introduced LVX, an approach for interpreting vision models using tree-structured
language explanations without hierarchical annotations. LVX leverages large language models to
connect visual attributes with image features, generating comprehensive explanations. We refined
attribute parse trees based on the model‚Äôs recognition capabilities, creating human-understandable
descriptions. Test samples were routed through the parse tree to generate sample-specific rationales.
LVX demonstrated effectiveness in interpreting vision models, offering potential for model calibration.
Our contributions include proposing LVX as the first approach to leverage language models for
explaining the visual recognition system. We hope this study potentially advances interpretable AI
and deepens our understanding of neural networks."
CONCLUSION,0.431784107946027,Acknowledgement
CONCLUSION,0.4332833583208396,"This project is supported by the Ministry of Education, Singapore, under its Academic Research Fund
Tier 2 (Award Number: MOE-T2EP20122-0006), and the National Research Foundation, Singapore,
under its AI Singapore Programme (AISG Award No: AISG2-RP-2021-023)."
REFERENCES,0.43478260869565216,References
REFERENCES,0.4362818590704648,"[1] Samira Abnar and Willem Zuidema.
Quantifying attention flow in transformers.
In Dan Jurafsky,
Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, pages 4190‚Äì4197, Online, July 2020. Association for
Computational Linguistics."
REFERENCES,0.43778110944527737,"[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,
Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for
few-shot learning. Advances in Neural Information Processing Systems, 35:23716‚Äì23736, 2022."
REFERENCES,0.43928035982008995,"[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,
Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi,
Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L Menick, Sebastian Borgeaud,
Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Miko≈Ç aj Bi¬¥nkowski, Ricardo Barreira, Oriol Vinyals,
Andrew Zisserman, and Kar√©n Simonyan. Flamingo: a visual language model for few-shot learning.
In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural
Information Processing Systems, volume 35, pages 23716‚Äì23736. Curran Associates, Inc., 2022."
REFERENCES,0.4407796101949025,"[4] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to compose neural networks
for question answering. arXiv preprint arXiv:1601.01705, 2016."
REFERENCES,0.44227886056971516,"[5] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 39‚Äì48, 2016."
REFERENCES,0.44377811094452774,"[6] Philip Bille. A survey on tree edit distance and related problems. Theoretical computer science, 337(1-
3):217‚Äì239, 2005."
REFERENCES,0.4452773613193403,"[7] Oana-Maria Camburu, Tim Rockt√§schel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language
inference with natural language explanations. Advances in Neural Information Processing Systems, 31,
2018."
REFERENCES,0.44677661169415295,"[8] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsuper-
vised learning of visual features by contrasting cluster assignments. 2020."
REFERENCES,0.4482758620689655,"[9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, and Armand
Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International
Conference on Computer Vision (ICCV), 2021."
REFERENCES,0.4497751124437781,"[10] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This looks like
that: deep learning for interpretable image recognition. Advances in neural information processing systems,
32, 2019."
REFERENCES,0.4512743628185907,"[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In Hal Daum√© III and Aarti Singh, editors, Proceedings of
the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pages 1597‚Äì1607. PMLR, 13‚Äì18 Jul 2020."
REFERENCES,0.4527736131934033,"[12] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive
learning. arXiv preprint arXiv:2003.04297, 2020."
REFERENCES,0.4542728635682159,"[13] Xinlei Chen*, Saining Xie*, and Kaiming He. An empirical study of training self-supervised vision
transformers. arXiv preprint arXiv:2104.02057, 2021."
REFERENCES,0.45577211394302847,"[14] Ying Chen, Feng Mao, Jie Song, Xinchao Wang, Huiqiong Wang, and Mingli Song. Self-born wiring
for neural trees. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
5047‚Äì5056, 2021."
REFERENCES,0.4572713643178411,"[15] Andy Clark. Whatever next? predictive brains, situated agents, and the future of cognitive science.
Behavioral and brain sciences, 36(3):181‚Äì204, 2013."
REFERENCES,0.4587706146926537,"[16] Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE transactions on information
theory, 13(1):21‚Äì27, 1967."
REFERENCES,0.46026986506746626,"[17] Mark Craven and Jude Shavlik. Extracting tree-structured representations of trained networks. Advances
in neural information processing systems, 8, 1995."
REFERENCES,0.4617691154422789,"[18] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with
cutout. arXiv preprint arXiv:1708.04552, 2017."
REFERENCES,0.46326836581709147,"[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020."
REFERENCES,0.46476761619190404,"[20] Ingo Feinerer and Kurt Hornik. wordnet: WordNet Interface, 2023. R package version 0.1-16."
REFERENCES,0.4662668665667166,"[21] Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into a soft decision tree. arXiv preprint
arXiv:1711.09784, 2017."
REFERENCES,0.46776611694152925,"[22] Yossi Gandelsman, Alexei A Efros, and Jacob Steinhardt. Interpreting CLIP‚Äôs image representation via
text-based decomposition. In The Twelfth International Conference on Learning Representations, 2024."
REFERENCES,0.46926536731634183,"[23] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland
Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and
robustness. In International Conference on Learning Representations, 2019."
REFERENCES,0.4707646176911544,"[24] Jean-Bastien Grill, Florian Strub, Florent Altch√©, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,
Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your
own latent-a new approach to self-supervised learning. Advances in neural information processing systems,
33:21271‚Äì21284, 2020."
REFERENCES,0.47226386806596704,"[25] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without
training. arXiv preprint arXiv:2211.11559, 2022."
REFERENCES,0.4737631184407796,"[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770‚Äì778, 2016."
REFERENCES,0.4752623688155922,"[27] Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor Darrell.
Generating visual explanations. In Computer Vision‚ÄìECCV 2016: 14th European Conference, Amsterdam,
The Netherlands, October 11‚Äì14, 2016, Proceedings, Part IV 14, pages 3‚Äì19. Springer, 2016."
REFERENCES,0.4767616191904048,"[28] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason:
End-to-end module networks for visual question answering. In Proceedings of the IEEE international
conference on computer vision, pages 804‚Äì813, 2017."
REFERENCES,0.4782608695652174,"[29] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.
Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 4700‚Äì4708, 2017."
REFERENCES,0.47976011994003,"[30] Yea-Shuan Huang, Cheng-Chin Chiang, Jun-Wei Shieh, and Eric Grimson. Prototype optimization for
nearest-neighbor classification. Pattern Recognition, 35(6):1237‚Äì1245, 2002."
REFERENCES,0.48125937031484256,"[31] Phillip Isola, Joseph J Lim, and Edward H Adelson. Discovering states and transformations in image
collections. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
1383‚Äì1391, 2015."
REFERENCES,0.4827586206896552,"[32] Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural
computation, 6(2):181‚Äì214, 1994."
REFERENCES,0.48425787106446777,"[33] Viggo Kann. On the approximability of the maximum common subgraph problem. In STACS, volume 92,
pages 377‚Äì388. Citeseer, 1992."
REFERENCES,0.48575712143928035,"[34] Monish Keswani, Sriranjani Ramakrishnan, Nishant Reddy, and Vineeth N Balasubramanian. Proto2proto:
Can you recognize the car, the way i do? In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 10233‚Äì10243, 2022."
REFERENCES,0.487256371814093,"[35] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. Textual explanations for
self-driving vehicles. Proceedings of the European Conference on Computer Vision (ECCV), 2018."
REFERENCES,0.48875562218890556,"[36] Diederik P Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.49025487256371814,"[37] Teuvo Kohonen and Teuvo Kohonen. Learning vector quantization. Self-organizing maps, pages 175‚Äì189,
1995."
REFERENCES,0.4917541229385307,"[38] Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota Bulo. Deep neural decision
forests. In Proceedings of the IEEE international conference on computer vision, pages 1467‚Äì1475, 2015."
REFERENCES,0.49325337331334335,"[39] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.4947526236881559,"[40] Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning through
prototypes: A neural network that explains its predictions. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 32, 2018."
REFERENCES,0.4962518740629685,"[41] Qing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo. Vqa-e: Explaining, elaborating, and enhancing
your answers for visual questions. In Proceedings of the European Conference on Computer Vision (ECCV),
pages 552‚Äì567, 2018."
REFERENCES,0.49775112443778113,"[42] Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoffmann, Cyprien de Masson d‚ÄôAutume, Phil Blunsom,
and Aida Nematzadeh. A systematic investigation of commonsense knowledge in large language models.
In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages
11838‚Äì11855, 2022."
REFERENCES,0.4992503748125937,"[43] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,
Shaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting foundation models with millions of
apis. arXiv preprint arXiv:2303.16434, 2023."
REFERENCES,0.5007496251874063,"[44] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural
information processing systems, 36, 2024."
REFERENCES,0.5022488755622189,"[45] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes
good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021."
REFERENCES,0.5037481259370314,"[46] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in
neural information processing systems, 30, 2017."
REFERENCES,0.5052473763118441,"[47] Chengzhi Mao, Revant Teotia, Amrutha Sundar, Sachit Menon, Junfeng Yang, Xin Wang, and Carl
Vondrick.
Doubly right object recognition: A why prompt for visual rationales.
arXiv preprint
arXiv:2212.06202, 2022."
REFERENCES,0.5067466266866567,"[48] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. arXiv
preprint arXiv:2210.07183, 2022."
REFERENCES,0.5082458770614693,"[49] Meike Nauta, Ron Van Bree, and Christin Seifert. Neural prototype trees for interpretable fine-grained im-
age recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 14933‚Äì14943, 2021."
REFERENCES,0.5097451274362819,"[50] Genevieve Patterson and James Hays. Coco attributes: Attributes for people, animals, and objects. In
Computer Vision‚ÄìECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14,
2016, Proceedings, Part VI 14, pages 85‚Äì100. Springer, 2016."
REFERENCES,0.5112443778110944,"[51] Chantal Pellegrini, Matthias Keicher, Ege √ñzsoy, Petra Jiraskova, Rickmer Braren, and Nassir Navab.
Xplainer: From x-ray observations to explainable zero-shot diagnosis. arXiv preprint arXiv:2303.13391,
2023."
REFERENCES,0.512743628185907,"[52] Khoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott Cohen, Quan Tran, and Abhinav Shrivastava.
Learning to predict visual attributes in the wild. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pages 13018‚Äì13028, June 2021."
REFERENCES,0.5142428785607196,"[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In International conference on machine learning, pages 8748‚Äì8763. PMLR,
2021."
REFERENCES,0.5157421289355323,"[54] John W Raymond and Peter Willett. Maximum common subgraph isomorphism algorithms for the
matching of chemical structures. Journal of computer-aided molecular design, 16:521‚Äì533, 2002."
REFERENCES,0.5172413793103449,"[55] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ""why should I trust you?"": Explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pages 1135‚Äì1144,
2016."
REFERENCES,0.5187406296851574,"[56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution
image synthesis with latent diffusion models, 2021."
REFERENCES,0.52023988005997,"[57] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.
International journal of computer vision, 115:211‚Äì252, 2015."
REFERENCES,0.5217391304347826,"[58] Dawid Rymarczyk, ≈Åukasz Struski, Jacek Tabor, and Bartosz Zieli¬¥nski. Protopshare: Prototypical parts
sharing for similarity discovery in interpretable image classification. In Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery & Data Mining, pages 1420‚Äì1430, 2021."
REFERENCES,0.5232383808095952,"[59] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 4510‚Äì4520, 2018."
REFERENCES,0.5247376311844077,"[60] Makoto Sato and Hiroshi Tsukimoto. Rule extraction from neural networks via decision tree induction.
In IJCNN‚Äô01. International Joint Conference on Neural Networks. Proceedings (Cat. No. 01CH37222),
volume 3, pages 1870‚Äì1875. IEEE, 2001."
REFERENCES,0.5262368815592204,"[61] Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv
preprint arXiv:2302.04761, 2023."
REFERENCES,0.527736131934033,"[62] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and
Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In
Proceedings of the IEEE international conference on computer vision, pages 618‚Äì626, 2017."
REFERENCES,0.5292353823088456,"[63] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:
Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023."
REFERENCES,0.5307346326836582,"[64] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagat-
ing activation differences. In International conference on machine learning, pages 3145‚Äì3153. PMLR,
2017."
REFERENCES,0.5322338830584707,"[65] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising
image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013."
REFERENCES,0.5337331334332833,"[66] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.5352323838080959,"[67] Chandan Singh, W. James Murdoch, and Bin Yu. Hierarchical interpretations for neural network predictions.
In International Conference on Learning Representations, 2019."
REFERENCES,0.5367316341829086,"[68] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi√©gas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017."
REFERENCES,0.5382308845577212,"[69] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in
neural information processing systems, 30, 2017."
REFERENCES,0.5397301349325337,"[70] J-E Stromberg, Jalel Zrida, and Alf Isaksson. Neural trees-using neural nets in a tree classifier structure.
In Acoustics, Speech, and Signal Processing, IEEE International Conference on, pages 137‚Äì140. IEEE
Computer Society, 1991."
REFERENCES,0.5412293853073463,"[71] Jun Sun, Min Zhang, and Chew Lim Tan. Tree sequence kernel for natural language. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 25, pages 921‚Äì926, 2011."
REFERENCES,0.5427286356821589,"[72] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages 1‚Äì9, 2015."
REFERENCES,0.5442278860569715,"[73] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2818‚Äì2826, 2016."
REFERENCES,0.545727136431784,"[74] Mohammad Reza Taesiri, Giang Nguyen, and Anh Nguyen. Visual correspondence-based explanations
improve ai robustness and human-ai team accuracy. Advances in Neural Information Processing Systems,
35:34287‚Äì34301, 2022."
REFERENCES,0.5472263868065967,"[75] Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi, and Aditya Nori. Adaptive neural
trees. In International Conference on Machine Learning, pages 6166‚Äì6175. PMLR, 2019."
REFERENCES,0.5487256371814093,"[76] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset.
Technical Report CNS-TR-2011-001, California Institute of Technology, 2011."
REFERENCES,0.5502248875562219,"[77] Alvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee, Henry Jin, Suzanne Petryk, Sarah Adel Bargal,
and Joseph E Gonzalez. Nbdt: neural-backed decision trees. arXiv preprint arXiv:2004.00221, 2020."
REFERENCES,0.5517241379310345,"[78] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers.
Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and
localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2097‚Äì2106, 2017."
REFERENCES,0.553223388305847,"[79] Xinyi Wang, Hieu Pham, Pengcheng Yin, and Graham Neubig. A tree-based decoder for neural machine
translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,
pages 4772‚Äì4777, 2018."
REFERENCES,0.5547226386806596,"[80] Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. Medclip: Contrastive learning from
unpaired medical images and text. arXiv preprint arXiv:2210.10163, 2022."
REFERENCES,0.5562218890554723,"[81] Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning‚Äîa compre-
hensive evaluation of the good, the bad and the ugly. IEEE transactions on pattern analysis and machine
intelligence, 41(9):2251‚Äì2265, 2018."
REFERENCES,0.5577211394302849,"[82] Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, and Zeynep Akata. Attribute prototype network for
zero-shot learning. Advances in Neural Information Processing Systems, 33:21969‚Äì21980, 2020."
REFERENCES,0.5592203898050975,"[83] Xingyi Yang, Jingwen Ye, and Xinchao Wang. Factorizing knowledge in neural networks. In Computer
Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part
XXXIV, pages 73‚Äì91. Springer, 2022."
REFERENCES,0.56071964017991,"[84] Xingyi Yang, Daquan Zhou, Songhua Liu, Jingwen Ye, and Xinchao Wang. Deep model reassembly.
Advances in neural information processing systems, 35:25739‚Äì25753, 2022."
REFERENCES,0.5622188905547226,"[85] Yongxin Yang, Irene Garcia Morillo, and Timothy M Hospedales. Deep neural decision trees. arXiv
preprint arXiv:1806.06988, 2018."
REFERENCES,0.5637181409295352,"[86] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar.
Language in a bottle: Language model guided concept bottlenecks for interpretable image classification.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19187‚Äì
19197, 2023."
REFERENCES,0.5652173913043478,"[87] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,
Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and
action. arXiv preprint arXiv:2303.11381, 2023."
REFERENCES,0.5667166416791605,"[88] Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point selection for
explaining deep neural networks. Advances in neural information processing systems, 31, 2018."
REFERENCES,0.568215892053973,"[89] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-
symbolic vqa: Disentangling reasoning from vision and language understanding. Advances in neural
information processing systems, 31, 2018."
REFERENCES,0.5697151424287856,"[90] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In Computer
Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part I 13, pages 818‚Äì833. Springer, 2014."
REFERENCES,0.5712143928035982,"[91] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit,
Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing
zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022."
REFERENCES,0.5727136431784108,"[92] Shu Zhang, Ran Xu, Caiming Xiong, and Chetan Ramaiah. Use all the labels: A hierarchical multi-label
contrastive learning framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 16660‚Äì16669, 2022."
REFERENCES,0.5742128935532234,"[93] Yuhui Zhang, Jeff Z HaoChen, Shih-Cheng Huang, Kuan-Chieh Wang, James Zou, and Serena Yeung.
Diagnosing and rectifying vision models using language. In International Conference on Learning
Representations (ICLR), 2023."
REFERENCES,0.5757121439280359,"[94] Qiangfu Zhao. Evolutionary design of neural network tree-integration of decision tree, neural network
and ga. In Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No. 01TH8546),
volume 1, pages 240‚Äì244. IEEE, 2001."
REFERENCES,0.5772113943028486,"[95] Xuhui Zhou, Yue Zhang, Leyang Cui, and Dandan Huang. Evaluating commonsense in pre-trained
language models. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages
9733‚Äì9740, 2020."
REFERENCES,0.5787106446776612,"[96] Jan Ruben Zilke, Eneldo Loza Menc√≠a, and Frederik Janssen. Deepred‚Äìrule extraction from deep neural
networks. In Discovery Science: 19th International Conference, DS 2016, Bari, Italy, October 19‚Äì21,
2016, Proceedings 19, pages 457‚Äì473. Springer, 2016."
REFERENCES,0.5802098950524738,"A
Appendix / supplemental material"
REFERENCES,0.5817091454272864,"This document presents supplementary experiments and information regarding our proposed LVX
framework. In Section C, we provide an overview of the algorithm pipeline for LVX. In Section D, we
detail the process of data collection and its subsequent analysis. In additionally, Section E provides
the user study results; Section F shows the calibrated training further improve the OOD performance.
Section G showcases the additional experimental results obtained from a specialized application on
X-Ray diagnosis. Furthermore, in Section H, we demonstrate the explanation results achieved using
self-supervised models. We also provide the raw experimental values in Section I. Finally, we outline
the experimental setup, metric definitions, and dataset collection protocols."
REFERENCES,0.5832083958020989,"B
Related Work"
REFERENCES,0.5847076461769115,"Neural Tree. Neural Trees (NTs) intend to harmonize the performance of Neural Networks (NNs)
and interpretability of Decision Trees (DTs) [17, 21, 60, 96, 14] within a unified model. They
evolved from mimicking NNs with DTs [17, 21, 60, 96, 14] to becoming inherently interpretable
tree-structured networks, adapting their structure via gradient descent [70, 94, 32, 85, 75, 38]. Neural-
Backed Decision Trees (NBDTs) [77] use a trained NN as a feature extractor, replacing its final
layer with a decision tree. Our model builds on these advances to create a hierarchical tree from
a pre-trained NN and provides post-hoc explanations without additional training, which increases
interpretability and potentially enhances performance."
REFERENCES,0.5862068965517241,"Prototype-based Explainable Model. Prototype models use representative training data points to
symbolize classes or outcomes [16, 30, 37]. Revived in deep learning and few-shot learning [69, 82],
they justify decisions by comparing new instances to key examples [10, 88, 40]. Recent work
has developed this approach through hierarchical and local prototypes [49, 34, 74]. However, the
prototypes serve as an indirect explanation for model‚Äôs prediction, necessitating further human
justification. Our LVX addresses this by assigning semantic roles to prototypes through LLM,
turning them from simple similarity points to data points with clear definitions, thereby enhancing
explainability."
REFERENCES,0.5877061469265368,"Composing Foundation Models. Model composition involves merging machine learning models to
address tasks, often using modular models for sub-tasks [5, 28, 4, 83], restructured by a symbolic
executor [89]. Recently, Language Learning Models (LLMs) have been used as central controllers,
guiding the logic of existing models [63, 25, 87, 43] or APIs [61], with language as a universal
interface [91]. However, composing language model with non-language ones lacks a unified interface
for bilateral communication. In this study, we propose the use of a text-to-image API as a medium to
enable the language model to share its knowledge with the visual task. This allows the vision model
to benefit from the linguistic context and knowledge hierarchy, thereby enhancing its transparency."
REFERENCES,0.5892053973013494,"C
Pseudo-code for LVX"
REFERENCES,0.5907046476761619,"In this section, we present the pseudocode for the LVX framework, encompassing both the construction
stage and the test stage. The algorithmic pipelines are outlined in Algorithm 1 and Algorithm 2."
REFERENCES,0.5922038980509745,"The category-level tree construction pipeline, as demonstrated in Algorithm 1, involves an iterative
process that utilizes a large language model (LLM), like ChatGPT. This process allows us to construct
an attribute tree for each category. It begins by generating prompts based on the category name and
using them to gather attribute information. This forms the initial tree structure. Support images are
collected using a text-to-image API, and their visual features are associated with the corresponding
tree nodes. The process iterates until the maximum run, continuously refining the attribute tree for
corresponding category."
REFERENCES,0.5937031484257871,"During the test stage, as outlined in Algorithm 2, the test samples undergo a traversal process within
the constructed category-level trees. The goal is to locate the subtree that best aligns with the correct"
REFERENCES,0.5952023988005997,Algorithm 1 Language Model as Visual Explainer (LVX)-Construction
REFERENCES,0.5967016491754122,"Input: Vision model f = ‚ó¶h, a large language model L, a text-to-image API T2I, a training set Dtr =
{xi, yi}M
i=1, class names C = {ci}n
i=1 and a concept prompt tree input-output example P.
Output: An explanatory tree Ti for each category ci."
REFERENCES,0.5982008995502249,"1: // Construct the initial Parse Tree
2: for i = 1 to n do
3:
In-context Prompt LLM: di = L(ci, P).
4:
Parse di into an initial tree T (0)
i
= {V (0)
i
, E(0)
i
}.
5:
Collect support images from text-to-image API: {exi}K
i=1 = T2I(v), where v ‚ààV (0)
i
.
6:
Extract features in each tree node: Pv = {pi}K
i=1 = {g(exi)|exi ‚àà{exi}K
i=1}.
7: end for
8: // Parse Tree Refinement
9: for t = 0 to tmax do
10:
for j = 1 to M do
11:
Extract feature for training data: qj = g(exj).
12:
Assign training data to a tree node: v‚àó= argminv‚ààV (t)
yj D(qj, Pv)."
REFERENCES,0.5997001499250375,"13:
end for
14:
Count the number of samples for each node: Cv‚àó= PM
j=1 1{v‚àó= argminv‚ààV (0)
yj D(qj, Pv)}"
REFERENCES,0.6011994002998501,"15:
Prune the least visited node: T (t)
i
= Prune(T (t)
i
).
16:
Grow the most visited node: T (t+1)
i
= Grow(T (t)
i
).
17:
Collect support images from text-to-image API: {exi}K
i=1 = T2I(v), where v ‚ààV (t+1)
i
.
18:
Extract features in new tree node: Pv = {pi}K
i=1 = {g(exi)|exi ‚àà{exi}K
i=1}.
19: end for
20: return T (tmax)
i
as Ti"
REFERENCES,0.6026986506746627,Algorithm 2 Language Model as Visual Explainer (LVX)-Test
REFERENCES,0.6041979010494752,"Input: Vision model f = g ¬∑ h, a test sample xts and explanatory trees Ti for each category.
Output: A explanatory tree T for test sample."
REFERENCES,0.6056971514242878,"1: Prediction on xts: q = g(xts) and ÀÜy = h(q).
2: Find the top-matched sub-tree in TÀÜy"
REFERENCES,0.6071964017991005,"T ‚àó= argmin
T ‚àó‚äÜTÀÜy k
X"
REFERENCES,0.6086956521739131,"i=1
D(q, Pvi)"
REFERENCES,0.6101949025487257,"s.t.
T ‚àó= {V ‚àó, E‚àó}, vi ‚ààV ‚àó, |V ‚àó| = k"
REFERENCES,0.6116941529235382,3: return T ‚àóas the prediction explanation.
REFERENCES,0.6131934032983508,"prediction rationales. This sample-wise attribute tree process enables the identification of pertinent
attributes and explanations linked to each test sample, providing insights into the decision-making
process of the model."
REFERENCES,0.6146926536731634,"In summary, the LVX employs an iterative approach to construct category-level trees, leveraging
the knowledge of LLMs. These trees are then utilized during the test stage to extract relevant
explanations. This methodology enables us to gain understanding of the model‚Äôs decision-making
process by revealing the underlying visual attributes and rationales supporting its predictions."
REFERENCES,0.616191904047976,"D
Data Collection and Analysis"
REFERENCES,0.6176911544227887,"D.1
Annotation Creation"
REFERENCES,0.6191904047976012,"The creation of test annotations for three datasets involved a semi-automated approach implemented
in two distinct steps. This process was a collaborative effort between human annotators, a language
model (ChatGPT), and CLIP [53], ensuring both efficiency and reliability."
REFERENCES,0.6206896551724138,"1. Concept Tree Creation. In this first step, we utilized ChatGPT 4 to generate initial attribute
trees for each class with the category name, detailed Section 3.1.
2. Attribute Verification. To determine whether an attribute was present or absent in an image,
we employed an ensemble of predictions from multiple CLIP [53] models 5. We filtered the
top-5 attributes predicted by CLIP and sought human judgments to verify their correctness.
To streamline this process, we developed an annotation tool with a user interface, which is
illustrated in Figure 7.
3. Manual Verification. In this phase, annotators examined the accuracy of existing attributes
and introduced new relevant ones to enrich the concept trees. Subsequently, human annota-
tors conducted a thorough review, refinement, and systematic organization of the attribute
trees for each class."
REFERENCES,0.6221889055472264,Figure 7: Software tool interface for parse tree annotation.
REFERENCES,0.623688155922039,Table 8: Support set Dataset Statistics.
REFERENCES,0.6251874062968515,"Dataset Name
No. Categories
No. Attributes
No. Images"
REFERENCES,0.6266866566716641,"CIFAR-10 Support
10
289
14,024"
REFERENCES,0.6281859070464768,"CIFAR-100 Support
100
2,359
19,168"
REFERENCES,0.6296851574212894,"ImageNet Support
1,000
26,928
142,034"
REFERENCES,0.631184407796102,"D.2
Support data"
REFERENCES,0.6326836581709145,"Data Collection. Our LVX model utilizes a custom support set for each task, created through a reject
sampling method. Initially, images are generated using either Bing or the Stable Diffusion Model.
Subsequently, CLIP is applied to determine if the CLIP score exceeds 0.5, based on the raw cosine
similarities calculated by averaging CLIP ViT-B/32,ViT-B/16,ViT-B/14. If the score is above the
specified threshold, the image is retained; otherwise, it is discarded."
REFERENCES,0.6341829085457271,"To optimize the image collection process, we merge the retrieved images from all models, leading to
time and effort savings. This approach allows us to reuse an image already in the dataset if it matches
an attribute generated by the LLM. As a result, after the initial models have finished collecting data,
subsequent models can simply pull relevant images from this existing pool instead of collecting new
ones, saving both time and effort."
REFERENCES,0.6356821589205397,"4https://chat.openai.com/
5https://github.com/mlfoundations/open_clip"
REFERENCES,0.6371814092953523,"3
4
5
6
Tree Depth"
REFERENCES,0.638680659670165,4 √ó 100
REFERENCES,0.6401799100449775,5 √ó 100
REFERENCES,0.6416791604197901,6 √ó 100
REFERENCES,0.6431784107946027,Number of Class
REFERENCES,0.6446776611694153,(a) Histogram of Tree Depth.
REFERENCES,0.6461769115442278,"0
1
2
3
4
5
6
7
8
9
Category ID 0 5 10 15 20 25 30 35 40"
REFERENCES,0.6476761619190404,Attribute
REFERENCES,0.6491754122938531,(b) No. Attribute per Category.
REFERENCES,0.6506746626686657,"0
1
2
3
4
5
6
7
8
9
Category ID 0 250 500 750 1000 1250 1500 1750 2000"
REFERENCES,0.6521739130434783,Images Per Category
REFERENCES,0.6536731634182908,(c) No. Instance per Category.
REFERENCES,0.6551724137931034,"3
4
5
6
Tree Depth 100 101 102"
REFERENCES,0.656671664167916,Number of Class
REFERENCES,0.6581709145427287,(d) Histogram of Tree Depth.
REFERENCES,0.6596701649175413,"0
10
20
30
40
50
60
70
80
90
Category ID 0 10 20 30 40"
REFERENCES,0.6611694152923538,Attribute
REFERENCES,0.6626686656671664,(e) No. Attribute per Category.
REFERENCES,0.664167916041979,"0
10
20
30
40
50
60
70
80
90
Category ID 0 50 100 150 200 250 300 350 400"
REFERENCES,0.6656671664167916,Images Per Category
REFERENCES,0.6671664167916042,(f) No. Instance per Category.
REFERENCES,0.6686656671664168,"3
4
5
6
Tree Depth 101 102 103"
REFERENCES,0.6701649175412294,Number of Class
REFERENCES,0.671664167916042,(g) Histogram of Tree Depth.
REFERENCES,0.6731634182908546,"0
100
200
300
400
500
600
700
800
900
Category ID 0 20 40 60 80 100 120 140 160"
REFERENCES,0.6746626686656672,Attribute
REFERENCES,0.6761619190404797,(h) No. Attribute per Category.
REFERENCES,0.6776611694152923,"0
100
200
300
400
500
600
700
800
900
Category ID 0 100 200 300 400 500 600 700 800"
REFERENCES,0.679160419790105,Images Per Category
REFERENCES,0.6806596701649176,(i) No. Instance per Category.
REFERENCES,0.6821589205397302,"Figure 8: Statistics of the support dataset sets of (Row1) CIFAR10, (Row2) CIFAR100 and (Row3) Im-
ageNet. We examine the (a,d,g) Tree Depth, (b,e,h) Number of Attributes for each category and
(c,f,i) Number of image for each category to demonstrate the diversity of collected attributes and the
completeness of hierarchical annotations."
REFERENCES,0.6836581709145427,"Data Statistics. We present the statistics of the support datasets collected for CIFAR10, CIFAR100,
and ImageNet, highlighting the diversity and comprehensiveness of our dataset. Table 8 and Figure 8
provide an overview of these statistics. Specifically, we include the number of attributes, the number
of samples for each category, the total number of samples in the dataset, as well as the distribution
of tree depths. This rich collection of data showcases the diverse range of attributes and categories
covered in our dataset, making it a valuable resource for training and evaluation purposes."
REFERENCES,0.6851574212893553,"D.3
Limitations of Support Dataset Collection"
REFERENCES,0.6866566716641679,"While collecting a newly curated dataset can be advantageous for our tasks, it is important to
acknowledge certain limitations when using such datasets for explanation purposes. Three key
limitations arise: false positive simages, the presence of out-of-distribution samples and potential
bias in the dataset."
REFERENCES,0.6881559220389805,"‚Ä¢ False Positive Images. We observed that both Bing and the Stable Diffusion model
occasionally generate imperfect images from textual descriptions, manifesting incorrect or
entangled patterns. For instance, the word ‚Äúcrane‚Äù could represent either a construction
machine or a bird, leading to ambiguity. Furthermore, an image described as ‚Äúa dog with a
long tail‚Äù could potentially include not only the tail but also the head and legs, reflecting a
broader scope than intended."
REFERENCES,0.6896551724137931,"‚Ä¢ Out-of-Distribution Samples. Newly collected datasets may include samples that are
out-of-distribution, i.e., they do not align with the source data distribution of interest. These
out-of-distribution samples can introduce challenges in generating accurate and reliable
explanations. As a result, explanations based solely on a newly collected dataset may not
generalize well to unseen instances outside the support dataset distribution."
REFERENCES,0.6911544227886057,"‚Ä¢ Data Bias. Biases in the collection process or the underlying data sources can inadvertently
influence the dataset, leading to biased explanations. Biases emerge due to various factors,
such as data collection source, or imbalances in attribute distributions. Consequently, relying
solely on a newly collected dataset for explanations may introduce unintended biases into
the interpretation process."
REFERENCES,0.6926536731634183,"Solutions. To deal with mistakes in the gathered images, we used two approaches. First, we used the
CLIP model to sift through the images because it‚Äôs good at understanding how close an image is to
text concepts, helping us remove most mixed-up and incorrect images. Second, we manually sorted
out words that have more than one meaning. For instance, we made it clear whether ‚Äúcrane‚Äù refers to
the bird or the machine by labeling it as ‚Äúcrane (bird)‚Äù or ‚Äúcrane (machine)‚Äù."
REFERENCES,0.6941529235382309,"To mitigate the challenges posed by OOD samples and data bias, we adopt a cautious approach.
Specifically, we do not directly train our models on the newly collected support dataset. Instead, we
utilize this dataset solely for the purpose of providing disentangled attributes for explanations. By
decoupling the training data from the support data, we aim to reduce the impact of OOD samples and
potential data biases, thus promoting a more robust and unbiased analysis."
REFERENCES,0.6956521739130435,"Despite these limitations, our emphasis on obtaining a dataset with distinct attributes sharpens our
analysis and interpretation of model behavior. This approach allows us to extract meaningful insights
in a controlled and clear way."
REFERENCES,0.697151424287856,"E
User Study"
REFERENCES,0.6986506746626686,"The evaluation of visual decision-making and categorization can be uncertain and subjective, posing
challenges in assessing the quality of explanations. To address this, we conducted a user study to
verify the plausibility of our explanations."
REFERENCES,0.7001499250374813,"Experiment Setup. Our study compared the performance of LVX, with three others: Subtree,
TrDec, and a new baseline called Single. The key difference with the Single baseline is that it
utilizes only the nearest neighbor node from the parse tree for its output. This contrasts with LVX,
which employs the top-k neighbor nodes from the parse tree."
REFERENCES,0.7016491754122939,"We recruited 37 participants for this study. Each was asked to respond to 15 questions, each with
one image and 4 choices. In each question, they choose the explanation that best matched the image,
based on their personal judgment. The format for each choice was arranged as ‚ÄúThe image is a
<CLASS_NAME> because <EXPLANATION>.‚Äù."
REFERENCES,0.7031484257871065,"Results. The user study results, as shown in Table 9, clearly indicate the superiority of the LVX
method. It was selected by participants 57.66% of the time, a significantly higher rate compared to
the other methods included in the study."
REFERENCES,0.704647676161919,"Table 9: User Study Results.
Method
Choice Percentage"
REFERENCES,0.7061469265367316,"Subtree
3.78%
TrDec
22.88%
Single
15.68%"
REFERENCES,0.7076461769115442,"LVX
57.66%"
REFERENCES,0.7091454272863568,"F
Experiments on Out-of-Distribution (OOD) Evaluation"
REFERENCES,0.7106446776611695,"In this section, we evaluate our calibrated model‚Äôs performance in Out-of-Distribution (OOD)
scenarios, focusing on its robustness and ability to generalize. This evaluation is conducted using a
ResNet50 and ViT-S trained on ImageNet, with and without calibration training, and tested on the
ImageNet-A and ImageNet-Sketch datasets."
REFERENCES,0.712143928035982,"Results. The results of OOD generalization, quantified by Top-1 Accuracy, are listed in Table 10. For
both ResNet-50 and ViT-S 16 models, we notice significant improvements in accuracy in ImageNet-A
and ImageNet-S compared to the baselines. This enhancement in Out-of-Distribution generalization
confirms the effectiveness of model calibration in not only improving in-domain performance (shown
in Figure 7 of the main paper) but also in boosting adaptability and robustness to out-of-domain data."
REFERENCES,0.7136431784107946,Table 10: OOD Generalization Results with and without calibration by Top-1 Accuracy.
REFERENCES,0.7151424287856072,"Model
ImageNet (In-Domain)
ImageNet-A
ImageNet-S"
REFERENCES,0.7166416791604198,"Baseline/Calibrated
Baseline/Calibrated
Baseline/Calibrated"
REFERENCES,0.7181409295352323,"ResNet-50
76.13/76.54(+0.41)
18.96/23.32(+4.36)
24.10/31.42(+7.32)"
REFERENCES,0.719640179910045,"ViT-S 16
77.85/78.21(+0.36)
13.39/18.72(+5.33)
32.40/37.21(+4.81)"
REFERENCES,0.7211394302848576,"G
Experiments on Chest X-Ray Diagnosis"
REFERENCES,0.7226386806596702,"In this section, we evaluate the performance of our LVX method in security-critical domains, specifi-
cally medical image analysis. We train neural networks for chest X-ray diagnosis and utilize LVX to
interpret and calibrate the predictions."
REFERENCES,0.7241379310344828,"We adopted the DenseNet-121 architecture for disease diagnosis in our study. The model was trained
on the Chestx-ray14 dataset [78], which consists of chest X-ray images encompassing 14 diseases,
along with an additional ‚ÄúNo Finding‚Äù class. The DenseNet-121 architecture is specifically designed
to generate 14 output logits corresponding to the different diseases. During training, we employed a
weighted binary cross-entropy loss [78] for each disease category to optimize the model."
REFERENCES,0.7256371814092953,Figure 9: Explanation performance comparison on Chestx-ray14 dataset.
REFERENCES,0.7271364317841079,"Ground-Truth 
Cardiomegaly"
REFERENCES,0.7286356821589205,"Pred: 
Cardiom egaly"
REFERENCES,0.7301349325337332,Substances
REFERENCES,0.7316341829085458,Environment
REFERENCES,0.7331334332833583,Attributes
REFERENCES,0.7346326836581709,Chambers
REFERENCES,0.7361319340329835,Cardiac
REFERENCES,0.7376311844077961,"Arteries 
and Veins"
REFERENCES,0.7391304347826086,Expansion ‚àö ‚àö
REFERENCES,0.7406296851574213,Cardiovascular
REFERENCES,0.7421289355322339,System
REFERENCES,0.7436281859070465,Dilation‚àö
REFERENCES,0.7451274362818591,"‚àö
Changes ‚àö"
REFERENCES,0.7466266866566716,"Blood 
vessels‚àö ‚àö"
REFERENCES,0.7481259370314842,Ground-Truth
REFERENCES,0.7496251874062968,Effusion
REFERENCES,0.7511244377811095,"Pred: 
Effusion"
REFERENCES,0.7526236881559221,Substance
REFERENCES,0.7541229385307346,Attribute Lung Fluid ‚àö
REFERENCES,0.7556221889055472,"‚àö
Lung Injury"
REFERENCES,0.7571214392803598,Function‚àö
REFERENCES,0.7586206896551724,"Pleural 
Space ‚àö Fluid"
REFERENCES,0.760119940029985,"Accumulation ‚àö ‚àö
‚àö"
REFERENCES,0.7616191904047976,Causes
REFERENCES,0.7631184407796102,Chambers‚àö ‚àö
REFERENCES,0.7646176911544228,Visible
REFERENCES,0.7661169415292354,"Space 
between 
the pleural 
layers ‚àö ‚àö"
REFERENCES,0.767616191904048,Figure 10: Explanation examples for the chest xray diagnosis task.
REFERENCES,0.7691154422788605,"For optimization, we utilized the Adam optimizer [36] with an initial learning rate of 1e-4, a weight
decay of 1e-5, and a batch size of 32. The model underwent training for a total of 18 epochs."
REFERENCES,0.7706146926536732,"Model Explanation. To enhance interpretability, we incorporated our LVX framework into the model.
Instead of acquiring images from online sources, we gathered the support set directly from the
training data. To accomplish this, we utilized a parse tree generated by the ChatGPT language model.
Leveraging this parse tree, we applied a MedCLIP [80] model to retrieve the most relevant images
for each attribute from the training set. These retrieved images served as our support sets for the LVX
framework."
REFERENCES,0.7721139430284858,"Compared to applying the LVX framework on single-label classification, the Chestx-ray14 dataset
poses a multi-label classification challenge. In this dataset, each sample can belong to multiple
disease categories simultaneously. Therefore, we modified the LVX framework to accommodate and
handle the multi-label nature of the classification task."
REFERENCES,0.7736131934032984,"Specifically, for each input image x, we predict its label ÀÜy = f(x) ‚àà{0, 1}14. To create the visual
parse tree, we begin by establishing the root node. If all elements of ÀÜy are 0, the root node is set
to ‚ÄúNo Findings‚Äù. Conversely, if any element of ÀÜy is non-zero, the root node is labeled as ‚Äúhas
Findings‚Äù. For each positive finding, we construct a separate parse tree, with these sub-trees"
REFERENCES,0.775112443778111,"becoming the children nodes of the root. By combining these sub-trees, we obtain a comprehensive
and coherent explanation for the image. This modification enables us to effectively handle the
multi-label nature of the classification task, providing meaningful and interpretable explanations for
images with multiple positive findings."
REFERENCES,0.7766116941529235,"To establish the ground-truth explanation label, we adopt a MedCLIP [80] model to filter the top-5
attributes for each positive finding of the image. These attributes are then organized into a tree
structure. This approach serves as an automatic explanation ground-truth, thereby eliminating the
requirement for manual annotations from domain experts."
REFERENCES,0.7781109445277361,"In addition to providing explanations, we aim to calibrate the model predictions with the parse tree.
To achieve this, we apply a modified hierarchical contrastive loss individually on each finding. We
then calculate the average of these losses, which serves as our overall loss term. We thus fine-tune the
model for 3 epochs using the hieracical term and weighted cross-entropy."
REFERENCES,0.7796101949025487,"Explanation Results. We compare the explanation performance of our proposed LVX against
the Random and Constant baselines. The numerical results, depicted in Figure 9, highlight the
superiority of our LVX approach."
REFERENCES,0.7811094452773614,"Additionally, we showcase the parsed visual tree in Figure 10, to provide a clearer illistration of our
results. Notably, our approach effectively interprets the decision-making process of black-box neural
networks. For instance, in the case on the right, our method accurately identifies the presence of
visible fluid in the lung space and establishes its relevance to the model‚Äôs prediction. Consequently,
LVX enables clinical professionals to make well-informed justifications for their patients, enhancing
the overall decision-making process."
REFERENCES,0.782608695652174,"Calibration Results. Table 11 presents the comparison between the baseline models and the model
with calibration, measured in terms of the Area Under the Curve (AUC) score for each disease
type. The AUC score provides a measure of the model‚Äôs ability to discriminate between positive and
negative cases."
REFERENCES,0.7841079460269865,"The calibrated model shows notable improvements in several disease types compared to the baseline.
Notably, Hernia demonstrates the most significant improvement, with an AUC score of 0.936
compared to 0.914 for the baseline. This indicates that the calibration process has enhanced the
model‚Äôs ability to accurately detect Hernia cases."
REFERENCES,0.7856071964017991,"In summary, the LVX method markedly improves model accuracy, demonstrated by enhanced cali-
bration performance across different disease types. The integration of visual attributes boosts both
accuracy and reliability of predictions, leading to better diagnostic results. These findings underscore
the LVX method‚Äôs potential to elevate model performance, particularly in medical diagnostics."
REFERENCES,0.7871064467766117,"Table 11: Model performance with and without calibration. AUC scores are reported for each disease
type. Avg. indicates the average score."
REFERENCES,0.7886056971514243,"Finding
Baseline
LVX"
REFERENCES,0.7901049475262368,"Atelectasis
0.767
0.779
Consolidation
0.747
0.755
Infiltration
0.683
0.698
Pneumothorax
0.865
0.873
Edema
0.845
0.851
Emphysema
0.919
0.930
Fibrosis
0.832
0.830
Effusion
0.826
0.831
Pneumonia
0.721
0.719
Pleural Thickening
0.784
0.793
Cardiomegaly
0.890
0.894
Nodule
0.758
0.776
Mass
0.814
0.830
Hernia
0.914
0.936"
REFERENCES,0.7916041979010495,"Avg.
0.812
0.821"
REFERENCES,0.7931034482758621,"H
Experiments on Self-supervised models"
REFERENCES,0.7946026986506747,"In this section, we focus on self-supervised models to assess their interpretability. Differing from
supervised models, self-supervised models develop representations without labeled data. Our aim
is to understand the interpretability of these representations and uncover the underlying structures
derived from the input data alone."
REFERENCES,0.7961019490254873,"Model to be Explained. Our objective is to offer a comprehensive explanation for self-supervised
models trained on ImageNet-1k. These models include ResNet50 trained using SimCLR [11],
BYOL [24], SwAV [8], MoCov3 [12], DINO [9], and ViT-S trained with MoCov3 [13] and DINO [9].
The networks are subsequently fine-tuned through linear probing while keeping the backbone fixed.
We then utilize our LVX to provide explanations for their predictions. Additionally, we compare these
self-supervised models with their supervised counterparts to highlight the differences in representation
between the two approaches."
REFERENCES,0.7976011994002998,"Numerical Results. Table 12 presents the results of self-supervised models. Our analysis reveals a
strong correlation between the explanatory performance and the overall model accuracy."
REFERENCES,0.7991004497751124,"However, we also noticed that self-supervised models based on transformer architecture exhibit
greater attribute disentanglement compared to supervised models, despite potentially having slightly
lower performance. This phenomenon is evident when comparing the DINO ViT-S/16 model and
the supervised ViT-S/16 model within the context of tree parsing explanation. Although the DINO
ViT-S/16 model shows slightly lower overall performance, it outperforms the supervised model in
terms of providing accurate attribute explanation."
REFERENCES,0.800599700149925,"These results underscore the potential benefits of self-supervised learning in uncovering meaningful
visual attributes without explicit supervision. While self-supervised models may exhibit marginally
lower performance on certain tasks, their ability to capture rich visual representations and attribute
disentanglement highlights their value in understanding complex visual data."
REFERENCES,0.8020989505247377,Table 12: Explanation performance analysis of self-supervised models utilizing linear probing.
REFERENCES,0.8035982008995503,"Method
Top-1 Acc
TED‚Üì
MCS‚Üë
TK ‚Üë"
REFERENCES,0.8050974512743628,"ResNet50-SimCLR
69.2
9.38
23.72
46.53
ResNet50-BYOL
71.8
9.29
24.66
48.29
ResNet50-MoCov3
74.6
9.19
25.59
50.17
ResNet50-DINO
75.3
9.14
25.77
50.71
ResNet50-SwAV
75.3
9.15
25.82
50.69
ResNet50-Sup
76.1
9.09
25.99
51.19"
REFERENCES,0.8065967016491754,"ViT-S/16-MoCov3
73.2
9.16
25.25
49.40
ViT-S/16-DINO
77.0
8.99
26.61
52.05
ViT-S/8-DINO
79.7
8.89
27.62
53.95
ViT-S/16-Sup
77.9
9.10
25.73
50.34"
REFERENCES,0.808095952023988,"I
Raw Results"
REFERENCES,0.8095952023988006,"This section presents the raw numerical results for Figure 5, as depicted in the main paper. Specifically,
Table 13 provides the results for CIFAR-10, Table 14 for CIFAR-100, and Table 15 for ImageNet.
We also observed that larger networks within the same model family deliver better results. As the
models improve, so does the accuracy of the explanations, suggesting that larger networks facilitate
more effective explanations. This is demonstrated by the increase in MCS and TK scores as ResNet
deepens on CIFAR-100 and ImageNet, aligning with the general belief that larger neural networks
offer enhanced generalization and structural representation capabilities."
REFERENCES,0.8110944527736131,"J
Experimental Setup"
REFERENCES,0.8125937031484258,"In this section, we provide detailed information about our experimental setup to ensure the repro-
ducibility of our method."
REFERENCES,0.8140929535232384,Table 13: Explanation performance comparison on CIFAR-10.
REFERENCES,0.815592203898051,"Model
TED‚Üì
MCS‚Üë
Tree Kernel‚Üë
rand.
const.
LVX
rand.
const.
LVX
rand.
const.
LVX"
REFERENCES,0.8170914542728636,"VGG13
9.21
32.97
8.21
28.98
18.77
32.31
59.49
58.41
63.43
VGG16
9.23
32.89
8.14
29.11
19.11
32.55
59.34
59.55
63.79
VGG19
9.15
32.85
8.15
30.67
19.10
31.78
59.44
59.39
63.32
ResNet18
9.21
32.90
8.52
28.95
18.92
30.24
58.94
58.87
61.19
ResNet34
9.21
32.92
8.16
28.95
18.98
32.07
59.06
59.01
63.27
ResNet50
9.21
32.89
8.44
28.96
19.00
31.09
59.16
59.21
62.06
DenseNet121
9.19
32.89
8.20
29.09
19.11
32.13
59.53
59.48
63.44
DenseNet161
9.20
32.88
8.19
29.07
19.11
32.12
59.35
59.48
63.73
DenseNet169
9.21
32.88
8.18
29.25
19.08
32.13
59.52
59.46
63.46
MobileNet_v2
9.20
32.89
8.41
29.24
19.09
31.61
59.53
59.38
61.87
GoogLeNet
9.23
32.96
8.41
28.66
18.86
30.75
58.62
58.71
61.38
Inception_v3
9.20
32.89
8.39
29.19
19.03
31.02
59.37
59.27
61.85"
REFERENCES,0.8185907046476761,Table 14: Explanation performance comparison on CIFAR-100.
REFERENCES,0.8200899550224887,"Model
TED‚Üì
MCS‚Üë
Tree Kernel‚Üë
rand.
const.
LVX
rand.
const.
LVX
rand.
const.
LVX"
REFERENCES,0.8215892053973014,"ResNet20
9.61
28.77
8.96
22.65
17.60
24.89
45.52
46.96
47.70
ResNet32
9.57
28.67
8.86
22.84
17.92
25.39
46.45
47.87
48.58
ResNet44
9.54
28.60
8.81
23.48
18.34
25.97
47.42
48.87
49.79
ResNet56
9.52
28.54
8.83
23.87
18.60
26.47
48.04
49.58
50.17
MBNv2-x0.5
9.55
28.58
8.87
23.43
18.19
25.50
47.13
48.58
49.08
MBNv2-x0.75
9.43
28.43
8.76
24.47
18.87
26.71
49.19
50.55
51.21
MBNv2-x1.0
9.48
28.35
8.73
24.35
19.02
27.09
49.27
50.68
51.47
MBNv2-x1.4
9.43
28.16
8.65
24.87
19.47
27.41
50.52
52.08
52.91
RepVGG A0
9.44
28.28
8.74
24.65
19.21
26.84
49.78
51.37
52.01
RepVGG A1
9.42
28.21
8.72
25.27
19.59
27.43
50.63
52.17
52.81
RepVGG A2
9.40
28.08
8.70
25.46
19.82
27.99
51.26
52.87
53.04"
REFERENCES,0.823088455772114,"J.1
Evaluation Metrics"
REFERENCES,0.8245877061469266,"To evaluate the effectiveness of our proposed tree parsing task, we have developed three metrics that
leverage conventional tree similarity and distance measurement techniques."
REFERENCES,0.8260869565217391,"‚Ä¢ Tree Kernels (TK): Tree Kernels (TK) evaluate tree similarity by leveraging shared sub-
structures, assigning higher scores to trees with common subtrees or substructures. To
enhance the match, we set the decaying factor for two adjacent tree layers to 0.5, where
larger values lead to better matches. Let‚Äôs define the subtree kernel mathematically:"
REFERENCES,0.8275862068965517,Table 15: Explanation performance comparison on ImageNet.
REFERENCES,0.8290854572713643,"Model
TED‚Üì
MCS‚Üë
Tree Kernel‚Üë
rand.
const.
LVX
rand.
const.
LVX
rand.
const.
LVX"
REFERENCES,0.8305847076461769,"ResNet18
9.83
34.15
9.30
22.52
16.82
23.87
45.32
44.85
46.85
ResNet34
9.75
33.78
9.17
23.74
17.71
25.09
47.66
47.16
49.24
ResNet50
9.68
33.58
9.09
24.59
18.35
25.99
49.38
48.97
51.19
ResNet101
9.64
33.48
9.04
24.94
18.66
26.51
50.25
49.77
51.99
ViT-T16
10.42
35.44
9.99
15.07
11.25
15.91
30.30
29.92
31.24
ViT-S16
9.69
33.61
9.10
24.16
18.01
25.73
48.53
48.05
50.34
ViT-B16
9.62
33.37
8.99
25.20
18.79
27.01
50.64
50.22
52.76
ViT-L16
9.45
32.84
8.79
27.27
20.35
29.29
54.83
54.36
57.14"
REFERENCES,0.8320839580209896,"Tree Kernel: Given two trees, T1 and T2, represented as rooted, labeled, and
ordered trees, we define the subtree kernel as follows:"
REFERENCES,0.8335832083958021,"TK(T1, T2) =
X T X"
REFERENCES,0.8350824587706147,"T ‚Ä≤
Œ∏(T, T ‚Ä≤) √ó Œ∏(T, T ‚Ä≤) √ó Œªmax(depth(r),depth(r‚Ä≤))"
REFERENCES,0.8365817091454273,"Let TK(T1, T2) denote the similarity between subtrees T1 and T2 using the
subtree kernel. Additionally, let Œ∏(T, T ‚Ä≤) represent the count of shared common
subtrees between trees T and T ‚Ä≤. Furthermore, let r and r‚Ä≤ be the roots of T and
T ‚Ä≤ respectively. Œª < 1.0 is the decaying factor that make sure the tree closer to
the root hold greater significance.
The Œ∏(T, T ‚Ä≤) is computed recursively as follows:"
REFERENCES,0.8380809595202399,"1. If both T and T ‚Ä≤ are leaf nodes, then Œ∏(T, T ‚Ä≤) = 1 if the labels of T and T ‚Ä≤
are the same, and 0 otherwise.
2. If either T or T ‚Ä≤ is a leaf node, then Œ∏(T, T ‚Ä≤) = 0.
3. Otherwise,
let {T1, T2, . . . , Tn} be the child subtrees of T,
and
{T ‚Ä≤
1, T ‚Ä≤
2, . . . , T ‚Ä≤
n‚Ä≤} be the child subtrees of T ‚Ä≤.
‚Äì If the labels of r and r‚Ä≤ are the same, then Œ∏(T, T ‚Ä≤) is the sum of the
products of Œ∏(Ti, T ‚Ä≤
j) for all combinations of i and j, where i ranges
from 1 to n and j ranges from 1 to n‚Ä≤.
‚Äì If the labels of r and r‚Ä≤ are different, then Œ∏(T, T ‚Ä≤) is 0.
‚Äì Additionally, if T and T ‚Ä≤ are isomorphic (have the same structure), then
Œ∏(T, T ‚Ä≤) is incremented by 1."
REFERENCES,0.8395802098950524,"In the paper, the Tree Kernel (TK) score is normalized to accommodate trees of different
sizes. The normalized TK score is computed as:
T K(Tpred,Tgt)√ó100
‚àö"
REFERENCES,0.841079460269865,"T K(Tpred,Tpred)T K(Tgt,Tgt). The kernel"
REFERENCES,0.8425787106446777,"value serves as a measure of similarity, where higher values indicate greater similarity."
REFERENCES,0.8440779610194903,"‚Ä¢ Maximum Common Subgraph (MCS)[54, 33]: The Maximum Common Subgraph (MCS)
identifies the largest shared subgraph between two trees, measuring the similarity and
overlap of their hierarchical structures. Here‚Äôs the mathematical definition of the Maximum
Common Subgraph:"
REFERENCES,0.8455772113943029,"Maximum Common Subgraph: Given two trees, T1 = (V1, E1) and T2 =
(V2, E2), where V1 and V2 are the sets of vertices and E1 and E2 are the sets of
edges for each graph, respectively, we define the Maximum Common Subgraph
as:"
REFERENCES,0.8470764617691154,"MCS(T1, T2) = (VMCS, EMCS)
maximize
|VMCS|
subject to
VMCS ‚äÜV 1
and
VMCS ‚äÜV 2
EMCS ‚äÜE1
and
EMCS ‚äÜE2
For any pair of vertices u, v in VMCS, if (u, v) is an edge in EMCS,
then (u, v) is an edge in T1 and T2, and vice versa."
REFERENCES,0.848575712143928,"In our paper, we report the normalized MCS score as our measurement of tree similarity
|MCS(Tpred,Tgt)|√ó100
‚àö"
REFERENCES,0.8500749625187406,"|Tpred||Tgt|
, where a higher score indicates greater similarity between the graphs."
REFERENCES,0.8515742128935532,"Here, | ¬∑ | represents the number of nodes in a tree. We employ this normalization to address
the scenario where one tree is significantly larger and encompasses all other trees as subtrees.
By dividing the MCS score by the square root of the product of the numbers of nodes in
the predicted tree (Tpred) and the ground truth tree (Tgt), we ensure a fair comparison across
trees of varying sizes."
REFERENCES,0.8530734632683659,"‚Ä¢ Tree Edit Distance (TED) [6]: The Tree Edit Distance (TED) quantifies the minimum
number of editing operations required to transform one hierarchical tree into another. It mea-
sures the structural dissimilarity between trees by considering node and edge modifications,
insertions, and deletions. With smaller TED, the two graphs are more similar. Let‚Äôs define
the Tree Edit Distance formally:"
REFERENCES,0.8545727136431784,"Tree Edit Distance: Given two trees T1 and T2, represented as rooted, la-
beled, and ordered trees, we define the Tree Edit Distance between them as
TED(T1, T2).
The TED(T1, T2) is computed recursively as follows:"
REFERENCES,0.856071964017991,"‚Äì If both T1 and T2 are empty trees, i.e., they do not have any nodes, then
TED(T1, T2) = 0.
‚Äì If either T1 or T2 is an empty tree, i.e., it does not have any nodes, then
TED(T1, T2) is the number of nodes in the non-empty tree.
‚Äì Otherwise, let r1 and r2 be the roots of T1 and T2, respectively. Let T
‚Ä≤
1 and
T
‚Ä≤
2 be the subtrees obtained by removing the roots r1 and r2 from T1 and
T2, respectively."
REFERENCES,0.8575712143928036,"* If r1 = r2, then TED(T1, T2) is the minimum among the following
values:"
REFERENCES,0.8590704647676162,"¬∑ TED(T
‚Ä≤
1, T
‚Ä≤
2)
+
TED(children of r1, children of r2),
where
TED(children of r1, children of r2) is the TED computed recur-
sively between the children of r1 and r2.
¬∑ 1 + TED(T
‚Ä≤
1, T
‚Ä≤
2), which represents the cost of deleting the root r1
and recursively computing TED between T
‚Ä≤
1 and T
‚Ä≤
2.
¬∑ 1 + TED(T1, T
‚Ä≤
2), which represents the cost of inserting the root r2
into T1 and recursively computing TED between T1 and T
‚Ä≤
2.
¬∑ 1 + TED(T
‚Ä≤
1, T2), which represents the cost of deleting the root r1
and inserting the root r2 into T2, and recursively computing TED
between T
‚Ä≤
1 and T2.
* If r1 Ã∏= r2, then TED(T1, T2) is the minimum among the following
values:"
REFERENCES,0.8605697151424287,"¬∑ TED(T
‚Ä≤
1, T
‚Ä≤
2)
+
TED(children of r1, children of r2),
where
TED(children of r1, children of r2) is the TED computed recur-
sively between the children of r1 and r2.
¬∑ 1 + TED(T
‚Ä≤
1, T2), which represents the cost of deleting the root r1
and recursively computing TED between T
‚Ä≤
1 and T2.
¬∑ 1 + TED(T1, T
‚Ä≤
2), which represents the cost of inserting the root r2
into T1 and recursively computing TED between T1 and T
‚Ä≤
2."
REFERENCES,0.8620689655172413,"The TED(T1, T2) is the final result obtained after applying the above recursive computation."
REFERENCES,0.863568215892054,"J.2
Model Checkpoints"
REFERENCES,0.8650674662668666,"For our experiments, we utilize publicly available pre-trained models. Specifically, we employ CI-
FAR10 models from https://github.com/huyvnphan/PyTorch_CIFAR10, CIFAR100 models
from https://github.com/chenyaofo/pytorch-cifar-models, and ImageNet models from
torchvision package and timm package. The self-supervised models are downloaded from their
respective official repositories."
REFERENCES,0.8665667166416792,"J.3
Explanation Baselines"
REFERENCES,0.8680659670164917,"To explain visual models using tree-structured language without annotations, we devised four basic
baselines, Random, Constant, Subtree and TrDec, for comparison with our LVX method."
REFERENCES,0.8695652173913043,"Random Baseline. The Random baseline generates random explanations by predicting an image‚Äôs
category and randomly sampling 5 nodes from its category-specific tree. The connected nodes form a
tree-structured explanation, providing a performance baseline for random guessing."
REFERENCES,0.8710644677661169,"Constant Baseline. The Constant baseline Produces a fixed tree-structured clue for images of the
same class, using an initial explanatory tree T (0)
i
as a template. This baseline assesses LVX against a
non-adaptive, static approach."
REFERENCES,0.8725637181409296,"Subtree Baseline. This method involves selecting the most common subtree from the test set for
explanations, testing the efficacy of using frequent dataset patterns for generic explanations."
REFERENCES,0.8740629685157422,"TreDec Baseline. Based on [79], the TrDec strategy implements a tree-topology RNN decoder over
an image encoder. In the absence of hierarchical annotations, this baseline uses the CLIP model to
verify nodes in the template trees, which act as pseudo-labels for training. This method focuses on
the effectiveness of a structured decoding process in explanation generation."
REFERENCES,0.8755622188905547,"This comparison demonstrates the effectiveness of LVX in creating explanations tailored to individual
image content, clearly outperforming methods based on random guessing, static templates, and basic
learning-based approaches."
REFERENCES,0.8770614692653673,"K
Limitations"
REFERENCES,0.8785607196401799,"Our system, LVX, depends on an external Large Language Model (LLM) to provide textual explana-
tions. While this integration adds significant functionality, it also introduces the risk of inaccuracies.
The LLM may not always deliver correct information, leading to potential misinformation or erro-
neous explanations."
REFERENCES,0.8800599700149925,"Additionally, our approach involves generating explanations based on the last embedding layer of
the neural network. This method overlooks the comprehensive, multi-level hierarchical structure of
deep features, potentially simplifying or omitting important contextual data that could enhance the
understanding of the network‚Äôs decisions."
REFERENCES,0.881559220389805,NeurIPS Paper Checklist
REFERENCES,0.8830584707646177,"The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and precede the (optional) supplemental material. The checklist does NOT
count towards the page limit."
REFERENCES,0.8845577211394303,"Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:"
REFERENCES,0.8860569715142429,"‚Ä¢ You should answer [Yes] , [No] , or [NA] .
‚Ä¢ [NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
‚Ä¢ Please provide a short (1‚Äì2 sentence) justification right after your answer (even for NA)."
REFERENCES,0.8875562218890555,"The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper."
REFERENCES,0.889055472263868,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
proper justification is given (e.g., ""error bars are not reported because it would be too computationally
expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found."
REFERENCES,0.8905547226386806,"IMPORTANT, please:"
REFERENCES,0.8920539730134932,"‚Ä¢ Delete this instruction block, but keep the section heading ‚ÄúNeurIPS paper checklist"",
‚Ä¢ Keep the checklist subsection headings, questions/answers and guidelines below.
‚Ä¢ Do not modify the questions and only use the provided macros for your answers."
CLAIMS,0.8935532233883059,1. Claims
CLAIMS,0.8950524737631185,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: we present LVX to explain vision model with LLM‚Äôs help.
Guidelines:"
CLAIMS,0.896551724137931,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.8980509745127436,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In the limitation section."
CLAIMS,0.8995502248875562,Guidelines:
CLAIMS,0.9010494752623688,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9025487256371814,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.904047976011994,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9055472263868066,Answer: [No]
THEORY ASSUMPTIONS AND PROOFS,0.9070464767616192,Justification: No proof made.
THEORY ASSUMPTIONS AND PROOFS,0.9085457271364318,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.9100449775112444,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9115442278860569,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9130434782608695,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9145427286356822,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9160419790104948,Justification: Details and code provided.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9175412293853074,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9190404797601199,‚Ä¢ The answer NA means that the paper does not include experiments.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9205397301349325,"‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9220389805097451,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9235382308845578,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9250374812593704,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9265367316341829,Justification: Code has been uploaded as supplementary material. Data will be available.
OPEN ACCESS TO DATA AND CODE,0.9280359820089955,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9295352323838081,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable)."
OPEN ACCESS TO DATA AND CODE,0.9310344827586207,"‚Ä¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9325337331334332,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9340329835082459,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9355322338830585,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9370314842578711,Justification: All details mentioned.
OPEN ACCESS TO DATA AND CODE,0.9385307346326837,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9400299850074962,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9415292353823088,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9430284857571214,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9445277361319341,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9460269865067467,Justification: Errorbar presented.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9475262368815592,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9490254872563718,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9505247376311844,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.952023988005997,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9535232383808095,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9550224887556222,Justification: All discussed in the main paper.
EXPERIMENTS COMPUTE RESOURCES,0.9565217391304348,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9580209895052474,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage."
EXPERIMENTS COMPUTE RESOURCES,0.95952023988006,"‚Ä¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper).
9. Code Of Ethics"
EXPERIMENTS COMPUTE RESOURCES,0.9610194902548725,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification:
Guidelines:"
EXPERIMENTS COMPUTE RESOURCES,0.9625187406296851,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
EXPERIMENTS COMPUTE RESOURCES,0.9640179910044977,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Error bar included.
Guidelines:"
EXPERIMENTS COMPUTE RESOURCES,0.9655172413793104,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
EXPERIMENTS COMPUTE RESOURCES,0.967016491754123,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [Yes]
Justification: We provide new annotation for an existing dataset.
Guidelines:"
EXPERIMENTS COMPUTE RESOURCES,0.9685157421289355,‚Ä¢ The answer NA means that the paper poses no such risks.
EXPERIMENTS COMPUTE RESOURCES,0.9700149925037481,"‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9715142428785607,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9730134932533733,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.974512743628186,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9760119940029985,"Justification: Yes, all properly credited."
LICENSES FOR EXISTING ASSETS,0.9775112443778111,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9790104947526237,"‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators."
NEW ASSETS,0.9805097451274363,13. New Assets
NEW ASSETS,0.9820089955022488,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9835082458770614,Answer: [Yes]
NEW ASSETS,0.9850074962518741,Justification: Code and data are well documented.
NEW ASSETS,0.9865067466266867,Guidelines:
NEW ASSETS,0.9880059970014993,"‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895052473763118,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910044977511244,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992503748125937,Answer: [No]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940029985007496,Justification: Not applicable
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955022488755623,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970014992503748,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [No]
Justification: Not applicable
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9985007496251874,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
