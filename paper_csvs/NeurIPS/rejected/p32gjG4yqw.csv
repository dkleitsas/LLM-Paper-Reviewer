Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010582010582010583,"We present a unified constructive universal approximation theorem covering a wide
1"
ABSTRACT,0.0021164021164021165,"range of learning machines including both shallow and deep neural networks based
2"
ABSTRACT,0.0031746031746031746,"on the group representation theory. Constructive here means that the distribution
3"
ABSTRACT,0.004232804232804233,"of parameters is given in a closed-form expression (called the ridgelet transform).
4"
ABSTRACT,0.005291005291005291,"Contrary to the case of shallow models, expressive power analysis of deep models
5"
ABSTRACT,0.006349206349206349,"has been conducted in a case-by-case manner. Recently, Sonoda et al. [33, 32]
6"
ABSTRACT,0.007407407407407408,"developed a systematic method to show a constructive approximation theorem
7"
ABSTRACT,0.008465608465608466,"from scalar-valued joint-group-invariant feature maps, covering a formal deep
8"
ABSTRACT,0.009523809523809525,"network. However, each hidden layer was formalized as an abstract group action, so
9"
ABSTRACT,0.010582010582010581,"it was not possible to cover real deep networks defined by composites of nonlinear
10"
ABSTRACT,0.01164021164021164,"activation function. In this study, we extend the method for vector-valued joint-
11"
ABSTRACT,0.012698412698412698,"group-equivariant feature maps, so to cover such real networks.
12"
INTRODUCTION,0.013756613756613757,"1
Introduction
13"
INTRODUCTION,0.014814814814814815,"An ultimate goal of the deep learning theory is to characterize the internal data processing procedure
14"
INTRODUCTION,0.015873015873015872,"inside deep neural networks obtained by deep learning. We may formulate this problem as a functional
15"
INTRODUCTION,0.016931216931216932,"equation problem: Let F denote a class of data generating functions, and let DNN[γ] denote a certain
16"
INTRODUCTION,0.01798941798941799,"deep neural network with parameter γ. Given a function f ∈F, find an unknown parameter γ so that
17"
INTRODUCTION,0.01904761904761905,"network DNN[γ] represents function f, i.e.
18"
INTRODUCTION,0.020105820105820106,"DNN[γ] = f,
(1)"
INTRODUCTION,0.021164021164021163,"which we call a DNN equation. An ordinary learning problem by empirical risk minimization, such
19"
INTRODUCTION,0.022222222222222223,"as minimizing Pn
i=1 |DNN[γ](xi) −f(xi)|2 with respect to γ, is understood as a weak form (or a
20"
INTRODUCTION,0.02328042328042328,"variational form) of this equation. Therefore, characterizing the solution space of this equation leads
21"
INTRODUCTION,0.02433862433862434,"to understanding the parameters obtained by deep learning. Following previous studies [21, 3, 28–
22"
INTRODUCTION,0.025396825396825397,"31], we call a solution operator R that satisfies DNN[R[f]] = f a ridgelet transform. Once such a
23"
INTRODUCTION,0.026455026455026454,"solution operator R is found, we can conclude a universality of the DNN in consideration because the
24"
INTRODUCTION,0.027513227513227514,"reconstruction formula DNN[R[f]] = f implies for any f ∈F there exists a DNN that represents f.
25"
INTRODUCTION,0.02857142857142857,"In particular, when R[f] is found in a closed-form manner, then it leads to a constructive proof of the
26"
INTRODUCTION,0.02962962962962963,"universality since R[f] could indicate how to assign parameters.
27"
INTRODUCTION,0.030687830687830688,"When the network has only one infinitely-wide hidden layer, though it is not deep but shallow, the
28"
INTRODUCTION,0.031746031746031744,"characterization problem has been well investigated. For example, the learning dynamics and the
29"
INTRODUCTION,0.0328042328042328,"global convergence property (of SGD) are well studied in the mean field theory [22, 25, 20, 5] and the
30"
INTRODUCTION,0.033862433862433865,"Langevin dynamics theory [35], and even closed-form solution operator to a “shallow” NN equation,
31"
INTRODUCTION,0.03492063492063492,"the original ridgelet transform, has already been presented [28–31].
32"
INTRODUCTION,0.03597883597883598,"On the other hand, when the network has more than one hidden layer, the problem is far from
33"
INTRODUCTION,0.037037037037037035,"solved, and it is common to either consider infinitely-deep mathematical models such as Neural
34"
INTRODUCTION,0.0380952380952381,"ODEs [27, 9, 17, 12, 4], or handcraft inner feature maps depending on the problem. For example,
35"
INTRODUCTION,0.039153439153439155,"construction methods such as the Telgarsky sawtooth function (or the Yarotsky scheme) and bit
36"
INTRODUCTION,0.04021164021164021,"extraction techniques [7, 36–39, 8, 6, 26, 24, 11] have been developed to demonstrate the depth
37"
INTRODUCTION,0.04126984126984127,"separation, super-convergence, and minmax optimality of deep ReLU networks. Various feature maps
38"
INTRODUCTION,0.042328042328042326,"have also been handcrafted in the contexts of geometric deep learning [1] and deep narrow networks
39"
INTRODUCTION,0.04338624338624339,"[19, 13, 18, 14, 23, 16, 2, 15]. Needless to say, there is no guarantee that these handcrafted feature
40"
INTRODUCTION,0.044444444444444446,"maps are acquired by deep learning, so these analyses are considered to be analyses of possible
41"
INTRODUCTION,0.0455026455026455,"worlds.
42"
INTRODUCTION,0.04656084656084656,"Recently, Sonoda et al. [33, 32] discovered a rich class of ridgelet transforms for learning machines
43"
INTRODUCTION,0.047619047619047616,"defined by scalar-valued joint-group-invariant feature maps, covering both depth-2 fully-connected
44"
INTRODUCTION,0.04867724867724868,"networks and the formal deep network (FDN), yielding the first ridgelet transform for deep models.
45"
INTRODUCTION,0.04973544973544974,"Their theory is indeed a breakthrough because it could cover both deep and shallow models simulta-
46"
INTRODUCTION,0.050793650793650794,"neously. However, each hidden layer in the FDN has to be formalized as an abstract group action,
47"
INTRODUCTION,0.05185185185185185,"so it was not possible to cover real deep networks defined by composites of nonlinear activation
48"
INTRODUCTION,0.05291005291005291,"function. In this study, we extend their arguments for vector-valued joint-group-equivariant feature
49"
INTRODUCTION,0.05396825396825397,"maps (Theorem 3 and Corollary 1), so to cover such real networks. As an important example, in
50"
INTRODUCTION,0.05502645502645503,"§ 4.2, we obtained the ridgelet transform for a more realistic DNN, the depth-n fully-connected
51"
INTRODUCTION,0.056084656084656084,"network with an arbitrary activation function (not limited to ReLU), without handcrafting network
52"
INTRODUCTION,0.05714285714285714,"architecture. In other words, it is a constructive proof of the L2(Rm; Rm)-universality of the DNNs,
53"
INTRODUCTION,0.0582010582010582,"and an explicit characterization of the solution space of the DNN equation for more realistic setup.
54"
INTRODUCTION,0.05925925925925926,"Thanks to Schur’s lemma, a basic and useful result in the representation theory, the proof of the main
55"
INTRODUCTION,0.06031746031746032,"theorem is surprisingly simple, yet the scope of application is wide. The significance of this study
56"
INTRODUCTION,0.061375661375661375,"lies in revealing the close relationship between machine learning theory and modern algebra. With
57"
INTRODUCTION,0.06243386243386243,"this study as a catalyst, we expect a major upgrade to machine learning theory from the perspective
58"
INTRODUCTION,0.06349206349206349,"of modern algebra.
59"
PRELIMINARIES,0.06455026455026455,"2
Preliminaries
60"
PRELIMINARIES,0.0656084656084656,"We quickly introduce the original integral representation and the ridgelet transform, a mathematical
61"
PRELIMINARIES,0.06666666666666667,"model of depth-2 fully-connected network and its right inverse. Then, we list a few facts in the group
62"
PRELIMINARIES,0.06772486772486773,"representation theory. In particular, Schur’s lemma and the Haar measure play key roles in the proof
63"
PRELIMINARIES,0.06878306878306878,"of the main results.
64"
PRELIMINARIES,0.06984126984126984,"Notation.
For any topological space X, Cc(X) denotes the Banach space of all compactly supported
65"
PRELIMINARIES,0.07089947089947089,"continuous functions on X. For any measure space X, Lp(X) denotes the Banach space of all p-
66"
PRELIMINARIES,0.07195767195767196,"integrable functions on X. S(Rd) and S′(Rd) denote the classes of rapidly decreasing functions (or
67"
PRELIMINARIES,0.07301587301587302,"Schwartz test functions) and tempered distributions on Rd, respectively.
68"
PRELIMINARIES,0.07407407407407407,"2.1
Integral Representation and Ridgelet Transform for Depth-2 Fully-Connected Network
69"
PRELIMINARIES,0.07513227513227513,"Definition 1. For any measurable functions σ : R →C and γ : Rm × R →C, put
70"
PRELIMINARIES,0.0761904761904762,"Sσ[γ](x) :=
Z"
PRELIMINARIES,0.07724867724867725,"Rm×R
γ(a, b)σ(a · x −b)dadb,
x ∈Rm.
(2)"
PRELIMINARIES,0.07830687830687831,"We call Sσ[γ] an (integral representation of) neural network, and γ a parameter distribution.
71"
PRELIMINARIES,0.07936507936507936,"The integration over all the hidden parameters (a, b) ∈Rm × R means all the neurons {x 7→
72"
PRELIMINARIES,0.08042328042328042,"σ(a · x −b) | (a, b) ∈Rm × R} are summed (or integrated, to be precise) with weight γ, hence
73"
PRELIMINARIES,0.08148148148148149,"formally Sσ[γ] is understood as a continuous neural network with a single hidden layer. We note,
74"
PRELIMINARIES,0.08253968253968254,"however, when γ is a finite sum of point measures such as γp = Pp
i=1 ciδ(ai,bi) (by appropriately
75"
PRELIMINARIES,0.0835978835978836,"extending the class of γ to Borel measures), then it can also reproduce a finite width network
76"
PRELIMINARIES,0.08465608465608465,"Sσ[γp](x) = p
X"
PRELIMINARIES,0.08571428571428572,"i=1
ciσ(ai · x −bi).
(3)"
PRELIMINARIES,0.08677248677248678,"In other words, the integral representation is a mathmatical model of depth-2 network with any width
77"
PRELIMINARIES,0.08783068783068783,"(ranging from finite to continuous).
78"
PRELIMINARIES,0.08888888888888889,"Next, we introduce the ridgelet transform, which is known to be a right-inverse operator to Sσ.
79"
PRELIMINARIES,0.08994708994708994,"Definition 2. For any measurable functions ρ : R →C and f : Rm →C, put
80"
PRELIMINARIES,0.091005291005291,"Rρ[f](a, b) :=
Z"
PRELIMINARIES,0.09206349206349207,"Rm f(x)ρ(a · x −b)dx,
(a, b) ∈Rm × R.
(4)"
PRELIMINARIES,0.09312169312169312,"We call Rρ a ridgelet transform.
81"
PRELIMINARIES,0.09417989417989418,"To be precise, it satisfies the following reconstruction formula.
82"
PRELIMINARIES,0.09523809523809523,"Theorem 1 (Reconstruction Formula). Suppose σ and ρ are a tempered distribution (S′) and a rapid
83"
PRELIMINARIES,0.0962962962962963,"decreasing function (S) respectively. There exists a bilinear form ((σ, ρ)) such that
84"
PRELIMINARIES,0.09735449735449736,"Sσ ◦Rρ[f] = ((σ, ρ))f,
(5)
for any square integrable function f ∈L2(Rm). Further, the bilinear form is given by ((σ, ρ)) =
85
R"
PRELIMINARIES,0.09841269841269841,"R σ♯(ω)ρ♯(ω)|ω|−mdω, where ♯denotes the 1-dimensional Fourier transform.
86"
PRELIMINARIES,0.09947089947089947,"See Sonoda et al. [29, Theorem 6] for the proof. In particular, according to Sonoda et al. [29,
87"
PRELIMINARIES,0.10052910052910052,"Lemma 9], for any activation function σ, there always exists ρ satisfying ((σ, ρ)) = 1. Here, σ
88"
PRELIMINARIES,0.10158730158730159,"being a tempered distribution means that typical activation functions are covered such as ReLU, step
89"
PRELIMINARIES,0.10264550264550265,"function, tanh, gaussian, etc... We can interpret the reconstruction formula as a universality theorem
90"
PRELIMINARIES,0.1037037037037037,"of continuous neural networks, since for any given data generating function f, a network with output
91"
PRELIMINARIES,0.10476190476190476,"weight γf = Rρ[f] reproduces f (up to factor ((σ, ρ))), i.e. S[γf] = f. In other words, the ridgelet
92"
PRELIMINARIES,0.10582010582010581,"transform indicates how the network parameters should be organized so that the network represents
93"
PRELIMINARIES,0.10687830687830688,"an individual function f.
94"
PRELIMINARIES,0.10793650793650794,"The original ridgelet transform was discovered by Murata [21] and Candès [3]. It is recently extended
95"
PRELIMINARIES,0.10899470899470899,"to a few modern networks by the Fourier slice method [34, see e.g.]. In this study, we present a
96"
PRELIMINARIES,0.11005291005291006,"systematic scheme to find the ridgelet transform for a variety of given network architecture based on
97"
PRELIMINARIES,0.1111111111111111,"the group theoretic arguments.
98"
PRELIMINARIES,0.11216931216931217,"2.2
Irreducible Unitary Representation and Schur’s Lemma
99"
PRELIMINARIES,0.11322751322751323,"Let G be a locally compact group, H be a nonzero Hilbert space, and U(H) be the group of unitary
100"
PRELIMINARIES,0.11428571428571428,"operators on H. For example, any finite group, discrete group, compact group, and finite-dimensional
101"
PRELIMINARIES,0.11534391534391535,"Lie group are locally compact, while an infinite-dimensional Lie group is not locally compact. A
102"
PRELIMINARIES,0.1164021164021164,"unitary representation π of G on H is a group homomorphism that is continuous with respect to
103"
PRELIMINARIES,0.11746031746031746,"the strong operator topology—that is, a map π : G →U(H) satisfying π(gh) = π(g)π(h) and
104"
PRELIMINARIES,0.11851851851851852,"π(g−1) = π(g)−1, and for any ψ ∈H, the map G ∋g 7→π(g)[ψ] ∈H is continuous.
105"
PRELIMINARIES,0.11957671957671957,"Suppose M is a closed subspace of H. M is called an invariant subspace when π(g)M ⊂M for all
106"
PRELIMINARIES,0.12063492063492064,"g ∈G. Particularly, π is called irreducible when it does not admit any nontrivial invariant subspace
107"
PRELIMINARIES,0.12169312169312169,"M ̸= {0} nor H. The following theorem is a fundamental result of group representation theory that
108"
PRELIMINARIES,0.12275132275132275,"characterizes the irreducibility.
109"
PRELIMINARIES,0.12380952380952381,"Theorem 2 (Schur’s lemma). A unitary representation (π, H) is irreducible iff any bounded operator
110"
PRELIMINARIES,0.12486772486772486,"T on H that commutes with π is always a constant multiple of the identity. In other words, if
111"
PRELIMINARIES,0.1259259259259259,"π(g)T = Tπ(g) for all g ∈G, then T = c IdH for some c ∈C.
112"
PRELIMINARIES,0.12698412698412698,"See Folland [10, Theorem 3.5(a)] for the proof. We use this as a key step in the proof of our main
113"
PRELIMINARIES,0.12804232804232804,"theorem.
114"
CALCULUS ON LOCALLY COMPACT GROUP,0.1291005291005291,"2.3
Calculus on Locally Compact Group
115"
CALCULUS ON LOCALLY COMPACT GROUP,0.13015873015873017,"By Haar’s theorem, if G is a locally compact group, then there uniquely exist left and right invariant
116"
CALCULUS ON LOCALLY COMPACT GROUP,0.1312169312169312,"measures dlg and drg, satisfying for any s ∈G and f ∈Cc(G),
117
Z"
CALCULUS ON LOCALLY COMPACT GROUP,0.13227513227513227,"G
f(sg)dlg =
Z"
CALCULUS ON LOCALLY COMPACT GROUP,0.13333333333333333,"G
f(g)dlg,
and
Z"
CALCULUS ON LOCALLY COMPACT GROUP,0.1343915343915344,"G
f(gs)drg =
Z"
CALCULUS ON LOCALLY COMPACT GROUP,0.13544973544973546,"G
f(g)drg."
CALCULUS ON LOCALLY COMPACT GROUP,0.1365079365079365,"Let X be a G-space with transitive left (resp. right) G-action g ·x (resp. x·g) for any (g, x) ∈G×X.
118"
CALCULUS ON LOCALLY COMPACT GROUP,0.13756613756613756,"Then, we can further induce the left (resp. right) invariant measure dlx (resp. drx) so that for any
119"
CALCULUS ON LOCALLY COMPACT GROUP,0.13862433862433862,"f ∈Cc(G),
120
Z"
CALCULUS ON LOCALLY COMPACT GROUP,0.13968253968253969,"X
f(x)dlx :=
Z"
CALCULUS ON LOCALLY COMPACT GROUP,0.14074074074074075,"G
f(g · o)dlg,
resp.
Z"
CALCULUS ON LOCALLY COMPACT GROUP,0.14179894179894179,"X
f(x)drx :=
Z"
CALCULUS ON LOCALLY COMPACT GROUP,0.14285714285714285,"G
f(o · g)drg,"
CALCULUS ON LOCALLY COMPACT GROUP,0.1439153439153439,"where o ∈X is a fixed point called the origin.
121 g.x input ξ g.y"
CALCULUS ON LOCALLY COMPACT GROUP,0.14497354497354498,output φ
CALCULUS ON LOCALLY COMPACT GROUP,0.14603174603174604,parameter
CALCULUS ON LOCALLY COMPACT GROUP,0.14708994708994708,G-equivariance g.x input g.ξ g.y
CALCULUS ON LOCALLY COMPACT GROUP,0.14814814814814814,output φ
CALCULUS ON LOCALLY COMPACT GROUP,0.1492063492063492,parameter
CALCULUS ON LOCALLY COMPACT GROUP,0.15026455026455027,joint-G-equivariance
CALCULUS ON LOCALLY COMPACT GROUP,0.15132275132275133,"Figure 1: An ordinary G-equivariant feature map ϕ : X ×Ξ →Y is a subclass of joint-G-equivariant
map where the G-action on parameter domain Ξ is trivial, i.e. g · ξ = ξ"
MAIN RESULTS,0.1523809523809524,"3
Main Results
122"
MAIN RESULTS,0.15343915343915343,"We introduce the joint-group-equivariant feature map, and present the ridgelet transforms for learning
123"
MAIN RESULTS,0.1544973544973545,"machines defined by joint-group-equivariant feature maps, yielding the universality of deep models.
124"
MAIN RESULTS,0.15555555555555556,"Let G be a locally compact group equipped with a left invariant measure dg. Let X and Ξ be
125"
MAIN RESULTS,0.15661375661375662,"G-spaces equipped with G-invariant measures dx and dξ, called the data domain and the parameter
126"
MAIN RESULTS,0.15767195767195769,"domain, respectively. Particularly, we call the product space X × Ξ the data-parameter domain (like
127"
MAIN RESULTS,0.15873015873015872,"time-frequency domain), and call any map ϕ on data-parameter domain X × Ξ a feature map. Let H
128"
MAIN RESULTS,0.15978835978835979,"be a separable Hilbert space, let U(H) be the space of unitary operators on H, and let υ : G →U(H)
129"
MAIN RESULTS,0.16084656084656085,"be a unitary representation of G on H. If there is no danger of confusion, we use the same symbol ·
130"
MAIN RESULTS,0.1619047619047619,"for the G-actions on X, H, and Ξ (e.g., g · x, g · v, and g · ξ).
131"
MAIN RESULTS,0.16296296296296298,"In the main theorem, the irreducibility of the following unitary representation π will be a sufficient
132"
MAIN RESULTS,0.164021164021164,"condition for the universality. Let L2(X; H) denote the space of H-valued square-integrable functions
133"
MAIN RESULTS,0.16507936507936508,"on X equipped with the inner product ⟨ϕ, ψ⟩L2(X;H) :=
R"
MAIN RESULTS,0.16613756613756614,"X⟨ϕ(x), ψ(x)⟩Hdx. Put
134"
MAIN RESULTS,0.1671957671957672,"πg[f](x) := g · f(g−1 · x),
x ∈X, f ∈L2(X; H), g ∈G.
(6)"
MAIN RESULTS,0.16825396825396827,"Then, it is a unitary representation of G on L2(X; H). In fact, πg[πh[f]](x) = g·h·f(h−1·g−1·x) =
135"
MAIN RESULTS,0.1693121693121693,"(gh) · f((gh)−1 · x) = πgh[f](x), and ⟨πg[f1], πg[f2]⟩L2(X;H) =
R"
MAIN RESULTS,0.17037037037037037,"X⟨υg[f1](g−1 · x), υg[f2](g−1 ·
136"
MAIN RESULTS,0.17142857142857143,"x)⟩Hdx =
R"
MAIN RESULTS,0.1724867724867725,"X⟨f1(x), υ∗
g[υg[f2]](x)⟩Hdx = ⟨f1, f2⟩L2(X;H).
137"
MAIN RESULTS,0.17354497354497356,"In addition, let L2(Ξ) denote the space of C-valued square-integrable functions on Ξ, and let bπ be
138"
MAIN RESULTS,0.1746031746031746,"the left-regular representation of G on L2(Ξ) given by
139"
MAIN RESULTS,0.17566137566137566,"bπg[γ](ξ) := γ(g−1 · ξ),
ξ ∈Ξ, γ ∈L2(Ξ), g ∈G.
(7)"
MAIN RESULTS,0.17671957671957672,"Similarly to π, bπ is also a unitary representation.
140"
MAIN RESULTS,0.17777777777777778,"Definition 3 (Joint G-Equivariant Feature Map). Let X, Y be data domains, and Ξ be a parameter
141"
MAIN RESULTS,0.17883597883597885,"domain (with G-actions). We say a feature map ϕ : X × Ξ →Y is joint-G-equivariant when
142"
MAIN RESULTS,0.17989417989417988,"ϕ(g · x, g · ξ) = g · ϕ(x, ξ),
(x, ξ) ∈X × Ξ,
(8)"
MAIN RESULTS,0.18095238095238095,"holds for all g ∈G. In other words, ϕ is a homomorphism (or G-map) of G-sets from X × Ξ to
143"
MAIN RESULTS,0.182010582010582,"Y . So by homG(X × Ξ, Y ), we denote the collection of all joint-G-equivariant maps. Additionally,
144"
MAIN RESULTS,0.18306878306878308,"when G-action on Y is trivial, i.e. ϕ(g · x, g · ξ) = ϕ(x, ξ), we say it is joint-G-invariant.
145"
MAIN RESULTS,0.18412698412698414,"Remark 1. The joint-G-equivariance extends an ordinary notion of G-equivariance, i.e. ϕ(g · x, ξ) =
146"
MAIN RESULTS,0.18518518518518517,"g · ϕ(x, ξ). In fact, G-equivariance is a special case of joint-G-equivariance where G acts trivially on
147"
MAIN RESULTS,0.18624338624338624,"parameter domain, i.e. g · ξ = ξ (see also Figure 1).
148"
MAIN RESULTS,0.1873015873015873,"In order to construct a (non-joint) group-equivariant network, we must carefully and precisely design
149"
MAIN RESULTS,0.18835978835978837,"the network architecture [see, e.g., a textbook of geometric deep learning 1]. On the other hand, we
150"
MAIN RESULTS,0.18941798941798943,"can easily and systematically construct joint-G-equivariant network from (not at all equivariant but)
151"
MAIN RESULTS,0.19047619047619047,"any map f : X →Y according to the following Lemmas 1 and 2.
152"
MAIN RESULTS,0.19153439153439153,"Lemma 1. Suppose group G acts on sets X and Y . Fix an arbitrary map f : X →Y , and put
153"
MAIN RESULTS,0.1925925925925926,"ϕ(x, g) := g · f(g−1 · x) for every x ∈X and g ∈G. Then, ϕ : X × G →Y is joint-G-equivariant.
154"
MAIN RESULTS,0.19365079365079366,"Proof. Straightforward. For any g ∈G, ϕ(g · x, g · h) = (gh) · f((gh)−1 · (g · x)) = g · ϕ(x, h).
155"
MAIN RESULTS,0.19470899470899472,"Lemma 2 (Depth-n Feature Map ϕ1:n). Given a sequence of G-equivariant feature maps ϕi :
156"
MAIN RESULTS,0.19576719576719576,"Xi−1 × Ξi →Xi (i = 1, . . . , n), put ϕ1:n : X0 × Ξ1 × · · · × Ξn →Xn by
157"
MAIN RESULTS,0.19682539682539682,"ϕ1:n(x, ξ1, . . . , ξn) := ϕn(•, ξn) ◦· · · ◦ϕ1(x, ξ1).
(9)"
MAIN RESULTS,0.19788359788359788,"Then, ϕ1:n is G-equivariant. Following the custom of counting the number of parameter domains
158"
MAIN RESULTS,0.19894179894179895,"(Ξi)n
i=1, we say ϕ1:n is depth-n.
159"
MAIN RESULTS,0.2,"Proof. In fact,
160"
MAIN RESULTS,0.20105820105820105,"ϕ1:n(g · x, g · ξ1, . . . , g · ξn) = ϕn(•, g · ξn) ◦· · · ◦ϕ2(•, g · ξ2) ◦ϕ1(g · x, g · ξ1)
= ϕn(•, g · ξn) ◦· · · ◦ϕ2(g · •, g · ξ2) ◦ϕ1(x, ξ1)
...
= ϕn(g · •, g · ξn) ◦· · · ◦ϕ2(•, ξ2) ◦ϕ1(x, ξ1)
= g · ϕn(•, ξn) ◦· · · ◦ϕ2(•, ξ2) ◦ϕ1(x, ξ1)
= g · ϕ1:n(x, ξ1, . . . , ξn)."
MAIN RESULTS,0.2021164021164021,"Definition 4 (ϕ-Network). For any vector-valued map ϕ : X × Ξ →H and scalar-valued map
161"
MAIN RESULTS,0.20317460317460317,"γ : Ξ →C, define a vector-valued map X →H by
162"
MAIN RESULTS,0.20423280423280424,"NN[γ; ϕ](x) :=
Z"
MAIN RESULTS,0.2052910052910053,"Ξ
γ(ξ)ϕ(x, ξ)dξ,
x ∈X,
(10)"
MAIN RESULTS,0.20634920634920634,"where the integral is understood as the Bocher integral.
163"
MAIN RESULTS,0.2074074074074074,"We call the integral transform NN[•; ϕ] a ϕ-transform, and each individual image NN[γ; ϕ] a ϕ-network
164"
MAIN RESULTS,0.20846560846560847,"for short. The ϕ-network extends the original integral representation. In particular, it inherits the
165"
MAIN RESULTS,0.20952380952380953,"concept of integrating all the possible parameters ξ and indirectly select which parameters to use
166"
MAIN RESULTS,0.2105820105820106,"by weighting on them, which linearize parametrization by lifting nonlinear parameters ξ to linear
167"
MAIN RESULTS,0.21164021164021163,"parameter γ.
168"
MAIN RESULTS,0.2126984126984127,"Definition 5 (ψ-Ridgelet Transform). For any H-valued feature map ψ : X × Ξ →H and H-valued
169"
MAIN RESULTS,0.21375661375661376,"Borel measurable function f on X, put a scalar-valued integral transform
170"
MAIN RESULTS,0.21481481481481482,"R[f; ψ](ξ) :=
Z"
MAIN RESULTS,0.21587301587301588,"X
⟨f(x), ψ(x, ξ)⟩Hdx,
ξ ∈Ξ.
(11)"
MAIN RESULTS,0.21693121693121692,"We call the integral transform R[•; ψ] a ψ-ridgelet transform for short.
171"
MAIN RESULTS,0.21798941798941798,"As long as the integrals are convergent, ϕ-ridgelet transform is the dual operator of ϕ-transform, since
172"
MAIN RESULTS,0.21904761904761905,"⟨γ, R[f; ϕ]⟩L2(Ξ) =
Z"
MAIN RESULTS,0.2201058201058201,"X×Ξ
γ(ξ)⟨ϕ(x, ξ), f(x)⟩Hdxdξ = ⟨NN[γ; ϕ], f⟩L2(X;H).
(12)"
MAIN RESULTS,0.22116402116402117,"Theorem 3 (Reconstruction Formula). Assume (1) H-valued feature maps ϕ, ψ : X × Ξ →H are
173"
MAIN RESULTS,0.2222222222222222,"joint-G-equivariant, (2) composite operator NNϕ ◦Rψ : L2(X; H) →L2(X; H) is bounded (i.e.,
174"
MAIN RESULTS,0.22328042328042327,"Lipschitz continuous), and (3) the unitary representation π defined in (6) is irreducible. Then, there
175"
MAIN RESULTS,0.22433862433862434,"exists a bilinear form ((ϕ, ψ)) ∈C (independent of f) such that for any H-valued square-integrable
176"
MAIN RESULTS,0.2253968253968254,"function f ∈L2(X; H),
177"
MAIN RESULTS,0.22645502645502646,"NNϕ ◦Rψ[f] = ((ϕ, ψ))f.
(13)"
MAIN RESULTS,0.2275132275132275,"In other words, the ψ-ridgelet transform Rψ is a right inverse operator of ϕ-transform NNϕ as long as
178"
MAIN RESULTS,0.22857142857142856,"((ϕ, ψ)) ̸= 0, ∞.
179"
MAIN RESULTS,0.22962962962962963,"Proof. We write NN[•; ϕ] as NNϕ and R[•; ϕ] as Rϕ for short. By using the unitarity of representation
180"
MAIN RESULTS,0.2306878306878307,"υ : G →U(H), left-invariance of measure dx, and G-equivariance of feature map ψ, for all g ∈G,
181"
MAIN RESULTS,0.23174603174603176,"we have
182"
MAIN RESULTS,0.2328042328042328,"Rψ[πg[f]](ξ) =
Z"
MAIN RESULTS,0.23386243386243386,"X
⟨g · f(g−1 · x), ψ(x, ξ)⟩Hdx =
Z"
MAIN RESULTS,0.23492063492063492,"X
⟨f(x), g−1 · ψ(g · x, ξ)⟩Hdx =
Z"
MAIN RESULTS,0.23597883597883598,"X
⟨f(x), ψ(x, g−1 · ξ)⟩Hdx = bπg[Rψ[f]](ξ).
(14) x"
MAIN RESULTS,0.23703703703703705,G-space input ξ1
MAIN RESULTS,0.23809523809523808,vector-space
MAIN RESULTS,0.23915343915343915,output φ1
MAIN RESULTS,0.2402116402116402,"G-space
parameter x1 ξ2 φ2
x2 ξ3 φ3
y"
MAIN RESULTS,0.24126984126984127,"G-space
feature"
MAIN RESULTS,0.24232804232804234,"Figure 2: Deep H-valued joint-G-equivariant network on G-space X is L2(X; H)-universal when
unitary representation π of G on L2(X; H) is irreducible, and the distribution of parameters for the
network to represent a given map f : X →H is exactly given by the ridgelet transform R[f]"
MAIN RESULTS,0.24338624338624337,"Similarly,
183"
MAIN RESULTS,0.24444444444444444,"NNϕ[bπg[γ]](x) =
Z"
MAIN RESULTS,0.2455026455026455,"Ξ
γ(g−1 · ξ)ϕ(x, ξ)dξ =
Z"
MAIN RESULTS,0.24656084656084656,"Ξ
γ(ξ)ϕ(x, g · ξ)dξ =
Z"
MAIN RESULTS,0.24761904761904763,"Ξ
γ(ξ)
 
g · ϕ(g−1 · x, ξ)

dξ = πg[NNϕ[γ]](x).
(15)"
MAIN RESULTS,0.24867724867724866,"Here, bπ∗denotes the dual representation of bπ with respect to L2(Ξ).
184"
MAIN RESULTS,0.24973544973544973,"As a consequence, NNϕ ◦Rψ : L2(X; H) →L2(X; H) commutes with π as below
185"
MAIN RESULTS,0.2507936507936508,"NNϕ ◦Rψ ◦πg = NNϕ ◦bπg ◦Rψ = πg ◦NNϕ ◦Rψ
(16)"
MAIN RESULTS,0.2518518518518518,"for all g ∈G. Hence by Schur’s lemma (Theorem 2), there exist a constant Cϕ,ψ ∈C such that
186"
MAIN RESULTS,0.2529100529100529,"NNϕ ◦Rψ = Cϕ,ψ IdL2(X). Since NNϕ ◦Rψ is bilinear in ϕ and ψ, Cϕ,ψ is bilinear in ϕ and ψ.
187"
MAIN RESULTS,0.25396825396825395,"In particular, because depth-n feature map ϕ1:n is G-equivariant (Lemma 2), the following depth-n
188"
MAIN RESULTS,0.25502645502645505,"H-valued deep network DNN[γ; ϕ1:n] is L2(X; H)-universal.
189"
MAIN RESULTS,0.2560846560846561,"Corollary 1 (Deep Ridgelet Transform). For any maps γ : X →C and f ∈L2(X; H), put
190"
MAIN RESULTS,0.2571428571428571,"DNN[γ; ϕ1:n](x) :=
Z"
MAIN RESULTS,0.2582010582010582,"Ξ1×···×Ξn
γ(ξ1, . . . , ξn)ϕn(•, ξn) ◦· · · ◦ϕ1(x, ξ1)dξ,
x ∈X,
(17)"
MAIN RESULTS,0.25925925925925924,"R[f; ψ1:n](ξ) :=
Z"
MAIN RESULTS,0.26031746031746034,"Ξ
⟨f(x), ψn(•, ξn) ◦· · · ◦ψ1(x, ξn)⟩Hdx,
ξ ∈Ξ1 × · · · × Ξn.
(18)"
MAIN RESULTS,0.2613756613756614,"Under the assumptions that DNNϕ1:n ◦Rψ1:n is bounded, and that π is irreducible, there exists a
191"
MAIN RESULTS,0.2624338624338624,"bilinear form ((ϕ1:n, ψ1:n)) satisfying DNNϕ1:n ◦Rψ1:n = ((ϕ1:n, ψ1:n)) IdL2(X;H).
192"
MAIN RESULTS,0.2634920634920635,"Again, it extends the original integral representation, and inherits the linearization trick of nonlinear
193"
MAIN RESULTS,0.26455026455026454,"parameters ξ by integrating all the possible parameters (beyond the difference of layers) and indirectly
194"
MAIN RESULTS,0.2656084656084656,"select which parameters to use by weighting on them.
195"
MAIN RESULTS,0.26666666666666666,"4
Example: Depth-n Fully-Connected Network with Arbitrary Activation
196"
MAIN RESULTS,0.2677248677248677,"As a concrete example, we present the ridgelet transform for depth-n fully-connected network.
197"
MAIN RESULTS,0.2687830687830688,"First, we show the depth-2 case based on a joint-affine-invariant argument, which was originally
198"
MAIN RESULTS,0.2698412698412698,"demonstrated by Sonoda et al. [33]. Then, we show the depth-n case based on a joint-equivariant
199"
MAIN RESULTS,0.2708994708994709,"argument by extending the original arguments.
200"
MAIN RESULTS,0.27195767195767195,"We use the following known facts.
201"
MAIN RESULTS,0.273015873015873,"Lemma 3. The regular representation π of the affine group Aff(m) on L2(Rm) (defined below) is
202"
MAIN RESULTS,0.2740740740740741,"irreducible.
203"
MAIN RESULTS,0.2751322751322751,"See Folland [10, Theorem 6.42] for the proof.
204"
MAIN RESULTS,0.2761904761904762,"Lemma 4. Suppose σ and ρ are a tempered distribution (S′) and a Schwartz test function, respectively.
205"
MAIN RESULTS,0.27724867724867724,"Then, Sσ ◦Rρ : L2(Rm) →L2(Rm) is bounded.
206"
MAIN RESULTS,0.2783068783068783,"See Sonoda et al. [29, Lemmas 7 and 8] for the proof.
207"
MAIN RESULTS,0.27936507936507937,"4.1
Depth-2
208"
MAIN RESULTS,0.2804232804232804,"Set X := Rm (data domain), Ξ := Rm ×R (parameter domain), and G := Aff(m) = GL(m)⋉Rm
209"
MAIN RESULTS,0.2814814814814815,"be the m-dimensional affine group, acting on data domain X by
210"
MAIN RESULTS,0.28253968253968254,"g · x := Lx + t,
g = (L, t) ∈GL(m) ⋉Rm, x ∈X.
(19)
Addition to this, let π be the regular representation of Aff(m) on L2(X), namely
211"
MAIN RESULTS,0.28359788359788357,"π(g)[f](x) := | det L|−1/2f(L−1(x −t)),
f ∈L2(X) and g = (L, t) ∈GL(m) ⋉Rm. (20)"
MAIN RESULTS,0.28465608465608466,"Further, define a dual action of Aff(m) on the parameter domain Ξ as
212"
MAIN RESULTS,0.2857142857142857,"g · (a, b) = (L−⊤a, b + t⊤L−⊤a),
g = (L, t) ∈GL(m) ⋉Rm, (a, b) ∈Ξ.
(21)
Then, we can see the feature map ϕ(x, (a, b)) := σ(a · x −b) is joint-G-invariant. In fact,
213"
MAIN RESULTS,0.2867724867724868,"ϕ(g · x, g · (a, b)) = σ
 
L−⊤a · (Lx + t) −(b + t⊤L−⊤a)

= σ(a · x −b) = ϕ(x, (a, b)).
By Lemma 3, the regular representation π of G = Aff(m) is irreducible. Therefore, by Theorem 3,
214"
MAIN RESULTS,0.2878306878306878,"the depth-2 neural network and corresponding ridgelet transform:
215"
MAIN RESULTS,0.28888888888888886,"NN[γ](x) =
Z"
MAIN RESULTS,0.28994708994708995,"Rm×R
γ(a, b)σ(a · x −b)dadb,
and
R2[f](a, b) =
Z"
MAIN RESULTS,0.291005291005291,"Rm f(x)ρ(a · x −b)dx,"
MAIN RESULTS,0.2920634920634921,"satisfy the reconstruction formula NN ◦R2 = ((σ, ρ)) IdL2(Rm). In Appendix A, we supplemented a
216"
MAIN RESULTS,0.2931216931216931,"detailed proof. In Appendix B, we discussed a geometric interpretation of dual G-action (21).
217"
DEPTH-N,0.29417989417989415,"4.2
Depth-n
218"
DEPTH-N,0.29523809523809524,"Following Corollary 1, we derive the ridgelet transform for depth-n fully-connected network by
219"
DEPTH-N,0.2962962962962963,"constructing a joint-equivariant network.
220"
DEPTH-N,0.29735449735449737,"Let O(m) be the m-dimensional orthogonal group acting on Rm by Qv for Q ∈O(m) and v ∈Rm,
221"
DEPTH-N,0.2984126984126984,"and (re)set G := O(m) × Aff(m) be the product group, acting on the data domain X by
222"
DEPTH-N,0.29947089947089944,"g · x := Lx + t,
x ∈X, g = (Q, L, t) ∈G = O(m) × (GL(m) ⋉Rm) .
(22)
Namely, we set the O(m)-action on X is trivial. Define a unitary representation π of G on vector-
223"
DEPTH-N,0.30052910052910053,"valued square-integrable functions L2(X; X) as
224"
DEPTH-N,0.30158730158730157,"πg[f](x) := Qf(L−1(x −t)),
x ∈X, g = (Q, L, t) ∈G, f ∈L2(X; X).
(23)
Lemma 5. The above π : G →L2(Rm; Rm) is an irreducible unitary representation.
225"
DEPTH-N,0.30264550264550266,"Proof. Recall that a tensor product of irreducible representations is irreducible. Since both O(m)-
226"
DEPTH-N,0.3037037037037037,"action on Rm and Aff(m)-action on L2(Rm) are irreducible, and L2(Rm; Rm) is a tensor product
227"
DEPTH-N,0.3047619047619048,"Rm ⊗L2(Rm), so the action π of product group O(m)×Aff(m) on tensor product Rm ⊗L2(Rm) =
228"
DEPTH-N,0.3058201058201058,"L2(Rm; Rm) is irreducible.
229"
DEPTH-N,0.30687830687830686,"Following the same arguments in Lemma 1, we first construct a depth-2 joint-G-equivariant network.
230"
DEPTH-N,0.30793650793650795,"Take an arbitrary square-integrable (not yet joint-G-equivariant) vector-field f0 ∈L2(X; X). Then,
231"
DEPTH-N,0.308994708994709,"the following network is joint-G-equivariant:
232"
DEPTH-N,0.3100529100529101,"NN(x, ξ) := NN[R2[πξ[f0]]](x) =
Z"
DEPTH-N,0.3111111111111111,"Rm×R
QR2[f0](a, b)σ
 
a⊤L−1(x −t) −b

dadb,
(24)"
DEPTH-N,0.31216931216931215,"for every x ∈X, ξ = (Q, L, t) ∈O(m) × GL(m) ⋉Rm. Here R2 is the ridgelet transform for
233"
DEPTH-N,0.31322751322751324,"depth-2 case (applied for vector-valued function by element-wise manner). This is joint-G-equivariant
234"
DEPTH-N,0.3142857142857143,"because NN(x, ξ) = πξ[f0](x). Henceforth, we (re)set Ξ := G.
235"
DEPTH-N,0.31534391534391537,"Finally, we construct a depth-n joint-G-equivariant network by composing the above depth-2 networks
236"
DEPTH-N,0.3164021164021164,"as below. Write ξ := (ξ1, . . . , ξn) ∈Ξn for short. For any measurable function γ : Ξn →C and
237"
DEPTH-N,0.31746031746031744,"vector-field f : Rm →Rm, put
238"
DEPTH-N,0.31851851851851853,"DNN(x) :=
Z"
DEPTH-N,0.31957671957671957,"Ξn γ(ξ)NN(•, ξn) ◦· · · ◦NN(x, ξ1)dξ,
x ∈X
(25)"
DEPTH-N,0.32063492063492066,"Rn[f](ξ) :=
Z"
DEPTH-N,0.3216931216931217,"X
f(x)⊤NN(•, ξn) ◦· · · ◦NN(x, ξ1)dx,
ξ ∈Ξn.
(26)"
DEPTH-N,0.32275132275132273,"Then, as a consequence of Corollary 1, there exists a constant c ∈C satisfying DNN ◦Rn[f] = cf for
239"
DEPTH-N,0.3238095238095238,"any f ∈L2(X; X).
240"
DEPTH-N,0.32486772486772486,"5
Example: Formal Deep Network
241"
DEPTH-N,0.32592592592592595,"We explain the formal deep network (FDN) introduced by Sonoda et al. [32]. Compared to the
242"
DEPTH-N,0.326984126984127,"depth-n fully-connected network introduced in the previous section, the FDN (introduced in the
243"
DEPTH-N,0.328042328042328,"previous study) is more abstract because the network architecture is not specified. Yet, we consider
244"
DEPTH-N,0.3291005291005291,"this is still useful for theoretical study of deep networks as it covers a wide range of groups and data
245"
DEPTH-N,0.33015873015873015,"domains (i.e., not limited to the affine group and the Euclidean space).
246"
FORMAL DEEP NETWORK,0.33121693121693124,"5.1
Formal Deep Network
247"
FORMAL DEEP NETWORK,0.3322751322751323,"Let G be an arbitrary locally compact group equipped with left-invariant measure dg, let X be a
248"
FORMAL DEEP NETWORK,0.3333333333333333,"G-space equipped with left-invariant measure dx, and set Ξ := G with right-invariant measure dξ.
249"
FORMAL DEEP NETWORK,0.3343915343915344,"The key concept is to identify each feature map ϕ : X × Ξ →X with a G-action g : X →X with
250"
FORMAL DEEP NETWORK,0.33544973544973544,"parameter domain Ξ being identified with group G, and the composite of feature maps, say g ◦h,
251"
FORMAL DEEP NETWORK,0.33650793650793653,"with product gh. Since a group is closed under its operation by definition, the proposed network can
252"
FORMAL DEEP NETWORK,0.33756613756613757,"represent literally any depth such as a single hidden layer g, double hidden layers g ◦h, triple hidden
253"
FORMAL DEEP NETWORK,0.3386243386243386,"layers g ◦h ◦k, and infinite hidden layers g ◦h ◦· · · . Besides, to lift the group action on a linear
254"
FORMAL DEEP NETWORK,0.3396825396825397,"space, the network is formulated as a regular action of group G on a hidden layer, say ψ ∈L2(X).
255"
FORMAL DEEP NETWORK,0.34074074074074073,"Definition 6 (Formal Deep Network). For any functions ψ ∈L2(X) and γ : Ξ →C, put
256"
FORMAL DEEP NETWORK,0.3417989417989418,"DNN[γ; ψ](x) :=
Z"
FORMAL DEEP NETWORK,0.34285714285714286,"G1⋊···⋊Gn
γ(ξ1, . . . , ξn) ψ ◦ξn ◦· · · ◦ξ1(x)dξ1 · · · dξn,
x ∈X.
(27)"
FORMAL DEEP NETWORK,0.3439153439153439,"Here, G = G1 ⋊· · · ⋊Gn denotes the semi-direct product of groups, suggesting that the network
257"
FORMAL DEEP NETWORK,0.344973544973545,"gets much complex and expressive as it gets deeper.
258"
FORMAL DEEP NETWORK,0.346031746031746,"To see the universality, define the dual action of G on the parameter domain Ξ = G as
259"
FORMAL DEEP NETWORK,0.3470899470899471,"g · ξ := ξg−1,
g ∈G, ξ ∈Ξ.
(28)"
FORMAL DEEP NETWORK,0.34814814814814815,"Then, we can see ϕ(x, ξ) := ψ ◦ξ(x) is joint-G-invariant. In fact,
260"
FORMAL DEEP NETWORK,0.3492063492063492,"ϕ(g · x, g · ξ) = ψ ◦(g · ξ)(g · x) = ψ ◦(ξ ◦g−1)(g(x)) = ψ ◦ξ(x) = ϕ(x, ξ)."
FORMAL DEEP NETWORK,0.3502645502645503,"Therefore, by Theorem 3, assuming that the regular representation π : G →U(L2(X)) is irreducible,
261"
FORMAL DEEP NETWORK,0.3513227513227513,"the ridgelet transform is given by
262"
FORMAL DEEP NETWORK,0.3523809523809524,"R[f](ξ1, . . . , ξn) =
Z"
FORMAL DEEP NETWORK,0.35343915343915344,"X
f(x)ψ ◦ξn ◦· · · ◦ξ1(x)dx,
(ξ1, . . . , ξn) ∈G1 ⋊· · · ⋊Gn
(29)"
FORMAL DEEP NETWORK,0.3544973544973545,"satisfying NN ◦R = ((σ, ρ)) IdL2(X).
263"
DEPTH SEPARATION,0.35555555555555557,"5.2
Depth Separation
264"
DEPTH SEPARATION,0.3566137566137566,"To enjoy the advantage of abstract formulation, we discuss the effect of depth. For the sake of
265"
DEPTH SEPARATION,0.3576719576719577,"simplicity, we assume G to be a finite group, which may be acceptable given that the data domain
266"
DEPTH SEPARATION,0.35873015873015873,"X in practice is often discretized (or coarse-grained) into finite sets of representative points, say
267"
DEPTH SEPARATION,0.35978835978835977,"X ≈X := {xi}p
i=1, and if so the G-action is also reduced to finite representative actions.
268"
DEPTH SEPARATION,0.36084656084656086,"Following the concept of the formal deep network, we call group G acting on X a network. Let us
269"
DEPTH SEPARATION,0.3619047619047619,"consider depth-1 network G and depth-n network G1 ⋊· · ·⋊Gn satisfying G = G1 ⋊· · ·⋊Gn. The
270"
DEPTH SEPARATION,0.362962962962963,"equation indicates that two networks have the same expressive power, because they can implement
271"
DEPTH SEPARATION,0.364021164021164,"the same class of maps g : X →X.
272"
DEPTH SEPARATION,0.36507936507936506,"Next, let us define the width of a single layer G as the cardinality |G|. This is reasonable because
273"
DEPTH SEPARATION,0.36613756613756615,"the set G parametrizes each map g : X →X. Then, under the assumption that each Gi is simple,
274"
DEPTH SEPARATION,0.3671957671957672,"the depth-n network G1 ⋊· · · ⋊Gn can express the same class of depth-1 network exponentially-
275"
DEPTH SEPARATION,0.3682539682539683,"effectively, because the total widths are Pn
i=1 |Gi| = O(n) for depth-n and Qn
i=1 |Gi| = exp O(n)
276"
DEPTH SEPARATION,0.3693121693121693,"for depth-1. This estimate can be interpreted as the classical thought that the hierarchical models
277"
DEPTH SEPARATION,0.37037037037037035,"such as deep networks can represent complex functions combinatorially more efficient than shallow
278"
DEPTH SEPARATION,0.37142857142857144,"models.
279"
DISCUSSION,0.3724867724867725,"6
Discussion
280"
DISCUSSION,0.37354497354497357,"We have developed a systematic method for deriving a ridgelet transform for a wide range of learning
281"
DISCUSSION,0.3746031746031746,"machines defined by joint-group-equivariant feature maps, yielding the universal approximation
282"
DISCUSSION,0.37566137566137564,"theorems as corollaries. The previous results by Sonoda et al. [33] was limited to scalar-valued
283"
DISCUSSION,0.37671957671957673,"joint-invariant functions, which were insufficient to deal with practical learning machines defined by
284"
DISCUSSION,0.37777777777777777,"composite mappings of vector-valued functions, such as deep neural networks. For example, they
285"
DISCUSSION,0.37883597883597886,"could only deal with abstract composite structures like formal deep network [32]. By extending their
286"
DISCUSSION,0.3798941798941799,"argument to vector-valued joint-equivariant functions, we were able to deal with deep structures.
287"
DISCUSSION,0.38095238095238093,"Traditionally, the techniques used in the expressive power analysis of deep networks were different
288"
DISCUSSION,0.382010582010582,"from those used in the analysis of shallow networks, as overviewed in the introduction. Nonetheless,
289"
DISCUSSION,0.38306878306878306,"our main theorem cover both deep and shallow networks from the unified perspective (joint-group-
290"
DISCUSSION,0.38412698412698415,"action on the data-parameter domain). Technically, this unification is due to Schur’s lemma, a basic
291"
DISCUSSION,0.3851851851851852,"and useful result in the representation theory. Thanks to this lemma, the proof of the main theorem is
292"
DISCUSSION,0.3862433862433862,"simple, yet the scope of application is wide. The significance of this study lies in revealing the close
293"
DISCUSSION,0.3873015873015873,"relationship between machine learning theory and modern algebra. With this study as a catalyst, we
294"
DISCUSSION,0.38835978835978835,"expect a major upgrade to machine learning theory from the perspective of modern algebra.
295"
LIMITATIONS,0.38941798941798944,"6.1
Limitations
296"
LIMITATIONS,0.3904761904761905,"In the main theorem, we assume the following: (1) joint-equivariance of feature map ϕ, (2) bound-
297"
LIMITATIONS,0.3915343915343915,"edness of composite operator NN ◦R, (3) irreducibility of unitary representation π. In addition,
298"
LIMITATIONS,0.3925925925925926,"throughout this study, we assume (4) local compactness of group G, and (5) that the network is given
299"
LIMITATIONS,0.39365079365079364,"by the integral representation.
300"
LIMITATIONS,0.39470899470899473,"As discussed in the main text, satisfying (1) is much easier than (non-joint) equivariance. Also, (2) is
301"
LIMITATIONS,0.39576719576719577,"often a textbook excersise when the specific expression is given. (3) is required for Schur’s lemma, and
302"
LIMITATIONS,0.3968253968253968,"it is often sufficient to synthesize the known results such as the one for the example of depth-n fully-
303"
LIMITATIONS,0.3978835978835979,"connected network. (4) is quite a frequent assumption in the standard group representation theory, but
304"
LIMITATIONS,0.39894179894179893,"it excludes infinite-dimensional groups. When formulated natively, nonparametric learning models
305"
LIMITATIONS,0.4,"including DNN can be infinite-dimensional groups. However, from the perspective of learnability,
306"
LIMITATIONS,0.40105820105820106,"it is nonsense to consider too large a model, and it is common to assume regularity conditions
307"
LIMITATIONS,0.4021164021164021,"such as sparsity and low rank in usual theoretical analysis. So, it is natural to impose additional
308"
LIMITATIONS,0.4031746031746032,"regularity conditions for satisfying local compactness. (5) may be rather an advantage because
309"
LIMITATIONS,0.4042328042328042,"there are established techniques to show the cc-universaity of finite models by discretizing integral
310"
LIMITATIONS,0.4052910052910053,"representations. Moreover, there is a fast discretization scheme called the Barron’s rate based on the
311"
LIMITATIONS,0.40634920634920635,"quasi-Monte Carlo method. On the other hand, problems like the minimum width in the field of deep
312"
LIMITATIONS,0.4074074074074074,"narrow networks are analyses of finite parameters, and they could be a different type of parameters.
313"
LIMITATIONS,0.4084656084656085,"Yet, the current mainstream solutions are the information theoretic method by Park et al. [23] and the
314"
LIMITATIONS,0.4095238095238095,"neural ODE method by Cai [2], and both arguments contain the discretization of continuous models.
315"
LIMITATIONS,0.4105820105820106,"Therefore, we may expect a high affinity with the integral representation theory.
316"
LIMITATIONS,0.41164021164021164,"This study is the first step in extending the harmonic analysis method, which was previously applicable
317"
LIMITATIONS,0.4126984126984127,"only to shallow models, to deep models. The above limitations will be resolved in our future works.
318"
BROADER IMPACT,0.41375661375661377,"7
Broader Impact
319"
BROADER IMPACT,0.4148148148148148,"This work studies theoretical aspects of neural networks for expressing square integrable functions.
320"
BROADER IMPACT,0.4158730158730159,"Since we do not propose a new method nor a new dataset, we expect that the impact of this work on
321"
BROADER IMPACT,0.41693121693121693,"ethical aspects and future societal consequences will be small, if any. Our work can help understand
322"
BROADER IMPACT,0.41798941798941797,"the theoretical benefit and limitations of neural networks in approximating functions. Our work and
323"
BROADER IMPACT,0.41904761904761906,"the proof technique improve our understanding of the theoretical aspect of deep neural networks and
324"
BROADER IMPACT,0.4201058201058201,"other learning machines used in machine learning, and may lead to better use of these techniques
325"
BROADER IMPACT,0.4211640211640212,"with possible benefits to the society.
326"
REFERENCES,0.4222222222222222,"References
327"
REFERENCES,0.42328042328042326,"[1] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veliˇckovi´c. Geometric Deep Learning: Grids, Groups, Graphs,
328"
REFERENCES,0.42433862433862435,"Geodesics, and Gauges. arXiv preprint: 2104.13478, 2021.
329"
REFERENCES,0.4253968253968254,"[2] Y. Cai. Achieve the Minimum Width of Neural Networks for Universal Approximation. In The Eleventh
330"
REFERENCES,0.4264550264550265,"International Conference on Learning Representations, 2023.
331"
REFERENCES,0.4275132275132275,"[3] E. J. Candès. Ridgelets: theory and applications. PhD thesis, Standford University, 1998.
332"
REFERENCES,0.42857142857142855,"[4] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. Neural Ordinary Differential Equations. In
333"
REFERENCES,0.42962962962962964,"Advances in Neural Information Processing Systems, volume 31, pages 6572–6583, Palais des Congrès de
334"
REFERENCES,0.4306878306878307,"Montréal, Montréal CANADA, 2018.
335"
REFERENCES,0.43174603174603177,"[5] L. Chizat and F. Bach. On the Global Convergence of Gradient Descent for Over-parameterized Models
336"
REFERENCES,0.4328042328042328,"using Optimal Transport. In Advances in Neural Information Processing Systems 32, pages 3036–3046,
337"
REFERENCES,0.43386243386243384,"Montreal, BC, 2018.
338"
REFERENCES,0.43492063492063493,"[6] A. Cohen, R. DeVore, G. Petrova, and P. Wojtaszczyk. Optimal Stable Nonlinear Approximation. Founda-
339"
REFERENCES,0.43597883597883597,"tions of Computational Mathematics, 22(3):607–648, 2022.
340"
REFERENCES,0.43703703703703706,"[7] N. Cohen, O. Sharir, and A. Shashua. On the Expressive Power of Deep Learning: A Tensor Analysis. In
341"
REFERENCES,0.4380952380952381,"29th Annual Conference on Learning Theory, volume 49, pages 1–31, 2016.
342"
REFERENCES,0.43915343915343913,"[8] I. Daubechies, R. DeVore, S. Foucart, B. Hanin, and G. Petrova. Nonlinear Approximation and (Deep)
343"
REFERENCES,0.4402116402116402,"ReLU Networks. Constructive Approximation, 55(1):127–172, 2022.
344"
REFERENCES,0.44126984126984126,"[9] W. E. A Proposal on Machine Learning via Dynamical Systems. Communications in Mathematics and
345"
REFERENCES,0.44232804232804235,"Statistics, 5(1):1–11, 2017.
346"
REFERENCES,0.4433862433862434,"[10] G. B. Folland. A Course in Abstract Harmonic Analysis. Chapman and Hall/CRC, New York, second
347"
REFERENCES,0.4444444444444444,"edition, 2015.
348"
REFERENCES,0.4455026455026455,"[11] P. Grohs, A. Klotz, and F. Voigtlaender. Phase Transitions in Rate Distortion Theory and Deep Learning.
349"
REFERENCES,0.44656084656084655,"Foundations of Computational Mathematics, 23(1):329–392, 2023.
350"
REFERENCES,0.44761904761904764,"[12] E. Haber and L. Ruthotto. Stable architectures for deep neural networks. Inverse Problems, 34(1):1–22,
351"
REFERENCES,0.4486772486772487,"2017.
352"
REFERENCES,0.4497354497354497,"[13] B. Hanin and M. Sellke. Approximating Continuous Functions by ReLU Nets of Minimal Width. arXiv
353"
REFERENCES,0.4507936507936508,"preprint: 1710.11278, 2017.
354"
REFERENCES,0.45185185185185184,"[14] P. Kidger and T. Lyons. Universal Approximation with Deep Narrow Networks. In Proceedings of Thirty
355"
REFERENCES,0.45291005291005293,"Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pages
356"
REFERENCES,0.45396825396825397,"2306–2327. PMLR, 2020.
357"
REFERENCES,0.455026455026455,"[15] N. Kim, C. Min, and S. Park. Minimum width for universal approximation using ReLU networks on
358"
REFERENCES,0.4560846560846561,"compact domain. In The Twelfth International Conference on Learning Representations, 2024.
359"
REFERENCES,0.45714285714285713,"[16] L. Li, Y. Duan, G. Ji, and Y. Cai. Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal
360"
REFERENCES,0.4582010582010582,"Approximation. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of
361"
REFERENCES,0.45925925925925926,"Proceedings of Machine Learning Research, pages 19460–19470, 2023.
362"
REFERENCES,0.4603174603174603,"[17] Q. Li and S. Hao. An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight
363"
REFERENCES,0.4613756613756614,"Neural Networks. In Proceedings of The 35th International Conference on Machine Learning, volume 80,
364"
REFERENCES,0.4624338624338624,"pages 2985–2994, Stockholm, 2018. PMLR.
365"
REFERENCES,0.4634920634920635,"[18] H. Lin and S. Jegelka. ResNet with one-neuron hidden layers is a Universal Approximator. In Advances in
366"
REFERENCES,0.46455026455026455,"Neural Information Processing Systems, volume 31, Montreal, BC, 2018.
367"
REFERENCES,0.4656084656084656,"[19] Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The Expressive Power of Neural Networks: A View from the
368"
REFERENCES,0.4666666666666667,"Width. In Advances in Neural Information Processing Systems, volume 30, 2017.
369"
REFERENCES,0.4677248677248677,"[20] S. Mei, A. Montanari, and P.-M. Nguyen. A mean field view of the landscape of two-layer neural networks.
370"
REFERENCES,0.4687830687830688,"Proceedings of the National Academy of Sciences, 115(33):E7665–E7671, 2018.
371"
REFERENCES,0.46984126984126984,"[21] N. Murata. An integral representation of functions using three-layered networks and their approximation
372"
REFERENCES,0.4708994708994709,"bounds. Neural Networks, 9(6):947–956, 1996.
373"
REFERENCES,0.47195767195767196,"[22] A. Nitanda and T. Suzuki. Stochastic Particle Gradient Descent for Infinite Ensembles. arXiv preprint:
374"
REFERENCES,0.473015873015873,"1712.05438, 2017.
375"
REFERENCES,0.4740740740740741,"[23] S. Park, C. Yun, J. Lee, and J. Shin. Minimum Width for Universal Approximation. In International
376"
REFERENCES,0.47513227513227513,"Conference on Learning Representations, 2021.
377"
REFERENCES,0.47619047619047616,"[24] G. Petrova and P. Wojtaszczyk. Limitations on approximation by deep and shallow neural networks.
378"
REFERENCES,0.47724867724867726,"Journal of Machine Learning Research, 24(353):1–38, 2023.
379"
REFERENCES,0.4783068783068783,"[25] G. Rotskoff and E. Vanden-Eijnden. Parameters as interacting particles: long time convergence and
380"
REFERENCES,0.4793650793650794,"asymptotic error scaling of neural networks. In Advances in Neural Information Processing Systems 31,
381"
REFERENCES,0.4804232804232804,"pages 7146–7155, Montreal, BC, 2018.
382"
REFERENCES,0.48148148148148145,"[26] J. W. Siegel. Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov
383"
REFERENCES,0.48253968253968255,"Spaces. Journal of Machine Learning Research, 24(357):1–52, 2023.
384"
REFERENCES,0.4835978835978836,"[27] S. Sonoda and N. Murata. Transportation analysis of denoising autoencoders: a novel method for analyzing
385"
REFERENCES,0.4846560846560847,"deep neural networks. In NIPS 2017 Workshop on Optimal Transport & Machine Learning (OTML), pages
386"
REFERENCES,0.4857142857142857,"1–10, Long Beach, 2017.
387"
REFERENCES,0.48677248677248675,"[28] S. Sonoda, I. Ishikawa, and M. Ikeda. Ridge Regression with Over-Parametrized Two-Layer Networks
388"
REFERENCES,0.48783068783068784,"Converge to Ridgelet Spectrum. In Proceedings of The 24th International Conference on Artificial
389"
REFERENCES,0.4888888888888889,"Intelligence and Statistics (AISTATS) 2021, volume 130, pages 2674–2682. PMLR, 2021.
390"
REFERENCES,0.48994708994708996,"[29] S. Sonoda, I. Ishikawa, and M. Ikeda. Ghosts in Neural Networks: Existence, Structure and Role of
391"
REFERENCES,0.491005291005291,"Infinite-Dimensional Null Space. arXiv preprint: 2106.04770, 2021.
392"
REFERENCES,0.49206349206349204,"[30] S. Sonoda, I. Ishikawa, and M. Ikeda. Universality of Group Convolutional Neural Networks Based
393"
REFERENCES,0.4931216931216931,"on Ridgelet Analysis on Groups. In Advances in Neural Information Processing Systems 35, pages
394"
REFERENCES,0.49417989417989416,"38680–38694, New Orleans, Louisiana, USA, 2022.
395"
REFERENCES,0.49523809523809526,"[31] S. Sonoda, I. Ishikawa, and M. Ikeda. Fully-Connected Network on Noncompact Symmetric Space
396"
REFERENCES,0.4962962962962963,"and Ridgelet Transform based on Helgason-Fourier Analysis. In Proceedings of the 39th International
397"
REFERENCES,0.4973544973544973,"Conference on Machine Learning, volume 162, pages 20405–20422, Baltimore, Maryland, USA, 2022.
398"
REFERENCES,0.4984126984126984,"[32] S. Sonoda, Y. Hashimoto, I. Ishikawa, and M. Ikeda. Deep Ridgelet Transform: Voice with Koopman
399"
REFERENCES,0.49947089947089945,"Operator Proves Universality of Formal Deep Networks. In Proceedings of the 2nd NeurIPS Workshop on
400"
REFERENCES,0.5005291005291005,"Symmetry and Geometry in Neural Representations, Proceedings of Machine Learning Research. PMLR,
401"
REFERENCES,0.5015873015873016,"2023.
402"
REFERENCES,0.5026455026455027,"[33] S. Sonoda, H. Ishi, I. Ishikawa, and M. Ikeda. Joint Group Invariant Functions on Data-Parameter Domain
403"
REFERENCES,0.5037037037037037,"Induce Universal Neural Networks. In Proceedings of the 2nd NeurIPS Workshop on Symmetry and
404"
REFERENCES,0.5047619047619047,"Geometry in Neural Representations, Proceedings of Machine Learning Research. PMLR, 2023.
405"
REFERENCES,0.5058201058201058,"[34] S. Sonoda, I. Ishikawa, and M. Ikeda. A unified Fourier slice method to derive ridgelet transform for a
406"
REFERENCES,0.5068783068783069,"variety of depth-2 neural networks. Journal of Statistical Planning and Inference, 233:106184, 2024.
407"
REFERENCES,0.5079365079365079,"[35] T. Suzuki. Generalization bound of globally optimal non-convex neural network training: Transportation
408"
REFERENCES,0.508994708994709,"map estimation by infinite dimensional Langevin dynamics. In Advances in Neural Information Processing
409"
REFERENCES,0.5100529100529101,"Systems 33, pages 19224–19237, 2020.
410"
REFERENCES,0.5111111111111111,"[36] M. Telgarsky. Benefits of depth in neural networks. In 29th Annual Conference on Learning Theory, pages
411"
REFERENCES,0.5121693121693122,"1–23, 2016.
412"
REFERENCES,0.5132275132275133,"[37] D. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks, 94:103–114,
413"
REFERENCES,0.5142857142857142,"2017.
414"
REFERENCES,0.5153439153439153,"[38] D. Yarotsky. Optimal approximation of continuous functions by very deep ReLU networks. In Proceedings
415"
REFERENCES,0.5164021164021164,"of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research,
416"
REFERENCES,0.5174603174603175,"pages 639–649. PMLR, 2018.
417"
REFERENCES,0.5185185185185185,"[39] D. Yarotsky and A. Zhevnerchuk. The phase diagram of approximation rates for deep neural networks. In
418"
REFERENCES,0.5195767195767196,"Advances in Neural Information Processing Systems, volume 33, pages 13005–13015, 2020.
419"
REFERENCES,0.5206349206349207,"A
Depth-2 Fully-Connected Neural Network and Ridgelet Transform
420"
REFERENCES,0.5216931216931217,"A non group theoretic proof by reducing to a Fourier expression is given in Sonoda et al. [29,
421"
REFERENCES,0.5227513227513227,"Theorem 6].
422"
REFERENCES,0.5238095238095238,"A.1
Proof
423"
REFERENCES,0.5248677248677248,"In the following, we identify the group G acting on data domain Rm with the affine group Aff(Rm),
424"
REFERENCES,0.5259259259259259,"and introduce the so-called twisted dual group action that leaves a function θ invariant. Then, we see
425"
REFERENCES,0.526984126984127,"the regular action π of G on functions space L2(Rm) commutes with composite Sσ ◦Rρ. Hence, by
426"
REFERENCES,0.5280423280423281,"Schur’s lemma, Sσ ◦Rρ is a constant multiple of identity, which concludes the assertion.
427"
REFERENCES,0.5291005291005291,"Proof. Let G be the affine group Aff(Rm) = GL(Rm) ⋉Rm. For any g = (L, t) ∈G, let
428"
REFERENCES,0.5301587301587302,"g · x := Lx + t,
x ∈Rm
(30)"
REFERENCES,0.5312169312169313,"be its action on Rm, and let
429"
REFERENCES,0.5322751322751322,π(g)[f](x) := | det L|−1/2f(g−1 · x)
REFERENCES,0.5333333333333333,"= | det L|−1/2f(L−1(x −t)),
f ∈L2(Rm)
(31)"
REFERENCES,0.5343915343915344,"be its left-regular action on L2(Rm).
430"
REFERENCES,0.5354497354497354,"Besides, putting
431"
REFERENCES,0.5365079365079365,"θ((a, b), x) := a · x −b,
(a, b) ∈Rm × R, x ∈Rm
(32)"
REFERENCES,0.5375661375661376,"we define the twisted dual action of G on Rm × R as
432"
REFERENCES,0.5386243386243387,"g · (a, b) := (L−⊤a, b + a · (L−1t)),
(a, b) ∈Rm × R
(33)"
REFERENCES,0.5396825396825397,"so that the following invariance hold:
433"
REFERENCES,0.5407407407407407,"θ(g · (a, b), g · x) = θ((a, b), x) = a · x −b.
(34)"
REFERENCES,0.5417989417989418,"To see this, use matrix expressions with extended variables
434"
REFERENCES,0.5428571428571428,"θ((a, b), x) =
 
a⊤
b
 
Im
0
0
−1"
REFERENCES,0.5439153439153439," 
x
1"
REFERENCES,0.544973544973545,"
=: ˜a⊤˜I ˜x,
(35)"
REFERENCES,0.546031746031746,"g
g · x :=

g · x
1"
REFERENCES,0.5470899470899471,"
=

L
t
0
1"
REFERENCES,0.5481481481481482," 
x
1"
REFERENCES,0.5492063492063493,"
=: ˜L˜x
(36)"
REFERENCES,0.5502645502645502,"and calculate
435"
REFERENCES,0.5513227513227513,"˜a⊤˜I ˜x = (˜a⊤˜I ˜L−1 ˜I−1)˜I(˜L˜x) = (˜I ˜L−⊤˜I˜a)⊤˜I(˜L˜x),
(37)"
REFERENCES,0.5523809523809524,"which suggests
^
g · (a, b) := ˜I ˜L−⊤˜I˜a, and we have
436"
REFERENCES,0.5534391534391534,"˜I ˜L−⊤˜I =

Im
0
0
−1"
REFERENCES,0.5544973544973545," 
L
t
0
1"
REFERENCES,0.5555555555555556,"−⊤
Im
0
0
−1 "
REFERENCES,0.5566137566137566,"=

Im
0
0
−1"
REFERENCES,0.5576719576719577," 
L−⊤
0
−t⊤L−⊤
1"
REFERENCES,0.5587301587301587," 
Im
0
0
−1"
REFERENCES,0.5597883597883598,"
=

L−⊤
0
t⊤L−⊤
1 
."
REFERENCES,0.5608465608465608,"Further, we define its regular-action by
437"
REFERENCES,0.5619047619047619,"bπ(g)[γ](a, b) := | det L|1/2γ(g−1 · (a, b))"
REFERENCES,0.562962962962963,"= | det L|1/2γ(L⊤a, b −a · t),
(a, b) ∈Rm × R.
(38)"
REFERENCES,0.564021164021164,"Then we can see that, for all g = (L, t) ∈G,
438"
REFERENCES,0.5650793650793651,"Rρ ◦π(g) = bπ(g) ◦Rρ,
and
Sσ ◦bπ(g) = π(g) ◦Sσ.
(39)"
REFERENCES,0.5661375661375662,"In fact, at every g = (L, t) ∈G and (a, b) ∈Rm × R,
439"
REFERENCES,0.5671957671957671,"Rρ[π(g)[f]](a, b) = | det L|−1/2
Z"
REFERENCES,0.5682539682539682,"Rm f(g−1 · x)ρ(θ((a, b), x))dx"
REFERENCES,0.5693121693121693,"by putting x = g · y = Ly + t with dx = | det L|dy,
440"
REFERENCES,0.5703703703703704,"= | det L|1/2
Z"
REFERENCES,0.5714285714285714,"Rm f(y)ρ(θ((a, b), g · y)))dy"
REFERENCES,0.5724867724867725,"= | det L|1/2
Z"
REFERENCES,0.5735449735449736,"Rm f(y)ρ(θ(g−1 · (a, b), y)))dy"
REFERENCES,0.5746031746031746,"= bπ(g)[Rρ[f]](a, b).
(40)"
REFERENCES,0.5756613756613757,"Similarly, at every g = (L, t) ∈G and x ∈Rm,
441"
REFERENCES,0.5767195767195767,"Sσ[bπ(g)[γ]](x) = | det L|1/2
Z"
REFERENCES,0.5777777777777777,"Rm×R
γ(g−1 · (a, b))σ(θ((a, b), x))dadb"
REFERENCES,0.5788359788359788,"by putting (a, b) := g · (ξ, η) = (L−⊤ξ, η + ξ · (L−1t)) with dadb = | det L|dξdη,
442"
REFERENCES,0.5798941798941799,"= | det L|−1/2
Z"
REFERENCES,0.580952380952381,"Rm×R
γ(ξ, η)σ(θ(g · (ξ, η), x))dξdη"
REFERENCES,0.582010582010582,"= | det L|−1/2
Z"
REFERENCES,0.5830687830687831,"Rm×R
γ(ξ, η)σ(θ((ξ, η), g−1 · x))dξdη"
REFERENCES,0.5841269841269842,"= π(g)[Sσ[γ]](x).
(41)"
REFERENCES,0.5851851851851851,"Hence Sσ ◦Rρ commutes with π(g) because
443"
REFERENCES,0.5862433862433862,Sσ ◦Rρ ◦π(g) = Sσ ◦bπ(g) ◦Rρ = π(g) ◦Sσ ◦Rρ.
REFERENCES,0.5873015873015873,"Since Sσ ◦Rρ : L2(Rm) →L2(Rm) is bounded (Lemma 4), and (π, L2(Rm)) is an irreducible
444"
REFERENCES,0.5883597883597883,"unitary representation of G (Lemma 3), Schur’s lemma (Theorem 2) yields that there exist a constant
445"
REFERENCES,0.5894179894179894,"Cσ,ρ ∈C such that
446"
REFERENCES,0.5904761904761905,"Sσ ◦Rρ[f] = Cσ,ρf
(42)"
REFERENCES,0.5915343915343916,"for all f ∈L2(Rm).
447"
REFERENCES,0.5925925925925926,"Finally, by directly computing the left-hand-side, namely Sσ ◦Rρ[f], we can verify that the constant
448"
REFERENCES,0.5936507936507937,"Cσ,ρ is given by
449"
REFERENCES,0.5947089947089947,"Cσ,ρ = ((σ, ρ)) := (2π)m−1
Z"
REFERENCES,0.5957671957671957,"R
σ♯(ω)ρ♯(ω)|ω|−mdω.
(43) 450"
REFERENCES,0.5968253968253968,"A.2
Proof for (33)
451"
REFERENCES,0.5978835978835979,"Use matrix expressions with extended variables
452"
REFERENCES,0.5989417989417989,"θ((a, b), x) =
 
a⊤
b
 
Im
0
0
−1"
REFERENCES,0.6," 
x
1"
REFERENCES,0.6010582010582011,"
=: ˜a⊤˜I ˜x,
(44)"
REFERENCES,0.6021164021164022,"g
g · x :=

g · x
1"
REFERENCES,0.6031746031746031,"
=

L
t
0
1"
REFERENCES,0.6042328042328042," 
x
1"
REFERENCES,0.6052910052910053,"
=: ˜L˜x
(45)"
REFERENCES,0.6063492063492063,"and calculate
453"
REFERENCES,0.6074074074074074,"˜a⊤˜I ˜x = (˜a⊤˜I ˜L−1 ˜I−1)˜I(˜L˜x) = (˜I ˜L−⊤˜I˜a)⊤˜I(˜L˜x),
(46)"
REFERENCES,0.6084656084656085,"which suggests
^
g · (a, b) := ˜I ˜L−⊤˜I˜a, and we have
454"
REFERENCES,0.6095238095238096,"˜I ˜L−⊤˜I =

Im
0
0
−1"
REFERENCES,0.6105820105820106," 
L
t
0
1"
REFERENCES,0.6116402116402117,"−⊤
Im
0
0
−1 "
REFERENCES,0.6126984126984127,"=

Im
0
0
−1"
REFERENCES,0.6137566137566137," 
L−⊤
0
−t⊤L−⊤
1"
REFERENCES,0.6148148148148148," 
Im
0
0
−1"
REFERENCES,0.6158730158730159,"
=

L−⊤
0
t⊤L−⊤
1 
."
REFERENCES,0.6169312169312169,"A.3
Proof for (39)
455"
REFERENCES,0.617989417989418,"In fact, at every g = (L, t) ∈G and (a, b) ∈Rm × R,
456"
REFERENCES,0.6190476190476191,"Rρ[π(g)[f]](a, b) = | det L|−1/2
Z"
REFERENCES,0.6201058201058202,"Rm f(g−1 · x)ρ(θ((a, b), x))dx"
REFERENCES,0.6211640211640211,"by putting x = g · y = Ly + t with dx = | det L|dy,
457"
REFERENCES,0.6222222222222222,"= | det L|1/2
Z"
REFERENCES,0.6232804232804233,"Rm f(y)ρ(θ((a, b), g · y)))dy"
REFERENCES,0.6243386243386243,"= | det L|1/2
Z"
REFERENCES,0.6253968253968254,"Rm f(y)ρ(θ(g−1 · (a, b), y)))dy"
REFERENCES,0.6264550264550265,"= bπ(g)[Rρ[f]](a, b).
(47)"
REFERENCES,0.6275132275132275,"Similarly, at every g = (L, t) ∈G and x ∈Rm,
458"
REFERENCES,0.6285714285714286,"Sσ[bπ(g)[γ]](x) = | det L|1/2
Z"
REFERENCES,0.6296296296296297,"Rm×R
γ(g−1 · (a, b))σ(θ((a, b), x))dadb"
REFERENCES,0.6306878306878307,"by putting (a, b) := g · (ξ, η) = (L−⊤ξ, η + ξ · (L−1t)) with dadb = | det L|dξdη,
459"
REFERENCES,0.6317460317460317,"= | det L|−1/2
Z"
REFERENCES,0.6328042328042328,"Rm×R
γ(ξ, η)σ(θ(g · (ξ, η), x))dξdη"
REFERENCES,0.6338624338624339,"= | det L|−1/2
Z"
REFERENCES,0.6349206349206349,"Rm×R
γ(ξ, η)σ(θ((ξ, η), g−1 · x))dξdη"
REFERENCES,0.635978835978836,"= π(g)[Sσ[γ]](x).
(48) 460"
REFERENCES,0.6370370370370371,"B
Geometric Interpretation of Dual Action for Original Ridgelet Transform
461"
REFERENCES,0.638095238095238,"We explain a geometric interpretation of the dual action (33) in the previous section. We note that
462"
REFERENCES,0.6391534391534391,"in general θ does not require any geometric interpretation as long as it is joint group invariant on
463"
REFERENCES,0.6402116402116402,"data-parameter domain.
464"
REFERENCES,0.6412698412698413,"For each (a, b) ∈Rm × R, put ξ(a, b) := {x ∈Rm | a · x −b = 0}. Then it is a hyperplane in Rm
465"
REFERENCES,0.6423280423280423,"through point x0 = ba/|a|2 with normal vector u := a/|a|.
466 o"
REFERENCES,0.6433862433862434,"ξ(a, b) u y0 y"
REFERENCES,0.6444444444444445,"ξ(a, a · x) x x0
yx"
REFERENCES,0.6455026455026455,"Figure 3: The invariant ϕ((a, b), x) = σ(a · x −b) is the euclidean distance between point x and
hyperplane ξ(a, b) followed by scaling and nonlinearity σ"
REFERENCES,0.6465608465608466,"For any point y in the hyperplane ξ(a, b), by definition a · y = b, thus
467"
REFERENCES,0.6476190476190476,"a · x −b = a · (x −y).
(49)"
REFERENCES,0.6486772486772486,"But this means a · x −b is a scaled distance between point x and hyperplane ξ(a, b),
468"
REFERENCES,0.6497354497354497,"= |a|dE(x, ξ(a, b)),
(50)"
REFERENCES,0.6507936507936508,"and further a scaled distance between hyperplanes ξ(a, a · x) through x with normal a/|a| and
ξ(a, b),
469"
REFERENCES,0.6518518518518519,"= |a|dE(ξ(a, a · x), ξ(a, b)).
(51)"
REFERENCES,0.6529100529100529,"Now, we can interpret the invariant θ((a, b), x) := a · x −b in a geometric manner, that is, it is the
470"
REFERENCES,0.653968253968254,"distance between point and hyperplane, or between hyperplanes. We note that we can regard entire
471"
REFERENCES,0.6550264550264551,"σ(a · x −b)—the distance modulated by both scaling and nonlinearity—as the invariant, say ϕ.
472"
REFERENCES,0.656084656084656,"Furthermore, the dual action g · (a, b) is understood as a parallel translation of hyperplane ξ(a, b) to
473"
REFERENCES,0.6571428571428571,"ξ(g · (a, b)) so as to leave the scaled distance θ invariant, namely
474"
REFERENCES,0.6582010582010582,"dE(g · x, g · ξ(a, b)) = dE(x, ξ(a, b)).
(52)"
REFERENCES,0.6592592592592592,"Indeed, for any g = (L, t) ∈G,
475"
REFERENCES,0.6603174603174603,"g · ξ(a, b) = {g · x | a · x −b = 0}"
REFERENCES,0.6613756613756614,"= {y | a · (g−1 · y) −b = 0}
(by letting y = g · x)"
REFERENCES,0.6624338624338625,"= {y | (L−⊤) · y −(b + a · (L−1t)) = 0}
= ξ(g · (a, b)),"
REFERENCES,0.6634920634920635,"meaning that the hyperplane with parameter (a, b) translated by g is identical to the hyperplane with
476"
REFERENCES,0.6645502645502646,"parameter g · (a, b).
477"
REFERENCES,0.6656084656084656,"To summarize, in the case of fully-connected neural network (and its corresponding ridgelet trans-
478"
REFERENCES,0.6666666666666666,"form), the invariant is a modulated distance σ(a · x −b), and the dual action is the parallel translation
479"
REFERENCES,0.6677248677248677,"of hyperplane so as to keep the distance invariant. Further, from this geometric perspective, we can
480"
REFERENCES,0.6687830687830688,"rewrite the fully-connected neural network in a geometric manner as
481"
REFERENCES,0.6698412698412698,"S[γ](x) :=
Z"
REFERENCES,0.6708994708994709,"R×Ξ
γ(ξ)σ(adE(x, ξ))dadξ,
(53)"
REFERENCES,0.671957671957672,"where a ∈R denotes signed scale and Ξ denotes the space of all hyperplanes (not always through
482"
REFERENCES,0.6730158730158731,"the origin). Since each hyperplane is parametrized by normal vectors u ∈m−1 and distance p ≥0
483"
REFERENCES,0.674074074074074,"from the origin, we can induce the product of spherical measure du and Lebesgue measure dp as a
484"
REFERENCES,0.6751322751322751,"measure dξ on the space Ξ of hyperplanes.
485"
REFERENCES,0.6761904761904762,"NeurIPS Paper Checklist
486"
CLAIMS,0.6772486772486772,"1. Claims
487"
CLAIMS,0.6783068783068783,"Question: Do the main claims made in the abstract and introduction accurately reflect the
488"
CLAIMS,0.6793650793650794,"paper’s contributions and scope?
489"
CLAIMS,0.6804232804232804,"Answer: [Yes]
490"
CLAIMS,0.6814814814814815,"Justification: Theorem 3 and Corollary 1
491"
CLAIMS,0.6825396825396826,"Guidelines:
492"
CLAIMS,0.6835978835978836,"• The answer NA means that the abstract and introduction do not include the claims
493"
CLAIMS,0.6846560846560846,"made in the paper.
494"
CLAIMS,0.6857142857142857,"• The abstract and/or introduction should clearly state the claims made, including the
495"
CLAIMS,0.6867724867724868,"contributions made in the paper and important assumptions and limitations. A No or
496"
CLAIMS,0.6878306878306878,"NA answer to this question will not be perceived well by the reviewers.
497"
CLAIMS,0.6888888888888889,"• The claims made should match theoretical and experimental results, and reflect how
498"
CLAIMS,0.68994708994709,"much the results can be expected to generalize to other settings.
499"
CLAIMS,0.691005291005291,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
500"
CLAIMS,0.692063492063492,"are not attained by the paper.
501"
LIMITATIONS,0.6931216931216931,"2. Limitations
502"
LIMITATIONS,0.6941798941798942,"Question: Does the paper discuss the limitations of the work performed by the authors?
503"
LIMITATIONS,0.6952380952380952,"Answer: [Yes]
504"
LIMITATIONS,0.6962962962962963,"Justification: § 6.1
505"
LIMITATIONS,0.6973544973544974,"Guidelines:
506"
LIMITATIONS,0.6984126984126984,"• The answer NA means that the paper has no limitation while the answer No means that
507"
LIMITATIONS,0.6994708994708995,"the paper has limitations, but those are not discussed in the paper.
508"
LIMITATIONS,0.7005291005291006,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
509"
LIMITATIONS,0.7015873015873015,"• The paper should point out any strong assumptions and how robust the results are to
510"
LIMITATIONS,0.7026455026455026,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
511"
LIMITATIONS,0.7037037037037037,"model well-specification, asymptotic approximations only holding locally). The authors
512"
LIMITATIONS,0.7047619047619048,"should reflect on how these assumptions might be violated in practice and what the
513"
LIMITATIONS,0.7058201058201058,"implications would be.
514"
LIMITATIONS,0.7068783068783069,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
515"
LIMITATIONS,0.707936507936508,"only tested on a few datasets or with a few runs. In general, empirical results often
516"
LIMITATIONS,0.708994708994709,"depend on implicit assumptions, which should be articulated.
517"
LIMITATIONS,0.71005291005291,"• The authors should reflect on the factors that influence the performance of the approach.
518"
LIMITATIONS,0.7111111111111111,"For example, a facial recognition algorithm may perform poorly when image resolution
519"
LIMITATIONS,0.7121693121693121,"is low or images are taken in low lighting. Or a speech-to-text system might not be
520"
LIMITATIONS,0.7132275132275132,"used reliably to provide closed captions for online lectures because it fails to handle
521"
LIMITATIONS,0.7142857142857143,"technical jargon.
522"
LIMITATIONS,0.7153439153439154,"• The authors should discuss the computational efficiency of the proposed algorithms
523"
LIMITATIONS,0.7164021164021164,"and how they scale with dataset size.
524"
LIMITATIONS,0.7174603174603175,"• If applicable, the authors should discuss possible limitations of their approach to
525"
LIMITATIONS,0.7185185185185186,"address problems of privacy and fairness.
526"
LIMITATIONS,0.7195767195767195,"• While the authors might fear that complete honesty about limitations might be used by
527"
LIMITATIONS,0.7206349206349206,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
528"
LIMITATIONS,0.7216931216931217,"limitations that aren’t acknowledged in the paper. The authors should use their best
529"
LIMITATIONS,0.7227513227513227,"judgment and recognize that individual actions in favor of transparency play an impor-
530"
LIMITATIONS,0.7238095238095238,"tant role in developing norms that preserve the integrity of the community. Reviewers
531"
LIMITATIONS,0.7248677248677249,"will be specifically instructed to not penalize honesty concerning limitations.
532"
THEORY ASSUMPTIONS AND PROOFS,0.725925925925926,"3. Theory Assumptions and Proofs
533"
THEORY ASSUMPTIONS AND PROOFS,0.726984126984127,"Question: For each theoretical result, does the paper provide the full set of assumptions and
534"
THEORY ASSUMPTIONS AND PROOFS,0.728042328042328,"a complete (and correct) proof?
535"
THEORY ASSUMPTIONS AND PROOFS,0.7291005291005291,"Answer: [Yes]
536"
THEORY ASSUMPTIONS AND PROOFS,0.7301587301587301,"Justification: We put the proof right after Theorem 3
537"
THEORY ASSUMPTIONS AND PROOFS,0.7312169312169312,"Guidelines:
538"
THEORY ASSUMPTIONS AND PROOFS,0.7322751322751323,"• The answer NA means that the paper does not include theoretical results.
539"
THEORY ASSUMPTIONS AND PROOFS,0.7333333333333333,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
540"
THEORY ASSUMPTIONS AND PROOFS,0.7343915343915344,"referenced.
541"
THEORY ASSUMPTIONS AND PROOFS,0.7354497354497355,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
542"
THEORY ASSUMPTIONS AND PROOFS,0.7365079365079366,"• The proofs can either appear in the main paper or the supplemental material, but if
543"
THEORY ASSUMPTIONS AND PROOFS,0.7375661375661375,"they appear in the supplemental material, the authors are encouraged to provide a short
544"
THEORY ASSUMPTIONS AND PROOFS,0.7386243386243386,"proof sketch to provide intuition.
545"
THEORY ASSUMPTIONS AND PROOFS,0.7396825396825397,"• Inversely, any informal proof provided in the core of the paper should be complemented
546"
THEORY ASSUMPTIONS AND PROOFS,0.7407407407407407,"by formal proofs provided in appendix or supplemental material.
547"
THEORY ASSUMPTIONS AND PROOFS,0.7417989417989418,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7428571428571429,"4. Experimental Result Reproducibility
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7439153439153439,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.744973544973545,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.746031746031746,"of the paper (regardless of whether the code and data are provided or not)?
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7470899470899471,"Answer: [NA]
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7481481481481481,"Justification: This study does not include experiments.
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7492063492063492,"Guidelines:
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7502645502645503,"• The answer NA means that the paper does not include experiments.
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7513227513227513,"• If the paper includes experiments, a No answer to this question will not be perceived
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7523809523809524,"well by the reviewers: Making the paper reproducible is important, regardless of
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7534391534391535,"whether the code and data are provided or not.
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7544973544973544,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7555555555555555,"to make their results reproducible or verifiable.
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7566137566137566,"• Depending on the contribution, reproducibility can be accomplished in various ways.
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7576719576719577,"For example, if the contribution is a novel architecture, describing the architecture fully
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7587301587301587,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7597883597883598,"be necessary to either make it possible for others to replicate the model with the same
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7608465608465609,"dataset, or provide access to the model. In general. releasing code and data is often
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7619047619047619,"one good way to accomplish this, but reproducibility can also be provided via detailed
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.762962962962963,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.764021164021164,"of a large language model), releasing of a model checkpoint, or other means that are
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.765079365079365,"appropriate to the research performed.
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7661375661375661,"• While NeurIPS does not require releasing code, the conference does require all submis-
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7671957671957672,"sions to provide some reasonable avenue for reproducibility, which may depend on the
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7682539682539683,"nature of the contribution. For example
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7693121693121693,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7703703703703704,"to reproduce that algorithm.
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7714285714285715,"(b) If the contribution is primarily a new model architecture, the paper should describe
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7724867724867724,"the architecture clearly and fully.
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7735449735449735,"(c) If the contribution is a new model (e.g., a large language model), then there should
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7746031746031746,"either be a way to access this model for reproducing the results or a way to reproduce
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7756613756613756,"the model (e.g., with an open-source dataset or instructions for how to construct
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7767195767195767,"the dataset).
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7777777777777778,"(d) We recognize that reproducibility may be tricky in some cases, in which case
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7788359788359789,"authors are welcome to describe the particular way they provide for reproducibility.
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7798941798941799,"In the case of closed-source models, it may be that access to the model is limited in
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.780952380952381,"some way (e.g., to registered users), but it should be possible for other researchers
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.782010582010582,"to have some path to reproducing or verifying the results.
586"
OPEN ACCESS TO DATA AND CODE,0.783068783068783,"5. Open access to data and code
587"
OPEN ACCESS TO DATA AND CODE,0.7841269841269841,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
588"
OPEN ACCESS TO DATA AND CODE,0.7851851851851852,"tions to faithfully reproduce the main experimental results, as described in supplemental
589"
OPEN ACCESS TO DATA AND CODE,0.7862433862433862,"material?
590"
OPEN ACCESS TO DATA AND CODE,0.7873015873015873,"Answer: [NA] .
591"
OPEN ACCESS TO DATA AND CODE,0.7883597883597884,"Justification: This study does not include experiments.
592"
OPEN ACCESS TO DATA AND CODE,0.7894179894179895,"Guidelines:
593"
OPEN ACCESS TO DATA AND CODE,0.7904761904761904,"• The answer NA means that paper does not include experiments requiring code.
594"
OPEN ACCESS TO DATA AND CODE,0.7915343915343915,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
595"
OPEN ACCESS TO DATA AND CODE,0.7925925925925926,"public/guides/CodeSubmissionPolicy) for more details.
596"
OPEN ACCESS TO DATA AND CODE,0.7936507936507936,"• While we encourage the release of code and data, we understand that this might not be
597"
OPEN ACCESS TO DATA AND CODE,0.7947089947089947,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
598"
OPEN ACCESS TO DATA AND CODE,0.7957671957671958,"including code, unless this is central to the contribution (e.g., for a new open-source
599"
OPEN ACCESS TO DATA AND CODE,0.7968253968253968,"benchmark).
600"
OPEN ACCESS TO DATA AND CODE,0.7978835978835979,"• The instructions should contain the exact command and environment needed to run to
601"
OPEN ACCESS TO DATA AND CODE,0.798941798941799,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
602"
OPEN ACCESS TO DATA AND CODE,0.8,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
603"
OPEN ACCESS TO DATA AND CODE,0.801058201058201,"• The authors should provide instructions on data access and preparation, including how
604"
OPEN ACCESS TO DATA AND CODE,0.8021164021164021,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
605"
OPEN ACCESS TO DATA AND CODE,0.8031746031746032,"• The authors should provide scripts to reproduce all experimental results for the new
606"
OPEN ACCESS TO DATA AND CODE,0.8042328042328042,"proposed method and baselines. If only a subset of experiments are reproducible, they
607"
OPEN ACCESS TO DATA AND CODE,0.8052910052910053,"should state which ones are omitted from the script and why.
608"
OPEN ACCESS TO DATA AND CODE,0.8063492063492064,"• At submission time, to preserve anonymity, the authors should release anonymized
609"
OPEN ACCESS TO DATA AND CODE,0.8074074074074075,"versions (if applicable).
610"
OPEN ACCESS TO DATA AND CODE,0.8084656084656084,"• Providing as much information as possible in supplemental material (appended to the
611"
OPEN ACCESS TO DATA AND CODE,0.8095238095238095,"paper) is recommended, but including URLs to data and code is permitted.
612"
OPEN ACCESS TO DATA AND CODE,0.8105820105820106,"6. Experimental Setting/Details
613"
OPEN ACCESS TO DATA AND CODE,0.8116402116402116,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
614"
OPEN ACCESS TO DATA AND CODE,0.8126984126984127,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
615"
OPEN ACCESS TO DATA AND CODE,0.8137566137566138,"results?
616"
OPEN ACCESS TO DATA AND CODE,0.8148148148148148,"Answer: [NA]
617"
OPEN ACCESS TO DATA AND CODE,0.8158730158730159,"Justification: This study does not include experiments
618"
OPEN ACCESS TO DATA AND CODE,0.816931216931217,"Guidelines:
619"
OPEN ACCESS TO DATA AND CODE,0.817989417989418,"• The answer NA means that the paper does not include experiments.
620"
OPEN ACCESS TO DATA AND CODE,0.819047619047619,"• The experimental setting should be presented in the core of the paper to a level of detail
621"
OPEN ACCESS TO DATA AND CODE,0.8201058201058201,"that is necessary to appreciate the results and make sense of them.
622"
OPEN ACCESS TO DATA AND CODE,0.8211640211640212,"• The full details can be provided either with the code, in appendix, or as supplemental
623"
OPEN ACCESS TO DATA AND CODE,0.8222222222222222,"material.
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8232804232804233,"7. Experiment Statistical Significance
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8243386243386244,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8253968253968254,"information about the statistical significance of the experiments?
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8264550264550264,"Answer: [NA]
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8275132275132275,"Justification: This study does not include experiments.
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8285714285714286,"Guidelines:
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8296296296296296,"• The answer NA means that the paper does not include experiments.
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8306878306878307,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8317460317460318,"dence intervals, or statistical significance tests, at least for the experiments that support
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8328042328042328,"the main claims of the paper.
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8338624338624339,"• The factors of variability that the error bars are capturing should be clearly stated (for
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.834920634920635,"example, train/test split, initialization, random drawing of some parameter, or overall
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8359788359788359,"run with given experimental conditions).
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.837037037037037,"• The method for calculating the error bars should be explained (closed form formula,
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8380952380952381,"call to a library function, bootstrap, etc.)
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8391534391534392,"• The assumptions made should be given (e.g., Normally distributed errors).
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8402116402116402,"• It should be clear whether the error bar is the standard deviation or the standard error
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8412698412698413,"of the mean.
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8423280423280424,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8433862433862434,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8444444444444444,"of Normality of errors is not verified.
645"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8455026455026455,"• For asymmetric distributions, the authors should be careful not to show in tables or
646"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8465608465608465,"figures symmetric error bars that would yield results that are out of range (e.g. negative
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8476190476190476,"error rates).
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8486772486772487,"• If error bars are reported in tables or plots, The authors should explain in the text how
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8497354497354498,"they were calculated and reference the corresponding figures or tables in the text.
650"
EXPERIMENTS COMPUTE RESOURCES,0.8507936507936508,"8. Experiments Compute Resources
651"
EXPERIMENTS COMPUTE RESOURCES,0.8518518518518519,"Question: For each experiment, does the paper provide sufficient information on the com-
652"
EXPERIMENTS COMPUTE RESOURCES,0.852910052910053,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
653"
EXPERIMENTS COMPUTE RESOURCES,0.8539682539682539,"the experiments?
654"
EXPERIMENTS COMPUTE RESOURCES,0.855026455026455,"Answer: [NA]
655"
EXPERIMENTS COMPUTE RESOURCES,0.8560846560846561,"Justification: This study does not include experiments.
656"
EXPERIMENTS COMPUTE RESOURCES,0.8571428571428571,"Guidelines:
657"
EXPERIMENTS COMPUTE RESOURCES,0.8582010582010582,"• The answer NA means that the paper does not include experiments.
658"
EXPERIMENTS COMPUTE RESOURCES,0.8592592592592593,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
659"
EXPERIMENTS COMPUTE RESOURCES,0.8603174603174604,"or cloud provider, including relevant memory and storage.
660"
EXPERIMENTS COMPUTE RESOURCES,0.8613756613756614,"• The paper should provide the amount of compute required for each of the individual
661"
EXPERIMENTS COMPUTE RESOURCES,0.8624338624338624,"experimental runs as well as estimate the total compute.
662"
EXPERIMENTS COMPUTE RESOURCES,0.8634920634920635,"• The paper should disclose whether the full research project required more compute
663"
EXPERIMENTS COMPUTE RESOURCES,0.8645502645502645,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
664"
EXPERIMENTS COMPUTE RESOURCES,0.8656084656084656,"didn’t make it into the paper).
665"
CODE OF ETHICS,0.8666666666666667,"9. Code Of Ethics
666"
CODE OF ETHICS,0.8677248677248677,"Question: Does the research conducted in the paper conform, in every respect, with the
667"
CODE OF ETHICS,0.8687830687830688,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
668"
CODE OF ETHICS,0.8698412698412699,"Answer: [Yes]
669"
CODE OF ETHICS,0.870899470899471,"Justification: We have reviewed the NeurIPS Code of Ethics.
670"
CODE OF ETHICS,0.8719576719576719,"Guidelines:
671"
CODE OF ETHICS,0.873015873015873,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
672"
CODE OF ETHICS,0.8740740740740741,"• If the authors answer No, they should explain the special circumstances that require a
673"
CODE OF ETHICS,0.8751322751322751,"deviation from the Code of Ethics.
674"
CODE OF ETHICS,0.8761904761904762,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
675"
CODE OF ETHICS,0.8772486772486773,"eration due to laws or regulations in their jurisdiction).
676"
BROADER IMPACTS,0.8783068783068783,"10. Broader Impacts
677"
BROADER IMPACTS,0.8793650793650793,"Question: Does the paper discuss both potential positive societal impacts and negative
678"
BROADER IMPACTS,0.8804232804232804,"societal impacts of the work performed?
679"
BROADER IMPACTS,0.8814814814814815,"Answer: [Yes]
680"
BROADER IMPACTS,0.8825396825396825,"Justification: § 7
681"
BROADER IMPACTS,0.8835978835978836,"Guidelines:
682"
BROADER IMPACTS,0.8846560846560847,"• The answer NA means that there is no societal impact of the work performed.
683"
BROADER IMPACTS,0.8857142857142857,"• If the authors answer NA or No, they should explain why their work has no societal
684"
BROADER IMPACTS,0.8867724867724868,"impact or why the paper does not address societal impact.
685"
BROADER IMPACTS,0.8878306878306879,"• Examples of negative societal impacts include potential malicious or unintended uses
686"
BROADER IMPACTS,0.8888888888888888,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
687"
BROADER IMPACTS,0.8899470899470899,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
688"
BROADER IMPACTS,0.891005291005291,"groups), privacy considerations, and security considerations.
689"
BROADER IMPACTS,0.8920634920634921,"• The conference expects that many papers will be foundational research and not tied
690"
BROADER IMPACTS,0.8931216931216931,"to particular applications, let alone deployments. However, if there is a direct path to
691"
BROADER IMPACTS,0.8941798941798942,"any negative applications, the authors should point it out. For example, it is legitimate
692"
BROADER IMPACTS,0.8952380952380953,"to point out that an improvement in the quality of generative models could be used to
693"
BROADER IMPACTS,0.8962962962962963,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
694"
BROADER IMPACTS,0.8973544973544973,"that a generic algorithm for optimizing neural networks could enable people to train
695"
BROADER IMPACTS,0.8984126984126984,"models that generate Deepfakes faster.
696"
BROADER IMPACTS,0.8994708994708994,"• The authors should consider possible harms that could arise when the technology is
697"
BROADER IMPACTS,0.9005291005291005,"being used as intended and functioning correctly, harms that could arise when the
698"
BROADER IMPACTS,0.9015873015873016,"technology is being used as intended but gives incorrect results, and harms following
699"
BROADER IMPACTS,0.9026455026455027,"from (intentional or unintentional) misuse of the technology.
700"
BROADER IMPACTS,0.9037037037037037,"• If there are negative societal impacts, the authors could also discuss possible mitigation
701"
BROADER IMPACTS,0.9047619047619048,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
702"
BROADER IMPACTS,0.9058201058201059,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
703"
BROADER IMPACTS,0.9068783068783068,"feedback over time, improving the efficiency and accessibility of ML).
704"
SAFEGUARDS,0.9079365079365079,"11. Safeguards
705"
SAFEGUARDS,0.908994708994709,"Question: Does the paper describe safeguards that have been put in place for responsible
706"
SAFEGUARDS,0.91005291005291,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
707"
SAFEGUARDS,0.9111111111111111,"image generators, or scraped datasets)?
708"
SAFEGUARDS,0.9121693121693122,"Answer: [NA]
709"
SAFEGUARDS,0.9132275132275133,"Justification: This study does not contain any code, data nor trained model
710"
SAFEGUARDS,0.9142857142857143,"Guidelines:
711"
SAFEGUARDS,0.9153439153439153,"• The answer NA means that the paper poses no such risks.
712"
SAFEGUARDS,0.9164021164021164,"• Released models that have a high risk for misuse or dual-use should be released with
713"
SAFEGUARDS,0.9174603174603174,"necessary safeguards to allow for controlled use of the model, for example by requiring
714"
SAFEGUARDS,0.9185185185185185,"that users adhere to usage guidelines or restrictions to access the model or implementing
715"
SAFEGUARDS,0.9195767195767196,"safety filters.
716"
SAFEGUARDS,0.9206349206349206,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
717"
SAFEGUARDS,0.9216931216931217,"should describe how they avoided releasing unsafe images.
718"
SAFEGUARDS,0.9227513227513228,"• We recognize that providing effective safeguards is challenging, and many papers do
719"
SAFEGUARDS,0.9238095238095239,"not require this, but we encourage authors to take this into account and make a best
720"
SAFEGUARDS,0.9248677248677248,"faith effort.
721"
LICENSES FOR EXISTING ASSETS,0.9259259259259259,"12. Licenses for existing assets
722"
LICENSES FOR EXISTING ASSETS,0.926984126984127,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
723"
LICENSES FOR EXISTING ASSETS,0.928042328042328,"the paper, properly credited and are the license and terms of use explicitly mentioned and
724"
LICENSES FOR EXISTING ASSETS,0.9291005291005291,"properly respected?
725"
LICENSES FOR EXISTING ASSETS,0.9301587301587302,"Answer: [NA]
726"
LICENSES FOR EXISTING ASSETS,0.9312169312169312,"Justification: This study does not contain any code, data nor trained model
727"
LICENSES FOR EXISTING ASSETS,0.9322751322751323,"Guidelines:
728"
LICENSES FOR EXISTING ASSETS,0.9333333333333333,"• The answer NA means that the paper does not use existing assets.
729"
LICENSES FOR EXISTING ASSETS,0.9343915343915344,"• The authors should cite the original paper that produced the code package or dataset.
730"
LICENSES FOR EXISTING ASSETS,0.9354497354497354,"• The authors should state which version of the asset is used and, if possible, include a
731"
LICENSES FOR EXISTING ASSETS,0.9365079365079365,"URL.
732"
LICENSES FOR EXISTING ASSETS,0.9375661375661376,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
733"
LICENSES FOR EXISTING ASSETS,0.9386243386243386,"• For scraped data from a particular source (e.g., website), the copyright and terms of
734"
LICENSES FOR EXISTING ASSETS,0.9396825396825397,"service of that source should be provided.
735"
LICENSES FOR EXISTING ASSETS,0.9407407407407408,"• If assets are released, the license, copyright information, and terms of use in the
736"
LICENSES FOR EXISTING ASSETS,0.9417989417989417,"package should be provided. For popular datasets, paperswithcode.com/datasets
737"
LICENSES FOR EXISTING ASSETS,0.9428571428571428,"has curated licenses for some datasets. Their licensing guide can help determine the
738"
LICENSES FOR EXISTING ASSETS,0.9439153439153439,"license of a dataset.
739"
LICENSES FOR EXISTING ASSETS,0.944973544973545,"• For existing datasets that are re-packaged, both the original license and the license of
740"
LICENSES FOR EXISTING ASSETS,0.946031746031746,"the derived asset (if it has changed) should be provided.
741"
LICENSES FOR EXISTING ASSETS,0.9470899470899471,"• If this information is not available online, the authors are encouraged to reach out to
742"
LICENSES FOR EXISTING ASSETS,0.9481481481481482,"the asset’s creators.
743"
NEW ASSETS,0.9492063492063492,"13. New Assets
744"
NEW ASSETS,0.9502645502645503,"Question: Are new assets introduced in the paper well documented and is the documentation
745"
NEW ASSETS,0.9513227513227513,"provided alongside the assets?
746"
NEW ASSETS,0.9523809523809523,"Answer: [NA]
747"
NEW ASSETS,0.9534391534391534,"Justification: This study does not provide any code, data nor trained model
748"
NEW ASSETS,0.9544973544973545,"Guidelines:
749"
NEW ASSETS,0.9555555555555556,"• The answer NA means that the paper does not release new assets.
750"
NEW ASSETS,0.9566137566137566,"• Researchers should communicate the details of the dataset/code/model as part of their
751"
NEW ASSETS,0.9576719576719577,"submissions via structured templates. This includes details about training, license,
752"
NEW ASSETS,0.9587301587301588,"limitations, etc.
753"
NEW ASSETS,0.9597883597883597,"• The paper should discuss whether and how consent was obtained from people whose
754"
NEW ASSETS,0.9608465608465608,"asset is used.
755"
NEW ASSETS,0.9619047619047619,"• At submission time, remember to anonymize your assets (if applicable). You can either
756"
NEW ASSETS,0.9629629629629629,"create an anonymized URL or include an anonymized zip file.
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.964021164021164,"14. Crowdsourcing and Research with Human Subjects
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9650793650793651,"Question: For crowdsourcing experiments and research with human subjects, does the paper
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9661375661375662,"include the full text of instructions given to participants and screenshots, if applicable, as
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9671957671957672,"well as details about compensation (if any)?
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9682539682539683,"Answer: [NA]
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693121693121693,"Justification: This study does not involve crowdsourcing nor research with human subjects.
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9703703703703703,"Guidelines:
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9714285714285714,"• The answer NA means that the paper does not involve crowdsourcing nor research with
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724867724867725,"human subjects.
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9735449735449735,"• Including this information in the supplemental material is fine, but if the main contribu-
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9746031746031746,"tion of the paper involves human subjects, then as much detail as possible should be
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9756613756613757,"included in the main paper.
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9767195767195768,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9777777777777777,"or other labor should be paid at least the minimum wage in the country of the data
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9788359788359788,"collector.
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798941798941799,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9809523809523809,"Subjects
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.982010582010582,"Question: Does the paper describe potential risks incurred by study participants, whether
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830687830687831,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9841269841269841,"approvals (or an equivalent approval/review based on the requirements of your country or
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851851851851852,"institution) were obtained?
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9862433862433863,"Answer: [NA]
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873015873015873,"Justification: This study does not involve crowdsourcing nor research with human subjects.
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883597883597883,"Guidelines:
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9894179894179894,"• The answer NA means that the paper does not involve crowdsourcing nor research with
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904761904761905,"human subjects.
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9915343915343915,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925925925925926,"may be required for any human subjects research. If you obtained IRB approval, you
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936507936507937,"should clearly state this in the paper.
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947089947089947,"• We recognize that the procedures for this may vary significantly between institutions
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957671957671957,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968253968253968,"guidelines for their institution.
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978835978835979,"• For initial submissions, do not include any information that would break anonymity (if
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989417989417989,"applicable), such as the institution conducting the review.
791"
