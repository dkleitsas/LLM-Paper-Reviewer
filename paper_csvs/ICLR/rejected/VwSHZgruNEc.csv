Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002369668246445498,"Search algorithms have been playing a vital role in the success of superhuman AI
in both perfect information and imperfect information games. Speciﬁcally, search
algorithms can generate a reﬁnement of Nash equilibrium (NE) approximation
in games such as Texas hold’em with theoretical guarantees. However, when
confronted with opponents of limited rationality, an NE strategy tends to be overly
conservative, because it prefers to achieve its low exploitability rather than actively
exploiting the weakness of opponents. In this paper, we investigate the dilemma of
safety and opponent exploitation. We present a new real-time search framework
that smoothly interpolates between the two extremes of strategy search, hence
unifying safe search and opponent exploitation. We provide our new strategy with
a theoretically upper-bounded exploitability and lower-bounded reward against
an opponent. Our algorithm enables computationally efﬁcient online adaptations
to a possibly changing opponent model. Empirical results show that our method
signiﬁcantly outperforms NE baselines when opponents play non-NE strategies
and keeps low exploitability at the same time. It is also much more efﬁcient than
previous safe exploitation baselines."
INTRODUCTION,0.004739336492890996,"1
INTRODUCTION"
INTRODUCTION,0.0071090047393364926,"Behind the recent breakthroughs of superhuman AIs in Go (Silver et al., 2016; 2017; Schrittwieser
et al., 2020), heads-up no-limit Texas hold’em (HUNL) (Brown et al., 2018; Moravcík et al., 2017;
Brown & Sandholm, 2019; Brown et al., 2020), and Hanabi (Lerer et al., 2020), search plays a vital
role. In perfect information games, Monte Carlo tree search (MCTS) is widely applied to improve
policy’s strength. In zero-sum imperfect information games such as poker, search algorithms are
used to ﬁnd a Nash equilibrium (NE) approximation in subgames encountered in real time (Brown
& Sandholm, 2017; Burch et al., 2014a). They are both theoretically sounded and empirically
powerful. In fully-cooperative imperfect information games, the search algorithm proposed in Tian
et al. (2020b) is proved to never be detrimental to the current policy. Lerer et al. (2020) also ensures
original performance asymptotically."
INTRODUCTION,0.009478672985781991,"In zero-sum games, NE-based search algorithms (Burch et al., 2014a; Moravcik et al., 2016; Brown
& Sandholm, 2017; Brown et al., 2018) ﬁnd safe strategies with low exploitability and produce strong
baselines against all opponents (Brown & Sandholm, 2019). However, it may be overly conservative
confronted with opponents with limited rationality and fail to take advantage of their weaknesses
to obtain higher rewards (McCracken & Bowling, 2004; Johanson et al., 2007; Li & Miikkulainen,
2018). From the other perspective, there have been extensive studies on opponent exploitation to
address the problem. Some typical works (Carmel & Markovitch, 1996; Billings et al., 2003; Gilpin
& Sandholm, 2006; Li & Miikkulainen, 2018) model the opponent’s strategy based on previous
observations and then search for a new strategy to exploit this model. However, these methods often
neglect the signiﬁcance of the strategy safety, thus being highly exploitable by the opponent. Few
exceptions including Johanson et al. (2007) and Ganzfried & Sandholm (2015a) aimed to search for
safe and robust counter strategies, but they are computationally inefﬁcient in an online setting where
the opponent model is being updated continuously with streamed data."
INTRODUCTION,0.011848341232227487,"In this paper, we study the dilemma of safety and opponent exploitation and present a new scalable
real-time search framework Safe Exploitation Search (SES) that smoothly interpolates between the
two extremes of strategy search, hence unifying safe search and opponent exploitation. It enables"
INTRODUCTION,0.014218009478672985,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.016587677725118485,"computationally efﬁcient online adaptations to a continuously changing opponent model, which is
hard to address by previous safe exploitation algorithms. The safety criterion requires the reﬁned
strategy to stay close to NE, formally speaking, to expose limited exploitability against any opponents,
while the opponent exploitation criterion requires the strategy to adapt to its speciﬁc opponent and
to exploit its weaknesses. We propose a novel maximization objective which combines the safety
objective and exploitation, controlled by the exploitation level α. We construct a new gadget game to
optimize this objective, which enables our method’s scalability to large games such as Texas Hold’em.
Theoretically, we prove that SES is guaranteed to outperform NE at the cost of some constant
increase in its own exploitability confronted with non-NE opponents. Empirically, we evaluate the
effectiveness of our search algorithm in 1 didactic matrix game 2 poker games: Leduc Hold’em
(Southey et al., 2005) and Flop Hold’em Poker (FHP)(Brown et al., 2019). The experiment results
demonstrate that our algorithm signiﬁcantly outperforms NE baselines against non-NE opponents and
keeps low exploitability at the same time. Additionally, SES is much more computationally efﬁcient
than previous safe exploitation baselines."
RELATED WORK,0.018957345971563982,"2
RELATED WORK"
RELATED WORK,0.02132701421800948,"This paper investigates the problem of safe opponent exploitation in two-player zero-sum imperfect
information games. We propose a novel search algorithm which balances between NE and exploiting
opponents. Two major relevant research areas are search algorithms in imperfect information games,
and opponent exploitation."
RELATED WORK,0.023696682464454975,"Search in imperfect information games. In recent literature, search techniques are witnessed to
be important in developing strong AI strategies in both perfect and imperfect information games
(Burch et al., 2014a; Moravcik et al., 2016; Brown & Sandholm, 2017). Texas hold ’em poker is
widely employed as a benchmark for imperfect information games. A primary part of the long-term
research on Texas hold’em poker is the evolution of subgame solving algorithms, which aim at
achieving a more accurate Nash equilibrium approximation in the subgame encountered given a
pre-computed strategy for the full game which we refer to as the blueprint strategy. Unsafe search
(Billings et al., 2003; Ganzfried & Sandholm, 2015b; Gilpin & Sandholm, 2006; 2007) estimates
the subgame reach probability assuming the opponent follows blueprint, and searches for a reﬁned
subgame strategy. Subgame resolving (Burch et al., 2014a) and maxmargin search (Moravcik et al.,
2016) are theoretically sounded safe search algorithms which ensure that the subgame strategy
obtained is no worse than the blueprint. They search in a gadget game and achieve safety by providing
the opponent with the option not entering the current subgame. DeepStack (Moravcík et al., 2017)
and Libratus (Brown et al., 2018) build strong poker AIs with the aid of search. Beyond poker,
search algorithms for subgame reﬁnement have also shown promise in improving joint strategies in
cooperative imperfect information games such as Hanabi (Lerer et al., 2020) and the bidding phase of
contract bridge (Tian et al., 2020a). The purpose of our search algorithm is different from previous
methods in poker literature. We seek to exploit opponents while keeping exploitability low, rather
than simply approximating NE."
RELATED WORK,0.026066350710900472,"Opponent exploitation. Most previous opponent exploitation researches (Carmel & Markovitch,
1996; Billings et al., 2003; Gilpin & Sandholm, 2006; Li & Miikkulainen, 2018) typically model the
opponent’s strategy based on previous observations and then search for a new strategy to exploit this
model, but put little emphasis on safety."
RELATED WORK,0.02843601895734597,"One similar work is Johanson et al. (2007) which proposes p-restricted Nash response (RNR) to ﬁnd
a safe exploitation strategy to the estimated opponent’s strategy. It calculates a Nash equilibrium for
the whole game restricting that the opponent plays the estimated strategy σﬁx with probability p, and
any strategy with probability 1 −p. In that paper, Johanson et al. (2007) prove that a p-RNR to σﬁx
is Pareto optimal with respect to exploitation and safety. However, it does not provide an explicit
bound. Additionally, whenever the estimated opponent model changes or we want to use a different p
to balance between safety and exploitation, p-RNR has to recompute the strategy for the whole game.
It is computationally inefﬁcient in an online setting, where the opponent model is updated after every
round with new game data. Our algorithm instead takes modelling error into account and provides
explicit bounds for both safety and exploitation. With the aid of real-time search, it only searches for
strategies in subgames encountered instead of the whole game. Our experiments show that it is more
efﬁcient than Johanson et al. (2007)."
RELATED WORK,0.030805687203791468,Under review as a conference paper at ICLR 2022
RELATED WORK,0.03317535545023697,"Ganzfried & Sandholm (2015a) study safe exploitation strategies in repeated games, which is a
different setting from this paper. Intuitively, it achieves safety by risking in exploitability at most
what it has earned over NE in expectation in previous rounds. Therefore, its expected value in the
whole repeated game is never worse than the NE. In contrast, this paper focuses on the safety of stage
game strategies. Furthermore, our algorithm is complementary to Ganzfried & Sandholm (2015a).
Ganzfried & Sandholm (2015a) calculate an ε-safe best response for the whole game at each iteration
with LP. This procedure is one of the main limitations on the algorithm’s scalability. Our algorithm,
which only reﬁnes strategies in subgames in real-time, can be a possible substitute for LP."
RELATED WORK,0.035545023696682464,"To our knowledge, we are the ﬁrst paper to investigate the safe opponent exploitation problem
in subgame resolving schemes. Subgame resolving enables online adaptations to a continuously
changing opponent model, eliminating the need to recompute a whole game strategy. It offers
computational beneﬁts in practical opponent exploitation circumstances."
RELATED WORK,0.037914691943127965,"It is worthwhile mentioning that there also has been extensive research (Albrecht & Stone, 2018)
on agent modeling. However, this paper only focuses on the theoretical and empirical results of the
search algorithm, but not the agent modeling techniques. We can use off-the-shelf agent modeling
algorithms to estimate opponent’s strategies. Agent modeling provides the ability to reason about the
others, making predictions of their strategies, types, etc. He et al. (2016) embeds opponent model
learning into deep Q-learning, which stabilizes the training process faced with opponents whose
policy is changing over time. LOLA (Foerster et al., 2018) considers the evolution of other agents in
the training process of multi-agent reinforcement learning. Raileanu et al. (2018) proposes to model
others from one’s own policy. Rabinowitz et al. (2018) uses meta learning to predict strategies of
different kinds of agents."
NOTATIONS AND BACKGROUND,0.04028436018957346,"3
NOTATIONS AND BACKGROUND"
NOTATIONS AND BACKGROUND,0.04265402843601896,"An extensive-form imperfect information game G = (P, H, Z, A, χ, ρ, ·, σc, u, I) describes sequen-
tial interactions among agents, where agents have private information. A ﬁnite set P consists of n
players and a chance node c which represents the stochastic nature of the environment. The set of
non-terminal decision nodes is denoted as H, and Z is a set of terminal nodes, or leaves. The set of
possible actions is A, and χ : H →2|A| is a function which assigns to each decision node h ∈H a
set of legal actions. A player function ρ : H →P assigns to each decision node a player p ∈P who
acts at that node. If action a leads from h to h′, we write h · a = h′. If there exists a sequence of
actions leading from h to h′, we write h ⊑h′. At each node h ∈H, the acting player p = ρ(h) picks
an action from legal actions a ∈χ(h), and leads node h into its child h · a. The chance node always
samples an action from its own distribution σc, which is common knowledge to all players. Utility
functions are u = (u1, u2, . . . , un), where ui : Z →R deﬁnes the utility of player i at terminal node
z ∈Z. The nature of imperfect information is characterized by infosets I = (I1, I2, . . . , In), where
Ii = (Ii,1, . . . , Ii,ki) is a partition of H for player i. Two states in the same infoset must have the
same acting player and the same legal action sets. We use I(h) to denote the infoset that h belongs to.
A player p cannot distinguish between states h1 and h2 if I(h1) = I(h2), and thus should behave
identically on all states in the same infoset."
NOTATIONS AND BACKGROUND,0.045023696682464455,"The strategy of a player p is σp : Ip × A →R, where σp(I, a) is a distribution over valid actions on
infoset I. For simplicity, we also use σp(h, a) to denote σp(I(h), a). We use πσ(h) to denote the prob-
ability of reaching state h from the root when agents choose a strategy proﬁle σ = ⟨σ1, σ2, . . . , σn⟩.
Formally, πσ(h) = Q
h′·a⊑h σρ(h′)(h′, a). We use πσ
−p(h) = Q
h′·a⊑h∧ρ(h′)̸=p σρ(h′)(h′, a) to de-
note the probability of reaching h when player p always chooses the action that leads to h whenever
possible. πσ(h, h′) is the reaching probability of h′ from h. πσ(h · a, h′) is the the probability of
reaching h′ from h if action a is taken at h. These probabilities can be formally deﬁned in a similar
manner."
NOTATIONS AND BACKGROUND,0.04739336492890995,"The expected utility of player p given strategy proﬁle σ is uσ
p = P"
NOTATIONS AND BACKGROUND,0.04976303317535545,"z∈Z πσ(z)up(z). The counter-
factual value vσ
p (I, a) is the expected utility that player p will obtain after taking action a at infoset
I, given the joint policy proﬁle is σ. Mathematically, it is the weighted sum of expected values at all
states h ∈I."
NOTATIONS AND BACKGROUND,0.052132701421800945,"vσ
p (I, a) = P"
NOTATIONS AND BACKGROUND,0.054502369668246446,"h∈I,z∈Z πσ
−p(h)πσ(h · a, z)up(z)
P"
NOTATIONS AND BACKGROUND,0.05687203791469194,"h∈I πσ
−p(h)
(1)"
NOTATIONS AND BACKGROUND,0.05924170616113744,Under review as a conference paper at ICLR 2022
NOTATIONS AND BACKGROUND,0.061611374407582936,"In the rest of the paper, we focus on two-player zero-sum games with perfect recall. Zero-sum means
∀z ∈Z, u1(z) + u2(z) = 0. Perfect recall means that no player will forget the information which
has been obtained previously in the game. This is a common assumption in related literature."
NOTATIONS AND BACKGROUND,0.06398104265402843,"A best response strategy BRp(σ−p) = arg maxσp u⟨σp,σ−p⟩
p
for player p is the strategy that maximize
his own expected utility against ﬁxed opponent strategy σ−p. The exploitability of strategy σp is
exp(σp) = uσ∗
p
−u⟨σp,BR−p(σp)⟩
p
where σ∗is the optimal strategy, and is an NE in two-player
zero-sum games. It measures the performance of σp against its best response comparing with the
NE. A counterfactual best response CBRp(σ−p) is a strategy where σp(I, a) > 0 if and only if
vσ
p (I, a) ≥maxb vσ
p (I, b). Counterfactual best response is a best response, but not vice versa. The"
NOTATIONS AND BACKGROUND,0.06635071090047394,"counterfactual best response value CBV σ−p
p
(I) = v⟨CBRp(σ−p),σ−p⟩
p
is the expected utility of the
counterfactual best response policy. Since we focus on two-player zero-sum games, we will use
CBV σp(I) as a shorthand notation for CBV σp
−p(I)."
NOTATIONS AND BACKGROUND,0.06872037914691943,"We follow the imperfect information subgame deﬁnition as in Burch et al. (2014b). An augmented
infoset contains states which cannot be distinguished by the remaining players."
NOTATIONS AND BACKGROUND,0.07109004739336493,"Deﬁnition 1. An imperfect information subgame S is a forest of trees, closed under both the
descendant relation and membership within augmented infosets for any player. Let Stop be the set of
nodes which are roots of each tree in S."
METHOD,0.07345971563981042,"4
METHOD"
METHOD,0.07582938388625593,"In this section, we introduce our novel search algorithm called safe exploitation search (SES), which
exploits the weaknesses of opponent while ensuring a bounded exploitability. Let σ be the pre-
computed blueprint strategy. Without loss of generality, assume we search for player 2’s reﬁned
strategy σS
2 by applying SES to all subgames S ∈S. Finally, the reﬁned strategy for P2 after search
is σ′
2, which is the same as σ2 in {Ii
2|∀S ∈S, Ii
2 /∈S} and is replaced with σS
2 in S ∈S."
SAFE EXPLOITATION SEARCH,0.07819905213270142,"4.1
SAFE EXPLOITATION SEARCH"
SAFE EXPLOITATION SEARCH,0.08056872037914692,"Our algorithm offers a uniﬁed approach to balance between these two demands with theoretical
guarantees. The objective of our search algorithm is to ﬁnd a new subgame strategy σS
2 for S ∈S
which maximizes"
SAFE EXPLOITATION SEARCH,0.08293838862559241,"SE(σS
2 ) = α
X"
SAFE EXPLOITATION SEARCH,0.08530805687203792,"Ij
1∈Stop
ˆp(Ij
1)

vσ
1 (Ij
1) −CBV σS
2
1
(Ij
1)

+ (1 −α) min
Ij
1∈Stop"
SAFE EXPLOITATION SEARCH,0.08767772511848342,"
vσ
1 (Ij
1) −CBV σS
2
1
(Ij
1)

,"
SAFE EXPLOITATION SEARCH,0.09004739336492891,"(2)
where α ∈[0, 1] is a hyper-parameter controlling the exploitation level, and ˆp(Ii
1) is the estimated
probability of player 1 entering infoset Ij
1 ∈Stop. Given P2’s strategy (which is the blueprint σ2)
and P1’s actual strategy (which does not have to be the blueprint σ1), the real probability of player 1
entering infoset Ij
1 ∈Stop (which we denote as p(Ij
1)) is determined. ˆp(Ii
1) is just an estimation of
p(Ij
1). For instance, in poker, it is the estimated distribution of private cards player 1 holds. Both
theoretically and empirically, such estimation does not have to be fully accurate. It can be done with
off-the-shelf opponent modeling techniques, which lies beyond the focus of this paper."
SAFE EXPLOITATION SEARCH,0.0924170616113744,"Though seemingly complicated, intuitively, the maximization objective achieves a balance between
opponent exploitation and safety, controlled by exploitation level α. The ﬁrst part of the objective"
SAFE EXPLOITATION SEARCH,0.0947867298578199,is maximized when P
SAFE EXPLOITATION SEARCH,0.0971563981042654,"Ij
1∈Stop ˆp(Ij
1)CBV σS
2
1
(Ij
1) is minimized. It aims at ﬁnding a strategy σS
2 which
results in the lowest value for P1 under the assumption that the reach probabilities is ˆp. It can be
interpreted as exploiting the estimated P1’s strategy. The second part of the objective demands the
resolved strategy to behave well against any reach probability distribution. We use the subgame
margin minIj
1∈Stop"
SAFE EXPLOITATION SEARCH,0.0995260663507109,"
vσ
1 (Ij
1) −CBV σS
2
1
(Ij
1)

(Moravcik et al., 2016) which can be regarded as the
worst-case utility increase for P2."
SAFE EXPLOITATION SEARCH,0.1018957345971564,"By maximizing the objective 2, we provide sound theoretical results for both safety and opponent
exploitation. Additionally, we provide analyses of how (1) exploitation level α, (2) accuracy of"
SAFE EXPLOITATION SEARCH,0.10426540284360189,Under review as a conference paper at ICLR 2022
SAFE EXPLOITATION SEARCH,0.1066350710900474,"opponent modeling, and (3) strength of the blueprint strategy impact the theoretical bound. By
gradually increasing α from 0 to 1, our algorithm tends to exploit rather than keeping safety."
SAFE EXPLOITATION SEARCH,0.10900473933649289,"Theorem 1. (safety) Let S be a disjoint set of subgames S. Let σ∗= ⟨σ∗
1, σ∗
2⟩be the NE where P2’s
strategy is constrained to be the same with σ2 outside S. Deﬁne ∆= maxS∈S,Ii
1∈Stop |CBV σ∗
2
1 (Ii
1)−
vσ
1 (Ii
1)|. Let ˜p(Ii
1) be the reach probability given by σ∗
1. Let ˆp(Ii
1) be the estimation of reach
probability p(Ii
1) given by the real opponent strategy. Deﬁne τ = maxS∈S,Ii
1∈Stop | ˆp(Ii
1)−˜p(Ii
1)
˜p(Ii
1)
|.
Whenever 1 −(2τ + 1)α > 0, we have a bounded exploitability given by:"
SAFE EXPLOITATION SEARCH,0.11137440758293839,"exp(σ′
2) ≤exp(σ∗
2) +
2
1 −(2τ + 1)α∆.
(3)"
SAFE EXPLOITATION SEARCH,0.11374407582938388,"Recall that σ′
2 is the reﬁned strategy after search. The proof is provided in Appendix A. This
theorem implies that the exploitability of the new strategy is smaller than that of strategy σ∗
2 plus a
constant value, which is the closest strategy to NE if constrained to differ from σ2 only in S. The
corresponding theoretical result of maxmargin search (Moravcik et al., 2016), a safe search algorithm
with no opponent exploitation abilities, is exp(σ′
2) ≤exp(σ∗
2)+2∆. Comparing these two results, we
can interpret the term 2/(1 −(2τ + 1)α) as the additional risk introduced by exploiting the opponent.
If exploitation level α = 0, then our bound is as tight as that of maxmargin search (Moravcik et al.,
2016). The bound also gets tighter if the τ gets smaller, or the blueprint σ2 is closer to σ∗
2."
SAFE EXPLOITATION SEARCH,0.11611374407582939,"Theorem 2. (opponent exploitation) Let ϵ = ∥ˆp−p∥1 be the L1 distance of the distribution p(Ii
1) and"
SAFE EXPLOITATION SEARCH,0.11848341232227488,"ˆp(Ii
1). Let η = minS∈S maxIj
1∈Stop"
SAFE EXPLOITATION SEARCH,0.12085308056872038,"
CBV1(Ij
1, σS
2 ) −CBV1(Ij
1, σ∗
2)

≥0. We use BR[S,σp]
p
(σ)
to denote the strategy for player p which maximizes its utility in subgame S ∈S against σ−p under
the constraint that BR[S,σp]
p
(σ) and σp differs only inside S. By maximizing objective 2, for all S ∈S,
the reﬁned strategy σ′
2 satisﬁes u"
SAFE EXPLOITATION SEARCH,0.12322274881516587,"D
BR[S,σ1]
1
(σ′
2),σ′
2
E"
SAFE EXPLOITATION SEARCH,0.12559241706161137,"2
(S) ≥u"
SAFE EXPLOITATION SEARCH,0.12796208530805686,"D
BR[S,σ1]
1
(σ∗
2),σ∗
2
E"
SAFE EXPLOITATION SEARCH,0.13033175355450238,"2
(S) + 1 −α"
SAFE EXPLOITATION SEARCH,0.13270142180094788,"α
(η −2∆) −ϵη
(4)"
SAFE EXPLOITATION SEARCH,0.13507109004739337,"The proof is provided in Appendix A. Observe that the reach probability p is characterized by P1’s
strategy outside S and ˆp is its estimation. Because the search algorithm always ﬁnd a stronger
response strategy for P1 in S (which is exactly BR[S,σ1]
1
(σ′
2)) as well, opponent exploitation refers
to adapting to P1’s strategy σ1 outside S. This theorem implies that the utility of the new strategy
σ′
2 is lower bounded by the utility of σ∗
2 when both confronted with P1’s unknown strategy outside
S. It provides theoretical guarantees for the opponent exploitation ability of our algorithm. ϵ can
be interpreted as estimation error. The lower bound increases if the estimation error get smaller or
the blueprint σ2 is closer to σ∗
2. We show empirically how exploitation level α and estimation error
impact both safety and exploitation abilities in section 5."
GADGET GAME,0.13744075829383887,"4.2
GADGET GAME"
GADGET GAME,0.13981042654028436,"In order to ﬁnd σS
2 which maximize objective 2, a straight-forward method is to reformulate the
maximization problem as a Linear Programming problem (Moravcik et al., 2016). However, LP
solvers (Koller et al., 1994) cannot handle large-scale problems. Alternatively, inspired by Moravcik
et al. (2016), we create a gadget game and then apply iteration-based NE algorithms such as CFR
(Zinkevich et al., 2007; Tammelin et al., 2015; Lanctot et al., 2009) in the gadget game. The gadget
game is carefully designed such that the NE solution found in it is exactly the solution to the original
optimization problem."
GADGET GAME,0.14218009478672985,"As shown in Figure 1, the original subgame is copied into two identical parts S1, S2 in the gadget
game. Player 2’s infosets stretch over both branches, while player 1 can distinguish between the two
parts. The procedure of constructing such gadget game can be summarized into 4 steps: (i). Create a
chance node at the top of the gadget game. (ii) For the left part of the gadget game, we construct a
P1 node to let P1 choose an infoset Ii
1 to enter. The following chance node samples a speciﬁc state
with probability proportional to πσ
−1(h) for all h ∈I1. (iii) For the right part, create a chance node
sampling an infoset Ii
1. The following chance node again samples a speciﬁc state with probability"
GADGET GAME,0.14454976303317535,"Under review as a conference paper at ICLR 2022 S1 C
C
C P1 S2 C
C
C C"
GADGET GAME,0.14691943127962084,"C
1 −𝛼
𝛼"
GADGET GAME,0.14928909952606634,"Figure 1: The gadget game of SES. The shadow and dashed line indicate that player 2 cannot
distinguish between the two branches. C represents chance node, P1 represents player 1’s action
node. S1 and S2 are two identical copies of the subgame S with utility shifted."
GADGET GAME,0.15165876777251186,"P2
P1
L
M
R"
GADGET GAME,0.15402843601895735,"U
3
2
4
O
2
3
9.9
D
3
2
9.9
F
-100
-100
10"
GADGET GAME,0.15639810426540285,"Table 1: The payoff matrix of the example zero-sum matrix game. The values are the payoffs of
player 2. We will resolve for player 2."
GADGET GAME,0.15876777251184834,"proportional to πσ
−1(h) for all h ∈I1. (iv). Shift the utility of the gadget game by vσ
1 (Ii
1). The details
are described below."
GADGET GAME,0.16113744075829384,"1. The chance node at the top goes to the left part with probability 1 −α, and the right part with
probability α. The outcome is visible to P1 but not P2. Therefore, corresponding nodes in both
branches are in the same infosets for P2, and his strategy σS
2 will be the same for both parts. Since
σS
1 is the best-response to σS
2 and the two parts only differ at how to go to each infoset of player 1,
player 1 will also keep his strategy the same in both parts."
GADGET GAME,0.16350710900473933,"2. We subtract u1(z) by vσ
1 (Ii
1) for all z ⊑h, h ∈Ii
1, and add u2(z) by vσ
1 (Ii
1) in order to keep
the subgame zero-sum. By doing so, the objective of a Nash Equilibrium of p2 will change from"
GADGET GAME,0.16587677725118483,"maximizing −CBV σS
2
1
(Ii
1) to maximizing vσ
1 (Ii
1) −CBV σS
2
1
(Ii
1)."
GADGET GAME,0.16824644549763032,"3. As for the left part of the gadget game, the P1 node on the second level in Figure 1 enables P1
to enter an arbitrary infoset Ii
1. Since this is a zero-sum game, in an NE strategy, he will enter the
one with lowest vσ
1 (Ii
1) −CBV1(Ii
1, σ∗
2) which is exactly the minimization in the second term of
SE(σS
2 )."
THE CHANCE NODE ON THE SECOND LEVEL OF THE RIGHT PART WILL SAMPLE AN INFOSET II,0.17061611374407584,"4. The chance node on the second level of the right part will sample an infoset Ii
1 according to reach
probability ˆp(Ii
1). So that the NE objective of this part is exactly the summation in the ﬁrst term of
SE(σS
2 )."
MATRIX GAME,0.17298578199052134,"4.3
MATRIX GAME"
MATRIX GAME,0.17535545023696683,"In this part, we offer a matrix game as an example to show the necessity of considering safety and
expected payoff simultaneously, and to demonstrate the superiority of SES over a simple mixing
strategy, which follows a best response to the estimated opponent model with probability α and
follows the blueprint with probability 1 −α."
MATRIX GAME,0.17772511848341233,"In the matrix game shown in Table 1, let’s consider two speciﬁc NEs. In both NEs, P1 will play L/M
with 0.5 probability each. P2 will play U/O with 0.5 probability in the ﬁrst NE and O/D with 0.5
probability in the second NE. Suppose the blueprint strategy is the ﬁrst NE. Consider the case when
P1 plays a rather weak strategy that he will only play action R. We apply SES to search for P2’s
reﬁned strategy."
MATRIX GAME,0.18009478672985782,Under review as a conference paper at ICLR 2022
MATRIX GAME,0.18246445497630331,"0.0
0.2
0.4
0.6
0.8
1.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0"
MATRIX GAME,0.1848341232227488,chips/h
MATRIX GAME,0.1872037914691943,Evaluation
MATRIX GAME,0.1895734597156398,"SES
Mixing"
MATRIX GAME,0.19194312796208532,"0.0
0.2
0.4
0.6
0.8
1.0 0 20 40 60 80 100"
MATRIX GAME,0.1943127962085308,chips/h
MATRIX GAME,0.1966824644549763,Exploitability
MATRIX GAME,0.1990521327014218,"SES
Mixing"
MATRIX GAME,0.2014218009478673,"Figure 2: Left: Expected payoffs of SES and the mixing strategy in the proposed matrix game
example. Right: Exploitability of the two algorithms."
MATRIX GAME,0.2037914691943128,"When the estimation of opponent strategy is accurate such that ˆp = p, the best response of P2 is
always playing F, which is highly exploitable, while SES ﬁnds the second NE under proper α. To
give more details, the exploitability and expected payoff of the strategy found by SES and the mixing
strategy are shown in Figure 2. We can see that SES achieves both lower exploitability and better
performance than the mixing strategy at almost all α values. The reason behind this success is that P1
always playing R is a Gift Strategy (Ganzfried & Sandholm, 2015a) in the designed matrix game and
SES manages to utilize such gift strategy. Strategy σ−p is a Gift Strategy if it is not a best response to
a NE strategy σ∗
p. Therefore, we can switch to σ∗
p to get better performance against σ−p while also
keeping exploitability low. However, the simple mixing strategy cannot ﬁnd such “good"" NE strategy
so that it will perform much worse in both exploitability and expected payoff."
EXPERIMENT,0.20616113744075829,"5
EXPERIMENT"
EXPERIMENT,0.20853080568720378,"Our experiment is done in Leduc Hold’em (Southey et al., 2005) and Flop Hold’em Poker (FHP)
(Brown et al., 2019). Leduc Hold’em is a smaller-scale poker games and FHP is a larger one. The rules
of these two pokers are provided in Appendix B. We demonstrate the exploitability and evaluation
performance of SES against opponents of various strengths. The exploitability measures a search
algorithm’s safety, while head-to-head evaluation measures the ability of opponent exploitation. We
also illustrate how estimation accuracy of opponent’s strategy and the exploitation level α impact the
results. Please refer to Appendix C for implementation details."
OPPONENTS,0.2109004739336493,"5.1
OPPONENTS"
OPPONENTS,0.2132701421800948,"In our experiments, we test the performance of our algorithm against opponents of various strengths.
For both Leduc Poker and FHP, we create 3 types of opponents with 3 random seeds each. The ﬁrst
type of opponent is an approximation of NE in the full game, and is regarded as a strong opponent.
It is computed in the same way as the blueprint strategy with different seed. For the second and
third type of opponents, we enumerate every infoset in the blueprint strategy and shift the action
distribution randomly with probability Prshufﬂe = 0.3 or 0.7. We multiply the probability of each
action by a random variable from Uniform(0, 1), and then re-normalize the probability distribution.
The procedure is motivated by Brown et al. (2018), in which such method is applied to create a
number of diverse but reasonably strong agents. Even when Prshufﬂe = 0.7, the strategy keeps close
to NE with average L1 distance of each infoset 0.132 comparing to 1.036 of a random strategy to NE.
So they are regarded as opponents who are not fully rational but with competitive strength."
SAFE OPPONENT SEARCH,0.2156398104265403,"5.2
SAFE OPPONENT SEARCH"
SAFE OPPONENT SEARCH,0.21800947867298578,"In Figure 3, we demonstrate the head-to-head evaluation performances and corresponding exploitabil-
ity of the reﬁned strategies found by SES against opponents of various strengths, under different
exploitation level α and estimation errors of opponent’s strategy. Different lines in each plot refers
to corresponding estimation error ϵ, which is the L1 distance of ˆp and p. We evaluate our reﬁned"
SAFE OPPONENT SEARCH,0.22037914691943128,Under review as a conference paper at ICLR 2022
SAFE OPPONENT SEARCH,0.22274881516587677,"0.0
0.2
0.4
0.6
0.8
1.0 70 60 50 40 30 20 10 0 mbb/h"
SAFE OPPONENT SEARCH,0.22511848341232227,Evaluation(Prshuffle = 0.0)
SAFE OPPONENT SEARCH,0.22748815165876776,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.22985781990521326,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.23222748815165878,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.23459715639810427,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.23696682464454977,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.23933649289099526,"0.0
0.2
0.4
0.6
0.8
1.0 60 40 20 0 20 40 mbb/h"
SAFE OPPONENT SEARCH,0.24170616113744076,Evaluation(Prshuffle = 0.3)
SAFE OPPONENT SEARCH,0.24407582938388625,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.24644549763033174,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.24881516587677724,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.25118483412322273,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.25355450236966826,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.2559241706161137,"0.0
0.2
0.4
0.6
0.8
1.0
340 350 360 370 380 390 mbb/h"
SAFE OPPONENT SEARCH,0.25829383886255924,Evaluation(Prshuffle = 0.7)
SAFE OPPONENT SEARCH,0.26066350710900477,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.26303317535545023,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.26540284360189575,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.2677725118483412,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.27014218009478674,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.2725118483412322,"0.0
0.2
0.4
0.6
0.8
1.0
0 50 100 150 200 250 300 350 mbb/h"
SAFE OPPONENT SEARCH,0.27488151658767773,Exploitability(Prshuffle = 0.0)
SAFE OPPONENT SEARCH,0.2772511848341232,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.2796208530805687,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.28199052132701424,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.2843601895734597,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.28672985781990523,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.2890995260663507,"0.0
0.2
0.4
0.6
0.8
1.0
0 50 100 150 200 250 300 350 mbb/h"
SAFE OPPONENT SEARCH,0.2914691943127962,Exploitability(Prshuffle = 0.3)
SAFE OPPONENT SEARCH,0.2938388625592417,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.2962085308056872,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.2985781990521327,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.3009478672985782,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.3033175355450237,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.3056872037914692,"0.0
0.2
0.4
0.6
0.8
1.0
0 50 100 150 200 250 300 350 mbb/h"
SAFE OPPONENT SEARCH,0.3080568720379147,Exploitability(Prshuffle = 0.7)
SAFE OPPONENT SEARCH,0.3104265402843602,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.3127962085308057,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.31516587677725116,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.3175355450236967,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.31990521327014215,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.3222748815165877,"0.0
0.2
0.4
0.6
0.8
1.0 0 1 2 3 4 5 mbb/h"
SAFE OPPONENT SEARCH,0.3246445497630332,Evaluation(Prshuffle = 0.0)
SAFE OPPONENT SEARCH,0.32701421800947866,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.3293838862559242,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.33175355450236965,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.3341232227488152,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.33649289099526064,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.33886255924170616,"0.0
0.2
0.4
0.6
0.8
1.0 4 5 6 7 8 9 10 mbb/h"
SAFE OPPONENT SEARCH,0.3412322274881517,Evaluation(Prshuffle = 0.3)
SAFE OPPONENT SEARCH,0.34360189573459715,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.3459715639810427,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.34834123222748814,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.35071090047393366,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.35308056872037913,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.35545023696682465,"0.0
0.2
0.4
0.6
0.8
1.0 10 12 14 16 18 mbb/h"
SAFE OPPONENT SEARCH,0.3578199052132701,Evaluation(Prshuffle = 0.7)
SAFE OPPONENT SEARCH,0.36018957345971564,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.36255924170616116,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.36492890995260663,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.36729857819905215,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.3696682464454976,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.37203791469194314,"0.0
0.2
0.4
0.6
0.8
1.0
20 25 30 35 40 45 50 mbb/h"
SAFE OPPONENT SEARCH,0.3744075829383886,Exploitability(Prshuffle = 0.0)
SAFE OPPONENT SEARCH,0.3767772511848341,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.3791469194312796,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.3815165876777251,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.38388625592417064,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.3862559241706161,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.3886255924170616,"0.0
0.2
0.4
0.6
0.8
1.0 25 30 35 40 45 50 mbb/h"
SAFE OPPONENT SEARCH,0.3909952606635071,Exploitability(Prshuffle = 0.3)
SAFE OPPONENT SEARCH,0.3933649289099526,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.3957345971563981,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.3981042654028436,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.4004739336492891,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.4028436018957346,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.4052132701421801,"0.0
0.2
0.4
0.6
0.8
1.0 25 30 35 40 45 50 mbb/h"
SAFE OPPONENT SEARCH,0.4075829383886256,Exploitability(Prshuffle = 0.7)
SAFE OPPONENT SEARCH,0.4099526066350711,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.41232227488151657,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.4146919431279621,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.41706161137440756,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.4194312796208531,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.4218009478672986,"Figure 3: Row 1&2: Experiment results on Leduc poker. Row 3&4: Experiment results on FHP.
From left to right, each column represents a type of opponent with Prshufﬂe = 0.0, 0.3, 0.7. The ﬁrst
row of each game shows the head-to-head expected payoffs against corresponding opponents, while
the second row demonstrates the exploitability calculated for different reﬁned strategies. The X-axis
is the parameter α which controls exploitation level."
SAFE OPPONENT SEARCH,0.42417061611374407,"strategy when ϵ = 0.0, 0.3, 0.6, 0.9, 1.2. The blue line is the result of blueprint strategy without
conducting any search."
SAFE OPPONENT SEARCH,0.4265402843601896,"Generally speaking, SES balances between safety and opponent exploitation. The increase of
exploitation level α helps win more chips from opponents, while resulting in the increase of the
strategy’s own exploitability. As can be seen in Figure 3, the exploitability increases when the
exploitation level α grows from 0 to 1, which is consistent with Theorem 1. One exception is in FHP
when ϵ is small: the exploitability surprisingly keeps decreasing even if SES puts more emphasis
on opponent exploitation. Similar situations have also occurred in previous literature (Brown &
Sandholm, 2017). The reason is that our opponent is quite close to NE outside the subgame which
will make ˆp close to ˜p when ϵ is small, which means the τ in Theorem 1 is small. As a result, we will
have a low-exploitability resolved strategy when using unsafe search and the exploitability increases
as ϵ increases."
SAFE OPPONENT SEARCH,0.42890995260663506,"When the estimation is completely correct (ϵ = 0.0, the yellow line), the expected payoff in FHP
increases as the exploitation level α grows higher. In Leduc poker, since the game is very small,
the pre-computed blueprint is very close to NE. Therefore, when confronted with relatively strong
opponents (Prshuﬄe = 0.0, 0.3) which are also close to NE, actually few things can be done other"
SAFE OPPONENT SEARCH,0.4312796208530806,Under review as a conference paper at ICLR 2022
SAFE OPPONENT SEARCH,0.43364928909952605,"than sticking with the blueprint. So the improvement introduced by SES is small. When facing
relatively weak opponent (Prshuﬄe = 0.7), the improvement margin is slightly larger."
SAFE OPPONENT SEARCH,0.43601895734597157,"SES relies on an estimation of opponent’s strategy. In order to test the robustness of our algorithm
when the prediction of p(Ii
1) is not accurate, we evaluate the performance of our algorithm with
different values of estimation error ϵ. As illustrated in Figure.3, the exploitability increases and the
expected payoff drops when ϵ grows larger. The result is expected since an accurate estimation always
provides beneﬁts. However, it also demonstrates that SES can still achieve a trade-off between safety
and opponent exploitation even when ϵ is considerably high. For instance, in FHP, ϵ is between 0 and
2, and ϵ = 1.2 means that the predicted distribution is almost random. When ϵ ≤0.6, the expected
payoff still keeps increasing with respect to α. In case of a bad estimation, we can always choose
smaller α to ensure safety."
COMPARISON WITH RESTRICTED NASH RESPONSE,0.43838862559241704,"5.3
COMPARISON WITH RESTRICTED NASH RESPONSE"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.44075829383886256,"0.0
0.2
0.4
0.6
0.8
1.0
p( ) 20 15 10 5 0 5 mbb/h"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.4431279620853081,Evaluation(Prshuffle = 0.0)
COMPARISON WITH RESTRICTED NASH RESPONSE,0.44549763033175355,"RNR(normal)
RNR(big)
SES"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.44786729857819907,"0.0
0.2
0.4
0.6
0.8
1.0
p( ) 15 10 5 0 5 10 mbb/h"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.45023696682464454,Evaluation(Prshuffle = 0.3)
COMPARISON WITH RESTRICTED NASH RESPONSE,0.45260663507109006,"RNR(normal)
RNR(big)
SES"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.4549763033175355,"0.0
0.2
0.4
0.6
0.8
1.0
p( ) 5 0 5 10 15 mbb/h"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.45734597156398105,Evaluation(Prshuffle = 0.7)
COMPARISON WITH RESTRICTED NASH RESPONSE,0.4597156398104265,"RNR(normal)
RNR(big)
SES"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.46208530805687204,"0.0
0.2
0.4
0.6
0.8
1.0
p( ) 20 30 40 50 60 70 mbb/h"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.46445497630331756,Exploitability(Prshuffle = 0.0)
COMPARISON WITH RESTRICTED NASH RESPONSE,0.466824644549763,"RNR(normal)
RNR(big)
SES"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.46919431279620855,"0.0
0.2
0.4
0.6
0.8
1.0
p( ) 20 30 40 50 60 70 mbb/h"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.471563981042654,Exploitability(Prshuffle = 0.3)
COMPARISON WITH RESTRICTED NASH RESPONSE,0.47393364928909953,"RNR(normal)
RNR(big)
SES"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.476303317535545,"0.0
0.2
0.4
0.6
0.8
1.0
p( ) 30 40 50 60 70 mbb/h"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.4786729857819905,Exploitability(Prshuffle = 0.7)
COMPARISON WITH RESTRICTED NASH RESPONSE,0.48104265402843605,"RNR(normal)
RNR(big)
SES"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.4834123222748815,"Figure 4: Comparison between SES and RNR. From left to right, each column represents a type of
opponent with Prshufﬂe = 0.0, 0.3, 0.7. The X-axis is the parameter α for SES and p for RNR."
COMPARISON WITH RESTRICTED NASH RESPONSE,0.48578199052132703,"We also compare SES with restricted Nash response (RNR) (Johanson et al., 2007) in FHP. In each
round, we limit the computation time of RNR(normal) to 10 CPU second1, which is the same for
SES. However, as stated in section 2, RNR needs to recompute a strategy for the whole game in each
round. It cannot converge in 10s. So we also compare with RNR(big), which has a budget of 10M
CFR iterations in each round (around 190 CPU second in time). In contrast, SES only uses 10M
CFR iterations to calculate its blueprint once. As is shown in Figure 4, SES signiﬁcantly outperforms
RNR(normal) in both exploitability and evaluation. SES also achieves much lower exploitability than
RNR(big) and comparable evaluation results with much less computation time."
CONCLUSION,0.4881516587677725,"6
CONCLUSION"
CONCLUSION,0.490521327014218,"We propose a novel safe exploitation search (SES) algorithm which uniﬁes both safe search and
opponent exploitation. With the aid of real-time search, SES can make online adaptations to a
changing opponent model. We also prove safety and opponent exploitation guarantees of SES in
Theorem 1 and Theorem 2. The experimental results in our designed matrix game conﬁrm the
existence of the reﬁned strategy which is both safe and actively exploiting the opponent. In games of
poker, our method outperforms NE baselines while keeping exploitability low. SES is also much more
efﬁcient than previous safe exploitation algorithms without search. The exploitation level α is now
regarded as a hyperparameter in our algorithm. However, ideally, α should be learnt automatically
from opponents, and should be adaptive to opponent’s strategy change. We leave this for future work."
CONCLUSION,0.4928909952606635,1We test it on Intel(R) Xeon(R) Platinum 8276L CPU @ 2.20GHz
CONCLUSION,0.495260663507109,Under review as a conference paper at ICLR 2022
REFERENCES,0.4976303317535545,REFERENCES
REFERENCES,0.5,"Stefano V. Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive
survey and open problems. Artiﬁcial Intelligence, 258:66–95, 2018. ISSN 0004-3702. doi:
https://doi.org/10.1016/j.artint.2018.01.002. URL https://www.sciencedirect.com/
science/article/pii/S0004370218300249."
REFERENCES,0.5023696682464455,"Darse Billings, Neil Burch, Aaron Davidson, Robert Holte, Jonathan Schaeffer, Terence Schauenberg,
and Duane Szafron. Approximating game-theoretic optimal strategies for full-scale poker. In
IJCAI, volume 3, pp. 661, 2003."
REFERENCES,0.504739336492891,"Noam Brown and Tuomas Sandholm. Safe and nested subgame solving for imperfect-information
games. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, pp. 689–699, 2017."
REFERENCES,0.5071090047393365,"Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):
885–890, 2019."
REFERENCES,0.509478672985782,"Noam
Brown,
Tuomas
Sandholm,
and
Brandon
Amos.
Depth-limited
solving
for
imperfect-information games.
In Advances in Neural Information Processing Sys-
tems
31:
Annual
Conference
on
Neural
Information
Processing
Systems,
Mon-
tréal,
Canada,
pp. 7674–7685,
2018.
URL http://papers.nips.cc/paper/
7993-depth-limited-solving-for-imperfect-information-games."
REFERENCES,0.5118483412322274,"Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret mini-
mization. In International conference on machine learning, pp. 793–802. PMLR, 2019."
REFERENCES,0.514218009478673,"Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. Combining deep reinforcement
learning and search for imperfect-information games. arXiv preprint arXiv:2007.13544, 2020."
REFERENCES,0.5165876777251185,"Neil Burch, Michael Johanson, and Michael Bowling. Solving imperfect information games using
decomposition. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 28,
2014a."
REFERENCES,0.518957345971564,"Neil Burch, Michael Johanson, and Michael Bowling. Solving imperfect information games using
decomposition. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 28,
2014b."
REFERENCES,0.5213270142180095,"David Carmel and Shaul Markovitch. Learning models of intelligent agents. In AAAI/IAAI, Vol. 1,
pp. 62–67, 1996."
REFERENCES,0.523696682464455,"Jakob Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor
Mordatch. Learning with opponent-learning awareness. In Proceedings of the 17th International
Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’18, pp. 122–130, Richland,
SC, 2018. International Foundation for Autonomous Agents and Multiagent Systems."
REFERENCES,0.5260663507109005,"Sam Ganzfried and Tuomas Sandholm. Safe opponent exploitation. ACM Transactions on Economics
and Computation (TEAC), 3(2):1–28, 2015a."
REFERENCES,0.5284360189573459,"Sam Ganzfried and Tuomas Sandholm. Endgame solving in large imperfect-information games. In
Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,
pp. 37–45. Citeseer, 2015b."
REFERENCES,0.5308056872037915,"Andrew Gilpin and Tuomas Sandholm. A competitive texas hold’em poker player via automated
abstraction and real-time equilibrium computation. In AAAI, pp. 1007–1013, 2006."
REFERENCES,0.533175355450237,"Andrew Gilpin and Tuomas Sandholm. Better automated abstraction techniques for imperfect
information games, with application to texas hold’em poker. In Proceedings of the 6th international
joint conference on Autonomous agents and multiagent systems, pp. 1–8, 2007."
REFERENCES,0.5355450236966824,"He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daumé, III. Opponent modeling in deep
reinforcement learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of
The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pp. 1804–1813, New York, New York, USA, 20–22 Jun 2016. PMLR. URL
http://proceedings.mlr.press/v48/he16.html."
REFERENCES,0.5379146919431279,Under review as a conference paper at ICLR 2022
REFERENCES,0.5402843601895735,"Michael Johanson, Michael Bowling, and Martin Zinkevich. Computing robust counter-strategies.
2007."
REFERENCES,0.542654028436019,"Michael Johanson, Neil Burch, Richard Valenzano, and Michael Bowling. Evaluating state-space
abstractions in extensive-form games. In Proceedings of the 2013 international conference on
Autonomous agents and multi-agent systems, pp. 271–278, 2013."
REFERENCES,0.5450236966824644,"Daphne Koller, Nimrod Megiddo, and Bernhard Von Stengel. Fast algorithms for ﬁnding randomized
strategies in game trees. In Proceedings of the twenty-sixth annual ACM symposium on Theory of
computing, pp. 750–759, 1994."
REFERENCES,0.54739336492891,"Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael H Bowling. Monte carlo sampling for
regret minimization in extensive games. In NIPS, pp. 1078–1086, 2009."
REFERENCES,0.5497630331753555,"Adam Lerer, Hengyuan Hu, Jakob Foerster, and Noam Brown. Improving policies via search
in cooperative partially observable games. Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, 34(05):7187–7194, Apr. 2020.
doi: 10.1609/aaai.v34i05.6208.
URL https:
//ojs.aaai.org/index.php/AAAI/article/view/6208."
REFERENCES,0.5521327014218009,"Xun Li and Risto Miikkulainen. Dynamic adaptation and opponent exploitation in computer poker.
In Workshops at the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018."
REFERENCES,0.5545023696682464,"Peter McCracken and Michael Bowling. Safe strategies for agent modelling in games. In Artiﬁcial
Multiagent Learning, Papers from the 2004 AAAI Fall Symposium. Arlington, VA, USA, October
22-24, 2004, volume FS-04-02, pp. 103–110. AAAI Press, 2004. URL https://www.aaai.
org/Library/Symposia/Fall/2004/fs04-02-014.php."
REFERENCES,0.556872037914692,"Matej Moravcik, Martin Schmid, Karel Ha, Milan Hladik, and Stephen Gaukrodger. Reﬁning
subgames in large imperfect information games. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 30, 2016."
REFERENCES,0.5592417061611374,"Matej Moravcík, Martina Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor Davis,
Kevin Waugh, Michael Johanson, and Michael H. Bowling. Deepstack: Expert-level artiﬁcial
intelligence in heads-up no-limit poker. Science, 356:508–513, 2017."
REFERENCES,0.5616113744075829,"Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, S. M. Ali Eslami, and Matthew
Botvinick. Machine theory of mind. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 4218–4227. PMLR, 10–15 Jul 2018. URL http://proceedings.
mlr.press/v80/rabinowitz18a.html."
REFERENCES,0.5639810426540285,"Roberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in
multi-agent reinforcement learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 4257–4266. PMLR, 10–15 Jul 2018. URL http://proceedings.
mlr.press/v80/raileanu18a.html."
REFERENCES,0.566350710900474,"Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020."
REFERENCES,0.5687203791469194,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the
game of go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016. doi:
10.1038/nature16961. URL https://doi.org/10.1038/nature16961."
REFERENCES,0.5710900473933649,"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi
by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815,
2017."
REFERENCES,0.5734597156398105,Under review as a conference paper at ICLR 2022
REFERENCES,0.5758293838862559,"Finnegan Southey, Michael Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings,
and Chris Rayner. Bayes’ bluff: opponent modelling in poker. In Proceedings of the Twenty-First
Conference on Uncertainty in Artiﬁcial Intelligence, pp. 550–558, 2005."
REFERENCES,0.5781990521327014,"Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up limit
texas hold’em. In Twenty-fourth international joint conference on artiﬁcial intelligence, 2015."
REFERENCES,0.580568720379147,"Yuandong Tian, Qucheng Gong, and Tina Jiang. Joint policy search for multi-agent collaboration
with imperfect information. arXiv preprint arXiv:2008.06495, 2020a."
REFERENCES,0.5829383886255924,"Yuandong Tian, Qucheng Gong, and Yu Jiang. Joint policy search for multi-agent collaboration with
imperfect information. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 19931–19942. Curran Asso-
ciates, Inc., 2020b. URL https://proceedings.neurips.cc/paper/2020/file/
e64f346817ce0c93d7166546ac8ce683-Paper.pdf."
REFERENCES,0.5853080568720379,"Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization
in games with incomplete information. Advances in neural information processing systems, 20:
1729–1736, 2007."
REFERENCES,0.5876777251184834,Under review as a conference paper at ICLR 2022
REFERENCES,0.590047393364929,"A
PROOFS"
REFERENCES,0.5924170616113744,"A.1
PROOF OF THEOREM 1"
REFERENCES,0.5947867298578199,"Theorem 1. (safety) Let S be a disjoint set of subgames S. Let σ∗= ⟨σ∗
1, σ∗
2⟩be the NE where P2’s
strategy is constrained to be the same with σ2 outside S. Deﬁne ∆= maxS∈S,Ii
1∈Stop |CBV σ∗
2
1 (Ii
1)−
vσ
1 (Ii
1)|. Let ˜p(Ii
1) be the reach probability given by σ∗
1. Let ˆp(Ii
1) be the estimation of reach
probability p(Ii
1) given by the real opponent strategy. Deﬁne τ = maxS∈S,Ii
1∈Stop | ˆp(Ii
1)−˜p(Ii
1)
˜p(Ii
1)
|.
Whenever 1 −(2τ + 1)α > 0, we have a bounded exploitability given by:"
REFERENCES,0.5971563981042654,"exp(σ′
2) ≤exp(σ∗
2) +
2
1 −(2τ + 1)α∆.
(5)"
REFERENCES,0.5995260663507109,Proof:
REFERENCES,0.6018957345971564,"For simplicity, we will omit the subscript of CBV σ∗
2
1
by default. In order to prove Theorem 1, we
will use mathematical induction on the level of the infoset. The depth L has the same deﬁnition as in
Brown & Sandholm (2017), i.e."
REFERENCES,0.6042654028436019,"• For all the infosets which are direct parents of the subgames, we deﬁne L(I) = 0.
• For the infosets that are not ancestors of the subgames, we deﬁne L(I) = 0.
• For any infosets that are ancestors of the subgames, we deﬁne
L(I) = maxI′∈succ(I) L(I′) + 1. That is, it has a higher level than any of its successors."
REFERENCES,0.6066350710900474,BASE CASE OF INDUCTION
REFERENCES,0.6090047393364929,"Firstly, we will prove that for any infoset with level 0, the inequality of theorem 1 holds. For
convenience, we consider that theorem 1 in a speciﬁc subgame S."
REFERENCES,0.6113744075829384,"We will prove the infoset at the top of the subgame ﬁrst. Since SE(σ∗
2) ≥(1 −α)(−∆) +
α P"
REFERENCES,0.6137440758293838,"i ˆp(Ii
1)(−∆) = −∆, we have"
REFERENCES,0.6161137440758294,"(1 −α) min
Ij
1"
REFERENCES,0.6184834123222749,"
vσ
1 (Ij
1) −CBV σS
2 (Ij
1)

+ α
X"
REFERENCES,0.6208530805687204,"i
ˆp(Ii
1)(vσ
1 (Ii
1) −CBV σS
2 (Ii
1))"
REFERENCES,0.6232227488151659,"=SE(σS
2 )
≥SE(σ∗
2)
≥−∆ (6)"
REFERENCES,0.6255924170616114,"since σS
2 = arg max˜σ2 SE(˜σ2)."
REFERENCES,0.6279620853080569,"Furthermore, we have
X"
REFERENCES,0.6303317535545023,"i
ˆp(Ii
1)(vσ
1 (Ii
1) −CBV σS
2 (Ii
1)) =
X"
REFERENCES,0.6327014218009479,"i
ˆp(Ii
1)(vσ
1 (Ii
1) −CBV σ∗
2 (Ii
1)) +
X"
REFERENCES,0.6350710900473934,"i
ˆp(Ii
1)(CBV σ∗
2 (Ii
1) −CBV σS
2 (Ii
1)) ≤∆+
X"
REFERENCES,0.6374407582938388,"i
ˆp(Ii
1)(CBV σ∗
2 (Ii
1) −CBV σS
2 (Ii
1)) =∆+
X"
REFERENCES,0.6398104265402843,"i
˜p(Ii
1)(CBV σ∗
2 (Ii
1) −CBV σS
2 (Ii
1)) +
X"
REFERENCES,0.6421800947867299,"i
(ˆp(Ii
1) −˜p(Ii
1))(CBV σ∗
2 (Ii
1) −CBV σS
2 (Ii
1)) (7)"
REFERENCES,0.6445497630331753,where the second term P
REFERENCES,0.6469194312796208,"i ˜p(Ii
1)(CBV σ∗
2 (Ii
1) −CBV σS
2 (Ii
1)) is no larger than 0 because
P"
REFERENCES,0.6492890995260664,"i ˜p(I1)CBV σ∗
2 (Ii
1) is exactly what σ∗
2 minimized. Otherwise, σ∗
2 can change the strategy in
the subgame so that he will get higher reward against σ∗
1 which conﬂicts the deﬁnition of NE."
REFERENCES,0.6516587677725119,Under review as a conference paper at ICLR 2022
REFERENCES,0.6540284360189573,"And we will further decompose Ii
1 ∈Stop into two parts, {Ii,−
1
} and {Ii,+
1
}. They have the property
that CBV σ∗
2 (Ii,−
1
) −CBV σS
2 (Ii,−
1
) ≤0 and CBV σ∗
2 (Ii,+
1
) −CBV σS
2 (Ii,+
1
) > 0. And since
P"
REFERENCES,0.6563981042654028,"i ˜p(Ii
1)(CBV σ∗
2 (Ii
1) −CBV σS
2 (Ii
1)) ≤0 as discussed above, we have
X"
REFERENCES,0.6587677725118484,"Ii,−
1"
REFERENCES,0.6611374407582938,"˜p(Ii
1)(CBV σ∗
2 (Ii,−
1
) −CBV σS
2 (Ii,−
1
)) +
X"
REFERENCES,0.6635071090047393,"Ii,+
1"
REFERENCES,0.6658767772511849,"˜p(Ii
1)(CBV σ∗
2 (Ii,+
1
) −CBV σS
2 (Ii,+
1
)) =
X"
REFERENCES,0.6682464454976303,"i
˜p(Ii
1)(CBV σ∗
2 (Ii
1) −CBV σS
2 (Ii
1)) ≤0 (8)"
REFERENCES,0.6706161137440758,"which implies that
X"
REFERENCES,0.6729857819905213,"Ii,+
1"
REFERENCES,0.6753554502369669,"˜p(Ii
1)(CBV σ∗
2 (Ii,+
1
) −CBV σS
2 (Ii,+
1
)) ≤−
X"
REFERENCES,0.6777251184834123,"Ii,−
1"
REFERENCES,0.6800947867298578,"˜p(Ii
1)(CBV σ∗
2 (Ii,−
1
) −CBV σS
2 (Ii,−
1
)) (9)"
REFERENCES,0.6824644549763034,"Then we have
X"
REFERENCES,0.6848341232227488,"i
(ˆp(Ii
1) −˜p(Ii
1))(CBV σ∗
2 (Ii
1) −CBV σS
2 (Ii
1)) =
X"
REFERENCES,0.6872037914691943,"Ii,−
1"
REFERENCES,0.6895734597156398,"(ˆp(Ii,−
1
) −˜p(Ii,−
1
))(CBV σ∗
2 (Ii,−
1
) −CBV σS
2 (Ii,−
1
)) +
X"
REFERENCES,0.6919431279620853,"Ii,+
1"
REFERENCES,0.6943127962085308,"(ˆp(Ii,+
1
) −˜p(Ii,+
1
))(CBV σ∗
2 (Ii,+
1
) −CBV σS
2 (I
Ii,+
1
1
))"
REFERENCES,0.6966824644549763,"≤τ

−
X"
REFERENCES,0.6990521327014217,"Ii,−
1"
REFERENCES,0.7014218009478673,"˜p(Ii,−
1
)(CBV σ∗
2 (Ii,−
1
) −CBV σS
2 (I
Ii,−
1
1
)) +
X"
REFERENCES,0.7037914691943128,"Ii,+
1"
REFERENCES,0.7061611374407583,"˜p(Ii,+
1
)(CBV σ∗
2 (Ii,+
1
) −CBV σS
2 (I
Ii,+
1
1
))
"
REFERENCES,0.7085308056872038,"≤−2τ
X"
REFERENCES,0.7109004739336493,"Ii,−
1"
REFERENCES,0.7132701421800948,"˜p(Ii,−
1
)(CBV σ∗
2 (Ii,−
1
) −CBV σS
2 (I
Ii,−
1
1
))"
REFERENCES,0.7156398104265402,"≤−2τ min
Ij
1"
REFERENCES,0.7180094786729858,"
CBV σ∗
2 (Ij
1) −CBV σS
2 (Ij
1)
 (10)"
REFERENCES,0.7203791469194313,"The last inequation holds since minIj
1"
REFERENCES,0.7227488151658767,"
CBV σ∗
2 (Ij
1) −CBV σS
2 (Ij
1)

≤0 since σ∗
2 is the strategy
with lowest exploitability by only changing strategy of σ2 in the subgames."
REFERENCES,0.7251184834123223,"Back to Equation 7, we have
X"
REFERENCES,0.7274881516587678,"i
ˆp(Ii
1)(vσ
1 (Ii
1) −CBV σS
2 (Ii
1)) ≤∆−2τ min
Ij
1"
REFERENCES,0.7298578199052133,"
CBV σ∗
2 (Ij
1) −CBV σS
2 (Ij
1)

(11)"
REFERENCES,0.7322274881516587,"And substitute it into Equation 6,"
REFERENCES,0.7345971563981043,"∆+ (1 −α −2ατ) min
Ij
1"
REFERENCES,0.7369668246445498,"
CBV σ∗
2 (Ij
1) −CBV σS
2 (Ij
1)
"
REFERENCES,0.7393364928909952,"≥(1 −α) min
Ij
1"
REFERENCES,0.7417061611374408,"
vσ
1 (Ij
1) −CBV σS
2 (Ij
1)

+ α(∆−2τ) min
Ij
1"
REFERENCES,0.7440758293838863,"
CBV σ∗
2 (Ij
1) −CBV σS
2 (Ij
1)
 ≥−∆ (12)"
REFERENCES,0.7464454976303317,"so that
CBV σS
2 (Ij
1) ≤CBV σ∗
2 (Ij
1) +
2
1 −(2τ + 1)α∆
(13)"
REFERENCES,0.7488151658767772,"for all Ij
1 in the subgame."
REFERENCES,0.7511848341232228,"And for infoset I out of the subgame with level 0, since the reﬁned strategy σS
2 and blueprint strategy
σ2 are the same here, the CBV value is exactly the same and the inequality holds."
REFERENCES,0.7535545023696683,Under review as a conference paper at ICLR 2022
REFERENCES,0.7559241706161137,INDUCTIVE STEP
REFERENCES,0.7582938388625592,The inductive step mostly follows that of Brown & Sandholm (2017).
REFERENCES,0.7606635071090048,"Since CBV σS
2 (I1) ≤CBV σ∗
2 (I1) +
2
1−(2τ+1)α∆holds for every subgame S, σ′
2 will also satisfy
this inequation since ∆and τ are deﬁned as maximum over all subgames."
REFERENCES,0.7630331753554502,"Now, suppose CBV σ′
2(I1) ≤CBV σ∗
2 (I1) +
2
1−(2τ+1)α∆holds for any infoset with level lower or
equal to k, we will prove that it also holds for infoset with level k + 1."
REFERENCES,0.7654028436018957,"By deﬁnition of CBV (I1),"
REFERENCES,0.7677725118483413,"CBV σ2(I1, a) =
 X"
REFERENCES,0.7701421800947867,"h∈I1
πσ2
−1(h)v⟨CBR(σ2),σ2⟩(h · a)

/
X"
REFERENCES,0.7725118483412322,"h∈I1
πσ2
−1(h) =
 X"
REFERENCES,0.7748815165876777,"h∈I1
πσ2
−1(h)
X"
REFERENCES,0.7772511848341233,"h′∈succ(h,a)
πσ2
−1(h, h′)v⟨CBR(σ2),σ2⟩(h′)

/
X"
REFERENCES,0.7796208530805687,"h∈I1
πσ2
−1(h) =
 X h∈I1 X"
REFERENCES,0.7819905213270142,"h′∈succ(h,a)
πσ2
−1(h′)v⟨CBR(σ2),σ2⟩(h′)

/
X"
REFERENCES,0.7843601895734598,"h∈I1
πσ2
−1(h) (14)"
REFERENCES,0.7867298578199052,"We can swap the two summations above since the game is perfect recall, then"
REFERENCES,0.7890995260663507,"CBV σ2(I1, a) =

X"
REFERENCES,0.7914691943127962,"I′
1∈succ(I1,a) X"
REFERENCES,0.7938388625592417,"h′∈I′
1
πσ2
−1(h′)v⟨CBR(σ2),σ2⟩(h′)

/
X"
REFERENCES,0.7962085308056872,"h∈I1
πσ2
−1(h)
(15)"
REFERENCES,0.7985781990521327,"By substituting the deﬁnition of CBV (I′
1) into the equation above,"
REFERENCES,0.8009478672985783,"CBV σ2(I1, a) =

X"
REFERENCES,0.8033175355450237,"I′
1∈succ(I1,a)
CBV σ2(I′
1)
X"
REFERENCES,0.8056872037914692,"h′∈I′
1
πσ2
−1(h′)

/
X"
REFERENCES,0.8080568720379147,"h∈I1
πσ2
−1(h)
(16)"
REFERENCES,0.8104265402843602,"And by the induction hypothesis,"
REFERENCES,0.8127962085308057,"CBV σ2(I1, a) ≤

X"
REFERENCES,0.8151658767772512,"I′
1∈succ(I1,a)
(CBV σ∗
2 (I′
1) + 2 −α"
REFERENCES,0.8175355450236966,"1 −α∆)
X"
REFERENCES,0.8199052132701422,"h′∈I′
1
πσ2
−1(h′)

/
X"
REFERENCES,0.8222748815165877,"h∈I1
πσ2
−1(h)
(17)"
REFERENCES,0.8246445497630331,"Because I1 is out of the subgame and σ∗
2, σ2 is exactly the same outside the subgame, we will get"
REFERENCES,0.8270142180094787,"CBV σ2(I1, a) ≤

X"
REFERENCES,0.8293838862559242,"I′
1∈succ(I1,a)
(CBV σ∗
2 (I′
1) + 2 −α"
REFERENCES,0.8317535545023697,"1 −α∆)
X"
REFERENCES,0.8341232227488151,"h′∈I′
1
πσ∗
2
−1(h′)

/
X"
REFERENCES,0.8364928909952607,"h∈I1
πσ∗
2
−1(h)"
REFERENCES,0.8388625592417062,"= CBV σ∗
2 (I1, a) + 2 −α"
REFERENCES,0.8412322274881516,"1 −α∆

X"
REFERENCES,0.8436018957345972,"I′
1∈succ(I1,a) X"
REFERENCES,0.8459715639810427,"h′∈I′
1
πσ∗
2
−1(h′)

/
X"
REFERENCES,0.8483412322274881,"h∈I1
πσ∗
2
−1(h)"
REFERENCES,0.8507109004739336,"= CBV σ∗
2 (I1, a) + 2 −α 1 −α∆ (18)"
REFERENCES,0.8530805687203792,"Finally, by mathematical induction we get"
REFERENCES,0.8554502369668247,"exp(σ′
2) ≤exp(σ∗
2) +
2
1 −(2τ + 1)α∆
(19)"
REFERENCES,0.8578199052132701,"A.2
PROOF OF THEOREM 2"
REFERENCES,0.8601895734597157,"Theorem 2. (opponent exploitation) Let ϵ = ∥ˆp−p∥1 be the L1 distance of the distribution p(Ii
1) and"
REFERENCES,0.8625592417061612,"ˆp(Ii
1). Let η = minS∈S maxIj
1∈Stop"
REFERENCES,0.8649289099526066,"
CBV1(Ij
1, σS
2 ) −CBV1(Ij
1, σ∗
2)

≥0. We use BR[S,σp]
p
(σ)
to denote the strategy for player p which maximizes its utility in subgame S ∈S against σ−p under
the constraint that BR[S,σp]
p
(σ) and σp differs only inside S. By maximizing objective 2, for all S ∈S,
the reﬁned strategy σ′
2 satisﬁes u"
REFERENCES,0.8672985781990521,"D
BR[S,σ1]
1
(σ′
2),σ′
2
E"
REFERENCES,0.8696682464454977,"2
(S) ≥u"
REFERENCES,0.8720379146919431,"D
BR[S,σ1]
1
(σ∗
2),σ∗
2
E"
REFERENCES,0.8744075829383886,"2
(S) + 1 −α"
REFERENCES,0.8767772511848341,"α
(η −2∆) −ϵη
(20)"
REFERENCES,0.8791469194312796,Under review as a conference paper at ICLR 2022
REFERENCES,0.8815165876777251,"Proof: Still, we only consider a speciﬁc subgame S ﬁrst."
REFERENCES,0.8838862559241706,"σS
2 is maximizing"
REFERENCES,0.8862559241706162,"(1 −α) min
Ij
1"
REFERENCES,0.8886255924170616,"
vσ
1 (Ij
1) −CBV σS
2 (Ij
1)
"
REFERENCES,0.8909952606635071,"|
{z
}
g(σS
2 ) +α
X"
REFERENCES,0.8933649289099526,"i
ˆp(Ii
1)(vσ
1 (Ii
1) −CBV σS
2 (Ii
1))"
REFERENCES,0.8957345971563981,"|
{z
}
f(σS
2 ) (21)"
REFERENCES,0.8981042654028436,"So, we have"
REFERENCES,0.9004739336492891,"(1 −α)g(σS
2 ) + αf(σS
2 ) ≥(1 −α)g(σ∗
2) + αf(σ∗
2)
(22) and"
REFERENCES,0.9028436018957346,"max
Ij
1
CBV (Ij
1, σS
2 ) −CBV (Ij
1, σ∗
2) = η ≥η"
REFERENCES,0.9052132701421801,"⇔g(σS
2 ) −∆≤−η"
REFERENCES,0.9075829383886256,"⇔g(σS
2 ) −∆≤∆+ g(σ∗
2) −η
(g(σ∗
2) ≥−∆) (23)"
REFERENCES,0.909952606635071,"Therefore,"
REFERENCES,0.9123222748815166,"αf(σS
2 ) ≥αf(σ∗
2) + (1 −α)(η −2∆)
(24)"
REFERENCES,0.9146919431279621,"which means
X"
REFERENCES,0.9170616113744076,"i
ˆp(Ii
1)(CBV σ∗
2 (I1) −CBV σS
2 (Ii
1)) ≥1 −α"
REFERENCES,0.919431279620853,"α
(η −2∆) +
X"
REFERENCES,0.9218009478672986,"i
ˆp(Ii
1)(CBV σ∗
2 (I1) −CBV σ∗
2 (Ii
1)) ⇔−
X"
REFERENCES,0.9241706161137441,"i
ˆp(Ii
1)CBV σS
2 (Ii
1) ≥1 −α"
REFERENCES,0.9265402843601895,"α
(η −2∆) −
X"
REFERENCES,0.9289099526066351,"i
ˆp(Ii
1)CBV σ∗
2 (Ii
1) ⇔−
X"
REFERENCES,0.9312796208530806,"i
p(Ii
1)CBV σS
2 (Ii
1) ≥1 −α"
REFERENCES,0.933649289099526,"α
(η −2∆) −
X"
REFERENCES,0.9360189573459715,"i
p(Ii
1)CBV σ∗
2 (Ii
1) −
X"
REFERENCES,0.9383886255924171,"i
(p(Ii
1) −ˆp(Ii
1))(CBV σS
2 (Ii
1) −CBV σ∗
2 (Ii
1)) ⇒−
X"
REFERENCES,0.9407582938388626,"i
p(Ii
1)CBV σS
2 (Ii
1) ≥1 −α"
REFERENCES,0.943127962085308,"α
(η −2∆) −
X"
REFERENCES,0.9454976303317536,"i
p(Ii
1)CBV σ∗
2 (Ii
1) −ϵη ⇔
X"
REFERENCES,0.9478672985781991,"i
p(Ii
1)V2(Ii
1, BR(σS
2 ), σS
2 ) ≥1 −α"
REFERENCES,0.9502369668246445,"α
(η −2∆) −ϵη +
X"
REFERENCES,0.95260663507109,"i
p(Ii
1)V2(Ii
1, BR(σ∗
2), σ∗
2)"
REFERENCES,0.9549763033175356,"⇔u
⟨BR[S,σ1]
1
(σ
[S←σS
2 ]
2
),σ
[S←σS
2 ]
2
⟩
2
(S) ≥u
⟨BR[S,σ1]
1
(σ∗
2),σ∗
2⟩
2
(S) + 1 −α"
REFERENCES,0.957345971563981,"α
(η −2∆) −ϵη (25)"
REFERENCES,0.9597156398104265,"Since η is deﬁned as minimum over all subgames, we have u"
REFERENCES,0.9620853080568721,"D
BR[S,σ1]
1
(σ′
2),σ′
2
E"
REFERENCES,0.9644549763033176,"2
(S) ≥u"
REFERENCES,0.966824644549763,"D
BR[S,σ1]
1
(σ∗
2),σ∗
2
E"
REFERENCES,0.9691943127962085,"2
(S) + 1 −α"
REFERENCES,0.9715639810426541,"α
(η −2∆) −ϵη
(26)"
REFERENCES,0.9739336492890995,"B
POKER RULES"
REFERENCES,0.976303317535545,RULES OF LEDUC POKER
REFERENCES,0.9786729857819905,"Leduc Poker is a two players game. In Leduc Poker, there are 6 cards in total, three ranks({J, Q, K})
with two suits({a, b}) each. And at the beginning, every player should put 1 chip into the pot and
then will be dealt with one private card. Then, two players alternatively bet. They can call, raise and
fold. If any of them fold, the game ends and all chips in the pot belongs to the other player. And"
REFERENCES,0.981042654028436,Under review as a conference paper at ICLR 2022
REFERENCES,0.9834123222748815,"when a player call, he has to put chips in the pot to ensure that he contributes equal chips as the other
player in the pot. When a player raise, he has to ensure that he contributes more chips than the other
player in the pot. A betting round ends when a player calls."
REFERENCES,0.985781990521327,"Leduc Poker is divided into two betting rounds. In the ﬁrst round, a private card is dealt to each
player and then two player start to bet. After the ﬁrst betting round ends and nobody folds, a public
card is dealt on board and the second betting round starts. When the second round ends, both of the
player show their private hands and the stronger hands win. If a player’s private card has the same
rank as the public card, then he wins. Otherwise, we have J < Q < K and the higher one wins.
And in each betting round, there will be at most two raises in our experiment and each raise should
contribute 1 more chip in the ﬁrst round and 2 more chips in the second round."
REFERENCES,0.9881516587677726,RULES OF FLOP HOLD’EM POKER
REFERENCES,0.990521327014218,"The rules of Flop Hold’em Poker is similar to that of Leduc Poker. In FHP, we use the standard
52-card deck. At the beginning, the ﬁrst player will contribute 1 chip to the pot and the second player
will contribute 2 chips. And then they will be dealt with 2 private cards each and the ﬁrst player start
to bet. There are still two betting rounds and the raise sizes are both 2 chips. At the end of the ﬁrst
betting round, there will be 3 public cards dealt on board. And the players will show their private
card at the end of the second betting round and the larger one wins the game. In FHP, we have the
same rule of card order as a standard Texas hold ’em ."
REFERENCES,0.9928909952606635,"C
IMPLEMENTATION DETAILS"
REFERENCES,0.995260663507109,"Leduc Poker. In Leduc Poker, we solve for a blueprint strategy using a variant of CFR algorithm
(Lanctot et al., 2009; Tammelin et al., 2015) with 1M iterations in the full game. Then we apply
search in subgames when the board card is dealt."
REFERENCES,0.9976303317535545,"Flop Hold’em Poker (FHP). As for FHP, there are 1,286,792 different infosets for each betting
sequence. We cluster them into 200 infosets by an abstraction algorithm (Johanson et al., 2013) in
order to make equilibrium ﬁnding feasible. Then, we compute a blueprint strategy in this abstraction
with 10,000,000 iterations. We apply search immediately once the ﬂop cards are dealt."
