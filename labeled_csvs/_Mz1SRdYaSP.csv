Section,Section Appearance Order,Paragraph
ABSTRACT,0.024390243902439025,"Abstract—Even if more and more deep learning models in the
field of target speech extraction (TSE) [1] achieved better perfor-
mance on certain datasets by continuously refining modules and
experimenting with new algorithms, they still remain constrained
by generic frameworks and have not been able to propose new
task decomposition mechanisms or utilize new information. In
this paper, we propose a novel model architecture that focuses
on extracting the target speaker and suppressing interfering
noise simultaneously, acknowledging the intrinsic similarity in
the nature of these tasks. The model is divided into two branches,
one for extracting the target speaker’s speech and the other
for computing the speech of the interferer, thus controlling
the shallow latent features to learn the essence of TSE task.
Additionally, we adopt a mechanism similar to self-enrollment,
where the latent features of the two branches are cross-fused at
each stage of the extraction process, in order to further leverage
the results obtained by both branches.
Index Terms—target speech extraction, dual-path mechanism,
information disentanglement, multi-task learning"
INTRODUCTION,0.04878048780487805,I. INTRODUCTION
INTRODUCTION,0.07317073170731707,"The human ear possesses selective auditory perception,
where individuals selectively focus on sounds of interest while
ignoring other ambient noises in many real-life scenarios.
For instance, in a concert, individuals can concentrate on
listening to the singer’s voice while occasionally engaging
in conversations with friends nearby. The objective of Target
Speech Extraction (TSE) is to mimic this selective auditory
perception of the human ear by separating the speech of
the target person of interest from the mixed audio. It is an
important research topic under the cocktail party [2] problem
in speech processing. This task has various applications such
as robust speech interfaces, auditory assistance systems, and
so forth.
To specify the task of target speech extraction, the system
has two input, one is the mixed audio wave, which contains
the voice of target speaker. Another is a clue, which in our
work is a segment of clean voice of the target speaker and
named as the enrollment speech, to inform the model of the
target speaker’s identity. Then the model does the extraction
process, and the output is the clean voice of the target speaker,
whose content is consistent with that in the mixed audio.
The earliest methods for target speech extraction can be
traced back to beamformers based on Singular Value Decom-
position (SVD) [3]. Some methods also reference techniques
from Blind Source Separation (BSS) tasks [4] [5] [7] [8].
However, these methods often rely on the access to a large"
INTRODUCTION,0.0975609756097561,"Fig. 1. An overall structure of Dual-Path Band-Split RNN (DP-BSRNN), the model contains four general steps and two branches."
INTRODUCTION,0.12195121951219512,"SpEx [13] and X-TF-GridNet [18] treat different scales as
channels and apply 2D convolution layers to mine cross-scale
information. VEVEN [14] model changes the fusion layer
into the delivery of RNN state to reduce the parameter count
of the model. What’s more, each work typically proposes a
new backbone network to accommodate their improvements
on the aforementioned modules, aiming to achieve the best
performance in experiments.
Even though these works did make significant improvement
by new module devises, they still share a common drawback,
that they seldom break the aforementioned framework and
seek for other promising mechanisms. For one simple example,
those models never make use of the interfering speaker’s voice,
though extracting the target speaker’s voice can be viewed
as ridding the interfering voice as well. X-TasNet [19] first
mentioned the feasibility of simultaneously outputting speech
signals of both the target speaker and interfering speakers,
which is also validated through experiments conducted by us
in the recent past. Then a model in the field of speech enhance-
ment utilized the dual-path processing and disentangled feature
learning to successfully achieve state-of-the-art performance.
Inspired by this, we propose our Dual-Path Band-Split RNN
(DP-BSRNN), which uses the newly proposed BSRNN [17] in
Music Source Separation task as the backbone, and extracts
both the target speaker’s voice and the interfering speaker’s
voice simultaneously."
IMPLEMENTATION/METHODS,0.14634146341463414,II. METHODS
IMPLEMENTATION/METHODS,0.17073170731707318,"The architecture of Dual-Path Band-Split RNN (DP-
BSRNN) is shown in Fig.1. Note that BSRNN [17] has
achieve an outstanding and stable performance in our former
experiments of TSE task, we choose it as the backbone model.
The DP-BSRNN has a clue encoder, a mixture encoder, a"
IMPLEMENTATION/METHODS,0.1951219512195122,"band split module at the beginning. Then the model bifurcate
into two branches, one is to extract the target speech, another
extracting the interfering voice. Each branch still follows the
common framework of a extraction module, a mask generator
and finally a voice decoder. We will introduce the details in
this section."
IMPLEMENTATION/METHODS,0.21951219512195122,A. Clue Encoder
IMPLEMENTATION/METHODS,0.24390243902439024,"We choose the simple ResNet34 followed by a mean-
pooling layer as clue encoder here, since clue encoder is not
the point in our work. The enrollment speech is extracted into
one single feature vector, which is treated as the vocal print
of the target speaker ideally."
IMPLEMENTATION/METHODS,0.2682926829268293,B. Band Split Module
IMPLEMENTATION/METHODS,0.2926829268292683,"The band split module follows the same design as that in
BSRNN. The model first takes the pure mixed audio signal
as input and get the complex-valued spectrogram X ∈CF×T ,
where F and T are the frequency and temporal dimensions
respectively. The fullband spectrogram is then split into K
subband spectrograms Bi ∈CGi×T, i = 1, ..., K with pre-
defined bandwidth {Gi}K
i=1 satisfying PK
i=1Gi = F. The
real and imaginary parts of each subband spectrogram Bi are
then concatenated and passed to a layer normalization module
[20] and a fully-connected (FC) layer to generated an N-
dimensional real-valued subband feature Zi ∈RN×T Note that
each subband spectrogram has its own normalization module
and FC layer. All K subband features{Zi}K
i=1 are then merged
to generated a transform feature tensor Z ∈RN×K×T."
IMPLEMENTATION/METHODS,0.3170731707317073,C. Fusion Module
IMPLEMENTATION/METHODS,0.34146341463414637,"more comprehensive and powerful framework. So the fusion
module is only a variable that need to be controlled in the
ablation study instead of a significant proposal. Note that the
enrollment speech is processed to one single feature vector,
based on our previous experimental results, vector dot product
generally achieves a better performance than addition and
concatenation. So we choose dot product as the fusion method
here. Note that the mixture embedding has been split into
subbands in this step, we conduct the fusion operation on
each subband instead of treating the mixture as an integral
whole and fuse only once. The cross-fusion operation won’t
change the dimension of the latent features (if we apply
concatenation, a linear layer will be following to rectify the
output dimension)."
IMPLEMENTATION/METHODS,0.36585365853658536,"Fig. 2. The beginning of DP-BSRNN where the mixture embedding is input
in two branches for further process. In the first block, we do not adopt
information cross-fusion operations to ensure the distinctiveness of the two
tasks."
IMPLEMENTATION/METHODS,0.3902439024390244,"Fig. 3.
The structure of intermediate block of the DP-BSRNN, before the
model inputs the latent features into the RNN block each time, the information
from both branches will undergo a cross-fusion operation."
IMPLEMENTATION/METHODS,0.4146341463414634,D. Dual-Path Band and Sequence Modeling Module
IMPLEMENTATION/METHODS,0.43902439024390244,"The general design of dual-path band and sequence mod-
eling module in each branch is the same as that in BSRNN
[17]. Our work’s innovation is to split the model into two
branches of BSRNN processing, one to extraction the target
speech, another to calculate the interfering voice. In this way,
we can reinforce the model’s understanding of the task based"
IMPLEMENTATION/METHODS,0.4634146341463415,"Fig. 4. A given example of the cross-fusion module between the two branches,
using cross-attention mechanism."
IMPLEMENTATION/METHODS,0.4878048780487805,"follow the design above. The only function of the first dual-
path block, as depicted in Fig.3, is to split the outputs from
one identical input. And we don’t introduce cross-fusion layer
in it. Regarding the fusion part, due to the isomorphic nature
of the tasks in the two branches, simple vector-level fusion
operations such as addition, multiplication, and concatenation
lack interpretability. Therefore, inspired by innovative fusion
operations which have achieved state-of-the-art performances
from models like SEF-Net [15], AV-Sepformer [16], and X-
TFGridNet [18], we decided to implement a cross-attention
mechanism as the information interaction between the two
branches, as illustrated in Fig.4.
Based on the devise, the output of each branch in this mod-
ule is the same as that in BSRNN, denoted by Q ∈RN×K×T."
IMPLEMENTATION/METHODS,0.5121951219512195,E. Dual-Path Mask generator
IMPLEMENTATION/METHODS,0.5365853658536586,"There is a mask generator following the sequence and band
modeling module to calculate the mask, which will be apply
on the mixture embedding in the next step. Recently some
works like MC-SpEx [13] starts to research on the information
mining ability of the mask generation step. Inspired by this,
we decide to enable the mask generator in the two branches
of the cross fusion mechanism, just like what the model
did in the previous module, as is depicted in Fig.5. Unlike
the dual-path sequence and band modeling module, the cross
information operation is optimal instead of imperative, just
like that between the RNN blocks. For the masks will be
applied on the embedding of the mixed audio, the output of
this module is denoted as Mi ∈CGi×T, i = 1, ..., K with
predefined bandwidth {Gi}K
i=1 satisfying PK
i=1Gi = F, then
the K subband masks are concatenated to restore the complete
mask M ∈CF×T."
IMPLEMENTATION/METHODS,0.5609756097560976,"Fig. 5.
The structure of the mask generator in DP-BSRNN, the fusion
operation is quite similar to the one in the previous module."
RESULTS/EXPERIMENTS,0.5853658536585366,III. EXPERIMENTS
RESULTS/EXPERIMENTS,0.6097560975609756,A. Datasets and experiment setup
RESULTS/EXPERIMENTS,0.6341463414634146,"In line with methodologies from prior studies, our ex-
periments are conducted using the Libri2Mix dataset. All
training and testing data were manually synthesized by us.
We selected two speakers’ voices to synthesize a mixed audio,
then extracted a segment of another speech from one speaker"
RESULTS/EXPERIMENTS,0.6585365853658537,"as the reference audio. In this way, the two speakers in the
same mixed audio would take turns as the target speaker,
responsible for two iterations of training or testing. There
are a total of 3000 segments of mixed audio in the testing
data, which means a total of 6000 tests can be conducted.
The remaining synthesized audios are used as training data,
totaling approximately 30,000 training samples.
We adopt a two-stage training mode with a pre-training
step and a fine-tuning step. First, we remove the cross-fusion
module between the two branches in the model, making each
branch to handle the extraction task independently without
interference, and pre-train the model. Then in the second
stage, we add the cross-fusion modules to the model, train
the parameters in them, and fine-tune other parameters in the
pure BSRNN process, which was trained in the first stage. In
this way, we can accelerate the training progress and tackle
the problem of chaotic gradient descent generated by direct
training on too complex models.
Specifically, In each training iteration, the model’s two out-
puts are separately compared with the clean target speech and
the interference speech to calculate the Scale-Invariant Signal-
to-Distortion Ratio (SI-SDR) loss function. The losses from
both outputs are weighted and summed before performing
gradient descent. After several attempts, we ultimately set the
weights of the two losses to 0.6 and 0.4 respectively, to achieve
the optimal performance.
In other training configurations, we set the batch size to
8, and the audio sampling rate to 16,000. Both the enroll-
ment speaker embedding and the feature representations of
mixed speech are set to a dimension of 256. The enrollment
speaker embedding mixing method is vector dot-product. The
information cross-fusion method for the two branches in the
model is cross-attention mechanism. The speaker encoder of
the model is pre-trained on a speaker recognition task first and
then fine-tuned jointly with the TSE task. We also compared
the performance of multi-task training, where the feature
vectors of registered speech undergo a speaker recognition
task simultaneously during training, and the calculated cross-
entropy loss is added to the gradient descent process. The
learning rate during training is set to 0.01, accompanied by a
weight decay of 0.0001."
RESULTS/EXPERIMENTS,0.6829268292682927,B. Experiment results
RESULTS/EXPERIMENTS,0.7073170731707317,"TABLE I
RESULTS OF OUR PROPOSED DUAL-PATH BAND-SPLIT RNN COMPARED WITH THE ORIGINAL BSRNN, NOTE THAT MO-DP-BSRNN REPRESENTS THE
MASK-ONLY DUAL-PATH MECHANISM, WHICH ONLY SPLIT THE MODEL INTO TWO BRANCHES IN THE MASK GENERATOR."
RESULTS/EXPERIMENTS,0.7317073170731707,"model
spk-embed fuse
dual-path fuse
weight-sharing
validation(tgt)
validation(inf)
inference
param-quantity(M)"
RESULTS/EXPERIMENTS,0.7560975609756098,"BSRNN
multiply
-
-
12.83
-
12.80
265.3
BSRNN
FiLM
-
-
13.01
-
12.63
265.7"
RESULTS/EXPERIMENTS,0.7804878048780488,"MO-DP-BSRNN
multiply
-
×
12.46
12.35
12.31
448.8
DP-BSRNN
multiply
multiply
×
14.00
13.17
13.55
566.5
DP-BSRNN
multiply
attention
×
12.96
12.05
12.78
589.1"
RESULTS/EXPERIMENTS,0.8048780487804879,"The performance of the model using the information cross-
fusion mechanism is slightly lower than that of the model
without it. We speculate that this is because the cross-fusion
method is too simple (in order to reduce the training cost,
we only adopted the basic single-layer cross-attention model,
which is much simplified compared to previous works using
similar mechanisms [15] [16]), we leave this part as a fu-
ture work. The performance of the model with a dual-path
mechanism only when generating masks (labeled as MO-DP-
BSRNN) is slightly lower than that of the single-path BSRNN.
We consider that this is due to the influence of the multi-
task mechanism. The single-path BSRNN is overfit on the
extraction work."
CONCLUSION/DISCUSSION,0.8292682926829268,IV. CONCLUSION AND FUTURE WORK
CONCLUSION/DISCUSSION,0.8536585365853658,"We decompose the model into two branches to extract both
the speech of the target speaker and the interfering speaker
simultaneously, with reference to the registered speech. During
this process, we cross-fuse the latent features computed by
the two branches of the model. In this way, We view the
subsequent computation as a refined further extraction task,
aiming to enhance the model’s understanding of the task of
extracting the target speaker and making the shallow features
more aligned with the essence of the extraction operation.
Ultimately, our model achieved significant improvements com-
pared to the previous single-branch BSRNN on Libri2Mix
dataset.
During the research process, we also identified some prob-
lems that deserve further investigation. We leave these ques-
tions to future work. They mainly focus on the following
aspects:"
CONCLUSION/DISCUSSION,0.8780487804878049,"• In this work, we expected that the cross-fusion of in-
formation from two branches could enable the two sub-
models to utilize more information, thereby enhancing
the overall performance. However, experimental results
indicate that although the DP-BSRNN with the cross-
fusion mechanism performs slightly better than the reg-
ular BSRNN, it is significantly inferior to the model
without the cross-fusion mechanism. We speculate that
this is due to the too simple cross-fusion module. The
information is inadequately refined before fusion, thus
disrupt the latent features instead. We will continue
to explore different configurations for the cross-fusion
module in subsequent research."
CONCLUSION/DISCUSSION,0.9024390243902439,"• Although we did achieve a noticeable performance im-
provement in the experiments, our model design also
resulted in a doubling of the parameter count. We wish
to balance the cost performance by implementing a
weight-sharing mechanism for the two branches. We
believe that within the deep BSRNN Block, latent fea-
tures have already been refined, making the tasks of the
two branches essentially isomorphic—both are a reverse-
extraction operations (given information of one speaker,
extract another speaker’s voice). This provides feasibility
for implementing a weight-sharing mechanism."
CONCLUSION/DISCUSSION,0.926829268292683,"• In our model, there are two instances of information
fusion. One is between the enrollment speech embedding
and the mixed speech embedding, while the other is the
fusion of latent features from the two branches. However,
the features of the clean enrollment speech, which is
under our supervise, are fused with the mixed speech only
once at the beginning of the BSRNN, whereas the lack
of interpretable intermediate layer outputs from the two
branches undergoes multiple cross-fusions. This raises
further consideration, which is whether it is feasible to
introduce multiple fusions of the enrollment speech while
fusing between the two branches, thereby enhancing the
model’s utilization of the crucial cue."
CONCLUSION/DISCUSSION,0.9512195121951219,"• One significant starting point in our work is to make
the model understand the essence of the target speaker
extraction task. We achieve this by letting the model
simultaneously perform extraction and denoising tasks
on the same input. However, we haven’t investigated
this aspect in the fusion module (the module where
enrollment speech is fused with mixed speech). There is
still much research to be done on how to enable the model
to correctly utilize registered speech, akin to humans
referencing enrollment speech’s voice and then extracting
content with consistent timbre. There is still much room
for exploration in incorporating this operation into the
fusion module."
REFERENCES,0.975609756097561,REFERENCES
