Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0015360983102918587,"Reinforcement learning encounters many challenges when applied directly in the
real world. Sim-to-real transfer is widely used to transfer the knowledge learned
from simulation to the real world. Domain randomization—one of the most pop-
ular algorithms for sim-to-real transfer—has been demonstrated to be effective in
various tasks in robotics and autonomous driving. Despite its empirical successes,
theoretical understanding on why this simple algorithm works is limited. In this
paper, we propose a theoretical framework for sim-to-real transfers, in which the
simulator is modeled as a set of MDPs with tunable parameters (corresponding to
unknown physical parameters such as friction). We provide sharp bounds on the
sim-to-real gap—the difference between the value of policy returned by domain
randomization and the value of an optimal policy for the real world. We prove
that sim-to-real transfer can succeed under mild conditions without any real-world
training samples. Our theory also highlights the importance of using memory (i.e.,
history-dependent policies) in domain randomization. Our proof is based on novel
techniques that reduce the problem of bounding the sim-to-real gap to the prob-
lem of designing efﬁcient learning algorithms for inﬁnite-horizon MDPs, which
we believe are of independent interest."
INTRODUCTION,0.0030721966205837174,"1
INTRODUCTION"
INTRODUCTION,0.004608294930875576,"Reinforcement Learning (RL) is concerned with sequential decision making, in which the agent
interacts with the environment to maximize its cumulative rewards. This framework has achieved
tremendous empirical successes in various ﬁelds such as Atari games, Go and StarCraft (Mnih et al.,
2013; Silver et al., 2017; Vinyals et al., 2019). However, state-of-the-art algorithms often require a
large amount of training samples to achieve such a good performance. While feasible in applications
that have a good simulator such as the examples above, these methods are limited in applications
where interactions with the real environment are costly and risky, such as healthcare and robotics."
INTRODUCTION,0.006144393241167435,"One solution to this challenge is sim-to-real transfer (Floreano et al., 2008; Kober et al., 2013). The
basic idea is to train an RL agent in a simulator that approximates the real world and then trans-
fer the trained agent to the real environment. This paradigm has been widely applied, especially
in robotics (Rusu et al., 2017; Peng et al., 2018; Chebotar et al., 2019) and autonomous driving
(Pouyanfar et al., 2019; Niu et al., 2021). Sim-to-real transfer is appealing as it provides an essen-
tially unlimited amount of data to the agent, and reduces the costs and risks in training."
INTRODUCTION,0.007680491551459293,"However, sim-to-real transfer faces the fundamental challenge that the policy trained in the simulated
environment may have degenerated performance in the real world due to the sim-to-real gap—the"
INTRODUCTION,0.009216589861751152,∗These two authors contributed equally.
INTRODUCTION,0.010752688172043012,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01228878648233487,"mismatch between simulated and real environments. In addition to building higher-ﬁdelity simula-
tors to alleviate this gap, domain randomization is another popular method (Sadeghi & Levine, 2016;
Tobin et al., 2017; Peng et al., 2018; OpenAI et al., 2018). Instead of training the agent in a single
simulated environment, domain randomization randomizes the dynamics of the environment, thus
exposes the agent to a diverse set of environments in the training phase. Policies learned entirely
in the simulated environment with domain randomization can be directly transferred to the physical
world with good performance (Sadeghi & Levine, 2016; Matas et al., 2018; OpenAI et al., 2018)."
INTRODUCTION,0.013824884792626729,"In this paper, we focus on understanding sim-to-real transfer and domain randomization from a
theoretical perspective. The empirical successes raise the question: can we provide guarantees
for the sub-optimality gap of the policy that is trained in a simulator with domain randomization
and directly transferred to the physical world? To do so, we formulate the simulator as a set of
MDPs with tunable latent variables, which corresponds to unknown parameters such as friction
coefﬁcient or wind velocity in the real physical world. We model the training process with domain
randomization as ﬁnding an optimal history-dependent policy for a latent MDP, in which an MDP
is randomly drawn from a set of MDPs in the simulator at the beginning of each episode."
INTRODUCTION,0.015360983102918587,Our contributions can be summarized as follows:
INTRODUCTION,0.016897081413210446,"• We propose a novel formulation of sim-to-real transfer and establish the connection be-
tween domain randomization and the latent MDP model (Kwon et al., 2021). The latent
MDP model illustrates the uniform sampling nature of domain randomization, and helps to
analyze the sim-to-real gap for the policy obtained from domain randomization."
INTRODUCTION,0.018433179723502304,"• We study the optimality of domain randomization in three different settings. Our results
indicate that the sim-to-real gap of the policy trained in the simulation can be o(H) when
the randomized simulator class is ﬁnite or satisﬁes certain smoothness condition, where
H is the horizon of the real-world interaction. We also provide a lower bound showing
that such benign conditions are necessary for efﬁcient learning. Our theory highlights the
importance of using memory (i.e., history-dependent policies) in domain randomization."
INTRODUCTION,0.019969278033794162,"• To analyze the optimality of domain randomization, we propose a novel proof framework
which reduces the problem of bounding the sim-to-real gap of domain randomization to
the problem of designing efﬁcient learning algorithms for inﬁnite-horizon MDPs, which
we believe are of independent interest."
INTRODUCTION,0.021505376344086023,"• As a byproduct of our proof, we provide the ﬁrst provably efﬁcient model-based algorithm
for learning inﬁnite-horizon average-reward MDPs with general function approximation
(Algorithm 4 in Appendix C.3). Our algorithm achieves a regret bound of ˜O(D√deT),
where T is the total timesteps and de is a complexity measure of a certain function class F
that depends on the eluder dimension (Russo & Van Roy, 2013; Osband & Van Roy, 2014)."
RELATED WORK,0.02304147465437788,"2
RELATED WORK"
RELATED WORK,0.02457757296466974,"Sim-to-Real and Domain Randomization
The basic idea of sim-to-real is to ﬁrst train an RL
agent in simulation, and then transfer it to the real environment. This idea has been widely applied
to problems such as robotics (e.g., Ng et al., 2006; Bousmalis et al., 2018; Tan et al., 2018; OpenAI
et al., 2018) and autonomous driving (e.g., Pouyanfar et al., 2019; Niu et al., 2021). To alleviate the
inﬂuence of reality gap, previous works have proposed different methods to help with sim-to-real
transfer, including progressive networks (Rusu et al., 2017), inverse dynamics models (Christiano
et al., 2016) and Bayesian methods (Cutler & How, 2015; Pautrat et al., 2018). Domain random-
ization is an alternative approach to making the learned policy to be more adaptive to different
environments (Sadeghi & Levine, 2016; Tobin et al., 2017; Peng et al., 2018; OpenAI et al., 2018),
thus greatly reducing the number of real-world interactions."
RELATED WORK,0.026113671274961597,"There are also theoretical works related to sim-to-real transfer. Jiang (2018) uses the number of
different state-action pairs as a measure of the gap between the simulator and the real environment.
Under the assumption that the number of different pairs is constant, they prove the hardness of
sim-to-real transfer and propose efﬁcient adaptation algorithms with further conditions. Feng et al.
(2019) prove that an approximate simulator model can effectively reduce the sample complexity
in the real environment by eliminating sub-optimal actions from the policy search space. Zhong"
RELATED WORK,0.027649769585253458,Published as a conference paper at ICLR 2022
RELATED WORK,0.029185867895545316,"et al. (2019) formulate a theoretical sim-to-real framework using the rich observation Markov de-
cision processes (ROMDPs), and show that the transfer can result in a smaller real-world sample
complexity. None of these results study beneﬁts of domain randomization in sim-to-real transfer.
Furthermore, all above works require real-world samples to ﬁne-tune their policy during training,
while our work and the domain randomization algorithm do not."
RELATED WORK,0.030721966205837174,"POMDPs and Latent MDPs
Partially observable Markov decision processes (POMDPs) are a
general framework for sequential decision-making problems when the state is not fully observ-
able (Smallwood & Sondik, 1973; Kaelbling et al., 1998; Vlassis et al., 2012; Jin et al., 2020a;
Xiong et al., 2021). Latent MDPs (Kwon et al., 2021), or LMDPs, are a special type of POMDPs,
in which the real environment is randomly sampled from a set of MDPs at the beginning of each
episode. This model has been widely investigated with different names such as hidden-model MDPs
and multi-model MDPs. There are also results studying the planning problem in LMDPs, when the
true parameters of the model is given (Chades et al., 2012; Buchholz & Scheftelowitsch, 2019;
Steimle et al., 2021) . Kwon et al. (2021) consider the regret minimization problem for LMDPs, and
provide efﬁcient learning algorithms under different conditions. We remark that all works mentioned
above focus on the problems of ﬁnding the optimal policies for POMDPs or latent MDPs, which is
perpendicular to the central problem of this paper—bounding the performance gap of transferring
the optimal policies of latent MDPs from simulation to the real environment."
RELATED WORK,0.03225806451612903,"Inﬁnite-horizon Average-Reward MDPs
Recent theoretical progress has produced many prov-
ably sample-efﬁcient algorithms for RL in inﬁnite-horizon average-reward setting. Nearly matching
upper bounds and lower bounds are known for the tabular setting (Jaksch et al., 2010; Fruit et al.,
2018; Zhang & Ji, 2019; Wei et al., 2020). Beyond the tabular case, Wei et al. (2021) propose
efﬁcient algorithms for inﬁnite-horizon MDPs with linear function approximation. To the best of
our knowledge, our result (Algorithm 4) is the ﬁrst efﬁcient algorithm with near-optimal regret for
inﬁnite-horizon average-reward MDPs with general function approximation."
PRELIMINARIES,0.03379416282642089,"3
PRELIMINARIES"
EPISODIC MDPS,0.03533026113671275,"3.1
EPISODIC MDPS"
EPISODIC MDPS,0.03686635944700461,"We consider episodic RL problems where each MDP is speciﬁed by M = (S, A, P, R, H, s1). S
and A are the state and the action space with cardinality S and A respectively. We assume that S
and A are ﬁnite but can be extremely large. P : S × A →∆(S) is the transition probability matrix
so that P(·|s, a) gives the distribution over states if action a is taken on state s, R : S × A →[0, 1]
is the reward function. H is the number of steps in one episode."
EPISODIC MDPS,0.03840245775729647,"For simplicity, we assume the agent always starts from the same state in each episode, and use s1
to denote the initial state at step h = 1. It is straight-forward to extend our results to the case with
random initialization. At step h ∈[H], the agent observes the current state sh ∈S, takes action
ah ∈A, receives reward R(sh, ah), and transits to state sh+1 with probability P(sh+1|sh, ah). The
episode ends when sH+1 is reached."
EPISODIC MDPS,0.039938556067588324,"We consider the history-dependent policy class Π, where π ∈Π is a collection of mappings
from the history observations to the distributions over actions.
Speciﬁcally, we use trajh =
{(s1, a1, s2, a2, · · · , sh) | si ∈S, ai ∈A, i ∈[h]} to denote the set of all possible trajec-
tories of history till step h.
We deﬁne a policy π ∈Π to be a collection of H policy func-
tions {πh : trajh →∆(A)}h∈[H].
We deﬁne V π
M,h : S →R to be the value function at"
EPISODIC MDPS,0.041474654377880185,"step h under policy π on MDP M, i.e., V π
M,h(s) = EM,π[PH
t=h R(st, at) | sh = s]. Accord-
ingly, we deﬁne Qπ
M,h : S × A →R to be the Q-value function at step h: Qπ
M,h(s, a) ="
EPISODIC MDPS,0.043010752688172046,"EM,π[R(sh, ah) + PH
t=h+1 R(st, at) | sh = s, ah = a]."
EPISODIC MDPS,0.0445468509984639,"We use π∗
M to denote the optimal policy for a single MDP M. It can be shown that there exists π∗
M
such that the policy at step h depends on only the state at step h but not any other prior history. That
is, π∗
M can be expressed as a collection of H policy functions mapping from S to ∆(A). We use
V ∗
M,h and Q∗
M,h to denote the optimal value and Q-functions under the optimal policy π∗
M at step
h."
EPISODIC MDPS,0.04608294930875576,Published as a conference paper at ICLR 2022
PRACTICAL IMPLEMENTATION OF DOMAIN RANDOMIZATION,0.047619047619047616,"3.2
PRACTICAL IMPLEMENTATION OF DOMAIN RANDOMIZATION"
PRACTICAL IMPLEMENTATION OF DOMAIN RANDOMIZATION,0.04915514592933948,"In this subsection, we brieﬂy introduce how domain randomization works in practical applications.
Domain randomization is a popular technique for improving domain transfer (Tobin et al., 2017;
Peng et al., 2018; Matas et al., 2018), which is often used for zero-shot transfer when the target
domain is unknown or cannot be easily used for training. For example, by highly randomizing the
rendering settings for their simulated training set, Sadeghi & Levine (2016) trained vision-based
controllers for a quadrotor using only synthetically rendered scenes. OpenAI et al. (2018) studied
the problem of dexterous in-hand manipulation. The training is performed entirely in a simulated
environment in which they randomize the physical parameters of the system like friction coefﬁcients
and vision properties such as object’s appearance."
PRACTICAL IMPLEMENTATION OF DOMAIN RANDOMIZATION,0.05069124423963134,"To apply domain randomization in the simulation training, the ﬁrst step before domain randomiza-
tion is usually to build a simulator that is close to the real environment. The simulated model is
further improved to match the physical system more closely through calibration. Though the simu-
lation is still a rough approximation of the physical setup after these engineering efforts, these steps
ensure that the randomized simulators generated by domain randomization can cover the real-world
variability. During the training phase, many aspects of the simulated environment are randomized
in each episode in order to help the agent learn a policy that generalizes to reality. The policy
trained with domain randomization can be represented using recurrent neural network with mem-
ory such as LSTM (Yu et al., 2018; OpenAI et al., 2018; Doersch & Zisserman, 2019). Such a
memory-augmented structure allows the policy to potentially identify the properties of the current
environment and adapt its behavior accordingly. With sufﬁcient data sampled using the simulator,
the agent can ﬁnd a near-optimal policy w.r.t. the average value function over a variety of simula-
tion environments. This policy has shown its great adaptivity in many previous results, and can be
directly applied to the physical world without any real-world ﬁne-tuning (Sadeghi & Levine, 2016;
Matas et al., 2018; OpenAI et al., 2018)."
FORMULATION,0.05222734254992319,"4
FORMULATION"
FORMULATION,0.053763440860215055,"In this section, we propose our theoretical formulation of sim-to-real and domain randomization.
The corresponding models will be used to analyze the optimality of domain randomization in the
next section, which can also serve as a starting point for future research on sim-to-real."
SIM-TO-REAL TRANSFER,0.055299539170506916,"4.1
SIM-TO-REAL TRANSFER"
SIM-TO-REAL TRANSFER,0.05683563748079877,"In this paper, we model the simulator as a set of MDPs with tunable latent parameters. We consider
an MDP set U representing the simulator model with joint state space S and joint action space A.
Each MDP M = (S, A, PM, R, H, s1) in U has its own transition dynamics PM, which corre-
sponds to an MDP with certain choice of latent parameters. Our result can be easily extended to the
case where the rewards are also inﬂuenced by the latent parameters. We assume that there exists an
MDP M∗∈U that represents the dynamics of the real environment."
SIM-TO-REAL TRANSFER,0.05837173579109063,"We can now explain our general framework of sim-to-real. For simplicity, we assume that during
the simulation phase (or training phase), we are given the entire set U that represents MDPs under
different tunable latent parameter. Or equivalently, the learning agent is allowed to interact with any
MDP M ∈U in arbitrary fashion, and sample arbitrary amount of trajectories. However, we do not
know which MDP M ∈U represents the real environment. The objective of sim-to-real transfer is
to ﬁnd a policy π purely based on U, which performs well in the real environment. In particular, we
measure the performance in terms of the sim-to-real gap, which is deﬁned as the difference between
the value of learned policy π and the value of an optimal policy for the real world:"
SIM-TO-REAL TRANSFER,0.059907834101382486,"Gap(π) = V ∗
M∗,1(s1) −V π
M∗,1(s1).
(1)"
SIM-TO-REAL TRANSFER,0.06144393241167435,"We remark that in our framework, the policy π is learned exclusively in simulation without the
use of any real world samples. We study this framework because (1) our primary interests—domain
randomization algorithm does not use any real-world samples for training; (2) we would like to focus
on the problem of knowledge transfer from simulation to the real world. The more general learning
paradigm that allows the ﬁne-tuning of policy learned in simulation using real-world samples can"
SIM-TO-REAL TRANSFER,0.0629800307219662,Published as a conference paper at ICLR 2022
SIM-TO-REAL TRANSFER,0.06451612903225806,"be viewed as a combination of sim-to-real transfer and standard on-policy reinforcement learning,
which we left as an interesting topic for future research."
DOMAIN RANDOMIZATION AND LMDPS,0.06605222734254992,"4.2
DOMAIN RANDOMIZATION AND LMDPS"
DOMAIN RANDOMIZATION AND LMDPS,0.06758832565284179,"We ﬁrst introduce Latent Markov decision processes (LMDPs) and then explain domain random-
ization in the viewpoint of LMDPs. A LMDP can be represented as (U, ν), where U is a set of
MDPs with joint state space S and joint action space A, and ν is a distribution over U. Each MDP
M = (S, A, PM, R, H, s1) in U has its own transition dynamics PM that may differs from other
MDPs. At the start of an episode, an MDP M ∈U is randomly chosen according to the distribution
ν. The agent does not know explicitly which MDP is sampled, but she is allowed to interact with
this MDP M for one entire episode."
DOMAIN RANDOMIZATION AND LMDPS,0.06912442396313365,"Domain randomization algorithm ﬁrst speciﬁes a distribution over tunable parameters, which equiv-
alently gives a distribution ν over MDPs in simulator U. This induces a LMDP with distribution
ν. The algorithm then samples trajectories from this LMDP, runs RL algorithms in order to ﬁnd the
near-optimal policy of this LMDP. We consider the ideal scenario that the domain randomization
algorithm eventually ﬁnd the globally optimal policy of this LMDP, which we formulate as domain
randomization oracle as follows:"
DOMAIN RANDOMIZATION AND LMDPS,0.0706605222734255,"Deﬁnition 1. (Domain Randomization Oracle) Let U be the set of MDPs generated by domain
randomization and ν be the uniform distribution over U. The domain randomization oracle returns
an optimal history-dependent policy π∗
DR of the LMDP (U, ν):"
DOMAIN RANDOMIZATION AND LMDPS,0.07219662058371736,"π∗
DR = arg max
π∈Π
EM∼νV π
M,1(s1).
(2)"
DOMAIN RANDOMIZATION AND LMDPS,0.07373271889400922,"Since LMDP is a special case of POMDPs, its optimal policy π∗
DR in general will depend on his-
tory. This is in sharp contrast with the optimal policy of a MDP, which is history-independent. We
emphasize that both the memory-augmented policy and the randomization of the simulated environ-
ment are critical to the optimality guarantee of domain randomization. We also note that we don’t
restrict the learning algorithm used to ﬁnd the policy π∗
DR, which can be either in a model-based or
model-free style. Also, we don’t explicitly deﬁne the behavior of π∗
DR. The only thing we know
about π∗
DR is that it satisﬁes the optimality condition deﬁned in Equation 2. In this paper, we aim to
bound the sim-to-real gap of π∗
DR, i.e., Gap(π∗
DR, U) under different regimes."
MAIN RESULTS,0.07526881720430108,"5
MAIN RESULTS"
MAIN RESULTS,0.07680491551459294,"We are ready to present the sim-to-real gap of π∗
DR in this section. We study the gap in three different
settings under our sim-to-real framework: ﬁnite simulator class (the cardinality |U| is ﬁnite) with the
separation condition (MDPs in U are distinct), ﬁnite simulator class without the separation condition,
and inﬁnite simulator class. During our analysis, we mainly study the long-horizon setting where
H is relatively large compared with other parameters. This is a challenging setting that has been
widely-studied in recent years (Gupta et al., 2019; Mandlekar et al., 2020; Pirk et al., 2020). We
show that the sim-to-real gap of π∗
DR is only O(log3(H)) for the ﬁnite simulator class with the
separation condition, and only ˜O(
√"
MAIN RESULTS,0.07834101382488479,"H) in the last two settings, matching the best possible lower
bound in terms of H."
MAIN RESULTS,0.07987711213517665,"In our analysis, we assume that the MDPs in U are communicating MDPs with a bounded diameter."
MAIN RESULTS,0.08141321044546851,"Assumption 1 (Communicating MDPs (Jaksch et al., 2010)). The diameter of any MDP M ∈U is
bounded by D. That is, consider the stochastic process deﬁned by a stationary policy π : S →A
on an MDP with initial state s. Let T(s′|M, π, s) denote the random variable for the ﬁrst time step
in which state s′ is reached in this process, then maxs̸=s′∈S minπ:S→A E [T (s′ | M, π, s)] ≤D."
MAIN RESULTS,0.08294930875576037,"This is a natural assumption widely used in the literature (Jaksch et al., 2010; Agrawal & Jia, 2017;
Fruit et al., 2020). The communicating MDP model also covers many real-world tasks in robotics.
For example, transferring the position or angle of a mechanical arm only costs constant time. More-
over, the diameter assumption is necessary under our framework."
MAIN RESULTS,0.08448540706605223,"Proposition 1. Without Assumption 1, there exists a hard instance U so that Gap(π∗
DR) = Ω(H)."
MAIN RESULTS,0.08602150537634409,Published as a conference paper at ICLR 2022
MAIN RESULTS,0.08755760368663594,"We prove Proposition 1 in Appendix G.1. Note that the worst possible gap of any policy is H, so
π∗
DR becomes ineffective without Assumption 1."
FINITE SIMULATOR CLASS WITH SEPARATION CONDITION,0.0890937019969278,"5.1
FINITE SIMULATOR CLASS WITH SEPARATION CONDITION"
FINITE SIMULATOR CLASS WITH SEPARATION CONDITION,0.09062980030721966,"As a starting point, we will show the sim-to-real gap when the MDP set U is a ﬁnite set with cardi-
nality M. Intuitively, a desired property of π∗
DR is the ability to identify the environment the agent is
exploring within a few steps. This is because π∗
DR is trained under uniform random environments, so
we hope it can learn to tell the differences between environments. As long as π∗
DR has this property,
the agent is able to identify the environment dynamics quickly, and behave optimally afterwards
(note that the MDP set U is known to the agent)."
FINITE SIMULATOR CLASS WITH SEPARATION CONDITION,0.09216589861751152,"Before presenting the general results, we ﬁrst examine a simpler case where all MDPs in U are
distinct. Concretely, we assume that any two MDPs in U are well-separated on at least one state-
action pair. Note that this assumption is much weaker than the separation condition in Kwon et al.
(2021), which assumes strongly separated condition for each state-action pair.
Assumption 2 (δ-separated MDP set). For any M1, M2 ∈U, there exists a state-action pair
(s, a) ∈S ×A, such that the L1 distance between the probability of next state of the different MDPs
is at least δ, i.e. ∥(PM1 −PM2) (· | s, a)∥1 ≥δ."
FINITE SIMULATOR CLASS WITH SEPARATION CONDITION,0.09370199692780339,"The following theorem shows the sim-to-real gap of π∗
DR in δ-separated MDP sets.
Theorem 1. Under Assumption 1 and Assumption 2, for any M ∈U, the sim-to-real gap of π∗
DR is
at most"
FINITE SIMULATOR CLASS WITH SEPARATION CONDITION,0.09523809523809523,"Gap(π∗
DR) = O
DM 3 log(MH) log2(SMH/δ) δ4"
FINITE SIMULATOR CLASS WITH SEPARATION CONDITION,0.0967741935483871,"
.
(3)"
FINITE SIMULATOR CLASS WITH SEPARATION CONDITION,0.09831029185867896,"The proof of Theorem 1 is deferred to Appendix D. Though the dependence on M and δ may not be
tight, our bound has only poly-logarithmic dependence on the horizon H."
FINITE SIMULATOR CLASS WITH SEPARATION CONDITION,0.09984639016897082,"The main difﬁculty to prove Theorem 1 is that we do not know what π∗
DR does exactly despite
knowing a simple and clean strategy in the real-world interaction with minimum sim-to-real gap.
That is, to ﬁrstly visit the state-action pairs that help the agent identify the environment quickly and
then follow the optimal policy in the real MDP M∗after identifying M∗. Therefore, we use a novel
constructive argument in the proof. We construct a base policy that implements the idea mentioned
above, and show that π∗
DR cannot be much worse than the base policy. The proof overview can be
found in Section 6."
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.10138248847926268,"5.2
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION"
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.10291858678955453,"Now we generalize the setting and study the sim-to-real gap of π∗
DR when U is ﬁnite but not necessary
a δ-separated MDP set. Surprisingly, we show that π∗
DR can achieve ˜O(
√"
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.10445468509984639,"H) sim-to-real gap when
|U| = M.
Theorem 2. Under Assumption 1, when the MDP set induced by domain randomization U is a ﬁnite
set with cardinality M, the sim-to-real gap of π∗
DR is upper bounded by"
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.10599078341013825,"Gap(π∗
DR) = O

D
p"
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.10752688172043011,"M 3H log(MH)

.
(4)"
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.10906298003072197,"Theorem 2 is proved in Appendix E. This theorem implies the importance of randomization and
memory in the domain randomization algorithms (Sadeghi & Levine, 2016; Tobin et al., 2017; Peng
et al., 2018; OpenAI et al., 2018). With both of them, we successfully reduce the worst possible
gap of π∗
DR from the order of H to the order of
√"
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.11059907834101383,"H, so per step loss will be only ˜O(H−1/2).
Without randomization, it is not possible to reduce the worst possible gap (i.e., the sim-to-real gap)
because the policy is even not trained on all environments. Without memory, the policy is not able
to implicitly “identify” the environments, so it cannot achieve sublinear loss in the worst case."
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.11213517665130568,"We also use a constructive argument to prove Theorem 2. However, it is more difﬁcult to construct
the base policy because we do not have any idea to minimize the gap without the well-separated
condition (Assumption 2). Fortunately, we observe that the base policy is also a memory-based"
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.11367127496159754,Published as a conference paper at ICLR 2022
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.1152073732718894,"policy, which basically can be viewed as an algorithm that seeks to minimize the sim-to-real gap in
an unknown underlying MDP in U. Therefore, we connect the sim-to-real gap of the base policy
with the regret bound of the algorithms in inﬁnite-horizon average-reward MDPs (Bartlett & Tewari,
2012; Fruit et al., 2018; Zhang & Ji, 2019). The proof overview is deferred to Section 6."
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.11674347158218126,"To illustrate the hardness of minimizing the worst case gap, we prove the following lower bound for
Gap(π, U) to show that any policy must suffer a gap at least Ω(
√"
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.11827956989247312,"H).
Theorem 3. Under Assumption 1, suppose A ≥10, SA ≥M ≥100, D ≥20 logA M, H ≥DM,
for any history dependent policy π = {πh : trajh →A}H
h=1, there exists a set of M MDPs
U = {Mm}M
m=1 and a choice of M∗∈U such that Gap(π) is at least Ω(
√ DMH)."
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.11981566820276497,"The proof of Theorem 3 follows the idea of the lower bound proof for tabular MDPs (Jaksch et al.,
2010), which we defer to Appendix G.2. This lower bound implies that Ω(
√"
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION,0.12135176651305683,"H) sim-to-real gap is
unavoidable for the policy π∗
DR when directly transferred to the real environment."
INFINITE SIMULATOR CLASS,0.1228878648233487,"5.3
INFINITE SIMULATOR CLASS"
INFINITE SIMULATOR CLASS,0.12442396313364056,"In real-world scenarios, the MDP class is very likely to be extensively large. For instance, many
physical parameters such as surface friction coefﬁcients and robot joint damping coefﬁcients are
sampled uniformly from a continuous interval in the Dexterous Hand Manipulation algorithms (Ope-
nAI et al., 2018). In these cases, the induced MDP set U is large and even inﬁnite. A natural question
is whether we can extend our analysis to the inﬁnite simulator class case, and provide a correspond-
ing sim-to-real gap."
INFINITE SIMULATOR CLASS,0.1259600614439324,"Intuitively, since the domain randomization approach returns the optimal policy in the average man-
ner, the policy π∗
DR can perform bad in the real world M∗if most MDPs in the randomized set differ
much with M∗. In other words, U must be ”smooth” near M∗for domain randomization to return
a nontrivial policy. By ”smoothness”, we mean that there is a positive probability that the uniform
distribution ν returns a MDP that is close to M∗. This is because the probability that ν samples
exactly M∗in a inﬁnite simulator class is 0, so domain randomization cannot work at all if such
smoothness does not hold."
INFINITE SIMULATOR CLASS,0.12749615975422426,"Formally, we assume there is a distance measure d(M1, M2) on U between two MDPs M1 and
M2. Deﬁne the ϵ-neighborhood CM∗,ϵ of M∗as CM∗,ϵ
def= {M ∈U : d(M, M∗) ≤ϵ}. The
smoothness condition is formally stated as follows:
Assumption 3 (Smoothness near M∗). There exists a positive real number ϵ0, and a Lipchitz con-
stant L, such that for the policy π∗
DR, the value function of any two MDPs in CM∗,ϵ0 is L-Lipchitz
w.r.t the distance function d, i.e.
V π∗
DR
M1,1(s1) −V π∗
DR
M2,1(s1)
 ≤L · d(M1, M2), ∀M1, M2 ∈CM∗,ϵ0.
(5)"
INFINITE SIMULATOR CLASS,0.12903225806451613,"For example, we can set d(M1, M2) = I[M1 ̸= M2] in the ﬁnite simulator class. For complicated
simulator class, we need to ensure there exists some d(·, ·) that L is not large."
INFINITE SIMULATOR CLASS,0.130568356374808,"With Assumption 3, it is possible to compute the sim-to-real gap of π∗
DR. In the ﬁnite simulator class,
we have shown that the gap depends on M polynomially, which can be viewed as the complexity of
U. The question is, how do we measure the complexity of U when it is inﬁnitely large?"
INFINITE SIMULATOR CLASS,0.13210445468509985,"Motivated by Ayoub et al. (2020), we consider the function class"
INFINITE SIMULATOR CLASS,0.1336405529953917,"F = {fM(s, a, λ) : S × A × Λ →R such that fM(s, a, λ) = PMλ(s, a) for M ∈U, λ ∈Λ} ,
(6)"
INFINITE SIMULATOR CLASS,0.13517665130568357,"where Λ = {λ∗
M, M ∈U} is the optimal bias functions of M ∈U in the inﬁnite-horizon average-
reward setting (Bartlett & Tewari (2012); Fruit et al. (2018); Zhang & Ji (2019)). We note this
function class is only used for analysis purposes to express our complexity measure; it does not
affect the domain randomization algorithm. We use the the ϵ-log-covering number and the ϵ-eluder
dimension of F to characterize the complexity of the simulator class U. In the setting of linear
combined models (Ayoub et al., 2020), the ϵ-log-covering number and the ϵ-eluder dimension are"
INFINITE SIMULATOR CLASS,0.13671274961597543,Published as a conference paper at ICLR 2022
INFINITE SIMULATOR CLASS,0.1382488479262673,"O (d log(1/ϵ)), where d is the dimension of the linear representation in linear combined models.
For readers not familiar with eluder dimension or inﬁnite-horizon average-reward MDPs, please see
Appendix A for preliminary explanations."
INFINITE SIMULATOR CLASS,0.13978494623655913,"Here comes our bound of sim-to-real gap for the inﬁnite simulator class setting, which is proved in
Appendix F.
Theorem 4. Under Assumption 1 and 3, the sim-to-real gap of the domain randomization policy
π∗
DR is at most for 0 ≤ϵ < ϵ0"
INFINITE SIMULATOR CLASS,0.141321044546851,"Gap(π∗
DR) = O D
p"
INFINITE SIMULATOR CLASS,0.14285714285714285,"deH log (H · N(F, 1/H))"
INFINITE SIMULATOR CLASS,0.1443932411674347,"ν (CM∗,ϵ)
+ Lϵ ! .
(7)"
INFINITE SIMULATOR CLASS,0.14592933947772657,"Here ν(CM∗,ϵ) is the probability of ν sampling a MDP in CM∗,ϵ, de = dimE(F, 1/H) is the 1/H-
eluder dimension F, and N(F, 1/H) is the 1/H-covering number of F w.r.t. L∞norm."
INFINITE SIMULATOR CLASS,0.14746543778801843,"Theorem 4 is a generalization of Theorem 2, since we can reduce Theorem 4 to Theorem 2 by setting
d(M1, M2) = I[M1 ̸= M2] and ϵ = 0, in which case ν(CM∗,ϵ) = 1/M and de ≤M."
INFINITE SIMULATOR CLASS,0.1490015360983103,"The proof overview can be found in Section 6. The main technique is still a reduction to the regret
minimization problem in inﬁnite-horizon average-reward setting. We construct a base policy and
shows that the regret of it is only ˜O(
√"
INFINITE SIMULATOR CLASS,0.15053763440860216,"H). A key point to note is that our construction of the base
policy also solves an open problem of designing efﬁcient algorithms that achieve ˜O(
√"
INFINITE SIMULATOR CLASS,0.15207373271889402,"T) regret in
the inﬁnite-horizon average-reward setting with general function approximation. This base policy is
of independent interests."
INFINITE SIMULATOR CLASS,0.15360983102918588,"To complement our positive results, we also provide a negative result that even if the MDPs in
U have nice low-rank properties (e.g., the linear low-rank property (Jin et al., 2020b; Zhou et al.,
2020)), the policy π∗
DR returned by the domain randomization oracle can still have Ω(H) sim-to-real
gap when the simulator class is large and the smoothness condition (Assumption 3) does not hold.
This explains the necessity of our preconditions. Please refer to Proposition 2 in Appendix G.3 for
details."
PROOF OVERVIEW,0.15514592933947774,"6
PROOF OVERVIEW"
PROOF OVERVIEW,0.15668202764976957,"In this section, we will give a short overview of our novel proof techniques for the results shown in
section 5. The main proof technique is based on reducing the problem of bounding the sim-to-real
gap to the problem of constructing base policies. In the settings without separation conditions, we
further connect the construction of the base policies to the design of efﬁcient learning algorithms for
the inﬁnite-horizon average-reward settings."
REDUCING TO CONSTRUCTING BASE POLICIES,0.15821812596006143,"6.1
REDUCING TO CONSTRUCTING BASE POLICIES"
REDUCING TO CONSTRUCTING BASE POLICIES,0.1597542242703533,"Intuitively, if there exists a base policy ˆπ ∈Π with bounded sim-to-real gap, then the gap of π∗
DR
will not be too large since π∗
DR deﬁned in Eqn 2 is the policy with the maximum average value.
Lemma 1. Suppose there exists a policy ˆπ ∈Π such that the sim-to-real gap of ˆπ for any MDP
M ∈U satisﬁes V ∗
M,1(s1) −V ˆπ
M,1(s1) ≤C, then we have Gap(π∗
DR) ≤MC when U is a ﬁnite
set with |U| = M. Furthermore, when U is an inﬁnite set satisfying the smoothness condition
(assumption 3), we have for any 0 < ϵ < ϵ0, Gap(π∗
DR) ≤C/ν (CM∗,ϵ) + Lϵ."
REDUCING TO CONSTRUCTING BASE POLICIES,0.16129032258064516,"We defer the proof to Appendix B.1. Now with this reduction lemma, the remaining problem is
deﬁned as follows: Suppose the real MDP M∗belongs to the MDP set U. We know the full
information (transition matrix) of any MDP in the MDP set U. How to design a history-dependent
policy ˆπ ∈Π with minimum sim-to-real gap maxM∈U
 
V ∗
M,1(s1) −V ˆπ
M,1(s1)

."
THE CONSTRUCTION OF THE BASE POLICIES,0.16282642089093702,"6.2
THE CONSTRUCTION OF THE BASE POLICIES"
THE CONSTRUCTION OF THE BASE POLICIES,0.16436251920122888,"With separation conditions
With the help of Lemma 1, we can bound the sim-to-real gap in the
setting of ﬁnite simulator class with separation condition by constructing a history-dependent policy"
THE CONSTRUCTION OF THE BASE POLICIES,0.16589861751152074,Published as a conference paper at ICLR 2022
THE CONSTRUCTION OF THE BASE POLICIES,0.1674347158218126,"ˆπ. The formal deﬁnition of the policy ˆπ can be found in Appendix C.1. The idea of the construction
is based on elimination: the policy ˆπ explicitly collects samples on the “informative” state-action
pairs and eliminates the MDP that is less likely to be the real MDP from the candidate set. Once the
agent identiﬁes the real MDP representing the dynamics of the physical environment, it follows the
optimal policy of the real MDP until the end of the interactions."
THE CONSTRUCTION OF THE BASE POLICIES,0.16897081413210446,"Without separation conditions
The main challenge in this setting is that, we can no longer con-
struct a policy ˆπ that “identify” the real MDP using the approaches as in the settings with separation
conditions. In fact, we may not be able to even “identify” the real MDP since there can be MDPs in
U that is very close to real MDP. Here, we use a different approach, which reduces the minimization
of sim-to-real gap of ˆπ to the regret minimization problem in the inﬁnite-horizon average-reward
MDPs."
THE CONSTRUCTION OF THE BASE POLICIES,0.17050691244239632,"The inﬁnite-horizon average-reward setting has been well-studied (e.g., Jaksch et al., 2010;
Agrawal & Jia, 2017; Fruit et al., 2018; Wei et al., 2020).
The main difference compared
with the episodic setting is that the agent interacts with the environment for inﬁnite steps.
The gain of a policy is deﬁned in the average manner.
The value of a policy π is deﬁned
as ρπ(s) = E[limT →∞
PT
t=1 R(st, π (st))/T
| s1 = s].
The optimal gain is deﬁned as
ρ∗(s)
def= maxs∈S maxπ ρπ(s), which is shown to be state-independent in Agrawal & Jia (2017),
so we use ρ∗for short.
The regret in the inﬁnite-horizon setting is deﬁned as Reg(T) ="
THE CONSTRUCTION OF THE BASE POLICIES,0.17204301075268819,"E
h
Tρ∗−PT
t=1 R(st, at)
i
, where the expectation is over the randomness of the trajectories. A
more detailed explanation of inﬁnite-horizon average-reward MDPs can be found in Appendix A.1."
THE CONSTRUCTION OF THE BASE POLICIES,0.17357910906298002,"For an MDP M ∈U, we can view it as a ﬁnite-horizon MDP with horizon H; or we can view it
as an inﬁnite-horizon MDP. This is because Assumption 1 ensures that the agent can travel to any
state from any state sH encountered at the H-th step (this may not be the case in the standard ﬁnite-
horizon MDPs, since people often assume that the states at the H-th level are terminating state). The
following lemma shows the connection between these two views."
THE CONSTRUCTION OF THE BASE POLICIES,0.17511520737327188,"Lemma 2. For a MDP M, let ρ∗
M and V ∗
M,1(s1) to be the optimal expected gain in the inﬁnite-
horizon view and the optimal value function in the episodic view respectively. We have the following
inequality: Hρ∗
M −D ≤V ∗
M,1(s1) ≤Hρ∗
M + D."
THE CONSTRUCTION OF THE BASE POLICIES,0.17665130568356374,"This lemma indicates that, if we can design an algorithm (i.e. the base policy) ˆπ in the inﬁnite-
horizon setting with regret Reg(H), then the sim-to-real gap of this algorithm in episodic setting
satisﬁes Gap(ˆπ) = V ∗
M,1(s1) −V ˆπ
M,1(s1) ≤Reg(H) + D. This lemma connects the sim-to-real
gap of ˆπ in ﬁnite-horizon setting to the regret in the inﬁnite-horizon setting."
THE CONSTRUCTION OF THE BASE POLICIES,0.1781874039938556,"With the help of Lemma 1 and 2, the remaining problem is to design an efﬁcient exploration algo-
rithm for inﬁnite-horizon average-reward MDPs with the knowledge that the real MDP M∗belongs
to a known MDP set U. Therefore, we propose two optimistic-exploration algorithms (Algorithm 3
and Algorithm 4) for the setting of ﬁnite simulator class and inﬁnite simulator class respectively.
The formal deﬁnition of the algorithms are deferred to Appendix C.2 and Appendix C.3. Note that
our Algorithm 4 is the ﬁrst efﬁcient algorithm with ˜O(
√"
THE CONSTRUCTION OF THE BASE POLICIES,0.17972350230414746,"T) regret in the inﬁnite-horizon average-
reward MDPs with general function approximation, which is of independent interest for efﬁcient
online exploration in reinforcement learning."
CONCLUSION,0.18125960061443933,"7
CONCLUSION"
CONCLUSION,0.1827956989247312,"In this paper, we study the optimality of policies learned from domain randomization in sim-to-real
transfer without real-world samples. We propose a novel formulation of sim-to-real transfer and
view domain randomization as an oracle that returns the optimal policy of an LMDP with uniform
initialization distribution. Following this idea, we show that the policy π∗
DR can suffer only o(H)
loss compared with the optimal value function of the real environment when the simulator class is
ﬁnite or satisﬁes certain smoothness condition, thus this policy can perform well in the long-horizon
cases. We hope our formulation and analysis can provide insight to design more efﬁcient algorithms
for sim-to-real transfer in the future."
CONCLUSION,0.18433179723502305,Published as a conference paper at ICLR 2022
ACKNOWLEDGMENTS,0.1858678955453149,"8
ACKNOWLEDGMENTS"
ACKNOWLEDGMENTS,0.18740399385560677,"Liwei Wang was supported by National Key R&D Program of China (2018YFB1402600), Ex-
ploratory Research Project of Zhejiang Lab (No.
2022RC0AN02), BJNSF (L172037), Project
2020BD006 supported by PKUBaidu Fund."
REFERENCES,0.1889400921658986,REFERENCES
REFERENCES,0.19047619047619047,"Shipra Agrawal and Randy Jia. Posterior sampling for reinforcement learning: worst-case regret
bounds. arXiv preprint arXiv:1705.07041, 2017."
REFERENCES,0.19201228878648233,"Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pp.
463–474. PMLR, 2020."
REFERENCES,0.1935483870967742,"Yu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably efﬁcient Q-learning with low
switching cost. arXiv preprint arXiv:1905.12849, 2019."
REFERENCES,0.19508448540706605,"Peter L Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement
learning in weakly communicating MDPs. arXiv preprint arXiv:1205.2661, 2012."
REFERENCES,0.1966205837173579,"Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrish-
nan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, et al. Using simulation and domain
adaptation to improve efﬁciency of deep robotic grasping. In 2018 IEEE international conference
on robotics and automation (ICRA), pp. 4243–4250. IEEE, 2018."
REFERENCES,0.19815668202764977,"Peter Buchholz and Dimitri Scheftelowitsch. Computation of weighted sums of rewards for concur-
rent MDPs. Mathematical Methods of Operations Research, 89(1):1–42, 2019."
REFERENCES,0.19969278033794163,"Iadine Chades, Josie Carwardine, Tara G Martin, Samuel Nicol, R´egis Sabbadin, and Olivier Buf-
fet. MOMDPs: a solution for modelling adaptive management problems. In Twenty-Sixth AAAI
Conference on Artiﬁcial Intelligence, 2012."
REFERENCES,0.2012288786482335,"Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff,
and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world
experience. In 2019 International Conference on Robotics and Automation (ICRA), pp. 8973–
8979. IEEE, 2019."
REFERENCES,0.20276497695852536,"Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter
Abbeel, and Wojciech Zaremba. Transfer from simulation to real world through learning deep
inverse dynamics model. arXiv preprint arXiv:1610.03518, 2016."
REFERENCES,0.20430107526881722,"Mark Cutler and Jonathan P How. Efﬁcient reinforcement learning for robots using informative
simulated priors. In 2015 IEEE International Conference on Robotics and Automation (ICRA),
pp. 2605–2612. IEEE, 2015."
REFERENCES,0.20583717357910905,"Carl Doersch and Andrew Zisserman. Sim2real transfer learning for 3d human pose estimation:
motion to the rescue. Advances in Neural Information Processing Systems, 32:12949–12961,
2019."
REFERENCES,0.2073732718894009,"Fei Feng, Wotao Yin, and Lin F Yang. How does an approximate model help in reinforcement
learning? arXiv preprint arXiv:1912.02986, 2019."
REFERENCES,0.20890937019969277,"Dario Floreano, Phil Husbands, and Stefano Nolﬁ. Evolutionary robotics. Technical report, Springer
Verlag, 2008."
REFERENCES,0.21044546850998463,"Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Efﬁcient bias-span-constrained
exploration-exploitation in reinforcement learning.
In International Conference on Machine
Learning, pp. 1578–1586. PMLR, 2018."
REFERENCES,0.2119815668202765,"Ronan Fruit, Matteo Pirotta, and Alessandro Lazaric. Improved analysis of UCRL2 with empirical
bernstein inequality. arXiv preprint arXiv:2007.05456, 2020."
REFERENCES,0.21351766513056836,Published as a conference paper at ICLR 2022
REFERENCES,0.21505376344086022,"Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy
learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint
arXiv:1910.11956, 2019."
REFERENCES,0.21658986175115208,"Thomas Jaksch, Ronald Ortner, and Peter Auer.
Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(4), 2010."
REFERENCES,0.21812596006144394,"Nan Jiang. PAC reinforcement learning with an imperfect model. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, volume 32, 2018."
REFERENCES,0.2196620583717358,"Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efﬁ-
cient? arXiv preprint arXiv:1807.03765, 2018."
REFERENCES,0.22119815668202766,"Chi Jin, Sham M Kakade, Akshay Krishnamurthy, and Qinghua Liu. Sample-efﬁcient reinforcement
learning of undercomplete POMDPs. arXiv preprint arXiv:2006.12484, 2020a."
REFERENCES,0.2227342549923195,"Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efﬁcient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137–2143.
PMLR, 2020b."
REFERENCES,0.22427035330261136,"Chi Jin, Qinghua Liu, and Sobhan Miryooseﬁ. Bellman Eluder dimension: New rich classes of rl
problems, and sample-efﬁcient algorithms. arXiv preprint arXiv:2102.00815, 2021."
REFERENCES,0.22580645161290322,"Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in
partially observable stochastic domains. Artiﬁcial Intelligence, 101(1–2):99–134, 1998."
REFERENCES,0.22734254992319508,"Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238–1274, 2013."
REFERENCES,0.22887864823348694,"Dingwen Kong, Ruslan Salakhutdinov, Ruosong Wang, and Lin F Yang. Online sub-sampling for
reinforcement learning with general function approximation. arXiv preprint arXiv:2106.07203,
2021."
REFERENCES,0.2304147465437788,"Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor. RL for latent MDPs:
Regret guarantees and a lower bound. arXiv preprint arXiv:2102.04939, 2021."
REFERENCES,0.23195084485407066,"Tor Lattimore and Csaba Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.23348694316436253,"Ajay Mandlekar, Danfei Xu, Roberto Mart´ın-Mart´ın, Silvio Savarese, and Li Fei-Fei.
Learn-
ing to generalize across long-horizon tasks from human demonstrations.
arXiv preprint
arXiv:2003.06085, 2020."
REFERENCES,0.2350230414746544,"Jan Matas, Stephen James, and Andrew J Davison.
Sim-to-real reinforcement learning for de-
formable object manipulation. In Conference on Robot Learning, pp. 734–743. PMLR, 2018."
REFERENCES,0.23655913978494625,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013."
REFERENCES,0.23809523809523808,"Andrew Y Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie Schulte, Ben Tse, Eric Berger, and
Eric Liang. Autonomous inverted helicopter ﬂight via reinforcement learning. In Experimental
robotics IX, pp. 363–372. Springer, 2006."
REFERENCES,0.23963133640552994,"Haoyi Niu, Jianming Hu, Zheyu Cui, and Yi Zhang.
DR2L: Surfacing corner cases to robus-
tify autonomous driving via domain randomization reinforcement learning.
arXiv preprint
arXiv:2107.11762, 2021."
REFERENCES,0.2411674347158218,"OpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafał J´ozefowicz, Bob McGrew,
Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szy-
mon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning dexterous
in-hand manipulation. CoRR, 2018. URL http://arxiv.org/abs/1808.00177."
REFERENCES,0.24270353302611367,"Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the Eluder dimension.
arXiv preprint arXiv:1406.1853, 2014."
REFERENCES,0.24423963133640553,Published as a conference paper at ICLR 2022
REFERENCES,0.2457757296466974,"R´emi Pautrat, Konstantinos Chatzilygeroudis, and Jean-Baptiste Mouret. Bayesian optimization
with automatic prior selection for data-efﬁcient direct policy search. In 2018 IEEE International
Conference on Robotics and Automation (ICRA), pp. 7571–7578. IEEE, 2018."
REFERENCES,0.24731182795698925,"Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of
robotic control with dynamics randomization. In 2018 IEEE international conference on robotics
and automation (ICRA), pp. 3803–3810. IEEE, 2018."
REFERENCES,0.2488479262672811,"S¨oren Pirk, Karol Hausman, Alexander Toshev, and Mohi Khansari. Modeling long-horizon tasks
as sequential interaction landscapes. arXiv preprint arXiv:2006.04843, 2020."
REFERENCES,0.250384024577573,"Samira Pouyanfar, Muneeb Saleem, Nikhil George, and Shu-Ching Chen. ROADS: Randomization
for obstacle avoidance and driving in simulation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops, pp. 0–0, 2019."
REFERENCES,0.2519201228878648,"Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic
exploration. In NIPS, pp. 2256–2264. Citeseer, 2013."
REFERENCES,0.2534562211981567,"Andrei A Rusu, Matej Veˇcer´ık, Thomas Roth¨orl, Nicolas Heess, Razvan Pascanu, and Raia Hadsell.
Sim-to-real robot learning from pixels with progressive nets. In Conference on Robot Learning,
pp. 262–270. PMLR, 2017."
REFERENCES,0.25499231950844853,"Fereshteh Sadeghi and Sergey Levine. Cad2rl: Real single-image ﬂight without a single real image.
arXiv preprint arXiv:1611.04201, 2016."
REFERENCES,0.2565284178187404,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go
without human knowledge. nature, 550(7676):354–359, 2017."
REFERENCES,0.25806451612903225,"Richard D Smallwood and Edward J Sondik. The optimal control of partially observable Markov
processes over a ﬁnite horizon. Operations research, 21(5):1071–1088, 1973."
REFERENCES,0.25960061443932414,"Lauren N Steimle, David L Kaufman, and Brian T Denton. Multi-model Markov decision processes.
IISE Transactions, pp. 1–16, 2021."
REFERENCES,0.261136712749616,"Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and
Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint
arXiv:1804.10332, 2018."
REFERENCES,0.2626728110599078,"Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. In
2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 23–30.
IEEE, 2017."
REFERENCES,0.2642089093701997,"Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010."
REFERENCES,0.26574500768049153,"Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha¨el Mathieu, Andrew Dudzik, Juny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019."
REFERENCES,0.2672811059907834,"Nikos Vlassis, Michael L Littman, and David Barber. On the computational complexity of stochastic
controller optimization in POMDPs. ACM Transactions on Computation Theory (TOCT), 4(4):
1–8, 2012."
REFERENCES,0.26881720430107525,"Ruosong Wang, Ruslan Salakhutdinov, and Lin F Yang.
Reinforcement learning with general
value function approximation: Provably efﬁcient approach via bounded Eluder dimension. arXiv
preprint arXiv:2005.10804, 2020."
REFERENCES,0.27035330261136714,"Chen-Yu Wei, Mehdi Jafarnia Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul Jain. Model-
free reinforcement learning in inﬁnite-horizon average-reward Markov decision processes. In
International Conference on Machine Learning, pp. 10170–10180. PMLR, 2020."
REFERENCES,0.271889400921659,Published as a conference paper at ICLR 2022
REFERENCES,0.27342549923195086,"Chen-Yu Wei, Mehdi Jafarnia Jahromi, Haipeng Luo, and Rahul Jain. Learning inﬁnite-horizon
average-reward MDPs with linear function approximation. In International Conference on Artiﬁ-
cial Intelligence and Statistics, pp. 3007–3015. PMLR, 2021."
REFERENCES,0.2749615975422427,"Yi Xiong, Ningyuan Chen, Xuefeng Gao, and Xiang Zhou. Sublinear regret for learning POMDPs.
arXiv preprint arXiv:2107.03635, 2021."
REFERENCES,0.2764976958525346,"Wenhao Yu, C Karen Liu, and Greg Turk. Policy transfer with strategy optimization. arXiv preprint
arXiv:1810.05751, 2018."
REFERENCES,0.2780337941628264,"Zihan Zhang and Xiangyang Ji. Regret minimization for reinforcement learning by evaluating the
optimal bias function. arXiv preprint arXiv:1906.05110, 2019."
REFERENCES,0.27956989247311825,"Yuren Zhong, Aniket Anand Deshmukh, and Clayton Scott. PAC reinforcement learning without
real-world feedback. arXiv preprint arXiv:1909.10449, 2019."
REFERENCES,0.28110599078341014,"Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learn-
ing for linear mixture Markov decision processes. arXiv preprint arXiv:2012.08507, 2020."
REFERENCES,0.282642089093702,"A
ADDITIONAL PRELIMINARIES"
REFERENCES,0.28417818740399386,"A.1
INFINITE-HORIZON AVERAGE-REWARD MDPS"
REFERENCES,0.2857142857142857,"The
inﬁnite-horizon
average-reward
setting
has
been
well-explored
in
the
recent
few
years (e.g. Jaksch et al. (2010); Agrawal & Jia (2017); Fruit et al. (2018); Wei et al. (2020)). The
main difference compared with the episodic setting is that the agent interacts with the environment
for inﬁnite steps instead of restarting every H steps. The gain of a policy is deﬁned in the average
manner."
REFERENCES,0.2872503840245776,"Deﬁnition 2. (Deﬁnition 4 in Agrawal & Jia (2017)) The gain ρπ(s) of a stationary policy π from
starting state s1 = s is deﬁned as:"
REFERENCES,0.2887864823348694,"ρπ(s) = E """
REFERENCES,0.2903225806451613,"lim
T →∞
1
T T
X"
REFERENCES,0.29185867895545314,"t=1
R(st, π (st)) | s1 = s # (8)"
REFERENCES,0.29339477726574503,"In this setting, a common assumption is that the MDP is communicating (Assumption 1). Under this
assumption, we have the following lemma."
REFERENCES,0.29493087557603687,"Lemma 3. (Agrawal & Jia, 2017, Lemma 2.1) For a communicating MDP M with diameter D: (a)
The optimal gain ρ∗is state-independent and is achieved by a deterministic stationary policy π∗
DR;
that is, there exists a deterministic policy π∗such that"
REFERENCES,0.2964669738863287,"ρ∗:= max
s′∈S max
π
ρπ (s′) = ρπ∗(s), ∀s ∈S
(9)"
REFERENCES,0.2980030721966206,(b) The optimal gain ρ∗satisﬁes the following equation:
REFERENCES,0.2995391705069124,"ρ∗= min
λ∈RS max
s,a [R(s, a) + Pλ(s, a) −λ(s)] = max
a
[R(s, a) + Pλ∗(s, a) −λ∗(s)] , ∀s
(10)"
REFERENCES,0.3010752688172043,"where Pλ(s, a) = P
s′ P(s′|s, a)λ(s′), and λ∗is the bias vector of the optimal policy π∗
DR satisfying"
REFERENCES,0.30261136712749614,"0 ≤λ∗(s) ≤D.
(11)"
REFERENCES,0.30414746543778803,"The regret minimization problem has been widely studied in this setting, with regret to be deﬁned
as Reg(T) = E
h
Tρ∗−PT
t=1 R(st, at)
i
, where the expectation is over the randomness of the
trajectories. For example, Jaksch et al. (2010) proposed an efﬁcient algorithm called UCRL2, which
achieves regret upper bound ˜O(DS
√"
REFERENCES,0.30568356374807987,"AT). For notation convenience, we use PV (s, a) or Pλ(s, a)
as a shorthand of P"
REFERENCES,0.30721966205837176,"s′∈S P(s′|s, a)V (s′) or P"
REFERENCES,0.3087557603686636,"s′∈S P(s′|s, a)λ(s′)."
REFERENCES,0.3102918586789555,Published as a conference paper at ICLR 2022
REFERENCES,0.3118279569892473,"A.2
ELUDER DIMENSION"
REFERENCES,0.31336405529953915,"Proposed by Russo & Van Roy (2013), eluder dimension has become a widely-used concept to
characterize the complexity of different function classes in bandits and RL (Wang et al., 2020;
Ayoub et al., 2020; Jin et al., 2021; Kong et al., 2021). In this work, we deﬁne eluder dimension to
characterize the complexity of the function F:"
REFERENCES,0.31490015360983103,"F = {fM(s, a, λ) : S × A × Λ →R such that fM(s, a, λ) = PMλ(s, a) for M ∈U, λ ∈Λ} ,
(12)"
REFERENCES,0.31643625192012287,"where Λ = {λ∗
M, M ∈U} is the optimal bias functions of M ∈U in the inﬁnite-horizon average-
reward setting (Bartlett & Tewari (2012); Fruit et al. (2018); Zhang & Ji (2019))."
REFERENCES,0.31797235023041476,"Deﬁnition 3. (Eluder dimension). Let ϵ ≥0 and Z = {(si, ai, λi)}n
i=1 ⊂S ×A×Λ be a sequence
of history samples."
REFERENCES,0.3195084485407066,"• A history sample (s, a, λ) ∈S × A × Λ is ϵ-dependent on Z with respect to F if any
f, f ′ ∈F satisfying ∥f −f ′∥Z ≤ϵ also satisﬁes ∥f(s, a)−f ′(s, a)∥≤ϵ. Here ∥f −f ′∥Z
is a shorthand of
qP"
REFERENCES,0.3210445468509985,"(s,a,λ)∈Z(f −f ′)2(s, a, λ)."
REFERENCES,0.3225806451612903,"• An (s, a, λ) is ϵ-independent of Z with respect to F if (s, a, λ) is not ϵ-dependent on Z."
REFERENCES,0.3241167434715822,"• The ϵ-eluder dimension of a function class F is the length of the longest sequence of el-
ements in S × A × Λ such that, for some ϵ′ ≥ϵ, every element is ϵ′-independent of its
predecessors."
REFERENCES,0.32565284178187404,"B
OMITTED PROOF IN SECTION 6"
REFERENCES,0.3271889400921659,"B.1
PROOF OF LEMMA 1"
REFERENCES,0.32872503840245776,"Proof. We ﬁrstly study the case where U is a ﬁnite set with |U| = M. For ˆπ, we have"
M,0.3302611367127496,"1
M X M∈U"
M,0.3317972350230415," 
V ∗
M,1(s1) −V ˆπ
M,1(s1)

≤C.
(13)"
M,0.3333333333333333,"By the optimality of π∗
DR, we know that"
M,0.3348694316436252,"1
M X"
M,0.33640552995391704,"M∈U
V π∗
DR
M,1 (s1) ≥1 M X"
M,0.3379416282642089,"M∈U
V ˆπ
M,1 (s1) .
(14)"
M,0.33947772657450076,"Therefore,"
M,0.34101382488479265,"1
M X M∈U"
M,0.3425499231950845,"
V ∗
M,1(s1) −V π∗
DR
M,1(s1)

≤C.
(15)"
M,0.34408602150537637,"Since the gap V ∗
M,1(s1)−V π∗
DR
M,1(s1) ≥0 for any i ∈[M], we have
1
M

V ∗
M ∗,1(s1) −V π∗
DR
M ∗,1(s1)

≤
C. That is,

V ∗
M ∗,1(s1) −V π∗
DR
M ∗,1(s1)

≤MC.
(16)"
M,0.3456221198156682,"For the case where U is an inﬁnite set satisfying Assumption 3, by the optimality of π∗
DR, we have"
M,0.34715821812596004,"EM∼ν
h
V π∗
DR
M,1(s1)
i
≥EM∼ν

V ˆπ
M,1(s1)

.
(17)"
M,0.3486943164362519,"Therefore,"
M,0.35023041474654376,"EM∼ν(CM∗,ϵ)"
M,0.35176651305683565,"h
V ∗
M∗,1(s1) −V π∗
DR
M,1(s1)
i
≤EM∼ν
h
V ∗
M∗,1(s1) −V π∗
DR
M,1(s1)
i
≤EM∼ν

V ∗
M∗,1(s1) −V ˆπ
M,1(s1)

. (18)"
M,0.3533026113671275,Published as a conference paper at ICLR 2022
M,0.3548387096774194,"By Assumption 3, for any M ∈C(M∗, ϵ), we have
V π∗
DR
M∗,1(s1) −V π∗
DR
M,1(s1)
 ≤Lϵ.
(19)"
M,0.3563748079877112,"Therefore, we have"
M,0.3579109062980031,"ν (CM∗,ϵ)

V ∗
M∗,1(s1) −V π∗
DR
M∗,1(s1) −Lϵ

≤EM∼ν(CM∗,ϵ)"
M,0.35944700460829493,"h
V ∗
M∗,1(s1) −V π∗
DR
M,1(s1)
i
(20)"
M,0.36098310291858676,"Combining Inq 18 and Inq 20, we have"
M,0.36251920122887865,"ν (CM∗,ϵ)

V ∗
M∗,1(s1) −V π∗
DR
M∗,1(s1) −Lϵ

≤C,
(21)"
M,0.3640552995391705,The lemma can be proved by reordering the above inequality.
M,0.3655913978494624,"B.2
PROOF OF LEMMA 2"
M,0.3671274961597542,"Proof. For MDP M, denote π∗
in as the optimal policy in the inﬁnite-horizon setting and {π∗
ep,h}H
h=1
as the optimal policy in the episodic setting. By the optimality of π∗
ep, we have V ∗
M,1(s1) ="
M,0.3686635944700461,"V
π∗
ep
M,1(s1) ≥V π∗
in
M,1(s1)."
M,0.37019969278033793,"By the Bellman equation in the inﬁnite-horizon setting, we know that"
M,0.3717357910906298,"λ∗
M(s) + ρ∗
M = R(s, π∗
in(s)) + PMλ∗
M(s, π∗
in(s)), ∀s ∈S
(22)"
M,0.37327188940092165,"For notation simplicity, we use dh(s1, π) to denote the state distribution at step h after starting from
state s1 at step 1 following policy π. From the above equation, we have"
M,0.37480798771121354,"λ∗
M(s1) + Hρ∗
M = H
X"
M,0.3763440860215054,"h=1
Esh∼dh(s1,π∗
in)R(sh, , π∗
in(sh)) + EsH+1∼dH+1(s1,π∗
in))λ∗
M(sH+1). (23)"
M,0.3778801843317972,"That is, | H
X"
M,0.3794162826420891,"h=1
Esh∼dh(s1,π∗
in)R(sh, , π∗
in(sh)) −Hρ∗
M| = |λ∗
M(s1) −EsH+1∼dH+1(s1,π∗
in)λ∗
M(sH+1)| ≤D, (24)"
M,0.38095238095238093,"where PH
h=1 Esh∼dh(s1,π∗
in)R(sh, , π∗
in(sh)) = V π∗
in
M,1(s1).
Therefore, we have Hρ∗
M −D ≤"
M,0.3824884792626728,"V π∗
in
M,1(s1) ≤V ∗
M,1(s1)."
M,0.38402457757296465,"For the second inequality, by the Bellman equation in the inﬁnite-horizon setting, we have"
M,0.38556067588325654,"λ∗
M(s1) + Hρ∗
M ≥ H
X"
M,0.3870967741935484,"h=1
Esh∼dh(s1,π∗
ep)R(sh, , π∗
ep,h(sh)) + EsH+1∼dH+1(s1,π∗
ep)λ∗
M(sH+1). (25)"
M,0.38863287250384027,"That is, H
X"
M,0.3901689708141321,"h=1
Esh∼dh(s1,π∗ep)R(sh, , π∗
ep,h(sh)) −Hρ∗
M ≤λ∗
M(s1) −EsH+1∼dH+1(s1,π∗ep)λ∗
M(sH+1) ≤D, (26)"
M,0.391705069124424,"where PH
h=1 Esh∼dh(s1,π∗ep)R(sh, , π∗
ep,h(sh)) = V ∗
M,1(s1)."
M,0.3932411674347158,"C
THE CONSTRUCTION OF LEARNING ALGORITHMS"
M,0.39477726574500765,"C.1
FINITE SIMULATOR CLASS WITH SEPARATION CONDITION"
M,0.39631336405529954,"In this subsection, we explicitly deﬁne the base policy ˆπ with sim-to-real gap guarantee under the
separation condition. Note that a history-dependent policy for LMDPs can also be regarded as an"
M,0.3978494623655914,Published as a conference paper at ICLR 2022
M,0.39938556067588327,Algorithm 1 Optimistic Exploration Under Separation Condition
M,0.4009216589861751,"1: Initialize: the MDP set D = U, n0 = c0 log2(SMH) log(MH)"
M,0.402457757296467,"δ4
for a constant c0
2: ▷Stage 1: Explore and ﬁnd the real MDP M∗"
M,0.4039938556067588,"3: while |D| ≥1 do
4:
Randomly select two MDPs M1 and M2 from the MDP set D
5:
Choose (s0, a0) = arg max(s,a)∈S×A ∥(PM1 −PM2) (· | s, a)∥1
6:
Call Subroutine 2 with parameter (s0, a0) and n0 to collect history samples HM1,M2
7:
if ∃s′ ∈HM1,M2, PM2(s′|s0, a0) = 0 or Q"
M,0.4055299539170507,"s′∈HM1,M2"
M,0.40706605222734255,"PM1(s′|s0,a0)
PM2(s′|s0,a0) ≥1 then"
M,0.40860215053763443,"8:
Eliminate M2 from the MDP set D
9:
else
10:
Eliminate M1 from the MDP set D
11:
end if
12: end while
13: ▷Stage 2: Run the optimal policy of M∗"
M,0.41013824884792627,"14: Denote ˆ
M as the remaining MDP in the MDP set D
15: Run the optimal policy of ˆ
M for the remaining steps"
M,0.4116743471582181,"Algorithm 2 Subroutine: collecting data for (s0, a0)"
M,0.41321044546851,"Input: informative state-action pair (s0, a0), required visitation count n0
Initialize: counter N(s0, a0) = 0, history data H = ∅
Denote πM(s, s′) as the policy with the minimum expected travelling time E [T (s′ | M, π, s)]
for MDP M (Deﬁned in Assumption 1)
while N(s0, a0) ≤n0 do
5:
for i = 1, · · · , M do"
M,0.4147465437788018,"Denote the current state as sinit
Run the policy πMi(sinit, s0) for 2D steps, breaking the loop immediately once the
agent enters state s0"
M,0.4162826420890937,"end for
if the agent enters state s0 then
10:
Execute a0, enter the next state s′.
counter N(s0, a0) = N(s0, a0) + 1, H = H S{s′}
end if
end while
Output: history data H"
M,0.41781874039938555,Published as a conference paper at ICLR 2022
M,0.41935483870967744,"algorithm for ﬁnite-horizon MDPs. By deriving an upper bound of the sim-to-real gap for ˆπ, we can
upper bound Gap(π∗
DR, U) with Lemma 1."
M,0.42089093701996927,"The policy ˆπ is formally deﬁned in Algorithm 1. There are two stages in Algorithm 1. In the ﬁrst
stage, the agent’s goal is to quickly explore the environment and ﬁnd the real MDP M∗from the
MDP set U. This stage contains at most M −1 parts. In each part, the agent randomly selects two
MDPs M1 and M2 from the remaining MDP set D. Since the agent knows the transition dynamics
of M1 and M2, it can ﬁnd the most informative state-action pair (s0, a0) with maximum total-
variation difference between PM1(·|s0, a0) and PM2(·|s0, a0). The algorithm calls Subroutine 2
to collect enough samples from (s0, a0) pairs, and then eliminates the MDP with less likelihood.
At the end of the ﬁrst stage, the MDP set D is ensured to contain only one MDP M∗with high
probability. Therefore, the agent can directly execute the optimal policy for the real MDP till step
H + 1 in the second stage."
M,0.42242703533026116,"Subroutine 2 is designed to collect enough samples from the given state-action pair (s0, a0). The
basic idea in Subroutine 2 is to quickly enter the state s0 and take action a0, until the visitation
counter N(s0, a0) = n0. Denote πM(s, s′) as the policy with the minimum expected travelling
time E [T (s′ | M, π, s)] for MDP M. Suppose the agent is currently at state sinit and runs the
policy πM∗(sinit, s0) in the following steps. By Assumption 1 and Markov’s inequality, the agent
will enter state s0 in 2D steps with probability at least 1/2. Therefore, in Subroutine 2, we runs the
policy πMi(sinit, s0) for 2D steps for i ∈[M] alternatively. This ensures that the agent can enter
state s0 in 2MD steps with probability at least 1/2."
M,0.423963133640553,"Theorem 5 states an upper bound of the sim-to-real gap for Algorithm 1, which is proved in Ap-
pendix D.1."
M,0.4254992319508449,"Theorem 5. Suppose we use ˆπ to denote the history-dependent policy represented by Algorithm 1.
Under Assumption 1 and Assumption 2, for any M ∈U, the sim-to-real gap of Algorithm 1 is at
most"
M,0.4270353302611367,"V ∗
M,1(s1) −V ˆπ
M,1(s1) ≤O
DM 2 log(MH) log2(SMH/δ) δ4"
M,0.42857142857142855,"
.
(27)"
M,0.43010752688172044,"C.2
FINITE SIMULATOR CLASS WITHOUT SEPARATION CONDITION"
M,0.43164362519201227,"In this subsection, we propose an efﬁcient algorithm in the inﬁnite-horizon average-reward setting
for ﬁnite simulator class. Our algorithm is described in Algorithm 3. In episode k, the agent executes
the optimal policy of the optimistic MDP Mk with the maximum expected gain ρ∗
Mk. Once the
agent collects enough data and realizes that the current MDP Mk is not M∗that represents the
dynamics of the real environment, the agent eliminates Mk from the MDP set."
M,0.43317972350230416,Algorithm 3 Optimistic Exploration
M,0.434715821812596,"1: Initialize: the MDP set U1 = U, the episode counter k = 1, h0 = 1
2: Calculate M1 = arg maxM∈U1 ρ∗
M
3: for step h = 1, · · · , H do
4:
Take action ah = π∗
Mk(sh), obtain the reward R(sh, ah), and observe the next state sh+1"
M,0.4362519201228879,"5:
if
Ph
t=h0
 
PMkλ∗
Mk (st, at) −λ∗
Mk (st+1)
 > D
p"
M,0.4377880184331797,2(h −h0) log(2HM) then
M,0.4393241167434716,"6:
Eliminate Mk from the MDP set Uk, denote the remaining set as Uk+1
7:
Calculate Mk+1 = arg maxM∈Uk+1 ρ∗
M
8:
Set h0 = h + 1, and k = k + 1.
9:
end if
10: end for"
M,0.44086021505376344,"To indicate the basic idea of the elimination condition deﬁned in Line 5 of Algorithm 3, we brieﬂy
explain our regret analysis of Algorithm 3. Suppose the MDP Mk selected in episode k satisﬁes the"
M,0.4423963133640553,Published as a conference paper at ICLR 2022
M,0.44393241167434716,"optimistic condition ρ∗
Mk ≥ρ∗
M∗, then the regret in H steps can be bounded as:"
M,0.445468509984639,"Hρ∗
M∗− H
X"
M,0.4470046082949309,"h=1
R(sh, ah)
(28) ≤ K
X k=1"
M,0.4485407066052227,"τ(k+1)−1
X"
M,0.4500768049155146,"h=τ(k)
(ρ∗
Mk −R(sh, ah))
(29) = K
X k=1"
M,0.45161290322580644,"τ(k+1)−1
X"
M,0.45314900153609833,"h=τ(k)
(PMkλ∗
Mk(sh, ah) −λ∗
Mk(sh))
(30) = K
X k=1"
M,0.45468509984639016,"τ(k+1)−1
X"
M,0.45622119815668205,"h=τ(k)
(PMkλ∗
Mk(sh, ah) −λ∗
Mk(sh+1)) + K
X k=1"
M,0.4577572964669739," 
λ∗
Mk(sτ(k+1)) −λ∗
Mk(sτ(k))

(31) ≤ K
X k=1"
M,0.4592933947772657,"τ(k+1)−1
X"
M,0.4608294930875576,"h=τ(k)
(PMkλ∗
Mk(sh, ah) −λ∗
Mk(sh+1)) + KD.
(32)"
M,0.46236559139784944,"Here we use K to denote the total number of episodes, and we use τ(k) to denote the ﬁrst step of
episode k. The ﬁrst inequality is due to the optimistic condition ρ∗
Mk ≥ρ∗
M∗. The ﬁrst equality
is due to the Bellman equation in the ﬁnite-horizon setting (Eqn 10). The last inequality is due to
0 ≤λ∗
M(s) ≤D. From the above inequality, we know that the regret in episode k depends on
the summation Pτ(k+1)−1
h=τ(k)
 
PMkλ∗
Mk(sh, ah) −λ∗
Mk(sh+1)

. If this term is relatively small, we
can continue following the policy π∗
Mk with little loss. Since λ∗
Mk(sh+1) is an empirical sample
of PM∗λ∗
Mk(sh, ah), we can guarantee that Mk is not M∗with high probability if this term is
relatively large."
M,0.46390168970814133,"Based on the above discussion, we can upper bound the regret of Algorithm 3. We defer the proof
of Theorem 6 to Appendix E.1.
Theorem 6. Under Assumption 1, the regret of Algorithm 3 is upper bounded by"
M,0.46543778801843316,"Reg(H) ≤O

D
p"
M,0.46697388632872505,"MH log(MH)

.
(33)"
M,0.4685099846390169,"C.3
INFINITE SIMULATOR CLASS"
M,0.4700460829493088,"In this subsection, we propose a provably efﬁcient model-based algorithm solving inﬁnite-horizon
average-reward MDPs with general function approximation. To the best of our knowledge, our
result is the ﬁrst efﬁcient algorithm with near-optimal regret for inﬁnite-horizon average-reward
MDPs with general function approximation."
M,0.4715821812596006,"Considering the model class U which covers the true MDP M∗, i.e. M∗∈U, we deﬁne the function
space Λ = {λ∗
M, M ∈U}, and space X = S × A × Λ. We deﬁne the function space"
M,0.4731182795698925,"F = {fM(s, a, λ) : X →R such that fM(s, a, λ) = PMλ(s, a) for M ∈U, λ ∈Λ} .
(34)"
M,0.47465437788018433,"Our algorithm, which is described in Algorithm 4, also follows the well-known principle of optimism
in the face of uncertainty. In each episode k, we calculate the optimistic MDP Mk with maximum
expected gain ρ∗
Mk. We execute the optimal policy of M∗to interact with the environment and
collect more samples. Once we have collected enough samples in episode k, we update the model
class Uk and compute the optimistic MDP for episode k + 1."
M,0.47619047619047616,"Compared with the setting of episodic MDP with general function approximation (Ayoub et al.,
2020; Wang et al., 2020; Jin et al., 2021), the additional problem in the inﬁnite-horizon setting is
that the regret technically has linear dependence on the number of total episodes, or the number of
steps that we update the optimistic model and the policy. This corresponds to the last term (KD)
in Inq 32. Therefore, to design efﬁcient algorithm with near-optimal regret in the inﬁnite-horizon
setting, the algorithm should maintain low-switching property (Bai et al., 2019; Kong et al., 2021).
Taking inspiration from the recent work that studies efﬁcient exploration with low switching cost"
M,0.47772657450076805,Published as a conference paper at ICLR 2022
M,0.4792626728110599,"in episodic setting (Kong et al., 2021), we deﬁne the importance score, supf1,f2∈F
∥f1−f2∥2
Znew
∥f1−f2∥2
Z+α ,
as a measure of the importance for new samples collected in current episode, and only update the
optimistic model and the policy when the importance score is greater than 1. Here ∥f1 −f2∥2
Z is a
shorthand of P"
M,0.4807987711213518,"(s,a,s′,λ)∈Z (f1(s, a, λ) −f2(s, a, λ))2."
M,0.4823348694316436,Algorithm 4 General Optimistic Algorithm
M,0.4838709677419355,"1: Initialize: the MDP set U1 = U, episode counter k = 1
2: Initialize: the history data set Z = ∅, Znew = ∅
3: α = 4D2 + 1, β = cD2 log (H · N(F, 1/H)) for a constant c.
4: Compute M1 = arg maxM∈U1 ρ⋆
M
5: for step h = 1, · · · , H do
6:
Take action ah = π∗
Mk(sh) in the current state sh, and transit to state sh+1
7:
Add (sh, ah, sh+1, λ∗
Mk) to the set Znew"
M,0.48540706605222733,"8:
if importance score supf1,f2∈F
∥f1−f2∥2
Znew
∥f1−f2∥2
Z+α ≥1 then"
M,0.4869431643625192,"9:
Add the history data Znew to the set Z
10:
Calculate ˆ
Mk+1 = arg minM∈U
P"
M,0.48847926267281105,"(sh,ah,sh+1,λh)∈Z (PMλh(sh, ah) −λh(sh+1))2"
M,0.49001536098310294,"11:
Update Uk+1 =

M ∈U : P"
M,0.4915514592933948,"(sh,ah,sh+1,λh)∈Z

PM −P ˆ
Mk+1"
M,0.4930875576036866,"
λh(sh, ah)
2
≤β
"
M,0.4946236559139785,"12:
Compute Mk+1 = arg maxM∈Uk+1 ρ⋆
M
13:
Episode counter k = k + 1
14:
end if
15: end for"
M,0.49615975422427033,"We state the regret upper bound of Algorithm 4 in Theorem 5, and defer the proof of the theorem to
Appendix F.1."
M,0.4976958525345622,"Theorem 7. Under Assumption 1, the regret of Algorithm 4 is uppder bounded by"
M,0.49923195084485406,"Reg(H) ≤O

D
p"
M,0.500768049155146,"deH log (H · N(F, 1/H))

,
(35)"
M,0.5023041474654378,"where de is the ϵ-eluder dimension of function class F with ϵ =
1
H , and N(F, 1/H) is the
1
H -
covering number of F w.r.t L∞norm."
M,0.5038402457757296,"For α > 0, we say the covering number N(F, α) of F w.r.t the L∞norm equals m if there is m
functions in F such that any function in F is at most α away from one of these m functions in norm
∥· ∥∞. The ∥· ∥∞norm of function f is deﬁned as ∥f∥∞
def= maxx∈X |f(x)|."
M,0.5053763440860215,"D
OMITTED PROOF FOR FINITE SIMULATOR CLASS WITH SEPARATION
CONDITION"
M,0.5069124423963134,"D.1
PROOF OF THEOREM 5"
M,0.5084485407066052,"The formal deﬁnition of ˆπ is given in Algorithm 1. To upper bound the sim-to-real gap of ˆπ, we
discuss on the following two benign properties of Algorithm 1 in Lemma 4 and Lemma 7. Lemma 4
states that the true MDP M∗will never be eliminated from the MDP set D. Therefore, in stage 2,
the agent will execute the optimal policy in the remaining steps with high probability. Lemma 7
states that the total number of steps in stage 1 is upper bounded by ˜O( M 2"
M,0.5099846390168971,"δ4 ). This is where the ﬁnal
bound in Theorem 5 comes from."
M,0.511520737327189,Lemma 4. With probability at least 1 −1
M,0.5130568356374808,"H , the true MDP M∗will never be eliminated from the
MDP set D in stage 1."
M,0.5145929339477726,"The while-loop in stage 1 will last for M −1 times. To prove Lemma 4, we need to prove that, if
the true MDP M∗is selected in a certain loop, then Q"
M,0.5161290322580645,"(s,a,s′)∈HM1,M2
PM∗(s′|s,a)"
M,0.5176651305683564,"PM(s′|s,a) ≥1 holds with
high probability. This is illustrated in the following lemma."
M,0.5192012288786483,Published as a conference paper at ICLR 2022
M,0.5207373271889401,"Lemma 5. Suppose H = {s′
i}n0
i=1 is a set of n0 = c0 log2(SMH/δ) log(MH)"
M,0.522273425499232,"δ4
independent samples
from a given state-action pair (s0, a0) and MDP M∗for a large constant c0. Let M1 denote
another MDP satisfying ∥(PM∗−PM1)(·|s0, a0)∥1 ≥δ, then the following inequality holds with
probability at least 1 −
1
MH : Y s′∈H"
M,0.5238095238095238,"PM∗(s′|s0, a0)
PM1(s′|s0, a0) > 1,
(36)"
M,0.5253456221198156,"Proof. The proof of Lemma 5 is inspired by the analysis in Kwon et al. (2021). To prove Inq 36, it
is enough to show that ln Y s′∈H"
M,0.5268817204301075,"PM∗(s′|s0, a0)
PM1(s′|s0, a0) ! =
X"
M,0.5284178187403994,"s′∈H
ln
PM∗(s′|s0, a0)"
M,0.5299539170506913,"PM1(s′|s0, a0)"
M,0.5314900153609831,"
> 0.
(37)"
M,0.533026113671275,"holds with probability at least 1 −
1
MH ."
M,0.5345622119815668,"Note that PM∗(s′|s0,a0)"
M,0.5360983102918587,"PM1(s′|s0,a0) can be unbounded since PM1(s′|s0, a0) can be zero for certain s′. To tackle"
M,0.5376344086021505,"this issue, we deﬁne ˜PM1 for a sufﬁciently small α ≤
δ
4S :"
M,0.5391705069124424,"˜PM1(s′|s, a) = α + (1 −αS)PM1(s′|s, a).
(38)"
M,0.5407066052227343,"We have


˜PM1 −PM1

(·|s, a)
 ≤2Sα ≤δ"
M,0.5422427035330261,"2, thus


˜PM1 −PM∗

(·|s, a)
 ≥δ"
M,0.543778801843318,"2. Also, we"
M,0.5453149001536098,"have ln

1
˜
PM1(s′|s0,a0)"
M,0.5468509984639017,"
≤ln(1/α) for any s′ ∈S. With the above deﬁnition, we can decompose
Inq 37 into two terms: X"
M,0.5483870967741935,"s′∈H
ln
PM∗(s′|s0, a0)"
M,0.5499231950844854,"PM1(s′|s0, a0) 
=
X"
M,0.5514592933947773,"s′∈H
ln
PM∗(s′|s0, a0)"
M,0.5529953917050692,"˜PM1(s′|s0, a0) 
+
X"
M,0.554531490015361,"s′∈H
ln"
M,0.5560675883256528,"˜PM1(s′|s0, a0)"
M,0.5576036866359447,"PM1(s′|s0, a0) !"
M,0.5591397849462365,".
(39)"
M,0.5606758832565284,"Taking expectation over s′ ∼PM∗(·|s, a) for the ﬁrst term, we have E"
M,0.5622119815668203,""" n0
X"
M,0.5637480798771122,"i=1
ln"
M,0.565284178187404,"PM∗(s′
i|s0, a0)
˜PM1(s′
i|s0, a0) !# = n0
X i=1 X"
M,0.5668202764976958,"s′
PM∗(s′|s0, a0) ln
PM∗(s′|s0, a0)"
M,0.5683563748079877,"˜PM1(s′|s0, a0)"
M,0.5698924731182796,"
(40)"
M,0.5714285714285714,"=n0DKL

PM∗(s′|s0, a0)| ˜PM1(s′|s0, a0)

(41) ≥n0δ2"
M,0.5729646697388633,"2
,
(42)"
M,0.5745007680491552,where the last inequality is due to Pinsker’s inequality.
M,0.576036866359447,"Lemma 6. (Lemma C.2 in Kwon et al. (2021)) Suppose X is an arbitrary discrete random variable
on a ﬁnite support X. Then, ln(1/P(X)) is a sub-exponential random variable (Vershynin, 2010)
with Orcliz norm ln(1/P(X))φ1 = 1/e."
M,0.5775729646697388,"From the above Lemma, we know that both ˜PM1(s′|s0, a0) and PM∗(s′|s0, a0) are sub-exponential
random variables. By Azuma-Hoefﬁng’s inequality, we have with probability at least 1 −δ0/2, X"
M,0.5791090629800307,"s′∈H
ln

1
˜PM1(s′|s0, a0) 
≥E"
M,0.5806451612903226,""" n0
X"
M,0.5821812596006144,"i=1
ln"
M,0.5837173579109063,"1
˜PM1(s′
i|s0, a0) !#"
M,0.5852534562211982,"−log(1/α)
p"
M,0.5867895545314901,2n0 log(2/δ0). (43)
M,0.5883256528417818,"By Proposition 5.16 in Vershynin (2010), with probability at least 1 −δ0/2, X"
M,0.5898617511520737,"s′∈H
ln (PM∗(s′|s0, a0)) ≥E"
M,0.5913978494623656,""" n0
X"
M,0.5929339477726574,"i=1
ln (PM∗(s′
i|s0, a0)) # −
p"
M,0.5944700460829493,"n0 log(2/δ0)/c,
(44)"
M,0.5960061443932412,Published as a conference paper at ICLR 2022
M,0.5975422427035331,"for a certain constant c > 0. Therefore, we can lower bound the ﬁrst term in Eqn 39, X"
M,0.5990783410138248,"s′∈H
ln
PM∗(s′|s0, a0)"
M,0.6006144393241167,"˜PM1(s′|s0, a0)"
M,0.6021505376344086,"
≥n0δ2"
M,0.6036866359447005,"2
−log(1/α)
p"
M,0.6052227342549923,"2n0 log(2/δ0) −
p"
M,0.6067588325652842,"n0 log(2/δ0)/c,
(45)"
M,0.6082949308755761,with probability at least 1 −δ0.
M,0.6098310291858678,"For the second term in Eqn 39, by the deﬁnition of ˜PM1, X"
M,0.6113671274961597,"s′∈H
ln"
M,0.6129032258064516,"˜PM1(s′|s0, a0)"
M,0.6144393241167435,"PM1(s′|s0, a0) !"
M,0.6159754224270353,"≥−2αSn0
(46)"
M,0.6175115207373272,"Combining Inq 45 and Inq 46, we have X"
M,0.6190476190476191,"s′∈H
ln
PM∗(s′|s0, a0)"
M,0.620583717357911,"PM1(s′|s0, a0)"
M,0.6221198156682027,"
≥n0δ2"
M,0.6236559139784946,"2
−log(1/α)
p"
M,0.6251920122887865,"2n0 log(2/δ0) −
p"
M,0.6267281105990783,n0 log(2/δ0)/c −2αSn0 (47)
M,0.6282642089093702,Setting α = δ2
M,0.6298003072196621,"8S , δ0 =
1
MH , and n0 = c0 log2(SMH/δ) log(MH)"
M,0.631336405529954,"δ4
, we have X"
M,0.6328725038402457,"s′∈H
ln
PM∗(s′|s0, a0)"
M,0.6344086021505376,"PM1(s′|s0, a0)"
M,0.6359447004608295,"
> 0
(48)"
M,0.6374807987711214,"holds with probability at least 1 −
1
MH ."
M,0.6390168970814132,"Lemma 7.
Suppose Stage 1 in Algorithm 1 ends in h0 steps.
We have E[h0]
≤
O( DM 2 log2(SMH/δ) log(MH)"
M,0.6405529953917051,"δ4
), where the expectation is over all randomness in the algorithm and
the environment."
M,0.642089093701997,"Proof. Recall that πM(s, s′) is the policy with the minimum expected travelling time
E [T (s′ | M, π, s)] for MDP M. By Assumption 1, we have"
M,0.6436251920122887,"E [T (s′ | M∗, πM(s, s′), s)] ≤D.
(49)"
M,0.6451612903225806,"Given state s and s′, by Markov’s inequality, we know that with probability at least 1 2,"
M,0.6466973886328725,"T (s′ | M∗, πM∗(s, s′), s) ≤2D.
(50)"
M,0.6482334869431644,"Consider the following stochastic process: In each episode k, the agent starts from a state sk that
is arbitrarily selected, and run the policy πM∗(sk, s0) for 2D steps on the MDP M∗. The process
terminates once the agent enters a certain state s0. By Inq 50, the probability that the process
terminates within k episodes is at least 1 −
1
2k . By the basic algebraic calculation, the expected
stopping episode can be bounded by a constant 4."
M,0.6497695852534562,"Now we return to the proof of Lemma 7. In Subroutine 2, we run policy πMi for each MDP
Mi ∈D alternately. By Lemma 4, the true MDP M∗is always contained in the MDP set D.
Therefore, the expected travelling time to enter state s0 for n0 times is bounded by n0 · M · 8D.
In stage 1, we call Subroutine 2 for M −1 times, which means that the expected steps in stage 1
satisﬁes E[h0] ≤8M 2n0d = O( DM 2 log2(SMH/δ) log(MH) δ4
)."
M,0.6513056835637481,Published as a conference paper at ICLR 2022
M,0.65284178187404,"Proof. (Proof of Theorem 5) Recall that we use h0 to denote the total steps in stage 1. Firstly, we
prove that Gap(ˆπ, U) is upper bounded by O (E[h0] + D)."
M,0.6543778801843319,"V ∗
M∗,1(s1) −V ˆπ
M∗,1(s1)
(51) =Eh0 """
M,0.6559139784946236,"EM∗,π∗
M∗"
M,0.6574500768049155,""" h0
X"
M,0.6589861751152074,"h=1
R(sh, ah) | h0 #"
M,0.6605222734254992,"+ EM∗,π∗
M∗

V ∗
M∗,h0+1(sh0+1) | h0

# (52) −Eh0 """
M,0.6620583717357911,"EM∗,ˆπ"
M,0.663594470046083,""" h0
X"
M,0.6651305683563749,"h=1
R(sh, ah) | h0 #"
M,0.6666666666666666,"+ EM∗,ˆπ

V ˆπ
M∗,h0+1(sh0+1) | h0

# (53) ≤Eh0 """
M,0.6682027649769585,"EM∗,π∗
M∗"
M,0.6697388632872504,""" h0
X"
M,0.6712749615975423,"h=1
R(sh, ah) | h0 ##"
M,0.6728110599078341,"+ Eh0
h
EM∗,π∗
M∗

V ∗
M∗,h0+1(sh0+1) | h0

−EM∗,ˆπ

V ˆπ
M∗,h0+1(sh0+1) | h0
i (54)"
M,0.674347158218126,"≤E[h0] + Eh0
h
EM∗,π∗
M∗

V ∗
M∗,h0+1(sh0+1) | h0

−EM∗,ˆπ

V ˆπ
M∗,h0+1(sh0+1) | h0
i
(55)"
M,0.6758832565284179,"The outer expectation is over all possible h0, while the inner expectation is over the possible trajec-
tories given ﬁxed h0. By Lemma 4, we know that ˆπ = π∗
DR after h0 steps with probability at least
1 −1"
M,0.6774193548387096,"H . If this high-probability event happens, the second part in the above inequality equals to"
M,0.6789554531490015,"E
h
EM∗,π∗
M∗

V ∗
M∗,h0+1(sh0+1) | h0

−EM∗,ˆπ

V ∗
M∗,h0+1(sh0+1) | h0
i
."
M,0.6804915514592934,"We
can
prove
that
this
term
is
upper
bounded
by
2D.
Given
ﬁxed
h0,
EM∗,π∗
M∗
h
V ∗
M∗,h0+1(sh0+1) | h0
i
−EM∗,ˆπ
h
V ∗
M∗,h0+1(sh0+1) | h0
i
is the difference of the"
M,0.6820276497695853,"value function in step h0 + 1 under two different distribution of sh0+1. We use dh(s, π) to denote
the state distribution in step h after starting from state s in step h0 + 1 following policy π."
M,0.6835637480798771,"V ∗
M∗,h0+1(sh0+1) = H
X"
M,0.685099846390169,"h=h0+1
Esh∼dh(sh0+1,π∗
M∗)R(sh, π∗
DR(sh))
(56) =
X"
M,0.6866359447004609,h=h0+1
M,0.6881720430107527,"
ρ∗
M∗+ Esh∼dh(sh0+1,π∗
M∗)λ∗
M∗(sh) −Esh+1∼dh+1(sh0+1,π∗
M∗)λ∗
M∗(sh+1)
 (57)"
M,0.6897081413210445,"=(H −h0)ρ∗
M∗+ λ∗
M∗(sh0+1) −EsH+1∼dH+1(sh0+1,π∗
M∗)λ∗
M∗(sH+1), (58)"
M,0.6912442396313364,"where the second equality is due to the Bellman equation in inﬁnite-horizon setting (Eqn 10). Note
that by the communicating property, we have 0 ≤λ∗
M∗(s) ≤D for any s ∈S. Therefore, we have"
M,0.6927803379416283,"EM∗,π∗
M∗

V ∗
M∗,h0+1(sh0+1) | h0

−EM∗,ˆπ

V ∗
M∗,h0+1(sh0+1) | h0

≤2D.
(59)"
M,0.6943164362519201,"If the high-probability event deﬁned in Lemma 4 does not hold (This happens with probability at
most 1"
M,0.695852534562212,"H ), we still have EM∗,π∗
M∗
h
V ∗
M∗,h0+1(sh0+1) | h0
i
−EM∗,ˆπ
h
V ˆπ
M∗,h0+1(sh0+1) | h0
i
≤H,
thus this does not inﬂuence the ﬁnal bound. Taking expectation over all possible h0, and plugging
the result back to Inq 51, we have"
M,0.6973886328725039,"V ∗
M∗,1(s1) −V ˆπ
M∗,1(s1) ≤O (E[h0] + D) = O
DM 2 log2(SMH/δ) log(MH) δ4"
M,0.6989247311827957,"
.
(60)"
M,0.7004608294930875,"D.2
PROOF OF THEOREM 1"
M,0.7019969278033794,Proof. The theorem can be proved by combining Theorem 5 and Lemma 1.
M,0.7035330261136713,"By Theorem 5, we prove that the constructed policy ˆπ satisﬁes"
M,0.7050691244239631,"V ∗
M∗,1(s1) −V ˆπ
M∗,1(s1) ≤O
DM 2 log2(SMH) log(MH) δ4"
M,0.706605222734255,"
.
(61)"
M,0.7081413210445469,Published as a conference paper at ICLR 2022
M,0.7096774193548387,"By Lemma 1, the sim-to-real gap of policy π∗
DR is bounded by"
M,0.7112135176651305,"Gap(π∗
DR, U) ≤O
DM 3 log2(SMH) log(MH) δ4"
M,0.7127496159754224,"
.
(62)"
M,0.7142857142857143,"E
OMITTED PROOF FOR FINITE SIMULATOR CLASS WITHOUT SEPARATION
CONDITION"
M,0.7158218125960062,"E.1
PROOF OF THEOREM 6"
M,0.717357910906298,"Lemma 8. (Optimism) With probability at least 1 −
1
MH , we have ρ∗
Mk ≥ρ∗
M∗for any k ∈[K]."
M,0.7188940092165899,"Proof. For any ﬁxed M ∈U, and ﬁxed step h ∈[H], by Azuma’s inequality, we have with proba-
bility at least 1 −
1
M 2H2 , h
X"
M,0.7204301075268817,"t=h0
(λ∗
M (st+1) −PM∗λ∗
M (st, at)) ≤D
p"
M,0.7219662058371735,"2(h −h0) log(2HM).
(63)"
M,0.7235023041474654,"Taking union bounds over all possible M and h, we know that the above event holds for all possible
M and h with probability 1 −
1
MH . Under the above event, the true MDP M∗will never be
eliminated from the MDP set Uk. Therefore, we have ρ∗
Mk ≥ρ∗
M∗."
M,0.7250384024577573,"Proof. (Proof of Theorem 6) By Lemma 8, we know that ρ∗
Mk ≥ρ∗
M∗for any k ∈[K]. We use
τ(h) to denote the episode that step h belongs to. We can upper bound the regret in H steps as
follows:"
M,0.7265745007680492,"Hρ∗
M∗− H
X"
M,0.728110599078341,"h=1
R(sh, ah)
(64) ≤ H
X h=1"
M,0.7296466973886329," 
ρ∗
Mτ(h) −R(sh, ah)

(65) = H
X h=1"
M,0.7311827956989247," 
PMτ(h)λ∗
Mτ(h)(sh, ah) −λ∗
Mτ(h)(sh)

(66) = H
X"
M,0.7327188940092166,"h=1
(PMτ(h) −PM∗) λ∗
Mτ(h)(sh, ah) + H
X h=1"
M,0.7342549923195084," 
PM∗λ∗
Mτ(h)(sh, ah) −λ∗
Mτ(h)(sh)

(67) = H
X"
M,0.7357910906298003,"h=1
(PMτ(h) −PM∗) λ∗
Mτ(h)(sh, ah) + PM∗λ∗
Mτ(h)(sh0, ah0) −λ∗
Mτ(h)(s1)
(68) + H−1
X h=1"
M,0.7373271889400922," 
PM∗λ∗
Mτ(h)(sh, ah) −λ∗
Mτ(h)(sh+1)

(69)"
M,0.738863287250384,"By Azuma’s inequality, we know that H−1
X h=1"
M,0.7403993855606759," 
PM∗λ∗
Mτ(h)(sh, ah) −λ∗
Mτ(h)(sh+1)

≤D
p"
M,0.7419354838709677,"H log(2MH),
(70)"
M,0.7434715821812596,"holds with probability at least 1 −
1
MH . Since 0 ≤λ∗
M ≤D, we have PM∗λ∗
Mτ(h)(sh0, ah0) −
λ∗
Mτ(h)(s1) ≤D. Therefore, we have"
M,0.7450076804915514,"Hρ∗
M∗− H
X"
M,0.7465437788018433,"h=1
R(sh, ah) ≤ H
X"
M,0.7480798771121352,"h=1
(PMτ(h) −PM∗) λ∗
Mτ(h)(sh, ah) + D
p"
M,0.7496159754224271,"H log(2HM) + D (71) ≤D
p"
M,0.7511520737327189,"2MH log(2MH) + M + D
p"
M,0.7526881720430108,"H log(2HM) + D.
(72)"
M,0.7542242703533026,Published as a conference paper at ICLR 2022
M,0.7557603686635944,"The ﬁrst term in (71) is bounded by the stopping condition (line 5 of Algorithm 3). By Lemma 2,
we have V ∗
M∗,1(s1) ≤Hρ∗
M∗+ D.
Therefore, we have V ∗
M∗,1(s1) −PH
h=1 R(sh, ah) ≤"
M,0.7572964669738863,"O

D
p"
M,0.7588325652841782,"MH log(MH)

with probability at least 1 −
2
MH . Therefore, we have"
M,0.7603686635944701,"V ∗
M∗,1(s1) −V π∗
DR
M∗,1(s1) ≤O

D
p"
M,0.7619047619047619,"MH log(MH) + H ·
2
MH )

= O

D
p"
M,0.7634408602150538,"MH log(MH)

. (73)"
M,0.7649769585253456,"E.2
PROOF OF THEOREM 2"
M,0.7665130568356375,"Theorem 2 can be proved by combining Theorem 6 , Lemma 1 and Lemma 2. By Theorem 6, for
any M ∈U, the policy ˆπ represented by Algorithm 3 has regret bound Hρ∗
M −PH
h=1 R(sh, ah) ≤"
M,0.7680491551459293,"O

D
p"
M,0.7695852534562212,"MH log(MH)

. Taking expectation over {sh, ah}h∈[H] and combining the inequality
with Lemma 2, we have for any M ∈U."
M,0.7711213517665131,"V ∗
M,1(s1) −V ˆπ
M,1(s1) ≤O

D
p"
M,0.7726574500768049,"MH log(MH)

.
(74)"
M,0.7741935483870968,Then the theorem can be proved by Lemma 1.
M,0.7757296466973886,"F
OMITTED PROOF FOR INFINITE SIMULATOR CLASS"
M,0.7772657450076805,"F.1
PROOF OF THEOREM 7"
M,0.7788018433179723,Lemma 9. (Low Switching) The total number of episode K is bounded by
M,0.7803379416282642,"K ≤O(dimE(F, 1/H) log(D2H) log(H))
(75)"
M,0.7818740399385561,"Proof. By Lemma 5 of Kong et al. (2021), we know that H
X"
M,0.783410138248848,"t=1
min ("
M,0.7849462365591398,"sup
f1,f2∈F"
M,0.7864823348694316,(f1(xt) −f2(xt))2
M,0.7880184331797235,"∥f1 −f2∥2
Zt + 1 , 1 )"
M,0.7895545314900153,"≤CdimE(F, 1/H) log(D2H) log(H)
(76)"
M,0.7910906298003072,for some constant C > 0.
M,0.7926267281105991,Our idea is to use this result to upper bound the number of total switching steps.
M,0.794162826420891,"Let τ(k) be the ﬁrst step of episode k. By the deﬁnition of the function class F, we have (f1 −
f2)2(xt) ≤4D2 for any f1, f2 ∈F and xt. By the switching rule, we know that, once the agent
starts a new episode after step τ(k + 1) −1, we have"
M,0.7956989247311828,"τ(k+1)−1
X"
M,0.7972350230414746,"t=τ(k)
(f1 −f2)2(xt) ≤"
M,0.7987711213517665,"τ(k)−1
X"
M,0.8003072196620584,"t=1
(f1 −f2)2(xt) + α + 4D2, ∀f1, f2, xt
(77)"
M,0.8018433179723502,"Therefore, we have"
M,0.8033794162826421,"τ(k+1)−1
X"
M,0.804915514592934,"t=1
(f1 −f2)2(xt) ≤2"
M,0.8064516129032258,"τ(k)−1
X"
M,0.8079877112135176,"t=1
(f1 −f2)2(xt) + α + 4D2, ∀f1, f2, xt
(78)"
M,0.8095238095238095,Published as a conference paper at ICLR 2022
M,0.8110599078341014,"Now we upper bound the importance score in the switching step τ(k + 1) −1. min 
"
M,0.8125960061443932,"sup
f1,f2"
M,0.8141321044546851,"Pτ(k+1)−1
t=τ(k)
(f1(xt) −f2(xt))2"
M,0.815668202764977,"∥f1 −f2∥2
Zτ(k) + α
, 1 
 ≤min 
 "
M,0.8172043010752689,"τ(k+1)−1
X"
M,0.8187403993855606,"t=τ(k)
sup
f1,f2"
M,0.8202764976958525,(f1(xt) −f2(xt))2
M,0.8218125960061444,"∥f1 −f2∥2
Zτ(k) + α, 1 
  (79) ≤min 
 "
M,0.8233486943164362,"τ(k+1)−1
X"
M,0.8248847926267281,"t=τ(k)
sup
f1,f2"
M,0.82642089093702,2(f1(xt) −f2(xt))2
M,0.8279569892473119,"∥f1 −f2∥2
Zτ(k+1) −4D2 + α, 1 
  (80) ≤min 
 "
M,0.8294930875576036,"τ(k+1)−1
X"
M,0.8310291858678955,"t=τ(k)
sup
f1,f2"
M,0.8325652841781874,2(f1(xt) −f2(xt))2
M,0.8341013824884793,"∥f1 −f2∥2
Zt −4D2 + α, 1 
  (81) ≤"
M,0.8356374807987711,"τ(k+1)−1
X"
M,0.837173579109063,"t=τ(k)
min ("
M,0.8387096774193549,"sup
f1,f2"
M,0.8402457757296466,2(f1(xt) −f2(xt))2
M,0.8417818740399385,"∥f1 −f2∥2
Zt −4D2 + α, 1 ) (82)"
M,0.8433179723502304,"Suppose the number of episodes is at most K. If we set α = 4D2 + 1, we have K
X"
M,0.8448540706605223,"k=1
min 
"
M,0.8463901689708141,"sup
f1,f2"
M,0.847926267281106,"Pτ(k+1)−1
t=τ(k)
(f1(xt) −f2(xt))2"
M,0.8494623655913979,"∥f1 −f2∥2
Zτ(k) + α
, 1 
 ≤ H
X"
M,0.8509984639016898,"t=1
min ("
M,0.8525345622119815,"sup
f1,f2"
M,0.8540706605222734,2(f1(xt) −f2(xt))2
M,0.8556067588325653,"∥f1 −f2∥2
Zt −4D2 + 2α, 1 ) (83)"
M,0.8571428571428571,"≤CdimE(F, 1/H) log(D2H) log(H)
(84)"
M,0.858678955453149,"By the switching rule, we have supf1,f2"
M,0.8602150537634409,"Pτ(k+1)−1
t=τ(k)
(f1(xt)−f2(xt))2"
M,0.8617511520737328,"∥f1−f2∥2
Zτ(k)+α
≥1. Therefore, the LHS of the"
M,0.8632872503840245,above inequality is exactly K. Thus we have
M,0.8648233486943164,"K ≤CdimE(F, 1/H) log(D2H) log(H).
(85)"
M,0.8663594470046083,Lemma 10. (Optimism) With probability at least 1 −1
M,0.8678955453149002,"H , M∗∈Uk holds for any episode k ∈[K]."
M,0.869431643625192,"Proof. This lemma comes directly from Theorem 6 of Ayoub et al. (2020). Deﬁne the Filtration
F = (Fh)h>0 so that Fh−1 is generated by (s1, a1, λ1, · · · , sh, ah, λh). Then we have E[λh(sh+1) |
Fh−1] = PM∗λh(sh, ah) = fM∗(sh, ah, λh). Meanwhile, λh(sh+1) −fM∗(sh, ah, λh) is condi-
tionally D"
M,0.8709677419354839,"2 -subgaussian given Fh−1. By Theorem 6 of Ayoub et al. (2020), we can directly know
that fM∗∈Uk for any k ∈[K] with probability at least 1 −1 H ."
M,0.8725038402457758,Published as a conference paper at ICLR 2022
M,0.8740399385560675,"Proof. (Proof of Theorem 7) Let τ(k) be the ﬁrst step of episode k. Under the high-probability
event deﬁned in Lemma 10, we can decompose the regret using the same trick in previous sections. K
X k=1"
M,0.8755760368663594,"τ(k+1)−1
X"
M,0.8771121351766513,"h=τ(k)
Hρ⋆
M∗−R(sh, ah)
(86) ≤ K
X k=1"
M,0.8786482334869432,"τ(k+1)−1
X"
M,0.880184331797235,"h=τ(k)
(ρ⋆
Mk −R(sh, ah))
(87) = K
X k=1"
M,0.8817204301075269,"τ(k+1)−1
X"
M,0.8832565284178188,"h=τ(k)
(PMkλh(sh, ah) −λh(sh))
(88) = K
X k=1"
M,0.8847926267281107,"τ(k+1)−1
X"
M,0.8863287250384024,"h=τ(k)
(PMk −PM∗)λh(sh, ah) + K
X k=1"
M,0.8878648233486943,"τ(k+1)−1
X"
M,0.8894009216589862,"h=τ(k)
(PM∗λh(sh, ah) −λh(sh))
(89) ≤ K
X k=1"
M,0.890937019969278,"τ(k+1)−1
X"
M,0.8924731182795699,"h=τ(k)
(PMk −PM∗)λh(sh, ah) + K
X k=1"
M,0.8940092165898618,"τ(k+1)−2
X"
M,0.8955453149001537,"h=τ(k)
(PM∗λh(sh, ah) −λh(sh+1)) + DK, (90)"
M,0.8970814132104454,"where the ﬁrst inequality is due to optimism condition in Lemma 10. The ﬁrst equality is due to the
Bellman equation 10 and λh = λ∗
Mk for τ(k) ≤h ≤τ(k + 1) −1. The last inequality is due to
0 ≤λh(s) ≤D for any s ∈S."
M,0.8986175115207373,"Now we bound the ﬁrst two terms in Eqn 90. The second term can be regarded as a martingale
difference sequence. By Azuma’s inequality, with probability at least 1 −1 H , K
X k=1"
M,0.9001536098310292,"τ(k+1)−1−1
X"
M,0.901689708141321,"h=τ(k)
(PM∗λh(sh, ah) −λh(sh+1)) ≤D
p"
M,0.9032258064516129,"2H log(H).
(91)"
M,0.9047619047619048,"Now we focus on the upper bound of the ﬁrst term in Eqn 90. Under the high-probability event
deﬁned in Lemma 10, the true model P is always in the model class Uk. For episode k, from the
construction of Uk we know that any f1, f2 ∈Uk satisﬁes ∥f1 −f2∥2
Zτ(k) ≤2β. Since Mk, M∗∈
Uk, we have"
M,0.9062980030721967,"T (k−1)
X"
M,0.9078341013824884,"t=1
((PMk −PM∗) λt(st, at))2 ≤2β
(92)"
M,0.9093701996927803,"Moreover, by the if condition in Line 8 of Alg. 4, we have for any τ(k) ≤h ≤τ(k + 1) −1, h
X"
M,0.9109062980030722,"t=τ(k)
((PMk −PM∗) λt(st, at))2 ≤2β + α + D2.
(93)"
M,0.9124423963133641,"Summing up the above two equations, we have h
X"
M,0.9139784946236559,"t=1
((PMk −PM∗) λt(st, at))2 ≤4β + α + D2.
(94)"
M,0.9155145929339478,"We invoke Lemma 26 of Jin et al. (2021) by setting G = F −F, Π = {δx(·)|x ∈X} where
δx(·) is the dirac measure centered at x, gt = fMk −fM∗, ω = 1/H, β = 4β + α + D2 and
µt = 1 {· = (st, at, λt)}, then we have"
M,0.9170506912442397,"Published as a conference paper at ICLR 2022 K
X τ=1"
M,0.9185867895545314,"T (τ)
X"
M,0.9201228878648233,"t=S(τ)
|(PMτ −PM∗) λt(st, at)| ≤O
p"
M,0.9216589861751152,"dimE(F, 1/H)βH + min (H, dimE(F, 1/H)) D + H · 1 H  (95) =O
p"
M,0.9231950844854071,"dimE(F, 1/H)βH

(96)"
M,0.9247311827956989,"Plugging the results back to Eqn 90, we have K
X k=1"
M,0.9262672811059908,"τ(k+1)−1
X"
M,0.9278033794162827,"h=τ(k)
Hρ⋆−R(sh, ah) ≤O

D
p"
M,0.9293394777265745,"HdimE(F, 1/H) log (H · N(F, 1/H))

(97)"
M,0.9308755760368663,"By Lemma 2, we have V ∗
M∗,1(s1) ≤Hρ∗
M∗+ D. Therefore, we have"
M,0.9324116743471582,"V ∗
M∗,1(s1) − H
X"
M,0.9339477726574501,"h=1
R(sh, ah) ≤O

D
p"
M,0.9354838709677419,"HdimE(F, 1/H) log (H · N(F, 1/H))

,
(98)"
M,0.9370199692780338,"with probability at least 1 −
2
MH . If the high-probability event doesn’t holds (This happens with"
M,0.9385560675883257,"probability at most
2
H ), then the gap V ∗
M∗,1(s1) −V π∗
DR
M∗,1(s1) still can be bounded by H. Taking
expectation over the trajectory {sh}h, we have"
M,0.9400921658986175,"V ∗
M∗,1(s1) −V π∗
DR
M∗,1(s1) ≤O

D
p"
M,0.9416282642089093,"HdimE(F, 1/H) log (H · N(F, 1/H)) + H · 2"
M,0.9431643625192012,"H )

(99)"
M,0.9447004608294931,"= O

D
p"
M,0.946236559139785,"HdimE(F, 1/H) log (H · N(F, 1/H))

.
(100)"
M,0.9477726574500768,"F.2
PROOF OF THEOREM 4"
M,0.9493087557603687,"Proof. Theorem 4 can be proved by combining Theorem 7 , Lemma 1 and Lemma 2. By Theo-
rem 7, for any M ∈U, the policy ˆπ represented by Algorithm 4 can obtain regret bound Hρ∗
M −
PH
h=1 R(sh, ah) ≤O

D
p"
M,0.9508448540706606,"deH log(H · N(F, 1/H))

. Taking expectation over {sh, ah}h∈[H]
and combining the inequality with Lemma 2, we have for any M ∈U."
M,0.9523809523809523,"V ∗
M,1(s1) −V ˆπ
M,1(s1) ≤O

D
p"
M,0.9539170506912442,"deH log(H · N(F, 1/H))

.
(101)"
M,0.9554531490015361,Then the theorem can be proved by Lemma 1.
M,0.956989247311828,"G
LOWER BOUNDS"
M,0.9585253456221198,"G.1
PROOF OF PROPOSITION 1"
M,0.9600614439324117,"Proof. Consider the following construction of U. There are 3M + 1 states. There are M actions
in the initial state s0, which is denoted as {ai}M
i=1. After taking action ai in state s0, the agent will
transit to state si,1 with probability 1. In state si,1 for i ∈[M], the agent can only take action a0,
and then transits to state si,2 with probability pi, and transits to state si,3 with probability 1 −pi.
State {si,2}M
i=1 and {si,3}M
i=1 are all absorbing states. That is, the agent can only take one action a0
in these states, and transits back to the current state with probability 1. The agent can only obtain
reward 1 in state si,2 for i ∈[M], and all the other states have zero rewards. Therefore, if the agent
knows the transition dynamics of the MDP, it should take action ai with i = arg maxi pi in state s0."
M,0.9615975422427036,"Now we deﬁne the transition dynamics of each MDP Mi. For each MDP Mi ∈U, we have pi = 1
and pj = 0 for all j ∈[M], j ̸= i. Therefore, the agent cannot identify M∗in state s0. The best
policy in state s0 for latent MDP U is to randomly take an action ai. In this case, the sim-to-real gap
can be at least Ω(H) since the agent takes the wrong action in state s0 with probability 1 −
1
M ."
M,0.9631336405529954,Published as a conference paper at ICLR 2022
M,0.9646697388632872,"G.2
PROOF OF THEOREM 3"
M,0.9662058371735791,"Proof. We ﬁrst show that Ω(
√"
M,0.967741935483871,"MH) holds with the hard instance for multi-armed bandit (Lattimore
& Szepesv´ari, 2020). Consider a class of K-armed bandits instances with K = M. For the bandit
instance Mi, the expected reward of arm i is 1"
M,0.9692780337941628,"2 + ϵ, while the expected rewards of other arms are 1"
M,0.9708141321044547,"2.
Note that this is exactly the hard instance for K-armed bandits. Following the proof idea of the lower
bound for multi-armed bandits, we know that the regret (sim-to-real gap) is at least Ω(
√ MH)."
M,0.9723502304147466,"We restate the hard instance construction from Jaksch et al. (2010). This hard instance construction
has also been applied to prove the lower bound in episodic setting (Jin et al., 2018). We ﬁrstly
introduce the two-state MDP construction. In their construction, the reward does not depend on
actions but states. State 1 always has reward 1 and state 0 always has reward 0. From state 1, any
action takes the agent to state 0 with probability δ, and to state 1 with probability 1 −δ. In state 0,
there is one action a∗takes the agent to state 1 with probability δ + ϵ, and the other action a takes
the agent to state 1 with probability δ. A standard Markov chain analysis shows that the stationary
distribution of the optimal policy (that is, the one that takes action a∗in state 0) has a probability of
being in state 1 of"
M,0.9738863287250384,"1
δ
1
δ +
1
δ+ε
= δ + ε"
M,0.9754224270353302,2δ + ε ≥1 2 + ε
M,0.9769585253456221,"6δ for ε ≤δ.
(102)"
M,0.978494623655914,"In contrast, acting sub-optimally (that is taking action a in state 0) leads to a uniform distribution
over the two states. The regret per time step of pulling a sub-optimal action is of order ϵ/δ."
M,0.9800307219662059,"Consider O(S) copies of this two-state MDP where only one of the copies has such a good action
a∗. These copies are connected into a single MDP with an A-ary tree structure. In this construction,
the agent needs to identify the optimal state-action pair over totally SA different choices. Setting"
M,0.9815668202764977,"δ =
1
D and ϵ = c
q"
M,0.9831029185867896,"SA
T D, Jaksch et al. (2010) prove that the regret in the inﬁnite-horizon setting is Ω(
√"
M,0.9846390168970814,DSAT).
M,0.9861751152073732,"Our analysis follows the same idea of Jaksch et al. (2010). For the MDP instance Mi, the opti-
mal state-action pair is (si, ai) ((si, ai) ̸= (sj, aj), ∀i ̸= j). With the knowledge of the transition
dynamics of each Mi, the agent needs to identify the optimal state-action pair over totally M dif-
ferent pairs in our setting. Therefore, we can similarly prove that the lower bound is Ω(
√"
M,0.9877112135176651,"DMH)
following their analysis."
M,0.989247311827957,"G.3
LOWER BOUND IN THE LARGE SIMULATOR CLASS"
M,0.9907834101382489,"Proposition 2. Suppose All MDPs in the MDP set U are linear mixture models (Zhou et al., 2020)
sharing a common low dimensional representation with dimension d = O(log(M)), there exists a
hard instance such that the sim-to-real gap of the policy π∗
DR returned by the domain randomization
oracle can be still Ω(H) when M ≥H."
M,0.9923195084485407,"Proof. We can consider the following linear bandit instance as a special case. Suppose there are two
actions with feature φ(a1) = (1, 0) and φ(a2) = (1, 1). In the MDP set M, there are M −1 MDPs
with parameter θi = ( 1"
M,0.9938556067588326,"2, −pi) with 1"
M,0.9953917050691244,4 < pi < 1
M,0.9969278033794163,"2, i ∈[M −1], and one MDP with parameter θM =
( 1 2, 1"
M,0.9984639016897081,"2). Suppose M = 4H + 5, the optimal policy of such an LMDP with uniform initialization
will never pull the action a2, which can suffer Ω(H) sim-to-real gap in the MDP with parameter
θM."
