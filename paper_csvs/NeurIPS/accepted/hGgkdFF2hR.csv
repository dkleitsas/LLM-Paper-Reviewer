Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000945179584120983,"Optimal transport (OT) is a general framework for finding a minimum-cost transport
plan, or coupling, between probability distributions, and has many applications
in machine learning. A key challenge in applying OT to massive datasets is the
quadratic scaling of the coupling matrix with the size of the dataset. Forrow et al.
(2019) introduced a factored coupling for the k-Wasserstein barycenter problem,
which Scetbon et al. (2021) adapted to solve the primal low-rank OT problem. We
derive an alternative parameterization of the low-rank problem based on the latent
coupling (LC) factorization previously introduced by Lin et al. (2021) generalizing
Forrow et al. (2019). The LC factorization has multiple advantages for low-rank
OT including decoupling the problem into three OT problems and greater flexibility
and interpretability. We leverage these advantages to derive a new algorithm
Factor Relaxation with Latent Coupling (FRLC), which uses coordinate mirror
descent to compute the LC factorization. FRLC handles multiple OT objectives
(Wasserstein, Gromov-Wasserstein, Fused Gromov-Wasserstein), and marginal
constraints (balanced, unbalanced, and semi-relaxed) with linear space complexity.
We provide theoretical results on FRLC, and demonstrate superior performance
on diverse applications – including graph clustering and spatial transcriptomics –
while demonstrating its interpretability."
INTRODUCTION,0.001890359168241966,"1
Introduction"
INTRODUCTION,0.002835538752362949,"Optimal transport (OT) is a powerful geometric framework for comparing probability distributions.
OT problems seek a transport plan P efficiently transforming one distribution (a) into another (b),
subject to a ground cost C. The minimum cost yields a distance between a and b, while the optimal
transport plan reveals key structural similarities between the distributions. Owing to its versatility –
different ground costs result in different ways to compare data – OT has found many applications in
machine learning and beyond: from self-attention Tay et al. (2020); Sander et al. (2022); Geshkovski
et al. (2023) and domain adaptation Courty et al. (2014); Solomon et al. (2015) to computational
biology Schiebinger et al. (2019); Yang et al. (2020); Bunne et al. (2023); Liu et al. (2023)."
INTRODUCTION,0.003780718336483932,"This versatility is compounded by several variants using different forms of the objective function
and/or constraints on the transport plan P . Wasserstein (W) OT Kantorovich (1942) compares
distributions over the same space through the expected work of P , while Gromov-Wasserstein (GW)
OT Mémoli (2011) compares distributions supported on distinct geometries through the expected
metric distortion of P . Fused Gromov-Wasserstein (FGW) Vayer et al. (2020) OT is suited to
structured data, taking a convex combination of the former two objectives. Independently, one can
relax constraints on the marginals of P : in computational applications, P is a matrix whose row-sum
P 1m and column-sum P T1n are called its left and right marginals. Balanced OT requires P 1m = a
and P T1n = b. Unbalanced OT Frogner et al. (2015) replaces these constraints with penalties in the"
INTRODUCTION,0.004725897920604915,"transport cost, and is more robust to outliers. Semi-relaxed OT can be used to understand how one
dataset embeds into another by imposing one hard constraint on either the left or right marginal, used
for feature transfer Dong et al. (2023), and alignment of spatiotemporal data Halmos et al. (2024)."
INTRODUCTION,0.005671077504725898,"An important consideration in applying OT is the quadratic space of the transport plan. To address
both the quadratic complexity and to provide robustness under sampling noise, Forrow et al. (2019) in-
troduced another variant of OT, optimizing a k-Wasserstein Barycenter proxy for the rank-constrained
Wasserstein objective. Their approach factors the transport plan through a small set of anchor
points called hubs. Generalizing this approach, Scetbon et al. (2021) introduce the factorization
P = Q diag (1/g)RT comprised of sub-coupling matrices Q and R sharing an inner marginal
g, meaning QT1n = RT1m = g. Building on this, Scetbon et al. (2021, 2022, 2023) derived
algorithms to compute low-rank optimal transport plans for the primal OT problem with general
costs, extending low-rank OT to GW and unbalanced problems using factored couplings."
INTRODUCTION,0.006616257088846881,"Interestingly, a different factorization of P was proposed by Lin et al. (2021) in the context of
k-Wasserstein barycenters. We call their factorization a latent coupling (LC) factorization, given by
P = Qdiag(1/gQ)T diag(1/gR)RT, with two inner marginals gQ = QT1n and gR = RT1n and
a general coupling T . Lin et al. (2021) constrain the transport between a and b through two sets
of learned anchor points, where the factorization is defined by three transport plans computed from
three cost matrices between the points and their anchors. This objective differs from that of Forrow
et al. (2019); Scetbon et al. (2021), who seek a minimal rank coupling with respect to a single, fixed
cost C. We observe that factored couplings of Forrow et al. (2019) correspond to LC factorizations
with diagonal T , suggesting the LC factorization of Lin et al. (2021) may provide an alternative
parameterization of transport plans for the low-rank OT problem considered in Forrow et al. (2019);
Scetbon et al. (2021). To our knowledge, this idea has not yet been explored."
INTRODUCTION,0.007561436672967864,"Contributions.
We present a new algorithm, Factor Relaxation with Latent Coupling (FRLC,
with the informal mnemonic “frolic”), to compute a minimum cost low-rank transport plan using
the LC factorization. Parameterizing low-rank transport plans with the LC factorization has a
number of advantages. First, optimization of the low-rank OT objective decouples into three OT
sub-problems on the LC factors Q, R, T , leading to a simpler optimization algorithm. Second, this
decoupling provides straightfoward extensions of FRLC to low-rank unbalanced and semi-relaxed
OT; similar extensions for factored couplings required additional work Scetbon et al. (2023) beyond
the balanced case. Third, the latent coupling T in the LC factorization provides additional flexibility
to model transport between datasets with different numbers of clusters, and to model mass-splitting
between these clusters, providing a high-level and interpretable description of P that differs from
the factored couplings of Forrow et al. (2019). FRLC computes the LC factorization using a novel
coordinate mirror descent scheme, alternating descent steps on variables (Q, R) and T , inspired
by the mirror descent approach of Scetbon et al. (2021). We call the descent step on (Q, R) factor
relaxation, as the factors Q and R have relaxed inner marginals, allowing FRLC to be solved by
OT sub-problems. FRLC handles multiple OT objectives (Wasserstein, Gromov-Wasserstein, Fused
Gromov-Wasserstein), and marginal constraints (balanced, unbalanced, and semi-relaxed). We show
FRLC performs better than existing state-of-the-art low-rank methods on a range of synthetic and
real datasets, retaining the interpretability of Lin et al. (2021), and inheriting the broad applicability
of Scetbon et al. (2021); Scetbon & Cuturi (2022); Scetbon et al. (2022, 2023)."
BACKGROUND,0.008506616257088847,"2
Background"
BACKGROUND,0.00945179584120983,"Wasserstein OT.
Let {x1, . . . , xn} and {y1, . . . ym} be datasets in a metric space X, and let ∆d
be the probability simplex of size d. Through probability vectors a ∈∆n and b ∈∆m, each dataset
is encoded as a probability measure: µ = Pn
i=1 aiδxi and ν = Pm
j=1 bjδyj. Let"
BACKGROUND,0.010396975425330813,"Πa,· := {P ∈Rn×m
+
: P 1m = a}, Π ·,b := {P ∈Rn×m
+
: P T1n = b}, Πa,b := Πa,· ∩Π ·,b."
BACKGROUND,0.011342155009451797,"Thus, Πa,b is the set of transport plans (probabilistic coupling matrices) with marginals a and b.
Given a cost function c : X × X →R+, define the cost matrix C ∈Rn×m
+
via Cij = c(xi, yj). The
Kantorovich formulation Kantorovich (1942) of discrete OT, also called the Wasserstein problem,
seeks a transport plan P of minimal cost :"
BACKGROUND,0.012287334593572778,"W(µ, ν) :=
min
P ∈Πa,b⟨C, P ⟩F .
(1)"
BACKGROUND,0.013232514177693762,"Gromov-Wasserstein OT.
In many applications, one wishes to compare datasets {x1, . . . , xn} ⊂
X and {y1, . . . , ym} ⊂Y across distinct metric spaces X and Y. The Gromov-Wasserstein (GW)
objective Mémoli (2007, 2011) addresses the absence of a common metric or coordinate system
through intra-domain cost functions c1 : X × X →R+ and c2 : Y × Y →R+, leading to
intra-domain cost matrices Aik = c1(xi, xk) and Bjl = c2(yj, yl). The GW objective function
QA,B(P ) := P"
BACKGROUND,0.014177693761814745,"i,j,k,l(Aik−Bjl)2PijPkl quantifies the expected metric distortion under P , leading
to the optimization problem:"
BACKGROUND,0.015122873345935728,"GW(µ, ν) :=
min
P ∈Πa,b QA,B(P ).
(2)"
BACKGROUND,0.01606805293005671,"The Fused Gromov-Wasserstein (FGW) objective function Vayer et al. (2020) is a convex combination
of the W and GW objectives, given as α⟨C, P ⟩F +(1−α)QA,B(P ), for hyperparameter α ∈(0, 1)."
BACKGROUND,0.017013232514177693,"Relaxed marginal constraints.
Balanced OT (1) constrains P to lie in Πa,b. Unbalanced OT
relaxes constraints P 1m = a and P T1n = b, replacing them with penalties in the form of KL
divergences (or other divergences, see Chizat et al. (2018)):"
BACKGROUND,0.017958412098298678,"U-W(µ, ν) :=
min
P ∈Rn×m
+
⟨C, P ⟩F + τLKL(P 1m∥a) + τRKL(P T1n∥b),
(3)"
BACKGROUND,0.01890359168241966,"where τL, τR > 0 control the strength of each penalty. Semi-relaxed optimal transport relaxes exactly
one of the hard constraints P 1m = a and P T1n = b in the same manner. The semi-relaxed version
of (1) obtained by relaxing only the “right” marginal constraint on b is:"
BACKGROUND,0.01984877126654064,"SRR-W(µ, ν) :=
min
P ∈Πa,·⟨C, P ⟩+ τKL(P T1n∥b),
(4)"
BACKGROUND,0.020793950850661626,"while it’s “left” marginal counterpart SRL-W(µ, ν) is defined analogously over P ∈Π ·,b, using
penalty τKL(P 1m∥a). Likewise, one can form semi-relaxed or unbalanced GW and FGW problems."
BACKGROUND,0.021739130434782608,"Entropy regularization.
The seminal work Cuturi (2013b) introduced the Sinkhorn algorithm to
solve an entropy regularized version of (1), Wϵ(µ, ν) := minP ∈Πa,b⟨C, P ⟩F −ϵH(P ), massively
improving the O(n3 log n) time complexity of classical techniques Orlin (1997); Tarjan (1997).
Above, H is the entropy, H(P ) = −P"
BACKGROUND,0.022684310018903593,"ij Pij(log Pij −1), and ϵ > 0 is the regularization strength."
BACKGROUND,0.023629489603024575,"Low-rank regularization. The nonnegative rank rk+(M) of matrix M is the least number of
nonnegative rank-one matrices summing to M. For r ≥1, define"
BACKGROUND,0.024574669187145556,"Πa,·(r) = {P ∈Πa,· : rk+(P ) ≤r},
Π ·,b(r) = {P ∈Π ·,b : rk+(P ) ≤r},
(5)"
BACKGROUND,0.02551984877126654,"and let Πa,b(r) = Πa,·(r) ∩Π ·,b(r). To estimate Wasserstein distances with greater stability and
accuracy under sampling noise, Forrow et al. (2019) proposed a low-rank regularization on the
coupling matrix, factoring the transport through a small set of anchor points. More explicitly, Scetbon
et al. (2021) parameterized the set as Πa,b(r) through the set FCa,b(r) of factored couplings,"
BACKGROUND,0.026465028355387523,"FCa,b(r) := {(Q, R, g) ∈Rn×r
+
× Rm×r
+
× (R∗
+)r : Q ∈Πa,g, R ∈Πb,g}."
BACKGROUND,0.027410207939508508,"The set FCa,b(r) parameterizes Πa,b(r) through (Q, R, g) 7→Qdiag(1/g)RT, as shown by Cohen
& Rothblum (1993)."
BACKGROUND,0.02835538752362949,"Scetbon et al. (2021) apply this factorization to solve the Wasserstein problem subject to P ∈Πa,b(r)
for general cost matrices:"
BACKGROUND,0.02930056710775047,"Wr(µ, ν) :=
min
P ∈Πa,b(r)⟨C, P ⟩F
(6)"
BACKGROUND,0.030245746691871456,"GW, unbalanced and semi-relaxed low-rank OT problems are defined as in (2), (3) and (4), replacing
Rn×m
+
, Πa,·, or Π ·,b with rank-constrained counterparts (5). Scetbon & Cuturi (2022); Scetbon et al.
(2022, 2023) developed a robust framework for solving all of these problems."
BACKGROUND,0.031190926275992438,"3
Factor Relaxation with Latent Coupling (FRLC) algorithm"
LATENT COUPLING FACTORIZATION,0.03213610586011342,"3.1
Latent Coupling Factorization"
LATENT COUPLING FACTORIZATION,0.0330812854442344,"We parameterize low-rank coupling matrices P ∈Πa,b(r) using a factorization introduced in Lin
et al. (2021), which we call the latent coupling (LC) factorization (Fig. 1). The key property of ! "" !! !"" # $ % # $ &"
LATENT COUPLING FACTORIZATION,0.034026465028355386,"Figure 1: (Left) The LC factorization P = Qdiag(1/gQ)T diag(1/gR)RT of coupling matrix P
with outer marginals a, b, inner marginals gQ, gR, factors Q, R, and latent coupling T . (Right)
Full-rank coupling matrix P ."
LATENT COUPLING FACTORIZATION,0.03497164461247637,"this factorization is the presence of a coupling matrix T linking two distinct inner marginals. For
simplicity we describe this factorization using an r-dimensional latent space, but we also extend to
non-square matrices linking two latent spaces of different dimensions, as demonstrated in the results.
Definition 3.1 (Inner marginals). Given a factorization P = QXRT of a coupling matrix P ∈
Πa,b(r), the inner marginals of Q and R are gQ := QT1n and gR := RT1m, respectively, where
gQ, gR ∈∆r."
LATENT COUPLING FACTORIZATION,0.035916824196597356,"To distinguish the different marginals, we refer to a and b as outer marginals.
Definition 3.2 (LC factorization). Given a coupling matrix P ∈Πa,b(r), a latent coupling (LC)
factorization of P is P = Qdiag(1/gQ)T diag(1/gR)RT, where gQ and gR are the inner marginals
of Q and R, Q ∈Πa,·, R ∈Πb,·, and T ∈ΠgQ,gR."
LATENT COUPLING FACTORIZATION,0.036862003780718335,"We call the factors Q, R, T in an LC factorization sub-couplings. Let R+ := Rn×r
+
×Rm×r
+
×Rr×r
+
.
Given probability vectors a ∈∆n, b ∈∆m and a positive integer rank r, let"
LATENT COUPLING FACTORIZATION,0.03780718336483932,"LCa,b(r) := {(Q, R, T ) ∈R+ : Q ∈Πa,·, R ∈Πb,·, T ∈ΠgQ,gR},"
LATENT COUPLING FACTORIZATION,0.038752362948960305,"be the set of admissible sub-couplings for the LC factorization. Definition 3.2 gives the following
map from LCa,b(r) to Πa,b(r):"
LATENT COUPLING FACTORIZATION,0.03969754253308128,"(Q, R, T ) 7→Qdiag(1/gQ)T diag(1/gR)RT =: P(Q,R,T ).
(7)"
LATENT COUPLING FACTORIZATION,0.04064272211720227,"Since this map is surjective, the set LCa,b(r) parameterizes Πa,b(r). Surjectivity follows from
the fact that FCa,b(r) maps injectively into LCa,b(r), through (Q, R, g) 7→(Q, R, diag(g)), and
FCa,b(r) maps surjectively onto Πa,b(r) via (Q, R, g) 7→Qdiag(1/g)RT. Definition 3.1 and
Definition 3.2 are readily extended in two directions: the case when the outer marginal constraints
are relaxed such that Q ∈Rn×m
+
or R ∈Rn×m
+
, while maintaining the constraint that T ∈ΠgQ,gR;
as well as the case of non-square T ."
THE BALANCED FRLC ALGORITHM,0.04158790170132325,"3.2
The Balanced FRLC Algorithm"
THE BALANCED FRLC ALGORITHM,0.04253308128544423,"We introduce an algorithm Factor Relaxation with Latent Coupling (FRLC), to compute a LC
factorization of minimum cost. We first describe the FRLC algorithm for the balanced Wasserstein
problem. Extensions to other and marginal constraints are discussed later. The FRLC objective
function, for low-rank, balanced Wasserstein OT, is"
THE BALANCED FRLC ALGORITHM,0.043478260869565216,"LLC(Q, R, T ) := ⟨C, P(Q,R,T )⟩F ,
(8)"
THE BALANCED FRLC ALGORITHM,0.0444234404536862,"where P(Q,R,T ) is defined by (7). Since LCa,b(r) parameterizes Πa,b(r), problem (8) is equivalent
to low rank problem (6). The FRLC algorithm is built from projections onto convex sets, described
by constraints on the outer marginals alone for (Q, R) and by the inner marginals alone for T . Given
(Q, R, T ) ∈LCa,b(r), sub-couplings Q and R are constrained by:"
THE BALANCED FRLC ALGORITHM,0.045368620037807186,"C1(a) := {(Q, R, T ) ∈R+ : Q1r = a},
C1(b) := {(Q, R, T ) ∈R+ : R1r = b}."
THE BALANCED FRLC ALGORITHM,0.046313799621928164,The convex sets constraining the latent coupling matrix T are
THE BALANCED FRLC ALGORITHM,0.04725897920604915,"C2(gQ) := {(Q, R, T ) ∈R+ : T 1r = gQ},
C2(gR) := {(Q, R, T ) ∈R+ : T T1r = gR},"
THE BALANCED FRLC ALGORITHM,0.048204158790170135,"where gQ = QT1n and gR = RT1m as per Definition 3.1. Writing C1 = C1(a) ∩C1(b) and
C2 = C2(gQ) ∩C2(gR), one has LCa,b(r) = C1 ∩C2."
THE BALANCED FRLC ALGORITHM,0.04914933837429111,"We use coordinate mirror descent to optimize (8), building on the mirror descent (MD) approach of
Scetbon et al. (2021); Scetbon & Cuturi (2022); Scetbon et al. (2022, 2023) for the low-rank problem.
First we take a descent step in the variables (Q, R) for a fixed T , using KL penalties on their inner
marginals. These “soft” constraints allow the joint optimization in (Q, R) to decouple into two
semi-relaxed OT problems, one for each variable. We call this step factor relaxation as this allows
(Q, R) to have relaxed inner marginals gQ and gR. Next we take a descent step in the latent coupling
variable T , fixing the Q and R, equivalent to solving a balanced OT problem. Thus, solving both
coordinate descent steps corresponds to solving three OT problems."
THE BALANCED FRLC ALGORITHM,0.0500945179584121,"We now provide further details on these coordinate descent steps, with the full algorithm given in
Algorithm 1. Let (γk)N
k=1 be a sequence of step sizes. As in Scetbon & Cuturi (2022), we choose
ℓ∞-normalization for the step-sizes. Our coordinate mirror descent in the factor relaxation step is:"
THE BALANCED FRLC ALGORITHM,0.05103969754253308,"(Qk+1, Rk+1) ←
arg min
(Q,R) : (Q,R,Tk)∈C1
⟨(Q, R), ∇(Q,R)LLC⟩+ 1"
THE BALANCED FRLC ALGORITHM,0.05198487712665406,"γk
KL((Q, R)∥(Qk, Rk))"
THE BALANCED FRLC ALGORITHM,0.052930056710775046,"+ τKL((QT1n, RT1m)∥(QT
k 1n, RT
k 1m))"
THE BALANCED FRLC ALGORITHM,0.05387523629489603,The Sinkhorn kernels for the semi-relaxed OT problems arising from the factor relaxation step are:
THE BALANCED FRLC ALGORITHM,0.054820415879017016,"K(k)
Q := Qk ⊙exp( −γk(CRkXT
k −1ndiag−1((CRkXT
k )TQkdiag(1/gQk))T) )"
THE BALANCED FRLC ALGORITHM,0.055765595463137994,"K(k)
R := Rk ⊙exp( −γk(CTQkXk −1mdiag−1(diag(1/gRk)RT
k CTQkXk)T) ),"
THE BALANCED FRLC ALGORITHM,0.05671077504725898,"introducing the shorthand X = diag (1/gQ) T diag (1/gR) and where diag−1(·) : Rr×r →Rr
denotes the matrix-to-vector extraction of the diagonal. This τ-dependent regularization also allows
us to show smoothness of the objective in Proposition E.5, from which the convergence guarantee
Proposition 3.3 follows. We derive the semi-relaxed projection Algorithm 2 of the sub-couplings Q
and R in Appendix G for completeness. We also show in Lemma A.1 that gQ and gR induced by the
semi-relaxed projection are both feasible and locally optimal, not requiring separate optimization."
THE BALANCED FRLC ALGORITHM,0.057655954631379965,"As (Qk+1, Rk+1, T ) ∈C2 if and only if T ∈ΠgQk+1,gRk+1 , after the factor relaxation step, we next
take a coordinate MD step on the latent coupling T :"
THE BALANCED FRLC ALGORITHM,0.05860113421550094,"Tk+1 ←
arg min
T : (Qk+1,Rk+1,T )∈C2
⟨T , ∇T LLC⟩+ 1"
THE BALANCED FRLC ALGORITHM,0.05954631379962193,"γk
KL(T ∥Tk).
(9)"
THE BALANCED FRLC ALGORITHM,0.06049149338374291,This is equivalent to applying Sinkhorn (Algorithm 5) to T given gQ and gR with the kernel:
THE BALANCED FRLC ALGORITHM,0.06143667296786389,"K(k)
T
:= Tk ⊙exp( −γkdiag(1/gQk+1)QT
k+1CRk+1diag(1/gRk+1) )."
THE BALANCED FRLC ALGORITHM,0.062381852551984876,"After the final iteration of the coordinate-MD scheme, X = diag (1/gQ) T diag (1/gR) satisfies
XgR = 1r and XTgQ = 1r as T is a coupling between gQ and gR. Thus Pr = QXRT ∈Πa,b
and the iterates (Qk, Tk, Rk) remain in the intersection of the constraint sets. Thus, in contrast to
other approaches Scetbon et al. (2021); Lin et al. (2021); Forrow et al. (2019), we do not require
Dykstra projections back into the intersection to maintain feasability. We note that our implementation
of FRLC allows for a non-square latent coupling T , providing greater interpretability in problem-
specific applications. Above, we presented FRLC in the simplest case that T is square."
THE BALANCED FRLC ALGORITHM,0.06332703213610585,"3.3
Initialization, convergence, and FRLC extensions"
THE BALANCED FRLC ALGORITHM,0.06427221172022685,"Full-rank random initializations of the sub-coupling matrices.
We propose a new initialization
of the sub-couplings (Q, R, T ) for the LC-factorization in Algorithm 6.This generates a full-rank
initialization (Proposition F.1) in the set of rank-r couplings Πa,b(r) and is accomplished by applying
Sinkhorn to random matrices. Our approach differs from Scetbon et al. (2021); Scetbon & Cuturi
(2022) who use initializations for the diagonal factorization of Forrow et al. (2019), and are not
applicable to a latent coupling that is non-diagonal, non-square, or with two distinct inner marginals."
THE BALANCED FRLC ALGORITHM,0.06521739130434782,"Algorithm 1
Balanced FRLC"
THE BALANCED FRLC ALGORITHM,0.0661625708884688,"Input C, r, a, b, τ, γ, δ, ε
Initialize gQ, gR = 1"
THE BALANCED FRLC ALGORITHM,0.0671077504725898,"r1r
Q0, R0, T0 ←Initialize-Couplings(a, b, gQ, gR)
# Alg. 6
X0 ←diag(1/QT
0 1n)T0 diag(1/RT
0 1m)
while ∆((Qk, Rk, Tk), (Qk−1, Rk−1, Tk−1)) > ε do
# ∆as in (10)
∇Q ←CRkXT
k −1ndiag−1((CRkXT
k )TQkdiag(1/gQ))T"
THE BALANCED FRLC ALGORITHM,0.06805293005671077,"∇R ←CTQkXk −1mdiag−1(diag(1/gR)RT
k CTQkXk)T
γk ←γ/ max{∥∇Q∥∞, ∥∇R∥∞}
# ℓ∞-normalization of Scetbon & Cuturi (2022)
K(k)
Q , K(k)
R ←Qk ⊙exp( −γk∇Q ), Rk ⊙exp( −γk∇R )"
THE BALANCED FRLC ALGORITHM,0.06899810964083176,"Qk ←SRR-projection(K(k)
Q , γk, τ, a, QT
k−11n, δ)
# Semi-relaxed OT, Alg. 2"
THE BALANCED FRLC ALGORITHM,0.06994328922495274,"Rk ←SRR-projection(K(k)
R , γk, τ, b, RT
k−11m, δ)
# Semi-relaxed OT
gQ, gR ←QT
k 1n, RT
k 1m
∇T = diag(1/gQ)QT
k CRk diag(1/gR)
γT = γ/∥∇T ∥∞
# ℓ∞-normalization
K(k)
T
←Tk ⊙exp( −γT ∇T )
Tk ←Sinkhorn(K(k)
T , gR, gQ, δ)
# Balanced OT, Alg. 5
Xk ←diag(1/gQ)T diag(1/gR)
end while
Return Pr = QXRT"
THE BALANCED FRLC ALGORITHM,0.07088846880907372,"Convergence analysis of FRLC.
As objective (8) is non-convex, it is important to have convergence
guarantees. Our convergence criterion ∆(·, ·) is defined in (10). To prove convergence we require a
lower bound on the entries of gQ and gR. Previous works introduce a lower-bound vector α ≤g
enforced element-wise for stability and smoothness Scetbon et al. (2021). In FRLC the use of
semi-relaxed projections naturally enforces a lower-bound. In Appendix E.5, we show that for any
δ ∈(0, 1"
THE BALANCED FRLC ALGORITHM,0.07183364839319471,"r), the FRLC algorithm’s τ-weighted regularization on the inner marginals can guarantee
a uniform lower-bound of δ on the entries: for sufficiently large τ and ˜O(m2/ϵ) iterations for the
sub-coupling Pham et al. (2020), one guarantees a lower bound of δ on gR and gQ. This allows us to
show objective smoothness in Proposition E.5. Previous work on low-rank optimal transport Scetbon
et al. (2021) use the non-asymptotic convergence criterion of Ghadimi et al. (2014). Following
existing works Dang & Lan (2015) establishing convergence rates of coordinate mirror-descent for
smooth objectives, we show in Proposition 3.3 this criterion may be extended to coordinate-MD by
adapting the block-descent lemma of Beck & Tetruashvili (2013).
Proposition 3.3. Suppose one has f ∈C1(X, R) with block-coordinate Lipschitz gradient and
block smoothness constants (Li)p
i=1, and a function h ∈C(X, R) which is α-strongly convex. For
Φ = f +h, suppose one performs coordinate mirror descent on Φ minimized over a product of closed
convex sets X = Qp
i=1 Xi. Let the sub-iterates with respect to the i-th block update be {xi
k}p
i=0
where xk := x0
k for k ∈[N] outer iterations. Then one has:"
THE BALANCED FRLC ALGORITHM,0.07277882797731569,"min
k ∆(xk, xk−1) ≤
D2L
N(α2/2L) = 2D2L2 Nα2 ,"
THE BALANCED FRLC ALGORITHM,0.07372400756143667,"where D is (36), L is the global smoothness constant, stepsizes γk,i := α/L, and convergence
criterion ∆(xk, xk−1) is given in (35)."
THE BALANCED FRLC ALGORITHM,0.07466918714555766,"Specialized to the LC-parametrization, the criterion ∆k(xk, xk+1) is:"
THE BALANCED FRLC ALGORITHM,0.07561436672967864,"∆k(xk, xk+1) := 1 γ2
k"
THE BALANCED FRLC ALGORITHM,0.07655954631379962,"
∥Qk+1 −Qk∥2
F + ∥Rk+1 −Rk∥2
F + ∥Tk+1 −Tk∥2
F

(10)"
THE BALANCED FRLC ALGORITHM,0.07750472589792061,"for xk = (Qk, Rk, Tk). We show through Propositions 3.3, E.5 the following result:
Proposition 3.4. The FRLC algorithm with step-sizes γk = α/L and iterates xk = (Qk, Rk, Tk)
has non-asymptotic stationary convergence in the criterion ∆(·, ·) with:"
THE BALANCED FRLC ALGORITHM,0.07844990548204159,"min
k∈1,..,N−1 ∆k(xk, xk+1) ≤2D2L2/Nα2"
THE BALANCED FRLC ALGORITHM,0.07939508506616257,"(a)
(b)
(c)"
THE BALANCED FRLC ALGORITHM,0.08034026465028356,"Figure 2: (a) Simulated dataset containing points from two moons (orange) and eight Gaussians
(blue). (b) Transport cost ⟨C, P ⟩F achieved by FRLC and LOT Scetbon et al. (2021) for the balanced
Wasserstein problem on the dataset in (a) for different ranks and initializations. FLRC full rank (blue
curve) is average over 10 random initializations. (c) Results on the 10D mixture of Gaussians dataset."
THE BALANCED FRLC ALGORITHM,0.08128544423440454,"Where N is the number of iterations, D the optimality-gap as in (36), and L = maxi∈{1,..,3}(Li) the
global smoothness for Li = poly(∥C∥F , n, m, r, δ) the block-wise smoothness constants."
THE BALANCED FRLC ALGORITHM,0.08223062381852551,"The proof of Proposition 3.4 follows directly from our extension of the non-asymptotic criterion
with the block-descent result Proposition 3.3 and the proof that this lemma holds in FRLC Proposi-
tion E.5. We also mention two improvements to other low rank approximation results in literature.
In Proposition F.2 we show that one can analytically solve for the block-optimal g for the factor-
ization of Scetbon et al. (2021), and we improve the bound on the low-rank approximation error in
Proposition E.7."
THE BALANCED FRLC ALGORITHM,0.0831758034026465,"FRLC for other marginal constraints and objectives.
The balanced FRLC algorithm can be
extended simply to other marginal constraints owing to the decoupling of the coordinate MD scheme.
In particular, by using either the semi-relaxed projections (Algorithm 2) or fully-relaxed (unbalanced)
projections (Algorithm 3) on sub-couplings Q and R, one can solve the balanced problem, the
problem with the left or right marginal relaxed, or the unbalanced problem. As such, all variants of
marginal constraints can be handled by a single algorithm, given in Algorithm 4."
THE BALANCED FRLC ALGORITHM,0.08412098298676748,"We also extend the FRLC algorithm to the Gromov-Wasserstein problem. This consists of computing
a GW-specific gradient with the appropriate marginal constraints applied to simplify their form,
and re-computing Sinkhorn kernels as exponentiations of these gradients. The matrix form of the
quadratic GW objective is 1T
mP TA⊙2P 1m + 1T
nP B⊙2P T1n −2⟨AP B, P ⟩, where ⊙denotes
the Hadamard (entrywise) product. Then the GW-specific Sinkhorn kernels are"
THE BALANCED FRLC ALGORITHM,0.08506616257088846,"K(k)
Q ←exp
 
2γk(2AQXRTBRXT −A⊙2Q1r1T
r )

,"
THE BALANCED FRLC ALGORITHM,0.08601134215500945,"K(k)
R ←exp
 
2γk(2BRXTQTAQX −B⊙2R1r1T
r )

,"
THE BALANCED FRLC ALGORITHM,0.08695652173913043,"K(k)
T
←exp(4γk diag(g−1
Q )QTAQXRTBR diag(g−1
R ))."
THE BALANCED FRLC ALGORITHM,0.08790170132325142,"In Algorithm 4, one can solve the GW-problem by using the kernels above. Here, we present the
kernels omitting a rank-1 perturbation, which is given in Appendix D. From the Wasserstein and GW
gradients, the FGW gradient is easily taken as a convex combination of the two. In this work, we
primarily focus on the LC-factorization for the rank r Wasserstein problem (6)."
EXPERIMENTAL RESULTS,0.0888468809073724,"4
Experimental Results"
EXPERIMENTAL RESULTS,0.08979206049149338,"We compare FRLC to existing low-rank and full-rank optimal transport algorithms on several datasets:
simulated datasets previously used in Tong et al. (2023) and Scetbon et al. (2021); a massive spatial-
transcriptomics dataset Chen et al. (2022); and a graph partitioning task Chowdhury & Needham
(2021). Further details of each experiment (e.g. pre-processing, validation) are in Appendices K, L,
and M. In the section below, LOT refers to the works of Scetbon et al. (2021, 2023, 2022) and Latent
OT refers to Lin et al. (2021)."
EXPERIMENTAL RESULTS,0.09073724007561437,"(a)
(d)
(c)
(b)"
EXPERIMENTAL RESULTS,0.09168241965973535,"FRLC (Rank 5)
FRLC (Rank 10)
LOT (Rank 5)
LOT (Rank 10) (e)"
EXPERIMENTAL RESULTS,0.09262759924385633,Ground Truth (Full rank)
EXPERIMENTAL RESULTS,0.09357277882797732,"Figure 3: LC-projections of couplings of Gaussians centered on the 5th-roots of unity (green) and 10th
roots of unity (yellow). (a) Ground-truth full-rank coupling. (b) Non-square rank-5 latent-coupling of
FRLC (c) LC-projection barycenters aligned with rank-5 diagonal coupling of LOT Scetbon et al.
(2021). (d) Square rank-10 latent coupling of FRLC. (e) Rank-10 diagonal coupling of LOT ."
EVALUATION OF LOW-RANK APPROXIMATIONS FOR BALANCED OT ON SYNTHETIC DATA,0.0945179584120983,"4.1
Evaluation of Low-rank Approximations for Balanced OT on Synthetic Data"
EVALUATION OF LOW-RANK APPROXIMATIONS FOR BALANCED OT ON SYNTHETIC DATA,0.09546313799621928,"We first compare the balanced OT version of FRLC with the the low-rank balanced OT algorithm
LOT of Scetbon et al. (2021) on a synthetic dataset following Tong et al. (2023). The dataset consists
of m = 1000 points from two moons and n = 1000 points sampled from eight 2D Gaussian densities
(Fig. 2a). We solve the Wasserstein problem (1) with cost matrix C computed using the Euclidean
distance. The full-rank coupling matrix P has rank 1000, and we compute both FRLC and LOT
solutions with rank between 20 and 200. For each rank, we initialize FRLC adapting the deterministic
rank-2 initialization proposed in Scetbon et al. (2021) and the random initialization of Alg. 6. We
initialize LOT using the rank-2 initialization and two other options in ott-jax Cuturi et al. (2022)."
EVALUATION OF LOW-RANK APPROXIMATIONS FOR BALANCED OT ON SYNTHETIC DATA,0.09640831758034027,"We find that FRLC obtains lower transport cost ⟨C, P ⟩F with increasing rank (Fig. 2b) and consis-
tently achieves lower transport cost than LOT across all ranks and all initializations. Specifically,
starting both methods at the same rank-2 initialization, FRLC consistently achieves a lower cost
than LOT for all ranks. Additionally, we observe smooth convergence of FRLC for both rank-2
initialization and the full-rank random initialization of Alg. 6 (Fig. 5)."
EVALUATION OF LOW-RANK APPROXIMATIONS FOR BALANCED OT ON SYNTHETIC DATA,0.09735349716446125,"We also evaluate FRLC and LOT on two datasets of Gaussian mixtures, one in 2-dimensions and
one in 10-dimensions, each with n = m = 5, 000 points from two mixtures of Gaussians, following
Scetbon et al. (2021), with further details in Appendix K. We observe the same trend as the previous
simulation for both datasets (Fig. 2c, Fig. 7), with FRLC achieving lower transport costs than LOT
across all ranks and all initializations. In addition FRLC has half the runtime of LOT (CPU) –
including the setup time of FRLC but excluding the setup time of LOT in ott-jax – on datasets
of n = m = 1000 points from all three datasets with rank r = 100 (Table 2). At the same time
FRLC achieves lower primal cost ⟨C, P ⟩F with tighter marginals ∥P 1n −a∥2 and ∥P T 1m −b∥2.
Lin et al. (2021) only solves a proxy for the rank-constrained Wasserstein problem, and thus is not
the focus of our comparisons. Nevertheless, we verify that on all synthetic experiments that FRLC
achieves significantly lower primal OT cost than Latent OT (Table 5)."
INTERPRETATION OF THE LATENT COUPLING AND LC-PROJECTION,0.09829867674858223,"4.2
Interpretation of the Latent Coupling and LC-Projection"
INTERPRETATION OF THE LATENT COUPLING AND LC-PROJECTION,0.09924385633270322,"We demonstrate the intepretability of the latent coupling T in the LC factorization. In both the LC
factorization and factored couplings, the sub-couplings Q and R each have associated barycentric
projection operators which coarse-grain input datasets Z(1), Z(2). In particular, the LC projection is
defined from the LC factorization as follows."
INTERPRETATION OF THE LATENT COUPLING AND LC-PROJECTION,0.1001890359168242,"Definition 4.1 (LC-Projection). Let Q diag(1/gQ)T diag(1/gR)RT be an LC factorization of
of a coupling matrix P ∈Πa,b(r) computed from datasets Z(1) ∈Rn×d, Z(2) ∈Rm×d, with
T ∈Rr1×r2
+
. The LC-projections Y(1) and Y(2) of Z(1) and Z(2) are Y(1) := diag(1/gQ)QTZ(1),
and Y(2) := diag(1/gR)RTZ(2)."
INTERPRETATION OF THE LATENT COUPLING AND LC-PROJECTION,0.10113421550094517,"By interpreting any factored coupling (Q, R, g) as an LC factorization (Q, R, diag(g)), Defini-
tion 4.1 describes the barycentric projections for both factorizations. We compare the projections of
the coupling computed by FRLC to those of LOT Scetbon et al. (2021) on a dataset containing 1000
samples from 2D-Gaussians centered at the 5th-roots of unity and 1000 samples from 2D Gaussians (a)"
INTERPRETATION OF THE LATENT COUPLING AND LC-PROJECTION,0.10207939508506617,"Objective
Algorithm
Spearman ρ
ARI
AMI"
INTERPRETATION OF THE LATENT COUPLING AND LC-PROJECTION,0.10302457466918714,"W
LOT-U
0.394
0.332
0.397
FRLC-U
0.465
0.466
0.484"
INTERPRETATION OF THE LATENT COUPLING AND LC-PROJECTION,0.10396975425330812,"LOT-SR
0.391
0.319
0.396
FRLC-SR
0.467
0.475
0.492"
INTERPRETATION OF THE LATENT COUPLING AND LC-PROJECTION,0.10491493383742911,"GW
LOT-U
0.005
0.0
0.0
FRLC-U
0.266
0.299
0.364"
INTERPRETATION OF THE LATENT COUPLING AND LC-PROJECTION,0.10586011342155009,"LOT-SR
0.004
0.0
0.0
FRLC-SR
0.275
0.318
0.381"
INTERPRETATION OF THE LATENT COUPLING AND LC-PROJECTION,0.10680529300567108,"FGW
LOT-U
0.391
0.330
0.400
FRLC-U
0.368
0.391
0.420"
INTERPRETATION OF THE LATENT COUPLING AND LC-PROJECTION,0.10775047258979206,"LOT-SR
0.396
0.337
0.401
FRLC-SR
0.465
0.469
0.499"
INTERPRETATION OF THE LATENT COUPLING AND LC-PROJECTION,0.10869565217391304,"Figure 4: (a) Brain marker gene Tubb2b expression and FRLC prediction. (b) Comparison of the
low-rank unbalanced (LOT-U) algorithm of Scetbon et al. (2023) and FRLC on aligning spatial
transcriptomics data. Bold indicates top performing method for each metric on each objective."
INTERPRETATION OF THE LATENT COUPLING AND LC-PROJECTION,0.10964083175803403,"centered at the 10th-roots of unity (the latter scaled by a factor of two, Fig. 3a). In both cases, the
latent coupling T or diag(g) is visualized as a transport between barycenters. We run FRLC and
LOT with ranks r = 5 and r = 10 to match the number of target and source clusters. In the rank-5
case, FRLC uses a non-square latent coupling T ∈R10×5
+
which correctly captures the coupling
between clusters (Fig. 3(b)), while the LOT rank-5 projection computes barycenters that are outside
of the clusters (Fig. 3c). A similar result is observed for square rank-10 latent couplings computed by
FRLC (Fig. 3d) and LOT (Fig. 3e) demonstrating that the LOT barycenters in Fig. 3b) are not an
artifact of using the lowest rank. We observe similar results on other simulated datasets (Fig. 11)."
EVALUATION ON SPATIAL TRANSCRIPTOMICS ALIGNMENT,0.11058601134215501,"4.3
Evaluation on Spatial Transcriptomics Alignment"
EVALUATION ON SPATIAL TRANSCRIPTOMICS ALIGNMENT,0.11153119092627599,"We compare FRLC and the algorithm (LOT-U) of Scetbon et al. (2023) (which solves unbalanced
low-rank Wasserstein, GW, and FGW problems) on the task of computing an alignment between
cells from different time points during mouse embryonic development. Specifically, we compute
an alignment between a spatial transcriptomics (ST) dataset of an E11.5 stage mouse embryo and
an E12.5 stage mouse embryo Chen et al. (2022). Optimal transport is a popular approach to align
single-cell Schiebinger et al. (2019) and spatial trancriptomics datasets Zeira et al. (2022); Liu et al.
(2023); Klein et al. (2023). In single-cell transcriptomics, one measures a gene expression vector
for each cell, and in spatial transcriptomics one additionally measures the 2D location of each cell.
The cost matrix C describes the difference between gene expression vectors and intra-domain cost
matrices A and B are derived from the 2D coordinates within each slice. Therefore, OT problems of
W, GW, and FGW objectives can be solved and the coupling matrix represents the cell-cell alignment
(Appendix M). However, computation of a full-rank OT solution is not feasible in our large-scale
dataset: the E11.5 slice has about 30,000 cells while the E12.5 slice has about 50,000 cells."
EVALUATION ON SPATIAL TRANSCRIPTOMICS ALIGNMENT,0.11247637051039698,"We evaluate the alignments by assessing performance on two prediction tasks from Scetbon et al.
(2023): (1) a gene expression prediction task where we predict the expression of a gene in E12.5
from expression of the gene in E11.5 using the alignment; (2) a cell type prediction task where we
predict the cell types of E12.5 from the cell type clustering of E11.5 (Appendix M). We evaluate
the accuracy of the gene expression prediction task through the Spearman correlation ρ between
the predicted expression and the ground truth expression of 10 test marker genes. We evaluate the
accuracy of the cell type prediction task by computing the Adjusted Rand Index (ARI) and Adjusted
Mutual Information (AMI) between the predicted cell types and the cell types derived in the original
publication Chen et al. (2022). Being a comparison between different objectives, this relies on
downstream metrics. For completeness, we validate the efficacy of FRLC on directly minimizing the
balanced Wasserstein cost ⟨C, P ⟩F against Scetbon et al. (2021) in Figure 8."
EVALUATION ON SPATIAL TRANSCRIPTOMICS ALIGNMENT,0.11342155009451796,"For a direct comparison, we use FRLC to solve the same unbalanced problems (denoted FRLC-U).
We perform an extensive grid search (Appendix M.3) to pick the best hyperparameters (including
rank ≪30, 000) for all algorithms. Scetbon et al. (2023) previously showed that unbalanced FGW
algorithm has the best performance on ST alignment. We find that unbalanced FRLC achieves
comparable or better results than the previous state-of-the-art unbalanced low-rank method on all
three objectives (Table 4). We also solve a semi-relaxed version of each problem motivated by the"
EVALUATION ON SPATIAL TRANSCRIPTOMICS ALIGNMENT,0.11436672967863894,"Method
Factorization
Cost
Variables
Algorithm
Sub-routine
for coupling"
EVALUATION ON SPATIAL TRANSCRIPTOMICS ALIGNMENT,0.11531190926275993,"Factored Coupling
Forrow et al. (2019)
Factored coupling
k-Wasserstein
barycenter
Anchors &
sub-couplings
Lloyd-type
Dykstra’s"
EVALUATION ON SPATIAL TRANSCRIPTOMICS ALIGNMENT,0.11625708884688091,"Latent OT
Lin et al. (2021)
Latent coupling
Extension of
k-Wasserstein
barycenter"
EVALUATION ON SPATIAL TRANSCRIPTOMICS ALIGNMENT,0.11720226843100189,"Anchors &
sub-couplings
Lloyd-type
Dykstra’s"
EVALUATION ON SPATIAL TRANSCRIPTOMICS ALIGNMENT,0.11814744801512288,"LOT
Scetbon et al. (2021) Factored coupling Primal OT cost Sub-couplings &
inner marginal
Mirror-descent
Dykstra’s"
EVALUATION ON SPATIAL TRANSCRIPTOMICS ALIGNMENT,0.11909262759924386,"FRLC (this work)
Latent coupling
Primal OT cost
Sub-couplings
Coordinate
mirror-descent
OT"
EVALUATION ON SPATIAL TRANSCRIPTOMICS ALIGNMENT,0.12003780718336483,"Table 1: Comparing aspects of low-rank OT methods. Factorization indicates the structure of the
inner matrix."
EVALUATION ON SPATIAL TRANSCRIPTOMICS ALIGNMENT,0.12098298676748583,"observation that all cells from E12.5 have an ancestor, but not all cells from E11.5 have the same
number of descendants due to cell growth and death. Thus the former marginal is tight, and the latter
relaxed Halmos et al. (2024). We run both semi-relaxed FRLC (FRLC-SR) and a setting of LOT-U
that recovers the semi-relaxed problem (LOT-SR). Semi-relaxed FRLC achieves the best results on
all three metrics by a large margin (Table 4). As one example, the expression of Tubb2b, a mouse
brain marker gene, agreeing with the expression predicted from the semi-relaxed alignment of FRLC
(Fig. 4a)."
ADDITIONAL EXPERIMENTS,0.1219281663516068,"4.4
Additional Experiments
We evaluate FRLC on an unsupervised graph partitioning problem Chowdhury & Needham (2021) on
four real-world graph datasets Yang & Leskovec (2012); Yin et al. (2017); Banerjee et al. (2013). We
benchmark the performance of the semi-relaxed and GW settings of FRLC against (1) GWL Xu et al.
(2019), solving a balanced GW problem; (2) SpecGWL Chowdhury & Needham (2021) using the
heat kernel on the graph Laplacian as the cost matrix. We find FRLC achieves the better clustering
performance than GWL and SpecGWL on 9/12 and 11/12 of the datasets (Table 3 and Appendix L).
5
Discussion
We provide comparison of existing low-rank solvers in Table 1. The FRLC algorithm has a number
of advantages, including (1) coarsening a full-rank plan P to non-diagonal latent coupling T ; (2)
minimizing the primal OT problem for general cost C rather than a barycenteric problem; (3)
optimizing only sub-couplings; and (4) using Sinkhorn alone as the sub-routine for low-rank OT.
While we argue these are substantial advantages, FRLC has limitations which warrant follow-up work.
In particular, three key limitations of our work, common to the existing low-rank OT algorithms,
are: (1) selecting values of the latent coupling ranks; (2) strengthening the convergence criterion; (3)
addressing sensitivity to the initialization from non-convexity of the objective. A limitation specific
to our work is the selection of the τ hyperparameter controlling the smoothness of the trajectory.
These and other limitations are discussed in Section N of the Appendix. Another direction for further
investigation is to better understand what structure LC factorizations capture when the optimal plan is
known to have full rank, e.g. when the Monge map exists, as has been explored by Liu et al. (2021).
6
Conclusion
We introduce FRLC, an algorithm to compute low-rank optimal transport plan from the latent
coupling (LC) factorization. FRLC handles different OT objective costs and relaxations of the
marginal constraints. Moreover, the LC factorization provides an interpretable coarse-graining
of the full transport plan and its marginals through the mapping (P , a, b) →(T , gQ, gR). We
demonstrate the superior performance of FRLC compared to state-of-the-art low-rank methods on
real and synthetic datasets."
ADDITIONAL EXPERIMENTS,0.12287334593572778,Acknowledgments and Disclosure of Funding
ADDITIONAL EXPERIMENTS,0.12381852551984877,"This work is supported by NCI grant U24CA248453 to B.J.R. J.G. gratefully acknowledges support
from the Schmidt DataX Fund at Princeton University made possible through a major gift from the
Schmidt Futures Foundation."
REFERENCES,0.12476370510396975,References
REFERENCES,0.12570888468809074,"Bakshi, A. and Woodruff, D. Sublinear Time Low-Rank Approximation of Distance Matrices.
Advances in Neural Information Processing Systems, 31, 2018."
REFERENCES,0.1266540642722117,"Banerjee, A., Chandrasekhar, A. G., Duflo, E., and Jackson, M. O. The Diffusion of Microfinance.
Science, 341(6144):1236498, 2013."
REFERENCES,0.1275992438563327,"Bauschke, H. H. and Lewis, A. S. Dykstras algorithm with Bregman projections: A convergence proof.
Optimization, 48(4):409–427, January 2000. ISSN 1029-4945. doi: 10.1080/02331930008844513.
URL http://dx.doi.org/10.1080/02331930008844513."
REFERENCES,0.1285444234404537,"Beck, A. and Tetruashvili, L. On the Convergence of Block Coordinate Descent Type Methods.
SIAM J. Optim., 23:2037–2060, 2013. URL https://api.semanticscholar.org/CorpusID:
6866704."
REFERENCES,0.12948960302457466,"Benamou, J.-D., Carlier, G., Cuturi, M., Nenna, L., and Peyré, G.
Iterative Bregman Projec-
tions for Regularized Transportation Problems.
SIAM Journal on Scientific Computing, 37
(2):A1111–A1138, January 2015. ISSN 1095-7197. doi: 10.1137/141000439. URL http:
//dx.doi.org/10.1137/141000439."
REFERENCES,0.13043478260869565,"Bregman, L. M. The relaxation method of finding the common point of convex sets and its application
to the solution of problems in convex programming. USSR computational mathematics and
mathematical physics, 7(3):200–217, 1967."
REFERENCES,0.13137996219281664,"Bunne, C., Stark, S. G., Gut, G., del Castillo, J. S., Levesque, M., Lehmann, K.-V., Pelkmans, L.,
Krause, A., and Rätsch, G. Learning single-cell perturbation responses using neural optimal
transport. Nature Methods, 20(11):1759–1768, September 2023. ISSN 1548-7105. doi: 10.1038/
s41592-023-01969-x. URL http://dx.doi.org/10.1038/s41592-023-01969-x."
REFERENCES,0.1323251417769376,"Charikar, M., Chen, B., Ré, C., and Waingarten, E. Fast Algorithms for a New Relaxation of Optimal
Transport. In Neu, G. and Rosasco, L. (eds.), Proceedings of Thirty Sixth Conference on Learning
Theory, volume 195 of Proceedings of Machine Learning Research, pp. 4831–4862. PMLR, 12–15
Jul 2023. URL https://proceedings.mlr.press/v195/charikar23a.html."
REFERENCES,0.1332703213610586,"Chen, A., Liao, S., Cheng, M., Ma, K., Wu, L., Lai, Y., Qiu, X., Yang, J., Xu, J., Hao, S., et al.
Spatiotemporal transcriptomic atlas of mouse organogenesis using DNA nanoball-patterned arrays.
Cell, 185(10):1777–1792, 2022."
REFERENCES,0.1342155009451796,"Chen, X. and Price, E. Condition number-free query and active learning of linear families. 2017."
REFERENCES,0.13516068052930058,"Chizat, L., Peyré, G., Schmitzer, B., and Vialard, F.-X. Unbalanced Optimal Transport: Dynamic and
Kantorovich Formulations. Journal of Functional Analysis, 274(11):3090–3123, June 2018. ISSN
0022-1236. doi: 10.1016/j.jfa.2018.03.008. URL http://dx.doi.org/10.1016/j.jfa.2018.
03.008."
REFERENCES,0.13610586011342155,"Chowdhury, S. and Needham, T. Generalized Spectral Clustering via Gromov-Wasserstein Learning.
In International Conference on Artificial Intelligence and Statistics, pp. 712–720. PMLR, 2021."
REFERENCES,0.13705103969754254,"Chung, F. Laplacians and the Cheeger Inequality for Directed Graphs. Annals of Combinatorics, 9:
1–19, 2005."
REFERENCES,0.13799621928166353,"Cohen, J. E. and Rothblum, U. G. Nonnegative Ranks, Decompositions, and Factorizations of
Nonnegative Matrices. Linear Algebra and its Applications, 190:149–168, 1993."
REFERENCES,0.1389413988657845,"Courty, N., Flamary, R., and Tuia, D. Domain adaptation with regularized optimal transport. In
Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD
2014, Nancy, France, September 15-19, 2014. Proceedings, Part I 14, pp. 274–289. Springer, 2014."
REFERENCES,0.13988657844990549,"Cuturi, M. Sinkhorn Distances: Lightspeed Computation of Optimal Transport. Advances in neural
information processing systems, 26, 2013a."
REFERENCES,0.14083175803402648,"Cuturi, M. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in Neural
Information Processing Systems, pp. 2292–2300, 2013b. URL https://proceedings.neurips.
cc/paper/2013/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html."
REFERENCES,0.14177693761814744,"Cuturi, M., Meng-Papaxanthos, L., Tian, Y., Bunne, C., Davis, G., and Teboul, O. Optimal Transport
Tools (OTT): A JAX Toolbox for all things Wasserstein. arXiv preprint arXiv:2201.12324, 2022."
REFERENCES,0.14272211720226843,"Dang, C. D. and Lan, G. Stochastic Block Mirror Descent Methods for Nonsmooth and Stochastic
Optimization. SIAM J. Optim., 25(2):856–881, January 2015."
REFERENCES,0.14366729678638943,"Dong, S., Pan, Z., Fu, Y., Xu, D., Shi, K., Yang, Q., Shi, Y., and Zhuo, C. Partial Unbalanced Feature
Transport for Cross-Modality Cardiac Image Segmentation. IEEE Transactions on Medical
Imaging, 2023."
REFERENCES,0.1446124763705104,"Dykstra, R. L. An Algorithm for Restricted Least Squares Regression. Journal of the American
Statistical Association, 78(384):837–842, December 1983. ISSN 1537-274X. doi: 10.1080/
01621459.1983.10477029. URL http://dx.doi.org/10.1080/01621459.1983.10477029."
REFERENCES,0.14555765595463138,"Forrow, A., Hütter, J.-C., Nitzan, M., Rigollet, P., Schiebinger, G., and Weed, J. Statistical Optimal
Transport via Factored Couplings. In Chaudhuri, K. and Sugiyama, M. (eds.), Proceedings of
the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89
of Proceedings of Machine Learning Research, pp. 2454–2465. PMLR, 16–18 Apr 2019. URL
https://proceedings.mlr.press/v89/forrow19a.html."
REFERENCES,0.14650283553875237,"Frieze, A., Kannan, R., and Vempala, S. Fast Monte-Carlo Algorithms for Finding Low-rank
Approximations. J. ACM, 51(6):1025–1041, nov 2004. ISSN 0004-5411. doi: 10.1145/1039488.
1039494. URL https://doi.org/10.1145/1039488.1039494."
REFERENCES,0.14744801512287334,"Frogner, C., Zhang, C., Mobahi, H., Araya, M., and Poggio, T. A. Learning with a Wasserstein Loss.
Advances in neural information processing systems, 28, 2015."
REFERENCES,0.14839319470699433,"Geshkovski, B., Letrouit, C., Polyanskiy, Y., and Rigollet, P.
A mathematical perspective on
Transformers. arXiv preprint arXiv:2312.10794, 2023."
REFERENCES,0.14933837429111532,"Ghadimi, S., Lan, G., and Zhang, H. Mini-batch stochastic approximation methods for nonconvex
stochastic composite optimization. Mathematical Programming, 155(1–2):267–305, December
2014. ISSN 1436-4646. doi: 10.1007/s10107-014-0846-1. URL http://dx.doi.org/10.
1007/s10107-014-0846-1."
REFERENCES,0.1502835538752363,"Halmos, P., Liu, X., Gold, J., Chen, F., Ding, L., and Raphael, B. J. DeST-OT: Alignment of
Spatiotemporal Transcriptomics Data. In International Conference on Research in Computational
Molecular Biology, pp. 434–437. Springer, 2024."
REFERENCES,0.15122873345935728,"Indyk, P., Vakilian, A., Wagner, T., and Woodruff, D. P. Sample-optimal low-rank approxima-
tion of distance matrices. In Beygelzimer, A. and Hsu, D. (eds.), Proceedings of the Thirty-
Second Conference on Learning Theory, volume 99 of Proceedings of Machine Learning Re-
search, pp. 1723–1751. PMLR, 25–28 Jun 2019. URL https://proceedings.mlr.press/
v99/indyk19a.html."
REFERENCES,0.15217391304347827,"Kantorovich, L. On the Translocation of Masses: Doklady akademii nauk ussr. 1942."
REFERENCES,0.15311909262759923,"Klein, D., Palla, G., Lange, M., Klein, M., Piran, Z., Gander, M., Meng-Papaxanthos, L., Sterr, M.,
Bastidas-Ponce, A., Tarquis-Medina, M., et al. Mapping cells through time and space with moscot.
bioRxiv, pp. 2023–05, 2023."
REFERENCES,0.15406427221172023,"Lin, C.-H., Azabou, M., and Dyer, E. L. Making transport more robust and interpretable by moving
data through a small number of anchor points. Proceedings of machine learning research, 139:
6631, 2021."
REFERENCES,0.15500945179584122,"Liu, W., Zhang, C., Zheng, N., and Qian, H. Approximating optimal transport via low-rank and sparse
factorization. CoRR, abs/2111.06546, 2021. URL https://arxiv.org/abs/2111.06546."
REFERENCES,0.15595463137996218,"Liu, X., Zeira, R., and Raphael, B. J. Partial alignment of multislice spatially resolved transcriptomics
data. Genome Research, 33(7):1124–1132, 2023."
REFERENCES,0.15689981096408318,"Mémoli, F. On the use of Gromov-Hausdorff Distances for Shape Comparison. 2007."
REFERENCES,0.15784499054820417,"Mémoli, F. Gromov–Wasserstein Distances and the Metric Approach to Object Matching. Founda-
tions of computational mathematics, 11:417–487, 2011."
REFERENCES,0.15879017013232513,"Nesterov, Y. Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems. SIAM
Journal on Optimization, 22(2):341–362, 2012."
REFERENCES,0.15973534971644612,"Orlin, J. B. A polynomial time primal network simplex algorithm for minimum cost flows. Mathe-
matical Programming, 78(2):109–129, Aug 1997. ISSN 1436-4646. doi: 10.1007/BF02614365.
URL https://link.springer.com/content/pdf/10.1007/BF02614365.pdf."
REFERENCES,0.16068052930056712,"Pham, K., Le, K., Ho, N., Pham, T., and Bui, H. H. On Unbalanced Optimal Transport: An
Analysis of Sinkhorn Algorithm. In International Conference on Machine Learning, 2020. URL
https://api.semanticscholar.org/CorpusID:211068892."
REFERENCES,0.16162570888468808,"Sander, M. E., Ablin, P., Blondel, M., and Peyré, G. Sinkformers: Transformers with Doubly
Stochastic Attention. In Camps-Valls, G., Ruiz, F. J. R., and Valera, I. (eds.), Proceedings
of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of
Proceedings of Machine Learning Research, pp. 3515–3530. PMLR, 28–30 Mar 2022. URL
https://proceedings.mlr.press/v151/sander22a.html."
REFERENCES,0.16257088846880907,"Scetbon, M. and Cuturi, M. Low-rank Optimal Transport: Approximation, Statistics and Debiasing.
In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information
Processing Systems, 2022. URL https://openreview.net/forum?id=4btNeXKFAQ."
REFERENCES,0.16351606805293006,"Scetbon, M., Cuturi, M., and Peyré, G. Low-Rank Sinkhorn Factorization. In International Con-
ference on Machine Learning, 2021. URL https://api.semanticscholar.org/CorpusID:
232147563."
REFERENCES,0.16446124763705103,"Scetbon, M., Peyré, G., and Cuturi, M. Linear-time Gromov Wasserstein Distances using Low Rank
Couplings and Costs. In International Conference on Machine Learning, pp. 19347–19365. PMLR,
2022."
REFERENCES,0.16540642722117202,"Scetbon, M., Klein, M., Palla, G., and Cuturi, M. Unbalanced Low-rank Optimal Transport Solvers,
2023."
REFERENCES,0.166351606805293,"Schiebinger, G., Shu, J., Tabaka, M., Cleary, B., Subramanian, V., Solomon, A., Gould, J., Liu, S.,
Lin, S., Berube, P., et al. Optimal-Transport Analysis of Single-Cell Gene Expression Identifies
Developmental Trajectories in Reprogramming. Cell, 176(4):928–943, 2019."
REFERENCES,0.16729678638941398,"Solomon, J., De Goes, F., Peyré, G., Cuturi, M., Butscher, A., Nguyen, A., Du, T., and Guibas, L.
Convolutional Wasserstein Distances: Efficient Optimal Transportation on Geometric Domains.
ACM Transactions on Graphics (ToG), 34(4):1–11, 2015."
REFERENCES,0.16824196597353497,"Ståhl, P. L., Salmén, F., Vickovic, S., Lundmark, A., Navarro, J. F., Magnusson, J., Giacomello, S.,
Asp, M., Westholm, J. O., Huss, M., et al. Visualization and analysis of gene expression in tissue
sections by spatial transcriptomics. Science, 353(6294):78–82, 2016."
REFERENCES,0.16918714555765596,"Tarjan, R. E.
Dynamic trees as search trees via Euler tours, applied to the network sim-
plex algorithm.
Mathematical Programming, 78(2):169–177, Aug 1997.
ISSN 1436-4646.
doi: 10.1007/BF02614369.
URL https://link.springer.com/content/pdf/10.1007/
BF02614369.pdf."
REFERENCES,0.17013232514177692,"Tay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C. Sparse Sinkhorn Attention. In III, H. D. and
Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume
119 of Proceedings of Machine Learning Research, pp. 9438–9447. PMLR, 13–18 Jul 2020. URL
https://proceedings.mlr.press/v119/tay20a.html."
REFERENCES,0.17107750472589792,"Tong, A., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Fatras, K., Wolf, G., and Bengio, Y.
Improving and generalizing flow-based generative models with minibatch optimal transport. In
ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023."
REFERENCES,0.1720226843100189,"Vayer, T., Chapel, L., Flamary, R., Tavenard, R., and Courty, N. Fused Gromov-Wasserstein
distance for structured objects. Algorithms, 13(9):212, August 2020. ISSN 1999-4893. doi:
10.3390/a13090212. URL http://dx.doi.org/10.3390/a13090212."
REFERENCES,0.17296786389413987,"Vincent-Cuaz, C., Flamary, R., Corneli, M., Vayer, T., and Courty, N. Semi-relaxed Gromov-
Wasserstein divergence and applications on graphs. In International Conference on Learning
Representations, 2022. URL https://openreview.net/forum?id=RShaMexjc-x."
REFERENCES,0.17391304347826086,"Wolf, F. A., Angerer, P., and Theis, F. J. SCANPY: large-scale single-cell gene expression data
analysis. Genome biology, 19:1–5, 2018."
REFERENCES,0.17485822306238186,"Xu, H., Luo, D., Zha, H., and Duke, L. C. Gromov-Wasserstein Learning for Graph Matching and
Node Embedding. In International conference on machine learning, pp. 6932–6941. PMLR, 2019."
REFERENCES,0.17580340264650285,"Yang, J. and Leskovec, J. Defining and Evaluating Network Communities based on Ground-truth. In
Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics, pp. 1–8, 2012."
REFERENCES,0.1767485822306238,"Yang, K. D., Damodaran, K., Venkatachalapathy, S., Soylemezoglu, A. C., Shivashankar, G., and
Uhler, C. Predicting cell lineages using autoencoders and optimal transport. PLoS computational
biology, 16(4):e1007828, 2020."
REFERENCES,0.1776937618147448,"Yin, H., Benson, A. R., Leskovec, J., and Gleich, D. F. Local Higher-Order Graph Clustering. In
Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data
mining, pp. 555–564, 2017."
REFERENCES,0.1786389413988658,"Zeira, R., Land, M., Strzalkowski, A., and Raphael, B. J. Alignment and integration of spatial
transcriptomics data. Nature Methods, 19(5):567–575, 2022."
REFERENCES,0.17958412098298676,"A
Low-rank optimal transport"
REFERENCES,0.18052930056710775,"A.1
Low-rank factorizations"
REFERENCES,0.18147448015122875,"The set of low-rank couplings.
Given M ∈Rn×m
+
, the nonnegative rank of M is the least number
of nonnegative, rank-1 matrices that sum to M:"
REFERENCES,0.1824196597353497,"rk+(M) = min
r≥1 ( M = r
X"
REFERENCES,0.1833648393194707,"i=1
Mi, such that rk(Mi) = 1 and Mi ≥0 for all i ) ."
REFERENCES,0.1843100189035917,"Let a ∈∆n, b ∈∆m be probability vectors, and let Πa,b(r) denote the set of rank-r coupling
matrices with marginals a and b:"
REFERENCES,0.18525519848771266,"Πa,b(r) = {P ∈Rn×m
+
: P T1m = a, P 1n = b, rk+(P ) ≤r}."
REFERENCES,0.18620037807183365,"To optimize any cost over Πa,b(r), one requires a parameterization of this set."
REFERENCES,0.18714555765595464,"Factored couplings.
The factored coupling parameterization of Πa,b(r) introduced in Forrow et al.
(2019), and used by Scetbon et al. (2021); Scetbon & Cuturi (2022); Scetbon et al. (2022, 2023) is"
REFERENCES,0.1880907372400756,"FCa,b(r) := {(Q, R, g) ∈Rn×r
+
× Rm×r
+
× (R∗
+)r : Q ∈Πa,g, R ∈Πb,g}."
REFERENCES,0.1890359168241966,"Cohen & Rothblum (1993) show that any P ∈Πa,b(r) may be decomposed as P = Qdiag(1/g)RT
for some triple (Q, R, g) ∈FC. Thus, for cost matrix C ∈Rn×m, the general low-rank optimal
transport problem is equivalent to an optimization over factored couplings:"
REFERENCES,0.1899810964083176,"min
P ∈Πa,b(r)⟨C, P ⟩F =
min
(Q,R,g)∈FCa,b(r)⟨C, Qdiag(1/g)RT⟩F .
(11)"
REFERENCES,0.19092627599243855,"Latent coupling factorization.
The latent coupling parameterization of Πa,b(r) introduced in Lin
et al. (2021), and used in the present work is"
REFERENCES,0.19187145557655955,"LCa,b(r) := {(Q, R, T ) ∈Rn×r
+
× Rm×r
+
× Rr×r
+
: Q ∈Πa,·, R ∈Πb,·, T ∈ΠgQ,gR},"
REFERENCES,0.19281663516068054,"where gQ, gR are the inner marginals of Q and R."
REFERENCES,0.1937618147448015,"Latent coupling diagonalization. The LC-factorization recovers the factorization of Forrow et al.
(2019) as a sub-case. While the diagonal factorization of previous works cannot be directly converted
to the LC-factorization, the LC-factorization can easily recover the diagonal factorization. In
particular, taking Q′ ←Q diag(1/gQ)T one can refactor"
REFERENCES,0.1947069943289225,Pr = Q diag(1/gQ)T diag(1/gR)RT = Q′ diag(1/gR)RT
REFERENCES,0.1956521739130435,or alternatively taking R′ = R diag(1/gR)T T may refactor as
REFERENCES,0.19659735349716445,Pr = Q diag(1/gQ)T diag(1/gR)RT = Q diag(1/gQ)(R′)T
REFERENCES,0.19754253308128544,"So that instead of returning (Q, R, T ) one may alternatively return (Q, R, T ) →(Q′, R, diag(gR))
or (Q, R, T ) →(Q, R′, diag(gQ)) to recover the Forrow et al. (2019) factorization. An example of
this diagonal-conversion is offered in Figure 12."
REFERENCES,0.19848771266540643,"A.2
Balanced low-rank optimal transport"
REFERENCES,0.1994328922495274,"The FRLC optimization problem
Our optimization problem is over the variables (Q, R, T ) and
defined as follows:"
REFERENCES,0.2003780718336484,"min
(Q,R,T )∈LCa,b(r) LLC(Q, R, T ),
(12)"
REFERENCES,0.20132325141776938,where our objective function LLC is
REFERENCES,0.20226843100189035,"LLC(Q, R, T ) = ⟨C, Q(diag(1/QT1n))T (diag(1/RT1m))RT⟩
(13)"
REFERENCES,0.20321361058601134,"Given (Q, R, T ) ∈LCa,b(r), sub-couplings Q and R are constrained by:"
REFERENCES,0.20415879017013233,"C1(a) := {(Q, R, T ) ∈R+ : Q1r = a},
C1(b) := {(Q, R, T ) ∈R+ : R1r = b},"
REFERENCES,0.2051039697542533,while the convex sets constraining the latent coupling matrix T are
REFERENCES,0.2060491493383743,"C2(gQ) := {(Q, R, T ) ∈R+ : T 1r = gQ},
C2(gR) := {(Q, R, T ) ∈R+ : T T1r = gR},"
REFERENCES,0.20699432892249528,"where gQ = QT1n and gR = RT1m as per Definition 3.1. Under these definitions, LCa,b(r) =
C1 ∩C2, where"
REFERENCES,0.20793950850661624,"C1 = C1(a) ∩C1(b)
and
C2 = C2(gQ) ∩C2(gR)
(14)"
REFERENCES,0.20888468809073724,"To solve (12), we separate the variables into two “blocks” of variables, (Q, R) and T , and perform
two block updates per iteration, as follows. Let (γk)k≥0 be a positive sequence of stepsizes. Suppose
we have (Qk, Rk, Tk) ∈C. We update the first variable block (Q, R) by taking a locally optimal
(mirror descent) update step, while the second variable block T is held fixed:"
REFERENCES,0.20982986767485823,"(Qk+1, Rk+1) ←
arg min
(Q,R):(Q,R,Tk)∈C1
⟨(Q, R), ∇(Q,R)LLC⟩+ 1"
REFERENCES,0.2107750472589792,"γk
KL((Q, R)∥(Qk, Rk))
(15)"
REFERENCES,0.21172022684310018,"Here, we slightly abused the notation by putting (Q, R) inside an inner product.
The triple
(Qk+1, Rk+1, Tk) produced by this update lies in C1. Next, we update T , the second variable
block, by taking another locally optimal (mirror descent) step, while the first variable block is held
fixed."
REFERENCES,0.21266540642722118,"Tk+1 ←
arg min
T :(Qk+1,Rk+1,T )∈C2
⟨T , ∇T LLC⟩+ 1"
REFERENCES,0.21361058601134217,"γk
KL(T ∥Tk).
(16)"
REFERENCES,0.21455576559546313,"By construction, the triple (Qk+1, Rk+1, Tk+1) ∈C2. However, because the set C1 only con-
strains the first variable block Qk+1, Rk+1, we have that (Qk+1, Rk+1, Tk+1) ∈C2, and hence
(Qk+1, Rk+1, Tk+1) ∈C. Thus, each iteration produces a feasible triple (Qk+1, Rk+1, Tk+1)
through a pair of locally optimal block updates."
REFERENCES,0.21550094517958412,"The LOT optimization problem Scetbon et al. (2021)
For comparison, recall the optimization
problem that is solved in the LOT framework:"
REFERENCES,0.21644612476370512,"min
(Q,R,g)∈e
C
LLOT,
(17)"
REFERENCES,0.21739130434782608,where the objective function LLOT is
REFERENCES,0.21833648393194707,"LLOT := ⟨C, Qdiag(1/g)RT⟩
(18)"
REFERENCES,0.21928166351606806,and where eC = eC1 ∩eC2 with:
REFERENCES,0.22022684310018903,"eC1 := {(Q, R, g) ∈Rn×r
+
× Rm×r
+
× (R∗
+)r : Q1r = a, R1r = b}
eC2 := {(Q, R, g) ∈Rn×r
+
× Rm×r
+
× Rr
+ : QT1n = g = RT1m}."
REFERENCES,0.22117202268431002,"Here, there are also three optimization variables (Q, R, g), but they are updated together in each
iteration of LOT. That is, given feasible (Qk, Rk, gk) ∈C, an iteration of LOT updates this triple via"
REFERENCES,0.222117202268431,"(Qk+1, Rk+1, gk+1) := arg min
(Q,R,g)∈e
C
⟨(Q, R, g), ∇(Q,R,g)LLOT⟩+ 1"
REFERENCES,0.22306238185255198,"γk
KL((Q, R, g)∥(Qk, Rk, gk)),"
REFERENCES,0.22400756143667297,"where (γk)k≥0 is again a positive sequence of stepsizes. Scetbon et al. (2021) then compute the
unconstrained argmin across all variables to yield a set of unconstrained kernels (KQ, KR, kg),
using Dykstra to jointly project the unconstrained update onto the intersection eC of the constraint sets."
REFERENCES,0.22495274102079396,"OT subroutine in FRLC
To see why we do not need Dykstra in the FRLC scheme, observe that
the update (15) of variables (Q, R) can be equivalently expressed as"
REFERENCES,0.22589792060491493,"(Qk+1, Rk+1) ←
arg min
Q,R : (Q,R,Tk)∈C1
⟨(Q, R), ∇(Q,R)LLC⟩+ 1"
REFERENCES,0.22684310018903592,"γk
KL((Q, R)∥(Qk, Rk)),"
REFERENCES,0.2277882797731569,"Thus, even though the pair (Q, R) is being updated at this step, solving for (Qk+1, Rk+1) above is
equivalent to updating each individually because Q and R do not share an inner marginal:"
REFERENCES,0.22873345935727787,"Qk+1 ←arg min
Q : Q1r=a
⟨Q, ∇QLLC⟩+ 1"
REFERENCES,0.22967863894139887,"γk
KL(Q∥Qk)
(19)"
REFERENCES,0.23062381852551986,"Rk+1 ←arg min
R : R1r=b
⟨R, ∇RLLC⟩+ 1"
REFERENCES,0.23156899810964082,"γk
KL(R∥Rk)
(20)"
REFERENCES,0.23251417769376181,"We choose to add the regularization τKL((QT1n, RT1m)∥(QT
k 1n, RT
k 1m)) to turn each update
here into an entropy-regularized semi-relaxed optimal transport problem, and to ensure β-smoothness:"
REFERENCES,0.2334593572778828,"Qk+1 ←arg min
Q : Q1r=a
⟨Q, ∇QLLC⟩+ 1"
REFERENCES,0.23440453686200377,"γk
KL(Q∥Qk) + τKL(QT1n∥QT
k 1n)
(21)"
REFERENCES,0.23534971644612476,"Rk+1 ←arg min
R : R1r=b
⟨R, ∇RLLC⟩+ 1"
REFERENCES,0.23629489603024575,"γk
KL(R∥Rk) + τKL(RT1m∥RT
k 1m)
(22)"
REFERENCES,0.23724007561436672,"After updating Q and R, the update on T then follows a similar form:"
REFERENCES,0.2381852551984877,"Tk+1 ←
arg min
T : (Qk+1,Rk+1,T )∈C2
⟨T , ∇T LLC⟩+ 1"
REFERENCES,0.2391304347826087,"γk
KL(T ∥Tk),
(23)"
REFERENCES,0.24007561436672967,"leading to balanced constraints on T of the form T T1r = QT
k+11n and T 1r = RT
k+11m, allowing
the problem to be solved by Sinkhorn."
REFERENCES,0.24102079395085066,"Importantly, there are no constraints in C1 involving both Q and R, which is what allows the
optimization to split in this way. If there were such constraints, we would have needed to use Dykstra
to update the pair (Q, R). Because our update scheme is equivalent to the three updates of individual
variables given in (21), (22), (23), we can solve for each update using optimal transport. As Q and R
are not required to match the inner marginals exactly, the OT problems associated to Q and R are
semi-relaxed by construction."
REFERENCES,0.24196597353497165,"The separation of our block updates into a step where (Q, R, Tk) ∈C1 and (Qk+1, Rk+1, T ) ∈C2
allows us to entirely remove the optimization over inner marginals gQ and gR, as done in all
previous works on low-rank optimal transport which optimize g explicitly as both a variable and a
constraint of the optimization Scetbon et al. (2021, 2023); Scetbon & Cuturi (2022). If one were to
introduce an extended loss in the style of previous works which adds gQ and gR as variables in the
form H(Q, R, T , gQ, gR) = ⟨Qdiag(1/gQ)T diag(1/gR)RT, C⟩F , one observes an equivalence
to simply taking a semi-relaxed projection."
REFERENCES,0.24291115311909262,"Lemma A.1. Define the function H(Q, R, T , gQ, gR) = ⟨C, Qdiag(1/gQ)T diag(1/gR)RT⟩F
and let LLC(Q, R, T ) be as in (13). One has the following equivalence:"
REFERENCES,0.2438563327032136,"min
gR∈∆r,gQ∈∆r,Q∈Πa,gQ,R∈Πb,gR
H(Q, R, Tk, gQ, gR) =
min
(Q,R,Tk)∈C1 LLC(Q, R, Tk)
(24)"
REFERENCES,0.2448015122873346,Thus the semi-relaxed projections yield locally optimal inner marginals.
REFERENCES,0.24574669187145556,"Proof. To see why this is true, notice that, so long as the outer marginals are tightly satisfied Q1r = a
and R1r = b for Q ≥0n×r, R ≥0m×r, one has
X"
REFERENCES,0.24669187145557656,"i
gR,i =
X"
REFERENCES,0.24763705103969755,"i
⟨RT
i,., 1m⟩=
X"
REFERENCES,0.2485822306238185,"i
⟨R.,i, 1m⟩=
X ℓ X"
REFERENCES,0.2495274102079395,"i
Rℓ,i =
X"
REFERENCES,0.2504725897920605,"ℓ
bℓ= 1 and
X"
REFERENCES,0.2514177693761815,"i
gQ,i =
X"
REFERENCES,0.2523629489603025,"i
⟨QT
i,., 1n⟩=
X"
REFERENCES,0.2533081285444234,"i
⟨Q.,i, 1m⟩=
X ℓ X"
REFERENCES,0.2542533081285444,"i
Qℓ,i =
X"
REFERENCES,0.2551984877126654,"ℓ
aℓ= 1"
REFERENCES,0.2561436672967864,"Therefore, the inner marginals g∗
Q = (Q∗)T1n and g∗
R = (R∗)T1m induced by the optimal Q∗, R∗"
REFERENCES,0.2570888468809074,"of the optimization problem on the right hand side satisfy the constraints g∗
R ∈∆r, g∗
Q ∈∆r on the
left hand side, so the two minimums coincide."
REFERENCES,0.2580340264650284,"This implies that an extra optimization for gQ and gR is unnecessary in a coordinate update which
alternates (Q, R) and T . If we did not perform a block-update, in the form of standard MD in
the objective described above, we would encounter some difficulty. In particular gQ and gR would
optimized with the constraint gQ ∈∆r and gR ∈∆r, and would concurrently constrain all of the
other variables as QT1n = gQ, T 1r = gQ, and RT1m = gR, T T1r = gR."
REFERENCES,0.2589792060491493,"B
Block-Coordinate steps for the OT sub-problems"
REFERENCES,0.2599243856332703,"We use a latent non-diagonal coupling instead of an inner diagonal coupling diag(g) of the form
of Forrow et al. (2019). This allows us to loosen the constraint that the inner marginals have to be
joined by a common coupling QT1n = RT1m = g. The fundamental advantage of this choice is
that we can decouple the convex-optimization problem for (Q, R, T ) entirely. One can simply solve
for the optimal Q and R independently, yield the associated inner marginals for each QT1n = gQ
and RT1m = gR, and then find the optimal T which links the two. This link is provided by the
aforementioned form of the problem, where:"
REFERENCES,0.2608695652173913,P = QXRT
REFERENCES,0.2618147448015123,"For Q, R in either the appropriate set of couplings or a relaxation thereof (which we will describe
shortly). X is related to T by:"
REFERENCES,0.2627599243856333,X = diag(1/gQ)T diag(1/gR)
REFERENCES,0.2637051039697543,"And, T ∈ΠgQ,gR consistently for all cases. As the semi-relaxed case is intermediate between
fully-relaxed and balanced, it has ideas which generalize to both directly. As such, we use it as the
leading example again. As in Scetbon et al. (2021), we take proximal-steps of the form:"
REFERENCES,0.2646502835538752,"min
ζ ⟨∇L(ζ) |ζk, ζ⟩F + 1"
REFERENCES,0.2655954631379962,"γk
KL(ζ∥K(k))"
REFERENCES,0.2665406427221172,"Where these steps are now in block-wise fashion on (Q, R) and T , rather than joint. One may
identify for each sub-factor in (Q, R) and T a linearized gradient as before, which yields a set of
objectives which each solve an independent optimal-transport for the sub-factors. In particular, we
have that:
⟨QXRT, C⟩F = Tr

QXRTCT
= ⟨CRXT, Q⟩F"
REFERENCES,0.2674858223062382,"⟨QXRT, C⟩F = Tr

QXRTCT
= ⟨CTQX, R⟩F"
REFERENCES,0.2684310018903592,"⟨QXRT, C⟩F = Tr

RTCTQX

= ⟨QTCR, X⟩F"
REFERENCES,0.26937618147448017,"A linearization in the left-slot of the inner product as ⟨Q, CRX(Qk)T⟩:= ⟨Q, CRXT⟩or
⟨CTQX(Rk), R⟩F := ⟨CTQX, R⟩F is common practice for quadratic problems. In this case, the
directional derivative of Q and R in the matrix-direction V are respectively:"
REFERENCES,0.27032136105860116,"D⟨CRXT, Q⟩F ◦(V ) = ⟨CRXT, V ⟩F =⇒∇QL = CRXT"
REFERENCES,0.2712665406427221,"D⟨CTQX, R⟩F ◦(V ) = ⟨CTQX, V ⟩F =⇒∇RL = CTQX"
REFERENCES,0.2722117202268431,"Without this linearization assumption on X, the full gradient may be evaluated as well. In particular,
we note that for diag−1(·) the matrix-to-vector extraction of the diagonal, the directional derivative
on Q is:"
REFERENCES,0.2731568998109641,"D⟨C, QXRT⟩F ◦V = ⟨CRXT, V ⟩F + ⟨C, QDXT ◦(V )RT⟩F
= ⟨CRXT −1ndiag−1((CRXT)TQdiag(1/gQ))T, V ⟩F"
REFERENCES,0.2741020793950851,"Thus, without the linearization assumption one may use product rule on X as an implicit function of
Q (resp. R) to take the total derivative:"
REFERENCES,0.27504725897920607,∇QLFRLC = CRXT −1ndiag−1((CRXT)TQdiag(1/gQ))T
REFERENCES,0.27599243856332706,"Likewise for R,"
REFERENCES,0.276937618147448,"D⟨C, QXRT⟩F ◦V = ⟨CTQX, V ⟩F + ⟨C, QDX ◦(V )RT⟩F
= ⟨CTQX −1mdiag−1(diag(1/gR)RTCTQX)T, V ⟩F ,"
REFERENCES,0.277882797731569,and so
REFERENCES,0.27882797731569,∇RLFRLC = CTQX −1mdiag−1(diag(1/gR)RTCTQX)T.
REFERENCES,0.27977315689981097,"Both the W and GW problem have these rank-one perturbations of the gradient from the derivative
with respect to X."
REFERENCES,0.28071833648393196,"Lastly, owing to the block-coordinate updates which fix (Q, R) preceding the update on T , the
derivative on T follows directly by chain rule on X:"
REFERENCES,0.28166351606805295,"D⟨QTCR, X⟩F ◦(V ) = ⟨QTCR, V ⟩F =⇒∇XL = QTCR"
REFERENCES,0.2826086956521739,∇T L = diag(1/gQ)QTCR diag(1/gR)
REFERENCES,0.2835538752362949,"Let (γk) be a sequence of step sizes and consider the first-order conditions required for the proximal
step as before. From these we have the updated proximal-step updates:"
REFERENCES,0.2844990548204159,"K(k)
Q ←min
Q ⟨CRXT, Q⟩F + 1"
REFERENCES,0.28544423440453687,"γk
KL(Qk∥K(k)
Q )"
REFERENCES,0.28638941398865786,"K(k)
R ←min
R ⟨CTQX, R⟩F + 1"
REFERENCES,0.28733459357277885,"γk
KL(Rk∥K(k)
R )"
REFERENCES,0.2882797731568998,"K(k)
T
←min
T ⟨QTCR, X⟩F + 1"
REFERENCES,0.2892249527410208,"γk
KL(Tk∥K(k)
T ),"
REFERENCES,0.29017013232514177,"with kernels K(k)
ζj , for j = 1, 2, 3 given by"
REFERENCES,0.29111531190926276,"K(k)
Q := Qk ⊙exp( −γkCRkXT
k )"
REFERENCES,0.29206049149338376,"K(k)
R := Rk ⊙exp( −γkCTQkXk )"
REFERENCES,0.29300567107750475,"K(k)
T
:= Tk ⊙exp( −γkdiag(g−1
Q )QTCRdiag(g−1
R ) )"
REFERENCES,0.2939508506616257,"Or, dropping the linearization assumption on (Q, R) the updates are: K(k)
ζj , for j = 1, 2 given by"
REFERENCES,0.2948960302457467,"K(k)
Q := Qk ⊙exp( −γk(CRkXT
k −1ndiag−1((CRkXT
k )TQkdiag(1/gQk))T) )"
REFERENCES,0.29584120982986767,"K(k)
R := Rk ⊙exp( −γk(CTQkXk −1mdiag−1(diag(1/gR)RT
k CTQkXk)T) )"
REFERENCES,0.29678638941398866,"The first projection is onto the set that satisfies the marginal constraint R1r = b. In particular,
following the discussion above, one has the coordinate-MD step:"
REFERENCES,0.29773156899810965,"min
(Q,R,T )
1
γk
KL ((Q, R, T ) ∥(KQ, KR, KT )) + τKL(Q1r∥a)"
REFERENCES,0.29867674858223064,"s.t.
R1r = b
(25)"
REFERENCES,0.2996219281663516,"As before, there is no difference from the previous case, where one takes the unconstrained projection
R = diag(b/KR1r)KR. To generalize this, we also consider adding a soft-constraint on the inner
marginal of R to be near that of the previous iteration. In particular, we consider the problem:"
REFERENCES,0.3005671077504726,"min
(Q,R,T )
1
γk
KL((Q, R, T )∥(KQ, KR, KT ))"
REFERENCES,0.30151228733459357,"+ τKL(Q1r∥a) + τKL(RT1m∥g(k−1)
R
≡RT
k−11m)
s.t.
R1r = b (26)"
REFERENCES,0.30245746691871456,"Which yields the relaxed solution of R = SRR-projection(KR, γk, τ, b, g(k−1)
R
) which generalizes
the original projection and is equivalent to it for τ = 0. This regularization is essential, as it ensures
β-smoothness of the objective. For Q, as the constraint on g is fully relaxed, the Lagrange multiplier
λ1 = 0 entirely, such that the problem 40 now becomes fully-unconstrained: inf
Q  1"
REFERENCES,0.30340264650283555,"γk
KL(Q∥KQ) + τKL(Q1r∥a)

(27)"
REFERENCES,0.30434782608695654,"To generalize this solution, we again consider adding a soft-regularization on the inner marginal of
Q, where we consider the alternate problem: inf
Q  1"
REFERENCES,0.3052930056710775,"γk
KL(Q∥KQ) + τKL(Q1r∥a) + τKL(QT1n∥g(k−1)
Q
≡QT
k−11n)

(28)"
REFERENCES,0.30623818525519847,"Which trivially recovers the original for τ = 0. This form has a solution given by an unbalanced
optimal transport with kernel KQ. We see these two, convex problems in sequence give independent
optimal solutions for Q and R as the two matrices are not required to share an inner marginal.
The last step is to link them via T , corresponding to the projection of (Qk, Rk, T ) onto the set
of valid rank-r couplings min(Qk,Rk,T )∈C LLC(Qk, Rk, T ) := minT ∈Π(gQ,gR) LLC(Qk, Rk, T ).
We verify in C that if T ∈ΠgQ=QT1n,gR=RT1m then P ∈Πa,b. As such, one does not require any
projection onto the intersection of convex sets, as done in Scetbon et al. (2021, 2023) via the Dykstra
projection algorithm Dykstra (1983). Alternating a coordinate-MD step in (Q, R) and a step on T ,
one not only minimizes the objective in an alternating fashion but remains in the feasible set without
the need for projection algorithms beyond Sinkhorn. As such, the final linking step is done via:"
REFERENCES,0.30718336483931946,"min
T
1
γk
KL(T ∥KT )"
REFERENCES,0.30812854442344045,"s.t.
T 1r = gQ, T T1r = gR
(29)"
REFERENCES,0.30907372400756145,"This formulation amounts to solving a balanced Sinkhorn problem on T with respect to the proximal
step kernel matrix."
REFERENCES,0.31001890359168244,"Algorithm 2
SRR-projection
(semi-relaxed OT, right marginal relaxed)"
REFERENCES,0.31096408317580343,"Input K, γ, τ, a, b, δ
u ←1n
v ←1r
repeat"
REFERENCES,0.31190926275992437,"˜u ←u
˜v ←v
u ←(a/Kv)"
REFERENCES,0.31285444234404536,"v ←
 
b/KTu
τ/(τ+γ−1)"
REFERENCES,0.31379962192816635,"until γ−1 max{∥log ˜u/u∥∞, ∥log ˜v/v∥∞} < δ
return diag(u)K diag(v)"
REFERENCES,0.31474480151228734,"Algorithm 3
U-projection
(unbalanced OT)"
REFERENCES,0.31568998109640833,"Input K, γ, τ, a, b, δ
u ←1n
v ←1r
repeat"
REFERENCES,0.3166351606805293,"˜u ←u
˜v ←v
u ←(a/Kv)τ/(τ+γ−1)"
REFERENCES,0.31758034026465026,"v ←
 
b/KTu
τ/(τ+γ−1)"
REFERENCES,0.31852551984877125,"until γ−1 max{∥log ˜u/u∥∞, ∥log ˜v/v∥∞} < δ
return diag(u)K diag(v)"
REFERENCES,0.31947069943289225,"C
The Latent Coupling Matrix"
REFERENCES,0.32041587901701324,"To solve the balanced form (and generalize the principle to the relaxed problems), we consider an
alternative parametrization of the inner matrix. In particular, previous works Scetbon et al. (2021,
2023) consider diag(1/g) to be the inner matrix, with marginals QT1n = RT1m = g to ensure that
the outer conditions Q1r = a and R1r = b hold. We instead relax the constraint that QT1n and
RT1m are equal, by allowing QT1n = gQ and RT1m = gR to vary arbitrarily, and considering a
non-diagonal inner matrix X ∈Rr×r in the place of diag(1/g) where we have the conditions:"
REFERENCES,0.32136105860113423,XgR = XRT1m = 1r
REFERENCES,0.3223062381852552,"And
XTgQ = XQT1n = 1r"
REFERENCES,0.32325141776937616,"Thus, if one considers the coupling formed as Pr = QXRT, one maintains that Pr ∈Πa,b from
the condition of C1, defined in the balanced case as in (14). We clearly have that:"
REFERENCES,0.32419659735349715,Pr1m = QXRT1m = Q1r = a
REFERENCES,0.32514177693761814,"And:
P T
r 1n = RXTQ1n = R1r = b"
REFERENCES,0.32608695652173914,"We first consider two approaches to optimizing for such a X, given that it is not a coupling. First, we
consider the appropriate proximal step for X"
REFERENCES,0.3270321361058601,"min
ζ ⟨∇L(ζ) |ξk, ζ⟩F + 1"
REFERENCES,0.3279773156899811,"γk
KL(ζ∥K(k))"
REFERENCES,0.32892249527410206,"We first note that the gradient of our loss with respect to X is given as QTCR. If one supposes that
X is invertible with X−1 = T , we have that for any such T :"
REFERENCES,0.32986767485822305,X−11r = T 1r = gR
REFERENCES,0.33081285444234404,"And
X−T 1r = T T1r = gQ
This implies that this inverse matrix T is a coupling such that T ∈ΠgR,gQ, which also suggests one
might be able to update it using Sinkhorn. In fact, being a density in Rr×r
+
it represents a transition
matrix between the latent r-dimensional variables. In particular, writing the proximal step in full, we
have:
min
T ⟨QTCR, T −1⟩F + 1"
REFERENCES,0.33175803402646503,"γk
KL(Tk∥K(k)
T )"
REFERENCES,0.332703213610586,"Noting the derivative D(X−1) ◦V = −X−1V X−1, we have from the first-order condition that:"
REFERENCES,0.333648393194707,−T −T QTCRT −T + 1
REFERENCES,0.33459357277882795,"γk
log"
REFERENCES,0.33553875236294894,"""
Tk
K(k)
T # = 0"
REFERENCES,0.33648393194706994,This implies the kernel matrix update:
REFERENCES,0.33742911153119093,"K(k)
T
= Tk ⊙exp{+γkT −T QTCRT −T }"
REFERENCES,0.3383742911153119,"Where one then takes the Sinkhorn projection J onto the set ΠgR,gQ as PΠgR,gQ (K(k)
T ) using the
Sinkhorn algorithm Cuturi (2013b). However, a more stable and inversion-free update exists which
ensures X remains positive by a diagonal re-scaling in the form introduced by Lin et al. (2021). In
particular, if one takes X = diag(1/gQ)T diag(1/gR), then"
REFERENCES,0.3393194706994329,XgR = diag(1/gQ)T diag(1/gR)gR = diag(1/gQ)T 1r = 1r
REFERENCES,0.34026465028355385,and likewise
REFERENCES,0.34120982986767484,XTgQ = diag(1/gR)T T diag(1/gQ)gQ = diag(1/gR)T T1r = 1r
REFERENCES,0.34215500945179583,"so that X necessarily satisfies XgR = 1r and XTgQ = 1r. Thus T 1r = gQ and T T1r = gR and
T ∈ΠgQ,gR. With analogous reasoning to before, one has a step for the coupling T in the form:"
REFERENCES,0.3431001890359168,"min
T ⟨QTCR, diag(1/gQ)T diag(1/gR)⟩F + 1"
REFERENCES,0.3440453686200378,"γk
KL(Tk∥K(k)
T )"
REFERENCES,0.3449905482041588,Which yields the kernel matrix:
REFERENCES,0.34593572778827975,"K(k)
T
= Tk ⊙exp{−γk diag(gQ)−1QTCR diag(gR)−1}"
REFERENCES,0.34688090737240074,"Which is likewise projected onto ΠgQ,gR using the Sinkhorn algorithm. From this T ∈ΠgQ,gR, one
takes X = diag(1/gQ)T diag(1/gR) as the inner matrix that corresponds the unequal marginals
gQ and gR which ensuring Pr ∈Πa,b."
REFERENCES,0.34782608695652173,"Algorithm 4
FRLC
(General marginal constraint low-rank optimal transport)"
REFERENCES,0.3487712665406427,"Input C, r, r2, a, b, τ, τ2, γ, δ, ε
Initialize gQ, gR = 1"
REFERENCES,0.3497164461247637,"r1r, 1"
REFERENCES,0.3506616257088847,"r2 1r2
Q0, R0, T0 ←Initialize-Couplings(a, b, gQ, gR)
if r = r2 then"
REFERENCES,0.3516068052930057,"X0 ←T −1
0
# Invertible case
else"
REFERENCES,0.35255198487712663,"X0 ←diag(1/QT
0 1n)T0 diag(1/RT
0 1m)
# General case
end if
while ∆((Qk, Rk, Tk), (Qk−1, Rk−1, Tk−1)) > ε do"
REFERENCES,0.3534971644612476,"∇Q ←CRkXT
k −1ndiag−1((CRkXT
k )TQkdiag(1/gQ))T"
REFERENCES,0.3544423440453686,"∇R ←CTQkXk −1mdiag−1(diag(1/gR)RT
k CTQkXk)T
γk ←γ/ max{∥∇Q∥∞, ∥∇R∥∞}
# ℓ∞-normalization of Scetbon & Cuturi (2022)
K(k)
Q , K(k)
R ←Qk ⊙exp( −γk∇Q ), Rk ⊙exp( −γk∇R )
if Balanced then"
REFERENCES,0.3553875236294896,"Qk ←SRR-projection(K(k)
Q , γk, τ, a, QT
k−11n, δ)
# Semi-relaxed OT"
REFERENCES,0.3563327032136106,"Rk ←SRR-projection(K(k)
R , γk, τ, b, RT
k−11m, δ)
# Semi-relaxed OT
else if Unbalanced then"
REFERENCES,0.3572778827977316,"Qk ←U-projection(K(k)
Q , γk, τ, a, QT
k−11n, δ)
# Unbalanced OT"
REFERENCES,0.35822306238185253,"Rk ←U-projection(K(k)
R , γk, τ2, b, RT
k−11m, δ)
# Unbalanced OT
else if Semi-Relaxed Left then"
REFERENCES,0.3591682419659735,"Qk ←U-projection(K(k)
Q , γk, τ, a, QT
k−11n, δ)
# Unbalanced OT"
REFERENCES,0.3601134215500945,"Rk ←SRR-projection(K(k)
R , γk, τ, b, RT
k−11m, δ)
# Semi-relaxed OT
else if Semi-Relaxed Right then"
REFERENCES,0.3610586011342155,"Qk ←SRR-projection(K(k)
Q , γk, τ, a, QT
k−11n, δ)
# Semi-relaxed OT"
REFERENCES,0.3620037807183365,"Rk ←U-projection(K(k)
R , γk, τ, b, RT
k−11m, δ)
# Unbalanced OT
end if
gQ, gR ←QT
k 1n, RT
k 1m
∇T = diag(gQ)−1QT
k CRk diag(gR)−1
γT = γ/∥∇T ∥∞
# ℓ∞-normalization
K(k)
T
←Tk ⊙exp{−γT ∇T }
Tk ←Sinkhorn(K(k)
T , gR, gQ, δ)
# Balanced OT
Xk ←diag(1/gQ)T diag(1/gR)
end while
Return Pr = QXRT"
REFERENCES,0.3629489603024575,"Algorithm 5
Sinkhorn Algorithm
(Cuturi (2013b), balanced OT)"
REFERENCES,0.3638941398865784,"Input K, a, b, δ
u ←1n
v ←1m
while ∥diag(u)Kv −a∥1 + ∥diag(v)KTu −b∥1 > δ do"
REFERENCES,0.3648393194706994,u(l+1) ←a/Kv(l)
REFERENCES,0.3657844990548204,"v(l+1) ←b/KTu(l+1)
end while
Return diag(u)K diag(v)"
REFERENCES,0.3667296786389414,"D
Gromov-Wasserstein (GW)"
REFERENCES,0.3676748582230624,"As defined in, the general Gromov-Wasserstein problem concerns a minimization of the energy:"
REFERENCES,0.3686200378071834,"QA,B(P ) =
X"
REFERENCES,0.3695652173913043,"i,j,k,l
(Aik −Bjl)2PijPkl
(30)"
REFERENCES,0.3705103969754253,"Where the minimization is over the set of all couplings Πa,b:"
REFERENCES,0.3714555765595463,"GW(µ, ν) :=
min
P ∈Πa,bQA,B(P )
(31)"
REFERENCES,0.3724007561436673,"We consider extending the semi-relaxed framework to the GW problem under the low-rank restriction
on P . This extension has thus far been considered for the balanced and unbalanced case in two
previous works Scetbon et al. (2023, 2022). In both of these works, the algorithms for the Wasserstein
case extend trivially to the Gromov-Wasserstein (GW) problem. In particular, each kernel with
variable ζ has an update of the form K ←ζ ⊙exp (−γk∇ζL(ζ)) for L(ζ) heretofore taken to
be the Wasserstein loss ⟨P (ζ), C⟩F where the coupling P = QXRT is interpreted as a function
of the low-rank sub-factor variables ζ ∈{Q, R, X}. Taking L := QA,B(P (ζ)) one can simply
extend the gradient through the GW-loss and directly use it in place of the Wasserstein gradient in the
update K ←ζ ⊙exp (−γk∇ζL(ζ)) for L(ζ) of each algorithm. The matrix-form of the GW-cost
is expressed as:"
REFERENCES,0.3733459357277883,"QA,B(P ) = 1T
mP TA⊙2P 1m + 1T
nP B⊙2P T1n −2⟨AP B, P ⟩"
REFERENCES,0.3742911153119093,"Which, using the constraints of C2 reduces the cost as a function of Q, R, X to:"
REFERENCES,0.3752362948960302,"QA,B(Q, R, X) = 1T
r QTA⊙2Q1r + 1T
r RTB⊙2R1r −2⟨QXRT, AQXRTB⟩F"
REFERENCES,0.3761814744801512,"∇QQA,B(Q, R, X) = 2A⊙2Q1r1T
r −4AQXRTBRXT"
REFERENCES,0.3771266540642722,"Which is proportional to ∇QQA,B(Q, R, X) ∝−4AQXRTBRXT for the balanced and right-
marginal semi-relaxed case. And:"
REFERENCES,0.3780718336483932,"∇RQA,B(Q, R, X) = 2B⊙2R1r1T
r −4BRXTQTAQX"
REFERENCES,0.3790170132325142,"Which likewise can be reduced in proportionality to ∇RQA,B(Q, R, g) ∝−4BRXTQTAQX
in the balanced and left-marginal semi-relaxed case. The gradients, as presented above, assume
a linearization in X ←Xk.
If one does not make this assumption and takes X(Q, R) =
diag(1/QT 1n)T diag(1/RT 1m), a rank-one perturbation must be added to the Q and R gradient
of the form:
∇(2)
Q = 41n diag−1(XRTB(QXRT)TAQ diag(1/gQ))T"
REFERENCES,0.3799621928166352,"∇(2)
R = 41m diag−1(XT QT AQXRTBR diag(1/gR))T"
REFERENCES,0.3809073724007561,"Analogously, for the gradient on T one can simply take the gradient with respect to X, and sub-
sequently T as X(T ) = diag(gQ)−1T diag(gR)−1. The gradient with respect to X is given
as:
∇XQA,B(Q, R, X) = −4QTAQXRTBR
And
thus,
the
directional
derivative
with
respect
to
X
in
the
direction
VX
=
diag(gQ)−1VT diag(gR)−1 and thus by the chain rule VT is:"
REFERENCES,0.3818525519848771,"DQA,B(Q, R, X) ◦(VX) = −4⟨QTAQXRTBR, VX⟩F
= −4⟨QTAQXRTBR, diag(1/gQ)VT diag(1/gR)⟩F"
REFERENCES,0.3827977315689981,So that the gradient with respect to the coupling matrix T is given as:
REFERENCES,0.3837429111531191,"∇T QA,B(Q, R, T ) = −4 diag(1/gQ)QTAQXRTBR diag(1/gR)"
REFERENCES,0.3846880907372401,"E
Convergence Analysis and Other Proofs"
REFERENCES,0.3856332703213611,"E.1
Convergence and Smoothness of the Objective"
REFERENCES,0.38657844990548207,"We show in Proposition 3.3 that directly applying the block-descent lemma of Beck & Tetruashvili
(2013) to the template of Ghadimi et al.’s proof Ghadimi et al. (2014) is sufficient to show the non-
asymptotic stationary convergence of a coordinate mirror descent procedure in Ghadimi’s criterion.
The non-asymptotic guarantee of Ghadimi et al. (2014) follows directly in the case of coordinate
mirror descent using Lemma E.1 and Lemma E.2 below. For completeness, we define all notation used,
describe the coordinate mirror descent algorithm in general, and discuss a few relevant preliminaries."
REFERENCES,0.387523629489603,"Suppose that the vector of n variables x ∈Rn is partitioned into p blocks, x = (x(1), . . . , x(p)),
where x(i) ∈Rni. Here, n1, . . . , np are positive integers summing to n. Following the notation of
Beck & Tetruashvili (2013); Nesterov (2012), we define matrices Ui ∈Rn×ni such that x(i) = UT
i x
for all i = 1, . . . , p. This also implies x = Pp
i=1 Uix(i). This allows us to define the vector of
partial derivatives corresponding to each block of variables x(i):"
REFERENCES,0.388468809073724,"∇if(x) := UT
i ∇f(x)."
REFERENCES,0.389413988657845,"In Beck & Tetruashvili (2013), the gradient of f is assumed to be block-coordinate-wise Lipschitz,
with Li the smoothness constant associated to the i-th block of variables: for all hi ∈Rni, one has"
REFERENCES,0.390359168241966,"∥∇if(x + Uihi) −∇if(x)∥≤Li∥hi∥,
(32)"
REFERENCES,0.391304347826087,"and for such functions we denote by L := maxi Li the (global) smoothness constant of ∇f. To be
clear, a smoothness constant associated to f is a Lipschitz constant of its gradient.
Lemma E.1 (Block descent lemma, Beck & Tetruashvili (2013), Lemma 3.2.). Suppose f ∈
C1(Rn, R) is a continuously differentiable function over Rn whose gradient is block-coordinatewise
Lipschitz (32) for Li the smoothness constant associated to the i-th block of variables x(i). Let u, v
be two vectors differing only in the i-th block: there exists hi ∈Rni such that v −u = Uihi. Then,"
REFERENCES,0.39224952741020797,"f(v) ≤f(u) + ⟨∇f(u), v −u⟩+ Li"
REFERENCES,0.3931947069943289,"2 ∥u −v∥2.
(33)"
REFERENCES,0.3941398865784499,"Lemma E.1 is central to adapting the proof of Theorem 1 of Ghadimi et al. (2014) to our case. Their
Theorem 1 concerns the non-asymptotic convergence of mirror descent for objectives of the form"
REFERENCES,0.3950850661625709,"min
x∈X f(x) + h(x),"
REFERENCES,0.3960302457466919,"where X is a closed, convex subset of Rn, f ∈C1(X, R) is a possibly non-convex objective, and
where h is an α-strongly convex function. Using notation similar to Ghadimi et al. (2014), we write
Φ(x) = f(x)+h(x). Additionally, Ghadimi et al. (2014) assume ∇f is L-Lipschitz for some L > 0."
REFERENCES,0.39697542533081287,"To unify our assumptions on f, we suppose that ∇f ∈C1(X, R) is block-coordinate Lipschitz (32)
with block Lipschitz constants (Li)p
i=1, and that X itself decomposes as a product X = Qp
i=1 Xi,
where each Xi is a closed convex set constraining the block variables x(i)."
REFERENCES,0.39792060491493386,"The proof of Ghadimi relies on β-smoothness of the objective in all variables. We show that
component-wise smoothness in each block is sufficient to achieve an analogous convergence result
for a coordinate mirror descent. To provide context for Proposition 3.3, we now describe in general
(1) mirror descent, and (2) block-coordinate mirror descent."
REFERENCES,0.3988657844990548,"We again follow the notation of Ghadimi et al. (2014). A function ω : X →R is a distance
generating function with modulus α > 0, with respect to the Euclidean norm ∥·∥, if ω is continuously
differentiable and strongly convex, so that"
REFERENCES,0.3998109640831758,"⟨x −z, ∇ω(x) −∇ω(z)⟩≥α∥x −z∥2,
for all x, z ∈X."
REFERENCES,0.4007561436672968,The prox-function (or Bregman divergence) associated with ω is then
REFERENCES,0.4017013232514178,"V (x, z) = ω(x) −ω(z) −⟨∇ω(z), x −z⟩,"
REFERENCES,0.40264650283553877,"and from this prox-function, γ > 0, and some g ∈Rn, we define the generalized projection"
REFERENCES,0.40359168241965976,"x+ := arg min
u∈X"
REFERENCES,0.4045368620037807,"
⟨g, u⟩+ 1"
REFERENCES,0.4054820415879017,"γ V (u, x) + h(u)

.
(34)"
REFERENCES,0.4064272211720227,"To describe the projected gradient descent algorithm (which coincides with mirror descent in our case
of interest), we first define the generalized projected gradient of Φ at x:"
REFERENCES,0.40737240075614367,"PX (x, g, γ) := 1"
REFERENCES,0.40831758034026466,γ (x −x+).
REFERENCES,0.40926275992438566,"The mirror descent (MD) algorithm is as follows: given initial point x0 ∈X, a total number of
iterations N, and positive stepsizes (γk)N
k=1, at step k, the (k + 1)-st iterate is computed via"
REFERENCES,0.4102079395085066,"xk+1 ←arg min
u∈X"
REFERENCES,0.4111531190926276,"
⟨∇f(xk), u⟩+ 1"
REFERENCES,0.4120982986767486,"γk
V (u, xk) + h(u)

."
REFERENCES,0.41304347826086957,"Among all iterates xk, the MD algorithm outputs the one at which the generalized projected gradient
is of least norm. Concretely, xR is the output of the MD algorithm, where"
REFERENCES,0.41398865784499056,"R := arg min
k=0,...,N
∥gX,k∥2,"
REFERENCES,0.41493383742911155,"and where gX,k is"
REFERENCES,0.4158790170132325,"gX,k := PX (xk, ∇f(xk), γk)."
REFERENCES,0.4168241965973535,"Having described the MD algorithm, let us consider a block-coordinate variant; we assume x admits
the block-coordinate structure described above. To simplify the presentation, we suppose that in
a given iteration k, the block variables are updated sequentially from i = 1, . . . , p. This leads to
doubly-indexed iterates (xi
k) with k = 1, . . . , N indexing each full iteration through all variables,
and i = 0, 1, . . . , p indexing the sub-iterations which update one block of variables at a time."
REFERENCES,0.41776937618147447,"The coordinate mirror descent (CMD) algorithm takes as input an initial point x0 ∈X, a number
of iterations N, and a sequence of positive stepsizes (γk,i)N,p
k=1,i=1. We set x0
0 = x0, and for
k = 0, . . . , N −1 and i = 1, . . . , p, we compute xi
k from xi−1
k
as follows:"
REFERENCES,0.41871455576559546,"xi
k(i) ←arg min
ui∈Xi"
REFERENCES,0.41965973534971646,"
⟨∇if(xi−1
k
), ui⟩+
1
γk,i
Vi(ui, UT
i xi−1
k
) + hi(ui)
"
REFERENCES,0.42060491493383745,"xi
k(j) ←xi−1
k
(j)
for j ̸= i"
REFERENCES,0.4215500945179584,"Here, we have assumed that V can be written as a composite function of the block variables,"
REFERENCES,0.4224952741020794,"V (x, z) = p
X"
REFERENCES,0.42344045368620037,"i=1
Vi(x(i), z(i)),"
REFERENCES,0.42438563327032136,"as is the case for the KL divergence. We also have assumed that h has this composite structure (as
with entropy):"
REFERENCES,0.42533081285444235,"h(x) = p
X"
REFERENCES,0.42627599243856334,"i=1
hi(x(i))."
REFERENCES,0.42722117202268434,"Lastly, for k = 0, . . . , N −1, we set x0
k+1 := xp
k. We define gX,k := (gX,k,1, . . . , gX,k,p) to be the
collection of block-wise differences, where by definition"
REFERENCES,0.4281663516068053,"gX,k,i = PXi(xi−1
k
, ∇if(xi−1
k
), γk,i) =
1
γk,i"
REFERENCES,0.42911153119092627," 
xi−1
k
−xi
k

= UT
i gX,k."
REFERENCES,0.43005671077504726,"The convergence criterion ∆we use in Proposition 3.3 is the one used by Ghadimi et al. (2014)
summed across blocks. In particular, the CMD algorithm returns iterate xR, where"
REFERENCES,0.43100189035916825,"R := arg min
k=0,...,N
∆(xk, xk−1),"
REFERENCES,0.43194706994328924,"∆(xk, xk−1) := ∥gX,k∥2 = p
X"
REFERENCES,0.43289224952741023,"i=1
∥gX,k,i∥2.
(35)"
REFERENCES,0.43383742911153117,"For Φ = f + h as above (f has global smoothness constant L), let Φ∗= arg minx∈X Φ(x), and
define"
REFERENCES,0.43478260869565216,"D :=
Φ(x0) −Φ∗ L"
REFERENCES,0.43572778827977315,"1/2
.
(36)"
REFERENCES,0.43667296786389415,"The other lemma used in the proof of Proposition 3.3 is as follows.
Lemma E.2 (Ghadimi et al. (2014), Lemma 1). Let x+ = arg minu∈X {⟨g, u⟩+ 1"
REFERENCES,0.43761814744801514,"γ V (u, x) + h(u)}
and PX (x, g, γ) = 1"
REFERENCES,0.43856332703213613,"γ (x −x+). Then for all x ∈Rn, all g ∈Rn, and γ > 0, one has:"
REFERENCES,0.43950850661625707,"⟨g, PX (x, g, γ)⟩≥α∥PX (x, g, γ)∥2 + 1"
REFERENCES,0.44045368620037806,γ (h(x+) −h(x)).
REFERENCES,0.44139886578449905,"Proposition E.3 (Proposition 3.3). Suppose one has f ∈C1(X, R) whose gradient is block-
coordinate Lipschitz, with block smoothness constants (Li)p
i=1, and a function h ∈C(X, R) which
is α-strongly convex. For Φ = f + h, suppose one performs a coordinate mirror descent on Φ
minimized over a product of closed convex sets X = Qp
i=1 Xi. Let the sub-iterates with respect to
the i-th block update be {xi
k}p
i=0 where xk := x0
k for k ∈[N] outer iterations. Then one has:"
REFERENCES,0.44234404536862004,"min
k ∆(xk, xk−1) ≤
D2L
N(α2/2L) = 2D2L2 Nα2 ,"
REFERENCES,0.44328922495274103,"where D is (36), L is the global smoothness constant of f, and convergence criterion ∆(xk, xk−1)
is given in (35). Above, the stepsizes γk,i in the coordinate mirror descent are γk,i := α/L."
REFERENCES,0.444234404536862,"Proof. As f satisfies the hypotheses of the block descent lemma, Lemma E.1, we apply (33) to
obtain:
f(xi
k) ≤f(xi−1
k
) + ⟨∇if(xi−1
k
), xi
k −xi−1
k
⟩+ Li"
REFERENCES,0.44517958412098296,"2 ∥xi
k −xi−1
k
∥2."
REFERENCES,0.44612476370510395,"Noting the definition gX,k,i =
1
γk,i
 
xi−1
k
−xi
k

, one has"
REFERENCES,0.44706994328922495,"f(xi
k) ≤f(xi−1
k
) −γk,i⟨∇if(xi−1
k
), gX,k,i⟩+ Li"
REFERENCES,0.44801512287334594,"2 γ2
k,i∥gX,k,i∥2."
REFERENCES,0.44896030245746693,"Lemma 1 of Ghadimi et al. (2014) (stated as Lemma E.2 above) applies identically through block-
wise optimality on Xi because gX,k,i = PXi(xi−1
k
, ∇if(xi−1
k
), γk,i). Thus for any value ∇if(xi−1
k
)
takes,"
REFERENCES,0.4499054820415879,"f(xi
k) ≤f(xi−1
k
) −

αγk,i∥gX,k,i∥2 + h(xi
k) −h(xi−1
k
)

+ Li"
REFERENCES,0.45085066162570886,"2 γ2
k,i∥gX,k,i∥2,"
REFERENCES,0.45179584120982985,"and thus,"
REFERENCES,0.45274102079395084,"f(xi
k) + h(xi
k) ≤f(xi−1
k
) + h(xi−1
k
) −

αγk,i −Li"
REFERENCES,0.45368620037807184,"2 γ2
k,i"
REFERENCES,0.4546313799621928,"
∥gX,k,i∥2."
REFERENCES,0.4555765595463138,"The right-hand side above only becomes larger, taking Li = L to be the global smoothness constant.
Introducing a sum over sub-iterates and total iterates, one has N,p
X"
REFERENCES,0.45652173913043476,"k,i
f(xi
k) + h(xi
k) ≤ N,p
X"
REFERENCES,0.45746691871455575,"k,i
f(xi−1
k
) + h(xi−1
k
) − N,p
X k,i"
REFERENCES,0.45841209829867674,"
αγk,i −L"
REFERENCES,0.45935727788279773,"2 γ2
k,i"
REFERENCES,0.4603024574669187,"
∥gX,k,i∥2, N,p
X"
REFERENCES,0.4612476370510397,"k,i
Φ(xi
k) ≤ N,p
X"
REFERENCES,0.4621928166351607,"k,i
Φ(xi−1
k
) − N,p
X k,i"
REFERENCES,0.46313799621928164,"
αγk,i −L"
REFERENCES,0.46408317580340264,"2 γ2
k,i"
REFERENCES,0.46502835538752363,"
∥gX,k,i∥2."
REFERENCES,0.4659735349716446,"Noting the end-point condition Φ(xp
k) = Φ(x0
k+1), one may cancel all intermediate terms:"
REFERENCES,0.4669187145557656,"Φ∗≤Φ(xN) ≤Φ(x0) − N,p
X k,i"
REFERENCES,0.4678638941398866,"
αγk,i −L"
REFERENCES,0.46880907372400754,"2 γ2
k,i"
REFERENCES,0.46975425330812853,"
∥gX,k,i∥2"
REFERENCES,0.4706994328922495,"Thus one finds the upper bound in terms of Φ(x0) and the minimum value Φ∗= minx∈X Φ(x) of Φ: N
X k p
X i"
REFERENCES,0.4716446124763705,"
αγk,i −L"
REFERENCES,0.4725897920604915,"2 γ2
k,i"
REFERENCES,0.4735349716446125,"
∥gX,k,i∥2 ≤Φ(x0) −Φ∗.
(37)"
REFERENCES,0.47448015122873344,"Taking γk,i = α/L as in Ghadimi et al. (2014), the bracketed term directly above becomes α2/2L,
and one has: N
X k  α2"
L,0.47542533081285443,2L
L,0.4763705103969754," 
min
k ∆(xk, xk−1)

= N
X k  α2"
L,0.4773156899810964,"2L  min
k p
X"
L,0.4782608695652174,"i
∥gX,k,i∥2
! ≤ N
X k  α2"
L,0.4792060491493384,"2L 
p
X"
L,0.48015122873345933,"i
∥gX,k,i∥2"
L,0.4810964083175803,"≤Φ(x0) −Φ∗= D2L,"
L,0.4820415879017013,"where D is as in (36), and where we used (37) to obtain the last line. Thus,"
L,0.4829867674858223,"min
k ∆(xk, xk−1) ≤
D2L
N(α2/2L) = 2D2L2 Nα2 ,"
L,0.4839319470699433,completing the proof.
L,0.4848771266540643,"Definition E.4 (Relative smoothness). Let β > 0 and let g ∈C1(Rn, R) be continuously differen-
tiable. Additionally, let ω be a distance generating function with associated prox-function V . The
function g is β-smooth relative to ω if the following holds:"
L,0.48582230623818523,"g(y) ≤g(x) + ⟨∇ω(x), x −y⟩+ βV (y, x)"
L,0.4867674858223062,"Proposition E.5. Let ϵ > 0 be a predefined error tolerance, r > 0 a small rank parameter, δ ∈(0, 1"
L,0.4877126654064272,"r)
a lower-bound parameter, and N the number of inner iterations required for the semi-relaxed
projection to converge for each iteration k as ∥QT1n −g(k−1)
Q
∥2 < ϵ = 1 N
  1"
L,0.4886578449905482,"r −δ

for g(0)
Q = 1"
L,0.4896030245746692,"r1r.
Then, the FRLC objective"
L,0.4905482041587902,"LLC(Q, R, T ) = ⟨Q diag(1/QT1n)T diag(1/RT1m)RT, C⟩F"
L,0.4914933837429111,"is component-wise smooth with respect to the variables Q, R, T with smoothness constants {βi}3
i=1
where βi = poly(∥C∥F , n, m, r, δ)."
L,0.4924385633270321,"Proof. The addition of a pair of regularizations on the inner marginals τKL(QT1n∥QT
k 1n) and
τKL(RT1m∥RT
k 1m) in 21 and 22 ensures that we may use standard results on relaxed optimal-
transport which bound how far the marginal QT
k 1n deviates across iterations."
L,0.4933837429111531,"In particular, for all ϵ > 0 there exists τ and N (number of iterations) sufficiently large, so that
∥RT1m −gR∥2
2 < ϵ and ∥QT1n −gQ∥2
2 < ϵ. In particular, Pham et al. (2020) shows that one
can attain convergence to any ϵ for the unbalanced problem in N = ˜O(m2/ϵ) iterations (hiding
logarithmic and τ-factors) for each sub-problem solving for Rk in Algorithm 4 and analogously
N = ˜O(n2/ϵ) for Q. Under the uniform initialization of gR, and after N iterations, we have:"
L,0.4943289224952741,"∥g(0)
R −g(N)
R ∥2 =

1
r 1r −g(N)
R 2
≤ N
X"
L,0.4952741020793951,"k=1
∥g(k)
R −g(k−1)
R
∥2."
L,0.4962192816635161,"With sufficiently large τ and N = ˜O(m2/ϵ) sub-iterations, one can guarantee that:"
L,0.497164461247637,"∥g(k)
R −g(k−1)
R
∥2 < ϵ = 1 N 1"
L,0.498109640831758,"r −δ

."
L,0.499054820415879,"This implies, for all iterations of the algorithm and all indices i, that"
L,0.5,"(gRk)i > δ,
and analogously,
(gQk)i > δ.
(38)"
L,0.500945179584121,"Thus, by adding the regularization on the inner marginal, one may guarantee a lower-bound on the
entries of gR and gQ. This is essential for demonstrating smoothness of the objective."
L,0.501890359168242,"First, we consider smoothness in Q. We note that the gradient in Q splits into two terms:"
L,0.502835538752363,"∇QLLC(Q, R, T ) = ∇(A)
Q LLC + ∇(B)
Q LLC"
L,0.503780718336484,= CRXT −1ndiag−1((CRXT)TQdiag(1/gQ))T
L,0.504725897920605,"= ∇(A)
Q LLC −1ndiag−1((∇(A)
Q LLC)TQdiag(1/gQ))T Where"
L,0.505671077504726,"∥∇QLLC(Qk+1, Rk, Tk) −∇QLLC(Qk, Rk, Tk)∥F"
L,0.5066162570888468,"≤∥∇(A)
Q LLC(Qk+1, Rk, Tk) −∇(A)
Q LLC(Qk, Rk, Tk)∥F"
L,0.5075614366729678,"+ ∥∇(B)
Q LLC(Qk+1, Rk, Tk) −∇(B)
Q LLC(Qk, Rk, Tk)∥F . (39)"
L,0.5085066162570888,"Starting with the first term on the right side of (39), one has:"
L,0.5094517958412098,"∥∇(A)
Q LLC(Qk+1, Rk, Tk) −∇(A)
Q LLC(Qk, Rk, Tk)∥F"
L,0.5103969754253308,"= ∥CRk(diag(1/gQk)Tkdiag(1/gRk))T −CRk(diag(1/gQk−1)Tkdiag(1/gRk))T∥F
≤∥diag(1/gRk)∥F ∥C∥F ∥Rk∥F ∥Tk∥F ∥diag(1/gQk) −diag(1/gQk−1)∥F ."
L,0.5113421550094518,"Note that ∥Rk∥2
F = P
i,j (Rk)2
i,j < P
i,j (Rk)i,j = 1, as Rk has marginals which sum to one. The
same bound holds for ∥Tk∥2
F , which is also a coupling. Invoking the lower-bound (38) of δ on the
entries of the inner marginals, and continuing from the above display,"
L,0.5122873345935728,"∥∇(A)
Q LLC(Qk+1, Rk, Tk) −∇(A)
Q LLC(Qk, Rk, Tk)∥F ≤∥C∥F"
L,0.5132325141776938,"δ
∥diag(1/gQk) −diag(1/gQk−1)∥F"
L,0.5141776937618148,= ∥C∥F
L,0.5151228733459358,"δ
∥diag(1/gQk−1)diag(1/gQk)(diag(gQk−1) −diag(gQk))∥F ≤∥C∥F"
L,0.5160680529300568,"δ3
∥diag(gQk−1) −diag(gQk)∥F ."
L,0.5170132325141777,"To further bound the right-hand side above, consider:"
L,0.5179584120982986,"∥diag(gQk) −diag(gQk−1)∥2
F = ∥QT
k 1n −QT
k−11n∥2
2 = r
X i=1  
r
X"
L,0.5189035916824196,"j=1
(Qk)i,j −(Qk−1)i,j   2"
L,0.5198487712665406,"While can easily be upper-bounded by an application of Jensen’s inequality as = r
X"
L,0.5207939508506616,"i=1
r2  
r
X j=1"
R,0.5217391304347826,"1
r"
R,0.5226843100189036,"
(Qk)i,j −(Qk−1)i,j

  2 ≤ r
X"
R,0.5236294896030246,"i=1
r2  
r
X j=1"
R,0.5245746691871456,"1
r"
R,0.5255198487712666,"
(Qk)i,j −(Qk−1)i,j
2
  = r r
X i=1 r
X j=1"
R,0.5264650283553876,"
(Qk)i,j −(Qk−1)i,j
2"
R,0.5274102079395085,"= r∥Qk −Qk−1∥2
F ."
R,0.5283553875236295,"Likewise, we have that ∥diag(gRk) −diag(gRk−1)∥2
F ≤r∥Rk −Rk−1∥2
F . Thus, it holds that ∥C∥F"
R,0.5293005671077504,"δ3
∥gQk −gQk−1∥2 ≤∥C∥F
√r
δ3
∥Qk −Qk−1∥F ."
R,0.5302457466918714,"Next, we focus on the ∇(B)
Q
term. Observe that"
R,0.5311909262759924,"∥1ndiag−1X∥2
F = Tr(1ndiag−1X)T(1ndiag−1X)"
R,0.5321361058601134,"= n∥diag−1X∥2
2 ≤n∥X∥2
F . Thus:"
R,0.5330812854442344,"∥∇(B)
Qk+1 −∇(B)
Qk ∥F ≤√n∥(∇(A)
Qk+1)TQk+1diag(1/gQk+1) −(∇(A)
Qk )TQkdiag(1/gQk)∥F"
R,0.5340264650283554,"Adding and subtracting terms in the norm and applying triangle inequality:
√n∥(∇(A)
Qk+1)TQk+1diag(1/gQk+1) −(∇(A)
Qk )TQk+1diag(1/gQk+1)"
R,0.5349716446124764,"+ (∇(A)
Qk )TQk+1diag(1/gQk+1) −(∇(A)
Qk )TQkdiag(1/gQk)∥F"
R,0.5359168241965974,"≤√n∥∇(A)
Qk+1 −∇(A)
Qk ∥F ∥Qk+1∥F ∥diag(1/gQk+1)∥F"
R,0.5368620037807184,"+ √n∥∇(A)
Qk ∥F ∥(Qk+1diag(1/gQk+1) −Qkdiag(1/gQk)∥F"
R,0.5378071833648393,"Invoking the lower-bound on the marginal, continuing from the above display, ≤
√n"
R,0.5387523629489603,"δ ∥∇(A)
Qk+1 −∇(A)
Qk ∥F"
R,0.5396975425330813,"+ √n∥∇(A)
Qk ∥F ∥(Qk+1diag(1/gQk+1) −Qkdiag(1/gQk)∥F"
R,0.5406427221172023,"Let us consider ∥∇(A)
Qk ∥F ≤∥Xk∥F ∥C∥F ∥Rk∥F ≤∥Xk∥F ∥C∥F . We have that:"
R,0.5415879017013232,"∥Xk∥2
F =
X i,j"
R,0.5425330812854442,"1
g2
(Qk)i
T 2
ij
1
g2
(Rk)j"
R,0.5434782608695652,"Where we always have that Tij ≤gQi and Tij ≤gRj by definition of T as a coupling. As such: ≤
X ij"
R,0.5444234404536862,"1
g2
Qi
(gQigRj)
1
g2
Rj
=
X ij"
GQI,0.5453686200378072,"1
gQi"
GRJ,0.5463137996219282,"1
gRj
=
D
g−1
Q g−T
R , 1m1T
r
E F ≤mr δ2"
GRJ,0.5472589792060492,"Thus ∥∇(A)
Qk ∥F ≤
√mr"
GRJ,0.5482041587901701,"δ
∥C∥F , and the bound above reduces to ≤
√n"
GRJ,0.5491493383742911,"δ ∥∇(A)
Qk+1 −∇(A)
Qk ∥F"
GRJ,0.5500945179584121,"+
√nmr"
GRJ,0.5510396975425331,"δ
∥C∥F ∥(Qk+1diag(1/gQk+1) −Qkdiag(1/gQk)∥F"
GRJ,0.5519848771266541,"Further bounding the last term in the norm
√nmr"
GRJ,0.552930056710775,"δ
∥C∥F ∥(Qk+1diag(1/gQk+1) −Qkdiag(1/gQk+1)"
GRJ,0.553875236294896,+ Qkdiag(1/gQk+1) −Qkdiag(1/gQk)∥F
GRJ,0.554820415879017,"≤
√nmr"
GRJ,0.555765595463138,"δ
∥C∥F (∥diag(1/gQk+1)∥F ∥Qk+1 −Qk∥F"
GRJ,0.556710775047259,+ ∥Qk∥F ∥diag(1/gQk+1) −diag(1/gQk)∥F )
GRJ,0.55765595463138,"≤
√nmr"
GRJ,0.558601134215501,"δ
∥C∥F 1"
GRJ,0.5595463137996219,"δ +
√r δ2"
GRJ,0.5604914933837429,"
∥Qk+1 −Qk∥F"
GRJ,0.5614366729678639,"Thus, the final bound on the ∇(B)
Q
term is: ≤
√n"
GRJ,0.5623818525519849,"δ ∥∇(A)
Qk+1 −∇(A)
Qk ∥F +
√nmr"
GRJ,0.5633270321361059,"δ
∥C∥F ∥(Qk+1diag(1/gQk+1) −Qkdiag(1/gQk)∥F"
GRJ,0.5642722117202268,"≤
∥C∥F
√nr
δ4
+
√nmr"
GRJ,0.5652173913043478,"δ2
∥C∥F"
GRJ,0.5661625708884688,"
1 +
√r δ"
GRJ,0.5671077504725898,"
∥Qk+1 −Qk∥F"
GRJ,0.5680529300567108,The total component-wise smoothness bound on Q is then
GRJ,0.5689981096408318,"∥∇QLLC(Qk+1, Rk, Tk) −∇QLLC(Qk, Rk, Tk)∥F ≤∥C∥F δ2 √nr"
GRJ,0.5699432892249527,"δ2
+ √nmr

1 +
√r δ"
GRJ,0.5708884688090737,"
+
√r δ"
GRJ,0.5718336483931947,"
∥Qk+1 −Qk∥F"
GRJ,0.5727788279773157,"Identical reasoning applies for ∇RLLC, where we similarly have the gradient split into two terms:"
GRJ,0.5737240075614367,"∇RLLC(Q, R, T ) = ∇(A)
R LLC∇(B)
R LLC
= CTQX −1mdiag−1(diag(1/gR)RTCTQX)T"
GRJ,0.5746691871455577,"= ∇(A)
R LLC −1mdiag−1(diag(1/gR)RT∇(A)
R LLC)T"
GRJ,0.5756143667296786,"As before, we may first show smoothness in ∇(A)
R
using the same steps as for Q"
GRJ,0.5765595463137996,"∥∇(A)
R LLC(Qk+1, Rk+1, Tk) −∇(A)
R LLC(Qk+1, Rk, Tk)∥F
= ∥CTQk+1diag(1/gQk+1)Tkdiag(1/gRk+1) −CTQk+1diag(1/gQk+1)Tkdiag(1/gRk)∥F
≤∥C∥F ∥diag(1/gQk+1)∥F ∥Qk+1∥F ∥Tk∥F ∥diag(1/gRk+1) −diag(1/gRk)∥F ≤∥C∥F"
GRJ,0.5775047258979206,"δ
∥diag(1/gRk+1) −diag(1/gRk)∥F ≤∥C∥F"
GRJ,0.5784499054820416,"δ3
∥gRk+1 −gRk∥2"
GRJ,0.5793950850661626,"≤∥C∥F
√r
δ3
∥Rk+1 −Rk∥F ."
GRJ,0.5803402646502835,"For ∇(B)
R , one may use the same reasoning as before to find:"
GRJ,0.5812854442344045,"∥∇(B)
R LLC(Qk+1, Rk+1, Tk) −∇(B)
R LLC(Qk+1, Rk, Tk)∥F"
GRJ,0.5822306238185255,"≤∥1m
h
diag−1(diag(1/gRk+1)RT
k+1∇(A)
Rk+1LLC) −diag−1(diag(1/gRk)RT
k ∇(A)
Rk LLC)
iT
∥F"
GRJ,0.5831758034026465,"≤√m∥diag(1/gRk+1)RT
k+1∇(A)
Rk+1LLC −diag(1/gRk)RT
k ∇(A)
Rk LLC∥F
As before, one may apply three rounds of triangle inequality inside the norm to bound this directly
in terms of ∥∇(A)
Rk+1 −∇(A)
Rk ∥F , ∥diag(1/Rk+1) −diag(1/Rk)∥F , and ∥Rk+1 −Rk∥F . Each of
these terms is smooth in R by the lower-bound argument, so that smoothness in R holds analogously
to Q. The remainder of the proof for smoothness in R thus follows identically to that of Q above."
GRJ,0.5841209829867675,"For T , the component-wise bound of"
GRJ,0.5850661625708885,"∥∇Tk+1LLC −∇TkLLC∥F = ∥Qk+1CRT
k+1 −Qk+1CRT
k+1∥2
F ≤LT ∥Tk+1 −Tk∥F
holds trivially for any LT > 0 as the gradient is uniquely determined by Q and R alone. Thus there
exist LQ, LR, LT > 0 as component-wise smoothness constants for LLC(Q, R, T )."
GRJ,0.5860113421550095,"We next prove Proposition 3.4, restated just below for convenience.
Proposition E.6 (Proposition 3.4). Consider the FRLC objective (8). The FRLC algorithm, Algorithm
4, yields β-smooth iterates for β = poly(∥C∥F , m, r, δ), where δ denotes the lower-bound on the
entries of gQ, gR. Consider the convergence metric of 3.3 adapted from Ghadimi et al. (2014), given
as:"
GRJ,0.5869565217391305,"∆k(xk, xk+1) = p
X"
GRJ,0.5879017013232514,"i=1
∥gX,k,i∥2 = 1 γ2
k"
GRJ,0.5888468809073724,"
∥Qk −Qk−1∥2
F + ∥Rk −Rk−1∥2
F + ∥Tk −Tk−1∥2
F
"
GRJ,0.5897920604914934,"for xk = (Qk, Tk, Rk). Define the gap to the optimal solution D as in (36), and let L = supi(Li)
to be the global smoothness constant across all components. Then for γk = α/L as defined in 3.3
the FRLC algorithm has the non-asymptotic stationary convergence guarantee that:"
GRJ,0.5907372400756143,"min
k∈1,..,N−1 ∆k ≤2D2L2 Nα2"
GRJ,0.5916824196597353,"Proof. The proof of the non-asymptotic stationary convergence of mirror descent of Ghadimi et al.
(2014), adapted for coordinate mirror descent using the block-descent lemma in 3.3, only requires
component-wise smoothness in (Q, R, T ). The proof of this for FRLC is given in ??, and the
guarantee follows directly for this value of L = max(LQ, LR, LT ) = poly(∥C∥F , m, r, δ)."
GRJ,0.5926275992438563,"Proposition E.7. Low-rank Approximation Error. Let SR-W⋆
r denote the optimal rank-r approxi-
mation for the semi-relaxed low-rank optimal transport problem, and let SR-W⋆denote the optimal
solution for the full-rank semi-relaxed optimal transport problem."
GRJ,0.5935727788279773,"Additionally, suppose cb = Pm
j=1 bj denotes the sum of the entries of the second marginal (cb = 1 if
a probability measure). Then we have the following upper-bound on the objective error:"
GRJ,0.5945179584120983,"|SR-W⋆
r(µb) −SR-W⋆(µb)| ≤cb"
GRJ,0.5954631379962193,"
max
p,q {Cpq} −min
p,q {Cpq}

ln (min{n, m}/(r −1))"
GRJ,0.5964083175803403,"We note that this bound also applies for the standard balanced optimal transport case, giving:"
GRJ,0.5973534971644613,"|W⋆
r(µa, µb) −W⋆(µa, µb)| ≤

max
p,q {Cpq} −min
p,q {Cpq}

ln (min{n, m}/(r −1))"
GRJ,0.5982986767485823,"and improves the previous bound of |W⋆
r(µa, µb) −W⋆(µa, µb)| ≤∥C∥∞ln (min{n, m}/(r −1))
as the distance matrix C contains only non-negative entries."
GRJ,0.5992438563327032,"Proof. We adapt the proof from Scetbon & Cuturi (2022) for the balanced case, which previously
gave the bound:"
GRJ,0.6001890359168242,"|W⋆
r(µa, µb) −W⋆(µa, µb)| ≤∥C∥∞ln (min{n, m}/(r −1))"
GRJ,0.6011342155009451,"In particular, for z = min{m, n}, there exists an optimal rank+(P ∗) ≤z where one may express
the optimal solution for the non-negative coupling matrix P as a sum of z rank-one, non-negative
outer products ˜qk ˜rT
k ≽0: P ∗= z
X"
GRJ,0.6020793950850661,"k=1
˜qk ˜rT
k = z
X"
GRJ,0.6030245746691871,"k=1
λkqkrT
k"
GRJ,0.6039697542533081,"Where we write this sum in terms of normalized vectors qk = ˜qk/∥˜qk∥1, rk = ˜rk/∥˜rk∥1, and
λk = ∥˜rk∥1∥˜qk∥1. Without any loss of generality, (λk)z
k=1 is ordered in terms of decreasing value
such that λ1 ≥λ2 ≥... ≥λz. Note that we have a fixed constraint for the sum of the entries of P
for the semi-relaxed case, assuming b is a general positive measure, as 1T
nP 1m =
 
P T1n
T 1m =
bT1m = Pm
j=1 bj := cb (where cb = 1 if b is chosen to be a probability measure, i.e. in the balanced
case). Moreover, it is simple to observe for these ordered values that λk ≤(cb/k). As in Scetbon &
Cuturi (2022), define the weighted average of the bottom z −r + 1 vectors of the decomposition to
be:"
GRJ,0.6049149338374291,"αr =
Pz
i=r λiqi
Pz
i=r λi"
GRJ,0.6058601134215501,"βr =
Pz
i=r λiri
Pz
i=r λi"
GRJ,0.6068052930056711,"And take the rank-r approximation using the optimal r −1 vectors of OPT and this weighted average
of the bottom to be: ˜Pr = r−1
X"
GRJ,0.6077504725897921,"i=1
λiqirT
i + z
X"
GRJ,0.6086956521739131,"i=r
λi !"
GRJ,0.6096408317580341,"αrβT
r"
GRJ,0.610586011342155,"Where, by the assumption that P ∗∈Πb is feasible:"
GRJ,0.611531190926276,"˜P T
r 1n = r−1
X"
GRJ,0.6124763705103969,"i=1
λiriqT
i 1n + z
X"
GRJ,0.6134215500945179,"i=r
λi !"
GRJ,0.6143667296786389,"βrαT
r 1n = r−1
X"
GRJ,0.6153119092627599,"i=1
λiri + z
X"
GRJ,0.6162570888468809,"i=r
λi ! βr = z
X"
GRJ,0.6172022684310019,"i=1
λiri = b"
GRJ,0.6181474480151229,"Thus ˜Pr ∈Πb,r is a feasible rank-r solution by feasibility of P ∗. One can verify that if P ∗∈Πa,b,
then ˜Pr1m = a and the solution is again feasible. From this, we observe that the difference between
this solution and P ∗is an upper-bound to the difference between P ∗and the optimal rank-r solution:"
GRJ,0.6190926275992439,"|SR-W⋆
r(µa, µb) −SR-W⋆(µa, µb)| = |⟨P ∗
r , C⟩F −⟨P ∗, C⟩F | ≤⟨˜Pr, C⟩F −⟨P ∗, C⟩F ="
GRJ,0.6200378071833649,"*r−1
X"
GRJ,0.6209829867674859,"i=1
λiqirT
i + z
X"
GRJ,0.6219281663516069,"i=r
λi !"
GRJ,0.6228733459357277,"αrβT
r − z
X"
GRJ,0.6238185255198487,"i=1
λiqirT
i , C + F = * z
X"
GRJ,0.6247637051039697,"i=r
λi !"
GRJ,0.6257088846880907,"αrβT
r − z
X"
GRJ,0.6266540642722117,"i=r
λiqirT
i , C + F"
GRJ,0.6275992438563327,"Noting that αr, βr and qi, ri are unit normalized positive vectors, the sum of the entries of the outer
product 1T
nqirT
i 1m = 1, and likewise for αrβT
r . Thus, continuing from the above display: = ( z
X"
GRJ,0.6285444234404537,"i=r
λi)⟨αrβT
r , C⟩F − z
X"
GRJ,0.6294896030245747,"i=r
λi⟨qirT
i , C⟩F ≤( z
X"
GRJ,0.6304347826086957,"i=r
λi)⟨αrβT
r , C⟩F −( z
X"
GRJ,0.6313799621928167,"i=r
λi) min
p,q {Cpq}"
GRJ,0.6323251417769377,"≤

max
p,q {Cpq} −min
p,q {Cpq}

z
X"
GRJ,0.6332703213610587,"i=r
λi ≤cb"
GRJ,0.6342155009451795,"
max
p,q {Cpq} −min
p,q {Cpq}

z
X i=r 1 i ≤cb"
GRJ,0.6351606805293005,"
max
p,q {Cpq} −min
p,q {Cpq}

ln (z/(r −1))"
GRJ,0.6361058601134215,"Concluding the proof. As discussed, this directly applies to the balanced case (for cb = 1)."
GRJ,0.6370510396975425,"F
Initialization"
GRJ,0.6379962192816635,"We propose a new initialization of the sub-couplings Q, R, T for the LC-factorization. Algorithm 6
generates a random full-rank initial condition in the set of couplings Πa,b which still satisfies the
marginal constraints. It accomplishes this by sampling random matrices which are full-rank and
applying the Sinkhorn algorithm to each of them. Scetbon et al. (2021) proposed an initialization
which represents an improvement over the rank-1 product measure which is rank-2. Follow-up work
proposed initialization using k-means Scetbon & Cuturi (2022). However, this assumes the previous
diagonal factorization and is thus not application for generating a latent coupling which may be
non-diagonal, non-square, and with two distinct inner marginals. Our initialization is tailored to the
LC-factorization, is effective, and has a full-rank guarantee. In particular, higher-rank initializations
may exhibit better convergence properties by allowing the gradient to explore a larger set of directions
immediately in the optimization. This initialization is given in Algorithm 6."
GRJ,0.6389413988657845,"Algorithm 6
Initialize-Couplings"
GRJ,0.6398865784499055,"Input a ∈∆n, b ∈∆m, gQ ∈∆r, gR ∈∆r
CQ ∼[0, 1]n×r, CR ∼[0, 1]m×r, CT ∼[0, 1]r×r"
GRJ,0.6408317580340265,"KQ ←eCQ, KR ←eCR, KT ←eCT
Q ←Sinkhorn(KQ, a, gQ)
R ←Sinkhorn(KR, b, gR)
T ←Sinkhorn(KT , gQ = QT1n, gR = RT1m)
Return (Q, R, T )"
GRJ,0.6417769376181475,"Proposition F.1. Suppose one samples an initial condition on the optimal transport coupling
using Algorithm 6, where we assume Cij ∼Unif(0, 1) such that P(rank(C) < min{n, m}) = 0.
Additionally, suppose that a, b > 0 holds elementwise for both marginals a ∈∆n, b ∈∆r. Then the
elementwise exponential exp{C} (or exp{−C}) has full-rank and the return Sinkhorn(e−C, a, b)
has full-rank."
GRJ,0.6427221172022685,"Proof. It is established that a random matrix C ∼[0, 1]n×m has full-rank with probability one.
For K = exp{C}, it holds that the matrix must be entry-wise positive with Kij ≥0. If columns
C.,i ̸= C.,j then clearly C⊙k
.,i ̸= C⊙k
.,j , and if C.,i, C.,j ≽0 and are independent remain so under
element-wise powers. One may easily show this by contrapositive. Suppose there exist constants
c1, c2 such that:"
GRJ,0.6436672967863895,"c1C⊙k
.,i + c2C⊙k
.,j = 0"
GRJ,0.6446124763705104,"As C.,i, C.,j ≽0, without loss of generality one may assume c1 > 0 and c2 < 0. Then:"
GRJ,0.6455576559546313,"c1C⊙k
.,i = c11 ⊙C⊙k
.,i = −c21 ⊙C⊙k
.,j = −c2C⊙k
.,j
=⇒

−c1 c2"
GRJ,0.6465028355387523,"1/k
1 = c1 = C.,j"
GRJ,0.6474480151228733,"C.,i
So clearly one has that C.,j −cC.,i = 0 for c > 0. This implies the columns C.,j and C.,i are
dependent. Thus it is clear that elementwise powers of entrywise positive independent vectors
preserve independence. The same principle extends trivially to exponentiation of the columns,
where if one assumes by contradiction that c1eC.,i + c2eC.,j = 0 for c1 > 0, c2 < 0, one finds
log

−c1 c2"
GRJ,0.6483931947069943,"
1 = C.,j −C.,i. Without loss of generality, assume 0 < −c2 ≤c1 and C.,j > C.,i > 0,"
GRJ,0.6493383742911153,"so that δ = log

−c1 c2"
GRJ,0.6502835538752363,"
≥0. Then, considering constants q1, q2:"
GRJ,0.6512287334593573,"q2C.,j + q1C.,i = (q1 + q2)C.,i + δq21 = 0
Assuming C.,i has greater than one unique entry, which we assume as the entries are sampled densely
in R, the two vectors are dependent if and only if q1 = −q2 and δ = 0, implying C.,i = C.,j.
Thus, for the set of independent column vectors of C, given as {C.,i}m
i=1, the set {eC.,i}m
i=1, is
also linearly independent. This holds analogously for the row vectors. As C is full-rank and
span({C.,i}) = Rmin{m,n}, we have that span(K) = Rmin{m,n} as K = eC = P∞
k=0
C⊙k"
GRJ,0.6521739130434783,"k!
(analogously e−C) and remains full-rank."
GRJ,0.6531190926275993,"Sinkhorn expresses each variable as
X = diag(u)K diag(v)
where rank(X) = rank(diag(u)K diag(v)). As Cuturi (2013b) updates the vectors u ←a/Kv
and v ←b/KTu from u0 = 1n and v0 = 1m, if a, b > 0 holds element-wise, one has that
u, v > 0 elementwise as well. Then, one has that null diag(v) = {0} and null diag(u) = {0},
implying that rank(diag(u)K diag(v)) = rank(K) = min{n, m}."
GRJ,0.6540642722117203,"Thus, our initialization returns a random coupling matrix X ∈Πa,b of full-rank."
GRJ,0.6550094517958412,"In the next proposition, we show that one can analytically solve for the block-optimal weights g for
the factorization of the coupling matrix P as P = Q diag(1/g)RT Forrow et al. (2019); Scetbon
et al. (2021).
Proposition F.2. For the minimization problem expressed as
min
g∈∆r⟨Q diag(1/g)RT, C⟩F"
GRJ,0.6559546313799622,One has the closed-form minimizer of g∗defined entrywise as:
GRJ,0.6568998109640832,"g∗
i =
√ωi
Pr
j=1
√ωj"
GRJ,0.6578449905482041,"For ω = diag−1(QTCR), when ω ≥0 holds entrywise."
GRJ,0.6587901701323251,"Proof. As ω ≥0, we consider the simplex condition Pr
j=1 gj = 1 alone. Writing out the Lagrangian
associated to our objective, with λ ∈R our equality-condition dual variable, we have:"
GRJ,0.6597353497164461,"L(g, λ) = ⟨Q diag(1/g)RT, C⟩F + λ(1 −
X j
gj)"
GRJ,0.6606805293005671,Let us consider a rewriting of the inner product term:
GRJ,0.6616257088846881,"⟨Q diag(1/g)RT, C⟩F = n
X i=1 m
X"
GRJ,0.6625708884688091,"j=1
Cij r
X"
GRJ,0.6635160680529301,"k=1
Qik  1 gk"
GRJ,0.6644612476370511,"
RT
kj = r
X k=1  1 gk 
n
X i=1 m
X"
GRJ,0.665406427221172,"j=1
QT
kiCijRjk = r
X k=1  1 gk"
GRJ,0.666351606805293,"  
QTCR
 k,k = r
X k=1 ωk"
GRJ,0.667296786389414,"gk
= ωT (1/g)"
GRJ,0.668241965973535,"Where division is interpreted element-wise in the last line. Thus, one can interpret the problem
as minimizing the weighted sum of reciprocals of a density. As a result, we can simplify our
Lagrangian’s Froebenius inner product to a vector dot-product as:"
GRJ,0.6691871455576559,"L(g, λ) = ωT (1/g) −λ(1 − r
X"
GRJ,0.6701323251417769,"j=1
gj)"
GRJ,0.6710775047258979,"Thus, the first order condition tells us that the value of the coupling weight gj is related to λ as:"
GRJ,0.6720226843100189,"∂gjL(g, λ) = −ωj"
GRJ,0.6729678638941399,"g2
j
+ λ = 0 =⇒gj =
rωj λ"
GRJ,0.6739130434782609,"And by relying on the summation condition on the probability density g, yields the Langrange
multiplier as
r
X"
GRJ,0.6748582230623819,"j=1
gj = 1 = r
X j=1 rωj λ"
GRJ,0.6758034026465028,so that one finds
GRJ,0.6767485822306238,"1 =
1
√ λ r
X j=1"
GRJ,0.6776937618147448,"√ωj =⇒λ =  
r
X j=1 √ωj   2"
GRJ,0.6786389413988658,Plugging our Lagrange-multiplier into the above expression yields:
GRJ,0.6795841209829868,"gj =
rωj"
GRJ,0.6805293005671077,"λ =
s
ωj
 Pr
i=1
√ωi
2 =
√ωj
Pr
i=1
√ωi"
GRJ,0.6814744801512287,"As the Hessian ∇2
gL = diag( ω"
GRJ,0.6824196597353497,"g3 ) ≽0, we conclude that this value of g indeed minimizes the loss
over ∆r."
GRJ,0.6833648393194707,"G
Alternating updates on the dual variables"
GRJ,0.6843100189035917,For the problem:
GRJ,0.6852551984877127,"inf
(Q,Rk,gk)∈C1∩C2  1"
GRJ,0.6862003780718336,"γk
KL(Q∥KQ) + τKL(Q1r∥a) −λT
1 QT1n"
GRJ,0.6871455576559546,"
(40)"
GRJ,0.6880907372400756,"one can find a simple set of semi-relaxed updates for the coupling matrix. We note the primal-dual
relationship of Sinkhorn, Q = diag(eγkf1)KQ diag(eγkh1), and consider the entry-wise first-order
condition required for the sub-coupling Q:"
GRJ,0.6890359168241966,"0 = γ−1
k
log"
GRJ,0.6899810964083176,"Qij
(KQ)ij !"
GRJ,0.6909262759924386,"+ τ log
⟨Qi,., 1r⟩ ai"
GRJ,0.6918714555765595,"
−λ1,i"
GRJ,0.6928166351606805,"=⇒log Qij = τγk log

ai
⟨Qi,., 1r⟩"
GRJ,0.6937618147448015,"
+ log (KQ)ij −λ1jγk (41) Thus: Qij ="
GRJ,0.6947069943289225,"ai
QT
i,.1r"
GRJ,0.6956521739130435,"!τγk
(KQ)ij e−λ1jγk"
GRJ,0.6965973534971645,"And in matrix-form, this yields:"
GRJ,0.6975425330812854,"Q = diag
 a Q1r"
GRJ,0.6984877126654064,"τγk
KQ diag(e−γkλ1)"
GRJ,0.6994328922495274,= diag(eγkf1)KQ diag(eγkh1)
GRJ,0.7003780718336484,"And expanding the Q1r term explicitly, noting that X diag(v)1 = Xv, we have:"
GRJ,0.7013232514177694,"diag
 a Q1r"
GRJ,0.7022684310018904,"τγk
KQ diag(e−γkλ1)"
GRJ,0.7032136105860114,"= diag

a
diag(eγkf1)KQ diag(eγkh1)1r"
GRJ,0.7041587901701323,"τγk
KQ diag(e−γkλ1)"
GRJ,0.7051039697542533,"= diag

a
eγkf1 ⊙KQeγkh1"
GRJ,0.7060491493383743,"τγk
KQ diag(e−γkλ1)"
GRJ,0.7069943289224953,"Thus, we identify eγkh1 = e−γkλ1 as the right dual vector, and identify the following relationship in
terms of the left dual vector:

a
eγkf1 ⊙KQeγkh1"
GRJ,0.7079395085066162,"τγk
= eγkf1 =⇒eγkf1 =

a
KQeγkh1"
GRJ,0.7088846880907372,"
τ
τ+1/γk"
GRJ,0.7098298676748582,"From 40, the condition that (Q, Rk, gk) ∈C1 ∩C2 implies that QT1m = gk := g. As such, we find
that:
QT1m = diag(eγkh1)KT
Q diag(eγkf1)1m = diag(eγkh1)KT
Qeγkf1 = g"
GRJ,0.7107750472589792,Implying an update for eγkh1 in the form:
GRJ,0.7117202268431002,eγkh1 =
GRJ,0.7126654064272212,"g
KT
Qeγkf1 !"
GRJ,0.7136105860113422,"Analogous reasoning applies for a relaxation of the other marginal, yielding the SRR-projection and
SRL-projection (i.e. semi-relaxed OT)."
GRJ,0.7145557655954632,"Algorithm 7
SRL-projection
(semi-relaxed OT, left marginal relaxed)"
GRJ,0.7155009451795841,"Input K, γ, τ, a, b, δ
u ←1n
v ←1r
repeat"
GRJ,0.7164461247637051,"˜u ←u
˜v ←v
u ←(a/Kv)τ/(τ+γ−1)"
GRJ,0.717391304347826,"v ←
 
b/KTu
"
GRJ,0.718336483931947,"until γ−1 max{∥log ˜u/u∥∞, ∥log ˜v/v∥∞} < δ
return diag(u)K diag(v)"
GRJ,0.719281663516068,"H
Discussion of Complexity"
GRJ,0.720226843100189,"For (Q, R, T ) ∈Rn×r1
+
× Rm×r2
+
× Rr1×r2
+
, the space complexity O(nr1 + r1r2 + mr2) is linear
if the ranks r1, r2 = o(1) are taken to be small constants. The time-complexity of Algorithm 4 is
O(BLr2(n + m)) for B the number of inner Sinkhorn iterations, L the number of mirror-descent
steps, n, m the number of samples in the first and second dataset, and r = max{r1, r2, d} for
r1, r2 the ranks of the latent coupling and d the rank of the factorized distance matrix C (generally
chosen to be a constant near r1, r2). Each matrix-multiplication is of max order (n + m)r2, which
happens a constant number of times in the computation of each gradient ∇i, and for the respective
Sinkhorn matrix-vector multiplications Kv and KTu. The L outer steps follow from the mirror-
descent convergence rate and the number of iterations B required for each projection follow from the
convergence of Sinkhorn. In particular, for ε a fixed error tolerance and η the entropy constant, one
finds a ±εD approximation for D the diameter of the data in B = poly(1/ηε) iterations using the
Sinkhorn algorithm Charikar et al. (2023); Cuturi (2013a)."
GRJ,0.72117202268431,"I
Review of Background Material"
GRJ,0.722117202268431,"I.1
Low-Rank Approximation of Pairwise Distance Matrices"
GRJ,0.723062381852552,"As mentioned previously, works such as Charikar et al. (2023) have developed algorithms with linear
O((n + m)1+o(1)poly(1/ϵ)) time-complexity and O((n + m)d) space-complexity for sketching
the optimal transport cost value. Recent works on low-rank factorization of the optimal transport
coupling matrix P (the matrix associated to the coupling γ ∈Π(µ, ν)) Scetbon & Cuturi (2022);
Scetbon et al. (2023, 2021) have achieved per-iteration time-complexities of O(T(n + m)dr) for
some constant non-negative rank r ≥1, d the dimension of the metric space, and T the number of
iterations. By the JL-lemma one can simply embed the points in dimension d = O(log (nm)/ϵ2)
while preserving pairwise distances, however, currently no proofs exist which offer the number of
iterations T until convergence to some tolerance ε. This is partially due to how recent these works
are, and also to the non-convexity of the objective which is sensitive to initial conditions Scetbon et al.
(2021). However, the space complexity of the algorithm is O((n + m)dr), which is noteworthy for
being linear in the number of points and avoids storing the potentially intractable O(nm) coupling
matrix P . To accomplish this, however, these works rely on a low-rank approximation of the pairwise
distance matrix. A number of works by Indyk and Woodruff have concerned algorithms for finding
low-rank approximations for such distance matrices. A seminal work Bakshi & Woodruff (2018)
developed an algorithm which, given two point sets {xi}n
i=1 and {yj}m
j=1 in some metric space
X, finds a rank r approximation in O((n + m)1+γpoly(r, 1/ε)) for γ > 0 an arbitrarily small
constant and ε > 0 an error parameter. A more recent work Indyk et al. (2019) improves on this
one, by reading a sample-optimal O((n + m)r/ε) entries of the input matrix with a run-time which
removes dependence on γ that is merely O((n + m)poly(r, 1/ε)). This algorithm is used by all
of the low-rank optimal transport works. These works, by finding a low-rank approximation to the
coupling matrix P ≈ABT ∈Rn×m
+
due to space limitations on the coupling, necessarily cannot
store the full distance matrix C ∈Rn×m
+
of the same size in memory either. As such, it must also
be approximated as C ≈V U before input to the algorithm, where we necessarily require very
effective approximations of U and V to tolerate the additional source of error from coarse-graining
the distance matrix to be low-rank. As such, we present some of the details and algorithm of Indyk
et al. (2019) as an essential component of the existing low-rank optimal transport solvers. We begin
by summarizing the main theorems in Indyk et al. (2019), which provide an algorithm (upper-bound)
on the low-rank distance-matrix approximation problem and a lower-bound on the number of entries
which must be read.
Theorem I.1. Indyk et al. (2019) There is a randomized algorithm that, given a distance matrix
C ∈Rn×m, reads O((n+m)r/ε) entries of C, runs in time ˜O(n+m)·poly(r, 1/ε)1 and computes
low-rank factors V ∈Rn×r, U ∈Rr×m that with probability 0.99 satisfy:"
GRJ,0.724007561436673,"∥C −V U∥2
F ≤∥C −Cr∥2
F + ε∥C∥2
F
(42)"
GRJ,0.724952741020794,For Cr the optimal rank-r approximation of C.
GRJ,0.725897920604915,"This is a remarkable result, especially in light of the next theorem."
GRJ,0.7268431001890359,"Theorem I.2. Indyk et al. (2019) Let r ≤m ≤n and ε > 0 such that r/ε = O(min{m, n1/3}).
Any randomized and possibly adaptive algorithm that given a distance matrix C ∈Rn×m computes
V ∈Rn×r, U ∈Rr×m satisfying ∥C −V U∥2
F ≤∥C −Cr∥2
F +ε∥C∥2
F must read Ω((n+m)r/ε)
entries of C in expectation. This lower bound also holds for symmetric distance matrices C ∈Sn."
GRJ,0.7277882797731569,"The lower-bound follows from a difficult argument which involves constructing a hard distribution
over distance matrices, involving the use of random matrix theory. The upper-bound, however, follows
relatively straightforwardly from a number of previous algorithms and their associated guarantees,
along with the algorithm presented below. We introduce the algorithm and also offer a proof of I.1
for completeness."
GRJ,0.7287334593572778,1Where ˜O(·) hides poly-log factors.
GRJ,0.7296786389413988,"Algorithm 8
Low-Rank approximation for distance matrix C"
GRJ,0.7306238185255198,"Input point sets {xi}n
i=1, {yj}M
j=1 in metric space X and metric d
Pick indices i∗∈[n], j∗∈[m] uniformly at random
for i = 1 to n do"
GRJ,0.7315689981096408,"Update sample probability pi = d(xi, yj∗)2 + d(xi∗, yj∗)2 + 1"
GRJ,0.7325141776937618,"m
Pm
j=1 d(xi∗, yj)2"
GRJ,0.7334593572778828,"end for
Sample O(r/ε) rows Ci,. ∼Categorical

pi
P i pi "
GRJ,0.7344045368620038,"Compute U using Frieze et al. (2004)
Compute V using Chen & Price (2017)
return V , U"
GRJ,0.7353497164461248,"This algorithm relies on two previous works, whose main results we summarize here."
GRJ,0.7362948960302458,"Theorem I.3. Frieze et al. (2004) Let C ∈Rn×m. For a sample of O(r/ε) rows according to
a distribution p ∈∆n which satisfies pi ≥Ω(1)∥Ci,.∥2
2/∥C∥2
F for i ∈[n]. Then in O(mr/ϵ +
poly(r, 1/ε)) time one may compute a matrix U ∈Rr×m from this sample which satisfies:"
GRJ,0.7372400756143668,"∥C −CU TU∥2
F ≤∥C −Ck∥2
F + ε∥C∥2
F"
GRJ,0.7381852551984878,With probability 0.99.
GRJ,0.7391304347826086,"Thus, to compute the first low-rank factor U, we need to ensure the pi generated from the algorithm
satisfies this requirement and offer the (short) proof below."
GRJ,0.7400756143667296,"Proof. First, it is helpful to note d(x, y)2 ≤(d(x, z) + d(z, y))2 = d(x, z)2 + 2d(x, z)d(y, z) +
d(y, z)2 ≤2(d(x, z)2 + d(y, z)2). Where in the last step one uses AM-GM where Q"
GRJ,0.7410207939508506,"i a1/n
i
≤ P"
GRJ,0.7419659735349716,"i ai
n
.
Rewriting the norm of row i we have:"
GRJ,0.7429111531190926,"∥Ci,.∥2
2 = m
X"
GRJ,0.7438563327032136,"j=1
d(xi, yj)2 ≤2 m
X"
GRJ,0.7448015122873346,"j=1
d(xi, yj∗)2 + d(yj∗, yj)2"
GRJ,0.7457466918714556,"= 2md(xi, yj∗)2 + 2 m
X"
GRJ,0.7466918714555766,"j=1
d(yj∗, yj)2"
GRJ,0.7476370510396976,"≤2md(xi, yj∗)2 + 4 m
X"
GRJ,0.7485822306238186,"j=1
d(yj∗, xi∗)2 + d(yj, xi∗)2"
GRJ,0.7495274102079396,"= 2md(xi, yj∗)2 + 4md(yj∗, xi∗)2 + 4 m
X"
GRJ,0.7504725897920604,"j=1
d(yj, xi∗)2 = 4m  1"
GRJ,0.7514177693761814,"2d(xi, yj∗)2 + d(yj∗, xi∗)2 + 1 m m
X"
GRJ,0.7523629489603024,"j=1
d(yj, xi∗)2 "
GRJ,0.7533081285444234,≤4mpi
GRJ,0.7542533081285444,"As we have the re-normalization pi ←
pi
P"
GRJ,0.7551984877126654,"i pi before sampling, we need to consider the value of the
expectation E[P"
GRJ,0.7561436672967864,"i pi] to conclude. E "" n
X"
GRJ,0.7570888468809074,"i=1
pi # = n
X i=1
E "
GRJ,0.7580340264650284,"d(xi, yj∗)2 + d(xi∗, yj∗)2 + 1 m m
X"
GRJ,0.7589792060491494,"j=1
d(xi∗, yj)2   = n
X"
GRJ,0.7599243856332704,"i=1
Ej∗∼[m]

d(xi, yj∗)2
+ Ei∗∼[n],j∗∼[m]

d(xi∗, yj∗)2 + 1 m m
X"
GRJ,0.7608695652173914,"j=1
Ei∗∼[n]

d(xi∗, yj)2 = n
X i=1"
M,0.7618147448015122,"1
m m
X"
M,0.7627599243856332,"j=1
d(xi, yj) + n n
X i=1 m
X j=1"
M,0.7637051039697542,"1
nmd(xi, yj) + n m m
X j=1"
N,0.7646502835538752,"1
n n
X"
N,0.7655954631379962,"i=1
d(xi, yj) = 3 m n
X i=1 m
X"
N,0.7665406427221172,"j=1
d(xi, yj) = 3"
N,0.7674858223062382,"m∥C∥2
F"
N,0.7684310018903592,"By an application of Markov’s inequality we have that: P   n
X"
N,0.7693761814744802,"i=1
pi !−1"
N,0.7703213610586012,"≥
3∥C∥2
F
mδ −1 = P "" n
X"
N,0.7712665406427222,"i=1
pi ≤3∥C∥2
F
mδ #"
N,0.7722117202268431,"≥1 −E[Pn
i=1 pi]"
N,0.7731568998109641,"3∥C∥2
F
mδ
= 1 −δ"
N,0.774102079395085,Thus with probability 1 −δ we have:
N,0.775047258979206,"pi
Pn
i′=1 pi′ ≥
mpiδ
3∥C∥2
F
=
4mpiδ
12∥C∥2
F
≥∥Ci,.∥2
2δ
12∥C∥2
F
= Ω(δ)∥Ci,.∥2
2
∥C∥2
F"
N,0.775992438563327,"This indicates the algorithm presented has probabilities with an appropriate bound for using the
algorithm of Frieze et al. (2004) to sample the O(r/ε) rows of C and generate a rank-r factor U."
N,0.776937618147448,"To conclude the result of Indyk et al. (2019) requires reference to an additional work which solves a
regression problem for V given C and U."
N,0.777882797731569,"Theorem I.4. Chen & Price (2017) There is a randomized algorithm A, given matrices C ∈Rn×m,
U ∈Rr×m reads only O(r/ε) columns of C with time-complexity O(mr) + poly(r/ε) and returns
V ∈Rn×r which satisfies"
N,0.77882797731569,"∥C −V U∥2
F ≤(1 + ε)
min
Z∈Rn×r∥C −ZU∥2
F"
N,0.779773156899811,with probability 0.99.
N,0.780718336483932,"Thus, using the result of Chen and Price Chen & Price (2017), one may find a satisfying V easily for
a fixed U, C. In particular, Indyk et al. (2019) concludes by tying together the low-rank distance
matrix algorithm and the guarantees of the algorithms from Frieze et al. (2004) Chen & Price (2017)
as follows"
N,0.781663516068053,"∥C −V U∥2
F ≤(1 + ε) min
Z ∥C −ZU∥2
F"
N,0.782608695652174,"≤(1 + ε)∥C −CU TU∥2
F
≤(1 + ε)(∥C −Cr∥2
F + ε∥C∥2
F )"
N,0.7835538752362949,"= ∥C −Cr∥2
F + ε∥C −Cr∥2
F + (1 + ε)ε∥C∥2
F
≤∥C −Cr∥2
F + (1 + 2ε)ε∥C∥2
F ≤∥C −Cr∥2
F + ˜ε∥C∥2
F"
N,0.7844990548204159,"Which achieves the bound up to a constant scaling of ε and shows the result of Indyk et al. (2019).
We next investigate low-rank optimal transport solvers, which assume the result and algorithm of
Indyk et al. (2019) to tractably scale to massive datasets."
N,0.7854442344045368,"J
Connection of Optimal Transport to Projection Problems"
N,0.7863894139886578,"Before discussing works which have address the problem of finding low-rank decompositions of
the coupling matrix P , we discuss a few relevant preliminaries. The Bregman-divergence of some
function F(x), defined by the first-order Taylor expansion of F:
DF (x, y) = F(x) −F(y) + ∇F(y)T (x −y)
For the negative entropy function −H(ξ), this corresponds to the KL-divergence KL(ζ | ξ). We
introduce the iterative-Bregman projection algorithm in the context of the KL-divergence owing to
the direct connection with entropically-regularized optimal transport.
Definition J.1. Iterative Bregman ProjectionsBregman (1967)"
N,0.7873345935727788,"Suppose C = ∩L
l=1Cl is an intersection of closed convex sets {Cl}L
l=1. For n > L, let the indexing be
L-periodic as Cn := Cn mod L. Suppose we want to find a minimizer ζ of the KL-divergence with
some positive vector ξ ∈Rn×m
+
such that ζ ∈C. This is to say, we hope to solve the problem of
minimizing a distance subject to the condition that one remains in this intersection of convex sets:
min
ζ∈C KL(ζ∥ξ)"
N,0.7882797731568998,"Where the projection of ξ onto the set C is denoted by
ζ∗= arg min
ζ∈C
KL(ζ∥ξ) := PKL
C
(ξ)"
N,0.7892249527410208,"Supposing that each Cl forms a quotient space Cl = V/U for a subspace U ⊂V , defined as
V/U = {v + U | v ∈V }2, the iterative Bregman projection algorithm alternates projections onto
each set Cn as
ζ(n) ←PKL
Cn (ζ(n−1))
(43)
starting from ζ(0) = ξ."
N,0.7901701323251418,"One may show Bregman (1967) the convergence of Bregman projections to the unique minimizer in
C, ζ∗, where we have the guarantee that ζ(n) →ζ∗as n →∞. These iterative projections only have
convergence guarantees when the constraint sets are quotient spaces–this is clearly not the case for
the constraints of the optimal transport LP, and a few more notions are required.
Definition J.2. Dykstra’s AlgorithmDykstra (1983)"
N,0.7911153119092628,"Given a point x0 ∈E for E a Euclidean space 3, to find the unique point in C = ∩L
l=1Cl for closed,
convex sets Cl that minimize the distance to x0 as
x∗= arg min
x∈C
∥x −x0∥2"
N,0.7920604914933838,"One may initialize the residuals q−(L−1) = ... = q0 = 0 and apply the algorithm:
xn = PCn(xn−1 + qn−1)
qn = (xn−1 −xn) + qn−L
Where Cn := Cn mod L as before and PCn denotes the projection operator onto the convex set Cn."
N,0.7930056710775047,"One can note that Dykstra’s algorithm for projections onto intersections of convex sets no longer
relies on the assumption that the set is a quotient space, and applies in the case that it is merely
closed under convex-combinations. To generalize Dykstra’s for more general functions that than the
ℓ2-norm, one may define the projection with respect to the Bregman divergence of a cost function
F and define the projection by the minimization: PCn := arg minx DF (x, y). It was proven in
Bauschke & Lewis (2000) that the generalized form of Dykstra’s iterations are given as:
xn = PCn (∇F ∗(∇F(xn−1) + qn−1))
qn = (∇F(xn−1) −∇F(xn)) + qn−L
For F ∗denoting the Fenchel-conjugate of F, which we define later in connection to the low-rank
dual problem. Notably, Bauschke & Lewis (2000) also provided guarantees of convergence to the
optimal solution which extend to the case that F is the negative entropy and DF the KL-divergence.
These constitute Dykstra’s algorithm with cyclic Bregman projections, and project a point to the
closest point in the intersection of convex sets C = ∩L
l=1Cl for an arbitrary cost function F and its
associated Bregman-divergence DF ."
N,0.7939508506616257,"2This is to say, x, y ∈Cl =⇒c1x + c2y ∈V/U for any c1, c2 ∈R.
3e.g. R, Rd, Rn×m, etc."
N,0.7948960302457467,"J.1
Dykstra’s algorithm with cyclic Bregman projections"
N,0.7958412098298677,"As such, we can see the updates for F being the negative entropy and DF the KL-divergence. Without
proof, the conjugate of the negative entropy is simply given as F ∗(ζ) = exp{ζ −1}. Thus, for the
minimization problem:
ζ∗= arg min
ζ∈C
KL(ζ∥ξ) := PKL
C
(ξ)"
N,0.7967863894139886,"Letting log q−(L−1) = ... = log q0 = 0, one may combine the two algorithms above to solve this
minimization using generalized Dykstra’s iterations. In particular, we have:"
N,0.7977315689981096,"ζ(n) = PCn

∇F ∗(∇F(ζ(n−1)) + log qn−1)
"
N,0.7986767485822306,"= PCn

exp

∇F(ζ(n−1)) + log qn−1 −1
"
N,0.7996219281663516,"= PCn

exp

log ζ(n−1) + 1 + log qn−1 −1
"
N,0.8005671077504726,"= PCn

ζ(n−1) ⊙qn−1
 And:"
N,0.8015122873345936,log qn = (∇F(ζ(n−1)) −∇F(ζ(n))) + log qn−N
N,0.8024574669187146,"=

log ζ(n−1) + 1 −(log ζ(n) + 1) + log qn−L
"
N,0.8034026465028355,"= log

qn−L ⊙ζ(n−1) ζ(n) "
N,0.8043478260869565,"So that:
ζ(n) ←PCn

ζ(n−1) ⊙qn−1

(44)"
N,0.8052930056710775,qn ←qn−L ⊙ζ(n−1)
N,0.8062381852551985,"ζ(n)
(45)"
N,0.8071833648393195,"Where division is interpreted to be element-wise, ⊙refers to the Hadamard product, and the logarithm
is applied elementwise."
N,0.8081285444234405,"J.2
Connection to Sinkhorn distances"
N,0.8090737240075614,"Interestingly, the Sinkhorn algorithm described in Algorithm 5 can be alternatively derived in the
context of Bregman iterations Benamou et al. (2015) as a minimization of the form:"
N,0.8100189035916824,"Wϵ(µ, ν) = ϵ
min
P ∈Π(µ,ν) KL(P ∥ξ)"
N,0.8109640831758034,"Where ξ is the kernel ξ = e−C/ϵ and Π(µ, ν) = C1 ∩C2 for the convex constraint sets C1 = {P :
P 1m = a} and C2 = {P : P T1n = b}. To cast this into the Bregman-projection framework, one
alternates between the two updates:"
N,0.8119092627599244,P (l) = PC1(P (l−1))
N,0.8128544423440454,P (l+1) = PC2(P (l))
N,0.8137996219281664,Where for the first projection one has the following first-order KKT condition:
N,0.8147448015122873,"∇

ϵKL(P ∥P (l−1)) + λT(P 1m −a)

= ϵ log

P
P (l−1)"
N,0.8156899810964083,"
+ λ1T
m = 0"
N,0.8166351606805293,=⇒P = diag(e−λ/ϵ)P (l−1)
N,0.8175803402646503,"With the constraint of C1 that P 1m = a, this implies diag(a/P (l−1)1m) = diag(e−λ/ϵ) and
recovers the first update of Sinkhorn P (l) ←diag(a/P (l−1)1m)P (l−1). An analogous argument
gives the second, where all iterates satisfy P (l) = diag(u(l))e−C/ϵ diag(v(l)) for u, v as defined in
Algorithm 5."
N,0.8185255198487713,"0
20
40
60
80
100
Iterations 0.25 0.30 0.35 0.40 0.45"
N,0.8194706994328923,"⟨C, P⟩F"
N,0.8204158790170132,"Rank-2
Full-rank"
N,0.8213610586011342,"Figure 5: Transport cost ⟨C, P ⟩F against number of iterations for FRLC with rank 200 on the
synthetic dataset of two moons and eight Gaussians. Smooth convergence is observed for both rank-2
and full-rank random initialization."
N,0.8223062381852552,"K
Additional Simulations"
N,0.8232514177693762,"We tested on two additional synthetic datasets, both used as benchmarking datasets in Scetbon et al.
(2021). We follow exactly the parameters provided in Scetbon et al. (2021) to simulate these datasets.
For the first one, we simulated n = m = 10, 000 points from two Gaussian mixtures in 2D (Fig. 6).
The first Gaussian mixture is a mixture of three Gaussian distributions with means (0, 0), (0, 1), (1, 1)
respectively. The mixture proportion is 1"
N,0.8241965973534972,"3 and the covariance is 0.05 × identity for each Gaussian. The
second Gaussian mixture is a mixture of two Gaussian distributions with means (0.5, 0.5), (−0.5, 0.5)
respectively. The mixture proportions is 1"
N,0.8251417769376181,2 and the covariance is 0.05 × identity for each Gaussian.
N,0.8260869565217391,"For the second dataset, we simulated n = m = 5, 000 points from two Gaussian mixtures
in 10D. The first Gaussian mixture is a mixture of three Gaussian distributions with means
(0, 0, 0, · · · , 0), (0, 1, 0, · · · , 0), (1, 1, 0, · · · , 0) respectively. The mixture proportions is 1"
AND THE,0.8270321361058601,"3 and the
covariance is 0.05 × identity for each Gaussian. The second Gaussian mixture is a mixture of
two Gaussian distributions with means (0.5, 0.5, 0, · · · , 0), (−0.5, 0.5, 0, · · · , 0) respectively. The
mixture proportions is 1"
AND THE,0.8279773156899811,2 and the covariance is 0.05 × identity for each Gaussian.
AND THE,0.8289224952741021,"For each dataset, we repeat the same procedure as in § 4.1, running FRLC and LOT with Euclidean
distance as cost to find a low-rank coupling matrix between the two Gaussian mixtures with rank
between 50 and 200. We observe the same pattern as Fig. 2b. FRLC obtains lower transport cost with
increasing rank, and achieves lower cost for each rank than LOT under all initializations (Fig. 23c,
Fig. 7)."
AND THE,0.8298676748582231,"L
Graph Partitioning"
AND THE,0.8308128544423441,"L.1
Evaluation on a Graph Partitioning Task"
AND THE,0.831758034026465,"We next evaluate FRLC on an unsupervised graph partitioning (node clustering) problem described by
Chowdhury & Needham (2021). Specifically, given a graph G = (V, E) of n nodes, we represent the
graph as G = (A, h), where A ∈Rn×n encodes the intra-graph node relationship (e.g. adjacency
matrix) and h ∈∆n is a uniform measure. We cluster the nodes of G by estimating a GW coupling"
AND THE,0.832703213610586,"Figure 6: Plot of the two simulated mixtures of Gaussians in 2D, following the same parameters as
Scetbon et al. (2021)."
AND THE,0.833648393194707,"60
80
100
120
140
160
180
200
Rank 0.165 0.170 0.175 0.180 0.185 0.190 0.195"
AND THE,0.834593572778828,"⟨C, P⟩F"
AND THE,0.8355387523629489,FRLC (rank-2)
AND THE,0.8364839319470699,FRLC (full-rank random initialization)
AND THE,0.8374291115311909,LOT (rank-2)
AND THE,0.8383742911153119,LOT (default initialization)
AND THE,0.8393194706994329,LOT (k-means)
AND THE,0.8402646502835539,"± 1 σ (Full-rank, FRLC)"
AND THE,0.8412098298676749,"Figure 7: Transport cost ⟨C, P ⟩F achieved by LOT Scetbon et al. (2021) and FRLC across different
ranks and different initializations on the Wasserstein problem on the synthetic dataset of two mixtures
of Gaussians in 2D."
AND THE,0.8421550094517959,"Method
Dataset
Time in seconds
Cost Value
∥P 1n −a∥2
∥P T 1m −b∥2
(CPU)
⟨C, P ⟩F
FRLC
2-moons,
0.379
0.207
1.56E-5
6.1E-18
LOT
8-Gaussians
0.751
0.210
1.90E-5
1.89E-5"
AND THE,0.8431001890359168,"FRLC
Gaussian mixture
0.354
0.178
1.05E-5
7.0E-18
LOT
(2D)
0.735
0.181
1.65E-5
1.70E-5"
AND THE,0.8440453686200378,"FRLC
Gaussian mixture
0.323
0.294
2.48E-6
8.9E-18
LOT
(10D)
0.677
0.307
1.39E-5
1.46E-5
Table 2: Runtime of FRLC and LOT on the synthetic datasets of 1000 samples, as well as
cost value ⟨C, P ⟩F and marginal tightness for context.
This was done with the FRLC set-
ting max_inneriters_balanced=1000, max_inneriters_relaxed=50, min_iter=7 and rank
r = 100. This time excludes the extra time incurred for ott-jax problem setup and includes the
setup time for FRLC."
AND THE,0.8449905482041588,"between G and a smaller graph G = (B, h), where B ∈Rm×m represents the relationship between
each of the m clusters and h ∈∆m is the proportion of nodes of G in each cluster. Without a priori
knowledge, h is set to be uniform and B is set as the identity matrix. Vincent-Cuaz et al. (2022)
notes that instead of solving a balanced GW problem, semi-relaxed GW with the right marginal h
relaxed learns h from data and leads to more accurate node clustering."
AND THE,0.8459357277882797,"Dataset
GWL
FRLC
SpecGWL
SpecFRLC
SR-GW
SR-GW"
AND THE,0.8468809073724007,"Wikipedia
sym, raw
0.314
0.387
0.372
0.444
(1998 nodes, 2700 edges)
sym, noisy
0.250
0.361
0.293
0.400
asym, raw
0.263
0.276
0.194
0.304
asym, noisy
0.208
0.201
0.141
0.177"
AND THE,0.8478260869565217,"EU-email
sym, raw
0.434
0.464
0.009
0.040
(1005 nodes, 25571 edges)
sym, noisy
0.392
0.422
0.009
0.014
asym, raw
0.388
0.398
0.012
0.028
asym, noisy
0.385
0.348
0.008
0.012"
AND THE,0.8487712665406427,"Amazon
raw
0.322
0.338
0.505
0.479
(1501 nodes, 4626 edges)
noisy
0.274
0.257
0.438
0.453"
AND THE,0.8497164461247637,"Village
raw
0.531
0.710
0.553
0.579
(1991 nodes, 8423 edges)
noisy
0.413
0.536
0.397
0.829
Table 3: Performance (measured using Adjusted Mutual Information (AMI)) in graph partitioning for
full-rank OT algorithms GWL, SpecGWL and full-rank semi-relaxed FRLC. The top performing
method for each dataset is highlighted in bold."
AND THE,0.8506616257088847,"We benchmark FRLC for semi-relaxed GW on four real-world graphs: a Wikipedia hyperlink
network with 15 webpage categories Yang & Leskovec (2012); an email interaction network within
a European institute with 42 departments Yin et al. (2017); an Amazon product network with 12
product categories Yang & Leskovec (2012); and a network of interactions between 12 Indian villages
Banerjee et al. (2013). We also test on the symmetric and noisy versions of each graph provided
by Chowdhury & Needham (2021). We compare with two OT-based methods: (1) GWL Xu et al.
(2019), which solves a balanced GW problem between G and G with the adjacency matrix of G
as the intra-domain cost matrix A; (2) SpecGWL Chowdhury & Needham (2021) which uses the
heat kernel on the graph Laplacian as A. We similarly run our FRLC algorithm using with both the
adjacency matrix (denoted FRLC-SR-GW) and heat kernel (denoted SpecFRLC-SR-GW). Since the
number of clusters in each dataset is not large, we compute the full-rank coupling matrix in each case."
AND THE,0.8516068052930057,"Overall, FRLC achieves the best clustering performance on 9 out of 12 datasets (Table 3). When
using the adjacency matrix, our semi-relaxed algorithm achieves better clustering result than GWL
on 9 out of 12 datasets. When using the heat kernel, our semi-relaxed algorithm achieves better"
AND THE,0.8525519848771267,"clustering result than SpecGWL on 11 out of 12 datasets. These results show the importance of
semi-relaxed OT on real-world problems, as well as the accuracy of FRLC."
AND THE,0.8534971644612477,"L.2
Problem Statement"
AND THE,0.8544423440453687,"As discussed in Vincent-Cuaz et al. (2022); Chowdhury & Needham (2021), it is possible to achieve
unsupervised graph partitioning (node clustering) through Gromov-Wasserstein (GW) OT. Given a
graph (V, E) of n nodes, we encode it as G = (A, h), where A ∈Rn×n encodes the intra-graph
node relationship (e.g. adjacency matrix, Laplacian) and h ∈∆n is a uniform measure over the
nodes. If we want to cluster the nodes of G into m clusters, we can define a new graph G = (B, h),
where B ∈Rm×m is a diagonal matrix representing the cluster’s connections and h is a distribution
over clusters estimating the proportion of nodes in G in each cluster. Since we don’t know the density
of clusters a priori, we can set h to be uniform. Usually B is set as the identity matrix."
AND THE,0.8553875236294896,"To cluster the nodes in G, we can solve a GW problem matching nodes in G with nodes in G, with
intra-domain cost matrices A and B: min
X"
AND THE,0.8563327032136105,"ij′kl′
(Aik −Bj′l′)2Pij′Pkl′"
AND THE,0.8572778827977315,"s.t.
P ∈Πh,h (46)"
AND THE,0.8582230623818525,"The cluster assignment of each node in G can then be recovered from P by finding the node in G
mapped to it with the maximum weight."
AND THE,0.8591682419659735,"However, since the proportion of nodes in G in each cluster is not known a priori, solving a balanced
GW problem fixing the marginal of P on G to be a uniform h significantly constrains the expressivity
of the algorithm. Therefore, as proposed in Vincent-Cuaz et al. (2022), we can instead solve a
semi-relaxed GW problem with the right marginal relaxed, fixing the marginal on G to be h but
allowing the marginal on G to deviate from h: min
X"
AND THE,0.8601134215500945,"ij′kl′
(Aik −Bj′l′)2Pij′Pkl′ + τKL(P T1n | h)"
AND THE,0.8610586011342155,"s.t.
P ∈Πh,· (47)"
AND THE,0.8620037807183365,"The learned h from semi-relaxed GW estimates the posterior proportion of nodes in G in each of the
m clusters."
AND THE,0.8629489603024575,"L.3
Dataset and Preprocessing"
AND THE,0.8638941398865785,"We run our semi-relaxed FRLC algorithm on four real-world graph datasets: a Wikipedia hyperlink
network with 1998 nodes and 15 clusters Yang & Leskovec (2012), a directed graph of email
interactions in a European research institute with 1005 nodes and 42 clusters Yin et al. (2017), an
Amazon product network with 1501 nodes and 12 clusters Yang & Leskovec (2012), and a network
of interactions between Indian villages with 1991 nodes and 12 clusters Banerjee et al. (2013). The
Wikipedia and EU-email graphs are directed, so we also use undirected versions of them. We also use
noisy version of each graph by adding up to 10% additional edges following Chowdhury & Needham
(2021), leading to a total of 12 different graphs to cluster."
AND THE,0.8648393194706995,"L.4
Experiment Settings"
AND THE,0.8657844990548205,"We compare our algorithm with two baseline methods, GWL and SpecGWL. GWL Xu et al. (2019)
solves the entropy-regularized version of the balanced GW problem of (46) with the adjacency matrix
of G as A. We set h such that the density of each node is proportional to its degree. We set h to be a
distribution estimated by sorting the weights of h and sampling m values via linear interpolation,
following Chowdhury & Needham (2021). We set B = diag(h). We set the entropy regularization"
AND THE,0.8667296786389413,"20
40
60
80
100
Rank 0.44 0.46 0.48 0.50 0.52 0.54 0.56"
AND THE,0.8676748582230623,"⟨C, P⟩F"
AND THE,0.8686200378071833,FRLC (rank-2)
AND THE,0.8695652173913043,FRLC (full-rank random initialization)
AND THE,0.8705103969754253,LOT (rank-2)
AND THE,0.8714555765595463,LOT (default initialization)
AND THE,0.8724007561436673,"Figure 8: FRLC achieves lower primal cost ⟨C, P ⟩F for P ∈Πa,b than Scetbon et al. (2021)
on a spatial-transcriptomics dataset of mouse embryonic development Chen et al. (2022). FRLC
demonstrates a more robust trend of improved performance with higher rank."
AND THE,0.8733459357277883,"η = 10−6. SpecGWL Chowdhury & Needham (2021) solves the same problem as GWL, but instead
of using the adjacency matrix as A, it uses the heat kernel on the normalized graph Laplacians Chung
(2005). We set the heat parameter t = 10."
AND THE,0.8742911153119093,"For our method, we solve the semi-relaxed GW problem of (47) with full rank solution. We use both
adjacency matrix and heat kernel as C and label the result of the two representations as FRLC-SR-GW
and SpecFRLC-SR-GW. We set τ = 0.01 as to minimize the conformation to the right marginal.
Since our method depends on random initialization, we run our method 10 times on each dataset and
report the mean performance. We evaluate the resulting clusterings of all methods by computing the
Adjusted Mutual Information (AMI) between the computed clustering and the ground truth clustering.
This experiment, and the experiments on mouse embryo spatial transcriptomics, were conducted on
cluster GPUs."
AND THE,0.8752362948960303,"M
Spatial Transcriptomics Alignment"
AND THE,0.8761814744801513,"M.1
Problem Statement"
AND THE,0.8771266540642723,"In this problem, we use FRLC to find a low-rank alignment matrix between cells from two spatial
transcriptomics (ST) Ståhl et al. (2016) slices, collected at two timepoints, then use the computed
alignment matrix for two downstream prediction tasks. An ST experiment on a 2D tissue slice yields
a pair (X, Z). X ∈Rn×p is the gene expression matrix, where n is the number of cells on the slice
and p is the number of genes measured. Xij ∈R is the gene expression level of gene j in cell i,
where a higher number indicates stronger expression. Z ∈Rn×2 is the spatial coordinate matrix,
where each row i stores the x-y coordinate of cell i on the slice. Therefore, each cell on the slice has
a gene expression vector of length p, which encodes the feature of the cell, as well as a coordinate
vector of length two, which encodes the geometrical information of the cell on the slice."
AND THE,0.8780718336483931,"Our input data is a pair of ST slices S0 = (X0, Z0), S1 = (X1, Z1), with n and m cells, of the
same tissue region. We assume S0 is collected at timepoint t = 0 and S1 is collected at timepoint
t = 1, hence the transition from S0 to S1 reflects the biological development of the tissue during
the time period. We would like to find the ancestor-descendant relationship between cells from S0
and S1 by computing an optimal transport coupling matrix between cells from S0 and cells from S1.
The state-of-the-art spatial transcriptomics alignment method moscot Klein et al. (2023) claims that"
AND THE,0.8790170132325141,"unbalanced OT is the most appropriate setup for this problem. Specifically, given discrete uniform
measure a and b over cells from S0 and S1, we solve the unbalanced Wasserstein problem"
AND THE,0.8799621928166351,"min
P ∈Rn×m
+
⟨C, P ⟩+ τKL(P 1m∥a) + τKL(P T1n∥b)
(48)"
AND THE,0.8809073724007561,"C ∈Rn×m
+
has entries Cij = c(X0
i , X1
j ), where c is the Euclidean distance between the features of
cell i from S0 and cell j from S1."
AND THE,0.8818525519848771,"The above formulation only considers the feature information of the two slices, but not the geometrical
information. Therefore, we can also solve the unbalanced GW problem"
AND THE,0.8827977315689981,"min
P ∈Rn×m
+ X"
AND THE,0.8837429111531191,"i,j′,k,l′
(Aik −Bj′l′)2Pij′Pkl′ + τKL(P 1m∥a) + τKL(P T1n∥b)
(49)"
AND THE,0.8846880907372401,"where A ∈Rn×n is the Euclidean distance matrix between the spatial location of cells within S0,
and B ∈Rm×m is the Euclidean distance matrix between the spatial location of cells within S1.
Similarly, we can combine the above two formulations to solve the unbalanced FGW problem."
AND THE,0.8856332703213611,"Notice the inherent asymmetry stemming from the temporal nature of the problem: all cells on S1
should have an ancestor from S1, but not all cells from S1 need to have a descendent in S0 because
of cell death. Therefore, the most natural OT task for this problem is semi-relaxed OT with the
left marginal (the marginal on the first/ancestor slice) relaxed. Specifically, we can also solve the
semi-relaxed Wasserstein problem"
AND THE,0.8865784499054821,"min
P ∈Π ·,b⟨C, P ⟩+ τKL(P 1m∥a)
(50)"
AND THE,0.8875236294896031,as well as semi-relaxed GW and FGW problem.
AND THE,0.888468809073724,"Gene expression prediction task
Given the alignment matrix P linking cells from S0 to S1, we
can predict properties of cells in S1 from properties of cells in S0. Let the expression of a gene j
in S0 be a vector fj ∈Rn, such that fji is the expression level of gene j in cell i, we can predict
the expression of gene j in S1 as e
fj = m × P T × fj ∈Rm. The accuracy of the prediction can
be measured by the Spearman correlation between the predicted expression and the ground truth
expression f j of gene j in S1: ρ( e
fj, f j). In this work, we test the prediction accuracy on 10 test
genes: Tubb2b, Pantr1, Actc1, Tnni1, Afp, Hbb-bh1, Fez1, Crabp1, Crabp2, Col3a1, which are
markers genes for various cell types in mouse embryo."
AND THE,0.889413988657845,"Cell type prediction task
We can also use the cell type labels of cells in S0 to predict the cell type
labels of cells in S1. Specifically, for each cell j in S1, we can assign it the type of the cell argmaxiPij
in S0. We can measure the accuracy of the cell type prediction by computing the Adjusted Rand
Index (ARI) and Adjusted Mutual Information (AMI) between the predicted clustering of cells in S1
and the ground truth clustering."
AND THE,0.8903591682419659,"M.2
Dataset and Preprocessing"
AND THE,0.8913043478260869,"We use the large-scale real-world dataset of mouse embryo development Chen et al. (2022), consisting
of eight timepoints of ST slices during the whole process of mouse embryo development. In this
work, we align the pair of adjacent timepoints of E11.5 and E12.5 embryos, consisting of 30,124
cells and 51,365 cells, respectively. We preprocess the dataset using the standard SCANPY Wolf
et al. (2018) pipeline. We first filter the two slices to have the same set of genes, resulting in 26,436
genes for all cells from both slices. We then log-normalize the gene expression of all cells from the
two slices, and apply Principle Component Analysis (PCA) to reduce the dimensionality of gene
expressions to 30. We take the Euclidean distance between the gene expression in the PCA space as
the cost matrix C. We take the Euclidean distance between the 2D coordinate of each cell within
each slice as the intra-domain cost matrices A and B."
AND THE,0.8922495274102079,"Fig. 9 visualizes the two slices in this dataset, with each cell annotated with a cell type from the
original publication."
AND THE,0.8931947069943289,"Figure 9: Visualization of the E11.5 and E12.5 mouse embryos, with each cell colored by the cell
type annotated by Chen et al. (2022)."
AND THE,0.8941398865784499,"M.3
Experiment Settings"
AND THE,0.8950850661625709,"We compare with the unbalanced low-rank optimal transport algorithm of Scetbon et al. (2023), the
backbone of the spatial transcriptomics alignment method moscot Klein et al. (2023), which was
shown by Scetbon et al. (2023); Klein et al. (2023) to achieve state-of-the art performance on spatial
transcriptomics alignment. We use Scetbon et al. (2023) to solve three unbalanced problems for
spatial transcriptomics alignment: the unbalanced Wasserstein problem of (48) (result denoted as
LOT-U-W), the unbalanced GW problem of (49) (LOT-U-GW), and the unbalanced FGW problem
with a convex combination of the previous two costs (LOT-U-FGW). We use FRLC to solve the same
three unbalanced problems (results denoted as FRLC-U-W, FRLC-U-GW, FRLC-U-FGW). Since the
two slices contain > 30,000 cells and > 50,000 cells respectively, a full-rank solution is not feasible,
hence we solve for low-rank solutions with the rank validated. We also solve semi-relaxed versions
of Wasserstein (result denoted as FRLC-SR-W), GW (FRLC-SR-GW), FGW (FRLC-SR-FGW)
problems using our FRLC algorithm, as well as using a particular setting of LOT-U that is equivalent
to semi-relaxed solver (results denoted as LOT-SR-W, LOT-SR-GW, LOT-SR-FGW)."
AND THE,0.8960302457466919,"We perform extensive grid search to find the best hyperparameter combinations for each method and
each problem. The grid of hyperparameters searched for each method is reported in Table. 6. The
best performing hyperparameter combination for each method is reported in Table. 7 along with
the performance on the validation genes. We pick the best hyperparameters using the Spearman
correlation on the gene expression prediction task for 10 validation genes: Ckb, Fabp7, Myl4, Tnnt2,
Apoa2, Hba-x, Tubb3, Epha7, Ldha, Col1a2, which are marker genes of various cell types as well.
We report the performance of the alignment computed by each method using the Spearman correlation
on the gene expression prediction task for 10 test genes, as well as the ARI and AMI on the cell type
prediction task. Fig. 10 visualizes the ground truth cell type classification versus the classification
predicted by our method FRLC-SR-W."
AND THE,0.8969754253308129,"M.4
Runtime"
AND THE,0.8979206049149339,"We report the runtime and OT cost of FRLC and LOT Scetbon et al. (2021) on this dataset (mouse
embryo E11.5-12.5) as well as two other datasets (E9.5-10.5, E10.5-11.5) from Chen et al. (2022) in
Table 4. For all three datasets, FRLC achieves a better OT cost in a shorter time."
AND THE,0.8988657844990549,"Dataset
LOT (seconds)
FRLC (seconds)
LOT (OT Cost)
FRLC (OT Cost)"
AND THE,0.8998109640831758,"Mouse embryo (E9.5–10.5)
2.545
1.112
0.440
0.385
Mouse embryo (E10.5–11.5)
4.209
1.190
0.371
0.344
Mouse embryo (E11.5–12.5)
8.667
1.889
0.478
0.439
Table 4: Comparison of methods on Stereo-Seq mouse embryo spatial transcriptomics datasets using
GPU and default settings: min_iter=10, max_iter=100, rank r = 50."
AND THE,0.9007561436672968,"N
nth-roots of unity"
AND THE,0.9017013232514177,"N.1
Problem Statement"
AND THE,0.9026465028355387,"To test if FRLC can effectively coarse-grain transport between two datasets with obvious cluster
structure, we generate a pair of two-dimensional datasets Z(1) and Z(2), with ten and five clusters
respectively. We run FRLC with T ∈R10×5
+
to see if the barycentric projections for the LC
factorization (Definition 4.1) can recover the cluster structure. These projections induce 10 and 5
barycenters for the first and second dataset, and are defined by"
AND THE,0.9035916824196597,"Y(1) := diag(1/gQ)QTZ(1),
Y(2) := diag(1/gR)RTZ(2)."
AND THE,0.9045368620037807,"We examine, visually, whether these barycenters are good representatives of the clusters in each
dataset, and whether the latent coupling depicts a reasonable transfer of mass between barycenters.
We also run FRLC with T ∈R10×10
+
and plot the barycenters from the resulting factorization for
comparison. As discussed in § 4.2, the barycentric projections defined above, and in Definition 4.1
can be applied to factored couplings Forrow et al. (2019); Scetbon et al. (2021), yielding projections
of the form:"
AND THE,0.9054820415879017,"Y(1) := diag(1/g)QTZ(1),
Y(2) := diag(1/g)RTZ(2)."
AND THE,0.9064272211720227,"Thus, we also ran the method of Scetbon et al. (2021) on this data, called LOT throughout the
experiment, with g ∈(R∗
+)5 and g ∈(R∗
+)10, plotting its barycenters in each case along with the
diagonal latent coupling diag(g)."
AND THE,0.9073724007561437,"N.2
Dataset and Preprocessing"
AND THE,0.9083175803402647,"We instantiate a pair of two-dimensional datasets Z(1) and Z(2) as follows. Let Un denote the
nth-roots of unity:"
AND THE,0.9092627599243857,"Un := {e2πik/n : k = 0, . . . , n}."
AND THE,0.9102079395085066,These complex numbers can be equivalently expressed as ordered pairs on the unit circle:
AND THE,0.9111531190926276,"ck =

Re(e2πik/n)
Im(e2πik/n)"
AND THE,0.9120982986767486,"
=

cos(2πk/n))
sin(2πk/n) 
."
AND THE,0.9130434782608695,"We consider n uniformly weighted mixtures of Gaussians, where for each sample X, we first sample
a root of unity uniformly,
k ∼Uniform(n),
and conditionally on k, we sample X from an isotropic Gaussian centered at this root of unity"
AND THE,0.9139886578449905,"X ∼N(ck, σ2Id2),"
AND THE,0.9149338374291115,"using a standard deviation σ = 0.1. Samples are generated with the make_blobs function from
sklearn.datasets. We generate two datasets in this way, Z(1) for n = 10, and Z(2) for n = 5.
To generate dataset Z(1), we first homogeneously scale the roots of unity to lie on a circle of
radius 3 before sampling, so that the two datasets do not overlap but still use the same standard
deviation. We do not scale the centers used for Z(2), so they all lie on the unit circle. We generate
Z(1) = {z(1)
1 , . . . , z(1)
1000} and Z(2) = {z(2)
1 , . . . , z(2)
1000} using 1000 samples each and form the
empirical measures µ :="
X,0.9158790170132325,"1000
X i=1"
X,0.9168241965973535,"1
1000δz(1)
i ,
ν :="
X,0.9177693761814745,"1000
X j=1"
X,0.9187145557655955,"1
1000δz(2)
j ."
X,0.9196597353497165,"supported on Z(1) and Z(2), corresponding to uniform probability vectors a, b ∈∆1000."
X,0.9206049149338374,"N.3
Experiment Settings"
X,0.9215500945179584,"We used default hyperparameter settings for both methods. In particular, FRLC sets τ = 75 and
γ = 90, with a maximum of 200 iterations and a minimum of 25 iterations, subject to the stopping
criterion ∆given in (10). The LOT default settings are γ = 10 (there is no hyperparameter analogous
to τ in LOT). Both methods use a random initialization and were run on CPU."
X,0.9224952741020794,"O
Discussion of differences between FRLC and existing low-rank optimal
transport algorithms"
X,0.9234404536862004,"We provide an extensive comparison of FRLC against the parametrization and objective of Scetbon
et al. (2021) in Appendix A. As Latent-OT of Lin et al. (2021) is an extension of the k-Wasserstein
barycenter problem, it has a distinct objective and thus performs worse on primal OT cost (Table 5),
making a direct experimental comparison on primal cost minimization only appropriate relative to the
works of Scetbon et al. (2021, 2023, 2022). This stated, we still list a number of distinctions between
FRLC and Latent-OT – noting that most differences between FRLC and Forrow et al. (2019) transfer
from this discussion since Lin et al. (2021) extends the k-Barycenter problem of Forrow et al. (2019).
(1) Lin et al. optimize two sets of variables: sub-couplings (Q, R, T ) and anchor points (zx, zy) on
which the sub-couplings depend. FRLC only has (Q, R, T ) as variables of the optimization. (2) Cost
matrices used in Lin et al. (2021) are built from distances between each dataset and its representative
anchor points for Q, R, or the distances between the two sets of anchor points for T . In contrast,
ground costs used in FRLC to update (Q, R, T ) are always derived from the distance matrix C in
the Wasserstein objective ⟨C, P ⟩F . Specifically, the cost matrices used by Lin et al. are:"
X,0.9243856332703214,"[CQ]ik = ∥xi −zx
k∥2
2,
[CR]jℓ= ∥yj −zy
ℓ∥2
2,
[CT ]kℓ= ∥zx
k −zy
ℓ∥2
2,"
X,0.9253308128544423,"optionally using a Wasserstein distance for the entries of CT . (3) FRLC costs are given in the
exponents of the Gibbs kernels written above and below Equation 9. These are derived directly from
the rank-r Wasserstein problem minP ∈Πr(a,b)⟨C, P ⟩F and differ substantially from those of the
proxy objective in Lin et al. (2021); Forrow et al. (2019). (4) The different objectives and variables
lead to very different algorithms: Lin et al. alternate updates to the sub-couplings (Q, R, T ) using
Dykstra, with updates to the latent anchor points (zx, zy) using first-order conditions. In contrast,
FRLC alternates semi-relaxed OT to update (Q, R) and balanced OT to update T . (5) Because FRLC
does not require anchor points to define costs, FRLC can handle cost matrices which are not simple
functions of distance. For example, if Cij is the price of transporting good i to warehouse j one may
not be able to re-evaluate a price c(xi, zx
k) between xi and latent anchor zx
k. In such situations, while
finding a low-rank plan may make sense (e.g. to approximate an assignment for a massive dataset),
an “anchor"" may not have clear definition in the setting of general cost matrices. (6) The Lin et al.
objective is only a proxy for a Wasserstein-type loss, and Lin et al. do not explore extensions to
Gromov-Wasserstein (GW), or Fused GW, which FRLC readily generalizes to. A summary of the
existing low-rank OT algorithms and key distinctions between them is given in Table 1."
X,0.9262759924385633,"For completeness, we offer a compare against the work Latent OT Lin et al. (2021), which solves a
variation of the k-Wasserstein barycenter problem. As discussed, while their factorization is similar,
their problem is distinct from FRLC as it does not solve the primal OT problem for general cost. We
report the cost obtained by FRLC and by Latent OT on various simulated datasets in Table 5."
X,0.9272211720226843,"Dataset
OT-cost (FRLC)
OT-cost (Lin et al.)"
X,0.9281663516068053,"5th and 10th roots of unity
(rank r1, r2 = 5, 10)
1.174
2.124"
X,0.9291115311909263,"Two-moons and 8-Gaussians
(rank r = 20)
2.716
4.291"
D GAUSSIAN MIXTURE,0.9300567107750473,"2D Gaussian mixture
(rank r = 20)
0.552
0.922"
D GAUSSIAN MIXTURE,0.9310018903591682,"10D Gaussian mixture
(rank r = 20)
1.038
1.298"
D GAUSSIAN MIXTURE,0.9319470699432892,"Table 5: Comparison against Lin et al. (2021) in primal OT-cost ⟨C, P ⟩F ."
D GAUSSIAN MIXTURE,0.9328922495274102,"P
Limitations"
D GAUSSIAN MIXTURE,0.9338374291115312,"Our method introduces an additional hyperparameter τ relative to previous approaches Scetbon et al.
(2021), controlling the strength of the KL penalty on the inner marginals when updating Q and R."
D GAUSSIAN MIXTURE,0.9347826086956522,"Empirically, we found FRLC to be robust to different choices of τ, but applying the method optimally
requires this additional hyperparamter in any grid-search."
D GAUSSIAN MIXTURE,0.9357277882797732,"We also note that the non-asymptotic criterion ∆(·, ·) is weak relative to stronger notions of conver-
gence, and that often users might prefer to simply run the method up to some number of maximal
iterations by setting the parameter for whether ∆(·, ·) is used to False. The W optimization em-
pirically converges to minima smoothly, so for the most part there is not much of a need for ∆
except for early stopping. We recommend that users plot the loss over iterations and use it to set the
tolerance parameter tol, and the minimum and maximum iteration parameters for the time-being.
The needs for these parameters might vary widely–the minimum number of iterations should be very
low (around 5) for simple datasets and substantially higher for high-dimensional, structured ones."
D GAUSSIAN MIXTURE,0.9366729678638941,"Although we demonstrate strong performance already, there is massive room for improvement as our
implementation is preliminary and not at the level of a high-performance library like ott-jax. We
use lightweight vanilla implementations of Sinkhorn as a sub-routine, not taking advantage of the
momentum-based techniques which could accelerate it massively. Thus, one can imagine that the
potential scalability and speed of this method could be much higher than reported in this document."
D GAUSSIAN MIXTURE,0.9376181474480151,"Q
Broader impacts"
D GAUSSIAN MIXTURE,0.9385633270321361,"FRLC is general enough to be used modularly within any ML algorithm using OT as a subroutine to
help with scalability. We also note that the LC factorization is similar to a PCA in the context of OT,
yielding an optimal low-rank coupling with an interpretable latent coupling factor."
D GAUSSIAN MIXTURE,0.9395085066162571,"Hyperparameter
Values"
D GAUSSIAN MIXTURE,0.9404536862003781,"rank (Both)
50, 100, 200
τ (Ours)
30, 50, 100
τ (LOT-U)
0.99, 0.9, 0.7
ϵ (LOT-U)
0.001, 0.01, 0.1
Table 6: Hyperparameter grid considered in hyperparameter search for validation. Scetbon et al.
(2023); Klein et al. (2023) scales τ ′ =
τ
1−τ and their τ ′ are in the same range."
D GAUSSIAN MIXTURE,0.941398865784499,"Solver
Rank
τ (Ours)
τ (UL)
ϵ (UL)
Spearman ρ (Validation)"
D GAUSSIAN MIXTURE,0.94234404536862,"FRLC-SR-W (Ours)
200
30
-
-
0.465
FRLC-SR-GW (Ours)
100
100
-
-
0.288
FRLC-SR-FGW (Ours)
200
50
-
-
0.465
FRLC-U-W (Ours)
200
100
-
-
0.471
FRLC-U-GW (Ours)
50
30
-
-
0.282
FRLC-U-FGW (Ours)
200
30
-
-
0.353
LOT-U-W
200
-
0.9
0.001
0.394
LOT-U-GW
100
-
0.99
0.01
0.001
LOT-U-FGW
200
-
0.7
0.001
0.393
LOT-SR-W
200
-
0.7
0.001
0.394
LOT-SR-GW
200
-
0.7
0.1
0.003
LOT-SR-FGW
200
-
0.99
0.001
0.399
Table 7: The best performing hyperparameters for each solver and the performance on the validation
genes."
D GAUSSIAN MIXTURE,0.943289224952741,Figure 10: Ground truth and the predicted cell type classification of the E12.5 embryo. (c)
D GAUSSIAN MIXTURE,0.944234404536862,"(a)
(b) (d)"
D GAUSSIAN MIXTURE,0.945179584120983,"Figure 11: (a) LC-projection barycenters aligned with FRLC latent-coupling T on (a) two moons
and eight Gaussians (r = 30), (b) LC-projection barycenters aligned with diag(g) from Scetbon
et al. (2021) (r = 30), (c) the checkerboard dataset with FRLC latent coupling aligned barycenters
(r = 12), and (d) with diagonal alignment Scetbon et al. (2021) (r = 12). We show in A.1 that the
output of FRLC can be diagonalized to the factorization of Forrow et al. (2019) (Figure 12)."
D GAUSSIAN MIXTURE,0.946124763705104,"Figure 12: As discussed in A.1, one may recover the factorization of Forrow et al. (2019) as a sub-case
of the LC-factorization. Shown is the factorization found by diagonalizing the output of FRLC from
(Q, R, T ) 7→(Q′, R, diag(gR))."
D GAUSSIAN MIXTURE,0.947069943289225,NeurIPS Paper Checklist
CLAIMS,0.9480151228733459,1. Claims
CLAIMS,0.9489603024574669,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.9499054820415879,Answer: [Yes]
CLAIMS,0.9508506616257089,"Justification: The abstract and introduction exactly describe the background preceding this
paper and placing it into context, and exactly describe the contribution and scope of the
paper to the field."
CLAIMS,0.9517958412098299,Guidelines:
CLAIMS,0.9527410207939508,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.9536862003780718,2. Limitations
LIMITATIONS,0.9546313799621928,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9555765595463138,Answer: [Yes]
LIMITATIONS,0.9565217391304348,"Justification: We describe the limitations of our method in Section P. These include an addi-
tional hyperparameter that FRLC introduces relative to previous work, and the substantial
room for code optimization."
LIMITATIONS,0.9574669187145558,Guidelines:
LIMITATIONS,0.9584120982986768,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9593572778827977,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9603024574669187,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All theoretical results state their assumptions clearly and all propositions are
followed by thorough line-by-line proofs with careful justifications made for each step. They
have checked over by all of the authors. All proofs are provided in the supplement with
detail and are referenced in the main body where appropriate.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9612476370510397,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.9621928166351607,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide all code used for generating the results of our paper. This not only
includes the source code for the optimization, but the experimental code used for bench-
marking. The algorithm is implemented exactly as described in the paper, and reviewers can
freely consult the code we provide to them.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9631379962192816,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset)."
THEORY ASSUMPTIONS AND PROOFS,0.9640831758034026,"(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
THEORY ASSUMPTIONS AND PROOFS,0.9650283553875236,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We will upload all code, and all code required to generate the synthetic data
experiments used. Any real data experiments using especially large-scale data have publicly
available and easily accessible datasets which we provide references for. We provide
comprehensive descriptions of how the data was pre-processed, and provide experimental
code which can be followed easily.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9659735349716446,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
THEORY ASSUMPTIONS AND PROOFS,0.9669187145557656,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We introduce all hyper-parameters with rigorous justification of their utility.
The values of the default hyper-parameters are accessible in the code we upload and any
experiment which does not use the default hyperparameter (e.g. for a validation search) has
the full table provided and the code used for the experiment.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9678638941398866,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
THEORY ASSUMPTIONS AND PROOFS,0.9688090737240076,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
THEORY ASSUMPTIONS AND PROOFS,0.9697542533081286,"Answer: [Yes]
Justification: The optimization is deterministic, so generally there’s no need for reporting
statistical significance. One small exception is that one of our proposed initializations is
randomized, and for experiments which use it we do include ±1σ error bars.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9706994328922496,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
THEORY ASSUMPTIONS AND PROOFS,0.9716446124763705,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]"
THEORY ASSUMPTIONS AND PROOFS,0.9725897920604915,"Justification: We indicate when experiments are run on CPU versus GPU in the experimental
details.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9735349716446124,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
THEORY ASSUMPTIONS AND PROOFS,0.9744801512287334,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: This research was conducted ethnically and conforms to the guidelines of the
code of ethics. We do not anticipate any major negative societal impact to result from this
work on optimal transport.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9754253308128544,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics."
THEORY ASSUMPTIONS AND PROOFS,0.9763705103969754,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
THEORY ASSUMPTIONS AND PROOFS,0.9773156899810964,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We address broader impacts in Section Q: FRLC can be used for scaling an
interpretability in any ML method using OT as a subroutine.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9782608695652174,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
THEORY ASSUMPTIONS AND PROOFS,0.9792060491493384,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We do not generate any new datasets other than a toy dataset of Gaussians
centered at nth roots of unity, which has no societal impact. We do not believe our optimal
transport work has significant risk of misuse.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9801512287334594,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
THEORY ASSUMPTIONS AND PROOFS,0.9810964083175804,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
THEORY ASSUMPTIONS AND PROOFS,0.9820415879017014,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9829867674858223,"Justification: All data that we use and all work that we build on are cited and credited
heavily."
THEORY ASSUMPTIONS AND PROOFS,0.9839319470699432,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.9848771266540642,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9858223062381852,13. New Assets
NEW ASSETS,0.9867674858223062,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9877126654064272,Answer: [NA]
NEW ASSETS,0.9886578449905482,Justification: We provide no new assets.
NEW ASSETS,0.9896030245746692,Guidelines:
NEW ASSETS,0.9905482041587902,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9914933837429112,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924385633270322,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933837429111532,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.994328922495274,Justification: This paper involves no research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995274102079395,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996219281663516,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997164461247637,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998109640831758,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.999054820415879,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
