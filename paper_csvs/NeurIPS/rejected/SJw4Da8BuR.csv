Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017574692442882249,"We propose TCSP, a novel method for compressing a transformer model by focus-
1"
ABSTRACT,0.0035149384885764497,"ing on reducing the hidden size of the model. By projecting the whole transform
2"
ABSTRACT,0.005272407732864675,"model into a subspace, we enable matrix operations between the weight matrices
3"
ABSTRACT,0.007029876977152899,"in the model and features in a reduced-dimensional space, leading to signiﬁcant
4"
ABSTRACT,0.008787346221441126,"reductions in model parameters and computing resources. To establish this sub-
5"
ABSTRACT,0.01054481546572935,"space, we decompose the feature matrix, derived from different layers of sampled
6"
ABSTRACT,0.012302284710017574,"data instances, into a projection matrix. For evaluation, TCSP is applied to com-
7"
ABSTRACT,0.014059753954305799,"press T5 and BERT models on the GLUE and SQuAD benchmarks. Experimental
8"
ABSTRACT,0.015817223198594025,"results demonstrate that TCSP achieves a compression ratio of 44% with at most
9"
ABSTRACT,0.01757469244288225,"1.6% degradation in accuracy, surpassing or matching prior compression methods.
10"
ABSTRACT,0.019332161687170474,"Furthermore, TCSP exhibits compatibility with other methods targeting ﬁlter and
11"
ABSTRACT,0.0210896309314587,"attention head size compression.
12"
INTRODUCTION,0.022847100175746926,"1
Introduction
13"
INTRODUCTION,0.02460456942003515,"The transformer model [1] is widely used in Natural Language Processing as well as other do-
14"
INTRODUCTION,0.026362038664323375,"mains such as Computer Vision [2, 3, 4] and Speech Recognition [5, 6, 7]. Despite its impres-
15"
INTRODUCTION,0.028119507908611598,"sive performance, the large size of transformer models and the high inference latency limit their
16"
INTRODUCTION,0.029876977152899824,"practical deployment. To address this challenge, model compression techniques including prun-
17"
INTRODUCTION,0.03163444639718805,"ing [8, 9, 10, 11, 12, 13] and low-rank decomposition [14, 15, 16, 17] have been proposed to reduce
18"
INTRODUCTION,0.033391915641476276,"model parameters and improve inference speed while ensuring that the performance of the compressed
19"
INTRODUCTION,0.0351493848857645,"model is not signiﬁcantly disturbed.
20"
INTRODUCTION,0.03690685413005272,"The transformer model consists of a series of weight matrices that are determined by the hidden
21"
INTRODUCTION,0.03866432337434095,"size d, attention head size dh in the multi-head attention layers, and the number of ﬁlters df in the
22"
INTRODUCTION,0.040421792618629174,"feed-forward network layers. Existing compression methods primarily focus on reducing the attention
23"
INTRODUCTION,0.0421792618629174,"head size dh [14, 8, 13] and the number of ﬁlters df [8, 13], or performing matrix decomposition
24"
INTRODUCTION,0.043936731107205626,"to transform d ⇥d matrices into two d ⇥k matrices [14, 15, 16]. However, none of these methods
25"
INTRODUCTION,0.04569420035149385,"directly address the reduction of the hidden size d. Although CoFi [13], a pruning method, attempts
26"
INTRODUCTION,0.04745166959578207,"to compress d, it can only achieve a 1% reduction while keeping model performance.
27"
INTRODUCTION,0.0492091388400703,"This paper introduces a novel method, named Transformer Compression via Subspace Projection
28"
INTRODUCTION,0.050966608084358524,"(TCSP), for compressing transform models by reducing the hidden size. By projecting the transformer
29"
INTRODUCTION,0.05272407732864675,"model into a subspace, TCSP achieves this compression. To create this subspace, we employ low-rank
30"
INTRODUCTION,0.054481546572934976,"factorization to decompose the feature matrix derived from sampled data instances into a projection
31"
INTRODUCTION,0.056239015817223195,"matrix. Speciﬁcally, we gather multiple data instances, pass each through the transformer model to
32"
INTRODUCTION,0.05799648506151142,"obtain their respective features from different layers, and concatenate them to form a feature matrix.
33"
INTRODUCTION,0.05975395430579965,"Decomposing this matrix yields the projection matrix, which allows us to project the model into a
34"
INTRODUCTION,0.061511423550087874,"subspace, resulting in a signiﬁcant reduction in model parameters. Furthermore, TCSP can be easily
35"
INTRODUCTION,0.0632688927943761,"combined with other compression methods that reduce attention head size and the number of ﬁlters,
36"
INTRODUCTION,0.06502636203866433,"ensuring compatibility. Although low-rank factorization is employed in TCSP, it is considered a
37"
INTRODUCTION,0.06678383128295255,"sub-technique to achieve the primary goal of reducing hidden size.
38"
INTRODUCTION,0.06854130052724078,"Our contributions can be summarized as follows:
39"
INTRODUCTION,0.070298769771529,"• We take a fresh perspective on model compression by reducing the hidden size of the
40"
INTRODUCTION,0.07205623901581722,"transformer model, which has been rarely explored before.
41"
INTRODUCTION,0.07381370826010544,"• We propose the TCSP technique to achieve the compression goal. TCSP decomposes the
42"
INTRODUCTION,0.07557117750439367,"feature matrix derived from sampled data instances into a projection matrix, which is then
43"
INTRODUCTION,0.0773286467486819,"used to project the transformer model into a subspace. In addition, TCSP is compatible with
44"
INTRODUCTION,0.07908611599297012,"other compression methods, such as reducing the multi-head attention size and the number
45"
INTRODUCTION,0.08084358523725835,"of ﬁlters, which could further speed up inference.
46"
INTRODUCTION,0.08260105448154657,"• Experimental results on two widely-used benchmarks, GLUE and SQuAD, show that TCSP
47"
INTRODUCTION,0.0843585237258348,"can compress 44% of the parameters of both T5 and BERT within 1.6% accuracy loss,
48"
INTRODUCTION,0.08611599297012303,"surpassing or matching prior compression methods.
49"
RELATED WORK,0.08787346221441125,"2
Related Work
50"
RELATED WORK,0.08963093145869948,"The transformer model is a general model that has been widely used in the ﬁelds of deep learning.
51"
RELATED WORK,0.0913884007029877,"To improve inference speed and reduce memory overhead, different approaches have been proposed
52"
RELATED WORK,0.09314586994727592,"to compress the transformer. Previous research [18] categorize these approaches into ﬁve distinct
53"
RELATED WORK,0.09490333919156414,"categories: quantization [19, 20, 21], pruning [8, 9, 10, 11, 12, 13], knowledge distillation [22, 23],
54"
RELATED WORK,0.09666080843585237,"low-rank factorization [14, 15, 16, 17], and weight sharing [24, 25]. These ﬁve types of methods are
55"
RELATED WORK,0.0984182776801406,"orthogonal to each other. In this work, we focus on exploring low-rank factorization and pruning
56"
RELATED WORK,0.10017574692442882,"approaches as means to directly reduce the number of parameters in ﬁned-tuned task-speciﬁc models.
57"
RELATED WORK,0.10193321616871705,"We do not delve into Knowledge distillation and weight sharing, as they involve training models from
58"
RELATED WORK,0.10369068541300527,"scratch. We do not cover quantization, which compresses models based on storage considerations.
59"
RELATED WORK,0.1054481546572935,"Low-Rank Factorization for Transformer. In recent years, various low-rank decomposition
60"
RELATED WORK,0.10720562390158173,"methods have been proposed speciﬁcally for Transformers. Ma et al. [26] introduce a block term
61"
RELATED WORK,0.10896309314586995,"tensor decomposition approach to decompose the multi-head attention in Transformers. Noach et
62"
RELATED WORK,0.11072056239015818,"al. [27] propose a two-stage method for Transformer compression, where they ﬁrst employ SVD to
63"
RELATED WORK,0.11247803163444639,"decompose Transformer’s weight matrix and then ﬁne-tune the model using knowledge distillation.
64"
RELATED WORK,0.11423550087873462,"However, direct SVD-based compression of the weight matrix assumes that it possesses low-rank
65"
RELATED WORK,0.11599297012302284,"properties, which may not always hold for Transformers. To address this issue, Chen et al. [14]
66"
RELATED WORK,0.11775043936731107,"propose a data-aware low-rank factorization method. This method minimizes the reconstruction error
67"
RELATED WORK,0.1195079086115993,"of the matrix multiplication between each weight matrix and its corresponding input feature matrix
68"
RELATED WORK,0.12126537785588752,"rather than directly minimizing the reconstruction error of the weight matrix itself. Hsu et al. [15]
69"
RELATED WORK,0.12302284710017575,"and Hua et al. [16] propose FWSVD and TFWSVD, respectively, which utilize Fisher information
70"
RELATED WORK,0.12478031634446397,"to measure the contribution of different parts of the weight matrix to model performance during the
71"
RELATED WORK,0.1265377855887522,"SVD process, achieving improved results compared to direct SVD. Alternatively, Tahaei et al. and
72"
RELATED WORK,0.1282952548330404,"Edalati et al. [28, 29] employ Kronecker decomposition to preserve the matrix rank and successfully
73"
RELATED WORK,0.13005272407732865,"compress models like BERT and GPT2.
74"
RELATED WORK,0.13181019332161686,"Pruning for Transformer. Pruning is a commonly used technique for eliminating unnecessary
75"
RELATED WORK,0.1335676625659051,"parameters in the model. Existing pruning methods can be broadly divided into two categories:
76"
RELATED WORK,0.13532513181019332,"unstructured pruning and structured pruning. Unstructured pruning aims to remove unimportant
77"
RELATED WORK,0.13708260105448156,"scalar values in model’s parameters. Various unstructured pruning approaches have been proposed
78"
RELATED WORK,0.13884007029876977,"for Transformer, such as magnitude-based [30], ﬁrst-order [31], second-order [32], and lottery
79"
RELATED WORK,0.140597539543058,"ticket hypothesis [33]. Although unstructured pruning algorithms can remove many redundant
80"
RELATED WORK,0.14235500878734622,"parameters while ensuring accuracy, compressed models require speciﬁc sparse data structures and
81"
RELATED WORK,0.14411247803163443,"hardware support to take advantage of unstructured pruning. For this reason, structure pruning
82"
RELATED WORK,0.14586994727592267,"approaches [8, 9, 10, 11, 12, 13] are proposed to remove weight blocks in the transformer model,
83"
RELATED WORK,0.14762741652021089,"including entire transformer layers, attention heads, and ﬁlters. Unlike unstructured pruning, structure
84"
RELATED WORK,0.14938488576449913,"pruning can accelerate inference speed and reduce memory overhead without specialized data
85"
RELATED WORK,0.15114235500878734,"structure and hardware.
86"
RELATED WORK,0.15289982425307558,"Original 
Transformer"
RELATED WORK,0.1546572934973638,Sampled
RELATED WORK,0.15641476274165203,"Dataset
Entire 
Dataset"
RELATED WORK,0.15817223198594024,"Compressed 
Transformer
Weight Matrices"
RELATED WORK,0.15992970123022848,"× 𝐿
× 𝐿"
RELATED WORK,0.1616871704745167,Compressed Matrices
RELATED WORK,0.1634446397188049,Feature Matrix
RELATED WORK,0.16520210896309315,"Finetuned
Compressed 
Transformer"
RELATED WORK,0.16695957820738136,"(1) Generating Projection Matrix
(2) Fusing Projection and Weight Matrix
(3) Fine-tuning Weight Matrix 𝑊
෡
𝑊"
RELATED WORK,0.1687170474516696,"𝑊
𝑃
෡
𝑊
SVD"
RELATED WORK,0.1704745166959578,"Σ
𝑃
𝑋
V 𝑋"
RELATED WORK,0.17223198594024605,Features …
RELATED WORK,0.17398945518453426,Projection Matrix
RELATED WORK,0.1757469244288225,Figure 1: Workﬂow of Transformer Compression via Subspace Projection (TCSP).
OVERVIEW,0.17750439367311072,"3
Overview
87"
OVERVIEW,0.17926186291739896,"3.1
Preliminary: Transformer Architecture
88"
OVERVIEW,0.18101933216168717,"We take T5 [34] as an example of the transformer model to present the proposed TCSP, although it
89"
OVERVIEW,0.1827768014059754,"can be easily extended to other transform models such as BERT [35] and GPT [36].
90"
OVERVIEW,0.18453427065026362,"A transformer model comprises a stack of blocks, each containing a multi-head attention (MHA)
91"
OVERVIEW,0.18629173989455183,"layer, an optional cross-attentive (CA) layer, and a feed-forward network (FFN) layer.
92"
OVERVIEW,0.18804920913884007,"Multi Head Attention (MHA). In a transformer encoder, each MHA layer consists of H attention
93"
OVERVIEW,0.18980667838312829,"heads that facilitate interactions between tokens, with a pre-normalization step. Other transform
94"
OVERVIEW,0.19156414762741653,"models such as BERT adopt post-normalization after the MHA layer.
95"
OVERVIEW,0.19332161687170474,"xM = x + MHA(xn),
xn = norm(x),
MHA(xn) = H
X i=1"
OVERVIEW,0.19507908611599298,"Att(i)(xn),
(1)"
OVERVIEW,0.1968365553602812,Att(i)(xn) = W (i)>
OVERVIEW,0.19859402460456943,"O
W (i)"
OVERVIEW,0.20035149384885764,V xn · Softmax((W (i)
OVERVIEW,0.20210896309314588,K xn)>(W (i)
OVERVIEW,0.2038664323374341,Q xn)/ p
OVERVIEW,0.2056239015817223,"d),
(2)"
OVERVIEW,0.20738137082601055,"where norm(x) represents the normalization function such as LayerNorm and RMSNorm,
96"
OVERVIEW,0.20913884007029876,"WQ, WK, WV , WO 2 Rdh⇥d are the parameters of an MHA layer, denoting the query, key, value,
97"
OVERVIEW,0.210896309314587,"and output matrices, respectively. Here d denotes the hidden size, and dh = d/H denotes the
98"
OVERVIEW,0.2126537785588752,"attention head size.
99"
OVERVIEW,0.21441124780316345,"In a transformer decoder, a CA layer is added after the MHA layer. The CA layer closely resembles
100"
OVERVIEW,0.21616871704745166,"the MHA, with one only distinction: the feature matrices multiplied by WQ and WK in the CA layer
101"
OVERVIEW,0.2179261862917399,"are sourced from the output of the Transformer encoder. Further information on the CA layer can be
102"
OVERVIEW,0.21968365553602812,"found in Appendix A.
103"
OVERVIEW,0.22144112478031636,"Feed Forward Network (FFN). Following each MHA layer, there is an FFN layer that takes xM as
104"
OVERVIEW,0.22319859402460457,"input and generates xF as output:
105"
OVERVIEW,0.22495606326889278,"xF = xM + FFN(xn),
xn = norm(xM),
FFN(xn) = W >"
OVERVIEW,0.22671353251318102,"D σ(WUxn),
(3)"
OVERVIEW,0.22847100175746923,"where σ is the activation function, and WU, WD 2 Rdf ⇥d are the parameters of the FFN layer, which
106"
OVERVIEW,0.23022847100175747,"correspond to the up and down matrices, respectively. Here df indicates the number of ﬁlters.
107"
OVERVIEW,0.23198594024604569,"3.2
Basic Idea of TCSP: Subspace Projection
108"
OVERVIEW,0.23374340949033393,"The basic idea of Transformer Compression via Subspace Projection (TCSP) is to project the
109"
OVERVIEW,0.23550087873462214,"Transformer model into a suitable subspace via matrix fusion. To provide a clearer explanation, we
110"
OVERVIEW,0.23725834797891038,"take the linear regression model as an example. Here we denote the original model F as:
111"
OVERVIEW,0.2390158172231986,"F(x) = Wx,
(4)"
OVERVIEW,0.24077328646748683,"where W 2 R1⇥d, x 2 Rd.
112"
OVERVIEW,0.24253075571177504,"Assume that the input x of the model is distributed in a k-dimensional subspace. In other words,
113"
OVERVIEW,0.24428822495606328,"there exists a projection matrix P 2 Rd⇥k such that every x in the dataset satisﬁes x = P ˆx, where
114"
OVERVIEW,0.2460456942003515,"ˆx 2 Rk. Thus, we have,
115"
OVERVIEW,0.2478031634446397,"ˆF(ˆx) = (WP)ˆx = ˆW ˆx,
(5)"
OVERVIEW,0.24956063268892795,"where ˆW = WP 2 R1⇥k represents the compressed weight matrix, ˆx 2 Rk. In this way, we project
116"
OVERVIEW,0.2513181019332162,"the original model from a d-dimensional space into a reduced k-dimensional subspace. We will
117"
OVERVIEW,0.2530755711775044,"elaborate on how to extend this method to the transformer model.
118"
OVERVIEW,0.2548330404217926,"Workﬂow of TCSP. The proposed TCSP comprises three stages, as illustrated in Figure 1. Firstly,
119"
OVERVIEW,0.2565905096660808,"given the training data and the original transformer model (e.g., a ﬁne-tuned T5), we sample a subset
120"
OVERVIEW,0.2583479789103691,"of the training data, feed it into the transformer model to obtain the feature matrix X, and employ
121"
OVERVIEW,0.2601054481546573,"SVD on X to derive the projection matrix P. Then, we project the weight matrices {W} of the
122"
OVERVIEW,0.2618629173989455,"original transformer to a subspace via the fusion of the model’s weight matrices and the projection
123"
OVERVIEW,0.26362038664323373,"matrix, resulting in { ˆW}. Finally, following prior work [14, 15, 16], we ﬁne-tune the compressed
124"
OVERVIEW,0.26537785588752194,"transformer with the entire training data.
125"
METHODOLOGY,0.2671353251318102,"4
Methodology
126"
GENERATING PROJECTION MATRIX,0.2688927943760984,"4.1
Generating Projection Matrix
127"
GENERATING PROJECTION MATRIX,0.27065026362038663,"Motivated by the fact that features produced by the transformer model usually tend to reside in a
128"
GENERATING PROJECTION MATRIX,0.27240773286467485,"low-dimensional subspace, our objective is to carry out the forward pass of the transformer model in
129"
GENERATING PROJECTION MATRIX,0.2741652021089631,"the low-dimensional subspace. To achieve this, the initial step of TCSP focuses on determining the
130"
GENERATING PROJECTION MATRIX,0.2759226713532513,"subspace where the features are located. This can be formalized as a optimization problem:
131"
GENERATING PROJECTION MATRIX,0.27768014059753954,arg min
GENERATING PROJECTION MATRIX,0.27943760984182775,"P
E
x2D N
X i=1"
GENERATING PROJECTION MATRIX,0.281195079086116,kL(1⇠i)(x) −PP >L(1⇠i)(x)k2 F !
GENERATING PROJECTION MATRIX,0.28295254833040423,"s.t.
P >P = I,
(6)"
GENERATING PROJECTION MATRIX,0.28471001757469244,"where N denotes the number of layers in the transformer, L(i) represents the i-th layer in the
132"
GENERATING PROJECTION MATRIX,0.28646748681898065,"transformer model, which can correspond to different components such as the MHA or the FFN
133"
GENERATING PROJECTION MATRIX,0.28822495606326887,"layer. L(1⇠i) = L(i) ◦. . . L(2) ◦L(1) represents the composite function that is formed by sequentially
134"
GENERATING PROJECTION MATRIX,0.28998242530755713,"applying the transformations from the ﬁrst layer to the i-th layer. L(1⇠i)(x) refers to the application
135"
GENERATING PROJECTION MATRIX,0.29173989455184535,"of the composition function L(1⇠i) on the input x, resulting in its corresponding feature. I is an
136"
GENERATING PROJECTION MATRIX,0.29349736379613356,"identity matrix, and P is the desired projection matrix. For a given projection matrix P, P >x can
137"
GENERATING PROJECTION MATRIX,0.29525483304042177,"be interpreted as projecting x from the original space into the subspace corresponding to the matrix
138"
GENERATING PROJECTION MATRIX,0.29701230228471004,"P, while PP >x can be viewed as projecting P >x from the subspace back into the original space.
139"
GENERATING PROJECTION MATRIX,0.29876977152899825,"Therefore, we can use x −PP >x to measure whether the vector x belongs to the subspace.
140"
GENERATING PROJECTION MATRIX,0.30052724077328646,"Eq. 6 is equivalent to the following equation, the proof of which is shown in Appendix B.
141"
GENERATING PROJECTION MATRIX,0.3022847100175747,arg min P
GENERATING PROJECTION MATRIX,0.30404217926186294,kX −PP >Xk2
GENERATING PROJECTION MATRIX,0.30579964850615116,"F
s.t.
P >P = I,
(7)"
GENERATING PROJECTION MATRIX,0.30755711775043937,"where
142 X = ⇥"
GENERATING PROJECTION MATRIX,0.3093145869947276,"L(1)(x1),· · ·, L(1)(xM), L(1⇠2)(x1),· · ·, L(1⇠2)(xM), L(1⇠N)(x1),· · ·, L(1⇠N)(xM) ⇤ . (8)"
GENERATING PROJECTION MATRIX,0.3110720562390158,"Let lm denote the sequence length of the m-th data instance, and d denote the hidden size. Conse-
143"
GENERATING PROJECTION MATRIX,0.31282952548330406,"quently, the shape of X can be expressed as d ⇥(N ⇥PM"
GENERATING PROJECTION MATRIX,0.3145869947275923,"m=1 lm), where N is the number of layers,
144"
GENERATING PROJECTION MATRIX,0.3163444639718805,"and M is the number of the sampled data instances.
145"
GENERATING PROJECTION MATRIX,0.3181019332161687,"Such an optimization problem in Eq. 7 can be well solved by SVD [37].
146"
GENERATING PROJECTION MATRIX,0.31985940246045697,"U, ⌃, V > = SVD(X),
(9)"
GENERATING PROJECTION MATRIX,0.3216168717047452,"where U and V are two orthogonal matrices, and ⌃is a diagonal matrix.
147"
GENERATING PROJECTION MATRIX,0.3233743409490334,"Then, the ﬁrst k columns of the matrix U (i.e., the top-k important eigenvectors of the feature matrix
148"
GENERATING PROJECTION MATRIX,0.3251318101933216,"X) compose the desired projection matrix P.
149"
GENERATING PROJECTION MATRIX,0.3268892794376098,"P = U:,:k
(10)"
GENERATING PROJECTION MATRIX,0.3286467486818981,"Appendix G.1 describes the process of generating the projection matrix P. Notably, although we
150"
GENERATING PROJECTION MATRIX,0.3304042179261863,"sample a subset of data instances for computing feature matrix X, the size of X is still large, resulting
151"
GENERATING PROJECTION MATRIX,0.3321616871704745,"in a signiﬁcant overhead of SVD. To mitigate this, we only select a subset of columns from X to
152"
GENERATING PROJECTION MATRIX,0.3339191564147627,"create a submatrix for SVD. Following previous work [14, 8], we approximate the SVD by sampling
153"
GENERATING PROJECTION MATRIX,0.335676625659051,"2,000 tokens, which corresponds to the number of columns in matrix X. In the experiments, we
154"
GENERATING PROJECTION MATRIX,0.3374340949033392,"examine the impact of the number of sampled tokens on the performance of TCSP.
155"
FUSING PROJECTION AND WEIGHT MATRIX,0.3391915641476274,"4.2
Fusing Projection and Weight Matrix
156"
FUSING PROJECTION AND WEIGHT MATRIX,0.3409490333919156,"Once the projection matrix P for the the transformer model has been obtained, we can construct an
157"
FUSING PROJECTION AND WEIGHT MATRIX,0.3427065026362039,"approximation of the original model within the subspace using the projection matrix. The basic idea
158"
FUSING PROJECTION AND WEIGHT MATRIX,0.3444639718804921,"behind this is to fuse the projection matrix with the weight matrices of the transformer.
159"
FUSING PROJECTION AND WEIGHT MATRIX,0.3462214411247803,"Given an input x, we pass it through the transformer model starting from the ﬁrst layer to the last
160"
FUSING PROJECTION AND WEIGHT MATRIX,0.34797891036906853,"N-th layer, resulting in the corresponding feature vector L(1⇠N)(x). According to Eq.6, we can use
161"
FUSING PROJECTION AND WEIGHT MATRIX,0.34973637961335674,"the projection matrix P to add dimensionality reduction and dimensionality enhancement operations
162"
FUSING PROJECTION AND WEIGHT MATRIX,0.351493848857645,"between each layer, while preserving the ﬁnal outcome, as shown in the following equation,
163"
FUSING PROJECTION AND WEIGHT MATRIX,0.3532513181019332,"L(1⇠N)(x) = L(N) ◦· · · ◦L(2) ◦L(1)
(11)"
FUSING PROJECTION AND WEIGHT MATRIX,0.35500878734622143,"⇡U ◦D ◦L(N) ◦· · · ◦L(2) ◦U ◦D ◦L(1) ◦U ◦D(x)
(12)"
FUSING PROJECTION AND WEIGHT MATRIX,0.35676625659050965,"⇡U ◦ˆL(N) ◦· · · ◦ˆL(2) ◦ˆL(1) ◦D(x)
(13)"
FUSING PROJECTION AND WEIGHT MATRIX,0.3585237258347979,"⇡P ˆL(1⇠N)(P >x),
(14)"
FUSING PROJECTION AND WEIGHT MATRIX,0.3602811950790861,"where U(x) = Px represents the dimensionality enhancement operation, D(x) = P >x represents
164"
FUSING PROJECTION AND WEIGHT MATRIX,0.36203866432337434,"the dimensionality reduction operation, ˆLi = D ◦Li ◦U is the projected layer, and ˆL(1⇠N) =
165"
FUSING PROJECTION AND WEIGHT MATRIX,0.36379613356766255,"ˆL(N) ◦· · · ◦ˆL(2) ◦ˆL(1) is the projected model.
166"
FUSING PROJECTION AND WEIGHT MATRIX,0.3655536028119508,"According to Section 3.1, any original layer L(i)(x) can be formulated as:
167"
FUSING PROJECTION AND WEIGHT MATRIX,0.36731107205623903,"L(x) = x + Layer(norm(x)),
(15)"
FUSING PROJECTION AND WEIGHT MATRIX,0.36906854130052724,"where the function “Layer” can be instantiated by various components such as MHA, CA, and FFN.
168"
FUSING PROJECTION AND WEIGHT MATRIX,0.37082601054481545,"The projected layer ˆL operates on a k-dimensional feature ˆx as input and produces a corresponding
169"
FUSING PROJECTION AND WEIGHT MATRIX,0.37258347978910367,"k-dimensional feature ˆL(ˆx), i.e.
170"
FUSING PROJECTION AND WEIGHT MATRIX,0.37434094903339193,"ˆL(ˆx) = P >(P ˆx + Layer(norm(P ˆx)) = ˆx + P >Layer(norm(P ˆx).
(16)"
FUSING PROJECTION AND WEIGHT MATRIX,0.37609841827768015,"Compared with the d-dimensional input and output vectors of the original layer L(x), the projected
171"
FUSING PROJECTION AND WEIGHT MATRIX,0.37785588752196836,"layer ˆL(ˆx) in Eq.16 reduces the dimensions of the layer’s input and output to k. However, this
172"
FUSING PROJECTION AND WEIGHT MATRIX,0.37961335676625657,"reduction in dimensionality does not alleviate the computational effort. Therefore, we propose two
173"
FUSING PROJECTION AND WEIGHT MATRIX,0.38137082601054484,"methods to compress the parameters within the projected layer: (1) Matrix fusion, which merges
174"
FUSING PROJECTION AND WEIGHT MATRIX,0.38312829525483305,"the multiple consecutive matrix multiplications into a single matrix. (2) Normalization function
175"
FUSING PROJECTION AND WEIGHT MATRIX,0.38488576449912126,"reconstruction, which is used to swap the computational order of matrix operations and normalized
176"
FUSING PROJECTION AND WEIGHT MATRIX,0.3866432337434095,"functions to increase the chance of matrix fusion. Next, we ﬁrst explain the normalization function
177"
FUSING PROJECTION AND WEIGHT MATRIX,0.3884007029876977,"reconstruction and then discuss how the matrix fusion technique can be employed to compress the
178"
FUSING PROJECTION AND WEIGHT MATRIX,0.39015817223198596,"MHA and FFA layers.
179"
FUSING PROJECTION AND WEIGHT MATRIX,0.39191564147627417,"Normalization Function Reconstruction. Within the projected layer ˆL as shown in Eq.16, we
180"
FUSING PROJECTION AND WEIGHT MATRIX,0.3936731107205624,"encounter the computation of norm(P ˆx). Additionally, we know that the operation following
181"
FUSING PROJECTION AND WEIGHT MATRIX,0.3954305799648506,"norm(P ˆx) is a matrix multiplication. Thus, if we can ﬁnd a new matrix ˆP along with a normalization
182"
FUSING PROJECTION AND WEIGHT MATRIX,0.39718804920913886,"function
ˆ
norm that satisﬁes norm(P ˆx) = ˆP
ˆ
norm(ˆx), we can compress the parameters of the
183"
FUSING PROJECTION AND WEIGHT MATRIX,0.3989455184534271,"transformer model by fusing the matrix ˆP with the subsequent matrix.
184"
FUSING PROJECTION AND WEIGHT MATRIX,0.4007029876977153,"The normalization function in T5 is RMSNorm [38]:
185"
FUSING PROJECTION AND WEIGHT MATRIX,0.4024604569420035,"norm(x) = γ
x
1
p"
FUSING PROJECTION AND WEIGHT MATRIX,0.40421792618629176,"dkxk.
(17)"
FUSING PROJECTION AND WEIGHT MATRIX,0.40597539543058,"Therefore, we have
186"
FUSING PROJECTION AND WEIGHT MATRIX,0.4077328646748682,"norm(P ˆx) = γ
P ˆx
1
p"
FUSING PROJECTION AND WEIGHT MATRIX,0.4094903339191564,dkP ˆxk = ( r
FUSING PROJECTION AND WEIGHT MATRIX,0.4112478031634446,"d
k γP)I
ˆx
1
p"
FUSING PROJECTION AND WEIGHT MATRIX,0.4130052724077329,"kkˆxk = ˆP
ˆ
norm(ˆx),
(18)"
FUSING PROJECTION AND WEIGHT MATRIX,0.4147627416520211,where ˆP = q
FUSING PROJECTION AND WEIGHT MATRIX,0.4165202108963093,"d
kγP,
ˆ
norm(ˆx) = I
ˆx
1
p"
FUSING PROJECTION AND WEIGHT MATRIX,0.4182776801405975,"k kˆxk (note that kˆxk = kP ˆxk).
187"
FUSING PROJECTION AND WEIGHT MATRIX,0.4200351493848858,"By incorporating Eq.18 into Eq.16, we can deﬁne
188"
FUSING PROJECTION AND WEIGHT MATRIX,0.421792618629174,"ˆ
Layer(ˆx) = P >Layer( ˆP
ˆ
norm(ˆx)) = P >Layer( ˆP ˆxn),
(19)"
FUSING PROJECTION AND WEIGHT MATRIX,0.4235500878734622,"where
ˆ
norm(ˆx) is abbreviated as as ˆxn.
189"
FUSING PROJECTION AND WEIGHT MATRIX,0.4253075571177504,"For detailed information on reconstructing other normalization functions such as LayerNorm [39]
190"
FUSING PROJECTION AND WEIGHT MATRIX,0.4270650263620387,"and BatchNorm [40], please refer to the appendix D. The post-normalization reconstruction is also
191"
FUSING PROJECTION AND WEIGHT MATRIX,0.4288224956063269,"explained in appendix D.
192"
FUSING PROJECTION AND WEIGHT MATRIX,0.4305799648506151,"MHA Layer Compression. We can compress the transformer model’s parameters by fusing multiple
193"
FUSING PROJECTION AND WEIGHT MATRIX,0.43233743409490333,"consecutive matrix multiplications into a single matrix. For the MHA layer, we have
194"
FUSING PROJECTION AND WEIGHT MATRIX,0.43409490333919154,"ˆ
MHA(ˆxn) = H
X i=1"
FUSING PROJECTION AND WEIGHT MATRIX,0.4358523725834798,P >W (i)>
FUSING PROJECTION AND WEIGHT MATRIX,0.437609841827768,"O
W (i)"
FUSING PROJECTION AND WEIGHT MATRIX,0.43936731107205623,"V
ˆP ˆxn · Softmax((W (i)"
FUSING PROJECTION AND WEIGHT MATRIX,0.44112478031634444,K ˆP ˆxn)>(W (i)
FUSING PROJECTION AND WEIGHT MATRIX,0.4428822495606327,Q ˆP ˆxn)/ p
FUSING PROJECTION AND WEIGHT MATRIX,0.4446397188049209,"d)
(20) = H
X i=1"
FUSING PROJECTION AND WEIGHT MATRIX,0.44639718804920914,ˆW (i)>
FUSING PROJECTION AND WEIGHT MATRIX,0.44815465729349735,"O
ˆW (i)"
FUSING PROJECTION AND WEIGHT MATRIX,0.44991212653778556,V ˆxn · Softmax(( ˆW (i)
FUSING PROJECTION AND WEIGHT MATRIX,0.45166959578207383,K ˆxn)>( ˆW (i)
FUSING PROJECTION AND WEIGHT MATRIX,0.45342706502636204,Q ˆxn)/ p
FUSING PROJECTION AND WEIGHT MATRIX,0.45518453427065025,"d),
(21)"
FUSING PROJECTION AND WEIGHT MATRIX,0.45694200351493847,where ˆW (i)
FUSING PROJECTION AND WEIGHT MATRIX,0.45869947275922673,"O
= W (i)"
FUSING PROJECTION AND WEIGHT MATRIX,0.46045694200351495,"O P, ˆW (i)"
FUSING PROJECTION AND WEIGHT MATRIX,0.46221441124780316,"V
= W (i)"
FUSING PROJECTION AND WEIGHT MATRIX,0.46397188049209137,"V
ˆP, ˆW (i)"
FUSING PROJECTION AND WEIGHT MATRIX,0.46572934973637964,"K
= W (i)"
FUSING PROJECTION AND WEIGHT MATRIX,0.46748681898066785,"K ˆP, ˆW (i)"
FUSING PROJECTION AND WEIGHT MATRIX,0.46924428822495606,"Q
= W (i)"
FUSING PROJECTION AND WEIGHT MATRIX,0.4710017574692443,"Q ˆP. The shape of these
195"
FUSING PROJECTION AND WEIGHT MATRIX,0.4727592267135325,"matrices is transformed from dh ⇥d to dh ⇥k, resulting in a reduction of k/d of the original number
196"
FUSING PROJECTION AND WEIGHT MATRIX,0.47451669595782076,"of parameters. We introduce the CA layer compression in Appendix E.
197"
FUSING PROJECTION AND WEIGHT MATRIX,0.47627416520210897,"FFN Layer Compression. For the FFN layer, we have
198"
FUSING PROJECTION AND WEIGHT MATRIX,0.4780316344463972,"ˆ
FFN(ˆxn) = P >W >"
FUSING PROJECTION AND WEIGHT MATRIX,0.4797891036906854,D σ(WU ˆP ˆxn) = ˆW >
FUSING PROJECTION AND WEIGHT MATRIX,0.48154657293497366,"D σ( ˆWU ˆxn),
(22)"
FUSING PROJECTION AND WEIGHT MATRIX,0.4833040421792619,"where ˆWD = WDP, ˆWU = WU ˆP, which has the same compression rate as the MHA Layer. The
199"
FUSING PROJECTION AND WEIGHT MATRIX,0.4850615114235501,"complete compression process is shown in Appendix G.2.
200"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.4868189806678383,"4.3
Combining TCSP with Other Compression Algorithms
201"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.48857644991212656,"TCSP compresses the hidden size d, and we can further compress the number of ﬁlters df and
202"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.4903339191564148,"attention head size dh by the prior pruning and low-rank factorization methods.
203"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.492091388400703,"Filter Pruning. We can remove the ﬁlters in the FFN layer by pruning. Following prior work [13, 8],
204"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.4938488576449912,"we introduce mask variables diag(m) into the FFN layer, where m indicates which ﬁlters in the FFN
205"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.4956063268892794,"layer are to be retained.
206"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.4973637961335677,FFN(x; m) = W >
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.4991212653778559,"D diag(m)σ(WUx).
(23)"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5008787346221442,"Then the pruning problem can be formalized as an optimization problem on the mask.
207"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5026362038664324,arg min
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5043936731107206,"m
L(m)
s.t.
cost(m) C.
(24)"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5061511423550088,"Our proposed TCSP method only involves the forward computation process of the model. However,
208"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.507908611599297,"most prior methods ﬁnd the optimal mask by leveraging the derivative of the loss function with respect
209"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5096660808435852,to the mask variable @L
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5114235500878734,"@m. As a result, TCSP cannot be directly integrated into the same framework as
210"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5131810193321616,"previous pruning algorithms. To address this, we propose TCSP-pruning, a ﬁlter pruning algorithm
211"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5149384885764499,"that exclusively relies on the forward process of the model, enabling the unifying of the pruning
212"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5166959578207382,"algorithm into the TCSP framework.
213"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5184534270650264,"TCSP-pruning employs a greedy algorithm to remove the least important ﬁlter at each step based on
214"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5202108963093146,"the following score function:
215"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5219683655536028,"score(i) = |WD,i| ⇤(E[σ(WU,ix)] + std[σ(WU,ix)]),
(25)"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.523725834797891,"where score(i) represents the signiﬁcance of the i-th ﬁlter. We believe that the importance of the i-th
216"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5254833040421792,"ﬁlter is inﬂuenced by the factors such as the norm of WD,i (the weight matrix associated with the
217"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5272407732864675,"i-th ﬁlter), the average activation value, and the variance of activation values. The pseudo-code of
218"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5289982425307557,"TCSP-pruning is shown in the Appendix G.3.
219"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5307557117750439,"Head Size Compression. Based on the low-rank property of data distribution, we can further
220"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5325131810193322,"compress the head size of the MHA layer. The objective can be expressed as:
221"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5342706502636204,"arg min M
E"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5360281195079086,x k(WKx)>(WQx) −(WKx)>M(WQx)k2
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5377855887521968,"F
s.t.
rank(M) = k,
(26)"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.539543057996485,"arg min M
E"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5413005272407733,x kW >
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5430579964850615,O WV x −W > O M
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5448154657293497,0WV xk2
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.546572934973638,"F
s.t.
rank(M"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5483304042179262,"0) = k.
(27)"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5500878734622144,"DRONE [14] gives the solution to the above two optimization problems. Assuming that the optimal
222"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5518453427065027,"solution obtained from the ﬁrst optimization problem is M ⇤. Afterward, we decompose M ⇤into the
223"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5536028119507909,product of two matrices M ⇤= U >
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5553602811950791,"MVM through SVD. Then, we can compress matrices WQ, WK
224"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5571177504393673,"through UM and VM ( ˆ
WK = UMWK, ˆ
WQ = VMWQ). The second problem is addressed in the
225"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5588752196836555,same way. We can also formalize Eq.7 as arg minM kX −MXk2
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.5606326889279437,"F s.t. rank(M) = k, but the
226"
COMBINING TCSP WITH OTHER COMPRESSION ALGORITHMS,0.562390158172232,"optimal solution obtained by both formulas is the same.
227"
EXPERIMENT,0.5641476274165202,"5
Experiment
228"
EXPERIMENTAL SETUP,0.5659050966608085,"5.1
Experimental Setup
229"
EXPERIMENTAL SETUP,0.5676625659050967,"To evaluate our method, we ﬁrst ﬁne-tune T5base and BERTbase on the training data of each task
230"
EXPERIMENTAL SETUP,0.5694200351493849,"of the GLUE[41] and SQuAD [42] benchmarks to obtain the baseline models, and then adopt the
231"
EXPERIMENTAL SETUP,0.5711775043936731,"proposed TCSP or the comparison compression methods to compress these baseline models. On each
232"
EXPERIMENTAL SETUP,0.5729349736379613,"task, for TCSP, we sample 2000 instances from its training data to produce the project matrix, using it
233"
EXPERIMENTAL SETUP,0.5746924428822495,"to compress the baseline models and ﬁnally ﬁne-tune the compressed models using the entire training
234"
EXPERIMENTAL SETUP,0.5764499121265377,"data. For the GLUE benchmark, We report accuracy for the MNLI [43], QQP [44], QNLI [41], and
235"
EXPERIMENTAL SETUP,0.5782073813708261,"SST2 [45] tasks, as well as F1 score and spearman correlation for the MRPC [46] and STSB [41]
236"
EXPERIMENTAL SETUP,0.5799648506151143,"tasks. For the SQuAD benchmark, we report the F1 score. For more comprehensive information
237"
EXPERIMENTAL SETUP,0.5817223198594025,"regarding the experimental setup, please refer to Appendix F.
238"
EXPERIMENTAL SETUP,0.5834797891036907,"Table 1: Performance of TCSP on T5base and BERTbase with various compression rate. “Avg. Diff”
refers to the average accuracy difference observed before and after applying model compression on
the GLUE and SQuAD datasets. “Speed Up” represents the rate of inference time speedup achieved
by “w TCSP{25%, 25%}” compared to the baseline model. “ft.” denotes ﬁne-tuning."
EXPERIMENTAL SETUP,0.5852372583479789,"MNLI
QQP
QNLI
SST-2
STS-B
MRPC
SQuAD1.1
SQuAD2.0
Avg. Diff"
EXPERIMENTAL SETUP,0.5869947275922671,"T5base
86.8
91.4
93.2
94.5
90.0
91.9
88.6
79.3
w TCSP {25%, 0%}
85.0
90.1
91.4
93.0
88.9
89.1
83.9
71.9
w TCSP {25%, 0%} + ft.
86.2
91.2
92.5
93.2
90.1
91.3
86.8
78.0
-0.6/-1.3
w TCSP {25%, 25%}
83.5
88.0
90.8
91.4
86.6
90.4
79.2
66.9
w TCSP {25%, 25%} + ft.
85.5
90.7
91.8
92.8
89.5
91.5
87.3
77.7
-1.0/-1.5"
EXPERIMENTAL SETUP,0.5887521968365553,"Speed Up
⇥1.25
⇥1.06
⇥1.08
⇥1.36
⇥1.22
⇥1.25
⇥1.15
⇥1.20"
EXPERIMENTAL SETUP,0.5905096660808435,"BERTbase
84.4
91.1
91.4
92.2
88.4
89.9
88.5
75.8
w TCSP {25%, 0%}
31.8
68.7
49.5
49.2
68.5
1.0
13.5
11.2
w TCSP {25%, 0%} + ft.
83.7
91.0
91.0
92.2
88.5
90.1
86.7
76.6
-0.2/-0.5
w TCSP {25%, 25%}
31.8
67.6
49.8
49.1
67.5
0.0
12.9
10.6
w TCSP {25%, 25%} + ft.
83.5
90.8
90.6
91.9
87.7
89.1
87.7
76.0
-0.6/-0.3"
EXPERIMENTAL SETUP,0.5922671353251318,"Speed Up
⇥1.16
⇥1.02
⇥1.62
⇥2.36
⇥1.96
⇥3.93
⇥1.36
⇥1.36"
EXPERIMENTAL SETUP,0.5940246045694201,Table 2: Performance comparison with prior compression methods using BERTbase as the baseline.
EXPERIMENTAL SETUP,0.5957820738137083,"Sajjad et al.(33.3%)
Kwon et al.(40%)
DRONE(-)
FWSVD(40%)
TFWSVD(40%)
TCSP(40%)"
EXPERIMENTAL SETUP,0.5975395430579965,"QQP
90.6(-0.5)
90.4(-0.6)
90.1(-0.8)
87.6⇤(−0.2)
86.9⇤(−0.9)
90.8(-0.3)
QNLI
89.7(-1.4)
90.0(-1.4)
89.3(-2.1)
89.5(-1.8)
90.3(-1.0)
90.6(-0.8)
SST-2
90.6(-1.8)
92.5(-1.1)
90.8(-1.5)
91.2(-1.8))
91.1(-1.9)
91.1(-1.1)
MRPC
79.4(-8.6)
85.3(-1.0)
88.0(-1.5)
88.0(+0.6)
89.0(+1.6)
89.1(-0.8)"
PERFORMANCE EVALUATION,0.5992970123022847,"5.2
Performance Evaluation
239"
PERFORMANCE EVALUATION,0.6010544815465729,"Table 1 shows the accuracy results of T5base and BERTbase with different compression rates. The
240"
PERFORMANCE EVALUATION,0.6028119507908611,"proposed TCSP can reduce the hidden size d. Additionally, in Section 4.3, we introduce a ﬁlter
241"
PERFORMANCE EVALUATION,0.6045694200351494,"pruning method that can reduce the number of ﬁlers df. Moreover, the existing DRONE [14] can
242"
PERFORMANCE EVALUATION,0.6063268892794376,"compress the attention head size dh. In TCSP{a%, b%}, the notation a% denotes the compress rate
243"
PERFORMANCE EVALUATION,0.6080843585237259,"of hidden size, while b% denotes the compress rate of the attention head size plus the number of
244"
PERFORMANCE EVALUATION,0.6098418277680141,"ﬁlters. Consequently, the overall compression rate of the model is a% + b% - a% * b%.
245"
PERFORMANCE EVALUATION,0.6115992970123023,"When employing TCSP with a compression rate of {25%, 0%} which retains 75% hidden size while
246"
PERFORMANCE EVALUATION,0.6133567662565905,"preserving the original attention head size and number of ﬁlters, we observe that the compressed
247"
PERFORMANCE EVALUATION,0.6151142355008787,"T5base and BERTbase models exhibit a maximum drop in accuracy of only 1.3% on the GLUE and
248"
PERFORMANCE EVALUATION,0.616871704745167,"SQuAD datasets. Building upon this, We further apply ﬁlter pruning and compress the head size,
249"
PERFORMANCE EVALUATION,0.6186291739894552,"resulting in a model denoted as TCSP {25%, 25%}. Remarkably, this additional compression does
250"
PERFORMANCE EVALUATION,0.6203866432337434,"not negatively impact the baseline performance, highlighting the compatibility of TCSP with the
251"
PERFORMANCE EVALUATION,0.6221441124780316,"ﬁlter pruning and attention head size compression methods. Overall, we achieve a compression
252"
PERFORMANCE EVALUATION,0.6239015817223199,"rate of 44% for both T5base and BERTbase models with only a 1.6% loss in accuracy. Like prior
253"
PERFORMANCE EVALUATION,0.6256590509666081,"work [14, 15, 16], ﬁne-tuning is necessary for ensuring performance. For more detailed information
254"
PERFORMANCE EVALUATION,0.6274165202108963,"on model performance at various compression rates, please refer to Appendix F.5.
255"
COMPARISON WITH PRIOR METHODS,0.6291739894551845,"5.3
Comparison with Prior Methods
256"
COMPARISON WITH PRIOR METHODS,0.6309314586994728,"We conduct a comprehensive comparison of TCSP with prior methods, considering both their
257"
COMPARISON WITH PRIOR METHODS,0.632688927943761,"performance and compression time cost.
258"
COMPARISON WITH PRIOR METHODS,0.6344463971880492,"Following the setting adopted by Kwon et al. [8], we compare TCSP with prior pruning methods and
259"
COMPARISON WITH PRIOR METHODS,0.6362038664323374,"low-rank factorization methods. The evaluation involves compressing models with compression rate
260"
COMPARISON WITH PRIOR METHODS,0.6379613356766256,"constraints on four tasks within the GLUE benchmark: QQP, QNLI, SST-2, and MRPC. Notably, we
261"
COMPARISON WITH PRIOR METHODS,0.6397188049209139,"conduct this evaluation without employing knowledge distillation. The comparison methods include
262"
COMPARISON WITH PRIOR METHODS,0.6414762741652021,"Sajjad et al. [47], Kwon et al. [8], DRONE [14], FWSVD [15], and TFWSVD [16]. To ensure a fair
263"
COMPARISON WITH PRIOR METHODS,0.6432337434094904,"comparison, we select BERTbase as the baseline model, as it is commonly employed across all the
264"
COMPARISON WITH PRIOR METHODS,0.6449912126537786,"comparison methods. To assess the impact of compression, we compare the amount of accuracy drop
265"
COMPARISON WITH PRIOR METHODS,0.6467486818980668,"Table 3: Compression time cost comparison with prior compression methods using BERTbase as
the baseline evaluated on the MNLI dataset. “# Epochs” represents the number of epochs to ﬁen-tune
the model weights on the entire training data."
COMPARISON WITH PRIOR METHODS,0.648506151142355,"DynaBERT [23]
EBERT [11]
BMP [12]
CoFi [13]
Kwon et al. [8]
TCSP"
COMPARISON WITH PRIOR METHODS,0.6502636203866432,"Time cost (hr)
12
5
17
33
0.01
2.16 (0.16 + 2.00)
# Epochs
4
6
20
40
0
2"
COMPARISON WITH PRIOR METHODS,0.6520210896309314,Table 4: Effect of sampled tokens.
COMPARISON WITH PRIOR METHODS,0.6537785588752196,"Number of tokens
QQP
QNLI
SST-2
MRPC"
K,0.655536028119508,"1K
91.2
92.3
93.8
91.5
2K
91.2
92.5
93.2
91.3
4K
91.2
92.5
93.7
91.8"
K,0.6572934973637962,Table 5: Effect of SVD
K,0.6590509666080844,"Method
QQP
QNLI
SST-2
MRPC"
K,0.6608084358523726,"TCSP-SVD
91.2
92.5
93.2
91.3
TCSP-Random
63.1
49.4
49.0
0.0"
K,0.6625659050966608,"experienced by each method since the absolute accuracy of the baseline BERTbase may vary slightly
266"
K,0.664323374340949,"across different papers. The compression rate is indicated in parentheses after each method’s name,
267"
K,0.6660808435852372,"except for DRONE, as their paper does not explicitly report the compression rate.
268"
K,0.6678383128295254,"Table 2 presents the performance of the compressed models and the difference from their correspond-
269"
K,0.6695957820738138,"ing baselines for all the compression methods under comparison. The performance of FWSVD and
270"
K,0.671353251318102,"TFWSVD is marked with an asterisk since they use the F1 metric instead of accuracy speciﬁcally on
271"
K,0.6731107205623902,"the QQP dataset. However, this metric does not impact the comparison of the metric difference. Our
272"
K,0.6748681898066784,"method TCSP demonstrates comparable or superior results compared to the prior methods (A lower
273"
K,0.6766256590509666,"accuracy drop indicates better performance).
274"
K,0.6783831282952548,"To assess the time cost, we evaluate the performance on the MNLI dataset, which is larger than other
275"
K,0.680140597539543,"datasets in GLUE. Table 3 shows that TCSP requires only 0.16 hours for model compression and an
276"
K,0.6818980667838312,"additional 2 hours to model ﬁne-tuning. This indicates that TCSP is signiﬁcantly faster than most
277"
K,0.6836555360281195,"of the comparison methods, with the exception of the pruning method proposed in Kwon et al. [8].
278"
K,0.6854130052724078,"However, the pruning method can be effectively combined with TCSP, allowing for compatibility and
279"
K,0.687170474516696,"further optimization.
280"
ABLATION STUDY,0.6889279437609842,"5.4
Ablation Study
281"
ABLATION STUDY,0.6906854130052724,"To prevent the computation of SVD on excessively large matrices, We sample a subset of columns
282"
ABLATION STUDY,0.6924428822495606,"from the feature matrix. In other words, we randomly choose several tokens within each instance to
283"
ABLATION STUDY,0.6942003514938488,"compute the projection matrix. Consequently, we evaluate the effect of the sampled tokens on the
284"
ABLATION STUDY,0.6959578207381371,"compression performance. Table 4 presents the results of this analysis, revealing that satisfactory
285"
ABLATION STUDY,0.6977152899824253,"outcomes can be achieved when the number of tokens (1,000) is greater than the hidden size of the
286"
ABLATION STUDY,0.6994727592267135,"model (768).
287"
ABLATION STUDY,0.7012302284710018,"Furthermore, we explore the inﬂuence of SVD. In Table 5, we replace the projection matrix generated
288"
ABLATION STUDY,0.70298769771529,"by SVD with a randomly generated matrix and show the experimental results in Table 5. It is observed
289"
ABLATION STUDY,0.7047451669595782,"that using a random matrix for compression signiﬁcantly diminishes the model’s performance.
290"
ABLATION STUDY,0.7065026362038664,"Therefore, it is necessary to use SVD to compute the projection matrix.
291"
CONCLUSION,0.7082601054481547,"6
Conclusion
292"
CONCLUSION,0.7100175746924429,"This paper proposes TCSP, a method for compressing the transformer model by leveraging subspace
293"
CONCLUSION,0.7117750439367311,"projection. TCSP employs SVD on the feature matrix of sampled data instances to derive a projection
294"
CONCLUSION,0.7135325131810193,"matrix. By fusing this matrix into the transformer’s weight matrix, we obtain a compressed model.
295"
CONCLUSION,0.7152899824253075,"The model is subsequently ﬁne-tuned using the entire dataset. TCSP is applied to both T5base and
296"
CONCLUSION,0.7170474516695958,"BERTbase and evaluated on GLUE and SQuAD datasets. Remarkably, TCSP achieves a compression
297"
CONCLUSION,0.718804920913884,"ratio of 44% while incurring only a 1.6% decrease in accuracy. As TCSP primarily focuses on
298"
CONCLUSION,0.7205623901581723,"compressing the hidden size, it can be easily combined with other methods that compress the number
299"
CONCLUSION,0.7223198594024605,"of ﬁlters and the attention head size, making it highly compatible.
300"
REFERENCES,0.7240773286467487,"References
301"
REFERENCES,0.7258347978910369,"[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
302"
REFERENCES,0.7275922671353251,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
303"
REFERENCES,0.7293497363796133,"processing systems, 30, 2017. 1
304"
REFERENCES,0.7311072056239016,"[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
305"
REFERENCES,0.7328646748681898,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
306"
REFERENCES,0.7346221441124781,"An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
307"
REFERENCES,0.7363796133567663,"arXiv:2010.11929, 2020. 1
308"
REFERENCES,0.7381370826010545,"[3] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
309"
REFERENCES,0.7398945518453427,"Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
310"
REFERENCES,0.7416520210896309,"of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. 1
311"
REFERENCES,0.7434094903339191,"[4] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
312"
REFERENCES,0.7451669595782073,"Hervé Jégou. Training data-efﬁcient image transformers & distillation through attention. In
313"
REFERENCES,0.7469244288224957,"International conference on machine learning, pages 10347–10357. PMLR, 2021. 1
314"
REFERENCES,0.7486818980667839,"[5] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
wav2vec 2.0:
315"
REFERENCES,0.7504393673110721,"A framework for self-supervised learning of speech representations.
Advances in neural
316"
REFERENCES,0.7521968365553603,"information processing systems, 33:12449–12460, 2020. 1
317"
REFERENCES,0.7539543057996485,"[6] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li,
318"
REFERENCES,0.7557117750439367,"Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-
319"
REFERENCES,0.7574692442882249,"training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing,
320"
REFERENCES,0.7592267135325131,"16(6):1505–1518, 2022. 1
321"
REFERENCES,0.7609841827768014,"[7] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,
322"
REFERENCES,0.7627416520210897,"and Abdelrahman Mohamed.
Hubert: Self-supervised speech representation learning by
323"
REFERENCES,0.7644991212653779,"masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language
324"
REFERENCES,0.7662565905096661,"Processing, 29:3451–3460, 2021. 1
325"
REFERENCES,0.7680140597539543,"[8] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt Keutzer, and
326"
REFERENCES,0.7697715289982425,"Amir Gholami. A fast post-training pruning framework for transformers. arXiv preprint
327"
REFERENCES,0.7715289982425307,"arXiv:2204.09656, 2022. 1, 2, 4.1, 4.3, 5.3, 3, F.2, F.3, 6
328"
REFERENCES,0.773286467486819,"[9] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models.
329"
REFERENCES,0.7750439367311072,"In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-
330"
REFERENCES,0.7768014059753954,"ing (EMNLP), pages 6151–6162, Online, November 2020. Association for Computational
331"
REFERENCES,0.7785588752196837,"Linguistics. 1, 2, 6
332"
REFERENCES,0.7803163444639719,"[10] Zi Lin, Jeremiah Liu, Zi Yang, Nan Hua, and Dan Roth. Pruning redundant mappings in
333"
REFERENCES,0.7820738137082601,"transformer models via spectral-normalized identity prior. In Findings of the Association for
334"
REFERENCES,0.7838312829525483,"Computational Linguistics: EMNLP 2020, pages 719–730, Online, November 2020. Association
335"
REFERENCES,0.7855887521968365,"for Computational Linguistics. 1, 2, 6
336"
REFERENCES,0.7873462214411248,"[11] Zejian Liu, Fanrong Li, Gang Li, and Jian Cheng. EBERT: Efﬁcient BERT inference with
337"
REFERENCES,0.789103690685413,"dynamic structured pruning. In Findings of the Association for Computational Linguistics:
338"
REFERENCES,0.7908611599297012,"ACL-IJCNLP 2021, pages 4814–4823, Online, August 2021. Association for Computational
339"
REFERENCES,0.7926186291739895,"Linguistics. 1, 2, 3, 6
340"
REFERENCES,0.7943760984182777,"[12] François Lagunas, Ella Charlaix, Victor Sanh, and Alexander Rush. Block pruning for faster
341"
REFERENCES,0.7961335676625659,"transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
342"
REFERENCES,0.7978910369068541,"Processing, pages 10619–10629, Online and Punta Cana, Dominican Republic, November 2021.
343"
REFERENCES,0.7996485061511424,"Association for Computational Linguistics. 1, 2, 3, 6
344"
REFERENCES,0.8014059753954306,"[13] Mengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning learns compact and accurate
345"
REFERENCES,0.8031634446397188,"models. In Proceedings of the 60th Annual Meeting of the Association for Computational
346"
REFERENCES,0.804920913884007,"Linguistics (Volume 1: Long Papers), pages 1513–1528, 2022. 1, 2, 4.3, 3
347"
REFERENCES,0.8066783831282952,"[14] Patrick Chen, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui Hsieh. Drone: Data-aware low-
348"
REFERENCES,0.8084358523725835,"rank compression for large nlp models. Advances in neural information processing systems,
349"
REFERENCES,0.8101933216168717,"34:29321–29334, 2021. 1, 2, 3.2, 4.1, 4.3, 5.2, 5.3, 6
350"
REFERENCES,0.81195079086116,"[15] Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, and Hongxia Jin. Language
351"
REFERENCES,0.8137082601054482,"model compression with weighted low-rank factorization. In International Conference on
352"
REFERENCES,0.8154657293497364,"Learning Representations. 1, 2, 3.2, 5.2, 5.3, 6
353"
REFERENCES,0.8172231985940246,"[16] Ting Hua, Yen-Chang Hsu, Felicity Wang, Qian Lou, Yilin Shen, and Hongxia Jin. Numerical
354"
REFERENCES,0.8189806678383128,"optimizations for weighted low-rank estimation on language models. In Proceedings of the
355"
REFERENCES,0.820738137082601,"2022 Conference on Empirical Methods in Natural Language Processing, pages 1404–1416,
356"
REFERENCES,0.8224956063268892,"Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
357"
REFERENCES,0.8242530755711776,"1, 2, 3.2, 5.2, 5.3, 6
358"
REFERENCES,0.8260105448154658,"[17] Hao Yu and Jianxin Wu. Compressing transformers: Features are low-rank, but weights are not!
359"
REFERENCES,0.827768014059754,"2023. 1, 2
360"
REFERENCES,0.8295254833040422,"[18] Canwen Xu and Julian McAuley. A survey on model compression and acceleration for pretrained
361"
REFERENCES,0.8312829525483304,"language models, 2022. 2
362"
REFERENCES,0.8330404217926186,"[19] Elias Frantar, Saleh Ashkboos, Torsten Hoeﬂer, and Dan Alistarh. Gptq: Accurate post-training
363"
REFERENCES,0.8347978910369068,"quantization for generative pre-trained transformers, 2023. 2
364"
REFERENCES,0.836555360281195,"[20] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang,
365"
REFERENCES,0.8383128295254832,"and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In
366"
REFERENCES,0.8400702987697716,"International Conference on Learning Representations. 2
367"
REFERENCES,0.8418277680140598,"[21] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert:
368"
REFERENCES,0.843585237258348,"Integer-only bert quantization. In International conference on machine learning, pages 5506–
369"
REFERENCES,0.8453427065026362,"5518. PMLR, 2021. 2
370"
REFERENCES,0.8471001757469244,"[22] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version
371"
REFERENCES,0.8488576449912126,"of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019. 2
372"
REFERENCES,0.8506151142355008,"[23] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic
373"
REFERENCES,0.8523725834797891,"bert with adaptive width and depth. Advances in Neural Information Processing Systems,
374"
REFERENCES,0.8541300527240774,"33:9782–9793, 2020. 2, 3, 6
375"
REFERENCES,0.8558875219683656,"[24] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
376"
REFERENCES,0.8576449912126538,"Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv
377"
REFERENCES,0.859402460456942,"preprint arXiv:1909.11942, 2019. 2
378"
REFERENCES,0.8611599297012302,"[25] Yingce Xia, Tianyu He, Xu Tan, Fei Tian, Di He, and Tao Qin. Tied transformers: Neural
379"
REFERENCES,0.8629173989455184,"machine translation with shared encoder and decoder. In Proceedings of the AAAI conference
380"
REFERENCES,0.8646748681898067,"on artiﬁcial intelligence, volume 33, pages 5466–5473, 2019. 2
381"
REFERENCES,0.8664323374340949,"[26] Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, and Dawei Song.
382"
REFERENCES,0.8681898066783831,"A tensorized transformer for language modeling. Advances in neural information processing
383"
REFERENCES,0.8699472759226714,"systems, 32, 2019. 2
384"
REFERENCES,0.8717047451669596,"[27] Matan Ben Noach and Yoav Goldberg. Compressing pre-trained language models by matrix
385"
REFERENCES,0.8734622144112478,"decomposition.
In Proceedings of the 1st Conference of the Asia-Paciﬁc Chapter of the
386"
REFERENCES,0.875219683655536,"Association for Computational Linguistics and the 10th International Joint Conference on
387"
REFERENCES,0.8769771528998243,"Natural Language Processing, pages 884–889, 2020. 2
388"
REFERENCES,0.8787346221441125,"[28] Marzieh Tahaei, Ella Charlaix, Vahid Nia, Ali Ghodsi, and Mehdi Rezagholizadeh. Kronecker-
389"
REFERENCES,0.8804920913884007,"BERT: Signiﬁcant compression of pre-trained language models through kronecker decomposi-
390"
REFERENCES,0.8822495606326889,"tion and knowledge distillation. In Proceedings of the 2022 Conference of the North American
391"
REFERENCES,0.8840070298769771,"Chapter of the Association for Computational Linguistics: Human Language Technologies,
392"
REFERENCES,0.8857644991212654,"pages 2116–2127, Seattle, United States, July 2022. Association for Computational Linguistics.
393 2
394"
REFERENCES,0.8875219683655536,"[29] Ali Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Nia, James Clark, and Mehdi Reza-
395"
REFERENCES,0.8892794376098418,"gholizadeh. Kronecker decomposition for gpt compression. In Proceedings of the 60th Annual
396"
REFERENCES,0.8910369068541301,"Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages
397"
REFERENCES,0.8927943760984183,"219–226, 2022. 2
398"
REFERENCES,0.8945518453427065,"[30] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv
399"
REFERENCES,0.8963093145869947,"preprint arXiv:1902.09574, 2019. 2
400"
REFERENCES,0.8980667838312829,"[31] Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by
401"
REFERENCES,0.8998242530755711,"ﬁne-tuning. Advances in Neural Information Processing Systems, 33:20378–20389, 2020. 2
402"
REFERENCES,0.9015817223198594,"[32] Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran,
403"
REFERENCES,0.9033391915641477,"Michael Goin, and Dan Alistarh. The optimal BERT surgeon: Scalable and accurate second-
404"
REFERENCES,0.9050966608084359,"order pruning for large language models. In Proceedings of the 2022 Conference on Empirical
405"
REFERENCES,0.9068541300527241,"Methods in Natural Language Processing, pages 4163–4181, Abu Dhabi, United Arab Emirates,
406"
REFERENCES,0.9086115992970123,"December 2022. Association for Computational Linguistics. 2
407"
REFERENCES,0.9103690685413005,"[33] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Training pruned neural
408"
REFERENCES,0.9121265377855887,"networks. CoRR, abs/1803.03635, 2018. 2
409"
REFERENCES,0.9138840070298769,"[34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
410"
REFERENCES,0.9156414762741653,"Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed
411"
REFERENCES,0.9173989455184535,"text-to-text transformer. CoRR, abs/1910.10683, 2019. 3.1
412"
REFERENCES,0.9191564147627417,"[35] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
413"
REFERENCES,0.9209138840070299,"deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-
414"
REFERENCES,0.9226713532513181,"ence of the North American Chapter of the Association for Computational Linguistics: Human
415"
REFERENCES,0.9244288224956063,"Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,
416"
REFERENCES,0.9261862917398945,"Minnesota, June 2019. Association for Computational Linguistics. 3.1
417"
REFERENCES,0.9279437609841827,"[36] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
418"
REFERENCES,0.929701230228471,"models are unsupervised multitask learners. 2019. 3.1
419"
REFERENCES,0.9314586994727593,"[37] Virginia Klema and Alan Laub. The singular value decomposition: Its computation and some
420"
REFERENCES,0.9332161687170475,"applications. IEEE Transactions on automatic control, 25(2):164–176, 1980. 4.1
421"
REFERENCES,0.9349736379613357,"[38] Biao Zhang and Rico Sennrich. Root mean square layer normalization. CoRR, abs/1910.07467,
422"
REFERENCES,0.9367311072056239,"2019. 4.2
423"
REFERENCES,0.9384885764499121,"[39] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. 4.2
424"
REFERENCES,0.9402460456942003,"[40] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
425"
REFERENCES,0.9420035149384886,"by reducing internal covariate shift. CoRR, abs/1502.03167, 2015. 4.2
426"
REFERENCES,0.9437609841827768,"[41] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
427"
REFERENCES,0.945518453427065,"GLUE: A multi-task benchmark and analysis platform for natural language understanding.
428"
REFERENCES,0.9472759226713533,"In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting
429"
REFERENCES,0.9490333919156415,"Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for
430"
REFERENCES,0.9507908611599297,"Computational Linguistics. 5.1, F.1, F.2
431"
REFERENCES,0.9525483304042179,"[42] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ ques-
432"
REFERENCES,0.9543057996485061,"tions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical
433"
REFERENCES,0.9560632688927944,"Methods in Natural Language Processing, pages 2383–2392, Austin, Texas, November 2016.
434"
REFERENCES,0.9578207381370826,"Association for Computational Linguistics. 5.1, F.1, F.2
435"
REFERENCES,0.9595782073813708,"[43] Seonhoon Kim, Inho Kang, and Nojun Kwak. Semantic sentence matching with densely-
436"
REFERENCES,0.961335676625659,"connected recurrent and co-attentive information. In Proceedings of the AAAI conference on
437"
REFERENCES,0.9630931458699473,"artiﬁcial intelligence, volume 33, pages 6586–6593, 2019. 5.1
438"
REFERENCES,0.9648506151142355,"[44] Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective matching for natural
439"
REFERENCES,0.9666080843585237,"language sentences. arXiv preprint arXiv:1702.03814, 2017. 5.1
440"
REFERENCES,0.968365553602812,"[45] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y
441"
REFERENCES,0.9701230228471002,"Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a
442"
REFERENCES,0.9718804920913884,"sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural
443"
REFERENCES,0.9736379613356766,"language processing, pages 1631–1642, 2013. 5.1
444"
REFERENCES,0.9753954305799648,"[46] William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential
445"
REFERENCES,0.9771528998242531,"paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005),
446"
REFERENCES,0.9789103690685413,"2005. 5.1
447"
REFERENCES,0.9806678383128296,"[47] Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. On the effect of dropping layers
448"
REFERENCES,0.9824253075571178,"of pre-trained transformer models. Computer Speech & Language, 77:101429, 2023. 5.3, 6
449"
REFERENCES,0.984182776801406,"[48] Andrzej Ma´ckiewicz and Waldemar Ratajczak. Principal components analysis (pca). Computers
450"
REFERENCES,0.9859402460456942,"& Geosciences, 19(3):303–342, 1993. B
451"
REFERENCES,0.9876977152899824,"[49] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
452"
REFERENCES,0.9894551845342706,"Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s
453"
REFERENCES,0.9912126537785588,"transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019. F.1
454"
REFERENCES,0.9929701230228472,"[50] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable
455"
REFERENCES,0.9947275922671354,"questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for
456"
REFERENCES,0.9964850615114236,"Computational Linguistics (Volume 2: Short Papers), pages 784–789, Melbourne, Australia,
457"
REFERENCES,0.9982425307557118,"July 2018. Association for Computational Linguistics. F.1, F.2
458"
