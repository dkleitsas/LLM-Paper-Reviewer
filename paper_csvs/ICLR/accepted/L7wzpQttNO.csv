Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002004008016032064,"Diffusion probabilistic models (DPMs) and their extensions have emerged as com-
petitive generative models yet confront challenges of efﬁcient sampling.
We
propose a new bilateral denoising diffusion model (BDDM) that parameterizes
both the forward and reverse processes with a schedule network and a score
network, which can train with a novel bilateral modeling objective. We show
that the new surrogate objective can achieve a lower bound of the log marginal
likelihood tighter than a conventional surrogate. We also ﬁnd that BDDM al-
lows inheriting pre-trained score network parameters from any DPMs and con-
sequently enables speedy and stable learning of the schedule network and op-
timization of a noise schedule for sampling. Our experiments demonstrate that
BDDMs can generate high-ﬁdelity audio samples with as few as three sam-
pling steps. Moreover, compared to other state-of-the-art diffusion-based neu-
ral vocoders, BDDMs produce comparable or higher quality samples indistin-
guishable from human speech, notably with only seven sampling steps (143x
faster than WaveGrad and 28.6x faster than DiffWave). We release our code at
https://github.com/tencent-ailab/bddm."
INTRODUCTION,0.004008016032064128,"1
INTRODUCTION"
INTRODUCTION,0.006012024048096192,"Deep generative models have shown a tremendous advancement in speech synthesis (van den Oord
et al., 2016; Kalchbrenner et al., 2018; Prenger et al., 2019; Kumar et al., 2019; Kong et al., 2020b;
Chen et al., 2020; Kong et al., 2021). Successful generative models can be mainly divided into two
categories: generative adversarial network (GAN) (Goodfellow et al., 2014) based and likelihood-
based. The former is based on adversarial learning, where the objective is to generate data indistin-
guishable from the training data. Yet, the training GANs can be very unstable, and the relevant train-
ing objectives are not suitable to compare against different GANs. The latter uses log-likelihood or
surrogate objectives for training, but they also have intrinsic limitations regarding generation speed
or quality. For example, the autoregressive models (van den Oord et al., 2016; Kalchbrenner et al.,
2018), while being capable of generating high-ﬁdelity data, are limited by their inherently slow
sampling process and the poor scaling properties on high-dimensional data. Likewise, the ﬂow-
based models (Dinh et al., 2016; Kingma & Dhariwal, 2018; Chen et al., 2018; Papamakarios et al.,
2021) rely on specialized architectures to build a normalized probability model, whose training is
less parameter-efﬁcient. Other prior works use surrogate objectives, such as the evidence lower
bound in variational auto-encoders (Kingma & Welling, 2014; Rezende et al., 2014; Maaløe et al.,
2019) and the contrastive divergence in energy-based models (Hinton, 2002; Carreira-Perpinan &
Hinton, 2005). These models, despite showing improved speed, typically only work well for low-
dimensional data, and, in general, the sample qualities are not competitive to the GAN-based and
the autoregressive models (Bond-Taylor et al., 2021)."
INTRODUCTION,0.008016032064128256,"An up-and-coming class of likelihood-based models is the diffusion probabilistic models (DPMs)
(Sohl-Dickstein et al., 2015), which introduces the idea of using a forward diffusion process to
sequentially corrupt a given distribution and learning the reversal of such diffusion process to restore
the data distribution for sampling. From a similar perspective, Song & Ermon (2019) proposed
the score-based generative models by applying the score matching technique (Hyvarinen & Dayan,"
INTRODUCTION,0.01002004008016032,Published as a conference paper at ICLR 2022
INTRODUCTION,0.012024048096192385,"2005) to train a neural network such that samples can be generated via Langevin dynamics. Along
these two lines of research, Ho et al. (2020) proposed the denoising diffusion probabilistic models
(DDPMs) for high-quality image syntheses. Dhariwal & Nichol (2021) demonstrated that improved
DDPMs Nichol & Dhariwal (2021) are capable of generating high-quality images of comparable
or even superior quality to the state-of-the-art (SOTA) GAN-based models. For speech syntheses,
DDPMs were also applied in Wavegrad (Chen et al., 2020) and DiffWave (Kong et al., 2021) to
produce higher-ﬁdelity audio samples than the conventional non-autoregressive models (Yamamoto
et al., 2020; Kumar et al., 2019; Yang et al., 2021; Bi´nkowski et al., 2020) and matched the quality
of the SOTA autoregressive methods (Chen et al., 2020)."
INTRODUCTION,0.014028056112224449,"Despite the compelling results, the diffusion generative models are two to three orders of magnitude
slower than other generative models such as GANs and VAEs. Their primary limitation is that they
require up to thousands of diffusion steps during training to learn the target distribution. Therefore a
large number of reverse steps are often required at sampling time. Recently, extensive investigations
have been conducted to reduce the sampling steps for efﬁciently generating high-quality samples,
which we will discuss in the related work in Section 2. Distinctively, we conceived that we might
train a neural network to efﬁciently and adaptively estimate a much shorter noise schedule for sam-
pling while achieving generation performances comparable or superior to the conventional DPMs.
With such an incentive, after introducing the conventional DPMs as our background in Section 3, we
propose in Section 4 bilateral denoising diffusion models (BDDMs), named after a bilateral mod-
eling perspective – parameterizing the forward and reverse processes with a schedule network and
a score network, respectively. We theoretically derive that the schedule network should be trained
after the score network is optimized. For training the schedule network, we propose a novel objec-
tive to minimize the gap between a newly derived lower bound and the log marginal likelihood. We
describe the training algorithm as well as the fast and high-quality sampling algorithm in Section 5.
The training of the schedule network converges very fast using our newly derived objective, and its
training only adds negligible overhead to DDPM’s. In Section 6, our neural vocoding experiments
demonstrated that BDDMs could generate high-ﬁdelity samples with as few as three sampling steps.
Moreover, our method can produce speech samples indistinguishable from human speech with only
seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave)."
RELATED WORK,0.01603206412825651,"2
RELATED WORK"
RELATED WORK,0.018036072144288578,"Prior works showed that noise scheduling is crucial for efﬁcient and high-ﬁdelity data generation in
DPMs. DDPMs (Ho et al., 2020) used a shared linear noise schedule for both training and sampling,
which, however, requires thousands of sampling iterations to obtain competitive results. To speed
up the sampling process, one class of related work, including (Chen et al., 2020; Kong et al., 2021;
Nichol & Dhariwal, 2021), attempts to use a different, shorter noise schedule for sampling. For
clarity, we thereafter denote the training noise schedule as β ∈RT and the sampling noise schedule
as ˆβ ∈RN with N < T. In particular, Chen et al. (2020) applied a grid search (GS) algorithm to
select ˆβ. Unfortunately, GS becomes prohibitively slow when N grows large, e.g., N = 6 took more
than a day on a single NVIDIA Tesla P40 GPU. This is because the time costs of GS algorithm grow
exponentially with N, i.e., O(9N) with 9 bins as the default setting in Chen et al. (2020). Instead
of searching, Kong et al. (2021) devised a fast sampling (FS) algorithm based on an expert-deﬁned
6-step noise schedule for their score network. However, this speciﬁcally tuned noise schedule is
hard to generalize to other score networks, tasks, or datasets."
RELATED WORK,0.02004008016032064,"Another class of noise scheduling methods searches for a subsequence of time indices of the training
noise schedule, which we call the time schedule. DDIMs (Song et al., 2021) introduced an accel-
erated reverse process that relies on a pre-speciﬁed time schedule. A linear and a quadratic time
schedule were used in DDIMs and showed superior generation quality over DDPMs within 10 to
100 sampling steps. Nichol & Dhariwal (2021) proposed a re-scaled noise schedule for fast sam-
pling, but this also requires pre-specifying the time schedule and the training noise schedule. Nichol
& Dhariwal (2021) also proposed learning variances for the reverse processes, whereas the variances
of the forward processes, i.e., the noise schedule, which affected both the means and variances of
the reverse processes, were not learnable. According to the results of (Song et al., 2021; Nichol &
Dhariwal, 2021), using a linear or quadratic time schedule resulted in quite different performances
in different datasets, implying that the optimal choice of schedule varies with the datasets. So, there
remains a challenge in ﬁnding a short and effective schedule for fast sampling on different datasets."
RELATED WORK,0.022044088176352707,Published as a conference paper at ICLR 2022
RELATED WORK,0.02404809619238477,"Notably, Kong & Ping (2021) proposed a method to map a noise schedule to a time schedule for
fast sampling. In this sense, searching for a time schedule becomes a sub-set of the noise scheduling
problem, which resembles the above category of methods."
RELATED WORK,0.026052104208416832,"Although DPMs (Sohl-Dickstein et al., 2015) and DDPMs (Ho et al., 2019) mentioned that the noise
schedule could be learned by re-parameterization, the approach was not investigated in their works.
Closely related works that learn a noise schedule emerged until very recently. San-Roman et al.
(2021) proposed a noise estimation (NE) method, which trained a neural net with a regression loss
to estimate the noise scale from the noisy sample at each time point, and then predicted the next
noise scale. However, NE requires a prior assumption of the noise schedule following a linear or
Fibonacci rule. Most recently, a concurrent work to ours by Kingma et al. (2021) jointly trained a
neural net to predict the signal-to-noise ratio (SNR) by maximizing the variational lower bound. The
SNR was then used for noise scheduling. Different from ours, this scheduling neural net only took
t as input and is independent of the noisy sample generated during the loop of sampling process.
Intrinsically, with limited information about the sampled data, the predicted SNR could deviate from
the actual SNR of the noisy data during sampling."
BACKGROUND,0.028056112224448898,"3
BACKGROUND"
BACKGROUND,0.03006012024048096,"3.1
DIFFUSION PROBABILISTIC MODELS (DPMS)"
BACKGROUND,0.03206412825651302,"Given i.i.d. samples {x0 ∈RD} from an unknown data distribution pdata(x0), diffusion prob-
abilistic models (DPMs) (Sohl-Dickstein et al., 2015) deﬁne a forward process q(x1:T |x0) =
QT
t=1 q(xt|xt−1) that converts any complex data distribution into a simple, tractable distribution af-
ter T steps of diffusion. A reverse process pθ(xt−1|xt) parameterized by θ is used to model the data
distribution: pθ(x0) =
R
π(xT ) QT
t=1 pθ(xt−1|xt)dx1:T , where π(xT ) is the prior distribution for
starting the reverse process. Then, the variational parameters θ can be learned by maximizing the
standard log evidence lower bound (ELBO):"
BACKGROUND,0.03406813627254509,"Felbo := Eq """
BACKGROUND,0.036072144288577156,"log pθ(x0|x1) − T
X"
BACKGROUND,0.03807615230460922,"t=2
DKL (q(xt−1|xt, x0)||pθ(xt−1|xt)) −DKL (q(xT |x0)||π(xT )) # . (1)"
BACKGROUND,0.04008016032064128,"3.2
DENOISING DIFFUSION PROBABILISTIC MODELS (DDPMS)"
BACKGROUND,0.04208416833667335,"As an extension to DPMs, denoising diffusion probabilistic models (DDPMs) (Ho et al., 2020)
applied the score matching technique (Hyvarinen & Dayan, 2005; Song & Ermon, 2019) to deﬁne
the reverse process. In particular, DDPMs considered a Gaussian diffusion process parameterized
by a noise schedule β ∈RT with 0 < β1, . . . , βT < 1:"
BACKGROUND,0.04408817635270541,"qβ(x1:T |x0) := T
Y"
BACKGROUND,0.04609218436873747,"t=1
qβt(xt|xt−1),
where
qβt(xt|xt−1) := N(
p"
BACKGROUND,0.04809619238476954,"1 −βtxt−1, βtI).
(2)"
BACKGROUND,0.050100200400801605,"Based on the nice property of isotropic Gaussians, one can express xt directly conditioned on x0:"
BACKGROUND,0.052104208416833664,"qβ(xt|x0) = N(αtx0, (1 −α2
t)I),
where
αt = tY i=1 p"
BACKGROUND,0.05410821643286573,"1 −βi.
(3)"
BACKGROUND,0.056112224448897796,"To revert this forward process, DDPMs employ a score network1 ϵθ(xt, αt) to deﬁne"
BACKGROUND,0.05811623246492986,pθ(xt−1|xt) := N
BACKGROUND,0.06012024048096192,"1
√1 −βt "
BACKGROUND,0.06212424849699399,"xt −
βt
p"
BACKGROUND,0.06412825651302605,"1 −α2
t
ϵθ (xt, αt) ! , Σt ! ,
(4)"
BACKGROUND,0.06613226452905811,"1Here, ϵθ(xt, αt) is conditioned on the continuous noise scale αt, as in (Song et al., 2020b; Chen et al.,
2020). Alternatively, the score network can also be conditioned on a discrete time index ϵθ(xt, t), as in (Song
et al., 2021; Ho et al., 2020). An approximate mapping of a noise schedule to a time schedule (Kong & Ping,
2021) exists, therefore we consider conditioning on noise scales as the general case."
BACKGROUND,0.06813627254509018,Published as a conference paper at ICLR 2022
BACKGROUND,0.07014028056112225,"Figure 1: A bilateral denoising diffusion model (BDDM) introduces a junctional variable xt and a
schedule network φ. The schedule network can optimize the shortened noise schedule ˆβn(φ) if we
know the score of the distribution at the junctional step, using the KL divergence to directly compare
pθ∗(ˆxn−1|ˆxn = xt) against the re-parameterized forward process posteriors."
BACKGROUND,0.07214428857715431,where Σt is the co-variance matrix deﬁned for the reverse process. Ho et al. (2020) showed that
BACKGROUND,0.07414829659318638,"setting Σt = ˜βtI =
1−α2
t−1
1−α2
t βtI is optimal for a deterministic x0, while setting Σt = βtI is optimal
for a white noise x0 ∼N(0, I). Alternatively, Nichol & Dhariwal (2021) proposed learnable
variances by interpolating the two optimals with a jointly trained neural network, i.e., Σt,θ(x) :=
diag(exp(vθ(x) log βt + (1 −vθ(x)) log ˜βt)), where vθ(x) ∈RD is a trainable network."
BACKGROUND,0.07615230460921844,"Note that the calculation of the complete ELBO in Eq. (1) requires T forward passes of the score
network, which would make the training computationally prohibitive for a large T. To feasibly
train the score network, instead of computing the complete ELBO, Ho et al. (2020) proposed an
efﬁcient training mechanism by sampling from a discrete uniform distribution: t ∼U{1, ..., T},
x0 ∼pdata(x0), ϵt ∼N(0, I) at each training iteration to compute the training loss:"
BACKGROUND,0.0781563126252505,"L(t)
ddpm(θ) :=
ϵt −ϵθ"
BACKGROUND,0.08016032064128256,"
αtx0 +
q"
BACKGROUND,0.08216432865731463,"1 −α2
tϵt, αt  2"
BACKGROUND,0.0841683366733467,"2
,
(5)"
BACKGROUND,0.08617234468937876,"which is a re-weighted form of DKL (qβ(xt−1|xt, x0)||pθ(xt−1|xt)). Ho et al. (2020) reported that
the re-weighting worked effectively for learning θ. Yet, we demonstrate it is deﬁcient for learning
the noise schedule β in our ablation experiment in Section 6.2."
BACKGROUND,0.08817635270541083,"4
BILATERAL DENOISING DIFFUSION MODELS (BDDMS)"
PROBLEM FORMULATION,0.09018036072144289,"4.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.09218436873747494,"For fast sampling with DPMs, we strive for a noise schedule ˆβ for sampling that is much shorter
than the noise schedule β for training. As shown in Fig. 1, we deﬁne two separate diffusion
processes corresponding to the noise schedules, β and ˆβ, respectively. The upper diffusion pro-
cess parameterized by β is the same as in Eq.
(2), whereas the lower process is deﬁned as
q ˆβ(ˆx1:N|ˆx0) = QN
n=1 q ˆβn(ˆxn|ˆxn−1) with much fewer diffusion steps (N ≪T). In our prob-"
PROBLEM FORMULATION,0.09418837675350701,"lem formulation, β is given, but ˆβ is unknown. The goal is to ﬁnd a ˆβ for the reverse process
pθ(ˆxn−1|ˆxn; ˆβn) such that ˆx0 can be effectively recovered from ˆxN with N reverse steps."
MODEL DESCRIPTION,0.09619238476953908,"4.2
MODEL DESCRIPTION"
MODEL DESCRIPTION,0.09819639278557114,"Although many prior arts (Ho et al., 2020; Chen et al., 2020; Song et al., 2021; San-Roman et al.,
2021) directly applied a shortened linear or Fibonacci noise schedule to the reverse process, we
argue that these are sub-optimal solutions. Theoretically, the diffusion process speciﬁed by a new
shortened noise schedule is essentially different from the one used to train the score network θ.
Therefore, θ is not guaranteed suitable for reverting the shortened diffusion process. This issue
motivated a novel modeling perspective to establish a link between the shortened schedule ˆβ and
the score network θ, i.e., to have ˆβ optimized according to θ."
MODEL DESCRIPTION,0.10020040080160321,Published as a conference paper at ICLR 2022
MODEL DESCRIPTION,0.10220440881763528,"As a starting point, we consider an N = ⌊T/τ⌋, where 1 ≤τ < T is a hyperparameter controlling
the step size such that each diffusion step between two consecutive variables in the shorter diffusion
process corresponds to τ diffusion steps in the longer one. Based on Eq. (2), we deﬁne the following:"
MODEL DESCRIPTION,0.10420841683366733,q ˆβn+1(ˆxn+1|ˆxn = xt) := qβ(xt+τ|xt) = N   s
MODEL DESCRIPTION,0.1062124248496994,"α2
t+τ
α2
t
xt,

1 −α2
t+τ
α2
t 
I "
MODEL DESCRIPTION,0.10821643286573146,",
(6)"
MODEL DESCRIPTION,0.11022044088176353,"where xt is an intermediate diffused variable we introduced to link the two differently indexed
diffusion sequences. We call it a junctional variable, which can be easily generated given x0 and β
during training: xt = αtx0 +
p"
MODEL DESCRIPTION,0.11222444889779559,"1 −α2
tϵn."
MODEL DESCRIPTION,0.11422845691382766,"Unfortunately, for the reverse process when x0 is not given, the junctional variable is intractable.
However, our key observation is that while using the score by a score network θ∗trained for the
long β-parameterized diffusion process, a short noise schedule ˆβ(φ) can be optimized accordingly
by introducing a schedule network φ. We provide its mathematical derivations in Appendix A.3.
Next, we present a formal deﬁnition of BDDM and derive its training objectives, L(n)
score(θ) and
L(n)
step(φ; θ∗), for the score network and the schedule network, respectively, in more detail."
SCORE NETWORK,0.11623246492985972,"4.3
SCORE NETWORK"
SCORE NETWORK,0.11823647294589178,"Recall that a DDPM starts the reverse process with a white noise xT ∼N(0, I) and takes T steps
to recover the data distribution:"
SCORE NETWORK,0.12024048096192384,"pθ(x0)
DDPM
:= EN(0,I)

Epθ(x1:T −1|xT ) [pθ(x0|x1:T )]

.
(7)"
SCORE NETWORK,0.12224448897795591,"A BDDM, in contrast, starts from the junctional variable xt, and reverts a shorter sequence of
diffusion random variables with only n steps:"
SCORE NETWORK,0.12424849699398798,"pθ(ˆx0)
BDDM
:= Eq ˆ
β(ˆxn−1;xt,ϵn)

Epθ(ˆx1:n−2|ˆxn−1) [pθ(ˆx0|ˆx1:n−1)]

,
2 ≤n ≤N,
(8)"
SCORE NETWORK,0.12625250501002003,"where q ˆβ(ˆxn−1; xt, ϵn) is deﬁned as a re-parameterization on the posterior:"
SCORE NETWORK,0.1282565130260521,"q ˆβ(ˆxn−1; xt, ϵn) :=q ˆβ  ˆxn−1"
SCORE NETWORK,0.13026052104208416,"ˆxn = xt, ˆx0 = xt −
p"
SCORE NETWORK,0.13226452905811623,"1 −ˆα2nϵn
ˆαn ! (9) =N  
1
q"
SCORE NETWORK,0.1342685370741483,"1 −ˆβn
xt −
ˆβn
q"
SCORE NETWORK,0.13627254509018036,"(1 −ˆβn)(1 −ˆα2n)
ϵn, 1 −ˆα2
n−1
1 −ˆα2n
ˆβnI "
SCORE NETWORK,0.13827655310621242,",
(10)"
SCORE NETWORK,0.1402805611222445,"where ˆαn = Qn
i=1 q"
SCORE NETWORK,0.14228456913827656,"1 −ˆβi, xt = αtx0 +
p"
SCORE NETWORK,0.14428857715430862,"1 −α2
tϵn is the junctional variable that maps xt
to ˆxn given an approximate index t ∼U{(n −1)τ, ..., nτ −1, nτ} and a sampled white noise
ϵn ∼N(0, I). Detailed derivation from Eq. (9) to (10) is provided in Appendix A.2."
TRAINING OBJECTIVE FOR SCORE NETWORK,0.1462925851703407,"4.3.1
TRAINING OBJECTIVE FOR SCORE NETWORK"
TRAINING OBJECTIVE FOR SCORE NETWORK,0.14829659318637275,"With the above deﬁnition, a new form of lower bound to the log marginal likelihood can be derived
such that log pθ(ˆx0) ≥F(n)
score(θ) := −L(n)
score(θ) −Rθ(ˆx0, xt), where"
TRAINING OBJECTIVE FOR SCORE NETWORK,0.15030060120240482,"L(n)
score(θ) :=DKL

pθ(ˆxn−1|ˆxn = xt)||q ˆβ(ˆxn−1; xt, ϵn)

,
(11)"
TRAINING OBJECTIVE FOR SCORE NETWORK,0.1523046092184369,"Rθ(ˆx0, xt) := −Epθ(ˆx1|ˆxn=xt) [log pθ(ˆx0|ˆx1)] .
(12)"
TRAINING OBJECTIVE FOR SCORE NETWORK,0.15430861723446893,"See detailed derivation in Proposition 1 in Appendix A.2. In the following Proposition 2, we prove
that via the junctional variable xt, the solution θ∗for optimizing the objective L(t)
ddpm(θ), ∀t ∈"
TRAINING OBJECTIVE FOR SCORE NETWORK,0.156312625250501,"{1, ..., T} is also the solution for optimizing L(n)
score(θ), ∀n ∈{2, ..., N}. Thereby, we show that the
score network θ can be trained with L(t)
ddpm(θ) and re-used for reverting the short diffusion process
over ˆxN:0. Although the newly derived lower bound result in the same objective as the conventional
score network, it for the ﬁrst time establishes a link between the score network θ and ˆxN:0. The
connection is essential for learning ˆβ, which we will describe next."
TRAINING OBJECTIVE FOR SCORE NETWORK,0.15831663326653306,Published as a conference paper at ICLR 2022
SCHEDULE NETWORK,0.16032064128256512,"4.4
SCHEDULE NETWORK"
SCHEDULE NETWORK,0.1623246492985972,"In BDDMs, a schedule network is introduced to the forward process by re-parameterizing ˆβn as
ˆβn(φ) = fφ

xt; ˆβn+1

, and recall that during training, we can use xt = αtx0 +
p"
SCHEDULE NETWORK,0.16432865731462926,"1 −α2
tϵn and"
SCHEDULE NETWORK,0.16633266533066132,"ˆβn+1 = 1 −
α2
t+τ
α2
t . Through the re-parameterization, the task of noise scheduling, i.e., searching"
SCHEDULE NETWORK,0.1683366733466934,"for ˆβ, can now be reformulated as training a schedule network fφ that ancestrally estimates data-
dependent variances. The schedule network learns to predict ˆβn based on the current noisy sample
xt – this makes our method fundamentally different from existing and concurrent work, includ-
ing Kingma et al. (2021) – as we reveal that, aside from ˆβn+1, t, or n that reﬂects diffusion step
information, xt is also essential for noise scheduling from a reverse direction at inference time."
SCHEDULE NETWORK,0.17034068136272545,"Speciﬁcally, we adopt the ancestral step information (ˆβn+1) to derive an upper bound for the current
step while leaving the schedule network only to take the current noisy sample xt as input to predict
a relative change of noise scales against the ancestral step. First, we derive an upper bound of ˆβn
by proving 0 < ˆβn < min
n
1 −
ˆα2
n+1
1−ˆβn+1 , ˆβn+1
o
in Appendix A.1. Then, by multiplying the upper"
SCHEDULE NETWORK,0.17234468937875752,"bound by a ratio estimated by a neural network σφ : RD 7→(0, 1), we deﬁne"
SCHEDULE NETWORK,0.1743486973947896,"fφ(xt; ˆβn+1) := min

1 −
ˆα2
n+1
1 −ˆβn+1
, ˆβn+1"
SCHEDULE NETWORK,0.17635270541082165,"
σφ(xt),
(13)"
SCHEDULE NETWORK,0.17835671342685372,"where the network parameter set φ is learned to estimate the ratio between two consecutive noise
scales (ˆβn and ˆβn+1) from the current noisy input xt."
SCHEDULE NETWORK,0.18036072144288579,"Finally, at inference time for noise scheduling, starting from a maximum reverse steps (N) and two
hyperparameters (ˆαN, ˆβN), we ancestrally predict the noise scale ˆβn(φ) = fφ

ˆxn; ˆβn+1

, for n"
SCHEDULE NETWORK,0.18236472945891782,"from N to 1, and cumulatively update the product ˆαn =
ˆαn+1
√"
SCHEDULE NETWORK,0.1843687374749499,1−ˆβn+1 .
TRAINING OBJECTIVE FOR SCHEDULE NETWORK,0.18637274549098196,"4.4.1
TRAINING OBJECTIVE FOR SCHEDULE NETWORK"
TRAINING OBJECTIVE FOR SCHEDULE NETWORK,0.18837675350701402,"Here we describe how to learn the network parameters φ effectively. First, we demonstrated that
φ should be trained after θ is well-optimized, referring to Proposition 3 in Appendix A.3. The
Proposition also shows that we are minimizing the gap between the lower bound F(n)
score(θ∗) and
log pθ∗(ˆx0), i.e., log pθ∗(ˆx0) −F(n)
score(θ∗), by minimizing the following objective"
TRAINING OBJECTIVE FOR SCHEDULE NETWORK,0.1903807615230461,"L(n)
step(φ; θ∗) :=DKL

pθ∗(ˆxn−1|ˆxn = xt)||q ˆβn(φ)(ˆxn−1; x0, αt)

,
(14)"
TRAINING OBJECTIVE FOR SCHEDULE NETWORK,0.19238476953907815,"which is deﬁned as a KL divergence to directly compare pθ∗(ˆxn−1|ˆxn = xt) against the re-
parameterized forward process posteriors, which are tractable when conditioned on the junctional
noise scale αt and x0."
TRAINING OBJECTIVE FOR SCHEDULE NETWORK,0.19438877755511022,"The detailed derivation of Eq. (14) is also provided in the proof of Proposition 3 to get its concrete
formulas as shown in Step (8-10) in Alg. 2."
TRAINING OBJECTIVE FOR SCHEDULE NETWORK,0.1963927855711423,"5
ALGORITHMS: TRAINING, NOISE SCHEDULING, AND SAMPLING"
TRAINING SCORE AND SCHEDULE NETWORKS,0.19839679358717435,"5.1
TRAINING SCORE AND SCHEDULE NETWORKS"
TRAINING SCORE AND SCHEDULE NETWORKS,0.20040080160320642,"Following the theoretical result in Appendix A.3, θ should be optimized before learning φ. Thereby
ﬁrst, to train the score network ϵθ, we refer to the settings in (Ho et al., 2020; Chen et al., 2020; Song
et al., 2021) to deﬁne β as a linear noise schedule: βt = βstart + t"
TRAINING SCORE AND SCHEDULE NETWORKS,0.20240480961923848,"T (βend −βstart),
for
1 ≤t ≤T,
where βstart and βend are two hyperparameter that speciﬁes the start value and the end value. This
results in Algorithm 1, which resembles the training algorithm in (Ho et al., 2020)."
TRAINING SCORE AND SCHEDULE NETWORKS,0.20440881763527055,"Next, based on the converged score network θ∗, we train the schedule network φ. We draw an
n ∼U{2, . . . , N} at each training step, and then draw a t ∼U{(n −1)τ, ..., nτ}. These together
can be re-formulated as directly drawing t ∼U{τ, ..., T −τ} for a ﬁner-scale time step. Then, we"
TRAINING SCORE AND SCHEDULE NETWORKS,0.20641282565130262,Published as a conference paper at ICLR 2022
TRAINING SCORE AND SCHEDULE NETWORKS,0.20841683366733466,Algorithm 1 Training Score Network (θ)
TRAINING SCORE AND SCHEDULE NETWORKS,0.21042084168336672,"1: Given T, {βt}T
t=1
2: {αt}T
t=1 = {Qt
i=1
√1 −βt}T
t=1
3: repeat
4:
x0 ∼pdata(x0)
5:
t ∼U{1, . . . , T}
6:
ϵt ∼N(0, I)
7:
xt = αtx0 +
p"
TRAINING SCORE AND SCHEDULE NETWORKS,0.2124248496993988,"1 −α2
tϵt
8:
L(t)
ddpm = ∥ϵt −ϵθ(xt, αt)∥2
2
9:
Take a gradient descent step on ∇θL(t)
ddpm
10: until converged"
TRAINING SCORE AND SCHEDULE NETWORKS,0.21442885771543085,Algorithm 3 Noise Scheduling
TRAINING SCORE AND SCHEDULE NETWORKS,0.21643286573146292,"1: Given θ∗, ˆαN, ˆβN, xN ∼N(0, I)
2: for n = N to 2 do
3:
ˆxn−1 ∼pθ∗(ˆxn−1|ˆxn; ˆαn, ˆβn)
4:
ˆαn−1 =
ˆαn
√"
TRAINING SCORE AND SCHEDULE NETWORKS,0.218436873747495,"1−ˆβn
5:
ˆβn−1 = min{1 −ˆα2
n−1, ˆβn}σφ(ˆxn−1)
6:
if ˆβn−1 < β1 then
7:
return ˆβn, . . . , ˆβN
8:
end if
9: end for
10: return ˆβ1, . . . , ˆβN"
TRAINING SCORE AND SCHEDULE NETWORKS,0.22044088176352705,Algorithm 2 Training Schedule Network (φ)
TRAINING SCORE AND SCHEDULE NETWORKS,0.22244488977955912,"1: Given θ∗, τ, T, {αt, βt}T
t=1
2: repeat
3:
x0 ∼pdata(x0)
4:
t ∼U{τ, . . . , T −τ}
5:
δt = 1 −α2
t
6:
ϵn ∼N(0, I)
7:
xt = αtx0 + √δtϵn"
TRAINING SCORE AND SCHEDULE NETWORKS,0.22444889779559118,"8:
ˆβn = min

δt, 1 −
α2
t+τ
α2
t"
TRAINING SCORE AND SCHEDULE NETWORKS,0.22645290581162325,"
σφ(xt)"
TRAINING SCORE AND SCHEDULE NETWORKS,0.22845691382765532,"9:
C = 4−1 log(δt/ˆβn) + 2−1D

ˆβn/δt −1
"
TRAINING SCORE AND SCHEDULE NETWORKS,0.23046092184368738,"10:
L(n)
step =
δt
2(δt−ˆβn)"
TRAINING SCORE AND SCHEDULE NETWORKS,0.23246492985971945,"ϵn −
ˆβn
δt ϵθ∗(xt, αt)

2 2 + C"
TRAINING SCORE AND SCHEDULE NETWORKS,0.23446893787575152,"11:
Take a gradient descent step on ∇φL(n)
step
12: until converged
Algorithm 4 Sampling"
TRAINING SCORE AND SCHEDULE NETWORKS,0.23647294589178355,"1: Given θ∗, {ˆβn}Ns
n=1, ˆxNs ∼N(0, I)"
TRAINING SCORE AND SCHEDULE NETWORKS,0.23847695390781562,"2: {ˆαn}Ns
n=1 =
Qn
i=1 q"
TRAINING SCORE AND SCHEDULE NETWORKS,0.24048096192384769,1 −ˆβn Ns
TRAINING SCORE AND SCHEDULE NETWORKS,0.24248496993987975,"n=1
3: for n = Ns to 1 do
4:
ˆxn−1 ∼pθ∗(ˆxn−1|ˆxn; ˆαn, ˆβn)
5: end for
6: return ˆx0"
TRAINING SCORE AND SCHEDULE NETWORKS,0.24448897795591182,"sequentially compute the variables needed for calculating L(n)
step(φ; θ∗), as presented in Algorithm 2.
We observed that, although a linear schedule is used to deﬁne β, the noise schedule of ˆβ predicted
by fφ is not limited to but rather different from a linear one."
NOISE SCHEDULING FOR FAST AND HIGH-QUALITY SAMPLING,0.24649298597194388,"5.2
NOISE SCHEDULING FOR FAST AND HIGH-QUALITY SAMPLING"
NOISE SCHEDULING FOR FAST AND HIGH-QUALITY SAMPLING,0.24849699398797595,"After the score network and the schedule network are trained, the inference procedure can divide
into two phases: (1) the noise scheduling phase and (2) the sampling phase."
NOISE SCHEDULING FOR FAST AND HIGH-QUALITY SAMPLING,0.250501002004008,"First, we run the noise scheduling process similarly to a sampling process with N iterations maxi-
mum. Different from training, where αt is forward-computed, ˆαn is instead a backward-computed
variable (from N to 1) that may deviate from the forward one because {ˆβi}n−1
i
are unknown in the
noise scheduling phase during inference. To start noise scheduling, we ﬁrst set two hyperparame-
ters: ˆαN and ˆβN. We use β1, the smallest noise scale seen in training, as a threshold to early stop
the noise scheduling process so that we can ignore small noise scales (< β1) that were never seen
by the score network. Overall, the noise scheduling process presents in Algorithm 3."
NOISE SCHEDULING FOR FAST AND HIGH-QUALITY SAMPLING,0.25250501002004005,"In practice, we apply a grid search algorithm of M bins to Algorithm 3, which takes O(M 2) time, to
ﬁnd proper values for (ˆαN, ˆβN). We used M = 9 as in (Chen et al., 2020). The grid search for our
noise scheduling algorithm can be evaluated on a small subset of the training samples. Empirically,
even as few as 1 sample for evaluation works well in our algorithm. Finally, given the predicted
noise schedule ˆβ ∈RNs, we generate samples with Ns sampling steps, as shown in Algorithm 4."
EXPERIMENTS,0.2545090180360721,"6
EXPERIMENTS"
EXPERIMENTS,0.2565130260521042,"We conducted a series of experiments on neural vocoding tasks to evaluate the proposed BDDMs.
First, we compared BDDMs against several strongest models that have been published: the mixture
of logistics (MoL) WaveNet (Oord et al., 2018) implemented in (Yamamoto, 2020), the WaveGlow
(Prenger et al., 2019) implemented in (Valle, 2020), the MelGAN (Kumar et al., 2019) implemented
in (Kumar, 2019), the HiFi-GAN (Kong et al., 2020b) implemented in (Kong et al., 2020a) and
the two most recently proposed diffusion-based vocoders, i.e., WaveGrad (Chen et al., 2020) and
DiffWave (Kong et al., 2021), both re-implemented in our code. The hyperparameter settings of
BDDMs and all these models are detailed in Appendix B."
EXPERIMENTS,0.25851703406813625,"In addition, we also compared BDDMs to a variety of scheduling and acceleration techniques ap-
plicable to DDPMs, including the grid search (GS) approach in WaveGrad, the fast sampling (FS)"
EXPERIMENTS,0.2605210420841683,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.2625250501002004,"Table 1: Comparison of neural vocoders in terms of MOS with 95% conﬁdence intervals, real-time
factor (RTF) and model size in megabytes (MB) for inference. The highest score and the scores that
are not signiﬁcantly different from the highest score (p-values ≥0.05) are bold-faced."
EXPERIMENTS,0.26452905811623245,"Neural Vocoder
MOS
RTF
Size"
EXPERIMENTS,0.2665330661322645,"Ground-truth
4.64 ± 0.08
—
—"
EXPERIMENTS,0.2685370741482966,"WaveNet (MoL) (Oord et al., 2018)
3.52 ± 0.16
318.6
282MB
WaveGlow (Prenger et al., 2019)
3.03 ± 0.15
0.0198
645MB
MelGAN (Kumar et al., 2019)
3.48 ± 0.14
0.00396
17MB
HiFi-GAN (Kong et al., 2020b)
4.33 ± 0.12
0.0134
54MB
WaveGrad - 1000 steps (Chen et al., 2020)
4.36 ± 0.13
38.2
183MB
DiffWave - 200 steps (Kong et al., 2021)
4.49 ± 0.13
7.30
27MB"
EXPERIMENTS,0.27054108216432865,"BDDM - 3 steps (ˆαN = 0.68, ˆβN = 0.53)
3.64 ± 0.13
0.110
27MB
BDDM - 7 steps (ˆαN = 0.62, ˆβN = 0.42)
4.43 ± 0.11
0.256
27MB
BDDM - 12 steps (ˆαN = 0.67, ˆβN = 0.12)
4.48 ± 0.12
0.438
27MB"
EXPERIMENTS,0.2725450901803607,"Table 2: Comparison of sampling acceleration methods with the same score network and the same
number of steps. The highest score and the scores that are not signiﬁcantly different from the highest
score (p-values ≥0.05) are bold-faced."
EXPERIMENTS,0.2745490981963928,"Steps
Acceleration Method
STOI
PESQ
MOS 3"
EXPERIMENTS,0.27655310621242485,"GS (Chen et al., 2020)
0.965 ± 0.009
3.66 ± 0.20
3.61 ± 0.12
FS (Kong et al., 2021)
0.939 ± 0.023
3.09 ± 0.23
3.10 ± 0.12
DDIM (Song et al., 2021)
0.943 ± 0.015
3.42 ± 0.27
3.25 ± 0.13
NE (San-Roman et al., 2021)
0.966 ± 0.010
3.62 ± 0.18
3.55 ± 0.12
BDDM
0.966 ± 0.011
3.63 ± 0.24
3.64 ± 0.13 7"
EXPERIMENTS,0.2785571142284569,"FS (Kong et al., 2021)
0.981 ± 0.006
3.68 ± 0.24
3.70 ± 0.14
DDIM (Song et al., 2021)
0.974 ± 0.008
3.85 ± 0.12
3.94 ± 0.12
NE (San-Roman et al., 2021)
0.978 ± 0.007
3.75 ± 0.18
4.02 ± 0.11
BDDM
0.983 ± 0.006
3.96 ± 0.09
4.43 ± 0.11"
EXPERIMENTS,0.280561122244489,"12
DDIM (Song et al., 2021)
0.979 ± 0.006
3.90 ± 0.10
4.16 ± 0.12
NE (San-Roman et al., 2021)
0.981 ± 0.007
3.82 ± 0.13
3.98 ± 0.14
BDDM
0.987 ± 0.006
3.98 ± 0.12
4.48 ± 0.12"
EXPERIMENTS,0.28256513026052105,"approach based on a user-deﬁned 6-step schedule in DiffWave, the DDIMs (Song et al., 2021) and
a noise estimation (NE) approach (San-Roman et al., 2021). For fair and reproducible comparison
with other models and approaches, we used the LJSpeech dataset (Ito & Johnson, 2017), which
consists of 13,100 22kHz audio clips of a female speaker. All diffusion models were trained on the
same training split as in (Chen et al., 2020). We also replicated the comparative experiment of neural
vocoding using a multi-speaker VCTK dataset (Yamagishi et al., 2019) as presented in Appendix C
and obtained a result consistent with that obtained from the LJSpeech dataset."
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.2845691382765531,"6.1
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS"
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.2865731462925852,"To assess the quality of each generated audio sample, we used both objective and subjective mea-
sures for comparing different neural vocoders given the same ground-truth spectrogram s as the
condition, i.e., ϵθ(x, s, αt). Speciﬁcally, we used two scale-invariant metrics: the perceptual evalu-
ation of speech quality (PESQ) (Rix et al., 2001) and the short-time objective intelligibility (STOI)
(Taal et al., 2010) to measure the noisiness and the distortion of the generated speech relative to the
reference speech. Mean opinion score (MOS) was also used as a subjective metric for evaluating the
naturalness of the generated speech. The assessment scheme of MOS is included in Appendix B."
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.28857715430861725,"In Table 1, we compared BDDMs against the state-of-the-art (SOTA) vocoders. To predict noise
schedules with different sampling steps (3, 7, and 12), we set three pairs of {ˆαN, ˆβN} for BDDMs
by running on Algorithm 3 a quick hyperparameter grid search, which is detailed in Appendix B.
Among the 9 evaluated vocoders, only our proposed BDDMs with 7 and 12 steps and DiffWave with
200 steps showed no statistic-signiﬁcant difference from the ground-truth in terms of MOS. More-
over, BDDMs signiﬁcantly outspeeded DiffWave in terms of RTFs. Notably, previous diffusion-"
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.2905811623246493,Published as a conference paper at ICLR 2022
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.2925851703406814,"Figure 2: Different training losses for σφ
Figure 3: Different lower bounds to log pθ(x0)"
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.29458917835671344,"based vocoders achieved high MOS scores at the cost of an unacceptable RTF for industrial deploy-
ment. In contrast, BDDMs managed to achieve a high standard of generation quality with only 7
sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave)."
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.2965931863727455,"In Table 2, we evaluated BDDMs and alternative accelerated sampling methods, which used the
same score network for a pair-to-pair comparison. The GS method performed stably when the
step number was small (i.e., N ≤6) but not scalable to more step numbers, which were therefore
bypassed in the comparisons of 7 and 12 steps. The FS method by Song et al. (2021) was linearly
interpolated to 3 and 7 steps for a fair comparison. Comparing its 7-step and 3-step results, we
observed that the FS performance degraded drastically. Both the DDIM and the NE methods were
stable across all the steps but were not performing competitively enough. In comparison, BDDMs
consistently attained the leading scores across all the steps. This evaluation conﬁrmed that BDDM
was superior to other acceleration methods for DPMs in terms of both stability and quality."
ABLATION STUDY AND ANALYSIS,0.2985971943887776,"6.2
ABLATION STUDY AND ANALYSIS"
ABLATION STUDY AND ANALYSIS,0.30060120240480964,"We attribute the primary advantage of BDDMs to the newly derived objective L(n)
step for learning φ.
To better reason about this, we performed an ablation study, where we substituted the proposed loss
with the standard negative ELBO for learning φ as mentioned by Sohl-Dickstein et al. (2015). We
plotted the network outputs with different training losses in Fig. 2. It turned out that, when using
L(n)
elbo to learn φ, the network output rapidly collapsed to zero within several training steps; whereas,
the network trained with L(n)
step produced ﬂuctuating outputs. The ﬂuctuation is a desirable property
showing the network properly predicts t-dependent noise scales, as t is a random time step drawn
from a uniform distribution in training."
ABLATION STUDY AND ANALYSIS,0.3026052104208417,"By setting ˆβ = β, we empirically validated that F(t)
bddm := F(t)
score+L(t)
step ≥F(t)
elbo with their respective
values at t ∈[20, 180] using the same optimized θ∗. Each value is provided with 95% conﬁdence
intervals, as shown in Fig. 3. In this experiment, we used the LJ speech dataset and set T = 200 and
τ = 20. Notably, we dropped their common entropy term Rθ(ˆx0, xt) < 0 to mainly compare their
KL divergences. This explains those positive lower bound values in the plot. The graph shows that
our proposed bound F(t)
bddm is always a tighter lower bound than the standard one across all examined
t. Moreover, we found that F(t)
bddm attained low values with a relatively much lower variance for
t ≤50, where F(t)
elbo was highly volatile. This implies that F(t)
bddm better tackles the difﬁcult training
part, i.e., when the score becomes more challenging to estimate as t →0."
CONCLUSIONS,0.3046092184368738,"7
CONCLUSIONS"
CONCLUSIONS,0.3066132264529058,"BDDMs parameterize the forward and reverse processes with a schedule network and a score net-
work, of which the former’s optimization is tied with the latter by introducing a junctional variable.
We derived a new lower bound that leads to the same training loss for the score network as in DDPMs
(Ho et al., 2020), which thus enables inheriting any pre-trained score networks in DDPMs. We also
showed that training the schedule network after a well-optimized score network can be viewed as
tightening the lower bound. Followed from the theoretical results, an efﬁcient training algorithm and
a noise scheduling algorithm were respectively designed for BDDMs. Finally, in our experiments,
BDDMs showed a clear edge over the previous diffusion-based vocoders."
CONCLUSIONS,0.30861723446893785,Published as a conference paper at ICLR 2022
REFERENCES,0.3106212424849699,REFERENCES
REFERENCES,0.312625250501002,"Mikołaj Bi´nkowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman
Casagrande, Luis C. Cobo, and Karen Simonyan. High ﬁdelity speech synthesis with adversarial
networks. International conference on learning representations, 2020."
REFERENCES,0.31462925851703405,"S Bond-Taylor, A Leach, Y Long, and CG Willcocks. Deep generative modelling: A comparative
review of vaes, gans, normalizing ﬂows, energy-based and autoregressive models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 2021."
REFERENCES,0.3166332665330661,"Miguel A Carreira-Perpinan and Geoffrey E Hinton. On contrastive divergence learning. AISTATS,
pp. 33–40, 2005."
REFERENCES,0.3186372745490982,"Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wave-
grad: Estimating gradients for waveform generation. In International conference on learning
representations, 2020."
REFERENCES,0.32064128256513025,"Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differ-
ential equations. Advances in neural information processing systems, pp. 6571–6583, 2018."
REFERENCES,0.3226452905811623,"Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances
in neural information processing systems, 34, 2021."
REFERENCES,0.3246492985971944,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016."
REFERENCES,0.32665330661322645,"Brendan J Frey. Local probability propagation for factor analysis. Advances in neural information
processing systems, 12:442–448, 1999."
REFERENCES,0.3286573146292585,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014."
REFERENCES,0.3306613226452906,"G. E. Hinton. Training products of experts by minimizing contrastive divergence. neural computa-
tion. Neural computation, pp. 14(8):1771–1800, 2002."
REFERENCES,0.33266533066132264,"Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving ﬂow-
based generative models with variational dequantization and architecture design. ICML, 2019."
REFERENCES,0.3346693386773547,"Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
neural information processing systems, 33:6840–6851, 2020."
REFERENCES,0.3366733466933868,"Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score match-
ing. Journal of Machine Learning Research, pp. 6(4), 2005."
REFERENCES,0.33867735470941884,"Keith Ito and Linda Johnson. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017."
REFERENCES,0.3406813627254509,"Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lock-
hart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. Efﬁcient neural
audio synthesis. In International Conference on Machine Learning, pp. 2410–2419. PMLR, 2018."
REFERENCES,0.342685370741483,"Diederik P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions.
Advances in neural information processing systems, pp. 10215–10224, 2018."
REFERENCES,0.34468937875751504,"Diederik P Kingma and Max Welling. Auto-encoding variational bayes. stat, 1050:1, 2014."
REFERENCES,0.3466933867735471,"Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In
Advances in neural information processing systems, 2021."
REFERENCES,0.3486973947895792,"Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hiﬁ-gan: Generative adversarial networks for
efﬁcient and high ﬁdelity speech synthesis. https://github.com/jik876/hifi-gan,
2020a."
REFERENCES,0.35070140280561124,"Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hiﬁ-gan: Generative adversarial networks for
efﬁcient and high ﬁdelity speech synthesis. Advances in neural information processing systems,
33, 2020b."
REFERENCES,0.3527054108216433,Published as a conference paper at ICLR 2022
REFERENCES,0.35470941883767537,"Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. In ICML Workshop
on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021."
REFERENCES,0.35671342685370744,"Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. International conference on learning representations, 2021."
REFERENCES,0.3587174348697395,"Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo,
Alexandre de Br´ebisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative adversarial
networks for conditional waveform synthesis. Advances in neural information processing systems,
32, 2019."
REFERENCES,0.36072144288577157,"Rithesh Kumar. Ofﬁcial repository for the paper melgan: Generative adversarial networks for condi-
tional waveform synthesis. https://github.com/descriptinc/melgan-neurips,
2019."
REFERENCES,0.3627254509018036,"Max WY Lam, Jun Wang, Dan Su, and Dong Yu. Effective low-cost time-domain audio separation
using globally attentive locally recurrent networks. In 2021 IEEE Spoken Language Technology
Workshop (SLT), pp. 801–808. IEEE, 2021."
REFERENCES,0.36472945891783565,"Lars Maaløe, Marco Fraccaro, Valentin Li´evin, and Ole Winther. Biva: A very deep hierarchy of
latent variables for generative modeling. Advances in neural information processing systems, pp.
6548–6558, 2019."
REFERENCES,0.3667334669338677,"Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.
In International Conference on Machine Learning, pp. 8162–8171. PMLR, 2021."
REFERENCES,0.3687374749498998,"Aaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu,
George Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel wavenet: Fast
high-ﬁdelity speech synthesis. In International conference on machine learning, pp. 3918–3926.
PMLR, 2018."
REFERENCES,0.37074148296593185,"George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lak-
shminarayanan. Normalizing ﬂows for probabilistic modeling and inference. JMLR, pp. 22(57):1–
64, 2021."
REFERENCES,0.3727454909819639,"Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A ﬂow-based generative network
for speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 3617–3621. IEEE, 2019."
REFERENCES,0.374749498997996,"Flavio Protasio Ribeiro, Dinei Florencio, Cha Zhang, and Mike Seltzer. CROWDMOS: An approach
for crowdsourcing mean opinion score studies. In ICASSP. IEEE, 2011. Edition: ICASSP."
REFERENCES,0.37675350701402804,"Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. ICML, pp. 1278–1286, 2014."
REFERENCES,0.3787575150300601,"Antony W Rix, John G Beerends, Michael P Hollier, and Andries P Hekstra. Perceptual evaluation
of speech quality (pesq)-a new method for speech quality assessment of telephone networks and
codecs. In 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing.
Proceedings (Cat. No. 01CH37221), volume 2, pp. 749–752. IEEE, 2001."
REFERENCES,0.3807615230460922,"Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion mod-
els. arXiv preprint arXiv:2104.02600, 2021."
REFERENCES,0.38276553106212424,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.3847695390781563,"Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learn-
ing, pp. 2256–2265, 2015."
REFERENCES,0.3867735470941884,"Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. Interna-
tional conference on learning representations, 2021."
REFERENCES,0.38877755511022044,Published as a conference paper at ICLR 2022
REFERENCES,0.3907815631262525,"Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in neural information processing systems, 32, 2019."
REFERENCES,0.3927855711422846,"Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
Advances in neural information processing systems, 33:12438–12448, 2020."
REFERENCES,0.39478957915831664,"Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach
to density and score estimation. UAI, pp. 574–584, 2020a."
REFERENCES,0.3967935871743487,"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In Interna-
tional conference on learning representations, 2020b."
REFERENCES,0.39879759519038077,"Cees H Taal, Richard C Hendriks, Richard Heusdens, and Jesper Jensen. A short-time objective
intelligibility measure for time-frequency weighted noisy speech. In 2010 IEEE international
conference on acoustics, speech and signal processing, pp. 4214–4217. IEEE, 2010."
REFERENCES,0.40080160320641284,"Rafael Valle.
Waveglow: a ﬂow-based generative network for speech synthesis.
https://
github.com/NVIDIA/waveglow, 2020."
REFERENCES,0.4028056112224449,"Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. Proc. 9th ISCA Speech Synthesis Workshop, pp. 125–125, 2016."
REFERENCES,0.40480961923847697,"Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to efﬁciently sam-
ple from diffusion probabilistic models. arXiv preprint arXiv:2106.03802, 2021."
REFERENCES,0.40681362725450904,"Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. CSTR VCTK Corpus: English
multi-speaker corpus for CSTR voice cloning toolkit (version 0.92), 2019."
REFERENCES,0.4088176352705411,"Ryuichi Yamamoto. Wavenet vocoder. https://github.com/r9y9/wavenet_vocoder,
2020."
REFERENCES,0.41082164328657317,"Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. Parallel. Wavegan: A fast waveform genera-
tion model based on generative adversarial networks with multi-resolution spectrogram. ICASSP,
2020."
REFERENCES,0.41282565130260523,"Geng Yang, Shan Yang, Kai Liu, Peng Fang, Wei Chen, and Lei Xie. Multi-band melgan: Faster
waveform generation for high-quality text-to-speech. In 2021 IEEE Spoken Language Technology
Workshop (SLT), pp. 492–498. IEEE, 2021."
REFERENCES,0.4148296593186373,Published as a conference paper at ICLR 2022
REFERENCES,0.4168336673346693,"A
THEORETICAL DERIVATIONS FOR BDDMS"
REFERENCES,0.4188376753507014,"In this section, we provide the theoretical supports for the following:"
REFERENCES,0.42084168336673344,• The derivation for upper bounding ˆβn (see Appendix A.1).
REFERENCES,0.4228456913827655,"• The score network θ trained with L(t)
ddpm(θ) for the reverse process pθ(xt−1|xt) can be
re-used for the reverse process pθ(ˆxn−1|ˆxn) (see Appendix A.2)."
REFERENCES,0.4248496993987976,"• The schedule network φ can be trained with L(n)
step(φ; θ∗) after the score network θ is opti-
mized. (see Appendix A.3)."
REFERENCES,0.42685370741482964,"A.1
DERIVING AN UPPER BOUND FOR NOISE SCALE"
REFERENCES,0.4288577154308617,"Since monotonic noise schedules have been successfully applied to in many prior arts including
DPMs (Ho et al., 2020; Kingma et al., 2021) and score-based methods (Song et al., 2020a; Song
& Ermon, 2020), we also follow the monotonic assumption and derive an upper bound for ˆβn as
below:
Remark 1. Suppose the noise schedule for sampling is monotonic, i.e., 0 < ˆβ1 < . . . < ˆβN < 1,
then, for 1 ≤n < N, ˆβn satisﬁes the following inequality:"
REFERENCES,0.4308617234468938,"0 < ˆβn < min

1 −
ˆα2
n+1
1 −ˆβn+1
, ˆβn+1"
REFERENCES,0.43286573146292584,"
.
(15)"
REFERENCES,0.4348697394789579,"Proof. By the general deﬁnition of noise schedule, we know that 0 < ˆβ1, . . . , ˆβN < 1 (Note: no"
REFERENCES,0.43687374749499,"inequality sign in between). Given that ˆαn = Qn
i=1 q"
REFERENCES,0.43887775551102204,"1 −ˆβi, we also have 0 < ˆα1, . . . , ˆαt < 1."
REFERENCES,0.4408817635270541,"First, we show that ˆβn < 1 −
ˆα2
n+1
1−ˆβn+1 :"
REFERENCES,0.44288577154308617,"ˆαn−1 =
ˆαn
q"
REFERENCES,0.44488977955911824,"1 −ˆβn
< 1 ⇐⇒ˆβn < 1 −ˆα2
n = 1 −
ˆα2
n+1
1 −ˆβn+1
.
(16)"
REFERENCES,0.4468937875751503,"Next, we show that ˆβn < 1 −ˆαn+1: ˆαn
q"
REFERENCES,0.44889779559118237,"1 −ˆβn
=
ˆαn
q"
REFERENCES,0.45090180360721444,1 −ˆβn
REFERENCES,0.4529058116232465,"1 −ˆβn
=
ˆαn+1
1 −ˆβn
< 1 ⇐⇒ˆβn < 1 −ˆαn+1.
(17)"
REFERENCES,0.45490981963927857,"Now, we have ˆβn < min
n
1 −
ˆα2
n+1
1−ˆβn+1 , 1 −ˆαn+1
o
. When 1 −ˆαn+1 < 1 −
ˆα2
n+1
1−ˆβn+1 , we can show"
REFERENCES,0.45691382765531063,that ˆβn+1 < 1 −ˆαn+1:
REFERENCES,0.4589178356713427,"1 −ˆαn+1 < 1 −
ˆα2
n+1
1 −ˆβn+1
= 1 −ˆα2
n ⇐⇒ˆαn+1 > ˆα2
n ⇐⇒ˆα2
n+1
ˆα2n
> ˆαn+1
(18)"
REFERENCES,0.46092184368737477,"⇐⇒1 −ˆα2
n+1
ˆα2n
< 1 −ˆαn+1 ⇐⇒ˆβn+1 < 1 −ˆαn+1.
(19)"
REFERENCES,0.46292585170340683,"By the assumption of monotonic sequence, we also have ˆβn < ˆβn+1. Knowing that ˆβn+1 <"
REFERENCES,0.4649298597194389,"1−ˆαn+1 is always true, we obtain a tighter bound for ˆβn: 0 < ˆβn < min
n
1 −
ˆα2
n+1
1−ˆβn+1 , ˆβn+1
o
."
REFERENCES,0.46693386773547096,"A.2
DERIVING THE TRAINING OBJECTIVE FOR SCORE NETWORK"
REFERENCES,0.46893787575150303,"First, followed from the data distribution modeling of BDDMs as proposed in Eq. (8):"
REFERENCES,0.4709418837675351,"pθ(ˆx0) := Eˆxn−1∼q ˆ
β(ˆxn−1;xt,ϵn)

Eˆx1:n−2∼pθ(ˆx1:n−2|ˆxn−1) [pθ(ˆx0|ˆx1:n−1)]

,
(20)"
REFERENCES,0.4729458917835671,we can derive a new lower bound to the log marginal likelihood as follows:
REFERENCES,0.4749498997995992,Published as a conference paper at ICLR 2022
REFERENCES,0.47695390781563124,"Proposition 1. Given xt ∼qβ(xt|x0), the following lower bound holds for n ∈{2, . . . , N}:"
REFERENCES,0.4789579158316633,"log pθ(ˆx0) ≥F(n)
score(θ) := −L(n)
score(θ) −Rθ(ˆx0, xt),
(21) where"
REFERENCES,0.48096192384769537,"L(n)
score(θ) := DKL

pθ(ˆxn−1|ˆxn = xt)||q ˆβ(ˆxn−1; xt, ϵn)

,
(22)"
REFERENCES,0.48296593186372744,"Rθ(ˆx0, xt) := −Epθ(ˆx1|ˆxn=xt) [log pθ(ˆx0|ˆx1)] .
(23)"
REFERENCES,0.4849699398797595,Proof.
REFERENCES,0.48697394789579157,"log pθ(ˆx0) = log
Z
pθ(ˆx0:n−2|ˆxn−1)q ˆβ(ˆxn−1; xt, ϵn)dˆx1:n−1
(24)"
REFERENCES,0.48897795591182364,"= log
Z
pθ(ˆx0:n−2|ˆxn−1)q ˆβ(ˆxn−1; xt, ϵn)pθ(ˆx1:n−1|ˆxn = xt)"
REFERENCES,0.4909819639278557,pθ(ˆx1:n−1|ˆxn = xt)dˆx1:n−1 (25)
REFERENCES,0.49298597194388777,"= log Epθ(ˆx1,n−1|ˆxn=xt)"
REFERENCES,0.49498997995991983,"""
pθ(ˆx0|ˆx1)q ˆβ(ˆxn−1; xt, ϵn)"
REFERENCES,0.4969939879759519,pθ(ˆxn−1|ˆxn = xt) # (26)
REFERENCES,0.49899799599198397,"[Jensen’s Inequality] ≥Epθ(ˆx1,ˆxn−1|ˆxn=xt) """
REFERENCES,0.501002004008016,"log
pθ(ˆx0|ˆx1)q ˆβ(ˆxn−1; xt, ϵn)"
REFERENCES,0.503006012024048,pθ(ˆxn−1|ˆxn = xt) # (27)
REFERENCES,0.5050100200400801,"=Epθ(ˆx1|ˆxn=xt) [log pθ(ˆx0|ˆx1)] −DKL

pθ(ˆxn−1|ˆxn = xt)||q ˆβ(ˆxn−1; xt, ϵn)
 (28)"
REFERENCES,0.5070140280561122,"= −L(n)
score(θ) −Rθ(ˆx0, xt)
(29)"
REFERENCES,0.5090180360721442,"Next, we show that the score network θ trained with L(t)
ddpm(θ) can be re-used in BDDMs. We ﬁrst
provide the derivation for Eq. (9- 10). We have"
REFERENCES,0.5110220440881763,"q ˆβ(ˆxn−1; xt, ϵn) := q ˆβ "
REFERENCES,0.5130260521042084,"ˆxn−1|ˆxn = xt, ˆx0 = xt −
p"
REFERENCES,0.5150300601202404,"1 −ˆα2nϵn
ˆαn ! (30) = N "
REFERENCES,0.5170340681362725,ˆαn−1 ˆβn
REFERENCES,0.5190380761523046,1 −ˆα2n
REFERENCES,0.5210420841683366,"xt −
p"
REFERENCES,0.5230460921843687,"1 −ˆα2nϵn
ˆαn
+ q"
REFERENCES,0.5250501002004008,"1 −ˆβn(1 −ˆα2
n−1)"
REFERENCES,0.5270541082164328,"1 −ˆα2n
xt, 1 −ˆα2
n−1
1 −ˆα2n
ˆβnI "
REFERENCES,0.5290581162324649,"
(31) = N   "
REFERENCES,0.531062124248497,"
ˆαn−1 ˆβn
ˆαn(1 −ˆα2n) + q"
REFERENCES,0.533066132264529,"1 −ˆβn(1 −ˆα2
n−1)"
REFERENCES,0.5350701402805611,1 −ˆα2n 
REFERENCES,0.5370741482965932,"xt −
ˆαn−1 ˆβn
ˆαn
p"
REFERENCES,0.5390781563126252,"1 −ˆα2n
ϵn, 1 −ˆα2
n−1
1 −ˆα2n
ˆβnI "
REFERENCES,0.5410821643286573,"
(32) = N  
1
q"
REFERENCES,0.5430861723446894,"1 −ˆβn
xt −
ˆβn
q"
REFERENCES,0.5450901803607214,"(1 −ˆβn)(1 −ˆα2n)
ϵn, 1 −ˆα2
n−1
1 −ˆα2n
ˆβnI "
REFERENCES,0.5470941883767535,".
(33)"
REFERENCES,0.5490981963927856,"Proposition 2. Suppose xt ∼qβ(xt|x0), then any solution satisfying θ∗= argminθL(t)
ddpm(θ), ∀t ∈"
REFERENCES,0.5511022044088176,"{1, ..., T}, also satisﬁes θ∗= argminθL(n)
score(θ), ∀n ∈{2, ..., N}."
REFERENCES,0.5531062124248497,"Proof. By the deﬁnition in Eq. (4), we have"
REFERENCES,0.5551102204408818,"pθ(ˆxn−1|ˆxn = xt) =N  
1
q"
REFERENCES,0.5571142284569138,1 −ˆβn 
REFERENCES,0.5591182364729459,"xt −
ˆβn
p"
REFERENCES,0.561122244488978,"1 −ˆα2n
ϵθ (xt, ˆαn) !"
REFERENCES,0.56312625250501,", 1 −ˆα2
n−1
1 −ˆα2n
ˆβnI "
REFERENCES,0.5651302605210421,".
(34)"
REFERENCES,0.5671342685370742,"Here, from the training objective in Eq. (5), since xt = αtx0+
p"
REFERENCES,0.5691382765531062,"1 −α2
tϵn, the noise scale argument
for the score network is known to be αt. Therefore, we can use ϵθ (xt, αt) instead of ϵθ (xt, ˆαn) for"
REFERENCES,0.5711422845691383,Published as a conference paper at ICLR 2022
REFERENCES,0.5731462925851704,"expanding L(n)
score(θ). Since pθ(ˆxn−1|ˆxn = xt) and q ˆβ(ˆxn−1; xt, ϵn) are two isotropic Gaussians
with the same variance, the KL divergence is a scaled ℓ2-norm of their means’ difference:"
REFERENCES,0.5751503006012024,"L(n)
score(θ) :=DKL

pθ(ˆxn−1|ˆxn = xt)||q ˆβ(ˆxn−1; xt, ϵn)

(35)"
REFERENCES,0.5771543086172345,"=
1 −ˆα2
n
2(1 −ˆα2
n−1)ˆβn "
Q,0.5791583166332666,"1
q"
Q,0.5811623246492986,1 −ˆβn 
Q,0.5831663326653307,"xt −
ˆβn
p"
Q,0.5851703406813628,"1 −ˆα2n
ϵθ (xt, αt) ! (36) −  
1
q"
Q,0.5871743486973948,"1 −ˆβn
xt −
ˆβn
q"
Q,0.5891783567134269,"(1 −ˆβn)(1 −ˆα2n)
ϵn    2 2 (37)"
Q,0.591182364729459,"= (1 −ˆβn)(1 −ˆα2
n)"
Q,0.593186372745491,"2(1 −ˆβn −ˆα2n)ˆβn  ˆβn
q"
Q,0.5951903807615231,"(1 −ˆβn)(1 −ˆα2n)
(ϵn −ϵθ (xt, αt))  2 2 (38)"
Q,0.5971943887775552,"= (1 −ˆβn)(1 −ˆα2
n)"
Q,0.5991983967935872,2(1 −ˆβn −ˆα2n)ˆβn
Q,0.6012024048096193,"ˆβ2
n
(1 −ˆα2n)(1 −ˆβn)
∥ϵn −ϵθ (xt, αt)∥2
2
(39)"
Q,0.6032064128256514,"=
ˆβn
2(1 −ˆβn −ˆα2n)"
Q,0.6052104208416834,ϵn −ϵθ
Q,0.6072144288577155,"
αtx0 +
q"
Q,0.6092184368737475,"1 −α2
tϵn, αt  2"
Q,0.6112224448897795,"2
,
(40)"
Q,0.6132264529058116,"which is proportional to L(t)
ddpm :=
ϵn −ϵθ

αtx0 +
p"
Q,0.6152304609218436,"1 −α2
tϵn, αt

2"
Q,0.6172344689378757,"2 as deﬁned in Eq. (5).
Thus,"
Q,0.6192384769539078,"argminθL(t)
ddpm(θ) ≡argminθL(n)
score(θ).
(41)"
Q,0.6212424849699398,"Next, we can simplify Rθ(ˆx0, xt) to a reconstruction loss for ˆx0:
Rθ(ˆx0, xt) := −Epθ(ˆx1|ˆxn=xt) [log pθ(ˆx0|ˆx1)]
(42)"
Q,0.6232464929859719,=Epθ(ˆx1|ˆxn=xt) 
Q,0.625250501002004,"log N  
1
q"
Q,0.627254509018036,1 −ˆβ1 
Q,0.6292585170340681,"ˆx1 −
ˆβ1
p"
Q,0.6312625250501002,"1 −ˆα2
1
ϵθ(ˆx1, ˆα1), ˆβ1I !  "
Q,0.6332665330661322,"
(43)"
Q,0.6352705410821643,=Epθ(ˆx1|ˆxn=xt)  D
Q,0.6372745490981964,"2 log 2π ˆβ1 +
1 2ˆβ1"
Q,0.6392785571142284,"ˆx0 −
1
q"
Q,0.6412825651302605,1 −ˆβ1 
Q,0.6432865731462926,"ˆx1 −
ˆβ1
q"
Q,0.6452905811623246,"ˆβ1
ϵθ(ˆx1, ˆα1)    2 2   (44) =D"
Q,0.6472945891783567,"2 log 2π ˆβ1 +
1"
Q,0.6492985971943888,"2ˆβ1
Epθ(ˆx1|ˆxn=xt)  "
Q,0.6513026052104208,"ˆx0 −
1
q"
Q,0.6533066132264529,1 −ˆβ1
Q,0.655310621242485,"
ˆx1 −
q"
Q,0.657314629258517,"ˆβ1ϵθ(ˆx1, ˆα1)
 2 2  ,"
Q,0.6593186372745491,"(45)
where pθ(ˆx1|ˆxn = xt) can be efﬁciently sampled using the reverse process in (Song et al., 2021).
Yet, in practice, similar to the training in (Song et al., 2021; Chen et al., 2020; Kong et al., 2021),
we dropped Rθ(ˆx0, xt) when training θ. In theory, we know that Rθ(ˆx0, xt) achieves its optimal
value at θ∗= argminθ∥ϵθ(ˆx1, ˆα1)−ϵ1∥2
2, which shares a similar objective as L(t)
ddpm. By minimizing"
Q,0.6613226452905812,"L(t)
ddpm, we train a score network θ∗that best minimizes ∆ϵt := ∥ϵt −ϵθ∗(αtx0 +
p"
Q,0.6633266533066132,"1 −α2
tϵt, αt)∥2
2
for all 1 ≤t ≤T. Since the ﬁrst diffusion step has the smallest effect on corrupting ˆx0 (i.e.,
β1 ≈0), it sufﬁces to consider a ˆα1 = √1 −β1 = α1, in which case we can jointly minimize
Rθ(ˆx0, xt) by minimizing L(1)
ddpm."
Q,0.6653306613226453,"In this sense, during training, given xt ∼qβ(xt|x0), we can train the score network with the same
training objective as in DDPMs and DDIMs. Practically, it is beneﬁcial for BDDMs as we can re-use
the score network θ of any well-trained DDPM or DDIM."
Q,0.6673346693386774,Published as a conference paper at ICLR 2022
Q,0.6693386773547094,"A.3
DERIVING THE TRAINING OBJECTIVE FOR SCHEDULE NETWORK"
Q,0.6713426853707415,"Given that θ can be trained to maximize the log evidence with the pre-speciﬁed noise schedule β for
training, the consequent question of interest in BDDMs is how to ﬁnd a fast and good enough noise
schedule ˆβ ∈RN for sampling given an optimized θ∗. In BDDMs, this problem is reduced to how
to effectively learn the network parameters φ ."
Q,0.6733466933867736,"Proposition 3. Suppose θ has been optimized and hypothetically converged to the optimal θ∗,
where by optimal it means that with θ∗we have pθ∗(ˆxn−1|ˆxn = xt) = q ˆβ(ˆxn−1; xt, ϵn) given"
Q,0.6753507014028056,"xt ∼qβ(xt|x0). When ˆβ is unknown but we have x0 = ˆx0 and ˆαn = αt, we can minimize the
gap between the optimal lower bound F(n)
score(θ∗) and log pθ∗(ˆx0), i.e, log pθ∗(ˆx0) −F(n)
score(θ∗), by
minimizing the following objective with respect to ˆβn:"
Q,0.6773547094188377,"L(n)
step(ˆβn; θ∗) :=DKL

pθ∗(ˆxn−1|ˆxn = xt)||q ˆβn(ˆxn−1|x0; αt)

(46)"
Q,0.6793587174348698,"=
δt
2(δt −ˆβn)"
Q,0.6813627254509018,"ϵn −
ˆβn
δt
ϵθ∗

αtx0 +
p"
Q,0.6833667334669339,"δtϵn, αt
 2"
Q,0.685370741482966,"2
+ C,
(47) where"
Q,0.687374749498998,"δt = 1 −α2
t,
C = 1"
Q,0.6893787575150301,4 log δt
Q,0.6913827655310621,"ˆβn
+ D 2 ˆβn δt
−1 !"
Q,0.6933867735470942,".
(48)"
Q,0.6953907815631263,"Proof. Note that ˆx0 = x0, ˆαn = αt, xt = αtx0 +
p"
Q,0.6973947895791583,"1 −α2
tϵn and pθ∗(ˆxn−1|ˆxn = xt) =
q ˆβ(ˆxn−1; xt, ϵn). When x0 is given to pθ∗, we can express the probability as follows:"
Q,0.6993987975951904,"pθ∗(ˆxn−1|ˆxn = xt(x0), ˆx0 = x0)
(49)"
Q,0.7014028056112225,"=
Z
N(z; 0, I)pθ∗(ˆxn−1|ˆxn = αtx0 +
q"
Q,0.7034068136272545,"1 −α2
tz)dz
(50)"
Q,0.7054108216432866,"=
Z
N(z; 0, I)q ˆβ(ˆxn−1; xt = αtx0 +
q"
Q,0.7074148296593187,"1 −α2
tz, ϵn = z)dz
(51)"
Q,0.7094188376753507,"=
Z
N(z; 0, I)N "
Q,0.7114228456913828,"ˆxn−1; αtx0 +
p"
Q,0.7134268537074149,"1 −α2
tz
q"
Q,0.7154308617234469,"1 −ˆβn
−
ˆβn
q"
Q,0.717434869739479,"(1 −ˆβn)(1 −ˆα2n)
z, 1 −ˆα2
n−1
1 −ˆα2n
ˆβnI  dz (52)"
Q,0.7194388777555111,"[See Eq. (2) in (Frey, 1999)] =N "
Q,0.7214428857715431,"
ˆxn−1;
αtx0
q"
Q,0.7234468937875751,"1 −ˆβn
,  
  
p"
Q,0.7254509018036072,"1 −α2
t
q"
Q,0.7274549098196392,"1 −ˆβn
−
ˆβn
q"
Q,0.7294589178356713,"(1 −ˆβn)(1 −α2
t)   2"
Q,0.7314629258517034,"+ 1 −α2
t/(1 −ˆβn)
1 −α2
t
ˆβn  
I  
 (53) =N "
Q,0.7334669338677354,"ˆxn−1;
αtx0
q"
Q,0.7354709418837675,"1 −ˆβn
, 1 −α2
t −ˆβn
1 −ˆβn
I "
Q,0.7374749498997996,"=: q ˆβn(ˆxn−1; x0, αt),
(54)"
Q,0.7394789579158316,"where, different from pθ∗(ˆxn−1|ˆxn = xt), from Eq. (49) to Eq. (50), instead of conditioning on a
speciﬁc xt, when x0 is given xt can be generated using any z ∼N(0, I)."
Q,0.7414829659318637,Published as a conference paper at ICLR 2022
Q,0.7434869739478958,"From this, we can express the gap between log pθ∗(ˆx0) and F(n)
score(θ∗) in the following form:"
Q,0.7454909819639278,"log pθ∗(ˆx0 = x0) −F(n)
score(θ∗)
(55)"
Q,0.7474949899799599,"= log pθ∗(ˆx0 = x0) −Epθ∗(ˆx1,n−1|ˆxn=xt) """
Q,0.749498997995992,"log
pθ(ˆx0|ˆx1)q ˆβ(ˆxn−1; xt, ϵn)"
Q,0.751503006012024,pθ(ˆxn−1|ˆxn = xt) # (56)
Q,0.7535070140280561,= log pθ∗(ˆx0 = x0) −Epθ∗(ˆx1:n−1|ˆxn=xt)
Q,0.7555110220440882,"
log pθ∗(ˆx0:n−1|ˆxn = xt)"
Q,0.7575150300601202,pθ∗(ˆx1:n−1|ˆxn = xt)
Q,0.7595190380761523,"
(57)"
Q,0.7615230460921844,=Epθ∗(ˆx1:n−1|ˆxn=xt)
Q,0.7635270541082164,"
log
pθ∗(ˆx1:n−1|ˆxn = xt)
pθ∗(ˆx1:n−1|ˆxn = xt, ˆx0 = x0)"
Q,0.7655310621242485,"
(58)"
Q,0.7675350701402806,"=Epθ∗(ˆxn−1|ˆxn=xt) """
Q,0.7695390781563126,log pθ∗(ˆxn−1|ˆxn = xt)
Q,0.7715430861723447,"q ˆβn(ˆxn−1; x0, αt) # (59)"
Q,0.7735470941883767,"=DKL

pθ∗(ˆxn−1|ˆxn = xt)||q ˆβn(ˆxn−1; x0, αt)

(60)"
Q,0.7755511022044088,"Next, we evaluate the above KL divergence term. By deﬁnition, we have"
Q,0.7775551102204409,"pθ∗(ˆxn−1|ˆxn = xt) = N  
1
q"
Q,0.779559118236473,1 −ˆβn 
Q,0.781563126252505,"xt −
ˆβn
p"
Q,0.7835671342685371,"1 −ˆα2n
ϵθ∗(xt, ˆαn) !"
Q,0.7855711422845691,", 1 −ˆα2
n−1
1 −ˆα2n
ˆβnI "
Q,0.7875751503006012,"
(61)"
Q,0.7895791583166333,"Together with Eq. (54), we have"
Q,0.7915831663326653,"L(n)
step(ˆβn; θ∗) := DKL

pθ∗(ˆxn−1|ˆxn = xt)||q ˆβn(ˆxn−1; x0, αt)

(62)"
Q,0.7935871743486974,"=
1 −ˆβn
2(1 −ˆβn −α2
t)  αt
q"
Q,0.7955911823647295,"1 −ˆβn
x0 −
1
q"
Q,0.7975951903807615,1 −ˆβn 
Q,0.7995991983967936,"xt −
ˆβn
p"
Q,0.8016032064128257,"1 −α2
t
ϵθ∗(xt, αt) ! 2 2"
Q,0.8036072144288577,"+ C
(63)"
Q,0.8056112224448898,"=
1 −ˆβn
2(1 −ˆβn −α2
t)  αt
q"
Q,0.8076152304609219,"1 −ˆβn
x0 −
1
q"
Q,0.8096192384769539,1 −ˆβn 
Q,0.811623246492986,"αtx0 +
q"
Q,0.8136272545090181,"1 −α2
tϵn −
ˆβn
p"
Q,0.8156312625250501,"1 −α2
t
ϵθ∗(xt, αt) ! 2 2 + C (64)"
Q,0.8176352705410822,"=
1 −ˆβn
2(1 −ˆβn −α2
t)  s"
Q,0.8196392785571143,"1 −α2
t
1 −βn
ϵn −
ˆβn
p"
Q,0.8216432865731463,"(1 −βn)(1 −α2
t)
ϵθ∗(xt, αt)  2 2"
Q,0.8236472945891784,"+ C
(65)"
Q,0.8256513026052105,"=
1 −α2
t
2(1 −ˆβn −α2
t)"
Q,0.8276553106212425,"ϵn −
ˆβn
1 −α2
t
ϵθ∗(xt, αt)  2"
Q,0.8296593186372746,"2
+ C
(66)"
Q,0.8316633266533067,"=
δt
2(δt −ˆβn)"
Q,0.8336673346693386,"ϵn −
ˆβn
δt
ϵθ∗

αtx0 +
p"
Q,0.8356713426853707,"δtϵn, αt
 2"
Q,0.8376753507014028,"2
+ C,
(67) where"
Q,0.8396793587174348,"δt = 1 −α2
t,
C = 1"
Q,0.8416833667334669,4 log δt
Q,0.843687374749499,"ˆβn
+ D 2 ˆβn δt
−1 !"
Q,0.845691382765531,".
(68)"
Q,0.8476953907815631,"As we use a schedule network φ to estimate ˆβn from (ˆαn+1, ˆβn+1) as deﬁned in Eq. (13), we obtain
the ﬁnal step loss for learning φ:"
Q,0.8496993987975952,"L(n)
step(φ; θ∗) =
δt
2(δt −ˆβn(φ))"
Q,0.8517034068136272,"ϵn −
ˆβn(φ)"
Q,0.8537074148296593,"δt
ϵθ∗(xt, αt)  2 2
+ 1"
LOG,0.8557114228456913,"4 log
δt
ˆβn(φ)
+ D 2"
LOG,0.8577154308617234,"ˆβn(φ) δt
−1 ! . (69)"
LOG,0.8597194388777555,Published as a conference paper at ICLR 2022
LOG,0.8617234468937875,"This proposed objective for training the schedule network can be interpreted as to better model
the data distribution (i.e., maximizing log pθ(ˆx0)) by correcting the gradient scale ˆβn for the next
reverse step (from ˆxn to ˆxn−1) given the gradient vector ϵθ∗estimated by the score network θ∗."
LOG,0.8637274549098196,"B
EXPERIMENTAL DETAILS"
LOG,0.8657314629258517,"B.1
CONVENTIONAL GRID SEARCH ALGORITHM FOR DDPMS"
LOG,0.8677354709418837,"We reproduced the grid search algorithm in (Chen et al., 2020), in which a 6-step noise schedule was
searched. In our paper, we generalized the grid search algorithm by similarly sweeping the N-step
noise schedule over the following possibilities with a bin width M = 9:"
LOG,0.8697394789579158,"{1, 2, 3, 4, 5, 6, 7, 8, 9} ⊗{10−6·N/N, 10−6·(N−1)/N, ..., 10−6·1/N},
(70)"
LOG,0.8717434869739479,"where ⊗denotes the cartesian product applied on two sets. LS-MSE was used as a metric to select
the solution during the search. When N = 6, we resemble the GS algorithm in (Chen et al., 2020).
Note that above searching method normally does not scale up to N > 6 steps for its exponential
computational cost O(9N)."
LOG,0.87374749498998,"B.2
HYPERPARAMETER SETTING IN BDDMS"
LOG,0.875751503006012,"Algorithm 2 took a skip factor τ to control the stride for training the schedule network. The value
of τ would affect the coverage of step sizes when training the schedule network, hence affecting
the predicted number of steps N for inference – the higher τ is, the shorter the predicted inference
schedule tends to be. We set τ = 66 for training the BDDM vocoders in this paper."
LOG,0.8777555110220441,"For initializing Algorithm 3 for noise scheduling, we could take as few as 1 training sample for
validation, perform a grid search on the hyperparameters {(ˆαN = 0.1αT i, ˆβN = 0.1j)} for i, j =
1, ..., 9, i.e., 81 possibilities in total, and use the PESQ measure as the selection metric. Then, the
predicted noise schedule corresponding to the maximum PESQ was stored and applied to the online
inference afterward, as shown in Algorithm 4. Note that this searching has a complexity of only
O(M 2) (e.g., M = 9 in this case), which is much more efﬁcient than O(M N) in the conventional
grid search algorithm in (Chen et al., 2020), as discussed in Section B.1."
LOG,0.8797595190380761,"B.3
IMPLEMENTATION DETAILS"
LOG,0.8817635270541082,"Our proposed BDDMs and the baseline methods were all implemented with the Pytorch library. The
score networks for the LJ and VCTK speech datasets were trained from scratch on a single NVIDIA
Tesla P40 GPU with batch size 32 for about 1M steps, which took about 3 days."
LOG,0.8837675350701403,"For the model architecture, we used the same architecture as in DiffWave (Kong et al., 2021) for
the score network with 128 residual channels; we adopted a lightweight GALR network (Lam et al.,
2021) for the schedule network. GALR was originally proposed for speech enhancement, so we con-
sidered it well suited for predicting the noise scales. For the conﬁguration of the GALR network, we
used a window length of 8 samples for encoding, a segment size of 64 for segmentation and only two
GALR blocks of 128 hidden dimensions, and other settings were inherited from (Lam et al., 2021).
To make the schedule network output with a proper range and dimension, we applied a sigmoid func-
tion to the last block’s output of the GALR network. Then the result was averaged over the segments
and the feature dimensions to obtain the predicted ratio: σφ(x) = AvgPool2D(σ(GALR(x))),
where GALR(·) denotes the GALR network, AvgPool2D(·) denotes the average pooling operation
applied to the segments and the feature dimensions, and σ(x) := 1/(1 + e−x). The same net-
work architecture was used for the NE approach for estimating α2
t and was shown better than the
ConvTASNet used in the original paper (San-Roman et al., 2021). It is also notable that the com-
putational cost of a schedule network is indeed fractional compared to the cost of a score network,
as predicting a noise scalar variable is intrinsically a relatively much easier task. Our GALR-based
schedule network, while being able to produce stable and reliable results, was about 3.6 times faster
than the score network. The training of schedule networks for BDDMs took only 10k steps to
converge, which consumed no more than an hour on a single GPU."
LOG,0.8857715430861723,Published as a conference paper at ICLR 2022
LOG,0.8877755511022044,Table 3: Ratings that have been used in evaluation of speech naturalness of synthetic samples.
LOG,0.8897795591182365,"Rating
Naturalness
Deﬁnition"
UNSATISFACTORY,0.8917835671342685,"1
Unsatisfactory
Very annoying, distortion is objectionable.
2
Poor
Annoying distortion, but not objectionable.
3
Fair
Perceptible distortion, slightly annoying.
4
Good
Slight perceptible level of distortion, but not annoying.
5
Excellent
Imperceptible level of distortion."
UNSATISFACTORY,0.8937875751503006,"Table 4: Performances of different noise schedules on the multi-speaker VCTK speech dataset, each
of which used the same score network (Chen et al., 2020) ϵθ(·) that was trained on VCTK for about
1M iterations."
UNSATISFACTORY,0.8957915831663327,"Noise schedule
LS-MSE (↓)
MCD (↓)
STOI (↑)
PESQ (↑)
MOS (↑)"
UNSATISFACTORY,0.8977955911823647,"DDPM (Ho et al., 2020; Chen et al., 2020)
8 steps (Grid Search)
101
2.09
0.787
3.31
4.22 ± 0.04
1,000 steps (Linear)
85.0
2.02
0.798
3.39
4.40 ± 0.05"
UNSATISFACTORY,0.8997995991983968,"DDIM (Song et al., 2021)
8 steps (Linear)
553
3.20
0.701
2.81
3.83 ± 0.04
16 steps (Linear)
412
2.90
0.724
3.04
3.88 ± 0.05
21 steps (Linear)
355
2.79
0.739
3.12
4.12 ± 0.05
100 steps (Linear)
259
2.58
0.759
3.30
4.27 ± 0.04"
UNSATISFACTORY,0.9018036072144289,"NE (San-Roman et al., 2021)
8 steps (Linear)
208
2.54
0.740
3.10
4.18 ± 0.04
16 steps (Linear)
183
2.53
0.742
3.20
4.26 ± 0.04
21 steps (Linear)
852
3.57
0.699
2.66
3.70 ± 0.03"
UNSATISFACTORY,0.9038076152304609,"BDDM (ˆαN, ˆβN)
8 steps (0.2, 0.9)
98.4
2.11
0.774
3.18
4.20 ± 0.04
16 steps (0.5, 0.5)
73.6
1.93
0.813
3.39
4.35 ± 0.05
21 steps (0.5, 0.1)
76.5
1.83
0.827
3.43
4.48 ± 0.06"
UNSATISFACTORY,0.905811623246493,"Regarding the image generation task, to demonstrate the generalizability of our method, we directly
adopted a score network pre-trained on the CIFAR-10 dataset implemented by a third-party open-
source repository. Regarding the schedule network, to demonstrate that it does not have to use
specialized architecture, we replaced GALR by the VGG11 (Simonyan & Zisserman, 2014), which
was also used by as a noise estimator in (San-Roman et al., 2021). The output dimension (number
of classes) of VGG11 was set to 1. Similar to the setting for GALR in speech synthesis, we added
a sigmoid activation to the last layer to ensure a [0, 1] output. Similar to the training in speech
domain, we trained the VGG11-based schedule networks while freezing the score networks for 10k
steps, which normally can be ﬁnished in about two hours."
UNSATISFACTORY,0.9078156312625251,"Our code for the speech vocoding and the image generation experiments will be uploaded to Github
after the ﬁnal decision of ICLR is released."
UNSATISFACTORY,0.9098196392785571,"B.4
CROWD-SOURCED SUBJECTIVE EVALUATION"
UNSATISFACTORY,0.9118236472945892,"All our Mean Opinion Score (MOS) tests were crowd-sourced. We refer to the MOS scores in (Pro-
tasio Ribeiro et al., 2011), and the scoring criteria have been included in Table 3 for completeness.
The samples were presented and rated one at a time by the testers."
UNSATISFACTORY,0.9138276553106213,"C
ADDITIONAL EXPERIMENTS"
UNSATISFACTORY,0.9158316633266533,"A
demonstration
page
at
https://bilateral-denoising-diffusion-model.
github.io shows some samples generated by BDDMs trained on LJ speech and VCTK datasets."
UNSATISFACTORY,0.9178356713426854,Published as a conference paper at ICLR 2022
UNSATISFACTORY,0.9198396793587175,"C.1
MULTI-SPEAKER SPEECH SYNTHESIS"
UNSATISFACTORY,0.9218436873747495,"In addition to the single-speaker speech synthesis, we evaluated BDDMs on the multi-speaker
speech synthesis benchmark VCTK (Yamagishi et al., 2019). VCTK consists of utterances sam-
pled at 48 KHz by 108 native English speakers with various accents. We split the VCTK dataset for
training and testing: 100 speakers were used for training the multi-speaker model and 8 speakers
for testing. We trained on a 44257-utterance subset (40 hours) and evaluated on a held-out 100-
utterance subset. For the score network, we used the Wavegrad architecture (Chen et al., 2020) so
as to examine whether the superiority of BDDMs remains in a different dataset and with a different
score network architecture."
UNSATISFACTORY,0.9238476953907816,"Results are presented in Table 4. For this multi-speaker VCTK dataset, we obtained consistent
observations with that for the single-speaker LJ dataset presented in the main paper. Again, the
proposed BDDM with only 16 or 21 steps outperformed the DDPM with 1,000 steps. To the best
of our knowledge, ours was the ﬁrst work that reported this degree of superior. When reducing
to 8 steps, BDDM obtained performance on par with (except for a worse PESQ) the costly grid-
searched 8 steps (which were unscalable to more steps) in DDPM. For NE, we could again observe
a degradation from its 16 steps to 21 steps, indicating the instability of NE for the VCTK dataset
likewise. In contrast, BDDM gave continuously improved performance while increasing the step
number."
UNSATISFACTORY,0.9258517034068137,"C.2
COMPARING DIFFERENT REVERSE PROCESSES FOR BDDMS"
UNSATISFACTORY,0.9278557114228457,"This section demonstrates that BDDMs do not restrict the sampling procedure to a specialized re-
verse process in Algorithm 4. In particular, we evaluated different reverse processes, including that
of DDPMs as shown in Eq. (4) and DDIMs (Song et al., 2021), for BDDMs and compared the ob-
jective scores on the generated samples. DDIMs (Song et al., 2021) formulate a non-Markovian gen-
erative process that accelerates the inference while keeping the same training procedure as DDPMs.
The original generative process in Eq. (4) in DDPMs is modiﬁed into"
UNSATISFACTORY,0.9298597194388778,"p(τ)
θ (x0:T ) := π(xT ) S
Y"
UNSATISFACTORY,0.9318637274549099,"i=1
p(γi)
θ
(xγi−1|xγi) ×
Y"
UNSATISFACTORY,0.9338677354709419,"t∈¯γ
p(t)
θ (x0|xt),
(71)"
UNSATISFACTORY,0.935871743486974,"where γ is a sub-sequence of length N of [1, ..., T] with γN = T, and ¯γ := {1, ..., T} \ γ is deﬁned
as its complement; Therefore, only part of the models are used in the sampling process."
UNSATISFACTORY,0.9378757515030061,"To achieve the above, DDIMs deﬁned a prediction function f (t)
θ (xt) that depends on ϵθ to predict
the observation x0 given xt directly:"
UNSATISFACTORY,0.9398797595190381,"f (t)
θ (xt) := 1 αt"
UNSATISFACTORY,0.9418837675350702,"
xt −
q"
UNSATISFACTORY,0.9438877755511023,"1 −α2
tϵθ(xt, αt)

.
(72)"
UNSATISFACTORY,0.9458917835671342,"By leveraging this prediction function, the conditionals in Eq. (71) are formulated as"
UNSATISFACTORY,0.9478957915831663,"p(γi)
θ
(xγi−1|xγi) = N
αγi−1"
UNSATISFACTORY,0.9498997995991983,"αγi
(xγi −ςϵθ(xγi, αγi)) , σ2
γiI

ifi ∈[N], i > 1
(73)"
UNSATISFACTORY,0.9519038076152304,"p(t)
θ (x0|xt) = N(f (t)
θ (xt), σ2
t I)
otherwise,
(74)"
UNSATISFACTORY,0.9539078156312625,"where the detailed derivation of σt and ς can be referred to (Song et al., 2021). In the original
DDIMs, the accelerated reverse process produces samples over the subsequence of β indexed by γ:
ˆβ = {βn|n ∈γ}. In BDDMs, to apply the DDIM reverse process, we use the ˆβ predicted by the
schedule network in place of a subsequence of the training schedule β."
UNSATISFACTORY,0.9559118236472945,"Finally. the objective scores are given in Table 5. Note that the subjective evaluation (MOS) is
omitted here since the other assessments above have shown that the MOS scores are highly corre-
lated with the objective measures, including STOI and PESQ. They indicate that applying BDDMs
to either DDPM or DDIM reverse process leads to comparable and competitive results. Meanwhile,
the results show some subtle differences: BDDMs over a DDPM reverse process gave slightly better
samples in terms of signal error and consistency metrics (i.e., LS-MSE and MCD), while BDDM
over a DDIM reverse process tended to generate better samples in terms of intelligibility and per-
ceptual metrics (i.e., STOI and PESQ)."
UNSATISFACTORY,0.9579158316633266,Published as a conference paper at ICLR 2022
UNSATISFACTORY,0.9599198396793587,"Table 5: Performances of different reverse processes for BDDMs on the VCTK speech dataset, each
of which used the same score network (Chen et al., 2020) ϵθ(·) and the same noise schedule."
UNSATISFACTORY,0.9619238476953907,"Noise schedule
LS-MSE (↓)
MCD (↓)
STOI (↑)
PESQ (↑)"
UNSATISFACTORY,0.9639278557114228,"BDDM (DDPM reverse process)
8 steps (0.3, 0.9, 1e−5)
91.3
2.19
0.936
3.22
16 steps (0.7, 0.1, 1e−6)
73.3
1.88
0.949
3.32
21 steps (0.5, 0.1, 1e−6)
72.2
1.91
0.950
3.33"
UNSATISFACTORY,0.9659318637274549,"BDDM (DDIM reverse process)
8 steps (0.3, 0.9, 1e−5)
91.8
2.19
0.938
3.26
16 steps (0.7, 0.1, 1e−6)
77.7
1.96
0.953
3.37
21 steps (0.5, 0.1, 1e−6)
77.6
1.96
0.954
3.39"
UNSATISFACTORY,0.9679358717434869,"Table 6: Comparing sampling methods for DDPM with different number of sampling steps in terms
of FIDs in CIFAR10."
UNSATISFACTORY,0.969939879759519,"Sampling method
Sampling steps
FID"
UNSATISFACTORY,0.9719438877755511,"DDPM (baseline) (Ho et al., 2020)
1000
3.17"
UNSATISFACTORY,0.9739478957915831,"DDPM (sub-VP) (Song et al., 2020b)
∼100
3.69"
UNSATISFACTORY,0.9759519038076152,"DDPM (DP + reweighting) (Watson et al., 2021)
128
5.24
64
6.74"
UNSATISFACTORY,0.9779559118236473,"DDIM (quadratic) (Song et al., 2021)
100
4.16
50
4.67"
UNSATISFACTORY,0.9799599198396793,"FastDPM (approx. STEP) (Kong & Ping, 2021)
100
2.86
50
3.20"
UNSATISFACTORY,0.9819639278557114,"2Improved DDPM (hybrid) (Nichol & Dhariwal, 2021)
100
4.63
50
5.09"
UNSATISFACTORY,0.9839679358717435,"VDM (augmented) (Kingma et al., 2021)
1000
7.413"
UNSATISFACTORY,0.9859719438877755,"Ours BDDM
100
2.38
50
2.93"
UNSATISFACTORY,0.9879759519038076,"C.3
UNCONDITIONAL IMAGE GENERATION"
UNSATISFACTORY,0.9899799599198397,"For the unconditional image generation task, we evaluated the proposed BDDMs on the benchmark
CIFAR-10 (32 × 32) dataset. The score functions, including those initially proposed in DDPMs
(Ho et al., 2020) or DDIMs (Song et al., 2021) and those pre-trained in the above third-party im-
plementations, are all conditioned on a discrete step-index. We estimated the noise schedule ˆβ in
continuous space using the VGG11 schedule network and then mapped it to discrete time schedule
using the approximation method in (Kong & Ping, 2021)."
UNSATISFACTORY,0.9919839679358717,"Table 6 shows the performances of different sampling methods for DDPMs in CIFAR-10. By set-
ting the maximum number of sampling steps (N) for noise scheduling, we can fairly compare the
improvements achieved by BDDMs against related methods in the literature in terms of FID. Re-
markably, BDDMs with 100 sampling steps not only surpassed the 1000-step DDPM baseline, but
also produced the SOTA FID performance amongst all generative models using less than or equal to
100 sampling steps."
UNSATISFACTORY,0.9939879759519038,"2Our implementation was based on https://github.com/openai/improved-diffusion
3The authors of VDM claimed that they tuned the hyperparameters only for minimizing the likelihood and
did not pursue further tuning of the model to improve FID."
UNSATISFACTORY,0.9959919839679359,Published as a conference paper at ICLR 2022 𝑛= 3 𝑛= 2 𝑛= 1 𝑛= 0
UNSATISFACTORY,0.9979959919839679,"Figure 4: Spectrum plots of the speech samples produced by BDDM within 3 sampling steps. The
ﬁrst row shows the spectrum of a random signal for starting the reverse process. Then, from the top
to the bottom, we show the spectrum of the resultant signal after each step of the reverse process
performed by the BDDM. We also provide the corresponding WAV ﬁles on our demo page."
