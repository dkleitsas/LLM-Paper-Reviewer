Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0020491803278688526,"Federated learning data is drawn from a distribution of distributions: clients are
drawn from a meta-distribution, and their data are drawn from local data distri-
butions. Thus generalization studies in federated learning should separate perfor-
mance gaps from unseen client data (out-of-sample gap) from performance gaps
from unseen client distributions (participation gap). In this work, we propose a
framework for disentangling these performance gaps. Using this framework, we
observe and explain differences in behavior across natural and synthetic federated
datasets, indicating that dataset synthesis strategy can be important for realistic
simulations of generalization in federated learning. We propose a semantic syn-
thesis strategy that enables realistic simulation without naturally-partitioned data.
Informed by our ﬁndings, we call out community suggestions for future federated
learning works."
INTRODUCTION,0.004098360655737705,"1
INTRODUCTION"
INTRODUCTION,0.006147540983606557,"Federated learning (FL) enables distributed clients to train a machine learning model collaboratively
via focused communication with a coordinating server. In cross-device FL settings, clients are
sampled from a population for participation in each round of training (Kairouz et al., 2019; Li et al.,
2020a). Each participating client possesses its own data distribution, from which ﬁnite samples are
drawn for federated training."
INTRODUCTION,0.00819672131147541,"Given this problem framing, deﬁning generalization in FL is not as obvious as in centralized learning.
Existing works generally characterize the difference between empirical and expected risk for clients
participating in training (Mohri et al., 2019; Yagli et al., 2020; Reddi et al., 2021; Karimireddy
et al., 2020; Yuan et al., 2021). However, in cross-device settings, which we focus on in this work,
clients are sampled from a large population with unreliable availability. Many or most clients may
never participate in training (Kairouz et al., 2019; Singhal et al., 2021). Thus it is crucial to better
understand expected performance for non-participating clients."
INTRODUCTION,0.010245901639344262,"In this work, we model clients’ data distributions as drawn from a meta population distribution (Wang
et al., 2021), an assumption we argue is reasonable in real-world FL settings. We use this framing
to deﬁne two generalization gaps to study in FL: the out-of-sample gap, or the difference between
empirical and expected risk for participating clients, and the participation gap, or the difference
in expected risk between participating and non-participating clients. Previous works generally
ignore the participation gap or fail to disentangle it from the out-of-sample gap, but we observe
signiﬁcant participation gaps in practice across six federated datasets (see Figure 1), indicating that
the participation gap is an important but neglected feature of generalization in FL."
INTRODUCTION,0.012295081967213115,"We present a systematic study of generalization in FL across six tasks. We observe that focusing
only on out-of-sample gaps misses important effects, including differences in generalization behavior
across naturally-partitioned and synthetically-partitioned federated datasets. We use our results to
inform a series of recommendations for future works studying generalization in FL."
INTRODUCTION,0.014344262295081968,∗Work was completed while at Google Research.
INTRODUCTION,0.01639344262295082,Published as a conference paper at ICLR 2022
INTRODUCTION,0.018442622950819672,"0
1000
2000
3000
round 0.97 0.98 0.99 1.00"
INTRODUCTION,0.020491803278688523,accuracy
INTRODUCTION,0.022540983606557378,EMNIST-10
INTRODUCTION,0.02459016393442623,"0
1000
2000
3000
round 0.800 0.825 0.850 0.875 0.900 0.925"
INTRODUCTION,0.02663934426229508,EMNIST-62
INTRODUCTION,0.028688524590163935,"500
1000
1500
round 0.7 0.8 0.9 1.0"
INTRODUCTION,0.030737704918032786,CIFAR-10
INTRODUCTION,0.03278688524590164,"500
1000
1500
round 0.4 0.6 0.8 1.0"
INTRODUCTION,0.03483606557377049,CIFAR-100
INTRODUCTION,0.036885245901639344,"0
1000
2000
3000
round 0.50 0.52 0.54 0.56 0.58 0.60"
SHAKESPEARE,0.0389344262295082,"0.62
Shakespeare"
SHAKESPEARE,0.040983606557377046,"0
2000
4000
6000
round 0.22 0.24 0.26"
STACKOVERFLOW,0.0430327868852459,"0.28
Stackoverflow"
STACKOVERFLOW,0.045081967213114756,"participating training
participating validation
unparticipating"
STACKOVERFLOW,0.0471311475409836,"Figure 1: Federated training results demonstrating participation gaps for six different tasks.
We conduct experiments on four image classiﬁcation tasks and two text prediction tasks. As described
in Section 3.1, the participation gap can be estimated as the difference in metrics between participating
validation and unparticipating data (deﬁned in Figure 2). Prior works either ignore the participation
gap or fail to separate it from other generalization gaps, indicating the participation gap is a neglected
feature of generalization in FL."
STACKOVERFLOW,0.04918032786885246,Our contributions:
STACKOVERFLOW,0.05122950819672131,"• Propose a three-way split for measuring out-of-sample and participation gaps in centralized and
FL settings where data is drawn from a distribution of distributions (see Figure 2).
• Observe signiﬁcant participation gaps across six different tasks (see Figure 1) and perform
empirical studies on how various factors, e.g., number of clients and client diversity, affect
generalization performance (see Section 5).
• Observe signiﬁcant differences in generalization behavior across naturally-partitioned and
synthetically-partitioned federated datasets, and propose semantic partitioning, a dataset synthesis
strategy that enables more realistic simulations of generalization behavior in FL without requiring
naturally-partitioned data (see Section 4).
• Present a model to deﬁne the participation gap (Section 2), reveal its connection with data
heterogeneity (Section 3.2), and explain differences in generalization behavior between label-
based partitioning and semantic partitioning (Section 4.2).
• Present recommendations for future FL works, informed by our ﬁndings (see Section 6).
• Release an extensible open-source code library for studying generalization in FL (see Repro-
ducibility Statement)."
RELATED WORK,0.05327868852459016,"1.1
RELATED WORK"
RELATED WORK,0.055327868852459015,"We brieﬂy discuss primary related work here and provide a detailed review in Appendix A. We
refer readers to Kairouz et al. (2019); Wang et al. (2021) for a more comprehensive introduction to
federated learning in general."
RELATED WORK,0.05737704918032787,"Distributional heterogeneity in FL. Distributional heterogeneity is one of the most important
patterns in federated learning (Kairouz et al., 2019; Wang et al., 2021). Existing literature on FL
heterogeneity is mostly focused on the impact of heterogeneity on the training efﬁciency (convergence
and communication) of federated optimizers (Karimireddy et al., 2020; Li et al., 2020b; Reddi
et al., 2021). In this work, we identify that the participation gap is another major outcome of the
heterogeneity in FL, and recommend using the participation gap as a natural measurement for dataset
heterogeneity."
RELATED WORK,0.05942622950819672,"Personalized FL. In this work, we propose to evaluate and distinguish the generalization performance
of clients participating and non-participating in training. Throughout this work, we focus on the
classic FL setting (Kairouz et al., 2019; Wang et al., 2021) in which a single global model is learned
from and served to all clients. In the personalized FL setting (Hanzely & Richt´arik, 2020; Fallah
et al., 2020; Singhal et al., 2021), the goal is to learn and serve different models for different
clients. While related, our focus and contribution is orthogonal to personalization. In fact, our
three-way split framework can be readily applied in various personalized FL settings. For example,
for personalization via ﬁne-tuning (Wang et al., 2019; Yu et al., 2020), the participating clients can
be deﬁned as the clients that contribute to the training of the base model. The participation gap can
then be deﬁned as the difference in post-ﬁne-tuned performance between participating clients and
unparticipating clients."
RELATED WORK,0.06147540983606557,Published as a conference paper at ICLR 2022
RELATED WORK,0.06352459016393443,"Out-of-distribution generalization. In this work, we propose to train models using a set of par-
ticipating clients and examine their performance on heldout data from these clients as well as an
additional set of non-participating clients. Because each client has a different data distribution, unpar-
ticipating clients’ data exhibits distributional shift compared to the participating clients’ validation
data. Therefore, our work is related to the ﬁeld of domain adaptation (Daum´e III, 2009; Ben-David
et al., 2007; Shimodaira, 2000; Patel et al., 2015), where a model is explicitly adapted to make
predictions on a test set that is not identically distributed to the training set. The participation gap
that we observe is consistent with ﬁndings from the out-of-distribution research community (Ovadia
et al., 2019; Amodei et al., 2016; Lakshminarayanan et al., 2016), which shows on centrally trained
(non-federated) models that even small deviations in the morphology of deployment examples can
lead to systematic degradations in performance. Our setting differs from these other settings in
that our problem framing assumes data is drawn from a distribution of client distributions, meaning
that the training and deployment distributions eventually converge as more clients participate in
training. In contrast, the typical OOD setup assumes that the distributions will never converge (since
the deployment data is out-of-distribution, by deﬁnition it does not contribute to training). Our
meta-distribution assumption makes the problem of generalizing to unseen distributions potentially
more tractable."
SETUP FOR GENERALIZATION IN FL,0.06557377049180328,"2
SETUP FOR GENERALIZATION IN FL"
SETUP FOR GENERALIZATION IN FL,0.06762295081967214,"We model each FL client as a data source associated with a local distribution and the overall population
as a meta-distribution over all possible clients."
SETUP FOR GENERALIZATION IN FL,0.06967213114754098,"Deﬁnition 2.1 (Federated Learning Problem). 1. Let Ξ be the (possibly inﬁnite) collection of all the
possible data elements, e.g., image-label pairs. For any parameters w in parameter space Θ, we
use f(w, ξ) to denote the loss at element ξ ∈Ξ with parameter w."
SETUP FOR GENERALIZATION IN FL,0.07172131147540983,"2. Let C be the (possibly inﬁnite) collection of all the possible clients. Every client c ∈C is associated
with a local distribution Dc supported on Ξ."
SETUP FOR GENERALIZATION IN FL,0.07377049180327869,"3. Further, we assume there is a meta-distribution P supported on client set C, and each client c is
associated with a weight ρc for aggregation."
SETUP FOR GENERALIZATION IN FL,0.07581967213114754,The goal is to optimize the following two-level expected loss as follows:
SETUP FOR GENERALIZATION IN FL,0.0778688524590164,"F(w) := Ec∼P [ρc · Eξ∼Dc[f(w; ξ)]] .
(1)"
SETUP FOR GENERALIZATION IN FL,0.07991803278688525,"Similar formulations as in Equation (1) have been proposed in existing literature (Wang et al.,
2021; Reisizadeh et al., 2020; Charles & Koneˇcn`y, 2020). To understand Equation (1), consider a
random procedure that repeatedly draws clients c from the meta-distribution P and then evaluates
the loss on samples ξ drawn from the local data distribution Dc. Equation (1) then characterizes the
weighted-average limit of the above process."
SETUP FOR GENERALIZATION IN FL,0.08196721311475409,"Remark. The selection of client weights {ρc : c ∈C} depends on the desired aggregation pattern.
For example, setting ρc ≡1 will equalize the performance share across all clients. Another common
example is setting ρc to be proportional to the training dataset size contributed by client c."
SETUP FOR GENERALIZATION IN FL,0.08401639344262295,"Intuitive Justiﬁcation. The formulation in Equation (1) is especially natural in cross-device FL
settings, where the number of clients is generally large and modeling clients’ local distributions
as sampled from a meta-distribution is reasonable. This assumption also makes the problem of
generalization to non-participating client distributions more tractable since samples from the meta-
distribution are seen during training."
SETUP FOR GENERALIZATION IN FL,0.0860655737704918,"Discretization. While the ultimate goal is to optimize the expected loss over the entire meta-
distribution P and client local distributions {Dc : c ∈C}, only ﬁnite training data and a ﬁnite number
of clients are accessible during training. We call the subset of clients that contributes training data the
participating clients, denoted as ˆC. We assume ˆC is drawn from the meta-distribution P. For each
participating client c ∈ˆC, we denote ˆΞc the training data contributed by client c. We call these data
participating training client data and assume ˆΞc satisﬁes the local distribution Dc."
SETUP FOR GENERALIZATION IN FL,0.08811475409836066,Published as a conference paper at ICLR 2022
SETUP FOR GENERALIZATION IN FL,0.09016393442622951,Deﬁnition 2.2. The empirical risk on the participating training client data is deﬁned by
SETUP FOR GENERALIZATION IN FL,0.09221311475409837,Fpart train(w) := 1 |ˆC| X c∈ˆC  ρc ·  1 |ˆΞc| X
SETUP FOR GENERALIZATION IN FL,0.0942622950819672,"ξ∈|ˆΞc|
f(w; ξ)   "
SETUP FOR GENERALIZATION IN FL,0.09631147540983606,".
(2)"
SETUP FOR GENERALIZATION IN FL,0.09836065573770492,"Equation (2) characterizes the performance of the model (at parameter w) on the observed data
possessed by observed clients."
SETUP FOR GENERALIZATION IN FL,0.10040983606557377,"There are two levels of generalization between Equation (2) and Equation (1): (i) the generalization
from ﬁnite training data to unseen data, and (ii) the generalization from ﬁnite participating clients to
unseen clients. To disentangle the effect of the two levels, a natural intermediate stage is to consider
the performance on unseen data of participating (seen) clients.
Deﬁnition 2.3. The semi-empirical risk on the participating validation client data is deﬁned by"
SETUP FOR GENERALIZATION IN FL,0.10245901639344263,Fpart val(w) := 1 |ˆC| X
SETUP FOR GENERALIZATION IN FL,0.10450819672131148,"c∈ˆC
[ρc · (Eξ∼Dcf(w; ξ))] .
(3)"
SETUP FOR GENERALIZATION IN FL,0.10655737704918032,"Equation (3) differs from Equation (2) by replacing the intra-client empirical loss with the expected
loss over Dc. We shall also call F(w) deﬁned in Equation (1) the unparticipating expected risk and
denote it as Funpart(w) for consistency. Now we are ready to deﬁne the two levels of generalization
gaps formally.
Deﬁnition 2.4. The out-of-sample gap is deﬁned as Fpart val(w) −Fpart train(w).
Deﬁnition 2.5. The participation gap is deﬁned as Funpart(w) −Fpart val(w)."
SETUP FOR GENERALIZATION IN FL,0.10860655737704918,"Note that these gaps are also meaningful in centralized learning settings where data is sampled from
a distribution of distributions."
UNDERSTANDING GENERALIZATION GAPS,0.11065573770491803,"3
UNDERSTANDING GENERALIZATION GAPS"
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.11270491803278689,"3.1
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT"
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.11475409836065574,"Both Fpart val and Funpart take an expectation over the distribution of clients or data. To estimate these
two risks in practice, we propose splitting datasets into three blocks. The procedure is demonstrated
in Figure 2. Given a dataset with client assignment, we ﬁrst hold out a percentage of clients (e.g.,
20%) as unparticipating clients, as shown in the rightmost two columns (in purple). The remaining
clients are participating clients. We refer to this split as inter-client split. Within each participating
client, we hold out a percentage of data (e.g., 20%) as participating validation data, as shown in the
upper left block (in orange). The remaining data is the participating training client data, as shown in
the lower left block (in blue). We refer to this second split as intra-client split."
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.1168032786885246,"Figure 2: Illustration of the three-way split via a
visualization of the EMNIST digits dataset. Each
column corresponds to the dataset of one client. A
dataset is split into participating training, participating
validation, and unparticipating data, which enables
separate measurement of out-of-sample and participa-
tion gaps (unlike other works). Note we only present
the digit “6” for illustrative purposes."
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.11885245901639344,"Existing FL literature and benchmarks typically conduct either an inter-client or intra-client train-
validation split. However, neither inter-client nor intra-client split alone can reveal the participation
gap.1 To the best of our knowledge, this is the ﬁrst work that conducts both splits simultaneously."
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.12090163934426229,"3.2
WHY IS THE PARTICIPATION GAP INTERESTING?"
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.12295081967213115,"Participation gap is an intrinsic property of FL due to heterogeneity. Heterogeneity across
clients is one of the most important phenomena in FL. We identify that the participation gap is another"
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.125,"1To see this, observe that inter-client split can only estimate Fpart train and Funpart, and intra-client split can
only estimate Fpart train and Fpart val."
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.12704918032786885,Published as a conference paper at ICLR 2022
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.1290983606557377,"outcome of heterogeneity in FL, in that the gap will not exist if data is homogeneous. Formally, we
can establish the following proposition."
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.13114754098360656,"Proposition 3.1. If Dc ≡D for any c ∈C and ρc ≡ρ, then for any participating clients ˆC ⊂C and
w in domain, the participation gap is always zero in that Funpart(w) ≡Fpart val(w)."
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.13319672131147542,Proposition 3.1 holds by deﬁnition as
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.13524590163934427,Fpart val(w) = 1 |ˆC| X
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.13729508196721313,"c∈ˆC
[ρ · (Eξ∼Dcf(w; ξ))] = ρ·(Eξ∼Df(w; ξ)) = Ec∼P [ρ · Eξ∼D[f(w; ξ)]] = Funpart(w)."
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.13934426229508196,"Remark. We assume unweighted risk with ρc ≡ρ for ease of exposition. Even if ρc are different,
one can still show Funpart(w)"
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.1413934426229508,"Fpart val(w) is always equal to a constant independent of w. Therefore the triviality
of the participation gap for homogeneous data still holds in the logarithmitic sense."
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.14344262295081966,"Participation gap can quantify client diversity. The participation gap can provide insight into a
federated dataset since it provides a quantiﬁable measure of client diversity / heterogeneity. With other
aspects controlled, a federated dataset with larger participation gap tends to have greater heterogeneity.
For example, using the same model and hyperparameters, we observe in Section 5 that CIFAR-100
exhibits a larger participation gap than CIFAR-10. Unlike other indirect measures (such as the
degradation of federated performance relative to centralized performance), the participation gap is
intrinsic in federated datasets and more consistent with respect to training hyperparameters."
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.14549180327868852,"Participation gap can measure overﬁtting on the population distribution. Just as a generalization
gap that increases over time in centralized training can indicate overﬁtting on training samples, a
large or increasing participation gap can indicate a training process is overﬁtting on participating
clients. We observe this effect in Figure 1 for Shakespeare and Stack Overﬂow tasks. Thus measuring
this gap can be important for researchers developing models or algorithms to reduce overﬁtting."
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.14754098360655737,"Participation gap can quantify model robustness to unseen clients. From a modeler’s perspective,
the participation gap quantiﬁes the loss of performance incurred by switching from seen clients to
unseen clients. The smaller the participation gap is, the more robust the model might be when
deployed. Therefore, estimating participation gap may guide modelers to design more robust models,
regularizers, and training algorithms."
ESTIMATING RISKS AND GAPS VIA THE THREE-WAY SPLIT,0.14959016393442623,"Participation gap can quantify the incentive for clients to participate. From a client’s perspec-
tive, the participation gap offers a measure of the performance gain realized by switching from
unparticipating (not contributing training data) to participating (contributing training data). This is a
fair comparison since both Fpart val and Funpart are estimated on unseen data. When the participation
gap is large (e.g., if only a few clients participate), modelers might report the participation gap as a
well-justiﬁed incentive to encourage more clients to join a federated learning process."
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.15163934426229508,"4
REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.15368852459016394,"Since participation gaps can quantify client dataset heterogeneity, we study how participation gaps
vary for different types of federated datasets. Many prior works (McMahan et al., 2017; Zhao et al.,
2018; Hsu et al., 2019; Reddi et al., 2021) have created synthetic federated versions of centralized
datasets. These centralized datasets do not have naturally-occurring client partitions and thus need to
be synthetically partitioned into clients. Due to the importance of heterogeneity in FL, partitioning
schemes generally ensure client datasets are heterogeneous in some respect. Previous works typically
impose heterogeneity at the label level. For example, Hsu et al. (2019) create heterogeneous federated
datasets by assigning each client a distribution over labels, where each local distribution is drawn
from a Dirichlet meta-distribution. Once conditioned on labels, the drawing process is homogeneous.
We refer to these schemes as label-based partitioning.2"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.1557377049180328,"While label heterogeneity is generally observed in natural federated datasets, it is not the only
observed form of heterogeneity. In particular, each client in a natural federated dataset has its own"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.15778688524590165,"2To avoid confusion, throughout this work, we use the term “partition” to refer to assigning data with no
client assignment into synthetic clients. The term “split” refers to splitting a federated dataset (with existing
client assignments) to measure different metrics (e.g., three-way-split)."
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.1598360655737705,Published as a conference paper at ICLR 2022
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.16188524590163936,"separate data generating process. For example, for Federated EMNIST (Cohen et al., 2017), different
clients write characters using different handwriting. Label-based partitioning does not account for
this form of heterogeneity. To show this, in Figure 3 we visualize the clustering of client data between
natural and label-based partitioning (Hsu et al., 2019) for Federated EMNIST. We project clients
from each partitioning into a 2D space using T-SNE (Van der Maaten & Hinton, 2008) applied to
the raw pixel data. Naturally partitioned examples clearly cluster more than label-based partitioned
examples, which appear to be distributed similarly to the full data distribution."
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.16393442622950818,Natural Partitioning
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.16598360655737704,Label-Based Partitioning
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.1680327868852459,"Figure 3: T-SNE projection of different partitionings of EMNIST. The top
panel shows the naturally-partitioned dataset (partitioned by writer), the bottom
panel shows the label-based synthetic dataset. The gray points are the projec-
tions of examples from each dataset, obtained by aggregating the data from 100
clients each. The blue points show projections of data from a single client. The
naturally-partitioned client data appears much more tightly clustered, whereas the
label-based partitioned data appears similarly distributed as the overall dataset,
indicating that label-based partitioning may not fully represent realistic client
heterogeneity."
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.17008196721311475,"Interestingly, differences in heterogeneity also signiﬁcantly affect generalization behavior. In Figure 4,
we compare the training progress of the naturally-partitioned EMNIST dataset with a label-based
partitioning following the scheme by Hsu et al. (2019). Despite showing greater label heterogeneity
(Fig. 4(a)), the label-based partitioning does not recover any signiﬁcant participation gap, in sharp
contrast to the natural partitioning (Fig. 4(d)). In Figure 5, we also see minimal participation gap in
label-based partitioning for CIFAR. This motivates a client partitioning approach that better preserves
the generalization behavior of naturally-partitioned datasets. 0 10 20 30 count"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.1721311475409836,"client 0
client 1"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.17418032786885246,0 1 2 3 4 5 6 7 8 9
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.1762295081967213,digits 0 10 20 30 count
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.17827868852459017,client 2
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.18032786885245902,0 1 2 3 4 5 6 7 8 9
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.18237704918032788,digits
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.18442622950819673,client 3
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.1864754098360656,"(a) Label histogram of
label-based partitioning"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.1885245901639344,"0
1000
2000
3000
round 0.965 0.970 0.975 0.980 0.985 0.990 0.995 1.000 1.005"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.19057377049180327,accuracy
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.19262295081967212,"participating training
participating validation
unparticipating"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.19467213114754098,"(b) Learning progress of
label-based partitioning 0 5 10 15 count"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.19672131147540983,"client 0
client 1"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.1987704918032787,0 1 2 3 4 5 6 7 8 9
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.20081967213114754,digits 0 5 10 15 count
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.2028688524590164,client 2
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.20491803278688525,0 1 2 3 4 5 6 7 8 9
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.2069672131147541,digits
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.20901639344262296,client 3
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.21106557377049182,"(c) Label histogram of
natural partitioning"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.21311475409836064,"0
1000
2000
3000
round 0.965 0.970 0.975 0.980 0.985 0.990 0.995 1.000 1.005"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.2151639344262295,accuracy
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.21721311475409835,"participating training
participating validation
unparticipating"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.2192622950819672,"(d) Learning progress of
natural partitioning"
"REFLECTIONS ON CLIENT HETEROGENEITY AND SYNTHETIC
PARTITIONING",0.22131147540983606,"Figure 4:
Comparison of label-based synthetic partitioning and natural partitioning of
EMNIST-10. Observe that label-based partitioning shows greater label heterogeneity (a) than
natural partitioning (c), while the participation gap (part val −unpart) for label-based synthetic
partitioning (b) is signiﬁcantly smaller than that for the natural partitioning (d)."
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.22336065573770492,"4.1
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP"
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.22540983606557377,"To explore and remediate differences in client heterogeneity across natural and synthetic datasets,
we propose a semantics-based framework to assign semantically similar examples to clients during
federated dataset partitioning. We instantiate this framework via an example of an image classiﬁcation
task."
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.22745901639344263,"Our goal is to reverse-engineer the federated dataset-generating process described in Equation (1)
so that each client possesses semantically similar data. For example, for the EMNIST dataset, we
expect every client (writer) to (i) write in a consistent style for each digit (intra-client intra-label
similarity) and (ii) use a consistent writing style across all digits (intra-client inter-label similarity).
A simple approach might be to cluster similar examples together and sample client data from clusters.
However, if one directly clusters the entire dataset, the resulting clusters may end up largely correlated
to labels. To disentangle the effect of label heterogeneity and semantic heterogeneity, we propose the
following algorithm to enforce intra-client intra-label similarity and intra-client inter-label similarity
in two separate stages."
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.22950819672131148,Published as a conference paper at ICLR 2022
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.23155737704918034,"• Stage 1: For each label, we embed examples using a pretrained neural network (extracting
semantic features), and ﬁt a Gaussian Mixture Model to cluster pretrained embeddings into groups.
Note that this results in multiple groups per label. This stage enforces intra-client intra-label
consistency.
• Stage 2: To package the clusters from different labels into clients, we aim to compute an optimal
multi-partite matching with cost-matrix deﬁned by KL-divergence between the Gaussian clusters.
To reduce complexity, we heuristically solve the optimal multi-partite matching by progressively
solving the optimal bipartite matching at each time for randomly-chosen label pairs. This stage
enforces intra-client inter-label consistency."
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.2336065573770492,"We relegate the detailed setup to Appendix D. Using this procedure we can generate clients which
have similar example semantics. We show in Figure 5 that this method of partitioning preserves the
participation gap. In Appendix D, we visualize several examples of our semantic partitioning on
various datasets, which can serve as benchmarks for future works."
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.23565573770491804,"500
1000
1500
round 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95"
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.23770491803278687,accuracy
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.23975409836065573,CIFAR-10 | label partition
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.24180327868852458,"500
1000
1500
round 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95"
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.24385245901639344,CIFAR-10 | semantic partition
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.2459016393442623,"500
1000
1500
round 0.3 0.4 0.5 0.6 0.7 0.8"
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.24795081967213115,CIFAR-100 | (coarse)-label partition
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.25,"500
1000
1500
round 0.3 0.4 0.5 0.6 0.7 0.8"
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.2520491803278688,CIFAR-100 | semantic partition
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.2540983606557377,"participating training
participating validation
unparticipating"
SEMANTIC CLIENT PARTITIONING AND THE PARTICIPATION GAP,0.25614754098360654,"Figure 5: Comparison of label-based partitioning and semantic partitioning (ours). Results for
CIFAR-10 and CIFAR-100 are shown. Observe that semantic partitioning recovers the participation
gap typically observed in naturally-partitioned data."
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.2581967213114754,"4.2
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING"
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.26024590163934425,"To explain the above behavior, we revisit our mathematical setup and the deﬁnition of the participation
gap. Recall that the participation gap is deﬁned as (we omit the weights by setting ρc ≡1 for
simplicity):"
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.26229508196721313,Ipart gap(w) := Funpart(w) −Fpart val(w) = Ec∼P [Eξ∼Dc[f(w; ξ)]] −1 |ˆC| X
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.26434426229508196,"c∈ˆC
[Eξ∼Dc[f(w; ξ)]]
(4)"
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.26639344262295084,"In order to express the ideas without diving into details of measure theory, we assume without loss of
generality that the meta-distribution P is a continuous distribution supported on C with probability
density function pP(c). We also assume that for each client c ∈C, the local distribution Dc is a
continuous distribution supported on Ξ with probability density function pDc(ξ). Therefore, the
participation gap becomes"
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.26844262295081966,"Iparticipation(w) =
Z"
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.27049180327868855,"ξ∈Ξ
f(w; ξ) ·  
Z"
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.2725409836065574,"c∈C
pDc(ξ)pP(c)dc −1 |ˆC| X"
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.27459016393442626,"c∈ˆC
pDc(ξ) "
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.2766393442622951,"dξ.
(5)"
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.2786885245901639,"Therefore the scale of participation gap could depend (negatively) on the concentration speed
from
1
|ˆC|
P"
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.2807377049180328,"c∈ˆC pDc(ξ) to
R"
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.2827868852459016,"c∈C pDc(ξ)pP(c)dc as |ˆC| →∞.3 We hypothesize that for label-based
partitioning, the concentration is fast because each client has a large entropy as it can cover the entire
distribution of a given label. On the other hand, for natural or semantic partitioning, the concentration
is slower as the local distribution of each client has lower entropy due to the (natural or synthetic)
semantic clustering."
EXPLAINING DIFFERENCES BETWEEN LABEL-BASED AND SEMANTIC PARTITIONING,0.2848360655737705,"We validate our hypothesis with an empirical estimation of local dataset entropy, shown in Figure 6.
We observe that the clients generated by label-based partitioning demonstrate much higher entropy
than the natural ones. Notably, our proposed semantic partitioning has a very similar entropy
distribution across clients as the natural partitioning. This indicates that the heterogeneity in EMNIST
is mostly attributed to semantic heterogeneity."
ONE CAN MAKE THE ABOVE CLAIM RIGOROUS WITH STANDARD LEARNING THEORY APPROACHES SUCH AS UNIFORM CONVER-,0.28688524590163933,"3One can make the above claim rigorous with standard learning theory approaches such as uniform conver-
gence and Rademacher complexity (Vapnik, 1998)."
ONE CAN MAKE THE ABOVE CLAIM RIGOROUS WITH STANDARD LEARNING THEORY APPROACHES SUCH AS UNIFORM CONVER-,0.2889344262295082,Published as a conference paper at ICLR 2022 0.02 0.02 0.02 0.01 0.01 0.01 0.0 0.0 0.0
ONE CAN MAKE THE ABOVE CLAIM RIGOROUS WITH STANDARD LEARNING THEORY APPROACHES SUCH AS UNIFORM CONVER-,0.29098360655737704,Natural Partitioning
ONE CAN MAKE THE ABOVE CLAIM RIGOROUS WITH STANDARD LEARNING THEORY APPROACHES SUCH AS UNIFORM CONVER-,0.2930327868852459,Semantic Partitioning
ONE CAN MAKE THE ABOVE CLAIM RIGOROUS WITH STANDARD LEARNING THEORY APPROACHES SUCH AS UNIFORM CONVER-,0.29508196721311475,Label-Based Partitioning
ONE CAN MAKE THE ABOVE CLAIM RIGOROUS WITH STANDARD LEARNING THEORY APPROACHES SUCH AS UNIFORM CONVER-,0.29713114754098363,"Entropy
20           40           60           80           100        120"
ONE CAN MAKE THE ABOVE CLAIM RIGOROUS WITH STANDARD LEARNING THEORY APPROACHES SUCH AS UNIFORM CONVER-,0.29918032786885246,Density
ONE CAN MAKE THE ABOVE CLAIM RIGOROUS WITH STANDARD LEARNING THEORY APPROACHES SUCH AS UNIFORM CONVER-,0.3012295081967213,"Figure 6:
Kernel density estimates of the distribution of
client entropy for naturally-partitioned clients (top), semantic-
partitioned clients (middle), and label-based partitioned clients
(bottom). While naturally and semantically partitioned clients appear
to have approximately the same distribution of client entropies, the
synthetically partitioned clients are distributed differently and have
higher average entropy (48 Nats) than the other forms of partitioning
(44 Nats). We refer readers to Appendix E for the detailed methodol-
ogy for the estimation of the entropy."
ONE CAN MAKE THE ABOVE CLAIM RIGOROUS WITH STANDARD LEARNING THEORY APPROACHES SUCH AS UNIFORM CONVER-,0.30327868852459017,"Table 1: Summary of experimental results. We perform federated and centralized training across
six tasks. EMNIST, Shakespeare, and StackOverﬂow are naturally-partitioned, and CIFAR is
semantically partitioned. Observe that the participation gaps (gap between part val and unpart)
are consistent across tasks. We provide other metric statistics across clients (such as percentiles of
metrics) in Table 2 of Appendix B.2."
ONE CAN MAKE THE ABOVE CLAIM RIGOROUS WITH STANDARD LEARNING THEORY APPROACHES SUCH AS UNIFORM CONVER-,0.305327868852459,"Federated Training
Centralized Training"
ONE CAN MAKE THE ABOVE CLAIM RIGOROUS WITH STANDARD LEARNING THEORY APPROACHES SUCH AS UNIFORM CONVER-,0.3073770491803279,"part train
part val
unpart
part train
part val
unpart"
ONE CAN MAKE THE ABOVE CLAIM RIGOROUS WITH STANDARD LEARNING THEORY APPROACHES SUCH AS UNIFORM CONVER-,0.3094262295081967,"EMNIST-10
100.0
99.6
98.9
100.0
99.5
98.9
EMNIST-62
91.8
86.3
85.4
93.8
87.1
86.1
CIFAR-10
97.5
83.3
81.6
99.7
86.3
84.9
CIFAR-100
99.8
57.2
54.2
99.9
59.7
55.4
Shakespeare
58.8
56.5
56.2
60.7
57.2
56.8
StackOverﬂow
26.4
25.5
25.2
27.9
26.2
25.6"
EXPERIMENTAL EVALUATION,0.3114754098360656,"5
EXPERIMENTAL EVALUATION"
EXPERIMENTAL EVALUATION,0.3135245901639344,"We conduct experiments in six settings, including four image classiﬁcation tasks: EMNIST-10 (digits
only), EMNIST-62 (digits and characters) (Cohen et al., 2017; Caldas et al., 2019), CIFAR-10 and
CIFAR-100 (Krizhevsky et al., 2009); and two next character/word prediction tasks: Shakespeare
(Caldas et al., 2019) and StackOverﬂow (Reddi et al., 2021). We use FEDAVGM for image classiﬁca-
tion tasks and FEDADAM for text-based tasks (Reddi et al., 2021).4 The detailed setups (including
model, dataset preprocessing, hyperparameter tuning) are relegated to Appendix C. We summarize
our main results in Figure 1 and Table 1. In the following subsections, we provide more detailed
ablation studies exploring how various aspects of training affect generalization performance."
"EFFECT OF THE NUMBER OF PARTICIPATING CLIENTS
IN THIS SUBSECTION WE STUDY THE EFFECT OF THE NUMBER OF PARTICIPATING CLIENTS ON THE GENERALIZATION",0.3155737704918033,"5.1
EFFECT OF THE NUMBER OF PARTICIPATING CLIENTS
In this subsection we study the effect of the number of participating clients on the generalization
performance on various tasks. To this end, we randomly sample subsets of clients of different scales
as participating clients, and perform federated training with the same settings otherwise. The results
are shown in Figure 7. As the number of participating clients increases, the unparticipating accuracy
monotonically improves, and the participation gap tends to decrease. This is consistent with our
theoretical understanding, as the participating clients can be interpreted as a discretization of the
overall client population distribution."
EFFECT OF CLIENT DIVERSITY,0.3176229508196721,"5.2
EFFECT OF CLIENT DIVERSITY
In this subsection, we study the effect of client diversity on generalization performance. Recall that
in the previous subsection, we vary the number of participating clients while keeping the amount
of training data per client unchanged. As a result, the total amount of training data will grow
proportionally with the number of participating clients."
EFFECT OF CLIENT DIVERSITY,0.319672131147541,"To disentangle the effect of diversity and the growth of training data size, in the following experiment,
we instead ﬁx the total amount of the training data, while varying the concentration across clients. The"
EFFECT OF CLIENT DIVERSITY,0.32172131147540983,"4In addition, we experimented with FEDYOGI on these tasks. The performance is comparable (in terms of
both participating validation and unparticipating metrics). We also experimented with vanilla FEDAVG and
FEDADAGRAD, which are less effective than the other adaptive optimizers in these settings, but the participation
gaps are generally consistent."
EFFECT OF CLIENT DIVERSITY,0.3237704918032787,Published as a conference paper at ICLR 2022
EFFECT OF CLIENT DIVERSITY,0.32581967213114754,"102
103
participating clients 0.975 0.980 0.985 0.990 0.995 1.000"
EFFECT OF CLIENT DIVERSITY,0.32786885245901637,accuracy
EFFECT OF CLIENT DIVERSITY,0.32991803278688525,EMNIST-10
EFFECT OF CLIENT DIVERSITY,0.3319672131147541,"102
103
participating clients 0.80 0.85 0.90 0.95 1.00"
EFFECT OF CLIENT DIVERSITY,0.33401639344262296,accuracy
EFFECT OF CLIENT DIVERSITY,0.3360655737704918,EMNIST-62
PARTICIPATING CLIENTS,0.33811475409836067,"102
participating clients 0.500 0.525 0.550 0.575 0.600"
PARTICIPATING CLIENTS,0.3401639344262295,accuracy
PARTICIPATING CLIENTS,0.3422131147540984,Shakespeare
PARTICIPATING CLIENTS,0.3442622950819672,"103
104"
PARTICIPATING CLIENTS,0.3463114754098361,participating clients 0.24 0.26 0.28
PARTICIPATING CLIENTS,0.3483606557377049,accuracy
PARTICIPATING CLIENTS,0.35040983606557374,Stackoverflow
PARTICIPATING CLIENTS,0.3524590163934426,"participating training
participating validation
unparticipating"
PARTICIPATING CLIENTS,0.35450819672131145,Figure 7: Effect of the number of participating clients. See Section 5.1 for discussion.
PARTICIPATING CLIENTS,0.35655737704918034,"5% part.
128/client"
PARTICIPATING CLIENTS,0.35860655737704916,10% part.
PARTICIPATING CLIENTS,0.36065573770491804,64/client
PARTICIPATING CLIENTS,0.36270491803278687,20% part.
PARTICIPATING CLIENTS,0.36475409836065575,32/client
PARTICIPATING CLIENTS,0.3668032786885246,40% part.
PARTICIPATING CLIENTS,0.36885245901639346,16/client
PARTICIPATING CLIENTS,0.3709016393442623,80% part.
PARTICIPATING CLIENTS,0.3729508196721312,8/client 0.960 0.965 0.970 0.975 0.980 0.985 0.990 0.995 1.000
PARTICIPATING CLIENTS,0.375,"participating training
participating validation
unparticipating"
PARTICIPATING CLIENTS,0.3770491803278688,"Figure 8: Effect of diversity on generalization. We ﬁx the total
amount of training data while varying the concentration across
clients. The concentration varies from taking only 5% clients as
participating clients where each client contributes 128 training
data, to the most diverse distribution with 80% clients as partic-
ipating clients but each client only contributes 8 training data.
Observe that while the total amount of training data is identical,
the more diverse settings exhibit better performance in terms of
both participating validation and unparticipating accuracy."
PARTICIPATING CLIENTS,0.3790983606557377,"experiment is conducted on the EMNIST digits dataset. As shown in Figure 8, the training data from
a new participating client can be more valuable than those contributed by the existing participating
clients. The intuition can also be justiﬁed by our model in that the data from a new participating
client is drawn from the overall population distribution
R"
PARTICIPATING CLIENTS,0.38114754098360654,"c∈C pP(c)pDc(ξ)dc, whereas the data from
existing clients are drawn from the distribution aggregated by existing clients
1
|ˆC|
P"
PARTICIPATING CLIENTS,0.3831967213114754,"c∈ˆC pDc(ξ). This
reveals the importance of client diversity in federated training."
OVERVIEW OF ADDITIONAL EXPERIMENTS IN APPENDICES,0.38524590163934425,"5.3
OVERVIEW OF ADDITIONAL EXPERIMENTS IN APPENDICES
We provide more detailed analysis and further experiments in Appendices B and C, including:"
OVERVIEW OF ADDITIONAL EXPERIMENTS IN APPENDICES,0.38729508196721313,"• Training progress of centralized optimizers on six tasks, see Appendix B.1.
• Detailed analysis of metrics distributions across clients, see Appendices B.2 and B.3. We observe
that the unparticipating clients tend to exhibit longer tails on the lower side of accuracy.
• Results on alternative hyperparameter choices, see Appendices C.1.1 and C.2.1.
• Effect of multiple local epochs per communication round, see Appendix C.1.2.
• Effect of the strength of weight decay, see Appendix C.2.2.
• Effect of model depth, see Appendix C.2.3."
COMMUNITY SUGGESTIONS,0.38934426229508196,"6
COMMUNITY SUGGESTIONS"
COMMUNITY SUGGESTIONS,0.39139344262295084,"In this work we have used the three-way-split, dataset partitioning strategies, and distributions of
metrics to systematically study generalization behavior in FL. Our results inform the following
suggestions for the FL community:"
COMMUNITY SUGGESTIONS,0.39344262295081966,"• Researchers can use the three-way split to disentangle out-of-sample and participation gaps in
empirical studies of FL algorithms.
• When proposing new federated algorithms, researchers might prefer using naturally-partitioned or
semantically-partitioned datasets for more realistic simulations of generalization behavior.
• Distributions of metrics across clients (e.g., percentiles, variance) may vary across groups in the
three-way split (see Table 2 and Figure 10). We suggest researchers report the distribution of
metrics across clients, instead of just the average, when reporting metrics for participating and
non-participating clients. We encourage researchers to pay attention to the difference of two
distributions (participating validation and unparticipating) as it may have fairness implications."
COMMUNITY SUGGESTIONS,0.39549180327868855,Published as a conference paper at ICLR 2022
COMMUNITY SUGGESTIONS,0.3975409836065574,ACKNOWLEDGEMENTS
COMMUNITY SUGGESTIONS,0.39959016393442626,"We would like to thank Zachary Charles, Zachary Garrett, Zheng Xu, Keith Rush, Hang Qi, Brendan
McMahan, Josh Dillon, and Sushant Prakash for helpful discussions at various stages of this work."
REPRODUCIBILITY STATEMENT,0.4016393442622951,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.4036885245901639,"We provide complete descriptions of experimental setups, including dataset preparation and prepro-
cessing, model conﬁgurations, and hyperparameter tuning in Appendix C. Appendix D describes
the detailed procedure for semantic partitioning, and Appendix E presents the detailed approach for
estimating the entropy that generates Figure 6."
REPRODUCIBILITY STATEMENT,0.4057377049180328,"We are also releasing an extensible code framework for measuring out-of-sample and participation
gaps and distributions of metrics (e.g., percentiles) for federated algorithms across several tasks.5
We include all tasks reported in this work; the framework is easily extended with additional tasks.
We also include libraries for performing label-based and semantic dataset partitioning (enabling new
benchmark datasets for future works, see Appendix D). This framework enables easy reproduction of
our results and facilitates future work. The framework is implemented using TensorFlow Federated
(Ingerman & Ostrowski, 2019). The code is released under Apache License 2.0. We hope that the
release of this code encourages researchers to take up our suggestions presented in Section 6."
REFERENCES,0.4077868852459016,REFERENCES
REFERENCES,0.4098360655737705,"Alekh Agarwal, John Langford, and Chen-Yu Wei. Federated Residual Learning. arXiv:2003.12880 [cs, stat],
2020."
REFERENCES,0.41188524590163933,"Maruan Al-Shedivat, Jennifer Gillenwater, Eric Xing, and Afshin Rostamizadeh. Federated Learning via
Posterior Averaging: A New Perspective and Practical Algorithms. In International Conference on Learning
Representations, 2021."
REFERENCES,0.4139344262295082,"Alyazeed Albasyoni, Mher Safaryan, Laurent Condat, and Peter Richt´arik. Optimal Gradient Compression for
Distributed and Federated Learning. arXiv:2010.03246 [cs, math], 2020."
REFERENCES,0.41598360655737704,"Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Concrete
problems in ai safety. arXiv preprint arXiv:1606.06565, 2016."
REFERENCES,0.4180327868852459,"Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-based meta-
learning. In Proceedings of the 36th International Conference on Machine Learning, volume 97. PMLR,
2019."
REFERENCES,0.42008196721311475,"Borja Balle, Peter Kairouz, Brendan McMahan, Om Dipakbhai Thakkar, and Abhradeep Thakurta. Privacy
ampliﬁcation via random check-ins. In Advances in Neural Information Processing Systems 33, volume 33,
2020."
REFERENCES,0.42213114754098363,"Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-sgd: Distributed SGD with
quantization, sparsiﬁcation and local computations. In Advances in Neural Information Processing Systems
32. Curran Associates, Inc., 2019."
REFERENCES,0.42418032786885246,"Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al. Analysis of representations for domain
adaptation. Advances in neural information processing systems, 19:137, 2007."
REFERENCES,0.4262295081967213,"Aleksandr Beznosikov, Samuel Horv´ath, Peter Richt´arik, and Mher Safaryan. On Biased Compression for
Distributed Learning. arXiv:2002.12410 [cs, math, stat], 2020."
REFERENCES,0.42827868852459017,"Ilai Bistritz, Ariana Mann, and Nicholas Bambos. Distributed Distillation for On-Device Learning. In Advances
in Neural Information Processing Systems 33, volume 33, 2020."
REFERENCES,0.430327868852459,"Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar. When is Memorization of Irrelevant
Training Data Necessary for High-Accuracy Learning? arXiv:2012.06421 [cs], 2020."
REFERENCES,0.4323770491803279,"Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint
arXiv:1509.00519, 2015."
REFERENCES,0.4344262295081967,5Please visit https://bit.ly/fl-generalization for the code repository.
REFERENCES,0.4364754098360656,Published as a conference paper at ICLR 2022
REFERENCES,0.4385245901639344,"Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Koneˇcn´y, H. Brendan McMahan, Virginia
Smith, and Ameet Talwalkar. LEAF: A Benchmark for Federated Settings. In NeurIPS 2019 Workshop on
Federated Learning for Data Privacy and Conﬁdentiality, 2019."
REFERENCES,0.4405737704918033,"Zachary Charles and Jakub Koneˇcn`y. On the outsized importance of learning rates in local update methods.
arXiv preprint arXiv:2007.00878, 2020."
REFERENCES,0.4426229508196721,"Fei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, and Xiuqiang He.
Federated Meta-Learning with Fast
Convergence and Efﬁcient Communication. arXiv:1802.07876 [cs], 2019."
REFERENCES,0.444672131147541,"Hong-You Chen and Wei-Lun Chao. FedBE: Making Bayesian Model Ensemble Applicable to Federated
Learning. In International Conference on Learning Representations, 2021."
REFERENCES,0.44672131147540983,"Xiangyi Chen, Steven Z. Wu, and Mingyi Hong. Understanding gradient clipping in private SGD: A geometric
perspective. In Advances in Neural Information Processing Systems 33, 2020."
REFERENCES,0.4487704918032787,"Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr´e van Schaik. EMNIST: An extension of MNIST to
handwritten letters. CoRR, abs/1702.05373, 2017."
REFERENCES,0.45081967213114754,"Hal Daum´e III. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815, 2009."
REFERENCES,0.45286885245901637,"Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive Personalized Federated Learning.
arXiv:2003.13461 [cs, stat], 2020."
REFERENCES,0.45491803278688525,"Enmao Diao, Jie Ding, and Vahid Tarokh. Heteroﬂ: Computation and communication efﬁcient federated learning
for heterogeneous clients. arXiv preprint arXiv:2010.01264, 2020."
REFERENCES,0.4569672131147541,"Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel M Roy, and Ali Ramezani-Kebrya. Adap-
tive gradient quantization for data-parallel SGD. In Advances in Neural Information Processing Systems,
volume 33. Curran Associates, Inc., 2020."
REFERENCES,0.45901639344262296,"Alireza Fallah, Aryan Mokhtari, and Asuman E. Ozdaglar. Personalized federated learning with theoretical
guarantees: A model-agnostic meta-learning approach. In Advances in Neural Information Processing
Systems 33, 2020."
REFERENCES,0.4610655737704918,"Jonas Geiping, Hartmut Bauermeister, Hannah Dr¨oge, and Michael Moeller. Inverting Gradients - How easy is it
to break privacy in federated learning? In Advances in Neural Information Processing Systems 33, 2020."
REFERENCES,0.46311475409836067,"Margalit Glasgow, Honglin Yuan, and Tengyu Ma. Sharp bounds for federated averaging (local sgd) and
continuous perspective. arXiv preprint arXiv:2111.03741, 2021."
REFERENCES,0.4651639344262295,"Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richt´arik. Linearly converging error compen-
sated SGD. In Advances in Neural Information Processing Systems 33, volume 33, 2020."
REFERENCES,0.4672131147540984,"Patrick J. Grother and Patricia A. Flanagan. NIST Handprinted Forms and Characters, NIST Special Database
19., 1995."
REFERENCES,0.4692622950819672,"Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Trading redundancy
for communication: Speeding up distributed SGD for non-convex optimization. In Proceedings of the 36th
International Conference on Machine Learning, volume 97. PMLR, 2019a."
REFERENCES,0.4713114754098361,"Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Local SGD with
periodic averaging: Tighter analysis and adaptive synchronization. In Advances in Neural Information
Processing Systems 32. Curran Associates, Inc., 2019b."
REFERENCES,0.4733606557377049,"Filip Hanzely and Peter Richt´arik.
Federated Learning of a Mixture of Global and Local Models.
arXiv:2002.05516 [cs, math, stat], 2020."
REFERENCES,0.47540983606557374,"Filip Hanzely, Slavom´ır Hanzely, Samuel Horv´ath, and Peter Richt´arik. Lower bounds and optimal algorithms
for personalized federated learning. In Advances in Neural Information Processing Systems 33, 2020."
REFERENCES,0.4774590163934426,"Weituo Hao, Nikhil Mehta, Kevin J. Liang, Pengyu Cheng, Mostafa El-Khamy, and Lawrence Carin. WAFFLe:
Weight Anonymized Factorization for Federated Learning. arXiv:2008.05687 [cs, stat], 2020."
REFERENCES,0.47950819672131145,"Chaoyang He, Murali Annavaram, and Salman Avestimehr. Group Knowledge Transfer: Federated Learning of
Large CNNs at the Edge. In Advances in Neural Information Processing Systems 33, volume 33, 2020."
REFERENCES,0.48155737704918034,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016."
REFERENCES,0.48360655737704916,Published as a conference paper at ICLR 2022
REFERENCES,0.48565573770491804,"Samuel Horv´ath and Peter Richt´arik. A Better Alternative to Error Feedback for Communication-Efﬁcient
Distributed Learning. arXiv:2006.11077 [cs, stat], 2020."
REFERENCES,0.48770491803278687,"Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip B. Gibbons. The Non-IID Data Quagmire of
Decentralized Machine Learning. arXiv:1910.00189 [cs, stat], 2019."
REFERENCES,0.48975409836065575,"Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the Effects of Non-Identical Data Distribution
for Federated Visual Classiﬁcation. arXiv:1909.06335 [cs, stat], 2019."
REFERENCES,0.4918032786885246,"Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. FL-NTK: A neural tangent kernel-based framework
for federated learning analysis. In Proceedings of the 38th International Conference on Machine Learning,
volume 139. PMLR, 2021."
REFERENCES,0.49385245901639346,"Alex Ingerman and Krzys Ostrowski. Introducing TensorFlow Federated, 2019."
REFERENCES,0.4959016393442623,"Rustem Islamov, Xun Qian, and Peter Richt´arik. Distributed Second Order Methods with Fast Rates and
Compressed Communication. In ICML 2021, 2021."
REFERENCES,0.4979508196721312,"Yihan Jiang, Jakub Koneˇcn´y, Keith Rush, and Sreeram Kannan. Improving Federated Learning Personalization
via Model Agnostic Meta Learning. arXiv:1909.12488 [cs, stat], 2019."
REFERENCES,0.5,"Yuang Jiang, Shiqiang Wang, Victor Valls, Bong Jun Ko, Wei-Han Lee, Kin K. Leung, and Leandros Tassiulas.
Model Pruning Enables Efﬁcient Federated Learning on Edge Devices. arXiv:1909.12326 [cs, stat], 2020."
REFERENCES,0.5020491803278688,"Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Salim El
Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri`a Gasc´on, Badih Ghazi, Phillip B. Gibbons,
Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin
Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Koneˇcn´y, Aleksandra Korolova, Farinaz Koushanfar,
Sanmi Koyejo, Tancr`ede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer ¨Ozg¨ur,
Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song,
Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram`er, Praneeth Vepakomma, Jianyu
Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and Open Problems
in Federated Learning. arXiv:1912.04977 [cs, stat], 2019."
REFERENCES,0.5040983606557377,"Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic Controlled Averaging for Federated Learning. In Proceed-
ings of the International Conference on Machine Learning 1 Pre-Proceedings (ICML 2020), 2020."
REFERENCES,0.5061475409836066,"Ahmed Khaled, Konstantin Mishchenko, and Peter Richt´arik. Tighter Theory for Local SGD on Identical and
Heterogeneous Data. In Proceedings of the Twenty Third International Conference on Artiﬁcial Intelligence
and Statistics, volume 108. PMLR, 2020."
REFERENCES,0.5081967213114754,"Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive Gradient-Based Meta-Learning
Methods. In Advances in Neural Information Processing Systems 32, volume 32. Curran Associates, Inc.,
2019."
REFERENCES,0.5102459016393442,"Diederik P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. arXiv
preprint arXiv:1807.03039, 2018."
REFERENCES,0.5122950819672131,"Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013."
REFERENCES,0.514344262295082,"Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian U. Stich. A Uniﬁed Theory
of Decentralized SGD with Changing Topology and Local Updates. In Proceedings of the International
Conference on Machine Learning 1 Pre-Proceedings (ICML 2020), 2020."
REFERENCES,0.5163934426229508,"Jakub Koneˇcn´y, H. Brendan McMahan, Daniel Ramage, and Peter Richt´arik. Federated Optimization: Distributed
Machine Learning for On-Device Intelligence. arXiv:1610.02527 [cs], 2016."
REFERENCES,0.5184426229508197,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.5204918032786885,"Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty
estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016."
REFERENCES,0.5225409836065574,"Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated Learning: Challenges, Methods,
and Future Directions. IEEE Signal Processing Magazine, 37(3), 2020a."
REFERENCES,0.5245901639344263,"Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. In Proceedings of Machine Learning and Systems 2020, 2020b."
REFERENCES,0.5266393442622951,Published as a conference paper at ICLR 2022
REFERENCES,0.5286885245901639,"Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning. In
International Conference on Learning Representations, 2020c."
REFERENCES,0.5307377049180327,"Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of FedAvg on
non-iid data. In International Conference on Learning Representations, 2020d."
REFERENCES,0.5327868852459017,"Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated learning on Non-IID
features via local batch normalization. In International Conference on Learning Representations, 2021."
REFERENCES,0.5348360655737705,"Paul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas B. Allen, Randy P. Auerbach, David Brent, Ruslan Salakhutdi-
nov, and Louis-Philippe Morency. Think Locally, Act Globally: Federated Learning with Local and Global
Representations. arXiv:2001.01523 [cs, stat], 2020."
REFERENCES,0.5368852459016393,"Tao Lin, Lingjing Kong, Sebastian U. Stich, and Martin Jaggi. Ensemble Distillation for Robust Model Fusion
in Federated Learning. In Advances in Neural Information Processing Systems 33, 2020."
REFERENCES,0.5389344262295082,"Ben London. PAC Identiﬁability in Federated Personalization. In NeurIPS 2020 Workshop on Scalability,
Privacy and Security in Federated Learning (SpicyFL), 2020."
REFERENCES,0.5409836065573771,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-
efﬁcient learning of deep networks from decentralized data. In Proceedings of the 20th International
Conference on Artiﬁcial Intelligence and Statistics, volume 54. PMLR, 2017."
REFERENCES,0.5430327868852459,"Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Proceedings of the
36th International Conference on Machine Learning, volume 97. PMLR, 2019."
REFERENCES,0.5450819672131147,"Alex Nichol, Joshua Achiam, and John Schulman. On First-Order Meta-Learning Algorithms. arXiv:1803.02999
[cs], 2018."
REFERENCES,0.5471311475409836,"Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating predictive
uncertainty under dataset shift. arXiv preprint arXiv:1906.02530, 2019."
REFERENCES,0.5491803278688525,"Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of
recent advances. IEEE signal processing magazine, 32(3):53–69, 2015."
REFERENCES,0.5512295081967213,"Reese Pathak and Martin J. Wainwright. FedSplit: An algorithmic framework for fast federated optimization. In
Advances in Neural Information Processing Systems 33, 2020."
REFERENCES,0.5532786885245902,"Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn´y, Sanjiv Kumar,
and H. Brendan McMahan. Adaptive Federated Optimization. In International Conference on Learning
Representations, 2021."
REFERENCES,0.555327868852459,"Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated learning: The
case of afﬁne distribution shifts. arXiv preprint arXiv:2006.08907, 2020."
REFERENCES,0.5573770491803278,"Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with
discretized logistic mixture likelihood and other modiﬁcations. arXiv preprint arXiv:1701.05517, 2017."
REFERENCES,0.5594262295081968,"Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood
function. Journal of statistical planning and inference, 90(2):227–244, 2000."
REFERENCES,0.5614754098360656,"Karan Singhal, Hakim Sidahmed, Zachary Garrett, Shanshan Wu, Keith Rush, and Sushant Prakash. Federated
Reconstruction: Partially Local Federated Learning. Advances in Neural Information Processing Systems,
2021."
REFERENCES,0.5635245901639344,"Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task learning. In
Advances in Neural Information Processing Systems 30. Curran Associates, Inc., 2017."
REFERENCES,0.5655737704918032,"Jinhyun So, Basak Guler, and Salman Avestimehr. A Scalable Approach for Privacy-Preserving Collaborative
Machine Learning. In Advances in Neural Information Processing Systems 33, 2020."
REFERENCES,0.5676229508196722,"Jy-yong Sohn, Dong-Jun Han, Beongjun Choi, and Jaekyun Moon. Election coding for distributed learning:
Protecting SignSGD against byzantine attacks. In Advances in Neural Information Processing Systems 33,
2020."
REFERENCES,0.569672131147541,"Sebastian U. Stich. Local SGD converges fast and communicates little. In International Conference on Learning
Representations, 2019."
REFERENCES,0.5717213114754098,Published as a conference paper at ICLR 2022
REFERENCES,0.5737704918032787,"Canh T. Dinh, Nguyen Tran, and Tuan Dung Nguyen. Personalized Federated Learning with Moreau Envelopes.
In Advances in Neural Information Processing Systems 33, 2020."
REFERENCES,0.5758196721311475,"Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning
research, 9(11), 2008."
REFERENCES,0.5778688524590164,"Vladimir Naumovich Vapnik. Statistical Learning Theory. Wiley, 1998."
REFERENCES,0.5799180327868853,"Jianyu Wang and Gauri Joshi.
Cooperative SGD: A uniﬁed Framework for the Design and Analysis of
Communication-Efﬁcient SGD Algorithms. arXiv:1808.07576 [cs, stat], 2018."
REFERENCES,0.5819672131147541,"Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat. SlowMo: Improving communication-efﬁcient
distributed SGD with slow momentum. In International Conference on Learning Representations, 2020a."
REFERENCES,0.5840163934426229,"Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen
Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated optimization.
arXiv preprint arXiv:2107.06917, 2021."
REFERENCES,0.5860655737704918,"Kangkang Wang, Rajiv Mathews, Chlo´e Kiddon, Hubert Eichner, Franc¸oise Beaufays, and Daniel Ramage.
Federated Evaluation of On-device Personalization. arXiv:1910.10252 [cs, stat], 2019."
REFERENCES,0.5881147540983607,"Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, and Dawn Song. A Principled Approach to Data
Valuation for Federated Learning. In Federated Learning, volume 12500. Springer International Publishing,
2020b."
REFERENCES,0.5901639344262295,"Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs Local SGD for Heterogeneous
Distributed Learning. In Advances in Neural Information Processing Systems 33, 2020."
REFERENCES,0.5922131147540983,"Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer
Vision (ECCV), 2018."
REFERENCES,0.5942622950819673,"Semih Yagli, Alex Dytso, and H. Vincent Poor. Information-Theoretic Bounds on the Generalization Error and
Privacy Leakage in Federated Learning. In 2020 IEEE 21st International Workshop on Signal Processing
Advances in Wireless Communications (SPAWC). IEEE, 2020."
REFERENCES,0.5963114754098361,"Hao Yu and Rong Jin. On the computation and communication complexity of parallel SGD with dynamic
batch sizes for stochastic non-convex optimization. In Proceedings of the 36th International Conference on
Machine Learning, volume 97. PMLR, 2019."
REFERENCES,0.5983606557377049,"Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efﬁcient momentum SGD
for distributed non-convex optimization. In Proceedings of the 36th International Conference on Machine
Learning, volume 97. PMLR, 2019."
REFERENCES,0.6004098360655737,"Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Salvaging Federated Learning by Local Adaptation.
arXiv:2002.04758 [cs, stat], 2020."
REFERENCES,0.6024590163934426,"Honglin Yuan and Tengyu Ma. Federated Accelerated Stochastic Gradient Descent. In Advances in Neural
Information Processing Systems 33, 2020."
REFERENCES,0.6045081967213115,"Honglin Yuan, Manzil Zaheer, and Sashank Reddi. Federated Composite Optimization. In Proceedings of the
38th International Conference on Machine Learning, 2021."
REFERENCES,0.6065573770491803,"Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. FedPD: A Federated Learning
Framework with Optimal Rates and Adaptivity to Non-IID Data. arXiv:2005.11418 [cs, stat], 2020."
REFERENCES,0.6086065573770492,"Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated Learning with
Non-IID Data. arXiv:1806.00582 [cs, stat], 2018."
REFERENCES,0.610655737704918,"Fan Zhou and Guojing Cong. On the convergence properties of a k-step averaging stochastic gradient descent
algorithm for nonconvex optimization. In Proceedings of the Twenty-Seventh International Joint Conference
on Artiﬁcial Intelligence, 2018."
REFERENCES,0.6127049180327869,"Wennan Zhu, Peter Kairouz, Brendan McMahan, Haicheng Sun, and Wei Li. Federated Heavy Hitters Dis-
covery with Differential Privacy. In Proceedings of the Twenty Third International Conference on Artiﬁcial
Intelligence and Statistics, volume 108. PMLR, 2020."
REFERENCES,0.6147540983606558,Published as a conference paper at ICLR 2022
REFERENCES,0.6168032786885246,Appendices
REFERENCES,0.6188524590163934,LIST OF APPENDICES
REFERENCES,0.6209016393442623,"A Additional Related Work
16"
REFERENCES,0.6229508196721312,"B
Additional Experimental Results
16"
REFERENCES,0.625,"B.1
Training progress of centralized optimizers
. . . . . . . . . . . . . . . . . . . . .
16"
REFERENCES,0.6270491803278688,"B.2
Percentiles of metrics across clients
. . . . . . . . . . . . . . . . . . . . . . . . .
16"
REFERENCES,0.6290983606557377,"B.3
Federated training progress at the 25th percentile acorss clients . . . . . . . . . . .
17"
REFERENCES,0.6311475409836066,"C Additional Details on Experimental Setup and Task-Speciﬁc Experiments
18"
REFERENCES,0.6331967213114754,"C.1
EMNIST Hand-written Character Recognition Task . . . . . . . . . . . . . . . . .
18"
REFERENCES,0.6352459016393442,"C.1.1
Consistency across various hyperparameters choices
. . . . . . . . . . . .
18"
REFERENCES,0.6372950819672131,"C.1.2
Effect of multiple local epochs per communication round . . . . . . . . . .
19"
REFERENCES,0.639344262295082,"C.2
CIFAR 10/100 Image Classiﬁcation Task
. . . . . . . . . . . . . . . . . . . . . .
19"
REFERENCES,0.6413934426229508,"C.2.1
Consistency across various hyperparameters choice . . . . . . . . . . . . .
19"
REFERENCES,0.6434426229508197,"C.2.2
Effect of Weight Decay Strength . . . . . . . . . . . . . . . . . . . . . . .
19"
REFERENCES,0.6454918032786885,"C.2.3
Effect of Model Depth . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21"
REFERENCES,0.6475409836065574,"C.3
Shakespeare Next Character Prediction Task . . . . . . . . . . . . . . . . . . . . .
21"
REFERENCES,0.6495901639344263,"C.4
Stackoverﬂow Next Word Prediction Task . . . . . . . . . . . . . . . . . . . . . .
21"
REFERENCES,0.6516393442622951,"D Details of Semanticically Partitioned Federated Dataset
22"
REFERENCES,0.6536885245901639,"D.1
Details of the Semantic Partitioning Scheme . . . . . . . . . . . . . . . . . . . . .
22"
REFERENCES,0.6557377049180327,"D.2
Visualization of Semanticically Partitioned CIFAR-100 Dataset . . . . . . . . . . .
22"
REFERENCES,0.6577868852459017,"D.3
Visualization of Semantically Partitioned MNIST Dataset . . . . . . . . . . . . . .
22"
REFERENCES,0.6598360655737705,"E
Methodology for Computing Entropy
24"
REFERENCES,0.6618852459016393,Published as a conference paper at ICLR 2022
REFERENCES,0.6639344262295082,"A
ADDITIONAL RELATED WORK"
REFERENCES,0.6659836065573771,"Recent years have observed a booming interest in various aspects of Federated Learning, including
communication-efﬁcient learning (McMahan et al., 2017; Koneˇcn´y et al., 2016; Zhou & Cong, 2018;
Haddadpour et al., 2019a; Wang & Joshi, 2018; Yu & Jin, 2019; Yu et al., 2019; Basu et al., 2019;
Stich, 2019; Khaled et al., 2020; Yuan & Ma, 2020; Woodworth et al., 2020; Yuan et al., 2021;
Li et al., 2021; Huang et al., 2021; Glasgow et al., 2021), model ensembling (Bistritz et al., 2020;
He et al., 2020; Lin et al., 2020; Chen & Chao, 2021), integration with compression (Faghri et al.,
2020; Gorbunov et al., 2020; Sohn et al., 2020; Beznosikov et al., 2020; Horv´ath & Richt´arik, 2020;
Albasyoni et al., 2020; Jiang et al., 2020; Islamov et al., 2021), systems heterogeneity (Smith et al.,
2017; Diao et al., 2020), data (distributional) heterogeneity (Haddadpour et al., 2019b; Khaled et al.,
2020; Li et al., 2020d; Koloskova et al., 2020; Woodworth et al., 2020; Mohri et al., 2019; Zhang
et al., 2020; Li et al., 2020b; Wang et al., 2020a; Karimireddy et al., 2020; Pathak & Wainwright,
2020; Al-Shedivat et al., 2021), fairness (Wang et al., 2020b; Li et al., 2020c; Mohri et al., 2019),
personalization (Smith et al., 2017; Nichol et al., 2018; Khodak et al., 2019; Balcan et al., 2019;
Jiang et al., 2019; Wang et al., 2019; Chen et al., 2019; Fallah et al., 2020; Hanzely et al., 2020;
London, 2020; T. Dinh et al., 2020; Yu et al., 2020; Hanzely & Richt´arik, 2020; Agarwal et al.,
2020; Deng et al., 2020; Hao et al., 2020; Liang et al., 2020), and privacy (Balle et al., 2020; Chen
et al., 2020; Geiping et al., 2020; London, 2020; So et al., 2020; Zhu et al., 2020; Brown et al.,
2020). These works often study generalization and convergence for newly proposed algorithms.
Huang et al. (2021) studied the generalization of Federated Learning in Neural-tangent kernel regime.
But, to our knowledge, there is no existing work that disentangles out-of-sample and participation
gaps in federated training. We refer readers to (Kairouz et al., 2019; Wang et al., 2021) for a more
comprehensive survey on the recent progress in Federated Learning."
REFERENCES,0.6680327868852459,"B
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.6700819672131147,"In this section, we present several experimental results omitted from the main body due to space
constraints. Additional task-speciﬁc ablation experiments can be found in Appendix C."
REFERENCES,0.6721311475409836,"B.1
TRAINING PROGRESS OF CENTRALIZED OPTIMIZERS"
REFERENCES,0.6741803278688525,"In this subsection, we repeat the experiment in Figure 1 with centralized training. The results are
shown in Figure 9. Observe that participation gap still exists with centralized optimizers. This is
because the participation gap is an intrinsic outcome of the heterogeneity of federated dataset."
REFERENCES,0.6762295081967213,"0
100
200
epoch 0.97 0.98 0.99 1.00"
REFERENCES,0.6782786885245902,accuracy
REFERENCES,0.680327868852459,EMNIST-10
REFERENCES,0.6823770491803278,"0
100
200
epoch 0.80 0.85 0.90"
REFERENCES,0.6844262295081968,"0.95
EMNIST-62"
REFERENCES,0.6864754098360656,"0
100
200
epoch 0.7 0.8 0.9 1.0"
REFERENCES,0.6885245901639344,CIFAR-10
REFERENCES,0.6905737704918032,"0
100
200
epoch 0.4 0.6 0.8 1.0"
REFERENCES,0.6926229508196722,CIFAR-100
REFERENCES,0.694672131147541,"0
10
20
epoch 0.50 0.52 0.54 0.56 0.58 0.60"
SHAKESPEARE,0.6967213114754098,"0.62
Shakespeare"
SHAKESPEARE,0.6987704918032787,"0
10
20
epoch 0.22 0.24 0.26"
STACKOVERFLOW,0.7008196721311475,"0.28
Stackoverflow"
STACKOVERFLOW,0.7028688524590164,"participating training
participating validation
unparticipating"
STACKOVERFLOW,0.7049180327868853,"Figure 9: Centralized training progress on six different federated tasks. Observe that the partici-
pation gap still exists even with centralized optimizers. We refer readers to Table 1 for a quantitative
comparison between federated optimizers and centralized optimizers."
STACKOVERFLOW,0.7069672131147541,"B.2
PERCENTILES OF METRICS ACROSS CLIENTS"
STACKOVERFLOW,0.7090163934426229,"In this subsection, we report the detailed statistics of metrics across clients. Recall that in Table 1, we
aggregated the metrics across clients by weighted averaging, where the weights are determined by the
number of elements contributed by each client. In the following Table 2, we report ﬁve percentiles
of metrics across clients: 95th, 75th, 50th (a.k.a. median), 25th, and 5th. These statistics provide a
detailed characterization on the metrics distribution across clients.6"
STACKOVERFLOW,0.7110655737704918,"6To make the percentiles comparable, we ensure the un-participating clients and participating validation
clients have the same scale of elements per client."
STACKOVERFLOW,0.7131147540983607,Published as a conference paper at ICLR 2022
STACKOVERFLOW,0.7151639344262295,"Table 2: Percentiles of metrics across clients on six federated tasks. We observe that the unpartici-
pating clients tend to exhibit longer tails on the lower side of accuracy. For example, the participating
clients of EMNIST-10 have perfect (100%) accuracy even for clients at the 5th percentile, whereas
the unparticipating clients only achieve 91.7%."
STACKOVERFLOW,0.7172131147540983,"percentile
Federated Training
Centralized Training"
STACKOVERFLOW,0.7192622950819673,"part val
unpart
part val
unpart"
STACKOVERFLOW,0.7213114754098361,EMNIST-10
TH,0.7233606557377049,"95th
100.0
100.0
100.0
100.0
75th
100.0
100.0
100.0
100.0
50th
100.0
100.0
100.0
100.0
25th
100.0
100.0
100.0
100.0
5th
100.0
91.7
100.0
91.7"
TH,0.7254098360655737,EMNIST-62
TH,0.7274590163934426,"95th
100.0
100.0
100.0
100.0
75th
93.3
92.3
93.5
93.5
50th
88.2
87.1
87.5
87.5
25th
82.1
78.6
81.8
79.2
5th
66.7
64.7
71.4
70.6"
TH,0.7295081967213115,CIFAR-10
TH,0.7315573770491803,"95th
93.2
90.0
95.5
92.9
75th
88.2
86.0
90.9
89.1
50th
83.0
81.3
87.0
85.7
25th
79.2
76.7
82.1
81.5
5th
71.1
69.6
77.1
73.7"
TH,0.7336065573770492,CIFAR-100
TH,0.735655737704918,"95th
65.4
62.4
66.7
62.6
75th
61.1
56.7
64.2
56.6
50th
57.3
53.8
61.3
55.7
25th
53.9
51.9
55.5
52.8
5th
46.9
46.4
50.4
50.4"
TH,0.7377049180327869,Shakespeare
TH,0.7397540983606558,"95th
68.4
71.4
70.1
71.4
75th
60.8
60.0
61.7
61.1
50th
57.3
56.8
58.2
57.5
25th
54.0
53.1
54.5
53.7
5th
38.4
38.2
42.6
40.9"
TH,0.7418032786885246,StackOverﬂow
TH,0.7438524590163934,"95th
31.0
31.3
31.8
31.4
75th
27.7
27.7
28.8
27.9
50th
25.6
25.4
26.3
25.9
25th
23.5
23.2
24.2
23.7
5th
20.1
20.0
20.8
20.7"
TH,0.7459016393442623,"B.3
FEDERATED TRAINING PROGRESS AT THE 25TH PERCENTILE ACORSS CLIENTS"
TH,0.7479508196721312,"To further inspect the distribution of metrics across clients, we plot the 25th percentile of accuracy
across clients versus communication rounds (training progress). The results are shown in Figure 10."
TH,0.75,"0
1000
2000
3000
round 0.97 0.98 0.99 1.00"
TH PERCENTILE,0.7520491803278688,25th percentile
TH PERCENTILE,0.7540983606557377,EMNIST-10
TH PERCENTILE,0.7561475409836066,"0
1000
2000
3000
round 0.775 0.800 0.825 0.850 0.875"
TH PERCENTILE,0.7581967213114754,"0.900
EMNIST-62"
TH PERCENTILE,0.7602459016393442,"500
1000
1500
round 0.70 0.72 0.74 0.76 0.78"
TH PERCENTILE,0.7622950819672131,"0.80
CIFAR-10"
TH PERCENTILE,0.764344262295082,"500
1000
1500
round 0.40 0.45 0.50 0.55"
TH PERCENTILE,0.7663934426229508,"0.60
CIFAR-100"
TH PERCENTILE,0.7684426229508197,"0
1000
2000
3000
round 0.50 0.52 0.54 0.56"
SHAKESPEARE,0.7704918032786885,"0.58
Shakespeare"
SHAKESPEARE,0.7725409836065574,"0
2000
4000
6000
round 0.20 0.21 0.22 0.23 0.24"
STACKOVERFLOW,0.7745901639344263,"0.25
Stackoverflow"
STACKOVERFLOW,0.7766393442622951,"participating training
participating validation
unparticipating"
STACKOVERFLOW,0.7786885245901639,Figure 10: Accuracies of the client at the 25th percentile versus the communication rounds.
STACKOVERFLOW,0.7807377049180327,Published as a conference paper at ICLR 2022
STACKOVERFLOW,0.7827868852459017,"C
ADDITIONAL DETAILS ON EXPERIMENTAL SETUP AND TASK-SPECIFIC
EXPERIMENTS"
STACKOVERFLOW,0.7848360655737705,"In this section we provide details of the experimental setup, including dataset preparation/prepro-
cessing, model choice and hyperparameter tuning. We also include task-speciﬁc experiments with
ablations."
STACKOVERFLOW,0.7868852459016393,"For every setting, unless otherwise stated, we tune the learning rate(s) to achieve the best sum of
participating validation accuracy and unparticipating accuracy (so that the result will not be biased
towards one of the accuracies)."
STACKOVERFLOW,0.7889344262295082,"C.1
EMNIST HAND-WRITTEN CHARACTER RECOGNITION TASK"
STACKOVERFLOW,0.7909836065573771,"Federated Dataset Description and Proprocessing. The EMNIST dataset (Cohen et al., 2017) is a
hand-written character recognition dataset derived from the NIST Special Database 19 (Grother &
Flanagan, 1995). We used the Federated version of EMNIST (Caldas et al., 2019) dataset, which is
partitioned based on the writer identiﬁcation. We consider both the full version (62 classes) as well
as the numbers-only version (10 classes). We adopt the federated EMNIST hosted by Tensorﬂow
Federated (TFF). In TFF, federated EMNIST has a default intra-client split, namely all the clients
appeared in both the “training” and “validation” dataset. To construct a three-way split, we hold
out 20% of the total clients as unparticipating clients. Within each participating client, we keep
the original training/validation split, i.e., the original training data that are assigned to participating
clients will become participating training data. We tested the performance under various number of
participating clients, as shown in Figure 7. The results reported in Table 1 are for the case with 272
participating clients."
STACKOVERFLOW,0.7930327868852459,"Model, Optimizer, and Hyperparameters. We train a shallow convolutional neural network with
approximately one million trainable parameters as in (Reddi et al., 2021). For centralized training,
we run 200 epochs of SGD with momentum = 0.9 with constant learning rate with batch size 50.
The (centralized) learning rate is tuned from {10−2.5, 10−2, . . . , 10−0.5}. For federated training,
we run 3000 rounds of FEDAVGM (Reddi et al., 2021) with server momentum = 0.9 and constant
server and client learning rates. For each communication round, we uniformly sample 20 clients to
train for 1 epoch with client batch size 20. The client and server learning rates are both tuned from
{10−2, 10−1.5, . . . , 1}."
STACKOVERFLOW,0.7950819672131147,"C.1.1
CONSISTENCY ACROSS VARIOUS HYPERPARAMETERS CHOICES"
STACKOVERFLOW,0.7971311475409836,"In Table 1, we only presented the best hyperparameter choice (learning rate combinations). In
this subsubsection, we show that the pattern of generalization gap is consistent across various
hyperparameter choices. The result is shown in Figure 11."
STACKOVERFLOW,0.7991803278688525,"0
1000
2000
3000
round 0.97 0.98 0.99 1.00"
STACKOVERFLOW,0.8012295081967213,accuracy
STACKOVERFLOW,0.8032786885245902,"s=100.0 | 
c=10
1.0"
STACKOVERFLOW,0.805327868852459,"0
1000
2000
3000
round"
STACKOVERFLOW,0.8073770491803278,"s=10
1.0 | 
c=10
0.5"
STACKOVERFLOW,0.8094262295081968,"0
1000
2000
3000
round"
STACKOVERFLOW,0.8114754098360656,"s=100.0 | 
c=10
1.5"
STACKOVERFLOW,0.8135245901639344,"0
1000
2000
3000
round"
STACKOVERFLOW,0.8155737704918032,"s=10
0.5 | 
c=10
0.5"
STACKOVERFLOW,0.8176229508196722,"participating training
participating validation
unparticipating"
STACKOVERFLOW,0.819672131147541,"Figure 11: Consistency of participation gaps across hyperparameter choice (learning rates
conﬁguration). We present the best four (4) combination of learning rates for federated training
of EMNIST-10. Here ηc stands for client learning rate, and ηs stands for server learning rate. We
observe that the participation gap is consistent across various conﬁgurations of learning rates."
STACKOVERFLOW,0.8217213114754098,Published as a conference paper at ICLR 2022
STACKOVERFLOW,0.8237704918032787,"C.1.2
EFFECT OF MULTIPLE LOCAL EPOCHS PER COMMUNICATION ROUND"
STACKOVERFLOW,0.8258196721311475,"In the main experiments we by default let each sampled client run one local epoch every communica-
tion round. In this subsubsection, we evaluate the effect of multiple local epochs on the generalization
performance. The result is shown in Figure 12."
STACKOVERFLOW,0.8278688524590164,"0
1000
2000
3000
round 0.800 0.825 0.850 0.875 0.900 0.925"
STACKOVERFLOW,0.8299180327868853,accuracy
STACKOVERFLOW,0.8319672131147541,epochs per round = 1
STACKOVERFLOW,0.8340163934426229,"0
1000
2000
3000
round"
STACKOVERFLOW,0.8360655737704918,epochs per round = 2
STACKOVERFLOW,0.8381147540983607,"0
1000
2000
3000
round"
STACKOVERFLOW,0.8401639344262295,epochs per round = 5
STACKOVERFLOW,0.8422131147540983,"0
1000
2000
3000
round"
STACKOVERFLOW,0.8442622950819673,epochs per round = 10
STACKOVERFLOW,0.8463114754098361,"participating training
participating validation
unparticipating"
STACKOVERFLOW,0.8483606557377049,"Figure 12: Effect of multiple client epochs per round on EMNIST-62. We repeat the experiment
on EMNIST-62 but instead let each sampled client run multiple local epochs per communication
round. The other settings (including the total communication rounds) remain the same. We observe
that the participation gap is consistent across various settings of local epochs."
STACKOVERFLOW,0.8504098360655737,"C.2
CIFAR 10/100 IMAGE CLASSIFICATION TASK"
STACKOVERFLOW,0.8524590163934426,"Federated Dataset Preprocessing. The CIFAR-10 and CIFAR-100 datasets (Krizhevsky et al.,
2009) are datasets of natural images distributed into 10 and 100 classes respectively. Since the dataset
does not come with user assignment, we ﬁrst shufﬂe the original dataset and assign to clients by
applying our proposed semantic synthesized partitioning. The CIFAR-10 and CIFAR-100 dataset
are partitioned into 300 and 100 clients, respectively. For three-way split, we hold out 20% (60 for
CIFAR-10, 20 for CIFAR-100) clients as unparticipating clients, and leave the remaining client as
participating clients. Within each participating client, we hold out 20% of data as (participating)
validation data."
STACKOVERFLOW,0.8545081967213115,"Model, Optimizer, and Hyperparameters We train a ResNet-18 (He et al., 2016) in which the
batch normalization is replaced by group normalization (Wu & He, 2018) for improved stability in
federated setting, as recommended by Hsieh et al. (2019). For centralized training, we run 200 epochs
of SGD with momentum = 0.9 with batch size 50, and decay the learning rate by 5x every 60 epochs.
The initial learning rate is tuned from {10−2.5, 10−2, . . . , 10−0.5}. For federated training, we run
2,000 rounds of FEDAVGM (Reddi et al., 2021) with server momentum = 0.9, and decay the server
learning rate by 5x every 600 communication rounds. For each communication round, we uniformly
sample 10 clients (for CIFAR-100) or 30 clients (for CIFAR-10), and let each client train for 1 local
epoch with batch size 20. The client learning rate is tuned from {10−2, 10−1.5, . . . , 1}; the server
learning rate is tuned from {10−1.5, 10−1, . . . , 100.5}."
STACKOVERFLOW,0.8565573770491803,"C.2.1
CONSISTENCY ACROSS VARIOUS HYPERPARAMETERS CHOICE"
STACKOVERFLOW,0.8586065573770492,"In the main result Table 1 we only present the best hyperparameter choice (learning rate combina-
tions). In this subsubsection, we show that the pattern of generalization gap is consistent across
hyperparameter choice. The result is shown in Figures 13 and 14."
STACKOVERFLOW,0.860655737704918,"C.2.2
EFFECT OF WEIGHT DECAY STRENGTH"
STACKOVERFLOW,0.8627049180327869,"In the main experiments we by default set the weight decay of ResNet-18 to be 10−4. In this
subsubsection, we experiment various other options of weight decay from 10−5 to 10−2"
STACKOVERFLOW,0.8647540983606558,The result is shown in Figure 15.
STACKOVERFLOW,0.8668032786885246,Published as a conference paper at ICLR 2022
STACKOVERFLOW,0.8688524590163934,"500
1000 1500
round 0.7 0.8 0.9 1.0"
STACKOVERFLOW,0.8709016393442623,accuracy
STACKOVERFLOW,0.8729508196721312,"s=100.5 | 
c=10
1.0"
STACKOVERFLOW,0.875,"500
1000 1500
round"
STACKOVERFLOW,0.8770491803278688,"s=100.5 | 
c=10
1.5"
STACKOVERFLOW,0.8790983606557377,"500
1000 1500
round"
STACKOVERFLOW,0.8811475409836066,"s=100.0 | 
c=10
1.0"
STACKOVERFLOW,0.8831967213114754,"500
1000 1500
round"
STACKOVERFLOW,0.8852459016393442,"s=100.5 | 
c=10
2.0"
STACKOVERFLOW,0.8872950819672131,"participating training
participating validation
unparticipating"
STACKOVERFLOW,0.889344262295082,"Figure 13: Consistency of participation gaps across hyperparameter choice (learning rates
conﬁguration). We present the best four (4) combination of learning rates for federated training of
CIFAR-10. Here ηc stands for client learning rate, and ηs stands for server learning rate. We observe
that the participation gap is largely consistent across various conﬁgurations of learning rates."
STACKOVERFLOW,0.8913934426229508,"500
1000 1500
round 0.4 0.6 0.8 1.0"
STACKOVERFLOW,0.8934426229508197,accuracy
STACKOVERFLOW,0.8954918032786885,"s=100.5 | 
c=10
1.5"
STACKOVERFLOW,0.8975409836065574,"500
1000 1500
round"
STACKOVERFLOW,0.8995901639344263,"s=100.0 | 
c=10
1.0"
STACKOVERFLOW,0.9016393442622951,"500
1000 1500
round"
STACKOVERFLOW,0.9036885245901639,"s=100.0 | 
c=10
0.5"
STACKOVERFLOW,0.9057377049180327,"500
1000 1500
round"
STACKOVERFLOW,0.9077868852459017,"s=100.5 | 
c=10
2.0"
STACKOVERFLOW,0.9098360655737705,"participating training
participating validation
unparticipating"
STACKOVERFLOW,0.9118852459016393,"Figure 14: Consistency of participation gaps across hyperparameter choice (learning rates
conﬁguration). We present the best four (4) combination of learning rates for federated training of
CIFAR-100. Here ηc stands for client learning rate, and ηs stands for server learning rate. We observe
that the participation gap is consistent across various conﬁgurations of learning rates."
STACKOVERFLOW,0.9139344262295082,"10
5
10
4
10
3
10
2
weight decay 0.52 0.53 0.54 0.55 0.56 0.57 0.58"
STACKOVERFLOW,0.9159836065573771,accuracy
STACKOVERFLOW,0.9180327868852459,"participating validation
unparticipating"
STACKOVERFLOW,0.9200819672131147,"Figure 15:
Effect of ℓ2 weight decay on
CIFAR-100 training. We federated train the
ResNet-18 networks for CIFAR-100 with vari-
ous levels of weight decay ranging from 10−5
to 10−2. We observe that a moderate scale of
weight decay might improve the unparticipat-
ing accuracy and therefore decrease the par-
ticipation gap. However, an overlarge weight
decay might hurt both participating validation
and unparticipating performance."
STACKOVERFLOW,0.9221311475409836,Published as a conference paper at ICLR 2022
STACKOVERFLOW,0.9241803278688525,"C.2.3
EFFECT OF MODEL DEPTH"
STACKOVERFLOW,0.9262295081967213,"In the main experiments we by default train a ResNet-18 for the CIFAR task. In this subsubsection,
we experiment a deeper model (ResNet-50) for the CIFAR-100. The result is shown in Figure 16."
STACKOVERFLOW,0.9282786885245902,"500
1000
1500
round 0.400 0.425 0.450 0.475 0.500 0.525 0.550 0.575 0.600"
STACKOVERFLOW,0.930327868852459,accuracy
STACKOVERFLOW,0.9323770491803278,CIFAR-100 | ResNet-18
STACKOVERFLOW,0.9344262295081968,"500
1000
1500
round"
STACKOVERFLOW,0.9364754098360656,CIFAR-100 | ResNet-50
STACKOVERFLOW,0.9385245901639344,"participating training
participating validation
unparticipating"
STACKOVERFLOW,0.9405737704918032,"Figure 16:
Effect of a deeper ResNet on
CIFAR-100 training. We federatedly train a
ResNet-50 for CIFAR-100 to compare with
our default choice (ResNet-18). We apply a
constant learning rate (instead of step decay
learning rate) for easy comparison. We observe
that while using a deeper model improves the
overall accuracy, the participation gap is still
reasonably large for ResNet-50."
STACKOVERFLOW,0.9426229508196722,"C.3
SHAKESPEARE NEXT CHARACTER PREDICTION TASK"
STACKOVERFLOW,0.944672131147541,"Federated Dataset Description and Preprocessing. The Shakespeare dataset (Caldas et al., 2019) is
a next character prediction dataset containing lines from the Complete Works of William Shakespeare
where each client is a different character from one of the plays. We adopt the federated shakespeare
dataset hosted by Tensorﬂow Federated (TFF). In TFF, the federated shakespeare dataset was by
default split intra-cliently, namely all the clients appeared in both the “training” and “validation”
dataset. To construct a three-way split, we hold out 20% of the total clients as unparticipating clients,
and leave the remaining (80%) clients as participating clients (which gives the result reported in
Table 1). Within each participating client, we keep the original training/validation split, e.g., the
original training data that are assigned to these participating clients will become participating training
data. We also tested the performance under other numbers of participating clients, as shown in
Figure 7."
STACKOVERFLOW,0.9467213114754098,"Model, Optimizer, and Hyperparameters We train the same recurrent neural network as in (Reddi
et al., 2021). For centralized training, we run 30 epochs of Adam (with ϵ = 10−4) with batch size
20. We tune the centralized learning rate from {10−3, 10−2.5, . . . , 10−1}. For federated training, we
run 3,000 rounds of FEDADAM (Reddi et al., 2021) with server ϵ = 10−4. For each communictaion
round, we uniformly sample 10 clients, and let each client train for 1 local epoch with batch size 10.
Both client and server learning rates are tuned from {10−2, 10−1.5, . . . , 1}."
STACKOVERFLOW,0.9487704918032787,"C.4
STACKOVERFLOW NEXT WORD PREDICTION TASK"
STACKOVERFLOW,0.9508196721311475,"Federated Dataset Description and Preprocessing. The Stack Overﬂow dataset consists of ques-
tions and answers taken from the website Stack Overﬂow. Each client is a different user of the
website. We adopt the stackoverﬂow dataset hosted by Tensorﬂow Federated (TFF). In TFF, the
federated stackoverﬂow dataset is splitted inter-cliently, namely the training data and validation data
belong to two disjoint subsets of clients. To construct a three-way split, we will treat the original
“validation” clients as unparticipating clients. Within each participating client, we randomly hold out
the max of 20% or 1000 elements as (participating) validation data, and the max of 80% or 1000
elements as (participating) training data. Due to the abundance of stackoverﬂow data, we randomly
sample a subset of clients from the original “training” clients as participating clients. The result
shown in Table 1 is for the case with 3425 participating clients. We also tested other various levels of
participating clients, shown in Figure 7."
STACKOVERFLOW,0.9528688524590164,"Model, Optimizer, and Hyperparameters We train the same recurent neural network as in (Reddi
et al., 2021). For centralized training, we run 30 epochs of Adam (with ϵ = 10−4) with batch size 200.
We tune the centralized learning rate from {10−3, 10−2.5, . . . , 10−1.5}. For federated training, we
run 6,000 rounds of FEDADAM (Reddi et al., 2021) with server ϵ = 10−4. For each communictaion
round, we randomly sample 100 clients, and let each client train for 1 local epoch with batch size 50.
Both client and server learning rates are tuned from {10−2, 10−1.5, . . . , 1}. The client learning rate is
tuned from {10−3, 10−1.5, . . . , 10−1}; the server learning rate is tuned from {10−2, 10−1.5, . . . , 1}."
STACKOVERFLOW,0.9549180327868853,Published as a conference paper at ICLR 2022
STACKOVERFLOW,0.9569672131147541,"D
DETAILS OF SEMANTICICALLY PARTITIONED FEDERATED DATASET"
STACKOVERFLOW,0.9590163934426229,"D.1
DETAILS OF THE SEMANTIC PARTITIONING SCHEME"
STACKOVERFLOW,0.9610655737704918,"In this section we provide the details of the proposed algorithm to semantically partition a federated
dataset for CIFAR-10 and CIFAR-100. For clarify, we use K to denote the number of classes, and C
to denote the number of clients partitioned into."
STACKOVERFLOW,0.9631147540983607,The ﬁrst stage aims to cluster each label into C clusters.
STACKOVERFLOW,0.9651639344262295,"1. Embed the original inputs of dataset using a pretrained EfﬁcientNetB3. This gives a embedding of
dimension 1280 for each input.
2. Reduce the dimension of the above embeddings to 256 dimensions via PCA.7"
STACKOVERFLOW,0.9672131147540983,"3. For each label, ﬁt the corresponding input with a Gaussian mixture model with C clusters. This
step yields C gaussian distribution for each of the K labels. Formally, we let Dk
c denote the
(Gaussian) distribution of the cluster c of label k."
STACKOVERFLOW,0.9692622950819673,"The second stage will package the clusters from different labels across clients. We aim to compute an
optimal multi-partite matching with cost-matrix deﬁned by KL-divergence between the Gaussian
clusters. To reduce complexity, we heuristically solve the optimal multi-partite matching by pro-
gressively solving the optimal bipartite matching at each time for some randomly-chosen label pairs.
Formally, we run the following procedure"
STACKOVERFLOW,0.9713114754098361,"1: Initialize Sunmatched ←{1, . . . , K}
2: Randomly sample a label k from Sunmatched, and remove k from Sunmatched.
3: while Sunmatched ̸= ∅do
4:
Randomly sample a label k′ from Sunmatched, and remove k′ from Sunmatched.
5:
Compute a cost matrix A of dimension C × C, where Aij ←DKL(Dk
i ||Dk′
j ).
6:
Solve and record the optimal bipartite matching with cost matrix A.
7:
Set k ←k′"
STACKOVERFLOW,0.9733606557377049,8: return the aggregation of all the bipartite matchings computed.
STACKOVERFLOW,0.9754098360655737,"D.2
VISUALIZATION OF SEMANTICICALLY PARTITIONED CIFAR-100 DATASET"
STACKOVERFLOW,0.9774590163934426,"Figure 17: Visualization of semantic partitioning of CIFAR-100. We partition the CIFAR-100
dataset into 100 clients without resorting to external user information (such as writer identiﬁcation).
Here we show 10 out of 100 clients featuring the label “apple”."
STACKOVERFLOW,0.9795081967213115,"D.3
VISUALIZATION OF SEMANTICALLY PARTITIONED MNIST DATASET"
STACKOVERFLOW,0.9815573770491803,"7Reducing the dimension is purely a computational issue since the original embedding dimension (1280) is
too large for downstream procedures such as GMM ﬁtting and optimal matching (measured by KL divergence).
While there may be other complicated dimension reduction technique, we found PCA to be simple enough to
generate reasonable results. The dimension of 256 is a trade-off of (down-stream) computational complexity and
embedding information."
STACKOVERFLOW,0.9836065573770492,Published as a conference paper at ICLR 2022
STACKOVERFLOW,0.985655737704918,"Figure 18: Visualization of semantic partitioning of MNIST. We partition the (classic) MNIST
dataset into 300 clients without resorting to external user information (such as writer identiﬁcation).
Here we show 5 out of 300 clients. Observe that the images within each client demonstrates consistent
writing styles both within label and across labels."
STACKOVERFLOW,0.9877049180327869,Published as a conference paper at ICLR 2022
STACKOVERFLOW,0.9897540983606558,"E
METHODOLOGY FOR COMPUTING ENTROPY"
STACKOVERFLOW,0.9918032786885246,"We hypothesize that a participation gap exists for naturally partitioned datasets and not for synthet-
ically partitioned datasets because the naturally partitioned datasets inherently contain correlated
inputs not drawn IID from the full data generating distribution. Put another way, the entropy of the
input data for a given label from a naturally partitioned client is lower than the entropy for that same
label from a synthetically partitioned client. To evaluate this claim, we need to (approximately) infer
the data generating distribution for each client, and then measure the entropy of this distribution,
deﬁned as:
H(q) = −Ex∼q(x) log q(x)
(6)"
STACKOVERFLOW,0.9938524590163934,"To infer the client data generating distribution, we used deep generative models. Because our clients
possess relatively few training examples (O(10) for a particular class), many deep generative models
such as Glow (Kingma & Dhariwal, 2018) or PixelCNN (Salimans et al., 2017) will not be able to
learn a reasonable density model. We instead used a Variational Autoencoder (Kingma & Welling,
2013) to approximate the deep generative process. This model is signiﬁcantly easier to train compared
to the much larger generative models, but does not have tractable log-evidence measurement. Instead,
models are trained by minimizing the negative Evidence Lower Bound (ELBO)."
STACKOVERFLOW,0.9959016393442623,"We ﬁltered each client to contain data only for a single label. Because of the sparseness of the data
after ﬁltering, we found that a 2 dimensional latent space was sufﬁcient to compress our data without
signiﬁcant losses. We used a Multivariate Normal distribution for our posterior and prior, and an
Independent Bernoulli distribution for our likelihood. The posterior was given a full covariance
matrix to account for correlations in the latent variable. All models were trained for 104 training
steps."
STACKOVERFLOW,0.9979508196721312,"In order to evaluate our models, we used a stochastic approximation to the log-evidence, given
by a 1000 sample IWAE (Burda et al., 2015). IWAE is a lower bound on the Bayesian Evidence
that becomes asymptotically tight when computed with a large number of samples. We evaluated
the entropy for 100 clients from naturally partitioned, syntactically partitioned, and synthetically
partitioned datasets, and computed the average across clients as our estimate for the client data
entropy. We ﬁnd that synthetic partitioning results in an average client entropy of 50 Nats, while
Natural partitioning results in clients with only 40 Nats of entropy. Syntactic partitioning falls in
between these two, having 45 Nats of entropy."
