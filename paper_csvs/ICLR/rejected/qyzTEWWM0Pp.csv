Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0025380710659898475,"In this paper, we propose Multiresolution Equivariant Graph Variational Autoen-
coders (MGVAE), the ﬁrst hierarchical generative model to learn and generate
graphs in a multiresolution and equivariant manner. At each resolution level, MG-
VAE employs higher order message passing to encode the graph while learning
to partition it into mutually exclusive clusters and coarsening into a lower res-
olution that eventually creates a hierarchy of latent distributions. MGVAE then
constructs a hierarchical generative model to variationally decode into a hierarchy
of coarsened graphs. Importantly, our proposed framework is end-to-end permuta-
tion equivariant with respect to node ordering. MGVAE achieves competitive re-
sults with several generative tasks including general graph generation, molecular
generation, unsupervised molecular representation learning to predict molecular
properties, link prediction on citation graphs, and graph-based image generation."
INTRODUCTION,0.005076142131979695,"1
INTRODUCTION"
INTRODUCTION,0.007614213197969543,"Understanding graphs in a multiscale and multiresolution perspective is essential for capturing the
structure of molecules, social networks, or the World Wide Web. Graph neural networks (GNNs)
utilizing various ways of generalizing the concept of convolution to graphs (Scarselli et al., 2009)
(Niepert et al., 2016b) (Li et al., 2016) have been widely applied to many learning tasks, including
modeling physical systems (Battaglia et al., 2016), ﬁnding molecular representations to estimate
quantum chemical computation (Duvenaud et al., 2015) (Kearnes et al., 2016) (Gilmer et al., 2017b)
(Hy et al., 2018), and protein interface prediction (Fout et al., 2017). One of the most popular
types of GNNs is message passing neural nets (MPNNs) that are constructed based on the message
passing scheme in which each node propagates and aggregates information, encoded by vectorized
messages, to and from its local neighborhood. While this framework has been immensely successful
in many applications, it lacks the ability to capture the multiscale and multiresolution structures that
are present in complex graphs (Rustamov & Guibas, 2013) (Chen et al., 2014) (Cheng et al., 2015)
(Xu et al., 2019)."
INTRODUCTION,0.01015228426395939,"Ying et al. (2018) proposed a multiresolution graph neural network that employs a differential pool-
ing operator to coarsen the graph. While this approach is effective in some settings, it is based on
soft assigment matrices, which means that (a) the sparsity of the graph is quickly lost in higher layers
(b) the algorithm isn’t able to learn an actual hard clustering of the vertices. The latter is important
in applications such as learning molecular graphs, where clusters should ideally be interpretable as
concrete subunits of the graphs, e.g., functional groups."
INTRODUCTION,0.012690355329949238,"In contrast, in this paper we propose an arhictecture called Multiresolution Graph Network (MGN),
and its generative cousin, Multiresolution Graph Variational Autoencoder (MGVAE), which explic-
itly learn a multilevel hard clustering of the vertices, leading to a true multiresolution hierarchy. In
the decoding stage, to “uncoarsen” the graph, MGVAE needs to generate local adjacency matrices,
which is inherently a second order task with respect to the action of permutations on the vertices,
hence MGVAE needs to leverage the recently developed framework of higher order permutation
equivariant message passing networks (Hy et al., 2018; Maron et al., 2019b)."
INTRODUCTION,0.015228426395939087,"Learning to generate graphs with deep generative models is a difﬁcult problem because graphs are
combinatorial objects that typically have high order correlations between their discrete substruc-
tures (subgraphs) (You et al., 2018a) (Li et al., 2018) (Liao et al., 2019) (Liu et al., 2019) (Dai
et al., 2020). Graph-based molecular generation (Gmez-Bombarelli et al., 2018) (Simonovsky &"
INTRODUCTION,0.017766497461928935,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02030456852791878,"Komodakis, 2018) (Cao & Kipf, 2018) (Jin et al., 2018) (Thiede et al., 2020) involves further chal-
lenges, including correctly recognizing chemical substructures, and importantly, ensuring that the
generated molecular graphs are chemically valid. MGN allows us to extend the existing model of
variational autoencoders (VAEs) with a hierarchy of latent distributions that can stochastically gen-
erate a graph in multiple resolution levels. Our experiments show that having a ﬂexible clustering
procedure from MGN enables MGVAE to detect, reconstruct and ﬁnally generate important graph
substructures, especially chemical functional groups."
RELATED WORK,0.02284263959390863,"2
RELATED WORK"
RELATED WORK,0.025380710659898477,"There have been signiﬁcant advances in understanding the invariance and equivariance properties of
neural networks in general (Cohen & Welling, 2016a) (Cohen & Welling, 2016b), of graph neural
networks (Maron et al., 2019b), of neural networks learning on sets (Zaheer et al., 2017) (Serviansky
et al., 2020) (Maron et al., 2020), along with their expressive power on graphs (Maron et al., 2019c)
(Maron et al., 2019a). Our work is in line with group equivariant networks operating on graphs and
sets. Multiscale, multilevel, multiresolution and coarse-grained techniques have been widely applied
to graphs and discrete domains such as diffusion wavelets (Coifman & Maggioni, 2006); spectral
wavelets on graphs (Hammond et al., 2011); ﬁnding graph wavelets based on partitioning/clustering
(Rustamov & Guibas, 2013); graph clustering and ﬁnding balanced cuts on large graphs (Dhillon
et al., 2005) (Dhillon et al., 2007) (Chiang et al., 2012) (Si et al., 2014); and link prediction on social
networks (Shin et al., 2012). Prior to our work, some authors such as (Zhou et al., 2019) proposed
a multiscale generative model on graphs using GAN (Goodfellow et al., 2014), but the hierarchical
structure was built by heuristics algorithm, not learnable and not ﬂexible to new data that is also
an existing limitation of the ﬁeld. In general, our work exploits the powerful group equivariant
networks to encode a graph and to learn to form balanced partitions via back-propagation in a data-
driven manner without using any heuristics as in the existing works."
RELATED WORK,0.027918781725888325,"In the ﬁeld of deep generative models, it is generally recognized that introducing a hierarchy of
latents and adding stochasticity among latents leads to more powerful models capable of learning
more complicated distributions (Blei et al., 2003) (Ranganath et al., 2016) (Ingraham & Marks,
2017) (Klushyn et al., 2019) (Wu et al., 2020) (Vahdat & Kautz, 2020). Our work combines the
hierarchical variational autoencoder with learning to construct the hierarchy that results into a gen-
erative model able to generate graphs at many resolution levels."
MULTIRESOLUTION GRAPH NETWORK,0.030456852791878174,"3
MULTIRESOLUTION GRAPH NETWORK"
CONSTRUCTION,0.03299492385786802,"3.1
CONSTRUCTION"
CONSTRUCTION,0.03553299492385787,"An undirected weighted graph G = (V,E,A,Fv,Fe) with node set V and edge set E is represented
by an adjacency matrix A ∈N∣V∣×∣V∣, where Aij > 0 implies an edge between node vi and vj
with weight Aij (e.g., Aij ∈{0,1} in the case of unweighted graph); while node features are
represented by a matrix Fv ∈R∣V∣×dv, and edge features are represented by a tensor Fe ∈R∣V∣×∣V∣×de.
The second-order tensor representation of edge features is necessary for our higher-order message
passing networks described in the next section. Indeed, Fv can be encoded in the diagonal of Fe.
Deﬁnition 1. A K-cluster partition of graph G is a partition of the set of nodes V into K mutually
exclusive clusters V1,..,VK. Each cluster corresponds to an induced subgraph Gk = G[Vk]."
CONSTRUCTION,0.03807106598984772,"Deﬁnition 2. A coarsening of G is a graph ˜G of K nodes deﬁned by a K-cluster partition in which
node ˜vk of ˜G corresponds to the induced subgraph Gk. The weighted adjacency matrix ˜
A ∈NK×K"
CONSTRUCTION,0.04060913705583756,of ˜G is
CONSTRUCTION,0.04314720812182741,"˜
Akk′ = {"
CONSTRUCTION,0.04568527918781726,"1
2 ∑vi,vj∈Vk Aij,
if k = k′,
∑vi∈Vk,vj∈Vk′ Aij,
if k ≠k′,"
CONSTRUCTION,0.048223350253807105,"where the diagonal of ˜
A denotes the number of edges inside each cluster, while the off-diagonal
denotes the number of edges between two clusters."
CONSTRUCTION,0.050761421319796954,"Fig. 3.1 shows an example of Defs. 1 and 2: a 3-cluster partition of the Aspirin C9H8O4 molecular
graph and its coarsening graph. Def. 3 deﬁnes the multiresolution of graph G in a bottom-up manner"
CONSTRUCTION,0.0532994923857868,Under review as a conference paper at ICLR 2022 V1 V2 V3 1 1 2 6 3
CONSTRUCTION,0.05583756345177665,"Figure 1: Aspirin C9H8O4, its 3-cluster partition and the corresponding coarsen graph"
CONSTRUCTION,0.0583756345177665,"in which the bottom level is the highest resolution (e.g., G itself) while the top level is the lowest
resolution (e.g., G is coarsened into a single node)."
CONSTRUCTION,0.06091370558375635,"Deﬁnition 3. An L-level coarsening of a graph G is a series of L graphs G(1),..,G(L) in which"
CONSTRUCTION,0.06345177664974619,1. G(L) is G itself.
CONSTRUCTION,0.06598984771573604,"2. For 1 ≤ℓ≤L −1, G(ℓ) is a coarsening graph of G(ℓ+1) as deﬁned in Def. 2. The number
of nodes in G(ℓ) is equal to the number of clusters in G(ℓ+1)."
CONSTRUCTION,0.06852791878172589,"3. The top level coarsening G(1) is a graph consisting of a single node, and the corresponding
adjacency matrix A(1) ∈N1×1."
CONSTRUCTION,0.07106598984771574,"Deﬁnition 4. An L-level Multiresolution Graph Network (MGN) of a graph G consists of L−1 tu-
ples of ﬁve network components {(c(ℓ),e(ℓ)
local,d(ℓ)
local,d(ℓ)
global,p(ℓ))}L
ℓ=2. The ℓ-th tuple encodes G(ℓ)"
CONSTRUCTION,0.07360406091370558,"and transforms it into a lower resolution graph G(ℓ−1) in the higher level. Each of these network
components has a separate set of learnable parameters (θ(ℓ)
1 ,θ(ℓ)
2 ,θ(ℓ)
3 ,θ(ℓ)
4 ,θ(ℓ)
5 ). For simplic-
ity, we collectively denote the learnable parameters as θ, and drop the superscript. The network
components are deﬁned as follows:"
"CLUSTERING
PROCEDURE",0.07614213197969544,"1. Clustering
procedure
c(G(ℓ);θ),
which
partitions
graph
G(ℓ)
into
K
clusters
V(ℓ)
1 ,...,V(ℓ)
K . Each cluster is an induced subgraph G(ℓ)
k
of G(ℓ) with adjacency matrix
A(ℓ)
k ."
"CLUSTERING
PROCEDURE",0.07868020304568528,"2. Local encoder elocal(G(ℓ)
k ;θ), which is a permutation equivariant (see Defs. 5, 6) graph
neural network that takes as input the subgraph G(ℓ)
k , and outputs a set of node latents
Z(ℓ)
k
represented as a matrix of size ∣V(ℓ)
k ∣× dz."
"CLUSTERING
PROCEDURE",0.08121827411167512,"3. Local decoder dlocal(Z(ℓ)
k ;θ), which is a permutation equivariant neural network that tries
to reconstruct the subgraph adjacency matrix A(ℓ)
k
for each cluster from the local encoder’s
output latents."
"CLUSTERING
PROCEDURE",0.08375634517766498,"4. (Optional) Global decoder dglobal(Z(ℓ);θ), which is a permutation equivariant neural net-
work that reconstructs the full adjacency matrix A(ℓ) from all the node latents of K clusters
Z(ℓ) = ⊕k Z(ℓ)
k
represented as a matrix of size ∣V(ℓ)∣× dz."
"CLUSTERING
PROCEDURE",0.08629441624365482,"5. Pooling network p(Z(ℓ)
k ;θ), which is a permutation invariant (see Defs. 5, 6) neural net-
work that takes the set of node latents Z(ℓ)
k
and outputs a single cluster latent ˜z(ℓ)
k
∈dz.
The coarsening graph G(ℓ−1) has adjacency matrix A(ℓ−1) built as in Def. 2, and the cor-
responding node features Z(ℓ−1) = ⊕k ˜z(ℓ)
k
represented as a matrix of size K × dz."
"CLUSTERING
PROCEDURE",0.08883248730964467,"Algorithmically, MGN works in a bottom-up manner as a tree-like hierarchy starting from the high-
est resolution graph G(L), going to the lowest resolution G(1) (see Fig. 3.1). Iteratively, at ℓ-th level,
MGN partitions the current graph into K clusters by running the clustering procedure c(ℓ). Then,
the local encoder e(ℓ)
local and local decoder d(ℓ)
global operate on each of the K subgraphs separately, and
can be executed in parallel. This encoder/decoder pair is responsible for capturing the local struc-
tures. Finally, the pooling network p(ℓ) shrinks each cluster into a single node of the next level."
"CLUSTERING
PROCEDURE",0.09137055837563451,Under review as a conference paper at ICLR 2022
"CLUSTERING
PROCEDURE",0.09390862944162437,"v0
v1
v2
v3
v4
v5
v6
v7
v8
v9
v10
v11
v12"
"CLUSTERING
PROCEDURE",0.09644670050761421,"V(2)
1
A(3)
1
=
⎛
⎜
⎝"
"CLUSTERING
PROCEDURE",0.09898477157360407,"0
1
1
1
0
0
1
0
0 ⎞
⎟
⎠"
"CLUSTERING
PROCEDURE",0.10152284263959391,"A(3)
2
="
"CLUSTERING
PROCEDURE",0.10406091370558376,"⎛
⎜⎜⎜⎜⎜⎜
⎝"
"CLUSTERING
PROCEDURE",0.1065989847715736,"0
1
0
0
0
1
1
0
1
0
0
0
0
1
0
1
0
0
0
0
1
0
1
0
0
0
0
1
0
1
1
0
0
0
1
0"
"CLUSTERING
PROCEDURE",0.10913705583756345,"⎞
⎟⎟⎟⎟⎟⎟
⎠"
"CLUSTERING
PROCEDURE",0.1116751269035533,"V(2)
2
V(2)
3
A(3)
3
=
⎛
⎜⎜
⎝"
"CLUSTERING
PROCEDURE",0.11421319796954314,"0
1
0
0
1
0
1
1
0
1
0
0
0
1
0
0"
"CLUSTERING
PROCEDURE",0.116751269035533,"⎞
⎟⎟
⎠"
"CLUSTERING
PROCEDURE",0.11928934010152284,"G(1)
A(2) =
⎛
⎜
⎝"
"CLUSTERING
PROCEDURE",0.1218274111675127,"2
1
0
1
6
1
0
1
3 ⎞
⎟
⎠"
"CLUSTERING
PROCEDURE",0.12436548223350254,Figure 2: Hierarchy of 3-level Multiresolution Graph Network on Aspirin molecular graph
"CLUSTERING
PROCEDURE",0.12690355329949238,"Optionally, the global decoder d(ℓ)
global makes sure that the whole set of node latents Z(ℓ) is able to
capture the inter-connection between clusters."
"CLUSTERING
PROCEDURE",0.12944162436548223,"In terms of time and space complexity, MGN is more efﬁcient than existing methods in the ﬁeld.
The cost of global decoding the highest resolution graph is proportional to ∣V∣2. For example, while
the encoder can exploit the sparsity of the graph and has complexity O(∣E∣), a simple dot-product
decoder dglobal(Z) = sigmoid(ZZT ) has both time and space complexity of O(∣V∣2) which is infea-
sible for large graphs. In contrast, the cost of running K local dot-product decoders is O(∣V∣2/K),
which is approximately K times more efﬁcient."
HIGHER ORDER MESSAGE PASSING,0.1319796954314721,"3.2
HIGHER ORDER MESSAGE PASSING"
HIGHER ORDER MESSAGE PASSING,0.13451776649746192,"In this paper we consider permutation symmetry, i.e., symmetry to the action of the symmetric
group, Sn. An element σ ∈Sn is a permutation of order n, or a bijective map from {1,...,n} to
{1,...,n}. The action of Sn on an adjacency matrix A ∈Rn×n and on a latent matrix Z ∈Rn×dz
are
[σ ⋅A]i1,i2 = Aσ−1(i1),σ−1(i2),
[σ ⋅Z]i,j = Zσ−1(i),j,
σ ∈Sn.
Here, the adjacency matrix A is a second order tensor with a single feature channel, while the latent
matrix Z is a ﬁrst order tensor with dz feature channels. In general, the action of Sn on a k-th order
tensor X ∈Rnk×d (the last index denotes the feature channels) is deﬁned similarly as:"
HIGHER ORDER MESSAGE PASSING,0.13705583756345177,"[σ ⋅X]i1,..,ik,j = Xσ−1(i1),..,σ−1(ik),j,
σ ∈Sn."
HIGHER ORDER MESSAGE PASSING,0.13959390862944163,"Network components of MGN (as deﬁned in Sec. 3.1) at each resolution level must be either equiv-
ariant, or invariant with respect to the permutation action on the node order of G(ℓ). Formally, we
deﬁne these properties in Def. 5.
Deﬁnition
5.
An
Sn-equivariant
(or
permutation
equivariant)
function
is
a
function
f∶Rnk×d →Rnk′×d′ that satisﬁes f(σ ⋅X) = σ ⋅f(X) for all σ ∈Sn and X ∈Rnk×d. Similarly,
we say that f is Sn-invariant (or permutation invariant) if and only if f(σ ⋅X) = f(X)."
HIGHER ORDER MESSAGE PASSING,0.14213197969543148,"Deﬁnition 6. An Sn-equivariant network is a function f ∶Rnk×d →Rnk′×d′ deﬁned as a composition
of Sn-equivariant linear functions f1,..,fT and Sn-equivariant nonlinearities γ1,..,γT :"
HIGHER ORDER MESSAGE PASSING,0.1446700507614213,f = γT ○fT ○.. ○γ1 ○f1.
HIGHER ORDER MESSAGE PASSING,0.14720812182741116,"On the another hand, an Sn-invariant network is a function f ∶Rnk×d →R deﬁned as a composition
of an Sn-equivariant network f ′ and an Sn-invariant function on top of it, e.g., f = f ′′ ○f ′."
HIGHER ORDER MESSAGE PASSING,0.14974619289340102,"In order to build higher order equivariant networks, we revisit some basic tensor operations: the
tensor product A ⊗B and tensor contraction A↓x1,..,xp (details and deﬁnitions are Sec. A). It can be
shown that these tensor operations respect permutation equivariance (Kondor et al., 2018). Based
on these tensor contractions and Def. 5, we can construct the second-order Sn-equivariant networks
as in Def. 6 (see Example 1): f = γ ○MT ○.. ○γ ○M1.
The second-order networks are par-
ticularly essential for us to extend the original variational autoencoder (VAE) (Kingma & Welling,"
HIGHER ORDER MESSAGE PASSING,0.15228426395939088,Under review as a conference paper at ICLR 2022
HIGHER ORDER MESSAGE PASSING,0.1548223350253807,"2014) model that approximates the posterior distribution by an isotropic Gaussian distribution with
a diagonal covariance matrix and uses a ﬁxed prior distribution N(0,1). In constrast, we generalize
by modeling the posterior by N(µ,Σ) in which Σ is a full covariance matrix, and we learn an adap-
tive parameterized prior N(ˆµ, ˆΣ) instead of a ﬁxed one. Only the second-order encoders can output
a permutation equivariant full covariance matrix, while lower-order networks such as MPNNs are
unable to. See Sec. 4.2, B and C for details.
Example 1. The second order message passing has the message H0 ∈R∣V∣×∣V∣×(dv+de) initialized
by promoting the node features Fv to a second order tensor (e.g., we treat node features as self-loop
edge features), and concatenating with the edge features Fe. Iteratively,"
HIGHER ORDER MESSAGE PASSING,0.15736040609137056,"Ht = γ(Mt),
Mt = Wt[⊕
i,j
(A ⊗Ht−1)↓i,j],"
HIGHER ORDER MESSAGE PASSING,0.1598984771573604,"where A ⊗Ht−1 results in a fourth order tensor while ↓i,j contracts it down to a second order
tensor along the i-th and j-th dimensions, ⊕denotes concatenation along the feature channels, and
Wt denotes a multilayer perceptron on the feature channels. We remark that the popular MPNNs
(Gilmer et al., 2017b) is a lower-order one and a special case in which Mt = D−1AHt−1Wt−1
where Dii = ∑j Aij is the diagonal matrix of node degrees. The message HT of the last iteration is
still second order, so we contract it down to the ﬁrst order latent Z = ⊕i HT ↓i."
LEARNING TO CLUSTER,0.16243654822335024,"3.3
LEARNING TO CLUSTER"
LEARNING TO CLUSTER,0.1649746192893401,"Deﬁnition 7. A clustering of n objects into k clusters is a mapping π ∶{1,..,n} →{1,..,k} in which
π(i) = j if the i-th object is assigned to the j-th cluster. The inverse mapping π−1(j) = {i ∈[1,n] ∶
π(i) = j} gives the set of all objects assigned to the j-th cluster. The clustering is represented by an
assignment matrix Π ∈{0,1}n×k such that Πi,π(i) = 1.
Deﬁnition 8. The action of Sn on a clustering π of n objects into k clusters and its corresponding
assignment matrix Π are"
LEARNING TO CLUSTER,0.16751269035532995,"[σ ⋅π](i) = π(σ−1(i)),
[σ ⋅Π]i,j = Πσ−1(i),j,
σ ∈Sn."
LEARNING TO CLUSTER,0.1700507614213198,"Deﬁnition 9. Let N be a neural network that takes as input a graph G of n nodes, and outputs a
clustering π of k clusters. N is said to be equivariant if and only if N(σ ⋅G) = σ ⋅N(G) for all
σ ∈Sn."
LEARNING TO CLUSTER,0.17258883248730963,"From Def. 9, intuitively the assignement matrix Π still represents the same clustering if we permute
its rows. The learnable clustering procedure c(G(ℓ);θ) is built as follows:"
LEARNING TO CLUSTER,0.1751269035532995,"1. A graph neural network parameterized by θ encodes graph G(ℓ) into a ﬁrst order tensor of K
feature channels ˜p(ℓ) ∈R∣V(ℓ)∣×K.
2. The clustering assignment is determined by a row-wise maximum pooling operation:"
LEARNING TO CLUSTER,0.17766497461928935,"π(ℓ)(i) = arg max
k∈[1,K]
˜p(ℓ)
i,k
(1)"
LEARNING TO CLUSTER,0.1802030456852792,that is an equivariant clustering in the sense of Def. 9.
LEARNING TO CLUSTER,0.18274111675126903,"A composition of an equivariant function (e.g., graph net) and an equivariant function (e.g., maxi-
mum pooling given in Eq. 1) is still an equivariant function with respect to the node permutation.
Thus, the learnable clustering procedure c(G(ℓ);θ) is permutation equivariant."
LEARNING TO CLUSTER,0.18527918781725888,"In practice, in order to make the clustering procedure differentiable for backpropagation, we replace
the maximum pooling in Eq. 1 by sampling from a categorical distribution. Let π(ℓ)(i) be a categor-
ical variable with class probabilities p(ℓ)
i,1 ,..,p(ℓ)
i,K computed as softmax from ˜p(ℓ)
i,∶. The Gumbel-max
trick (Gumbel, 1954)(Maddison et al., 2014)(Jang et al., 2017) provides a simple and efﬁcient way
to draw samples π(ℓ)(i):"
LEARNING TO CLUSTER,0.18781725888324874,"Π(ℓ)
i
= one-hot(arg max
k∈[1,K]
[gi,k + log p(ℓ)
i,k]),"
LEARNING TO CLUSTER,0.19035532994923857,"where gi,1,..,gi,K are i.i.d samples drawn from Gumbel(0,1). Given the clustering assignment
matrix Π(ℓ), the coarsened adjacency matrix A(ℓ−1) (see Defs. 1 and 2) can be constructed as
Π(ℓ)TA(ℓ)Π(ℓ)."
LEARNING TO CLUSTER,0.19289340101522842,Under review as a conference paper at ICLR 2022
LEARNING TO CLUSTER,0.19543147208121828,"It is desirable to have a balanced K-cluster partition in which clusters V(ℓ)
1 ,..,V(ℓ)
K have similar sizes
that are close to ∣V(ℓ)∣/K. The local encoders tend to generalize better for same-size subgraphs. We
want the distribution of nodes into clusters to be close to the uniform distribution. We enforce
the clustering procedure to produce a balanced cut by minimizing the following Kullback–Leibler
divergence:"
LEARNING TO CLUSTER,0.19796954314720813,"DKL(P∣∣Q) =
K
∑
k=1
P(k)log P(k)"
LEARNING TO CLUSTER,0.20050761421319796,"Q(k)
where
P = (∣V(ℓ)
1 ∣
∣V(ℓ)∣,.., ∣V(ℓ)
K ∣
∣V(ℓ)∣),
Q = ( 1"
LEARNING TO CLUSTER,0.20304568527918782,"K ,.., 1"
LEARNING TO CLUSTER,0.20558375634517767,"K ).
(2)"
LEARNING TO CLUSTER,0.20812182741116753,"The whole construction of MGN is equivariant with respect to node permutations of G. In the case
of molecular property prediction, we want MGN to learn to predict a real value y ∈R for each graph
G while learning to ﬁnd a balanced cut in each resolution to construct a hierarchical structure of
latents and coarsen graphs. The total loss function is"
LEARNING TO CLUSTER,0.21065989847715735,"LMGN(G,y) = ∣∣f(
L
⊕
ℓ=1
R(Z(ℓ))) −y∣∣
2"
LEARNING TO CLUSTER,0.2131979695431472,"2 +
L
∑
ℓ=1
λ(ℓ)DKL(P (ℓ)∣∣Q(ℓ)),
(3)"
LEARNING TO CLUSTER,0.21573604060913706,"where f is a multilayer perceptron, ⊕denotes the vector concatenation, R is a readout function that
produces a permutation invariant vector of size d given the latent Z∣V(ℓ)∣×d at the ℓ-th resolution,
λ(ℓ) ∈R is a hyperparamter, and DKL(P (ℓ)∣∣Q(ℓ)) is the balanced-cut loss as deﬁned in Eq. 2."
HIERARCHICAL GENERATIVE MODEL,0.2182741116751269,"4
HIERARCHICAL GENERATIVE MODEL"
HIERARCHICAL GENERATIVE MODEL,0.22081218274111675,"In this section, we introduce our hierarchical generative model for multiresolution graph generation
based on variational principles."
BACKGROUND ON GRAPH VARIATIONAL AUTOENCODER,0.2233502538071066,"4.1
BACKGROUND ON GRAPH VARIATIONAL AUTOENCODER"
BACKGROUND ON GRAPH VARIATIONAL AUTOENCODER,0.22588832487309646,"Suppose that we have input data consisting of m graphs (data points) G = {G1,..,Gm}. The stan-
dard variational autoencoders (VAEs), introduced by Kingma & Welling (2014) have the following
generation process, in which each data graph Gi for i ∈{1,2,..,m} is generated independently:"
BACKGROUND ON GRAPH VARIATIONAL AUTOENCODER,0.22842639593908629,"1. Generate the latent variables Z = {Z1,..,Zm}, where each Zi ∈R∣Vi∣×dz is drawn i.i.d. from a
prior distribution p0 (e.g., standard Normal distribution N(0,1)).
2. Generate the data graph Gi ∼pθ(Gi∣Zi) from the model conditional distribution pθ."
BACKGROUND ON GRAPH VARIATIONAL AUTOENCODER,0.23096446700507614,"We want to optimize θ to maximize the likelihood pθ(G) = ∫pθ(Z)pθ(G∣Z)dZ. However, this re-
quires computing the posterior distribution pθ(G∣Z) = ∏m
i=1 pθ(Gi∣Zi), which is usually intractable.
Instead, VAEs apply the variational principle, proposed by Wainwright & Jordan (2005), to approxi-
mate the posterior distribution as qφ(Z∣G) = ∏m
i=1 qφ(Zi∣Gi) via amortized inference and maximize
the evidence lower bound (ELBO) that is a lower bound of the likelihood:
LELBO(φ,θ) = Eqφ(Z∣G)[log pθ(G∣Z)] −DKL(qφ(Z∣G)∣∣p0(Z))"
BACKGROUND ON GRAPH VARIATIONAL AUTOENCODER,0.233502538071066,"=
m
∑
i=1
[Eqφ(Zi∣Gi)[log pθ(Gi∣Zi)] −DKL(qφ(Zi∣Gi)∣∣p0(Zi))].
(4)"
BACKGROUND ON GRAPH VARIATIONAL AUTOENCODER,0.23604060913705585,"The probabilistic encoder qφ(Z∣G), the approximation to the posterior of the generative model
pθ(G,Z), is modeled using equivariant graph neural networks (see Example 1) as follows. As-
sume the prior over the latent variables to be the centered isotropic multivariate Gaussian pθ(Z) =
N(Z;0,I). We let qφ(Zi∣Gi) be a multivariate Gaussian with a diagonal covariance structure:"
BACKGROUND ON GRAPH VARIATIONAL AUTOENCODER,0.23857868020304568,"log qφ(Zi∣Gi) = log N(Zi;µi,σ2
i I),
(5)"
BACKGROUND ON GRAPH VARIATIONAL AUTOENCODER,0.24111675126903553,"where µi,σi ∈R∣Vi∣×dz are the mean and standard deviation of the approximate posterior output by
two equivariant graph encoders. We sample from the posterior qφ by using the reparameterization
trick: Zi = µi + σi ⊙ϵ, where ϵ ∼N(0,I) and ⊙is the element-wise product."
BACKGROUND ON GRAPH VARIATIONAL AUTOENCODER,0.2436548223350254,"On the another hand, the probabilistic decoder pθ(Gi∣Zi) deﬁnes a conditional distribution over the
entries of the adjacency matrix Ai: pθ(Gi∣Zi) = ∏(u,v)∈V2
i pθ(Aiuv = 1∣Ziu,Ziv). For example,
Kipf & Welling (2016) suggests a simple dot-product decoder that is trivially equivariant: pθ(Aiuv =
1∣Ziu,Ziv) = γ(ZT
iuZiv), where γ denotes the sigmoid function."
BACKGROUND ON GRAPH VARIATIONAL AUTOENCODER,0.24619289340101522,Under review as a conference paper at ICLR 2022
MULTIRESOLUTION VAES,0.24873096446700507,"4.2
MULTIRESOLUTION VAES"
MULTIRESOLUTION VAES,0.2512690355329949,"Based on the construction of multiresolution graph network (see Sec. 3.1), the latent vari-
ables are partitioned into disjoint groups, Zi = {Z(1)
i
,Z(2)
i
,..,Z(L)
i
} where Z(ℓ)
i
= {[Z(ℓ)
i
]k ∈"
MULTIRESOLUTION VAES,0.25380710659898476,"R∣[V(ℓ)
i
]k∣×dz}k is the set of latents at the ℓ-th resolution level in which the graph G(ℓ)
i
is partitioned
into a number of clusters [G(ℓ)
i
]k."
MULTIRESOLUTION VAES,0.2563451776649746,"In the area of normalzing ﬂows (NFs), Wu et al. (2020) has shown that stochasticity (e.g., a chain
of stochastic sampling blocks) overcomes expressivity limitations of NFs. In general, our MGVAE
is a stochastic version of the deterministic MGN such that stochastic sampling is applied at each
resolution level in a bottom-up manner. The prior (Eq. 6) and the approximate posterior (Eq. 7) are
represented by"
MULTIRESOLUTION VAES,0.25888324873096447,"p(Zi) =
L
∏
ℓ=1
p(Z(ℓ)
i
) =
L
∏
ℓ=1
∏
k
p([Z(ℓ)
i
]k),
(6)"
MULTIRESOLUTION VAES,0.2614213197969543,"qφ(Zi∣Gi) = qφ(Z(L)
i
∣G(L)
i
)
1
∏
ℓ=L−1
qφ(Z(ℓ)
i
∣Z(ℓ+1)
i
,G(ℓ)
i
),
(7)"
MULTIRESOLUTION VAES,0.2639593908629442,"in which each conditional in the approximate posterior are in the form of factorial Normal distribu-
tions, in particular"
MULTIRESOLUTION VAES,0.26649746192893403,"qφ(Z(ℓ)
i
∣Z(ℓ+1)
i
,G(ℓ)
i
) = ∏
k
qφ([Z(ℓ)
i
]k∣Z(ℓ+1)
i
,[G(ℓ)
i
]k),"
MULTIRESOLUTION VAES,0.26903553299492383,"where each probabilistic encoder qφ([Z(ℓ)
i
]k∣Z(ℓ+1)
i
,[G(ℓ)
i
]k) operates on a subgraph [G(ℓ)
i
]k as
follows:"
MULTIRESOLUTION VAES,0.2715736040609137,"• The pooling network p(ℓ+1) shrinks the latent Z(ℓ+1)
i
into the node features of G(ℓ)
i
as in
the construction of MGN (see Def. 4)."
MULTIRESOLUTION VAES,0.27411167512690354,"• The local (deterministic) graph encoder d(ℓ)
local encodes each subgraph [G(ℓ)
i
]k into a mean
vector and a diagonal covariance matrix (see Eq. 5). A second order encoder can produce a
positive semideﬁnite non-diagonal covariance matrix, that can be interpreted as a Gaussian
Markov Random Fields (details in Sec. B). The new subgraph latent [Z(ℓ)
i
]k is sampled by
the reparameterization trick."
MULTIRESOLUTION VAES,0.2766497461928934,"The prior can be either the isotropic Gaussian N(0,1) as in standard VAEs, or be implemented
as a parameterized Gaussian N(ˆµ, ˆΣ) where ˆµ and ˆΣ are learnable equivariant functions (details in
Sec. C). The reparameterization trick for conventional N(0,1) prior is the same as in Sec. 4.1, while
the new one for the generalized and learnable prior N(ˆµ, ˆΣ) is given in Sec. B. On the another hand,
the probabilistic decoder pθ(G(1)
i
,..,G(L)
i
∣Z(1)
i
,..,Z(L)
i
) deﬁnes a conditional distribution over all
subgraph adjacencies at each resolution level:"
MULTIRESOLUTION VAES,0.27918781725888325,"pθ(G(1)
i
,..,G(L)
i
∣Z(1)
i
,..,Z(L)
i
) = ∏
ℓ
pθ(G(ℓ)
i
∣Z(ℓ)
i
) = ∏
ℓ
∏
k
pθ([A(ℓ)
i ]k∣[Z(ℓ)
i
]k)."
MULTIRESOLUTION VAES,0.2817258883248731,"Extending from Eq. 4, we write our multiresolution variational lower bound LMGVAE(φ,θ) on
log p(G) compactly as"
MULTIRESOLUTION VAES,0.28426395939086296,"LMGVAE(φ,θ) = ∑
i
∑
ℓ
[Eqφ(Z(ℓ)
i
∣G(ℓ)
i
)[log pθ(G(ℓ)
i
∣Z(ℓ)
i
)] −DKL(qφ(Z(ℓ)
i
∣G(ℓ)
i
)∣∣p0(Z(ℓ)
i
))], (8)"
MULTIRESOLUTION VAES,0.2868020304568528,"where the ﬁrst term denotes the reconstruction loss (e.g., ∣∣A(ℓ)
i
−ˆ
A(ℓ)
i ∣∣where A(L)
i
is Gi itself,
A(ℓ<L)
i
is the adjacency produced by MGN at level ℓ, and ˆ
A(ℓ)
i
are the reconstructed ones by the de-"
MULTIRESOLUTION VAES,0.2893401015228426,"coders); and the second term is indeed DKL(N(µ(ℓ)
i ,Σ(ℓ)
i )∣∣N(ˆµ(ℓ), ˆΣ(ℓ))) where µ(ℓ)
i
∈R∣V(ℓ)
i
∣×d"
MULTIRESOLUTION VAES,0.2918781725888325,"and Σ(ℓ)
i
∈R∣V(ℓ)
i
∣×∣V(ℓ)
i
∣×d are the mean and covariance tensors produced by the ℓ-th encoder for
graph Gi, while ˆµ(ℓ)and ˆΣ(ℓ) are learnable ones in an equivariant manner as in Sec. C. In general,
the overall optimization is given as follows:"
MULTIRESOLUTION VAES,0.29441624365482233,"min
φ,θ,{ˆµ(ℓ),ˆΣ(ℓ)}ℓ
LMGVAE(φ,θ;{ˆµ(ℓ), ˆΣ(ℓ)}ℓ) + ∑
i,ℓ
λ(ℓ)DKL(P (ℓ)
i
∣∣Q(ℓ)
i ),
(9)"
MULTIRESOLUTION VAES,0.2969543147208122,"where φ denotes all learnable parameters of the encoders, θ denotes all learnable parameters of the
decoders, and DKL(P (ℓ)
i
∣∣Q(ℓ)
i ) is the balanced-cut loss for graph Gi at level ℓas deﬁned in Sec. 3.3."
MULTIRESOLUTION VAES,0.29949238578680204,Under review as a conference paper at ICLR 2022
MULTIRESOLUTION VAES,0.3020304568527919,"Figure 3: MGVAE generates molecules on QM9 (4 on the left) and ZINC (the rest) equivariantly.
There are many more examples of generated molecules in Sec. D.2. Both equivariant MGVAE and
autoregressive MGN generate high-quality molecules with complicated structures such as rings."
MULTIRESOLUTION VAES,0.30456852791878175,"Dataset
Method
Training size Input features Validity Novelty Uniqueness QM9"
MULTIRESOLUTION VAES,0.30710659898477155,GraphVAE ∼100K Graph
MULTIRESOLUTION VAES,0.3096446700507614,"61.00%
85.00%
40.90%
CGVAE
100%
94.35%
98.57%
MolGAN
98.1%
94.2%
10.4%
Autoregressive MGN
10K
100%
95.01%
97.44%
All-at-once MGVAE
100%
100%
95.16% ZINC"
MULTIRESOLUTION VAES,0.31218274111675126,GraphVAE
MULTIRESOLUTION VAES,0.3147208121827411,"∼200K
Graph"
MULTIRESOLUTION VAES,0.31725888324873097,"14.00%
100%
31.60%
CGVAE
100%
100%
99.82%
JT-VAE
100%
-
-
Autoregressive MGN
1K
100%
99.89%
99.69%
All-at-once MGVAE
10K
Chemical
99.92%
100%
99.34%"
MULTIRESOLUTION VAES,0.3197969543147208,"Table 1: Molecular graph generation results. GraphVAE results are taken from (Liu et al., 2018)."
EXPERIMENTS,0.3223350253807107,"5
EXPERIMENTS"
EXPERIMENTS,0.3248730964467005,Many more experimental results and details are presented in the Sec. D of the Appendix.
MOLECULAR GRAPH GENERATION,0.32741116751269034,"5.1
MOLECULAR GRAPH GENERATION"
MOLECULAR GRAPH GENERATION,0.3299492385786802,"We examine the generative power of MGN and MGVAE in the challenging task of molecule genera-
tion, in which the graphs are highly structured. We demonstrate that MGVAE is the ﬁrst hierarchical
graph VAE model generating graphs in a permutation-equivariant manner that is competitive against
autoregressive results. We train on two datasets that are standard in the ﬁeld:"
MOLECULAR GRAPH GENERATION,0.33248730964467005,"1. QM9 (Ruddigkeit et al., 2012) (Ramakrishnan et al., 2014): contains 134K organic
molecules with up to nine atoms (C, H, O, N, and F) out of the GDB-17 universe of
molecules.
2. ZINC (Sterling & Irwin, 2015): contains 250K purchasable drug-like chemical compounds
with up to twenty-three heavy atoms."
MOLECULAR GRAPH GENERATION,0.3350253807106599,"We only use the graph features as the input, including the adjacency matrix, the one-hot vector
of atom types (e.g., carbon, hydrogen, etc.) and the bond types (single bond, double bond, etc.)
without any further domain knowledge from chemistry or physics. First, we train autoencoding
task of reconstructing the adjacency matrix and node features. We use a learnable equivariant prior
(see Sec. C) instead of the conventional N(0,1). Then, we generate 5,000 different samples from
the prior, and decode each sample into a generated graph (see Fig. 3). We implement our graph
construction (decoding) in two approaches:"
MOLECULAR GRAPH GENERATION,0.33756345177664976,"1. All-at-once: We reconstruct the whole adjacency matrix by running the probabilistic de-
coder (see Sec. 4). MGVAE enables us to generate a graph at any given resolution level ℓ.
In this particular case, we select the highest resolution ℓ= L. This approach of decoding
preserves equivariance, but is harder to train. On ZINC, we extract several chemical/atomic
features from RDKit as the input for the encoders to reach a good convergence in training.
2. Autoregressive: The graph is constructed iteratively by adding one edge in each iteration,
similarly to (Liu et al., 2018). But this approach does not respect permutation equivariance."
MOLECULAR GRAPH GENERATION,0.3401015228426396,"In our setting for small molecules, L = 3 and K = 2ℓ−1 for the ℓ-th level. We compare our methods
with other graph-based generative models including GraphVAE (Simonovsky & Komodakis, 2018),
CGVAE (Liu et al., 2018), MolGAN (Cao & Kipf, 2018), and JT-VAE (Jin et al., 2018). We evaluate
the quality of generated molecules in three metrics: (i) validity, (ii) novelty and (iii) uniqueness as"
MOLECULAR GRAPH GENERATION,0.3426395939086294,Under review as a conference paper at ICLR 2022
MOLECULAR GRAPH GENERATION,0.34517766497461927,"COMMUNITY-SMALL
EGO-SMALL"
MOLECULAR GRAPH GENERATION,0.3477157360406091,"MODEL
DEGREE
CLUSTER
ORBIT
DEGREE
CLUSTER
ORBIT"
MOLECULAR GRAPH GENERATION,0.350253807106599,"GRAPHVAE
0.35
0.98
0.54
0.13
0.17
0.05
DEEPGMG
0.22
0.95
0.4
0.04
0.10
0.02
GRAPHRNN
0.08
0.12
0.04
0.09
0.22
0.003
GNF
0.20
0.20
0.11
0.03
0.10
0.001
GRAPHAF
0.06
0.10
0.015
0.04
0.04
0.008"
MOLECULAR GRAPH GENERATION,0.35279187817258884,"MGVAE
0.002
0.01
0.01
1.74e-05
0.0006
6.53e-05"
MOLECULAR GRAPH GENERATION,0.3553299492385787,"Table 2: Graph generation results depicting MMD for various graph statistics between the test set
and generated graphs. MGVAE outperforms all competing methods."
MOLECULAR GRAPH GENERATION,0.35786802030456855,"the percentage of the generated molecules that are chemically valid, different from all molecules
in the training set, and not redundant, respectively. Because of high complexity, we only train on a
small random subset of examples while all other methods are trained on the full datasets. Our models
are equivalent with the state-of-the-art, even with a limited training set (see Table 1).
Admittedly,
molecule generation is a somewhat subject task that can only be evaluated with objective numerical
measures up to a certain point. Qualitatively, however the molecules that MGVAE generates are as
good as the state of the art, in some cases better in terms of producing several high-quality drug-like
molecules with complicated functional groups and structures. Many further samples generated by
MGVAE and their analysis can be found in the Appendix."
GENERAL GRAPH GENERATION BY MGVAE,0.3604060913705584,"5.2
GENERAL GRAPH GENERATION BY MGVAE"
GENERAL GRAPH GENERATION BY MGVAE,0.3629441624365482,"We further examine the expressive power of hierarchical latent structure of MGVAE in the task of
general graph generation. We choose two datasets from GraphRNN paper (You et al., 2018a):"
GENERAL GRAPH GENERATION BY MGVAE,0.36548223350253806,"1. Community-small: A synthetic dataset of 100 2-community graphs where 12 ≤∣V ∣≤20.
2. Ego-small: 200 3-hop ego networks extracted from the Citeseer network (Sen et al., 2008)
where 4 ≤∣V ∣≤18."
GENERAL GRAPH GENERATION BY MGVAE,0.3680203045685279,"The datasets are generated by the scripts from the GraphRNN codebase (You et al., 2018b). We keep
80% of the data for training and the rest for testing. We evaluate our generated graphs by computing
Maximum Mean Discrepancy (MMD) distance between the distributions of graph statistics on the
test set and the generated set as proposed by (You et al., 2018a). The graph statistics are node
degrees, clustering coefﬁcients, and orbit counts. As suggested by (Liu et al., 2019), we execute 15
runs with different random seeds, and we generate 1,024 graphs for each run, then we average the
results over 15 runs. We compare MGVAE against GraphVAE (Simonovsky & Komodakis, 2018),
DeepGMG (Li et al., 2018), GraphRNN (You et al., 2018a), GNF (Liu et al., 2019), and GraphAF
(Shi et al., 2020). The baselines are taken from GNF paper (Liu et al., 2019) and GraphAF paper (Shi
et al., 2020). In our setting of (all-at-once) MGVAE, we implement only L = 2 levels of resolution
and K = 2ℓclusters for each level. Our encoders have 10 layers of message passing. Instead of
using a high order equivariant network as the global decoder for the bottom resolution, we only
implement a simple fully connected network that maps the latent Z(L) ∈R∣V∣×dz into an adjacency
matrix of size ∣V∣× ∣V∣. For the ego dataset in particular, we implement the learnable equivariant
prior as in Sec. B and Sec.C. Table 2 includes our quantitative results in comparison with other
methods. MGVAE outperforms all competing methods. Figs. 10 11 show some generated examples
and training examples on the 2-community and ego datasets."
CONCLUSION,0.37055837563451777,"6
CONCLUSION"
CONCLUSION,0.3730964467005076,"We introduced MGVAE built upon MGN, the ﬁrst generative model to learn and generate graphs in
a multiresolution and equivariant manner. The key idea of MGVAE is learning to construct a series
of coarsened graphs along with a hierarchy of latent distributions in the encoding process while
learning to decode each latent into the corresponding coarsened graph at every resolution level.
MGVAE achieves state-of-the-art results from link prediction to molecule and graph generation,
suggesting that accounting for the multiscale structure of graphs is a promising way to make graph
neural networks even more powerful."
CONCLUSION,0.3756345177664975,Under review as a conference paper at ICLR 2022
REFERENCES,0.37817258883248733,REFERENCES
REFERENCES,0.38071065989847713,"Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and Koray Kavukcuoglu.
Interaction networks for learning about objects, relations and physics. In Advances in Neural
Information Processing Systems, volume 29. Curran Associates, Inc., 2016."
REFERENCES,0.383248730964467,"David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. J. Mach. Learn.
Res., 3(null):9931022, March 2003. ISSN 1532-4435."
REFERENCES,0.38578680203045684,"Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs,
2018."
REFERENCES,0.3883248730964467,"Xu Chen, Xiuyuan Cheng, and Stephane Mallat. Unsupervised deep haar scattering on graphs. In
Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014."
REFERENCES,0.39086294416243655,"Xiuyuan Cheng, Xu Chen, and St´ephane Mallat.
Deep haar scattering networks.
CoRR,
abs/1509.09187, 2015."
REFERENCES,0.3934010152284264,"Kai-Yang Chiang, Joyce Jiyoung Whang, and Inderjit S. Dhillon. Scalable clustering of signed
networks using balance normalized cut. In Proceedings of the 21st ACM International Conference
on Information and Knowledge Management, CIKM ’12, pp. 615624, New York, NY, USA, 2012.
Association for Computing Machinery. ISBN 9781450311564. doi: 10.1145/2396761.2396841."
REFERENCES,0.39593908629441626,"Taco S. Cohen and Max Welling. Group equivariant convolutional networks. Proceedings of The
33rd International Conference on Machine Learning, 48:2990–2999, 2016a."
REFERENCES,0.39847715736040606,"Taco S. Cohen and Max Welling. Steerable cnns. ICLR’17, 2016b."
REFERENCES,0.4010152284263959,"Ronald R. Coifman and Mauro Maggioni. Diffusion wavelets. Applied and Computational Har-
monic Analysis, 21(1):53–94, 2006.
ISSN 1063-5203.
doi: https://doi.org/10.1016/j.acha.
2006.04.004.
URL https://www.sciencedirect.com/science/article/pii/
S106352030600056X. Special Issue: Diffusion Maps and Wavelets."
REFERENCES,0.4035532994923858,"Hanjun Dai, Azade Nazi, Yujia Li, Bo Dai, and Dale Schuurmans. Scalable deep generative model-
ing for sparse graphs. In Proceedings of the 37th International Conference on Machine Learning,
volume 119 of Proceedings of Machine Learning Research, pp. 2302–2312. PMLR, 13–18 Jul
2020."
REFERENCES,0.40609137055837563,"Inderjit Dhillon, Yuqiang Guan, and Brian Kulis. A fast kernel-based multilevel algorithm for graph
clustering. In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge
Discovery in Data Mining, KDD ’05, pp. 629634, New York, NY, USA, 2005. Association for
Computing Machinery. ISBN 159593135X. doi: 10.1145/1081870.1081948."
REFERENCES,0.4086294416243655,"Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis. Weighted graph cuts without eigenvectors a
multilevel approach. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(11):
1944–1957, 2007. doi: 10.1109/TPAMI.2007.1115."
REFERENCES,0.41116751269035534,"Adji B. Dieng, Francisco J. R. Ruiz, David M. Blei, and Michalis K. Titsias. Prescribed generative
adversarial networks, 2019."
REFERENCES,0.4137055837563452,"David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular ﬁn-
gerprints. In Advances in Neural Information Processing Systems, volume 28. Curran Associates,
Inc., 2015."
REFERENCES,0.41624365482233505,"Vijay Prakash Dwivedi, Chaitanya K. Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.
Benchmarking graph neural networks. CoRR, abs/2003.00982, 2020."
REFERENCES,0.41878172588832485,"Jack Edmonds and Richard M. Karp. Theoretical improvements in algorithmic efﬁciency for net-
work ﬂow problems. Journal of the ACM (JACM), 1972. doi: 10.1145/321694.321699."
REFERENCES,0.4213197969543147,"Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph
convolutional networks. In Proceedings of the 31st International Conference on Neural Informa-
tion Processing Systems, NIPS’17, pp. 65336542, Red Hook, NY, USA, 2017. Curran Associates
Inc. ISBN 9781510860964."
REFERENCES,0.42385786802030456,Under review as a conference paper at ICLR 2022
REFERENCES,0.4263959390862944,"J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for
quantum chemistry. 70, 2017a."
REFERENCES,0.4289340101522843,"Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1263–1272.
PMLR, 06–11 Aug 2017b."
REFERENCES,0.43147208121827413,"Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and
Statistics, volume 9 of Proceedings of Machine Learning Research, pp. 249–256, Chia Laguna
Resort, Sardinia, Italy, 13–15 May 2010. PMLR."
REFERENCES,0.434010152284264,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, volume 27. Curran Associates, Inc., 2014."
REFERENCES,0.4365482233502538,"E. J. Gumbel.
Statistical theory of extreme values and some practical applications: a series of
lectures. US Govt. Print. Ofﬁce, Number 33, 1954."
REFERENCES,0.43908629441624364,"Rafael Gmez-Bombarelli, Jennifer N. Wei, David Duvenaud, Jos Miguel Hernndez-Lobato, Ben-
jamn Snchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel,
Ryan P. Adams, and Aln Aspuru-Guzik. Automatic chemical design using a data-driven con-
tinuous representation of molecules. ACS Central Science, 4(2):268–276, 2018. doi: 10.1021/
acscentsci.7b00572. PMID: 29532027."
REFERENCES,0.4416243654822335,"David K. Hammond, Pierre Vandergheynst, and Rmi Gribonval.
Wavelets on graphs via spec-
tral graph theory.
Applied and Computational Harmonic Analysis, 30(2):129–150, 2011.
ISSN 1063-5203.
doi: https://doi.org/10.1016/j.acha.2010.04.005.
URL https://www.
sciencedirect.com/science/article/pii/S1063520310000552."
REFERENCES,0.44416243654822335,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017."
REFERENCES,0.4467005076142132,"P. Hohenberg and W. Kohn. Inhomogeneous electron gas. Phys. Rev., 136:864–871, 1964."
REFERENCES,0.44923857868020306,"Truong Son Hy, Shubhendu Trivedi, Horace Pan, Brandon M. Anderson, , and Risi Kondor. Pre-
dicting molecular properties with covariant compositional networks. The Journal of Chemical
Physics, 148, 2018."
REFERENCES,0.4517766497461929,"John Ingraham and Debora Marks. Variational inference for sparse and undirected models. In Pro-
ceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pp. 1607–1616. PMLR, 06–11 Aug 2017."
REFERENCES,0.4543147208121827,"Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
ICLR, 2017."
REFERENCES,0.45685279187817257,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola.
Junction tree variational autoencoder for
molecular graph generation. In Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2323–2332. PMLR,
10–15 Jul 2018."
REFERENCES,0.4593908629441624,"Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph
convolutions: moving beyond ﬁngerprints. Journal of Computer-Aided Molecular Design, 30(8):
595608, Aug 2016. ISSN 1573-4951. doi: 10.1007/s10822-016-9938-8."
REFERENCES,0.4619289340101523,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, 2015."
REFERENCES,0.46446700507614214,"Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014."
REFERENCES,0.467005076142132,Under review as a conference paper at ICLR 2022
REFERENCES,0.46954314720812185,"Thomas N. Kipf and Max Welling. Variational graph auto-encoders, 2016."
REFERENCES,0.4720812182741117,"Alexej Klushyn, Nutan Chen, Richard Kurle, Botond Cseke, and Patrick van der Smagt. Learning
hierarchical priors in vaes. In Advances in Neural Information Processing Systems, volume 32.
Curran Associates, Inc., 2019."
REFERENCES,0.4746192893401015,"Daphne Koller and Nir Friedman. In Probabilistic Graphical Models: Principles and Techniques.
MIT Press, 2009."
REFERENCES,0.47715736040609136,"Risi Kondor, Truong Son Hy, Horace Pan, Brandon M. Anderson, and Shubhendu Trivedi. Covariant
compositional networks for learning graphs, 2018."
REFERENCES,0.4796954314720812,"Nils M. Kriege, Pierre-Louis Giscard, and Richard C. Wilson. On valid optimal assignment kernels
and applications to graph classiﬁcation. In Proceedings of the 30th International Conference on
Neural Information Processing Systems, NIPS’16, pp. 16231631, Red Hook, NY, USA, 2016.
Curran Associates Inc. ISBN 9781510838819."
REFERENCES,0.48223350253807107,"Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The mnist database of handwritten
digits. URL http://yann.lecun.com/exdb/mnist/."
REFERENCES,0.4847715736040609,"Yujia Li, Richard Zemel, Marc Brockschmidt, and Daniel Tarlow. Gated graph sequence neural
networks. In Proceedings of ICLR’16, April 2016."
REFERENCES,0.4873096446700508,"Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter W. Battaglia. Learning deep gener-
ative models of graphs. ICML’18, abs/1803.03324, 2018."
REFERENCES,0.48984771573604063,"Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel
Urtasun, and Richard Zemel. Efﬁcient graph generation with graph recurrent attention networks.
In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.49238578680203043,"Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples in
generative adversarial networks. In Advances in Neural Information Processing Systems, vol-
ume 31. Curran Associates, Inc., 2018."
REFERENCES,0.4949238578680203,"Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing ﬂows. In
Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.49746192893401014,"Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander Gaunt. Constrained graph varia-
tional autoencoders for molecule design. In Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018."
REFERENCES,0.5,"Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. Disentangled graph convolutional
networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 4212–4221. PMLR, 09–15 Jun 2019."
REFERENCES,0.5025380710659898,"Chris J Maddison, Daniel Tarlow, and Tom Minka. A* sampling. In Advances in Neural Information
Processing Systems, volume 27. Curran Associates, Inc., 2014."
REFERENCES,0.5050761421319797,"Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. In Advances in Neural Information Processing Systems, volume 32. Curran Associates,
Inc., 2019a."
REFERENCES,0.5076142131979695,"Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. In International Conference on Learning Representations, 2019b."
REFERENCES,0.5101522842639594,"Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant
networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 4363–4371. PMLR, 09–15 Jun 2019c."
REFERENCES,0.5126903553299492,"Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements.
In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Pro-
ceedings of Machine Learning Research, pp. 6734–6744. PMLR, 13–18 Jul 2020."
REFERENCES,0.5152284263959391,Under review as a conference paper at ICLR 2022
REFERENCES,0.5177664974619289,"F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. Geometric deep learn-
ing on graphs and manifolds using mixture model cnns. In 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 5425–5434, Los Alamitos, CA, USA, jul 2017. IEEE
Computer Society. doi: 10.1109/CVPR.2017.576."
REFERENCES,0.5203045685279187,"Kevin P. Murphy. Chapter 19: Undirected graphical models (markov random ﬁelds). In Machine
Learning: A Probabilistic Perspective, pp. 663–707. MIT Press, 2012."
REFERENCES,0.5228426395939086,"M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In
Proceedings of the International Conference on Machine Learning, 2016a."
REFERENCES,0.5253807106598984,"Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-
works for graphs. In Proceedings of The 33rd International Conference on Machine Learning,
volume 48 of Proceedings of Machine Learning Research, pp. 2014–2023, New York, New York,
USA, 20–22 Jun 2016b. PMLR."
REFERENCES,0.5279187817258884,"B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. Pro-
ceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (KDD), pp. 701–710, 2014."
REFERENCES,0.5304568527918782,"Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks, 2016."
REFERENCES,0.5329949238578681,"Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld. Quantum
chemistry structures and properties of 134 kilo molecules. Scientiﬁc Data, 1, 2014."
REFERENCES,0.5355329949238579,"Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In Proceedings of
The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pp. 324–333, New York, New York, USA, 20–22 Jun 2016. PMLR."
REFERENCES,0.5380710659898477,"Lars Ruddigkeit, Ruud van Deursen, Lorenz C. Blum, and Jean-Louis Reymond.
Enumeration
of 166 billion organic small molecules in the chemical universe database gdb-17. Journal of
Chemical Information and Modeling, 52(11):2864–2875, 2012. doi: 10.1021/ci300415d. PMID:
23088335."
REFERENCES,0.5406091370558376,"Havard Rue and Leonhard Held. Gaussian markov random ﬁelds: Theory and applications. In
Monographs on Statistics and Applied Probability, volume 104, London, 2005. Chapman & Hall."
REFERENCES,0.5431472081218274,"Raif Rustamov and Leonidas J Guibas. Wavelets on graphs via deep learning. In Advances in Neural
Information Processing Systems, volume 26. Curran Associates, Inc., 2013."
REFERENCES,0.5456852791878173,"Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009.
doi: 10.1109/TNN.2008.2005605."
REFERENCES,0.5482233502538071,"Maximilian Seitzer. pytorch-ﬁd: FID Score for PyTorch. https://github.com/mseitzer/
pytorch-fid, August 2020. Version 0.1.1."
REFERENCES,0.550761421319797,"P. Sen, G. M. Namata, M. Bilgic, L. Getoor, B. Gallagher, , and T. Eliassi-Rad. Collective classiﬁ-
cation in network data. AI Magazine, 29(3):93–106, 2008."
REFERENCES,0.5532994923857868,"Hadar Serviansky, Nimrod Segol, Jonathan Shlomi, Kyle Cranmer, Eilam Gross, Haggai Maron,
and Yaron Lipman. Set2graph: Learning graphs from sets. In Advances in Neural Information
Processing Systems, volume 33, pp. 22080–22091. Curran Associates, Inc., 2020."
REFERENCES,0.5558375634517766,"Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M.
Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(77):
2539–2561, 2011."
REFERENCES,0.5583756345177665,"Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf:
a ﬂow-based autoregressive model for molecular graph generation.
In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
S1esMkHYPr."
REFERENCES,0.5609137055837563,Under review as a conference paper at ICLR 2022
REFERENCES,0.5634517766497462,"Donghyuk Shin, Si Si, and Inderjit S. Dhillon.
Multi-scale link prediction.
In Proceedings of
the 21st ACM International Conference on Information and Knowledge Management, CIKM
’12, pp. 215224, New York, NY, USA, 2012. Association for Computing Machinery.
ISBN
9781450311564. doi: 10.1145/2396761.2396792."
REFERENCES,0.565989847715736,"Si Si, Donghyuk Shin, Inderjit S Dhillon, and Beresford N Parlett. Multi-scale spectral decompo-
sition of massive graphs. In Advances in Neural Information Processing Systems, volume 27.
Curran Associates, Inc., 2014."
REFERENCES,0.5685279187817259,"Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using
variational autoencoders. CoRR, abs/1802.03480, 2018."
REFERENCES,0.5710659898477157,"Akash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, and Charles Sutton. Vee-
gan: Reducing mode collapse in gans using implicit variational learning. In Advances in Neural
Information Processing Systems, volume 30. Curran Associates, Inc., 2017."
REFERENCES,0.5736040609137056,"Teague Sterling and John J. Irwin. Zinc 15 ligand discovery for everyone. Journal of Chemical
Information and Modeling, 55(11):2324–2337, 2015. doi: 10.1021/acs.jcim.5b00559. PMID:
26479676."
REFERENCES,0.5761421319796954,"L. Tang and H. Liu. Leveraging social media networks for classiﬁcation. Data Mining and Knowl-
edge Discovery, 23(3):447–478, 2011."
REFERENCES,0.5786802030456852,"Erik Henning Thiede, Truong Son Hy, and Risi Kondor. The general theory of permutation equiv-
arant neural networks and higher order graph variational encoders. CoRR, abs/2004.03990, 2020."
REFERENCES,0.5812182741116751,"Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. In Advances in
Neural Information Processing Systems, volume 33, pp. 19667–19679. Curran Associates, Inc.,
2020."
REFERENCES,0.583756345177665,"Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li, and Yoshua Ben-
gio. Graph attention networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.5862944162436549,"Martin J. Wainwright and Michael I. Jordan. A variational principle for graphical models. New
Directions in Statistical Signal Processing, 2005."
REFERENCES,0.5888324873096447,"Hao Wu, Jonas K¨ohler, and Frank Noe.
Stochastic normalizing ﬂows.
In Advances in Neural
Information Processing Systems, volume 33, pp. 5933–5944. Curran Associates, Inc., 2020."
REFERENCES,0.5913705583756346,"Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng. Graph wavelet neural network.
In International Conference on Learning Representations, 2019."
REFERENCES,0.5939086294416244,"Yiding Yang, Zunlei Feng, Mingli Song, and Xinchao Wang.
Factorizable graph convolutional
networks. In Advances in Neural Information Processing Systems, volume 33, pp. 20286–20296.
Curran Associates, Inc., 2020."
REFERENCES,0.5964467005076142,"Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hi-
erarchical graph representation learning with differentiable pooling. In Advances in Neural Infor-
mation Processing Systems, volume 31. Curran Associates, Inc., 2018."
REFERENCES,0.5989847715736041,"Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. GraphRNN: Generat-
ing realistic graphs with deep auto-regressive models. In Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp.
5708–5717. PMLR, 10–15 Jul 2018a."
REFERENCES,0.6015228426395939,"Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure Leskovec. Code for graphrnn:
Generating realistic graphs with deep auto-regressive model. 2018b."
REFERENCES,0.6040609137055838,"Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, vol-
ume 30. Curran Associates, Inc., 2017."
REFERENCES,0.6065989847715736,"Dawei Zhou, Lecheng Zheng, Jiejun Xu, and Jingrui He. Misc-gan: A multi-scale generative model
for graphs. Frontiers in Big Data, 2:3, 2019. ISSN 2624-909X. doi: 10.3389/fdata.2019.00003.
URL https://www.frontiersin.org/article/10.3389/fdata.2019.00003."
REFERENCES,0.6091370558375635,Under review as a conference paper at ICLR 2022
REFERENCES,0.6116751269035533,"A
BASIC TENSOR OPERATIONS"
REFERENCES,0.6142131979695431,"In order to build higher order equivariant networks, we revisit some basic tensor operations: tensor
product (see Def. 10) and tensor contraction (see Def. 11). It can be shown that these tensor opera-
tions respect permutation equivariance. Based on them, we build our second order message passing
networks.
Deﬁnition 10. The tensor product of A ∈Rna with B ∈Rnb yields a tensor C = A ⊗B ∈Rna+b"
REFERENCES,0.616751269035533,"where
Ci1,i2,..,ia+b = Ai1,i2,..,iaBia+1,ia+2,..,ia+b
Deﬁnition 11. The contraction of A ∈Rna along the pair of dimensions {x,y} (assuming x < y)
yields a (a −2)-th order tensor"
REFERENCES,0.6192893401015228,"Ci1,..,ix−1,j,ix+1,..,iy−1,j,iy+1,..,ia = ∑
ix,iy
Ai1,..,ia"
REFERENCES,0.6218274111675127,"where we assume that ix and iy have been removed from amongst the indices of C. Using Einstein
notation, this can be written more compactly as"
REFERENCES,0.6243654822335025,"C{i1,i2,..,ia}∖{ix,iy} = Ai1,i2,..,iaδix,iy"
REFERENCES,0.6269035532994924,"where δ is the Kronecker delta. In general, the contraction of A along dimensions {x1,..,xp} yields
a tensor C = A↓x1,..,xp ∈Rna−p where"
REFERENCES,0.6294416243654822,"A↓x1,..,xp = ∑
ix1
∑
ix2
... ∑
ixp
Ai1,i2,..,ia"
REFERENCES,0.631979695431472,"or compactly as
A↓x1,..,xp = Ai1,i2,..,iaδix1,ix2,..,ixp."
REFERENCES,0.6345177664974619,"B
MARKOV RANDOM FIELDS"
REFERENCES,0.6370558375634517,"Undirected graphical models have been widely applied in the domains spatial or relational data,
such as image analysis and spatial statistics. In general, k-th order graph encoders encode an undi-
rected graph G = (V,E) into a k-th order latent z ∈Rnk×dz, with learnable parameters θ, can be
represented as a parameterized Markov Random Field (MRF) or Markov network. Based on the
Hammersley-Clifford theorem (Murphy, 2012) (Koller & Friedman, 2009), a positive distribution
p(z) > 0 satisﬁes the conditional independent properties of an undirected graph G iff p can be
represented as a product of potential functions ψ, one per maximal clique, i.e.,"
REFERENCES,0.6395939086294417,"p(z∣θ) =
1
Z(θ) ∏
c∈C
ψc(zc∣θc)
(10)"
REFERENCES,0.6421319796954315,"where C is the set of all the (maximal) cliques of G, and Z(θ) is the partition function to ensure the
overall distribution sums to 1, and given by"
REFERENCES,0.6446700507614214,"Z(θ) = ∑
z
∏
c∈C
ψc(zc∣θc)"
REFERENCES,0.6472081218274112,Eq. 10 can be further written down as
REFERENCES,0.649746192893401,"p(z∣θ) ∝∏
v∈V
ψv(zv∣θ) ∏
(s,t)∈E
ψst(zst∣θ) ⋅⋅⋅
∏
c=(i1,..,ik)∈Ck
ψc(zc∣θ)"
REFERENCES,0.6522842639593909,"where ψv, ψst, and ψc are the ﬁrst order, second order and k-th order outputs of the encoder, cor-
responding to every vertex in V, every edge in E and every clique of size k in Ck, respectively.
However, factorizing a graph into set of maximal cliques has an exponential time complexity, since
the problem of determining if there is a clique of size k in a graph is known as an NP-complete
problem. Thus, the factorization based on Hammersley-Clifford theorem is intractable. The second
order encoder relaxes the restriction of maximal clique into edges, that is called as pairwise MRF:"
REFERENCES,0.6548223350253807,"p(z∣θ) ∝∏
s∼t
ψst(zs,zt)"
REFERENCES,0.6573604060913706,Under review as a conference paper at ICLR 2022
REFERENCES,0.6598984771573604,"Our second order encoder inherits Gaussian MRF introduced by (Rue & Held, 2005) as pairwise
MRF of the following form"
REFERENCES,0.6624365482233503,"p(z∣θ) ∝∏
s∼t
ψst(zs,zt)∏
t
ψt(zt)"
REFERENCES,0.6649746192893401,"where ψst(zs,zt) = exp( −1"
REFERENCES,0.6675126903553299,"2zsΛstzt) is the edge potential, and ψt(zt) = exp( −1"
REFERENCES,0.6700507614213198,"2Λttz2
t + ηtzt)
is the vertex potental. The joint distribution can be written in the information form of a multivariate
Gaussian in which
Λ = Σ−1"
REFERENCES,0.6725888324873096,η = Λµ
REFERENCES,0.6751269035532995,p(z∣θ) ∝exp(ηT z −1
REFERENCES,0.6776649746192893,"2zT Λz)
(11)"
REFERENCES,0.6802030456852792,"Sampling z from p(z∣θ) in Eq. 11 is the same as sampling from the multivariate Gaussian N(µ,Σ).
To ensure end-to-end equivariance, we set the latent layer to be two tensors µ ∈Rn×dz and Σ ∈
Rn×n×dz that corresponds to dz multivariate Gaussians, whose ﬁrst index, and second index are
ﬁrst order and second order equivariant with permutations. Computation of Σ is trickier than µ,
simply because Σ must be invertible to be a covariance matrix. Thus, our second order encoder
produces tensor L as the second order activation, and set Σ = LLT . The reparameterization trick
from Kingma & Welling (2014) is changed to"
REFERENCES,0.682741116751269,"z = µ + Lϵ,
ϵ ∼N(0,1)"
REFERENCES,0.6852791878172588,"C
EQUIVARIANT LEARNABLE PRIOR"
REFERENCES,0.6878172588832487,"The original VAE published by Kingma & Welling (2014) limits each covariance matrix Σ to be
diagonal and the prior to be N(0,1). Our second order encoder removes the diagonal restriction on
the covariance matrix. Furthermore, we allow the prior N(ˆµ, ˆΣ) to be learnable in which ˆµ and
ˆΣ are parameters optimized by back propagation in a data driven manner. Importantly, ˆΣ cannot
be learned directly due to the invertibility restriction. Instead, similarly to the second order encoder,
a matrix ˆL is optimized, and the prior covariance matrix is constructed by setting ˆΣ = ˆLˆL
T . The
Kullback-Leibler divergence between the two distributions N(µ,Σ) and N(ˆµ, ˆΣ) is as follows:"
REFERENCES,0.6903553299492385,"DKL(N(µ,Σ)∣∣N(ˆµ, ˆΣ)) = 1"
REFERENCES,0.6928934010152284,"2(tr(ˆΣ
−1Σ) + (ˆµ −µ)T ˆΣ
−1(ˆµ −µ) −n + ln(det ˆΣ"
REFERENCES,0.6954314720812182,"detΣ))
(12)"
REFERENCES,0.6979695431472082,"Even though Σ is invertible, but gradient computation through the KL-divergence loss can be nu-
merical instable because of Cholesky decomposition procedure in matrix inversion. Thus, we add
neglectable noise ϵ = 10−4 to the diagonal of both covariance matrices."
REFERENCES,0.700507614213198,"Importantly, during training, the KL-divergence loss breaks the permutation equivariance. Suppose
the set of vertices are permuted by a permutation matrix P σ for σ ∈Sn. Since µ and Σ are the ﬁrst
order and second order equivariant outputs of the encoder, they are changed to P σµ and P σΣP T
σ
accordingly. But"
REFERENCES,0.7030456852791879,"DKL(N(µ,Σ)∣∣N(ˆµ, ˆΣ)) ≠DKL(N(P σµ,P σΣP T
σ )∣∣N(ˆµ, ˆΣ))"
REFERENCES,0.7055837563451777,"To address the equivariance issue, we want to solve the following convex optimization problem that
is our new equivariant loss function"
REFERENCES,0.7081218274111675,"min
σ∈Sn
DKL(N(P σµ,P σΣP T
σ )∣∣N(ˆµ, ˆΣ))
(13)"
REFERENCES,0.7106598984771574,"However, solving the optimization based on Eq. 13 is computationally expensive. One solution
is to solve the minimum-cost maximum-matching in a bipartite graph (Hungarian matching) with
the cost matrix Cij = ∣∣µi −ˆµj∣∣by O(n4) algorithm published by Edmonds & Karp (1972), that
can be still improved further into O(n3). The Hungarian matching preserves equiariance, but is
still computationally expensive. In practice, instead of ﬁnding a optimal permutation, we apply
a free-matching scheme to ﬁnd an assignment matrix Π such that: Πij∗= 1 if and only if j∗=
arg minj ∣∣µi −ˆµj∣∣, for each i ∈[1,n]. The free-matching scheme preserves equivariance and can
be done efﬁciently in a simple O(n2) algorithm that is also suitable for GPU computation."
REFERENCES,0.7131979695431472,Under review as a conference paper at ICLR 2022
REFERENCES,0.7157360406091371,"D
EXPERIMENTS"
REFERENCES,0.7182741116751269,"D.1
LINK PREDICTION ON CITATION GRAPHS"
REFERENCES,0.7208121827411168,"We demonstrate the ability of the MGVAE models to learn meaningful latent embeddings on a link
prediction task on popular citation network datasets Cora and Citeseer (Sen et al., 2008). At training
time, 15% of the citation links (edges) were removed while all node features are kept, the models are
trained on an incomplete graph Laplacian constructed from the remaining 85% of the edges. From
previously removed edges, we sample the same number of pairs of unconnected nodes (non-edges).
We form the validation and test sets that contain 5% and 10% of edges with an equal amount of
non-edges, respectively."
REFERENCES,0.7233502538071066,We compare our model MGVAE against popular methods in the ﬁeld:
REFERENCES,0.7258883248730964,"1. Spectral clustering (SC) (Tang & Liu, 2011)
2. Deep walks (DW) (Perozzi et al., 2014)
3. Variational graph autoencoder (VGAE) (Kipf & Welling, 2016)"
REFERENCES,0.7284263959390863,"on the ability to correctly classify edges and non-edges using two metrics: area under the ROC curve
(AUC) and average precision (AP). Numerical results of SC and DW are experimental settings are
taken from (Kipf & Welling, 2016). We reran the implementation of VGAE as in (Kipf & Welling,
2016)."
REFERENCES,0.7309644670050761,"For MGVAE, we initialize weights by Glorot initialization (Glorot & Bengio, 2010). We repeat the
experiments with 5 different random seeds and calculate the average AUC and AP along with their
standard deviations. The number of message passing layers ranges from 1 to 4. The size of latent
representation is 128. The number of coarsening levels is L ∈{3,7}. In the ℓ-th coarsening level, we
partition the graph G(ℓ) into 2ℓ(for L = 7) or 4ℓ(for L = 3) clusters. We train for 2,048 epochs using
Adam optimization (Kingma & Ba, 2015) with a starting learning rate of 0.01. Hyperparameters
optimization (e.g. number of layers, dimension of the latent representation, etc.) is done on the
validation set. MGVAE outperforms all other methods (see Table 3)."
REFERENCES,0.733502538071066,"We propose our learning to cluster algorithm to achieve the balanced K-cut at every resolution level.
Besides, we also implement two ﬁxed clustering algorithms:"
REFERENCES,0.7360406091370558,"1. Spectral: It is similar to the one implemented in (Rustamov & Guibas, 2013)."
REFERENCES,0.7385786802030457,"• First, we embed each node i ∈V into Rnmax as (ξ1(i)/λ1(i),..,ξnmax(i)/λnmax(i)),
where {λn,ξn}nmax
n=0
are the eigen-pairs of the graph Laplacian L = D−1(D−A) where
Dii = ∑j Aij. We assume that λ0 ≤.. ≤λnmax. In this case, nmax = 10.
• At the ℓ-th resolution level, we apply the K-Means clustering algorithm based on the
above node embedding to partition graph G(ℓ).
2. K-Means:"
REFERENCES,0.7411167512690355,"• First, we apply PCA to compress the sparse word frequency vectors (of size 1,433 on
Cora and 3,703 on Citeseer) associating with each node into 10 dimensions.
• We use the compressed node embedding for the K-Means clustering."
REFERENCES,0.7436548223350253,"Tables 4 and 5 show that our learning to cluster algorithm returns a much more balanced cut on
the highest resolution level comparing to both Spectral and K-Means clusterings. For instance, we"
REFERENCES,0.7461928934010152,Table 3: Citation graph link prediction results (AUC & AP)
REFERENCES,0.748730964467005,"Dataset
Cora
Citeseer
Method
AUC (ROC)
AP
AUC (ROC)
AP
SC
84.6 ± 0.01
88.5 ± 0.00
80.5 ± 0.01
85.0 ± 0.01
DW
83.1 ± 0.01
85.0 ± 0.00
80.5 ± 0.02
83.6 ± 0.01
VGAE
90.97 ± 0.77
91.88 ± 0.83
89.63 ± 1.04
91.10 ± 1.02
MGVAE (Spectral)
91.19 ± 0.76
92.27 ± 0.73
90.55 ± 1.17
91.89 ± 1.27
MGVAE (K-Means)
93.07 ± 5.61
92.49 ± 5.77
90.81 ± 1.19
91.98 ± 1.02
MGVAE
95.67 ± 3.11
95.02 ± 3.36
93.93 ± 5.87
93.06 ± 6.33"
REFERENCES,0.751269035532995,Under review as a conference paper at ICLR 2022
REFERENCES,0.7538071065989848,"Method
Min
Max
STD
KL divergence
Spectral
1
2020
177.52
3.14
K-Means
1
364
40.17
0.84
Learn to cluster
10
36
4.77
0.02"
REFERENCES,0.7563451776649747,Table 4: Learning to cluster algorithm returns balanced cuts on Cora.
REFERENCES,0.7588832487309645,"Method
Min
Max
STD
KL divergence
Spectral
1
3320
292.21
4.51
K-Means
1
326
41.69
0.74
Learn to cluster
11
38
4.93
0.01"
REFERENCES,0.7614213197969543,Table 5: Learning to cluster algorithm returns balanced cuts on Citeseer.
REFERENCES,0.7639593908629442,"have L = 7 resolution levels and we partition the ℓ-th resolution into K = 2ℓclusters. Thus, on the
bottom levels, we have 128 clusters. If we distribute nodes into clusters uniformly, the expected
number of nodes in a cluster is 21.15 and 25.99 on Cora (2,708 nodes) and Citeseer (3,327 nodes),
respectively. We measure the minimum, maximum, standard deviation of the numbers of nodes in
128 clusters. Furthermore, we measure the Kullback–Leibler divergence between the distribution of
nodes into clusters and the uniform distribution. Our learning to cluster algorithm achieves low KL
losses of 0.02 and 0.01 on Cora and Citeseer, respectively."
REFERENCES,0.766497461928934,"D.2
MOLECULAR GRAPH GENERATION"
REFERENCES,0.7690355329949239,"In this case, MGVAE and MGN are implemented with L = 3 resolution levels, and the ℓ-th resolution
graph is partitioned into K = 2ℓ−1 clusters. On each resolution level, the local encoders and local
decoders are second-order Sn-equivariant networks with up to 4 equivariant layers. The number of
channels for each node latent dz is set to 256. We apply two approaches for graph decoding:"
REFERENCES,0.7715736040609137,"1. All-at-once: MGVAE reconstructs all resolution adjacencies by equivariant decoder net-
works. Furthermore, we apply learnable equivariant prior as in Sec. C. Our second order
encoders are interpreted as Markov Random Fields (see Sec. B). This approach preserves
permutation equivariance. In addition, we implement a correcting process: the decoder
network of the highest resolution level returns a probability for each edge, we sort these
probabilities in a descending order and gradually add the edges in that order to satisfy all
chemical constraints. Furthermore, we investigate the expressive power of the second order
Sn-equivariant decoder by replacing it by a multilayer perceptron (MLP) decoder with 2
hidden layers of size 512 and sigmoid nonlinearity. We ﬁnd that the higher order decoder
outperforms the MLP decoder given the same encoding architecture. Table 6 shows the
comparison between the two decoding models.
2. Autoregressive: This decoding process is constructed in an autoregressive manner simi-
larly to (Liu et al., 2018). First, we sample each vertex latent z independently. We randomly
select a starting node v0, then we apply Breath First Search (BFS) to determine a particular
node ordering from the node v0, however that breaks the permutation equivariance. Then
iteratively we add/sample new edge to the existing graph Gt at the t-th iteration (given a
randomly selected node v0 as the start graph G0) until completion. We apply second-order
MGN with gated recurrent architecture to produce the probability of edge (u,v) where one
vertex u is in the existing graph Gt and the another one is outside; and also the probability
of its label. Intuitively, the decoding process is a sequential classiﬁcation."
REFERENCES,0.7741116751269036,"We randomly select 10,000 training examples for QM9; and 1,000 (autoregressive) and 10,000 (all-
at-once) training examples for ZINC. It is important to note that our training sets are much smaller
comparing to other methods. For all of our generation experiments, we only use graph features as
the input for the encoder such as one-hot atomic types and bond types. Since ZINC molecules are
larger then QM9 ones, it is more difﬁcult to train with the second order Sn-equivariant decoders
(e.g., the number of bond/non-bond predictions or the number of entries in the adjacency matrices
are proportional to squared number of nodes). Therefore, we input several chemical/atomic features"
REFERENCES,0.7766497461928934,Under review as a conference paper at ICLR 2022
REFERENCES,0.7791878172588832,"Dataset
Method
Validity
Novelty
Uniqueness"
REFERENCES,0.7817258883248731,"QM9
MLP decoder
100%
99.98%
77.62%
Sn decoder
100%
100%
95.16%"
REFERENCES,0.7842639593908629,Table 6: All-at-once MGVAE with MLP decoder vs. second order decoder.
REFERENCES,0.7868020304568528,"Feature
Type
Number
Description
GetAtomicNum
Integer
1
Atomic number
IsInRing
Boolean
1
Belongs to a ring?
IsInRingSize
Boolean
9
Belongs to a ring of size k ∈{1, .., 9}?
GetIsAromatic
Boolean
1
Aromaticity?
GetDegree
Integer
1
Vertex degree
GetExplicitValance
Integer
1
Explicit valance
GetFormalCharge
Integer
1
Formal charge
GetIsotope
Integer
1
Isotope
GetMass
Double
1
Atomic mass
GetNoImplicit
Boolean
1
Allowed to have implicit Hs?
GetNumExplicitHs
Integer
1
Number of explicit Hs
GetNumImplicitHs
Integer
1
Number of implicit Hs
GetNumRadicalElectrons
Integer
1
Number of radical electrons
GetTotalDegree
Integer
1
Total degree
GetTotalNumHs
Integer
1
Total number of Hs
GetTotalValence
Integer
1
Total valance"
REFERENCES,0.7893401015228426,"Table 7: The list of chemical/atomic features used for the all-at-once MGVAE on ZINC. We denote
each feature by its API in RDKit."
REFERENCES,0.7918781725888325,"computed from RDKit for the all-at-once MGVAE on ZINC (see Table 7). We concatenate all these
features into a vector of size 24 for each atom."
REFERENCES,0.7944162436548223,"We train our models with Adam optimization method (Kingma & Ba, 2015) with the initial learning
rate of 10−3. Figs. 4 and 5 show some selected examples out of 5,000 generated molecules on QM9
by all-at-once MGVAE, while Fig. 6 shows the molecules generated by autoregressive MGN. Qual-
itatively, both the decoding approaches capture similar molecular substructures (bond structures).
Fig. 9 shows an example of interpolation on the latent space on ZINC with the all-at-once MGVAE.
Fig. 7 shows some generated molecules on ZINC by the all-at-once MGVAE. Fig. 8 and table 8
show some generated molecules by the autoregressive MGN on ZINC dataset with high Quantita-
tive Estimate of Drug-Likeness (QED) computed by RDKit and their SMILES strings. On ZINC,
the average QED score of the generated molecules is 0.45 with standard deviation 0.21. On QM9,
the QED score is 0.44 ± 0.07."
REFERENCES,0.7969543147208121,"D.3
GENERAL GRAPH GENERATION BY MGVAE"
REFERENCES,0.799492385786802,"Figs. 10 11 show some generated examples and training examples on the 2-community and ego
datasets, respectively."
REFERENCES,0.8020304568527918,"D.4
UNSUPERVISED MOLECULAR PROPERTIES PREDICTION ON QM9"
REFERENCES,0.8045685279187818,"Density Function Theory (DFT) is the most successful and widely used approach of modern quan-
tum chemistry to compute the electronic structure of matter, and to calculate many properties of
molecular systems with high accuracy (Hohenberg & Kohn, 1964). However, DFT is computa-
tionally expensive (Gilmer et al., 2017a), that leads to the use of machine learning to estimate the
properties of compounds from their chemical structure rather than computing them explicitly with
DFT (Hy et al., 2018). To demonstrate that MGVAE can learn a useful molecular representations
and capture important molecular structures in an unsupervised and variational autoencoding manner,
we extract the highest resolution latents (at ℓ= L) and use them as the molecular representations
for the downstream tasks of predicting DFT’s molecular properties on QM9 including 13 learning"
REFERENCES,0.8071065989847716,Under review as a conference paper at ICLR 2022
REFERENCES,0.8096446700507615,"Figure 4:
Some generated examples on QM9 by the all-at-once MGVAE with second order Sn-
equivariant decoders."
REFERENCES,0.8121827411167513,"Figure 5:
Some generated examples on QM9 by the all-at-once MGVAE with a MLP decoder
instead of the second order Sn-equivariant one. It generates more tree-like structures."
REFERENCES,0.8147208121827412,Under review as a conference paper at ICLR 2022
REFERENCES,0.817258883248731,Figure 6: Some generated examples on QM9 by the autoregressive MGN.
REFERENCES,0.8197969543147208,"Figure 7:
Some generated examples on ZINC by the all-at-once MGVAE with second order Sn-
equivariant decoders. In addition of graph features such as one-hot atomic types, we include several
chemical features computed from RDKit (as in Table 7) as the input for the encoders. A generated
example can contain more than one connected components, each of them is a valid molecule."
REFERENCES,0.8223350253807107,Under review as a conference paper at ICLR 2022
REFERENCES,0.8248730964467005,"QED = 0.711
QED = 0.715
QED = 0.756
QED = 0.751"
REFERENCES,0.8274111675126904,"QED = 0.879
QED = 0.805
QED = 0.742
QED = 0.769"
REFERENCES,0.8299492385786802,"QED = 0.710
QED = 0.790
QED = 0.850
QED = 0.859"
REFERENCES,0.8324873096446701,"QED = 0.730
QED = 0.901
QED = 0.786
QED = 0.729"
REFERENCES,0.8350253807106599,"QED = 0.703
QED = 0.855
QED = 0.895
QED = 0.809"
REFERENCES,0.8375634517766497,"Figure 8: Some generated molecules on ZINC by the autoregressive MGN with high QED (drug-
likeness score)."
REFERENCES,0.8401015228426396,Under review as a conference paper at ICLR 2022
REFERENCES,0.8426395939086294,"Row
Column
SMILES 1"
REFERENCES,0.8451776649746193,"1
O=C1NC(CCCF)c2[nH]nnc21
2
OCC(OSBr)c1ccc(-c2cccc(Cl)c2Cl)[nH]1
3
C=CC1=CC=c2c(cc(=C3ONC(Cl)=C3Cl)[nH]c2=O)O1
4
COC(=CN1NC=CN1)C=C1C=CC(Cl)=CO1 2"
REFERENCES,0.8477157360406091,"1
[NH-]C(CNS1(=O)=NNc2c(F)cccc21)C1CC1
2
CS(=O)N1CC[SH](C)C(CNCc2ccccc2)C1
3
C=C(Cl)[SH](=O)(NC)C1c2ccc(Cl)c(n2)CC1O
4
CC(F)C(=C1C[NH2+]C([O-])N1)S(=O)Cc1ccccc1 3"
REFERENCES,0.850253807106599,"1
CC1(NC2=CONN2c2ccccc2)C=C(C=O)N[N-]1
2
CC(=O)NN1N=C(C(O)c2cccc3ccoc23)C(=O)C1=O
3
C=CC(C)=C1C(F)=CC(C=C2ONN=NS2=O)=C1SCl
4
CCN1ON=C(C=C(Cl)c2ccco2)C(F)(F)C1=O 4"
REFERENCES,0.8527918781725888,"1
O=C(CCN(c1[nH+]cc(S)s1)c1ccc2cc1SC2)C1=NCC=C1Cl
2
CC=CNC1=C2Oc3ccccc3C(C)S2=S=N1
3
O=C(SC1=CC=NS1(=O)=O)c1ccc(Cl)cc1S1=NN=NN=N1
4
COCCNCc1cc2ccccc2[nH]1 5"
REFERENCES,0.8553299492385786,"1
ClC=C1CON=C(c2ncno2)N1CC(Cl)(Br)Br
2
CS(=O)(=O)c1ccc[nH+]c1SNCc1ccccc1Cl
3
O=S1(=O)CNS(=O)(N(c2ccccc2F)c2ccccc2Cl)=N1
4
O=C1NS(c2ccccc2Cl)=S2(=NSN=N2)O1"
REFERENCES,0.8578680203045685,"Table 8:
SMILES of the generated molecules included in Fig. 8. Online drawing tool: https:
//pubchem.ncbi.nlm.nih.gov//edit3/index.html"
REFERENCES,0.8604060913705583,"Figure 9: Interpolation on the latent space: we randomly select two molecules from ZINC and we
reconstruct the corresponding molecular graphs on the interpolation line between the two latents."
REFERENCES,0.8629441624365483,"Figure 10: The top row includes generated examples and the bottom row includes training examples
on the synthetic 2-community dataset."
REFERENCES,0.8654822335025381,Under review as a conference paper at ICLR 2022
REFERENCES,0.868020304568528,Generated examples
REFERENCES,0.8705583756345178,Training examples
REFERENCES,0.8730964467005076,Figure 11: EGO-SMALL.
REFERENCES,0.8756345177664975,Under review as a conference paper at ICLR 2022
REFERENCES,0.8781725888324873,"Target
Unit
Mean
STD
Description
α
bohr3
75.2808
8.1729
Norm of the static polarizability
Cv
cal/mol/K
31.6204
4.0674
Heat capacity at room temperature
G
eV
-70.8352
9.4975
Free energy of atomization
gap
eV
6.8583
1.2841
Difference between HOMO and LUMO
H
eV
-77.0167
10.4884
Enthalpy of atomization at room temperature
HOMO
eV
-6.5362
0.5977
Highest occupied molecular orbital
LUMO
eV
0.3220
1.2748
Lowest unoccupied molecular orbital
µ
D
2.6729
1.5034
Norm of the dipole moment
ω1
cm−1
3504.1155
266.8982
Highest fundamental vibrational frequency
R2
bohr2
1189.4091
280.4725
Electronic spatial extent
U
eV
-76.5789
10.4143
Atomization energy at room temperature
U0
eV
-76.1145
10.3229
Atomization energy at 0 K
ZPVE
eV
4.0568
0.9016
Zero point vibrational energy"
REFERENCES,0.8807106598984772,Table 9: Description and statistics of 13 learning targets on QM9.
REFERENCES,0.883248730964467,"alpha
Cv
G
gap
H
HOMO LUMO
mu
omega1
R2
U
U0
ZPVE
WL
3.75
2.39 4.84 0.92 5.45
0.38
0.89
1.03
192
154 5.41 5.36
0.51
NGF
3.51
1.91 4.36 0.86 4.92
0.34
0.82
0.94
168
137 4.89 4.85
0.45
PSCN
1.63
1.09 3.13 0.77 3.56
0.30
0.75
0.81
152
61
3.54 3.50
0.38
CCN 2D
1.30
0.93 2.75 0.69 3.14
0.23
0.67
0.72
120
53
3.02 2.99
0.35
MGVAE
2.83
0.91 1.78 0.66 1.87
0.34
0.58
0.95
195
90
1.89 1.90
0.14"
REFERENCES,0.8857868020304569,"Table 10: Unsupervised molecular representation learning by MGVAE to predict molecular prop-
erties calculated by DFT on QM9 dataset."
REFERENCES,0.8883248730964467,"targets. For the training, we normalize all learning targets to have mean 0 and standard deviation 1.
The name, physical unit, and statistics of these learning targets are detailed in Table 9."
REFERENCES,0.8908629441624365,"The implementation of MGVAE is the same as detailed in Sec. D.2. MGVAE is trained to reconstruct
the highest resolution (input) adjacency, its coarsening adjacencies and the node atomic features. In
this case, we do not use any chemical features: the node atomic features are just one-hot atomic
types. After MGVAE is converged, to obtain the Sn-invariant molecular representation, we average
the node latents at the L-th level into a vector of size 256. Finally, we apply a simple Multilayer
Perceptron with 2 hidden layers of size 512, sigmoid nonlinearity and a linear layer on top to predict
the molecular properties based on the extracted molecular representation. We compare the results
in Mean Average Error (MAE) in the corresponding physical units with four methods on the same
split of training and testing from (Hy et al., 2018):"
REFERENCES,0.8934010152284264,"1. Support Vector Machine on optimal-assignment Weisfeiler-Lehman (WL) graph kernel
(Shervashidze et al., 2011) (Kriege et al., 2016)"
REFERENCES,0.8959390862944162,"2. Neural Graph Fingerprint (NGF) (Duvenaud et al., 2015)"
REFERENCES,0.8984771573604061,"3. PATCHY-SAN (PSCN) (Niepert et al., 2016a)"
REFERENCES,0.9010152284263959,"4. Second order Sn-equivariant Covariant Compositional Networks (CCN 2D) (Kondor et al.,
2018) (Hy et al., 2018)"
REFERENCES,0.9035532994923858,"Our unsupervised results show that MGVAE is able to learn a universal molecular representation in
an unsupervised manner and outperforms WL in 12, NGF in 10, PSCN in 8, and CCN 2D in 8 out
of 13 learning targets, respectively (see Table 10). There are other recent methods in the ﬁeld that
use several chemical and geometric information but comparing to them would be unfair."
REFERENCES,0.9060913705583756,"D.5
SUPERVISED MOLECULAR PROPERTIES PREDICTION ON ZINC"
REFERENCES,0.9086294416243654,"To further demonstrate the comprehensiveness of MGN, we apply our model in a supervised regres-
sion task to predict the solubility (LogP) on the ZINC dataset. We use the same split of 10K/1K/1K"
REFERENCES,0.9111675126903553,Under review as a conference paper at ICLR 2022
REFERENCES,0.9137055837563451,"Method
MLP
GCN
GAT
MoNet
DiscenGCN
FactorGCN
GatedGCNE
MGN
MAE
0.667
0.503
0.479
0.407
0.538
0.366
0.363
0.290"
REFERENCES,0.916243654822335,Table 11: Supervised MGN to predict solubility on ZINC dataset.
REFERENCES,0.9187817258883249,"for training/validation/testing as in (Dwivedi et al., 2020). The implementation of MGN is almost
the same as detailed in Sec. D.2, except we include the latents of all resolution levels into the pre-
diction. In particular, in each resolution level, we average all the node latents into a vector of size
256; then we concatentate all these vectors into a long vector of size 256×L and apply a linear layer
for the regression task. The baseline results are taken from (Yang et al., 2020) including:"
REFERENCES,0.9213197969543148,"1. Multilayer Perceptron (MLP),"
REFERENCES,0.9238578680203046,"2. Graph Convolution Networks (GCN),"
REFERENCES,0.9263959390862944,"3. Graph Attention Networks (GAT) (Velikovi et al., 2018),"
REFERENCES,0.9289340101522843,"4. MoNet (Monti et al., 2017),"
REFERENCES,0.9314720812182741,"5. Disentangled Graph Convolutional Networks (DisenGCN) (Ma et al., 2019),"
REFERENCES,0.934010152284264,"6. Factorizable Graph Convolutional Networks (FactorGCN) (Yang et al., 2020),"
REFERENCES,0.9365482233502538,"7. GatedGCNE (Dwivedi et al., 2020) that uses additional edge information."
REFERENCES,0.9390862944162437,"Our supervised result shows that MGN outperforms the state-of-the-art models in the ﬁeld with a
margin of 20% (see Table 11)."
REFERENCES,0.9416243654822335,"D.6
GRAPH-BASED IMAGE GENERATION BY MGVAE"
REFERENCES,0.9441624365482234,"In this additional experiment, we apply MGVAE into the task of image generation. Instead of matrix
representation, an image I ∈RH×W is represented by a grid graph of H ⋅W nodes in which each
node represents a pixel, each edge is between two neighboring pixels, and each node feature is the
corresponding pixel’s color (e.g., R1 in gray scale, and R3 in RGB scale). Fig. 12 demonstrates an
exmaple of graph representation for images. Since images have natural spatial clustering, instead of
learning to cluster, we implement a ﬁxed clustering procedure as follows:"
REFERENCES,0.9467005076142132,"• For the ℓ-th resolution level, we divide the grid graph of size H(ℓ) × W (ℓ) into clusters
of size h × w that results into a grid graph of size H(ℓ)"
REFERENCES,0.949238578680203,"h
× W (ℓ)"
REFERENCES,0.9517766497461929,"w , supposingly h and w are
divisible by H(ℓ) and W (ℓ), respectively. Each resolution is associated with an image I(ℓ)"
REFERENCES,0.9543147208121827,that is a zoomed out version of I(ℓ+1).
REFERENCES,0.9568527918781726,"• The global encoder e(ℓ) is implemented with 10 layers of message passing that operates
on the whole H(ℓ) × W (ℓ) grid graph. We sum up all the node latents into a single latent
vector Z(ℓ) ∈Rdz. The global decoder d(ℓ) is implemented by the convolutional neural
network architecture of the generator of DCGAN model (Radford et al., 2016) to map Z(ℓ)"
REFERENCES,0.9593908629441624,"into an approximated image ˆI(ℓ). The Sn-invariant pooler p(ℓ) is a network operating on
each small h × w grid graph to produce the corresponding node feature for the next level
ℓ+ 1. MGVAE is trained to reconstruct all resolution images. Fig. 13 shows an example of
reconstruction at each resolution on a test image of MNIST (after the network converged)."
REFERENCES,0.9619289340101523,"We evaluate our MGVAE architecture on the MNIST dataset (LeCun et al.) with 60,000 training
examples and 10,000 testing examples. The original image size is 28×28. We pad zero pixels to get
the image size of 25 × 25 (e.g., H(5) = W (5) = 32). Each cluster is a small grid graph of size 2 × 2
(e.g., h = w = 2). Accordingly, the image sizes for all resolutions are 32 × 32, 16 × 16, 8 × 8, etc. In
this case, the whole network architecture is a 2-dimensional quadtree. The latent size dz is selected
as 256. We train our model for 256 epochs by Adam optimizer (Kingma & Ba, 2015) with the initial
learning rate 10−3. In the testing process, for the ℓ-th resolution, we sample a random vector of size
dz from prior N(0,1) and use the decoder d(ℓ) to decode the corresponding image. We generate
10,000 examples for each resolution. We compute the Frechet Inception Distance (FID) proposed by
(Heusel et al., 2017) between the testing set and the generated set as the metric to evaluate the quality"
REFERENCES,0.9644670050761421,Under review as a conference paper at ICLR 2022
REFERENCES,0.9670050761421319,"Method
FID↓(32 × 32)
FID↓(16 × 16)
FID↓(8 × 8)
DCGAN
113.129"
REFERENCES,0.9695431472081218,"N/A
N/A
VEEGAN
68.749
PACGAN
58.535
PresGAN
42.019
MGVAE
39.474
64.289
39.038"
REFERENCES,0.9720812182741116,"Table 12:
Quantitative evaluation of the generated set by FID metric for each resolution level on
MNIST. It is important to note that the generation for each resolution is done separately: for the ℓ-th
resolution, we sample a random vector of size dz = 256 from N(0,1), and use the global decoder
d(ℓ) to decode into the corresponding image size. The baselines are taken from (Dieng et al., 2019)."
REFERENCES,0.9746192893401016,"Figure 12:
An image of digit 8 from MNIST (left) and its grid graph representation at 16 × 16
resolution level (right)."
REFERENCES,0.9771573604060914,"of our generated examples. We use the FID implementation from (Seitzer, 2020). We compare our
MGVAE against variants of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014)
including DCGAN (Radford et al., 2016), VEEGAN (Srivastava et al., 2017), PacGAN (Lin et al.,
2018), and PresGAN (Dieng et al., 2019). Table 12 shows our quantitative results in comparison
with other competing generative models. The baseline results are taken from Prescribed Generative
Adversarial Networks paper (Dieng et al., 2019). MGVAE outperforms all the baselines for the
highest resolution generation. Figs. 14 and 15 show some generated examples of the 32 × 32 and
16 × 16 resolution, respectively."
REFERENCES,0.9796954314720813,Under review as a conference paper at ICLR 2022
REFERENCES,0.9822335025380711,"32 × 32
16 × 16
8 × 8"
REFERENCES,0.9847715736040609,Target
REFERENCES,0.9873096446700508,Reconstruction
REFERENCES,0.9898477157360406,Figure 13: An example of reconstruction on each resolution level for a test image in MNIST.
REFERENCES,0.9923857868020305,Figure 14: Generated examples at the highest 32 × 32 resolution level.
REFERENCES,0.9949238578680203,Under review as a conference paper at ICLR 2022
REFERENCES,0.9974619289340102,Figure 15: Generated examples at the 16 × 16 resolution level.
