Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.007575757575757576,"Reinforcement learning (RL), especially deep reinforcement learning,
has achieved impressive performance on different control tasks. Unfor-
tunately, most online reinforcement learning algorithms require a large
number of interactions with the environment to learn a reliable control
policy. This assumption of the availability of repeated interactions with
the environment does not hold for many real-world applications due to
safety concerns, the cost/inconvenience related to interactions, or the
lack of an accurate simulator to enable effective sim2real training. As
a consequence, there has been a surge in research addressing this issue,
including batch reinforcement learning. Batch RL aims to learn a good
control policy from a previously collected dataset. Most existing batch
RL algorithms are designed for a single batch setting and assume that
we have a large number of interaction samples in ﬁxed data sets. These
assumptions limit the use of batch RL algorithms in the real world. We
use transfer learning to address this data efﬁciency challenge. This ap-
proach is evaluated on multiple continuous control tasks against several
robust baselines. Compared with other batch RL algorithms, the meth-
ods described here can be used to deal with more general real-world
scenarios."
INTRODUCTION,0.015151515151515152,"1
INTRODUCTION"
INTRODUCTION,0.022727272727272728,"Reinforcement learning aims to learn an optimal control policy through interactions with the envi-
ronment (Sutton & Barto, 2018). Deep reinforcement learning (Deep RL or DRL) combines neural
networks with reinforcement learning and further enables RL agents to deal with more complex
environments. DRL has achieved impressive successes in different areas including the game of Go
(Silver et al., 2017), Atari games (Mnih et al., 2015), and continuous control tasks (Lillicrap et al.,
2015). However, deploying RL algorithms for real-world problems can be very challenging. Com-
pared with supervised learning algorithms (e.g., classiﬁcation or regression), most reinforcement
learning algorithms need to interact with the environment many times to learn a reliable control
policy. This process can be very costly or even dangerous for some real-world applications, e.g.,
safety-critical applications. The current success of deep RL algorithms heavily depends on a large
number of interactions with the environment. Thus the practical application of reinforcement learn-
ing algorithms in the real world is critically limited by its poor data efﬁciency and its inﬂexibility of
learning in an ofﬂine fashion."
INTRODUCTION,0.030303030303030304,"Batch reinforcement learning (also known as ofﬂine reinforcement learning) algorithms have been
developed to solve this issue. Batch RL aims to learn a control policy from a previously collected
dataset without further interactions with the environment. Batch reinforcement learning has attracted
a considerable amount of attention due to its potential in dealing with real-world problems. There
have been many efforts in developing batch RL methods: early approaches include ﬁtted Q iteration
method (Ernst et al., 2005) which uses a tree-based model to estimate the state-action value function
(Q function) in an iterative way, the neural ﬁtted Q method (Riedmiller, 2005), which leverages a
multilayer perceptron (MLP) to approximate the Q function. Since these earlier efforts, a number of
batch reinforcement learning algorithms have been developed that further improve the learning per-
formance. These approaches can be generally categorized as Q function based methods (Fujimoto
et al., 2019; Kumar et al., 2020) and imitation learning based methods (Wang et al., 2018; Peng et al.,"
INTRODUCTION,0.03787878787878788,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.045454545454545456,"2019). Examples of Q function based methods include (Fujimoto et al., 2019), in which the authors
constrain updates of Q values to deal with distribution drift, and (Kumar et al., 2020), where an
additional penalty term was added to constrain the update for the Q function. Best Action Imitation
Learning (BAIL) (Chen et al., 2019) , on the other hand, leveraging the so-called upper envelope,
directly select a sub-batch of the dataset and then execute imitation learning on the selected data to
learn the control policy."
INTRODUCTION,0.05303030303030303,"Even with these advances, most existing batch RL algorithms assume that we have a large number of
data points in the batch. In the real world, this may be an unrealistic assumption. For example, when
learning energy management control policies, we may only have a very limited amount of collected
data for newly built houses and buildings. It is hard for most current batch RL algorithms to learn a
reliable policy with a limited amount of data points. In this work, we use transfer learning (Torrey
& Shavlik, 2010) to address this issue. Transfer learning aims to use the knowledge from source
domains (domains for which we have a large amount of data) to improve the learning performance
in the target domain (the domain we are interested in, but for which we only have a limited amount
of data). Depending on the manner in which knowledge is reused from the source domains, there
are three main categories of transfer learning: sample transfer, model transfer, and representation
transfer. In this work, we use sample transfer, i.e., we transfer some related data points from the
source tasks to improve the learning performance in the target control task. The proposed algorithm
is referred to as BAIL+ as it is a direct extension of the BAIL algorithm proposed by (Chen et al.,
2019)."
INTRODUCTION,0.06060606060606061,"Multitask learning (MTL) (Caruana, 1997) aims to learn a set of tasks jointly instead of learning
them separately to achieve a better overall learning performance. In general, multitask learning can
help to improve the data efﬁciency via reusing the shared representations and training on data from
multiple sources. Multitask learning has shown its effectiveness on different applications such as
image classiﬁcation (Li et al., 2015; 2018), nature language processing (Collobert & Weston, 2008;
Liu et al., 2019), and speech recognition (Siohan & Rybach, 2015). It has shown to be useful in deep
reinforcement learning ﬁeld in recent years (Li et al., 2019). The main objective is to learn one (or
more) policy that can perform well on multiple tasks. In (Rusu et al., 2015), the authors proposed to
learn a multitask policy from a set of DQN experts that are pre-trained on a set of tasks separately.
Lately, (Hessel et al., 2019) investigated how to balance the learning over multiple tasks with one
deep neural network based policy network. The beneﬁt of representation sharing is also investigated
in (D’Eramo et al., 2019) for learning deep RL based control agent. Most of existing multitask RL
works are based on online RL. There are some preliminary investigations on batch RL. As shown
in (Li et al., 2019), the authors proposed to learn distill pretarined BCQ models into one model.
However, despite a few attempts, how to further improve the task-level generalization for batch RL
is still an open question. In this work, different from previous works, we propose to ﬁrst improve the
learning performance on single tasks and then utilize the policy distillation to combine the learned
policies into one single policy."
INTRODUCTION,0.06818181818181818,"Batch RL algorithms are typically designed to deal with single task scenario (a single batch setting).
In the real world, it is more common to have batches collected from a set of tasks that have similar
Markov Decision Process (MDP) settings. For example, we may have collected datasets from a set
of houses/buildings in the same area. Thus, it will very helpful if one general policy that can be
learned from different bacthes that perform well on these different tasks even including unseen tasks
without further adaption. In (Li et al., 2019), the authors implemented preliminary investigations
on multitask batch reinforcement learning. In our work, to improve the task-level generalization
of the policy learned with batch RL, we further extend BAIL+ for multi-batch settings via policy
distillation. The resulting algorithm is referred to as MBAIL. Speciﬁcally, we aims to learn a general
policy without the need to inferring the task identity which can make the batch RL more applicable
in real world."
INTRODUCTION,0.07575757575757576,"The remainder of this paper is organized as follows. The preliminaries of this work including batch
reinforcement learning, multi-batch reinforcement learning, and BAIL algorithm are presented in
Section II. The details for BAIL+ and MBAIL are presented in Section III. The effectiveness of the
proposed methods are showcased in Section IV. Finally, conclusions and future work are presented
in Section V."
INTRODUCTION,0.08333333333333333,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.09090909090909091,"2
PRELIMINARIES"
PRELIMINARIES,0.09848484848484848,"This section reviews the main technical background and notation used in this paper including rein-
forcement learning, batch reinforcement learning, BAIL, and multi-batch reinforcement learning."
PRELIMINARIES,0.10606060606060606,"Reinforcement Learning
Reinforcement learning (Sutton & Barto, 2018) is a learning paradigm
in which a learning agent aims to learn an optimal control policy by interacting with the environment.
Reinforcement learning has been successfully applied in many areas including transportation (Hay-
dari & Yilmaz, 2020), smart grids (Yang et al., 2020), and recommendation systems (Zou et al.,
2019). Formally, an RL problem is typically formulated as a Markov Decision Process (MDP), i.e.,
a tuple ⟨S, A, p, r, µ, γ⟩, where S is the state space, A is the action space, p : S ⊗A →S is the
state transition function, r : S ⊗A →R is the reward function, µ is the initial state distribution, and
γ is the discount factor. The solution to an RL problem (control policy) is a function: π : S →A.
To obtain this solution, the agent needs to achieve the maximum expected cumulative reward, that
is, the so-called value function. Given a state s ∈S and a policy π, the state value function (a.k.a.,
value function) under policy π is deﬁned by V π(s) = Eπ(R1 + γR2 + · · · γn−1Rn), where Ri
denotes the reward obtained at time step i."
PRELIMINARIES,0.11363636363636363,"Batch Reinforcement Learning
In batch reinforcement learning (batch RL), the goal is to learn
a high performance control policy using an ofﬂine dataset without further interactions with the en-
vironment. The dataset consists of N data points B = {(st, at, rt, s
′
t)|t = 1, .., N}. In general, we
have no prior requests on the policy used to collect the batch B. B can be obtained while training
an RL policy in a episodic fashion or running some other control policy (e.g., rule-based methods)
in the same way. The distribution of states and actions in the dataset can be far from those induced
by the current policy under consideration. This complicates computations related to evaluating and
optimizing behaviors, and this has been the primary consideration of several Batch RL solution
methods. To counter this extrapolation problem, (Fujimoto et al., 2019) proposed an algorithm to
learn policies with soft constrain to lie near the batch, which alleviate the extrapolation problem."
PRELIMINARIES,0.12121212121212122,"BAIL: Best Action Imitation Learning
Best Action Imitation Learning (BAIL) (Chen et al.,
2019) is a simple and computationally efﬁcient imitation learning based batch RL algorithm. The
core concept of BAIL is very simple: ﬁnding actions that can achieve high return for each state s
and then learning a control policy based on these selection state-action pairs. To be more speciﬁc,
for a particular state-action pair (s, a), let G(s, a) denote the return starting in state s and action
a, under the policy π. Denote the optimal value function by V ∗(s). Then if the action a∗satisﬁes
G(s, a∗) = V ∗(s), a∗is an optimal action for state s. The problem now becomes how to obtain
V ∗in a batch setting. Since there is no further interaction with the environment, it is impossible to
ﬁnd V ∗. Therefore we seek to eliminate as many useless state-action pairs in the batch as possible,
to avoid the algorithm inferring bad actions. To do this, we estimate a supremum of the optimal
value function V ∗, which is referred to as the upper envelope. Given φ = (w, b), a neural network
parameterized Vφ : S →R, a regularization weight λ and a dataset D of size m, where Di =
(si, Gi) and Gi is the accumulated return of the state si computed within the given batch, then the
upper envelope function V ∗: S →R is estimated by minimizing the following loss function: min
φ m
X"
PRELIMINARIES,0.12878787878787878,"i=1
[Vφ(si) −Gi]2 + λ||w||2
s.t. Vφ(si) > Gi
where i = 1, 2, · · · m
(1)"
PRELIMINARIES,0.13636363636363635,"Once the upper envelope function Vφ is estimated, the best state-action pairs can be selected from
the batch data B based on the estimated Vφ. One way of selecting such pair is that for a ﬁxed β > 0,
we choose all (si, ai) pairs from the batch data set B such that:"
PRELIMINARIES,0.14393939393939395,"Gi > βVφ(si)
(2)"
PRELIMINARIES,0.15151515151515152,"Typically, one can set β such that p% of the data points are selected, where p is a hyper-parameter.
In this work, we follow the same setting as (Chen et al., 2019), in which β is set to ensure that
approximately 25% of all the data points are selected for each batch."
PRELIMINARIES,0.1590909090909091,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.16666666666666666,"Algorithm 1 BAIL+: Best Action Imitation Learning with Multi-source Sample Transfer
Input: A target task batch Bt and N source task batches B1, B2, · · · , BN and the pre-deﬁned sample
selection ratio threshold ˜α"
PRELIMINARIES,0.17424242424242425,"1: Learn the upper envelope function Vt and the reward function ˆrt for batch Bt.
2: for j = 1, · · · , N do
3:
for d = 1, · · · , M do
4:
Denote the current state action pair by (sj
d, aj
d).
5:
Following equation 5, estimate return of sample (sj
d, aj
d), denote by ˆGj
d.
6:
Compute the sample selection ratio α(sj
d, aj
d) via equation 4.
7:
if α(sj
d, aj
d) > ˜α then
8:
Append (sj
d, aj
d) to dataset Bt
9:
end if
10:
end for
11:
Learn the ﬁnal policy πt on Bt via imitation learning
12: end for
Output: the ﬁnal policy of the target task πt"
PRELIMINARIES,0.18181818181818182,"Multi-Batch Reinforcement Learning
To make batch RL more suitable for real-world applica-
tions, it is desirable that the learned control policy performs well in multiple situations. In this work,
we aim to learn one RL agent from a set of batches sampled from a set of tasks {T1, · · · , TN}. Then
the multi-task (multi-batch) batch reinforcement learning can be formulated as:"
PRELIMINARIES,0.1893939393939394,"arg max
θ
J(θ) = ETi∼p(T )[JTi(πθ)]
(3)"
PRELIMINARIES,0.19696969696969696,"where JTi(πθ) is referred to as the performance of control policy πθ on task i. Here p(T) deﬁnes
the task distribution and for each task i, we have a corresponding dataset consisting of K tuples
B = {(st
i, at
i, rt
i, st′
i )|t = 1, .., K}."
METHODOLOGY,0.20454545454545456,"3
METHODOLOGY"
METHODOLOGY,0.21212121212121213,"In this work, we aim to improve the RL agent’s performance over mutiple tasks given a set datasets
collected from multiple task. We tackle this problem in two stages. We ﬁrst improve the BAIL
algorithm via sample transfer, which results in the BAIL+ algorithm, in which we have one target
task with a set of related source tasks. Furthermore, leveraging policy distillation, we extend BAIL+
to MBAIL (Multi-batch Best Action based Imitation learning with sample transfer) to achieve better
generalization over multiple tasks. Below we will illustrate both of our methods in more details."
METHODOLOGY,0.2196969696969697,"3.1
BAIL+"
METHODOLOGY,0.22727272727272727,"When dealing with a set of related tasks, it is natural to think about how to leverage data across all
the tasks. To achieve this, one straight forward approach is through sample transfer. The core idea
of sample transfer is to utilize samples from numerous source tasks to construct a comprehensive
dataset to improve the learning on the target task. In terms of BAIL, we employ an effective ap-
proach, that is, given a target task Tt, a state action pair (s, a) from any source task and its trajectory
ηs,a = ((s, a), (s1, a1), · · · , (sk, ak)), we deﬁne the sample selection ratio of the state action pair
(s, a) similar to the deﬁnition of BAIL’s sample selection ratio β, i.e."
METHODOLOGY,0.23484848484848486,"α(s, a) =
ˆGt(ηs,a)"
METHODOLOGY,0.24242424242424243,"V t(s)
(4)"
METHODOLOGY,0.25,"Note ˆGt(η) is the estimated return of the source task samples evaluated on the target task and the
same is for V t(s). Then given a selection threshold ˜α, if any state action pair (s, a) has α(s, a) > ˜α,
we will incorporate this pair into our newly selected batch. The motivation for this is that, by
assuming the correct estimation of Gt and V t, we just need to follow the original BAIL routine to
pick the state action pair that induces the best action."
METHODOLOGY,0.25757575757575757,Under review as a conference paper at ICLR 2022
METHODOLOGY,0.26515151515151514,Figure 1: The overview for multi-batch BAIL.
METHODOLOGY,0.2727272727272727,"Then, the imminent problem is to obtain an estimate of the return ˆGt evaluated on the target task. To
solve this, we ﬁrst learn a reward function on the target task ˆrt : S × A →R. Then given a discount
factor γ and a trajectory of state action pairs η = (s1, a1), (s2, a2), · · · , (sM, aM) from any source
task batch, we can obtain its return estimate on the target task, that is:"
METHODOLOGY,0.2803030303030303,"ˆGt(η) = M
X"
METHODOLOGY,0.2878787878787879,"i=1
γi−1ˆrt(si, ai)
(5)"
METHODOLOGY,0.29545454545454547,"Note that we assume all tasks share the same transition function, and the batch is collected via the
same policy, therefore Equation 5 is a reasonable estimation of the return on the target task. Once
the return estimation is complete, we just need to select the samples based on the selection ratio
function α and some threshold ˜α. Similarly to BAIL, we select ˜α such that the top p% of all datasets
from the source tasks. Once the data is selected, we then use a standard supervised learning based
imitation learning method to obtain the ﬁnal BAIL+ policy."
METHODOLOGY,0.30303030303030304,"The pseudo code for BAIL+ is given in Algorithm 1. Assume we have one target task Tt and N
source tasks T1, T2, · · · , TN and their batch dataset Bt and B1, B2, · · · , BN of state action pairs.
These tasks share the same state space, action space and transition functions, but have different
reward functions. Assume |B1| = · · · = |BN| = M. For the sake of simplicity, we also assume the
length of all trajectories is L. Note this assumption can be easily lifted."
METHODOLOGY,0.3106060606060606,"3.2
MULTI-BATCH BAIL+"
METHODOLOGY,0.3181818181818182,"In multitask reinforcement learning, we are often faced with a set of similar tasks and it is desirable
to learn a policy that is able to leverage knowledge from all tasks and obtain a policy that has similar
or better performance across all tasks. Policy distillation Rusu et al. (2015) is a classic multitask
reinforcement learning approach, where the distillation agent aggregates knowledge from all of the
policies and distill them into one consistent policy (Rusu et al., 2015). This distillation process
leverages knowledge from all tasks and thus can potentially further improve policy performance. In
this section, we will introduce our approach for multitask batch reinforcement learning based on the
previous BAIL+ method, we will refer to this method as MBAIL."
METHODOLOGY,0.32575757575757575,"Given a set of policies Π = {πi|i = [1, 2, · · · , N]} and corresponding tasks T1, · · · , TN, we want to
learn a policy π : S →A such that PN
i=1
P"
METHODOLOGY,0.3333333333333333,"s∈Bi d(π(s), πi(s)) is minimized, where d is a distance
measure, and is chosen to be L2 distance. In addition, to help the task identiﬁcation, we incorporate
a task inference module q : S × A × R × S →Rk. The distilled policy and the task inference
module are all parameterized by a neural network. Denote the context tuple c = (s, a, R, s
′), our
algorithm aims to minimize the following loss function:"
METHODOLOGY,0.3409090909090909,"Lπ = 1 N N
X"
METHODOLOGY,0.3484848484848485,"i=1
E
s,ci∼Bi
[(πi(s) −π(s, zi))2 + βKL(q(ci)||N(0, 1))], zi ∼q(ci)
(6)"
METHODOLOGY,0.3560606060606061,"However, this approach is not directly deployable under the batch setting. In (Li et al., 2019), the
authors observed that the task inference module has learned to model the posterior over task identity
as conditionally dependent on only the state-action pairs, but omit the effect of reward, which is"
METHODOLOGY,0.36363636363636365,Under review as a conference paper at ICLR 2022
METHODOLOGY,0.3712121212121212,"Algorithm 2 MBAIL: Best Action Imitation Learning for Multiple Batches
Input: Batches B1, · · · , BN of N tasks, maximum number of epochs E"
METHODOLOGY,0.3787878787878788,"1: for t = 1, · · · , N do
2:
Following Algorithm 1, train policy πt.
3: end for
4: for i = 1, · · · , E do
5:
Compute the distillation loss Lπ via Equation 6
6:
Compute the triplet loss Ltriplet via Equation 7.
7:
Do gradient descent w.r.t. π and q for the loss function: L = Ltriplet + Lπ
8: end for
Output: The distilled policy π and the task inference module q."
METHODOLOGY,0.38636363636363635,"crucial in the multitask setting. The reason for this behavior lies in the fact that there is no overlap
(or little) between each batch. Therefore in this case, minimizing Equation 6 only leads to the
algorithm learning the trivial correlations."
METHODOLOGY,0.3939393939393939,"To avoid this problem, in (Li et al., 2019), the authors propose to add an additional loss function,
namely the triplet loss. The motivation behind this loss is to enforce reward information to take part
in the task inference. The authors achieve this by introducing a relabeling process. Given a context
tuple ci = (si, ai, Ri, s
′
i) from batch Bi and a reward estimation of task j, ˆrj : S × A →R, the
relabelling of ci to task j, denoted by cj
i, is deﬁned as: cj
i = (si, ai, ˆrj(si, ai), s
′
i). Then the triplet
loss function is deﬁned by:"
METHODOLOGY,0.4015151515151515,"Ltriplet =
1
N(N −1) N
X i=1 N
X"
METHODOLOGY,0.4090909090909091,"j=1,j̸=i
[d(q(cj
i, q(ci))) −d(q(cj
i), q(cj)) + a]+
(7)"
METHODOLOGY,0.4166666666666667,"where a is the triplet margin, [·]+ is the ReLU function , q outputs the posterior over task identity and
d is a divergence measure which is chosen to be the KL diverge. Through minimizing Equation 7,
we essentially encourage q to infer similar task representations when given either ci or cj
i. Moreover,
it helps enforcing q to infer different task identities for cj
i and cj, which forces q to account for the
reward information instead of only relying on the state-action pairs."
METHODOLOGY,0.42424242424242425,"Now, to add with the previous distillation loss, our multi-batch policy distillation loss is:"
METHODOLOGY,0.4318181818181818,"L = Ltriplet + Lπ
(8)"
METHODOLOGY,0.4393939393939394,"By minimizing the loss function, we will be able to obtain the ﬁnal distilled policy π, as well as the
inference module q. The pseudo-code of MBAIL is summarized in Algorithm 2. There are two main
stages for MBAIL. In the ﬁrst stage, BAIL+ is used to train policies for each task or each group of
tasks with identical properties for BAIL+. In the second stage, the policies learned from each of the
individual policies are distilled into one single multitask policy."
EXPERIMENTS,0.44696969696969696,"4
EXPERIMENTS"
EXPERIMENTS,0.45454545454545453,"In the experiment, we evaluate both BAIL+ and MBAIL on a set of datasets introduced in (Li et al.,
2019). As baseline comparisons, we have also included the results for single task BAIL, as well
as contextual BCQ, where the networks in BCQ are modiﬁed to accept the inferred task identity as
input as illustrated in (Li et al., 2019)."
DATASET DESCRIPTION,0.4621212121212121,"4.1
DATASET DESCRIPTION"
DATASET DESCRIPTION,0.4696969696969697,"The datasets consist three challenging task distributions, namely Ant-Dir, Ant-Goal and HalfChee-
tahVel, from MuJoCo (Todorov et al., 2012). For the multi-task setup, we utilize the exact data
and evaluation protocols from (Li et al., 2019). In Ant-Dir, a target direction deﬁnes a task, where
the agent maximizes returns by running with maximal speed in the target direction. In Ant-Goal, a
task is deﬁned by a goal location, to which the agent should navigate. In HalfCheetahVel, a task is
deﬁned as a constant velocity the agent should achieve. As speciﬁed by the dataset authors (Li et al.,"
DATASET DESCRIPTION,0.4772727272727273,Under review as a conference paper at ICLR 2022
DATASET DESCRIPTION,0.48484848484848486,"(a) The directions of all 10 tasks for Ant-
Dir. The dashed black line indicates the an-
gle border of the task distribution. The sim-
ulated robot ant starts at (0, 0) and needs to
move as quickly as possible in each of the
designated directions."
DATASET DESCRIPTION,0.49242424242424243,"(b) Goal distribution of all 10 tasks for Ant-
Goal. The simulated robot ant starts at (0,
0) and needs to navigate to each designated
goal represented by dots in the ﬁgure."
DATASET DESCRIPTION,0.5,"(c) An example of the simulated ant robot.
This robot is used for both Ant-Dir and Ant-
Goal environments."
DATASET DESCRIPTION,0.5075757575757576,"(d) An example of the simulated half cheetah
robot. This robot is used for the Halfchee-
tahVel environment. The task for this envi-
ronment is for the half cheetah robot to reach
and maintain a speciﬁc speed."
DATASET DESCRIPTION,0.5151515151515151,Figure 2: Illustrations of the experiment environment and their task distribution.
DATASET DESCRIPTION,0.5227272727272727,"2019), there are in total 200,000 samples for Ant-Dir, 300,000 samples for Ant-Goal and 60,000
trajectories for HalfCheetahVel. The state measurements for each task follows exactly the OpenAI
gym state. No task-speciﬁc information, for example goal location or the target velocity, is added to
the state. For Ant-Dir the target directions and goals are sampled from a 120◦circular arc. Figure 2
illustrates these environments in detail."
EXPERIMENT SETUP,0.5303030303030303,"4.2
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.5378787878787878,"In the experiments, we illustrate the performance of our approaches for both Algorithm 1 and Al-
gorithm 2. For single task BAIL, we use a neural network of two layers with 128 neurons for each
layer to ﬁrst approximate the upper envelope function. As mentioned in the previous section, we
select β such that the top 25% data for each batch will be selected. Afterwards, we use a three-layers
MLP with 128 neurons on each layer to approximate the policy via simple behavior cloning method
based on supervised learning. Both networks use ReLu activation function (Nair & Hinton, 2010)
and are optimized via Adam (Kingma & Ba, 2014) with learning rate 0.001."
EXPERIMENT SETUP,0.5454545454545454,"For BAIL+, the upper envelope function is approximated by an MLP of the same structure as for
BAIL. To estimate the returns ˆGt, we directly use the trained reward models from (Li et al., 2019),
which is an ensemble of 10 neural networks. We then follow Equation 5 to estimate the return with
the discount factor set to 0.99. Then the transfer selection ratio threshold ˜α is determined so that
2.5% of all the source tasks’ data is selected. The imitation learning procedure follows the same as
for vanilla BAIL."
EXPERIMENT SETUP,0.553030303030303,"For our MBAIL method, the distilled policy π follows the same structure as before, i.e. three layers
of 128 neurons with ReLu activation function. The inference module is parameterized by a two-"
EXPERIMENT SETUP,0.5606060606060606,Under review as a conference paper at ICLR 2022
EXPERIMENT SETUP,0.5681818181818182,"Improvement (%)
Training samples
ratio (%)
5
10
20
40
60
80
100"
EXPERIMENT SETUP,0.5757575757575758,Ant-Dir
EXPERIMENT SETUP,0.5833333333333334,"BAIL (Single)
-0.12
3.18
5.79
14.82
19.87
22.33
23.21
BAIL+
0.37
3.38
6.30
14.95
20.20
21.45
22.91
MBAIL
5.70
5.65
5.90
16.47
18.72
23.43
23.84
Contextual BCQ
0.74
2.40
4.08
11.87
14.46
18.59
19.49"
EXPERIMENT SETUP,0.5909090909090909,Ant-Goal
EXPERIMENT SETUP,0.5984848484848485,"BAIL (Single)
-76.81
-51.04
-50.31
-50.08
-49.91
-50.40
-49.00
BAIL+
-62.87
-51.90
-50.30
-49.74
-50.61
-50.19
-49.74
MBAIL
-50.94
-49.86
-50.24
-47.07
-47.27
-46.83
-47.44
Contextual BCQ
-72.06
-44.09
-34.49
-30.25
-29.30
-26.18
-27.29"
EXPERIMENT SETUP,0.6060606060606061,HalfCheetahVel
EXPERIMENT SETUP,0.6136363636363636,"BAIL (Single)
-22.15
-22.46
-22.80
-22.08
-20.08
-17.41
-13.35
BAIL+
-22.09
-22.72
-22.52
-16.05
-13.33
-12.87
-11.22
MBAIL
-16.95
-16.85
-16.76
-15.12
-12.43
-12.04
-11.34
Contextual BCQ
-20.50
-20.77
-20.44
-16.03
-12.69
-10.69
-9.05"
EXPERIMENT SETUP,0.6212121212121212,Table 1: Improvement percentage over baseline single task BAIL.
EXPERIMENT SETUP,0.6287878787878788,"layered ReLu network with 200 neurons in each layer. We also use Adam to optimize both of these
networks with 0.0005 learning rate."
EXPERIMENT SETUP,0.6363636363636364,"Contextual BCQ have the same components as the vanilla BCQ (Fujimoto et al., 2019), including
a Q function, a perturbation network and a variational autoencoder (Kingma & Welling, 2013). For
contextual BCQ, we use the exact same model structure as in (Li et al., 2019), where the Q function
is estimated via a 9 layers neural network with 1024 neurons for each layer, the perturbation network
is parameterized via 8 layers neural network with 1024 neurons and the VAE has 7 layers of the same
number of neurons."
EXPERIMENT RESULTS,0.6439393939393939,"4.3
EXPERIMENT RESULTS"
EXPERIMENT RESULTS,0.6515151515151515,"To evaluate our models, we run the learned policies on each of the training environments for 1,000
steps and compute their returns. We then normalize all the returns for each of the tasks by the average
of the Monte Carlo estimated in-batch returns ¯Gt for task t. The reason for this normalization is
that each task, although in the same environment, may differ in reward scaling (e.g., it may be
intrinsically easier to walk slowly or reach a nearby goal compared with fast running or distant
goals) - note we inherit this property from the public dataset and have not modiﬁed the environment
code or raw data. Next, we average all the normalized returns to form an evaluation on the current
task. In general, for a policy π, let rπ
i denote reward collected at time step i when following the
policy π, we evaluate all the learned policies by Equation :"
EXPERIMENT RESULTS,0.6590909090909091,"Eπ
t = L
X i=1"
EXPERIMENT RESULTS,0.6666666666666666,"rπ
i
| ¯Gt|
(9)"
EXPERIMENT RESULTS,0.6742424242424242,"To illustrate the beneﬁt of sample transfer as well as multitask learning, we show the results using
different sizes of the training batches. Because the batch for each environment varies in sample size,
for the simplicity in the result presentation, we evaluate the models with different percentages of the
training batches. We take the ﬁrst x% of the batch as the training batch, where x takes the value of
5, 10, 20, 40, 60, 80, 100, respectively. The averaged normalized returns for different training sample
ratios are listed in Table 1 and the learning curves for all environments on training sample ratio 0.1
and 0.8 are presented in Figure 3."
RESULTS ANALYSIS,0.6818181818181818,"4.4
RESULTS ANALYSIS"
RESULTS ANALYSIS,0.6893939393939394,"From Table 1, we can ﬁrst clearly observe that our BAIL+ and MBAIL outperform the single trained
BAIL method for nearly all of the environments. This is especially the case when encountering
smaller training sample sizes. This shows that our algorithms, by leveraging information from other
tasks, whether it’s from sample transfer or policy distillation, have on average improved the policy
performance on each task."
RESULTS ANALYSIS,0.696969696969697,"More speciﬁcally, from Table 1, we can see that BAIL+ consistently outperforms single task BAIL.
This result shows that BAIL+, by transferring sample from source tasks to the target task, has im-
proved the performance on the original single task BAIL policy. In addition, as one can see from the"
RESULTS ANALYSIS,0.7045454545454546,Under review as a conference paper at ICLR 2022
RESULTS ANALYSIS,0.7121212121212122,"Figure 3: Learning curves for our methods (MBAIL and BAIL+) and the baseline single task BAIL
and contextual BCQ. The top row is trained on a batch of sample ratio 0.1, that is 30,000 training
examples for Ant-Goal, 20,000 for Ant-Dir and 6,000 for HalfCheetahVel. The bottom row is trained
on a batch of sample ratio 0.8, that is 240,000 training examples for Ant-Goal, 160,000 for Ant-Dir
and 48,000 for HalfCheetahVel."
RESULTS ANALYSIS,0.7196969696969697,"learning curve ﬁgures 3, when the sample size is too small, namely 20,000 for Ant-Goal and 6,000
for HalfCheetahVel, there is some model degeneration happening when training for longer epochs.
We can see that although model degeneration does happen for BAIL+, it can often converge to a
better solution. Moreover, in HalfCheetahVel, even with 48,000 examples, this degeneration still
happens for BAIL, while for BAIL+ it does not affect its learning. These results further showcase
the beneﬁts of BAIL+ compared to BAIL by simply enriching the target batch’s dataset."
RESULTS ANALYSIS,0.7272727272727273,"Another observation is that our MBAIL method consistently outperforms BAIL+ across almost all
different sample sizes, as shown in Table 1. This indicates that the policy distillation process of
our MBAIL method does further improve the policy’s performance on top of the BAIL+ algorithm.
Unlike single BAIL or BAIL+, even with low amount of data, as shown in the Figure 3, the distilled
policy does not seem to have model degeneration as observed for other methods, indicating a stable
performance under small data regime."
RESULTS ANALYSIS,0.7348484848484849,"One interesting result is that it seems on Ant-Goal environment, none of the BAIL-based methods
achieves the desired level of return. This behavior persists across all sample sizes. It appears that
BAIL is struggling to obtain a meaningful policy for this particular environment. One potential rea-
son is that the upper envelope function approximated for this environment is not accurate enough,
thus resulting in sub-optimal batches to be selected. Further analysis of the failure cases and relative
performance of BAIL across conditions is an important question, but one left for future work. De-
spite the weakness in all BAIL-driven methods for this one environment, MBAIL still outperforms
single task BAIL and BAIL+, further indicating that our approach for multi-batch BAIL works as
expected."
CONCLUSION AND FUTURE WORK,0.7424242424242424,"5
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.75,"Sample efﬁciency still remains a main obstacle for most reinforcement learning (RL) algorithms be
applicable for real-world applications. Batch reinforcement learning has shown to be promising to
deal with real-world sequential decision making problems. To further improve the batch RL model’s
performance with a limited amount of training data points and its performance over multiple tasks,
in this work, we propose to tackle these problems with sample transfer and policy distillation. Ex-
periment results show that the proposed methods (BAIL+ and MBAIL) can signiﬁcantly outperform
other baselines when the training data is limited. In the future, we plan to do more evaluation of the
proposed methods and investigate how to improve the policy distillation method used in this work."
CONCLUSION AND FUTURE WORK,0.7575757575757576,Under review as a conference paper at ICLR 2022
REFERENCES,0.7651515151515151,REFERENCES
REFERENCES,0.7727272727272727,"Rich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997."
REFERENCES,0.7803030303030303,"Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Best-action
imitation learning for batch deep reinforcement learning. arXiv preprint arXiv:1910.12179, 2019."
REFERENCES,0.7878787878787878,"Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep
neural networks with multitask learning. In Proceedings of the 25th international conference on
Machine learning, pp. 160–167, 2008."
REFERENCES,0.7954545454545454,"Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Sharing knowl-
edge in multi-task deep reinforcement learning. In International Conference on Learning Repre-
sentations, 2019."
REFERENCES,0.803030303030303,"Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503–556, 2005."
REFERENCES,0.8106060606060606,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In Proceedings of the International Conference on Machine Learning (ICML), 2019."
REFERENCES,0.8181818181818182,"Ammar Haydari and Yasin Yilmaz. Deep reinforcement learning for intelligent transportation sys-
tems: A survey. IEEE Transactions on Intelligent Transportation Systems, 2020."
REFERENCES,0.8257575757575758,"Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van
Hasselt. Multi-task deep reinforcement learning with popart. In Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence, volume 33, pp. 3796–3803, 2019."
REFERENCES,0.8333333333333334,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.8409090909090909,"Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
arXiv:1312.6114, 2013."
REFERENCES,0.8484848484848485,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. In Proc. Neural Information Processing Systems NeurIPS, Vancouver,
Canada, 2020."
REFERENCES,0.8560606060606061,"Ao Li, Zhiqiang Wu, Huaiyin Lu, Deyun Chen, and Guanglu Sun. Collaborative self-regression
method with nonlinear feature based on multi-task learning for image classiﬁcation. IEEE Access,
6:43513–43525, 2018."
REFERENCES,0.8636363636363636,"Jiachen Li, Quan Vuong, Shuang Liu, Minghua Liu, Kamil Ciosek, Keith Ross, Henrik Iskov Chris-
tensen, and Hao Su. Multi-task batch reinforcement learning with metric learning. arXiv preprint
arXiv:1909.11373, 2019."
REFERENCES,0.8712121212121212,"Jiayi Li, Hongyan Zhang, and Liangpei Zhang. Efﬁcient superpixel-level multitask joint sparse rep-
resentation for hyperspectral image classiﬁcation. IEEE Transactions on Geoscience and Remote
Sensing, 53(10):5338–5351, 2015."
REFERENCES,0.8787878787878788,"Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015."
REFERENCES,0.8863636363636364,"Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks
for natural language understanding. arXiv preprint arXiv:1901.11504, 2019."
REFERENCES,0.8939393939393939,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.9015151515151515,"Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.
In Icml, 2010."
REFERENCES,0.9090909090909091,Under review as a conference paper at ICLR 2022
REFERENCES,0.9166666666666666,"Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019."
REFERENCES,0.9242424242424242,"Martin Riedmiller. Neural ﬁtted q iteration–ﬁrst experiences with a data efﬁcient neural reinforce-
ment learning method. In European conference on machine learning, pp. 317–328. Springer,
2005."
REFERENCES,0.9318181818181818,"Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk-
patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distil-
lation. arXiv preprint arXiv:1511.06295, 2015."
REFERENCES,0.9393939393939394,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354–359, 2017."
REFERENCES,0.946969696969697,"Olivier Siohan and David Rybach. Multitask learning and system combination for automatic speech
recognition.
In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding
(ASRU), pp. 589–595. IEEE, 2015."
REFERENCES,0.9545454545454546,"R. S. Sutton and A. G. Barto. Reinforcement learning: An Introduction. MIT press, Boston, MA,
2018."
REFERENCES,0.9621212121212122,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033,
2012. doi: 10.1109/IROS.2012.6386109."
REFERENCES,0.9696969696969697,"Lisa Torrey and Jude Shavlik. Transfer learning. In Handbook of research on machine learning
applications and trends: algorithms, methods, and techniques, pp. 242–264. IGI global, 2010."
REFERENCES,0.9772727272727273,"Qing Wang, Jiechao Xiong, Lei Han, Han Liu, Tong Zhang, et al. Exponentially weighted imita-
tion learning for batched historical data. In Neural Information Processing Systems (NeurIPS),
Montreal, Canada, 2018."
REFERENCES,0.9848484848484849,"Ting Yang, Liyuan Zhao, Wei Li, and Albert Y Zomaya. Reinforcement learning in sustainable
energy and electric systems: A survey. Annual Reviews in Control, 49:145–163, 2020."
REFERENCES,0.9924242424242424,"Lixin Zou, Long Xia, Zhuoye Ding, Jiaxing Song, Weidong Liu, and Dawei Yin. Reinforcement
learning to optimize long-term user engagement in recommender systems. In Proceedings of
the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
2810–2818, 2019."
