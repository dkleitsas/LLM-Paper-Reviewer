Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002232142857142857,"Recent advancements in real image editing have been attributed to the exploration
of Generative Adversarial Networks (GANs) latent space. However, a key chal-
lenge in this process is GAN inversion, which aims to accurately map images
to the latent space. Current methods working on the extended latent space W+
struggle to achieve low distortion and high editability simultaneously. In response
to this challenge, we propose an approach that operates in the native latent space
W and fine-tunes the generator network to restore missing image details. This
method introduces a novel regularization strategy with learnable coefficients ac-
quired through training a randomized StyleGAN 2 model - WRanGAN, surpass-
ing traditional approaches in terms of reconstruction quality and computational
efficiency. It achieves the lowest distortion compared to traditional methods. Fur-
thermore, we observe a slight improvement in the quality of constructing hyper-
planes corresponding to binary image attributes. The effectiveness of our approach
is validated through experiments on two complex datasets: Flickr-Faces-HQ and
LSUN Church"
INTRODUCTION,0.004464285714285714,"1
INTRODUCTION"
INTRODUCTION,0.006696428571428571,"The advent of generative adversarial networks (GANs) has significantly advanced the realm of high-
fidelity image synthesis. Among the prominent models in this domain is StyleGAN, renowned for
its exceptional achievements. Furthermore, a body of research Pfau et al. (2020); Khrulkov et al.
(2021); Peebles et al. (2020); Voynov & Babenko (2020); Shen et al. (2020); H¨ark¨onen et al. (2020)
has underscored the rich interpretability of GANs, laying the foundation for intricate image manip-
ulation. This distinct characteristic empowers the selective modification of specific attributes while
preserving the fundamental identity of the image in relation to others. Nevertheless, the practical
application of this feature to real-world images has been constrained by the necessity for precise
mapping into the latent space. This intricate task, commonly referred to as GAN inversion, origi-
nally focused on accurately projecting images into the intrinsic latent space denoted as W. However,
as highlighted in Abdal et al. (2019), this methodology has been demonstrated to yield substantial
disparities between the original and synthesized images. Subsequent studies have redirected their
efforts towards the utilization of the expanded latent space W+ Abdal et al. (2020); Richardson
et al. (2021); Alaluf et al. (2021a); Tov et al. (2021); Wang et al. (2022); Hu et al. (2022), which en-
hances image reconstruction quality but compromises editability. This phenomenon, referred to as
the distortion-editability tradeoff Tov et al. (2021), imposes limitations on the practical application
of codes acquired within the W+ space."
INTRODUCTION,0.008928571428571428,"Another innovative approach to addressing this challenge was proposed in Roich et al. (2021), in-
troducing a minor adjustment in the generator parameters for operations conducted within the latent
space W - the pivotal tuning inversion (PTI)."
INTRODUCTION,0.011160714285714286,"In this paper, we introduce a new approach based on tuning generator parameters with learnable
regularization. Instead of using small, equal regularization coefficients for all model parameters, we
propose to learn the optimal ones. This allows for achieving high-quality image reconstructions by
tuning only 25% of the model parameters. This approach is based on a randomized version of the
StyleGAN 2 model called WRanGAN, in which part of the model weights are assumed to be nor-
mally distributed with trainable mean and variance. To apply the learned regularization coefficients
during inversion, we utilized the reparameterization trick Kingma & Welling (2014). We evaluated"
INTRODUCTION,0.013392857142857142,Published as a conference paper at ICOMP 2024
INTRODUCTION,0.015625,"the effectiveness of our technique on two complex datasets, the Flickr-Faces-HQ Dataset (FFHQ)
Karras et al. (2019) and LSUN Churches Yu et al. (2015). Our contributions can be summarized as
follows:"
INTRODUCTION,0.017857142857142856,"• We present a novel adaptive regularization scheme and investigate their effect on recon-
struction quality and editability.
• We introduce WRanGAN, a model that learns appropriate regularization coefficients via a
randomization of the StyleGAN 2 model.
• We evaluate WRanGAN in terms of generation, reconstruction, binary attributes extraction,
and computational cost, and compare it to several baselines."
LIT REVIEW,0.020089285714285716,"2
PROBLEM SETTING"
LIT REVIEW,0.022321428571428572,"2.1
LATENT SPACE MANIPULATION"
LIT REVIEW,0.024553571428571428,"GANs enable the creation of images that can be manipulated along semantic directions, as evidenced
in Pfau et al. (2020); Khrulkov et al. (2021); Peebles et al. (2020); Voynov & Babenko (2020);
Shen et al. (2020); H¨ark¨onen et al. (2020). Notably, the work presented in H¨ark¨onen et al. (2020)
introduced the concept of estimating subspaces that remain invariant under random-walk diffusion
for identification purposes. Additionally, in the study by Shen et al. (2020), supervision in the
form of facial attribute labels was employed to discern meaningful linear directions within the latent
space. Furthermore, the utilization of principal component analysis (PCA) for the identification of
latent directions was proposed in H¨ark¨onen et al. (2020)."
LIT REVIEW,0.026785714285714284,"2.2
GAN INVERSION"
LIT REVIEW,0.029017857142857144,"Recent efforts have been dedicated to enhancing the quality of GAN inversion for image reconstruc-
tion, a task focusing on precisely identifying the latent code required to accurately reproduce a real
image. This pursuit can be broadly classified into two groups: optimization techniques that directly
manipulate the latent code to minimize a loss function Abdal et al. (2019); Pernuˇs et al. (2021), and
encoder-based methods that use a trained encoder to generate an image Guan et al. (2020); Alaluf
et al. (2021a); Richardson et al. (2021); Tov et al. (2021). Typically, these methods function within
the native latent space W, potentially leading to significant visual differences compared to the orig-
inal image Abdal et al. (2019). Conversely, operating in the extended latent space W+ provides
increased expressiveness, enabling the recreation of finer image details. Nevertheless, this approach
is limited by fixed generator parameters. In response to this constraint, certain strategies have sug-
gested adjustments to the generator network to address visual inconsistencies, as seen in Roich et al.
(2021). Other approaches employ hypernetworks to forecast changes in generator parameters, with
the objective of reducing distortion and preserving the fidelity of the generated image, illustrated in
Alaluf et al. (2021b) and Dinh et al. (2022)."
LIT REVIEW,0.03125,"2.3
DISTORTION-EDITABILITY TRADEOFF"
LIT REVIEW,0.033482142857142856,"The utilization of the extended latent space W+ in GAN inversion has shown notable improvement
in the reconstruction of real images, albeit at the cost of reduced editability, a phenomenon known as
the distortion-editability trade-off Tov et al. (2021). Notably, recent works such as Zhu et al. (2020)
and Tov et al. (2021) have proposed methods to seek editable latent codes within the extended
latent space W+. Conversely, alternative approaches, as presented in the works by Alaluf et al.
(2021b) and Roich et al. (2021), offer a completely distinct solution to this challenge. Rather than
striving to strike a balance between editability and distortion, these authors advocate for leveraging
the advantages of projecting into the latent space W and updating generator parameters to minimize
distortion. In our study, we also employed projection into the native latent space to achieve enhanced
editability."
LIT REVIEW,0.03571428571428571,"2.4
GENERATOR TUNING"
LIT REVIEW,0.03794642857142857,"Model tuning significantly improves ability to reproduce real image Roich et al. (2021); Alaluf et al.
(2021b); Dinh et al. (2022). But changing the parameters of the model can damage its quality."
LIT REVIEW,0.04017857142857143,Published as a conference paper at ICOMP 2024
LIT REVIEW,0.04241071428571429,"Figure 1: Comparison between the PTI and WRanGAN regularization strategies. PTI, employing
pivotal tuning with a high regularization coefficient, falls short in comparison to WRanGAN, which
incorporates optimal regularization coefficients. This is evident in the reconstruction results, partic-
ularly the enlarged area around the glasses, where PTI displays inferior performance. Furthermore,
the impact on image editing is apparent, as altering the age results in darkened areas around the eyes,
and removing glasses fails to produce the desired effect on the image."
LIT REVIEW,0.044642857142857144,"In order to improve the realism of generated images after modification of the generator weights,
non-saturating GAN loss was used to train hypernetworks Alaluf et al. (2021b); Dinh et al. (2022)
(encoders predicting the necessary weight shift) . Despite the significant improvement in the quality
of reproduction, these methods are still inferior to the PTI approach Roich et al. (2021) based on
direct weight optimization. But optimization of model parameters without any additional constraints
requires imposing a regularization with a high coefficient in order not to damage the realism of the
generated images and forces to optimize all the parameters of the model to reach low distortion. As
a result, it leads to a significant increase in the computational costs."
IMPLEMENTATION/METHODS,0.046875,"3
METHOD"
IMPLEMENTATION/METHODS,0.049107142857142856,"The proposed method addresses the issue by utilizing non-uniform learnable regularization. This
approach enables the setting of an appropriate regularization coefficient for each parameter based
on its impact on model performance (realism of generated images). Initially, we determine suitable
regularization coefficients for the inversion task through adversarial training of a generator with
partially randomized parameters. Subsequently, these learned coefficients are applied in an inversion
procedure that involves encoder projection and regularized optimization to minimize a specific loss
function."
IMPLEMENTATION/METHODS,0.05133928571428571,"3.1
REGULARIZED INVERSION"
IMPLEMENTATION/METHODS,0.05357142857142857,"In order to avoid degradation of realism in generated images, regularization term is added to opti-
mization procedure:"
IMPLEMENTATION/METHODS,0.05580357142857143,"ˆw, ˆθG = arg min
w,θG L(G(w, θG), ˆx) + αreg∥θG −θG,0∥2
2"
IMPLEMENTATION/METHODS,0.05803571428571429,"where ˆx represents a real image, αreg∥θG −θG,0∥2
2 is the regularization term, G(w, θG) is the re-
constructed image, θG,0 are the initial values of the generator weights.For the WRanGAN inversion,
we used L = 2L2 + LLPIPS and initialized the intermediate latent code w by mapping the output"
IMPLEMENTATION/METHODS,0.060267857142857144,Published as a conference paper at ICOMP 2024
IMPLEMENTATION/METHODS,0.0625,"Algorithm 1 Algorithm of WRanGAN inversion
Input: real image ˆx, generator parameters µθ, σθ
Parameter: regularization coefficient αreg
Output: latent code w and parameterized randomization ϵ"
IMPLEMENTATION/METHODS,0.06473214285714286,"Initialize w = E(x) by the output of encoder network
Initialize ϵ with small value (10−4)
for number of iterations do"
IMPLEMENTATION/METHODS,0.06696428571428571,"Set generator weights θG ←µθ + σθϵ
Update parameters w, ϵ minimizing:"
IMPLEMENTATION/METHODS,0.06919642857142858,"L(G(w, θG), ˆx) + αreg∥ϵ∥2
2"
IMPLEMENTATION/METHODS,0.07142857142857142,"end for
return w,ϵ"
IMPLEMENTATION/METHODS,0.07366071428571429,Table 1: Quantitative comparison of memory cost on the number of randomized layers
IMPLEMENTATION/METHODS,0.07589285714285714,"Number of
randomized layers"
IMPLEMENTATION/METHODS,0.078125,"Relative increase in
amount of parameters
4
7%
6
23%
8
39%"
IMPLEMENTATION/METHODS,0.08035714285714286,"of the trained encoder E to the intermediate latent space W: w = f(E(ˆx)). The regularisation
coefficient αreg is chosen to obtain low distortion and not corrupt the model’s generative quality.
We considered two strategies of regularization to illustrate this paradigm:"
IMPLEMENTATION/METHODS,0.08258928571428571,"• high regularization value (PTI)
• appropriate regularization coefficients (WRanGAN)"
IMPLEMENTATION/METHODS,0.08482142857142858,"Based on the outcomes of our experiments, depicted in Figure 1, it is evident that a high regu-
larization coefficient does not yield the lowest distortion, as illustrated by the enlarged area in the
reconstructed image. Moreover, it results in the manipulation of specific image attributes, leading to
inaccurate behavior. In the showcased example, the proposed WRanGAN model exhibits superior
performance in reconstruction and consistently generates realistic images during editing: altering
attributes such as the presence of eyeglasses, gender, and human age."
IMPLEMENTATION/METHODS,0.08705357142857142,"3.2
WRANGAN INVERSION"
IMPLEMENTATION/METHODS,0.08928571428571429,"In this segment, we have explored the application of regularization to randomized model parameters
θG ∼N(µθ, σθ). For this purpose, we employ the reparameterization trick, which asserts that
θi
G = µi
θ + ϵiσi
θ where ϵ ∼N(0, 1) and i denotes the index of a specific parameter. By imposing
regularization on the parameter ϵ, we derive the following equation:"
IMPLEMENTATION/METHODS,0.09151785714285714,"αreg∥ϵ∥2
2 =
X i"
IMPLEMENTATION/METHODS,0.09375,"αreg
(σi
θ)2 (θi
G −µi
θ)2 =
X"
IMPLEMENTATION/METHODS,0.09598214285714286,"i
αi
reg(θi
G −θi
G,0)2"
IMPLEMENTATION/METHODS,0.09821428571428571,"Here, we have utilized the notations αi
reg =
αreg
(σi
θ)2 and µi
θ = θi
G,0, and have derived a standard regu-
larization formulation with distinct regularization coefficients for each randomized model parameter.
The insights discussed in this section are encapsulated in Algorithm 1."
IMPLEMENTATION/METHODS,0.10044642857142858,"3.3
WEIGHT RANDOMIZATION"
IMPLEMENTATION/METHODS,0.10267857142857142,"The concept of randomizing the model was influenced by Bayesian GAN Saatci & Wilson (2017),
in which both the generator and discriminator networks are assumed to have distributions over their
internal parameters. However, randomizing the entire network is computationally intensive due to
the increased number of parameters needed for training and tuning the generator parameters during"
IMPLEMENTATION/METHODS,0.10491071428571429,Published as a conference paper at ICOMP 2024
IMPLEMENTATION/METHODS,0.10714285714285714,"10
1
100
101
102
103
104
Regularization coefficient 0.000 0.005 0.010 0.015 0.020 0.025 MSE"
IMPLEMENTATION/METHODS,0.109375,"N = 4
N = 6
N = 8"
IMPLEMENTATION/METHODS,0.11160714285714286,"Figure 2: Dependence of MSE on number of randomized layers N versus regularization coefficient.
The lower the curve the better chosen number of layers."
IMPLEMENTATION/METHODS,0.11383928571428571,"Algorithm 2 WRanGAN training algorithm.
Input: pretrained StyleGAN 2 weights θG,0, dataset ˆx
Parameter: batch size m
Output: µθ, σθ"
IMPLEMENTATION/METHODS,0.11607142857142858,"Initialize µθ = θG,0
Initialize σθ = 1 for randomized parameters
for number of training iterations do"
IMPLEMENTATION/METHODS,0.11830357142857142,"Sample z(1), ..., z(m) ∼N(0, 1)
Map to intermediate latent space w(i) = f(z(i))
Sample ˆx(1), ..., ˆx(m) from training dataset ˆx
Sample ϵ ∼N(0, 1) and calculate θG = µθ + ϵσθ
Update discriminator weights θD minimizing:"
IMPLEMENTATION/METHODS,0.12053571428571429,"1
m m
X"
IMPLEMENTATION/METHODS,0.12276785714285714,"i=1
LD(D(ˆx(i)), D(G(w(i), θG)))"
IMPLEMENTATION/METHODS,0.125,"Sample z(1), ..., z(m) ∼N(0, 1)
Map to intermediate latent space w(i) = f(z(i))
Sample ϵ ∼N(0, 1) and calculate θG = µθ + ϵσθ
Update parameters (µθ, σθ) minimizing:"
IMPLEMENTATION/METHODS,0.12723214285714285,"1
m m
X"
IMPLEMENTATION/METHODS,0.12946428571428573,"i=1
Lg(D(G(w(i), θG)))"
IMPLEMENTATION/METHODS,0.13169642857142858,"end for
return µθ,σθ"
IMPLEMENTATION/METHODS,0.13392857142857142,"the inversion step. Our previous discussion demonstrated the process of inversion using suitable
regularization coefficients, and now we will outline how to acquire these coefficients."
IMPLEMENTATION/METHODS,0.13616071428571427,Published as a conference paper at ICOMP 2024
IMPLEMENTATION/METHODS,0.13839285714285715,"Table 2: Quantitative reconstruction outcomes of the WRanGAN model are juxtaposed with the
StyleGAN 2 inversion methodologies, encompassing encoder and optimization-based techniques.
The evaluation encompasses various standard metrics, with each metri direction of improvement
indicated by the arrow (lower ↓/ higher ↑). The best results for each metric are prominently denoted
in bold. Notably, metrics highlighted in blue signify instances where we surpass PTI in performance."
IMPLEMENTATION/METHODS,0.140625,"Domain
Model
Method
MSE↓
LPIPS↓
MS-SSIM↑
Time
VGG
Alex
(s)↓"
IMPLEMENTATION/METHODS,0.14285714285714285,"FFHQ
StyleGAN 2"
IMPLEMENTATION/METHODS,0.14508928571428573,"E4E
0.062
0.389
0.235
0.605
1.64
Restyle
0.035
0.335
0.154
0.72
0.28
SG2 W+
0.04
0.14
0.138
0.783
97.9
HyperStyle
0.026
0.288
0.105
0.788
0.31
PTI
0.024
0.293
0.06
0.776
35.46"
IMPLEMENTATION/METHODS,0.14732142857142858,"WRanGAN
WRanGAN
inversion
0.007
0.085
0.083
0.929
23.27"
IMPLEMENTATION/METHODS,0.14955357142857142,"LSUN
Church"
IMPLEMENTATION/METHODS,0.15178571428571427,StyleGAN 2
IMPLEMENTATION/METHODS,0.15401785714285715,"E4E
0.142
0.506
0.418
0.263
1.64
Restyle
0.087
0.411
0.25
0.489
0.28
SG2 W+
0.107
0.225
0.235
0.543
97.9
PTI
0.053
0.411
0.065
0.643
47"
IMPLEMENTATION/METHODS,0.15625,"WRanGAN
WRanGAN
inversion
0.033
0.177
0.224
0.782
23.27"
IMPLEMENTATION/METHODS,0.15848214285714285,"Table 3: The assessment of WRanGAN model quality involved the utilization of FID, Precision, and
Recall metrics across two domains: FFHQ and LSUN Church. The best results for each domain and
metric are distinctly highlighted in bold."
IMPLEMENTATION/METHODS,0.16071428571428573,"Domain
Model
FID
Precision
Recall
Human
Faces"
IMPLEMENTATION/METHODS,0.16294642857142858,"StyleGAN 2
4.27
0.7
0.42
WRanGAN
5.61
0.65
0.45
LSUN
Church"
IMPLEMENTATION/METHODS,0.16517857142857142,"StyleGAN 2
4.3
0.61
0.37
WRanGAN
3.57
0.55
0.42"
IMPLEMENTATION/METHODS,0.16741071428571427,"How many parameters to randomize?
The study conducted in Alaluf et al. (2021b). aimed to
identify the most effective parameters to be modified in the generator. It was determined that re-
stricting the randomization to the last few convolutional layers, excluding the toRGB layers, while
leaving the discriminator architecture unchanged, was the optimal approach. A grid search was car-
ried out over N = 4, 6, 8 with varying regularization coefficients to ascertain the appropriate number
of layers for randomization. The findings of this search are detailed in Figure 2, and the associated
computational costs are provided in Table 1. The results led to the conclusion that randomizing
only the last N = 6 convolutional layers produced the best outcomes with a minimal increase in
computational expenses."
IMPLEMENTATION/METHODS,0.16964285714285715,"How to train?
When training the WRanGAN model, a pre-trained model was utilized to initialize
the mean value of the model parameters, µθ = θG,0. A standard deviation of one was then incor-
porated into each randomized parameter. The generator and discriminator underwent concurrent
training to achieve the global optima, as delineated in Algorithm 2."
RESULTS/EXPERIMENTS,0.171875,"4
EXPERIMENTS"
RESULTS/EXPERIMENTS,0.17410714285714285,"This section presents the results of the evaluation of the proposed WRanGAN model. Below are
presented the details of conducted experiments: datasets, baselines, and hyperparameters."
RESULTS/EXPERIMENTS,0.17633928571428573,Technical details.
RESULTS/EXPERIMENTS,0.17857142857142858,"• Models: We used the StyleGAN 2 Karras et al. (2020) model as a basis, with pre-trained
models and base code for implementation taken from an open resource1."
RESULTS/EXPERIMENTS,0.18080357142857142,1https://github.com/rosinality/stylegan2-pytorch
RESULTS/EXPERIMENTS,0.18303571428571427,Published as a conference paper at ICOMP 2024 FFHQ
RESULTS/EXPERIMENTS,0.18526785714285715,"Real
E4E
Restyle
SG2 W+
HyperStyle
PTI
WRanGAN FFHQ"
RESULTS/EXPERIMENTS,0.1875,(cropped)
RESULTS/EXPERIMENTS,0.18973214285714285,"Figure 3: Qualitative evaluation of WRanGAN inversion results compared to ones produced by
StyleGAN 2 using various approaches for FFHQ domain. For each reconstruction provided zoomed
version (interesting regions were cropped) to see the difference in details completely. LSUN"
RESULTS/EXPERIMENTS,0.19196428571428573,Church
RESULTS/EXPERIMENTS,0.19419642857142858,"Real
E4E
Restyle
SG2 W+
PTI
WRanGAN LSUN"
RESULTS/EXPERIMENTS,0.19642857142857142,Church
RESULTS/EXPERIMENTS,0.19866071428571427,(cropped)
RESULTS/EXPERIMENTS,0.20089285714285715,"Figure 4: Qualitative evaluation of WRanGAN inversion results compared to ones produced by
StyleGAN 2 using various approaches for LSUN Church domain. For each reconstruction provided
zoomed version (interesting regions were cropped) to see the difference in details completely."
RESULTS/EXPERIMENTS,0.203125,"• Datasets: We trained using the Flickr-Faces-HQ Dataset (FFHQ) Karras et al. (2019) with
pictures resized to resolution 256x256, and LSUN Churches Yu et al. (2015) with pictures
center-cropped and resized to 256x256. We randomly sampled 1000 images from both
datasets for testing."
RESULTS/EXPERIMENTS,0.20535714285714285,"• WRanGAN training details: We used standard parameters for StyleGAN 2, and trained
on 2 GPUs with a batch size of 8 for 200k iterations."
RESULTS/EXPERIMENTS,0.20758928571428573,"• WRanGAN inversion details: For the encoder E in Algorithm 1, we used the architecture
proposed by Tov et al. (2021) and trained with default parameters. We used the Adam
optimizer with a learning rate of lr = 10−3, and the number of iterations needed for
convergence was set to 500. The randomization parameter was initialized with the value
ϵ = 10−4, and we used a regularization coefficient of αreg = 10−4."
RESULTS/EXPERIMENTS,0.20982142857142858,Experiments were conducted on 4 Tesla V100-SXM2 GPUs with 16 GB of memory.
RESULTS/EXPERIMENTS,0.21205357142857142,"4.1
WRANGAN MODEL EVALUATION"
RESULTS/EXPERIMENTS,0.21428571428571427,"We evaluated the performance of our WRanGAN model by running several metrics such as FID,
Precision, and Recall Kynk¨a¨anniemi et al. (2019) and comparing our results with those produced
by the StyleGAN 2 model (see Table 3). WRanGAN showed an improvement in the Recall metric
for both data domains, which suggests that the generator is more likely to reproduce particular real
images. However, we observed a slight decrease in the Precision metric. For further details on
the randomized parameters of the model and their effect on the generated images, please refer to
Appendix A."
RESULTS/EXPERIMENTS,0.21651785714285715,Published as a conference paper at ICOMP 2024 PTI Real
RESULTS/EXPERIMENTS,0.21875,WRanGAN
RESULTS/EXPERIMENTS,0.22098214285714285,Inversion Mouth
RESULTS/EXPERIMENTS,0.22321428571428573,"open
Eyeglasses
Gender
Smile
Age"
RESULTS/EXPERIMENTS,0.22544642857142858,"Figure 5: Qualitative evaluation of WRanGAN editing quality compared to PTI approach applied
over StyleGAN 2 model. PTI Real"
RESULTS/EXPERIMENTS,0.22767857142857142,WRanGAN
RESULTS/EXPERIMENTS,0.22991071428571427,"Inversion
= 0.25
= 0.5
= 0.75
Inversion
Real"
RESULTS/EXPERIMENTS,0.23214285714285715,"Figure 6: Qualitative evaluation of WRanGAN interpolation quality compared to PTI approach
applied over StyleGAN 2 model. Here α denotes interpolation step."
RESULTS/EXPERIMENTS,0.234375,"4.2
INVERSION QUALITY ASSESSMENT"
RESULTS/EXPERIMENTS,0.23660714285714285,"The evaluation of inversion quality encompassed several encoder-based approaches, including e4e
Tov et al. (2021), ReStyle Alaluf et al. (2021a), and HyperStyle Alaluf et al. (2021b), as well as
optimization-based approaches such as SG2 W+ Karras et al. (2020) and PTI Roich et al. (2021).
Standard metrics such as mean squared error (MSE), LPIPS Zhang et al. (2018) with the VGG
and Alex feature network, and MS-SSIM Wang et al. (2003) were employed for the assessment.
The summarized results in Table 2 indicate that the proposed WRanGAN model outperformed all
other methods applied to StyleGAN 2 across most metrics. Not only did it achieve the lowest
distortion, but its computational efficiency also far exceeded that of PTI, as the tuning procedure
necessitated optimizing 4 times fewer parameters. Moreover, the computational speed was 1.5 and
2 times faster for FFHQ and LSUN Church domains, respectively. The visual representation of the
results further demonstrated how the enhancement in reconstruction affected the image, with the
WRanGAN approach capable of reproducing unique details such as bangs, the outline of the eyes,
and small church windows. More detailed visualizations and an exploration of the distribution of
randomized parameters for real mapped images can be found in Appendices B and C, respectively."
RESULTS/EXPERIMENTS,0.23883928571428573,"4.3
EDITING AND INTERPOLATION ASSESSMENT"
RESULTS/EXPERIMENTS,0.24107142857142858,"Our experiment was designed to verify that the WRanGAN model exhibits a similar excellent prop-
erty as the StyleGAN 2 model - specifically, for any binary attribute, there exists a hyperplane in
latent space such that all samples from the same side have the same attribute Shen et al. (2020). To
achieve this, we trained a classifier to predict attributes including Gender, Eyeglasses, Smile, Age,
and Open Mouth. Subsequently, we constructed hyperplanes in the latent space corresponding to the
selected attributes and evaluated their accuracy, as detailed in Table 4. The findings from our exper-
iment indicate that the WRanGAN model outperforms the basic StyleGAN 2 model, as evidenced
in the visualization provided in Figure 5. This visualization clearly shows the significant impact of
attributes such as eyeglasses in the original image on attribute editing using the PTI method, while
WRanGAN exhibits exceptional performance. Furthermore, the interpolation comparison presented"
RESULTS/EXPERIMENTS,0.24330357142857142,Published as a conference paper at ICOMP 2024
RESULTS/EXPERIMENTS,0.24553571428571427,"Table 4: Classification accuracy (%) on separation boundaries in latent space with respect to different
face attributes. The best results are highlighted in bold."
RESULTS/EXPERIMENTS,0.24776785714285715,"Attribute
StyleGAN 2
WRanGAN
Gender
73.9
75.0
Eyeglasses
99.8
99.9
Smile
99.5
99.8
Age
99.5
99.4
Mouth open
98.2
98.5"
RESULTS/EXPERIMENTS,0.25,"in Figure 6 highlights the superior performance of WRanGAN. Additional examples are available
in Appendix D for further reference."
CONCLUSION/DISCUSSION,0.25223214285714285,"5
CONCLUSION"
CONCLUSION/DISCUSSION,0.2544642857142857,"We introduced WRanGAN, a randomized variant of the StyleGAN 2 model that autonomously
learns the optimal scaling (standard deviation) for each parameter to determine the appropriate reg-
ularization coefficient. Our non-uniform regularization coefficient approach for GAN tuning exhib-
ited superior performance in terms of distortion and computational efficiency when compared to
the highly successful pivotal tuning inversion method. Importantly, our method maintained model
integrity, facilitating image editing capabilities. Furthermore, we illustrated the ease of constructing
hyperplanes corresponding to standard image attributes in the FFHQ domain within the latent space
of a randomized model."
CONCLUSION/DISCUSSION,0.25669642857142855,"Our approach reduces memory requirements per image during the inversion process, facilitating
parallelized computations. Moreover, it exhibits slight dependency on the network architecture,
allowing for potential adaptation to other structures such as StyleGAN 3 Karras et al. (2021)."
REFERENCES,0.25892857142857145,REFERENCES
REFERENCES,0.2611607142857143,"Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the
stylegan latent space? In 2019 IEEE/CVF International Conference on Computer Vision (ICCV),
pp. 4431–4440, 2019. doi: 10.1109/ICCV.2019.00453."
REFERENCES,0.26339285714285715,"Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded im-
ages? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), June 2020."
REFERENCES,0.265625,"Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Restyle: A residual-based stylegan encoder via
iterative refinement, 2021a."
REFERENCES,0.26785714285714285,"Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit H. Bermano. Hyperstyle: Stylegan
inversion with hypernetworks for real image editing, 2021b."
REFERENCES,0.2700892857142857,"Tan M. Dinh, Anh Tuan Tran, Rang Nguyen, and Binh-Son Hua. Hyperinverter: Improving stylegan
inversion via hypernetwork. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2022."
REFERENCES,0.27232142857142855,"Shanyan Guan, Ying Tai, Bingbing Ni, Feida Zhu, Feiyue Huang, and Xiaokang Yang. Collabora-
tive learning for faster stylegan embedding, 2020. URL https://arxiv.org/abs/2007.
01758."
REFERENCES,0.27455357142857145,"Xueqi Hu, Qiusheng Huang, Zhengyi Shi, Siyuan Li, Changxin Gao, Li Sun, and Qingli Li. Style
transformer for image inversion and editing. arXiv preprint arXiv:2203.07932, 2022."
REFERENCES,0.2767857142857143,"Erik H¨ark¨onen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering
interpretable gan controls. In Proc. NeurIPS, 2020."
REFERENCES,0.27901785714285715,"Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative ad-
versarial networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 4396–4405, 2019. doi: 10.1109/CVPR.2019.00453."
REFERENCES,0.28125,Published as a conference paper at ICOMP 2024
REFERENCES,0.28348214285714285,"Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Ana-
lyzing and improving the image quality of stylegan. In 2020 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 8107–8116, 2020. doi: 10.1109/CVPR42600.2020.
00813."
REFERENCES,0.2857142857142857,"Tero Karras, Miika Aittala, Samuli Laine, Erik H¨ark¨onen, Janne Hellsten, Jaakko Lehtinen, and
Timo Aila. Alias-free generative adversarial networks. In Proc. NeurIPS, 2021."
REFERENCES,0.28794642857142855,"Valentin Khrulkov, Leyla Mirvakhabova, Ivan Oseledets, and Artem Babenko. Disentangled repre-
sentations from non-disentangled models, 2021."
REFERENCES,0.29017857142857145,"Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114,
2014."
REFERENCES,0.2924107142857143,"Tuomas Kynk¨a¨anniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved
precision and recall metric for assessing generative models. CoRR, abs/1904.06991, 2019."
REFERENCES,0.29464285714285715,"William Peebles, John Peebles, Jun-Yan Zhu, Alexei A. Efros, and Antonio Torralba. The hessian
penalty: A weak prior for unsupervised disentanglement. In Proceedings of European Conference
on Computer Vision (ECCV), 2020."
REFERENCES,0.296875,"Martin Pernuˇs, Vitomir ˇStruc, and Simon Dobriˇsek. High resolution face editing with masked gan
latent code optimization, 2021."
REFERENCES,0.29910714285714285,"David Pfau, Irina Higgins, Aleksandar Botev, and S´ebastian Racani`ere. Disentangling by subspace
diffusion. Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.3013392857142857,"Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel
Cohen-Or. Encoding in style: A stylegan encoder for image-to-image translation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2287–
2296, June 2021."
REFERENCES,0.30357142857142855,"Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based
editing of real images. arXiv preprint arXiv:2106.05744, 2021."
REFERENCES,0.30580357142857145,"Yunus Saatci and Andrew G Wilson. Bayesian gan. In Advances in neural information processing
systems, pp. 3622–3631, 2017."
REFERENCES,0.3080357142857143,"Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for
semantic face editing. In CVPR, 2020."
REFERENCES,0.31026785714285715,"Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. Designing an encoder
for stylegan image manipulation. In ACM Transactions on Graphics, volume 40, pp. 1–14, 2021.
doi: 10.1145/3450626.3459838."
REFERENCES,0.3125,"Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan
latent space. In International Conference on Machine Learning, pp. 9786–9796. PMLR, 2020."
REFERENCES,0.31473214285714285,"Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and Qifeng Chen. High-fidelity gan inversion
for image attribute editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2022."
REFERENCES,0.3169642857142857,"Z. Wang, Eero Simoncelli, and Alan Bovik.
Multiscale structural similarity for image qual-
ity assessment.
volume 2, pp. 1398 – 1402 Vol.2, 12 2003.
ISBN 0-7803-8104-1.
doi:
10.1109/ACSSC.2003.1292216."
REFERENCES,0.31919642857142855,"Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao.
Lsun: Construction of
a large-scale image dataset using deep learning with humans in the loop.
arXiv preprint
arXiv:1506.03365, 2015."
REFERENCES,0.32142857142857145,"Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In CVPR, 2018."
REFERENCES,0.3236607142857143,"Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image
editing. In Proceedings of European Conference on Computer Vision (ECCV), 2020."
OTHER,0.32589285714285715,Published as a conference paper at ICOMP 2024
OTHER,0.328125,"A
INVESTIGATION OF WRANGAN MODEL RANDOMIZATION"
OTHER,0.33035714285714285,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
OTHER,0.3325892857142857,Density
OTHER,0.33482142857142855,"Layer 7(mean = 0.8, std = 0.5)"
OTHER,0.33705357142857145,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
OTHER,0.3392857142857143,Density
OTHER,0.34151785714285715,"Layer 8(mean = 0.8, std = 0.4)"
OTHER,0.34375,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0 0.2 0.4 0.6 0.8"
OTHER,0.34598214285714285,Density
OTHER,0.3482142857142857,"Layer 9(mean = 0.7, std = 0.4)"
OTHER,0.35044642857142855,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0 0.2 0.4 0.6 0.8 1.0"
OTHER,0.35267857142857145,Density
OTHER,0.3549107142857143,"Layer 10(mean = 0.5, std = 0.4)"
OTHER,0.35714285714285715,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
OTHER,0.359375,Density
OTHER,0.36160714285714285,"Layer 11(mean = 0.5, std = 0.4)"
OTHER,0.3638392857142857,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
OTHER,0.36607142857142855,Density
OTHER,0.36830357142857145,"Layer 12(mean = 0.5, std = 0.4)"
OTHER,0.3705357142857143,"Figure 7: Dependence of distribution of randomized parameters variance with respect to index of
convolutional layer for model trained on FFHQ dataset."
OTHER,0.37276785714285715,Published as a conference paper at ICOMP 2024
OTHER,0.375,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
OTHER,0.37723214285714285,Density
OTHER,0.3794642857142857,"Layer 7(mean = 0.8, std = 0.5)"
OTHER,0.38169642857142855,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0 0.2 0.4 0.6 0.8"
OTHER,0.38392857142857145,Density
OTHER,0.3861607142857143,"Layer 8(mean = 0.7, std = 0.5)"
OTHER,0.38839285714285715,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0 0.2 0.4 0.6 0.8 1.0 1.2"
OTHER,0.390625,Density
OTHER,0.39285714285714285,"Layer 9(mean = 0.5, std = 0.4)"
OTHER,0.3950892857142857,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0 0.5 1.0 1.5 2.0"
OTHER,0.39732142857142855,Density
OTHER,0.39955357142857145,"Layer 10(mean = 0.4, std = 0.3)"
OTHER,0.4017857142857143,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.40401785714285715,Density
OTHER,0.40625,"Layer 11(mean = 0.3, std = 0.3)"
OTHER,0.40848214285714285,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.4107142857142857,Density
OTHER,0.41294642857142855,"Layer 12(mean = 0.3, std = 0.3)"
OTHER,0.41517857142857145,"Figure 8: Dependence of distribution of randomized parameters variance with respect to index of
convolutional layer for model trained on LSUN Church dataset."
OTHER,0.4174107142857143,"The WRanGAN model is a type of generative model that optimizes two parameters - the mean
and variance - in its learning process. It is hypothesized that a larger variance value allows for"
OTHER,0.41964285714285715,Published as a conference paper at ICOMP 2024
OTHER,0.421875,Table 5: Percentage of small variances (σθ < 10−3) for each randomized convolutional layer.
OTHER,0.42410714285714285,"Layer index
Percentage
7
0.13 %
8
0.16 %
9
0.18 %
10
0.29 %
11
0.40 %
12
0.52 %"
OTHER,0.4263392857142857,"Real
E4E
Restyle
SG2 W+
HyperStyle
PTI
WRanGAN"
OTHER,0.42857142857142855,Figure 9: Qualitative reconstruction examples for images taken from FFHQ dataset.
OTHER,0.43080357142857145,Table 6: Influence of each randomized layer on generator output.
OTHER,0.4330357142857143,"Layer index
MSE
7
0.017
8
0.017
9
0.015
10
0.015
11
0.014
12
0.014"
OTHER,0.43526785714285715,"more changes to be made, which can be observed by examining the distributions of the variance
values of the trained WRanGAN model. Figures 7 and 8 present the distributions for the FFHQ
and LSUN Church domains respectively. It can be seen that the distribution is shifted towards zero"
OTHER,0.4375,Published as a conference paper at ICOMP 2024
OTHER,0.43973214285714285,"Real
E4E
Restyle
SG2 W+
PTI
WRanGAN"
OTHER,0.4419642857142857,Figure 10: Qualitative reconstruction examples for images taken from LSUN Church dataset.
OTHER,0.44419642857142855,"as the number of randomized layers increases. In order to verify this idea, a test was conducted to
measure the number of parameters that have a variance close to zero (σθ < 10−3) in each layer.
The results, as presented in Table 5, indicate that the last layers have a greater effect on the model
performance. To further investigate this idea, a procedure was conducted in which the parameters
of different layers were changed and the impact of these changes on the output image was assessed
by calculating the MSE metric (Table 6). The results confirm the hypothesis that the last layers have
the greatest influence."
OTHER,0.44642857142857145,"B
ADDITIONAL QUALITATIVE RESULTS ON RECONSTRUCTION"
OTHER,0.4486607142857143,"The proposed approach of WRanGAN demonstrates a clear advantage in terms of reproducing
unique details, as demonstrated in the additional examples of Figure 9 and Figure 10. Difference
maps in Figure 11 and Figure 12 more clearly show the areas of the image that differ from the
original, where the WRanGAN model has more accurately reproduced the unique details of facial
skin tones, wrinkles, clothing elements, complex hairstyles, and background elements, such as the
windows, friezes, pilasters, and clock in the case of churches. However, some unique details are not
completely restored even with the use of the new approach."
OTHER,0.45089285714285715,Published as a conference paper at ICOMP 2024
OTHER,0.453125,Reconstruction
OTHER,0.45535714285714285,"Real
E4E
Restyle
SG2 W+
HyperStyle
PTI
WRanGAN"
OTHER,0.4575892857142857,Difference
OTHER,0.45982142857142855,"map
Reconstruction
Difference"
OTHER,0.46205357142857145,"map
Reconstruction
Difference map"
OTHER,0.4642857142857143,"Figure 11: Qualitative reconstruction examples for FFHQ dataset with difference map, which rep-
resents pixel wise difference between reconstructed and original images. Map colors range from
purple to yellow - from exact match to maximal difference."
OTHER,0.46651785714285715,"C
INVESTIGATION OF INVERSION PROCEDURE RESULTS"
OTHER,0.46875,"The proposed methodology utilizes a reparametrization trick to work with randomized parameters.
Specifically, the generator parameter θg is represented as θg = µθ + ϵσθ, where ϵ is sampled from
a normal distribution. Figures 13, 14, 15, 16, 17, and Figures 18, 19, 20, 21, 22, provide examples
of the resulting distribution of ϵ for images taken from FFHQ and LSUN Church respectively. In
general, the parameter is close to normally distributed, albeit with a variance that is 10 times lower
than that which was specified at the Wrangan training stage for FFHQ, and 7 times lower for LSUN
Church. Despite this reduction, the proposed methodology is still successful."
OTHER,0.47098214285714285,"D
ADDITIONAL QUALITATIVE COMPARISONS FOR EDITING"
OTHER,0.4732142857142857,"We conducted additional comparative experiments of the proposed WRanGAN approach and the
PTI inversion method for the StyleGAN 2 model in two domains: the FFHQ domain, with semantic
directions corresponding to binary image attributes Shen et al. (2020), and the LSUN Church do-
main, with the first 4 vectors obtained by PCA approach H¨ark¨onen et al. (2020). The results were
captured in Figures 23, 24, 25, 26, and 27."
OTHER,0.47544642857142855,Published as a conference paper at ICOMP 2024
OTHER,0.47767857142857145,Reconstruction
OTHER,0.4799107142857143,"Real
E4E
Restyle
SG2 W+
PTI
WRanGAN"
OTHER,0.48214285714285715,Difference
OTHER,0.484375,"map
Reconstruction
Difference"
OTHER,0.48660714285714285,"map
Reconstruction
Difference map"
OTHER,0.4888392857142857,"Figure 12: Qualitative reconstruction examples for LSUN Church dataset with difference map,
which represents pixel wise difference between reconstructed and original images. Map colors range
from purple to yellow - from exact match to maximal difference."
OTHER,0.49107142857142855,Published as a conference paper at ICOMP 2024
OTHER,0.49330357142857145,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0"
OTHER,0.4955357142857143,Density
OTHER,0.49776785714285715,"Layer 7(mean = -0.0, std = 0.1)"
OTHER,0.5,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0"
OTHER,0.5022321428571429,Density
OTHER,0.5044642857142857,"Layer 8(mean = -0.0, std = 0.1)"
OTHER,0.5066964285714286,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0"
OTHER,0.5089285714285714,Density
OTHER,0.5111607142857143,"Layer 9(mean = 0.0, std = 0.1)"
OTHER,0.5133928571428571,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0"
OTHER,0.515625,Density
OTHER,0.5178571428571429,"Layer 10(mean = 0.0, std = 0.11)"
OTHER,0.5200892857142857,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.5223214285714286,Density
OTHER,0.5245535714285714,"Layer 11(mean = 0.0, std = 0.12)"
OTHER,0.5267857142857143,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.5290178571428571,Density
OTHER,0.53125,"Layer 12(mean = -0.0, std = 0.13)"
OTHER,0.5334821428571429,Figure 13: Distribution of parameter ϵ among randomized layers for image taken from FFHQ.
OTHER,0.5357142857142857,Published as a conference paper at ICOMP 2024
OTHER,0.5379464285714286,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0 1 2 3 4"
OTHER,0.5401785714285714,Density
OTHER,0.5424107142857143,"Layer 7(mean = -0.0, std = 0.09)"
OTHER,0.5446428571428571,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0 1 2 3 4"
OTHER,0.546875,Density
OTHER,0.5491071428571429,"Layer 8(mean = 0.0, std = 0.09)"
OTHER,0.5513392857142857,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0 1 2 3 4"
OTHER,0.5535714285714286,Density
OTHER,0.5558035714285714,"Layer 9(mean = 0.0, std = 0.1)"
OTHER,0.5580357142857143,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0"
OTHER,0.5602678571428571,Density
OTHER,0.5625,"Layer 10(mean = -0.0, std = 0.1)"
OTHER,0.5647321428571429,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.5669642857142857,Density
OTHER,0.5691964285714286,"Layer 11(mean = 0.0, std = 0.12)"
OTHER,0.5714285714285714,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.5736607142857143,Density
OTHER,0.5758928571428571,"Layer 12(mean = -0.0, std = 0.13)"
OTHER,0.578125,Figure 14: Distribution of parameter ϵ among randomized layers for image taken from FFHQ.
OTHER,0.5803571428571429,Published as a conference paper at ICOMP 2024
OTHER,0.5825892857142857,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.5848214285714286,Density
OTHER,0.5870535714285714,"Layer 7(mean = 0.0, std = 0.11)"
OTHER,0.5892857142857143,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.5915178571428571,Density
OTHER,0.59375,"Layer 8(mean = -0.0, std = 0.11)"
OTHER,0.5959821428571429,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.5982142857142857,Density
OTHER,0.6004464285714286,"Layer 9(mean = 0.0, std = 0.12)"
OTHER,0.6026785714285714,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.6049107142857143,Density
OTHER,0.6071428571428571,"Layer 10(mean = 0.0, std = 0.13)"
OTHER,0.609375,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5"
OTHER,0.6116071428571429,Density
OTHER,0.6138392857142857,"Layer 11(mean = 0.0, std = 0.14)"
OTHER,0.6160714285714286,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.6183035714285714,Density
OTHER,0.6205357142857143,"Layer 12(mean = -0.0, std = 0.14)"
OTHER,0.6227678571428571,Figure 15: Distribution of parameter ϵ among randomized layers for image taken from FFHQ.
OTHER,0.625,Published as a conference paper at ICOMP 2024
OTHER,0.6272321428571429,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.6294642857142857,Density
OTHER,0.6316964285714286,"Layer 7(mean = -0.0, std = 0.1)"
OTHER,0.6339285714285714,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0"
OTHER,0.6361607142857143,Density
OTHER,0.6383928571428571,"Layer 8(mean = 0.0, std = 0.11)"
OTHER,0.640625,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.6428571428571429,Density
OTHER,0.6450892857142857,"Layer 9(mean = 0.0, std = 0.12)"
OTHER,0.6473214285714286,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.6495535714285714,Density
OTHER,0.6517857142857143,"Layer 10(mean = 0.0, std = 0.12)"
OTHER,0.6540178571428571,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.65625,Density
OTHER,0.6584821428571429,"Layer 11(mean = 0.0, std = 0.14)"
OTHER,0.6607142857142857,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.6629464285714286,Density
OTHER,0.6651785714285714,"Layer 12(mean = 0.0, std = 0.13)"
OTHER,0.6674107142857143,Figure 16: Distribution of parameter ϵ among randomized layers for image taken from FFHQ.
OTHER,0.6696428571428571,Published as a conference paper at ICOMP 2024
OTHER,0.671875,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.6741071428571429,Density
OTHER,0.6763392857142857,"Layer 7(mean = 0.0, std = 0.11)"
OTHER,0.6785714285714286,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.6808035714285714,Density
OTHER,0.6830357142857143,"Layer 8(mean = 0.0, std = 0.11)"
OTHER,0.6852678571428571,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.6875,Density
OTHER,0.6897321428571429,"Layer 9(mean = 0.0, std = 0.12)"
OTHER,0.6919642857142857,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.6941964285714286,Density
OTHER,0.6964285714285714,"Layer 10(mean = 0.0, std = 0.13)"
OTHER,0.6986607142857143,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.7008928571428571,Density
OTHER,0.703125,"Layer 11(mean = 0.0, std = 0.14)"
OTHER,0.7053571428571429,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5"
OTHER,0.7075892857142857,Density
OTHER,0.7098214285714286,"Layer 12(mean = 0.0, std = 0.15)"
OTHER,0.7120535714285714,Figure 17: Distribution of parameter ϵ among randomized layers for image taken from FFHQ.
OTHER,0.7142857142857143,Published as a conference paper at ICOMP 2024
OTHER,0.7165178571428571,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0"
OTHER,0.71875,Density
OTHER,0.7209821428571429,"Layer 7(mean = -0.0, std = 0.12)"
OTHER,0.7232142857142857,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0"
OTHER,0.7254464285714286,Density
OTHER,0.7276785714285714,"Layer 8(mean = -0.0, std = 0.12)"
OTHER,0.7299107142857143,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.7321428571428571,Density
OTHER,0.734375,"Layer 9(mean = 0.0, std = 0.13)"
OTHER,0.7366071428571429,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.7388392857142857,Density
OTHER,0.7410714285714286,"Layer 10(mean = -0.0, std = 0.14)"
OTHER,0.7433035714285714,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.7455357142857143,Density
OTHER,0.7477678571428571,"Layer 11(mean = -0.0, std = 0.15)"
OTHER,0.75,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.7522321428571429,Density
OTHER,0.7544642857142857,"Layer 12(mean = -0.0, std = 0.16)"
OTHER,0.7566964285714286,"Figure 18: Distribution of parameter ϵ among randomized layers for image taken from LSUN
Church."
OTHER,0.7589285714285714,Published as a conference paper at ICOMP 2024
OTHER,0.7611607142857143,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.7633928571428571,Density
OTHER,0.765625,"Layer 7(mean = -0.0, std = 0.12)"
OTHER,0.7678571428571429,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.7700892857142857,Density
OTHER,0.7723214285714286,"Layer 8(mean = 0.0, std = 0.12)"
OTHER,0.7745535714285714,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5"
OTHER,0.7767857142857143,Density
OTHER,0.7790178571428571,"Layer 9(mean = -0.0, std = 0.14)"
OTHER,0.78125,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5"
OTHER,0.7834821428571429,Density
OTHER,0.7857142857142857,"Layer 10(mean = 0.0, std = 0.15)"
OTHER,0.7879464285714286,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0"
OTHER,0.7901785714285714,Density
OTHER,0.7924107142857143,"Layer 11(mean = -0.0, std = 0.17)"
OTHER,0.7946428571428571,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5"
OTHER,0.796875,Density
OTHER,0.7991071428571429,"Layer 12(mean = 0.0, std = 0.17)"
OTHER,0.8013392857142857,"Figure 19: Distribution of parameter ϵ among randomized layers for image taken from LSUN
Church."
OTHER,0.8035714285714286,Published as a conference paper at ICOMP 2024
OTHER,0.8058035714285714,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.8080357142857143,Density
OTHER,0.8102678571428571,"Layer 7(mean = 0.0, std = 0.14)"
OTHER,0.8125,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
OTHER,0.8147321428571429,Density
OTHER,0.8169642857142857,"Layer 8(mean = -0.0, std = 0.13)"
OTHER,0.8191964285714286,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.8214285714285714,Density
OTHER,0.8236607142857143,"Layer 9(mean = -0.0, std = 0.14)"
OTHER,0.8258928571428571,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5"
OTHER,0.828125,Density
OTHER,0.8303571428571429,"Layer 10(mean = -0.0, std = 0.15)"
OTHER,0.8325892857142857,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0"
OTHER,0.8348214285714286,Density
OTHER,0.8370535714285714,"Layer 11(mean = -0.0, std = 0.16)"
OTHER,0.8392857142857143,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5"
OTHER,0.8415178571428571,Density
OTHER,0.84375,"Layer 12(mean = -0.0, std = 0.17)"
OTHER,0.8459821428571429,"Figure 20: Distribution of parameter ϵ among randomized layers for image taken from LSUN
Church."
OTHER,0.8482142857142857,Published as a conference paper at ICOMP 2024
OTHER,0.8504464285714286,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.8526785714285714,Density
OTHER,0.8549107142857143,"Layer 7(mean = 0.0, std = 0.14)"
OTHER,0.8571428571428571,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.859375,Density
OTHER,0.8616071428571429,"Layer 8(mean = -0.0, std = 0.14)"
OTHER,0.8638392857142857,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5"
OTHER,0.8660714285714286,Density
OTHER,0.8683035714285714,"Layer 9(mean = 0.0, std = 0.15)"
OTHER,0.8705357142857143,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0"
OTHER,0.8727678571428571,Density
OTHER,0.875,"Layer 10(mean = 0.0, std = 0.17)"
OTHER,0.8772321428571429,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00"
OTHER,0.8794642857142857,Density
OTHER,0.8816964285714286,"Layer 11(mean = 0.0, std = 0.18)"
OTHER,0.8839285714285714,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0"
OTHER,0.8861607142857143,Density
OTHER,0.8883928571428571,"Layer 12(mean = 0.0, std = 0.18)"
OTHER,0.890625,"Figure 21: Distribution of parameter ϵ among randomized layers for image taken from LSUN
Church."
OTHER,0.8928571428571429,Published as a conference paper at ICOMP 2024
OTHER,0.8950892857142857,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.8973214285714286,Density
OTHER,0.8995535714285714,"Layer 7(mean = 0.0, std = 0.13)"
OTHER,0.9017857142857143,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
OTHER,0.9040178571428571,Density
OTHER,0.90625,"Layer 8(mean = 0.0, std = 0.13)"
OTHER,0.9084821428571429,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5"
OTHER,0.9107142857142857,Density
OTHER,0.9129464285714286,"Layer 9(mean = 0.0, std = 0.15)"
OTHER,0.9151785714285714,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0"
OTHER,0.9174107142857143,Density
OTHER,0.9196428571428571,"Layer 10(mean = 0.0, std = 0.16)"
OTHER,0.921875,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0"
OTHER,0.9241071428571429,Density
OTHER,0.9263392857142857,"Layer 11(mean = 0.0, std = 0.17)"
OTHER,0.9285714285714286,"1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
0.0 0.5 1.0 1.5 2.0 2.5"
OTHER,0.9308035714285714,Density
OTHER,0.9330357142857143,"Layer 12(mean = 0.0, std = 0.16)"
OTHER,0.9352678571428571,"Figure 22: Distribution of parameter ϵ among randomized layers for image taken from LSUN
Church."
OTHER,0.9375,Published as a conference paper at ICOMP 2024 PTI Real
OTHER,0.9397321428571429,WRanGAN
OTHER,0.9419642857142857,Inversion Mouth
OTHER,0.9441964285714286,"open
Eyeglasses
Gender
Smile
Age"
OTHER,0.9464285714285714,"PTI
WRanGAN
PTI
WRanGAN"
OTHER,0.9486607142857143,Figure 23: Qualitative editing comparisons for FFHQ dataset
OTHER,0.9508928571428571,Published as a conference paper at ICOMP 2024 PTI Real
OTHER,0.953125,WRanGAN
OTHER,0.9553571428571429,Inversion Mouth
OTHER,0.9575892857142857,"open
Eyeglasses
Gender
Smile
Age"
OTHER,0.9598214285714286,"PTI
WRanGAN
PTI
WRanGAN"
OTHER,0.9620535714285714,Figure 24: Qualitative editing comparisons for FFHQ dataset
OTHER,0.9642857142857143,Published as a conference paper at ICOMP 2024 PTI Real
OTHER,0.9665178571428571,WRanGAN
OTHER,0.96875,Inversion Mouth
OTHER,0.9709821428571429,"open
Eyeglasses
Gender
Smile
Age"
OTHER,0.9732142857142857,"PTI
WRanGAN
PTI
WRanGAN"
OTHER,0.9754464285714286,Figure 25: Qualitative editing comparisons for FFHQ dataset
OTHER,0.9776785714285714,Published as a conference paper at ICOMP 2024 PTI Real
OTHER,0.9799107142857143,WRanGAN
OTHER,0.9821428571428571,"Inversion
Factor 1
Factor 2
Factor 3
Factor 4"
OTHER,0.984375,"PTI
WRanGAN
PTI
WRanGAN"
OTHER,0.9866071428571429,Figure 26: Qualitative editing comparisons for LSUN Church
OTHER,0.9888392857142857,Published as a conference paper at ICOMP 2024 PTI Real
OTHER,0.9910714285714286,WRanGAN
OTHER,0.9933035714285714,"Inversion
Factor 1
Factor 2
Factor 3
Factor 4"
OTHER,0.9955357142857143,"PTI
WRanGAN
PTI
WRanGAN"
OTHER,0.9977678571428571,Figure 27: Qualitative editing comparisons for LSUN Church
