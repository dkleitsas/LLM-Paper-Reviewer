Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008880994671403197,"Neural Network Pruning has been established as driving force in the exploration of
1"
ABSTRACT,0.0017761989342806395,"memory and energy efficient solutions with high throughput both during training
2"
ABSTRACT,0.0026642984014209592,"and at test time. In this paper, we introduce a novel criterion for model com-
3"
ABSTRACT,0.003552397868561279,"pression, named “Expressiveness"". Unlike existing pruning methods that rely
4"
ABSTRACT,0.004440497335701598,"on the inherent “Importance"" of neurons’ and filters’ weights, “Expressiveness""
5"
ABSTRACT,0.0053285968028419185,"emphasizes a neuron’s or group of neurons ability to redistribute informational
6"
ABSTRACT,0.006216696269982238,"resources effectively, based on the overlap of activations. This characteristic is
7"
ABSTRACT,0.007104795737122558,"strongly correlated to a network’s initialization state, establishing criterion auton-
8"
ABSTRACT,0.007992895204262877,"omy from the learning state (stateless) and thus setting a new fundamental basis
9"
ABSTRACT,0.008880994671403197,"for the expansion of compression strategies in regards to the “When to Prune""
10"
ABSTRACT,0.009769094138543518,"question. We show that expressiveness is effectively approximated with arbitrary
11"
ABSTRACT,0.010657193605683837,"data or limited dataset’s representative samples, making ground for the exploration
12"
ABSTRACT,0.011545293072824156,"of Data-Agnostic strategies. Our work also facilitates a “hybrid"" formulation of
13"
ABSTRACT,0.012433392539964476,"expressiveness and importance-based pruning strategies, illustrating their com-
14"
ABSTRACT,0.013321492007104795,"plementary benefits and delivering up to 10× extra gains w.r.t. weight-based
15"
ABSTRACT,0.014209591474245116,"approaches in parameter compression ratios, with an average of 1% in performance
16"
ABSTRACT,0.015097690941385435,"degradation. We also show that employing expressiveness (independently) for
17"
ABSTRACT,0.015985790408525755,"pruning leads to an improvement over top-performing and foundational methods in
18"
ABSTRACT,0.016873889875666074,"terms of compression efficiency. Finally, on YOLOv8, we achieve a 46.1% MACs
19"
ABSTRACT,0.017761989342806393,"reduction by removing 55.4% of the parameters, with an increase of 3% in the
20"
ABSTRACT,0.018650088809946713,"mean Absolute Precision (mAP50−95) for object detection on COCO dataset.
21"
INTRODUCTION,0.019538188277087035,"1
Introduction
22"
INTRODUCTION,0.020426287744227355,"To address the computational constraints of existing models, Model Compression [7] has emerged as
23"
INTRODUCTION,0.021314387211367674,"a prominent solution in exploring models that achieve comparable performance, but with reduced
24"
INTRODUCTION,0.022202486678507993,"computational complexity [52]. Within this scope, Floating Point Operations (FLOPs) are used to
25"
INTRODUCTION,0.023090586145648313,"estimate a model’s computational complexity, by measuring the arithmetic operations required for a
26"
INTRODUCTION,0.023978685612788632,"forward pass, while parameters (params) are associated with a model’s size in terms of memory space
27"
INTRODUCTION,0.02486678507992895,"[48] and their reduction can be a precursor towards more energy efficient solutions [5]. Although
28"
INTRODUCTION,0.02575488454706927,"FLOPs and params often correlate, their relationship isn’t strictly linear. For instance, VGG16 [43]
29"
INTRODUCTION,0.02664298401420959,"has 17× more parameters than ResNet-56 [17] but only 3× more FLOPs, largely due to VGG16’s
30"
INTRODUCTION,0.027531083481349913,"extensive use of fully connected layers. At first sight, this can be attributed to the differences in
31"
INTRODUCTION,0.028419182948490232,"network topologies. From a deeper perspective, the intricacies of various operations at handling
32"
INTRODUCTION,0.02930728241563055,"computational workloads, such as residual structures [17, 55], depthwise separable convolutions [19],
33"
INTRODUCTION,0.03019538188277087,"inverted residual modules [18], channel shuffle operations [59] and shift operations [53], coupled
34"
INTRODUCTION,0.03108348134991119,"with their interplay, may significantly affect the relationship between FLOPs and params in a neural
35"
INTRODUCTION,0.03197158081705151,"network. In a nutshell, besides the use of more computationally efficient operations as above-
36"
INTRODUCTION,0.03285968028419183,"mentioned, Model Compression aims to maintain model performance while optimizing the two
37"
INTRODUCTION,0.03374777975133215,"aforementioned metrics via tensor decomposition, data quantization, and network sparsification [7].
38"
INTRODUCTION,0.03463587921847247,"In this paper we emphasize on the sparsification strategy of pruning [49], which we use as a basis
39"
INTRODUCTION,0.035523978685612786,"framework to introduce “Expressiveness"" as a new criterion for compressing neural networks.
40"
INTRODUCTION,0.03641207815275311,"Existing pruning methods focus on removing redundant network elements – be they weights, neurons,
41"
INTRODUCTION,0.037300177619893425,"or structures of neurons – in ways that minimally affect the overall performance of a network, based
42"
INTRODUCTION,0.03818827708703375,"on the criterion of “Importance"", e.g. [38, 58, 20, 30]. Importance-based methods address questions
43"
INTRODUCTION,0.03907637655417407,"like “How much does the removal of a network’s element cost in terms of performance degradation?""
44"
INTRODUCTION,0.03996447602131439,"and “How much information does a network element contain?"" in various ways. More specifically,
45"
INTRODUCTION,0.04085257548845471,"they are motivated by the information inherent in network elements, such as the magnitude of weights
46"
INTRODUCTION,0.041740674955595025,"[15, 28], similarity of weights or weight matrices [29, 60] ; and their sensitivity to the network’s loss
47"
INTRODUCTION,0.04262877442273535,"function, such as the magnitude of gradients [38] and more [49, 3]. Such dependencies on weights’
48"
INTRODUCTION,0.043516873889875664,"distributions constitute the aforementioned pruning methods to be “data-aware"" since they intrinsically
49"
INTRODUCTION,0.04440497335701599,"rely on the input data and the information state of the model, making the importance estimation
50"
INTRODUCTION,0.0452930728241563,"of the network’s elements challenging and often costly due to factors like i) the stochasticity from
51"
INTRODUCTION,0.046181172291296625,"training with minibatches, ii) the presence of plateau areas in the optimization space, and iii) the
52"
INTRODUCTION,0.04706927175843695,"complexity introduced by nonlinearities [38]. Liu et al. [36] have also discussed limitations in the
53"
INTRODUCTION,0.047957371225577264,"perception of importance within trained models, i.e. the authors criticize the ability of network’s
54"
INTRODUCTION,0.04884547069271759,"elements importance to generalize to pruned derivatives, while also questioning the necessity of
55"
INTRODUCTION,0.0497335701598579,"training large-scale models prior pruning.
56"
INTRODUCTION,0.050621669626998225,"Inspired by the concepts of “Information Plasticity"" [2] and the “Lottery Ticket Hypothesis"" (LTH)
57"
INTRODUCTION,0.05150976909413854,"[12], we aim to address the limitations of previous importance-based methods through elaborating
58"
INTRODUCTION,0.052397868561278864,"the “Expressiveness"" criterion in model compression. In contrast to “Importance"", we focus on
59"
INTRODUCTION,0.05328596802841918,"understanding the capability of network elements to redistribute informational resources to subsequent
60"
INTRODUCTION,0.0541740674955595,"network elements. We define “Expressiveness"" as - “A neuron’s or group’s of neurons potential
61"
INTRODUCTION,0.055062166962699825,"(when a network is not fully trained) or ability (when it is trained) to extract features that maximally
62"
INTRODUCTION,0.05595026642984014,"separate different samples"". As derived by [2], the early training phase of a model is crucial in
63"
INTRODUCTION,0.056838365896980464,"shaping its expressiveness, with the formation of critical paths —strong connections that determine
64"
INTRODUCTION,0.05772646536412078,"the “workload distribution""— being particularly significant during these initial stages. It’s essential to
65"
INTRODUCTION,0.0586145648312611,"note that the network’s initialization state influences the formation of those paths, which interestingly
66"
INTRODUCTION,0.05950266429840142,"enables ""Expressiveness"" to be a fit criterion for compression during all time instances of a networks’
67"
INTRODUCTION,0.06039076376554174,"convergence [12], setting a baseline for answering the question of ""When to prune?"" [42]. Our
68"
INTRODUCTION,0.06127886323268206,"proposed pruning metric centers on measuring the overlap of activations between datapoints of the
69"
INTRODUCTION,0.06216696269982238,"feature space. In that way, expressiveness is based on effectively evaluating the inherent ability of the
70"
INTRODUCTION,0.0630550621669627,"network’s neurons to differentiate sub-spaces within the feature space. We experimentally show that
71"
INTRODUCTION,0.06394316163410302,"utilizing either small sets of arbitrary data points from the feature space or stratified sampling [34]
72"
INTRODUCTION,0.06483126110124333,"from each class yields consistent estimations of expressiveness. Finally, we propose and implement a
73"
INTRODUCTION,0.06571936056838366,"new “hybrid"" pruning optimization strategy that cooperatively searches, exploits and characterizes
74"
INTRODUCTION,0.06660746003552398,"the complementary benefits between “Importance"" and “Expressiveness"" for model compression.
75"
INTRODUCTION,0.0674955595026643,"In summary, this work offers the following four-fold contribution: (i) we propose Expressiveness,
76"
INTRODUCTION,0.06838365896980461,"a novel criterion based on the overlap of activations for model compression; (ii) we provide an
77"
INTRODUCTION,0.06927175843694494,"in-depth theoretical analysis of both the fundamental principles and the technical intricacies of the
78"
INTRODUCTION,0.07015985790408526,"proposed criterion; (iii) we validate the hypothesis that Expressiveness can be approximated with
79"
INTRODUCTION,0.07104795737122557,"little to none input data, opening the road for data-agnostic pruning strategies; and (iv) through
80"
INTRODUCTION,0.0719360568383659,"extensive experimentation we offer a thorough comparison w.r.t to both foundational and state-of-
81"
INTRODUCTION,0.07282415630550622,"the-art methods demonstrating the efficiency and effectiveness of the proposed technique in model
82"
INTRODUCTION,0.07371225577264653,"compression, while also examining the feasibility and effectiveness of a “hybrid"" expressiveness-
83"
INTRODUCTION,0.07460035523978685,"importance pruning strategy.
84"
INTRODUCTION,0.07548845470692718,"Specifically, we validate “Expressiveness"" on the CIFAR-10 [24] and ImageNet [40] datasets using
85"
INTRODUCTION,0.0763765541740675,"a variety of models with different design characteristics [44, 17, 45, 21, 19]. We demonstrate the
86"
INTRODUCTION,0.07726465364120781,"superiority of our novel criterion over existing solutions, including many top performing structural
87"
INTRODUCTION,0.07815275310834814,"pruning methods [31, 61, 58, 32, 23, 46, 11], and show significant params reduction while maintaining
88"
INTRODUCTION,0.07904085257548846,"comparable performance. We experimentally explore and analyze the complementary nature of
89"
INTRODUCTION,0.07992895204262877,"expressiveness and importance, showing that summary numeric evaluation provides up to 10×
90"
INTRODUCTION,0.08081705150976909,"additional parameter compression ratio gains, with an average of 1% loss decrease w.r.t group ℓ1-
91"
INTRODUCTION,0.08170515097690942,"norm [28]. Finally, we experiment on the current state-of-the-art computer vision model (YOLOv8
92"
INTRODUCTION,0.08259325044404973,"[9, 22]), showcasing notable compression rates of 53.9% together with performance gains of 3% on
93"
INTRODUCTION,0.08348134991119005,"the COCO dataset [33], and highlighting the ability of more expressive neurons to better recover lost
94"
INTRODUCTION,0.08436944937833037,"information from the pruning operation.
95"
RELATED WORK,0.0852575488454707,"2
Related Work
96"
RELATED WORK,0.08614564831261101,"Weight (Non-Structural) Importance. Han et al. [15, 14] and Guo et al. [13] approached the
97"
RELATED WORK,0.08703374777975133,"importance of weights based on their magnitude, removing connections below given thresholds.
98"
RELATED WORK,0.08792184724689166,"However, earlier works [25, 16] emphasized on the Hessian of the loss and have questioned whether
99"
RELATED WORK,0.08880994671403197,"magnitude is a reliable indicator of weight’s importance, as small weights can be necessary for
100"
RELATED WORK,0.08969804618117229,"low error. In this direction, several studies [4, 47, 41, 8] have proposed strategies of iterative
101"
RELATED WORK,0.0905861456483126,"magnitude pruning, in the form of “adaptive weight importance"", where weights are ranked based on
102"
RELATED WORK,0.09147424511545293,"their sensitivity to the loss. From a different perspective, Yang et al. [56] address the limitations of
103"
RELATED WORK,0.09236234458259325,"individual weight’s saliency that fail to account for their collective influence and provide a formulation
104"
RELATED WORK,0.09325044404973357,"of weight’s importance based on the error minimization of the output feature maps. Expanding on this
105"
RELATED WORK,0.0941385435168739,"concept, Xu et al. [54] propose a layer-adaptive pruning scheme that encapsulates the intra-relation
106"
RELATED WORK,0.09502664298401421,"of weights between layers, focusing on minimizing the output distortion of the network. Amongst
107"
RELATED WORK,0.09591474245115453,"other factors and limitations (as also discussed in 1), weight importance is very expensive to measure,
108"
RELATED WORK,0.09680284191829484,"mainly because of the increased complexity induced by the mutual influences of the weights among
109"
RELATED WORK,0.09769094138543517,"interconnected neurons. This, coupled with the requirement for specialized hardware to manage the
110"
RELATED WORK,0.09857904085257549,"irregular sparsity patterns resulting from weight pruning [57], has shifted research focus towards
111"
RELATED WORK,0.0994671403197158,"structural pruning [28], where neurons or entire filters are removed.
112"
RELATED WORK,0.10035523978685613,"Neuron and Filter (Structural) Importance. Many where driven by the success of Iterative
113"
RELATED WORK,0.10124333925399645,"Shrinkage and Thresholding Algorithms (ISTA) [6] in non-structural sparse pruning and proposed
114"
RELATED WORK,0.10213143872113677,"filter-level adaptations [28, 29, 32, 26], based on the relaxation (ℓ1 and ℓ2) of ℓ0 norm minimization.
115"
RELATED WORK,0.10301953818827708,"However, the loss of universality of such magnitude-based methods remains a limitation in the
116"
RELATED WORK,0.10390763765541741,"approximation of importance even in the structural scope. Yu et al. [58] further elaborate on the
117"
RELATED WORK,0.10479573712255773,"idea of error propagation ignorance, where the analysis is limited to the statistical properties of a
118"
RELATED WORK,0.10568383658969804,"single [28, 29] or two consecutive layers [37]. The authors suggest that the importance of neurons
119"
RELATED WORK,0.10657193605683836,"is better approximated from the minimization of the reconstruction error in the final response layer
120"
RELATED WORK,0.10746003552397869,"from which it is propagated to previous layers. In contrast to this view, Zhuang et al. [61] emphasize
121"
RELATED WORK,0.108348134991119,"on the discriminative power of a filter as a more effective measure of importance and highlight that
122"
RELATED WORK,0.10923623445825932,"this aspect is not effectively assessed by the minimization of the reconstruction error. In a manner
123"
RELATED WORK,0.11012433392539965,"that reflects the progression of weight importance, Molchanov et al. [38] define “adaptive filter
124"
RELATED WORK,0.11101243339253997,"importance"" as the squared change in loss and apply first and second-order Taylor expansions to
125"
RELATED WORK,0.11190053285968028,"accelerate importance’s computations. Predominantly, the data-awareness imposed by most pruning
126"
RELATED WORK,0.1127886323268206,"strategies is added to their already high-complexity – i.e. mostly non-convex, NP-Hard problems
127"
RELATED WORK,0.11367673179396093,"that require combinatorial searches. This renders the estimation of importance both computationally
128"
RELATED WORK,0.11456483126110124,"expensive and labor-intensive, similarly to non-structural approaches. Notably, Lin et al. [30] propose
129"
RELATED WORK,0.11545293072824156,"a less data-dependent solution based on the observation that the average rank of multiple feature maps
130"
RELATED WORK,0.11634103019538189,"generated by a single filter remains constant. HRank [30], alongside several other feature-guided
131"
RELATED WORK,0.1172291296625222,"filter pruning approaches, are valuable indicators towards data independence. Such works form a
132"
RELATED WORK,0.11811722912966252,"principle that pruning elements are better evaluated in the activation phase, where the importance of
133"
RELATED WORK,0.11900532859680284,"information and the richness of characteristics for both input data and filters are better reflected. In
134"
RELATED WORK,0.11989342806394317,"this work, we expand on this belief and we through extensive experimental analysis, we demonstrate
135"
RELATED WORK,0.12078152753108348,"that neither the information state nor the input data is required for the discriminative characterization
136"
RELATED WORK,0.1216696269982238,"of an element.
137"
NEURAL EXPRESSIVENESS,0.12255772646536411,"3
Neural Expressiveness
138"
NEURAL EXPRESSIVENESS,0.12344582593250444,"3.1
Weights and Activations: Importance vs Expressiveness
139"
NEURAL EXPRESSIVENESS,0.12433392539964476,"Neurons are the main constituent element of a neural network. Given a neural network N, we
140"
NEURAL EXPRESSIVENESS,0.12522202486678508,"denote neurons by a(l)
i , where l∈L is indicative of the neuron’s layer in a network with L =
141"
NEURAL EXPRESSIVENESS,0.1261101243339254,"{l0, ..., ll, ..., l|L|} layers and i of its position in the given layer l = {a0, ..., ai, ..., a|l|}. Another
142"
NEURAL EXPRESSIVENESS,0.1269982238010657,"important element are the learning parameters of the network. Otherwise the weights represent the
143"
NEURAL EXPRESSIVENESS,0.12788632326820604,"strength of connections between neurons in adjacent layers and are denoted by w(l)
ij , where i and j
144"
NEURAL EXPRESSIVENESS,0.12877442273534637,"index the neurons in the current and previous layers. In that manner, neuron’s can be perceived as
145"
NEURAL EXPRESSIVENESS,0.12966252220248667,"switches that allow or block information from propagating through-out a network. The activation
146"
NEURAL EXPRESSIVENESS,0.130550621669627,"(or not) of a neuron a(l)
i
depends on the output value of its activation function σ(·), where there are
147"
NEURAL EXPRESSIVENESS,0.13143872113676733,"many popular options for the definition of σ, e.g., sigmoid, tanh, and ReLU functions. Specifically, a
148"
NEURAL EXPRESSIVENESS,0.13232682060390763,"neuron’s output is defined as follows,
149"
NEURAL EXPRESSIVENESS,0.13321492007104796,"a(l)
i
= σ X"
NEURAL EXPRESSIVENESS,0.1341030195381883,"j
w(l)
ij a(l−1)
j
+ b(l)
i ! (1)"
NEURAL EXPRESSIVENESS,0.1349911190053286,"where b(l)
i
denotes the bias term. From eq. 1, we observe that a neuron’s activation is affected by
150"
NEURAL EXPRESSIVENESS,0.13587921847246892,"the activation of the previous layers, hence affecting in the same way the consecutive layers. This
151"
NEURAL EXPRESSIVENESS,0.13676731793960922,"interdependence between activations a(l), for a given layer l defines a recurrent form that can be
152"
NEURAL EXPRESSIVENESS,0.13765541740674955,"generalized as follows,
153"
NEURAL EXPRESSIVENESS,0.13854351687388988,"a(l) = σ

W (l)f

a(l−2), . . . , a(1)
+ b(l)
.
(2)"
NEURAL EXPRESSIVENESS,0.13943161634103018,"On the other hand, weights are a more static representation of information as they modulate how
154"
NEURAL EXPRESSIVENESS,0.14031971580817051,"much influence one neuron’s activation has on another’s, compared to activations that control the
155"
NEURAL EXPRESSIVENESS,0.14120781527531084,"flow of information in a network. This differentiation has motivated us to define two axes of study in
156"
NEURAL EXPRESSIVENESS,0.14209591474245115,"the categorisation of pruning criteria, one based on the weights (“importance"") and one based on the
157"
NEURAL EXPRESSIVENESS,0.14298401420959148,"activation phase (“expressiveness"").
158"
NEURAL EXPRESSIVENESS,0.1438721136767318,"Generalization of concepts in a structural level.
The aforementioned principles extend to the
159"
NEURAL EXPRESSIVENESS,0.1447602131438721,"structural representations of weights and activations, the most common being Convolutional Neural
160"
NEURAL EXPRESSIVENESS,0.14564831261101244,"Networks (CNNs). For a CNN model with a set of K convolutional layers, where Cl is the l −th
161"
NEURAL EXPRESSIVENESS,0.14653641207815277,"convolutional layer. We denote filters (weight maps) and feature maps (activation maps) as F l
k and
162"
NEURAL EXPRESSIVENESS,0.14742451154529307,"Cl
k respectively, where k the is index within a layer. Given filter with dimensions m × n, eq. 1 is
163"
NEURAL EXPRESSIVENESS,0.1483126110124334,"adapted as follows,
164"
NEURAL EXPRESSIVENESS,0.1492007104795737,"C(l)
k (x, y) = σ m
X i=1 n
X"
NEURAL EXPRESSIVENESS,0.15008880994671403,"j=1
F (l,k)
ij
a(l−1)
x+i−1,y+j−1 + b(l)
k ! (3)"
NEURAL EXPRESSIVENESS,0.15097690941385436,"where (i, j) and (x, y) are the coordinates of weights and output activations within the filter and the
165"
NEURAL EXPRESSIVENESS,0.15186500888099466,"output activation map respectively. Similarly, a convolution layer l can be analyticaly expressed as
166"
NEURAL EXPRESSIVENESS,0.152753108348135,"follows,
167"
NEURAL EXPRESSIVENESS,0.15364120781527532,"C(l) = 
 "
NEURAL EXPRESSIVENESS,0.15452930728241562,"σ
LK(1)"
NEURAL EXPRESSIVENESS,0.15541740674955595,"k=1 F (1,k) ∗X + B(1)
if l = 1"
NEURAL EXPRESSIVENESS,0.15630550621669628,"σ
LK(l)"
NEURAL EXPRESSIVENESS,0.15719360568383658,"k=1 F (l,k) ∗C(l−1) + B(l)
if l > 1
(4)"
NEURAL EXPRESSIVENESS,0.15808170515097691,"with X being the input to the first layer of the network, and where symbol ∗denotes convolution
168"
NEURAL EXPRESSIVENESS,0.15896980461811722,"operation and L denotes the concatenation operation. Within this context1, eq. 2 is generalized as
169"
NEURAL EXPRESSIVENESS,0.15985790408525755,"follows,
170"
NEURAL EXPRESSIVENESS,0.16074600355239788,C(l) = σ  
NEURAL EXPRESSIVENESS,0.16163410301953818,"K(l)
M"
NEURAL EXPRESSIVENESS,0.1625222024866785,"k=1
F (l,k) ∗f

C(l−2), . . . , C(1)
+ B(l) "
NEURAL EXPRESSIVENESS,0.16341030195381884,".
(5)"
NEURAL EXPRESSIVENESS,0.16429840142095914,"Conceptualization of information propagation.
Consider a task with X = {xi}|D|
i=1 denoting
171"
NEURAL EXPRESSIVENESS,0.16518650088809947,"dataset samples, where |D| is the size of the dataset. Given the information state (weight state) of
172"
NEURAL EXPRESSIVENESS,0.1660746003552398,"a CNN model with K convolutional layers at a given time ti, X is mapped through the network as
173"
NEURAL EXPRESSIVENESS,0.1669626998223801,"f(X, Wti), where Wti = {F 1
ti, . . . , F l
ti, . . . , F |K|
ti
} and F l
ti = {F (l,1)
ti
, . . . , F (l,k)
ti
, . . . , F (l,K(l))
ti
},
174"
NEURAL EXPRESSIVENESS,0.16785079928952043,"with K(l) being the amount of weight maps (filters) in a given layer l. This process can be further
175"
NEURAL EXPRESSIVENESS,0.16873889875666073,"analyzed as follows,
176"
NEURAL EXPRESSIVENESS,0.16962699822380106,"f(X, Wti) = F|K|(F|K|−1(. . . F1(X; F1
ti); F2
ti); . . . ; F|K|
ti ),
(6)"
NEURAL EXPRESSIVENESS,0.1705150976909414,"where Fl represents the mapping operation of convolutional layer l.
177"
NEURAL EXPRESSIVENESS,0.1714031971580817,"Based on eq. 2 and eq. 5, the equivalent of the previous based on the activations of the layers can be
178"
NEURAL EXPRESSIVENESS,0.17229129662522202,"expressed as,
179"
NEURAL EXPRESSIVENESS,0.17317939609236235,"f(X, Wti) = C(|K|) 
. . .

C(2) 
C(1)  
X, F1
ti

, F2
ti

. . .

, F|K|
ti

.
(7)"
NEURAL EXPRESSIVENESS,0.17406749555950266,"1We do not include pooling and batch normalization layers in the formulations; however, the equations can
be expanded to incorporate them as intermediate steps based on each architecture."
NEURAL EXPRESSIVENESS,0.17495559502664298,"Here, C(l) represents the activation map of the l-th layer, where C(l) = Fl(C(l−1); Fl
ti) aligns
180"
NEURAL EXPRESSIVENESS,0.17584369449378331,"with the structure defined in eq. 4. In this formulation, C(1) is the activation map of the first layer,
181"
NEURAL EXPRESSIVENESS,0.17673179396092362,"computed using the input X and the first layer’s filters F1
ti. Subsequent layers’ activation maps
182"
NEURAL EXPRESSIVENESS,0.17761989342806395,"C(l) are derived from the previous layer’s output C(l−1) and their respective filters Fl
ti. Assuming a
183"
NEURAL EXPRESSIVENESS,0.17850799289520428,"classification task, the final layer C(|K|) is considered the classification layer, effectively summarizing
184"
NEURAL EXPRESSIVENESS,0.17939609236234458,"the hierarchical feature extraction and transformation process across all convolutional layers.
185"
NEURAL EXPRESSIVENESS,0.1802841918294849,"3.2
Mathematical Foundation of Neural Expressiveness.
186"
NEURAL EXPRESSIVENESS,0.1811722912966252,"We observe that the training parameters of the model, in this case Wti
2, are responsible
187"
NEURAL EXPRESSIVENESS,0.18206039076376554,"for transforming the original input feature space X into a sequence of intermediate feature
188"
NEURAL EXPRESSIVENESS,0.18294849023090587,"spaces{C(1), . . . , C(|K|−1)}, progressing towards the final prediction formulated by the prediction
189"
NEURAL EXPRESSIVENESS,0.18383658969804617,"layer C(|K|).
190"
NEURAL EXPRESSIVENESS,0.1847246891651865,"Based on this intrinsic characteristic of neural networks and inspired by the goal of optimizing
191"
NEURAL EXPRESSIVENESS,0.18561278863232683,"feature discrimination, akin to the entropy reduction strategy in decision trees [51], we assess network
192"
NEURAL EXPRESSIVENESS,0.18650088809946713,"elements ability, in this scenario filters, to extract features, i.e., activation patterns, that maximally
193"
NEURAL EXPRESSIVENESS,0.18738898756660746,"separate different input samples xi. In other words, we score the expressiveness of the filters within
194"
NEURAL EXPRESSIVENESS,0.1882770870337478,"Wti, based on the discriminative quality of the intermediate feature spaces they generate, where the
195"
NEURAL EXPRESSIVENESS,0.1891651865008881,"feature space generated by a filter F l
k, is denoted as Cl
k.
196"
NEURAL EXPRESSIVENESS,0.19005328596802842,"Neural Expressiveness foundational concept.
When assessing the expressiveness of an element
197"
NEURAL EXPRESSIVENESS,0.19094138543516873,"within Wti based on its generated feature spaces, e.g., NEXP(F l
ti; Cl), we cooperatively evaluate
198"
NEURAL EXPRESSIVENESS,0.19182948490230906,"all of its preceding elements, as derived from eq. 5. This can be formulated as,
199"
NEURAL EXPRESSIVENESS,0.19271758436944939,"NEXP(F l
ti; Cl) = NEXP(F l
ti; (C(l−1), C(l−2), . . . , C(1))),
(8)"
NEURAL EXPRESSIVENESS,0.1936056838365897,"which can be further extended to incorporate the inter-dependencies between the examined element
200"
NEURAL EXPRESSIVENESS,0.19449378330373002,"and its predecessors, in accordance with eq. 7, as detailed below:
201"
NEURAL EXPRESSIVENESS,0.19538188277087035,"NEXP

F l
ti;

C(l−1), C(l−2), . . . , C(1)
="
NEURAL EXPRESSIVENESS,0.19626998223801065,"NEXP

F l
ti;

(C(l−2), Fl−1
ti ), (C(l−3), Fl−2
ti ), . . . , (X, F1
ti)

.
(9)"
NEURAL EXPRESSIVENESS,0.19715808170515098,"The aforementioned eqs. 8 and 9 provide the foundational concepts for utilizing the evaluation of
202"
NEURAL EXPRESSIVENESS,0.1980461811722913,"the activation phase, in an endeavor to encourage the development of more universal solutions by
203"
NEURAL EXPRESSIVENESS,0.1989342806394316,"addressing the limitations of universality inherent in the assessment of the weight state alone (as also
204"
NEURAL EXPRESSIVENESS,0.19982238010657194,"discussed in sections 1 and 2).
205"
NEURAL EXPRESSIVENESS,0.20071047957371227,"Formulation of Neural Expressiveness (NEXP) Score.
Diving deeper into the Neural Expressive-
206"
NEURAL EXPRESSIVENESS,0.20159857904085257,"ness (NEXP) scoring process, we follow eq. 9 previously and assume a mini-batch X
′ = {x
′
i}N
i=1,
207"
NEURAL EXPRESSIVENESS,0.2024866785079929,"with N being the number of samples in it.
Mapping the batch through the network, based
208"
NEURAL EXPRESSIVENESS,0.2033747779751332,"on eqs. 6 and 7, generates a set of sequences of feature spaces (activation maps), denoted as
209"
NEURAL EXPRESSIVENESS,0.20426287744227353,"S = {s1, . . . , si, . . . , sN}, where si = {x
′
i, . . . , Cl
i, . . . , C|K|
i
} is the sequence of the activation
210"
NEURAL EXPRESSIVENESS,0.20515097690941386,"patterns generated from sample x
′
i ∈X
′ and |si| = |K| + 1 is its cardinality, including the feature
211"
NEURAL EXPRESSIVENESS,0.20603907637655416,"space of sample x
′
i. To evaluate a specific filter k in layer l, denoted as F l
k, we utilize the retrieved
212"
NEURAL EXPRESSIVENESS,0.2069271758436945,"activation patterns from that filter, denoted as {sl
i,k}N
i=1, where sl
i,k = Cl
i,k is the activation pattern
213"
NEURAL EXPRESSIVENESS,0.20781527531083482,"retrieved from filter k in layer l.
214"
NEURAL EXPRESSIVENESS,0.20870337477797513,"To score the Neural Expressiveness of F l
k, we first construct a N × N matrix that expresses all
215"
NEURAL EXPRESSIVENESS,0.20959147424511546,"possible combinations of the activation patterns derived from the different input samples. This table
216"
NEURAL EXPRESSIVENESS,0.21047957371225579,"can be visualised as follows,
217 "
NEURAL EXPRESSIVENESS,0.2113676731793961,"


"
NEURAL EXPRESSIVENESS,0.21225577264653642,"sl
(1,1),k
sl
(1,2),k
· · ·
sl
(1,N),k
sl
(2,1),k
sl
(2,2),k
· · ·
sl
(2,N),k
...
...
...
...
sl
(N,1),k
sl
(N,2),k
· · ·
sl
(N,N),k "
NEURAL EXPRESSIVENESS,0.21314387211367672,"


.
(10)"
NEURAL EXPRESSIVENESS,0.21403197158081705,2Bias terms are excluded for simplicity.
NEURAL EXPRESSIVENESS,0.21492007104795738,"Figure 1: Expressiveness statistics of feature maps from different convolutional layers and
architectures on CIFAR-10."
NEURAL EXPRESSIVENESS,0.21580817051509768,"where sl
(i,j),k denotes the dissimilarity of activations patterns between the i-th and the j-th sample
218"
NEURAL EXPRESSIVENESS,0.216696269982238,"of the batch. In other words, the matrix in eq. 10 represents all the possible combinations of NEXP
219"
NEURAL EXPRESSIVENESS,0.21758436944937834,"calculations, where each element sl
(i,j),k derives from f(sl
i,k, sl
j,k), with f being any dissimilarity
220"
NEURAL EXPRESSIVENESS,0.21847246891651864,"function. Without loss of generality, for the rest of the study, we use the Hamming distance as the
221"
NEURAL EXPRESSIVENESS,0.21936056838365897,"operator implementing dissimilarity function. Activations are first binarized (values greater than 0
222"
NEURAL EXPRESSIVENESS,0.2202486678507993,"become 1, and the rest become 0), i.e. enabling to evaluate the degree of overlap between the binary
223"
NEURAL EXPRESSIVENESS,0.2211367673179396,"activation patterns using f.
224"
NEURAL EXPRESSIVENESS,0.22202486678507993,"We note that the matrix’s diagonal, where i equals j, along with the elements below the diagonal,
225"
NEURAL EXPRESSIVENESS,0.22291296625222023,"where i is greater than j, do not contribute additional value to quantifying the discriminative ability
226"
NEURAL EXPRESSIVENESS,0.22380106571936056,"of an element. The diagonal elements represent comparisons of the same sample’s activation patterns,
227"
NEURAL EXPRESSIVENESS,0.2246891651865009,"rendering them redundant. Meanwhile, the lower triangular elements are considered duplicates
228"
NEURAL EXPRESSIVENESS,0.2255772646536412,"since sl
(i,j),k is equal to sl
(j,i),k, thereby not adding any new information. Drawing from these two
229"
NEURAL EXPRESSIVENESS,0.22646536412078153,"observations, we define the Neural Expressiveness score (NEXP) as follows,
230"
NEURAL EXPRESSIVENESS,0.22735346358792186,"NEXP(F l
k) =
1"
NEURAL EXPRESSIVENESS,0.22824156305506216,"N(N−1) 2 N
X i=1 N
X"
NEURAL EXPRESSIVENESS,0.2291296625222025,"j=i+1
f(sl
i,k, sl
j,k)
(11)"
NEURAL EXPRESSIVENESS,0.23001776198934282,"The more similar the activation patterns derived from an element are, the less expressive it is
231"
NEURAL EXPRESSIVENESS,0.23090586145648312,"declared to be. In eq. 11, we also normalize the score w.r.t the total amount of combinations
232"
NEURAL EXPRESSIVENESS,0.23179396092362345,( N(N−1)
NEURAL EXPRESSIVENESS,0.23268206039076378,"2
), thereby deriving the average expressiveness score. This average score is then utilized to
233"
NEURAL EXPRESSIVENESS,0.23357015985790408,"characterize the discriminative capability/capacity of the examined network element. In this study,
234"
NEURAL EXPRESSIVENESS,0.2344582593250444,"we used the mean operation, however, we note that alternate statistical measures, e.g., minimum,
235"
NEURAL EXPRESSIVENESS,0.2353463587921847,"maximum, median, etc., could feasibly be applied in the computation of the overall score.
236"
DEPENDENCY TO INPUT DATA,0.23623445825932504,"3.3
Dependency to Input Data
237"
DEPENDENCY TO INPUT DATA,0.23712255772646537,"NEXP evaluates the inherent property of network elements to maximally distinguish between input
238"
DEPENDENCY TO INPUT DATA,0.23801065719360567,"samples. We extend this line of thought and assess its sensitivity to input data X and mini-batch
239"
DEPENDENCY TO INPUT DATA,0.238898756660746,"size N, in order to delineate the dependence between NEXP and the input data. To achieve that,
240"
DEPENDENCY TO INPUT DATA,0.23978685612788633,"we perform a sensitivity analysis of NEXP to the mini-batch data X, using two input sampling
241"
DEPENDENCY TO INPUT DATA,0.24067495559502664,"strategies to assemble a batch with 60 samples, namely random sampling (denoted as ‘random’) and
242"
DEPENDENCY TO INPUT DATA,0.24156305506216696,"class-representative sampling via k-means (denoted as ‘k-means’). We define the true NEXP score
243"
DEPENDENCY TO INPUT DATA,0.2424511545293073,"(denoted as ‘non-approx’) for each filter as the value obtained by comparing all activation patterns
244"
DEPENDENCY TO INPUT DATA,0.2433392539964476,"across the entire training dataset (more info in A.1). Fig. 1 presents a detailed comparative illustration
245"
DEPENDENCY TO INPUT DATA,0.24422735346358793,"of the results that highlight the similarities in NEXP estimations across various trained networks,
246"
DEPENDENCY TO INPUT DATA,0.24511545293072823,"including VGGNet [44], ResNet [17], MobileNet [19] and DenseNet [21] on CIFAR-10 dataset.
247"
DEPENDENCY TO INPUT DATA,0.24600355239786856,"Columns represent the aforementioned sampling strategies, while colors indicate expressiveness
248"
DEPENDENCY TO INPUT DATA,0.2468916518650089,"levels, with higher values signifying greater expressiveness. In each sub-figure, the x-axis indicates
249"
DEPENDENCY TO INPUT DATA,0.2477797513321492,Algorithm 1 NEXP Pruning Algorithm
DEPENDENCY TO INPUT DATA,0.24866785079928952,"Define: NEXPmap = {{NEXP (F l
k)}|Cl|
k=1}|K|
l=1
Require: A mini-batch X, a neural network N(Wti), a theoretical
speed-up target, denoted τ, and the allowed amount of pruning
steps, denoted stepsmax.
Ensure:
FLOPs(N )
FLOPs(Npruned) ≥τ
1: Initialize NEXPmap ←f(X; Wti)
2: Initialize τcurrent as 1
3: Initialize stepscurrent as 1
4: Initialize Npruned as N
5: while (τcurrent < τ) and (stepscurrent ≤stepsmax) do
6:
Fto_prune = bottomκ(NEXPmap)
7:
Npruned(Wpruned) = prune(Npruned, Fto_prune)
8:
NEXPmap ←f(X; Wpruned)
9:
τcurrent =
FLOPs(N )
FLOPs(Npruned)
10:
stepscurrent + +
11: end while
12: return Npruned(Wpruned)"
DEPENDENCY TO INPUT DATA,0.24955595026642985,"Figure 2:
Pruning YOLOv8m trained on
COCO for Object Detection. Comparative re-
sults between neural expressiveness (NEXP) and
layer-adaptive magnitude-based pruning method
(LAMP) [26]. More comparisons in the supple-
mentary material."
DEPENDENCY TO INPUT DATA,0.25044404973357015,"convolutional layer indices, and the y-axis shows feature map indices per layer, standardized through
250"
DEPENDENCY TO INPUT DATA,0.25133214920071045,"pixel-wise interpolation to align with the layer having the most feature maps. Fig. 1 confirms that
251"
DEPENDENCY TO INPUT DATA,0.2522202486678508,"NEXP can be effectively estimated using random and limited data samples. Detailed results of this
252"
DEPENDENCY TO INPUT DATA,0.2531083481349911,"analysis, are presented in Appendix A. The comparative analysis reveals that a mini-batch of 60
253"
DEPENDENCY TO INPUT DATA,0.2539964476021314,"samples (0.4% of D in this case) effectively approximates the NEXP scores calculated from the entire
254"
DEPENDENCY TO INPUT DATA,0.25488454706927177,"dataset, yielding consistent similarity scores above 99% across most similarity metrics (Table. 3).
255"
PRUNING PROCESS,0.2557726465364121,"3.4
Pruning Process
256"
PRUNING PROCESS,0.2566607460035524,"Alg. 1 describes the proposed NEXP-based pruning process, and it has been implemented as extension
257"
PRUNING PROCESS,0.25754884547069273,"in the DepGraph pruning framework [11]. A target theoretical speed-up is specified, referred to
258"
PRUNING PROCESS,0.25843694493783304,"as the Compression FLOPs Ratio (↓) and denoted by τ. This ratio is calculated using the formula
259"
PRUNING PROCESS,0.25932504440497334,"original FLOPs
compressed FLOPs. To achieve this target ratio, the network may undergo pruning in one or several steps,
260"
PRUNING PROCESS,0.2602131438721137,"dictated by the intricacies of the pruning criterion and adjusted according to the quantity of elements
261"
PRUNING PROCESS,0.261101243339254,"removed at each step. For example, NEXP benefits from additional steps, since a filter’s score is
262"
PRUNING PROCESS,0.2619893428063943,"reliant on its preceding elements (Section 3.2), and a more gradual update on the scores allows for
263"
PRUNING PROCESS,0.26287744227353466,"improved pruning precision. A more in-depth analysis of Alg. 1 along with more details on the
264"
PRUNING PROCESS,0.26376554174067496,"implementation options are presented in Appendix B.
265"
EXPERIMENTAL EVALUATION,0.26465364120781526,"4
Experimental Evaluation
266"
EXPERIMENTAL EVALUATION,0.2655417406749556,"Details on the experimental settings can be found in Appendix C, including the (a) Datasets and
267"
EXPERIMENTAL EVALUATION,0.2664298401420959,"Models (C.1), (b) Adversaries (C.2), (c) Evaluation Metrics (C.3) and (d) Configurations (C.4).
268"
EXPERIMENTAL EVALUATION,0.2673179396092362,"4.1
Comparison w.r.t. State-of-Art Model Compression Strategies
269"
EXPERIMENTAL EVALUATION,0.2682060390763766,"Image Classification on CIFAR-10 and Imagenet-1k.
We compare against a plethora of foun-
270"
EXPERIMENTAL EVALUATION,0.2690941385435169,"dational and top-performing approaches, ranging from filter magnitude-based [28, 32, 29] and loss
271"
EXPERIMENTAL EVALUATION,0.2699822380106572,"sensitivity-based [58] methods to feature-guided strategies [23, 30] and search algorithms [35, 31].
272"
EXPERIMENTAL EVALUATION,0.27087033747779754,"Outcomes and Discussion. Our findings for various target FLOPs pruning ratios are presented in
273"
EXPERIMENTAL EVALUATION,0.27175843694493784,"Tab. 1 (and Tab.6-9 in Appendix D.2) for CIFAR-10, and in Tab. 2 for ImageNet. It is essential to
274"
EXPERIMENTAL EVALUATION,0.27264653641207814,"acknowledge the subjectivity in reported performance metrics (accuracy), influenced by the fine-
275"
EXPERIMENTAL EVALUATION,0.27353463587921845,"tuning process post-pruning, e.g. the authors in DCP [61] fine-tune for 400 epochs, in contrast to ours
276"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2744227353463588,"100. We observe that our approach yields consistent improvements in params reduction compared to
277"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2753108348134991,"other methods for given FLOPs ratios, which notably scale significantly for regimes of higher target
278"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2761989342806394,"FLOPs compression ratios τ. For example, on ResNet-56 we show +0.92× average params reduction
279"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.27708703374777977,"gains in the 2×-2.20× FLOPs reduction regime, with -0.38%, +0.05% and -0.37% percentage
280"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.27797513321492007,"difference in loss respectively to ABC [31], SCP [23] and HRank [30], while on ResNet-110 we
281"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.27886323268206037,"Table 1: Analytical Comparison of Importance-based solutions and Expressiveness on CIFAR-10
using ResNet architectures [17] - ResNet-56 (left) and ResNet-110 (right)."
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2797513321492007,"top-1 acc
Compression Ratio ↓
Method
Base (%)
∆(%)
#Params
#FLOPs"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.28063943161634103,"L1 [28]
93.06
+0.02
1.16×
1.37×
NEXP (Ours)
93.36
+0.05
1.69×
1.53×
GAL-0.6 [32]
93.26
+0.12
1.13×
1.60×
NISP-56 [58]
-
-0.03
1.74×
1.77×
DCP-Adapt [61]
93.80
+0.01
3.37×
1.89×
HRank [30]
93.26
-0.09
1.74×
2.01×
SCP [23]
93.69
-0.46
1.94×
2.06×
NEXP (Ours)
93.36
-0.41
2.87×
2.11×
ABC [31]
93.26
-0.03
2.18×
2.18×
NEXP (Ours)
93.36
-1.58
4.3×
2.50×
GAL-0.8 [32]
93.26
-1.68
2.93×
2.51×
HRank [30]
93.26
-2.54
3.15×
3.86×
NEXP (Ours)
93.36
-5.12
21.5×
5.00×"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.28152753108348133,"top-1 acc
Compression Ratio ↓
Method
Base (%)
∆(%)
#Params
#FLOPs"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2824156305506217,"L1 [28]
93.55
+0.02
1.02×
1.19×
NEXP (Ours)
93.79
+0.66
1.10×
1.20×
GAL-0.1 [32]
93.50
+0.09
1.04×
1.23×
HRank [30]
93.50
+0.73
1.65×
1.70×
NISP-110 [58]
-
-0.18
1.76×
1.78×
NEXP (Ours)
93.79
+0.18
1.78×
1.80×
GAL-0.5 [32]
93.50
-0.76
1.81×
1.94×
HRank [30]
93.50
-0.14
2.46×
2.39×
NEXP (Ours)
93.79
+0.10
2.72×
2.42×
ABC [31]
93.50
+0.08
3.09×
2.87×
NEXP (Ours)
93.79
-0.37
3.81×
3.01×
HRank [30]
93.50
-0.85
3.25×
3.19×
NEXP (Ours)
93.79
-0.59
4.38×
3.27×"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.283303730017762,"show +1.21× average params reduction gains in the 2.87×-3.27× FLOPs reduction regime, with
282"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2841918294849023,"-0.67% and +0.26% percentage difference in loss respectively to ABC [31] and HRank [30]. Similar
283"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.28507992895204265,"observations are evident across all tables, where in certain regimes we also show notable performance
284"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.28596802841918295,"gains, up to +1.5%, especially for VGGNet, which is more prone to params reductions due to its
285"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.28685612788632325,"plain structure.
286"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2877442273534636,"Object Detection with YOLOv8.
We evaluate expressiveness against four importance based
287"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2886323268206039,"methods, i.e layer-adaptive magnitude-based pruning (LAMP) [26], network slimming (SLIM) [35],
288"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2895204262877442,"Wang’s et al. proposed method (DepGraph) [11] and random pruning that serves as a generic pruning
289"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.29040852575488457,"baseline [3]. The experiments were conducted on the YOLOv8m model version [22], utilizing the
290"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2912966252220249,"DepGraph pruning framework [11] with an iterative pruning schedule of 16 steps, where after each
291"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2921847246891652,"pruning step the model was fine-tuned for 10 epochs using the coco128 dataset.
292"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.29307282415630553,"Outcomes and Discussion. We report the comparative pruning progress of expressiveness versus
293"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.29396092362344584,"the baseline methods, i.e. the remaining percentage of the original model in terms of MACs and
294"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.29484902309058614,"params after each pruning step, named MACs Size Percentage (MSP) and Parameters Size Percentage
295"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.29573712255772644,"(PSP) respectively, and highlight the mAP val
50−95 both after pruning (pruned mAP) and fine-tuning
296"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2966252220248668,"(recovered mAP). We observe that expressiveness outperforms the rest of the reported methods across
297"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2975133214920071,"the whole pruning spectrum, as shown in Fig. 2 (more in Appendix D.2), preserving the initial
298"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.2984014209591474,"performance of the model for percentage sizes that reach up to 40% (2.5 ↓) of that of the original
299"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.29928952042628776,"model, with less than 0.5% of recovered performance degradation. Our method even achieves a 3%
300"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.30017761989342806,"increase in recovered mAP for 46.1% MSP (2.17 ↓), in comparison to the baselines that showcase
301"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.30106571936056836,"weak recovery capabilities after the 60% (1.67 ↓) mark in both MSP and PSP. This can be attributed
302"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.3019538188277087,"to the intrinsic property of expressiveness to maintain network elements that are more robust to
303"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.302841918294849,"information redistribution, in contrast to “important"" labeled structures by other methods. In our
304"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.3037300177619893,"experimental scenario, that characteristic is further amplified by the iterative pruning format and the
305"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.3046181172291297,"higher amount of fine-tuning epochs at each step, in comparison to conventional pruning schedules
306"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.30550621669627,"that fine-tune for 1 epoch after each iteration or perform a unified fine-tuning session after the last
307"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.3063943161634103,"pruning iteration. Interestingly, our criterion also demonstrates significant resistance to performance
308"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.30728241563055064,"loss after pruning, achieving 18% increased average performance in terms of pruned mAP compared
309"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.30817051509769094,"to the importance-based methods. We have empirically observed that expressiveness benefits from
310"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.30905861456483125,"increased cardinality in pruning granularity settings, e.g amount of intermediate steps to achieve a
311"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.3099467140319716,"given compression ratio. This stems from expressiveness interactive nature of all elements, as also
312"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.3108348134991119,"explained in Sec. 3, where smaller pruning steps combined with iterative fine-tuning, enhance pruning
313"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.3117229129662522,"precision and allow for “smoother"" redistribution of information in a network, thus contributing to
314"
WE OBSERVE THAT OUR APPROACH YIELDS CONSISTENT IMPROVEMENTS IN PARAMS REDUCTION COMPARED TO,0.31261101243339257,"the increased resistance to performance deficits after each pruning step.
315"
ASSESSING HYBRID COMPRESSION SPACE,0.31349911190053287,"4.2
Assessing Hybrid Compression space
316"
ASSESSING HYBRID COMPRESSION SPACE,0.31438721136767317,"In this section, we assess the potential efficiency of “hybrid"" pruning strategies exploiting the
317"
ASSESSING HYBRID COMPRESSION SPACE,0.31527531083481347,"cooperation between importance and expressiveness. We explore the solution space of “hybrid""
318"
ASSESSING HYBRID COMPRESSION SPACE,0.31616341030195383,"compression, using a linear combination of importance and neural expressiveness criteria. We guide
319"
ASSESSING HYBRID COMPRESSION SPACE,0.31705150976909413,"exploration through the scoring function: Wimp · IMP + Wnexp · NEXP and conduct experiments with
320"
ASSESSING HYBRID COMPRESSION SPACE,0.31793960923623443,"Table 2: Analytical Comparison of Importance-based solu-
tions and Expressiveness on ImageNet-1k using ResNet-50
[17]."
ASSESSING HYBRID COMPRESSION SPACE,0.3188277087033748,"Method
Base (%)
∆Acc (%)
Compression Ratio
top-1
top-5
top-1
top-5
#Params ↓
#FLOPs ↓"
ASSESSING HYBRID COMPRESSION SPACE,0.3197158081705151,"NISP-50-B [58]
-
-
-0.89
-
1.78×
1.79×
NEXP (Ours)
76.13
92.86
-1.35
-0.93
2.00×
2.02×
ThiNet [37]
72.88
91.14
-1.87
-1.12
2.06×
2.25×
DCP [61]
76.01
92.93
-1.06
-0.56
2.06×
2.25×
ABC [31]
76.01
92.96
-2.49
-1.45
2.27×
2.30×
NEXP (Ours)
76.13
92.86
-6.77
-3.43
4.05×
3.04×
GAL-1-joint [32]
76.15
92.87
-6.84
-3.75
2.50×
3.68×
Hrank [30]
76.15
92.87
-7.15
-3.29
3.08×
4.17×"
ASSESSING HYBRID COMPRESSION SPACE,0.3206039076376554,"Figure 3: Linear exploration of the
combinatorial space between impor-
tance and expressiveness."
ASSESSING HYBRID COMPRESSION SPACE,0.32149200710479575,"various weight combinations, subject to the constraint Wimp + Wnexp = 1. Given that exhaustive
321"
ASSESSING HYBRID COMPRESSION SPACE,0.32238010657193605,"search is impractical, we introduce the hyper-parameter α ∈{0.0, 0.2, . . . , 0.8, 1.0} to restrict the
322"
ASSESSING HYBRID COMPRESSION SPACE,0.32326820603907636,"set of permissible combinations, and modify the constraint to (1 −α) · Wimp + α · Wnexp = 1. We
323"
ASSESSING HYBRID COMPRESSION SPACE,0.3241563055062167,"use group L1-norm [28] as the importance criterion (IMP) and assess all permissible combinations
324"
ASSESSING HYBRID COMPRESSION SPACE,0.325044404973357,"across a linear scale, denoted as τ, representing the target FLOPs compression ratios that we utilized
325"
ASSESSING HYBRID COMPRESSION SPACE,0.3259325044404973,"for pruning, on ResNet-56 for CIFAR-10. The outcomes are visualized in Figure 3, which maps our
326"
ASSESSING HYBRID COMPRESSION SPACE,0.3268206039076377,"predetermined τ values on the x-axis against the various parameter compression ratios achieved by
327"
ASSESSING HYBRID COMPRESSION SPACE,0.327708703374778,"each combination. Regarding performance, we report the averaged percentage differences in top-1
328"
ASSESSING HYBRID COMPRESSION SPACE,0.3285968028419183,"accuracy between the baseline importance method (L1) and each hybrid format: -0.21% for hb-0.2,
329"
ASSESSING HYBRID COMPRESSION SPACE,0.32948490230905864,"-0.96% for hb-0.4, -1.55% for hb-0.6, -1.07% for hb-0.8, and -2.18% for NEXP.
330"
ASSESSING HYBRID COMPRESSION SPACE,0.33037300177619894,"Observations. A consistent pattern is observed across the values of α, where larger values yield
331"
ASSESSING HYBRID COMPRESSION SPACE,0.33126110124333924,"higher params compression ratios. Notably, hybrid derivatives allow us to explore sub-spaces with
332"
ASSESSING HYBRID COMPRESSION SPACE,0.3321492007104796,"higher parameter compression ratios by sacrificing slight performance accuracy. We also observe
333"
ASSESSING HYBRID COMPRESSION SPACE,0.3330373001776199,"that the solution vectors corresponding to IMP and EXP act as extremal points in the solution space
334"
ASSESSING HYBRID COMPRESSION SPACE,0.3339253996447602,"of hybrid combinations, thus suggesting a degree of partial orthogonality between the two criteria.
335"
ASSESSING HYBRID COMPRESSION SPACE,0.33481349911190056,"Furthermore, the findings reveal a polynomial relationship between parameter compression ratios and
336"
ASSESSING HYBRID COMPRESSION SPACE,0.33570159857904086,"FLOPs reduction, with compression ratios increasing polynomially to linear increments in FLOPs
337"
ASSESSING HYBRID COMPRESSION SPACE,0.33658969804618116,"reduction, and thus enabling more efficient explorations.
338"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.33747779751332146,"4.3
Evaluating Neural Expressiveness at Initialization
339"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.3383658969804618,"The nature of NEXP allows to be applied in a weight agnostic manner, i.e. on untrained networks.
340"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.3392539964476021,"An extended version of the section’s 3.3 analysis, which also includes untrained models (Appendix
341"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.3401420959147424,"A), reveals that NEXPmap’s obtained at initialization and after network convergence share some
342"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.3410301953818828,"expressiveness pattern similarities, particularly in the initial layers. Our numeric evaluation shows a
343"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.3419182948490231,"notable correlation between the initialization and converged states for DenseNet-40 and VGG-19,
344"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.3428063943161634,"with cosine similarities of 84.10% and 86.82%, respectively. It also indicates greater consistency in
345"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.34369449378330375,"neural expressiveness measurements for the first layers of all networks, which could be considered
346"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.34458259325044405,"important for the formation of critical paths [2]. Motivated by these observations, we also assess the
347"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.34547069271758435,"efficacy of expressiveness as criterion for Pruning at Initialization against various SOTA approaches
348"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.3463587921847247,"[27, 50, 46] (Appendix D.1). Our method consistently outperforms (in terms of top-1 acc) all other
349"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.347246891651865,"algorithms, particularly in regimes of lower compression, up to 102(↓) with an average increase of
350"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.3481349911190053,"1.21% over SynFlow, while maintaining competitiveness at higher compression levels, above 102(↓)
351"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.34902309058614567,"with an average percentage difference of 4.82%, 3.72% and -2.74%, compared to [50], [27] and [46].
352"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.34991119005328597,"In summary, under the assumption that the selection of hyperparameters remains congruent with
353"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.35079928952042627,"the initialization [12], consistent map measurements between initial and final states can effectively
354"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.35168738898756663,"evaluate NEXP’s ability to identify winning tickets. However, a robust evaluation should also consider
355"
EVALUATING NEURAL EXPRESSIVENESS AT INITIALIZATION,0.35257548845470693,"the initial state quality and the training process, while addressing the ""When to prune"" question [42].
356"
CONCLUSIONS,0.35346358792184723,"5
Conclusions
357"
CONCLUSIONS,0.3543516873889876,"In this work, we have introduced “Neural Expressiveness"" as a new criterion for model compression.
358"
CONCLUSIONS,0.3552397868561279,"In our NEXP steps, we will explore optimal solutions for the “When"" and “How"" to prune questions.
359"
REFERENCES,0.3561278863232682,"References
360"
REFERENCES,0.35701598579040855,"[1] Armstrong Aboah, Bin Wang, Ulas Bagci, and Yaw Adu-Gyamfi. Real-time multi-class helmet violation
361"
REFERENCES,0.35790408525754885,"detection using few-shot data sampling technique and yolov8. In Proceedings of the IEEE/CVF Conference
362"
REFERENCES,0.35879218472468916,"on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 5350–5358, June 2023.
363"
REFERENCES,0.35968028419182946,"[2] Alessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep networks. In
364"
REFERENCES,0.3605683836589698,"International Conference on Learning Representations, 2019.
365"
REFERENCES,0.3614564831261101,"[3] Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of neural
366"
REFERENCES,0.3623445825932504,"network pruning? In I. Dhillon, D. Papailiopoulos, and V. Sze, editors, Proceedings of Machine Learning
367"
REFERENCES,0.3632326820603908,"and Systems, volume 2, pages 129–146, 2020.
368"
REFERENCES,0.3641207815275311,"[4] Miguel Á. Carreira-Perpiñán and Yerlan Idelbayev. “learning-compression” algorithms for neural net
369"
REFERENCES,0.3650088809946714,"pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
370"
REFERENCES,0.36589698046181174,"June 2018.
371"
REFERENCES,0.36678507992895204,"[5] Y Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze. Understanding the limitations of existing energy-
372"
REFERENCES,0.36767317939609234,"efficient design approaches for deep neural networks. Energy, 2(L1):L3, 2018.
373"
REFERENCES,0.3685612788632327,"[6] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems
374"
REFERENCES,0.369449378330373,"with a sparsity constraint. Communications on Pure and Applied Mathematics, 57(11):1413–1457, 2004.
375"
REFERENCES,0.3703374777975133,"[7] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware acceleration
376"
REFERENCES,0.37122557726465366,"for neural networks: A comprehensive survey. Proceedings of the IEEE, 108(4):485–532, 2020.
377"
REFERENCES,0.37211367673179396,"[8] Xiaohan Ding, guiguang ding, Xiangxin Zhou, Yuchen Guo, Jungong Han, and Ji Liu. Global sparse
378"
REFERENCES,0.37300177619893427,"momentum sgd for pruning very deep neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer,
379"
REFERENCES,0.3738898756660746,"F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems,
380"
REFERENCES,0.3747779751332149,"volume 32. Curran Associates, Inc., 2019.
381"
REFERENCES,0.3756660746003552,"[9] Tausif Diwan, G Anirudh, and Jitendra V Tembhurne. Object detection using yolo: Challenges, architectural
382"
REFERENCES,0.3765541740674956,"successors, datasets and applications. multimedia Tools and Applications, 82(6):9243–9275, 2023.
383"
REFERENCES,0.3774422735346359,"[10] Andrei Dumitriu, Florin Tatui, Florin Miron, Radu Tudor Ionescu, and Radu Timofte. Rip current
384"
REFERENCES,0.3783303730017762,"segmentation: A novel benchmark and yolov8 baseline results. In Proceedings of the IEEE/CVF Conference
385"
REFERENCES,0.37921847246891655,"on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 1261–1271, June 2023.
386"
REFERENCES,0.38010657193605685,"[11] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards
387"
REFERENCES,0.38099467140319715,"any structural pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
388"
REFERENCES,0.38188277087033745,"Recognition (CVPR), pages 16091–16101, June 2023.
389"
REFERENCES,0.3827708703374778,"[12] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
390"
REFERENCES,0.3836589698046181,"networks, 2019.
391"
REFERENCES,0.3845470692717584,"[13] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In D. Lee,
392"
REFERENCES,0.38543516873889877,"M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing
393"
REFERENCES,0.38632326820603907,"Systems, volume 29. Curran Associates, Inc., 2016.
394"
REFERENCES,0.3872113676731794,"[14] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with
395"
REFERENCES,0.38809946714031973,"pruning, trained quantization and huffman coding, 2016.
396"
REFERENCES,0.38898756660746003,"[15] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient
397"
REFERENCES,0.38987566607460034,"neural network. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in
398"
REFERENCES,0.3907637655417407,"Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.
399"
REFERENCES,0.391651865008881,"[16] Babak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon. In
400"
REFERENCES,0.3925399644760213,"S. Hanson, J. Cowan, and C. Giles, editors, Advances in Neural Information Processing Systems, volume 5.
401"
REFERENCES,0.39342806394316165,"Morgan-Kaufmann, 1992.
402"
REFERENCES,0.39431616341030196,"[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
403"
REFERENCES,0.39520426287744226,"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
404"
REFERENCES,0.3960923623445826,"[18] Andrew Howard, Andrey Zhmoginov, Liang-Chieh Chen, Mark Sandler, and Menglong Zhu. Inverted
405"
REFERENCES,0.3969804618117229,"residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. In CVPR,
406"
REFERENCES,0.3978685612788632,"2018.
407"
REFERENCES,0.3987566607460036,"[19] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
408"
REFERENCES,0.3996447602131439,"Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile
409"
REFERENCES,0.4005328596802842,"vision applications, 2017.
410"
REFERENCES,0.40142095914742454,"[20] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven neuron
411"
REFERENCES,0.40230905861456484,"pruning approach towards efficient deep architectures, 2016.
412"
REFERENCES,0.40319715808170514,"[21] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convo-
413"
REFERENCES,0.40408525754884544,"lutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
414"
REFERENCES,0.4049733570159858,"(CVPR), July 2017.
415"
REFERENCES,0.4058614564831261,"[22] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralytics yolov8, 2023.
416"
REFERENCES,0.4067495559502664,"[23] Minsoo Kang and Bohyung Han. Operation-aware soft channel pruning using differentiable masks. In
417"
REFERENCES,0.40763765541740676,"Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine
418"
REFERENCES,0.40852575488454707,"Learning, volume 119 of Proceedings of Machine Learning Research, pages 5122–5131. PMLR, 13–18
419"
REFERENCES,0.40941385435168737,"Jul 2020.
420"
REFERENCES,0.4103019538188277,"[24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical
421"
REFERENCES,0.411190053285968,"report, 2009.
422"
REFERENCES,0.41207815275310833,"[25] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In D. Touretzky, editor, Advances in
423"
REFERENCES,0.4129662522202487,"Neural Information Processing Systems, volume 2. Morgan-Kaufmann, 1989.
424"
REFERENCES,0.413854351687389,"[26] Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin. Layer-adaptive sparsity for the
425"
REFERENCES,0.4147424511545293,"magnitude-based pruning, 2021.
426"
REFERENCES,0.41563055062166965,"[27] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Snip: Single-shot network pruning based
427"
REFERENCES,0.41651865008880995,"on connection sensitivity, 2019.
428"
REFERENCES,0.41740674955595025,"[28] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient
429"
REFERENCES,0.4182948490230906,"convnets, 2017.
430"
REFERENCES,0.4191829484902309,"[29] Yuchao Li, Shaohui Lin, Baochang Zhang, Jianzhuang Liu, David Doermann, Yongjian Wu, Feiyue Huang,
431"
REFERENCES,0.4200710479573712,"and Rongrong Ji. Exploiting kernel sparsity and entropy for interpretable cnn compression. In Proceedings
432"
REFERENCES,0.42095914742451157,"of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
433"
REFERENCES,0.4218472468916519,"[30] Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, and Ling Shao.
434"
REFERENCES,0.4227353463587922,"Hrank: Filter pruning using high-rank feature map. In Proceedings of the IEEE/CVF Conference on
435"
REFERENCES,0.42362344582593253,"Computer Vision and Pattern Recognition (CVPR), June 2020.
436"
REFERENCES,0.42451154529307283,"[31] Mingbao Lin, Rongrong Ji, Yuxin Zhang, Baochang Zhang, Yongjian Wu, and Yonghong Tian. Channel
437"
REFERENCES,0.42539964476021314,"pruning via automatic structure search. arXiv preprint arXiv:2001.08565, 2020.
438"
REFERENCES,0.42628774422735344,"[32] Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan Cao, Qixiang Ye, Feiyue Huang,
439"
REFERENCES,0.4271758436944938,"and David Doermann. Towards optimal structured cnn pruning via generative adversarial learning. In
440"
REFERENCES,0.4280639431616341,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June
441"
REFERENCES,0.4289520426287744,"2019.
442"
REFERENCES,0.42984014209591476,"[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
443"
REFERENCES,0.43072824156305506,"and C. Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In David Fleet, Tomas Pajdla,
444"
REFERENCES,0.43161634103019536,"Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision – ECCV 2014, Lecture Notes in Computer
445"
REFERENCES,0.4325044404973357,"Science, pages 740–755, Cham, 2014. Springer International Publishing.
446"
REFERENCES,0.433392539964476,"[34] Tantan Liu and Gagan Agrawal. Stratified k-means clustering over a deep web data source. In Proceedings
447"
REFERENCES,0.4342806394316163,"of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages
448"
REFERENCES,0.4351687388987567,"1113–1121, 2012.
449"
REFERENCES,0.436056838365897,"[35] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning
450"
REFERENCES,0.4369449378330373,"efficient convolutional networks through network slimming, 2017.
451"
REFERENCES,0.43783303730017764,"[36] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network
452"
REFERENCES,0.43872113676731794,"pruning, 2019.
453"
REFERENCES,0.43960923623445824,"[37] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network
454"
REFERENCES,0.4404973357015986,"compression. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.
455"
REFERENCES,0.4413854351687389,"[38] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for
456"
REFERENCES,0.4422735346358792,"neural network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
457"
REFERENCES,0.44316163410301956,"Recognition (CVPR), June 2019.
458"
REFERENCES,0.44404973357015987,"[39] Sumit Pandey, Kuan-Fu Chen, and Erik B. Dam. Comprehensive multimodal segmentation in medical
459"
REFERENCES,0.44493783303730017,"imaging: Combining yolov8 with sam and hq-sam models. In Proceedings of the IEEE/CVF International
460"
REFERENCES,0.44582593250444047,"Conference on Computer Vision (ICCV) Workshops, pages 2592–2598, October 2023.
461"
REFERENCES,0.4467140319715808,"[40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
462"
REFERENCES,0.44760213143872113,"Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large
463"
REFERENCES,0.44849023090586143,"Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):211–252, December
464"
REFERENCES,0.4493783303730018,"2015.
465"
REFERENCES,0.4502664298401421,"[41] Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning. In
466"
REFERENCES,0.4511545293072824,"H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information
467"
REFERENCES,0.45204262877442275,"Processing Systems, volume 33, pages 20378–20389. Curran Associates, Inc., 2020.
468"
REFERENCES,0.45293072824156305,"[42] Maying Shen, Pavlo Molchanov, Hongxu Yin, and Jose M. Alvarez. When to prune? a policy towards
469"
REFERENCES,0.45381882770870335,"early structural pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
470"
REFERENCES,0.4547069271758437,"Recognition (CVPR), pages 12247–12256, June 2022.
471"
REFERENCES,0.455595026642984,"[43] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-
472"
REFERENCES,0.4564831261101243,"tion, 2015.
473"
REFERENCES,0.4573712255772647,"[44] Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image
474"
REFERENCES,0.458259325044405,"Recognition, April 2015. arXiv:1409.1556 [cs].
475"
REFERENCES,0.4591474245115453,"[45] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
476"
REFERENCES,0.46003552397868563,"Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of
477"
REFERENCES,0.46092362344582594,"the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.
478"
REFERENCES,0.46181172291296624,"[46] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without
479"
REFERENCES,0.4626998223801066,"any data by iteratively conserving synaptic flow. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
480"
REFERENCES,0.4635879218472469,"and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 6377–6389.
481"
REFERENCES,0.4644760213143872,"Curran Associates, Inc., 2020.
482"
REFERENCES,0.46536412078152756,"[47] Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Huszár. Faster gaze prediction with dense
483"
REFERENCES,0.46625222024866786,"networks and fisher pruning, 2018.
484"
REFERENCES,0.46714031971580816,"[48] Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. The computational limits of
485"
REFERENCES,0.46802841918294846,"deep learning, 2022.
486"
REFERENCES,0.4689165186500888,"[49] Sunil Vadera and Salem Ameen. Methods for pruning deep neural networks. IEEE Access, 10:63280–63300,
487"
REFERENCES,0.4698046181172291,"2022.
488"
REFERENCES,0.4706927175843694,"[50] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving
489"
REFERENCES,0.4715808170515098,"gradient flow, 2020.
490"
REFERENCES,0.4724689165186501,"[51] Qing Ren Wang and Ching Y. Suen. Analysis and design of a decision tree based on entropy reduction
491"
REFERENCES,0.4733570159857904,"and its application to large character set recognition. IEEE Transactions on Pattern Analysis and Machine
492"
REFERENCES,0.47424511545293074,"Intelligence, PAMI-6(4):406–417, 1984.
493"
REFERENCES,0.47513321492007105,"[52] Shiqiang Wang. Efficient deep learning. Nature Computational Science, 1(3):181–182, March 2021.
494"
REFERENCES,0.47602131438721135,"Number: 3 Publisher: Nature Publishing Group.
495"
REFERENCES,0.4769094138543517,"[53] Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin, Sicheng Zhao, Noah Golmant, Amir Gholaminejad,
496"
REFERENCES,0.477797513321492,"Joseph Gonzalez, and Kurt Keutzer. Shift: A zero flop, zero parameter alternative to spatial convolutions.
497"
REFERENCES,0.4786856127886323,"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
498"
REFERENCES,0.47957371225577267,"[54] Kaixin Xu, Zhe Wang, Xue Geng, Min Wu, Xiaoli Li, and Weisi Lin. Efficient joint optimization of
499"
REFERENCES,0.48046181172291297,"layer-adaptive weight pruning in deep neural networks. In Proceedings of the IEEE/CVF International
500"
REFERENCES,0.48134991119005327,"Conference on Computer Vision (ICCV), pages 17447–17457, October 2023.
501"
REFERENCES,0.4822380106571936,"[55] Pengtao Xu, Jian Cao, Fanhua Shang, Wenyu Sun, and Pu Li. Layer pruning via fusible residual convolu-
502"
REFERENCES,0.48312611012433393,"tional block for deep neural networks, 2020.
503"
REFERENCES,0.48401420959147423,"[56] Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutional neural networks
504"
REFERENCES,0.4849023090586146,"using energy-aware pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
505"
REFERENCES,0.4857904085257549,"Recognition (CVPR), July 2017.
506"
REFERENCES,0.4866785079928952,"[57] Jiecao Yu, Andrew Lukefahr, David Palframan, Ganesh Dasika, Reetuparna Das, and Scott Mahlke. Scalpel:
507"
REFERENCES,0.48756660746003555,"Customizing dnn pruning to the underlying hardware parallelism. In Proceedings of the 44th Annual
508"
REFERENCES,0.48845470692717585,"International Symposium on Computer Architecture, ISCA ’17, page 548–560, New York, NY, USA, 2017.
509"
REFERENCES,0.48934280639431615,"Association for Computing Machinery.
510"
REFERENCES,0.49023090586145646,"[58] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I. Morariu, Xintong Han, Mingfei Gao, Ching-
511"
REFERENCES,0.4911190053285968,"Yung Lin, and Larry S. Davis. Nisp: Pruning networks using neuron importance score propagation. In
512"
REFERENCES,0.4920071047957371,"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
513"
REFERENCES,0.4928952042628774,"[59] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional
514"
REFERENCES,0.4937833037300178,"neural network for mobile devices. In Proceedings of the IEEE Conference on Computer Vision and
515"
REFERENCES,0.4946714031971581,"Pattern Recognition (CVPR), June 2018.
516"
REFERENCES,0.4955595026642984,"[60] Zhengguang Zhou, Wengang Zhou, Houqiang Li, and Richang Hong. Online filter clustering and pruning
517"
REFERENCES,0.49644760213143874,"for efficient convnets. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages
518"
REFERENCES,0.49733570159857904,"11–15, 2018.
519"
REFERENCES,0.49822380106571934,"[61] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, and
520"
REFERENCES,0.4991119005328597,"Jinhui Zhu. Discrimination-aware Channel Pruning for Deep Neural Networks. In S. Bengio, H. Wallach,
521"
REFERENCES,0.5,"H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information
522"
REFERENCES,0.5008880994671403,"Processing Systems, volume 31. Curran Associates, Inc., 2018.
523"
REFERENCES,0.5017761989342806,"A
Duality of Independence: Data (X) and Information State (Wti)
524"
REFERENCES,0.5026642984014209,"Fig. 4 presents a detailed comparative illustration that highlights the similarities in NEXP estimations
525"
REFERENCES,0.5035523978685613,"across various networks, including VGGNet [44], ResNet [17], MobileNet [19] and DenseNet [21]
526"
REFERENCES,0.5044404973357016,"on CIFAR-10 dataset. Specifically, for each network architecture, we showcase expressiveness
527"
REFERENCES,0.5053285968028419,"distributions in both untrained (PaI) and trained (PaT) states. In each sub-figure, the x-axis indicates
528"
REFERENCES,0.5062166962699822,"convolutional layer indices, and the y-axis shows feature map indices per layer, standardized through
529"
REFERENCES,0.5071047957371225,"pixel-wise interpolation to align with the layer having the most feature maps. Columns represent
530"
REFERENCES,0.5079928952042628,"various sampling strategies, while colors indicate expressiveness levels, with higher values signifying
531"
REFERENCES,0.5088809946714032,"greater expressiveness. In other words, the figure illustrates a two-fold sensitivity analysis of NEXP
532"
REFERENCES,0.5097690941385435,"to (i) the mini-batch data (X, as outlined in Alg. 1), using two input sampling strategies to assemble
533"
REFERENCES,0.5106571936056838,"a batch with 60 samples, namely random sampling (denoted as ‘random’) and class-representative
534"
REFERENCES,0.5115452930728241,"sampling via k-means (denoted as ‘k-means’), and (ii) the information state (Wti), specifically
535"
REFERENCES,0.5124333925399644,"comparing expressiveness at initialization (PaI) against expressiveness after training (PaT), when
536"
REFERENCES,0.5133214920071048,"weights have converged.
537"
REFERENCES,0.5142095914742452,"A.1
True NEXP value (non-approx).
538"
REFERENCES,0.5150976909413855,"We define the true NEXP score for each filter as the value obtained by comparing all activation
539"
REFERENCES,0.5159857904085258,"patterns across the entire training dataset D. In that way, the ability of each element to extract
540"
REFERENCES,0.5168738898756661,"maximal features is evaluated for every data-point in the input feature space of a task at hand. In
541"
REFERENCES,0.5177619893428064,"this study however, due to GPU memory constraints (limited to 12GB of GDDR6 SDRAM), we
542"
REFERENCES,0.5186500888099467,"employed 25% of the total training set, ensuring class distribution is preserved, to determine these
543"
REFERENCES,0.5195381882770871,"exact NEXP scores, denoted as non-approx.
544"
REFERENCES,0.5204262877442274,"A.2
Data Agnostic.
545"
REFERENCES,0.5213143872113677,"To evaluate NEXP’s sensitivity to input data, we conduct a similarity analysis for each row in
546"
REFERENCES,0.522202486678508,"Fig. 4. For each information state (PaI and PaT), we compare the expressiveness map (NEXPmap)
547"
REFERENCES,0.5230905861456483,"derived from each sampling strategy against the true NEXP values (non-approx), corresponding to
548"
REFERENCES,0.5239786856127886,"each respective state. For a comprehensive comparison, we utilize various similarity metrics, such
549"
REFERENCES,0.5248667850799289,"as Euclidean Distance, Cosine Similarity, Pearsonr Similarity, and the Structural Similarity Index
550"
REFERENCES,0.5257548845470693,"Measure (ssim_index). Detailed results of this analysis, specific to each state, are presented in Tables
551"
REFERENCES,0.5266429840142096,"3 (PaT) and 4 (PaI). The comparative analysis reveals that a mini-batch of 60 samples, with a balanced
552"
REFERENCES,0.5275310834813499,"representation from each class, effectively approximates the NEXP scores calculated from the entire
553"
REFERENCES,0.5284191829484902,"dataset, yielding consistent similarity scores above 99% across all similarity metrics for both PaI
554"
REFERENCES,0.5293072824156305,"and PaT. Interestingly, random sampling consistently outperforms the k-means selection strategy,
555"
REFERENCES,0.5301953818827708,"which involves selecting 6 representative samples per CIFAR-10 class. This is especially notable in
556"
REFERENCES,0.5310834813499112,"PaT, with random sampling showing up to a 7.51% higher Pearson correlation, 5% improvement in
557"
REFERENCES,0.5319715808170515,"ssim_index, and 1.14 reduction in Euclidean distance compared to k-means. This further reinforces
558"
REFERENCES,0.5328596802841918,"the statement that comparing activation patterns reflects the intrinsic ability of neural networks to
559"
REFERENCES,0.5337477797513321,"distinguish various input spaces, thus effectively extending the NEXP criterion to random input data
560"
REFERENCES,0.5346358792184724,"and laying the foundation for investigating Data-Agnostic strategies.
561"
REFERENCES,0.5355239786856127,"A.3
Weight Agnostic
562"
REFERENCES,0.5364120781527532,"Fig. 4 reveals that NEXPmap’s obtained at initialization and after network convergence share some
563"
REFERENCES,0.5373001776198935,"expressiveness pattern similarities, particularly in the initial layers. Detailed comparisons of these
564"
REFERENCES,0.5381882770870338,"similarities across all layers, and specifically for the first five, are presented in Table 5, contrasting
565"
REFERENCES,0.5390763765541741,"the initial maps with the true NEXPmap post-training. The summary of our numeric evaluation
566"
REFERENCES,0.5399644760213144,"confirms a notable correlation between the initialization and converged states for DenseNet-40 and
567"
REFERENCES,0.5408525754884547,"VGG-19, showing up to 84.10% and 86.82% in cosine similarity respectively. It also indicates
568"
REFERENCES,0.5417406749555951,"greater consistency in neural expressiveness measurements for the first layers of all networks, which
569"
REFERENCES,0.5426287744227354,"could be considered important for the formation of critical paths. In this context, the formation
570"
REFERENCES,0.5435168738898757,"of the final state depends on hyperparameter choices, like weight decay and learning rate, and the
571"
REFERENCES,0.544404973357016,"stochastic nature of training, that could potentially alter the model’s progression from its initial state,
572"
REFERENCES,0.5452930728241563,"as also highlighted by Frankle et al. [12]. In that manner, under the assumption that the selection
573"
REFERENCES,0.5461811722912966,"of hyperparameters remains congruent with the initialization, “Expressiveness"" can be considered a
574"
REFERENCES,0.5470692717584369,"fit criterion for Pruning at Initialization (PaI). In summary, the consistency of map measurements
575"
REFERENCES,0.5479573712255773,"between initial and final states may serve as a solid metric for evaluating NEXP’s ability to identify
576"
REFERENCES,0.5488454706927176,"winning tickets. Nevertheless, a more robust process of its evaluation should also take into account
577"
REFERENCES,0.5497335701598579,"the quality of the initial state as well as the subsequent training process.
578"
REFERENCES,0.5506216696269982,"Figure 4: Expressiveness statistics of feature maps from different convolutional layers and
architectures on CIFAR-10 (Extended). For each architecture we demonstrate the expressiveness
distribution for both an untrained instance of the model (PaI), as well as a converged one (PaT). The
x-axis represents the indices of convolutional layers and y-axis that of the feature maps in each layer.
To maintain consistency across the y-axis, we have interpolated each layer’s feature maps (pixel-wise)
to match the layer with the most feature maps. Columns denote different sampling strategies and
different colors denote different expressiveness values (the higher the value, the more expressive the
feature map). To approximate the expressiveness score of each element, denoted as “non-approx"",
we used 25% of all dataset’s samples (not 100% due to memory limitations) maintaining the label’s
distribution. As can be seen, the rank of each feature map (column of the sub-figure) is almost
unchanged (the same color), regardless of the image batches. Hence, even a small number of images
can effectively estimate the average rank of each feature map in different architectures."
REFERENCES,0.5515097690941385,"Table 3: Sensitivity analysis of the input’s sampling strategies after training (PaT) using various
similarity metrics."
REFERENCES,0.5523978685612788,"Model
Sampling
Euclidean
Cosine
Pearsonr
ssim_index
Strategy
Distance
Similarity
Similarity"
REFERENCES,0.5532859680284192,"ResNet-56 [17]
random
0.2349
0.9998
-
0.9979
k-means
1.3729
0.9949
-
0.9479"
REFERENCES,0.5541740674955595,"MobileNet-v2 [19]
random
0.2903
0.9994
0.9810
0.9988
k-means
1.1197
0.9960
0.9059
0.9794"
REFERENCES,0.5550621669626998,"DenseNet-40 [21]
random
0.2751
0.9997
0.9818
0.9970
k-means
1.1669
0.9959
0.9527
0.9614"
REFERENCES,0.5559502664298401,"VGG-19 [44]
random
0.5150
0.9989
0.9814
0.9894
k-means
0.8438
0.9964
0.9556
0.9728"
REFERENCES,0.5568383658969804,"Table 4: Sensitivity analysis of the input’s sampling strategies at Initialization (PaI) using various
similarity metrics."
REFERENCES,0.5577264653641207,"Model
Sampling
Euclidean
Cosine
Pearsonr
ssim_index
Strategy
Distance
Similarity
Similarity"
REFERENCES,0.5586145648312612,"ResNet-56 [17]
random
0.1333
0.9996
0.9979
0.9984
k-means
0.3948
0.9984
0.9868
0.9859"
REFERENCES,0.5595026642984015,"MobileNet-v2 [19]
random
0.0340
0.9565
-
0.9994
k-means
0.2441
0.9454
-
0.9776"
REFERENCES,0.5603907637655418,"DenseNet-40 [21]
random
0.2297
0.9997
0.9927
0.9977
k-means
0.2972
0.9994
0.9941
0.9955"
REFERENCES,0.5612788632326821,"VGG-19 [44]
random
0.2688
0.9988
0.9652
0.9950
k-means
0.4882
0.9975
0.9724
0.9856"
REFERENCES,0.5621669626998224,"Table 5: Sensitivity analysis of NEXPmap’s retrieved at initialization compared with the true
NEXPmap following model convergence."
REFERENCES,0.5630550621669627,"Model
Metric
random
k-means
non-approx (PaI)
All
first-5
All
first-5
All
first-5"
REFERENCES,0.5639431616341031,ResNet-56 [17]
REFERENCES,0.5648312611012434,"Euclidean Distance
9.0326
5.2005
8.8029
5.1177
8.9986
5.1850
Cosine Similarity
0.7584
0.8765
0.7677
0.8784
0.7592
0.8751
ssim_index
0.0194
0.3794
0.0243
0.3990
0.0206
0.3810"
REFERENCES,0.5657193605683837,MobileNet-v2 [19]
REFERENCES,0.566607460035524,"Euclidean Distance
10.5470
7.4966
10.6056
8.0843
10.5492
7.5134
Cosine Similarity
0.4645
0.6478
0.4910
0.5862
0.6702
0.6461
ssim_index
-0.0018
0.1187
-0.0011
0.0942
-0.0020
0.1142"
REFERENCES,0.5674955595026643,DenseNet-40 [21]
REFERENCES,0.5683836589698046,"Euclidean Distance
6.1326
4.6957
6.0594
4.7157
6.1043
4.7364
Cosine Similarity
0.8357
0.8769
0.8410
0.8762
0.8378
0.8761
ssim_index
0.0169
0.4552
0.0101
0.4493
0.0150
0.4464"
REFERENCES,0.5692717584369449,VGG-19 [44]
REFERENCES,0.5701598579040853,"Euclidean Distance
6.3171
4.9532
6.1194
4.8525
6.3083
4.9810
Cosine Similarity
0.8610
0.8979
0.8682
0.9030
0.8624
0.8972
ssim_index
0.0808
0.3798
0.0812
0.3844
0.0808
0.3712"
REFERENCES,0.5710479573712256,"B
Pruning Process: An in-depth analysis
579"
REFERENCES,0.5719360568383659,"B.1
Global vs local -scope pruning.
580"
REFERENCES,0.5728241563055062,"NEXP is used in the pruning process to evaluate and rank different network elements, guiding their
581"
REFERENCES,0.5737122557726465,"subsequent removal based on their scores. In our study, we focused on the removal of filters, i.e.,
582"
REFERENCES,0.5746003552397868,"Filter Pruning, where we pruned convolutional structures by removing the least expressive filters.
583"
REFERENCES,0.5754884547069272,"This can be approached in two ways: (i) on a local (layer-by-layer) basis, where filters are assessed
584"
REFERENCES,0.5763765541740675,"and removed according to their expressiveness relative to other filters within the same layer, e.g.,
585"
REFERENCES,0.5772646536412078,"eliminating the least µ expressive filters from each layer. (ii) On a global (network-wide) basis, where
586"
REFERENCES,0.5781527531083481,"all filters across layers are normalized in terms of their scores, allowing for the removal of the least
587"
REFERENCES,0.5790408525754884,"κ expressive filters from the entire network. We experimentally observed that “Global Pruning""
588"
REFERENCES,0.5799289520426287,"yields consistent results and outperforms “Local Pruning"" when using the NEXP pruning criterion.
589"
REFERENCES,0.5808170515097691,"Therefore, all the experiments reported in this paper were conducted using the “Global Pruning""
590"
REFERENCES,0.5817051509769094,"approach.
591"
REFERENCES,0.5825932504440497,"B.2
One-shot vs Iterative pruning.
592"
REFERENCES,0.58348134991119,"Furthermore, another design parameter to consider in the pruning process is its coordination with
593"
REFERENCES,0.5843694493783304,"fine-tuning. In this context, two widely adopted strategies are: (a) “One-Shot"" pruning, where pruning
594"
REFERENCES,0.5852575488454707,"is completed entirely before any fine-tuning occurs, and (b) “Iterative"" pruning, which involves
595"
REFERENCES,0.5861456483126111,"alternating between pruning and fine-tuning via an iterative sequence. The first one (a) can be
596"
REFERENCES,0.5870337477797514,"considered a more lightweight approach and allows for a more robust evaluation of the pruning metric
597"
REFERENCES,0.5879218472468917,"at hand, when compared to the later one (b). This is because it has no extra dependency on the training
598"
REFERENCES,0.588809946714032,"data and its efficiency does not depend on the iterative re-calibration of the information state through
599"
REFERENCES,0.5896980461811723,"the fine-tuning process. In this study, most experiments where conducted using “One-Shot"" pruning,
600"
REFERENCES,0.5905861456483126,"while we also explored the integration of NEXP in an “Iterative"" pruning process with YOLOv8
601"
REFERENCES,0.5914742451154529,"(more details on 4.1), where we noted a reduction in performance declines and an improvement in
602"
REFERENCES,0.5923623445825933,"the performance recovery after each pruning step, leading to better overall results.
603"
REFERENCES,0.5932504440497336,"B.3
Detailed description of all algorithmic steps.
604"
REFERENCES,0.5941385435168739,"More in detail regarding Algorithm 1, we define the data structure NEXPmap, i.e., a dictionary
605"
REFERENCES,0.5950266429840142,"in our implementation, to store the NEXP scores for every filter in the neural network after each
606"
REFERENCES,0.5959147424511545,"iteration. Given a neural network N with its current weight state Wti, we initially set up all variables
607"
REFERENCES,0.5968028419182948,"required for the pruning loop (Lines 1-4). The network is then gradually pruned until one of the
608"
REFERENCES,0.5976909413854352,"following conditions is met: the target ratio is achieved or the allowed number of pruning steps
609"
REFERENCES,0.5985790408525755,"is exceeded (Line 5). During each pruning iteration, the κ least expressive filters from the current
610"
REFERENCES,0.5994671403197158,"pruned state of the network are initially selected (Line 6). These filters are then removed, followed
611"
REFERENCES,0.6003552397868561,"by an update to NEXPmap for the subsequent iteration (Lines 7-8). To obtain the NEXP scores,
612"
REFERENCES,0.6012433392539964,"a forward pass f(X; Wpruned) is conducted using a user-provided mini-batch as input. Finally, the
613"
REFERENCES,0.6021314387211367,"conditions variables are updated in preparation for the next pruning iteration (Lines 9-10).
614"
REFERENCES,0.6030195381882771,"B.4
Acceleration of NEXP computations.
615"
REFERENCES,0.6039076376554174,"In Algorithm 1, Line 8 accounts for the bulk of the computational complexity. Specifically, the
616"
REFERENCES,0.6047957371225577,"calculation of NEXPmap can be divided into two sub-processes: (i) performing a forward pass to
617"
REFERENCES,0.605683836589698,"retrieve all activation patterns, and (ii) estimating the NEXP score for each element in the map.
618"
REFERENCES,0.6065719360568383,"However, performing a forward pass can be considered negligible compared to computing the NEXP
619"
REFERENCES,0.6074600355239786,"score for each filter. This is because the later involves multiple comparisons between the activation
620"
REFERENCES,0.6083481349911191,"patterns of all samples in the mini-batch X for every filter. Two effective ways to reduce this
621"
REFERENCES,0.6092362344582594,"computational demand are: first, all operations involved in computing the NEXP score are compatible
622"
REFERENCES,0.6101243339253997,"with widely-used BLAS libraries, facilitating hardware acceleration; second, the frequency of score
623"
REFERENCES,0.61101243339254,"updates can be strategically decreased under certain conditions, e.g., every n pruning iterations.
624"
REFERENCES,0.6119005328596803,"C
Experimental Settings
625"
REFERENCES,0.6127886323268206,"C.1
Datasets and Models.
626"
REFERENCES,0.6136767317939609,"This paper explores Computer Vision tasks through extensive experiments on various datasets, such
627"
REFERENCES,0.6145648312611013,"as CIFAR-10 [24] and ImageNet [40] for image classification, and COCO [33] for object detection.
628"
REFERENCES,0.6154529307282416,"To demonstrate the robustness of our approach, we experiment on several popular architectures and a
629"
REFERENCES,0.6163410301953819,"wide span of architectural elements, including VGGNet with a plain structure [44], ResNet with a
630"
REFERENCES,0.6172291296625222,"residual structure [17], GoogLeNet with inception modules [45], MobileNet with depthwise separable
631"
REFERENCES,0.6181172291296625,"convolutions [19], DenseNet with dense blocks [21] and YOLOv8 with a variety of different modules,
632"
REFERENCES,0.6190053285968028,"e.g. C2f and SPPF [22].
633"
REFERENCES,0.6198934280639432,"C.2
Adversaries.
634"
REFERENCES,0.6207815275310835,"We assess the efficacy of expressiveness as criterion for Pruning both after Training (PaT) and at
635"
REFERENCES,0.6216696269982238,"Initialization (PaI), using arbitrary (random) data-points. For PaT (4.1), we compare against a plethora
636"
REFERENCES,0.6225577264653641,"of foundational and state-of-the-art approaches, ranging from filter magnitude-based [28, 32, 29]
637"
REFERENCES,0.6234458259325044,"and loss sensitivity-based [58] methods to feature-guided strategies [23, 30] and search algorithms
638"
REFERENCES,0.6243339253996447,"[35, 31]. Regarding PaI (4.3 and D.1), our comparison is two-fold, as we evaluate expressiveness
639"
REFERENCES,0.6252220248667851,"using (i) single-shot and (ii) iterative pruning. More specifically, the adversaries for PaI include
640"
REFERENCES,0.6261101243339254,"pruning with random scoring, two state-of-the-art single-shot pruning strategies, namely SNIP [27]
641"
REFERENCES,0.6269982238010657,"and GraSP [50], as well as one state-of-the-art iterative pruning strategy, named SynFlow [46].
642"
REFERENCES,0.627886323268206,"C.3
Evaluation Metrics.
643"
REFERENCES,0.6287744227353463,"To effectively quantify the efficiency of reported solutions, we adopt a 3-dimensional evaluation
644"
REFERENCES,0.6296625222024866,"space, consisting of i) two widely-used metrics i.e. FLOPs and params, that define the 2-dimensional
645"
REFERENCES,0.6305506216696269,"compression solution efficiency, alongside with ii) an NN model accuracy to assess the predictions
646"
REFERENCES,0.6314387211367674,"of pruned derivatives [3]. Within the compression space, we define, (a) Compression Ratio(↓) =
647"
REFERENCES,0.6323268206039077,"original size
compressed size and (b) Compressed Size Percentage (%) = compressed size"
REFERENCES,0.633214920071048,"original size
· 100. To assess task-specific
648"
REFERENCES,0.6341030195381883,"capabilities, we report the top-1 accuracy of pruned models for image classification on CIFAR-10
649"
REFERENCES,0.6349911190053286,"[24], both top-1 and top-5 accuracies for ImageNet [40], and the mean Average Precision (mAP) over
650"
REFERENCES,0.6358792184724689,"IoU (Intersection over Union) thresholds ranging from 0.5 to 0.95, denoted as mAP val
50−95, for object
651"
REFERENCES,0.6367673179396093,"detection on the COCO dataset [33].
652"
REFERENCES,0.6376554174067496,"C.4
Configurations.
653"
REFERENCES,0.6385435168738899,"We implement the proposed “expressiveness"" pruning criterion on PyTorch, version 2.0.1+cu117, by
654"
REFERENCES,0.6394316163410302,"extending the DepGraph pruning framework [11] to maintain models compatibility and to ensure
655"
REFERENCES,0.6403197158081705,"structural coupling during the removal of network elements e.g., simultaneously removing any inter-
656"
REFERENCES,0.6412078152753108,"dependent network elements such as kernel pairs of convolutional and batch-normalization batched
657"
REFERENCES,0.6420959147424512,"layers. All experiments are conducted on a NVIDIA GeForce RTX 3060 GPU with 12GB of GDDR6
658"
REFERENCES,0.6429840142095915,"SDRAM. For all experiments we use a batch of 64 random data-points to estimate expressiveness,
659"
REFERENCES,0.6438721136767318,"except those that are reported for CIFAR-10 and ImageNet on 4.1, where we used K-Means to select
660"
REFERENCES,0.6447602131438721,"60 samples (6 from each class). Additionally, the baseline models on CIFAR-10 were trained for 200
661"
REFERENCES,0.6456483126110124,"epochs by using 128 batch size and Stochastic Gradient Descent algorithm (SGD) with an initial
662"
REFERENCES,0.6465364120781527,"learning rate of 0.1 that is divided by 10 after 60 and 120 epochs respectively. For ImageNet models
663"
REFERENCES,0.6474245115452931,"and YOLOv8, we utilize the available pre-trained weights on PyTorch vision library and ultralytics
664"
REFERENCES,0.6483126110124334,"[22]. We fine-tune the pruned networks for 100 epochs on CIFAR-10 and for 30 epochs on ImageNet
665"
REFERENCES,0.6492007104795737,"to compensate for the performance loss, using a batch size of 128 and 32 respectively.
666"
REFERENCES,0.650088809946714,"D
Supplementary Experimental Results
667"
REFERENCES,0.6509769094138543,"D.1
Neural Expressiveness at Initialization: A comparative study
668"
REFERENCES,0.6518650088809946,"Adversaries. We establish our comparative study in a two-fold manner, as we compare expressiveness
669"
REFERENCES,0.6527531083481349,"against (i) single-shot and (ii) iterative pruning approaches. More specifically, the adversaries include
670"
REFERENCES,0.6536412078152753,"pruning with random scoring, two state-of-the-art single-shot pruning strategies, namely SNIP [27]
671"
REFERENCES,0.6545293072824157,"and GraSP [50], as well as one state-of-the-art iterative pruning strategy, named SynFlow [46]. For
672"
REFERENCES,0.655417406749556,"our approach, we implement one-shot pruning, utilizing a batch of 64 arbitrary data points for the
673"
REFERENCES,0.6563055062166963,"estimation of expressiveness.
674"
REFERENCES,0.6571936056838366,"Experimental Setup. We adopt the experimental framework of Tanaka et al. [46], who assess
675"
REFERENCES,0.6580817051509769,"algorithm performance across an exponential scale (10r) of parameters compression ratios r ∈
676"
REFERENCES,0.6589698046181173,"{0.00, 0.25, 0.50, 0.75, . . . }. Their proposed settings also enable for the evaluation of an algorithm’s
677"
REFERENCES,0.6598579040852576,"resilience to ""layer collapses"", typically observed at higher compression levels. Results. We prune
678"
REFERENCES,0.6607460035523979,"VGG-16 on CIFAR-10 and compare against the findings of [46]. We remain consistent with our
679"
REFERENCES,0.6616341030195382,"adversaries and train the model for 160 epochs, using a batch size of 128 and an initial learning rate
680"
REFERENCES,0.6625222024866785,"of 0.1, which is reduced by a factor of 10 after 60 and 120 epochs. The results are illustrated on
681"
REFERENCES,0.6634103019538188,"Fig. 5.
682"
REFERENCES,0.6642984014209592,"Figure 5: Pruning VGG-16 at Initialization on CIFAR-10. A comparative visualisation of SOTA
methods across an exponential scale of params compression ratios."
REFERENCES,0.6651865008880995,"Observations. Our method consistently outperforms all other algorithms, particularly in regimes of
683"
REFERENCES,0.6660746003552398,"lower compression, up to 102(↓) with an average increase of 1.21% over SynFlow, while maintaining
684"
REFERENCES,0.6669626998223801,"competitiveness at higher compression levels, above 102(↓) with an average percentage difference of
685"
REFERENCES,0.6678507992895204,"4.82%, 3.72% and -2.74%, compared to GraSP, SNIP and SynFlow respectively.
686"
REFERENCES,0.6687388987566607,"D.2
Additional Experimental Results: Tables and Figures
687"
REFERENCES,0.6696269982238011,"CIFAR-10.
We present further experiments and comparisons with state-of-the-art methods,
688"
REFERENCES,0.6705150976909414,"including HRANK [30], GAL [32], ABC [31] and DCP [61], specifically for GoogLeNet and
689"
REFERENCES,0.6714031971580817,"MobileNet-v2 networks. For MobileNet-v2, our method attains an increased compression ratio of
690"
REFERENCES,0.672291296625222,"0.94× in parameters and 0.75× in FLOPs (↓), with a minimal decrease of only -0.09% in performance
691"
REFERENCES,0.6731793960923623,"compared to DCP. In the GoogLeNet case, we demonstrate a notable enhancement in parameters
692"
REFERENCES,0.6740674955595026,"compression within the 1.60× to 2.20× FLOPs compression range, surpassing GAL and HRANK
693"
REFERENCES,0.6749555950266429,"with margins of 1.8× and 1.52× respectively, with an average improvement of 7.5% in performance
694"
REFERENCES,0.6758436944937833,"degradation.
695"
REFERENCES,0.6767317939609236,"Table 6: Analytical Comparison of Importance-based solutions and Expressiveness on CIFAR-10
using VGGNet architectures [44]."
REFERENCES,0.677619893428064,"top-1 acc
Compression Ratio ↓
Model
Method
Base (%)
∆(%)
#Params
#FLOPs"
REFERENCES,0.6785079928952042,VGG-16
REFERENCES,0.6793960923623446,"L1 [28]
93.25
+0.15
2.78×
1.52×
GAL-0.05 [32]
93.96
-0.19
4.46×
1.65×
GAL-0.1 [32]
-0.54
5.61×
1.82×
HRank [30]
93.96
-0.53
5.97×
2.15×"
REFERENCES,0.6802841918294849,"HRank [30]
93.96
-1.62
5.67×
2.89×
SCP [23]
93.85
-0.06
15.38×
2.96×
NEXP (Ours)
93.87
-0.16
5.62×
3.03×"
REFERENCES,0.6811722912966253,"ABC [31]
93.02
+0.06
8.80×
3.80×
NEXP (Ours)
93.87
-0.35
13.13×
4.01×
HRank [30]
93.96
-2.73
8.41×
4.26×"
REFERENCES,0.6820603907637656,VGG-19
REFERENCES,0.6829484902309059,"DCP-Adapt [61]
93.99
+0.58
15.58×
2.86×
SCP [23]
93.84
-0.02
20.88×
3.86×
NEXP (Ours)
94.00
-0.53
22.73×
4.75×"
REFERENCES,0.6838365896980462,"Table 7: Analytical Comparison of Importance-based solutions and Expressiveness on CIFAR-10
using GoogLeNet [45]."
REFERENCES,0.6847246891651865,"top-1 acc
Compression Ratio ↓
Model
Method
Base (%)
∆(%)
#Params
#FLOPs"
REFERENCES,0.6856127886323268,GoogLeNet
REFERENCES,0.6865008880994672,"GAL-0.5 [32]
95.05
-0.49
1.97×
1.62×
NEXP (Ours)
94.97
-0.43
3.77×
2.12×
Hrank [30]
95.05
-0.52
2.25×
2.20×"
REFERENCES,0.6873889875666075,"ABC [31]
95.05
-0.21
2.51×
2.99×
NEXP (Ours)
94.97
-1.07
7.02×
3.01×
Hrank [30]
95.05
-0.98
3.31×
3.38×"
REFERENCES,0.6882770870337478,"Table 8: Analytical Comparison of Importance-based solutions and Expressiveness on CIFAR-10
using DenseNet-40 [21]."
REFERENCES,0.6891651865008881,"top-1 acc
Compression Ratio ↓
Model
Method
Base (%)
∆(%)
#Params
#FLOPs"
REFERENCES,0.6900532859680284,DenseNet-40
REFERENCES,0.6909413854351687,"GAL-0.5 [32]
95.05
-0.49
1.97×
1.62×
Hrank [30]
95.05
-0.52
2.25×
2.20×
NEXP (Ours)
94.64
-0.89
2.72×
2.25×"
REFERENCES,0.6918294849023091,"NEXP (Ours)
94.64
-0.84
3.12×
2.51×
ABC [31]
95.05
-0.21
2.51×
2.99×
Hrank [30]
95.05
-0.98
3.31×
3.38×"
REFERENCES,0.6927175843694494,Table 9: Performance Outcomes for MobileNet-v2 on the CIFAR-10 Dataset.
REFERENCES,0.6936056838365897,"Method
Base (%)
∆Acc (%)
#Params ↓
#FLOPs ↓"
REFERENCES,0.69449378330373,"DCP [61]
94.47
+0.22
1.31×
1.36×
NEXP (Ours)
94.32
+0.13
2.25×
2.11×"
REFERENCES,0.6953818827708703,"YOLOv8.
Figure 6 compares Neural Expressiveness (NEXP) with Layer-Adaptive Magnitude-
696"
REFERENCES,0.6962699822380106,"Based Pruning (LAMP) [26], Network Slimming (SLIM) [35], Wang et al.’s DepGraph [11], and
697"
REFERENCES,0.6971580817051509,"Random Pruning for Object Detection on the COCO dataset, as discussed in 4.1.
698"
REFERENCES,0.6980461811722913,"Motivation. YOLOv8 [22] is the current state-of-the-art for Object Detection and Image Segmen-
699"
REFERENCES,0.6989342806394316,"tation, and has already been widely adopted by many for a variety of real-time applications, e.g.
700"
REFERENCES,0.6998223801065719,"Traffic Safety [1], Medical Imaging [39], Rip Currents Detection [10], and more. Such applications
701"
REFERENCES,0.7007104795737122,"could majorly benefit from model compression optimizations, achieving higher throughput ratios that
702"
REFERENCES,0.7015985790408525,"translate to increased resolution (FPS), and enabling deployment on hardware with strict resource
703"
REFERENCES,0.7024866785079928,"constraints.
704"
REFERENCES,0.7033747779751333,Figure 6: Pruning YOLOv8m trained on COCO for Object Detection.
REFERENCES,0.7042628774422736,"NeurIPS Paper Checklist
705"
CLAIMS,0.7051509769094139,"1. Claims
706"
CLAIMS,0.7060390763765542,"Question: Do the main claims made in the abstract and introduction accurately reflect the
707"
CLAIMS,0.7069271758436945,"paper’s contributions and scope?
708"
CLAIMS,0.7078152753108348,"Answer: [Yes]
709"
CLAIMS,0.7087033747779752,"Justification: The main contributions have been reflected and discussed across the whole
710"
CLAIMS,0.7095914742451155,"paper.
711"
CLAIMS,0.7104795737122558,"Guidelines:
712"
CLAIMS,0.7113676731793961,"• The answer NA means that the abstract and introduction do not include the claims
713"
CLAIMS,0.7122557726465364,"made in the paper.
714"
CLAIMS,0.7131438721136767,"• The abstract and/or introduction should clearly state the claims made, including the
715"
CLAIMS,0.7140319715808171,"contributions made in the paper and important assumptions and limitations. A No or
716"
CLAIMS,0.7149200710479574,"NA answer to this question will not be perceived well by the reviewers.
717"
CLAIMS,0.7158081705150977,"• The claims made should match theoretical and experimental results, and reflect how
718"
CLAIMS,0.716696269982238,"much the results can be expected to generalize to other settings.
719"
CLAIMS,0.7175843694493783,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
720"
CLAIMS,0.7184724689165186,"are not attained by the paper.
721"
LIMITATIONS,0.7193605683836589,"2. Limitations
722"
LIMITATIONS,0.7202486678507993,"Question: Does the paper discuss the limitations of the work performed by the authors?
723"
LIMITATIONS,0.7211367673179396,"Answer: [Yes]
724"
LIMITATIONS,0.7220248667850799,"Justification: While our work does not implicitly provide a Discussion section, we have
725"
LIMITATIONS,0.7229129662522202,"incorporated any discussions on the limitations and the intricacies of the provided solution
726"
LIMITATIONS,0.7238010657193605,"at its section separately.
727"
LIMITATIONS,0.7246891651865008,"Guidelines:
728"
LIMITATIONS,0.7255772646536413,"• The answer NA means that the paper has no limitation while the answer No means that
729"
LIMITATIONS,0.7264653641207816,"the paper has limitations, but those are not discussed in the paper.
730"
LIMITATIONS,0.7273534635879219,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
731"
LIMITATIONS,0.7282415630550622,"• The paper should point out any strong assumptions and how robust the results are to
732"
LIMITATIONS,0.7291296625222025,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
733"
LIMITATIONS,0.7300177619893428,"model well-specification, asymptotic approximations only holding locally). The authors
734"
LIMITATIONS,0.7309058614564832,"should reflect on how these assumptions might be violated in practice and what the
735"
LIMITATIONS,0.7317939609236235,"implications would be.
736"
LIMITATIONS,0.7326820603907638,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
737"
LIMITATIONS,0.7335701598579041,"only tested on a few datasets or with a few runs. In general, empirical results often
738"
LIMITATIONS,0.7344582593250444,"depend on implicit assumptions, which should be articulated.
739"
LIMITATIONS,0.7353463587921847,"• The authors should reflect on the factors that influence the performance of the approach.
740"
LIMITATIONS,0.7362344582593251,"For example, a facial recognition algorithm may perform poorly when image resolution
741"
LIMITATIONS,0.7371225577264654,"is low or images are taken in low lighting. Or a speech-to-text system might not be
742"
LIMITATIONS,0.7380106571936057,"used reliably to provide closed captions for online lectures because it fails to handle
743"
LIMITATIONS,0.738898756660746,"technical jargon.
744"
LIMITATIONS,0.7397868561278863,"• The authors should discuss the computational efficiency of the proposed algorithms
745"
LIMITATIONS,0.7406749555950266,"and how they scale with dataset size.
746"
LIMITATIONS,0.7415630550621669,"• If applicable, the authors should discuss possible limitations of their approach to
747"
LIMITATIONS,0.7424511545293073,"address problems of privacy and fairness.
748"
LIMITATIONS,0.7433392539964476,"• While the authors might fear that complete honesty about limitations might be used by
749"
LIMITATIONS,0.7442273534635879,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
750"
LIMITATIONS,0.7451154529307282,"limitations that aren’t acknowledged in the paper. The authors should use their best
751"
LIMITATIONS,0.7460035523978685,"judgment and recognize that individual actions in favor of transparency play an impor-
752"
LIMITATIONS,0.7468916518650088,"tant role in developing norms that preserve the integrity of the community. Reviewers
753"
LIMITATIONS,0.7477797513321492,"will be specifically instructed to not penalize honesty concerning limitations.
754"
THEORY ASSUMPTIONS AND PROOFS,0.7486678507992895,"3. Theory Assumptions and Proofs
755"
THEORY ASSUMPTIONS AND PROOFS,0.7495559502664298,"Question: For each theoretical result, does the paper provide the full set of assumptions and
756"
THEORY ASSUMPTIONS AND PROOFS,0.7504440497335702,"a complete (and correct) proof?
757"
THEORY ASSUMPTIONS AND PROOFS,0.7513321492007105,"Answer: [Yes]
758"
THEORY ASSUMPTIONS AND PROOFS,0.7522202486678508,"Justification: To the best of our knowledge, all the provided set of assumptions presented in
759"
THEORY ASSUMPTIONS AND PROOFS,0.7531083481349912,"Section 3 are complete.
760"
THEORY ASSUMPTIONS AND PROOFS,0.7539964476021315,"Guidelines:
761"
THEORY ASSUMPTIONS AND PROOFS,0.7548845470692718,"• The answer NA means that the paper does not include theoretical results.
762"
THEORY ASSUMPTIONS AND PROOFS,0.7557726465364121,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
763"
THEORY ASSUMPTIONS AND PROOFS,0.7566607460035524,"referenced.
764"
THEORY ASSUMPTIONS AND PROOFS,0.7575488454706927,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
765"
THEORY ASSUMPTIONS AND PROOFS,0.7584369449378331,"• The proofs can either appear in the main paper or the supplemental material, but if
766"
THEORY ASSUMPTIONS AND PROOFS,0.7593250444049734,"they appear in the supplemental material, the authors are encouraged to provide a short
767"
THEORY ASSUMPTIONS AND PROOFS,0.7602131438721137,"proof sketch to provide intuition.
768"
THEORY ASSUMPTIONS AND PROOFS,0.761101243339254,"• Inversely, any informal proof provided in the core of the paper should be complemented
769"
THEORY ASSUMPTIONS AND PROOFS,0.7619893428063943,"by formal proofs provided in appendix or supplemental material.
770"
THEORY ASSUMPTIONS AND PROOFS,0.7628774422735346,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
771"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7637655417406749,"4. Experimental Result Reproducibility
772"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7646536412078153,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
773"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7655417406749556,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
774"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7664298401420959,"of the paper (regardless of whether the code and data are provided or not)?
775"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7673179396092362,"Answer: [Yes]
776"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7682060390763765,"Justification: The paper specifies in great detail all the information necessary to understand
777"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7690941385435168,"the results, while also any subjectivity imposed by the experimental settings in regards to
778"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7699822380106572,"our claims and conclusions has been discussed. Detailed analysis of both the mathematical,
779"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7708703374777975,"technical and experimental intricacies have been included in our work.
780"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7717584369449378,"Guidelines:
781"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7726465364120781,"• The answer NA means that the paper does not include experiments.
782"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7735346358792184,"• If the paper includes experiments, a No answer to this question will not be perceived
783"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7744227353463587,"well by the reviewers: Making the paper reproducible is important, regardless of
784"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7753108348134992,"whether the code and data are provided or not.
785"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7761989342806395,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
786"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7770870337477798,"to make their results reproducible or verifiable.
787"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7779751332149201,"• Depending on the contribution, reproducibility can be accomplished in various ways.
788"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7788632326820604,"For example, if the contribution is a novel architecture, describing the architecture fully
789"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7797513321492007,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
790"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7806394316163411,"be necessary to either make it possible for others to replicate the model with the same
791"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7815275310834814,"dataset, or provide access to the model. In general. releasing code and data is often
792"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7824156305506217,"one good way to accomplish this, but reproducibility can also be provided via detailed
793"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.783303730017762,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
794"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7841918294849023,"of a large language model), releasing of a model checkpoint, or other means that are
795"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7850799289520426,"appropriate to the research performed.
796"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7859680284191829,"• While NeurIPS does not require releasing code, the conference does require all submis-
797"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7868561278863233,"sions to provide some reasonable avenue for reproducibility, which may depend on the
798"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7877442273534636,"nature of the contribution. For example
799"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7886323268206039,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
800"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7895204262877442,"to reproduce that algorithm.
801"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7904085257548845,"(b) If the contribution is primarily a new model architecture, the paper should describe
802"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7912966252220248,"the architecture clearly and fully.
803"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7921847246891652,"(c) If the contribution is a new model (e.g., a large language model), then there should
804"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7930728241563055,"either be a way to access this model for reproducing the results or a way to reproduce
805"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7939609236234458,"the model (e.g., with an open-source dataset or instructions for how to construct
806"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7948490230905861,"the dataset).
807"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7957371225577264,"(d) We recognize that reproducibility may be tricky in some cases, in which case
808"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7966252220248667,"authors are welcome to describe the particular way they provide for reproducibility.
809"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7975133214920072,"In the case of closed-source models, it may be that access to the model is limited in
810"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7984014209591475,"some way (e.g., to registered users), but it should be possible for other researchers
811"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7992895204262878,"to have some path to reproducing or verifying the results.
812"
OPEN ACCESS TO DATA AND CODE,0.8001776198934281,"5. Open access to data and code
813"
OPEN ACCESS TO DATA AND CODE,0.8010657193605684,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
814"
OPEN ACCESS TO DATA AND CODE,0.8019538188277087,"tions to faithfully reproduce the main experimental results, as described in supplemental
815"
OPEN ACCESS TO DATA AND CODE,0.8028419182948491,"material?
816"
OPEN ACCESS TO DATA AND CODE,0.8037300177619894,"Answer: [No]
817"
OPEN ACCESS TO DATA AND CODE,0.8046181172291297,"Justification: We plan to release the full code of the implementation and experiments upon
818"
OPEN ACCESS TO DATA AND CODE,0.80550621669627,"acceptance.
819"
OPEN ACCESS TO DATA AND CODE,0.8063943161634103,"Guidelines:
820"
OPEN ACCESS TO DATA AND CODE,0.8072824156305506,"• The answer NA means that paper does not include experiments requiring code.
821"
OPEN ACCESS TO DATA AND CODE,0.8081705150976909,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
822"
OPEN ACCESS TO DATA AND CODE,0.8090586145648313,"public/guides/CodeSubmissionPolicy) for more details.
823"
OPEN ACCESS TO DATA AND CODE,0.8099467140319716,"• While we encourage the release of code and data, we understand that this might not be
824"
OPEN ACCESS TO DATA AND CODE,0.8108348134991119,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
825"
OPEN ACCESS TO DATA AND CODE,0.8117229129662522,"including code, unless this is central to the contribution (e.g., for a new open-source
826"
OPEN ACCESS TO DATA AND CODE,0.8126110124333925,"benchmark).
827"
OPEN ACCESS TO DATA AND CODE,0.8134991119005328,"• The instructions should contain the exact command and environment needed to run to
828"
OPEN ACCESS TO DATA AND CODE,0.8143872113676732,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
829"
OPEN ACCESS TO DATA AND CODE,0.8152753108348135,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
830"
OPEN ACCESS TO DATA AND CODE,0.8161634103019538,"• The authors should provide instructions on data access and preparation, including how
831"
OPEN ACCESS TO DATA AND CODE,0.8170515097690941,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
832"
OPEN ACCESS TO DATA AND CODE,0.8179396092362344,"• The authors should provide scripts to reproduce all experimental results for the new
833"
OPEN ACCESS TO DATA AND CODE,0.8188277087033747,"proposed method and baselines. If only a subset of experiments are reproducible, they
834"
OPEN ACCESS TO DATA AND CODE,0.8197158081705151,"should state which ones are omitted from the script and why.
835"
OPEN ACCESS TO DATA AND CODE,0.8206039076376554,"• At submission time, to preserve anonymity, the authors should release anonymized
836"
OPEN ACCESS TO DATA AND CODE,0.8214920071047958,"versions (if applicable).
837"
OPEN ACCESS TO DATA AND CODE,0.822380106571936,"• Providing as much information as possible in supplemental material (appended to the
838"
OPEN ACCESS TO DATA AND CODE,0.8232682060390764,"paper) is recommended, but including URLs to data and code is permitted.
839"
OPEN ACCESS TO DATA AND CODE,0.8241563055062167,"6. Experimental Setting/Details
840"
OPEN ACCESS TO DATA AND CODE,0.8250444049733571,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
841"
OPEN ACCESS TO DATA AND CODE,0.8259325044404974,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
842"
OPEN ACCESS TO DATA AND CODE,0.8268206039076377,"results?
843"
OPEN ACCESS TO DATA AND CODE,0.827708703374778,"Answer: [Yes]
844"
OPEN ACCESS TO DATA AND CODE,0.8285968028419183,"Justification: The paper specifies all the training and test details necessary to understand
845"
OPEN ACCESS TO DATA AND CODE,0.8294849023090586,"the results. Each experiment is accompanied by a discussion of its experimental details
846"
OPEN ACCESS TO DATA AND CODE,0.8303730017761989,"and a reference to its experimental settings (Section 4). Additionally, an overview of the
847"
OPEN ACCESS TO DATA AND CODE,0.8312611012433393,"experiment settings can be found in Appendix C, and an in-depth analysis of the pruning
848"
OPEN ACCESS TO DATA AND CODE,0.8321492007104796,"procedure, including its implementation choices, is described in Appendix B.
849"
OPEN ACCESS TO DATA AND CODE,0.8330373001776199,"Guidelines:
850"
OPEN ACCESS TO DATA AND CODE,0.8339253996447602,"• The answer NA means that the paper does not include experiments.
851"
OPEN ACCESS TO DATA AND CODE,0.8348134991119005,"• The experimental setting should be presented in the core of the paper to a level of detail
852"
OPEN ACCESS TO DATA AND CODE,0.8357015985790408,"that is necessary to appreciate the results and make sense of them.
853"
OPEN ACCESS TO DATA AND CODE,0.8365896980461812,"• The full details can be provided either with the code, in appendix, or as supplemental
854"
OPEN ACCESS TO DATA AND CODE,0.8374777975133215,"material.
855"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8383658969804618,"7. Experiment Statistical Significance
856"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8392539964476021,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
857"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8401420959147424,"information about the statistical significance of the experiments?
858"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8410301953818827,"Answer: [No]
859"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8419182948490231,"Justification: While we do not explicitly address the statistical significance of each experi-
860"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8428063943161634,"ment (in a quantitative manner), we do discuss in great detail any assumptions or statistical
861"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8436944937833037,"implications of our experiments (in a qualitative manner).
862"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.844582593250444,"Guidelines:
863"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8454706927175843,"• The answer NA means that the paper does not include experiments.
864"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8463587921847247,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
865"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8472468916518651,"dence intervals, or statistical significance tests, at least for the experiments that support
866"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8481349911190054,"the main claims of the paper.
867"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8490230905861457,"• The factors of variability that the error bars are capturing should be clearly stated (for
868"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.849911190053286,"example, train/test split, initialization, random drawing of some parameter, or overall
869"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8507992895204263,"run with given experimental conditions).
870"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8516873889875666,"• The method for calculating the error bars should be explained (closed form formula,
871"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8525754884547069,"call to a library function, bootstrap, etc.)
872"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8534635879218473,"• The assumptions made should be given (e.g., Normally distributed errors).
873"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8543516873889876,"• It should be clear whether the error bar is the standard deviation or the standard error
874"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8552397868561279,"of the mean.
875"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8561278863232682,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
876"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8570159857904085,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
877"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8579040852575488,"of Normality of errors is not verified.
878"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8587921847246892,"• For asymmetric distributions, the authors should be careful not to show in tables or
879"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8596802841918295,"figures symmetric error bars that would yield results that are out of range (e.g. negative
880"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8605683836589698,"error rates).
881"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8614564831261101,"• If error bars are reported in tables or plots, The authors should explain in the text how
882"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8623445825932504,"they were calculated and reference the corresponding figures or tables in the text.
883"
EXPERIMENTS COMPUTE RESOURCES,0.8632326820603907,"8. Experiments Compute Resources
884"
EXPERIMENTS COMPUTE RESOURCES,0.8641207815275311,"Question: For each experiment, does the paper provide sufficient information on the com-
885"
EXPERIMENTS COMPUTE RESOURCES,0.8650088809946714,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
886"
EXPERIMENTS COMPUTE RESOURCES,0.8658969804618117,"the experiments?
887"
EXPERIMENTS COMPUTE RESOURCES,0.866785079928952,"Answer: [Yes]
888"
EXPERIMENTS COMPUTE RESOURCES,0.8676731793960923,"Justification: While we do not provide the exact times of executions and memory require-
889"
EXPERIMENTS COMPUTE RESOURCES,0.8685612788632326,"ments for each experiment, we do provide an in-depth analysis of all the parameters and
890"
EXPERIMENTS COMPUTE RESOURCES,0.8694493783303731,"experimental specifications, along side with the overview of the configurations that were
891"
EXPERIMENTS COMPUTE RESOURCES,0.8703374777975134,"used for this work Appendix C and Section 4.
892"
EXPERIMENTS COMPUTE RESOURCES,0.8712255772646537,"Guidelines:
893"
EXPERIMENTS COMPUTE RESOURCES,0.872113676731794,"• The answer NA means that the paper does not include experiments.
894"
EXPERIMENTS COMPUTE RESOURCES,0.8730017761989343,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
895"
EXPERIMENTS COMPUTE RESOURCES,0.8738898756660746,"or cloud provider, including relevant memory and storage.
896"
EXPERIMENTS COMPUTE RESOURCES,0.8747779751332149,"• The paper should provide the amount of compute required for each of the individual
897"
EXPERIMENTS COMPUTE RESOURCES,0.8756660746003553,"experimental runs as well as estimate the total compute.
898"
EXPERIMENTS COMPUTE RESOURCES,0.8765541740674956,"• The paper should disclose whether the full research project required more compute
899"
EXPERIMENTS COMPUTE RESOURCES,0.8774422735346359,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
900"
EXPERIMENTS COMPUTE RESOURCES,0.8783303730017762,"didn’t make it into the paper).
901"
CODE OF ETHICS,0.8792184724689165,"9. Code Of Ethics
902"
CODE OF ETHICS,0.8801065719360568,"Question: Does the research conducted in the paper conform, in every respect, with the
903"
CODE OF ETHICS,0.8809946714031972,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
904"
CODE OF ETHICS,0.8818827708703375,"Answer: [Yes]
905"
CODE OF ETHICS,0.8827708703374778,"Justification: We have thoroughly reviewed the research conducted in the paper and fully
906"
CODE OF ETHICS,0.8836589698046181,"agree that it conforms to the NeurIPS Code of Ethics.
907"
CODE OF ETHICS,0.8845470692717584,"Guidelines:
908"
CODE OF ETHICS,0.8854351687388987,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
909"
CODE OF ETHICS,0.8863232682060391,"• If the authors answer No, they should explain the special circumstances that require a
910"
CODE OF ETHICS,0.8872113676731794,"deviation from the Code of Ethics.
911"
CODE OF ETHICS,0.8880994671403197,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
912"
CODE OF ETHICS,0.88898756660746,"eration due to laws or regulations in their jurisdiction).
913"
BROADER IMPACTS,0.8898756660746003,"10. Broader Impacts
914"
BROADER IMPACTS,0.8907637655417406,"Question: Does the paper discuss both potential positive societal impacts and negative
915"
BROADER IMPACTS,0.8916518650088809,"societal impacts of the work performed?
916"
BROADER IMPACTS,0.8925399644760214,"Answer: [NA]
917"
BROADER IMPACTS,0.8934280639431617,"Justification: While our work does not directly discuss societal impacts, we do reference the
918"
BROADER IMPACTS,0.894316163410302,"eco-friendly implications of efficient models in the Introduction section 1. Additionally, we
919"
BROADER IMPACTS,0.8952042628774423,"highlight the potential indirect societal benefits that can arise from optimizing models, such
920"
BROADER IMPACTS,0.8960923623445826,"as in the case of YOLOv8 D.2. .
921"
BROADER IMPACTS,0.8969804618117229,"Guidelines:
922"
BROADER IMPACTS,0.8978685612788633,"• The answer NA means that there is no societal impact of the work performed.
923"
BROADER IMPACTS,0.8987566607460036,"• If the authors answer NA or No, they should explain why their work has no societal
924"
BROADER IMPACTS,0.8996447602131439,"impact or why the paper does not address societal impact.
925"
BROADER IMPACTS,0.9005328596802842,"• Examples of negative societal impacts include potential malicious or unintended uses
926"
BROADER IMPACTS,0.9014209591474245,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
927"
BROADER IMPACTS,0.9023090586145648,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
928"
BROADER IMPACTS,0.9031971580817052,"groups), privacy considerations, and security considerations.
929"
BROADER IMPACTS,0.9040852575488455,"• The conference expects that many papers will be foundational research and not tied
930"
BROADER IMPACTS,0.9049733570159858,"to particular applications, let alone deployments. However, if there is a direct path to
931"
BROADER IMPACTS,0.9058614564831261,"any negative applications, the authors should point it out. For example, it is legitimate
932"
BROADER IMPACTS,0.9067495559502664,"to point out that an improvement in the quality of generative models could be used to
933"
BROADER IMPACTS,0.9076376554174067,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
934"
BROADER IMPACTS,0.9085257548845471,"that a generic algorithm for optimizing neural networks could enable people to train
935"
BROADER IMPACTS,0.9094138543516874,"models that generate Deepfakes faster.
936"
BROADER IMPACTS,0.9103019538188277,"• The authors should consider possible harms that could arise when the technology is
937"
BROADER IMPACTS,0.911190053285968,"being used as intended and functioning correctly, harms that could arise when the
938"
BROADER IMPACTS,0.9120781527531083,"technology is being used as intended but gives incorrect results, and harms following
939"
BROADER IMPACTS,0.9129662522202486,"from (intentional or unintentional) misuse of the technology.
940"
BROADER IMPACTS,0.9138543516873889,"• If there are negative societal impacts, the authors could also discuss possible mitigation
941"
BROADER IMPACTS,0.9147424511545293,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
942"
BROADER IMPACTS,0.9156305506216696,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
943"
BROADER IMPACTS,0.91651865008881,"feedback over time, improving the efficiency and accessibility of ML).
944"
SAFEGUARDS,0.9174067495559503,"11. Safeguards
945"
SAFEGUARDS,0.9182948490230906,"Question: Does the paper describe safeguards that have been put in place for responsible
946"
SAFEGUARDS,0.9191829484902309,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
947"
SAFEGUARDS,0.9200710479573713,"image generators, or scraped datasets)?
948"
SAFEGUARDS,0.9209591474245116,"Answer: [NA]
949"
SAFEGUARDS,0.9218472468916519,"Justification: Our work emphasizes on efficiently compressing Neural Networks and does
950"
SAFEGUARDS,0.9227353463587922,"not target any specific use-case scenario, rather it addresses the greater challenge of Vision
951"
SAFEGUARDS,0.9236234458259325,"as whole.
952"
SAFEGUARDS,0.9245115452930728,"Guidelines:
953"
SAFEGUARDS,0.9253996447602132,"• The answer NA means that the paper poses no such risks.
954"
SAFEGUARDS,0.9262877442273535,"• Released models that have a high risk for misuse or dual-use should be released with
955"
SAFEGUARDS,0.9271758436944938,"necessary safeguards to allow for controlled use of the model, for example by requiring
956"
SAFEGUARDS,0.9280639431616341,"that users adhere to usage guidelines or restrictions to access the model or implementing
957"
SAFEGUARDS,0.9289520426287744,"safety filters.
958"
SAFEGUARDS,0.9298401420959147,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
959"
SAFEGUARDS,0.9307282415630551,"should describe how they avoided releasing unsafe images.
960"
SAFEGUARDS,0.9316163410301954,"• We recognize that providing effective safeguards is challenging, and many papers do
961"
SAFEGUARDS,0.9325044404973357,"not require this, but we encourage authors to take this into account and make a best
962"
SAFEGUARDS,0.933392539964476,"faith effort.
963"
LICENSES FOR EXISTING ASSETS,0.9342806394316163,"12. Licenses for existing assets
964"
LICENSES FOR EXISTING ASSETS,0.9351687388987566,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
965"
LICENSES FOR EXISTING ASSETS,0.9360568383658969,"the paper, properly credited and are the license and terms of use explicitly mentioned and
966"
LICENSES FOR EXISTING ASSETS,0.9369449378330373,"properly respected?
967"
LICENSES FOR EXISTING ASSETS,0.9378330373001776,"Answer: [Yes]
968"
LICENSES FOR EXISTING ASSETS,0.9387211367673179,"Justification: All the creators and original owners of the assets that were utilized for this
969"
LICENSES FOR EXISTING ASSETS,0.9396092362344582,"work were properly credited through-out all parts of the paper, while also a detailed report
970"
LICENSES FOR EXISTING ASSETS,0.9404973357015985,"of them can be found in Appendix C.
971"
LICENSES FOR EXISTING ASSETS,0.9413854351687388,"Guidelines:
972"
LICENSES FOR EXISTING ASSETS,0.9422735346358793,"• The answer NA means that the paper does not use existing assets.
973"
LICENSES FOR EXISTING ASSETS,0.9431616341030196,"• The authors should cite the original paper that produced the code package or dataset.
974"
LICENSES FOR EXISTING ASSETS,0.9440497335701599,"• The authors should state which version of the asset is used and, if possible, include a
975"
LICENSES FOR EXISTING ASSETS,0.9449378330373002,"URL.
976"
LICENSES FOR EXISTING ASSETS,0.9458259325044405,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
977"
LICENSES FOR EXISTING ASSETS,0.9467140319715808,"• For scraped data from a particular source (e.g., website), the copyright and terms of
978"
LICENSES FOR EXISTING ASSETS,0.9476021314387212,"service of that source should be provided.
979"
LICENSES FOR EXISTING ASSETS,0.9484902309058615,"• If assets are released, the license, copyright information, and terms of use in the
980"
LICENSES FOR EXISTING ASSETS,0.9493783303730018,"package should be provided. For popular datasets, paperswithcode.com/datasets
981"
LICENSES FOR EXISTING ASSETS,0.9502664298401421,"has curated licenses for some datasets. Their licensing guide can help determine the
982"
LICENSES FOR EXISTING ASSETS,0.9511545293072824,"license of a dataset.
983"
LICENSES FOR EXISTING ASSETS,0.9520426287744227,"• For existing datasets that are re-packaged, both the original license and the license of
984"
LICENSES FOR EXISTING ASSETS,0.9529307282415631,"the derived asset (if it has changed) should be provided.
985"
LICENSES FOR EXISTING ASSETS,0.9538188277087034,"• If this information is not available online, the authors are encouraged to reach out to
986"
LICENSES FOR EXISTING ASSETS,0.9547069271758437,"the asset’s creators.
987"
NEW ASSETS,0.955595026642984,"13. New Assets
988"
NEW ASSETS,0.9564831261101243,"Question: Are new assets introduced in the paper well documented and is the documentation
989"
NEW ASSETS,0.9573712255772646,"provided alongside the assets?
990"
NEW ASSETS,0.9582593250444049,"Answer: [NA]
991"
NEW ASSETS,0.9591474245115453,"Justification: The paper does not release new assets besides the conceptualization and both
992"
NEW ASSETS,0.9600355239786856,"then technical and theoretical formulation of Neural Expressiveness. However, we plan to
993"
NEW ASSETS,0.9609236234458259,"release the full code of the implementation and experiments upon acceptance.
994"
NEW ASSETS,0.9618117229129662,"Guidelines:
995"
NEW ASSETS,0.9626998223801065,"• The answer NA means that the paper does not release new assets.
996"
NEW ASSETS,0.9635879218472468,"• Researchers should communicate the details of the dataset/code/model as part of their
997"
NEW ASSETS,0.9644760213143873,"submissions via structured templates. This includes details about training, license,
998"
NEW ASSETS,0.9653641207815276,"limitations, etc.
999"
NEW ASSETS,0.9662522202486679,"• The paper should discuss whether and how consent was obtained from people whose
1000"
NEW ASSETS,0.9671403197158082,"asset is used.
1001"
NEW ASSETS,0.9680284191829485,"• At submission time, remember to anonymize your assets (if applicable). You can either
1002"
NEW ASSETS,0.9689165186500888,"create an anonymized URL or include an anonymized zip file.
1003"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9698046181172292,"14. Crowdsourcing and Research with Human Subjects
1004"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9706927175843695,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1005"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9715808170515098,"include the full text of instructions given to participants and screenshots, if applicable, as
1006"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724689165186501,"well as details about compensation (if any)?
1007"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9733570159857904,"Answer: [NA]
1008"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742451154529307,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
1009"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9751332149200711,"Guidelines:
1010"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760213143872114,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1011"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769094138543517,"human subjects.
1012"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977797513321492,"• Including this information in the supplemental material is fine, but if the main contribu-
1013"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9786856127886323,"tion of the paper involves human subjects, then as much detail as possible should be
1014"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795737122557726,"included in the main paper.
1015"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804618117229129,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1016"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813499111900533,"or other labor should be paid at least the minimum wage in the country of the data
1017"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9822380106571936,"collector.
1018"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831261101243339,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1019"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840142095914742,"Subjects
1020"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9849023090586145,"Question: Does the paper describe potential risks incurred by study participants, whether
1021"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9857904085257548,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1022"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9866785079928952,"approvals (or an equivalent approval/review based on the requirements of your country or
1023"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875666074600356,"institution) were obtained?
1024"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884547069271759,"Answer: [NA]
1025"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893428063943162,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
1026"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902309058614565,"Guidelines:
1027"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9911190053285968,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1028"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920071047957372,"human subjects.
1029"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9928952042628775,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1030"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937833037300178,"may be required for any human subjects research. If you obtained IRB approval, you
1031"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946714031971581,"should clearly state this in the paper.
1032"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955595026642984,"• We recognize that the procedures for this may vary significantly between institutions
1033"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964476021314387,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1034"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9973357015985791,"guidelines for their institution.
1035"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9982238010657194,"• For initial submissions, do not include any information that would break anonymity (if
1036"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991119005328597,"applicable), such as the institution conducting the review.
1037"
