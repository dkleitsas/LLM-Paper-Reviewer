Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0022675736961451248,"While a popular limit of inﬁnite-width neural networks, the Neural Tangent Ker-
nel (NTK) often exhibits performance gaps from ﬁnite-width neural networks on
standard datasets, due to lack of feature learning. Although the feature learning
maximal update limit, or µ-limit (Yang and Hu, 2020) of wide networks has closed
the gap for 1-hidden-layer linear models, no one has been able to demonstrate
this for deep nonlinear multi-layer perceptrons (MLP) because of µ-limit’s com-
putational difﬁculty in this setting. Here, we solve this problem by proposing a
novel feature learning limit, the π-limit, that bypasses the computational issues.
The π-limit, in short, is the limit of a form of projected gradient descent, and
the π-limit of an MLP is roughly another MLP where gradients are appended
to weights during training. We evaluate it on CIFAR10 and Omniglot against
NTK as well as ﬁnite networks, ﬁnding the π-limit outperform ﬁnite-width models
trained normally (without projection) in both settings, closing the performance gap
between ﬁnite- and inﬁnite-width neural networks previously left by NTK. Code
for this work is available at github.com/santacml/pilim."
ABSTRACT,0.0045351473922902496,"NTK
Finite-Width Network
-Width -Limit (Ours)"
ABSTRACT,0.006802721088435374,"Figure 1: PCA of representations of images from 5 classes (= 5 colors) in Omniglot test set.
We compare the representations from our best performing ﬁnite-width µ-parametrized network,
our proposed ∞-width π-limit, and NTK. The representations from former two form neat clusters
according to their classes while those from the latter are mixed up together. See Appendix B.5.3."
INTRODUCTION,0.009070294784580499,"1
INTRODUCTION"
INTRODUCTION,0.011337868480725623,"The theory of Neural Tangent Kernel (NTK) (Jacot et al., 2018) led to many theoretical discoveries of
neural networks, but they ultimately apply only to a limited, unrealistic regime where the network
does not learn features. Recently, Yang and Hu (2020) discovered the maximal update, or µ,
parametrization that induces a feature learning ∞-width limit, called the µ-limit of deep neural
networks, which generalizes the mean ﬁeld limit of shallow networks (Sirignano and Spiliopoulos,
2018; Mei et al., 2018; Chizat and Bach, 2018; Rotskoff and Vanden-Eijnden, 2018). They explicitly
trained such ∞-width limits on Word2Vec and MAML, tasks relying crucially on feature learning.
These µ-limits outperformed both NTK baselines and ﬁnite-width networks. Thus, the µ-limit seems
to capture key properties of neural networks in practice and is a promising direction to further our
theoretical understanding of them.
However, a signiﬁcant roadblock to this end is that the µ-limit for general neural networks is both hard
to compute empirically and hard to analyze theoretically. Indeed, Yang and Hu (2020) restricted main
experiments to linear MLPs where exact calculation is fast as a special case. According to them, the
main complications come from the interleaving of forward and backward propagations that leads to
the need to evaluate Gaussian integrals involving nested nonlinearities. Likewise, these complicated
integrals also contribute to the difﬁculty of theoretical analysis, especially in deep networks."
INTRODUCTION,0.013605442176870748,"∗Correspondence to {gregyang,Michael.Santacroce}@microsoft.com"
INTRODUCTION,0.015873015873015872,Published as a conference paper at ICLR 2022
INTRODUCTION,0.018140589569160998,"Contributions
In this work, we bypass these issues by considering a form of projected gradient
descent instead. Roughly speaking, with Vφ denoting the V-transform (aka dual) of φ (Deﬁnition 3.1)."
INTRODUCTION,0.02040816326530612,"MLP with nonlinearity φ
trained by projected gradient accumulation
width→∞
−−−−−−−−−−−−−−→
MLP with nonlinearity Vφ
trained by gradient concatenation
(⋆)"
INTRODUCTION,0.022675736961451247,"Here gradient accumulation is just the normal training process where gradient is added to a parameter,
whereas in gradient concatenation, gradient is appended to a parameter. The RHS retains the feature
learning capability while making the computation and the mathematics much simpler compared to the
µ-limit. We explicitly evaluate our new limit, which we call the π-limit, on CIFAR10 and Omniglot,
and compare against ﬁnite-width neural networks as well as NTK and NNGP. We ﬁnd the π-limit
always outperform wide standard-parametrized or µ-parametrized networks, and they all signiﬁcantly
outperform the kernels. For example, on Omniglot, this is evidenced by Fig. 1. We also demonstrate
transfer learning from ImageNet to CIFAR10. To our knowledge, this is the ﬁrst time deep nonlinear
feature learning ∞-width limits are evaluated on nontrivial natural datasets."
INTRODUCTION,0.024943310657596373,"2
WHY IS THE µ-LIMIT HARD TO COMPUTE? AN EXAMPLE"
INTRODUCTION,0.027210884353741496,"While the µ-limit is the “optimal” feature learning limit of a neural network as described in Yang
and Hu (2020), it is difﬁcult to calculate analytically. Let’s demonstrate this difﬁculty with a simple
1-hidden-layer µ-parametrized network (abbreviated µ-net, whose ∞-width limit is the µ-limit) f
with 1-dimensional input ξ ∈R and output f(ξ) ∈R:
f(ξ) = n−1/2v⊤φ(√nuξ) ∈R,
(1)
where u, v ∈Rn are column vectors and φ : R →R is the nonlinearity (e.g. relu). Note here n is the
width of f. According to µ-parametrization, we initialize uα, vα ∼N(0, 1"
INTRODUCTION,0.02947845804988662,"n).
At initialization, one easily sees limn→∞f(ξ) = 0 because"
INTRODUCTION,0.031746031746031744,"f(ξ) = 1 n n
X"
INTRODUCTION,0.034013605442176874,"α=1
(√nv)αφ(ξ√nuα)"
INTRODUCTION,0.036281179138321996,"is the average of a large number of iid random variables (√nv)αφ(ξ√nuα) with zero mean. In the
language of Tensor Programs (Yang, 2019b;a; 2020a;b), we have an almost sure convergence"
INTRODUCTION,0.03854875283446712,"f(ξ) →E Z
√nvφ(ξZ
√nu) = 0,
at initialization,"
INTRODUCTION,0.04081632653061224,"where Z
√nv and Z
√nu represent the independent standard Gaussian random variables describing the
coordinate distributions of √nv and √nu.
However, one can see that f(ξ) has a limit of the form"
INTRODUCTION,0.04308390022675737,"f(ξ) →E(Z
√nv + · · · )φ(ξZ
√nu + (· · · )φ′(· · · )),
after 1 step of SGD,
(2)
where u and v still represent values at initialization, and · · · represent terms coming from the gradient
update.1 Even though the · · · appearing inside φ() turn out to be all Gaussians, this expectation is
hard to evaluate analytically due to the nesting of φ′ inside φ, unless φ is polynomial. Further-
more, as more SGD steps are taken, this nesting quickly becomes more complicated, so even if φ is
polynomial, the time needed for evaluation compounds exponentially."
INTRODUCTION,0.045351473922902494,"3
SOLUTION: PROJECT THE GRADIENT"
INTRODUCTION,0.047619047619047616,"To alleviate this difﬁculty, we propose to study the limit of training a neural network under a projected
form of gradient descent which we call π-SGD, i.e., we update θ ←θ −ηΠ∇θL where Π is a linear
projection and θ is the vector of all parameters. We shall ﬁrst describe this projection Π for the simple
motivating example above before doing so for the general case."
INTRODUCTION,0.049886621315192746,"3.1
CONTINUING THE 1-HIDDEN-LAYER EXAMPLE"
INTRODUCTION,0.05215419501133787,"In the example of Eq. (1), Π acts on the gradients as follows: 1) Π leaves the output layer gradient
∇vL untouched, but 2) projects the input layer gradient2 ∇uL = (· · · )φ′(· · · ) to the linear span of
u0, the initial value of input weights u:3"
INTRODUCTION,0.05442176870748299,"Π(∇vL, ∇uL) = (∇vL, Πu0∇uL) = (∇vL, cu0)
for some c ∈R."
INTRODUCTION,0.05668934240362812,"1See Appendix C for a detailed calculation.
2The precise form is ∇uL = L′vφ′(√nuξ0).
3This projection may look excessively reductive, but this is just an artifact of the 1-dimensional input and the
shallow depth. See Section 3.2 for the general case."
INTRODUCTION,0.05895691609977324,Published as a conference paper at ICLR 2022
INTRODUCTION,0.061224489795918366,"Here Πu0 denotes the orthogonal projection to the span of u0. In particular, after projection, ∇uL
now has roughly iid Gaussian coordinates (proportional to u0).4 Then unlike Eq. (2), we avoid the
nesting of φ′ inside φ:"
INTRODUCTION,0.06349206349206349,"f(ξ) →E(Z
√nv + · · · )φ(˜cZ
√nu),
after 1 step of π-SGD,
(3)"
INTRODUCTION,0.06575963718820861,"for some deterministic scalar ˜c. This expectation can then be evaluated routinely using the V-
transform of φ (e.g. Cho and Saul (2009) for relu; see Deﬁnition 3.1 below), and likewise, so can the
expectations involved in all later steps. After formalization inside a Tensor Program, this rigorously
gives rise to the ∞-width limit of f trained under π-SGD, which we call the π-limit of f. See
Theorem 3.2.
Deﬁnition 3.1. Let Vφ : R3 →R denote the V-transform of φ, such that Vφ(E XY, E X2, E Y 2) =
E φ(X)φ(Y ) for any centered jointly Gaussian random variables (X, Y ) ∈R2. We will treat Vφ like
an activation function and automatically vectorize when Vφ is applied to 3 vectors of the same length."
INTRODUCTION,0.06802721088435375,"3.1.1
COMPUTING THE π-LIMIT
Here we sketch how to compute the π-limit of Eq. (1). Let ut, vt, ft denote corresponding objects at
time t, with t = 0 denoting initialization."
INTRODUCTION,0.07029478458049887,"Memory and Time Requirements
What information do we need to store in order to calculate the
π-limit? Because of the projection, ut = ctu0 for some ct ∈R. So to calculate the limit of ft, we
need to track ct (or rather its ∞-width limit). This memory requirement is Θ(1) in training time t.
To see the form of vt, it helps to simplify our setup a bit more by 1) initializing v0 = 0 and by 2)
assuming each π-SGD step involves a minibatch containing a sole input ξt. Correspondingly, because
the gradient of vt is always proportional to φ(ξt
√nut) = φ(ξtct
√nu0), there exist coefﬁcients
{as ∈R}t−1
s=0, {bs ∈R}t−1
s=0 (which are random, but deterministic conditioned on u0, v0) such that"
INTRODUCTION,0.07256235827664399,"√nvt = t−1
X"
INTRODUCTION,0.07482993197278912,"s=0
asφ(bs
√nu0).
(4)"
INTRODUCTION,0.07709750566893424,"Here bs = ξscs and as is formed from the learning rate and loss derivative. So to calculate the limit
of ft, we need {as ∈R}t−1
s=0, {bs ∈R}t−1
s=0 (or rather their n →∞limits). Note that ct, as, bs all
have nontrivial ﬂuctuations for ﬁnite n, but become deterministic as n →∞.
This implies a memory requirement of Θ(t), which is also the total requirement for computing the
limit of ft. As we will see, each forward and backward pass has runtime Θ(t) as well, so the total
runtime for calculating limn→∞ft is Θ(t2),5 compared to the exponential runtime of general µ-limit."
INTRODUCTION,0.07936507936507936,"Forward Pass
Using Eq. (4), we can intuit (and formalize using Tensor Programs) that"
INTRODUCTION,0.08163265306122448,"ft(ξ) = 1 n n
X α=1 t−1
X"
INTRODUCTION,0.08390022675736962,"s=0
asφ(bs
√nu0α) !"
INTRODUCTION,0.08616780045351474,"φ(ξct
√nu0α) → t−1
X"
INTRODUCTION,0.08843537414965986,"s=0
˚as E φ(˚bsZ
√nu0)φ(ξ˚ctZ
√nu0)
(5)"
INTRODUCTION,0.09070294784580499,"where˚as,˚bs,˚ct denote the deterministic limits of the corresponding quantities. The expectation in the
RHS can be evaluated using V-transforms. The t terms in the summation implies a runtime of Θ(t)."
INTRODUCTION,0.09297052154195011,"Backward Pass
The gradient update for the output weights vt is clearly represented by setting
(at+1, bt+1) ←(−ηL′, ξct), where L′ denotes the loss derivative ∂L(ft(ξ), label)/∂ft(ξ). However,
a priori, it seems unclear how to calculate the limit of the projected gradient Πu0∇utL of the input
weights. Fortunately, because we can express the entire unrolled π-SGD training inside a Tensor
Program, the Master Theorem (Yang, 2020b) can automatically give us the exact formulas for the
gradient limit; see Appendix F.
But in fact, there is a more intuitive way of obtaining the backward pass limit by recognizing that a
projected gradient is just the maximal ascent direction in the projected space.6 In our case, Πu0∇utL"
INTRODUCTION,0.09523809523809523,"4This is because, while c has ﬂuctuations for ﬁnite n, it is roughly constant for large n.
5Assuming minibatch size is constant, and we train for a constant number of epochs, this translates to Θ(N 2)
to train lim ft, where N is dataset size, and Θ(N) to do inference on a single input. This compares favorably
with Gaussian Processes, which requires Θ(N 3) to “train” (i.e. inverting the kernel), and Θ(N) for inference.
However, in our experiments here, the constant in our Θ(N 2) in practice will make training lim ft slower than
the corresponding NTK or NNGP kernel regression.
6See Lemma F.1 for more details."
INTRODUCTION,0.09750566893424037,Published as a conference paper at ICLR 2022
INTRODUCTION,0.09977324263038549,"Figure 2: Summary of the π-limit for the 1-hidden-layer network of Eq. (1). (Left) Representation
of width-n µ-net with nonlinearity φ and its π-SGD update. Here v is depicted as
1
√nA⊤φ(B ⊗
√nu0), which is equivalent to Eq. (4). In π-SGD, the ﬁrst layer gradient is projected to the space
spanned by u0 before being accumulated to the weights. (Right) This network’s ∞-width limit can
be roughly thought of as another 1-hidden-layer neural network with nonlinearity Vφ (Eq. (6)). But
updates are appended to A, B instead of added. See Theorem 3.2 and compare to Eq. (⋆)."
INTRODUCTION,0.10204081632653061,"is just the maximal ascent direction of L in the span of u0, i.e. Πu0∇utL = (∇ctL)u0, where ∇ctL
is the derivative w.r.t. the coefﬁcient ct of ut in terms of u0. In the ∞-width limit, this derivative can
be obtained by just auto-differentiating Eq. (5) when the V-transform has an explicit form, like for
φ = relu (Cho and Saul, 2009). So a π-SGD step on ut is equivalent to taking ct+1 ←ct −η∇ctL."
INTRODUCTION,0.10430839002267574,"Summary
Below, for brevity, we say training routine to mean the package of learning rate η,
training sequence of singleton minibatches {(ξt, yt)}t≥0 (where ξt is the input and yt is the label),7
and a loss function L(f(ξ), y) that is continuously differentiable in the prediction of the model f(ξ)."
INTRODUCTION,0.10657596371882086,"Theorem 3.2. Consider the simple motivating example of 1-hidden-layer network f in Eq. (1) with 1-
dimensional input and outputs, initialized by uα ∼N(0, 1/n) and vα ←0. Suppose its nonlinearity
φ has a polynomially bounded 2nd derivative. For any training routine, f trained by π-SGD for T
steps has an ∞-width limit ˚
fT , in the sense that,"
INTRODUCTION,0.10884353741496598,"as n →∞,
fT (ξ) →˚
fT (ξ),
for any ξ ∈R,"
INTRODUCTION,0.1111111111111111,"where the limit is almost sure. ˚
fT is given as follows:
Initialize A0, B0 as empty column vectors, and initialize C0 ∈R as 1. Recall Vφ is the V-transform
of φ. For each t = 0, 1, . . ., deﬁne the function (where dot product of empty vectors is 0)"
INTRODUCTION,0.11337868480725624,"˚
ft(ξ) def= A⊤
t Vφ(CtξBt, Bt ◦Bt, C2
t ξ21),
for any ξ ∈R,
(6)"
INTRODUCTION,0.11564625850340136,"where ◦denotes coordinatewise product and 1 denotes the all-1s vector of appropriate shape, and
At, Bt, Ct are inductively given by Ct+1
def= Ct −η∇CtL(˚
ft(ξt), yt) and"
INTRODUCTION,0.11791383219954649,"At+1
def= append(At, −η∇˚
ft(ξt)L(˚
ft(ξt), yt)),
Bt+1
def= append(Bt, Ctξt)"
INTRODUCTION,0.12018140589569161,"where append(v, p) appends element p to the end of vector v, increasing the dimension of v by 1."
INTRODUCTION,0.12244897959183673,"Here At, Bt correspond to the column vectors formed from the ∞-width limits of {as ∈R}t−1
s=0, {bs ∈
R}t−1
s=0, and Ct corresponds to the limit of ct.8 As discussed above, computing ˚
fT requires Θ(T)
memory and Θ(T 2) time. Theorem 3.2 is summarized by Fig. 2."
INTRODUCTION,0.12471655328798185,"3.2
π-PARAMETRIZATION FOR DEEP NETWORKS"
INTRODUCTION,0.12698412698412698,"We can straightforwardly generalize π-SGD and the π-limit theorem (Theorem 3.2) to deep MLPs.
However, due to the n × n Gaussian matrix initialization in the middle of the network, the memory
requirement will be Θ(T 2) and runtime will be Θ(T 3) for training T steps, for the same reason as
discussed in Yang and Hu (2020, Sec 8). This is not scalable to datasets like CIFAR10. Therefore,
we propose a different initialization that brings down the memory requirement to Θ(T) and runtime
to Θ(T 2), just like the 1-hidden-layer case. Consider an L-hidden-layer µ-net f : Rd →Rdout: For"
INTRODUCTION,0.1292517006802721,"7For simplicity, we only consider batch size 1; it’s straightforward to generalize to larger batch sizes.
8Technically, we should have written ˚
At, ˚
Bt, ˚
Ct to maintain the convention that ˚□denotes limit of □, but
for the sake of brevity, we drop this convention in Theorem 3.2."
INTRODUCTION,0.13151927437641722,Published as a conference paper at ICLR 2022
INTRODUCTION,0.13378684807256236,"weight matrices w1 ∈Rn×d and w2, . . . , wL ∈Rn×n, and nonlinearity φ : R →R, such a neural
network on input ξ ∈Rd is given by h1(ξ) = √nw1ξ ∈Rn, and"
INTRODUCTION,0.1360544217687075,"xl(ξ) = φ(hl(ξ)) ∈Rn,
hl+1(ξ) = wl+1xl(ξ) ∈Rn,
for l = 1, . . . , L −1,
(7)"
INTRODUCTION,0.1383219954648526,"and the network output is f(ξ) = n−1/2wL+1xL(ξ) for wL+1 ∈Rdout×n. In µ-parametrization, we
would initialize wl
αβ ∼N(0, 1/n) for any α, β, l. Next, we describe our alternative proposal."
INTRODUCTION,0.14058956916099774,"π-Initialization
Choose integers r, M, to be explained shortly; these numbers should all be thought
of as constant in width n. Suppose we are given a collection P of matrices"
INTRODUCTION,0.14285714285714285,"P def= {Al, Bl ∈RM×r}L
l=2 ∪{A1 ∈Rd×r} ∪{BL+1 ∈RM×r, AL+1 ∈RM×dout}.
(8)"
INTRODUCTION,0.14512471655328799,"Then we can initialize weights wl by ﬁrst sampling a standard random Gaussian matrix Ω∈
Rn×r, Ωαi ∼N(0, 1), before setting wl ←1"
INTRODUCTION,0.1473922902494331,"nΩAl⊤φ(BlΩ⊤) ∈Rn×n, for all hidden l = 2, . . . , L;"
INTRODUCTION,0.14965986394557823,"w1 ←
1
√nΩA1⊤∈Rn×d;
wL+1 ←
1
√nAL+1⊤φ(BL+1Ω⊤) ∈Rdout×n
(9)"
INTRODUCTION,0.15192743764172337,"As an example, the initialization of the 1-hidden-layer case in Theorem 3.2 corresponds to d =
dout = r = 1, M = 0 and A1 = 1. See Fig. 3(Left) for an illustration.
Let’s digest this initialization scheme: 1) Of course, Al, Bl need to be initialized themselves, and
in this paper we will just do so with Gaussian random initialization; see Appendix B.1 for the
pseudo-algorithm. 2) Al, Bl, l ∈[2, L], will play the same roles as A, B in the 1-hidden-layer
example above, whereas A1 plays the same role as C there. 3) r is a measure of dimension of the
the projection. More precisely, we will project hidden weight gradients from Rn×n to Rn×r; see
below. 4) Al, Bl will grow in the M dimension with training time, just like how the sizes of A, B
grow in the 1-hidden-layer example. 5) Eq. (9) can be interpreted as saying wl are generated by
1-hidden-layer MLPs FAl,Bl : Rr →(Rr or Rdout) with weights Al, Bl, like so:
wl
αβ ←1/n⟨Ωα, FAl,Bl(Ωβ)⟩, for all hidden l = 2, . . . , L;
wL+1
:β
←1/nFAL+1,BL+1(Ωβ). (10)"
INTRODUCTION,0.15419501133786848,"This is reminiscent of hypernetworks (Ha et al., 2016), but the crucial difference is that the “batch
dimension” (indexed by α, β) of the generator FAl,Bl becomes the width dimension of the generated
network f, so that the same generators can generate networks of arbitrary width."
INTRODUCTION,0.1564625850340136,"π-Projection
We leave the output weights wL+1 alone, but for any w ∈{w1, w2, . . . , wL}, where
w has shape n × n or n × d, we project the gradient ∇wL of the same shape by left multiplying
by ΠΩ, the projection matrix to the r-dimensional space spanned by the columns of Ω. This means
that preactivations hl are always in this space. However, the input side of w is not projected, so
we are optimizing the hidden weights in a (r × n)-dimensional space, which still becomes inﬁnite-
dimensional as n →∞. We refer to SGD with π-projected gradients as π-SGD.
This brings us to the following
Deﬁnition 3.3 (π-Parametrization and π-Limit). We deﬁne π-parametrization as the package of
π-initialization and π-SGD,9 where r and the initial M are clear from context. A network in
π-parametrization is abbreviated π-network or π-net, and we deﬁne π-limit as its ∞-width limit."
INTRODUCTION,0.15873015873015872,"3.2.1
COMPUTING THE π-LIMIT OF DEEP MLP
Here we show the π-limit forward and backward passes can be efﬁciently computed."
INTRODUCTION,0.16099773242630386,"π-Limit Forward Pass
Let’s write f P for the (random) neural network π-initialized by P as in
Eqs. (8) and (9). Then it’s straightforward to see f P has a deterministic limit ˚
f P expressible as a
composition of matrix multiplications and V-transforms:
Theorem 3.4 (π-Limit Forward Pass). Suppose φ has a polynomially bounded 2nd derivative. Then"
INTRODUCTION,0.16326530612244897,"as n →∞,
f P(ξ) →˚
f P(ξ),
for any ξ ∈Rd,"
INTRODUCTION,0.1655328798185941,"where convergence is almost sure, and ˚
f P(ξ) is deﬁned as follows. Write g1 def= A1⊤ξ ∈Rr,"
INTRODUCTION,0.16780045351473924,"gl def= Al⊤Vφ(Blgl−1, Bl ◦Bl, ∥gl−1∥21) ∈
Rr
for l = 2, . . . , L
Rdout
for l = L + 1,"
INTRODUCTION,0.17006802721088435,"and ˚
f P(ξ) def= gL+1, where B ◦B yields a size-M column vector of squared norms of B’s rows."
INTRODUCTION,0.17233560090702948,9with learning rate independent of width (but may vary with training time)
INTRODUCTION,0.1746031746031746,Published as a conference paper at ICLR 2022
INTRODUCTION,0.17687074829931973,"Figure 3: Summary of the π-limit for deep MLPs Eq. (7), in the same style as Fig. 2. Here we
take the example of 3-hidden-layer MLPs. See Theorems 3.4 and 3.5 and compare to Eq. (⋆)."
INTRODUCTION,0.17913832199546487,"See Fig. 3(Right) for an illustration. Here, gl represents the coefﬁcients of preactivation hl (c.f.
Eq. (7)) in the columns of Ω. As we will see next, π-projection ensures that at any point during
training, ft has an ∞-width limit of the form ˚
f P for some P."
INTRODUCTION,0.18140589569160998,"π-Limit Backward Pass
Just like the 1-hidden-layer case, we can interpret the projected gradient
as the maximal ascent direction in the column space of Ω. Formalizing the intuition using Tensor
Programs yields the following theorem that we also empirically verify in Appendix E.
Theorem 3.5 (π-Limit Backward Pass). Let P0 denote the matrices (c.f. Eq. (8)) used in the π-
initialization of f as in Eq. (9). Suppose its nonlinearity φ has a polynomially bounded 2nd derivative.
Then for any training routine, f trained by π-SGD for T steps has an ∞-width limit which is equal
to ˚
f PT for some PT given below, i.e."
INTRODUCTION,0.1836734693877551,"as n →∞,
fT (ξ) →˚
f PT (ξ),
for any ξ ∈Rd,"
INTRODUCTION,0.18594104308390022,"where the limit is almost sure. PT is given inductively through its elements Al
T , Bl
T as follows:
A1
t+1
def= A1
t −η∇A1L(˚
f Pt(ξt), yt), and, for l = 2, . . . , L + 1,"
INTRODUCTION,0.18820861678004536,"Al
t+1
def= append(Al
t, −η∇gl
tL(˚
f Pt(ξt), yt)),
Bl
t+1
def= append(Bl
t, gl−1
t
)."
INTRODUCTION,0.19047619047619047,"Here gl
t corresponds to gl in Theorem 3.4 evaluated for input ξt and function ˚
f Pt, and append(B, g)
means appending g as a new row vector of B, increasing by 1 the dimension represented by M."
INTRODUCTION,0.1927437641723356,"Theorems 3.4 and 3.5 are summarized by Fig. 3.10 As conveyed by Eq. (⋆), the π-limit can be
thought of as another MLP with activation Vφ and trained by gradient concatenation.11 Because
π-parametrization has similar scaling with width as µ-parametrization, it is easy to show that the
former admits feature learning in the same way the latter does, i.e. the feature kernel of every layer
evolves nontrivially during training. Using Tensor Programs, it is straightforward to extend the
above theorems to more general settings such as biases, large batch size, per layer learning rate, or
metalearning (Appendix A). Appendix D also makes several observations on the π-limit theorems."
INTRODUCTION,0.19501133786848074,"π-Limit vs µ-Limit
While the projection means optimization is slower in the π-limit, we believe
with sufﬁciently large r, the π-limit should perform similarly to the µ-limit. Indeed, prior works
such as Li et al. (2018) has shown that optimizing a neural network in a much smaller, random
parameter subspace can recover most of the performance of training all parameters. This is also
evidenced by our experimental results below, where the π-limit generally only slightly outperform
wide µ-networks."
INTRODUCTION,0.19727891156462585,"10One may wonder how would Theorem 3.5 compare with just directly accumulating gradients on P. In fact,
this would train very badly because Vφ is very smooth around 0, so ˚
f P would look very linear for a long time.
Empirically, direct SGD on P would yield ≤53% test and ≤80% training accuracy on CIFAR10.
11Note that once a vector is appended to Al or Bl, it is not touched again. Therefore, despite the similarity
between the π-limit and an MLP, Al and Bl should not be thought of “parameters” in the usual sense."
INTRODUCTION,0.19954648526077098,Published as a conference paper at ICLR 2022
INTRODUCTION,0.2018140589569161,"Table 1: Best Test Accuracies on CIFAR10 and Omniglot, best of MLPs up to 4 hidden layers,
width 2048, r 400, as well as random search over a host of hyperparameters; see Appendix B. Note
the µ-Net numbers are also the optimal numbers for standard parametrization ﬁnite networks, as
discussed in Footnote 12. π-Limit ImageNet Transfer means pretraining a π-limit with r = 200 on
ImageNet32 and perform kernel regression with its feature kernel (i.e. kernel of last layer activations)
on CIFAR10; see Section 4.2 and Appendix B.4 for details and ﬁnite network results. Also compare
with feature kernel regression without pretraining (Table 8)."
INTRODUCTION,0.20408163265306123,"NNGP
NTK
NTK
perf gap
µ-Net
π-Net
π-Limit
π-Limit
ImageNet Transfer
CIFAR10
58.92
59.63
←→
61.31
60.64
61.50
64.39
Omniglot
43.80
51.72
←→
91.22
92.21
91.46
-"
EXPERIMENTS,0.20634920634920634,"4
EXPERIMENTS"
EXPERIMENTS,0.20861678004535147,"Here we compare the performance of the relu π-limit on CIFAR10 (Krizhevsky, 2009) and Omniglot
(Lake et al., 2015) against that of NTK, NNGP, and ﬁnite-width π- and µ-nets.12 As we will see,
1) π-limits of large enough r beat ﬁnite µ-nets; 2) ﬁnite π-nets underperform the above on CIFAR10
but, interestingly, outperform them on Omniglot; 3) all of the above beat NTK and NNGP."
EXPERIMENTS,0.2108843537414966,"4.1
CLASSIFICATION ON CIFAR10"
EXPERIMENTS,0.21315192743764172,"Experimental Setup
For π- and µ-networks, inﬁnite or ﬁnite, we train for 50 epochs. We adopt
a step learning rate schedule, with a learning rate drop of 0.15 at a certain milestone, which is a
hyperparameter. We sweep over a variety of hyperparameters such as the learning rate, gradient
clipping, weight decay, the LR drop milestone, etc, as well as width, r, and depth. For NTK and
NNGP, we perform kernel regression following Lee et al. (2018) using centered labels. For them, we
sweep over the initialization variances and learning rate multipliers, along with ridge coefﬁcient. See
Appendix B for more details."
EXPERIMENTS,0.21541950113378686,"Results
In the literature, the performance gap between CNN and its NTK (Arora et al., 2019) is
often cited for the deﬁciency of the NTK theory for explaining neural networks in practice. However,
in fact, on MLPs we already see a nontrivial gap, as seen in Table 1 (compare µ-Net with NTK,
NNGP).13 The π-limit closes this gap, having almost a 2-point advantage over NTK. The ﬁnite-width
π-net outperforms NTK and underperforms π-limit both by about 1 point."
EXPERIMENTS,0.21768707482993196,"Feature Learning in π-Limit and Finite Networks
We show the advantage of π-limit over NTK
and NNGP is due to feature learning. To do so, we track the kernel regression performance of its
(last-layer) feature kernel over the course of training. As seen in Fig. 4(Left), while the feature kernel
at initialization underperforms both NNGP and NTK, it improves consistently over time to eventually
exceed them.14 Similarly, the kernel performance of the best π- and µ-nets improves over time as
well, though the accuracy of the feature kernel regression is slightly less than the network itself, likely
due to the low rank property of the feature kernel. On the contrary, the π-limit beneﬁts from feature
kernel regression, improving the test accuracy from 61.5% to 61.85%; see Table 8."
EXPERIMENTS,0.2199546485260771,"Effects of Width, r, and Depth
As shown in Fig. 4(Middle), accuracy of π-net increases monoton-
ically with width and with r, approaching the accuracy of π-limit and µ-net, respectively, from below.
This can also be seen in feature kernel regression across training time, Fig. 6. In contrast, performance
is not monotonic in depth for any of µ-net, π-net, or π-limit, beyond a consistent improvement from
1 to 2 hidden layers; see Fig. 7."
EXPERIMENTS,0.2222222222222222,"12Because 1) standard parametrization differs from µ-parametrization only in factors depending on width,
2) we sweep such factors in hyperparameter optimization, and 3) our ﬁnite networks have width at most 2048
(so these factors are in practice constants), our best accuracies for µ-nets will also be best accuracies for
standard-parametrized MLPs. We also train π-nets with untied Ωs; see Appendix A.7.
13Note that, contrary to Lee et al. (2018; 2020), which claimed that ﬁnite-width neural networks underperform
the kernels, here we ﬁnd the former outperform the latter. This is primarily due to the single learning rate
drop we adopted, while Lee et al. (2018; 2020) used a constant learning rate. We believe this provides a fairer
comparison to the kernels since 1) the step LR schedule is more common than constant learning rate in practice,
and 2) the kernels would obtain the same performance if we did kernel gradient descent for inﬁnite-time (which
is equivalent to kernel regression) with learning rate drop, since this optimization is convex.
14In both NNGP and NTK limits, the feature kernel stays ﬁxed throughout training (Yang and Hu, 2020), so
the dotted line in Fig. 4(Left) shows the result if we do the same experiments for NNGP and NTK."
EXPERIMENTS,0.22448979591836735,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.22675736961451248,"0
10
20
30
40
50
Train Epochs 50 55 60"
EXPERIMENTS,0.2290249433106576,Test Kernel Accuracy
EXPERIMENTS,0.23129251700680273,CIFAR10 Feature Kernel Regression
EXPERIMENTS,0.23356009070294784,-limit
EXPERIMENTS,0.23582766439909297,"-net
-net"
EXPERIMENTS,0.23809523809523808,"NTK
NNGP"
EXPERIMENTS,0.24036281179138322,"128
256
512 1024 2048 Width"
EXPERIMENTS,0.24263038548752835,"50 100 200 400
-net
r"
EXPERIMENTS,0.24489795918367346,CIFAR10 Val Acc 56 57 58 59 60 61 128 256 512 1024 2048 4096 8192 Width 50 100 200 400 800 1600 3200 -net r
EXPERIMENTS,0.2471655328798186,Omniglot Val Acc 91 92 93 94 95
EXPERIMENTS,0.2494331065759637,"Figure 4: (Left) Feature kernels of π-limit, π-net, and µ-net all improve in quality with training, as
measured by kernel regression on CIFAR10. All models are the best from our hyperparameter sweeps,
but note that feature kernel regression causes accuracy decrease in ﬁnite models. (Middle) CIFAR10
validation accuracy is monotonic across width and r for π-net, π-limit, and µ-net. (Right) Omniglot
validation accuracy, in contrast, is not monotonic in width, and µ-net can underperform π-net of large
r. (Note we did not run π-limit with r ≥800 in consideration of computational costs). All numbers
in the heatmaps are the best from our random hyperparameter searches for 2-hidden-layer networks."
EXPERIMENTS,0.25170068027210885,"4.2
TRANSFER LEARNING FROM IMAGENET TO CIFAR10"
EXPERIMENTS,0.25396825396825395,"Pretraining-and-transfer-learning is an important setting where feature learning is crucial. As pointed
out in Yang and Hu (2020), the NTK limit trivializes pretraining, while the µ-limit both theoretical
and empirically beneﬁt from it. Here we investigate pretraining for the π-limit in the image domain
by pretraining on ImageNet32 (Chrabaszcz et al., 2017)15 and transferring to CIFAR10."
EXPERIMENTS,0.2562358276643991,"Experimental Setup
We pretrain the π-limit with r = 200 (as well as µ-Net, π-Net with r = 200,
and π-Net with r = 400) for 30 epochs on a ﬁxed subset of ImageNet32 with 250 (out of 1000)
randomly subsampled classes. To evaluate on CIFAR10, we compute the kernel induced by the
pretrained ﬁnal-layer-features and perform kernel regression."
EXPERIMENTS,0.2585034013605442,"Results
As shown in Table 1, ImageNet32 pretraining nontrivially raises the downstream CIFAR10
performance over without pretraining, altogether creating a 5% gap compared to NTK (which has the
same test accuracy whether or not it is pretrained on ImageNet32 because of the disparate classes, as
shown in Yang and Hu (2020)). This again demonstrates the feature learning capability of the π-limit.
Table 7 shows the beneﬁt of pretraining seems to be directly related to the capacity of the model."
FEW-SHOT LEARNING VIA METALEARNING ON OMNIGLOT,0.26077097505668934,"4.3
FEW-SHOT LEARNING VIA METALEARNING ON OMNIGLOT"
FEW-SHOT LEARNING VIA METALEARNING ON OMNIGLOT,0.26303854875283444,"Following Yang and Hu (2020), we also evaluate few-shot learning on Omniglot. Compared to
traditional classiﬁcation settings like CIFAR10, doing well on Omniglot requires learning features
that can rapidly be adapted to new unseen data (Raghu et al., 2019). We will adopt a metalearning
approach to this, following Finn et al. (2017).
Unlike Yang and Hu (2020), we will train our models using ANIL (Almost No Inner Loop) (Raghu
et al., 2019) rather than ﬁrst-order MAML (Model Agnostic Meta-Learning). Brieﬂy, ANIL is a
variant of second-order MAML, where in the inner loop, we only adapt the output layer, while in
the outer loop, we only train the network body. We adopt ANIL because: 1) We would like to train
deep MLPs with SGD and without Adam or batchnorm, but empirically this makes optimization of
ﬁrst-order MAML difﬁcult for both ﬁnite and ∞-width networks compared to second-order MAML.
2) While a π-parametrized network trained by second-order MAML has an ∞-width limit calculable
using Tensor Programs, it does not stay in the f P form of Theorem 3.4 because of the second-order
gradient, rendering the limit computation inefﬁcient. 3) Fortunately, π-networks trained by ANIL
do not have this issue because the inner and outer loop gradients are on different parameters. In
addition, ANIL performs on par with MAML on standard ﬁnite-width networks (Raghu et al., 2019).
4) Furthermore, ANIL training more clearly delineates the role of the network body for learning
reusable features and the role of the head for adapting to new data, which is more ﬁtting for our goals
in this paper."
FEW-SHOT LEARNING VIA METALEARNING ON OMNIGLOT,0.2653061224489796,"Experimental Setup
We focus on the 5-way, 1-shot task16, with only 1 step of ANIL adaption. For
π- and µ-networks, inﬁnite or ﬁnite, we train for 50 epochs, 1000 batches per epoch, and 8 tasks per
batch. Following Antoniou et al. (2019), we use cosine annealing schedule on the meta-learning rate."
FEW-SHOT LEARNING VIA METALEARNING ON OMNIGLOT,0.2675736961451247,"15i.e. ImageNet (Deng et al., 2009; Russakovsky et al., 2015) downsampled to 32 × 32 resolution
16i.e. each task consists of 5 different classes, with 1 example provided for each class in training."
FEW-SHOT LEARNING VIA METALEARNING ON OMNIGLOT,0.2698412698412698,Published as a conference paper at ICLR 2022
FEW-SHOT LEARNING VIA METALEARNING ON OMNIGLOT,0.272108843537415,"For NTK and NNGP, ANIL meta-training has no effect, and meta-testing amounts to just taking 1
kernel gradient descent step on each task.17 We sweep over a variety of hyperparameters such as the
outer and inner learning rates, gradient clipping, etc, as well as width, r, and depth. See Appendix B
for more details."
FEW-SHOT LEARNING VIA METALEARNING ON OMNIGLOT,0.2743764172335601,"Results
As seen in Table 1, µ-net, π-net, and π-limit all outperform NNGP and NTK by about 40
points. Interestingly, while π-limit is slightly better than µ-net, they are both outperformed by π-net.
This is related to the width nonmonotonicity seen on Omniglot, as we describe next."
FEW-SHOT LEARNING VIA METALEARNING ON OMNIGLOT,0.2766439909297052,"Effect of Width, r, and Depth
While π-net performance with r is still roughly monotonic, in
contrast to in CIFAR10, here it can decrease with width, as seen in Fig. 4(Right). In addition, µ-net
seems to underperform π-net of the same width for sufﬁciently large r. We ﬁnd this is primarily due to
optimization and not generalization because Omniglot is hard to overﬁt; see Fig. 8. Counterintuitively,
π-net of large r can optimize faster than µ-net despite the projection. This is likely a side effect of
π-initialization, as we see this advantage persist when r > width, where projection is a no-op. We
do not see width nonmonotonicity on CIFAR10 because overﬁtting is much easier there, so the results
depend much more on the implicit bias of each model.
On the other hand, performance seems to monotonically increase along diagonals of ﬁxed r/width
ratio, peaking around r/width ≈1/2. This suggests a different limit of r ∝width →∞, which
warrants investigation in future works.
Unlike CIFAR10, test accuracy is more monotonic in depth here (Fig. 7). Again, this is probably
because the extra weights help with optimization, which is the bottleneck on Omniglot."
RELATED WORKS,0.2789115646258503,"5
RELATED WORKS"
RELATED WORKS,0.2811791383219955,"The π-limit bears some superﬁcial similarities to hierarchical kernel processes such as deep GPs
(Damianou and Lawrence, 2013; Salimbeni and Deisenroth, 2017) or deep kernel processes (Aitchison
et al., 2021). The crucial difference here is that 1) during inference, the π-limit is deterministic and
not a hierarchically sampled process, and 2) the π-limit is not trained variationally.
Relatedly, deep GPs or kernel processes can be construed as a kind of randomly initialized inﬁnite-
width neural network with ﬁnite bottleneck layers (Agrawal et al., 2020; Aitchison, 2020). However,
their variational inference procedures do not correspond to the limit of SGD. In contrast, Littwin
et al. (2021) derived such an SGD limit where the network between consecutive bottlenecks is in
NTK parametrization, and the limit takes the form of a kind of intertwined system of kernel gradient
descent. However, the computation of this limit is not scalable to datasets like CIFAR10.
During training, the π-limit’s Al and Bl grow in size, reminiscent of works on growing neural
networks (Liu et al., 2019; Wu et al., 2021; Gu et al., 2021; Gong et al., 2019). Of course, these works
focus on improving training efﬁciency of practical models, which is not our goal here. Nevertheless,
it will be interesting to explore whether insights from π-limit can contribute back to this literature.
Our π-initialization (given ﬁxed P) can be construed as a special form of the initialization in deep
mean ﬁeld limits of MLPs (Araújo et al., 2019; Sirignano and Spiliopoulos, 2020; Fang et al., 2020;
Nguyen, 2019; Nguyen and Pham, 2020). However, without π-projection, these limits’ analytical
computation suffers exponential runtime blowup like the µ-limit."
CONCLUSION,0.2834467120181406,"6
CONCLUSION"
CONCLUSION,0.2857142857142857,"A good model for studying wide neural networks should 1) capture the desired behavior of neural
networks in practice, including feature learning and performance on real datasets, and 2) come
with a set of theoretical tools for working researchers. But an attractive model must also balance
these properties with 3) computational simplicity for empirical investigations, and 4) mathematical
simplicity for theoretical investigations.
Previously, NTK and NNGP satisfy 2), 3), 4) but not 1), as shown in this and prior works. The µ-limit
satisﬁes 1) and 2) but, arguably, not 3) and 4). In this work, we presented the π-limit which, we
believe, satisﬁes all 4 properties and that can prove fruitful for future researches.
While our work closed the ∞-width performance gap, it also opens many new questions, e.g., Can
we derive the π-limit for a ﬁxed r/width ratio? When precisely does the width nonmonotonicity of
π-net occur? What about modern architectures, beyond MLP? We leave these questions to future
work."
CONCLUSION,0.28798185941043086,"17NTK and NNGP can be trained under second-order MAML, but we found their performance strictly
decreases with metatraining, as the randomization of labels across tasks confuse the readout layer."
CONCLUSION,0.29024943310657597,Published as a conference paper at ICLR 2022
REFERENCES,0.2925170068027211,REFERENCES
REFERENCES,0.2947845804988662,"Devanshu Agrawal, Theodore Papamarkou, and Jacob Hinkle. Wide Neural Networks with Bot-
tlenecks are Deep Gaussian Processes.
arXiv:2001.00921 [cs, stat], January 2020.
URL
http://arxiv.org/abs/2001.00921."
REFERENCES,0.29705215419501135,"Laurence Aitchison. Why bigger is not always better: on ﬁnite and inﬁnite neural networks. In Pro-
ceedings of the 37th International Conference on Machine Learning, pages 156–164. PMLR,
November 2020.
URL https://proceedings.mlr.press/v119/aitchison20a.
html."
REFERENCES,0.29931972789115646,"Laurence Aitchison, Adam Yang, and Sebastian W. Ober. Deep kernel processes. In Proceedings of
the 38th International Conference on Machine Learning, pages 130–140. PMLR, July 2021. URL
https://proceedings.mlr.press/v139/aitchison21a.html."
REFERENCES,0.30158730158730157,"Antreas Antoniou, Harrison Edwards, and Amos Storkey.
How to train your MAML.
arXiv:1810.09502 [cs, stat], March 2019. URL http://arxiv.org/abs/1810.09502."
REFERENCES,0.30385487528344673,"Dyego Araújo, Roberto I. Oliveira, and Daniel Yukimura. A mean-ﬁeld limit for certain deep neural
networks. arXiv:1906.00193 [cond-mat, stat], June 2019. URL http://arxiv.org/abs/
1906.00193."
REFERENCES,0.30612244897959184,"Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an inﬁnitely wide neural net, 2019."
REFERENCES,0.30839002267573695,"Lenaic Chizat and Francis Bach.
On the Global Convergence of Gradient Descent for Over-
parameterized Models using Optimal Transport. arXiv:1805.09545 [cs, math, stat], May 2018.
URL http://arxiv.org/abs/1805.09545."
REFERENCES,0.31065759637188206,"Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Advances in neural
information processing systems, pages 342–350, 2009. URL http://papers.nips.cc/
paper/3628-kernel-methods-for-deep-learning."
REFERENCES,0.3129251700680272,"Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an
alternative to the cifar datasets, 2017."
REFERENCES,0.31519274376417233,"Andreas C. Damianou and Neil D. Lawrence. Deep gaussian processes, 2013."
REFERENCES,0.31746031746031744,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pages 248–255, 2009. doi:10.1109/CVPR.2009.5206848."
REFERENCES,0.3197278911564626,"Cong Fang, Jason D. Lee, Pengkun Yang, and Tong Zhang. Modeling from Features: a Mean-ﬁeld
Framework for Over-parameterized Deep Neural Networks. arXiv:2007.01452 [cs, math, stat],
July 2020. URL http://arxiv.org/abs/2007.01452."
REFERENCES,0.3219954648526077,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation
of Deep Networks. arXiv:1703.03400 [cs], July 2017. URL http://arxiv.org/abs/1703.
03400."
REFERENCES,0.3242630385487528,"Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efﬁcient Training
of BERT by Progressively Stacking. In Proceedings of the 36th International Conference on
Machine Learning, pages 2337–2346. PMLR, May 2019. URL https://proceedings.mlr.
press/v97/gong19a.html."
REFERENCES,0.32653061224489793,"Xiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, and Jiawei Han.
On the Trans-
former Growth for Progressive BERT Training. In Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 5174–5180, Online, June 2021. Association for Computational Linguis-
tics. doi:10.18653/v1/2021.naacl-main.406. URL https://aclanthology.org/2021.
naacl-main.406."
REFERENCES,0.3287981859410431,"David Ha, Andrew Dai, and Quoc V. Le. HyperNetworks. arXiv:1609.09106 [cs], December 2016.
URL http://arxiv.org/abs/1609.09106."
REFERENCES,0.3310657596371882,Published as a conference paper at ICLR 2022
REFERENCES,0.3333333333333333,"Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural Tangent Kernel: Convergence and
Generalization in Neural Networks. arXiv:1806.07572 [cs, math, stat], June 2018. URL http:
//arxiv.org/abs/1806.07572."
REFERENCES,0.3356009070294785,Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.3378684807256236,"Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332–1338, 2015. ISSN 0036-8075.
doi:10.1126/science.aab3050.
URL https://science.sciencemag.org/content/
350/6266/1332."
REFERENCES,0.3401360544217687,"Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha Sohl-
dickstein. Deep Neural Networks as Gaussian Processes. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=B1EA-M-0Z."
REFERENCES,0.3424036281179138,"Jaehoon Lee, Samuel S. Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman
Novak, and Jascha Sohl-Dickstein.
Finite Versus Inﬁnite Neural Networks: an Empirical
Study. arXiv:2007.15801 [cs, stat], September 2020. URL http://arxiv.org/abs/2007.
15801."
REFERENCES,0.34467120181405897,"Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Dimension
of Objective Landscapes. arXiv:1804.08838 [cs, stat], April 2018. URL http://arxiv.org/
abs/1804.08838."
REFERENCES,0.3469387755102041,"Etai Littwin, Omid Saremi, Shuangfei Zhai, Vimal Thilak, Hanlin Goh, Joshua M. Susskind, and
Greg Yang. Implicit acceleration and feature learning in inﬁnitely wide neural networks with
bottlenecks, 2021."
REFERENCES,0.3492063492063492,"Qiang Liu, Lemeng Wu, and Dilin Wang. Splitting steepest descent for growing neural architectures,
2019."
REFERENCES,0.35147392290249435,"Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean ﬁeld view of the landscape of
two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–
E7671, August 2018. ISSN 0027-8424, 1091-6490. doi:10.1073/pnas.1806579115. URL https:
//www.pnas.org/content/115/33/E7665."
REFERENCES,0.35374149659863946,"Phan-Minh Nguyen. Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks.
arXiv:1902.02880 [cond-mat, stat], February 2019. URL http://arxiv.org/abs/1902.
02880."
REFERENCES,0.35600907029478457,"Phan-Minh Nguyen and Huy Tuan Pham. A Rigorous Framework for the Mean Field Limit of
Multilayer Neural Networks. arXiv:2001.11443 [cond-mat, stat], January 2020. URL http:
//arxiv.org/abs/2001.11443."
REFERENCES,0.35827664399092973,"Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid Learning or Feature Reuse?
Towards Understanding the Effectiveness of MAML. arXiv:1909.09157 [cs, stat], September
2019. URL http://arxiv.org/abs/1909.09157."
REFERENCES,0.36054421768707484,"Grant M. Rotskoff and Eric Vanden-Eijnden. Neural Networks as Interacting Particle Systems:
Asymptotic Convexity of the Loss Landscape and Universal Scaling of the Approximation Er-
ror. arXiv:1805.00915 [cond-mat, stat], May 2018. URL http://arxiv.org/abs/1805.
00915."
REFERENCES,0.36281179138321995,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet
large scale visual recognition challenge, 2015."
REFERENCES,0.36507936507936506,"Hugh Salimbeni and Marc Deisenroth. Doubly Stochastic Variational Inference for Deep Gaussian
Processes. arXiv:1705.08933 [stat], November 2017. URL http://arxiv.org/abs/1705.
08933."
REFERENCES,0.3673469387755102,"Justin Sirignano and Konstantinos Spiliopoulos.
Mean Field Analysis of Neural Networks.
arXiv:1805.01053 [math], May 2018. URL http://arxiv.org/abs/1805.01053."
REFERENCES,0.36961451247165533,Published as a conference paper at ICLR 2022
REFERENCES,0.37188208616780044,"Justin Sirignano and Konstantinos Spiliopoulos. Mean Field Analysis of Deep Neural Networks.
arXiv:1903.04440 [math, stat], February 2020.
URL http://arxiv.org/abs/1903.
04440."
REFERENCES,0.3741496598639456,"Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Fireﬂy neural architecture descent: a general
approach for growing neural networks, 2021."
REFERENCES,0.3764172335600907,"Greg Yang. Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture
are Gaussian Processes. arXiv:1910.12478 [cond-mat, physics:math-ph], December 2019a. URL
http://arxiv.org/abs/1910.12478."
REFERENCES,0.3786848072562358,"Greg Yang. Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process
Behavior, Gradient Independence, and Neural Tangent Kernel Derivation. arXiv:1902.04760 [cond-
mat, physics:math-ph, stat], February 2019b. URL http://arxiv.org/abs/1902.04760."
REFERENCES,0.38095238095238093,"Greg Yang. Tensor Programs II: Neural Tangent Kernel for Any Architecture. arXiv:2006.14548
[cond-mat, stat], August 2020a. URL http://arxiv.org/abs/2006.14548."
REFERENCES,0.3832199546485261,"Greg Yang. Tensor Programs III: Neural Matrix Laws. arXiv:2009.10685 [cs, math], May 2020b.
URL http://arxiv.org/abs/2009.10685."
REFERENCES,0.3854875283446712,"Greg Yang and Edward J. Hu. Feature Learning in Inﬁnite-Width Neural Networks. arXiv:2011.14522
[cond-mat], November 2020. URL http://arxiv.org/abs/2011.14522."
REFERENCES,0.3877551020408163,"Greg Yang and Etai Littwin. Tensor Programs IIb: Architectural Universality of Neural Tangent
Kernel Training Dynamics. arXiv:2105.03703 [cs, math], May 2021. URL http://arxiv.
org/abs/2105.03703."
REFERENCES,0.3900226757369615,"Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder,
Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor Programs V: Tuning Large Neural
Networks via Zero-Shot Hyperparameter Transfer. arXiv:2203.03466 [cond-mat], March 2022.
URL http://arxiv.org/abs/2203.03466."
REFERENCES,0.3922902494331066,Published as a conference paper at ICLR 2022
REFERENCES,0.3945578231292517,"A
EXTENSIONS OF π-LIMIT"
REFERENCES,0.3968253968253968,"A.1
BIASES"
REFERENCES,0.39909297052154197,"Biases are straightforward to add.
Consider an L-hidden-layer µ-net f : Rd →Rdout with biases: such a neural network on input
ξ ∈Rd is given by h1(ξ) = √nw1ξ + √nb1 ∈Rn, and"
REFERENCES,0.4013605442176871,"xl(ξ) = φ(hl(ξ)) ∈Rn,
hl+1(ξ) = wl+1xl(ξ) + √nbl ∈Rn,
for l = 1, . . . , L −1,
(11)"
REFERENCES,0.4036281179138322,"and the network output is f(ξ) = n−1/2wL+1xL(ξ) + bL+1 for wL+1 ∈Rdout×n.
In µ-
parametrization, the biases are initialized as bl
α ∼N(0, σ2
b/n) for l = 1, . . . , L, and bL+1
i
∼
N(0, σ2
b), for a hyperparameter σb. However, in π-parametrization, we will do this a bit differently."
REFERENCES,0.40589569160997735,"π-Initialization
Suppose we are given a collection P of matrices and vectors"
REFERENCES,0.40816326530612246,"P def= (RHS of Eq. (8)) ∪{βl ∈Rr}L
l=1 ∪{βL+1 ∈Rdout}.
(12)"
REFERENCES,0.41043083900226757,"Then we can initialize weights and biases by ﬁrst sampling a standard random Gaussian matrix
Ω∈Rn×r, Ωαi ∼N(0, 1), and 1) set wl as in Eq. (9), and 2) set biases as"
REFERENCES,0.4126984126984127,"bL+1 ←βL+1
and
bl ←
1
√nΩβl
for all l = 1, . . . , L.
(13)"
REFERENCES,0.41496598639455784,This constitutes the π-initialization of biases.
REFERENCES,0.41723356009070295,"π-Projection
For π-projection, we also project the bias gradients by ΠΩ."
REFERENCES,0.41950113378684806,π-Limit Calculation
REFERENCES,0.4217687074829932,"Theorem A.1 (π-Limit Forward Pass with Bias). Let P be some collection as in Eq. (12). As n →∞,"
REFERENCES,0.42403628117913833,"f P(ξ) →˚
f P(ξ),
for any ξ ∈Rd,"
REFERENCES,0.42630385487528344,"where convergence is almost sure, and ˚
f P(ξ) is deﬁned as follows. Write g1 def= A1⊤ξ + β1 ∈Rr,"
REFERENCES,0.42857142857142855,"gl def= Al⊤Vφ(Blgl−1, Bl ◦Bl, ∥gl−1∥21) + βl ∈
Rr
for l = 2, . . . , L
Rdout
for l = L + 1,"
REFERENCES,0.4308390022675737,"and ˚
f P(ξ) def= gL+1, where B ◦B yields a size-M column vector of squared norms of B’s rows."
REFERENCES,0.4331065759637188,"Theorem A.2 (π-Limit Backward Pass). For the same setting as in Theorem 3.5 but with P as in
Eq. (12), Al, Bl are updated exactly as in Theorem 3.5, and βl is updated by gradient accumulation
like A1:"
REFERENCES,0.43537414965986393,"βl
t+1
def= βl
t −η∇βlL(˚
f Pt(ξt), yt),
for all l = 1, . . . , L + 1"
REFERENCES,0.4376417233560091,"A.2
PARAMETER MULTIPLIERS"
REFERENCES,0.4399092970521542,We can insert to Eq. (11) constant parameter multipliers αw for each parameter w like so
REFERENCES,0.4421768707482993,"hl+1(ξ) = αwl+1wl+1xl(ξ) + √nαblbl ∈Rn,
for l = 1, . . . , L −1,
(14)"
REFERENCES,0.4444444444444444,"These multipliers are tuneable hyperparameters. They affect both the forward and backward passes
of the network.
In the π-limit, the forward pass is the same as in Theorem A.1 except we replace Al with αwlAl and
βl with αblβl. The backward pass is the same as in Theorem A.2, but we just have to make sure that
we backprop through the multipliers (in contrast, if we instead have absorbed the multipliers into the
initialization, then we would not backprop through the multilpliers).
In our experiments, we only consider the input weight multiplier, output weight multiplier, and a
single multiplier for all biases."
REFERENCES,0.4467120181405896,Published as a conference paper at ICLR 2022
REFERENCES,0.4489795918367347,"A.3
LEARNING RATE MULTIPLIERS"
REFERENCES,0.4512471655328798,"We may have custom learning rates for speciﬁc weights or biases. In our experiments, we implement
this with learning rate multipliers relative to the global learning rate η, e.g., the learning rate of a
parameter w becomes γwη if the multiplier is denoted γw. Then in the limit, we just need to replace
η in Theorem 3.5 or Theorem A.2 with γwlη, i.e."
REFERENCES,0.45351473922902497,"A1
t+1
def= A1
t −γw1η∇A1L(˚
f Pt(ξt), yt)"
REFERENCES,0.4557823129251701,"Al
t+1
def= append(Al
t, −γwlη∇gl
tL(˚
f Pt(ξt), yt)),
for all l = 2, . . . , L + 1"
REFERENCES,0.4580498866213152,"βl
t+1
def= βl
t −γblη∇βlL(˚
f Pt(ξt), yt),
for all l = 1, . . . , L + 1."
REFERENCES,0.4603174603174603,"But note we do not modify the Bl update. In our experiments, we will only sometimes use a LR
multiplier on the input layer (“Input Layer LR Mult”), one on the output layer (“Output Layer LR
Mult”), and/or a single multiplier for all biases (“Bias LR Mult”)."
REFERENCES,0.46258503401360546,"A.4
LARGE BATCH SIZE"
REFERENCES,0.46485260770975056,"For batch size S > 1, we still accumulate gradients into A1 and βl as if they are regular parameters,
and for the hidden weights, we just append the S gradient vectors as if they are from S unit-sized
batches."
REFERENCES,0.4671201814058957,"A.5
GRADIENT CLIPPING"
REFERENCES,0.46938775510204084,"The Frobenius norms of the projected gradients of the input weight w1 and all biases bl converge to
exactly the Frobenius norms of the gradients of A1 and βl, i.e."
REFERENCES,0.47165532879818595,"∥ΠΩ∇w1L∥F →∥∇A1L∥F
∥ΠΩ∇blL∥→∥∇βlL∥"
REFERENCES,0.47392290249433106,"where the convergence is almost sure. For any hidden weights wl, suppose its gradient over an
S-sized batch in the π-limit is given by"
REFERENCES,0.47619047619047616,"Al
t+1
def= append(Al
t, −η ˜A),
Bl
t+1
def= append(Bl
t, ˜B)."
REFERENCES,0.47845804988662133,"where ˜A, ˜B ∈RS×r, and append(B, ˜B) means appending all S rows of ˜B into B, increasing the
latter’s column length by S. Then"
REFERENCES,0.48072562358276644,∥ΠΩ∇wlL∥F → r
REFERENCES,0.48299319727891155,"tr

˜A⊤˜φ( ˜B ˜B⊤) ˜A
"
REFERENCES,0.4852607709750567,where the convergence is almost sure. Then clipping the gradient in the π-limit just means clipping
REFERENCES,0.4875283446712018,"the gradients of A1 and βl, and, for hidden weights, rescale ˜A such that
r"
REFERENCES,0.4897959183673469,"tr

˜A⊤˜φ( ˜B ˜B⊤) ˜A

is at"
REFERENCES,0.49206349206349204,most the clipping threshold.
REFERENCES,0.4943310657596372,"A.6
WEIGHT DECAY"
REFERENCES,0.4965986394557823,The limit of decaying wl ←wl(1 −lr · wd) is just Al ←Al(1 −lr · wd).
REFERENCES,0.4988662131519274,"A.7
DECOUPLING LAYERS"
REFERENCES,0.5011337868480725,"We can actually use different rs and Ms for every layer, and a similar limit theorem can be proved.
We can also use different, independently sampled Ωfor each layer (“untied Ωs”). However, the limit
would be exactly the same as before, as is apparent in our proof. In addition, we verify that even in
ﬁnite π-nets, there is no difference in performance between tying Ωacross layers or not (Fig. 5). All
of our experiments are actually done with untied Ωs."
REFERENCES,0.5034013605442177,"B
EXPERIMENTAL DETAILS"
REFERENCES,0.5056689342403629,"All of our experiments are done on V100 GPUs. All of our networks use relu activation. The
V-transform of relu is (Cho and Saul, 2009)"
REFERENCES,0.5079365079365079,"Vrelu(r1r2c, r2
1, r2
2) = 1 2π p"
REFERENCES,0.5102040816326531,"1 −c2 + (π −arccos(c))c

r1r2"
REFERENCES,0.5124716553287982,"for any r1, r2 > 0 and c ∈[−1, 1]."
REFERENCES,0.5147392290249433,Published as a conference paper at ICLR 2022
REFERENCES,0.5170068027210885,"0
10
20
30
40
50
Train Epochs 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8"
REFERENCES,0.5192743764172335,Train Loss
REFERENCES,0.5215419501133787,"Tied Omegas
Untied Omegas"
REFERENCES,0.5238095238095238,"0
10
20
30
40
50
Train Epochs 1.25 1.50 1.75 2.00 2.25 2.50 2.75"
REFERENCES,0.5260770975056689,Test Loss
REFERENCES,0.528344671201814,"Tied Omegas
Untied Omegas"
REFERENCES,0.5306122448979592,"Figure 5: Tying vs Untying ΩAcross Layers Make No Difference in π-Nets. Using the same
procedure as in Section 4.1, we train π-net with r = 400 using the best hyperparameter combination
we found (whose result shown in Table 1) 50 times, each with different independently sampled Ωs,
either with Ωtied across layers (blue curve) or not (orange curve). We plot their mean training loss
and test loss curves here, with shade indicating 95% conﬁdence interval."
REFERENCES,0.5328798185941043,"B.1
NETWORK INITIALIZATION AND PARAMETRIZATION"
REFERENCES,0.5351473922902494,"At initialization, 1) biases are always 0, 2) M is set to r so that Al, Bl are square matrices for
l = 2, . . . , L, and 3) we sample Al, Bl as standard Gaussians and then scale them as follows:"
REFERENCES,0.5374149659863946,"A1 = A1/A1.norm(dim = 0, keepdim = True)"
REFERENCES,0.5396825396825397,"Bl = Bl/Bl.norm(dim = 1, keepdim = True)
for all l = 2, . . . , L"
REFERENCES,0.5419501133786848,"Al = Al/
p"
REFERENCES,0.54421768707483,"A1.shape[0]
for all l = 2, . . . , L, and"
REFERENCES,0.546485260770975,AL+1 = 0
REFERENCES,0.5487528344671202,"B.2
FEATURE KERNEL"
REFERENCES,0.5510204081632653,"When we talk about the “feature kernel” of a π-limit ˚
f P, we always mean the n →∞limit of
the feature kernel of f P, and not the kernel induced by gL in ˚
f P. This feature kernel K on inputs
{ξ1, . . . , ξk} is calculated as"
REFERENCES,0.5532879818594104,"Kij = Vφ(⟨gL
i , gL
j ⟩, ∥gL
i ∥2, ∥gL
j ∥2)"
REFERENCES,0.5555555555555556,"where gL
i and gL
j are the gL in Theorem 3.4 evaluated for two inputs ξi and ξj."
REFERENCES,0.5578231292517006,"B.3
CIFAR10"
REFERENCES,0.5600907029478458,"B.3.1
µ-NET, π-NET, AND π-LIMIT
For π- and µ-networks, inﬁnite or ﬁnite, we train for 50 epochs. We adopt a step learning rate
schedule, with a learning rate drop of 0.15 at a milestone hyperparameter. We also clip gradients
with a hyperparameter threshold, where the clip is triggered individually for each parameter by the
parameter’s norm, rather than total parameter norm."
REFERENCES,0.562358276643991,"Hyperparameter Optimization
We ﬁrst perform random search on hyperparameters listed in
Table 2 for 2-hidden-layer π-net, π-limit, and µ-net: We sample at least 512 hyperparameter com-
binations 1) for each (width, r) ∈{128, 256, 512, 1024, 2048} × {50, 100, 200, 400} for π-net, 2)
for each width ∈{128, 256, 512, 1024, 2048} for µ-net, and 3) for each r ∈{50, 100, 200, 400} for
π-limit. The best accuracies per (width, r) are shown in Fig. 4(Middle).
Then we take the best hyperparameter combinations overall for π-net, π-limit, and µ-net, and for
depth ∈{1, 2, 3, 4} (where depth denotes number of hidden layers), we resweep only learning
rate and weight decay in a grid search, where the grid is (lr∗· {2−2, 2−1.5, . . . , 21.5}) × (wd∗·
{2−2, 2−1.5, . . . , 21.5}) and lr∗and wd∗are the optimal hyperparameters from the 2-hidden-layer
sweep. The best test accuracies per depth are shown in Fig. 7, while the overall best test accuracies
over all depth are shown in Table 1."
REFERENCES,0.564625850340136,Published as a conference paper at ICLR 2022
REFERENCES,0.5668934240362812,"Table 2: CIFAR10 Hyperparameter Grid for µ-Net, π-Net, and π-Limit"
REFERENCES,0.5691609977324263,"Hyperparameter
Grid"
REFERENCES,0.5714285714285714,"Gradient Clip
0.4 · {2−3, 2−2 . . . , 24}
Learning Rate
0.5 · {2−3, 2−2 . . . , 24}
Weight Decay
2 · 10−5 · {2−3, 2−2 . . . , 24}
Bias Mult
0.5 · {2−3, 2−2 . . . , 24}
LR Drop Milestone
{30, 35, 40}
Input Weight LR Mult
0.1 · {2−3, 2−2 . . . , 24}
Output Weight LR Mult
16 · {2−3, 2−2 . . . , 24}
Input Weight Mult
{2−3, 2−2 . . . , 24}
Output Weight Mult
{2−3, 2−2 . . . , 24}
Batch Size
{4, 8, 16, 32}"
REFERENCES,0.5736961451247166,"Table 3: CIFAR10 Hyperparameter Grid for NNGP and NTK
Hyperparameter
GP
NTK"
REFERENCES,0.5759637188208617,"Bias Variance
{2−4, 2−3.5, . . . , 22}
0.5 · {2−4, 2−3.5, . . . , 22}
Bias LR Multiplier
n.a.
{2−4, 2−3.5, . . . , 22}
Input Weight LR Multiplier
n.a.
0.5 · {2−4, 2−3.5, . . . , 22}
Output Weight LR Multiplier
n.a.
{1, 20.25, . . . , 25}
Ridge
{10−8, 10−7 · · · , 10−1}"
REFERENCES,0.5782312925170068,"B.3.2
NNGP AND NTK
For NNGP and NTK, we perform kernel regression following Lee et al. (2018) using centered labels.
For each depth ∈{1, 2, 3, 4}, we sweep over hyperparameters listed in Table 3 (which change
the kernels) using (complete) grid search, with weight initialization variance ﬁxed at 1 in the NTK
parametrization.18 For each of NNGP and NTK, the best test accuracy over all depths is listed in
Table 1."
REFERENCES,0.5804988662131519,"B.4
IMAGENET TRANSFER"
REFERENCES,0.5827664399092971,"We pretrain the π-Limit with r = 200 and 2 hidden layers for 30 epochs on a ﬁxed subset of
ImageNet32 with 250 (out of 1000) randomly subsampled classes. To evaluate on CIFAR10, we
compute the kernel induced by the pretrained ﬁnal-layer-features and perform kernel ridge regression.
We optimize the hyperparameters in Table 4 via random search."
REFERENCES,0.5850340136054422,"B.5
OMNIGLOT"
REFERENCES,0.5873015873015873,"We focus on the 5-way, 1-shot task, with only 1 step of ANIL adaption."
REFERENCES,0.5895691609977324,"B.5.1
µ-NET, π-NET, AND π-LIMIT
For π- and µ-networks, inﬁnite or ﬁnite, we train for 50 epochs, 1000 batches per epoch, and 8 tasks
per batch. In each epoch, we validate on 500 batches from the validation set. Following Antoniou et al.
(2019), we use cosine annealing learning rate schedule. We also clip gradients with a hyperparameter
threshold, where the clip is triggered by the total parameter norm."
REFERENCES,0.5918367346938775,"Hyperparameter Optimization
We ﬁrst perform random search on hyperparameters listed in
Table 5 for 2-hidden-layer π-net, π-limit, and µ-net: We sample at least 512 hyperparameter com-
binations 1) for each (width, r) ∈{128, 256, 512, 1024, 2048} × {50, 100, 200, 400} for π-net, 2)
for each width ∈{128, 256, 512, 1024, 2048} for µ-net, and 3) for each r ∈{50, 100, 200, 400} for
π-limit.
Then we take the best hyperparameter combinations overall (based on validation accuracy) for π-net,
π-limit, and µ-net, and for depth ∈{1, 2, 3, 4} (where depth denotes number of hidden layers), we
resweep only the meta learning rate (i.e. outer learning rate) and step size (i.e. inner learning rate)
in a grid search, where the grid is (ilr∗· {2−2, 2−1.5, . . . , 21.5}) × (olr∗· {2−2, 2−1.5, . . . , 21.5})"
REFERENCES,0.5941043083900227,18This is without loss of generality because relu is homogeneous and we are sweeping the bias variance.
REFERENCES,0.5963718820861678,Published as a conference paper at ICLR 2022
REFERENCES,0.5986394557823129,"Table 4: ImageNet Transfer Hyperparameter Grid for µ-Net, π-Net, and π-Limit"
REFERENCES,0.6009070294784581,"Hyperparameter
π-Limit Transfer"
REFERENCES,0.6031746031746031,"Bias Mult
0.5 · {2−3, 2−2 . . . , 23}
Batch Size
{6, 8, 16}
Learning Rate
0.01 · {2−5, 2−4 . . . , 26}
Input Weight Mult
0.5 · {1.50, 1.5.25 . . . , 1.52.75}
Output Weight Mult
{2−0.5, 20 . . . , 23}
Weight Decay
{2−7, 2−6 . . . , 20}
Gradient Clip
{0.1, 0.2, 0.4, 0.6, 0.8, 0.9, 0}
Ridge
{10−8, 10−7 · · · , 10−1}"
REFERENCES,0.6054421768707483,"Table 5: Omniglot Hyperparameter Grid for µ-Net, π-Net, and π-Limit"
REFERENCES,0.6077097505668935,"Hyperparameter
Grid"
REFERENCES,0.6099773242630385,"Step Size
0.5 · {2−2, 2−1.75, . . . , 22}
Meta Learning Rate
16 · {2−3, 2−2.75 . . . , 23}
Gradient Clip
0.1 · {2−2, 2−2.75 . . . , 22}
Bias Mult
1 · {2−2, 2−3.75 . . . , 22}
Input Weight Mult
2 · {2−2, 2−1.75, . . . , 22}
Input Weight LR Mult
0.2 · {2−2, 2−1.75 . . . , 22}"
REFERENCES,0.6122448979591837,"and ilr∗and olr∗are the optimal inner and outer learning rates from the 2-hidden-layer sweep. The
best validation accuracies per depth are shown in Fig. 7. Then we take the models with overall best
validation accuracies over all depth and evaluate them on the test set using 10000 batches. These test
results are shown in Table 1.
To investigate the width non-monotonicity more thoroughly, we further perform random search on hy-
perparameters listed in Table 5 for 2-hidden-layer π-net and µ-net. We sample at least 512 hyperparam-
eter combinations 1) for each (width, r) ∈{128, 256, 512, . . . , 8192} × {50, 100, 200, . . . , 32000}
for π-net, and 2) for each width ∈{128, 256, 512, . . . , 8192} for µ-net. The best validation accura-
cies per (width, r) are shown in Fig. 4(Right)."
REFERENCES,0.6145124716553289,"B.5.2
NNGP AND NTK
As discussed in Section 4.3, for NTK and NNGP, ANIL meta-training has no effect, and meta-testing
amounts to just taking 1 kernel gradient descent step on each task. We ﬁx the step size (i.e. inner
learning rate) at 0.5. For each depth ∈{1, 2, 3, 4}, we sweep over hyperparameters listed in Table 6
using (complete) grid search, with weight initialization variance ﬁxed at 1 in the NTK parametrization.
For each of NNGP and NTK, the best test accuracy over all depths is listed in Table 1."
REFERENCES,0.6167800453514739,"B.5.3
VISUALIZATION OF IMAGE REPRESENTATIONS
We sample 5 random classes and 10 random images from each class from the Omniglot test set, for a
total of 50 images. We take the best performing NTK, µ-net, and π-limit and evaluate their feature
kernels (c.f. Appendix B.2) on the 50 images. We then do PCA on these kernels to produce Fig. 1. In
Fig. 9, we also do the same for our best performing π-net."
REFERENCES,0.6190476190476191,"C
DETAILED CALCULATIONS OF 1-STEP SGD"
REFERENCES,0.6213151927437641,"Suppose we present an input ξ0 to the network and perform a step of gradient descent with learning
rate η and loss L. Then simple calculations show that the updates ∆u, ∆v to u, v are
√n∆v = cφ(ξ0
√nu),
∆u = cv ⊙φ′(ξ0
√nu)
(15)"
REFERENCES,0.6235827664399093,"where c = −ηL′. Then, via Tensor Programs, f(ξ) for any ξ now has a limit of the form"
REFERENCES,0.6258503401360545,"lim
n→∞f(ξ) = E(Z
√nv + Z
√n∆v)φ(Z
√nuξ + Z
√n∆uξ)"
REFERENCES,0.6281179138321995,"= E(Z
√nv +˚cφ(Z
√nuξ0))φ(Z
√nuξ +˚cZ
√nvφ′(Z
√nuξ0)),"
REFERENCES,0.6303854875283447,where˚c is the deterministic limit of c that is shown to exist by Tensor Programs.
REFERENCES,0.6326530612244898,Published as a conference paper at ICLR 2022
REFERENCES,0.6349206349206349,"Table 6: Omniglot Hyperparameter Grid for NNGP and NTK
Hyperparameter
NNGP
NTK"
REFERENCES,0.63718820861678,"Bias Variance
0.1 · {22, 22.5, . . . , 210}
{2−1, 2−0.5, . . . , 25}
Bias LR Mult
n.a.
{2−4, 2−3.5, . . . , 22}
Input Layer LR Mult
n.a.
0.1 · {2−4, 2−3.5, . . . , 24}
Output Layer LR Mult
n.a.
0.1 · {2−4, 2−3.5, . . . , 24}"
REFERENCES,0.6394557823129252,"Table 7: Pretraining on ImageNet32 and Evaluating on CIFAR10, Full Results. We pretrained
µ-net, π-net with r = 200, π-net with r = 400, and π-limit with r = 400 on ImageNet32 and
evaluated the result model on CIFAR10. Here, the π-limit number 64.39 is the same as in Table 1
under π-Limit ImageNet Transfer. For reference, we also include the NNGP and NTK numbers in
the left block. The * indicates we are comparing the π-limit transfer performance with r = 200 vs
π-limit CIFAR10 number with r = 400, so the +2.79 is an underestimate of the improvement due to
pretraining. The beneﬁt of pretraining seems to be directly related to the capacity of the model, as
π-Net with r = 200 < π-Net with r = 400 < µ-Net < π-Limit."
REFERENCES,0.6417233560090703,"NNGP
NTK
µ-Net
π-Net
r=200
π-Net
r=400
π-Limit
r=200
Transfer
58.92
59.63
61.84
58.02
59.36
64.39
vs Table 1
+0
+0
+0.53
-
-1.28
+2.79*"
REFERENCES,0.6439909297052154,"D
REMARKS ON THE π-LIMIT THEOREMS"
REFERENCES,0.6462585034013606,"1. π-projection actually ensures that, even for ﬁnite n, ft = f P for some P. However, this P
is random, with ﬂuctuation coming from the sampling of Ω(which is ﬁxed at initialization).
Taking n →∞reduces this ﬂuctuation to 0."
REFERENCES,0.6485260770975056,"2. Again, even with the projection, we are optimizing in an inﬁnite-dimensional space, though
“linearly inﬁnite” r × ∞instead of “quadratically inﬁnite” ∞× ∞like in the µ-limit."
REFERENCES,0.6507936507936508,"3. The forward pass of ˚
f P can be thought of as that of another MLP with nonlinearity Vφ, as
illustrated in Fig. 3. In this view, M becomes the width of this MLP."
REFERENCES,0.6530612244897959,"E
NUMERICAL VERIFICATION OF THE π-LIMIT THEOREMS"
REFERENCES,0.655328798185941,"Here we numerically show that wide π-nets have nearly identical loss curves as the π-limit. See
Figs. 10, 11 and 13. In addition, Fig. 12 veriﬁes the convergence of the feature kernel to its inﬁnite-
width limit."
REFERENCES,0.6575963718820862,"F
PROOFS"
REFERENCES,0.6598639455782312,"We will just prove Theorems 3.4 and 3.5, as Theorem 3.2 is a special case of them. At a high level, we
need to do two things: 1) Show that fT converges almost surely to something, and 2) this something
is ˚
f PT . Here we assume the reader is familiar with Tensor Programs (Yang, 2019b;a; 2020a;b; Yang
and Hu, 2020; Yang and Littwin, 2021; Yang et al., 2022), in particular the techniques used in Yang
and Hu (2020)."
REFERENCES,0.6621315192743764,"F.1
ALMOST SURE CONVERGENCE"
REFERENCES,0.6643990929705216,"Showing almost sure convergence is straightforward using the Tensor Programs technique, i.e. express
π-initialization and the entire π-SGD training trajectory inside a Tensor Program (a NETSOR⊤+
program in particular) and apply the Master Theorem (c.f. Yang (2020b) in general and Yang and Hu
(2020, Sec H.3, H.4) in particular). Below we sketch the program construction."
REFERENCES,0.6666666666666666,"Initial Vectors and Matrices
Unlike the program for the µ-limit, our program does not have initial
matrix variables because in π-parametrization we do not initialize any n×n matrices as iid Gaussians.
This means that we do not use MatMul instructions in our program. The initial vector variables in
our program are just the r columns Ω:1, . . . , Ω:r of Ω."
REFERENCES,0.6689342403628118,Published as a conference paper at ICLR 2022
REFERENCES,0.671201814058957,"Table 8: Feature Kernel Regression (FKR) on CIFAR10. We take the best performing µ-net, π-net,
and π-limit, and evaluate their learned feature kernels on CIFAR10 via kernel regression. We list
their test accuracies in the middle block. For reference, we also include the NNGP and NTK numbers
in the left block, as well as the ImageNet transfer results in the right block."
REFERENCES,0.673469387755102,"NNGP
NTK
µ-Net
π-Net
π-Limit
π-Limit ImageNet Transfer"
REFERENCES,0.6757369614512472,"FKR
58.92
59.63
59.12
59.72
61.85
64.39
vs Table 1
+0
+0
-2.19
-0.92
+0.35
-"
REFERENCES,0.6780045351473923,"0
10
20
30
40
50
Train Epochs 0.40 0.45 0.50 0.55 0.60"
REFERENCES,0.6802721088435374,Test Kernel Accuracy
REFERENCES,0.6825396825396826,"500
1000
5000
10000
20000
30000
40000"
REFERENCES,0.6848072562358276,"Figure 6: Feature Kernel Regression of π-Nets vs Training Time, for Varying Widths. We
took our best performing π-limit and trained ﬁnite-width versions of it, for width from 500 to
40000. Throughout training, we measure their feature kernel regression accuracy. Altogether, we see
consistent increase in performance across width at any moment in time. Note that the visible gap
between π-limit and the widest π-net (even at initialization) is to a large extent due to the dependence
of kernel regression accuracy on the smallest eigenvalues of the kernel. See Fig. 12."
REFERENCES,0.6870748299319728,"Network Preactivations
All preactivations of the network will be of the form ΩC for some C ∈Rr
whose entries are scalar variables in the program, so that ΩC can be expressed as a vector variable
using Nonlin+."
REFERENCES,0.6893424036281179,"Weight Matrices
We will sketch the constructions surrounding hidden weight matrices; the input
and output weight matrices are similar and easier.
Like in Eq. (4), each hidden weight matrix in deep π-nets can be written mathematically as a sum of
vector outer products."
REFERENCES,0.691609977324263,"wl = 1 n M+t
X"
REFERENCES,0.6938775510204082,"s=1
(ΩAl
s) ⊗φ(ΩBl
s),
at time t
(16)"
REFERENCES,0.6961451247165533,"where Al
s, Bl
s ∈Rr are the sth rows of Al and Bl. Here φ(ΩBl
s) is the activation going into wl at
time s, and ΩAl
s is the projected gradient ΠΩ∇hlL at time s. Note the sum here is from 1 to M + t,
where M comes from the initialization and t is from t steps of training. All entries of Al and Bl will
be constructed as scalar variables in the program, so ΩAl
s and φ(ΩBl
s) are both vector variables.
In the program, we do not express wl directly, but rather through its matrix-vector product with vectors
such as the incoming activation xl−1, which would be expressed via a combination of Moment and
Nonlin+ instructions like so:"
REFERENCES,0.6984126984126984,"wlxl−1 = M+t
X"
REFERENCES,0.7006802721088435,"s=1
θs(ΩAl
s) ∈Rn
Nonlin+"
REFERENCES,0.7029478458049887,"where
θs = 1"
REFERENCES,0.7052154195011338,"n⟨φ(ΩBl
s), xl−1⟩∈R
Moment (17)"
REFERENCES,0.7074829931972789,"We express (wl)⊤indirectly through its matrix-vector products likewise, just with the roles of
(ΩAl
s), φ(ΩBl
s) reversed."
REFERENCES,0.7097505668934241,"Gradient Projection
In the program, we do not express ΠΩdirectly, but rather indirectly by
expressing the matrix-vector product ΠΩv for vector variables v, such as v = ∇hlL. The projection"
REFERENCES,0.7120181405895691,Published as a conference paper at ICLR 2022
REFERENCES,0.7142857142857143,"1
2
3
4
depth 58 59 60 61"
REFERENCES,0.7165532879818595,test acc
REFERENCES,0.7188208616780045,CIFAR10
REFERENCES,0.7210884353741497,"-limit
-net
-net"
REFERENCES,0.7233560090702947,"1
2
3
4
depth 75 80 85 90 95"
REFERENCES,0.7256235827664399,val acc
REFERENCES,0.7278911564625851,Omniglot 94 96
REFERENCES,0.7301587301587301,"Figure 7: Performance vs Depth on CIFAR10 and Omniglot. We take best performing µ-net,
π-net, and π-limit from our thorough sweep of 2-hidden-layer networks, and resweep the learning
rate and weight decay (for CIFAR10) or outer and inner learning rates (for Omniglot) for {1, 2, 3, 4}
hidden layers. We plot the best test accuracies of each depth here. See Appendix B for more details."
REFERENCES,0.7324263038548753,"128
256
512
1024 2048
Width"
REFERENCES,0.7346938775510204,"50 100 200 400
-net
r"
REFERENCES,0.7369614512471655,CIFAR10 Train Acc 80 85 90 95 100 128 256 512 1024 2048 4096 8192 Width 50 100 200 400 800 1600 3200 -net r
REFERENCES,0.7392290249433107,Omniglot Train Acc 94 96 98
REFERENCES,0.7414965986394558,"Figure 8: Best Training Accuracy vs Width vs r on CIFAR10 and Omniglot, taken over all of
our random hyperparameter searches. While networks with moderately large r and width can overﬁt
CIFAR10 completely, no µ-net, π-limit, or π-net with width up to 8192 and r up to 3200 is able to
do so on Omniglot. See Appendix B.3.1 for experimental details."
REFERENCES,0.7437641723356009,"matrix ΠΩis mathematically equal to Ω(Ω⊤Ω)+Ω⊤(for any width), where ()+ denotes pseudo-
inverse. In the program, we would ﬁrst express (the entries of) 1"
REFERENCES,0.746031746031746,"nΩ⊤Ω∈Rr×r using many Moment
instructions ( 1"
REFERENCES,0.7482993197278912,nΩ⊤Ω)ij = 1
REFERENCES,0.7505668934240363,"n⟨Ω:i, Ω:j⟩∈R
Moment"
REFERENCES,0.7528344671201814,"Then its pseudo-inverse can be expressed as another Moment instruction (with purely scalar argu-
ments). ( 1"
REFERENCES,0.7551020408163265,"nΩ⊤Ω)+
ij = 1 n n
X"
REFERENCES,0.7573696145124716,"α=1
fij(; {( 1"
REFERENCES,0.7596371882086168,"nΩ⊤Ω)ij}ij) ∈R
Moment"
REFERENCES,0.7619047619047619,= fij(; {( 1
REFERENCES,0.764172335600907,"nΩ⊤Ω)ij}ij) ∈R
Moment"
REFERENCES,0.7664399092970522,"where fij takes a matrix to the ijth entry of its pseudoinverse. Note the above expressions depend on Ω
only, and not on v. Finally, we express ΠΩv = Ωγ ∈Rn, γ = ( 1"
REFERENCES,0.7687074829931972,"nΩ⊤Ω)+θ ∈Rr, θ = 1"
REFERENCES,0.7709750566893424,"nΩ⊤v ∈Rr
in the program like so"
REFERENCES,0.7732426303854876,"ΠΩv = Ωγ = r
X"
REFERENCES,0.7755102040816326,"i=1
γiΩ:i ∈Rn
Nonlin+
(18)"
REFERENCES,0.7777777777777778,"where
γi = r
X"
REFERENCES,0.780045351473923,"j=1
( 1"
REFERENCES,0.782312925170068,"nΩ⊤Ω)+
ijθj ∈R
Moment
(19)"
REFERENCES,0.7845804988662132,"where
θj = 1"
REFERENCES,0.7868480725623582,"n⟨Ω:j, v⟩∈R
Moment
(20)"
REFERENCES,0.7891156462585034,"Wrapping Up
Other than what is discussed above, the unrolling of π-SGD follows identical to the
unrolling of SGD in Yang and Hu (2020, Sec H.3, H.4). In particular, ft(ξ) for any input ξ and time t
is a scalar variable in the program."
REFERENCES,0.7913832199546486,Published as a conference paper at ICLR 2022
REFERENCES,0.7936507936507936,"NTK
-Net
-Net
-Limit"
REFERENCES,0.7959183673469388,"Figure 9: PCA of representations of images from 5 classes in Omniglot test set. Same setting as
in Fig. 1, but here including our best performing π-net as well."
REFERENCES,0.7981859410430839,"Table 9: CIFAR10 Compute Time (in Seconds) Comparison. We measure the average training
time (in seconds) per epoch for 50 epochs of CIFAR10 using half precision on a NVIDIA V100
GPU. We evaluate a µ-Net, π-Net, and the π-Limit, each of depth 1, 2, 3, and 4; the π-Nets and the
π-Limits have r = 400. Because the π-Limit has a linearly increasing compute time per epoch, we
also give an estimate expression for the compute time of the π-Limit in terms of t epochs."
REFERENCES,0.800453514739229,"Hidden Layers
µ-Net
π-Net
π-Limit Average
π-Limit Epoch Estimate"
REFERENCES,0.8027210884353742,"1
6.78
6.83
83.01
30.97 + 2.23t
2
7.72
8.29
160.99
40.70 + 4.99t
3
9.03
9.80
200.31
48.413 + 7.75t
4
10.17
11.29
263.11
61.20 + 10.36t"
REFERENCES,0.8049886621315193,"Getting Almost Sure Convergence
We apply the Master Theorem (Yang and Hu (2020, Thm 7.4)
or Yang (2020b, Thm E.15)) to the program to get almost sure convergence to some limit. We just
need to check the conditions of the theorem. They are all straightforward except that we need to
check the pseudoinverse operation we took is almost surely continuous (in order to satisfy Yang and
Hu (2020, Assm F.4(1))).19 However, the only pseudoinverse we took was ( 1"
REFERENCES,0.8072562358276644,"nΩ⊤Ω)+, and Ωhas
rank r (full rank) almost surely for any n > r. Therefore, our pseudoinverse operation is almost
surely continuous as pseudoinverse is continuous on matrices of constant rank."
REFERENCES,0.8095238095238095,"F.2
FORM OF THE LIMIT"
REFERENCES,0.8117913832199547,"Forward Pass (Theorem 3.4)
In the large-n limit, by the Master Theorem, for hidden weights,
Eq. (17) becomes"
REFERENCES,0.8140589569160998,"Zwlxl−1 = M+t
X"
REFERENCES,0.8163265306122449,"s=1
θs r
X"
REFERENCES,0.81859410430839,"i=1
Al
siZΩ:i = r
X"
REFERENCES,0.8208616780045351,"i=1
ZΩ:i
M+t
X"
REFERENCES,0.8231292517006803,"s=1
θsAl
si"
REFERENCES,0.8253968253968254,"where
θs = E """
REFERENCES,0.8276643990929705,"φ(Zhl−1)φ r
X"
REFERENCES,0.8299319727891157,"i=1
Bl
siZΩ:i
!#"
REFERENCES,0.8321995464852607,"Inductively, if gl ∈Rr represents the coefﬁcients of Zhl
= Zwlxl−1 in terms of ZΩ
def=
(ZΩ:1, . . . , ZΩ:r) (which is distributed as a standard isotropic Gaussian vector), then"
REFERENCES,0.8344671201814059,"gl
i = M+t
X"
REFERENCES,0.8367346938775511,"s=1
Al
si E φ(⟨gl−1, ZΩ⟩)φ(⟨Bl
s, ZΩ⟩),"
REFERENCES,0.8390022675736961,"which can be rearranged straightforwardly into the equation of Theorem 3.4. The equations for input
and output layers can be derived similarly."
REFERENCES,0.8412698412698413,"Backward Pass (Theorem 3.5)
For hidden weight wl, since we maintain wl in the form of Eq. (16),
the gradient update"
REFERENCES,0.8435374149659864,wl ←wl −ηΠΩ∇wlL = wl + (−ηΠΩ∇hlL) ⊗φ(hl−1)
REFERENCES,0.8458049886621315,"19Yang and Hu (2020, Assm F.4(1)) actually requires Moment nonlinearities with only parameter arguments
to be continuous eveywhere, but because our result is almost sure anyway, we can ignore any measure zero event."
REFERENCES,0.8480725623582767,Published as a conference paper at ICLR 2022
REFERENCES,0.8503401360544217,"Table 10: Omniglot Compute Time (in Seconds) Comparison. We measure average training time
(in seconds) per epoch for 50 epochs of Omniglot using half precision on a NVIDIA V100 GPU. We
evaluate a µ-Net, π-Net, and the π-Limit, each of depth 1, 2, 3, and 4; the π-Nets and the π-Limits
have r = 400. Because the π-Limit has a linearly increasing compute time per epoch due to gradient
concatenation, we also give an estimate expression for the compute time of the π-Limit in terms of t
epochs. This is not necessary for the 1-hidden-layer case, as there ANIL only trains the ﬁrst layer,
which does gradient accumulation."
REFERENCES,0.8526077097505669,"Hidden Layers
µ-Net
π-Net
π-Limit Average
π-Limit Epoch Estimate"
REFERENCES,0.854875283446712,"1
28.68
29.33
36.48
N.A.
2
32.38
35.99
313.4
58.65 + 10.19t
3
38.47
43.06
596.98
73.48 + 20.94t
4
42.24
49.37
872.83
86.83 + 31.44t"
REFERENCES,0.8571428571428571,"0
25
50
75
100
125
150
175
200
iter 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8594104308390023,training loss width
REFERENCES,0.8616780045351474,"8
512
32768
inf"
REFERENCES,0.8639455782312925,"4
6
8
10
12
14
log2(width) 9 8 7 6 5 4"
REFERENCES,0.8662131519274376,"log2(med loss
)"
REFERENCES,0.8684807256235828,"legend
med loss"
REFERENCES,0.8707482993197279,C/ width
REFERENCES,0.873015873015873,"Figure 10: Wide 1-hidden-layer π-nets with r = 2 have nearly identical loss curves as their
π-limit. (Left) We train π-nets of r = 2 and width 23, 29, 215 as well as their common π-limit on
a 128-image subset of CIFAR10 over 200 steps, with batch size 32 per step. We plot the training
loss vs steps on the left. While the width-8 π-net ﬂuctuates a bit around the π-limit curve, width-512
and -32768 π-nets have nearly identical loss curves as the π-limit. (Right) With the same dataset
and training procedure, we sweep widths 2{3,4,...,15} and 100 random seeds (which affect only the
random initialization). For each width and seed, we calculate the median loss deviation of the π-net
of that width from the π-limit, where the median is calculated over the 200 steps of training. Finally,
for each width, we plot the median of those medians over the 100 seeds as the blue curve (with 95%
conﬁdence interval in shade). This curve shows that the loss curve of a π-net converges roughly as
1/
√"
REFERENCES,0.8752834467120182,width to that of the π-limit.
REFERENCES,0.8775510204081632,in the limit corresponds to
REFERENCES,0.8798185941043084,"Al ←append(Al, coef(Z−nηΠΩ∇hlL)),
Bl ←append(Bl, coef(Zhl−1))"
REFERENCES,0.8820861678004536,"where coef(Zv) is the coefﬁcient of Zv in terms of ZΩ= (ZΩ:1, . . . , ZΩ:r). (Note the factor of n in
Z−nηΠΩ∇hlL) is just there so that nηΠΩ∇hlL has Θ(1)-sized coordinates). Of course, coef(Zhl−1)
is just gl−1 ∈Rr by deﬁnition. It remains to show that coef(Z−nηΠΩ∇hlL) = ∇gl lim L, where
lim L = limn→∞L(f(ξ), y) is the loss at the limit (which is deterministic).
Using the Master Theorem, it is not hard to see that, for each preactivation hl, Zn∇hlL is the Frechet
derivative ∂lim L"
REFERENCES,0.8843537414965986,"∂Zhl with respect to the random variable Zhl, where the Frechet derivative is deﬁned
with respect to the Hilbert space H of square integrable random variables in the σ-algebra generated
by ZΩdef= (ZΩ:1, . . . , ZΩ:r). Furthermore, using the limits of Eqs. (18) to (20) obtained from the
Master Theorem, it is easy to see that"
REFERENCES,0.8866213151927438,ZnΠΩ∇hlL = ΠZΩZn∇hlL
REFERENCES,0.8888888888888888,"where ΠZΩis the orthogonal projection to the linear subspace of H spanned by the random variables
ZΩ:1, . . . , ZΩ:r. Then Lemma F.1 applies and we get that"
REFERENCES,0.891156462585034,"ZnΠΩ∇hlL = r
X i=1"
REFERENCES,0.8934240362811792,∂lim L
REFERENCES,0.8956916099773242,"∂gl
i
ZΩ:i"
REFERENCES,0.8979591836734694,"coef(ZnΠΩ∇hlL) = ∇gl lim L,"
REFERENCES,0.9002267573696145,Published as a conference paper at ICLR 2022
REFERENCES,0.9024943310657596,"0
25
50
75
100
125
150
175
200
iter 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
REFERENCES,0.9047619047619048,training loss width
REFERENCES,0.9070294784580499,"8
512
32768
inf"
REFERENCES,0.909297052154195,"4
6
8
10
12
14
log2(width) 9 8 7 6 5 4 3"
REFERENCES,0.9115646258503401,"log2(med loss
)"
REFERENCES,0.9138321995464853,"legend
med loss"
REFERENCES,0.9160997732426304,C/ width
REFERENCES,0.9183673469387755,"Figure 11: Wide 2-hidden-layer π-nets with r = 2 have nearly identical loss curves as their
π-limit. We repeat the procedure in Fig. 10 for 2-hidden-layer π-nets. (Left) Width-32768 π-net has
almost identical loss curve as the π-limit. Compared to the 1-hidden-layer case, the width 512 π-net
is not as close to the limit, but this is expected as depth slows down convergence with width. (Right)
Nevertheless, we still see 1/
√"
REFERENCES,0.9206349206349206,width convergence to the π-limit in terms of training loss.
REFERENCES,0.9229024943310657,"2
7
2
9
2
11
2
13
2
15 Width 2
7 2
6 2
5 2
4 2
3 2
2"
REFERENCES,0.9251700680272109,Frob. Norm Distance
REFERENCES,0.927437641723356,empirical deviation
REFERENCES,0.9297052154195011,1/ width
REFERENCES,0.9319727891156463,"0
5000
10000
15000
20000
25000
30000
Width 14 15 16 17 18 19 20 21"
REFERENCES,0.9342403628117913,Accuracy
REFERENCES,0.9365079365079365,empirical
REFERENCES,0.9387755102040817,-Limit
REFERENCES,0.9410430839002267,"Figure 12: Convergence of feature kernel at initialization, as measured by (left) Frobenius
distance and (right) kernel regression accuracy. We perform all experiments here on a subset
of CIFAR10 with 400 training and 400 testing examples. (Left) We empirically verify that, at
initialization, the feature kernels of π-nets (with 2 hidden layers, r = 400) converge to the feature
kernel of the π-limit in Frobenius norm at a 1/
√"
REFERENCES,0.9433106575963719,"width rate. Here in blue we plot the Frobenius
distance of the π-net feature kernel to the limit kernel, normalized by the Frobenius norm of the limit
kernel. The shade represents 95% interval of the mean, taken over 10 random seeds. (Right) We
compute the feature kernel regression accuracy of randomly initialized π-nets of different widths
(blue solid curve) and their common π-limit (orange dashed curve). The shade represents 95%
conﬁdence interval of the mean, taken over 10 random seeds. We see a convergence of this accuracy
as one would expect from the theory. However, note that because the stability of kernel regression
depends crucially on small eigenvalues of the kernel, the width needs to quite large compared to the
data size (= kernel size) in order to visibly see convergence of accuracy; for data size beyond 400
training samples, we cannot see such convergence for width < 40, 000. This is why in Fig. 6, even at
initialization we see a large gap in accuracy between π-nets and the π-limit."
REFERENCES,0.9455782312925171,where the derivative is now an ordinary partial derivative.
REFERENCES,0.9478458049886621,"Lemma F.1. Let H be a Hilbert space and V be a k-dimensional subspace of V , where k is ﬁnite.
Let L : H →R be a Frechet differentiable function. Suppose Γ have for columns an orthonormal set
of basis for V , and w = Γb ∈V for some b ∈Rk. Then"
REFERENCES,0.9501133786848073,"ΠV ∇wL(w) = ∇bL(Γb),"
REFERENCES,0.9523809523809523,where ΠV : H →V is the orthogonal projection to V .
REFERENCES,0.9546485260770975,"Proof. By the theory of proximal gradients,"
REFERENCES,0.9569160997732427,"ΠV ∇wL(w) = w −min
v∈V"
REFERENCES,0.9591836734693877,"
⟨v, ∇wL(w)⟩+ 1"
REFERENCES,0.9614512471655329,"2∥v −w∥2

."
REFERENCES,0.963718820861678,Published as a conference paper at ICLR 2022
REFERENCES,0.9659863945578231,"0
25
50
75
100
125
150
175
200
iter 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9682539682539683,training loss width
REFERENCES,0.9705215419501134,"128
2048
32768
inf"
REFERENCES,0.9727891156462585,"0
25
50
75
100
125
150
175
200
iter 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.9750566893424036,training acc width
REFERENCES,0.9773242630385488,"128
2048
32768
inf"
REFERENCES,0.9795918367346939,"Figure 13: Wide 2-hidden-layer π-nets with r = 400 have nearly identical loss curves as their
π-limit. We repeat the procedure in Fig. 10 for 2-hidden-layer π-nets, but now with r = 400 and
widths 128, 2048, and 32768. (Left) Training loss. (Right) Training accuracy. In both subplots,
width-32768 π-net has almost identical curves as the π-limit, and the width-2048 curves follow them
closely."
REFERENCES,0.981859410430839,"Changing coordinates via Γ, we have"
REFERENCES,0.9841269841269841,"ΠV ∇wL(w) = w −min
c∈Rk"
REFERENCES,0.9863945578231292,"
⟨Γc, Γ∇bL(Γb)⟩+ 1"
REFERENCES,0.9886621315192744,"2∥Γc −Γb∥2
"
REFERENCES,0.9909297052154195,"= w −min
c∈Rk"
REFERENCES,0.9931972789115646,"
⟨c, ∇bL(Γb)⟩+ 1"
REFERENCES,0.9954648526077098,"2∥c −b∥2
"
REFERENCES,0.9977324263038548,= ∇bL(Γb)
