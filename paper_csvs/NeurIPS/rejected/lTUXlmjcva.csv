Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007974481658692185,"In the last decade, we have witnessed the introduction of several novel deep
1"
ABSTRACT,0.001594896331738437,"neural network (DNN) architectures exhibiting ever-increasing performance across
2"
ABSTRACT,0.0023923444976076554,"diverse tasks. Explaining the upward trend of their performance, however, remains
3"
ABSTRACT,0.003189792663476874,"difficult as different DNN architectures of comparable depth and width – common
4"
ABSTRACT,0.003987240829346092,"factors associated with their expressive power – may exhibit a drastically different
5"
ABSTRACT,0.004784688995215311,"performance even when trained on the same dataset. In this paper, we introduce
6"
ABSTRACT,0.005582137161084529,"the concept of the non-linearity signature of DNN, the first theoretically sound
7"
ABSTRACT,0.006379585326953748,"solution for approximately measuring the non-linearity of deep neural networks.
8"
ABSTRACT,0.007177033492822967,"Built upon a score derived from closed-form optimal transport mappings, this
9"
ABSTRACT,0.007974481658692184,"signature provides a better understanding of the inner workings of a wide range
10"
ABSTRACT,0.008771929824561403,"of DNN architectures and learning paradigms, with a particular emphasis on the
11"
ABSTRACT,0.009569377990430622,"computer vision task. We provide extensive experimental results that highlight the
12"
ABSTRACT,0.01036682615629984,"practical usefulness of the proposed non-linearity signature and its potential for
13"
ABSTRACT,0.011164274322169059,"long-reaching implications.
14"
INTRODUCTION,0.011961722488038277,"1
Introduction
15"
INTRODUCTION,0.012759170653907496,"Deep neural networks (DNNs) are undoubtedly the most powerful AI models currently available
16"
INTRODUCTION,0.013556618819776715,"[1, 2, 3, 4, 5]. Their performance on many tasks, including natural language processing (NLP) [6]
17"
INTRODUCTION,0.014354066985645933,"and computer vision [7], is already on par or exceeds that of a human being. One of the reasons
18"
INTRODUCTION,0.015151515151515152,"explaining such progress is of course the increasing computational resources [8, 9]. Another one is
19"
INTRODUCTION,0.01594896331738437,"the endeavour for finding ever more efficient neural architectures pursued by researchers over the
20"
INTRODUCTION,0.01674641148325359,"last decade. As of today, the transformer architecture [10] has firmly imposed itself as a number
21"
INTRODUCTION,0.017543859649122806,"one choice for most, if not all, of the recent breakthroughs [11, 12, 13] in the machine learning and
22"
INTRODUCTION,0.018341307814992026,"artificial intelligence fields.
23 24"
INTRODUCTION,0.019138755980861243,"Limitations
But why transformers are more capable than other architectures? Answering this
25"
INTRODUCTION,0.019936204146730464,"question requires finding a meaningful measure to compare the different famous models over
26"
INTRODUCTION,0.02073365231259968,"time gauging the trend of their intrinsic capacity. For such a comparison to be informative, it is
27"
INTRODUCTION,0.0215311004784689,"particularly appropriate to consider the computer vision field that produced many of the landmark
28"
INTRODUCTION,0.022328548644338118,"neural architectures improving upon each other over the years. Indeed, the decade-long revival of
29"
INTRODUCTION,0.023125996810207338,"deep learning started with Alexnet’s [14] architecture, the winner of the ImageNet Large Scale Visual
30"
INTRODUCTION,0.023923444976076555,"Recognition Challenge [15] in 2012. By achieving a significant improvement over the traditional
31"
INTRODUCTION,0.024720893141945772,"approaches, Alexnet was the first truly deep neural network to be trained on a dataset of such
32"
INTRODUCTION,0.025518341307814992,"scale, suggesting that deeper models were likely to bring even more gains. In the following years,
33"
INTRODUCTION,0.02631578947368421,"researchers proposed novel ways to train deeper models with hundreds of layers [16, 17, 18, 19]
34"
INTRODUCTION,0.02711323763955343,"pushing the performance frontier even further. The AI research landscape then reached a turning
35"
INTRODUCTION,0.027910685805422646,"point with the proposal of transformers [10], starting their unprecedented dominance first in NLP and
36"
INTRODUCTION,0.028708133971291867,"then in computer vision [20]. Surprisingly, transformers are not particularly deep, and the size of
37"
INTRODUCTION,0.029505582137161084,"their landmark vision architecture is comparable to that of Alexnet, and this despite a significant
38"
INTRODUCTION,0.030303030303030304,"performance gap between the two. Ultimately, this gap should be explained by the differences in the
39"
INTRODUCTION,0.03110047846889952,"expressive power [21] of the two models: a term used to denote the ability of a DNN to approximate
40"
INTRODUCTION,0.03189792663476874,"functions of a certain complexity. Unfortunately, the existing theoretical results related to this either
41"
INTRODUCTION,0.03269537480063796,"associate higher expressive power with depth [22, 23, 24] or width [25, 26, 27, 28] falling short in
42"
INTRODUCTION,0.03349282296650718,"comparing different families of architectures. This, in turn, limits our ability to understand what
43"
INTRODUCTION,0.0342902711323764,"underpins the achieved progress and what challenges and limitations still exist in the field, guiding
44"
INTRODUCTION,0.03508771929824561,"future research efforts.
45 46"
INTRODUCTION,0.03588516746411483,"Contributions We argue that quantifying the non-linearity of a DNN may be what we were missing
47"
INTRODUCTION,0.03668261562998405,"so far to understand the evolution of the deep learning models at a more fine-grained level. To verify
48"
INTRODUCTION,0.037480063795853266,"this hypothesis in practice, we put forward the following contributions:
49"
INTRODUCTION,0.03827751196172249,"1. We propose a first theoretically sound measure, called the affinity score, that estimates the
50"
INTRODUCTION,0.03907496012759171,"non-linearity of a given (activation) function using optimal transport (OT) theory. We use
51"
INTRODUCTION,0.03987240829346093,"the proposed affinity score to introduce the concept of the non-linearity signature of DNNs
52"
INTRODUCTION,0.04066985645933014,"defined as a set of affinity scores of all its activation functions.
53"
WE COMPARE NON-LINEARITY SIGNATURES OF A WIDE RANGE OF POPULAR DNNS USED IN COMPUTER,0.04146730462519936,"2. We compare non-linearity signatures of a wide range of popular DNNs used in computer
54"
WE COMPARE NON-LINEARITY SIGNATURES OF A WIDE RANGE OF POPULAR DNNS USED IN COMPUTER,0.04226475279106858,"vision: from Alexnet to vision transformers (ViT) and their more recent variations. Through
55"
WE COMPARE NON-LINEARITY SIGNATURES OF A WIDE RANGE OF POPULAR DNNS USED IN COMPUTER,0.0430622009569378,"this, we clearly illustrate the disruptive patterns in the evolution of the deep learning field.
56"
WE DEMONSTRATE THAT NON-LINEARITY SIGNATURE CAN BE PREDICTIVE OF DNNS PERFORMANCE AND,0.043859649122807015,"3. We demonstrate that non-linearity signature can be predictive of DNNs performance and
57"
WE DEMONSTRATE THAT NON-LINEARITY SIGNATURE CAN BE PREDICTIVE OF DNNS PERFORMANCE AND,0.044657097288676235,"used to meaningfully identify the family of approaches to which a given DNN belongs. We
58"
WE DEMONSTRATE THAT NON-LINEARITY SIGNATURE CAN BE PREDICTIVE OF DNNS PERFORMANCE AND,0.045454545454545456,"further show that the non-linearity signature is unique as it doesn’t correlate strongly with
59"
WE DEMONSTRATE THAT NON-LINEARITY SIGNATURE CAN BE PREDICTIVE OF DNNS PERFORMANCE AND,0.046251993620414676,"other potential candidates used for this task.
60"
WE DEMONSTRATE THAT NON-LINEARITY SIGNATURE CAN BE PREDICTIVE OF DNNS PERFORMANCE AND,0.04704944178628389,"The rest of the paper is organized as follows. We start by presenting the relevant background
61"
WE DEMONSTRATE THAT NON-LINEARITY SIGNATURE CAN BE PREDICTIVE OF DNNS PERFORMANCE AND,0.04784688995215311,"knowledge on OT in Section 2. Then, we introduce the affinity score together with its different
62"
WE DEMONSTRATE THAT NON-LINEARITY SIGNATURE CAN BE PREDICTIVE OF DNNS PERFORMANCE AND,0.04864433811802233,"theoretical properties in Section 3. Section 4 presents experimental evaluations on a wide range of
63"
WE DEMONSTRATE THAT NON-LINEARITY SIGNATURE CAN BE PREDICTIVE OF DNNS PERFORMANCE AND,0.049441786283891544,"popular convolutional neural networks. Finally, we conclude in Section 5.
64"
BACKGROUND,0.050239234449760764,"2
Background
65"
BACKGROUND,0.051036682615629984,"Optimal Transport
Let (X, d) be a metric space equipped with a lower semi-continuous cost
66"
BACKGROUND,0.051834130781499205,"function c : X × X →R≥0, e.g the Euclidean distance c(x, y) = ∥x −y∥. Then, the Kantorovich
67"
BACKGROUND,0.05263157894736842,"formulation of the OT problem between two probability measures µ, ν ∈P(X) is given by
68"
BACKGROUND,0.05342902711323764,"OTc(µ, ν) =
min
γ∈ADM(µ,ν) Eγ[c],
(1)"
BACKGROUND,0.05422647527910686,"where ADM(µ, ν) is the set of joint probabilities with marginals µ and ν, and Eν[f] denotes the
69"
BACKGROUND,0.05502392344497608,"expected value of f under ν. The optimal γ minimizing equation 1 is called the OT plan. Denote by
70"
BACKGROUND,0.05582137161084529,"L(X) the law of a random variable X. Then, the OT problem extends to random variables X, Y and
71"
BACKGROUND,0.05661881977671451,"we write OTc(X, Y ) meaning OTc(L(X), L(Y )).
72"
BACKGROUND,0.05741626794258373,"Assuming that either of the considered measures is absolutely continuous, then the Kantorovich
73"
BACKGROUND,0.058213716108452954,"problem is equivalent to the Monge problem
74"
BACKGROUND,0.05901116427432217,"OTc(µ, ν) =
min
T :T#µ=ν EX∼µ[c(X, T(X))],
(2)"
BACKGROUND,0.05980861244019139,"where the unique minimizing T is called the OT map, and T#µ denotes the push-forward measure,
75"
BACKGROUND,0.06060606060606061,"which is equivalent to the law of T(X), where X ∼µ.
76"
BACKGROUND,0.06140350877192982,"Wasserstein distance Let X be a random variable over Rd satisfying E[∥X −x0∥2] < ∞for some
77"
BACKGROUND,0.06220095693779904,"x0 ∈Rd, and thus for any x ∈Rd. We denote this class of random variables by P2(Rd). Then, the
78"
BACKGROUND,0.06299840510366826,"2-Wasserstein distance W2 between X, Y ∈P2(Rd) is defined as
79"
BACKGROUND,0.06379585326953748,"W2(X, Y ) = OT||x−y||2(X, Y )
1
2 .
(3)"
BACKGROUND,0.0645933014354067,"We now proceed to the presentation of our main contribution.
80"
NON-LINEARITY SIGNATURE OF DEEP NEURAL NETWORKS,0.06539074960127592,"3
Non-linearity signature of deep neural networks
81"
NON-LINEARITY SIGNATURE OF DEEP NEURAL NETWORKS,0.06618819776714513,"Among all non-linear operations introduced into DNNs in the last several decades, activation functions
82"
NON-LINEARITY SIGNATURE OF DEEP NEURAL NETWORKS,0.06698564593301436,"remain the only structural piece that they all inevitably share. Without non-linear activation functions,
83"
NON-LINEARITY SIGNATURE OF DEEP NEURAL NETWORKS,0.06778309409888357,"most of DNNs, no matter how deep, reduce to a linear function unable to learn complex patterns.
84"
NON-LINEARITY SIGNATURE OF DEEP NEURAL NETWORKS,0.0685805422647528,"Activation functions were also early identified [29, 30, 31, 32] as a key to making even a shallow
85"
NON-LINEARITY SIGNATURE OF DEEP NEURAL NETWORKS,0.06937799043062201,"network capable of approximating any function, however complex it may be, to arbitrary precision.
86"
NON-LINEARITY SIGNATURE OF DEEP NEURAL NETWORKS,0.07017543859649122,"We thus build our study on the following intuition: if activation functions play in important role
87"
NON-LINEARITY SIGNATURE OF DEEP NEURAL NETWORKS,0.07097288676236045,"in making DNNs non-linear, then measuring their degree of non-linearity can provide us with an
88"
NON-LINEARITY SIGNATURE OF DEEP NEURAL NETWORKS,0.07177033492822966,"approximation of the DNN’s non-linearity itself. To implement this intuition in practice, however, we
89"
NON-LINEARITY SIGNATURE OF DEEP NEURAL NETWORKS,0.07256778309409888,"first need to find a way to measure the non-linearity of an activation function. Surprisingly, there is
90"
NON-LINEARITY SIGNATURE OF DEEP NEURAL NETWORKS,0.0733652312599681,"no widely accepted measure for this, neither in the field of mathematics nor in the field of computer
91"
NON-LINEARITY SIGNATURE OF DEEP NEURAL NETWORKS,0.07416267942583732,"science. To fill this gap, we will use the OT theory to develop a so-called affinity score below.
92"
AFFINITY SCORE,0.07496012759170653,"3.1
Affinity score
93"
AFFINITY SCORE,0.07575757575757576,"Identifiability
We consider the pre-activation signal X of an activation function within a neural
94"
AFFINITY SCORE,0.07655502392344497,"network, and the post-activation signal σ(X) denoted by Y as input and output random variables.
95"
AFFINITY SCORE,0.0773524720893142,"Our first step to build the affinity score then is to ensure that we can identify when σ is linear with
96"
AFFINITY SCORE,0.07814992025518341,"respect to (wrt) X (for instance, when an otherwise non-linear activation is locally linear at the
97"
AFFINITY SCORE,0.07894736842105263,"support of X). To show that such an identifiability condition can be satisfied with OT, we first recall
98"
AFFINITY SCORE,0.07974481658692185,"the following classic result from the literature characterizing the OT maps.
99"
AFFINITY SCORE,0.08054226475279107,"Theorem 3.1 ([33]). Let X ∈P2(Rd), T(x) = ∇ϕ(x) for a convex function ϕ with T(X) ∈P2(Rd).
100"
AFFINITY SCORE,0.08133971291866028,"Then, T is the unique optimal OT map between µ and T#µ.
101"
AFFINITY SCORE,0.08213716108452951,"Using this theorem about the uniqueness of OT maps expressed as gradients of convex functions, we
102"
AFFINITY SCORE,0.08293460925039872,"can prove the following result (all proofs can be found in the Appendix C):
103"
AFFINITY SCORE,0.08373205741626795,"Corollary 3.2. Without loss of generality, let X, Y ∈P2(Rd) be centered, and let Y = σ(X) = TX,
104"
AFFINITY SCORE,0.08452950558213716,"where T is a positive definite linear transformation. Then, T is the OT map from X to Y .
105"
AFFINITY SCORE,0.08532695374800638,"Whenever the activation function σ is linear, the solution to the OT problem T exactly reproduces it.
106"
AFFINITY SCORE,0.0861244019138756,"Characterization
We now seek to understand whether T can be characterized more explicitly. For
107"
AFFINITY SCORE,0.08692185007974482,"this, we prove the following theorem stating that T can be computed in closed-form using the normal
108"
AFFINITY SCORE,0.08771929824561403,"approximations of X and Y .
109"
AFFINITY SCORE,0.08851674641148326,"Theorem 3.3. Let X, Y ∈P2(Rd) be centered and Y = TX for a positive definite matrix T. Let
110"
AFFINITY SCORE,0.08931419457735247,"NX ∼N(µ(X), Σ(X)) and NY ∼N(µ(Y ), Σ(Y )) be their normal approximations where µ and
111"
AFFINITY SCORE,0.09011164274322168,"Σ denote mean and covariance, respectively. Then, W2(NX, NY ) = W2(X, Y ) and T = Taff, where
112"
AFFINITY SCORE,0.09090909090909091,"Taff is the OT map between NX and NY and can be calculated in closed-form
113"
AFFINITY SCORE,0.09170653907496013,"Taff(x) = Ax + b,
A = Σ(Y )
1
2

Σ(Y )
1
2 Σ(X)Σ(Y )
1
2
−1"
AFFINITY SCORE,0.09250398724082935,"2 Σ(Y )
1
2 ,"
AFFINITY SCORE,0.09330143540669857,"b = µ(Y ) −Aµ(X).
(4)"
AFFINITY SCORE,0.09409888357256778,"Upper bound
When the activation σ is non-linear wrt X, the affine OT mapping Taff(X) will
114"
AFFINITY SCORE,0.094896331738437,"deviate from the true activation outputs Y . One important step toward quantifying this deviation is
115"
AFFINITY SCORE,0.09569377990430622,"given by the famous Gelbrich bound, formalized by means of the following theorem:
116"
AFFINITY SCORE,0.09649122807017543,"Theorem 3.4 (Gelbrich bound [34]). Let X, Y ∈P2(Rd) and let NX, NY be their normal approxi-
117"
AFFINITY SCORE,0.09728867623604466,"mations. Then, W2(NX, NY ) ≤W2(X, Y ).
118"
AFFINITY SCORE,0.09808612440191387,"This upper bound provides a first intuition of why OT can be a great tool for measuring non-linearity:
119"
AFFINITY SCORE,0.09888357256778309,"the cost of the affine map solving the OT problem on the left-hand side increases when the map
120"
AFFINITY SCORE,0.09968102073365231,"becomes non-linear. We now upper bound the difference between W2(NX, NY ) and W2(X, Y ), two
121"
AFFINITY SCORE,0.10047846889952153,"quantities that coincide only when σ is linear.
122"
AFFINITY SCORE,0.10127591706539076,"Proposition 3.5. Let X, Y ∈P2(Rd) and NX, NY be their normal approximations. Then,
123"
AFFINITY SCORE,0.10207336523125997,"1. |W2(NX, NY ) −W2(X, Y )| ≤
2 Tr
h
(Σ(X)Σ(Y ))
1
2
i
√"
AFFINITY SCORE,0.10287081339712918,"Tr[Σ(X)]+Tr[Σ(Y )].
124"
AFFINITY SCORE,0.10366826156299841,Higher mismatch
AFFINITY SCORE,0.10446570972886762,"Non-linearity signature = [ 
(ReLU1), 
(ReLU2), 
(ReLU3), 
... 
, 
(ReLUn)]"
AFFINITY SCORE,0.10526315789473684,"Convolution 
+ 
ReLU3"
AFFINITY SCORE,0.10606060606060606,"Convolution 
+ 
ReLU2"
AFFINITY SCORE,0.10685805422647528,"Convolution 
+ 
ReLU1
Kernel"
AFFINITY SCORE,0.1076555023923445,Output ReLUn
AFFINITY SCORE,0.10845295055821372,"Figure 1: Illustration of how the non-linearity of a given neural network is measured. (Top) The
non-linearity signature of a DNN is a collection of affinity scores calculated for each activation
function spread across its hidden layers. (Bottom) The affinity score is calculated based on 3 main
steps. First, given an input (grey) and an output (red) of an activation function (left), we estimate
the best affine OT fit Taff(X) (green) transporting the input to the output (middle-left). Second, we
measure the mismatch between the two by summing the transportation costs (middle-right) to obtain
the Wasserstein distance W2(TaffX, Y ). Finally, this distance is normalized with the magnitudes of
variance (arrows in the rightmost plot) of the output data based on its covariance matrix."
AFFINITY SCORE,0.10925039872408293,"2. For Taff as in (4), W2(TaffX, Y ) ≤
p"
AFFINITY SCORE,0.11004784688995216,"2 Tr [Σ(Y )].
125"
AFFINITY SCORE,0.11084529505582137,"To have a more informative non-linearity measure, we now need to normalize the non-negative Wasser-
126"
AFFINITY SCORE,0.11164274322169059,"stein distance W2(TaffX, Y ) to an interpretable interval of [0, 1]. The bound given in Proposition 3.5
127"
AFFINITY SCORE,0.11244019138755981,"lets us define the following affinity score
128"
AFFINITY SCORE,0.11323763955342903,"ρaff(X, σ(X)) = 1 −W2(TaffX, σ(X))
p"
AFFINITY SCORE,0.11403508771929824,"2 Tr[Σ(σ(X))]
.
(5)"
AFFINITY SCORE,0.11483253588516747,"The proposed affinity score quantifies how far a given activation σ is from an affine transformation.
129"
AFFINITY SCORE,0.11562998405103668,"It is equal to 1 for any input for which the activation function is linear, and 0 when it is maximally
130"
AFFINITY SCORE,0.11642743221690591,"non-linear, i.e., when TaffX and σ(X) are independent random variables.
131"
AFFINITY SCORE,0.11722488038277512,"Remark 3.6. One may wonder whether a simpler alternative to the affinity score can be to use,
132"
AFFINITY SCORE,0.11802232854864433,"instead of Taff, a mapping TW (x) = Wx defined as a solution of a linear regression problem
133"
AFFINITY SCORE,0.11881977671451356,"minW ||Y −WX||2
F . Then, one can use the coefficient of determination (R2 score) to measure how
134"
AFFINITY SCORE,0.11961722488038277,"well TW fits the observed data. This approach, however, has two drawbacks. First, following the
135"
AFFINITY SCORE,0.12041467304625199,"famous Gauss-Markov theorem, TW is an optimal linear (linear in Y ) estimator. On the contrary, Taff
136"
AFFINITY SCORE,0.12121212121212122,"is a globally optimal non-linear mapping aligning X and Y . Second, R2 compares the fit of TW with
137"
AFFINITY SCORE,0.12200956937799043,"that of a mapping outputting µ(Y ) for any value of X. This is contrary to ρaff that compares how
138"
AFFINITY SCORE,0.12280701754385964,"well Taff fits the data wrt to the worst possible cost incurred by Taff as quantified in Proposition 3.5.
139"
AFFINITY SCORE,0.12360446570972887,"This gives us a bounded score, i.e. ρaff ∈[0, 1], whereas R2 is not lower bounded, i.e. R2 ∈[−∞, 1].
140"
AFFINITY SCORE,0.12440191387559808,"We confirm experimentally in Section 4 that the two coefficients do not correlate consistently across
141"
AFFINITY SCORE,0.1251993620414673,"the studied DNNs suggesting that R2 is a poor proxy to ρaff.
142"
AFFINITY SCORE,0.12599681020733652,"(A)
(B) (C)"
AFFINITY SCORE,0.12679425837320574,"Figure 2: (A) Non-linearity of ReLU depends on the range of input values (red); (B) ReLU, Tanh,
and Sigmoid exhibit different degrees of non-linearity for the same input; (C) Affinity score captures
the increasing non-linearity of polynomials of different degrees."
NON-LINEARITY SIGNATURE,0.12759170653907495,"3.2
Non-linearity signature
143"
NON-LINEARITY SIGNATURE,0.1283891547049442,"We now turn our attention to the definition of a non-linearity signature of deep neural networks. We
144"
NON-LINEARITY SIGNATURE,0.1291866028708134,"define a neural network N as a composition of layers Fi where each layer Fi is a function taking
145"
NON-LINEARITY SIGNATURE,0.12998405103668262,"as input a tensor Xi ∈Rhi×wi×ci (for instance, an image of size 224 × 224 × 3 for i = 1) and
146"
NON-LINEARITY SIGNATURE,0.13078149920255183,"outputting a tensor Yi ∈Rhi+1×wi+1×ci+1 used as an input of the following layer Fi+1. This defines
147"
NON-LINEARITY SIGNATURE,0.13157894736842105,N = FL ⊙... ⊙Fi ... ⊙F1 = J
NON-LINEARITY SIGNATURE,0.13237639553429026,"k=1,...,L Fk where ⊙stands for a composition.
148"
NON-LINEARITY SIGNATURE,0.1331738437001595,"We now present the definition of a non-linearity signature of a network N. Below, we abuse the
149"
NON-LINEARITY SIGNATURE,0.1339712918660287,"compositional structure of Fi and see it as an ordered sequence of functions.
150"
NON-LINEARITY SIGNATURE,0.13476874003189793,Definition 3.1. Let N = J
NON-LINEARITY SIGNATURE,0.13556618819776714,"k=1,...,L Fk be a neural network. Define by A a finite set of common
activation functions such that A := {σ|σ : Rh×w×c →Rh×w×c}. Let r be a pooling operation such
that r : Rh×w×c →Rc. Then, the non-linearity signature of N given an input X is defined as follows:"
NON-LINEARITY SIGNATURE,0.13636363636363635,"ρaff(N; X) = {ρaff(r(Xi), σ(r(Xi))),
∀σ ∈Fi ∩A,
i = {1, . . . , L}}."
NON-LINEARITY SIGNATURE,0.1371610845295056,"Non-linearity signature, illustrated in Figure 1, associates to each network N a vector of affinity
151"
NON-LINEARITY SIGNATURE,0.1379585326953748,"scores calculated over the inputs and outputs of all activation functions encountered across its layers.
152 153"
NON-LINEARITY SIGNATURE,0.13875598086124402,"What makes an activation function non-linear?
We now want to understand the mechanism
154"
NON-LINEARITY SIGNATURE,0.13955342902711323,"behind achieving a lower or higher non-linearity with a given (activation) function. This will
155"
NON-LINEARITY SIGNATURE,0.14035087719298245,"explain what the different values of the affinity scores stand for when defining the non-linearity
156"
NON-LINEARITY SIGNATURE,0.14114832535885166,"signature of a DNN. In Figure 2(A), we show how the ReLU function [35], defined element-wise as
157"
NON-LINEARITY SIGNATURE,0.1419457735247209,"ReLU(x) = max(0, x), achieves its varying degree of non-linearity. Interestingly, this degree depends
158"
NON-LINEARITY SIGNATURE,0.14274322169059012,"only on the range of the input values. Second, in Figure 2(B) we also show how the shape of activation
159"
NON-LINEARITY SIGNATURE,0.14354066985645933,"functions impacts their non-linearity for a fixed input: surprisingly, piece-wise linear ReLU function
160"
NON-LINEARITY SIGNATURE,0.14433811802232854,"is more non-linear than Sigmoid(x) = 1/(e−x + 1) [36] or Tanh(x) = (e−x −ex)/(e−x + ex).
161"
NON-LINEARITY SIGNATURE,0.14513556618819776,"Similar observations also apply to compare polynomials of varying degrees (Figure 2(C)). We refer
162"
NON-LINEARITY SIGNATURE,0.145933014354067,"the reader to Appendix D for more visualizations of the affinity score of popular activation functions.
163"
RELATED WORK,0.1467304625199362,"3.3
Related work
164"
RELATED WORK,0.14752791068580542,"Layer-wise similarity analysis of DNNs
A line of work that can be distantly related to our main
165"
RELATED WORK,0.14832535885167464,"proposal is that of quantifying the similarity of the hidden layers of the DNNs as proposed [37] and
166"
RELATED WORK,0.14912280701754385,"[38] (see [39] for a complete survey of the subsequent works). [37] extracts activation patterns of
167"
RELATED WORK,0.14992025518341306,"the hidden layers in the DNNs and use CCA on the singular vectors extracted from them to measure
168"
RELATED WORK,0.1507177033492823,"how similar the two layers are. Their analysis brings many interesting insights regarding the learning
169"
RELATED WORK,0.15151515151515152,"dynamics of the different convnets, although they do not discuss the non-linearity propagation in the
170"
RELATED WORK,0.15231259968102073,"convnets, nor do they propose a way to measure it. [38] proposed to use a normalized Frobenius
171"
RELATED WORK,0.15311004784688995,"inner product between kernel matrices calculated on the extracted activations of the hidden layers
172"
RELATED WORK,0.15390749601275916,"and argued that such a similarity measure is more meaningful than that proposed by [37].
173"
RELATED WORK,0.1547049441786284,"Impact of activation functions
[40] provides the most comprehensive survey on the activation
174"
RELATED WORK,0.15550239234449761,"functions used in DNNs. Their work briefly discusses the non-linearity of the different activation
175"
RELATED WORK,0.15629984051036683,"functions suggesting that piecewise linear activation functions with more linear components are more
176"
RELATED WORK,0.15709728867623604,"non-linear (e.g., ReLU vs. ReLU6). [41] show theoretically that smooth versions of ReLU allow
177"
RELATED WORK,0.15789473684210525,"for more efficient information propagation in DNNs with a positive impact on their performance.
178"
RELATED WORK,0.15869218500797447,"Our work provides a first extensive comparison of all popular activation functions; we also show that
179"
RELATED WORK,0.1594896331738437,"smooth version of ReLU exhibit wider regions of high non-linearity (see Appendix D).
180"
RELATED WORK,0.16028708133971292,"Non-linearity measure
The only work similar to ours in spirit is the paper by [42] proposing the
181"
RELATED WORK,0.16108452950558214,"non-linearity coefficient in order to predict the train and test error of DNNs. Their coefficient is
182"
RELATED WORK,0.16188197767145135,"defined as a square root of the Jacobian of the neural network calculated wrt its input, multiplied by
183"
RELATED WORK,0.16267942583732056,"the covariance matrix of the Jacobian, and normalized by the covariance matrix of the input. The
184"
RELATED WORK,0.1634768740031898,"presence of the Jacobian in it calls for the differentiability assumption making its application to
185"
RELATED WORK,0.16427432216905902,"most of the neural networks with ReLU non-linearity impossible as is. The authors didn’t provide
186"
RELATED WORK,0.16507177033492823,"any implementation of their coefficient and we were not able to find any other study reporting the
187"
RELATED WORK,0.16586921850079744,"reproduced results from this work.
188"
EXPERIMENTAL EVALUATIONS,0.16666666666666666,"4
Experimental evaluations
189"
EXPERIMENTAL EVALUATIONS,0.1674641148325359,"We consider computer vision models trained and evaluated on the same Imagenet dataset with 1,000
190"
EXPERIMENTAL EVALUATIONS,0.1682615629984051,"output categories (Imagenet-1K) publicly available at [43]. The non-linearity signatures of different
191"
EXPERIMENTAL EVALUATIONS,0.16905901116427433,"studied models presented in the paper is calculated by passing batches of size 512 through the
192"
EXPERIMENTAL EVALUATIONS,0.16985645933014354,"pre-trained models for the entirety of the Imagenet-1K validation set (see Appendix H for more
193"
EXPERIMENTAL EVALUATIONS,0.17065390749601275,"datasets) with a total of 50,000 images. We include the following landmark architectures in our study:
194"
EXPERIMENTAL EVALUATIONS,0.17145135566188197,"Alexnet [14], four VGG models [16], Googlenet [44], Inception v3 [17], five Resnet models [18],
195"
EXPERIMENTAL EVALUATIONS,0.1722488038277512,"four Densenet models [19], four MNASNet models [45], four EfficientNet models [46], five ViT
196"
EXPERIMENTAL EVALUATIONS,0.17304625199362042,"models, three Swin transformer [47] and four Convnext models [48]. We include MNASNet and
197"
EXPERIMENTAL EVALUATIONS,0.17384370015948963,"EfficientNet models as prominent representatives of the neural architecture search approach [49].
198"
EXPERIMENTAL EVALUATIONS,0.17464114832535885,"Such models are expected to explicitly maximize the accuracy for a given computational budget.
199"
EXPERIMENTAL EVALUATIONS,0.17543859649122806,"Swin transformer and Convnext models are introduced as ViTs with traditional computer vision
200"
EXPERIMENTAL EVALUATIONS,0.1762360446570973,"priors. Their presence will be useful to better grasp how such priors impact ViTs. We refer the reader
201"
EXPERIMENTAL EVALUATIONS,0.17703349282296652,"to Appendix E for more practical details.
202"
EXPERIMENTAL EVALUATIONS,0.17783094098883573,"History of deep vision models at a glance
We give a general outlook of the developments in
203"
EXPERIMENTAL EVALUATIONS,0.17862838915470494,"computer vision over the last decade when seen through the lens of their non-linearity. In Figure 3
204"
EXPERIMENTAL EVALUATIONS,0.17942583732057416,"we present the minimum, median, and maximum values of the affinity scores calculated for the
205"
EXPERIMENTAL EVALUATIONS,0.18022328548644337,"considered neural networks (see Appendix F for raw non-linearity signatures). We immediately
206"
EXPERIMENTAL EVALUATIONS,0.1810207336523126,"see that until the arrival of transformers, the trend of the landmark models was to decrease their
207"
EXPERIMENTAL EVALUATIONS,0.18181818181818182,"non-linearity, rather than to increase it. On a more fine-grained level, we note that pure convolution
208"
EXPERIMENTAL EVALUATIONS,0.18261562998405104,"architectures such as Alexnet (2012) and VGGs (2014) exhibit a very low spread of the affinity
209"
EXPERIMENTAL EVALUATIONS,0.18341307814992025,"score values. This trend changes with the arrival of the inception module first used in Googlenet
210"
EXPERIMENTAL EVALUATIONS,0.18421052631578946,"(2014): the latter includes activation functions that extend the range of the non-linearity on both
211"
EXPERIMENTAL EVALUATIONS,0.1850079744816587,"ends of the spectrum. Importantly, we can see that the trend toward increasing the maximum and
212"
EXPERIMENTAL EVALUATIONS,0.18580542264752792,"average non-linearity of the neural networks has continued for almost the whole decade. Even more
213"
EXPERIMENTAL EVALUATIONS,0.18660287081339713,"surprisingly, EfficientNet models (2019), trained through neural architecture search, have strong
214"
EXPERIMENTAL EVALUATIONS,0.18740031897926634,"negative skewness toward higher linearity, although they were state-of-the-art in their time. The
215"
EXPERIMENTAL EVALUATIONS,0.18819776714513556,"second surprising finding comes with the arrival of ViTs (2020): they break the trend and leverage
216"
EXPERIMENTAL EVALUATIONS,0.18899521531100477,"the non-linearity of their hidden activation functions becoming more or more non-linear with the
217"
EXPERIMENTAL EVALUATIONS,0.189792663476874,"varying size of the patches (see Appendix F for a more detailed comparison with raw signatures).
218"
EXPERIMENTAL EVALUATIONS,0.19059011164274323,"This trend remains valid also for Swin transformers (2021), although introducing the computer vision
219"
EXPERIMENTAL EVALUATIONS,0.19138755980861244,"priors into them makes their non-linearity signature look more similar to pure convolutional networks
220"
EXPERIMENTAL EVALUATIONS,0.19218500797448165,"from the early 2010s, such as Alexnet and VGGs. Finally, we observe that the non-linearity signature
221"
EXPERIMENTAL EVALUATIONS,0.19298245614035087,"of a modern Convnext architecture (2022), designed as a convnet for 2020s using the best practices
222"
EXPERIMENTAL EVALUATIONS,0.1937799043062201,"of Swin transformers, further confirms this observation.
223"
EXPERIMENTAL EVALUATIONS,0.19457735247208932,"Figure 3: Median, minimum, and maximum values of non-linearity signatures of the different
architectures spanning a decade (2012-2022) of computer vision research. We observe a clear trend
toward the increase of the spread and the maximum values of the linearity in neural networks lasting
until the arrival of transformers in 2020. ViTs have a distinct pattern of maximizing the non-linearity
of their activation functions. Swin transformers and Convnext models retain this property from them
while remaining close to the pure convolutional networks."
EXPERIMENTAL EVALUATIONS,0.19537480063795853,"60
80
Accuracy 1.1 1.2"
EXPERIMENTAL EVALUATIONS,0.19617224880382775,Max/Median
EXPERIMENTAL EVALUATIONS,0.19696969696969696,R2 :  0.512
EXPERIMENTAL EVALUATIONS,0.19776714513556617,"60
80
Accuracy 10 20 30 Depth"
EXPERIMENTAL EVALUATIONS,0.19856459330143542,"R2 :  0.686
Pre-residual"
EXPERIMENTAL EVALUATIONS,0.19936204146730463,"70
80
90
Accuracy 1.2 1.4"
EXPERIMENTAL EVALUATIONS,0.20015948963317384,Max/Median
EXPERIMENTAL EVALUATIONS,0.20095693779904306,R2 :  0.814
EXPERIMENTAL EVALUATIONS,0.20175438596491227,"70
80
90
Accuracy 5 10 Ops"
EXPERIMENTAL EVALUATIONS,0.2025518341307815,"R2 :  0.666
Residual"
EXPERIMENTAL EVALUATIONS,0.20334928229665072,"70
80
90
Accuracy 0.075 0.100 0.125 Std"
EXPERIMENTAL EVALUATIONS,0.20414673046251994,R2 :  0.767
EXPERIMENTAL EVALUATIONS,0.20494417862838915,"70
80
90
Accuracy 1 2"
EXPERIMENTAL EVALUATIONS,0.20574162679425836,Nb params
EXPERIMENTAL EVALUATIONS,0.2065390749601276,"R2 :  0.955
NAS-based"
EXPERIMENTAL EVALUATIONS,0.20733652312599682,"70
80
90
Accuracy 0.1 0.2 0.3 Min"
EXPERIMENTAL EVALUATIONS,0.20813397129186603,R2 :  0.974
EXPERIMENTAL EVALUATIONS,0.20893141945773525,"70
80
90
Accuracy 0 500 1000 Ops"
EXPERIMENTAL EVALUATIONS,0.20972886762360446,"R2 :  0.842
ViTs"
EXPERIMENTAL EVALUATIONS,0.21052631578947367,"80
90
Accuracy 0.55 0.60"
EXPERIMENTAL EVALUATIONS,0.2113237639553429,Median
EXPERIMENTAL EVALUATIONS,0.21212121212121213,R2 :  0.818
EXPERIMENTAL EVALUATIONS,0.21291866028708134,"80
90
Accuracy 10 15 Depth"
EXPERIMENTAL EVALUATIONS,0.21371610845295055,"R2 :  0.788
Post-ViT"
EXPERIMENTAL EVALUATIONS,0.21451355661881977,"Figure 4: Best found dependency between the different statistics extracted from the non-linearity
signatures of the DNN families and their respective Imagenet-1K accuracy. The results are compared
in terms of the R2 score against the most precise of the other common DNN characteristics such as
depth, size, and the GFLOPS."
EXPERIMENTAL EVALUATIONS,0.215311004784689,"Densenets
Resnets
NAS + Inception 
Swins
Convnext
Pure convnets
ViTs A. D. B."
EXPERIMENTAL EVALUATIONS,0.21610845295055822,"Convolution 
+ 
ReLU 1"
EXPERIMENTAL EVALUATIONS,0.21690590111642744,ReLU 2
EXPERIMENTAL EVALUATIONS,0.21770334928229665,Convolution
EXPERIMENTAL EVALUATIONS,0.21850079744816586,Residual connection C.
EXPERIMENTAL EVALUATIONS,0.21929824561403508,"Figure 5: Comparing the different families of the neural architectures based on their non-linearity
signatures. (A) Hierarchical clustering of all DNNs considered in our study revealing meaningful
clusters with close architectural characteristics; (B) 9 representative architectures from all studied
families and the similarities between them. Note how the similarities between early convnets and other
models is decreasing with time until computer vision priors are introduced into Swin transformers in
2021; (C) Distributions of affinity scores in each network. Most models expand the non-linearity
ranges of their activation functions compared to early convnets. ViTs are dominated by highly
non-linear activation functions, Resnets have a bimodal distribution, Densenets, and EfficientNets
have a diametrically skewed distribution compared to ViTs. (D) Comparing the same convnet with 20
layers when trained with (Residual Resnet20) and without (Plain Resnet20) residual connections (top
row). Residual connections introduce a clear trend toward a bimodal distribution of affinity scores;
the same effect is observed for Resnet18 and Resnet34 (bottom row)."
EXPERIMENTAL EVALUATIONS,0.22009569377990432,"Closer look at accuracy/non-linearity trade-off Different families of vision models leverage differ-
224"
EXPERIMENTAL EVALUATIONS,0.22089314194577353,"ent characteristics of their internal non-linearity to achieve better performance. To better understand
225"
EXPERIMENTAL EVALUATIONS,0.22169059011164274,"this phenomenon, we now turn our attention to a more detailed analysis of the accuracy/non-linearity
226"
EXPERIMENTAL EVALUATIONS,0.22248803827751196,"trade-off by looking for a statistic extracted from their non-linearity signatures that is the most predic-
227"
EXPERIMENTAL EVALUATIONS,0.22328548644338117,"tive of their accuracy as measured by the R2 score. Additionally, we also want to understand whether
228"
EXPERIMENTAL EVALUATIONS,0.2240829346092504,"the non-linearity of DNNs can explain their performance better than the traditional characteristics
229"
EXPERIMENTAL EVALUATIONS,0.22488038277511962,"such as the number of parameters, the number of giga floating point operations per second (GFLOPS),
230"
EXPERIMENTAL EVALUATIONS,0.22567783094098884,"and the depth. From the results presented in Figure 4, we observe the following. First, the information
231"
EXPERIMENTAL EVALUATIONS,0.22647527910685805,"extracted from the non-linearity signatures often correlates more with the final accuracy, than the
232"
EXPERIMENTAL EVALUATIONS,0.22727272727272727,"usual DNN characteristics. This is the case for Residual networks (ResNets and DenseNets), ViTs,
233"
EXPERIMENTAL EVALUATIONS,0.22807017543859648,"and vision models influenced by transformers (Post-ViT). Unsurprisingly, for models based on neural
234"
EXPERIMENTAL EVALUATIONS,0.22886762360446572,"architecture search (NAS-based, i.e. EfficientNets and MNASNets) the number of parameters is
235"
EXPERIMENTAL EVALUATIONS,0.22966507177033493,"the most informative metric as they are specifically designed to reach the highest accuracy with the
236"
EXPERIMENTAL EVALUATIONS,0.23046251993620415,"increasing model size and compute. For Pre-residual pure convolutional models (Alexnet, VGGs,
237"
EXPERIMENTAL EVALUATIONS,0.23125996810207336,"Googlenet, and Inception), the spread of the non-linearity explains the accuracy increase similarly to
238"
EXPERIMENTAL EVALUATIONS,0.23205741626794257,"depth. Second, we observe that all models preceding ViTs were implicitly optimizing the spread of
239"
EXPERIMENTAL EVALUATIONS,0.23285486443381181,"their affinity score values to achieve better performance. After the arrival of the transformers, the
240"
EXPERIMENTAL EVALUATIONS,0.23365231259968103,"observed trend is to increase either the median or the minimum values of the non-linearity. This
241"
EXPERIMENTAL EVALUATIONS,0.23444976076555024,"suggests a fundamental shift in the implicit bias that the transformers carry.
242"
EXPERIMENTAL EVALUATIONS,0.23524720893141945,"Table 1: Pearson correlations between the non-linearity signature and other metrics, for all the
architectures evaluated in this study. The highest absolute value in each group is reported in bold."
EXPERIMENTAL EVALUATIONS,0.23604465709728867,"Models
CKA
NORM
SPARSITY
ENTROPY
R2"
EXPERIMENTAL EVALUATIONS,0.23684210526315788,"VGGs
0.0 ± 0.05
-0.67 ± 0.06
-0.18 ± 0.03
-0.90 ± 0.04
-0.21 ± 0.06
ResNets
0.53 ± 0.04
-0.41 ± 0.19
-0.68 ± 0.02
-0.38 ± 0.12
-0.48 ± 0.24
DenseNets
0.88 ± 0.02
-0.76 ± 0.02
-0.89 ± 0.02
-0.66 ± 0.03
0.85 ± 0.04
MNASNets
0.67 ± 0.11
-0.54 ± 0.14
-0.63 ± 0.07
-0.55 ± 0.16
0.45 ± 0.17
EfficientNets
0.42 ± 0.10
-0.16 ± 0.22
-0.17 ± 0.23
-0.16 ± 0.14
0.21 ± 0.12
ViTs
-0.22 ± 0.40
-0.67 ± 0.20
-0.09 ± 0.56
0.17 ± 0.25
-0.10 ± 0.34
Swins
-0.15 ± 0.13
-0.53 ± 0.10
-0.26 ± 0.17
0.06 ± 0.35
-0.13 ± 0.13
Convnexts
0.69 ± 0.08
0.21 ± 0.15
0.23 ± 0.16
0.02 ± 0.09
0.79 ± 0.05
Average
0.33 ± 0.45
-0.44 ± 0.34
-0.32 ± 0.42
-0.31 ± 0.39
0.14 ± 0.49"
EXPERIMENTAL EVALUATIONS,0.23763955342902712,"Distinct signature for every architecture
Non-linearity signature correctly identifies the different
243"
EXPERIMENTAL EVALUATIONS,0.23843700159489634,"families of neural architectures. To show this, we perform hierarchical clustering using pairwise
244"
EXPERIMENTAL EVALUATIONS,0.23923444976076555,"dynamic time warping (DTW) distances [50] between the non-linearity signatures of the models from
245"
EXPERIMENTAL EVALUATIONS,0.24003189792663476,"Figure 3. The results in Figure 5 (A), as well as the pairwise distance matrix between a representative
246"
EXPERIMENTAL EVALUATIONS,0.24082934609250398,"of each studied family in Figure 5 (B) (see Appendix G for the full matrix), show that we correctly
247"
EXPERIMENTAL EVALUATIONS,0.24162679425837322,"cluster all similar models together, both within their respective families (such as the different
248"
EXPERIMENTAL EVALUATIONS,0.24242424242424243,"variations of the same architecture) and across them (such as the cluster of Swin and pure convolution
249"
EXPERIMENTAL EVALUATIONS,0.24322169059011164,"models). Additionally, we highlight the individual affinity scores’ distributions of representative
250"
EXPERIMENTAL EVALUATIONS,0.24401913875598086,"models in Figure 5 (C). Finally, we highlight the exact effect of residual connections proposed in
251"
EXPERIMENTAL EVALUATIONS,0.24481658692185007,"2016 and used ever since by every benchmark model in Figure 5 (D). It reveals vividly that residual
252"
EXPERIMENTAL EVALUATIONS,0.24561403508771928,"connections make the distribution of the affinity scores bimodal with one such mode centered around
253"
EXPERIMENTAL EVALUATIONS,0.24641148325358853,"highly linear activation functions. This confirms in a principled way that residual connections indeed
254"
EXPERIMENTAL EVALUATIONS,0.24720893141945774,"tend to enable the learning of the identity function just as suggested in the seminal work that proposed
255"
EXPERIMENTAL EVALUATIONS,0.24800637958532695,"them [18]. Non-linearity signatures can also be applied to meaningfully identify training methods,
256"
EXPERIMENTAL EVALUATIONS,0.24880382775119617,"such as popular nowadays self-supervised approaches, for a fixed architecture (see Appendix I).
257 258"
EXPERIMENTAL EVALUATIONS,0.24960127591706538,"Uniqueness of the affinity score No other metric extracted from the activation functions of the
259"
EXPERIMENTAL EVALUATIONS,0.2503987240829346,"considered networks exhibits a strong consistent correlation with the non-linearity signature. To
260"
EXPERIMENTAL EVALUATIONS,0.2511961722488038,"validate this claim, we compare in Table 1 the Pearson correlation between the non-linearity signature
261"
EXPERIMENTAL EVALUATIONS,0.25199362041467305,"and several other metrics comparing the inputs and the outputs of the activation functions. We can see
262"
EXPERIMENTAL EVALUATIONS,0.2527910685805423,"that for different models the non-linearity correlates with different metrics suggesting that it captures
263"
EXPERIMENTAL EVALUATIONS,0.2535885167464115,"the information that other metrics fail to capture consistently across all architectures. This becomes
264"
EXPERIMENTAL EVALUATIONS,0.2543859649122807,"even more apparent when analyzing the individual correlation values (in Appendix G). Overall, the
265"
EXPERIMENTAL EVALUATIONS,0.2551834130781499,"proposed affinity score and the non-linearity signatures derived from it offer a unique perspective on
266"
EXPERIMENTAL EVALUATIONS,0.25598086124401914,"the developments in the ML field.
267"
DISCUSSIONS,0.2567783094098884,"5
Discussions
268"
DISCUSSIONS,0.25757575757575757,"We proposed the first sound approach to measure non-linearity of activation functions in neural
269"
DISCUSSIONS,0.2583732057416268,"networks and defined their non-linearity signature based on it. We further used non-linearity signatures
270"
DISCUSSIONS,0.259170653907496,"to provide a meaningful overview of the evolution of neural architectures proposed over the last
271"
DISCUSSIONS,0.25996810207336524,"decade with clear interpretable patterns. We showed that until the arrival of transformers, the trend in
272"
DISCUSSIONS,0.2607655502392344,"DNNs was to decrease their non-linearity, rather than to increase it. Vision transformers changed
273"
DISCUSSIONS,0.26156299840510366,"this pattern drastically. We also showcased that our measure is unique, as no other metric correlates
274"
DISCUSSIONS,0.2623604465709729,"strongly with it across all architectures.
275"
DISCUSSIONS,0.2631578947368421,"In the future, our work can be applied to study the non-linearity of the LLM models to better under-
276"
DISCUSSIONS,0.26395534290271133,"stand the effect of different architectural choices in them. On a higher level, our approach can also be
277"
DISCUSSIONS,0.2647527910685805,"used to identify new disruptive neural architectures by identifying those of them that leverage different
278"
DISCUSSIONS,0.26555023923444976,"internal non-linearity characteristics to obtain better performance. This capacity of identifying novel
279"
DISCUSSIONS,0.266347687400319,"technologies is even more crucial in the age of very large models where experimenting with the
280"
DISCUSSIONS,0.2671451355661882,"building blocks of the optimized backbone comes at a very high cost.
281"
REFERENCES,0.2679425837320574,"References
282"
REFERENCES,0.2687400318979266,"[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,
283"
REFERENCES,0.26953748006379585,"2015.
284"
REFERENCES,0.2703349282296651,"[2] Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks,
285"
REFERENCES,0.2711323763955343,"61:85–117, 2015.
286"
REFERENCES,0.2719298245614035,"[3] Michael I Jordan and Tom M Mitchell. Machine learning: Trends, perspectives, and prospects.
287"
REFERENCES,0.2727272727272727,"Science, 349(6245):255–260, 2015.
288"
REFERENCES,0.27352472089314195,"[4] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive computation and machine
289"
REFERENCES,0.2743221690590112,"learning. MIT Press, 2016.
290"
REFERENCES,0.2751196172248804,"[5] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud A.A. Setio, Francesco Ciompi,
291"
REFERENCES,0.2759170653907496,"Mohsen Ghafoorian, Jeroen A.W.M. van der Laak, Bram van Ginneken, and Clara I. Sánchez.
292"
REFERENCES,0.2767145135566188,"A survey on deep learning in medical image analysis. Medical image analysis, 42:60–88, 2017.
293"
REFERENCES,0.27751196172248804,"[6] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Wei Chen. Deberta: Decoding-enhanced
294"
REFERENCES,0.2783094098883573,"bert with disentangled attention. In Proceedings of the International Conference on Learning
295"
REFERENCES,0.27910685805422647,"Representations, 2021.
296"
REFERENCES,0.2799043062200957,"[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:
297"
REFERENCES,0.2807017543859649,"Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE
298"
REFERENCES,0.28149920255183414,"International Conference on Computer Vision, page 1026–1034, 2015.
299"
REFERENCES,0.2822966507177033,"[8] OpenAI. Ai and compute. 2018. Accessed: March 13, 2024.
300"
REFERENCES,0.28309409888357256,"[9] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for
301"
REFERENCES,0.2838915470494418,"deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019.
302"
REFERENCES,0.284688995215311,"[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
303"
REFERENCES,0.28548644338118023,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-
304"
REFERENCES,0.2862838915470494,"tion Processing Systems, pages 5998–6008, 2017.
305"
REFERENCES,0.28708133971291866,"[11] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
306"
REFERENCES,0.2878787878787879,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
307"
REFERENCES,0.2886762360446571,"few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
308"
REFERENCES,0.2894736842105263,"[12] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
309"
REFERENCES,0.2902711323763955,"thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,
310"
REFERENCES,0.29106858054226475,"Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
311"
REFERENCES,0.291866028708134,"language models, 2023.
312"
REFERENCES,0.2926634768740032,"[13] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.
313"
REFERENCES,0.2934609250398724,"[14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep
314"
REFERENCES,0.2942583732057416,"convolutional neural networks. Advances in neural information processing systems, 25:1097–
315"
REFERENCES,0.29505582137161085,"1105, 2012.
316"
REFERENCES,0.2958532695374801,"[15] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
317"
REFERENCES,0.2966507177033493,"Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
318"
REFERENCES,0.2974481658692185,"ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision,
319"
REFERENCES,0.2982456140350877,"115(3):211–252, 2015.
320"
REFERENCES,0.29904306220095694,"[16] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
321"
REFERENCES,0.29984051036682613,"image recognition. In International Conference on Learning Representations, 2015.
322"
REFERENCES,0.30063795853269537,"[17] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Arman Alemi. Rethinking the
323"
REFERENCES,0.3014354066985646,"inception architecture for computer vision. arXiv preprint arXiv:1512.00567, 2016.
324"
REFERENCES,0.3022328548644338,"[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
325"
REFERENCES,0.30303030303030304,"recognition. arXiv preprint arXiv:1512.03385, 2016.
326"
REFERENCES,0.3038277511961722,"[19] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
327"
REFERENCES,0.30462519936204147,"convolutional networks. arXiv preprint arXiv:1608.06993, 2017.
328"
REFERENCES,0.3054226475279107,"[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
329"
REFERENCES,0.3062200956937799,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
330"
REFERENCES,0.30701754385964913,"Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
331"
REFERENCES,0.3078149920255183,"recognition at scale. In ICLR, 2021.
332"
REFERENCES,0.30861244019138756,"[21] Ingo Gühring, Mones Raslan, and Gitta Kutyniok. Expressivity of deep neural networks.
333"
REFERENCES,0.3094098883572568,"arXiv:2007.04759, 2020.
334"
REFERENCES,0.310207336523126,"[22] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In 29th
335"
REFERENCES,0.31100478468899523,"Annual Conference on Learning Theory, pages 907–940, 2016.
336"
REFERENCES,0.3118022328548644,"[23] Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with
337"
REFERENCES,0.31259968102073366,"neural networks. In Proceedings of the 34th International Conference on Machine Learning,
338"
REFERENCES,0.3133971291866029,"pages 2979–2987, 2017.
339"
REFERENCES,0.3141945773524721,"[24] Peter L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-
340"
REFERENCES,0.3149920255183413,"dimension and pseudodimension bounds for piecewise linear neural networks. Journal of
341"
REFERENCES,0.3157894736842105,"Machine Learning Research, 20(63):1–17, 2019.
342"
REFERENCES,0.31658692185007975,"[25] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the
343"
REFERENCES,0.31738437001594894,"expressive power of deep neural networks. In Proceedings of the International Conference on
344"
REFERENCES,0.3181818181818182,"Machine Learning, pages 2847–2854, 2017.
345"
REFERENCES,0.3189792663476874,"[26] Guido Montúfar, Razvan Pascanu, KyungHyun Cho, and Yoshua Bengio. On the number of
346"
REFERENCES,0.3197767145135566,"linear regions of deep neural networks. In NeurIPS, pages 2924–2932, 2014.
347"
REFERENCES,0.32057416267942584,"[27] Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power
348"
REFERENCES,0.32137161084529503,"of neural networks: a view from the width. In Advances in Neural Information Processing
349"
REFERENCES,0.32216905901116427,"Systems, page 6232–6240, 2017.
350"
REFERENCES,0.3229665071770335,"[28] Gal Vardi, Gilad Yehudai, and Ohad Shamir. On the optimal memorization power of relu neural
351"
REFERENCES,0.3237639553429027,"networks. In The Tenth International Conference on Learning Representations, ICLR, 2022.
352"
REFERENCES,0.32456140350877194,"[29] Kurt Hornik. Multilayer feedforward networks are universal approximators. Neural Networks,
353"
REFERENCES,0.3253588516746411,"2(5):359–366, 1989.
354"
REFERENCES,0.32615629984051037,"[30] Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Mach.
355"
REFERENCES,0.3269537480063796,"Learn., 14(1):115–133, 1994.
356"
REFERENCES,0.3277511961722488,"[31] Kurt and Hornik. Approximation capabilities of multilayer feedforward networks. Neural
357"
REFERENCES,0.32854864433811803,"Networks, 4(2):251–257, 1991.
358"
REFERENCES,0.3293460925039872,"[32] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
359"
REFERENCES,0.33014354066985646,"Signals, and Systems (MCSS), 2(4):303–314, 1989.
360"
REFERENCES,0.3309409888357257,"[33] Cyril S Smith and Martin Knott. Note on the optimal transportation of distributions. Journal of
361"
REFERENCES,0.3317384370015949,"Optimization Theory and Applications, 52(2):323–329, 1987.
362"
REFERENCES,0.33253588516746413,"[34] Matthias Gelbrich. On a formula for the l2 wasserstein metric between measures on euclidean
363"
REFERENCES,0.3333333333333333,"and hilbert spaces. Mathematische Nachrichten, 147(1):185–203, 1990.
364"
REFERENCES,0.33413078149920256,"[35] Vinod Nair and Geoffrey E. Hinton.
Rectified linear units improve restricted boltzmann
365"
REFERENCES,0.3349282296650718,"machines. In Proceedings of the International Conference on Machine Learning, pages 807–
366"
REFERENCES,0.335725677830941,"814, 2010.
367"
REFERENCES,0.3365231259968102,"[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating
368"
REFERENCES,0.3373205741626794,"errors. Nature, 323(6088):533–536, 1986.
369"
REFERENCES,0.33811802232854865,"[37] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular
370"
REFERENCES,0.33891547049441784,"vector canonical correlation analysis for deep learning dynamics and interpretability. In NIPS’17,
371"
REFERENCES,0.3397129186602871,"page 6078–6087, 2017.
372"
REFERENCES,0.3405103668261563,"[38] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
373"
REFERENCES,0.3413078149920255,"network representations revisited. In ICML, volume 97, pages 3519–3529. PMLR, 09–15 Jun
374"
REFERENCES,0.34210526315789475,"2019.
375"
REFERENCES,0.34290271132376393,"[39] MohammadReza Davari, Stefan Horoi, Amine Natik, Guillaume Lajoie, Guy Wolf, and Eugene
376"
REFERENCES,0.3437001594896332,"Belilovsky. Reliability of CKA as a similarity measure in deep learning. In ICLR, 2023.
377"
REFERENCES,0.3444976076555024,"[40] Shiv Ram Dubey, Satish Kumar Singh, and Bidyut Baran Chaudhuri. Activation functions in
378"
REFERENCES,0.3452950558213716,"deep learning: A comprehensive survey and benchmark. Neurocomput., 503(C):92–108, 2022.
379"
REFERENCES,0.34609250398724084,"[41] Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function
380"
REFERENCES,0.34688995215311,"on deep neural networks training. In Proceedings of the 36th International Conference on
381"
REFERENCES,0.34768740031897927,"Machine Learning, pages 2672–2680, 2019.
382"
REFERENCES,0.3484848484848485,"[42] George Philipp. The nonlinearity coefficient - A practical guide to neural architecture design.
383"
REFERENCES,0.3492822966507177,"CoRR, abs/2105.12210, 2021.
384"
REFERENCES,0.35007974481658694,"[43] TorchVision maintainers and contributors. Torchvision: Pytorch’s computer vision library.
385"
REFERENCES,0.3508771929824561,"GitHub repository, 2016.
386"
REFERENCES,0.35167464114832536,"[44] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,
387"
REFERENCES,0.3524720893141946,"Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
388"
REFERENCES,0.3532695374800638,"arXiv preprint arXiv:1409.4842, 2014.
389"
REFERENCES,0.35406698564593303,"[45] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
390"
REFERENCES,0.3548644338118022,"Quoc V. Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of
391"
REFERENCES,0.35566188197767146,"the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.
392"
REFERENCES,0.35645933014354064,"[46] Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural
393"
REFERENCES,0.3572567783094099,"networks. In Proceedings of the International Conference on Machine Learning, pages 6105–
394"
REFERENCES,0.3580542264752791,"6114, 2019.
395"
REFERENCES,0.3588516746411483,"[47] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
396"
REFERENCES,0.35964912280701755,"Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
397"
REFERENCES,0.36044657097288674,"of the IEEE/CVF International Conference on Computer Vision, 2021.
398"
REFERENCES,0.361244019138756,"[48] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining
399"
REFERENCES,0.3620414673046252,"Xie. A convnet for the 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision
400"
REFERENCES,0.3628389154704944,"and Pattern Recognition, 2022.
401"
REFERENCES,0.36363636363636365,"[49] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey.
402"
REFERENCES,0.36443381180223283,"Journal of Machine Learning Research, 20(55):1–21, 2019.
403"
REFERENCES,0.3652312599681021,"[50] Hiroaki Sakoe and Seibi Chiba. Dynamic programming algorithm optimization for spoken word
404"
REFERENCES,0.3660287081339713,"recognition. IEEE transactions on acoustics, speech, and signal processing, 26(1):43–49, 1978.
405"
REFERENCES,0.3668261562998405,"[51] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
406"
REFERENCES,0.36762360446570974,"Geoffrey Gordon, David Dunson, and Miroslav Dudík, editors, Proceedings of the Fourteenth
407"
REFERENCES,0.3684210526315789,"International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings
408"
REFERENCES,0.36921850079744817,"of Machine Learning Research, pages 315–323, Fort Lauderdale, FL, USA, 11–13 Apr 2011.
409"
REFERENCES,0.3700159489633174,"PMLR.
410"
REFERENCES,0.3708133971291866,"[52] Dan Hendrycks and Kevin Gimpel.
Gaussian error linear units (gelus).
arXiv preprint
411"
REFERENCES,0.37161084529505584,"arXiv:1606.08415, 2016.
412"
REFERENCES,0.372408293460925,"[53] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Wei Wang, Wenhan Weng,
413"
REFERENCES,0.37320574162679426,"Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
414"
REFERENCES,0.3740031897926635,"mobile vision applications. In Proceedings of the 2017 IEEE Conference on Computer Vision
415"
REFERENCES,0.3748006379585327,"and Pattern Recognition, pages 4200–4210. IEEE, 2017.
416"
REFERENCES,0.37559808612440193,"[54] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural
417"
REFERENCES,0.3763955342902711,"network acoustic models. In Proceedings of the ICML Workshop on Deep Learning for Audio,
418"
REFERENCES,0.37719298245614036,"Speech and Language Processing, 2013.
419"
REFERENCES,0.37799043062200954,"[55] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network
420"
REFERENCES,0.3787878787878788,"function approximation in reinforcement learning. Neural networks, 107:3–11, 2018.
421"
REFERENCES,0.379585326953748,"[56] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan,
422"
REFERENCES,0.3803827751196172,"Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3.
423"
REFERENCES,0.38118022328548645,"In Proceedings of the IEEE/CVF international conference on computer vision, pages 1314–1324,
424"
REFERENCES,0.38197767145135564,"2019.
425"
REFERENCES,0.3827751196172249,"[57] Olivier Ledoit and Michael Wolf. Honey, i shrunk the sample covariance matrix. Journal of
426"
REFERENCES,0.3835725677830941,"Portfolio Management, 30(4):110–119, 2004.
427"
REFERENCES,0.3843700159489633,"[58] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
428"
REFERENCES,0.38516746411483255,"Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural
429"
REFERENCES,0.38596491228070173,"information processing systems, 33:9912–9924, 2020.
430"
REFERENCES,0.386762360446571,"[59] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski,
431"
REFERENCES,0.3875598086124402,"and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings
432"
REFERENCES,0.3883572567783094,"of the IEEE/CVF international conference on computer vision, pages 9650–9660, 2021.
433"
REFERENCES,0.38915470494417864,"[60] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
434"
REFERENCES,0.38995215311004783,"unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on
435"
REFERENCES,0.39074960127591707,"computer vision and pattern recognition, pages 9729–9738, 2020.
436"
REFERENCES,0.3915470494417863,"[61] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Procedings of the British
437"
REFERENCES,0.3923444976076555,"Machine Vision Conference 2016. British Machine Vision Association, 2016.
438"
REFERENCES,0.39314194577352474,"[62] Mert Bülent Sarıyıldız, Yannis Kalantidis, Karteek Alahari, and Diane Larlus. No reason for
439"
REFERENCES,0.3939393939393939,"no supervision: Improved generalization in supervised models. In The Eleventh International
440"
REFERENCES,0.39473684210526316,"Conference on Learning Representations, 2023.
441"
REFERENCES,0.39553429027113235,"[63] Julien Denize, Jaonary Rabarisoa, Astrid Orcesi, Romain Hérault, and Stéphane Canu. Similarity
442"
REFERENCES,0.3963317384370016,"contrastive estimation for self-supervised soft contrastive learning. In Proceedings of the
443"
REFERENCES,0.39712918660287083,"IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2706–2716, 2023.
444"
REFERENCES,0.39792663476874,"[64] Guangrun Wang, Keze Wang, Guangcong Wang, Philip HS Torr, and Liang Lin. Solving
445"
REFERENCES,0.39872408293460926,"inefficiency of self-supervised representation learning.
In Proceedings of the IEEE/CVF
446"
REFERENCES,0.39952153110047844,"International Conference on Computer Vision, pages 9505–9515, 2021.
447"
REFERENCES,0.4003189792663477,"[65] Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Changshui Zhang, Xiaogang Wang, and
448"
REFERENCES,0.4011164274322169,"Chang Xu. Ressl: Relational self-supervised learning with weak augmentation. Advances in
449"
REFERENCES,0.4019138755980861,"Neural Information Processing Systems, 34:2543–2555, 2021.
450"
REFERENCES,0.40271132376395535,"A
Broader Impacts
451"
REFERENCES,0.40350877192982454,"This paper presents work whose goal is to advance the field of Machine Learning and better understand
452"
REFERENCES,0.4043062200956938,"the underlying behavior of Deep Neural Networks architectures. There are many potential societal
453"
REFERENCES,0.405103668261563,"consequences of our work, none which we feel must be specifically highlighted here.
454"
REFERENCES,0.4059011164274322,"B
Limitations
455"
REFERENCES,0.40669856459330145,"An important assumption of Theorem 3.3, is that the activation function that we want to analyze
456"
REFERENCES,0.40749601275917063,"through ρaff needs to be a positive definite transformation of the inputs. Fortunately, this is the case for
457"
REFERENCES,0.4082934609250399,"activation functions, that we consider in this paper. Finally, we note that despite the strong correlation
458"
REFERENCES,0.4090909090909091,"between the statistics extracted from the non-linearity signatures for certain DNNs’ architectures,
459"
REFERENCES,0.4098883572567783,"we are yet to show that explicitly optimizing affinity scores through backpropagation can have an
460"
REFERENCES,0.41068580542264754,"actionable impact on DNNs performance or its other properties, such as robustness or transferability.
461"
REFERENCES,0.41148325358851673,"C
Proofs of main theoretical results
462"
REFERENCES,0.41228070175438597,"In this section, we provide proofs of the main theoretical results from the paper.
463"
REFERENCES,0.4130781499202552,"Corollary 3.2. Without loss of generality, let X, Y ∈P2(Rd) be centered, and such that Y = TX,
464"
REFERENCES,0.4138755980861244,"where T is a positive semi-definite linear transformation. Then, T is the OT map from X to Y .
465"
REFERENCES,0.41467304625199364,"Proof. We first proof that we can consider centered distributions without loss of generality. To this
466"
REFERENCES,0.4154704944178628,"end, we note that
467"
REFERENCES,0.41626794258373206,"W 2
2 (X, Y ) = W 2
2 (X −E[X], Y −E[Y ]) + ∥E[X] −E[Y ]∥2,
(6)"
REFERENCES,0.41706539074960125,"implying that splitting the 2-Wasserstein distance into two independent terms concerning the L2
468"
REFERENCES,0.4178628389154705,"distance between the means and the 2-Wasserstein distance between the centered measures.
469"
REFERENCES,0.41866028708133973,"Furthermore, if we have an OT map T ′ between X −E[X] and Y −E[Y ], then
470"
REFERENCES,0.4194577352472089,"T(x) = T ′(x −E[X]) + E[Y ],
(7)"
REFERENCES,0.42025518341307816,"is the OT map between X and Y .
471"
REFERENCES,0.42105263157894735,"To prove the statement of the Corollary, we now need to apply Theorem 3.1 to the convex ϕ(x) =
472"
REFERENCES,0.4218500797448166,"xT Tx, where T is positive semi-definite.
473"
REFERENCES,0.4226475279106858,"Theorem 3.3. Let X, Y ∈P2(Rd) be centered and Y = TX for a positive definite matrix T. Let
474"
REFERENCES,0.423444976076555,"NX ∼N(µ(X), Σ(X)) and NY ∼N(µ(Y ), Σ(Y )) be their normal approximations where µ and Σ
475"
REFERENCES,0.42424242424242425,"denote mean and covariance, respectively. Then, W2(NX, NY ) = W2(X, Y ) and T = Taff, where
476"
REFERENCES,0.42503987240829344,"Taff is the OT map between NX and NY and can be calculated in closed-form
477"
REFERENCES,0.4258373205741627,"Taff(x) = Ax + b,
A = Σ(Y )
1
2

Σ(Y )
1
2 Σ(X)Σ(Y )
1
2
−1"
REFERENCES,0.4266347687400319,"2 Σ(Y )
1
2 ,"
REFERENCES,0.4274322169059011,"b = µ(Y ) −Aµ(X).
(8)"
REFERENCES,0.42822966507177035,"Proof. Corollary 3.2 states that T is an OT map, and
478"
REFERENCES,0.42902711323763953,Σ(TNX) = TΣ(X)T = Σ(Y ).
REFERENCES,0.4298245614035088,"Therefore, TNX = NY , and by Theorem 3.1, T is the OT map between NX and NY . Finally, we
479"
REFERENCES,0.430622009569378,"compute
480"
REFERENCES,0.4314194577352472,"W 2
2 (NX, NY ) = Tr[Σ(X)] + Tr[TΣ(X)T] −2 Tr[T
1
2 Σ(X)T
1
2 ]"
REFERENCES,0.43221690590111644,"= arg min
T :T (X)=Y
EX[∥X −T(X)∥2]"
REFERENCES,0.43301435406698563,"=W 2
2 (X, Y ). 481"
REFERENCES,0.43381180223285487,"Proposition 3.5. Let X, Y ∈P2(Rd) and NX, NY be their normal approximations. Then,
482"
REFERENCES,0.43460925039872406,"1. |W2(NX, NY ) −W2(X, Y )| ≤
2 Tr
h
(Σ(X)Σ(Y ))
1
2
i
√"
REFERENCES,0.4354066985645933,"Tr[Σ(X)]+Tr[Σ(Y )].
483"
REFERENCES,0.43620414673046254,"2. For Taff as in (4), W2(TaffX, Y ) ≤
√"
REFERENCES,0.4370015948963317,2 Tr [Σ(Y )]
REFERENCES,0.43779904306220097,"1
2 .
484"
REFERENCES,0.43859649122807015,"Proof. By Theorem 3.4, we have W2(NX, NY ) ≤W2(X, Y ). On the other hand,
485"
REFERENCES,0.4393939393939394,"W 2
2 (X, Y ) =
min
γ∈ADM(X,Y ) Z"
REFERENCES,0.44019138755980863,"Rd×Rd ∥x −y∥2dγ(x, y) ≤
Z Rd×Rd"
REFERENCES,0.4409888357256778," 
∥x∥2 + ∥y∥2
dγ(x, y)"
REFERENCES,0.44178628389154706,= Tr[Σ(X)] + Tr[Σ(Y )].
REFERENCES,0.44258373205741625,"Combining the above inequalities, we get
486"
REFERENCES,0.4433811802232855,"|W2(NX, NY ) −W2(X, Y )| ≤

p"
REFERENCES,0.44417862838915473,"Tr[Σ(X)] + Tr[Σ(Y )] −W2(NX, NY )
 ."
REFERENCES,0.4449760765550239,"Let a = Tr[Σ(X)] + Tr[Σ(Y )], and so W 2
2 (NX, NY ) = a −b, where b = 2 Tr
h
(Σ(X)Σ(Y ))"
REFERENCES,0.44577352472089316,"1
2
i
.
487"
REFERENCES,0.44657097288676234,"Then the RHS of can be written as
488"
REFERENCES,0.4473684210526316,"√a −
√"
REFERENCES,0.4481658692185008,"a −b
 = |a −(a −b)|
√a +
√"
REFERENCES,0.44896331738437,"a −b ≤
b
√a,"
REFERENCES,0.44976076555023925,"where the inequality follows from positivity of W2(NX, NY ) =
√"
REFERENCES,0.45055821371610844,"a −b. Letting X = TaffX in the
489"
REFERENCES,0.4513556618819777,"obtained bound gives 2).
490"
REFERENCES,0.45215311004784686,"20
10
0
10
20 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.4529505582137161,activation 0.00 0.25 0.50 0.75 1.00 1.25 aff
REFERENCES,0.45374800637958534,"sigmoid
aff
baseline"
REFERENCES,0.45454545454545453,"20
10
0
10
20 1.0 0.5 0.0 0.5 1.0"
REFERENCES,0.45534290271132377,activation 0.00 0.25 0.50 0.75 1.00 1.25 aff
REFERENCES,0.45614035087719296,"tanh
aff
baseline"
REFERENCES,0.4569377990430622,"20
10
0
10
20 1.0 0.5 0.0 0.5 1.0"
REFERENCES,0.45773524720893144,activation 0.00 0.25 0.50 0.75 1.00 1.25 aff
REFERENCES,0.4585326953748006,"hardtanh
aff
baseline"
REFERENCES,0.45933014354066987,"20
10
0
10
20 0 5 10 15 20"
REFERENCES,0.46012759170653905,activation 0.00 0.25 0.50 0.75 1.00 1.25 aff
REFERENCES,0.4609250398724083,"relu
aff
baseline"
REFERENCES,0.46172248803827753,"20
10
0
10
20 0 5 10 15 20"
REFERENCES,0.4625199362041467,activation 0.00 0.25 0.50 0.75 1.00 1.25 aff
REFERENCES,0.46331738437001596,"leaky_relu
aff
baseline"
REFERENCES,0.46411483253588515,"20
10
0
10
20 0 2 4 6"
REFERENCES,0.4649122807017544,activation 0.00 0.25 0.50 0.75 1.00 1.25 aff
REFERENCES,0.46570972886762363,"relu6
aff
baseline"
REFERENCES,0.4665071770334928,"20
10
0
10
20 0 5 10 15 20"
REFERENCES,0.46730462519936206,activation 0.00 0.25 0.50 0.75 1.00 1.25 aff
REFERENCES,0.46810207336523124,"gelu
aff
baseline"
REFERENCES,0.4688995215311005,"20
10
0
10
20 0 5 10 15 20"
REFERENCES,0.4696969696969697,activation 0.00 0.25 0.50 0.75 1.00 1.25 aff
REFERENCES,0.4704944178628389,"hardswish
aff
baseline"
REFERENCES,0.47129186602870815,"20
10
0
10
20 0 5 10 15 20"
REFERENCES,0.47208931419457734,activation 0.00 0.25 0.50 0.75 1.00 1.25 aff
REFERENCES,0.4728867623604466,"silu
aff
baseline"
REFERENCES,0.47368421052631576,"Figure 6: Median affinity scores of Sigmoid, ReLU, GELU, ReLU6, LeakyReLU with a default
value of slope, Tanh, HardTanh, SiLU, and HardSwish obtained across random draws from Gaussian
distribution with a sliding mean and varying stds used as their input. Whiskers of boxplots show the
whole range of values obtained for each mean across all stds. The baseline value is the affinity score
obtained for a sample covering the whole interval. The ranges and extreme values of each activation
function over its subdomain are indicative of its non-linearity limits."
REFERENCES,0.474481658692185,"D
Affinity scores of other popular activation functions
491"
REFERENCES,0.47527910685805425,"Many works aimed to improve the way how the non-linearity – represented by activation functions –
492"
REFERENCES,0.47607655502392343,"can be defined in DNNs. As an example, a recent survey on the commonly used activation functions in
493"
REFERENCES,0.4768740031897927,"deep neural networks [40] identifies over 40 activation functions with first references to sigmoid dating
494"
REFERENCES,0.47767145135566186,"back to the seminal paper [36] published in late 80s. The fashion for activation functions used in deep
495"
REFERENCES,0.4784688995215311,"neural networks evolved over the years in a substantial way, just as the neural architectures themselves.
496"
REFERENCES,0.47926634768740034,"Saturating activations, such as sigmoid and hyperbolic tan, inspired by computational neuroscience
497"
REFERENCES,0.4800637958532695,"were a number one choice up until the arrival of rectifier linear unit (ReLU) in 2010. After being the
498"
REFERENCES,0.48086124401913877,"workhorse of many famous models over the years, the arrival of transformers popularized Gaussian
499"
REFERENCES,0.48165869218500795,"Error Linear Unit (GELU) which is now commonly used in many large language models including
500"
REFERENCES,0.4824561403508772,"GPTs.
501"
REFERENCES,0.48325358851674644,"We illustrate in Figure 6 the affinity scores obtained after a single pass of the data through the
502"
REFERENCES,0.4840510366826156,"following activation functions: Sigmoid, ReLU [51], GELU [52], ReLU6 [53], LeakyReLU [54]
503"
REFERENCES,0.48484848484848486,"with a default value of the slope, Tanh, HardTanh, SiLU [55], and HardSwish [56]. As the non-
504"
REFERENCES,0.48564593301435405,"linearity of activation functions depends on the domain of their input, we fix 20 points in their
505"
REFERENCES,0.4864433811802233,"domain equally spread in [−20, 20] interval. We use these points as means {mi}20
i=1 of Gaussian
506"
REFERENCES,0.48724082934609253,"distributions from which we sample 1000 points in R300 with standard deviation (std) σ taking values
507"
REFERENCES,0.4880382775119617,"in [2, 1, 0.5, 0.25, 0.1, 0.01]. Each sample denoted by Xσj
mi is then passed through the activation
508"
REFERENCES,0.48883572567783096,"function act ∈{sigmoid, ReLU, GELU} to obtain ρmi,σj
aff
:= ρaff(Xσj
mi, act(Xσj
mi)). Larger std
509"
REFERENCES,0.48963317384370014,"values make it more likely to draw samples that are closer to the region where the studied activation
510"
REFERENCES,0.4904306220095694,"functions become non-linear. We present the obtained results in Figure S2 where each of 20 boxplots
511"
REFERENCES,0.49122807017543857,"showcases median(ρmi,σ·
aff
) values with 50% confidence intervals and whiskers covering the whole
512"
REFERENCES,0.4920255183413078,"range of obtained values across all σj.
513"
REFERENCES,0.49282296650717705,"This plot allows us to derive several important conclusions. We observe that each activation function
514"
REFERENCES,0.49362041467304624,"can be characterized by 1) the lowest values of its non-linearity obtained for some subdomain of the
515"
REFERENCES,0.4944178628389155,"considered interval and 2) the width of the interval in which it maintains its non-linearity. We note
516"
REFERENCES,0.49521531100478466,"that in terms of 1) both GELU and ReLU may attain affinity scores that are close to 0, which is not
517"
REFERENCES,0.4960127591706539,"the case for Sigmoid. For 2), we observe that the non-linearity of Sigmoid and GELU is maintained
518"
REFERENCES,0.49681020733652315,"in a wide range, while for ReLU it is rather narrow. We can also see a distinct pattern of more
519"
REFERENCES,0.49760765550239233,"modern activation functions, such as SiLU and HardSwish having a stronger non-linearity pattern in
520"
REFERENCES,0.4984051036682616,"large subdomains. We also note that despite having a shape similar to Sigmoid, Tanh may allow for
521"
REFERENCES,0.49920255183413076,"much lower affinity scores. Finally, the variations of ReLU seem to have a very similar shape with
522"
REFERENCES,0.5,"LeakyReLU being on average more linear than ReLU and ReLU6.
523 ave. flat. sum  0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 aff"
REFERENCES,0.5007974481658692,gelu: 0.61±. 003
REFERENCES,0.5015948963317385,hardswish: 0.7±. 002
REFERENCES,0.5023923444976076,hardtanh: 0.78±. 002
REFERENCES,0.5031897926634769,"leakyrelu:0.63±. 003
relu, relu6: 0.62±(. 004, . 002)"
REFERENCES,0.5039872408293461,sigmoid: 0.91±. 001
REFERENCES,0.5047846889952153,silu: 0.68±. 002
REFERENCES,0.5055821371610846,tanh: 0.81±. 001 500 1500 2500 3500 4500 d 0.65 0.70 0.75 0.80 0.85 0.90 0.95 aff
REFERENCES,0.5063795853269537,gelu: 0.78
REFERENCES,0.507177033492823,"hardswish: 0.83
hardtanh: 0.88"
REFERENCES,0.5079744816586922,"leakyrelu:0.79
relu, relu6: 0.78"
REFERENCES,0.5087719298245614,sigmoid: 0.95
REFERENCES,0.5095693779904307,silu: 0.82
REFERENCES,0.5103668261562998,tanh: 0.9 500 1500 2500 3500 4500 d 0.60 0.65 0.70 0.75 0.80 0.85 0.90 aff
REFERENCES,0.511164274322169,gelu: 0.61
REFERENCES,0.5119617224880383,hardswish: 0.7
REFERENCES,0.5127591706539075,hardtanh: 0.78
REFERENCES,0.5135566188197768,"leakyrelu:0.63
relu, relu6: 0.62"
REFERENCES,0.5143540669856459,sigmoid: 0.91
REFERENCES,0.5151515151515151,silu: 0.67
REFERENCES,0.5159489633173844,tanh: 0.81
REFERENCES,0.5167464114832536,"Figure 7: (Top left) Affinity score is robust to the dimensionality reduction both when using averaging
and summation over the spatial dimensions; (Top right) When d > n, sample covariance matrix
estimation leads to a lack of robustness in the estimation of the affinity score; (Bottom) Shrinkage of
the covariance matrix leads to constant values of the affinity scores with increasing d."
REFERENCES,0.5175438596491229,"E
Implementation details
524"
REFERENCES,0.518341307814992,"Dimensionality reduction
Manipulating 4-order tensors is computationally prohibitive and thus
525"
REFERENCES,0.5191387559808612,"we need to find an appropriate lossless function r to facilitate this task. One possible choice for r
526"
REFERENCES,0.5199362041467305,"may be a vectorization operator that flattens each tensor into a vector. In practice, however, such
527"
REFERENCES,0.5207336523125997,"flattening still leads to very high-dimensional data representations. In our work, we propose to use
528"
REFERENCES,0.5215311004784688,"averaging over the spatial dimensions to get a suitable representation of the manipulated tensors. In
529"
REFERENCES,0.5223285486443381,"Figure 7 (left), we show that the affinity score is robust wrt such an averaging scheme and maintains
530"
REFERENCES,0.5231259968102073,"the same values as its flattened counterpart.
531"
REFERENCES,0.5239234449760766,"Computational considerations
The non-linearity signature requires calculating the affinity score
532"
REFERENCES,0.5247208931419458,"over “wide” matrices. Indeed, after the reduction step is applied to a batch of n tensors of size
533"
REFERENCES,0.5255183413078149,"h × w × c, we end up with matrices of size n × c where n may be much smaller than c. This is also
534"
REFERENCES,0.5263157894736842,"the case when input tensors are 2D when the batch size is smaller than the dimensionality of the
535"
REFERENCES,0.5271132376395534,"embedding space. To obtain a well-defined estimate of the covariance matrix in this case, we use a
536"
REFERENCES,0.5279106858054227,"known tool from the statistics literature called Ledoit-Wolfe shrinkage [57]. In Figure 7 (right), we
537"
REFERENCES,0.5287081339712919,"show that shrinkage allows us to obtain a stable estimate of the affinity scores that remain constant in
538"
REFERENCES,0.529505582137161,"all regimes.
539"
REFERENCES,0.5303030303030303,"Robustness to batch size and different seeds
In this section, we highlight the robustness of the
540"
REFERENCES,0.5311004784688995,"non-linearity signature with respect to the batch size and the random seed used for training. To this
541"
REFERENCES,0.5318979266347688,"end, we concentrate on VGG16 architecture and CIFAR10 dataset to avoid costly Imagenet retraining.
542"
REFERENCES,0.532695374800638,"In Figure 8, we present the obtained result where the batch size was varied between 128 and 1024
543"
REFERENCES,0.5334928229665071,"with an increment of 128 (left plot) and when VGG16 model was retrained with seeds varying from
544"
REFERENCES,0.5342902711323764,"1 to 9 (right plot). The obtained results show that the affinity score is robust to these parameters
545"
REFERENCES,0.5350877192982456,"suggesting that the obtained results are not subject to a strong stochasticity.
546"
REFERENCES,0.5358851674641149,"1
4
8
11
15
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5366826156299841,Batch size
REFERENCES,0.5374800637958532,"1024
128
256
512"
REFERENCES,0.5382775119617225,"1
4
8
11
15
Depth 0 0.2 0.4 0.6 0.8 1 aff Seed"
REFERENCES,0.5390749601275917,"1
2
3
4
5
6
8
9
7"
REFERENCES,0.539872408293461,"Figure 8: Non-linearity signature of VGG16 on CIFAR10 with a varying batch size (left) and when
retrained from 9 different random seeds (right)."
REFERENCES,0.5406698564593302,"1
4
8
11
15
Depth 0 0.2 0.4 0.6 0.8 1 aff Init"
REFERENCES,0.5414673046251993,"False
True"
REFERENCES,0.5422647527910686,"Figure 9: Non-linearity signatures of VGG16 on CIFAR10 in the beginning and end of training on
Imagenet."
REFERENCES,0.5430622009569378,"Impact of training
Finally, we also show how a non-linearity signature of a VGG16 model looks
547"
REFERENCES,0.543859649122807,"like at the beginning and in the end of training on Imagenet. We extract its non-linearity signature
548"
REFERENCES,0.5446570972886763,"at initialization when making a feedforward pass over the whole CIFAR10 dataset and compare it
549"
REFERENCES,0.5454545454545454,"to the non-linearity signature obtained in the end. In Figure 9, we can see that at initialization the
550"
REFERENCES,0.5462519936204147,"network’s non-linearity signature is increasing, reaching almost a perfectly linear pattern in the last
551"
REFERENCES,0.5470494417862839,"layers. Training the network enhances the non-linearity in a non-monotone way. Importantly, it also
552"
REFERENCES,0.5478468899521531,"highlights that the non-linearity signature is capturing information from the training process.
553"
REFERENCES,0.5486443381180224,"1
2
4
5
7
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5494417862838915,"Alexnet (ReLU, std=0.005)"
REFERENCES,0.5502392344497608,"1
4
8
11
15
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.55103668261563,"Vgg16 (ReLU, std=0.008)"
REFERENCES,0.5518341307814992,"0
10
20
30
40
50
60
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5526315789473685,"Inception v3 (ReLU, std=0.004)"
REFERENCES,0.5534290271132376,"0
25
50
75
100
125
150
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5542264752791068,"Resnet152 (ReLU, std=0.005)"
REFERENCES,0.5550239234449761,"0
25
50
75
100
125
150
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5558213716108453,"Densenet161 (ReLU, std=0.020)"
REFERENCES,0.5566188197767146,"0
50
100
150
Depth 0.0 0.2 0.4 0.6 0.8 1.0 1.2 aff"
REFERENCES,0.5574162679425837,Efficientnet b6 (std=0.008)
REFERENCES,0.5582137161084529,"SiLU
Squeeze (SiLU)
Excite (Sigmoid)"
REFERENCES,0.5590111642743222,"0
5
10
15
20
25
30
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5598086124401914,"Vit Huge 14x14 (GELU, std=0.013)"
REFERENCES,0.5606060606060606,"0
2
4
6
8
10
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5614035087719298,"Swin T (GELU, std=0.022)"
REFERENCES,0.562200956937799,"0
5
10
15
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5629984051036683,"Convnext (GELU, std=0.019)"
REFERENCES,0.5637958532695375,"Figure 10: Raw non-linearity signatures of popular DNN architectures, plotted as affinity scores over
the depth throughout the network."
REFERENCES,0.5645933014354066,"0
5
10
15
20
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5653907496012759,"Vit Large 16x16 (GELU, std=0.024)"
REFERENCES,0.5661881977671451,"0
5
10
15
20
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5669856459330144,"Vit Large 32x32 (GELU, std=0.014)"
REFERENCES,0.5677830940988836,"0
5
10
15
20
25
30
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5685805422647527,"Vit Huge 14x14 (GELU, std=0.013)"
REFERENCES,0.569377990430622,Figure 11: ViTs: Large ViT with 16x16 and 32x32 patch sizes and Huge ViT.
REFERENCES,0.5701754385964912,"F
Raw signatures
554"
REFERENCES,0.5709728867623605,"In Figure 10, we portray the raw non-linearity signatures of several representative networks studied
555"
REFERENCES,0.5717703349282297,"in the main paper. We use different color codes for distinct activation functions appearing repeatedly
556"
REFERENCES,0.5725677830940988,"in the considered architecture (for instance, every first ReLU in a residual block of a Resnet). We
557"
REFERENCES,0.5733652312599681,"also indicate the mean standard deviation of the affinity scores over batches in the title.
558"
REFERENCES,0.5741626794258373,"We see that the non-linearities across ReLU activations in all of Alexnet’s 8 layers remain stable. Its
559"
REFERENCES,0.5749601275917066,"successor, VGG network, reveals tiny, yet observable, variations in the non-linearity propagation with
560"
REFERENCES,0.5757575757575758,"increasing depth and, slightly lower overall non-linearity values. We attribute this to the decreased
561"
REFERENCES,0.5765550239234449,"size of the convolutional filters (3x3 vs. 7x7). The Googlenet architecture was the first model
562"
REFERENCES,0.5773524720893142,"to consider learning features at different scales in parallel within the so-called inception modules.
563"
REFERENCES,0.5781499202551834,"This add more variability as affinity scores of activation in Googlenet vary between 0.6 and 0.9.
564"
REFERENCES,0.5789473684210527,"Despite being almost 20 times smaller than VGG16, the accuracy of Googlenet on Imagenet remains
565"
REFERENCES,0.5797448165869219,"comparable, suggesting that increasing and varying the linearity is a way to have high accuracy with
566"
REFERENCES,0.580542264752791,"a limited computational complexity compared to predecessors. This finding is further confirmed with
567"
REFERENCES,0.5813397129186603,"Inception v3 that pushed the spread of the affinity score toward being more linear in some hidden
568"
REFERENCES,0.5821371610845295,"layers. When comparing this behavior with Alexnet, we note just how far we are from it. Resnets
569"
REFERENCES,0.5829346092503987,"achieve the same spread of values of the non-linearity but in a different, and arguably, simpler way.
570"
REFERENCES,0.583732057416268,"Indeed, the activation after the skip connection exhibits affinity scores close to 1, while the activations
571"
REFERENCES,0.5845295055821371,"in the hidden layers remain much lower. Densenet, that connect each layer to all previous layers and
572"
REFERENCES,0.5853269537480064,"1
3
5
7
10
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5861244019138756,"Vgg11 (ReLU, std=0.007)"
REFERENCES,0.5869218500797448,"1
3
6
9
12
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5877192982456141,"Vgg13 (ReLU, std=0.007)"
REFERENCES,0.5885167464114832,"1
4
8
11
15
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5893141945773525,"Vgg16 (ReLU, std=0.008)"
REFERENCES,0.5901116427432217,"1
5
9
13
18
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5909090909090909,"Vgg19 (ReLU, std=0.008)"
REFERENCES,0.5917065390749602,Figure 12: Impact of depth on the non-linearity signature of VGGs.
REFERENCES,0.5925039872408293,"5
10
15
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5933014354066986,"Resnet18 (ReLU, std=0.012)"
REFERENCES,0.5940988835725678,"0
10
20
30
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.594896331738437,"Resnet34 (ReLU, std=0.010)"
REFERENCES,0.5956937799043063,"0
10
20
30
40
50
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5964912280701754,"Resnet50 (ReLU, std=0.011)"
REFERENCES,0.5972886762360446,"0
20
40
60
80
100
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5980861244019139,"Resnet101 (ReLU, std=0.018)"
REFERENCES,0.5988835725677831,"0
25
50
75
100
125
150
Depth 0 0.2 0.4 0.6 0.8 1 aff"
REFERENCES,0.5996810207336523,"Resnet152 (ReLU, std=0.005)"
REFERENCES,0.6004784688995215,Figure 13: Impact of depth on the non-linearity signature of Resnets.
REFERENCES,0.6012759170653907,"not just to the one that precedes it, is slightly more non-linear than Resnet152, although the two bear
573"
REFERENCES,0.60207336523126,"a striking similarity: they both have an activation function that maintains the non-linearity low with
574"
REFERENCES,0.6028708133971292,"increasing depth. Additionally, transition layers in Densenet act as linearizers and allow it to reset the
575"
REFERENCES,0.6036682615629984,"non-linearity propagation in the network by reducing the feature map size. ViTs (Large with 16x16
576"
REFERENCES,0.6044657097288676,"and 32x32 patch sizes, and Huge with 14x14 patches) are all highly non-linear models to the degree
577"
REFERENCES,0.6052631578947368,"yet unseen. Interestingly, as seen in Figure 11 the patch size affects the non-linearity propagation
578"
REFERENCES,0.6060606060606061,"in a non-trivial way: for 16x16 size a model is more non-linear in the early layers, while gradually
579"
REFERENCES,0.6068580542264753,"becoming more and more linear later, while 32x32 patch size leads to a plateau in the hidden layers
580"
REFERENCES,0.6076555023923444,"of MLP blocks, with a steep change toward linearity only in the final layer. We hypothesize that
581"
REFERENCES,0.6084529505582137,"attention modules in ViT act as a focusing lens and output the embeddings in the domain where the
582"
REFERENCES,0.6092503987240829,"activation function is the most non-linear.
583"
REFERENCES,0.6100478468899522,"Finally, we explore the role of increasing depth for VGG and Resnet architectures. We consider
584"
REFERENCES,0.6108452950558214,"VGG11, VGG13, VGG16 and VGG19 models in the first case, and Resnet18, Resnet34, Resnet50,
585"
REFERENCES,0.6116427432216905,"Resnet101 and Resnet152. The results are presented in Figure 12 and Figure 13 for VGGs and
586"
REFERENCES,0.6124401913875598,"Resnets, respectively. Interestingly, VGGs do not change their non-linearity signature with increasing
587"
REFERENCES,0.613237639553429,"depth. In the case of Resnets, we can see that the separation between more linear post-residual
588"
REFERENCES,0.6140350877192983,"activations becomes more distinct and approaches 1 for deeper networks.
589"
REFERENCES,0.6148325358851675,"Table 2: Pearson correlations between the affinity score and other metrics, for all the architectures
evaluated in this study. We see that no other metric can reliably provide the same information as the
proposed non-linearity signature across different neural architectures."
REFERENCES,0.6156299840510366,"Model
CKA
Norm
Sparsity
Entropy
R2"
REFERENCES,0.6164274322169059,"alexnet
-0.75
-0.86
0.14
-0.80
-0.41
vgg11
-0.07
-0.76
-0.15
-0.95
-0.27
vgg13
0.08
-0.66
-0.23
-0.93
-0.26
vgg16
0.01
-0.63
-0.19
-0.88
-0.17
vgg19
-0.01
-0.62
-0.15
-0.86
-0.14
googlenet
0.74
-0.60
-0.83
-0.49
0.73
inception v3
0.69
-0.66
-0.75
-0.45
0.35
resnet18
0.59
-0.17
-0.67
-0.30
-0.44
resnet34
0.48
-0.18
-0.65
-0.19
-0.08
resnet50
0.56
-0.60
-0.71
-0.50
-0.78
resnet101
0.51
-0.57
-0.70
-0.51
-0.64
resnet152
0.52
-0.51
-0.68
-0.42
-0.48
densenet121
0.84
-0.75
-0.87
-0.62
0.82
densenet161
0.87
-0.74
-0.87
-0.67
0.81
densenet169
0.87
-0.74
-0.87
-0.67
0.81
densenet201
0.89
-0.75
-0.91
-0.67
0.90
efficientnet b1
0.35
-0.41
-0.39
0.01
0.03
efficientnet b2
0.49
-0.02
-0.44
-0.06
0.34
efficientnet b3
0.32
-0.12
-0.18
-0.13
0.18
efficientnet b4
0.30
-0.51
-0.29
-0.44
0.11
vit b 32
0.47
-0.31
-0.29
0.39
0.51
vit l 32
-0.14
-0.61
-0.47
-0.02
-0.06
vit b 16
-0.27
-0.71
0.04
0.39
-0.22
vit l 16
-0.39
-0.89
-0.66
-0.23
-0.24
vit h 14
-0.77
-0.83
0.92
0.31
-0.49
swin t
-0.12
-0.39
-0.02
-0.42
-0.06
swin s
-0.003
-0.61
-0.31
0.18
-0.03
swin b
-0.32
-0.59
-0.43
0.42
-0.32
convnext tiny
0.77
-0.01
-0.04
0.09
0.80
convnext small
0.57
0.22
0.25
0.13
0.72
convnext base
0.67
0.41
0.35
-0.03
0.82
convnext large
0.75
0.23
0.35
-0.10
0.84
Average
0.31 ± 0.45
-0.44 ± 0.35
-0.31 ± 0.43
-0.29 ± 0.39
0.13 ± 0.50"
REFERENCES,0.6172248803827751,"G
Detailed comparisons between architectures
590"
REFERENCES,0.6180223285486444,"We consider the following metrics as 1) the linear CKA [38] commonly used to assess the similarity
591"
REFERENCES,0.6188197767145136,"of neural representations, the average change in 2) SPARSITY and 3) ENTROPY before and after the
592"
REFERENCES,0.6196172248803827,"application of the activation function as well as the 4) Frobenius NORM between the input and output
593"
REFERENCES,0.620414673046252,"of the activation functions, and the 5) R2 score between the linear model fitted on the input and the
594"
REFERENCES,0.6212121212121212,"output of the activation function. We present in Table 2, the detailed values of Pearson correlations
595"
REFERENCES,0.6220095693779905,"obtained for each architecture and all the metrics considered in this study. In Figure 14, we show the
596"
REFERENCES,0.6228070175438597,"full matrix of pairwise DTW distances [50] obtained between architectures, then used to obtain the
597"
REFERENCES,0.6236044657097288,"clustering presented in the main text.
598"
REFERENCES,0.6244019138755981,"alexnet
convnext base"
REFERENCES,0.6251993620414673,convnext large
REFERENCES,0.6259968102073366,convnext small
REFERENCES,0.6267942583732058,convnext tiny
REFERENCES,0.6275917065390749,densenet121
REFERENCES,0.6283891547049442,densenet161
REFERENCES,0.6291866028708134,densenet169
REFERENCES,0.6299840510366826,densenet201
REFERENCES,0.6307814992025519,efficientnet b1
REFERENCES,0.631578947368421,efficientnet b2
REFERENCES,0.6323763955342903,efficientnet b3
REFERENCES,0.6331738437001595,efficientnet b4
REFERENCES,0.6339712918660287,googlenet
REFERENCES,0.6347687400318979,inception v3
REFERENCES,0.6355661881977671,mnasnet0 5
REFERENCES,0.6363636363636364,mnasnet0 75
REFERENCES,0.6371610845295056,mnasnet1 0
REFERENCES,0.6379585326953748,mnasnet1 3
REFERENCES,0.638755980861244,resnet101
REFERENCES,0.6395534290271132,resnet152
REFERENCES,0.6403508771929824,resnet18
REFERENCES,0.6411483253588517,resnet34
REFERENCES,0.6419457735247209,resnet50
REFERENCES,0.6427432216905901,swin b
REFERENCES,0.6435406698564593,swin s
REFERENCES,0.6443381180223285,swin t vgg11 vgg13 vgg16 vgg19
REFERENCES,0.6451355661881978,vit b 16
REFERENCES,0.645933014354067,vit b 32
REFERENCES,0.6467304625199362,vit h 14
REFERENCES,0.6475279106858054,vit l 16
REFERENCES,0.6483253588516746,vit l 32
REFERENCES,0.6491228070175439,"alexnet
convnext base
convnext large
convnext small"
REFERENCES,0.6499202551834131,convnext tiny
REFERENCES,0.6507177033492823,"densenet121
densenet161
densenet169
densenet201
efficientnet b1
efficientnet b2
efficientnet b3
efficientnet b4"
REFERENCES,0.6515151515151515,"googlenet
inception v3"
REFERENCES,0.6523125996810207,"mnasnet0 5
mnasnet0 75"
REFERENCES,0.65311004784689,"mnasnet1 0
mnasnet1 3"
REFERENCES,0.6539074960127592,"resnet101
resnet152"
REFERENCES,0.6547049441786283,"resnet18
resnet34
resnet50"
REFERENCES,0.6555023923444976,swin b
REFERENCES,0.6562998405103668,swin s
REFERENCES,0.6570972886762361,"swin t
vgg11
vgg13
vgg16
vgg19
vit b 16
vit b 32
vit h 14"
REFERENCES,0.6578947368421053,"vit l 16
vit l 32
0.0 0.5 1.0 1.5 2.0 2.5 3.0"
REFERENCES,0.6586921850079744,Figure 14: Full matrix of DTW distances between non-linearity signatures.
REFERENCES,0.6594896331738437,alexnet
REFERENCES,0.6602870813397129,convnext base
REFERENCES,0.6610845295055822,convnext large
REFERENCES,0.6618819776714514,convnext small
REFERENCES,0.6626794258373205,convnext tiny
REFERENCES,0.6634768740031898,densenet121
REFERENCES,0.664274322169059,densenet161
REFERENCES,0.6650717703349283,densenet169
REFERENCES,0.6658692185007975,densenet201
REFERENCES,0.6666666666666666,maxvit t
REFERENCES,0.6674641148325359,resnet101
REFERENCES,0.6682615629984051,resnet152
REFERENCES,0.6690590111642744,resnet18
REFERENCES,0.6698564593301436,resnet34
REFERENCES,0.6706539074960127,resnet50
REFERENCES,0.671451355661882,shufflenet v2 x0 5
REFERENCES,0.6722488038277512,shufflenet v2 x1 0
REFERENCES,0.6730462519936204,shufflenet v2 x1 5
REFERENCES,0.6738437001594896,shufflenet v2 x2 0
REFERENCES,0.6746411483253588,swin b
REFERENCES,0.6754385964912281,swin s
REFERENCES,0.6762360446570973,swin t vgg11 vgg13 vgg16 vgg19
REFERENCES,0.6770334928229665,vit b 16
REFERENCES,0.6778309409888357,vit b 32
REFERENCES,0.6786283891547049,vit h 14
REFERENCES,0.6794258373205742,vit l 16
REFERENCES,0.6802232854864434,vit l 32
REFERENCES,0.6810207336523126,wide resnet101 2
REFERENCES,0.6818181818181818,wide resnet50 2 0.0 0.5 1.0 1.5
REFERENCES,0.682615629984051,"CIFAR10: 0.28±0.11
CIFAR100: 0.26±0.11
Random: 0.77±0.32"
REFERENCES,0.6834130781499203,"Figure 15: Deviation in terms of the Euclidean distance of the non-linearity signature obtained on
CIFAR10, CIFAR100, and Random datasets from the non-linearity signature of the Imagenet dataset."
REFERENCES,0.6842105263157895,"H
Results on more datasets
599"
REFERENCES,0.6850079744816587,"Below, we compare the results obtained on CIFAR10, CIFAR100 datasets as well as when the random
600"
REFERENCES,0.6858054226475279,"data tensors are passed through the network. As the number of plots for all chosen 33 models on
601"
REFERENCES,0.6866028708133971,"these datasets will not allow for a meaningful visual analysis, we rather plot the differences – in terms
602"
REFERENCES,0.6874003189792663,"of the DTW distance – between the non-linearity signature of the model on Imagenet dataset with
603"
REFERENCES,0.6881977671451356,"respect to three other datasets. We present the obtained results in Figure 15.
604"
REFERENCES,0.6889952153110048,"We can see that the overall deviation for CIFAR10 and CIFAR100 remains lower than for Random
605"
REFERENCES,0.689792663476874,"dataset suggesting that these datasets are semantically closer to Imagenet.
606"
REFERENCES,0.6905901116427432,resnet50 swav 800
REFERENCES,0.6913875598086124,resnet50 dino
REFERENCES,0.6921850079744817,resnet50 swav 400
REFERENCES,0.6929824561403509,resnet50 moco 800
REFERENCES,0.69377990430622,resnet50 moco 200
REFERENCES,0.6945773524720893,resnet50 swav 200
REFERENCES,0.6953748006379585,resnet50 trexstar
REFERENCES,0.6961722488038278,resnet50 trex
REFERENCES,0.696969696969697,wide resnet50 2
REFERENCES,0.6977671451355661,resnet50
REFERENCES,0.6985645933014354,resnet50 sce 300
REFERENCES,0.6993620414673046,resnet50 sce 200
REFERENCES,0.7001594896331739,resnet50 sce 1000
REFERENCES,0.7009569377990431,resnet50 sce 100
REFERENCES,0.7017543859649122,resnet50 triplet 200
REFERENCES,0.7025518341307815,resnet50 triplet 1k
REFERENCES,0.7033492822966507,resnet50 ressl 200 0.00 0.05 0.10
REFERENCES,0.70414673046252,Distance
REFERENCES,0.7049441786283892,"Figure 16: Hierarchical clustering of supervised and self-supervised pre-trained Resnet50 using the
DTW distances between their non-linearity signatures."
REFERENCES,0.7057416267942583,"Table 3: Robustness of the different criteria when considering the same architectures pre-trained for
different tasks. Affinity score achieves the lowest standard deviation suggesting that it is capable of
correctly identifying the architecture even when it was trained differently."
REFERENCES,0.7065390749601276,"Criterion
Mean ± std
ρaff
0.76±0.04
Linear CKA
0.90±0.07
Norm
448.56±404.61
Sparsity
0.56±0.16
Entropy
0.39±0.46"
REFERENCES,0.7073365231259968,"I
Results for self-supervised methods
607"
REFERENCES,0.7081339712918661,"In this section, we show that the non-linearity signature of a network remains almost unchanged
608"
REFERENCES,0.7089314194577353,"when considering other pertaining methodologies such as for instance, self-supervised ones. To this
609"
REFERENCES,0.7097288676236044,"end, we use 17 Resnet50 architecture pre-trained on Imagenet within the next 3 families of learning
610"
REFERENCES,0.7105263157894737,"approaches:
611"
REFERENCES,0.7113237639553429,"1. SwAV [58], DINO [59], and MoCo [60] that belong to the family of contrastive learning
612"
REFERENCES,0.7121212121212122,"methods with prototypes;
613"
REFERENCES,0.7129186602870813,"2. Resnet50 [18], Wide Resnet50 [61], TRex, and TRex* [62] that are supervised learning
614"
REFERENCES,0.7137161084529505,"approaches;
615"
REFERENCES,0.7145135566188198,"3. SCE [63], Truncated Triplet [64], and ReSSL [65] that perform contrastive learning using
616"
REFERENCES,0.715311004784689,"relational information.
617"
REFERENCES,0.7161084529505582,"From the dendrogram presented in Figure 16, we can observe that the DTW distances between the
618"
REFERENCES,0.7169059011164274,"non-linearity signatures of all the learning methodologies described above allow us to correctly cluster
619"
REFERENCES,0.7177033492822966,"them into meaningful groups. This is rather striking as the DTW distances between the different
620"
REFERENCES,0.7185007974481659,"instances of the Resnet50 model are rather small in magnitude suggesting that the affinity scores still
621"
REFERENCES,0.7192982456140351,"retain the fact that it is the same model being trained in many different ways.
622"
REFERENCES,0.7200956937799043,"While providing a fine-grained clustering of different pre-trained models for a given fixed architecture,
623"
REFERENCES,0.7208931419457735,"the average affinity scores over batches remain surprisingly concentrated as shown in Table 3. This
624"
REFERENCES,0.7216905901116427,"hints at the fact that the non-linearity signature is characteristic of architecture but can also be subtly
625"
REFERENCES,0.722488038277512,"multi-faceted when it comes to its different variations.
626"
REFERENCES,0.7232854864433812,resnet50
REFERENCES,0.7240829346092504,resnet50 dino
REFERENCES,0.7248803827751196,resnet50 moco 200
REFERENCES,0.7256778309409888,resnet50 moco 800
REFERENCES,0.726475279106858,resnet50 ressl 200
REFERENCES,0.7272727272727273,resnet50 sce 100
REFERENCES,0.7280701754385965,resnet50 sce 1000
REFERENCES,0.7288676236044657,resnet50 sce 200
REFERENCES,0.7296650717703349,resnet50 sce 300
REFERENCES,0.7304625199362041,resnet50 swav 200
REFERENCES,0.7312599681020734,resnet50 swav 400
REFERENCES,0.7320574162679426,resnet50 swav 800
REFERENCES,0.7328548644338118,resnet50 trex
REFERENCES,0.733652312599681,resnet50 trexstar
REFERENCES,0.7344497607655502,resnet50 triplet 1k
REFERENCES,0.7352472089314195,resnet50 triplet 200
REFERENCES,0.7360446570972887,wide resnet50 2
REFERENCES,0.7368421052631579,resnet50
REFERENCES,0.7376395534290271,resnet50 dino
REFERENCES,0.7384370015948963,resnet50 moco 200
REFERENCES,0.7392344497607656,resnet50 moco 800
REFERENCES,0.7400318979266348,resnet50 ressl 200
REFERENCES,0.740829346092504,resnet50 sce 100
REFERENCES,0.7416267942583732,resnet50 sce 1000
REFERENCES,0.7424242424242424,resnet50 sce 200
REFERENCES,0.7432216905901117,resnet50 sce 300
REFERENCES,0.7440191387559809,resnet50 swav 200
REFERENCES,0.74481658692185,resnet50 swav 400
REFERENCES,0.7456140350877193,resnet50 swav 800
REFERENCES,0.7464114832535885,resnet50 trex
REFERENCES,0.7472089314194578,resnet50 trexstar
REFERENCES,0.748006379585327,resnet50 triplet 1k
REFERENCES,0.7488038277511961,resnet50 triplet 200
REFERENCES,0.7496012759170654,wide resnet50 2 0.00 0.05 0.10 0.15 0.20 0.25
REFERENCES,0.7503987240829346,"Figure 17: DTW distances associated with the clustering presented in Figure 16. We can see distinct
clusters as revealed by the dendrogram."
REFERENCES,0.7511961722488039,"NeurIPS Paper Checklist
627"
CLAIMS,0.751993620414673,"1. Claims
628"
CLAIMS,0.7527910685805422,"Question: Do the main claims made in the abstract and introduction accurately reflect the
629"
CLAIMS,0.7535885167464115,"paper’s contributions and scope?
630"
CLAIMS,0.7543859649122807,"Answer: [Yes]
631"
CLAIMS,0.75518341307815,"Justification: Proposition of affinity score and non-linearity signature in Section 3. Experi-
632"
CLAIMS,0.7559808612440191,"ments showing non-linearity signatures of DNNs, prediction of performance, clustering and
633"
CLAIMS,0.7567783094098883,"uniqueness in Section 4.
634"
CLAIMS,0.7575757575757576,"Guidelines:
635"
CLAIMS,0.7583732057416268,"• The answer NA means that the abstract and introduction do not include the claims
636"
CLAIMS,0.759170653907496,"made in the paper.
637"
CLAIMS,0.7599681020733652,"• The abstract and/or introduction should clearly state the claims made, including the
638"
CLAIMS,0.7607655502392344,"contributions made in the paper and important assumptions and limitations. A No or
639"
CLAIMS,0.7615629984051037,"NA answer to this question will not be perceived well by the reviewers.
640"
CLAIMS,0.7623604465709729,"• The claims made should match theoretical and experimental results, and reflect how
641"
CLAIMS,0.7631578947368421,"much the results can be expected to generalize to other settings.
642"
CLAIMS,0.7639553429027113,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
643"
CLAIMS,0.7647527910685805,"are not attained by the paper.
644"
LIMITATIONS,0.7655502392344498,"2. Limitations
645"
LIMITATIONS,0.766347687400319,"Question: Does the paper discuss the limitations of the work performed by the authors?
646"
LIMITATIONS,0.7671451355661882,"Answer: [Yes]
647"
LIMITATIONS,0.7679425837320574,"Justification: We discuss limitations in Appendix B.
648"
LIMITATIONS,0.7687400318979266,"Guidelines:
649"
LIMITATIONS,0.7695374800637959,"• The answer NA means that the paper has no limitation while the answer No means that
650"
LIMITATIONS,0.7703349282296651,"the paper has limitations, but those are not discussed in the paper.
651"
LIMITATIONS,0.7711323763955343,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
652"
LIMITATIONS,0.7719298245614035,"• The paper should point out any strong assumptions and how robust the results are to
653"
LIMITATIONS,0.7727272727272727,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
654"
LIMITATIONS,0.773524720893142,"model well-specification, asymptotic approximations only holding locally). The authors
655"
LIMITATIONS,0.7743221690590112,"should reflect on how these assumptions might be violated in practice and what the
656"
LIMITATIONS,0.7751196172248804,"implications would be.
657"
LIMITATIONS,0.7759170653907496,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
658"
LIMITATIONS,0.7767145135566188,"only tested on a few datasets or with a few runs. In general, empirical results often
659"
LIMITATIONS,0.777511961722488,"depend on implicit assumptions, which should be articulated.
660"
LIMITATIONS,0.7783094098883573,"• The authors should reflect on the factors that influence the performance of the approach.
661"
LIMITATIONS,0.7791068580542265,"For example, a facial recognition algorithm may perform poorly when image resolution
662"
LIMITATIONS,0.7799043062200957,"is low or images are taken in low lighting. Or a speech-to-text system might not be
663"
LIMITATIONS,0.7807017543859649,"used reliably to provide closed captions for online lectures because it fails to handle
664"
LIMITATIONS,0.7814992025518341,"technical jargon.
665"
LIMITATIONS,0.7822966507177034,"• The authors should discuss the computational efficiency of the proposed algorithms
666"
LIMITATIONS,0.7830940988835726,"and how they scale with dataset size.
667"
LIMITATIONS,0.7838915470494418,"• If applicable, the authors should discuss possible limitations of their approach to
668"
LIMITATIONS,0.784688995215311,"address problems of privacy and fairness.
669"
LIMITATIONS,0.7854864433811802,"• While the authors might fear that complete honesty about limitations might be used by
670"
LIMITATIONS,0.7862838915470495,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
671"
LIMITATIONS,0.7870813397129187,"limitations that aren’t acknowledged in the paper. The authors should use their best
672"
LIMITATIONS,0.7878787878787878,"judgment and recognize that individual actions in favor of transparency play an impor-
673"
LIMITATIONS,0.7886762360446571,"tant role in developing norms that preserve the integrity of the community. Reviewers
674"
LIMITATIONS,0.7894736842105263,"will be specifically instructed to not penalize honesty concerning limitations.
675"
THEORY ASSUMPTIONS AND PROOFS,0.7902711323763956,"3. Theory Assumptions and Proofs
676"
THEORY ASSUMPTIONS AND PROOFS,0.7910685805422647,"Question: For each theoretical result, does the paper provide the full set of assumptions and
677"
THEORY ASSUMPTIONS AND PROOFS,0.7918660287081339,"a complete (and correct) proof?
678"
THEORY ASSUMPTIONS AND PROOFS,0.7926634768740032,"Answer: [Yes]
679"
THEORY ASSUMPTIONS AND PROOFS,0.7934609250398724,"Justification: Full proofs in Appendix C.
680"
THEORY ASSUMPTIONS AND PROOFS,0.7942583732057417,"Guidelines:
681"
THEORY ASSUMPTIONS AND PROOFS,0.7950558213716108,"• The answer NA means that the paper does not include theoretical results.
682"
THEORY ASSUMPTIONS AND PROOFS,0.79585326953748,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
683"
THEORY ASSUMPTIONS AND PROOFS,0.7966507177033493,"referenced.
684"
THEORY ASSUMPTIONS AND PROOFS,0.7974481658692185,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
685"
THEORY ASSUMPTIONS AND PROOFS,0.7982456140350878,"• The proofs can either appear in the main paper or the supplemental material, but if
686"
THEORY ASSUMPTIONS AND PROOFS,0.7990430622009569,"they appear in the supplemental material, the authors are encouraged to provide a short
687"
THEORY ASSUMPTIONS AND PROOFS,0.7998405103668261,"proof sketch to provide intuition.
688"
THEORY ASSUMPTIONS AND PROOFS,0.8006379585326954,"• Inversely, any informal proof provided in the core of the paper should be complemented
689"
THEORY ASSUMPTIONS AND PROOFS,0.8014354066985646,"by formal proofs provided in appendix or supplemental material.
690"
THEORY ASSUMPTIONS AND PROOFS,0.8022328548644339,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
691"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.803030303030303,"4. Experimental Result Reproducibility
692"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8038277511961722,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
693"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8046251993620415,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8054226475279107,"of the paper (regardless of whether the code and data are provided or not)?
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.80622009569378,"Answer: [Yes]
696"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8070175438596491,"Justification: All models are pretrained checkpoints from torchvision. Experiments are
697"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8078149920255183,"conducted on Imagenet, publicly available.
698"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8086124401913876,"Guidelines:
699"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8094098883572568,"• The answer NA means that the paper does not include experiments.
700"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.810207336523126,"• If the paper includes experiments, a No answer to this question will not be perceived
701"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8110047846889952,"well by the reviewers: Making the paper reproducible is important, regardless of
702"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8118022328548644,"whether the code and data are provided or not.
703"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8125996810207337,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
704"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8133971291866029,"to make their results reproducible or verifiable.
705"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8141945773524721,"• Depending on the contribution, reproducibility can be accomplished in various ways.
706"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8149920255183413,"For example, if the contribution is a novel architecture, describing the architecture fully
707"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8157894736842105,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
708"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8165869218500797,"be necessary to either make it possible for others to replicate the model with the same
709"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.817384370015949,"dataset, or provide access to the model. In general. releasing code and data is often
710"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8181818181818182,"one good way to accomplish this, but reproducibility can also be provided via detailed
711"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8189792663476874,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
712"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8197767145135566,"of a large language model), releasing of a model checkpoint, or other means that are
713"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8205741626794258,"appropriate to the research performed.
714"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8213716108452951,"• While NeurIPS does not require releasing code, the conference does require all submis-
715"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8221690590111643,"sions to provide some reasonable avenue for reproducibility, which may depend on the
716"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8229665071770335,"nature of the contribution. For example
717"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8237639553429027,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
718"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8245614035087719,"to reproduce that algorithm.
719"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8253588516746412,"(b) If the contribution is primarily a new model architecture, the paper should describe
720"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8261562998405104,"the architecture clearly and fully.
721"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8269537480063796,"(c) If the contribution is a new model (e.g., a large language model), then there should
722"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8277511961722488,"either be a way to access this model for reproducing the results or a way to reproduce
723"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.828548644338118,"the model (e.g., with an open-source dataset or instructions for how to construct
724"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8293460925039873,"the dataset).
725"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8301435406698564,"(d) We recognize that reproducibility may be tricky in some cases, in which case
726"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8309409888357256,"authors are welcome to describe the particular way they provide for reproducibility.
727"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8317384370015949,"In the case of closed-source models, it may be that access to the model is limited in
728"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8325358851674641,"some way (e.g., to registered users), but it should be possible for other researchers
729"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8333333333333334,"to have some path to reproducing or verifying the results.
730"
OPEN ACCESS TO DATA AND CODE,0.8341307814992025,"5. Open access to data and code
731"
OPEN ACCESS TO DATA AND CODE,0.8349282296650717,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
732"
OPEN ACCESS TO DATA AND CODE,0.835725677830941,"tions to faithfully reproduce the main experimental results, as described in supplemental
733"
OPEN ACCESS TO DATA AND CODE,0.8365231259968102,"material?
734"
OPEN ACCESS TO DATA AND CODE,0.8373205741626795,"Answer: [Yes]
735"
OPEN ACCESS TO DATA AND CODE,0.8381180223285486,"Justification: Anonymized code to reproduce experiments is available as a zip file, with a
736"
OPEN ACCESS TO DATA AND CODE,0.8389154704944178,"README file to explain how to run it.
737"
OPEN ACCESS TO DATA AND CODE,0.8397129186602871,"Guidelines:
738"
OPEN ACCESS TO DATA AND CODE,0.8405103668261563,"• The answer NA means that paper does not include experiments requiring code.
739"
OPEN ACCESS TO DATA AND CODE,0.8413078149920256,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
740"
OPEN ACCESS TO DATA AND CODE,0.8421052631578947,"public/guides/CodeSubmissionPolicy) for more details.
741"
OPEN ACCESS TO DATA AND CODE,0.8429027113237639,"• While we encourage the release of code and data, we understand that this might not be
742"
OPEN ACCESS TO DATA AND CODE,0.8437001594896332,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
743"
OPEN ACCESS TO DATA AND CODE,0.8444976076555024,"including code, unless this is central to the contribution (e.g., for a new open-source
744"
OPEN ACCESS TO DATA AND CODE,0.8452950558213717,"benchmark).
745"
OPEN ACCESS TO DATA AND CODE,0.8460925039872408,"• The instructions should contain the exact command and environment needed to run to
746"
OPEN ACCESS TO DATA AND CODE,0.84688995215311,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
747"
OPEN ACCESS TO DATA AND CODE,0.8476874003189793,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
748"
OPEN ACCESS TO DATA AND CODE,0.8484848484848485,"• The authors should provide instructions on data access and preparation, including how
749"
OPEN ACCESS TO DATA AND CODE,0.8492822966507177,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
750"
OPEN ACCESS TO DATA AND CODE,0.8500797448165869,"• The authors should provide scripts to reproduce all experimental results for the new
751"
OPEN ACCESS TO DATA AND CODE,0.8508771929824561,"proposed method and baselines. If only a subset of experiments are reproducible, they
752"
OPEN ACCESS TO DATA AND CODE,0.8516746411483254,"should state which ones are omitted from the script and why.
753"
OPEN ACCESS TO DATA AND CODE,0.8524720893141946,"• At submission time, to preserve anonymity, the authors should release anonymized
754"
OPEN ACCESS TO DATA AND CODE,0.8532695374800638,"versions (if applicable).
755"
OPEN ACCESS TO DATA AND CODE,0.854066985645933,"• Providing as much information as possible in supplemental material (appended to the
756"
OPEN ACCESS TO DATA AND CODE,0.8548644338118022,"paper) is recommended, but including URLs to data and code is permitted.
757"
OPEN ACCESS TO DATA AND CODE,0.8556618819776715,"6. Experimental Setting/Details
758"
OPEN ACCESS TO DATA AND CODE,0.8564593301435407,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
759"
OPEN ACCESS TO DATA AND CODE,0.8572567783094099,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
760"
OPEN ACCESS TO DATA AND CODE,0.8580542264752791,"results?
761"
OPEN ACCESS TO DATA AND CODE,0.8588516746411483,"Answer: [Yes]
762"
OPEN ACCESS TO DATA AND CODE,0.8596491228070176,"Justification: Experimental details are described in Section 4 and Appendix E.
763"
OPEN ACCESS TO DATA AND CODE,0.8604465709728868,"Guidelines:
764"
OPEN ACCESS TO DATA AND CODE,0.861244019138756,"• The answer NA means that the paper does not include experiments.
765"
OPEN ACCESS TO DATA AND CODE,0.8620414673046252,"• The experimental setting should be presented in the core of the paper to a level of detail
766"
OPEN ACCESS TO DATA AND CODE,0.8628389154704944,"that is necessary to appreciate the results and make sense of them.
767"
OPEN ACCESS TO DATA AND CODE,0.8636363636363636,"• The full details can be provided either with the code, in appendix, or as supplemental
768"
OPEN ACCESS TO DATA AND CODE,0.8644338118022329,"material.
769"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8652312599681021,"7. Experiment Statistical Significance
770"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8660287081339713,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
771"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8668261562998405,"information about the statistical significance of the experiments?
772"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8676236044657097,"Answer: [Yes]
773"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.868421052631579,"Justification: Standard deviations across multiple batch of data are reported.
774"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8692185007974481,"Guidelines:
775"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8700159489633174,"• The answer NA means that the paper does not include experiments.
776"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8708133971291866,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
777"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8716108452950558,"dence intervals, or statistical significance tests, at least for the experiments that support
778"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8724082934609251,"the main claims of the paper.
779"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8732057416267942,"• The factors of variability that the error bars are capturing should be clearly stated (for
780"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8740031897926634,"example, train/test split, initialization, random drawing of some parameter, or overall
781"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8748006379585327,"run with given experimental conditions).
782"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8755980861244019,"• The method for calculating the error bars should be explained (closed form formula,
783"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8763955342902712,"call to a library function, bootstrap, etc.)
784"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8771929824561403,"• The assumptions made should be given (e.g., Normally distributed errors).
785"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8779904306220095,"• It should be clear whether the error bar is the standard deviation or the standard error
786"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8787878787878788,"of the mean.
787"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.879585326953748,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
788"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8803827751196173,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
789"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8811802232854864,"of Normality of errors is not verified.
790"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8819776714513556,"• For asymmetric distributions, the authors should be careful not to show in tables or
791"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8827751196172249,"figures symmetric error bars that would yield results that are out of range (e.g. negative
792"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8835725677830941,"error rates).
793"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8843700159489634,"• If error bars are reported in tables or plots, The authors should explain in the text how
794"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8851674641148325,"they were calculated and reference the corresponding figures or tables in the text.
795"
EXPERIMENTS COMPUTE RESOURCES,0.8859649122807017,"8. Experiments Compute Resources
796"
EXPERIMENTS COMPUTE RESOURCES,0.886762360446571,"Question: For each experiment, does the paper provide sufficient information on the com-
797"
EXPERIMENTS COMPUTE RESOURCES,0.8875598086124402,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
798"
EXPERIMENTS COMPUTE RESOURCES,0.8883572567783095,"the experiments?
799"
EXPERIMENTS COMPUTE RESOURCES,0.8891547049441786,"Answer: [Yes]
800"
EXPERIMENTS COMPUTE RESOURCES,0.8899521531100478,"Justification: All experiments are carried out on a single A100 GPU.
801"
EXPERIMENTS COMPUTE RESOURCES,0.8907496012759171,"Guidelines:
802"
EXPERIMENTS COMPUTE RESOURCES,0.8915470494417863,"• The answer NA means that the paper does not include experiments.
803"
EXPERIMENTS COMPUTE RESOURCES,0.8923444976076556,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
804"
EXPERIMENTS COMPUTE RESOURCES,0.8931419457735247,"or cloud provider, including relevant memory and storage.
805"
EXPERIMENTS COMPUTE RESOURCES,0.8939393939393939,"• The paper should provide the amount of compute required for each of the individual
806"
EXPERIMENTS COMPUTE RESOURCES,0.8947368421052632,"experimental runs as well as estimate the total compute.
807"
EXPERIMENTS COMPUTE RESOURCES,0.8955342902711324,"• The paper should disclose whether the full research project required more compute
808"
EXPERIMENTS COMPUTE RESOURCES,0.8963317384370016,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
809"
EXPERIMENTS COMPUTE RESOURCES,0.8971291866028708,"didn’t make it into the paper).
810"
CODE OF ETHICS,0.89792663476874,"9. Code Of Ethics
811"
CODE OF ETHICS,0.8987240829346093,"Question: Does the research conducted in the paper conform, in every respect, with the
812"
CODE OF ETHICS,0.8995215311004785,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
813"
CODE OF ETHICS,0.9003189792663477,"Answer: [Yes]
814"
CODE OF ETHICS,0.9011164274322169,"Justification: Standard and public datasets used, no experiments on human subjects.
815"
CODE OF ETHICS,0.9019138755980861,"Guidelines:
816"
CODE OF ETHICS,0.9027113237639554,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
817"
CODE OF ETHICS,0.9035087719298246,"• If the authors answer No, they should explain the special circumstances that require a
818"
CODE OF ETHICS,0.9043062200956937,"deviation from the Code of Ethics.
819"
CODE OF ETHICS,0.905103668261563,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
820"
CODE OF ETHICS,0.9059011164274322,"eration due to laws or regulations in their jurisdiction).
821"
BROADER IMPACTS,0.9066985645933014,"10. Broader Impacts
822"
BROADER IMPACTS,0.9074960127591707,"Question: Does the paper discuss both potential positive societal impacts and negative
823"
BROADER IMPACTS,0.9082934609250398,"societal impacts of the work performed?
824"
BROADER IMPACTS,0.9090909090909091,"Answer: [Yes]
825"
BROADER IMPACTS,0.9098883572567783,"Justification: We discuss broader impacts in Appendix A.
826"
BROADER IMPACTS,0.9106858054226475,"Guidelines:
827"
BROADER IMPACTS,0.9114832535885168,"• The answer NA means that there is no societal impact of the work performed.
828"
BROADER IMPACTS,0.9122807017543859,"• If the authors answer NA or No, they should explain why their work has no societal
829"
BROADER IMPACTS,0.9130781499202552,"impact or why the paper does not address societal impact.
830"
BROADER IMPACTS,0.9138755980861244,"• Examples of negative societal impacts include potential malicious or unintended uses
831"
BROADER IMPACTS,0.9146730462519936,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
832"
BROADER IMPACTS,0.9154704944178629,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
833"
BROADER IMPACTS,0.916267942583732,"groups), privacy considerations, and security considerations.
834"
BROADER IMPACTS,0.9170653907496013,"• The conference expects that many papers will be foundational research and not tied
835"
BROADER IMPACTS,0.9178628389154705,"to particular applications, let alone deployments. However, if there is a direct path to
836"
BROADER IMPACTS,0.9186602870813397,"any negative applications, the authors should point it out. For example, it is legitimate
837"
BROADER IMPACTS,0.919457735247209,"to point out that an improvement in the quality of generative models could be used to
838"
BROADER IMPACTS,0.9202551834130781,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
839"
BROADER IMPACTS,0.9210526315789473,"that a generic algorithm for optimizing neural networks could enable people to train
840"
BROADER IMPACTS,0.9218500797448166,"models that generate Deepfakes faster.
841"
BROADER IMPACTS,0.9226475279106858,"• The authors should consider possible harms that could arise when the technology is
842"
BROADER IMPACTS,0.9234449760765551,"being used as intended and functioning correctly, harms that could arise when the
843"
BROADER IMPACTS,0.9242424242424242,"technology is being used as intended but gives incorrect results, and harms following
844"
BROADER IMPACTS,0.9250398724082934,"from (intentional or unintentional) misuse of the technology.
845"
BROADER IMPACTS,0.9258373205741627,"• If there are negative societal impacts, the authors could also discuss possible mitigation
846"
BROADER IMPACTS,0.9266347687400319,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
847"
BROADER IMPACTS,0.9274322169059012,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
848"
BROADER IMPACTS,0.9282296650717703,"feedback over time, improving the efficiency and accessibility of ML).
849"
SAFEGUARDS,0.9290271132376395,"11. Safeguards
850"
SAFEGUARDS,0.9298245614035088,"Question: Does the paper describe safeguards that have been put in place for responsible
851"
SAFEGUARDS,0.930622009569378,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
852"
SAFEGUARDS,0.9314194577352473,"image generators, or scraped datasets)?
853"
SAFEGUARDS,0.9322169059011164,"Answer: [NA]
854"
SAFEGUARDS,0.9330143540669856,"Justification: No such risks, no checkpoints released.
855"
SAFEGUARDS,0.9338118022328549,"Guidelines:
856"
SAFEGUARDS,0.9346092503987241,"• The answer NA means that the paper poses no such risks.
857"
SAFEGUARDS,0.9354066985645934,"• Released models that have a high risk for misuse or dual-use should be released with
858"
SAFEGUARDS,0.9362041467304625,"necessary safeguards to allow for controlled use of the model, for example by requiring
859"
SAFEGUARDS,0.9370015948963317,"that users adhere to usage guidelines or restrictions to access the model or implementing
860"
SAFEGUARDS,0.937799043062201,"safety filters.
861"
SAFEGUARDS,0.9385964912280702,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
862"
SAFEGUARDS,0.9393939393939394,"should describe how they avoided releasing unsafe images.
863"
SAFEGUARDS,0.9401913875598086,"• We recognize that providing effective safeguards is challenging, and many papers do
864"
SAFEGUARDS,0.9409888357256778,"not require this, but we encourage authors to take this into account and make a best
865"
SAFEGUARDS,0.9417862838915471,"faith effort.
866"
LICENSES FOR EXISTING ASSETS,0.9425837320574163,"12. Licenses for existing assets
867"
LICENSES FOR EXISTING ASSETS,0.9433811802232854,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
868"
LICENSES FOR EXISTING ASSETS,0.9441786283891547,"the paper, properly credited and are the license and terms of use explicitly mentioned and
869"
LICENSES FOR EXISTING ASSETS,0.9449760765550239,"properly respected?
870"
LICENSES FOR EXISTING ASSETS,0.9457735247208932,"Answer: [Yes]
871"
LICENSES FOR EXISTING ASSETS,0.9465709728867624,"Justification: Torchvision contributors credited for checkpoints, and datasets as well, in
872"
LICENSES FOR EXISTING ASSETS,0.9473684210526315,"Section 4.
873"
LICENSES FOR EXISTING ASSETS,0.9481658692185008,"Guidelines:
874"
LICENSES FOR EXISTING ASSETS,0.94896331738437,"• The answer NA means that the paper does not use existing assets.
875"
LICENSES FOR EXISTING ASSETS,0.9497607655502392,"• The authors should cite the original paper that produced the code package or dataset.
876"
LICENSES FOR EXISTING ASSETS,0.9505582137161085,"• The authors should state which version of the asset is used and, if possible, include a
877"
LICENSES FOR EXISTING ASSETS,0.9513556618819776,"URL.
878"
LICENSES FOR EXISTING ASSETS,0.9521531100478469,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
879"
LICENSES FOR EXISTING ASSETS,0.9529505582137161,"• For scraped data from a particular source (e.g., website), the copyright and terms of
880"
LICENSES FOR EXISTING ASSETS,0.9537480063795853,"service of that source should be provided.
881"
LICENSES FOR EXISTING ASSETS,0.9545454545454546,"• If assets are released, the license, copyright information, and terms of use in the
882"
LICENSES FOR EXISTING ASSETS,0.9553429027113237,"package should be provided. For popular datasets, paperswithcode.com/datasets
883"
LICENSES FOR EXISTING ASSETS,0.956140350877193,"has curated licenses for some datasets. Their licensing guide can help determine the
884"
LICENSES FOR EXISTING ASSETS,0.9569377990430622,"license of a dataset.
885"
LICENSES FOR EXISTING ASSETS,0.9577352472089314,"• For existing datasets that are re-packaged, both the original license and the license of
886"
LICENSES FOR EXISTING ASSETS,0.9585326953748007,"the derived asset (if it has changed) should be provided.
887"
LICENSES FOR EXISTING ASSETS,0.9593301435406698,"• If this information is not available online, the authors are encouraged to reach out to
888"
LICENSES FOR EXISTING ASSETS,0.960127591706539,"the asset’s creators.
889"
NEW ASSETS,0.9609250398724083,"13. New Assets
890"
NEW ASSETS,0.9617224880382775,"Question: Are new assets introduced in the paper well documented and is the documentation
891"
NEW ASSETS,0.9625199362041468,"provided alongside the assets?
892"
NEW ASSETS,0.9633173843700159,"Answer: [Yes]
893"
NEW ASSETS,0.9641148325358851,"Justification: Anonymized code to reproduce experiments is available as a zip file, with a
894"
NEW ASSETS,0.9649122807017544,"README file to explain how to run it.
895"
NEW ASSETS,0.9657097288676236,"Guidelines:
896"
NEW ASSETS,0.9665071770334929,"• The answer NA means that the paper does not release new assets.
897"
NEW ASSETS,0.967304625199362,"• Researchers should communicate the details of the dataset/code/model as part of their
898"
NEW ASSETS,0.9681020733652312,"submissions via structured templates. This includes details about training, license,
899"
NEW ASSETS,0.9688995215311005,"limitations, etc.
900"
NEW ASSETS,0.9696969696969697,"• The paper should discuss whether and how consent was obtained from people whose
901"
NEW ASSETS,0.970494417862839,"asset is used.
902"
NEW ASSETS,0.9712918660287081,"• At submission time, remember to anonymize your assets (if applicable). You can either
903"
NEW ASSETS,0.9720893141945773,"create an anonymized URL or include an anonymized zip file.
904"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9728867623604466,"14. Crowdsourcing and Research with Human Subjects
905"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9736842105263158,"Question: For crowdsourcing experiments and research with human subjects, does the paper
906"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9744816586921851,"include the full text of instructions given to participants and screenshots, if applicable, as
907"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9752791068580542,"well as details about compensation (if any)?
908"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760765550239234,"Answer: [NA]
909"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9768740031897927,"Justification: No experiments on human subjects.
910"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776714513556619,"Guidelines:
911"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9784688995215312,"• The answer NA means that the paper does not involve crowdsourcing nor research with
912"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792663476874003,"human subjects.
913"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9800637958532695,"• Including this information in the supplemental material is fine, but if the main contribu-
914"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808612440191388,"tion of the paper involves human subjects, then as much detail as possible should be
915"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981658692185008,"included in the main paper.
916"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824561403508771,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
917"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9832535885167464,"or other labor should be paid at least the minimum wage in the country of the data
918"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840510366826156,"collector.
919"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848484848484849,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
920"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856459330143541,"Subjects
921"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864433811802232,"Question: Does the paper describe potential risks incurred by study participants, whether
922"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872408293460925,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
923"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880382775119617,"approvals (or an equivalent approval/review based on the requirements of your country or
924"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988835725677831,"institution) were obtained?
925"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896331738437002,"Answer: [NA]
926"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904306220095693,"Justification: No experiments on or with human subjects.
927"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912280701754386,"Guidelines:
928"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920255183413078,"• The answer NA means that the paper does not involve crowdsourcing nor research with
929"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992822966507177,"human subjects.
930"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936204146730463,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
931"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9944178628389154,"may be required for any human subjects research. If you obtained IRB approval, you
932"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952153110047847,"should clearly state this in the paper.
933"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9960127591706539,"• We recognize that the procedures for this may vary significantly between institutions
934"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968102073365231,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
935"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976076555023924,"guidelines for their institution.
936"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9984051036682615,"• For initial submissions, do not include any information that would break anonymity (if
937"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9992025518341308,"applicable), such as the institution conducting the review.
938"
